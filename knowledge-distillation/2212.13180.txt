# 2212.13180.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-distillation/2212.13180.pdf
# File size: 4072720 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 1
Prototype-guided Cross-task Knowledge Distillation
for Large-scale Models
Deng Li, Aming Wu, Yahong Han, Qi Tian
Abstract —Recently, large-scale pre-trained models have shown
their advantages in many tasks. However, due to the huge
computational complexity and storage requirements, it is chal-
lenging to apply the large-scale model to real scenes. A common
solution is knowledge distillation which regards the large-scale
model as a teacher model and helps to train a small student
model to obtain a competitive performance. Standard knowledge
distillation methods mainly require the teacher model and the
student model to perform the same task, e.g., the student
model and the teacher model share the same label space,
which limits its application in the real scenario. Cross-task
Knowledge distillation can transfer the knowledge of teacher
models to student models of different label spaces without ﬁne-
tuning, which expands the application scenarios of the large-scale
pre-trained model. Existing knowledge distillation works focus
on directly mimicking the ﬁnal prediction or the intermediate
layers of the teacher model, which represent the global-level
characteristics and are task-speciﬁc. To alleviate the constraint
of different label spaces, capturing invariant intrinsic local object
characteristics (such as the shape characteristics of the leg and
tail of the cattle and horse) plays a key role. Considering the
complexity and variability of real scene tasks, we propose a
Prototype-guided Cross-task Knowledge Distillation (ProC-KD)
approach to transfer the intrinsic local-level object knowledge of
a large-scale teacher network to various task scenarios. First, to
better transfer the generalized knowledge in the teacher model
in cross-task scenarios, we propose a prototype learning module
to learn from the essential feature representation of objects in
the teacher model. Secondly, for diverse downstream tasks, we
propose a task-adaptive feature augmentation module to enhance
the features of the student model with the learned generalization
prototype features and guide the training of the student model
to improve its generalization ability. The experimental results
on various visual tasks demonstrate the effectiveness of our
approach for large-scale model cross-task knowledge distillation
scenes.
Index Terms —Knowledge Distillation, Cross-task, Prototype
learning, Large-scale Pretrained Model.
I. I NTRODUCTION
RECENTLY, the Transformer network [1] has achieved
great advances in some visual tasks, such as image
classiﬁcation [2]–[4], object detection [5], [6], image seg-
mentation [7], action recognition [8], and visual language
joint learning [9]–[12]. Transformer networks based on a self-
attention mechanism can process complete input sequences
and own the advantage of parallelization. Therefore, it is
Deng Li and Yahong Han are with the College of Intelligence and
Computing, Tianjin University, Tianjin, China. (email: lideng@tju.edu.cn;
yahong@tju.edu.cn)
Aming Wu is with the School of Electronic Engineering, Xidian University,
Xi’an, China. (email: amwu@xidian.edu.cn).
Qi Tian is with Cloud & AI, Huawei Technologies, Shenzhen, China (email:
tian.qi1@huawei.com).usually used to obtain the pre-trained model from large-scale
datasets [13]. Currently, the common strategy for using large-
scale pre-trained models on cross-task downstream tasks is
ﬁne-tuning. After learning the generalized feature represen-
tation from large-scale datasets, it is then ﬁne-tuning on the
downstream task with a small number of datasets to improve
the performance of the small model. However, due to the huge
computational complexity and huge storage requirements of
these models, it has become a great challenge to apply them
to practical application scenarios with limited resources, e.g.,
mobile devices.
To solve the above model application issue, some model
compression and acceleration technologies are proposed, e.g.,
parameter pruning [14], [15], model quantization [16], and
knowledge distillation (KD) [17]. Particularly, knowledge dis-
tillation is an effective method of model compression, which
distills the knowledge from a large deep neural network
(teacher model) into a small network (student model) [17]–
[21]. Unlike other model compression methods, knowledge
distillation can reduce the size of the network and improve the
performance of small models on downstream tasks regardless
of the structural differences between teacher and student
networks. Its success has been witnessed in a wide range of
applications such as computer vision [17], [22]–[27], speech
recognition [28]–[30], and natural language processing [31]–
[33].
However, these knowledge distillation approaches mainly
require the teacher model and the student model to be the
same task, e.g., the student model and teacher model share
the same label space, which limits their application in the real
scenario such as downstream tasks in different label spaces
(as shown in Fig. 1 (a)).
Cross-task knowledge distillation can transfer the knowl-
edge of the teacher model to downstream tasks in different
label spaces, which expands the application of the teacher
model on a variety of downstream tasks. Existing Same-
task knowledge distillation works mainly to transfer the ﬁnal
prediction logits or the intermediate-layers knowledge of the
teacher model, which are global-level knowledge alignments
and can not be applied to cross-task knowledge distillation
directly. Earlier cross-task knowledge distillation work [34]
aligns the high-order comparison relationship between models
in a local manner, while, this method lag in the representation
power of the invariant intrinsic object and is a two-stage
distillation method.
Under the context of cross-task knowledge distillation, the
intrinsic object characteristics can give beneﬁt guidance to
the training of the student model, for example, the shapearXiv:2212.13180v1  [cs.CV]  26 Dec 2022

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 2
Fig. 1. Comparison between the conventional and proposed methods. (a) The conventional knowledge distillation method in which the teacher model and
student model share the same label space. The downstream task is limited to the same task as the teacher model and is global-level knowledge alignment.
(b) The proposed prototype-guided cross-task knowledge distillation method in which the teacher model and student model with different label spaces. The
prototypes learn the invariant intrinsic local-level representation from the embedding of the large-scale teacher model and guide the learning of various
cross-task student models.
features of the legs of a cattle and a horse when the cattle
and horse belong to the datasets of the teacher model and the
student model, respectively. Considering the complexity and
variability of real scene tasks and the generalization capability
of large-scale pre-trained models, in this paper, we propose
a Prototype-guided Cross-task Knowledge Distillation (ProC-
KD) approach to transfer the local intrinsic knowledge of a
large-scale teacher network to various task scenarios (as shown
in Fig. 1 (b)). And our method of obtaining the downstream
tasks small model is a one-stage training process.
Speciﬁcally, our proposed approach consists of two in-
tegrated modules: a prototype-based representation learning
module and a feature augmentation module.
The prototype learning module is carefully designed to
capture essential feature information from the intermediate
feature of the teacher model. Next, the learned prototype
representation is fed into the feature augmentation module.
The feature augmentation module targets at enriching the
student model feature which is more related to the prototype
representation while suppressing the unrelated feature.
To guide the training of the student model with the learned
generalized prototype representation, we employ a consistency
loss to enable the maximum agreement between the prototype
augmented features and student network features.
In the experiments, we ﬁrst verify the effectiveness of the
proposed method on various cross-task knowledge distillation
tasks. Then, we evaluate the proposed method on standard
knowledge distillation tasks. The experiment results in two
scenarios demonstrate the effectiveness and generality of our
method.
Our contributions in this paper are summarized as follows:
(1) We propose a prototype-guided knowledge distillation
approach to transfer the intrinsic knowledge from a large-scale
model to different cross-task small models without ﬁne-tuning
the teacher model on the downstream task dataset and improve
the student model generalization ability.(2) We propose a prototype learning module and a feature
augmentation module to learn the invariant intrinsic knowledge
of the large-scale teacher model and enhance the small student
model with the attention mechanism, respectively.
(3) We verify our method on both cross-task knowledge dis-
tillation and standard knowledge distillation on various visual
tasks. The experiment results demonstrate the effectiveness
and generality of our approach.
II. R ELATED WORK
A. Large-scale Model
Transformer-based large-scale models have achieved great
success in natural language processing [35], computer vision
[2]–[4], and multi-modal task learning [9], [10].
In this paper, we mainly discuss the recent efforts of the
Transformer-based model in the ﬁeld of computer vision. As
a pioneered work, ViT [2] constructs the token embedding for
the Transformer by directly dividing each image into 16 16
patches and projecting them into embeddings. The experiments
are carried out on large-scale training datasets ( e.g., ImageNet-
21k and JFT-300M). Deit [3] introduced several training
strategies to speed up the training of ViT. Deit [3] introduced a
distillation method to transfer CNN-based features to a visual
Transformer. However, it is not the paradigm of distilling
the knowledge networks to smaller student networks, which
means the parameters of a model after distillation is not
greatly reduced. Swin Transformer [4] introduces shifted non-
overlapping window partitions and restricts self-attention com-
putation within sub-windows. Apart from Transformer-based
large-scale model, series of CNN-based large-scale models
[36] have also been proposed. Radosavovic et al. presents a
new network design paradigm that combines the advantages
of manual design and neural architecture search (NAS). These
cumbersome large-scale models demand heavy computation
power and fail to be applied on devices with limited resources.

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 3
A Large-scale  Teacher  Model
A Downstream Student ModelClassifierAugment ation
ModuleHidden 
Embeddings
Embedding -layer  DistillationConcatCross -EntropyLoss
Cross -EntropyLossConsistency
Loss
ClassifierClassifierPrototype
Module
Fig. 2. Illustration of our proposed prototype-guided cross-task knowledge distillation framework. ProC-KD includes an embedding layer distillation module,
a prototype learning module, and a feature augmentation module. The blue arrow in the framework represents generalized representation learning based on
prototypes. The green and red arrows indicate that the prototypes are used to enhance the features extracted from the teacher model and the student model,
respectively. The augmentation module and the student model share the same classiﬁer.
B. Knowledge Distillation
Knowledge distillation is a model compression technology
that transfers the knowledge from a larger deep neural network
into a small network.
The methods of knowledge distillation are mainly divided
into response-based knowledge distillation [17], [24], feature-
based knowledge distillation [22], [23], and relation-based
knowledge distillation [25], [26].
The main idea of response-based knowledge distillation is
to directly transfer the last output layer neural response of the
teacher model. Hinton et al. [17] and Ba et al. [37] propose to
shift the knowledge by learning the probabilities distribution
via softened labels. However, this method depends on the
class probability distribution. The effective method to this is
to distill the feature-based or the relation-based knowledge
from the teacher model. The goal of feature-based knowledge
distillation is to match the intermediate representation of the
student model with the teacher model. Fitnets [22] initially
introduce intermediate representations learning, in which hints
are deﬁned as the outputs of a teacher’s hidden layer to
improve the student’s learning process. Inspired by [22], a
variety of feature-based knowledge distillation methods [38]–
[40] are proposed.
Relation-based knowledge distillation methods explore the
relationships between different layers [25] or data samples
[26].
KD has also been extensively studied in Transformer-based
language models [31]–[33]. While previous knowledge dis-
tillation methods for Transformer-based models mainly focus
on the NLP domain and the task of the teacher model is the
same as the student model. Ye et al. [34] deal with a scenario
distilling the knowledge from a cross-task teacher. However, it
lags in the representation of the invariant intrinsic object andis a two-stage distillation method
Different from existing works, in this paper, we explore
the scenario of learning the intrinsic local-level features and
reusing the knowledge of large-scale models for different
downstream tasks in a cross-task manner.
III. M AINAPPROACH
Our approach aims to distill the knowledge in the large-scale
model to different downstream small models. The label space
of the student model is different from the teacher model, which
is called a cross-task knowledge distillation. Existing same-
task knowledge distillation methods mainly directly mimic the
ﬁnal prediction or the intermediate layers of the teacher model,
which transfer the global features and are task-speciﬁc. The
local intrinsic representations can greatly beneﬁt the cross-
task student model in understanding the novel dataset of the
downstream task. To improve the generalization ability of the
downstream model, we propose a prototype-guided cross-task
knowledge distillation method to transfer the invariant intrinsic
knowledge from large-scale teacher to small student model as
shown in Fig. 2. The key module of our method contains a
prototype learning module and a feature augmentation module.
A. Prototype-based Representation Learning
Compared to previous methods that directly mimic the ﬁnal
predicted logits or the intermediate layers of the teacher model,
our approach designed a module to learn the intrinsic repre-
sentation. Recent studies [41], [42] have demonstrated that
constructing prototype learning in models can help to solve
novel dataset problems. The category-speciﬁc information can
be captured by prototype learning. Inspired by this idea, we

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 4
Fig. 3. The schematic illustration of (a) prototype learning module and (b) feature augmentation module. Conv and FC Layer separately indicates convolution
and fully-connected layer. 	,,
,, and [,] denote the residual operation, element-wise multiplication, matrix multiplication, element-wise addition, and
concatenation operation, respectively.
propose a prototype-guided module for the teacher-student dis-
tillation architectures to learn the generalized representations
with the guidance of prototypes.
The architecture of the prototype learning module is shown
in Fig. 3 (a). The forward process is ﬁrst to align the prototypes
with the input feature, then reconstruct the prototype-related
feature with the attention mechanism, and ﬁnally, aggregate
the reconstructed attention features with the input features.
The whole process can be divided into three sub-processes,
which are Alignment ,Attention , and Aggregation .
Speciﬁcally, before feeding forward the two-dimensional
(2D) hidden layer feature extracted by the Transformer-based
model, we reshape it to F2RDHW, whereD,H,
andWrepresent the feature dimension, height, and width,
respectively.
The prototypes are deﬁned as P(pi2RD;i= 1;2;:::;n ).
In the Alignment sub-process, both the deﬁned prototype
tensor matrix and the input feature tensor matrix are expanded
tonD(WH), and we align them with residual operation
(F P). In the Attention sub-process, we calculate the feature
descriptors Vbased on the attention maps, which can be
expressed as follows:
Vi=WHX
j=1eLji
Pn
i=1eLji(Fj pi): (1)
In the Aggregation sub-process, we ﬁrst concatenate the
feature descriptors Vand the input features, then transform
the result with a nonlinear transformation block f. It can be
formulated as follows:
Opro=f(concat [F; V rWp+bp]); (2)
whereVris the reshaped feature descriptors V,Wpandbpare
the weight and bias of the fully-connected layer, respectively,
andconcat [;]indicates the concatenation operation. The shape
of the output feature Oprois the same as the input feature F.
Through the above processes. The generalized representa-
tion of the prototypes could be learned from the input featureof the large-scale teacher model. Upon such generalized pro-
totypes, we seek to enhance the student features with these
prototypes.
B. Feature Augmentation with Prototypes
To improve the generalization ability of student models of
different downstream tasks. The learned generalized proto-
types are used to enhance the feature in the feature augmenta-
tion module, as shown in the right part of Fig. 2. The main idea
is to enrich the feature which is more related to the prototype
representation while suppressing the unrelated feature.
Fig. 3 (b) shows the architecture of feature augmentation
module. The forward process is ﬁrst to pay attention to the
feature related to the prototype representation, then enhance
the input feature with the prototype-related feature. The whole
feature augmentation process can also be broken into two sub-
processes, which are Attention andAugmentation .
Concretely, for the learned prototypes Pl2RnDand the
hidden features Fh2RtD. In the Attention sub-process,
we ﬁrst encode the prototypes and input features respectively.
Attention map Ais obtained by calculating the softmax of
cross-product between learned prototypes Pland encoded
featureFe, which can be expressed as follows:
A=softmax 
FePT
l
; (3)
and then the attention feature is obtained by calculating the
cross-product between the attention map and the prototypes.
In the Augmentation sub-process, we concatenate the at-
tention feature with the encoded input features. The fully-
connected layer is applied to transform the result. The original
input hidden layer feature is enhanced with the prototype-
related feature through element-wise sum operation. The
whole feature augmentation process can be written as:
Oaug=ReLU ( (concat [Fe; AP l]) +Fr); (4)
whereOaugis the output of the feature augmentation module,
indicates the function of the fully-connected layer, Fris
the reshaped of Fh,Whis the linear transform weight matrix,

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 5
andconcat [;]indicates the concatenation operation. At last,
the enhanced feature is input to the classiﬁer shared with the
student model to predict the categories.
C. Cross-task Knowledge Distillation
In this paper, the knowledge distillation of student models
and teacher models in different label spaces is deﬁned as
cross-task knowledge distillation. we proposed the prototype
learning module and the feature augmentation module to guide
the training of the student model in cross-task distillation
scenarios and improve its generalization ability. Above we
have described the designed prototype module and feature
enhancement module respectively. Here, we also design some
loss functions to constrain the training of cross-task knowledge
distillation. The distillation training loss function includes the
embedding-layer knowledge distillation loss function and the
prototype learning loss function.
For the embedding-layer knowledge distillation, followed
by [33], we distill both the knowledge of attention maps and
hidden state features from the large-scale Transformer-based
teacher model. Assuming that we are distilling the knowledge
from amlayers Transformer-based teacher model to the n
layers Transformer-based student model. We need to select n
out ofmlayers from the teacher model. Speciﬁcally, for the
attention map, the student learns to ﬁt the selected multi-head
attention maps in the teacher network, and the loss function
for attention-based distillation can be deﬁned as follows:
Lemb=nX
j=1hX
i=1MSE (AS
i; AT
i) +nX
j=1MSE (FS
iWh; FT
i);
(5)
whereTindicates the teacher model, Srefers to the student
model,his the attention head number, Ai2Rllmeans
the attention matrix corresponding to the i-th head of teacher
or student, lis the input token length, FS2Rld0is the
student hidden feature and FT2Rldis the hidden feature
of the teacher model. dandd0denote the hidden embedding
sizes of the teacher model and student model, respectively.
Wh2Rd0dis a learnable transformation weight matrix,
which transforms the hidden layer features of the student
model into the same dimensions as the features of the teacher
model, and MSE ()indicates the mean squared error loss
function.
For the prototype learning, we deﬁned the consistency loss
functionLconand the classiﬁcation loss function Lprocls .
The consistency loss is obtained by calculating the Kullback-
Leibler (KL) divergence loss between the predicted logits ycon
from the prototype augmentation module and the predicted
logitsystufrom the student model, Lcon=H(ycon;ystu)).
The classiﬁcation loss is the softmax cross-entropy loss be-
tween prototype prediction yconand ground-truth labels y.
Thus the loss function of the prototype learning can be
expressed as:
Lpro=Lcon+Lprocls; (6)
In addition to the embedding layer feature distillation loss
function and the prototype learning loss function, we deﬁnethe student model loss function as Lstu. The joint training
loss function for our cross-task knowledge distillation can be
expressed as:
Ltotal=embLemb+proLpro+stuLstu; (7)
whereemb,pro, andstuare the weights of the embedding
layer feature distillation loss, the prototype learning loss, and
the student model loss, respectively.
IV. E XPERIMENTS
To evaluate the general effectiveness of our method of
distilling knowledge from the large-scale models in the cross-
task scenarios, we conducted experiments on cross-task knowl-
edge distillation and standard same-task knowledge distillation
settings. Experiments were carried out on different visual
tasks, e.g., image classiﬁcation and object detection for each
setting. In this paper, the knowledge distillation scheme is set
as ofﬂine distillation, which means the weights of the teacher
network are frozen during the training process.
A. Cross-Task Knowledge Distillation
1) Image Classiﬁcation: We carried out experiments on the
Transformer-based model in three downstream tasks, including
standard image classiﬁcation, long-tailed image classiﬁcation,
and cross-domain image classiﬁcation. Here, all the teacher
models are trained on ImageNet-1K [13] dataset in our exper-
iments.
Datasets. CIFAR-100 [44] consists of 50,000 training im-
ages and 10,000 validation images. It contains 100 categories
and each class contains 600 images. The size of each image
is 3232. Following [45], [46], the long-tailed CIFAR-100
is created by reducing the number of training samples for
each class, but with the veriﬁcation set unchanged. We deﬁne
an imbalance ratio ,i.e.,=Nmax=Nmin.represents
the ratio of sample sizes between the most frequent class
and the least frequent class. In this way, sample sizes decay
exponentially between classes. In our experiment, we set the
imbalance ratio to 10. iNaturalist 2018 [47] contains over
450,000 training images from 8,142 different species of birds,
mammals, reptiles, and plants among others. Compared with
ImageNet and other image classiﬁcation datasets, iNaturalist
exhibits a long-tail distribution, and many species have rela-
tively few images. We used the ofﬁcial training and validation
split in the experiment, with 437,513 images for training and
24,424 images for validation. The Ofﬁce-Home [48] dataset
has been created to evaluate domain adaptation methods for
image classiﬁcation. It consists of 15,500 images from four
different domains: Artistic images (Ar), Clip Art (Cl), Product
images (Pr), and Real-World images (Rw). Each domain in this
dataset contains 65 categories, and the images are from ofﬁce
or home scenes. In our experiments, the Real-World images
(Rw) are used as the training set, and the other domains are
used as the test sets.
Implementation Details. We conduct our experiments on
the well-known ViT [2] and Swin-Transformer [4] models.
For the ViT, the teacher model is a 12-layer ViT-B model
and the student model is a 6-layer small ViT model. The

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 6
TABLE I
THE MEAN ACCURACY (%) OF THREE CROSS -TASK IMAGE CLASSIFICATION KNOWLEDGE DISTILLATION .THE CROSS -DOMAIN IMAGE CLASSIFICATION
TASK IS PERFORMED ON THE OFFICE -HOME DATASET . THELT-CIFAR INDICATES LONG -TAILED CIFAR 100. FBKD-P ROC-KD IS OUR METHOD BY
PLUGGING THE PROPOSED PROC-KD INTO THE FBKD METHOD . THE TEACHER MODELS ARE ALL TRAINED ON IMAGE NET-1K [13] AND FIXED THE
WEIGHT DURING DISTILLATION TRAINING .
Teacher (param.) Method param.Standard Long-tailed Cross-domain
CIFAR-100 LT-CIFAR iNaturalist 2018 Rw!Ar Rw!Cl Rw!Pr
ViT-B [2] (86M)Student Model 43M 78.84 55.83 - 20.85 16.91 34.85
RKD [26] 43M 87.13 76.52 - 57.31 40.64 73.50
FBKD [33] 43M 86.16 72.69 57.14 60.28 40.02 75.51
FBKD-ProC-KD (Ours) 43M 87.46 78.32 58.67 61.41 40.96 76.83
Swin-L [36] (197M )Student Model 110M 78.90 41.48 - 26.87 20.51 43.32
RKD [26] 110M 83.99 58.94 - 22.95 15.50 30.71
FBKD [33] 110M 83.63 57.83 70.19 41.29 29.03 63.40
FBKD-ProC-KD (Ours) 110M 84.21 68.23 72.11 42.16 30.24 64.27
Fig. 4. The comparison of attention maps in FBKD and FBKD-ProC-KD (ours) by using the Transformer Interpretability method [43]. Here, the second
layer and the last layer of the ViT are selected as the shallow layer and the deep layer, respectively.
indexes of hidden layers selected for distillation in the teacher
model are [2, 4, 6, 8, 10, 12]. Both the attention map
knowledge and hidden layer feature knowledge are distilled.
All the coefﬁcients of the loss function Eq. 7 are 1 except for
theembis 0.3. In the training phase, the 4-th,8-th, and 12-
th layer hidden features are selected to concatenate and then
input to the prototype learning module and augment module
to train the prototypes. The number of prototypes is set to 72.
The AdamW optimizer is used with a learning rate of 5e-4
and a weight decay of 0.05. The input size of the image is
224224and the batch size is set to 32 for each GPU.
For the Swin-Transformer, The teacher model is 24-layer
Swin-L, we distill the knowledge from the middle 18-layers
to 6-layers, and the student model is a 12-layers Swin-
Transformer. The last two hidden layers features of thebackbone are selected for the prototype learning module and
augmentation module after the concatenation operation. We
use the AdamW optimizer with an initial learning rate of 5e-4
and a weight decay of 0.05. The training batch size is set to
64 for each GPU.
All the experiments are run on 8 Nvidia Tesla V100 GPUs
(32GB VRAM, PCIe connection). We use NCCL for multi-
node parallel training. Gradient accumulation is also applied
to reduce multi-GPU communication overheads.
Results and Analysis. Table I shows the experimental
results on the standard, long-tailed, and cross-domain image
classiﬁcation tasks. We compare it with Relation Knowledge
Distillation (RKD) [26] by reimplementing it in our experi-
mental setting. Followed by Tinybert [33], FBKD is a feature-
based knowledge distillation method for the Transformer-

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 7
TABLE II
RESULTS (%) OF CROSS -TASK OBJECT DETECTION KNOWLEDGE DISTILLATION ON CITYSCAPES AND FOGGY CITYSCAPES . THE TEACHER MODELS ARE
ALL TRAINED ON THE COCO [49] DATASET AND FIXED THE WEIGHT DURING DISTILLATION TRAINING .
Cityscapes
Method person rider car truck bus train motorcycle bicycle mAP
Student Model 53.1 55.1 70.1 31.3 56.1 31.6 40.2 44.6 47.8
CWD [50] 63.2 65.2 77.7 48.6 72.8 49.8 54.2 58.1 61.2
ProC-KD (Ours) 63.9 65.1 77.8 51.9 74.3 51.7 52.5 59.9 62.1
FoggyCityscapes
Student Model 40.8 40.3 63.0 27.8 42.3 11.6 27.3 31.8 35.6
FBKD [33] 52.0 54.1 68.6 37.5 53.9 35.7 40.6 49.0 48.9
CWD [50] 53.3 55.8 71.9 37.5 57.1 46.8 43.2 50.9 52.1
FBOD [51] 52.1 51.2 69.3 36.0 53.7 42.6 41.4 45.4 48.9
FBKD-ProC-KD (Ours) 51.8 55.0 68.8 38.7 53.4 47.1 38.8 45.2 49.9
ProC-KD (Ours) 53.8 57.9 73.1 40.3 57.7 51.2 44.2 51.5 53.7
based model that distills the knowledge from embedding layers
and attention maps to the student model. FBKD-ProC-KD is
our method by plugging the proposed ProC-KD into the FBKD
method.
Standard Image Classiﬁcation. Compared with the baseline
method, our ProC-KD improves by 1.3% and 0.6% on ViT
and Swin-Transformer, respectively. This demonstrates that
our ProC-KD method can promote the prototypes to learn the
generalized representation and improve the performance of the
student model in cross-task image classiﬁcation scenes.
Long-tailed Image Classiﬁcation. For the long-tailed
CIFAR-100 dataset, our ProC-KD improves the performance
by 5.6% and 10.4% on ViT and Swin-Transformer, respec-
tively. For the iNaturalist 2018 dataset, our ProC-KD sepa-
rately improves the performance by 1.5% and 1.9% on ViT
and Swin-Transformer. This demonstrates that ProC-KD can
improve the generalization ability of the student model in the
long-tailed image classiﬁcation tasks.
Cross-domain Image Classiﬁcation. The cross-domain im-
age classiﬁcation experiment is conducted on the Ofﬁce-Home
dataset. As can be seen, compared with the FBKD baseline,
ProC-KD improves the performance by 1.3% on domain shift
Rw!Pr and by 1.2% on hardest domain shift Rw !Cl. This
demonstrates that distilling the knowledge from the model
trained on large-scale datasets can improve the performance
of the student model in the cross-domain scene, and our ProC-
KD can further improve the generalization ability.
Fig. 4 shows the visualization results of the attention maps
in FBKD and our FBKD-ProC-KD. As can be seen, compared
with the FBKD baseline, the attention map of our method
FBKD-ProC-KD focuses more on objects in both the shallow
layer and deep layer. It indicates that our ProC-KD method can
promote the training of the student model in both the shallow
layers and the deep layers.
2) Object Detection: We also conduct experiments on stan-
dard object detection and cross-domain object detection. For
the cross-domain object detection task, we only take the source
domain as the training set and the target domain as the testing
set. Here, all the teacher models are trained on COCO [49]
dataset and frozen the weight during the distillation training.
Datasets. Cityscapes [52] is an urban street dataset of 8
categories of objects. It contains 2975 and 500 images in
the training and validation set. FoggyCityscapes [53] is adataset obtained by synthesizing different degrees of fog on
Cityscapes [52]. It contains 2975 training and 500 validation
images, respectively. Daytime-sunny, Dusk-rainy, and Night-
rainy [53] are three street scene datasets under different
weather environments collected from the BDD-100k dataset.
In our experiments, we select 27,708 images from Daytime-
sunny as the training set, and 2,494 and 3,501 images from
Night-rainy and Dusk-rainy as the test set for the two scenar-
ios, respectively.
Implementation Details. For the cross-task knowledge
distillation experiment of standard object detection and cross-
domain object detection, the teacher model is the Cascade
Mask-RCNN with the backbone of 24 layers Swin-Base [4]
model trained on the COCO [49] dataset and the student model
is the Cascade Mask-RCNN with the backbone of 12-layers
Swin-Tiny model. In addition to the distillation learning of the
hidden layer knowledge in the teacher model, the fourth layer
feature of the FPN is selected as the input to the prototype
learning module and augmentation module for generalized
representation learning. The coefﬁcients of proandemb
are both set to 1. We run the SGD optimizer with the initial
learning rate of 0.01 and the parameter decay of 0.0001 for
36 epochs. The batch size is set to 2 for each GPU.
To compare with the state-of-the-art knowledge distillation
methods of object detection. We also reimplement some repre-
sentative feature-based knowledge distillation object detection
methods based on our experimental setting, e.g., CWD [50]
and FBOD [51].
Results and Analysis of Standard Object Detection.
Table II shows detection results on Cityscapes and FoggyCi-
tyscapes. Here, we reimplement the method of CWD [50] and
FBOD [51] in our experimental setting. FBKD is a knowledge
distillation method that follows Tinybert [33], and we only
distill the hidden features of the backbone. We can see that our
method boosts the performance under the cross-task knowl-
edge distillation scene signiﬁcantly. For Cityscapes, compared
to the baseline of CWD [50], our method separately improves
the performance by 0.9%. For FoggyCityscapes, compared to
FBKD and CWD [50], our method separately improves the
performance by 1.0% and 1.4%. This demonstrates that the
generalized prototype representation is helpful for the learning
of the student model on object detection.
Fig. 5 and Fig. 6 show the visualization of the detection

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 8
Fig. 5. Qualitative results on Cityscapes. Compared with CWD baseline, our ProC-KD method could localize and recognize objects accurately, e.g., the bus,
bicycle, truck, person.
Fig. 6. Qualitative results on FoggyCityscapes. Compared with CWD [50] baseline, our ProC-KD method could localize and recognize objects accurately in
foggy images, e.g., the rider, bicycle, bus, car.
TABLE III
RESULTS (%) OF CROSS -TASK OBJECT DETECTION KNOWLEDGE DISTILLATION ON CROSS -DOMAIN OF CITYSCAPES!FOGGY CITYSCAPES . HERE, W E
TRAIN THE MODEL ON THE TRAINING DATA OF CITYSCAPES AND TEST THE MODEL ON THE TEST DATASET OF FOGGY CITYSCAPES . THE TEACHER
MODELS ARE ALL TRAINED ON THE COCO [49] DATASET AND FIXED THE WEIGHT DURING DISTILLATION TRAINING .
Method person rider car truck bus train motorcycle bicycle mAP
Student Model 22.3 17.6 18.0 4.5 9.1 0.0 9.1 20.5 12.6
CWD [50] 41.0 48.7 51.3 20.2 33.1 10.6 30.8 44.4 35.0
ProC-KD (Ours) 41.3 48.9 51.7 24.6 33.8 13.5 32.6 44.4 36.3
results on Cityscape and FoggyCityscapes respectively. The
ﬁrst row is the ground truth of the objects, the second row is
the detection results of the baseline method CWD [50], and
the third row is the detection results of ProC-KD. We can
see that, compared with CWD, our method ProC-KD could
localize and recognize objects accurately in the normal scene
and foggy images.
Results and Analysis of Cross-domain Object Detection.
Table III shows the results on cross-domain object detection
of Cityscapes!FoggyCityscapes. As we can see, compared
with baseline CWD [50] our ProC-KD improves the perfor-mance signiﬁcantly by 1.3%. For most of the object categories,
our method outperforms CWD [50]. This demonstrated that
our ProC-KD method could improve the generalized ability
of the student model on cross-domain object detection.
Table IV shows the results on cross-domain object detection
of Daytime-sunny !Night-rainy and Daytime-sunny !Dusk-
rainy. For Daytime-sunny !Night-rainy, compared with
the CWD [50] baseline, our ProC-KD method improves the
mAP by 0.9%. For Daytime-sunny !Dusk-rainy, our ProC-
KD method improves the mAP by 0.5%. The reason that the
performance of our ProC-KD is lower than CWD on the object

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 9
TABLE IV
RESULTS (%) OF CROSS -TASK OBJECT DETECTION KNOWLEDGE DISTILLATION ON CROSS -DOMAIN OF DAYTIME -SUNNY!NIGHT -RAINY AND
DAYTIME -SUNNY!DUSK-RAINY . HERE, W E TRAIN THE MODEL ON THE DAYTIME -SUNNY DATA AND TEST THE MODEL ON THE NIGHT -RAINY AND
DUSK-RAINY DATA . THE TEACHER MODELS ARE ALL TRAINED ON THE COCO [49] DATASET AND FIXED THE WEIGHT DURING DISTILLATION TRAINING .
Daytime-sunny!Night-rainy
Method bicycle bus car motorcycle person rider truck mAP
Student Model 24.3 9.1 33.8 1.1 12.3 9.1 16.1 15.1
CWD [50] 38.6 17.1 49.4 9.7 24.4 15.6 34.4 27.0
ProC-KD (Ours) 40.9 18.3 49.4 8.6 26.1 18.2 35.7 27.9
Daytime-sunny!Dusk-rainy
Student Model 40.6 14.9 66.0 11.5 25.8 15.2 39.7 30.5
CWD [50] 49.9 34.8 73.9 24.0 43.9 32.0 54.7 44.7
ProC-KD (Ours) 52.6 36.6 73.3 21.6 46.5 31.6 54.6 45.2
Fig. 7. Qualitative results on Daytime-sunny !Night-rainy. Compared with CWD [50] baseline, our ProC-KD method could localize and recognize objects
accurately in foggy images, e.g., the person, bus, truck, car.
Fig. 8. Qualitative results on Daytime-sunny !Dusk-rainy. Compared with CWD baseline, our ProC-KD method could localize and recognize objects accurately
in rainy images, e.g., the person, bus, car, truck, bycicle.
of the motorcycle may be that the number of the ground truth
of the motorcycle in the testing set is quite small. It only
contains 49 ground truths of motorcycles in the Daytime-sunny!Night-rainy testing set and only 110 ground truths in the
Daytime-sunny!Dusk-rainy testing set.
Fig. 7 and Fig. 8 show the visualization of the cross-domain

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 10
Fig. 9. The tSNE of the REFILLED (left) and Ours (right) over 10 classes sampled from CIFAR-10. The larger the NMI value means the better embedding
quality.
object detection results on the Daytime-sunny !Night-rainy
and Daytime-sunny !Dusk-rainy respectively. The ﬁrst row is
the ground truth of the objects, the second row is the detection
results of the baseline method CWD [50], and the third row is
the detection results of ProC-KD. Compared with CWD, our
ProC-KD could localize and recognize objects in both night
rainy images and dusk rainy images accurately.
B. Standard Same-task Knowledge Distillation
We regard knowledge distillation in which the teacher model
and student model share the same label space as standard
knowledge distillation. The method we proposed is a general
method, which can also be used in the setting of standard
knowledge distillation. Here we verify the effectiveness of
our method on standard knowledge distillation in the image
classiﬁcation task and object detection task respectively.
1) Image Classiﬁcation: In this part, the teacher model
and student model share the same image label space. Here,
we also verify the effectiveness of our method on the clas-
siﬁcation model of CNN structure, we set the teacher model
and the student model to be Wide-ResNet which is a CNN-
based network structure. By changing the depth and width
of the student model, we can get different student models
to verify the adaptability of the method to different network
structures. The dataset used in the experiment is CIFAR-100
[44]. Followed with REFILLED [34], all teacher models are
set as Wide-ResNet with a depth of 40 and width of 2 in these
experiments. The accuracy of the teacher model is 74.44%.
Different from the two-stage optimization of REFILLED [34],
our ProC-KD only performs one-stage optimization.
Results and Analysis. Table V shows comparison results
between our method and other SOTA distillation methods
with different student models. Same with REFILLED [34], theTABLE V
RESULTS (%) ON THE STANDARD IMAGE CLASSIFICATION KNOWLEDGE
DISTILLATION SCENE . HERE,THE TEACHER MODEL AND STUDENT MODEL
SHARE THE SAME CIFAR-100 [44] LABEL SPACE .
Method/(depth, width) (40, 1) (16, 2) (16, 1)
Student 68.97 70.15 65.44
KD [17] 70.46 71.87 66.54
FitNet [22] 68.66 70.89 65.38
AT [38] 69.85 71.06 65.31
NST [23] 68.00 71.19 64.95
VID-I [55] 71.51 73.31 66.32
RKD [26] 72.18 72.56 65.22
REFILLED [34] 72.72 74.01 67.56
ProC-KD (Ours) 74.05 74.49 68.06
accuracy of our method is the result of the test on the test set
after the training convergence on the training set. The results of
other comparison methods are cited from REFILLED [34]. It
can be seen from Table V that compared with other knowledge
distillation methods, our method achieves the best accuracy
in three student models with different structures. Compared
with REFILLED [34], our ProC-KD outperforms by 1.3%,
0.48%, and 0.5% in the three student network structures
of (depth, width)=(40,1), (depth, width)=(16,2), and (depth,
width)=(16,1). This demonstrates the effectiveness of our
method in standard image classiﬁcation knowledge distillation
scene.
Figure 9 shows the visualization results of the randomly
sampled 10 classes embedding features with tSNE [54]. The
normalized mutual information (NMI) is used as the criterion
to measure the embedding quality, the value is larger means the
embedding quality is better. we can see that for the embedding
features of 10 categories sampled randomly, our method is
more discriminative and has higher NMI values.

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 11
Fig. 10. The error analysis Precision-Recall curve of all objects, large size objects, medium size objects, and small size objects on the COCO [49] dataset.
The top row is from the baseline CWD [50] and the bottom row is from our ProC-KD. Here, C75 indicates the results at 0.75 IoU threshold, C50 indicates the
Results at 0.50 IoU threshold, Loc indicates the results after ignoring localization errors, Sim indicates the results after ignoring the similar classes from the
same supercategory false positives, Oth indicates the results after ignoring all category confusions, BG indicates the results after ignoring all false positives,
and FN indicates the results after ignoring all false negatives.
TABLE VI
RESULTS (%) ON THE STANDARD OBJECT DETECTION KNOWLEDGE
DISTILLATION SCENE . HERE,THE TEACHER MODEL AND STUDENT MODEL
SHARE THE SAME COCO [49] LABEL SPACE . ’]’INDICATES THE RESULTS
THAT WE REIMPLEMENT WITH THE RELEASED CODE
Method AP AP 50 AP75 APs APm APl
Teacher 44.3 62.7 48.4 25.4 48.4 58.1
Student 38.4 59.0 42.0 21.5 42.1 50.3
Chen [56] 38.7 59.0 42.1 22.0 41.9 51.0
Wang [57] 39.1 59.8 42.8 22.2 42.9 51.1
Heo [58] 38.9 60.1 42.6 21.8 42.7 50.7
FBOD [51] 41.5 62.2 45.1 23.5 45.0 55.3
CWD [50] 41.7 62.0 45.5 23.3 45.5 55.5
CWD][50] 41.6 61.6 45.5 22.4 45.9 55.0
ProC-KD (Ours) 42.1 62.7 46.0 23.5 45.8 57.1
2) Object Detection: We also apply our prototype-guided
knowledge distillation method to the standard object detection
knowledge distillation task. To make a fair comparison, fol-
lowed with CWD [50] and [51] the teacher model in the exper-
iments is set as Cascade Mask RCNN with ResNeXt101 back-
bone, and the student model is Faster-RCNN with ResNet-50
backbone. Different from the cross-task knowledge distilla-
tion experiments, here, the training dataset of the pre-trained
teacher model and knowledge distillation process are both
performed on the COCO dataset.
Results and Analysis. Table VI shows comparison results
between our method and other state-of-the-art distillation
methods on object detection. The results of other methods
are cited from CWD [50]. It can be seen that our method
outperforms other knowledge distillation methods in different
IoU and different object sizes. In particular, we achieve a 1.6%
improvement over the CWD [50] on APl.
Figure 10 shows the error analysis Precision-Recall curve of
all objects, large size objects, medium size objects, and smallsize objects under different conditions on the COCO dataset.
The top row is the results of the baseline method CWD [50]
and the bottom row is the detection results of ProC-KD. We
can see that compared with CWD [50] our ProC-KD achieves
better performance on different IoU thresholds for all different
size objects. Compared with CWD, Ours improves by 0.029
and 0.024 on large objects and small objects when ignoring the
localization errors, respectively. This indicates our method can
provide more precise classiﬁcation information. Our ProC-KD
outperforms the CWD [50] by an average of 0.014 on all area
objects after ignoring localization errors, ignoring the similar
classes from the same supercategory, ignoring all category
confusions, and ignoring all false positives, which demonstrate
a better location and recognition ability of our method.
C. Ablation Study
In this part, we ablate the number of prototypes and the
important design elements in the proposed prototype-guided
knowledge distillation.
1) design elements: We study the effects of the design
elements on the FoggyCityscapes. Results are shown in Table
VII, it is observed that (a) Compared with the CWD [50]
baseline, 0.3% (52.4%-52.1%) mAP boost can be obtained
with the prototype learning module, indicating that the pro-
totype representation is beneﬁcial to knowledge distillation.
(b) The combination of the prototype learning module and
feature augmentation module leads to a signiﬁcantly mAP
improvement, which is 1.3% (53.7%-52.4%). The reason may
be that the feature augmentation module enriches the feature
which is more related to the object.
2) number of prototypes: We performed ablation experi-
ments on the number of prototypes on the long-tailed CIFAR-
100 dataset with ViT model. The teacher model is a 12-layer

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 12
TABLE VII
ABLATION STUDY RESULTS (%) OF DESIGN ELEMENTS ON FOGGY CITYSCAPES [53].
Method bicycle bus car motorcycle person rider train truck mAP
CWD [50] 53.3 55.8 71.9 37.5 57.1 46.8 43.2 50.9 52.1
ProC-KD (w/proto) 53.4 56.5 72.3 38.1 55.3 47.7 45.4 50.5 52.4
ProC-KD 53.8 57.9 73.1 40.3 57.7 51.2 44.2 51.5 53.7
TABLE VIII
THE MEAN ACCURACY (%) OFPROC-KD WITH THE DIFFERENT NUMBER
OF PROTOTYPES ON THE LONG -TAILED CIFAR-100 DATASET .
Model/Number 24 48 72 96
ProC-KD 77.75 78.11 78.32 78.28
Base version of ViT model, and the student model is a 6-
layer ViT model. The number of prototypes is set as 24, 48,
72, and 96 respectively. Here, we only use a different number
of prototypes and keep other network settings unchanged to
compare the performance of the model. Table VIII shows
that the accuracy of the model increases as the number of
prototypes increases, and the best accuracy is 78.32% when the
number of prototypes is 72. It indicates that the small number
of prototypes could not learn the generalized representation
sufﬁciently. In our experiment, the number of prototypes in
prototype-guided knowledge distillation methods is set to 72.
TABLE IX
ABLATION STUDY OF LOSS FUNCTION HYPERPARAMETER ON
FOGGY CITYSCAPES [53].
Method/Number embprostu mAP(%)
Student 0 0 1 35.6
CWD 1 0 1 52.1
1 0.3 1 1 49.5
2 1 0.3 1 52.5
3 1 1 0.3 51.2
4 0.5 1 1 50.7
5 1 0.5 1 52.5
6 1 1 0.5 52.0
7 0.8 1 1 52.1
8 1 0.8 1 52.9
9 1 1 0.8 52.8
10 1 1 1 53.7
3) Hyperparameters: We conduct the ablation study of
hyperparameters in Eq. (7) on the FoggyCityscapes. As shown
in Table IX, we set the loss weights emb,pro, andstu
with different values and get the object detection results.
The ablation study results of loss weights also reveal the
effectiveness of our proposed prototype learning method in
object detection knowledge distillation.
V. C ONCLUSION
To solve the issue of applying a large-scale model to
different downstream tasks, we propose a Prototype-guided
Cross-task Knowledge Distillation method (ProC-KD), where
the label space of the teacher model and the student model
is inconsistent. Speciﬁcally, the prototype learning module is
trained to learn the invariant intrinsic local-level representation
with the help of powerful ability from the teacher model.Then, the learned prototypes are used to augment the student
model features to improve the generalization ability of the
student model. We conduct the experiments in both cross-
task knowledge distillation and standard same-task knowledge
distillation of image classiﬁcation and object detection. Both
quantitative and qualitative results verify the effectiveness of
our proposed method for knowledge distillation.
REFERENCES
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems , vol. 30, 2017.
[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,
“An image is worth 16x16 words: Transformers for image recognition
at scale,” arXiv preprint arXiv:2010.11929 , 2020.
[3] H. Touvron, M. Cord, D. Matthijs, F. Massa, A. Sablayrolles, and
H. Jegou, “Training data-efﬁcient image transformers & distillation
through attention,” in ICML 2021: 38th International Conference on
Machine Learning , 2021, pp. 10 347–10 357.
[4] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and
B. Guo, “Swin transformer: Hierarchical vision transformer using shifted
windows,” in Proceedings of the IEEE/CVF International Conference on
Computer Vision , 2021, pp. 10 012–10 022.
[5] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:
Deformable transformers for end-to-end object detection,” in ICLR 2021:
The Ninth International Conference on Learning Representations , 2021.
[6] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and
S. Zagoruyko, “End-to-end object detection with transformers,” in
European Conference on Computer Vision , 2020, pp. 213–229.
[7] L. Ye, M. Rochan, Z. Liu, and Y . Wang, “Cross-modal self-attention
network for referring image segmentation,” in 2019 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR) , 2019, pp.
10 502–10 511.
[8] S. Alfasly, C. K. Chui, Q. Jiang, J. Lu, and C. Xu, “An effective video
transformer with synchronized spatiotemporal and spatial self-attention
for action recognition,” IEEE Transactions on Neural Networks and
Learning Systems , pp. 1–14, 2022.
[9] H. Tan and M. Bansal, “Lxmert: Learning cross-modality encoder
representations from transformers,” in Proceedings of the 2019 Con-
ference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , 2019, pp. 5099–5110.
[10] W. Su, X. Zhu, Y . Cao, B. Li, L. Lu, F. Wei, and J. Dai, “Vl-bert:
Pre-training of generic visual-linguistic representations,” in ICLR 2020
: Eighth International Conference on Learning Representations , 2020.
[11] L. Li, Y .-C. Chen, Y . Cheng, Z. Gan, L. Yu, and J. Liu, “Hero: Hierar-
chical encoder for video+language omni-representation pre-training,” in
Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , 2020, pp. 2046–2065.
[12] Y .-C. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y . Cheng,
and J. Liu, “Uniter: Universal image-text representation learning,” in
European Conference on Computer Vision , 2020, pp. 104–120.
[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
A large-scale hierarchical image database,” in 2009 IEEE Conference on
Computer Vision and Pattern Recognition , 2009, pp. 248–255.
[14] M. H. Zhu and S. Gupta, “To prune, or not to prune: exploring the
efﬁcacy of pruning for model compression,” in ICLR (Workshop) , 2017.
[15] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, “Pruning
convolutional neural networks for resource efﬁcient inference,” in ICLR
(Poster) , 2016.

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 13
[16] J. Wu, C. Leng, Y . Wang, Q. Hu, and J. Cheng, “Quantized convolutional
neural networks for mobile devices,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition , 2016, pp.
4820–4828.
[17] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” arXiv preprint arXiv:1503.02531 , 2015.
[18] S. Li, M. Lin, Y . Wang, Y . Wu, Y . Tian, L. Shao, and R. Ji, “Distilling
a powerful student model via online knowledge distillation,” IEEE
Transactions on Neural Networks and Learning Systems , pp. 1–10, 2022.
[19] Q. Zhao, J. Dong, H. Yu, and S. Chen, “Distilling ordinal relation and
dark knowledge for facial age estimation,” IEEE Transactions on Neural
Networks and Learning Systems , vol. 32, no. 7, pp. 3108–3121, 2021.
[20] M. Zhu, J. Li, N. Wang, and X. Gao, “Knowledge distillation for face
photo–sketch synthesis,” IEEE Transactions on Neural Networks and
Learning Systems , vol. 33, no. 2, pp. 893–906, 2022.
[21] C. Yang, Z. An, L. Cai, and Y . Xu, “Knowledge distillation using
hierarchical self-supervision augmented distribution,” IEEE Transactions
on Neural Networks and Learning Systems , pp. 1–15, 2022.
[22] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y . Ben-
gio, “Fitnets: Hints for thin deep nets,” in ICLR 2015 : International
Conference on Learning Representations 2015 , 2015.
[23] Z. Huang and N. Wang, “Like what you like: Knowledge distill
via neuron selectivity transfer.” arXiv: Computer Vision and Pattern
Recognition , 2017.
[24] R. R. M ¨uller, S. Kornblith, and G. Hinton, “When does label smoothing
help,” in Advances in Neural Information Processing Systems , vol. 32,
2019, pp. 4694–4703.
[25] J. Yim, D. Joo, J. Bae, and J. Kim, “A gift from knowledge distillation:
Fast optimization, network minimization and transfer learning,” in 2017
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,
2017, pp. 7130–7138.
[26] W. Park, D. Kim, Y . Lu, and M. Cho, “Relational knowledge distilla-
tion,” in 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) , 2019, pp. 3967–3976.
[27] J. Gou, B. Yu, S. J. Maybank, and D. Tao, “Knowledge distillation: A
survey,” International Journal of Computer Vision , vol. 129, no. 6, pp.
1789–1819, 2021.
[28] Y . Chebotar and A. Waters, “Distilling knowledge from ensembles of
neural networks for speech recognition.” in Interspeech , 2016, pp. 3439–
3443.
[29] G. Kurata and G. Saon, “Knowledge distillation from ofﬂine to streaming
rnn transducer for end-to-end speech recognition.” in Interspeech , 2020,
pp. 2117–2121.
[30] J. W. Yoon, H. Lee, H. Y . Kim, W. I. Cho, and N. S. Kim, “Tutornet: To-
wards ﬂexible knowledge distillation for end-to-end speech recognition,”
IEEE/ACM Transactions on Audio, Speech, and Language Processing ,
vol. 29, pp. 1626–1638, 2021.
[31] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled
version of bert: smaller, faster, cheaper and lighter,” arXiv preprint
arXiv:1910.01108 , 2019.
[32] S. Sun, Y . Cheng, Z. Gan, and J. Liu, “Patient knowledge distillation
for bert model compression,” in Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Processing (EMNLP-
IJCNLP) , 2019, pp. 4322–4331.
[33] X. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and
Q. Liu, “Tinybert: Distilling bert for natural language understanding,”
inFindings of the Association for Computational Linguistics: EMNLP
2020 , 2020, pp. 4163–4174.
[34] H.-J. Ye, S. Lu, and D.-C. Zhan, “Distilling cross-task knowledge via
relationship matching,” in 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) , 2020, pp. 12 396–12 405.
[35] J. Devlin, M.-W. Chang, K. Lee, and K. N. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language understanding,”
inProceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , 2018, pp. 4171–4186.
[36] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, and P. Dollar,
“Designing network design spaces,” in 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) , 2020, pp. 10 428–
10 436.
[37] J. Ba and R. Caruana, “Do deep nets really need to be deep,” in Advances
in Neural Information Processing Systems 27 , vol. 27, 2014, pp. 2654–
2662.
[38] S. Zagoruyko and N. Komodakis, “Paying more attention to attention:
improving the performance of convolutional neural networks via atten-
tion transfer,” in ICLR (Poster) , 2016.[39] N. Passalis and A. Tefas, “Learning deep representations with probabilis-
tic knowledge transfer,” in Proceedings of the European Conference on
Computer Vision (ECCV) , 2018, pp. 283–299.
[40] D. Chen, J.-P. Mei, Y . Zhang, C. Wang, Z. Wang, Y . Feng, and C. Chen,
“Cross-layer distillation with semantic calibration,” in Proceedings of
the AAAI Conference on Artiﬁcial Intelligence , vol. 35, no. 8, 2021, pp.
7028–7036.
[41] J. Snell, K. Swersky, and R. S. Zemel, “Prototypical networks for few-
shot learning,” in Advances in Neural Information Processing Systems ,
vol. 30, 2017, pp. 4077–4087.
[42] J. Liu, L. Song, and Y . Qin, “Prototype rectiﬁcation for few-shot
learning,” in European Conference on Computer Vision , 2019, pp. 741–
756.
[43] H. Chefer, S. Gur, and L. Wolf, “Transformer interpretability beyond
attention visualization,” in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2021, pp. 782–791.
[44] S.-A. Rebufﬁ, H. Bilen, and A. Vedaldi, “Learning multiple visual
domains with residual adapters,” arXiv preprint arXiv:1705.08045 , 2017.
[45] K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “Learning
imbalanced datasets with label-distribution-aware margin loss,” arXiv
preprint arXiv:1906.07413 , 2019.
[46] Y . Cui, M. Jia, T.-Y . Lin, Y . Song, and S. Belongie, “Class-balanced loss
based on effective number of samples,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , 2019, pp. 9268–
9277.
[47] G. Van Horn, O. Mac Aodha, Y . Song, Y . Cui, C. Sun, A. Shepard,
H. Adam, P. Perona, and S. Belongie, “The inaturalist species classiﬁ-
cation and detection dataset,” in Proceedings of the IEEE conference on
computer vision and pattern recognition , 2018, pp. 8769–8778.
[48] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan,
“Deep hashing network for unsupervised domain adaptation,” in Pro-
ceedings of the IEEE conference on computer vision and pattern
recognition , 2017, pp. 5018–5027.
[49] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll ´ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision . Springer, 2014,
pp. 740–755.
[50] C. Shu, Y . Liu, J. Gao, Z. Yan, and C. Shen, “Channel-wise knowledge
distillation for dense prediction,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision , 2021, pp. 5311–5320.
[51] L. Zhang and K. Ma, “Improve object detection with feature-based
knowledge distillation: Towards accurate and efﬁcient detectors,” in
International Conference on Learning Representations , 2020.
[52] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-
nenson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset
for semantic urban scene understanding,” in Proceedings of the IEEE
conference on computer vision and pattern recognition , 2016, pp. 3213–
3223.
[53] C. Sakaridis, D. Dai, and L. Van Gool, “Semantic foggy scene under-
standing with synthetic data,” International Journal of Computer Vision ,
vol. 126, no. 9, pp. 973–992, 2018.
[54] L. Van Der Maaten and K. Weinberger, “Stochastic triplet embedding,”
in2012 IEEE International Workshop on Machine Learning for Signal
Processing . IEEE, 2012, pp. 1–6.
[55] S. Ahn, S. X. Hu, A. Damianou, N. D. Lawrence, and Z. Dai, “Varia-
tional information distillation for knowledge transfer,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2019, pp. 9163–9171.
[56] W. Choi, M. Chandraker, G. Chen, and X. Yu, “Learning efﬁcient object
detection models with knowledge distillation,” Sep. 20 2018, uS Patent
App. 15/908,870.
[57] T. Wang, L. Yuan, X. Zhang, and J. Feng, “Distilling object detectors
with ﬁne-grained feature imitation,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2019, pp.
4933–4942.
[58] B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, and J. Y . Choi, “A
comprehensive overhaul of feature distillation,” in Proceedings of the
IEEE/CVF International Conference on Computer Vision , 2019, pp.
1921–1930.
