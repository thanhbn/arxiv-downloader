# Co-training and Co-distillation for
# Quality Improvement and Compression of Language Models
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-distillation/2311.02849.pdf
# File size: 1898866 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Co-training và Co-distillation để
Cải thiện Chất lượng và Nén các Mô hình Ngôn ngữ
Hayeon Lee1∗Rui Hou1Jongpil Kim1
Davis Liang2Hongbo Zhang1Sung Ju Hwang3Alexander Min1
Meta AI1Abridge AI2KAIST3
hayeonlee@meta.com rayhou@meta.com jongpil@meta.com
davis@abridge.com hbzhang@meta.com sjhwang82@kaist.ac.kr alexmin@meta.com
Tóm tắt
Knowledge Distillation (KD) nén các mô hình ngôn ngữ được huấn luyện trước (PLMs) tốn kém về mặt tính toán bằng cách chuyển giao kiến thức của chúng sang các mô hình nhỏ hơn, cho phép sử dụng chúng trong các cài đặt hạn chế tài nguyên hoặc thời gian thực. Tuy nhiên, hầu hết các mô hình nhỏ hơn đều không thể vượt qua hiệu suất của mô hình lớn gốc, dẫn đến việc hy sinh hiệu suất để cải thiện tốc độ suy luận. Để giải quyết vấn đề này, chúng tôi đề xuất Co-Training và Co-Distillation (CTCD), một khung công tác mới cải thiện hiệu suất và tốc độ suy luận cùng nhau bằng cách đồng huấn luyện hai mô hình trong khi chưng cất kiến thức lẫn nhau. Khung công tác CTCD thành công đạt được điều này dựa trên hai phát hiện quan trọng: 1) Chưng cất kiến thức từ mô hình nhỏ hơn sang mô hình lớn hơn trong quá trình đồng huấn luyện cải thiện hiệu suất của mô hình lớn hơn. 2) Hiệu suất được nâng cao của mô hình lớn hơn càng thúc đẩy hiệu suất của mô hình nhỏ hơn. Khung công tác CTCD cho thấy triển vọng vì nó có thể được kết hợp với các kỹ thuật hiện có như thiết kế kiến trúc hoặc tăng cường dữ liệu, thay thế các phương pháp KD một chiều, để đạt được cải thiện hiệu suất hơn nữa. Các nghiên cứu ablation mở rộng chứng minh tính hiệu quả của CTCD, và mô hình nhỏ được chưng cất bởi CTCD vượt trội hơn mô hình lớn gốc với biên độ đáng kể là 1.66 trên benchmark GLUE.

1 Giới thiệu
Trong những năm gần đây, chi phí tính toán cao của các mô hình ngôn ngữ được huấn luyện trước (PLMs) (Radford et al., 2019; Yang et al., 2019; Dai et al., 2019; Shoeybi et al., 2019; Li et al., 2020; Brown et al., 2020) trở thành một ràng buộc trong việc phục vụ các ứng dụng hạn chế tài nguyên hoặc thời gian thực. Knowledge Distillation (KD) (Hinton et al., 2015; Romero et al., 2015) là một kỹ thuật nén mô hình phổ biến để giải quyết vấn đề này, với một mô hình nhỏ hơn (học sinh) học (chưng cất) từ một mô hình lớn hơn (giáo viên).

∗Công việc được thực hiện trong thời gian thực tập tại Meta AI.

Tình huống lý tưởng sẽ bao gồm việc nén giáo viên để phù hợp với kích thước của học sinh nhỏ mà không có bất kỳ sụt giảm hiệu suất nào. Tuy nhiên, bất chấp nghiên cứu sâu rộng trong lĩnh vực KD (Turc et al., 2019; Tsai et al., 2019; Tang et al., 2019; Jiao et al., 2020; Sanh et al., 2019a; Sun et al., 2019; Wang et al., 2020b,a), không có phương pháp hiện tại nào đã thành công tránh được sự suy giảm hiệu suất trong quá trình chưng cất. Một số cách tiếp cận đã cố gắng giảm thiểu khoảng cách hiệu suất bằng cách kết hợp các yếu tố bên ngoài. Ví dụ, Jiao et al. (2020) kết hợp tăng cường dữ liệu, trong khi Wang et al. (2020b,a) tập trung vào thiết kế kiến trúc học sinh. Tuy nhiên, các cách tiếp cận này có hạn chế vì chúng chỉ cung cấp các kỹ thuật bổ sung thay vì giải quyết vấn đề cơ bản của mất hiệu suất trong KD. Điều này đặt ra một câu hỏi thú vị: Liệu chúng ta có thể nén một mô hình mà không hy sinh hiệu suất thông qua KD?

Trong công trình này, chúng tôi đề xuất một khung công tác mới gọi là Co-Training và Co-Distillation (CTCD) để cải thiện hiệu suất của mô hình ngôn ngữ trong khi nén nó thông qua KD. CTCD bao gồm việc huấn luyện chung mô hình giáo viên và học sinh, cho phép chúng chuyển giao kiến thức cho nhau theo hai chiều, từ giáo viên sang học sinh và ngược lại. Công trình của chúng tôi khám phá hai phát hiện chính trong khung công tác CTCD: Thứ nhất, chúng tôi chứng minh rằng việc chuyển giao kiến thức từ mô hình nhỏ hơn sang mô hình lớn hơn trong quá trình đồng huấn luyện cải thiện đáng kể hiệu suất của mô hình lớn hơn. Nói cách khác, bằng cách sử dụng knowledge distillation (KD), chúng tôi nâng cao hiệu suất của mô hình giáo viên so với huấn luyện độc lập. Điều này khác biệt đáng kể so với KD một chiều thông thường, nơi mô hình giáo viên không thể hưởng lợi từ quá trình chưng cất vì nó không còn được huấn luyện hoặc chưng cất. Thứ hai, hiệu suất được cải thiện của mô hình lớn hơn dẫn đến các cải thiện hơn nữa trong hiệu suất của mô hình nhỏ hơn thông qua KD từ mô hình giáo viên sang mô hình học sinharXiv:2311.02849v2  [cs.CL]  7 Nov 2023

--- TRANG 2 ---
Hình 1: Phương pháp KD bất khả tri nhiệm vụ Khung công tác được đề xuất là các phương pháp KD bất khả tri nhiệm vụ thực hiện KD trong giai đoạn tiền huấn luyện. Điều này cho phép mô hình được chưng cất được triển khai và tinh chỉnh trên các nhiệm vụ downstream khác nhau.

sinh. Những phát hiện mới này cho phép mô hình nhỏ hơn vượt qua mô hình lớn hơn được huấn luyện độc lập trong khi duy trì hiệu quả suy luận của nó trong khung công tác CTCD. Hơn nữa, khung công tác CTCD có thể được kết hợp trực giao với các kỹ thuật hiện có cho các yếu tố bên ngoài như thiết kế kiến trúc học sinh Wang et al. (2020b,a) hoặc tăng cường dữ liệu (Jiao et al., 2020). Chúng tôi đạt được điều này bằng cách thay thế các phương pháp KD một chiều truyền thống bằng khung công tác CTCD được đề xuất, có triển vọng cải thiện hiệu suất đáng kể trong các nhiệm vụ mô hình ngôn ngữ.

Ngoài khung công tác CTCD của chúng tôi, chúng tôi giới thiệu Community KD sử dụng khung công tác CTCD với mô hình giáo viên được tiền huấn luyện bằng cách kết hợp CTCD với KD thông thường. Community KD chưng cất kiến thức từ hai học sinh cho nhau, cũng như từ giáo viên được tiền huấn luyện cho mỗi học sinh, trong quá trình đồng huấn luyện của các học sinh. Chúng tôi chỉ ra rằng chưng cất từ học sinh khác cũng như giáo viên được tiền huấn luyện tốt hơn so với chỉ chưng cất một chiều từ giáo viên được tiền huấn luyện, như được thực hiện trong KD thông thường. Lưu ý rằng chi phí suy luận trên một nhiệm vụ giống như trong KD thông thường, vì chúng tôi lấy một trong các học sinh được chưng cất bởi Community KD khi triển khai nó cho nhiệm vụ.

Chúng tôi xác thực tính hiệu quả và hiệu quả của CTCD của chúng tôi trên benchmark GLUE (Wang et al., 2019), bao gồm các nhiệm vụ hiểu ngôn ngữ khác nhau. Như được hiển thị trong Hình 1, chúng tôi tập trung vào các tình huống KD bất khả tri nhiệm vụ, nơi KD xảy ra trong giai đoạn tiền huấn luyện. Sau đó, chúng tôi tinh chỉnh các PLM được chưng cất trên mỗi nhiệm vụ downstream và đánh giá hiệu suất của chúng. Điều này thách thức hơn so với KD dành riêng cho nhiệm vụ, vì nó đòi hỏi tài nguyên tính toán đáng kể để huấn luyện PLM trên một corpus văn bản lớn, và kiến thức được chưng cất nên có thể chuyển giao qua các nhiệm vụ downstream đa dạng. Nghiên cứu ablation mở rộng của chúng tôi tiết lộ rằng mô hình lớn hơn được hưởng lợi từ việc chưng cất của mô hình nhỏ hơn, và việc cải thiện hiệu suất của mô hình lớn hơn càng nâng cao hiệu suất của mô hình nhỏ hơn. Trong các thí nghiệm của chúng tôi, mô hình học sinh được nén bởi khung công tác CTCD đạt được 1.66 lợi ích cao hơn so với mô hình lớn gốc được huấn luyện bằng phương pháp độc lập, chứng minh rằng cách tiếp cận của chúng tôi có thể cải thiện chất lượng mô hình và hiệu quả suy luận đồng thời.

Tóm lại, các đóng góp của chúng tôi như sau:
• Chúng tôi đề xuất một khung công tác knowledge distillation mới gọi là Co-Training và Co-Distillation (CTCD) để cải thiện hiệu suất của các mô hình trong khi nén chúng thông qua KD.
• Thông qua các thí nghiệm của chúng tôi, chúng tôi chứng minh rằng việc chưng cất kiến thức từ mô hình nhỏ hơn sang mô hình lớn hơn trong quá trình đồng huấn luyện cải thiện hiệu suất của mô hình lớn hơn.
• Ngoài ra, chúng tôi nhấn mạnh rằng hiệu suất được nâng cao của mô hình lớn hơn càng thúc đẩy hiệu suất của mô hình nhỏ hơn, dẫn đến hiệu suất được cải thiện cho cả hai mô hình.
• Chúng tôi cung cấp những hiểu biết có giá trị về việc điều chỉnh trọng số mất mát và độ dài của giai đoạn huấn luyện để áp dụng hiệu quả khung công tác CTCD thông qua nghiên cứu ablation mở rộng.

2 Công trình Liên quan
Knowledge Distillation Một chiều Knowledge distillation (KD) (Hinton et al., 2015) là một kỹ thuật nén mô hình trong đó một mô hình học sinh nhỏ hơn chưng cất kiến thức từ một mô hình giáo viên lớn hơn, được tiền huấn luyện. Gần đây, nhiều nhà nghiên cứu đã khám phá KD để giải quyết độ phức tạp tính toán cao của mô hình ngôn ngữ do kích thước tăng lên của nó. Điều này bao gồm các phương pháp KD bất khả tri nhiệm vụ cho giai đoạn tiền huấn luyện (Sanh et al., 2019a; Wang et al., 2020b,a), phương pháp KD dành riêng cho nhiệm vụ cho

--- TRANG 3 ---
PredictionCo-distillation
Large 
Text CorpusTeacher
…
Scratch
Student
…
ScratchPredictionCo-trainingteacher soft lossstudent soft loss
Teacher
Soft Label
Student 
Soft LabelBack propagation
Loss = teacher hard loss + teacher soft loss
Co-training
Hard Label
Back propagation
Loss = student hard loss + student soft lossFigure 2: Co-Training và Co-Distillation (CTCD) Trong quá trình huấn luyện, các mô hình giáo viên và học sinh có thể học hiệu quả hơn bằng cách so sánh đầu ra dự đoán của chúng không chỉ với ground truth mà còn với dự đoán của nhau. Chúng tôi gọi cái trước là "hard" loss và cái sau là "soft" loss. Ví dụ, student soft loss nắm bắt khoảng cách giữa dự đoán của học sinh và dự đoán của giáo viên, và ngược lại. Cách tiếp cận co-training và co-distillation này cải thiện hiệu suất của mô hình giáo viên, từ đó có lợi cho hiệu suất của mô hình học sinh.

giai đoạn tinh chỉnh (Sun et al., 2019), và cả giai đoạn tiền huấn luyện và tinh chỉnh (Jiao et al., 2020). Ngoài ra, Wang et al. (2020b,a) đã thiết kế lại kiến trúc của các mô hình ngôn ngữ học sinh. Tuy nhiên, KD một chiều có thể dẫn đến mất kiến thức, dẫn đến các mô hình nhỏ hơn thường gặp khó khăn trong việc đạt được hiệu suất của các mô hình lớn hơn, dẫn đến suy giảm hiệu suất. Ngược lại, CTCD của chúng tôi có thể cải thiện chất lượng của cả mô hình giáo viên và học sinh, làm cho học sinh có thể đạt được chất lượng tương tự như mô hình giáo viên gốc.

Reversed Knowledge Distillation Gần đây, các nhà nghiên cứu (Yuan et al., 2020; Qin et al., 2022; Lee et al., 2023) đã chứng minh rằng reversed Knowledge Distillation (reversed KD), chuyển giao kiến thức từ một mô hình nhỏ hơn hoặc kém hơn sang một mô hình lớn hơn, có thể cải thiện hiệu suất của mô hình học sinh. Đặc biệt, Qin et al. (2022); Lee et al. (2023) đã điều tra việc áp dụng reversed KD trong PLMs, cho thấy rằng một mô hình lớn hơn có thể được hưởng lợi từ một mô hình kém hơn và được tiền huấn luyện cho một nhiệm vụ downstream cụ thể. Được truyền cảm hứng từ thành công của reversed KD, chúng tôi thiết kế một khung công tác co-distillation bao gồm reversed KD để cải thiện hiệu suất của mô hình giáo viên bằng cách chưng cất kiến thức từ mô hình học sinh nhỏ hơn. Không giống như các phương pháp reversed KD hiện có, bị hạn chế trong việc cải thiện hiệu suất của mô hình lớn hơn, khung công tac co-distillation được đề xuất của chúng tôi có thể đạt được cả cải thiện hiệu suất và nén mô hình, bằng cách cho thấy một giáo viên chất lượng tốt hơn dẫn đến một học sinh chất lượng tốt hơn.

3 Co-training và Co-distillation
Trước tiên chúng tôi giới thiệu các khái niệm về co-training và co-distillation một cách ngắn gọn:

Co-training huấn luyện hai mô hình (có kích thước khác nhau) (ví dụ, một giáo viên và học sinh) đồng thời với mục tiêu đạt được chất lượng mô hình tương tự.

Co-distillation chuyển giao kiến thức theo cả hai hướng giữa hai mô hình (ví dụ, một giáo viên và học sinh), trong quá trình co-training.

Hình 2 minh họa cách các mô hình được đồng huấn luyện học cùng nhau bằng cách so sánh đầu ra dự đoán của chúng với dự đoán từ nhau và với các nhãn hard (hoặc "ground truth"). Chúng tôi gọi cái trước là "soft" loss và cái sau là "hard" loss. Ví dụ, soft loss của mô hình học sinh đo độ chính xác của dự đoán của học sinh bằng cách xem xét dự đoán của giáo viên như một soft label, và ngược lại.

Công thức hóa Nhiệm vụ Giả sử rằng chúng ta được cho một nhiệm vụ phân loại với K lớp. Cho mỗi instance huấn luyện x và nhãn ground truth y của nó, chúng ta ký hiệu rằng phân phối ground truth trên các nhãn là q(k|x) (q(k) cho đơn giản) trong đó cho mỗi nhãn k ∈ {1...K}, q(y) = 1 và q(k) = 0 cho tất cả k ≠ y.

Cho mỗi x, mô hình giáo viên tϕ được tham số hóa bởi ϕ và mô hình học sinh sθ được tham số hóa bởi θ dự đoán xác suất của mỗi nhãn k như pτϕ(k|x) và pτθ(k|x), tương ứng như sau:

pτϕ(k|x) = f(zt) = exp(ztk/τ) / ΣKi=1 exp(zti/τ)

pτθ(k|x) = f(zs) = exp(zsk/τ) / ΣKi=1 exp(zsi/τ)

trong đó f là hàm softmax, zt = {zti}Ki=1 = tϕ(x) là output logit của mô hình giáo viên, zs = {zsi}Ki=1 = sθ(x) là output logit của mô hình học sinh, và τ là temperature để làm mềm pϕ(k) và pθ(k).

--- TRANG 4 ---
Prediction
Large 
Text Corpus
Hard LabelTeacher
…
Pre-trained
Student 1
…
Scratch
…
ScratchStudent 2soft losssoft loss
Teacher
Soft Label
Student 2 
Soft LabelCo-distillation
Student 1 
Soft Labelsoft losshard loss hard loss
soft loss
Co-trainingBack propagation
Loss = hard loss + soft loss from student 2 + soft loss from teacher Prediction
Co-trainingBack propagation
Loss = hard loss + soft loss from student 2 + soft loss from teacher PredictionHình 3: Community KD Khác với KD thông thường, nơi mỗi học sinh học từ dự đoán của giáo viên được tiền huấn luyện, cách tiếp cận được đề xuất học mỗi học sinh từ cả dự đoán của giáo viên được tiền huấn luyện và dự đoán của học sinh khác trong quá trình co-training. Lưu ý rằng vì chúng tôi lấy một trong các học sinh được tiền huấn luyện để thích ứng nó với các nhiệm vụ downstream, chi phí suy luận giống như huấn luyện học sinh với KD thông thường.

Mục tiêu LCTCD(θ,ϕ) được đề xuất bao gồm một mục tiêu KD thông thường LKD:t→s để chưng cất kiến thức từ mô hình giáo viên sang mô hình học sinh và một mục tiêu reversed KD LReKD:s→t để chưng cất kiến thức từ mô hình học sinh sang mô hình giáo viên, trong quá trình co-training của chúng.

Giáo viên → Học sinh Mục tiêu KD thông thường LKD:t→s(θ,ϕ) nhằm huấn luyện mô hình học sinh bằng cách tối thiểu hóa tổng có trọng số của cross-entropy loss H(q, pθ) giữa ground truth q và dự đoán học sinh pθ và Kullback-Leibler divergence (KL divergence) D(pτϕ, pτθ) giữa các dự đoán của học sinh và giáo viên như sau:

LKD(θ,ϕ) = αh·H(q, pθ) + αs·D(pτϕ, pτθ)    (1)

trong đó

H(q, pθ) = -ΣKk=1 q(k) log(pθ(k)),

D(pτϕ, pτθ) = ΣKk=1 pτϕ(k)·log(pτϕ(k)/pτθ(k)),

αh và αs là các giá trị siêu tham số trọng số cho cross-entropy loss và KL divergence, tương ứng. Chúng tôi coi cross-entropy loss H(q, pθ) là hard loss cho mô hình học sinh, KL divergence D(pτϕ, pτθ) là soft loss cho mô hình học sinh, và theo BERT (Devlin et al., 2019), H(q, pθ) biểu thị Masked Language Modeling loss (MLM loss). Trong mục tiêu KD, chúng tôi xem các tham số giáo viên ϕ là hằng số vì chúng tôi chỉ huấn luyện các tham số học sinh θ trong khi mô hình giáo viên tϕ được cố định:

LKD:t→s(θ, StopG(ϕ))    (2)

trong đó StopG(x) biểu thị rằng chúng tôi không tính toán gradient của x. Trong phương pháp KD thông thường, Phương trình (2) là mục tiêu cuối cùng để học mô hình học sinh chỉ với mô hình giáo viên được tiền huấn luyện.

Học sinh → Giáo viên Khác với phương pháp KD một chiều như vậy, chúng tôi giới thiệu mục tiêu reversed KD LReKD:s→t(StopG(θ),ϕ) để huấn luyện mô hình giáo viên tϕ như sau:

LReKD:s→t(StopG(θ),ϕ) = βh·H(q, pϕ) + βs·D(pτθ, pτϕ)    (3)

trong đó βh và βs là các giá trị siêu tham số trọng số của hard loss H(q, pϕ) và soft loss D(pτθ, pτϕ) cho mô hình giáo viên, tương ứng. Bằng cách tối thiểu hóa KL divergence D(pτθ, pτϕ) giữa các dự đoán của mô hình học sinh (pτθ) và mô hình giáo viên (pτϕ), mô hình giáo viên học từ mô hình học sinh. Trong mục tiêu reversed KD, chúng tôi chỉ huấn luyện mô hình giáo viên bằng cách áp dụng StopG(x) cho gradient của các tham số học sinh θ.

Co-training Với các Phương trình (2) và (3), chúng ta có mục tiêu cuối cùng LCTCD(θ,ϕ) như sau:

θ*, ϕ* = argmin(θ,ϕ) LCTCD(θ,ϕ) = LKD(θ, StopG(ϕ)) + LReKD(StopG(θ), ϕ)    (4)

Thích ứng với Nhiệm vụ Downstream Sau co-training/-distillation mô hình, mô hình nhỏ hơn (học sinh) được huấn luyện sθ* với tham số được huấn luyện θ* có thể được triển khai cho nhiều nhiệm vụ downstream để cải thiện hiệu quả suy luận. Để tinh chỉnh mô hình cho một nhiệm vụ downstream cụ thể, chúng tôi thích ứng tham số được huấn luyện θ* bằng cách sử dụng dataset cho nhiệm vụ đó.

--- TRANG 5 ---
4 Community KD
Hơn nữa, chúng tôi giới thiệu một ứng dụng CTCD nâng cao có tên Community KD có thể sử dụng khung công tác CTCD với mô hình giáo viên được tiền huấn luyện, như được hiển thị trong Hình 3. Nó bao gồm một giáo viên được tiền huấn luyện tϕ* với các tham số được tiền huấn luyện ϕ* và hai học sinh sθ1 và sθ2 được tham số hóa bởi θ1 và θ2, tương ứng. Trong quá trình co-training của hai học sinh, mỗi học sinh học từ các nhãn hard, các nhãn soft được tạo ra từ dự đoán giáo viên được tiền huấn luyện, và các nhãn soft được tạo ra từ dự đoán học sinh khác. Nói cách khác, chúng tôi tiến hành knowledge distillation một chiều từ giáo viên được tiền huấn luyện đến mỗi học sinh bằng cách tối thiểu hóa KL divergence giữa dự đoán giáo viên và dự đoán của mỗi học sinh D(pτθ1, pτϕ*) và D(pτθ2, pτϕ*) và co-distillation giữa các học sinh theo cả hai hướng bằng cách tối thiểu hóa LCTCD(θ1,θ2). Mục tiêu cuối cùng LCM(θ1,θ2,StopG(ϕ*)) như sau:

θ*1, θ*2 = argmin(θ1,θ2) LCM(θ1,θ2,StopG(ϕ*)) = LCTCD(θ1,θ2) + D(pτθ1, pτϕ*) + D(pτθ2, pτϕ*)    (5)

Chúng tôi chọn một trong hai học sinh được chưng cất bởi Community KD (θ* = θ*1 hoặc θ* = θ*2) và tinh chỉnh học sinh đơn được chọn sθ* cho các nhiệm vụ downstream, dẫn đến chi phí suy luận không tăng so với phương pháp KD thông thường.

5 Thí nghiệm
Chúng tôi trình bày một phân tích toàn diện về phương pháp CTCD được đề xuất thông qua các thí nghiệm thực nghiệm. Trong Mục 5.1, chúng tôi xác thực phương pháp CTCD của chúng tôi bằng cách so sánh hiệu suất của các mô hình nhỏ được chưng cất bởi CTCD với mô hình lớn gốc trên benchmark GLUE (Wang et al., 2019). Trong Mục 5.2, chúng tôi phân tích tác động của co-distillation bằng cách điều chỉnh trọng số loss cho các soft loss của học sinh và giáo viên. Trong Mục 5.3, chúng tôi nghiên cứu tác động của độ dài huấn luyện đối với phương pháp CTCD, cho phép chúng tôi xác định độ dài huấn luyện tối ưu cho phương pháp CTCD. Trong Mục 5.4, chúng tôi chứng minh hiệu quả của Community KD bằng cách so sánh hiệu suất của mô hình được chưng cất bởi Community KD với mô hình được chưng cất bởi KD một chiều.

Chi tiết Triển khai Chúng tôi sử dụng learning rate 5e-4, linear warm-up 5%, optimizer AdamW (Loshchilov và Hutter, 2019), và batch size 128 với GPU A100 cho tiền huấn luyện. Chúng tôi huấn luyện các mô hình giáo viên và học sinh từ đầu trong 20 epochs trong Mục 5.1. Để phân tích tính hiệu quả của phương pháp CTCD, chúng tôi huấn luyện các mô hình giáo viên và học sinh trong 10 epochs và 20 epochs trong Mục 5.2 và Mục 5.3, tương ứng. Trong Mục 5.4, chúng tôi huấn luyện các mô hình trong 3 epochs sau parameter remapping, giống như trong phương pháp KD một chiều thông thường (Sanh et al., 2019a).

Thời gian Huấn luyện Đối với distillation một chiều, chúng tôi cần 1 GPU day để huấn luyện giáo viên trong 10 epochs và 1.3 GPU days để chưng cất kiến thức từ giáo viên được tiền huấn luyện sang học sinh trong 10 epochs khác, tiêu tốn tổng cộng 2.3 days. Đối với CTCD, phải mất 3 GPU days để huấn luyện cả mô hình giáo viên và học sinh từ đầu trong 10 epochs. Chúng tôi sử dụng automatic mixed precision (AMP) của PyTorch (Paszke et al., 2019) để tăng tốc huấn luyện cho tất cả các mô hình của chúng tôi.

Dataset Để xác thực phương pháp CTCD của chúng tôi, chúng tôi sử dụng một dataset giảm (30M) được tạo bằng cách lấy mẫu đồng đều 1 trong mỗi 4 câu từ dataset tiền huấn luyện gốc (BookCorpus (Zhu et al., 2015) + Wikipedia (Foundation)) được sử dụng trong phương pháp KD một chiều thông thường (Sanh et al., 2019a). Chúng tôi đánh giá các mô hình được chưng cất của chúng tôi trên các dev sets của benchmark GLUE (Wang et al., 2019), bao gồm chín nhiệm vụ phân loại cấp câu. Trong Mục 5.4, chúng tôi sử dụng dataset tiền huấn luyện gốc để huấn luyện phương pháp Community KD.

Kiến trúc Mô hình Chúng tôi sử dụng mô hình BERT 6-layer (Devlin et al., 2019) làm giáo viên và mô hình BERT 4-layer làm học sinh để phân tích tính hiệu quả và hiệu quả của phương pháp CTCD của chúng tôi. Trong Mục 5.4, chúng tôi sử dụng mô hình BERT-base được tiền huấn luyện làm giáo viên và mô hình BERT 6-layer làm học sinh.

--- TRANG 6 ---
Performance GAP w/ Teacher
Original Teacher 78.06 -
StudentOne-way Distil. (10 epoch) 77.46 -0.60
CTCD (10 epoch) 77.94 -0.12
One-way Distil. (20 epoch) 78.39 +0.33
CTCD (20 epoch) 79.12 +1.66

Bảng 1: Hiệu suất trung bình trên dev sets của benchmark GLUE Học sinh được chưng cất bởi CTCD vượt trội đáng kể so với giáo viên gốc được huấn luyện bằng phương pháp độc lập, đạt được lợi ích cao hơn là 1.66.

(a) Hiệu suất Giáo viên
(b) Hiệu suất Học sinh

Hình 5: Điều chỉnh Trọng số Loss Chúng tôi điều tra tác động của distillation đối với mô hình giáo viên và mô hình học sinh bằng cách điều chỉnh trọng số loss (αh, αs, βh, βs) cho hard loss và soft loss. (a) Chúng tôi (co-)huấn luyện mô hình giáo viên chưng cất kiến thức từ học sinh bằng cách cố định αh:αs = 1:1 và thay đổi βh:βs trên corpus văn bản lớn. (b) Chúng tôi (co-)huấn luyện mô hình học sinh chưng cất kiến thức từ giáo viên bằng cách cố định βh:βs = 1:1 và thay đổi αh:αs trên corpus văn bản lớn. Sau đó chúng tôi báo cáo hiệu suất trung bình của mỗi mô hình được tiền huấn luyện sau khi tinh chỉnh nó trên các nhiệm vụ downstream (dev sets) của benchmark GLUE.

Hình 6: Độ dài Huấn luyện Chúng tôi tiền huấn luyện các mô hình học sinh dưới hai độ dài huấn luyện khác nhau 10/20 epochs trong khi chưng cất kiến thức từ các mô hình giáo viên qua phương pháp của chúng tôi hoặc phương pháp KD thông thường. Sau đó chúng tôi thích ứng các mô hình học sinh được tiền huấn luyện trên nhiệm vụ CoLA. Với huấn luyện đủ dài (20 epoch), mô hình học sinh được chưng cất bởi phương pháp của chúng tôi vượt trội đáng kể so với mô hình học sinh được chưng cất bởi phương pháp KD thông thường, với lợi ích cao hơn là 5.22.

5.1 Liệu Knowledge Distillation Có Thể Giúp Cải thiện Hiệu suất?
Trong Bảng 1 và Hình 4, chúng tôi hiển thị hiệu suất trung bình của các mô hình được huấn luyện bằng các phương pháp khác nhau trên corpus văn bản lớn và được tinh chỉnh so với benchmark GLUE. Stand-alone huấn luyện một mô hình mà không sử dụng bất kỳ knowledge distillation nào. Co-training & One-way distillation huấn luyện các mô hình giáo viên và học sinh cùng nhau từ đầu, với kiến thức chỉ chảy từ giáo viên sang học sinh. Co-training & Co-distillation (Của chúng tôi) là phương pháp CTCD, huấn luyện cả mô hình giáo viên và học sinh cùng nhau từ đầu và chưng cất kiến thức giữa nhau theo cả hai hướng. Đối với các phương pháp distillation, chúng tôi đặt trọng số của hard losses cho giáo viên và học sinh là 1. Trọng số của soft losses được chọn từ tập {0.5,1,2,4}, và kết quả được báo cáo với trọng số có hiệu suất tốt nhất.

1) Kết quả Tổng thể Như được hiển thị trong Bảng 1, học sinh được chưng cất bởi phương pháp CTCD vượt trội đáng kể so với giáo viên gốc được huấn luyện bằng phương pháp stand-alone về hiệu suất trung bình của benchmark GLUE, đạt được lợi ích cao hơn là 1.66. Hơn nữa, mô hình học sinh được chưng cất bởi phương pháp CTCD của chúng tôi vượt trội hơn học sinh được chưng cất bởi phương pháp distillation một chiều về hiệu suất trung bình của benchmark GLUE. Sau khi huấn luyện trong 10 và 20 epochs, học sinh được chưng cất bởi phương pháp CTCD của chúng tôi liên tục có lợi ích cao hơn

--- TRANG 7 ---
so với học sinh được chưng cất bởi distillation một chiều, như 77.46 vs. 77.94 và 78.39 vs. 79.12, tương ứng.

2) Liệu việc học từ một học sinh nhỏ và yếu có mang lại lợi ích hiệu suất cho giáo viên không?
Như được hiển thị trong Hình 4, việc chưng cất kiến thức từ mô hình học sinh sang mô hình giáo viên đã được chứng minh là cải thiện đáng kể chất lượng của mô hình giáo viên, với mức tăng trung bình là 1.88 so với các phương pháp huấn luyện giáo viên không kết hợp quá trình chưng cất như vậy (như Stand-alone và Co-training & one-way distillation).

3) Liệu việc cải thiện hiệu suất của giáo viên có được phản ánh trong việc cải thiện hiệu suất của học sinh không?
Hơn nữa, chúng tôi thấy rằng một giáo viên tốt hơn dẫn đến cải thiện hiệu suất hơn nữa của học sinh từ 77.46 lên 77.94. Kết quả chứng minh rằng quá trình distillation thành công cải thiện hiệu suất của cả giáo viên và học sinh. Học sinh được huấn luyện với phương pháp CTCD của chúng tôi đạt được hiệu suất tốt hơn so với học sinh được huấn luyện với các phương pháp Stand-alone hoặc Co-training & one-way distillation, với cải thiện trung bình là 0.88 và 0.47, tương ứng.

5.2 Nghiên cứu Sâu 1: Điều chỉnh Trọng số Loss
Trong Mục này, chúng tôi điều tra tác động của distillation đối với hiệu suất của cả học sinh và giáo viên bằng cách điều chỉnh trọng số loss (αh, αs, βh, βs) cho hard loss và soft loss. Ví dụ, đặt αh:αs = 1:4 nhấn mạnh việc học từ kiến thức của giáo viên (tức là, soft loss cho học sinh) gấp 4 lần so với phân phối nhãn ground truth (tức là, hard loss cho học sinh), trong quá trình co-training. Điều này cho phép chúng tôi hiểu rõ hơn tác động của distillation đối với mỗi mô hình và tối ưu hóa hiệu suất của chúng.

Phía Giáo viên Trong Hình 5(a), chúng tôi co-train mô hình giáo viên bằng distillation để chuyển giao kiến thức từ mô hình học sinh. Chúng tôi cố định các giá trị trọng số cho các loss của học sinh thành αh:αs = 1:1 và thay đổi các giá trị trọng số cho các loss của giáo viên, βh:βs, trong khi huấn luyện trên corpus văn bản lớn. Sau đó chúng tôi đánh giá hiệu suất trung bình của các mô hình giáo viên được tiền huấn luyện trên các nhiệm vụ downstream từ dev sets của benchmark GLUE.

Kết quả của chúng tôi cho thấy co-training giáo viên với distillation vượt trội hơn huấn luyện giáo viên đơn lẻ, bất kể các giá trị trọng số cho soft loss, bằng cách đạt được lợi ích cao hơn là 0.36, 0.77, 1.80, và 1.02 cho βh:βs = 1:1, 1:2, 1:4, và 4:4, tương ứng. Ngoài ra, chúng tôi thấy rằng việc cho trọng số nhiều hơn cho soft loss của giáo viên trong quá trình huấn luyện (αh:αs:βh:βs = 1:1:1:1→1:1:1:2→1:1:1:4) dẫn đến cải thiện hiệu suất, với điểm trung bình là 78.42→78.83→79.86. Hơn nữa, chúng tôi quan sát thấy rằng việc nhấn mạnh chỉ soft loss của giáo viên (1:1:1:4) mang lại hiệu suất tốt hơn so với việc nhấn mạnh cả hard và soft losses của giáo viên (1:1:4:4), với điểm trung bình là 79.86 vs. 79.09.

Phía Học sinh Chúng tôi thấy rằng hiệu suất của mô hình học sinh không nhạy cảm với các giá trị trọng số cho hard và soft losses của học sinh, (αh:αs). Bất kể các giá trị được chọn, co-training học sinh với distillation liên tục cải thiện hiệu suất của nó so với huấn luyện học sinh đơn lẻ. Ví dụ, khi chúng tôi nhấn mạnh soft loss của học sinh bằng cách tăng giá trị trọng số cho (αs) như 1:1:1:1→1:2:1:1→1:4:1:1, chúng tôi quan sát thấy mức độ hiệu suất tương tự cho mô hình học sinh.

5.3 Nghiên cứu Sâu 2: Độ dài Huấn luyện
Chúng tôi nghiên cứu tác động của độ dài co-training đối với hiệu quả của phương pháp CTCD (xem Hình 6). Chúng tôi thấy rằng huấn luyện lâu hơn dẫn đến cải thiện hiệu suất, như được chứng minh bởi các thí nghiệm của chúng tôi sử dụng hai độ dài huấn luyện khác nhau: 10 epochs và 20 epochs. Sau khi tiền huấn luyện các mô hình học sinh với những độ dài khác nhau này, chúng tôi thích ứng chúng với các nhiệm vụ downstream CoLA và đánh giá hiệu suất của chúng bằng Matthew Correlation.

Kết quả Bằng cách tăng độ dài (co-)training từ 10 epochs lên 20 epochs, CTCD (Của chúng tôi) cải thiện đáng kể hiệu suất của mô hình học sinh từ 35.02 lên 46.23, với lợi ích là 11.21. Điều này vượt trội hơn phương pháp KD thông thường, chỉ đạt được lợi ích là 0.07 từ 40.94 lên 41.01, mặc dù thời gian huấn luyện lâu hơn. Phương pháp KD thông thường dựa vào mô hình giáo viên được tiền huấn luyện để huấn luyện mô hình học sinh, cho phép hội tụ nhanh nhưng hạn chế việc học của mô hình học sinh. Ngược lại, phương pháp CTCD cho phép các lợi ích hiệu suất bổ sung cho cả mô hình giáo viên và học sinh bằng cách cho phép chúng học và phát triển cùng nhau trong quá trình co-training. Điều này có thể cung cấp các lợi ích hơn nữa cho hiệu suất của mô hình học sinh với co-training lâu hơn.

5.4 Hiệu quả của Community KD
Chúng tôi so sánh Community KD được đề xuất với phương pháp KD một chiều thông thường (Sanh et al.,

--- TRANG 8 ---
Downstream Task MNLI QQP QNLI SST-2 CoLA STSB MRPC RTE Average
Metric AMP Acc. Acc. Acc. Acc. Matthew. Pearson. Spear. F1 Acc. Acc. Acc.
Dataset Size 392.7k 363.8k 104.7k 67.3k 8.5k 5.7k 3.7k 2.5k
Teacher BERT (109M) 84.17 90.89 90.68 91.86 57.54 88.84 88.56 89.31 85.04 65.34 83.23
Student
(67M)One-way KD (Sanh et al., 2019b) FP32 81.93 90.05 87.72 90.94 52.03 86.28 86.07 87.94 82.59 57.76 80.33
Ours: Student 1 FP16 81.88 90.17 88.24 91.51 54.82 86.70 86.49 89.76 85.29 59.21 81.40
Ours: Student 2 FP16 81.34 89.75 88.37 90.71 56.08 86.42 86.44 89.80 85.29 59.20 81.34

Bảng 2: Hiệu quả của Community KD BERT được tiền huấn luyện và BERT 6-layer là kiến trúc mô hình giáo viên và học sinh, tương ứng, cho cả phương pháp của chúng tôi và phương pháp KD một chiều thông thường. Chúng tôi tinh chỉnh các học sinh được chưng cất trên dev sets của benchmark GLUE. Chúng tôi quan sát thấy rằng việc học từ soft knowledge của mô hình học sinh khác cải thiện hiệu suất so với phương pháp KD một chiều thông thường trên hầu hết các nhiệm vụ downstream.

2019a). Để đảm bảo so sánh công bằng, chúng tôi sử dụng BERT-base được tiền huấn luyện làm mô hình giáo viên cho cả hai phương pháp và BERT 6-layer làm học sinh, đây là kiến trúc giống như được sử dụng trong phương pháp KD một chiều thông thường. Như được mô tả trong Mục 4, chúng tôi huấn luyện hai mô hình học sinh đồng thời và chúng học từ BERT được tiền huấn luyện, các nhãn ground truth, và kiến thức của nhau. Lưu ý rằng vì chúng tôi tinh chỉnh một trong hai học sinh được chưng cất bởi Community KD cho các nhiệm vụ downstream, chi phí suy luận giống như phương pháp KD một chiều thông thường. Trong Bảng 2, chúng tôi báo cáo kết quả của BERT và phương pháp KD một chiều thông thường sử dụng các checkpoint được cung cấp bởi Hugging Face (HuggingFace) và cả hai học sinh (Của chúng tôi: Student 1 và Của chúng tôi: Student 2) trên dev sets của benchmark GLUE. Chúng tôi áp dụng Automatic Mixed Precision (AMP) (Paszke et al., 2019) cho Community KD, thường tăng tốc huấn luyện nhưng có thể làm tổn hại hiệu suất.

Kết quả Kết quả được trình bày trong Bảng 2 cho thấy Community KD dẫn đến cải thiện hiệu suất trên các nhiệm vụ downstream như QQP, QNLI, SST-2, CoLA, STSB, MRPC, và RTE, ngay cả khi áp dụng các kỹ thuật quantization. Cụ thể, lợi ích hiệu suất trung bình của mô hình học sinh được chưng cất bằng phương pháp Community KD của chúng tôi cao hơn 1.04 (1.2%) so với mô hình học sinh được chưng cất bởi phương pháp KD một chiều thông thường. Điều này gợi ý rằng việc kết hợp knowledge distillation từ cả mô hình học sinh và mô hình giáo viên được tiền huấn luyện hiệu quả hơn so với chỉ sử dụng knowledge distillation từ mô hình giáo viên được tiền huấn luyện.

6 Hạn chế & Công việc Tương lai
Hạn chế Phương pháp được đề xuất co-train các mô hình từ đầu và có thể đòi hỏi thời gian tiền huấn luyện lâu hơn so với phương pháp KD thông thường. Tuy nhiên, như chúng tôi đã mô tả trong Mục 5.3, khi mô hình học sinh được huấn luyện đủ lâu với giáo viên của nó, nó có thể vượt trội hơn các mô hình được huấn luyện với KD thông thường trên nhiệm vụ downstream. Phương pháp co-training được đề xuất có thể tăng chi phí huấn luyện tổng thể so với distillation một chiều, và nó có thể trở thành một nút thắt cổ chai hiệu suất tùy thuộc vào các ràng buộc tài nguyên huấn luyện. Tuy nhiên, lưu ý rằng CTCD có thể cải thiện chất lượng mô hình trong khi có cùng chi phí suy luận như distillation một chiều trên các nhiệm vụ downstream.

Công việc Tương lai 1) Chia sẻ Kiến trúc. Các mô hình có thể chia sẻ một số kiến trúc của chúng bằng cách tái sử dụng đầu ra của các kiến trúc như vậy và cập nhật chúng cùng nhau trong quá trình back-propagation. Điều này có thể giúp giảm chi phí tính toán và bộ nhớ bổ sung phát sinh bởi co-training mô hình, trong khi cải thiện chất lượng mô hình, đặc biệt là cho mô hình học sinh. 2) Tích hợp Thiết kế Kiến trúc Học sinh và Tăng cường Dữ liệu. Nghiên cứu tương lai có thể tập trung vào việc kết hợp hiệu quả khung công tác CTCD với thiết kế kiến trúc học sinh và các kỹ thuật tăng cường dữ liệu. Việc tích hợp này cung cấp một thay thế đầy hứa hẹn cho các phương pháp knowledge distillation một chiều truyền thống, dẫn đến các cải thiện đáng kể trong các nhiệm vụ mô hình ngôn ngữ.

7 Kết luận
Kích thước và độ phức tạp của các mô hình ngôn ngữ được tiền huấn luyện (PLMs) có thể cản trở tính thực tiễn của chúng cho các nhiệm vụ downstream trực tuyến. Để giải quyết điều này, chúng tôi đã giới thiệu một khung công tác mới gọi là co-training và co-distillation (CTCD). Bằng cách huấn luyện các mô hình có kích thước khác nhau cùng nhau và trích xuất kiến thức liên mô hình theo cả hai hướng, khung công tác CTCD được đề xuất cải thiện cả hiệu quả và hiệu suất mô hình. Khung công tác được đề xuất vượt qua sự đánh đổi giữa hiệu quả và hiệu suất trong các phương pháp knowledge distillation một chiều truyền thống. Đáng chú ý, các mô hình được nén của chúng tôi đạt được lợi ích ấn tượng là 1.66 trên benchmark GLUE, vượt trội hơn các mô hình lớn được huấn luyện bằng các phương pháp độc lập.

--- TRANG 9 ---
Tài liệu tham khảo
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. In NeurIPS .

Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-XL: Attentive language models beyond
a fixed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics , pages 2978–2988, Florence, Italy. Asso-
ciation for Computational Linguistics.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2019, Minneapolis, MN, USA,
June 2-7, 2019, Volume 1 (Long and Short Papers) ,
pages 4171–4186.

Wikimedia Foundation. Wikimedia downloads.

Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.
Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2(7).

HuggingFace. Huggingface.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao
Chen, Linlin Li, Fang Wang, and Qun Liu. 2020.
TinyBERT: Distilling BERT for natural language un-
derstanding. In Findings of the Association for Com-
putational Linguistics: EMNLP 2020 , pages 4163–
4174, Online. Association for Computational Lin-
guistics.

Hayeon Lee, Rui Hou, Jongpil Kim, Davis Liang,
Sung Ju Hwang, and Alexander Min. 2023. A study
on knowledge distillation from weak teacher for scal-
ing up pre-trained language models. arXiv preprint
arXiv:2305.18239 .

Chunyuan Li, Xiang Gao, Yuan Li, Xiujun Li, Baolin
Peng, Yizhe Zhang, and Jianfeng Gao. 2020. Opti-
mus: Organizing sentences via pre-trained modeling
of a latent space. In EMNLP .

Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In 7th International
Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019 . OpenRe-
view.net.

Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch:

Một thư viện deep learning hiệu suất cao, phong cách
imperative. In Advances in Neural Information Process-
ing Systems 32 , pages 8024–8035. Curran Associates,
Inc.

Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han,
Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng
Li, Maosong Sun, and Jie Zhou. 2022. Knowledge
inheritance for pre-trained language models. In Pro-
ceedings of the 2022 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
3921–3937, Seattle, United States. Association for
Computational Linguistics.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-
hou, Antoine Chassang, Carlo Gatta, and Yoshua
Bengio. 2015. Fitnets: Hints for thin deep nets. In In
Proceedings of ICLR .

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019a. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 .

Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. 2019b. Distilbert, a distilled version
of bert: smaller, faster, cheaper and lighter. arXiv
preprint arXiv:1910.01108 .

Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. 2019. Megatron-lm: Training multi-billion
parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053 .

Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.
Patient knowledge distillation for BERT model com-
pression. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
4323–4332, Hong Kong, China. Association for Com-
putational Linguistics.

Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga
Vechtomova, and Jimmy Lin. 2019. Distilling task-
specific knowledge from bert into simple neural net-
works. arXiv preprint arXiv:1903.12136 .

Henry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-
vazhagan, Xin Li, and Amelia Archer. 2019. Small
and practical BERT models for sequence labeling. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3632–
3636, Hong Kong, China. Association for Computa-
tional Linguistics.

--- TRANG 10 ---
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. 2019. Well-read students learn better:
The impact of student initialization on knowledge
distillation. arXiv preprint arXiv:1908.08962 , 13.

Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2019.
GLUE: A multi-task benchmark and analysis plat-
form for natural language understanding. In Interna-
tional Conference on Learning Representations .

Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,
and Furu Wei. 2020a. Minilmv2: Multi-head
self-attention relation distillation for compress-
ing pretrained transformers. arXiv preprint
arXiv:2012.15828 .

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan
Yang, and Ming Zhou. 2020b. Minilm: Deep self-
attention distillation for task-agnostic compression
of pre-trained transformers. Advances in Neural In-
formation Processing Systems , 33:5776–5788.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.
Xlnet: Generalized autoregressive pretraining for lan-
guage understanding. In Advances in Neural Infor-
mation Processing Systems , volume 32. Curran Asso-
ciates, Inc.

Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and
Jiashi Feng. 2020. Revisiting knowledge distillation
via label smoothing regularization. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 3903–3911.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-
dinov, Raquel Urtasun, Antonio Torralba, and Sanja
Fidler. 2015. Aligning books and movies: Towards
story-like visual explanations by watching movies
and reading books. In Proceedings of the IEEE in-
ternational conference on computer vision , pages
19–27.
