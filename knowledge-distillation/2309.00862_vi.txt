# 2309.00862.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2309.00862.pdf
# Kích thước tệp: 901977 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Học liên tục Few-shot được dẫn dắt bởi Mô hình lớn
Ziqi Gu1#, Chunyan Xu1#, Zihan Lu1, Xin Liu2, Anbo Dai2, Zhen Cui1∗
1Trường Đại học Khoa học và Công nghệ Nanjing, Nanjing, Trung Quốc
2SeetaCloud, Nanjing, Trung Quốc
Tóm tắt
Học liên tục few-shot (FSCL) đã thu hút sự chú ý tích cực và đạt được một số tiến bộ trong những năm gần đây, nhưng hiện tại rất khó để lại tạo ra một bước tiến lớn về độ chính xác do sự hạn chế của chỉ có các mẫu tăng trưởng few-shot. Lấy cảm hứng từ khả năng nhận thức đặc biệt của con người trong việc học suốt đời (Flesch et al. 2018; Kudithipudi et al. 2022), trong nghiên cứu này, chúng tôi đề xuất một khung Học liên tục Few-shot được dẫn dắt bởi Mô hình lớn mới (B-FSCL) để dần dần phát triển mô hình dưới sự dẫn dắt của các mô hình lớn của thế giới (như kiến thức tích lũy của con người). Cụ thể, chúng tôi thực hiện việc học chuyển giao được dẫn dắt bởi mô hình lớn để tận dụng khả năng mã hóa mạnh mẽ của những mô hình lớn hiện có này, có thể thích ứng mô hình liên tục với một số ít mẫu mới được thêm vào trong khi tránh vấn đề over-fitting. Xem xét rằng mô hình lớn và mô hình liên tục có thể có kết quả nhận thức khác nhau cho những hình ảnh giống hệt nhau, chúng tôi giới thiệu một cơ chế quyết định thích ứng cấp độ thể hiện để cung cấp hỗ trợ nhận thức linh hoạt cấp cao được điều chỉnh theo các mẫu khác nhau. Ngược lại, quyết định thích ứng có thể được áp dụng thêm để tối ưu hóa các tham số của mô hình liên tục, thực hiện chưng cất thích ứng thông tin kiến thức của mô hình lớn. Kết quả thử nghiệm của B-FSCL được đề xuất của chúng tôi trên ba bộ dữ liệu phổ biến (bao gồm CIFAR100, minilmageNet và CUB200) vượt trội hoàn toàn so với tất cả các phương pháp FSCL tiên tiến nhất.

Giới thiệu
Các kỹ thuật học sâu gần đây đã tạo ra những bước tiến đáng kể trong lĩnh vực thị giác máy tính, như được chứng minh bởi những thành công đạt được trong một số nghiên cứu (Simonyan và Zisserman 2014; He et al. 2016). Sự sẵn có của dữ liệu được gán nhãn và sức mạnh tính toán đã thúc đẩy những tiến bộ trong học máy. Tuy nhiên, các ứng dụng thực tế thường liên quan đến các luồng dữ liệu mới và chưa được nhìn thấy liên tục, tạo ra thách thức cho các mô hình truyền thống bị giới hạn trong các tập nhãn được xác định trước. Điều này đòi hỏi một mạng nơ-ron có khả năng học liên tục, đặc biệt khi một số lớp mới có mẫu few-shot. Do đó, nghiên cứu này tập trung vào việc giải quyết các vấn đề của học liên tục few-shot. Có hai thách thức trong nhiệm vụ học liên tục few-shot: i): một số ít mẫu huấn luyện cho các lớp mới, điều này cản trở việc thu nhận kiến thức hiệu quả cho những lớp này; ii): Nhu cầu giảm thiểu việc quên thảm khốc và duy trì kiến thức đã được thu nhận trước đó trong quá trình học liên tục.

Gần đây, sự chú ý lớn đã được dành cho học liên tục few-shot từ nhiều góc độ khác nhau (Tao et al. 2020a; Chi et al. 2022; Zhao et al. 2023). Ví dụ, cấu trúc tô-pô của không gian kiến thức được tạo ra bởi các lớp khác nhau đã được xem xét cho học liên tục few-shot với mạng gas nơ-ron hoặc các mô hình dựa trên đồ thị (Tao et al. 2020a; Zhang et al. 2021; Dong et al. 2021). Một số phương pháp meta-learning (Chi et al. 2022; Javed và White 2019) cũng đã được giới thiệu để tạo điều kiện cho việc bảo tồn kiến thức trước đó và cho phép thích ứng với các lớp mới trong quá trình học liên tục. Tương tự, kỹ thuật tự giám sát đã được sử dụng để tăng cường khả năng trích xuất đặc trưng của mô hình, kết hợp sự hỗ trợ của một hàm mục tiêu tự giám sát trong giai đoạn huấn luyện (Kalla và Biswas 2022; Mazumder, Singh, và Rai 2021). Hơn nữa, phương pháp huấn luyện tương thích tiến (Zhou et al. 2022) đã áp dụng nguyên mẫu ảo để nén các embedding của các lớp đã học và dự đoán các lớp mới tiềm năng, ngăn chặn việc quên những kiến thức đã học này. Một cấu trúc chưng cất song phương (Zhao et al. 2023) đã được sử dụng để giảm thiểu vấn đề over-fitting khi đối mặt với những mẫu được gán nhãn few-shot này. Trong (Yang et al. 2023), tình trạng khó khăn về sự không đồng bộ trong học liên tục few-shot đã được giải quyết bằng cách lấy cảm hứng từ hiện tượng sụp đổ nơ-ron. Một phương pháp học phân tích được nhúng kernel Gaussian (Zhuang et al. 2023) đã được đề xuất để cân bằng hiệu quả sự ưu tiên giữa các nhiệm vụ cũ và mới, đặc biệt trong thiết lập few-shot. Mặc dù những phương pháp FCSL trên đã đạt được một số lợi thế, nhưng rất khó khăn cho các mô hình mạng hiện tại để tạo ra một bước tiến lớn với các mẫu few-shot hạn chế.

Lấy cảm hứng từ cơ chế nhận thức của não bộ con người trong việc học suốt đời (Flesch et al. 2018; Kudithipudi et al. 2022), chúng tôi cố gắng giải quyết vấn đề học liên tục few-shot từ các mô hình lớn của thế giới (như CLIP thời thượng gần đây (Radford et al. 2021a)), có thể được hiểu như kinh nghiệm tích lũy của con người. Trong nghiên cứu này, chúng tôi đề xuất một khung Học liên tục Few-shot được dẫn dắt bởi Mô hình lớn mới (B-FSCL), có thể thúc đẩy một mạng nơ-ron sâu học tiến bộ từ một luồng tuần tự các mẫu few-shot được gán nhãn. Ở đây chúng tôi giải quyết những thách thức chính của học liên tục few-shot từ hai góc độ.arXiv:2309.00862v1  [cs.CV]  2 Sep 2023

--- TRANG 2 ---
độ. Thứ nhất, để khai thác các khả năng mạnh mẽ của một mô hình lớn được huấn luyện trước với không gian kích thích được nhúng tốt, chúng tôi giới thiệu một mô-đun chuyển giao embedding được dẫn dắt bởi mô hình lớn để tăng cường khả năng mã hóa của mô hình liên tục. Để đạt được điều này, chúng tôi tối ưu hóa các lớp tích chập của mô hình liên tục bằng cách tối đa hóa thông tin tương hỗ của các phân phối đặc trưng đa tỷ lệ giữa hai mô hình. Do đó, kiến thức đã học của mô hình liên tục sẽ phù hợp hơn với kiến thức được nhúng trong mô hình lớn của thế giới. Việc chuyển giao embedding được dẫn dắt bởi mô hình lớn có thể tạo điều kiện cho mô hình liên tục thích ứng với những dữ liệu tăng trưởng few-shot này, đồng thời giảm thiểu các vấn đề over-fitting và quên thảm khốc trong quá trình FSCL. Thứ hai, việc học mô hình và đạt được dự đoán chính xác hơn với các mẫu hạn chế là một thách thức. Xem xét rằng mô hình lớn và mô hình liên tục có thể thể hiện các khả năng suy luận khác nhau cho dữ liệu giống hệt nhau, chúng tôi giới thiệu một cơ chế quyết định thích ứng cấp độ thể hiện điều chỉnh quá trình suy luận một cách hiệu quả cho các mẫu khác nhau. Bằng cách thúc đẩy giao tiếp hiệu quả giữa các mô hình, quyết định cuối cùng có thể tích hợp các lợi thế lý luận của cả mô hình lớn và mô hình liên tục để đạt được kết quả chính xác hơn.

Những đóng góp chính của nghiên cứu này như sau:
• Lấy cảm hứng từ cơ chế nhận thức của não bộ con người (Flesch et al. 2018), chúng tôi đề xuất một khung Học liên tục Few-shot được dẫn dắt bởi Mô hình lớn mới (B-FSCL), nhằm tận dụng khả năng embedding và suy luận mạnh mẽ của mô hình lớn của thế giới để giải quyết nhiệm vụ FSCL.
• Chúng tôi thực hiện cập nhật tiến bộ của mô hình liên tục từ hai khía cạnh, cụ thể là chuyển giao embedding được dẫn dắt bởi mô hình lớn và quyết định thích ứng cấp độ thể hiện, nhanh chóng thích ứng để học các khái niệm mới đặc biệt cho những mẫu tăng trưởng few-shot này.
• Các đánh giá và so sánh toàn diện trên ba bộ dữ liệu công khai (bao gồm CUB200 (Wah et al. 2011), CIFAR100 (Krizhevsky 2009), và miniImageNet (Russakovsky et al. 2015)) đã chứng minh rõ ràng rằng khung B-FSCL của chúng tôi đạt được một cải thiện đầy hứa hẹn lớn khi so sánh với các phương pháp tiên tiến nhất hiện có trong nhiệm vụ FSCL.

Nghiên cứu liên quan
Học liên tục few-shot
Học liên tục few-shot đã nhận được sự chú ý ngày càng tăng trong những năm gần đây, với mục tiêu cho phép một mô hình học liên tục từ một chuỗi dữ liệu được gán nhãn few-shot. Một số phương pháp đã được đề xuất để giải quyết những thách thức trong nhiệm vụ này, bao gồm quên lãng, over-fitting và mất cân bằng dữ liệu. Ví dụ, Tao et al. (Tao et al. 2020a) đề xuất sử dụng một mạng gas nơ-ron để học và duy trì cấu trúc tô-pô của các đặc trưng danh mục khác nhau cho học liên tục few-shot. Một cấu trúc chưng cất song phương (Zhao et al. 2023) được đề xuất để giảm thiểu việc quên thảm khốc kiến thức bằng cách xem xét vấn đề over-fitting. Lấy cảm hứng từ hiện tượng sụp đổ nơ-ron, Yang et al.(Yang et al. 2023) đã giải quyết tình trạng khó khăn về sự không đồng bộ trong học liên tục few-shot. Ngoài ra, Zhuang et al. (Zhuang et al. 2023) đã áp dụng một phương pháp học phân tích được nhúng kernel Gaussian để cân bằng sự ưu tiên giữa các nhiệm vụ cũ và mới. Zhang et al. (Zhang et al. 2021) giới thiệu một bộ phân loại phát triển dựa trên mạng chú ý đồ thị để tạo điều kiện cho việc truyền bá thông tin giữa các bộ phân loại. FACT (Zhou et al. 2022) tạo ra các lớp ảo và bảo tồn kiến thức của các lớp cũ để giải quyết vấn đề quên thảm khốc. Một phương pháp tối ưu hóa hai cấp (Chi et al. 2022) được đề xuất để giảm việc quên kiến thức đã học và thích ứng với các lớp mới dựa trên meta-learning. Các bộ phân loại ngẫu nhiên tự giám sát cũng được đề xuất trong (Kalla và Biswas 2022) để giải quyết học tăng trưởng lớp few-shot sử dụng cơ chế tự giám sát. Phương pháp chưng cất kiến thức nhận thức ngữ nghĩa (Cheraghian et al. 2021) coi embedding từ như thông tin bổ sung và kết hợp các thuật ngữ chưng cất kiến thức để giảm thiểu vấn đề quên lãng. Michael et al. (Hersche et al. 2022) đề xuất phương pháp học tăng trưởng lớp few-shot có ràng buộc, sử dụng embedding siêu chiều để cho phép học liên tục của nhiều lớp hơn vượt quá số chiều cố định trong không gian đặc trưng, với mục đích cân bằng độ chính xác và chi phí tính toán-bộ nhớ của việc học các lớp mới. Hơn nữa, Zhu et al. (Zhu et al. 2021) đề xuất một sơ đồ học nguyên mẫu tăng trưởng để đạt được học liên tục bằng cách tinh chỉnh các nguyên mẫu lớp sử dụng các nguyên mẫu đã biết và các mẫu few-shot từ các lớp mới để huấn luyện mô hình, và cập nhật các nguyên mẫu dựa trên điều này.

Mô hình thế giới mở
Lấy cảm hứng từ những thành tựu của các mô hình lớn của thế giới như (CLIP) (Radford et al. 2021a), SAM (Kirillov et al. 2023), GPT-3 (Dale 2021) và Dino (Liu et al. 2023), một loạt nghiên cứu đã liên quan đến việc huấn luyện trước các mô hình thị giác-ngôn ngữ trên các bộ dữ liệu hình ảnh-văn bản quy mô lớn (Radford et al. 2021a; Jia et al. 2021). Trong số những phương pháp này, huấn luyện trước thị giác-ngôn ngữ đối比 (CLIP) (Radford et al. 2021a) cho thấy kết quả ấn tượng trên các nhiệm vụ downstream đa dạng. Mô hình CLIP bao gồm một bộ mã hóa hình ảnh và một bộ mã hóa văn bản. Trong giai đoạn huấn luyện trước, học đối比 được sử dụng, trong đó một cặp hình ảnh-văn bản khớp phục vụ như một ví dụ tích cực, trong khi các cặp hình ảnh-văn bản khác nhau được coi như tiêu cực. Nhiều nghiên cứu đã đề xuất các phương pháp huấn luyện khác nhau để cải thiện hiệu suất của các mô hình ngôn ngữ thị giác trong các nhiệm vụ downstream. Ví dụ, CLIP-FSAR (Wang et al. 2023) giới thiệu các phương pháp tăng cường dữ liệu hoặc thay đổi kiến trúc để tăng cường hiệu suất tổng quát hóa của các mô hình CLIP trên các nhiệm vụ khác nhau. CLIP-Adapter (Gao et al. 2021) được tối ưu hóa cho các nhiệm vụ cụ thể bằng cách thêm các adapter tại các lớp khác nhau trong mô hình. Thengane et al. (Thengane et al. 2022) đã chứng minh rằng dự đoán zero-shot của mô hình CLIP đạt được mức hiệu suất cao nhất trong học liên tục ngay cả khi không có bất kỳ huấn luyện nào. ZSCL (Zheng et al. 2023) tăng cường mối tương quan giữa hình ảnh và văn bản bằng cách cải thiện chiến lược học đối比, dẫn đến hiệu suất tốt hơn trong chuyển giao zero-shot. Bên cạnh đó, Zhuang et al. (Ding et al. 2022) đã áp dụng một biến thể thích ứng của kỹ thuật học mà không quên

--- TRANG 3 ---
sử dụng các câu được tạo ngẫu nhiên để tính toán các tổn thất chưng cất.

Mô tả vấn đề
Học liên tục few-shot (FSCL) nhằm đạt được việc học liên tục của mô hình từ một luồng tuần tự các mẫu được gán nhãn thưa thớt, trong đó dữ liệu từ các nhiệm vụ trước đó không thể được truy cập khi học một nhiệm vụ few-shot mới. Chính thức, chúng tôi định nghĩa tập huấn luyện, tập gán nhãn và tập kiểm tra lần lượt là X, Y và Z. Một chuỗi các bộ dữ liệu huấn luyện được gán nhãn X1, X2, . . ., XT được sử dụng để học mô hình, trong đó Xt đại diện cho tập huấn luyện cho phiên thứ t, Yt đại diện cho các nhãn tương ứng, và T biểu thị tổng số phiên học liên tục. Trong phiên cơ sở FSCL (tức là, t = 1), mô hình được huấn luyện trên các lớp cơ sở với một số lượng đủ mẫu trong tập huấn luyện X1 và tập nhãn tương ứng Y1. Trong mỗi phiên học liên tục t (t > 1), tập huấn luyện Xt có N lớp, mỗi lớp chứa K mẫu (tức là, cấu hình "N-way K-shot"). Mỗi tập huấn luyện được xây dựng để tránh việc lặp lại các nhãn lớp. Nói cách khác, đối với bất kỳ cặp phiên i và j nào trong đó i ≠ j, các tập nhãn Yi và Yj không có lớp nào chồng chéo, tức là, Yi ∩ Yj = ∅. Đối với t > 1, tập huấn luyện Xt bao gồm các mẫu few-shot cho các lớp mới được giới thiệu trong phiên thứ t, và kích thước của nó nhỏ hơn so với kích thước của X1. Tập kiểm tra Z được sử dụng để đánh giá độ chính xác phân loại tại mỗi phiên t, và nó có thể chứa các lớp từ tất cả các tập nhãn huấn luyện kết hợp, ký hiệu là {Y1 ∪ Y2 ··· ∪ YT}.

Phương pháp B-FSCL
Động lực
Học liên tục few-shot tập trung vào việc tối ưu hóa mô hình để học và thích ứng hiệu quả với kiến thức mới với các mẫu hạn chế trong khi duy trì kiến thức đã học. Có hai thách thức nghiêm trọng mà chúng ta cần giải quyết: i) Dữ liệu huấn luyện khan hiếm cho các lớp mới, điều này cản trở việc thu nhận kiến thức hiệu quả cho những lớp này; ii) Sự cần thiết phải giảm thiểu việc quên thảm khốc và bảo tồn kiến thức đã học trước đó trong khi trải qua việc học liên tục. Trên thực tế, con người có thể nhanh chóng học từng nhiệm vụ mới mà không quên những nhiệm vụ trước đó, trong khi các mạng nơ-ron nhân tạo không có những khả năng phi thường như vậy. Sau khi điều tra các cơ chế nhận thức của não bộ con người, các nghiên cứu (Flesch et al. 2018; Kudithipudi et al. 2022) phát hiện rằng việc tăng cường các mạng nơ-ron quy mô lớn với kinh nghiệm giám sát/không giám sát phong phú có thể học được một embedding tốt của không gian kích thích (tương tự như những gì quan sát được ở con người), và tiếp tục giảm việc quên thảm khốc trong quá trình học liên tục.

Lấy cảm hứng từ những phát hiện trên, chúng tôi cố gắng tận dụng khả năng hiểu biết mạnh mẽ của các mô hình lớn hiện có của thế giới (như CLIP (Radford et al. 2021b)), có thể được hiểu như kiến thức kinh nghiệm mà con người đã học được ở một mức độ nhất định. Ở đây chúng tôi giải quyết nhiệm vụ FSCL từ hai khía cạnh sau. Thứ nhất, chúng tôi có thể thực hiện việc học chuyển giao được dẫn dắt bởi mô hình lớn để sử dụng khả năng mã hóa mạnh mẽ của mô hình lớn được huấn luyện trước, có thể thích ứng mô hình liên tục với một số ít mẫu mới được thêm vào

Hình 1: Quy trình của khung B-FSCL của chúng tôi. Dưới sự dẫn dắt của mô hình lớn của thế giới, chúng tôi dần dần tối ưu hóa các tham số của mô hình liên tục (tức là, Θ) trong quá trình huấn luyện. Chúng tôi sử dụng Ψ* để đại diện cho mô hình lớn, vì các tham số của nó luôn được đóng băng. Ở đây chúng tôi thiết kế một mô-đun chuyển giao embedding được dẫn dắt bởi mô hình lớn và một mô-đun quyết định thích ứng theo từng thể hiện để cải thiện khả năng mã hóa và suy luận của nó khi giải quyết nhiệm vụ FSCL.

trong khi giảm thiểu các vấn đề over-fitting và quên thảm khốc. Thứ hai, xem xét rằng mô hình lớn và mô hình liên tục có thể có kết quả nhận thức khác nhau ngay cả đối với cùng một vật, chúng tôi giới thiệu một quyết định thích ứng cấp độ thể hiện để cung cấp hỗ trợ suy luận cấp cao được điều chỉnh theo các mẫu hình ảnh khác nhau. Tương tự như giao tiếp hiệu quả giữa mọi người, các quyết định cuối cùng có thể xem xét tốt các điểm mạnh suy luận của cả mô hình lớn và mô hình liên tục dưới sự hướng dẫn của cơ chế thích ứng cấp độ thể hiện. Hơn nữa, kết quả có thể được sử dụng để dần dần tối ưu hóa các tham số của mô hình liên tục, thực hiện chưng cất kiến thức từ mô hình lớn của thế giới trong quá trình học liên tục.

Mô hình B-FSCL
Hình 1 minh họa quy trình của khung B-FSCL được đề xuất của chúng tôi. Khác với các phương pháp FSCL hiện có, toàn bộ B-FSCL được học liên tục dưới sự hướng dẫn/dẫn dắt của các mô hình lớn hiện có của thế giới, có khả năng embedding và lý luận mạnh mẽ cho dữ liệu cần được xử lý. Đầu tiên, bằng cách tận dụng tối đa mô hình lớn của thế giới (ký hiệu là Ψ*), chúng tôi thiết kế một mô-đun chuyển giao embedding được dẫn dắt bởi mô hình lớn để tăng cường khả năng mã hóa của mô hình liên tục (ký hiệu là Θ), đặc biệt cho các phiên học few-shot. Cụ thể, chúng tôi tối đa hóa thông tin tương hỗ của hai phân phối giữa đặc trưng tích chập F(x, Θconv)(l) của mô hình liên tục và đặc trưng tương ứng F(x, Ψ*)(l) của mô hình lớn trên nhiều tỷ lệ (tức là, l = 1, 2, . . ., L). Thông qua việc học chuyển giao được dẫn dắt bởi mô hình lớn, chúng tôi có thể nhanh chóng học các khái niệm mới bằng cách thích ứng mô hình liên tục với các mẫu few-shot mới, trong khi giảm thiểu các vấn đề over-fitting và quên thảm khốc của nhiệm vụ FSCL. Hơn nữa, xem xét những khác biệt tiềm năng trong khả năng hiểu biết giữa mô hình lớn và mô hình liên tục đối với mẫu giống hệt nhau, chúng tôi giới thiệu mô-đun quyết định thích ứng cấp độ thể hiện

--- TRANG 4 ---
để đưa ra dự đoán hợp lý hơn cho các hình ảnh đầu vào khác nhau. Ở đây chúng tôi xây dựng một α-Net để học một tham số quyết định thích ứng α với F(x, Ψ*) từ mô hình lớn và F(x, Θconv) từ mô hình liên tục làm đầu vào. Dưới sự hướng dẫn của tham số α đã học, chúng tôi có thể thu được kết quả phân loại cuối cùng bằng cách xem xét các xác suất quyết định của mô hình lớn và mô hình liên tục (tức là, pbg và pcmt). Chúng tôi sử dụng cùng chiến lược huấn luyện được mô tả ở trên cho cả phiên cơ sở và phiên mới trong khung B-FSCL được đề xuất của chúng tôi.

Chuyển giao Embedding được dẫn dắt bởi mô hình lớn
Khả năng học hạn chế của mô hình liên tục thường cản trở khả năng nắm bắt các khái niệm từ một số ít mẫu mới được thêm vào. Trong hầu hết các phương pháp FSCL hiện có (Zhang et al. 2021; Zhou et al. 2022), một chiến lược phổ biến để giảm thiểu việc quên là chỉ cập nhật lớp fully-connected cuối cùng trong khi đóng băng các lớp tích chập mã hóa. Tuy nhiên, những phương pháp này khiến việc học các khái niệm mới từ các mẫu few-shot trở nên khó khăn. Lấy cảm hứng từ cơ chế nhận thức của não bộ con người (Flesch et al. 2018), chúng tôi cố gắng tận dụng các khả năng mạnh mẽ của một mô hình lớn được huấn luyện trước sở hữu không gian kích thích được nhúng tốt. Ở đây chúng tôi thực hiện chuyển giao embedding được dẫn dắt bởi mô hình lớn để tăng cường khả năng mã hóa của mô hình liên tục, từ đó giảm thiểu các vấn đề quên thảm khốc và over-fitting ở một mức độ nhất định.

Dưới sự dẫn dắt của mô hình lớn của thế giới, chúng tôi hy vọng rằng kiến thức đã học của mô hình liên tục có thể nghiêng về phía mô hình lớn hơn. Cụ thể, chúng tôi tối đa hóa thông tin tương hỗ của các phân phối đặc trưng đa tỷ lệ như sau:

arg max
Θconv ∑(l=1 to L) IΘconv(F(x,Θconv)(l), F(x,Ψ*)(l)), (1)

trong đó Θconv biểu thị các tham số tích chập của mô hình liên tục, Ψ* đề cập đến mô hình lớn cố định, và L đại diện cho tổng số tỷ lệ tích chập được liên quan. F(x, Θconv)(l) biểu thị đặc trưng tích chập của hình ảnh đầu vào x trong lớp thứ l của mô hình liên tục, trong khi F(x, Ψ*)(l) đại diện cho đặc trưng tích chập tương ứng của mô hình lớn. Vì thông tin tương hỗ của các đặc trưng rời rạc khó tính toán, chúng tôi sử dụng cận dưới cho thông tin tương hỗ dựa trên phân kỳ Kullback-Leibler (Donsker và Varadhan 1975). Quá trình chuyển giao embedding được dẫn dắt bởi mô hình lớn trong Công thức (1) có thể được biểu diễn thêm như:

arg max
Θconv,φ ∑(l=1 to L) EJ[Tφ(F(x,Θconv)(l), F(x,Ψ*)(l))]
−log EM[eTφ(F(x,Θconv)(l),F(x,Ψ*)(l))], (2)

trong đó Tφ(·) là một hàm phân biệt giữa các đặc trưng của các mô hình khác nhau. EJ đại diện cho kỳ vọng của phân phối chung của F(x, Θconv)(l) và F(x, Ψ*)(l), trong khi EM biểu thị kỳ vọng của các phân phối lề của F(x, Θconv)(l) và F(x, Ψ*)(l). Thông qua việc dần dần tối ưu hóa các tham số mạng Θconv và φ, các đặc trưng của mô hình liên tục F(x, Θconv) sẽ hội tụ ổn định về phía embedding của mô hình lớn của thế giới F(x, Ψ*)(l), điều này cũng làm cho mô hình liên tục thích ứng với những mẫu hình ảnh tăng trưởng này.

Quyết định Thích ứng cấp độ Thể hiện
Trong quá trình học liên tục, một thách thức lớn khác là việc học mô hình và đạt được kết quả dự đoán chính xác chỉ với các mẫu few-shot là khó khăn. Xem xét rằng mô hình lớn và mô hình liên tục có thể sở hữu các khả năng suy luận khác nhau cho cùng một mẫu, chúng tôi giới thiệu một cơ chế quyết định thích ứng cấp độ thể hiện để cung cấp suy luận hiệu quả được điều chỉnh cho các mẫu khác nhau. Bằng cách thúc đẩy giao tiếp hiệu quả giữa các mô hình, quyết định cuối cùng có thể tích hợp toàn diện các lợi thế lý luận của cả mô hình lớn và mô hình liên tục, từ đó cho phép ra quyết định chính xác hơn. Để hướng dẫn quyết định thích ứng theo từng thể hiện, chúng tôi xây dựng một α-Net để học hệ số trọng số α cho mỗi đầu vào, chính thức,

α = H(F(x,Θ), F(x,Ψ*), ωα), (3)

trong đó F(x, Θ) và F(x, Ψ*) đại diện cho các đặc trưng embedding của hình ảnh x từ mô hình liên tục và mô hình lớn, tương ứng. ωα biểu thị các tham số của α-Net cần được học. Trọng số α ∈ [0,1] được đạt được bằng cách xem xét các embedding đặc trưng của hai mô hình (tức là, F(x, Θ) và F(x, Ψ*)). Dưới sự hướng dẫn của trọng số α đã học, quyết định thích ứng cấp độ thể hiện có thể được thực hiện như sau:

p = α * pctm + (1−α) * pbg, (4)

trong đó pctm và pbg đại diện cho các phân phối xác suất đạt được từ mô hình liên tục và mô hình lớn, tương ứng. p đề cập đến phân phối xác suất của quyết định cuối cùng.

Để tối ưu hóa các tham số mạng của mô hình liên tục và α-Net (tức là, Θ và ωα), chúng tôi giảm thiểu giá trị entropy chéo giữa sự thật cơ bản và xác suất của quyết định cuối cùng của chúng tôi:

arg min
Θ,ωα −∑(i ∈ |Xt|) yi log pi, (5)

trong đó i là chỉ số của tập huấn luyện Xt. Đối với mỗi hình ảnh đầu vào xi, yi là sự thật cơ bản của nó và xác suất quyết định cuối cùng pi là giá trị tối đa trong phân phối xác suất của pi. Quyết định thích ứng cấp độ thể hiện cho phép tích hợp toàn diện các lợi thế suy luận giữa mô hình lớn và mô hình liên tục trong các quyết định cuối cùng, đạt được kết quả phân loại hiệu quả hơn. Trong khi đó, quá trình này có thể chưng cất kiến thức một cách chọn lọc từ mô hình lớn của thế giới đến mô hình liên tục, có thể làm cho mô hình liên tục thích ứng với những mẫu mới được thêm vào này.

Thí nghiệm
Thiết lập Thí nghiệm
Bộ dữ liệu: Chúng tôi đánh giá hiệu suất của B-FSCL trên ba bộ dữ liệu có sẵn công khai: CIFAR100 (Krizhevsky 2009), miniImageNet (Russakovsky et al. 2015), và

--- TRANG 5 ---
Bảng 1: So sánh hiệu suất giữa B-FSCL được đề xuất của chúng tôi và các phương pháp tiên tiến nhất khác trên bộ dữ liệu CUB200.
Kết quả được đánh dấu bằng * được lấy từ mã được xuất bản của các tác giả.

Phương pháp | Độ chính xác trong mỗi phiên (%) ↑ | KR↑ | ∆Final ↑ | Avg↑
---|---|---|---|---
| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |

Ft-CNN | 68.68 | 43.70 | 25.05 | 17.72 | 18.08 | 16.95 | 15.10 | 10.60 | 8.93 | 8.93 | 8.47 | 12.33 | +60.87 | 22.02
NCM (Hou et al. 2019) | 68.68 | 57.12 | 44.21 | 28.78 | 26.71 | 25.66 | 24.62 | 21.52 | 20.12 | 20.06 | 19.87 | 28.93 | +49.47 | 32.49
iCaRL (Rebuffi et al. 2017) | 68.68 | 52.65 | 48.61 | 44.16 | 36.62 | 29.52 | 27.83 | 26.26 | 24.01 | 23.89 | 21.16 | 30.80 | +48.18 | 36.67
EEIL (Castro et al. 2018) | 68.68 | 53.63 | 47.91 | 44.20 | 36.30 | 27.46 | 25.93 | 24.70 | 23.95 | 24.13 | 22.11 | 32.19 | +47.23 | 36.27
TOPIC (Tao et al. 2020b) | 68.68 | 62.49 | 54.81 | 49.99 | 45.25 | 41.40 | 38.35 | 35.36 | 32.22 | 28.31 | 26.28 | 38.26 | +43.06 | 43.92
SPPR (Zhu et al. 2021) | 68.68 | 61.85 | 57.43 | 52.68 | 50.19 | 46.88 | 44.65 | 43.07 | 40.17 | 39.63 | 37.33 | 54.35 | +32.01 | 49.32
D-DeepEMD (Zhang et al. 2020) | 75.35 | 70.69 | 66.68 | 62.34 | 59.76 | 56.54 | 54.61 | 52.52 | 50.73 | 49.20 | 47.60 | 63.17 | +21.74 | 58.73
D-NegCosine (Liu et al. 2020) | 74.96 | 70.57 | 66.62 | 61.32 | 60.09 | 56.06 | 55.03 | 52.78 | 51.50 | 50.08 | 48.47 | 64.66 | +20.87 | 58.86
D-Cosine (Vinyals et al. 2016) | 75.52 | 70.95 | 66.46 | 61.20 | 60.86 | 56.88 | 55.40 | 53.49 | 51.94 | 50.93 | 49.31 | 65.29 | +20.03 | 59.36
CEC (Zhang et al. 2021) | 75.80 | 71.94 | 68.50 | 63.50 | 62.43 | 58.27 | 57.73 | 55.81 | 54.83 | 53.52 | 52.28 | 68.97 | +17.06 | 61.33
Mate-FSCIL (Chi et al. 2022) | 75.90 | 72.41 | 68.78 | 64.78 | 62.96 | 59.99 | 58.30 | 56.85 | 54.78 | 53.83 | 52.64 | 69.35 | +16.70 | 61.93
FACT* (Zhou et al. 2022) | 77.38 | 73.91 | 70.32 | 65.91 | 65.02 | 61.82 | 61.29 | 59.53 | 57.92 | 57.63 | 56.46 | 72.95 | +13.68 | 64.29
GKEAL(Zhuang et al. 2023) | 78.88 | 75.62 | 72.32 | 68.62 | 67.23 | 64.26 | 62.98 | 61.89 | 60.20 | 59.21 | 58.67 | 74.38 | +10.67 | 66.35
NC-FSCIL(Yang et al. 2023) | 80.45 | 75.98 | 72.30 | 70.28 | 68.17 | 65.16 | 64.43 | 63.25 | 60.66 | 60.01 | 59.44 | 73.88 | +9.90 | 67.28
ALICE(Peng et al. 2022) | 77.40 | 72.70 | 70.60 | 67.20 | 65.90 | 63.40 | 62.90 | 61.90 | 60.50 | 60.60 | 60.10 | 77.65 | +9.24 | 65.75
CABD(Zhao et al. 2023) | 79.12 | 75.63 | 73.21 | 69.93 | 68.32 | 66.30 | 65.15 | 64.96 | 64.20 | 64.03 | 63.81 | 80.65 | +5.53 | 68.61
B-FSCL(Ours) | 81.64 | 79.45 | 77.29 | 72.85 | 73.54 | 71.86 | 71.83 | 70.16 | 69.55 | 68.93 | 69.34 | 84.93 | - | 73.31
Cải thiện so với CABD | +2.52 | +3.82 | +4.08 | +2.92 | +5.22 | +5.19 | +5.56 | +6.68 | +5.20 | +4.90 | +5.53 | +4.28 | - | +4.70

Hình 2: Ma trận nhầm lẫn của kết quả cuối cùng trên bộ dữ liệu CUB200. (a) B-FSCL. (b) Baseline. (c) Mô hình lớn (CLIP). Chúng tôi vạch ranh giới giữa lớp cơ sở và lớp mới bằng các đường màu đỏ. Phương pháp của chúng tôi cải thiện hiệu suất dự đoán của mô hình, dẫn đến sự tập trung hơn.

CUB200 (Wah et al. 2011). CIFAR100 (Krizhevsky 2009) bao gồm 100 lớp với tổng cộng 60.000 hình ảnh RGB. miniImageNet (Russakovsky et al. 2015) là một tập con của ImageNet, bao gồm 100 lớp. CUB200 (Wah et al. 2011) là một bộ dữ liệu phân loại chi tiết với 11.788 hình ảnh được phân bố trong 200 lớp. Đối với các bộ dữ liệu CIFAR100 và miniImageNet, chúng tôi sử dụng tất cả dữ liệu huấn luyện của 60 lớp cơ sở để huấn luyện một mạng cơ sở. 40 lớp mới sau đó được sử dụng cho tám nhiệm vụ học liên tục 5-way 5-shot. Chúng tôi chia 200 lớp của CUB200 thành 100 lớp cơ sở và 100 lớp mới. 100 lớp mới được sử dụng cho mười nhiệm vụ học liên tục 10-way 5-shot. Để đảm bảo so sánh công bằng, chúng tôi tuân theo cùng thiết lập phân chia trên ba bộ dữ liệu, như trong FSCIL (Tao et al. 2020b).

Chi tiết Triển khai: Tham khảo FSCIL (Tao et al. 2020b), chúng tôi sử dụng ResNet20 làm backbone cho CIFAR100 (Krizhevsky 2009), trong khi ResNet18 phục vụ làm backbone cho cả miniImageNet (Russakovsky et al. 2015) và CUB200 (Wah et al. 2011). Mạng ước lượng thông tin tương hỗ φ bao gồm ba lớp tích chập với kích thước kernel 1, 3 và 5. α-network bao gồm ba lớp fully connected với hàm kích hoạt sigmoid và xuất ra phép đo α cho mỗi mẫu. Trong các thí nghiệm của chúng tôi, chúng tôi chọn CLIP (Radford et al. 2021b) làm mô hình lớn của thế giới áp dụng 'ViT-L/14' làm mô hình được huấn luyện trước và "The image depicts a {}" làm prompt. Trong giai đoạn huấn luyện, chúng tôi kết hợp các kỹ thuật tăng cường dữ liệu, bao gồm cắt ngẫu nhiên, co giãn ngẫu nhiên và lật ngang ngẫu nhiên. Để đánh giá hiệu suất của phương pháp được đề xuất một cách toàn diện, chúng tôi sử dụng nhiều chỉ số. "Acc t" đại diện cho độ chính xác Top-1 trong giai đoạn liên tục thứ t, trong khi "Avg" biểu thị điểm trung bình, được tính là ∑(t=1 to T) Acct/T. "∆Final" chỉ ra sự khác biệt trong Acc t giữa phương pháp của chúng tôi và phương pháp được so sánh trong giai đoạn cuối cùng. "KR" viết tắt của tỷ lệ duy trì kiến thức, được định nghĩa là Acc T/Acc1. Tất cả các thí nghiệm được tiến hành bằng framework PyTorch trên NVIDIA GeForce 4090 GPU. Do hạn chế về không gian, một số kết quả thí nghiệm được hoãn lại cho tài liệu bổ sung.

So sánh với các phương pháp tiên tiến nhất
Để đánh giá tính ưu việt của khung B-FSCL được đề xuất của chúng tôi, chúng tôi tiến hành đánh giá trên bộ dữ liệu CUB200 (Wah et al. 2011) và so sánh hiệu suất của nó với các phương pháp tiên tiến nhất khác. Kết quả chi tiết được trình bày trong Bảng 1, và các đường cong độ chính xác qua các phiên học liên tục được minh họa trong Hình 3(c). Khung B-FSCL của chúng tôi cho thấy hiệu suất ấn tượng, vượt trội đáng kể so với tất cả các phương pháp được so sánh khác. Đáng chú ý, B-FSCL của chúng tôi thể hiện những cải thiện đáng kể trong mỗi giai đoạn so với phương pháp tốt thứ hai CABD (Zhao et al. 2023), điều này xác nhận tính ưu việt của phương pháp chúng tôi. Trong giai đoạn liên tục cuối cùng, phương pháp của chúng tôi vượt trội đáng kể so với CABD với ∆Final là 5.53%. Kết quả này nổi bật khả năng học liên tục nhất quán của B-FSCL trong suốt giai đoạn liên tục. Chúng tôi xác nhận rằng mô-đun chuyển giao embedding được dẫn dắt bởi mô hình lớn có thể chuyển giao hiệu quả khả năng mã hóa mạnh mẽ của mô hình lớn đến mô hình liên tục, từ đó giải quyết vấn đề khả năng học không đủ cho các lớp mới và cải thiện khả năng thích ứng với các nhiệm vụ mới với các mẫu few-shot. So với phương pháp tốt nhất hiện tại (tức là,

--- TRANG 6 ---
Bảng 2: So sánh phương pháp của chúng tôi sử dụng các thành phần khác nhau trên bộ dữ liệu CUB200.

Baseline | Big-model (CLIP) | BET | IAD | Độ chính xác trong mỗi phiên (%) ↑ | KR | Avg
---|---|---|---|---|---|---
| | | | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |
✓ | | | | 64.27 | 60.44 | 59.42 | 56.84 | 58.71 | 58.41 | 57.89 | 54.99 | 54.60 | 55.58 | 57.02 | - | -
✓ | ✓ | | | 79.42 | 75.65 | 71.35 | 66.88 | 66.13 | 63.99 | 62.84 | 60.73 | 60.23 | 59.45 | 58.32 | 73.43 | 65.91
✓ | | ✓ | | 79.92 | 77.24 | 74.11 | 69.57 | 69.62 | 67.16 | 66.12 | 64.75 | 63.45 | 62.57 | 61.67 | 77.65 | 68.74
✓ | | | ✓ | 81.03 | 77.99 | 75.52 | 70.69 | 71.89 | 70.47 | 69.64 | 68.07 | 66.11 | 66.08 | 66.26 | 81.77 | 71.25
✓ | ✓ | ✓ | ✓ | 81.64 | 79.45 | 77.29 | 72.85 | 73.54 | 71.86 | 71.83 | 70.16 | 69.55 | 68.93 | 69.34 | 84.93 | 73.31

Hình 3: Các đường cong thay đổi độ chính xác mô tả các phiên liên tục của các phương pháp khác nhau trên ba bộ dữ liệu được hiển thị trong ba hình ảnh. Những hình ảnh này chia sẻ một chú giải chung, được trình bày trong bộ dữ liệu CUB200 (Wah et al. 2011).

CABD(Zhao et al. 2023)), B-FSCL được đề xuất của chúng tôi đạt được 4.70% về chỉ số Avg. Nó chỉ ra rằng B-FSCL của chúng tôi có hiệu suất vượt trội qua tất cả các giai đoạn học liên tục. Phát hiện này tiếp tục xác nhận rằng B-FSCL được đề xuất có thể giảm thiểu hiệu quả việc thiếu khả năng học của mô hình liên tục. Kết quả ấn tượng trong các chỉ số Avg và ∆Final tiếp tục xác minh tính ưu việt của B-FSCL của chúng tôi trong việc học kiến thức mới và giảm thiểu việc quên thảm khốc.

Kết quả thí nghiệm của chúng tôi liên tục chứng minh rằng B-FSCL vượt trội so với tất cả các phương pháp khác, vượt qua phương pháp CABD tốt nhất (Zhao et al. 2023) với 4.28% trong chỉ số KR. Chỉ số KR đo lường tỷ lệ kiến thức giữa giai đoạn liên tục cuối cùng và giai đoạn cơ sở, trong đó giá trị KR cao hơn chỉ ra việc duy trì kiến thức đã học tốt hơn trong các giai đoạn học liên tục. Hơn nữa, những kết quả này cho thấy B-FSCL có thể tăng cường hiệu quả độ chính xác phân loại few-shot bằng cách tích hợp các mô-đun chuyển giao embedding được dẫn dắt bởi mô hình lớn và quyết định thích ứng cấp độ thể hiện. B-FSCL được đề xuất của chúng tôi thể hiện hiệu suất xuất sắc bằng cách tích hợp các lợi thế lý luận của cả mô hình lớn và mô hình liên tục, dẫn đến việc ra quyết định chính xác hơn. Ngoài ra, các đường cong thay đổi độ chính xác được mô tả trong Hình 3(c) xác nhận trực quan tính ưu việt của khung B-FSCL của chúng tôi. Bên cạnh đó, chúng tôi kết hợp các đường cong độ chính xác riêng biệt cho các bộ dữ liệu miniImageNet và CIFAR100 trong Hình 3(a) và Hình 3(b), tương ứng. Kết quả đánh giá rõ ràng chứng minh hiệu suất vượt trội của phương pháp B-FSCL của chúng tôi trên các bộ dữ liệu khác nhau. Điều này tiếp tục nhấn mạnh hiệu quả của các mô-đun chuyển giao embedding được dẫn dắt bởi mô hình lớn và quyết định thích ứng cấp độ thể hiện được đề xuất của chúng tôi trong việc thu nhận kiến thức của các lớp mới trong khi giải quyết vấn đề quên thảm khốc.

Nghiên cứu loại bỏ
Tất cả các nghiên cứu loại bỏ được tiến hành trên bộ dữ liệu CUB200 để đánh giá hiệu quả của các mô-đun được đề xuất.

Hiệu quả của các thành phần khác nhau: Chúng tôi tiến hành thí nghiệm để đánh giá hiệu quả của các mô-đun chuyển giao embedding được dẫn dắt bởi mô hình lớn (BET) và quyết định thích ứng cấp độ thể hiện (IAD) được đề xuất. Kết quả chi tiết được trình bày trong Bảng 2. Hiệu suất của mô hình lớn (CLIP) được đánh giá tại mỗi giai đoạn của quá trình học liên tục. Hiệu suất mô hình lớn (CLIP) vẫn ổn định trong suốt toàn bộ giai đoạn học liên tục. So với baseline, hiệu suất của Avg và KR đã cải thiện lần lượt 2.83% và 4.22%. Những kết quả này cho thấy rằng mô-đun BET được đề xuất của chúng tôi chuyển giao khả năng mã hóa mạnh mẽ của mô hình lớn đến mô hình liên tục, từ đó tăng cường khả năng học các lớp mới của nó. Do đó, hiệu quả của mô-đun chuyển giao embedding được dẫn dắt bởi mô hình lớn được đề xuất của chúng tôi được xác nhận. Mục tiêu chính của mô-đun IAD là giới thiệu một cơ chế quyết định thích ứng để cung cấp suy luận hiệu quả được điều chỉnh cho các mẫu khác nhau. Điều này dẫn đến những cải thiện hiệu suất đáng kể 8.34% và 5.34% về các chỉ số KR và Avg, tương ứng. Tóm lại, việc tích hợp cả hai mô-đun dẫn đến hiệu suất được tăng cường, thiết lập một mức hiệu suất tiên tiến nhất mới. Điều này xác nhận hiệu quả của khung B-FSCL được đề xuất.

Phân tích ma trận nhầm lẫn: Chúng tôi tiếp tục hình dung các ma trận nhầm lẫn của B-FSCL, baseline và mô hình lớn (CLIP) của chúng tôi trong Hình 2. Rõ ràng là các phần tử đường chéo của ma trận nhầm lẫn B-FSCL thể hiện màu sắc đậm hơn so với mô hình baseline và mô hình lớn (CLIP), rõ ràng chỉ ra rằng B-FSCL của chúng tôi hiệu quả hơn trong việc học và nhận biết các lớp mới. Hơn nữa, ma trận nhầm lẫn B-FSCL cho thấy phân phối tập trung hơn của các dự đoán

--- TRANG 7 ---
Bảng 3: So sánh hiệu suất của các phương pháp khác nhau với hiệu suất cơ sở tương tự.

Phương pháp | Độ chính xác trong mỗi phiên (%) ↑ | KR↑ | ∆Final ↑ | Avg↑
---|---|---|---|---
| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 |

ALICE(Peng et al. 2022) | 77.40 | 72.70 | 70.60 | 67.20 | 65.90 | 63.40 | 62.90 | 61.90 | 60.50 | 60.60 | 60.10 | 77.65 | +3.43 | 65.75
FACT* (Zhou et al. 2022) | 77.38 | 73.91 | 70.32 | 65.91 | 65.02 | 61.82 | 61.29 | 59.53 | 57.92 | 57.63 | 56.46 | 72.95 | +7.07 | 64.29
GKEAL(Zhuang et al. 2023) | 78.88 | 75.62 | 72.32 | 68.62 | 67.23 | 64.26 | 62.98 | 61.89 | 60.20 | 59.21 | 58.67 | 74.38 | +4.86 | 66.35
B-FSCL | 77.70 | 75.17 | 72.72 | 67.69 | 68.98 | 67.63 | 67.07 | 65.07 | 63.36 | 63.35 | 63.53 | 81.76 | - | 68.39

Hình 4: So sánh hiệu suất giữa CEC+Ours và CEC.

Hình 5: So sánh hiệu suất với các số shot khác nhau.

Hình 6: Hiệu suất của các lớp mới trong mỗi phiên học liên tục.

cho các lớp mới so với mô hình baseline và mô hình lớn (CLIP), nổi bật trực quan khả năng học kiến thức vượt trội của B-FSCL của chúng tôi.

So sánh hiệu suất của các phương pháp khác nhau với hiệu suất cơ sở tương tự: Trong Bảng 3, chúng tôi trình bày một so sánh giữa khung B-FSCL với ba phương pháp truyền thống: GKEAL(Zhuang et al. 2023), ALICE(Peng et al. 2022), và FACT* (Zhou et al. 2022) trong đó hiệu suất lớp cơ sở được cố định ở khoảng 78.0%. Chúng ta có thể quan sát rằng B-FSCL vượt trội so với phương pháp khác trên cả ba chỉ số đánh giá, mặc dù giữ hiệu suất mô hình cơ sở cố định. Điều này ngụ ý rằng hiệu suất vượt trội không phải do khả năng của mô hình cơ sở, mà đóng góp vào khả năng mã hóa của mô hình liên tục và cơ chế quyết định thích ứng được xây dựng. Những kết quả này chứng minh hiệu quả của B-FSCL được đề xuất trong việc thu nhận kiến thức mới trong khi giảm thiểu thách thức của việc quên thảm khốc.

Tích hợp mô hình CEC: Chúng tôi thực hiện một thí nghiệm so sánh bằng cách tích hợp B-FSCL với CEC (Zhang et al. 2021) được trình bày trong Hình 4. Kết quả chỉ ra rằng hiệu suất của CEC được cải thiện đáng kể bởi B-FSCL của chúng tôi. Khả năng trích xuất đặc trưng của khung B-FSCL mạnh hơn so với các mạng backbone được sử dụng trong CEC. Nó chứng minh rằng khả năng mã hóa mạnh mẽ của mô hình lớn (CLIP) và cơ chế quyết định thích ứng được tận dụng bởi B-FSCL của chúng tôi, dẫn đến việc học kiến thức mới hiệu quả hơn và giảm thiểu việc quên thảm khốc. Điều này xác nhận hiệu quả của khung B-FSCL trong việc giải quyết vấn đề học liên tục few-shot. Từ đó tiếp tục xác nhận hiệu quả của khung B-FSCL trong việc giải quyết vấn đề học liên tục few-shot.

Các số shot K khác nhau: Các đường cong độ chính xác với các số shot K khác nhau được trình bày trong Hình 5. Những kết quả này chứng minh rằng việc sử dụng nhiều mẫu hơn có thể cải thiện đáng kể hiệu suất của học liên tục lớp few-shot. Điều này là do thực tế là một lượng lớn mẫu cho phép phân phối chính xác hơn của các thuộc tính lớp, từ đó cải thiện việc thu nhận kiến thức mới. Hơn nữa, một khi lượng dữ liệu vượt quá 15, tỷ lệ cải thiện hiệu suất giảm dần. Nó cho thấy rằng một mức hiệu suất thỏa mãn có thể đạt được với một lượng dữ liệu mới được chú thích tối ưu, từ đó xác nhận hiệu quả của B-FSCL để giải quyết nhiệm vụ học liên tục few-shot.

Hiệu suất của các lớp mới trong mỗi phiên học liên tục: Để đánh giá hiệu suất của B-FSCL khi học các lớp mới, chúng tôi trình bày độ chính xác của các lớp mới cho baseline, B-FSCL và mô hình lớn (CLIP) trong mỗi giai đoạn học liên tục, như được mô tả trong Hình 6. Kết quả thí nghiệm cho thấy rằng B-FSCL được đề xuất của chúng tôi vượt trội so với cả baseline và mô hình big-model(CLIP) trong độ chính xác lớp mới. Nó rõ ràng xác nhận tính ưu việt của B-FSCL được đề xuất trong việc học kiến thức mới hiệu quả và giảm thiểu việc quên thảm khốc.

Kết luận
Trong nghiên cứu này, chúng tôi đề xuất một khung Học liên tục Few-shot được dẫn dắt bởi Mô hình lớn (B-FSCL), có thể sử dụng tốt các khả năng embedding và lý luận mạnh mẽ của mô hình lớn hiện có để giải quyết hai thách thức chính trong nhiệm vụ FSCL. Dưới sự dẫn dắt kiến thức của mô hình lớn của thế giới, chúng tôi giới thiệu việc học chuyển giao embedding được dẫn dắt bởi mô hình lớn để tăng cường khả năng mã hóa tích chập của mô hình liên tục, điều này cũng có thể giảm thiểu các vấn đề quên thảm khốc và over-fitting trong quá trình học liên tục. Bằng cách tạo điều kiện cho giao tiếp hiệu quả giữa các mô hình, chúng tôi đặc biệt thiết kế một mô-đun quyết định thích ứng cấp độ thể hiện để có được kết quả nhận biết chính xác hơn, có thể được sử dụng để hướng dẫn chưng cất kiến thức cho mô hình liên tục. Các thí nghiệm rộng rãi đã chứng minh hiệu quả của B-FSCL được đề xuất của chúng tôi. Trong nghiên cứu tương lai, chúng tôi sẽ tiếp tục sử dụng các lợi thế của mô hình lớn của thế giới để thúc đẩy khả năng học liên tục của các mạng nơ-ron nhân tạo.

--- TRANG 8 ---
Tài liệu tham khảo
Castro, F. M.; Marín-Jiménez, M. J.; Guil, N.; Schmid, C.; và Alahari, K. 2018. End-to-end incremental learning. Trong Proceedings of the European conference on computer vision (ECCV), 233–248.

Cheraghian, A.; Rahman, S.; Fang, P.; Roy, S. K.; Petersson, L.; và Harandi, M. 2021. Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning. computer vision and pattern recognition.

Chi, Z.; Gu, L.; Liu, H.; Wang, Y.; Yu, Y.; và Tang, J. 2022. MetaFSCIL: A Meta-Learning Approach for Few-Shot Class Incremental Learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 14166–14175.

Dale, R. 2021. GPT-3: What's it good for? Natural Language Engineering, 27(1): 113–118.

Ding, Y.; Liu, L.; Tian, C.; Yang, J.; và Ding, H. 2022. Don't Stop Learning: Towards Continual Learning for the CLIP Model. arXiv preprint arXiv:2207.09248.

Dong, S.; Hong, X.; Tao, X.; Chang, X.; Wei, X.; và Gong, Y. 2021. Few-Shot Class-Incremental Learning via Relation Knowledge Distillation. AAAI.

Donsker, M. D.; và Varadhan, S. S. 1975. On a variational formula for the principal eigenvalue for operators with maximum principle. Proceedings of the National Academy of Sciences, 72(3): 780–783.

Flesch, T.; Balaguer, J.; Dekker, R.; Nili, H.; và Summerfield, C. 2018. Comparing continual task learning in minds and machines. Proceedings of the National Academy of Sciences, 115(44): E10313–E10322.

Gao, P.; Geng, S.; Zhang, R.; Ma, T.; Fang, R.; Zhang, Y.; Li, H.; và Qiao, Y. 2021. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544.

He, K.; Zhang, X.; Ren, S.; và Sun, J. 2016. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, 770–778.

Hersche, M.; Karunaratne, G.; Cherubini, G.; Benini, L.; Sebastian, A.; và Rahimi, A. 2022. Constrained Few-shot Class-incremental Learning.

Hou, S.; Pan, X.; Loy, C. C.; Wang, Z.; và Lin, D. 2019. Learning a unified classifier incrementally via rebalancing. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 831–839.

Javed, K.; và White, M. 2019. Meta-learning representations for continual learning. Advances in Neural Information Processing Systems, 32.

Jia, C.; Yang, Y.; Xia, Y.; Chen, Y.-T.; Parekh, Z.; Pham, H.; Le, Q.; Sung, Y.-H.; Li, Z.; và Duerig, T. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. Trong International conference on machine learning, 4904–4916. PMLR.

Kalla, J.; và Biswas, S. 2022. S3C: Self-Supervised Stochastic Classifiers for Few-Shot Class-Incremental Learning. Trong European Conference on Computer Vision, 432–448. Springer.

Kirillov, A.; Mintun, E.; Ravi, N.; Mao, H.; Rolland, C.; Gustafson, L.; Xiao, T.; Whitehead, S.; Berg, A. C.; Lo, W.-Y.; và cộng sự. 2023. Segment anything. arXiv preprint arXiv:2304.02643.

Krizhevsky, A. 2009. Learning Multiple Layers of Features from Tiny Images.

Kudithipudi, D.; Aguilar-Simon, M.; Babb, J.; Bazhenov, M.; Blackiston, D.; Bongard, J.; Brna, A. P.; Chakravarthi Raja, S.; Cheney, N.; Clune, J.; và cộng sự. 2022. Biological underpinnings for lifelong learning machines. Nature Machine Intelligence, 4(3): 196–210.

Liu, B.; Cao, Y.; Lin, Y.; Li, Q.; Zhang, Z.; Long, M.; và Hu, H. 2020. Negative margin matters: Understanding margin in few-shot classification. Trong European conference on computer vision, 438–455. Springer.

Liu, S.; Zeng, Z.; Ren, T.; Li, F.; Zhang, H.; Yang, J.; Li, C.; Yang, J.; Su, H.; Zhu, J.; và cộng sự. 2023. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499.

Mazumder, P.; Singh, P.; và Rai, P. 2021. Few-shot lifelong learning. Trong Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, 2337–2345.

Peng, C.; Zhao, K.; Wang, T.; Li, M.; và Lovell, B. C. 2022. Few-shot class-incremental learning from an open-set perspective. Trong European Conference on Computer Vision, 382–397. Springer.

Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; và cộng sự. 2021a. Learning transferable visual models from natural language supervision. Trong International conference on machine learning, 8748–8763. PMLR.

Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; và cộng sự. 2021b. Learning transferable visual models from natural language supervision. Trong International conference on machine learning, 8748–8763. PMLR.

Rebuffi, S.-A.; Kolesnikov, A.; Sperl, G.; và Lampert, C. H. 2017. icarl: Incremental classifier and representation learning. Trong Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2001–2010.

Russakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.; Ma, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.; và cộng sự. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3): 211–252.

Simonyan, K.; và Zisserman, A. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. computer vision and pattern recognition.

--- TRANG 9 ---
Tao, X.; Hong, X.; Chang, X.; Dong, S.; Wei, X.; và Gong, Y. 2020a. Few-shot class-incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12183–12192.

Tao, X.; Hong, X.; Chang, X.; Dong, S.; Wei, X.; và Gong, Y. 2020b. Few-Shot Class-Incremental Learning. computer vision and pattern recognition.

Thengane, V.; Khan, S.; Hayat, M.; và Khan, F. 2022. Clip model is an efficient continual learner. arXiv preprint arXiv:2210.03114.

Vinyals, O.; Blundell, C.; Lillicrap, T.; Wierstra, D.; và cộng sự. 2016. Matching networks for one shot learning. Advances in neural information processing systems, 29.

Wah, C.; Branson, S.; Welinder, P.; Perona, P.; và Belongie, S. 2011. The Caltech-UCSD Birds-200-2011 Dataset.

Wang, X.; Zhang, S.; Cen, J.; Gao, C.; Zhang, Y.; Zhao, D.; và Sang, N. 2023. CLIP-guided prototype modulating for few-shot action recognition. arXiv preprint arXiv:2303.02982.

Yang, Y.; Yuan, H.; Li, X.; Lin, Z.; Torr, P.; và Tao, D. 2023. Neural collapse inspired feature-classifier alignment for few-shot class incremental learning. arXiv preprint arXiv:2302.03004.

Zhang, C.; Cai, Y.; Lin, G.; và Shen, C. 2020. Deepemd: Few-shot image classification with differentiable earth mover's distance and structured classifiers. Trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 12203–12213.

Zhang, C.; Song, N.; Lin, G.; Zheng, Y.; Pan, P.; và Xu, Y. 2021. Few-shot incremental learning with continually evolved classifiers. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 12455–12464.

Zhao, L.; Lu, J.; Xu, Y.; Cheng, Z.; Guo, D.; Niu, Y.; và Fang, X. 2023. Few-Shot Class-Incremental Learning via Class-Aware Bilateral Distillation. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11838–11847.

Zheng, Z.; Ma, M.; Wang, K.; Qin, Z.; Yue, X.; và You, Y. 2023. Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models. arXiv preprint arXiv:2303.06628.

Zhou, D.-W.; Wang, F.-Y.; Ye, H.-J.; Ma, L.; Pu, S.; và Zhan, D.-C. 2022. Forward compatible few-shot class-incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 9046–9056.

Zhu, K.; Cao, Y.; Zhai, W.; Cheng, J.; và Zha, Z.-J. 2021. Self-promoted prototype refinement for few-shot class-incremental learning. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 6801–6810.

Zhuang, H.; Weng, Z.; He, R.; Lin, Z.; và Zeng, Z. 2023. GKEAL: Gaussian Kernel Embedded Analytic Learning for Few-Shot Class Incremental Task. Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 7746–7755.
