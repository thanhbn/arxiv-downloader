# 2310.19820.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.19820.pdf
# Kích thước tệp: 436892 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bài viết chủ đề: tinyML
NetDistiller: Trao quyền cho Deep Learning nhỏ
thông qua Chưng cất tại chỗ
Shunyao Zhang, Đại học Rice, Houston, TX, 77005, USA
Yonggan Fu, Viện Công nghệ Georgia, Atlanta, GA, 30332, USA
Shang Wu, Đại học Rice, Houston, TX, 77005, USA
Jyotikrishna Dass, Đại học Rice, Houston, TX, 77005, USA
Haoran You, Viện Công nghệ Georgia, Atlanta, GA, 30332, USA
Yingyan (Celine) Lin, Viện Công nghệ Georgia, Atlanta, GA, 30332, USA
Tóm tắt—Tăng cường độ chính xác nhiệm vụ của các mạng neural nhỏ (TNNs) đã trở thành một thách thức cơ bản để cho phép triển khai TNNs trên các thiết bị edge bị hạn chế bởi những giới hạn nghiêm ngặt về bộ nhớ, tính toán, băng thông và nguồn cung cấp điện. Để đạt được điều này, chúng tôi đề xuất một framework gọi là NetDistiller để tăng cường độ chính xác có thể đạt được của TNNs bằng cách coi chúng như các mạng con của một giáo viên chia sẻ trọng số được xây dựng bằng cách mở rộng số lượng kênh của TNN. Cụ thể, mô hình TNN mục tiêu được huấn luyện chung với mô hình giáo viên chia sẻ trọng số thông qua (1) phẫu thuật gradient để giải quyết xung đột gradient giữa chúng và (2) chưng cất nhận thức về độ không chắc chắn để giảm thiểu overfitting của mô hình giáo viên. Các thí nghiệm rộng rãi trên các nhiệm vụ đa dạng xác nhận hiệu quả của NetDistiller trong việc tăng cường độ chính xác có thể đạt được của TNNs so với các phương pháp hiện đại. Mã của chúng tôi có sẵn tại https://github.com/GATECH-EIC/NetDistiller.

Hiệu suất kỷ lục gần đây của các mạng neural (NNs) đã thúc đẩy việc ứng dụng ngày càng tăng của chúng trong các ngành khoa học và kỹ thuật khác nhau. Song song với đó, dự báo rằng số lượng thiết bị kết nối Internet of Things (IoT) trên toàn thế giới sẽ đạt 30,9 tỷ đơn vị vào năm 2025 [1]. Triển khai trí tuệ được cung cấp bởi NN trên nhiều thiết bị IoT có ý nghĩa quan trọng, vì nó cho phép sử dụng dữ liệu thu thập tại edge cho các chức năng thông minh trên thiết bị khác nhau có thể cách mạng hóa cuộc sống con người. Nhu cầu tăng trưởng mạnh mẽ này đã dẫn đến sự ra đời của lĩnh vực mạng neural nhỏ (TNNs) đã thu hút sự chú ý tăng lên đáng kể. Điều này là do TNNs cho phép các thiết bị edge nhỏ và rẻ tiền làm việc trực tiếp trên dữ liệu cục bộ với chi phí điện năng và tính toán thấp hơn, dẫn đến cả việc giảm độ trễ và tăng cường quyền riêng tư vì nó giảm bớt hoặc thậm chí loại bỏ sự cần thiết của kết nối internet để chia sẻ và tập trung hóa dữ liệu trên máy chủ đám mây. Tuy nhiên, hiệu suất nhiệm vụ có thể đạt được của TNNs vẫn chưa thỏa mãn do khả năng mô hình hạn chế của chúng. Do đó, cải thiện hiệu suất nhiệm vụ của TNNs đã trở thành một thách thức cơ bản để cho phép việc áp dụng rộng rãi của chúng, điều này rất được mong muốn trong nhiều ứng dụng edge thực tế.

Để giải quyết thách thức nêu trên và từ đó mở khóa tiềm năng của TNNs tại edge, đã có một nỗ lực nghiên cứu ngày càng tăng hướng tới việc tăng cường hiệu suất nhiệm vụ có thể đạt được của chúng. Cụ thể, đã được chỉ ra rằng việc huấn luyện TNNs về cơ bản khác với việc huấn luyện NNs lớn. Ví dụ, các tác giả của [2] xác định rằng TNNs gặp phải under-fitting do khả năng mô hình hạn chế của chúng, trái ngược với NNs lớn, dễ bị over-fitting. Họ cũng quan sát thấy rằng các kỹ thuật tăng cường dữ liệu và regularization, vốn tăng cường độ chính xác ImageNet cho các NNs lớn over-parameterized như ResNet50, có tác động có hại đến độ chính xác của TNNs, chẳng hạn như MobileNetV2-Tiny [3], nhỏ hơn 174× so với ResNet50.

Lấy cảm hứng từ các nghiên cứu trước, chúng tôi giả thuyết rằng việc tăng cường khả năng mô hình (ví dụ, kênh) trong quá trình huấn luyện cho phép TNNs thu được kiến thức bổ sung, dẫn đến cải thiện độ chính xác nhiệm vụ. Trong chưng cất kiến thức vanilla [4], kiến thức được mã hóa bởi một mô hình lớn được chuyển giao cho một mô hình nhỏ hơn bằng cách huấn luyện mô hình nhỏ với các đầu ra và/hoặc activations của mô hình lớn. Kết quả là, mô hình nhỏ có thể đạt được độ chính xác cao hơn bằng cách bắt chước hành vi của mô hình lớn. Trong công trình này, chúng tôi ủng hộ một lược đồ chưng cất kiến thức tại chỗ mới, vuông góc với lược đồ vanilla, tiếp tục tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs. Chúng tôi đóng góp như sau:

• Chúng tôi là những người đầu tiên chứng minh rằng việc tích hợp một supernet chia sẻ trọng số với chưng cất tại chỗ có thể phục vụ như một công thức huấn luyện hiệu quả để tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs. Cụ thể, chúng tôi đề xuất một framework tên là NetDistiller, kết hợp TNN mục tiêu như một mô hình học sinh trong một supernet chia sẻ trọng số hoạt động như một mô hình giáo viên để tăng cường hiệu suất nhiệm vụ của các TNNs được huấn luyện mà không gây ra bất kỳ overhead suy luận nào.

• Chúng tôi xác định rằng chưng cất tại chỗ vanilla có thể gây ra xung đột gradient nghiêm trọng giữa supernet giáo viên và sub-network học sinh (tức là TNN mục tiêu). Cụ thể, chúng tôi phát hiện rằng tới 50% gradient trọng số trong mô hình học sinh có độ tương tự cosine âm với những gradient của mô hình giáo viên. Điều này dẫn đến hội tụ kém khi các gradient này tích lũy trên các trọng số chia sẻ của chúng. Hơn nữa, chưng cất tại chỗ vanilla có xu hướng gây ra overfitting trong mô hình giáo viên, do đó làm giảm hiệu quả của chưng cất tại chỗ của chúng tôi.

• Để giảm thiểu cả hai vấn đề được xác định ở trên, NetDistiller đề xuất hai giải pháp: (1) loại bỏ các gradient xung đột bằng cách chiếu các gradient xung đột của giáo viên lên mặt phẳng pháp tuyến của gradient học sinh, và (2) tích hợp chưng cất nhận thức về độ không chắc chắn để động chọn hàm loss học sinh giữa Kullback–Leibler divergence và cross-entropy loss dựa trên độ chắc chắn của đầu ra mô hình học sinh. Những cải tiến này cho phép NetDistiller giải phóng hiệu quả hứa hẹn của chưng cất tại chỗ trong việc huấn luyện TNNs thuận lợi hơn.

• Chúng tôi thực hiện đánh giá rộng rãi và nghiên cứu ablation để xác nhận hiệu quả của framework NetDistiller của chúng tôi trong việc tăng cường độ chính xác có thể đạt được của TNNs khi so sánh với phương pháp hiện đại (SOTA). Ví dụ, chúng tôi quan sát thấy độ chính xác cao hơn 2,3% so với NetAug [2], khi huấn luyện mô hình MobileNet-V3-w0.35 trên dataset ImageNet. Chúng tôi hiểu rằng NetDistiller đã mở ra một quan điểm mới để tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs và làm phong phú lĩnh vực chưng cất kiến thức.

CÔNG TRÌNH LIÊN QUAN

Mạng Neural Hiệu quả / Nhỏ. Tiến bộ đáng kể đã được đạt trong việc thiết kế NNs hiệu quả và thân thiện với thiết bị di động. Ví dụ, MobileNets [5] sử dụng Convolutions Tách biệt theo Chiều sâu, thay thế một lớp convolution tiêu chuẩn bằng sự kết hợp của depthwise convolution và pointwise convolution, chứng minh tiềm năng giảm chi phí tính toán trong khi duy trì độ chính xác nhiệm vụ. Song song với các mạng hiệu quả được thiết kế thủ công và các lược đồ nén, học máy tự động đã được sử dụng thành công thông qua tìm kiếm kiến trúc neural [6], [7]. Trái ngược với các kỹ thuật trên, framework đề xuất của chúng tôi nhằm cải thiện độ chính xác suy luận của TNNs thông qua chưng cất tại chỗ nơi kiến trúc TNN mục tiêu được sử dụng như một học sinh sub-network của một mô hình giáo viên supernet chia sẻ trọng số.

Chưng cất Kiến thức. Chưng cất Kiến thức (KD) [8] đề cập đến ý tưởng chuyển giao kiến thức thu được bởi một mô hình được huấn luyện trước và over-parameterized (giáo viên) cho một mô hình nhỏ (học sinh) phù hợp hơn cho triển khai edge. Cụ thể, mô hình nhỏ thường có khả năng không đủ để học một biểu diễn kiến thức ngắn gọn, và KD trao quyền cho mô hình học sinh học hành vi chính xác của mô hình giáo viên bằng cách bắt chước các đầu ra của giáo viên ở mỗi cấp độ, tức là soft labels. Trong công trình này, chúng tôi ủng hộ một lược đồ chưng cất tại chỗ mới, cụ thể là NetDistiller để tăng cường hiệu suất nhiệm vụ của TNNs. Trái ngược với KD vanilla nơi mô hình giáo viên là một NN lớn được huấn luyện trước trên một dataset khác và mô hình học sinh là một mô hình nhỏ hơn riêng biệt, NetDistiller là một cách tiếp cận vuông góc kết hợp TNN như một học sinh sub-network trong một supernet chia sẻ trọng số hoạt động như mô hình giáo viên cho chưng cất tại chỗ.

Tăng cường Mạng. Các tác giả trong [2] đề xuất tăng cường mạng, được biết đến như NetAug, để tăng cường độ chính xác của deep learning nhỏ bằng cách giảm thiểu vấn đề under-fitting. Cụ thể, NetAug động tăng cường mạng trong quá trình huấn luyện, kết hợp mô hình nhỏ như một sub-model trong một mô hình lớn hơn để giám sát phụ trợ bên cạnh chức năng độc lập của nó. Trái ngược, NetDistiller cung cấp một lược đồ thay thế để tăng cường hiệu suất của TNNs thông qua chưng cất kiến thức tại chỗ. Cụ thể, TNN trong NetDistiller hoạt động như một sub-network (học sinh) trong một supernet chia sẻ trọng số tĩnh (giáo viên) được xây dựng bằng cách mở rộng các kênh của TNN mục tiêu. Trong phần kết quả thí nghiệm, chúng tôi cung cấp một nghiên cứu so sánh của NetDistiller đề xuất với lược đồ SOTA NetAug và phát hiện rằng NetDistiller vượt trội hơn NetAug, ví dụ, đạt được độ chính xác cao hơn 2,3% khi huấn luyện mô hình MobileNet-V3-w0.35 trên dataset ImageNet.

Framework NetDistiller

NetDistiller đề xuất của chúng tôi là một công thức huấn luyện để tăng cường độ chính xác của TNNs bằng cách kết hợp TNN mục tiêu như một mô hình học sinh (sub-network) trong một supernet chia sẻ trọng số, hoạt động như một mô hình giáo viên. Thông qua chưng cất tại chỗ, NetDistiller chưng cất và chuyển giao kiến thức từ một mô hình giáo viên supernet cho mô hình học sinh sub-network là TNN mục tiêu của chúng tôi. Trong phần này, chúng tôi đầu tiên mô tả việc xây dựng supernet chia sẻ trọng số từ TNN theo sau là triển khai thực tế của chưng cất tại chỗ của chúng tôi. Tiếp theo, chúng tôi mô tả các kỹ thuật để giải quyết xung đột gradient giữa các mô hình giáo viên và học sinh, cũng như giảm thiểu vấn đề over-fitting trong mô hình giáo viên trong giai đoạn huấn luyện cuối thông qua chưng cất nhận thức về độ không chắc chắn. Cuối cùng, chúng tôi thảo luận về overhead huấn luyện và suy luận gây ra bởi NetDistiller.

Yếu tố kích hoạt 0 của NetDistiller: Xây dựng Mô hình Giáo viên Chia sẻ Trọng số

NetDistiller mở rộng các kênh của TNN mục tiêu để xây dựng một supernet chia sẻ trọng số hoạt động như mô hình giáo viên. Do đó, TNN mục tiêu hoạt động như một mô hình sub-network. Cả mô hình học sinh và giáo viên chia sẻ trọng số trên tất cả các lớp convolution trong khi duy trì các lớp Batch-Normalization tương ứng của chúng, giải thích cho các thống kê chạy khác nhau (tức là means và variances) của các giá trị activation của chúng. Như một công thức huấn luyện mới để tăng cường khả năng của một TNN mục tiêu để giảm thiểu các vấn đề under-fitting và để tăng cường độ chính xác, NetDistiller xây dựng một mô hình giáo viên với số kênh gấp 3× so với TNN mục tiêu. Hình 1 mô tả việc xây dựng mô hình giáo viên chia sẻ trọng số được mô tả ở trên từ TNN mục tiêu.

Yếu tố kích hoạt 1 của NetDistiller: Chưng cất Tại chỗ

Mục tiêu của chưng cất tại chỗ của chúng tôi là ổn định việc huấn luyện supernet và cải thiện hiệu suất của các sub-networks. Cụ thể, TNNs có nhiều khả năng bị kẹt trong các minimum cục bộ do khả năng không đủ, điều này giới hạn hiệu suất của chúng so với các NNs lớn over-parameterized [2]. Để giải quyết điều này, NetDistiller tích hợp TNN mục tiêu như một mô hình học sinh sub-network trong một mô hình giáo viên supernet chia sẻ trọng số được xây dựng bằng cách mở rộng các kênh của NN mục tiêu như được chứng minh trong Hình 1. Theo hiểu biết tốt nhất của chúng tôi, NetDistiller là đầu tiên chứng minh rằng việc áp dụng chưng cất tại chỗ cho một supernet chia sẻ trọng số [9] có thể phục vụ như một công thức huấn luyện hiệu quả để tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs.

Cụ thể, chưng cất tại chỗ tận dụng 'soft labels' được dự đoán bởi supernet như các tín hiệu giám sát cho mô hình học sinh sub-network trong mỗi lần lặp huấn luyện trong khi sử dụng ground truth labels cho mô hình giáo viên. Một cách chính thức, tại lần lặp huấn luyện n, tham số supernet W được cập nhật bởi

Wn←Wn−1+ηg(Wn−1),

--- TRANG 2 ---
tinyML

HÌNH 1. Tổng quan về NetDistiller. TNN mục tiêu là một mô hình học sinh phục vụ như một sub-network trong một mô hình giáo viên chia sẻ trọng số, được xây dựng bằng cách mở rộng số lượng kênh của TNN mục tiêu. Các mô hình giáo viên và học sinh được huấn luyện đồng thời trong khi mô hình giáo viên được huấn luyện với các ground truth labels và mục tiêu để huấn luyện mô hình học sinh được quyết định thích ứng đầu vào giữa một cơ chế chưng cất tại chỗ và ground truth labels dựa trên độ không chắc chắn đầu ra của nó. Để giảm thiểu vấn đề xung đột gradient quan sát được trong quá trình huấn luyện, các gradient của giáo viên được sửa đổi thông qua phẫu thuật gradient để loại bỏ các thành phần xung đột dựa trên gradient của học sinh.

trong đó η là learning rate, và:

g(Wn−1) = ∇W[LD(W) + Lstu[W,Wstu];Wn−1]|W=Wn−1  (1)

Ở đây, LD là cross-entropy loss của supernet giáo viên trên dataset huấn luyện D, W và Wstu biểu thị các mô hình giáo viên và học sinh, tương ứng, và Lstu là student loss được điều chỉnh bởi chưng cất nhận thức về độ không chắc chắn (được giới thiệu trong phần Yếu tố kích hoạt 3). Bổ sung, quá trình chưng cất trong NetDistiller là single-shot, tức là nó được triển khai tại chỗ trong quá trình huấn luyện mà không có chi phí tính toán và bộ nhớ bổ sung, không giống như KD vanilla hai bước nơi một mô hình lớn phải được huấn luyện trước.

Yếu tố kích hoạt 2 của NetDistiller: Phẫu thuật Gradient để Giải quyết Xung đột Gradient

Xem xét rằng các gradient từ cả mô hình học sinh và giáo viên tích lũy trên các trọng số chia sẻ, chúng tôi xác định rằng chưng cất tại chỗ vanilla có thể gây ra xung đột gradient nghiêm trọng giữa supernet giáo viên và sub-network học sinh (TNN mục tiêu). Cụ thể, chúng tôi phát hiện rằng tới 50% gradient của mô hình học sinh có độ tương tự cosine âm với những gradient của mô hình giáo viên. Lấy cảm hứng từ PCGrad [10] thực hiện phẫu thuật gradient cho học đa nhiệm vụ, NetDistiller giải quyết điều này bằng cách chiếu các gradient giáo viên xung đột lên mặt phẳng pháp tuyến của gradient học sinh, do đó loại bỏ các thành phần xung đột trong gradient giáo viên và cải thiện hiệu suất của TNNs. Cụ thể, hãy để ∇lstu và ∇ltea biểu thị các gradient của mô hình học sinh và giáo viên, tương ứng. Chúng tôi định nghĩa φ là góc giữa hai gradient trên và g là gradient cuối cùng để cập nhật trọng số. Để đảm bảo việc huấn luyện mô hình học sinh, chúng tôi chiếu gradient xung đột của giáo viên, proj(∇ltea), khi độ tương tự cosine cos(φ) = ∇lstu.∇ltea / (||∇lstu||||∇ltea||) là âm, được công thức hóa như sau:

g = ∇lstu + proj(∇ltea), trong đó
proj(∇ltea) = {
  ∇ltea - (∇lTtea∇lstu / ||∇lstu||²)∇lstu, nếu cos(φ) < 0
  ∇ltea, ngược lại
}  (2)

Yếu tố kích hoạt 3 của NetDistiller: Chưng cất Nhận thức về Độ không chắc chắn

Vì các mô hình học sinh và giáo viên chia sẻ trọng số được huấn luyện chung từ đầu và mô hình giáo viên over-parameterized hội tụ nhanh hơn so với mô hình học sinh, chúng tôi quan sát thấy rằng mô hình giáo viên supernet gặp phải over-fitting ở giai đoạn huấn luyện cuối. Bổ sung, [11] ủng hộ rằng các mô hình lớn có cải thiện lớn nhất trên các mẫu nơi mô hình nhỏ không chắc chắn nhất. Đối với các ví dụ nhất định, thậm chí những ví dụ mà mô hình nhỏ không đặc biệt chính xác, các mô hình lớn thường không thể cải thiện. Dựa trên những hiểu biết này, chúng tôi giả thuyết rằng mô hình giáo viên không phải lúc nào cũng là một giáo viên tốt trong suốt quá trình huấn luyện.

Dựa trên điều này, chúng tôi đề xuất một kỹ thuật gọi là chưng cất nhận thức về độ không chắc chắn (UD) để động chọn các hàm loss học sinh giữa Kullback–Leibler (KL) divergence và cross-entropy losses dựa trên độ chắc chắn của đầu ra mô hình học sinh (xem Hình 1). Cụ thể, chúng tôi đo độ không chắc chắn thông qua entropy của các đầu ra học sinh. Khi entropy của đầu ra học sinh cao (tức là không chắc chắn), học sinh được chưng cất bởi giáo viên chia sẻ trọng số thông qua KL divergence loss, ngược lại, học sinh được huấn luyện bởi ground truth label thông qua cross-entropy loss. Chúng tôi công thức hóa quá trình này như sau:

Lstu = {
  KL(Wstu(x), W(x)), uncertainty ≥ T
  CE(Wstu(x), y), ngược lại
}  (3)

Ở đây, T biểu thị ngưỡng độ không chắc chắn; Lstu biểu thị student model loss; KL() và CE() biểu thị KL divergence loss và cross-entropy loss, tương ứng; x và y biểu thị dữ liệu đầu vào và ground truth labels; và uncertainty biểu thị entropy của các đầu ra mô hình học sinh Wstu(x).

Phân tích Overhead Huấn luyện và Suy luận

Trái ngược với quá trình chưng cất hai bước trong KD vanilla, NetDistiller thực hiện chưng cất tại chỗ one-shot kiến thức từ supernet giáo viên cho mô hình học sinh sub-network mà không có bất kỳ chi phí tính toán và bộ nhớ bổ sung nào. Tương tự như NetAug, NetDistiller giới thiệu zero overhead suy luận bổ sung bởi vì chỉ TNN mục tiêu được sử dụng trong quá trình suy luận, cho phép triển khai TNNs khả thi trên các thiết bị edge bị hạn chế tài nguyên. Mặc dù mở rộng mô hình TNN mục tiêu gấp 3×, chúng tôi quan sát thấy chỉ tăng 20% thời gian huấn luyện của NetDistiller so với TNNs vanilla.

KẾT QUẢ THÍ NGHIỆM

Thiết lập Thí nghiệm

Mô hình. Chúng tôi đo điểm chuẩn NetDistiller với các phương pháp huấn luyện TNNs SOTA, ví dụ, NetAug [2], và KD [4], trên năm TNNs được áp dụng phổ biến [3], [2], bao gồm MobileNet-V2-Tiny, MobileNet-V2 (w0.35 và w1.0), MobileNet-V3, MCUNet (256kb-1mb), và ProxylessNAS (w0.35, w1.0). Cụ thể, w0.35 chỉ ra các mô hình có 0.35× số kênh so với mô hình vanilla (tức là w1.0). Theo định nghĩa mô hình trong [2], các kênh của mô hình w0.35 được làm tròn thành tích của 8.

Datasets. Theo [2], chúng tôi xem xét dataset ImageNet với độ phân giải đầu vào r144, r160, và r176 cho các TNN mục tiêu khác nhau. Trong các thí nghiệm chưng cất kiến thức bên ngoài, độ phân giải đầu vào cho giáo viên bên ngoài được đặt thành r224 để phù hợp với cấu hình huấn luyện trước của nó. Các thí nghiệm phát hiện đối tượng được huấn luyện trên datasets PASCAL VOC 2007+2012 và được đánh giá trên tập validation PASCAL VOC 2007 với độ phân giải đầu vào r416.

Metrics Đánh giá. Chúng tôi đánh giá NetDistiller và các phương pháp baseline về độ chính xác top-1 trên ImageNet và average precision tại IoU= 0.5 (AP50) cho phát hiện đối tượng trên PASCAL VOC.

Thiết lập Huấn luyện. Theo thiết lập huấn luyện trong [2], chúng tôi huấn luyện TNNs trong 180 epochs sử dụng optimizer SGD với momentum 0.9 và learning rate ban đầu 0.4 với cosine learning rate scheduler. Chúng tôi áp dụng learning rate warm-up trong 5 epochs và các gradient được cắt thành 1.0 trong suốt quá trình huấn luyện. Kỹ thuật label smoothing với hệ số 0.1 được áp dụng khi sử dụng ground truth label. Đối với NetDistiller với chưng cất nhận thức về độ không chắc chắn, chúng tôi sử dụng cùng một công thức huấn luyện nhưng tăng epochs huấn luyện lên 360. Ngưỡng độ không chắc chắn T được đặt thành 3.75 dựa trên quan sát thực nghiệm của nghiên cứu ablation của chúng tôi. Tất cả các thí nghiệm ImageNet được chạy trên 8 GPUs với batch size 1024. Như một nghiên cứu gần đây [2] đã phát hiện rằng tăng cường dữ liệu và regularization có thể có hại cho việc huấn luyện TNN, chúng tôi chỉ sử dụng các tăng cường dữ liệu tiêu chuẩn (ví dụ random flip, random crop) và tắt các phương pháp regularization như dropout và drop path. Đối với transfer learning trên nhiệm vụ phát hiện đối tượng, các mô hình MobileNet-V2-w0.35 và MobileNet-V3-w0.35 được kết nối với một YOLO-v4 head. Tất cả các thí nghiệm phát hiện đối tượng được huấn luyện thông qua optimizer SGD với momentum 0.9 và learning rate ban đầu 1e-4 được giảm bởi cosine learning rate scheduler trong 100 epochs với batch size 8.

Đo điểm chuẩn với Các Phương pháp Huấn luyện TNN SOTA

Như được hiển thị trong Bảng 1, các quan sát của chúng tôi như sau: (1) NetDistiller cải thiện tới 4.5% độ chính xác so với tất cả baselines và đạt được độ chính xác cao hơn 2.3% so với baseline cạnh tranh nhất NetAug trên MobileNet-V3-w0.35. (2) Chưng cất tại chỗ đóng góp nhiều nhất: TNNs được huấn luyện chỉ thông qua chưng cất tại chỗ có thể phù hợp hoặc vượt qua phương pháp huấn luyện TNNs SOTA, NetAug [2]. (3) Trong tất cả các mô hình, TNNs đạt được tăng cường độ chính xác khoảng 0.5% sau khi giới thiệu PCGrad [10] để giảm thiểu xung đột gradient. (4) NetDistiller đề xuất của chúng tôi, kết hợp chưng cất tại chỗ với PCGrad và chưng cất nhận thức về độ không chắc chắn, được chứng minh là một cách tiếp cận huấn luyện hiệu quả để tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs.

--- TRANG 3 ---
tinyML

trong đó η là learning rate, và:

g(Wn−1) = ∇W[LD(W) + Lstu[W,Wstu];Wn−1]|W=Wn−1  (1)

Ở đây, LD là cross-entropy loss của supernet giáo viên trên dataset huấn luyện D, W và Wstu biểu thị các mô hình giáo viên và học sinh, tương ứng, và Lstu là student loss được điều chỉnh bởi chưng cất nhận thức về độ không chắc chắn (được giới thiệu trong phần Yếu tố kích hoạt 3). Bổ sung, quá trình chưng cất trong NetDistiller là single-shot, tức là nó được triển khai tại chỗ trong quá trình huấn luyện mà không có chi phí tính toán và bộ nhớ bổ sung, không giống như KD vanilla hai bước nơi một mô hình lớn phải được huấn luyện trước.

Yếu tố kích hoạt 2 của NetDistiller: Phẫu thuật Gradient để Giải quyết Xung đột Gradient

Xem xét rằng các gradient từ cả mô hình học sinh và giáo viên tích lũy trên các trọng số chia sẻ, chúng tôi xác định rằng chưng cất tại chỗ vanilla có thể gây ra xung đột gradient nghiêm trọng giữa supernet giáo viên và sub-network học sinh (TNN mục tiêu). Cụ thể, chúng tôi phát hiện rằng tới 50% gradient của mô hình học sinh có độ tương tự cosine âm với những gradient của mô hình giáo viên. Lấy cảm hứng từ PCGrad [10] thực hiện phẫu thuật gradient cho học đa nhiệm vụ, NetDistiller giải quyết điều này bằng cách chiếu các gradient giáo viên xung đột lên mặt phẳng pháp tuyến của gradient học sinh, do đó loại bỏ các thành phần xung đột trong gradient giáo viên và cải thiện hiệu suất của TNNs. Cụ thể, hãy để ∇lstu và ∇ltea biểu thị các gradient của mô hình học sinh và giáo viên, tương ứng. Chúng tôi định nghĩa φ là góc giữa hai gradient trên và g là gradient cuối cùng để cập nhật trọng số. Để đảm bảo việc huấn luyện mô hình học sinh, chúng tôi chiếu gradient xung đột của giáo viên, proj(∇ltea), khi độ tương tự cosine cos(φ) = ∇lstu.∇ltea / (||∇lstu||||∇ltea||) là âm, được công thức hóa như sau:

g = ∇lstu + proj(∇ltea), trong đó
proj(∇ltea) = {
  ∇ltea - (∇lTtea∇lstu / ||∇lstu||²)∇lstu, nếu cos(φ) < 0
  ∇ltea, ngược lại
}  (2)

Yếu tố kích hoạt 3 của NetDistiller: Chưng cất Nhận thức về Độ không chắc chắn

Vì các mô hình học sinh và giáo viên chia sẻ trọng số được huấn luyện chung từ đầu và mô hình giáo viên over-parameterized hội tụ nhanh hơn so với mô hình học sinh, chúng tôi quan sát thấy rằng mô hình giáo viên supernet gặp phải over-fitting ở giai đoạn huấn luyện cuối. Bổ sung, [11] ủng hộ rằng các mô hình lớn có cải thiện lớn nhất trên các mẫu nơi mô hình nhỏ không chắc chắn nhất. Đối với các ví dụ nhất định, thậm chí những ví dụ mà mô hình nhỏ không đặc biệt chính xác, các mô hình lớn thường không thể cải thiện. Dựa trên những hiểu biết này, chúng tôi giả thuyết rằng mô hình giáo viên không phải lúc nào cũng là một giáo viên tốt trong suốt quá trình huấn luyện.

Dựa trên điều này, chúng tôi đề xuất một kỹ thuật gọi là chưng cất nhận thức về độ không chắc chắn (UD) để động chọn các hàm loss học sinh giữa Kullback–Leibler (KL) divergence và cross-entropy losses dựa trên độ chắc chắn của đầu ra mô hình học sinh (xem Hình 1). Cụ thể, chúng tôi đo độ không chắc chắn thông qua entropy của các đầu ra học sinh. Khi entropy của đầu ra học sinh cao (tức là không chắc chắn), học sinh được chưng cất bởi giáo viên chia sẻ trọng số thông qua KL divergence loss, ngược lại, học sinh được huấn luyện bởi ground truth label thông qua cross-entropy loss. Chúng tôi công thức hóa quá trình này như sau:

Lstu = {
  KL(Wstu(x), W(x)), uncertainty ≥ T
  CE(Wstu(x), y), ngược lại
}  (3)

Ở đây, T biểu thị ngưỡng độ không chắc chắn; Lstu biểu thị student model loss; KL() và CE() biểu thị KL divergence loss và cross-entropy loss, tương ứng; x và y biểu thị dữ liệu đầu vào và ground truth labels; và uncertainty biểu thị entropy của các đầu ra mô hình học sinh Wstu(x).

Phân tích Overhead Huấn luyện và Suy luận

Trái ngược với quá trình chưng cất hai bước trong KD vanilla, NetDistiller thực hiện chưng cất tại chỗ one-shot kiến thức từ supernet giáo viên cho mô hình học sinh sub-network mà không có bất kỳ chi phí tính toán và bộ nhớ bổ sung nào. Tương tự như NetAug, NetDistiller giới thiệu zero overhead suy luận bổ sung bởi vì chỉ TNN mục tiêu được sử dụng trong quá trình suy luận, cho phép triển khai TNNs khả thi trên các thiết bị edge bị hạn chế tài nguyên. Mặc dù mở rộng mô hình TNN mục tiêu gấp 3×, chúng tôi quan sát thấy chỉ tăng 20% thời gian huấn luyện của NetDistiller so với TNNs vanilla.

KẾT QUẢ THÍ NGHIỆM

Thiết lập Thí nghiệm

Mô hình. Chúng tôi đo điểm chuẩn NetDistiller với các phương pháp huấn luyện TNNs SOTA, ví dụ, NetAug [2], và KD [4], trên năm TNNs được áp dụng phổ biến [3], [2], bao gồm MobileNet-V2-Tiny, MobileNet-V2 (w0.35 và w1.0), MobileNet-V3, MCUNet (256kb-1mb), và ProxylessNAS (w0.35, w1.0). Cụ thể, w0.35 chỉ ra các mô hình có

--- TRANG 4 ---
tinyML

0.35× số kênh so với mô hình vanilla (tức là w1.0). Theo định nghĩa mô hình trong [2], các kênh của mô hình w0.35 được làm tròn thành tích của 8.

Datasets. Theo [2], chúng tôi xem xét dataset ImageNet với độ phân giải đầu vào r144, r160, và r176 cho các TNN mục tiêu khác nhau. Trong các thí nghiệm chưng cất kiến thức bên ngoài, độ phân giải đầu vào cho giáo viên bên ngoài được đặt thành r224 để phù hợp với cấu hình huấn luyện trước của nó. Các thí nghiệm phát hiện đối tượng được huấn luyện trên datasets PASCAL VOC 2007+2012 và được đánh giá trên tập validation PASCAL VOC 2007 với độ phân giải đầu vào r416.

Metrics Đánh giá. Chúng tôi đánh giá NetDistiller và các phương pháp baseline về độ chính xác top-1 trên ImageNet và average precision tại IoU= 0.5 (AP50) cho phát hiện đối tượng trên PASCAL VOC.

Thiết lập Huấn luyện. Theo thiết lập huấn luyện trong [2], chúng tôi huấn luyện TNNs trong 180 epochs sử dụng optimizer SGD với momentum 0.9 và learning rate ban đầu 0.4 với cosine learning rate scheduler. Chúng tôi áp dụng learning rate warm-up trong 5 epochs và các gradient được cắt thành 1.0 trong suốt quá trình huấn luyện. Kỹ thuật label smoothing với hệ số 0.1 được áp dụng khi sử dụng ground truth label. Đối với NetDistiller với chưng cất nhận thức về độ không chắc chắn, chúng tôi sử dụng cùng một công thức huấn luyện nhưng tăng epochs huấn luyện lên 360. Ngưỡng độ không chắc chắn T được đặt thành 3.75 dựa trên quan sát thực nghiệm của nghiên cứu ablation của chúng tôi. Tất cả các thí nghiệm ImageNet được chạy trên 8 GPUs với batch size 1024. Như một nghiên cứu gần đây [2] đã phát hiện

BẢNG 3. Nghiên cứu ablation về phẫu thuật gradient trên các mô hình MobileNet-V2-w0.35 và MobileNet-V3-w0.35 trong 360 epochs. Chúng tôi tắt phẫu thuật gradient và tính toán độ tương tự cosine giữa hai gradient (của giáo viên và học sinh) của mỗi lớp convolutional. Các giá trị phần trăm được hiển thị dưới các epochs khác nhau phản ánh tỷ lệ trung bình của số lớp có độ tương tự cosine âm (xung đột gradient) so với tổng số lớp trong mô hình.

Epoch 1 90 180 270 360
MobileNet-V2-w0.35 51.5% 40.1% 37.4% 39.4% 38.2%
MobileNet-V3-w0.35 50.1% 45.2% 34.7% 38.5% 37.4%

BẢNG 4. Nghiên cứu ablation về các ngưỡng chưng cất nhận thức về độ không chắc chắn khác nhau trên MobileNet-V2-w0.35 và MobileNet-V2-w1.0. Hàng đầu tiên là các ngưỡng. Chưng cất nhận thức về độ không chắc chắn chưng cất mô hình học sinh nếu entropy đầu ra của nó (độ không chắc chắn) cao hơn ngưỡng và huấn luyện mô hình học sinh với ground truth labels ngược lại.

Model Ngưỡng Độ không chắc chắn
2.5 3.75 5.0
MobileNet-V2-w0.35 59.1% 59.3% 58.9%
MobileNet-V2-w1.0 71.9% 72.6% 71.2%

rằng tăng cường dữ liệu và regularization có thể có hại cho việc huấn luyện TNN, chúng tôi chỉ sử dụng các tăng cường dữ liệu tiêu chuẩn (ví dụ random flip, random crop) và tắt các phương pháp regularization như dropout và drop path. Đối với transfer learning trên nhiệm vụ phát hiện đối tượng, các mô hình MobileNet-V2-w0.35 và MobileNet-V3-w0.35 được kết nối với một YOLO-v4 head. Tất cả các thí nghiệm phát hiện đối tượng được huấn luyện thông qua optimizer SGD với momentum 0.9 và learning rate ban đầu 1e-4 được giảm bởi cosine learning rate scheduler trong 100 epochs với batch size 8.

Đo điểm chuẩn với Các Phương pháp Huấn luyện TNN SOTA

Như được hiển thị trong Bảng 1, các quan sát của chúng tôi như sau: (1) NetDistiller cải thiện tới 4.5% độ chính xác so với tất cả baselines và đạt được độ chính xác cao hơn 2.3% so với baseline cạnh tranh nhất NetAug trên MobileNet-V3-

--- TRANG 5 ---
tinyML

BẢNG 1. Đo điểm chuẩn NetDistiller với các phương pháp SOTA để huấn luyện TNNs. r160: Độ phân giải hình ảnh đầu vào là 160×160. w0.35: Mô hình có 0.35× số kênh so với mô hình vanilla.

Model MobileNet-V2-Tiny MCUNet MobileNet-V3, r160 ProxylessNAS, r160 MobileNet-V2, r160
r144 r176 w0.35 w0.35 w1.0 w0.35 w1.0
Params 0.75M 0.74M 2.2M 1.8M 4.1M 1.7M 3.5M
MACs 23.5M 81.8M 19.6M 35.7M 164.1M 30.9M 154.1M
Baseline 51.7% 61.5% 58.1% 59.1% 71.2% 56.3% 69.7%
NetAug [2] 53.3% 62.7% 60.3% 60.8% 71.9% 57.8% 70.6%
In-situ 54.1% 62.7% 62.1% 60.7% 71.2% 58.5% 71.2%
In-situ + PCGrad [10] 54.5% 63.4% 62.3% 61.3% 72.5% 59.0% 72.0%
NetDistiller (ours) 54.8% 64.2% 62.6% 61.5% 72.8% 59.3% 72.6%

BẢNG 2. Nghiên cứu ablation về tỷ lệ mở rộng kênh trên MobileNet-V2-w0.35 (MBV2-w0.35) và MobileNet-V3-w0.35 (MBV3-w0.35). Các kích thước giáo viên khác nhau trong hàng đầu tiên chỉ ra tỷ lệ mở rộng kênh. Xem xét cải thiện hạn chế (0.2%) giữa giáo viên ×4 và ×3 trên mô hình MobileNet-V2-w0.35 và hiệu quả huấn luyện, giáo viên với kích thước ×3 được chọn trong NetDistiller.

Teacher Size Baseline ×2 ×3 ×4 ×5
MobileNet-V2-0.35 56.3% 58.0% 58.5% 58.7% 58.3%
MobileNet-V3-0.35 58.1% 61.3% 62.1% 61.8% 61.8%

w0.35. (2) Chưng cất tại chỗ đóng góp nhiều nhất: TNNs được huấn luyện chỉ thông qua chưng cất tại chỗ có thể phù hợp hoặc vượt qua phương pháp huấn luyện TNNs SOTA, NetAug [2]. (3) Trong tất cả các mô hình, TNNs đạt được tăng cường độ chính xác khoảng 0.5% sau khi giới thiệu PCGrad [10] để giảm thiểu xung đột gradient. (4) NetDistiller đề xuất của chúng tôi, kết hợp chưng cất tại chỗ với PCGrad và chưng cất nhận thức về độ không chắc chắn, được chứng minh là một cách tiếp cận huấn luyện hiệu quả để tăng cường hiệu suất nhiệm vụ có thể đạt được của TNNs.

Nghiên cứu Ablation của NetDistiller

Tỷ lệ Mở rộng Kênh của Mô hình Giáo viên. Vì NetDistiller mở rộng các kênh của TNNs để tạo ra một supernet chia sẻ trọng số như mô hình giáo viên, tỷ lệ mở rộng kênh của mô hình giáo viên đóng vai trò quan trọng trong việc xác định hiệu quả của cơ chế chưng cất tại chỗ. Điều này đặc biệt quan trọng xem xét rằng một mô hình giáo viên mỏng có thể có khả năng hạn chế, trong khi một giáo viên quá rộng có thể không truyền đạt thông tin của nó một cách hiệu quả. Để xác định tỷ lệ mở rộng kênh tối ưu, chúng tôi đánh giá NetDistiller với tỷ lệ mở rộng kênh ×2, ×3, ×4, và ×5 trên hai TNNs, MobileNet-V2-w0.35 và MobileNet-V3-w0.35, tương ứng. Như được hiển thị trong Bảng 2, chúng tôi quan sát thấy: (1) tất cả bốn mô hình giáo viên tăng cường độ chính xác của TNNs, chứng minh hiệu quả tổng thể của NetDistiller, và (2) MobileNet-V2-w0.35 và MobileNet-V3-w0.35 đạt được độ chính xác cao nhất khi tỷ lệ mở rộng kênh lần lượt là ×4 và ×3. Để giảm thiểu overhead huấn luyện gây ra bởi mô hình giáo viên được mở rộng, chúng tôi mặc định đặt tỷ lệ mở rộng kênh thành ×3 trong NetDistiller.

Định lượng Xung đột Gradient. Do cơ chế chia sẻ trọng số, việc huấn luyện chung của mô hình học sinh và giáo viên dẫn đến việc tích lũy gradient trên cùng các trọng số, không thể tránh khỏi dẫn đến xung đột gradient. Để xác minh sự xuất hiện của xung đột gradient ở các giai đoạn huấn luyện khác nhau, chúng tôi tính toán tỷ lệ các lớp có độ tương tự cosine âm, được tính trung bình trên tập validation, so với tổng số lớp trong mô hình trong suốt quá trình huấn luyện. Kết quả trong Bảng 3 chứng minh rằng các mô hình giáo viên và học sinh thực sự trải qua xung đột gradient, với tới 51.5% các lớp thể hiện gradient xung đột. Để giải quyết vấn đề này, việc áp dụng phẫu thuật gradient dẫn đến cải thiện độ chính xác 0.5% và 0.2% cho MobileNet-V2-w0.35 và MobileNet-V3-w0.35, tương ứng, như được hiển thị trong Bảng 1.

Ngưỡng của Chưng cất Nhận thức về Độ không chắc chắn. Cơ chế chưng cất nhận thức về độ không chắc chắn đề xuất của chúng tôi động chọn mục tiêu cho mô hình học sinh, lựa chọn giữa chưng cất tại chỗ hoặc cross-entropy dựa trên độ không chắc chắn của các đầu ra mô hình học sinh. Để quyết định ngưỡng độ không chắc chắn, chúng tôi xác thực các mô hình MobileNet-V2-w0.35 và MobileNet-V2-w1.0 với ngưỡng độ không chắc chắn 5.0, 3.75, và 2.5, xem xét entropy của các mô hình ImageNet nằm trong khoảng [1.5, 10] khi áp dụng hệ số label smoothing 0.1. Như được hiển thị trong Bảng 4, chúng tôi quan sát thấy cả hai mô hình đạt được độ chính xác cao nhất khi ngưỡng độ không chắc chắn được đặt thành 3.75, dẫn đến cải thiện độ chính xác 0.3% và 0.6%, tương ứng, so với NetDistiller không có chưng cất nhận thức về độ không chắc chắn. Do đó, chúng tôi áp dụng ngưỡng độ không chắc chắn mặc định 3.75 khi kích hoạt chưng cất nhận thức về độ không chắc chắn, mà không có overhead bổ sung nào.

Chưng cất Kiến thức Bổ sung. Một baseline tự nhiên để đánh giá NetDistiller là chưng cất kiến thức tiêu chuẩn. Các quan sát gần đây đã đề xuất rằng khoảng cách đáng kể giữa các mô hình giáo viên và học sinh có thể dẫn đến hiệu suất chưng cất kiến thức dưới tối ưu [12]. Do đó, chúng tôi giả thuyết rằng (a) chưng cất tại chỗ đề xuất của chúng tôi phục vụ như một cơ chế hiệu quả hơn để cho phép huấn luyện TNN khi so sánh với chưng cất kiến thức, và (b) chưng cất kiến thức bổ sung cho phương pháp của chúng tôi và có thể được áp dụng đồng thời. Để xác thực các giả thuyết này, chúng tôi chưng cất kiến thức từ một ResNet-50 được huấn luyện trước ImageNet cho cả mô hình giáo viên và học sinh của NetDistiller. Quá trình này được gọi là chưng cất bên ngoài để phân biệt nó với chưng cất tại chỗ của chúng tôi. Sau đó chúng tôi đo điểm chuẩn cách tiếp cận này so với (1) NetDistiller vanilla và (2) chưng cất kiến thức tiêu chuẩn. Như được hiển thị trong

--- TRANG 6 ---
tinyML

BẢNG 5. Nghiên cứu ablation về việc kết hợp NetDistiller và chưng cất kiến thức bên ngoài. KD: Chưng cất TNN mục tiêu với một giáo viên bên ngoài (ResNet-50 được huấn luyện trước ImageNet). NetDistiller w/o UD: Chưng cất nhận thức về độ không chắc chắn được tắt trong các thí nghiệm KD bên ngoài. NetDistiller+KD: Giáo viên bên ngoài chưng cất cả mô hình giáo viên và học sinh của NetDistiller.

Model Baseline KD NetDistiller w/o UD NetDistiller+KD
MobileNet-V2-Tiny, r144 51.7% 53.7% (+2.0%) 55.5% (+3.8%) 56.1% (+4.4%)
MobileNet-V2-w0.35, r160 56.3% 58.4% (+2.1%) 59.0% (+2.7%) 59.5% (+3.2%)
MobileNet-V3-w0.35, r160 58.1% 61.6% (+3.5%) 62.3% (+4.2%) 62.5% (+4.4%)
ProxylessNAS-w0.35, r160 59.1% 60.8% (+1.7%) 61.3% (+2.2%) 61.9% (+2.8%)

Bảng 5, chúng tôi có thể quan sát thấy (1) NetDistiller vanilla có thể vượt trội hơn chưng cất kiến thức tiêu chuẩn, ví dụ, cải thiện độ chính xác 3.8% và 4.2% trên MobileNet-V2-Tiny và MobileNet-V3-w0.35 tương ứng, do đó xác nhận giả thuyết (a) của chúng tôi; và (2) chưng cất kiến thức vuông góc với NetDistiller, vì việc áp dụng chưng cất kiến thức trên NetDistiller dẫn đến cải thiện độ chính xác 4.4% trên MobileNet-V2-Tiny và MobileNet-V3-w0.35, tương ứng, xác thực giả thuyết (b) của chúng tôi.

Trực quan hóa Quỹ đạo Huấn luyện

Chúng tôi trình bày các đường cong huấn luyện cho MobileNet-V2-w0.35 và MobileNet-V3-w0.35 trong suốt 180 epochs huấn luyện sử dụng NetDistiller, so với các baseline huấn luyện tiêu chuẩn tương ứng, trong Hình 2. Đáng chú ý, chúng tôi quan sát thấy cả độ chính xác huấn luyện và đánh giá của NetDistiller luôn vượt trội hơn các baseline tương ứng ở cùng epoch huấn luyện. Ví dụ, NetDistiller đạt được cải thiện độ chính xác 2.7% cho MobileNet-V2-w0.35 và cải thiện độ chính xác ấn tượng 4.2% cho MobileNet-V3-w0.35 so với các baseline. Hơn nữa, để đạt được độ chính xác tương đương, NetDistiller yêu cầu ít epochs huấn luyện hơn, dẫn đến giảm 44% epochs huấn luyện, như được chỉ ra trong Hình 2.

Nghiên cứu Transfer Learning trên Phát hiện Đối tượng

Để đánh giá tính tổng quát của các biểu diễn được học bởi NetDistiller, chúng tôi chuyển các MobileNet-V2-w0.35 và MobileNet-V3-w0.35 được huấn luyện bởi NetDistiller sang một nhiệm vụ phát hiện đối tượng downstream. Sau đó chúng tôi so sánh hiệu suất của chúng với các mô hình được huấn luyện trước tiêu chuẩn, có hoặc không có chưng cất kiến thức, trên ImageNet. Cụ thể, chúng tôi thay thế các lớp pooling và linear cuối cùng trong MobileNet-V2-w0.35 và MobileNet-V3-w0.35 bằng head phát hiện đối tượng YOLO-v4. Như được hiển thị trong Bảng 6, NetDistiller luôn có khả năng chuyển giao tốt hơn với Average Precision (AP) cao hơn

BẢNG 6. Transfer learning trên các nhiệm vụ phát hiện đối tượng sử dụng datasets PASCAL VOC 2007+2012 và các mô hình MobileNet-V2-w0.35 (MBV2) hoặc MobileNet-V3-w0.35 (MBV3). Cả hai mô hình đều được kết nối với head phát hiện YOLO-v4. Mô hình được khởi tạo với các trọng số được huấn luyện trước NetDistiller trên ImageNet. Chúng tôi báo cáo Average Precision tại IoU= 0.5 (AP50).

Model Baseline AP50 KD AP50 NetDistiller AP50
MobileNet-v2-w0.35 60.4% 61.1% 62.3%
MobileNet-v3-w0.35 63.6% 62.8% 65.2%

1.9%/1.6% trên MobileNet-V2-w0.35/MobileNet-V3-w0.35 so với các baseline huấn luyện tiêu chuẩn. Lưu ý rằng mặc dù các tính năng được huấn luyện trước trên các nhiệm vụ phân loại có thể không nhất thiết hữu ích cho các nhiệm vụ downstream [2], điều này cũng được phản ánh với kết quả của MobileNet-V3-w0.35 được huấn luyện trước KD của chúng tôi trong Bảng 6, NetDistiller quản lý để cải thiện AP có thể đạt được lên tới 1.9% trên cả hai mô hình. Điều này nhấn mạnh khả năng ứng dụng rộng rãi của phương pháp chúng tôi trên các nhiệm vụ và datasets đa dạng.

KẾT LUẬN

Tăng cường độ chính xác nhiệm vụ của TNNs là một thách thức quan trọng trong việc cho phép triển khai chúng trên các thiết bị IoT bị hạn chế tài nguyên. Framework đề xuất của chúng tôi, NetDistiller, giải quyết thách thức này bằng cách xem xét TNNs như các sub-networks của một mô hình giáo viên chia sẻ trọng số, đạt được bằng cách mở rộng số lượng kênh trong TNN. Bằng cách kết hợp phẫu thuật gradient để xử lý xung đột gradient và chưng cất nhận thức về độ không chắc chắn để giảm thiểu overfitting mô hình giáo viên, NetDistiller cải thiện đáng kể độ chính xác có thể đạt được của TNNs. Các thí nghiệm rộng rãi trên các nhiệm vụ khác nhau chứng minh hiệu quả vượt trội của NetDistiller so với các lược đồ huấn luyện TNN SOTA. Tiến bộ này đánh dấu một bước quan trọng hướng tới việc hiện thực hóa toàn bộ tiềm năng của TNNs trong các ứng dụng IoT thực tế.

--- TRANG 7 ---
tinyML

-30% epochs+2.7% Acc
-44% epochs+4.2% Acc

Epoch Epoch Epoch Epoch Top1 Acc  
Top1 Acc  
Top1 Acc  
Top1 Acc  MobileNet-V2-w0.35 Train MobileNet-V2-w0.35 Eval  MobileNet-V3-w0.35 Train MobileNet-V3-w0.35 Eval  

HÌNH 2. Trực quan hóa quá trình huấn luyện của NetDistiller và các baseline cho các mô hình MobileNet-V2-w0.35 và MobileNet-V3-w0.35. Nó tiết lộ rằng NetDistiller tăng cường đáng kể cả độ chính xác huấn luyện và đánh giá của TNNs. Điều này làm nổi bật tiềm năng đáng kể của NetDistiller trong việc tăng cường TNNs và giảm thiểu vấn đề underfitting.

1.9%/1.6% trên MobileNet-V2-w0.35/MobileNet-V3-w0.35 so với các baseline huấn luyện tiêu chuẩn. Lưu ý rằng mặc dù các tính năng được huấn luyện trước trên các nhiệm vụ phân loại có thể không nhất thiết hữu ích cho các nhiệm vụ downstream [2], điều này cũng được phản ánh với kết quả của MobileNet-V3-w0.35 được huấn luyện trước KD của chúng tôi trong Bảng 6, NetDistiller quản lý để cải thiện AP có thể đạt được lên tới 1.9% trên cả hai mô hình. Điều này nhấn mạnh khả năng ứng dụng rộng rãi của phương pháp chúng tôi trên các nhiệm vụ và datasets đa dạng.

KẾT LUẬN

Tăng cường độ chính xác nhiệm vụ của TNNs là một thách thức quan trọng trong việc cho phép triển khai chúng trên các thiết bị IoT bị hạn chế tài nguyên. Framework đề xuất của chúng tôi, NetDistiller, giải quyết thách thức này bằng cách xem xét TNNs như các sub-networks của một mô hình giáo viên chia sẻ trọng số, đạt được bằng cách mở rộng số lượng kênh trong TNN. Bằng cách kết hợp phẫu thuật gradient để xử lý xung đột gradient và chưng cất nhận thức về độ không chắc chắn để giảm thiểu overfitting mô hình giáo viên, NetDistiller cải thiện đáng kể độ chính xác có thể đạt được của TNNs. Các thí nghiệm rộng rãi trên các nhiệm vụ khác nhau chứng minh hiệu quả vượt trội của NetDistiller so với các lược đồ huấn luyện TNN SOTA. Tiến bộ này đánh dấu một bước quan trọng hướng tới việc hiện thực hóa toàn bộ tiềm năng của TNNs trong các ứng dụng IoT thực tế.

--- TRANG 8 ---
tinyML

Lời cảm ơn

Công trình được hỗ trợ bởi National Science Foundation (NSF) thông qua chương trình CCF (Số giải thưởng: 2211815) và bởi CoCoSys, một trong bảy trung tâm trong JUMP 2.0, một chương trình Semiconductor Research Corporation (SRC) được tài trợ bởi DARPA.

TÀI LIỆU THAM KHẢO

1. Lionel Sujay Vailshery. Iot and non-iot connections worldwide 2010-2025. https://www.statista.com/statistics/1101442/iot-number-of-connected-devices-worldwide/. Truy cập: 2021-03-08.

2. Han Cai, Chuang Gan, Ji Lin, and song han. Network augmentation for tiny deep learning. In International Conference on Learning Representations, 2022.

3. Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. Mcunet: Tiny deep learning on iot devices. Advances in Neural Information Processing Systems, 33:11711–11722, 2020.

4. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2(7), 2015.

5. Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.

6. Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105–6114. PMLR, 2019.

7. Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, and Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In Proceedings of the IEEE/CVF Conference on CVPR, pages 10734–10742, 2019.

8. Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in NIPS, volume 27. Curran Associates, Inc., 2014.

9. Dilin Wang, Chengyue Gong, Meng Li, Qiang Liu, and Vikas Chandra. Alphanet: Improved training of supernets with alpha-divergence. In International Conference on Machine Learning, pages 10760–10771. PMLR, 2021.

10. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. Advances in NIPS, 33:5824–5836, 2020.

11. Taman Narayan, Heinrich Jiang, Sen Zhao, and Sanjiv Kumar. Predicting on the edge: Identifying where a larger model does better. arXiv preprint arXiv:2202.07652, 2022.

12. Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedings of the IEEE/CVF ICCV, pages 4794–4802, 2019.

Shunyao Zhang là một nghiên cứu sinh tiến sĩ tại Đại học Rice, Houston, USA. Anh ấy nhận được bằng thạc sĩ về Kỹ thuật Điện và Máy tính từ Đại học Carnegie Mellon, Pittsburgh, USA. Sở thích nghiên cứu của anh ấy là tiny ML và adversarial robustness. Liên hệ anh ấy tại sz74@rice.edu.

Yonggan Fu là một nghiên cứu sinh tiến sĩ tại Viện Công nghệ Georgia. Trước đó, anh ấy có được bằng Cử nhân từ Trường The Gifted Young tại Đại học Khoa học và Công nghệ Trung Quốc. Trọng tâm nghiên cứu và đam mê của anh ấy là phát triển các thuật toán AI hiệu quả và mạnh mẽ và đồng thiết kế các bộ tăng tốc phần cứng tương ứng hướng tới chiến thắng ba bên về độ chính xác, hiệu quả và tính mạnh mẽ. Liên hệ anh ấy tại yfu314@gatech.edu.

Shang Wu là một sinh viên thạc sĩ tại Đại học RICE nơi anh ấy chuyên ngành Kỹ thuật Điện và Máy tính. Anh ấy nhận được bằng cử nhân về khoa học máy tính tại Đại học George Washington. Sở thích nghiên cứu của anh ấy là trong ML hiệu quả, ML mạnh mẽ và AI tạo sinh. Liên hệ anh ấy tại sw99@rice.edu.

Jyotikrishna Dass là một Nhà Khoa học Nghiên cứu tại Đại học Rice và quản lý các hoạt động tại Chương trình Rice Data to Knowledge. Sở thích nghiên cứu của anh ấy là trong các hệ thống machine-learning phân tán và song song cho điện toán edge hiệu quả. Trước đây, anh ấy là một postdoc tại phòng thí nghiệm của Dr. Yingyan Lin. Dr. Dass nhận được Ph.D. về Kỹ thuật Máy tính từ Đại học Texas A&M. Liên hệ: jdass@rice.edu.

Haoran You hiện là một nghiên cứu sinh tiến sĩ trong Khoa CS của Viện Công nghệ Georgia. Anh ấy nhận được bằng cử nhân trong lớp nâng cao tại Đại học Khoa học và Công nghệ Huazhong và bằng thạc sĩ tại Đại học Rice. Anh ấy đang theo đuổi bằng tiến sĩ trong lĩnh vực machine learning và kiến trúc máy tính. Sở thích nghiên cứu của anh ấy bao gồm nhưng không giới hạn ở machine learning bị hạn chế tài nguyên, computer vision, deep learning, và đồng thiết kế thuật toán/bộ tăng tốc. Liên hệ: haoran.you@gatech.edu.

--- TRANG 9 ---
tinyML

Yingyan (Celine) Lin hiện là Phó Giáo sư trong Trường Khoa học Máy tính và là thành viên của Trung tâm Machine Learning tại Viện Công nghệ Georgia. Cô ấy lãnh đạo Phòng thí nghiệm Điện toán Hiệu quả và Thông minh (EIC). Nghiên cứu của cô ấy tập trung vào phát triển các kỹ thuật machine learning hiệu quả thông qua đổi mới đa lớp, trải dài từ các thuật toán trí tuệ nhân tạo (AI) hiệu quả đến bộ tăng tốc phần cứng AI và thiết kế chip AI, và nhằm mục đích thúc đẩy AI xanh và trí thông minh được cung cấp bởi AI phổ biến. Cô ấy nhận được bằng Ph.D. về Kỹ thuật Điện và Máy tính từ Đại học Illinois tại Urbana-Champaign vào năm 2017. Giáo sư Lin đã nhận được Giải thưởng NSF CAREER, Giải thưởng IBM Faculty, Giải thưởng Facebook Faculty Research, và Giải thưởng ACM SIGDA Outstanding Young Faculty. Cô ấy đã phục vụ trong các Ủy ban Chương trình Kỹ thuật cho các hội nghị khác nhau bao gồm DAC, ICCAD, MLSys, MICRO, và NeurIPS. Cô ấy hiện là Biên tập viên Phụ cho IEEE Transactions on Circuits and Systems II: Express Briefs. Liên hệ cô ấy tại celine.lin@gatech.edu.

Tháng 11 năm 2023 NetDistiller: Trao quyền cho Deep Learning nhỏ thông qua Chưng cất tại chỗ 9
