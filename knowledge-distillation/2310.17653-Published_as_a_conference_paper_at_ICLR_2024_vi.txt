# 2310.17653.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.17653.pdf
# Kích thước tệp: 1640328 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
NHỮNG LỢI ÍCH TUYỆT VỜI VÀ NƠI TÌM THẤY CHÚNG: VỀ
SỰ TỒN TẠI VÀ TRIỂN VỌNG CỦA VIỆC CHUYỂN GIAO KIẾN THỨC
TỔNG QUÁT GIỮA BẤT KỲ MÔ HÌNH ĐÃ ĐƯỢC HUẤN LUYỆN TRƯỚC NÀO
Karsten Roth1,∗, Lukas Thede1,∗, A. Sophia Koepke1
Oriol Vinyals2, Olivier Hénaff2, Zeynep Akata1,3,4
1Tübingen AI Center & University of Tübingen,2Google DeepMind
3Helmholtz Munich,4Technical University of Munich
∗đóng góp ngang nhau
TÓM TẮT
Huấn luyện mạng sâu đòi hỏi nhiều quyết định thiết kế khác nhau liên quan đến ví dụ như kiến trúc của chúng, tăng cường dữ liệu, hoặc tối ưu hóa. Trong công trình này, chúng tôi thấy rằng những biến đổi trong huấn luyện này dẫn đến việc các mạng học được những tập hợp đặc trưng độc đáo từ dữ liệu. Sử dụng các thư viện mô hình công cộng bao gồm hàng nghìn mô hình được huấn luyện trên các tập dữ liệu chuẩn như ImageNet, chúng tôi quan sát thấy rằng đối với các cặp mô hình đã được huấn luyện trước tùy ý, một mô hình trích xuất bối cảnh dữ liệu quan trọng không có sẵn trong mô hình khác - độc lập với hiệu suất tổng thể. Với bất kỳ cặp mô hình đã được huấn luyện trước tùy ý nào và không có xếp hạng bên ngoài (như các tập kiểm tra riêng biệt, ví dụ do bảo mật dữ liệu), chúng tôi điều tra liệu có thể chuyển giao kiến thức "bổ sung" như vậy từ một mô hình sang mô hình khác mà không làm giảm hiệu suất - một nhiệm vụ đặc biệt khó khăn vì kiến thức bổ sung có thể được chứa trong các mô hình mạnh hơn, tương đương hiệu suất hoặc yếu hơn. Tuy nhiên, việc tạo điều kiện cho việc chuyển giao mạnh mẽ trong các kịch bản bất khả tri với các cặp mô hình đã được huấn luyện trước sẽ mở khóa hướng dẫn huấn luyện, lợi ích phụ trợ và hợp nhất kiến thức từ bất kỳ kho mô hình nào mà không có hạn chế về mô hình & các đặc điểm cụ thể của vấn đề - bao gồm cả từ các mô hình yếu hơn, hiệu suất thấp hơn. Công trình này cung cấp một khám phá đầu tiên, sâu sắc về tính khả thi của việc chuyển giao kiến thức đa mục đích như vậy. Qua các thí nghiệm quy mô lớn, chúng tôi đầu tiên tiết lộ những thiếu sót của các kỹ thuật chưng cất kiến thức tiêu chuẩn, và sau đó đề xuất một mở rộng tổng quát thông qua phân vùng dữ liệu để chuyển giao thành công giữa gần như tất cả các mô hình đã được huấn luyện trước - điều này cũng có thể được thực hiện không có giám sát. Cuối cùng, chúng tôi đánh giá cả khả năng mở rộng và tác động của các thuộc tính mô hình đối với việc chuyển giao kiến thức bất khả tri mô hình thành công.

1 GIỚI THIỆU
Huấn luyện mạng nơ-ron trên các tập dữ liệu cụ thể đã trở thành một tiêu chuẩn học máy để giải quyết vô số thách thức nghiên cứu và công nghiệp, liên quan đến một số lượng lớn các quyết định rõ ràng và ngầm ẩn từ lựa chọn kiến trúc đến các giao thức tối ưu hóa cụ thể, sự lựa chọn cụ thể của tăng cường dữ liệu, lấy mẫu dữ liệu và thậm chí cả thứ tự dữ liệu. Tất cả những yếu tố này có thể tác động đến kiến thức ngữ nghĩa mà một mô hình có thể trích xuất từ một tập dữ liệu (Bouthillier et al., 2021; Schmidt et al., 2021; Wang et al., 2023; Raghu et al., 2021; Wagner et al., 2022; Roth et al., 2020; Balestriero et al., 2023; Teney et al., 2020; Roth et al., 2023), và cùng nhau cung cấp một dấu vân tay độc đáo về khả năng của một mô hình. Trong công trình này, chúng tôi đầu tiên làm nổi bật mức độ của tuyên bố này thông qua các thí nghiệm mở rộng. Chúng tôi xây dựng trên các thư viện mô hình mở lớn (ví dụ: timm (Wightman, 2019) hoặc huggingface) để so sánh số lượng lớn các cặp mô hình đã được huấn luyện trước tùy ý. Làm như vậy, chúng tôi khám phá sự tồn tại nhất quán của kiến thức bổ sung đáng kể - thông tin về dữ liệu mà một mô hình ("giáo viên") nắm giữ mà không có sẵn trong mô hình khác ("học sinh"). Thú vị là, chúng tôi thấy rằng kiến thức bổ sung tồn tại bất kể xếp hạng hiệu suất bên ngoài hoặc các yếu tố như họ mô hình (CNNs (LeCun and Bengio, 1995), Transformer (Dosovitskiy et al., 2021), MLP (Tolstikhin et al., 2021)), và thường tập hợp trong các lĩnh vực chuyên môn ngữ nghĩa: Đối với các giáo viên mạnh hơn, nhưng đặc biệt cũng tương tự hoặc yếu hơn (theo một số chỉ số kiểm tra), có thể tìm thấy kiến thức đáng kể về dữ liệu không có sẵn cho học sinh.

Sự có sẵn tổng quát của kiến thức bổ sung như vậy đặt ra câu hỏi về tiện ích tiềm năng của nó. Để trả lời những câu hỏi đó, chúng tôi cung cấp một khám phá đầu tiên, sâu sắc. Cụ thể, với các cặp mô hình tùy ý được huấn luyện trước trên cùng dữ liệu mà không có quyền truy cập vào các biện pháp xếp hạng bên ngoài (như tập kiểm tra, do ví dụ bảo mật dữ liệu, máy chủ kiểm tra riêng biệt, ...), chúng tôi khám phá liệu việc chuyển giao kiến thức bổ sung giữa bất kỳ giáo viên và học sinh nào có thể thực hiện được mà không làm giảm hiệu suất. Việc đạt được việc chuyển giao như vậy thông qua bất kỳ cặp mô hình nào có thể mở khóa bất kỳ bộ sưu tập mô hình có sẵn miễn phí hoặc tự tạo nào như một tài nguyên phụ trợ để đạt được lợi ích trong việc huấn luyện trước chuẩn và cụ thể cho vấn đề. Nó cũng tránh được nhu cầu chuyển giao cụ thể cho mô hình đòi hỏi kiến thức chuyên môn, và giảm sự phụ thuộc vào các biện pháp đánh giá bên ngoài để lựa chọn mô hình. Quan trọng hơn, nó cũng cho phép cải thiện các mô hình lớn hơn bằng việc chuyển giao kiến thức từ các mô hình yếu hơn, tài nguyên thấp hơn, mà không cần dữ liệu & giám sát bổ sung một cách rõ ràng, hoặc hy sinh trong ví dụ tốc độ, công bằng, khả năng diễn giải hoặc dễ sử dụng.

Chúng tôi điều tra giới hạn của chưng cất kiến thức (Hinton et al., 2015; Tian et al., 2020) cho nhiệm vụ này, điều mà trái ngược với các phương pháp không có dữ liệu (ví dụ Wortsman et al. (2022a)), hoạt động độc lập với các lựa chọn mô hình. Tuy nhiên, các khung chưng cất kiến thức tiêu chuẩn giả định thông tin được chưng cất cho một học sinh chưa được huấn luyện. Ngược lại, chúng tôi chỉ muốn chuyển giao kiến thức không có sẵn trong một mô hình học sinh đã được huấn luyện, thậm chí có thể vượt trội so với giáo viên của nó. Điều này đòi hỏi một cách quan trọng một sự đánh đổi thành công giữa tăng kiến thức và duy trì. Thực vậy, đối với việc chuyển giao kiến thức giữa các mô hình đã được huấn luyện trước tùy ý, chưng cất thông thường (§5.1) thể hiện sự phụ thuộc mạnh vào mô hình/siêu tham số và giảm hiệu suất cho phần lớn các mô hình học sinh, đặc biệt là đối với các giáo viên yếu hơn/tương đương hiệu suất. Điều này có thể được quy cho việc quên thảm khốc (Kirkpatrick et al., 2016; Zenke et al., 2017) vượt qua lợi ích của việc chuyển giao kiến thức bổ sung từ giáo viên.

Để có một sự đánh đổi thuận lợi giữa quên và tăng kiến thức, chúng tôi coi quá trình chuyển giao như một vấn đề học liên tục, nơi một mô hình được trình bày liên tục với bối cảnh mới cho dữ liệu đã được nhìn thấy. Để khuyến khích duy trì, chúng tôi đầu tiên nghiên cứu nội suy trọng số (Stojanovski et al., 2022; Wortsman et al., 2022b). Mặc dù tốt hơn so với chưng cất bình thường, nó thường là một ràng buộc quá mạnh khi các giáo viên có các lĩnh vực chuyên môn thích hợp hoặc tổng thể mạnh hơn. Do đó chúng tôi đề xuất ràng buộc chưng cất ở cấp độ dữ liệu bằng cách phân vùng nó thành hai tập - một với các mẫu nơi việc chuyển giao từ một giáo viên được mong muốn, và một nơi chúng tôi muốn duy trì hành vi của học sinh. Điều này giới thiệu ít ràng buộc hơn đáng kể trên các trọng số mô hình để học từ bối cảnh giáo viên tùy ý, trong khi giảm quên bằng cách duy trì hiệu suất ban đầu trên các mẫu nơi giáo viên có tác động tích cực hạn chế (thậm chí có hại). Hơn nữa, phân vùng dữ liệu của chúng tôi có thể được thực hiện mà không có bất kỳ giám sát nào.

Làm như vậy, chúng tôi thấy tăng đáng kể trong tỷ lệ thành công (lợi ích khác không của học sinh) cho tất cả các cặp giáo viên-học sinh - từ 32.5% với chưng cất bình thường lên 92.5% với phân vùng dữ liệu. Điều hòa cấp độ dữ liệu của chúng tôi là cài đặt duy nhất cho phép chuyển giao tích cực nhất quán từ các giáo viên yếu hơn, trong khi duy trì hiệu suất chuyển giao của chưng cất bình thường đối với các giáo viên mạnh hơn nhiều và thậm chí vượt trội so với chưng cất bình thường đối với những người tương đương hiệu suất. Ngoài ra, nó cho phép chuyển giao kiến thức chuyên môn (§5.1) và không yêu cầu siêu tham số cụ thể cho cặp. Không giống như các phương pháp tập hợp (Lakshminarayanan et al., 2017; Gontijo-Lopes et al., 2022; Sinha et al., 2021; Dietterich, 2000), phương pháp của chúng tôi duy trì chi phí suy luận ban đầu và xử lý sự khác biệt hiệu suất cao. Cuối cùng, chúng tôi nghiên cứu các thuộc tính kiến trúc và tác động của chúng đối với quá trình chuyển giao (§5.1) vượt ra ngoài phương pháp chuyển giao, và xem xét khả năng mở rộng đối với việc chuyển giao kiến thức từ nhiều mô hình, nơi chúng tôi thấy rằng việc chuyển giao tuần tự đơn giản có thể hoạt động thuận lợi khi tận dụng phương pháp chuyển giao của chúng tôi, đạt được những cải thiện rõ ràng so với việc chuyển giao từ chỉ một mô hình giáo viên tốt nhất duy nhất.

Nhìn chung, những đóng góp của chúng tôi có thể được tóm tắt như: (1) Chúng tôi khám phá sự tồn tại nhất quán của kiến thức bổ sung giữa các mô hình tùy ý được huấn luyện trước trên cùng tập dữ liệu - ngay cả khi họ mô hình hoặc hiệu suất khác nhau. (2) Chúng tôi tiến hành các nghiên cứu mở rộng, khám phá để điều tra khả năng chuyển giao được đảm bảo độc lập với mô hình và hiệu suất của kiến thức bổ sung mà không làm giảm hiệu suất. (3) Chúng tôi đề xuất một phương pháp chuyển giao thành công được thúc đẩy thông qua lăng kính của học liên tục, tận dụng phương pháp phân vùng dữ liệu dựa trên độ tin cậy, không có siêu tham số. (4) Chúng tôi cung cấp các nghiên cứu về mối quan hệ của các thuộc tính mô hình tổng quát với việc chuyển giao kiến thức tổng quát, và (5) điều tra việc chuyển giao kiến thức giữa nhiều mô hình. Mã sẽ được phát hành khi được chấp nhận.

2 CÔNG TRÌNH LIÊN QUAN
Các công trình đầu trong chưng cất kiến thức tập trung vào việc nén các mô hình giáo viên lớn thành các mô hình học sinh nhỏ hơn. Bucila et al. (2006) đạt được điều này bằng cách phù hợp với các mục tiêu mềm của giáo viên. Hinton et al. (2015) đề xuất chia tỷ lệ nhiệt độ cho các xác suất thấp hơn. Các công trình gần đây mở rộng điều này với bối cảnh cấu trúc: chuyển giao chú ý (Zagoruyko and Komodakis, 2017) khuyến khích các mẫu phản hồi đặc trưng tương tự

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

−20 −10 0 10 20
Sự Khác Biệt Hiệu Suất0510152025Kiến Thức Bổ Sung [%]Hiệu Suất Ngang Bằng
Kiến Thức Bổ Sung Tối Thiểu
(a)
0 5 10 15 20 25
Kiến Thức Bổ Sung [%]5.86.06.26.46.66.8Entropy
Phân Phối Đồng Đều (b)

Hình 1: (a) Chúng tôi cho thấy tỷ lệ kiến thức bổ sung (ρpos, tỷ lệ phần trăm của việc đảo nhãn tích cực của giáo viên so với học sinh) so với sự khác biệt hiệu suất ∆acc cho 466 cặp giáo viên/học sinh, và thấy bối cảnh bổ sung đáng kể ngay cả đối với các giáo viên yếu hơn nhiều. (b) Nhìn vào entropy của phân phối kiến thức bổ sung qua các lớp, chúng tôi thấy bối cảnh được chuyên môn hóa hơn đối với các giáo viên yếu hơn.

mẫu, Romero et al. (2015) và Zagoruyko and Komodakis (2017) đề xuất hồi quy có trọng số hướng dẫn các kích hoạt đặc trưng của học sinh. Tuy nhiên, những phương pháp này bị hạn chế trong việc phù hợp với kiến trúc học sinh và giáo viên. Tian et al. (2020) đề xuất chưng cất tương phản, căn chỉnh không gian đặc trưng của giáo viên và học sinh với tính linh hoạt trong các chiều đặc trưng, trong khi nổi bật sự trùng lặp hiệu suất giữa hầu hết các mục tiêu chưng cất. Những hiểu biết này chuyển giao đến chưng cất giáo viên đa dạng (Luo et al., 2019; de Carvalho et al., 2022; Shen et al., 2019a;b), ví dụ thông qua tái trọng số đầu ra giáo viên (Wu et al., 2021; Liu et al., 2020; Yu et al., 2022; Yuan et al., 2020a). Không giống như chưng cất tiêu chuẩn, chúng tôi xem xét việc chuyển giao kiến thức giữa các mô hình tùy ý, đã được huấn luyện - một nhiệm vụ khó khăn hơn nhiều, đặc biệt khi không có hạn chế nào (trái ngược với ví dụ (Yuan et al., 2020b)) được áp đặt lên hiệu suất tương đối hoặc kiến trúc, và kiến thức ban đầu nên được giữ lại. Ở mức độ khái niệm, điều này cũng kết nối với các công trình gần đây về tổng quát hóa mô hình yếu-đến-mạnh cho siêu căn chỉnh, mà công trình của chúng tôi có thể cung cấp một góc nhìn trực giao và những hiểu biết thực tế hữu ích (Burns et al., 2023).

Wortsman et al. (2022a;b) cho thấy rằng khi kiến trúc và huấn luyện trước được chia sẻ, nội suy tuyến tính có thể hiệu quả đáng ngạc nhiên đối với các lưu vực mất mát phù hợp (Neyshabur et al., 2020). Tuy nhiên, việc lựa chọn mô hình thông qua các chỉ số xác thực là chìa khóa cho các bộ sưu tập đa dạng. Ngược lại, chúng tôi kết hợp kiến thức mô hình mà không có hạn chế bằng cách tận dụng các góc nhìn từ Học Liên Tục. Các phương pháp được phân loại thành điều hòa (hạn chế thay đổi trọng số (Kirkpatrick et al., 2017; Schwarz et al., 2018; Li and Hoiem, 2016; Rebuffi et al., 2016; Castro et al., 2018)), phát lại (Rebuffi et al., 2016; Lopez-Paz and Ranzato, 2017; Chaudhry et al., 2019; Buzzega et al., 2020; Prabhu et al., 2020) thông qua một bộ nhớ dữ liệu, và các phương pháp tái cấu trúc mạng (Mallya and Lazebnik, 2018; Mallya et al., 2018; Zhang et al., 2020). Ngoài ra, nội suy cũng đã chứng minh hữu ích khi liên tục thích ứng từ một mô hình đã được huấn luyện trước (Stojanovski et al., 2022), mà chúng tôi bao gồm trong nghiên cứu chuyển giao của mình.

3 KIẾN THỨC BỔ SUNG GIỮA CÁC MÔ HÌNH ĐÃ ĐƯỢC HUẤN LUYỆN TRƯỚC

Trong những năm gần đây, hàng nghìn mô hình được huấn luyện trước hoàn thành trên các tập dữ liệu chuẩn như ImageNet đã được công khai, với các biến đổi qua tất cả các lựa chọn huấn luyện có thể (§1), điều có thể tác động đến tổng quát hóa - mức độ mà chúng tôi muốn nghiên cứu trong phần này. Cụ thể, chúng tôi sử dụng timm (Wightman, 2019), bao gồm hàng trăm mô hình được huấn luyện trên ImageNet dưới các điều kiện khác nhau, và xem xét vấn đề phân loại hình ảnh với không gian đầu vào X và không gian nhãn Y với c=|Y| nhãn. Cho f(·, θ) : X → Rc là một bộ phân loại với các tham số θ∈Θ, logits z=f(x, θ) và softmax σ(z) với σj(z) = exp(zj)/∑i exp(zi) liên kết với các mẫu x∈X. Chúng tôi sử dụng ft, fs để biểu thị giáo viên và học sinh đã được huấn luyện trước, với các tham số θt và θs tương ứng. Để đánh giá tính bổ sung của kiến thức giữa bất kỳ ft và fs nào, chúng tôi theo phương pháp của Lopes et al. (2022) và nghiên cứu hiệu suất trên tập xác thực ImageNet. Cụ thể, chúng tôi đo các đảo dự đoán tích cực giữa ft và fs, ρpos=1/n ∑ni=1 ρposi, trong đó ρposi biểu thị một đảo tích cực. Điều này định lượng tỷ lệ các mẫu được phân loại đúng bởi giáo viên nhưng sai bởi học sinh - kiến thức bổ sung. Đối với phần còn lại của công trình này, chúng tôi sẽ đề cập đến kiến thức bổ sung và sự tồn tại của những đảo tích cực này một cách thay thế nhau.

Sự tồn tại của kiến thức bổ sung. Sử dụng timm, chúng tôi chọn ngẫu nhiên 466 cặp mô hình (ft,fs) bao gồm 301 mô hình độc đáo với kiến trúc, kích thước và hiệu suất khác nhau. Trong Hình 1, chúng tôi

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Các Lớp ImageNet0204060iii) Giáo Viên Mạnh
(Khác biệt: +18)
Các Lớp ImageNet02040ii) Độ chính xác ngang bằng
(Khác biệt: ±0)
Các Lớp ImageNet0102030Kiến Thức Bổ Sung 
 mỗi Lớp [%]i) Giáo Viên Yếu
(Khác biệt: -13)
(a)
Top-2% Top-5% Top-20% Top-50%
Các Lớp với Top-X% Kiến Thức Bổ Sung+6%+8%+10%+12%Khác biệt so với Sim. Trung bình [%]
 (b)

Hình 2: (a) Biểu đồ được sắp xếp của kiến thức bổ sung mỗi lớp (đảo dự đoán tích cực) như tỷ lệ của tổng số mẫu lớp cho ba cặp giáo viên-học sinh (giáo viên yếu, ngang bằng và mạnh). Bối cảnh bổ sung được tập trung xung quanh một số ít lớp. (b) Sự tương tự ngữ nghĩa giữa các lớp top-X % được sắp xếp theo lượng kiến thức bổ sung. Được hiển thị như sự khác biệt tương đối so với sự tương tự lớp trung bình. Các lớp có kiến thức bổ sung nhiều nhất có khả năng liên quan về mặt ngữ nghĩa.

điều tra tỷ lệ kiến thức bổ sung so với sự khác biệt trong hiệu suất giữa ft và fs. Điều này được đo như sự khác biệt trong độ chính xác xác thực ∆acc=acc(ft)−acc(fs) cho mỗi cặp. Chúng tôi thấy một tỷ lệ lớn các đảo dự đoán tích cực khi ft vượt trội so với fs. Nhưng ngay cả khi ft hoạt động kém đáng kể so với mô hình học sinh (lên đến 20%), một tỷ lệ cao các đảo tích cực vẫn có thể được tìm thấy. Hội tụ khoảng 2%, tỷ lệ phần trăm này cao hơn một bậc độ lớn so với nhiễu ngẫu nhiên - đối với c=1000 lớp, xác suất để một mô hình sửa một mẫu một cách tình cờ là khoảng 0.1% (ví dụ các giáo viên ResNet50 ngẫu nhiên hóa cho thấy kiến thức bổ sung chỉ khoảng 0.03%). Như vậy, kết quả của chúng tôi chỉ ra sự tồn tại nhất quán bổ sung giữa bất kỳ mô hình đã được huấn luyện trước nào.

Hiểu về kiến thức bổ sung. Để tìm hiểu liệu kiến thức bổ sung được phân phối đều giữa các lớp hay nó được nhóm một cách có hệ thống trong các tập con cụ thể, chúng tôi phân tích phân phối của các đảo dự đoán qua tất cả các lớp. Đối với một tập con được chọn trong Hình 2a, nơi các lớp được sắp xếp theo lượng kiến thức bổ sung được mã hóa, chúng tôi thấy một số lớp mang một lượng bối cảnh không tương xứng, đặc biệt trong trường hợp của một giáo viên yếu hơn. Đối với những người mạnh hơn, bối cảnh bổ sung mà giáo viên có thể cung cấp trở nên được phân phối đều hơn. Điều này được hỗ trợ thêm khi nhìn vào entropy của những phân phối này cho tất cả các cặp (ft,fs) trong Hình 1 (phải), nơi chúng tôi thấy một xu hướng rõ ràng hướng tới bối cảnh tập hợp hơn đối với các giáo viên yếu hơn khi entropy giảm xuống. Trong tất cả các trường hợp tuy nhiên, một mức độ đáng kể của nhóm bối cảnh vẫn còn. Chúng tôi biểu thị những nhóm lớp này với kiến thức bổ sung đáng kể như các lĩnh vực chuyên môn tương đối. Khái niệm này trở nên rõ ràng hơn khi chúng tôi điều tra mối quan hệ ngữ nghĩa giữa chúng trong Hình 2b. Đối với điều này, chúng tôi đo sự tương tự ngữ nghĩa của các lớp chứa 2%, 5%, 10%, 20% và 50% đầu tiên của các đảo tích cực (dựa trên xếp hạng mỗi lớp theo kiến thức bổ sung như trong Hình 2a) sử dụng một mô hình ngôn ngữ đã được huấn luyện trước (CLIP, Radford et al. (2021)). So sánh sự tương tự được đo với sự tương tự trung bình của tất cả các lớp, chúng tôi thấy sự tăng tương đối trong sự tương tự ngữ nghĩa gần gấp đôi trung bình cho các lớp mã hóa kiến thức bổ sung nhiều nhất.

Tóm lại, chúng tôi quan sát thấy rằng kiến thức bổ sung giữa bất kỳ cặp mô hình đã được huấn luyện trước nào tồn tại, và kiến thức này mà một giáo viên đã được huấn luyện trước có thể chuyển cho học sinh được tập trung xung quanh các lĩnh vực chuyên môn bao gồm các lớp liên quan về mặt ngữ nghĩa. Sự tồn tại của kiến thức bổ sung phổ biến thúc đẩy nghiên cứu của chúng tôi về các công cụ chuyển giao kiến thức đa mục đích có thể.

4 PHƯƠNG PHÁP CHUYỂN GIAO KIẾN THỨC TỔNG QUÁT

Phần này giải thích các mục tiêu chuyển giao kiến thức có thể, bắt đầu từ chưng cất kiến thức tiêu chuẩn (§4.1 đến các mở rộng được đề xuất của chúng tôi trong §4.2, điều này làm nổi bật cách thức và lý do tại sao vấn đề này nên được coi như một vấn đề học liên tục. §4.3 mở rộng điều này đến nhiều giáo viên đã được huấn luyện trước.

4.1 CHUYỂN GIAO KIẾN THỨC THÔNG QUA CHƯNG CẤT KIẾN THỨC

Chưng Cất Kiến Thức (chưng cất KL) được tiên phong bởi Hinton et al. (2015), đề xuất giảm thiểu phân kỳ KL giữa các mục tiêu mềm của mô hình giáo viên và học sinh σ(zt) và σ(zs):
LKL=T²/n∑ⁿᵢ₌₁KL[σ(zs,i/T), σ(zt,i/T)], (1)

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Mẫu Batch Mẫu Batch 
Mô hình 
Học sinh  - Chuyển giao 
Kiến thức ≤  =
> =
Mô hình 
Giáo viên   f 
Mô hình    
Học sinh-
Giáo viên    t

Hình 3: Đối với việc chuyển giao kiến thức tổng quát giữa bất kỳ mô hình đã được huấn luyện trước nào, chúng tôi đề xuất điều hòa cấp độ dữ liệu: Các mẫu được tách ra dựa trên việc chúng có nên được dạy thông qua giáo viên ft hay được giữ lại thông qua một phiên bản đông lạnh của học sinh ban đầu, fst. Tất cả các mô hình chuyển tiếp cùng một batch, và các đầu ra σ(zt) và σ(zst) được hợp nhất ở cấp độ mẫu thông qua các mặt nạ lựa chọn mt và mst được rút ra từ độ tin cậy mô hình (Eq. 4). Cuối cùng, chúng tôi tính toán KL-div. với các đầu ra σ(zs) của học sinh thích ứng (fs).

với nhiệt độ T. Chúng tôi sử dụng Eq. 1 như mục tiêu chuyển giao cơ sở của chúng tôi (chuyển giao KL-Dist), vì nó vẫn còn phổ biến và cạnh tranh (Beyer et al., 2022; Rajasegaran et al., 2020; Tian et al., 2020) (xem Phụ lục cho các mục tiêu khác). Chưng cất KL thường được mở rộng với các mất mát phân loại phụ trợ (ví dụ cross-entropy LXE, (Hinton et al., 2015; Tian et al., 2020; Rajasegaran et al., 2020; Beyer et al., 2022)) để ổn định quá trình chưng cất. Chúng tôi biểu thị sự kết hợp có trọng số λ như chưng cất XE-KL, và việc chuyển giao liên kết như chuyển giao XE-KL-Dist. hoặc XE-KL:
Ldist=λ·LKL+(1−λ)·LXE. (2)

Hầu hết nghiên cứu chưng cất kiến thức xem xét việc chưng cất từ một giáo viên đã được huấn luyện sang một học sinh chưa được huấn luyện, trái ngược mạnh mẽ với mục tiêu của chúng tôi là chuyển giao kiến thức giữa các mô hình đã được huấn luyện trước trong khi giữ lại kiến thức học sinh đã có được trước đó. Và thực vậy, khi được áp dụng cho việc chuyển giao kiến thức giữa các mô hình đã được huấn luyện trước trong §5.1, chưng cất kiến thức tiêu chuẩn gặp khó khăn trong việc chuyển giao kiến thức mà không làm giảm hiệu suất cho hầu hết các cặp giáo viên-học sinh. Chúng tôi đo điều này thông qua delta chuyển giao ∆transf=acc(fskt)−acc(fs), điều này định lượng sự thay đổi trong độ chính xác top-1 của học sinh, với fskt là mô hình học sinh sau khi chuyển giao kiến thức.

4.2 CHUYỂN GIAO KIẾN THỨC THÔNG QUA CHƯNG CẤT KIẾN THỨC LIÊN TỤC

Để chuyển giao kiến thức thành công, cần có một sự đánh đổi thuận lợi giữa việc giữ lại kiến thức học sinh hiện có và kết hợp kiến thức giáo viên bổ sung. Điều này có sự giống nhau với các khung Học Liên Tục (CL) (Kirkpatrick et al., 2016; Zenke et al., 2017; Aljundi et al., 2019), nhằm huấn luyện các mô hình trên các luồng dữ liệu tăng dần mà không quên bối cảnh đã học trước đó.

Ràng buộc cập nhật trọng số. Không giống như CL tiêu chuẩn tuy nhiên, vấn đề của chúng tôi không phải là của một luồng dữ liệu tăng dần, mà là của các tín hiệu học tập mới liên tục từ giáo viên qua cùng dữ liệu chuyển giao. Điều này loại trừ các phương pháp dựa trên bộ nhớ, nhưng cho phép điều hòa (xem §2 để biết chi tiết), nơi Stojanovski et al. (2022) cho thấy rằng đối với việc thích ứng liên tục của các mô hình đã được huấn luyện, nội suy trọng số momentum (MCL) chứng minh hiệu quả hơn so với các điều hòa hiện có. Do đó chúng tôi mở rộng mục tiêu XE-KL cơ sở của chúng tôi với MCL (chuyển giao XE-KL-Dist+MCL/XE-KL+MCL). Nội suy trọng số trong MCL giữ lại một bản sao chậm của các trọng số học sinh θslow_s ngoài các trọng số nhanh θfast_s được cập nhật trong quá trình chuyển giao. Tại tần suất được xác định trước N và lần lặp i, θslow_s được cập nhật thông qua nội suy không gian trọng số:
θslow,i+1_s =τ·θslow,i_s+(1−τ)·θfast,i_s, (3)

với momentum τ hướng dẫn ràng buộc trọng số. Cả N và τ có thể được điều chỉnh để cân bằng sự đánh đổi tính dẻo-ổn định. Tuy nhiên, vì nội suy trọng số ràng buộc cập nhật trọng số cho tất cả các mẫu một cách như nhau, nó gặp khó khăn trong việc tận dụng các lĩnh vực chuyên môn tương đối của các giáo viên một cách đầy đủ (c.f. §5.1).

Ràng buộc dữ liệu chuyển giao. Thay vì các hạn chế cấp độ trọng số, chúng tôi đề xuất điều hòa ở cấp độ dữ liệu bằng cách phân vùng dữ liệu chuyển giao thành các mẫu nơi học sinh có thể hưởng lợi từ phản hồi của giáo viên và những mẫu nơi kiến thức trước đó nên được giữ lại. Cụ thể, đối với các mẫu cần thiết cho việc chuyển giao kiến thức bổ sung, chúng tôi chưng cất từ giáo viên ft. Tuy nhiên, đối với tập mẫu khác, thay vào đó chúng tôi chưng cất từ mô hình học sinh ban đầu, đông lạnh (được ký hiệu là fst cho học sinh-giáo viên), với mục tiêu giữ lại hành vi mô hình ban đầu. Việc lựa chọn các tập con dữ liệu này theo một heuristic tham lam đơn giản và hiệu quả gán mỗi mẫu tùy thuộc vào xác suất dự đoán cao nhất cho lớp thực tế tương ứng, đưa ra các mặt nạ dữ liệu mt và mst
mt_i=I[σj(zt,i)>σj(zst,i)], mst_i=I[σj(zt,i)≤σj(zst,i)], (4)

cho mỗi mẫu i và j=yi, và với σj(z) đầu ra softmax cho lớp j. Trong thực tế, chúng tôi thấy rằng việc sử dụng những mặt nạ này cung cấp đủ ổn định cho quá trình chuyển giao nơi phân loại phụ trợ không còn cần thiết. Ngoài ra, chúng tôi cũng thấy rằng heuristic có giám sát này có thể được thay thế bằng một heuristic hoàn toàn không giám sát bằng cách gán các mẫu dựa trên xác suất dự đoán tối đa cho một mẫu, tức là chọn mô hình theo độ tin cậy. Mặc dù điều này có thể gặp phải tình trạng quá tin cậy, trong thực tế hiệu suất phù hợp với cài đặt có giám sát. Điều này có nghĩa là việc chuyển giao kiến thức có thể được thực hiện mà không cần truy cập vào nhãn. Chúng tôi cung cấp một hình ảnh hóa phân vùng dữ liệu (DP) của chúng tôi trong Hình 3. Phương pháp chuyển giao cuối cùng mà chúng tôi gọi là chuyển giao KL-Dist+DP, do đó được đưa ra như:
Ldist=T²/n∑ⁿᵢ₌₀mt_i·KL[σ(zs,i/T), σ(zt,i/T)]+mst_i·KL[σ(zs,i/T), σ(zst,i/T)].(5)

Như có thể thấy, chuyển giao KL-Dist+DP (hoặc KL+DP) không yêu cầu siêu tham số bổ sung so với chưng cất kiến thức tiêu chuẩn, với độ mạnh mẽ mạnh mẽ đối với các lựa chọn nhiệt độ (c.f. Phụ lục).

4.3 CHUYỂN GIAO KIẾN THỨC ĐA GIÁO VIÊN

Với nhiều mô hình có sẵn trong các sở thú mô hình, việc nghiên cứu cách chuyển giao bối cảnh giữa nhiều chuyên gia là một mở rộng tự nhiên, mà chúng tôi đề xuất nghiên cứu ba phương pháp. Thứ nhất, phù hợp với chưng cất kiến thức đa giáo viên tiêu chuẩn, tất cả các giáo viên có thể được sử dụng cùng lúc để chuyển giao kiến thức (song song), trong khi vẫn tận dụng bộ điều hòa chuyển giao được đề xuất của chúng tôi được mô tả ở trên để đảm bảo chuyển giao tích cực. Cụ thể, việc lựa chọn dữ liệu tham lam được mở rộng để tạo ra các tập con dữ liệu cho mỗi mô hình giáo viên tương ứng. Thứ hai, quá trình chuyển giao đa mô hình có thể được thực hiện tuần tự phù hợp với việc xử lý liên tục của quá trình chuyển giao mô hình đơn. Sau mỗi lần chuyển giao mô hình giáo viên, học sinh được chưng cất được coi như học sinh đã được huấn luyện trước (mới) cho bước chuyển giao tiếp theo. Cuối cùng, phần thí nghiệm của chúng tôi cũng điều tra việc sử dụng Model Soups (Wortsman et al., 2022a) cho vấn đề chuyển giao kiến thức độc lập kiến trúc. Ở đây, mô hình học sinh được chưng cất độc lập từ mỗi mô hình giáo viên, tạo ra một tập hợp các biến thể mô hình được chưng cất {fi_s}i∈1,...,Kt với Kt mô hình giáo viên. Sau khi chuyển giao, nội suy trọng số đơn giản giữa tất cả các biến thể được thực hiện (soup).

5 NGHIÊN CỨU THÍ NGHIỆM VỀ CHUYỂN GIAO KIẾN THỨC HIỆU QUẢ

Chúng tôi đầu tiên tiến hành một nghiên cứu quy mô lớn về các phương pháp chuyển giao kiến thức (c.f. §4.1-4.3) trong §5.1, làm nổi bật lợi ích của phương pháp học liên tục, và đặc biệt là sự vượt trội của phương pháp phân vùng dữ liệu của chúng tôi. Để khám phá, chúng tôi sử dụng một biến thể có giám sát (Eq. 4) nhưng cho thấy trong Tab. 3 rằng biến thể không giám sát phù hợp với hiệu suất. Cuối cùng, chúng tôi điều tra mối quan hệ của các thuộc tính mô hình và sự thành công chuyển giao tổng quát (cũng §5.1), và nghiên cứu chuyển giao từ nhiều mô hình trong §5.2.

Chi tiết thí nghiệm. Chúng tôi sử dụng các cụm tính toán NVIDIA 2080Ti với PyTorch 1.13.1 (Paszke et al., 2017) và ffcv 0.0.3 (Leclerc et al., 2022) để tải dữ liệu nhanh. Mặc dù hiệu suất có thể bị ảnh hưởng một chút, các thay đổi tương đối được giữ lại (Leclerc et al., 2022), cho phép các thí nghiệm quy mô lớn trên tính toán hợp lý. Đối với các đánh giá quy mô lớn của chúng tôi về các phương pháp chuyển giao, chúng tôi sử dụng một tập con ImageNet 10% được phân tầng (lấy mẫu con mỗi lớp), sau đó xác thực các tuyên bố chính của chúng tôi trên tập dữ liệu ImageNet đầy đủ. Chúng tôi thực hiện chuyển giao qua một ngân sách bị ràng buộc của 20 epoch để nghiên cứu các phương pháp chuyển giao kiến thức đa mục đích với các yêu cầu thực tế. Các tham số tối ưu hóa khác được xác định thông qua tìm kiếm lưới (xem Phụ lục). Mã: github.com/ExplainableML/General-Knowledge-Transfer.

5.1 ĐÁNH GIÁ CÁC PHƯƠNG PHÁP KHÁC NHAU CHO CHUYỂN GIAO KIẾN THỨC

Hiệu quả của chưng cất kiến thức tiêu chuẩn cho chuyển giao kiến thức. Để nghiên cứu tính phù hợp của chưng cất KL tiêu chuẩn cho chuyển giao kiến thức đa mục đích, chúng tôi chọn 400 cặp giáo viên-học sinh (Tab. 5 trong Phụ lục để biết chi tiết), tất cả đều thể hiện một tỷ lệ phần trăm đáng kể của kiến thức bổ sung (c.f. §3). Qua các cặp này cho mỗi mô hình học sinh, chúng tôi đo tỷ lệ phần trăm của các giáo viên mà một delta chuyển giao tích cực ∆transf. được thu được. Kết quả được hình ảnh hóa trong Hình 4a, và tiết lộ rằng đối với phần lớn học sinh có ít hơn 40% giáo viên với tăng hiệu suất. Một

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Chuyển giao
KL-Dist.Chuyển giao
XE-KL-Dist.Chuyển giao XE-KL-Dist +
MCLChuyển giao
KL-Dist + DP020406080100Tỷ lệ Thành công Chuyển giao Kiến thức [%]     
(a) Tỷ lệ phần trăm chuyển giao thành công.

−10 −5 0 5 10
Sự Khác biệt Hiệu suất của Giáo viên và Học sinh−20246Delta Chuyển giao Kiến thức
Chuyển giao KL-Dist.
Chuyển giao XE-KL-Dist. + MCL
Chuyển giao KL-Dist + DP (b) Tương quan thuộc tính giáo viên & thành công chuyển giao.

Hình 4: (a) Tỷ lệ giáo viên dẫn đến chuyển giao kiến thức tích cực (tỷ lệ thành công) cho các biến thể chưng cất kiến thức (xanh dương) và các mở rộng học liên tục (cam). Mỗi hộp đại diện cho 400 thí nghiệm chuyển giao, với sự tăng rõ ràng cho các thiết lập học liên tục. (b) Delta chuyển giao theo sự khác biệt hiệu suất giáo viên-học sinh được nhóm. Để báo cáo mạnh mẽ hơn, chúng tôi hiển thị delta chuyển giao trung bình của top 25% cho mỗi nhóm/phương pháp với cùng 400 cặp giáo viên-học sinh. Kết quả cho thấy Chuyển giao KL-Dist+DP cho phép lợi ích nhất quán từ các giáo viên yếu hơn và mạnh hơn.

Bảng 1: Kết quả chuyển giao kiến thức trên ImageNet đầy đủ, từ bốn giáo viên đến tám mô hình học sinh được chọn (xem Phụ lục Tab. 6). Kết quả cung cấp bằng chứng thêm rằng phương pháp học liên tục với vấn đề chuyển giao cho phép chuyển giao kiến thức nhất quán hơn qua các cặp mô hình tùy ý.

Học sinh Loại Độ chính xác # Tham số ∆transf. KL-Dist. ∆transf. KL-Dist+DP
XCiT-P16 (El-Nouby et al., 2021) Trafo 82.89 189.10 0.13 (±1.01) 0.65 (±0.26)
PiT-B (Heo et al., 2021) Trafo 82.44 73.76 0.33 (±0.86) 0.55 (±0.25)
PiT-XS (Heo et al., 2021) Trafo 78.19 10.62 0.10 (±0.49) 0.40 (±0.12)
SeNet154 (He et al., 2018) CNN 81.23 115.09 -0.05 (±0.26) 0.27 (±0.17)
ConvNext (Liu et al., 2022) CNN 84.57 50.22 -0.51 (±0.85) 0.33 (±0.14)
ResNetV2 (He et al., 2016b) CNN 82.80 25.55 -0.09 (±0.34) 0.23 (±0.08)
Mixer-B16 (Tolstikhin et al., 2021) MLP 82.30 59.88 -0.29 (±0.58) 0.15 (±0.13)
ResMLP-24 (Touvron et al., 2021) MLP 80.76 30.02 0.15 (±0.36) 0.33 (±0.19)

mất mát phân loại bổ sung (XE-KL, §4.1) có thể nâng tỷ lệ thành công trung vị lên hơi trên 40%. Trong cả hai trường hợp, tuy nhiên, việc ghi đè kiến thức có sẵn trước thường xuyên hơn không thì che khuất lợi ích thu được từ việc chuyển giao kiến thức thực tế, đặc biệt khi chuyển giao từ một mô hình giáo viên yếu hơn như được hiển thị trong Hình 4b (Chuyển giao KL-Dist.), nơi các thay đổi hiệu suất tuyệt đối sau khi chuyển giao được hình ảnh hóa so với các sự khác biệt hiệu suất giáo viên-học sinh ban đầu (như được đo trên dữ liệu xác thực riêng biệt). Ngoài ra, chúng tôi thấy rằng những giới hạn này cũng áp dụng khi triển khai các điều hòa bổ sung đơn giản như làm mịn nhãn (54%, Yuan et al. (2020b)), với chuyển giao âm nhất quán cho các giáo viên yếu hơn, và hiệu quả giảm cho những người mạnh hơn.

Tận dụng học liên tục. Coi việc chuyển giao kiến thức tổng quát như một nhiệm vụ học liên tục thông qua điều hòa trọng số (XE-KL+MCL) nâng tỷ lệ thành công trung vị một cách đáng kể (80%, Hình 4a). Tuy nhiên, chúng tôi thấy thiếu hiệu quả khi kiến thức được chuyên môn hóa cho các lĩnh vực chuyên môn, và khi các giáo viên mạnh hơn, điều mà chúng tôi giải quyết với điều hòa cấp độ dữ liệu (KL+DP), nâng tỷ lệ thành công lên 92.5%. Như được hiển thị trong Hình 4b, những lợi ích này có thể được quy cho các delta chuyển giao tích cực ngay cả đối với các giáo viên yếu hơn nhiều (xem sự khác biệt hiệu suất thấp hơn nhiều so với không), và, không giống như điều hòa cấp độ trọng số nghiêm ngặt trong ví dụ MCL, hầu như không hạn chế lợi ích từ các giáo viên mạnh hơn nhiều. Thực vậy, chúng tôi thấy rằng đối với một số giáo viên mạnh hơn, đặc biệt là nơi sự khác biệt hiệu suất không nổi bật, điều hòa cấp độ dữ liệu thậm chí có thể mang lại lợi thế so với chưng cất bình thường.

Thí nghiệm ImageNet đầy đủ. Chúng tôi mở rộng các thí nghiệm của chúng tôi từ trên đến ImageNet đầy đủ trong Tab. 1, và xác minh rằng những hiểu biết chuyển giao, với KL+DP hoạt động đáng tin cậy hơn đáng kể so với chưng cất bình thường. Chúng tôi cũng cung cấp một tổng quan chi tiết hơn trong Bảng 2, làm nổi bật việc chuyển giao thành công từ cả giáo viên mạnh hơn, nhưng đặc biệt là yếu hơn - ngay cả khi các mô hình học sinh đã mạnh (≈+0.3% cho PiT-B (Heo et al., 2021), 82.44% ImageNet top-1 và ConvNeXt (Liu et al., 2022), 84.57%, với sự khác biệt hiệu suất gần 5%). Để biết kết quả bổ sung về ImageNet và thông tin mô hình, chúng tôi tham khảo các bảng 6 và 7 trong phụ lục. Ngoài ra, phụ lục cũng tiết lộ thành công chuyển giao tương tự bằng cách sử dụng KL+DP cho các tập dữ liệu khác như CUB200 (Wah et al.,

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Top-2% Top-5% Top-20% Top-50% Top-100%
Các Lớp với Top-X% Kiến Thức Bổ Sung20406080100Tỷ lệ Chuyển giao [%]
(a) Tỷ lệ chuyển giao cho các lớp pos. flip top-X%.

10 15 20 40 60 80 120 200
Số Tham số Học sinh [M]20406080Tỷ lệ Chuyển giao [%]
Trafo
CNN
MLP (b) Tỷ lệ chuyển giao theo kích thước học sinh và họ mô hình.

Hình 5: (a) Tỷ lệ chuyển giao cho các tập hợp lớp chứa top 2% - 100% kiến thức bổ sung mỗi họ mô hình. (b) Tỷ lệ chuyển giao so với kích thước học sinh được tách theo họ mô hình.

2011), StanfordCars (Krause et al., 2013), và Caltech256 (Griffin et al., 2007). Hơn nữa, chúng tôi tận dụng các lần chạy ImageNet đầy đủ trong Tab. 3 để chứng minh rằng biến thể không giám sát của chúng tôi về chuyển giao KL+DP (§4.2) hoạt động tương đương và trong một số trường hợp thậm chí tốt hơn so với đối tác có giám sát của nó (ví dụ ∆transf.=0.55 so với ∆transf.=0.95 cho học sinh PiT-B). Điều này cho thấy rằng việc chuyển giao kiến thức tổng quát có thể được thực hiện ngay cả không có giám sát bằng cách tận dụng điều hòa học liên tục cấp độ dữ liệu.

Bảng 2: Ví dụ cho các delta chuyển giao của hai mô hình học sinh mỗi mô hình được chưng cất với một mô hình giáo viên mạnh hơn và yếu hơn trên ImageNet đầy đủ bằng cách sử dụng phương pháp chuyển giao KL-Dist+DP của chúng tôi.

Giáo viên → V olo-D2 ResMLP-36
Học sinh ↓ ∆acc ∆transf. ∆acc ∆transf.
PiT-B +2.75 0.86 -2.67 0.31
ConvNext +0.62 0.44 -4.80 0.26

Bảng 3: So sánh giữa chuyển giao KL-Dist+DP có giám sát và không giám sát trên ImageNet cho tám học sinh được chọn và bốn giáo viên, tương ứng. Kết quả cho thấy rằng việc chuyển giao kiến thức hoàn toàn không giám sát giữa các chuyên gia không chỉ có thể mà thậm chí có thể vượt trội so với chuyển giao có giám sát.

Học sinh∆transf. KL+DP ∆transf. KL+DP
Có giám sát Không giám sát
XCiT-P16 (El-Nouby et al., 2021) 0.65 (±0.26) 0.96 (±0.42)
PiT-B (Heo et al., 2021) 0.55 (±0.25) 0.95 (±0.52)
PiT-XS (Heo et al., 2021) 0.40 (±0.12) 0.35 (±0.18)
SeNet154 (He et al., 2018) 0.27 (±0.17) 0.25 (±0.15)
ConvNext (Liu et al., 2022) 0.33 (±0.14) 0.28 (±0.14)
ResNetV2 (He et al., 2016b) 0.23 (±0.08) 0.23 (±0.09)
Mixer-B16 (Tolstikhin et al., 2021) 0.15 (±0.13) 0.17 (±0.15)
ResMLP-24 (Touvron et al., 2021) 0.33 (±0.19) 0.29 (±0.24)

Chúng tôi thực sự thấy tăng phương sai do việc lựa chọn dữ liệu không giám sát. Để hiểu rõ hơn về nó, chúng tôi nghiên cứu số lượng mẫu flip tích cực (giáo viên đúng, học sinh sai) và flip âm (ngược lại) được gán cho giáo viên. Lý tưởng nhất, điều này chỉ bao gồm các mẫu flip tích cực. Đối với các cặp mô hình được trình bày trong Bảng 6 sử dụng chuyển giao KL-Dist.+DP, chúng tôi thấy rằng 72% mẫu flip tích cực và 9% mẫu flip âm được gán cho giáo viên. Điều này có nghĩa là trong khi phân vùng đơn giản dựa trên độ tin cậy không gán mẫu một cách hoàn hảo, nó vẫn căn chỉnh mạnh mẽ với các lĩnh vực chuyên môn tương ứng.

Nhìn chung, chúng tôi thấy việc chuyển giao rất hứa hẹn qua các cặp giáo viên-học sinh - ngay cả không có giám sát. Mặc dù lợi ích không đạt được tổng kiến thức bổ sung §3 - có thể quy cho sự đánh đổi giữa duy trì và chuyển giao - chúng tôi tin rằng kết quả của chúng tôi mang lại một bằng chứng khái niệm mạnh mẽ cho nghiên cứu tương lai và tiềm năng của việc chuyển giao kiến thức thực sự tổng quát.

Kiến thức bổ sung thúc đẩy lợi ích chuyển giao. Các thí nghiệm của chúng tôi ở trên cho thấy lợi ích rõ ràng khi tiến hành chuyển giao KL+DP qua tất cả các loại cặp mô hình. Trong phần này, chúng tôi cho thấy rằng lợi ích thực sự xuất phát từ việc chuyển giao kiến thức bổ sung giữa các cặp này (c.f. §3). Để làm điều đó, chúng tôi phân tích sự cải thiện của học sinh mỗi lớp tương đối với kiến thức bổ sung có sẵn mỗi lớp, được ký hiệu là tỷ lệ chuyển giao, cho tất cả 400 lần chạy chuyển giao được nghiên cứu trong Hình 4a. Kết quả trong Hình 5a vẽ tỷ lệ chuyển giao so với các lớp chứa top-X% kiến thức bổ sung. Sau khi sắp xếp các lớp theo tỷ lệ kiến thức bổ sung (c.f. trong Hình 2a, chúng tôi xem cách X% đầu tiên của kiến thức bổ sung được chuyển giao). Theo ký hiệu của chúng tôi từ §5.1, các tỷ lệ phần trăm nhỏ hơn chủ yếu chứa các flip từ lĩnh vực chuyên môn tương đối của một giáo viên, điều này được làm mềm cho các giá trị lớn hơn. Thực vậy, khi sử dụng chuyển giao KL+DP được đề xuất của chúng tôi, chúng tôi thấy một dấu hiệu rõ ràng nơi kiến thức bổ sung liên kết với các lĩnh vực chuyên môn mạnh hơn trong mô hình giáo viên có cơ hội gần như được đảm bảo để được chuyển giao cho mô hình học sinh. Điều này giảm khi di chuyển hướng tới các flip liên kết với các lớp ít được đại diện hơn. Lưu ý rằng không có sự ưu tiên hướng tới các lĩnh vực chuyên môn tương đối, người ta sẽ mong đợi một đường ngang ở tỷ lệ chuyển giao trung bình. Điều này cho thấy rằng việc xử lý dựa trên CL của chúng tôi về việc chuyển giao kiến thức cho phép bất kỳ mô hình giáo viên nào truyền đạt kiến thức cụ thể và rằng người ta có thể hướng dẫn một cách rõ ràng bối cảnh được học bởi một mô hình học sinh bằng cách chọn một giáo viên với lĩnh vực chuyên môn phù hợp.

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 4: Chuyển giao từ nhiều giáo viên theo cách tuần tự, song song và dựa trên soup (c.f. §4.3). Chúng tôi thấy chuyển giao tuần tự hoạt động thuận lợi.

Học sinh Loại Độ chính xác # Tham số∆transf. Giáo viên Đơn ∆transf. Nhiều Giáo viên
Trung bình Tối thiểu Tối đa Tuần tự Song song Soup
XCiT-P16 (El-Nouby et al., 2021) Trafo 82.89 189.1 0.73 0.36 0.93 0.97 0.91 0.63
Twins (Chu et al., 2021) Trafo 83.68 99.27 0.22 0.20 0.26 0.65 0.30 0.21
PiT-B (Heo et al., 2021) Trafo 82.44 73.76 0.64 0.31 0.86 1.04 0.77 0.69

Thuộc tính của một mô hình học sinh tốt. Chúng tôi kết thúc các nghiên cứu chuyển giao mô hình đơn của chúng tôi bằng cách điều tra liệu và cách các thuộc tính mô hình học sinh cụ thể có thể tạo điều kiện cho việc tiếp nhận kiến thức bổ sung. Qua các thí nghiệm của chúng tôi được thực hiện trong Phần 5.1, chúng tôi thấy trong Hình 5b rằng tỷ lệ chuyển giao của kiến thức bổ sung có mối quan hệ đáng kể với khả năng mô hình (số lượng tham số). Chúng tôi quan sát xu hướng tổng quát này qua các họ kiến trúc bất kể các thành kiến quy nạp thị giác được mã hóa. Tuy nhiên, chúng tôi nhấn mạnh rằng trong khi các mô hình lớn hơn thường tiếp nhận tốt hơn đối với bối cảnh bổ sung, kiến trúc kiểu CNN, hoặc thường là các mô hình với thành kiến quy nạp thị giác mạnh hơn, dễ bị ghi đè kiến thức hiện có của chúng, dẫn đến delta chưng cất thấp hơn. Xem Phụ lục để biết chi tiết và thí nghiệm bổ sung.

5.2 CHUYỂN GIAO KIẾN THỨC TỪ NHIỀU MÔ HÌNH ĐÃ ĐƯỢC HUẤN LUYỆN TRƯỚC

Cuối cùng, chúng tôi nghiên cứu chuyển giao từ nhiều giáo viên trên ImageNet đầy đủ, kiểm tra chuyển giao KL+DP tuần tự và song song, và một biến thể model soups bằng cách nội suy giữa các biến thể học sinh được chuyển giao (c.f. §4.3). Các thí nghiệm của chúng tôi bao gồm ba học sinh và giáo viên (Phụ lục B.6), nơi chúng tôi tập trung vào Transformers, mà trong các thí nghiệm của chúng tôi đã cho thấy năng khiếu cao nhất đối với việc tiếp nhận kiến thức.

Kết quả được hiển thị trong Tab. 4, nơi chúng tôi so sánh với hiệu suất của chuyển giao giáo viên đơn.

Chúng tôi thấy rằng học sinh có thể liên tục đạt được kiến thức từ mỗi giáo viên khi chuyển giao tuần tự. Tuy nhiên, khi học sinh cải thiện, lợi nhuận giảm dần cho đến khi các delta chuyển giao đạt đến mức ổn định, vì việc quên trở nên phổ biến hơn khi chúng ta di chuyển xa hơn từ việc huấn luyện trước học sinh ban đầu. Tuy nhiên, việc chuyển giao tuần tự của ba giáo viên đạt được lợi ích trung bình 59% so với delta chuyển giao giáo viên đơn tốt nhất (ví dụ ∆transf=0.26→∆transf=0.65 cho Twins (Chu et al., 2021) hoặc ∆transf=0.86→∆transf=1.04 cho PiT-B (Heo et al., 2021)). Ở đầu đối diện, chúng tôi thấy chuyển giao KL-Dist. vanilla không thành công trong cài đặt đa giáo viên, gạch dưới lợi ích của chuyển giao KL+DP (xem thêm Phụ lục). Hơn nữa, trong khi chúng tôi thấy lợi ích kiến thức nhất quán bất kể thứ tự giáo viên, các thí nghiệm của chúng tôi chỉ ra rằng một chuỗi giáo viên giảm dần (tức là mạnh nhất trước) thực sự có thể gây ra việc quên không tương xứng cao hơn, vì mô hình di chuyển ra khỏi kiến thức cơ sở của nó nhanh hơn. Cuối cùng, không giống như chuyển giao tuần tự, chuyển giao song song của nhiều giáo viên không cải thiện so với hiệu suất chuyển giao giáo viên đơn tốt nhất. Điều này là do một lượng giữ lại giảm xảy ra vì điều hòa tập con tương ứng (c.f. §4.3) không giữ lại đủ mẫu cho việc giữ lại kiến thức tích cực. Cuối cùng, chúng tôi thấy việc tính trung bình trọng số của các biến thể học sinh được chưng cất với mỗi giáo viên tương ứng hoạt động tệ nhất (hoạt động kém hơn so với chuyển giao giáo viên đơn tốt nhất), mà chúng tôi quy cho thiếu khả năng nội suy và sự giảm sút tiếp theo trong việc giữ lại kiến thức.

6 KẾT LUẬN

Trong công trình này, chúng tôi cho thấy rằng bất kỳ cặp mô hình nào được huấn luyện trên cùng dữ liệu với các giao thức huấn luyện khác nhau (ví dụ thay đổi trong kiến trúc hoặc tối ưu hóa) thể hiện kiến thức bổ sung đáng kể - bối cảnh dữ liệu được mã hóa trong một mô hình và không có trong mô hình khác. Dựa trên sự hiện diện của kiến thức bổ sung, chúng tôi đưa ra một khám phá đầu tiên về một cơ chế tổng quát để chuyển giao nó giữa bất kỳ cặp mô hình nào mà không làm giảm hiệu suất và phụ thuộc vào các biện pháp xếp hạng bên ngoài. Điều này mở khóa bất kỳ kho mô hình nào như một tài nguyên để đạt được lợi ích mô hình, và tùy chọn cải thiện các mô hình lớn với bối cảnh từ những mô hình yếu hơn, tài nguyên thấp hơn. Các thí nghiệm quy mô lớn của chúng tôi tiết lộ giới hạn của chưng cất kiến thức đơn giản như một cơ chế chuyển giao tổng quát, và đề xuất các mở rộng thông qua lăng kính của học liên tục và phân vùng dữ liệu dựa trên độ tin cậy. Điều này nâng tỷ lệ thành công chuyển giao từ dưới 40% lên trên 92%, với chuyển giao tích cực từ cả giáo viên mạnh hơn và yếu hơn. Chúng tôi cũng cung cấp hiểu biết về các thuộc tính mô hình tổng quát tạo điều kiện cho việc chuyển giao, thấy khả năng mô hình và thành kiến quy nạp thị giác giảm là có lợi. Cuối cùng, chúng tôi trình bày chuyển giao từ nhiều mô hình với cơ chế chuyển giao của chúng tôi. Nhìn chung, chúng tôi cung cấp động lực thí nghiệm và các bước đầu tiên hướng tới các công cụ chuyển giao kiến thức bổ sung đa mục đích giữa các kiến trúc mô hình tùy ý.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

LỜI CẢM ÖN

Karsten Roth cảm ơn Chương trình Tiến sĩ Phòng thí nghiệm Châu Âu về Hệ thống Học tập và Thông minh (ELLIS) và Trường Nghiên cứu Quốc tế Max Planck về Hệ thống Thông minh (IMPRS-IS) vì sự hỗ trợ. Công trình này được hỗ trợ bởi dự án DFG số 276693517, bởi BMBF FKZ: 01IS18039A, bởi ERC (853489 - DEXIM), bởi EXC số 2064/1 – số dự án 390727645.

TÀI LIỆU THAM KHẢO

Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019.

Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gordon Wilson, Jonas Geiping, Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash, Yann LeCun, and Micah Goldblum. A cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210, 2023.

Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, and Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, Samira Ebrahimi Kahou, Vincent Michalski, Tal Arbel, Chris Pal, Gael Varoquaux, and Pascal Vincent. Accounting for variance in machine learning benchmarks. In Conference on Machine Learning and Systems (MLSys), 2021.

Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In International Conference on Knowledge Discovery and Data Mining (KDD), 2006.

Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision, 2023.

Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Dark experience for general continual learning: a strong, simple baseline. In Advances in Neural Information Processing Systems (NeurIPS), 2020.

Francisco M. Castro, Manuel J. Marín-Jiménez, Nicolás Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In ECCV, 2018.

Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelong learning with a-gem. In International Conference on Learning Representations (ICLR), 2019.

Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2021.

Marcus Vinícius de Carvalho, Mahardhika Pratama, Jie Zhang, and Yajuan San. Class-incremental learning via knowledge amalgamation. In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD), 2022.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.

Thomas G. Dietterich. Ensemble methods in machine learning. In International Workshop on Multiple Classifier Systems (MCS), 2000.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations (ICLR), 2021.

Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, and Hervé Jegou. Xcit: Cross-covariance image transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

Raphael Gontijo-Lopes, Yann Dauphin, and Ekin Dogus Cubuk. No one representation to rule them all: Overlapping features of training methods. In International Conference on Learning Representations (ICLR), 2022.

Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. Technical report, California Institute of Technology, 2007.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016a.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision (ECCV), 2016b.

Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag of tricks for image classification with convolutional neural networks. In European Conference on Computer Vision (ECCV), 2018.

Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial dimensions of vision transformers. In International Conference on Learning Representations (ICLR), 2021.

Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences (PNAS), 2016.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences (PNAS), 2017.

Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In IEEE International Conference on Computer Vision (ICCV) Workshops, 2013.

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander Madry. FFCV: Accelerating training by removing data bottlenecks. https://github.com/libffcv/ffcv/, 2022. commit 2544abd.

Yann LeCun and Yoshua Bengio. Convolutional Networks for Images, Speech, and Time-Series, page 255–258. MIT Press, 1995.

Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2016.

Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le. Pay attention to mlps. In Advances in Neural Information Processing Systems (NeurIPS), 2021a.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Iou-Jen Liu, Jian Peng, and Alexander G. Schwing. Knowledge flow: Improve upon your teachers. In International Conference on Learning Representations (ICLR), 2019.

Yuang Liu, W. Zhang, and Jun Wang. Adaptive multi-teacher multi-level knowledge distillation. Neurocomputing, 2020.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In IEEE International Conference on Computer Vision (ICCV), 2021b.

Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

Raphael Gontijo Lopes, Yann Dauphin, and Ekin Dogus Cubuk. No one representation to rule them all: Overlapping features of training methods. In International Conference on Learning Representations (ICLR), 2022.

David Lopez-Paz and Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

Sihui Luo, Xinchao Wang, Gongfan Fang, Yao Hu, Dapeng Tao, and Mingli Song. Knowledge amalgamation from heterogeneous networks by common feature learning. In International Joint Conference on Artificial Intelligence (IJCAI), 2019.

Arun Mallya and Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative pruning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In European Conference on Computer Vision (ECCV), 2018.

Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? In Advances in Neural Information Processing Systems (NeurIPS), 2020.

Jinhyuk Park and Albert No. Prune your model before distill it. In European Conference on Computer Vision (ECCV), 2021.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, 2019.

Ameya Prabhu, Philip H. S. Torr, and Puneet Kumar Dokania. Gdumb: A simple approach that questions our progress in continual learning. In European Conference on Computer Vision (ECCV), 2020.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021.

Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Designing network design spaces. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.

Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? In Advances in Neural Information Processing Systems (NeurIPS), 2021.

Jathushan Rajasegaran, Salman Hameed Khan, Munawar Hayat, Fahad Shahbaz Khan, and Mubarak Shah. Self-supervised knowledge distillation for few-shot learning. In British Machine Vision Conference (BMVC), 2020.

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, G. Sperl, and Christoph H. Lampert. icarl: Incremental classifier and representation learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. In International Conference on Learning Representations (ICLR), 2015.

Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjorn Ommer, and Joseph Paul Cohen. Revisiting training strategies and generalization performance in deep metric learning. In International Conference on Machine Learning (ICML), 2020.

Karsten Roth, Timo Milbich, Bjorn Ommer, Joseph Paul Cohen, and Marzyeh Ghassemi. Simultaneous similarity-based self-distillation for deep metric learning. In International Conference on Machine Learning (ICML), 2021.

Karsten Roth, Oriol Vinyals, and Zeynep Akata. Integrating language guidance into vision-based deep metric learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt. Disentanglement of correlated factors via hausdorff factorized support. In International Conference on Learning Representations (ICLR), 2023.

Robin M Schmidt, Frank Schneider, and Philipp Hennig. Descending through a crowded valley - benchmarking deep learning optimizers. In International Conference on Machine Learning (ICML), 2021.

Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress and compress: A scalable framework for continual learning. In International Conference on Machine Learning (ICML), 2018.

Chengchao Shen, Xinchao Wang, Jie Song, Li Sun, and Mingli Song. Amalgamating knowledge towards comprehensive classification. In AAAI Conference on Artificial Intelligence (AAAI), 2019a.

Chengchao Shen, Mengqi Xue, Xinchao Wang, Jie Song, Li Sun, and Mingli Song. Customizing student networks from heterogeneous teachers via adaptive knowledge amalgamation. IEEE International Conference on Computer Vision (ICCV), 2019b.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations (ICLR), 2015.

Samarth Sinha, Homanga Bharadhwaj, Anirudh Goyal, Hugo Larochelle, Animesh Garg, and Florian Shkurti. Dibs: Diversity inducing information bottleneck in model ensembles. In AAAI Conference on Artificial Intelligence (AAAI), 2021.

Zafir Stojanovski, Karsten Roth, and Zeynep Akata. Momentum-based weight interpolation of strong zero-shot models for continual learning. In Workshop on Interpolation Regularizers and Beyond, held at NeurIPS, 2022.

Mingxing Tan and Quoc Chen. Mixconv: Mixed depthwise convolutional kernels. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.

Damien Teney, Ehsan Abbasnejad, and Anton van den Hengel. Unshuffling data for improved generalization in visual question answering. In IEEE International Conference on Computer Vision (ICCV), 2020.

Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In International Conference on Learning Representations (ICLR), 2020.

Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Peter Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. MLP-mixer: An all-MLP architecture for vision. In Advances in Neural Information Processing Systems (NeurIPS), 2021.

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Hervé Jégou. Resmlp: Feedforward networks for image classification with data-efficient training. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

Diane Wagner, Fabio Ferreira, Danny Stoll, Robin Tibor Schirrmeister, Samuel Müller, and Frank Hutter. On the importance of hyperparameters and data augmentation for self-supervised learning. In Workshop of Pre-training: Perspectives, Pitfalls, and Paths Forward, held at ICML, 2022.

C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-ucsd birds 200. Technical report, California Institute of Technology, 2011.

Zeyu Wang, Yutong Bai, Yuyin Zhou, and Cihang Xie. Can cnns be more robust than transformers? In International Conference on Learning Representations (ICLR), 2023.

Ross Wightman. Pytorch image models, 2019.

Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: Averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning (ICML), 2022a.

Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022b.

Chuhan Wu, Fangzhao Wu, and Yongfeng Huang. One teacher is enough? pre-trained language model distillation from multiple teachers. In Findings of the Association for Computational Linguistics (ACL-IJCNLP), 2021.

Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

ZeLun Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Self-supervised learning with swin transformers. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image transformers. In IEEE International Conference on Computer Vision (ICCV), 2021.

Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

Longhui Yu, Zhenyu Weng, Yuqing Wang, and Yuesheng Zhu. Multi-teacher knowledge distillation for incremental implicitly-refined classification. In Advances in Neural Information Processing Systems (NeurIPS), 2022.

Fei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong, Yan Fu, and Daxin Jiang. Reinforced multi-teacher selection for knowledge distillation. In AAAI Conference on Artificial Intelligence (AAAI), 2020a.

Li Yuan, Francis E. H. Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via label smoothing regularization. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, 2020b.

Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan. Volo: Vision outlooker for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.

Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In International Conference on Learning Representations (ICLR), 2017.

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In International Conference on Machine Learning (ICML), 2017.

Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci, Larry P. Heck, Heming Zhang, and C.-C. Jay Kuo. Class-incremental learning via deep model consolidation. In WACV, 2020.

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

PHỤ LỤC

NHỮNG LỢI ÍCH TUYỆT VỜI VÀ NƠI TÌM THẤY CHÚNG: VỀ SỰ TỒN TẠI VÀ TRIỂN VỌNG CỦA VIỆC CHUYỂN GIAO KIẾN THỨC TỔNG QUÁT GIỮA BẤT KỲ MÔ HÌNH ĐÃ ĐƯỢC HUẤN LUYỆN TRƯỚC NÀO

A CHI TIẾT TRIỂN KHAI VÀ HIỂU BIẾT THÍ NGHIỆM

Trong phần này, chúng tôi mô tả chi tiết triển khai của các thí nghiệm để đánh giá hiệu quả của các phương pháp và kỹ thuật khác nhau để chuyển giao kiến thức bổ sung giữa các mô hình chuyên gia đã được huấn luyện trước được huấn luyện trên cùng tập dữ liệu.

Đối với các thí nghiệm ban đầu và khám phá của chúng tôi, chúng tôi sử dụng một tập con phân tầng 10% của ImageNet (Deng et al., 2009) để giảm thời gian chạy, nhằm tiến hành một loạt thí nghiệm rộng hơn qua một số lượng lớn các cặp mô hình. Chi tiết, chúng tôi rút 130 mẫu mỗi lớp sử dụng tập xác thực ImageNet tiêu chuẩn để đánh giá. Tất cả các thí nghiệm của chúng tôi sử dụng trình tối ưu SGD với momentum 0.9 và weight decay 1e-3. Các siêu tham số khác được điều chỉnh riêng cho mỗi phương pháp chuyển giao được điều tra.

A.1 TRIỂN KHAI CÁC BIẾN THỂ CHUYỂN GIAO KIẾN THỨC DỰA TRÊN CHƯNG CẤT

Để đặt tốc độ học cho phương pháp chuyển giao dựa trên chưng cất kiến thức mặc định của chúng tôi sử dụng phân kỳ KL (như trong Phần 4.1), chúng tôi tiến hành tìm kiếm tham số trên một tập hợp 33 cặp giáo viên-học sinh được chọn ngẫu nhiên từ thư viện timm Wightman (2019), với tốc độ học lr∈{1e-2,1e-3,1e-4,1e-5}, mà chúng tôi thấy tốc độ học 1e-4 thường hoạt động tốt nhất, mặc dù bất kể các giá trị được chọn, delta chuyển giao trung bình luôn âm một cách nhất quán.

Theo Phần 4.1, chúng tôi cũng mở rộng chưng cất KL với một mất mát phân loại cross-entropy. Trong trường hợp này, các siêu tham số được xác định trên một lưới bao gồm tốc độ học lr∈{1e-2,1e-3,1e-4,1e-5}, nhiệt độ softmax T∈{0.1,1,4,10} và trọng số λ∈{0.05,0.1,0.25,0.5}. Một lần nữa, chúng tôi thấy rằng tốc độ học 1e-4 hiệu quả nhất trung bình, nhưng thấy sự biến đổi cụ thể trong trọng số λ, nơi chúng tôi quan sát thấy rằng giá trị λ lớn hơn - đặt trọng số cao hơn cho chưng cất - phù hợp hơn để chuyển giao kiến thức từ một giáo viên mạnh hơn sang một học sinh yếu hơn, trong khi λ nhỏ hơn dường như được ưa thích khi chuyển giao kiến thức từ một giáo viên yếu hơn sang một học sinh mạnh hơn. Điều này làm nổi bật thêm sự đánh đổi giữa tăng kiến thức và duy trì, nơi đối với một giáo viên yếu hơn, việc duy trì đóng một vai trò quan trọng hơn nhiều để đảm bảo hiệu suất tổng thể cao, vì kiến thức học sinh bị ghi đè.

Đối với nhiệt độ softmax, chúng tôi thấy rằng nhiệt độ nhỏ 0.1 hạn chế sự giảm trong hiệu suất của học sinh khi chuyển giao từ một mô hình giáo viên yếu hơn, nhưng cũng hạn chế việc chuyển giao kiến thức nói chung. Điều này dẫn đến chỉ tăng nhỏ trong hiệu suất của học sinh ngay cả khi chuyển giao từ một mô hình giáo viên mạnh hơn. Hinton et al. (2015) đề xuất sử dụng nhiệt độ lớn hơn là 4 để phù hợp với các mục tiêu mềm để đại diện tốt hơn các xác suất nhỏ hơn trong đầu ra của một mẫu đơn. Tuy nhiên, chúng tôi không thấy nhiệt độ lớn hơn có lợi cho hiệu suất chuyển giao.

Nói chung, chúng tôi thấy rằng đặc biệt là tham số nhiệt độ và trọng số hướng dẫn tính tích cực của phương pháp chuyển giao dựa trên chưng cất, điều này phụ thuộc cao vào động lực giáo viên và học sinh được quan sát của cặp mô hình chuyên gia đã được huấn luyện trước được cung cấp. Sự biến đổi cao qua các cặp mô hình tùy ý như vậy làm cho chưng cất kiến thức bình thường, ngay cả khi được ghép nối với một mất mát phân loại bổ sung để ổn định, không phù hợp như một công cụ chuyển giao kiến thức tổng quát.

A.2 TRIỂN KHAI CHUYỂN GIAO KIẾN THỨC CHƯNG CẤT TƯƠNG PHẢN

Mặc dù các phương pháp chưng cất kiến thức phù hợp với các mục tiêu mềm của mô hình giáo viên và học sinh vẫn phổ biến, nhiều phương pháp gần đây cho rằng kiến thức cấu trúc hơn có thể được chuyển giao bằng cách khuyến khích mô hình học sinh cũng phù hợp với các biểu diễn trung gian của mô hình giáo viên (Liu et al., 2019; 2020; Wu et al., 2021; Park and No, 2021). Do đó, trong phần này, chúng tôi làm nổi bật kết quả của việc khám phá của chúng tôi về tính khả thi của việc sử dụng các biểu diễn trung gian và mối quan hệ của chúng để chuyển giao kiến thức giữa các chuyên gia đã được huấn luyện trước.

Chúng tôi đặc biệt theo Tian et al. (2020), người đề xuất mở rộng phương pháp chưng cất kiến thức cơ bản của Hinton et al. (2015) bằng cách căn chỉnh các biểu diễn đặc trưng của các mô hình giáo viên và học sinh. Ở đây, học sinh được khuyến khích cung cấp các biểu diễn đặc trưng gần với những cái của giáo viên cho các hình ảnh tương tự trong khi đẩy lùi biểu diễn đặc trưng của các hình ảnh không tương tự. Không giống như các phương pháp chưng cất hiện có khác hoạt động trên các biểu diễn đặc trưng, phương pháp tương phản như vậy đặt ít hạn chế hơn trên kiến trúc của mô hình giáo viên và học sinh, đặc biệt là vì các biểu diễn đặc trưng của cả hai mô hình có thể được chiếu rẻ vào một không gian đặc trưng chung sử dụng một lớp chiếu được học cho cả hai mô hình. Điều này cho phép chưng cất giữa các mô hình của các kiến trúc khác nhau, và cho phép chúng tôi khám phá một thay thế cho mục tiêu Chưng cất KL cơ sở được sử dụng của chúng tôi cho việc chuyển giao kiến thức tổng quát (Phần 4.1).

Để đánh giá tính khả thi của việc phù hợp biểu diễn cho việc chuyển giao kiến thức giữa các mô hình chuyên gia, chúng tôi triển khai hai phương pháp học tương phản. Đầu tiên, chúng tôi sử dụng một phương pháp đơn giản khuyến khích khoảng cách giữa các biểu diễn đặc trưng của một cặp hình ảnh tương tự cho cả mô hình giáo viên và học sinh. Do đó, nếu hai hình ảnh dẫn đến các biểu diễn đặc trưng tương tự trong không gian nhúng của giáo viên, học sinh được khuyến khích cũng cung cấp các biểu diễn đặc trưng với sự gần gũi trong không gian nhúng tương ứng của chúng. Việc phù hợp dựa trên sự tương tự tương đối như vậy đã thấy thành công trong học tương phản có giám sát tiêu chuẩn, như trong (Roth et al., 2021; 2022). Sử dụng t và s để biểu thị giáo viên và học sinh tương ứng, điều này cho
LCD=KL(σ(Ss), σ(St)), (6)
nơi S là một ma trận tương tự chứa các tương tự cosine của các biểu diễn đặc trưng được chuẩn hóa của batch hiện tại (Sij=cos sim(norm(si),norm(sj)),∀i,j∈0,...,n). Chúng tôi biểu thị phương pháp này như Chưng cất CD.

Thứ hai, chúng tôi triển khai phương pháp chưng cất biểu diễn tương phản (chưng cất CRD) của Tian et al. (2020). Như đã lưu ý, chưng cất CRD căn chỉnh trực tiếp các biểu diễn bằng cách khuyến khích học sinh gần với giáo viên cho các cặp tích cực (các tăng cường khác nhau của cùng một hình ảnh) trong khi đẩy xa các biểu diễn đặc trưng của các cặp âm (hình ảnh của các lớp khác nhau). Mục tiêu tương ứng do đó được cho như:
LCRD=arg max fs max h Eq(t,s|C=1)[log h(t,s)]+kEq(t,s|C=1)[log(1−h(t,s))], (7)
nơi chúng tôi sử dụng t,s như viết tắt cho các biểu diễn giáo viên và học sinh tương ứng. Ngoài ra, chúng tôi sử dụng h:t,s→[0,1] để đại diện cho một discriminator ước tính liệu biểu diễn đặc trưng t và s có được rút ra từ cùng phân phối liên kết hay từ tích biên tương ứng. Trong thiết lập này, k biểu thị số lượng cặp âm được rút ra từ tích của các biên.

Cả hai phương pháp chưng cất tương phản đều tính toán mất mát chưng cất tổng thể Ldist như một sự kết hợp có trọng số của mất mát tương phản tương ứng LCD hoặc LCRD và một mất mát phân loại cross-entropy LXE như cũng được sử dụng trong các mục tiêu chưng cất Phân kỳ KL tiêu chuẩn Beyer et al. (2022); Rajasegaran et al. (2020).

Đối với việc chuyển giao kiến thức dựa trên chưng cất CD, chúng tôi kiểm tra các trọng số khác nhau giữa mất mát tương phản và mất mát phân loại cũng như các tốc độ học khác nhau trên một tập nhỏ các kết hợp giáo viên-học sinh. Trên một lưới siêu tham số tương tự như được lưu ý trong phần trước, chúng tôi thấy trọng số bằng nhau của cả hai mất mát kết hợp với tốc độ học 1e-4 phù hợp nhất trung bình, mặc dù với một sự đánh đổi tương tự như được mô tả trong Phần A.1. Đối với việc chuyển giao chưng cất CRD, chúng tôi thấy các siêu tham số như được cung cấp trong Tian et al. (2020) hoạt động tốt.

A.3 TRIỂN KHAI CÁC PHƯƠNG PHÁP CHUYỂN GIAO DỰA TRÊN HỌC LIÊN TỤC

Cuối cùng, chúng tôi mô tả các siêu tham số và các nghiên cứu siêu tham số tương ứng được sử dụng cho phần mở rộng học liên tục của chúng tôi đối với việc chuyển giao kiến thức dựa trên chưng cất (xem Phần 4.2), cụ thể là thiết lập cho chuyển giao XE-KL-Dist+MCL và chuyển giao KL-Dist+DP.

Đối với chuyển giao XE-KL+MCL, chúng tôi tiến hành tìm kiếm tham số trên một lưới tốc độ học với cùng độ phân giải như trước. Tuy nhiên, vì có một số tham số khác để xác thực, chúng tôi chỉ kiểm tra lr∈{1e-2,1e-3}. Ngoài ra, chúng tôi theo Stojanovski et al. (2022) và kiểm tra momentum cho các giá trị trong τ∈{0.99,0.999,0.9999}) và tần suất nội suy N∈{2,10,50,100}). Đối với trọng số so với mục tiêu phân loại, λ, chúng tôi kiểm tra 0.5 và 0.7. Chúng tôi tiến hành

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 5: Lựa chọn các mô hình học sinh và giáo viên được sử dụng cho các thí nghiệm trên tập con ImageNet 10%. Mỗi tập hợp mô hình được chọn để chứa nhiều loại kiến trúc và bao phủ một phạm vi rộng của kích thước mô hình và mức hiệu suất.

Mô hình Học sinh Loại Độ chính xác # Tham số
XCiT-Large-24-P16 El-Nouby et al. (2021) Trafo 82.89 189.10
ViT-Base-P16 Dosovitskiy et al. (2021) Trafo 84.53 86.57
PiT-B Heo et al. (2021) Trafo 82.44 73.76
ViT-Relpos-Medium-P16 Trafo 82.46 38.75
PiT-XS Heo et al. (2021) Trafo 78.19 11.00
PiT-XE-dist Heo et al. (2021) Trafo 79.31 11.00
IG-ResNext101-32x16d Xie et al. (2017) CNN 84.17 194.03
Gluon-SeNet154 He et al. (2018) CNN 81.23 115.09
Wide-ResNet50-2 He et al. (2016a) CNN 81.46 68.88
ResNet101 He et al. (2016a) CNN 79.54 44.57
ResNetV2-50 He et al. (2016b) CNN 80.40 25.55
ResNet34-v1b He et al. (2016a) CNN 74.59 21.80
ResNetv2-50-dist He et al. (2016b) CNN 82.80 25.55
Mixer-L16 Tolstikhin et al. (2021) MLP 72.07 208.20
Mixer-B16-miil Tolstikhin et al. (2021) MLP 82.30 59.88
Mixer-B16 Tolstikhin et al. (2021) MLP 76.61 59.88
ResMLP-36 Touvron et al. (2021) MLP 79.77 44.69
ResMLP-24 Touvron et al. (2021) MLP 79.39 30.02
ResMLP-12 Touvron et al. (2021) MLP 76.66 15.35
ResMLP-24-dist Touvron et al. (2021) MLP 80.76 30.02

Mô hình Giáo viên Loại Độ chính xác # Tham số
ConvNext Liu et al. (2022) CNN 86.64 197.77
VOLO-D4 Yuan et al. (2021) Trafo 85.88 192.96
RegNety-320 Radosavovic et al. (2020) CNN 80.80 145.05
VGG13 Simonyan and Zisserman (2015) CNN 71.60 133.05
RegNetx-320 Radosavovic et al. (2020) CNN 80.24 107.81
TWINS Chu et al. (2021) Trafo 83.68 99.27
SWSL-ResNext101 Xie et al. (2017) CNN 84.29 88.79
SWIN-S3 Liu et al. (2021b) Trafo 83.93 71.13
TWINS-pcpvt Chu et al. (2021) Trafo 83.14 60.99
VOLO-D2 Yuan et al. (2021) Trafo 85.19 58.68
ResMLP-36 Touvron et al. (2021) MLP 79.77 44.69
DLA102 Yu et al. (2018) CNN 78.03 33.27
SWSL-ResNext50 Xie et al. (2021) CNN 82.18 25.03
ViT-P16 Dosovitskiy et al. (2021) Trafo 81.40 22.05
gMLP-S16 Liu et al. (2021a) MLP 79.64 19.42
COAT-lite Xu et al. (2021) Trafo 79.09 11.01
MixNet Tan and Chen (2019) MLP 78.98 7.33
RegNety-006 Radosavovic et al. (2020) CNN 75.25 6.06
MixNet Tan and Chen (2019) MLP 76.00 4.13
XCiT-nano-12-P8 El-Nouby et al. (2021) Trafo 73.92 3.05

tìm kiếm tham số như một tìm kiếm ngẫu nhiên trên lưới tham số. Cuối cùng, chúng tôi thấy một thiết lập tham số sử dụng momentum cao 0.9999 kết hợp với tần suất nội suy cao (mỗi lần lặp khác) và tốc độ học 0.01 với điểm trọng số 0.7 hoạt động tốt nhất trung bình. Không giống như việc chuyển giao dựa trên Chưng cất KL đơn giản, một kết hợp siêu tham số cố định bây giờ dẫn đến cả delta chuyển giao tích cực trung bình, và số lượng giáo viên mà mỗi học sinh có thể học từ đó tăng đáng kể (c.f. Hình 4a)

Đối với phương pháp chuyển giao KL+DP được đề xuất cuối cùng của chúng tôi, chúng tôi một lần nữa tiến hành tìm kiếm tham số tương tự. Tuy nhiên, không giống như chuyển giao XE-KL+MCL, phương pháp KL+DP không giới thiệu siêu tham số bổ sung so với thiết lập chưng cất kiến thức dựa trên KL tiêu chuẩn. Do đó, chúng tôi sử dụng một lưới lr∈{1e-3,1e-4}, λ∈{0.5,0.75,0.9,1} và T∈{0.1,1,10}. Lưu ý rằng trong khi chúng tôi ablated việc sử dụng một mất mát phân loại cross-entropy bên ngoài, chúng tôi thấy hiệu suất tốt nhất để đến một cách nhất quán cho λ=1- bằng cách tắt mục tiêu phân loại phụ trợ. Điều này cung cấp bằng chứng mạnh mẽ rằng các biện pháp bên ngoài cho sự ổn định huấn luyện không còn cần thiết. Cuối cùng, qua tất cả các thí nghiệm còn lại, chúng tôi sử dụng tốc độ học 1e-4 và nhiệt độ 1. Mặc dù các tìm kiếm tham số sâu hơn có thể cung cấp một kết hợp tham số sẽ cải thiện tỷ lệ thành công trung bình, chúng tôi tin rằng kết quả đạt được trong thiết lập hiện tại của nó để cung cấp bằng chứng khái niệm đủ.

A.4 DANH SÁCH MÔ HÌNH: CÁC NGHIÊN CỨU QUY MÔ LỚN TRÊN CÁC TẬP CON IMAGE NET PHÂN TẦNG

Bảng 5 trình bày một tóm tắt toàn diện về các mô hình giáo viên và học sinh đã được huấn luyện trước được sử dụng trong đánh giá của chúng tôi về các kỹ thuật chuyển giao khác nhau trên tập con 10% của tập dữ liệu ImageNet (§5.1). Những mô hình này được chọn cẩn thận để bao gồm các họ kiến trúc đa dạng, thể hiện các mức hiệu suất khác nhau, và thể hiện một loạt kích thước mô hình. Việc lựa chọn này cho phép chúng tôi kiểm tra kỹ lưỡng hiệu quả của các phương pháp chuyển giao kiến thức trong các kịch bản và thiết lập khác nhau. Lưu ý rằng đối với việc khám phá bối cảnh bổ sung (§3) chúng tôi tận dụng một tập hợp rộng hơn của 466 cặp giáo viên-học sinh bao gồm 301 mô hình đã được huấn luyện trước riêng lẻ được rút ra ngẫu nhiên từ thư viện timm Wightman (2019).

A.5 CÁC MÔ HÌNH ĐƯỢC ĐÁNH GIÁ TRÊN IMAGE NET ĐẦY ĐỦ

Bảng 6 trình bày các thông số kỹ thuật chi tiết của các mô hình học sinh và giáo viên được sử dụng trong các thí nghiệm ImageNet quy mô đầy đủ của chúng tôi (tham khảo Phần 5.1). Trong bối cảnh chuyển giao kiến thức từ nhiều mô hình giáo viên (§4.3), chúng tôi sử dụng cùng tập hợp các mô hình giáo viên kết hợp với một tập con các mô hình học sinh.

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 6: Lựa chọn mô hình học sinh và giáo viên được sử dụng cho các thí nghiệm trên ImageNet đầy đủ. Các mô hình học sinh được chọn để chứa nhiều loại kiến trúc và bao phủ một phạm vi rộng của kích thước mô hình và mức hiệu suất.

Mô hình Học sinh Loại Độ chính xác # Tham số
XCiT-large-24-p16 El-Nouby et al. (2021) Trafo 82.89 189.10
PiT-B Heo et al. (2021) Trafo 82.44 73.76
PiT-XS Heo et al. (2021) Trafo 78.19 11.00
Gluon-SeNet154 He et al. (2018) CNN 81.23 115.09
ConvNext Liu et al. (2022) CNN 84.57 50.22
ResNetV2-50-dist He et al. (2016b) CNN 82.80 25.55
Mixer-B16-miil Tolstikhin et al. (2021) MLP 82.30 59.88
ResMLP-24-dist Touvron et al. (2021) MLP 80.76 30.02

Mô hình Giáo viên Loại Độ chính xác # Tham số
SWSL-ResNext101 Xie et al. (2017) CNN 84.29 88.79
VOLO-D2 Yuan et al. (2021) Trafo 85.19 58.68
ResMLP-36 Touvron et al. (2021) MLP 79.77 44.69
CoaT-lite-mini Xu et al. (2021) Trafo 79.09 11.01

B KẾT QUẢ THÍ NGHIỆM MỞ RỘNG

Trong phần này, chúng tôi trình bày kết quả thí nghiệm bổ sung của các thí nghiệm của chúng tôi được tiến hành trong Phần 5.

B.1 CÁC THÍ NGHIỆM BỔ SUNG VỀ CÁC BIẾN THỂ CHUYỂN GIAO KIẾN THỨC DỰA TRÊN CHƯNG CẤT

Trong phần con sau, chúng tôi trình bày các thí nghiệm bổ sung được tiến hành để tăng cường hiệu suất của các biến thể chuyển giao kiến thức cho việc chuyển giao kiến thức giữa các mô hình đã được huấn luyện trước.

Sử dụng mục tiêu chuyển giao chưng cất cộng cross-entropy. Như một thay thế cho phân kỳ KL được sử dụng trong Phương trình (1) chúng tôi bổ sung điều tra tiềm năng của việc sử dụng mất mát cross-entropy giữa các mục tiêu mềm của mô hình giáo viên và học sinh, tương tự như Hinton et al. (2015). Tuy nhiên, kết quả của chúng tôi cho thấy không có lợi thế trong việc sử dụng mất mát cross-entropy so với phân kỳ KL. Thực tế, chúng tôi quan sát thấy một delta chuyển giao trung bình thấp hơn 1.2 điểm phần trăm khi sử dụng mất mát cross-entropy so với phân kỳ KL trên một tập hợp 60 cặp giáo viên-học sinh. Chúng tôi cũng khám phá việc sử dụng một epoch khởi động nơi chỉ các lớp tuyến tính của mô hình học sinh được huấn luyện sử dụng mất mát phân kỳ KL, nhưng không thấy cải thiện trong hiệu suất chuyển giao.

Hạn chế tập hợp các lớp để tính toán mất mát chuyển giao dựa trên chưng cất. Trong các thí nghiệm bổ sung của chúng tôi, chúng tôi điều tra tác động của việc hạn chế mất mát chưng cất để tập trung chỉ vào top-10 hoặc top-100 lớp có khả năng cao nhất. Phương pháp này nhằm giải quyết thách thức do số lượng lớp lớn trong tập dữ liệu ImageNet, cụ thể là thành kiến tiềm năng hướng tới việc phù hợp với đuôi dài của các phân phối mục tiêu mềm. Để đánh giá giả thuyết này, chúng tôi so sánh phân kỳ KL giữa các mục tiêu mềm đầy đủ và các tập con của các mục tiêu mềm. Bằng cách chọn top-10 và top-100 lớp có khả năng cao nhất dựa trên dự đoán của giáo viên, chúng tôi quan sát thấy rằng một số cặp giáo viên-học sinh thể hiện phân kỳ cao hơn trên tất cả các lớp so với các tập con được chọn. Điều này chỉ ra ảnh hưởng của các lớp với xác suất dự đoán thấp đối với phân kỳ KL.

Chuyển giao
XE-KL
ChưngcấtChuyển giao
CD
ChưngcấtChuyển giao
CRD
ChưngcấtChuyển giao
KL+DP020406080100Tỷ lệ Thành công Chuyển giao Kiến thức [%]     

Hình 6: Tỷ lệ giáo viên tăng hiệu suất học sinh (tỷ lệ thành công) cho chưng cất tương phản (xanh lá) so với chưng cất hướng dẫn phân loại (xanh dương) và KL+DP dựa trên học liên tục (cam).

Được thúc đẩy bởi những phát hiện này, chúng tôi tiếp tục kiểm tra tác động của việc chỉ xem xét top-10 hoặc top-100 lớp đối với hiệu suất chuyển giao. Qua sáu cặp giáo viên-học sinh, việc sử dụng phân kỳ top-10 dẫn đến sự gia tăng trung bình 0.20 điểm phần trăm trong delta chuyển giao. Hơn nữa, chúng tôi quan sát thấy rằng mức độ cải thiện phù hợp với sự khác biệt giữa phân kỳ KL top-10 và tổng. Các phát hiện của chúng tôi cho thấy rằng việc hạn chế phân kỳ cho các lớp được chọn có thể có lợi khi xử lý số lượng lớp lớn, mặc dù mức độ cải thiện vẫn hạn chế.

Chưng cất tương phản cho việc chuyển giao kiến thức giữa các mô hình tùy ý Để hiểu mức độ phù hợp của các kỹ thuật chưng cất tương phản cho việc chuyển giao kiến thức giữa các

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

2 4 6 81.02.03.04.0Tăng Kiến ThứcChuyển giao KL-Dist.Mô hình Học sinh Trafo
CNN
MLP
Trafo
2 4 6 8123Mô hình Học sinh CNN
0 2 4 60246810Mô hình Học sinh MLP
0.5 1.0 1.5 2.0
Mất Kiến Thức0.51.01.52.02.5Tăng Kiến ThứcChuyển giao KL-Dist + DP
1 2 3 4
Mất Kiến Thức123
0 1 2 3
Mất Kiến Thức0246
255075100150200Số Tham số Học sinh [M]

Hình 7: Tỷ lệ kiến thức được chuyển giao (tăng kiến thức) được hình ảnh hóa so với tỷ lệ kiến thức bị mất cho chưng cất KL vanilla và phương pháp chưng cất KL+DP được đề xuất của chúng tôi. Các mô hình học sinh được nhóm theo loại kiến trúc tương ứng của chúng. Mỗi marker đại diện cho một cặp giáo viên-học sinh. Màu sắc của các marker đại diện cho kích thước của học sinh, trong khi hình dạng marker xác định kiến trúc giáo viên. Kích thước marker hình ảnh hóa hiệu suất của giáo viên. Kết quả cho thấy lợi ích rõ ràng của KL+DP, di chuyển hầu hết các điểm đến các khu vực chuyển giao kiến thức tích cực (trên đường chéo đỏ).

mô hình đã được huấn luyện trước tùy ý, chúng tôi đo tỷ lệ thành công chuyển giao trung bình cho cả chuyển giao chưng cất CD và CRD (§A.2), với kết quả được hiển thị trong Hình 6. Chúng tôi tận dụng cùng thiết lập thí nghiệm trên ImageNet 10% như cho các phương pháp chuyển giao khác (xem §5.1). Kết quả thí nghiệm rõ ràng cho thấy các phương pháp chưng cất tương phản không thể cải thiện mô hình học sinh cho hầu hết các mô hình giáo viên. Khi kiểm tra kỹ hơn kết quả, chúng ta có thể thấy rằng các phương pháp chưng cất tương phản dẫn đến mức độ chuyển giao kiến thức tương tự từ giáo viên sang học sinh, nhưng dường như cũng gây ra việc ghi đè tổng thể mạnh hơn nhiều, khiến học sinh mất các phần lớn kiến thức trước đó của chúng. Mặc dù rất phù hợp cho chưng cất sang học sinh chưa được huấn luyện, hành vi này thật không may không phù hợp cho việc chuyển giao kiến thức giữa các mô hình chuyên gia đã được huấn luyện.

B.2 KẾT QUẢ MỞ RỘNG VỀ CHUYỂN GIAO KIẾN THỨC GIỮA CÁC MÔ HÌNH ĐÃ ĐƯỢC HUẤN LUYỆN TRƯỚC

Đối với các thí nghiệm tỷ lệ thành công chuyển giao kiến thức của chúng tôi được tiến hành trong Phần 5.1, chúng tôi cung cấp một phiên bản mở rộng và chi tiết hơn cho Hình 4a trong Hình 7. Sử dụng biểu đồ phân tán, chúng tôi liên hệ tỷ lệ kiến thức được chuyển giao cho mô hình học sinh (tăng kiến thức) so với tỷ lệ kiến thức đã được huấn luyện trước của học sinh bị ghi đè trong quá trình chuyển giao (mất kiến thức). Mỗi mô hình học sinh được biểu thị bằng một lựa chọn màu tương ứng liên kết với số lượng tham số của nó. Kích thước và màu sắc ký hiệu biểu thị cả họ và hiệu suất của các mô hình giáo viên tương ứng. Đường đỏ biểu thị một sự đánh đổi bằng nhau giữa tăng và mất kiến thức, với các mục nhập trên đường chéo chỉ ra một chuyển giao kiến thức tích cực. So sánh kết quả của chuyển giao KL-Dist. vanilla và chuyển giao Kl+DP dựa trên học liên tục, chúng ta thấy rằng đa số các điểm được đẩy lên đường chéo, cho phép chuyển giao ngay cả từ các mô hình yếu hơn (ký hiệu nhỏ, phân tán nặng hướng tới khu vực đường chéo dưới trong phương pháp chưng cất kiến thức bình thường). Hành vi này cũng làm nổi bật rằng các phương pháp chưng cất kiến thức bình thường thường ghi đè kiến thức thay vì tăng cường, và được phản ánh trong các nghiên cứu tương quan của chúng tôi trong Hình 4a.

Nhìn chung, những kết quả này chỉ đơn giản mở rộng những hiểu biết được cung cấp trong phần chính của công trình này từ một góc nhìn chi tiết hơn, làm nổi bật rằng một cách xử lý học liên tục của vấn đề chuyển giao kiến thức có thể nâng cao tỷ lệ thành công chuyển giao một cách đáng kể. Tuy nhiên, chúng tôi lưu ý rằng góc nhìn chi tiết hơn này cung cấp hỗ trợ tốt hơn về khía cạnh có hại của các thành kiến quy nạp thị giác mạnh hơn cho việc chuyển giao kiến thức tổng quát, vì chúng tôi thấy các học sinh CNN thường hoạt động tệ nhất, ngay cả khi tận dụng chuyển giao KL+DP.

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 7: Kết quả Chuyển giao Kiến thức trên ImageNet đầy đủ, từ bốn giáo viên đến tám mô hình học sinh được chọn. Các bảng bao gồm các delta chuyển giao riêng lẻ của tất cả các cặp giáo viên-học sinh.

∆transf. KL-Dist. ∆transf. KL-Dist.+DP ∆transf. KL-Dist.+DP (không giám sát)
Giáo viên → SWSL-V olo-D2 ResMLP36CoaT- SWSL-V olo-D2 ResMLP36CoaT- SWSL-V olo-D2 ResMLP36CoaT-
↓Học sinh ResNext101 lite-mini ResNext101 lite-mini ResNext101 lite-mini
XCiT-P16 0.95 1.40 -0.55 -0.88 0.93 0.90 0.36 0.42 1.29 1.45 0.57 0.52
PiT-B 1.16 1.42 -0.24 -0.74 0.74 0.86 0.31 0.31 1.35 1.59 0.43 0.45
PiT-XS 0.54 0.43 0.14 -0.71 0.55 0.44 0.37 0.23 0.51 0.53 0.29 0.08
SeNet154 0.38 -0.07 -0.20 -0.32 0.38 0.02 0.22 0.48 0.42 0.13 0.09 0.38
ConvNext 0.23 0.41 -1.10 -1.57 0.49 0.44 0.26 0.12 0.44 0.38 0.22 0.09
ResNetV2 0.34 0.11 -0.23 -0.56 0.32 0. 17 0.28 0.13 0.34 0.18 0.29 0.11
Mixer-B16 0.32 0.22 -0.64 -1.07 0.31 0.22 0.11 -0.05 0.35 0.26 0.07 -0.02
ResMLP-24 0.58 0.43 -0.16 -0.26 0.57 0.45 0.10 0.20 0.57 0.36 0.10 0.13

Bảng 8: Bảng dưới đây cho thấy kết quả của việc chuyển giao kiến thức với phương pháp chuyển giao KL-Dist. + DP được đề xuất của chúng tôi trên ImageNet đầy đủ. Nó bao gồm hai chỉ số mô tả các thay đổi trong các đảo dự đoán tích cực và âm, và mở rộng thông tin được cung cấp trong Bảng 1. Đối với mỗi học sinh, chúng tôi báo cáo trung bình và độ lệch chuẩn trên tất cả các mô hình giáo viên, có thể được tìm thấy trong Bảng 6.

Học sinh Loại Độ chính xác # Tham số ∆transf. ∆ρpos ∆ρneg
XCiT-P16 (El-Nouby et al., 2021) Trafo 82.89 189.10 0.65 (±0.26) -0.74 (±0.25) -0.08 (±0.04)
PiT-B (Heo et al., 2021) Trafo 82.44 73.76 0.55 (±0.25) -0.64 (±0.28) -0.08 (±0.05)
PiT-XS (Heo et al., 2021) Trafo 78.19 10.62 0.40 (±0.12) -0.45 (±0.13) -0.05 (±0.04)
SeNet154 (He et al., 2018) CNN 81.23 115.09 0.27 (±0.17) -0.35 (±0.14) -0.07 (±0.04)
ConvNext (Liu et al., 2022) CNN 84.57 50.22 0.33 (±0.14) -0.37 (±0.09) -0.04 (±0.06)
ResNetV2 (He et al., 2016b) CNN 82.80 25.55 0.23 (±0.08) -0.35 (±0.09) -0.13 (±0.03)
Mixer-B16 (Tolstikhin et al., 2021) MLP 82.30 59.88 0.15 (±0.13) -0.16 (±0.08) -0.01 (±0.07)
ResMLP-24 (Touvron et al., 2021) MLP 80.76 30.02 0.33 (±0.19) -0.30 (±0.16) +0.03 (±0.04)

Bảng sau đây cho thấy các delta chuyển giao riêng lẻ của các cặp giáo viên-học sinh từ Bảng 1 và Bảng 3.

Để hỗ trợ thêm phân tích của chúng tôi trong Phần 5.1, chúng tôi cung cấp kết quả bổ sung liên quan đến sự thay đổi trong tỷ lệ các đảo dự đoán tích cực và âm trong quá trình chuyển giao kiến thức. Các đảo dự đoán tích cực ρpos đề cập đến các trường hợp nơi giáo viên đúng, nhưng học sinh sai. Ngược lại, các đảo dự đoán âm ρneg đề cập đến các trường hợp nơi giáo viên sai, nhưng học sinh đúng. Để đo sự thay đổi này, chúng tôi định nghĩa hai chỉ số mới, delta flip tích cực ∆ρpos và delta flip âm ∆ρneg, tương tự như delta chuyển giao. Chúng tôi trình bày trung bình và độ lệch chuẩn cho cả hai chỉ số cho tất cả các mô hình học sinh sử dụng phương pháp chuyển giao KL+DP của chúng tôi trong Bảng 8, mở rộng kết quả từ Bảng 1.

Mục tiêu của chúng tôi với việc chuyển giao kiến thức là chuyển giao kiến thức bổ sung, tức là các đảo dự đoán tích cực. Điều này có nghĩa là số lượng mẫu nơi giáo viên đúng nhưng học sinh sai nên giảm càng nhiều càng tốt. Tuy nhiên, chúng ta phải đồng thời bảo tồn kiến thức trước đó của học sinh. Kết quả là, số lượng mẫu nơi học sinh đúng và giáo viên sai (các đảo dự đoán âm) không nên giảm.

Kết quả thí nghiệm chứng minh một cách thuyết phục hiệu quả của phương pháp của chúng tôi trong việc giảm tỷ lệ các đảo dự đoán tích cực cho tất cả các mô hình học sinh. Điều này gạch dưới khả năng của phương pháp của chúng tôi để chuyển giao kiến thức bổ sung giữa các mô hình. Hơn nữa, những thay đổi nhỏ trong các đảo dự đoán âm cung cấp bằng chứng thuyết phục về khả năng của phương pháp để bảo tồn kiến thức trước đó của học sinh.

B.3 KẾT QUẢ MỞ RỘNG VỀ TÁC ĐỘNG CỦA CÁC THUỘC TÍNH MÔ HÌNH HỌC SINH KHÁC NHAU ĐỐI VỚI CHUYỂN GIAO KIẾN THỨC

Trong phần này, chúng tôi cung cấp một đánh giá gần gũi hơn về tác động của các thuộc tính mô hình học sinh đối với hành vi chuyển giao kiến thức, như được đo thông qua delta chuyển giao. Cụ thể, chúng tôi xem xét hiệu suất, kích thước (được đo bằng số lượng tham số) và họ mô hình. Đối với đánh giá này, chúng tôi chọn cho mỗi thuộc tính mô hình các cặp hoặc bộ ba học sinh với các giá trị tương tự cho hai trong ba thuộc tính để cô lập mỗi biến đơn lẻ càng tốt càng có thể. Mặc dù một can thiệp chính xác không thể được thực hiện bằng cách chỉ tận dụng các mô hình đã được huấn luyện trước, thiết lập này cung cấp những hiểu biết có kiểm soát hơn, mà chúng tôi hình ảnh hóa trong Hình 8 cho các thí nghiệm được tiến hành trên ImageNet 10% sử dụng phương pháp chuyển giao KL+DP.

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

75.0 77.5 80.0 82.5 85.0
Độ Chính Xác Học Sinh−0.50.00.51.01.5Delta Chuyển Giao Kiến Thức
a) Hiệu Suất Học Sinh
5 10 20 40 80120200
Tham Số Học Sinh−2−101
b) Kích Thước Học Sinh
Nhỏ Trung Bình Lớn Rất Lớn
Kích Thước Mô Hình Học Sinh−0.50.00.5
c) Kiến Trúc Học Sinh
Trafo
CNN
MLP

Hình 8: Đánh giá tác động của các thuộc tính mô hình học sinh a) hiệu suất, b) kích thước (được đo bằng số lượng tham số) và c) loại kiến trúc đối với delta chuyển giao kiến thức. Mỗi marker đại diện cho một mô hình học sinh được chọn được chưng cất với 20 mô hình giáo viên khác nhau. Chúng tôi nhóm học sinh thành các cặp hoặc bộ ba dựa trên các thuộc tính mô hình còn lại bằng cách kết nối các marker tương ứng.

Lưu ý rằng mỗi marker đại diện cho một mô hình học sinh được đánh giá với tất cả 20 mô hình giáo viên. Chúng tôi kết nối các cặp hoặc bộ ba học sinh có thể được so sánh, với màu sắc của các đường và marker đại diện cho họ mô hình của học sinh.

Kết quả của chúng tôi sao chép những hiểu biết được lưu ý trong phần chính của công trình này, đặc biệt là Hình 5 (phải). Chúng tôi thấy rằng ngay cả khi kiểm soát các yếu tố khác như độ chính xác ban đầu, khả năng học sinh tổng thể dường như có tương quan mạnh với khả năng nhận kiến thức mới mà không ghi đè trước đó. Đây là một hành vi đặc biệt rõ ràng trong các mô hình với thành kiến quy nạp thị giác mạnh như CNN. Hình con bên phải nhất cho thấy rằng khi nhìn vào hành vi trung bình của một họ mô hình (được chia thành các kích thước mô hình khác nhau), rằng quy mô có thể mang lại khả năng chuyển giao nổi lên trong CNN - mặc dù không có sẵn trước đó - đối với bất kỳ loại kiến trúc cụ thể nào - kích thước tăng có thể cho phép khả năng chuyển giao được cải thiện đáng kể.

B.4 KẾT QUẢ MỞ RỘNG VỀ CÁC TẬP DỮ LIỆU BỔ SUNG

Để củng cố kết quả của chúng tôi trên ImageNet, chúng tôi bổ sung tiến hành các thí nghiệm trên các tập dữ liệu CUB200 Wah et al. (2011), Caltech256 Griffin et al. (2007), và Stanford-Cars Krause et al. (2013). Đối với mỗi tập dữ liệu, chúng tôi kết hợp chín mô hình học sinh và bốn mô hình giáo viên như được hiển thị trong Bảng 6 dẫn đến tổng cộng 36 kết hợp giáo viên-học sinh. Chúng tôi tinh chỉnh lớp phân loại của các mô hình học sinh và giáo viên sử dụng dữ liệu cụ thể cho tập dữ liệu trước khi bắt đầu chuyển giao kiến thức. Chúng tôi sử dụng dữ liệu huấn luyện của tập dữ liệu như tập chuyển giao.

−10 0 10
Sự Khác Biệt Hiệu Suất của Giáo Viên và Học Sinh−7.5−5.0−2.50.02.55.0Delta Chuyển Giao Kiến Thức
Chuyển giao KL-Dist.
Chuyển giao XE-KL-Dist. + MCL
Chuyển giao KL-Dist. + DP
(a) CUB200

−20 −15 −10 −5 0 5
Sự Khác Biệt Hiệu Suất của Giáo Viên và Học Sinh−7.5−5.0−2.50.02.55.0Delta Chuyển Giao Kiến Thức
Chuyển giao KL-Dist.
Chuyển giao XE-KL-Dist. + MCL
Chuyển giao KL-Dist. + DP (b) Caltech256

−20 0 20
Sự Khác Biệt Hiệu Suất của Giáo Viên và Học Sinh−7.5−5.0−2.50.02.55.0Delta Chuyển Giao Kiến Thức
Chuyển giao KL-Dist.
Chuyển giao XE-KL-Dist. + MCL
Chuyển giao KL-Dist. + DP (c) Stanford Cars

Hình 9: Delta chuyển giao kiến thức dựa trên sự khác biệt hiệu suất giáo viên-học sinh cho ba tập dữ liệu bổ sung: a) CUB200, b) Caltech256, và c) Stanford Cars. Chúng tôi so sánh chuyển giao KL-Dist. đơn giản với chuyển giao XE-KL-Dist.+MCL và Chuyển giao KL-Dist.+DP. Các cặp giáo viên-học sinh được phân loại thành các nhóm được xác định bởi các phân vùng đều của các sự khác biệt hiệu suất tương ứng của chúng. Để giảm thiểu ảnh hưởng của các ngoại lệ, chúng tôi báo cáo delta chuyển giao trung bình của top 25% trong mỗi nhóm và phương pháp.

Qua tất cả các tập dữ liệu, chúng tôi liên tục quan sát phương pháp chuyển giao KL-Dist.+DP không chỉ cho phép chuyển giao kiến thức từ các giáo viên kém thành thạo hơn mà không làm tổn hại đến hiệu suất của học sinh mà còn chứng minh khả năng chuyển giao các phần kiến thức đáng kể trong các trường hợp nơi giáo viên vượt trội so với học sinh một cách đáng kể, phù hợp với hiệu quả của chuyển giao KL-Dist. đơn giản. Những kết quả này phù hợp với các quan sát của chúng tôi trên ImageNet (c.f. Hình 4b) và gạch dưới những điểm mạnh của chuyển giao KL+DP.

B.5 KẾT QUẢ MỞ RỘNG VỀ CHUYỂN GIAO KIẾN THỨC DƯỚI ĐIỀU KIỆN CHUYỂN MIỀN

Chúng tôi tiếp tục khám phá việc chuyển giao kiến thức trong bối cảnh chuyển miền giữa mô hình giáo viên và học sinh. Với mục đích này, chúng tôi tinh chỉnh mô hình giáo viên trên tập dữ liệu domainnet infograph Peng et al. (2019) trước khi tiến hành chuyển giao kiến thức. Quá trình chuyển giao được thực hiện trên tập con 10% của ImageNet. Đánh giá toàn diện của chúng tôi bao gồm một nhóm 9 mô hình học sinh khác biệt và 4 mô hình giáo viên (xem Bảng 6).

−30 −20 −10 0
Sự Khác Biệt Hiệu Suất của Giáo Viên và Học Sinh−10.0−7.5−5.0−2.50.0Delta Chuyển Giao Kiến Thức
Chuyển giao KL-Dist.
Chuyển giao XE-KL-Dist. + MCL
Chuyển giao KL-Dist. + DP

Hình 10: Phân tích delta chuyển giao kiến thức liên miền cho chuyển giao KL-Dist. và KL-Dist.+DP. Chúng tôi điều tra delta chuyển giao từ việc chuyển giao kiến thức từ một mô hình giáo viên được huấn luyện trên DomainNet Infograph đến một mô hình học sinh đã được huấn luyện trước ImageNet.

Đáng chú ý, các phát hiện của chúng tôi gạch dưới hiệu quả của phương pháp chuyển giao KL-Dist.+DP, điều này tạo điều kiện cho việc chuyển giao kiến thức từ giáo viên được huấn luyện Infograph đến mô hình học sinh trên miền ImageNet, do đó cải thiện hiệu suất của học sinh. Ngược lại, chuyển giao KL-Dist. thông thường cho thấy sự giảm đáng kể trong độ chính xác của học sinh, đặc biệt khi sử dụng một giáo viên kém thành thạo hơn.

B.6 KẾT QUẢ MỞ RỘNG VỀ CHUYỂN GIAO TỪ NHIỀU GIÁO VIÊN

0 10 20 30 40 50 60
Epoch Chuyển Giao0.51.01.52.02.53.03.5Tăng/Mất Kiến Thức [%] ResMLP-36 SWSL-ResNext VOLO-D2Tăng Kiến Thức
Mất Kiến Thức
Delta Chuyển Giao Kiến Thức

Hình 11: Chuyển giao kiến thức (tăng kiến thức) và mất kiến thức trước đó của học sinh (mất kiến thức) trong quá trình huấn luyện tuần tự của PiT-B Heo et al. (2021) với ba mô hình giáo viên khác nhau được sắp xếp theo hiệu suất tăng dần.

Cuối cùng, chúng tôi trình bày những hiểu biết bổ sung về việc chuyển giao kiến thức tuần tự từ nhiều mô hình giáo viên đến một mô hình học sinh đã được huấn luyện trước duy nhất. Đối với tất cả các thí nghiệm chuyển giao kiến thức đa giáo viên, chúng tôi chọn ba mô hình học sinh (XCiT-P16, Twins, PiT-B) và ba mô hình giáo viên (SWSL-ResNext101, VOLO-D2, ResMLP-36) từ Tab. 6. Phụ lục B.6 hình ảnh hóa việc chuyển giao kiến thức (tăng kiến thức), tỷ lệ kiến thức huấn luyện trước của học sinh bị mất (mất kiến thức) và delta chuyển giao tổng thể qua các epoch chuyển giao cho mô hình học sinh PiT-B Heo et al. (2021) được trình bày trong §5.2. Như đã lưu ý ở đó, chúng tôi chưng cất học sinh với ba mô hình giáo viên khác nhau (xem Bảng 6). Đối với hình ảnh hóa cụ thể này, chúng tôi sắp xếp giáo viên theo hiệu suất tăng dần, nhưng thấy chuyển giao liên tục tích cực cũng có thể đạt được từ các chuỗi khác. Đối với mỗi giáo viên, chúng tôi phân bổ một ngân sách chuyển giao cố định 20 epoch. Như đã lưu ý trong Bảng 2, hình làm nổi bật trực quan rằng các delta chuyển giao tích cực có thể được đạt được từ một giáo viên đến giáo viên tiếp theo (delta chuyển giao mạnh hơn so với học sinh đơn mạnh nhất, ∆dist=1.04), nhưng với lợi nhuận giảm dần. Chúng ta có thể quy điều này cho tỷ lệ quên tăng - trong khi tăng kiến thức đang tăng đều đặn, việc di chuyển liên tục học sinh khỏi trọng số huấn luyện trước ban đầu của nó gây ra mất kiến thức mạnh hơn ngày càng tăng, ngay cả khi tận dụng chuyển giao Kl+DP.

Để có thêm hiểu biết, chúng tôi so sánh kết quả của các thí nghiệm đa giáo viên của chúng tôi sử dụng chuyển giao KL-Dist.+DP với chuyển giao KL-Dist. vanilla (Tab. 9). Kết quả rõ ràng cho thấy rằng chuyển giao KL-Dist. tuần tự

--- TRANG 20 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 9: Chuyển giao kiến thức từ nhiều giáo viên vào một học sinh đã được huấn luyện trước sử dụng chuyển giao KL-Dist. vanilla tuần tự và dựa trên soup (c.f. §4.3). Chúng tôi so sánh với các delta chuyển giao thu được từ việc chuyển giao kiến thức giáo viên đơn.

Học sinh Loại Độ chính xác # Tham số∆transf. Giáo viên Đơn ∆transf. Nhiều Giáo viên
Trung bình Tối thiểu Tối đa Tuần tự Soup
XCiT-P16 El-Nouby et al. (2021) Transf. 82.89 189.1 0.47 -0.85 1.31 0.48 0.89
Twins Chu et al. (2021) Transf. 83.68 99.27 -0.04 -1.04 0.63 0.01 0.43
PiT-B Heo et al. (2021) Transf. 82.44 73.76 0.69 -0.24 1.39 0.80 1.19

chuyển giao không thể đạt được lợi ích lớn hơn so với chỉ giáo viên tốt nhất mà dẫn đến lợi ích hiệu suất trong phạm vi delta chuyển giao trung bình qua ba giáo viên. Điều này một lần nữa cho thấy rằng thay vì chỉ chuyển giao kiến thức bổ sung, chuyển giao KL-Dist. vanilla ghi đè kiến thức trước đó của học sinh bằng kiến thức của mô hình giáo viên. Do đó khi chuyển giao kiến thức tuần tự từ nhiều giáo viên, những cải thiện từ việc chuyển giao trước đó bị mất trong quá trình chuyển giao từ giáo viên tiếp theo. Lưu ý rằng phương pháp chuyển giao KL-Dist. vanilla không thể được áp dụng trực tiếp để chuyển giao kiến thức từ nhiều mô hình giáo viên song song, do đó chúng tôi bỏ qua baseline này.
