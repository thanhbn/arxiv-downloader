# 2310.18628.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.18628.pdf
# Kích thước tệp: 911175 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Chưng cất Cá nhân hóa: Trao quyền cho các LLM Mã nguồn mở với Học tập Thích ứng cho Sinh mã
Hailin Chen∗♣♠, Amrita Saha∗♠, Steven HOI♠, Shafiq Joty♣♠
♣Đại học Công nghệ Nanyang, Singapore
♠Nghiên cứu Salesforce
{hailin001, srjoty}@ntu.edu.sg
{amrita.saha, shoi}@salesforce.com
Tóm tắt
Với sự nổi lên của các LLM mã nguồn đóng mạnh mẽ
(ChatGPT, GPT-4), có ngày càng nhiều quan tâm
trong việc chưng cất các khả năng của các LLM mã
nguồn đóng sang các LLM mã nguồn mở nhỏ hơn. Các
phương pháp chưng cất trước đây thường nhắc
ChatGPT để tạo ra một tập hợp các hướng dẫn và câu
trả lời, để mô hình học sinh học hỏi. Tuy nhiên,
cách tiếp cận chưng cất tiêu chuẩn như vậy bỏ qua
các ưu điểm và điều kiện của mô hình học sinh.
Được truyền cảm hứng từ các nguyên tắc giảng dạy
hiện đại, chúng tôi thiết kế một quy trình chưng cất
cá nhân hóa, trong đó học sinh cố gắng giải quyết
một nhiệm vụ trước, sau đó giáo viên cung cấp một
sự cải tiến thích ứng để học sinh cải thiện. Thay vì
cho học sinh ăn kiến thức trước của giáo viên, chưng
cất cá nhân hóa cho phép học tập cá nhân hóa cho
mô hình học sinh, vì nó chỉ học trên các ví dụ mà
nó mắc lỗi và học cách cải thiện giải pháp của chính
mình. Trong sinh mã, chưng cất cá nhân hóa liên tục
vượt trội hơn chưng cất tiêu chuẩn chỉ với một phần
ba dữ liệu. Chỉ với 2.5-3K ví dụ cá nhân hóa mà
phát sinh chi phí thu thập dữ liệu 4-6$, chúng tôi
tăng CodeGen-mono-16B lên 7% để đạt 36.4% pass@1
và StarCoder lên 12.2% để đạt 45.8% pass@1 trên
HumanEval.1
1 Giới thiệu
Gần đây, các mô hình ngôn ngữ lớn mã nguồn đóng
mạnh mẽ (LLMs) bao gồm ChatGPT, GPT-4 đã trở
nên chiếm ưu thế, tích lũy hơn 170 triệu người dùng
trong vòng 5 tháng kể từ khi ra mắt. Các LLM mã
nguồn đóng như vậy thể hiện hiệu suất mạnh trong
một loạt các nhiệm vụ, từ cải thiện kỹ năng viết
đến sinh mã. Tuy nhiên, do bản chất mã nguồn đóng
của chúng, các mối quan ngại đã được nêu ra liên
quan đến các yếu tố như tính khả dụng của các dịch
vụ này, chi phí cao liên quan, mối quan ngại về đạo
đức và an toàn, và các tác động tiềm ẩn về quyền
riêng tư dữ liệu, *Các tác giả này đóng góp bằng
nhau cho công trình này
1Mã của chúng tôi sẽ có sẵn tại https://github.
com/salesforce/PersDistill tất cả đều hạn chế việc
tích hợp liền mạch của chúng vào các ứng dụng thực
tế. Trước những mối quan ngại này, một câu hỏi tự
nhiên nảy sinh: Liệu chúng ta có thể chưng cất những
khả năng đáng chú ý được thể hiện bởi các LLM mã
nguồn đóng vào các LLM mã nguồn mở nhỏ hơn không?
Các nhà nghiên cứu đã khám phá ý tưởng chưng cất
như vậy (Taori et al., 2023; Wang et al., 2022; Xu
et al., 2023b), bằng cách truy vấn ChatGPT để tạo
ra các cặp hướng dẫn nhiệm vụ và giải pháp, và sử
dụng dữ liệu thu thập được để tinh chỉnh một mô hình
học sinh. Tuy nhiên, cách tiếp cận chưng cất tiêu
chuẩn này phù hợp với các mô hình học sinh khác
nhau với cùng một phân phối dữ liệu (kiến thức trước
của giáo viên), bỏ qua các khả năng và năng lực độc
đáo của chúng. Trong lĩnh vực giáo dục, học tập cá
nhân hóa cung cấp trải nghiệm học tập tùy chỉnh
thích ứng với tiến trình học tập và năng lực của học
sinh, đã được chứng minh là rất hiệu quả và được
áp dụng rộng rãi (Roberts-Mahoney et al., 2016;
Shemshack and Spector, 2020). Được truyền cảm hứng
từ phát hiện như vậy, chúng tôi đưa ra giả thuyết
rằng học tập cá nhân hóa cũng có lợi cho việc chưng
cất mô hình.

Trong công trình này, chúng tôi đề xuất chưng cất cá
nhân hóa và đánh giá thực nghiệm tính hiệu quả của
nó trong lĩnh vực sinh mã. Tương tự như chưng cất
tiêu chuẩn, trước tiên chúng tôi sử dụng ChatGPT để
tạo ra các hướng dẫn nhiệm vụ kèm theo các trường
hợp kiểm tra đơn vị. Sau đó chúng tôi thực hiện ba
bước cho chưng cất cá nhân hóa như được hiển thị
trong Hình 1. Đầu tiên, chúng tôi để mô hình học
sinh cố gắng giải quyết nhiệm vụ. Sau đó, chúng tôi
đánh giá nỗ lực của học sinh với các trường hợp kiểm
tra đơn vị và nhận phản hồi thực thi. Nếu phản hồi
thực thi chứa lỗi, trong bước cuối cùng chúng tôi
nhắc mô hình giáo viên (ChatGPT) để cải tiến nỗ lực
của học sinh. Quy trình thu thập dữ liệu như vậy làm
cho trải nghiệm học tập vừa tương tác — vì học sinh
tham gia để đưa ra nỗ lực, và cá nhân hóa — cả đầu
vào (nhiệm vụ) và đầu ra (dữ liệu cải tiến) đều được
tùy chỉnh cho học sinh. Về cơ bản, dữ liệu được gán
nhãn cá nhân hóa giúp học sinh cải tiến chính sách
của riêng mình, thay vì áp dụng một kiến thức trước
mới của giáo viên.

Với dữ liệu mã cá nhân hóa làm đầu ra mục tiêu, chúng
tôi xây dựng ba biến thể của dữ liệu tinh chỉnh (i)
dữ liệu PERsD định dạng nó như một nhiệm vụ sinh mã
từ văn bản điển hình, (ii) PERsD-refine coi nó như
một nhiệm vụ cải tiến mã, được đưa ra một hướng dẫn
nhiệm vụ, mã không chính xác và phản hồi lỗi thực
thi (iii) PERsD-combine đơn giản kết hợp dữ liệu
tinh chỉnh PERsD và PERsD-refine, tức là các nhiệm
vụ sinh mã và cải tiến.

Chúng tôi thu thập 10K ví dụ chưng cất tiêu chuẩn và
khoảng 2.5-3K ví dụ cá nhân hóa để tiền huấn luyện.
Thông qua đánh giá zero-shot trên HumanEval (Chen
et al., 2021) và MBPP (Austin et al., 2021), chúng
tôi quan sát thấy tất cả các biến thể PERsD liên tục
vượt trội hơn các đối tác sử dụng chưng cất tiêu chuẩn.
Kết quả thuyết phục này mạnh mẽ xác nhận giả thuyết
của chúng tôi về những lợi thế của chưng cất cá nhân
hóa. Các nghiên cứu loại bỏ tiếp tục củng cố giả
thuyết của chúng tôi, khám phá ra các tính chất thú
vị như lợi ích của chưng cất cá nhân hóa nhiều vòng
và khả năng của các mô hình của chúng tôi để tận
dụng phản hồi thực thi để tự sửa chữa. Đáng chú ý,
chưng cất cá nhân hóa tăng cường mô hình tiền huấn
luyện mã nguồn mở tiên tiến StarCoder (Li et al.,
2023a) đáng kể — lên 12.2% để đạt 45.8 trong pass@1
và 82.3 trong pass@100 trên HumanEval.

2 Công trình liên quan
2.1 Chưng cất từ ChatGPT
Các công trình trước đây đã khám phá chưng cất từ
ChatGPT bao gồm Alpaca(Taori et al., 2023), Vicuna(Chiang et al., 2023) và Baize(Xu et al.,
2023b). Tuy nhiên, tất cả các công trình này đều có
thể được coi là chưng cất tiêu chuẩn vì chúng không
xem xét các điều kiện và năng lực của mô hình học
sinh. Wiz-

Phương pháp Cá nhân hóa Tương tác Liên quan đến mã
Alpaca ✗ ✗ ✗
Vicuna ✗ ✗ ✗
Baize ✗ ✗ ✗
WizardLM ✗ ✗ ✗
WizardCoder ✗ ✗ ✓
Lion Đầu vào ✓ ✗
PERsD Đầu vào + Đầu ra ✓ ✓

Bảng 1: Công trình liên quan về chưng cất từ ChatGPT

ardLM(Xu et al., 2023a) và WizardCoder(Luo
et al., 2023) lặp đi lặp lại nhắc mô hình giáo viên
để tạo ra các hướng dẫn phức tạp hơn. Cách tiếp
cận của họ có thể được xem như một tiến bộ trực
giao có thể được kết hợp với chưng cất cá nhân hóa.

Lion (Jiang et al., 2023) đề xuất tích hợp câu trả
lời của mô hình học sinh và lấy mẫu nhiều nhiệm vụ
khó hơn mà học sinh không thể giải quyết. Do đó,
Lion có thể được coi là chưng cất cá nhân hóa đầu
vào vì chỉ các nhiệm vụ đầu vào được tùy chỉnh cho
các học sinh khác nhau. Cách tiếp cận của chúng tôi
khác biệt vì chúng tôi cung cấp tùy chỉnh cả đầu vào
và đầu ra, và chúng tôi chứng minh thực nghiệm rằng
việc cá nhân hóa nhãn là có lợi một cách quan trọng.

2.2 Sinh mã với Phản hồi
Gần đây, đã có một lượng ngày càng tăng nghiên cứu
về việc khám phá cách sử dụng phản hồi cho việc
sinh mã lặp và cải thiện thông qua cải tiến mã.
Self-refine(Madaan et al., 2023), Self-debug(Chen
et al., 2023b) và Reflexion (Shinn et al., 2023) là
các phương pháp thời gian suy luận sử dụng các LLM
mã nguồn đóng mạnh mẽ để tạo ra mã tốt hơn từ
phản hồi bên trong hoặc bên ngoài. Mặc dù chúng
cho thấy hiệu suất cao,

--- TRANG 2 ---
Hình 1: Tổng quan về khung của chúng tôi. Trái: chưng cất tiêu chuẩn .1Giáo viên tạo ra câu trả lời tiêu chuẩn cho một
vấn đề đã cho để học sinh học hỏi Phải: chưng cất cá nhân hóa .1Học sinh trước tiên tạo ra nỗ lực riêng để
giải quyết nhiệm vụ. 2Trình thực thi đánh giá mã được tạo ra với các trường hợp kiểm tra đơn vị. 3Giáo viên cung cấp cải tiến thích ứng
được đưa ra nỗ lực của học sinh và phản hồi thực thi của nó.

--- TRANG 3 ---
Phương pháp Huấn luyện Suy luận
Nguồn dữ liệu đơn Cá nhân hóa w/ thực thi w/o
Mô hình phản hồi ChatGPT
Self-refine ✓ Không Huấn luyện ✗ ✗ ✗
Self-debug ✓ Không Huấn luyện ✗ ✓ ✗
Reflexion ✓ Không Huấn luyện ✗ ✓ ✗
Self-edit ✗ GT tiêu chuẩn ✗ ✓ ✓
Self-correct ✗ Tự khám phá ✓ ✓ ✓
ILF ✗ Gán nhãn bởi con người ✓ ✓ ✓
PERsD-refine ✓ ChatGPT ✓ ✓ ✓

Bảng 2: Công trình liên quan về Sinh mã w/ phản hồi

các phương pháp này bị hạn chế vì chúng yêu cầu truy cập vào các LLM mã nguồn đóng.

Self-edit (Zhang et al., 2023) huấn luyện một trình chỉnh sửa mã riêng biệt để sửa chữa mã được tạo ra từ một LLM cơ sở.
Nhãn huấn luyện là từ câu trả lời vàng gốc,
do đó không được cá nhân hóa nhãn. Tương tự, Self-correct
(Welleck et al., 2022) huấn luyện một mô hình corrector riêng biệt để sửa chữa đầu ra từ một mô hình
generator cố định. Tuy nhiên, nhãn huấn luyện là từ
tự khám phá của mô hình corrector: lấy mẫu
nhiều cải tiến và chọn cái dẫn đến phần thưởng cao hơn. Cuối cùng, ILF (Chen et al., 2023a)
thu thập dữ liệu cải tiến mã được con người chú thích để huấn luyện một mô hình cải tiến riêng biệt trên đó. Mô hình cải tiến được sử dụng để tạo ra dữ liệu text-to-code
cho việc tinh chỉnh LLM sinh mã. Không giống như
ILF, cách tiếp cận của chúng tôi có thể mở rộng hơn vì chúng tôi không yêu cầu chú thích của con người và dữ liệu cá nhân hóa của chúng tôi chứng minh hiệu quả đáng kể hơn ILF như chúng tôi điều tra thực nghiệm trong §5.

2.3 Học tăng cường từ Phản hồi (Con người)
Sau khi ra mắt ChatGPT, việc điều chỉnh các LLM theo sở thích của con người đã thu hút sự chú ý to lớn của các cộng đồng nghiên cứu. Là một trong những cách tiếp cận có ảnh hưởng nhất trong hướng này, học tăng cường từ phản hồi của con người (RLHF) (Ouyang
et al., 2022; Li et al., 2023b), áp dụng một khung actor-critic, trong đó mô hình học sinh được tối ưu hóa
để tạo ra các phản hồi nhận được phần thưởng cao hơn từ
mô hình critic. Trong InstructGPT (Ouyang et al.,
2022), critic (mô hình phần thưởng) được huấn luyện từ
chú thích của con người. Tối ưu hóa Sở thích Trực tiếp (DPO) (Rafailov et al., 2023) bỏ qua nhu cầu
huấn luyện một mô hình phần thưởng, bằng cách sử dụng một
LLM tham chiếu và các quỹ đạo ngoại tuyến để ước tính phần thưởng.
Chain-of-Hindsight (Liu et al., 2023) chuyển đổi các chú thích sở thích của con người thành phản hồi ngôn ngữ tự nhiên đơn giản, và do đó chuyển tối ưu hóa RL
thành sinh có điều kiện. Trong các phương pháp trên, giả định là không có mục tiêu chân lý cơ sở và do đó chúng cố gắng cải thiện LLM dựa trên
đánh giá (critic) của nhiều đầu ra được tạo ra.
Tuy nhiên, việc huấn luyện kiểu RL như vậy sẽ kém hiệu quả và hiệu quả hơn so với tinh chỉnh có giám sát, đặc biệt đối với các nhiệm vụ thách thức với phần thưởng thưa thớt –
ví dụ: giải các câu đố toán học hoặc các nhiệm vụ lập trình. Không giống như
các phương pháp này, cách tiếp cận của chúng tôi có thể thu được đầu ra "chân lý cơ sở" từ một giáo viên cá nhân hóa, do đó tinh chỉnh có giám sát có thể được áp dụng làm cho
việc học tập hiệu quả và hiệu quả, ngay cả đối với các
nhiệm vụ thách thức như giải quyết các vấn đề lập trình.

3 Phương pháp
3.1 Chưng cất Tiêu chuẩn
Giả sử một tập dữ liệu của các nhiệm vụ sinh mã
D={(t, u)} trong đó mỗi vấn đề (hoặc nhiệm vụ)
bao gồm một hướng dẫn nhiệm vụ t và một bộ sưu tập kiểm tra đơn vị u. Trong quá trình huấn luyện, chúng ta có quyền truy cập
vào một mô hình giáo viên πϕ và một mô hình học sinh πθ.
Mục tiêu là chưng cất cách giáo viên giải quyết
các nhiệm vụ sinh mã cho mô hình học sinh, trong
bối cảnh của D. Đối với mỗi nhiệm vụ (t, u), trước tiên chúng ta truy vấn
giáo viên πϕ(t) với hướng dẫn nhiệm vụ, để nhận một
đoạn mã được tạo ra trực tiếp cϕ. Sau đó, chúng ta thực thi
mã được tạo ra cϕ với các trường hợp kiểm tra đơn vị u
và nhận phản hồi thực thi f←EXEC (cϕ, u),
trong đó hàm EXEC trả về passed nếu mã
vượt qua tất cả các kiểm tra đơn vị, nếu không nó trả về
một thông báo lỗi từ trình thực thi. Bằng cách lọc ra
các nhiệm vụ mà cϕ không vượt qua tất cả các kiểm tra đơn vị
(tức là, f̸=passed ), chúng ta nhận được một tập dữ liệu sạch mới
DSTAND={(t, u, c )}, trong đó mỗi nhiệm vụ bao gồm
một hướng dẫn nhiệm vụ t, một bộ kiểm tra đơn vị u và một
mã giải pháp chính xác c.

Sau đó chúng ta tinh chỉnh mô hình học sinh πθ trên
{(u, c)} ∼ D STAND, trong đó đầu vào là hướng dẫn nhiệm vụ
u và đầu ra là mã giải pháp tương ứng c. Chúng ta đặt tên cho cách tiếp cận này là STAND.

3.2 Chưng cất Cá nhân hóa
Cách tiếp cận STAND đơn giản lấy mẫu các ví dụ huấn luyện
(hướng dẫn và nhãn) từ phân phối trước của mô hình giáo viên và
đưa nó cho học sinh mà không xem xét các
điều kiện của mô hình học sinh. Được truyền cảm hứng bởi
các nguyên tắc giáo dục hiện đại ủng hộ
trải nghiệm học tập tương tác và cá nhân hóa,
chúng tôi đề xuất chưng cất cá nhân hóa: thích ứng
tài liệu giảng dạy với kiến thức hiện tại của học sinh

--- TRANG 4 ---
Thuật toán 1 chưng cất cá nhân hóa cho sinh mã
(PERsD-combined).
1:Đầu vào: Tập dữ liệu DSTAND, LLM học sinh πθ, trình thực thi kiểm tra đơn vị EXEC, mẫu cải tiến Trefine, LLM giáo viên
πϕ
2:Drefine←{}▷dữ liệu cải tiến cho tinh chỉnh
3:Dcode←{}▷dữ liệu sinh trực tiếp
4:for(t, u, c )∈ D STANDdo
5: cθ←πθ(t) ▷học sinh tạo ra cθ
6: f←EXEC(cθ, u) ▷phản hồi thực thi cho cθ
7: iff̸=passed then
8: // cải tiến cá nhân hóa từ giáo viên
9: crefine←πϕ(t, cθ, f)
10: // tạo hướng dẫn nhiệm vụ cải tiến
11: trefine←Trefine(t, cθ, f)
12: ifEXEC(crefine, u) =passed then
13: Drefine.insert ({trefine, crefine})
14: Dcode.insert ({t, c})
15: end if
16: end if
17: end for
18:πθ∗←FINETUNE (πθ,Drefine +Dcode)

và năng lực. Chúng tôi đề xuất ba biến thể:
PER SD-combined Thuật toán 1 hiển thị các
bước chi tiết cho PER SD-combined. Phương pháp này lấy
tập dữ liệu chưng cất tiêu chuẩn DSTAND từ §3.1 và
trước tiên để mô hình học sinh tạo ra giải pháp cho mỗi nhiệm vụ.
Sau đó nó lọc ra các nhiệm vụ mà mô hình học sinh
đã có thể giải quyết chính xác. Đối với các nhiệm vụ còn lại,
nó thu được cải tiến cá nhân hóa của giáo viên có
điều kiện trên nỗ lực của học sinh và phản hồi lỗi thực thi của nó, và chỉ giữ lại các nhiệm vụ mà
cải tiến của giáo viên là hợp lệ (tức là, vượt qua tất cả các trường hợp kiểm tra đơn vị). Hình 1 hình tượng hóa ba bước này.
Đối với tập nhiệm vụ cuối cùng này, chúng ta tạo ra hai tập dữ liệu:
i)Dcode chứa hướng dẫn nhiệm vụ làm đầu vào và
câu trả lời trực tiếp của giáo viên làm đầu ra, và ii) Drefine
chứa hướng dẫn cải tiến nhiệm vụ làm đầu vào và
câu trả lời cải tiến cá nhân hóa làm đầu ra. Hướng dẫn cải tiến nhiệm vụ (dòng 9 trong Thuật toán
1) được tạo ra bằng cách nối hướng dẫn nhiệm vụ t,
nỗ lực của học sinh cθ và phản hồi thực thi f của nó
với một mẫu cải tiến Trefine (Chi tiết thêm trong
Phụ lục C). Hướng dẫn cải tiến như vậy biến
sinh mã tiêu chuẩn thành một nhiệm vụ cải tiến mã, dạy học sinh cách cải tiến giải pháp của chính mình. PER SD-combined sau đó tinh chỉnh
mô hình học sinh trên Drefine kết hợp với Dcode.
PER SD-refine Tương tự như PER SD-combined, biến thể này
theo dòng 1-15 của Thuật toán 1 để thu thập
dữ liệu cải tiến Drefine. Tuy nhiên, nó khác với
mô hình trên vì nó chỉ sử dụng Drefine để tinh chỉnh
mô hình học sinh.

PER SD Biến thể này lấy dữ liệu huấn luyện Drefine
từ PER SD-refine và thay thế đầu vào của mỗi
điểm dữ liệu từ lời nhắc cải tiến mã thành hướng dẫn nhiệm vụ gốc. Do đó nó huấn luyện mô hình học sinh
với các nhãn cá nhân hóa về sinh mã.

Để minh họa sự khác biệt giữa cải tiến cá nhân hóa
và giải pháp trực tiếp của giáo viên, chúng tôi hiển thị
một ví dụ thực trong Hình 2. Phần trên hiển thị cải tiến cá nhân hóa cho nhiệm vụ đã cho, trong khi
phần dưới hiển thị sinh trực tiếp của giáo viên cho cùng một nhiệm vụ. Lưu ý cách sinh trực tiếp của giáo viên khác biệt đáng kể so với nỗ lực của mô hình học sinh, trong khi cải tiến của giáo viên theo nỗ lực của học sinh và cải thiện trên đó. Chúng tôi đưa ra giả thuyết rằng cải tiến thích ứng như vậy khi giáo viên điều chỉnh theo sinh của học sinh, giúp học sinh học hỏi hiệu quả và hiệu quả hơn, tương tự như cách con người được hưởng lợi từ
học tập cá nhân hóa.

Hình 2: Ví dụ: (Trên) Cải tiến cá nhân hóa từ
nỗ lực của học sinh và phản hồi thực thi; (Dưới) Giải pháp trực tiếp được tạo ra bởi giáo viên có điều kiện trên nhiệm vụ.

3.3 Suy luận Lặp
Gọi Dtest ={(t, u)} biểu thị tập kiểm tra của chúng ta cho
suy luận, trong đó mỗi điểm dữ liệu (t, u) bao gồm
một hướng dẫn nhiệm vụ t và một bộ các trường hợp kiểm tra đơn vị ẩn u. Chúng ta cũng giả sử rằng hướng dẫn nhiệm vụ chứa một số trường hợp kiểm tra đơn vị đơn giản
trong doc-string của nó (như thường thấy trong các hướng dẫn sinh mã), mà chúng ta có thể trích xuất và định dạng
sử dụng các heuristic dựa trên quy tắc để có được một bộ các trường hợp kiểm tra đơn vị đã thấy useen (Chi tiết thêm trong Phụ lục A).

--- TRANG 5 ---
Đối với suy luận một bước, chúng ta sử dụng cách tiếp cận tiêu chuẩn
để đánh giá pass@k. Cụ thể, đối với
mỗi nhiệm vụ t, chúng ta truy vấn mô hình n lần với
hướng dẫn nhiệm vụ: ci
θ←πθ(t)cho i= 1. . . n . Sau đó,
theo (Chen et al., 2021), chúng ta ước tính pass@k
từ số lần thử đã vượt qua các trường hợp kiểm tra đơn vị ẩn: E XEC (ci
θ, u) =passed .

Suy luận nhiều bước Nếu mô hình πθ đã được
huấn luyện để sửa chữa, theo cách tiếp cận của chúng ta trong
PERsD-refine hoặc PERsD-combine, và nếu các kiểm tra đơn vị
có sẵn trong quá trình suy luận, chúng ta có thể thực hiện suy luận 2 bước: đối với mỗi nỗ lực được tạo ra ci
θ trong bước 1, trước tiên chúng ta nhận phản hồi thực thi
fi
seen←EXEC (ci
θ, useen). Nếu fi
seen =passed ,
chúng ta tái sử dụng nỗ lực gốc làm nỗ lực 2 bước.
Nếu không, chúng ta tạo ra một hướng dẫn cải tiến
ti←Trefine(t, ci
θ, fi
seen) theo cách tiếp cận trong PERsD-refine hoặc PERsD-combined, và
truy vấn cùng một mô hình với hướng dẫn cải tiến
cho nỗ lực 2 bước: ci
θ,2-step←πθ(ti). Sau đó chúng ta
tính pass@k trên các sinh 2 bước
tương tự như suy luận 1 bước.

4 Thiết lập Thí nghiệm
4.1 Đường cơ sở
Đường cơ sở đầu tiên là STAND, cách tiếp cận chưng cất tiêu chuẩn
được đề cập trong §3.1.

Để đo lường tính hiệu quả của các nhãn cá nhân hóa
một cách định lượng, chúng tôi cũng so sánh với
các đường cơ sở chưng cất cá nhân hóa đầu vào, trong đó chỉ các nhiệm vụ đầu vào được lựa chọn theo cách
tùy chỉnh cho khả năng của học sinh. Tuy nhiên, các
nhãn đầu ra không được cá nhân hóa, vì chúng được lấy
từ sinh trực tiếp của giáo viên c thay vì cải tiến cá nhân hóa crefine. Chúng ta bắt đầu với Dcode
từ PER SD-combined và có ba biến thể:
INPD Chúng ta tinh chỉnh mô hình học sinh πθ trên
{(t, c)} ∼ D code, trong đó đầu vào là một hướng dẫn nhiệm vụ
và đầu ra là một giải pháp mã. Biến thể này
được tùy chỉnh hơn STAND vì nó lọc ra
các nhiệm vụ mà học sinh đã có thể giải quyết chính xác.

INPD-refine Tương tự như PERsD-refine, InpD-refine
huấn luyện mô hình học sinh để sửa chữa nỗ lực sai
của nó. Sự khác biệt là trong InpD-refine, mã
được cải tiến là từ giải pháp trực tiếp của giáo viên c,
thay vì cải tiến cá nhân hóa crefine.

INPD-combined Tương tự như PER SD-combined,
InpD-combined huấn luyện học sinh để sửa chữa câu trả lời của nó cũng như giải quyết trực tiếp nhiệm vụ. Sự khác biệt là trong InpD-combined, các nhãn cho cả cải tiến mã và sinh mã đều được lấy từ giải pháp trực tiếp của giáo viên c.

4.2 Xây dựng Dữ liệu Tiền huấn luyện
Để xây dựng dữ liệu tiền huấn luyện của chúng tôi, chúng tôi áp dụng
quy trình thu thập dữ liệu trong code-alpaca(Chaudhary,
2023) và sử dụng một tập hợp 374 nhiệm vụ hạt giống từ MBPP
(task-ids 601-974) như lời nhắc trong ngữ cảnh để truy vấn
ChatGPT cho các nhiệm vụ sinh mã mới. Tập hạt giống này tăng khả năng ChatGPT
tạo ra mã python.

Thông qua quy trình này, chúng tôi thu được một kho dữ liệu
20K nhiệm vụ sinh mã từ ChatGPT mỗi
nhiệm vụ bao gồm một hướng dẫn nhiệm vụ và mã
được tạo ra tương ứng, thường là một hàm python đơn lẻ. Tiếp theo chúng tôi hiển thị mỗi instance được tạo ra cho ChatGPT một lần nữa và nhắc nó tạo ra 5 đầu vào trường hợp kiểm tra độc đáo (tức là giá trị đối số đầu vào) cho
hàm python. Sau đó chúng tôi phân tích và định dạng
đầu vào trường hợp kiểm tra được tạo ra và thực thi mã
được tạo ra trên đó để có được một đầu ra. Do đó, trong số
20K, đối với 14880 instances chúng tôi có thể thành công
tạo ra và phân tích 5 đầu vào trường hợp kiểm tra đơn vị và đối với 10172 instances chúng tôi có thể thành công thực thi mã được tạo ra và có được đầu ra trên tất cả
5 đầu vào. Kho dữ liệu cuối cùng này gồm 10K nhiệm vụ sinh mã, mỗi nhiệm vụ bao gồm một hướng dẫn nhiệm vụ và mã
được tạo ra tương ứng cùng với 5 đầu vào và đầu ra kiểm tra đơn vị tạo thành tập dữ liệu chưng cất tiêu chuẩn DSTAND của chúng tôi.

Để thu thập dữ liệu chưng cất cá nhân hóa, chúng tôi
theo §3.2 để trước tiên yêu cầu mô hình học sinh tạo ra 1
mã đầu ra cho mỗi nhiệm vụ, đặt nhiệt độ lấy mẫu
thành 0.3. Sau đó chúng tôi đánh giá nỗ lực của học sinh và
chỉ giữ lại các nhiệm vụ với các sinh sai (tức là
những cái thất bại bất kỳ trường hợp kiểm tra đơn vị nào). Chúng tôi
sử dụng điều này để truy vấn ChatGPT cho các cải tiến cá nhân hóa và chỉ giữ lại các cải tiến hợp lệ mà
vượt qua tất cả các kiểm tra đơn vị. Lời nhắc của chúng tôi cho ChatGPT chứa hướng dẫn nhiệm vụ gốc và mã từ
DSTAND cùng với mã được tạo ra của mô hình học sinh
và phản hồi thực thi (lỗi trình biên dịch hoặc
thất bại kiểm tra đơn vị). Hướng dẫn của chúng tôi cho ChatGPT là
tạo ra một giải pháp chính xác sửa chữa các lỗi
và gần nhất về ngữ nghĩa với mã của học sinh
(Chi tiết thêm trong Phụ lục B). Bảng 3 hiển thị
thống kê của quy trình xây dựng dữ liệu cá nhân hóa.

--- TRANG 6 ---
Mô hình Học sinh # Nỗ lực Sai # Nhiệm vụ Cá Chi phí
bởi Học sinh nhân hóa Đã xác thực Dữ liệu
CodeGen-mono-6B (Nijkamp et al., 2023) 6.5K 3.25K 5.5$
CodeGen-mono-6B (round2) 4K 1.4K 4.4$
CodeGen-mono-16B 6.2K 2.8K 6.5$
StarCoder (Li et al., 2023a) 4.3K 2.5K 4.3$

Bảng 3: Thống kê Xây dựng Dữ liệu Cá nhân hóa

4.3 Đánh giá Mô hình
Chúng tôi đánh giá các mô hình của chúng tôi trên hai tập dữ liệu: HumanEval(Chen et al., 2021), chứa 164
vấn đề Python, và tập con MBPP(Austin
et al., 2021) tập được làm sạch không có sự chồng chéo với
các nhiệm vụ hạt giống MBPP của chúng tôi để thu thập dữ liệu tiền huấn luyện. Điều này tương ứng với các phần test+validation+prompt
của MBPP-sanitized và bao gồm 306
vấn đề Python. Chúng tôi sử dụng nucleus sampling với
nhiệt độ 0.2 để tạo ra 20 ứng viên cho mỗi
nhiệm vụ để ước tính pass@1, và với nhiệt độ 0.8, 100 ứng viên cho mỗi nhiệm vụ để ước tính
pass@5/10/20/50/100.

Đối với suy luận nhiều bước, trước tiên chúng tôi trích xuất
các trường hợp kiểm tra đơn vị "đã thấy" từ doc-string của
hướng dẫn nhiệm vụ (Chi tiết thêm trong Phụ lục A).
Tiếp theo, chúng tôi tạo ra các mẫu đầu ra theo kiểu sinh mã thông thường tạo thành tập hợp các sinh 1 bước cho mỗi instance. Mỗi sinh ứng viên này sau đó được thực thi trên các trường hợp kiểm tra đơn vị "đã thấy" được trích xuất để có được một mã được cải tiến, do đó tạo thành tập hợp các sinh 2 bước.

4.4 Thiết lập Tiền huấn luyện
Đối với tất cả các thí nghiệm với backbone CodeGen-mono-6B, chúng tôi sử dụng kích thước batch hiệu quả là 1024 và tiền huấn luyện trong 20 epoch. Đối với backbone là CodeGen-mono-16B, chúng tôi sử dụng kích thước batch hiệu quả là 1024
và tiền huấn luyện trong 3 epoch, vì việc huấn luyện hội tụ
nhanh hơn nhiều so với CodeGen-mono-6B. Đối với PERsD-
combine với mô hình StarCoder, chúng tôi sử dụng kích thước batch hiệu quả là 1024 và tiền huấn luyện trong 8 epoch, dẫn đến mất mát huấn luyện tương tự như CodeGen-mono-16B. Chúng tôi triển khai sử dụng HuggingFace transformers(Wolf et al., 2020) và DeepSpeed Zero (Rajbhandari et al., 2020). Tất cả các thí nghiệm được
tiến hành trên một cụm 8 GPU A100-40GB.

5 Kết quả Thí nghiệm
5.1 Kết quả Chính
Chúng tôi kiểm tra thực nghiệm giả thuyết rằng chưng cất cá nhân hóa giúp mô hình học sinh học hỏi hiệu quả hơn, bằng cách so sánh các mô hình PERsD với các phương pháp chưng cất cơ sở (InpD, StanD) trong Bảng 4.

Dữ liệu được gán nhãn cá nhân hóa thường tốt hơn dữ liệu tiêu chuẩn So sánh PERsD-combine với InpD-combine, chúng tôi thấy PERsD-combine vượt trội hơn InpD-combine trong tất cả các thiết lập, thường với biên độ đáng kể (hai backbone, hai tập dữ liệu, hai bước suy luận, 4 metric pass@k). Quan sát tương tự đúng khi so sánh PERsD-refine với InpD-refine (ngoại trừ 2/32 thiết lập), và PERsD với InpD. Do đó, chúng tôi kết luận rằng các biến thể PERsD thường tốt hơn đáng kể so với các đối tác InpD của chúng, cung cấp bằng chứng mạnh mẽ rằng các nhãn cá nhân hóa hiệu quả hơn cho mô hình học sinh học hỏi so với các nhãn tiêu chuẩn.

PERsD vượt trội hơn StanD với ít hơn một phần ba dữ liệu của nó Chúng tôi quan sát thấy PERsD vượt trội hơn StanD cho mọi pass@k trên cả backbone CodeGen-mono 16B và 6B trên cả HumanEval và MBPP, mặc dù StanD có 10K dữ liệu và PERsD chỉ có 3.3K và 2.8K ví dụ cho CodeGen-mono-6B và 16B. Ngoại lệ duy nhất là trong thiết lập CodeGen-mono-16B, MBPP, pass@1, trong đó StanD vượt qua PERsD 1.2 điểm. Cho rằng dữ liệu tiền huấn luyện của chúng tôi được xây dựng từ các nhiệm vụ hạt giống được lấy từ MBPP, chúng tôi đưa ra giả thuyết rằng StanD có thể được hưởng lợi bất công do có gấp ba lần dữ liệu hơn, làm cho nó dễ bị rò rỉ dữ liệu hơn. Chúng tôi xác minh giả thuyết như vậy thêm trong §5.2. Tóm lại, với PERsD vượt trội hơn StanD trong 15 trên 16 thiết lập trong khi có ít hơn một phần ba dữ liệu, rõ ràng là dữ liệu được gán nhãn cá nhân hóa làm cho việc học tập hiệu quả hơn.

Suy luận nhiều bước liên tục cải thiện chất lượng câu trả lời Đối với các mô hình PERsD-refine và PERsD-combine, chúng tôi thấy rằng suy luận 2 bước liên tục cải thiện hiệu suất trên HumanEval và MBPP. Điều này cho thấy các mô hình đã thành công học được cách sửa chữa giải pháp của nó dựa trên phản hồi lỗi thực thi. Lưu ý rằng InpD-refine mang lại độ chính xác tệ hơn với suy luận 2 bước trên HumanEval pass@10/20, củng cố lợi thế của dữ liệu được gán nhãn cá nhân hóa so với dữ liệu được gán nhãn tiêu chuẩn.

5.2 Phân tích chồng chéo Train-Test
Như quan sát được trong Bảng 4, các biến thể PersD được hưởng lợi từ những cải thiện trung bình cao hơn so với các đối tác InpD của chúng, trên HumanEvan hơn là trên MBPP. Để đi sâu hơn, chúng tôi tiến hành phân tích chồng chéo dữ liệu. Đối với mỗi nhiệm vụ kiểm tra, chúng tôi trích xuất nhiệm vụ huấn luyện tương tự nhất và sử dụng GPT-3.5-turbo để chấm điểm tương đồng ngữ nghĩa của chúng, với 0 chỉ ra không có mối quan hệ và 1

--- TRANG 7 ---
(a) Backbone là CodeGen-mono-6B
Phương pháp #Dữ liệu Pass@1 Pass@5 Pass@10 Pass@20
step=1 step=2 step=1 step=2 step=1 step=2 step=1 step=2
HumanEval
StanD 10K 32.41 - 41.79 - 45.67 - 49.26 -
InpD 3.3K 31.65 - 44.55 - 50.72 - 56.76 -
-refine 3.3K 29.70 29.70 43.82 41.99 51.28 47.89 58.29 53.51
-combined 6.5K 30.15 32.30 42.94 45.27 47.91 50.50 52.54 55.46
PERsD 3.3K 34.63 - 49.34 - 55.34 - 60.41 -
-refine 3.3K 32.35 33.35 48.69 49.35 56.07 56.87 63.60 64.76
-combined 6.5K 33.81 35.53 44.64 49.67 49.96 55.67 55.23 61.21
MBPP
StanD 10K 43.11 - 55.24 - 59.07 - 62.51 -
InpD 3.3K 43.59 - 55.83 - 63.13 - 67.34 -
-refine 3.3K 44.44 47.81 62.25 66.43 67.61 71.44 71.68 75.22
-combined 6.5K 42.69 47.25 56.70 62.17 61.39 66.49 65.46 70.22
PERsD 3.3K 45.47 - 59.90 - 64.85 - 69.73 -
-refine 3.3K 48.24 52.65 63.65 68.49 69.00 73.34 73.16 77.62
-combined 6.5K 42.77 48.92 56.91 62.29 61.43 66.89 65.22 70.96

(b) Backbone là CodeGen-mono-16B
Phương pháp #Dữ liệu Pass@1 Pass@5 Pass@10 Pass@20
step=1 step=2 step=1 step=2 step=1 step=2 step=1 step=2
HumanEval
StanD 10K 33.96 - 50.56 - 57.69 - 63.82 -
InpD 2.8K 36.68 - 49.51 - 53.85 - 57.47 -
-refine 2.8K 30.55 31.28 48.40 48.13 55.00 54.52 61.31 60.62
-combined 5.6K 34.66 36.49 50.65 53.89 56.75 60.07 62.78 65.85
PERsD 2.8K 37.74 - 56.57 - 63.92 - 69.97 -
-refine 2.8K 36.77 37.99 51.86 54.23 58.07 60.92 63.17 67.13
-combined 5.6K 36.40 37.74 53.57 55.80 60.81 63.37 67.3 70.50
MBPP
StanD 10K 48.90 - 62.21 - 66.91 - 71.33 -
InpD 2.8K 46.27 - 58.45 - 62.61 - 66.43 -
-refine 2.8K 48.79 54.87 66.89 71.32 72.24 75.71 75.82 78.84
-combined 5.6K 47.39 53.59 59.14 66.38 63.48 70.76 67.10 74.35
PERsD 2.8K 47.68 - 65.80 - 71.56 - 76.02 -
-refine 2.8K 51.50 56.21 66.82 71.86 72.06 76.78 76.03 80.42
-combined 5.6K 51.44 56.44 66.45 71.31 71.64 76.43 76.04 80.20

Bảng 4: So sánh các mô hình PERsD với StanD & InpD

chỉ ra sự chồng chéo ngữ nghĩa hoàn toàn (chi tiết thêm trong Phụ lục D). Bảng 5 tiết lộ nhiều chồng chéo hơn trong MBPP so với HumanEval, và nhiều chồng chéo hơn cho StanD so với PERsD. Sự chồng chéo này có thể là lý do tại sao StanD vượt qua PERsD trong thiết lập 1/16 (CodeGen-mono-16B, MBPP, pass@1), vì StanD có lợi thế bất công khi có rò rỉ dữ liệu đáng kể hơn. Ngoài ra, nếu chúng ta kiểm tra các phương pháp của chúng ta trên clean-MBPP nơi các điểm dữ liệu bị rò rỉ được loại bỏ, thì PERsD trở nên gần như ngang bằng với StanD trong thiết lập cụ thể này trong khi có biên độ lớn hơn so với StanD trên 15/16 thiết lập còn lại (từ biên độ trung bình 4.8 điểm thành 5.9 điểm, chi tiết thêm tại Phụ lục E). Tổng thể, phân tích chồng chéo này, kết hợp với kết quả từ MBPP đã làm sạch, tiếp tục nhấn mạnh những lợi thế của chưng cất cá nhân hóa.

Phương pháp Backbone %("leak") Tương đồng
HumanEval
StanD 6B,16B 6.1% 0.22
PERsD 6B 3.6% 0.18
PERsD 16B 3.05% 0.22
MBPP
StanD 6B,16B 18.24% 0.40
PERsD 6B 8.47% 0.30
PERsD 16B 7.49% 0.30

Bảng 5: Phân tích Chồng chéo Train-Test. 6B/16B biểu thị
backbone CodeGen-mono-{6/16}B. %("leak") biểu thị
tỷ lệ phần trăm dữ liệu kiểm tra bị rò rỉ ngữ nghĩa trong dữ liệu huấn luyện. 'Tương đồng' đại diện cho điểm tương đồng trung bình (phạm vi: 0 đến 1; giá trị cao hơn chỉ ra tương đồng lớn hơn)

5.3 Tác động của việc trộn dữ liệu StanD và InpD
Bảng 6 hiển thị nghiên cứu loại bỏ về việc trộn dữ liệu chưng cất tiêu chuẩn với PERsD-refine và InpD-refine: trong khi việc trộn dữ liệu tiêu chuẩn với InpD-refine cải thiện hiệu suất 1 bước của nó trên MBPP và

Phương pháp Inf Pass@1 Pass@5 Pass@10 Pass@50 Pass@100
Step HumanEval
StanD + InpD-refine
130.59 40.04 44.20 54.23 58.54
StanD + InpD-refine* 29.45 39.83 44.07 54.55 59.76
StanD + PERsD-refine 32.13 43.82 48.66 59.55 64.02
PERsD-refine 32.35 48.69 56.07 72.10 77.44
StanD + InpD-refine
230.87 42.88 47.90 58.21 60.98
StanD + InpD-refine* 30.12 42.71 47.42 58.69 64.02
StanD + PERsD-refine 35.00 47.89 52.96 64.36 69.51
PERsD-refine 33.35 49.35 56.87 74.13 79.88
MBPP
StanD + InpD-refine
142.60 53.18 56.49 62.11 63.07
StanD + InpD-refine* 44.08 54.12 57.82 64.96 66.34
StanD + PERsD-refine 45.63 53.20 56.38 63.02 65.36
PERsD-refine 48.24 63.65 69.00 78.16 81.70
StanD + InpD-refine
246.32 58.84 62.80 69.80 71.23
StanD + InpD-refine* 46.92 58.18 62.03 68.82 68.95
StanD + PERsD-refine 48.44 58.37 62.47 70.64 73.20
PERsD-refine 52.65 68.49 73.34 82.72 85.62

Bảng 6: Loại bỏ về việc trộn StanD, với Backbone là
CodeGen-mono 6B. InpD-refine* biểu thị sử dụng tất cả 6.5K
nhiệm vụ mà mô hình học sinh mắc lỗi, bao gồm khoảng 3K nhiệm vụ hơn so với InpD-refine.

gần như duy trì hiệu suất của nó trong các thiết lập khác, việc trộn dữ liệu StanD với PERsD-refine làm giảm đáng kể hiệu suất của nó (ngoại trừ pass@1 inf-step=2 trên HumanEval). Chúng tôi đưa ra giả thuyết rằng vì StanD có khối lượng dữ liệu lớn hơn nhiều so với PERsD-refine, nó áp đảo việc huấn luyện học sinh trên chưng cất tiêu chuẩn. Tuy nhiên, việc kết hợp với dữ liệu cá nhân hóa đầu vào cân bằng có thể có lợi, như chúng ta quan sát từ hiệu suất tốt của PERsD-combined trong Bảng 4 trên CodeGen-mono-16B.

Tương tự, trong Bảng 7 chúng tôi hiển thị một loại bỏ khác:
việc trộn dữ liệu InpD với PERsD gần như duy trì hiệu suất trên HumanEval nhưng giảm trên MBPP. Điều này cho thấy các nhãn cá nhân hóa có chất lượng cao hơn và việc trộn các nhãn không cá nhân hóa cho cùng một nhiệm vụ thường làm tổn hại hiệu suất.

--- TRANG 8 ---
Phương pháp Pass@1 Pass@5 Pass@10 Pass@50 Pass@100
HumanEval
PERsD 34.63 49.34 55.34 65.56 67.93
PERsD + InpD 34.88 48.35 54.06 64.88 68.90
MBPP
PERsD 45.47 59.90 64.85 76.05 80.07
PERsD + InpD 43.84 59.02 63.77 71.69 74.84

Bảng 7: Loại bỏ về việc trộn PERsD với InpD với
CodeGen-mono 6B làm backbone

Vòng Inf Pass@1 Pass@5 Pass@10 Pass@50 Pass@100
Step HumanEval
1133.81 44.64 49.96 61.75 70.73
2 32.74 45.50 51.52 66.14 71.95
1235.53 49.67 55.67 68.16 77.44
2 36.75 49.71 56.13 70.24 75.00
MBPP
1142.77 56.91 61.43 68.84 70.67
2 45.07 57.75 62.27 70.49 72.55
1248.92 62.29 66.89 75.09 77.25
2 49.59 63.43 68.30 76.00 78.10

Bảng 8: Loại bỏ về chưng cất nhiều vòng trên PERsD-
combined với CodeGen-mono 6B làm backbone

5.4 Chưng cất Nhiều vòng
Sau khi tinh chỉnh mô hình học sinh với dữ liệu chưng cất cá nhân hóa, liệu chúng ta có thể thực hiện một vòng chưng cất cá nhân hóa khác, trên mô hình mới? Chúng tôi hiển thị nghiên cứu loại bỏ như vậy trong Bảng 8. Đáng khích lệ, chúng tôi thấy PERsD-combined vòng-2 thường vượt trội hơn PERsD-combined vòng-1 với biên độ khiêm tốn. Sự cải thiện này cung cấp bằng chứng thêm về lợi ích của học tập cá nhân hóa, ngay cả khi được áp dụng cho các mô hình được huấn luyện với chưng cất cá nhân hóa. Những phát hiện này gợi ý khả năng thú vị của một phiên bản trực tuyến hoặc tích cực của chưng cất cá nhân hóa, trong đó thu thập dữ liệu và huấn luyện mô hình xảy ra đồng thời để đảm bảo mỗi batch được cá nhân hóa hoàn toàn và có hiệu quả mẫu cao hơn. Tuy nhiên, chúng tôi sẽ để lại việc khám phá thú vị như vậy cho công việc tương lai.

5.5 Sử dụng phản hồi cho Suy luận nhiều bước
Để hiểu rõ hơn vai trò của phản hồi thực thi trong quá trình huấn luyện và suy luận nhiều bước, chúng tôi hiển thị nghiên cứu loại bỏ trong Bảng 9, nơi chúng tôi so sánh PERsD-combine với một biến thể cụ thể (PERsD-combine*) loại trừ phản hồi trong cả huấn luyện và suy luận. chúng tôi quan sát thấy PERsD-combine* hoạt động tương đương với PERsD-combine trên HumanEval và hơi tốt hơn trên MBPP cho suy luận 1 bước. Tuy nhiên, đối với suy luận 2 bước, PERsD-combine* liên tục kém hiệu quả hơn PERsD-combine. Kết quả này phù hợp tốt với kỳ vọng của chúng tôi rằng việc sửa chữa mã cần phản hồi thực thi để hướng dẫn việc cải tiến.

Phương pháp Inf Pass@1 Pass@5 Pass@10 Pass@50 Pass@100
Step HumanEval
PERsD-combine133.81 44.64 49.96 61.75 70.73
PERsD-combine* 33.29 45.47 50.90 62.87 68.29
PERsD-combine235.53 49.67 55.67 68.16 77.44
PERsD-combine* 34.59 49.54 55.59 67.27 71.95
MBPP
PERsD-combine142.77 56.91 61.43 68.84 70.67
PERsD-combine* 44.76 56.95 60.85 68.67 71.57
PERsD-combine248.92 62.29 66.89 75.09 77.25
PERsD-combine* 47.83 61.28 65.54 73.03 75.49

Bảng 9: Loại bỏ về việc loại bỏ phản hồi thực thi với
CodeGen-mono 6B làm backbone. PERsD-combine*
biểu thị chưng cất cá nhân hóa kết hợp không có phản hồi thực thi trong lời nhắc đầu vào.

5.6 Chưng cất Cá nhân hóa Chéo-Mô hình
Để điều tra liệu dữ liệu chưng cất cá nhân hóa của một mô hình có thể có lợi cho mô hình khác không, chúng tôi tiến hành loại bỏ trong Bảng 10 bằng cách sử dụng dữ liệu PERsD-combined của CodeGen-mono-6B để huấn luyện CodeGen-mono-16B. Kết quả cho thấy dữ liệu cá nhân hóa chéo-mô hình như vậy không hoạt động tốt như dữ liệu cá nhân hóa thực: dẫn đến sụt giảm hiệu suất liên tục với biên độ lớn. Phát hiện này củng cố khái niệm của chúng tôi rằng dữ liệu học tập nên được điều chỉnh theo mô hình học sinh cụ thể, vì dữ liệu cá nhân hóa phù hợp với một mô hình có thể không nhất thiết có lợi cho các mô hình khác.

Mô hình Inf Pass@1 Pass@5 Pass@10 Pass@50 Pass@100
Step HumanEval
CodeGen-mono-6B
133.81 44.64 49.96 61.75 70.73
CodeGen-mono-16B* 32.99 47.81 54.58 69.31 73.98
CodeGen-mono-16B 36.40 53.57 60.81 74.64 79.88
CodeGen-mono-6B
235.53 49.67 55.67 68.16 77.44
CodeGen-mono-16B* 35.85 51.31 58.23 74.02 76.60
CodeGen-mono-16B 37.74 55.80 63.37 77.14 81.10
MBPP
CodeGen-mono-6B
142.77 56.91 61.43 68.84 70.67
CodeGen-mono-16B* 43.24 60.14 65.19 72.31 74.19
CodeGen-mono-16B 51.44 66.45 71.64 80.62 82.93
CodeGen-mono-6B
248.92 62.29 66.89 75.09 77.25
CodeGen-mono-16B* 48.12 65.31 70.02 76.60 78.70
CodeGen-mono-16B 56.44 71.31 76.43 84.39 86.76

Bảng 10: Loại bỏ về chưng cất cá nhân hóa chéo-mô hình với PERsD-combined. CodeGen-mono-16B*
có nghĩa là dữ liệu chưng cất từ CodeGen-mono-6B.

5.7 So sánh với các Mô hình Sinh mã dựa trên Phản hồi khác
So sánh với ILF (Chen et al., 2023a): Để so sánh với ILF, một trong những công trình liên quan gần nhất của chúng tôi, chúng tôi thí nghiệm trên một thiết lập riêng biệt:

--- TRANG 9 ---
bắt đầu với tập dữ liệu MBPP đầy đủ (974 nhiệm vụ) và
sử dụng Task-Ids 11-111 làm phần kiểm tra và 863 còn lại làm dữ liệu huấn luyện. Trên tập huấn luyện, mô hình học sinh CodeGen-6B của chúng tôi (giống như ILF) đã tạo ra nỗ lực sai trên 562 nhiệm vụ, được hiển thị cho ChatGPT cùng với hướng dẫn nhiệm vụ và phản hồi lỗi thực thi để cuối cùng thu thập 288 nhãn sửa chữa mã cá nhân hóa hợp lệ.

Dữ liệu text-to-code MBPP gốc
và dữ liệu cải tiến mã cá nhân hóa
được thu thập này cho 288 nhiệm vụ

Tập Kiểm tra MBPP
Phương pháp Chi phí Pass@1 Pass@10
ILF >4K$ 36 68
PER SD 0.65$ 46.8 67.4
-refine 0.65$ 41.8 66.8
-combined 0.65$ 47.8 64.8

Bảng 11: So sánh với ILF

tương ứng tạo thành
dữ liệu tinh chỉnh
Dcode và Drefine
mà chúng tôi huấn luyện
các mô hình PER SD và
PER SD-refine. Chúng tôi
tiếp tục kết hợp
Dcode và Drefine
để huấn luyện PER SD-
combined. Kết quả thí nghiệm của chúng tôi trong Bảng
11 cho thấy tất cả các biến thể PER SD vượt trội đáng kể
hơn ILF 11.8% ở pass@1 với chi phí thấp hơn 1e-4
lần so với ILF, do đó thể hiện sự thiếu khả năng mở rộng của các mô hình kiểu ILF.

So sánh với Self-Edit : Vì Self-Edit
(Zhang et al., 2023) sử dụng một mô hình trình chỉnh sửa mã CodeGen-350M có thể huấn luyện và một mô hình sinh mã cố định, thiết lập thí nghiệm của chúng tôi không thể so sánh trực tiếp với của họ. Tuy nhiên, các mô hình INPD-refine và INPD-combined của chúng tôi thực sự có thể được coi là các đối tác rất gần với một phiên bản của Self-Edit với mô hình sinh mã và cải tiến mã được chia sẻ và backbone CodeGen-6B. Sự cải thiện hiệu suất liên tục của các mô hình chưng cất cá nhân hóa so với các mô hình được chưng cất đầu vào trên toàn bộ, ám chỉ về triển vọng rằng các mô hình PER SD thực sự hiệu quả hơn các mô hình kiểu Self-Edit.

5.8 So sánh với các Mô hình SOTA
Cuối cùng, chúng tôi so sánh các mô hình PERsD-combine
với các mô hình tiên tiến mã nguồn mở và mã nguồn đóng trên HumanEval trong Bảng 12. Chúng tôi thấy
rằng các phương pháp PERsD-combine có thể cải thiện đáng kể
mô hình backbone, với mức tăng hiệu suất
6.2 điểm cho CodeGen-mono 6B (giảm lỗi 8.4%), 5.9 điểm cho CodeGen-mono 16B
(giảm lỗi 8.3%) và 12.2 điểm cho StarCoder (giảm lỗi 18.4%). Hơn nữa, StarCoder với PERsD-combined, vượt trội hơn các
mô hình mã nguồn mở khác ngoại trừ WizardCoder.

Mô hình Kích thước mô hình Pass@1 Pass@10 Pass@100
Mô hình mã nguồn đóng
LaMDA 137B 14.0 - 47.3
PaLM 540B 26.2 - 76.2
Codex 12B 28.8 46.8 72.3
code-cushman-001 - 33.5 54.3 77.4
code-davinci-002 - 47.0 74.9 92.1
GPT-3.5 - 48.1 - -
phi-1 1.3B 50.6 - -
GPT-4 - 67.0 - -
Mô hình mã nguồn mở
CodeGeeX 13B 22.9 39.6 60.9
LLaMA 65B 23.7 - 79.3
StarCoder 15B 33.6 - -
CodeGen-mono 6B 26.1 42.3 65.8
CodeGen-mono 16B 29.3 49.9 75.0
InstructCodeT5+ 16B 35.0 54.5 77.9
WizardCoder 15B 57.3 - -
CodeGen-mono (PERsD-combined) 6B 33.8 50.0 70.7
CodeGen-mono (PERsD-combined) 16B 36.4 60.8 79.9
StarCoder (PERsD-combined) 15B 45.8 68.3 82.3

Bảng 12: Kết quả của pass@k (%) trên HumanEval

Lưu ý
rằng mô hình của chúng tôi sử dụng 5K ví dụ dữ liệu trong khi WizardCoder sử dụng 78K. Như đã đề cập trong §2.1, WizardCoder là một cách tiếp cận trực giao có thể được tích hợp vào chưng cất cá nhân hóa.

6 Kết luận
Trong bài báo này, chúng tôi đã giới thiệu chưng cất cá nhân hóa như một phương pháp thu thập dữ liệu được gán nhãn tùy chỉnh thích ứng với năng lực của các mô hình học sinh, dẫn đến việc học tập hiệu quả hơn. Chúng tôi đã chứng minh những lợi thế của chưng cất cá nhân hóa so với chưng cất tiêu chuẩn trong lĩnh vực sinh mã, đạt được hiệu suất vượt trội trên cả tập dữ liệu HumanEval và MBPP. Thông qua các nghiên cứu loại bỏ toàn diện, chúng tôi đã xác nhận rằng chưng cất cá nhân hóa dẫn đến chất lượng dữ liệu cao hơn, được hưởng lợi từ chưng cất nhiều vòng, và cho phép các mô hình tận dụng phản hồi thực thi để tự sửa chữa. Chúng tôi tin rằng chưng cất cá nhân hóa đại diện cho một bước tiến thú vị hướng tới việc chưng cất tốt hơn các LLM mã nguồn đóng sang các mô hình mã nguồn mở.

Hạn chế
Trong phần này, chúng tôi thảo luận về một số hạn chế của bài báo này và các hướng tương lai để làm cho nó có giá trị hơn:

Về Quy mô Dữ liệu Để so sánh công bằng, chúng tôi đã tiến hành tất cả các thí nghiệm dựa trên cùng dữ liệu DSTAND 10K (được giới thiệu §4.2) và dữ liệu cá nhân hóa tương ứng được xử lý từ DSTAND có kích thước 2-3K như được hiển thị trong Bảng 3. Tuy nhiên, vì chúng tôi đã chứng minh chưng cất cá nhân hóa hỗ trợ

--- TRANG 10 ---
việc học tập hiệu quả và hiệu quả hơn, thật thú vị khi điều tra chưng cất cá nhân hóa mở rộng tốt như thế nào với kích thước dữ liệu. Ví dụ, nếu chúng ta mở rộng dữ liệu chưng cất cá nhân hóa lên 50K, các phương pháp PERsD sẽ nhận được bao nhiều mức tăng hiệu suất hơn so với InpD và StanD với việc mở rộng kích thước dữ liệu.

Chưng cất Cá nhân hóa Trực tuyến Như đã thảo luận trong §5.4, việc tiến hành vòng thứ hai chưng cất cá nhân hóa tiếp tục cải thiện một mô hình học sinh đã được huấn luyện với PERsD-combine. Quan sát như vậy gợi ý tiềm năng của một phiên bản trực tuyến của chưng cất cá nhân hóa, thu thập một batch dữ liệu cá nhân hóa một cách tức thời với mô hình giáo viên, sau mỗi bước tối ưu hóa trong quá trình tinh chỉnh. Vì chúng tôi đã chứng minh rằng dữ liệu cá nhân hóa thực sự có lợi hơn dữ liệu tiêu chuẩn hoặc dữ liệu cá nhân hóa chéo-mô hình (§5.6), chưng cất cá nhân hóa trực tuyến như vậy về nguyên tắc sẽ được hưởng lợi tối đa từ chưng cất cá nhân hóa, vì mỗi batch dữ liệu huấn luyện được điều chỉnh hoàn toàn cho mô hình học sinh.

Tài liệu tham khảo
Jacob Austin, Augustus Odena, Maxwell I. Nye,
Maarten Bosma, Henryk Michalewski, David Dohan,
Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V . Le,
và Charles Sutton. 2021. Program synthesis with
large language models. CoRR , abs/2108.07732.

Sahil Chaudhary. 2023. Code alpaca: An
instruction-following llama model for code genera-
tion. https://github.com/sahil280114/
codealpaca .

Angelica Chen, Jérémy Scheurer, Tomasz Korbak,
Jon Ander Campos, Jun Shern Chan, Samuel R. Bow-
man, Kyunghyun Cho, và Ethan Perez. 2023a. Im-
proving code generation by training with natural lan-
guage feedback.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Pondé de Oliveira Pinto, Jared Kaplan,
Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles

Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, và Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. CoRR ,
abs/2107.03374.

Xinyun Chen, Maxwell Lin, Nathanael Schärli, và
Denny Zhou. 2023b. Teaching large language mod-
els to self-debug. CoRR , abs/2304.05128.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, và Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.

Yuxin Jiang, Chunkit Chan, Mingyang Chen, và
Wei Wang. 2023. Lion: Adversarial distillation
of closed-source large language model. CoRR ,
abs/2305.12870.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou,
Marc Marone, Christopher Akiki, Jia Li, Jenny
Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue
Zhuo, Thomas Wang, Olivier Dehaene, Mishig
Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh
Shliazhko, Nicolas Gontier, Nicholas Meade, Armel
Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi,
Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov,
Zhiruo Wang, Rudra Murthy V , Jason Stillerman,
Siva Sankalp Patel, Dmitry Abulkhanov, Marco
Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Ur-
vashi Bhattacharyya, Wenhao Yu, Swayam Singh,
Sasha Luccioni, Paulo Villegas, Maxim Kunakov,
Fedor Zhdanov, Manuel Romero, Tony Lee, Na-
dav Timor, Jennifer Ding, Claire Schlesinger, Hai-
ley Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra,
Alex Gu, Jennifer Robinson, Carolyn Jane Ander-
son, Brendan Dolan-Gavitt, Danish Contractor, Siva
Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jer-
nite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas
Wolf, Arjun Guha, Leandro von Werra, và Harm
de Vries. 2023a. Starcoder: may the source be with
you! CoRR , abs/2305.06161.

Zihao Li, Zhuoran Yang, và Mengdi Wang. 2023b. Re-
inforcement learning with human feedback: Learn-
ing dynamic choices via pessimism. CoRR ,
abs/2305.18438.

Hao Liu, Carmelo Sferrazza, và Pieter Abbeel. 2023.
Chain of hindsight aligns language models with feed-
back. CoRR , abs/2302.02676.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,
Qingwei Lin, và Daxin Jiang. 2023. Wizardcoder:
Empowering code large language models with evol-
instruct. CoRR , abs/2306.08568.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,

--- TRANG 11 ---
Sean Welleck, Bodhisattwa Prasad Majumder,
Shashank Gupta, Amir Yazdanbakhsh, và Peter
Clark. 2023. Self-refine: Iterative refinement with
self-feedback. CoRR , abs/2303.17651.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
Wang, Yingbo Zhou, Silvio Savarese, và Caiming
Xiong. 2023. Codegen: An open large language
model for code with multi-turn program synthesis.
ICLR .

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul F. Christiano, Jan Leike, và Ryan Lowe.
2022. Training language models to follow instruc-
tions with human feedback. In NeurIPS .

Rafael Rafailov, Archit Sharma, Eric Mitchell, Ste-
fano Ermon, Christopher D. Manning, và Chelsea
Finn. 2023. Direct preference optimization: Your
language model is secretly a reward model. CoRR ,
abs/2305.18290.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
và Yuxiong He. 2020. Zero: memory optimizations
toward training trillion parameter models. In SC,
page 20. IEEE/ACM.

Heather Roberts-Mahoney, Alexander J. Means, và
Mark J. Garrison. 2016. Netflixing human capital
development: personalized learning technology and
the corporatization of k-12 education. Journal of
Education Policy , 31(4):405–420.

Atikah Shemshack và Jonathan Michael Spector. 2020.
A systematic literature review of personalized learn-
ing terms. Smart Learning Environments , 7(1):1–20.

Noah Shinn, Federico Cassano, Beck Labash, Ashwin
Gopinath, Karthik Narasimhan, và Shunyu Yao.
2023. Reflexion: Language agents with verbal rein-
forcement learning.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, và Tatsunori B. Hashimoto. 2023. Stan-
ford alpaca: An instruction-following llama
model. https://github.com/tatsu-lab/
stanford_alpaca .

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, và Hannaneh
Hajishirzi. 2022. Self-instruct: Aligning language
model with self generated instructions.

Sean Welleck, Ximing Lu, Peter West, Faeze Brah-
man, Tianxiao Shen, Daniel Khashabi, và Yejin
Choi. 2022. Generating sequences by learning to
self-correct. CoRR , abs/2211.00053.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,

Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le
Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, và Alexander M. Rush. 2020. Transform-
ers: State-of-the-art natural language processing. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing: System
Demonstrations , pages 38–45, Online. Association
for Computational Linguistics.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin
Jiang. 2023a. Wizardlm: Empowering large lan-
guage models to follow complex instructions. CoRR ,
abs/2304.12244.

Canwen Xu, Daya Guo, Nan Duan, và Julian McAuley.
2023b. Baize: An open-source chat model with
parameter-efficient tuning on self-chat data. arXiv
preprint arXiv:2304.01196 .

Kechi Zhang, Zhuo Li, Jia Li, Ge Li, và Zhi Jin. 2023.
Self-edit: Fault-aware code editor for code genera-
tion. CoRR , abs/2305.04087.

A Chi tiết trong Đánh giá Mô hình Nhiều bước
Vì các docstring được định dạng kém trong HumanEval,
chúng tôi viết một đoạn mã phân tích dựa trên quy tắc đơn giản để trích xuất các trường hợp kiểm tra đơn vị đã thấy của nó. Trung bình
mỗi nhiệm vụ, có 2 trường hợp kiểm tra đơn vị đã thấy và 4.2
trường hợp kiểm tra đơn vị chưa thấy. Sự chồng chéo giữa các kiểm tra đã thấy
và chưa thấy là 11.33%. Đối với MBPP, vì theo
quy ước lời nhắc hướng dẫn được xây dựng
bằng cách lấy mô tả nhiệm vụ và các cách sử dụng ví dụ
(từ các trường hợp kiểm tra đơn vị) như một phần của doc-string,
chúng tôi coi tất cả các trường hợp kiểm tra đơn vị là "đã thấy" và
sử dụng tất cả chúng cho suy luận nhiều bước.

B Mẫu Lời nhắc ChatGPT cho
Chưng cất Cá nhân hóa
Trong Hình 3, chúng tôi hiển thị mẫu lời nhắc mà chúng tôi sử dụng
để truy vấn ChatGPT cho cải tiến cá nhân hóa. Đối với
mỗi ví dụ nhiệm vụ, với hướng dẫn nhiệm vụ t, các trường hợp kiểm tra đơn vị u và mã chính xác c, chúng tôi truy vấn ChatGPT API
với lịch sử cuộc trò chuyện hai lượt.

Đối với lượt đầu tiên, chúng tôi sử dụng mẫu trong Hình 3a và thay thế «TASK», «HEADER» bằng hướng dẫn nhiệm vụ thực tế t và tiêu đề hàm được trích xuất. Điều này được thêm vào đầu vào người dùng lượt đầu tiên và
mã chính xác c được bao gồm như đầu ra trợ lý lượt đầu tiên. Đối với lượt thứ hai, chúng tôi sử dụng mẫu trong
Hình 3b và thay thế «CODE», «ERROR» bằng
nỗ lực của mô hình học sinh và phản hồi thực thi của nó. Điều này được thêm vào đầu vào người dùng lượt thứ hai và
chúng tôi truy vấn ChatGPT với lịch sử cuộc trò chuyện được xây dựng để nhận đầu ra trợ lý lượt thứ hai
như cải tiến mã cá nhân hóa.

--- TRANG 12 ---
(a) Mẫu Lời nhắc Lượt-1
(b) Mẫu Lời nhắc Lượt-2

Hình 3: Mẫu lời nhắc để truy vấn cải tiến cá nhân hóa. Trên(a): mẫu lời nhắc cho cuộc trò chuyện lượt đầu tiên, Dưới(b): mẫu lời nhắc cho cuộc trò chuyện lượt thứ hai.

C Mẫu Lời nhắc cho Tinh chỉnh Cải tiến Mã
Hình 4 hiển thị mẫu cải tiến Trefine được giới thiệu trong §3.2), được sử dụng để xây dựng lời nhắc đầu vào
cho tinh chỉnh cải tiến mã. chúng tôi thay thế
«TASK» bằng hướng dẫn nhiệm vụ, «CODE» bằng nỗ lực sai ban đầu từ học sinh, «ERROR» bằng
phản hồi thực thi, và «HEADER» bằng tiêu đề hàm được trích xuất từ hướng dẫn nhiệm vụ.

D Chi tiết trong Phân tích Chồng chéo Dữ liệu
Phần này mô tả các quy trình chi tiết để
tiến hành phân tích chồng chéo dữ liệu train-test. Mục tiêu là đánh giá mức độ rò rỉ dữ liệu trong các tập dữ liệu kiểm tra xuất phát từ kho dữ liệu tiền huấn luyện tự xây dựng của chúng tôi.

Hình 4: Mẫu lời nhắc cho tinh chỉnh cải tiến mã.

Đầu tiên, chúng tôi đã thực hiện so khớp chuỗi chính xác
và không tìm thấy rò rỉ dữ liệu nào trong bất kỳ dữ liệu kiểm tra nào (HumanEval/MBPP).

Để đo lường tương đồng ngữ nghĩa giữa
các nhiệm vụ huấn luyện/kiểm tra, chúng tôi đã làm như sau:
1. Đối với mỗi nhiệm vụ trong kiểm tra (MBPP/HumanEval)
chúng tôi truy xuất hai nhiệm vụ huấn luyện gần nhất (dựa
trên tương đồng cosine của embedding starcoder
& vector tf-idf của mô tả nhiệm vụ).
2.Chúng tôi sử dụng gpt-3.5-turbo-16k để xác định liệu
có rò rỉ dữ liệu giữa một instance huấn luyện và kiểm tra hay không bằng cách phân loại cặp thành ("leak",
"somewhat similar", "somewhat not similar",
"not related"). Chúng tôi sử dụng một lời nhắc với hướng dẫn và các ví dụ few-shot được tạo thủ công và yêu cầu gpt-3.5 tạo ra lý luận
và phân loại. Chúng tôi đã kiểm tra thủ công
một số ví dụ cho mỗi danh mục để đảm bảo lý luận và phán đoán được thực hiện chính xác và
nhất quán.
3.Ánh xạ các danh mục tương đồng sang điểm tương đồng 0-1
("leak" -> 1, "somewhat similar" ->
0.75, "somewhat not similar" -> 0.25, "not
related" -> 0) và hiển thị điểm trung bình và %
trường hợp được phân loại là "leak". Lưu ý rằng StanD
& PERsD có dữ liệu huấn luyện 10K & 3K
tương ứng nên điểm của chúng khác nhau.

E Kết quả trong MBPP-Cleaned
Trong Phụ lục D, chúng tôi tìm thấy 55 instance dữ liệu có thể bị rò rỉ (với điểm tương đồng = 1) trong
dữ liệu kiểm tra MBPP. Trong phần này, chúng tôi xây dựng một
tập dữ liệu MBPP-Cleaned mới, nơi các điểm dữ liệu bị rò rỉ được loại bỏ (ban đầu 306 vấn đề →251
vấn đề sau khi lọc). Kết quả trên tập dữ liệu
MBPP-Cleaned mới này được hiển thị trong Bảng 13. Từ

--- TRANG 13 ---
kết quả, chúng ta có thể thấy đối với thiết lập CodeGen-mono-
16B, pass@1, PERsD trở nên gần như ngang bằng với
StanD (từ khoảng cách -1.21 thành -0.17). Đối với 15/16 thiết lập còn lại của PERsD so với StanD, biên độ trung bình của nó được tăng từ 4.8 điểm lên 5.9
điểm. Bên cạnh đó, PERsD-refine trên MBPP-Cleaned
cho thấy những cải thiện nhất quán và đáng kể hơn so với InpD-refine, với lợi thế trung bình +0.86 cho
suy luận 1 bước, và +1.91 cho suy luận hai bước.
Nhìn chung, với dữ liệu kiểm tra chồng chéo được loại bỏ, các phương pháp PERsD cho thấy lợi thế thậm chí lớn hơn so với
các phương pháp StanD hoặc InpD.

(a) Backbone là CodeGen-mono-6B
Phương pháp #Dữ liệu Pass@1 Pass@5 Pass@10 Pass@20
step=1 step=2 step=1 step=2 step=1 step=2 step=1 step=2
MBPP-Cleaned
StanD 10K 37.51 - 50.89 - 55.15 - 58.87 -
InpD 3.3K 38.80 - 53.91 - 58.47 - 62.73 -
-refine 3.3K 37.58 42.95 57.65 62.29 63.52 67.79 67.92 71.96
-combined 6.5K 38.11 43.01 52.69 58.32 57.36 62.75 61.19 66.18
PERsD 3.3K 41.30 - 56.20 - 61.86 - 67.53 -
-refine 3.3K 43.86 47.73 59.33 64.41 65.19 69.95 69.62 74.33
-combined 6.5K 38.86 43.75 52.78 57.04 57.35 61.78 61.52 66.19

(b) Backbone là CodeGen-mono-16B
Phương pháp #Dữ liệu Pass@1 Pass@5 Pass@10 Pass@20
step=1 step=2 step=1 step=2 step=1 step=2 step=1 step=2
MBPP-Cleaned
StanD 10K 43.10 - 57.53 - 62.92 - 68.12 -
InpD 2.8K 40.64 - 53.88 - 58.82 - 62.88 -
-refine 2.8K 43.67 49.60 63.14 68.21 69.27 73.28 73.36 76.85
-combined 5.6K 41.63 47.77 54.74 62.24 59.67 67.33 63.75 71.57
PERsD 2.8K 42.93 - 62.40 - 68.90 - 74.10 -
-refine 2.8K 47.73 52.63 63.62 69.21 69.84 75.17 74.90 79.69
-combined 5.6K 46.33 51.67 63.46 68.65 69.49 74.26 74.53 78.83

Bảng 13: So sánh hiệu suất của các mô hình PERsD với
StanD & InpD trên MBPP-Cleaned
