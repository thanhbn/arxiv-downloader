# 2210.02871.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2210.02871.pdf
# Kích thước tệp: 865030 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023
TỰ CHƯNG CẤT CHO VIỆC TIẾP TỤC TIỀN HUẤN LUYỆN CÁC TRANSFORMER
Seanie Lee1†Minki Kang1,2Juho Lee1,2Sung Ju Hwang1Kenji Kawaguchi3∗
KAIST1, AITRICS2, Đại học Quốc gia Singapore3
{lsnfamily02, zzxc1133, juholee, sjhwang82 }@kaist.ac.kr
kenji@comp.nus.edu.sg

TÓM TẮT
Việc ứng dụng tiền huấn luyện các mô hình transformer lớn trên lượng dữ liệu không được gán nhãn khổng lồ và tinh chỉnh chúng trên các tập dữ liệu được gán nhãn cho các tác vụ downstream đa dạng đã chứng minh được thành công đáng kể trong nhiều tác vụ thị giác và xử lý ngôn ngữ tự nhiên. Tuy nhiên, phương pháp tinh chỉnh trực tiếp có thể dẫn đến hiệu suất dưới mức tối ưu nếu tồn tại sự khác biệt đáng kể giữa các miền tiền huấn luyện và tinh chỉnh. Để giải quyết vấn đề này, một số nghiên cứu trước đó đã đề xuất các chiến lược tiếp tục tiền huấn luyện để tiếp tục tiền huấn luyện mô hình trên tập dữ liệu không được gán nhãn đích trước khi tinh chỉnh. Tuy nhiên, các chiến lược này bị giới hạn ở các mô hình ngôn ngữ và có thể dẫn đến overfitting khi áp dụng cho Vision Transformers. Để vượt qua hạn chế này, chúng tôi trình bày một phương pháp mới là tự chưng cất như một phương pháp chính quy hóa cho giai đoạn tiếp tục tiền huấn luyện. Phương pháp của chúng tôi trước tiên tiếp tục tiền huấn luyện mô hình tiền huấn luyện ban đầu trên dữ liệu không được gán nhãn đích, và sau đó sử dụng nó làm giáo viên cho tự chưng cất. Sau đó chúng tôi lấy cùng mô hình tiền huấn luyện ban đầu làm học sinh, và ép buộc các biểu diễn ẩn của nó gần với những của giáo viên trong khi tối ưu hóa học sinh với mục tiêu mã hóa tự động có mặt nạ. Các thí nghiệm của chúng tôi chứng minh tính ưu việt của tự chưng cất so với các baseline liên quan trên nhiều tập dữ liệu chuẩn cho các tác vụ phân loại hình ảnh và văn bản. Hơn nữa, chúng tôi cung cấp phân tích lý thuyết về phương pháp đề xuất của chúng tôi sử dụng một mô hình đơn giản hóa để làm sáng tỏ cách tự chưng cất cho tiếp tục tiền huấn luyện có thể tiềm năng nâng cao hiệu suất của các tác vụ downstream.

1 GIỚI THIỆU
Các mô hình transformer tiền huấn luyện (Devlin et al., 2019; Brown et al., 2020; Liu et al., 2019; He et al., 2022) đã có hiệu quả trên nhiều tác vụ thị giác và xử lý ngôn ngữ tự nhiên khác nhau. Các mô hình tiền huấn luyện học biểu diễn tổng quát từ một khối lượng lớn dữ liệu không được gán nhãn để chúng tổng quát hóa tốt cho nhiều tác vụ downstream khác nhau khi chúng được tinh chỉnh trên mỗi tác vụ với một tập dữ liệu được gán nhãn.

Tuy nhiên, trong nhiều ứng dụng thực tế, việc thích ứng mô hình tiền huấn luyện với một miền tác vụ downstream cụ thể đòi hỏi một lượng nỗ lực đáng kể vì tồn tại sự khác biệt phân phối đáng kể giữa dữ liệu cho giai đoạn tiền huấn luyện và tinh chỉnh. Hơn nữa, việc thu thập một lượng lớn dữ liệu được gán nhãn cho các miền cụ thể như vậy là khó khăn, điều này làm cho việc thích ứng mô hình tiền huấn luyện với các tác vụ downstream trở nên thách thức hơn.

[THIS IS FIGURE: Graph showing test accuracy vs further pre-training steps for image classification on CUB, comparing Self-Distillation and Further Pre-training]

Một số công trình đã đề xuất để giải quyết vấn đề thích ứng các mô hình tiền huấn luyện với một miền cụ thể. Một phương pháp phổ biến để thích ứng mô hình tiền huấn luyện là tiếp tục tiền huấn luyện, nơi chúng ta tiếp tục cập nhật các tham số của mô hình tiền huấn luyện trên dữ liệu không được gán nhãn chuyên biệt về miền được tuyển chọn thêm với tự giám sát (Beltagy et al., 2019; Lee et al., 2020), trước khi tinh chỉnh nó trên dữ liệu được gán nhãn đích như được mô tả trong Hình 2b. Gururangan et al. (2020) cũng cho thấy rằng tiếp tục tiền huấn luyện chỉ với dữ liệu không được gán nhãn đích vẫn hiệu quả mà không cần bất kỳ dữ liệu bổ sung nào. Tuy nhiên, hầu hết các phương pháp tiếp tục tiền huấn luyện hiện có đã tập trung vào các mô hình ngôn ngữ, và chúng tôi thấy rằng việc tiếp tục tiền huấn luyện

∗Tác giả liên hệ †Công việc được thực hiện khi tác giả là thực tập sinh tại NUS.
1arXiv:2210.02871v3  [cs.CV]  9 Jun 2023

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

[THIS IS FIGURE: Diagram showing three different approaches: (a) Fine-tuning, (b) Further Pre-training, and (c) Self-distillation in Further Pre-training, with various components like Transformer, Classifier, and different loss functions]

Hình 2: Khái niệm. So sánh giữa các phương pháp thích ứng transformers tiền huấn luyện với miền đích. (a) Tinh chỉnh mà không cần bất kỳ tiếp tục tiền huấn luyện nào. (b) Tiếp tục tiền huấn luyện và tinh chỉnh. (c) Tự chưng cất trong tiếp tục tiền huấn luyện và tinh chỉnh.

chiến lược không hiệu quả đối với Vision Transformer (ViT) (Dosovitskiy et al., 2021). Như được hiển thị trong Hình 1, ViT dễ bị overfitting và không tổng quát hóa tốt cho các tác vụ downstream khi chúng ta tiếp tục tiền huấn luyện nó trên dữ liệu không được gán nhãn đích.

Một số phương pháp chính quy hóa (Chen et al., 2020a; Gouk et al., 2021; Aghajanyan et al., 2021) đã đề xuất để giải quyết vấn đề overfitting của các mô hình tiền huấn luyện lớn, tuy nhiên, chúng không xem xét quá trình thích ứng như tiếp tục tiền huấn luyện. Thay vào đó, chúng ép buộc khoảng cách giữa trọng số tinh chỉnh cuối cùng và trọng số tiền huấn luyện nhỏ để thúc đẩy việc chuyển giao kiến thức thu được từ tiền huấn luyện sang các tác vụ downstream để tổng quát hóa tốt hơn. Tuy nhiên, những chính quy hóa này cản trở việc thích ứng của các mô hình tiền huấn luyện với các tác vụ downstream đặc biệt khi có sự chuyển dịch phân phối đáng kể giữa dữ liệu tiền huấn luyện và dữ liệu đích. Cuối cùng điều này dẫn đến tổng quát hóa tệ hơn so với chiến lược tinh chỉnh đơn giản.

Để giải quyết những hạn chế này, chúng tôi đề xuất tự chưng cất như một chính quy hóa cho tiếp tục tiền huấn luyện trên tập dữ liệu không được gán nhãn đích để chúng ta có thể thích ứng hiệu quả các mô hình tiền huấn luyện với tác vụ downstream của các miền khác nhau với một lượng hạn chế dữ liệu được gán nhãn. Đối với tự giám sát, chúng tôi tập trung vào mã hóa tự động có mặt nạ cho tiền huấn luyện vì nó không phụ thuộc vào bất kỳ tăng cường dữ liệu nào, so với các phương pháp học tự giám sát khác (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; Zbontar et al., 2021; Chen & He, 2021; Caron et al., 2021) yêu cầu tăng cường dữ liệu để xây dựng các cặp dương cho mục tiêu học tự giám sát như học tương phản. Điều này đặc biệt hữu ích khi khó định nghĩa các tăng cường dữ liệu có ý nghĩa cho một miền đích.

Cụ thể, chúng tôi lấy mô hình tiền huấn luyện với một bộ mã hóa fθinit và một bộ giải mã gϕinit được tiền huấn luyện trên một lượng khổng lồ dữ liệu không được gán nhãn từ miền tổng quát, và tiếp tục tiền huấn luyện nó với mục tiêu mã hóa tự động có mặt nạ (MAE) (Devlin et al., 2019; He et al., 2022) trên dữ liệu không được gán nhãn đích để thu được fθ0 và gϕ0. Sau đó, chúng tôi đặt bộ mã hóa fθ0 làm giáo viên cho tự chưng cất. Sau đó chúng tôi lấy bản sao của mô hình tiền huấn luyện (fθinit, gϕinit) làm học sinh, và khớp các biểu diễn của bộ mã hóa học sinh và những của bộ mã hóa giáo viên trong khi tối ưu hóa học sinh với MAE trên dữ liệu không được gán nhãn đích. Cuối cùng, chúng tôi tinh chỉnh học sinh tự chưng cất fθ1 trên dữ liệu được gán nhãn đích cho tác vụ downstream. Chúng tôi minh họa tổng quan về phương pháp của chúng tôi trong Hình 2c.

Để xác minh hiệu quả của phương pháp chúng tôi, chúng tôi chứng minh thực nghiệm rằng nó cải thiện đáng kể hiệu suất tổng quát hóa của ViT tiền huấn luyện và mô hình ngôn ngữ RoBERTA (Liu et al., 2019), và vượt trội hơn các baseline liên quan trên nhiều tập dữ liệu phân loại hình ảnh và văn bản khác nhau. Hơn nữa, chúng tôi phân tích lý thuyết phương pháp đề xuất với một mô hình đơn giản hóa để hiểu cách tự chưng cất cho tiếp tục tiền huấn luyện có thể tiềm năng giúp cải thiện hiệu suất tổng quát hóa trên các tác vụ đích sau tinh chỉnh.

Đóng góp của chúng tôi gồm ba phần:
• Chúng tôi đề xuất tự chưng cất cho tiếp tục tiền huấn luyện trên tập dữ liệu không được gán nhãn đích, nơi chúng tôi ép buộc biểu diễn của học sinh gần với những của giáo viên tiếp tục tiền huấn luyện trong khi huấn luyện học sinh với mục tiêu mã hóa-tự động có mặt nạ.
• Chúng tôi phân tích lý thuyết phương pháp đề xuất với một mô hình đơn giản hóa để hiểu cách tự chưng cất cho tiếp tục tiền huấn luyện có thể tiềm năng dẫn đến hiệu suất tổng quát hóa tốt hơn của các tác vụ downstream.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

• Chúng tôi xác thực rộng rãi phương pháp của chúng tôi trên nhiều tập dữ liệu phân loại hình ảnh và văn bản với các transformers tiền huấn luyện và cho thấy phương pháp của chúng tôi vượt trội hơn các baseline liên quan.

2 CÔNG TRÌNH LIÊN QUAN

Tự Chưng Cất Chưng cất kiến thức là chuyển giao kiến thức từ giáo viên sang học sinh bằng cách tối thiểu hóa một độ phân kỳ giữa đầu ra của giáo viên và học sinh (Hinton et al., 2014). Khi tham số hóa của học sinh và giáo viên giống hệt nhau, chúng ta gọi đó là tự chưng cất như một trường hợp đặc biệt của chưng cất kiến thức. Mặc dù không có thông tin mới trong quá trình tự chưng cất, Furlanello et al. (2018) đã cho thấy rằng học sinh từ tự chưng cất đạt được hiệu suất tổng quát hóa tốt hơn so với giáo viên. Một hiện tượng tương tự đã được quan sát thấy một cách nhất quán trong các công trình khác (Yang et al., 2019; Ahn et al., 2019). Một số công trình đề xuất tự chưng cất mà không có mạng giáo viên tiền huấn luyện (Sun et al., 2019; Zhang et al., 2019; 2022). Họ thêm các bộ phân loại phụ trợ vào các lớp trung gian và huấn luyện các bộ phân loại để tối thiểu hóa độ phân kỳ giữa đầu ra của bộ phân loại của lớp cuối cùng và đầu ra của các bộ phân loại phụ trợ. Mobahi et al. (2020) phân tích lý thuyết cách tự chưng cất gây ra chính quy hóa và giảm overfitting trong không gian Hilbert. Tuy nhiên, tất cả chúng đều tập trung vào tự chưng cất cho học có giám sát. Thay vào đó, chúng tôi chứng minh thực nghiệm và lý thuyết rằng tự chưng cất cho tiếp tục tiền huấn luyện với tự giám sát dẫn đến tổng quát hóa tốt hơn của các tác vụ downstream sau khi tinh chỉnh mô hình tự chưng cất với dữ liệu được gán nhãn đích.

Tiếp Tục Tiền Huấn Luyện Lee et al. (2020); Beltagy et al. (2019); Sun et al. (2020) đã cho thấy thành công của việc tiếp tục tiền huấn luyện mô hình ngôn ngữ trên một số lượng lớn corpus được thu thập từ miền đích và tinh chỉnh mô hình trên tập dữ liệu được gán nhãn đích. Tuy nhiên, việc tiếp tục tiền huấn luyện mô hình trên một lượng lớn dữ liệu văn bản không được gán nhãn tốn kém về mặt tính toán và có thể không khả thi để thu thập quy mô lớn như vậy của dữ liệu không được gán nhãn trên các miền nhất định. Thay vào đó, Gururangan et al. (2020) thiết kế một tiền huấn luyện thích ứng tác vụ, nơi chúng ta chỉ sử dụng dữ liệu không được gán nhãn đích cho tiếp tục tiền huấn luyện mô hình ngôn ngữ trước khi tinh chỉnh mô hình trên dữ liệu được gán nhãn đích. Để cải thiện hiệu quả của tiếp tục tiền huấn luyện, Kang et al. (2020); Ye et al. (2021) đề xuất học để che đầu vào cho mã hóa tự động có mặt nạ với tối ưu hóa bilevel, yêu cầu chi phí tính toán cấm đoán. Tuy nhiên, tất cả chúng đều tập trung duy nhất vào các mô hình ngôn ngữ tiền huấn luyện và chúng tôi thấy thực nghiệm rằng tiếp tục tiền huấn luyện đơn thuần không hiệu quả đối với Vision Transformers.

Chính Quy Hóa cho Tinh Chỉnh Có một số công trình đề xuất chính quy hóa cho tinh chỉnh một mô hình tiền huấn luyện. Chen et al. (2020a) đề xuất sửa đổi bộ tối ưu hóa Adam (Kingma & Ba, 2015), được gọi là RecAdam, ép buộc mô hình tinh chỉnh gần với mô hình tiền huấn luyện ban đầu bằng cách tối thiểu hóa khoảng cách L2 giữa trọng số tinh chỉnh và trọng số tiền huấn luyện ban đầu. Tương tự, Gouk et al. (2021) chiếu trọng số tinh chỉnh cho mỗi cập nhật gradient descent sao cho nó nằm trong hình cầu tâm trên trọng số tiền huấn luyện ban đầu với khoảng cách được gây ra bởi chuẩn của tổng tuyệt đối hàng tối đa (MARS). Thay vì tối thiểu hóa khoảng cách một cách rõ ràng, được động lực bởi lý thuyết vùng tin cậy, Aghajanyan et al. (2021) đề xuất tối thiểu hóa độ phân kỳ KL đối xứng giữa đầu ra mô hình của đầu vào gốc và đầu ra của đầu vào bị nhiễu bởi Gaussian. Tuy nhiên, tất cả chúng đều không xem xét việc thích ứng các mô hình tiền huấn luyện với một miền đích cụ thể, dẫn đến hiệu suất tổng quát hóa tệ hơn của các tác vụ downstream so với chiến lược tinh chỉnh đơn giản.

3 PHƯƠNG PHÁP

3.1 KIẾN THỨC CƠ SỞ

Phát Biểu Vấn Đề Chúng tôi giả định rằng chúng ta được cho các tham số (θinit, ϕinit) của mạng nơ-ron gϕinit◦fθinit được tiền huấn luyện trên một khối lượng lớn dữ liệu không được gán nhãn với mục tiêu mã hóa tự động có mặt nạ, nơi fθinit là một bộ mã hóa trích xuất biểu diễn ẩn của một đầu vào và gϕinit là một bộ giải mã tái tạo một đầu vào có mặt nạ. Mục tiêu của chúng ta là tinh chỉnh mô hình tiền huấn luyện fθinit với một đầu đặc biệt cho tác vụ được khởi tạo ngẫu nhiên hω trên tập dữ liệu được gán nhãn Dtr={(x(i), y(i))}n i=1 của một tác vụ phân loại downstream sao cho mô hình tổng quát hóa tốt với tập dữ liệu kiểm tra chưa được nhìn thấy Dtest. Một phương pháp điển hình để đạt được mục tiêu này là tối thiểu hóa rủi ro thực nghiệm như sau:

minimize θ,ω LCE(θ, ω; Dtr) thông qua thuật toán A như 
(θ∗, ω∗) = A(LCE; θinit, Dtr), (1)

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

[Phần này tiếp tục với Algorithm 1 và Algorithm 2, cùng với các công thức toán học chi tiết...]

Algorithm 1 Tự Chưng Cất
Yêu cầu: Tập dữ liệu không được gán nhãn Du, khởi tạo θinit, ϕinit, tốc độ học α∈R≥0, vòng tự chưng cất T′∈N+, xác suất che γ∈(0,1) và kích thước batch B.
1: θ0←Tiếp Tục Tiền Huấn Luyện (Du, θinit, ϕinit, α, γ, B)
2: for all t←1, . . . , T′ do
3: Khởi tạo θt←θinit và ϕt←ϕinit
4: while chưa hội tụ do
5: Lấy mẫu một mini-batch {x(j)}B j=1 từ Du
6: for all j←1, . . . , B do
7: Lấy mẫu một mặt nạ z(j)∼pγ,K(z)
8: Z(j)←PK k=1z(j) k
9: Lấy một đầu vào có mặt nạ ˆx(j) với z(j)
10: ℓ1 j← −PK k=1z(j) k Z(j)logpθt,ϕt(x(j) k|ˆx(j))
11: ℓ2 j←‖fθt(x(j))−StopGrad (fθ0(x(j)))‖2 2
12: end for
13: L1←1 B PB j=1ℓ1 j, L2←1 B PB j=1ℓ2 j
14: θt←θt−α∂(L1+L2) ∂θ|θ=θt
15: ϕt←ϕt−α∂L1 ∂ϕ|ϕ=ϕt
16: end while
17: θ0←θt
18: end for
19: return θT′

Algorithm 2 Tiếp Tục Tiền Huấn Luyện
Yêu cầu: Tập dữ liệu không được gán nhãn Du, khởi tạo θinit, ϕinit, tốc độ học α∈R≥0, xác suất che γ∈(0,1), và kích thước batch B.
1: Khởi tạo θ0←θinit và ϕ0←ϕinit
2: while chưa hội tụ do
3: Lấy mẫu một mini-batch {x(j)}B j=1 từ Du
4: for all j←1, . . . , B do
5: Lấy mẫu một mặt nạ z(j)∼pγ,T(z)
6: Z(j)←PK k=1z(j) k
7: Lấy một đầu vào có mặt nạ ˆx(j) với z(j)
8: pk←pθ0,ϕ0(x(j) k|ˆx(j))
9: ℓ1 j← −PK k=1z(j) k Z(j)logpk
10: end for
11: L ←1 B PB j=1ℓ1 j
12: θ0←θ0−α∂L ∂θ|θ=θ0
13: ϕ0←ϕ0−α∂L ∂ϕ|ϕ=ϕ0
14: end while
15: return θ0

nơi LCE là một mất mát cross-entropy và A biểu thị một thuật toán gradient descent ngẫu nhiên để tối thiểu hóa LCE trên tập dữ liệu Dtr với khởi tạo θinit.

Tiếp Tục Tiền Huấn Luyện Tuy nhiên, mô hình tiền huấn luyện dễ bị overfitting khi nó được tinh chỉnh trên một lượng nhỏ dữ liệu được gán nhãn chuyên biệt về miền. Gururangan et al. (2020) đã cho thấy rằng tiếp tục tiền huấn luyện, nơi chúng ta tiếp tục tiền huấn luyện mô hình gϕinit◦fθinit trên tập dữ liệu không được gán nhãn đích Du={x(i)}n i=1 và sau đó tinh chỉnh nó trên Dtr, là hiệu quả để cải thiện hiệu suất tổng quát hóa khi không có đủ dữ liệu được gán nhãn chuyên biệt về miền. Lưu ý rằng Du hoàn toàn giống như Dtr ngoại trừ việc chúng ta loại bỏ các nhãn y(i). Trong công trình này, chúng tôi tập trung vào mã hóa tự động có mặt nạ (Devlin et al., 2019; He et al., 2022) như một hàm mục tiêu tiền huấn luyện vì tính tổng quát của nó so với các phương pháp tự giám sát khác (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; He et al., 2020; Chen & He, 2021; Caron et al., 2021) yêu cầu tăng cường dữ liệu được định nghĩa tốt để xây dựng các cặp dương cho tự học có giám sát.

Mã Hóa Tự Động Có Mặt Nạ Chúng tôi mô tả ngắn gọn mục tiêu mã hóa tự động có mặt nạ (Liu et al., 2019; He et al., 2022) cho một mô hình ngôn ngữ như RoBERTA (Liu et al., 2019) và Vision Transformer (ViT) (Dosovitskiy et al., 2021). Cho x(i) = (x(i) 1, . . . , x(i) K) là một chuỗi các patch cho một hình ảnh hoặc token cho một câu với độ dài K. Sau đó chúng ta lấy mẫu độc lập một mặt nạ nhị phân từ phân phối Bernoulli với xác suất γ cho mỗi x(i) k, ký hiệu là z(i) = (z(i) 1, . . . , z(i) K). Nếu z(i) k = 1, thì x(i) k được thay thế bằng một token "mặt nạ" đặc biệt. Ngược lại, chúng ta sử dụng cùng x(i) k cho một đầu vào có mặt nạ. Cho ˆx(i) = (ˆx(i) 1, . . . , ˆx(i) K) là một đầu vào có mặt nạ và cho fθ, gϕ là một bộ mã hóa và bộ giải mã, tương ứng. Sau đó mục tiêu cuối cùng cho mã hóa tự động có mặt nạ được định nghĩa như sau:

LMAE(θ, ϕ; Du) = 1 n ∑n i=1 Ez(i)∼pγ,T(z)[ −∑K k=1 z(i) k Z(i) · logpθ,ϕ(x(i) k|ˆx(i))], Z(i) = ∑K k=1 z(i) k, (2)

nơi pγ,K(z) biểu thị một phân phối Binomial với các tham số γ của nó cho xác suất rằng zk = 1 và K cho số lần thử. Lưu ý rằng negative log-likelihood được thể hiện dưới dạng mất mát cross-entropy cho các mô hình ngôn ngữ hoặc mean square error cho Vision Transformers. Xem Phụ lục C để biết thêm chi tiết.

3.2 TỰ CHƯNG CẤT CHO TIẾP TỤC TIỀN HUẤN LUYỆN

Mặc dù chiến lược tiếp tục tiền huấn luyện đã hiệu quả trên miền văn bản (Gururangan et al., 2020; Lee et al., 2020; Sun et al., 2020), chúng tôi thấy thực nghiệm rằng ViT với tiếp tục tiền huấn luyện overfit

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

dữ liệu không được gán nhãn đích và không tổng quát hóa tốt cho các tác vụ phân loại hình ảnh downstream. Để giải quyết vấn đề này, chúng tôi đề xuất tự chưng cất như một chính quy hóa cho tiếp tục tiền huấn luyện. Cụ thể, cho một mô hình tiền huấn luyện gϕinit◦fθinit, trước tiên chúng tôi tiếp tục huấn luyện mô hình trên dữ liệu không được gán nhãn đích Du với mục tiêu mã hóa tự động có mặt nạ như được mô tả trong phương trình 2 để thu được bộ mã hóa fθ0 và bộ giải mã gϕ0. Chúng tôi loại bỏ bộ giải mã và coi bộ mã hóa fθ0 như một giáo viên cho tự chưng cất. Sau đó chúng tôi lấy bản sao của mạng tiền huấn luyện ban đầu gϕinit◦fθinit như một học sinh và tiếp tục tiền huấn luyện học sinh với mục tiêu mã hóa tự động có mặt nạ nhưng ép buộc biểu diễn ẩn của bộ mã hóa của học sinh fθinit gần với biểu diễn của giáo viên fθ0 như sau:

(θ1, ϕ1) ∈ arg min θ,ϕ (LMAE(θ, ϕ; Du) + LDistill(θ; θ0, Du))
LDistill(θ; θ0, Du) = 1/n ∑n i=1 ‖fθ(x(i)) − StopGrad(fθ0(x(i)))‖²₂ (3)

nơi θ và ϕ được khởi tạo với các tham số tiền huấn luyện θinit và ϕinit, tương ứng và StopGrad biểu thị phép toán stop-gradient không lan truyền ngược qua đầu vào. Như được mô tả trong Algorithm 1, chúng ta có thể lặp lại quá trình này để thực hiện nhiều vòng tự chưng cất (T′ > 1) nơi học sinh của vòng trước trở thành giáo viên và một học sinh mới được khởi tạo với các trọng số tiền huấn luyện θinit và ϕinit cho vòng tiếp theo. Chúng tôi thấy thực nghiệm rằng vòng đầu tiên của tự chưng cất đóng vai trò quan trọng nhất trong việc cải thiện hiệu suất tổng quát hóa cuối cùng của các tác vụ downstream. Hơn nữa, phân tích lý thuyết cho thấy rằng vòng đầu tiên của tự chưng cất có tác động lớn nhất đến chính quy hóa. Do đó, chúng tôi thực hiện một vòng duy nhất tự chưng cất để đạt hiệu quả tính toán. Sau tự chưng cất, chúng tôi loại bỏ bộ giải mã gϕ1 và tinh chỉnh bộ mã hóa của học sinh fθ1 cùng với một đầu đặc biệt cho tác vụ được khởi tạo ngẫu nhiên hω bằng cách tối thiểu hóa LCE(θ, ω; Dtr) với khởi tạo θ1 như được mô tả trong phương trình 1.

4 PHÂN TÍCH LÝ THUYẾT

Trong phần này, chúng tôi phân tích cách tự chưng cất ảnh hưởng đến mô hình cuối cùng sau tinh chỉnh về mặt tổng quát hóa và chính quy hóa. Phần này chứng minh một ràng buộc tổng quát hóa trên mất mát có giám sát cho phương pháp của chúng tôi và cho thấy rằng ràng buộc tổng quát hóa giảm nghiêm ngặt khi số lượng tự chưng cất tăng lên. Hơn nữa, chúng tôi cho thấy rằng tự chưng cất hoạt động như một bộ chính quy hóa trên khoảng cách giữa trọng số ban đầu trước tiếp tục tiền huấn luyện và trọng số cuối cùng sau tinh chỉnh. Hiệu ứng chính quy hóa được cho thấy có tác động lớn nhất trong vòng đầu tiên của tự chưng cất, điều này gợi ý rằng vòng đầu tiên của tự chưng cất đóng vai trò quan trọng hơn trong hiệu suất cuối cùng khi so sánh với các vòng khác.

[Phần còn lại tiếp tục với các định lý và chứng minh toán học chi tiết...]

--- TRANG 6 ---

[Tiếp tục với các định lý và chứng minh toán học...]

Định lý 1. Tồn tại một hằng số c (chỉ phụ thuộc vào M) sao cho với xác suất ít nhất 1−δ, điều sau đây đúng:

Ex,y[ℓ(wt,T, x, y)] ≤ 1/n ∑n i=1 ℓ(wt,T, xi, yi) + ζ(t)√(4c²R²p/n) + M√(ln(2/δ)/2n), (4)

nơi hàm ζ(t) giảm nghiêm ngặt trong t ∈ N0.

[Tiếp tục với Định lý 2 và các chứng minh...]

--- TRANG 7 ---

[Phần thí nghiệm bắt đầu...]

5 THÍ NGHIỆM

Tập Dữ Liệu Đối với bài toán phân loại hình ảnh, chúng tôi sử dụng sáu tập dữ liệu — FGVC Aircraft (Aircraft) (Maji et al., 2013), Caltech UCSD Birds 200 (CUB) (Wah et al., 2011), Chest X-ray (Kermany et al., 2018), Describable Textures Dataset (DTD) (Cimpoi et al., 2014), Stanford Dogs (Khosla et al., 2011), và Oxford 102 Flower (Nilsback & Zisserman, 2008). Đối với bài toán phân loại văn bản, chúng tôi sử dụng bốn tập dữ liệu — Chemprot (Kringelum et al., 2016), ACL-ARC (Jurgens et al., 2018), SCIERC (Luan et al., 2018), và Twitter-Emotion (Mohammad et al., 2018). Vui lòng xem Phụ lục D để biết thêm chi tiết.

Chi Tiết Triển Khai Đối với bài toán phân loại hình ảnh, chúng tôi sử dụng Vision Transformer tiền huấn luyện trên tập dữ liệu ImageNet không được gán nhãn với mã hóa tự động có mặt nạ (He et al., 2022) và tinh chỉnh nó trên tác vụ downstream với bộ tối ưu hóa AdamW (Loshchilov & Hutter, 2019) trong 10,000 bước với kích thước batch 32. Về tiếp tục tiền huấn luyện và tự chưng cất, chúng tôi tiếp tục tiền huấn luyện mô hình trong 20,000 bước với kích thước batch 64. Chúng tôi đánh giá các Vision Transformers với độ chính xác.

Đối với phân loại văn bản, theo thiết lập thí nghiệm từ Gururangan et al. (2020), chúng tôi sử dụng RoBERTA (Liu et al., 2019) như một mạng cơ sở và tinh chỉnh nó trên tập dữ liệu được gán nhãn đích

--- TRANG 8 ---

với bộ tối ưu hóa AdamW trong 10 epochs với kích thước batch 32. Về tiếp tục tiền huấn luyện và tự chưng cất, chúng tôi tiếp tục tiền huấn luyện RoBERTA trong 100 epochs với kích thước batch 128. Chúng tôi đánh giá các mô hình với macro F1 cho tập dữ liệu SCIERC, ACL-ARC, và Twitter-Emotion, và micro F1 cho tập dữ liệu Chemprot.

Baselines Chúng tôi so sánh phương pháp của chúng tôi với các baseline sau đây nhắm vào tinh chỉnh các mô hình tiền huấn luyện. Tất cả các mô hình được khởi tạo với các trọng số tiền huấn luyện θinit và ϕinit.

1. Tinh chỉnh: Mô hình được tinh chỉnh trên tập dữ liệu được gán nhãn đích Dtr mà không có bất kỳ tiếp tục tiền huấn luyện hoặc chính quy hóa nào ngoại trừ dropout và weight decay.

2. RecAdam (Chen et al., 2020a): Mô hình được huấn luyện với bộ tối ưu hóa RecAdam là một biến thể của bộ tối ưu hóa Adam (Kingma & Ba, 2015) và thêm phạt khoảng cách L2 giữa trọng số tinh chỉnh và trọng số tiền huấn luyện ban đầu.

3. MARS (Gouk et al., 2021): Mô hình được huấn luyện để tối thiểu hóa mất mát cross-entropy cùng với chính quy hóa chiếu trọng số tinh chỉnh nằm trong một hình cầu tâm trên trọng số tiền huấn luyện ban đầu. Đối với mỗi lớp, khoảng cách được gây ra bởi chuẩn ma trận Maximum Absolute Row Sum (MARS) (max j ∑ i=1|Wj,i−Uj,i|) được sử dụng cho chính quy hóa.

4. R3F (Aghajanyan et al., 2021): Mô hình được huấn luyện để tối thiểu hóa mất mát cross-entropy cũng như độ phân kỳ KL đối xứng giữa đầu ra softmax của đầu vào gốc và đầu ra của đầu vào bị nhiễu bởi Gaussian.

5. Tiếp tục Tiền huấn luyện (Gururangan et al., 2020): Tiền huấn luyện thích ứng tác vụ nơi chúng ta tiếp tục tiền huấn luyện mô hình trên tập dữ liệu không được gán nhãn đích Du với mục tiêu mã hóa tự động có mặt nạ và tinh chỉnh nó trên tập dữ liệu được gán nhãn đích Dtr.

6. Tự Chưng cất: Đây là mô hình của chúng tôi được tiếp tục tiền huấn luyện trên tập dữ liệu không được gán nhãn đích Du với phương trình 3 và tinh chỉnh trên tập dữ liệu được gán nhãn đích Dtr.

5.1 KẾT QUẢ CHÍNH

Như được hiển thị trong Bảng 1, tự chưng cất nhất quán vượt trội hơn tất cả các phương pháp chính quy hóa và phương pháp tiếp tục tiền huấn luyện trên các tập dữ liệu hình ảnh. Đáng chú ý, phương pháp của chúng tôi cải thiện đáng kể hiệu suất của tập dữ liệu Chest X-ray bao gồm các hình ảnh thang xám để chẩn đoán viêm phổi. Ngoài ra, tự chưng cất hiệu quả giải quyết tập dữ liệu Flower chỉ chứa 2,040 ví dụ được gán nhãn. Ngược lại, các baseline khác không cho thấy cải thiện nhất quán trên tất cả các tập dữ liệu hình ảnh. Ví dụ, tiếp tục tiền huấn luyện hiệu quả cho tập dữ liệu Aircraft, nhưng làm giảm đáng kể độ chính xác kiểm tra trên tập dữ liệu DTD. Các phương pháp chính quy hóa như RecAdam, MARS, và R3F hầu như không cải thiện hiệu suất tổng quát hóa trên hầu hết các tập dữ liệu hoặc kém hiệu quả hơn chiến lược tinh chỉnh đơn giản trên các tập dữ liệu nhất định. Bằng chứng thực nghiệm này hỗ trợ rằng các chính quy hóa ép buộc các mô hình tinh chỉnh gần với trọng số tiền huấn luyện ban đầu không hiệu quả để thích ứng một mô hình tiền huấn luyện với các tập dữ liệu đích của các miền cụ thể.

[Bảng 1: Trung bình và độ lệch chuẩn của độ chính xác với 5 lần chạy cho các tập dữ liệu phân loại hình ảnh.]

Hơn nữa, như được hiển thị trong Bảng 2, chúng tôi cung cấp kết quả thí nghiệm bổ sung cho các tác vụ phân loại văn bản. Một lần nữa, tự chưng cất vượt trội đáng kể so với tất cả các baseline trên cả bốn tập dữ liệu,

[Bảng 2: Trung bình và độ lệch chuẩn của điểm F1 với 5 lần chạy cho các tập dữ liệu phân loại văn bản.]

--- TRANG 9 ---

ngoại trừ RecAdam trong tập dữ liệu Chemprot. Ngược lại với thí nghiệm trước đó, phương pháp tiếp tục tiền huấn luyện cải thiện điểm F1 kiểm tra của phương pháp tinh chỉnh đơn giản, nhưng vẫn kém hiệu quả hơn mô hình của chúng tôi. Đối với các phương pháp chính quy hóa — RecAdam, MARS, và R3F, chúng không đạt được cải thiện nhất quán trên cả ba tập dữ liệu. RecAdam cải thiện vừa phải điểm F1 trên tập dữ liệu SCIERC và Chemprot nhưng làm giảm đáng kể hiệu suất tổng quát hóa trên tập dữ liệu ACL-ARC. Cả MARS và R3F đều cho thấy hiệu suất kém trên các tập dữ liệu SCIERC và ACL-ARC, và hiệu suất của chúng hơi tệ hơn phương pháp Tinh chỉnh trên tập dữ liệu Chemprot.

Kết Quả cho Dữ Liệu Tài Nguyên Thấp Chúng tôi tiếp tục thực hiện thí nghiệm để cho thấy cách tự chưng cất hiệu quả xử lý tài nguyên thấp của dữ liệu được gán nhãn. Cho một tập dữ liệu CIFAR-100 đầy đủ (Krizhevsky et al., 2009) chứa 50,000 cặp huấn luyện của một hình ảnh và nhãn tương ứng, chúng tôi vẽ độ chính xác kiểm tra của mỗi mô hình bằng cách thay đổi số lượng thể hiện huấn luyện. Lưu ý rằng chúng tôi cũng giảm số lượng hình ảnh không được gán nhãn được sử dụng cho tiếp tục tiền huấn luyện hoặc tự chưng cất. Như được hiển thị trong Hình 3, tự chưng cất nhất quán cải thiện hiệu suất tổng quát hóa của cả phương pháp tinh chỉnh và mô hình được tiếp tục tiền huấn luyện trên các hình ảnh từ tập dữ liệu CIFAR-100. Đáng chú ý, lợi ích từ tự chưng cất trở nên lớn hơn khi các mô hình được huấn luyện với một số lượng cực kỳ nhỏ thể hiện. Ví dụ, tự chưng cất đạt được cải thiện 13% và 6% độ chính xác kiểm tra so với mô hình với tinh chỉnh đơn giản khi có 1,000 và 2,500 ví dụ được gán nhãn, tương ứng. Những kết quả thực nghiệm này xác minh rằng tự chưng cất có thể thích ứng hiệu quả mô hình tiền huấn luyện với tập dữ liệu đích ngay cả khi có lượng dữ liệu được gán nhãn cực kỳ nhỏ.

[Hình 3: Độ chính xác với việc thay đổi số lượng dữ liệu huấn luyện.]

Nghiên Cứu Ablation Chúng tôi thực hiện nghiên cứu ablation để xác minh hiệu quả của từng thành phần của tự chưng cất. Trong Bảng 3, chúng tôi hiển thị kết quả thực nghiệm trên cả tập dữ liệu CUB và tập dữ liệu SCIERC trong khi loại bỏ hoặc thay thế các thành phần khác nhau của tự chưng cất. Đầu tiên, chúng tôi loại bỏ mục tiêu mã hóa tự động có mặt nạ LMAE và huấn luyện mô hình chỉ với mất mát chưng cất LDistill trước tinh chỉnh. Trên tập dữ liệu hình ảnh CUB, nó không tạo ra sự khác biệt đáng kể, tuy nhiên, việc loại bỏ mục tiêu mã hóa tự động có mặt nạ làm giảm hiệu suất tổng quát hóa của mô hình ngôn ngữ trên tập dữ liệu phân loại văn bản SCIERC. Thay vào đó, chúng tôi loại bỏ mất mát chưng cất LDistill trong phương trình 3, dẫn đến phương pháp tiếp tục tiền huấn luyện. Hơn nữa, chúng tôi tiếp tục tiền huấn luyện mô hình trong gấp đôi số bước dài hơn so với phương pháp tiếp tục tiền huấn luyện ban đầu, ký hiệu là Tiếp tục Tiền huấn luyện ×2, để cho thấy rằng độ chính xác kiểm tra cao hơn của tự chưng cất không phải là hệ quả của tiền huấn luyện dài hơn. Cả hai mô hình đều kém hiệu quả đáng kể so với tự chưng cất, điều này cho thấy hiệu quả của mất mát tự chưng cất. Cuối cùng, chúng tôi thực hiện thí nghiệm cho các biến thể của mất mát chưng cất LDistill trong phương trình 3. Thay vì khớp biểu diễn của giáo viên và học sinh, chúng tôi ép buộc việc tái tạo các đầu vào có mặt nạ bởi giáo viên và học sinh nhất quán, tức là tối thiểu hóa θ,ϕ ‖gϕ◦fθ(ˆx)−gϕ0◦fθ0(ˆx)‖²₂ cho ViT hoặc tối thiểu hóa θ,ϕ ∑T t=1 DKL(pθ0,ϕ0(xt|ˆx)‖pθ,ϕ(xt|ˆx)) cho RoBERTA, ký hiệu là Khớp-Dự đoán. Hơn nữa, chúng tôi thay thế mất mát chưng cất bằng một mất mát tối thiểu hóa khoảng cách L2 hoặc MARS giữa các tham số của học sinh và giáo viên, ký hiệu là Khớp-Trọng số. Như được hiển thị trong Bảng 3, tất cả các biến thể này không hiệu quả so với biến thể tối thiểu hóa khoảng cách giữa biểu diễn ẩn của học sinh và giáo viên.

[Bảng 3: Ablation trên CUB và SCIERC.]

Nhiều Vòng Tự Chưng Cất Cuối cùng, chúng tôi chứng minh thực nghiệm rằng vòng đầu tiên của tự chưng cất đóng vai trò quan trọng nhất trong việc cải thiện hiệu suất tổng quát hóa. Cụ thể, chúng tôi tinh chỉnh mỗi mô hình sau t vòng tự chưng cất và vẽ độ chính xác kiểm tra trên tập dữ liệu Oxford 102 Flower, nơi 0 vòng tự chưng cất (t = 0) biểu thị mô hình với tiếp tục tiền huấn luyện. Như được hiển thị trong Hình 4, vòng đầu tiên của tự chưng cất cải thiện đáng kể độ chính xác kiểm tra của mô hình với tiếp tục tiền huấn luyện và lợi ích từ tự chưng cất trở nên nhỏ sau vòng đầu tiên. Xem xét chi phí tính toán bổ sung và cải thiện nhỏ của nhiều vòng tự chưng cất, chúng tôi thực hiện một vòng duy nhất tự chưng cất cho tất cả các thí nghiệm.

[Hình 4: Độ chính xác kiểm tra với việc thay đổi vòng tự chưng cất.]

--- TRANG 10 ---

5.2 PHÂN TÍCH THÊM

Trong phần này, chúng tôi trình bày các thí nghiệm số để phân tích tại sao tự chưng cất có thể tiềm năng giúp cải thiện hiệu suất tổng quát hóa của các tác vụ downstream so với tiếp tục tiền huấn luyện và chứng minh thực nghiệm rằng Định lý 1 và 2 có thể được mở rộng cho các mạng nơ-ron sâu — transformers.

(a) Khoảng cách tổng quát hóa: Trong Hình 5a, chúng tôi vẽ khoảng cách tổng quát hóa, là mất mát kiểm tra trừ mất mát huấn luyện trên mỗi tập dữ liệu được gán nhãn, của tự chưng cất và phương pháp tiếp tục tiền huấn luyện. Tự chưng cất cải thiện khoảng cách tổng quát hóa của phương pháp tiếp tục tiền huấn luyện trên tất cả các tập dữ liệu. Điều này nhất quán với Định lý 1 cho thấy rằng tự chưng cất với một mô hình đơn giản hóa giảm nghiêm ngặt ràng buộc tổng quát hóa trên mất mát có giám sát của giai đoạn tinh chỉnh.

(b) Hiệu ứng của tự chưng cất trên khoảng cách: Để xác thực thực nghiệm Định lý 2 về hiệu ứng chính quy hóa bởi tự chưng cất trên khoảng cách L2 giữa trọng số tiền huấn luyện ban đầu θinit và trọng số cuối cùng sau tinh chỉnh, chúng tôi vẽ khoảng cách thu được từ tự chưng cất và tiếp tục tiền huấn luyện. Cụ thể, chúng tôi so sánh khoảng cách ‖θinit−θ1,T‖₂ và ‖θinit−θ0,T‖₂, nơi θt,τ là tham số sau t vòng tự chưng cất và τ bước gradient descent cho tinh chỉnh. Như được hiển thị trong Hình 5b, tự chưng cất nhất quán giảm khoảng cách và khoảng cách giảm tương quan với khoảng cách tổng quát hóa tốt hơn trong Hình 5a. Những kết quả thực nghiệm này xác nhận mối liên hệ giữa khoảng cách L2 từ khởi tạo và ràng buộc tổng quát hóa (Nagarajan & Kolter, 2019).

(c) Hiệu ứng của nhiều vòng tự chưng cất: Cuối cùng, chúng tôi xác thực thực nghiệm một phần của Định lý 2 cho thấy rằng vòng đầu tiên của tự chưng cất đóng vai trò quan trọng nhất của chính quy hóa trên khoảng cách L2 giữa trọng số tiền huấn luyện ban đầu θinit và trọng số cuối cùng θt,T ký hiệu là tham số sau t vòng tự chưng cất và T bước gradient descent cho tinh chỉnh trên tập dữ liệu VGG flower 102. Như được hiển thị trong Hình 5c, tự chưng cất giảm đáng kể khoảng cách ở vòng đầu tiên (t = 1) và hiệu ứng chính quy hóa trên khoảng cách giảm dần sau đó, nơi 0 vòng tự chưng cất (t = 0) biểu thị mô hình với tiếp tục tiền huấn luyện nhưng không có tự chưng cất.

[Hình 5: (a) Khoảng cách tổng quát hóa, (b) Hiệu ứng của tự chưng cất trên khoảng cách, (c) Hiệu ứng của nhiều vòng tự chưng cất]

6 KẾT LUẬN

Để thích ứng hiệu quả các transformers tiền huấn luyện với một miền đích, chúng tôi đã đề xuất tự chưng cất như một chính quy hóa cho tiếp tục tiền huấn luyện. Cụ thể, trước tiên chúng tôi lấy transformer tiền huấn luyện ban đầu và tiếp tục tiền huấn luyện nó với mục tiêu mã hóa tự động có mặt nạ trên tập dữ liệu không được gán nhãn đích và coi phần bộ mã hóa của mô hình như một giáo viên cho tự chưng cất. Sau đó chúng tôi lấy bản sao của cùng mô hình tiền huấn luyện ban đầu như một học sinh và ép buộc biểu diễn của học sinh gần với những của giáo viên trong khi tối ưu hóa học sinh với mục tiêu mã hóa tự động có mặt nạ trên tập dữ liệu không được gán nhãn đích. Cuối cùng, chúng tôi tinh chỉnh học sinh tự chưng cất trên tập dữ liệu được gán nhãn đích. Đánh giá thực nghiệm của chúng tôi trên nhiều tập dữ liệu chuẩn phân loại hình ảnh và văn bản khác nhau cho thấy rằng tự chưng cất nhất quán cải thiện hiệu suất tổng quát hóa so với các baseline liên quan. Cuối cùng, chúng tôi cung cấp phân tích lý thuyết của phương pháp đề xuất với một mô hình đơn giản hóa để hiểu cách tự chưng cất cho tiếp tục tiền huấn luyện có thể tiềm năng giúp cải thiện hiệu suất tổng quát hóa của các tác vụ downstream.

--- TRANG 11 ---

TUYÊN BỐ VỀ KHẢ NĂNG TÁI TẠO

Chúng tôi sử dụng Pytorch (Paszke et al., 2019) và thư viện transformers (Wolf et al., 2020) từ Huggingface để triển khai tất cả các baseline và phương pháp đề xuất của chúng tôi trong các thí nghiệm. Chúng tôi đã mô tả phương pháp tự chưng cất cho tiếp tục tiền huấn luyện của chúng tôi trong Algorithm 1 và chỉ định tất cả thiết lập thí nghiệm bao gồm các siêu tham số trong Phần 5 và Phụ lục E. Đối với phân tích lý thuyết, chúng tôi đã cung cấp tất cả các chứng minh trong Phụ lục A.

LỜI CẢM ƠN

Công trình này được hỗ trợ bởi viện Kế hoạch & Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2019-0-00075, Chương trình Trường Đại học Trí tuệ Nhân tạo (KAIST)), Chương trình Trung tâm Nghiên cứu Kỹ thuật thông qua Quỹ Nghiên cứu Quốc gia Hàn Quốc (NRF) được tài trợ bởi Chính phủ Hàn Quốc MSIT (NRF-2018R1A5A1059921), viện Kế hoạch & Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2021-0-02068, Trung tâm Đổi mới Trí tuệ Nhân tạo), viện Kế hoạch & Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2022-0-00184, Phát triển và Nghiên cứu Công nghệ AI để Tuân thủ Chính sách Đang phát triển về Đạo đức một cách Không tốn kém), viện Kế hoạch & Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2022-0-00713), Trung tâm AI Siêu sáng tạo KAIST-NAVER, và Samsung Electronics (IO201214-08145-01). Tài liệu này dựa trên công việc được hỗ trợ bởi chương trình Google Cloud Research Credits với giải thưởng (6NW8-CF7K-3AG4-1WH1).

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo đầy đủ từ trang 11-13...]

--- TRANG 14 ---

PHỤ LỤC

A CHỨNG MINH

[Các chứng minh toán học chi tiết từ trang 14-22...]
