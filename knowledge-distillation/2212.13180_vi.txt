# 2212.13180.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2212.13180.pdf
# Kích thước tệp: 4072720 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 1
Chưng cất Tri thức Đa nhiệm có Hướng dẫn Nguyên mẫu
cho các Mô hình Quy mô lớn
Deng Li, Aming Wu, Yahong Han, Qi Tian
Tóm tắt —Gần đây, các mô hình được huấn luyện trước ở quy mô lớn đã thể hiện
những ưu thế của chúng trong nhiều nhiệm vụ. Tuy nhiên, do độ phức tạp tính toán
và yêu cầu lưu trữ khổng lồ, việc áp dụng mô hình quy mô lớn vào các cảnh thực tế
là một thách thức. Một giải pháp phổ biến là chưng cất tri thức, coi mô hình quy mô lớn
như một mô hình giáo viên và giúp huấn luyện một mô hình học sinh nhỏ để đạt được
hiệu suất cạnh tranh. Các phương pháp chưng cất tri thức tiêu chuẩn chủ yếu yêu cầu
mô hình giáo viên và mô hình học sinh thực hiện cùng một nhiệm vụ, ví dụ, mô hình
học sinh và mô hình giáo viên chia sẻ cùng một không gian nhãn, điều này hạn chế
ứng dụng của nó trong tình huống thực tế. Chưng cất tri thức đa nhiệm có thể chuyển
giao tri thức của các mô hình giáo viên sang các mô hình học sinh có không gian nhãn
khác nhau mà không cần tinh chỉnh, điều này mở rộng các kịch bản ứng dụng của mô hình
được huấn luyện trước ở quy mô lớn. Các công trình chưng cất tri thức hiện tại tập trung
vào việc bắt chước trực tiếp dự đoán cuối cùng hoặc các lớp trung gian của mô hình
giáo viên, đại diện cho các đặc tính cấp độ toàn cầu và đặc thù cho nhiệm vụ. Để giảm bớt
ràng buộc của các không gian nhãn khác nhau, việc nắm bắt các đặc tính đối tượng cục bộ
nội tại bất biến (như các đặc tính hình dạng của chân và đuôi của bò và ngựa) đóng vai trò
then chốt. Xem xét độ phức tạp và tính biến đổi của các nhiệm vụ cảnh thực tế, chúng tôi
đề xuất phương pháp Chưng cất Tri thức Đa nhiệm có Hướng dẫn Nguyên mẫu (ProC-KD)
để chuyển giao tri thức đối tượng cục bộ nội tại của mạng giáo viên quy mô lớn sang các
kịch bản nhiệm vụ khác nhau. Đầu tiên, để chuyển giao tốt hơn tri thức tổng quát trong mô hình
giáo viên trong các kịch bản đa nhiệm, chúng tôi đề xuất một mô-đun học nguyên mẫu để học
từ biểu diễn đặc trưng cơ bản của các đối tượng trong mô hình giáo viên. Thứ hai, đối với
các nhiệm vụ hạ nguồn đa dạng, chúng tôi đề xuất một mô-đun tăng cường đặc trưng thích ứng
nhiệm vụ để nâng cao các đặc trưng của mô hình học sinh với các đặc trưng nguyên mẫu tổng quát
đã học và hướng dẫn việc huấn luyện mô hình học sinh để cải thiện khả năng tổng quát của nó.
Kết quả thực nghiệm trên các nhiệm vụ thị giác khác nhau chứng minh tính hiệu quả của phương pháp
của chúng tôi cho các cảnh chưng cất tri thức đa nhiệm mô hình quy mô lớn.
Thuật ngữ chỉ mục —Chưng cất Tri thức, Đa nhiệm, Học nguyên mẫu, Mô hình Được huấn luyện trước Quy mô lớn.

I. GIỚI THIỆU
GẦN đây, mạng Transformer [1] đã đạt được những tiến bộ lớn trong một số nhiệm vụ thị giác, như phân loại hình ảnh [2]–[4], phát hiện đối tượng [5], [6], phân đoạn hình ảnh [7], nhận dạng hành động [8], và học kết hợp ngôn ngữ thị giác [9]–[12]. Các mạng Transformer dựa trên cơ chế tự chú ý có thể xử lý các chuỗi đầu vào hoàn chỉnh và có ưu thế về song song hóa. Do đó, nó

Deng Li và Yahong Han thuộc Trường Trí tuệ và Máy tính, Đại học Thiên Tân, Thiên Tân, Trung Quốc. (email: lideng@tju.edu.cn; yahong@tju.edu.cn)
Aming Wu thuộc Trường Kỹ thuật Điện tử, Đại học Tây An Điện tử, Tây An, Trung Quốc. (email: amwu@xidian.edu.cn).
Qi Tian thuộc Cloud & AI, Huawei Technologies, Thâm Quyến, Trung Quốc (email: tian.qi1@huawei.com).

thường được sử dụng để có được mô hình được huấn luyện trước từ các bộ dữ liệu quy mô lớn [13]. Hiện tại, chiến lược phổ biến để sử dụng các mô hình được huấn luyện trước quy mô lớn trên các nhiệm vụ hạ nguồn đa nhiệm là tinh chỉnh. Sau khi học biểu diễn đặc trưng tổng quát từ các bộ dữ liệu quy mô lớn, sau đó tinh chỉnh trên nhiệm vụ hạ nguồn với một số lượng nhỏ bộ dữ liệu để cải thiện hiệu suất của mô hình nhỏ. Tuy nhiên, do độ phức tạp tính toán khổng lồ và yêu cầu lưu trữ khổng lồ của các mô hình này, việc áp dụng chúng vào các kịch bản ứng dụng thực tế với tài nguyên hạn chế, ví dụ như thiết bị di động, đã trở thành một thách thức lớn.

Để giải quyết vấn đề ứng dụng mô hình trên, một số công nghệ nén và tăng tốc mô hình được đề xuất, ví dụ như cắt tỉa tham số [14], [15], lượng tử hóa mô hình [16], và chưng cất tri thức (KD) [17]. Đặc biệt, chưng cất tri thức là một phương pháp nén mô hình hiệu quả, chưng cất tri thức từ một mạng nơ-ron sâu lớn (mô hình giáo viên) vào một mạng nhỏ (mô hình học sinh) [17]–[21]. Không giống như các phương pháp nén mô hình khác, chưng cất tri thức có thể giảm kích thước của mạng và cải thiện hiệu suất của các mô hình nhỏ trên các nhiệm vụ hạ nguồn bất kể sự khác biệt cấu trúc giữa mạng giáo viên và học sinh. Thành công của nó đã được chứng kiến trong một loạt các ứng dụng như thị giác máy tính [17], [22]–[27], nhận dạng giọng nói [28]–[30], và xử lý ngôn ngữ tự nhiên [31]–[33].

Tuy nhiên, các phương pháp chưng cất tri thức này chủ yếu yêu cầu mô hình giáo viên và mô hình học sinh là cùng một nhiệm vụ, ví dụ như mô hình học sinh và mô hình giáo viên chia sẻ cùng một không gian nhãn, điều này hạn chế ứng dụng của chúng trong tình huống thực tế như các nhiệm vụ hạ nguồn trong các không gian nhãn khác nhau (như được hiển thị trong Hình 1 (a)).

Chưng cất tri thức đa nhiệm có thể chuyển giao tri thức của mô hình giáo viên sang các nhiệm vụ hạ nguồn trong các không gian nhãn khác nhau, điều này mở rộng ứng dụng của mô hình giáo viên trên nhiều nhiệm vụ hạ nguồn khác nhau. Các công trình chưng cất tri thức cùng nhiệm vụ hiện tại chủ yếu chuyển giao các logit dự đoán cuối cùng hoặc tri thức các lớp trung gian của mô hình giáo viên, đây là các căn chỉnh tri thức cấp độ toàn cầu và không thể được áp dụng trực tiếp cho chưng cất tri thức đa nhiệm. Công trình chưng cất tri thức đa nhiệm sớm hơn [34] căn chỉnh mối quan hệ so sánh bậc cao giữa các mô hình theo cách cục bộ, tuy nhiên, phương pháp này tụt hậu trong khả năng biểu diễn của đối tượng nội tại bất biến và là một phương pháp chưng cất hai giai đoạn.

Trong bối cảnh chưng cất tri thức đa nhiệm, các đặc tính đối tượng nội tại có thể đưa ra hướng dẫn có lợi cho việc huấn luyện mô hình học sinh, ví dụ như các đặc trưng hình dạng

arXiv:2212.13180v1  [cs.CV]  26 Dec 2022

--- TRANG 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 2

Hình 1. So sánh giữa các phương pháp thông thường và được đề xuất. (a) Phương pháp chưng cất tri thức thông thường trong đó mô hình giáo viên và mô hình học sinh chia sẻ cùng một không gian nhãn. Nhiệm vụ hạ nguồn bị giới hạn ở cùng một nhiệm vụ với mô hình giáo viên và là căn chỉnh tri thức cấp độ toàn cầu. (b) Phương pháp chưng cất tri thức đa nhiệm có hướng dẫn nguyên mẫu được đề xuất trong đó mô hình giáo viên và mô hình học sinh có các không gian nhãn khác nhau. Các nguyên mẫu học biểu diễn cục bộ nội tại bất biến từ nhúng của mô hình giáo viên quy mô lớn và hướng dẫn việc học của các mô hình học sinh đa nhiệm khác nhau.

của chân của một con bò và một con ngựa khi bò và ngựa thuộc về các bộ dữ liệu của mô hình giáo viên và mô hình học sinh, tương ứng. Xem xét độ phức tạp và tính biến đổi của các nhiệm vụ cảnh thực tế và khả năng tổng quát của các mô hình được huấn luyện trước quy mô lớn, trong bài báo này, chúng tôi đề xuất phương pháp Chưng cất Tri thức Đa nhiệm có Hướng dẫn Nguyên mẫu (ProC-KD) để chuyển giao tri thức nội tại cục bộ của mạng giáo viên quy mô lớn sang các kịch bản nhiệm vụ khác nhau (như được hiển thị trong Hình 1 (b)). Và phương pháp của chúng tôi để có được mô hình nhỏ cho các nhiệm vụ hạ nguồn là một quá trình huấn luyện một giai đoạn.

Cụ thể, phương pháp được đề xuất của chúng tôi bao gồm hai mô-đun tích hợp: một mô-đun học biểu diễn dựa trên nguyên mẫu và một mô-đun tăng cường đặc trưng.

Mô-đun học nguyên mẫu được thiết kế cẩn thận để nắm bắt thông tin đặc trưng cơ bản từ đặc trưng trung gian của mô hình giáo viên. Tiếp theo, biểu diễn nguyên mẫu đã học được đưa vào mô-đun tăng cường đặc trưng. Mô-đun tăng cường đặc trưng nhắm mục tiêu làm phong phú đặc trưng mô hình học sinh có liên quan nhiều hơn đến biểu diễn nguyên mẫu trong khi triệt tiêu đặc trưng không liên quan.

Để hướng dẫn việc huấn luyện mô hình học sinh với biểu diễn nguyên mẫu tổng quát đã học, chúng tôi sử dụng một hàm mất mát nhất quán để cho phép sự đồng thuận tối đa giữa các đặc trưng được tăng cường nguyên mẫu và các đặc trưng mạng học sinh.

Trong các thí nghiệm, trước tiên chúng tôi xác minh tính hiệu quả của phương pháp được đề xuất trên các nhiệm vụ chưng cất tri thức đa nhiệm khác nhau. Sau đó, chúng tôi đánh giá phương pháp được đề xuất trên các nhiệm vụ chưng cất tri thức tiêu chuẩn. Kết quả thí nghiệm trong hai kịch bản chứng minh tính hiệu quả và tổng quát của phương pháp của chúng tôi.

Đóng góp của chúng tôi trong bài báo này được tóm tắt như sau:
(1) Chúng tôi đề xuất một phương pháp chưng cất tri thức có hướng dẫn nguyên mẫu để chuyển giao tri thức nội tại từ một mô hình quy mô lớn sang các mô hình nhỏ đa nhiệm khác nhau mà không cần tinh chỉnh mô hình giáo viên trên bộ dữ liệu nhiệm vụ hạ nguồn và cải thiện khả năng tổng quát của mô hình học sinh.

(2) Chúng tôi đề xuất một mô-đun học nguyên mẫu và một mô-đun tăng cường đặc trưng để học tri thức nội tại bất biến của mô hình giáo viên quy mô lớn và nâng cao mô hình học sinh nhỏ với cơ chế chú ý, tương ứng.

(3) Chúng tôi xác minh phương pháp của chúng tôi trên cả chưng cất tri thức đa nhiệm và chưng cất tri thức tiêu chuẩn trên các nhiệm vụ thị giác khác nhau. Kết quả thí nghiệm chứng minh tính hiệu quả và tổng quát của phương pháp của chúng tôi.

II. CÔNG TRÌNH LIÊN QUAN

A. Mô hình Quy mô lớn

Các mô hình quy mô lớn dựa trên Transformer đã đạt được thành công lớn trong xử lý ngôn ngữ tự nhiên [35], thị giác máy tính [2]–[4], và học nhiệm vụ đa phương thức [9], [10].

Trong bài báo này, chúng tôi chủ yếu thảo luận về những nỗ lực gần đây của mô hình dựa trên Transformer trong lĩnh vực thị giác máy tính. Như một công trình tiên phong, ViT [2] xây dựng nhúng token cho Transformer bằng cách trực tiếp chia mỗi hình ảnh thành các miếng 16×16 và chiếu chúng thành các nhúng. Các thí nghiệm được thực hiện trên các bộ dữ liệu huấn luyện quy mô lớn (ví dụ ImageNet-21k và JFT-300M). Deit [3] giới thiệu một số chiến lược huấn luyện để tăng tốc quá trình huấn luyện của ViT. Deit [3] giới thiệu một phương pháp chưng cất để chuyển giao các đặc trưng dựa trên CNN sang một Transformer thị giác. Tuy nhiên, đây không phải là mô hình chưng cất tri thức từ mạng sang các mạng học sinh nhỏ hơn, có nghĩa là các tham số của một mô hình sau chưng cất không được giảm đáng kể. Swin Transformer [4] giới thiệu phân vùng cửa sổ không chồng lấp dịch chuyển và hạn chế tính toán tự chú ý trong các cửa sổ con. Ngoài mô hình quy mô lớn dựa trên Transformer, các loạt mô hình quy mô lớn dựa trên CNN [36] cũng đã được đề xuất. Radosavovic et al. trình bày một mô hình thiết kế mạng mới kết hợp các ưu điểm của thiết kế thủ công và tìm kiếm kiến trúc mạng nơ-ron (NAS). Những mô hình quy mô lớn cồng kềnh này đòi hỏi sức mạnh tính toán nặng và không thể được áp dụng trên các thiết bị với tài nguyên hạn chế.

--- TRANG 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 3

Một Mô hình Giáo viên Quy mô lớn
Một Mô hình Học sinh Hạ nguồn

Bộ phân loại
Mô-đun Tăng cường

Nhúng Ẩn

Chưng cất Lớp-nhúng

Nối
Hàm mất mát Entropy chéo

Hàm mất mát Entropy chéo

Hàm mất mát Nhất quán

Bộ phân loại
Bộ phân loại

Mô-đun Nguyên mẫu

Hình 2. Minh họa khung chưng cất tri thức đa nhiệm có hướng dẫn nguyên mẫu được đề xuất của chúng tôi. ProC-KD bao gồm một mô-đun chưng cất lớp nhúng, một mô-đun học nguyên mẫu, và một mô-đun tăng cường đặc trưng. Mũi tên màu xanh trong khung biểu diễn việc học biểu diễn tổng quát dựa trên nguyên mẫu. Các mũi tên màu xanh lá cây và màu đỏ chỉ ra rằng các nguyên mẫu được sử dụng để nâng cao các đặc trưng được trích xuất từ mô hình giáo viên và mô hình học sinh, tương ứng. Mô-đun tăng cường và mô hình học sinh chia sẻ cùng một bộ phân loại.

B. Chưng cất Tri thức

Chưng cất tri thức là một công nghệ nén mô hình chuyển giao tri thức từ một mạng nơ-ron sâu lớn hơn vào một mạng nhỏ.

Các phương pháp chưng cất tri thức chủ yếu được chia thành chưng cất tri thức dựa trên phản hồi [17], [24], chưng cất tri thức dựa trên đặc trưng [22], [23], và chưng cất tri thức dựa trên quan hệ [25], [26].

Ý tưởng chính của chưng cất tri thức dựa trên phản hồi là trực tiếp chuyển giao phản hồi nơ-ron lớp đầu ra cuối cùng của mô hình giáo viên. Hinton et al. [17] và Ba et al. [37] đề xuất chuyển giao tri thức bằng cách học phân phối xác suất thông qua các nhãn được làm mềm. Tuy nhiên, phương pháp này phụ thuộc vào phân phối xác suất lớp. Phương pháp hiệu quả cho điều này là chưng cất tri thức dựa trên đặc trưng hoặc dựa trên quan hệ từ mô hình giáo viên. Mục tiêu của chưng cất tri thức dựa trên đặc trưng là khớp biểu diễn trung gian của mô hình học sinh với mô hình giáo viên. Fitnets [22] ban đầu giới thiệu việc học biểu diễn trung gian, trong đó các gợi ý được định nghĩa là các đầu ra của lớp ẩn của giáo viên để cải thiện quá trình học của học sinh. Lấy cảm hứng từ [22], nhiều phương pháp chưng cất tri thức dựa trên đặc trưng [38]–[40] được đề xuất.

Các phương pháp chưng cất tri thức dựa trên quan hệ khám phá các mối quan hệ giữa các lớp khác nhau [25] hoặc các mẫu dữ liệu [26].

KD cũng đã được nghiên cứu rộng rãi trong các mô hình ngôn ngữ dựa trên Transformer [31]–[33]. Trong khi các phương pháp chưng cất tri thức trước đây cho các mô hình dựa trên Transformer chủ yếu tập trung vào lĩnh vực NLP và nhiệm vụ của mô hình giáo viên giống với mô hình học sinh. Ye et al. [34] xử lý một kịch bản chưng cất tri thức từ một giáo viên đa nhiệm. Tuy nhiên, nó tụt hậu trong biểu diễn của đối tượng nội tại bất biến và là một phương pháp chưng cất hai giai đoạn.

Khác với các công trình hiện có, trong bài báo này, chúng tôi khám phá kịch bản học các đặc trưng cục bộ nội tại và tái sử dụng tri thức của các mô hình quy mô lớn cho các nhiệm vụ hạ nguồn khác nhau theo cách đa nhiệm.

III. PHƯƠNG PHÁP CHÍNH

Phương pháp của chúng tôi nhắm mục tiêu chưng cất tri thức trong mô hình quy mô lớn sang các mô hình nhỏ hạ nguồn khác nhau. Không gian nhãn của mô hình học sinh khác với mô hình giáo viên, điều này được gọi là chưng cất tri thức đa nhiệm. Các phương pháp chưng cất tri thức cùng nhiệm vụ hiện tại chủ yếu bắt chước trực tiếp dự đoán cuối cùng hoặc các lớp trung gian của mô hình giáo viên, chuyển giao các đặc trưng toàn cầu và đặc thù cho nhiệm vụ. Các biểu diễn nội tại cục bộ có thể mang lại lợi ích lớn cho mô hình học sinh đa nhiệm trong việc hiểu bộ dữ liệu mới của nhiệm vụ hạ nguồn. Để cải thiện khả năng tổng quát của mô hình hạ nguồn, chúng tôi đề xuất một phương pháp chưng cất tri thức đa nhiệm có hướng dẫn nguyên mẫu để chuyển giao tri thức nội tại bất biến từ giáo viên quy mô lớn sang mô hình học sinh nhỏ như được hiển thị trong Hình 2. Mô-đun chính của phương pháp chúng tôi bao gồm một mô-đun học nguyên mẫu và một mô-đun tăng cường đặc trưng.

A. Học Biểu diễn dựa trên Nguyên mẫu

So với các phương pháp trước đây bắt chước trực tiếp các logit dự đoán cuối cùng hoặc các lớp trung gian của mô hình giáo viên, phương pháp của chúng tôi thiết kế một mô-đun để học biểu diễn nội tại. Các nghiên cứu gần đây [41], [42] đã chứng minh rằng việc xây dựng học nguyên mẫu trong các mô hình có thể giúp giải quyết các vấn đề bộ dữ liệu mới. Thông tin đặc thù cho từng danh mục có thể được nắm bắt bởi việc học nguyên mẫu. Lấy cảm hứng từ ý tưởng này, chúng tôi

--- TRANG 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 4

Hình 3. Minh họa sơ đồ của (a) mô-đun học nguyên mẫu và (b) mô-đun tăng cường đặc trưng. Conv và FC Layer tương ứng chỉ ra lớp tích chập và lớp kết nối đầy đủ. ⊕, ⊙, ⊗, ⊞, và [,] tương ứng biểu thị phép toán dư, phép nhân theo từng phần tử, phép nhân ma trận, phép cộng theo từng phần tử, và phép toán nối.

đề xuất một mô-đun có hướng dẫn nguyên mẫu cho các kiến trúc chưng cất giáo viên-học sinh để học các biểu diễn tổng quát với sự hướng dẫn của các nguyên mẫu.

Kiến trúc của mô-đun học nguyên mẫu được hiển thị trong Hình 3 (a). Quá trình tiến được chia làm ba giai đoạn con, đó là Căn chỉnh, Chú ý, và Tập hợp.

Cụ thể, trước khi đưa vào quá trình tiến đặc trưng lớp ẩn hai chiều (2D) được trích xuất bởi mô hình dựa trên Transformer, chúng tôi định hình lại nó thành F ∈ R^{D×H×W}, trong đó D, H, và W tương ứng biểu diễn chiều đặc trưng, chiều cao, và chiều rộng.

Các nguyên mẫu được định nghĩa là P (p_i ∈ R^D; i = 1, 2, ..., n).

Trong giai đoạn con Căn chỉnh, cả tensor ma trận nguyên mẫu được định nghĩa và tensor ma trận đặc trưng đầu vào đều được mở rộng thành n×D×(W×H), và chúng tôi căn chỉnh chúng với phép toán dư (F⊕P). Trong giai đoạn con Chú ý, chúng tôi tính toán các bộ mô tả đặc trưng V dựa trên các bản đồ chú ý, có thể được biểu diễn như sau:

V_i = ∑_{j=1}^{W×H} (e^{L_{ji}}/∑_{i=1}^n e^{L_{ji}}) (F_j ⊙ p_i). (1)

Trong giai đoạn con Tập hợp, trước tiên chúng tôi nối các bộ mô tả đặc trưng V và các đặc trưng đầu vào, sau đó biến đổi kết quả với một khối biến đổi phi tuyến f. Nó có thể được công thức hóa như sau:

O_{pro} = f(concat[F; V^r W_p + b_p]); (2)

trong đó V^r là các bộ mô tả đặc trưng V được định hình lại, W_p và b_p tương ứng là trọng số và bias của lớp kết nối đầy đủ, và concat[·;·] chỉ ra phép toán nối. Hình dạng của đặc trưng đầu ra O_{pro} giống với đặc trưng đầu vào F.

Thông qua các quá trình trên. Biểu diễn tổng quát của các nguyên mẫu có thể được học từ đặc trưng đầu vào của mô hình giáo viên quy mô lớn. Dựa trên những nguyên mẫu tổng quát như vậy, chúng tôi tìm cách nâng cao các đặc trưng học sinh với những nguyên mẫu này.

B. Tăng cường Đặc trưng với Nguyên mẫu

Để cải thiện khả năng tổng quát của các mô hình học sinh của các nhiệm vụ hạ nguồn khác nhau. Các nguyên mẫu tổng quát đã học được sử dụng để nâng cao đặc trưng trong mô-đun tăng cường đặc trưng, như được hiển thị ở phần bên phải của Hình 2. Ý tưởng chính là làm phong phú đặc trưng có liên quan nhiều hơn đến biểu diễn nguyên mẫu trong khi triệt tiêu đặc trưng không liên quan.

Hình 3 (b) hiển thị kiến trúc của mô-đun tăng cường đặc trưng. Quá trình tiến đầu tiên là chú ý đến đặc trưng liên quan đến biểu diễn nguyên mẫu, sau đó nâng cao đặc trưng đầu vào với đặc trưng liên quan đến nguyên mẫu. Toàn bộ quá trình tăng cường đặc trưng cũng có thể được chia thành hai giai đoạn con, đó là Chú ý và Tăng cường.

Cụ thể, đối với các nguyên mẫu đã học P_l ∈ R^{n×D} và các đặc trưng ẩn F_h ∈ R^{t×D}. Trong giai đoạn con Chú ý, trước tiên chúng tôi mã hóa các nguyên mẫu và đặc trưng đầu vào tương ứng. Bản đồ chú ý A được thu được bằng cách tính toán softmax của tích chéo giữa các nguyên mẫu đã học P_l và đặc trưng được mã hóa F_e, có thể được biểu diễn như sau:

A = softmax(F_e P_l^T); (3)

và sau đó đặc trưng chú ý được thu được bằng cách tính toán tích chéo giữa bản đồ chú ý và các nguyên mẫu.

Trong giai đoạn con Tăng cường, chúng tôi nối đặc trưng chú ý với các đặc trưng đầu vào được mã hóa. Lớp kết nối đầy đủ được áp dụng để biến đổi kết quả. Đặc trưng lớp ẩn đầu vào gốc được nâng cao với đặc trưng liên quan đến nguyên mẫu thông qua phép toán tổng theo từng phần tử. Toàn bộ quá trình tăng cường đặc trưng có thể được viết như:

O_{aug} = ReLU(ψ(concat[F_e; AP_l]) + F^r); (4)

trong đó O_{aug} là đầu ra của mô-đun tăng cường đặc trưng, ψ chỉ ra chức năng của lớp kết nối đầy đủ, F^r là định hình lại của F_h, W_h là ma trận trọng số biến đổi tuyến tính,

--- TRANG 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 5

và concat[·;·] chỉ ra phép toán nối. Cuối cùng, đặc trưng được nâng cao được đưa vào bộ phân loại được chia sẻ với mô hình học sinh để dự đoán các danh mục.

C. Chưng cất Tri thức Đa nhiệm

Trong bài báo này, việc chưng cất tri thức của các mô hình học sinh và giáo viên trong các không gian nhãn khác nhau được định nghĩa là chưng cất tri thức đa nhiệm. chúng tôi đề xuất mô-đun học nguyên mẫu và mô-đun tăng cường đặc trưng để hướng dẫn việc huấn luyện mô hình học sinh trong các kịch bản chưng cất đa nhiệm và cải thiện khả năng tổng quát của nó. Ở trên chúng tôi đã mô tả mô-đun nguyên mẫu được thiết kế và mô-đun nâng cao đặc trưng tương ứng. Ở đây, chúng tôi cũng thiết kế một số hàm mất mát để ràng buộc việc huấn luyện chưng cất tri thức đa nhiệm. Hàm mất mát huấn luyện chưng cất bao gồm hàm mất mát chưng cất tri thức lớp-nhúng và hàm mất mát học nguyên mẫu.

Đối với chưng cất tri thức lớp-nhúng, theo [33], chúng tôi chưng cất cả tri thức của các bản đồ chú ý và các đặc trưng trạng thái ẩn từ mô hình giáo viên dựa trên Transformer quy mô lớn. Giả sử rằng chúng tôi đang chưng cất tri thức từ một mô hình giáo viên dựa trên Transformer m lớp sang mô hình học sinh dựa trên Transformer n lớp. Chúng tôi cần chọn n trong số m lớp từ mô hình giáo viên. Cụ thể, đối với bản đồ chú ý, học sinh học để khớp các bản đồ chú ý đa đầu được chọn trong mạng giáo viên, và hàm mất mát cho chưng cất dựa trên chú ý có thể được định nghĩa như sau:

L_{emb} = ∑_{j=1}^n ∑_{i=1}^h MSE(A_i^S, A_i^T) + ∑_{j=1}^n MSE(F_i^S W_h, F_i^T); (5)

trong đó T chỉ ra mô hình giáo viên, S đề cập đến mô hình học sinh, h là số đầu chú ý, A_i ∈ R^{l×l} có nghĩa là ma trận chú ý tương ứng với đầu thứ i của giáo viên hoặc học sinh, l là độ dài token đầu vào, F^S ∈ R^{l×d'} là đặc trưng ẩn học sinh và F^T ∈ R^{l×d} là đặc trưng ẩn của mô hình giáo viên. d và d' biểu thị kích thước nhúng ẩn của mô hình giáo viên và mô hình học sinh, tương ứng. W_h ∈ R^{d'×d} là một ma trận trọng số biến đổi có thể học, biến đổi các đặc trưng lớp ẩn của mô hình học sinh thành cùng chiều với các đặc trưng của mô hình giáo viên, và MSE(·) chỉ ra hàm mất mát lỗi bình phương trung bình.

Đối với việc học nguyên mẫu, chúng tôi định nghĩa hàm mất mát nhất quán L_{con} và hàm mất mát phân loại L_{pro}^{cls}. Mất mát nhất quán được thu được bằng cách tính toán mất mát phân kỳ Kullback-Leibler (KL) giữa các logit dự đoán y_{con} từ mô-đun tăng cường nguyên mẫu và các logit dự đoán y_{stu} từ mô hình học sinh, L_{con} = H(y_{con}; y_{stu})). Mất mát phân loại là mất mát entropy chéo softmax giữa dự đoán nguyên mẫu y_{con} và nhãn thực tế y. Do đó hàm mất mát của việc học nguyên mẫu có thể được biểu diễn như:

L_{pro} = L_{con} + L_{pro}^{cls}; (6)

Ngoài hàm mất mát chưng cất đặc trưng lớp nhúng và hàm mất mát học nguyên mẫu, chúng tôi định nghĩa hàm mất mát mô hình học sinh là L_{stu}. Hàm mất mát huấn luyện kết hợp cho chưng cất tri thức đa nhiệm của chúng tôi có thể được biểu diễn như:

L_{total} = λ_{emb}L_{emb} + λ_{pro}L_{pro} + λ_{stu}L_{stu}; (7)

trong đó λ_{emb}, λ_{pro}, và λ_{stu} là các trọng số của mất mát chưng cất đặc trưng lớp nhúng, mất mát học nguyên mẫu, và mất mát mô hình học sinh, tương ứng.

IV. THÍ NGHIỆM

Để đánh giá tính hiệu quả tổng quát của phương pháp chưng cất tri thức từ các mô hình quy mô lớn trong các kịch bản đa nhiệm, chúng tôi đã tiến hành thí nghiệm trên chưng cất tri thức đa nhiệm và thiết lập chưng cất tri thức tiêu chuẩn cùng nhiệm vụ. Thí nghiệm được thực hiện trên các nhiệm vụ thị giác khác nhau, ví dụ như phân loại hình ảnh và phát hiện đối tượng cho mỗi thiết lập. Trong bài báo này, sơ đồ chưng cất tri thức được thiết lập như chưng cất ngoại tuyến, có nghĩa là các trọng số của mạng giáo viên bị đóng băng trong quá trình huấn luyện.

A. Chưng cất Tri thức Đa nhiệm

1) Phân loại Hình ảnh: Chúng tôi thực hiện thí nghiệm trên mô hình dựa trên Transformer trong ba nhiệm vụ hạ nguồn, bao gồm phân loại hình ảnh tiêu chuẩn, phân loại hình ảnh đuôi dài, và phân loại hình ảnh đa miền. Ở đây, tất cả các mô hình giáo viên đều được huấn luyện trên bộ dữ liệu ImageNet-1K [13] trong các thí nghiệm của chúng tôi.

Bộ dữ liệu. CIFAR-100 [44] bao gồm 50.000 hình ảnh huấn luyện và 10.000 hình ảnh xác thực. Nó chứa 100 danh mục và mỗi lớp chứa 600 hình ảnh. Kích thước của mỗi hình ảnh là 32×32. Theo [45], [46], CIFAR-100 đuôi dài được tạo ra bằng cách giảm số lượng mẫu huấn luyện cho mỗi lớp, nhưng với tập xác thực không thay đổi. Chúng tôi định nghĩa một tỷ lệ mất cân bằng ρ, tức là ρ = N_{max}/N_{min}. ρ biểu thị tỷ lệ kích thước mẫu giữa lớp thường xuyên nhất và lớp ít thường xuyên nhất. Theo cách này, kích thước mẫu giảm theo cấp số nhân giữa các lớp. Trong thí nghiệm của chúng tôi, chúng tôi đặt tỷ lệ mất cân bằng là 10. iNaturalist 2018 [47] chứa hơn 450.000 hình ảnh huấn luyện từ 8.142 loài chim, động vật có vú, bò sát, và thực vật khác nhau. So với ImageNet và các bộ dữ liệu phân loại hình ảnh khác, iNaturalist thể hiện phân phối đuôi dài, và nhiều loài có tương đối ít hình ảnh. Chúng tôi sử dụng phân chia huấn luyện và xác thực chính thức trong thí nghiệm, với 437.513 hình ảnh để huấn luyện và 24.424 hình ảnh để xác thực. Bộ dữ liệu Office-Home [48] đã được tạo ra để đánh giá các phương pháp thích ứng miền cho phân loại hình ảnh. Nó bao gồm 15.500 hình ảnh từ bốn miền khác nhau: hình ảnh Nghệ thuật (Ar), Clip Art (Cl), hình ảnh Sản phẩm (Pr), và hình ảnh Thế giới thực (Rw). Mỗi miền trong bộ dữ liệu này chứa 65 danh mục, và các hình ảnh là từ các cảnh văn phòng hoặc nhà ở. Trong các thí nghiệm của chúng tôi, hình ảnh Thế giới thực (Rw) được sử dụng làm tập huấn luyện, và các miền khác được sử dụng làm tập thử nghiệm.

Chi tiết Triển khai. Chúng tôi tiến hành thí nghiệm của mình trên các mô hình ViT [2] và Swin-Transformer [4] nổi tiếng. Đối với ViT, mô hình giáo viên là mô hình ViT-B 12 lớp và mô hình học sinh là mô hình ViT nhỏ 6 lớp. Các

--- TRANG 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 6

BẢNG I
ĐỘ CHÍNH XÁC TRUNG BÌNH (%) CỦA BA CHƯNG CẤT TRI THỨC PHÂN LOẠI HÌNH ẢNH ĐA NHIỆM. NHIỆM VỤ PHÂN LOẠI HÌNH ẢNH ĐA MIỀN ĐƯỢC THỰC HIỆN TRÊN BỘ DỮ LIỆU OFFICE-HOME. LT-CIFAR CHỈ RA CIFAR 100 ĐUÔI DÀI. FBKD-PROC-KD LÀ PHƯƠNG PHÁP CỦA CHÚNG TÔI BẰNG CÁCH CẮM PROC-KD ĐỀ XUẤT VÀO PHƯƠNG PHÁP FBKD. CÁC MÔ HÌNH GIÁO VIÊN ĐỀU ĐƯỢC HUẤN LUYỆN TRÊN IMAGE NET-1K [13] VÀ CỐ ĐỊNH TRỌNG SỐ TRONG QUÁ TRÌNH HUẤN LUYỆN CHƯNG CẤT.

Giáo viên (param.) Phương pháp param. Tiêu chuẩn Đuôi dài Đa miền
CIFAR-100 LT-CIFAR iNaturalist 2018 Rw→Ar Rw→Cl Rw→Pr
ViT-B [2] (86M) Mô hình Học sinh 43M 78.84 55.83 - 20.85 16.91 34.85
RKD [26] 43M 87.13 76.52 - 57.31 40.64 73.50
FBKD [33] 43M 86.16 72.69 57.14 60.28 40.02 75.51
FBKD-ProC-KD (Của chúng tôi) 43M 87.46 78.32 58.67 61.41 40.96 76.83
Swin-L [36] (197M) Mô hình Học sinh 110M 78.90 41.48 - 26.87 20.51 43.32
RKD [26] 110M 83.99 58.94 - 22.95 15.50 30.71
FBKD [33] 110M 83.63 57.83 70.19 41.29 29.03 63.40
FBKD-ProC-KD (Của chúng tôi) 110M 84.21 68.23 72.11 42.16 30.24 64.27

Hình 4. So sánh các bản đồ chú ý trong FBKD và FBKD-ProC-KD (của chúng tôi) bằng cách sử dụng phương pháp Giải thích Transformer [43]. Ở đây, lớp thứ hai và lớp cuối cùng của ViT được chọn làm lớp nông và lớp sâu, tương ứng.

chỉ số của các lớp ẩn được chọn để chưng cất trong mô hình giáo viên là [2, 4, 6, 8, 10, 12]. Cả tri thức bản đồ chú ý và tri thức đặc trưng lớp ẩn đều được chưng cất. Tất cả các hệ số của hàm mất mát Phương trình 7 đều là 1 ngoại trừ λ_{emb} là 0.3. Trong giai đoạn huấn luyện, các đặc trưng lớp ẩn thứ 4, 8, và 12 được chọn để nối và sau đó đưa vào mô-đun học nguyên mẫu và mô-đun tăng cường để huấn luyện các nguyên mẫu. Số lượng nguyên mẫu được đặt là 72. Bộ tối ưu AdamW được sử dụng với tốc độ học 5e-4 và độ giảm trọng số là 0.05. Kích thước đầu vào của hình ảnh là 224×224 và kích thước batch được đặt là 32 cho mỗi GPU.

Đối với Swin-Transformer, mô hình giáo viên là Swin-L 24 lớp, chúng tôi chưng cất tri thức từ 18 lớp giữa sang 6 lớp, và mô hình học sinh là Swin-Transformer 12 lớp. Hai đặc trưng lớp ẩn cuối cùng của backbone được chọn cho mô-đun học nguyên mẫu và mô-đun tăng cường sau phép toán nối. Chúng tôi sử dụng bộ tối ưu AdamW với tốc độ học ban đầu 5e-4 và độ giảm trọng số 0.05. Kích thước batch huấn luyện được đặt là 64 cho mỗi GPU.

Tất cả các thí nghiệm được chạy trên 8 GPU Nvidia Tesla V100 (32GB VRAM, kết nối PCIe). Chúng tôi sử dụng NCCL cho huấn luyện song song đa nút. Tích lũy gradient cũng được áp dụng để giảm chi phí giao tiếp đa GPU.

Kết quả và Phân tích. Bảng I hiển thị kết quả thí nghiệm trên các nhiệm vụ phân loại hình ảnh tiêu chuẩn, đuôi dài, và đa miền. Chúng tôi so sánh nó với Chưng cất Tri thức Quan hệ (RKD) [26] bằng cách triển khai lại nó trong thiết lập thí nghiệm của chúng tôi. Theo Tinybert [33], FBKD là một phương pháp chưng cất tri thức dựa trên đặc trưng cho Transformer-

--- TRANG 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 7

BẢNG II
KẾT QUẢ (%) CỦA CHƯNG CẤT TRI THỨC PHÁT HIỆN ĐỐI TƯỢNG ĐA NHIỆM TRÊN CITYSCAPES VÀ FOGGY CITYSCAPES. CÁC MÔ HÌNH GIÁO VIÊN ĐỀU ĐƯỢC HUẤN LUYỆN TRÊN BỘ DỮ LIỆU COCO [49] VÀ CỐ ĐỊNH TRỌNG SỐ TRONG QUÁ TRÌNH HUẤN LUYỆN CHƯNG CẤT.

Cityscapes
Phương pháp người đi xe đạp người lái xe ô tô xe tải xe buýt tàu xe máy xe đạp mAP
Mô hình Học sinh 53.1 55.1 70.1 31.3 56.1 31.6 40.2 44.6 47.8
CWD [50] 63.2 65.2 77.7 48.6 72.8 49.8 54.2 58.1 61.2
ProC-KD (Của chúng tôi) 63.9 65.1 77.8 51.9 74.3 51.7 52.5 59.9 62.1

FoggyCityscapes
Mô hình Học sinh 40.8 40.3 63.0 27.8 42.3 11.6 27.3 31.8 35.6
FBKD [33] 52.0 54.1 68.6 37.5 53.9 35.7 40.6 49.0 48.9
CWD [50] 53.3 55.8 71.9 37.5 57.1 46.8 43.2 50.9 52.1
FBOD [51] 52.1 51.2 69.3 36.0 53.7 42.6 41.4 45.4 48.9
FBKD-ProC-KD (Của chúng tôi) 51.8 55.0 68.8 38.7 53.4 47.1 38.8 45.2 49.9
ProC-KD (Của chúng tôi) 53.8 57.9 73.1 40.3 57.7 51.2 44.2 51.5 53.7

dựa trên mô hình chưng cất tri thức từ các lớp nhúng và bản đồ chú ý sang mô hình học sinh. FBKD-ProC-KD là phương pháp của chúng tôi bằng cách cắm ProC-KD đề xuất vào phương pháp FBKD.

Phân loại Hình ảnh Tiêu chuẩn. So với phương pháp cơ sở, ProC-KD của chúng tôi cải thiện 1.3% và 0.6% trên ViT và Swin-Transformer, tương ứng. Điều này chứng minh rằng phương pháp ProC-KD của chúng tôi có thể thúc đẩy các nguyên mẫu học biểu diễn tổng quát và cải thiện hiệu suất của mô hình học sinh trong các cảnh phân loại hình ảnh đa nhiệm.

Phân loại Hình ảnh Đuôi dài. Đối với bộ dữ liệu CIFAR-100 đuôi dài, ProC-KD của chúng tôi cải thiện hiệu suất 5.6% và 10.4% trên ViT và Swin-Transformer, tương ứng. Đối với bộ dữ liệu iNaturalist 2018, ProC-KD của chúng tôi cải thiện hiệu suất 1.5% và 1.9% trên ViT và Swin-Transformer tương ứng. Điều này chứng minh rằng ProC-KD có thể cải thiện khả năng tổng quát của mô hình học sinh trong các nhiệm vụ phân loại hình ảnh đuôi dài.

Phân loại Hình ảnh Đa miền. Thí nghiệm phân loại hình ảnh đa miền được thực hiện trên bộ dữ liệu Office-Home. Như có thể thấy, so với cơ sở FBKD, ProC-KD cải thiện hiệu suất 1.3% trên chuyển đổi miền Rw→Pr và 1.2% trên chuyển đổi miền khó nhất Rw→Cl. Điều này chứng minh rằng việc chưng cất tri thức từ mô hình được huấn luyện trên các bộ dữ liệu quy mô lớn có thể cải thiện hiệu suất của mô hình học sinh trong cảnh đa miền, và ProC-KD của chúng tôi có thể cải thiện thêm khả năng tổng quát.

Hình 4 hiển thị kết quả trực quan hóa của các bản đồ chú ý trong FBKD và FBKD-ProC-KD của chúng tôi. Như có thể thấy, so với cơ sở FBKD, bản đồ chú ý của phương pháp FBKD-ProC-KD của chúng tôi tập trung nhiều hơn vào các đối tượng ở cả lớp nông và lớp sâu. Nó chỉ ra rằng phương pháp ProC-KD của chúng tôi có thể thúc đẩy việc huấn luyện mô hình học sinh ở cả các lớp nông và các lớp sâu.

2) Phát hiện Đối tượng: Chúng tôi cũng tiến hành thí nghiệm trên phát hiện đối tượng tiêu chuẩn và phát hiện đối tượng đa miền. Đối với nhiệm vụ phát hiện đối tượng đa miền, chúng tôi chỉ lấy miền nguồn làm tập huấn luyện và miền đích làm tập thử nghiệm. Ở đây, tất cả các mô hình giáo viên đều được huấn luyện trên bộ dữ liệu COCO [49] và đóng băng trọng số trong quá trình huấn luyện chưng cất.

Bộ dữ liệu. Cityscapes [52] là một bộ dữ liệu đường phố đô thị của 8 danh mục đối tượng. Nó chứa 2975 và 500 hình ảnh trong tập huấn luyện và xác thực. FoggyCityscapes [53] là một bộ dữ liệu thu được bằng cách tổng hợp các mức độ sương mù khác nhau trên Cityscapes [52]. Nó chứa 2975 hình ảnh huấn luyện và 500 hình ảnh xác thực, tương ứng. Daytime-sunny, Dusk-rainy, và Night-rainy [53] là ba bộ dữ liệu cảnh đường phố dưới các môi trường thời tiết khác nhau được thu thập từ bộ dữ liệu BDD-100k. Trong các thí nghiệm của chúng tôi, chúng tôi chọn 27.708 hình ảnh từ Daytime-sunny làm tập huấn luyện, và 2.494 và 3.501 hình ảnh từ Night-rainy và Dusk-rainy làm tập thử nghiệm cho hai kịch bản, tương ứng.

Chi tiết Triển khai. Đối với thí nghiệm chưng cất tri thức đa nhiệm của phát hiện đối tượng tiêu chuẩn và phát hiện đối tượng đa miền, mô hình giáo viên là Cascade Mask-RCNN với backbone của mô hình Swin-Base [4] 24 lớp được huấn luyện trên bộ dữ liệu COCO [49] và mô hình học sinh là Cascade Mask-RCNN với backbone của mô hình Swin-Tiny 12 lớp. Ngoài việc học chưng cất tri thức lớp ẩn trong mô hình giáo viên, đặc trưng lớp thứ tư của FPN được chọn làm đầu vào cho mô-đun học nguyên mẫu và mô-đun tăng cường để học biểu diễn tổng quát. Các hệ số của λ_{pro} và λ_{emb} đều được đặt là 1. Chúng tôi chạy bộ tối ưu SGD với tốc độ học ban đầu 0.01 và độ giảm tham số 0.0001 trong 36 epoch. Kích thước batch được đặt là 2 cho mỗi GPU.

Để so sánh với các phương pháp chưng cất tri thức phát hiện đối tượng tiên tiến. Chúng tôi cũng triển khai lại một số phương pháp chưng cất tri thức phát hiện đối tượng dựa trên đặc trưng đại diện dựa trên thiết lập thí nghiệm của chúng tôi, ví dụ như CWD [50] và FBOD [51].

Kết quả và Phân tích Phát hiện Đối tượng Tiêu chuẩn. Bảng II hiển thị kết quả phát hiện trên Cityscapes và FoggyCityscapes. Ở đây, chúng tôi triển khai lại phương pháp CWD [50] và FBOD [51] trong thiết lập thí nghiệm của chúng tôi. FBKD là một phương pháp chưng cất tri thức theo Tinybert [33], và chúng tôi chỉ chưng cất các đặc trưng ẩn của backbone. Chúng ta có thể thấy rằng phương pháp của chúng tôi tăng cường hiệu suất dưới cảnh chưng cất tri thức đa nhiệm một cách đáng kể. Đối với Cityscapes, so với cơ sở CWD [50], phương pháp của chúng tôi cải thiện hiệu suất 0.9% tương ứng. Đối với FoggyCityscapes, so với FBKD và CWD [50], phương pháp của chúng tôi cải thiện hiệu suất 1.0% và 1.4% tương ứng. Điều này chứng minh rằng biểu diễn nguyên mẫu tổng quát hữu ích cho việc học của mô hình học sinh về phát hiện đối tượng.

Hình 5 và Hình 6 hiển thị trực quan hóa của phát hiện

--- TRANG 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 8

Hình 5. Kết quả định tính trên Cityscapes. So với cơ sở CWD, phương pháp ProC-KD của chúng tôi có thể định vị và nhận dạng các đối tượng một cách chính xác, ví dụ như xe buýt, xe đạp, xe tải, người.

Hình 6. Kết quả định tính trên FoggyCityscapes. So với cơ sở CWD [50], phương pháp ProC-KD của chúng tôi có thể định vị và nhận dạng các đối tượng một cách chính xác trong hình ảnh có sương mù, ví dụ như người lái xe, xe đạp, xe buýt, ô tô.

BẢNG III
KẾT QUẢ (%) CỦA CHƯNG CẤT TRI THỨC PHÁT HIỆN ĐỐI TƯỢNG ĐA NHIỆM TRÊN ĐA MIỀN CỦA CITYSCAPES→FOGGY CITYSCAPES. Ở ĐÂY, CHÚNG TÔI HUẤN LUYỆN MÔ HÌNH TRÊN DỮ LIỆU HUẤN LUYỆN CỦA CITYSCAPES VÀ THỬ NGHIỆM MÔ HÌNH TRÊN BỘ DỮ LIỆU THỬ NGHIỆM CỦA FOGGY CITYSCAPES. CÁC MÔ HÌNH GIÁO VIÊN ĐỀU ĐƯỢC HUẤN LUYỆN TRÊN BỘ DỮ LIỆU COCO [49] VÀ CỐ ĐỊNH TRỌNG SỐ TRONG QUÁ TRÌNH HUẤN LUYỆN CHƯNG CẤT.

Phương pháp người đi xe đạp người lái xe ô tô xe tải xe buýt tàu xe máy xe đạp mAP
Mô hình Học sinh 22.3 17.6 18.0 4.5 9.1 0.0 9.1 20.5 12.6
CWD [50] 41.0 48.7 51.3 20.2 33.1 10.6 30.8 44.4 35.0
ProC-KD (Của chúng tôi) 41.3 48.9 51.7 24.6 33.8 13.5 32.6 44.4 36.3

kết quả trên Cityscape và FoggyCityscapes tương ứng. Hàng đầu tiên là sự thật cơ sở của các đối tượng, hàng thứ hai là kết quả phát hiện của phương pháp cơ sở CWD [50], và hàng thứ ba là kết quả phát hiện của ProC-KD. Chúng ta có thể thấy rằng, so với CWD, phương pháp ProC-KD của chúng tôi có thể định vị và nhận dạng các đối tượng một cách chính xác trong cảnh bình thường và hình ảnh có sương mù.

Kết quả và Phân tích Phát hiện Đối tượng Đa miền. Bảng III hiển thị kết quả về phát hiện đối tượng đa miền của Cityscapes→FoggyCityscapes. Như chúng ta có thể thấy, so với cơ sở CWD [50], ProC-KD của chúng tôi cải thiện hiệu suất đáng kể 1.3%. Đối với hầu hết các danh mục đối tượng, phương pháp của chúng tôi vượt trội hơn CWD [50]. Điều này chứng minh rằng phương pháp ProC-KD của chúng tôi có thể cải thiện khả năng tổng quát của mô hình học sinh về phát hiện đối tượng đa miền.

Bảng IV hiển thị kết quả về phát hiện đối tượng đa miền của Daytime-sunny→Night-rainy và Daytime-sunny→Dusk-rainy. Đối với Daytime-sunny→Night-rainy, so với cơ sở CWD [50], phương pháp ProC-KD của chúng tôi cải thiện mAP 0.9%. Đối với Daytime-sunny→Dusk-rainy, phương pháp ProC-KD của chúng tôi cải thiện mAP 0.5%. Lý do hiệu suất của ProC-KD của chúng tôi thấp hơn CWD trên đối tượng

--- TRANG 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 9

BẢNG IV
KẾT QUẢ (%) CỦA CHƯNG CẤT TRI THỨC PHÁT HIỆN ĐỐI TƯỢNG ĐA NHIỆM TRÊN ĐA MIỀN CỦA DAYTIME-SUNNY→NIGHT-RAINY VÀ DAYTIME-SUNNY→DUSK-RAINY. Ở ĐÂY, CHÚNG TÔI HUẤN LUYỆN MÔ HÌNH TRÊN DỮ LIỆU DAYTIME-SUNNY VÀ THỬ NGHIỆM MÔ HÌNH TRÊN DỮ LIỆU NIGHT-RAINY VÀ DUSK-RAINY. CÁC MÔ HÌNH GIÁO VIÊN ĐỀU ĐƯỢC HUẤN LUYỆN TRÊN BỘ DỮ LIỆU COCO [49] VÀ CỐ ĐỊNH TRỌNG SỐ TRONG QUÁ TRÌNH HUẤN LUYỆN CHƯNG CẤT.

Daytime-sunny→Night-rainy
Phương pháp xe đạp xe buýt ô tô xe máy người người lái xe tải mAP
Mô hình Học sinh 24.3 9.1 33.8 1.1 12.3 9.1 16.1 15.1
CWD [50] 38.6 17.1 49.4 9.7 24.4 15.6 34.4 27.0
ProC-KD (Của chúng tôi) 40.9 18.3 49.4 8.6 26.1 18.2 35.7 27.9

Daytime-sunny→Dusk-rainy
Mô hình Học sinh 40.6 14.9 66.0 11.5 25.8 15.2 39.7 30.5
CWD [50] 49.9 34.8 73.9 24.0 43.9 32.0 54.7 44.7
ProC-KD (Của chúng tôi) 52.6 36.6 73.3 21.6 46.5 31.6 54.6 45.2

Hình 7. Kết quả định tính trên Daytime-sunny→Night-rainy. So với cơ sở CWD [50], phương pháp ProC-KD của chúng tôi có thể định vị và nhận dạng các đối tượng một cách chính xác trong hình ảnh có sương mù, ví dụ như người, xe buýt, xe tải, ô tô.

Hình 8. Kết quả định tính trên Daytime-sunny→Dusk-rainy. So với cơ sở CWD, phương pháp ProC-KD của chúng tôi có thể định vị và nhận dạng các đối tượng một cách chính xác trong hình ảnh mưa, ví dụ như người, xe buýt, ô tô, xe tải, xe đạp.

xe máy có thể là do số lượng sự thật cơ sở của xe máy trong tập thử nghiệm khá nhỏ. Nó chỉ chứa 49 sự thật cơ sở của xe máy trong tập thử nghiệm Daytime-sunny→Night-rainy và chỉ 110 sự thật cơ sở trong tập thử nghiệm Daytime-sunny→Dusk-rainy.

Hình 7 và Hình 8 hiển thị trực quan hóa của đa miền

--- TRANG 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 10

Hình 9. tSNE của REFILLED (trái) và Của chúng tôi (phải) trên 10 lớp được lấy mẫu từ CIFAR-10. Giá trị NMI càng lớn có nghĩa là chất lượng nhúng càng tốt.

kết quả phát hiện đối tượng trên Daytime-sunny→Night-rainy và Daytime-sunny→Dusk-rainy tương ứng. Hàng đầu tiên là sự thật cơ sở của các đối tượng, hàng thứ hai là kết quả phát hiện của phương pháp cơ sở CWD [50], và hàng thứ ba là kết quả phát hiện của ProC-KD. So với CWD, ProC-KD của chúng tôi có thể định vị và nhận dạng các đối tượng trong cả hình ảnh mưa đêm và hình ảnh mưa hoàng hôn một cách chính xác.

B. Chưng cất Tri thức Tiêu chuẩn Cùng nhiệm vụ

Chúng tôi coi chưng cất tri thức trong đó mô hình giáo viên và mô hình học sinh chia sẻ cùng một không gian nhãn là chưng cất tri thức tiêu chuẩn. Phương pháp chúng tôi đề xuất là một phương pháp tổng quát, cũng có thể được sử dụng trong thiết lập chưng cất tri thức tiêu chuẩn. Ở đây chúng tôi xác minh tính hiệu quả của phương pháp của chúng tôi trên chưng cất tri thức tiêu chuẩn trong nhiệm vụ phân loại hình ảnh và nhiệm vụ phát hiện đối tượng tương ứng.

1) Phân loại Hình ảnh: Trong phần này, mô hình giáo viên và mô hình học sinh chia sẻ cùng một không gian nhãn hình ảnh. Ở đây, chúng tôi cũng xác minh tính hiệu quả của phương pháp của chúng tôi trên mô hình phân loại có cấu trúc CNN, chúng tôi đặt mô hình giáo viên và mô hình học sinh là Wide-ResNet là một cấu trúc mạng dựa trên CNN. Bằng cách thay đổi độ sâu và độ rộng của mô hình học sinh, chúng ta có thể có được các mô hình học sinh khác nhau để xác minh khả năng thích ứng của phương pháp với các cấu trúc mạng khác nhau. Bộ dữ liệu được sử dụng trong thí nghiệm là CIFAR-100 [44]. Theo REFILLED [34], tất cả các mô hình giáo viên được đặt là Wide-ResNet với độ sâu 40 và độ rộng 2 trong những thí nghiệm này. Độ chính xác của mô hình giáo viên là 74.44%. Khác với tối ưu hóa hai giai đoạn của REFILLED [34], ProC-KD của chúng tôi chỉ thực hiện tối ưu hóa một giai đoạn.

Kết quả và Phân tích. Bảng V hiển thị kết quả so sánh giữa phương pháp của chúng tôi và các phương pháp chưng cất SOTA khác với các mô hình học sinh khác nhau. Giống như REFILLED [34], độ chính xác của phương pháp chúng tôi là kết quả của thử nghiệm trên tập thử nghiệm sau khi huấn luyện hội tụ trên tập huấn luyện. Kết quả của các phương pháp so sánh khác được trích dẫn từ REFILLED [34]. Có thể thấy từ Bảng V rằng so với các phương pháp chưng cất tri thức khác, phương pháp của chúng tôi đạt được độ chính xác tốt nhất trong ba mô hình học sinh với các cấu trúc khác nhau. So với REFILLED [34], ProC-KD của chúng tôi vượt trội 1.3%, 0.48%, và 0.5% trong ba cấu trúc mạng học sinh có (độ sâu, độ rộng)=(40,1), (độ sâu, độ rộng)=(16,2), và (độ sâu, độ rộng)=(16,1). Điều này chứng minh tính hiệu quả của phương pháp của chúng tôi trong cảnh chưng cất tri thức phân loại hình ảnh tiêu chuẩn.

BẢNG V
KẾT QUẢ (%) TRÊN CẢNH CHƯNG CẤT TRI THỨC PHÂN LOẠI HÌNH ẢNH TIÊU CHUẨN. Ở ĐÂY, MÔ HÌNH GIÁO VIÊN VÀ MÔ HÌNH HỌC SINH CHIA SẺ CÙNG MỘT KHÔNG GIAN NHÃN CIFAR-100 [44].

Phương pháp/(độ sâu, độ rộng) (40, 1) (16, 2) (16, 1)
Học sinh 68.97 70.15 65.44
KD [17] 70.46 71.87 66.54
FitNet [22] 68.66 70.89 65.38
AT [38] 69.85 71.06 65.31
NST [23] 68.00 71.19 64.95
VID-I [55] 71.51 73.31 66.32
RKD [26] 72.18 72.56 65.22
REFILLED [34] 72.72 74.01 67.56
ProC-KD (Của chúng tôi) 74.05 74.49 68.06

Hình 9 hiển thị kết quả trực quan hóa của các đặc trưng nhúng 10 lớp được lấy mẫu ngẫu nhiên với tSNE [54]. Thông tin tương hỗ chuẩn hóa (NMI) được sử dụng làm tiêu chí để đo lường chất lượng nhúng, giá trị càng lớn có nghĩa là chất lượng nhúng càng tốt. chúng ta có thể thấy rằng đối với các đặc trưng nhúng của 10 danh mục được lấy mẫu ngẫu nhiên, phương pháp của chúng tôi có tính phân biệt cao hơn và có giá trị NMI cao hơn.

--- TRANG 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 11

Hình 10. Đường cong Precision-Recall phân tích lỗi của tất cả các đối tượng, đối tượng kích thước lớn, đối tượng kích thước trung bình, và đối tượng kích thước nhỏ trên bộ dữ liệu COCO [49]. Hàng trên cùng là từ cơ sở CWD [50] và hàng dưới cùng là từ ProC-KD của chúng tôi. Ở đây, C75 chỉ ra kết quả ở ngưỡng IoU 0.75, C50 chỉ ra kết quả ở ngưỡng 0.50 IoU, Loc chỉ ra kết quả sau khi bỏ qua lỗi định vị, Sim chỉ ra kết quả sau khi bỏ qua các lớp tương tự từ cùng một siêu danh mục dương tính giả, Oth chỉ ra kết quả sau khi bỏ qua tất cả các nhầm lẫn danh mục, BG chỉ ra kết quả sau khi bỏ qua tất cả các dương tính giả, và FN chỉ ra kết quả sau khi bỏ qua tất cả các âm tính giả.

BẢNG VI
KẾT QUẢ (%) TRÊN CẢNH CHƯNG CẤT TRI THỨC PHÁT HIỆN ĐỐI TƯỢNG TIÊU CHUẨN. Ở ĐÂY, MÔ HÌNH GIÁO VIÊN VÀ MÔ HÌNH HỌC SINH CHIA SẺ CÙNG MỘT KHÔNG GIAN NHÃN COCO [49]. ']' CHỈ RA KẾT QUẢ MÀ CHÚNG TÔI TRIỂN KHAI LẠI VỚI MÃ ĐƯỢC PHÁT HÀNH

Phương pháp AP AP50 AP75 APs APm APl
Giáo viên 44.3 62.7 48.4 25.4 48.4 58.1
Học sinh 38.4 59.0 42.0 21.5 42.1 50.3
Chen [56] 38.7 59.0 42.1 22.0 41.9 51.0
Wang [57] 39.1 59.8 42.8 22.2 42.9 51.1
Heo [58] 38.9 60.1 42.6 21.8 42.7 50.7
FBOD [51] 41.5 62.2 45.1 23.5 45.0 55.3
CWD [50] 41.7 62.0 45.5 23.3 45.5 55.5
CWD] [50] 41.6 61.6 45.5 22.4 45.9 55.0
ProC-KD (Của chúng tôi) 42.1 62.7 46.0 23.5 45.8 57.1

2) Phát hiện Đối tượng: Chúng tôi cũng áp dụng phương pháp chưng cất tri thức có hướng dẫn nguyên mẫu cho nhiệm vụ chưng cất tri thức phát hiện đối tượng tiêu chuẩn. Để so sánh công bằng, theo CWD [50] và [51], mô hình giáo viên trong các thí nghiệm được đặt là Cascade Mask RCNN với backbone ResNeXt101, và mô hình học sinh là Faster-RCNN với backbone ResNet-50. Khác với các thí nghiệm chưng cất tri thức đa nhiệm, ở đây, bộ dữ liệu huấn luyện của mô hình giáo viên được huấn luyện trước và quy trình chưng cất tri thức đều được thực hiện trên bộ dữ liệu COCO.

Kết quả và Phân tích. Bảng VI hiển thị kết quả so sánh giữa phương pháp của chúng tôi và các phương pháp chưng cất tiên tiến khác về phát hiện đối tượng. Kết quả của các phương pháp khác được trích dẫn từ CWD [50]. Có thể thấy rằng phương pháp của chúng tôi vượt trội hơn các phương pháp chưng cất tri thức khác ở các IoU khác nhau và các kích thước đối tượng khác nhau. Đặc biệt, chúng tôi đạt được cải thiện 1.6% so với CWD [50] trên APl.

Hình 10 hiển thị đường cong Precision-Recall phân tích lỗi của tất cả các đối tượng, đối tượng kích thước lớn, đối tượng kích thước trung bình, và đối tượng kích thước nhỏ dưới các điều kiện khác nhau trên bộ dữ liệu COCO. Hàng trên cùng là kết quả của phương pháp cơ sở CWD [50] và hàng dưới cùng là kết quả phát hiện của ProC-KD. Chúng ta có thể thấy rằng so với CWD [50], ProC-KD của chúng tôi đạt được hiệu suất tốt hơn trên các ngưỡng IoU khác nhau cho tất cả các đối tượng có kích thước khác nhau. So với CWD, Của chúng tôi cải thiện 0.029 và 0.024 trên các đối tượng lớn và đối tượng nhỏ khi bỏ qua các lỗi định vị, tương ứng. Điều này chỉ ra phương pháp của chúng tôi có thể cung cấp thông tin phân loại chính xác hơn. ProC-KD của chúng tôi vượt trội hơn CWD [50] trung bình 0.014 trên tất cả các đối tượng khu vực sau khi bỏ qua lỗi định vị, bỏ qua các lớp tương tự từ cùng một siêu danh mục, bỏ qua tất cả các nhầm lẫn danh mục, và bỏ qua tất cả các dương tính giả, điều này chứng minh khả năng định vị và nhận dạng tốt hơn của phương pháp chúng tôi.

C. Nghiên cứu Ablation

Trong phần này, chúng tôi nghiên cứu ablation số lượng nguyên mẫu và các yếu tố thiết kế quan trọng trong chưng cất tri thức có hướng dẫn nguyên mẫu được đề xuất.

1) các yếu tố thiết kế: Chúng tôi nghiên cứu ảnh hưởng của các yếu tố thiết kế trên FoggyCityscapes. Kết quả được hiển thị trong Bảng VII, quan sát thấy rằng (a) So với cơ sở CWD [50], có thể đạt được tăng cường mAP 0.3% (52.4%-52.1%) với mô-đun học nguyên mẫu, chỉ ra rằng biểu diễn nguyên mẫu có lợi cho chưng cất tri thức. (b) Sự kết hợp của mô-đun học nguyên mẫu và mô-đun tăng cường đặc trưng dẫn đến cải thiện mAP đáng kể, là 1.3% (53.7%-52.4%). Lý do có thể là mô-đun tăng cường đặc trưng làm phong phú đặc trưng có liên quan nhiều hơn đến đối tượng.

2) số lượng nguyên mẫu: Chúng tôi thực hiện thí nghiệm ablation về số lượng nguyên mẫu trên bộ dữ liệu CIFAR-100 đuôi dài với mô hình ViT. Mô hình giáo viên là một

--- TRANG 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 12

BẢNG VII
KẾT QUẢ NGHIÊN CỨU ABLATION (%) CỦA CÁC YẾU TỐ THIẾT KẾ TRÊN FOGGY CITYSCAPES [53].

Phương pháp xe đạp xe buýt ô tô xe máy người người lái tàu xe tải mAP
CWD [50] 53.3 55.8 71.9 37.5 57.1 46.8 43.2 50.9 52.1
ProC-KD (w/proto) 53.4 56.5 72.3 38.1 55.3 47.7 45.4 50.5 52.4
ProC-KD 53.8 57.9 73.1 40.3 57.7 51.2 44.2 51.5 53.7

BẢNG VIII
ĐỘ CHÍNH XÁC TRUNG BÌNH (%) CỦA PROC-KD VỚI SỐ LƯỢNG NGUYÊN MẪU KHÁC NHAU TRÊN BỘ DỮ LIỆU CIFAR-100 ĐUÔI DÀI.

Mô hình/Số lượng 24 48 72 96
ProC-KD 77.75 78.11 78.32 78.28

mô hình ViT cơ bản 12 lớp, và mô hình học sinh là một mô hình ViT 6 lớp. Số lượng nguyên mẫu được đặt là 24, 48, 72, và 96 tương ứng. Ở đây, chúng tôi chỉ sử dụng số lượng nguyên mẫu khác nhau và giữ các thiết lập mạng khác không thay đổi để so sánh hiệu suất của mô hình. Bảng VIII hiển thị rằng độ chính xác của mô hình tăng khi số lượng nguyên mẫu tăng, và độ chính xác tốt nhất là 78.32% khi số lượng nguyên mẫu là 72. Nó chỉ ra rằng số lượng nguyên mẫu nhỏ không thể học biểu diễn tổng quát một cách đầy đủ. Trong thí nghiệm của chúng tôi, số lượng nguyên mẫu trong các phương pháp chưng cất tri thức có hướng dẫn nguyên mẫu được đặt là 72.

BẢNG IX
NGHIÊN CỨU ABLATION CỦA SIÊU THAM SỐ HÀM MẤT MÁT TRÊN FOGGY CITYSCAPES [53].

Phương pháp/Số λemb λpro λstu mAP(%)
Học sinh 0 0 1 35.6
CWD 1 0 1 52.1
1 0.3 1 1 49.5
2 1 0.3 1 52.5
3 1 1 0.3 51.2
4 0.5 1 1 50.7
5 1 0.5 1 52.5
6 1 1 0.5 52.0
7 0.8 1 1 52.1
8 1 0.8 1 52.9
9 1 1 0.8 52.8
10 1 1 1 53.7

3) Siêu tham số: Chúng tôi tiến hành nghiên cứu ablation của các siêu tham số trong Phương trình (7) trên FoggyCityscapes. Như được hiển thị trong Bảng IX, chúng tôi đặt các trọng số mất mát λemb, λpro, và λstu với các giá trị khác nhau và có được kết quả phát hiện đối tượng. Kết quả nghiên cứu ablation của các trọng số mất mát cũng tiết lộ tính hiệu quả của phương pháp học nguyên mẫu được đề xuất của chúng tôi trong chưng cất tri thức phát hiện đối tượng.

V. KẾT LUẬN

Để giải quyết vấn đề áp dụng mô hình quy mô lớn cho các nhiệm vụ hạ nguồn khác nhau, chúng tôi đề xuất phương pháp Chưng cất Tri thức Đa nhiệm có Hướng dẫn Nguyên mẫu (ProC-KD), trong đó không gian nhãn của mô hình giáo viên và mô hình học sinh không nhất quán. Cụ thể, mô-đun học nguyên mẫu được huấn luyện để học biểu diễn cục bộ nội tại bất biến với sự trợ giúp của khả năng mạnh mẽ từ mô hình giáo viên. Sau đó, các nguyên mẫu đã học được sử dụng để tăng cường các đặc trưng mô hình học sinh để cải thiện khả năng tổng quát của mô hình học sinh. Chúng tôi tiến hành thí nghiệm trong cả chưng cất tri thức đa nhiệm và chưng cất tri thức tiêu chuẩn cùng nhiệm vụ của phân loại hình ảnh và phát hiện đối tượng. Cả kết quả định lượng và định tính đều xác minh tính hiệu quả của phương pháp được đề xuất của chúng tôi cho chưng cất tri thức.

TÀI LIỆU THAM KHẢO
[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929, 2020.
[3] H. Touvron, M. Cord, D. Matthijs, F. Massa, A. Sablayrolles, và H. Jegou, "Training data-efficient image transformers & distillation through attention," trong ICML 2021: 38th International Conference on Machine Learning, 2021, pp. 10 347–10 357.
[4] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, và B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10 012–10 022.
[5] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, và J. Dai, "Deformable detr: Deformable transformers for end-to-end object detection," trong ICLR 2021: The Ninth International Conference on Learning Representations, 2021.
[6] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, và S. Zagoruyko, "End-to-end object detection with transformers," trong European Conference on Computer Vision, 2020, pp. 213–229.
[7] L. Ye, M. Rochan, Z. Liu, và Y. Wang, "Cross-modal self-attention network for referring image segmentation," trong 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 10 502–10 511.
[8] S. Alfasly, C. K. Chui, Q. Jiang, J. Lu, và C. Xu, "An effective video transformer with synchronized spatiotemporal and spatial self-attention for action recognition," IEEE Transactions on Neural Networks and Learning Systems, pp. 1–14, 2022.
[9] H. Tan và M. Bansal, "Lxmert: Learning cross-modality encoder representations from transformers," trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 5099–5110.
[10] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, và J. Dai, "Vl-bert: Pre-training of generic visual-linguistic representations," trong ICLR 2020: Eighth International Conference on Learning Representations, 2020.
[11] L. Li, Y.-C. Chen, Y. Cheng, Z. Gan, L. Yu, và J. Liu, "Hero: Hierarchical encoder for video+language omni-representation pre-training," trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020, pp. 2046–2065.
[12] Y.-C. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y. Cheng, và J. Liu, "Uniter: Universal image-text representation learning," trong European Conference on Computer Vision, 2020, pp. 104–120.
[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, và L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," trong 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248–255.
[14] M. H. Zhu và S. Gupta, "To prune, or not to prune: exploring the efficacy of pruning for model compression," trong ICLR (Workshop), 2017.
[15] P. Molchanov, S. Tyree, T. Karras, T. Aila, và J. Kautz, "Pruning convolutional neural networks for resource efficient inference," trong ICLR (Poster), 2016.

--- TRANG 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. XX, NO. XX, AUGUST XX 13

[16] J. Wu, C. Leng, Y. Wang, Q. Hu, và J. Cheng, "Quantized convolutional neural networks for mobile devices," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 4820–4828.
[17] G. Hinton, O. Vinyals, và J. Dean, "Distilling the knowledge in a neural network," arXiv preprint arXiv:1503.02531, 2015.
[18] S. Li, M. Lin, Y. Wang, Y. Wu, Y. Tian, L. Shao, và R. Ji, "Distilling a powerful student model via online knowledge distillation," IEEE Transactions on Neural Networks and Learning Systems, pp. 1–10, 2022.
[19] Q. Zhao, J. Dong, H. Yu, và S. Chen, "Distilling ordinal relation and dark knowledge for facial age estimation," IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 7, pp. 3108–3121, 2021.
[20] M. Zhu, J. Li, N. Wang, và X. Gao, "Knowledge distillation for face photo–sketch synthesis," IEEE Transactions on Neural Networks and Learning Systems, vol. 33, no. 2, pp. 893–906, 2022.
[21] C. Yang, Z. An, L. Cai, và Y. Xu, "Knowledge distillation using hierarchical self-supervision augmented distribution," IEEE Transactions on Neural Networks and Learning Systems, pp. 1–15, 2022.
[22] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, và Y. Bengio, "Fitnets: Hints for thin deep nets," trong ICLR 2015: International Conference on Learning Representations 2015, 2015.
[23] Z. Huang và N. Wang, "Like what you like: Knowledge distill via neuron selectivity transfer." arXiv: Computer Vision and Pattern Recognition, 2017.
[24] R. R. Müller, S. Kornblith, và G. Hinton, "When does label smoothing help," trong Advances in Neural Information Processing Systems, vol. 32, 2019, pp. 4694–4703.
[25] J. Yim, D. Joo, J. Bae, và J. Kim, "A gift from knowledge distillation: Fast optimization, network minimization and transfer learning," trong 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 7130–7138.
[26] W. Park, D. Kim, Y. Lu, và M. Cho, "Relational knowledge distillation," trong 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 3967–3976.
[27] J. Gou, B. Yu, S. J. Maybank, và D. Tao, "Knowledge distillation: A survey," International Journal of Computer Vision, vol. 129, no. 6, pp. 1789–1819, 2021.
[28] Y. Chebotar và A. Waters, "Distilling knowledge from ensembles of neural networks for speech recognition." trong Interspeech, 2016, pp. 3439–3443.
[29] G. Kurata và G. Saon, "Knowledge distillation from offline to streaming rnn transducer for end-to-end speech recognition." trong Interspeech, 2020, pp. 2117–2121.
[30] J. W. Yoon, H. Lee, H. Y. Kim, W. I. Cho, và N. S. Kim, "Tutornet: Towards flexible knowledge distillation for end-to-end speech recognition," IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1626–1638, 2021.
[31] V. Sanh, L. Debut, J. Chaumond, và T. Wolf, "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter," arXiv preprint arXiv:1910.01108, 2019.
[32] S. Sun, Y. Cheng, Z. Gan, và J. Liu, "Patient knowledge distillation for bert model compression," trong Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, pp. 4322–4331.
[33] X. Jiao, Y. Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, và Q. Liu, "Tinybert: Distilling bert for natural language understanding," trong Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 4163–4174.
[34] H.-J. Ye, S. Lu, và D.-C. Zhan, "Distilling cross-task knowledge via relationship matching," trong 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 12 396–12 405.
[35] J. Devlin, M.-W. Chang, K. Lee, và K. N. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2018, pp. 4171–4186.
[36] I. Radosavovic, R. P. Kosaraju, R. Girshick, K. He, và P. Dollar, "Designing network design spaces," trong 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020, pp. 10 428–10 436.
[37] J. Ba và R. Caruana, "Do deep nets really need to be deep," trong Advances in Neural Information Processing Systems 27, vol. 27, 2014, pp. 2654–2662.
[38] S. Zagoruyko và N. Komodakis, "Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer," trong ICLR (Poster), 2016.
[39] N. Passalis và A. Tefas, "Learning deep representations with probabilistic knowledge transfer," trong Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 283–299.
[40] D. Chen, J.-P. Mei, Y. Zhang, C. Wang, Z. Wang, Y. Feng, và C. Chen, "Cross-layer distillation with semantic calibration," trong Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 8, 2021, pp. 7028–7036.
[41] J. Snell, K. Swersky, và R. S. Zemel, "Prototypical networks for few-shot learning," trong Advances in Neural Information Processing Systems, vol. 30, 2017, pp. 4077–4087.
[42] J. Liu, L. Song, và Y. Qin, "Prototype rectification for few-shot learning," trong European Conference on Computer Vision, 2019, pp. 741–756.
[43] H. Chefer, S. Gur, và L. Wolf, "Transformer interpretability beyond attention visualization," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 782–791.
[44] S.-A. Rebuffi, H. Bilen, và A. Vedaldi, "Learning multiple visual domains with residual adapters," arXiv preprint arXiv:1705.08045, 2017.
[45] K. Cao, C. Wei, A. Gaidon, N. Arechiga, và T. Ma, "Learning imbalanced datasets with label-distribution-aware margin loss," arXiv preprint arXiv:1906.07413, 2019.
[46] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, và S. Belongie, "Class-balanced loss based on effective number of samples," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 9268–9277.
[47] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, và S. Belongie, "The inaturalist species classification and detection dataset," trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8769–8778.
[48] H. Venkateswara, J. Eusebio, S. Chakraborty, và S. Panchanathan, "Deep hashing network for unsupervised domain adaptation," trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 5018–5027.
[49] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, và C. L. Zitnick, "Microsoft coco: Common objects in context," trong European conference on computer vision. Springer, 2014, pp. 740–755.
[50] C. Shu, Y. Liu, J. Gao, Z. Yan, và C. Shen, "Channel-wise knowledge distillation for dense prediction," trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 5311–5320.
[51] L. Zhang và K. Ma, "Improve object detection with feature-based knowledge distillation: Towards accurate and efficient detectors," trong International Conference on Learning Representations, 2020.
[52] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, và B. Schiele, "The cityscapes dataset for semantic urban scene understanding," trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 3213–3223.
[53] C. Sakaridis, D. Dai, và L. Van Gool, "Semantic foggy scene understanding with synthetic data," International Journal of Computer Vision, vol. 126, no. 9, pp. 973–992, 2018.
[54] L. Van Der Maaten và K. Weinberger, "Stochastic triplet embedding," trong 2012 IEEE International Workshop on Machine Learning for Signal Processing. IEEE, 2012, pp. 1–6.
[55] S. Ahn, S. X. Hu, A. Damianou, N. D. Lawrence, và Z. Dai, "Variational information distillation for knowledge transfer," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 9163–9171.
[56] W. Choi, M. Chandraker, G. Chen, và X. Yu, "Learning efficient object detection models with knowledge distillation," Sep. 20 2018, US Patent App. 15/908,870.
[57] T. Wang, L. Yuan, X. Zhang, và J. Feng, "Distilling object detectors with fine-grained feature imitation," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4933–4942.
[58] B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, và J. Y. Choi, "A comprehensive overhaul of feature distillation," trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 1921–1930.
