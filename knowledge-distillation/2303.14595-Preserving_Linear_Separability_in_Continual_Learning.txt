# 2303.14595.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-distillation/2303.14595.pdf
# File size: 1115022 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preserving Linear Separability in Continual Learning
by Backward Feature Projection
Qiao Gu
University of Toronto
qgu@cs.toronto.eduDongsub Shim
LG AI Research
dongsub.shim@lgresearch.aiFlorian Shkurti
University of Toronto
florian@cs.toronto.edu
Abstract
Catastrophic forgetting has been a major challenge in
continual learning, where the model needs to learn new
tasks with limited or no access to data from previously seen
tasks. To tackle this challenge, methods based on knowl-
edge distillation in feature space have been proposed and
shown to reduce forgetting [16, 19, 27]. However, most fea-
ture distillation methods directly constrain the new features
to match the old ones, overlooking the need for plasticity.
To achieve a better stability-plasticity trade-off, we propose
Backward Feature Projection (BFP), a method for contin-
ual learning that allows the new features to change up to
a learnable linear transformation of the old features. BFP
preserves the linear separability of the old classes while al-
lowing the emergence of new feature directions to accom-
modate new classes. BFP can be integrated with existing
experience replay methods and boost performance by a sig-
nificant margin. We also demonstrate that BFP helps learn
a better representation space, in which linear separability is
well preserved during continual learning and linear prob-
ing achieves high classification accuracy. The code can be
found at https://github.com/rvl-lab-utoronto/BFP.
1. Introduction
Despite their many successes, deep neural networks
remain prone to catastrophic forgetting [39], whereby a
modelâ€™s performance on old tasks degrades significantly
while it is learning to solve new tasks. Catastrophic forget-
ting has become a major challenge for continual learning
(CL) scenarios, where the model is trained on a sequence
of tasks, with limited or no access to old training data.
The ability to learn continually without forgetting is cru-
cial to many real-world applications, such as computer vi-
sion [38, 50], intelligent robotics [32], and natural language
processing [6, 25]. In these settings, an agent learns from
a stream of new data or tasks, but training on the old data
is restricted due to limitations in storage, scaling of training
time, or even concerns about privacy.
Feature space ð‘§â€² (t-SNE) 
after training on task 1Feature space ð‘§ (t-SNE) 
after training on task 2Class 3
Class 4Task 2
Class 1
Class 2Task 1Continual LearningBackward Feature Projection
ð¿ðµð¹ð‘ƒ=ð´ð‘§âˆ’ð‘§â€²2
Figure 1. Feature distribution before and after training on a task
in a class incremental learning experiment on MNIST, visualized
by t-SNE. Left: before training on task 2, seen classes (1,2) are
learned to be separable along the horizontal axis for classification,
while unseen classes (3, 4) are not separable. Right : after training
on task 2, the new vertical axis is learned to separate new classes
(3,4). Based on this observation, we propose the Backward Fea-
ture Projection loss LBFP , which allows new feature dimensions
to emerge to separate new classes in feature space and also pre-
serves the linear separability of old classes to reduce catastrophic
forgetting.
The continual learning problem has received significant
attention and multiple solution themes have emerged. Ex-
perience replay methods [8, 35], for example, store a lim-
ited number of (or generate) old training examples and use
them together with new data in continual learning. Parame-
ter regularization methods [31,54] restrict the change of im-
portant network parameters. Knowledge distillation meth-
ods [16, 19, 33] regularize the intermediate output of the
CL model to preserve the knowledge from old tasks. Ar-
chitectural methods [36, 44, 52] adopt expansion and isola-
tion techniques with neural networks to prevent forgetting.
All these methods strive to balance learning new knowledge
(plasticity) and retaining old knowledge (stability).
We present a continual learning algorithm, focusing on
knowledge distillation (KD) in feature space. In the contin-
ual learning context, KD treats the continual learning model
as the student and its old checkpoint as the teacher and reg-
ularizes the network intermediate outputs to reduce forget-
1arXiv:2303.14595v3  [cs.LG]  28 Jun 2023

--- PAGE 2 ---
ting [4, 8, 11, 16, 17, 19, 27, 33]. Although recent CL meth-
ods based on KD have been effective at reducing forgetting,
they typically adopt the L2distance for distillation, forc-
ing the learned features to be close to their exact old values.
This is too restrictive and results in CL models that are more
rigid in retaining old knowledge (stronger stability), but less
flexible in adapting to new tasks (weaker plasticity). Our
method has a better tradeoff of stability and plasticity.
In this paper, we pay attention to the feature space in CL
and study its evolution. We show that a small number of
principal directions explain most of the variance in feature
space and only these directions are important for classifi-
cation. A large number of directions in the feature space
have little variance and remain unused. When the model
is trained on new tasks, new features need to be learned
along those unused directions to accommodate new classes,
as illustrated in Figure 1. Without handling forgetting, the
old principal directions, along which the old classes are lin-
early separable, will be forgotten. Our results indicate that
such forgetting of learned principal directions in the feature
space is an important reason for catastrophic forgetting.
Based on this insight, as shown in Figure 1, we propose
a Backward Feature Projection (BFP) loss, an effective fea-
ture distillation loss that enforces feature consistency up to a
learnable linear transformation, not imposing exact equality
of features. This transformation aims to preserve the linear
separability of features backward in time. We show that this
linear projection is important because it can rotate, reflect,
and scale features, while maintaining the linear separability
of the previously learned classes in the new feature space.
Projecting backward allows the features to change and new
decision boundaries to be learned along the unused feature
directions to classify new classes. BFP can be integrated
into existing CL methods in a straightforward way and ex-
periments show that this simple change boosts the perfor-
mance over baselines by a large margin.
Our experiments show that the proposed BFP regular-
ization loss can improve the baseline methods by up to 6%-
8% on the challenging Split-CIFAR10 and Split-CIFAR100
datasets, achieving state-of-the-art class-incremental learn-
ing accuracy. More importantly, the linear probing experi-
ments show that BFP results in a better feature space where
different classes are more separable. See Figure 1 for an
illustrative example. Our contributions are as follows:
â€¢ We provide an analysis of feature space evolution dur-
ing continual learning, distinguishing the important
feature components from unimportant ones.
â€¢ We propose the Backward Feature Projection (BFP)
loss, which preserves the linear separability of old
classes while allowing plasticity during continual
learning, i.e. features are allowed to change.
â€¢ When combined with simple experience replay base-
lines, BFP helps learn better feature space and achievesstate-of-the-art performance on challenging datasets.
2. Related Work
2.1. Experience Replay Methods
Experience replay or rehearsal methods use a small
memory buffer to keep the training data of old tasks. When
the model is training on the new task, old training examples
are extracted and trained together with new ones. Recent
replay-based CL approaches mainly differ in three compo-
nents, namely which examples to store, how examples are
replayed, and how to update the network using old exam-
ples. Recent work has focused on evolving the three com-
ponents mentioned above. ICaRL [42] chooses examples
into the memory such that the mean in the feature space of
the memory buffer matches that of training data. MIR [2]
prioritizes the replay of the examples that are mostly in-
terfered with by a virtual update on the network param-
eters. DER/DER++ [8] augments the cross entropy loss
with a logit distillation loss when the memory data is re-
played. GEM [35] and A-GEM [13] develop optimization
constraints when trained on new tasks using the old data
from memory. Some other work [34, 46, 49] also learn
to generate images for replay during CL, but the contin-
ual training of the generation network adds some additional
challenges. Although the idea is straightforward, experi-
ence replay methods often achieve better performance than
other types of methods, which marks the importance of stor-
ing the old data.
2.2. Parameter Regularization Methods
Parameter regularization methods study the effect of
neural network weight changes on task losses and limit the
movement of important ones, which would otherwise cause
forgetting on old tasks. This line of work typically does not
rely on a replay buffer for old task data. One of the pioneer-
ing works along this line is EWC [28], which proposed to
use the empirical Fisher Information Matrix to estimate the
importance of the weight and regularize the weight change
in continual learning. SI [54] uses the estimated path inte-
gral in the optimization process as the regularization weight
for network parameters. MAS [1] improves this idea by
adopting the gradient magnitude as a sensitivity measure.
RWalk [12] combines Fisher information matrix and online
path integral to approximate the parameter importance and
also keeps a memory to improve results.
2.3. Knowledge Distillation Methods
Originally designed to transfer the learned knowledge of
a larger network (teacher) to a smaller one (student), knowl-
edge distillation methods have been adapted to reduce acti-
vation and feature drift in continual learning. Different from
parameter regularization methods that directly regularize
2

--- PAGE 3 ---
the network weights, KD methods regularize the network
intermediate outputs. Li et al. [33] proposed an approach
called Learning without Forgetting (LwF), regularizing the
output logits between the online learned model and the old
checkpoint. DER/DER++ [8] combines this logit regular-
ization with experience replay and further improves the per-
formance. Later Jung et al. [26] proposed to do knowledge
distillation on the feature maps from the penultimate layer
and freeze the final classification layer. Pooled Output Dis-
tillation (PODNet) [19] extended the knowledge distillation
method to intermediate feature maps, and studied how dif-
ferent ways of feature map pooling affect continual learning
performance. They proposed to pool the feature maps along
the height and weight dimensions respectively to achieve a
good stability-plasticity trade-off. Recent work [16,27] also
used gradient information (e.g. Grad-CAM) as weighting
terms in the feature distillation loss, such that feature maps
that are important to old tasks will change less during con-
tinual learning.
Different from existing KD methods, we use a learnable
linear layer to project new features to the old ones. This
idea was explored in [21,22], but their work only integrated
it in a contrastive learning framework and used a nonlin-
ear mapping function. However, in this work, we use a
learnable linear transformation and formulate it as a sim-
ple knowledge distillation loss in the feature space. We
demonstrate that our method promotes linear separability
in feature space during continual learning. We also show
the value of BFP in the supervised CL setting with experi-
ence replay, and no augmentation or contrastive learning is
needed.
3. Method
3.1. Setting and Notation
In a typical continual learning setting, a model fis se-
quentially trained on a set of tasks T={1,2,3,Â·Â·Â·, T}.
Within each task t, input xand the corresponding ground
truth output yare drawn i.i.d. from the task data distribu-
tionDt= (Xt, Yt)and used to train the model. Here Xt
andYtdenote the set of inputs and outputs from task t. To
illustrate our method, the CL model is decomposed into two
parts fÎ¸(x) =gÏ•(hÏˆ(x)) = gÏ•â—¦hÏˆ(x)withÎ¸={Ï•, Ïˆ},
where h, parameterized by Ïˆ, is a non-linear feature ex-
tractor, mapping input image xto a low-dimensional fea-
turezâˆˆRd. The classification head g, parameterized by
Ï•, is a linear layer that maps the latent feature zto clas-
sification logits oâˆˆRc, where cis the total number of
classes. In this paper, we mainly consider the class incre-
mental learning (Class-IL) setting and the task incremental
learning (Task-IL) setting, and the proposed method works
on both settings. In these settings, Dtcontains training data
from a set of classes Ct, where Ctare disjoint for different
0 20 40 60 800100200300400500600Singular value
0 20 40 60 800.00.20.40.60.8Accuracy: proj-acc( k
0 20 40 60 80
Singular value index0.00.20.40.60.81.0Relative singular value
0 20 40 60 80
Number of (largest) principal
directions used: k0.00.20.40.60.81.0Relative accuracy: rel-proj-acc( k)
JT on 10 classes
JT on 40 classes
JT on 70 classes
JT on 100 classes
FT after 10 classes
FT after 40 classes
FT after 70 classes
FT after 100 classesFigure 2. Feature distribution and contribution to classification
during continual learning on Split-CIFAR100 with 10 classes per
task. Left: feature variance (singular values) along each principal
direction; right : classification accuracies proj-acc( k) using pro-
jected features. Upper plots show the absolute singular values
and accuracies, and lower ones show their relative values, nor-
malized by the maximum on each curve. Finetuning (FT) is a
naive CL baseline that does not handle forgetting at all and gives
a lower-bound performance. Joint Training (JT) is an oracle CL
method that is jointly trained on all classes seen so far and shows
the upper bound. Contrasting JT with FT reveals the ideal proper-
ties for good CL methods. As the model is continually trained on
more classes, more feature dimensions are learned and needed for
classification. However, compared to the full feature dimension
(512), only a small subspace (around 10 principal directions for
10 classes and 80 for 100 classes) is crucial for CL performance,
as the relative accuracies quickly saturate with the number of prin-
cipal directions used.
taskt. In Task-IL, task identifiers tfor each input are avail-
able during evaluation time, and thus the model can focus
the decision boundaries within each task. On the contrary,
Class-IL requires making a decision among all classes dur-
ing inference time and thus is more challenging. We further
denote the model after training on task jasfj=gjâ—¦hj,
and the feature extracted by hjfrom a datapoint in task i
aszj
i=hj(x), xâˆˆ D i. The set of all zj
iforms a feature
matrix Zj
i=hj(Di)âˆˆRdÃ—n, and nis the number of dat-
apoints in Di. And similarly, the set of features extracted
from D1toDiusing hjis denoted by Zj
1:i=hj(D1:i).
3.2. Analyzing Feature Forgetting in CL
Motivated by recent work showing that representation
drifts in the feature space have been a major cause for catas-
trophic forgetting [10, 20, 53], we study the evolution of
feature space in CL and answer two key questions: (1)
how many dimensions (principal directions) in the learned
3

--- PAGE 4 ---
feature space are occupied by the data? And (2) how
many of them are used for classification? We answer the
first question by conducting principal component analysis
(PCA) [40] on the feature matrix Zt
1:t, which contains fea-
ture extracted by htfrom all data seen so far D1:t. Suppose
its singular vector decomposition gives Zt
1:t=USVT,
and then principal directions are the left singular vectors
U= [u1, u2,Â·Â·Â·, ud], where ulâˆˆRdare sorted by the
corresponding singular values sl. PCA gives the data dis-
tribution of seen tasks in the feature space and thus answers
the first question. The second question is answered by eval-
uating the features projected onto a subspace spanned by
the first kprincipal directions U1:k. Specifically, we define
the classification accuracy of the projected features as
proj-acc (k) =acc 
y, gt(U1:kUT
1:kz)
(1)
where kis the number of largest principal components used
and proj-acc is computed over the testing set of task t. With
a larger k, more information is retained in the projected
feature U1:kUT
1:kzand used for classification. The changes
of proj-acc with the increase of kreflect the importance of
each principal direction being added.
In Figure 2, we plot skand proj-acc (k)versus kwhen a
model has been trained on a certain number of classes dur-
ing CL. We compare two simple CL methods: finetuning
(FT) where the model is continually trained on the online
data stream without any means to reduce catastrophic for-
getting, and joint training (JT) where all the training data
seen so far is used to train the network. Typically, FT serves
as a naive lower bound for CL performance and JT an oracle
upper bound. Contrasting FT and JT reveals the difference
in feature space obtained from the worst and the ideal CL
methods.
We can see from Figure 2, for JT, as the network is
trained on more classes, the learned features span a larger
subspace and the classifier needs more principal direc-
tions to achieve good classification accuracies (high relative
proj-acc). This shows that during continual learning, more
feature directions are needed to make new classes linearly
separable in the feature space. However, for the naive FT
baseline, the number of principal directions with large vari-
ance does not increase with the number of seen classes. This
indicates feature forgetting: a poor CL method only focuses
on the feature directions that are important for the current
task. The feature directions for old tasks are suppressed to
low variance and thus forgotten. On the other hand, com-
pared to the full feature dimension d= 512 , JT accura-
cies still saturate with a relatively small k= 80 , which is
roughly the number of classes seen so far. Other feature
directions that have low variance are not used for classifica-
tion, and such â€œunusedâ€ feature directions could leave room
for future tasks in CL.
Based on this insight, we argue for the benefit of preserv-ing important feature directions for old tasks while allowing
new ones to emerge for new tasks during continual learning.
Therefore, we propose to learn a linear transformation that
projects the new feature space back to the old one and in the
following section, we show it can achieve both goals.
3.3. Backward Feature Projection
We denote the feature extractor that is being trained on
the current task tash, which may not have converged, and
the converged model checkpoint at the end of the last task as
hâ€²=htâˆ’1. Given an input example x, the extracted features
are denoted as z=h(x)andzâ€²=hâ€²(x). To preserve infor-
mation in feature space such that the new feature zshould
contain at least the information as that in zâ€², we can learn a
projection function pthat satisfies zâ€²=p(z)[21, 22].
In this work, we propose that a linear transformation ma-
trixAcan well preserve linear separability and suffice to
reduce forgetting. Formally, we propose the Backward Fea-
ture Projection (BFP) loss in continual learning. Given a
training example x,
LBFP(x;Ïˆ, A) =âˆ¥Azâˆ’zâ€²âˆ¥2 (2)
=âˆ¥AhÏˆ(x)âˆ’hâ€²(x)âˆ¥2, (3)
where we omit the bias term by adding a fixed entry of 1
in the feature vector z. Here we only optimize hÏˆandA,
while we freeze hâ€²and thus omit its parameters.
In the following, we show that the BFP loss can preserve
the linear separability of old classes while allowing new
classes to be classified along the unused directions in the
old feature space. Consider the extracted features from any
two examples from the old classes zâ€²
i=hâ€²(xi), xiâˆˆC1
andzâ€²
j=hâ€²(xj), xjâˆˆC2, where C1, C2âˆˆ Ctâˆ’1. If they
are learned to be linear separable at the end of task tâˆ’1,
then there exists a vector wand a threshold b, such that
wTzâ€²
i> b > wTzâ€²
j,âˆ€iâˆˆC1,âˆ€jâˆˆC2. (4)
Then if the BFP loss in Equation 3 is well optimized, i.e.
zâ€²â‰ˆAzwith a linear transformation A. Then for the fea-
tures extracted from h,
wTAzi>b > wTAzj,âˆ€iâˆˆC1,âˆ€jâˆˆC2 (5)
â‡’(ATw)Tzi>b > (ATw)Tzj,âˆ€iâˆˆC1,âˆ€jâˆˆC2.(6)
Therefore the feature vectors zfrom the old classes C1, C2
remain linearly separable along the direction ATwin the
new feature space. The linear classifier gis trained to find
this decision boundary during CL with experience replay.
To classify new classes, the network needs to map them
to linearly separable regions in the feature space. The lin-
ear transformation in BFP achieves it by arranging the new
classes along the â€œunusedâ€ feature directions that have low
variance and thus are not occupied by the old tasks. Con-
sider that the features extracted from future task Dtusing
4

--- PAGE 5 ---
the old model hâ€²are probably not separable and mixed to-
gether. This is natural as hâ€²has not been trained on it. As
we can see from Section 3.2 and Figure 2, there exists many
principal directions with low variance, along which fea-
tures from different classes are not separable, Ideally, a CL
model should take up these â€œunusedâ€ feature directions to
learn features that are needed to classify new classes. With-
out loss of generality, suppose before the model is trained
on a new task t, the feature extracted from the new task
zâ€²=hâ€²(x),xâˆˆXt, are all mapped to zero along an
â€œunusedâ€ feature direction v, i.e. vTzâ€²= 0. Then after
learning on task t, the feature z=h(x)from new classes
C3, C4âˆˆ Ctcan be learned to be separable along that fea-
ture direction v,
vTzi> vTzj,âˆ€iâˆˆC3,âˆ€jâˆˆC4. (7)
In this case, Acan be learned such that v /âˆˆCol(A)and thus
vT(Az) = 0 while vTzÌ¸= 0(satisfying Equation 7). In this
way, the BFP loss in Equation 3 allows the new class to be
separable along vand still can be minimized. Note that dur-
ing the actual continual learning with BFP, neither w,vnor
the dimensionality of them is defined or needed. They are
learned implicitly in the matrix Athrough gradient descent
optimization. wandvcan be extracted and analyzed by
PCA decomposition, but it is not required for training.
3.4. Loss functions
We integrate the proposed backward feature projection
method into an experience replay framework [8], where we
keep a buffer Mstoring training examples from old tasks.
We focus on experience replay methods because they are
simple and outperform other types of CL methods by a
large margin according to a recent survey [37]. We keep the
model checkpoint at the end of the last task ftâˆ’1together
with the online trained model f. During continual learning,
the online model fis trained on a batch from the online data
stream of the current task Dtusing cross-entropy loss.
Lce(Dt;Î¸) =X
x,yâˆˆD tcross-entropy (y, fÎ¸(x)) (8)
Meanwhile, we sample another batch from Mfor experi-
ence replay. Following [8], a cross-entropy loss and a logit
distillation loss are applied on the replayed data
Lrep-ce(M;Î¸) =X
x,yâˆˆMcross-entropy (y, fÎ¸(x)),(9)
Lrep-logit (M;Î¸) =X
x,yâˆˆMâˆ¥fÎ¸(x)âˆ’ftâˆ’1(x)âˆ¥2
2. (10)And we apply our backward feature projection loss on both
the online data stream Dtand the replayed examples M
LBFP(Dt, M;Ïˆ, A) =X
x,yâˆˆD t,Mâˆ¥AhÏˆ(x)âˆ’htâˆ’1(x)âˆ¥2.
(11)
The total loss function used in continual learning is the
weighted sum of the losses above.
L(Dt, M;Î¸, A) =Lce(Dt;Î¸) +Î±L rep-ce(M;Î¸)
+Î²Lrep-logit (M;Î¸) +Î³LBFP(Dt, M;Ïˆ, A)(12)
During training on task t, both the linear transformation A
and the model fÎ¸are optimized, and the old model check-
point ftâˆ’1remains fixed. In our experiments, the matrix
Ais randomly initialized at the beginning of each task.
The pseudo-code of the proposed algorithm can be found
in the Appendix.
4. Experiments
4.1. Experimental Setting
Continual Learning Settings . We follow [8,14,51] and
test all methods using both the Class-IL and Task-IL set-
tings in our CL experiments. Both Class-IL and Task-IL
split the dataset into a sequence of tasks, each containing
a disjoint set of classes, while task identifiers are available
during testing under Task-IL. Task-IL thus has extra advan-
tages during inference (e.g. select proper prediction head)
and becomes an easier CL scenario. Our work is designed
for Class-IL and its Task-IL performance is obtained by
only considering the logits within the ground truth task.
Datasets . We evaluate baselines and our methods on the
following datasets using varying buffer sizes: Split CIFAR-
10divides the original CIFAR-10 [30] dataset into 5 tasks,
with each task composed of 2 classes. Each class includes
5000 training and 1000 testing images of shape 32 Ã—32.
Split CIFAR-100 divides CIFAR-100 [30] into 10 tasks,
with 10 classes per task. Each class has 500 training and
100 testing images of shape 32 Ã—32.Split TinyImageNet
splits TinyImageNet [48] into 10 tasks, with 20 classes per
task. Each class contains 500 training images, 50 validation
images, and 50 testing images. These datasets are challeng-
ing and state-of-the-art continual learning methods still fall
far behind the Joint Training (JT) baseline, especially in the
Class-IL setting as shown in Table 1.
Metrics . Following [3, 7, 8, 37], we report the perfor-
mance of each compared method using Final Average Ac-
curacy (FAA). Suppose at
iis the testing classification accu-
racy on the ithtask when the training finishes on task t, FAA
is the accuracy of the final model averaged all tasks:
FAA =1
TTX
i=1aT
i. (13)
5

--- PAGE 6 ---
Setting Method S-CIFAR10 S-CIFAR100 S-TinyImageNet
Buffer Size 200 500 500 2000 4000
Class-IL Joint Training (JT) 91.27 Â±0.57 70.68 Â±0.57 59.61 Â±0.25
Finetuning (FT) 36.20 Â±2.02 9.36 Â±0.07 8.11 Â±0.08
iCaRL [42] 63.58 Â±1.22 62.62 Â±2.07 46.66 Â±0.23 52.60 Â±0.38 31.47 Â±0.46
FDR [5] 31.24 Â±2.61 28.72 Â±2.86 22.64 Â±0.56 34.84 Â±1.03 26.52 Â±0.41
LUCIR [24] 58.53 Â±3.03 70.37 Â±0.97 35.14 Â±0.57 48.95 Â±1.21 29.79 Â±0.70
BiC [51] 59.53 Â±1.77 75.41 Â±1.14 35.96 Â±1.38 45.44 Â±0.96 15.98 Â±1.01
ER-ACE [10] 63.54 Â±0.42 71.17 Â±1.38 38.86 Â±0.72 50.20 Â±0.39 37.72 Â±0.16
ER [43] 58.07 Â±2.92 68.04 Â±2.18 20.34 Â±0.96 37.44 Â±1.48 23.29 Â±0.54
ER w/ BFP (Ours) 63.27 Â±1.09 (+5.21) 71.51 Â±1.58 (+3.47) 22.54 Â±1.10 (+2.20) 38.92 Â±1.94 (+1.48) 26.33 Â±0.68 (+3.04)
DER++ [8] 65.41 Â±1.60 72.65 Â±0.33 38.88 Â±0.91 52.74 Â±0.79 41.24 Â±0.64
DER++ w/ BFP (Ours) 72.21 Â±0.22 (+6.80) 76.02 Â±0.79 (+3.37) 47.45 Â±1.30 (+8.56) 57.91 Â±0.66 (+5.17) 43.40 Â±0.41 (+2.16)
Task-IL Joint Training (JT) 98.19 Â±0.16 91.40 Â±0.43 82.21 Â±0.33
Finetuning (FT) 65.01 Â±5.12 35.54 Â±2.63 18.46 Â±1.26
iCaRL [42] 90.27 Â±1.21 90.05 Â±1.46 84.45 Â±0.48 86.24 Â±0.47 66.06 Â±0.75
FDR [5] 91.42 Â±1.03 93.40 Â±0.31 74.66 Â±0.16 82.15 Â±0.26 66.79 Â±0.66
LUCIR [24] 94.30 Â±0.79 94.99 Â±0.14 85.13 Â±0.20 87.50 Â±0.44 70.09 Â±0.40
BiC [51] 95.41 Â±0.73 96.45 Â±0.34 85.16 Â±0.36 87.03 Â±0.30 68.44 Â±4.40
ER-ACE [10] 92.12 Â±0.62 94.09 Â±0.27 77.00 Â±0.80 83.30 Â±0.54 68.91 Â±0.38
ER [43] 92.06 Â±0.65 93.60 Â±0.76 72.42 Â±1.74 81.34 Â±1.06 64.96 Â±0.45
ER w/ BFP (Ours) 95.50 Â±0.41 (+3.44) 96.11 Â±0.27 (+2.51) 79.79 Â±1.67 (+7.38) 84.16 Â±1.18 (+2.81) 71.43 Â±0.58 (+6.47)
DER++ [8] 91.71 Â±0.83 93.76 Â±0.27 76.96 Â±0.24 83.59 Â±0.41 71.14 Â±0.53
DER++ w/ BFP (Ours) 95.95 Â±0.22 (+4.23) 96.29 Â±0.26 (+2.53) 83.64 Â±0.64 (+6.68) 87.20 Â±0.32 (+3.61) 73.07 Â±0.28 (+1.94)
Table 1. Final Average Accuracies (FAA, in %) in Class-IL and Task-IL setting of baselines and our methods on various datasets and
buffer sizes. The green numbers in parentheses show the absolute improvements brought by BFP over the corresponding ER or DER++
baselines. The proposed BFP method can boost the Class-IL performance by up to 6-8% in some challenge settings with small buffer sizes.
DER++ w/ BFP (Ours) outperforms all baselines in terms of Class-IL accuracies and has Task-IL accuracies that are better or close to the
top-performing methods. Mean and standard deviation are computed over 5 runs with different seeds. Joint Training (JT) shows the upper
bound performance, where the model is trained on data from all tasks and Finetune (FT) is the lower bound, where the model is trained
sequentially without handling forgetting.
We also report the Final Forgetting (FF), which reflects
the accuracy drop between the peak performance on one
task and its final performance:
FF=1
Tâˆ’1Tâˆ’1X
i=1max
jâˆˆ1,Â·Â·Â·,Tâˆ’1(aj
iâˆ’aT
i). (14)
Lower FF means less forgetting and better CL performance.
Training details . We use ResNet-18 [23] as the net-
work backbone, and instead of the simple reservoir buffer
used in [8], we use class-balanced reservoir sampling [9]
for pushing examples into the buffer. All the baselines
we compare are updated with this change. We use an
SGD optimizer to optimize the model fÎ¸and another
SGD+Momentum optimizer with a learning rate 0.1for the
projection matrix A. The optimizers and the matrix Aare
re-initialized at the beginning of each task. The network
is trained for 50 epochs per task for Split CIFAR-10 and
Split CIFAR-100 and 100 epochs per task for Split Tiny-
ImageNet. The learning rate is divided by 10 after a cer-
tain number of epochs within each task ( [35,45]for Split
CIFAR-100 and [35,60,75]for Split TinyImageNet). In
this work, we focus on this offline CL setting where each
task is trained for multiple epochs. Although we are also in-
terested in online CL, multi-epoch training helps disentan-
gle underfitting and catastrophic forgetting [3, 8]. BFP in-troduces only one extra hyperparameter Î³, which is set to 1
for all experiments. We found that Î³= 1works well for all
datasets and buffer sizes and did not perform hyperparam-
eter searches for individual experiment settings. Hyperpa-
rameters Î±andÎ²used in Equation 12 are adopted from [8].
Most baselines adopt different hyperparameters for differ-
ent settings, for which we adopt the hyperparameters that
have been optimized by grid search by [8] and [7] for a fair
comparison. The details can be found in the Appendix.
4.2. Baselines
First, we evaluate the performance of Joint Training
(JT) andFinetuning (FT) baselines on each dataset. JT
trains the network on all training data, does not have the for-
getting problem, and thus indicates the upper-bound perfor-
mance of CL methods. On the contrary, FT simply performs
SGD using the current task data without handling forgetting
at all and indicates a lower-bound performance.
As a feature distillation method, our method can be com-
bined with most continual learning methods. In our evalu-
ation, we test our method by combing it with two popular
experience replay methods, ER [43] and DER++ [8]. ER
uses a memory buffer to store the training examples from
past tasks and interleaves them with the current task data
for training. In addition to this, DER++ records the output
logits of the examples in the memory and performs logit
6

--- PAGE 7 ---
distillation when doing experience replay. We combine the
proposed BFP loss with ER and DER++ and denote them
asER w/ BFP andDER++ w/ BFP respectively.
We also compare the proposed method with some
other state-of-the-art CL baselines as listed in Ta-
ble 1. Incremental Classifier and Presentation Learn-
ing (iCaRL) [42] performs classification using the near-
est mean-of-exemplars, where the exemplars selected by
herding algorithm in the feature space. Functional Dis-
tance Regularization (FDR) [5] regularize the output of
the network to its past value. Different from DER/DER++,
FDR applies the regularization on the output classification
probability. Learning a Unified Classifier Incrementally
via Rebalancing (LUCIR) [24] augments experience re-
play with multiple modifications to preserve old knowledge
and enforce separation class separation in continual learn-
ing. Bias Correction (BiC) [51] augments the experience
replay by learning a separate layer to correct the bias in the
output logits. ER with Asymmetric Cross-Entropy (ER-
ACE) [10] proposes to reduce representation drift by using
separate cross-entropy loss for online and replayed training
data.
4.3. Results
The Final Average Accuracies in the Class-IL and Task-
IL settings are reported in Table 1. The corresponding table
for Final Forgetting can be found in the Appendix. We test
the methods on three datasets with various sizes of memory
buffers. Experiments are averaged over 5 runs with different
seeds and mean and standard deviation are reported. First,
we observe that there is a still big gap between the current
best CL methods and the JT oracle baselines on all datasets,
especially in the Class-IL setting, which indicates that CL
is still an unsolved and challenging problem. Comparing
DER++ and DER++ w/ BFP, we can see that BFP boosts
the Class-IL accuracies by a significant margin, especially
with a small buffer size (6.8% on S-CIFAR10 with buffer
size 200 and 8.5% on S-CIFAR100 with buffer size 500).
DER++ w/ BFP thus outperforms all baseline methods in
the Class-IL setting, which are very challenging as the final
model needs to distinguish testing examples from all seen
classes. Previous CL methods struggle to have satisfactory
performance in this setting. Under the Task-IL setting that is
easier because task identifiers are known during evaluation
time, our model also helps achieve much higher accuracies
over the base ER or DER++ method. And among all the CL
methods compared, the proposed method also achieves the
best or close-to-best accuracies under the Task-IL setting.
4.4. Linear Probing
Some latest work on continual learning studied catas-
trophic forgetting in the final feature space hT(x)[15, 56].
They show that although the accuracy using the continually
1% 10% 100%
Portion of training data used for linear probing0.450.500.550.600.650.700.75Linear Probing AccuracyDER++ w/ BFP
DER++ w/ FD
DER++
FT w/ BFP
FT w/ FD
FTFigure 3. Linear probing accuracies on the frozen feature extrac-
tor that is obtained after training on Split-CIFAR10 with 200 buffer
size, using different methods. A higher linear probing accuracy in-
dicates a better feature space where data points are more linearly
separable. Note that with the help of BFP, even a simple FT base-
line can learn a representation as good as the powerful DER++
method.
trained classifier gTdegrades heavily due to catastrophic
forgetting, the learned knowledge in hTis well maintained.
This is shown by fitting a linear classifier gâˆ—on top of the
frozen feature extractor at the end of continual learning hT.
Such linear probing accuracies obtained by gâˆ—â—¦hTcan be
much higher than gTâ—¦hT. Therefore recent work argues
that catastrophic forgetting mainly happens at the last lin-
ear classification layer and the linear probing accuracies can
be used as a quality measure for the representation learned
from continual learning [21]. We conduct a similar linear
probing analysis on baselines combined with the BFP, and
we additionally test the effect of our method on the naive
FT baseline, which is denoted as FT w/ BFP in Figure 3.
In FT w/ BFP, we do not use the memory buffer and thus
Î±=Î²= 0 in Equation 3, but we apply the BFP loss on
the online data stream with Î³= 1. Linear probing accura-
cies on Split CIFAR-10 are reported in Figure 3, where we
also vary the portion of training data used for linear probing.
The results show that BFP boosts the linear probing accu-
racies of the FT baseline by a significant margin, achieving
a similar performance with the powerful experience replay
method, DER++. When combined with DER++, BFP also
helps improve the linear probing accuracies. This indicates
that either with or without a memory buffer, BFP helps learn
a better feature space during CL, where examples from dif-
ferent classes remain linearly separable.
4.5. Ablation Study
We study the effect of different types of projection lay-
ers used for backward feature projection, in Equation 3. Our
main results are obtained using a linear projection layer as
the learnable transformation layer (denoted as BFP ). We
also test our method using an identity function A=Ias
7

--- PAGE 8 ---
Dataset Buffer FD BFP BFP-2
S-CIFAR10200 68.44Â±1.35 72.21Â±0.22 72.04Â±0.96
500 74.50Â±0.41 76.02Â±0.79 76.63Â±0.63
S-CIFAR100500 43.81Â±1.35 47.45Â±1.30 47.45Â±1.08
2000 56.56Â±0.55 57.91Â±0.66 57.27Â±0.67
S-TinyImg 4000 42.40Â±1.02 43.40Â±0.41 42.91Â±0.50
Table 2. Class-IL Final Average Accuracy using different types
of layers for backward feature projection. The baseline method is
DER++.
2 3 4 5
Task0.20.40.60.81.0CKA SimilaritySeen - DER++ w/ BFP
Seen - DER++ w/ FD
Seen - DER++
Unseen - DER++ w/ BFP
Unseen - DER++ w/ FD
Unseen - DER++
Figure 4. Feature similarity at different tasks of training on Split-
CIFAR10, with 200 buffer size, using different CL methods.
the projector, which is essentially a feature distillation loss
(FD) on the final feature, as well as using a non-linear func-
tion (a two-layer MLP with ReLU activation in between)
asp, which is denoted as BFP-2 . These variations are
tested when integrated with DER++ [8] and the results are
shown in Table 2. According to Table 2, while the simple
FD method already outperforms the baseline, the proposed
learnable BFP further boosts the accuracies by a large mar-
gin. This is expected because FD regularizes the learned
features directly to those from the old model, while the old
model has not learned from the new data and may give use-
less features. In this case, FD promotes stability while lack-
ing plasticity. On the contrary, BFP is learnable and thus
provides the desired plasticity that allows new knowledge
to appear while maintaining the old ones. Furthermore, we
can also see that the performance already saturates with a
linear projection layer and a more complex non-linear pro-
jection (BFP-2) does not improve further. We hypothesize
that because BFP is applied on the feature space just before
the linear classifier, linear separability is better maintained
with a linear transformation rather than a non-linear func-
tion.
4.6. Feature Similarity Analysis
To demonstrate that the proposed BFP method regular-
izes the features that are already learned while allowing fea-
tures of new data to freely evolve, we conduct an analysis
of feature similarity. Following [15, 41], we adopt Cen-
tered Kernal Alignment (CKA) [29] to measure the fea-
ture similarity before and after training on a task. CKA isa similarity measure for deep learned representations, and
itâ€™s invariant to isotropic scaling and orthogonal transforma-
tion [29]. CKA between two feature matrices Z1âˆˆRd1Ã—n
andZ2âˆˆRd2Ã—nwith a linear kernel is defined as
CKA (Z1, Z2) =âˆ¥Z1ZT
2âˆ¥2
F
âˆ¥Z1ZT
1âˆ¥2
Fâˆ¥Z2ZT
2âˆ¥2
F. (15)
Recall that the feature matrix extracted from Diusing
model hjis denoted as Zj
i=hj(Di), and similar Zj
1:i=
hi(D1:i). During learning on task t, we consider two sets
of features, features from D1:tâˆ’1that have been learned by
the model (seen) and features from Dtthat are new to the
model (unseen). We define their CKA similarity before and
after learning on task trespectively as follows
CKAseen
t=CKA (Ztâˆ’1
1:tâˆ’1, Zt
1:tâˆ’1) (16)
CKAunseen
t =CKA (Ztâˆ’1
t, Zt
t). (17)
Note that Ztâˆ’1
trepresents the features extracted from
future data by the old model, and itâ€™s expected that they
do not provide useful information. On the contrary, Ztâˆ’1
1:tâˆ’1
has already been well learned and we want to preserve its
structure. Therefore, we want CKAseen
tto be high to re-
tain knowledge, while CKAunseen
t low to allow the feature
of unseen data to change freely in continual learning. We
plot CKAseen
tand CKAunseen
t during CL in Figure 4 and the
results confirm our desire. DER++ applies no direct con-
straint on the feature space during CL and thus similarity
is low for both seen and unseen data. On the contrary, FD
poses a strong constraint on both seen and unseen data, re-
sulting in high similarities. In this way, FD gains more sta-
bility at the cost of lower plasticity. Combining their respec-
tive advantages, BFP keeps a high CKAseen
twhile allowing
the unseen features to change (low CKAunseen
t ), and thus is
able to achieve a better trade-off between stability and plas-
ticity.
5. Conclusion
In this paper, we reduce catastrophic forgetting in con-
tinual learning (CL) by proposing Backward Feature Pro-
jection (BFP), a learnable feature distillation method. We
show that during CL, despite the large dimension of the
feature space, only a small number of feature directions
are used for classification. Without regularization, previ-
ously learned feature directions diminish and harm linear
separability, resulting in catastrophic forgetting. The pro-
posed BFP helps maintain linear separability learned from
old tasks while allowing new feature directions to be learned
for new tasks. In this way, BFP achieves a better trade-off
between plasticity and stability. BFP can be combined with
existing experience replay methods and experiments show
that it can boost performance by a significant margin. We
8

--- PAGE 9 ---
also show that BFP results in a more linearly separable fea-
ture space, on which a linear classifier can recover higher
accuracies.
References
[1] Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny,
Marcus Rohrbach, and Tinne Tuytelaars. Memory aware
synapses: Learning what (not) to forget. In ECCV , 2018.
2
[2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Lau-
rent Charlin, Massimo Caccia, Min Lin, and Lucas Page-
Caccia. Online continual learning with maximal interfered
retrieval. In NeurIPS , 2019. 2
[3] Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learn-
ing fast, learning slow: A general continual learning method
based on complementary learning system. In ICLR , 2022. 5,
6
[4] Tommaso Barletti, Niccol Â´o Biondi, Federico Pernici, Matteo
Bruni, and Alberto Del Bimbo. Contrastive supervised distil-
lation for continual representation learning. In ICIAP , pages
597â€“609. Springer, 2022. 2
[5] Ari S. Benjamin, David Rolnick, and Konrad P. K Â¨ording.
Measuring and regularizing networks in function space. In
ICLR , 2019. 6, 7, 13
[6] Magdalena Biesialska, Katarzyna Biesialska, and Marta R
Costa-Jussa. Continual lifelong learning in natural language
processing: A survey. arXiv preprint arXiv:2012.09823 ,
2020. 1
[7] Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, An-
gelo Porrello, and Simone Calderara. Class-incremental con-
tinual learning into the extended der-verse. arXiv preprint
arXiv:2201.00766 , 2022. 5, 6, 11
[8] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide
Abati, and Simone Calderara. Dark experience for general
continual learning: a strong, simple baseline. NeurIPS , 2020.
1, 2, 3, 5, 6, 8, 11, 13
[9] Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Si-
mone Calderara. Rethinking experience replay: a bag of
tricks for continual learning. In ICPR , 2021. 6, 11
[10] Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuyte-
laars, Joelle Pineau, and Eugene Belilovsky. New insights
on reducing abrupt representation change in online continual
learning. In ICLR , 2022. 3, 6, 7, 13
[11] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Con-
trastive continual learning. In ICCV , 2021. 2
[12] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-
than, and Philip HS Torr. Riemannian walk for incremen-
tal learning: understanding forgetting and intransigence. In
ECCV , 2018. 2
[13] Arslan Chaudhry, Marcâ€™Aurelio Ranzato, Marcus Rohrbach,
and Mohamed Elhoseiny. Efficient lifelong learning with a-
gem. In ICLR , 2019. 2
[14] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny,
Thalaiyasingam Ajanthan, Puneet K Dokania, Philip HS
Torr, and Marcâ€™Aurelio Ranzato. Continual learning with
tiny episodic memories. In ICML , 2019. 5[15] MohammadReza Davari, Nader Asadi, Sudhir Mudur, Rahaf
Aljundi, and Eugene Belilovsky. Probing representation for-
getting in supervised and unsupervised continual learning. In
CVPR , 2022. 7, 8
[16] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng,
Ziyan Wu, and Rama Chellappa. Learning without mem-
orizing. In CVPR , 2019. 1, 2, 3
[17] Nikita Dhawan, Sicong Huang, Juhan Bae, and Roger
Grosse. Efficient parametric approximations of neural net-
work function space distance. In ICML , 2023. 2
[18] Thang Doan, Mehdi Abbana Bennani, Bogdan Mazoure,
Guillaume Rabusseau, and Pierre Alquier. A theoretical
analysis of catastrophic forgetting through the ntk overlap
matrix. In AISTATS , pages 1072â€“1080. PMLR, 2021. 14
[19] Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas
Robert, and Eduardo Valle. Podnet: Pooled outputs distil-
lation for small-tasks incremental learning. In ECCV , pages
86â€“102. Springer, 2020. 1, 2, 3
[20] Laura N Driscoll, Lea Duncker, and Christopher D Har-
vey. Representational drift: Emerging theories for continual
learning and experimental future directions. Current Opin-
ion in Neurobiology , 76:102609, 2022. 3
[21] Enrico Fini, Victor G Turrisi da Costa, Xavier Alameda-
Pineda, Elisa Ricci, Karteek Alahari, and Julien Mairal. Self-
supervised models are continual learners. In CVPR , 2022. 3,
4, 7
[22] Alex Gomez-Villa, Bartlomiej Twardowski, Lu Yu, An-
drew D Bagdanov, and Joost van de Weijer. Continually
learning self-supervised representations with projected func-
tional regularization. In CVPR Workshop , 2022. 3, 4
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In CVPR ,
2016. 6
[24] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a unified classifier incrementally via
rebalancing. In ICCV , 2019. 6, 7, 13
[25] Joel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,
Janghoon Han, Gyeonghun Kim, Stanley Jungkyu Choi, and
Minjoon Seo. Towards continual knowledge learning of lan-
guage models. arXiv preprint arXiv:2110.03215 , 2021. 1
[26] Heechul Jung, Jeongwoo Ju, Minju Jung, and Junmo Kim.
Less-forgetting learning in deep neural networks. arXiv
preprint arXiv:1607.00122 , 2016. 3
[27] Minsoo Kang, Jaeyoo Park, and Bohyung Han. Class-
incremental learning by knowledge distillation with adaptive
feature consolidation. In CVPR , 2022. 1, 2, 3
[28] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proceedings of the national academy of sci-
ences , 114(13):3521â€“3526, 2017. 2
[29] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and
Geoffrey Hinton. Similarity of neural network representa-
tions revisited. In ICML , pages 3519â€“3529. PMLR, 2019.
8
[30] Alex Krizhevsky. Learning multiple layers of features from
tiny images. Technical report, Citeseer, 2009. 5
9

--- PAGE 10 ---
[31] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha,
and Byoung-Tak Zhang. Overcoming catastrophic forgetting
by incremental moment matching. In NeurIPS , 2017. 1, 14
[32] Timoth Â´ee Lesort, Vincenzo Lomonaco, Andrei Stoian, Da-
vide Maltoni, David Filliat, and Natalia D Â´Ä±az-Rodr Â´Ä±guez.
Continual learning for robotics: Definition, framework,
learning strategies, opportunities and challenges. Informa-
tion Fusion , 2020. 1
[33] Zhizhong Li and Derek Hoiem. Learning without forgetting.
PAMI , 2017. 1, 2, 3
[34] Xialei Liu, Chenshen Wu, Mikel Menta, Luis Herranz, Bog-
dan Raducanu, Andrew D Bagdanov, Shangling Jui, and
Joost van de Weijer. Generative feature replay for class-
incremental learning. In CVPR Workshops , 2020. 2
[35] David Lopez-Paz and Marcâ€™Aurelio Ranzato. Gradient
episodic memory for continual learning. In NeurIPS , 2017.
1, 2
[36] Arun Mallya and Svetlana Lazebnik. Packnet: Adding mul-
tiple tasks to a single network by iterative pruning. In CVPR ,
2018. 1
[37] Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel
Menta, Andrew D Bagdanov, and Joost van de Weijer. Class-
incremental learning: survey and performance evaluation
on image classification. arXiv preprint arXiv:2010.15277 ,
2020. 5
[38] Marc Masana, Xialei Liu, BartÅ‚omiej Twardowski, Mikel
Menta, Andrew D Bagdanov, and Joost van de Weijer. Class-
incremental learning: survey and performance evaluation on
image classification. PAMI , 2022. 1
[39] Michael McCloskey and Neal J Cohen. Catastrophic inter-
ference in connectionist networks: The sequential learning
problem. In Psychology of learning and motivation , vol-
ume 24, pages 109â€“165. Elsevier, 1989. 1
[40] Karl Pearson. Liii. on lines and planes of closest fit to sys-
tems of points in space. The London, Edinburgh, and Dublin
philosophical magazine and journal of science , 2(11):559â€“
572, 1901. 4
[41] Vinay V Ramasesh, Ethan Dyer, and Maithra Raghu.
Anatomy of catastrophic forgetting: Hidden representations
and task semantics. arXiv preprint arXiv:2007.07400 , 2020.
8
[42] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert. icarl: Incremental clas-
sifier and representation learning. In CVPR , 2017. 2, 6, 7,
13
[43] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu,
Irina Rish, Yuhai Tu, and Gerald Tesauro. Learning to learn
without forgetting by maximizing transfer and minimizing
interference. In ICLR , 2019. 6, 13, 14
[44] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
arXiv , 2016. 1
[45] Gobinda Saha, Isha Garg, Aayush Ankit, and Kaushik Roy.
Space: Structured compression and sharing of representa-
tional space for continual learning. IEEE Access , 9:150480â€“
150494, 2021. 14[46] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.
Continual learning with deep generative replay. In NeurIPS ,
2017. 2
[47] Christian Simon, Piotr Koniusz, and Mehrtash Harandi. On
learning the geodesic path for incremental learning. In
CVPR , pages 1591â€“1600, 2021. 14
[48] Stanford. Tiny imagenet challenge, cs231n course., CS231N.
5
[49] Gido M Van de Ven and Andreas S Tolias. Generative replay
with feedback connections as a general strategy for continual
learning. arXiv preprint arXiv:1809.10635 , 2018. 2
[50] Jianren Wang, Xin Wang, Yue Shang-Guan, and Abhinav
Gupta. Wanderlust: Online continual object detection in the
real world. In ICCV , pages 10829â€“10838, 2021. 1
[51] Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye,
Zicheng Liu, Yandong Guo, and Yun Fu. Large scale in-
cremental learning. In CVPR , 2019. 5, 6, 7, 13
[52] Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynam-
ically expandable representation for class incremental learn-
ing. In CVPR , pages 3014â€“3023, 2021. 1
[53] Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz,
Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van de
Weijer. Semantic drift compensation for class-incremental
learning. In CVPR , 2020. 3
[54] Friedemann Zenke, Ben Poole, and Surya Ganguli. Contin-
ual learning through synaptic intelligence. In ICML , 2017.
1, 2
[55] Jingxin Zhang, Donghua Zhou, and Maoyin Chen. Mon-
itoring multimode processes: A modified pca algorithm
with continual learning ability. Journal of Process Control ,
103:76â€“86, 2021. 14
[56] Xiao Zhang, Dejing Dou, and Ji Wu. Feature forget-
ting in continual representation learning. arXiv preprint
arXiv:2205.13359 , 2022. 7
[57] Fei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-lin Liu.
Class-incremental learning via dual augmentation. NeurIPS ,
34:14306â€“14318, 2021. 14
10

--- PAGE 11 ---
A. Acknowledgements
The authors thank Jongseong Jang, Yizhou (Philip)
Huang, Kevin Xie, and Nikita Dhawan for discussions and
useful feedback.
B. Implementation Details
B.1. Complete Algorithm
In Algorithm 1, we provide the pseudocode of continual
learning with the proposed BFP method. Note that follow-
ing [8], we sample training data points from the memory
buffer for each loss independently. We empirically find this
results in better performance than using the same set of re-
played samples for all losses. The images without augmen-
tation xoare pushed into the memory and replayed images
are augmented on the fly. The classification model is trained
using an SGD optimizer ( sgd) and the projection matrix A
is trained using an SGD+Momentum optimizer ( sgdm ).
B.2. Class-balanced Reservoir Sampling
We adopt the class-balanced reservoir sampling
(BRS) [9] for memory buffer management. The detail
of this algorithm is described in Algorithm 2. Compared
to regular Reservoir Sampling (RS), BRS ensures that
every class has an equal number of examples stored in
the memory buffer. All experiments are incorporated with
this change. Empirically we find that BRS does not bring
Algorithm 1 - Continual Learning with BFP
Input: dataset {D1,Â·Â·Â·,DT}, parameters Î¸={Ï•, Ïˆ}, scalars
Î±,Î²andÎ³, optimizer sgd,sgdm ,
Mâ† {}
fortfrom 1toTdo
Aâ†random-init ()
sgdmâ†reinit (sgdm )
for(xo, yo)inDtdo
x, yâ†augment (xo, yo)
Lâ†cross-entropy (y, fÎ¸(x)) {Eq. 8}
ift >1then
x, yâ†augment (sample (M))
Lrep-ceâ†cross-entropy (y, fÎ¸(x)) {Eq. 9}
x, yâ†augment (sample (M))
Lrep-logits â† âˆ¥fÎ¸(x)âˆ’fold(x)âˆ¥2 {Eq. 10}
x, yâ†augment (sample (M))
LBFPâ† âˆ¥AhÏˆ(x)âˆ’hold(x)âˆ¥2 {Eq. 11}
L=L+Lrep-ce +Lrep-logits +LBFP {Eq. 12}
end if
Î¸â†sgd(Î¸,âˆ‡Î¸L)
Aâ†sgdm (A,âˆ‡AL)
Mâ†balanced-reservoir (M,(xo, yo)) {Alg. 2}
end for
fold=freeze (fÎ¸)
end forAlgorithm 2 Balanced Reservoir Sampling [9]
1:Input: replay buffer M, exemplar (x, y),
2: number of seen examples N.
3:if|M|> N then
4:M[N]â†(x, y)
5:else
6:jâ†RandInt([0 , N])
7: ifj <|M|then
8: M[j]â†(x, y)Reservoir Sampling
9: Ëœyâ†argmax ClassCounts( M, y)
10: kâ†RandChoice( {Ëœk;M[Ëœk] = (x, y), y= Ëœy})
11: M[k]â†(x, y)Balanced Reservoir Sampling
12: end if
13:end if
significant changes compared to RS, but it helps to reduce
variance in the results.
B.3. Training details
Image sizes are 32Ã—32in Split-CIFAR10 and Split-
CIFAR100 and 64Ã—64in Split-TinyImageNet. All exper-
iments use the same data augmentation procedure, applied
on input from both the current task and the memory buffer
independently. Data augmentation includes a full-size ran-
dom crop with a padding of 4 pixels, a random horizontal
flip, and normalization.
For all experiments involving BFP, the optimizer for the
matrix Ais an SGD+Momentum optimizer with a learning
rate of 0.1 and momentum of 0.9. The weighting term Î³in
Equation 12 is 1. Empirically we find that the BFP perfor-
mance is not sensitive to these hyperparameters, and we use
this one set of hyperparameters for BFP loss in all experi-
ments.
B.4. Hyperparameters
In this section, we list the best hyperparameters used for
the compared baselines mentioned in Section 4.2 and their
results are reported in Table 1. These hyperparameters are
adopted from [8] and [7], where they were selected by a hy-
perparameter search conducted on a held-out 10% training
set for validation. Please refer to [7, 8] for further details.
The proposed BFP only introduced a single hyperpa-
rameter Î³, which is set to a constant value of 1through-
out all experiments and does not need extra tuning. Other
hyperparameters like Î±andÎ²are inherited from ER and
DER++ [8] and we simply adopt the same set of hyperpa-
rameters from [8]. We do not further tune or modify them.
11

--- PAGE 12 ---
B.4.1 Split CIFAR-10
FT:lr= 0.1
JT:lr= 0.1
Buffer size = 200
iCaRL :lr= 0.1,wd= 10âˆ’5
FDR :lr= 0.03,Î±= 0.3
LUCIR :Î»base= 5,mom = 0.9,k= 2, epochfitting = 20 ,
lr= 0.03, lrfitting= 0.01,m= 0.5
BiC:Ï„= 2, epochsBiC= 250 ,lr= 0.03
ER-ACE :lr= 0.03
ER:lr= 0.1
DER++ :lr= 0.03,Î±= 0.1,Î²= 0.5
Buffer size = 500
iCaRL :lr= 0.1,wd= 10âˆ’5
FDR :lr= 0.03,Î±= 1
LUCIR :Î»base= 5,mom = 0.9,k= 2, epochfitting = 20 ,
lr= 0.03, lrfitting= 0.01,m= 0.5
BiC:Ï„= 2, epochsBiC= 250 ,lr= 0.03
ER-ACE :lr= 0.03
ER:lr= 0.1
DER++ :lr= 0.03,Î±= 0.2,Î²= 0.5
B.4.2 Split CIFAR-100
FT:lr= 0.03
JT:lr= 0.03
Buffer size = 500
iCaRL :lr= 0.3,wd= 10âˆ’5
FDR :lr= 0.03,Î±= 0.3
LUCIR :Î»base= 5,mom = 0.9,k= 2, epochfitting = 20 ,
lr= 0.03, lrfitting= 0.01,m= 0.5
BiC:Ï„= 2, epochsBiC= 250 ,lr= 0.03
ER-ACE :lr= 0.03
ER:lr= 0.1
DER++ :lr= 0.03,Î±= 0.2,Î²= 0.5
Buffer size = 2000
iCaRL :lr= 0.3,wd= 10âˆ’5
FDR :lr= 0.03,Î±= 1
LUCIR :Î»base= 5,mom = 0.9,k= 2, epochfitting = 20 ,
lr= 0.03, lrfitting= 0.01,m= 0.5
BiC:Ï„= 2, epochsBiC= 250 ,lr= 0.03
ER-ACE :lr= 0.03
ER:lr= 0.1
DER++ :lr= 0.03,Î±= 0.1,Î²= 0.5
B.4.3 Split TinyImageNet
FT:lr= 0.03
JT:lr= 0.03
Buffer size = 4000iCaRL :lr= 0.03,wd= 10âˆ’5
FDR :lr= 0.03,Î±= 0.3
LUCIR :Î»base=5,mom =0.9, k=2, epochfitting=20,lr=0.03,
lrfitting=0.01, m=0.5
BiC:Ï„= 2, epochsBiC= 250 ,lr= 0.03
ER-ACE :lr= 0.03
ER:lr= 0.1
DER++ :lr= 0.1,Î±= 0.3,Î²= 0.8
C. Additional results
C.1. Final Forgetting
Final Forgetting (FF) measures the performance drop be-
tween the end of each task and the end of CL. A CL method
with a lower FF has a better ability to retain knowledge
during CL and thus better stability. However, higher sta-
bility may come with the price of plasticity, and we re-
mind readers that the Final Average Accuracy (FAA) re-
ported in Table 1 can better reflect the trade-off between
stability and plasticity. The Final Forgetting for all base-
lines and our methods can be found in Table 4. As we
can see from Table 4, in the class-IL setting, the proposed
DER++ w/ BFP method helps reduce FF compared to the
base DER++ method by 11% and 12% on S-CIFAR10 with
200 buffer size and S-CIFAR100 with 500 buffer size re-
spectively. DER++ w/ BFP also achieves the lowest FF
among all compared methods in the class-IL setting. Final
Forgettings in the Task-IL setting are generally much lower
than those from the Class-IL setting because Task-IL pro-
vides the oracle task identifiers during the testing time and
thus becomes a much easier CL scenario. In this setting, the
proposed BFP also brings large improvements over the base
ER and DER++ methods.
Dataset Buffer FD BFP BFP-2
S-CIFAR10200 55.10Â±1.85 63.27Â±1.09 60.61Â±2.72
500 66.37Â±1.37 71.51Â±1.58 70.25Â±1.18
S-CIFAR100500 20.02Â±0.09 22.54Â±1.10 21.25Â±0.73
2000 36.81Â±0.71 38.92Â±1.94 39.42Â±2.54
S-TinyImg 4000 23.13Â±0.77 26.33Â±0.68 25.87Â±0.86
Table 3. Class-IL Final Average Accuracy using different types of
layers for backward feature projection. The base method is ER.
C.2. Ablation Study based on Experience Replay
We conduct the same ablation study as that in Sec-
tion 4.5, on different types of the projection layer used in ER
w/ BFP. The results are reported in Table 3. From Table 3,
we can draw the same conclusion as in Section 4.5. BFP
uses learnable linear transformation when distilling features
and thus results in better plasticity during CL compared to
the simple FD method. Results show that BFP outperforms
FD by a significant margin and has better performance than
BFP-2 in most cases. This further shows that enforcing a
12

--- PAGE 13 ---
Setting Method S-CIFAR10 S-CIFAR100 S-TinyImageNet
Buffer Size 200 500 500 2000 4000
Class-IL Joint Training - - -
Finetune 96.44 Â±0.28 89.54 Â±0.16 78.54 Â±0.45
iCaRL [42] 27.75 Â±0.82 25.31 Â±4.35 30.13 Â±0.28 24.72 Â±0.66 16.82 Â±0.51
FDR [5] 76.08 Â±4.87 83.16 Â±4.72 73.71 Â±0.68 60.90 Â±1.41 57.01 Â±0.59
LUCIR [24] 46.36 Â±3.17 29.11 Â±0.63 53.24 Â±0.56 34.16 Â±1.19 25.50 Â±1.86
BiC [51] 44.36 Â±2.73 20.88 Â±2.17 51.86 Â±1.57 41.42 Â±1.61 61.67 Â±1.42
ER-ACE [10] 21.59 Â±1.19 15.07 Â±0.99 38.32 Â±1.29 28.69 Â±0.87 30.83 Â±0.23
ER [43] 42.19 Â±5.19 26.64 Â±6.33 47.62 Â±33.70 44.03 Â±18.79 49.61 Â±16.47
ER w/ BFP (Ours) 32.23 Â±5.58 (-9.96) 22.67 Â±6.64 (-3.97) 47.69 Â±30.30 (-0.07) 37.49 Â±18.06 (-6.54) 41.59 Â±20.77 (-8.02)
DER++ [8] 28.28 Â±1.06 20.16 Â±1.49 42.58 Â±2.03 26.29 Â±1.66 16.03 Â±1.20
DER++ w/ BFP (Ours) 16.69 Â±0.28 (-11.59) 13.25 Â±0.64 (-6.91) 29.85 Â±0.97 (-12.73) 20.91 Â±0.86 (-5.39) 9.42 Â±1.04 (-6.28)
Task-IL Joint Training - - -
Finetune 39.72 Â±6.27 60.46 Â±2.74 67.04 Â±1.27
iCaRL [42] 4.29 Â±1.00 1.91 Â±2.12 3.67 Â±0.40 1.82 Â±0.32 3.56 Â±0.46
FDR [5] 7.03 Â±1.38 4.47 Â±0.45 16.63 Â±0.20 9.17 Â±0.33 13.73 Â±0.30
LUCIR [24] 2.83 Â±0.99 2.04 Â±0.27 2.61 Â±0.17 1.08 Â±0.13 4.95 Â±0.61
BiC [51] 0.81 Â±0.77 0.24 Â±0.25 3.95 Â±0.35 2.36 Â±0.40 7.08 Â±3.74
ER-ACE [10] 6.10 Â±0.72 3.64 Â±0.29 13.95 Â±0.45 7.36 Â±0.43 10.67 Â±0.41
ER [43] 5.71 Â±0.60 3.54 Â±1.15 11.55 Â±6.31 6.12 Â±2.49 11.77 Â±2.06
ER w/ BFP (Ours) 1.38 Â±0.29 (-4.34) 0.77 Â±0.38 (-2.77) 5.63 Â±1.56 (-5.92) 2.95 Â±0.75 (-3.16) 3.31 Â±1.19 (-8.46)
DER++ [8] 3.88 Â±0.51 1.65 Â±0.17 11.68 Â±0.55 4.80 Â±0.45 6.73 Â±0.41
DER++ w/ BFP (Ours) 1.04 Â±0.23 (-2.84) 0.53 Â±0.23 (-1.12) 6.36 Â±0.43 (-5.32) 3.26 Â±0.15 (-1.54) 4.17 Â±0.37 (-2.49)
Table 4. Final Forgetting (FF, in %, lower is better) in Class-IL and Task-IL setting of baselines and our methods on various datasets and
buffer sizes. The green numbers in parentheses show the absolute improvements over the corresponding ER or DER++ baselines brought
by BFP.
linear relationship between the new and old features could
better preserve linear separability and result in less forget-
ting in CL.
C.3. Linear Probing
We conduct the same linear probing analysis as
Section 4.4 Figure 3 on Split-CIFAR100 and Split-
TinyImageNet, and the results are reported in Figure 5. On
these two datasets, while FD and BFP result in similar lin-
ear probing performance when based on DER++, BFP still
leads to better linear probing accuracies when based on FT,
especially when a large subset of training data is used for
linear probing. FT w/ BFP (without the memory buffer) has
a similar or even better performance than DER++ (with the
memory buffer). This shows that BFP help learns a better
feature space from CL, where features from different class
are more linearly separable.
C.4. Feature Similarity Analysis
We perform the same feature similarity analysis as
Section 4.6 and Figure 4 on Split-CIFAR100 and Split-
TinyImageNet, and the results are reported in Figure 6.
From Figure 6, although the curves have high variance
throughout continual learning, we can see that BFP has fea-
ture similarities that are higher than the DER++ baseline but
lower than the naive FD, and thus achieve a better trade-off
between stability and plasticity.Method DER++ w/ FD w/ BFP w/ BFP-2
Class-IL 49.20 Â±1.99 51.89 Â±3.42 54.45 Â±0.86 52.88 Â±1.86
Task-IL 69.01 Â±2.01 71.23 Â±2.80 72.05 Â±1.04 70.56 Â±1.47
Table 5. Final Average Accuracy on ImageNet-100. (mean Â±std
over 3 runs)
C.5. Experiments on Split-ImageNet100
To demonstrate that the proposed BFP method scales
to large datasets, we conduct experiments on Ima-
geNet100 [24, 42]. We split ImageNet100 into 10 tasks
with 10 classes per task and use a memory buffer of size
2000. The model is trained for 65 epochs on each task us-
ing an SGD optimizer with an initial learning rate of 0.1 and
weight decay of 5Ã—10âˆ’4. Within each task, the learning
rate goes through a linear warm-up scheduler for the first 5
epochs and then decays with a 0.1 rate after 15, 30, 40, and
45 epochs. The results are reported in Table 5, which shows
that the proposed BFP method still gives a significant im-
provement (over 5% in Class-IL setting) over the DER++
baseline, confirming our existing results.
C.6. Effect of Î³on Plasticity and Stability
In continual learning, the weight of regularization loss
controls how closely and strictly the model should resem-
ble the old checkpoints. Therefore the weight serves as a
control knob on the trade-off between stability and plastic-
ity: with a stronger regularization loss, the model forgets
old tasks less but instead has a hard time learning new tasks.
13

--- PAGE 14 ---
Although we did not perform an extensive hyperparam-
eter search on Î³for individual combinations of datasets and
buffer sizes, we are still interested in how the varying Î³af-
fects the trade-off between stability and plasticity in con-
tinual learning. Therefore, we train DER++ w/ BFP on S-
CIFAR10 with different Î³and report the performance in
Table 6. Besides FAA and FF, we also report the Average
Learning Accuracy (ALA) [43], which measures the learn-
ing ability on new tasks in continual learning and thus re-
flects the plasticity. Using the notation from Sec. 4.1, ALA
is defined as
ALA =1
TTX
i=1ai
i. (18)
From Table 6, we can see that the effect of Î³aligns with
our intuition. A higher Î³poses a strong regularization on
the feature space, resulting in a lower FF (more stable) but
also a lower ALA (less plastic). Also, we can observe that
the final performance (FAA) remains robust to the value of
Î³within a considerable range.
D. More Related Work
There has been some recent work that also employs PCA
computation in continual learning. Note that the proposed
BFP does not require PCA computation during training and
the feature directions are learned implicitly when optimiz-
ing matrix A. However, to provide a complete understand-
ing of the literature, we briefly review the related work that
also uses PCA for continual learning.
Doan et al. [18] proposed PCA-OGD, which combines
PCA analysis with Orthogonal Gradient Descent (OGD).
PCA-OGD projects the gradients onto the residuals sub-
space to reduce the interference of gradient updates from the
new tasks on the old tasks. Zhu et al. [57] decomposed the
learned features during CL using PCA. They showed that
feature directions with larger eigenvalues have larger sim-
ilarities (corresponding angles) before and after learning a
task. They proposed that these feature directions are more
transferable and less forgettable. They showed that their
dual augmentation method can encourage learned features
to have more directions with large eigenvalues. GeoDL [47]
constructs low-dimensional manifolds for the features ex-
tracted by the online model and the old checkpoints and per-
forms knowledge distillation on the manifolds. PCA com-
putation is explicitly conducted on the learned features for
the manifold construction. SPACE [45] used PCA analysis
for network compression and pruning in continual learning.
Similar to our analysis, they use PCA to split the learned fil-
ters in a network into Core, which is important for the cur-
rent task, and Residual, which can be compressed and freed
up to learn future tasks. In their work, PCA computation is
required during continual learning on every layer of the net-work in order to do pruning, This poses a significant compu-
tational overhead in CL compared to our BFP. Instead of ap-
plying PCA analysis in continual learning, Zhang et al. [55]
designed a modified PCA algorithm based on EWC [31] so
that it has continual learning ability. They aim to reduce the
forgetting problem in monitoring multimode processes.
Î³ 0.1 0.3 1.0 3.0 10.0
FAA 74.56 75.77 76.68 76.00 73.54
FF 16.11 14.63 13.16 13.07 12.69
ALA 87.45 87.32 87.21 86.45 83.70
Table 6. Results on CIFAR10 (buffer size 500) with different Î³.
1% 10% 100%
Portion of training data used for linear probing0.250.300.350.400.450.500.550.60Linear Probing AccuracyDER++ w/ BFP
DER++ w/ FD
DER++
FT w/ BFP
FT w/ FD
FT
1% 10% 100%
Portion of training data used for linear probing0.150.200.250.300.350.400.45Linear Probing AccuracyDER++ w/ BFP
DER++ w/ FD
DER++
FT w/ BFP
FT w/ FD
FT
Figure 5. Linear probing accuracies on the fixed feature extrac-
tor obtained after training on Split-CIFAR100 (top) and TinyIma-
geNet (bottom). DER++ and its variants use a buffer size of 500
for CIFAR100 and 4000 for TinyImageNet.
14

--- PAGE 15 ---
2 3 4 5 6 7 8 9 10
Task0.30.40.50.60.70.80.9CKA SimilaritySeen - DER++ w/ BFP
Seen - DER++ w/ FD
Seen - DER++
Unseen - DER++ w/ BFP
Unseen - DER++ w/ FD
Unseen - DER++
2 3 4 5 6 7 8 9 10
Task0.40.50.60.70.80.9CKA SimilaritySeen - DER++ w/ BFP
Seen - DER++ w/ FD
Seen - DER++
Unseen - DER++ w/ BFP
Unseen - DER++ w/ FD
Unseen - DER++Figure 6. Feature similarity at different tasks of training on Split-
CIFAR100 with buffer size 500 (top) and Split-TinyImageNet with
buffer size 4000 (bottom), using different CL methods.
15
