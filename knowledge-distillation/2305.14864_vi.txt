Lưu ý rằng xu hướng này tương phản với những gì pretraining với fine-grained pruning quan sát trong Sheared Llama (Xia et al., 2023).

Hình 2: Hiệu suất language modeling và task accuracy trung bình được vẽ theo ngân sách token cho LayerCHOP so với baseline pretraining. Chúng tôi quan sát rằng đối với các mô hình 300M và 1.1B nhỏ hơn, pruning và finetuning của các mô hình hội tụ về baseline pretraining. Tuy nhiên, xu hướng này không rõ ràng ở quy mô 7B.

nén 2:4 LayerCHOP
Uncompressed latency 312ms 351ms
Compressed latency 251ms 191ms
Speed 1.24x 1.84x
Breakeven query 240M 360M

Bảng 3: So sánh giữa tốc độ suy luận end-to-end của các mô hình 2:4 semi-structured pruned và LayerCHOP. Các số cho 2:4 pruning được lấy từ Wanda (Sun et al., 2023). LayerCHOP, một dạng structured pruning, nhanh hơn 1.84x so với mô hình cơ sở, so với cải thiện tốc độ 1.24x của mô hình 2:4 pruned. Chi phí huấn luyện bổ sung của LayerCHOP có thể được hấp thụ bởi lợi ích tốc độ suy luận trong số truy vấn breakeven được đưa ra cho mỗi loại pruning.

Hình 3: So sánh task accuracy trung bình và language modeling giữa LayerCHOP và DimCHOP trên ngân sách tính toán cố định. Chúng tôi chứng minh rằng LayerCHOP hiệu quả hơn 2x trong quá trình nén so với DimCHOP bằng cách pruning tất cả layers ngay lập tức (Phụ lục C), trong khi DimCHOP chọn phương pháp pruning lặp đi lặp lại hơn.

5.3 So sánh LayerCHOP và DimCHOP

Trong phần này, chúng tôi so sánh kết quả của LayerCHOP và DimCHOP về hiệu suất và hiệu quả. Hình 3 cho thấy cách cả hai phương pháp hoạt động trong cùng một phạm vi về hiệu suất nhiệm vụ trung bình và language modeling sau khi tiêu thụ cùng số lượng pretraining tokens; tuy nhiên, sự khác biệt thuật toán trong hai chiến lược pruning làm cho LayerCHOP hiệu quả hơn nhiều để huấn luyện so với DimCHOP. LayerCHOP ngay lập tức prune một nửa số lượng layers từ mô hình và tiếp tục pretraining chỉ các mô hình kích thước một nửa. Đồng thời, DimCHOP là một thuật toán lặp đi lặp lại prune dimensions sau mỗi vài steps. Tuy nhiên, nó tiếp tục huấn luyện các dimensions được zeroed-out trước khi đánh giá lại chúng cho pruning dựa trên importance scores (Phần 3.2).

5.4 Hiệu quả của distillation với corpus pretraining lớn

Hình 4 so sánh kết quả hiệu suất nhiệm vụ trung bình và language modeling perplexity cho LayerCHOP và DimCHOP so với các phiên bản được tăng cường distillation của các thuật toán được mô tả trong Phần 3.3. Homotopic distillation (Liang et al., 2023) thấy thiết lập này hiệu quả và có hiệu suất cao cho nén task-agnostic của các mô hình BERT-style; tuy nhiên, trong chế độ dữ liệu lớn của chúng tôi với decoder-only LLM lớn hơn, chúng tôi thấy rằng việc tăng cường pruning với distillation losses không chỉ kém hiệu suất trên điểm số nhiệm vụ trung bình, khi được đánh giá theo cách zero-shot mà còn trên language modeling.

Với teacher và student model lớn hơn, việc tăng cường thiết lập pruning distillation losses trong các phần khác nhau của kiến trúc phát sinh penalty hiệu quả nặng nề, như rõ ràng từ Hình 4. Các phương pháp distillation trước đây cũng trực giao với các paradigm modeling hiệu quả mới hơn như flash-attention, không materialize ma trận attention O(n²) mà chúng tôi có thể áp dụng mục tiêu distillation được đề xuất trước đó trong Phần 3.3.

Hình 4: So sánh task accuracy trung bình và language modeling giữa LayerCHOP, DimCHOP và các phiên bản được tăng cường distillation loss của chúng. Trong chế độ dữ liệu lớn, đối với student và teacher models lớn, chúng tôi thấy rằng việc thêm distillation trở nên không hiệu quả với paradigm pretrain-then-finetune của chúng tôi cho việc nén LLM.

6 Thảo luận và Kết luận

Công trình này thảo luận về cách các phương pháp nén LLM hiện có không tuân theo paradigm pretrain-then-finetune. Chúng tôi chỉ ra rằng với việc tiếp tục pretraining bổ sung của các mô hình nén sau pruning, kết hợp với các kỹ thuật pruning đơn giản đến xấu hổ, có thể đánh bại các phương pháp structured pruning phức tạp và cạnh tranh với các phương pháp nén semi-structured với hiệu quả được cải thiện.

Các phương pháp LayerCHOP của chúng tôi thiết lập tiêu chuẩn hiệu suất mới cho việc nén LLM trong khi cải thiện tốc độ suy luận lên 1.84× so với mô hình baseline, so với cải thiện tốc độ end-to-end hạn chế 1.24× từ nén 2:4 semi-structured thông qua kernels accelerator NVIDIA chuyên biệt.

Công trình gần nhất với chúng tôi là Sheared Llama (Xia et al., 2023), nơi họ áp dụng Co-Fi (Xia et al., 2022) như fine-grained pruning và tiếp tục pretraining trên dữ liệu RedPajama. Công trình của họ lựa chọn các thành phần của mô hình 7B (ví dụ: attention heads) để lắp ráp các mô hình nhỏ hơn, ở quy mô 1.3B và 2.7B, sử dụng heuristic pruning huấn luyện thêm trên dữ liệu domain-specific. Công trình của chúng tôi khác biệt bằng cách recovery kết quả nén state-of-the-art bằng tiếp tục pretrain mặc dù pruning các mô hình ở mức granularity thô hơn nhiều. Các phương pháp của chúng tôi cũng không yêu cầu lựa chọn dữ liệu domain-specific, vì chúng tôi tiếp tục pretraining trên cùng distribution dữ liệu pretraining.

Cuối cùng, chúng tôi chứng minh rằng dưới paradigm pretrain-then-finetune mới này cho việc nén LLM, việc tăng cường pruning với distillation losses không cải thiện hiệu suất như trường hợp của các mô hình BERT-style. Chúng tôi cũng xác định cách student-teacher distillation trên corpus pretraining lớn dẫn đến các thuật toán nén không hiệu quả. CHOP cung cấp nén tương thích với các tối ưu hóa modeling đương đại (ví dụ: Flash Attention) để vượt qua distillation với các kỹ thuật đơn giản đến xấu hổ. Chúng tôi hy vọng công trình của chúng tôi đóng góp vào một cuộc trò chuyện mới về cách huấn luyện một mô hình ngôn ngữ lớn nén, và cách học từ các corpus lớn cho các mô hình tốt hơn, nhỏ hơn.

Hạn chế

Trong khi công trình của chúng tôi theo tinh thần giảm kích thước mô hình và cải thiện hiệu quả — chúng tôi yêu cầu tài nguyên tính toán đáng kể cho các thử nghiệm của mình đòi hỏi cả mức sử dụng năng lượng cao và sức mạnh xử lý. Các thử nghiệm như model pretraining và distillation ở quy mô đòi hỏi nhiều ngày thời gian huấn luyện sử dụng 32xA100 GPUs với interconnect băng thông cao. Do đó, việc tái tạo các thử nghiệm của chúng tôi chỉ khả thi một cách hợp lý với tài nguyên GPU tương đương có thể không khả thi đối với một số nhà nghiên cứu.

Ngoài ra, chúng tôi chứng minh các phát hiện của mình so với các phương pháp pretraining và distillation và các lựa chọn thay thế được công bố gần đây trong thiết lập decoder-only của chúng tôi. Chúng tôi áp dụng phương pháp này để báo cáo cách chiến lược nén điển hình nhất có thể được chuyển đến LLM đương đại. Các phát hiện của chúng tôi không chỉ ra distillation là một chiến lược nén hiệu quả tiềm năng cho các mô hình GPT-style trong chế độ dữ liệu lớn, tuy nhiên, công trình của chúng tôi bị hạn chế ở chỗ có thể tồn tại một số chiến lược distillation không điển hình với hiệu suất thậm chí tốt hơn. Chúng tôi khuyến khích công trình và thảo luận trong tương lai về cách các phương pháp này có thể được cải thiện trong vấn đề này.

Tuyên bố Đạo đức

Chúng tôi báo cáo tất cả các thử nghiệm pretraining với corpus C4 được sử dụng rộng rãi. Corpus này đã được tìm thấy chứa các artifacts và biases có hại (Dodge et al., 2021) mà các mô hình của chúng tôi có thể kế thừa, tuy nhiên, việc nghiên cứu hiện tượng này nằm ngoài phạm vi công trình của chúng tôi nhưng có thể thông báo cho nghiên cứu tương lai. Nén mô hình đã được liên kết với việc tăng bias và toxicity trong một mô hình (Hooker et al., 2020) nhưng hiện tại không rõ cách những ảnh hưởng như vậy mở rộng đến setting của chúng tôi; đặc biệt là khi chúng tôi expose student với cùng corpus như teacher. Cần nghiên cứu thêm trong lĩnh vực này để kiểm tra cách nén ảnh hưởng đến biases trong các mô hình ngôn ngữ ngày càng lớn (Solaiman et al., 2023).

Tài liệu tham khảo

Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, và James Hensman. 2024. Slicegpt: Compress large language models by deleting rows and columns. ArXiv, abs/2401.15024.

Jimmy Ba, Jamie Ryan Kiros, và Geoffrey E. Hinton. 2016. Layer normalization. ArXiv, abs/1607.06450.

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, và Yejin Choi. 2019. Piqa: Reasoning about physical commonsense in natural language. Trong AAAI Conference on Artificial Intelligence.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Language models are few-shot learners. Trong Advances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.

Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, và Michael Carbin. 2020a. The lottery ticket hypothesis for pre-trained bert networks. ArXiv, abs/2007.12223.

Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, và Quoc V. Le. 2023. Symbolic discovery of optimization algorithms. ArXiv, abs/2302.06675.

Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, và Jingjing Liu. 2020b. Early-bert: Efficient bert training via early-bird lottery tickets. Trong Annual Meeting of the Association for Computational Linguistics.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. ArXiv, abs/2204.02311.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Trong Advances in Neural Information Processing Systems, volume 35, pages 16344–16359. Curran Associates, Inc.

Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim M. Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetić, Dustin Tran, Thomas Kipf, Mario Lucić, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, và Neil Houlsby. 2023. Scaling vision transformers to 22 billion parameters. ArXiv, abs/2302.05442.

Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. 2022. Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. Trong Advances in Neural Information Processing Systems, volume 35, pages 30318–30332. Curran Associates, Inc.

Jesse Dodge, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, và Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1286–1305, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Angela Fan, Edouard Grave, và Armand Joulin. 2019. Reducing transformer depth on demand with structured dropout. ArXiv, abs/1909.11556.

Jonathan Frankle và Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks. Trong International Conference on Learning Representations.

Elias Frantar và Dan Alistarh. 2023. Sparsegpt: Massive language models can be accurately pruned in one-shot. ArXiv, abs/2301.00774.

Sanchit Gandhi, Patrick von Platen, và Alexander M. Rush. 2023. Distil-whisper: Robust knowledge distillation via large-scale pseudo labelling. ArXiv, abs/2311.00430.

Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, và Andy Zou. 2021. A framework for few-shot language model evaluation.

Sara Hooker, Nyalleng Moorosi, Gregory Clark, Samy Bengio, và Emily Denton. 2020. Characterising bias in compressed models.

Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, và Qun Liu. 2020. Dynabert: Dynamic bert with adaptive width and depth. ArXiv, abs/2004.04037.

J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, và Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. Trong Findings.

Sheng-Chun Kao, Amir Yazdanbakhsh, Suvinay Subramanian, Shivani Agrawal, Utku Evci, và Tushar Krishna. 2022. Training recipe for n: M structured sparsity with decaying pruning mask. ArXiv, abs/2209.07617.

Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, và Hyoung-Kyu Song. 2024. Shortened llama: A simple depth pruning for large language models. ArXiv, abs/2402.02834.

Yoon Kim và Alexander M. Rush. 2016. Sequence-level knowledge distillation. ArXiv, abs/1606.07947.

Jiaoda Li, Ryan Cotterell, và Mrinmaya Sachan. 2021a. Differentiable subset pruning of transformer heads. Transactions of the Association for Computational Linguistics, 9:1442–1459.

Jiaoda Li, Ryan Cotterell, và Mrinmaya Sachan. 2021b. Differentiable subset pruning of transformer heads. Transactions of the Association for Computational Linguistics, 9:1442–1459.

Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin Yin, và Tuo Zhao. 2023. Homodistil: Homotopic task-agnostic distillation of pre-trained transformers. ArXiv, abs/2302.09632.

Ilya Loshchilov và Frank Hutter. 2017. Fixing weight decay regularization in adam. ArXiv, abs/1711.05101.

J. Scott McCarley, Rishav Chakravarti, và Avirup Sil. 2019. Structured pruning of a bert-based question answering model. arXiv: Computation and Language.

Paul Michel, Omer Levy, và Graham Neubig. 2019. Are sixteen heads really better than one? Trong Neural Information Processing Systems.

Todor Mihaylov, Peter Clark, Tushar Khot, và Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. Trong Conference on Empirical Methods in Natural Language Processing.

Subhabrata Mukherjee, Ahmed Hassan Awadallah, và Jianfeng Gao. 2021. Xtremedistiltransformers: Task transfer for task-agnostic distillation. ArXiv, abs/2106.04563.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. 2019. Language models are unsupervised multitask learners.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.

Hassan Sajjad, Fahim Dalvi, Nadir Durrani, và Preslav Nakov. 2020. Poor man's bert: Smaller and faster transformer models. ArXiv, abs/2004.03844.

Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. 2019. Winogrande: An adversarial winograd schema challenge at scale. ArXiv, abs/1907.10641.

Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108.

Victor Sanh, Thomas Wolf, và Alexander M. Rush. 2020. Movement pruning: Adaptive sparsity by fine-tuning. ArXiv, abs/2005.07683.

Noam M. Shazeer. 2020. Glu variants improve transformer. ArXiv, abs/2002.05202.

Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, Hal Daumé III au2, Jesse Dodge, Ellie Evans, Sara Hooker, Yacine Jernite, Alexandra Sasha Luccioni, Alberto Lusoli, Margaret Mitchell, Jessica Newman, Marie-Therese Png, Andrew Strait, và Apostol Vassilev. 2023. Evaluating the social impact of generative ai systems in systems and society.

Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, A. Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnuson, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hanna Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, và Kyle Lo. 2024. Dolma: an open corpus of three trillion tokens for language model pretraining research. ArXiv, abs/2402.00159.

Mingjie Sun, Zhuang Liu, Anna Bair, và J. Zico Kolter. 2023. A simple and effective pruning approach for large language models. ArXiv, abs/2306.11695.

S. Sun, Yu Cheng, Zhe Gan, và Jingjing Liu. 2019. Patient knowledge distillation for bert model compression. Trong Conference on Empirical Methods in Natural Language Processing.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, và Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited devices. ArXiv, abs/2004.02984.

Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, và Jimmy J. Lin. 2019. Distilling task-specific knowledge from bert into simple neural networks. ArXiv, abs/1903.12136.

Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cantón Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288.

Iulia Turc, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models. arXiv: Computation and Language.

Tycho F. A. van der Ouderaa, Markus Nagel, Mart van Baalen, Yuki Markus Asano, và Tijmen Blankevoort. 2023. The llm surgeon. ArXiv, abs/2312.17244.

Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Neural Information Processing Systems.

Elena Voita, David Talbot, F. Moiseev, Rico Sennrich, và Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. ArXiv, abs/1905.09418.

Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, và Furu Wei. 2020a. Minilmv2: Multi-head self-attention relation distillation for compressing pre-trained transformers. Trong Findings.

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, và Ming Zhou. 2020b. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. ArXiv, abs/2002.10957.

Johannes Welbl, Nelson F. Liu, và Matt Gardner. 2017. Crowdsourcing multiple choice science questions. ArXiv, abs/1707.06209.

M. Xia, Zexuan Zhong, và Danqi Chen. 2022. Structured pruning learns compact and accurate models. Trong Annual Meeting of the Association for Computational Linguistics.

Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, và Danqi Chen. 2023. Sheared llama: Accelerating language model pre-training via structured pruning. ArXiv, abs/2310.06694.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? Trong Annual Meeting of the Association for Computational Linguistics.

Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, và Tuo Zhao. 2022. PLATON: Pruning large transformer models with upper confidence bound of weight importance. Trong Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 26809–26823. PMLR.

# Params Dim Heads Layers Batch Size LR Token Budget
180M 1024 16 12 2M 6.0e-4 160B
300M 1024 16 24 2M 3.0e-4 160B
610M 2048 16 12 2M 2.5e-4 160B
1.1B 2048 16 24 2M 2.0e-4 160B
3.5B 4096 32 16 4M 3.0e-4 160B
7B 4086 32 32 4M 3.0e-4 2T

Bảng 4: Cấu hình cho các mô hình được sử dụng trong các thử nghiệm của chúng tôi ở quy mô khác nhau. Mô hình 7B được sử dụng là llama2-7B đã pretrained.

A Kiến trúc Mô hình

Chúng tôi liệt kê chi tiết về các mô hình decoder-only của chúng tôi trong Bảng 4. Đối với kích thước 7B, chúng tôi prune một llama2-7B đã pretrained (Touvron et al., 2023). Đối với các kích thước mô hình nhỏ hơn, chúng tôi huấn luyện một mô hình kiến trúc tùy chỉnh trên 160B C4 tokens.

Đối với mô hình tùy chỉnh, chúng tôi tuân theo kiến trúc PaLM (Chowdhery et al., 2022) nhờ hiệu quả thông lượng được cải thiện. Cụ thể, các modules attention và feed-forward network (FFN) song song thay vì tuần tự (Radford et al., 2019). SwiGLU activation (Shazeer, 2020) được sử dụng trong module FFN. Multi-head attention sử dụng triển khai Flash-Attention tương đương (Dao et al., 2022, FA). Layer đầu tiên của module FFN và các layers tạo attention query, key, và value được fusion. Tương tự, layer thứ hai của module FFN và feed-forward layer sau hoạt động attention được fusion. LayerNorm (Ba et al., 2016) nằm trước layer feed-forward fusion đầu tiên. Các vector query và key được truyền qua các layer normalization bổ sung để tăng ổn định huấn luyện theo Dehghani et al. (2023). Cấu trúc khối này được lặp lại với skip connections để tạo thành kiến trúc Transformer decoder-only của chúng tôi.

B Thiết lập đánh giá

Chúng tôi chi tiết các nhiệm vụ được sử dụng trong bộ đánh giá zero-shot của chúng tôi trong Bảng 5. Mỗi nhiệm vụ trong bảng báo cáo accuracy phân loại hoặc length normalized classification accuracy. Bộ đánh giá của chúng tôi, trực tuyến (chạy như một vòng lặp validation sau một số training steps), được điều chỉnh để khớp với kết quả từ Eleuther AI eval harness (Gao et al., 2021).

C LayerCHOP

Các mô hình baseline của chúng tôi chứa 24 decoder layers. Để xác định layers nào chúng tôi nên prune để có hiệu suất nén tốt nhất, chúng tôi định nghĩa năm cấu hình layer pruning được hiển thị trong Hình 5, mỗi cấu hình loại bỏ 12 trong số 24 decoder layers của các mô hình 300M và 1.1B. Trong tất cả các cấu hình pruning này, chúng tôi luôn giữ layers đầu tiên và cuối cùng vì chúng tương tác với embedding table. Chúng tôi đưa ra quyết định thiết kế này dựa trên các thử nghiệm sớm. Bảng 6 tóm tắt kết quả của ablation này. Chúng tôi báo cáo điểm perplexity trên tập validation C4 và task accuracy trung bình qua 6 nhiệm vụ trong bộ đánh giá của chúng tôi (Bảng 5).

Đối với mô hình 300M cơ sở, các cấu hình pruning max-gap và both hoạt động tốt nhất trong số năm cấu hình có thể. Đối với mô hình 1.1B, pruning layers từ cấu hình input mang lại kết quả tốt nhất cho cả hai metrics được báo cáo. Cấu hình pruning output dẫn đến hiệu suất tệ nhất qua các kích thước mô hình, cho thấy rằng pruning layers về phía output của mô hình nên được tránh. Với những kết quả này, chúng tôi sử dụng cấu hình pruning max-gap cho tất cả các thử nghiệm mô hình 300M của chúng tôi và cấu hình input cho tất cả các thử nghiệm mô hình 1.1B của chúng tôi, và sử dụng cấu hình từ các thử nghiệm mô hình 1.1B cho các thử nghiệm pruning llama2-7B của chúng tôi.

Chúng tôi có thể quyết định khi nào prune layers trong khi huấn luyện các mô hình với LM loss theo một trong hai cách: hoặc loại bỏ các layers đã chọn cùng một lúc hoặc loại bỏ từng cái một, mỗi cái sau một số cố định các training tokens. Chúng tôi chạy thử nghiệm này trong bốn cấu hình để xem liệu việc tăng khoảng cách giữa mỗi layer pruning có tăng ổn định huấn luyện hoặc hiệu suất mô hình hay không. Bốn cấu hình là: dropping tất cả layers cùng một lúc (0M token gap giữa pruning mỗi layer) hoặc pruning chúng sau 100M, 500M, và 1B training tokens mỗi cái. Chúng tôi chạy thử nghiệm này cho kích thước mô hình 300M và 1.1B. Chúng tôi prune 12/24 layers từ các mô hình decoder-only của chúng tôi cho ablation này, mỗi cái tại một training token gap được đề cập trong một trong bốn cấu hình trên. Kết quả của thử nghiệm này được tóm tắt trong Hình 6.

category task metric
common sense reasoning PIQA (Bisk et al., 2019) len norm acc
Hellaswag (Zellers et al., 2019) len norm acc
Winogrande (Sakaguchi et al., 2019) acc
science question answering OpenBookQA (Mihaylov et al., 2018) len norm acc
SciQ (Welbl et al., 2017) acc
Arc-Easy (Clark et al., 2018) acc

Bảng 5: Các nhiệm vụ downstream zero-shot để đánh giá các mô hình nén và baseline của chúng tôi. Mỗi nhiệm vụ hoặc báo cáo classification accuracy hoặc length normalized classification accuracy.

Hình 5: Cấu hình khởi tạo cắt ngắn cho layer pruning trong mô hình ngôn ngữ decoder-only. Các layers được làm nổi bật (xanh lá) được loại bỏ. Chúng tôi giữ lại layer đầu tiên và cuối cùng vì những layers này tương tác với embedding table.

Model Token Budget Task Metric max-gap input output middle both Pre-compression
300M-160B 20B ppl (↓) 23.0 24.5 25.6 24.0 23.0 16.2
300M-160B 20B avg acc (↑) 53.2 52.9 51.6 52.9 53.2 55.8
1.1B-160B 20B ppl (↓) 18.1 17.3 22.0 18.6 18.6 13.0
1.1B-160B 20B avg acc (↑) 54.8 55.1 53.1 54.7 53.8 59.8

Bảng 6: Ảnh hưởng lên hiệu suất nhiệm vụ trung bình và C4 validation perplexity của các chiến lược khởi tạo cắt ngắn khác nhau từ Hình 5 cho các mô hình kích thước 300M và 1.1B. Điểm hiệu suất nhiệm vụ trung bình là qua 6 nhiệm vụ được liệt kê trong Bảng 5, và số cao hơn là tốt hơn. Đối với điểm perplexity, thấp hơn là tốt hơn.

Pruning layers từng cái một với ngân sách token tăng dần giữa mỗi layer pruning không có lợi cho task accuracy trung bình hoặc C4 validation perplexity. Thực tế, có một sự ưu tiên biên để prune layers càng sớm vào quá trình huấn luyện càng tốt. Do đó, chúng tôi quyết định prune tất cả 12/24 layers đồng thời cho các thử nghiệm khác.

Hình 6: Task accuracy trung bình (Bảng 5) và perplexity trên tập validation C4 cho kích thước mô hình 300M và 1.1B so sánh các lịch trình cho khi nào prune layers trong quá trình tiếp tục pretraining. Chúng tôi thấy sự giảm hiệu suất biên khi chúng tôi loại bỏ layers từng cái một cách xa hơn trong quá trình tiếp tục pretraining.
