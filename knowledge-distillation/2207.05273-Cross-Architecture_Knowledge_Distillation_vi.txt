# Chuyển giao Kiến thức Đa Kiến trúc
Yufan Liu1;2[0000000284269335], Jiajiong Cao5, Bing Li1;4?, Weiming Hu1;2;3,
Jingting Ding5, và Liang Li5
1Phòng thí nghiệm Quốc gia Nhận dạng Mẫu, Viện Tự động hóa, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
2Trường Trí tuệ Nhân tạo, Đại học Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
3Trung tâm Xuất sắc CAS về Khoa học Não bộ và Công nghệ Thông minh, Bắc Kinh, Trung Quốc
4PeopleAI, Inc., Bắc Kinh, Trung Quốc
5Tập đoàn Dịch vụ Tài chính Ant, Bắc Kinh, Trung Quốc
bli@nlpr.ia.ac.cn

Tóm tắt. Transformer thu hút nhiều sự chú ý vì khả năng học các mối quan hệ toàn cục và hiệu suất vượt trội. Để đạt được hiệu suất cao hơn, việc chưng cất kiến thức bổ sung từ Transformer sang mạng nơ-ron tích chập (CNN) là điều tự nhiên. Tuy nhiên, hầu hết các phương pháp chưng cất kiến thức hiện có chỉ xem xét chưng cất kiến trúc đồng nhất, chẳng hạn như chưng cất kiến thức từ CNN sang CNN. Chúng có thể không phù hợp khi áp dụng cho các tình huống đa kiến trúc, chẳng hạn như từ Transformer sang CNN. Để giải quyết vấn đề này, một phương pháp chưng cất kiến thức đa kiến trúc mới được đề xuất. Cụ thể, thay vì trực tiếp bắt chước đầu ra/các đặc trưng trung gian của giáo viên, projector attention chéo một phần và projector tuyến tính theo nhóm được giới thiệu để căn chỉnh các đặc trưng học sinh với đặc trưng của giáo viên trong hai không gian đặc trưng được chiếu. Và một sơ đồ huấn luyện bền vững đa góc nhìn được trình bày thêm để cải thiện tính bền vững và ổn định của khung. Các thí nghiệm mở rộng cho thấy phương pháp đề xuất vượt trội hơn 14 phương pháp hiện đại trên cả các tập dữ liệu quy mô nhỏ và quy mô lớn.

Từ khóa: Chưng cất kiến thức · Đa kiến trúc · Nén mô hình.

1 Giới thiệu

Chưng cất kiến thức (KD) đã trở thành một chủ đề cơ bản để nâng cao hiệu suất mô hình. Nó đã được áp dụng thành công vào các ứng dụng khác nhau bao gồm nén mô hình [1] và chuyển giao kiến thức [2]. KD thường áp dụng khung giáo viên-học sinh, trong đó mô hình học sinh được huấn luyện dưới sự hướng dẫn của kiến thức của giáo viên. Kiến thức thường được định nghĩa bởi các đầu ra mềm hoặc các đặc trưng trung gian của mô hình giáo viên.

Các phương pháp KD hiện có tập trung vào mạng nơ-ron tích chập (CNN). Tuy nhiên, gần đây xuất hiện nhiều mạng mới như Transformer. Nó cho thấy ưu thế trên các nhiệm vụ thị giác máy tính khác nhau bao gồm phân loại hình ảnh [3] và phát hiện [4], trong khi việc tính toán khổng lồ và sự hỗ trợ gia tốc nền tảng hạn chế làm hạn chế việc ứng dụng Transformer, đặc biệt là cho các thiết bị biên. Mặt khác, với một số năm phát triển, có đủ thư viện gia tốc bao gồm CUDA [5], TensorRT [6] và NCNN [7], làm cho CNN thân thiện với phần cứng trên cả máy chủ và thiết bị biên. Vì mục đích này, việc chưng cất kiến thức từ Transformer hiệu suất cao sang CNN nhỏ gọn là một ý tưởng tự nhiên. Tuy nhiên, có một khoảng cách lớn giữa hai kiến trúc. Như được hiển thị trong Hình 1-(a), Transformer bao gồm các khối transformer dựa trên self-attention trong khi CNN chứa một chuỗi các khối tích chập. Hơn nữa, các đặc trưng được sắp xếp theo cách hoàn toàn khác nhau. Các đầu ra trung gian của CNN được hình thành với c kênh của bản đồ đặc trưng h′w′. Khác với CNN, các đặc trưng của Transformer bao gồm N vector đặc trưng với 3hw phần tử, trong đó N đề cập đến số lượng patch.

Thật không may, các phương pháp hiện có tập trung vào KD kiến trúc đồng nhất như CNN→CNN và Transformer→Transformer, không phù hợp cho các tình huống đa kiến trúc. Như được hiển thị trong Hình 1-(b), "khả năng chuyển giao" kiến thức được định nghĩa định lượng. Cụ thể, đặc trưng đầu ra của học sinh được căn chỉnh với không gian đặc trưng của giáo viên, và sau đó, độ tương tự cosine của vector đặc trưng học sinh đã căn chỉnh và vector đặc trưng giáo viên được tính toán. Đối với các trường hợp kiến trúc đồng nhất, khả năng chuyển giao nằm trong khoảng 0.6∼0.7, trong khi nó thấp hơn nhiều, thường thấp hơn 0.55, trong điều kiện đa kiến trúc. Do đó, việc chưng cất kiến thức qua các kiến trúc khác nhau khó khăn hơn và một khung KD mới cần được thiết kế để giải quyết vấn đề này.

Trong công trình này, một phương pháp chưng cất kiến thức đa kiến trúc mới được đề xuất để bắc cầu khoảng cách lớn giữa Transformer và CNN. Với sự trợ giúp của khung được đề xuất, kiến thức từ Transformer được chuyển giao hiệu quả đến mạng CNN học sinh và khả năng chuyển giao kiến thức được cải thiện đáng kể thông qua phương pháp này. Nó khuyến khích học sinh học cả các đặc trưng không gian cục bộ (với mô hình CNN gốc) và các đặc trưng toàn cục bổ sung (từ mô hình giáo viên transformer). Cụ thể, hai projector bao gồm projector attention chéo một phần (PCA) và projector tuyến tính theo nhóm (GL), được thiết kế. Thay vì trực tiếp bắt chước đầu ra của giáo viên, hai projector này căn chỉnh đặc trưng học sinh trung gian vào hai không gian đặc trưng khác nhau và chưng cất kiến thức được thực hiện thêm trong hai không gian đặc trưng. Projector PCA ánh xạ đặc trưng học sinh vào không gian attention Transformer của giáo viên. Projector này khuyến khích học sinh học mối quan hệ toàn cục từ giáo viên Transformer. Projector GL ánh xạ đặc trưng học sinh vào không gian đặc trưng Transformer theo cách pixel-by-pixel. Projector này trực tiếp giảm bớt sự khác biệt trong hình thành đặc trưng giữa giáo viên và học sinh. Ngoài ra, để giảm bớt sự bất ổn định do tính đa dạng trong khung đa kiến trúc, chúng tôi đề xuất một sơ đồ huấn luyện bền vững đa góc nhìn. Các mẫu đa góc nhìn được tạo ra để gây nhiễu mạng học sinh. Và một discriminator đối kháng đa góc nhìn được xây dựng để phân biệt các đặc trưng giáo viên và các đặc trưng học sinh bị nhiễu, trong khi học sinh được huấn luyện để gây nhầm lẫn cho discriminator. Sau khi hội tụ, học sinh có thể bền vững và ổn định hơn.

Các thí nghiệm mở rộng được tiến hành trên cả tập dữ liệu quy mô lớn và quy mô nhỏ, bao gồm ImageNet [8] và CIFAR [9]. Kết quả thí nghiệm của các cặp giáo viên-học sinh khác nhau chứng minh rằng phương pháp đề xuất hoạt động ổn định tốt hơn 14 phương pháp hiện đại. Tóm lại, những đóng góp chính của công trình chúng tôi gồm ba khía cạnh:

–Chúng tôi đề xuất một khung chưng cất kiến thức đa kiến trúc để chưng cất kiến thức Transformer xuất sắc nhằm hướng dẫn CNN. Trong khung này, projector attention chéo một phần (PCA) và projector tuyến tính theo nhóm (GL) được thiết kế để căn chỉnh không gian đặc trưng học sinh và thúc đẩy khả năng chuyển giao giữa các đặc trưng giáo viên và đặc trưng học sinh.

–Chúng tôi đề xuất một sơ đồ huấn luyện bền vững đa góc nhìn để cải thiện tính ổn định và bền vững của mạng học sinh.

–Kết quả thí nghiệm cho thấy phương pháp đề xuất hiệu quả và vượt trội hơn 14 phương pháp hiện đại trên cả tập dữ liệu quy mô lớn và quy mô nhỏ.

2 Công trình Liên quan

Hinton et al. [10] đề xuất khái niệm chưng cất kiến thức, sử dụng đầu ra mềm của giáo viên để hướng dẫn việc học của học sinh. Gần đây, nó được áp dụng chủ yếu vào nén mô hình [1] và chuyển giao kiến thức [2]. Các hình thức khác nhau của kiến thức được chưng cất được khám phá để hướng dẫn mạng học sinh tốt hơn, bao gồm đầu ra cuối cùng [10, 11] và kiến thức lớp gợi ý [12–19]. Đối với kiến thức lớp gợi ý, nhiều nỗ lực đã được thực hiện để khớp các lớp gợi ý học sinh và các lớp được hướng dẫn bởi giáo viên. Ví dụ, AT [12] định nghĩa bản đồ attention kênh đơn làm kiến thức. Tuy nhiên, việc tính toán các bản đồ attention gây ra mất thông tin chiều kênh. FitNet [13] trực tiếp chưng cất các đặc trưng từ các lớp trung gian mà không mất thông tin. Tuy nhiên, hạn chế này hơi khắt khe và không phải tất cả thông tin đều có lợi. Liu et al. [17] chưng cất kiến thức được gọi là đồ thị mối quan hệ thể hiện (IRG), chứa đặc trưng thể hiện, mối quan hệ đặc trưng thể hiện và biến đổi không gian đặc trưng. Nó không bị giới hạn bởi sự không khớp chiều giữa giáo viên và học sinh.

Các phương pháp trên đều tập trung vào mạng nơ-ron tích chập (CNN). Gần đây, Transformer ngày càng trở nên phổ biến vì hiệu suất ấn tượng. Tuy nhiên, do kiến trúc hoàn toàn khác nhau, nhiều phương pháp KD trước đây không thể được áp dụng trực tiếp cho Transformers. Có một số công trình [20–22] nghiên cứu chưng cất kiến thức giữa các Transformers. DeiT [20] đề xuất một token chưng cất tương tự như token lớp, để làm cho Transformer học sinh học nhãn cứng từ giáo viên và ground truth (GT). MINILM [21] tập trung vào các cơ chế attention trong Transformer và chưng cất thông tin self-attention tương ứng. IR [22] chưng cất các biểu diễn nội bộ (ví dụ, bản đồ self-attention) từ Transformer giáo viên sang Transformer học sinh.

Tóm lại, các phương pháp hiện có thường trình bày một phép biến đổi để khớp các đặc trưng của giáo viên và các đặc trưng của học sinh. Tuy nhiên, gần như tất cả chúng đều yêu cầu kiến trúc tương tự hoặc thậm chí giống nhau giữa giáo viên và học sinh. Để giải quyết vấn đề chưng cất kiến thức đa kiến trúc, chúng tôi thiết kế cẩn thận các projector để khớp giáo viên và học sinh trong cùng một không gian đặc trưng. Do đó, một mô hình CNN học sinh nhỏ gọn có thể học tốt đặc trưng toàn cục từ mô hình Transformer giáo viên bất chấp khoảng cách lớn trong các kiến trúc.

3 Phương pháp Đề xuất

Trong phần này, khung của phương pháp đề xuất được giới thiệu đầu tiên. Sau đó, hai thành phần chính của khung bao gồm các projector đa kiến trúc và một sơ đồ huấn luyện bền vững đa góc nhìn được trình bày. Thành phần đầu được xây dựng để giảm bớt sự không khớp đặc trưng cho các tình huống đa kiến trúc và giúp học sinh học mối quan hệ toàn cục của các đặc trưng, trong khi thành phần sau được áp dụng để cải thiện tính bền vững và ổn định của học sinh. Cuối cùng, hàm mất mát và quy trình huấn luyện được mô tả.

3.1 Khung

Khung tổng thể của phương pháp đề xuất được mô tả trong Hình 2. Trong hình này, mạng màu hồng phía trên đại diện cho mạng giáo viên, trong khi mạng màu xanh phía dưới là mạng học sinh. Đối với transformer giáo viên T, mẫu đầu vào x∈R^(3×H×W) được chia thành (N=HW/hw) patches {x_n∈R^(3×h×w)}^N_(n=1). Sau khi suy luận của một số khối transformer, đặc trưng h_T∈R^(N×(3hw)) được tạo ra. Và khả năng dự đoán cuối cùng sau đó được tính toán thông qua một đầu perceptron đa lớp (MLP) như được hiển thị trong Hình 2. Đối với CNN học sinh S, nó nhận toàn bộ hình ảnh mà không có phân vùng theo patch làm đầu vào. Tương tự, sau khi suy luận của một số khối CNN, đặc trưng học sinh cuối cùng h_S∈R^(c×(h′×w′)) có thể được thu được. Lưu ý rằng c là số kênh và h′w′=HW/2^(2s). s biểu thị số giai đoạn CNN (thường bằng 4). Sau đó nó được sử dụng để dự đoán lớp.

Do sự khác biệt của các nguyên tắc thiết kế và kiến trúc giữa transformers và CNNs, việc làm cho các đặc trưng học sinh trực tiếp bắt chước các đặc trưng giáo viên sử dụng các phương pháp KD hiện có là khó khăn. Để giải quyết vấn đề này, chúng tôi đề xuất một projector đa kiến trúc bao gồm một projector attention chéo một phần (PCA) và một projector tuyến tính theo nhóm (GL). Projector PCA ánh xạ các đặc trưng học sinh vào không gian attention transformer. Bằng cách ánh xạ không gian đặc trưng CNN vào không gian attention này, học sinh dễ dàng học mối quan hệ toàn cục giữa các vùng khác nhau bằng cách tối thiểu hóa khoảng cách giữa các bản đồ attention học sinh và các bản đồ attention giáo viên. Projector GL ánh xạ các đặc trưng học sinh vào không gian đặc trưng transformer. Trong không gian đặc trưng transformer này, học sinh được hướng dẫn để bắt chước các đặc trưng transformer toàn cục theo cách pixel-by-pixel.

Để cải thiện tính bền vững và ổn định của học sinh, một sơ đồ huấn luyện bền vững đa góc nhìn được đề xuất. Các mẫu đa góc nhìn được tạo ra bởi một generator đa góc nhìn ngẫu nhiên thực hiện một số phép biến đổi và tạo ra mặt nạ và nhiễu thêm vào các đầu vào. Được cung cấp với các đầu vào đa góc nhìn, học sinh tạo ra các đặc trưng khác nhau. Một discriminator đối kháng đa góc nhìn được xây dựng để phân biệt các đặc trưng giáo viên và các đặc trưng học sinh trong không gian đặc trưng transformer. Sau đó mục tiêu là làm rối discriminator.

Cuối cùng, chúng tôi tích hợp các mất mát đề xuất và đưa ra huấn luyện end-to-end để có được một mạng học sinh mạnh mẽ.

3.2 Projector đa kiến trúc

(1) Projector attention chéo một phần Projector attention chéo một phần (PCA) ánh xạ không gian đặc trưng học sinh vào không gian attention transformer. Nó được thiết kế để ánh xạ các đặc trưng CNN thành ma trận Query, Key, Value và sau đó bắt chước cơ chế attention. Nó bao gồm ba lớp tích chập 3×3:

{Q_S, K_S, V_S} = Proj_1(h_S), (1)

trong đó các ma trận Q_S, K_S, V_S được tính toán và căn chỉnh để bắt chước query Q_T, key K_T và value V_T của Transformer giáo viên. Trong không gian attention transformer, self-attention của học sinh được tính như:

Attn_S = softmax(Q_S(K_S)^T/√d)V_S, (2)

trong đó d là kích thước query. Việc tính toán Attn_T tương tự. Do đó, chúng ta có thể tối thiểu hóa khoảng cách giữa các bản đồ attention của giáo viên và những bản đồ của học sinh để hướng dẫn mạng học sinh. Để cải thiện thêm tính bền vững của học sinh, chúng tôi xây dựng attention chéo một phần của học sinh để thay thế Attn_S gốc:

PCAttn_S = softmax(g(Q_S)(g(K_S))^T/√d)g(V_S),

s.t. g(M_S(i,j)) = {
    M_T(i,j), nếu p ≥ 0.5
    M_S(i,j), nếu p < 0.5
}, (M = Q, K, V). (3)

Lưu ý rằng (i,j) biểu thị chỉ số phần tử ma trận của M. Hàm g() thay thế các ma trận Q_S, K_S, V_S của học sinh bằng các ma trận tương ứng của giáo viên, với xác suất p tuân theo phân phối đều. Theo cách này, mất mát được xây dựng:

L_proj1 = ||Attn_T - PCAttn_S||_2^2 + ||V_T V_T^T/√d - V_S V_S^T/√d||_2^2, (4)

để làm cho học sinh bắt chước giáo viên trong không gian attention.

(2) Projector tuyến tính theo nhóm Projector tuyến tính theo nhóm (GL) ánh xạ đặc trưng học sinh vào không gian đặc trưng transformer. Nó bao gồm một số lớp kết nối đầy đủ (FC) có trọng số chia sẻ:

h'_S = Proj_2(h_S), (5)

trong đó h'_S∈R^(N×(3hw)) được căn chỉnh để có cùng chiều với đặc trưng giáo viên h_T. Cụ thể, đối với đầu vào hình ảnh thông thường có kích thước 224×224, các chiều là h_S∈R^(256×196) và h'_S∈R^(196×768). Để thực hiện cách ánh xạ pixel-by-pixel, projector cần ít nhất 196 lớp FC với 256×768 tham số mỗi lớp, mỗi lớp ánh xạ pixel từ không gian đặc trưng gốc sang "pixel" tương ứng trong không gian transformer. Một số lượng lớn lớp FC có thể gây ra tính toán khổng lồ. Để có được một projector nhỏ gọn, chúng tôi đề xuất projector tuyến tính theo nhóm trong đó một vùng lân cận 4×4 chia sẻ một lớp FC. Do đó, projector GL chỉ chứa 16 lớp FC. Hơn nữa, drop-out cũng được áp dụng để giảm tính toán và cải thiện tính bền vững. Cuối cùng, sau khi thu được đặc trưng học sinh căn chỉnh mới, mất mát được tính như:

L_proj2 = ||h_T - h'_S||_2^2, (6)

để tối thiểu hóa khoảng cách giữa đặc trưng giáo viên và đặc trưng học sinh trong không gian đặc trưng transformer.

3.3 Huấn luyện bền vững đa góc nhìn

Do sự khác biệt lớn giữa các kiến trúc của giáo viên và học sinh, việc học sinh học để trở nên bền vững không dễ dàng. Để cải thiện tính bền vững và ổn định của mạng học sinh, chúng tôi đề xuất một sơ đồ huấn luyện bền vững đa góc nhìn. Sơ đồ huấn luyện đề xuất chứa hai thành phần quan trọng, tức là một generator đa góc nhìn (MVG) và discriminator đối kháng đa góc nhìn tương ứng. MVG lấy hình ảnh gốc làm đầu vào, và tạo ra hình ảnh với các phép biến đổi khác nhau với một xác suất nhất định:

x̃ = MVG(x) = {
    Trans(x), nếu p ≥ 0.5
    x, nếu p < 0.5
}, (7)

trong đó Trans() chứa các phép biến đổi phổ biến, chẳng hạn như color jittering, cắt ngẫu nhiên, xoay, mặt nạ theo patch, v.v. Xác suất p cũng tuân theo phân phối đều. Các phiên bản được biến đổi của các mẫu này sau đó được đưa vào mạng học sinh. Tiếp theo, discriminator đối kháng đa góc nhìn được xây dựng để phân biệt đặc trưng giáo viên h_T và đặc trưng học sinh được biến đổi h'_S, được cấu thành từ một mạng ba lớp FC. Theo cách này, mục tiêu của huấn luyện bền vững đa góc nhìn là làm rối discriminator và có được một đặc trưng học sinh bền vững. Mất mát huấn luyện của discriminator được tính như:

L_MAD = (1/m)∑_{k=1}^m [log D(h_T^(k)) - log(1 - D(h_S'^(k)))]. (8)

Lưu ý rằng D() biểu thị discriminator đối kháng đa góc nhìn. Và m là tổng số mẫu huấn luyện. Đối với mạng học sinh có thể được xem như generator trong huấn luyện đối kháng, mất mát được viết như:

L_MVG = (1/m)∑_{k=1}^m [log(1 - D(h_S'^(k)))]. (9)

Tối thiểu hóa mất mát này có thể giúp tạo ra đặc trưng học sinh h'_S phân phối tương tự như đặc trưng giáo viên h_T.

3.4 Tối ưu hóa

Trong phần này, chúng tôi giới thiệu tối ưu hóa tổng thể và quy trình huấn luyện của phương pháp đề xuất. Để huấn luyện mạng học sinh, hàm mất mát có thể được thu được bằng:

L_total = α(L_proj1 + L_proj2) + L_MVG, (10)

trong đó α là hệ số phạt cân bằng các số hạng mất mát. Đối với discriminator đối kháng đa góc nhìn, hàm mất mát là L_MAD trong Phương trình (8).

Quy trình huấn luyện tổng thể của phương pháp đề xuất được tóm tắt trong Thuật toán 1. Chi tiết, khung giáo viên-học sinh đa kiến trúc được xây dựng đầu tiên. Projector PCA và projector GL sau đó được nhúng trong mạng học sinh để ánh xạ các đặc trưng học sinh vào không gian attention giáo viên và không gian đặc trưng. Tiếp theo, một sơ đồ huấn luyện bền vững đa góc nhìn được áp dụng để huấn luyện khung. Thân chính của khung (tức là S, Proj_1() và Proj_2()) và discriminator đối kháng đa góc nhìn D() được cập nhật luân phiên. Sau khi hội tụ, các module Proj_1(), Proj_2() và D() được loại bỏ và chỉ mạng học sinh nhỏ gọn S được giữ lại để thực hiện giai đoạn suy luận.

Thuật toán 1: Quy trình chưng cất kiến thức đa kiến trúc.
Đầu vào: Cơ sở dữ liệu D_train = {x_train, y_train}, S, T, D(), Proj_1(), Proj_2().
1 e = 0;
2 Khởi tạo S, Proj_1(), Proj_2() và D();
3 repeat
4   Tính các đặc trưng được biến đổi h'_S và {Q_S, K_S, V_S} thông qua Proj_1() và Proj_2(), sử dụng Phương trình (1) và Phương trình (5);
5   Cập nhật S, Proj_1() và Proj_2() sử dụng Phương trình (10);
6   if e%5 = 0 then
7     Cập nhật D() sử dụng Phương trình (8);
8   end
9   e = e + 1;
10 until done;
11 Loại bỏ Proj_1(), Proj_2() và D(), và dự đoán nhãn thông qua S trong giai đoạn suy luận;
12 return S.

4 Thí nghiệm

4.1 Cài đặt

Cơ sở dữ liệu và Mạng. Chúng tôi đánh giá phương pháp đề xuất trên hai cơ sở dữ liệu: CIFAR [9] và ImageNet [8]. Dữ liệu được tăng cường sử dụng các chiến lược tương tự như trong các ví dụ chính thức PyTorch [23]. Đối với mạng, chúng tôi sử dụng các CNN phổ biến làm mạng học sinh, bao gồm ResNets [24], MobileNet v2 [25], Xception [26] và EfficientNet [27]. Các Transformers điển hình được áp dụng làm mạng giáo viên, chẳng hạn như ViT [3], và Swin Transformer [28].

Chi tiết Triển khai. Chúng tôi huấn luyện tất cả các mạng từ đầu. Đối với tập dữ liệu CIFAR, tổng số epoch là 200 với kích thước batch tiêu chuẩn là 64. Tốc độ học được khởi tạo là 0.1 và nhân với 0.1 tại epoch 100 và epoch 150. Đối với ImageNet, tổng số epoch là 120 với kích thước batch 256. Tốc độ học được khởi tạo là 0.1 và nhân với 0.1 tại epoch 30, epoch 60 và epoch 90, tương ứng. Một bộ tối ưu hóa gradient ngẫu nhiên tiêu chuẩn (SGD) với độ suy giảm trọng số 10^-4 và động lượng 0.9 được áp dụng. Tất cả các thí nghiệm được tiến hành trên một nền tảng với 8 card GPU Nvidia Tesla và CPU Intel(R) Xeon(R) Platinum 8163 96 lõi. Ngoài ra, mỗi cài đặt đơn lẻ được lặp lại 5 lần với các hạt giống ngẫu nhiên khác nhau trên Pytorch.

4.2 So sánh Hiệu suất

Chúng tôi so sánh hiệu suất của phương pháp chúng tôi với 14 phương pháp chưng cất kiến thức hiện đại, bao gồm Logits [10], FitNet [13], AT [12], IRG [17], RKD [29], CRD [30], OFD [14], ReviewKD [31], LONDON [32], AFD [33], AB [34], FT [35], DeiT [20] và MINILM [21]. Trong số đó, Logits, FitNet, AT, IRG, RKD, CRD, OFD, ReviewKD và LONDON là các phương pháp KD dựa trên CNN, và DeiT và MINILM là các phương pháp KD dựa trên transformer. Tồn tại ít công trình liên quan cho khung Transformer-CNN. Do đó, một số phương pháp dựa trên CNN bao gồm logits, RKD và IRG được áp dụng cho các tình huống đa kiến trúc, vì các phương pháp này không dựa vào các kiến trúc CNN. Bên cạnh đó, để so sánh công bằng, chúng tôi chọn CNNs và Transformers với Phép toán Dấu phẩy động (FLOPs) tương tự hoặc độ chính xác tương tự làm mạng giáo viên hoặc mạng học sinh.

Đánh giá trên CIFAR. Bảng 1 trình bày kết quả KD trên CIFAR100. Như được hiển thị trong bảng này, ba chế độ KD của khung giáo viên-học sinh, bao gồm CNN-CNN, Transformer-CNN và Transformer-Transformer, được đánh giá. Có thể thấy rằng phương pháp đề xuất có hiệu suất tốt nhất trong tất cả các phương pháp, bao gồm các phương pháp KD dựa trên CNN và các phương pháp dựa trên transformer. Đối với chế độ CNN-CNN được sử dụng phổ biến nhất, phương pháp KD đa kiến trúc đề xuất cho thấy hiệu suất vượt trội. Điều này là do học sinh CNN học thông tin toàn cục bổ sung từ giáo viên Transformer. Khoảng cách hiệu suất thậm chí còn lớn hơn (thường hơn 1%) khi giáo viên Transformer và giáo viên CNN có FLOPs tương tự. Bởi vì dưới chi phí tính toán tương tự, giáo viên Transformer thường có độ chính xác cao hơn giáo viên CNN. Đối với chế độ Transformer-CNN, một mức tăng hiệu suất cao hơn (mức tăng trung bình 2.7%) được thu được so với các phương pháp CNN-CNN. Điều này cho thấy các phương pháp KD hiện có không tận dụng hết lợi thế của giáo viên Transformer, mặc dù chúng có thể được áp dụng cho tình huống đa kiến trúc. Trong chế độ Transformer-Transformer, kết quả của phương pháp đề xuất hầu hết vượt trội hơn kết quả KD dựa trên Transformer. Mặc dù mô hình Xceptionx2 hơi kém hơn mô hình ViT-B/16, mức tăng hiệu suất của Xceptionx2 cao hơn mức tăng của ViT-B/16. Điều này cho thấy KD đa kiến trúc có thể thu được mức thúc đẩy cao hơn so với KD kiến trúc đồng nhất thông thường. Bên cạnh đó, trong khung đa kiến trúc của chúng tôi, việc áp dụng và gia tốc học sinh CNN vào ứng dụng thực tế dễ dàng hơn.

Đánh giá trên ImageNet. Thí nghiệm được tiến hành trên ImageNet để xác minh thêm tính tổng quát và hiệu quả của phương pháp đề xuất. Như được hiển thị trong Bảng 2, phương pháp của chúng tôi thể hiện hiệu suất tốt nhất trên ImageNet. Tương tự như cài đặt của CIFAR, hai chế độ kiến trúc đồng nhất bao gồm CNN-CNN và Transformer-Transformer và một chế độ đa kiến trúc, tức là Transformer-CNN, được so sánh. Khác với các phương pháp kiến trúc đồng nhất, khung đa kiến trúc đề xuất khuyến khích học sinh học cả các đặc trưng không gian cục bộ (với mô hình CNN gốc) và các đặc trưng toàn cục bổ sung (từ mô hình giáo viên transformer). Do đó, học sinh CNN thu được hiệu suất cao hơn. Đặc biệt, từ Bảng 2, một số CNNs (ví dụ, ResNet50x2-80.72%) được hướng dẫn bởi Transformer thậm chí vượt qua Transformer với tính toán mô hình tương tự (ví dụ, ViT-B/32-78.29%), hơn 1.03% độ chính xác. Với các thuộc tính thân thiện với phần cứng, những CNNs được cải thiện này có tiềm năng hơn cho các ứng dụng thiết bị biên.

4.3 Nghiên cứu Loại bỏ

(1) Các cặp giáo viên-học sinh khác nhau. Để xác minh tính tổng quát của phương pháp đề xuất, chúng tôi đánh giá nó với các cặp giáo viên-học sinh đa kiến trúc khác nhau trong Bảng 3. Có thể quan sát thấy rằng phương pháp đa kiến trúc của chúng tôi thu được mức thúc đẩy hiệu suất đáng kể qua các cặp giáo viên-học sinh khác nhau, so với baseline. Ngoài ra, độ chính xác của học sinh tiếp tục tăng khi hiệu suất của giáo viên trở nên tốt hơn. Cuối cùng, Transformer có thể là một giáo viên xuất sắc vì nó thường thu được hiệu suất tốt hơn với FLOPs tương tự so với mạng CNN. Sử dụng Transformer để hướng dẫn việc học của học sinh CNN có thể là một hướng tiềm năng.

(2) Hiệu quả của projector đề xuất. Chúng tôi phân tích hiệu quả của projector PCA đề xuất và projector GL. Kết quả thí nghiệm trên ImageNet trong Hình 3-(a) cho thấy mức tăng hiệu suất lớn khi hai projector được tham gia trong quá trình KD. Điều này cho thấy các projector PCA và GL cải thiện đáng kể chất lượng của đặc trưng CNN, mặc dù chúng được loại bỏ trong giai đoạn suy luận. Chúng tôi đánh giá thêm khả năng chuyển giao sau khi thêm hai projector này trong Hình 3-(b). Độ tương tự cosine được tăng lên một biên độ lớn và thậm chí cao hơn so với kiến trúc đồng nhất. Do đó, có thể tăng khả năng chuyển giao kiến thức giữa Transformer và CNN bằng các phương pháp KD được thiết kế cẩn thận.

(3) Hiệu quả của huấn luyện bền vững đa góc nhìn. Như được báo cáo trong Hình 3-(a), đối với đánh giá thông thường không có nhiễu, các mạng học sinh thu được mức tăng độ chính xác top-1 0.2%-0.4% trên ImageNet với sơ đồ huấn luyện bền vững đa góc nhìn. Để xác minh thêm hiệu quả của nó, chúng tôi cũng báo cáo kết quả cho đánh giá nhiễu, trong đó tập dữ liệu xác thực được tăng cường khác với tăng cường huấn luyện. Dưới giao thức này, mức tăng độ chính xác top-1 sau khi thêm sơ đồ huấn luyện bền vững đa góc nhìn được mở rộng lên hơn 1.0%. Điều này chứng minh rằng sơ đồ huấn luyện bền vững đề xuất tăng cường tính bền vững nhiễu của mạng học sinh.

(4) Ứng dụng cho các nhiệm vụ khác. Phương pháp KD đa kiến trúc đề xuất cũng hoạt động tốt trên các nhiệm vụ khác. Như được hiển thị trong Bảng 4, phương pháp của chúng tôi được đánh giá trên ba nhiệm vụ thị giác bao gồm phát hiện đối tượng [36], phân đoạn thể hiện [37] và chống giả mạo khuôn mặt [38].

Đối với phát hiện và phân đoạn, chúng tôi tuân theo giao thức gần đây của cơ sở dữ liệu COCO [39] và báo cáo độ chính xác trung bình (AP). Lưu ý rằng AP trong phân đoạn được tính bằng cách sử dụng giao điểm mặt nạ trên hợp (IoU). Phương pháp đề xuất cho thấy ưu thế so với phương pháp KD thông thường trong Bảng 4. Đối với phương pháp KD thông thường Logits, hiệu suất của chế độ đa kiến trúc thậm chí còn tệ hơn hiệu suất của chế độ kiến trúc đồng nhất. Điều này thể hiện thêm rằng phương pháp của chúng tôi giải quyết hiệu quả vấn đề không khớp của KD đa kiến trúc. Ngoài ra, đối với chống giả mạo khuôn mặt, là một nhiệm vụ phân loại nhị phân, chúng tôi áp dụng ResNet18, Inception-v3 và ResNext26 làm xương sống của học sinh. Tỷ lệ Lỗi Bằng nhau (EER) được báo cáo làm thước đo đánh giá. Và các thí nghiệm được tiến hành trên CelebA-Spoof [38], là một trong những tập dữ liệu lớn nhất cho chống giả mạo khuôn mặt. Đáng chú ý rằng có ít thông tin hữu ích về tương quan lớp trong nhiệm vụ phân loại nhị phân. Do đó, phương pháp KD thông thường Logits có sự cải thiện biên so với học sinh. Ngược lại, phương pháp đề xuất cũng thu được hiệu suất thỏa đáng từ Bảng 4. Thú vị khi nhận thấy rằng, mặc dù phương pháp đề xuất được thiết kế cho nhiệm vụ phân loại, nó có tính tổng quát tốt khi được áp dụng trực tiếp cho các nhiệm vụ khác như phát hiện và phân đoạn.

5 Kết luận

Trong bài báo này, một phương pháp chưng cất kiến thức đa kiến trúc mới được đề xuất. Cụ thể, hai projector bao gồm projector attention chéo một phần (PCA) và projector tuyến tính theo nhóm (GL) được trình bày. Hai projector này thúc đẩy khả năng chuyển giao kiến thức từ giáo viên sang học sinh. Để cải thiện thêm tính bền vững và ổn định của khung, một sơ đồ huấn luyện bền vững đa góc nhìn được đề xuất. Kết quả thí nghiệm mở rộng cho thấy phương pháp của chúng tôi vượt trội hơn 14 phương pháp hiện đại trên cả tập dữ liệu quy mô lớn và quy mô nhỏ.

[Bảng và hình sẽ được dịch tương tự nhưng tôi sẽ không dịch chi tiết tất cả các bảng số liệu để tiết kiệm không gian. Cấu trúc và tiêu đề sẽ được dịch đầy đủ.]

Lời cảm ơn Công trình này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia Trung Quốc (Số hiệu 2020AAA0106800), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62192785, Số hiệu 61902401, Số 61972071, Số U1936204, Số 62122086, Số 62036011, Số 62192782 và Số 61721004), Quỹ Khoa học Tự nhiên Bắc Kinh Số M22005, Chương trình Nghiên cứu Trọng điểm CAS về Khoa học Tiền phong (Số hiệu QYZDJ-SSW-JSC040). Công trình của Bing Li cũng được hỗ trợ bởi Hiệp hội Thúc đẩy Đổi mới Thanh niên, CAS.

Tài liệu Tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên định dạng gốc với 39 mục từ [1] đến [39]]
