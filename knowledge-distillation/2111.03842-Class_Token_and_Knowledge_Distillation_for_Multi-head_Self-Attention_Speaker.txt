# 2111.03842.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-distillation/2111.03842.pdf
# File size: 1505798 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Class Token and Knowledge Distillation for Multi-head Self-Attention Speaker
Veriï¬cation Systems
Victoria Mingote, Antonio Miguel, Alfonso Ortega, Eduardo Lleida
ViVoLab, AragÂ´ on Institute for Engineering Research (I3A), University of Zaragoza, Spain.
fvmingote,amiguel,ortega,lleida g@unizar.es
Abstract
This paper explores three novel approaches to improve the performance of speaker veriï¬cation (SV) systems based
on deep neural networks (DNN) using Multi-head Self-Attention (MSA) mechanisms and memory layers. Firstly, we
propose the use of a learnable vector called Class token to replace the average global pooling mechanism to extract the
embeddings. Unlike global average pooling, our proposal takes into account the temporal structure of the input what is
relevant for the text-dependent SV task. The class token is concatenated to the input before the ï¬rst MSA layer, and its
state at the output is used to predict the classes. To gain additional robustness, we introduce two approaches. First, we
have developed a new sampling estimation of the class token. In this approach, the class token is obtained by sampling
from a list of several trainable vectors. This strategy introduces uncertainty that helps to generalize better compared
to a single initialization as it is shown in the experiments. Second, we have added a distilled representation token
for training a teacher-student pair of networks using the Knowledge Distillation (KD) philosophy, which is combined
with the class token. This distillation token is trained to mimic the predictions from the teacher network, while the
class token replicates the true label. All the strategies have been tested on the RSR2015-Part II and DeepMine-Part 1
databases for text-dependent SV , providing competitive results compared to the same architecture using the average
pooling mechanism to extract average embeddings.
Keywords: Class Token, Teacher-Student Learning, Distillation Token, Speaker Veriï¬cation, Multi-head
Self-Attention, Memory Layers
1. Introduction
The performance in speaker veriï¬cation (SV) tasks
has improved greatly in recent years thanks to the deep
learning (DL) advances in signal representations and
optimization metrics [1, 2, 3, 4, 5] that have been
adapted from the state-of-the-art face veriï¬cation, im-
age recognition, or text-modelling systems. In these
systems, Convolutional Neural Network (CNN) or Time
Delay Neural Network (TDNN) [2] are still the most
employed approaches to obtain the signal representa-
tions or embeddings. Nevertheless, self-attention mech-
anisms are becoming a dominant approach in many
ï¬elds beyond text-related tasks. For example, Trans-
formers [6] are spreading to many tasks [7, 8, 9, 10]
where large scale databases are available. In SV tasks,
this kind of architecture has started to be successfully
applied in text-independent SV [11, 12, 13, 14] where
there are no constraints in the uttered phrase and bigdatabases are available. However, in text-dependent SV ,
there is still room for improvement since the amount of
public data is not very large. Besides, text-dependent
SV consists of deciding whether a speech sample has
been uttered by the correct speaker pronouncing the
ï¬xed passphrase selected. So, the phonetic informa-
tion of the signal is relevant to determine the identity.
Therefore, keeping the temporal structure is needed to
obtain representations that encode correctly both phrase
and speaker information.
In the context of text-dependent SV tasks, our pre-
vious works [15, 16, 17] showed the advantages of re-
placing the traditional pooling mechanism based on av-
eraging the temporal information with an external align-
ment mechanism to obtain a supervector embedding.
This supervector allowed to keep the temporal struc-
ture and represent both phrase and speaker information,
but the temporal alignment had to be performed by us-
ing an external method as a phone decoder, a GaussianarXiv:2111.03842v2  [eess.AS]  10 Feb 2023

--- PAGE 2 ---
Mixture Model (GMM) [18, 19] or a Hidden Markov
Model (HMM) [20]. As an alternative approach, in [21],
we introduced Multi-head Self-Attention (MSA) mech-
anisms [6] combined with memory layers [22] to sub-
stitute the alignment mechanisms. The use of MSA al-
lowed the model to focus on the most relevant frames
of the sequence to discriminate better among utterances
and speakers. However, the proposed architecture based
on MSA employed an average pooling mechanism to
obtain the ï¬nal representation embedding.
In this work, to substitute the global average pooling,
we have introduced a learnable vector known as Class
token, which is inherited from Natural Language Pro-
cessing (NLP) [7], and recently, many image recogni-
tion systems [8]. However, this approach has not yet
been applied to SV tasks. To introduce this vector into
the system based on DNN with MSA and memory lay-
ers, the class token is concatenated to the input before
the ï¬rst MSA layer, and the state at the output is em-
ployed to perform the class prediction. During train-
ing, the temporal information is encoded in the token,
and this token interacts with the whole input sequence
through self-attention and learns a global description
similar to a supervector approach [16, 23] since the mul-
tiple heads act as slots of the supervector. A similar
mechanism has also been used recently in [10]. There-
fore, the average pooling mechanism is not needed to
obtain a representation. The multiple heads can encode
more details about the sequence order than the average,
playing the role of the states and improving the results
as shown in [16],[17] with the use of external align-
ment mechanisms based on HMM and GMM. In ad-
dition, the information encoded in these multiple heads
can be represented and analyzed, which improves the
interpretability of the results of this kind of approach.
To improve the performance obtained with the class to-
ken approach, we also introduce a novel multiple initial-
ization sampling mechanism to reduce possible initial-
ization problems and give more robustness against the
lack of data to model predictions. Since it is a case of
use in the industry to develop custom speciï¬c systems
with small in-domain datasets and this kind of approach
could be a possible solution.
Moreover, this work contributes with another ap-
proach based on Transformer architecture and Knowl-
edge Distillation (KD) [24, 9]. We propose a teacher-
student approach combined with Random Erasing data
augmentation [25, 26] which allows modelling the un-
certainty in the parameters of a teacher model with a
compact student model and get more reliable predic-
tions. Following the idea proposed in [9], we have also
introduced the Distillation token in the student networkto replicate the predictions of the teacher network, while
the class token is trained to reproduce the true label as
Fig.3 depicts. Unlike the objective in [9], in our work,
the distillation process is not intended to compress the
teacher model, but rather both models are trained to-
gether and the student model learn to better capture the
intrinsic variability of the teacher predictions.
To summarize, the main contributions are:
ÂˆWe replace the global average pooling mechanism
by a learnable class token to obtain a global utter-
ance descriptor associated to the concept of super-
vector in speaker veriï¬cation.
ÂˆWe propose a new approach based on a sampling
approximation to estimate the class token.
ÂˆWe introduce a teacher-student architecture with
an additional token known as distillation token
which is combined with the class token to provide
robustness to the learned student model.
This paper is organized as follows. In Section 2, we
show an overview of the MSA and memory layers. Sec-
tion 3 explains the strategy of introducing a learnable
class token using sampling. In Section 4, we introduce
the approach based on KD combined with the tokens
employed to develop our system. Section 5 describes
the system used. In Section 6, we present the experi-
mental data, and Section 7 explains the results achieved.
Conclusions are presented in Section 8.
2. Overview of Transformer Encoder
The original transformer architecture [6] is composed
of two main parts: the encoder and decoder parts. How-
ever, in many tasks, the transformer encoder is the only
part used to create the DL systems. The core mecha-
nism of each encoder block is Multi-head Self-Attention
(MSA) layer which is composed of multiple dot-product
attention. As we only employ the encoder part, the input
to this attention mechanism is the same for the query,
key and value signals ( Q;K;V):
Qh=xWQ
h;Kh=xWK
h;Vh=xWV
h; (1)
where xis the input to this layer, and WQ
h;WK
h;WV
hare
learnable weight matrices to make the linear projec-
tions. After these projections, a softmax operation is
performed over the temporal axis, which allows each
head to focus on certain frames of the input sequence.
The result of this softmax operation is known as the self-
attention matrix for each head and can be deï¬ned as:
Ah=so f tmax t QhK|
hp
dk!
; (2)
2

--- PAGE 3 ---
where dkis the number of dimensions of the query /key
vector, and|denotes transpose. This self-attention ma-
trix learns the most relevant information among the dif-
ferent data. Using this information, the value Vfeature
vectors are aggregated to obtain the output of each head.
The ï¬nal output of each head can be calculated as,
Hh=AhVh: (3)
Thus, MSA is deï¬ned as the concatenation of the out-
puts from each head Hh:
MS A (X)=[H1;H2:::Hdhead]Whead; (4)
where Xis the input to the attention layer, Wheadis a
learnable weight matrix to make a ï¬nal linear projec-
tion, and dheadis the number of attention heads in the
h thlayer.
The transformer encoder alternates the MSA layer
with a second layer which is the feed-forward (FF)
layer. However, in [21], we proposed the replacement
of FF layers by memory layers as in [22]. With this
layer, the input data is compared with all the keys using
a product key-attention, and the scores obtained are used
to select the closest keys, which have the highest scores.
After that, the associated weight vectors are computed
with the following expression:
w=so f tmax n(xUK); (5)
where xis the input to the layer, UKis the keys matrix,
and the softmax is computed over the memory index
axis to focus on certain contents of the memory that will
be used to provide the output. Once these vectors are
obtained, these weights are combined with the memory
values of the selected keys, and the output is concate-
nated with the previous attention output:
xout=x+wUV; (6)
where ware the weights of the selected keys obtained
with (5), and UVare the memory values associated with
the keys. After the encoder blocks are applied, an aver-
age pooling mechanism is usually employed to reduce
the temporal information and represent variable-length
utterances with ï¬xed-length vectors. However, this av-
eraging may neglect the order of the phonetic informa-
tion, which is relevant for text-dependent SV tasks.
3. Representation using Class Token
In many tasks of NLP and computer vision, the trans-
former architecture uses a learnable vector called Class
Token ( xCLS), as in the original BERT model [7] or Vi-
sion Transformer (ViT) [8], instead of a global averagepooling. To employ this token in the transformer en-
coder, the vector is concatenated to the input of the ï¬rst
MSA layer to perform the classiï¬cation task. With this
token, the self-attention is forced to capture the most rel-
evant information with the class token to obtain a repre-
sentation as a global utterance descriptor similar to the
supervector approach. Instead of mixing all the infor-
mation with an average pooling mechanism, the tem-
poral structure can be kept since the attention mecha-
nism acts as a weighted sum of the temporal tokens for
each layer. The output vector is the concatenation of
dierent head subvectors and each of them is the result
of a di erent attention outcome. Thus, the mechanism
can be seen similar to those used in our previous work
[16], where the heads play the role of the states and
the supervector in [23]. The supervector mechanism is
also similar to [27] but in that case, the task was text-
independent SV and MSA layers were not used. Be-
sides, this type of mechanism allows to enhance the in-
terpretability of what the neural network learns through
the self-attention layers.
In [23], this mechanism to obtain the supervector
is deï¬ned similar to a conventional GMM supervector
with the following expression:
sc=P
txtwtcP
twtc=X
txtÂ¯wtc; (7)
where wtcare the weights obtained by a softmax func-
tion on the output of a learnable layer, scare vectors
per state /component Cof dimension Dthat summarize
the information associated along the sequence of fea-
ture vectors xtof dimension D, and Â¯ wtcare the nor-
malized weights deï¬ned as wtc=P
twtc. The ï¬nal su-
pervector is built by the concatenation of these vectors
S=fs1;:::;sCgand is used to represent the whole se-
quence. In this work, the output feature vectors for each
head Hof the MSA layer are obtained with (3) as a
weighted sum equivalent to (7), where Â¯ wtccorresponds
to the rows of the matrix of self-attention weights Ah
obtained with (2). In particular, for the class token, the
normalized weights would be obtained from the last row
ofAh. Therefore, the ï¬nal class token obtained with this
mechanism is the concatenation of the di erent head
subvectors corresponding to the class token position,
which can be expressed as the supervector presented
previously SCLS=fs1 CLS;:::;sH CLSg.
To introduce the class token in the system, one train-
able vector parameter with the dimension of the feature
vectors is deï¬ned when the network is initialized. For
each batch, it is replicated and concatenated at the end
of each input feature sequence in the training batch as
3

--- PAGE 4 ---
â€¦ğ›¼=ğ‘…â€¦
ğ›¼=1ğ›¼=	"#Iteration1	(Initial)Iterationn
IterationN	(End)ğ¶ğ¿ğ‘†!		ğ¶ğ¿ğ‘†#ğ¶ğ¿ğ‘†$%!ğ¶ğ¿ğ‘†$ğ¶ğ¿ğ‘†&%!ğ¶ğ¿ğ‘†&â€¦â€¦â€¦â€¦ğ¶ğ¿ğ‘†!		ğ¶ğ¿ğ‘†#ğ¶ğ¿ğ‘†$%!ğ¶ğ¿ğ‘†$ğ¶ğ¿ğ‘†&%!ğ¶ğ¿ğ‘†&â€¦â€¦â€¦â€¦ğ¶ğ¿ğ‘†!		ğ¶ğ¿ğ‘†#ğ¶ğ¿ğ‘†$%!ğ¶ğ¿ğ‘†$ğ¶ğ¿ğ‘†&%!ğ¶ğ¿ğ‘†&â€¦â€¦â€¦â€¦Figure 1: Evolution of the number of vectors in the token matrix that are available for sampling from the beginning of the training process (iteration
1) to the ï¬nal iteration (iteration N). In each iteration, the dark vectors represent the enabled class tokens, while the light vectors are the disabled
tokens.
an additional token. Hence, a single shared vector is
trained to learn the ï¬nal embedding representation.
In this work, we propose the use of a new sampling
approach [28], and instead of having a single class to-
ken shared for the whole batch, we assume this sensi-
tive parameter is the result of sampling a list of several
vectors to be selected during the training by sampling
them. In order to do that, we deï¬ne a matrix of Rvec-
tors ( Token Matrix ) and sample it to take one of them
for each example in the batch introducing uncertainty in
the class token ( CLS Token ). However, the use of this
approach leads to a complex and slower evaluation pro-
cess since a sampling inference would have to be car-
ried out to obtain the representations. For this reason, to
avoid making the sampling inference, we have sched-
uled a forced reduction of the available vectors in the
Token Matrix throughout the training process. Thus,
at the end of this process, only one weight is di erent
to zero, and the class token vector parameter is ï¬xed.
This strategy allows us to start the training ( Iteration 1)
with a matrix of several vectors to sample from and,
gradually, we reduce the number of vectors as the train-
ing progresses to ï¬nish ( Iteration N ) with only one as
the original class token as Fig.1 depicts. Therefore, the
training leads the system progressively to focus the rel-evant information on the ï¬rst vector of the matrix. In
addition, using this sampling approach, the system is
trained to capture the uncertainty introduced by initially
having a Token Matrix with Rvectors to combine with
the training batch data. Thus, each example from the
batch is combined with a random vector from the ma-
trix which is reduced in size after each epoch until only
one vector remains at the end, so more variability has to
be modelled which helps to improve the robustness of
the system.
To carry out this process, we deï¬ne the following vec-
tor, which indicates to the neural network the number of
tokens available at each iteration of the training process:
=(1;:::; n;:::; N);n=R;:::;1;with R2R(8)
where Ris the number of tokens deï¬ned in the matrix,
andNis the total number of iterations for training pro-
cess. Among the number of tokens available at each
iteration, a random selection of the batch size is made
to select the index of the vectors. These vectors are se-
lected from the distribution ( Token Matrix ) and used as
class tokens ( CLS Token ) in the batch to concatenate to
the input of the ï¬rst MSA layer. The overall process is
described in Algorithm 1. Besides, Fig.2 shows a graph-
ical example of how this sampling process is made in
4

--- PAGE 5 ---
Step1â€¦
MSAPooling
Memoryğ’™"++SamplingCLS	Token
ğ¶ğ¿ğ‘†!		ğ¶ğ¿ğ‘†#$!	ğ¶ğ¿ğ‘†!ğ¶ğ¿ğ‘†%ğµStep2ğ›¼=	"#â€¦â€¦ğ¶ğ¿ğ‘†!		ğ¶ğ¿ğ‘†#ğ¶ğ¿ğ‘†$%!ğ¶ğ¿ğ‘†$ğ¶ğ¿ğ‘†&%!ğ¶ğ¿ğ‘†&â€¦â€¦TokenMatrix-IterationnFigure 2: Example of the sampling steps in iteration n of the training process. In Step 1, the available vectors of the token matrix in that iteration
are deï¬ned and the random indeces of batch size ( B) are calculated. In Step 2, the class tokens are selected and added to the input of MSA layer.
an intermediate iteration ( Iteration n ). In this graphical
explanation, it can be observed how in an intermediate
iteration the number of vectors has been reduced forcing
the network to put the relevant information to represent
the utterances in the vectors still available.
4. Knowledge Distillation with Tokens
Motivated by the beneï¬ts obtained when the training
databases are not very large with Teacher-Student archi-
tecture based on CNNs [26], we have implemented this
architecture using two transformer networks as Fig.3
depicts. Using a Bayesian approach similar to [29],
the teacher-student architecture allows providing ro-
bustness to the system. In this approach, the teacher
and student networks are trained at the same time, un-
like previous works [30, 31] in which the teacher net-
work is usually a pre-trained model to reduce complex-
ity. Whether the teacher network had been a frozen
model, negative training examples that obtain high pos-
terior values in the teacher network would be learned
as positive examples by the student network. Besides,
dierent sources of distortion are applied to each of the
input signals of both networks, so we have employed a
data augmentation method called Random Erasing (RE)
[25] to provide more variability to the input training
data. With this kind of architecture, the teacher networkhas to predict augmented unseen data and the student
network tries to mimic the label predictions produced by
the teacher network using the class token output. This
training strategy allows the student network to capture
the variability in the predictions produced by the ï¬rst
network and model this uncertainty in the parameters
during the training process. However, inspired by [9],
we have also included an extra learnable token in the
student network which is known as Distillation Token
(Distill Token ). The introduction of this extra token al-
lows to implement of a multi-objective optimization by
using the class token to reproduce the true label while
the distillation token is trained to mimic the predictions
of the teacher network. To achieve this, the Kullback-
Leibler Divergence (KLD) loss between the student and
teacher distributions is minimized. The KLD loss can
be formulated as,
KLD = PI
i=1PJ
j=1pT(ycls
ijxj)log (pS(ydist
ijxj))+const;(9)
where iandjare the speaker and utterance indices, xjis
the input signal, pT(ycls
ijxj) is the output posterior proba-
bility of the label ycls
ifrom the class token of the teacher
model, pS(ydist
ijxj) is the output posterior probability of
the label ydist
ifrom the distillation token of the student
network for the same example, and const is deï¬ned in
[29]. Hence, to train the teacher-student architecture
5

--- PAGE 6 ---
Algorithm 1: Algorithm for sampling class token and introducing it before the pooling part.
Input: Input examplesX, batch size B, examples of batch x, the number of layers L, the total number of
tokens to sample R, and the number of epochs N
1. Deï¬ne the vector with the number of sample vectors available to select each epoch:
=(1;:::; n;:::; N),n=R;:::;1;with R2R
2. Deï¬ne the random matrix of class token vectors:
TokenMatrix =random matrix (R)
forn=1toNdo
forx2Xdo
3. Sampling Process
3.1 Step 1, every update Binteger indexes are randomly generated from the available nvectors:
inds=random integer (n,B)
3.2 Step 2, the correspondent tokens are selected:
CLS Token =TokenMatrix [inds]
4. Network Training
4.1 Step 1, the class token is concatenated with the input to the MSA layer:
xl=[xjjCLS Token]
4.2 Step 2, the new input is introduced to the ï¬rst MSA layer in the pooling part and the L layers
are applied:
forl=1toLdo
x0
l=xl+MS A (xl)
xl=x0
l+Memory (x0
l)
end
4.3 Step 3, the state at the output of the last layer in the pooling block of the class token is used as
ï¬nal representation:
xCLS=xend
l
end
end
showed in Fig.3, we employ the following two loss ex-
pressions for teacher and student networks:
Loss T=CE(ycls
T;y); (10)
Loss S=KLD (ydist
S;ycls
T)+CE(ycls
S;y); (11)
where CEis the cross-entropy loss, ycls
Sis the class
token output from the student network, and yare the
ground truth labels.
5. System Description
In this section, we describe the system architecture
used in this work for text-dependent SV . Fig.3 depicts
this architecture where a teacher-student approach is
employed. Both architectures follow the structure de-
scribed in [21] with the same backbone and poolingparts. The backbone is based on two Residual Network
(RN) [32] blocks with three layers each block. Addi-
tionally, these architectures need embeddings with po-
sitional information to help guiding the attention mask
in the MSA layers. In this work, these embeddings
(eph) are extracted by a phonetic classiï¬er network in-
stead of using temporal position information [27]. For
the pooling part, two MSA layers of 16 heads com-
bined with two memory layers are employed. More-
over, before the ï¬rst MSA layer, the class token is con-
catenated to the input. In the case of the student net-
work, the distillation token is also included. Thanks to
the self-attention mechanism, these tokens learn to ob-
tain a global representation for each utterance without
applying the global average pooling. These represen-
tations, similar to supervector, are more convenient for
text-dependent SV task since these global representa-
6

--- PAGE 7 ---
ResBlock
MSA+MemoryInput featuresPhembğ’†ğ’‘ğ’‰TeacherNetwork
RandomErasingğ’™CLS	Token
CE LossGradPhemb
RandomErasing
KLD LossGrad
Softmax
FCStudentNetwork
DistillToken
ResBlockğ’†ğ’‘ğ’‰BackbonePoolingğ’™ğ‘ªğ‘³ğ‘ºğ’™#
MSA+Memory
ResBlock
FC
MSA+Memory
ResBlock
ğ’™ğ‘ªğ‘³ğ‘º
MSA+MemoryBackbonePoolingğ’™ğ‘«ğ‘°ğ‘ºğ‘»CLS	Tokenğ’†ğ’‘ğ’‰ğ’†ğ’‘ğ’‰
ğ’šğ‘»ğ‘ªğ‘³ğ‘ºğ’šğ‘»ğ‘ªğ‘³ğ‘ºğ’šğ‘ºğ‘«ğ‘°ğ‘ºğ‘»ğ’šğ‘ºğ‘ªğ‘³ğ‘ºğ’šğ’š+
FC
CE Loss
Softmax
Softmaxğ’™
ğ’™#ğ’™#ğ’™#Figure 3: Teacher-student architecture used to create the system, where the dashed line indicates the process of backpropagation of the gradients.
Both networks are employed to train while for testing, the student network is the only used.
tions do not neglect the sequence order and are obtained
automatically thanks to the self-attention mechanism.
So external alignment mechanisms are not necessary to
obtain them as in [15, 16, 17], where GMM or HMM
posterior probabilities are needed to align speech frames
to supervectors. Besides, the use of memory layers in-
creases the amount of knowledge obtained by the net-
work that can be stored. After training the system, a co-
sine similarity over the token representations is applied
to perform the veriï¬cation process. Note that this kind
of system based on teacher-student consists of training
of two architectures at the same time. Therefore, this
process may involve a higher computational cost. How-
ever, during inference, only the student network is em-
ployed to extract the embeddings, so there is no extra
inference time.
6. Experimental Setup
6.1. Datasets
For the experiments, two text-dependent speaker ver-
iï¬cation datasets have been employed. The ï¬rst set of
experiments has been reported on the RSR2015 text-
dependent speaker veriï¬cation dataset [33]. This dataset
comprises recordings from 157 males and 143 females.
For each speaker, there are 9 sessions with 30 di erent
phrases. This data is divided into three speaker sub-
sets: background (bkg), development (dev) and evalua-
tion (eval). In this paper, we develop our experimentswith Part II, which is composed of short control com-
mand with a strong overlap of lexical content, and we
employ only the bkg data for training. The eval part is
used for enrollment and trial evaluation. This dataset
has three evaluation conditions, but in this work, the
most challenging, which is the Impostor-Correct case,
is the only one that has been evaluated and employed
in the text-dependent SV . Note that there are other sys-
tems that obtain relevant results for this dataset, similar
to those presented below. Nevertheless, such systems
are based on traditional models such as Hidden Markov
Models (HMMs) [33, 34] or neural network architec-
tures focused on two di erent streams for speaker and
utterance information [35, 36].
The second dataset used is the DeepMine database
[37]. This corpus consists of three di erent parts
of which we employ the ï¬les selected for the Short-
duration Speaker Veriï¬cation (SdSV) Challenge 2020
[38] from Part 1. Part 1 is the text-dependent part which
is composed of 5 Persian and 5 English phrases and con-
tains 963 females and males speakers. This data is di-
vided into two subsets: train with 101.063 audio ï¬les
and evaluation with 69.542 audio ï¬les. Finally, the pho-
netic classiï¬cation network [27] has been trained using
LibriSpeech [39] to extract phonetic embeddings. Un-
like other works presented in the challenge [40, 41], we
have not used V oxCeleb 1 and 2 datasets [42, 43] in the
neural network training process. Motivated by the fact
7

--- PAGE 8 ---
that in some situations and applications is required the
implementation of custom systems with the few avail-
able in domain-data. For this reason, we have developed
systems only with the in-domain data.
6.2. Experimental Description
To carry out the experiments with the RSR2015
dataset, a set of features composed of 20 dimen-
sional Mel-Frequency Cepstral Coe cients (MFCC)
with their derivates are employed as input. While for
the experiments using the DeepMine dataset, we have
employed a feature vector based on mel-scale ï¬lter
banks. With this feature extractor, we obtain two log
ï¬lter banks of sizes 24 and 32, which are concatenated
with the log energy to obtain a ï¬nal input dimension of
57. Moreover, phonetic embeddings of 256 dimensions
have been used as positional information. As the opti-
mizer for the experiments in this work, the Adam op-
timizer is employed with a learning rate that increases
from 10 3to 510 3during 60 epochs and then decays
from 510 3to 10 4. In addition, training data is fed
into the systems with a minibatch size of 32.
7. Results
In this paper, two sets of experiments have been car-
ried out to evaluate the proposals with both databases.
We compare the di erent approaches to obtain the rep-
resentations with a single neural network using the same
architecture as the teacher network: the use of the tradi-
tional global average pooling ( AVG ), the attentive pool-
ing ( ATT ) and the introduction of the learnable class
token ( CLS ). For the class token approach, we eval-
uate our proposal of sampling a matrix of Rvectors
and reducing it until having a single vector ( S ampling ).
This parameter is also swept for di erent values of R,
including R=1 that corresponds to the original idea
of having a single token and repeating it. Moreover,
we analyze the e ect produced by the fact of using a
teacher-student architecture with an extra distillation to-
ken ( CLS DIS T ).
In order to evaluate these experiments, we have mea-
sured the performance using three metrics. Equal Error
Rate (EER) which measures the discrimination ability
of the system. NIST 2008 and 2010 minimum Detection
Cost Functions (DCF08, DCF10) [44, 45] which mea-
sure the cost of detection errors in terms of a weighted
sum of false alarm and miss probabilities for a decision
threshold, and a priori probability.7.1. Class Token Study
A ï¬rst set of experiments was performed to com-
pare the use of a class token to obtain global utterance
descriptors with the use of a global average pooling
method or the attentive pooling proposed in [46]. Thus,
we study the two approaches to introduce this vector ex-
plained during this work and the e ect of the number of
vectors chosen for the sampling approach.
Table 1 presents EER, DCF08 and DCF10 results for
the experiments with RSR2015-part II dataset. Regard-
less of the number of vectors in the sampling for class
tokens, if we apply our proposed strategy to introduce
the tokens with a sampling alternative, the obtained per-
formance is better. In addition, the results show how
employing a learnable token outperforms the use of an
average embedding or an attentive pooling embedding.
Note that the token is trained through self-attention and
keeping the temporal structure to obtain a global utter-
ance representation, while the average embedding ne-
glects this information that is relevant to the SV task.
As we can also observe with the sweep of Rvalue, the
use of several vectors to create the token matrix is bet-
ter than using a single vector and repeating it for the
whole batch, which corresponds to the original way of
applying this approach. The case of having a single vec-
tor and repeat it corresponds with the experiments with
R=1. However, when the number of available tokens
is too large, the performance begins to degrade. This
degradation could be caused by the introduction of too
much variability that the system is not able to model
as the architectures employed are not so large, which
means that there are a limited number of di erent to-
kens to carry out the training process.
In Table 2, the results obtained in DeepMine-part 1
database are shown. Unlike the other dataset, the train-
ing data in DeepMine is larger, which indicates that the
lack of data is not so critical to train a powerful and ro-
bust system. Therefore, the replacement of the average
embedding or attentive pooling embedding by a class
token improves the performance only slightly. Besides,
the sweep of Rvalue shows that the evolution of the fe-
male and male results separately do not follow the same
trend as occurs in the RSR-Part II results.
7.2. E ect of Knowledge Distillation using Tokens
In this section, we analyze the e ect of introducing an
approach based on Knowledge Distillation philosophy
which consists of a teacher-student architecture. Fur-
thermore, in this approach, an extra distillation token
(CLS DIS T ) is incorporated [9]. This approach has
been employed to compare the performance obtained in
8

--- PAGE 9 ---
Table 1: Experimental results on RSR2015 Part II [33] eval subset, showing EER%, DCF08 and DCF10. These results were obtained to compare
the di erent approaches to obtain the representations: average, attentive or sampling strategies.
Architecture Female Male Female +Male
Type T /S Sampling EER% DCF08 DCF10 EER% DCF08 DCF10 EER% DCF08 DCF10
A VG no   4:64 0:228 0:669 4:92 0:244 0:716 4:79 0:237 0:706
ATT no   4:07 0:188 0:618 4:44 0:211 0:615 4:53 0:213 0:646
CLS no R =1 3:71 0:174 0:580 4:27 0:215 0:679 4:12 0:201 0:634
R=50 3:37 0:169 0:580 4:04 0:199 0:601 3:75 0:187 0:606
R=100 3.33 0.158 0.552 3.68 0.182 0.552 3.57 0.173 0.565
R=200 3:55 0:171 0:562 4:09 0:199 0:607 3:86 0:189 0:587
Table 2: Experimental results on DeepMine [37] eval subset, showing EER%, DCF08 and DCF10. These results were obtained to compare the
dierent approaches to obtain the representations: average, attentive or sampling strategies.
Architecture Female Male Female +Male
Type T /S Sampling EER% DCF08 DCF10 EER% DCF08 DCF10 EER% DCF08 DCF10
A VG no   3:92 0:135 0:411 3:02 0:137 0:676 3:58 0:136 0:521
ATT no   5:73 0:193 0:468 5:39 0:210 0:664 5:60 0:200 0:551
CLS no R =1 3:81 0:128 0:389 3:32 0:143 0:697 3:60 0:134 0:520
R=50 3:92 0:131 0:393 3:19 0:140 0:668 3:62 0:134 0:519
R=100 3.69 0.124 0.379 3:09 0:137 0:658 3.43 0.129 0.505
R=200 3:89 0:133 0:417 2.92 0.133 0.655 3:50 0:133 0:521
the case of the average global pooling as well as in the
proposed sampling approach to use the class token. In
this second case, we have developed the teacher-student
architecture using the Rvalue of the best conï¬guration
obtained in the previous section, and also, the case of
R=1 as it is the usual way to apply this class token
approach in the literature.
Results of these experiments in RSR-Part II are
shown in Table 3. Regardless of the kind of approach
to obtain the representations used, we can observe that
the use of an architecture based on a teacher-student
approach improves the robustness and achieves better
performance in all the alternatives to extract the repre-
sentations. Moreover, the best performance is obtained
applying our proposed strategy to introduce the tokens
with a sampling alternative with more than a single vec-
tor.
On the other hand, Table 4 presents the performance
of systems with DeepMine-part 1. In this case, the
results show that the application of only the teacher-
student architecture does not improve the systems.
However, the use of the teacher-student architecture and
the extra distillation token ( CLS DIS T ), combined
with the sampling strategy with several token vectors
also allows achieving a more robust system and a sig-
niï¬cant improvement in the results.7.3. Analysis of Class Token Self-Attention Representa-
tions
In view of the relevant results obtained, we have
also conducted an analysis to interpret what the self-
attention matrix Ais learning in each system. To per-
form this analysis, we have employed the system with
the best performance from each database, and within
these systems, the last MSA layer of the student model
has been selected to make the representations. In addi-
tion, we have chosen di erent utterances to analyze in
Fig.4 and Fig.5. For each utterance, three ï¬gures are
plotted: the spectrogram of the utterance, the matrix of
attention weights corresponding to the class token for
each of the 16 heads of the MSA layer, and the sum of
the weights of these class token attentions.
In Fig.4, two examples of utterances of di erent
phrases ( â€œCall sisterâ€ ,â€œCall brotherâ€ ) pronounced
by the same speaker are shown. These examples are
obtained from the evaluation set of the RSR-Part II
database. Whether we look in the middle and bottom
ï¬gures, we can observe the relevant information learned
by the self-attention weights to correctly determine the
phrase and speaker of each utterance using the class to-
ken. Note that these two phrases of example begin ex-
actly the same with the word Call, so focusing on the be-
ginning of the ï¬gures, we observe how the self-attention
gives similar relevance in both cases to the areas of same
phonemes. Moreover, we can also see that the weights
do not pay attention to the area at the beginning and end
9

--- PAGE 10 ---
Table 3: Experimental results on RSR2015 Part II [33] eval subset, showing EER%, DCF08 and DCF10. These results were obtained to compare
the use of a teacher-student architecture for the di erent approaches to obtain the representations: average or sampling strategies.
Architecture Female Male Female +Male
Type T /S Sampling EER% DCF08 DCF10 EER% DCF08 DCF10 EER% DCF08 DCF10
A VG no   4:64 0:228 0:669 4:92 0:244 0:716 4:79 0:237 0:706
yes  3:52 0:170 0:587 3:78 0:186 0:579 3:74 0:185 0:602
CLS no R =1 3:71 0:174 0:580 4:27 0:215 0:679 4:12 0:201 0:634
CLS-DIST yes R =1 3:01 0:148 0:548 3:40 0:173 0:557 3:31 0:167 0:558
CLS no R =100 3:33 0:158 0:552 3:68 0:182 0:552 3:57 0:173 0:565
CLS-DIST yes R =100 2.47 0.122 0.414 2.83 0.138 0.463 2.68 0.133 0.443
Table 4: Experimental results on DeepMine [37] eval subset, showing EER%, DCF08 and DCF10. These results were obtained to compare the use
of a teacher-student architecture for the di erent approaches to obtain the representations: average or sampling strategies.
Architecture Female Male Female +Male
Type T /S Sampling EER% DCF08 DCF10 EER% DCF08 DCF10 EER% DCF08 DCF10
A VG no   3:92 0:135 0:411 3:02 0:137 0:676 3:58 0:136 0:521
yes  4:07 0:135 0:401 3:04 0:141 0:646 3:65 0:138 0:501
CLS no R =1 3:81 0:128 0:389 3:32 0:143 0:697 3:60 0:134 0:520
CLS-DIST yes R =1 3:80 0:131 0:395 3:25 0:144 0.621 3:57 0:135 0:494
CLS no R =100 3:69 0:124 0.379 3:09 0:137 0:658 3:43 0:129 0:505
CLS-DIST yes R=100 3.51 0.122 0:385 2.68 0.122 0:652 3.19 0.123 0.492
(a) Call sister
 (b) Call brother
Figure 4: Visualizing two examples of di erent phrases of RSR-Part II which are pronounced by the same speaker. In both cases, three representa-
tions are presented. The ï¬gure on top shows the spectrogram of each phrase. In the middle, the attention weights learnt by the class token for each
of the 16 heads in the last MSA layer are depicted. Finally, the plot on bottom is the sum of the rows of the previous weight attention matrix.
of the utterances that correspond to moments of silence.
Fig.5 represents two examples of utterances of the
same phrase ( â€œOK Googleâ€ ) pronounced by di erent
speakers. In this case, the examples are obtained from
the evaluation set of the DeepMine database. Note thatsince these ï¬gures are of the same phrase, self-attention
is focused on the same areas, but di erent relevance is
given to some of them. Besides, the e ect of not fo-
cusing on the beginning and end of the utterance also
occurs in these examples.
10

--- PAGE 11 ---
(a) OK Google
 (b) OK Google
Figure 5: Visualizing two examples of the same phrase of DeepMine which are pronounced by di erent speakers. In both cases, three representa-
tions are presented. The ï¬gure on top shows the spectrogram of each phrase. In the middle, the attention weights learnt by the class token for each
of the 16 heads in the last MSA layer are depicted. Finally, the plot on bottom is the sum of the rows of the previous weight attention matrix.
8. Conclusion
In this paper, we have presented a novel approach
for the SV task. This approach is based on the use
of a learnable class token to obtain a global utterance
descriptor instead of employing the average pooling.
Moreover, we have developed an alternative to cre-
ate the class token with a sampling strategy that intro-
duces uncertainty that helps to generalize better. Apart
from the previous approach, we have also employed a
teacher-student architecture combined with an extra dis-
tillation token to develop a more robust system. Us-
ing this architecture, the distillation token in the stu-
dent network learns to replicate the predictions from the
teacher network. Both proposals were evaluated in two
text-dependent SV databases. Results achieved show
in RSR2015-part II that each of the approaches intro-
duced to obtain a robust system and reduce potential
underperformance due to the lack of data improves the
overall performance. However, in DeepMine-part 1, the
results obtained replacing only the average embedding
by the class token present a small improvement, while
the use of a teacher-student architecture achieves a great
improvement and conï¬rms the power of this kind of ap-
proach to train the systems.
Credit Authorship Contribution Statement
Victoria Mingote : Conceptualization, Investigation,
Methodology, Software, Writing- Original draft prepa-ration; Antonio Miguel : Conceptualization, Investiga-
tion, Methodology, Software, Supervision, Writing- Re-
viewing and Editing.; Alfonso Ortega : Supervision,
Writing- Reviewing and Editing; Eduardo Lleida : Su-
pervision, Writing- Reviewing and Editing.
Acknowledgment
This project has received funding from
the European Unionâ€™s Horizon 2020 re-
search and innovation programme under Marie
SkÅ‚odowska-Curie Grant 101007666; in part by
MCIN /AEI/10.13039 /501100011033 and by the Eu-
ropean Union â€œNextGenerationEUâ€ /PRTR under
Grant PDC2021-120846-C41, by the Spanish Ministry
of Economy and Competitiveness and the European
Social Fund through the grant PRE2018-083312, by the
Government of Arag Â´on (Grant Group T36 20R), and
by Nuance Communications, Inc.
References
[1] Y . Taigman, M. Yang, M. Ranzato, L. Wolf, DeepFace: Closing
the Gap to Human-Level Performance in Face Veriï¬cation, 2014
IEEE Conference on Computer Vision and Pattern Recognition
(2014) 1701â€“1708 doi:10.1109/CVPR.2014.220 .
[2] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur,
X-vectors: Robust dnn embeddings for speaker recognition, in:
2018 ICASSP, pp. 5329â€“5333.
11

--- PAGE 12 ---
[3] E. Ho er, N. Ailon, Deep metric learning using triplet net-
work, Lecture Notes in Computer Science (including subseries
Lecture Notes in Artiï¬cial Intelligence and Lecture Notes in
Bioinformatics) 9370 (2010) (2015) 84â€“92. doi:10.1007/
978-3-319-24261-3{\_}7 .
[4] F. Schro , D. Kalenichenko, J. Philbin, Facenet: A uniï¬ed em-
bedding for face recognition and clustering, in: Proceedings of
the IEEE conference on computer vision and pattern recogni-
tion, 2015, pp. 815â€“823.
[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero,
Y . Carmiel, S. Khudanpur, Deep neural network-based speaker
embeddings for end-to-end speaker veriï¬cation, in: Spoken
Language Technology Workshop (SLT), 2016 IEEE, IEEE,
2016, pp. 165â€“170.
[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Å. Kaiser, I. Polosukhin, Attention is all you need, in:
Advances in neural information processing systems, 2017, pp.
5998â€“6008.
[7] J. D. M.-W. C. Kenton, L. K. Toutanova, Bert: Pre-training of
deep bidirectional transformers for language understanding, in:
Proceedings of NAACL-HLT, 2019, pp. 4171â€“4186.
[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,
S. Gelly, et al., An image is worth 16x16 words: Transformers
for image recognition at scale, arXiv preprint arXiv:2010.11929.
[9] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,
H. JÂ´egou, Training data-e cient image transformers & distil-
lation through attention, arXiv preprint arXiv:2012.12877.
[10] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran,
G. Heigold, J. Uszkoreit, A. Dosovitskiy, T. Kipf, Object-
Centric Learning with Slot Attention, in: NeurIPS 2020, 2020.
[11] M. India, P. Safari, J. Hernando, Self multi-head attention for
speaker recognition, Proc. Interspeech 2019 (2019) 4305â€“4309.
[12] H.-j. Shim, J. Heo, J.-h. Park, G.-h. Lee, H.-J. Yu, Graph at-
tentive feature aggregation for text-independent speaker veriï¬-
cation, in: ICASSP 2022-2022 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), IEEE,
2022, pp. 7972â€“7976.
[13] R. Wang, J. Ao, L. Zhou, S. Liu, Z. Wei, T. Ko, Q. Li,
Y . Zhang, Multi-View Self-Attention Based Transformer for
Speaker Recognition, in: ICASSP 2022-2022 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), IEEE, 2022, pp. 6732â€“6736.
[14] B. Han, Z. Chen, Y . Qian, Local Information Modeling with
Self-Attention for Speaker Veriï¬cation, in: ICASSP 2022-2022
IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), IEEE, 2022, pp. 6727â€“6731.
[15] V . Mingote, A. Miguel, A. Ortega, E. Lleida, Di er-
entiable Supervector Extraction for Encoding Speaker
and Phrase Information in Text Dependent Speaker Ver-
iï¬cation, Proceedings of IberSPEECH 2018 (2018) 1â€“
5doi:10.21437/IberSPEECH.2018-1 .
URL http://dx.doi.org/10.21437/IberSPEECH.
2018-1
[16] V . Mingote, A. Miguel, A. Ortega, E. Lleida, Supervector Ex-
traction for Encoding Speaker and Phrase Information with Neu-
ral Networks for Text-Dependent Speaker Veriï¬cation, Applied
Sciences 9 (16) (2019) 3295.
[17] V . Mingote, A. Miguel, A. Ortega, E. Lleida, Optimization of
the area under the ROC curve using neural network supervec-
tors for text-dependent speaker veriï¬cation, Computer Speech
& Language 63 (2020) 101078.
[18] D. A. Reynolds, R. C. Rose, et al., Robust text-independent
speaker identiï¬cation using Gaussian mixture speaker models,
IEEE transactions on speech and audio processing 3 (1) (1995)72â€“83.
[19] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker veriï¬cation
using adapted Gaussian mixture models, Digital signal process-
ing 10 (1-3) (2000) 19â€“41.
[20] L. R. Rabiner, A tutorial on hidden Markov models and se-
lected applications in speech recognition, Proceedings of the
IEEE 77 (2) (1989) 257â€“286.
[21] V . Mingote, A. Miguel, A. Ortega, E. Lleida, Memory Lay-
ers with Multi-Head Attention Mechanisms for Text-Dependent
Speaker Veriï¬cation, in: ICASSP 2021 - 2021 IEEE Inter-
national Conference on Acoustics, Speech and Signal Pro-
cessing (ICASSP), 2021, pp. 6154â€“6158. doi:10.1109/
ICASSP39728.2021.9414859 .
[22] G. Lample, A. Sablayrolles, M. Ranzato, L. Denoyer, H. J Â´egou,
Large memory layers with product keys, in: Advances in Neural
Information Processing Systems, 2019, pp. 8546â€“8557.
[23] W. Cai, Z. Cai, X. Zhang, X. Wang, M. Li, A novel learnable
dictionary encoding layer for end-to-end language identiï¬ca-
tion, in: 2018 ICASSP, pp. 5189â€“5193.
[24] G. Hinton, O. Vinyals, J. Dean, Distilling the Knowledge in a
Neural Network, NIPS 2014 Deep Learning Workshop (2015)
1â€“9doi:10.1063/1.4931082 .
URL http://arxiv.org/abs/1503.02531
[25] Z. Zhong, L. Zheng, G. Kang, S. Li, Y . Yang, Random erasing
data augmentation, in: Proceedings of the AAAI Conference on
Artiï¬cial Intelligence, V ol. 34, 2020, pp. 13001â€“13008.
[26] V . Mingote, A. Miguel, D. Ribas, A. Ortega, E. Lleida, Knowl-
edge Distillation and Random Erasing Data Augmentation for
Text-Dependent Speaker Veriï¬cation, in: 2020 IEEE Interna-
tional Conference on Acoustics, Speech and Signal Processing
(ICASSP), IEEE, 2020, pp. 6824â€“6828.
[27] I. Vi Ëœnals, D. Ribas, V . Mingote, J. Llombart, P. Gimeno,
A. Miguel, A. Ortega, E. Lleida, Phonetically-Aware Embed-
dings, Wide Residual Networks with Time-Delay Neural Net-
works and Self Attention Models for the 2018 NIST Speaker
Recognition Evaluation, Proc. Interspeech 2019 (2019) 4310â€“
4314.
[28] C. Blundell, J. Cornebise, K. Kavukcuoglu, D. Wierstra, Weight
uncertainty in neural network, in: International Conference on
Machine Learning, PMLR, 2015, pp. 1613â€“1622.
[29] A. Korattikara, V . Rathod, K. Murphy, M. Welling,
Bayesian Dark Knowledge, arXiv (2015) 1â€“9 doi:
10.1017/CBO9781107415324.004 .
[30] P. Shen, X. Lu, S. Li, H. Kawai, Feature Representation of Short
Utterances Based on Knowledge Distillation for Spoken Lan-
guage Identiï¬cation., in: Interspeech, 2018, pp. 1813â€“1817.
[31] P. Shen, X. Lu, S. Li, H. Kawai, Interactive Learning of Teacher-
student Model for Short Utterance Spoken Language Identiï¬ca-
tion, in: Proc. ICASSP, IEEE, 2019, pp. 5981â€“5985.
[32] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for
image recognition, in: Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 770â€“778.
[33] A. Larcher, K. A. Lee, B. Ma, H. Li, Text-dependent speaker
veriï¬cation: Classiï¬ers, databases and RSR2015, Speech Com-
munication 60 (2014) 56â€“77.
[34] R. K. Das, M. Madhavi, H. Li, Compensating utterance informa-
tion in ï¬xed phrase speaker veriï¬cation, in: 2018 Asia-Paciï¬c
Signal and Information Processing Association Annual Summit
and Conference (APSIPA ASC), IEEE, 2018, pp. 1708â€“1712.
[35] T. Liu, M. C. Madhavi, R. K. Das, H. Li, A Uniï¬ed Framework
for Speaker and Utterance Veriï¬cation., Proc. Interspeech 2019
(2019) 4320â€“4324.
[36] T. Liu, R. K. Das, M. Madhavi, S. Shen, H. Li, Speaker-
Utterance Dual Attention for Speaker and Utterance Veriï¬ca-
tion, Proc. Interspeech 2020 (2020) 4293â€“4297.
12

--- PAGE 13 ---
[37] H. Zeinali, L. Burget, J. Cernocky, A Multi Purpose and Large
Scale Speech Corpus in Persian and English for Speaker and
Speech Recognition: the DeepMine Database, in: Proc. ASRU
2019.
[38] H. Zeinali, K. A. Lee, J. Alam, L. Burget, Short-duration
Speaker Veriï¬cation (SdSV) Challenge 2020: the Challenge
Evaluation Plan., Tech. rep., arXiv preprint arXiv:1912.06311.
[39] V . Panayotov, G. Chen, D. Povey, S. Khudanpur, Librispeech:
An ASR corpus based on public domain audio books, in: 2015
ICASSP, pp. 5206â€“5210.
[40] A. Lozano-Diez, A. Silnova, B. Pulugundla, J. Rohdin,
K. Vesel `y, L. Burget, O. Plchot, O. Glembek, O. Novotn `y,
P. Matejka, BUT Text-Dependent Speaker Veriï¬cation System
for SdSV Challenge 2020., in: INTERSPEECH, 2020, pp. 761â€“
765.
[41] Z. Chen, Y . Lin, Improving X-Vector and PLDA for Text-
Dependent Speaker Veriï¬cation., in: INTERSPEECH, 2020, pp.
726â€“730.
[42] A. Nagrani, J. S. Chung, A. Zisserman, V oxCeleb: A Large-
Scale Speaker Identiï¬cation Dataset, in: Proc. Interspeech 2017,
pp. 2616â€“2620.
[43] J. S. Chung, A. Nagrani, A. Zisserman, V oxCeleb2: Deep
Speaker Recognition, in: Proc. Interspeech 2018, 2018, pp.
1086â€“1090.
[44] The NIST Year 2008 Speaker Recognition Evaluation Plan
(2008).
URL https://www.nist.gov/sites/default/
files/documents/2017/09/26/sre08$_$evalplan$_
$release4.pdf
[45] The NIST Year 2010 Speaker Recognition Evaluation Plan
(2010).
URL https://www.nist.gov/sites/default/
files/documents/itl/iad/mig/NIST$_$SRE10$_
$evalplan-r6.pdf.
[46] B. Desplanques, J. Thienpondt, K. Demuynck, ECAPA-TDNN:
Emphasized Channel Attention, Propagation and Aggregation
in TDNN Based Speaker Veriï¬cation, Proc. Interspeech 2020
(2020) 3830â€“3834.
Victoria Mingote received the
Bachelorâ€™s and Masterâ€™s degree in Telecommunication
Engineering from the University of Zaragoza, Spain,
in 2014 and 2016, respectively. After that, she joined
ViV oLab research group as a Ph.D student and re-
ceived her Ph.D. degree in 2022 from the University of
Zaragoza. She is currently a post-doctoral researcher
in the ViV oLab research group. She has achieved sev-
eral publications of her work in di erent international
journals and conference proceedings. Her research in-
terests expands through the areas of signal processing,
machine learning, multimodal veriï¬cation and identiï¬-
cation (voice and face), and language identiï¬cation.
Antonio Miguel was born in
Zaragoza, Spain. He received the M.Sc. degree in
telecommunication engineering and the Ph.D. degree
from the University of Zaragoza (UZ), Zaragoza, Spain,
in 2001 and 2008, respectively. From 2000 to 2006,
he was with the Communication Technologies Group,
Department of Electronic Engineering and Communi-
cations, UZ, under a research grant. Since 2006, he
has been an Associate Professor in the Department of
Electronic Engineering and Communications, UZ. His
current research interests include acoustic modeling for
speech and speaker recognition.
Alfonso Ortega received the
Telecommunication Engineering and the Ph.D. degrees
from the University of Zaragoza, Zaragoza, Spain, in
2000 and 2005, respectively. He is Associate Director
of the Aragon Institute for Engineering Research (I3A),
University of Zaragoza. In 2006, he was visiting scholar
in the Center for Robust Speech Systems, University of
Texas at Dallas, USA. He has participated in more than
50 research projects funded by national or international
public institutions and more than 30 research projects
for several companies. He is the author of more than
100 papers published in international journals or con-
ference proceedings and several international patents.
He is currently Associate Professor in the Department
of Electronic Engineering and Communications, Uni-
versity of Zaragoza. His research interests include
speech processing, analysis and modeling, automatic
speaker veriï¬cation, and automatic speech recognition.
His Ph.D. thesis, advised by Dr. E. Lleida, received the
Ph.D. Extraordinary Award and the Telefonica Chair
Award to the best technological Ph.D.
13

--- PAGE 14 ---
Eduardo Lleida received the
M.Sc. degree in telecommunication engineering and the
Ph.D. degree in signal processing from the Universitat
Politecnica de Catalunya (UPC), Barcelona, Spain, in
1985 and 1990, respectively. From 1986 to 1988, he
was involved in his doctoral work in the Department of
Signal Theory and Communications, UPC. From 1989
to 1990, he was an Assistant Professor, and from 1991
to 1993, he was an Associated Professor in the De-
partment of Signal Theory and Communications, UPC.
From February 1995 to January 1996, he was with
AT&T Bell Laboratories, Murray Hill, NJ, USA, as a
Consultant in speech recognition. He is currently a full
Professor of signal theory and communications in the
Department of Electronic Engineering and Communi-
cations, University of Zaragoza, Zaragoza, Spain, and
a member of the Aragon Institute for Engineering Re-
search, where he is heading the ViV oLab research group
in speech technologies. He has been the doctoral ad-
visor of 12 doctoral students. He has managed more
than 50 speech-related projects, being inventor in seven
worldwide patents, and coauthored more than 200 tech-
nical papers in the ï¬eld of speech, speaker and language
recognition, speech enhancement and recognition in ad-
verse acoustic environments, acoustic modeling, conï¬-
dence measures, and spoken dialogue systems.
14
