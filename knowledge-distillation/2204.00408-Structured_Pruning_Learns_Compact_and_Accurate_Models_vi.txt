# 2204.00408.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2204.00408.pdf
# Kích thước tệp: 2963951 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cắt tỉa có cấu trúc học được các mô hình nhỏ gọn và chính xác
Mengzhou Xia Zexuan Zhong Danqi Chen
Đại học Princeton
{mengzhou,zzhong,danqic}@cs.princeton.edu
Tóm tắt
Kích thước ngày càng tăng của các mô hình ngôn ngữ thần kinh đã dẫn đến sự quan tâm gia tăng trong việc nén mô hình. Hai phương pháp chủ đạo là cắt tỉa, loại bỏ dần các trọng số từ một mô hình đã được huấn luyện trước, và chưng cất, huấn luyện một mô hình nhỏ gọn hơn để phù hợp với một mô hình lớn hơn. Các phương pháp cắt tỉa có thể giảm đáng kể kích thước mô hình nhưng khó đạt được tăng tốc lớn như chưng cất. Tuy nhiên, các phương pháp chưng cất yêu cầu lượng lớn dữ liệu không được gán nhãn và tốn kém để huấn luyện. Trong công trình này, chúng tôi đề xuất một phương pháp cắt tỉa có cấu trúc đặc thù cho tác vụ CoFi¹ (Cắt tỉa thô và tinh), cung cấp các mạng con có tính song song cao và phù hợp với các phương pháp chưng cất về cả độ chính xác và độ trễ, mà không cần dựa vào bất kỳ dữ liệu không được gán nhãn nào. Hiểu biết chính của chúng tôi là cắt tỉa đồng thời các mô-đun thô (ví dụ: các lớp) và tinh (ví dụ: các đầu và đơn vị ẩn), điều này kiểm soát quyết định cắt tỉa của mỗi tham số với các mặt nạ có độ chi tiết khác nhau. Chúng tôi cũng thiết kế một chiến lược chưng cất theo lớp để chuyển giao kiến thức từ các mô hình chưa cắt tỉa sang các mô hình đã cắt tỉa trong quá trình tối ưu hóa. Các thí nghiệm của chúng tôi trên bộ dữ liệu GLUE và SQuAD cho thấy CoFi tạo ra các mô hình với tăng tốc hơn 10× với sự giảm độ chính xác nhỏ, cho thấy tính hiệu quả và hiệu suất của nó so với các phương pháp cắt tỉa và chưng cất trước đây.²

1 Giới thiệu
Các mô hình ngôn ngữ được huấn luyện trước (Devlin et al., 2019; Liu et al., 2019a; Raffel et al., 2020, cùng các tác giả khác) đã trở thành trụ cột trong xử lý ngôn ngữ tự nhiên. Những mô hình này có chi phí cao về mặt lưu trữ, bộ nhớ và thời gian tính toán và điều này đã thúc đẩy một khối lượng lớn công việc về nén mô hình để làm cho chúng nhỏ hơn và nhanh hơn để sử dụng trong các ứng dụng thực tế (Ganesh et al., 2021).

¹CoFi được phát âm là .
²Mã và mô hình của chúng tôi có sẵn công khai tại https://github.com/princeton-nlp/CoFiPruning.

[BẢNG 1: So sánh các phương pháp chưng cất và cắt tỉa tiên tiến. U và T ký hiệu liệu có sử dụng dữ liệu Không gán nhãn và Đặc thù cho tác vụ cho chưng cất hoặc cắt tỉa. Tăng tốc suy luận (×) được báo cáo so với mô hình BERT cơ sở và chúng tôi đánh giá tất cả các mô hình trên GPU NVIDIA V100 (§4.1). Các mô hình được ghi là z sử dụng mô hình giáo viên khác và không phải là so sánh trực tiếp. Các mô hình nhanh hơn một bậc độ lớn.³]

Hai phương pháp chủ đạo để nén mô hình là cắt tỉa và chưng cất (Bảng 1). Các phương pháp cắt tỉa tìm kiếm một mạng con chính xác trong một mô hình được huấn luyện trước lớn hơn. Nghiên cứu gần đây đã điều tra cách cắt tỉa có cấu trúc các mạng Transformer (Vaswani et al., 2017), từ việc loại bỏ toàn bộ các lớp (Fan et al., 2020; Sajjad et al., 2020), đến cắt tỉa các đầu (Michel et al., 2019; Voita et al., 2019), các chiều trung gian (McCarley et al., 2019; Wang et al., 2020b) và các khối trong ma trận trọng số (Lagunas et al., 2021). Xu hướng của cắt tỉa có cấu trúc nghiêng về việc loại bỏ các đơn vị tinh để cho phép các cấu trúc cuối cùng linh hoạt. Tuy nhiên, cho đến nay, các mô hình được cắt tỉa hiếm khi đạt được tăng tốc lớn (cải thiện tối đa 2-3×).

³Theo công việc trước đây, chúng tôi loại trừ ma trận nhúng trong việc tính toán số lượng tham số. Chúng tôi loại trừ tăng cường dữ liệu đặc thù cho tác vụ để so sánh công bằng. Nhiều kết quả hơn với tăng cường dữ liệu có thể được tìm thấy trong Bảng 3.

arXiv:2204.00408v3 [cs.CL] 2 May 2022

--- TRANG 2 ---
Ngược lại, các phương pháp chưng cất thường đầu tiên chỉ định một kiến trúc mô hình cố định và thực hiện một bước chưng cất tổng quát trên một corpus không được gán nhãn, trước khi tinh chỉnh hoặc chưng cất thêm trên dữ liệu đặc thù cho tác vụ (Sanh et al., 2019; Turc et al., 2019; Sun et al., 2019; Jiao et al., 2020). Các kiến trúc học sinh được thiết kế tốt đạt được sự đánh đổi hiệu suất-tăng tốc hấp dẫn, tuy nhiên việc chưng cất cho các mạng học sinh được khởi tạo ngẫu nhiên này trên dữ liệu không được gán nhãn lớn là cực kỳ chậm.⁴ Ví dụ, TinyBERT (Jiao et al., 2020) đầu tiên được huấn luyện trên 2.500M token trong 3 epoch, điều này yêu cầu huấn luyện 3,5 ngày trên 4 GPU (Hình 1).⁵

Trong công trình này, chúng tôi đề xuất một phương pháp cắt tỉa có cấu trúc đặc thù cho tác vụ được gọi là CoFi (Cắt tỉa thô và tinh) và cho thấy rằng cắt tỉa có cấu trúc có thể đạt được các mạng con rất nhỏ gọn và thu được tăng tốc lớn và độ chính xác cạnh tranh như các phương pháp chưng cất, trong khi yêu cầu ít tính toán hơn nhiều. Hiểu biết chính của chúng tôi là cắt tỉa đồng thời các đơn vị thô (ví dụ: các lớp tự chú ý hoặc feed-forward) và các đơn vị tinh (ví dụ: các đầu, chiều ẩn). Khác với các công trình hiện có, phương pháp của chúng tôi kiểm soát quyết định cắt tỉa của mọi tham số đơn lẻ bằng nhiều mặt nạ có độ chi tiết khác nhau. Đây là chìa khóa cho việc nén lớn, vì nó cho phép sự linh hoạt lớn nhất của các cấu trúc được cắt tỉa và làm dễ dàng việc tối ưu hóa so với chỉ cắt tỉa các đơn vị nhỏ.

Người ta biết rằng cắt tỉa với một mục tiêu chưng cất có thể cải thiện hiệu suất đáng kể (Sanh et al., 2020; Lagunas et al., 2021). Khác với một kiến trúc học sinh cố định, các cấu trúc được cắt tỉa không được biết trước khi huấn luyện và việc chưng cất giữa các lớp trung gian của các mô hình chưa cắt tỉa và đã cắt tỉa là thách thức (Jiao et al., 2020). Do đó, chúng tôi đề xuất một phương pháp chưng cất theo lớp, học động thái về ánh xạ lớp giữa hai cấu trúc. Chúng tôi cho thấy rằng chiến lược này có thể dẫn đến cải thiện hiệu suất tốt hơn ngoài việc chưng cất lớp dự đoán đơn giản.

Các thí nghiệm của chúng tôi cho thấy CoFi cung cấp các mô hình chính xác hơn ở tất cả các mức độ tăng tốc và kích thước mô hình trên bộ dữ liệu GLUE (Wang et al., 2019) và SQuAD v1.1 (Rajpurkar et al., 2016), so với các baseline cắt tỉa và chưng cất mạnh. Cụ thể, nó đạt được tăng tốc hơn 10× và độ thưa 95% trên tất cả các bộ dữ liệu trong khi bảo tồn hơn 90% độ chính xác. Kết quả của chúng tôi cho thấy rằng cắt tỉa có cấu trúc đặc thù cho tác vụ là một giải pháp hấp dẫn trong thực tế, tạo ra các mô hình nhỏ hơn và nhanh hơn mà không cần dữ liệu không được gán nhãn bổ sung cho chưng cất tổng quát.

⁴Có những ngoại lệ như DistillBERT (Sanh et al., 2020), khởi tạo học sinh từ giáo viên bằng cách lấy một lớp trong hai, tuy nhiên không rõ cách tổng quát hóa sơ đồ khởi tạo này cho các cấu trúc nhỏ gọn khác.
⁵Xem chi tiết đo thời gian huấn luyện trong Phụ lục J.

2 Nền tảng

2.1 Transformers
Một mạng Transformer (Vaswani et al., 2017) được cấu thành từ L khối và mỗi khối bao gồm một lớp tự chú ý đa đầu (MHA) và một lớp feed-forward (FFN). Một lớp MHA với Nh đầu nhận đầu vào X và xuất ra:

MHA(X) = ∑_{i=1}^{Nh} Att(W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}, W_O^{(i)}, X);

trong đó W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}, W_O^{(i)} ∈ R^{d×dh} ký hiệu các ma trận query, key, value và output tương ứng và Att() là một hàm chú ý. Ở đây d ký hiệu kích thước ẩn (ví dụ: 768) và dh = d/Nh ký hiệu chiều đầu ra của mỗi đầu (ví dụ: 64).

Tiếp theo là một lớp feed-forward, bao gồm một lớp up-projection và down-projection, được tham số hóa bởi W^U ∈ R^{d×df} và W^D ∈ R^{df×d}:

FFN(X) = gelu(XW^U)W^D.

Thông thường, df = 4d. Cũng có một kết nối dư và một hoạt động chuẩn hóa lớp sau mỗi lớp MHA và FFN.

MHAs, FFNs chiếm 1/3 và 2/3 tham số mô hình trong Transformers (loại trừ nhúng). Theo Ganesh et al. (2021), cả MHAs và FFNs đều mất thời gian tương tự trên GPU trong khi FFNs trở thành nút thắt cổ chai trên CPU.

2.2 Chưng cất
Chưng cất kiến thức (Hinton et al., 2015) là một phương pháp nén mô hình chuyển giao kiến thức từ một mô hình giáo viên lớn hơn sang một mô hình học sinh nhỏ hơn. Chưng cất tổng quát (Sanh et al., 2019; Sun et al., 2020; Wang et al., 2020a) và chưng cất đặc thù cho tác vụ (Sun et al., 2019) khai thác dữ liệu không được gán nhãn và dữ liệu đặc thù cho tác vụ tương ứng để chuyển giao kiến thức. Sự kết hợp của hai phương pháp này dẫn đến hiệu suất tăng cao (Jiao et al., 2020). Chưng cất tổng quát hoặc huấn luyện trước mạng học sinh trên corpus không được gán nhãn là cần thiết để giữ lại hiệu suất trong khi tốn kém về mặt tính toán (Turc et al., 2019; Jiao et al., 2020).

--- TRANG 3 ---
[Hình 1: So sánh (a) TinyBERT (Jiao et al., 2020) và (b) phương pháp cắt tỉa CoFi của chúng tôi. TinyBERT huấn luyện một mạng được khởi tạo ngẫu nhiên thông qua chưng cất hai bước: (1) chưng cất tổng quát trên một corpus không được gán nhãn lớn, mất 3,5 ngày để hoàn thành trên 4 GPU, và (2) chưng cất đặc thù cho tác vụ trên bộ dữ liệu tác vụ. CoFi trực tiếp cắt tỉa mô hình BERT đã được tinh chỉnh và đồng thời học năm loại biến mặt nạ (tức là z_{FFN}, z_{int}, z_{MHA}, z_{head}, z_{hidn}) để cắt tỉa các loại đơn vị khác nhau (§3.1). CoFi mất tối đa 20 giờ để hoàn thành trên 1 GPU trên tất cả các bộ dữ liệu GLUE (các bộ dữ liệu nhỏ hơn cần <3 giờ).⁶]

Các mục tiêu chưng cất khác nhau cũng đã được khám phá. Ngoài chưng cất tiêu chuẩn từ lớp dự đoán (Hinton et al., 2015), việc chuyển giao kiến thức theo từng lớp từ các biểu diễn (Jiao et al., 2020; Sun et al., 2020) và ma trận chú ý đa đầu (Wang et al., 2020a; Jiao et al., 2020; Sun et al., 2020) dẫn đến cải thiện đáng kể. Hầu hết các phương pháp chưng cất giả định một cấu trúc học sinh cố định trước khi huấn luyện. Hou et al. (2020) cố gắng chưng cất cho một cấu trúc động với chiều rộng và chiều cao được chỉ định. Yin et al. (2021) áp dụng một giải pháp Neural Architecture Search một lần để tìm kiếm kiến trúc của các mạng học sinh.

2.3 Cắt tỉa
Cắt tỉa dần loại bỏ các tham số dư thừa từ một mô hình giáo viên, chủ yếu tạo ra các mô hình đặc thù cho tác vụ. Các công trình trước đây tập trung vào cắt tỉa các thành phần khác nhau trong các mô hình Transformer, từ các đơn vị thô đến tinh.

Cắt tỉa lớp Fan et al. (2020) và Sajjad et al. (2020) khám phá các chiến lược để bỏ toàn bộ các khối Transformer (một cặp lớp MHA và FFN) từ một mô hình được huấn luyện trước. Bằng chứng thực nghiệm cho thấy rằng 50% các lớp có thể được bỏ mà không làm giảm độ chính xác nhiều, dẫn đến tăng tốc 2×.

Cắt tỉa đầu Voita et al. (2019); Michel et al. (2019) cho thấy rằng chỉ có một tập con nhỏ các đầu là quan trọng và phần lớn có thể được cắt tỉa. Chúng tôi theo các công trình này để che các đầu bằng cách giới thiệu các biến z_{head}^{(i)} ∈ {0,1} cho chú ý đa đầu:

MHA(X) = ∑_{i=1}^{Nh} z_{head}^{(i)} Att(W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}, W_O^{(i)}, X).

Chỉ loại bỏ các đầu không dẫn đến cải thiện độ trễ lớn—Li et al. (2021) chứng minh tăng tốc 1,4× với chỉ một đầu còn lại trên mỗi lớp.

Cắt tỉa FFN Phần chính khác—các lớp feed-forward (FFNs)—cũng được biết là có quá nhiều tham số. Các chiến lược để cắt tỉa một lớp FFN để tăng tốc suy luận bao gồm cắt tỉa toàn bộ lớp FFN (Prasanna et al., 2020; Chen et al., 2020b) và ở mức tinh hơn, cắt tỉa các chiều trung gian (McCarley et al., 2019; Hou et al., 2020) bằng cách giới thiệu z_{int} ∈ {0,1}^{df}:

FFN(X) = gelu(XW^U)diag(z_{int})W^D.

Cắt tỉa khối và phi cấu trúc Gần đây hơn, việc cắt tỉa trên một đơn vị nhỏ hơn, các khối, từ MHAs và FFNs đã được khám phá (Lagunas et al., 2021). Tuy nhiên, việc tối ưu hóa các mô hình với các khối được cắt tỉa là khó khăn cho đến nay: Yao et al. (2021) cố gắng tối ưu hóa các mô hình cắt tỉa khối với kernel MatMul thưa khối được cung cấp bởi Triton (Tillet et al., 2019), nhưng các kết quả được báo cáo không cạnh tranh. Tương tự, cắt tỉa phi cấu trúc nhằm loại bỏ các trọng số riêng lẻ và đã được nghiên cứu rộng rãi

⁶CoFi yêu cầu thời gian huấn luyện hơi dài hơn so với chưng cất đặc thù cho tác vụ của TinyBERT, vì CoFi tìm kiếm cấu trúc mô hình và học tham số đồng thời.

--- TRANG 4 ---
trong tài liệu (Chen et al., 2020a; Huang et al., 2021). Mặc dù độ thưa đạt tới 97% (Sanh et al., 2020), việc thu được tăng tốc suy luận trên phần cứng hiện tại là khó khăn.

Kết hợp với chưng cất Cắt tỉa thường được kết hợp với một mục tiêu chưng cất lớp dự đoán (Sanh et al., 2020; Lagunas et al., 2021). Tuy nhiên không rõ cách áp dụng các chiến lược chưng cất theo lớp vì kiến trúc của mô hình học sinh được cắt tỉa phát triển trong quá trình huấn luyện.

3 Phương pháp
Chúng tôi đề xuất một phương pháp cắt tỉa có cấu trúc CoFi, cắt tỉa đồng thời các đơn vị Thô và Tinh (§3.1) với một mục tiêu chưng cất theo lớp chuyển giao kiến thức từ các mô hình chưa cắt tỉa sang các mô hình đã cắt tỉa (§3.2). Sự kết hợp của hai phương pháp này dẫn đến các mô hình được nén cao với tăng tốc suy luận lớn.

3.1 Cắt tỉa thô và tinh
Xu hướng gần đây trong cắt tỉa có cấu trúc chuyển hướng cắt tỉa các đơn vị nhỏ hơn để linh hoạt mô hình. Cắt tỉa các đơn vị tinh tự nhiên đòi hỏi cắt tỉa các đơn vị thô—ví dụ, cắt tỉa Nh (ví dụ: 12) đầu tương đương với cắt tỉa một lớp MHA hoàn chỉnh. Tuy nhiên, chúng tôi quan sát thấy điều này hiếm khi xảy ra trong thực tế và gây khó khăn cho việc tối ưu hóa đặc biệt ở chế độ thưa cao.

Để khắc phục vấn đề này, chúng tôi trình bày một giải pháp đơn giản: chúng tôi cho phép cắt tỉa các lớp MHA và FFN một cách rõ ràng cùng với các đơn vị tinh (như được hiển thị trong §2.3) bằng cách giới thiệu hai mặt nạ bổ sung z_{MHA} và z_{FFN} cho mỗi lớp. Bây giờ lớp tự chú ý đa đầu và lớp feed-forward trở thành:

MHA(X) = z_{MHA} ∑_{i=1}^{Nh} (z_{head}^{(i)} Att(W_Q^{(i)}, W_K^{(i)}, W_V^{(i)}, W_O^{(i)}, X));

FFN(X) = z_{FFN} gelu(XW^U)diag(z_{int})W^D.

Với các mặt nạ lớp này, chúng tôi cắt tỉa rõ ràng toàn bộ một lớp, thay vì cắt tỉa tất cả các đầu trong một lớp MHA (hoặc tất cả các chiều trung gian trong một lớp FFN). Khác với các chiến lược bỏ lớp trong Fan et al. (2020); Sajjad et al. (2020), chúng tôi bỏ các lớp MHA và FFN riêng biệt, thay vì cắt tỉa chúng như một tổng thể.

Hơn nữa, chúng tôi cũng xem xét cắt tỉa các chiều đầu ra của MHA(X) và FFN(X), được gọi là 'chiều ẩn' trong bài báo này, để cho phép linh hoạt hơn trong cấu trúc mô hình cuối cùng. Chúng tôi định nghĩa một tập hợp các mặt nạ z_{hidn} ∈ {0,1}^d, được chia sẻ giữa các lớp vì mỗi chiều trong một biểu diễn ẩn được kết nối với cùng một chiều trong lớp tiếp theo thông qua một kết nối dư. Các biến mặt nạ này được áp dụng cho tất cả các ma trận trọng số trong mô hình, ví dụ: diag(z_{hidn})W_Q. Thực nghiệm, chúng tôi thấy rằng chỉ có một số ít chiều được cắt tỉa (ví dụ: 768→760), nhưng nó vẫn giúp cải thiện hiệu suất đáng kể (§4.3).

CoFi khác với các phương pháp cắt tỉa trước đây ở chỗ nhiều biến mặt nạ cùng kiểm soát quyết định cắt tỉa của một tham số đơn lẻ. Ví dụ, một trọng số trong một lớp FFN bị cắt tỉa khi toàn bộ lớp FFN, hoặc chiều trung gian tương ứng của nó, hoặc chiều ẩn bị cắt tỉa. Để so sánh, một công trình gần đây Block Pruning (Lagunas et al., 2021) áp dụng một phương pháp lai áp dụng một chiến lược cắt tỉa duy nhất trên MHAs và FFNs riêng biệt.

Để học các biến mặt nạ này, chúng tôi sử dụng chính quy hóa l0 được mô hình hóa với các phân phối concrete cứng theo Louizos et al. (2018). Chúng tôi cũng theo Wang et al. (2020b) để thay thế mục tiêu l0 vanilla bằng một nhân tử Lagrangian để kiểm soát tốt hơn độ thưa mong muốn của các mô hình được cắt tỉa.⁷ Chúng tôi điều chỉnh hàm thưa tương ứng để phù hợp với các mặt nạ cắt tỉa có độ chi tiết khác nhau:

ŝ = (1/M)[4dh ∑_i^L ∑_j^{Nh} ∑_k^d z_{MHA}^{(i)} z_{head}^{(i,j)} z_{hidden}^{(k)} + (1/M) 2 ∑_i^L ∑_j^{df} ∑_k^d z_{FFN}^{(i)} z_{int}^{(i,j)} z_{hidden}^{(k)}];

trong đó ŝ là độ thưa mong đợi và M ký hiệu kích thước mô hình đầy đủ. Tất cả các biến che được học như các số thực trong [0,1] trong quá trình huấn luyện và chúng tôi ánh xạ các biến che dưới một ngưỡng về 0 trong quá trình suy luận và có được một cấu trúc được cắt tỉa cuối cùng trong đó ngưỡng được xác định bởi độ thưa mong đợi của mỗi ma trận trọng số (xem Phụ lục B để biết thêm chi tiết).

3.2 Chưng cất cho các mô hình được cắt tỉa
Công trình trước đây đã cho thấy rằng việc kết hợp chưng cất với cắt tỉa cải thiện hiệu suất, trong đó mục tiêu chưng cất chỉ liên quan đến một mất mát cross-entropy giữa phân phối xác suất đầu ra của học sinh được cắt tỉa và của giáo viên p_s và p_t (Sanh et al., 2020; Lagunas et al., 2021):

L_{pred} = D_{KL}(p_s || p_t).

Ngoài chưng cất lớp dự đoán, các công trình gần đây cho thấy lợi ích lớn trong việc chưng cất các lớp trung gian (Sun et al., 2019; Jiao et al., 2020). Trong bối cảnh các phương pháp chưng cất, kiến trúc của mô hình học sinh được xác định trước và việc định nghĩa một ánh xạ lớp giữa mô hình học sinh và giáo viên là đơn giản. Ví dụ, mô hình TinyBERT 4 lớp chưng cất từ lớp 3, 6, 9 và 12 của một mô hình giáo viên 12 lớp.

Tuy nhiên, việc chưng cất các lớp trung gian trong quá trình cắt tỉa là thách thức vì cấu trúc mô hình thay đổi trong suốt quá trình huấn luyện.

Chúng tôi đề xuất một phương pháp chưng cất theo lớp cho cắt tỉa để tận dụng tốt nhất các tín hiệu từ mô hình giáo viên. Thay vì định nghĩa trước một ánh xạ lớp cố định, chúng tôi tìm kiếm động một ánh xạ lớp giữa mô hình giáo viên đầy đủ và mô hình học sinh được cắt tỉa. Cụ thể, cho T ký hiệu một tập hợp các lớp giáo viên mà chúng tôi sử dụng để chưng cất kiến thức cho mô hình học sinh. Chúng tôi định nghĩa một hàm ánh xạ lớp m(), tức là m(i) đại diện cho lớp học sinh chưng cất từ lớp giáo viên i. Mất mát chưng cất lớp ẩn được định nghĩa là

L_{layer} = ∑_{i∈T} MSE(W_{layer}H_s^{m(i)}, H_t^i);

trong đó W_{layer} ∈ R^{d×d} là một ma trận biến đổi tuyến tính, được khởi tạo như một ma trận đơn vị. H_s^{m(i)}, H_t^i là các biểu diễn ẩn từ lớp FFN học sinh thứ m(i) và lớp FFN giáo viên thứ i. Hàm ánh xạ lớp m() được xác định động trong quá trình huấn luyện để khớp một lớp giáo viên với lớp gần nhất của nó trong mô hình học sinh:

m(i) = arg min_{j: z_{FFN}^{(j)} > 0} MSE(W_{layer}H_s^j, H_t^i).

Việc tính toán khoảng cách giữa hai tập hợp lớp có tính song song cao và đưa ra một chi phí huấn luyện tối thiểu. Để giải quyết vấn đề không khớp lớp, chủ yếu xảy ra cho các bộ dữ liệu kích thước nhỏ, ví dụ: RTE, MRPC, chúng tôi thêm một ràng buộc để chỉ cho phép khớp một lớp giáo viên với một lớp học sinh thấp hơn so với lớp học sinh được khớp trước đó. Khi cắt tỉa với các bộ dữ liệu kích thước lớn hơn, không khớp lớp hiếm khi xảy ra, cho thấy tính ưu việt của việc khớp động—các lớp giữa mô hình học sinh và giáo viên khớp theo cách có lợi cho quá trình cắt tỉa nhất.

Cuối cùng, chúng tôi kết hợp chưng cất lớp với chưng cất lớp dự đoán:

L_{distil} = λL_{pred} + (1-λ)L_{layer};

trong đó λ kiểm soát đóng góp của mỗi mất mát.

⁷Chúng tôi cũng đã thử một ước lượng straight-through như được đề xuất trong Sanh et al. (2020) và thấy hiệu suất tương đương. Chúng tôi chọn chính quy hóa l0 vì nó dễ dàng kiểm soát độ thưa một cách chính xác.

4 Thí nghiệm

4.1 Thiết lập
Bộ dữ liệu Chúng tôi đánh giá phương pháp của chúng tôi trên tám tác vụ GLUE (Wang et al., 2019) và SQuAD v1.1 (Rajpurkar et al., 2016). Các tác vụ GLUE bao gồm SST-2 (Socher et al., 2013), MNLI (Williams et al., 2018), QQP, QNLI, MRPC (Dolan and Brockett, 2005), CoLA (Warstadt et al., 2019), STS-B (Cer et al., 2017) và RTE (xem Phụ lục D để biết kích thước bộ dữ liệu và các thước đo).

Thiết lập huấn luyện Trong các thí nghiệm của chúng tôi, độ thưa được tính như số lượng tham số được cắt tỉa chia cho kích thước mô hình đầy đủ (loại trừ nhúng). Theo Wang et al. (2020b); Lagunas et al. (2021), đầu tiên chúng tôi tinh chỉnh mô hình với mục tiêu chưng cất, sau đó chúng tôi tiếp tục huấn luyện mô hình với mục tiêu cắt tỉa với một bộ lập lịch để tăng tuyến tính độ thưa đến giá trị mục tiêu. Chúng tôi tinh chỉnh mô hình được cắt tỉa cho đến khi hội tụ (xem Phụ lục A để biết thêm chi tiết huấn luyện).

Chúng tôi huấn luyện các mô hình với độ thưa mục tiêu {60%, 70%, 75%, 80%, 85%, 90%, 95%} trên mỗi bộ dữ liệu. Đối với tất cả các thí nghiệm, chúng tôi bắt đầu từ mô hình BERT cơ sở⁸ và đóng băng trọng số nhúng theo Sanh et al. (2020). Chúng tôi báo cáo kết quả trên các tập phát triển của tất cả các bộ dữ liệu.

Baseline Chúng tôi so sánh CoFi với một số baseline: DistillBERT 6 (Sanh et al., 2019), TinyBERT 6 và TinyBERT 4 (Jiao et al., 2020), DynaBERT (Hou et al., 2020), và Block Pruning (Lagunas et al., 2021) (xem Phụ lục C để biết chi tiết). Chúng tôi cũng so sánh với các phương pháp cắt tỉa khác như FLOP (Wang et al., 2020b), LayerDrop (Fan et al., 2020), Movement Pruning (Sanh et al., 2020) và các phương pháp chưng cất như

⁸Chúng tôi cũng chạy thí nghiệm trên các mô hình RoBERTa (Liu et al., 2019a). Vui lòng tham khảo Phụ lục I để biết chi tiết.

--- TRANG 5 ---
[Hình 2: Độ chính xác so với tăng tốc (trên) hoặc kích thước mô hình (dưới). Chúng tôi so sánh CoFi với các baseline chưng cất và cắt tỉa tiên tiến. Lưu ý rằng chúng tôi loại trừ kích thước nhúng khi tính toán kích thước mô hình theo công việc trước đây, vì chuyển tiếp qua lớp nhúng có ít ảnh hưởng đến thời gian suy luận.]

[Bảng 2: CoFi so với các mô hình TinyBERT 4 (Jiao et al., 2020) với tăng tốc 10×. GD: chưng cất tổng quát, chưng cất mô hình học sinh trên một corpus không được gán nhãn lớn. Thời gian huấn luyện được đo bằng giờ GPU (xem Phụ lục J để biết chi tiết). Số lượng tham số cho cả hai mô hình đều khoảng 5M (khoảng 95% độ thưa). CoFi thu hẹp khoảng cách giữa chưng cất và cắt tỉa với tính toán ít hơn đáng kể. Lưu ý rằng chúng tôi loại bỏ tăng cường dữ liệu từ TinyBERT để so sánh công bằng, xem Bảng 3 cho các thí nghiệm với dữ liệu được tăng cường.]

[Bảng 3: CoFi so với TinyBERT 4 được huấn luyện với tăng cường dữ liệu đặc thù cho tác vụ được giới thiệu trong Jiao et al. (2020). Tất cả các mô hình đều có khoảng 5M tham số (~95% độ thưa) và đạt được tăng tốc tương tự (11-12×). Các số trước → là không có tăng cường dữ liệu.]

MobileBERT (Sun et al., 2020) và AutoTinyBERT (Yin et al., 2021) trong Phụ lục F.⁹

Đối với TinyBERT và DynaBERT, các mô hình được phát hành được huấn luyện với dữ liệu được tăng cường đặc thù cho tác vụ. Để so sánh công bằng, chúng tôi huấn luyện hai mô hình này với mã được phát hành mà không có tăng cường dữ liệu.¹⁰ Đối với Block Pruning, chúng tôi huấn luyện các mô hình với các checkpoint được phát hành trên các tác vụ GLUE và sử dụng kết quả SQuAD từ bài báo.

Đánh giá tăng tốc Tỷ lệ tăng tốc là một phép đo chính mà chúng tôi sử dụng trong suốt bài báo vì tỷ lệ nén không nhất thiết phản ánh cải thiện thực tế trong độ trễ suy luận.¹¹ Chúng tôi sử dụng BERT cơ sở chưa được cắt tỉa làm baseline và đánh giá tất cả các mô hình với cùng thiết lập phần cứng trên một GPU NVIDIA V100 duy nhất để đo tăng tốc suy luận. Kích thước đầu vào là 128 cho các tác vụ GLUE và 384 cho SQuAD, và chúng tôi sử dụng kích thước batch là 128. Lưu ý rằng kết quả có thể khác với các bài báo gốc vì môi trường cho mỗi nền tảng là khác nhau.

4.2 Kết quả chính
Hiệu suất tổng thể Trong Hình 2, chúng tôi so sánh độ chính xác của các mô hình CoFi với các phương pháp khác về cả tăng tốc suy luận và kích thước mô hình. CoFi cung cấp các mô hình chính xác hơn so với các baseline chưng cất và cắt tỉa ở mọi mức tăng tốc và kích thước mô hình. Block Pruning (Lagunas et al., 2021), một công trình gần đây cho thấy hiệu suất mạnh so với TinyBERT 6, không thể đạt được tăng tốc tương đương như TinyBERT 4. Thay vào đó, CoFi có tùy chọn cắt tỉa cả lớp và đầu & đơn vị trung gian và có thể đạt được một mô hình với hiệu suất tương đương hoặc cao hơn so với TinyBERT 4 và tất cả các mô hình khác. Ngoài ra, DynaBERT hoạt động kém hơn nhiều về tốc độ vì nó bị hạn chế loại bỏ tối đa một nửa số lớp MHA và FFN.

So sánh với TinyBERT 4 Trong Bảng 2, chúng tôi cho thấy CoFi tạo ra các mô hình với tăng tốc suy luận hơn 10× và đạt được hiệu suất tương đương hoặc thậm chí tốt hơn TinyBERT 4. Chưng cất tổng quát (GD), chưng cất thông tin từ một corpus lớn, là cần thiết để huấn luyện các mô hình chưng cất, đặc biệt đối với các bộ dữ liệu kích thước nhỏ (ví dụ: TinyBERT 4 w/o GD hoạt động kém trên CoLA, RTE và STS-B). Trong khi chưng cất tổng quát có thể mất tới hàng trăm giờ GPU để huấn luyện, CoFi huấn luyện tối đa 20 giờ trên một bộ dữ liệu đặc thù cho tác vụ với một GPU duy nhất. Chúng tôi cho rằng các phương pháp cắt tỉa—được huấn luyện với các mục tiêu chưng cất như CoFi—tiết kiệm và hiệu quả hơn trong việc đạt được các mô hình được nén.

Chúng tôi tiếp tục so sánh CoFi với TinyBERT 4 trong bối cảnh tăng cường dữ liệu trong Bảng 3. Vì bộ dữ liệu được tăng cường không được phát hành công khai, chúng tôi làm theo kho lưu trữ GitHub của nó để tạo dữ liệu được tăng cường của riêng chúng tôi. Chúng tôi huấn luyện CoFi với cùng một tập dữ liệu được tăng cường và thấy rằng nó vẫn vượt trội hơn TinyBERT 4 trên hầu hết các bộ dữ liệu.¹²

4.3 Nghiên cứu loại bỏ
Đơn vị cắt tỉa Đầu tiên chúng tôi tiến hành một nghiên cứu loại bỏ để điều tra cách các đơn vị cắt tỉa bổ sung như các lớp MHA, lớp FFN và đơn vị ẩn trong CoFi ảnh hưởng đến hiệu suất mô hình và tăng tốc suy luận ngoài thực hành tiêu chuẩn của việc cắt tỉa đầu và chiều FFN. Chúng tôi hiển thị kết quả trong Bảng 4 cho các mô hình có kích thước tương tự. Loại bỏ tùy chọn cắt tỉa chiều ẩn (z_{hidn}) dẫn đến một mô hình hơi nhanh hơn với sự giảm hiệu suất trên toàn bộ và chúng tôi thấy rằng nó loại bỏ nhiều lớp hơn CoFi và không dẫn đến hiệu suất tối ưu dưới một ràng buộc độ thưa cụ thể. Ngoài ra, loại bỏ các mặt nạ lớp (z_{MHA}, z_{FFN}) mang lại sự giảm đáng kể trong tăng tốc trên các mô hình được nén cao (95%, 5M). Kết quả này cho thấy rằng ngay cả với cùng một lượng tham số, các cấu hình khác nhau cho một mô hình có thể dẫn đến tăng tốc khác biệt đáng kể. Tuy nhiên, nó không ảnh hưởng đến chế độ độ thưa thấp (60%, 34M). Tóm lại, bằng cách đặt các biến che ở các mức khác nhau, quy trình tối ưu hóa được khuyến khích cắt tỉa các đơn vị tương ứng dưới ràng buộc độ thưa trong khi tối đa hóa hiệu suất mô hình.

Mục tiêu chưng cất Chúng tôi cũng loại bỏ các mục tiêu chưng cất để xem mỗi phần đóng góp như thế nào vào hiệu suất của CoFi trong Bảng 5. Đầu tiên chúng tôi quan sát thấy rằng loại bỏ hoàn toàn chưng cất dẫn đến sự giảm hiệu suất tới 1,9-6,8 điểm trên các bộ dữ liệu khác nhau, cho thấy sự cần thiết của việc kết hợp cắt tỉa và chưng cất để duy trì hiệu suất. Mục tiêu chưng cất lớp ẩn được đề xuất khớp động các lớp từ mô hình giáo viên với mô hình học sinh. Chúng tôi cũng thử nghiệm với một giải pháp đơn giản tức là "Chưng cất ẩn cố định", khớp mỗi lớp từ mô hình giáo viên với lớp tương ứng trong mô hình học sinh – nếu một lớp đã được cắt tỉa, mục tiêu chưng cất sẽ không được thêm vào. Chúng tôi thấy rằng chưng cất ẩn cố định kém hiệu quả hơn mục tiêu khớp lớp động được sử dụng cho CoFi. Thú vị là, mục tiêu khớp lớp động được đề xuất luôn hội tụ đến một sự sắp xếp cụ thể giữa các lớp của mô hình giáo viên và mô hình học sinh. Ví dụ, chúng tôi thấy rằng trên QNLI, quá trình huấn luyện khớp động các lớp 3, 6, 9, 12 trong mô hình giáo viên với các lớp 1, 2, 4, 9 trong mô hình học sinh.¹³ Hơn nữa, như được hiển thị trong bảng, việc loại bỏ nó làm tổn hại hiệu suất cho tất cả các bộ dữ liệu ngoại trừ SST-2.

⁹Chúng tôi hiển thị các kết quả này trong Phụ lục F vì chúng không thể so sánh trực tiếp với CoFi.
¹⁰Đối với TinyBERT, dữ liệu được tăng cường lớn hơn 20× so với dữ liệu gốc, làm cho quá trình huấn luyện chậm hơn đáng kể.
¹¹Các mô hình có cùng tỷ lệ nén có thể có tăng tốc khác biệt đáng kể.
¹²Chúng tôi chỉ tiến hành thí nghiệm với tăng cường dữ liệu trên bốn bộ dữ liệu vì huấn luyện trên dữ liệu được tăng cường rất tốn kém. Ví dụ, huấn luyện trên bộ dữ liệu được tăng cường cho MNLI mất hơn 200 giờ GPU tổng cộng. Xem thêm chi tiết trong Phụ lục E.
¹³Vui lòng tham khảo §G.1 để biết thêm chi tiết.

--- TRANG 6 ---
[Bảng 4: Nghiên cứu loại bỏ về các đơn vị cắt tỉa trên QNLI, MNLI và SQuAD. ×: tăng tốc. Các mô hình được cắt tỉa với độ thưa 60% và 95% có kích thước mô hình lần lượt là 34M và 5M. layer: Khi chúng tôi không cắt tỉa toàn bộ các lớp (không có z_{MHA} hoặc z_{FFN}), tăng tốc giảm đáng kể đối với độ thưa cao ví dụ: 95%; hidden: khi chúng tôi loại bỏ các biến mặt nạ tương ứng với các đơn vị ẩn (z_{hidn}), chúng tôi quan sát thấy sự giảm đáng kể về độ chính xác.]

[Bảng 5: Nghiên cứu loại bỏ về các mục tiêu chưng cất khác nhau trên các mô hình được cắt tỉa với độ thưa = 95%. Chưng cất ẩn cố định: đơn giản khớp mỗi lớp của học sinh và giáo viên, xem §4.3 để biết thêm chi tiết. Trong §G.2, chúng tôi cho thấy rằng mục tiêu chưng cất lớp động cải thiện hiệu suất mô hình đáng kể hơn ở các tỷ lệ độ thưa thấp hơn.]

4.4 Cấu trúc của các mô hình được cắt tỉa
Cuối cùng, chúng tôi nghiên cứu các cấu trúc được cắt tỉa do CoFi tạo ra. Chúng tôi đặc trưng hóa các mô hình được cắt tỉa với độ thưa {60%, 70%, 80%, 90%, 95%} trên năm bộ dữ liệu. Đối với mỗi thiết lập, chúng tôi chạy CoFi ba lần. Hình 3 minh họa số lượng đầu và chiều trung gian còn lại của các mô hình được cắt tỉa cho các độ thưa khác nhau.¹⁴ Thú vị là, chúng tôi khám phá ra các mẫu cấu trúc chung trong các mô hình được cắt tỉa: (1) Các lớp feed-forward bị cắt tỉa đáng kể trên tất cả các độ thưa. Ví dụ, ở mức độ thưa 60%, số lượng chiều trung gian trung bình trong các lớp FFN sau khi cắt tỉa giảm 71% (3.072→884), và số lượng đầu trung bình trong MHA giảm 39% (12→7,3). Điều này cho thấy các lớp FFN dư thừa hơn các lớp MHA. (2) CoFi có xu hướng cắt tỉa các mô-đun con nhiều hơn từ các lớp trên so với các lớp dưới. Ví dụ, các lớp MHA trên có ít đầu còn lại hơn các lớp dưới trung bình.

Hơn nữa, chúng tôi nghiên cứu số lượng lớp FFN và MHA còn lại và hiển thị kết quả trong Bảng 6 cho các mô hình được nén cao (độ thưa = 95%). Mặc dù tất cả các mô hình đều có kích thước gần như nhau, chúng trình bày các mẫu khác nhau trên các bộ dữ liệu, cho thấy rằng tồn tại các mạng con tối ưu khác nhau cho mỗi bộ dữ liệu. Chúng tôi thấy rằng trên SST-2 và QNLI, lớp MHA đầu tiên được bảo tồn nhưng có thể được loại bỏ trên QQP và SQuAD. Chúng tôi cũng quan sát thấy rằng một số lớp đặc biệt quan trọng trên tất cả các bộ dữ liệu. Ví dụ, lớp MHA đầu tiên và lớp MHA thứ hai được bảo tồn hầu hết thời gian, trong khi các lớp ở giữa thường bị loại bỏ. Nói chung, các mô hình được cắt tỉa chứa nhiều lớp MHA hơn lớp FFN (xem Phụ lục H), cho thấy rằng các lớp MHA quan trọng hơn để giải quyết các tác vụ downstream. Tương tự như Press et al. (2020), chúng tôi thấy rằng mặc dù các mạng Transformer tiêu chuẩn có các lớp FFN và MHA xen kẽ, trong các mô hình được cắt tỉa của chúng tôi, các lớp FFN/MHA liền kề có thể dẫn đến hiệu suất tốt hơn.

[Hình 3: Các chiều trung gian trung bình tại mỗi lớp FFN và số lượng đầu trung bình tại mỗi lớp MHA trong các mô hình được cắt tỉa trên năm bộ dữ liệu (SST-2, MNLI, QQP, QNLI, và SQuAD 1.1). Chúng tôi nghiên cứu các độ thưa khác nhau {60%, 70%, 80%, 90%, 95%}.]

[Bảng 6: Các lớp còn lại trong các mô hình được cắt tỉa bởi CoFi trên các bộ dữ liệu khác nhau. Tất cả các mô hình được cắt tỉa ở độ thưa 95%. Đối với mỗi thiết lập, chúng tôi chạy thí nghiệm ba lần để có được ba mô hình được cắt tỉa khác nhau. M đại diện cho một lớp MHA còn lại và F đại diện cho một lớp FFN còn lại.]

¹⁴Chúng tôi hiển thị thêm phân tích lớp trong Phụ lục H.

5 Công trình liên quan
Cắt tỉa có cấu trúc đã được khám phá rộng rãi trong thị giác máy tính, trong đó cắt tỉa kênh (He et al., 2017; Luo et al., 2017; Liu et al., 2017, 2019c,b; Molchanov et al., 2019; Guo et al., 2020) là một phương pháp tiêu chuẩn cho các mạng nơ-ron tích chập. Các kỹ thuật này có thể được điều chỉnh cho các mô hình dựa trên Transformer như được giới thiệu trong §2.3. Cắt tỉa phi cấu trúc là một hướng nghiên cứu chính khác, đặc biệt trở nên phổ biến trong lý thuyết về Giả thuyết Vé số (Frankle and Carbin, 2019; Zhou et al., 2019; Renda et al., 2020; Frankle et al., 2020; Chen et al., 2020a). Cắt tỉa phi cấu trúc tạo ra các mô hình với độ thưa cao (Sanh et al., 2020; Xu et al., 2021; Huang et al., 2021) tuy nhiên khó mang lại tăng tốc suy luận thực tế. Phát triển các nền tảng tính toán cho các hoạt động tensor thưa hiệu quả là một lĩnh vực nghiên cứu tích cực. DeepSparse¹⁵ là một công cụ suy luận CPU tận dụng độ thưa phi cấu trúc để tăng tốc. Huang et al. (2021) đo tăng tốc suy luận thực tế được tạo ra bởi cắt tỉa phi cấu trúc trên nền tảng phần cứng mới nhất của Moffett AI ANTOM. Chúng tôi không so sánh trực tiếp với các phương pháp này vì môi trường đánh giá khác nhau.

Trong khi tất cả các phương pháp được đề cập ở trên tạo ra các mô hình đặc thư cho tác vụ thông qua cắt tỉa, một số công trình khám phá cắt tỉa upstream trong đó họ cắt tỉa một mô hình được huấn luyện trước lớn với tác vụ mô hình hóa ngôn ngữ có mặt nạ. Chen et al. (2020a) cho thấy một mô hình 70% độ thưa giữ lại độ chính xác MLM được tạo ra bởi cắt tỉa độ lớn lặp lại. Zafrir et al. (2021) cho thấy lợi thế tiềm năng của cắt tỉa phi cấu trúc upstream so với cắt tỉa downstream. Chúng tôi coi việc áp dụng CoFi cho cắt tỉa upstream là một hướng tương lai đầy hứa hẹn để tạo ra các mô hình bất khả tri tác vụ với cấu trúc linh hoạt.

Bên cạnh cắt tỉa, nhiều kỹ thuật khác đã được khám phá để đạt được tăng tốc suy luận cho các mô hình Transformer, bao gồm chưng cất như được giới thiệu trong §2.2, lượng tử hóa (Shen et al., 2020; Fan et al., 2021), tăng tốc suy luận động (Xin et al., 2020) và phân rã ma trận (Noach and Goldberg, 2020). Chúng tôi giới thiệu độc giả đến Ganesh et al. (2021) để có một khảo sát toàn diện.

6 Kết luận
Chúng tôi đề xuất CoFi, một phương pháp cắt tỉa có cấu trúc kết hợp tất cả các mức độ cắt tỉa, bao gồm các lớp MHA/FFN, các đầu riêng lẻ và các chiều ẩn cho các mô hình dựa trên Transformer. Kết hợp với một mục tiêu chưng cất được thiết kế riêng cho cắt tỉa có cấu trúc, chúng tôi cho thấy rằng CoFi nén các mô hình thành một cấu trúc khá khác với các mô hình chưng cất tiêu chuẩn nhưng vẫn đạt được kết quả cạnh tranh với tăng tốc hơn 10×. Chúng tôi kết luận rằng cắt tỉa có cấu trúc đặc thư cho tác vụ từ các mô hình kích thước lớn có thể là một sự thay thế hấp dẫn cho chưng cất để đạt được nén mô hình cực đoan, mà không cần đến huấn luyện trước tốn kém hoặc tăng cường dữ liệu. Mặc dù CoFi có thể được áp dụng trực tiếp cho cắt tỉa có cấu trúc cho các mô hình bất khả tri tác vụ, chúng tôi đóng khung phạm vi của công việc này cho cắt tỉa đặc thư cho tác vụ do độ phức tạp của các lựa chọn thiết kế cho cắt tỉa upstream. Chúng tôi hy vọng rằng nghiên cứu tương lai tiếp tục dòng công việc này, vì cắt tỉa từ một mô hình được huấn luyện trước lớn có thể phát sinh ít tính toán hơn so với chưng cất tổng quát và dẫn đến các cấu trúc mô hình linh hoạt hơn.

Lời cảm ơn
Các tác giả cảm ơn Tao Lei từ Google Research, Ameet Deshpande, Dan Friedman, Sadhika Malladi từ Đại học Princeton và các nhà đánh giá ẩn danh vì phản hồi có giá trị của họ về bài báo của chúng tôi. Nghiên cứu này được hỗ trợ bởi Học bổng Hisashi và Masae Kobayashi *67 và Giải thưởng Google Research Scholar.

¹⁵https://github.com/neuralmagic/deepsparse

--- TRANG 7 ---
[TIẾP TỤC VỚI CÁC TRANG CÒN LẠI...]

[Phần còn lại của bài báo bao gồm các tài liệu tham khảo và phụ lục với nhiều chi tiết kỹ thuật, bảng và biểu đồ bổ sung. Do giới hạn độ dài, tôi đã dịch phần chính của bài báo. Bạn có muốn tôi tiếp tục dịch phần còn lại không?]
