# 2310.03188.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-distillation/2310.03188.pdf
# File size: 625462 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Talking Models: Distill Pre-trained Knowledge to
Downstream Models via Interactive Communication
Zhe Zhao1, Qingyun Liu1, Huan Gui1, Bang An2∗, Lichan Hong1, Ed H. Chi1
1Google DeepMind, {zhezhao,qyl,hgui,lichan,edchi}@google.com
2University of Maryland, bangan@umd.edu
Abstract
Many recent breakthroughs in machine learning have been enabled by the pre-
trained foundation models. By scaling up model parameters, training data, and
computation resources, foundation models have significantly advanced the state-
of-the-art in many applications. However, it is still an open question of how to
use these models to perform downstream tasks efficiently. Knowledge distillation
(KD) has been explored to tackle this challenge. KD is a technique that transfers
knowledge from a large teacher model to a smaller student model. While KD
has been successful in improving student model performance, recent research has
discovered that a powerful teacher does not necessarily lead to a powerful student,
due to their huge capacity gap. In addition, the potential distribution shifts between
the pre-training data and downstream tasks can make knowledge transfer in KD
sub-optimal for improving downstream task performance.
In this paper, we extend the knowledge distillation paradigm by introducing an
interactive communication process to help student models of downstream tasks
learn effectively from pre-trained foundation models. Our design is inspired by the
way humans learn from teachers who can explain knowledge in a way that meets
the students’ needs. Specifically, we let each model (i.e., student and teacher) train
two components: (1) an encoder which encodes the model’s hidden states to a
message in a shared message space and (2) a decoder which decodes any message
to its own hidden states. With encoder and decoder, not only can the teacher model
transfer rich information by encoding its hidden states to messages, but also the
student model can send messages with information of downstream tasks to teacher
so that the teacher can interpret and generate responses. With this interactive
communication process, knowledge passing from teacher to student can be tailored
to the student’s model capacity and downstream tasks’ distributions. We conducted
experiments on benchmark datasets for computer vision and recommendation
tasks to show that our communication mechanism outperforms state-of-the-art
distillation techniques.
1 Introduction
Scaling up machine learning models has been shown to improve performance in many applications,
including Natural Language Processing (NLP) (Chowdhery et al., 2022), Computer Vision (CV)
(Yuan et al., 2021) and Information Retrieval tasks (Tay et al., 2022). For example, the number of
parameters of newly developed language models for NLP tasks has grown from hundreds of millions
to hundreds of billions in the past few years (Zhao et al., 2023). By scaling both model size and
training data, researchers have found that not only can large models reduce training loss (Kaplan et al.,
2020), they can also show emergent abilities to solve tasks that smaller models cannot solve, such as
∗This work was done when Bang was an intern at Google.
Preprint. Under review.arXiv:2310.03188v1  [cs.AI]  4 Oct 2023

--- PAGE 2 ---
in-context learning and instruction following (Wei et al., 2022). Similarly, in CV , large models can
improve performance of multiple tasks of different datasets (Dosovitskiy et al., 2020). The training of
these large models can be extremely costly, but once they are trained, they can be stored as pre-trained
foundation models to be fine-tuned for downstream tasks (Raffel et al., 2020; Ridnik et al., 2021).
As the size of these foundation models grows bigger, the inference cost of the fine-tuned models also
grows. On the contrary, for many real-world machine learning applications, researchers have built
their own sophisticated but efficient models, such as real-time traffic sign detection (Bousarhane et al.,
2021), or recommendation and retrieval (Zhao et al., 2019). Without techniques that can significantly
bring down the inference cost of the fine-tuned models, it is challenging to apply foundation models
to a wide range of applications with extensive inference requests and strict constrains on latency.
Existing efforts in reducing the inference cost of large machine learning models include model pruning
(Frantar and Alistarh, 2022), quantization (Liang et al., 2021) and knowledge distillation (Hinton
et al., 2015). Model pruning (Frantar and Alistarh, 2022) techniques prune learned weights based on
their importance, and quantization (Dettmers et al., 2022) techniques approximate learned weights
with data-types of low-bit-width. While both techniques provide a trade-off between efficiency
and quality, they either need to be carefully designed for specific model architectures, or would
degrade the quality of the model regardless of the downstream tasks. When transferring knowledge
of pre-trained foundation models to perform downstream tasks, it is desirable to have flexibility for
the downstream models adopting different model architectures, such as decoders from the pre-trained
model. Therefore, knowledge distillation techniques, which transfer knowledge from one model to
another by using model outputs (e.g., predictions), can provide such flexibility (Gou et al., 2021). In
this paper, we explore the direction of distilling the knowledge from pre-trained foundation models
(teacher) to smaller models for downstream tasks (students) by knowledge distillation.
Some very recent research (Hsieh et al., 2023) successfully distilled large language model (LLM)
to smaller models for downstream tasks, by using LLM generating rationale of the labels for the
downstream models. As this research is specific to NLP applications and cannot be easily extended
to other machine learning applications, these exist two additional challenges on applying KD to
distill pre-trained knowledge to improve downstream models. Firstly, recent research discovers that a
huge capacity gap between teacher and student will hurt KD (Wang et al., 2022; Huang et al., 2022).
Therefore it becomes difficult to apply KD to improve smaller downstream models by learning from
much larger pre-trained models. Secondly, downstream tasks can have different distributions from
the pre-trained data. Therefore knowledge from the pre-trained model may not be directly useful to
downstream tasks. Although some recent study found that distillation from a fine-tuned teacher can
improve the performance of the student model (Wu et al., 2023), fine-tuning pre-trained foundation
models is very costly and cannot scale to many downstream tasks.
We hypothesize that the challenges arise when the pre-trained teacher do not have a good understand-
ing of the student’s capacity and ability in learning downstream tasks. We view KD as a one-way
communication process between the teacher and student: the teacher generates information (e.g.,
logits, hidden states) from training data and passes it to the student. And one-way communication
can be sub-optimal for knowledge transfer since teacher has no information about the student. In
this paper, we extend the KD framework by introducing a two-way communication process, where
teacher can provide specific guidance by interpreting student’s request. This is inspired by how
human communicate and learn from each other: persons from different background and expertise can
exchange knowledge effectively by interactive conversations; and students ask question and learn
from teacher’s response and continue asking follow-up questions. This can be formulated as the
Osgood and Schramm Model of communication (Wrench et al., 2020) (shown in Figure 2), which is
an interpersonal interaction model indicating that messages can go in two directions with a heightened
focus on cyclical feedback.
We adapt the Osgood and Schramm Model to design the interactive communication process for KD,
where students can ask questions to teachers and learn from the answers. Our proposed process,
named Talking-model Distillation (TD), is shown in Figure 1 (on the right, compared to KD on the
left). To be more specific, for each model (i.e., the teacher or student), we introduce two modules used
for communication: an encoder that encodes model’s hidden states to a message, which will be passed
to the other model; and a decoder that decodes the message to its hidden states. The communication
process starts with student model encoding its hidden states given an input to a message. Teacher
then decodes the message into hidden states in its own hidden space. We introduce an interpreting
2

--- PAGE 3 ---
Figure 1: Talking-model Distillation (TD, right) compared to Knowledge Distillation (left, which
can be label distillation (Hinton et al., 2015), feature distillation (Heo et al., 2019; Romero et al.,
2014) or hybrid of both (Zhu and Wang, 2021)). In TD, student Mgencodes its hidden states given
input xto message mgwithEg, teacher Mhthen decodes it with Dh, and interprets the decoded
hidden states with its hidden layers. Mhencodes the interpreted states to message mhwithEh, Then
Mgdecodes it with Dgand use it for distillation. Mgcan also interpret decoded hidden states and
generates “follow-up” messages to communicate for multiple iterations.
step where teacher runs the decoded hidden states (for bottom layers) using its own learned weights
to generate new hidden states. Teacher then encodes the interpreted hidden states into message and
returns it to student. Student uses the returned message for knowledge distillation by aligning the
decoded hidden states in the returned message with its own hidden states. The training of the encoder
and decoder for both teacher and student can happen at the same time as training the student, while
the learned weights of the teacher will not be updated. Given the encoder and decoder for each
model can be as simple as projection layers using a small number of parameters just to align the
corresponding hidden space to a shared message space, the learning of communication parameters
does not add much overhead in training the student model.
By introducing this interactive communication process, the student can request information from
teacher based on its own hidden states generated from the downstream tasks. The information teacher
provides through the message space is better aligned with downstream tasks by both models’ encoders
and decoders. Moreover, student can also interpret teacher’s returned message and generate a new
message back to teacher, as a follow-up. This enables the cyclical communication behavior where
multiple iterations of communication can happen, so that student can get sufficient tailored guidance
from teacher, even when downstream tasks have extremely sparse training data.
To show that our proposed method can be used in different domains and applications, we conduct
experiments on multiple benchmark datasets including computer vision tasks and recommendation
tasks. Compared with state-of-the-art distillation methods (e.g., a hybrid of feature and label
distillation (Zhu and Wang, 2021)), our interactive communication process significantly improves
knowledge transfer between pre-trained teacher and students of downstream tasks.
2 Related work
In this paper, instead of improving general KD, we focus on a specific use-case: use pre-trained
large models to teach smaller students for downstream tasks, without extensive fine-tuning of the
teacher. Therefore, many most recent KD algorithms cannot be directly applied, as they assume
similar tasks between teachers and students (Yang et al., 2022b; Beyer et al., 2022). In this section, we
discuss existing research for efficient model tuning, including model compression and KD, and their
inspirations to improve efficiency for downstream models in the pre-training fine-tuning paradigm.
3

--- PAGE 4 ---
2.1 Model compression for efficient serving
There exist many widely studied research directions to reduce the serving cost of deep neural networks,
including but not limited to model pruning (Hassibi et al., 1993), quantization (Guo, 2018), and
knowledge distillation (Hinton et al., 2015). Recent model pruning methods (Frantar and Alistarh,
2022; El Halabi et al., 2022; Kwon et al., 2022) have considered large pre-trained models and
potential sparse downstream tasks. In these methods, sub-modules of the pre-trained models can be
pruned based on their importance towards a pre-defined metric (Frantar and Alistarh, 2022; El Halabi
et al., 2022), or masks can be learned to mask out learned weights given downstream tasks (Kwon
et al., 2022). While pruning methods need to be designed based on different model architectures,
quantization (Krishnamoorthi, 2018; Guo, 2018) can be applied to any learned weights by replacing
them with low-precision data-types.
Both pruning and quatization directly modify the learned weights of pre-trained models, however,
a model with a different structure can be desirable for downstream tasks. Both techniques cannot
be applied to newly initialized weights. Knowledge Distillation provides such flexibility and can be
complimentary to be used together with pruning and quatization methods (Shin et al., 2019).
2.2 Knowledge distillation
Knowledge Distillation uses knowledge from a larger, more powerful “teacher” model to improve the
performance of a smaller, more efficient “student” model (Gou et al., 2021). In recent years, KD has
been used in many applications, such as NLP (Sun et al., 2019; Sanh et al., 2019), CV (Park et al.,
2019; Tung and Mori, 2019), and Recommendation Systems (Tang and Wang, 2018).
However, recent research discoveries in understanding knowledge distillation (Wang et al., 2022;
Huang et al., 2022; Zhu et al., 2022) suggest a larger teacher does not necessarily guarantee a better
student. On the contrary, a huge capacity gap between teachers and students can lead to little or no
improvement for the students. This is because some specific representations can only be learned
with a large capacity model (Zhu et al., 2022), or an information bottleneck can be created due to the
capacity gap (Wang et al., 2022). This is especially challenging when distilling knowledge from large
pre-trained foundation models.
To deal with the capacity gap, existing works introduce intermediate steps/models such as teaching
assistant models (Mirzadeh et al., 2020). Additionally, they try to leverage more information from
teachers besides their predictions (or logits), such as feature representations (Heo et al., 2019; Yang
et al., 2019; Zhu and Wang, 2021; Romero et al., 2014), relationships among labels and representations
(Hao et al., 2022; Huang et al., 2022), or the weights of teacher models (Fu et al., 2021).
In this paper, we discuss a specific case of distilling pre-trained foundation models for downstream
tasks, where downstream tasks can be different from pre-trained tasks. Fine-tuning a pre-trained
foundation model for each downstream task to obtain a specific teacher model can be costly and
cannot scale up to many different downstream tasks. Existing research in cross-task distillation
(Yang et al., 2022a; Clark et al., 2019; Zhong et al., 2022) shows that a teacher model trained from
multiple tasks can be potentially helpful to a single task student (Clark et al., 2019). However, for
downstream tasks which are not present in teacher models’ pre-trained tasks, auxiliary tasks will be
needed for both the teacher and student (Yang et al., 2022a). Therefore, we focus on directly distilling
information from pre-trained models to students, using their feature representations in hidden spaces
similar to the feature distillation techniques (Zhu and Wang, 2021).
2.3 Improve foundation model efficiency for downstream tasks
The successes of large pre-trained foundation models inspire researchers to explore techniques to
improve the efficiency of using them in real-world applications. Many existing research has been
focusing on how to improve the efficiency in training or fine-tuning the foundation models. This
includes adaptor based method (Houlsby et al., 2019), which freezes most of the learned weights
and only trains a small adaptor layer for each downstream task. More recently, parameter-efficient
fine-tuning techniques extend adaptors to low-rank adaptors which can further improve the training
efficiency while reducing serving latency (Hu et al., 2021; He et al., 2022). However, the serving cost
of adaptor based fine-tuned model is still comparable to directly using the foundation models.
4

--- PAGE 5 ---
Figure 2: The interpersonal communication models. One-way communication model (left, Early
Schramm Model) and interactive communication model (right, Osgood-Schramm’s Model)
More recently, there were early empirical results (Hsieh et al., 2023; Wu et al., 2023) showing
that distillation method can be used to distill knowledge from pre-trained foundation models to
downstream tasks. In NLP specific tasks, by using pre-trained model to generate rationales and
explanations of downstream tasks labels, more information can be passed from teacher to student
to improve knowledge transfer (Hsieh et al., 2023). For text-image foundation model, empirical
results showed that vanilla distillation might not directly improve downstream task unless a fine-tuned
teacher is used (Wu et al., 2023). Inspired by their discoveries, we target at developing distillation
techniques that don’t require fine-tuning to downstream tasks and can be applied to different machine
learning applications and downstream tasks such as CV and Recommendation Systems.
3 Proposed method
In this section, we introduce our proposed method named Talking-model Distillation (TD). Our key
innovation is to generalize the existing KD methods as a one-way communication process and extend
it to a two-way interactive communication process. We first describe how to adapt interpersonal
human communication process to machine learning models for KD between teacher and student. We
then show that existing KD can be categorized as a one-way communication process. Then we will
introduce how TD enables the teacher and student to communicate interactively so that the student
can learn rich information from the teacher with specific focus on downstream tasks.
3.1 Communication encoder and decoder
In Figure 2, we show two interpersonal communication models in communication theory (Wrench
et al., 2020), named Early Schramm Model and Osgood-Schramm’s Model. Early Schramm Model
is a one-way communication model where a sender encodes a message and passes it to a receiver to
decode. The Osgood-Schramm’s Model is an interactive communication model that two individuals
can send, receive and parse messages in a cyclical way, so that the messages being passed can capture
feed-backs from each other.
The key components for both communication processes are: (1) Encoder that encodes information to
communicate as messages; (2) Decoder that decodes message to understandable information; (3) A
shared message space. To apply these communication models to knowledge distillation, for each of
the teacher and student models, we introduce encoder and decoder for communication.
The first question is what information can encoder encode, while the original KD used logits (Hinton
et al., 2015), we build encoder on top of the hidden representations of the model, which are more
flexible and are not tied to the final prediction tasks (Heo et al., 2019). Given a model Mgwhich
generates predictions ygfor labels ygiven input x, it typically has multiple hidden layers Hg
1, ...Hg
ng
(e.g., Multi-layer perception (Gardner and Dorling, 1998), Convolution layers (Yamashita et al.,
2018), or Transformer layers (Vaswani et al., 2017)). The encoder can encode any hidden states
from these layers to messages. Without loss of generality, we categorize the hidden states into
two categories: (1) lower level hidden states sg, which capture the lower-level representations or
processed input signals from Hg
1toHg
lg, and (2) higher level hidden states eg, which capture the
embeddings that can be used for a few top hidden layers from Hg
lg+1toHg
hgto generate predictions.
These hidden layers for Mgto generate predictions yggiven xcan be written as:
5

--- PAGE 6 ---
yg=Hg
hg+1,...,n g(eg) =Hg
hg+1,...,n g(Hg
lg+1,...,h g(sg)) =Hg
hg+1,...,n g(Hg
lg+1,...,h g(Hg
1,...lg(x)))
Then the encoder Egfor model Mgencodes hidden states from both lower and higher levels into
message mgin the message space mg=Eg({sg;eg}). Therefore another model Mhcan use its
decoder Dhto decode mginto hidden states for Mh:{s′
h;e′
h}=Dh(mg).
The selection of communication encoder and decoder’s structure depends on the model architecture
as well as the design of the message space. For example, it can be Transformer layers if the model is
a transformer and message space contains sequences of embeddings. In this paper, for simplicity, we
only explored linear or Dense-Relu-Dense encoders and decoders with message space of m∈Rmd,
where mdis the dimensionality of the message space.
3.2 Existing KD techniques as one-way communication
Next we show how we can formulate existing KD techniques as a one-way communication process
with the encoder and decoder introduced in the previous subsection. KD techniques here can refer to
logit distillation (Hinton et al., 2015), feature distillation (Romero et al., 2014; Heo et al., 2019) or a
hybrid of both (Zhu and Wang, 2021). We denote student model as Mgand teacher model as Mh
For logits distillation, we can simply view the logit space as the message space given both teacher
and student having the same prediction tasks. And the distillation loss is Llogit=d(yg, yh), where d
is a distance metric such as L2 loss or KL divergence between the teacher and student logits.
For feature distillation method (such as (Heo et al., 2019)) which projects hidden states of different
dimensionalities into a shared space, i.e., the shared message space, the distillation loss can be
calculated by distance measurement between messages in the message space.
Lfeature =d(mg, mh) =d(Eg({sg;eg}), Eh({sh;eh})) =
d(Eg({Hg
1,...lg(x);Hg
lg+1,...,h g(Hg
1,...lg(x))}), Eh({Hh
1,...lh(x);Hh
lh+1,...,h h(Hh
1,...lh(x))}))
Heredis the distance between two messages, which can be L2, KL-divergence or any of the relational
metrics that have shown promising results for feature distillation such as Pearson correlation (Huang
et al., 2022) or manifold loss (Hao et al., 2022). Encoders for EgandEhare learned parameters for
feature distillation.
Another slightly different feature distillation method, Fitnet (Romero et al., 2014), which projects
teacher model hidden states to student model’s hidden states, can be viewed as student model
Mgdirectly decoding teacher model Mh’s hidden states (as message, where Ehis an identity
transformation), hence the KD loss can be written as:
Lfitnet =d({sg;eg}, Dg(mh)) =d({sg;eg}, Dg({sh;eh})) =
d({Hg
1,...lg(x);Hg
lg+1,...,h g(Hg
1,...lg(x))}, Dg({Hh
1,...lh(x);Hh
lh+1,...,h h(Hh
1,...lh(x))}))
where Dg’s parameters are learned. We can see that Lfitnet distills knowledge by minimizing the
distance between original student model’s hidden states and decoded states from teacher’s hidden
states. And Feature distillation Lfeature distills knowledge by minimizing the distance between
encoded student’s hidden states and teacher’s hidden states in a shared message space.
3.3 Interactive communication
Our proposed method, named Talking-model Distillation (TD) uses interactive communication
process, which allow both teacher and student model to interpret messages and return new messages.
To be more specific, below we describe one iteration of such interactive communication:
Student passes message to teacher Student model Mggenerates a message mg=Eg({sg;eg})
from input xand pass it to teacher model Mh. Then teacher model decodes the message into hidden
states with its decoder {s′
h;e′
h}=Dh(mg).
6

--- PAGE 7 ---
Teacher interprets message and encodes returned message We introduce an interpreting step,
where teacher model interprets the decoded message by its own learned weights. Teacher model
uses the decoded lower-level hidden states s′
has input to its hidden layers Hh
lh+1, ..., Hh
hhto generate
interpreted states ˜eh=Hh
lh+1,...,h h(s′
h). Then teacher encodes the interpreted states (along with the
decoded lower-level hidden states) as a returned message: mh=Eh({s′
h,˜eh}). The interpreting step
is crucial for the interactive communication process, as it enables messages from teacher encodes
information of teacher’s knowledge (model parameters) being applied on student’s messages.
Student decodes returned message and learns from teacher The student model Mgdecodes the
returned message to its hidden space: {s′
g;e′
g}=Dg(mh). Then student can learn from teacher by
minimizing the distance of the decoded states with its original states:
Linteract =d({sg;eg},{s′
g;e′
g}) =d({sg;eg}, Dg(Eh({s′
h; ˜eh})))
Heredcan be any distance metric used in existing feature distillation techniques. In this paper, we use
L2 loss as dfor all distillation methods. We can see the key differences between Linteract and other
distillation losses ( Llogit, Lfeature , andLfitnet ), is that we apply teacher’s hidden layers and learned
weights on interpreted student’s messages, instead of input x. By doing so, along with the training of
both models’ encoder and decoder, teacher can provide feedbacks that fit student model’s capacity
and learned representation space. Note that e′
his not used to calculate Linteract for learning from the
teacher. But it is used for training the teacher’s decoder in LSCdiscussed in the next subsection.
Student interprets and generates follow-up messages After receiving the returned message
from teacher, student model can also interpret the message mhand generate an interpreted state:
˜eg=Hg
lg+1,...,h g(s′
g). Then student model can encode it with s′
gtom2
h=Eg({s′
g,˜eg}). Here m2
h
refers to the second iteration of the message for interactive communication. It will be passed to
teacher again to start the next iteration of communication.
Note that new iteration of communication won’t consume new input xand this can continue for as
many iterations as possible. By doing so, rich information from teacher model can pass to student
based on student’s request, even when downstream tasks are extremely sparse. The iterative update
scheme here shares similarity to some techniques in Semi-Supervised Learning and Self-Training
(Xie et al., 2020), however, our proposed method used in knowledge distillation are more light-weight
and only updates the student model with iterations of interactive communications. We explore the
number of iterations for interactive communication as a hyper-parameter in our experiment.
To sum up, our proposed communication process is shown in Figure 1, compared with other KD
methods. By comparing Figure 1 with Figure 2, we can see that the differences between our method
and other KD techniques are similar to the differences between the two interpersonal communication
models. A detailed algorithm of our proposed method is included in Appendix 6.1.
3.4 Training of the encoder and decoder for TD
In order to make the interpreting step and interactive communication effective, it is important for
the encoder and decoder to learn a reasonable aligned projection between the message space and
each of the model’s hidden space. This means that given the same input to both teacher and student,
encoders of both models need to generate similar messages, and decoded states need to be similar to
their original states. To achieve this, besides the Linteract used to train student model as well as both
student and teacher’s encoders and decoders, we introduce following two consistency losses.
Message space consistency Given the same input x, student model Mg’s encoder Egand teacher
model Mh’s encoder Ehwill generate similar messages, described below as the message consis-
tency loss between the two messages. This is similar to Lfeature for feature distillation. Message
consistency loss will be used to train the encoder of both models.
LMC=d(mg, mh) =d(Eg(sg, eg), Eh(sh, eh))
State space consistency Given the same input x, the hidden states decoded by two model’s message
should be consistent with its own hidden states. We introduce the state consistency loss between
7

--- PAGE 8 ---
Methods ML(Dense) ML(Sparse) CIFAR10 CIFAR100 ImageNet Avg.
Train from Scratch (—baseline to calculate relative improvement—)
LD +0.16% +1.49% +0.03% +1.86% -0.21% +0.83%
FD +0.29% +2.68% -0.12% -0.08% -0.14% +0.66%
FitNet +0.81% +2.19% -0.09% +0.53% +0.29% +0.93%
Hybrid +0.93% +2.91% +0.23% +1.94% +0.02% +1.50%
Our Method ( TD) +1.34% +3.39% +0.45% +2.41% +2.56% +2.54%
Table 1: Relative improvement of different distillation methods compared to a student model without
distillation. Detailed results with standard error are shown in Appendix 6.5.
Methods ML(Dense) ML(Sparse) CIFAR10 CIFAR100 ImageNet Avg.
Train from Scratch (—baseline to calculate relative improvement—)
No Interaction +0.81% +2.02% +0.43% +2.26% +2.44% +1.99%
1 iteration +1.31% +3.08% +0.42% +2.31% +2.49% +2.40%
>1 iterations +1.34% +3.30% +0.45% +2.41% +2.56% +2.52%
Table 2: Relative improvement of using interactive communication. Detailed results with standard
error are shown in Appendix 6.5.
decoded states and original states below. This is similar to Lfitnet in FitNet and is used to train both
encoder and decoder of each model.
LSC=d({sg;eg}, Dg(mh)) +d({sh;eh}, Dh(mg))
The communication encoder and decoder will be co-trained with the training of student model, using
the combined loss below:
L(x, y, M g, Mh) =L(y, yg) +w1Linteract +w2LMC+w3LSC
Where L(y, yg)is the groundtruth loss, w1, w2andw3are hyper-parameters of loss weights.
Note that even though we introduce 3 losses ( Linteract , LMCandLSC), the parameters we added
for encoder and decoder are comparable to other feature distillation methods. During training, we
freeze teacher model and only update student model and the encoder and decoder of both models.
Therefore our method doesn’t add notable more weights to learn. However, our distillation process
takes more time if the number of iterations of interactive communication is larger than one, due to
both teacher and student models need to interpret each other’s input multiple times.
4 Experiment
In this section, we evaluate whether our proposed method can improve knowledge distillation by
introducing the interactive communication process, for the case of distilling pre-trained teacher to
smaller models for downstream tasks. The teacher model is trained with (multiple) pre-training
task(s) or multiple datasets. And the student model will be trained and evaluated on a much smaller
dataset with a specific downstream task.
4.1 Experiment setup
Datasets We choose multiple widely-used real-world benchmark datasets, including MovieLens
(Harper and Konstan, 2015), CIFAR10, CIFAR100 (Krizhevsky, 2009), and ImageNet (Russakovsky
et al., 2015). These datasets cover applications of recommendation and image classification. For
MovieLens (ML), we split the data by timestamps, so that 90% of the past events will be used to
train models evaluated by the 10% of the future events, which is close to the real-world setup. The
pretrained task is to predict movie ratings given a user and a movie for all genres of movies. And
downstream tasks are movie rating prediction for a specific genre. Teacher is an MLP model and
8

--- PAGE 9 ---
Figure 3: Representation Similarity between teacher (column) and student (row).
student model has only 1/4 of the neurons for each layer in teacher model. For image classification
datasets, we adopt the same setup as Vision Transformer (ViT) (Dosovitskiy et al., 2020), where we
pretrain a large ViT teacher model on ImageNet21K and evaluate student models using CIFAR10,
CIFAR100 and ImageNet. Teacher is the pre-trained 12-layer ViT-B/32 model, and student model is
the same architecture but only has 4 transformer layers. Dataset details are in Appendix 6.2.
Baseline methods We compare with four baseline methods: Label Distillation ( LD(Hinton et al.,
2015)), Feature Distillation ( FD(Heo et al., 2019)), FitNet (Romero et al., 2014) and a Hybrid
version of Label and Feature Distillation ( Hybrid (Zhu and Wang, 2021)). Note that most recent
KD approaches (such as Beyer et al. (2022), Yang et al. (2022a)) focus on one single application
such as image classification or recommendation, and assume teacher and student share similar tasks.
We cannot directly apply them in our setup, therefore we compare with the general KD algorithms
that can be extended to our use case. For a fair comparison, all methods use the same teacher and
student structure. Each of the methods’ KD loss weights are tuned. Details about baseline methods
and Hyper-parameter tuning are shown in Appendix 6.3.
4.2 Overall improvement
The overall improvement of our method compared to baseline methods is shown in Table 1. We can
see that our method outperforms all baseline methods on all 5 tasks. For MovieLens, we report results
on two types of downstream tasks: ML(Dense) is from one genre that has dense data and ML(Sparse)
contains 4 genres that are much sparser. We include results on all other genres in Appendix 6.5.
4.3 Ablation study: understand the interactive communication
To evaluate the performance of the interactive communication process, we conduct ablation study by
adjusting number of iterations for interactive communication. Results are shown in Table 2. For the
“No Interaction” row, we disable interactive communication (no Linteract ) but keep the encoder and
decoder with consistency losses ( LMCandLSC). We use 3 as the maximum number of iterations
for MovieLens and 2 for ViT. The best results are achieved with the maximum number of iterations.
We didn’t explore larger number of iterations since our training time will be ktimes slower where k
is the number of iterations. But in real-world scenarios, the distillation cost can be relatively small
when downstream tasks are sparse. We also conduct ablation studies on the consistency losses LMC
andLSCto show the importance of aligning the message spaces and decoded states between the two
models. The results are shown in Appendix 6.5.
4.4 Cast study: bridge the capacity gap on undistillable downstream classes
To show that TD can improve student’s performance when there is a huge capacity gap between
teacher and student, we conduct case study on ImageNet by measuring the representation similarity
between student model and teacher model on downstream classes. Similar to the analysis measuring
“undistillable” classes (classes cannot be distilled) due to capacity gap (Zhu et al., 2022), we use the
Center Kernel Alignment (CKA) (Kornblith et al., 2019) to measure similarities between representa-
tions from teacher ( shandeh) and student ( sg,egand the message space mg). For each class, we
use 20 examples for the analysis. The results are shown in Figure 3. The higher the value (i.e., the
9

--- PAGE 10 ---
lighter the color) on the diagonal term, the better aligned the two representation spaces are. We can
see that the learned message space is more aligned with teacher compared to distillation baseline
even on “undistillable” classes (classes with darker color on higher layer representation similarities
between teacher and student, i.e., between egandeh). And the student models learned with TD are
more aligned with teacher’s representation on the top layer hidden states ( egandeh).
5 Conclusion
In this paper, we propose Talking-model Distillation (TD), a new distillation paradigm that enables
model to communicate interactively, for transferring the knowledge from large pre-trained foundation
models to efficient downstream models. By adapting interpersonal communication models to KD,
we first show that existing KD techniques utilize one-way communication from teacher to student.
We then design an interactive communication paradigm where teacher and student can exchange
messages. In this paradigm, teacher can pass knowledge to student based on student’s encoded
messages using downstream tasks. And the interactive communication can have multiple iterations,
which enables distilling rich information even when downstream data are sparse. We conduct
experiments on multiple benchmark datasets and we show that our proposed method outperforms
other distillation baselines on improving student performance by distilling pre-trained teacher.
Implication and limitation Our proposed method provides a new way of thinking about how
models can transfer knowledge and interact with each other. Even though we show early results of
interactive communication can further improve model performance for knowledge distillation, the
gap between teacher and student is still very large. In the same time, we increase the computation
resource used in the communication process, even though it is relatively negligible compared to
fine-tuning large foundation models. We also believe that by spending computation resources in
communication, we can enable models from different tasks even different modalities to learn from
each other more efficiently, without the needs of going over large training data.
References
Beyer, L., Zhai, X., Royer, A., Markeeva, L., Anil, R., and Kolesnikov, A. (2022). Knowledge
distillation: A good teacher is patient and consistent. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition , pages 10925–10934.
Bousarhane, B., Bensiali, S., and Bouzidi, D. (2021). Road signs recognition: state-of-the-art and
perspectives. International Journal of Data Analysis Techniques and Strategies , 13(1-2):128–150.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,
H. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311 .
Clark, K., Luong, M.-T., Khandelwal, U., Manning, C. D., and Le, Q. V . (2019). Bam! born-again
multi-task networks for natural language understanding. arXiv preprint arXiv:1907.04829 .
Dettmers, T., Lewis, M., Belkada, Y ., and Zettlemoyer, L. (2022). Llm. int8 (): 8-bit matrix
multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 .
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,
Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words: Transformers
for image recognition at scale. arXiv preprint arXiv:2010.11929 .
El Halabi, M., Srinivas, S., and Lacoste-Julien, S. (2022). Data-efficient structured pruning via
submodular optimization. Advances in Neural Information Processing Systems , 35:36613–36626.
Frantar, E. and Alistarh, D. (2022). Optimal brain compression: A framework for accurate post-
training quantization and pruning. arXiv preprint arXiv:2208.11580 .
Fu, S., Li, Z., Liu, Z., and Yang, X. (2021). Interactive knowledge distillation for image classification.
Neurocomputing , 449:411–421.
10

--- PAGE 11 ---
Gardner, M. W. and Dorling, S. (1998). Artificial neural networks (the multilayer perceptron)—a
review of applications in the atmospheric sciences. Atmospheric environment , 32(14-15):2627–
2636.
Gou, J., Yu, B., Maybank, S. J., and Tao, D. (2021). Knowledge distillation: A survey. International
Journal of Computer Vision , 129:1789–1819.
Guo, Y . (2018). A survey on methods and theories of quantized neural networks. arXiv preprint
arXiv:1808.04752 .
Hao, Z., Guo, J., Jia, D., Han, K., Tang, Y ., Zhang, C., Hu, H., and Wang, Y . (2022). Learning
efficient vision transformers via fine-grained manifold distillation. Advances in Neural Information
Processing Systems , 35:9164–9175.
Harper, F. M. and Konstan, J. A. (2015). The movielens datasets: History and context. Acm
transactions on interactive intelligent systems (tiis) , 5(4):1–19.
Hassibi, B., Stork, D. G., and Wolff, G. J. (1993). Optimal brain surgeon and general network pruning.
InIEEE international conference on neural networks , pages 293–299. IEEE.
He, X., Li, C., Zhang, P., Yang, J., and Wang, X. E. (2022). Parameter-efficient fine-tuning for vision
transformers. arXiv preprint arXiv:2203.16329 .
Heo, B., Kim, J., Yun, S., Park, H., Kwak, N., and Choi, J. Y . (2019). A comprehensive overhaul
of feature distillation. In Proceedings of the IEEE/CVF International Conference on Computer
Vision , pages 1921–1930.
Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 .
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan,
M., and Gelly, S. (2019). Parameter-efficient transfer learning for nlp. In International Conference
on Machine Learning , pages 2790–2799. PMLR.
Hsieh, C.-Y ., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y ., Ratner, A., Krishna, R., Lee, C.-Y ., and
Pfister, T. (2023). Distilling step-by-step! outperforming larger language models with less training
data and smaller model sizes. arXiv preprint arXiv:2305.02301 .
Hu, E. J., Shen, Y ., Wallis, P., Allen-Zhu, Z., Li, Y ., Wang, S., Wang, L., and Chen, W. (2021). Lora:
Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 .
Huang, T., You, S., Wang, F., Qian, C., and Xu, C. (2022). Knowledge distillation from a stronger
teacher. arXiv preprint arXiv:2205.10536 .
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford,
A., Wu, J., and Amodei, D. (2020). Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 .
Kornblith, S., Norouzi, M., Lee, H., and Hinton, G. (2019). Similarity of neural network representa-
tions revisited. In International Conference on Machine Learning , pages 3519–3529. PMLR.
Krishnamoorthi, R. (2018). Quantizing deep convolutional networks for efficient inference: A
whitepaper. arXiv preprint arXiv:1806.08342 .
Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Technical report.
Kwon, W., Kim, S., Mahoney, M. W., Hassoun, J., Keutzer, K., and Gholami, A. (2022). A fast
post-training pruning framework for transformers. arXiv preprint arXiv:2204.09656 .
Liang, T., Glossner, J., Wang, L., Shi, S., and Zhang, X. (2021). Pruning and quantization for deep
neural network acceleration: A survey. Neurocomputing , 461:370–403.
Mirzadeh, S. I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., and Ghasemzadeh, H. (2020).
Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on
artificial intelligence , volume 34, pages 5191–5198.
11

--- PAGE 12 ---
Park, W., Kim, D., Lu, Y ., and Cho, M. (2019). Relational knowledge distillation. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 3967–3976.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., and Liu,
P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. The
Journal of Machine Learning Research , 21(1):5485–5551.
Ridnik, T., Ben-Baruch, E., Noy, A., and Zelnik-Manor, L. (2021). Imagenet-21k pretraining for
the masses. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track (Round 1) .
Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., and Bengio, Y . (2014). Fitnets: Hints
for thin deep nets. arXiv preprint arXiv:1412.6550 .
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla,
A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale Visual Recognition
Challenge. International Journal of Computer Vision (IJCV) , 115(3):211–252.
Sanh, V ., Debut, L., Chaumond, J., and Wolf, T. (2019). Distilbert, a distilled version of bert: smaller,
faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 .
Shin, S., Boo, Y ., and Sung, W. (2019). Empirical analysis of knowledge distillation technique for
optimization of quantized deep neural networks. arXiv preprint arXiv:1909.01688 .
Sun, S., Cheng, Y ., Gan, Z., and Liu, J. (2019). Patient knowledge distillation for bert model
compression. arXiv preprint arXiv:1908.09355 .
Tang, J. and Wang, K. (2018). Ranking distillation: Learning compact ranking models with high
performance for recommender system. In Proceedings of the 24th ACM SIGKDD international
conference on knowledge discovery & data mining , pages 2289–2298.
Tay, Y ., Tran, V ., Dehghani, M., Ni, J., Bahri, D., Mehta, H., Qin, Z., Hui, K., Zhao, Z., Gupta, J., et al.
(2022). Transformer memory as a differentiable search index. Advances in Neural Information
Processing Systems , 35:21831–21843.
Tung, F. and Mori, G. (2019). Similarity-preserving knowledge distillation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision , pages 1365–1374.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and
Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing
systems , 30.
Wang, C., Yang, Q., Huang, R., Song, S., and Huang, G. (2022). Efficient knowledge distillation
from model checkpoints. arXiv preprint arXiv:2210.06458 .
Wei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M.,
Zhou, D., Metzler, D., et al. (2022). Emergent abilities of large language models. arXiv preprint
arXiv:2206.07682 .
Wrench, J. S., Punyanunt-Carter, N. M., and Thweatt, K. S. (2020). Interpersonal communication: A
mindful approach to relationships.
Wu, Q., Wang, H., Ma, X., and Fu, Y . (2023). Distilling text-image foundation models.
Xie, Q., Luong, M.-T., Hovy, E., and Le, Q. V . (2020). Self-training with noisy student improves
imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition , pages 10687–10698.
Yamashita, R., Nishio, M., Do, R. K. G., and Togashi, K. (2018). Convolutional neural networks: an
overview and application in radiology. Insights into imaging , 9:611–629.
Yang, C., Pan, J., Gao, X., Jiang, T., Liu, D., and Chen, G. (2022a). Cross-task knowledge distillation
in multi-task recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 36, pages 4318–4326.
12

--- PAGE 13 ---
Yang, C., Xie, L., Su, C., and Yuille, A. L. (2019). Snapshot distillation: Teacher-student optimization
in one generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 2859–2868.
Yang, Z., Li, Z., Zeng, A., Li, Z., Yuan, C., and Li, Y . (2022b). Vitkd: Practical guidelines for vit
feature knowledge distillation. arXiv preprint arXiv:2209.02432 .
Yuan, L., Chen, D., Chen, Y .-L., Codella, N., Dai, X., Gao, J., Hu, H., Huang, X., Li, B., Li, C., et al.
(2021). Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432 .
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y ., Min, Y ., Zhang, B., Zhang, J., Dong, Z.,
et al. (2023). A survey of large language models. arXiv preprint arXiv:2303.18223 .
Zhao, Z., Hong, L., Wei, L., Chen, J., Nath, A., Andrews, S., Kumthekar, A., Sathiamoorthy, M., Yi,
X., and Chi, E. (2019). Recommending what video to watch next: a multitask ranking system. In
Proceedings of the 13th ACM Conference on Recommender Systems , pages 43–51.
Zhong, T., Chi, Z., Gu, L., Wang, Y ., Yu, Y ., and Tang, J. (2022). Meta-dmoe: Adapting to domain
shift by meta-distillation from mixture-of-experts. arXiv preprint arXiv:2210.03885 .
Zhu, Y ., Liu, N., Xu, Z., Liu, X., Meng, W., Wang, L., Ou, Z., and Tang, J. (2022). Teach less, learn
more: On the undistillable classes in knowledge distillation. In Advances in Neural Information
Processing Systems .
Zhu, Y . and Wang, Y . (2021). Student customized knowledge distillation: Bridging the gap between
student and teacher. In Proceedings of the IEEE/CVF International Conference on Computer
Vision , pages 5057–5066.
13

--- PAGE 14 ---
6 Appendix
6.1 Pseudo-Code and more discussion of TD
We provide the pseudo code of our proposed Talking-model Distillation (TD) method in Algorithm 1.
Note that the state consistency loss LSCand message consistency loss LMCwill only be applied on
m0
gandm0
h, which require input xbeing fed to both teacher and student. These two losses are used
to train encoders and decoders. They can be disabled in the later training stage. They can also be
optional (ablation results shown in Secion 6.5). Without these losses, teacher model doesn’t need to
access any input data xof downstream tasks.
Algorithm 1: Pseudo-Code of the proposed TD
Require: Trained teacher model Mh, initialized student model Mg(or a pretrained smaller model).
Initialized encoders ( Eg,Eh) and decoders ( Dg,Dh) for both teacher and students. Downstream
dataset D={X, Y}.kiterations for interactive communication.
1:getxandyfrom D
2:L= 0.0// Total loss.
3:yg,{sg;eg}=Mg(x)
4:m0
g=Eg({sg;eg})
5:yh,{sh;eh}=Mh(x)
6:m0
h=Eh({sh;eh})
7:// Adding state consistency and message consistency loss to train
8:// communication encoders/decoders.
9:L=L(y, yg) +w2LSC(m0
g, Dh, m0
h, Dg) +w3LMC(m0
g, m0
h)
10:foreach iteration i in [0, k] do
11: {s′
h;e′
h}=Dh(mi
g)// Teacher decodes message from student.
12: ˜eh=Hh
lh+1,...,h h(s′
h)// Teacher interpreting step.
13: mi+1
h=Eg({s′
h,˜eh})// Teacher encodes returned message.
14: {s′
g;e′
g}=Dg(mi+1
h)// Student decodes returned message.
15: L=L+w1Linteract ({sg;eg},{s′
g;e′
g})// Interactive communication loss for iteration i.
16: ˜eg=Hg
lg+1,...,h g(s′
g)// Student interpreting step for next iteration.
17: {sg;eg}={s′
g; ˜eg}// set student state for next iteration.
18: mi+1
g=Eg({sg;eg})// Student encodes message for next iteration.
19:end for
20:Use optimizer to update the model via total loss L.
Another design choice we made is to make the communication modules exactly the same between
teacher and student. This means teacher encoder and student encoder map their states into the
same embedding space: both encoders encode all hidden states (i.e., lower layer and higher layer
representations); and decoders decode message to all hidden states. However, for distillation, a
specialized design might further improve student performance. For example, message from student
to teacher only encodes lower layer representation. (Note that message from teacher to student
still needs to encode both lower layer and higher layer representations due to student does both
distillation and interpreting). In this paper, we intentionally keep the communication operations the
same between teacher and student due to the following reasons.
•Simplicity. We want to verify the concept of communication for knowledge distillation (KD)
and the effectiveness of introducing interactive communication to KD. Therefore, we adopt
the simple design to keep the communication mechanism the same between teacher and
student.
•Future work of multi-way communication. Communication can also happen between a
group of models besides two models. Therefore, it is straight-forward to extend the current
communication paradigm (both teacher and student adopt same communication mechanism)
to multi-way communication, e.g., multiple teachers, and/or multiple students.
•Future work of multi-way transfer learning. The communication algorithm we propose in
this paper can be applied not only in knowledge distillation, but also can be used as a generic
14

--- PAGE 15 ---
way for transferring knowledge among models. Therefore, it can be applied to two models
where they can learn from each other, e.g., between a large Vision Transformer Model and a
large language model. Therefore, communication can be used for transfer learning with the
unified design of communication encoder and decoder.
6.2 Dataset description and experiment setup
MovieLens100k (Harper and Konstan, 2015) We use the MovieLens100K dataset included in
Tensorflow Dataset2. It contains 100K movie ratings. We use ‘user_id’, ‘movie_id’, and ‘movie_title’
and ‘movie_genres’ as features. We split the data by timestamps. The 90% of the data with earlier
timestamps are used for training and 10% of the data with later timestamps are used for evaluation.
The time split for evaluation is more realistic than random split for recommendation tasks, since it
can capture problems such as user preference shifting overtime as well as cold-starting for new users.
We treat training using data from all genres as pre-training task and training on data with movies from
specific genres as downstream tasks. We evaluate downstream tasks for 8 most dense genres (with
more than 500 evaluation examples), and report the Root Mean Squared Error(RMSE) for rating
prediction.
CIFAR10 (Krizhevsky, 2009) We use the CIFAR10 dataset included in Tensorflow Dataset3. It
contains 60,000 32*32 color images in 10 classes. We use the default training and test split, where
there are 50,000 images used for training and 10,000 images used for testing.
CIFAR100 (Krizhevsky, 2009) We use the CIFAR100 dataset included in Tensorflow Dataset4. It
contains the same 60,000 23*32 color images as CIFAR10, but in 100 classes. We use the same train
and test split as CIFAR10.
ImageNet (Russakovsky et al., 2015) We use the ImageNet dataset described in Tensorflow Dataset
5. It contains 1,281,168 images for training, 50,000 images for validation and 100,000 images for
test.
Teacher model For MovieLens tasks, the teacher model is a Multi-layer Perceptron (Gardner
and Dorling, 1998), with input dimension 300 (100 for ‘user_id’, 100 for ‘movie_id’, and 50 for
‘movie_title’ using bag-of-words and 50 for ‘movie_genres’ using bag-of-words). It has two relu
layers of 512 units and 256 units. The model size is tuned as hyper-parameters with a upper limit of
cost measured by number of flops, and the optimal teacher model size is below the upper limit.
For Image classification tasks, we use Vision Transformer (ViT) (Dosovitskiy et al., 2020) pre-trained
on ImageNet21k with available code6, hyper-parameter setups and checkpoints7. The teacher model
has 16 transformer layers.
Student model For MovieLens, we set student model size to be 1/4 as teacher model: two relu
layers of 128 units and 64 units, where we see significantly quality drop compared to teacher models.
For image classification tasks, student model only has 4 transformer layers. We find that using the
teacher model’s pre-trained weights as initialization for the 4 transformer layers and all other layers
results in better and more stable performance compared to random initialization.
Encoder/Decoder For encoder and decoder, we use the dense-relu-dense model architecture, with
layer norm and dropout. We didn’t do extensive hyper-parameter search to choose the best model
size, instead, we manually pick relu layer size and message dimension to match the size between
teacher and student model. For MovieLens, encoder and decoder have a relu layer with 256 hidden
units, and the message dimensionality is 128. For ViT encoder and decoder have a relu layer with
512 hidden units and the message dimensionality is 512.
2https://www.tensorflow.org/datasets/catalog/movielens
3https://www.tensorflow.org/datasets/catalog/cifar10
4https://www.tensorflow.org/datasets/catalog/cifar100
5https://www.tensorflow.org/datasets/catalog/imagenet2012
6https://github.com/google-research/vision_transformer
7https://huggingface.co/google/vit-base-patch32-224-in21k
15

--- PAGE 16 ---
Methods Genre 1 Genre 2 Genre 3 Genre 4
Train from Scratch 1.0102±0.0003 1 .0884±0.0004 1 .0555±0.0000 1 .0502±0.0000
Teacher 1.0120±0.0000 1 .0854±0.0000 1 .0563±0.0000 1 .0689±0.0000
LD 1.0083±0.0000 1 .1029±0.0000 1 .0601±0.0000 1 .0826±0.0000
FD 1.0075±0.0009 1 .0997±0.0030 1 .0537±0.0007 1 .0791±0.0005
FitNet 1.0018±0.0000 1 .1018±0.0000 1 .0534±0.0000 1 .0849±0.0000
Hybrid 1.0015±0.0008 1 .0955±0.0013 1 .0517±0.0012 1.0737±0.0014
Our Method ( TD)0.9965±0.0001 1 .0908±0.0002 1 .0475±0.0001 1.0761±0.0002
Methods Genre 5 Genre 6 Genre 7 Genre 8
Train from Scratch 1.1609±0.0000 1 .1160±0.0000 1 .0038±0.0000 1 .0937±0.0000
Teacher 1.1501±0.0000 1 .1090±0.0000 1 .0193±0.0000 1 .0660±0.0000
LD 1.1964±0.0000 1 .1415±0.0000 1 .0088±0.0000 1 .0602±0.0000
FD 1.1929±0.0013 1 .1269±0.0029 1 .0024±0.0000 1 .0527±0.0036
FitNet 1.1873±0.0000 1 .1260±0.0000 1 .0032±0.0000 1 .0605±0.0000
Hybrid 1.1843±0.0008 1 .1215±0.0028 1 .0076±0.0009 1 .0499±0.0014
Our Method ( TD)1.1656±0.0010 1 .1050±0.0000 1 .0022±0.0000 1 .0485±0.0013
Table 3: RMSE of rating prediction on MovieLens Genre 1 to Genre 8 (Dense to Sparse), compared
to baseline methods. bold numbers for the best improvement given a certain genre.
Methods CIFAR10 CIFAR100 ImageNet
No Distillation 0.93678 0 .74764 0 .48683
LD 0.93709 0 .76162 0 .48576
FD 0.93565 0 .74702 0 .48615
FitNet 0.93586 0 .75164 0 .48828
Hybrid 0.93894 0 .76213 0 .48691
Our Method ( TD)0.94100 0 .76562 0 .49930
Table 4: Accuracy of image classification tasks, compared to baseline methods. bold numbers for the
best results on a dataset.
6.3 Hyper-parameter tuning
Model parameter For the teacher model on MovieLens, we tune the model size with a upper limit
of cost along with learning rate, dropout rate and number of train steps. We don’t tune the size of
student model, but tune student model’s learning rate, dropout rate and number of train steps. For
ViT, we use the reported hyper-parameter setup (Dosovitskiy et al., 2020), with fine-tuning steps set
to 20000.
Baseline methods For each of the baseline methods, we tune their KD loss weight combined with
the groundtruch loss weight. For Label Distillation (Hinton et al., 2015) ( LD) it is the weight of
Llogit, For Feature Distillation (Heo et al., 2019) ( FD), it is the weight of Lfeature . For FitNet
(Romero et al., 2014), it is the weight of Lfitnet . And for Hybrid Distillation (Zhu and Wang, 2021)
(Hybrid ), we tune the weights of overall Llogit andLfeature and report the best results.
Our method We tune the three weights w1,w2andw3for our method, which corresponds to
the weight of Linteract ,LSCandLMC. We also tune the number of iterations for interactive
communication. For MovieLens, it is 0, 1, 2 or 3. And for ViT it is 0, 1 or 2. We report the results
with different iteration numbers in our ablation study in Section 4.3.
6.4 Computation resources
The training of MovieLens can be done on a CPU machine with less than 12 hours for all methods.
And the finetuning of ViT models runs on a 4-chip TPU, where all methods finish fine-tuning in 12
hours.
16

--- PAGE 17 ---
Methods Genre 1 Genre 2 Genre 3 Genre 4
No Interactions 1.0018±0.0002 1 .1016±0.0002 1.0475±0.0001 1.0767±0.0000
1 iteration 0.9971±0.0003 1.0908±0.0002 1.0502±0.0007 1 .0783±0.0002
2 to 3 iterations 0.9965±0.0001 1.0921±0.0005 1 .0508±0.0002 1.0761±0.0002
Methods Genre 5 Genre 6 Genre 7 Genre 8
No Interactions 1.1864±0.0014 1 .1249±0.0000 1.0022±0.0000 1.0656±0.0000
1 iteration 1.1667±0.0008 1 .1134±0.0008 1 .0033±0.0002 1 .0496±0.0026
2 to 3 iterations 1.1663±0.0016 1 .1050±0.0000 1.0039±0.0004 1.0495±0.0004
Table 5: RMSE of rating prediction on MovieLens Genre 1 to Genre 8 (Dense to Sparse), with
different number of iterations for interactive communication. bold numbers for the best results given
a certain genre.
Methods CIFAR10 CIFAR100 ImageNet
No Interactions 0.94089 0 .76460 0 .49873
1 iteration 0.94069 0 .76511 0 .49893
2 iterations 0.94100 0 .76562 0 .49930
Table 6: Accuracy of image classification tasks, with different number of iterations for interactive
communication. bold numbers for the best results on a dataset.
6.5 Additional experimental results
In this subsection, we include detailed experimental results. For experiments on MovieLens, both
teacher and student models are random initialized. For each result, we run the same setup 5 times and
report the mean RMSE with standard error. For experiments using image classification tasks, teacher
is pre-trained ViT and students are initialized using the learned weights from pre-trained teacher
(only the first four transformer layers), therefore the results have low variance and we only run each
setup once due to the limit of computation resources.
MovieLens100k per genre results Results (RMSE, lower is better) on MoiveLens are shown
in Table 3. From where we can see that our method outperforms baseline methods on 7 of the 8
genres. We also include the results of the teacher model, which is trained on all genres. The teacher
model is not fine-tuned to each downstream genre, and different genres can have very different data
distributions. Therefore, in some genres a model trained from scratch is better than the teacher model.
And distillation from teacher to student could even hurt the student’s performance for some genres.
This real challenge in recommendation tasks and many other downstream applications inspires us to
design the interactive communication process so that knowledge aligned with downstream tasks can
be transferred effectively. We can see that our method, though does not improve the student model on
some specific genres, can out-perform both teacher and student on most genres.
Vision Transformer results Results (classification accuracy, higher is better) for image classifica-
tion tasks are shown in Table 4. We can see that our method outperforms baseline methods on all
downstream tasks. Our improvement is most significant on ImageNet, which is a much more difficult
task compared to CIFAR10 and CIFAR100. Note that the pre-trained teacher cannot be directly
applied to downstream tasks, due to classification label mismatch, so we don’t report teacher’s results.
However, the fine-tuned results can be found in the ViT paper (Dosovitskiy et al., 2020) (0.98, 0.92,
0.81 for CIFAR10, CIFAR100 and ImageNet). We can see that there is still a huge gap between
teacher and student.
We want to point out that in this paper we don’t discuss the upper limit of the student model nor try
to close the gap between teacher and student. In our case, we expect the student with 4 transformer
layers to perform much worse than the teacher with 12 transformer layers. We want to verify that
by using the proposed interactive communication process, we can transfer more useful knowledge
from a powerful pre-trained foundation model to much smaller models for downstream applications,
compared to existing KD baseline methods.
17

--- PAGE 18 ---
Methods Genre 1 Genre 2 Genre 3 Genre 4
NoLMC 0.9965±0.0001 1 .0908±0.0003 1.0481±0.0010 1 .0767±0.0003
NoLSC 0.9968±0.0003 1 .0916±0.0004 1 .0501±0.0002 1 .0763±0.0002
Our Method( TD)0.9965±0.0001 1 .0908±0.0002 1 .0475±0.0001 1 .0761±0.0002
Methods Genre 5 Genre 6 Genre 7 Genre 8
NoLMC 1.1683±0.0007 1 .1104±0.0007 1 .0035±0.0000 1.0485±0.0013
NoLSC 1.1658±0.0012 1 .1058±0.0002 1 .0034±0.0001 1 .0494±0.0004
Our Method( TD)1.1656±0.0010 1 .1050±0.0000 1 .0022±0.0000 1 .0485±0.0013
Table 7: RMSE of rating prediction on MovieLens Genre 1 to Genre 8 (Dense to Sparse), ablating
different losses. bold numbers for the best results given a certain genre.
Methods Genre 1 Genre 2 Genre 3 Genre 4
Add Noise 0.9966±0.0001 1 .0910±0.0002 1 .0511±0.0003 1 .0762±0.0003
No Noise 0.9966±0.0002 1 .0912±0.0004 1 .0475±0.0001 1 .0766±0.0002
TrainE∗, D∗separately 1.0008±0.0002 1 .1066±0.0001 1 .0475±0.0001 1 .0812±0.0000
Train together 0.9965±0.0001 1 .0908±0.0002 1 .0508±0.0002 1 .0761±0.0002
Methods Genre 5 Genre 6 Genre 7 Genre 8
Add Noise 1.1663±0.0009 1 .1052±0.0001 1 .0033±0.0001 1 .0487±0.0014
No Noise 1.1668±0.0015 1 .1051±0.0002 1 .0026±0.0003 1 .0504±0.0012
TrainE∗, D∗separately 1.1782±0.0002 1 .1165±0.0002 1 .0022±0.0000 1 .0603±0.0002
Train together 1.1656±0.0010 1 .1050±0.0000 1 .0061±0.0001 1 .0485±0.0013
Table 8: RMSE of rating prediction on MovieLens Genre 1 to Genre 8 (Dense to Sparse), by adding
noise before teacher’s interpreting or separately training encoder/decoder.
Ablation of interactive communication We evaluate the effectiveness of interactive communica-
tion by changing the number of iterations for calculating the interactive communication loss Linteract .
Results on MovieLens are shown in Table 5 and results on image classification are shown in Table 6.
We can see that even without interactive communication, our method can outperform some baseline
methods. We think this is because the introduction of both LSCandLMCenables better alignment
between the student and teacher’s hidden states. It can be viewed as a combination of FitNet and
feature distillation. And by introducing interactive communication, we further improve the student
model.
Ablation of consistency losses We also evaluate the importance of the consistency losses we
introduced to help training the communication encoder and decoder. Results on MovieLens are
shown in Table 7 and results on image classification are shown in Table 9. For MovieLens, we can see
that both message consistency loss LMCand state consistency loss LSCare useful for most genres.
For ViT, we always add LMCsince we observe that without LMCthe communication encoder and
decoder is hard to train. And we see that LSCimproves the model on CIFAR10 and CIFAR100 but
not ImageNet. We think applying Linteract with multiple iterations can train the communication
encoder and decoder reasonable well, therefore the consistency losses may not always be useful on
all downstream tasks.
Adding noises during communication Inspired by ideas in self-training and semi-supervised
learning (Xie et al., 2020), where noise can be added to input or representation to improve the
generalization and robustness of knowledge transfer, we also explored the option of adding noise in
the interpreting process. Specifically, we add a small Gaussian noise on s′
h, which is the decoded
lower layer presentation for teacher model to interpret. Results on MovieLens are shown in Table 8
and results on image classification are shown in Table 9. We can see that adding noise can improve
performance on some downstream tasks but not all of them.
Separate training of encoder/decoder We also explored different ways of improving the learning
of communication encoder and decoder. One way is to introduce a ramp-up stage where only these
18

--- PAGE 19 ---
Methods CIFAR10 CIFAR100 ImageNet
NoLSC 0.94069 0 .76398 0 .49930
Add Noise 0.94089 0 .76562 0 .49917
No Noise 0.94100 0 .76511 0 .49930
Our Method( TD) 0.94100 0 .76562 0 .49930
Table 9: Accuracy of image classification tasks, ablating LSCor adding noises before teacher’s
interpreting.
encoders and decoders are trained. To do this, we first train student model a few steps (1000 on
MovieLens) and then we freeze the student model and only train both teacher and student’s encoders
and decoders for another few steps (500 or 1000 on MovieLens). We report the results on MoiveLens
in Table 8, where we can see it does not necessarily improve the student model’s performance.
One reason is that introducing this ramp-up step will relatively reduce the train steps of end-to-end
training. Therefore, it requires more tuning on learning rate, train steps to identify improvement
with this training schema. To keep the experiment and algorithm design as simple as possible, in our
experiments, we train everything (student model, both teacher and student’s encoders and decoders)
together in a single stage.
19
