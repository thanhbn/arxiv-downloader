# 2211.00789.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-distillation/2211.00789.pdf
# File size: 949359 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Beyond Not-Forgetting: Continual Learning with
Backward Knowledge Transfer
Sen Lin
School of ECEE
Arizona State University
slin70@asu.eduLi Yang
School of ECEE
Arizona State University
lyang166@asu.edu
Deliang Fan
School of ECEE
Arizona State University
dfan@asu.eduJunshan Zhang
Department of ECE
University of California, Davis
jazh@ucdavis.edu
Abstract
By learning a sequence of tasks continually, an agent in continual learning (CL) can
improve the learning performance of both a new task and ‚Äòold‚Äô tasks by leveraging
the forward knowledge transfer and the backward knowledge transfer, respectively.
However, most existing CL methods focus on addressing catastrophic forgetting in
neural networks by minimizing the modiÔ¨Åcation of the learnt model for old tasks.
This inevitably limits the backward knowledge transfer from the new task to the
old tasks, because judicious model updates could possibly improve the learning
performance of the old tasks as well. To tackle this problem, we Ô¨Årst theoretically
analyze the conditions under which updating the learnt model of old tasks could
be beneÔ¨Åcial for CL and also lead to backward knowledge transfer, based on the
gradient projection onto the input subspaces of old tasks. Building on the theoretical
analysis, we next develop a ContinUal learning method with Backward knowlEdge
tRansfer (CUBER), for a Ô¨Åxed capacity neural network without data replay. In
particular, CUBER Ô¨Årst characterizes the task correlation to identify the positively
correlated old tasks in a layer-wise manner, and then selectively modiÔ¨Åes the learnt
model of the old tasks when learning the new task. Experimental studies show that
CUBER can even achieve positive backward knowledge transfer on several existing
CL benchmarks for the Ô¨Årst time without data replay, where the related baselines
still suffer from catastrophic forgetting (negative backward knowledge transfer).
The superior performance of CUBER on the backward knowledge transfer also
leads to higher accuracy accordingly.
1 Introduction
One ultimate goal of artiÔ¨Åcial intelligence is to build an agent that can continually learn a sequence
of different tasks, so as to echo the remarkable learning capability of human beings during their
lifespan. With more tasks being learnt, the agent is expected to be able to learn a new task more
easily by leveraging the accumulated knowledge from old tasks (forward knowledge transfer), and
also further improve the learning performance of old tasks based on the gained knowledge of related
new tasks (backward knowledge transfer). Such a learning paradigm is known as continual learning
(CL) [5, 23], which has recently attracted much attention.
Most existing CL methods focus on addressing the catastrophic forgetting problem [ 20], i.e., the
neural network may easily forget the knowledge of old tasks when learning a new task. The main
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2211.00789v1  [cs.LG]  1 Nov 2022

--- PAGE 2 ---
strategy therein is to avoid the model change of old tasks when learning the new task. For example,
regularization-based methods (e.g., [ 12,1,18]) penalize the modiÔ¨Åcation of important weights of
old tasks; parameter-isolation based methods (e.g., [ 7,26,31,9]) Ô¨Åx the model learnt for old tasks;
and memory-based methods (e.g., [ 3,6,25]) aim to update the model with minimal interference
introduced to old tasks. While effectively mitigating catastrophic forgetting, such a model-freezing
strategy inevitably limits the backward knowledge transfer, by implicitly and conservatively treating
the model update of the new task as interference to old tasks. Intuitively, careful modiÔ¨Åcations of the
learnt model of old tasks may further improve the learning performance especially when the new task
shares similar knowledge to the old tasks.
This work seeks to explore CL from a new perspective by going beyond merely addressing forgetting:
When and how could we improve the learnt model of old tasks to facilitate backward knowledge
transfer? In particular, we consider a Ô¨Åxed neural network and the scenario where the data of old
tasks is not accessible when learning a new task, which naturally rules out the expansion-based
methods (e.g., [ 31,9]) and the experience-replay methods (e.g., [ 22,4]). Clearly, achieving backward
knowledge transfer is very challenging in this case as mitigating forgetting herein is already nontrivial.
To tackle this challenge, note that the orthogonal-projection based CL methods (e.g., [ 25,17]), which
update the model with gradients orthogonal to the input subspaces of old tasks, have demonstrated the
state-of-the-art performance under the setting above. Thus motivated, we propose a new orthogonal-
projection based CL method to carefully modify the learnt model of old tasks for better backward
knowledge transfer.
The main contributions of this work include 1) the quantiÔ¨Åcation of the conditions under which
modifying the learnt model of old tasks is beneÔ¨Åcial and 2) a new CL method inspired by the analysis,
so as to answer the question above. More speciÔ¨Åcally, we Ô¨Årst introduce notions of ‚ÄòsufÔ¨Åcient
projection‚Äô and ‚Äòpositive correlation‚Äô based on the gradient projection onto the subspaces of old
tasks to characterize the task correlation. We theoretically show that when the task gradients are
sufÔ¨Åciently aligned in the old task subspace, appropriately updating the learnt model of old tasks
when learning the new task could improve the learning performance and possibly lead to a better
model for the old tasks. Based on this analysis, we next propose an orthogonal-projection based
ContinUal learning method with Backward knowlEdge tRansfer (CUBER), which 1) Ô¨Årst identiÔ¨Åes
the positively correlated old tasks for the new task and 2) carefully updates the learnt model of selected
old tasks along with the model learning for the new task. The experimental results on standard CL
benchmarks show that CUBER can achieve the best backward knowledge transfer compared to all
the considered baseline CL methods, which consequently improves the overall learning performance.
In particular, to the best of our knowledge, CUBER is the Ô¨Årst to demonstrate positive backward
knowledge transfer on these benchmarks using a Ô¨Åxed capacity network without experience replay,
whereas all the compared baselines still suffer from catastrophic forgetting with negative backward
knowledge transfer.
2 Related work
Continual learning. Existing CL methods can be generally divided into three categories: 1)
Regularization-based methods (e.g., [ 12,1,15,18]) add additional regularization in the loss function
to penalize the model change of the old tasks when learning the new task. For example, Elastic Weight
Consolidation (EWC) [ 12] regularizes the update on the important weights that are evaluated using the
Fisher Information matrix; a recursive gradient optimization method is proposed in [ 18] to modify the
gradient direction for the objective function regularized by the model change during CL. 2) Parameter-
isolation based methods (e.g., [ 24,16,31,30,26,29]) allocate different model parameters to different
tasks, Ô¨Åx the parameters for old tasks by masking them out during the new task learning and expand
the network when needed. For example, Additive Parameter Decomposition (APD) [30] proposes a
hierarchical knowledge consolidation method to separate task-speciÔ¨Åc parameters and task-shared
parameters, where the task-speciÔ¨Åc parameters are kept intact to address forgetting. 3) Memory-based
methods can be further divided into experience-replay methods and orthogonal-projection based
methods depending on if the data of old tasks is available for the new task. Experience-replay methods
(e.g., [ 3,22,8]) store and replay the old tasks data when learning the new task, while orthogonal-
projection based methods (e.g., [ 32,6,25,17]) update the model in the orthogonal direction of
old tasks without storing the raw data of old tasks. In particular, [ 10] proposes natural continual
learning based on Bayesian learning to unify weight regularization and orthogonal gradient projection.
2

--- PAGE 3 ---
However, [ 10] leverages the knowledge of the current task in an implicit way to improve on the
old tasks, whereas our method attempts more explicitly to update the model of old tasks whenever
relevant new information is available.
Knowledge transfer. Most of CL methods focus on the catastrophic forgetting problem, and little
attention has been put on the knowledge transfer across tasks in CL with neural networks. Progressive
Network [ 24] considers the forward knowledge transfer but learns one model for each task. [ 17]
proposes trust region gradient projection (TRGP) to facilitate forward knowledge transfer from
correlated old tasks to the new task through a scaling matrix, but the model is still updated along
the direction orthogonal to the input subspaces of old tasks to mitigate forgetting. [ 11] studies the
knowledge transfer in a mixed sequence of similar and dissimilar tasks, but employs a complicated
task similarity detection mechanism where two additional networks need to be trained Ô¨Årst for each
task before learning the new task.
3 When could we improve the learnt model of old tasks?
In continual learning, a sequence of tasks T=ftgT
t=1arrives sequentially. For each task t, there is a
dataset Dt=f(xt
i;yt
i)gNt
i=1withNtsample pairs, which is sampled from some unknown distribution
Pt. In this work, we consider a Ô¨Åxed capacity neural network with weights w. When learning a new
taskt, we only have access to the dataset Dtand no datapoints of old tasks are available. We further
denoteL(w;Dt) =Lt(w)as the loss function for training, e.g., mean squared and cross-entropy
loss, and wtas the model after learning task t.
Intuitively, if the new task thas strong similarity with some old tasks, appropriate model update of
the new task would not introduce forgetting of the similar old tasks, but can lead to a better model for
these old tasks due to the backward knowledge transfer. To formally characterize this task similarity,
we introduce the following conditions on the task gradients.
DeÔ¨Ånition 1 (SufÔ¨Åcient Projection) .For any new task t2[1;T], we say it has sufÔ¨Åcient gradient
projection on the input subspace of old task j2[0;t 1]if for some12(0;1)
kProjSj(rLt(wt 1))k21krL t(wt 1)k2:
Here ProjSjdeÔ¨Ånes the projection on the input subspace Sjof taskj:ProjSj(A) =ABj(Bj)0for
some matrix AandBjis the bases for Sj. This deÔ¨Ånition of sufÔ¨Åcient projection follows the same
line of the trust region deÔ¨Åned in [ 17], which implies that task tandjmay have sufÔ¨Åcient common
bases between their input subspaces and hence are strongly correlated, because the gradient lies in
the span of the input [33].
DeÔ¨Ånition 2 (Positive Correlation) .For any new task t2[1;T], we say it has positive correlation
with old task j2[0;t 1]if for some22(0;1)
hrL j(wj);rLt(wt 1)i2krL j(wj)k2krL t(wt 1)k2:
While sufÔ¨Åcient projection suggests the possibly strong correlation between two tasks tandj,
DeÔ¨Ånition 2 goes one step further by introducing the positive correlation between tasks, in the sense
that the initial model gradient of task tis not conÔ¨Çicting with the model gradient when learning task
j. In fact, it can be shown that the positive correlation implies the sufÔ¨Åcient projection in DeÔ¨Ånition 1
for some12. Note that both DeÔ¨Ånition 1 and 2 characterize the correlation based on the initial
model wt 1, which allows the task correlation detection before learning the new task t.
For ease of exposition, consider the scenario with a sequence of two tasks 1 and 2. Let F(w) =
L(w;D1)+L(w;D2),g1(w) =rwL(w;D1)andg2(w) =rwL(w;D2). Given the model learnt
for task 1 as w1, we consider the following two model update rules for learning task 2, where only
the data of task 2 is available :
‚Ä¢ (Rule #1):wk+1=wk [g2(wk) ProjS1(g2(wk))],
‚Ä¢ (Rule #2):wk+1=wk g2(wk).
Here w0=w1andk2[0;K 1]. Rule #1characterizes the model update of the orthogonal-
projection based methods, where no interference is introduced to task 1 as the model change is
orthogonal to the input subspace S1of task 1 [ 17] (and hence the learnt model of task 1 will not
3

--- PAGE 4 ---
be updated). In contrast, the vanilla gradient descent Rule #2for learning task 2 will inevitably
modify the learnt model w1for task 1 unless ProjS1(g2(wk)) = 0 , i.e., the extreme case where both
forgetting and knowledge transfer do not occur. In what follows, we evaluate the performance of
these update rules.
Theorem 1. Suppose that the loss LisB-Lipschitz andH
2-smooth. Let <minn
1
H;kg1(w0)k
HBKo
and2(2+2)kg1(w0)k
4kg2(w0)kfor some2(0;1). We have the following results:
(1) IfLis convex, Rule #2for task 2 converges to the optimal model w= arg minF(w);
(2) IfLis nonconvex, Rule #2for task 2 converges to the Ô¨Årst order stationary point, i.e.,
min
kkrF (wk)k2<2
K[F(w0) F(w)] +4 +2
2kg1(w0)k2:
Theorem 1 indicates that updating the model with Rule #2will lead to the convergence to the
minimizer of the joint objective function F(w)in the convex case, and the convergence to the Ô¨Årst
order stationary point in the nonconvex case, when task 1 and 2 satisfy the positive correlation with
2(2+2)kg1(w0)k
4kg2(w0)k. That is to say, Rule #2not only results in a good model for task 2, but can also
be beneÔ¨Åcial for the joint learning of task 1 and 2. Note that since w0is the learnt model of task 1, in
general we havekg1(w0)k<kg2(w0)k. Proof of Theorem 1 can be found in Appendix A.
Theorem 2. Suppose that the loss LisB-Lipschitz andH
2-smooth. We have the following results:
(1) Let wcandwrbe the model parameters after applying one update to some initial model wby
using Rule #1and Rule #2, respectively. Suppose <minn
1
H;kg1(w0)k
HBKo
,1q
1+2H
2+Hand
2(2+2)kg1(w0)k
4kg2(w0)kfor some2(0;1). It follows thatF(wr)F(wc);
(2) Let wkbe thek-th iterate for task 2 with Rule #2. Suppose thathg1(w0);g2(wi)i 
2kg1(w0)kkg2(wi)kfori2[0;k 1]and42kg1(w0)k
HBk1:5. It follows thatL1(wk)L 1(w1).
Intuitively, the Ô¨Årst part of Theorem 2 shows that updating with Rule #2can achieve lower loss
value compared to Rule #1after one step gradient update when task 1 and 2 satisfy the sufÔ¨Åcient
projection with 1q
1+2H
2+Hand the positive correlation with 2(2+2)kg1(w0)k
4kg2(w0)k. If the positive
correlation condition also holds for iterates of Rule #2when learning the task 2, the second part of
Theorem 2 indicates that updating the model indeed leads to a better model for task 1 with respect to
L1. In a nutshell, when 1) the task 2 has sufÔ¨Åcient gradient projection onto the subspace of task 1 and
2) the projected gradient is also aligned well with the gradient of task 1 in the subspace of task 1,
updating the model along g2will modify the learnt model w1towards a favorable direction for CL
and enable the backward knowledge transfer to task 1. Proof of Theorem 2 is in Appendix B.
It is worth noting that the conditions for Theorem 1 and 2 depend only on the initial model gradient
g2(w1)before learning task 2 and the gradient g1(w1)when learning task 1, which are both easily
accessible and calculated. Particularly, the positive correlation can be evaluated by only storing
the gradient g1(w1)instead of the data of task 1. In stark contrast, the task gradient correlation
characterization in [ 3,22] involves the gradient evaluation of old tasks with respect to the current
model weight, and hence requires the data of old tasks when learning the new task.
4 Continual learning with backward knowledge transfer
Based on the theoretical analysis above, we next propose a continual learning method with backward
knowledge transfer (CUBER), by selectively updating the learnt model of old tasks when learning
the new task. In particular, CUBER works in a layer-wise manner: Given a L-layer network, CUBER
Ô¨Årst characterizes the task correlation for each layer, and then employs different strategies to learn the
new task depending on the task correlation.
More speciÔ¨Åcally, denote the set of weights as w=fwlgL
l=1, where wlis the layer-wise weight
for layerl. Given the data input xt
ifor taskt, letxt
l;ibe the input of layer landxt
1;i=xt
i. Denote
fas the operation of the network layer. The output xt
l+1;ifor layerlcan be then computed as
4

--- PAGE 5 ---
Figure 1: A simple illustration of the layer-wise task correlation detection. Given the new task t,
an old taskjbelongs to (1) Regime 1 if the initial model gradient rLt(wt 1
l)of taskthas small
projection onto the subspace Sj
lof taskj; (2) Regime 2 if the strong projection condition is satisÔ¨Åed
while the projection of rLt(wt 1
l)ontoSj
lis not aligned well with the gradient of task j; (3) Regime
3 if both the strong projection condition and the positive correlation condition are satisÔ¨Åed.
xt
l+1;i=f(wl;xt
l;i). Here we denote xt
l;ias the representations of the input xt
iat layerl. Given a
new taskt2, we characterize its task correlation with some old task j2[1;t 1]for layerlinto
three different regimes based on DeÔ¨Ånition 1 and 2.
Regime 1 (no forgetting): We say that task j2Regt
l;1for layerlif the following holds:
kProjSj
l(rLt(wt 1
l))k1krL t(wt 1
l))k:
In this case, the layer-wise input subspace Sj
lfor taskjandSt
lfor tasktare treated as nearly
orthogonal ((a) in Fig. 1). As a result, there is little knowledge transfer between these two tasks, and
updating the model along with rLt(wl)would not introduce much interference to task j. To reinforce
the knowledge protection for task j, the model will be updated based on orthogonal projection:
rLt(wl)  rL t(wl) ProjSj
l(rLt(wl)):
Regime 2 (forward knowledge transfer) : We say that task j2Regt
l;2for layerlif the following holds:
kProjSj
l(rLt(wt 1
l))k1krL t(wt 1
l))k;
hrL j(wj
l);rLt(wt 1
l)i2krL j(wj
l)kkrL t(wt 1
l)k:
In this case, task jand tasktcan be strongly correlated for layer lbut possibly with ‚Äònegative‚Äô corre-
lation ((b) in Fig. 1), in the sense that updating the model along with rLt(wl)would substantially
modify the learnt model ProjSj
l(wt 1)for taskjin an unfavorable way and lead to the forgetting
of taskj. As there will be better forward knowledge transfer from the old task jto the new task
tfor layerl, we leverage the scaled weight projection in [ 17] to facilitate the forward knowledge
transfer through a scaling matrix Qj;t
l, whereas the model is still updated using orthogonal projection
to protect the knowledge of old tasks:
rLt(wl)  rL t(wl) ProjSj
l(rLt(wl));
Qj;t
l  Qj;t
l rQLt(wl ProjSj
l(wl) +wlBj
lQj;t
l(Bj
l)0): (1)
HereBj
lis the bases matrix for subspace Sj
l. Intuitively, the scaled weight projection wlBj
lQj;t
l(Bj
l)0
replaces the weight projection ProjSj
l(wl)of taskjby a scaled version, which transforms the
knowledge of task jto the appropriate model of the new task tthrough the optimization of Qj;t
l.
Regime 3 (backward knowledge transfer) : We say task j2Regt
l;3for layerlif the following holds:
kProjSj
l(rLt(wt 1
l))k1krL t(wt 1
l))k;
hrL j(wj
l);rLt(wt 1
l)i2krL j(wj
l)kkrL t(wt 1
l)k:
5

--- PAGE 6 ---
With the sufÔ¨Åcient projection and the positive correlation, updating the model along with rLt(wl)
could possibly lead to a better model for continual learning and also improve the learning performance
of the old task j((c) in Fig. 1). To avoid overly-optimistic modiÔ¨Åcation on the learnt model of task
j, we further regularize the projection of the model change on the subspace Sj
l, given that the
model projection is indeed frozen for task jto address forgetting with orthogonal projection, i.e.,
ProjSj
l(wt 1
l) = ProjSj
l(wj
l). This gives the following model update:
wl  wl r[Lt(wl) +kProjSj
l(wl wt 1
l)k]:
Note that the gradient projection on the subspaces of old tasks in Regime 1 and 2 is removed from the
gradient in the model update above. Besides, we also learn a scaling matrix Qj;t
lfor better forward
knowledge transfer from the old task j2Regt
l;3to the new task tas in Eq. (1).
Figure 2: A simple illustration
of the case where updating the
model withrLtbeneÔ¨Åts the
old taskj2Regt
l;3at the be-
ginning but eventually conÔ¨Çicts
withrLj.However, continuous model update with task tgradientrLt(wl)
will eventually lead to the task speciÔ¨Åc model for task t, which
usually differs from the model of task jin Regime 3 (Fig. 2).
To address this problem, note that the second part of Theorem
2 characterizes the condition under which updating the model
withrLt(wl)will result in backward knowledge transfer. Thus
motivated, for a model update iterate wl;katk-th iteration when
learning the new task t, we evaluate the following condition for
taskj2Regt
l;3:
hrLj(wj
l);rLt(wl;k)i2krLj(wj
l)kkrLt(wl;k)k(2)
and degenerate task jto Regime 2, i.e., remove the gradient projec-
tion on the subspace Sj
lfrom the task tgradient and stop modifying
the model for task j, if the condition (2) does not hold.
Bases extraction: After learning the model wtfor taskt, we construct the input subspace for each
layerlby extracting bases from its representation xt
l;ibased on singular value decomposition (SVD)
[17]. More speciÔ¨Åcally, given a batch of nsamples and the learnt model wt, the representation matrix
for layerlis denoted as Rt
l= [xt
l;1;xt
l;2;:::;xt
l;n]. Since the bases of the old tasks may include
important bases for the new task t, we determine the bases for task tfrom the union of the bases of
the old tasks and the newly generated bases. Towards this end, we Ô¨Årst concatenate the bases Bj
lfor
j2[0;t 1]together in a matrix Ot
land remove the common bases. Then SVD is applied on the
residual representation matrix ~Rt
l=Rt
l Rt
lOt
l(Ot
l)0, i.e., ~Rt
l=Ut
lt
l(Vt
l)0, whereUt
lis the left
singular matrix. We construct Bt
lby selecting the most important bases from the pool of bases in Ot
l
andUt
ldepending on their eigenvalues, which yields a low rank matrix approximation of Rt
l.
To conclude, the optimization problem for learning the new task tcan be summarized as follows:
min
w;fQj;t
lgl;j2Regt
l;2SRegt
l;3Lt(f~wlgl) +X
lX
j2Regt
l;3kProjSj
l(wl wt 1
l)k; (3)
s:t: ~wl=wl+X
j2Regt
l;2SRegt
l;3[wlBj
lQj;t
l(Bj
l)0 ProjSj
l(wl)];
rLt(wl) =rLt(wl) X
j2Regt
l;1SRegt
l;2ProjSj
l(rLt(wl)):
The key idea is that we conservatively update the model for old tasks in Regime 3 while using
orthogonal projection to preserve the knowledge of other old tasks; in the meanwhile, we leverage
the scaled weight projection to reuse the model knowledge of old tasks in both Regime 2 and 3 to
facilitate forward knowledge transfer. It is worth to note that the task correlation is determined before
learning the new task t, as both the strong projection condition and the positive correlation condition
only depend on the initial model gradient for the new task. And this can be achieved by a simple
forward-backward pass through the initial model with a batch of new task data. The overview of
CUBER can be found in Algorithm 1.
6

--- PAGE 7 ---
Algorithm 1 Continual learning with backward knowledge transfer (CUBER)
1: Input: task sequence T=ftgT
t=1;
2: Learn the Ô¨Årst task using vanilla stochastic gradient descent;
3: Extract the bases fB1
lgbased on SVD using the learnt model w1;
4:foreach tasktdo
5: Calculate gradient rLt(wt 1);
6: Evaluate the strong projection and the positive correlation conditions for layer-wise task correlation
detection to determine Regt
l;1,Regt
l;2andRegt
l;3;
7: fork=1, 2,... do
8: Update the model and scaling matrices by solving Eq. (3);
9: fortaskj <t andj2Regt
l;3do
10: ifhrL j(wj
l);rLt(wl;k)i< 2krL j(wj
l)kkrL t(wl;k)kthen
11: Degenerate task j to Regt
l;2;
12: end if
13: end for
14: end for
15: Store the gradient rLt(wt)in the task memory;
16: Extract the bases fBt
lgbased on SVD using the learnt model wt;
17:end for
5 Experiments
Datasets. We evaluate the performance of CUBER on four standard CL benchmarks. (1) Permuted
MNIST: a variant of the MNIST dataset [ 14] where random permutations are applied to the input
pixels. Following [ 19,25], we divide the dataset into 10 tasks with different permutations and each
task includes 10 classes. (2) Split CIFAR-100: we divide the CIFAR-100 dataset [ 13] into 10 different
tasks, where each task is a 10-way multi-class classiÔ¨Åcation problem. (3) 5-Datasets: we consider a
sequence of 5 datasets, i.e., CIFAR-10, MNIST, SVHN [ 21], not-MNIST[ 2], Fashion MNIST[ 28],
and the classiÔ¨Åcation problem on each dataset is a task. (4) Split MiniImageNet: we divide the
MiniImageNet dataset [27] into 20 tasks, where each task includes 5 classes.
Baselines. In this work, we compare CUBER with the following baselines on the benchmarks
mentioned above. (1) EWC [ 12]: a regularization-based method that leverages Fisher Information
matrix for weights importance evaluation; (2) HAT [ 26]: learns a hard attention mask to preserve the
knowledge of old task; (3) Orthogonal Weight Modulation (OWM) [ 32]: learns a projector matrix to
project the gradient of the new task to the orthogonal direction of input subspace of old tasks; (4)
Gradient Projection Memory (GPM) [ 25]: stores the bases of the input subspace of old tasks and then
updates the model with the gradient projection orthogonal to the subspace spanned by these bases;
(5) TRGP [ 17]: proposes a scaled weight projection to facilitate the forward knowledge transfer from
related old tasks to the new task while updating the model based on orthogonal gradient projection,
which demonstrates the state-of-the-art performance for a Ô¨Åxed capacity network; (6) Averaged GEM
(A-GEM) [ 3]: constrains the new task learning with the gradient calculated using the stored data of
old tasks; (7) Experience Replay with Reservior sample (ER_Res) [ 4]: uses a small episodic memory
to store old task samples for addressing forgetting; (8) Multitask: jointly learns all tasks once with a
single network using all datasets, which usually serves as a performance upper bound in CL [25].
Network and training details. For a given dataset, we study all CL methods using the same network
architecture. More speciÔ¨Åcally, for Permuted MNIST, we consider a 3-layer fully-connected network
including 2 hidden layers with 100 units. And we train the network for 5 epochs with a batch
size of 10 for every task. For Split CIFAR-100, we use a version of 5-layer AlexNet by following
[25,17]. When learning each task, we train the network for a maximum of 200 epochs with early
termination based on the validation loss, and use a batch size of 64. For 5-Datasets, we use a reduced
ResNet-18 [ 19] and follow the same training strategy as in Split CIFAR-100. For Split MiniImageNet,
a reduced ResNet-18 is also used, and we train the network for a maximum of 100 epoches with early
termination. The batch size is 64. Similar to [ 17], we select at most two tasks to be in Regime 2 and
3 for each layer with the largest gradient projection norm, to reduce the performance sensitivity on
the choice of 1. In the experiments, we set 1= 0:5.
To evaluate the learning performance, we consider the following two metrics, i.e., accuracy (ACC)
which measures the Ô¨Ånal accuracy averaged over all tasks, and backward transfer (BWT) which
7

--- PAGE 8 ---
measures the average accuracy change of each task after learning new tasks:
ACC =1
TXT
i=1AT;i; BWT =1
T 1XT 1
i=1(AT;i Ai;i)
whereAi;jrepresents the testing accuracy of task jafter learning task i.
5.1 Main results
As shown in Table 1, CUBER demonstrates the best performance of BWT on all datasets compared
to the baselines. In particular, positive BWT can be obtained by CUBER on Split CIFAR-100 and
Split MiniImageNet, which has not been achieved by previous works in a Ô¨Åxed capacity network
without data-replay . For 5-Dataset, since the tasks therein are less related to each other as in GPM
[25], we do not expect much knowledge transfer across tasks (but knowledge transfer still exists in
terms of the layer-wise features), and there is no forgetting in CUBER. Achieving zero-forgetting is
very difÔ¨Åcult for Permuted MNIST, because all the tasks share one output layer and there is no task
identiÔ¨Åer during testing (domain-incremental) [ 25]. However, even in this case CUBER still achieves
the best BWT, i.e, nearly non-forgetting, among all methods. Clearly, the strong performance on
BWT indicates that CUBER can effectively facilitate the backward knowledge transfer by wisely
modifying the learnt model of old tasks.
BeneÔ¨Åting from the superior performance on the backward knowledge transfer, CUBER also achieves
the best or comparable performance on the averaged accuracy. More speciÔ¨Åcally, CUBER improves
around 1%in ACC over the best prior results on Split CIFAR-100, Split MiniImageNet and Permuted
MNIST, while showing the comparable performance with the state-of-the-art method TRGP even
on 5-datasets where tasks are less correlated. Moreover, CUBER performs better than Multitask on
both 5-Dataset and Permuted MNIST, which implies the importance of studying knowledge transfer
in CL: Both TRGP and CUBER outperform Multitask on 5-Dataset because of the scaled weight
projection to facilitate forward knowledge transfer, whereas by facilitating backward knowledge
CUBER becomes the only method that outperforms Multitask on Permuted MNIST.
Table 1: The ACC and BWT with the standard deviation values over 5 different runs on different
datasets. Here for Split CIFAR-100, Split MiniImageNet and 5-Dataset we use a multi-head network,
while a single-head network is used for Permuted MNIST. Moreover, 2= 0:0.
MethodMulti-head Domain-incremental
Split CIFAR-100 Split MiniImageNet 5-Dataset Permuted MNIST
ACC(%) BWT(%) ACC(%) BWT(%) ACC(%) BWT(%) ACC(%) BWT(%)
Multitask 79:580:54 - 69:460:62 - 91:540:28 - 96:700:02 -
OWM 50:940:60 301 - - - - 90:710:11 10
EWC 68:800:88 21 52:012:53 123 88:640:26 41 89:970:57 41
HAT 72:060:50 00 59:780:57 30 91:320:18 10 - -
A-GEM 63:981:22 152 57:240:72 121 84:040:33 121 83:560:16 141
ER_Res 71:730:63 61 58:940:85 71 88:310:22 40 87:240:53 111
GPM 72:480:40 0:90 60:410:61 0:70:4 91:220:20 10 93:910:16 30
TRGP 74:460:32 0:90:01 61:780:60 0:50:6 93:5693:5693:560:10 0:040:01 96:340:11 0:80:1
CUBER (ours) 75:5475:5475:540:22 0:130:130:130:08 62:6762:6762:670:74 0:230:230:230:15 93:4893:4893:480:10 0:000:000:000:02 97:2597:2597:250:00 0:02 0:02 0:020:00
5.2 Ablation studies
Table 2: The comparison between CUBER and TRGP in OL-CIFAR100. The selected old task (*) in
(b) represents the old task with the largest number of layers in Regime 3 of the new task.
(a) The comparison of ACC and BWT.
Method ACC( %) BWT( %)
CUBER 74.94 0.28
TRGP 74.33 -0.18(b) BWT-S of the selected old tasks.
Task 1 Task 2 Task 4 Task 5 Average
selected old task* Task 0 Task 1 Task 3 Task 0 -
BWT-S (CUBER) 0.50 0.71 0.03 0.07 0.33
BWT-S (TRGP) -0.20 0.10 -0.60 -0.20 -0.23
Backward knowledge transfer. The value of backward knowledge transfer indicates the average
accuracy improvement for each old task after learning all tasks, which implies that the new task
learning provides additional useful information for learning features of similar old tasks. Intuitively,
this value should depend on the task similarity in CL. To better understand the advantage of CUBER
8

--- PAGE 9 ---
in terms of backward knowledge transfer, we further consider a special setup which includes a
sequence of similar and dissimilar tasks. SpeciÔ¨Åcally, different with Split-CIFAR100 where no tasks
have overlapping classes, we split the Ô¨Årst 50 classes in CIFAR100 into 7 tasks (OL-CIFAR100):
Task 0-6 contain classes 0-9, 5-14, 10-19, 20-29, 25-34, 30-39, 40-49, respectively. We compare the
performance of CUBER with TRGP in Table 2. As shown in Table 2a, CUBER clearly outperforms
TRGP in both ACC and BWT. In particular, CUBER has a positive BWT of 0:28%, where TRGP
suffers from forgetting. We further analyze the task selections in Table 2b, where ‚Äúselected old task‚Äù
refers to the task that has the largest number of layers in Regime 3 of the new task. For example,
according to the setup of OL-CIFAR100, the selected old task for Task 4 should be Task 3 with a
high probability as they have overlapping classes. And we denote a new metric, namely BWT-S, to
evaluate the backward knowledge transfer of the selected old task after learning the new task, i.e.,
BWT-S=At;j At 1;jwherejis the ‚Äúselected old task‚Äù of the new task t. As shown in Table 2b,
CUBER correctly identiÔ¨Åes the correlated tasks for most new tasks except Task 5. This is a reasonable
result because the task correlation detection is based on the initial model gradient which is noisy
in general, and hence only serves as an estimation of underlying true correlation. However, such
an estimated task correlation characterization is indeed sufÔ¨Åcient to effectively facilitate backward
knowledge transfer, as corroborated by the positive BWT-S and the superior performance of CUBER.
Forward knowledge transfer. We also evaluate the forward knowledge transfer (FWT) in CUBER,
compared to the best two baseline methods GPM and TRGP. Here the FWT measures the gap between
Ai;iand the accuracy of learning task ionly from scratch. For simplicity, we use the FWT of GPM as
a baseline and evaluate the improvements of TRGP and CUBER over GPM. It can be seen from Table
3 that CUBER performs even better than TRGP although CUBER follows the same strategy, i.e., the
scaled weight projection, to facilitate the forward knowledge transfer, and achieves the best FWT
among the three methods in most cases. The reason behind is that the characterization of Regime 3 in
CUBER not only allows the modiÔ¨Åcation of the learnt model of the old tasks to prompt the backward
knowledge transfer, but also relaxes the constraint on the gradient update for the new task, i.e., the
model can be now updated in the subspace of the selected old tasks for learning the new task. This
gradient constraint relaxation consequently leads to better model learning of the new task.
Table 3: Comparison of FWT among GPM, TRGP and CUBER. The value for GPM is zero because
we treat GPM as the baseline and consider the relative FWT improvement over GPM.
Method Split CIFAR-100 Split MiniImageNet 5-Dataset Permuted MNIST
GPM 0 0 0 0
TRGP 2.01 2.36 1.98 0.18
CUBER 2.79 3.13 1.96 0.8
Impact of2.It is clear that the selection of layer-wise Regime 3 depends on the value of the
threshold2. To show the impact of 2, we evaluate the learning performance under different values
of2in Split CIFAR-100. As shown in Table 4, the performance on ACC is comparable for all
three cases and the BWT decreases as the value of 2increases. Intuitively, 2characterizes the
conservatism in selecting tasks to Regime 3 and modifying the model of selected tasks. SpeciÔ¨Åcally,
with a larger 2, we just consider the backward knowledge transfer to the old tasks that are strongly
correlated with the new task, and only slightly modify the learnt model of these selected tasks,
because the condition Eq. (2) can be quickly violated with the model update and CUBER will stop
changing the learnt model of the selected tasks.
Table 4: The impact of 2on the performance in Split CIFAR-100.
2= 0:0 2= 0:2 2= 0:5
ACC(%) BWT(%) ACC(%) BWT(%) ACC(%) BWT(%)
75.54 0.22 75.73 0.03 75.55 0.01
6 Conclusion
In this work, we study the problem of backward knowledge transfer in CL. Different from most
existing methods that generally freeze the learnt model of the old tasks so as to mitigate catastrophic
forgetting, this study seeks to carefully modify the learnt model to facilitate backward knowledge
transfer from the new task to the old tasks. To this end, we Ô¨Årst introduce notions of strong projection
9

--- PAGE 10 ---
and positive correlation to characterize the task correlation, and show that when the task gradients
are sufÔ¨Åciently aligned in the old task subspace, appropriate model change for the old tasks could be
beneÔ¨Åcial for CL and result in better backward knowledge transfer. Based on the theoretical analysis,
we next propose CUBER to carefully learn the model for the new task, which would carefully update
the learnt model of the old tasks that are positively correlated with the new task. As shown in the
experimental results, CUBER can successfully improve the backward knowledge transfer on the
standard CL benchmarks in contrast to related baselines.
Impact and limitations. The mainstream strategy nowadays to address forgetting is to minimize
the interference to old tasks and avoid the learnt model change, which may however conÔ¨Çict with the
goal of CL, in the sense that the backward knowledge transfer from the new task to old tasks can be
restricted without modifying the learnt model. In this work, we go beyond this strategy and shed light
on the relationship between backward knowledge transfer and model modiÔ¨Åcation, by characterizing
the task correlations with gradient projection. We hope that this work will serve as initial steps and
motivate further research in CL community on the important while less explored problem, i.e., how
to design algorithms that can provide targeted treatments to achieve backward knowledge transfer.
However, CUBER also comes with several limitations. As in recent orthogonal-projection based
CL methods, CUBER extracts the bases of the task subspaces based on SVD, which may lead to
high computational cost for large dimensional data. How to reduce the complexity is an interesting
direction. Another limitation is that we assume that clear task boundaries exist between different
tasks. In future work, it is of great interests to extend CUBER to more general CL settings.
Acknowledgement
The work of S. Lin and J. Zhang was supported in part by the U.S. National Science Foundation
Grants CNS-2203239, CNS-2203412, RINGS-2148253, and CCSS-2203238. The work of L. Yang
and D. Fan was supported in part by the U.S. National Science Foundation Grants No. 1931871 and
No. 2144751.
References
[1]Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte-
laars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European
Conference on Computer Vision (ECCV) , pages 139‚Äì154, 2018.
[2]Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available:
http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html , 2, 2011.
[3]Arslan Chaudhry, Marc‚ÄôAurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. EfÔ¨Åcient
lifelong learning with a-gem. arXiv preprint arXiv:1812.00420 , 2018.
[4]Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K
Dokania, Philip HS Torr, and M Ranzato. Continual learning with tiny episodic memories.
2019.
[5]Zhiyuan Chen and Bing Liu. Lifelong machine learning. Synthesis Lectures on ArtiÔ¨Åcial
Intelligence and Machine Learning , 12(3):1‚Äì207, 2018.
[6]Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for
continual learning. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics , pages
3762‚Äì3773. PMLR, 2020.
[7]Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu,
Alexander Pritzel, and Daan Wierstra. Pathnet: Evolution channels gradient descent in super
neural networks. arXiv preprint arXiv:1701.08734 , 2017.
[8]Yunhui Guo, Mingrui Liu, Tianbao Yang, and Tajana Rosing. Improved schemes for episodic
memory based lifelong learning algorithm. In Conference on Neural Information Processing
Systems , 2020.
[9]Steven CY Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-
Song Chen. Compacting, picking and growing for unforgetting continual learning. arXiv
preprint arXiv:1910.06562 , 2019.
10

--- PAGE 11 ---
[10] Ta-Chu Kao, Kristopher Jensen, Gido van de Ven, Alberto Bernacchia, and Guillaume Hen-
nequin. Natural continual learning: success is a journey, not (just) a destination. Advances in
Neural Information Processing Systems , 34:28067‚Äì28079, 2021.
[11] Zixuan Ke, Bing Liu, and Xingchang Huang. Continual learning of a mixed sequence of similar
and dissimilar tasks. Advances in Neural Information Processing Systems , 33:18493‚Äì18504,
2020.
[12] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,
Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.
Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of
sciences , 114(13):3521‚Äì3526, 2017.
[13] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
[14] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/ ,
1998.
[15] Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang. Overcoming
catastrophic forgetting by incremental moment matching. arXiv preprint arXiv:1703.08475 ,
2017.
[16] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern
analysis and machine intelligence , 40(12):2935‚Äì2947, 2017.
[17] Sen Lin, Li Yang, Deliang Fan, and Junshan Zhang. Trgp: Trust region gradient projection for
continual learning. Tenth International Conference on Learning Representations, ICLR 2022 ,
2022.
[18] Hao Liu and Huaping Liu. Continual learning with recursive gradient optimization. arXiv
preprint arXiv:2201.12522 , 2022.
[19] David Lopez-Paz and Marc‚ÄôAurelio Ranzato. Gradient episodic memory for continual learning.
Advances in neural information processing systems , 30:6467‚Äì6476, 2017.
[20] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks:
The sequential learning problem. In Psychology of learning and motivation , volume 24, pages
109‚Äì165. Elsevier, 1989.
[21] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.
Reading digits in natural images with unsupervised feature learning. 2011.
[22] Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Ger-
ald Tesauro. Learning to learn without forgetting by maximizing transfer and minimizing
interference. arXiv preprint arXiv:1810.11910 , 2018.
[23] Mark Bishop Ring et al. Continual learning in reinforcement environments. 1994.
[24] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv
preprint arXiv:1606.04671 , 2016.
[25] Gobinda Saha, Isha Garg, and Kaushik Roy. Gradient projection memory for continual learning.
InInternational Conference on Learning Representations , 2021.
[26] Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In International Conference on Machine Learning ,
pages 4548‚Äì4557. PMLR, 2018.
[27] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks
for one shot learning. Advances in neural information processing systems , 29, 2016.
[28] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
[29] Li Yang, Sen Lin, Junshan Zhang, and Deliang Fan. Grown: Grow only when necessary for
continual learning. arXiv preprint arXiv:2110.00908 , 2021.
[30] Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Scalable and order-robust
continual learning with additive parameter decomposition. In Eighth International Conference
on Learning Representations, ICLR 2020 . ICLR, 2020.
11

--- PAGE 12 ---
[31] Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with
dynamically expandable networks. arXiv preprint arXiv:1708.01547 , 2017.
[32] Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continual learning of context-dependent
processing in neural networks. Nature Machine Intelligence , 1(8):364‚Äì372, 2019.
[33] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM , 64(3):107‚Äì
115, 2021.
Checklist
The checklist follows the references. Please read the checklist guidelines carefully for information on
how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or
[N/A] . You are strongly encouraged to include a justiÔ¨Åcation to your answer , either by referencing
the appropriate section of your paper or providing a brief inline description. For example:
‚Ä¢ Did you include the license to the code and datasets? [Yes] See Section ??.
‚Ä¢Did you include the license to the code and datasets? [No] The code and the data are
proprietary.
‚Ä¢ Did you include the license to the code and datasets? [N/A]
Please do not modify the questions and only use the provided macros for your answers. Note that the
Checklist section does not count towards the page limit. In your paper, please delete this instructions
block and only keep the Checklist section heading above along with the questions/answers below.
1. For all authors...
(a)Do the main claims made in the abstract and introduction accurately reÔ¨Çect the paper‚Äôs
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See Section 6.
(c)Did you discuss any potential negative societal impacts of your work? [Yes] See
Section 6.
(d)Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a)Did you state the full set of assumptions of all theoretical results? [Yes] See Theorem 1
and 2.
(b) Did you include complete proofs of all theoretical results? [Yes] See the appendix.
3. If you ran experiments...
(a)Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [Yes] We include the
code in the supplemental material.
(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] See section 5.
(c)Did you report error bars (e.g., with respect to the random seed after running experi-
ments multiple times)? [Yes] See section 5.
(d)Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] See the appendix.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] See section 5.
(b) Did you mention the license of the assets? [Yes] See the appendix.
(c)Did you include any new assets either in the supplemental material or as a URL? [Yes]
We include code in the supplemental material.
(d)Did you discuss whether and how consent was obtained from people whose data you‚Äôre
using/curating? [N/A]
12

--- PAGE 13 ---
(e)Did you discuss whether the data you are using/curating contains personally identiÔ¨Åable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects...
(a)Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b)Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c)Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
13

--- PAGE 14 ---
Appendix
A Proof of Theorem 1
For aH=2-smooth loss function L, it can be easily shown that FisH-smooth.
(1) For anyk2[0;K], we can have
F(wk+1)F(wk) +rF(wk)T(wk+1 wk) +H
2kwk+1 wkk2
=F(wk) + (g1(wk) +g2(wk))T( g2(wk)) +2H
2kg2(wk)k2
=F(wk) 
 2H
2
kg2(wk)k2 hg1(wk);g2(wk)i: (4)
For the termhg1(wk);g2(wk)i, it follows that
hg1(wk);g2(wk)i
=hg1(wk) g1(w0) +g1(w0);g2(wk)i
=hg1(wk) g1(w0);g2(wk)i+hg1(w0);g2(wk)i
=hg1(wk) g1(w0);g2(wk)i+hg1(w0);g2(wk) g2(w0)i+hg1(w0);g2(w0)i: (5)
Because
2hg1(wk) g1(w0);g2(wk)i+kg1(wk) g1(w0)k2+kg2(wk)k2
=kg1(wk) g1(w0) +g2(wk)k20;
we have
hg1(wk) g1(w0);g2(wk)i 1
2kg1(wk) g1(w0)k2 1
2kg2(wk)k2: (6)
Following the same line, it can be shown that
hg1(w0);g2(wk) g2(w0)i 1
2kg2(wk) g2(w0)k2 1
2kg1(w0)k2: (7)
Combining Eq. (5), Eq. (6) and Eq. (7) gives a lower bound on hg1(wk);g2(wk)i, i.e.,
hg1(wk);g2(wk)i
 1
2kg1(wk) g1(w0)k2 1
2kg2(wk)k2
 1
2kg2(wk) g2(w0)k2 1
2kg1(w0)k2+hg1(w0);g2(w0)i
 H2
8kwk w0k2 1
2kg2(wk)k2
 H2
8kwk w0k2 1
2kg1(w0)k2+hg1(w0);g2(w0)i
= H2
4kwk w0k2 1
2kg2(wk)k2 1
2kg1(w0)k2+hg1(w0);g2(w0)i; (8)
where the second inequality is true because of the smoothness of the loss function.
Based on the update Rule #2, it can be seen that
wk=w0 k 1X
i=0g2(wi): (9)
Therefore, continuing with Eq. (4), we can have
F(wk+1)
F(wk) 
 2H
2
kg2(wk)k2 hg1(wk);g2(wk)i
14

--- PAGE 15 ---
F(wk) 
 2H
2
kg2(wk)k2+3H2
4kk 1X
i=0g2(wi)k2+
2kg2(wk)k2
+
2kg1(w0)k2 hg1(w0);g2(w0)i
=F(wk) 
2 2H
2
kg2(wk)k2+3H2
4kk 1X
i=0g2(wi)k2+
2kg1(w0)k2 hg1(w0);g2(w0)i
F(wk) 
2 2H
2
kg2(wk)k2+3H2
4kk 1X
i=0g2(wi)k2+
2kg1(w0)k2
 2kg1(w0)kkg2(w0)k;
where the last inequality is based on DeÔ¨Ånition 2.
Next, it can be shown that
kg1(w0)k
HBKkg1(w0)k
HkPk 1
i=0g2(wi)k:
It then follows that
1
2kg1(w0)k2+2H2
4kk 1X
i=0g2(wi)k2
1
2kg1(w0)k2+2kg1(w0)k2
4H2kPk 1
i=0g2(wi)k2H2kk 1X
i=0g2(wi)k2
=2 +2
4kg1(w0)k2: (10)
Therefore, we can obtain that
F(wk+1)F(wk) 
2 2H
2
kg2(wk)k2+(2 +2)
4kg1(w0)k2 2kg1(w0)kkg2(w0)k
F(wk) 
2 2H
2
kg2(wk)k2
<F(wk);
where the second inequality is true because 2(2+2)kg1(w0)k
4kg2(w0)k. This sufÔ¨Åcient decrease of the
objective function value indicates that the optimal F(w)can be obtained eventually for convex loss
functions.
(2) For a non-convex loss function L, we can have the following as in Eq. (4):
F(wr
k+1)F(wk) 
 2H
2
kg2(wk)k2 hg1(wk);g2(wk)i
(a)=F(wk) 
 2H
2
kg2(wk)k2 
2[krF (wk)k2 kg1(wk)k2 kg2(wk)k2]
=F(wk) 
2 2H
2
kg2(wk)k2 
2krF (wk)k2+
2kg1(wk)k2
=F(wk) 
2 2H
2
kg2(wk)k2 
2krF (wk)k2+
2kg1(wk) g1(w0) +g1(w0)k2
F(wk) 
2 2H
2
kg2(wk)k2 
2krF (wk)k2+kg1(wk) g1(w0)k2
+kg1(w0)k2
(b)
F(wk) 
2 2H
2
kg2(wk)k2 
2krF (wk)k2+H23
4kk 1X
i=0g2(wi)k2
15

--- PAGE 16 ---
+kg1(w0)k2;
where (a) is because rF(wk) =g1(wk) +g2(wk), and (b) is because of the smoothness of Land
Eq. (9).
Therefore,
min
kkrF (wk)k2
1
KK 1X
k=0krF (wk)k2
2
KK 1X
k=0"
F(wk) F(wk+1) +H23
4kk 1X
i=0g2(wi)k2+kg1(w0)k2 
2 2H
2
kg2(wk)k2#
2
K[F(w0) F(wK)] +H22
2(K 1)K 1X
k=1kk 1X
i=0g2(wi)k2+ 2kg1(w0)k2 1 H
KK 1X
k=0kg2(wk)k2
(a)
2
K[F(w0) F(wK)] +2
2kg1(w0)k2+ 2kg1(w0)k2 1 H
KK 1X
k=0kg2(wk)k2
2
K[F(w0) F(w)] +4 +2
2kg1(w0)k2 1 H
KK 1X
k=0kg2(wk)k2
2
K[F(w0) F(w)] +4 +2
2kg1(w0)k2
where (a) holds due to F(w)F(wK)andkPk 1
i=0g2(wi)k22
2H2kg1(w0)k2based on
Eq. (10).
B Proof of Theorem 2
(1) For Rule #1, we have
wc=w [g2(w) ProjS1(g2(w))] = w ~g2(w): (11)
For Rule #2, we have
wr=w g2(w): (12)
Based on the smoothness of the objective function, we can have an upper bound on F(wr):
F(wr)F(w) 
 2H
2
kg2(w)k2 hg1(w);g2(w)i; (13)
and a lower bound on F(wc):
F(wc)F(w) +hrF(w);wc wi H
2kwc wk2: (14)
Combining Eq. (13) and Eq. (14), it can be shown that
F(wr)
F(wc) hrF (w);wc wi+H
2kwc wk2 
 2H
2
kg2(w)k2 hg1(w);g2(w)i
=F(wc) hg1(w) +g2(w); ~g2(w)i+H2
2k~g2(w)k2 
 2H
2
kg2(w)k2
 hg1(w);g2(w)i
=F(wc) +hg1(w);~g2(w)i+hg2(w);~g2(w)i+H2
2k~g2(w)k2 
 2H
2
kg2(w)k2
 hg1(w);g2(w)i
=F(wc) +k~g2(w)k2+H2
2k~g2(w)k2 
 2H
2
kg2(w)k2 hg1(w);g2(w)i(15)
16

--- PAGE 17 ---
where the last equality is true because both g1(w)andProj1(g2(w))are orthogonal to ~g2(w).
For the termhg1(w);g2(w)i, based on Eq. (8), it follows that
hg1(w);g2(w)i
 H2
4kw w0k2 1
2kg2(w)k2 1
2kg1(w0)k2+hg1(w0);g2(w0)i
 H2
4kw w0k2 1
2kg2(w)k2 1
2kg1(w0)k2+2kg1(w0)kkg2(w0)k: (16)
Suppose that wis the model update at n-th iteration where nK. For Rule #1,
kw w0k2=2knX
i=0~g2(wi)k2
2kg1(w0)k2
H2B2K2nnX
i=0k~g2(wi)k2
2n2kg1(w0)k2
H2K2
2kg1(w0)k2
H2:
Similarly for Rule #2, we can also obtain that
kw w0k22kg1(w0)k2
H2:
Therefore, continuing with Eq. (16), we can have
hg1(w);g2(w)i
 2 +2
4kg1(w0)k2+2kg1(w0)kkg2(w0)k 1
2kg2(w)k2
 1
2kg2(w)k2
where the last inequality holds because 2(2+2)kg1(w0)k
4kg2(w0)k.
Based on Eq. (15), it follows that
F(wr)F(wc) +k~g2(w)k2+H2
2k~g2(w)k2 
 2H
2
kg2(w)k2+
2kg2(w)k2
=F(wc) 
2 2H
2
kg2(w)k2+
+2H
2
k~g2(w)k2
(a)
F(wc) 
2 2H
2
kg2(w)k2+ (1 2
1)
+2H
2
kg2(w)k2
(b)
F(wc)
where (a) holds because
kg2(w)k2=kProj1(g2(w)) + ~g2(w)k2
=kProj1(g2(w))k2+k~g2(w)k2
2
1kg2(w)k2+k~g2(w)k2;
and (b) is true because 1q
1+2H
2+H.
17

--- PAGE 18 ---
(2) It can be seen that
L1(wk)L 1(w0) +hg1(w0);wk w0i+H
4kwk w0k2
=L1(w0) +hg1(w0); k 1X
i=0g2(wi)i+2H
4kk 1X
i=0g2(wi)k2
=L1(w0) k 1X
i=0hg1(w0);g2(wi)i+2H
4kk 1X
i=0g2(wi)k2
L 1(w0) 2kg1(w0)k[k 1X
i=0kg2(wi)k] +2Hk
4k 1X
i=0kg2(wi)k2:
For42kg1(w0)k
HBk1:5, we can have
Hk
4k 1X
i=0kg2(wi)k22kg1(w0)k
Bp
kk 1X
i=0kg2(wi)k2
2kg1(w0)kPk 1
i=0kg2(wi)k2
qPk 1
i=0kg2(wi)k2
2kg1(w0)kvuutk 1X
i=0kg2(wi)k2
2kg1(w0)k[k 1X
i=0kg2(wi)k]:
Therefore, it follows that L1(wk)L 1(w0).
C More experimental results
We also study the accuracy evolution curves for each task during the continual learning procedure
in Figure 3. To clearly demonstrate the accuracy evolution behaviors, we selectively study task 2,
task 4, task 6 in PMNIST [ 14], CIFAR-100 Split [ 13], MiniImageNet [ 27], and task 1, task 2, task 3
in 5-Dataset, by comparing the performance among CUBER, TRGP and GPM. As shown in Figure
3, CUBER (the red curve) has the most stable performance on most tasks, as a result of the better
backward knowledge transfer facilitated by selectively updating the learnt model of old tasks. All the
experiments are conducted by using one Nvidia Quadro RTX 5000 GPU.
D More experimental details
D.1 Network details
3-layer fully-connected network: The network consists of 3 fully-connected layers with 784, 100,
100 units respectively. We use ReLU activation layer after the Ô¨Årst two layers.
5-layer AlexNet: Following [ 25,17], the AlexNet used for experiments on Split CIFAR-100 consists
of 3 convolutional layers and 2 fully-connected layers, where batch normalization is added in each
layer expect the classiÔ¨Åer layer. The convolutional layers have 64, 128 and 256 Ô¨Ålters with 44,
33and22kernel sizes, respectively, and each fully-connected layer contains 2048 units. We
use ReLU activation function and 22max-pooling after the convolutional layers, and dropout of
0.2 for the Ô¨Årst two layers and 0.5 for other layers.
A reduced ResNet-18: Following [ 19,25], we use a reduced ResNet-18 for experiments on both
5-Datasets and miniImageNet. It includes 17 convolutional layers with 3 short-cut connections. The
convolutional layers in 4 stages have 180, 360, 720, 1440 Ô¨Ålters, respectively. We adapt the 2x2
18

--- PAGE 19 ---
2 4 6 8
Number of tasks959697T est Accuracy (%)
Accuracy envolution for T ask 2
TRGP
GPM
Ours
4 6 8
Number of tasks95.596.096.597.0T est Accuracy (%)
Accuracy envolution for T ask 4
5 6 7 8 9
Number of tasks96.096.597.0T est Accuracy (%)
Accuracy envolution for T ask 6(a) PMNIST
2 4 6 8
Number of tasks666870T est Accuracy (%)
Accuracy envolution for T ask 2
TRGP
GPM
Ours
4 6 8
Number of tasks707274T est Accuracy (%)
Accuracy envolution for T ask 4
5 6 7 8 9
Number of tasks727476T est Accuracy (%)
Accuracy envolution for T ask 6
(b) CIFAR-100 Split
0 1 2 3 4
Number of tasks767880T est Accuracy (%)
Accuracy envolution for T ask 1
TRGP
GPM
Ours
1 2 3 4
Number of tasks98.899.099.299.4T est Accuracy (%)
Accuracy envolution for T ask 2
2.0 2.5 3.0 3.5 4.0
Number of tasks889092T est Accuracy (%)
Accuracy envolution for T ask 3
(c) 5-Dataset
5 10 15
Number of tasks586062T est Accuracy (%)
Accuracy envolution for T ask 2
5 10 15
Number of tasks606264T est Accuracy (%)
Accuracy envolution for T ask 4
5 10 15
Number of tasks74767880T est Accuracy (%)
Accuracy envolution for T ask 6
(d) MiniImageNet
Figure 3: Accuracy evolution for different tasks on PMNIST, CIFAR-100 Split, 5-Datasets and
MiniImageNet, respectively.
average pooling layer before the classiÔ¨Åer. In addition, we use convolution with stride 2 in the Ô¨Årst
layer for miniImageNet, since it has larger input resolution (i.e., 84x84) than 5-dataset (i.e., 32x32).
D.2 List of hyperparameters
In what follows, we list the hyperparameters for all the methods considered in this work. For TRGP,
we use the hyperparameters provided in [ 17]. For other baseline methods, we use the hyperparameters
for GPM as provided in [ 25], and provide the hyperparameters for the rest following [ 25] for being
consistent with the corresponding reported results herein.
As shown in the Table 5, we use ‚Äòlr‚Äô to represent the initial learning rate, and ‚Äòcifar‚Äô, ‚Äòmini‚Äô, ‚Äò5d‚Äô and
‚Äòpm‚Äô to represent ‚ÄòSplit CIFAR-100‚Äô, ‚ÄòSplit MiniImageNet‚Äô, ‚Äò5-Dataset‚Äô and ‚ÄòPermuted MNIST‚Äô,
respectively.
Optimizer/learning rate: For all experiments, we adapt the SGD optimizer by modifying the
gradient based on the optimization problem (3). This is consistent with the orthogonal-projection
based methods (e.g., [ 6,25,17]) where the gradient direction used in the plain SGD optimizer is
modiÔ¨Åed to minimize the interference to the old tasks. The learning rates for different datasets are
shown in Table 5. Here for PMNIST, we use a Ô¨Åxed learning rate, while for other datasets the learning
rate decays during the training process.
19

--- PAGE 20 ---
Table 5: List of hyperparameters for CUBER and the related baseline methods.
Methods Hyperparameters
OWM lr: 0.01 (cifar), 0.3 (pm)
EWClr: 0.05 (cifar), 0.03 (mini, 5d, pm)
regularization coefÔ¨Åcient ( ): 5000 (cifar, mini, 5d), 1000 (pm)
HATlr: 0.05 (cifar), 0.03 (mini), 0.1 (5d)
smax: 400 (cifar, mini, 5d)
c: 0.75 (cifar, mini, 5d)
A-GEMlr: 0.05 (cifar), 0.1 (mini, 5d, pm)
memory size (number of samples): 2000 (cifar), 500 (mini), 3000 (5d), 1000 (pm)
ER_Reslr: 0.05 (cifar), 0.1 (mini, 5d, pm)
memory size (number of samples): 2000 (cifar), 500 (mini), 3000 (5d), 1000 (pm)
GPMlr: 0.01 (cifar, pm), 0.1 (mini, 5d)
number of samples for base extraction ( n): 125 (cifar), 100 (mini, 5d), 300 (pm)
th: 0.97, increase by 0.003 with t(cifar); 0.985, increase by 0.003 with t(mini)
th: 0.965 (5d); 0.95 for the Ô¨Årst layer, otherwise 0.99 (pm)
TRGPlr: 0.01 (cifar, pm), 0.1 (mini, 5d)
number of samples for base extraction ( n): 125 (cifar), 100 (mini, 5d), 300 (pm)
th: same with GPM
l: 0.5 (cifar, mini, 5d, pm)
CUBERlr: 0.01 (cifar, pm), 0.1 (mini, 5d)
number of samples for base extraction ( n): 125 (cifar), 100 (mini, 5d), 300 (pm)
th: same with GPM
1: 0.5 (cifar, mini, 5d, pm); 2: 0 (cifar, mini, 5d, pm); : 1 (cifar, mini, 5d, pm)
Early stopping: For all experiments, the minimum learning rate is set to 1e-5, the learning rate decay
factor is set to 2, and the number of holds before decaying the learning rate is 6. After each model
update, we evaluate the validation loss for the current task using its validation dataset, and count
the times whenever the validation loss increases. When the counter is greater than 6, we decay the
learning rate by 2 and reset the counter to 0. The training process will be stopped when the learning
rate decays to the minimum learning rate.
D.3 Memory and time cost
Storing gradient: After learning each task, most elements in the gradient matrix for each layer can
be close to zero. When evaluating the conditions for regime 3, we Ô¨Çatten the gradient matrix into a
vector for each layer, e.g., a gradient matrix in Rmnto a gradient vector in Rmn. The inner product
between gradient matrices hence becomes the inner product between gradient vectors, where the
near-zero elements have little impact on the value of the product. Therefore, to save the computation
cost and memory, we prune the layer-wise average gradient vector after learning each task with a
certain sparsity ratio, and only store a gradient vector with non-zero elements and the corresponding
indices in the original gradient vector.
Memory and training time: As shown in Figure 4 and Table 6, we compare the memory utilization
and training time between CUBER and the related baseline methods in terms of the normalized value
with respect to the value of GPM, following the same strategy as in [ 17]. The values of OWM, EWC,
HAT, A-GEM, ER _Res are from the reported results in GPM [ 25]. The value of TRGP is from the
reported results in [ 17]. It can be seen that CUBER indeed has comparable complexity with the
baseline methods. While SVD is used to extract bases in CUBER, the training time of CUBER is still
less than some baseline methods due to its relatively simple and fast model update. And because we
only store part of the gradient for each old task, the required memory utilization does not increase a
lot.
20

--- PAGE 21 ---
012345670.00.20.40.60.81.0PMNIST
01234560.00.20.40.60.81.0CIFAR-100
0123450.00.20.40.60.81.01.2miniImageNet
0123450.00.20.40.60.81.05-dataset
012345670.00.20.40.60.81.0PMNIST
OWM
GEM
A-GEM
ER_Res
GPM_Max
GPM
TRGP
CUBERFigure 4: Comparison of memory utilization on PMNIST, CIFAR-100 Split, MiniImageNet and
5-Datasets, respectively.
Table 6: Training time comparison on CIFAR-100 Split, 5-Datasets and MiniImageNet. Here the
training time is normalized with respect to the value of GPM. Please refer [ 25] for more speciÔ¨Åc time.
DatasetMethods
OWM EWC HAT A-GEM ER_Res GPM TRGP Ours (CUBER)
CIFAR-100 2.41 1.76 1.62 3.48 1.49 1 1.65 1.86
5-Datasets - 1.52 1.47 2.41 1.40 1 1.21 1.55
MiniImageNet - 1.22 0.91 1.79 0.82 1 1.34 1.61
D.4 Baseline implementations
To ensure fair comparisons, we implement CUBER based on the released code of GPM [ 25] and
TRGP [ 17], and follow the exactly same experimental setups. We reproduced the reported results
of GPM and TRGP using the provided hyperparameters in the papers. Therefore, for other baseline
methods considered in this work, we directly follow these two papers and present the reported results
therein in Section 5. Besides, we also implemented all the baseline methods based on their released
ofÔ¨Åcial code, and show the reproduced results in Table 7.
Table 7: The ACC and BWT with the standard deviation values over 5 different runs on different
datasets. Here for Split CIFAR-100, Split MiniImageNet and 5-Dataset we use a multi-head network,
while we consider domain-incremental setup for Permuted MNIST. Moreover, 2= 0:0.
MethodMulti-head Domain-incremental
Split CIFAR-100 Split MiniImageNet 5-Dataset Permuted MNIST
ACC(%) BWT(%) ACC(%) BWT(%) ACC(%) BWT(%) ACC(%) BWT(%)
Multitask 79:580:54 - 69:460:62 - 91:540:28 - 96:700:02 -
OWM 50:440:72 301 - - - - 89:630:21 10
EWC 68:300:65 21 50:782:98 124 87:670:37 31 89:050:57 42
HAT 73:210:76 00 60:230:57 41 91:820:34 10 - -
A-GEM 64:161:41 143 56:880:87 132 82:480:56 131 83:050:12 142
ER_Res 72:240:57 72 58:040:73 71 88:540:36 41 88:720:56 101
GPM 72:480:40 0:90 60:410:61 0:70:4 91:220:20 10 93:910:16 30
TRGP 74:460:32 0:90:01 61:780:60 0:50:6 93:5693:5693:560:10 0:040:01 96:340:11 0:80:1
CUBER (ours) 75:5475:5475:540:22 0:130:130:130:08 62:6762:6762:670:74 0:230:230:230:15 93:4893:4893:480:10 0:000:000:000:02 97:2597:2597:250:00 0:02 0:02 0:020:00
21
