# 2204.00408.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2204.00408.pdf
# Kích thước tệp: 2963951 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cắt tỉa có cấu trúc học được các mô hình chính xác và nhỏ gọn
Mengzhou Xia Zexuan Zhong Danqi Chen
Đại học Princeton
{mengzhou,zzhong,danqic}@cs.princeton.edu
Tóm tắt
Kích thước ngày càng tăng của các mô hình ngôn ngữ thần kinh đã dẫn đến sự quan tâm gia tăng trong nén mô hình. Hai phương pháp chủ đạo là cắt tỉa, dần dần loại bỏ trọng số từ một mô hình đã được huấn luyện trước, và chưng cất, huấn luyện một mô hình nhỏ gọn nhỏ hơn để phù hợp với một mô hình lớn hơn. Các phương pháp cắt tỉa có thể giảm đáng kể kích thước mô hình nhưng khó đạt được tốc độ tăng lớn như chưng cất. Tuy nhiên, các phương pháp chưng cất đòi hỏi lượng lớn dữ liệu không nhãn và tốn kém để huấn luyện. Trong công trình này, chúng tôi đề xuất một phương pháp cắt tỉa có cấu trúc đặc thù cho tác vụ CoFi¹ (Cắt tỉa thô và tinh), cung cấp các mạng con có thể song song hóa cao và phù hợp với các phương pháp chưng cất về cả độ chính xác và độ trễ, mà không cần dựa vào bất kỳ dữ liệu không nhãn nào. Hiểu biết chính của chúng tôi là cắt tỉa đồng thời các đơn vị thô (ví dụ: các lớp) và tinh (ví dụ: các đầu và đơn vị ẩn), điều này kiểm soát quyết định cắt tỉa của từng tham số với các mặt nạ có độ chi tiết khác nhau. Chúng tôi cũng thiết kế một chiến lược chưng cất theo lớp để chuyển giao kiến thức từ các mô hình chưa cắt tỉa sang đã cắt tỉa trong quá trình tối ưu hóa. Các thí nghiệm của chúng tôi trên bộ dữ liệu GLUE và SQuAD cho thấy CoFi tạo ra các mô hình có tốc độ tăng hơn 10× với sự sụt giảm độ chính xác nhỏ, cho thấy hiệu quả và hiệu suất của nó so với các phương pháp cắt tỉa và chưng cất trước đây.²

1 Giới thiệu
Các mô hình ngôn ngữ đã được huấn luyện trước (Devlin et al., 2019; Liu et al., 2019a; Raffel et al., 2020, trong số các nghiên cứu khác) đã trở thành trụ cột chính trong xử lý ngôn ngữ tự nhiên. Những mô hình này có chi phí cao về mặt lưu trữ, bộ nhớ và thời gian tính toán và điều này đã thúc đẩy một khối lượng lớn công việc về nén mô hình để làm cho chúng nhỏ hơn và nhanh hơn để sử dụng trong các ứng dụng thực tế (Ganesh et al., 2021).

¹CoFi được phát âm là .
²Mã và mô hình của chúng tôi có sẵn công khai tại https://github.com/princeton-nlp/CoFiPruning .

[THIS IS TABLE: Bảng so sánh các phương pháp chưng cất và cắt tỉa tiên tiến, hiển thị tỷ lệ tăng tốc, số tham số và độ chính xác MNLI]

Hai phương pháp chủ đạo để nén mô hình là cắt tỉa và chưng cất (Bảng 1). Các phương pháp cắt tỉa tìm kiếm một mạng con chính xác trong một mô hình lớn đã được huấn luyện trước. Công việc gần đây đã điều tra cách cắt tỉa có cấu trúc các mạng Transformer (Vaswani et al., 2017), từ việc loại bỏ toàn bộ các lớp (Fan et al., 2020; Sajjad et al., 2020), đến cắt tỉa các đầu (Michel et al., 2019; Voita et al., 2019), các chiều trung gian (McCarley et al., 2019; Wang et al., 2020b) và các khối trong ma trận trọng số (Lagunas et al., 2021). Xu hướng của cắt tỉa có cấu trúc hướng tới việc loại bỏ các đơn vị tinh để cho phép các cấu trúc cuối cùng linh hoạt. Tuy nhiên, cho đến nay, các mô hình đã cắt tỉa hiếm khi đạt được tốc độ tăng lớn (cải thiện 2-3× nhiều nhất).

³Theo công việc trước đây, chúng tôi loại trừ ma trận nhúng trong việc tính toán số lượng tham số. Chúng tôi loại trừ tăng cường dữ liệu đặc thù cho tác vụ để so sánh công bằng. Kết quả chi tiết hơn với tăng cường dữ liệu có thể tìm thấy trong Bảng 3.

--- TRANG 2 ---
Ngược lại, các phương pháp chưng cất thường đầu tiên chỉ định một kiến trúc mô hình cố định và thực hiện bước chưng cất tổng quát trên một kho ngữ liệu không nhãn, trước khi tiếp tục tinh chỉnh hoặc chưng cất trên dữ liệu đặc thù cho tác vụ (Sanh et al., 2019; Turc et al., 2019; Sun et al., 2019; Jiao et al., 2020). Các kiến trúc học sinh được thiết kế tốt đạt được sự cân bằng tốc độ-hiệu suất hấp dẫn, tuy nhiên chưng cất đến các mạng học sinh được khởi tạo ngẫu nhiên này trên dữ liệu lớn không nhãn là cực kỳ chậm.⁴ Ví dụ, TinyBERT (Jiao et al., 2020) đầu tiên được huấn luyện trên 2.500M token trong 3 epoch, điều này đòi hỏi huấn luyện 3,5 ngày trên 4 GPU (Hình 1).⁵

Trong công trình này, chúng tôi đề xuất một phương pháp cắt tỉa có cấu trúc đặc thù cho tác vụ được gọi là CoFi (Cắt tỉa thô và tinh) và cho thấy rằng cắt tỉa có cấu trúc có thể đạt được các mạng con rất nhỏ gọn và có được tốc độ tăng lớn và độ chính xác cạnh tranh như các phương pháp chưng cất, trong khi đòi hỏi ít tính toán hơn nhiều. Hiểu biết chính của chúng tôi là cắt tỉa đồng thời các đơn vị thô (ví dụ: các lớp tự chú ý hoặc feed-forward) và các đơn vị tinh (ví dụ: các đầu, chiều ẩn). Khác với các công trình hiện có, phương pháp của chúng tôi kiểm soát quyết định cắt tỉa của từng tham số đơn lẻ bằng nhiều mặt nạ có độ chi tiết khác nhau. Đây là chìa khóa cho nén lớn, vì nó cho phép sự linh hoạt lớn nhất của các cấu trúc đã cắt tỉa và làm dễ dàng quá trình tối ưu hóa so với việc chỉ cắt tỉa các đơn vị nhỏ.

Người ta biết rằng cắt tỉa với một mục tiêu chưng cất có thể cải thiện đáng kể hiệu suất (Sanh et al., 2020; Lagunas et al., 2021). Không giống như một kiến trúc học sinh cố định, các cấu trúc đã cắt tỉa không được biết trước khi huấn luyện và việc chưng cất giữa các lớp trung gian của các mô hình chưa cắt tỉa và đã cắt tỉa là thách thức (Jiao et al., 2020). Do đó, chúng tôi đề xuất một phương pháp chưng cất theo lớp, động học được ánh xạ lớp giữa hai cấu trúc. Chúng tôi cho thấy rằng chiến lược này có thể dẫn đến lợi ích hiệu suất tốt hơn ngoài chưng cất lớp dự đoán đơn giản.

Các thí nghiệm của chúng tôi cho thấy CoFi cung cấp các mô hình chính xác hơn ở tất cả các mức độ tăng tốc và kích thước mô hình trên các bộ dữ liệu GLUE (Wang et al., 2019) và SQuAD v1.1 (Rajpurkar et al., 2016), so với các baseline cắt tỉa và chưng cất mạnh. Cụ thể, nó đạt được tốc độ tăng hơn 10× và độ thưa 95% trên tất cả các bộ dữ liệu trong khi bảo tồn hơn 90% độ chính xác. Kết quả của chúng tôi cho thấy rằng cắt tỉa có cấu trúc đặc thù cho tác vụ là một giải pháp hấp dẫn trong thực tế, tạo ra các mô hình nhỏ hơn và nhanh hơn mà không cần dữ liệu không nhãn bổ sung cho chưng cất tổng quát.

⁴Có ngoại lệ như DistillBERT (Sanh et al., 2020), khởi tạo học sinh từ giáo viên bằng cách lấy một lớp từ hai lớp, tuy nhiên không rõ làm thế nào để tổng quát hóa sơ đồ khởi tạo này cho các cấu trúc nhỏ gọn khác.
⁵Xem chi tiết đo thời gian huấn luyện trong Phụ lục J.

2 Nền tảng

2.1 Transformers
Một mạng Transformer (Vaswani et al., 2017) được cấu thành từ L khối và mỗi khối bao gồm một lớp tự chú ý đa đầu (MHA) và một lớp feed-forward (FFN). Một lớp MHA với Nh đầu nhận đầu vào X và xuất ra:

MHA(X) = ΣᵢᴺʰAtt(W⁽ⁱ⁾_Q; W⁽ⁱ⁾_K; W⁽ⁱ⁾_V; W⁽ⁱ⁾_O; X);

trong đó W⁽ⁱ⁾_Q; W⁽ⁱ⁾_K; W⁽ⁱ⁾_V; W⁽ⁱ⁾_O ∈ Rᵈˣᵈʰ biểu thị các ma trận truy vấn, khóa, giá trị và đầu ra tương ứng và Att() là một hàm chú ý. Ở đây d biểu thị kích thước ẩn (ví dụ: 768) và dh = d/Nh biểu thị chiều đầu ra của mỗi đầu (ví dụ: 64).

Tiếp theo là một lớp feed-forward, bao gồm một lớp up-projection và một lớp down-projection, được tham số hóa bởi WU ∈ Rᵈˣᵈᶠ và WD ∈ Rᵈᶠˣᵈ:

FFN(X) = gelu(XWU)WD:

Thông thường, df = 4d. Cũng có một kết nối dư và một hoạt động chuẩn hóa lớp sau mỗi lớp MHA và FFN.

MHA, FFN chiếm 1/3 và 2/3 tham số mô hình trong Transformers (loại trừ nhúng). Theo Ganesh et al. (2021), cả MHA và FFN đều mất thời gian tương tự trên GPU trong khi FFN trở thành nút thắt cổ chai trên CPU.

2.2 Chưng cất
Chưng cất kiến thức (Hinton et al., 2015) là một phương pháp nén mô hình chuyển giao kiến thức từ một mô hình giáo viên lớn hơn sang một mô hình học sinh nhỏ hơn. Chưng cất tổng quát (Sanh et al., 2019; Sun et al., 2020; Wang et al., 2020a) và chưng cất đặc thù cho tác vụ (Sun et al., 2019) khai thác dữ liệu không nhãn và dữ liệu đặc thù cho tác vụ tương ứng để chuyển giao kiến thức. Sự kết hợp của cả hai dẫn đến tăng hiệu suất (Jiao et al., 2020). Chưng cất tổng quát hoặc huấn luyện trước mạng học sinh trên kho ngữ liệu không nhãn là thiết yếu để giữ lại hiệu suất trong khi tốn kém về mặt tính toán (Turc et al., 2019; Jiao et al., 2020).

--- TRANG 3 ---
[Hình 1: So sánh (a) TinyBERT (Jiao et al., 2020) và (b) phương pháp cắt tỉa CoFi của chúng tôi]

Các mục tiêu chưng cất khác nhau cũng đã được khám phá. Bên cạnh chưng cất tiêu chuẩn từ lớp dự đoán (Hinton et al., 2015), việc chuyển giao kiến thức theo từng lớp từ các biểu diễn (Jiao et al., 2020; Sun et al., 2020) và ma trận chú ý đa đầu (Wang et al., 2020a; Jiao et al., 2020; Sun et al., 2020) dẫn đến những cải thiện đáng kể. Hầu hết các phương pháp chưng cất đều giả định một cấu trúc học sinh cố định trước khi huấn luyện. Hou et al. (2020) cố gắng chưng cất đến một cấu trúc động với chiều rộng và chiều cao được chỉ định. Yin et al. (2021) áp dụng giải pháp Tìm kiếm Kiến trúc Thần kinh một lần để tìm kiếm kiến trúc của các mạng học sinh.

2.3 Cắt tỉa
Cắt tỉa dần dần loại bỏ các tham số dư thừa từ một mô hình giáo viên, chủ yếu tạo ra các mô hình đặc thù cho tác vụ. Các công trình trước đây tập trung vào việc cắt tỉa các thành phần khác nhau trong các mô hình Transformer, từ các đơn vị thô đến tinh.

Cắt tỉa lớp Fan et al. (2020) và Sajjad et al. (2020) khám phá các chiến lược để bỏ toàn bộ các khối Transformer (một cặp lớp MHA và FFN) từ một mô hình đã được huấn luyện trước. Bằng chứng thực nghiệm cho thấy rằng 50% các lớp có thể bị bỏ mà không có sự sụt giảm độ chính xác lớn, dẫn đến tốc độ tăng 2×.

Cắt tỉa đầu Voita et al. (2019); Michel et al. (2019) cho thấy rằng chỉ một tập con nhỏ các đầu là quan trọng và phần lớn có thể được cắt tỉa. Chúng tôi theo các công trình này để che đầu bằng cách giới thiệu các biến z⁽ⁱ⁾_head ∈ {0,1} vào chú ý đa đầu:

MHA(X) = Σᵢᴺʰ z⁽ⁱ⁾_head Att(W⁽ⁱ⁾_Q; W⁽ⁱ⁾_K; W⁽ⁱ⁾_V; W⁽ⁱ⁾_O; X):

Việc chỉ loại bỏ các đầu không dẫn đến cải thiện độ trễ lớn—Li et al. (2021) chứng minh tốc độ tăng 1,4× với chỉ một đầu còn lại trên mỗi lớp.

Cắt tỉa FFN Phần chính khác—các lớp feed-forward (FFN)—cũng được biết là bị tham số hóa quá mức. Các chiến lược để cắt tỉa một lớp FFN để tăng tốc suy luận bao gồm cắt tỉa toàn bộ lớp FFN (Prasanna et al., 2020; Chen et al., 2020b) và ở mức tinh hơn, cắt tỉa các chiều trung gian (McCarley et al., 2019; Hou et al., 2020) bằng cách giới thiệu zint ∈ {0,1}ᵈᶠ:

FFN(X) = gelu(XWU)diag(zint)WD:

Cắt tỉa khối và không có cấu trúc Gần đây hơn, việc cắt tỉa trên một đơn vị nhỏ hơn, các khối, từ MHA và FFN đã được khám phá (Lagunas et al., 2021). Tuy nhiên, khó tối ưu hóa các mô hình với các khối đã cắt tỉa cho đến nay: Yao et al. (2021) cố gắng tối ưu hóa các mô hình đã cắt tỉa khối với kernel MatMul thưa khối được cung cấp bởi Triton (Tillet et al., 2019), nhưng các kết quả báo cáo không cạnh tranh. Tương tự, cắt tỉa không có cấu trúc nhằm loại bỏ các trọng số riêng lẻ và đã được nghiên cứu rộng rãi trong tài liệu (Chen et al., 2020a; Huang et al., 2021). Mặc dù độ thưa đạt đến 97% (Sanh et al., 2020), rất khó có được tốc độ tăng suy luận trên phần cứng hiện tại.

Kết hợp với chưng cất Cắt tỉa thường được kết hợp với một mục tiêu chưng cất lớp dự đoán (Sanh et al., 2020; Lagunas et al., 2021). Tuy nhiên không rõ làm thế nào để áp dụng các chiến lược chưng cất theo lớp vì kiến trúc của mô hình học sinh đã cắt tỉa phát triển trong quá trình huấn luyện.

⁶CoFi yêu cầu thời gian huấn luyện hơi lâu hơn so với chưng cất đặc thù cho tác vụ của TinyBERT, vì CoFi tìm kiếm cấu trúc mô hình và học các tham số đồng thời.

--- TRANG 4 ---
3 Phương pháp
Chúng tôi đề xuất một phương pháp cắt tỉa có cấu trúc CoFi, cắt tỉa đồng thời các đơn vị Thô và Tinh (§3.1) với một mục tiêu chưng cất theo lớp chuyển giao kiến thức từ các mô hình chưa cắt tỉa sang đã cắt tỉa (§3.2). Sự kết hợp của cả hai dẫn đến các mô hình nén cao với tốc độ tăng suy luận lớn.

3.1 Cắt tỉa thô và tinh
Xu hướng gần đây trong cắt tỉa có cấu trúc hướng tới việc cắt tỉa các đơn vị nhỏ hơn để có tính linh hoạt mô hình. Cắt tỉa các đơn vị tinh tự nhiên đòi hỏi cắt tỉa các đơn vị thô—ví dụ, cắt tỉa Nh (ví dụ: 12) đầu tương đương với cắt tỉa một toàn bộ lớp MHA. Tuy nhiên, chúng tôi quan sát thấy điều này hiếm khi xảy ra trong thực tế và gây khó khăn cho việc tối ưu hóa đặc biệt ở chế độ thưa cao.

Để khắc phục vấn đề này, chúng tôi trình bày một giải pháp đơn giản: chúng tôi cho phép cắt tỉa các lớp MHA và FFN một cách rõ ràng cùng với các đơn vị tinh (như đã trình bày trong §2.3) bằng cách giới thiệu hai mặt nạ bổ sung zMHA và zFFN cho mỗi lớp. Bây giờ lớp tự chú ý đa đầu và lớp feed-forward trở thành:

MHA(X) = zMHA Σᵢᴺʰ (z⁽ⁱ⁾_head Att(W⁽ⁱ⁾_Q; W⁽ⁱ⁾_K; W⁽ⁱ⁾_V; W⁽ⁱ⁾_O; X));
FFN(X) = zFFN gelu(XWU)diag(zint)WD:

Với các mặt nạ lớp này, chúng tôi cắt tỉa rõ ràng toàn bộ một lớp, thay vì cắt tỉa tất cả các đầu trong một lớp MHA (hoặc tất cả các chiều trung gian trong một lớp FFN). Khác với các chiến lược bỏ lớp trong Fan et al. (2020); Sajjad et al. (2020), chúng tôi bỏ các lớp MHA và FFN riêng biệt, thay vì cắt tỉa chúng như một tổng thể.

Hơn nữa, chúng tôi cũng xem xét việc cắt tỉa các chiều đầu ra của MHA(X) và FFN(X), được gọi là 'chiều ẩn' trong bài báo này, để cho phép tính linh hoạt hơn trong cấu trúc mô hình cuối cùng. Chúng tôi định nghĩa một tập hợp các mặt nạ zhidn ∈ {0,1}ᵈ, được chia sẻ giữa các lớp vì mỗi chiều trong một biểu diễn ẩn được kết nối với cùng chiều trong lớp tiếp theo thông qua một kết nối dư. Các biến mặt nạ này được áp dụng cho tất cả các ma trận trọng số trong mô hình, ví dụ: diag(zhidn)WQ. Thực nghiệm, chúng tôi thấy rằng chỉ một số lượng nhỏ chiều được cắt tỉa (ví dụ: 768→760), nhưng nó vẫn giúp cải thiện hiệu suất đáng kể (§4.3).

CoFi khác với các phương pháp cắt tỉa trước đây ở chỗ nhiều biến mặt nạ cùng kiểm soát quyết định cắt tỉa của một tham số đơn lẻ. Ví dụ, một trọng số trong một lớp FFN được cắt tỉa khi toàn bộ lớp FFN, hoặc chiều trung gian tương ứng của nó, hoặc chiều ẩn được cắt tỉa. Để so sánh, một công trình gần đây Block Pruning (Lagunas et al., 2021) áp dụng một phương pháp lai áp dụng một chiến lược cắt tỉa duy nhất trên MHA và FFN riêng biệt.

Để học các biến mặt nạ này, chúng tôi sử dụng chính quy hóa l0 được mô hình hóa với phân phối concrete cứng theo Louizos et al. (2018). Chúng tôi cũng theo Wang et al. (2020b) để thay thế mục tiêu l0 vani với một nhân tử Lagrangian để kiểm soát tốt hơn độ thưa mong muốn của các mô hình đã cắt tỉa.⁷ Chúng tôi điều chỉnh hàm độ thưa tương ứng để phù hợp với việc cắt tỉa các mặt nạ có độ chi tiết khác nhau:

ŝ = 1/M [4dh ΣᴸᵢΣᴺʰⱼΣᵈₖ z⁽ⁱ⁾_MHA z⁽ⁱ'ʲ⁾_head z⁽ᵏ⁾_hidden + 1/M 2ΣᴸᵢΣᵈᶠⱼΣᵈₖ z⁽ⁱ⁾_FFN z⁽ⁱ'ʲ⁾_int z⁽ᵏ⁾_hidden];

trong đó ŝ là độ thưa dự kiến và M biểu thị kích thước mô hình đầy đủ. Tất cả các biến che được học như các số thực trong [0,1] trong quá trình huấn luyện và chúng tôi ánh xạ các biến che dưới một ngưỡng thành 0 trong quá trình suy luận và có được một cấu trúc đã cắt tỉa cuối cùng trong đó ngưỡng được xác định bởi độ thưa dự kiến của mỗi ma trận trọng số (xem Phụ lục B để biết thêm chi tiết).

3.2 Chưng cất đến các mô hình đã cắt tỉa
Công việc trước đây đã cho thấy rằng việc kết hợp chưng cất với cắt tỉa cải thiện hiệu suất, trong đó mục tiêu chưng cất chỉ bao gồm một mất entropy chéo giữa phân phối xác suất đầu ra của học sinh đã cắt tỉa ps và của giáo viên pt (Sanh et al., 2020; Lagunas et al., 2021):

Lpred = DKL(ps||pt):

Ngoài chưng cất lớp dự đoán, các công trình gần đây cho thấy lợi ích lớn trong việc chưng cất các lớp trung gian (Sun et al., 2019; Jiao et al., 2020). Trong bối cảnh các phương pháp chưng cất, kiến trúc của mô hình học sinh được chỉ định trước và việc định nghĩa một ánh xạ lớp giữa mô hình học sinh và giáo viên là đơn giản. Ví dụ, mô hình TinyBERT4 4 lớp chưng cất từ lớp 3, 6, 9 và 12 của một mô hình giáo viên 12 lớp.

Tuy nhiên, việc chưng cất các lớp trung gian trong quá trình cắt tỉa là thách thức vì cấu trúc mô hình thay đổi trong suốt quá trình huấn luyện.

Chúng tôi đề xuất một phương pháp chưng cất theo lớp cho việc cắt tỉa để tận dụng tốt nhất các tín hiệu từ mô hình giáo viên. Thay vì định nghĩa trước một ánh xạ lớp cố định, chúng tôi động tìm kiếm một ánh xạ lớp giữa mô hình giáo viên đầy đủ và mô hình học sinh đã cắt tỉa. Cụ thể, gọi T biểu thị một tập hợp các lớp giáo viên mà chúng tôi sử dụng để chưng cất kiến thức đến mô hình học sinh. Chúng tôi định nghĩa một hàm ánh xạ lớp m(), tức là m(i) biểu thị lớp học sinh chưng cất từ lớp giáo viên i. Mất mát chưng cất lớp ẩn được định nghĩa là

Llayer = Σᵢ∈T MSE(Wlayer H⁽ᵐ⁽ⁱ⁾⁾s, H⁽ⁱ⁾t);

trong đó Wlayer ∈ Rᵈˣᵈ là một ma trận biến đổi tuyến tính, được khởi tạo như một ma trận đơn vị. H⁽ᵐ⁽ⁱ⁾⁾s, H⁽ⁱ⁾t là các biểu diễn ẩn từ lớp FFN học sinh m(i) và lớp FFN giáo viên i. Hàm ánh xạ lớp m() được xác định động trong quá trình huấn luyện để phù hợp một lớp giáo viên với lớp gần nhất của nó trong mô hình học sinh:

m(i) = arg minⱼ:z⁽ʲ⁾FFN>0 MSE(Wlayer H⁽ʲ⁾s, H⁽ⁱ⁾t):

Việc tính toán khoảng cách giữa hai tập hợp lớp có thể song song hóa cao và giới thiệu chi phí huấn luyện tối thiểu. Để giải quyết vấn đề không khớp lớp, chủ yếu xảy ra với các bộ dữ liệu kích thước nhỏ, ví dụ: RTE, MRPC, chúng tôi thêm một ràng buộc chỉ cho phép khớp một lớp giáo viên với một lớp học sinh thấp hơn so với lớp học sinh đã khớp trước đó. Khi cắt tỉa với các bộ dữ liệu kích thước lớn hơn, việc không khớp lớp hiếm khi xảy ra, cho thấy sự ưu việt của khớp động—các lớp giữa các mô hình học sinh và giáo viên khớp theo cách có lợi nhất cho quá trình cắt tỉa.

Cuối cùng, chúng tôi kết hợp chưng cất lớp với chưng cất lớp dự đoán:

Ldistil = αLpred + (1-α)Llayer;

trong đó α kiểm soát đóng góp của mỗi mất mát.

⁷Chúng tôi cũng đã thử một ước lượng straight-through như đề xuất trong Sanh et al. (2020) và thấy hiệu suất tương đương. Chúng tôi chọn chính quy hóa l0 vì dễ dàng kiểm soát độ thưa một cách chính xác hơn.

--- TRANG 5 ---
4 Thí nghiệm

4.1 Thiết lập
Bộ dữ liệu Chúng tôi đánh giá phương pháp của mình trên tám tác vụ GLUE (Wang et al., 2019) và SQuAD v1.1 (Rajpurkar et al., 2016). Các tác vụ GLUE bao gồm SST-2 (Socher et al., 2013), MNLI (Williams et al., 2018), QQP, QNLI, MRPC (Dolan and Brockett, 2005), CoLA (Warstadt et al., 2019), STS-B (Cer et al., 2017) và RTE (xem Phụ lục D để biết kích thước bộ dữ liệu và số liệu).

Thiết lập huấn luyện Trong các thí nghiệm của chúng tôi, độ thưa được tính như số lượng tham số đã cắt tỉa chia cho kích thước mô hình đầy đủ (loại trừ nhúng). Theo Wang et al. (2020b); Lagunas et al. (2021), chúng tôi đầu tiên tinh chỉnh mô hình với mục tiêu chưng cất, sau đó chúng tôi tiếp tục huấn luyện mô hình với mục tiêu cắt tỉa với một bộ lập lịch để tăng tuyến tính độ thưa đến giá trị mục tiêu. Chúng tôi tinh chỉnh mô hình đã cắt tỉa cho đến khi hội tụ (xem Phụ lục A để biết thêm chi tiết huấn luyện).

Chúng tôi huấn luyện các mô hình với độ thưa mục tiêu {60%, 70%, 75%, 80%, 85%, 90%, 95%} trên mỗi bộ dữ liệu. Đối với tất cả các thí nghiệm, chúng tôi bắt đầu từ mô hình BERT base⁸ và đóng băng trọng số nhúng theo Sanh et al. (2020). Chúng tôi báo cáo kết quả trên các tập phát triển của tất cả các bộ dữ liệu.

Baseline Chúng tôi so sánh CoFi với một số baseline: DistillBERT₆ (Sanh et al., 2019), TinyBERT₆ và TinyBERT₄ (Jiao et al., 2020), DynaBERT (Hou et al., 2020), và Block Pruning (Lagunas et al., 2021) (xem Phụ lục C để biết chi tiết). Chúng tôi cũng so sánh với các phương pháp cắt tỉa khác như FLOP (Wang et al., 2020b), LayerDrop (Fan et al., 2020), Movement Pruning (Sanh et al., 2020) và các phương pháp chưng cất như MobileBERT (Sun et al., 2020) và AutoTinyBERT (Yin et al., 2021) trong Phụ lục F.⁹

Đối với TinyBERT và DynaBERT, các mô hình được phát hành được huấn luyện với dữ liệu tăng cường đặc thù cho tác vụ. Để so sánh công bằng, chúng tôi huấn luyện hai mô hình này với mã được phát hành mà không có tăng cường dữ liệu.¹⁰ Đối với Block Pruning, chúng tôi huấn luyện các mô hình với các checkpoint được phát hành trên các tác vụ GLUE và sử dụng kết quả SQuAD từ bài báo.

Đánh giá tốc độ tăng Tỷ lệ tốc độ tăng là một phép đo chính mà chúng tôi sử dụng trong suốt bài báo vì tỷ lệ nén không nhất thiết phản ánh sự cải thiện thực tế trong độ trễ suy luận.¹¹ Chúng tôi sử dụng BERT base chưa cắt tỉa làm baseline và đánh giá tất cả các mô hình với cùng một thiết lập phần cứng trên một GPU NVIDIA V100 duy nhất để đo tốc độ tăng suy luận. Kích thước đầu vào là 128 cho các tác vụ GLUE và 384 cho SQuAD, và chúng tôi sử dụng kích thước batch là 128. Lưu ý rằng kết quả có thể khác với các bài báo gốc vì môi trường cho mỗi nền tảng là khác nhau.

4.2 Kết quả chính
Hiệu suất tổng thể Trong Hình 2, chúng tôi so sánh độ chính xác của các mô hình CoFi với các phương pháp khác về cả tốc độ tăng suy luận và kích thước mô hình. CoFi cung cấp các mô hình chính xác hơn so với các baseline chưng cất và cắt tỉa ở mọi mức độ tăng tốc và kích thước mô hình. Block Pruning (Lagunas et al., 2021), một công trình gần đây cho thấy hiệu suất mạnh so với TinyBERT₆, không thể đạt được tốc độ tăng tương đương với TinyBERT₄. Thay vào đó, CoFi có tùy chọn cắt tỉa cả lớp và đầu & đơn vị trung gian và có thể đạt được một mô hình với hiệu suất tương đương hoặc cao hơn so với TinyBERT₄ và tất cả các mô hình khác. Ngoài ra, DynaBERT hoạt động kém hơn nhiều về tốc độ vì nó bị hạn chế chỉ loại bỏ nhiều nhất một nửa các lớp MHA và FFN.

So sánh với TinyBERT₄ Trong Bảng 2, chúng tôi cho thấy CoFi tạo ra các mô hình với tốc độ tăng suy luận hơn 10× và đạt được hiệu suất tương đương hoặc thậm chí tốt hơn TinyBERT₄. Chưng cất tổng quát (GD), chưng cất thông tin từ một kho ngữ liệu lớn, là thiết yếu để huấn luyện các mô hình chưng cất, đặc biệt đối với các bộ dữ liệu kích thước nhỏ (ví dụ: TinyBERT₄ w/o GD hoạt động kém trên CoLA, RTE và STS-B). Trong khi chưng cất tổng quát có thể mất đến hàng trăm giờ GPU để huấn luyện, CoFi huấn luyện tối đa 20 giờ trên một bộ dữ liệu đặc thù cho tác vụ với một GPU duy nhất. Chúng tôi lập luận rằng các phương pháp cắt tỉa—được huấn luyện với các mục tiêu chưng cất như CoFi—kinh tế và hiệu quả hơn trong việc đạt được các mô hình nén.

Chúng tôi tiếp tục so sánh CoFi với TinyBERT₄ trong thiết lập tăng cường dữ liệu trong Bảng 3. Vì bộ dữ liệu tăng cường không được phát hành công khai, chúng tôi theo kho GitHub của nó để tạo dữ liệu tăng cường của riêng chúng tôi. Chúng tôi huấn luyện CoFi với cùng tập dữ liệu tăng cường và thấy rằng nó vẫn vượt trội hơn TinyBERT₄ trên hầu hết các bộ dữ liệu.¹²

4.3 Nghiên cứu loại bỏ
Đơn vị cắt tỉa Chúng tôi đầu tiên tiến hành một nghiên cứu loại bỏ để điều tra cách các đơn vị cắt tỉa bổ sung như các lớp MHA, lớp FFN và đơn vị ẩn trong CoFi ảnh hưởng đến hiệu suất mô hình và tốc độ tăng suy luận ngoài thực hành tiêu chuẩn của việc cắt tỉa các đầu và chiều FFN. Chúng tôi hiển thị kết quả trong Bảng 4 cho các mô hình có kích thước tương tự. Việc loại bỏ tùy chọn cắt tỉa chiều ẩn (zhidn) dẫn đến một mô hình nhanh hơn một chút với sự sụt giảm hiệu suất trên toàn bộ và chúng tôi thấy rằng nó loại bỏ nhiều lớp hơn CoFi và không dẫn đến hiệu suất tối ưu dưới một ràng buộc độ thưa cụ thể. Ngoài ra, việc loại bỏ các mặt nạ lớp (zMHA, zFFN) mang lại sự sụt giảm đáng kể về tốc độ tăng trên các mô hình nén cao (95%, 5M). Kết quả này cho thấy rằng ngay cả với cùng số lượng tham số, các cấu hình khác nhau cho một mô hình có thể dẫn đến tốc độ tăng rất khác nhau. Tuy nhiên, nó không ảnh hưởng đến chế độ độ thưa thấp hơn (60%, 34M). Tóm lại, bằng cách đặt các biến che ở các mức khác nhau, quá trình tối ưu hóa được khuyến khích cắt tỉa các đơn vị tương ứng dưới ràng buộc độ thưa trong khi tối đa hóa hiệu suất mô hình.

Mục tiêu chưng cất Chúng tôi cũng loại bỏ các mục tiêu chưng cất để xem mỗi phần đóng góp như thế nào vào hiệu suất của CoFi trong Bảng 5. Chúng tôi đầu tiên quan sát thấy việc loại bỏ hoàn toàn chưng cất dẫn đến sự sụt giảm hiệu suất lên đến 1,9-6,8 điểm trên các bộ dữ liệu khác nhau, cho thấy sự cần thiết phải kết hợp cắt tỉa và chưng cất để duy trì hiệu suất. Mục tiêu chưng cất lớp ẩn được đề xuất động khớp các lớp từ mô hình giáo viên với mô hình học sinh. Chúng tôi cũng thử nghiệm với một lựa chọn thay thế đơn giản tức là "Chưng cất ẩn cố định", khớp mỗi lớp từ mô hình giáo viên với lớp tương ứng trong mô hình học sinh – nếu một lớp đã được cắt tỉa, mục tiêu chưng cất sẽ không được thêm vào. Chúng tôi thấy rằng chưng cất ẩn cố định kém hiệu suất hơn mục tiêu khớp lớp động được sử dụng cho CoFi. Thú vị là, mục tiêu khớp lớp động được đề xuất luôn hội tụ đến một sự liên kết cụ thể giữa các lớp của mô hình giáo viên và mô hình học sinh. Ví dụ, chúng tôi thấy rằng trên QNLI, quá trình huấn luyện động khớp các lớp 3, 6, 9, 12 trong mô hình giáo viên với các lớp 1, 2, 4, 9 trong mô hình học sinh.¹³ Hơn nữa, như thể hiện trong bảng, việc loại bỏ nó làm tổn hại hiệu suất cho tất cả các bộ dữ liệu trừ SST-2.

4.4 Cấu trúc của các mô hình đã cắt tỉa
Cuối cùng, chúng tôi nghiên cứu các cấu trúc đã cắt tỉa được tạo ra bởi CoFi. Chúng tôi đặc trưng hóa các mô hình đã cắt tỉa với độ thưa {60%, 70%, 80%, 90%, 95%} trên năm bộ dữ liệu. Đối với mỗi thiết lập, chúng tôi chạy CoFi ba lần. Hình 3 chứng minh số lượng đầu còn lại và chiều trung gian của các mô hình đã cắt tỉa cho các độ thưa khác nhau.¹⁴ Thú vị là, chúng tôi khám phá các mẫu cấu trúc chung trong các mô hình đã cắt tỉa: (1) Các lớp feed-forward được cắt tỉa đáng kể trên tất cả các độ thưa. Ví dụ, ở mức độ thưa 60%, số lượng chiều trung gian trung bình trong các lớp FFN sau khi cắt tỉa giảm 71% (3.072→884), và số lượng đầu trung bình trong MHA giảm 39% (12→7,3). Điều này cho thấy các lớp FFN dư thừa hơn các lớp MHA. (2) CoFi có xu hướng cắt tỉa các đơn vị con nhiều hơn từ các lớp trên so với các lớp dưới. Ví dụ, các lớp MHA trên có ít đầu còn lại hơn các lớp dưới trung bình.

Hơn nữa, chúng tôi nghiên cứu số lượng lớp FFN và MHA còn lại và trực quan hóa kết quả trong Bảng 6 cho các mô hình nén cao (độ thưa = 95%). Mặc dù tất cả các mô hình có kích thước gần như tương tự, chúng trình bày các mẫu khác nhau trên các bộ dữ liệu, điều này cho thấy rằng tồn tại các mạng con tối ưu khác nhau cho mỗi bộ dữ liệu. Chúng tôi thấy rằng trên SST-2 và QNLI, lớp MHA đầu tiên được bảo tồn nhưng có thể bị loại bỏ trên QQP và SQuAD. Chúng tôi cũng quan sát thấy một số lớp đặc biệt quan trọng trên tất cả các bộ dữ liệu. Ví dụ, lớp MHA đầu tiên và lớp MHA thứ hai được bảo tồn hầu hết thời gian, trong khi các lớp giữa thường bị loại bỏ. Nói chung, các mô hình đã cắt tỉa chứa nhiều lớp MHA hơn lớp FFN (xem Phụ lục H), điều này cho thấy rằng các lớp MHA quan trọng hơn để giải quyết các tác vụ downstream. Tương tự như Press et al. (2020), chúng tôi thấy rằng mặc dù các mạng Transformer tiêu chuẩn có các lớp FFN và MHA xen kẽ, trong các mô hình đã cắt tỉa của chúng tôi, các lớp FFN/MHA liền kề có thể dẫn đến hiệu suất tốt hơn.

⁸Chúng tôi cũng chạy thí nghiệm trên các mô hình RoBERTa (Liu et al., 2019a). Vui lòng tham khảo Phụ lục I để biết chi tiết.
⁹Chúng tôi hiển thị các kết quả này trong Phụ lục F vì chúng không thể so sánh trực tiếp với CoFi.
¹⁰Đối với TinyBERT, dữ liệu tăng cường lớn hơn 20× so với dữ liệu gốc, làm cho quá trình huấn luyện chậm hơn đáng kể.
¹¹Các mô hình có cùng tỷ lệ nén có thể có tốc độ tăng khác nhau đáng kể.
¹²Chúng tôi chỉ tiến hành thí nghiệm với tăng cường dữ liệu trên bốn bộ dữ liệu vì huấn luyện trên dữ liệu tăng cường rất tốn kém. Ví dụ, huấn luyện trên bộ dữ liệu tăng cường cho MNLI mất hơn 200 giờ GPU tổng cộng. Xem thêm chi tiết trong Phụ lục E.
¹³Vui lòng tham khảo §G.1 để biết thêm chi tiết.
¹⁴Chúng tôi hiển thị thêm phân tích lớp trong Phụ lục H.

--- TRANG 6 ---
[Hình 2: Độ chính xác so với tốc độ tăng (trên) hoặc kích thước mô hình (dưới)]

[Bảng 2: CoFi so với TinyBERT₄ với tốc độ tăng 10×]

[Bảng 3: CoFi so với TinyBERT₄ được huấn luyện với tăng cường dữ liệu đặc thù cho tác vụ]

--- TRANG 7 ---
[Bảng 4: Nghiên cứu loại bỏ về các đơn vị cắt tỉa]

[Bảng 5: Nghiên cứu loại bỏ về các mục tiêu chưng cất khác nhau]

--- TRANG 8 ---
[Hình 3: Các chiều trung gian trung bình và số đầu trung bình trong các mô hình đã cắt tỉa]

[Bảng 6: Các lớp còn lại trong các mô hình được cắt tỉa bởi CoFi]

--- TRANG 9 ---
5 Công trình liên quan
Cắt tỉa có cấu trúc đã được khám phá rộng rãi trong thị giác máy tính, nơi cắt tỉa kênh (He et al., 2017; Luo et al., 2017; Liu et al., 2017, 2019c,b; Molchanov et al., 2019; Guo et al., 2020) là một phương pháp tiêu chuẩn cho các mạng thần kinh tích chập. Các kỹ thuật có thể được điều chỉnh cho các mô hình dựa trên Transformer như đã giới thiệu trong §2.3. Cắt tỉa không có cấu trúc là một hướng nghiên cứu chính khác, đặc biệt được ưa chuộng trong lý thuyết Giả thuyết Vé số (Frankle and Carbin, 2019; Zhou et al., 2019; Renda et al., 2020; Frankle et al., 2020; Chen et al., 2020a). Cắt tỉa không có cấu trúc tạo ra các mô hình với độ thưa cao (Sanh et al., 2020; Xu et al., 2021; Huang et al., 2021) nhưng khó mang lại tốc độ tăng suy luận thực tế. Phát triển các nền tảng tính toán cho các hoạt động tensor thưa hiệu quả là một lĩnh vực nghiên cứu tích cực. DeepSparse¹⁵ là một động cơ suy luận CPU tận dụng độ thưa không có cấu trúc để tăng tốc. Huang et al. (2021) đo tốc độ tăng suy luận thực tế được tạo ra bởi cắt tỉa không có cấu trúc trên nền tảng phần cứng mới nhất ANTOM của Moffett AI. Chúng tôi không so sánh trực tiếp với các phương pháp này vì môi trường đánh giá khác nhau.

Trong khi tất cả các phương pháp nêu trên tạo ra các mô hình đặc thù cho tác vụ thông qua cắt tỉa, một số công trình khám phá cắt tỉa upstream nơi họ cắt tỉa một mô hình lớn đã được huấn luyện trước với tác vụ mô hình hóa ngôn ngữ có mặt nạ. Chen et al. (2020a) cho thấy một mô hình độ thưa 70% giữ lại độ chính xác MLM được tạo ra bởi cắt tỉa độ lớn lặp đi lặp lại. Zafrir et al. (2021) cho thấy lợi thế tiềm năng của cắt tỉa không có cấu trúc upstream so với cắt tỉa downstream. Chúng tôi coi việc áp dụng CoFi cho cắt tỉa upstream như một hướng tương lai hứa hẹn để tạo ra các mô hình bất khả tri tác vụ với cấu trúc linh hoạt.

Ngoài cắt tỉa, nhiều kỹ thuật khác đã được khám phá để có được tốc độ tăng suy luận cho các mô hình Transformer, bao gồm chưng cất như đã giới thiệu trong §2.2, lượng tử hóa (Shen et al., 2020; Fan et al., 2021), tăng tốc suy luận động (Xin et al., 2020) và phân tách ma trận (Noach and Goldberg, 2020). Chúng tôi giới thiệu người đọc đến Ganesh et al. (2021) để có một khảo sát toàn diện.

6 Kết luận
Chúng tôi đề xuất CoFi, một phương pháp cắt tỉa có cấu trúc kết hợp tất cả các cấp độ cắt tỉa, bao gồm các lớp MHA/FFN, các đầu riêng lẻ và các chiều ẩn cho các mô hình dựa trên Transformer. Kết hợp với một mục tiêu chưng cất được thiết kế riêng cho cắt tỉa có cấu trúc, chúng tôi cho thấy CoFi nén các mô hình thành một cấu trúc khá khác biệt so với các mô hình chưng cất tiêu chuẩn nhưng vẫn đạt được kết quả cạnh tranh với tốc độ tăng hơn 10×. Chúng tôi kết luận rằng cắt tỉa có cấu trúc đặc thư cho tác vụ từ các mô hình kích thước lớn có thể là một sự thay thế hấp dẫn cho chưng cất để đạt được nén mô hình cực đoan, mà không cần dựa vào huấn luyện trước tốn kém hoặc tăng cường dữ liệu. Mặc dù CoFi có thể được áp dụng trực tiếp cho cắt tỉa có cấu trúc cho các mô hình bất khả tri tác vụ, chúng tôi đóng khung phạm vi của công trình này cho cắt tỉa đặc thư cho tác vụ do sự phức tạp của các lựa chọn thiết kế cho cắt tỉa upstream. Chúng tôi hy vọng rằng nghiên cứu tương lai tiếp tục dòng công việc này, cho rằng cắt tỉa từ một mô hình lớn đã được huấn luyện trước có thể phát sinh ít tính toán hơn so với chưng cất tổng quát và dẫn đến các cấu trúc mô hình linh hoạt hơn.

Lời cảm ơn
Các tác giả cảm ơn Tao Lei từ Google Research, Ameet Deshpande, Dan Friedman, Sadhika Malladi từ Đại học Princeton và các nhà phản biện ẩn danh vì phản hồi có giá trị của họ về bài báo của chúng tôi. Nghiên cứu này được hỗ trợ bởi Học bổng Hisashi và Masae Kobayashi *67 và Giải thưởng Google Research Scholar.

¹⁵https://github.com/neuralmagic/deepsparse

--- TRANG 10 ---
Tài liệu tham khảo

Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1–14, Vancouver, Canada. Association for Computational Linguistics.

Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and Michael Carbin. 2020a. The lottery ticket hypothesis for pre-trained BERT networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 15834–15846.

Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing Liu. 2020b. Early-bert: Efficient bert training via early-bird lottery tickets. arXiv preprint arXiv:2101.00063.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics (NAACL), pages 4171–4186.

William B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).

Angela Fan, Edouard Grave, and Armand Joulin. 2020. Reducing Transformer depth on demand with structured dropout. In International Conference on Learning Representations (ICLR).

Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Rémi Gribonval, Herve Jegou, and Armand Joulin. 2021. Training with quantization noise for extreme model compression. In International Conference on Learning Representations (ICLR).

Jonathan Frankle and Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations (ICLR).

Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. 2020. Linear mode connectivity and the lottery ticket hypothesis. In International Conference on Machine Learning (ICML), pages 3259–3269.

Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. 2021. Compressing large-scale Transformer-based models: A case study on BERT. Transactions of the Association of Computational Linguistics (TACL), 9:1061–1080.

Shaopeng Guo, Yujie Wang, Quanquan Li, and Junjie Yan. 2020. Dmcp: Differentiable markov channel pruning for neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).

Yihui He, Xiangyu Zhang, and Jian Sun. 2017. Channel pruning for accelerating very deep neural networks. In International Conference on Computer Vision (ICCV), pages 1389–1397.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.

Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. 2020. DynaBERT: Dynamic bert with adaptive width and depth. In Advances in Neural Information Processing Systems (NeurIPS), volume 33.

Shaoyi Huang, Dongkuan Xu, Ian EH Yen, Sung-en Chang, Bingbing Li, Shiyang Chen, Mimi Xie, Hang Liu, and Caiwen Ding. 2021. Sparse progressive distillation: Resolving overfitting under pretrain-and-finetune paradigm. arXiv e-prints, pages arXiv–2110.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. TinyBERT: Distilling BERT for natural language understanding. In Findings of Empirical Methods in Natural Language Processing (EMNLP), pages 4163–4174.

François Lagunas, Ella Charlaix, Victor Sanh, and Alexander M Rush. 2021. Block pruning for faster transformers. arXiv preprint arXiv:2109.04838.

Jiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2021. Differentiable subset pruning of Transformer heads. Transactions of the Association of Computational Linguistics (TACL).

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019a. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.

Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, and Jian Sun. 2019b. Metapruning: Meta learning for automatic neural network channel pruning. In International Conference on Computer Vision (ICCV), pages 3296–3305.

Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. 2017. Learning efficient convolutional networks through network slimming. In International Conference on Computer Vision (ICCV), pages 2736–2744.

Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. 2019c. Rethinking the value of network pruning. In International Conference on Learning Representations (ICLR).

--- TRANG 11 ---
C Louizos, M Welling, and DP Kingma. 2018. Learning sparse neural networks through l0 regularization. In International Conference on Learning Representations (ICLR).

Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. 2017. Thinet: A filter level pruning method for deep neural network compression. In International Conference on Computer Vision (ICCV), pages 5058–5066.

JS McCarley, Rishav Chakravarti, and Avirup Sil. 2019. Structured pruning of a BERT-based question answering model. arXiv preprint arXiv:1910.06360.

Paul Michel, Omer Levy, and Graham Neubig. 2019. Are sixteen heads really better than one? In Advances in Neural Information Processing Systems (NeurIPS).

Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. 2019. Importance estimation for neural network pruning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11264–11272.

Matan Ben Noach and Yoav Goldberg. 2020. Compressing pre-trained language models by matrix decomposition. In Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 884–889.

Sai Prasanna, Anna Rogers, and Anna Rumshisky. 2020. When BERT plays the lottery, all tickets are winning. In Empirical Methods in Natural Language Processing (EMNLP), pages 3208–3229.

Ofir Press, Noah A Smith, and Omer Levy. 2020. Improving transformer models by reordering their sub-layers. In Association for Computational Linguistics (ACL), pages 2996–3005.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text Transformer. The Journal of Machine Learning Research (JMLR), 21(140).

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP), pages 2383–2392.

Alex Renda, Jonathan Frankle, and Michael Carbin. 2020. Comparing rewinding and fine-tuning in neural network pruning. In International Conference on Learning Representations (ICLR).

Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov. 2020. Poor man's BERT: Smaller and faster transformer models. arXiv preprint arXiv:2004.03844.

Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.

Victor Sanh, Thomas Wolf, and Alexander Rush. 2020. Movement pruning: Adaptive sparsity by fine-tuning. Advances in Neural Information Processing Systems (NeurIPS), 33.

Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-BERT: Hessian based ultra low precision quantization of BERT. In Conference on Artificial Intelligence (AAAI), pages 8815–8821.

Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Empirical Methods in Natural Language Processing (EMNLP).

Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019. Patient knowledge distillation for BERT model compression. In Empirical Methods in Natural Language Processing (EMNLP), pages 4314–4323.

Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. MobileBERT: a compact task-agnostic bert for resource-limited devices. In Association for Computational Linguistics (ACL), pages 2158–2170.

Philippe Tillet, Hsiang-Tsung Kung, and David Cox. 2019. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10–19.

Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in Neural Information Processing Systems (NIPS), 30:5998–6008.

Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Association for Computational Linguistics (ACL), pages 5797–5808.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations (ICLR).

--- TRANG 12 ---
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020a. MiniLM: Deep self-attention distillation for task-agnostic compression of pre-trained Transformers. In Advances in Neural Information Processing Systems (NeurIPS).

Ziheng Wang, Jeremy Wohlwend, and Tao Lei. 2020b. Structured pruning of large language models. In Empirical Methods in Natural Language Processing (EMNLP), pages 6151–6162.

Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2019. Neural network acceptability judgments. In Transactions of the Association of Computational Linguistics (TACL), volume 7, pages 625–641.

Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT).

Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. DeeBERT: Dynamic early exiting for accelerating BERT inference. In Association for Computational Linguistics (ACL), pages 2246–2251.

Dongkuan Xu, Ian En-Hsu Yen, Jinxi Zhao, and Zhibin Xiao. 2021. Rethinking network pruning–under the pre-train and fine-tune paradigm. In North American Chapter of the Association for Computational Linguistics (NAACL), pages 2376–2382.

Zhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, and Michael W Mahoney. 2021. MLPruning: A multilevel structured pruning framework for transformer-based models. arXiv preprint arXiv:2105.14636.

Yichun Yin, Cheng Chen, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. 2021. AutoTinyBERT: Automatic hyper-parameter optimization for efficient pre-trained language models. In Association for Computational Linguistics (ACL), pages 5146–5157.

Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, and Moshe Wasserblat. 2021. Prune once for all: Sparse pre-trained language models. arXiv preprint arXiv:2111.05754.

Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. 2019. Deconstructing lottery tickets: Zeros, signs, and the supermask. In Advances in Neural Information Processing Systems (NeurIPS), volume 32. Curran Associates, Inc.

--- TRANG 13 ---
A Khả năng tái tạo & Siêu tham số
Chúng tôi báo cáo các siêu tham số mà chúng tôi sử dụng trong thí nghiệm của mình trong Bảng 7.

Siêu tham số
α 0,1; 0,3; 0,5
nhiệt độ chưng cất t 2
số epoch tinh chỉnh 20
tốc độ học tinh chỉnh 1e-5; 2e-5; 3e-5
tốc độ học huấn luyện 2e-5 (GLUE), 3e-5 (SQuAD)
kích thước batch 32 (GLUE), 16 (SQuAD)

Bảng 7: Siêu tham số trong các thí nghiệm.

Đối với bốn bộ dữ liệu GLUE tương đối lớn hơn, MNLI, QNLI, SST-2 và QQP, và SQuAD, chúng tôi huấn luyện mô hình trong tổng cộng 20 epoch và tinh chỉnh mạng con cuối cùng trong 20 epoch khác. Trong 20 epoch đầu, theo Lagunas et al. (2021) và Wang et al. (2020b), chúng tôi đầu tiên tinh chỉnh mô hình với mục tiêu chưng cất trong 1 epoch, và sau đó bắt đầu cắt tỉa với một lịch trình tuyến tính để đạt được độ thưa mục tiêu trong 2 epoch. Đối với bốn bộ dữ liệu GLUE nhỏ, chúng tôi huấn luyện mô hình trong tổng cộng 100 epoch và tinh chỉnh trong 20 epoch. Chúng tôi tinh chỉnh mô hình với mục tiêu chưng cất trong 4 epoch và cắt tỉa đến độ thưa mục tiêu trong 20 epoch tiếp theo. Lưu ý rằng ngay cả khi độ thưa cuối cùng đạt được, quá trình cắt tỉa tiếp tục tìm kiếm các cấu trúc hoạt động tốt hơn trong các epoch huấn luyện còn lại. Ngoài ra, chúng tôi thấy rằng việc tinh chỉnh mạng con cuối cùng là thiết yếu cho các mô hình độ thưa cao.

Các siêu tham số như α, kích thước batch và tốc độ học nói chung không ảnh hưởng nhiều đến hiệu suất.

B Chi tiết tối ưu hóa
Louizos et al. (2018) đề xuất tối ưu hóa l0 cho nén mô hình trong đó các mặt nạ được mô hình hóa với phân phối concrete cứng như sau:

u ~ U(0,1)
s = sigmoid((log u/(1-u) + log γ)/ζ)
s̃ = s(r-l) + l
z = min(1, max(0, s̃)):

U(0,1) là một phân phối đều trong khoảng [0,1]; l < 0 và r > 0 là hai hằng số kéo dài đầu ra sigmoid vào khoảng (l,r). ζ là một siêu tham số kiểm soát độ dốc của hàm sigmoid và log γ là các tham số có thể học chính. Chúng tôi học các mặt nạ thông qua việc cập nhật các tham số có thể học của các phân phối mà từ đó các mặt nạ được lấy mẫu trong lượt truyền tiến.

Trong các thí nghiệm sơ bộ của chúng tôi, chúng tôi thấy rằng việc tối ưu hóa ||z||₀ với các tốc độ học và lịch trình cắt tỉa khác nhau có thể hội tụ đến các mô hình có kích thước khác nhau một cách drastic. Do đó, chúng tôi theo Wang et al. (2020b) để thêm một thuật ngữ Lagrangian, áp đặt một ràng buộc đẳng thức ŝ = t bằng cách giới thiệu một hình phạt vi phạm:

Lc = λ₁(ŝ-t) + λ₂(ŝ-t)²;

trong đó ŝ là độ thưa mô hình dự kiến được tính từ z và t là độ thưa mục tiêu.

C Chi tiết của các phương pháp baseline
Chúng tôi so sánh với một số mô hình cắt tỉa và chưng cất mạnh, bao gồm 1) DistillBERT₆ (Sanh et al., 2019); 2) TinyBERT₆ và TinyBERT₄ (Jiao et al., 2020) cả hai đều bao gồm chưng cất tổng quát để huấn luyện trước và chưng cất đặc thù cho tác vụ; 3) DynaBERT (Hou et al., 2020): một phương pháp cung cấp các mô hình kích thước động bằng cách chỉ định chiều rộng và chiều sâu; 4) Block Pruning (Lagunas et al., 2021): một phương pháp cắt tỉa kết hợp với chưng cất lớp dự đoán. Chúng tôi chọn phương pháp mạnh nhất của họ "Hybrid Filled" làm baseline của chúng tôi.

D Thống kê dữ liệu
Chúng tôi hiển thị kích thước huấn luyện và số liệu cho mỗi bộ dữ liệu chúng tôi sử dụng trong Bảng 8.

Tác vụ    Kích thước huấn luyện    Số liệu
SST-2     67k                      độ chính xác
QNLI      105k                     độ chính xác
MNLI      393k                     độ chính xác
QQP       364k                     độ chính xác
CoLA      8,5k                     Matthews corr.
RTE       2,5k                     độ chính xác
STS-B     7k                       Spearman corr.
MRPC      3,7k                     độ chính xác
SQuAD     88k                      F1

Bảng 8: Thống kê dữ liệu của các bộ dữ liệu GLUE và SQuAD.

--- TRANG 14 ---
E TinyBERT₄ với tăng cường dữ liệu
Chúng tôi tiến hành chưng cất đặc thù cho tác vụ với script được cung cấp bởi kho TinyBERT.¹⁶ Tuy nhiên, các kết quả tái tạo của chúng tôi thấp hơn một chút so với các kết quả được báo cáo trong (Jiao et al., 2020). Sự khác biệt giữa hai tập điểm số này có thể xuất phát từ dữ liệu tăng cường hoặc các mô hình giáo viên. Lưu ý rằng các tác giả của TinyBERT không phát hành bộ dữ liệu tăng cường. Chúng tôi chạy mã của họ để có được các bộ dữ liệu tăng cường. Chúng tôi so sánh CoFi và TinyBERT trong cùng thiết lập nơi chúng tôi sử dụng cùng mô hình giáo viên và cùng tập dữ liệu tăng cường.

SST-2  QNLI  RTE   MRPC
TinyBERT₄ tái hiện  91,6  87,6  62,5  83,6
Jiao et al. (2020)  92,7  88,0  65,7  85,7

Bảng 9: Kết quả tái hiện (TinyBERT₄ tái hiện) và các kết quả được báo cáo trong Jiao et al. (2020).

F So sánh bổ sung

F.1 So sánh với Movement Pruning
Chúng tôi so sánh CoFi với một phương pháp cắt tỉa không có cấu trúc tiên tiến, Movement Pruning (Sanh et al., 2020) trong Hình 4. Vì Movement Pruning được huấn luyện chỉ với chưng cất lớp dự đoán (logit), chúng tôi cũng hiển thị kết quả của CoFi được huấn luyện với cùng mục tiêu chưng cất. Chúng tôi quan sát thấy CoFi vượt trội hơn Movement Pruning một cách lớn ngay cả khi không có chưng cất theo lớp trên MNLI và có thể so sánh với SQuAD trên các mô hình có kích thước trên 10M tham số. CoFi, như một phương pháp cắt tỉa có cấu trúc, kém hiệu suất hơn trên các mô hình có độ thưa lên đến 95%, vì tính linh hoạt cắt tỉa bị hạn chế lớn bởi đơn vị cắt tỉa nhỏ nhất. Tuy nhiên, các mô hình đã cắt tỉa từ CoFi đạt được tốc độ tăng suy luận 2-11× trong khi không có lợi ích tốc độ tăng nào đạt được từ Movement Pruning.

F.2 So sánh với Block Pruning
Trong Hình 6, chúng tôi so sánh CoFi với Block Pruning trong khi thống nhất mục tiêu chưng cất. Không có mục tiêu chưng cất lớp, CoFi vẫn vượt trội hoặc ngang bằng với Block Pruning. Block Pruning không bao giờ đạt được tốc độ tăng 10× ngay cả khi mô hình đã cắt tỉa có kích thước tương tự như CoFi (SST-2), ủng hộ lập luận của chúng tôi rằng việc cắt tỉa các lớp cho các mô hình độ thưa cao là chìa khóa cho tốc độ tăng cao.

F.3 Thêm baseline
Chúng tôi hiển thị các phương pháp cắt tỉa và chưng cất bổ sung không thể so sánh trực tiếp với CoFi trong Bảng 10. CoFi vẫn vượt trội hơn lớn các baseline này ngay cả khi các phương pháp này có lợi thế vốn có do một giáo viên hoặc mô hình cơ sở mạnh hơn.

[Bảng 10: Thêm baseline cắt tỉa và chưng cất]

G Thêm phân tích về chưng cất lớp

G.1 Liên kết lớp
Chúng tôi thấy rằng sự liên kết giữa các lớp của mô hình học sinh và mô hình giáo viên thay đổi trong quá trình huấn luyện. Lấy SST-2 làm ví dụ, khi quá trình huấn luyện tiếp tục, mô hình học được sự liên kết để khớp các lớp 7, 9, 10, 11 của mô hình học sinh với các lớp 3, 6, 9, 12 của mô hình giáo viên. Đối với QQP, mô hình cuối cùng học được cách ánh xạ các lớp 2, 5, 8, 11 với bốn lớp của mô hình giáo viên. Sự liên kết cuối cùng cho thấy rằng mục tiêu chưng cất khớp lớp động của chúng tôi có thể tìm thấy sự liên kết đặc thù cho tác vụ và cải thiện hiệu suất.

G.2 Loại bỏ về mục tiêu chưng cất
Trong Bảng 11, chúng tôi hiển thị các nghiên cứu loại bỏ về việc thêm chưng cất lớp động vào chưng cất dự đoán trên tất cả các độ thưa. Việc sử dụng mất mát chưng cất lớp rõ ràng giúp cải thiện hiệu suất trên tất cả các tỷ lệ độ thưa và các tác vụ khác nhau.

H Các lớp FFN/MHA trong các mô hình đã cắt tỉa
Hình 5 hiển thị số lượng lớp FFN và lớp MHA trung bình trong các mô hình đã cắt tỉa bởi CoFi. Chúng tôi nghiên cứu các độ thưa khác nhau {60%, 70%, 80%, 90%, 95%}. Rõ ràng là khi độ thưa tăng, các mô hình đã cắt tỉa trở nên nông hơn (tức là số lượng lớp trở nên ít hơn). Hơn nữa, chúng tôi thấy rằng các mô hình đã cắt tỉa thường có nhiều lớp MHA hơn lớp FFN. Điều này có thể chỉ ra rằng các lớp MHA quan trọng hơn để giải quyết các tác vụ downstream này so với các lớp FFN.

[Hình 5: Số lượng lớp FFN và MHA trung bình trong các mô hình đã cắt tỉa ở các độ thưa khác nhau]

I Cắt tỉa RoBERTa
Chúng tôi hiển thị kết quả CoFi với RoBERTa trong Hình 7 trên các độ thưa từ 60% đến 95%. Tương tự như BERT, các mô hình với 60% trọng số đã cắt tỉa có thể duy trì hiệu suất của mô hình đầy đủ. Cắt tỉa từ RoBERTa vượt trội hơn BERT trên các độ thưa thấp hơn 90% nhưng khi độ thưa tăng hơn nữa, BERT vượt qua RoBERTa. Các mẫu tương tự được quan sát từ DynaBERT (Hou et al., 2020).

J Đo thời gian huấn luyện
Chúng tôi sử dụng GPU NVIDIA RTX 2080Ti để đo thời gian huấn luyện của TinyBERT. Đối với bước chưng cất tổng quát của TinyBERT, chúng tôi đo thời gian huấn luyện trên một kho ngữ liệu nhỏ (chứa 10,6M token) trên 4 GPU và ước tính thời gian huấn luyện trên kho ngữ liệu gốc (chứa 2500M token) bằng cách tỷ lệ thời gian với sự khác biệt kích thước kho ngữ liệu. Cụ thể, mất 430s để hoàn thành một epoch trên 10,6M token với 4 GPU, và chúng tôi ước tính rằng sẽ mất 338 giờ GPU (hoặc 3,5 ngày với 4 GPU) để hoàn thành ba epoch trên 2500M token.

¹⁶https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/TinyBERT

--- TRANG 15 ---
[Hình 6: CoFi so với Block Pruning với cùng mục tiêu chưng cất]

[Bảng 11: Nghiên cứu loại bỏ về mục tiêu chưng cất lớp được đề xuất trên tất cả các độ thưa]

[Hình 7: CoFi với BERT và RoBERTa trên SST-2 và MNLI]

--- TRANG 16 ---
[Hình 4: CoFi so với Movement Pruning (cắt tỉa không có cấu trúc)]
