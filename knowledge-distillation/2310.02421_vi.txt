# 2310.02421.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.02421.pdf
# Kích thước tệp: 148196 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
arXiv:2310.02421v1  [cs.LG]  3 Oct 2023Liệu một mô hình Ngôn ngữ Lớn học sinh có thể hoạt động tốt
như giáo viên của nó?
Sia Gholami
Viện Kỹ sư Điện và Điện tử, Thành viên IEEE
gholami@ieee.org
Marwan Omar
Viện Công nghệ Illinois
momar3@iit.edu
Tóm tắt
Sự phức tạp ngày càng tăng của các mô hình học sâu đương đại, trong khi đạt được
độ chính xác chưa từng có, đã vô tình tạo ra những thách thức triển khai
trong môi trường hạn chế tài nguyên. Chưng cất tri thức, một kỹ thuật nhằm
chuyển giao tri thức từ một mô hình "giáo viên" công suất cao đến một
mô hình "học sinh" được sắp xếp hợp lý, nổi lên như một giải pháp đầy hứa hẹn cho tình trạng khó xử này. Bài báo này
cung cấp một cái nhìn tổng quan toàn diện về mô hình chưng cất tri thức, nhấn
mạnh các nguyên lý nền tảng của nó như tiện ích của nhãn mềm và ý nghĩa
của việc điều chỉnh nhiệt độ. Thông qua việc kiểm tra tỉ mỉ, chúng tôi làm sáng tỏ
các yếu tố quyết định quan trọng của chưng cất thành công, bao gồm kiến trúc của
mô hình học sinh, chất lượng của giáo viên, và sự cân bằng tinh tế của các siêu tham số. Trong khi thừa nhận những lợi thế sâu sắc của nó, chúng tôi cũng đi sâu vào
những phức tạp và thách thức vốn có trong quy trình. Nghiên cứu của chúng tôi nhấn mạnh
tiềm năng của chưng cất tri thức như một kỹ thuật then chốt trong việc tối ưu hóa sự đánh đổi
giữa hiệu suất mô hình và hiệu quả triển khai.
1 Giới thiệu
Trong những năm gần đây, bối cảnh học sâu đã được đặc trưng bởi các mô hình ngày càng
lớn và phức tạp. Trong khi những mô hình như vậy, thường tự hào có hàng tỷ tham số, liên tục thiết lập
các tiêu chuẩn mới về độ chính xác, cường độ tính toán của chúng tạo ra
những thách thức triển khai, đặc biệt trong môi trường có tài nguyên tính toán hạn chế, chẳng hạn
như các thiết bị biên [Tao et al., 2020]. Chưng cất tri thức cung cấp một giải pháp khả thi cho tình trạng khó xử này, tạo điều kiện cho việc chuyển giao tri thức từ
một mô hình "giáo viên" tinh vi, công suất cao đến một mô hình "học sinh" nhỏ gọn hơn, nhằm giữ lại
càng nhiều hiệu suất càng tốt [Hinton et al., 2015].
Trung tâm của chưng cất tri thức là nguyên lý rằng việc học có thể được tăng cường khi các mô hình được
huấn luyện không chỉ trên nhãn cứng mà còn trên các đầu ra xác suất phong phú hơn của một mô hình giáo viên. Những
nhãn mềm này có thể được nhận thức như việc nắm bắt phân bố độ tin cậy của giáo viên trên các lớp, cung cấp
những hiểu biết tinh tế mà nhãn cứng có thể bỏ qua [Bucilua et al., 2006].
Một thành phần quan trọng của phương pháp này là điều chỉnh nhiệt độ, điều này điều tiết mức độ chi tiết của
những nhãn mềm này. Tham số nhiệt độ, được giới thiệu bởi Hinton et al. [2015], đóng một vai trò then chốt
trong việc kiểm soát "độ sắc nét" của các phân bố đầu ra của giáo viên, do đó ảnh hưởng đến chất lượng của
thông tin được chuyển tiếp đến mô hình học sinh.
Bản thảo. Đang được xem xét.

--- TRANG 2 ---
Việc huấn luyện mô hình học sinh sau đó thường được hướng dẫn bởi một hàm mất mát có trọng số cân bằng
giữa mất mát entropy chéo thông thường và sự phân kỳ từ các đầu ra của giáo viên, thường
được đo bằng sự phân kỳ Kullback-Leibler [Lopez-Paz et al., 2015].
Tuy nhiên, quy trình không phải là không có phức tạp. Kiến trúc tối ưu của mô hình học sinh,
chất lượng của giáo viên, và sự cân bằng chính xác của các siêu tham số đều là những yếu tố quyết định trong
thành công của chưng cất [Polino et al., 2018]. Những phức tạp của các yếu tố này và sự tương tác của chúng
vẫn là một điểm tập trung của nghiên cứu đương đại.
Kết luận, chưng cất tri thức nổi lên như một kỹ thuật quan trọng trong bộ công cụ học sâu, bắc cầu
khoảng cách giữa hiệu suất tiên tiến và triển khai thực tế, hiệu quả. Việc tiếp tục khám phá nó chứa đựng lời hứa về việc tinh chỉnh và mở rộng thêm
khả năng ứng dụng của nó trên các lĩnh vực đa dạng.
Để sử dụng chưng cất tri thức cho việc tạo ra các transformer hiệu quả, quy trình thường bao gồm
các bước sau:
1. Huấn luyện một mô hình transformer lớn, phức tạp như mô hình giáo viên trên nhiệm vụ quan tâm.
2. Tạo ra một tập dữ liệu các ví dụ cho nhiệm vụ, và sử dụng mô hình giáo viên để tạo ra
dự đoán cho mỗi ví dụ.
3. Huấn luyện một mô hình transformer nhỏ hơn, đơn giản hơn như mô hình học sinh trên cùng nhiệm vụ, sử dụng
các dự đoán của mô hình giáo viên làm mục tiêu.
4. Sử dụng sự kết hợp của mất mát nhiệm vụ gốc và mất mát chưng cất để huấn luyện mô hình học sinh.
Mất mát chưng cất khuyến khích mô hình học sinh bắt chước các dự đoán của mô hình giáo viên, thay vì chỉ cố gắng tối ưu hóa mất mát nhiệm vụ gốc.
Bằng cách sử dụng chưng cất tri thức theo cách này, có thể tạo ra các mô hình transformer hiệu quả
nhỏ hơn và nhanh hơn mô hình gốc, trong khi vẫn đạt được hiệu suất tương đương hoặc thậm chí tốt hơn trên nhiệm vụ quan tâm.
Có một số lợi ích khi sử dụng chưng cất tri thức trong việc xây dựng các transformer hiệu quả:
1. Cải thiện hiệu quả: Chưng cất tri thức cho phép bạn tạo ra các mô hình Transformer nhỏ hơn, hiệu quả hơn mà yêu cầu ít tài nguyên tính toán hơn cho huấn luyện và suy luận.
Điều này cho phép xử lý nhanh hơn và giảm sử dụng bộ nhớ, làm cho việc triển khai các
mô hình trên các thiết bị hạn chế tài nguyên như điện thoại di động hoặc thiết bị biên dễ dàng hơn.
2. Giảm tiêu thụ năng lượng: Các mô hình nhỏ hơn được tạo ra thông qua chưng cất tri thức
tiêu thụ ít năng lượng hơn trong quá trình suy luận, điều này rất quan trọng cho các thiết bị chạy bằng pin và
các giải pháp AI bền vững.
3. Suy luận nhanh hơn: Kích thước và độ phức tạp giảm của các mô hình đã chưng cất dẫn đến thời gian suy luận nhanh hơn, điều này rất cần thiết trong các tình huống mà xử lý thời gian thực hoặc độ trễ thấp là
cần thiết, chẳng hạn như hiểu ngôn ngữ tự nhiên trong trợ lý giọng nói hoặc dịch văn bản
thời gian thực.
4. Tăng cường khái quát hóa: Chưng cất tri thức chuyển giao tri thức từ một mô hình lớn, hiệu suất cao đến một mô hình nhỏ hơn bằng cách huấn luyện trên các mục tiêu mềm. Những mục tiêu mềm này chứa thông tin phong phú hơn về mối quan hệ giữa các lớp khác nhau, có thể
giúp mô hình học sinh học các biểu diễn tốt hơn và khái quát hóa tốt hơn với dữ liệu chưa thấy [Komodakis and Zagoruyko, 2017].
5. Giữ lại hiệu suất: Mặc dù có sự giảm kích thước và độ phức tạp, các mô hình Transformer đã chưng cất có thể giữ lại phần lớn hiệu suất của các mô hình giáo viên lớn hơn. Điều này có nghĩa là bạn
vẫn có thể đạt được kết quả mạnh mẽ trên các nhiệm vụ NLP trong khi hưởng lợi từ những cải thiện về hiệu quả.
6. Triển khai hiệu quả về chi phí: Yêu cầu tính toán giảm của các mô hình đã chưng cất
có thể dẫn đến chi phí thấp hơn khi triển khai các giải pháp AI, đặc biệt trong các dịch vụ dựa trên đám mây
nơi chi phí tính toán được xác định bởi tài nguyên tiêu thụ.
7. Phân phối và cập nhật dễ dàng hơn: Các mô hình nhỏ hơn dễ phân phối và cập nhật hơn, giảm
thời gian và băng thông cần thiết cho người dùng tải xuống và cài đặt cập nhật, điều này
đặc biệt có lợi cho các ứng dụng trên thiết bị di động hoặc trong các khu vực có kết nối internet hạn chế.
2

--- TRANG 3 ---
Nhìn chung, chưng cất tri thức cung cấp một kỹ thuật mạnh mẽ để xây dựng các transformer hiệu quả có thể đạt được độ chính xác cao, khái quát hóa tốt, và được triển khai trên các thiết bị hạn chế tài nguyên.
2 Các Công trình Liên quan
Xử lý Ngôn ngữ Tự nhiên (NLP) đã là một lĩnh vực nghiên cứu chính trong Trí tuệ Nhân tạo và
Học Máy từ những ngày đầu của khoa học máy tính [Voorhees et al., 1999, Moldovan et al.,
2000, Brill et al., 2002, Ferrucci et al., 2010, Gholami and Noori, 2021, 2022, Gholami et al., 2022,
Gholami and Khashe, 2022a,b, Brand et al., 2022, Gholami and Omar, 2023]. Có một số ví dụ về việc sử dụng chưng cất tri thức để tạo ra các mô hình Transformer hiệu quả trong tài liệu. Dưới đây
là một số trường hợp đáng chú ý:
DistilBERT [Sanh et al., 2019] là một ví dụ phổ biến về việc áp dụng chưng cất tri thức để tạo ra một
phiên bản nhỏ hơn của BERT, một mô hình Transformer được huấn luyện trước quy mô lớn cho các nhiệm vụ NLP. DistilBERT
có ít hơn 40% tham số so với mô hình BERT gốc nhưng giữ lại 95% hiệu suất của nó trên
các nhiệm vụ benchmark khác nhau.
Sun et al. [Sun et al., 2020] giới thiệu MobileBERT, một mô hình Transformer hiệu quả được tạo ra bằng cách sử dụng
sự kết hợp của chưng cất tri thức và tìm kiếm kiến trúc. MobileBERT được thiết kế cho
các nhiệm vụ NLP trên thiết bị và đạt được tốc độ suy luận nhanh hơn 4.3 lần và kích thước mô hình nhỏ hơn 2.4 lần so với
BERT-base, trong khi duy trì mức hiệu suất tương tự.
Jiao et al. [Jiao et al., 2019] trình bày TinyBERT, một ví dụ khác về việc áp dụng chưng cất tri thức để tạo ra một phiên bản nhỏ hơn và nhanh hơn của BERT. TinyBERT bao gồm một quy trình chưng cất tri thức hai bước: chưng cất tổng quát trên một tập dữ liệu quy mô lớn và chưng cất cụ thể nhiệm vụ trên một tập dữ liệu nhiệm vụ mục tiêu. Phương pháp này tạo ra một mô hình nhỏ hơn 7.5 lần và nhanh hơn 9.4 lần so với BERT-base
trong khi duy trì hiệu suất cạnh tranh.
Touvron et al. [Touvron et al., 2021] đề xuất Data-efficient Image Transformers (DeiT), là
một mô hình Transformer cho phân loại hình ảnh. Mặc dù DeiT tập trung vào lĩnh vực thị giác,
các tác giả đã sử dụng chưng cất tri thức từ một mô hình giáo viên mạng nơ-ron tích chập (CNN) để
cải thiện hiệu suất của mô hình học sinh Transformer. Điều này chứng minh tiềm năng của
chưng cất tri thức xuyên phương thức trong việc tạo ra các mô hình Transformer hiệu quả.
Wang et al. [Wang et al., 2020] đề xuất MiniLM, một phiên bản đã chưng cất của mô hình BERT được huấn luyện trước.
MiniLM sử dụng sự kết hợp của chưng cất tri thức tự chú ý và chưng cất tri thức lớp trung gian, nhằm bảo tồn tri thức ngôn ngữ và thông tin cấu trúc của mô hình gốc. MiniLM đạt được sự giảm đáng kể về kích thước mô hình và suy luận nhanh hơn trong khi duy trì hiệu suất cạnh tranh trên các benchmark NLP khác nhau.
Fan et al. [fan] đề xuất LayerDrop, một kỹ thuật chính quy hóa để huấn luyện và chưng cất hiệu quả các mô hình Transformer sâu. LayerDrop huấn luyện một mô hình đơn có thể được cắt tỉa hiệu quả tại thời gian suy luận bằng cách bỏ các lớp, tạo ra một họ các mô hình nhỏ hơn với sự đánh đổi khác nhau
giữa hiệu suất và hiệu quả. Phương pháp này có thể được kết hợp với chưng cất tri thức để
tạo ra các mô hình Transformer hiệu quả hơn nữa.
Gao et al. [Chakraborty et al., 2021] giới thiệu RocketQAv2, một phiên bản đã chưng cất của T5 (Text-to-Text
Transfer Transformer) cho các nhiệm vụ hỏi đáp miền mở. RocketQAv2 dựa trên mô hình T5 nhưng sử dụng các kỹ thuật chưng cất tri thức để tạo ra một mô hình nhỏ hơn hiệu quả hơn
trong việc phục vụ các ứng dụng thế giới thực.
Những ví dụ này làm nổi bật hiệu quả của chưng cất tri thức trong việc tạo ra các mô hình Transformer nhỏ hơn, nhanh hơn, và
hiệu quả hơn trong khi duy trì hiệu suất cạnh tranh trên các nhiệm vụ NLP và
thị giác khác nhau.
3 Phương pháp
Chưng cất tri thức là một kỹ thuật được sử dụng trong học máy để chuyển giao tri thức từ một
mô hình lớn hơn, phức tạp hơn (gọi là mô hình giáo viên) đến một mô hình nhỏ hơn, đơn giản hơn (gọi là mô hình học sinh). Mục tiêu là tạo ra một mô hình học sinh nhẹ, hiệu quả về mặt tính toán mà giữ lại
càng nhiều hiệu suất của mô hình giáo viên càng tốt. Điều này đặc biệt hữu ích cho việc triển khai
3

--- TRANG 4 ---
các mô hình học máy trên các thiết bị hạn chế tài nguyên hoặc trong các tình huống mà thời gian suy luận và
tiêu thụ năng lượng là quan trọng.
Phương pháp chưng cất tri thức bao gồm việc huấn luyện một mô hình nhỏ hơn, tập trung hơn để sao chép
kết quả của một mô hình ngôn ngữ lớn hơn, rộng hơn, như GPT-3 [Brown et al., 2020]. Tri thức biểu tượng cấp cao của mô hình lớn hơn nên được giảm xuống thành một mô hình nhỏ hơn, hiệu quả hơn có thể thực hiện chính xác các nhiệm vụ cụ thể. Phương pháp này bao gồm việc huấn luyện một mô hình Transformer học sinh sử dụng chưng cất tri thức từ một mô hình giáo viên lớn hơn. Mô hình giáo viên cung cấp nhãn mềm
cho dữ liệu huấn luyện, được sử dụng để huấn luyện mô hình học sinh. Điều này cho phép mô hình học sinh học từ biểu diễn phong phú của mô hình giáo viên trong khi hiệu quả hơn do kích thước nhỏ hơn của nó [Freitag et al., 2017]. Trong các thí nghiệm của chúng tôi, chúng tôi đã sử dụng mô hình được giới thiệu bởi Gholami và Omar
[2023] (GPT-Efficio) làm giáo viên.
Ở đây chúng tôi nghiên cứu một phương pháp cụ thể gọi là quy trình chưng cất ẩn dụ (FD) bao gồm ba
thành phần chính: một mô hình giáo viên, một mô hình học sinh, và một tập hợp các quy tắc. Mô hình giáo viên là
mô hình ngôn ngữ lớn hơn, tổng quát hơn, trong khi mô hình học sinh là mô hình nhỏ hơn, chuyên biệt hơn
đang được huấn luyện. Các quy tắc định nghĩa các ràng buộc và mối quan hệ giữa các khái niệm khác nhau trong
lĩnh vực được mô hình hóa. Có ba bước trong quy trình FD. Mô hình học sinh được huấn luyện đầu tiên sử dụng
một tập hợp các ví dụ huấn luyện được tạo ra bởi mô hình hướng dẫn. Để tăng cường hiệu suất của mô hình học sinh, một tập dữ liệu cụ thể nhiệm vụ được sử dụng để tinh chỉnh nó. Đầu ra của mô hình học sinh sau đó được tinh chỉnh thêm sử dụng các quy tắc biểu tượng để đảm bảo nó tuân thủ các hạn chế và mối quan hệ
được chỉ định bởi các quy tắc.
Cả học có giám sát và không giám sát đều có thể được bao gồm trong quy trình FD. Mô hình giáo viên
tạo ra một tập hợp các ví dụ có nhãn trong môi trường có giám sát, sau đó được sử dụng cho
việc huấn luyện mô hình học sinh. Trong một kịch bản không giám sát, mô hình học sinh được huấn luyện sử dụng các phương pháp
như học tương phản sử dụng một tập hợp các ví dụ không nhãn mà mô hình giáo viên đã tạo ra.
Mục tiêu của chưng cất ẩn dụ là học một mô hình nhỏ hơn, chuyên biệt hơn, fS, từ một tập dữ liệu
của các cặp đầu vào/đầu ra, D= (x1,y1),...,(xn,yn), trong đó xi là một chuỗi đầu vào, và yi là một chuỗi
đầu ra biểu tượng. Một mô hình ngôn ngữ tổng quát được huấn luyện trước, fG, có thể thực hiện cùng một nhiệm vụ như fG nhưng
với ít tham số hơn. Chúng tôi đề xuất một kỹ thuật chưng cất ẩn dụ để hoàn thành mục tiêu này, bao gồm
việc huấn luyện mô hình nhỏ hơn, fS, sử dụng sự kết hợp của học có giám sát và không giám sát. Đối với
chưng cất ẩn dụ, hàm mất mát như sau:
Hàm mất mát cho chưng cất tri thức, bao gồm sự kết hợp của mất mát entropy chéo
với nhãn thực và mất mát KL-divergence giữa đầu ra của giáo viên và học sinh, thường
được viết như:
L=α×CE(y,Student (x))+(1−α)×T2×KL(Student(x
T)||Teacher(x
T)) (1)
Trong đó L là hàm mất mát tổng, α là một yếu tố trọng số xác định sự cân bằng giữa
mất mát gốc và mất mát chưng cất, CE là hàm mất mát entropy chéo, y là nhãn thực,
Student(x) là dự đoán của mô hình học sinh, T là tham số nhiệt độ được sử dụng để làm mềm xác suất, KL là phân kỳ Kullback-Leibler, và Teacher(x) là dự đoán của mô hình giáo viên.
Phân kỳ KL và entropy chéo đều được đo trên tất cả các lớp, và chúng ta đang tổng hợp
các phép đo này để có được một mất mát vô hướng.
Phân kỳ Kullback-Leibler (KL) là một thước đo về mức độ một phân bố xác suất phân kỳ
khỏi một phân bố xác suất thứ hai, được mong đợi. Đối với các phân bố xác suất rời rạc P và Q, phân kỳ KL được định nghĩa như:
KL(P||Q) =/summationdisplay
P(i)×log(P(i)
Q(i)) (2)
trong đó tổng là trên tất cả các sự kiện có thể i, P(i) là xác suất của sự kiện i dưới phân bố P,
và Q(i) là xác suất của sự kiện i dưới phân bố Q.
Đối với các phân bố liên tục, tổng được thay thế bằng một tích phân trên tất cả các kết quả có thể.
4

--- TRANG 5 ---
4 Thí nghiệm
Trong phần này chúng tôi trình bày kết quả của từng phương pháp trong bối cảnh mô hình ngôn ngữ
(tức là các nhiệm vụ hoàn thành) và hỏi đáp.
4.1 Kết quả
Phần này điều tra các kỹ thuật chưng cất tri thức để nén các mô hình lớn thành các mô hình nhỏ hơn
trong khi giữ lại một phần tốt của hiệu suất của mô hình gốc. Một số tác động của
việc sử dụng chưng cất tri thức trên một mô hình transformer là:
1. Hiệu quả được cải thiện: Một mô hình đã chưng cất nhỏ hơn có ít tham số hơn và do đó yêu cầu
ít tài nguyên tính toán hơn cho suy luận. Điều này giúp triển khai mô hình trên
các thiết bị có tài nguyên hạn chế, chẳng hạn như thiết bị di động.
2. Tốc độ: Mô hình nhỏ hơn cũng nên nhanh hơn, cả về thời gian huấn luyện và suy luận,
so với mô hình lớn hơn gốc.
3. Hiệu suất: Nói chung, mô hình đã chưng cất sẽ hoạt động kém hơn mô hình lớn hơn gốc,
nhưng tốt hơn một mô hình có kích thước tương tự được huấn luyện từ đầu [Du et al., 2017]. Mục tiêu là giữ lại càng nhiều hiệu suất càng tốt với các ràng buộc về kích thước mô hình.
4. Tính mạnh mẽ: Trong một số trường hợp, chưng cất tri thức cũng có thể tăng tính mạnh mẽ của
mô hình và khả năng khái quát hóa của nó, vì nó học cách bắt chước dự đoán của giáo viên trên một
loạt rộng các mẫu chứ không chỉ là sự thật cơ bản.
Tuy nhiên, điều quan trọng cần lưu ý là những lợi ích này phụ thuộc vào đặc điểm của nhiệm vụ, kiến trúc
của các mô hình giáo viên và học sinh, và quy trình huấn luyện được sử dụng. Chưng cất được cấu hình kém
có thể dẫn đến một mô hình hoạt động không tốt hơn hoặc thậm chí kém hơn một mô hình có cùng kích thước được huấn luyện
từ đầu.
Bảng 1: Hiệu suất của phương pháp chưng cất tri thức trên các nhiệm vụ hoàn thành
Mô hình nparams LAMBADA
(acc)LAMBADA
(ppl)StoryCloze
(acc)HellaSwag
(acc)
GPT-3 Zero-Shot 175B 76.2 3.00 83.2 78.9
GPT-3 One-Shot 175B 72.5 3.35 84.7 78.1
GPT-3 Few-Shot 175B 86.4 1.92 87.7 79.3
GPT-Efficio (giáo viên) 950M 67.1 9.2 80.5 72.6
GPT-Efficio (học sinh) 320M 52.47 13.53 61.28 63.52
Bảng 1 chứng minh hiệu suất của các mô hình giáo viên và học sinh GPT-Efficio so với
GPT-3.
Bảng 2: Hiệu suất của phương pháp chưng cất tri thức trên các nhiệm vụ QA
Mô hình nparams NQ WebQ TriviaQA
GPT-3 Zero-Shot 175B 14.6 14.4 64.3
GPT-3 One-Shot 175B 23.0 25.3 68.0
GPT-3 Few-Shot 175B 29.9 41.5 71.2
GPT-Efficio (giáo viên) 950M 27.5 40.6 69.2
GPT-Efficio (học sinh) 320M 19.61 30.52 53.61
Bảng 2 cho thấy hiệu suất của các mô hình giáo viên và học sinh GPT-Efficio so với GPT-3.
5 Phân tích
Trong bối cảnh chưng cất tri thức, có một số siêu tham số chính có thể tác động đến
hiệu suất của mô hình học sinh:
5

--- TRANG 6 ---
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-Shot
GPT-Efficio (giáo viên) GPT-Efficio (học sinh)00.20.40.60.81
0.760.730.86
0.67
0.520.83 0.850.88
0.81
0.610.79 0.78 0.79
0.73
0.64Độ chính xácLAMBADA acc StoryCloze acc HellaSwag acc
Hình 1: Hiệu suất của phương pháp chưng cất tri thức trên các nhiệm vụ hoàn thành
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-Shot
GPT-Efficio (giáo viên) GPT-Efficio (học sinh)020406080100
14.62329.927.5
19.61
14.425.341.5 40.6
30.5264.36871.269.2
53.61Độ chính xácNQ WebQ TriviaQA
Hình 2: Hiệu suất của phương pháp chưng cất tri thức trên các nhiệm vụ QA.
6

--- TRANG 7 ---
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-Shot
GPT-Efficio (giáo viên) GPT-Efficio (học sinh)204060Độ chính xácHiệu suất của phương pháp chưng cất tri thức trên các nhiệm vụ QA
NQ
WebQ
TriviaQA
Hình 3: Hiệu suất của phương pháp chưng cất tri thức trên các nhiệm vụ QA.
1. Nhiệt độ ( T): Nhiệt độ là một tham số trong hàm softmax được sử dụng để
"làm mềm" đầu ra của các mô hình giáo viên và học sinh trong quá trình chưng cất.
Nhiệt độ cao hơn dẫn đến phân bố xác suất mềm hơn trên các lớp, làm cho
quá trình chưng cất hiệu quả hơn bằng cách làm nổi bật mối quan hệ giữa các
lớp khác nhau. [Hinton et al., 2015]. Tuy nhiên, đặt nhiệt độ quá cao có thể dẫn đến làm mềm quá mức và dẫn đến mất thông tin có giá trị.
2. Hệ số Chưng cất ( α): Đây là trọng số được gán cho mất mát gốc (thường là
mất mát entropy chéo với nhãn thực) trong hàm mất mát kết hợp. Sự cân bằng
giữa mất mát gốc này và mất mát chưng cất (phân kỳ KL giữa đầu ra của giáo viên
và học sinh) là rất quan trọng. Nếu α được đặt quá cao, học sinh có thể tập trung quá nhiều
vào việc khớp nhãn thực và không đủ vào việc học từ dự đoán của giáo viên. [Zagoruyko and Komodakis, 2016].
3. Kiến trúc Mô hình: Mặc dù không thực sự là một siêu tham số của quá trình chưng cất,
kiến trúc của mô hình học sinh có thể tác động đáng kể đến hiệu suất của nó. Nếu
mô hình học sinh quá nhỏ, nó có thể không có đủ khả năng để học từ giáo viên một cách hiệu quả. Ngược lại, nếu nó quá lớn, những lợi ích của chưng cất (như tăng hiệu quả
và tốc độ) có thể không được hiện thực hóa.
4. Tốc độ Học và Các Tham số Huấn luyện Khác: Như với bất kỳ mô hình học máy nào,
tốc độ học và các tham số huấn luyện khác có thể tác động đáng kể đến hiệu suất của
mô hình học sinh.
5. Số Epoch Chưng cất: Số epoch mà học sinh được huấn luyện để khớp với
đầu ra của giáo viên cũng có thể ảnh hưởng đến hiệu suất. Quá ít epoch có thể dẫn đến underfitting,
trong khi quá nhiều có thể dẫn đến overfitting.
Như mọi khi, những siêu tham số này có thể cần được điều chỉnh tùy thuộc vào đặc điểm của nhiệm vụ,
tập dữ liệu, và kiến trúc của các mô hình giáo viên và học sinh.
Trong phần này chúng tôi điều tra tác động của Nhiệt độ ( T) đối với mô hình học sinh. Tham số nhiệt độ T trong chưng cất tri thức đóng vai trò quan trọng trong việc kiểm soát "độ sắc nét" của
phân bố xác suất đầu ra bởi mô hình giáo viên.
7

--- TRANG 8 ---
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-ShotGPT-Efficio T- GPT-Efficio T1 GPT-Efficio T2020406080100
76.2
72.5
86.4
67.1
52.47
49.483.2
84.7
87.7
80.5
61.28
59.2978.9
78.1
79.3
72.6
63.52
60.59Độ chính xácLAMBADA (acc) StoryCloze (acc) HellaSwag (acc)
Hình 4: Phân tích tác động của siêu tham số T trên các nhiệm vụ hoàn thành
Trong bối cảnh mô hình ngôn ngữ, nếu T thấp (gần 1 hoặc ít hơn), xác suất đầu ra của mô hình giáo viên sẽ "sắc nét" hoặc "nhọn" hơn, có nghĩa là mô hình sẽ gán xác suất cao cho một vài từ được chọn và xác suất rất thấp cho phần còn lại. Điều này có thể khiến mô hình học sinh khó học các hành vi tinh tế hơn từ mô hình giáo viên vì gradient của hàm mất mát trở nên thưa thớt và việc học chậm lại.
Mặt khác, nếu T cao, xác suất đầu ra của mô hình giáo viên trở nên đồng đều hoặc "mềm" hơn. Điều này có nghĩa là mô hình gán xác suất phân bố đều hơn trên một tập lớn hơn các từ. Điều này có thể có lợi trong các trường hợp có nhiều câu trả lời đúng, vì nó khuyến khích mô hình học sinh xem xét một loạt rộng hơn các khả năng thay vì chỉ tập trung vào một câu trả lời đúng duy nhất. Về cơ bản, nó cung cấp một tập thông tin phong phú hơn cho mô hình học sinh trong quá trình huấn luyện.
Tuy nhiên, đặt T quá cao cũng có thể có những nhược điểm. Nếu xác suất đầu ra của giáo viên trở nên quá đồng đều, mô hình học sinh có thể gặp khó khăn trong việc xác định các từ có khả năng nhất và ít khả năng nhất. Điều này có thể dẫn đến underfitting, nơi mô hình học sinh trở nên kém chính xác hơn vì nó ít tự tin hơn trong các dự đoán của mình.
Vậy, nhiệt độ T nên được đặt sao cho cân bằng nhu cầu cho mô hình học sinh học các hành vi tinh tế từ mô hình giáo viên trong khi cũng đảm bảo mô hình học sinh có thể phân biệt
giữa các dự đoán có khả năng cao và thấp hơn. Việc tinh chỉnh tham số này có thể yêu cầu một số thử nghiệm hoặc xác thực trên một tập dev riêng.
Bảng 3: Phân tích tác động của siêu tham số T trên các nhiệm vụ hoàn thành
Mô hình T n params LAMBADA
(acc)LAMBADA
(ppl)StoryCloze
(acc)HellaSwag
(acc)
GPT-3 Zero-Shot - 175B 76.2 3.00 83.2 78.9
GPT-3 One-Shot - 175B 72.5 3.35 84.7 78.1
GPT-3 Few-Shot - 175B 86.4 1.92 87.7 79.3
GPT-Efficio (giáo viên) - 950M 67.1 9.2 80.5 72.6
GPT-Efficio (học sinh) 1 320M 52.47 13.53 61.28 63.52
GPT-Efficio (học sinh) 2 320M 49.40 14.69 59.29 60.59
Bảng 3 chứng minh hiệu suất của các mô hình giáo viên và học sinh GPT-Efficio với các giá trị T khác nhau
so với GPT-3.
8

--- TRANG 9 ---
Bảng 4: Phân tích tác động của siêu tham số T trên các nhiệm vụ QA
Mô hình T n params NQ WebQ TriviaQA
GPT-3 Zero-Shot - 175B 14.6 14.4 64.3
GPT-3 One-Shot - 175B 23.0 25.3 68.0
GPT-3 Few-Shot - 175B 29.9 41.5 71.2
GPT-Efficio (giáo viên) - 950M 27.5 40.6 69.2
GPT-Efficio (học sinh) 1 320M 19.61 30.52 53.61
GPT-Efficio (học sinh) 2 320M 17.19 27.87 48.50
GPT-3 Zero-Shot GPT-3 One-Shot GPT-3 Few-ShotGPT-Efficio T- GPT-Efficio T1 GPT-Efficio T2020406080100
14.6
23
29.9
27.5
19.61
17.1914.4
25.3
41.5
40.6
30.52
27.8764.3
68
71.2
69.2
53.61
48.5Độ chính xácNQ WebQ TriviaQA
Hình 5: Phân tích tác động của siêu tham số T trên các nhiệm vụ QA
Bảng 4 cho thấy hiệu suất của các mô hình giáo viên và học sinh GPT-Efficio với các
giá trị T khác nhau so với GPT-3.
6 Hạn chế
Mặc dù chưng cất tri thức là một công cụ mạnh mẽ, nó không phải là không có những hạn chế và thách thức. Dưới đây
là một số điều cần xem xét:
1. Khoảng cách Hiệu suất: Hiệu suất của mô hình học sinh đã chưng cất, mặc dù tốt hơn một
mô hình có kích thước tương tự được huấn luyện từ đầu, thường không đạt được mức hiệu suất
của mô hình giáo viên lớn hơn. Thường có một sự đánh đổi giữa kích thước mô hình và độ chính xác.
2. Phụ thuộc vào Mô hình Giáo viên Tốt: Hiệu quả của chưng cất tri thức phụ thuộc rất nhiều
vào chất lượng của mô hình giáo viên. Mô hình học sinh chỉ có thể tốt bằng mức mà
mô hình giáo viên cho phép. Nếu mô hình giáo viên không được huấn luyện tốt hoặc không hoạt động tốt, mô hình học sinh cũng khó có thể hoạt động tốt.
3. Độ nhạy cảm với Siêu tham số: Quá trình chưng cất tri thức đòi hỏi việc điều chỉnh cẩn thận
một số siêu tham số, chẳng hạn như tham số nhiệt độ và trọng số giữa
mất mát gốc và mất mát chưng cất. Việc tìm cài đặt phù hợp có thể khó khăn và có thể
yêu cầu rất nhiều thử nghiệm.
4. Chi phí Tính toán: Mặc dù mô hình học sinh nhỏ hơn và hiệu quả hơn,
quá trình chưng cất chính nó yêu cầu mô hình giáo viên tạo ra dự đoán cho dữ liệu,
điều này có thể tốn kém về mặt tính toán, đặc biệt đối với các mô hình lớn và tập dữ liệu.
9

--- TRANG 10 ---
5. Quá trình Mờ ám: Quá trình chưng cất tri thức có phần mờ ám và khó
diễn giải. Không phải lúc nào cũng rõ ràng tại sao một mô hình học sinh nhất định học hiệu quả từ một mô hình giáo viên, hoặc tại sao các siêu tham số nhất định hoạt động tốt hơn những cái khác.
6. Khả năng Mô hình Học sinh: Cũng có giới hạn về mức độ mà một mô hình học sinh nhỏ hơn có thể
học từ một mô hình giáo viên lớn. Nếu khả năng của mô hình học sinh quá nhỏ, nó có thể không
học hiệu quả tri thức của giáo viên.
7. Nguy cơ Overfitting: Nếu mô hình giáo viên đã overfit với dữ liệu huấn luyện, có thể
mô hình học sinh có thể học những dự đoán overfit này, dẫn đến khái quát hóa kém với dữ liệu mới.
Mặc dù có những hạn chế này, chưng cất tri thức vẫn có thể là một kỹ thuật rất hữu ích, đặc biệt khi
đối phó với các ràng buộc về tài nguyên tính toán hoặc khi triển khai các mô hình trong các ứng dụng thế giới thực
nơi hiệu quả là chìa khóa.
7 Công việc Tương lai
Có một số hướng mà công việc tương lai về chưng cất tri thức có thể theo để cải thiện thêm
kỹ thuật này và ứng dụng của nó trong các lĩnh vực khác nhau:
• Hiểu biết Cải thiện về Động học Chưng cất: Cần thêm nghiên cứu để hiểu
động học của việc chuyển giao tri thức trong quá trình chưng cất. Ví dụ, hiểu
những khía cạnh nào của tri thức của giáo viên được chuyển giao hiệu quả nhất và tại sao có thể
giúp tối ưu hóa quá trình.
• Điều chỉnh Siêu tham số Tự động: Với độ nhạy cảm của quá trình chưng cất đối với các siêu tham số như nhiệt độ và trọng số giữa mất mát gốc và mất mát chưng cất, việc phát triển các phương pháp điều chỉnh siêu tham số tự động hoặc hiệu quả hơn có thể
có lợi.
• Kỹ thuật Chưng cất Tiên tiến: Khám phá các kỹ thuật chưng cất tiên tiến ngoài
phương pháp tiêu chuẩn có thể dẫn đến kết quả tốt hơn. Điều này có thể bao gồm các hàm mất mát mới,
phương pháp huấn luyện, hoặc các loại mối quan hệ giáo viên-học sinh.
• Chưng cất Đa-Giáo viên: Ý tưởng chưng cất tri thức từ nhiều mô hình giáo viên
vào một mô hình học sinh duy nhất là một lĩnh vực thú vị để khám phá. Điều này có thể
kết hợp thế mạnh của các mô hình khác nhau thành một mô hình học sinh hiệu quả duy nhất.
• Thích ứng Cụ thể Lĩnh vực: Thích ứng và tối ưu hóa các kỹ thuật chưng cất tri thức
cho các lĩnh vực hoặc nhiệm vụ cụ thể cũng có thể là một hướng có giá trị cho công việc tương lai. Các
nhiệm vụ khác nhau có thể hưởng lợi từ các chiến lược chưng cất khác nhau.
• Quyền riêng tư và Bảo mật trong Chưng cất: Vì chưng cất bao gồm việc chuyển giao tri thức từ
một mô hình giáo viên, có thể có những lo ngại về quyền riêng tư và bảo mật, đặc biệt khi
mô hình giáo viên đã được huấn luyện trên dữ liệu nhạy cảm. Công việc tương lai có thể xem xét cách đảm bảo
rằng chưng cất không làm rò rỉ thông tin nhạy cảm.
• Hiểu Hạn chế và Thất bại: Thêm nghiên cứu về khi nào và tại sao chưng cất tri thức
thất bại có thể giúp phát triển các phương pháp chưng cất mạnh mẽ và đáng tin cậy hơn.
Lĩnh vực này đang phát triển nhanh chóng và mức độ liên quan của những hướng này có thể thay đổi khi có thêm nghiên cứu được
thực hiện và các kỹ thuật mới hơn được phát triển.
8 Kết luận
Sự gia tăng mạnh mẽ về độ sâu và độ phức tạp của các kiến trúc nơ-ron đã nhấn mạnh nhu cầu cấp thiết
cho các chiến lược triển khai hiệu quả trong các kịch bản thế giới thực. Chưng cất tri thức đã được
chiếu sáng như một ngọn hải đăng trong cuộc tìm kiếm này, cung cấp một phương pháp để khai thác sức mạnh của các mô hình tiên tiến trong các ranh giới dễ quản lý hơn, thân thiện với triển khai. Trong suốt bài báo này, chúng tôi đã mổ xẻ
các khía cạnh phức tạp của kỹ thuật này, từ những tinh tế của việc sử dụng nhãn mềm đến vai trò then chốt
của điều chỉnh nhiệt độ, và làm nổi bật vô số yếu tố quyết định ảnh hưởng đến thành công của
quá trình chưng cất.
10

--- TRANG 11 ---
Tuy nhiên, như với nhiều giải pháp trong lĩnh vực học sâu, chưng cất tri thức không phải là không có
thách thức. Sự tương tác chính xác của động học giáo viên-học sinh, cài đặt siêu tham số tối ưu, và những phức tạp kiến trúc của mô hình học sinh nhấn mạnh bản chất đa chiều
của quá trình. Hơn nữa, sự đánh đổi giữa hiệu quả mô hình và hiệu suất, mặc dù được giảm bớt,
vẫn là một yếu tố cần được điều hướng cẩn thận.
Nhìn về phía trước, rõ ràng là lĩnh vực chưng cất tri thức cung cấp một tấm thảm phong phú của
các hướng nghiên cứu. Khi chúng ta tiếp tục thúc đẩy ranh giới của hiệu suất mô hình, việc theo đuổi đồng thời
hiệu quả trở nên không thể thiếu. Chưng cất tri thức, trong bối cảnh này, đứng như
cả một minh chứng cho những tiến bộ của chúng ta và một biên giới đầy hứa hẹn cho việc khám phá tương lai. Nó bao hàm
bản chất của nghiên cứu học sâu đương đại: sự kết hôn giữa hiệu suất và thực dụng,
nhằm mang các giải pháp AI tiên tiến gần hơn với khả năng ứng dụng thế giới thực.
Tài liệu tham khảo
Ryan Brand, Sia Gholami, Daniel Horowitz, Liutong Zhou, and Sourav Bhabesh. Text classification
for online conversations with machine learning on aws. AWS Machine Learning Blog, 2022.
Eric Brill, Susan Dumais, and Michele Banko. An analysis of the askmsr question-answering system.
InProceedings of the 2002 Conference on Empirical Methods in Natural Language Processing
(EMNLP 2002), pages 257–264, 2002.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,
pages 535–541, 2006.
Soham Chakraborty, Sourav Patel, and Murti V Salapaka. Recovery of power flow to critical infrastructures using mode-dependent droop-based inverters. arXiv preprint arXiv:2102.00046, 2021.
Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading
comprehension. arXiv preprint arXiv:1705.00106, 2017.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A Kalyanpur,
Adam Lally, J William Murdock, Eric Nyberg, John Prager, et al. Building watson: An overview
of the deepqa project. AI magazine, 31(3):59–79, 2010.
Markus Freitag, Yaser Al-Onaizan, and Baskaran Sankaran. Ensemble distillation for neural machine translation. arXiv preprint arXiv:1702.01802, 2017.
Sia Gholami and Saba Khashe. Alexa, predict my flight delay. arXiv preprint arXiv:2208.09921,
2022a.
Sia Gholami and Saba Khashe. Flight delay prediction using deep learning and conversational voicebased agents. American Academic Scientific Research Journal for Engineering, Technology, and
Sciences, 89(1):60–72, 2022b.
Sia Gholami and Mehdi Noori. Zero-shot open-book question answering. arXiv preprint
arXiv:2111.11520, 2021.
Sia Gholami and Mehdi Noori. You don't need labeled data for open-book question answering.
Applied Sciences, 12(1):111, 2022.
Sia Gholami and Marwan Omar. Do generative large language models need billions of parameters?
arXiv preprint arXiv:2309.06589, 2023.
Sia Gholami, Danny Byrd, Francisco Calderon Rodriguez, Muhyun Kim, Yohei Nakayama, Mehdi
Noori, and Nathalie Rauschmayr. Create, train, and deploy a billion-parameter language model
on terabytes of data with tensorflow and amazon sagemaker. AWS Machine Learning Blog, 2022.
11

--- TRANG 12 ---
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.
Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351,
2019.
Nikos Komodakis and Sergey Zagoruyko. Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. In ICLR, 2017.
David Lopez-Paz, Léon Bottou, Bernhard Schölkopf, and Vladimir Vapnik. Unifying distillation
and privileged information. arXiv preprint arXiv:1511.03643, 2015.
Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Roxana Girju, Richard Goodrum,
and Vasile Rus. The structure and performance of an open-domain question answering system. In
Proceedings of the 38th annual meeting of the Association for Computational Linguistics, pages
563–570, 2000.
Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. arXiv preprint arXiv:1802.05668, 2018.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of
bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984,
2020.
Yudong Tao, Rui Ma, Mei-Ling Shyu, and Shu-Ching Chen. Challenges in energy-efficient deep
neural network training with fpga. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition Workshops, pages 400–401, 2020.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and
Hervé Jégou. Training data-efficient image transformers & distillation through attention. In
International conference on machine learning, pages 10347–10357. PMLR, 2021.
Ellen M Voorhees et al. The trec-8 question answering track report. In Trec, volume 99, pages
77–82. Citeseer, 1999.
Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776–5788, 2020.
Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer.arXiv preprint arXiv:1612.03928,
2016.
12

--- TRANG 13 ---
Hình này "Picture9.png" có sẵn trong định dạng "png"
từ:
http://arxiv.org/ps/2310.02421v1
