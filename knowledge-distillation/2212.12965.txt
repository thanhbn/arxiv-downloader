# 2212.12965.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-distillation/2212.12965.pdf
# File size: 5166325 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
BD-KD: Balancing the Divergences for Online
Knowledge Distillation
Ibtihel Amara1, Nazanin Sepahvand1, Brett H. Meyer1, Warren J. Gross1, and
James J. Clark1
McGill University
Abstract. We address the challenge of producing trustworthy and ac-
curate compact models for edge devices. While Knowledge Distillation
(KD) has improved model compression in terms of achieving high accu-
racy performance, calibration of these compact models has been over-
looked. We introduce BD-KD (Balanced Divergence Knowledge Distil-
lation), a framework for logit-based online KD. BD-KD enhances both
accuracy and model calibration simultaneously, eliminating the need for
post-hoc recalibration techniques, which add computational overhead
to the overall training pipeline and degrade performance. Our method
encourages student-centered training by adjusting the conventional on-
line distillation loss on both the student and teacher losses, employing
sample-wise weighting of forward and reverse Kullback-Leibler diver-
gence. This strategy balances student network confidence and boosts
performance. Experiments across CIFAR10, CIFAR100, TinyImageNet,
and ImageNet datasets, and various architectures demonstrate improved
calibration and accuracy compared to recent online KD methods.
Keywords: Knowledge Distillation ·Student-centered training ·Model
Calibration
1 Introduction
A prime factor in the viability of numerous critical edge applications is the fusion
of true reliability with exceptional accuracy. A compelling illustration of this can
bewitnessedintherealmofautonomousvehicles,adomainwheretheimperative
of compactness is principal. In such cases, the deep neural networks responsible
for intricate decision-making processes must be meticulously sized to seamlessly
integrate with electronic chips having restricted computational power and lim-
ited memory capacity. The success of such applications demands maintaining a
balance between compactness, precision, and calibrated predictive confidence for
uncertainty. Much research [3,5,8,9,11,12,34] has focused on compressing large
well-performing deep models with billions of parameters. Among the more suc-
cessful compression techniques is knowledge distillation (KD), a teacher-student
training paradigm. KD is known for its simplicity and versatility as it is archi-
tecture agnostic, with no constraints on the type of architecture of either the
teacher or the student networks. However, KD techniques have been hamperedarXiv:2212.12965v2  [cs.CV]  14 Dec 2024

--- PAGE 2 ---
2 Amara et al.
Fig.1: Distillation losses in the proposed framework. (A) depicts the conven-
tional online distillation loss [38]. (B) depicts the proposed student-centered distillation
loss. The feedback signal from teacher to student takes in a sample-wise weighting of
both forward and backward KL. We also exploit the reverse KL for teacher training.
The parameters in red font detached (stop gradient flow) during training.
by problems that arise when there is a large capacity gap between teacher net-
works and smaller student networks, as was first noted by [20].
The predominant focus of knowledge distillation (KD) research has evolved
around enhancing the student’s accuracy, employing intricate design principles
and multifaceted objective functions. However, insufficient attention has been di-
rected toward the improvement of calibration in the resultant compact student
model. Calibration is associated with the dependability and trustworthiness of
networks [6]. A well-calibrated model is one where where the estimated confi-
dence probabilities reflect the genuine probabilities of the associated outcomes.
For example, should the model provide an answer with a confidence estimate
of 0.95, in a well-calibrated network this would signify that the predicted event
materializes in reality 95% of the time. This alignment between prediction and
actuality lays the foundation for robust and enlightened decision-making, thus
accentuating the model’s reliability and efficacy.
In our study, we build upon recent empirical observations [16,26,38] that
accentuate the efficacy of online distillation in augmenting student model accu-
racy. We focus on obtaining compact student models that are simultaneously
well-performing and better-calibrated. We propose a straightforward learning
framework for efficient online KD training called BD-KD: Balanced divergence
Knowledge Distillation . Our approach exploits the salient principle of collabo-
rative training with a special focus on the student’s learning process, wherein
both the teacher and student networks’ training are orchestrated to engage in a
reciprocal exchange of informative feedback signals via the integration of their
Kullback-Leibler (KL) divergence loss.
Our main contributions are:
–We propose an online KD training strategy with a focus on student model
training where the conventional forward KL loss in the student objective
function is replaced with a sample-wise weighting of both forward and back-
ward KL. We demonstrate that this weighting scheme significantly enhances
the student’s performance via better output uncertainty estimation and cal-
ibration.

--- PAGE 3 ---
Abbreviated paper title 3
–Ourmethodoutperformsstate-of-the-artKDmodels,bothonlineandoffline,
in terms of accuracy, for four different computer vision datasets and across
multiple network architectures.
–Our approach improves student network calibration, compared to both ex-
isting online and offline methods. To the best of our knowledge, we are the
first to report the connection between online distillation and improved model
calibration.
–Our approach can mitigate the capacity gap issue so that compact student
networks profit from larger capacity teachers without performance loss.
The novelty of our paper is most evident in student-focused distillation, which
sculpts and redefines both model compression and calibration paradigms. By
leveraging a confidence-balancing mechanism during distillation training from
an uncalibrated large teacher network, we achieve noteworthy performance. Fur-
thermore, our framework readily integrates with diverse objective functions for
various tasks and applications.
2 Related works
Student-focused Online KD. A prominent contribution in the domain of
student-focusedonlineKDwasintroducedby[27].Theirmethod,calledSwitOKD,
attempts to enhance the student model accuracy by bridging the accuracy gap
during training. This is ensured by strategically slowing down the training pro-
cess of the teacher, enabling the student network to make substantial gains. This
approach shifts the offline and online KD phases, infusing the training process
with a student-centered focus. Similarly, [16] propose a proxy teacher training
paradigm, called SHAKE, where the weights are updated through the use of the
teacher model (offline KD) and the student during online KD training. They
believe that the proxy teacher is encouraged to be aware of the capacity of
the student’s training, which encourages student-centered training. While these
methods set impressive benchmarks for online KD, they do come with some
caveats. SwitOKD prolongs training due to the fact we are slowing down the
training of the teacher network, and SHAKE adds complexity through the use
of a proxy teacher and the proposed overall design. In this paper, we propose
a new online distillation method where, just like the aforementioned methods,
we focus on having student-centered online KD training with the main goal of
achieving high performance with improved calibration. Our new approach in-
volves modification to the online KD objective functions, where both forward
and reverse KL divergence losses are employed.
Symmetric KL Insights. Incorporating both forward and backward KL has
demonstrated robust training and significant performance [1,35]. Certain data-
freeofflineKDapproacheshaveembracedtheJensen-Shannondivergence(JSD),
harnessing forward and reverse KL divergences to synthesize non-redundant im-
ages more effectively. This approach also aids in quantifying teacher-student
discrepancy in data-free scenarios [1]. Lee et al. [15] employed the simple sym-
metricKLbycombiningbothforwardandreverseKLtermsintheirproposedof-

--- PAGE 4 ---
4 Amara et al.
fline self-distillation through dropout. Unlike JSD-based methods, our approach
uniquely balances forward and backward KL divergences based on student-
teacher uncertainty, adapting to mode-seeking and mean-seeking behavior. This
crucialdistinction,whichsetsourworkapartanddynamicallyaddressesstudent-
teacheruncertaintyestimationleadingtosignificantcalibrationandperformance
improvements.
Sample-wise re-weighting in KD. In KD, sample-wise re-weighting of the
KD loss has proven highly efficient for effective distillation performance. Tang et
al. [31] showed that dynamically adjusting student model weights during train-
ing,basedonalogitrescalingfactorderivedfromtheteacher’srelativeprediction
confidence for each sample, significantly improves the student’s overall perfor-
mance. Similarly, Zhou et al. [41] suggested a weighting method for soft labels,
applying smaller weights if the student model outperforms the teacher model for
a given sample. Their experiments demonstrated that sample-wise re-weighting
enhancesdistillationperformance.Suetal.[30]proposedanentropyre-weighting
strategy, employing the teacher’s entropy to re-weight the student KD loss.
Our work differs substantially from the aforementioned sample-wise re-weighting
techniques. Most of these techniques are used in the offline KD setting, utiliz-
ing a single term of KL divergence, usually the forward KL, and applying the
sample-wise weighting factor solely on that term. In contrast, BD-KD harmo-
nizes the contributions of both forward and reverse KL divergences within the
KD loss by using the uncertainty gap (i.e entropy gap) between the large teacher
and the compact student networks as a sample-wise weighting factor.
Calibration in KD. Model calibration has captured the attention of much re-
search [6,21,22,36,40] as it addresses the uncertainty of deep networks given
their empirical accuracy. There are many calibration techniques in the litera-
ture. Some focus on enhancing calibration through extensive data augmenta-
tion, such as mixup [32], while others focus on post-hoc techniques like temper-
ature scaling [6], which add computation overhead to training and reduce per-
formance. Recent research has shown that conventional distillation techniques
encourage the student model to be better calibrated as compared to the same
model trained from scratch [29,36]. Current methods are shifting towards ob-
taining well-calibrated networks via KD. In our work, by adaptive weighting
between the reverse and forward KL to balance the over-confidence and under-
confidence of the compact student in training, making the distillation training
more student-centric, we obtain better-calibrated compact student models with-
out affecting their overall performance.
3 Methodology
Our framework is presented in Figure 1. In the original distillation method of
[10], soft labels obtained from a pre-trained teacher are used to train a student
network. In this form of KD, the objective function used to guide the student’s
training consists of two terms: (1) classic cross-entropy loss, LCE, that minimizes
thedistancebetweenstudent’spredictionandhardlabels,and(2)thedistillation

--- PAGE 5 ---
Abbreviated paper title 5
loss, which consists of the KL divergence, LDistill=LKL, whose goal is to bring
student’s logits closer to that of the teacher.
The type of KL divergence used in most KD approaches is the forward KL :
KL(pt||ps) =P
xpt(x) log
pt(x)
ps(x)
, where ptandpsare the softmax prob-
abilities of the teacher and the student, respectively. One major shortcoming
of forward KL divergence is that it can overestimate the original distribution’s
uncertainty [19,23]. More details about understanding the over-estimation and
under-estimation can be found in the supplemental material. The KL divergence
is asymmetrical. A variant of the forward KL acts in the opposite direction, and
is known as the reverse KL :KL(ps||pt) =P
xps(x) log
ps(x)
pt(x)
. Forward
KL is known to exhibit mean-seeking behavior [19,23]. Looking at its expression,
the quantity inside the log is high when pt(x)is high and ps(x)is low. To avoid
this high value during the minimization process, the mass of psshould be spread
out wherever pt(x)has some mass, which explains the mean-seeking property.
Conversely, reverse KL is known to have a mode-seeking property [19,23]. To
minimize this term, the mass of psshould cover the region where pt(x)is high
making psprioritize covering high modes of ptand ignoring other modes, which
explains the mode-seeking property. We provide in the supplemental materials
details and examples of understanding the effects of mean-seeking and mode-
seeking with respect to the student’s uncertainty. Given these properties of for-
ward and reverse KL, and to ensure better alignment and higher fidelity between
student and teacher softmax distributions, in our proposed method, two major
changes are made in the original KD loss: (1) instead of using the conventional
forward KL in the student’s objective function, we use both forward and reverse
KL, alongside with a sample-wise weighting mechanism that balances the un-
certainty of the student model and (2) reverse KL divergence is used for the
teacher’s objective function (see Eq.3 and Eq.1). The loss functions of the stu-
dent and teacher for our approach are shown in Eq. 1 and Eq. 3:
Ls=αsX
iLCE(ps
i, yi) +Ls
KD(1)
where,
Ls
KD=τ2βsX
iδi,fKL(pt
i,τ||ps
i,τ) +τ2βsX
iδi,rKL(ps
i,τ||pt
i,τ)(2)
and,
Lt=αtX
iLCE(pt
i, yi) +Lt
KD(3)
where,
Lt
KD=τ2βtX
iKL(ˆpt
i,τ||ˆps
i,τ) (4)
where ptandpsare the teacher’s and student’s predictions, respectively and
τis the temperature to soften these probabilities for distillation. irepresents

--- PAGE 6 ---
6 Amara et al.
thei-th samples in the training set. yiis the ground truth label of sample
i, one-hot encoded. We see from Eq. 3 that the teacher’s KL loss is different
than those used in other online methods. In our method, the reverse KL term
used in the teacher objective function acts as a regularizer, which combats the
overconfidence problem commonly observed in large networks.
The Sample-wise Weighting Mechanism. Let’s assume fsandftare the
representations of the student and the teacher respectively, where the teacher’s
logit, zt
i, for the i-th sample ( xi, yi), is:zit=ft(xi, yi). Similarly, we de-
fine the student’s logit as zis=fs(xi, yi). The output of the two networks
is a categorical predictive distribution over the cclasses: ps(yi=j|xi) =
σ(zs,τ
i) = exp( zs
i/τ)/Pc
j=1exp(zs
j/τ)andpt(yi=j|xi) =σ(zt,τ
i) =
exp(zt
i/τ)/Pc
j=1exp(zt
j/τ). The per-sample entropy of the predictive distri-
bution is given as:
Ht(xi, yi) =−cX
j=0pt(yi=j|xi) log( pt(yi=j|xi))
Hs(xi, yi) =−cX
j=0ps(yi=j|xi) log( ps(yi=j|xi)).
These entropy values are used to balance the forward and backward KL terms
of the student’s objective function:
δi,r=1ifHs(xi, yi)−Ht(xi, yi)<0
votherwise
δi,f=vifHs(xi, yi)−Ht(xi, yi)<0
1otherwise
where δf
iandδr
iare respectively the per-sample weights of the forward and
reserve KL loss in Eq.1 and vis a hyper-parameter with values greater than one.
The term Hs(xi, yi)−Ht(xi, yi)measures the difference between student and
teacher entropies per input sample, with negative values indicating a "mode-
seeking" student (underestimation of the teacher’s uncertainties). To address
this, we increase weights on forward KL with hyperparameters δi,fset to v >>
1andδi,r= 1. Conversely, positive values signify a "mean-seeking" student
with higher entropy (overestimation of the teacher’s uncertainty). In this case,
we boost reverse KL weights with δi,f= 1andδi,r=vfor re-adjustments.
We offer a comprehensive and lucid breakdown of our algorithm, presented
in a PyTorch-style format in Algorithm 1 of the supplemental material. Our
code can easily be integrated into a spectrum of distillation frameworks and
across diverse downstream tasks, further enhancing its adaptability and utility.
We intend to make the code associated with our paper publicly available upon
its acceptance for publication.

--- PAGE 7 ---
Abbreviated paper title 7
Table 1: Accuracy (%) comparison on state-of-the art online distillation techniques
and the baseline offline vanilla KD technique. (*) are values provided in [27]. Results
for CIFAR-10, CIFAR-100, and Tiny-ImageNet are taken across three random seeds.
We report the mean and the standard deviation. We observe that BD-KD substantially
improves both student and teacher networks.
Networks Vanilla KD(*) [10] DML(*) [38] KDCL(*) [7] SwitOKD(*) [27] BD-KD (Ours)
CIFAR-10
Student WRN-16-1 91.45 ±0.06 91.96 ±0.08 91.86 ±0.11 92.50 ±0.1792.69±0.18
Teacher WRN-16-8 95.21 ±0.12 95.06 ±0.0595.33±0.1794.76±0.12 94.43 ±0.10
CIFAR-100
Student 0.5MobileNetV2 60.07 ±0.40 66.23 ±0.36 66.83 ±0.05 67.24 ±0.0468.69±0.18
Teacher WRN-16-2 72.90 ±0.09 73.85 ±0.21 73.75 ±0.26 73.90 ±0.4074.28±0.06
Tiny-ImageNet
Student 1.4MobileNetV2 50.98 ±0.32 55.70 ±0.61 57.79 ±0.30 58.71 ±0.1159.05±0.17
Teacher ResNet34 63.18 ±0.37 64.49 ±0.43 65.47 ±0.32 63.31 ±0.0466.27±0.07
Student ResNet20 52.35 ±0.15 53.98 ±0.26 53.74 ±0.39 55.03 ±0.1955.19±0.02
Teacher WRN-16-2 56.59 ±0.22 57.54 ±0.19 57.71 ±0.30 57.41 ±0.0658.29±0.07
ImageNet Top-1
Student ResNet18 69.76 70.81 70.91 71.75 71.75 [90.40 top-5]
Teacher ResNet34 73.27 73.47 73.70 73.65 74.02[91.77 top-5]
4 Experiments and Results
4.1 Experimental Details
In this section, we perform a series of experiments to demonstrate the effec-
tiveness of the proposed online distillation training BD-KD. We compare our
method to state-of-the-art approaches using various network architectures and
different classification datasets.
Datasets. We performed our experiments on four different datasets: CIFAR10,
CIFAR100 [13], Tiny-ImageNet [14], and ImageNet [4]. We use standard data
augmentationssuchasRandomcropandRandomhorizontalflip.Wenormalized
all images channel-wise using the means and standard deviations. In addition
to vanilla KD, we compare our method with recent works on online distillation
such as DML [38], KDCL [7], SwitOKD [26], and SHAKE [16]. We also contrast
our findings with current offline KD methods, such as CRD [33], DKD [39], Fit-
Net [28], etc.
Training Details. We follow the training procedure of previous works [26,33]
and we report the performance on multiple datasets. We mainly use SGD op-
timizer with 0.9 momentum for all datasets. For CIFAR100, the total number
of epochs is set to 240. The initial learning rate is set to 0.01 for most net-
works, and we perform a scheduler on the learning rate. The latter is divided
by 10 every 150th, 180th, and 210th epochs. For CIFAR-10, the total number of
epochs is set to 300. We divide the learning rate by 10 every 100th, 150th, and
200th epoch. For Tiny-ImageNet, the total number of epochs is set to 120 and
the initial learning rate is at 0.01. We set the temperature τfor KD to 2. The

--- PAGE 8 ---
8 Amara et al.
Table 2: Accuracy (%) comparison of student network performances with other KD
techniques using similar architectures (SA) or different architecture (DA). Gain(KD)
and Gain(DML) represent, respectively, the performance improvement over the clas-
sical KD and online KD baseline DML. Most Values are taken from [39] and [16],
and values with (*) are from our own implementation. Results are averaged across 3
different independent runs. We adopted the benchmarking approach outlined in this
source [33]. We put the best performing performance in bold and underlined the second
best.
Same Architecture Different Architecture
Teacher WRN-40-2 R32x4 VGG13 VGG13 R50 R32x4
Student WRN-16-2 R8x4 VGG8 MV2 VGG8 ShV2
Teacher 75.61 79.42 74.64 74.64 79.34 79.42
Student 73.26 72.50 70.36 64.60 70.36 71.82
Vanilla KD [10] 74.92 73.33 72.98 67.37 73.81 74.45
FitNets [28] 73.58 73.50 71.02 64.14 70.69 73.54
CRD [33] 75.48 75.51 73.94 69.7374.30 75.65
DKD [39] 76.24 76.32 74.68 69.71 74.50 *77.07
DML [38] 75.33 74.30 73.64 68.52 74.22 75.71 *
KDCL [7] 74.25 74.03 71.26 65.76 73.03 75.63 *
SHAKE [16] 76.62 77.35 74.8470.03 74.76 76.48 *
CTKD [17] 75.45 74.20 73.22 68.72 73.45 * 75.42
ER-KD [30] 75.69 75.25 74.02 68.95 74.42 * 75.87
ER-CTKD [30] 75.74 75.28 73.69 68.22 74.25 * 76.10
BD-KD (Ours) 76.47 76.6774.96 69.3374.87 76.95
Gain (KD) + 1.55 + 3.34 + 1.98 + 1.96 + 1.06 + 2.5
Gain (DML) + 1.14 + 2.37 + 1.32 + 0.81 + 0.65 + 1.24
chosen value for vis 2 for the adaptive sample-wise weighting. We evaluate the
performance of the models using classification accuracy.
We align our experiments with the common benchmarking practices within the
KD community, ensuring a fair and unbiased evaluation of our method. All
teacher-student pairs examined in our paper were sourced from previous papers
in which comprehensive hyperparameter tuning had been conducted to achieve
optimal results.
4.2 Results
Results on CIFAR-10, 100, and Tiny-ImageNet. Weshowtheexperimen-
tal results on CIFAR-10, CIFAR-100, and Tiny ImageNet and compare BD-KD
torecentonlineandofflineKDmethods.Table1presentsthevalidationaccuracy
in % of recent online KD methods. Overall, our approach gives improvements
across all datasets and all student-teacher pairs. With BD-KD, we observe an
increase between 1% - 3% compared to the online distillation baseline DML [38]
across all datasets and teacher-student pairs. Indeed, a substantial-high in-
crease in student performance with the architecture of mobileNetv2 distilled
from WRN-16-2 on CIFAR-100 is observable. We see a boost of performance by
8% when compared to vanilla KD, by almost 2% when compared to DML and
KDCL, and by 1% compared to SwitOKD. Our method shows a substantial in-
crease in the teacher’s performance as well. For example, for Tiny-ImageNet, we

--- PAGE 9 ---
Abbreviated paper title 9
Table 3: Top-1 and top-5 accuracy (%) on the ImageNet validation set. We used
ResNet50 to be our teacher network and MobileNetV1 as our student.
Acc./Method Vanilla KD [10] CRD [33] ReviewKD [2] DKD [39] AT [37]; [20] RKD [25] DML [38] SHAKE [16] BD-KD (Ours)
Top-1 70.68 71.40 72.56 72.05 70.72 71.32 71.13 72.66 72.97
Top-5 90.30 90.42 91.00 91.05 90.03 90.62 90.22 91.35 90.98
witness an increase of almost 3% in the teacher’s performance when compared to
SwitOKD. This performance gain on both student and teacher can be explained
by the fact that with our method, during training, both networks produce a bet-
ter quality of transferable knowledge via their logits. Also, complicated training
that the teacher endures with techniques like SwitOKD hinders the teacher’s
accuracy. In addition, our teacher is trained with reverse KL (mode-seeking),
which tries to fit the student’s noisy distribution in early epochs. This works as
a regularization on the teacher, avoiding over-fitting and yielding better perfor-
mance than current online KD techniques such as DML, KDCL, and SwitOKD.
To further validate our method, we also compare BD-KD with other offline KD
techniques. Table 2 shows the performance on the test set on CIFAR-100 using
various pairs of teacher-student architectures. Notably, our method exhibits con-
sistentimprovementinallteacher-studentpairsoverthebaselineclassicalvanilla
KDand the baseline onlineKD (e.g.DML).BD-KD achieves comparableor even
better accuracy than other offline or hybrid (switching or combining offline and
online KD) KD-based methods. We notice that with BD-KD, the performance
gain over the baselines is greatest when the teacher-student pair are of similar
architectures. One explanation for this is that with similar architectures, it is
easier for both models to update each other’s weights and optimize accordingly
hence better performance.
Results on ImageNet. Top-1 and top-5 accuracies on ImageNet are reported
in Table 1 and Table 3 for teacher-student pairs ResNet18 - Resnet32 and
Resnet50 - MobilenetV1. Our proposed method BD-KD produces substantially
higher or similar performance when compared to the baseline KD methods
whether online or offline and as well as other KD techniques. For instance, we
achieve a performance gain of 2.29 % when compared to the vanilla KD and
1.84 % compared to DML. We even reached an approximately 0.31% increase
when compared to recent hybrid KD methods such as SHAKE [16], which use
both offline and online KD through a proxy teacher setting. This shows that our
technique is scalable to large-scale datasets.
Mitigating the capacity gap problem. One of the main consequences of the
capacity gap problem in KD is that the student does not benefit from larger
teacher distillation [20]. In Figure 2a, we compare the available online distillation
techniques and observe their behavior with respect to the capacity gap problem.
Predominantly, BD-KD is performing well overall. We observe a steady boost as
we increase our network size. As a matter of fact, with DML, online distillation
has reduced performance when a higher-capacity teacher is used. Similarly, with
SwitOKD, student performance plateaus as we increase the teacher size.

--- PAGE 10 ---
10 Amara et al.
(a)
 (b)
 (c)
Fig.2:(a)Capacity gap Curve ; Student (ResNet20) distilled from different teacher
capacity (WRN-16-2 to WRN-16-8) on CIFAR100. (b) TSNE visualization [18] of
the penultimate feature layer of the student model (WRN-16-1) trained with SwitOKD
on the test images from CIFAR10. (c) TSNE visualization [18] of the penultimate
feature layer of the student model (WRN-16-1) trained with BD-KD on the test images
from CIFAR10.
Table 4: Accuracy with three network setting using online distillation techniques on
CIFAR-100 dataset. (*) are values provided in [27]. The network with (T) or (S) means
it serves as a teacher or a student , respectively. (S/T) means that the network serves
as both a teacher and a student at the same time.
Networks Vanilla(*) DML(*) KDCL(*)SwitOKD(*)
(1T2S)SwitOKD(*)
(2T1S)BD-KD (Ours)
(1T2S)BD-KD(Ours)
(2T1S)
MobileNet 58.65 (S) 63.75 (S) 62.13 (S) 64.62 (S) 65.03 (S) 67.72(S)68.36(S)
WRN-16-2 73.37 (S/T) 74.30 (S/T) 73.94 (S/T) 75.02(S) 71.73 (T) 74.70 (S/T) 74.62 (S/T)
WRN-16-10 79.45 (T) 77.82 (T) 80.71(T) 77.33 (T) 77.07 (T) 79.26 (T) 79.05 (T)
Extension to multiple networks. Similar to DML and SwitOKD, BD-KD
can be extended for multiple networks. Since the loss functions between the
teacher and the student are different, we adopt the same multiple-teacher set-
ting as SwitchOKD. For instance, in a three-network training, there are two
approaches to training the student model: the one-teacher-two-student setting
(1T2S) and the two-teacher-one-student setting (2T1S). As can be seen in Table
4, we achieve a higher accuracy student across all methods using the 2T1S sce-
nario, whereas the student for 1T2S is a little lower. An explanation for this is
that for 2T1S, the compact student learns an ensemble-like and diverse knowl-
edgefrommultipleteachers.Asforourteachernetworks,ourlargestteacherwith
BD-KD achieves comparable results to the vanilla KD at 79.45% and KDCL at
80.71% but much higher than DML and SwitOKD 1T2S and 2T1S. More details
on these multi-network settings are provided in the supplemental material.
Calibration Analyses. To quantify and measure model calibration perfor-
mance, we use the Expected Calibration Error [24]. We also qualitatively eval-
uate calibration with reliability curves (also called calibration curves). Figure
3 shows the calibration curves of a student network ResNet20 distilled from a
WideResNet of depth 16 and width 8 (WRN-16-8) using offline vanilla KD and
online KD methods on CIFAR100. We observe that performing offline distilla-
tion gives an over-confident compact student network as compared to online KD

--- PAGE 11 ---
Abbreviated paper title 11
Table5:Toptable: ComparisonofmodelcalibrationperformancesinECEofstudent
networks with vanilla offline KD and recent online distillation techniques. For ECE,
the lower the better. Bottom table: Expected Calibration Error (ECE) of Student
MobileNet and WRN-16-2 trained with WRN-16-10 teacher on CIFAR-100. Results
show that BD-KD achieves lower ECE overall.
Teacher WRN-40-2 ResNet32x4 VGG13 ResNet50 WRN-16-6
Student WRN-16-2 ResNet8x4 VGG8 VGG8 ResNet20
Vanilla KD 6.45% 5.81% 8.46% 7.21% 11.93%
DML 3.33% 2.60% 7.72% 4.81% 4.79%
SwitOKD 4.36% 2.01% 6.12% 6.49& 4.65%
BD-KD (Ours) 3.15% 2.59% 4.03% 4.27% 2.40%
Networks SwitOKD BD-KD (ours)
MobileNet (trained with 1T2S) 3.4% 2.8%
WRN-16-2 (trained with 1T2S) 3.7% 3.4%
techniques. With offline vanilla KD and by making the student match the con-
fidence of the large pre-trained network, the confidence of the compact network
is easily affected, shifting it to be more over-confident.
(a) Vanilla-KD
ECE =0.1207
(b) DML
ECE =0.04298
(c) SwitOKD
ECE = 0.04108
(d) BD-KD (Ours)
ECE = 0.0252
Fig.3: Calibration curves (ResNet 20 student, teacher WRN-16-8). Online KD
methods improve calibration compared to vanilla KD, and of the tested online methods
BD-KD improves calibration the most.
A solution to mitigate miscalibration during distillation is to opt for online KD
techniques. In Figure 3 (b), (c), and (d) we see that the same compact online
student network is better calibrated than the baseline offline. We see a drop in
the ECE from 0.1207 for offline to 0.0429 with DML, 0.04108 with SwitOKD,
and an even lower drop to 0.0252 using BD-KD.
Table 5 (top table) reports the student expected calibration error ECE on the
test set of CIFAR-100 with different teacher-student pairs and various KD meth-
ods. We observe that BD-KD produces lower values of ECE, meaning that the
resulting compact student is better calibrated. Indeed, we accomplish an ECE
reduction by almost half when distilling WRN-16-6 to ResNet20 and similarly
when distilling VGG13-VGG8 with BD-KD compared to DML and SwitOKD.
This decrease in the ECE metric indicates that the proposed weighting scheme
related to balancing the forward and backward KL during the student’s train-
ing to overcome the over-confidence and under-confidence of the student model
helps in calibrating the student network. Additionally, the obtained student net-

--- PAGE 12 ---
12 Amara et al.
(a)
 (b)
 (c)
Fig.4:(a) Accuracy gap between teacher (ResNet32x4) and student (ShuffleNetV2)
networks on the test set of CIFAR-100 dataset using BD-KD and SwitOKD.(b) Accu-
racy gap between teacher (ResNet50) and student (MBV1) networks on the validation
set of the ImageNet dataset using BD-KD and SwitOKD. (c) Evolution of HtandHs
during training of student (MobileNet V2) and teacher (WRN-16-2) on CIFAR100.
work has more compact class clusters with better class separation (in Figure 2c)
than SwitOKD (in Figure 2b), which makes for better generalization and hence
better calibration. In some cases, BD-KD showed almost similar performance as
SwitOKD. We argue that the optimization process of SwitOKD is sophisticated,
which may lead sometimes to different outcomes of calibration results depending
on the teacher network and the initialization of both networks. Table 5 (bottom
table) shows the ECE of student networks MobileNet and WRN-16-2 trained
simultaneously with one single teacher network WRN-16-10 with SwitOKD and
BD-KD. We observe that our method resulted in a lower ECE error overall (the
lower the better). We observe a higher drop in ECE with the smallest capacity
student network using our method.
Table 6: Accuracy and ECE performances of both student (WRN-16-2) and teacher
(WRN-40-2) networks under different KD setting on CIFAR-100.
KD strategy Accuracy T/S ECE
Vanilla 75.84 75.06 6.45%
Vanilla + BD-KD EQ2 75.84 75.43 4.02%
Vanilla + JS 75.84 75.34 5.1%
DML 78.57 75.81 3.60 %
DML + BD-KD EQ2 78.29 75.89 3.20 %
DML + JS 78.35 75.82 3.51%
SwitOKD 75.84 74.90 4.40 %
SwitOKD + BD-KD EQ2 77.10 75.93 3.21 %
BD-KD (OURS) 78.44 76.26 3.00%
4.3 Analyses and Ablations
BD-KD for offline KD settings? Consistent with earlier discussions, BD-
KD seamlessly integrates into various KD losses, both in offline and online KD
configurations. In Table 6, we provide a comparative analysis of the performance

--- PAGE 13 ---
Abbreviated paper title 13
of student and teacher models trained under the offline KD setting, also known
as vanilla KD. Specifically, in offline KD, we replace the conventional student
loss with that of BD-KD (Eq. 2). Our findings reveal a notable enhancement in
performance with Vanilla KD + BD-KD (75.43% accuracy) compared to vanilla
KD alone (75.06% accuracy).
Utilizing BD-KD across diverse online KD configurations. Wepresent
comparisons in Table 6 between DML alone and DML enriched with the student-
centered loss of BD-KD (Eq. 2), as well as SwitOKD trained with BD-KD’s stu-
dent loss. We observe consistent performance enhancements, including improve-
ments in accuracy and ECE, across student models trained with our approach.
For example, with SwitOKD, we witness an increase from 74.90% to 75.93% in
accuracy, anda reduction in ECE from 4.40%to 3.21%. Thus, BD-KD effectively
improves both the performance and calibration of the compact student model.
JS Divergence vs. Sample-wise Reweighting in BD-KD. In Table 6, we
contrast various KD settings trained with JS divergence and BD-KD. While JS
divergence improves the baselines, BD-KD demonstrates superior performance
overall, in terms of both accuracy and model calibration. This highlights the sig-
nificant advantage of sample-wise weighting across different divergence elements.
Additionally, to have a better understanding of the effect of the sample-wise
weighting of both KL terms with respect to model confidence calibration, we
compare the calibration curves and the ECE values of a student model trained
with sample weighting to that without weighting of the KL divergences in Eq.1.
Figure 5 shows that the student model trained without the proposed sample-
wise weighting scheme for the KL divergences is miscalibrated with an ECE
of 0.0461, whereas BD-KD shows a better-calibrated network with an ECE of
0.0315 (around 1.5% decrease). The reliability diagram of BD-KD shows that
our student became under-confident at low confidence values, but almost per-
fectly calibrated on the important mid-to-high confidence values.
Accuracy and Entropy Gap. WeprovideinFigure4(b)theaccuracygapbe-
tween teacher (ResNet50) and student (MobileNetV1) on the ImageNet valida-
tiondatasetonbothBD-KDandSwitOKD.Similarly,inFigure4(a),wepresent
the accuracy gap between the teacher (ResNet32x4) and student (ShuffleNetV2)
on CIFAR-100 using BD-KD and SwitOKD. We observe that, throughout the
training process, BD-KD can maintain a low training accuracy gap between the
teacher and student networks, which contributes to the fidelity factor and hence
better generalization and calibration. Additionally, this explains why BD-KD is
a student-centered training. By maintaining this gap very low, we make sure that
the student network is able to catch up with the teacher’s knowledge and confi-
dence. We also observe a lower accuracy gap when using our proposed method,
compared to SwitOKD. BD-KD is able to maintain this performance through-
out the training process. Furthermore, we observe a decreasing entropy gap in
Figure 4 (c) throughout the learning process of the networks. Indeed, BD-KD is
designed to maintain a lower uncertainty gap, enhancing student model calibra-
tion, as seen in our experiments.
Sensitivity to v.We vary the hyperparameter vaccording to the following val-

--- PAGE 14 ---
14 Amara et al.
(a)
 (b)
Fig.5:StudentWRN-16-2distilledfromteacherWRN-40-2onCIFAR100.(a)Student
network trained without weighting (i.e. δ1= 1andδ2= 1. (b) Student network
trained using our proposed sample-weighting of the KL terms according to Eq. 3 and
Eq.1.
ues [1,2,3,4] (in Table 7) and record the accuracies of both teacher and student
networks on the test set of CIFAR-100. We see that the optimal choice for vis
2. As we increase the value of this hyperparameter performance degrades. One
must be careful in choosing the appropriate value of vbased on the downstream
task and the architecture of the student since a high value can lead to unnor-
malized gradients and even can cause gradient explosion when vis too high.
Sensitivity to τ.We performed hyperparameter sensitivity analysis on τ, var-
ied its values and observed the accuracies in % of both student and teacher
networks in Table 7. We find that τ= 2is the optimal value. As we increase τ,
we observe a decrease in performance in both networks. This could be linked to
the fact that higher temperatures contribute to maximizing the entropy gap be-
tween the teacher and the student, hence making it very difficult for the student
to follow the teacher.
Table 7: Sensitivity of BD-KD to vandτ
v=1 v=2 v=3 v=4
ResNet20 (S) 71.90 72.0471.70 71.72
WRN-40-2 (T) 77.19 77.31 77.58 77.83Networks τ= 1τ= 2τ= 3τ= 4
Student WRN_16_2 75.51 76.4276.37 76.30
Teacher WRN_16_40 78.00 78.80 79.02 78.92
5 Conclusion
Our proposed approach addresses the challenges of model compression and net-
work reliability in tandem. By dynamically balancing (i.e. sample-wise weight-
ing) the forward and reverse KL in the student’s objective function and using a
reverse KL for teacher distillation, we enhance the student’s accuracy and cal-
ibration surpassing other online distillation techniques. Extensive experiments
validate the effectiveness of our method. Mainly, our method is loss and task-
agnostic, achieves better accuracy and model-calibration performance, making
it easily integrated into other objective functions and onto different tasks.

--- PAGE 15 ---
Abbreviated paper title 15
References
1. Binici, K., Pham, N.T., Mitra, T., Leman, K.: Preventing catastrophic forgetting
anddistributionmismatchinknowledgedistillationviasyntheticdata.In:Proceed-
ings of the IEEE/CVF Winter Conference on Applications of Computer Vision.
pp. 663–671 (2022) 3
2. Chen, P., Liu, S., Zhao, H., Jia, J.: Distilling knowledge via knowledge review.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 5008–5017 (2021) 9
3. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neural
networks with binary weights during propagations. Advances in neural information
processing systems 28(2015) 1
4. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-
scale hierarchical image database. In: 2009 IEEE conference on computer vision
and pattern recognition. pp. 248–255. Ieee (2009) 7
5. Denton, E.L., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R.: Exploiting linear
structure within convolutional networks for efficient evaluation. Advances in neural
information processing systems 27(2014) 1
6. Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neural
networks. In: International conference on machine learning. pp. 1321–1330. PMLR
(2017) 2, 4, 18
7. Guo, Q., Wang, X., Wu, Y., Yu, Z., Liang, D., Hu, X., Luo, P.: Online knowledge
distillation via collaborative learning. In: Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition. pp. 11020–11029 (2020) 7,
8
8. Guo, Y., Yao, A., Chen, Y.: Dynamic network surgery for efficient dnns. Advances
in neural information processing systems 29(2016) 1
9. Han, S., Pool, J., Tran, J., Dally, W.: Learning both weights and connections for
efficient neural network. Advances in neural information processing systems 28
(2015) 1
10. Hinton, G., Vinyals, O., Dean, J., et al.: Distilling the knowledge in a neural net-
work. arXiv preprint arXiv:1503.02531 2(7) (2015) 4, 7, 8, 9
11. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H.,
Kalenichenko,D.:Quantizationandtrainingofneuralnetworksforefficientinteger-
arithmetic-only inference. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 2704–2713 (2018) 1
12. Kim, J., Park, S., Kwak, N.: Paraphrasing complex network: Network compression
via factor transfer. Advances in neural information processing systems 31(2018)
1
13. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny
images (2009) 7
14. Le, Y., Yang, X.: Tiny imagenet visual recognition challenge. CS 231N 7(7), 3
(2015) 7
15. Lee, H., Park, Y., Seo, H., joo Kang, M.: Self-knowledge distillation via dropout.
ArXivabs/2208.05642 (2022) 3
16. Li,L.,Zhe,J.:Shadowknowledgedistillation:Bridgingofflineandonlineknowledge
transfer. In: Thirty-sixth Conference on Neural Information Processing Systems
(NeurIPS) (2022) 2, 3, 7, 8, 9
17. Li, Z., Li, X., Yang, L., Zhao, B., Song, R., Luo, L., Li, J., Yang, J.: Curriculum
temperature for knowledge distillation. In: Proceedings of the AAAI Conference
on Artificial Intelligence. vol. 37, pp. 1504–1512 (2023) 8

--- PAGE 16 ---
16 Amara et al.
18. Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of machine
learning research 9(11) (2008) 10
19. Minka, T., et al.: Divergence measures and message passing. Tech. rep., Technical
report, Microsoft Research (2005) 5
20. Mirzadeh, S.I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., Ghasemzadeh,
H.: Improved knowledge distillation via teacher assistant. In: Proceedings of the
AAAI conference on artificial intelligence. vol. 34, pp. 5191–5198 (2020) 2, 9
21. Mukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P., Dokania, P.: Cal-
ibrating deep neural networks using focal loss. Advances in Neural Information
Processing Systems 33, 15288–15299 (2020) 4
22. Müller,R.,Kornblith,S.,Hinton,G.E.:Whendoeslabelsmoothinghelp?Advances
in neural information processing systems 32(2019) 4, 22
23. Murphy, K.P.: Machine learning: a probabilistic perspective, chap. Variational In-
ference. MIT press (2012) 5
24. Naeini, M.P., Cooper, G., Hauskrecht, M.: Obtaining well calibrated probabili-
ties using bayesian binning. In: Proceedings of the AAAI conference on artificial
intelligence. vol. 29 (2015) 10
25. Park, W., Kim, D., Lu, Y., Cho, M.: Relational knowledge distillation. In: Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 3967–3976 (2019) 9
26. Qian, B., Wang, Y., Yin, H., Hong, R., Wang, M.: Switchable online knowledge
distillation. In: European Conference on Computer Vision. pp. 449–466. Springer
(2022) 2, 7
27. Qian, B., Wang, Y., Yin, H., Hong, R., Wang, M.: Switchable online knowledge
distillation. arXiv preprint arXiv:2209.04996 (2022) 3, 7, 10
28. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets:
Hints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014) 7, 8
29. Stanton, S., Izmailov, P., Kirichenko, P., Alemi, A.A., Wilson, A.G.: Does knowl-
edge distillation really work? Advances in Neural Information Processing Systems
34, 6906–6919 (2021) 4
30. Su, C.P., Tseng, C.H., Lee, S.J.: Knowledge from the dark side: Entropy-
reweighted knowledge distillation for balanced knowledge transfer. arXiv preprint
arXiv:2311.13621 (2023) 4, 8
31. Tang, J., Shivanna, R., Zhao, Z., Lin, D., Singh, A., Chi, E.H., Jain, S.: Under-
standing and improving knowledge distillation. arXiv preprint arXiv:2002.03532
(2020) 4
32. Thulasidasan,S.,Chennupati,G.,Bilmes,J.A.,Bhattacharya,T.,Michalak,S.:On
mixup training: Improved calibration and predictive uncertainty for deep neural
networks. Advances in Neural Information Processing Systems 32(2019) 4
33. Tian, Y., Krishnan, D., Isola, P.: Contrastive representation distillation. arXiv
preprint arXiv:1910.10699 (2019) 7, 8, 9
34. Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J.: Quantized convolutional neural
networks for mobile devices. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 4820–4828 (2016) 1
35. Yin, H., Molchanov, P., Alvarez, J.M., Li, Z., Mallya, A., Hoiem, D., Jha, N.K.,
Kautz, J.: Dreaming to distill: Data-free knowledge transfer via deepinversion.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 8715–8724 (2020) 3
36. Yun, S., Park, J., Lee, K., Shin, J.: Regularizing class-wise predictions via self-
knowledge distillation. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. pp. 13876–13885 (2020) 4

--- PAGE 17 ---
Abbreviated paper title 17
37. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the
performanceofconvolutionalneuralnetworksviaattentiontransfer.arXivpreprint
arXiv:1612.03928 (2016) 9
38. Zhang, Y., Xiang, T., Hospedales, T.M., Lu, H.: Deep mutual learning. In: Pro-
ceedings of the IEEE conference on computer vision and pattern recognition. pp.
4320–4328 (2018) 2, 7, 8, 9
39. Zhao, B., Cui, Q., Song, R., Qiu, Y., Liang, J.: Decoupled knowledge distillation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 11953–11962 (2022) 7, 8, 9
40. Zhong, Z., Cui, J., Liu, S., Jia, J.: Improving calibration for long-tailed recogni-
tion. In: Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. pp. 16489–16498 (2021) 4
41. Zhou, H., Song, L., Chen, J., Zhou, Y., Wang, G., Yuan, J., Zhang, Q.: Rethinking
soft labels for knowledge distillation: A bias-variance tradeoff perspective. arXiv
preprint arXiv:2102.00650 (2021) 4
A Algorithm of the Proposed Method
Illustrated in Algorithm 1, the pseudocode of our BD-KD framework bears re-
semblances to online knowledge distillation, with a pivotal divergence being the
infusion of a balancing mechanism via uncertainty (entropy). Our code can eas-
ily be integrated into a spectrum of distillation frameworks and across diverse
downstream tasks, further enhancing its adaptability and utility.
B Understanding Over-estimation and Under-estimation
of Uncertainty
Fig.6:Understanding over-estimation (left) and under-estimation (right) of uncer-
tainty. We say that the compact student network under-estimates the uncertainty of
the teacher when its prediction entropy is small compared to the teacher’s uncertainty.
Similarly, the student over-estimates the teacher’s uncertainty when its entropy pre-
diction is higher than the teacher’s entropy.
According to Figure 1, When we consider a teacher’s softmax distribution
over the different classes pt(in pink), we can observe two distinct outcomes for
the student’s distribution ps. The outcome hinges on the choice between using

--- PAGE 18 ---
18 Amara et al.
forward or reverse KL divergence. Notably, these choices lead to different esti-
mations of uncertainty. When, employing forward KL, the student’s distribution
ps(in blue) is driven to assign non-zero probabilities across all classes. In con-
trast, reverse KL can result in a psthat primarily encompasses the dominant
mode in pt, allocating the highest probability to a single class while minimizing
probabilities for others. This observation highlights that minimizing forward KL
might generate distributions with greater uncertainty than the original pt, while
minimizing reverse KL could yield the opposite effect.
C Experimental Setup
In this section, we include more details of our experimental setting in our paper.
For BD-KD experiments, we perform a hyperparameter search and we report
the value that leads to the highest performance. In Table 8 below, we provide
particulars of the hyperparameter space considered per dataset type: CIFAR-10,
CIFAR-100, TinyImageNet, and ImageNet.
D Expected Calibration Error and Reliability Curves
In order to evaluate the degree of miscalibration of the student network, we use
Calibration Curves and Expected Calibration Error measures.
Calibration Curves. To assess if a model is well calibrated, we often plot
thecalibration curve , also referred to as reliability diagram [6]. The diagrams
are one way of depicting model calibration where the expected accuracy of the
network is plotted as a function of its confidence (i.e. softmax probabilities). A
"perfectly" calibrated model is represented by the identity function. A deviation
above (under-confidence) or below (over-confidenc) the diagonal is an indication
of a miscalibrated model. To obtain this curve, first, we partition the confidence
estimates into M= 10equal interval bins Im. Then, we proceed to calculate
the accuracy of each bin as follows:
acc(Bm) =1
|Bm|X
i∈Bm1(pi=yi)
where ˆyiandyiare, respectively, the predicted and true class labels of sample
i, and Bmis the set of sample indices whose confidence falls into interval m.
Finally, we compute the average confidence within each bin:
Bm:conf (Bm) =1
|Bm|X
i∈Bmˆpi
where ˆpiis the confidence for sample i.
Expected Calibration Error. In contrast to the calibration curves, which
are visual diagrams capturing the degree of model calibration, the Expected
Calibration Error (ECE) is a function that returns a scalar value measuring

--- PAGE 19 ---
Abbreviated paper title 19
the degree of miscalibration of the model. Similar to the calibration curves,
confidence values are partitioned into M= 10equal bins. ECE is defined as
the weighted average of the difference between the accuracy and confidence at
each bin:
ECE =MX
m=1|Bm|
n|acc(Bm)−conf (Bm)|
E Other Related Calibration Results
(a) Vanilla-KD
 (b) DML
 (c) SwitOKD
 (d) BD-KD (Ours)
Fig.7: Confidence histograms (ResNet 20 student, teacher WRN_16_8) on
CIFAR-100.
(a) Vanilla-KD
ECE =0.119
(b) DML
ECE =0.0479
(c) SwitOKD
ECE = 0.0465
(d) BD-KD (Ours)
ECE = 0.0240
Fig.8: Calibration curves. (ResNet 20 student, teacher WRN_16_6) on CIFAR-
100. Online KD methods improve calibration. Out of the online methods (DML,
SwitOKD), BD-KD improves calibration the most.
Figure 7 shows the confidence calibration (i.e. the distribution of prediction
confidence or probabilities associated with the target labels) of a ResNet20 stu-
dent network trained with various KD techniques (vanilla offline KD (a), DML
(b), SwitOKD (c ), and BD-KD (d) ). With the vanilla offline KD, we observe a
lot of concentration in the highest confidence bins (between 0.9- 1.0) and there is
a huge gap between the accuracy and the expected confidence when compared to
the online KD techniques. For both DML and SwitOKD, we see a slight decrease
in the gap between the accuracy of the student model and the average accuracy.
We witness that BD-KD (ours) resulted in the most significant decrease in the
gap. Indeed, we see that the average confidence of the student ResNet20 matches
very closely the expected confidence. Figure 8, shows the calibration curves with
ResNet20 student with a smaller capacity teacher WRN-16-6 (than WRN-16-
10). Similar to our previous observations, BD-KD leads to a better-calibrated
model overall (the bars are almost aligned along the diagonal).

--- PAGE 20 ---
20 Amara et al.
F Extension to multi-networks (details)
We present the objective functions for both teacher and student(s) under multi-
ple network settings. pt
i,τandps
i,τare the softmax probabilities of the teacher
and the student, respectively.
F.1 One Teacher Two Students (1T2S) loss functions
The student losses in this case are:
Ls1
KD=αs1X
iLCE(ps1
i, yi)
+τ2βs1X
iδi,fs1,tKL(pt
i,τ||ps1
i,τ)
+τ2βs1X
iδi,rs1,tKL(ps1
i,τ||pt
i,τ)
+τ2βs1X
iδi,fs1,s2KL(ps2
i,τ||ps1
i,τ)
+τ2βs1X
iδi,rs1,s2KL(ps1
i,τ||ps2
i,τ)
Ls2
KD=αs2X
iLCE(ps2
i, yi)
+τ2βs2X
iδi,fs2,tKL(pt
i,τ||ps2
i,τ)
+τ2βs2X
iδi,rs2,tKL(ps2
i,τ||pt
i,τ)
+τ2βs2X
iδi,fs2,s2KL(ps1
i,τ||ps2
i,τ)
+τ2βs2X
iδi,rs2,s1KL(ps2
i,τ||ps1
i,τ)
and the teacher loss is:
Lt
KD=αtX
iLCE(pt
i, yi)
+τ2βtX
i(KL(pt
i,τ||ps1
i,τ) +KL(pt
i,τ||ps2
i,τ))

--- PAGE 21 ---
Abbreviated paper title 21
F.2 Two Teachers One student (2T1S) loss functions
The teacher losses, in this case, are defined as follows:
Lt1
KD = αt1X
iLCE(pt1
i, yi) + τ2βt1X
iKL(pt1
i,τ||ps
i,τ)
Lt2
KD = αt2X
iLCE(pt2
i, yi) + τ2βt2X
iKL(pt2
i,τ||ps
i,τ)
and the student loss is:
Ls
KD=αsX
iLCE(ps
i, yi)
+τ2βsX
iδi,fs,t1KL(pt1
i,τ||ps
i,τ)
+τ2βsX
iδi,rs,t1KL(ps
i,τ||pt1
i,τ)
+τ2βsX
iδi,ft2,sKL(ps
i,τ||pt2
i,τ)
+τ2βsX
iδi,rs,t2KL(ps
i,τ||pt2
i,τ)
G T-SNE Visualizations
(a) ResNet32x4
Teacher with
BD-KD
(b) ResNet8x4
Student with
BD-KD
(c) ResNet32x4
Teacher with
SwitOKD
(d) ResNet8x4
Student with
SwitOKD
Fig.9: TSNE visualization of the penultimate feature layer of both teacher
(ResNet32x4) and student (ResNet8x4) networks on CIFAR-100. We randomly picked
and visualized 10 classes.
We provide in Figure 9 the TSNE visualization of the final layer (before the
classifier) for the teacher network ResNet32x4 and student network ResNet8x4
trained on CIFAR-100. We also illustrate in Figure 10 the TSNE visualization
on CIFAR-100 with teacher network VGG13 and student network VGG8. We
randomly picked 10 classes and visualized them. We observe that with BD-KD
the classes are more separable when compared to the same teachers trained with
SwitOKD. Similarly, for the student network, we could see a better separation
of classes using BD-KD (Figure 10) when compared to the same student archi-
tecture.

--- PAGE 22 ---
22 Amara et al.
(a) VGG13 Teacher
with BD-KD
(b) VGG8 Student
with BD-KD
(c) VGG13 Teacher
with SwitOKD
(d) VGG8 Student
with SwitOKD
Fig.10: TSNE visualization of the penultimate feature layer of both teacher (VGG13)
and student (VGG8) networks on CIFAR-100. We randomly picked and visualized 10
classes.
H Frequently Asked Questions
H.1 Why use online distillation to improve calibration?
Online distillation, where both teacher and student networks engage in mutual
training, offers significant advantages over offline methods, as demonstrated by
the SwitOKD paper. In online distillation, the student actively learns from the
teacher and can continuously provide feedback signals to the teacher model. This
interaction enables the teacher to adapt and update its guidance to the student
based on recent information. In contrast, offline distillation typically involves
a one-way transfer. Furthermore, this mutual real-time exchange of feedback
between both models in KD is instrumental for calibration as shown in our
experiments and ablation in the main manuscript. Our findings indicate that
online KD techniques are better calibrated than offline KD. We also have shown
that BD-KD is the best calibrated among online techniques, and simultaneously
provides better performance accuracy.
H.2 The predictive distribution of S matches that of T during
training?
The main objective of balancing the divergences is not to achieve a perfect
match between the predictive distribution of the student network and that of
the teacher network. Instead, it serves two key goals: Firstly, it aims to mitigate
the accuracy gap during training, which is crucial in preventing disparities be-
tween the knowledge and confidence levels of the teacher and student models. To
illustrate this, we included in our main submission visualizations of the behavior
of the accuracy gap between teacher and student models. Additionally, balanc-
ing the divergences helps to align the uncertainty levels between the student and
the teacher networks. This alignment is particularly important for maintaining
a stable and effective knowledge transfer, even when an inherent capacity gap
exists between two networks. These combined objectives enable the student to
avoid aggressive mimicry of the teacher, driven by the substantial differences in
confidence levels. A study [22] has pointed out a noticeable gap between the pre-
dictivedistributionofthestudentandtheteacherinmostdistillationtechniques.

--- PAGE 23 ---
Abbreviated paper title 23
In light of this challenge, our proposed loss function is specifically designed to
tackle this issue. Our loss function strives to bridge the gap making distillation
more reliable.

--- PAGE 24 ---
24 Amara et al.
Algorithm 1: PyTorch-style pseudocode for BD-KD
# x:input image
# y:ground truth
# model_s: student model
# model_t: teacher model
# H_s, H_t: entropy student and teacher
#v: hyper-parameter related to BD-KD
# alpha_s,alpha_t,beta_s,beta_t: hyper parameters related to KD
for (x,y) in Batches:
# values of logits
output_t = model_t(x)
output_s = model_s(x)
# calculate the ce losses
loss_ce_t = beta_t * ce_loss(output_t, y)
loss_ce_s = beta_s * ce_loss(output_s, y)
# calculate the kl loss teacher
loss_kl_t = T*T*alpha_t * kl_div_loss(output_t,
output_s.detach())
# teacher total loss
total_loss_t= loss_ce_t + loss_kl_t
# calculate entropy gap
H_gap = H_s - H_t
# initializing sample weights
delta_reverse = 1
delta_forward = 1
if H_gap ≥0:
delta_reverse = v
else:
delta_forward = v
# calculate the kl loss student
loss_kl_s = T*T*alpha_s *
weighted_kl_div_loss(output_s,output_t.detach(), delta_reverse)
+
weighted_kl_div_loss(output_t.detach(),output_s, delta_forward)
# student total loss
total_loss_s = loss_ce_t + loss_kl_s
# updates
total_loss_t.backward()
total_loss_s.backward()

--- PAGE 25 ---
Abbreviated paper title 25
Table 8: Hyperparameter search space on CIFAR-10, CIFAR-100, TinyImageNet, and
ImageNet. BS: batch size, LR: learning rate, WD: weight decay
Dataset Hyperparameter search space
CIFAR-10 and CIFAR-100 BS={64,128}
LR={0.1,0.01, 0.02, 0.05}
τ={1,2,3,4}
v= {2,3,4}
WD = {1e-4, 5e-4}
αS,αT,βS,βT= {1}
TinyImageNet BS={64,128,256}
LR={0.01, 0.02, 0.05}
τ={2}
v= {2}
WD = {1e-4, 5e-4}
αS,αT,βS,βT= {1}
ImageNet BS={256, 512}
LR={0.1, 0.2}
τ={1,2}
v= {2}
WD = {1e-4, 5e-4}
αS,αT,βS,βT= {1}
