# Class Token và Knowledge Distillation cho Hệ thống Xác minh Người nói Multi-head Self-Attention
Victoria Mingote, Antonio Miguel, Alfonso Ortega, Eduardo Lleida
ViVoLab, Viện Nghiên cứu Kỹ thuật Aragón (I3A), Đại học Zaragoza, Tây Ban Nha.
fvmingote,amiguel,ortega,lleida g@unizar.es

## Tóm tắt
Bài báo này khám phá ba phương pháp tiểu thuyết để cải thiện hiệu suất của các hệ thống xác minh người nói (SV) dựa trên mạng nơ-ron sâu (DNN) sử dụng cơ chế Multi-head Self-Attention (MSA) và các lớp bộ nhớ. Trước tiên, chúng tôi đề xuất việc sử dụng một vector có thể học được gọi là Class token để thay thế cơ chế pooling trung bình toàn cục để trích xuất các embedding. Khác với global average pooling, đề xuất của chúng tôi có tính đến cấu trúc thời gian của đầu vào, điều này có liên quan đến nhiệm vụ SV phụ thuộc văn bản. Class token được nối vào đầu vào trước lớp MSA đầu tiên, và trạng thái của nó ở đầu ra được sử dụng để dự đoán các lớp. Để đạt được độ mạnh mẽ bổ sung, chúng tôi giới thiệu hai phương pháp tiếp cận. Đầu tiên, chúng tôi đã phát triển một ước lượng lấy mẫu mới của class token. Trong phương pháp này, class token được thu thập bằng cách lấy mẫu từ một danh sách các vector có thể huấn luyện. Chiến lược này giới thiệu tính không chắc chắn giúp tổng quát hóa tốt hơn so với một khởi tạo đơn lẻ như được thể hiện trong các thí nghiệm. Thứ hai, chúng tôi đã thêm một token biểu diễn chưng cất để huấn luyện một cặp mạng giáo viên-học sinh sử dụng triết lý Knowledge Distillation (KD), được kết hợp với class token. Token chưng cất này được huấn luyện để bắt chước các dự đoán từ mạng giáo viên, trong khi class token sao chép nhãn thực. Tất cả các chiến lược đã được thử nghiệm trên cơ sở dữ liệu RSR2015-Part II và DeepMine-Part 1 cho SV phụ thuộc văn bản, cung cấp kết quả cạnh tranh so với cùng một kiến trúc sử dụng cơ chế average pooling để trích xuất các embedding trung bình.

**Từ khóa:** Class Token, Teacher-Student Learning, Distillation Token, Speaker Verification, Multi-head Self-Attention, Memory Layers

## 1. Giới thiệu
Hiệu suất trong các nhiệm vụ xác minh người nói (SV) đã được cải thiện đáng kể trong những năm gần đây nhờ vào những tiến bộ của deep learning (DL) trong biểu diễn tín hiệu và các chỉ số tối ưu hóa [1, 2, 3, 4, 5] đã được điều chỉnh từ các hệ thống xác minh khuôn mặt, nhận dạng hình ảnh, hoặc mô hình hóa văn bản tiên tiến. Trong các hệ thống này, Convolutional Neural Network (CNN) hoặc Time Delay Neural Network (TDNN) [2] vẫn là những phương pháp được sử dụng nhiều nhất để thu thập các biểu diễn tín hiệu hoặc embedding. Tuy nhiên, các cơ chế self-attention đang trở thành một phương pháp thống trị trong nhiều lĩnh vực ngoài các nhiệm vụ liên quan đến văn bản. Ví dụ, Transformers [6] đang lan rộng đến nhiều nhiệm vụ [7, 8, 9, 10] nơi các cơ sở dữ liệu quy mô lớn có sẵn. Trong các nhiệm vụ SV, loại kiến trúc này đã bắt đầu được áp dụng thành công trong SV độc lập văn bản [11, 12, 13, 14] nơi không có ràng buộc trong cụm từ được phát âm và các cơ sở dữ liệu lớn có sẵn. Tuy nhiên, trong SV phụ thuộc văn bản, vẫn còn chỗ để cải thiện vì lượng dữ liệu công khai không quá lớn. Bên cạnh đó, SV phụ thuộc văn bản bao gồm việc quyết định liệu một mẫu giọng nói có được phát âm bởi người nói đúng phát âm cụm từ mật khẩu cố định được chọn hay không. Vì vậy, thông tin ngữ âm của tín hiệu có liên quan để xác định danh tính. Do đó, việc giữ cấu trúc thời gian là cần thiết để thu thập các biểu diễn mã hóa chính xác cả thông tin cụm từ và người nói.

Trong bối cảnh của các nhiệm vụ SV phụ thuộc văn bản, các công trình trước đây của chúng tôi [15, 16, 17] đã cho thấy những ưu điểm của việc thay thế cơ chế pooling truyền thống dựa trên việc tính trung bình thông tin thời gian bằng một cơ chế căn chỉnh bên ngoài để thu thập một embedding supervector. Supervector này cho phép giữ cấu trúc thời gian và biểu diễn cả thông tin cụm từ và người nói, nhưng việc căn chỉnh thời gian phải được thực hiện bằng cách sử dụng một phương pháp bên ngoài như một bộ giải mã âm vị, một Gaussian Mixture Model (GMM) [18, 19] hoặc một Hidden Markov Model (HMM) [20]. Như một phương pháp tiếp cận thay thế, trong [21], chúng tôi đã giới thiệu các cơ chế Multi-head Self-Attention (MSA) [6] kết hợp với các lớp bộ nhớ [22] để thay thế các cơ chế căn chỉnh. Việc sử dụng MSA cho phép mô hình tập trung vào các khung hình liên quan nhất của chuỗi để phân biệt tốt hơn giữa các phát âm và người nói. Tuy nhiên, kiến trúc được đề xuất dựa trên MSA đã sử dụng một cơ chế average pooling để thu thập embedding biểu diễn cuối cùng.

Trong công trình này, để thay thế global average pooling, chúng tôi đã giới thiệu một vector có thể học được được gọi là Class token, được kế thừa từ Natural Language Processing (NLP) [7], và gần đây, nhiều hệ thống nhận dạng hình ảnh [8]. Tuy nhiên, phương pháp này chưa được áp dụng cho các nhiệm vụ SV. Để giới thiệu vector này vào hệ thống dựa trên DNN với MSA và các lớp bộ nhớ, class token được nối vào đầu vào trước lớp MSA đầu tiên, và trạng thái ở đầu ra được sử dụng để thực hiện dự đoán lớp. Trong quá trình huấn luyện, thông tin thời gian được mã hóa trong token, và token này tương tác với toàn bộ chuỗi đầu vào thông qua self-attention và học một mô tả toàn cục tương tự như phương pháp supervector [16, 23] vì các đầu múltiple hoạt động như các khe của supervector. Một cơ chế tương tự cũng đã được sử dụng gần đây trong [10]. Do đó, cơ chế average pooling không cần thiết để thu thập một biểu diễn. Các đầu múltiple có thể mã hóa nhiều chi tiết hơn về thứ tự chuỗi so với trung bình, đóng vai trò của các trạng thái và cải thiện kết quả như được hiển thị trong [16], [17] với việc sử dụng các cơ chế căn chỉnh bên ngoài dựa trên HMM và GMM. Ngoài ra, thông tin được mã hóa trong các đầu múltiple này có thể được biểu diễn và phân tích, điều này cải thiện khả năng diễn giải của kết quả của loại phương pháp này.

Để cải thiện hiệu suất thu được với phương pháp class token, chúng tôi cũng giới thiệu một cơ chế lấy mẫu khởi tạo múltiple mới để giảm các vấn đề khởi tạo có thể xảy ra và tạo thêm độ mạnh mẽ chống lại việc thiếu dữ liệu để mô hình hóa các dự đoán. Vì đây là một trường hợp sử dụng trong ngành để phát triển các hệ thống tùy chỉnh cụ thể với các tập dữ liệu trong miền nhỏ và loại phương pháp này có thể là một giải pháp khả thi.

Hơn nữa, công trình này đóng góp với một phương pháp khác dựa trên kiến trúc Transformer và Knowledge Distillation (KD) [24, 9]. Chúng tôi đề xuất một phương pháp giáo viên-học sinh kết hợp với tăng cường dữ liệu Random Erasing [25, 26] cho phép mô hình hóa tính không chắc chắn trong các tham số của một mô hình giáo viên với một mô hình học sinh nhỏ gọn và nhận được các dự đoán đáng tin cậy hơn. Theo ý tưởng được đề xuất trong [9], chúng tôi cũng đã giới thiệu Distillation token trong mạng học sinh để sao chép các dự đoán của mạng giáo viên, trong khi class token được huấn luyện để tái tạo nhãn thực như Hình 3 mô tả. Khác với mục tiêu trong [9], trong công trình của chúng tôi, quá trình chưng cất không nhằm mục đích nén mô hình giáo viên, mà thay vào đó cả hai mô hình được huấn luyện cùng nhau và mô hình học sinh học để nắm bắt tốt hơn tính biến thiên nội tại của các dự đoán giáo viên.

Tóm lại, các đóng góp chính là:

- Chúng tôi thay thế cơ chế global average pooling bằng một class token có thể học được để thu thập một bộ mô tả phát âm toàn cục liên quan đến khái niệm supervector trong xác minh người nói.

- Chúng tôi đề xuất một phương pháp mới dựa trên một phép gần đúng lấy mẫu để ước lượng class token.

- Chúng tôi giới thiệu một kiến trúc giáo viên-học sinh với một token bổ sung được gọi là distillation token được kết hợp với class token để cung cấp độ mạnh mẽ cho mô hình học sinh đã học.

Bài báo này được tổ chức như sau. Trong Phần 2, chúng tôi trình bày tổng quan về MSA và các lớp bộ nhớ. Phần 3 giải thích chiến lược giới thiệu một class token có thể học được sử dụng lấy mẫu. Trong Phần 4, chúng tôi giới thiệu phương pháp dựa trên KD kết hợp với các token được sử dụng để phát triển hệ thống của chúng tôi. Phần 5 mô tả hệ thống được sử dụng. Trong Phần 6, chúng tôi trình bày dữ liệu thí nghiệm, và Phần 7 giải thích các kết quả đạt được. Kết luận được trình bày trong Phần 8.

## 2. Tổng quan về Transformer Encoder

Kiến trúc transformer ban đầu [6] bao gồm hai phần chính: phần encoder và decoder. Tuy nhiên, trong nhiều nhiệm vụ, transformer encoder là phần duy nhất được sử dụng để tạo ra các hệ thống DL. Cơ chế cốt lõi của mỗi khối encoder là lớp Multi-head Self-Attention (MSA) được cấu thành từ nhiều dot-product attention. Vì chúng tôi chỉ sử dụng phần encoder, đầu vào cho cơ chế attention này là giống nhau cho các tín hiệu query, key và value (Q;K;V):

Q_h = x W^Q_h; K_h = x W^K_h; V_h = x W^V_h; (1)

trong đó x là đầu vào cho lớp này, và W^Q_h, W^K_h, W^V_h là các ma trận trọng số có thể học được để thực hiện các phép chiếu tuyến tính. Sau các phép chiếu này, một phép toán softmax được thực hiện trên trục thời gian, cho phép mỗi đầu tập trung vào một số khung hình nhất định của chuỗi đầu vào. Kết quả của phép toán softmax này được gọi là ma trận self-attention cho mỗi đầu và có thể được định nghĩa là:

A_h = softmax_t (Q_h K_h^T / √d_k); (2)

trong đó d_k là số chiều của vector query/key, và T biểu thị chuyển vị. Ma trận self-attention này học thông tin liên quan nhất giữa các dữ liệu khác nhau. Sử dụng thông tin này, các vector đặc trưng giá trị V được tổng hợp để thu thập đầu ra của mỗi đầu. Đầu ra cuối cùng của mỗi đầu có thể được tính toán như sau:

H_h = A_h V_h; (3)

Do đó, MSA được định nghĩa là sự nối của các đầu ra từ mỗi đầu H_h:

MSA(X) = [H_1; H_2 :::H_d_head] W^head; (4)

trong đó X là đầu vào cho lớp attention, W^head là một ma trận trọng số có thể học được để thực hiện một phép chiếu tuyến tính cuối cùng, và d_head là số lượng đầu attention trong lớp thứ h.

Transformer encoder xen kẽ lớp MSA với một lớp thứ hai là lớp feed-forward (FF). Tuy nhiên, trong [21], chúng tôi đã đề xuất việc thay thế các lớp FF bằng các lớp bộ nhớ như trong [22]. Với lớp này, dữ liệu đầu vào được so sánh với tất cả các key sử dụng một product key-attention, và các điểm số thu được được sử dụng để chọn các key gần nhất, có điểm số cao nhất. Sau đó, các vector trọng số liên quan được tính toán với biểu thức sau:

w = softmax_n(x U^K); (5)

trong đó x là đầu vào cho lớp, U^K là ma trận key, và softmax được tính toán trên trục chỉ số bộ nhớ để tập trung vào một số nội dung nhất định của bộ nhớ sẽ được sử dụng để cung cấp đầu ra. Một khi các vector này được thu thập, các trọng số này được kết hợp với các giá trị bộ nhớ của các key được chọn, và đầu ra được nối với đầu ra attention trước đó:

x_out = x + w U^V; (6)

trong đó w là trọng số của các key được chọn thu được với (5), và U^V là các giá trị bộ nhớ liên quan đến các key. Sau khi các khối encoder được áp dụng, một cơ chế average pooling thường được sử dụng để giảm thông tin thời gian và biểu diễn các phát âm có độ dài biến đổi với các vector có độ dài cố định. Tuy nhiên, việc tính trung bình này có thể bỏ qua thứ tự của thông tin ngữ âm, điều này có liên quan đến các nhiệm vụ SV phụ thuộc văn bản.

## 3. Biểu diễn sử dụng Class Token

Trong nhiều nhiệm vụ của NLP và computer vision, kiến trúc transformer sử dụng một vector có thể học được gọi là Class Token (x_CLS), như trong mô hình BERT ban đầu [7] hoặc Vision Transformer (ViT) [8], thay vì một global average pooling. Để sử dụng token này trong transformer encoder, vector được nối vào đầu vào của lớp MSA đầu tiên để thực hiện nhiệm vụ phân loại. Với token này, self-attention bị buộc phải nắm bắt thông tin liên quan nhất với class token để thu thập một biểu diễn như một bộ mô tả phát âm toàn cục tương tự như phương pháp supervector. Thay vì trộn tất cả thông tin với một cơ chế average pooling, cấu trúc thời gian có thể được giữ lại vì cơ chế attention hoạt động như một tổng có trọng số của các token thời gian cho mỗi lớp. Vector đầu ra là sự nối của các subvector đầu khác nhau và mỗi trong số chúng là kết quả của một kết quả attention khác nhau. Do đó, cơ chế có thể được nhìn thấy tương tự như những cơ chế được sử dụng trong công trình trước đây của chúng tôi [16], nơi các đầu đóng vai trò của các trạng thái và supervector trong [23]. Cơ chế supervector cũng tương tự như [27] nhưng trong trường hợp đó, nhiệm vụ là SV độc lập văn bản và các lớp MSA không được sử dụng. Bên cạnh đó, loại cơ chế này cho phép tăng cường khả năng diễn giải những gì mạng nơ-ron học thông qua các lớp self-attention.

Trong [23], cơ chế này để thu thập supervector được định nghĩa tương tự như một supervector GMM thông thường với biểu thức sau:

s_c = (Σ_t x_t w_tc) / (Σ_t w_tc) = Σ_t x_t w̄_tc; (7)

trong đó w_tc là các trọng số thu được bằng một hàm softmax trên đầu ra của một lớp có thể học được, s_c là các vector mỗi trạng thái/thành phần C có chiều D tóm tắt thông tin liên quan dọc theo chuỗi các vector đặc trưng x_t có chiều D, và w̄_tc là các trọng số được chuẩn hóa được định nghĩa là w_tc/Σ_t w_tc. Supervector cuối cùng được xây dựng bởi sự nối của các vector này S = {s_1,...,s_C} và được sử dụng để biểu diễn toàn bộ chuỗi. Trong công trình này, các vector đặc trưng đầu ra cho mỗi đầu H của lớp MSA được thu thập với (3) như một tổng có trọng số tương đương với (7), trong đó w̄_tc tương ứng với các hàng của ma trận trọng số self-attention A_h thu được với (2). Cụ thể, đối với class token, các trọng số được chuẩn hóa sẽ được thu thập từ hàng cuối cùng của A_h. Do đó, class token cuối cùng thu được với cơ chế này là sự nối của các subvector đầu khác nhau tương ứng với vị trí class token, có thể được biểu thị là supervector được trình bày trước đây S_CLS = {s_1_CLS,...,s_H_CLS}.

Để giới thiệu class token vào hệ thống, một tham số vector có thể huấn luyện với chiều của các vector đặc trưng được định nghĩa khi mạng được khởi tạo. Đối với mỗi batch, nó được sao chép và nối vào cuối mỗi chuỗi đặc trưng đầu vào trong batch huấn luyện như một token bổ sung. Do đó, một vector chia sẻ duy nhất được huấn luyện để học biểu diễn embedding cuối cùng.

Trong công trình này, chúng tôi đề xuất việc sử dụng một phương pháp lấy mẫu mới [28], và thay vì có một class token duy nhất được chia sẻ cho toàn bộ batch, chúng tôi giả định tham số nhạy cảm này là kết quả của việc lấy mẫu một danh sách nhiều vector được chọn trong quá trình huấn luyện bằng cách lấy mẫu chúng. Để làm điều đó, chúng tôi định nghĩa một ma trận R vector (Token Matrix) và lấy mẫu nó để lấy một trong số chúng cho mỗi ví dụ trong batch giới thiệu tính không chắc chắn trong class token (CLS Token). Tuy nhiên, việc sử dụng phương pháp này dẫn đến một quá trình đánh giá phức tạp và chậm hơn vì một suy luận lấy mẫu sẽ phải được thực hiện để thu thập các biểu diễn. Vì lý do này, để tránh thực hiện suy luận lấy mẫu, chúng tôi đã lên lịch một sự giảm bắt buộc của các vector có sẵn trong Token Matrix trong suốt quá trình huấn luyện. Do đó, ở cuối quá trình này, chỉ một trọng số khác không, và tham số vector class token được cố định. Chiến lược này cho phép chúng tôi bắt đầu huấn luyện (Iteration 1) với một ma trận nhiều vector để lấy mẫu từ và, dần dần, chúng tôi giảm số lượng vector khi quá trình huấn luyện tiến triển để kết thúc (Iteration N) chỉ với một như class token ban đầu như Hình 1 mô tả. Do đó, việc huấn luyện dẫn hệ thống dần dần tập trung thông tin liên quan vào vector đầu tiên của ma trận. Ngoài ra, sử dụng phương pháp lấy mẫu này, hệ thống được huấn luyện để nắm bắt tính không chắc chắn được giới thiệu bởi việc ban đầu có một Token Matrix với R vector để kết hợp với dữ liệu batch huấn luyện. Do đó, mỗi ví dụ từ batch được kết hợp với một vector ngẫu nhiên từ ma trận được giảm kích thước sau mỗi epoch cho đến khi chỉ còn lại một vector ở cuối, vì vậy phải mô hình hóa nhiều tính biến thiên hơn giúp cải thiện độ mạnh mẽ của hệ thống.

Để thực hiện quá trình này, chúng tôi định nghĩa vector sau, cho biết cho mạng nơ-ron số lượng token có sẵn tại mỗi iteration của quá trình huấn luyện:

α = (α_1,...,α_n,...,α_N); α_n = R,...,1; với R ∈ ℝ (8)

trong đó R là số lượng token được định nghĩa trong ma trận, và N là tổng số iteration cho quá trình huấn luyện. Trong số lượng token có sẵn tại mỗi iteration, một lựa chọn ngẫu nhiên của kích thước batch được thực hiện để chọn chỉ số của các vector. Các vector này được chọn từ phân phối (Token Matrix) và được sử dụng làm class token (CLS Token) trong batch để nối vào đầu vào của lớp MSA đầu tiên. Quá trình tổng thể được mô tả trong Thuật toán 1. Bên cạnh đó, Hình 2 cho thấy một ví dụ đồ họa về cách quá trình lấy mẫu này được thực hiện trong một iteration trung gian (Iteration n). Trong giải thích đồ họa này, có thể quan sát thấy cách trong một iteration trung gian số lượng vector đã được giảm buộc mạng đặt thông tin liên quan để biểu diễn các phát âm trong các vector vẫn có sẵn.

## 4. Knowledge Distillation với Tokens

Được thúc đẩy bởi những lợi ích thu được khi các cơ sở dữ liệu huấn luyện không quá lớn với kiến trúc Teacher-Student dựa trên CNN [26], chúng tôi đã triển khai kiến trúc này sử dụng hai mạng transformer như Hình 3 mô tả. Sử dụng một phương pháp Bayesian tương tự như [29], kiến trúc giáo viên-học sinh cho phép cung cấp độ mạnh mẽ cho hệ thống. Trong phương pháp này, các mạng giáo viên và học sinh được huấn luyện cùng một lúc, khác với các công trình trước đây [30, 31] trong đó mạng giáo viên thường là một mô hình được huấn luyện trước để giảm độ phức tạp. Liệu mạng giáo viên có phải là một mô hình được đông lạnh hay không, các ví dụ huấn luyện tiêu cực thu được giá trị posterior cao trong mạng giáo viên sẽ được học như các ví dụ tích cực bởi mạng học sinh. Bên cạnh đó, các nguồn biến dạng khác nhau được áp dụng cho mỗi tín hiệu đầu vào của cả hai mạng, vì vậy chúng tôi đã sử dụng một phương pháp tăng cường dữ liệu được gọi là Random Erasing (RE) [25] để cung cấp thêm tính biến thiên cho dữ liệu huấn luyện đầu vào. Với loại kiến trúc này, mạng giáo viên phải dự đoán dữ liệu chưa thấy được tăng cường và mạng học sinh cố gắng bắt chước các dự đoán nhãn được tạo ra bởi mạng giáo viên sử dụng đầu ra class token. Chiến lược huấn luyện này cho phép mạng học sinh nắm bắt tính biến thiên trong các dự đoán được tạo ra bởi mạng đầu tiên và mô hình hóa tính không chắc chắn này trong các tham số trong quá trình huấn luyện. Tuy nhiên, được truyền cảm hứng bởi [9], chúng tôi cũng đã bao gồm một token có thể học được bổ sung trong mạng học sinh được gọi là Distillation Token (Distill Token). Việc giới thiệu token bổ sung này cho phép triển khai một tối ưu hóa đa mục tiêu bằng cách sử dụng class token để tái tạo nhãn thực trong khi distillation token được huấn luyện để bắt chước các dự đoán của mạng giáo viên. Để đạt được điều này, tổn thất Kullback-Leibler Divergence (KLD) giữa các phân phối học sinh và giáo viên được giảm thiểu. Tổn thất KLD có thể được công thức hóa như sau:

KLD = Σ^I_{i=1} Σ^J_{j=1} p^T(y^{cls}_{ij}|x_j) log(p^S(y^{dist}_{ij}|x_j)) + const; (9)

trong đó i và j là các chỉ số người nói và phát âm, x_j là tín hiệu đầu vào, p^T(y^{cls}_{ij}|x_j) là xác suất posterior đầu ra của nhãn y^{cls}_i từ class token của mô hình giáo viên, p^S(y^{dist}_{ij}|x_j) là xác suất posterior đầu ra của nhãn y^{dist}_i từ distillation token của mạng học sinh cho cùng một ví dụ, và const được định nghĩa trong [29]. Do đó, để huấn luyện kiến trúc giáo viên-học sinh được hiển thị trong Hình 3, chúng tôi sử dụng hai biểu thức tổn thất sau cho mạng giáo viên và học sinh:

Loss_T = CE(y^{cls}_T, y); (10)
Loss_S = KLD(y^{dist}_S, y^{cls}_T) + CE(y^{cls}_S, y); (11)

trong đó CE là tổn thất cross-entropy, y^{cls}_S là đầu ra class token từ mạng học sinh, và y là các nhãn sự thật cơ bản.

## 5. Mô tả Hệ thống

Trong phần này, chúng tôi mô tả kiến trúc hệ thống được sử dụng trong công trình này cho SV phụ thuộc văn bản. Hình 3 mô tả kiến trúc này nơi một phương pháp giáo viên-học sinh được sử dụng. Cả hai kiến trúc đều tuân theo cấu trúc được mô tả trong [21] với cùng các phần backbone và pooling. Backbone dựa trên hai khối Residual Network (RN) [32] với ba lớp mỗi khối. Ngoài ra, các kiến trúc này cần embedding với thông tin vị trí để giúp hướng dẫn mặt nạ attention trong các lớp MSA. Trong công trình này, các embedding này (e_ph) được trích xuất bởi một mạng phân loại ngữ âm thay vì sử dụng thông tin vị trí thời gian [27]. Đối với phần pooling, hai lớp MSA của 16 đầu kết hợp với hai lớp bộ nhớ được sử dụng. Hơn nữa, trước lớp MSA đầu tiên, class token được nối vào đầu vào. Trong trường hợp của mạng học sinh, distillation token cũng được bao gồm. Nhờ vào cơ chế self-attention, các token này học để thu thập một biểu diễn toàn cục cho mỗi phát âm mà không áp dụng global average pooling. Các biểu diễn này, tương tự như supervector, thuận tiện hơn cho nhiệm vụ SV phụ thuộc văn bản vì các biểu diễn toàn cục này không bỏ qua thứ tự chuỗi và được thu thập tự động nhờ vào cơ chế self-attention. Vì vậy các cơ chế căn chỉnh bên ngoài không cần thiết để thu thập chúng như trong [15, 16, 17], nơi các xác suất posterior GMM hoặc HMM cần thiết để căn chỉnh các khung hình giọng nói với supervector. Bên cạnh đó, việc sử dụng các lớp bộ nhớ tăng lượng kiến thức thu được bởi mạng có thể được lưu trữ. Sau khi huấn luyện hệ thống, một độ tương tự cosine trên các biểu diễn token được áp dụng để thực hiện quá trình xác minh. Lưu ý rằng loại hệ thống này dựa trên giáo viên-học sinh bao gồm việc huấn luyện hai kiến trúc cùng một lúc. Do đó, quá trình này có thể liên quan đến chi phí tính toán cao hơn. Tuy nhiên, trong quá trình suy luận, chỉ mạng học sinh được sử dụng để trích xuất các embedding, vì vậy không có thời gian suy luận bổ sung.

## 6. Thiết lập Thí nghiệm

### 6.1. Tập dữ liệu

Để thí nghiệm, hai tập dữ liệu xác minh người nói phụ thuộc văn bản đã được sử dụng. Tập thí nghiệm đầu tiên đã được báo cáo trên tập dữ liệu xác minh người nói phụ thuộc văn bản RSR2015 [33]. Tập dữ liệu này bao gồm các bản ghi âm từ 157 nam và 143 nữ. Đối với mỗi người nói, có 9 phiên với 30 cụm từ khác nhau. Dữ liệu này được chia thành ba tập con người nói: background (bkg), development (dev) và evaluation (eval). Trong bài báo này, chúng tôi phát triển các thí nghiệm của mình với Part II, được cấu thành từ lệnh điều khiển ngắn với sự chồng chéo mạnh về nội dung từ vựng, và chúng tôi chỉ sử dụng dữ liệu bkg để huấn luyện. Phần eval được sử dụng để đăng ký và đánh giá thử nghiệm. Tập dữ liệu này có ba điều kiện đánh giá, nhưng trong công trình này, điều kiện thách thức nhất, là trường hợp Impostor-Correct, là điều kiện duy nhất đã được đánh giá và sử dụng trong SV phụ thuộc văn bản. Lưu ý rằng có các hệ thống khác thu được kết quả liên quan cho tập dữ liệu này, tương tự như những kết quả được trình bày dưới đây. Tuy nhiên, các hệ thống như vậy dựa trên các mô hình truyền thống như Hidden Markov Models (HMM) [33, 34] hoặc các kiến trúc mạng nơ-ron tập trung vào hai luồng khác nhau cho thông tin người nói và phát âm [35, 36].

Tập dữ liệu thứ hai được sử dụng là cơ sở dữ liệu DeepMine [37]. Corpus này bao gồm ba phần khác nhau trong đó chúng tôi sử dụng các tệp được chọn cho Short-duration Speaker Verification (SdSV) Challenge 2020 [38] từ Part 1. Part 1 là phần phụ thuộc văn bản được cấu thành từ 5 cụm từ tiếng Ba Tư và 5 cụm từ tiếng Anh và chứa 963 người nói nữ và nam. Dữ liệu này được chia thành hai tập con: train với 101.063 tệp âm thanh và evaluation với 69.542 tệp âm thanh. Cuối cùng, mạng phân loại ngữ âm [27] đã được huấn luyện sử dụng LibriSpeech [39] để trích xuất embedding ngữ âm. Khác với các công trình khác được trình bày trong thử thách [40, 41], chúng tôi đã không sử dụng các tập dữ liệu VoxCeleb 1 và 2 [42, 43] trong quá trình huấn luyện mạng nơ-ron. Được thúc đẩy bởi thực tế rằng trong một số tình huống và ứng dụng yêu cầu việc triển khai các hệ thống tùy chỉnh với ít dữ liệu trong miền có sẵn. Vì lý do này, chúng tôi đã phát triển các hệ thống chỉ với dữ liệu trong miền.

### 6.2. Mô tả Thí nghiệm

Để thực hiện các thí nghiệm với tập dữ liệu RSR2015, một tập các đặc trưng được cấu thành từ Mel-Frequency Cepstral Coefficients (MFCC) 20 chiều với các đạo hàm của chúng được sử dụng làm đầu vào. Trong khi đối với các thí nghiệm sử dụng tập dữ liệu DeepMine, chúng tôi đã sử dụng một vector đặc trưng dựa trên các ngân hàng bộ lọc mel-scale. Với bộ trích xuất đặc trưng này, chúng tôi thu được hai ngân hàng bộ lọc log có kích thước 24 và 32, được nối với log energy để thu được một chiều đầu vào cuối cùng là 57. Hơn nữa, embedding ngữ âm 256 chiều đã được sử dụng làm thông tin vị trí. Là trình tối ưu hóa cho các thí nghiệm trong công trình này, trình tối ưu hóa Adam được sử dụng với tốc độ học tăng từ 10^-3 đến 5×10^-3 trong 60 epoch và sau đó giảm từ 5×10^-3 đến 10^-4. Ngoài ra, dữ liệu huấn luyện được đưa vào các hệ thống với kích thước minibatch là 32.

## 7. Kết quả

Trong bài báo này, hai tập thí nghiệm đã được thực hiện để đánh giá các đề xuất với cả hai cơ sở dữ liệu. Chúng tôi so sánh các phương pháp khác nhau để thu thập các biểu diễn với một mạng nơ-ron đơn lẻ sử dụng cùng kiến trúc như mạng giáo viên: việc sử dụng global average pooling truyền thống (AVG), attentive pooling (ATT) và việc giới thiệu class token có thể học được (CLS). Đối với phương pháp class token, chúng tôi đánh giá đề xuất của chúng tôi về việc lấy mẫu một ma trận R vector và giảm nó cho đến khi có một vector duy nhất (Sampling). Tham số này cũng được quét cho các giá trị khác nhau của R, bao gồm R=1 tương ứng với ý tưởng ban đầu có một token duy nhất và lặp lại nó. Hơn nữa, chúng tôi phân tích hiệu ứng được tạo ra bởi thực tế sử dụng một kiến trúc giáo viên-học sinh với một distillation token bổ sung (CLS DIST).

Để đánh giá các thí nghiệm này, chúng tôi đã đo hiệu suất sử dụng ba chỉ số. Equal Error Rate (EER) đo khả năng phân biệt của hệ thống. NIST 2008 và 2010 minimum Detection Cost Functions (DCF08, DCF10) [44, 45] đo chi phí của các lỗi phát hiện theo thuật ngữ tổng có trọng số của các xác suất báo động giả và bỏ sót cho một ngưỡng quyết định, và xác suất tiên nghiệm.

### 7.1. Nghiên cứu Class Token

Một tập thí nghiệm đầu tiên được thực hiện để so sánh việc sử dụng class token để thu thập các bộ mô tả phát âm toàn cục với việc sử dụng phương pháp global average pooling hoặc attentive pooling được đề xuất trong [46]. Do đó, chúng tôi nghiên cứu hai phương pháp để giới thiệu vector này được giải thích trong suốt công trình này và hiệu ứng của số lượng vector được chọn cho phương pháp lấy mẫu.

Bảng 1 trình bày kết quả EER, DCF08 và DCF10 cho các thí nghiệm với tập dữ liệu RSR2015-part II. Bất kể số lượng vector trong việc lấy mẫu cho class token, nếu chúng tôi áp dụng chiến lược được đề xuất của chúng tôi để giới thiệu các token với một phương án lấy mẫu, hiệu suất thu được tốt hơn. Ngoài ra, kết quả cho thấy cách sử dụng token có thể học được vượt trội so với việc sử dụng embedding trung bình hoặc embedding attentive pooling. Lưu ý rằng token được huấn luyện thông qua self-attention và giữ cấu trúc thời gian để thu thập một biểu diễn phát âm toàn cục, trong khi embedding trung bình bỏ qua thông tin này có liên quan đến nhiệm vụ SV. Như chúng ta cũng có thể quan sát với việc quét giá trị R, việc sử dụng nhiều vector để tạo token matrix tốt hơn so với việc sử dụng một vector duy nhất và lặp lại nó cho toàn bộ batch, tương ứng với cách ban đầu áp dụng phương pháp này. Trường hợp có một vector duy nhất và lặp lại nó tương ứng với các thí nghiệm với R=1. Tuy nhiên, khi số lượng token có sẵn quá lớn, hiệu suất bắt đầu giảm. Sự giảm này có thể được gây ra bởi việc giới thiệu quá nhiều tính biến thiên mà hệ thống không thể mô hình hóa vì các kiến trúc được sử dụng không quá lớn, có nghĩa là có một số lượng hạn chế các token khác nhau để thực hiện quá trình huấn luyện.

Trong Bảng 2, kết quả thu được trong cơ sở dữ liệu DeepMine-part 1 được hiển thị. Khác với tập dữ liệu khác, dữ liệu huấn luyện trong DeepMine lớn hơn, điều này cho thấy việc thiếu dữ liệu không quá quan trọng để huấn luyện một hệ thống mạnh mẽ và bền vững. Do đó, việc thay thế embedding trung bình hoặc embedding attentive pooling bằng class token chỉ cải thiện hiệu suất một chút. Bên cạnh đó, việc quét giá trị R cho thấy sự tiến triển của kết quả nữ và nam riêng biệt không theo cùng xu hướng như xảy ra trong kết quả RSR-Part II.

### 7.2. Hiệu ứng của Knowledge Distillation sử dụng Tokens

Trong phần này, chúng tôi phân tích hiệu ứng của việc giới thiệu một phương pháp dựa trên triết lý Knowledge Distillation bao gồm một kiến trúc giáo viên-học sinh. Hơn nữa, trong phương pháp này, một distillation token bổ sung (CLS DIST) được kết hợp [9]. Phương pháp này đã được sử dụng để so sánh hiệu suất thu được trong trường hợp global average pooling cũng như trong phương pháp lấy mẫu được đề xuất để sử dụng class token. Trong trường hợp thứ hai này, chúng tôi đã phát triển kiến trúc giáo viên-học sinh sử dụng giá trị R của cấu hình tốt nhất thu được trong phần trước, và cũng, trường hợp R=1 vì đây là cách thông thường để áp dụng phương pháp class token này trong tài liệu.

Kết quả của các thí nghiệm này trong RSR-Part II được hiển thị trong Bảng 3. Bất kể loại phương pháp để thu thập các biểu diễn được sử dụng, chúng ta có thể quan sát rằng việc sử dụng một kiến trúc dựa trên phương pháp giáo viên-học sinh cải thiện độ mạnh mẽ và đạt được hiệu suất tốt hơn trong tất cả các phương án để trích xuất các biểu diễn. Hơn nữa, hiệu suất tốt nhất được thu thập bằng cách áp dụng chiến lược được đề xuất của chúng tôi để giới thiệu các token với một phương án lấy mẫu với nhiều hơn một vector duy nhất.

Mặt khác, Bảng 4 trình bày hiệu suất của các hệ thống với DeepMine-part 1. Trong trường hợp này, kết quả cho thấy việc áp dụng chỉ kiến trúc giáo viên-học sinh không cải thiện các hệ thống. Tuy nhiên, việc sử dụng kiến trúc giáo viên-học sinh và distillation token bổ sung (CLS DIST), kết hợp với chiến lược lấy mẫu với nhiều token vector cũng cho phép đạt được một hệ thống mạnh mẽ hơn và một cải thiện đáng kể trong kết quả.

### 7.3. Phân tích Biểu diễn Self-Attention Class Token

Trong quan điểm về các kết quả liên quan thu được, chúng tôi cũng đã tiến hành một phân tích để diễn giải những gì ma trận self-attention A đang học trong mỗi hệ thống. Để thực hiện phân tích này, chúng tôi đã sử dụng hệ thống với hiệu suất tốt nhất từ mỗi cơ sở dữ liệu, và trong các hệ thống này, lớp MSA cuối cùng của mô hình học sinh đã được chọn để tạo các biểu diễn. Ngoài ra, chúng tôi đã chọn các phát âm khác nhau để phân tích trong Hình 4 và Hình 5. Đối với mỗi phát âm, ba hình được vẽ: phổ đồ của phát âm, ma trận trọng số attention tương ứng với class token cho mỗi trong 16 đầu của lớp MSA, và tổng của các trọng số của các class token attention này.

Trong Hình 4, hai ví dụ về phát âm của các cụm từ khác nhau ("Call sister", "Call brother") được phát âm bởi cùng một người nói được hiển thị. Các ví dụ này được thu thập từ tập đánh giá của cơ sở dữ liệu RSR-Part II. Liệu chúng ta nhìn vào các hình ở giữa và dưới, chúng ta có thể quan sát thông tin liên quan được học bởi các trọng số self-attention để xác định chính xác cụm từ và người nói của mỗi phát âm sử dụng class token. Lưu ý rằng hai cụm từ ví dụ này bắt đầu chính xác giống nhau với từ Call, vì vậy tập trung vào phần đầu của các hình, chúng ta quan sát cách self-attention đưa ra sự liên quan tương tự trong cả hai trường hợp đến các khu vực của cùng các âm vị. Hơn nữa, chúng ta cũng có thể thấy rằng các trọng số không chú ý đến khu vực ở đầu và cuối của các phát âm tương ứng với các khoảnh khắc im lặng.

Hình 5 biểu diễn hai ví dụ về phát âm của cùng một cụm từ ("OK Google") được phát âm bởi các người nói khác nhau. Trong trường hợp này, các ví dụ được thu thập từ tập đánh giá của cơ sở dữ liệu DeepMine. Lưu ý rằng vì các hình này là của cùng một cụm từ, self-attention tập trung vào cùng các khu vực, nhưng sự liên quan khác nhau được đưa ra cho một số trong số chúng. Bên cạnh đó, hiệu ứng không tập trung vào phần đầu và cuối của phát âm cũng xảy ra trong các ví dụ này.

## 8. Kết luận

Trong bài báo này, chúng tôi đã trình bày một phương pháp tiểu thuyết cho nhiệm vụ SV. Phương pháp này dựa trên việc sử dụng class token có thể học được để thu thập một bộ mô tả phát âm toàn cục thay vì sử dụng average pooling. Hơn nữa, chúng tôi đã phát triển một phương án để tạo class token với một chiến lược lấy mẫu giới thiệu tính không chắc chắn giúp tổng quát hóa tốt hơn. Ngoài phương pháp trước đây, chúng tôi cũng đã sử dụng một kiến trúc giáo viên-học sinh kết hợp với một distillation token bổ sung để phát triển một hệ thống mạnh mẽ hơn. Sử dụng kiến trúc này, distillation token trong mạng học sinh học để sao chép các dự đoán từ mạng giáo viên. Cả hai đề xuất đều được đánh giá trong hai cơ sở dữ liệu SV phụ thuộc văn bản. Kết quả đạt được cho thấy trong RSR2015-part II rằng mỗi phương pháp được giới thiệu để thu thập một hệ thống mạnh mẽ và giảm hiệu suất kém tiềm năng do thiếu dữ liệu cải thiện hiệu suất tổng thể. Tuy nhiên, trong DeepMine-part 1, kết quả thu được chỉ thay thế embedding trung bình bằng class token trình bày một cải thiện nhỏ, trong khi việc sử dụng kiến trúc giáo viên-học sinh đạt được một cải thiện lớn và xác nhận sức mạnh của loại phương pháp này để huấn luyện các hệ thống.

## Tuyên bố Đóng góp Tác giả

**Victoria Mingote**: Khái niệm hóa, Điều tra, Phương pháp học, Phần mềm, Viết - Chuẩn bị bản thảo ban đầu; **Antonio Miguel**: Khái niệm hóa, Điều tra, Phương pháp học, Phần mềm, Giám sát, Viết - Xem xét và Chỉnh sửa; **Alfonso Ortega**: Giám sát, Viết - Xem xét và Chỉnh sửa; **Eduardo Lleida**: Giám sát, Viết - Xem xét và Chỉnh sửa.

## Lời cảm ơn

Dự án này đã nhận được tài trợ từ chương trình nghiên cứu và đổi mới Horizon 2020 của Liên minh Châu Âu theo Grant Marie Skłodowska-Curie 101007666; một phần bởi MCIN/AEI/10.13039/501100011033 và bởi Liên minh Châu Âu "NextGenerationEU"/PRTR theo Grant PDC2021-120846-C41, bởi Bộ Kinh tế và Cạnh tranh Tây Ban Nha và Quỹ Xã hội Châu Âu thông qua grant PRE2018-083312, bởi Chính phủ Aragón (Grant Group T36 20R), và bởi Nuance Communications, Inc.

## Tài liệu tham khảo

[1] Y. Taigman, M. Yang, M. Ranzato, L. Wolf, DeepFace: Closing the Gap to Human-Level Performance in Face Verification, 2014 IEEE Conference on Computer Vision and Pattern Recognition (2014) 1701–1708 doi:10.1109/CVPR.2014.220.

[2] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, S. Khudanpur, X-vectors: Robust dnn embeddings for speaker recognition, in: 2018 ICASSP, pp. 5329–5333.

[3] E. Hoffer, N. Ailon, Deep metric learning using triplet network, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 9370 (2010) (2015) 84–92. doi:10.1007/978-3-319-24261-3_7.

[4] F. Schroff, D. Kalenichenko, J. Philbin, Facenet: A unified embedding for face recognition and clustering, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 815–823.

[5] D. Snyder, P. Ghahremani, D. Povey, D. Garcia-Romero, Y. Carmiel, S. Khudanpur, Deep neural network-based speaker embeddings for end-to-end speaker verification, in: Spoken Language Technology Workshop (SLT), 2016 IEEE, IEEE, 2016, pp. 165–170.

[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin, Attention is all you need, in: Advances in neural information processing systems, 2017, pp. 5998–6008.

[7] J. D. M.-W. C. Kenton, L. K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in: Proceedings of NAACL-HLT, 2019, pp. 4171–4186.

[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929.

[9] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. Jégou, Training data-efficient image transformers & distillation through attention, arXiv preprint arXiv:2012.12877.

[10] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold, J. Uszkoreit, A. Dosovitskiy, T. Kipf, Object-Centric Learning with Slot Attention, in: NeurIPS 2020, 2020.

[11] M. India, P. Safari, J. Hernando, Self multi-head attention for speaker recognition, Proc. Interspeech 2019 (2019) 4305–4309.

[12] H.-j. Shim, J. Heo, J.-h. Park, G.-h. Lee, H.-J. Yu, Graph attentive feature aggregation for text-independent speaker verification, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp. 7972–7976.

[13] R. Wang, J. Ao, L. Zhou, S. Liu, Z. Wei, T. Ko, Q. Li, Y. Zhang, Multi-View Self-Attention Based Transformer for Speaker Recognition, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp. 6732–6736.

[14] B. Han, Z. Chen, Y. Qian, Local Information Modeling with Self-Attention for Speaker Verification, in: ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2022, pp. 6727–6731.

[15] V. Mingote, A. Miguel, A. Ortega, E. Lleida, Differentiable Supervector Extraction for Encoding Speaker and Phrase Information in Text Dependent Speaker Verification, Proceedings of IberSPEECH 2018 (2018) 1–5doi:10.21437/IberSPEECH.2018-1.
URL http://dx.doi.org/10.21437/IberSPEECH.2018-1

[16] V. Mingote, A. Miguel, A. Ortega, E. Lleida, Supervector Extraction for Encoding Speaker and Phrase Information with Neural Networks for Text-Dependent Speaker Verification, Applied Sciences 9 (16) (2019) 3295.

[17] V. Mingote, A. Miguel, A. Ortega, E. Lleida, Optimization of the area under the ROC curve using neural network supervectors for text-dependent speaker verification, Computer Speech & Language 63 (2020) 101078.

[18] D. A. Reynolds, R. C. Rose, et al., Robust text-independent speaker identification using Gaussian mixture speaker models, IEEE transactions on speech and audio processing 3 (1) (1995) 72–83.

[19] D. A. Reynolds, T. F. Quatieri, R. B. Dunn, Speaker verification using adapted Gaussian mixture models, Digital signal processing 10 (1-3) (2000) 19–41.

[20] L. R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition, Proceedings of the IEEE 77 (2) (1989) 257–286.

[21] V. Mingote, A. Miguel, A. Ortega, E. Lleida, Memory Layers with Multi-Head Attention Mechanisms for Text-Dependent Speaker Verification, in: ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6154–6158. doi:10.1109/ICASSP39728.2021.9414859.

[22] G. Lample, A. Sablayrolles, M. Ranzato, L. Denoyer, H. Jégou, Large memory layers with product keys, in: Advances in Neural Information Processing Systems, 2019, pp. 8546–8557.

[23] W. Cai, Z. Cai, X. Zhang, X. Wang, M. Li, A novel learnable dictionary encoding layer for end-to-end language identification, in: 2018 ICASSP, pp. 5189–5193.

[24] G. Hinton, O. Vinyals, J. Dean, Distilling the Knowledge in a Neural Network, NIPS 2014 Deep Learning Workshop (2015) 1–9doi:10.1063/1.4931082.
URL http://arxiv.org/abs/1503.02531

[25] Z. Zhong, L. Zheng, G. Kang, S. Li, Y. Yang, Random erasing data augmentation, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, 2020, pp. 13001–13008.

[26] V. Mingote, A. Miguel, D. Ribas, A. Ortega, E. Lleida, Knowledge Distillation and Random Erasing Data Augmentation for Text-Dependent Speaker Verification, in: 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2020, pp. 6824–6828.

[27] I. Viñals, D. Ribas, V. Mingote, J. Llombart, P. Gimeno, A. Miguel, A. Ortega, E. Lleida, Phonetically-Aware Embeddings, Wide Residual Networks with Time-Delay Neural Networks and Self Attention Models for the 2018 NIST Speaker Recognition Evaluation, Proc. Interspeech 2019 (2019) 4310–4314.

[28] C. Blundell, J. Cornebise, K. Kavukcuoglu, D. Wierstra, Weight uncertainty in neural network, in: International Conference on Machine Learning, PMLR, 2015, pp. 1613–1622.

[29] A. Korattikara, V. Rathod, K. Murphy, M. Welling, Bayesian Dark Knowledge, arXiv (2015) 1–9 doi:10.1017/CBO9781107415324.004.

[30] P. Shen, X. Lu, S. Li, H. Kawai, Feature Representation of Short Utterances Based on Knowledge Distillation for Spoken Language Identification., in: Interspeech, 2018, pp. 1813–1817.

[31] P. Shen, X. Lu, S. Li, H. Kawai, Interactive Learning of Teacher-student Model for Short Utterance Spoken Language Identification, in: Proc. ICASSP, IEEE, 2019, pp. 5981–5985.

[32] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.

[33] A. Larcher, K. A. Lee, B. Ma, H. Li, Text-dependent speaker verification: Classifiers, databases and RSR2015, Speech Communication 60 (2014) 56–77.

[34] R. K. Das, M. Madhavi, H. Li, Compensating utterance information in fixed phrase speaker verification, in: 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), IEEE, 2018, pp. 1708–1712.

[35] T. Liu, M. C. Madhavi, R. K. Das, H. Li, A Unified Framework for Speaker and Utterance Verification., Proc. Interspeech 2019 (2019) 4320–4324.

[36] T. Liu, R. K. Das, M. Madhavi, S. Shen, H. Li, Speaker-Utterance Dual Attention for Speaker and Utterance Verification, Proc. Interspeech 2020 (2020) 4293–4297.

[37] H. Zeinali, L. Burget, J. Cernocky, A Multi Purpose and Large Scale Speech Corpus in Persian and English for Speaker and Speech Recognition: the DeepMine Database, in: Proc. ASRU 2019.

[38] H. Zeinali, K. A. Lee, J. Alam, L. Burget, Short-duration Speaker Verification (SdSV) Challenge 2020: the Challenge Evaluation Plan., Tech. rep., arXiv preprint arXiv:1912.06311.

[39] V. Panayotov, G. Chen, D. Povey, S. Khudanpur, Librispeech: An ASR corpus based on public domain audio books, in: 2015 ICASSP, pp. 5206–5210.

[40] A. Lozano-Diez, A. Silnova, B. Pulugundla, J. Rohdin, K. Veselý, L. Burget, O. Plchot, O. Glembek, O. Novotný, P. Matejka, BUT Text-Dependent Speaker Verification System for SdSV Challenge 2020., in: INTERSPEECH, 2020, pp. 761–765.

[41] Z. Chen, Y. Lin, Improving X-Vector and PLDA for Text-Dependent Speaker Verification., in: INTERSPEECH, 2020, pp. 726–730.

[42] A. Nagrani, J. S. Chung, A. Zisserman, VoxCeleb: A Large-Scale Speaker Identification Dataset, in: Proc. Interspeech 2017, pp. 2616–2620.

[43] J. S. Chung, A. Nagrani, A. Zisserman, VoxCeleb2: Deep Speaker Recognition, in: Proc. Interspeech 2018, 2018, pp. 1086–1090.

[44] The NIST Year 2008 Speaker Recognition Evaluation Plan (2008).
URL https://www.nist.gov/sites/default/files/documents/2017/09/26/sre08_evalplan_release4.pdf

[45] The NIST Year 2010 Speaker Recognition Evaluation Plan (2010).
URL https://www.nist.gov/sites/default/files/documents/itl/iad/mig/NIST_SRE10_evalplan-r6.pdf.

[46] B. Desplanques, J. Thienpondt, K. Demuynck, ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification, Proc. Interspeech 2020 (2020) 3830–3834.

**Victoria Mingote** nhận bằng Cử nhân và Thạc sĩ về Kỹ thuật Viễn thông từ Đại học Zaragoza, Tây Ban Nha, lần lượt vào năm 2014 và 2016. Sau đó, cô gia nhập nhóm nghiên cứu ViVoLab với tư cách là một sinh viên Tiến sĩ và nhận bằng Tiến sĩ vào năm 2022 từ Đại học Zaragoza. Hiện tại cô là một nhà nghiên cứu sau tiến sĩ trong nhóm nghiên cứu ViVoLab. Cô đã đạt được nhiều công bố về công trình của mình trong các tạp chí quốc tế khác nhau và kỷ yếu hội nghị. Sở thích nghiên cứu của cô mở rộng qua các lĩnh vực xử lý tín hiệu, học máy, xác minh và nhận dạng đa phương thức (giọng nói và khuôn mặt), và nhận dạng ngôn ngữ.

**Antonio Miguel** sinh tại Zaragoza, Tây Ban Nha. Anh nhận bằng Thạc sĩ về kỹ thuật viễn thông và bằng Tiến sĩ từ Đại học Zaragoza (UZ), Zaragoza, Tây Ban Nha, lần lượt vào năm 2001 và 2008. Từ năm 2000 đến 2006, anh làm việc với Nhóm Công nghệ Truyền thông, Khoa Kỹ thuật Điện tử và Truyền thông, UZ, dưới một học bổng nghiên cứu. Từ năm 2006, anh là Phó Giáo sư tại Khoa Kỹ thuật Điện tử và Truyền thông, UZ. Sở thích nghiên cứu hiện tại của anh bao gồm mô hình hóa âm thanh cho nhận dạng giọng nói và người nói.

**Alfonso Ortega** nhận bằng Kỹ thuật Viễn thông và bằng Tiến sĩ từ Đại học Zaragoza, Zaragoza, Tây Ban Nha, lần lượt vào năm 2000 và 2005. Anh là Phó Giám đốc của Viện Nghiên cứu Kỹ thuật Aragón (I3A), Đại học Zaragoza. Năm 2006, anh là học giả thăm viếng tại Trung tâm Hệ thống Giọng nói Mạnh mẽ, Đại học Texas tại Dallas, Mỹ. Anh đã tham gia hơn 50 dự án nghiên cứu được tài trợ bởi các tổ chức công quốc gia hoặc quốc tế và hơn 30 dự án nghiên cứu cho nhiều công ty. Anh là tác giả của hơn 100 bài báo được xuất bản trong các tạp chí quốc tế hoặc kỷ yếu hội nghị và nhiều bằng sáng chế quốc tế. Hiện tại anh là Phó Giáo sư tại Khoa Kỹ thuật Điện tử và Truyền thông, Đại học Zaragoza. Sở thích nghiên cứu của anh bao gồm xử lý, phân tích và mô hình hóa giọng nói, xác minh người nói tự động, và nhận dạng giọng nói tự động. Luận án Tiến sĩ của anh, được hướng dẫn bởi Dr. E. Lleida, đã nhận Giải thưởng Đặc biệt Tiến sĩ và Giải thưởng Ghế Telefonica cho Tiến sĩ công nghệ tốt nhất.

**Eduardo Lleida** nhận bằng Thạc sĩ về kỹ thuật viễn thông và bằng Tiến sĩ về xử lý tín hiệu từ Universitat Politecnica de Catalunya (UPC), Barcelona, Tây Ban Nha, lần lượt vào năm 1985 và 1990. Từ năm 1986 đến 1988, anh tham gia công việc tiến sĩ của mình tại Khoa Lý thuyết Tín hiệu và Truyền thông, UPC. Từ năm 1989 đến 1990, anh là Trợ giảng, và từ năm 1991 đến 1993, anh là Phó Giáo sư tại Khoa Lý thuyết Tín hiệu và Truyền thông, UPC. Từ tháng 2 năm 1995 đến tháng 1 năm 1996, anh làm việc với AT&T Bell Laboratories, Murray Hill, NJ, Mỹ, với tư cách là Cố vấn về nhận dạng giọng nói. Hiện tại anh là Giáo sư chính về lý thuyết tín hiệu và truyền thông tại Khoa Kỹ thuật Điện tử và Truyền thông, Đại học Zaragoza, Zaragoza, Tây Ban Nha, và là thành viên của Viện Nghiên cứu Kỹ thuật Aragón, nơi anh đứng đầu nhóm nghiên cứu ViVoLab về công nghệ giọng nói. Anh đã là cố vấn tiến sĩ cho 12 sinh viên tiến sĩ. Anh đã quản lý hơn 50 dự án liên quan đến giọng nói, là nhà phát minh trong bảy bằng sáng chế toàn cầu, và đồng tác giả hơn 200 bài báo kỹ thuật trong lĩnh vực nhận dạng giọng nói, người nói và ngôn ngữ, tăng cường giọng nói và nhận dạng trong môi trường âm thanh bất lợi, mô hình hóa âm thanh, các biện pháp tin cậy, và hệ thống đối thoại nói.
