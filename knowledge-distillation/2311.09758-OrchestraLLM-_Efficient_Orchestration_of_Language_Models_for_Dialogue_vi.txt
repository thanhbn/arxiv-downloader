# 2311.09758.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2311.09758.pdf
# Kích thước tệp: 878319 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
OrchestraLLM: Điều phối hiệu quả các mô hình ngôn ngữ cho việc theo dõi trạng thái đối thoại
Chia-Hsuan Lee
Đại học Washington
chiahlee@uw.eduHao Cheng
Microsoft Research
chehao@microsoft.comMari Ostendorf
Đại học Washington
ostendor@uw.edu
Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) đã cách mạng hóa bối cảnh của Xử lý Ngôn ngữ Tự nhiên, nhưng lại tốn kém về mặt tính toán. Để giảm chi phí mà không hy sinh hiệu suất, các nghiên cứu trước đã khám phá nhiều cách tiếp cận khác nhau để khai thác tiềm năng của các Mô hình Ngôn ngữ Nhỏ hơn (SLMs) như những lựa chọn thay thế hiệu quả về chi phí cho các đối tác lớn hơn của chúng. Được thúc đẩy bởi những phát hiện rằng SLMs và LLMs thể hiện những điểm mạnh bổ sung trong một nhiệm vụ trích xuất kiến thức có cấu trúc, công trình này trình bày một khung định tuyến SLM/LLM mới được thiết kế để cải thiện hiệu quả tính toán và nâng cao hiệu suất nhiệm vụ. Trong các nhiệm vụ theo dõi trạng thái đối thoại, khung định tuyến được đề xuất nâng cao hiệu suất đáng kể so với việc chỉ dựa vào LLMs, đồng thời giảm chi phí tính toán hơn 50%.
1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLMs) đã trở thành những công cụ đa năng có khả năng giải quyết một loạt rộng các nhiệm vụ chỉ với một vài ví dụ huấn luyện. Tuy nhiên, kích thước mở rộng của chúng đã mang lại những yêu cầu tính toán leo thang. Ngược lại, các Mô hình Ngôn ngữ Nhỏ hơn (SLMs) hiệu quả hơn thường đòi hỏi một lượng lớn dữ liệu tinh chỉnh để trở nên thực sự hiệu quả. Công trình này giải quyết các tình huống trong đó chỉ có sẵn dữ liệu cụ thể cho nhiệm vụ hạn chế, làm cho các SLMs đã tinh chỉnh ít đáng tin cậy hơn. Mục tiêu của chúng tôi là phát triển một khung định tuyến điều phối SLMs và LLMs, nâng cao hiệu suất nhiệm vụ đồng thời giảm chi phí tính toán.
Đối thoại hướng nhiệm vụ là rất quan trọng để tương tác hiệu quả giữa con người và máy tính, cho phép các hệ thống hiểu và hỗ trợ các nhiệm vụ cụ thể như đặt vé máy bay hoặc lên lịch cuộc họp. Các đối thoại hướng nhiệm vụ liên quan đến dữ liệu có cấu trúc thường dựa vào Theo dõi Trạng thái Đối thoại (DST), nơi ý định của người dùng được trích xuất từ lịch sử đối thoại giữa người dùng và tác nhân dưới dạng các giá trị slot được liên kết với một lược đồ được định nghĩa trước. Các SLMs đã tinh chỉnh đã được sử dụng trong DST trong vài năm, bao gồm cả LMs tự hồi quy (Ham và cộng sự, 2020; Hosseini-Asl và cộng sự, 2020; Peng và cộng sự, 2020) và LMs chuỗi-đến-chuỗi (Lee và cộng sự, 2021; Su và cộng sự, 2022; Bang và cộng sự, 2023; Imrattanatrai và Fukuda, 2023; Wang và cộng sự, 2023). LLMs đã được sử dụng cho việc học trong ngữ cảnh few-shot trong DST (Xie và cộng sự, 2022; Hude ˇcek và Dušek, 2023; Hu và cộng sự, 2022; King và Flanigan, 2023a) nơi LLMs được nhắc nhở với các mô tả nhiệm vụ được tác giả bởi con người hoặc các mẫu trong ngữ cảnh. Trong công trình của chúng tôi, chúng tôi tìm cách tận dụng hiệu quả của LLMs với một lượng nhỏ dữ liệu huấn luyện nhưng giảm chi phí.
Các chiến lược tận dụng cả SLMs và LLMs đã được phát triển để giảm thiểu các yêu cầu tính toán của LLMs. Các cách tiếp cận dựa trên cascade chỉ đạo một truy vấn đến LLM khi nó không thể được giải quyết bởi SLM (Chen và cộng sự, 2023; Madaan và cộng sự, 2023). Những cách tiếp cận này gây ra độ trễ và sự dư thừa tính toán vì chúng liên tục truy vấn SLMs. Các cách tiếp cận khác sử dụng các bộ phân loại nhị phân để dự đoán LM thích hợp nhất để sử dụng (Kag và cộng sự, 2022; Šakota và cộng sự, 2023). Một hạn chế của các cách tiếp cận dựa trên bộ phân loại là sự cần thiết phải huấn luyện lại khi giới thiệu các mô hình mới.
Trong công trình này, chúng tôi đề xuất một khung định tuyến động, OrchestraLLM (được minh họa trong Hình 1), tận dụng các chuyên gia LM nhỏ (đã tinh chỉnh) và lớn. Giả thuyết rằng các ví dụ có những nhúng ngữ nghĩa tương tự có cùng mức độ khó khăn, chúng tôi chọn một chuyên gia thích hợp dựa trên khoảng cách nhúng giữa thể hiện kiểm thử và các thể hiện trong các nhóm chuyên gia. Các nhóm chuyên gia chứa các ví dụ đại diện cho các loại ngữ cảnh đối thoại nơi các LMs khác nhau cung cấp những câu trả lời đáng tin cậy hơn. Sau khi truy xuất k ví dụ gần nhất hàng đầu, một chuyên gia được chọn dựa trên bỏ phiếu đa số. Không giống như các cách tiếp cận dựa trên cascade và dựa trên bộ phân loại, khung được đề xuất loại bỏ sự cần thiết cho việc huấn luyện bộ định tuyến, mặc dù dữ liệu được gắn nhãn bằng tay là cần thiết để tạo ra các nhóm chuyên gia. Ngoài ra, bộ truy xuất có thể được tinh chỉnh với nhãn nhiệm vụ đích hoặc thông tin chuyên gia để đạt được định tuyến hiệu quả và chính xác hơn.
Tóm lại, đóng góp chính của công trình này là việc giới thiệu một mô hình chuyển đổi mới được thiết kế để giảm chi phí tính toán liên quan đến LLMs đồng thời nâng cao hiệu suất. Kết quả thực nghiệm trên hai benchmarks DST đa miền khác nhau (Multi-WOZ (Budzianowski và cộng sự, 2018; Ye và cộng sự, 2022) và SGD (Rastogi và cộng sự, 2020)) chứng minh rằng OrchestraLLM tận dụng các thành thạo của các chuyên gia khác nhau, vượt trội hơn các hệ thống LLM đồng thời đạt được sự giảm đáng kể hơn 50% chi phí tính toán.
2 Theo dõi Trạng thái Đối thoại
Trong công trình này, chúng tôi tập trung vào việc kết hợp LLMs đa mục đích và SLMs cụ thể cho nhiệm vụ để đạt được hiệu quả tốt hơn cho việc theo dõi trạng thái đối thoại (DST). Trong phần sau, chúng tôi đầu tiên cung cấp các thiết lập nhiệm vụ cần thiết và sau đó chi tiết hai mô hình DST đại diện sử dụng LLMs và SLMs tương ứng.
Một đối thoại hướng nhiệm vụ (TOD) bao gồm một chuỗi các trao đổi giữa hai bên, mỗi bên được khởi tạo bởi người dùng và theo sau bởi một phản hồi từ hệ thống. Ở đây, chúng tôi ký hiệu mỗi trao đổi như một lượt dẫn đến một chuỗi, U1, A1, ..., U T, AT, trong đó UtandAtđại diện cho phát ngôn của người dùng và phản hồi của hệ thống, tương ứng. Đối với lượt thứ t, người dùng cung cấp một phát ngôn mới Ut, và tác nhân hệ thống phản hồi với phát ngôn At. Tại lượt t, ngữ cảnh đối thoại tương ứng là Ct={U1, A1, . . . , A t−1, Ut}, loại trừ phản hồi hệ thống mới nhất At. Mục tiêu của DST là trích xuất thông tin liên quan đến nhiệm vụ như các biểu diễn có cấu trúc (trạng thái đối thoại) từ các phát ngôn người dùng-hệ thống để các yêu cầu của người dùng có thể được thực hiện tương ứng. Để hỗ trợ điều này, thường có một lược đồ cụ thể cho nhiệm vụ được cung cấp. Trong một tình huống đa miền được xem xét trong bài báo này, lược đồ chứa Mmiền D={d1, . . . , d M}vàNslots S={s1, . . . , s N}để theo dõi. DST t, trạng thái đối thoại tại lượt t, định nghĩa các ánh xạ hiện tại từ các cặp ( dm,sn) thành một giá trị vdựa trên ngữ cảnh đối thoại Ct. Cụ thể,
DST t={(dm, sn, vt
mn)|vt
ij̸=null},
chỉ chứa các slots không null được tích lũy cho đến thời điểm đó. Thay vì trực tiếp dự đoán toàn bộ trạng thái đối thoại từ đầu, chúng tôi xây dựng các dự đoán trạng thái đối thoại dựa trên niềm tin mức lượt (TLB) như được thực hiện bởi Hu và cộng sự (2022), cho phép một sự kết hợp linh hoạt hơn của LLMs và SLMs. Tại lượt t, mô hình DST chỉ dự đoán TLB t, nơi các slots được biểu hiện mới hoặc slots với các giá trị được cập nhật được sử dụng để có được DST t mới nhất thông qua việc tập hợp tất cả TLBs trước đó.1
Trong tài liệu, các mô hình DST dựa trên SLM cụ thể cho nhiệm vụ thường được tinh chỉnh với các cập nhật tham số đầy đủ trong khi các mô hình DST sử dụng LLMs được thực hiện thông qua việc học trong ngữ cảnh few-shot. Chúng tôi thảo luận về hai mô hình DST khác nhau được xem xét dưới đây.
LLM DST. IC-DST (Hu và cộng sự, 2022) là một khung học trong ngữ cảnh (ICL) cho phép DST few-shot với LLMs. Dự đoán là sự thay đổi trong mỗi cặp lượt thay vì các trạng thái đối thoại tích lũy. Để có được các trạng thái đối thoại tích lũy, các thay đổi lượt được tập hợp qua các lượt. Trạng thái đối thoại của các lượt trước được sử dụng như một bản tóm tắt của lịch sử ngữ cảnh và nó cho phép phù hợp với nhiều mẫu hơn điều này rất quan trọng cho hiệu suất ICL. Cụ thể, cho bảng lược đồ, Kcác mẫu trong ngữ cảnh, trạng thái đối thoại của lượt trước, và thể hiện đầu vào (cặp phát ngôn tác nhân-người dùng gần đây nhất), LLM đầu ra
TLB t=LLM (T, E 1:K, DST t−1, At−1, Ut)(1)
trong đó Tlà bảng lược đồ cho tất cả miền, Eklà các ví dụ của các cặp thay đổi lượt và các đầu ra liên quan.
SLM DST. Ở đây, chúng tôi phát triển một mô hình DST dựa trên prompt (ký hiệu là Prompt-DST ) với SLM (T5 (Raffel và cộng sự, 2020)). Đầu vào của Prompt-DST tương tự như IC-DST, ngoại trừ việc các mẫu trong ngữ cảnh được loại trừ. Cụ thể, cho đầu vào được tăng cường prompt lược đồ, mô hình đầu ra
TLB t=SLM (T, DST t−1, At−1, Ut).(2)
Ở đây, mô hình được huấn luyện sử dụng mục tiêu học bằng cách tối đa hóa log-likelihood của các giá trị slot vt(dm, sn)cho TLB hiện tại, tức là,
max log P(TLB t|T, DST t−1, At−1, Ut).(3)
Trong quá trình suy luận, một thủ tục giải mã tham lam được áp dụng trực tiếp, tức là,chỉ token có khả năng cao nhất trong từ vựng mô hình đã cho được dự đoán tại mỗi bước giải mã.
3 Phương pháp Định tuyến
Ở đây, chúng tôi trình bày phương pháp của chúng tôi cho việc định tuyến với OrchestraLLM được áp dụng cho nhiệm vụ DST. Khung tổng thể được minh họa trong Hình 1. Chúng tôi ký hiệu các mô hình DST khác nhau như chuyên gia . Cho một thể hiện đầu vào mới (bộ ba (DST t−1, At−1, Ut)), OrchestraLLM đầu tiên tính toán nhúng ngữ nghĩa của nó, so sánh nó với các nhúng mẫu của các bộ ba từ mỗi nhóm chuyên gia sử dụng khoảng cách cosine, và truy xuất các mẫu K hàng đầu. Bộ định tuyến gán đầu vào cho một chuyên gia dựa trên bỏ phiếu đa số. Mặc dù cách tiếp cận của chúng tôi lấy cảm hứng từ công trình của Jang và cộng sự (2023), điều quan trọng cần lưu ý rằng cách tiếp cận của họ chủ yếu tập trung vào việc tối ưu hóa hiệu suất nhiệm vụ trong các tình huống chuyển giao nhiệm vụ zero-shot, trong khi trọng tâm của chúng tôi nằm ở việc cải thiện hiệu quả tính toán trong các thiết lập học few-shot.
3.1 Xây dựng Nhóm Chuyên gia
Đối với mỗi đối thoại trong một tập nhỏ được giữ lại, các chuyên gia SLM và LLM được sử dụng để dự đoán TLB tại mỗi lượt người dùng ( TLB t) một cách riêng lẻ. Nếu cả hai chuyên gia đều dự đoán đúng TLB, bộ ba thể hiện được bao gồm trong nhóm SLM. Khi chỉ một chuyên gia dự đoán đúng TLB, thể hiện được gán cho nhóm của chuyên gia đó. Các thể hiện không được dự đoán đúng không được sử dụng trong cả hai nhóm.
3.2 Học Biểu diễn Bộ ba
Tương tự như công trình gần đây về truy xuất dày đặc (Karpukhin và cộng sự, 2020), bộ truy xuất sử dụng kiến trúc bi-encoder, mã hóa các đối thoại với nhãn và dự đoán vào không gian nhúng. Trong suốt công trình, SenBERT (Reimers và Gurevych, 2019) được sử dụng như mô hình nhúng xương sống. Bi-encoder được tinh chỉnh sử dụng một tập nhỏ các đối thoại, cùng như được sử dụng để xây dựng các nhóm chuyên gia. Chúng tôi sử dụng một mất mát đối lập sao cho độ tương tự giữa một cặp ví dụ tích cực cao và độ tương tự giữa một cặp ví dụ tiêu cực thấp. Ba phương pháp khác nhau để xây dựng các cặp tích cực và tiêu cực được khám phá: nhận thức nhiệm vụ, nhận thức chuyên gia, và sự kết hợp của chúng.
Giám sát Nhận thức Nhiệm vụ xác định các cặp thể hiện tích cực và tiêu cực để huấn luyện bằng cách đầu tiên tính toán độ tương tự từng cặp cho mỗi mẫu trong tập giữ lại. Sau đó, các cặp có điểm số cao nhất và thấp nhất được sử dụng như các ví dụ tích cực và tiêu cực, tương ứng. Hàm tương tự tận dụng các chú thích vàng của các đối thoại tập giữ lại. Cho hai thể hiện, aandb, độ tương tự là một kết hợp có trọng số của độ tương tự slot-value của trạng thái trước (DST) và TLB hiện tại:
1
2Sim DST+Sim TLB.
Cho TLB x={(sx
1, vx
1),···,(sx
m, vx
m)}là TLB của thể hiện x. Theo Hu và cộng sự (2022), độ tương tự cặp slot-value là
Fslot-value =F({(sa
1, va
1),···,(sa
m, va
m)},
{(sb
1, vb
1),···,(sb
n, vb
n)}).
và độ tương tự slot là
Fslot=F({sa
1,···, sa
m},{sb
1,···, sb
n}).
trong đó Flà định nghĩa tiêu chuẩn của điểm F1 tức là,
F=2PR
P+R, trong đó Plà precision, và Rlà recall.
Điểm tương tự giữa TLB avàTLB blà
Sim(TLB a, TLB b) =Fslot-value +Fslot−1
Độ tương tự lịch sử ngữ cảnh Sim DST được định nghĩa cùng cách.
Giám sát Nhận thức Chuyên gia đầu tiên nhóm các thể hiện trong tập giữ lại theo chuyên gia nào đưa ra dự đoán chính xác nhất. (Đối với các trường hợp hòa, SLM được chọn.) Sau đó chúng tôi tính toán độ tương tự bộ ba từng cặp sử dụng một embedder có sẵn (ví dụ, SenBERT). Các cặp có điểm số cao nhất với cùng nhãn chuyên gia là các ví dụ tích cực, và các cặp có điểm số thấp nhất với nhãn chuyên gia khác nhau là các ví dụ tiêu cực.
Giám sát Nhận thức Nhiệm vụ+Chuyên gia đơn giản tập hợp cả hai tập các cặp tích cực và tiêu cực.
Lưu ý rằng giám sát nhận thức nhiệm vụ là bất khả tri đối với các chuyên gia được sử dụng trong định tuyến, vì vậy mô hình nhúng không cần được huấn luyện lại khi các chuyên gia được thêm hoặc cập nhật. Giám sát nhận thức chuyên gia sẽ yêu cầu cập nhật mô hình nhúng nếu các chuyên gia thay đổi. Trong tất cả các trường hợp, các nhóm chuyên gia sẽ cần được cập nhật với các thay đổi đối với các chuyên gia.
4 Thực nghiệm
4.1 Bộ dữ liệu
Chúng tôi sử dụng hai bộ dữ liệu được chi tiết dưới đây cho các thực nghiệm. Một bản tóm tắt các bộ dữ liệu DST được báo cáo trong Bảng 1.
MultiWOZ (Budzianowski và cộng sự, 2018) là một bộ dữ liệu đối thoại hướng nhiệm vụ đa miền chứa hơn 10K đối thoại được viết bởi con người-con người qua 8 miền và đã là một trong những benchmark phổ biến nhất trong tài liệu DST. Sau khi xuất bản Budzianowski và cộng sự (2018), nhiều công trình cải thiện chất lượng nhãn, ví dụ, MultiWOZ 2.1 (Eric và cộng sự, 2020) và MultiWOZ 2.4 (Ye và cộng sự, 2021). Chúng tôi thực nghiệm sử dụng phiên bản gần đây nhất, MultiWOZ 2.4.
SGD (Rastogi và cộng sự, 2020) là một bộ dữ liệu đối thoại hướng nhiệm vụ chứa hơn 16k cuộc hội thoại đa miền bao trùm 41 miền, có đặc trưng đánh giá ngoài miền. 15 trong số 21 miền trong tập kiểm thử không có mặt trong tập huấn luyện và 77% các lượt đối thoại trong tập kiểm thử chứa ít nhất một miền không có mặt trong tập huấn luyện.
4.2 Thiết lập Thực nghiệm
Trong công trình này, chúng tôi xem xét một thiết lập few-shot cho DST. Theo thiết lập thực nghiệm đa miền từ Wu và cộng sự (2020), chúng tôi lấy mẫu ngẫu nhiên 5% dữ liệu huấn luyện từ MultiWOZ và SGD tương ứng để huấn luyện các mô hình chuyên gia.
Thiết lập Mô hình và Siêu tham số. Đối với Prompt-DST, chúng tôi sử dụng T5-base và T5-large như mô hình xương sống cho MWOZ và SGD tương ứng, vì sau này phức tạp hơn về mặt lược đồ và nhiều lượt đối thoại hơn. Đối với IC-DST, chúng tôi sử dụng ChatGPT như mô hình xương sống2với 10 mẫu trong ngữ cảnh. Chúng tôi khởi tạo bộ truy xuất định tuyến từ SenBERT (all-mpnet-base-v2). Chúng tôi chạy suy luận trên 100 đối thoại được lấy mẫu ngẫu nhiên từ các tập validation của MWOZ và SGD như các tập giữ lại. Cùng 100 đối thoại được sử dụng để huấn luyện bộ truy xuất. Đối với tất cả thực nghiệm, l= 25 được sử dụng cho các ví dụ tích cực và tiêu cực cho việc học đối lập. Trong quá trình suy luận, chúng tôi lấy mẫu ngẫu nhiên 100 lượt từ các tập giữ lại để phục vụ như nhóm SLM và nhóm LLM tương ứng cho các thực nghiệm MWOZ và 300 lượt cho các thực nghiệm SGD.3Chúng tôi để lại cho công việc tương lai việc tạo ra các chiến lược mới cho việc lựa chọn thể hiện nhóm chuyên gia. Chúng tôi sử dụng k= 10 cho bỏ phiếu đa số và phá vỡ sự hòa bằng cách ưu tiên SLM.
4.3 Đánh giá
4.3.1 Độ chính xác
Theo truyền thống, các hệ thống DST được đánh giá bằng độ chính xác mục tiêu chung (JGA) trên các trạng thái đối thoại tích lũy (Henderson và cộng sự, 2014). Thước đo này đánh giá tính đúng đắn của trạng thái đối thoại tại mỗi lượt và coi nó là chính xác chỉ khi tất cả các giá trị slot trong mỗi miền khớp chính xác với các giá trị ground-truth. Rất khó để đánh giá chính xác mức độ hoạt động tốt của một hệ thống trên các lượt đơn lẻ với DST JGA. Do đó chúng tôi cũng báo cáo niềm tin mức lượt (TLB JGA) (Dey và cộng sự, 2022).
4.3.2 Hiệu quả
Các phép toán điểm nổi mỗi Giây (FLOPs) đại diện cho số lượng các phép toán số học điểm nổi (phép cộng và phép nhân) mà một mô hình thực hiện trong một lần chạy. FLOPs thường được sử dụng để ước tính chi phí tính toán hoặc khối lượng công việc cần thiết cho huấn luyện hoặc suy luận. Huấn luyện một mô hình lớn đòi hỏi một số lượng đáng kể các lần chạy ngược, đắt hơn các lần chạy thuận, tuy nhiên suy luận là một quá trình liên tục xảy ra bất cứ khi nào mô hình được sử dụng, do đó tích lũy thêm chi phí theo thời gian. NVIDIA (Leopold, 2019) và Amazon (Barr, 2019) báo cáo khoảng 90% khối lượng công việc ML là xử lý suy luận trong các dịch vụ đám mây của họ. Do đó, chúng tôi chọn báo cáo FLOPs cho việc sử dụng thời gian suy luận.
Chúng tôi ước tính chi phí tính toán tổng hợp, được đo bằng TeraFLOPs, cần thiết để thực hiện suy luận trên toàn bộ bộ dữ liệu kiểm thử. Điều quan trọng cần lưu ý rằng IC-DST dựa vào ChatGPT, một mô hình không thể truy cập công khai, do đó loại trừ việc đánh giá trực tiếp hiệu quả tính toán của nó. Dựa trên phỏng đoán phổ biến trong lĩnh vực công cộng, ChatGPT được giả định là một phiên bản được tinh chỉnh của mô hình GPT-3 với một số lượng tham số đáng kể là 175 tỷ (Brown và cộng sự, 2020). Để ước tính các yêu cầu tính toán, chúng tôi tiến hành các phép đo FLOPs trên mô hình GPT-2 (Radford và cộng sự) và sau đó chia tỷ lệ các phép đo này theo sự khác biệt kích thước tham số giữa GPT-2 và ChatGPT. Chi phí tính toán của bộ truy xuất, được đo bằng FLOPs, cho mỗi thể hiện lượt, là khoảng 0.02 TeraFLOPs. Tải tính toán này trở nên không đáng kể khi được xem xét kết hợp với ChatGPT trong OrchestraLLM . ChatGPT yêu cầu khoảng 3000 TeraFLOPs cho mỗi thể hiện lượt.
4.4 Đường cơ sở
Định tuyến Dựa trên Phân loại
Chúng tôi so sánh khung định tuyến của chúng tôi với các cách tiếp cận dựa trên phân loại hiện có cho việc chuyển đổi mô hình, như những cách được đề xuất bởi Šakota và cộng sự (2023) và Kag và cộng sự (2022). Những cách tiếp cận hiện có này thường huấn luyện một bộ phân loại nhị phân để phục vụ như bộ định tuyến. Chúng tôi huấn luyện BERT (Devlin và cộng sự, 2019) (bert-base-cased) với các nhãn chuyên gia trong tập giữ lại các đối thoại với một mục tiêu phân loại nhị phân để thực hiện định tuyến như một đường cơ sở. Định tuyến Dựa trên Cascade
Các cách tiếp cận dựa trên cascade Chen và cộng sự (2023); Madaan và cộng sự (2023) thường truy vấn một SLM và chuyển hướng thể hiện đến một LLM nếu mô hình ngôn ngữ nhỏ hơn không đủ tự tin. Chúng tôi chọn sử dụng xác suất mức chuỗi được chuẩn hóa của đầu ra SLM như thước đo tin cậy. Chúng tôi điều chỉnh ngưỡng xác suất trên tập giữ lại và sử dụng ngưỡng để quyết định có chuyển hướng thể hiện đến LLM trong quá trình suy luận hay không.
4.5 Kết quả
4.5.1 MultiWOZ
Chúng tôi chứng minh các thực nghiệm MultiWOZ trong một thiết lập few-shot trong Bảng 2. Chúng tôi sử dụng 5% đối thoại trong tập huấn luyện để tinh chỉnh Prompt-DST và bộ truy xuất của IC-DST. Prompt-DST và IC-DST thực hiện suy luận trên 100 đối thoại khác trong tập validation để phục vụ như nhóm chuyên gia. Chúng tôi lựa chọn ngẫu nhiên 100 lượt từ những đối thoại này cho mỗi chuyên gia để phục vụ như các nhóm chuyên gia cho định tuyến động.
Như mong đợi, IC-DST vượt trội hơn Prompt-DST trong thiết lập few-shot, chỉ ra rằng LLM có khả năng tổng quát hóa hơn SLM được tinh chỉnh. Bộ định tuyến phân loại dựa trên BERT gặp khó khăn trong việc khai thác hiệu quả khả năng của cả hai mô hình. Để thiết lập một ranh giới hiệu suất trên cho bộ định tuyến đã học, chúng tôi giới thiệu bộ định tuyến oracle, tập hợp các dự đoán từ cả LLM và SLM khi cả hai mô hình đều đúng, với sự ưu tiên cho SLM bất cứ khi nào có sẵn. Ngay cả với một SenBERT vanilla như một bộ truy xuất, OrchestraLLM vượt trội hơn IC-DST trong khi tiết kiệm 60% lời gọi đến LLM, chứng minh hiệu quả của khung được đề xuất của chúng tôi. Tinh chỉnh thêm bộ truy xuất với các ví dụ đối lập nhận thức nhiệm vụ được đề xuất định tuyến các ví dụ hiệu quả hơn và cải thiện DST JGA khoảng 3% so với IC-DST. Với việc huấn luyện nhận thức chuyên gia bổ sung của bộ truy xuất, chúng tôi có thể tiết kiệm thêm khoảng 7% lưu lượng đến LLM với hiệu suất vượt trội so với IC-DST. Mặc dù có kích thước nhỏ gọn, Prompt-DST được tinh chỉnh để phù hợp với kiến thức trong miền cụ thể và các artifacts cụ thể cho nhiệm vụ ( ví dụ, ràng buộc lược đồ và chiến lược gắn nhãn tùy chỉnh). Ngược lại, IC-DST được làm giàu với một kho kiến thức rộng lớn được có được trong giai đoạn huấn luyện trước của LLM, trao cho nó khả năng lý luận ngữ cảnh và sự hiểu biết nâng cao về kiến thức thường thức (Phần 5.2). Vì hai mô hình này bổ sung cho nhau, một sự tích hợp hiệu quả có thể vượt qua hiệu suất của mô hình IC-DST.
6

--- TRANG 7 ---
Router SGD MWOZ
Tỷ lệ Gán TLB JGA Tỷ lệ Gán TLB JGA
SenBERT 50% 65.97 60% 80.74
Task-Aware (SGD) 55% 67.25 54% 80.75
Task-Aware (MWOZ) 43% 66.57 55% 82.43
Bảng 4: Kết quả định tuyến chéo bộ dữ liệu của OrchestraLLM trên SGD và MWOZ. Chúng tôi ký hiệu % các lượt kiểm thử được định tuyến đến Prompt-DST (SLM) như Tỷ lệ Gán .
4.5.2 SGD
Để đánh giá hệ thống của chúng tôi trong các tình huống ngoài miền, chúng tôi hiển thị kết quả thực nghiệm trong một thiết lập few-shot trên SGD trong Bảng 3. Chúng tôi sử dụng 5% đối thoại trong tập huấn luyện để tinh chỉnh Prompt-DST và bộ truy xuất của IC-DST. Prompt-DST và IC-DST thực hiện suy luận trên 100 đối thoại khác trong tập validation để phục vụ như nhóm chuyên gia. Chúng tôi lựa chọn ngẫu nhiên 300 lượt từ mỗi chuyên gia để phục vụ như các nhóm chuyên gia cho định tuyến động.
Như chúng tôi quan sát trong MultiWOZ, việc kết hợp một SenBERT có sẵn như bộ định tuyến cải thiện điểm TLB và cũng tiết kiệm khoảng 50% tính toán. Tinh chỉnh SenBERT với mục tiêu nhận thức nhiệm vụ cải thiện hiệu quả 5% và tăng cả điểm TLB và DST. Với giám sát nhận thức chuyên gia bổ sung, nhiều lượt được định tuyến đến SLM và cải thiện điểm TLB. Thiết lập này vượt trội hơn IC-DST hơn 4% TLB JGA và tiết kiệm 57% FLOPs, chứng minh rằng bộ định tuyến của chúng tôi đủ phổ quát để hỗ trợ việc gán chéo miền và thành công cải thiện độ chính xác hệ thống.
5 Phân tích
5.1 Tổng quát hóa Chéo Miền
Out-of-Domain (OOD) trong SGD Để đánh giá hiệu quả của OrchestraLLM trong việc tổng quát hóa đến các miền chưa thấy, chúng tôi trình bày kết quả phân tích trên SGD trong Hình 2. Đầu tiên, chúng tôi quan sát rằng Prompt-DST hoạt động tốt hơn IC-DST trên các đối thoại trong miền nhưng tụt hậu so với IC-DST trên tất cả các loại đối thoại khác. Điều này gợi ý rằng khả năng tổng quát hóa của Các Mô hình Ngôn ngữ Lớn (LLMs) vượt trội hơn Các Mô hình Ngôn ngữ Nhỏ hơn (SLMs). Tất cả các biến thể của OrchestraLLM vượt trội hơn IC-DST trong các tình huống OOD, chứng minh khả năng của bộ định tuyến để hiệu quả gửi các thể hiện ngay cả khi chúng nằm ngoài miền.
Bộ Truy xuất Chéo Bộ dữ liệu Chúng tôi đánh giá thêm khung được đề xuất của chúng tôi trong một tình huống thách thức hơn nơi bộ định tuyến và các mô hình xương sống được huấn luyện trong các bộ dữ liệu khác nhau. Chúng tôi huấn luyện mô hình bộ truy xuất trên các đối thoại tập giữ lại MWOZ và đánh giá khung trên các đối thoại kiểm thử SGD và ngược lại. Kết quả được hiển thị trong Bảng 4. Đáng chú ý, khung định tuyến của chúng tôi vẫn có thể hiệu quả điều phối hai LLMs với một bộ truy xuất được huấn luyện với một bộ dữ liệu khác và vượt trội hơn IC-DST đồng thời đạt được tiết kiệm chi phí tính toán khoảng 54% trên MultiWOZ và 43% trên SGD. So với việc tinh chỉnh khớp bộ dữ liệu, việc tinh chỉnh không khớp với một bộ dữ liệu khác chỉ làm tổn hại nhẹ đến độ chính xác và hiệu quả.
5.2 Chuyên môn của SLM và LLM
Để hiểu rõ hơn bản chất bổ sung của các LMs, chúng tôi kiểm tra các ví dụ để xác định chuyên môn của chúng. Chúng tôi cung cấp các ví dụ đại diện từ các nhóm chuyên gia trong Bảng 5. Một lỗi phổ biến được thực hiện bởi LLM là không tuân thủ lược đồ. Trong ví dụ này, LLM đơn giản sao chép văn bản ("affordable") từ lượt như một dự đoán DST, trong khi SLM có khả năng căn cứ giá trị trong định dạng cụ thể cho lược đồ ("cheap"). Tuy nhiên, chúng tôi xác định hai điểm mạnh mà LLM sở hữu so với SLM. Thứ nhất, nó xuất sắc trong việc xử lý kiến thức thường thức, ví dụ, nó có thể suy ra số lượng khách đúng ở tại nhà nghỉ từ ngữ cảnh ("me and my mum"). Thứ hai, nó thể hiện thành thạo trong lý luận ngữ cảnh dài. Khi có một tham chiếu đến ngữ cảnh trước qua các miền, LLM luôn đưa ra suy luận đúng, trong khi SLM thường bỏ qua ngữ cảnh và tạo ra các giá trị ngẫu nhiên.
5.3 Định tuyến một LM Mới
Để chứng minh tính linh hoạt của OrchestraLLM , chúng tôi cung cấp kết quả định tuyến khi tích hợp một LM mới, cụ thể là T5-3b được tinh chỉnh với phương pháp Prompt-DST trên dữ liệu huấn luyện 5% MultiWOZ. Chúng tôi áp dụng OrchestraLLM với một SenBERT có sẵn để định tuyến giữa T5-base và T5-3b. Kết quả được hiển thị trong Bảng 6 nhấn mạnh khả năng thích ứng của OrchestraLLM trong việc hiệu quả định tuyến các ví dụ với một LM mới được giới thiệu, tất cả mà không cần bất kỳ huấn luyện bổ sung nào của bộ truy xuất.
6 Công trình Liên quan
6.1 Suy luận Thích ứng Mẫu
Để phân bổ các mức độ tài nguyên tính toán biến đổi cho việc xử lý các mẫu đầu vào khác nhau, hai loại cách tiếp cận chủ đạo đã xuất hiện trong lĩnh vực này: thoát sớm (Liu và cộng sự, 2020; Xin và cộng sự, 2021; Zhou và cộng sự, 2020) và loại bỏ token (Goyal và cộng sự, 2020; Guan và cộng sự, 2022; Kim và Cho, 2021). Salehi và cộng sự (2023) cũng nghiên cứu để chỉ đạo các mẫu khác nhau đến các mạng con với độ rộng biến đổi. Khung định tuyến được đề xuất của chúng tôi cũng nắm lấy suy luận thích ứng mẫu. Tuy nhiên, nó phân biệt chính nó bằng cách tận dụng không chỉ một mô hình đơn lẻ mà là một sự kết hợp các mô hình.
6.2 Chuyển đổi Mô hình
Đã có một khối lượng nghiên cứu ngày càng tăng tập trung vào khái niệm chuyển đổi mô hình, trong đó các ví dụ đầu vào được định tuyến một cách thông minh giữa các mô hình nhỏ và lớn dựa trên mức độ phức tạp cá nhân của chúng. Ví dụ, Madaan và cộng sự (2023) đề xuất một phương pháp tận dụng một meta-verifier bên ngoài để xác định tính đúng đắn của các dự đoán được thực hiện bởi một SLM và để quyết định liệu một ví dụ có đáng được định tuyến đến một LLM hay không. Ngược lại, cách tiếp cận của chúng tôi không đòi hỏi việc sử dụng các verifiers bổ sung. Một tập hợp các cách tiếp cận liên quan khác, được minh họa bằng công trình của Šakota và cộng sự (2023); Kag và cộng sự (2022), liên quan đến việc huấn luyện các bộ phân loại nhị phân để phân loại các ví dụ như phù hợp cho xử lý SLM hoặc LLM. Cách tiếp cận này yêu cầu bộ định tuyến được huấn luyện trên dữ liệu được gắn nhãn nơi các mô hình ngôn ngữ đã thực hiện dự đoán. Ngược lại, phương pháp của chúng tôi thể hiện khả năng tận dụng bộ truy xuất có sẵn, nâng cao tính linh hoạt của nó.
6.3 Theo dõi trạng thái đối thoại Few-Shot
Để giảm nhu cầu về dữ liệu được gắn nhãn trong DST, nhiều cách tiếp cận được đề xuất cho DST few-shot (Li và cộng sự, 2021; Lin và cộng sự, 2021; Shin và cộng sự, 2022; Hu và cộng sự, 2022). Mô hình DST few-shot tối tân là (King và Flanigan, 2023b), trong đó các tác giả tái cấu trúc DST như một nhiệm vụ lập trình Python và tận dụng Codex (Chen và cộng sự, 2021) như LLM xương sống, không còn có thể truy cập được nữa. Ngoài ra, cách tiếp cận của họ liên quan đến nhiều lần chạy giải mã cho một lượt đơn và dựa vào điểm xác suất của các token, có thể không luôn sẵn có.
7 Kết luận
Chúng tôi giới thiệu OrchestraLLM , một khung định tuyến tích hợp một cách liền mạch một SLM và một LLM, được điều phối bởi một bộ định tuyến dựa trên truy xuất. Trong quá trình suy luận, một bộ định tuyến động hướng dẫn các thể hiện đến cả LM dựa trên khoảng cách nhúng ngữ nghĩa của chúng với các mẫu LM được truy xuất, tận dụng chuyên môn của cả SLM và LLM. Đánh giá của chúng tôi về DST chứng minh rằng OrchestraLLM vượt trội hơn các hệ thống dựa trên LLM đồng thời đạt được tiết kiệm chi phí tính toán hơn 50%. Nghiên cứu này đại diện cho một bước tiến quan trọng hướng tới sự hợp tác hiệu quả của các mô hình ngôn ngữ, đặc biệt trong một hệ thống tương tác đa lượt giữa con người và máy tính như đối thoại hướng nhiệm vụ.
8 Hạn chế
Nghiên cứu của chúng tôi chứng minh lợi ích của việc kết hợp SLM và LLM để cải thiện hiệu suất nhiệm vụ đồng thời quản lý chi phí tính toán, đặc biệt trong bối cảnh các nhiệm vụ theo dõi trạng thái đối thoại. Tuy nhiên, điều quan trọng cần thừa nhận rằng khả năng áp dụng cách tiếp cận của chúng tôi có thể không mở rộng một cách liền mạch đến tất cả các loại nhiệm vụ trong lĩnh vực NLP rộng lớn hơn. Ngoài ra, trong khung hiện tại của chúng tôi, chúng tôi tập trung vào việc tận dụng một SLM và một LLM. Tuy nhiên, các ứng dụng thế giới thực thường liên quan đến một loạt rộng các nhiệm vụ đa dạng, mỗi nhiệm vụ có thể yêu cầu LMs với chuyên môn biến đổi. Như một hướng nghiên cứu tương lai, chúng tôi có ý định khám phá việc điều phối nhiều LMs đồng thời.
Tài liệu tham khảo
Namo Bang, Jeehyun Lee, và Myoung-Wan Koo. 2023. Task-optimized adapters for an end-to-end task-oriented dialogue system. Trong Findings of the Association for Computational Linguistics: ACL 2023 , trang 7355–7369, Toronto, Canada. Association for Computational Linguistics.
Jeff Barr. 2019. Amazon ec2 update. Blog Post. Truy cập: 2021-06-01.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. 2020. Language models are few-shot learners. Advances in neural information processing systems , 33:1877–1901.
Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ramadan, và Milica Gasic. 2018. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. Trong Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , trang 5016–5026.
Lingjiao Chen, Matei Zaharia, và James Zou. 2023. Frugalgpt: How to use large language models while reducing cost and improving performance. arXiv preprint arXiv:2305.05176 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, và cộng sự. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 .
Tim Dettmers, Mike Lewis, Younes Belkada, và Luke Zettlemoyer. 2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339 .
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314 .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , trang 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.
Suvodip Dey, Ramamohan Kummara, và Maunendra Desarkar. 2022. Towards fair evaluation of dialogue state tracking by flexible incorporation of turn-level performances. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , trang 318–324.
Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, và Dilek Hakkani-Tur. 2020. Multiwoz 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. Trong Proceedings of the 12th Language Resources and Evaluation Conference , trang 422–428.
Angela Fan, Edouard Grave, và Armand Joulin. 2019. Reducing transformer depth on demand with structured dropout. Trong International Conference on Learning Representations .
Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan Chakaravarthy, Yogish Sabharwal, và Ashish Verma. 2020. Power-bert: Accelerating bert inference via progressive word-vector elimination. Trong International Conference on Machine Learning , trang 3690–3699. PMLR.
Yue Guan, Zhengyi Li, Jingwen Leng, Zhouhan Lin, và Minyi Guo. 2022. Transkimmer: Transformer learns to layer-wise skim. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , trang 7275–7286.
Donghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, và Kee-Eung Kim. 2020. End-to-end neural pipeline for goal-oriented dialogue systems using gpt-2. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , trang 583–592.
Matthew Henderson, Blaise Thomson, và Jason D. Williams. 2014. The second dialog state tracking challenge. Trong Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL) , trang 263–272, Philadelphia, PA, U.S.A. Association for Computational Linguistics.
Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, và Richard Socher. 2020. A simple language model for task-oriented dialogue. Advances in Neural Information Processing Systems , 33:20179–20191.
Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, và Mari Ostendorf. 2022. In-context learning for few-shot dialogue state tracking. TrongFindings of the Association for Computational Linguistics: EMNLP 2022 , trang 2627–2643, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
V ojtˇech Hude ˇcek và Ond ˇrej Dušek. 2023. Are large language models all you need for task-oriented dialogue? Trong Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue , trang 216–228.
Wiradee Imrattanatrai và Ken Fukuda. 2023. End-to-end task-oriented dialogue systems based on schema. TrongFindings of the Association for Computational Linguistics: ACL 2023 , trang 10148–10161, Toronto, Canada. Association for Computational Linguistics.
Joel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, và Minjoon Seo. 2023. Exploring the benefits of training expert language models over instruction tuning. arXiv preprint arXiv:2302.03202 .
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu. 2020. Tinybert: Distilling bert for natural language understanding. Trong Findings of the Association for Computational Linguistics: EMNLP 2020 , trang 4163–4174.
Anil Kag, Igor Fedorov, Aditya Gangrade, Paul Whatmough, và Venkatesh Saligrama. 2022. Efficient edge inference by selective query. Trong The Eleventh International Conference on Learning Representations .
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, và Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , trang 6769–6781, Online. Association for Computational Linguistics.
Gyuwan Kim và Kyunghyun Cho. 2021. Length-adaptive transformer: Train once with length drop, use anytime with search. Trong Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) , trang 6501–6511, Online. Association for Computational Linguistics.
Brendan King và Jeffrey Flanigan. 2023a. Diverse retrieval-augmented in-context learning for dialogue state tracking. Trong Findings of the Association for Computational Linguistics: ACL 2023 , trang 5570–5585.
Brendan King và Jeffrey Flanigan. 2023b. Diverse retrieval-augmented in-context learning for dialogue state tracking. Trong Findings of the Association for Computational Linguistics: ACL 2023 , trang 5570–5585, Toronto, Canada. Association for Computational Linguistics.
François Lagunas, Ella Charlaix, Victor Sanh, và Alexander M Rush. 2021. Block pruning for faster transformers. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , trang 10619–10629.
Chia-Hsuan Lee, Hao Cheng, và Mari Ostendorf. 2021. Dialogue state tracking with a language model using schema-driven prompting. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , trang 4937–4949.
George Leopold. 2019. Aws to offer nvidia's t4 gpus for ai inferencing. Online. Truy cập: 2021-06-01.
Shuyang Li, Jin Cao, Mukund Sridhar, Henghui Zhu, Shang-Wen Li, Wael Hamza, và Julian McAuley. 2021. Zero-shot generalization in dialog state tracking through generative question answering. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , trang 1063–1074, Online. Association for Computational Linguistics.
Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul A Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu, Andrea Madotto, Eunjoon Cho, và Rajen Subba. 2021. Leveraging slot descriptions for zero-shot cross-domain dialogue state tracking. Trong Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , trang 5640–5648.
Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, và Qi Ju. 2020. FastBERT: a self-distilling BERT with adaptive inference time. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , trang 6035–6044, Online. Association for Computational Linguistics.
Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, và cộng sự. 2023. Automix: Automatically mixing language models. arXiv preprint arXiv:2310.12963 .
Paul Michel, Omer Levy, và Graham Neubig. 2019. Are sixteen heads really better than one? Advances in neural information processing systems , 32.
Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh, Lars Liden, và Jianfeng Gao. 2020. Soloist: Few-shot task-oriented dialog with a single pre-trained auto-regressive model.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, và cộng sự. Language models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551.
Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, và Pranav Khaitan. 2020. Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset. Trong Proceedings of the AAAI conference on artificial intelligence , tập 34, trang 8689–8696.
Nils Reimers và Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. TrongProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing . Association for Computational Linguistics.
Marija Šakota, Maxime Peyrard, và Robert West. 2023. Fly-swat or cannon? cost-effective language model choice via meta-modeling. arXiv preprint arXiv:2308.06077 .
Mohammadreza Salehi, Sachin Mehta, Aditya Kusupati, Ali Farhadi, và Hannaneh Hajishirzi. 2023. Sharcs: Efficient transformers through routing with dynamic width sub-networks. arXiv preprint arXiv:2310.12126 .
Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 .
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, và Kurt Keutzer. 2020. Q-bert: Hessian based ultra low precision quantization of bert. Trong Proceedings of the AAAI Conference on Artificial Intelligence , tập 34, trang 8815–8821.
Jamin Shin, Hangyeol Yu, Hyeongdon Moon, Andrea Madotto, và Juneyoung Park. 2022. Dialogue summaries as dialogue states (DS2), template-guided summarization for few-shot dialogue state tracking. TrongFindings of the Association for Computational Linguistics: ACL 2022 , trang 3824–3846, Dublin, Ireland. Association for Computational Linguistics.
Yixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta, Deng Cai, Yi-An Lai, và Yi Zhang. 2022. Multi-task pre-training for plug-and-play task-oriented dialogue system. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , trang 4661–4676.
Iulia Turc, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. Well-read students learn better: On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962 .
Qingyue Wang, Liang Ding, Yanan Cao, Yibing Zhan, Zheng Lin, Shi Wang, Dacheng Tao, và Li Guo. 2023. Divide, conquer, and combine: Mixture of semantic-independent experts for zero-shot dialogue state tracking. arXiv preprint arXiv:2306.00434 .
Ziheng Wang, Jeremy Wohlwend, và Tao Lei. 2020. Structured pruning of large language models. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , trang 6151–6162.
Chien-Sheng Wu, Steven CH Hoi, và Caiming Xiong. 2020. Improving limited labeled dialogue state tracking with self-supervision. Trong Findings of the Association for Computational Linguistics: EMNLP 2020 , trang 4462–4472.
Mengzhou Xia, Zexuan Zhong, và Danqi Chen. 2022. Structured pruning learns compact and accurate models. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , trang 1513–1528.
Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, và cộng sự. 2022. Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966 .
Ji Xin, Raphael Tang, Yaoliang Yu, và Jimmy Lin. 2021. BERxiT: Early exiting for BERT with better fine-tuning and extension to regression. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume , trang 91–104, Online. Association for Computational Linguistics.
Fanghua Ye, Jarana Manotumruksa, và Emine Yilmaz. 2021. Multiwoz 2.4: A multi-domain task-oriented dialogue dataset with essential annotation corrections to improve state tracking evaluation. arXiv preprint arXiv:2104.00773 .
Fanghua Ye, Jarana Manotumruksa, và Emine Yilmaz. 2022. MultiWOZ 2.4: A multi-domain task-oriented dialogue dataset with essential annotation corrections to improve state tracking evaluation. Trong Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue , trang 351–360, Edinburgh, UK. Association for Computational Linguistics.
Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, và Furu Wei. 2020. Bert loses patience: Fast and robust inference with early exit. Advances in Neural Information Processing Systems , 33:18330–18341.
12

--- TRANG 13 ---
A Công trình Liên quan
A.1 Suy luận Thích ứng Mẫu
Để cho phép các mức độ tài nguyên tính toán biến đổi được phân bổ để xử lý các mẫu đầu vào khác nhau, hai loại cách tiếp cận chủ đạo đã xuất hiện trong lĩnh vực này: thoát sớm và loại bỏ token. Các chiến lược thoát sớm thường kết hợp các bộ phân loại bổ sung tại các lớp trung gian trong một mô hình, quyết định liệu một ví dụ đầu vào đã cho có nên chấm dứt xử lý sớm và kiềm chế không lan truyền đến các lớp tiếp theo (Liu và cộng sự, 2020; Xin và cộng sự, 2021; Zhou và cộng sự, 2020). Mặt khác, các kỹ thuật loại bỏ token nhằm giảm động chiều dài của chuỗi đầu vào, đạt được bằng cách lựa chọn loại trừ các token nhất định từ chuỗi đầu vào, sau đó không được chuyển đến các lớp tiếp theo trong mô hình (Goyal và cộng sự, 2020; Guan và cộng sự, 2022; Kim và Cho, 2021). Salehi và cộng sự (2023) cũng nghiên cứu để chỉ đạo các mẫu khác nhau đến các mạng con với độ rộng biến đổi. Khung định tuyến được đề xuất của chúng tôi cũng nắm lấy suy luận thích ứng mẫu. Tuy nhiên, nó phân biệt chính nó bằng cách tận dụng không chỉ một mô hình đơn lẻ mà là một sự kết hợp các mô hình.
A.2 Nén Mô hình
Nén mô hình chủ yếu rơi vào ba mô hình chính: cắt tỉa, chưng cất, và lượng tử hóa. Các chiến lược cắt tỉa chủ yếu được thiết kế để giảm chi phí tính toán bằng cách lựa chọn và giữ lại một mạng con trong một mô hình lớn hơn (Fan và cộng sự, 2019; Michel và cộng sự, 2019; Wang và cộng sự, 2020; Lagunas và cộng sự, 2021; Xia và cộng sự, 2022). Ngược lại, các kỹ thuật chưng cất đòi hỏi việc huấn luyện một mô hình học sinh nhỏ gọn, với mục tiêu truyền đạt kiến thức và hiệu suất của một mô hình giáo viên lớn hơn (Sanh và cộng sự, 2019; Turc và cộng sự, 2019; Jiao và cộng sự, 2020). Cuối cùng, các phương pháp lượng tử hóa nhằm giảm nhu cầu bộ nhớ bằng cách biểu diễn các tham số mô hình với ít bit hơn, do đó đánh đổi một mức độ chính xác để nâng cao hiệu quả (Shen và cộng sự, 2020; Dettmers và cộng sự, 2022, 2023). Lưu ý rằng các phương pháp được đề cập ở trên được đặc trưng là có bản chất "tĩnh", vì chúng chủ yếu tập trung vào việc tối ưu hóa các kiến trúc mô hình cố định cho mỗi điểm dữ liệu. Ngược lại, khung định tuyến được giới thiệu trong công trình này áp dụng một quan điểm động.
13
