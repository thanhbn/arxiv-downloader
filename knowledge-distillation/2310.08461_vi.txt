# 2310.08461.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.08461.pdf
# Kích thước tệp: 1886760 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
DISTILL SPEC: CẢI THIỆN GIẢI MÃ ĐỒN ĐOÁN
THÔNG QUA CHƯNG CẤT TRI THỨC
Yongchao Zhou1,3∗, Kaifeng Lyu1,4∗, Ankit Singh Rawat1, Aditya Krishna Menon1,
Afshin Rostamizadeh1, Sanjiv Kumar1, Jean-François Kagy1†, Rishabh Agarwal2,5†
1Google Research2Google DeepMind3University of Toronto4Princeton University5Mila
TÓM TẮT
Giải mã đồn đoán (SD) tăng tốc suy luận mô hình ngôn ngữ lớn bằng cách sử dụng một
mô hình nháp nhanh hơn để tạo ra nhiều token, sau đó được xác minh song song bởi
mô hình đích lớn hơn, kết quả là văn bản được tạo theo phân phối mô hình đích. Tuy
nhiên, việc xác định một mô hình nháp nhỏ gọn mà được căn chỉnh tốt với mô hình đích
là thách thức. Để giải quyết vấn đề này, chúng tôi đề xuất DistillSpec, một phương pháp
sử dụng chưng cất tri thức để căn chỉnh tốt hơn mô hình nháp với mô hình đích trước khi
áp dụng SD. DistillSpec đưa ra hai lựa chọn thiết kế chính, mà chúng tôi chứng minh qua
nghiên cứu hệ thống là quan trọng để cải thiện sự căn chỉnh giữa nháp và đích: sử dụng
tạo dữ liệu on-policy từ mô hình nháp, và điều chỉnh hàm phân kỳ theo nhiệm vụ và chiến
lược giải mã. Đáng chú ý, DistillSpec mang lại tăng tốc 10−45% so với SD tiêu chuẩn trên
một loạt điểm chuẩn, sử dụng cả lấy mẫu tham lam và không tham lam. Chúng tôi cho
thấy mô hình đã chưng cất có thể được chuyển giao tốt cho các nhiệm vụ khác nhau với
tăng tốc trung bình 26%. Hơn nữa, chúng tôi kết hợp DistillSpec với SD có tổn thất để
đạt được kiểm soát tinh tế trên sự cân đối giữa độ trễ và hiệu suất nhiệm vụ. Cuối cùng,
trong các tình huống thực tế với các mô hình có kích thước khác nhau, đầu tiên sử dụng
chưng cất để tăng hiệu suất của mô hình đích và sau đó áp dụng DistillSpec để huấn luyện
một mô hình nháp căn chỉnh tốt có thể giảm độ trễ giải mã 6−10× với sự sụt giảm hiệu
suất tối thiểu, so với giải mã tiêu chuẩn không có chưng cất.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) đã cách mạng hóa việc hiểu và tạo ngôn ngữ tự nhiên trên các
ứng dụng đa dạng (OpenAI, 2023; Anil et al., 2023). Tuy nhiên, bản chất tạo tự hồi quy của chúng
đặt ra những thách thức tính toán đáng kể, đặc biệt trong các triển khai thời gian thực với các ràng
buộc độ trễ nghiêm ngặt (Thoppilan et al., 2022; Pope et al., 2023). Ngược lại, các mô hình ngôn
ngữ nhỏ hơn, trong khi hiệu quả về mặt tính toán, thường thiếu sức mạnh biểu đạt của các đối tác
lớn hơn và đạt hiệu suất kém. Trong khi việc giảm chi phí suy luận của các mô hình lớn hơn, ví dụ,
thông qua lượng tử hóa hoặc cắt tỉa, hoặc cải thiện hiệu suất của các mô hình nhỏ hơn, ví dụ, thông
qua chưng cất tri thức (KD) (Hinton et al., 2015), tạo thành các phương pháp tự nhiên để cho phép
sự cân đối thuận lợi giữa hiệu suất và chi phí suy luận, các phương pháp này thường xuyên dẫn đến
khoảng cách hiệu suất không thể chấp nhận được so với các mô hình lớn chất lượng cao. Điều này
đã truyền cảm hứng cho một văn học ngày càng phát triển về thiết kế các cơ chế kết hợp cả mô hình
lớn và nhỏ tại thời điểm suy luận để xấp xỉ hiệu suất của các mô hình lớn hơn mà không phải chịu
chi phí tính toán cao của chúng.

Trong số các phương pháp thông thường, phân tầng mô hình nhằm xác định các trường hợp dễ dàng
mà các mô hình nhỏ hơn đủ để đạt hiệu suất tốt, và chỉ yêu cầu các mô hình lớn hơn trên một tập
con các trường hợp khó (Rowley et al., 1998; Xu et al., 2014) hoặc nhiệm vụ (Cai et al., 2023b).
Khác với phân tầng cấp độ nhiệm vụ hoặc trường hợp như vậy, giải mã đồn đoán (SD) (Leviathan
et al., 2023; Chen et al., 2023) khai thác tính biến thiên cấp độ token trong nhu cầu tính toán trong
quá trình suy luận LLM bằng cách gọi tương tác một mô hình "nháp" nhỏ và một mô hình "đích"
lớn. Tại một giai đoạn nhất định trong quá trình suy luận, mô hình nháp tạo ra các token ứng cử
liên tiếp cho nhiều bước suy luận thông qua giải mã tự hồi quy. Mô hình đích sau đó xác minh các
token ứng cử thông qua giải mã song song, và sử dụng lấy mẫu từ chối để chấp nhận một tập con
các token ứng cử tại các vị trí liền kề.

Mục tiêu chính của SD là tăng tốc tạo văn bản trong khi đảm bảo rằng các token được giải mã tuân
theo phân phối mô hình đích. SD dựa trên hiểu biết rằng chi phí kết hợp của giải mã tự hồi quy với
một mô hình nháp nhỏ theo sau bởi xác minh song song với mô hình đích là
∗Nghiên cứu sinh tại Google Research.†Đóng góp tư vấn. Tác giả liên hệ:
<yczhou@cs.toronto.edu>, <jfkagy@google.com>, và <rishabhagarwal@google.com>.
1arXiv:2310.08461v2  [cs.CL]  31 Mar 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
50100150Tăng tốc Giải mã Đồn đoán (%)+25% +19%
XSum+16%+18%
GSM8K+41%+46%
CNNDM+10%+10%
WMT+23%+28%
BBH (Chuyển giao)DistillSpec (Tham lam) SD Tiêu chuẩn (Tham lam) DistillSpec (Không tham lam) SD Tiêu chuẩn (Không tham lam)

Hình 1: So sánh hiệu suất của giải mã đồn đoán tiêu chuẩn (SD) so với DistillSpec được đề xuất của chúng tôi, với
các mô hình kích thước nhỏ và XL từ họ T5 v1.1 (Raffel et al., 2020) được sử dụng làm mô hình nháp và mô hình
đích, tương ứng. DistillSpec tăng cường tốc độ SD bằng cách căn chỉnh tốt hơn nháp với đích thông qua chưng cất tri
thức white-box, dẫn đến cải thiện tăng tốc nhất quán 10−45% so với SD tiêu chuẩn trên các tập dữ liệu khác nhau.
Mô hình nháp đã chưng cất từ GSM8K chuyển giao tốt sang 23 nhiệm vụ BigBenchHard chưa thấy (Suzgun et al.,
2022), dẫn đến tăng tốc trung bình 26%. Xem § 5.1 để biết thêm chi tiết.

thấp hơn chi phí giải mã tự hồi quy chỉ với mô hình đích. Tuy nhiên, việc giảm chi phí suy luận
được thực hiện hoặc cải thiện độ trễ phụ thuộc quan trọng vào tỷ lệ chấp nhận của các token do
nháp tạo ra bởi mô hình đích, điều này có thể được chỉ ra là liên quan trực tiếp đến sự căn chỉnh
giữa các phân phối token của mô hình nháp và đích. Do đó, một ứng dụng thành công của SD phụ
thuộc vào việc xác định một mô hình nháp nhỏ gọn mà đồng thời có chi phí giải mã tự hồi quy nhỏ
và được căn chỉnh chặt chẽ với mô hình đích.

Trong công việc này, chúng tôi đề xuất DistillSpec, một phương pháp mới dựa trên KD (Hinton
et al., 2015) để có được một mô hình nháp hiệu quả. Không giống như ứng dụng tiêu chuẩn của
KD chủ yếu tập trung vào cải thiện hiệu suất nhiệm vụ của một mô hình học sinh nhỏ, DistillSpec
nhằm căn chỉnh mô hình học sinh (nháp) với mô hình giáo viên (đích) để tăng cường tỷ lệ chấp
nhận trong SD. Chúng tôi thực hiện một khám phá toàn diện về quá trình chưng cất để tăng tốc
SD, xem xét một số yếu tố bao gồm thành phần của dữ liệu huấn luyện, lựa chọn các hàm phân kỳ
để định nghĩa mục tiêu huấn luyện cho KD, và các chiến lược giải mã. Đáng chú ý, các phát hiện
của chúng tôi nhấn mạnh rằng việc sử dụng dữ liệu do mô hình tạo ra là quan trọng để đảm bảo sự
căn chỉnh mạnh mẽ giữa học sinh và giáo viên trên các nhiệm vụ khác nhau thông qua KD, và rằng
việc lựa chọn hàm phân kỳ có hiệu suất tốt nhất trong DistillSpec phụ thuộc rất nhiều vào nhiệm vụ
và nhạy cảm với chiến lược giải mã (tức là, tham lam so với không tham lam). Hơn nữa, chúng tôi
khám phá tính hữu dụng của DistillSpec cho SD có tổn thất (Leviathan et al., 2023) cho phép lấy
mẫu khỏi phân phối mô hình đích. Chúng tôi cho thấy rằng việc kết hợp DistillSpec với SD có tổn
thất cho phép kiểm soát tinh tế hơn trên sự cân đối giữa độ trễ và hiệu suất nhiệm vụ.

Cuối cùng, chúng tôi thực hiện một nghiên cứu hệ thống về cách thiết kế một sơ đồ suy luận hiệu
quả trong một môi trường thực tế mà người ta có quyền truy cập vào nhiều mô hình có kích thước
và chất lượng tăng dần. Tận dụng những hiểu biết mà chúng tôi đã đặt ra về KD và SD, nghiên cứu
của chúng tôi kết luận rằng chiến lược hiệu quả nhất bao gồm đầu tiên chưng cất một mô hình lớn
thành một mô hình nhỏ hơn như mô hình đích tiềm năng để tối ưu hóa hiệu suất, theo sau bởi
DistillSpec để chưng cất một mô hình thậm chí nhỏ hơn để được sử dụng làm mô hình nháp trong
SD. Phương pháp này dẫn đến một sự giảm đáng kể 6−10× về độ trễ, so với một mô hình đích độc
lập không được chưng cất cùng kích thước, với sự suy giảm hiệu suất tối thiểu.

Các đóng góp chính của chúng tôi là:
(i) Chúng tôi đề xuất DistillSpec, một phương pháp sử dụng KD để tăng cường sự căn chỉnh mô
hình nháp với mô hình đích (§4), và cho thấy phương pháp của chúng tôi có thể cải thiện tốc độ
SD 10−45% trong khi bảo tồn hiệu suất mô hình trên các tập dữ liệu đa dạng dưới lấy mẫu tham
lam và không tham lam (Hình 1).
(ii) Chúng tôi tiến hành một phân tích mở rộng về công thức chưng cất tối ưu (§5.2) cho sự căn
chỉnh mô hình, bao gồm các yếu tố như tạo dữ liệu huấn luyện và các phân kỳ khác nhau, và nhấn
mạnh những khác biệt giữa KD tiêu chuẩn và chưng cất được điều chỉnh cho SD.
(iii) Chúng tôi mở rộng DistillSpec sang SD có tổn thất, cho phép kiểm soát tinh tế trên sự cân đối
chất lượng-độ trễ. Hơn nữa, chúng tôi cung cấp hiểu biết để kết hợp KD và SD khi có một số mô
hình (§5.3).

2 CÔNG VIỆC LIÊN QUAN
Giải mã đồn đoán (SD). Do bản chất tuần tự vốn có của giải mã tự hồi quy,
nút cổ chai độ trễ chính trong suy luận LLM phát sinh từ các hoạt động đọc/ghi bộ nhớ hơn là
các tính toán số học (Pope et al., 2023). Giải mã đồn đoán (Leviathan et al., 2023;

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Chen et al., 2023) (SD) giải quyết thách thức này bằng cách sử dụng một mô hình nháp nhỏ gọn để
tạo ra một lô token tuần tự, trong khi xác thực chúng song song với một mô hình đích lớn hơn. Trước
SD, các mô hình tính toán song song khác nhau đã được khám phá cho các mô hình tự hồi quy, bao
gồm lấy mẫu song song khối (Stern et al., 2018), giải mã hung hăng nông (Sun et al., 2021), và giải
mã hung hăng (Ge et al., 2022). Tuy nhiên, những phương pháp này không dễ dàng thích ứng với
các mô hình ngôn ngữ điển hình do các độ lệch tiềm ẩn từ phân phối của mô hình đích, các ràng
buộc đầu vào nghiêm ngặt, hoặc hỗ trợ hạn chế cho lấy mẫu ngẫu nhiên tổng quát. Đáng chú ý, các
biến thể gần đây của SD đã xem xét các tương tác khác nhau giữa mô hình nháp và đích để giảm
tính toán không cần thiết (Kim et al., 2023) và tích hợp tính toán song song dọc theo trục lô, đôi
khi kết hợp với xác minh cây token, như được thấy trong SpecTr (Sun et al., 2023), SpecInfer (Miao
et al., 2023), và Medusa (Cai et al., 2023a). Ngược lại, công việc của chúng tôi tập trung vào việc
tăng cường SD bằng cách cải thiện sự căn chỉnh giữa mô hình nháp nhỏ và mô hình đích lớn thông
qua KD, điều này không yêu cầu bất kỳ thay đổi nào đối với cơ sở hạ tầng phục vụ đã triển khai SD
và bổ sung cho các biến thể gần đây của SD. Hơn nữa, chúng tôi tiến hành một nghiên cứu hệ thống
về SD có tổn thất để cung cấp kiểm soát tinh tế trên sự cân đối giữa chất lượng và độ trễ cho các
mô hình phục vụ cụ thể.

Chưng cất tri thức (KD) cho LLM. KD (Bucilu ˇa et al., 2006; Hinton et al., 2015),
huấn luyện các mô hình học sinh nhỏ hơn chất lượng cao với sự giám sát của các mô hình giáo viên
lớn hơn, đã nổi lên như một kỹ thuật quan trọng để giảm chi phí suy luận trong khi duy trì chất
lượng mô hình trên một loạt các lĩnh vực. Trong bối cảnh của LLM, các ứng dụng trước đây của
KD (Taori et al., 2023; Fu et al., 2023) chủ yếu tập trung vào KD black-box, trong đó chỉ các thế
hệ đầu ra của giáo viên, thường thông qua API, có thể truy cập được trong quá trình huấn luyện
học sinh. Tuy nhiên, với sự phổ biến của các LLM mã nguồn mở (Zhang et al., 2022; Touvron et
al., 2023), cho phép truy cập vào trọng số và logit của giáo viên, có một sự quan tâm ngày càng tăng
đối với KD white-box. KD white-box cho phép các mô hình học sinh hưởng lợi từ các tín hiệu giám
sát phong phú hơn được cung cấp bởi các mô hình giáo viên white-box, dẫn đến khả năng ngôn ngữ
được tăng cường (Agarwal et al., 2023; Gu et al., 2023; Wen et al., 2023).

Không giống như các công việc trước đây tập trung vào tạo ra các mô hình học sinh độc lập có khả
năng cao, chúng tôi khai thác KD để thúc đẩy sự hợp tác chặt chẽ hơn giữa các mô hình nhỏ hơn và
lớn hơn trong SD, điều này có thể đặc biệt có giá trị khi một mô hình đã chưng cất nhỏ một mình
không thể đáp ứng các yêu cầu chất lượng nghiêm ngặt. Trong khi Stern et al. (2018) sử dụng một
phương pháp KD black-box (SeqKD) để tăng cường giải mã song song theo khối, các mẫu của họ
được tạo ra từ mô hình đích lớn, điều này tốn kém một cách cấm đoán đối với LLM. Hơn nữa, họ
bỏ qua các logit của mô hình giáo viên và huấn luyện mô hình nháp của họ chỉ sử dụng các nhãn
giáo viên một-hot—một lựa chọn hợp lý cho giải mã tham lam nhưng một lựa chọn kém hiệu quả
hơn cho lấy mẫu không tham lam (Hình 2). Đồng thời, Liu et al. (2023) đề xuất cải thiện SD sử
dụng KD, nhưng họ giả định một thiết lập trực tuyến với phân phối truy vấn thay đổi, và tập trung
vào cải thiện tỷ lệ chấp nhận hơn là giảm độ trễ thực tế.

3 BỐI CẢNH: GIẢI MÃ ĐỒN ĐOÁN
Ký hiệu. Cho một chuỗi đầu vào x bao gồm các token từ một từ vựng được định nghĩa trước, một
mô hình ngôn ngữ M cung cấp một phân phối trên các chuỗi đầu ra có thể y. Giả sử chúng ta sử
dụng SD với một mô hình nháp nhỏ gọn Mq để hỗ trợ một mô hình đích lớn hơn Mp. Đặt p(yt|x,
y<t) và q(yt|x, y<t) đại diện cho các phân phối điều chỉnh dự đoán token tiếp theo tại bước thời
gian t cho Mp và Mq, tương ứng, cho bối cảnh ρ={x, y<t}. Cho đầu vào x như tiền tố, đặt p≤T(y|x)
và q≤T(y|x) đại diện cho các phân phối điều chỉnh chuỗi y được lấy mẫu tự hồi quy từ Mp và Mq,
tương ứng, trong đó việc tạo dừng lại khi một token kết thúc chuỗi được lấy mẫu hoặc độ dài chuỗi
tối đa T được đạt đến. Để đơn giản, chúng tôi sử dụng p(yt) và q(yt) như các cách viết tắt cho
p(yt|x, y<t) và q(yt|x, y<t), bất cứ khi nào bối cảnh ρ rõ ràng. Tương tự, p≤T(y) và q≤T(y) phục
vụ như các cách viết tắt cho p≤T(y|x) và q≤T(y|x), bất cứ khi nào đầu vào x rõ ràng.

Lấy mẫu đồn đoán. SD tiêu chuẩn sử dụng một thủ tục được gọi là lấy mẫu đồn đoán để tạo ra
token từ mô hình nháp trong khi duy trì cùng một phân phối đầu ra như mô hình đích. Như được
chi tiết trong Thuật toán A.1 (Phụ lục), mỗi bước của SD hoạt động như sau. Đầu tiên, một khối γ
token, được ký hiệu là yt, . . . , yt+γ−1, được lấy mẫu tự hồi quy từ q(yt), . . . , q(yt+γ−1). Tiếp theo,
γ token được xác minh song song bằng cách truyền chúng đến Mp như một khối toàn bộ, mà tuần
tự chấp nhận token yt+i với xác suất min(1, p(yt+i)/q(yt+i)). Nếu bất kỳ token yt+i nào bị từ chối
trước khi kết thúc khối, các token tiếp theo bị loại bỏ và token bị từ chối được lấy mẫu lại từ phân
phối điều chỉnh p′(yt+i) ∝ max(0, p(yt+i) − q(yt+i)); ngược lại, các token được soạn thảo đều được
chấp nhận và một token bổ sung được lấy mẫu từ p(yt+γ) và được thêm vào chuỗi đầu ra. Quá
trình này đảm bảo rằng chuỗi các token được chấp nhận và lấy mẫu lại tuân theo cùng một phân
phối đầu ra

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
như p(yt+i) (Leviathan et al., 2023). Thủ tục được lặp lại cho đến khi một token kết thúc chuỗi được
chấp nhận, hoặc độ dài chuỗi tối đa T đã được đạt đến.

Thước đo hiệu quả: tỷ lệ chấp nhận. Mỗi bước SD mất một lượng thời gian không đổi, vì vậy
thời gian wall-clock tỷ lệ tuyến tính với số lượng bước. Số này bằng tổng số lần mà mô hình đích từ
chối một token, cộng với số lượng khối được chấp nhận như một toàn bộ, trong đó thuật ngữ sau
nhỏ đối với γ lớn. Điều này thúc đẩy chúng tôi sử dụng tỷ lệ chấp nhận làm thước đo hiệu quả thay
thế cho thời gian wall-clock. Đối với một quá trình SD lý tưởng với γ=∞, chúng tôi định nghĩa tỷ
lệ chấp nhận cấp độ chuỗi α(x) cho một đầu vào x nhất định như sau:
α(x) := E[số lượng token được chấp nhận trong việc tạo ra y] / E[số lượng token trong y] = Ey∼p≤T(y|x)[Σ|y|t=1 β(x, y<t)] / Lp(x), (1)

trong đó β(x, y<t) := Eyt∼q(yt)[min(1, p(yt)/q(yt))] là tỷ lệ chấp nhận cấp độ token, và các kỳ
vọng được lấy trên tính ngẫu nhiên trong SD. Vì lấy mẫu đồn đoán bảo tồn phân phối đầu ra của
mô hình đích, mẫu số đơn giản bằng độ dài kỳ vọng của đầu ra đích Lp(x) := Ey∼p≤T(y|x)[|y|], mà
không thay đổi theo lựa chọn mô hình nháp. Do đó, tỷ lệ chấp nhận liên quan trực tiếp đến tổng số
kỳ vọng của các token bị từ chối (1−α(x))·Lp(x), mà giới hạn dưới số bước SD kỳ vọng.

Thước đo hiệu quả: hiệu quả khối. Trong thực tế, SD thường được sử dụng với kích thước khối
hữu hạn cố định γ; do đó, một thước đo hiệu quả liên quan hơn là hiệu quả khối τ. Cho một đầu
vào x, chúng tôi tính hiệu quả khối τ(x) như số lượng kỳ vọng của các token được chấp nhận mỗi
khối. Lưu ý rằng, đối với một kích thước khối nhất định γ, giá trị tối đa của τ(x) là γ+1, tương ứng
với trường hợp mà tất cả các token được soạn thảo được chấp nhận và được tăng cường với một
token bổ sung được lấy mẫu bởi mô hình đích. Nếu chúng ta giả định rằng các tỷ lệ cấp độ token
β(x, y<t) là i.i.d., thì tỷ lệ chấp nhận cấp độ chuỗi thỏa mãn α=E[β] và τ(x) = (1−αγ+1)/(1−α)
(Leviathan et al., 2023).

Cải thiện thời gian wall-clock. Đối với hiệu quả khối τ(x) nhất định, tăng tốc kỳ vọng của SD
được cho bởi τ(x)/(cγ+1), trong đó độ trễ tương đối c là tỷ số giữa các thời gian trôi qua khi thực
hiện một lần chuyển tiếp đơn qua mô hình nháp Mq và qua mô hình đích Mp.

4 DISTILL SPEC: CHƯNG CẤT TRI THỨC CHO GIẢI MÃ ĐỒN ĐOÁN
Như được mô tả trong § 3, giải mã đồn đoán (SD) tận dụng một mô hình nhỏ (nháp) để giảm độ trễ
giải mã từ phân phối mô hình lớn hơn (đích) mà không có bất kỳ sự sụt giảm hiệu suất nào. Tuy
nhiên, tăng tốc được thực hiện phụ thuộc quan trọng vào mức độ "căn chỉnh tốt" của mô hình nháp
với mô hình đích. Trong công việc này, chúng tôi đề xuất DistillSpec, một khung tổng quát cải thiện
SD bằng cách căn chỉnh tốt hơn mô hình đích và mô hình nháp sử dụng chưng cất tri thức white-box
(KD). Chúng tôi đầu tiên trình bày huấn luyện mô hình nháp dựa trên KD, và làm nổi bật cách mục
tiêu của chúng tôi về tăng cường SD thông qua KD ảnh hưởng đến việc lựa chọn phương pháp tạo
dữ liệu huấn luyện và hàm phân kỳ—hai thành phần chính của DistillSpec. Sau đó chúng tôi thảo
luận về cách DistillSpec có thể được mở rộng sang SD có tổn thất.

Đặt mô hình nháp Mθq được tham số hóa bởi θ. DistillSpec sử dụng các dự đoán từ mô hình đích
Mp như một nguồn giám sát trong việc huấn luyện mô hình nháp Mθq. Chúng tôi giả định quyền
truy cập white-box vào cả hai mô hình, tức là, chúng tôi có thể có được các phân phối token tiếp
theo p(yt) và q(yt) của họ, và do đó chúng tôi có thể tạo ra các mẫu từ cả hai mô hình. Cho một
hàm phân kỳ D đo sự lệch lạc giữa hai phân phối, huấn luyện mô hình nháp dựa trên KD tìm cách
tối thiểu hóa phân kỳ giữa các phân phối giáo viên (đích) và học sinh (nháp) trên một tập huấn
luyện G:
θ∗:= arg min E(x,y)∼G[D(Mp∥Mθq)(y|x)], (2)

trong đó D(Mp∥Mθq)(y|x) = 1/|y| Σ|y|t=1 D(p(·|x, y<t)∥qθ(·|x, y<t)). Chúng tôi lưu ý rằng sự linh
hoạt trong cách G được xây dựng và lựa chọn phân kỳ D tạo ra các thuật toán KD khác nhau có thể.
Ví dụ, G có thể bao gồm các cặp đầu vào-đầu ra cụ thể cho nhiệm vụ (X, Y) hoặc các chuỗi được
tạo ra từ Mp hoặc Mθq. Trong khi forward KL (DFKL) là phân kỳ thường được sử dụng cho KD
(Hinton et al., 2015), các công việc gần đây (Agarwal et al., 2023; Wei et al., 2022) đã chỉ ra hiệu
quả của các phân kỳ thay thế, bao gồm reverse KL (DRKL), phân kỳ Jensen–Shannon (DJSD[β]), và
khoảng cách biến thiên tổng (DTVD). Thêm chi tiết về mỗi phân kỳ có thể được tìm thấy trong Phụ
lục A.1. Bảng 1 tóm tắt các công thức chưng cất khác nhau, mỗi cái là một trường hợp chuyên biệt
của Thuật toán A.2.

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 1: Tóm tắt các thuật toán KD khác nhau về mặt dữ liệu huấn luyện G và phân kỳ D (cf. Eq. 2). Wen
et al. (2023); Agarwal et al. (2023) cũng xem xét D khác; chúng tôi liệt kê cái đại diện nhất.

Tên Phân kỳ Dữ liệu Huấn luyện (G)
SeqKD (Kim & Rush, 2016) (KD Black-box) FKL Dữ liệu được tạo bởi giáo viên Mp
Supervised KD (Hinton et al., 2015) FKL Tập dữ liệu cố định của các cặp đầu vào-đầu ra
ImitKD (Lin et al., 2020) FKL Tập dữ liệu cố định + dữ liệu được tạo bởi Mθq
f-Distill (Wen et al., 2023) TVD Dữ liệu được tạo bởi Mθq và Mp
On-policy GKD (Agarwal et al., 2023) FKL / JSD Dữ liệu on-policy từ học sinh Mθq

Các lựa chọn của chúng tôi cho G và D được hướng dẫn bởi cách mô hình đã chưng cất kết quả,
một khi được sử dụng làm mô hình nháp, cải thiện tốc độ của SD. Hướng tới điều này, chúng tôi
đầu tiên làm nổi bật vai trò mà DTVD giữa p(yt) và q(yt) đóng trong việc quyết định tỷ lệ chấp
nhận (§ 3)—một thước đo hiệu quả chính cho SD.

TVD như đại diện cho tỷ lệ chấp nhận. Leviathan et al. (2023, Hệ quả 3.6) cho thấy rằng tỷ lệ
chấp nhận cấp độ token β(x, y<t) thỏa mãn β(x, y<t) = 1−DTVD(p(yt), q(yt)). Do đó, Eq. 1 ngụ ý
rằng việc tối đa hóa tỷ lệ chấp nhận cấp độ chuỗi α(x) tương đương với việc tối thiểu hóa DTVD
kỳ vọng giữa p(yt) và q(yt) trên phân phối chuỗi đầu ra của Mp, tức là:
α(x) = 1−Ey∼p≤T(y|x)[Σ|y|t=1 DTVD(p(yt), q(yt))]/Lp(x). (3)

Lựa chọn phân kỳ. Dựa trên Eq. 3, có vẻ như việc trực tiếp tối thiểu hóa DTVD có thể là một mục
tiêu có nguyên tắc cho chưng cất mô hình nháp. Trong khi việc tối ưu hóa DTVD(p, q) được truyền
cảm hứng lý thuyết, nghiên cứu thực nghiệm của chúng tôi cho thấy rằng một mục tiêu như vậy có
thể không mang lại kết quả tối ưu một cách nhất quán. Chúng tôi thấy rằng lựa chọn phân kỳ phù
hợp nhất phụ thuộc rất nhiều vào nhiệm vụ (§ 5.2).

Lựa chọn dữ liệu huấn luyện. Về G, người ta có thể dựa vào một tập dữ liệu ground-truth hiện
có, tuy nhiên phân phối đầu ra của giáo viên có thể lệch khỏi phân phối ground-truth mặc dù giáo
viên đã được tinh chỉnh trên nó. Hơn nữa, các tập dữ liệu ground-truth thường bị hạn chế về kích
thước, vì vậy việc huấn luyện chỉ trên dữ liệu như vậy có thể dẫn đến overfitting. Để giảm thiểu
những vấn đề này, chúng tôi sử dụng các đầu ra do mô hình tạo ra cho chưng cất. Cụ thể, chúng
tôi nhắc mô hình với một đầu vào nhiệm vụ được lấy mẫu từ một tập dữ liệu huấn luyện ground-
truth, và sử dụng phản hồi mô hình như dữ liệu cho chưng cất. Cả mô hình giáo viên và học sinh
đều có thể được sử dụng để tạo ra các ví dụ chưng cất.

Dữ liệu chưng cất do mô hình tạo ra. Eq. 3 gợi ý việc tối ưu hóa DTVD kỳ vọng trên các đầu ra
được tạo ra từ giáo viên. Giải mã từ một giáo viên lớn thường tốn kém một cách cấm đoán, đặc biệt
ở quy mô tập dữ liệu cần thiết cho KD. Để giảm chi phí tạo, chúng tôi khám phá việc sử dụng dữ
liệu on-policy trong quá trình chưng cất, tức là, các chuỗi đầu ra được lấy mẫu từ chính học sinh.
Ngoài việc hiệu quả hơn về mặt tính toán so với việc tạo giáo viên, phương pháp này được thúc đẩy
bởi Gu et al. (2023); Agarwal et al. (2023), những người đã chỉ ra rằng việc chưng cất trên dữ liệu
on-policy cải thiện hiệu suất nhiệm vụ của học sinh. Tuy nhiên, khác với những công việc trước đây
này, trọng tâm chính của chúng tôi là cải thiện sự căn chỉnh học sinh-giáo viên. Do đó, có thể không
rõ ràng ngay lập tức liệu việc tối thiểu hóa DTVD kỳ vọng trên dữ liệu on-policy (do học sinh tạo
ra) có đảm bảo một tỷ lệ chấp nhận được cải thiện hay không, mà được tính như một kỳ vọng trên
phân phối đầu ra của giáo viên (cf. Eq. 3). Kết quả sau đây của chúng tôi cho thấy rằng điều này
thực sự là trường hợp.

Định lý 4.1. Đối với SD, nếu mô hình nháp Mθq đạt được tổn thất KD on-policy ϵ = Ex∼X,y∼q≤T(y|x)[DTVD(Mp∥Mθq)(y|x)], thì tỷ lệ chấp nhận cấp độ chuỗi ít nhất là
Ex∼X[α(x)] ≥ 1−T·Ex∼X[T/Lp(x)]ϵ. (4)

Khi độ dài đầu ra đích luôn là T, giới hạn đơn giản hóa thành Ex∼X[α(x)] ≥ 1−Tϵ.

Chúng tôi hoãn chứng minh đến Phụ lục A.2. Một cách trực quan, nó xây dựng dựa trên những hiểu
biết sau đây. Nếu tổn thất KD on-policy nhỏ, thì, đối với bất kỳ 1≤t≤T nào, cùng một tổn thất được
đánh giá chỉ tại token thứ t cũng nên nhỏ. Vì việc tạo token đầu tiên độc lập với bất kỳ token nào
khác, một giá trị nhỏ của tổn thất KD on-policy đảm bảo rằng các phân phối token đầu tiên của mô
hình nháp và đích là gần. Sau đó, một lập luận quy nạp cho thấy rằng một khi nháp và đích tương
tự trên t token đầu tiên, các phân phối của token thứ (t+1) cũng nên gần. Chứng minh của chúng
tôi làm cho lập luận này trở nên nghiêm ngặt bằng cách sử dụng các biểu diễn biến thiên của DTVD,
dẫn đến một giới hạn lỗi tuyến tính trong T.

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
2.602.803.003.203.40Hiệu quả Khối (τ)
Không có ChưngcấtXSum
4.004.505.00
Không có ChưngcấtCNNDM
4.804.905.005.10
Không có ChưngcấtWMT
3.253.503.754.004.25
Không có ChưngcấtGSM8KSupervisedKD SeqKD ImitKD f-Distill GKD

Hình 2: Chưng cất tăng cường hiệu quả khối (τ) trên các tập dữ liệu đa dạng, làm nổi bật tính ưu việt của dữ liệu
do mô hình tạo ra so với dữ liệu ground truth cố định (SupervisedKD) và nhấn mạnh tầm quan trọng của chưng cất
white-box trong việc giải quyết hiệu suất kém của SeqKD.

DistillSpec với SD có tổn thất. DistillSpec tăng cường hiệu quả SD mà không có bất kỳ tổn thất
chất lượng nào so với mô hình đích Mp. Trong các ứng dụng thực tế, một sự giảm chất lượng mô
hình có thể được biện minh để hỗ trợ suy luận thậm chí nhanh hơn. Đối với các tình huống như
vậy, chúng tôi mở rộng DistillSpec sang SD có tổn thất (Leviathan et al., 2023), sử dụng một hàm
khoan dung f(p, ϵ) mà sửa đổi xác suất chấp nhận từ min(1, p(yt)/q(yt)) thành min(1, f(p(yt),
ϵ)/q(yt)) (cf. § 3). Ở đây f: [0,1]2→R+ tăng và giảm theo đối số thứ nhất và thứ hai của nó, tương
ứng, và ϵ∈[0,1] là một tham số tự do (cf. Thuật toán A.1). Trong công việc này, chúng tôi đánh giá
nhiều lựa chọn cho các hàm khoan dung: flin(p, ϵ) = p/ϵ, fsq(p, ϵ) = p/ϵ2, và fexp(p, ϵ) = pϵ. Ví
dụ, khi hàm khoan dung là fsq(p, ϵ) và ϵ = 0.1, token x được lấy mẫu từ q(yt) trở nên có khả
năng được chấp nhận bởi đích cao gấp trăm lần, do đó cho phép suy luận nhanh hơn với chi phí
của một sự sụt giảm tiềm ẩn trong chất lượng tạo. Khoan dung đã được thảo luận bởi Leviathan et
al. (2023) trong bối cảnh của flin và việc xử lý của họ chỉ tập trung vào cải thiện độ trễ, trong khi
chúng tôi khám phá việc sử dụng các hàm khoan dung khác nhau như một cơ chế kiểm soát chính
xác để đạt được hồ sơ chất lượng-độ trễ mong muốn.

5 THỰC NGHIỆM
5.1 TĂNG CƯỜNG GIẢI MÃ ĐỒN ĐOÁN THÔNG QUA CHƯNG CẤT
Chúng tôi đánh giá hiệu quả của KD trong việc cải thiện tốc độ của giải mã đồn đoán (SD). Chúng
tôi tuân theo Leviathan et al. (2023) và điều tra tác động của nó lên tỷ lệ chấp nhận α, hiệu quả
khối τ, và tăng tốc độ trễ thực tế với kích thước lô là 1 dưới lấy mẫu tham lam (T=0) và lấy mẫu
nhiệt độ tiêu chuẩn (T=1).

Nhiệm vụ và mô hình. Theo Leviathan et al. (2023), chúng tôi đánh giá hai loại mô hình: 1) Các
mô hình Transformer chỉ giải mã giống GPT được huấn luyện trên nhiệm vụ LM1B (Chelba et al.,
2013) sử dụng mục tiêu tự hồi quy tiêu chuẩn, trong đó các mô hình đích và nháp có 234M và 33M
tham số, tương ứng; và 2) các mô hình encoder-decoder T5 v1.1 tiêu chuẩn (Raffel et al., 2020)
được tinh chỉnh trên bốn nhiệm vụ khác nhau, với T5-XL (3B) và T5-Small (77M) phục vụ như các
mô hình đích và nháp, tương ứng. Chúng tôi sử dụng hai tập dữ liệu từ bài báo T5, cụ thể là WMT
EnDe (Bojar et al., 2014) và CNN/DM (Hermann et al., 2015), xử lý dịch thuật và tóm tắt văn bản,
tương ứng. Hai nhiệm vụ còn lại được sử dụng để kiểm tra các mô hình T5 là XSum (Narayan et
al., 2018) và GSM8K (Cobbe et al., 2021), xử lý tóm tắt trừu tượng và lý luận số học, tương ứng.
Xem Phụ lục B để biết thêm chi tiết.

Tăng tốc giải mã. Hình 1 cho thấy rằng tác động của chưng cất lên tốc độ SD rõ ràng, mang lại
cải thiện 10−46% một cách nhất quán trên các tập dữ liệu khác nhau. Hiệu ứng này rõ rệt nhất khi
sử dụng giải mã tham lam. Hiệu suất của KD cho các kích thước khối khác nhau và các chiến lược
giải mã trên năm tập dữ liệu được trình bày trong Bảng C.1 (Phụ lục). Những phát hiện này chứng
minh rằng KD đáng kể tăng cường tỷ lệ chấp nhận và hiệu quả khối của SD cho cả mô hình chỉ
giải mã và encoder-decoder trên tất cả các tập dữ liệu. Các thuật toán chưng cất sử dụng dữ liệu do
mô hình tạo ra liên tục vượt trội hơn các phương pháp khác, dẫn đến tăng tốc bổ sung ~20% so với
SD tiêu chuẩn trên LM1B, XSum, CNN/DM, và GSM8K.

Hiệu quả khối. Hình 2 trình bày một so sánh hiệu quả khối trên các thuật toán khác nhau, sử dụng
lấy mẫu nhiệt độ (T=1) với kích thước khối γ=7. Hình minh họa tính hữu dụng của dữ liệu do mô
hình tạo ra: việc sử dụng dữ liệu ground-truth (tức là, Supervised KD) xếp hạng thấp nhất trên tất
cả các cài đặt. Ngược lại, f-Distill và GKD, chỉ sử dụng dữ liệu do mô hình tạo ra, vượt trội đáng
kể so với các biến thể KD khác. Hiệu suất kém của SeqKD, mặc dù được huấn luyện thuần túy trên
dữ liệu

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
0 20 40
Hiệu quả Huấn luyện (Thời gian Wall trong giờ)0.620.640.660.680.700.72Tỷ lệ Chấp nhận (α)
SupKD
ImitKD
SeqKD
f-Distill
GKD
(a) tỷ lệ chấp nhận vs. thời gian wall
0 250 500 750 1000 1250
Chỉ số Dữ liệu Kiểm tra Được Sắp xếp23456Hiệu quả Khối (τ)
Drafter Đã Chưng cất
Drafter Tiêu chuẩn (b) hiệu quả khối cấp độ trường hợp (τ)
2.5 3.0 3.5 4.0
Hiệu quả Khối Lý thuyết2.53.03.54.04.5Hiệu quả Khối Thực nghiệmy=xxsum
cnndm
wmt
gsm8k (c) sự đồng ý của τ

Hình 3: (a) KD white-box sử dụng logit giáo viên và dữ liệu do mô hình tạo ra là quan trọng. Dữ liệu on-policy
của mô hình nháp có thể hiệu quả như dữ liệu mô hình đích. GKD đạt được cải thiện hiệu suất thời gian wall tốt
nhất trên XSum. (b) Chưng cất cải thiện hiệu quả khối cho tất cả các ví dụ trên GSM8K. (c) Hiệu quả khối thực
nghiệm phù hợp tốt với đối tác lý thuyết dựa trên DTVD.

được tạo ra bởi mô hình đích, gợi ý rằng chưng cất white-box (tức là, giám sát từ logit của mô hình
đích) là quan trọng cho SD. Điều này được củng cố bởi Hình 3a, minh họa sự phát triển của tỷ lệ
chấp nhận trong suốt quá trình huấn luyện. Supervised KD xếp hạng thấp nhất, và hiệu suất của
nó đạt đỉnh sớm trong quá trình huấn luyện do tính chất tĩnh của tập dữ liệu. Ngược lại, các thuật
toán sử dụng dữ liệu do mô hình tạo ra dẫn đến cải thiện liên tục của tỷ lệ chấp nhận. Mặc dù f-
Distill tốn kém hơn nhiều về mặt tính toán so với GKD do việc sử dụng dữ liệu do giáo viên tạo ra,
cả hai thuật toán đều thể hiện hiệu suất tương đương. Đáng chú ý, GKD đạt được cải thiện hiệu
suất thời gian wall tốt nhất. Xem Phụ lục C.1.2 để có thêm hình ảnh về cải thiện hiệu suất trong
quá trình huấn luyện.

Chúng tôi cũng điều tra liệu KD cải thiện hiệu quả khối một cách phổ quát hay tác động đến một
tập con hạn chế các ví dụ. Hình 3b mô tả sự cải thiện trên mỗi ví dụ. Chúng tôi quan sát những lợi
ích nhất quán trong hiệu quả khối trên hầu hết các ví dụ, điều này cũng có thể được thấy trong các
tập dữ liệu khác nhau (xem Hình C.16). Hình 3c minh họa sự đồng ý mạnh mẽ giữa các giá trị hiệu
quả khối lý thuyết và thực nghiệm cho một số mô hình đã chưng cất (mỗi mô hình được biểu diễn
như một vòng tròn đầy). Mặc dù các giá trị lý thuyết thỉnh thoảng đánh giá quá cao hoặc đánh giá
thấp các giá trị thực nghiệm, có thể do các độ lệch tiềm ẩn từ giả định cấp độ token i.i.d. (cf. §3),
thứ hạng của các mô hình đã chưng cất vẫn rất nhất quán. Tóm lại, những phát hiện này phần lớn
xác nhận rằng KD hiệu quả tối ưu hóa hiệu quả khối.

Khả năng chuyển giao của các mô hình đã chưng cất. Tiếp theo chúng tôi kiểm tra khả năng
chuyển giao của các mô hình đã chưng cất trên các tập dữ liệu đa dạng chưa thấy trong quá trình
huấn luyện. Chúng tôi sử dụng một mô hình nháp được chưng cất trên GSM8K và kiểm tra khả
năng của nó trong việc tăng tốc SD trên lời nhắc chuỗi suy nghĩ (CoT) zero-shot trên 23 nhiệm vụ
lý luận từ bộ BigBenchHard (Suzgun et al., 2022). Kết quả, được minh họa trong Hình 1, chỉ ra khả
năng chuyển giao hiệu quả sang các tập dữ liệu khác. So với SD tiêu chuẩn, mô hình đã chưng cất
đáng kể tăng cường tốc độ giải mã trung bình, mang lại cải thiện tăng tốc từ 1.93× và 1.78× thành
2.21× và 2.02× cho các phương pháp giải mã tham lam và không tham lam, tương ứng. Phân tích
thêm trong Hình C.1 tiết lộ rằng việc sử dụng T5-Small đã chưng cất của chúng tôi làm mô hình
nháp cũng tương thích với các mô hình đích lớn hơn (T5-XXL) mặc dù được chưng cất từ một mô
hình kích thước khác (T5-XL). Mặc dù không được tối ưu hóa hoàn toàn, cấu hình này liên tục
vượt trội hơn SD tiêu chuẩn 7%−37% trên các tập dữ liệu khác nhau. Xem Phụ lục C.1 để biết
thêm chi tiết.

5.2 CÔNG THỨC DISTILL SPEC
Bây giờ chúng tôi tập trung vào việc xác định phương pháp KD tối ưu cho SD. Theo các giao thức
huấn luyện và đánh giá trong § 5.1, chúng tôi khám phá bốn phương pháp xây dựng dữ liệu huấn
luyện và bốn hàm phân kỳ trên XSum và GSM8K. Cụ thể, chúng tôi khám phá các biến thể sau
đây của dữ liệu huấn luyện: 1) tập dữ liệu ground-truth cố định DTrain, 2) dữ liệu được tạo chỉ từ
mô hình nháp Mθq, 3) dữ liệu được tạo chỉ từ đích Mp, 4) dữ liệu được tạo từ cả Mθq và Mp theo
tỷ lệ bằng nhau. Chúng tôi cũng kiểm tra các phân kỳ sau đây: 1) forward KL (FKL), 2) phân kỳ
Jenson-Shannon (JSD), 3) reverse KL (RKL), và 4) khoảng cách biến thiên tổng (TVD).

Tầm quan trọng của dữ liệu huấn luyện và phân kỳ trong DistillSpec. Hình 4 minh họa cải
thiện hiệu quả khối trên XSum và GSM8K, phù hợp với các quan sát từ § 5.1. Chúng tôi lưu ý rằng
việc sử dụng dữ liệu do mô hình tạo ra (ba hàng cuối) mang lại hiệu suất vượt trội hơn việc sử dụng
một tập dữ liệu cố định (hàng đầu tiên). Cụ thể, trên XSum với giải mã tham lam, việc sử dụng dữ
liệu được tạo ra từ cả Mθq và Mp dẫn đến hiệu suất tốt nhất, với JSD hơi vượt trội hơn các phân
kỳ khác. Tuy nhiên,

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
FKL JSD RKL TVDCố định (Train)
Học sinh (q)
q+p
Giáo viên (p)
0.01 0.08 0.13 0.17
0.48 0.49 0.45 0.46
0.52 0.53 0.5 0.5
0.51 0.53 0.5 0.51Cải thiện Hiệu quả Khối
0.00.10.20.30.40.5
(a) XSum: τ(tham lam)
FKL JSD RKL TVDCố định (Train)
Học sinh (q)
q+p
Giáo viên (p)
0.23 0.29 0.29 0.26
0.61 0.42 0.43 0.38
0.49 0.43 0.47 0.41
0.35 0.43 0.46 0.42Cải thiện Hiệu quả Khối
0.00.10.20.30.40.50.6 (b) GSM8K: τ(tham lam)
FKL JSD RKL TVDCố định (Train)
Học sinh (q)
q+p
Giáo viên (p)
0.22 0.31 0.32 0.29
0.42 0.43 0.45 0.41
0.36 0.47 0.47 0.44
0.33 0.44 0.49 0.43Cải thiện Hiệu quả Khối
0.00.10.20.30.4 (c) GSM8K: τ(không tham lam)

Hình 4: Công thức DistillSpec. Chúng tôi báo cáo cải thiện trong hiệu quả khối thực nghiệm sau chưng cất trên (a)
XSum với lấy mẫu tham lam, và GSM8K với (b) lấy mẫu tham lam và (c) không tham lam. Hàm phân kỳ nào và
xây dựng dữ liệu nào dẫn đến cải thiện hiệu quả khối lớn nhất phụ thuộc rất nhiều vào nhiệm vụ. Chúng nên được
coi như các siêu tham số được điều chỉnh trên nhiệm vụ quan tâm của chúng ta.

trên GSM8K với giải mã tham lam, FKL với chỉ dữ liệu do nháp tạo ra nổi lên như thiết lập tốt nhất.
Ngược lại, với lấy mẫu nhiệt độ (tại T=1), một xu hướng khác được quan sát khi RKL kết hợp với
dữ liệu được tạo ra bởi Mp là thiết lập hiệu quả nhất. Xem Phụ lục C.2.1 cho kết quả trên các tập
dữ liệu khác nhau và các chiến lược giải mã. Tuy nhiên, việc chỉ sử dụng dữ liệu do nháp tạo ra
được thấy là cạnh tranh.

Tác động của chưng cất lên chất lượng nháp so với tính tương thích. Chúng tôi cũng nghiên cứu
cách các công thức chưng cất khác nhau ảnh hưởng đến hiệu suất nhiệm vụ và liệu có bất kỳ lựa
chọn thiết kế nào đồng thời tối ưu cho việc cải thiện cả hiệu suất nhiệm vụ mô hình nháp và tính
hữu dụng của nó cho SD hay không (cf. Hình C.23, C.24). Tương tự như các quan sát trước đây của
chúng tôi, việc sử dụng dữ liệu được tạo ra là tối quan trọng để cải thiện hiệu suất nháp. Đáng chú
ý, việc sử dụng dữ liệu được tạo ra từ Mθq mang lại kết quả tương đương hoặc vượt trội so với việc
sử dụng dữ liệu được tạo ra từ Mp. Tuy nhiên, thuật toán KD nào tối ưu phụ thuộc phần lớn vào
nhiệm vụ hiện tại và chiến lược giải mã cơ bản. Hình 5a làm nổi bật một sự phân đôi thú vị giữa
cải thiện hiệu quả khối và lợi ích hiệu suất nhiệm vụ thông qua KD: các mô hình đã chưng cất với
hiệu suất nhiệm vụ cao không nhất thiết dịch thành các drafter căn chỉnh hơn cho SD.

Khuyến nghị. Thú vị, mặc dù TVD là mục tiêu chúng ta nên nhằm tối ưu hóa cho SD (cf. Eq. 3),
việc tối ưu hóa trực tiếp của nó không mang lại hiệu suất tốt nhất trong hầu hết các cài đặt được
khám phá. Chúng tôi thường thấy rằng lựa chọn phân kỳ trong KD là một siêu tham số cần được
điều chỉnh dựa trên nhiệm vụ hiện tại và chiến lược giải mã được sử dụng. Đối với xây dựng dữ
liệu huấn luyện, chúng tôi đề xuất sử dụng mô hình nháp Mθq để tạo dữ liệu vì nó có thể đạt được
hiệu suất tương tự hoặc vượt trội so với mô hình đích Mp, nhưng với chi phí thấp hơn nhiều.

5.3 CÂN ĐỐI CHẤT LƯỢNG VERSUS ĐỘ TRỄ
Giải mã đồn đoán có tổn thất. Chúng tôi phân tích sự cân đối chất lượng-độ trễ sử dụng các biến
thể SD có tổn thất, như được chi tiết trong Thuật toán A.1. Như Hình 5b minh họa, việc sử dụng
KD (⋆) hoặc SD (×) một mình không hoàn toàn thu hẹp các khoảng cách hiệu suất hoặc độ trễ,
tương ứng. Trong trường hợp như vậy, một tham số khoan dung (ε) có thể giúp nội suy giữa hai
phương pháp này, như được chứng minh trong Hình 5b nơi mỗi điểm trong một nhóm nhất định
tương ứng với một giá trị khác nhau của ε. Như thí nghiệm GSM8K cho thấy, sức mạnh của nội
suy có thể bị hạn chế: thậm chí sử dụng khoan dung cho phép của ε=10−5, flin vẫn dẫn đến hiệu
suất cao nhưng độ trễ cao, trong khi fsq theo dõi một đường cong cân đối tương tự nhưng hơi mở
rộng. Mặc dù fexp làm cho nội suy có thể, nó mang lại sự cân đối chất lượng-độ trễ tệ hơn. Thú vị,
có thể giảm đáng kể độ trễ trong khi bảo tồn hầu hết chất lượng trên GSM8K, có thể bởi vì nhiều
token không quan trọng đối với hiệu suất cuối cùng, và nhiều đề xuất có thể được chấp nhận một
cách an toàn với tác động tối thiểu lên chất lượng tạo. Xem Phụ lục C.3.1 để so sánh giữa các mô
hình nháp không được chưng cất và đã chưng cất, nơi chúng tôi cho thấy rằng một mô hình nháp
đã chưng cất cho phép sự cân đối chất lượng vs. độ trễ tốt hơn nhiều.

DistillSpec gặp khu vườn mô hình. Trong nhiều tình huống thực tế, chúng ta có quyền truy cập
vào nhiều mô hình có kích thước khác nhau—một khu vườn mô hình—để thiết kế pipeline suy
luận. Chúng tôi mô phỏng thiết lập này bằng cách tập trung vào năm kích thước mô hình trong họ
mô hình T5: T5-Small (77M), T5-Base (250M), T5-Large (800M), T5-XL (3B), và T5-XXL (11B).
Chúng tôi nghiên cứu các đường cong cân đối chất lượng-độ trễ thu được từ việc áp dụng KD và
SD như sau: 1) raw: triển khai các mô hình T5 được tinh chỉnh có giám sát (SFT); 2) distilled: áp
dụng KD bằng cách chưng cất các mô hình nhỏ hơn từ các mô hình T5 lớn hơn; 3) speculative: áp
dụng SD sử dụng các mô hình T5; và 4) DistillSpec: áp dụng KD trên các mô hình T5 và sử dụng
SD với các mô hình đã chưng cất làm mô hình đích và nháp.

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
4 6 8
Cải thiện Độ chính xác0.250.300.350.400.450.50Cải thiện Block Efficiency
Phân kỳ
FKL
JSD0.5
RKL
TVD
Dữ liệu Huấn luyện
Giáo viên (p)
Học sinh (q)
Cố định (Train)
q+p
(a) chất lượng vs. tính tương thích SD (GSM8K)
0.5 1.0 1.5 2.0 2.5 3.0 3.5
Độ trễ Tương đối51015202530Hiệu suất - Độ chính xácflin(p,ε)=p/ε
fsq(p,ε)=p/ε²
fexp(p,ε)=pᵋ
Không Khoan dung
Nháp Thô
Nháp Đã Chưng cất (b) DistillSpec với SD có tổn thất (GSM8K)

Hình 5: (a) Sự cải thiện trên giải mã đồn đoán và hiệu suất nhiệm vụ downstream chỉ tương quan yếu. Một mô hình
đã chưng cất chất lượng cao không ngụ ý nó có thể là một mô hình nháp hiệu quả trong giải mã đồn đoán. (b) Chúng
tôi sử dụng khoan dung như một cơ chế kiểm soát chính xác để đạt được hồ sơ chất lượng-độ trễ mong muốn.

0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
Độ trễ Tương đối1416182022Hiệu suất - Rouge2Mô hình Đích
T5-Small
T5-Base
T5-Large
T5-XL
T5-XXLChiến lược Phục vụ
Raw
Distilled
Speculative
DistillSpec
0 2 4 6 8 10 12 14
Độ trễ Tương đối102030Hiệu suất - Độ chính xácMô hình Đích
T5-Small
T5-Base
T5-Large
T5-XL
T5-XXLChiến lược Phục vụ
Raw
Distilled
Speculative
DistillSpec

Hình 6: DistillSpec xuất sắc trong cả chất lượng và độ trễ, mang lại giảm độ trễ đáng kể 6.4x và 10.7x trên XSum
(trái) và GSM8K (phải), trong khi duy trì hiệu suất gần như giống hệt.

Hình 6 cho thấy rằng SD hiệu quả chuyển đường cong cân đối sang trái, đặc biệt với các kích thước
mô hình lớn hơn. Tuy nhiên, hiệu quả của nó giảm với các kích thước mô hình nhỏ hơn khi thời
gian tính toán giữa các mô hình nháp và đích được khớp chặt chẽ. Ngược lại, chưng cất, tối ưu hóa
mô hình cho hiệu suất nhiệm vụ downstream, dường như cung cấp một sự cân đối vượt trội giữa
chất lượng và độ trễ, đặc biệt cho các mô hình nhỏ hơn. Ngược lại, một xu hướng ngược được quan
sát cho các kích thước mô hình lớn hơn khi đánh giá mô hình với lấy mẫu nhiệt độ. Hình C.30a chỉ
ra một khoảng cách đáng kể giữa mô hình đã chưng cất và mô hình giáo viên lớn hơn, trong khi
phương pháp dựa trên SD có thể giảm đáng kể độ trễ. Điều này gợi ý rằng khi có các ràng buộc
hiệu suất và chiến lược giải mã nghiêm ngặt, SD vẫn là một phương pháp có giá trị. Phương pháp
DistillSpec của chúng tôi, kết hợp lợi ích của chưng cất và SD, liên tục đạt được sự cân đối tốt nhất
giữa chất lượng và độ trễ, mang lại một sự giảm ấn tượng về độ trễ trong khi duy trì hiệu suất gần
như giống hệt. Cụ thể, DistillSpec giảm độ trễ tương đối từ 17.3 xuống 2.7 và từ 15.0 xuống 1.4
trên XSum và GSM8K, tương ứng, đại diện cho cải thiện tăng tốc 6.4× và 10.7×. Ngược lại, điểm
Rouge2 chỉ giảm nhẹ, từ 23.1 xuống 23.0 trên XSum, trong khi độ chính xác mô hình trên GSM8K
thực sự cải thiện từ 33.1 lên 34.8.

6 KẾT LUẬN
Trong bài báo này, chúng tôi đánh giá hiệu quả của chưng cất tri thức white-box (KD) trong việc
tăng cường giải mã đồn đoán (SD) thông qua cải thiện sự căn chỉnh giữa các mô hình đích và nháp.
Một phân tích kỹ lưỡng được tiến hành để hiểu tác động của xây dựng dữ liệu huấn luyện và các
hàm phân kỳ lên hiệu suất KD. Chúng tôi nhấn mạnh tầm quan trọng của việc sử dụng dữ liệu do
mô hình tạo ra và lập luận rằng việc sử dụng dữ liệu on-policy của mô hình nháp trong KD là một
cách hiệu quả về mặt chi phí để cải thiện sự căn chỉnh mô hình. Ngoài ra, chúng tôi đánh giá sự
cân đối giữa chất lượng và độ trễ trong phạm vi khoan dung và tính khả dụng của nhiều mô hình
có chất lượng và kích thước khác nhau (khu vườn mô hình), kết luận rằng KD cho phép một sự cân
đối vượt trội so với SD tiêu chuẩn. Chiến lược tối ưu bao gồm đầu tiên áp dụng KD cho hiệu suất
nhiệm vụ downstream, theo sau bởi SD sử dụng một mô hình nháp đã chưng cất, dẫn đến một sự
giảm từ sáu đến mười lần về độ trễ với tổn thất hiệu suất không đáng kể. Nghiên cứu của chúng tôi
đóng góp những hiểu biết mới về các thuật toán KD white-box cho LLM và cung cấp hướng dẫn
để đạt được một sự cân bằng hiệu quả giữa chất lượng và độ trễ sử dụng KD và SD.

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
LỜI CẢM ƠN
Chúng tôi muốn gửi lời cảm ơn đặc biệt đến Neha Gupta, Wittawat Jitkrittum, Nino Veillard,
Yaniv Leviathan, Matan Kalman, Danny Vainstein, Natan Potikha, Ananda Theertha Suresh, Laz
Karydas, Aishwarya PS, Pranav Nair, Praneeth Netrapalli, Nikunj Saunshi, Ziteng Sun, Keiran
Paster, Olivier Bachem, Aleksandra Faust vì những thảo luận sâu sắc và phản hồi có giá trị.

TÀI LIỆU THAM KHẢO
Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos, Matthieu Geist,
and Olivier Bachem. On-policy distillation of language models: Learning from self-generated
mistakes. arXiv preprint arXiv:2306.13649 , 2023.
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.
arXiv preprint arXiv:2305.10403 , 2023.
Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes
Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia
Specia, and Ale s Tamchyna. Findings of the 2014 workshop on statistical machine trans-
lation. In Proceedings of the Ninth Workshop on Statistical Machine Translation , pp. 12–
58, Baltimore, Maryland, USA, June 2014. Association for Computational Linguistics. URL
http://www.aclweb.org/anthology/W/W14/W14-3302 .
Cristian Bucilu ˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Pro-
ceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data
mining , pp. 535–541, 2006.
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, and Tri Dao. Medusa: Simple frame-
work for accelerating llm generation with multiple decoding heads. https://github.com/
FasterDecoding/Medusa , 2023a.
Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as
tool makers. arXiv preprint arXiv:2305.17126 , 2023b.
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
arXiv preprint arXiv:1312.3005 , 2013.
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint
arXiv:2302.01318 , 2023.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.
Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language
models towards multi-step reasoning. arXiv preprint arXiv:2301.12726 , 2023.
Tao Ge, Heming Xia, Xin Sun, Si-Qing Chen, and Furu Wei. Lossless acceleration for seq2seq
generation with aggressive decoding. arXiv preprint arXiv:2205.10350 , 2022.
Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation of large language models.
arXiv preprint arXiv:2306.08543 , 2023.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa
Suleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in
neural information processing systems , pp. 1693–1701, 2015.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531 , 2015.
Ferenc Huszár. How (not) to train your generative model: Scheduled sampling, likelihood, adver-
sary? arXiv preprint arXiv:1511.05101 , 2015.

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Sehoon Kim, Karttikeya Mangalam, Jitendra Malik, Michael W Mahoney, Amir Gholami, and Kurt
Keutzer. Big little transformer decoder. arXiv preprint arXiv:2302.07863 , 2023.
Yoon Kim and Alexander M Rush. Sequence-level knowledge distillation. arXiv preprint
arXiv:1606.07947 , 2016.
Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative
decoding. In International Conference on Machine Learning , pp. 19274–19286. PMLR, 2023.
Alexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei. Autoregressive knowledge distilla-
tion through imitation learning. arXiv preprint arXiv:2009.07253 , 2020.
Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang.
Online speculative decoding. arXiv preprint arXiv:2310.07177 , 2023.
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,
Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating
generative llm serving with speculative inference and token tree verification. arXiv preprint
arXiv:2305.09781 , 2023.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don't give me the details, just the summary!
topic-aware convolutional neural networks for extreme summarization. ArXiv , abs/1808.08745,
2018.
OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023. URL https://api.
semanticscholar.org/CorpusID:257532815 .
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan
Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference.
Proceedings of Machine Learning and Systems , 5, 2023.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.
H. A. Rowley, S. Baluja, and T. Kanade. Neural network-based face detection. IEEE Transactions
on Pattern Analysis and Machine Intelligence , 20(1):23–38, 1998. doi: 10.1109/34.655647.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
InInternational Conference on Machine Learning , pp. 4596–4604. PMLR, 2018.
Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autore-
gressive models. Advances in Neural Information Processing Systems , 31, 2018.
Xin Sun, Tao Ge, Furu Wei, and Houfeng Wang. Instantaneous grammatical error correction with
shallow aggressive decoding. arXiv preprint arXiv:2106.04970 , 2021.
Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu,
Michael Riley, and Sanjiv Kumar. Spectr: Fast speculative decoding via optimal transport.
InWorkshop on Efficient Systems for Foundation Models @ ICML2023 , 2023. URL https:
//openreview.net/forum?id=d0mGsaheuT .
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks
and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261 , 2022.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca , 2023.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog
applications. arXiv preprint arXiv:2201.08239 , 2022.

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
Neural Information Processing Systems , 35:24824–24837, 2022.
Yuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level
knowledge distillation. arXiv preprint arXiv:2307.15190 , 2023.
Zhixiang (Eddie) Xu, Matt J. Kusner, Kilian Q. Weinberger, Minmin Chen, and Olivier Chapelle.
Classifier cascades and trees for minimizing feature evaluation cost. Journal of Machine Learning
Research , 15(62):2113–2144, 2014.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer
language models. arXiv preprint arXiv:2205.01068 , 2022.

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Phụ lục
Mục lục
A Phương pháp 14
A.1 Mô tả các hàm phân kỳ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 Biện minh việc sử dụng dữ liệu on-policy . . . . . . . . . . . . . . . . . . . . . . . . 15
A.3 Các thuật toán DistillSpec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B Chi tiết Thực hiện 19
B.1 Tập dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 Mô hình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.3 Chưng cất . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.4 Đánh giá . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C Kết quả Bổ sung 22
C.1 Tăng cường giải mã đồn đoán thông qua chưng cất tri thức . . . . . . . . . . . . 22
C.2 Công thức chưng cất . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
C.3 Cân đối chất lượng versus độ trễ . . . . . . . . . . . . . . . . . . . . . . . . . . 38

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
A PHƯƠNG PHÁP
A.1 MÔ TẢ CÁC HÀM PHÂN KỲ
Dưới đây là một số hàm phân kỳ phổ biến được sử dụng trong chưng cất, cho hai phân phối xác suất rời rạc P(C) và Q(C).

Phân kỳ Kullback-Leibler (KL).
DKL(P∥Q) = Σc∈C P(c) log P(c)/Q(c) (A.1)

Chúng tôi lưu ý rằng phân kỳ KL không đối xứng, tức là, DKL(P∥Q) ≠ DKL(Q∥P). Như vậy, chúng tôi gọi DKL(P∥Q) là forward KL (FKL) và DRKL(P||Q) := DKL(Q∥P) là reverse KL (RKL) giữa P và Q. Việc tối thiểu hóa FKL dưới một phân phối dữ liệu thực nghiệm tương ứng với ước lượng hợp lý tối đa (MLE), là mục tiêu tổn thất điển hình được sử dụng trong học có giám sát cho một tập dữ liệu cố định.

Phân kỳ Jensen–Shannon (JS).
DJS(P∥Q) = 1/2(DKL(P∥M) + DKL(Q∥M), trong đó M = 1/2(P + Q) (A.2)

Phân kỳ Jensen-Shannon tổng quát (DJSD[β]).
DJSD[β](P∥Q) = βDKL(P∥(βP + (1−β)Q)) + (1−β)DKL(Q∥(βP + (1−β)Q)) (A.3)

Thú vị, có thể chứng minh rằng limβ→0 DJSD(β)(P∥Q)/β = DKL(P∥Q) (Huszár, 2015). Như vậy, DJSD[β] hành xử tương tự như forward KL cho các giá trị nhỏ của β. Tương tự, DJSD[β] có hành vi tương tự như reverse KL cho β gần 1, vì DJSD[β](P∥Q) = DJSD[1−β](Q∥P).

Khoảng cách biến thiên tổng (TVD).
DTVD(P∥Q) = Σc∈C |P(c) − Q(c)|/2 (A.4)

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
A.2 BIỆN MINH VIỆC SỬ DỤNG DỮ LIỆU ON-POLICY
Trong phần này, chúng tôi chứng minh Định lý 4.1 mà thúc đẩy việc sử dụng dữ liệu on-policy của chúng tôi. Chúng tôi tuân theo các ký hiệu trong Phần 3 và 4. Ngoài ra, chúng tôi viết ϵ(x) := Ey∼q≤T(y|x)[DTVD(Mp∥Mθq)(y|x)] cho tổn thất chưng cất của một đầu vào x đơn.

Để dễ dàng phân tích, hãy phân tách α(x) và ϵ(x) thành tổng các đóng góp từ mỗi token.

Bổ đề A.1. Đối với tất cả x, α(x) = 1 − 1/Lp(x) ΣTt=1 At và ϵ(x) ≥ 1/T ΣTt=1 Et, trong đó:
At(x) := Ey∼p≤T(y|x)[1{t≤|y|}DTVD(p(yt|x, y<t), q(yt|x, y<t))], (A.5)
Et(x) := Ey∼q≤T(y|x)[1{t≤|y|}DTVD(p(yt|x, y<t), q(yt|x, y<t))]. (A.6)

Chứng minh. Bởi Phương trình 3, chúng ta có:
α(x) = 1 − Ey∼p≤T(y|x)[Σ|y|t=1 DTVD(p(yt|x, y<t), q(yt|x, y<t))]/Lp(x).

Chúng ta có thể viết lại Σ|y|t=1 DTVD(p(yt), q(yt)) = ΣTt=1 1{t≤|y|}DTVD(p(yt), q(yt)) và hoán đổi thứ tự giữa tổng và kỳ vọng để có được:
α(x) = 1 − ΣTt=1 Ey∼p≤T(y|x)[1{t≤|y|}DTVD(p(yt|x, y<t), q(yt|x, y<t))]/Lp(x),

điều này chứng minh Phương trình A.5.

Tương tự, theo định nghĩa của ϵ(x) và DTVD chúng ta có:
ϵ(x) = Ey∼q≤T(y|x)[1/|y| Σ|y|t=1 DTVD(p(yt|x, y<t), q(yt|x, y<t))]
≥ Ey∼q≤T(y|x)[1/T Σ|y|t=1 DTVD(p(yt|x, y<t), q(yt|x, y<t))].

Một lần nữa, chúng ta có thể viết lại Σ|y|t=1 DTVD(p(yt), q(yt)) = ΣTt=1 1{t≤|y|}DTVD(p(yt), q(yt)) và sau đó hoán đổi thứ tự giữa tổng và kỳ vọng để có được:
ϵ(x) ≥ 1/T ΣTt=1 Ey∼q≤T(y|x)[1{t≤|y|}DTVD(p(yt|x, y<t), q(yt|x, y<t))],

điều này chứng minh Phương trình A.6.

Bổ đề A.1 thúc đẩy chúng ta nghiên cứu At(x) và Et(x) thay thế. Dưới đây chúng tôi viết lại chúng trong các dạng biến thiên sẽ được sử dụng sau này. Cho điều này, chúng tôi giới thiệu một số định nghĩa.

Định nghĩa A.1. Đối với bất kỳ chuỗi z ∈ {P,Q}τ chỉ bao gồm các chữ cái P và Q, chúng tôi định nghĩa M(x, y, z) như phân phối của các chuỗi được lấy mẫu như sau:
1. Khởi tạo một chuỗi token như y;
2. Nếu có t−1 token, lấy mẫu token thứ t từ Mp nếu zt = P, và từ Mq ngược lại;
3. Lặp lại cho đến khi một token kết thúc chuỗi được lấy mẫu, hoặc độ dài chuỗi đã đạt τ.

Chúng tôi sử dụng cách viết tắt Pk và Qk để đại diện cho chuỗi k chữ cái liên tiếp của P và Q tương ứng. Đặt Ωk biểu thị tập hợp của tất cả các chuỗi có thể có độ dài k, và δ: Ωt → [−1/2,1/2] là một hàm chung ánh xạ một chuỗi t token thành một số thực trong [−1,1]. Chúng tôi lạm dụng ký hiệu và gán δ(y) = 0 cho tất cả y ∉ Ωt.

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bổ đề A.2. Đối với tất cả x và tất cả 1≤t≤T:
At(x) = supδ:Ωt→[−1/2,1/2] {Ey∼M(x,∅,Pt)[δ(y)] − Ey∼M(x,∅,Pt−1Q)[δ(y)]}, (A.7)
Et(x) = supδ:Ωt→[−1/2,1/2] {Ey∼M(x,∅,Qt−1P)[δ(y)] − Ey∼M(x,∅,Qt)[δ(y)]}. (A.8)

Chứng minh. Đối với một cặp x và y cố định, chúng tôi viết lại khoảng cách biến thiên tổng giữa p(yt|x, y<t) và q(yt|x, y<t) như dạng biến thiên sau:
DTVD(p(yt|x, y<t), q(yt|x, y<t))
= supδ̃:Ω→[−1/2,1/2] {Eyt∼p(yt|x,y<t)[δ̃(yt)] − Eyt∼q(yt|x,y<t)[δ̃(yt)]} =:∆(x,y<t,δ̃).

Sau khi lấy kỳ vọng:
At(x) = Ey∼p≤T(y|x)[DTVD(p(yt|x, y<t), q(yt|x, y<t))]
= Ey∼p≤T(y|x)[supδ̃:Ω→[−1/2,1/2] {∆(x, y<t, δ̃)}]
= Ey∼M(x,∅,Pt−1)[supδ̃:Ω→[−1/2,1/2] {1{|y|=t−1}∆(x, y, δ̃)}]
= supδ:Ωt→[−1/2,1/2] Ey∼M(x,∅,Pt−1)[1{|y|=t−1}∆(x, y, δ(y,·))],

trong đó đẳng thức thứ ba sử dụng quan sát rằng đối với bất kỳ hàm f nào, Ey∼p≤T(y|x)[f(x, y<t)] có thể được thay thế bằng Ey∼M(x,∅,Pt−1)[1{|y|=t−1}f(x, y)], và đẳng thức cuối hoán đổi thứ tự giữa kỳ vọng và supremum. Cuối cùng, chúng tôi di chuyển các kỳ vọng trong ∆ để hợp nhất với kỳ vọng bên ngoài và có được:
At(x) = supδ:Ωt→[−1/2,1/2] {Ey∼M(x,∅,Pt)[1{|y|=t}δ(y)] − Ey∼M(x,∅,Pt−1Q)[1{|y|=t}δ(y)]}
= supδ:Ωt→[−1/2,1/2] {Ey∼M(x,∅,Pt)[δ(y)] − Ey∼M(x,∅,Pt−1Q)[δ(y)]},

trong đó bước cuối tuân theo lạm dụng ký hiệu của chúng tôi rằng δ(y) = 0 cho tất cả y ∉ Ωt. Điều này chứng minh Phương trình A.7, và Phương trình A.8 có thể được chứng minh tương tự.

Dựa trên các dạng biến thiên của At(x) và Et(x), chúng tôi có được bổ đề sau để giới hạn At(x) theo Et(x).

Bổ đề A.3. Đối với tất cả x và 1≤t≤T:
At(x) ≤ 2Σt−1k=1 Ek(x) + Et(x). (A.9)

Chứng minh. Đủ để tìm một giới hạn trên cho Ey∼M(x,∅,Pt)[δ(y)] và một giới hạn dưới cho Ey∼M(x,∅,Pt−1Q)[δ(y)] cho tất cả δ: Ωt → [−1/2,1/2].

Đối với tất cả 1≤k≤t, chúng ta có thể thay thế P đầu tiên trong Ey∼M(x,∅,Qk−1Pt−k+1)[δ(y)] bằng Q chỉ bằng cách đưa vào một lỗi Ek+1(x):
Ey∼M(x,∅,Qk−1Pt−k+1)[δ(y)] = Ey∼M(x,∅,Qk−1P)[1{|y|=k}Ey′∼M(x,y,Pt−k)[δ(y′)]]
≤ Ey∼M(x,∅,Qk)[1{|y|=k}Ey′∼M(x,y,Pt−k)[δ(y′)]] + Ek(x)
= Ey∼M(x,∅,QkPt−k)[δ(y)] + Ek(x),

trong đó bất đẳng thức thứ hai đúng vì chúng ta có thể định nghĩa một hàm δ′: Ωk → [−1/2,1/2], sao cho δ′(y) = 1{|y|=k}Ey′∼M(x,y,Pt−k)[δ(y′)] và sau đó áp dụng Phương trình A.8.

Bây giờ lấy một tổng telescoping trên 1≤k≤t, chúng ta có được:
Ey∼M(x,∅,Pt)[δ(y)] ≤ Ey∼M(x,∅,Qt)[δ(y)] + Σtk=1 Ek(x). (A.10)

Tương tự, đối với tất cả 1≤k≤t−1, chúng ta có thể thay thế P đầu tiên trong Ey∼M(x,∅,Qk−1Pt−kQ)[δ(y)] bằng Q và chỉ đưa vào một lỗi Ek(x):
Ey∼M(x,∅,Qk−1Pt−kQ)[δ(y)] = Ey∼M(x,∅,Qk−1P)[1{|y|=k}Ey′∼M(x,y,Pt−k−1Q)[δ(y′)]]
= −Ey∼M(x,∅,Qk−1P)[−1{|y|=k}Ey′∼M(x,y,Pt−k−1Q)[δ(y′)]]
≥ −Ey∼M(x,∅,Qk)[−1{|y|=k}Ey′∼M(x,y,Pt−k−1Q)[δ(y′)]] + Ek(x)
= Ey∼M(x,∅,QkPt−k−1Q)[δ(y)] − Ek(x).

Lấy một tổng telescoping trên 1≤k≤t−1 mang lại:
Ey∼M(x,∅,Pt−1Q)[δ(y)] ≥ Ey∼M(x,∅,Qt)[δ(y)] − Σt−1k=1 Ek(x). (A.11)

Trừ Phương trình (A.11) từ Phương trình (A.10), chúng ta có điều sau đây đúng cho tất cả các hàm δ:S→[−1/2,1/2]:
Ey∼M(x,∅,Pt)[δ(y)] − Ey∼M(x,∅,Pt−1Q)[δ(y)] ≤ Σtk=1 Ek(x) + Σt−1k=1 Ek(x) = 2Σt−1k=1 Ek(x) + Et(x),

điều này chứng minh khẳng định.

Chứng minh của Định lý 4.1. Tổng Phương trình A.9 trên 1≤t≤T, chúng ta có:
ΣTt=1 At(x) ≤ ΣTt=1(1 + 2(T−t))Et(x).

Kết hợp điều này với Bổ đề A.1 mang lại:
α(x) = 1 − 1/Lp(x) ΣTt=1 At ≥ 1 − 1/Lp(x) ΣTt=1(1 + 2(T−t))Et(x)
≥ 1 − 2T/Lp(x) ΣTt=1 Et(x)
≥ 1 − 2T²/Lp(x) ϵ(x),

điều này chứng minh phát biểu định lý sau khi lấy kỳ vọng trên x∼X.

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
A.3 CÁC THUẬT TOÁN DISTILLSPEC

Thuật toán A.1 Bước giải mã đồn đoán
Yêu cầu: mô hình đích Mp, mô hình nháp Mq, bối cảnh ρ={x, y<t}.
Siêu tham số: kích thước khối (γ), hàm khoan dung f(p, ϵ) (ϵ=1 cho giải mã không tổn thất và ϵ<1 cho giải mã có tổn thất).
1: for all i=0 to γ−1 do
2:    qt+i(y)←Mq(x, y<t+i), yt+i∼qt+i(y) ▷ Lấy mẫu γ token từ Mq tự hồi quy.
3: end for
4: (pt(y), . . . , pt+γ(y))←(Mp(x, y<t), . . . ,Mp(x, y<t+γ)) ▷ Chạy Mp song song.
5: ri←f(pi(y),ϵ)/qi(y), ∀t≤i < t+γ ▷ Tính các ngưỡng từ chối.
6: ut∼Uniform(0,1), . . . , ut+γ−1∼Uniform(0,1) ▷ Tạo γ giá trị ngẫu nhiên.
7: n←min({i|0≤i < γ, ut+i > rt+i} ∪ {γ}) ▷ Xác định số lượng token được chấp nhận n.
8: if n < γ then
9:    yt+n∼norm(max(0, pt+n(y)−qt+n(y))) ▷ Lấy mẫu từ phân phối điều chỉnh.
10: else
11:    yt+n∼pt+n(y) ▷ Lấy mẫu từ Mp.
12: end if
Return {x, y<t+n+1} ▷ Thêm n token từ Mq và một token từ Mp.

Thuật toán A.2 Chưng cất tri thức
Yêu cầu: mô hình đích Mp, mô hình nháp Mθq, tập dữ liệu (X, Y) chứa đầu vào x và có thể đầu ra y.
Siêu tham số: tỷ lệ dữ liệu cố định λ1∈[0,1], tỷ lệ dữ liệu học sinh λ2∈[0,1], hàm phân kỳ D, tỷ lệ học η.
1: u1∼Uniform(0,1), u2∼Uniform(0,1) ▷ Tạo hai giá trị ngẫu nhiên.
2: if u1≤λ1 then
3:    B={(xb, yb)}Bb=1, trong đó (xi, yi)∼(X, Y) ▷ Lấy mẫu đầu vào và đầu ra từ (X, Y).
4: else
5:    B′={(xb)}Bb=1, trong đó xi∼X ▷ Lấy mẫu một lô đầu vào từ X.
6:    if u2≤λ2 then
7:       B={(xb, yb)}Bb=1, trong đó xi∼B′, yi∼Mθq(·|xi) ▷ Lấy mẫu dữ liệu từ Mq.
8:    else
9:       B={(xb, yb)}Bb=1, trong đó xi∼B′, yi∼Mp(·|xi) ▷ Lấy mẫu dữ liệu từ Mp.
10:   end if
11: end if
Return θ←θ−η 1/B Σ(x,y)∈B ∇θD(Mp∥Mθq)(y|x) ▷ Cập nhật θ để tối thiểu hóa D(Mp∥Mθq).

--- TRANG 18 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
B CHI TIẾT THỰC HIỆN
B.1 TẬP DỮ LIỆU
Trong phần này, chúng tôi trình bày một tổng quan toàn diện về các tập dữ liệu được sử dụng trong nghiên cứu này.

XSum (Narayan et al., 2018). Tập dữ liệu Extreme Summarization (XSum) phục vụ như một điểm chuẩn đánh giá cho các hệ thống tóm tắt tài liệu đơn trừu tượng. Tập dữ liệu này bao gồm 226,711 bài báo tin tức, được lấy từ các bài báo BBC trong khoảng thời gian từ 2010 đến 2017. Những bài báo này bao gồm một loạt các lĩnh vực, bao gồm Tin tức, Chính trị, Thể thao, Thời tiết, Kinh doanh, Công nghệ, Khoa học, Sức khỏe, Gia đình, Giáo dục, Giải trí, và Nghệ thuật. Hiệu suất tóm tắt được đánh giá bằng điểm ROUGE trên phần chia tập xác thực của XSum và chủ yếu nhấn mạnh ROUGE-2, nhưng với các xu hướng tương tự được quan sát cho ROUGE-LSum và ROUGE-1. Độ dài đầu vào tối đa là 1,024 và độ dài đầu ra tối đa là 64 được sử dụng cho chưng cất và đánh giá.

CNN/DM (Hermann et al., 2015). Tập dữ liệu CNN/Daily Mail (CNN/DM) được thiết kế cho tóm tắt văn bản. Nó bao gồm các dấu đầu dòng tóm tắt trừu tượng được tạo ra bởi con người từ các câu chuyện tin tức trên các trang web CNN và Daily Mail, được trình bày dưới dạng câu hỏi với các thực thể được ẩn. Các câu hỏi có thể được trả lời bằng cách sử dụng các đoạn văn liên quan từ văn bản nguồn. Tương tự như XSum, điểm ROUGE trên tập dữ liệu xác thực được báo cáo, chủ yếu nhấn mạnh ROUGE-2 nhưng với các xu hướng tương tự được quan sát cho ROUGE-LSum và ROUGE-1. Độ dài đầu vào tối đa là 2,048 và độ dài đầu ra tối đa là 128 được sử dụng cho chưng cất và đánh giá.

WMT EnDe (Bojar et al., 2014). Tập dữ liệu WMT14 EnDe đóng vai trò như một điểm chuẩn tiêu chuẩn cho dịch máy. Nhiệm vụ bao gồm dịch văn bản tiếng Anh sang tiếng Đức trong khi bảo tồn nội dung, ngữ nghĩa và phong cách. Đánh giá dựa trên điểm BLEU, đo sự tương tự của văn bản được dịch bằng máy với các bản dịch tham chiếu chất lượng cao. Độ dài đầu vào tối đa là 80 và độ dài đầu ra tối đa là 80 được sử dụng cho chưng cất và đánh giá, với hiệu suất được đánh giá trên phần chia kiểm tra gốc.

GSM8K (Cobbe et al., 2021). Tập dữ liệu GSM8K bao gồm 8.5K bài toán từ cấp tiểu học chất lượng cao, đa dạng về ngôn ngữ được tạo ra bởi các nhà viết bài toán con người. Tập dữ liệu được chia thành 7.5K bài toán huấn luyện và 1K bài toán kiểm tra, với các giải pháp thường yêu cầu 2 đến 8 bước bao gồm các tính toán cơ bản sử dụng các phép toán số học cơ bản. Để tăng cường khả năng lý luận, chúng tôi khám phá chưng cất cùng với chuỗi suy nghĩ zero-shot (CoT), như được mô tả trong Agarwal et al. (2023). Độ dài đầu vào tối đa là 256 và độ dài đầu ra tối đa là 320 được sử dụng cho chưng cất và đánh giá.

LM1B Chelba et al. (2013). Tập dữ liệu One Billion Word (LM1B) là một điểm chuẩn được công nhận rộng rãi cho mô hình hóa ngôn ngữ. Dữ liệu huấn luyện và held-out được lấy từ tập dữ liệu WMT 2011 News Crawl sử dụng các script Bash shell và Perl. Độ dài đầu vào tối đa là 128 và độ dài đầu ra tối đa là 128 được sử dụng cho chưng cất và đánh giá.

B.2 MÔ HÌNH
Theo Leviathan et al. (2023), chúng tôi đánh giá hai loại mô hình: 1) Các mô hình Transformer chỉ giải mã giống GPT được huấn luyện trên nhiệm vụ LM1B (Chelba et al., 2013) sử dụng mục tiêu tự hồi quy tiêu chuẩn, trong đó các mô hình đích và nháp có 234M và 33M tham số, tương ứng; và 2) các mô hình encoder-decoder T5 v1.1 tiêu chuẩn (Raffel et al., 2020) được tinh chỉnh có giám sát trên bốn nhiệm vụ khác nhau, với T5-XL (3B) và T5-Small (77M) phục vụ như các mô hình đích và nháp, tương ứng.

Mô hình đích Mp trong thí nghiệm mô hình chỉ giải mã có chiều ẩn 1,024, chiều feed-forward 4,096, 12 lớp và 16 đầu attention mỗi khối transformer, tổng cộng 234M tham số. Mô hình nháp tương ứng Mθq có chiều ẩn 512, chiều feed-forward 1,024, 4 lớp và 4 đầu attention mỗi khối transformer, tổng cộng 33M tham số. Tất cả các mô hình sử dụng tokenizer T5 với 32k token. Đối với các checkpoint T5 cơ sở, chúng tôi bắt đầu từ các mô hình T5v1.1 được thích ứng LM. Các mô hình được thích ứng LM này được khởi tạo từ T5v1.1 và được huấn luyện thêm 100K bước trên mục tiêu LM được thảo luận trong bài báo T5 (Raffel et al., 2020). Những checkpoint này có sẵn tại https://console.cloud.google.com/storage/browser/t5-data/pretrained_models.

Trong các thí nghiệm chưng cất encoder-decoder của chúng tôi, cả mô hình học sinh và giáo viên đều được khởi tạo từ các mô hình được tinh chỉnh có giám sát trên tập dữ liệu huấn luyện gốc. Quá trình này cho mỗi tập dữ liệu được chi tiết như sau:

• XSum: đối với các mô hình T5-Small, -Base, -Large, -XL và -XXL, chúng tôi bắt đầu ấm chưng cất từ các mô hình LM-Adapted T5v1.1 được tinh chỉnh có giám sát trong 100K, 50K, 30k, 20K và 8K bước, tương ứng.

• CNN/DM: đối với các mô hình T5-Small, -Base, -Large, -XL và -XXL, chúng tôi bắt đầu ấm chưng cất từ các mô hình LM-Adapted T5v1.1 được tinh chỉnh có giám sát trong 200K, 80K, 20k, 20k và 4K bước, tương ứng.

• WMT: đối với các mô hình T5-Small, -Base, -Large, -XL và -XXL, chúng tôi bắt đầu ấm chưng cất từ các mô hình LM-Adapted T5v1.1 được tinh chỉnh có giám sát trong 250K, 250K, 110k, 50K và 10K bước, tương ứng.

• GSM8K: tất cả các mô hình được tinh chỉnh có giám sát bắt đầu từ các mô hình FLAN-T5 trên tập dữ liệu CoT được tạo ra bởi PaLM-540 trong 10K bước.

B.3 CHƯNG CẤT
Dữ liệu Huấn luyện cho KD. Chúng tôi nghiên cứu năm thuật toán KD được nêu trong Bảng 1. Đối với SeqKD (Kim & Rush, 2016) và f-Distill (Wen et al., 2023), chúng tôi chọn phương pháp tạo dữ liệu giáo viên trực tuyến thay vì tập dữ liệu do giáo viên tạo ra cố định ngoại tuyến thông thường. Phương pháp này, trong khi tốn kém về mặt tính toán, mang lại một tập dữ liệu đa dạng hơn. Đối với GKD, chúng tôi loại trừ dữ liệu ground truth tĩnh và chỉ dựa vào dữ liệu được tạo ra bởi mô hình học sinh trực tuyến. Tất cả dữ liệu được tạo ra bởi giáo viên hoặc học sinh dựa trên lấy mẫu nhiệt độ với nhiệt độ 1.0 (xem Phụ lục C.1.3 cho một nghiên cứu ablation về nhiệt độ lấy mẫu).

Chi tiết Huấn luyện. Chúng tôi sử dụng trình tối ưu hóa Adafactor (Shazeer & Stern, 2018) để huấn luyện mô hình học sinh nháp (Mθq) trong tất cả các thí nghiệm của chúng tôi, theo các hướng dẫn được nêu trong Thuật toán A.2. Trong bối cảnh hàm tổn thất chưng cất tri thức (KD) của chúng tôi, được định nghĩa trong Eq. 2, chúng tôi duy trì nhiệt độ cho cả mô hình đích, được ký hiệu là Tp, và mô hình nháp, được ký hiệu là Tq, ở giá trị không đổi là 1.0. Chúng tôi cần nhấn mạnh tầm quan trọng của việc duy trì cài đặt nhiệt độ đồng nhất này vì nó đóng một vai trò quan trọng trong giải mã đồn đoán, bằng cách đảm bảo một giải thích ngữ nghĩa nhất quán và mạch lạc cho cả Mp và Mθq. Một tóm tắt các siêu tham số được sử dụng trong quá trình chưng cất tri thức của chúng tôi có thể được tìm thấy trong Bảng B.1.

Bảng B.1: Siêu tham số cho các thí nghiệm chưng cất.
siêu tham số giá trị
bước huấn luyện 300,000
kích thước lô 32
dropout 0.0
tỷ lệ học (LR) 0.0003
bước khởi động LR 5,000
làm mát LR (bắt đầu, kết thúc) (150,000, 300,000)
lịch trình khởi động tuyến tính (từ 0 đến LR)
lịch trình làm mát cosine decay (từ LR đến 0.1LR)

--- TRANG 20 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
B.4 ĐÁNH GIÁ
Để có được điểm cho mỗi nhiệm vụ (cụ thể, ROUGE-2 cho XSum và CNN/DM, BLEU cho WMT, và độ chính xác cho GSM8K), chúng tôi sử dụng phương pháp đánh giá trong Agarwal et al. (2023). Chúng tôi đánh giá các mô hình trên tất cả các ví dụ trong các tập kiểm tra hoặc xác thực và báo cáo hiệu suất trung bình của chúng. Để đánh giá hiệu suất giải mã đồn đoán thực nghiệm, tức là, tỷ lệ chấp nhận thực nghiệm và hiệu quả khối thực nghiệm, chúng tôi tiến hành đánh giá trên tất cả các trường hợp trong các tập kiểm tra hoặc xác thực và báo cáo giá trị trung bình của các thước đo này.

Để đo độ trễ thực tế, chúng tôi tuân theo Leviathan et al. (2023) và thực thi cả mô hình đích và mô hình nháp của chúng tôi trên cùng một thiết bị TPUv4 mà không sử dụng song song mô hình. Chúng tôi lấy mẫu ngẫu nhiên 500 ví dụ từ tập kiểm tra hoặc xác thực, và đo thời gian giải mã wall-clock trên kích thước lô là 1. Thủ tục đo này được lặp lại ba lần, và hiệu suất trung bình được báo cáo. Chúng tôi đã quan sát phương sai tối thiểu trên các seed ngẫu nhiên khác nhau trong kết quả của chúng tôi.

--- TRANG 21 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
C KẾT QUẢ BỔ SUNG
C.1 TĂNG CƯỜNG GIẢI MÃ ĐỒN ĐOÁN THÔNG QUA CHƯNG CẤT TRI THỨC

Bảng C.1: DistillSpec cải thiện hiệu suất giải mã đồn đoán trên các tập dữ liệu và kích thước khối khác nhau, cho cả giải mã tham lam và lấy mẫu nhiệt độ. BBH-AVG chứa đánh giá của mô hình nháp đã chưng cất từ GSM8K trên tất cả các nhiệm vụ BIG-Bench Hard (BBH) (Suzgun et al., 2022); chúng tôi báo cáo kết quả trung bình trên 23 nhiệm vụ BBH. Xem Hình C.2 và C.3 để có bảng phân tích chi tiết hiệu suất trên các nhiệm vụ BBH riêng lẻ.

[Bảng hiển thị kết quả so sánh hiệu suất với/không có chưng cất trên các tập dữ liệu khác nhau với các cài đặt khác nhau]

--- TRANG 22 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
[Tiếp tục nội dung các trang còn lại tương tự...]
