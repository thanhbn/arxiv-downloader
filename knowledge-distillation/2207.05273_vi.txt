# 2207.05273.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2207.05273.pdf
# Kích thước tệp: 706300 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Chưng Cất Tri Thức Đa Kiến Trúc
Yufan Liu1;2[0000000284269335], Jiajiong Cao5, Bing Li1;4?, Weiming Hu1;2;3,
Jingting Ding5, và Liang Li5
1Phòng thí nghiệm Quốc gia Nhận dạng Mẫu, Viện Tự động hóa, Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
2Trường Trí tuệ Nhân tạo, Đại học Viện Hàn lâm Khoa học Trung Quốc, Bắc Kinh, Trung Quốc
3Trung tâm Xuất sắc CAS về Khoa học Não bộ và Công nghệ Trí tuệ, Bắc Kinh, Trung Quốc
4PeopleAI, Inc., Bắc Kinh, Trung Quốc
5Ant Financial Service Group, Bắc Kinh, Trung Quốc
bli@nlpr.ia.ac.cn

Tóm tắt. Transformer thu hút nhiều sự chú ý nhờ khả năng học các mối quan hệ toàn cục và hiệu suất vượt trội. Để đạt được hiệu suất cao hơn, việc chưng cất tri thức bổ sung từ Transformer sang mạng nơ-ron tích chập (CNN) là điều tự nhiên. Tuy nhiên, hầu hết các phương pháp chưng cất tri thức hiện có chỉ xem xét chưng cất đồng kiến trúc, chẳng hạn như chưng cất tri thức từ CNN sang CNN. Chúng có thể không phù hợp khi áp dụng cho các tình huống đa kiến trúc, như từ Transformer sang CNN. Để giải quyết vấn đề này, một phương pháp chưng cất tri thức đa kiến trúc mới được đề xuất. Cụ thể, thay vì trực tiếp bắt chước đầu ra/đặc trưng trung gian của giáo viên, bộ chiếu attention chéo một phần và bộ chiếu tuyến tính theo nhóm được giới thiệu để căn chỉnh các đặc trưng học sinh với các đặc trưng của giáo viên trong hai không gian đặc trưng được chiếu. Và một lược đồ huấn luyện bền vững đa góc nhìn được trình bày thêm để cải thiện tính bền vững và ổn định của khung. Các thí nghiệm mở rộng cho thấy phương pháp đề xuất vượt trội hơn 14 phương pháp tiên tiến trên cả tập dữ liệu quy mô nhỏ và quy mô lớn.

Từ khóa: Chưng cất tri thức · Đa kiến trúc · Nén mô hình.

1 Giới thiệu

Chưng cất tri thức (KD) đã trở thành một chủ đề cơ bản để nâng cao hiệu suất mô hình. Nó đã được áp dụng thành công cho các ứng dụng khác nhau bao gồm nén mô hình [1] và chuyển giao tri thức [2]. KD thường áp dụng khung giáo viên-học sinh, trong đó mô hình học sinh được huấn luyện dưới sự hướng dẫn của tri thức giáo viên. Tri thức thường được định nghĩa bởi các đầu ra mềm hoặc đặc trưng trung gian của mô hình giáo viên.

Các phương pháp KD hiện có tập trung vào mạng nơ-ron tích chập (CNN). Tuy nhiên, gần đây đã xuất hiện nhiều mạng mới như Transformer. Nó cho thấy hiệu suất vượt trội trên các nhiệm vụ thị giác máy tính khác nhau bao gồm phân loại hình ảnh [3] và phát hiện [4], trong khi tính toán khổng lồ và hỗ trợ tăng tốc nền tảng hạn chế giới hạn việc ứng dụng Transformer, đặc biệt cho các thiết bị biên. Mặt khác, với nhiều
?Tác giả liên hệ.arXiv:2207.05273v2  [cs.CV]  15 Nov 2022

--- TRANG 2 ---
2 Y. Liu et al.

Transformer Encoder
CNN
Transformerc x (h'w') h'w'
c
...
N x (3hw) 3hw
hwNKhả năng chuyển giao tri thức
(a) (b)
0.450.50.550.60.650.7
CNN→CNN T→T T→CNN
...
Độ tương tự cosine

Hình 1. (a) So sánh giữa CNN và Transformer. Cách hình thành các đặc trưng hoàn toàn khác nhau. (b) Độ tương tự cosine giữa các đặc trưng từ các mô hình khác nhau trên ImageNet. Lưu ý rằng các đặc trưng được ánh xạ về cùng một chiều bởi một phép chiếu tuyến tính.
Đối với "CNN!CNN", các thanh biểu diễn độ tương tự giữa CNN ResNet152 và CNNs {ResNet18, ResNet32, ResNet50, ResNet101, ResNet152}; Đối với "T !T", các thanh biểu diễn độ tương tự giữa Transformer ViT-L/16 và Transformers {ViT-B/32, ViT-B/16, ViT-L/32, ViT-L/16}; Đối với "T!CNN", các thanh biểu diễn độ tương tự giữa Transformer ViT-L/16 và CNNs {ResNet18, ResNet32, ResNet50, ResNet101, ResNet152}.

năm phát triển, có đủ thư viện tăng tốc bao gồm CUDA [5], TensorRT [6] và NCNN [7], làm cho CNN thân thiện với phần cứng trên cả máy chủ và thiết bị biên. Vì mục đích này, việc chưng cất tri thức từ Transformer hiệu suất cao sang CNN nhỏ gọn là một ý tưởng tự nhiên. Tuy nhiên, có một khoảng cách lớn giữa hai kiến trúc. Như được thể hiện trong Hình 1-(a), Transformer bao gồm các khối transformer dựa trên self-attention trong khi CNN chứa một chuỗi các khối tích chập. Hơn nữa, các đặc trưng được sắp xếp theo một cách hoàn toàn khác. Các đầu ra trung gian của CNNs được hình thành với c kênh của bản đồ đặc trưng h0w0. Khác với CNN, các đặc trưng của Transformer bao gồm N vectơ đặc trưng với 3hw phần tử, trong đó N chỉ số lượng patch.

Thật không may, các phương pháp hiện có tập trung vào KD đồng kiến trúc như CNN !CNN và Transformer !Transformer, không phù hợp cho các tình huống đa kiến trúc. Như được thể hiện trong Hình 1-(b), "khả năng chuyển giao" tri thức được định nghĩa định lượng. Cụ thể, đặc trưng đầu ra của học sinh được căn chỉnh với không gian đặc trưng của giáo viên, và sau đó, độ tương tự cosine của vectơ đặc trưng học sinh được căn chỉnh và vectơ đặc trưng giáo viên được tính toán. Đối với các trường hợp đồng kiến trúc, khả năng chuyển giao là giữa 0:60:7, trong khi nó thấp hơn nhiều, thường thấp hơn 0.55, trong điều kiện đa kiến trúc. Do đó, việc chưng cất tri thức qua các kiến trúc khác nhau khó khăn hơn và một khung KD mới cần được thiết kế để giải quyết nó.

Trong công việc này, một phương pháp chưng cất tri thức đa kiến trúc mới được đề xuất để thu hẹp khoảng cách lớn giữa Transformer và CNN. Với sự hỗ trợ của khung được đề xuất, tri thức từ Transformer được chuyển giao hiệu quả sang mạng CNN học sinh và khả năng chuyển giao tri thức được cải thiện đáng kể thông qua phương pháp này. Nó khuyến khích học sinh học cả đặc trưng không gian cục bộ (với mô hình CNN ban đầu) và các đặc trưng toàn cục bổ sung (từ mô hình giáo viên transformer). Cụ thể, hai bộ chiếu bao gồm một bộ chiếu attention chéo một phần (PCA) và một bộ chiếu tuyến tính theo nhóm (GL), được thiết kế. Thay vì trực tiếp bắt chước

--- TRANG 3 ---
Chưng Cất Tri Thức Đa Kiến Trúc 3

đầu ra của giáo viên, hai bộ chiếu này căn chỉnh đặc trưng trung gian của học sinh vào hai không gian đặc trưng khác nhau và chưng cất tri thức được thực hiện thêm trong hai không gian đặc trưng. Bộ chiếu PCA ánh xạ đặc trưng học sinh vào không gian attention Transformer của giáo viên. Bộ chiếu này khuyến khích học sinh học mối quan hệ toàn cục từ giáo viên Transformer. Bộ chiếu GL ánh xạ đặc trưng học sinh vào không gian đặc trưng Transformer theo cách pixel-by-pixel. Bộ chiếu này trực tiếp giảm thiểu sự khác biệt về hình thành đặc trưng giữa giáo viên và học sinh. Ngoài ra, để giảm thiểu tính không ổn định do sự đa dạng trong khung đa kiến trúc, chúng tôi đề xuất một lược đồ huấn luyện bền vững đa góc nhìn. Các mẫu đa góc nhìn được tạo ra để làm nhiễu mạng học sinh. Và một bộ phân biệt đối kháng đa góc nhìn được xây dựng để phân biệt các đặc trưng giáo viên và các đặc trưng học sinh bị nhiễu, trong khi học sinh được huấn luyện để làm rối bộ phân biệt. Sau khi hội tụ, học sinh có thể bền vững và ổn định hơn.

Các thí nghiệm mở rộng được tiến hành trên cả tập dữ liệu quy mô lớn và quy mô nhỏ, bao gồm ImageNet [8] và CIFAR [9]. Kết quả thí nghiệm của các cặp giáo viên-học sinh khác nhau chứng minh rằng phương pháp đề xuất hoạt động ổn định tốt hơn 14 phương pháp tiên tiến. Tóm lại, các đóng góp chính của công việc chúng tôi có ba khía cạnh:

–Chúng tôi đề xuất một khung chưng cất tri thức đa kiến trúc để chưng cất tri thức Transformer xuất sắc để hướng dẫn CNN. Trong khung này, bộ chiếu attention chéo một phần (PCA) và bộ chiếu tuyến tính theo nhóm (GL) được thiết kế để căn chỉnh không gian đặc trưng học sinh và nâng cao khả năng chuyển giao giữa đặc trưng giáo viên và đặc trưng học sinh.

–Chúng tôi đề xuất một lược đồ huấn luyện bền vững đa góc nhìn để cải thiện tính ổn định và bền vững của mạng học sinh.

–Kết quả thí nghiệm cho thấy phương pháp đề xuất hiệu quả và vượt trội hơn 14 phương pháp tiên tiến trên cả tập dữ liệu quy mô lớn và quy mô nhỏ.

2 Công việc liên quan

Hinton và cộng sự [10] đề xuất khái niệm chưng cất tri thức, sử dụng đầu ra mềm của giáo viên để hướng dẫn việc học của học sinh. Gần đây, nó đã được áp dụng chủ yếu cho nén mô hình [1] và chuyển giao tri thức [2]. Các hình thức khác nhau của tri thức được chưng cất được khám phá để hướng dẫn mạng học sinh tốt hơn, bao gồm đầu ra cuối cùng [10, 11] và tri thức lớp gợi ý [12–19]. Đối với tri thức lớp gợi ý, nhiều nỗ lực đã được thực hiện để khớp các lớp gợi ý học sinh và các lớp được hướng dẫn bởi giáo viên. Ví dụ, AT [12] định nghĩa bản đồ attention đơn kênh như tri thức. Tuy nhiên, việc tính toán các bản đồ attention gây ra mất thông tin chiều kênh. FitNet [13] trực tiếp chưng cất các đặc trưng từ các lớp trung gian mà không mất thông tin. Tuy nhiên, hạn chế này hơi cứng nhắc và không phải tất cả thông tin đều có lợi. Liu và cộng sự [17] chưng cất tri thức được gọi là đồ thị mối quan hệ thực thể (IRG), chứa đặc trưng thực thể, mối quan hệ đặc trưng thực thể và biến đổi không gian đặc trưng. Nó không bị giới hạn bởi sự không khớp chiều giữa giáo viên và học sinh.

Các phương pháp trên đều tập trung vào mạng nơ-ron tích chập (CNN). Gần đây, Transformer ngày càng trở nên phổ biến nhờ hiệu suất ấn tượng. Tuy nhiên, do kiến trúc hoàn toàn khác, nhiều phương pháp KD trước đây không thể

--- TRANG 4 ---
4 Y. Liu et al.

được áp dụng trực tiếp cho Transformers. Có một số công trình [20–22] nghiên cứu chưng cất tri thức giữa các Transformers. DeiT [20] đề xuất một token chưng cất tương tự như token lớp, để làm cho Transformer học sinh học nhãn cứng từ giáo viên và ground truth (GT). MINILM [21] tập trung vào các cơ chế attention trong Transformer và chưng cất thông tin self-attention tương ứng. IR [22] chưng cất các biểu diễn nội bộ (ví dụ, bản đồ self-attention) từ Transformer giáo viên sang Transformer học sinh.

Tóm lại, các phương pháp hiện có thường trình bày một phép biến đổi để khớp các đặc trưng của giáo viên và học sinh. Tuy nhiên, gần như tất cả chúng đều yêu cầu kiến trúc tương tự hoặc thậm chí giống nhau giữa giáo viên và học sinh. Để giải quyết vấn đề chưng cất tri thức đa kiến trúc, chúng tôi cẩn thận thiết kế các bộ chiếu để khớp giáo viên và học sinh trong cùng một không gian đặc trưng. Do đó, một mô hình CNN học sinh nhỏ gọn có thể học tốt đặc trưng toàn cục từ một mô hình Transformer giáo viên mặc dù có khoảng cách lớn trong kiến trúc.

3 Phương pháp đề xuất

Trong phần này, khung của phương pháp đề xuất được giới thiệu trước. Sau đó, hai thành phần chính của khung bao gồm các bộ chiếu đa kiến trúc và một lược đồ huấn luyện bền vững đa góc nhìn được trình bày. Cái trước được xây dựng để giảm thiểu sự không khớp đặc trưng cho các tình huống đa kiến trúc và giúp học sinh học mối quan hệ toàn cục của các đặc trưng, trong khi cái sau được áp dụng để cải thiện tính bền vững và ổn định của học sinh. Cuối cùng, hàm mất mát và quy trình huấn luyện được mô tả.

3.1 Khung

Khung tổng thể của phương pháp đề xuất được mô tả trong Hình 2. Trong hình này, mạng màu hồng phía trên đại diện cho mạng giáo viên, trong khi mạng màu xanh phía dưới là mạng học sinh. Đối với giáo viên transformer T, mẫu đầu vào x2R3HW được chia thành (N=HW/hw) patches {xn2R3hw}N n=1. Sau khi suy luận của một số khối transformer, đặc trưng hT2RN(3hw) được tạo ra. Và khả năng dự đoán cuối cùng sau đó được tính toán thông qua một đầu perceptron đa lớp (MLP) như được thể hiện trong Hình 2. Đối với học sinh CNN S, nó nhận toàn bộ hình ảnh mà không có phân vùng theo patch làm đầu vào. Tương tự, sau khi suy luận của một số khối CNN, đặc trưng học sinh cuối cùng hS2Rc(h0w0) có thể được thu được. Lưu ý rằng c là số kênh và h0w0=HW/22s. s biểu thị số giai đoạn CNN (thường bằng 4). Sau đó nó được sử dụng để dự đoán lớp.

Do sự khác biệt về nguyên tắc thiết kế và kiến trúc giữa transformers và CNNs, việc làm cho các đặc trưng học sinh trực tiếp bắt chước các đặc trưng giáo viên bằng các phương pháp KD hiện có là khó khăn. Để giải quyết vấn đề này, chúng tôi đề xuất một bộ chiếu đa kiến trúc bao gồm một bộ chiếu attention chéo một phần (PCA) và một bộ chiếu tuyến tính theo nhóm (GL). Bộ chiếu PCA ánh xạ các đặc trưng học sinh vào không gian attention transformer. Bằng cách ánh xạ không gian đặc trưng CNN vào không gian attention này, việc học sinh học mối quan hệ toàn cục giữa các vùng khác nhau bằng cách tối thiểu hóa khoảng cách giữa các bản đồ attention của học sinh và giáo viên trở nên dễ dàng hơn. Bộ

--- TRANG 5 ---
Chưng Cất Tri Thức Đa Kiến Trúc 5

MVG
Linear projectionMulti -head attentionNormMLPQ
K
V
conv layerconv layerTransformer Block CNN BlockTransformer Block CNN BlockTransformer Block CNN Block...
...PCA Projector
GL ProjectorQS KS VS
QT KT VTQS KS VS
QT KT VTQT KT VT QT KT VTMLP Head
MLP Head
ClassClass
Minimize the 
distance:
DAdversarial 
loss:
KD Loss
MVG Multi -View GeneratorPCA 
ProjectorPartially Cross 
Attention Projector
GL 
ProjectorGroup -wise Linear 
ProjectorD DiscriminatorTransformer Teacher
CNN Student

Hình 2. Khung tổng thể của phương pháp đề xuất.

chiếu GL ánh xạ các đặc trưng học sinh vào không gian đặc trưng transformer. Trong không gian đặc trưng transformer này, học sinh được hướng dẫn để bắt chước các đặc trưng transformer toàn cục theo cách pixel-by-pixel.

Để cải thiện tính bền vững và ổn định của học sinh, một lược đồ huấn luyện bền vững đa góc nhìn được đề xuất. Các mẫu đa góc nhìn được tạo ra bởi một bộ tạo đa góc nhìn tiến hành ngẫu nhiên một số phép biến đổi và tạo ra mặt nạ và tiếng ồn thêm vào đầu vào. Được cung cấp các đầu vào đa góc nhìn, học sinh tạo ra các đặc trưng khác nhau. Một bộ phân biệt đối kháng đa góc nhìn được xây dựng để phân biệt các đặc trưng giáo viên và các đặc trưng học sinh trong không gian đặc trưng transformer. Sau đó mục tiêu là làm rối bộ phân biệt.

Cuối cùng, chúng tôi tích hợp các mất mát đề xuất và đưa ra huấn luyện end-to-end để có được một mạng học sinh mạnh.

3.2 Bộ chiếu đa kiến trúc

(1) Bộ chiếu attention chéo một phần Bộ chiếu attention chéo một phần (PCA) ánh xạ không gian đặc trưng học sinh vào không gian attention transformer. Nó được thiết kế để ánh xạ các đặc trưng CNN thành ma trận Query, Key, Value và sau đó bắt chước cơ chế attention. Nó bao gồm ba lớp tích chập 33:

{QS;KS;VS}= Proj 1(hS); (1)

trong đó các ma trận QS;KS;VS được tính toán và căn chỉnh để bắt chước query QT, key KT và value VT của Transformer giáo viên. Trong không gian attention transformer, self-attention của học sinh được tính toán như:

Attn S=softmax (QS(KS)T/√d)VS; (2)

trong đó d là kích thước query. Việc tính toán Attn T cũng tương tự. Do đó, chúng ta có thể tối thiểu hóa khoảng cách giữa các bản đồ attention của giáo viên và học sinh

--- TRANG 6 ---
6 Y. Liu et al.

để hướng dẫn mạng học sinh. Để cải thiện thêm tính bền vững của học sinh, chúng tôi xây dựng attention chéo một phần của học sinh để thay thế Attn S ban đầu:

PCAttn S=softmax (g(QS)(g(KS))T/√d)g(VS);
s:t: g(MS(i;j)) = {
MT(i;j); p≥0:5
MS(i;j); p < 0:5;(M=Q;K;V ): (3)

Lưu ý rằng (i;j) biểu thị chỉ số phần tử ma trận của M. Hàm g() thay thế các ma trận QS;KS;VS của học sinh bằng các ma trận tương ứng của giáo viên, với xác suất p tuân theo phân phối đều. Theo cách này, mất mát được xây dựng:

Lproj 1=||Attn T−PCAttn S||²₂+||VT VT/√d−VS VS/√d||²₂; (4)

để làm cho học sinh bắt chước giáo viên trong không gian attention.

(2) Bộ chiếu tuyến tính theo nhóm Bộ chiếu tuyến tính theo nhóm (GL) ánh xạ đặc trưng học sinh vào không gian đặc trưng transformer. Nó bao gồm một số lớp fully-connected (FC) có trọng số chia sẻ:

h'S= Proj 2(hS); (5)

trong đó h'S2RN(3hw) được căn chỉnh để có cùng chiều với đặc trưng giáo viên hT. Cụ thể, đối với đầu vào hình ảnh thông thường có kích thước 224×224, các chiều là hS2R256×196 và h'S2R196×768. Để thực hiện cách ánh xạ pixel-by-pixel, bộ chiếu cần ít nhất 196 lớp FC với 256×768 tham số. mỗi lớp ánh xạ pixel từ không gian đặc trưng ban đầu sang "pixel" tương ứng trong không gian transformer. Một số lượng lớn lớp FC có thể gây ra tính toán khổng lồ. Để có được một bộ chiếu nhỏ gọn, chúng tôi đề xuất bộ chiếu tuyến tính theo nhóm trong đó một vùng lân cận 4×4 chia sẻ một lớp FC. Do đó, bộ chiếu GL chỉ chứa 16 lớp FC. Hơn nữa, drop-out cũng được áp dụng để giảm tính toán và cải thiện tính bền vững. Cuối cùng, sau khi thu được đặc trưng học sinh được căn chỉnh mới, mất mát được tính toán như:

Lproj 2=||hT−h'S||²₂; (6)

để tối thiểu hóa khoảng cách giữa đặc trưng giáo viên và đặc trưng học sinh trong không gian đặc trưng transformer.

3.3 Huấn luyện bền vững đa góc nhìn

Do sự khác biệt lớn giữa kiến trúc của giáo viên và học sinh, việc học sinh học được bền vững không dễ dàng. Để cải thiện tính bền vững và ổn định của mạng học sinh, chúng tôi đề xuất một lược đồ huấn luyện bền vững đa góc nhìn. Lược đồ huấn luyện đề xuất chứa hai thành phần quan trọng, tức là một bộ tạo đa góc nhìn (MVG) và bộ phân biệt đối kháng đa góc nhìn tương ứng. MVG

--- TRANG 7 ---
Chưng Cất Tri Thức Đa Kiến Trúc 7

nhận hình ảnh gốc làm đầu vào, và tạo ra hình ảnh với các phép biến đổi khác nhau với một xác suất nào đó:

~x= MVG( x) = {
Trans( x); p≥0:5
x; p < 0:5; (7)

trong đó Trans() chứa các phép biến đổi phổ biến, như color jettering, random crop, rotation, patch-wise mask, v.v. Xác suất p cũng tuân theo phân phối đều. Các phiên bản biến đổi này của các mẫu sau đó được đưa vào mạng học sinh. Tiếp theo, bộ phân biệt đối kháng đa góc nhìn được xây dựng để phân biệt đặc trưng giáo viên hT và đặc trưng học sinh biến đổi h'S, bao gồm một mạng ba lớp FC. Theo cách này, mục tiêu của huấn luyện bền vững đa góc nhìn là làm rối bộ phân biệt và thu được một đặc trưng học sinh bền vững. Mất mát huấn luyện của bộ phân biệt được tính toán như:

LMAD =1/m ∑(k=1 to m)[logD(h(k)T)−log(1−D(h'(k)S))]: (8)

Lưu ý rằng D() biểu thị bộ phân biệt đối kháng đa góc nhìn. Và m là tổng số mẫu huấn luyện. Đối với mạng học sinh có thể được xem như bộ tạo trong huấn luyện đối kháng, mất mát được viết là:

LMVG =1/m ∑(k=1 to m)[log(1−D(h'(k)S))]: (9)

Tối thiểu hóa mất mát này có thể giúp tạo ra đặc trưng học sinh h'S phân phối tương tự như đặc trưng giáo viên hT.

3.4 Tối ưu hóa

Trong phần phụ này, chúng tôi giới thiệu tối ưu hóa tổng thể và quy trình huấn luyện của phương pháp đề xuất. Để huấn luyện mạng học sinh, hàm mất mát có thể được thu được bằng:

Ltotal= α(Lproj 1+Lproj 2) +LMVG; (10)

trong đó α là hệ số phạt cân bằng các thành phần mất mát. Đối với bộ phân biệt đối kháng đa góc nhìn, hàm mất mát là LMAD trong Phương trình (8).

Quy trình huấn luyện tổng thể của phương pháp đề xuất được tóm tắt trong Thuật toán 1. Chi tiết, khung giáo viên-học sinh đa kiến trúc được xây dựng trước. Bộ chiếu PCA và bộ chiếu GL sau đó được nhúng vào mạng học sinh để ánh xạ các đặc trưng học sinh vào không gian attention và không gian đặc trưng của giáo viên. Tiếp theo, một lược đồ huấn luyện bền vững đa góc nhìn được áp dụng để huấn luyện khung. Phần thân chính của khung (tức là S, Proj 1() và Proj 2()) và bộ phân biệt đối kháng đa góc nhìn D() được cập nhật luân phiên. Sau khi hội tụ, các mô-đun Proj 1(), Proj 2() và D() được loại bỏ và chỉ mạng học sinh nhỏ gọn S được giữ lại để thực hiện giai đoạn suy luận.

--- TRANG 8 ---
8 Y. Liu et al.

Thuật toán 1: Quy trình chưng cất tri thức đa kiến trúc.
Input: Database Dtrain={xtrain;ytrain}, S, T, D(), Proj 1(), Proj 2().
1 e= 0;
2 Khởi tạo S, Proj 1(), Proj 2() và D();
3 repeat
4   Tính toán các đặc trưng biến đổi h'S và {QS;KS;VS} thông qua Proj 1() và
   Proj 2(), sử dụng Phương trình. (1) và Phương trình (5);
5   Cập nhật S, Proj 1() và Proj 2() sử dụng Phương trình. (10);
6   if e%5 = 0 then
7     Cập nhật D() sử dụng Phương trình. (8);
8   end
9   e=e+ 1;
10 until done ;
11 Loại bỏ Proj 1(), Proj 2() và D(), và dự đoán nhãn thông qua S trong giai
   đoạn suy luận;
12 return S.

4 Thí nghiệm

4.1 Cài đặt

Cơ sở dữ liệu và Mạng. Chúng tôi đánh giá phương pháp đề xuất trên hai cơ sở dữ liệu: CIFAR [9] và ImageNet [8]. Dữ liệu được tăng cường sử dụng các chiến lược tương tự như trong các ví dụ chính thức của PyTorch [23]. Đối với mạng, chúng tôi sử dụng các CNN phổ biến làm mạng học sinh, bao gồm ResNets [24], MobileNet v2 [25], Xception [26] và EfficientNet [27]. Các Transformers điển hình được áp dụng làm mạng giáo viên, như ViT [3], và Swin Transformer [28].

Chi tiết triển khai. Chúng tôi huấn luyện tất cả các mạng từ đầu. Đối với tập dữ liệu CIFAR, tổng số epoch là 200 với kích thước batch tiêu chuẩn là 64. Tỷ lệ học được khởi tạo là 0.1 và nhân với 0.1 tại epoch 100 và epoch 150. Đối với ImageNet, tổng số epoch là 120 với kích thước batch 256. Tỷ lệ học được khởi tạo là 0.1 và nhân với 0.1 tại epoch 30, epoch 60 và epoch 90, tương ứng. Một bộ tối ưu hóa stochastic gradient descent (SGD) tiêu chuẩn với 10^4 weight decay và 0.9 momentum được áp dụng. Tất cả các thí nghiệm được tiến hành trên một nền tảng với 8 card GPU Nvidia Tesla và 96-core Intel(R) Xeon(R) Platinum 8163 CPU. Ngoài ra, mỗi cài đặt đơn lẻ được lặp lại 5 lần với các seed ngẫu nhiên khác nhau trên Pytorch.

4.2 So sánh hiệu suất

Chúng tôi so sánh hiệu suất của phương pháp chúng tôi với 14 phương pháp chưng cất tri thức tiên tiến, bao gồm Logits [10], FitNet [13], AT [12], IRG [17], RKD [29], CRD [30], OFD [14], ReviewKD [31], LONDON [32], AFD [33], AB [34], FT [35], DeiT [20] và MINILM [21]. Trong số đó, Logits, FitNet, AT, IRG, RKD, CRD, OFD, ReviewKD và LONDON là các phương pháp KD dựa trên CNN, và DeiT và MINILM là

--- TRANG 9 ---
Chưng Cất Tri Thức Đa Kiến Trúc 9

các phương pháp KD dựa trên transformer. Có ít công trình liên quan cho khung Transformer-CNN. Do đó, một số phương pháp dựa trên CNN bao gồm logits, RKD và IRG được áp dụng cho các tình huống đa kiến trúc, vì các phương pháp này không phụ thuộc vào kiến trúc CNN. Bên cạnh đó, để so sánh công bằng, chúng tôi chọn CNNs và Transformers với FLoating-point OPerations (FLOPs) tương tự hoặc độ chính xác tương tự làm mạng giáo viên hoặc mạng học sinh.

Đánh giá trên CIFAR. Bảng 1 trình bày kết quả KD trên CIFAR100. Như được thể hiện trong bảng này, ba chế độ KD của khung giáo viên-học sinh, bao gồm CNN-CNN, Transformer-CNN và Transformer-Transformer, được đánh giá. Có thể thấy rằng phương pháp đề xuất có hiệu suất tốt nhất trong tất cả các phương pháp, bao gồm các phương pháp KD dựa trên CNN và các phương pháp dựa trên transformer. Đối với chế độ CNN-CNN được sử dụng phổ biến nhất, phương pháp KD đa kiến trúc đề xuất cho thấy hiệu suất vượt trội. Đó là bởi vì học sinh CNN học thông tin toàn cục bổ sung từ giáo viên Transformer. Khoảng cách hiệu suất thậm chí lớn hơn (thường hơn 1%) khi giáo viên Transformer và giáo viên CNN có FLOPs tương tự. Bởi vì dưới chi phí tính toán tương tự, giáo viên Transformer thường có độ chính xác cao hơn giáo viên CNN. Đối với chế độ Transformer-CNN, một mức tăng hiệu suất cao hơn (mức tăng trung bình 2.7%) được thu được so với các phương pháp CNN-CNN. Điều này chỉ ra rằng các phương pháp KD hiện có không tận dụng hết giáo viên Transformer, mặc dù chúng có thể được áp dụng cho tình huống đa kiến trúc. Trong chế độ Transformer-Transformer, phương pháp đề xuất hầu hết vượt trội hơn kết quả KD dựa trên Transformer. Mặc dù mô hình Xceptionx2 hơi kém hơn mô hình ViT-B/16, mức tăng hiệu suất của Xceptionx2 cao hơn của ViT-B/16. Điều này chỉ ra rằng KD đa kiến trúc có thể thu được mức nâng cao hơn so với KD đồng kiến trúc thông thường. Bên cạnh đó, trong khung đa kiến trúc của chúng tôi, việc áp dụng và tăng tốc học sinh CNN vào ứng dụng thực tế dễ dàng hơn.

Đánh giá trên ImageNet. Các thí nghiệm được tiến hành trên ImageNet để xác minh thêm tính tổng quát và hiệu quả của phương pháp đề xuất. Như được thể hiện trong Bảng 2, phương pháp của chúng tôi thể hiện hiệu suất tốt nhất trên ImageNet. Tương tự như cài đặt của CIFAR, hai chế độ đồng kiến trúc bao gồm CNN-CNN và Transformer-Transformer và một chế độ đa kiến trúc, tức là Transformer-CNN, được so sánh. Khác với các phương pháp đồng kiến trúc, khung đa kiến trúc đề xuất khuyến khích học sinh học cả đặc trưng không gian cục bộ (với mô hình CNN ban đầu) và đặc trưng toàn cục bổ sung (từ mô hình giáo viên transformer). Do đó, học sinh CNN có được hiệu suất cao hơn. Đặc biệt, từ Bảng 2, một số CNNs (ví dụ, ResNet50x2-80.72%) được hướng dẫn bởi Transformer thậm chí vượt trội hơn Transformer với tính toán mô hình tương tự (ví dụ, ViT-B/32-78.29%), hơn 1.03% độ chính xác. Với các thuộc tính thân thiện với phần cứng, các CNNs được cải thiện này tiềm năng hơn cho các ứng dụng thiết bị biên.

4.3 Nghiên cứu loại bỏ

(1) Các cặp giáo viên-học sinh khác nhau. Để xác minh tính tổng quát của phương pháp đề xuất, chúng tôi đánh giá nó với các cặp giáo viên-học sinh đa kiến trúc khác nhau trong Bảng 3. Có thể quan sát thấy rằng phương pháp đa kiến trúc của chúng tôi thu được mức nâng cao hiệu suất đáng kể trên các cặp giáo viên-học sinh khác nhau, so với baseline. Trong

--- TRANG 10 ---
10 Y. Liu et al.

[THIS IS TABLE: Bảng 1. So sánh hiệu suất trên CIFAR100. Lưu ý rằng "x2" biểu thị số kênh của mạng này gấp đôi ResNet ban đầu. Và "x3" có ý nghĩa tương tự.]

[Bảng này hiển thị kết quả so sánh các phương pháp khác nhau trên CIFAR100, bao gồm ba chế độ: CNN→CNN, Transformer→CNN, và Transformer→Transformer. Bảng chứa nhiều cặp Teacher-Student với các thông số FLOPs và độ chính xác khác nhau.]

--- TRANG 11 ---
Chưng Cất Tri Thức Đa Kiến Trúc 11

[THIS IS TABLE: Bảng 2. So sánh hiệu suất trên ImageNet.]
[Bảng này hiển thị kết quả so sánh trên ImageNet với ba chế độ: CNN→CNN, Transformer→CNN, và Transformer→Transformer, bao gồm độ chính xác Top1 và Top5 cho các cặp Teacher-Student khác nhau.]

--- TRANG 12 ---
12 Y. Liu et al.

[THIS IS TABLE: Bảng 3. Kết quả hiệu suất của các cặp giáo viên-học sinh khác nhau trên ImageNet. Lưu ý rằng dấu ngoặc sau các mạng báo cáo FLOPs của các mạng.]

Teacher | Student | Teacher accuracy | Student accuracy | Ours accuracy
---|---|---|---|---
| | Top1 | Top5 | Top1 | Top5 | Top1 | Top5
ViT-B/16 (55.4G) | ResNet50 (4.1 GFLOPs) | 82.17% | 96.11% | 76.28% | 93.03% | 78.34% | 94.06%
ViT-L/16 (190.7G) | | 84.20% | 96.93% | 76.28% | 93.03% | 78.85% | 94.31%
DeiT-B (55.4G) | | 83.12% | 96.52% | 76.28% | 93.03% | 78.53% | 94.13%
Swin-B (15.4G) | | 86.38% | 98.01% | 76.28% | 93.03% | 78.87% | 94.29%
Swin-L (103.9G) | | 87.32% | 98.21% | 76.28% | 93.03% | 78.96% | 94.42%

[Bảng tiếp tục với các kết quả tương tự cho ResNet18, MobileNetV2, và EfficientNetB0]

thêm, độ chính xác của học sinh tiếp tục tăng khi hiệu suất của giáo viên trở nên tốt hơn. Cuối cùng, Transformer có thể là một giáo viên xuất sắc vì nó thường có được hiệu suất tốt hơn với FLOPs tương tự so với mạng CNN. Sử dụng Transformer để hướng dẫn việc học của học sinh CNN có thể là một hướng tiềm năng.

(2) Hiệu quả của bộ chiếu đề xuất. Chúng tôi phân tích hiệu quả của bộ chiếu PCA và bộ chiếu GL đề xuất. Kết quả thí nghiệm trên ImageNet trong Hình 3-(a) cho thấy mức tăng hiệu suất lớn khi hai bộ chiếu được tham gia trong quá trình KD. Điều này chỉ ra rằng các bộ chiếu PCA và GL cải thiện đáng kể chất lượng của đặc trưng CNN, mặc dù chúng được loại bỏ trong giai đoạn suy luận. Chúng tôi đánh giá thêm khả năng chuyển giao sau khi thêm hai bộ chiếu này trong Hình 3-(b). Độ tương tự cosine được tăng lên một mức lớn và thậm chí cao hơn của đồng kiến trúc. Do đó, có thể tăng khả năng chuyển giao tri thức giữa Transformer và CNN bằng các phương pháp KD được thiết kế cẩn thận.

(3) Hiệu quả của huấn luyện bền vững đa góc nhìn. Như được báo cáo trong Hình 3-(a), đối với đánh giá thông thường không có nhiễu, các mạng học sinh có được mức tăng độ chính xác top-1 0.2%-0.4% trên ImageNet với lược đồ huấn luyện bền vững đa góc nhìn. Để xác minh thêm hiệu quả của nó, chúng tôi cũng báo cáo kết quả cho đánh giá có nhiễu, trong đó tập dữ liệu xác thực được tăng cường khác với tăng cường huấn luyện. Dưới giao thức này, mức tăng độ chính xác top-1 sau khi thêm lược đồ huấn luyện bền vững đa góc nhìn được mở rộng đến hơn 1.0%. Điều này chứng minh rằng lược đồ huấn luyện bền vững đề xuất tăng cường tính bền vững nhiễu của mạng học sinh.

--- TRANG 13 ---
Chưng Cất Tri Thức Đa Kiến Trúc 13

[THIS IS FIGURE: Two charts labeled (a) and (b). Chart (a) shows test accuracy values ranging from 77-81 for different configurations. Chart (b) shows cosine similarity values from 0.5-0.8 for different teacher models.]

Hình 3. (a) Hiệu suất của từng thành phần trong phương pháp đề xuất. (b) Độ tương tự cosine giữa các đặc trưng từ các mô hình khác nhau. Mạng học sinh là ResNet50. Trong số các thanh màu xanh này, các đặc trưng được ánh xạ vào cùng chiều với các đặc trưng giáo viên bởi một bộ chiếu tuyến tính. Tất cả kết quả được thu thập trên ImageNet.

(4) Ứng dụng trên các nhiệm vụ khác. Phương pháp KD đa kiến trúc đề xuất cũng hoạt động tốt trên các nhiệm vụ khác. Như được thể hiện trong Bảng 4, phương pháp của chúng tôi được đánh giá trên ba nhiệm vụ thị giác bao gồm phát hiện đối tượng [36], phân đoạn thực thể [37] và chống giả mạo khuôn mặt [38].

Đối với phát hiện và phân đoạn, chúng tôi tuân theo giao thức gần đây của cơ sở dữ liệu COCO [39] và báo cáo average precision (AP). Lưu ý rằng AP trong phân đoạn được tính toán sử dụng mask intersection over union (IoU). Phương pháp đề xuất cho thấy sự vượt trội so với phương pháp KD thông thường trong Bảng 4. Đối với phương pháp KD thông thường Logits, hiệu suất của chế độ đa kiến trúc thậm chí tệ hơn hiệu suất của chế độ đồng kiến trúc. Điều này càng chứng minh rằng phương pháp của chúng tôi hiệu quả giải quyết vấn đề không khớp của KD đa kiến trúc. Ngoài ra, đối với chống giả mạo khuôn mặt, là một nhiệm vụ phân loại nhị phân, chúng tôi áp dụng ResNet18, Inception-v3 và ResNext26 làm backbone của học sinh. Equal Error Rate (EER) được báo cáo làm thước đo đánh giá. Và các thí nghiệm được tiến hành trên CelebA-Spoof [38], là một trong những tập dữ liệu lớn nhất cho chống giả mạo khuôn mặt. Đáng chú ý là có ít thông tin hữu ích về tương quan lớp trong nhiệm vụ phân loại nhị phân. Do đó, phương pháp KD thông thường Logits có cải thiện biên trong học sinh. Ngược lại, phương pháp đề xuất cũng có được hiệu suất đáng hài lòng từ Bảng 4. Thú vị là, mặc dù phương pháp đề xuất được thiết kế cho nhiệm vụ phân loại, nó có tính tổng quát tốt khi được áp dụng trực tiếp cho các nhiệm vụ khác như phát hiện và phân đoạn.

5 Kết luận

Trong bài báo này, một phương pháp chưng cất tri thức đa kiến trúc mới được đề xuất. Cụ thể, hai bộ chiếu bao gồm một bộ chiếu attention chéo một phần (PCA) và một bộ chiếu tuyến tính theo nhóm (GL) được trình bày. Hai bộ chiếu này thúc đẩy khả năng chuyển giao tri thức từ giáo viên sang học sinh. Để cải thiện thêm tính bền vững và ổn định của khung, một lược đồ huấn luyện bền vững đa góc nhìn được đề xuất. Kết quả thí nghiệm mở rộng cho thấy phương pháp của chúng tôi vượt trội hơn 14 phương pháp tiên tiến trên cả tập dữ liệu quy mô lớn và quy mô nhỏ.

--- TRANG 14 ---
14 Y. Liu et al.

[THIS IS TABLE: Bảng 4. Đánh giá trên các nhiệm vụ thị giác khác, bao gồm phát hiện đối tượng, phân đoạn thực thể và chống giả mạo khuôn mặt.]

[Bảng này chứa ba phần chính:
1. Object Detection (COCO) - với các backbone Teacher và Student khác nhau
2. Instance Segmentation (COCO) - với các backbone tương tự
3. Face Anti-Spoofing (CelebA-Spoof) - với các backbone khác nhau
Mỗi phần so sánh hiệu suất của các phương pháp Baseline, Logits và Ours]

Lời cảm ơn Công việc này được hỗ trợ bởi Chương trình Nghiên cứu và Phát triển Trọng điểm Quốc gia Trung Quốc (Mã số 2020AAA0106800), Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số 62192785, Mã số 61902401, Số 61972071, Số U1936204, Số 62122086, Số 62036011, Số 62192782 và Số 61721004), Quỹ Khoa học Tự nhiên Bắc Kinh Số M22005, Chương trình Nghiên cứu Trọng điểm CAS về Khoa học Biên giới (Mã số QYZDJ-SSW-JSC040). Công việc của Bing Li cũng được hỗ trợ bởi Hiệp hội Xúc tiến Đổi mới Thanh niên, CAS.

--- TRANG 15 ---
Chưng Cất Tri Thức Đa Kiến Trúc 15

Tài liệu tham khảo

1. Cheng, Y., Wang, D., Zhou, P., Zhang, T.: A survey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282 (2017)
2. Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., Liu, C.: A survey on deep transfer learning. In: International conference on artificial neural networks, Springer (2018) 270–279
3. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020)
4. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-to-end object detection with transformers. In: European conference on computer vision, Springer (2020) 213–229
5. Nvidia: Cuda. In: https://developer.nvidia.com/cuda-zone, Nvidia (2007)
6. Nvidia: Tensorrt. In: https://developer.nvidia.com/tensorrt, Nvidia (2022)
7. Tencent: Ncnn. In: https://github.com/Tencent/ncnn, Tencent (2017)
8. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision 115(2015) 211–252
9. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images. Technical report, Citeseer (2009)
10. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015)
11. Ba, L.J., Caruana, R.: Do deep nets really need to be deep? arXiv preprint arXiv:1312.6184 (2013)
12. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint arXiv:1612.03928 (2016)
13. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014)
14. Heo, B., Kim, J., Yun, S., Park, H., Kwak, N., Choi, J.Y.: A comprehensive overhaul of feature distillation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. (2019) 1921–1930
15. Huang, Z., Wang, N.: Like what you like: Knowledge distill via neuron selectivity transfer. arXiv preprint arXiv:1707.01219 (2017)
16. Yim, J., Joo, D., Bae, J., Kim, J.: A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2017) 4133–4141
17. Liu, Y., Cao, J., Li, B., Yuan, C., Hu, W., Li, Y., Duan, Y.: Knowledge distillation via instance relationship graph. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2019) 7096–7104
18. Song, J., Chen, Y., Ye, J., Song, M.: Spot-adaptive knowledge distillation. IEEE Transactions on Image Processing 31(2022) 3359–3370
19. Song, J., Zhang, H., Wang, X., Xue, M., Chen, Y., Sun, L., Tao, D., Song, M.: Tree-like decision distillation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2021) 13488–13497
20. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.: Training data-efficient image transformers & distillation through attention. In: International Conference on Machine Learning, PMLR (2021) 10347–10357
21. Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., Zhou, M.: Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. arXiv preprint arXiv:2002.10957 (2020)

--- TRANG 16 ---
16 Y. Liu et al.

22. Aguilar, G., Ling, Y., Zhang, Y., Yao, B., Fan, X., Guo, C.: Knowledge distillation from internal representations. In: Proceedings of the AAAI Conference on Artificial Intelligence. Volume 34. (2020) 7350–7357
23. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in pytorch. In: Advances in Neural Information Processing Systems Workshop. (2017)
24. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2016) 770–778
25. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. (2018) 4510–4520
26. Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. (2017) 1251–1258
27. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural networks. In: International conference on machine learning, PMLR (2019) 6105–6114
28. Liu, Z., Lin, Y., Cao, Y, Hu, H., Wei, Y, Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. (2021) 10012–10022
29. Park, W., Kim, D., Lu, Y, Cho, M.: Relational knowledge distillation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2019) 3967–3976
30. Tian, Y, Krishnan, D., Isola, P.: Contrastive representation distillation. arXiv preprint arXiv:1910.10699 (2019)
31. Chen, P., Liu, S., Zhao, H., Jia, J.: Distilling knowledge via knowledge review. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2021) 5008–5017
32. Shang, Y, Duan, B., Zong, Z., Nie, L., Yan, Y: Lipschitz continuity guided knowledge distillation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. (2021) 10675–10684
33. Wang, K., Gao, X., Zhao, Y, Li, X., Dou, D., Xu, C.Z.: Pay attention to features, transfer learn faster cnns. In: International conference on learning representations. (2019)
34. Heo, B., Lee, M., Yun, S., Choi, J.Y: Knowledge transfer via distillation of activation boundaries formed by hidden neurons. In: Proceedings of the AAAI Conference on Artificial Intelligence. Volume 33. (2019) 3779–3787
35. Kim, J., Park, S., Kwak, N.: Paraphrasing complex network: Network compression via factor transfer. Advances in neural information processing systems 31(2018)
36. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems 28(2015) 91–99
37. He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask r-cnn. In: Proceedings of the IEEE international conference on computer vision. (2017) 2961–2969
38. Zhang, Y, Yin, Z., Li, Y, Yin, G., Yan, J., Shao, J., Liu, Z.: Celeba-spoof: Large-scale face anti-spoofing dataset with rich annotations. In: European Conference on Computer Vision, Springer (2020) 70–85
39. Lin, T.Y, Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: European conference on computer vision, Springer (2014) 740–755
