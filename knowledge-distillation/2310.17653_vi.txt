# 2310.17653.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.17653.pdf
# Kích thước tệp: 1640328 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
NHỮNG LỢI ÍCH TUYỆT VỜI VÀ NƠIIIIIIIII TÌM RA CHÚNG : VỀ
SỰ TỒN TẠI VÀ TRIỂN VỌNG CỦA CHUYỂN GIAO KIẾN THỨC TỔNG QUÁT
GIỮA BẤT KỲ MÔ HÌNH TIỀN HUẤN LUYỆN NÀO
Karsten Roth1,∗, Lukas Thede1,∗, A. Sophia Koepke1
Oriol Vinyals2, Olivier Hénaff2, Zeynep Akata1,3,4
1Tübingen AI Center & University of Tübingen,2Google DeepMind
3Helmholtz Munich,4Technical University of Munich
∗đóng góp ngang nhau
TÓM TẮT
Huấn luyện mạng sâu đòi hỏi nhiều quyết định thiết kế khác nhau liên quan đến ví dụ như
kiến trúc của chúng, tăng cường dữ liệu, hoặc tối ưu hóa. Trong công trình này, chúng tôi
thấy rằng những biến thể huấn luyện này dẫn đến việc mạng học được những tập hợp đặc
trưng độc đáo từ dữ liệu. Sử dụng các thư viện mô hình công khai bao gồm hàng nghìn mô
hình được huấn luyện trên các tập dữ liệu kinh điển như ImageNet, chúng tôi quan sát thấy
rằng đối với việc ghép cặp tùy ý của các mô hình tiền huấn luyện, một mô hình trích xuất
ngữ cảnh dữ liệu quan trọng không có sẵn trong mô hình khác – độc lập với hiệu suất tổng
thể. Với bất kỳ cặp ghép tùy ý nào của các mô hình tiền huấn luyện và không có xếp hạng
bên ngoài (như các tập kiểm tra riêng biệt, ví dụ do quyền riêng tư dữ liệu), chúng tôi nghiên
cứu xem có thể chuyển giao kiến thức "bổ sung" như vậy từ mô hình này sang mô hình khác
mà không làm suy giảm hiệu suất hay không – một nhiệm vụ đặc biệt khó khăn vì kiến thức
bổ sung có thể được chứa trong các mô hình mạnh hơn, tương đương hoặc yếu hơn. Tuy
nhiên, việc tạo điều kiện cho chuyển giao mạnh mẽ trong các tình huống bất khả tri đối với
việc ghép cặp mô hình tiền huấn luyện sẽ mở khóa hướng dẫn huấn luyện, lợi ích phụ trợ
và hợp nhất kiến thức từ bất kỳ kho mô hình nào mà không có hạn chế về đặc tính mô hình
& vấn đề - bao gồm từ các mô hình yếu hơn, hiệu suất thấp hơn. Công trình này cung cấp
một khảo sát đầu tiên, sâu sắc về tính khả thi của chuyển giao kiến thức đa mục đích như vậy.
Qua các thí nghiệm quy mô lớn, trước tiên chúng tôi tiết lộ những thiếu sót của kỹ thuật
chưng cất kiến thức tiêu chuẩn, và sau đó đề xuất một mở rộng tổng quát thông qua phân
vùng dữ liệu để chuyển giao thành công giữa gần như tất cả các mô hình tiền huấn luyện -
cũng có thể được thực hiện không giám sát. Cuối cùng, chúng tôi đánh giá cả khả năng mở
rộng và tác động của các thuộc tính mô hình đến chuyển giao kiến thức bất khả tri mô hình
thành công.

1 GIỚI THIỆU
Huấn luyện mạng neural trên các tập dữ liệu cụ thể đã trở thành tiêu chuẩn học máy để giải quyết
vô số thách thức nghiên cứu và công nghiệp, bao gồm một số lượng lớn các quyết định rõ ràng và
ngầm định từ lựa chọn kiến trúc đến các giao thức tối ưu hóa cụ thể, việc lựa chọn cụ thể về tăng
cường dữ liệu, lấy mẫu dữ liệu và thậm chí thứ tự dữ liệu. Tất cả những yếu tố này có thể ảnh
hưởng đến kiến thức ngữ nghĩa mà một mô hình có thể trích xuất từ một tập dữ liệu (Bouthillier
et al., 2021; Schmidt et al., 2021; Wang et al., 2023; Raghu et al., 2021; Wagner et al., 2022; Roth
et al., 2020; Balestriero et al., 2023; Teney et al., 2020; Roth et al., 2023), và cùng nhau cung cấp
một dấu vân tay độc đáo về khả năng của mô hình. Trong công trình này, trước tiên chúng tôi nêu
bật mức độ của tuyên bố này thông qua các thí nghiệm mở rộng. Chúng tôi xây dựng trên các thư
viện mô hình mở lớn (ví dụ timm (Wightman, 2019) hoặc huggingface) để so sánh số lượng lớn
các cặp mô hình tiền huấn luyện tùy ý. Làm như vậy, chúng tôi khám phá ra sự tồn tại nhất quán
của kiến thức bổ sung đáng kể - thông tin về dữ liệu mà một mô hình ("giáo viên") nắm giữ không
có sẵn trong mô hình khác ("học sinh"). Thú vị là, chúng tôi thấy rằng kiến thức bổ sung tồn tại
bất kể xếp hạng hiệu suất bên ngoài hoặc các yếu tố như họ mô hình (CNN (LeCun và Bengio,
1995), Transformer (Dosovitskiy et al., 2021), MLP (Tolstikhin et al., 2021)), và thường tập hợp
trong các lĩnh vực chuyên môn ngữ nghĩa: Đối với những giáo viên mạnh hơn, nhưng đặc biệt
cũng tương tự hoặc yếu hơn (theo một số thước đo kiểm tra), có thể tìm thấy kiến thức quan trọng
về dữ liệu không có sẵn cho học sinh.

Sự có sẵn tổng quát của kiến thức bổ sung như vậy đặt ra câu hỏi về tiềm năng hữu ích của nó.
Để trả lời những điều đó, chúng tôi cung cấp một khảo sát đầu tiên, sâu sắc. Cụ thể, với các cặp
mô hình tùy ý được tiền huấn luyện trên cùng dữ liệu mà không có quyền truy cập vào các biện
pháp xếp hạng bên ngoài (như tập kiểm tra, do ví dụ quyền riêng tư dữ liệu, máy chủ kiểm tra
riêng biệt, ...), chúng tôi khám phá xem việc chuyển giao kiến thức bổ sung giữa bất kỳ giáo viên
và học sinh nào có thể thực hiện được mà không làm suy giảm hiệu suất hay không. Việc đạt được
chuyển giao như vậy thông qua bất kỳ cặp mô hình có thể nào sẽ mở khóa bất kỳ bộ sưu tập mô
hình có sẵn miễn phí hoặc tự tạo nào như một nguồn tài nguyên phụ trợ cho các lợi ích trong tiền
huấn luyện kinh điển và dành riêng cho vấn đề. Nó cũng tránh được nhu cầu chuyển giao dành
riêng cho mô hình đòi hỏi kiến thức chuyên gia, và giảm sự phụ thuộc vào các biện pháp đánh giá
bên ngoài để lựa chọn mô hình. Quan trọng hơn, nó cũng cho phép cải thiện các mô hình lớn hơn
bằng việc chuyển giao kiến thức từ các mô hình yếu hơn, tài nguyên thấp hơn, mà không cần rõ
ràng dữ liệu & giám sát bổ sung, hoặc hy sinh về ví dụ tốc độ, công bằng, khả năng diễn giải hoặc
dễ sử dụng.

Chúng tôi nghiên cứu giới hạn của chưng cất kiến thức (Hinton et al., 2015; Tian et al., 2020) cho
nhiệm vụ này, trong khi tương phản với các phương pháp không có dữ liệu (ví dụ Wortsman et al.
(2022a)), hoạt động độc lập với các lựa chọn mô hình. Tuy nhiên, các khung chưng cất kiến thức
tiêu chuẩn giả định thông tin được chưng cất đến một học sinh chưa được huấn luyện. Ngược lại,
chúng tôi chỉ muốn chuyển giao kiến thức không có sẵn trong một mô hình học sinh đã được huấn
luyện, thậm chí có thể vượt trội hơn giáo viên của nó. Điều này quan trọng kéo theo một sự đánh
đổi thành công giữa việc thu được kiến thức và duy trì. Thật vậy, đối với chuyển giao kiến thức
giữa các mô hình tiền huấn luyện tùy ý, chưng cất phổ thông (§5.1) thể hiện sự phụ thuộc mạnh
mẽ vào mô hình/siêu tham số và sụt giảm hiệu suất cho phần lớn các mô hình học sinh, đặc biệt
đối với các giáo viên yếu hơn/tương đương. Điều này có thể được quy cho việc quên thảm khốc
(Kirkpatrick et al., 2016; Zenke et al., 2017) vượt qua lợi ích của chuyển giao kiến thức bổ sung
từ giáo viên.

Để có sự đánh đổi thuận lợi giữa việc quên và thu được kiến thức, chúng tôi coi quá trình chuyển
giao như một vấn đề học liên tục, nơi một mô hình được liên tục trình bày với ngữ cảnh mới cho
dữ liệu đã thấy. Để khuyến khích duy trì, trước tiên chúng tôi nghiên cứu nội suy trọng số
(Stojanovski et al., 2022; Wortsman et al., 2022b). Mặc dù tốt hơn chưng cất bình thường, nhưng
thường là ràng buộc quá mạnh khi các giáo viên có lĩnh vực chuyên môn thích hợp hoặc tổng thể
mạnh hơn. Do đó chúng tôi đề xuất ràng buộc chưng cất ở cấp độ dữ liệu bằng cách phân vùng
nó thành hai tập - một với các mẫu mà chuyển giao từ giáo viên được mong muốn, và một nơi
chúng tôi muốn duy trì hành vi của học sinh. Điều này đưa ra ít ràng buộc hơn đáng kể trên trọng
số mô hình để học từ ngữ cảnh giáo viên tùy ý, trong khi giảm việc quên bằng cách duy trì hiệu
suất ban đầu trên các mẫu mà giáo viên có tác động tích cực hạn chế (thậm chí có hại). Hơn nữa,
phân vùng dữ liệu của chúng tôi có thể đạt được mà không cần bất kỳ giám sát nào.

Làm như vậy, chúng tôi thấy sự gia tăng đáng kể trong tỷ lệ thành công (lợi ích khác không của
học sinh) cho tất cả các cặp ghép giáo viên-học sinh - từ 32,5% với chưng cất bình thường đến
92,5% với phân vùng dữ liệu. Cách chính quy hóa cấp độ dữ liệu của chúng tôi là cài đặt duy nhất
cho phép chuyển giao tích cực nhất quán từ các giáo viên yếu hơn, trong khi duy trì hiệu suất
chuyển giao của chưng cất bình thường cho các giáo viên mạnh hơn nhiều và thậm chí vượt trội
hơn chưng cất bình thường cho những giáo viên tương đương. Ngoài ra, nó cho phép chuyển giao
kiến thức chuyên biệt (§5.1) và không yêu cầu siêu tham số dành riêng cho việc ghép cặp. Không
giống như các phương pháp kết hợp (Lakshminarayanan et al., 2017; Gontijo-Lopes et al., 2022;
Sinha et al., 2021; Dietterich, 2000), phương pháp của chúng tôi duy trì chi phí suy luận ban đầu
và xử lý sự khác biệt hiệu suất cao. Cuối cùng, chúng tôi nghiên cứu các thuộc tính kiến trúc và
tác động của chúng đến quá trình chuyển giao (§5.1) ngoài phương pháp chuyển giao, và xem xét
khả năng mở rộng đến chuyển giao kiến thức từ nhiều mô hình, nơi chúng tôi thấy rằng chuyển
giao tuần tự đơn giản có thể hoạt động tốt khi tận dụng phương pháp chuyển giao của chúng tôi,
đạt được những cải thiện rõ ràng so với chuyển giao chỉ từ mô hình giáo viên tốt nhất duy nhất.

Nhìn chung, đóng góp của chúng tôi có thể được tóm tắt như: (1) Chúng tôi khám phá ra sự tồn
tại nhất quán của kiến thức bổ sung giữa các mô hình tùy ý được tiền huấn luyện trên cùng tập dữ
liệu - ngay cả khi họ mô hình hoặc hiệu suất khác nhau. (2) Chúng tôi tiến hành các nghiên cứu
khám phá mở rộng để điều tra khả năng chuyển giao được đảm bảo độc lập với mô hình và hiệu
suất của kiến thức bổ sung mà không làm suy giảm hiệu suất. (3) Chúng tôi đề xuất một phương
pháp chuyển giao thành công được thúc đẩy thông qua góc độ học liên tục, tận dụng phương pháp
phân vùng dữ liệu dựa trên độ tin cậy, không có siêu tham số. (4) Chúng tôi cung cấp các nghiên
cứu về mối quan hệ của các thuộc tính mô hình tổng quát đến chuyển giao kiến thức tổng quát, và
(5) điều tra chuyển giao kiến thức giữa nhiều mô hình. Mã sẽ được phát hành khi được chấp nhận.

2 CÔNG TRÌNH LIÊN QUAN
Các công trình đầu trong chưng cất kiến thức tập trung vào việc nén các mô hình giáo viên lớn
thành các mô hình học sinh nhỏ hơn. Bucila et al. (2006) đạt được điều này bằng cách khớp các
mục tiêu mềm của giáo viên. Hinton et al. (2015) đề xuất chia tỷ lệ nhiệt độ cho xác suất thấp
hơn. Các công trình gần đây mở rộng điều này với ngữ cảnh cấu trúc: chuyển giao chú ý
(Zagoruyko và Komodakis, 2017) khuyến khích các mẫu phản hồi đặc trưng tương tự

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

−20 −10 0 10 20
Sự Khác Biệt Hiệu Suất0510152025Kiến Thức Bổ Sung [%]Hiệu Suất Ngang Bằng
Kiến Thức Bổ Sung Tối Thiểu
(a)
0 5 10 15 20 25
Kiến Thức Bổ Sung [%]5.86.06.26.46.66.8Entropy
Phân Phối Đồng Đều (b)

Hình 1: (a) Chúng tôi hiển thị tỷ lệ kiến thức bổ sung (ρpos, phần trăm các lật nhãn dương của
giáo viên w.r.t. học sinh) so với sự khác biệt hiệu suất ∆acc cho 466 cặp giáo viên/học sinh, và
thấy ngữ cảnh bổ sung đáng kể ngay cả đối với các giáo viên yếu hơn nhiều. (b) Nhìn vào entropy
của phân phối kiến thức bổ sung trên các lớp, chúng tôi thấy ngữ cảnh chuyên biệt hơn cho các
giáo viên yếu hơn.

Romero et al. (2015) và Zagoruyko và Komodakis (2017) đề xuất hồi quy có trọng số hướng dẫn
kích hoạt đặc trưng học sinh. Tuy nhiên, những phương pháp này bị giới hạn trong việc khớp kiến
trúc học sinh và giáo viên. Tian et al. (2020) đề xuất chưng cất tương phản, căn chỉnh không gian
đặc trưng giáo viên và học sinh với tính linh hoạt trong chiều đặc trưng, trong khi nêu bật sự chồng
chéo hiệu suất giữa hầu hết các mục tiêu chưng cất. Những hiểu biết này chuyển sang chưng cất
nhiều giáo viên (Luo et al., 2019; de Carvalho et al., 2022; Shen et al., 2019a;b), ví dụ thông qua
tái trọng số đầu ra giáo viên (Wu et al., 2021; Liu et al., 2020; Yu et al., 2022; Yuan et al., 2020a).
Không giống như chưng cất tiêu chuẩn, chúng tôi xem xét chuyển giao kiến thức giữa các mô hình
tùy ý, đã được huấn luyện - một nhiệm vụ khó khăn hơn nhiều, đặc biệt khi không có hạn chế
(trái ngược với ví dụ (Yuan et al., 2020b)) được áp đặt lên hiệu suất tương đối hoặc kiến trúc, và
kiến thức ban đầu nên được duy trì. Ở cấp độ khái niệm, điều này cũng kết nối với các công trình
gần đây về tổng quát hóa mô hình yếu đến mạnh cho superalignment, mà công trình của chúng tôi
có thể cung cấp một góc nhìn trực giao và những hiểu biết thực tế hữu ích (Burns et al., 2023).

Wortsman et al. (2022a;b) cho thấy rằng khi kiến trúc và tiền huấn luyện được chia sẻ, nội suy
tuyến tính có thể hiệu quả đáng ngạc nhiên cho các lưu vực mất mát phù hợp (Neyshabur et al.,
2020). Tuy nhiên, việc lựa chọn mô hình thông qua các thước đo xác thực là chìa khóa cho các bộ
sưu tập đa dạng. Ngược lại, chúng tôi kết hợp kiến thức mô hình mà không có hạn chế bằng cách
tận dụng các góc nhìn từ Học Liên Tục. Các phương pháp được phân loại thành chính quy hóa
(giới hạn thay đổi trọng số (Kirkpatrick et al., 2017; Schwarz et al., 2018; Li và Hoiem, 2016;
Rebuffi et al., 2016; Castro et al., 2018)), phát lại (Rebuffi et al., 2016; Lopez-Paz và Ranzato,
2017; Chaudhry et al., 2019; Buzzega et al., 2020; Prabhu et al., 2020) thông qua bộ nhớ dữ liệu,
và các phương pháp tái cấu trúc mạng (Mallya và Lazebnik, 2018; Mallya et al., 2018; Zhang et
al., 2020). Ngoài ra, nội suy cũng đã chứng minh hữu ích khi liên tục thích ứng từ một mô hình
tiền huấn luyện (Stojanovski et al., 2022), mà chúng tôi bao gồm trong nghiên cứu chuyển giao
của mình.

3 KIẾN THỨC BỔ SUNG GIỮA CÁC MÔ HÌNH TIỀN HUẤN LUYỆN

Trong những năm gần đây, hàng nghìn mô hình được tiền huấn luyện hoàn chỉnh trên các tập dữ
liệu kinh điển như ImageNet đã được công khai, với các biến thể trên tất cả các lựa chọn huấn luyện
có thể (§1), có khả năng ảnh hưởng đến tổng quát hóa - mức độ mà chúng tôi muốn nghiên cứu
trong phần này. Cụ thể, chúng tôi sử dụng timm (Wightman, 2019), bao gồm hàng trăm mô hình
được huấn luyện trên ImageNet dưới các điều kiện khác nhau, và xem xét vấn đề phân loại hình
ảnh với không gian đầu vào X và không gian nhãn Y với c=|Y| nhãn. Đặt f(·, θ) : X → Rc là một
bộ phân loại với tham số θ ∈ Θ, logits z=f(x, θ) và softmax σ(z) với σj(z) = exp(zj)/∑i exp(zi)
liên kết với các mẫu x ∈ X. Chúng tôi sử dụng ft, fs để biểu thị giáo viên và học sinh tiền huấn
luyện, với tham số θt và θs tương ứng. Để đánh giá tính bổ sung của kiến thức giữa bất kỳ ft và
fs nào, chúng tôi tuân theo phương pháp của Lopes et al. (2022) và nghiên cứu hiệu suất trên tập
xác thực ImageNet. Cụ thể, chúng tôi đo các lật dự đoán tích cực giữa ft và fs, ρpos=1/n ∑n i=1
ρpos i, trong đó ρpos i chỉ ra một lật tích cực. Điều này định lượng tỷ lệ các mẫu được phân loại
đúng bởi giáo viên nhưng sai bởi học sinh - kiến thức bổ sung. Đối với phần còn lại của công trình
này, chúng tôi sẽ gọi kiến thức bổ sung và sự tồn tại của những lật tích cực này một cách hoán đổi.

Sự tồn tại của kiến thức bổ sung. Sử dụng timm, chúng tôi ngẫu nhiên chọn 466 cặp mô hình
(ft, fs) bao gồm 301 mô hình độc đáo có kiến trúc, kích thước và hiệu suất khác nhau. Trong Hình
1, chúng tôi

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Các Lớp ImageNet0204060iii) Giáo Viên Mạnh
(Khác biệt: +18)
Các Lớp ImageNet02040ii) Độ Chính Xác Ngang Bằng
(Khác biệt: ±0)
Các Lớp ImageNet0102030Kiến Thức Bổ Sung 
 trên Lớp [%]i) Giáo Viên Yếu
(Khác biệt: -13)
(a)
Top-2% Top-5% Top-20% Top-50%
Các Lớp với Top-X% Kiến Thức Bổ Sung+6%+8%+10%+12%Khác biệt với Sim. Trung bình [%]
 (b)

Hình 2: (a) Biểu đồ sắp xếp của kiến thức bổ sung trên lớp (lật dự đoán tích cực) như tỷ lệ của
tổng mẫu lớp cho ba cặp giáo viên-học sinh (giáo viên yếu, ngang bằng và mạnh). Ngữ cảnh bổ
sung được tập trung xung quanh một số lớp. (b) Độ tương tự ngữ nghĩa giữa các lớp top-X% được
sắp xếp theo lượng kiến thức bổ sung. Hiển thị như sự khác biệt tương đối với độ tương tự lớp
trung bình. Các lớp có nhiều kiến thức bổ sung nhất có khả năng liên quan ngữ nghĩa.

điều tra tỷ lệ kiến thức bổ sung so với sự khác biệt trong hiệu suất giữa ft và fs. Điều này được đo
bằng sự khác biệt trong độ chính xác xác thực ∆acc=acc(ft)−acc(fs) cho mỗi cặp. Chúng tôi thấy
một tỷ lệ lớn các lật dự đoán tích cực khi ft vượt trội fs. Nhưng ngay cả khi ft hoạt động kém hơn
đáng kể so với mô hình học sinh (lên đến 20%), vẫn có thể tìm thấy tỷ lệ cao các lật tích cực.
Hội tụ khoảng 2%, tỷ lệ phần trăm này cao hơn một bậc so với nhiễu ngẫu nhiên - đối với c=1000
lớp, xác suất để một mô hình sửa một mẫu bằng cơ hội là khoảng 0,1% (ví dụ các giáo viên
ResNet50 ngẫu nhiên hóa chỉ ra kiến thức bổ sung khoảng chỉ 0,03%). Như vậy, kết quả của
chúng tôi chỉ ra sự tồn tại nhất quán kiến thức bổ sung giữa bất kỳ mô hình tiền huấn luyện nào.

Hiểu về kiến thức bổ sung. Để tìm hiểu xem kiến thức bổ sung có được phân phối đều giữa các
lớp hay nó được nhóm một cách có hệ thống trong các tập con cụ thể, chúng tôi phân tích phân
phối các lật dự đoán trên tất cả các lớp. Đối với một tập con được chọn trong Hình 2a, nơi các lớp
được sắp xếp theo lượng kiến thức bổ sung được mã hóa, chúng tôi thấy một số lớp mang lượng
ngữ cảnh không tỷ lệ, đặc biệt trong trường hợp giáo viên yếu hơn. Đối với những giáo viên mạnh
hơn, ngữ cảnh bổ sung mà giáo viên có thể cung cấp trở nên phân phối đều hơn. Điều này được
hỗ trợ thêm khi nhìn vào entropy của những phân phối này cho tất cả các cặp (ft, fs) trong Hình 1
(phải), nơi chúng tôi thấy một xu hướng rõ ràng hướng tới ngữ cảnh tập hợp hơn cho các giáo viên
yếu hơn khi entropy giảm xuống. Tuy nhiên, trong tất cả các trường hợp, một mức độ nhóm ngữ
cảnh đáng kể vẫn còn. Chúng tôi gọi những nhóm lớp này với kiến thức bổ sung đáng kể là các
lĩnh vực chuyên môn tương đối. Khái niệm này trở nên rõ ràng hơn khi chúng tôi điều tra mối
quan hệ ngữ nghĩa giữa chúng trong Hình 2b. Cho điều này, chúng tôi đo độ tương tự ngữ nghĩa
của các lớp chứa 2%, 5%, 10%, 20% và 50% đầu tiên của các lật tích cực (dựa trên xếp hạng trên
lớp theo kiến thức bổ sung như trong Hình 2a) sử dụng một mô hình ngôn ngữ tiền huấn luyện
(CLIP, Radford et al. (2021)). So sánh độ tương tự đo được với độ tương tự trung bình của tất cả
các lớp, chúng tôi thấy sự gia tăng tương đối trong độ tương tự ngữ nghĩa gần như gấp đôi trung
bình cho các lớp mã hóa nhiều kiến thức bổ sung nhất.

Tóm lại, chúng tôi quan sát thấy rằng kiến thức bổ sung giữa bất kỳ cặp mô hình tiền huấn luyện
nào tồn tại, và kiến thức này mà một giáo viên tiền huấn luyện có thể truyền cho học sinh được
tập trung xung quanh các lĩnh vực chuyên môn bao gồm các lớp liên quan ngữ nghĩa. Sự tồn tại
của kiến thức bổ sung phổ biến thúc đẩy nghiên cứu của chúng tôi về các công cụ chuyển giao
kiến thức đa mục đích có thể.

4 PHƯƠNG PHÁP CHUYỂN GIAO KIẾN THỨC TỔNG QUÁT

Phần này giải thích các mục tiêu chuyển giao kiến thức có thể, bắt đầu từ chưng cất kiến thức tiêu
chuẩn (§4.1) đến các mở rộng được đề xuất của chúng tôi trong §4.2, điều này nêu bật cách thức
và lý do tại sao vấn đề này nên được coi như một vấn đề học liên tục. §4.3 mở rộng điều này đến
nhiều giáo viên tiền huấn luyện.

4.1 CHUYỂN GIAO KIẾN THỨC THÔNG QUA CHƯNG CẤT KIẾN THỨC

Chưng Cất Kiến Thức (chưng cất KL) được tiên phong bởi Hinton et al. (2015), đề xuất tối thiểu
hóa độ phân kỳ KL giữa các mục tiêu mềm của mô hình giáo viên và học sinh σ(zt) và σ(zs):
LKL=T²/n ∑ⁿᵢ₌₁ KL[σ(zs,i/T), σ(zt,i/T)], (1)

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Các Mẫu Batch Các Mẫu Batch 
Mô Hình 
Học Sinh  - Chuyển Giao 
Kiến Thức ≤  =
> =
Mô Hình 
Giáo Viên   f 
Mô Hình 
Học Sinh-Giáo Viên    t

Hình 3: Đối với chuyển giao kiến thức tổng quát giữa bất kỳ mô hình tiền huấn luyện nào, chúng
tôi đề xuất chính quy hóa cấp độ dữ liệu: Các mẫu được tách ra dựa trên việc chúng nên được
dạy thông qua giáo viên ft hay được giữ lại thông qua phiên bản đông lạnh của học sinh ban đầu,
fst. Tất cả các mô hình chuyển tiếp cùng batch, và các đầu ra σ(zt) và σ(zst) được hợp nhất ở cấp
độ mẫu thông qua mặt nạ lựa chọn mt và mst được dẫn xuất từ độ tin cậy mô hình (Phương trình
4). Cuối cùng, chúng tôi tính KL-div. với các đầu ra của học sinh thích ứng (fs) σ(zs).

với nhiệt độ T. Chúng tôi sử dụng Phương trình 1 như mục tiêu chuyển giao cơ sở của chúng tôi
(chuyển giao KL-Dist), vì nó vẫn phổ biến và cạnh tranh (Beyer et al., 2022; Rajasegaran et al.,
2020; Tian et al., 2020) (xem Phụ lục cho các mục tiêu khác). Chưng cất KL thường được mở
rộng với các mất mát phân loại phụ trợ (ví dụ cross-entropy LXE, (Hinton et al., 2015; Tian et al.,
2020; Rajasegaran et al., 2020; Beyer et al., 2022)) để ổn định quá trình chưng cất. Chúng tôi ký
hiệu kết hợp trọng số λ là chưng cất XE-KL, và chuyển giao liên kết là chuyển giao XE-KL-Dist.
hoặc XE-KL:
Ldist=λ· LKL+ (1−λ)· LXE. (2)

Hầu hết nghiên cứu chưng cất kiến thức xem xét việc chưng cất từ một giáo viên được huấn luyện
đến một học sinh chưa được huấn luyện, trái ngược hoàn toàn với mục tiêu của chúng tôi là chuyển
giao kiến thức giữa các mô hình tiền huấn luyện trong khi duy trì kiến thức học sinh đã có được từ
trước. Và thật vậy, khi áp dụng cho chuyển giao kiến thức giữa các mô hình tiền huấn luyện trong
§5.1, chưng cất kiến thức tiêu chuẩn gặp khó khăn trong việc chuyển giao kiến thức mà không
sụt giảm hiệu suất cho hầu hết các cặp giáo viên-học sinh. Chúng tôi đo điều này thông qua delta
chuyển giao ∆transf=acc(fskt)−acc(fs), định lượng sự thay đổi trong độ chính xác top-1 của học
sinh, với fskt là mô hình học sinh sau chuyển giao kiến thức.

4.2 CHUYỂN GIAO KIẾN THỨC THÔNG QUA CHƯNG CẤT KIẾN THỨC LIÊN TỤC

Để chuyển giao kiến thức thành công, cần có sự đánh đổi thuận lợi giữa việc duy trì kiến thức học
sinh hiện có và kết hợp kiến thức giáo viên bổ sung. Điều này có sự tương tự với các khung Học
Liên Tục (CL) (Kirkpatrick et al., 2016; Zenke et al., 2017; Aljundi et al., 2019), nhằm huấn luyện
các mô hình trên các luồng dữ liệu tăng dần mà không quên ngữ cảnh đã học trước đó.

Ràng buộc cập nhật trọng số. Không giống như CL tiêu chuẩn, vấn đề của chúng tôi không phải
là của một luồng dữ liệu tăng dần, mà là các tín hiệu học tập mới liên tục từ giáo viên trên cùng
dữ liệu chuyển giao. Điều này loại trừ các phương pháp dựa trên bộ nhớ, nhưng cho phép chính
quy hóa (xem §2 để biết chi tiết), nơi Stojanovski et al. (2022) cho thấy rằng đối với thích ứng
liên tục của các mô hình được huấn luyện, nội suy trọng số động lượng (MCL) chứng minh hiệu
quả hơn các chính quy hóa hiện có. Do đó chúng tôi mở rộng mục tiêu XE-KL cơ sở của chúng
tôi với MCL (chuyển giao XE-KL-Dist+MCL /XE-KL+MCL). Nội suy trọng số trong MCL duy
trì một bản sao chậm của trọng số học sinh θslow s ngoài trọng số nhanh θfast s được cập nhật
trong quá trình chuyển giao. Ở tần suất định trước N và lần lặp i, θslow s được cập nhật thông
qua nội suy không gian trọng số:
θslow,i+1 s =τ·θslow,i s+ (1−τ)·θfast,i s, (3)

với động lượng τ hướng dẫn ràng buộc trọng số. Cả N và τ có thể được điều chỉnh để cân bằng
sự đánh đổi tính dẻo-ổn định. Tuy nhiên, vì nội suy trọng số ràng buộc cập nhật trọng số cho tất
cả các mẫu một cách bình đẳng, nó gặp khó khăn trong việc tận dụng các lĩnh vực chuyên môn
tương đối của giáo viên đến mức tối đa (tham khảo §5.1).

Ràng buộc dữ liệu chuyển giao. Thay vì hạn chế ở cấp độ trọng số, chúng tôi đề xuất chính quy
hóa ở cấp độ dữ liệu bằng cách phân vùng dữ liệu chuyển giao thành các mẫu mà học sinh có thể
hưởng lợi từ phản hồi giáo viên và những mẫu mà kiến thức trước đó nên được duy trì. Cụ thể,
đối với các mẫu cần thiết cho việc chuyển giao kiến thức bổ sung, chúng tôi chưng cất từ giáo viên
ft. Tuy nhiên, đối với tập mẫu khác, thay vào đó chúng tôi chưng cất từ mô hình học sinh ban đầu,
đông lạnh (ký hiệu là fst cho học sinh-giáo viên), với mục tiêu duy trì hành vi mô hình ban đầu.
Việc lựa chọn những tập con dữ liệu này tuân theo một phương pháp heuristic tham lam đơn giản
và hiệu quả gán mỗi mẫu tùy thuộc vào xác suất dự đoán cao nhất cho lớp thực tế tương ứng, đưa
ra mặt nạ dữ liệu mt và mst

mt i=I[σj(zt,i)> σj(zst,i)], mst i=I[σj(zt,i)≤σj(zst,i)], (4)

cho mỗi mẫu i và j=yi, và với σj(z) là đầu ra softmax cho lớp j. Trong thực tế, chúng tôi thấy
rằng việc sử dụng những mặt nạ này cung cấp đủ ổn định cho quá trình chuyển giao nơi phân loại
phụ trợ không còn cần thiết. Ngoài ra, chúng tôi cũng thấy rằng phương pháp heuristic có giám sát
này có thể được thay thế bằng một phương pháp hoàn toàn không giám sát bằng cách gán các mẫu
dựa trên xác suất dự đoán tối đa cho một mẫu, tức là chọn mô hình theo độ tin cậy. Mặc dù điều
này có thể bị ảnh hưởng bởi quá tin tưởng, trong thực tế hiệu suất khớp với cài đặt có giám sát.
Điều này có nghĩa là chuyển giao kiến thức có thể được thực hiện mà không cần truy cập vào nhãn.
Chúng tôi cung cấp một hình ảnh trực quan về phân vùng dữ liệu (DP) của chúng tôi trong Hình 3.
Phương pháp chuyển giao cuối cùng mà chúng tôi gọi là chuyển giao KL-Dist +DP, do đó được
đưa ra như:
Ldist=T²/n ∑ⁿᵢ₌₀ mt i·KL[σ(zs,i/T), σ(zt,i/T)] +mst i·KL[σ(zs,i/T), σ(zst,i/T)].(5)

Như có thể thấy, chuyển giao KL-Dist + DP (hoặc KL+DP) không yêu cầu siêu tham số bổ sung
so với chưng cất kiến thức tiêu chuẩn, với tính mạnh mẽ cao đối với lựa chọn nhiệt độ (tham khảo
Phụ lục).

4.3 CHUYỂN GIAO KIẾN THỨC NHIỀU GIÁO VIÊN

Với nhiều mô hình có sẵn trong các khu vườn mô hình, việc nghiên cứu cách chuyển giao ngữ
cảnh giữa nhiều chuyên gia là một mở rộng tự nhiên, mà chúng tôi đề xuất nghiên cứu ba phương
pháp. Thứ nhất, phù hợp với chưng cất kiến thức nhiều giáo viên tiêu chuẩn, tất cả giáo viên có
thể được sử dụng cùng lúc để chuyển giao kiến thức (song song), trong khi vẫn tận dụng bộ chính
quy hóa chuyển giao được đề xuất của chúng tôi được mô tả ở trên để đảm bảo chuyển giao tích
cực. Cụ thể, việc lựa chọn dữ liệu tham lam được mở rộng để tạo ra các tập con dữ liệu cho mỗi
mô hình giáo viên tương ứng. Thứ hai, quá trình chuyển giao nhiều mô hình có thể được thực hiện
tuần tự phù hợp với cách xử lý liên tục của quá trình chuyển giao mô hình đơn. Sau mỗi lần
chuyển giao mô hình giáo viên, học sinh được chưng cất được coi như học sinh tiền huấn luyện
(mới) cho bước chuyển giao tiếp theo. Cuối cùng, phần thí nghiệm của chúng tôi cũng điều tra
việc sử dụng Model Soups (Wortsman et al., 2022a) cho vấn đề chuyển giao kiến thức độc lập
kiến trúc. Ở đây, mô hình học sinh được chưng cất độc lập từ mỗi mô hình giáo viên, tạo ra một
tập các biến thể mô hình được chưng cất {fi s}i∈1,...,Kt với Kt mô hình giáo viên. Sau chuyển
giao, nội suy trọng số đơn giản giữa tất cả các biến thể được thực hiện (súp).

5 NGHIÊN CỨU THỰC NGHIỆM VỀ CHUYỂN GIAO KIẾN THỨC HIỆU QUẢ

Trước tiên chúng tôi tiến hành một nghiên cứu quy mô lớn về các phương pháp chuyển giao kiến
thức (tham khảo §4.1-4.3) trong §5.1, nêu bật ưu điểm của phương pháp học liên tục, và đặc biệt
sự vượt trội của phương pháp phân vùng dữ liệu của chúng tôi. Để khám phá, chúng tôi sử dụng
một biến thể có giám sát (Phương trình 4) nhưng cho thấy trong Bảng 3 rằng biến thể không giám
sát khớp với hiệu suất. Cuối cùng, chúng tôi điều tra mối quan hệ của các thuộc tính mô hình và
thành công chuyển giao tổng quát (cũng §5.1), và nghiên cứu chuyển giao từ nhiều mô hình trong
§5.2.

Chi tiết thí nghiệm. Chúng tôi sử dụng cụm tính toán NVIDIA 2080Ti với PyTorch 1.13.1
(Paszke et al., 2017) và ffcv 0.0.3 (Leclerc et al., 2022) để tải dữ liệu nhanh. Mặc dù hiệu suất
có thể bị ảnh hưởng nhẹ, các thay đổi tương đối được duy trì (Leclerc et al., 2022), cho phép thí
nghiệm quy mô lớn trên tính toán hợp lý. Đối với các đánh giá quy mô lớn của chúng tôi về các
phương pháp chuyển giao, chúng tôi sử dụng tập con ImageNet 10% được phân tầng (lấy mẫu phụ
trên lớp), sau đó xác thực các tuyên bố chính của chúng tôi trên tập dữ liệu ImageNet đầy đủ.
Chúng tôi thực hiện chuyển giao trong ngân sách hạn chế 20 epoch để nghiên cứu các phương
pháp cho chuyển giao kiến thức đa mục đích với yêu cầu thực tế. Các tham số tối ưu hóa khác
được xác định thông qua tìm kiếm lưới (xem Phụ lục). Mã: github.com/ExplainableML/General-Knowledge-Transfer.

5.1 ĐÁNH GIÁ CÁC PHƯƠNG PHÁP KHÁC NHAU CHO CHUYỂN GIAO KIẾN THỨC

Hiệu quả của chưng cất kiến thức tiêu chuẩn cho chuyển giao kiến thức. Để nghiên cứu tính phù
hợp của chưng cất KL tiêu chuẩn cho chuyển giao kiến thức đa mục đích, chúng tôi chọn 400 cặp
giáo viên-học sinh (Bảng 5 trong Phụ lục để biết chi tiết), tất cả đều thể hiện tỷ lệ phần trăm đáng
kể của kiến thức bổ sung (tham khảo §3). Qua những cặp này cho mỗi mô hình học sinh, chúng
tôi đo tỷ lệ phần trăm giáo viên mà delta chuyển giao tích cực ∆transf. được thu được. Kết quả
được hình ảnh hóa trong Hình 4a, và tiết lộ rằng đối với phần lớn học sinh có ít hơn 40% giáo viên
với sự gia tăng hiệu suất. Một

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Chuyển Giao
KL-Dist.Chuyển Giao
XE-KL-Dist.Chuyển Giao XE-KL-Dist +
MCL Chuyển Giao
KL-Dist + DP020406080100Tỷ Lệ Thành Công Chuyển Giao Kiến Thức [%]     
(a) Tỷ lệ phần trăm chuyển giao thành công.

−10 −5 0 5 10
Sự Khác Biệt Hiệu Suất của Giáo Viên và Học Sinh−20246Delta Chuyển Giao Kiến Thức
Chuyển Giao KL-Dist.
Chuyển Giao XE-KL-Dist. + MCL
Chuyển Giao KL-Dist + DP (b) Tương quan thuộc tính giáo viên & thành công chuyển giao.

Hình 4: (a) Tỷ lệ giáo viên dẫn đến chuyển giao kiến thức tích cực (tỷ lệ thành công) cho các biến
thể chưng cất kiến thức (xanh dương) và các mở rộng học liên tục (cam). Mỗi hộp đại diện cho
400 thí nghiệm chuyển giao, với sự gia tăng rõ ràng cho các thiết lập học liên tục. (b) Delta
chuyển giao theo sự khác biệt hiệu suất giáo viên-học sinh được phân nhóm. Để báo cáo mạnh
mẽ hơn, chúng tôi hiển thị delta chuyển giao trung bình của top 25% cho mỗi nhóm/phương pháp
với cùng 400 cặp giáo viên-học sinh. Kết quả cho thấy Chuyển Giao KL-Dist+DP cho phép lợi
ích nhất quán từ các giáo viên yếu hơn và mạnh hơn.

Bảng 1: Kết quả chuyển giao kiến thức trên ImageNet đầy đủ, từ bốn giáo viên đến tám mô hình
học sinh được chọn (xem Phụ lục Bảng 6). Kết quả cung cấp bằng chứng thêm rằng phương pháp
học liên tục cho vấn đề chuyển giao cho phép chuyển giao kiến thức nhất quán hơn qua các cặp
mô hình tùy ý.

[THIS IS TABLE: Contains student models with their types, accuracy, parameters, and transfer deltas for KL-Dist and KL-Dist+DP methods]

mất mát phân loại bổ sung (XE-KL, §4.1) có thể nâng tỷ lệ thành công trung vị lên hơi trên 40%.
Tuy nhiên, trong cả hai trường hợp, việc ghi đè kiến thức có sẵn trước thường hơn không thì
làm lu mờ lợi ích thu được từ chuyển giao kiến thức thực tế, đặc biệt khi chuyển giao từ một mô
hình giáo viên yếu hơn như được hiển thị trong Hình 4b (Chuyển Giao KL-Dist.), nơi các thay
đổi hiệu suất tuyệt đối sau chuyển giao được hình ảnh hóa so với sự khác biệt hiệu suất giáo viên-
học sinh ban đầu (như được đo trên dữ liệu xác thực riêng biệt). Ngoài ra, chúng tôi thấy rằng
những giới hạn này cũng tồn tại khi triển khai các bộ chính quy hóa bổ sung đơn giản như làm
mịn nhãn (54%, Yuan et al. (2020b)), với chuyển giao âm nhất quán cho các giáo viên yếu hơn,
và hiệu quả giảm cho những giáo viên mạnh hơn.

Tận dụng học liên tục. Coi chuyển giao kiến thức tổng quát như một nhiệm vụ học liên tục thông
qua chính quy hóa trọng số (XE-KL+MCL) nâng tỷ lệ thành công trung vị đáng kể (80%, Hình
4a). Tuy nhiên, chúng tôi thấy thiếu hiệu quả khi kiến thức được chuyên biệt hóa cho các lĩnh
vực chuyên môn, và khi giáo viên mạnh hơn, mà chúng tôi giải quyết với chính quy hóa cấp độ
dữ liệu (KL+DP), nâng tỷ lệ thành công lên 92,5%. Như được hiển thị trong Hình 4b, những lợi
ích này có thể được quy cho các delta chuyển giao tích cực ngay cả đối với các giáo viên yếu hơn
nhiều (xem sự khác biệt hiệu suất thấp hơn nhiều so với không), và, không giống như chính quy
hóa cấp độ trọng số nghiêm ngặt trong ví dụ MCL, hầu như không giới hạn lợi ích từ các giáo
viên mạnh hơn nhiều. Thật vậy, chúng tôi thấy rằng đối với một số giáo viên mạnh hơn, đặc biệt
là nơi sự khác biệt hiệu suất không nổi bật, chính quy hóa cấp độ dữ liệu thậm chí có thể cung
cấp một lợi thế so với chưng cất bình thường.

Thí nghiệm ImageNet đầy đủ. Chúng tôi mở rộng thí nghiệm từ trên đến ImageNet đầy đủ trong
Bảng 1, và xác minh rằng những hiểu biết chuyển giao, với KL+DP hoạt động đáng tin cậy hơn
nhiều so với chưng cất bình thường. Chúng tôi cũng cung cấp tổng quan chi tiết hơn trong Bảng
2, nêu bật chuyển giao thành công từ cả giáo viên mạnh hơn, nhưng đặc biệt là yếu hơn - ngay
cả khi các mô hình học sinh đã mạnh (≈+0,3% cho PiT-B (Heo et al., 2021), 82,44% ImageNet
top-1 và ConvNeXt (Liu et al., 2022), 84,57%, với chênh lệch hiệu suất gần 5%). Để biết kết
quả bổ sung trên ImageNet và thông tin mô hình, chúng tôi tham khảo bảng 6 và 7 trong phụ lục.
Ngoài ra, phụ lục cũng tiết lộ thành công chuyển giao tương tự sử dụng KL+DP cho các tập dữ
liệu khác như CUB200 (Wah et al.,

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Top-2% Top-5% Top-20% Top-50% Top-100%
Các Lớp với Top-X% Kiến Thức Bổ Sung20406080100Tỷ Lệ Chuyển Giao [%]
(a) Tỷ lệ chuyển giao cho các lớp top-X% lật dương.

10 15 20 40 60 80 120 200
Số Tham Số Học Sinh [M]20406080Tỷ Lệ Chuyển Giao [%]
Trafo
CNN
MLP (b) Tỷ lệ chuyển giao theo kích thước học sinh và họ mô hình.

Hình 5: (a) Tỷ lệ chuyển giao cho các tập lớp chứa top 2% - 100% kiến thức bổ sung trên họ mô
hình. (b) Tỷ lệ chuyển giao so với kích thước học sinh được phân chia theo họ mô hình.

2011), StanfordCars (Krause et al., 2013), và Caltech256 (Griffin et al., 2007). Hơn nữa, chúng
tôi tận dụng các lần chạy ImageNet đầy đủ trong Bảng 3 để chứng minh rằng biến thể không
giám sát của chúng tôi về chuyển giao KL+DP (§4.2) hoạt động tương đương và trong một số
trường hợp thậm chí tốt hơn so với đối tác có giám sát của nó (ví dụ ∆transf.= 0,55 so với
∆transf.= 0,95 cho học sinh PiT-B). Điều này cho thấy rằng chuyển giao kiến thức tổng quát có
thể được thực hiện ngay cả mà không có giám sát bằng cách tận dụng chính quy hóa học liên tục
cấp độ dữ liệu.

Bảng 2: Ví dụ cho các delta chuyển giao của hai mô hình học sinh mỗi cái được chưng cất với một
mô hình giáo viên mạnh hơn và yếu hơn trên ImageNet đầy đủ sử dụng phương pháp chuyển giao
KL-Dist+DP của chúng tôi.

[THIS IS TABLE: Shows transfer deltas for teacher-student pairs]

Bảng 3: So sánh giữa chuyển giao KL-Dist+DP có giám sát và không giám sát trên ImageNet cho
tám học sinh được chọn và bốn giáo viên, tương ứng. Kết quả cho thấy rằng chuyển giao kiến
thức hoàn toàn không giám sát giữa các chuyên gia không chỉ có thể mà thậm chí có thể vượt trội
hơn chuyển giao có giám sát.

[THIS IS TABLE: Compares supervised vs unsupervised transfer performance]

Chúng tôi thật vậy thấy tăng phương sai do việc lựa chọn dữ liệu không giám sát. Để hiểu rõ hơn
về nó, chúng tôi nghiên cứu số lượng các mẫu lật tích cực (giáo viên đúng, học sinh sai) và lật
âm (ngược lại) được gán cho giáo viên. Lý tưởng, điều này chỉ bao gồm các mẫu lật tích cực. Đối
với các cặp mô hình được trình bày trong Bảng 6 sử dụng chuyển giao KL-Dist. + DP, chúng tôi
thấy rằng 72% lật tích cực và 9% các mẫu lật âm được gán cho giáo viên. Điều này có nghĩa là
mặc dù phân vùng dựa trên độ tin cậy đơn giản không hoàn hảo gán các mẫu, nó vẫn căn chỉnh
mạnh mẽ với các lĩnh vực chuyên môn tương ứng.

Nhìn chung, chúng tôi thấy chuyển giao rất hứa hẹn qua các cặp giáo viên-học sinh - ngay cả
mà không có giám sát. Mặc dù lợi ích không đạt đến tổng số kiến thức bổ sung §3 - có thể quy
cho sự đánh đổi giữa duy trì và chuyển giao - chúng tôi tin rằng kết quả của chúng tôi cung cấp
một bằng chứng khái niệm mạnh mẽ cho nghiên cứu tương lai và tiềm năng của chuyển giao kiến
thức thực sự tổng quát.

Kiến thức bổ sung thúc đẩy lợi ích chuyển giao. Thí nghiệm của chúng tôi ở trên cho thấy lợi
ích rõ ràng khi tiến hành chuyển giao KL+DP qua tất cả các loại cặp mô hình. Trong phần này,
chúng tôi cho thấy rằng lợi ích thật sự bắt nguồn từ việc chuyển giao kiến thức bổ sung giữa
những cặp này (tham khảo §3). Cho điều đó, chúng tôi phân tích cải thiện của học sinh trên lớp
liên quan đến kiến thức bổ sung có sẵn trên lớp, được ký hiệu là tỷ lệ chuyển giao, cho tất cả 400
lần chạy chuyển giao được nghiên cứu trong Hình 4a. Kết quả trong Hình 5a vẽ tỷ lệ chuyển
giao so với các lớp chứa top-X% kiến thức bổ sung. Sau khi sắp xếp các lớp theo tỷ lệ kiến thức
bổ sung (tham khảo trong Hình 2a), chúng tôi xem xét cách X% đầu tiên của kiến thức bổ sung
được chuyển giao). Theo ký hiệu của chúng tôi từ §5.1, tỷ lệ phần trăm nhỏ hơn chủ yếu chứa
các lật từ lĩnh vực chuyên môn tương đối của giáo viên, được làm mềm cho các giá trị lớn hơn.
Thật vậy, khi sử dụng chuyển giao KL+DP được đề xuất của chúng tôi, chúng tôi thấy một dấu
hiệu rõ ràng nơi kiến thức bổ sung liên kết với các lĩnh vực chuyên môn mạnh hơn trong mô hình
giáo viên có cơ hội gần như được đảm bảo để được chuyển giao cho mô hình học sinh. Điều này
giảm khi di chuyển về phía các lật liên kết với các lớp ít được đại diện. Lưu ý rằng mà không có
sở thích hướng tới các lĩnh vực chuyên môn tương đối, người ta sẽ mong đợi một đường ngang ở
tỷ lệ chuyển giao trung bình. Điều này cho thấy rằng cách xử lý dựa trên CL của chúng tôi về
chuyển giao kiến thức cho phép bất kỳ mô hình giáo viên nào truyền đạt kiến thức cụ thể và
người ta có thể hướng dẫn rõ ràng ngữ cảnh được học bởi một mô hình học sinh bằng cách chọn
một giáo viên với lĩnh vực chuyên môn phù hợp.

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 4: Chuyển giao từ nhiều giáo viên theo cách tuần tự, song song và dựa trên súp (tham khảo
§4.3). Chúng tôi thấy chuyển giao tuần tự hoạt động tốt.

[THIS IS TABLE: Shows transfer results from multiple teachers with columns for Students, Type, Acc., # Param., and different transfer methods (Single Teacher, Multiple Teachers)]

Thuộc tính của một mô hình học sinh tốt. Chúng tôi kết thúc các nghiên cứu chuyển giao mô hình
đơn của chúng tôi bằng cách điều tra xem liệu và cách các thuộc tính mô hình học sinh cụ thể có
thể tạo điều kiện cho việc tiếp nhận kiến thức bổ sung. Qua các thí nghiệm của chúng tôi được
thực hiện trong Phần 5.1, chúng tôi thấy trong Hình 5b rằng tỷ lệ chuyển giao kiến thức bổ sung
có mối quan hệ đáng kể với khả năng mô hình (số lượng tham số). Chúng tôi quan sát xu hướng
tổng quát này qua các họ kiến trúc bất kể các thiên hướng quy nạp trực quan được mã hóa. Tuy
nhiên, chúng tôi nêu bật rằng mặc dù các mô hình lớn hơn nói chung tiếp nhận tốt hơn đối với
ngữ cảnh bổ sung, kiến trúc kiểu CNN, hoặc nói chung các mô hình với thiên hướng quy nạp trực
quan mạnh hơn, dễ bị ghi đè kiến thức hiện có của chúng, dẫn đến delta chưng cất thấp hơn. Xem
Phụ lục để biết chi tiết và thí nghiệm bổ sung.

5.2 CHUYỂN GIAO KIẾN THỨC TỪ NHIỀU MÔ HÌNH TIỀN HUẤN LUYỆN

Cuối cùng, chúng tôi nghiên cứu chuyển giao từ nhiều giáo viên trên ImageNet đầy đủ, kiểm tra
chuyển giao KL+DP tuần tự và song song, và một biến thể súp mô hình bằng cách nội suy giữa
các biến thể học sinh được chuyển giao (tham khảo §4.3). Thí nghiệm của chúng tôi bao gồm ba
học sinh và giáo viên (Phụ lục B.6), nơi chúng tôi tập trung vào Transformer, trong các thí nghiệm
của chúng tôi đã cho thấy khả năng cao nhất đối với việc tiếp nhận kiến thức.

Kết quả được hiển thị trong Bảng 4, nơi chúng tôi so sánh với hiệu suất của chuyển giao giáo
viên đơn.

Chúng tôi thấy rằng học sinh có thể liên tục thu được kiến thức từ mỗi giáo viên khi chuyển giao
tuần tự. Tuy nhiên, khi học sinh cải thiện, lợi nhuận giảm dần cho đến khi delta chuyển giao ổn
định, vì việc quên trở nên phổ biến hơn khi chúng tôi di chuyển xa hơn từ tiền huấn luyện học
sinh ban đầu. Tuy nhiên, chuyển giao tuần tự của ba giáo viên đạt được lợi ích trung bình 59%
so với delta chuyển giao giáo viên đơn tốt nhất (ví dụ ∆transf= 0,26→∆transf= 0,65 cho Twins
(Chu et al., 2021) hoặc ∆transf= 0,86→∆transf= 1,04 cho PiT-B (Heo et al., 2021)). Ở đầu
đối diện, chúng tôi thấy chuyển giao KL-Dist. vanilla không thành công trong cài đặt nhiều giáo
viên, nhấn mạnh lợi ích của chuyển giao KL+DP (xem cũng Phụ lục). Hơn nữa, mặc dù chúng
tôi thấy lợi ích kiến thức nhất quán bất kể thứ tự giáo viên, thí nghiệm của chúng tôi chỉ ra rằng
một chuỗi giáo viên giảm dần (tức là mạnh nhất trước) thực sự có thể gây ra việc quên không tỷ
lệ cao hơn, vì mô hình di chuyển ra khỏi kiến thức cơ sở của nó nhanh hơn. Cuối cùng, không
giống như chuyển giao tuần tự, chuyển giao song song của nhiều giáo viên không cải thiện so với
hiệu suất chuyển giao giáo viên đơn tốt nhất. Điều này là do lượng duy trì giảm xảy ra vì chính
quy hóa tập con tương ứng (tham khảo §4.3) không duy trì đủ mẫu cho việc duy trì kiến thức
tích cực. Cuối cùng, chúng tôi thấy trung bình trọng số của các biến thể học sinh được chưng cất
với mỗi giáo viên tương ứng hoạt động tệ nhất (hoạt động kém hơn chuyển giao giáo viên đơn
tốt nhất), mà chúng tôi quy cho thiếu khả năng nội suy và sự giảm tiếp theo trong việc duy trì
kiến thức.

6 KẾT LUẬN

Trong công trình này, chúng tôi cho thấy rằng bất kỳ cặp ghép nào của các mô hình được huấn
luyện trên cùng tập dữ liệu với các giao thức huấn luyện khác nhau (ví dụ thay đổi trong kiến
trúc hoặc tối ưu hóa) thể hiện kiến thức bổ sung đáng kể - ngữ cảnh dữ liệu được mã hóa trong
một mô hình và không có trong mô hình khác. Dựa trên sự hiện diện của kiến thức bổ sung, chúng
tôi cung cấp một khám phá đầu tiên về một cơ chế tổng quát để chuyển giao nó giữa bất kỳ cặp
mô hình nào mà không làm suy giảm hiệu suất và phụ thuộc vào các biện pháp xếp hạng bên
ngoài. Điều này mở khóa bất kỳ kho mô hình nào như một nguồn tài nguyên cho lợi ích mô hình,
và tùy chọn cải thiện các mô hình lớn với ngữ cảnh từ những mô hình yếu hơn, tài nguyên thấp
hơn. Các thí nghiệm quy mô lớn của chúng tôi tiết lộ giới hạn của chưng cất kiến thức đơn giản
như một cơ chế chuyển giao tổng quát, và đề xuất các mở rộng thông qua góc độ học liên tục và
phân vùng dữ liệu dựa trên độ tin cậy. Điều này nâng tỷ lệ thành công chuyển giao từ dưới 40%
lên hơn 92%, với chuyển giao tích cực từ cả giáo viên mạnh hơn và yếu hơn. Chúng tôi cũng cung
cấp hiểu biết về các thuộc tính mô hình tổng quát tạo điều kiện cho chuyển giao, thấy khả năng
mô hình và thiên hướng quy nạp trực quan giảm có lợi. Cuối cùng, chúng tôi trình bày chuyển
giao từ nhiều mô hình với cơ chế chuyển giao của chúng tôi. Nhìn chung, chúng tôi cung cấp động
lực thí nghiệm và bước đầu tiên hướng tới các công cụ chuyển giao kiến thức bổ sung đa mục đích
giữa các kiến trúc mô hình tùy ý.

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

LỜI CẢM ƠN
Karsten Roth cảm ơn chương trình Tiến sĩ Phòng thí nghiệm Châu Âu về Học tập và Hệ thống
Thông minh (ELLIS) và Trường Nghiên cứu Quốc tế Max Planck về Hệ thống Thông minh
(IMPRS-IS) đã hỗ trợ. Công trình này được hỗ trợ bởi dự án DFG số 276693517, bởi BMBF
FKZ: 01IS18039A, bởi ERC (853489 - DEXIM), bởi EXC số 2064/1 – số dự án 390727645.

TÀI LIỆU THAM KHẢO

Rahaf Aljundi, Min Lin, Baptiste Goujaud, và Yoshua Bengio. Gradient based sample selection
for online continual learning. Trong Advances in Neural Information Processing Systems (NeurIPS),
2019.

Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari Morcos, Shashank Shekhar, Tom Goldstein,
Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gor-
don Wilson, Jonas Geiping, Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash,
Yann LeCun, và Micah Goldblum. A cookbook of self-supervised learning. arXiv preprint
arXiv:2304.12210, 2023.

Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, và Alexander Kolesnikov.
Knowledge distillation: A good teacher is patient and consistent. Trong IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2022.

Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin
Szeto, Nazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, Samira
Ebrahimi Kahou, Vincent Michalski, Tal Arbel, Chris Pal, Gael Varoquaux, và Pascal Vincent.
Accounting for variance in machine learning benchmarks. Trong Conference on Machine Learning
and Systems (MLSys), 2021.

Cristian Bucila, Rich Caruana, và Alexandru Niculescu-Mizil. Model compression. Trong International
Conference on Knowledge Discovery and Data Mining (KDD), 2006.

Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner,
Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, và Jeff Wu. Weak-to-
strong generalization: Eliciting strong capabilities with weak supervision, 2023.

Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, và Simone Calderara. Dark
experience for general continual learning: a strong, simple baseline. Trong Advances in Neural
Information Processing Systems (NeurIPS), 2020.

Francisco M. Castro, Manuel J. Marín-Jiménez, Nicolás Guil, Cordelia Schmid, và Karteek Alahari.
End-to-end incremental learning. Trong ECCV, 2018.

Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, và Mohamed Elhoseiny. Efficient
lifelong learning with a-gem. Trong International Conference on Learning Representations (ICLR),
2019.

Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, và
Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. Trong Advances
in Neural Information Processing Systems (NeurIPS), 2021.

Marcus Vinícius de Carvalho, Mahardhika Pratama, Jie Zhang, và Yajuan San. Class-incremental
learning via knowledge amalgamation. Trong European Conference on Machine Learning and
Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD), 2022.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. Trong IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2009.

Thomas G. Dietterich. Ensemble methods in machine learning. Trong International Workshop on
Multiple Classifier Systems (MCS), 2000.

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.
Trong International Conference on Learning Representations (ICLR), 2021.

Alaaeldin El-Nouby, Hugo Touvron, Mathilde Caron, Piotr Bojanowski, Matthijs Douze, Armand
Joulin, Ivan Laptev, Natalia Neverova, Gabriel Synnaeve, Jakob Verbeek, và Hervé Jegou. Xcit:
Cross-covariance image transformers. Trong IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2021.

Raphael Gontijo-Lopes, Yann Dauphin, và Ekin Dogus Cubuk. No one representation to rule
them all: Overlapping features of training methods. Trong International Conference on Learning
Representations (ICLR), 2022.

Gregory Griffin, Alex Holub, và Pietro Perona. Caltech-256 object category dataset. Báo cáo kỹ
thuật, California Institute of Technology, 2007.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image
recognition. Trong IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016a.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Identity mappings in deep residual
networks. Trong European Conference on Computer Vision (ECCV), 2016b.

Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, và Mu Li. Bag of tricks for
image classification with convolutional neural networks. Trong European Conference on Computer
Vision (ECCV), 2018.

Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, và Seong Joon Oh.
Rethinking spatial dimensions of vision transformers. Trong International Conference on Learning
Representations (ICLR), 2021.

Geoffrey E. Hinton, Oriol Vinyals, và Jeffrey Dean. Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531, 2015.

James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, An-
drei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, và Raia Hadsell. Overcoming catastrophic
forgetting in neural networks. Proceedings of the National Academy of Sciences (PNAS), 2016.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis,
Claudia Clopath, Dharshan Kumaran, và Raia Hadsell. Overcoming catastrophic forgetting in
neural networks. Proceedings of the National Academy of Sciences (PNAS), 2017.

Jonathan Krause, Michael Stark, Jia Deng, và Li Fei-Fei. 3d object representations for fine-grained
categorization. Trong IEEE International Conference on Computer Vision (ICCV) Workshops, 2013.

Balaji Lakshminarayanan, Alexander Pritzel, và Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. Trong Advances in Neural Information Processing
Systems (NeurIPS), 2017.

Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, và Aleksander
Madry. FFCV: Accelerating training by removing data bottlenecks. https://github.com/
libffcv/ffcv/, 2022. commit 2544abd.

Yann LeCun và Yoshua Bengio. Convolutional Networks for Images, Speech, and Time-Series, trang
255–258. MIT Press, 1995.

Zhizhong Li và Derek Hoiem. Learning without forgetting. IEEE Transactions on Pattern Analysis
and Machine Intelligence (TPAMI), 2016.

Hanxiao Liu, Zihang Dai, David R. So, và Quoc V. Le. Pay attention to mlps. Trong Advances in Neural
Information Processing Systems (NeurIPS), 2021a.

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Iou-Jen Liu, Jian Peng, và Alexander G. Schwing. Knowledge flow: Improve upon your teachers.
Trong International Conference on Learning Representations (ICLR), 2019.

Yuang Liu, W. Zhang, và Jun Wang. Adaptive multi-teacher multi-level knowledge distillation.
Neurocomputing, 2020.

Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo.
Swin transformer: Hierarchical vision transformer using shifted windows. Trong IEEE International
Conference on Computer Vision (ICCV), 2021b.

Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, và Saining
Xie. A convnet for the 2020s. Trong IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2022.

Raphael Gontijo Lopes, Yann Dauphin, và Ekin Dogus Cubuk. No one representation to rule
them all: Overlapping features of training methods. Trong International Conference on Learning
Representations (ICLR), 2022.

David Lopez-Paz và Marc'Aurelio Ranzato. Gradient episodic memory for continual learning. Trong
Advances in Neural Information Processing Systems (NeurIPS), 2017.

Sihui Luo, Xinchao Wang, Gongfan Fang, Yao Hu, Dapeng Tao, và Mingli Song. Knowledge
amalgamation from heterogeneous networks by common feature learning. Trong International Joint
Conference on Artificial Intelligence (IJCAI), 2019.

Arun Mallya và Svetlana Lazebnik. Packnet: Adding multiple tasks to a single network by iterative
pruning. Trong IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

Arun Mallya, Dillon Davis, và Svetlana Lazebnik. Piggyback: Adapting a single network to multiple
tasks by learning to mask weights. Trong European Conference on Computer Vision (ECCV), 2018.

Behnam Neyshabur, Hanie Sedghi, và Chiyuan Zhang. What is being transferred in transfer learning?
Trong Advances in Neural Information Processing Systems (NeurIPS), 2020.

Jinhyuk Park và Albert No. Prune your model before distill it. Trong European Conference on Computer
Vision (ECCV), 2021.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, và Adam Lerer. Automatic differentiation in
pytorch. Trong Advances in Neural Information Processing Systems (NeurIPS), 2017.

Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, và Bo Wang. Moment matching
for multi-source domain adaptation. Trong Proceedings of the IEEE International Conference on
Computer Vision, 2019.

Ameya Prabhu, Philip H. S. Torr, và Puneet Kumar Dokania. Gdumb: A simple approach that
questions our progress in continual learning. Trong European Conference on Computer Vision (ECCV),
2020.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, và Ilya Sutskever.
Learning transferable visual models from natural language supervision. Trong International Conference
on Machine Learning (ICML), 2021.

Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, và Piotr Dollár. Designing
network design spaces. Trong IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2020.

Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, và Alexey Dosovitskiy. Do
vision transformers see like convolutional neural networks? Trong Advances in Neural Information
Processing Systems (NeurIPS), 2021.

Jathushan Rajasegaran, Salman Hameed Khan, Munawar Hayat, Fahad Shahbaz Khan, và Mubarak
Shah. Self-supervised knowledge distillation for few-shot learning. Trong British Machine Vision
Conference (BMVC), 2020.

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, G. Sperl, và Christoph H. Lampert. icarl: Incre-
mental classifier and representation learning. Trong IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.

Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, và
Yoshua Bengio. Fitnets: Hints for thin deep nets. Trong International Conference on Learning
Representations (ICLR), 2015.

Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjorn Ommer, và Joseph Paul Co-
hen. Revisiting training strategies and generalization performance in deep metric learning. Trong
International Conference on Machine Learning (ICML), 2020.

Karsten Roth, Timo Milbich, Bjorn Ommer, Joseph Paul Cohen, và Marzyeh Ghassemi. Simulta-
neous similarity-based self-distillation for deep metric learning. Trong International Conference on
Machine Learning (ICML), 2021.

Karsten Roth, Oriol Vinyals, và Zeynep Akata. Integrating language guidance into vision-based
deep metric learning. Trong IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2022.

Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, và Diane Bouchacourt. Disentan-
glement of correlated factors via hausdorff factorized support. Trong International Conference on
Learning Representations (ICLR), 2023.

Robin M Schmidt, Frank Schneider, và Philipp Hennig. Descending through a crowded valley
- benchmarking deep learning optimizers. Trong International Conference on Machine Learning
(ICML), 2021.

Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye
Teh, Razvan Pascanu, và Raia Hadsell. Progress and compress: A scalable framework for
continual learning. Trong International Conference on Machine Learning (ICML), 2018.

Chengchao Shen, Xinchao Wang, Jie Song, Li Sun, và Mingli Song. Amalgamating knowledge
towards comprehensive classification. Trong AAAI Conference on Artificial Intelligence (AAAI), 2019a.

Chengchao Shen, Mengqi Xue, Xinchao Wang, Jie Song, Li Sun, và Mingli Song. Customizing
student networks from heterogeneous teachers via adaptive knowledge amalgamation. IEEE
International Conference on Computer Vision (ICCV), 2019b.

Karen Simonyan và Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. Trong International Conference on Learning Representations (ICLR), 2015.

Samarth Sinha, Homanga Bharadhwaj, Anirudh Goyal, Hugo Larochelle, Animesh Garg, và Florian
Shkurti. Dibs: Diversity inducing information bottleneck in model ensembles. Trong AAAI Conference
on Artificial Intelligence (AAAI), 2021.

Zafir Stojanovski, Karsten Roth, và Zeynep Akata. Momentum-based weight interpolation of strong
zero-shot models for continual learning. Trong Workshop on Interpolation Regularizers and Beyond,
held at NeurIPS, 2022.

Mingxing Tan và Quoc Chen. Mixconv: Mixed depthwise convolutional kernels. Trong IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), 2019.

Damien Teney, Ehsan Abbasnejad, và Anton van den Hengel. Unshuffling data for improved
generalizationin visual question answering. Trong IEEE International Conference on Computer Vision
(ICCV), 2020.

Yonglong Tian, Dilip Krishnan, và Phillip Isola. Contrastive representation distillation. Trong Interna-
tional Conference on Learning Representations (ICLR), 2020.

Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-
terthiner, Jessica Yung, Andreas Peter Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, và
Alexey Dosovitskiy. MLP-mixer: An all-MLP architecture for vision. Trong Advances in Neural
Information Processing Systems (NeurIPS), 2021.

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard
Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, và Hervé Jégou.
Resmlp: Feedforward networks for image classification with data-efficient training. Trong IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2021.

Diane Wagner, Fabio Ferreira, Danny Stoll, Robin Tibor Schirrmeister, Samuel Müller, và Frank
Hutter. On the importance of hyperparameters and data augmentation for self-supervised learning.
Trong Workshop of Pre-training: Perspectives, Pitfalls, and Paths Forward, held at ICML, 2022.

C. Wah, S. Branson, P. Welinder, P. Perona, và S. Belongie. Caltech-ucsd birds 200. Báo cáo kỹ
thuật, California Institute of Technology, 2011.

Zeyu Wang, Yutong Bai, Yuyin Zhou, và Cihang Xie. Can cnns be more robust than transformers?
Trong International Conference on Learning Representations (ICLR), 2023.

Ross Wightman. Pytorch image models, 2019.

Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,
Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, và Ludwig
Schmidt. Model soups: Averaging weights of multiple fine-tuned models improves accuracy
without increasing inference time. Trong International Conference on Machine Learning (ICML),
2022a.

Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,
Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, và Ludwig
Schmidt. Robust fine-tuning of zero-shot models. Trong IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2022b.

Chuhan Wu, Fangzhao Wu, và Yongfeng Huang. One teacher is enough? pre-trained language model
distillation from multiple teachers. Trong Findings of the Association for Computational Linguistics
(ACL-IJCNLP), 2021.

Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, và Kaiming He. Aggregated residual
transformations for deep neural networks. Trong IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2017.

ZeLun Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, và Kaiming He. Self-supervised learning
with swin transformers. Trong IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2021.

Weijian Xu, Yifan Xu, Tyler Chang, và Zhuowen Tu. Co-scale conv-attentional image transformers.
Trong IEEE International Conference on Computer Vision (ICCV), 2021.

Fisher Yu, Dequan Wang, Evan Shelhamer, và Trevor Darrell. Deep layer aggregation. Trong IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 2018.

Longhui Yu, Zhenyu Weng, Yuqing Wang, và Yuesheng Zhu. Multi-teacher knowledge distillation
for incremental implicitly-refined classification. Trong Advances in Neural Information Processing
Systems (NeurIPS), 2022.

Fei Yuan, Linjun Shou, Jian Pei, Wutao Lin, Ming Gong, Yan Fu, và Daxin Jiang. Reinforced
multi-teacher selection for knowledge distillation. Trong AAAI Conference on Artificial Intelligence
(AAAI), 2020a.

Li Yuan, Francis E. H. Tay, Guilin Li, Tao Wang, và Jiashi Feng. Revisiting knowledge distillation
via label smoothing regularization. Trong 2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, CVPR, 2020b.

Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, và Shuicheng Yan. Volo: Vision outlooker for visual
recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.

Sergey Zagoruyko và Nikos Komodakis. Paying more attention to attention: Improving the
performance of convolutional neural networks via attention transfer. Trong International Conference
on Learning Representations (ICLR), 2017.

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Friedemann Zenke, Ben Poole, và Surya Ganguli. Continual learning through synaptic intelligence.
Trong International Conference on Machine Learning (ICML), 2017.

Junting Zhang, Jie Zhang, Shalini Ghosh, Dawei Li, Serafettin Tasci, Larry P. Heck, Heming Zhang,
và C.-C. Jay Kuo. Class-incremental learning via deep model consolidation. Trong WACV, 2020.

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

PHỤ LỤC

NHỮNG LỢI ÍCH TUYỆT VỜI VÀ NƠI TÌM RA CHÚNG : VỀ SỰ TỒN TẠI VÀ TRIỂN VỌNG CỦA CHUYỂN GIAO KIẾN THỨC TỔNG QUÁT GIỮA BẤT KỲ MÔ HÌNH TIỀN HUẤN LUYỆN NÀO

A CHI TIẾT THỰC HIỆN VÀ HIỂU BIẾT THỰC NGHIỆM

Trong phần này, chúng tôi mô tả chi tiết thực hiện của các thí nghiệm để đánh giá hiệu quả của các phương pháp và kỹ thuật khác nhau để chuyển giao kiến thức bổ sung giữa các mô hình chuyên gia tiền huấn luyện được huấn luyện trên cùng tập dữ liệu.

Đối với các thí nghiệm ban đầu và khám phá của chúng tôi, chúng tôi sử dụng một tập con 10% được phân tầng của ImageNet (Deng et al., 2009) để giảm thời gian chạy, nhằm tiến hành một loạt thí nghiệm rộng hơn qua một số lượng lớn các cặp mô hình. Chi tiết, chúng tôi rút 130 mẫu trên lớp sử dụng tập xác thực ImageNet tiêu chuẩn để đánh giá. Tất cả thí nghiệm của chúng tôi sử dụng bộ tối ưu SGD với động lượng 0.9 và suy giảm trọng số 1e-3. Các siêu tham số thêm được điều chỉnh riêng cho mỗi phương pháp chuyển giao được điều tra.

A.1 THỰC HIỆN CÁC BIẾN THỂ CHUYỂN GIAO KIẾN THỨC DỰA TRÊN CHƯNG CẤT

Để đặt tốc độ học cho phương pháp chuyển giao dựa trên chưng cất kiến thức mặc định của chúng tôi sử dụng độ phân kỳ KL (như trong Phần 4.1), chúng tôi tiến hành tìm kiếm tham số trên một tập 33 cặp giáo viên-học sinh được chọn ngẫu nhiên từ thư viện timm Wightman (2019), với tốc độ học lr ∈ {1e-2, 1e-3, 1e-4, 1e-5}, mà chúng tôi thấy tốc độ học 1e-4 nói chung hoạt động tốt nhất, mặc dù bất kể giá trị được chọn, delta chuyển giao trung bình vẫn nhất quán âm.

Theo Phần 4.1, chúng tôi cũng mở rộng chưng cất KL với mất mát phân loại cross-entropy. Trong trường hợp này, các siêu tham số được xác định trên một lưới bao gồm tốc độ học lr ∈ {1e-2, 1e-3, 1e-4, 1e-5}, nhiệt độ softmax T ∈ {0.1, 1, 4, 10} và trọng số λ ∈ {0.05, 0.1, 0.25, 0.5}. Một lần nữa, chúng tôi thấy rằng tốc độ học 1e-4 là hiệu quả nhất trung bình, nhưng thấy phương sai cụ thể trong trọng số λ, nơi chúng tôi quan sát rằng giá trị λ lớn hơn - đặt trọng tâm cao hơn vào chưng cất - phù hợp hơn để chuyển giao kiến thức từ giáo viên mạnh hơn đến học sinh yếu hơn, trong khi λ nhỏ hơn dường như thích hợp hơn khi chuyển giao kiến thức từ giáo viên yếu hơn đến học sinh mạnh hơn. Điều này tiếp tục nêu bật sự đánh đổi giữa thu được kiến thức và duy trì, nơi đối với giáo viên yếu hơn, việc duy trì đóng vai trò quan trọng hơn nhiều để đảm bảo hiệu suất tổng thể cao, khi kiến thức học sinh bị ghi đè.

Đối với nhiệt độ softmax, chúng tôi thấy rằng nhiệt độ nhỏ 0.1 giới hạn việc giảm hiệu suất của học sinh khi chuyển giao từ mô hình giáo viên yếu hơn, nhưng cũng giới hạn chuyển giao kiến thức nói chung. Điều này dẫn đến chỉ những tăng nhỏ trong hiệu suất của học sinh ngay cả khi chuyển giao từ mô hình giáo viên mạnh hơn. Hinton et al. (2015) đề xuất sử dụng nhiệt độ lớn hơn 4 để khớp mục tiêu mềm để đại diện tốt hơn xác suất nhỏ hơn trong đầu ra của một mẫu đơn. Tuy nhiên, chúng tôi không thấy nhiệt độ lớn hơn có lợi cho hiệu suất chuyển giao.

Nói chung, chúng tôi thấy rằng đặc biệt tham số nhiệt độ và trọng số hướng dẫn tính tích cực của phương pháp chuyển giao dựa trên chưng cất, phụ thuộc cao vào động lực giáo viên và học sinh được quan sát của cặp mô hình chuyên gia tiền huấn luyện được cung cấp. Phương sai cao qua các cặp mô hình tùy ý như vậy làm cho chưng cất kiến thức bình thường, ngay cả khi ghép cặp với mất mát phân loại bổ sung để ổn định, không phù hợp như một công cụ chuyển giao kiến thức tổng quát.

A.2 THỰC HIỆN CHUYỂN GIAO KIẾN THỨC CHƯNG CẤT TƯƠNG PHẢN

Mặc dù các phương pháp chưng cất kiến thức khớp mục tiêu mềm của mô hình giáo viên và học sinh vẫn phổ biến, nhiều phương pháp gần đây lập luận rằng kiến thức cấu trúc hơn có thể được chuyển giao bằng cách khuyến khích mô hình học sinh cũng khớp biểu diễn trung gian của mô hình giáo viên (Liu et al., 2019; 2020; Wu et al., 2021; Park và No, 2021). Do đó, trong phần này, chúng tôi nêu bật kết quả khám phá của chúng tôi về tính khả thi của việc sử dụng biểu diễn trung gian và mối quan hệ của chúng để chuyển giao kiến thức giữa các chuyên gia tiền huấn luyện.

Chúng tôi đặc biệt tuân theo Tian et al. (2020), người đề xuất mở rộng phương pháp chưng cất kiến thức cơ bản của Hinton et al. (2015) bằng cách căn chỉnh biểu diễn đặc trưng của mô hình giáo viên và học sinh. Ở đây, học sinh được khuyến khích cung cấp biểu diễn đặc trưng gần với những biểu diễn của giáo viên cho hình ảnh tương tự trong khi đẩy lùi biểu diễn đặc trưng của hình ảnh không tương tự. Không giống như các phương pháp chưng cất khác hoạt động trên biểu diễn đặc trưng, phương pháp tương phản như vậy đặt ít hạn chế hơn lên kiến trúc của mô hình giáo viên và học sinh, đặc biệt vì biểu diễn đặc trưng của cả hai mô hình có thể được chiếu rẻ vào không gian đặc trưng chung sử dụng lớp chiếu được học cho cả hai mô hình. Điều này cho phép chưng cất giữa các mô hình có kiến trúc khác nhau, và cho phép chúng tôi khám phá một thay thế cho mục tiêu Chưng Cất KL cơ sở được sử dụng của chúng tôi để chuyển giao kiến thức tổng quát (Phần 4.1).

Để đánh giá tính khả thi của khớp biểu diễn cho chuyển giao kiến thức giữa các mô hình chuyên gia, chúng tôi thực hiện hai phương pháp học tương phản. Đầu tiên, chúng tôi sử dụng một phương pháp đơn giản khuyến khích khoảng cách giữa biểu diễn đặc trưng của một cặp hình ảnh tương tự cho cả mô hình giáo viên và học sinh. Do đó, nếu hai hình ảnh dẫn đến biểu diễn đặc trưng tương tự trong không gian nhúng của giáo viên, học sinh được khuyến khích cũng cung cấp biểu diễn đặc trưng với độ gần gũi trong không gian nhúng tương ứng của chúng. Khớp dựa trên độ tương tự tương đối như vậy đã thấy thành công trong học tương phản có giám sát tiêu chuẩn, như trong (Roth et al., 2021; 2022). Sử dụng t và s để biểu thị giáo viên và học sinh tương ứng, điều này cho
LCD=KL(σ(Ss), σ(St)), (6)
trong đó S là ma trận độ tương tự chứa độ tương tự cosin của biểu diễn đặc trưng được chuẩn hóa của batch hiện tại (Sij= cos sim(norm(si), norm(sj)), ∀i, j ∈ 0, ..., n). Chúng tôi ký hiệu phương pháp này là Chưng Cất CD.

Thứ hai, chúng tôi thực hiện phương pháp chưng cất biểu diễn tương phản (chưng cất CRD) của Tian et al. (2020). Như đã lưu ý, chưng cất CRD căn chỉnh trực tiếp biểu diễn bằng cách khuyến khích học sinh gần với giáo viên cho các cặp tích cực (tăng cường khác nhau của cùng hình ảnh) trong khi đẩy xa biểu diễn đặc trưng của các cặp tiêu cực (hình ảnh của các lớp khác nhau). Mục tiêu tương ứng do đó được đưa ra như:
LCRD= arg max fs max h Eq(t,s|C=1)[log h(t, s)] + k Eq(t,s|C=1)[log(1−h(t, s))], (7)
trong đó chúng tôi sử dụng t, s như viết tắt cho biểu diễn giáo viên và học sinh tương ứng. Ngoài ra, chúng tôi sử dụng h: t, s → [0,1] để đại diện cho một bộ phân biệt ước tính xem biểu diễn đặc trưng t và s có được rút từ cùng phân phối chung hay từ tích cận biên tương ứng. Trong thiết lập này, k biểu thị số lượng cặp tiêu cực được rút từ tích của các cận biên.

Cả hai phương pháp chưng cất tương phản đều tính mất mát chưng cất tổng thể Ldist như một kết hợp có trọng số của mất mát tương phản tương ứng LCD hoặc LCRD và mất mát phân loại cross-entropy LXE như cũng được sử dụng trong các mục tiêu chưng cất Độ Phân Kỳ KL tiêu chuẩn Beyer et al. (2022); Rajasegaran et al. (2020).

Đối với chuyển giao kiến thức dựa trên chưng cất CD, chúng tôi kiểm tra các trọng số khác nhau giữa mất mát tương phản và mất mát phân loại cũng như các tốc độ học khác nhau trên một tập nhỏ các kết hợp giáo viên-học sinh. Trên một lưới siêu tham số tương tự như được ghi chú trong phần trước, chúng tôi thấy trọng số bằng nhau của cả hai mất mát kết hợp với tốc độ học 1e-4 phù hợp nhất trung bình, mặc dù với sự đánh đổi tương tự như được mô tả trong Phần A.1. Đối với chuyển giao chưng cất CRD, chúng tôi thấy các siêu tham số như được cung cấp trong Tian et al. (2020) hoạt động tốt.

A.3 THỰC HIỆN CÁC PHƯƠNG PHÁP CHUYỂN GIAO DỰA TRÊN HỌC LIÊN TỤC

Cuối cùng, chúng tôi mô tả các siêu tham số và các nghiên cứu siêu tham số tương ứng được sử dụng cho phần mở rộng học liên tục của chúng tôi đối với chuyển giao kiến thức dựa trên chưng cất (xem Phần 4.2), cụ thể là thiết lập cho chuyển giao XE-KL-Dist+MCL và chuyển giao KL-Dist+DP.

Đối với chuyển giao XE-KL+MCL, chúng tôi tiến hành tìm kiếm tham số trên lưới tốc độ học với cùng độ phân giải như trước. Tuy nhiên, vì có nhiều tham số khác để xác thực, chúng tôi chỉ kiểm tra lr ∈ {1e-2, 1e-3}. Ngoài ra, chúng tôi tuân theo Stojanovski et al. (2022) và kiểm tra động lượng cho các giá trị trong τ ∈ {0.99, 0.999, 0.9999}) và tần suất nội suy N ∈ {2, 10, 50, 100}). Đối với trọng số so với mục tiêu phân loại, λ, chúng tôi kiểm tra 0.5 và 0.7. Chúng tôi tiến hành

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 5: Lựa chọn các mô hình học sinh và giáo viên được sử dụng cho các thí nghiệm trên tập con
10% ImageNet. Mỗi tập mô hình được chọn để chứa nhiều loại kiến trúc và bao phủ một loạt
kích thước mô hình và mức hiệu suất.

[THIS IS TABLE: Table showing Student Models and Teacher Models with columns for Type, Acc., and # Param. - detailed model specifications with accuracy percentages and parameter counts]

tìm kiếm tham số như một tìm kiếm ngẫu nhiên trên lưới tham số. Cuối cùng, chúng tôi thấy một thiết lập tham số sử dụng động lượng cao 0.9999 kết hợp với tần suất nội suy cao (mỗi lần lặp khác) và tốc độ học 0.01 với điểm trọng số 0.7 hoạt động tốt nhất trung bình. Không giống như chuyển giao dựa trên Chưng Cất KL đơn giản, một kết hợp siêu tham số cố định bây giờ dẫn đến cả delta chuyển giao tích cực trung bình, và số lượng giáo viên mà mỗi học sinh có thể học từ đó tăng lên đáng kể (tham khảo Hình 4a)

Đối với phương pháp chuyển giao KL+DP được đề xuất cuối cùng của chúng tôi, chúng tôi lại tiến hành tìm kiếm tham số tương tự. Tuy nhiên, không giống như chuyển giao XE-KL+MCL, phương pháp KL+DP không đưa ra các siêu tham số bổ sung so với thiết lập chưng cất kiến thức dựa trên KL tiêu chuẩn. Do đó, chúng tôi sử dụng lưới lr ∈ {1e-3, 1e-4}, λ ∈ {0.5, 0.75, 0.9, 1} và T ∈ {0.1, 1, 10}. Lưu ý rằng trong khi chúng tôi đã thử nghiệm việc sử dụng mất mát phân loại cross-entropy bên ngoài, chúng tôi thấy hiệu suất tốt nhất nhất quán đến cho λ= 1- bằng cách tắt mục tiêu phân loại phụ trợ. Điều này cung cấp bằng chứng mạnh mẽ rằng các biện pháp bên ngoài cho sự ổn định huấn luyện không còn cần thiết. Cuối cùng, qua tất cả các thí nghiệm còn lại, chúng tôi sử dụng tốc độ học 1e-4 và nhiệt độ 1. Mặc dù tìm kiếm tham số sâu hơn có thể cung cấp một kết hợp tham số sẽ cải thiện tỷ lệ thành công trung bình, chúng tôi tin rằng kết quả đạt được trong thiết lập hiện tại cung cấp đủ bằng chứng khái niệm.

A.4 DANH SÁCH MÔ HÌNH: CÁC NGHIÊN CỨU QUY MÔ LỚN TRÊN CÁC TẬP CON IMAGE NET ĐƯỢC PHÂN TẦNG

Bảng 5 trình bày tóm tắt toàn diện các mô hình giáo viên và học sinh tiền huấn luyện được sử dụng trong đánh giá của chúng tôi về các kỹ thuật chuyển giao khác nhau trên tập con 10% của tập dữ liệu ImageNet (§5.1). Những mô hình này được chọn cẩn thận để bao gồm các họ kiến trúc đa dạng, thể hiện các mức hiệu suất khác nhau, và có một loạt kích thước mô hình. Việc lựa chọn này cho phép chúng tôi kiểm tra kỹ lưỡng hiệu quả của các phương pháp chuyển giao kiến thức trong các tình huống và thiết lập khác nhau. Lưu ý rằng để khám phá ngữ cảnh bổ sung (§3) chúng tôi đã tận dụng một tập rộng hơn gồm 466 cặp giáo viên-học sinh bao gồm 301 mô hình tiền huấn luyện cá nhân được rút ngẫu nhiên từ thư viện timm Wightman (2019).

A.5 CÁC MÔ HÌNH ĐƯỢC ĐÁNH GIÁ TRÊN IMAGE NET ĐẦY ĐỦ

Bảng 6 trình bày các thông số kỹ thuật chi tiết của các mô hình học sinh và giáo viên được sử dụng trong các thí nghiệm ImageNet quy mô đầy đủ của chúng tôi (tham khảo Phần 5.1). Trong bối cảnh chuyển giao kiến thức từ nhiều mô hình giáo viên (§4.3), chúng tôi đã sử dụng cùng tập mô hình giáo viên kết hợp với một tập con các mô hình học sinh.

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 6: Lựa chọn các mô hình học sinh và giáo viên được sử dụng cho các thí nghiệm trên ImageNet
đầy đủ. Các mô hình học sinh được chọn để chứa nhiều loại kiến trúc và bao phủ một loạt kích
thước mô hình và mức hiệu suất.

[THIS IS TABLE: Two-column table showing Student Models and Teacher Models with their types, accuracy, and parameters]

B KẾT QUẢ THỰC NGHIỆM MỞ RỘNG

Trong phần này, chúng tôi trình bày kết quả thực nghiệm bổ sung của các thí nghiệm được tiến
hành trong Phần 5.

B.1 CÁC THÍ NGHIỆM BỔ SUNG VỀ CÁC BIẾN THỂ CHUYỂN GIAO KIẾN THỨC DỰA TRÊN
CHƯNG CẤT

Trong phần phụ sau, chúng tôi trình bày các thí nghiệm bổ sung được tiến hành để tăng cường
hiệu suất của các biến thể chuyển giao kiến thức cho chuyển giao kiến thức giữa các mô hình tiền
huấn luyện.

Sử dụng mục tiêu chuyển giao cross-entropy cộng chưng cất. Như một thay thế cho độ phân kỳ
KL được sử dụng trong Phương trình (1) chúng tôi bổ sung điều tra tiềm năng của việc sử dụng
mất mát cross-entropy giữa các mục tiêu mềm của mô hình giáo viên và học sinh, tương tự như
Hinton et al. (2015). Tuy nhiên, kết quả của chúng tôi cho thấy không có lợi thế trong việc sử
dụng mất mát cross-entropy so với độ phân kỳ KL. Thực tế, chúng tôi quan sát một delta chuyển
giao trung bình thấp hơn 1,2 điểm phần trăm khi sử dụng mất mát cross-entropy so với độ phân
kỳ KL trên một tập 60 cặp giáo viên-học sinh. Chúng tôi cũng khám phá việc sử dụng epoch khởi
động nơi chỉ các lớp tuyến tính của mô hình học sinh được huấn luyện sử dụng mất mát độ phân
kỳ KL, nhưng không thấy cải thiện trong hiệu suất chuyển giao.

Hạn chế tập các lớp để tính mất mát chuyển giao dựa trên chưng cất. Trong các thí nghiệm bổ
sung của chúng tôi, chúng tôi điều tra tác động của việc giới hạn mất mát chưng cất để chỉ tập
trung vào top-10 hoặc top-100 lớp có xác suất cao nhất. Phương pháp này nhằm giải quyết thách
thức do số lượng lớp lớn trong tập dữ liệu ImageNet, cụ thể là thiên hướng tiềm ẩn đối với việc
khớp đuôi dài của phân phối mục tiêu mềm. Để đánh giá giả thuyết này, chúng tôi so sánh độ
phân kỳ KL giữa mục tiêu mềm đầy đủ và tập con của mục tiêu mềm. Bằng cách chọn top-10 và
top-100 lớp có xác suất cao nhất dựa trên dự đoán của giáo viên, chúng tôi quan sát rằng một số
cặp giáo viên-học sinh thể hiện độ phân kỳ cao hơn trên tất cả các lớp so với các tập con được
chọn. Điều này chỉ ra ảnh hưởng của các lớp với xác suất dự đoán thấp đến độ phân kỳ KL.

Chưng Cất
XE-KL Chưng Cất
CD Chưng Cất
CRD Chưng Cất
KL+DP020406080100Tỷ Lệ Thành Công Chuyển Giao Kiến Thức [%]     

Hình 6: Tỷ lệ giáo viên tăng hiệu suất học sinh (tỷ lệ thành công) cho chưng cất tương phản (xanh
lá) so với chưng cất hướng dẫn phân loại (xanh dương) và KL+DP dựa trên học liên tục (cam).

Được thúc đẩy bởi những phát hiện này, chúng tôi tiếp tục kiểm tra tác động của việc chỉ xem xét
top-10 hoặc top-100 lớp đến hiệu suất chuyển giao. Qua sáu cặp giáo viên-học sinh, việc sử dụng
độ phân kỳ top-10 dẫn đến sự gia tăng trung bình 0,20 điểm phần trăm trong delta chuyển giao.
Hơn nữa, chúng tôi quan sát rằng mức độ cải thiện phù hợp với sự khác biệt giữa độ phân kỳ KL
top-10 và tổng. Những phát hiện của chúng tôi cho thấy rằng việc giới hạn độ phân kỳ đến các lớp
được chọn có thể có lợi khi xử lý số lượng lớp lớn, mặc dù mức độ cải thiện vẫn hạn chế.

Chưng cất tương phản cho chuyển giao kiến thức giữa các mô hình tùy ý Để hiểu cách các kỹ
thuật chưng cất tương phản phù hợp với chuyển giao kiến thức giữa các

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

2 4 6 81.02.03.04.0Lợi Ích Kiến ThứcChuyển Giao KL-Dist.Các Mô Hình Học Sinh Trafo
CNN
MLP
Trafo
2 4 6 8123Các Mô Hình Học Sinh CNN
0 2 4 60246810Các Mô Hình Học Sinh MLP
0.5 1.0 1.5 2.0
Mất Mát Kiến Thức0.51.01.52.02.5Lợi Ích Kiến ThứcChuyển Giao KL-Dist + DP
1 2 3 4
Mất Mát Kiến Thức123
0 1 2 3
Mất Mát Kiến Thức0246
255075100150200Số Tham Số Học Sinh [M]

Hình 7: Tỷ lệ kiến thức được chuyển giao (lợi ích kiến thức) được hình ảnh hóa so với tỷ lệ kiến
thức mất đi cho chưng cất KL vanilla và phương pháp chưng cất KL+DP được đề xuất của chúng
tôi. Các mô hình học sinh được nhóm theo loại kiến trúc tương ứng của chúng. Mỗi marker đại
diện cho một cặp giáo viên-học sinh. Màu của các marker đại diện cho kích thước của học sinh,
trong khi hình dạng marker xác định kiến trúc giáo viên. Kích thước marker hình ảnh hóa hiệu
suất của giáo viên. Kết quả cho thấy lợi ích rõ ràng của KL+DP, di chuyển hầu hết các điểm đến
các khu vực chuyển giao kiến thức tích cực (trên đường chéo đỏ).

mô hình tiền huấn luyện tùy ý, chúng tôi đo tỷ lệ thành công chuyển giao trung bình cho cả
chuyển giao chưng cất CD và CRD (§A.2), với kết quả được hiển thị trong Hình 6. Chúng tôi
tận dụng cùng thiết lập thí nghiệm trên 10% ImageNet như cho các phương pháp chuyển giao
khác (xem §5.1). Kết quả thí nghiệm cho thấy rõ ràng các phương pháp chưng cất tương phản
không thể cải thiện mô hình học sinh cho hầu hết các mô hình giáo viên. Khi kiểm tra kỹ hơn kết
quả, chúng tôi có thể thấy rằng các phương pháp chưng cất tương phản dẫn đến mức độ chuyển
giao kiến thức tương tự từ giáo viên đến học sinh, nhưng dường như cũng gây ra việc ghi đè tổng
thể mạnh hơn nhiều, khiến học sinh mất những phần lớn kiến thức trước đó của mình. Mặc dù
rất phù hợp cho chưng cất đến học sinh chưa được huấn luyện, hành vi này thật không may không
phù hợp với chuyển giao kiến thức giữa các mô hình chuyên gia đã được huấn luyện.

B.2 KẾT QUẢ MỞ RỘNG VỀ CHUYỂN GIAO KIẾN THỨC GIỮA CÁC MÔ HÌNH TIỀN HUẤN
LUYỆN

Đối với các thí nghiệm tỷ lệ thành công chuyển giao kiến thức của chúng tôi được tiến hành trong
Phần 5.1, chúng tôi cung cấp phiên bản mở rộng và chi tiết hơn cho Hình 4a trong Hình 7. Sử
dụng biểu đồ phân tán, chúng tôi liên hệ tỷ lệ kiến thức được chuyển giao cho mô hình học sinh
(lợi ích kiến thức) so với tỷ lệ kiến thức tiền huấn luyện của học sinh bị ghi đè trong quá trình
chuyển giao (mất mát kiến thức). Mỗi mô hình học sinh được ký hiệu bằng lựa chọn màu tương
ứng liên kết với số lượng tham số của nó. Kích thước và màu ký hiệu biểu thị cả họ và hiệu suất
của các mô hình giáo viên tương ứng. Đường màu đỏ biểu thị sự đánh đổi bằng nhau giữa lợi ích
và mất mát kiến thức, với các mục trên đường chéo chỉ ra chuyển giao kiến thức tích cực. So sánh
kết quả của chuyển giao KL-Dist. vanilla và chuyển giao Kl+DP dựa trên học liên tục, chúng ta
thấy rằng phần lớn các điểm được đẩy lên đường chéo, cho phép chuyển giao ngay cả từ các mô
hình yếu hơn (ký hiệu nhỏ, phân tán nhiều về phía khu vực đường chéo dưới trong phương pháp
chưng cất kiến thức bình thường). Hành vi này cũng nêu bật rằng các phương pháp chưng cất
kiến thức bình thường nói chung ghi đè kiến thức thay vì tăng cường, và được phản ánh trong
các nghiên cứu tương quan của chúng tôi trong Hình 4a.

Nhìn chung, những kết quả này chỉ đơn giản mở rộng những hiểu biết được cung cấp trong phần
chính của công trình này từ một quan điểm chi tiết hơn, nêu bật rằng cách xử lý học liên tục của
vấn đề chuyển giao kiến thức có thể tăng đáng kể tỷ lệ thành công chuyển giao. Tuy nhiên, chúng
tôi lưu ý rằng góc nhìn chi tiết hơn này cung cấp hỗ trợ tốt hơn về khía cạnh có hại của thiên
hướng quy nạp trực quan mạnh hơn cho chuyển giao kiến thức tổng quát, vì chúng tôi thấy các
học sinh CNN nói chung hoạt động tệ nhất, ngay cả khi tận dụng chuyển giao KL+DP.

--- TRANG 18 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 7: Kết quả Chuyển Giao Kiến Thức trên ImageNet đầy đủ, từ bốn giáo viên đến tám mô hình
học sinh được chọn. Các bảng bao gồm các delta chuyển giao cá nhân của tất cả các cặp giáo viên-
học sinh.

[THIS IS TABLE: Large data table showing transfer deltas for different student-teacher pairs across three conditions: KL-Dist., KL-Dist.+DP, and KL-Dist.+DP (unsup.)]

Bảng 8: Bảng dưới đây hiển thị kết quả chuyển giao kiến thức với phương pháp chuyển giao
KL-Dist. + DP được đề xuất của chúng tôi trên ImageNet đầy đủ. Nó bao gồm hai chỉ số mô tả
những thay đổi trong các lật dự đoán tích cực và tiêu cực, và mở rộng thông tin được cung cấp
trong Bảng 1. Đối với mỗi học sinh, chúng tôi báo cáo trung bình và độ lệch chuẩn trên tất cả
các mô hình giáo viên, có thể được tìm thấy trong Bảng 6.

[THIS IS TABLE: Table showing transfer results with columns for Students, Type, Acc., # Param., and various delta measurements]

Bảng sau hiển thị các delta chuyển giao cá nhân của các cặp giáo viên-học sinh từ Bảng 1 và
Bảng 3.

Để hỗ trợ thêm phân tích của chúng tôi trong Phần 5.1, chúng tôi đã cung cấp kết quả bổ sung
liên quan đến sự thay đổi trong tỷ lệ các lật dự đoán tích cực và tiêu cực trong quá trình chuyển
giao kiến thức. Các lật dự đoán tích cực ρpos đề cập đến trường hợp giáo viên đúng, nhưng học
sinh sai. Ngược lại, các lật dự đoán tiêu cực ρneg đề cập đến trường hợp giáo viên sai, nhưng
học sinh đúng. Để đo sự thay đổi này, chúng tôi đã định nghĩa hai chỉ số mới, delta lật-pos
∆ρpos và delta lật-neg ∆ρneg, tương tự như delta chuyển giao. Chúng tôi trình bày trung bình
và độ lệch chuẩn cho cả hai chỉ số cho tất cả các mô hình học sinh sử dụng phương pháp chuyển
giao KL+DP của chúng tôi trong Bảng 8, mở rộng kết quả từ Bảng 1.

Mục tiêu của chúng tôi với chuyển giao kiến thức là chuyển giao kiến thức bổ sung, tức là các lật
dự đoán tích cực. Điều này có nghĩa là số lượng mẫu mà giáo viên đúng nhưng học sinh sai nên
giảm càng nhiều càng tốt. Tuy nhiên, chúng tôi phải đồng thời bảo tồn kiến thức trước đó của
học sinh. Do đó, số lượng mẫu mà học sinh đúng và giáo viên sai (các lật dự đoán tiêu cực) không
nên giảm.

Kết quả thí nghiệm kết luận chứng minh hiệu quả của phương pháp của chúng tôi trong việc giảm
tỷ lệ các lật dự đoán tích cực cho tất cả các mô hình học sinh. Điều này nhấn mạnh khả năng của
phương pháp của chúng tôi để chuyển giao kiến thức bổ sung giữa các mô hình. Hơn nữa, những
thay đổi nhỏ trong các lật dự đoán tiêu cực cung cấp bằng chứng thuyết phục về khả năng của
phương pháp trong việc bảo tồn kiến thức trước đó của học sinh.

B.3 KẾT QUẢ MỞ RỘNG VỀ TẮC ĐỘNG CỦA CÁC THUỘC TÍNH MÔ HÌNH HỌC SINH KHÁC
NHAU ĐỐI VỚI CHUYỂN GIAO KIẾN THỨC

Trong phần này, chúng tôi cung cấp đánh giá gần hơn về tác động của các thuộc tính mô hình
học sinh đối với hành vi chuyển giao kiến thức, được đo thông qua delta chuyển giao. Cụ thể,
chúng tôi xem xét hiệu suất, kích thước (được đo bằng số lượng tham số) và họ mô hình. Đối với
đánh giá này, chúng tôi đã chọn cho mỗi thuộc tính mô hình các cặp hoặc bộ ba học sinh với
giá trị tương tự cho hai trong ba thuộc tính để cô lập từng biến đơn lẻ càng tốt càng có thể. Mặc
dù can thiệp chính xác không thể được thực hiện bằng cách chỉ tận dụng các mô hình tiền huấn
luyện, thiết lập này cung cấp những hiểu biết được kiểm soát hơn, mà chúng tôi hình ảnh hóa
trong Hình 8 cho các thí nghiệm được tiến hành trên 10% ImageNet sử dụng phương pháp
chuyển giao KL+DP.

--- TRANG 19 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

75.0 77.5 80.0 82.5 85.0
Độ Chính Xác Học Sinh−0.50.00.51.01.5Delta Chuyển Giao Kiến Thức
a) Hiệu Suất Học Sinh
5 10 20 40 80120200
Tham Số Học Sinh−2−101
b) Kích Thước Học Sinh
Nhỏ Trung Bình Lớn XLớn
Kích Thước Mô Hình Học Sinh−0.50.00.5
c) Kiến Trúc Học Sinh
Trafo
CNN
MLP

Hình 8: Đánh giá tác động của các thuộc tính mô hình học sinh a) hiệu suất, b) kích thước (được
đo bằng số lượng tham số) và c) loại kiến trúc đối với delta chuyển giao kiến thức. Mỗi marker
đại diện cho một mô hình học sinh được chọn được chưng cất với 20 mô hình giáo viên khác nhau.
Chúng tôi nhóm học sinh thành các cặp hoặc bộ ba dựa trên các thuộc tính mô hình còn lại bằng
cách kết nối các marker tương ứng.

Lưu ý rằng mỗi marker đại diện cho một mô hình học sinh được đánh giá với tất cả 20 mô hình
giáo viên. Chúng tôi kết nối các cặp hoặc bộ ba học sinh có thể so sánh được, với màu của các
đường và marker đại diện cho họ mô hình của học sinh.

Kết quả của chúng tôi sao chép những hiểu biết được ghi chú trong phần chính của công trình
này, đặc biệt là Hình 5 (phải). Chúng tôi thấy rằng ngay cả khi kiểm soát các yếu tố khác như
độ chính xác ban đầu, khả năng học sinh tổng thể dường như có mối tương quan mạnh với khả
năng tiếp nhận kiến thức mới mà không ghi đè lên kiến thức trước đó. Đây là hành vi đặc biệt
rõ rệt trong các mô hình có thiên hướng quy nạp trực quan mạnh như CNN. Hình phụ bên phải
cho thấy rằng khi nhìn vào hành vi trung bình của một họ mô hình (được chia thành các kích
thước mô hình khác nhau), quy mô có thể cung cấp khả năng chuyển giao nổi lên trong CNN -
mặc dù không có sẵn trước đó - đối với bất kỳ loại kiến trúc cụ thể nào - kích thước tăng có thể
cho phép khả năng chuyển giao được cải thiện đáng kể.

B.4 KẾT QUẢ MỞ RỘNG VỀ CÁC TẬP DỮ LIỆU BỔ SUNG

Để củng cố kết quả của chúng tôi trên ImageNet, chúng tôi bổ sung tiến hành thí nghiệm trên các
tập dữ liệu CUB200 Wah et al. (2011), Caltech256 Griffin et al. (2007), và Stanford-Cars Krause
et al. (2013). Đối với mỗi tập dữ liệu, chúng tôi kết hợp chín mô hình học sinh và bốn mô hình
giáo viên như được hiển thị trong Bảng 6 dẫn đến tổng cộng 36 kết hợp giáo viên-học sinh.
Chúng tôi tinh chỉnh lớp phân loại của các mô hình học sinh và giáo viên sử dụng dữ liệu cụ thể
của tập dữ liệu trước khi bắt đầu chuyển giao kiến thức. Chúng tôi sử dụng dữ liệu huấn luyện
của tập dữ liệu làm tập chuyển giao.

−10 0 10
Sự Khác Biệt Hiệu Suất của Giáo Viên và Học Sinh−7.5−5.0−2.50.02.55.0Delta Chuyển Giao Kiến Thức
Chuyển Giao KL-Dist.
Chuyển Giao XE-KL-Dist. + MCL
Chuyển Giao KL-Dist. + DP
(a) CUB200
−20 −15 −10 −5 0 5
Sự Khác Biệt Hiệu Suất của Giáo Viên và Học Sinh−7.5−5.0−2.50.02.55.0Delta Chuyển Giao Kiến Thức
Chuyển Giao KL-Dist.
Chuyển Giao XE-KL-Dist. + MCL
Chuyển Giao KL-Dist. + DP (b) Caltech256
−20 0 20
Sự Khác Biệt Hiệu Suất của Giáo Viên và Học Sinh−7.5−5.0−2.50.02.55.0Delta Chuyển Giao Kiến Thức
Chuyển Giao KL-Dist.
Chuyển Giao XE-KL-Dist. + MCL
Chuyển Giao KL-Dist. + DP (c) Stanford Cars

Hình 9: Delta chuyển giao kiến thức dựa trên sự khác biệt hiệu suất giáo viên-học sinh cho ba tập
dữ liệu bổ sung: a) CUB200, b) Caltech256, và c) Stanford Cars. Chúng tôi so sánh chuyển giao
KL-Dist. đơn giản với chuyển giao XE-KL-Dist.+MCL và Chuyển Giao KL-Dist.+DP. Các cặp
giáo viên-học sinh được phân loại thành các nhóm được xác định bởi các phân vùng bằng nhau
của sự khác biệt hiệu suất tương ứng của chúng. Để giảm thiểu ảnh hưởng của các ngoại lệ,
chúng tôi báo cáo delta chuyển giao trung bình của top 25% trong mỗi nhóm và phương pháp.

Qua tất cả các tập dữ liệu, chúng tôi nhất quán quan sát phương pháp chuyển giao KL-Dist.+DP
không chỉ cho phép chuyển giao kiến thức từ các giáo viên kém thành thạo hơn mà không làm
tổn hại hiệu suất học sinh mà còn chứng minh khả năng chuyển giao các phần kiến thức đáng kể
trong trường hợp giáo viên vượt trội hơn học sinh đáng kể, phù hợp với hiệu quả của chuyển
giao KL-Dist. đơn giản. Những kết quả này phù hợp với quan sát của chúng tôi trên ImageNet
(tham khảo Hình 4b) và nhấn mạnh điểm mạnh của chuyển giao KL+DP.

--- TRANG 20 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

B.5 KẾT QUẢ MỞ RỘNG VỀ CHUYỂN GIAO KIẾN THỨC DƯỚI CHUYỂN DỊCH MIỀN

Chúng tôi tiếp tục khám phá chuyển giao kiến thức trong thiết lập chuyển dịch miền giữa mô hình
giáo viên và học sinh. Với mục đích này, chúng tôi tinh chỉnh mô hình giáo viên trên tập dữ liệu
domainnet infograph Peng et al. (2019) trước khi tiến hành chuyển giao kiến thức. Quá trình
chuyển giao được thực hiện trên tập con 10% của ImageNet. Đánh giá toàn diện của chúng tôi
bao gồm một nhóm 9 mô hình học sinh riêng biệt và 4 mô hình giáo viên (xem Bảng 6).

−30 −20 −10 0
Sự Khác Biệt Hiệu Suất của Giáo Viên và Học Sinh−10.0−7.5−5.0−2.50.0Delta Chuyển Giao Kiến Thức
Chuyển Giao KL-Dist.
Chuyển Giao XE-KL-Dist. + MCL
Chuyển Giao KL-Dist. + DP

Hình 10: Phân tích delta chuyển giao kiến thức liên miền cho chuyển giao KL-Dist. và KL-Dist.+DP.
Chúng tôi điều tra delta chuyển giao do chuyển giao kiến thức từ một mô hình giáo viên được
huấn luyện trên DomainNet Infograph đến một mô hình học sinh tiền huấn luyện ImageNet.

Đáng chú ý, những phát hiện của chúng tôi nhấn mạnh hiệu quả của phương pháp chuyển giao
KL-Dist.+DP, tạo điều kiện cho việc chuyển giao kiến thức từ giáo viên được huấn luyện
Infograph đến mô hình học sinh trên miền ImageNet, từ đó cải thiện hiệu suất của học sinh. Ngược
lại hoàn toàn, chuyển giao KL-Dist. thông thường thể hiện sự giảm đáng kể trong độ chính xác
của học sinh, đặc biệt khi sử dụng giáo viên kém thành thạo hơn.

B.6 KẾT QUẢ MỞ RỘNG VỀ CHUYỂN GIAO TỪ NHIỀU GIÁO VIÊN

0 10 20 30 40 50 60
Epoch Chuyển Giao0.51.01.52.02.53.03.5Lợi Ích/Mất Mát Kiến Thức [%] ResMLP-36 SWSL-ResNext VOLO-D2Lợi Ích Kiến Thức
Mất Mát Kiến Thức
Delta Chuyển Giao Kiến Thức

Hình 11: Chuyển giao kiến thức (lợi ích kiến thức) và mất mát kiến thức trước đó của học sinh
(mất mát kiến thức) trong quá trình huấn luyện tuần tự của PiT-B Heo et al. (2021) với ba mô
hình giáo viên khác nhau được sắp xếp theo hiệu suất tăng dần.

Cuối cùng, chúng tôi trình bày những hiểu biết bổ sung về chuyển giao kiến thức tuần tự từ nhiều
mô hình giáo viên đến một mô hình học sinh tiền huấn luyện duy nhất. Đối với tất cả các thí
nghiệm chuyển giao kiến thức nhiều giáo viên, chúng tôi chọn ba mô hình học sinh (XCiT-P16,
Twins, PiT-B) và ba mô hình giáo viên (SWSL-ResNext101, VOLO-D2, ResMLP-36) từ Bảng 6.
Phụ lục B.6 hình ảnh hóa chuyển giao kiến thức (lợi ích kiến thức), tỷ lệ kiến thức tiền huấn
luyện của học sinh bị mất (mất mát kiến thức) và delta chuyển giao tổng thể qua các epoch chuyển
giao cho mô hình học sinh PiT-B Heo et al. (2021) được trình bày trong §5.2. Như đã lưu ý ở
đó, chúng tôi chưng cất học sinh với ba mô hình giáo viên khác nhau (xem Bảng 6). Đối với hình
ảnh hóa cụ thể này, chúng tôi sắp xếp giáo viên theo hiệu suất tăng dần, nhưng thấy chuyển giao
liên tục tích cực cũng có thể đạt được từ các chuỗi khác. Đối với mỗi giáo viên, chúng tôi phân
bổ ngân sách chuyển giao cố định 20 epoch. Như đã lưu ý trong Bảng 2, hình vẽ nêu bật trực
quan rằng delta chuyển giao tích cực có thể đạt được khi đi từ giáo viên này đến giáo viên tiếp
theo (delta chuyển giao mạnh hơn so với học sinh đơn mạnh nhất, ∆dist= 1,04), nhưng với lợi
nhuận giảm dần. Chúng tôi có thể quy điều này cho tỷ lệ quên tăng - trong khi lợi ích kiến thức
liên tục tăng, việc di chuyển liên tục học sinh khỏi trọng số tiền huấn luyện ban đầu gây ra mất
mát kiến thức mạnh hơn ngày càng tăng, ngay cả khi tận dụng chuyển giao Kl+DP.

Để có thêm hiểu biết, chúng tôi so sánh kết quả của các thí nghiệm nhiều giáo viên của chúng
tôi sử dụng chuyển giao KL-Dist.+DP với chuyển giao KL-Dist. vanilla (Bảng 9). Kết quả cho
thấy rõ ràng rằng chuyển giao KL-Dist. tuần tự

--- TRANG 21 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 9: Chuyển giao kiến thức từ nhiều giáo viên vào một học sinh tiền huấn luyện sử dụng
chuyển giao KL-Dist. vanilla tuần tự và dựa trên súp (tham khảo §4.3). Chúng tôi so sánh với
các delta chuyển giao thu được từ chuyển giao kiến thức giáo viên đơn.

[THIS IS TABLE:
Table showing transfer results for different student models with columns for Type, Acc., # Param., and transfer deltas for single teacher vs multiple teachers (sequential and soup methods)]

chuyển giao không thể đạt được lợi ích lớn hơn như chỉ giáo viên tốt nhất nhưng dẫn đến lợi ích
hiệu suất trong phạm vi delta chuyển giao trung bình qua ba giáo viên. Điều này một lần nữa cho
thấy rằng thay vì chuyển giao chỉ kiến thức bổ sung, chuyển giao KL-Dist. vanilla ghi đè kiến
thức trước đó của học sinh với kiến thức của mô hình giáo viên. Do đó khi chuyển giao tuần tự
kiến thức từ nhiều giáo viên, những cải thiện từ chuyển giao trước đó bị mất trong quá trình
chuyển giao từ giáo viên tiếp theo. Lưu ý rằng phương pháp chuyển giao KL-Dist. vanilla không
thể được áp dụng trực tiếp để chuyển giao kiến thức từ nhiều mô hình giáo viên song song, do
đó chúng tôi bỏ qua baseline này.
