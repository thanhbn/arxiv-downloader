--- TRANG 13 ---
Tiêu đề bài báo rút gọn 13

Tài liệu tham khảo
1. Binici, K., Pham, N.T., Mitra, T., Leman, K.: Ngăn chặn catastrophic forgetting và distribution mismatch trong knowledge distillation thông qua dữ liệu tổng hợp. Trong: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. pp. 663–671 (2022) 3
2. Chen, P., Liu, S., Zhao, H., Jia, J.: Distilling knowledge qua knowledge review. Trong: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 5008–5017 (2021) 9
3. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Đào tạo mạng nơ-ron sâu với trọng số nhị phân trong quá trình lan truyền. Advances in neural information processing systems 28(2015) 1
4. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: Cơ sở dữ liệu hình ảnh phân cấp quy mô lớn. Trong: 2009 IEEE conference on computer vision and pattern recognition. pp. 248–255. Ieee (2009) 7
5. Denton, E.L., Zaremba, W., Bruna, J., LeCun, Y., Fergus, R.: Khai thác cấu trúc tuyến tính trong mạng tích chập để đánh giá hiệu quả. Advances in neural information processing systems 27(2014) 1
6. Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: Về hiệu chuẩn của mạng nơ-ron hiện đại. Trong: International conference on machine learning. pp. 1321–1330. PMLR (2017) 2, 4, 18
7. Guo, Q., Wang, X., Wu, Y., Yu, Z., Liang, D., Hu, X., Luo, P.: Online knowledge distillation qua collaborative learning. Trong: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11020–11029 (2020) 7, 8
8. Guo, Y., Yao, A., Chen, Y.: Dynamic network surgery cho DNN hiệu quả. Advances in neural information processing systems 29(2016) 1
9. Han, S., Pool, J., Tran, J., Dally, W.: Học cả trọng số và kết nối cho mạng nơ-ron hiệu quả. Advances in neural information processing systems 28 (2015) 1
10. Hinton, G., Vinyals, O., Dean, J., et al.: Distilling the knowledge trong một mạng nơ-ron. arXiv preprint arXiv:1503.02531 2(7) (2015) 4, 7, 8, 9
11. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H., Kalenichenko,D.: Quantization và đào tạo mạng nơ-ron cho suy luận số nguyên hiệu quả chỉ với số học. Trong: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 2704–2713 (2018) 1
12. Kim, J., Park, S., Kwak, N.: Paraphrasing complex network: Nén mạng qua factor transfer. Advances in neural information processing systems 31(2018) 1
13. Krizhevsky, A., Hinton, G., et al.: Học nhiều lớp tính năng từ hình ảnh nhỏ (2009) 7
14. Le, Y., Yang, X.: Tiny imagenet visual recognition challenge. CS 231N 7(7), 3 (2015) 7
15. Lee, H., Park, Y., Seo, H., joo Kang, M.: Self-knowledge distillation qua dropout. ArXivabs/2208.05642 (2022) 3
16. Li,L.,Zhe,J.: Shadow knowledge distillation: Nối cầu chuyển giao tri thức ngoại tuyến và trực tuyến. Trong: Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS) (2022) 2, 3, 7, 8, 9
17. Li, Z., Li, X., Yang, L., Zhao, B., Song, R., Luo, L., Li, J., Yang, J.: Curriculum temperature cho knowledge distillation. Trong: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 37, pp. 1504–1512 (2023) 8

--- TRANG 14 ---
14 Amara et al.
18. Van der Maaten, L., Hinton, G.: Trực quan hóa dữ liệu sử dụng t-sne. Journal of machine learning research 9(11) (2008) 10
19. Minka, T., et al.: Divergence measures và message passing. Tech. rep., Technical report, Microsoft Research (2005) 5
20. Mirzadeh, S.I., Farajtabar, M., Li, A., Levine, N., Matsukawa, A., Ghasemzadeh, H.: Improved knowledge distillation qua teacher assistant. Trong: Proceedings of the AAAI conference on artificial intelligence. vol. 34, pp. 5191–5198 (2020) 2, 9
21. Mukhoti, J., Kulharia, V., Sanyal, A., Golodetz, S., Torr, P., Dokania, P.: Hiệu chuẩn mạng nơ-ron sâu sử dụng focal loss. Advances in Neural Information Processing Systems 33, 15288–15299 (2020) 4
22. Müller,R.,Kornblith,S.,Hinton,G.E.: Khi nào label smoothing giúp ích? Advances in neural information processing systems 32(2019) 4, 22
23. Murphy, K.P.: Machine learning: a probabilistic perspective, chap. Variational Inference. MIT press (2012) 5
24. Naeini, M.P., Cooper, G., Hauskrecht, M.: Đạt được xác suất được hiệu chuẩn tốt sử dụng bayesian binning. Trong: Proceedings of the AAAI conference on artificial intelligence. vol. 29 (2015) 10
25. Park, W., Kim, D., Lu, Y., Cho, M.: Relational knowledge distillation. Trong: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 3967–3976 (2019) 9
26. Qian, B., Wang, Y., Yin, H., Hong, R., Wang, M.: Switchable online knowledge distillation. Trong: European Conference on Computer Vision. pp. 449–466. Springer (2022) 2, 7
27. Qian, B., Wang, Y., Yin, H., Hong, R., Wang, M.: Switchable online knowledge distillation. arXiv preprint arXiv:2209.04996 (2022) 3, 7, 10
28. Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets: Hints cho thin deep nets. arXiv preprint arXiv:1412.6550 (2014) 7, 8
29. Stanton, S., Izmailov, P., Kirichenko, P., Alemi, A.A., Wilson, A.G.: Knowledge distillation có thực sự hoạt động không? Advances in Neural Information Processing Systems 34, 6906–6919 (2021) 4
30. Su, C.P., Tseng, C.H., Lee, S.J.: Knowledge từ phía tối: Entropy-reweighted knowledge distillation cho balanced knowledge transfer. arXiv preprint arXiv:2311.13621 (2023) 4, 8
31. Tang, J., Shivanna, R., Zhao, Z., Lin, D., Singh, A., Chi, E.H., Jain, S.: Hiểu và cải thiện knowledge distillation. arXiv preprint arXiv:2002.03532 (2020) 4
32. Thulasidasan,S.,Chennupati,G.,Bilmes,J.A.,Bhattacharya,T.,Michalak,S.: Về mixup training: Hiệu chuẩn cải thiện và sự không chắc chắn dự đoán cho mạng nơ-ron sâu. Advances in Neural Information Processing Systems 32(2019) 4
33. Tian, Y., Krishnan, D., Isola, P.: Contrastive representation distillation. arXiv preprint arXiv:1910.10699 (2019) 7, 8, 9
34. Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J.: Quantized convolutional neural networks cho thiết bị di động. Trong: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4820–4828 (2016) 1
35. Yin, H., Molchanov, P., Alvarez, J.M., Li, Z., Mallya, A., Hoiem, D., Jha, N.K., Kautz, J.: Dreaming to distill: Data-free knowledge transfer qua deepinversion. Trong: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 8715–8724 (2020) 3
36. Yun, S., Park, J., Lee, K., Shin, J.: Regularizing class-wise predictions qua self-knowledge distillation. Trong: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 13876–13885 (2020) 4

--- TRANG 15 ---
Tiêu đề bài báo rút gọn 15
37. Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Cải thiện hiệu suất của mạng nơ-ron tích chập qua attention transfer. arXiv preprint arXiv:1612.03928 (2016) 9
38. Zhang, Y., Xiang, T., Hospedales, T.M., Lu, H.: Deep mutual learning. Trong: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4320–4328 (2018) 2, 7, 8, 9
39. Zhao, B., Cui, Q., Song, R., Qiu, Y., Liang, J.: Decoupled knowledge distillation. Trong: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11953–11962 (2022) 7, 8, 9
40. Zhong, Z., Cui, J., Liu, S., Jia, J.: Cải thiện hiệu chuẩn cho long-tailed recognition. Trong: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 16489–16498 (2021) 4
41. Zhou, H., Song, L., Chen, J., Zhou, Y., Wang, G., Yuan, J., Zhang, Q.: Rethinking soft labels cho knowledge distillation: Một quan điểm bias-variance tradeoff. arXiv preprint arXiv:2102.00650 (2021) 4

A Thuật toán của Phương pháp Đề xuất
Được minh họa trong Algorithm 1, pseudocode của framework BD-KD của chúng tôi có điểm tương đồng với online knowledge distillation, với sự khác biệt quan trọng là việc truyền vào cơ chế cân bằng qua sự không chắc chắn (entropy). Mã của chúng tôi có thể dễ dàng được tích hợp vào một loạt các framework distillation và trên các nhiệm vụ downstream đa dạng, tăng cường thêm khả năng thích ứng và tiện ích của nó.

B Hiểu Sự Ước tính Quá mức và Thiếu của Sự Không chắc chắn

Hình 6: Hiểu sự ước tính quá mức (trái) và ước tính thiếu (phải) của sự không chắc chắn. Chúng ta nói rằng mạng học sinh nhỏ gọn ước tính thiếu sự không chắc chắn của giáo viên khi entropy dự đoán của nó nhỏ so với sự không chắc chắn của giáo viên. Tương tự, học sinh ước tính quá mức sự không chắc chắn của giáo viên khi entropy dự đoán của nó cao hơn entropy của giáo viên.

Theo Hình 1, Khi chúng ta xem xét phân phối softmax của giáo viên trên các lớp khác nhau pt(màu hồng), chúng ta có thể quan sát hai kết quả khác biệt cho phân phối của học sinh ps. Kết quả phụ thuộc vào lựa chọn giữa việc sử dụng

--- TRANG 16 ---
16 Amara et al.
divergence KL thuận hoặc nghịch. Đáng chú ý, những lựa chọn này dẫn đến các ước tính sự không chắc chắn khác nhau. Khi, sử dụng KL thuận, phân phối của học sinh ps(màu xanh) được thúc đẩy để gán xác suất khác không trên tất cả các lớp. Ngược lại, KL nghịch có thể dẫn đến pschiếm mode chủ yếu trong pt, phân bổ xác suất cao nhất cho một lớp duy nhất trong khi tối thiểu hóa xác suất cho các lớp khác. Quan sát này làm nổi bật rằng việc tối thiểu hóa KL thuận có thể tạo ra các phân phối với sự không chắc chắn lớn hơn so với pt ban đầu, trong khi việc tối thiểu hóa KL nghịch có thể mang lại hiệu ứng ngược lại.

C Thiết lập Thí nghiệm
Trong phần này, chúng tôi bao gồm thêm chi tiết về thiết lập thí nghiệm trong bài báo của chúng tôi. Đối với các thí nghiệm BD-KD, chúng tôi thực hiện tìm kiếm siêu tham số và chúng tôi báo cáo giá trị dẫn đến hiệu suất cao nhất. Trong Bảng 8 dưới đây, chúng tôi cung cấp chi tiết về không gian siêu tham số được xem xét cho mỗi loại bộ dữ liệu: CIFAR-10, CIFAR-100, TinyImageNet, và ImageNet.

D Expected Calibration Error và Reliability Curves
Để đánh giá mức độ miscalibration của mạng học sinh, chúng tôi sử dụng Calibration Curves và Expected Calibration Error measures.

Calibration Curves. Để đánh giá xem một mô hình có được hiệu chuẩn tốt không, chúng ta thường vẽ calibration curve, còn được gọi là reliability diagram [6]. Các biểu đồ là một cách mô tả hiệu chuẩn mô hình trong đó độ chính xác mong đợi của mạng được vẽ như một hàm của độ tin cậy của nó (tức xác suất softmax). Một mô hình được "hiệu chuẩn hoàn hảo" được biểu diễn bởi hàm đồng nhất. Sự lệch phía trên (thiếu tin cậy) hoặc dưới (quá tin cậy) đường chéo là một dấu hiệu của mô hình bị miscalibrated. Để thu được đường cong này, đầu tiên, chúng ta phân chia các ước tính tin cậy thành M= 10 bin khoảng bằng nhau Im. Sau đó, chúng ta tiến hành tính toán độ chính xác của mỗi bin như sau:

acc(Bm) =1
|Bm|X
i∈Bm1(pi=yi)

trong đó ŷiandyiare, tương ứng là nhãn lớp dự đoán và thực của mẫu i, và Bmis tập hợp các chỉ số mẫu có độ tin cậy rơi vào khoảng m. Cuối cùng, chúng ta tính toán độ tin cậy trung bình trong mỗi bin:

Bm:conf (Bm) =1
|Bm|X
i∈Bmˆpi

trong đó ˆpiis độ tin cậy cho mẫu i.

Expected Calibration Error. Ngược lại với các đường cong hiệu chuẩn, vốn là các biểu đồ trực quan nắm bắt mức độ hiệu chuẩn mô hình, Expected Calibration Error (ECE) là một hàm trả về một giá trị vô hướng đo lường

--- TRANG 17 ---
Tiêu đề bài báo rút gọn 17
mức độ miscalibration của mô hình. Tương tự như các đường cong hiệu chuẩn, các giá trị tin cậy được phân chia thành M= 10 bin bằng nhau. ECE được định nghĩa là trung bình có trọng số của sự khác biệt giữa độ chính xác và tin cậy tại mỗi bin:

ECE =MX
m=1|Bm|
n|acc(Bm)−conf (Bm)|

E Các Kết quả Hiệu chuẩn Liên quan Khác

(a) Vanilla-KD
 (b) DML
 (c) SwitOKD
 (d) BD-KD (Ours)

Hình 7: Histogram tin cậy (ResNet 20 học sinh, giáo viên WRN_16_8) trên CIFAR-100.

(a) Vanilla-KD
ECE =0.119
(b) DML
ECE =0.0479
(c) SwitOKD
ECE = 0.0465
(d) BD-KD (Ours)
ECE = 0.0240

Hình 8: Đường cong hiệu chuẩn. (ResNet 20 học sinh, giáo viên WRN_16_6) trên CIFAR-100. Các phương pháp KD trực tuyến cải thiện hiệu chuẩn. Trong số các phương pháp trực tuyến (DML, SwitOKD), BD-KD cải thiện hiệu chuẩn nhiều nhất.

Hình 7 cho thấy hiệu chuẩn tin cậy (tức phân phối của tin cậy dự đoán hoặc xác suất liên kết với nhãn mục tiêu) của mạng học sinh ResNet20 được đào tạo với các kỹ thuật KD khác nhau (vanilla KD ngoại tuyến (a), DML (b), SwitOKD (c), và BD-KD (d)). Với vanilla KD ngoại tuyến, chúng ta quan sát nhiều tập trung trong các bin tin cậy cao nhất (giữa 0.9- 1.0) và có một khoảng cách lớn giữa độ chính xác và tin cậy mong đợi khi so sánh với các kỹ thuật KD trực tuyến. Đối với cả DML và SwitOKD, chúng ta thấy sự giảm nhẹ trong khoảng cách giữa độ chính xác của mô hình học sinh và độ chính xác trung bình. Chúng ta chứng kiến rằng BD-KD (của chúng tôi) dẫn đến sự giảm đáng kể nhất trong khoảng cách. Thực vậy, chúng ta thấy rằng tin cậy trung bình của học sinh ResNet20 khớp rất gần với tin cậy mong đợi. Hình 8, cho thấy các đường cong hiệu chuẩn với học sinh ResNet20 với giáo viên có năng lực nhỏ hơn WRN-16-6 (hơn WRN-16-10). Tương tự như các quan sát trước đây của chúng tôi, BD-KD dẫn đến mô hình được hiệu chuẩn tốt hơn tổng thể (các thanh gần như được căn chỉnh dọc theo đường chéo).

--- TRANG 18 ---
18 Amara et al.

F Mở rộng cho multi-networks (chi tiết)
Chúng tôi trình bày các hàm mục tiêu cho cả giáo viên và học sinh dưới thiết lập nhiều mạng. pt
i,τandps
i,τare là xác suất softmax của giáo viên và học sinh, tương ứng.

F.1 Các hàm loss One Teacher Two Students (1T2S)
Các loss học sinh trong trường hợp này là:

Ls1
KD=αs1X
iLCE(ps1
i, yi)
+τ2βs1X
iδi,fs1,tKL(pt
i,τ||ps1
i,τ)
+τ2βs1X
iδi,rs1,tKL(ps1
i,τ||pt
i,τ)
+τ2βs1X
iδi,fs1,s2KL(ps2
i,τ||ps1
i,τ)
+τ2βs1X
iδi,rs1,s2KL(ps1
i,τ||ps2
i,τ)

Ls2
KD=αs2X
iLCE(ps2
i, yi)
+τ2βs2X
iδi,fs2,tKL(pt
i,τ||ps2
i,τ)
+τ2βs2X
iδi,rs2,tKL(ps2
i,τ||pt
i,τ)
+τ2βs2X
iδi,fs2,s2KL(ps1
i,τ||ps2
i,τ)
+τ2βs2X
iδi,rs2,s1KL(ps2
i,τ||ps1
i,τ)

và loss giáo viên là:

Lt
KD=αtX
iLCE(pt
i, yi)
+τ2βtX
i(KL(pt
i,τ||ps1
i,τ) +KL(pt
i,τ||ps2
i,τ))

--- TRANG 19 ---
Tiêu đề bài báo rút gọn 19

F.2 Các hàm loss Two Teachers One student (2T1S)
Các loss giáo viên, trong trường hợp này, được định nghĩa như sau:

Lt1
KD = αt1X
iLCE(pt1
i, yi) + τ2βt1X
iKL(pt1
i,τ||ps
i,τ)

Lt2
KD = αt2X
iLCE(pt2
i, yi) + τ2βt2X
iKL(pt2
i,τ||ps
i,τ)

và loss học sinh là:

Ls
KD=αsX
iLCE(ps
i, yi)
+τ2βsX
iδi,fs,t1KL(pt1
i,τ||ps
i,τ)
+τ2βsX
iδi,rs,t1KL(ps
i,τ||pt1
i,τ)
+τ2βsX
iδi,ft2,sKL(ps
i,τ||pt2
i,τ)
+τ2βsX
iδi,rs,t2KL(ps
i,τ||pt2
i,τ)

G Trực quan hóa T-SNE

(a) ResNet32x4
Teacher với
BD-KD
(b) ResNet8x4
Student với
BD-KD
(c) ResNet32x4
Teacher với
SwitOKD
(d) ResNet8x4
Student với
SwitOKD

Hình 9: Trực quan hóa TSNE của lớp đặc trưng penultimate của cả mạng giáo viên (ResNet32x4) và học sinh (ResNet8x4) trên CIFAR-100. Chúng tôi chọn ngẫu nhiên và trực quan hóa 10 lớp.

Chúng tôi cung cấp trong Hình 9 trực quan hóa TSNE của lớp cuối cùng (trước classifier) cho mạng giáo viên ResNet32x4 và mạng học sinh ResNet8x4 được đào tạo trên CIFAR-100. Chúng tôi cũng minh họa trong Hình 10 trực quan hóa TSNE trên CIFAR-100 với mạng giáo viên VGG13 và mạng học sinh VGG8. Chúng tôi chọn ngẫu nhiên 10 lớp và trực quan hóa chúng. Chúng tôi quan sát rằng với BD-KD các lớp có thể phân tách tốt hơn khi so sánh với cùng các giáo viên được đào tạo với SwitOKD. Tương tự, đối với mạng học sinh, chúng tôi có thể thấy sự phân tách lớp tốt hơn sử dụng BD-KD (Hình 10) khi so sánh với cùng kiến trúc học sinh.

--- TRANG 20 ---
20 Amara et al.

(a) VGG13 Teacher
với BD-KD
(b) VGG8 Student
với BD-KD
(c) VGG13 Teacher
với SwitOKD
(d) VGG8 Student
với SwitOKD

Hình 10: Trực quan hóa TSNE của lớp đặc trưng penultimate của cả mạng giáo viên (VGG13) và học sinh (VGG8) trên CIFAR-100. Chúng tôi chọn ngẫu nhiên và trực quan hóa 10 lớp.

H Các Câu hỏi Thường gặp

H.1 Tại sao sử dụng distillation trực tuyến để cải thiện hiệu chuẩn?
Distillation trực tuyến, trong đó cả mạng giáo viên và học sinh tham gia vào đào tạo tương hỗ, mang lại lợi thế đáng kể so với các phương pháp ngoại tuyến, như đã được chứng minh bởi bài báo SwitOKD. Trong distillation trực tuyến, học sinh tích cực học từ giáo viên và có thể liên tục cung cấp tín hiệu phản hồi cho mô hình giáo viên. Tương tác này cho phép giáo viên thích ứng và cập nhật hướng dẫn của mình cho học sinh dựa trên thông tin gần đây. Ngược lại, distillation ngoại tuyến thường bao gồm chuyển giao một chiều. Hơn nữa, việc trao đổi phản hồi tương hỗ thời gian thực này giữa cả hai mô hình trong KD là công cụ cho hiệu chuẩn như được hiển thị trong các thí nghiệm và ablation của chúng tôi trong bản thảo chính. Các phát hiện của chúng tôi cho thấy rằng các kỹ thuật KD trực tuyến được hiệu chuẩn tốt hơn so với KD ngoại tuyến. Chúng tôi cũng đã chỉ ra rằng BD-KD được hiệu chuẩn tốt nhất trong số các kỹ thuật trực tuyến, và đồng thời cung cấp độ chính xác hiệu suất tốt hơn.

H.2 Phân phối dự đoán của S khớp với T trong quá trình đào tạo?
Mục tiêu chính của việc cân bằng các divergence không phải là đạt được sự khớp hoàn hảo giữa phân phối dự đoán của mạng học sinh và của mạng giáo viên. Thay vào đó, nó phục vụ hai mục tiêu chính: Thứ nhất, nó nhằm giảm thiểu khoảng cách độ chính xác trong quá trình đào tạo, điều này quan trọng trong việc ngăn ngừa sự khác biệt giữa các mức độ tri thức và tin cậy của các mô hình giáo viên và học sinh. Để minh họa điều này, chúng tôi đã bao gồm trong bản nộp chính của chúng tôi các trực quan hóa về hành vi của khoảng cách độ chính xác giữa các mô hình giáo viên và học sinh. Ngoài ra, việc cân bằng các divergence giúp điều chỉnh các mức độ không chắc chắn giữa các mạng học sinh và giáo viên. Sự điều chỉnh này đặc biệt quan trọng để duy trì việc chuyển giao tri thức ổn định và hiệu quả, ngay cả khi tồn tại khoảng cách năng lực vốn có giữa hai mạng. Những mục tiêu kết hợp này cho phép học sinh tránh việc bắt chước hung hăng giáo viên, được thúc đẩy bởi những khác biệt đáng kể trong các mức độ tin cậy. Một nghiên cứu [22] đã chỉ ra một khoảng cách đáng chú ý giữa phân phối dự đoán của học sinh và giáo viên trong hầu hết các kỹ thuật distillation. Trước thách thức này, hàm loss được đề xuất của chúng tôi được thiết kế đặc biệt để giải quyết vấn đề này. Hàm loss của chúng tôi cố gắng thu hẹp khoảng cách làm cho distillation đáng tin cậy hơn.

--- TRANG 21 ---
Tiêu đề bài báo rút gọn 21

Algorithm 1: Pseudocode kiểu PyTorch cho BD-KD
# x:hình ảnh đầu vào
# y:ground truth
# model_s: mô hình học sinh
# model_t: mô hình giáo viên
# H_s, H_t: entropy học sinh và giáo viên
#v: siêu tham số liên quan đến BD-KD
# alpha_s,alpha_t,beta_s,beta_t: siêu tham số liên quan đến KD
for (x,y) in Batches:
# giá trị của logits
output_t = model_t(x)
output_s = model_s(x)
# tính toán ce losses
loss_ce_t = beta_t * ce_loss(output_t, y)
loss_ce_s = beta_s * ce_loss(output_s, y)
# tính toán kl loss giáo viên
loss_kl_t = T*T*alpha_t * kl_div_loss(output_t,
output_s.detach())
# tổng loss giáo viên
total_loss_t= loss_ce_t + loss_kl_t
# tính toán entropy gap
H_gap = H_s - H_t
# khởi tạo trọng số mẫu
delta_reverse = 1
delta_forward = 1
if H_gap ≥0:
delta_reverse = v
else:
delta_forward = v
# tính toán kl loss học sinh
loss_kl_s = T*T*alpha_s *
weighted_kl_div_loss(output_s,output_t.detach(), delta_reverse)
+
weighted_kl_div_loss(output_t.detach(),output_s, delta_forward)
# tổng loss học sinh
total_loss_s = loss_ce_t + loss_kl_s
# cập nhật
total_loss_t.backward()
total_loss_s.backward()

--- TRANG 22 ---
Tiêu đề bài báo rút gọn 22

Bảng 8: Không gian tìm kiếm siêu tham số trên CIFAR-10, CIFAR-100, TinyImageNet, và ImageNet. BS: batch size, LR: learning rate, WD: weight decay

Dataset Không gian tìm kiếm siêu tham số
CIFAR-10 và CIFAR-100 BS={64,128}
LR={0.1,0.01, 0.02, 0.05}
τ={1,2,3,4}
v= {2,3,4}
WD = {1e-4, 5e-4}
αS,αT,βS,βT= {1}
TinyImageNet BS={64,128,256}
LR={0.01, 0.02, 0.05}
τ={2}
v= {2}
WD = {1e-4, 5e-4}
αS,αT,βS,βT= {1}
ImageNet BS={256, 512}
LR={0.1, 0.2}
τ={1,2}
v= {2}
WD = {1e-4, 5e-4}
αS,αT,βS,βT= {1}
