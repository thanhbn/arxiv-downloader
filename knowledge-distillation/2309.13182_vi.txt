# Chưng cất hiệu quả khả năng suy luận dựa trên bảng từ các LLM
Bohao Yang1, Chen Tang1,2, Kun Zhao3, Chenghao Xiao4, Chenghua Lin1
1Khoa Khoa học Máy tính, Đại học Manchester, Vương quốc Anh
2Khoa Khoa học Máy tính, Đại học Surrey, Vương quốc Anh  
3Khoa Kỹ thuật Điện và Máy tính, Đại học Pittsburgh, Hoa Kỳ
4Khoa Khoa học Máy tính, Đại học Durham, Vương quốc Anh
j98519by@student.manchester.ac.uk, chen.tang@surrey.ac.uk,
kun.zhao@pitt.edu, chenghao.xiao@durham.ac.uk
chenghua.lin@manchester.ac.uk

## Tóm tắt
Các Mô hình Ngôn ngữ Lớn (LLM) đã chứng minh hiệu suất đáng chú ý trên nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên. Tuy nhiên, kích thước tham số khổng lồ và yêu cầu cực kỳ cao về sức mạnh tính toán tạo ra thách thức cho việc triển khai thực tế của chúng. Nghiên cứu gần đây đã tiết lộ rằng các khả năng cụ thể của LLM, chẳng hạn như suy luận số học, có thể được chuyển giao cho các mô hình nhỏ hơn thông qua chưng cất. Một số nghiên cứu khám phá tiềm năng của việc tận dụng LLM để thực hiện suy luận dựa trên bảng. Tuy nhiên, chưa có công trình nào trước đây tập trung vào kỹ năng suy luận bảng trong các mô hình nhỏ hơn được thiết kế riêng cho các nhiệm vụ tạo văn bản từ bảng khoa học. Trong bài báo này, chúng tôi đề xuất một phương pháp chưng cất suy luận dựa trên bảng mới, với mục tiêu chưng cất LLM thành các mô hình nhỏ hơn được thiết kế riêng. Kết quả thực nghiệm của chúng tôi đã cho thấy rằng một mô hình 220 triệu tham số (Flan-T5-base) được tinh chỉnh bằng dữ liệu đã chưng cất, không chỉ đạt được cải thiện đáng kể so với các baseline được tinh chỉnh truyền thống, mà còn vượt qua các LLM cụ thể trên tập dữ liệu tạo văn bản từ bảng khoa học. Mã của chúng tôi có sẵn tại https://github.com/Bernard-Yang/DistillTableCoT.

Từ khóa: suy luận dựa trên bảng, chưng cất, tạo văn bản từ bảng

## 1. Giới thiệu
Bảng, như một phương tiện lưu trữ kiến thức phổ biến và quan trọng, đã nhận được sự chú ý ngày càng tăng trong nghiên cứu hiện đại. Dữ liệu dạng bảng, khi kết hợp với dữ liệu văn bản, cung cấp một nguồn thông tin có giá trị và bổ sung. Giao điểm của thông tin dạng bảng và văn bản tạo thành một vấn đề đã được thiết lập tốt trong lĩnh vực Xử lý Ngôn ngữ Tự nhiên (NLP), với tác động trải rộng trên một phổ đa dạng của các nhiệm vụ downstream, bao gồm trả lời câu hỏi bảng (Pasupat và Liang, 2015; Cho et al., 2019; Nan et al., 2022), và kiểm tra sự thật bảng (Chen et al., 2020c; Gupta et al., 2020; Aly et al., 2021; Lu et al., 2023).

Các phương pháp truyền thống cho suy luận dựa trên bảng (Pasupat và Liang, 2015; Zhong et al., 2017; Yu et al., 2018) chủ yếu dựa vào việc tổng hợp các ngôn ngữ thực thi như SQL hoặc SPARQL để hỗ trợ truy xuất thông tin từ bảng. Tuy nhiên, những ngôn ngữ ký hiệu này thường đòi hỏi các giả định cứng nhắc về cấu trúc bảng, khiến chúng không thể nắm bắt được ngữ nghĩa nhúng trong các phân đoạn văn bản trong bảng. Một sự hiểu biết toàn diện về các bảng web đòi hỏi việc hiểu suy luận có cấu trúc cùng với suy luận văn bản. Để đạt được mục đích này, sự xuất hiện của các mô hình được pre-train dựa trên bảng (Herzig et al., 2020; Liu et al., 2021; Jiang et al., 2022; Cai et al., 2022) đã nhấn mạnh hiệu quả của việc pre-train các mô hình trên cả dữ liệu văn bản và dạng bảng để tăng cường khả năng suy luận. Cải thiện này bắt nguồn từ kiến thức rộng lớn thu được từ việc thu thập hoặc tổng hợp quy mô lớn dữ liệu dạng bảng và văn bản.

Trong những năm gần đây, sự ra đời của các Mô hình Ngôn ngữ Lớn (LLM) (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023) đã cách mạng hóa bối cảnh NLP, mở ra một kỷ nguyên mới được đánh dấu bằng hiệu suất đáng chú ý được chứng minh trên nhiều nhiệm vụ tạo văn bản có thể kiểm soát (Tang et al., 2022; Yang et al., 2023; Zhao et al., 2023a; Tang et al., 2023b). Các Mô hình Ngôn ngữ Lớn (LLM) ngầm nắm bắt các mối quan hệ phức tạp giữa các token trong chuỗi đầu vào, cho phép chúng hiểu một cách thành thạo các đặc trưng không đồng nhất hiện diện, bất kể định dạng cấu trúc của chúng, chẳng hạn như biểu diễn đồ thị, dữ liệu dạng bảng, hoặc các mẫu tuần tự (Huang et al., 2022; Goldsack et al., 2023; Tang et al., 2023a). Những mô hình này tận dụng corpus văn bản khổng lồ và trải qua pre-training rộng rãi, thể hiện khả năng đặc biệt để giải quyết các nhiệm vụ suy luận toán học và thông thường phức tạp, thường trong bối cảnh của các kịch bản học few-shot và zero-shot (Wei et al., 2022; Wang et al., 2022; Drozdov et al., 2022; Loakman et al., 2023; Zhou et al., 2023).

Lấy cảm hứng từ những phát triển đột phá này, một loạt các nghiên cứu (Chen, 2023; Ye et al., 2023; Cheng et al., 2023; Gemmell và Dalton, 2023; Lu et al., 2023) đã nổi lên để nối bật hiệu suất cạnh tranh của LLM so với các mô hình được tinh chỉnh tiên tiến trong lĩnh vực các nhiệm vụ suy luận bảng (ví dụ, trả lời câu hỏi bảng và kiểm tra sự thật bảng). Ví dụ, Zhao et al. (2023b) đã tìm hiểu tiềm năng của việc sử dụng LLM được tăng cường với các kỹ thuật Chain-of-Thought (CoT) trong tập dữ liệu LogicNLG (Chen et al., 2020c) cho các nhiệm vụ tạo văn bản từ bảng. Mặc dù có những tiến bộ đáng kể, nghiên cứu trước đây đã không tập trung vào lĩnh vực thách thức của nhiệm vụ tạo văn bản từ bảng khoa học nhận thức suy luận phức tạp hơn sử dụng LLM. Hơn nữa, số lượng tham số đáng kể và yêu cầu tính toán đòi hỏi cao đặt ra trở ngại cho việc triển khai khả thi của chúng. Do đó, chưng cất các khả năng suy luận dựa trên bảng nội tại của LLM thành các lựa chọn thay thế nhẹ hơn là một phương pháp hiệu quả và thân thiện với tài nguyên hơn.

Trong bài báo này, chúng tôi điều tra các khả năng của LLM trong nhiệm vụ tạo văn bản từ bảng khoa học nhận thức suy luận, và đề xuất một phương pháp chưng cất hai bước để chuyển giao khả năng suy luận dựa trên bảng của LLM vào các mô hình nhỏ hơn. Bản chất của nhiệm vụ tạo văn bản từ bảng khoa học phức tạp đòi hỏi LLM phải hiểu toàn diện các bảng được cung cấp và tham gia vào suy luận số học bao gồm cả dữ liệu dạng bảng và văn bản, thay vì chỉ đơn giản là chuyển đổi nội dung bảng thành các mô tả bề ngoài. Pipeline chưng cất của chúng tôi được thể hiện trong Hình 1, bao gồm việc sử dụng LLM để tạo ra nội dung suy luận dựa trên bảng và mô tả cho bảng đầu vào. Chúng tôi tiến hành thí nghiệm trên tập dữ liệu SciGen (Moosavi et al., 2021b), tập dữ liệu tạo văn bản từ bảng khoa học đầu tiên và thách thức hơn các benchmark tạo văn bản từ bảng tiêu chuẩn khác, chẳng hạn như Wiseman et al. (2017), Parikh et al. (2020), và Chen et al. (2020a), vì nó chứa nhiều suy luận số học hơn. Chúng tôi cũng cung cấp một ví dụ trong Hình 1, trong đó mô tả được tạo bởi T5-CoT tốt hơn T5-traditional, vì T5-CoT được tinh chỉnh với suy luận và mô tả đã chưng cất từ LLM. Điều này là do suy luận ví dụ mô tả hàng "S→M→Med", cho phép mô hình tập trung vào hàng cụ thể đó của bảng trong việc tinh chỉnh thêm các mô hình học sinh.

Đóng góp của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi khám phá tiềm năng của việc giải quyết nhiệm vụ tạo văn bản từ bảng khoa học nhận thức suy luận bằng LLM.
• Chúng tôi đề xuất một framework chưng cất hai giai đoạn chứa các giai đoạn tạo dữ liệu và tinh chỉnh. Trong giai đoạn tạo dữ liệu, chúng tôi sử dụng LLM để tạo ra suy luận dựa trên bảng và các phát biểu nhất quán về mặt sự thật, có thể mô tả bảng một cách chính xác dựa trên bảng đầu vào, sử dụng phương pháp Chain-of-Thought (CoT) một lần. Tiếp theo, trong giai đoạn tinh chỉnh, chúng tôi sử dụng dữ liệu CoT đã chưng cất được tạo bởi LLM để truyền đạt khả năng suy luận bảng cho các mô hình nhỏ hơn.
• Chúng tôi trình bày một loạt kết quả thực nghiệm nhấn mạnh rằng tinh chỉnh các mô hình nhỏ hơn với dữ liệu suy luận dựa trên bảng đã chưng cất từ LLM dẫn đến những cải thiện hiệu suất đáng kể so với các mô hình baseline trong bối cảnh các nhiệm vụ tạo văn bản từ bảng khoa học.
• Chúng tôi chứng minh rằng, chưng cất trao quyền cho các mô hình học sinh với chỉ 220 triệu tham số (ví dụ, chỉ 0,1% kích thước của mô hình giáo viên) để vượt trội hơn mô hình giáo viên 175 tỷ tham số trong một số metric nhất định.

## 2. Công trình liên quan

### 2.1. Suy luận dựa trên bảng
Các nhiệm vụ suy luận dựa trên bảng đòi hỏi khả năng suy luận trên cả ngôn ngữ tự nhiên và các bảng có cấu trúc. Suy luận dựa trên bảng truyền thống liên quan đến việc sử dụng phân tích ngữ nghĩa để thực thi các lệnh trên bảng, với các benchmark bao gồm WikiTableQuestions (Pasupat và Liang, 2015), WikiSQL (Zhong et al., 2017), và Spider (Yu et al., 2018). Những mô hình này được thiết kế để tạo ra SQL để tương tác với bảng. Tuy nhiên, những ngôn ngữ này áp đặt các tiêu chí nghiêm ngặt lên bảng và làm cho những phương pháp này không thể hiểu được ngữ nghĩa của các phân đoạn văn bản. Một số công trình đề xuất học các biểu diễn kết hợp bằng cách pre-train trên dữ liệu bảng và văn bản (Herzig et al., 2020; Liu et al., 2021; Zhao et al., 2022). Thông qua việc pre-train mô hình trên dữ liệu tổng hợp rộng lớn, chúng có thể đạt được hiệu suất mong muốn trên các nhiệm vụ liên quan đến bảng. Các công trình gần đây (Chen, 2023; Ye et al., 2023; Nan et al., 2023) đã chỉ ra khả năng của LLM trong các nhiệm vụ suy luận bảng thông qua học trong ngữ cảnh. Lu et al. (2023) sử dụng LLM để thực hiện suy luận trong nhiệm vụ kiểm tra sự thật bảng khoa học. Nhiệm vụ này đòi hỏi suy luận kết hợp sử dụng các bảng khoa học làm bằng chứng. BINDER (Cheng et al., 2023) sử dụng Codex để tổng hợp các truy vấn SQL để thực thi các dạng logic đối với bảng trong một nhiệm vụ trả lời câu hỏi.

### 2.2. Suy luận chuỗi tư duy
Prompting chuỗi tư duy (CoT) khuyến khích LLM chia nhỏ một nhiệm vụ suy luận thành một loạt các bước trung gian, do đó tăng cường khả năng suy luận trên các nhiệm vụ khác nhau (Wei et al., 2022; Shao et al., 2024). Với một vài ví dụ suy luận CoT, LLM có thể đạt được hiệu suất tiên tiến trên các nhiệm vụ suy luận số học phức tạp. CoT tự nhất quán (Wang et al., 2023) liên quan đến việc lấy mẫu nhiều CoT và chọn cái nhất quán nhất bằng cách tìm kiếm beam. Kojima et al. (2022) đề xuất CoT zero-shot bằng cách đầu tiên tạo ra các template CoT và tạo ra câu trả lời cuối cùng với LLM trong setting zero-shot.

### 2.3. Chưng cất kiến thức
Chưng cất đã chứng minh hiệu quả của nó trong việc chuyển giao các khả năng có giá trị từ một mô hình lớn hơn sang một mô hình nhỏ hơn (Hinton et al., 2015; Sanh et al., 2019; Zeng et al., 2022). Các công trình gần đây đã chỉ ra rằng dữ liệu tổng hợp được tạo bởi mô hình giáo viên có thể hiệu quả chuyển giao các khả năng chuyên biệt, chẳng hạn như suy luận số học, cho mô hình học sinh. Chung et al. (2022) sử dụng dữ liệu CoT được tạo thủ công để tinh chỉnh phiên bản dựa trên FLAN của PaLM (Chowdhery et al., 2022). Fu et al. (2023) sử dụng dữ liệu chuỗi tư duy phong phú để chuyên biệt hóa một mô hình nhỏ hơn. Ho et al. (2023) đề xuất phương pháp CoT đa dạng bằng cách lấy mẫu các đầu ra suy luận khác nhau từ một mô hình lớn để sau đó tinh chỉnh một mô hình nhỏ hơn. Magister et al. (2023) sử dụng pipeline hai bước để chuyển giao khả năng suy luận của các mô hình lớn sang các mô hình nhỏ hơn. Hsieh et al. (2023) trích xuất các cơ sở lý luận từ LLM và tích hợp dữ liệu đó trong framework tinh chỉnh hướng dẫn mô hình nhỏ hơn. Zhu et al. (2023) sử dụng LLM để chưng cất các chương trình, tiêm khả năng suy luận vào các mô hình nhỏ. Chúng tôi mở rộng những ý tưởng trên vào nhiệm vụ suy luận dựa trên bảng, cụ thể trong lĩnh vực tạo văn bản từ bảng khoa học, trong đó dữ liệu CoT được tạo ra dẫn đến hiệu suất suy luận bảng được cải thiện.

## 3. Phương pháp
Framework đề xuất của chúng tôi được minh họa trong Hình 2, bao gồm hai bước: tổng hợp dữ liệu từ LLM và tinh chỉnh các mô hình học sinh với dữ liệu đã chưng cất. Mục đích chính của giai đoạn đầu tiên là tạo ra suy luận dựa trên bảng và mô tả với LLM cho các bảng đầu vào thông qua CoT. Trong giai đoạn thứ hai, khả năng suy luận dựa trên bảng được chuyển giao vào các mô hình nhỏ hơn bằng cách tinh chỉnh với dữ liệu đã chưng cất từ LLM.

### 3.1. Định nghĩa nhiệm vụ
Chúng tôi định nghĩa nhiệm vụ như sau: Dữ liệu dạng bảng được serialize đầu vào được ký hiệu là T. Ngoài ra, dữ liệu suy luận dựa trên bảng đã chưng cất từ LLM được ký hiệu là R = r1, r2, ..., rn, trong đó ri là token của suy luận. Mục tiêu chính của nhiệm vụ này là tạo ra một mô tả Y = y1, y2, ..., ym, trong đó yi là token của mô tả và mô hình hoạt động bằng cách mô phỏng phân phối xác suất có điều kiện P(Y|T, R). Mô tả được tạo ra phải nhất quán về mặt sự thật với bảng đã cho, và chứa suy luận trên bảng.

### 3.2. Tạo suy luận dựa trên bảng
Quá trình tổng hợp dữ liệu của phương pháp đề xuất của chúng tôi được minh họa ở phần trên bên phải của Hình 2, dựa trên học trong ngữ cảnh (Brown et al., 2020), một khả năng nổi lên của LLM (Wei et al., 2022). Khác với tinh chỉnh truyền thống, học trong ngữ cảnh cho phép LLM đưa ra dự đoán dựa trên ngữ cảnh đầu vào nơi chỉ có một vài ví dụ được minh họa, mà không cần cập nhật tham số.

Chúng tôi sử dụng một LLM giáo viên lớn, gpt-3.5-turbo, để tạo ra suy luận dựa trên bảng thông qua CoT. Chúng tôi xây dựng quá trình tạo dữ liệu như sau: cho một bảng serialize đầu vào T, chúng tôi prompt LLM với ví dụ minh họa CoT một lần để tạo ra một suy luận R và một mô tả Y nhất quán về mặt sự thật với bảng đầu vào. Cụ thể, các ví dụ minh họa C = (T, R, Y) là một bộ ba bảng, suy luận và mô tả, trong đó R và Y được tạo thủ công. Cuối cùng, chúng tôi có thể tạo dữ liệu như sau:

Ri, Yi = LLMs(C, Ti) (1)

trong đó chúng tôi đặt trước ví dụ minh họa C như tiền tố cho bảng đầu vào Ti. Sau đó LLM sẽ làm theo hướng dẫn và học mẫu từ ví dụ để tạo ra suy luận Ri và mô tả Yi tương ứng.

**Suy luận đa dạng.** Nhiệm vụ bảng-sang-văn bản cho phép mô hình tạo ra các mô tả đa dạng bằng cách tập trung vào các vùng bảng khác nhau hoặc thực hiện các thao tác suy luận khác nhau, với điều kiện là các mô tả được tạo ra nhất quán về mặt sự thật với bảng (Zhao et al., 2023b). Để tối đa hóa khả năng suy luận được chưng cất từ LLM, chúng tôi sử dụng phương pháp suy luận đa dạng (Ho et al., 2023; Zhu et al., 2023; Zhao et al., 2023b) để tạo ra hai ví dụ suy luận và mô tả khác nhau cho một bảng khoa học đã cho. Chúng tôi không tạo thêm các cặp suy luận-mô tả cho mỗi bảng vì giới hạn ngữ cảnh tối đa của LLM và độ dài trung bình của các bảng và mô tả trong tập dữ liệu SciGen lớn hơn so với các tập dữ liệu bảng-sang-văn bản khác. Cụ thể, quá trình tạo dữ liệu được hiển thị như sau: cho một ngữ cảnh C và bảng Ti, LLM được yêu cầu tạo ra hai cặp suy luận và mô tả.

{(R1, Y1), (R2, Y2)} = LLMs(C, Ti) (2)

**Lọc dữ liệu.** Dữ liệu CoT dựa trên bảng được tổng hợp có thể chứa các mẫu không chính xác do vấn đề ảo giác của các mô hình tạo sinh (Zhu et al., 2023). Do đó, chúng tôi cần lọc dữ liệu CoT được tạo ra sai. Để lọc, chúng tôi theo Madaan et al. (2023) và sử dụng phương pháp Self-Refine. Cụ thể, khi tạo ra một tập dữ liệu mới (Ri, Yi) cho Ti, chúng tôi yêu cầu LLM xác minh xem mô tả được tạo ra Yi có nhất quán với bảng đầu vào Ti hay không. Chúng tôi có thể lọc ra các mẫu không chính xác để tinh chỉnh dữ liệu CoT được tạo ra của chúng tôi. Việc xác minh và lọc là quan trọng vì dữ liệu huấn luyện chất lượng cao sẽ cải thiện hiệu suất. Cuối cùng, chúng tôi có được 16.858 ví dụ đã được xác thực làm dữ liệu huấn luyện.

### 3.3. Tinh chỉnh các mô hình nhỏ
Khi chúng tôi có được dữ liệu suy luận dựa trên bảng đã tạo, chúng tôi sử dụng chúng để tinh chỉnh các mô hình nhỏ hơn và tiêm khả năng suy luận vào chúng. Về việc lựa chọn các mô hình nhỏ hơn, chúng tôi chọn T5 (Raffel et al., 2019) và Flan-T5 (Chung et al., 2022). Điều này là do các công trình gần đây (Fu et al., 2023; Zhu et al., 2023; Magister et al., 2023) đã tiết lộ rằng những mô hình này có thể đạt được khả năng suy luận số học đáng chú ý khi được huấn luyện với dữ liệu CoT trong nhiệm vụ giải quyết vấn đề toán học phức tạp.

Chúng tôi tinh chỉnh mô hình nhỏ hơn với dữ liệu suy luận dựa trên bảng đã tạo. Cụ thể, chúng tôi nối bảng T với suy luận dựa trên bảng R, được phân tách bởi một token đặc biệt được thêm vào "<CoT>". Chuỗi đầu vào kết quả có dạng sau: "T<CoT>R". Chúng tôi cung cấp một ví dụ trong Hình 3. Do đó, mô tả Y được tạo ra dựa trên cả bảng serialize đầu vào T và suy luận dựa trên bảng R với hàm mất mát sau:

L = -1/N ∑(n=1 to N) logP(Y|T, R) (3)

trong đó N biểu thị kích thước của dữ liệu huấn luyện, và L là mất mát entropy chéo.

## 4. Thí nghiệm

### 4.1. Tập dữ liệu
Chúng tôi tiến hành tạo văn bản từ bảng khoa học trên tập dữ liệu SciGen (Moosavi et al., 2021a). Thống kê của dữ liệu được hiển thị trong Bảng 1. Nó bao gồm ba setting khác nhau: few-shot, medium và large. Các tập train/val/test của setting medium được chia thành các kích thước 13.607/3.452/1.038. Setting large được chia thành 39.969/12.129/1.038. Chúng tôi chọn các setting medium và large để tiến hành thí nghiệm. Điều này là do setting few-shot chỉ chứa 200 ví dụ dữ liệu huấn luyện và không đủ cho tinh chỉnh.

### 4.2. Baseline
Chúng tôi theo Moosavi et al. (2021a) và chọn T5 (Raffel et al., 2019) và BART (Lewis et al., 2020) làm baseline mô hình học sinh. Đối với baseline BART, chúng tôi sử dụng BART-large với 0,40B tham số. Đối với mô hình T5, chúng tôi sử dụng T5-base và T5-large với 0,22B và 0,77B tham số, tương ứng. Đối với các mô hình giáo viên, chúng tôi chọn text-davinci-002 và gpt-3.5-turbo làm baseline. Đối với setting prompt một lần, chúng tôi theo các công trình trước đây (Chen, 2023; Zhao et al., 2023b), đặt trước một ví dụ minh họa cho bảng đầu vào. Chúng tôi so sánh với hai biến thể của các mô hình giáo viên, được gọi là 1-shot direct và 1-shot CoT. Đối với công thức prompt của 1-shot direct, chúng tôi theo setting của Moosavi et al. (2021a) để tuyến tính hóa bảng và nối nó với mô tả vàng như một minh họa. Về prompt của 1-shot CoT, chúng tôi đặt trước bảng đầu vào với hai suy luận dựa trên bảng và mô tả được tạo thủ công.

| Setting | Text | Train | Val | Test |
|---------|------|-------|-----|------|
| Few-shot | 116 | 200 | 100 | 1.038 |
| Medium | 124 | 13.607 | 3.452 | 1.038 |
| Large | 133 | 39.969 | 12.129 | 1.038 |

**Bảng 1:** Thống kê tập dữ liệu SciGen. Text chỉ ra độ dài trung bình tính bằng từ của các mô tả.

### 4.3. Cài đặt thí nghiệm
Để sử dụng các baseline tạo văn bản text-to-text ở trên, chúng tôi theo setting trong Moosavi et al. (2021a) và chuyển đổi bảng thành các chuỗi văn bản. Để bảo tồn và giúp mô hình học tốt hơn cấu trúc bảng, chúng tôi thêm bốn token đặc biệt để chỉ định điểm bắt đầu của hàng, ô, tiêu đề bảng và suy luận CoT với các token "<R>", "<C>", "<CAP>", "<CoT>", tương ứng. Hình 3 hiển thị một bảng gốc từ một bài báo khoa học (Nam et al., 2019) và biểu diễn đầu vào tuyến tính tương ứng của nó. Suy luận và mô tả được tạo từ LLM cũng được cung cấp.

### 4.4. Metric đánh giá tự động
Chúng tôi sử dụng một loạt các metric đánh giá tự động từ các cấp độ khác nhau để đánh giá hiệu suất của mô hình.

**Cấp độ bề mặt.** Theo Moosavi et al. (2021a), chúng tôi chọn METEOR (Banerjee và Lavie, 2005), BERTScore (Zhang et al., 2020), và BLEURT (Sellam et al., 2020) để đo độ tương tự bề mặt của các phát biểu được tạo ra với các tham chiếu vàng.

METEOR căn chỉnh văn bản đầu ra với văn bản tham chiếu và tính toán điểm tương tự cấp câu dựa trên các căn chỉnh.

BERTScore sử dụng embedding BERT, căn chỉnh các từ trong cả câu được tạo ra và câu tham chiếu bằng độ tương tự cosine. Nó tính toán điểm precision, recall và F1.

BLEURT là một metric đánh giá đã học dựa trên BERT. Nó được pre-train đầu tiên trên các ví dụ tổng hợp và sau đó được tinh chỉnh trên các đánh giá của con người cho nhiệm vụ dịch máy.

Tuy nhiên, Moosavi et al. (2021a) đã nêu rằng những metric này không đủ vì phạm vi giá trị khá thấp (ngoại trừ BERTScore). Ngoài ra, trong một số trường hợp, các mô tả không chính xác ghi điểm cao hơn những mô tả chính xác.

**Cấp độ trung thực.** Các công trình gần đây (Moosavi et al., 2021a; Liu et al., 2022a) đã chỉ ra rằng các metric cấp độ bề mặt ở trên không thể đo lường tính chính xác về mặt sự thật của các mô tả được tạo ra cho các bảng tương ứng. Nhiệm vụ SciGen đòi hỏi mô hình tạo ra các phát biểu chứa suy luận số học trên các giá trị bảng. Ngoài ra, các phát biểu được tạo ra có thể bao gồm một vùng bảng khác với tham chiếu vàng. Do đó, chúng tôi thêm hai metric cấp độ trung thực (để đánh giá xem câu được tạo ra có được dựa trên bảng đầu vào hay không), TAPAS-Acc và TAPEX-Acc (Liu et al., 2022a) để đánh giá tính nhất quán và độ trung thực về mặt sự thật, đã được sử dụng rộng rãi cho đánh giá bảng-sang-văn bản.

TAPAS-Acc tinh chỉnh TAPAS (Herzig et al., 2020) trên tập dữ liệu TabFact (Chen et al., 2020b) và đạt được độ chính xác kiểm tra 81%.

TAPEX-Acc sử dụng TAPEX (Liu et al., 2022b) được tinh chỉnh trên tập dữ liệu TabFact và đạt được độ chính xác kiểm tra 84%. Các công trình trước đây (Liu et al., 2022a; Zhao et al., 2023b) đã nêu rằng TAPAS-Acc quá tích cực về các dự đoán, trong khi TAPEX-Acc đáng tin cậy hơn cho việc đánh giá tính trung thực của các câu được tạo ra. Cả hai metric không tham chiếu ở trên đều ghi điểm các mô tả được tạo ra là 0 cho bị bác bỏ và 1 cho được xác nhận cho các bảng tương ứng.

## 5. Kết quả
Trong phần này, chúng tôi đánh giá cả hiệu suất của LLM giáo viên và các mô hình nhỏ hơn được tinh chỉnh trên nhiệm vụ tạo văn bản từ bảng khoa học. Chúng tôi tiến hành đánh giá tự động trên cả metric Cấp độ bề mặt và Cấp độ trung thực. Kết quả tổng thể được hiển thị trong Bảng 2. So sánh các metric Cấp độ trung thực giữa các mô hình giáo viên và mô hình học sinh trên large được trình bày trong Hình 5 và Hình 6.

### 5.1. Hiệu suất của LLM
Thí nghiệm của chúng tôi bao gồm hai phương pháp học trong ngữ cảnh, Direct Prompt và CoT Prompt. Chúng tôi chọn text-davinci-002 và gpt-3.5-turbo để tiến hành thí nghiệm trên tập dữ liệu SciGen. Như được hiển thị trong Bảng 2, về các metric cấp độ bề mặt, cả Direct Prompt và CoT Prompt đều không thể đạt được hiệu suất tốt nhất, ngoại trừ gpt-3.5-turbo (1-shot direct) đạt được hiệu suất tốt nhất trên BERTScore. Tuy nhiên, các metric cấp độ bề mặt không thể đo lường chính xác tính trung thực và độ chính xác của đầu ra được tạo ra bởi các mô hình. Về các metric cấp độ trung thực, text-davinci-002 (1-shot direct) có thể đạt được độ chính xác trên 64% và gpt-3.5-turbo (1-shot direct) có thể đạt được độ chính xác trên 70% trên cả TAPAS-Acc và TAPEX-Acc, vượt trội hơn các mô hình baseline được tinh chỉnh truyền thống (tức là BART và T5). Khi kết hợp prompt trực tiếp với suy luận CoT, độ chính xác của cả text-davinci-002 (1-shot CoT) và gpt-3.5-turbo (1-shot CoT) tăng khoảng 10% trên cả hai metric.

### 5.2. Hiệu suất của mô hình nhỏ hơn được tinh chỉnh
Về các metric cấp độ bề mặt, các mô hình nhỏ hơn, dù được tinh chỉnh với dữ liệu CoT hay không, đều thể hiện một phạm vi hẹp các giá trị thấp, với các giá trị tuyệt đối nằm trong phạm vi thang đo Likert 0-1. Kết quả thực nghiệm phù hợp với các phát biểu trong bài báo SciGen (Moosavi et al., 2021a) rằng các metric cấp độ bề mặt không đủ để phản ánh khả năng của các mô hình trên nhiệm vụ phức tạp này.

**Các mô hình nhỏ với tinh chỉnh truyền thống không hoạt động tốt trên các metric cấp độ trung thực.** Về các mô hình nhỏ hơn được tinh chỉnh mà không có dữ liệu CoT, BART-large được tinh chỉnh trên tập dữ liệu medium đạt được kết quả tốt nhất trên các metric cấp độ bề mặt. Tuy nhiên, về cấp độ trung thực, tất cả các baseline BART và T5 chỉ đạt được độ chính xác hơi cao hơn cơ hội ngẫu nhiên. Chúng tôi tiếp tục điều tra tác động của kích thước tập dữ liệu, từ Setting Medium đến Setting Large. Mặc dù kích thước của tập dữ liệu Setting Large gấp ba lần Setting Medium, cải thiện hiệu suất không đáng kể như vậy (tức là chỉ tăng khoảng 2% trên các metric cấp độ trung thực). Tuy nhiên, đối với các metric cấp độ bề mặt, các mô hình được huấn luyện với các tập dữ liệu Medium đạt được hiệu suất tổng thể tốt hơn, đặc biệt trong METEOR và BLEURT.

**Các mô hình nhỏ được tinh chỉnh với dữ liệu CoT đạt được cải thiện hiệu suất đáng kể.** Mặt khác, các mô hình T5 và Flan-T5 với tinh chỉnh CoT có thể đạt được hiệu suất tổng thể tốt nhất trên các metric cấp độ trung thực trong số tất cả các mô hình nhỏ. Tất cả hiệu suất của các mô hình tinh chỉnh CoT đều ngang hàng với mô hình giáo viên (tức là gpt-3.5-turbo (1-shot CoT)) trên các metric cấp độ trung thực. Ví dụ, T5-large-CoT và Flan-T5-base-CoT đạt được TAPAS-Acc cao nhất (80,62%) và TAPEX-Acc (82,75%), và chỉ kém hiệu suất mô hình giáo viên với hiệu suất tốt nhất 2%. Những kết quả này chỉ ra rằng tinh chỉnh với dữ liệu CoT đã chưng cất từ LLM có thể chuyển giao khả năng suy luận dựa trên bảng vào các mô hình nhỏ hơn.

**Kích thước mô hình lớn hơn không đảm bảo cải thiện hiệu suất khi được tinh chỉnh mà không có dữ liệu CoT.** Hơn nữa, thí nghiệm của chúng tôi cũng điều tra tác động của kích thước mô hình đối với tinh chỉnh CoT, từ biến thể base đến large. Mặc dù trực quan mong đợi cải thiện hiệu suất với các mô hình lớn hơn, kết quả thực nghiệm trên metric TAPEX-Acc tiết lộ rằng các mô hình với số lượng tham số lớn hơn, chẳng hạn như T5-large và Flan-T5-large, không liên tục vượt trội hơn các đối tác nhỏ hơn của chúng, T5-base và Flan-T5-base. Tuy nhiên, về TAPAS-Acc, cải thiện hiệu suất nhất quán, với kích thước mô hình tăng từ base (0,22B) đến large (0,77B).

### 5.3. So sánh giữa mô hình giáo viên và học sinh
Chúng tôi cũng so sánh hiệu suất trên các metric cấp độ trung thực (TAPAS-Acc và TAPEX-Acc) của cả mô hình giáo viên (LLM) và mô hình học sinh trong Hình 5 và Hình 6. Đối với mô hình giáo viên, gpt-3.5-turbo (1-shot direct) vượt trội hơn tất cả các mô hình baseline nhỏ hơn (các mô hình nhỏ hơn được tinh chỉnh mà không có dữ liệu CoT) và text-davinci-002 (1-shot direct). Ngoài ra, gpt-3.5-turbo (1-shot CoT) đạt được hiệu suất tốt nhất trên cả metric TAPAS-Acc và TAPEX-Acc trong số cả mô hình giáo viên và học sinh. Về các mô hình nhỏ hơn, cả T5 và Flan-T5 chỉ có thể đạt được khoảng 55% độ chính xác trên cả hai metric cấp độ trung thực mà không được tinh chỉnh với dữ liệu CoT. Tuy nhiên, những mô hình nhỏ hơn này có thể được tiêm khả năng suy luận sau khi tinh chỉnh với dữ liệu CoT, đạt được khoảng 80% độ chính xác trên cả hai metric.

### 5.4. Nghiên cứu khử thành phần
Nghiên cứu khử thành phần của tinh chỉnh với dữ liệu CoT được hiển thị trong cả Bảng 3 và Hình 4. Đối với cả mô hình T5 và Flan-T5, chúng tôi có thể quan sát thấy sự tăng đáng kể sau khi tinh chỉnh với dữ liệu CoT trong cả TAPAS-Acc và TAPEX-Acc trên nhiệm vụ tạo văn bản từ bảng SciGen. Đối với metric TAPAS-Acc, các mô hình T5 và Flan-T5 base (0,22B) và large (0,77B) chỉ có thể đạt được độ chính xác trên 55%. Tuy nhiên, khi tinh chỉnh với dữ liệu CoT dựa trên bảng từ LLM, có sự tăng độ chính xác đáng kể (trên 20%) được quan sát. Ví dụ, độ chính xác 55% của T5-large với tinh chỉnh tiêu chuẩn có thể được cải thiện lên 80% sau khi được tinh chỉnh với dữ liệu CoT. Về metric TAPEX-Acc, có thể quan sát thấy xu hướng tương tự, trong đó cải thiện tổng thể về độ chính xác trên 25%. Ví dụ, cải thiện đáng kể nhất có thể được quan sát trong mô hình T5-base, từ 53% (tinh chỉnh truyền thống) lên 82% (tinh chỉnh CoT).

### 5.5. Phân tích dữ liệu được tạo
LLM chúng tôi sử dụng trong bài báo này đóng góp vào việc tổng hợp dữ liệu CoT dựa trên bảng chất lượng cao. Tuy nhiên, trong quá trình tạo ra, có một số dữ liệu được tạo ra sai do tính chất ảo giác của LLM. Do đó, chúng tôi tiến hành phân tích toàn diện các mẫu được tạo ra bởi LLM. Kết quả đánh giá được hiển thị trong Hình 7, gpt-3.5-turbo đạt được độ chính xác 85% trên tập huấn luyện, nơi các mô tả được tạo ra được xác minh là chính xác. Về tập kiểm tra của SciGen, độ chính xác trên 90%, và với ít hơn 10% các mẫu được coi là không chính xác. Về nhiệm vụ tạo văn bản từ bảng, cả suy luận và mô tả được tạo ra đều thể hiện sự nhất quán và đồng nhất chất lượng cao cho bảng đầu vào.

## 6. Kết luận
Trong bài báo này, chúng tôi giới thiệu một framework chưng cất hai giai đoạn chưng cất dữ liệu CoT dựa trên bảng từ LLM. Thí nghiệm của chúng tôi minh họa rằng phương pháp này có thể hiệu quả chuyển giao khả năng suy luận bảng cho các mô hình nhỏ hơn trong nhiệm vụ tạo văn bản từ bảng khoa học. Cải thiện hiệu suất thậm chí có thể vượt trội hơn một số LLM giáo viên nhất định (ví dụ, gpt-3.5-turbo). Phương pháp đề xuất của chúng tôi đạt được ưu thế toàn diện trong nhiệm vụ cụ thể này trong khi yêu cầu ít dữ liệu hơn và các mô hình nhỏ hơn.

## Tài liệu tham khảo thư mục
[Các tài liệu tham khảo được duy trì nguyên bản tiếng Anh như trong bản gốc]
