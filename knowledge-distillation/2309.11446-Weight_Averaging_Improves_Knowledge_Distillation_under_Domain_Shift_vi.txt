# 2309.11446.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2309.11446.pdf
# Kích thước tệp: 249386 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Trung Bình Hóa Trọng Số Cải Thiện Chưng Cất Tri Thức dưới Sự Dịch Chuyển Miền
Valeriy Berezovskiy
Đại học HSE
vsberezovskiy@edu.hse.ruNikita Morozov
Đại học HSE
nvmorozov@hse.ru
Tóm tắt
Chưng cất tri thức (KD) là một kỹ thuật nén mô hình mạnh mẽ được sử dụng rộng rãi trong các ứng dụng học sâu thực tế. Nó tập trung vào việc huấn luyện một mạng học viên nhỏ để bắt chước một mạng giáo viên lớn hơn. Mặc dù được biết rộng rãi rằng KD có thể mang lại cải thiện cho khả năng tổng quát hóa của học viên trong môi trường i.i.d, hiệu suất của nó dưới sự dịch chuyển miền, tức là hiệu suất của các mạng học viên trên dữ liệu từ các miền chưa thấy trong quá trình huấn luyện, đã nhận được ít sự chú ý trong tài liệu. Trong bài báo này, chúng tôi thực hiện một bước hướng tới việc kết nối các lĩnh vực nghiên cứu chưng cất tri thức và tổng quát hóa miền. Chúng tôi chỉ ra rằng các kỹ thuật trung bình hóa trọng số được đề xuất trong tài liệu tổng quát hóa miền, như SWAD và SMA, cũng cải thiện hiệu suất của chưng cất tri thức dưới sự dịch chuyển miền. Ngoài ra, chúng tôi đề xuất một chiến lược trung bình hóa trọng số đơn giản không yêu cầu đánh giá trên dữ liệu xác thực trong quá trình huấn luyện và chỉ ra rằng nó hoạt động tương đương với SWAD và SMA khi áp dụng cho KD. Chúng tôi đặt tên cho phương pháp chưng cất cuối cùng của mình là Chưng Cất Tri Thức Trung Bình Hóa Trọng Số (WAKD).
1. Giới thiệu
Các mô hình thị giác quy mô lớn qua nhiều năm đã cho thấy kết quả ấn tượng trong các nhiệm vụ thị giác máy tính khác nhau [12, 34, 16, 9, 22]. Tuy nhiên, việc sử dụng các mô hình như vậy trong thực tế thường bị hạn chế bởi các tài nguyên tính toán có sẵn. Chưng cất tri thức [13] là một kỹ thuật nén phổ biến có thể được sử dụng để cải thiện cả hiệu quả bộ nhớ và thời gian tính toán của các mạng nơ-ron lớn hoặc tập hợp mạng nơ-ron. KD chuyển giao tri thức từ một mô hình giáo viên cồng kềnh đến một mô hình học viên nhẹ hơn bằng cách huấn luyện học viên để phù hợp với hành vi của giáo viên trên dữ liệu có sẵn thay vì dự đoán các nhãn thực.
Tổng quát hóa miền (DG) [5] tập trung vào một thách thức khác thường xuất hiện trong các ứng dụng học máy: dịch chuyển miền. Các mô hình của chúng ta hoạt động tốt như thế nào khi phân phối dữ liệu huấn luyện và kiểm tra khác nhau? Một ví dụ có thể là một mô hình thị giác gặp phải một điều kiện thời tiết hoặc thời gian trong ngày chưa thấy trong quá trình huấn luyện. Trong các điểm chuẩn DG, các mô hình được huấn luyện trên một số miền nguồn và hiệu suất của chúng được đánh giá trên một miền đích khác không có sẵn trong quá trình huấn luyện để mô phỏng môi trường dịch chuyển miền.
Trọng tâm của bài báo này nằm ở giao điểm của hai lĩnh vực này. Khả năng tổng quát hóa của giáo viên đối với các miền chưa thấy được chuyển giao đến học viên như thế nào trong quá trình KD? Câu hỏi này tự nhiên xuất hiện trong tình huống khi chúng ta có một mô hình lớn hoạt động tốt dưới sự dịch chuyển miền và muốn nén nó thành một mô hình nhỏ hơn, bảo tồn khả năng tổng quát hóa đến các miền chưa thấy. Do đó, kịch bản quan tâm của chúng tôi là khi cả việc huấn luyện giáo viên và quá trình chưng cất đều được thực hiện trên các miền nguồn, trong khi thước đo hiệu suất được tính toán trên miền đích. Mặc dù có một lượng lớn nghiên cứu hiện có về KD và DG, môi trường này đã nhận được ít sự chú ý trong tài liệu hiện có.
Trong công trình này, chúng tôi áp dụng các kỹ thuật trung bình hóa trọng số để cải thiện hiệu suất chưng cất tri thức dưới sự dịch chuyển phân phối. Trung bình hóa trọng số mô hình được đề xuất để cải thiện việc huấn luyện mạng nơ-ron trong SWA [14], nơi trọng số của nhiều mô hình từ một quỹ đạo huấn luyện đơn lẻ được trung bình hóa để xây dựng một mô hình tổng quát hóa tốt hơn (không nên nhầm lẫn với tập hợp, nơi các dự đoán của nhiều mô hình được trung bình hóa). Một số sửa đổi SWA đã được tạo ra và áp dụng thành công cho tổng quát hóa miền, như SWAD [8] và SMA [2], cho thấy rằng trung bình hóa trọng số cũng cải thiện khả năng tổng quát hóa ngoài phân phối. Chúng tôi áp dụng SWAD và SMA cho chưng cất tri thức, cho thấy rằng chúng cải thiện hiệu suất KD trên hai bộ dữ liệu tổng quát hóa miền (PACS [18] và Office-Home [38]) và hai kiến trúc mạng nơ-ron giáo viên-học viên (ResNet và ViT). Ngoài ra, chúng tôi giới thiệu một chiến lược trung bình hóa trọng số đơn giản bằng cách sửa đổi SMA để bao gồm tất cả trọng số mạng cho đến cuối quỹ đạo huấn luyện và chỉ ra rằng nó hoạt động tương đương với các đối tác của nó. Ưu điểm của chiến lược này so với SWAD và SMA là nó không cần tính toán hiệu suất tập xác thực của học viên để chọn đoạn trung bình hóa, do đó giảm chi phí tính toán của toàn bộ quy trình.
Chúng tôi đặt tên cho phương pháp chưng cất tri thức như vậy là Chưng Cất Tri Thức Trung Bình Hóa Trọng Số (WAKD), xây dựng một đường cơ sở đơn giản nhưng mạnh mẽ cho nghiên cứu tương lai về chưng cất tri thức dưới sự dịch chuyển miền. Mã nguồn của chúng tôi có sẵn tại https://github.com/vorobeevich/ distillation-in-dg .
2. Công trình liên quan
Tổng quát hóa miền: Vấn đề tổng quát hóa miền được giới thiệu chính thức trong [5]. Qua nhiều năm, một loạt các phương pháp tổng quát hóa miền đã được đề xuất trong tài liệu, bao gồm các kỹ thuật dựa trên căn chỉnh miền [24, 23, 20], học meta [19, 3, 49], học tự giám sát [7, 40, 1, 6] và tăng cường dữ liệu [39, 30, 43]. Xem [50] để có một khảo sát toàn diện. Một công trình quan trọng trong lĩnh vực [11] đã chỉ ra rằng việc lựa chọn mô hình có thể rất quan trọng, và khi được tính đến một cách chính xác trong quá trình so sánh các phương pháp, đường cơ sở tối thiểu hóa rủi ro thực nghiệm (ERM [37]) đã cho thấy hiệu suất mạnh mẽ so với các phương pháp DG của thời đó. Trọng tâm chính của công trình chúng tôi là các phương pháp dựa trên trung bình hóa trọng số.
SWAD [8] lập luận rằng việc tìm kiếm các điểm cực tiểu phẳng dẫn đến hiệu suất tốt hơn của mô hình dưới sự dịch chuyển miền và đề xuất một chiến lược trung bình hóa trọng số dày đặc cho mục đích này. Một nhược điểm của SWAD là thực tế nó đưa ba siêu tham số bổ sung vào quá trình huấn luyện được sử dụng để tìm đoạn trọng số để trung bình hóa. Nhược điểm này được trả lời trong [2], nơi, ngoài các đóng góp khác, các tác giả trình bày SMA: một chiến lược trung bình hóa trọng số thay thế không có siêu tham số.
Chưng cất tri thức: Chưng cất tri thức được đề xuất trong [13] như một kỹ thuật nén mô hình. Phương pháp ban đầu là tối thiểu hóa phân kỳ KL giữa các phân phối dự đoán của giáo viên và học viên, do đó huấn luyện học viên để phù hợp với các dự đoán của giáo viên. Một số biến thể và sửa đổi của phương pháp đã được đề xuất sau công trình ban đầu [28, 45, 35, 47, 42, 48]. Trái ngược với việc phát triển các mục tiêu chưng cất mới, [4] đã chỉ ra rằng phương pháp ban đầu vẫn là một phương pháp mạnh mẽ và cạnh tranh trong các nhiệm vụ thị giác máy tính khi tính đến một số lựa chọn thiết kế, như huấn luyện lâu hơn và chuyển cùng những hình ảnh tăng cường đến giáo viên và học viên.
Có một số công trình sử dụng ý tưởng từ tài liệu KD để xây dựng các phương pháp mới cho tổng quát hóa miền [41, 17, 32, 31], tuy nhiên những công trình này không sử dụng chưng cất tri thức như một kỹ thuật nén mô hình.
Các tác giả của [10] xem xét một môi trường giống với của chúng tôi, với sự khác biệt là mục tiêu của học viên là phù hợp với hiệu suất của giáo viên trên miền mà giáo viên được huấn luyện, mà không có quyền truy cập vào bất kỳ dữ liệu nào từ miền này.
Trong số các công trình duy nhất nghiên cứu môi trường tương tự như chúng tôi, [51] chỉ ra rằng các kỹ thuật tăng cường dữ liệu như CutMix [44] và Mixup [46] cải thiện hiệu suất KD dưới sự dịch chuyển miền. Các tác giả của [25] quan sát thấy sự thống nhất cao giữa các dự đoán của giáo viên và học viên trên các miền chưa thấy bởi cả hai, điều này có thể chỉ ra rằng KD thực sự chuyển giao tri thức về các miền chưa thấy. Trong một hướng công trình liên quan khác, [21] nghiên cứu tổng quát hóa ngoài phân phối khi chưng cất các mô hình thị giác-ngôn ngữ (ví dụ: CLIP [27]).
3. Phương pháp luận
3.1. Tổng quát hóa miền
Môi trường chính thức của tổng quát hóa miền như sau. Gọi X là không gian đầu vào và Y là không gian đích. Chúng ta có quyền truy cập vào dữ liệu từ K miền nguồn Sk, mỗi miền liên quan đến một phân phối chung PkXY trên các cặp từ X × Y.
Mục tiêu của DG là học một mô hình dự đoán f: X → Y chỉ sử dụng dữ liệu miền nguồn sao cho lỗi dự đoán trên một miền đích chưa thấy T được tối thiểu hóa.
Phân phối chung miền đích tương ứng được ký hiệu là PTXY, và được giả định rằng PTXY ≠ PkXY, ∀k ∈ {1, . . . , K}.
Các bộ dữ liệu DG thường chứa dữ liệu từ nhiều miền được gán nhãn. Để đánh giá hiệu suất của một phương pháp huấn luyện, tất cả các phân chia có thể được xem xét trong đó một miền được chọn làm đích và các miền khác được chọn làm nguồn.
Điều quan trọng cần lưu ý là môi trường DG tiêu chuẩn cấm bất kỳ việc sử dụng dữ liệu miền đích nào trong cả quá trình huấn luyện và lựa chọn mô hình, có nghĩa là cả dữ liệu huấn luyện và xác thực đều đến từ các miền nguồn. Tuy nhiên, [11] đã chỉ ra rằng việc chọn một mô hình với hiệu suất dữ liệu xác thực nguồn tốt nhất là một chiến lược khả thi cho việc lựa chọn mô hình trong DG.
3.2. Chưng cất tri thức
Trong các thí nghiệm của chúng tôi, chúng tôi xem xét các vấn đề phân loại hình ảnh và sử dụng mục tiêu chưng cất được sử dụng rộng rãi và nghiên cứu kỹ lưỡng được đề xuất trong công trình ban đầu [13]: phân kỳ KL giữa các vectơ xác suất lớp dự đoán của giáo viên và học viên.
Gọi zt và zs là các logit lớp đầu ra được dự đoán bởi mạng giáo viên và học viên tương ứng trên cùng một điểm dữ liệu. Khi đó mất mát chưng cất là
LKD(zs,zt) := −τ2ΣCj=1σj(zt/τ)logσj(zs/τ), (1)
trong đó σi(z) := exp(zi)/Σjexp(zj) là hàm softmax và C là số lượng lớp. Tham số nhiệt độ τ được giới thiệu để điều chỉnh entropy của các phân phối xác suất softmax dự đoán trước khi chúng được sử dụng trong tính toán mất mát. Mất mát được trình bày không chứa entropy của giáo viên vì nó không phụ thuộc vào các dự đoán của học viên.
Trong quá trình chưng cất, trọng số của mạng giáo viên được đóng băng, trong khi mạng học viên tối thiểu hóa LKD, do đó

--- TRANG 2 ---
học để phù hợp với các đầu ra của giáo viên thay vì dự đoán các nhãn cứng thực. Toàn bộ quy trình trong nghiên cứu của chúng tôi bao gồm việc huấn luyện mạng giáo viên trên tập hợp các miền nguồn có sẵn và sau đó chưng cất nó thành mạng học viên trên cùng dữ liệu. Thước đo quan tâm của chúng tôi là độ chính xác của mạng học viên trên miền đích. Để chính xác hơn, đó là sự cải thiện độ chính xác miền đích khi sử dụng mục tiêu chưng cất so với việc huấn luyện cùng mạng để dự đoán nhãn cứng.
3.3. Chiến lược trung bình hóa trọng số
Các phương pháp trung bình hóa trọng số theo ý tưởng chọn một tập con các mô hình từ một quỹ đạo huấn luyện thay vì một mô hình đơn lẻ và trung bình hóa trọng số của chúng để tạo ra một mô hình tổng quát hóa tốt hơn cuối cùng.
SWAD đánh giá mô hình hiện tại trong quá trình huấn luyện với một tần suất nào đó, chọn một đoạn của các lần lặp huấn luyện để trung bình hóa trọng số dựa trên các giá trị mất mát xác thực của các mô hình được đánh giá. Điểm bắt đầu của đoạn được chọn là lần lặp đầu tiên mà giá trị mất mát không còn giảm trong Ns lần lặp. Điểm kết thúc của đoạn được chọn là lần lặp đầu tiên mà giá trị mất mát vượt quá mất mát tại điểm bắt đầu của đoạn nhân dung sai r trong Ne lần lặp. Sau đó, tất cả các mô hình cho mỗi lần lặp huấn luyện bên trong đoạn được trung bình hóa. Ns, Ne, và r là các siêu tham số của phương pháp. Chúng tôi tham khảo người đọc đến [8] để có mô tả chi tiết hơn về thuật toán.
SMA cung cấp một giải pháp thay thế đơn giản hơn cho SWAD. Nó cũng trung bình hóa tất cả các mô hình bên trong một đoạn của các lần lặp huấn luyện, nhưng luôn lấy điểm bắt đầu của đoạn là một lần lặp cố định gần điểm bắt đầu của quá trình huấn luyện. Độ chính xác xác thực được tính toán cho các mô hình đã được trung bình hóa từ điểm bắt đầu của đoạn đến lần lặp hiện tại thay vì các mô hình riêng lẻ. Điểm kết thúc của đoạn được lấy để tối đa hóa độ chính xác xác thực của mô hình được trung bình hóa.
Trong phần tiếp theo, chúng tôi thực hiện chưng cất tri thức với SWAD và SMA, cho thấy rằng chúng tạo ra các học viên với độ chính xác miền đích tốt hơn so với việc đơn giản chọn một mô hình riêng lẻ có hiệu suất xác thực tốt nhất từ một quỹ đạo huấn luyện.
Quan sát thấy rằng SWAD với mất mát chưng cất thường hoạt động theo cách mà điểm kết thúc của đoạn được chọn gần điểm kết thúc của quá trình huấn luyện, chúng tôi đưa ra một chiến lược thậm chí đơn giản hơn so với cả hai. Chúng tôi bắt đầu trung bình hóa từ một lần lặp cố định gần điểm bắt đầu của quá trình huấn luyện (10% trong các thí nghiệm của chúng tôi) giống như SMA, nhưng trung bình hóa tất cả trọng số cho đến cuối quỹ đạo huấn luyện. Ưu điểm của chiến lược này so với SWAD và SMA là nó không yêu cầu bất kỳ xác thực mô hình nào trong quá trình huấn luyện. Do đó quy trình WAKD được đề xuất của chúng tôi bao gồm việc chạy một quá trình chưng cất tri thức thông thường và sau đó trung bình hóa trọng số của tất cả các mô hình từ quỹ đạo huấn luyện ngoại trừ 10% đầu tiên.
4. Thí nghiệm
4.1. Chi tiết thí nghiệm
Chúng tôi thực hiện đánh giá thí nghiệm trên hai bộ dữ liệu tổng quát hóa miền: PACS [18] (9,991 hình ảnh, 7 lớp, và 4 miền) và OfficeHome [38] (15,588 hình ảnh, 65 lớp, và 4 miền).
Chúng tôi khám phá hai kiến trúc giáo viên-học viên: ResNet-50 đến ResNet-18 [12] và DeiT-Small đến DeiT-Tiny [36]. Điều quan trọng cần lưu ý là các tác giả của DeiT đã trình bày phương pháp chưng cất mới của riêng họ [36], trong khi chúng tôi sử dụng mục tiêu KD ban đầu (xem Phương trình 1) để tạo ra một thiết lập thí nghiệm thống nhất.
Không có sự cần thiết đặc biệt nào trong việc sử dụng giáo viên có hiệu suất tốt nhất có thể trong thiết lập thí nghiệm của chúng tôi vì mối quan tâm chủ yếu nằm ở hiệu suất của KD, do đó chúng tôi đã chọn SWAD để huấn luyện các giáo viên như một phương pháp được thiết lập tốt trong tài liệu tổng quát hóa miền.
Chúng tôi huấn luyện các mô hình giáo viên sử dụng SWAD trong 5,000 lần lặp (cùng số lượng như trong bài báo SWAD), cũng như các đường cơ sở cho các mô hình học viên, nơi một mô hình của kiến trúc học viên được huấn luyện để dự đoán nhãn cứng độc lập với giáo viên sử dụng SWAD. Theo các quan sát từ [4], chúng tôi tăng số lượng lên 50,000 lần lặp cho tất cả các thí nghiệm chưng cất. Đối với cả SWAD và SMA, chúng tôi sử dụng cùng tần suất xác thực 100 lần lặp để so sánh công bằng.
Đối với SMA và WAKD, chúng tôi cố định lần lặp bắt đầu để trung bình hóa tại 5,000 (10% của thời lượng huấn luyện). Tất cả các mô hình được khởi tạo với trọng số được tiền huấn luyện ImageNet-1k [29] ở đầu quá trình huấn luyện.
Ngoài các học viên chưng cất được trung bình hóa trọng số, chúng tôi trình bày hiệu suất của một mô hình đơn lẻ từ lần lặp có độ chính xác xác thực tốt nhất (được ký hiệu là ERM trong kết quả), hoạt động như một đường cơ sở cho các chiến lược trung bình hóa trọng số.
Tất cả các mô hình được huấn luyện bằng Adam [15] với tốc độ học 5e-5 và kích thước batch 64. Chúng tôi đặt tham số nhiệt độ τ = 5 cho KD. Đối với SWAD, chúng tôi sử dụng các giá trị siêu tham số tiêu chuẩn từ bài báo: Ns = 3, Ne = 6 và r = 1.3.
Chúng tôi sử dụng một tập hợp tiêu chuẩn các tăng cường dữ liệu được mô tả trong [11], được sử dụng rộng rãi trong tài liệu tổng quát hóa miền. Sự khác biệt duy nhất là trong trường hợp chưng cất, chúng tôi sử dụng các cắt xén ngẫu nhiên "kiểu inception" tích cực hơn [33] như được đề xuất trong [4].
Đối với mỗi thí nghiệm, chúng tôi trình bày các giá trị trung bình và độ lệch chuẩn được tính toán trên ba hạt giống ngẫu nhiên. Đối với mỗi hạt giống, chúng tôi chia dữ liệu miền nguồn thành các phân chia train/val 80% và 20% như được mô tả trong [11], sau đó huấn luyện giáo viên và chưng cất nó thành học viên. Để cho phép so sánh công bằng, tất cả các chiến lược trung bình hóa được so sánh trên cùng các quỹ đạo huấn luyện, có nghĩa là đối với mỗi hạt giống (và đối với mỗi lựa chọn miền đích) cả việc huấn luyện giáo viên và chưng cất chỉ được thực hiện một lần. Tổng thể, việc chạy các thí nghiệm được trình bày đã mất 75 ngày GPU trên NVIDIA V100.

--- TRANG 3 ---
PACS OFFICE HOME
MÔ HÌNH A C P S TB. A C P R TB.
RESNET-50 (SWAD) 91.6±0.8 85.1±1.0 98.5±0.2 82.5±0.4 89.4 70.8±0.6 57.5±1.1 81.3±0.3 82.5±0.2 73.0
RESNET-18 (SWAD) 82.9±0.2 76.7±1.9 94.9±0.2 77.5±1.6 83.0 54.2±0.3 50.6±1.0 70.6±0.6 72.2±0.3 61.9
RESNET-18 KD (ERM) 85.8±0.8 81.3±1.3 94.6±0.4 82.8±0.3 85.9 57.8±0.4 54.6±0.5 74.4±0.4 76.2±0.2 65.8
RESNET-18 KD (SWAD) 87.4±0.7 81.9±0.9 95.2±0.1 82.1±0.4 86.7 59.7±0.1 55.1±0.8 74.9±0.4 76.8±0.3 66.6
RESNET-18 KD (SMA) 86.8±0.7 81.6±1.1 95.1±0.1 81.2±0.9 86.2 59.7±0.1 55.0±0.9 74.9±0.4 76.9±0.3 66.6
RESNET-18 KD (CỦA CHÚNG TÔI) 87.3±0.2 81.9±0.2 95.1±0.2 82.1±0.1 86.6 59.8±0.2 55.2±0.9 74.9±0.4 76.9±0.4 66.7
Bảng 1. Chưng cất tri thức với các mô hình ResNet. Giáo viên là ResNet-50 và các học viên là ResNet-18. Thước đo được trình bày là độ chính xác miền đích. ResNet-50 cho thấy hiệu suất tốt hơn ở đây so với bài báo SWAD [8] do việc sử dụng phiên bản mới hơn của trọng số được tiền huấn luyện trong PyTorch [26].

PACS OFFICE HOME
MÔ HÌNH A C P S TB. A C P R TB.
DEIT-SMALL (SWAD) 91.9±0.4 85.1±0.6 98.9±0.1 82.4±1.9 89.6 72.8±0.7 61.6±1.1 81.8±0.8 83.7±0.1 75.0
DEIT-TINY (SWAD) 85.2±1.3 79.0±0.8 96.9±0.2 79.8±0.6 85.2 61.8±0.8 51.6±1.0 74.8±0.5 76.6±0.3 66.2
DEIT-TINY KD (ERM) 85.8±0.4 81.2±1.6 95.5±0.3 80.1±1.3 85.7 63.2±0.5 56.2±0.9 78.1±0.6 79.3±0.5 69.2
DEIT-TINY KD (SWAD) 88.2±0.3 82.2±0.3 97.0±0.2 83.1±0.3 87.6 66.4±0.4 56.6±0.6 79.0±0.5 80.2±0.2 70.5
DEIT-TINY KD (SMA) 88.2±0.4 82.2±0.4 96.8±0.1 82.3±1.2 87.4 65.8±0.1 56.2±0.2 78.9±0.8 80.3±0.1 70.3
DEIT-TINY KD (CỦA CHÚNG TÔI) 88.0±0.2 82.3±0.3 96.9±0.2 83.1±0.4 87.6 66.1±0.4 56.8±0.7 78.9±0.6 80.3±0.1 70.5
Bảng 2. Chưng cất tri thức với các mô hình DeiT. Giáo viên là DeiT-Small và các học viên là DeiT-Tiny. Thước đo được trình bày là độ chính xác miền đích.

4.2. Kết quả
Kết quả của các thí nghiệm được trình bày trong Bảng 1 và Bảng 2. Việc chưng cất không có trung bình hóa trọng số (ERM) vượt trội hơn đường cơ sở của các mạng học viên được huấn luyện độc lập trên cả hai bộ dữ liệu và cả hai kiến trúc, cho thấy rằng khả năng tổng quát hóa ngoài miền của giáo viên được chuyển giao đến học viên ở một mức độ nào đó và KD có thể hoạt động tốt dưới sự dịch chuyển miền. Ngoài ra, việc thêm chiến lược trung bình hóa trọng số của chúng tôi vào KD dẫn đến cải thiện độ chính xác miền đích thêm +0.8pp trung bình trong trường hợp ResNets và +1.6pp trong trường hợp DeiTs.
Khi so sánh với hai chiến lược khác, WAKD cho thấy hiệu suất tương tự với chưng cất với SWAD và vượt trội hơn một chút so với chưng cất với SMA, trong khi đơn giản hơn và không cần tính toán hiệu suất xác thực trong quá trình huấn luyện. Tuy nhiên, trong tất cả các trường hợp vẫn có một khoảng cách đáng kể giữa hiệu suất miền đích của giáo viên và học viên, điều này cho thấy rằng vẫn còn chỗ để cải thiện.

5. Kết luận
Trong bài báo này, chúng tôi đã nghiên cứu môi trường thực hiện chưng cất tri thức dưới sự dịch chuyển miền. Chúng tôi đã chỉ ra rằng các kỹ thuật trung bình hóa trọng số từ tài liệu tổng quát hóa miền, cụ thể là SWAD và SMA, có thể cải thiện hiệu suất của các học viên chưng cất trên các miền chưa thấy.
Chúng tôi đã đề xuất một phương pháp trung bình hóa trọng số đơn giản mới không yêu cầu đánh giá trên tập xác thực trong quá trình huấn luyện và đã chỉ ra tính hiệu quả của nó khi áp dụng cho KD dưới sự dịch chuyển miền. Hiểu rõ hơn về các lý do đằng sau những cải thiện nói trên và mối liên hệ của chúng với các điểm cực tiểu phẳng, cũng như khám phá các mục tiêu chưng cất khác là một số hướng công việc tương lai có thể.

Lời cảm ơn
Các tác giả biết ơn Dmitry Vetrov vì những thảo luận và phản hồi có giá trị. Nghiên cứu này được hỗ trợ một phần thông qua tài nguyên tính toán của các cơ sở HPC tại Đại học HSE.

Tài liệu tham khảo
[1] Isabela Albuquerque, Nikhil Naik, Junnan Li, Nitish Keskar, và Richard Socher. Improving out-of-distribution generalization via multi-task self-supervised pretraining. arXiv preprint arXiv:2003.13525, 2020. 2
[2] Devansh Arpit, Huan Wang, Yingbo Zhou, và Caiming Xiong. Ensemble of averages: Improving model selection and boosting performance in domain generalization. Advances in Neural Information Processing Systems, 35:8265–8277, 2022. 1, 2
[3] Yogesh Balaji, Swami Sankaranarayanan, và Rama Chellappa. Metareg: Towards domain generalization using meta-regularization. Advances in neural information processing systems, 31, 2018. 2

--- TRANG 4 ---
[4] Lucas Beyer, Xiaohua Zhai, Amélie Royer, Larisa Markeeva, Rohan Anil, và Alexander Kolesnikov. Knowledge distillation: A good teacher is patient and consistent. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10925–10934, 2022. 2, 3
[5] Gilles Blanchard, Gyemin Lee, và Clayton D. Scott. Generalizing from several related classification tasks to a new unlabeled sample. In NIPS, 2011. 1, 2
[6] Silvia Bucci, Antonio D'Innocente, Yujun Liao, Fabio Maria Carlucci, Barbara Caputo, và Tatiana Tommasi. Self-supervised learning across domains. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 2
[7] Fabio M Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, và Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2229–2238, 2019. 2
[8] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, và Sungrae Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Information Processing Systems, 34:22405–22418, 2021. 1, 2, 3, 4
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1
[10] Gongfan Fang, Yifan Bao, Jie Song, Xinchao Wang, Donglin Xie, Chengchao Shen, và Mingli Song. Mosaicking to distill: Knowledge distillation from out-of-domain data. Advances in Neural Information Processing Systems, 34:11920–11932, 2021. 2
[11] Ishaan Gulrajani và David Lopez-Paz. In search of lost domain generalization. In International Conference on Learning Representations, 2021. 2, 3
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 1, 3
[13] Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 1, 2
[14] P Izmailov, AG Wilson, D Podoprikhin, D Vetrov, và T Garipov. Averaging weights leads to wider optima and better generalization. In 34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018, pages 876–885, 2018. 1
[15] Diederik P Kingma và Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 3
[16] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, và Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16, pages 491–507. Springer, 2020. 1
[17] Kyungmoon Lee, Sungyeon Kim, và Suha Kwak. Cross-domain ensemble distillation for domain generalization. In European Conference on Computer Vision, pages 1–20. Springer, 2022. 2
[18] Da Li, Yongxin Yang, Yi-Zhe Song, và Timothy M Hospedales. Deeper, broader and artier domain generalization. In Proceedings of the IEEE international conference on computer vision, pages 5542–5550, 2017. 1, 3
[19] Da Li, Yongxin Yang, Yi-Zhe Song, và Timothy M Hospedales. Learning to generalize: Meta-learning for domain generalization. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. 2
[20] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, và Alex C Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5400–5409, 2018. 2
[21] Xuanlin Li, Yunhao Fang, Minghua Liu, Zhan Ling, Zhuowen Tu, và Hao Su. Distilling large vision-language model with out-of-distribution generalizability. arXiv preprint arXiv:2307.03135, 2023. 2
[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012–10022, 2021. 1
[23] Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, và Gianfranco Doretto. Unified deep supervised domain adaptation and generalization. In Proceedings of the IEEE international conference on computer vision, pages 5715–5725, 2017. 2
[24] Krikamol Muandet, David Balduzzi, và Bernhard Schölkopf. Domain generalization via invariant feature representation. In International Conference on Machine Learning, pages 10–18. PMLR, 2013. 2
[25] Utkarsh Ojha, Yuheng Li, và Yong Jae Lee. What knowledge gets distilled in knowledge distillation? arXiv preprint arXiv:2205.16004, 2022. 2
[26] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 4
[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 2
[28] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, và Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. 2
[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211–252, 2015. 3

--- TRANG 5 ---
[30] Yichun Shi, Xiang Yu, Kihyuk Sohn, Manmohan Chandraker, và Anil K Jain. Towards universal representation learning for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6817–6826, 2020. 2
[31] Ankur Singh và Senthilnath Jayavelu. Robust representation learning with self-distillation for domain generalization. ArXiv, abs/2302.06874, 2023. 2
[32] Maryam Sultana, Muzammal Naseer, Muhammad Haris Khan, Salman Khan, và Fahad Shahbaz Khan. Self-distilled vision transformer for domain generalization. In Proceedings of the Asian Conference on Computer Vision, pages 3068–3085, 2022. 2
[33] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, và Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015. 3
[34] Mingxing Tan và Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 6105–6114. PMLR, 2019. 1
[35] Yonglong Tian, Dilip Krishnan, và Phillip Isola. Contrastive representation distillation. In International Conference on Learning Representations, 2019. 2
[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và Hervé Jégou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347–10357. PMLR, 2021. 3
[37] Vladimir Vapnik và Vlamimir Vapnik. Statistical learning theory wiley. New York, 1(624):2, 1998. 2
[38] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, và Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5018–5027, 2017. 1, 3
[39] Riccardo Volpi và Vittorio Murino. Addressing model vulnerability to distributional shifts over image transformation sets. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7980–7989, 2019. 2
[40] Shujun Wang, Lequan Yu, Caizi Li, Chi-Wing Fu, và Pheng-Ann Heng. Learning from extrinsic and intrinsic supervisions for domain generalization. In European Conference on Computer Vision, pages 159–176. Springer, 2020. 2
[41] Yufei Wang, Haoliang Li, Lap-pui Chau, và Alex C Kot. Embracing the dark knowledge: Domain generalization using regularized knowledge distillation. In Proceedings of the 29th ACM International Conference on Multimedia, pages 2595–2604, 2021. 2
[42] Kunran Xu, Lai Rui, Yishi Li, và Lin Gu. Feature normalized knowledge distillation for image classification. In European conference on computer vision, pages 664–680. Springer, 2020. 2
[43] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, và Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. arXiv preprint arXiv:2007.13003, 2020. 2
[44] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, và Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023–6032, 2019. 2
[45] Sergey Zagoruyko và Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In International Conference on Learning Representations, 2016. 2
[46] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, và David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. 2
[47] Linfeng Zhang, Yukang Shi, Zuoqiang Shi, Kaisheng Ma, và Chenglong Bao. Task-oriented feature distillation. Advances in Neural Information Processing Systems, 33:14759–14771, 2020. 2
[48] Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, và Jiajun Liang. Decoupled knowledge distillation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 11953–11962, 2022. 2
[49] Yuyang Zhao, Zhun Zhong, Fengxiang Yang, Zhiming Luo, Yaojin Lin, Shaozi Li, và Nicu Sebe. Learning to generalize unseen domains via memory-based multi-source meta-learning for person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6277–6286, 2021. 2
[50] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, và Chen Change Loy. Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 2
[51] Kaiyang Zhou, Yuanhan Zhang, Yuhang Zang, Jingkang Yang, Chen Change Loy, và Ziwei Liu. On-device domain generalization. arXiv preprint arXiv:2209.07521, 2022. 2
