# 2310.18628.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/knowledge-distillation/2310.18628.pdf
# Kích thước tệp: 911175 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Chưng cất Cá nhân hóa: Trao quyền cho các LLM Mã nguồn mở với Học tập Thích ứng cho Sinh tạo Mã
Hailin Chen∗♣♠, Amrita Saha∗♠, Steven HOI♠, Shafiq Joty♣♠
♣Nanyang Technological University, Singapore
♠Salesforce Research
{hailin001, srjoty}@ntu.edu.sg
{amrita.saha, shoi}@salesforce.com
Tóm tắt
Với sự phát triển của các LLM mã nguồn đóng mạnh mẽ (ChatGPT, GPT-4), có sự quan tâm ngày càng tăng trong việc chưng cất các khả năng của LLM mã nguồn đóng sang các LLM mã nguồn mở nhỏ hơn. Các phương pháp chưng cất trước đây thường nhắc ChatGPT tạo ra một tập hợp các hướng dẫn và câu trả lời, để mô hình học sinh học hỏi. Tuy nhiên, cách tiếp cận chưng cất tiêu chuẩn như vậy bỏ qua những ưu điểm và điều kiện của mô hình học sinh. Lấy cảm hứng từ các nguyên tắc giảng dạy hiện đại, chúng tôi thiết kế một quá trình chưng cất cá nhân hóa, trong đó học sinh cố gắng giải quyết một nhiệm vụ trước, sau đó giáo viên cung cấp một sự cải tiến thích ứng để học sinh cải thiện. Thay vì đưa cho học sinh với kiến thức tiên nghiệm của giáo viên, chưng cất cá nhân hóa cho phép học tập cá nhân hóa cho mô hình học sinh, vì nó chỉ học trên các ví dụ mà nó mắc lỗi và học để cải thiện giải pháp của chính mình. Trong sinh tạo mã, chưng cất cá nhân hóa liên tục vượt trội hơn chưng cất tiêu chuẩn chỉ với một phần ba dữ liệu. Chỉ với 2.5-3K ví dụ cá nhân hóa phát sinh chi phí thu thập dữ liệu 4-6$, chúng tôi tăng CodeGen-mono-16B lên 7% để đạt 36.4% pass@1 và StarCoder lên 12.2% để đạt 45.8% pass@1 trên HumanEval.1

1 Giới thiệu
Gần đây, các mô hình ngôn ngữ lớn mã nguồn đóng mạnh mẽ (LLM) bao gồm ChatGPT, GPT-4 đã trở nên chiếm ưu thế, tích lũy hơn 170 triệu người dùng trong vòng 5 tháng kể từ khi ra mắt. Các LLM mã nguồn đóng như vậy thể hiện hiệu suất mạnh trong nhiều nhiệm vụ, từ cải thiện thành thạo viết lách đến sinh tạo mã. Tuy nhiên, do bản chất mã nguồn đóng của chúng, những lo ngại đã được nêu ra liên quan đến các yếu tố như tính khả dụng của các dịch vụ này, chi phí cao liên quan, lo ngại về đạo đức và an toàn, và những tác động tiềm tàng đối với quyền riêng tư dữ liệu, *Những tác giả này đóng góp như nhau cho công trình này 1Mã của chúng tôi sẽ có sẵn tại https://github.com/salesforce/PersDistilltất cả đều hạn chế việc tích hợp liền mạch của chúng vào các ứng dụng thế giới thực. Trước những lo ngại này, một câu hỏi tự nhiên nảy sinh: Liệu chúng ta có thể chưng cất những khả năng đáng chú ý được thể hiện bởi các LLM mã nguồn đóng vào các LLM mã nguồn mở nhỏ hơn không?

Các nhà nghiên cứu đã khám phá ý tưởng chưng cất như vậy (Taori et al., 2023; Wang et al., 2022; Xu et al., 2023b), bằng cách truy vấn ChatGPT để tạo ra các cặp hướng dẫn nhiệm vụ và giải pháp, và sử dụng dữ liệu được thu thập để tinh chỉnh một mô hình học sinh. Tuy nhiên, cách tiếp cận chưng cất tiêu chuẩn này khớp các mô hình học sinh khác nhau với cùng một phân phối dữ liệu (kiến thức tiên nghiệm của giáo viên), bất chấp khả năng và năng lực độc đáo của chúng. Trong lĩnh vực giáo dục, học tập cá nhân hóa cung cấp trải nghiệm học tập tùy chỉnh thích ứng với tiến trình học tập và năng lực của học sinh, đã được chứng minh là rất hiệu quả và được áp dụng rộng rãi (Roberts-Mahoney et al., 2016; Shemshack và Spector, 2020). Lấy cảm hứng từ phát hiện như vậy, chúng tôi đưa ra giả thuyết rằng học tập cá nhân hóa cũng có lợi cho việc chưng cất mô hình.

Trong công trình này, chúng tôi đề xuất chưng cất cá nhân hóa và đánh giá thực nghiệm hiệu quả của nó trong lĩnh vực sinh tạo mã. Tương tự như chưng cất tiêu chuẩn, trước tiên chúng tôi sử dụng ChatGPT để tạo ra các hướng dẫn nhiệm vụ kèm theo các trường hợp kiểm thử đơn vị. Sau đó, chúng tôi tuân theo ba bước cho chưng cất cá nhân hóa như được hiển thị trong Hình 1. Đầu tiên, chúng tôi để mô hình học sinh cố gắng giải quyết nhiệm vụ. Sau đó, chúng tôi đánh giá nỗ lực của học sinh với các trường hợp kiểm thử đơn vị và nhận phản hồi thực thi. Nếu phản hồi thực thi chứa lỗi, trong bước cuối cùng chúng tôi nhắc mô hình giáo viên (ChatGPT) cải tiến nỗ lực của học sinh. Quá trình thu thập dữ liệu như vậy làm cho trải nghiệm học tập vừa tương tác — vì học sinh tham gia để thực hiện các nỗ lực, vừa cá nhân hóa — cả đầu vào (nhiệm vụ) và đầu ra (dữ liệu cải tiến) đều được tùy chỉnh cho học sinh. Về cơ bản, dữ liệu được gắn nhãn cá nhân hóa giúp học sinh cải tiến chính sách của chính mình, thay vì áp dụng một kiến thức tiên nghiệm mới của giáo viên.

Với dữ liệu mã cá nhân hóa làm đầu ra mục tiêu, chúng tôi xây dựng ba biến thể của dữ liệu tinh chỉnh (i) Dữ liệu PERsD định dạng nó như một nhiệm vụ sinh tạo mã từ văn bản điển hình, (ii) PERsD-refine coi nó như một nhiệm vụ cải tiến mã, được đưa ra một hướng dẫn nhiệm vụ, mã không chính xác và phản hồi lỗi thực thi (ii) PERsD-combine chỉ đơn giản kết hợp dữ liệu tinh chỉnh PERsD và PERsD-refine, tức là các nhiệm vụ sinh tạo mã và cải tiến.

Chúng tôi thu thập 10K ví dụ chưng cất tiêu chuẩn và khoảng 2.5-3K ví dụ cá nhân hóa để tiền huấn luyện. Thông qua đánh giá zero-shot trên HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021), chúng tôi quan sát thấy tất cả các biến thể PERsD liên tục vượt trội hơn các đối tác của chúng sử dụng chưng cất tiêu chuẩn. Kết quả thuyết phục này mạnh mẽ xác nhận giả thuyết của chúng tôi về những lợi thế của chưng cất cá nhân hóa. Các nghiên cứu loại bỏ thêm củng cố giả thuyết của chúng tôi, khám phá những tính chất thú vị như lợi ích của chưng cất cá nhân hóa nhiều vòng và khả năng của các mô hình của chúng tôi để tận dụng phản hồi thực thi cho việc tự điều chỉnh. Đáng chú ý, chưng cất cá nhân hóa tăng cường đáng kể mô hình tiền huấn luyện mã nguồn mở tiên tiến StarCoder (Li et al., 2023a) — lên 12.2% để đạt 45.8 trong pass@1 và 82.3 trong pass@100 trên HumanEval.

2 Công trình Liên quan
2.1 Chưng cất từ ChatGPT
Các công trình trước đã khám phá chưng cất từ ChatGPT bao gồm Alpaca(Taori et al., 2023), Vicuna(Chiang et al., 2023) và Baize(Xu et al., 2023b). Tuy nhiên, những công trình này đều có thể được coi là chưng cất tiêu chuẩn vì chúng không xem xét điều kiện và năng lực của mô hình học sinh. Wiz-

Phương pháp | Cá nhân hóa | Tương tác | Liên quan đến mã
--- | --- | --- | ---
Alpaca | ✗ | ✗ | ✗
Vicuna | ✗ | ✗ | ✗
Baize | ✗ | ✗ | ✗
WizardLM | ✗ | ✗ | ✗
WizardCoder | ✗ | ✗ | ✓
Lion | Đầu vào | ✓ | ✗
PERsD | Đầu vào + Đầu ra | ✓ | ✓

Bảng 1: Công trình liên quan về chưng cất từ ChatGPT

ardLM(Xu et al., 2023a) và WizardCoder(Luo et al., 2023) lặp đi lặp lại nhắc mô hình giáo viên tạo ra các hướng dẫn phức tạp hơn. Cách tiếp cận của chúng có thể được xem như một tiến bộ trực giao có thể được kết hợp với chưng cất cá nhân hóa.

Lion (Jiang et al., 2023) đề xuất kết hợp câu trả lời của mô hình học sinh và lấy mẫu nhiều nhiệm vụ khó hơn mà học sinh không thể giải quyết. Do đó, Lion có thể được coi là chưng cất cá nhân hóa đầu vào vì chỉ các nhiệm vụ đầu vào được tùy chỉnh cho các học sinh khác nhau. Cách tiếp cận của chúng tôi khác biệt vì chúng tôi cung cấp tùy chỉnh cho cả đầu vào và đầu ra, và chúng tôi thực nghiệm cho thấy rằng cá nhân hóa nhãn là có lợi quan trọng.

2.2 Sinh tạo Mã với Phản hồi
Gần đây, đã có một lượng nghiên cứu ngày càng tăng về khám phá cách sử dụng phản hồi cho việc sinh tạo mã lặp đi lặp lại và cải thiện thông qua cải tiến mã. Self-refine(Madaan et al., 2023), Self-debug(Chen et al., 2023b) và Reflexion (Shinn et al., 2023) là các phương pháp thời gian suy luận sử dụng các LLM mã nguồn đóng mạnh mẽ để tạo ra mã tốt hơn từ phản hồi nội bộ hoặc bên ngoài. Mặc dù chúng cho thấy hiệu suất cao,

--- TRANG 2 ---
Hình 1: Tổng quan về khung của chúng tôi. Trái: chưng cất tiêu chuẩn .1Giáo viên tạo ra câu trả lời tiêu chuẩn cho một vấn đề nhất định để học sinh học hỏi Phải: chưng cất cá nhân hóa .1Học sinh trước tiên tạo ra nỗ lực của riêng mình để giải quyết nhiệm vụ. 2Thực thi đánh giá mã được tạo ra với các trường hợp kiểm thử đơn vị. 3Giáo viên cung cấp cải tiến thích ứng dựa trên nỗ lực của học sinh và phản hồi thực thi của nó.

out, chúng tôi xây dựng ba biến thể của dữ liệu tinh chỉnh (i) Dữ liệu PERsD định dạng nó như một nhiệm vụ sinh tạo mã từ văn bản điển hình, (ii) PERsD-refine coi nó như một nhiệm vụ cải tiến mã, được đưa ra một hướng dẫn nhiệm vụ, mã không chính xác và phản hồi lỗi thực thi (ii) PERsD-combine chỉ đơn giản kết hợp dữ liệu tinh chỉnh PERsD và PERsD-refine, tức là các nhiệm vụ sinh tạo mã và cải tiến.

Chúng tôi thu thập 10K ví dụ chưng cất tiêu chuẩn và khoảng 2.5-3K ví dụ cá nhân hóa để tiền huấn luyện. Thông qua đánh giá zero-shot trên HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021), chúng tôi quan sát thấy tất cả các biến thể PERsD liên tục vượt trội hơn các đối tác của chúng sử dụng chưng cất tiêu chuẩn. Kết quả thuyết phục này mạnh mẽ xác nhận giả thuyết của chúng tôi về những lợi thế của chưng cất cá nhân hóa. Các nghiên cứu loại bỏ thêm củng cố giả thuyết của chúng tôi, khám phá những tính chất thú vị như lợi ích của chưng cất cá nhân hóa nhiều vòng và khả năng của các mô hình của chúng tôi để tận dụng phản hồi thực thi cho việc tự điều chỉnh. Đáng chú ý, chưng cất cá nhân hóa tăng cường đáng kể mô hình tiền huấn luyện mã nguồn mở tiên tiến StarCoder (Li et al., 2023a) — lên 12.2% để đạt 45.8 trong pass@1 và 82.3 trong pass@100 trên HumanEval.

2 Công trình Liên quan
2.1 Chưng cất từ ChatGPT
Các công trình trước đã khám phá chưng cất từ ChatGPT bao gồm Alpaca(Taori et al., 2023), Vi-cuna(Chiang et al., 2023) và Baize(Xu et al., 2023b). Tuy nhiên, những công trình này đều có thể được coi là chưng cất tiêu chuẩn vì chúng không xem xét điều kiện và năng lực của mô hình học sinh. Wiz-

--- TRANG 3 ---
Phương pháp | Huấn luyện | Suy luận
--- | --- | ---
 | Nguồn dữ liệu đơn | Cá nhân hóa | với phản hồi thực thi | không có ChatGPT
Mô hình |  |  |  | 
Self-refine | ✓ | Không huấn luyện | ✗ | ✗ | ✗
Self-debug | ✓ | Không huấn luyện | ✗ | ✓ | ✗
Reflexion | ✓ | Không huấn luyện | ✗ | ✓ | ✗
Self-edit | ✗ | GT tiêu chuẩn | ✗ | ✓ | ✓
Self-correct | ✗ | Tự khám phá | ✓ | ✓ | ✓
ILF | ✗ | Gán nhãn bởi con người | ✓ | ✓ | ✓
PERsD-refine | ✓ | ChatGPT | ✓ | ✓ | ✓

Bảng 2: Công trình liên quan về Sinh tạo Mã với phản hồi

các phương pháp này bị hạn chế vì chúng yêu cầu quyền truy cập vào các LLM mã nguồn đóng.

Self-edit (Zhang et al., 2023) huấn luyện một trình soạn thảo mã riêng biệt để sửa chữa mã được tạo ra từ một LLM cơ sở. Nhãn huấn luyện là từ câu trả lời vàng gốc, do đó không được cá nhân hóa nhãn. Tương tự, Self-correct (Welleck et al., 2022) huấn luyện một mô hình hiệu chỉnh riêng biệt để sửa chữa đầu ra từ một mô hình sinh cố định. Tuy nhiên, nhãn huấn luyện là từ tự khám phá của mô hình hiệu chỉnh: lấy mẫu nhiều cải tiến và chọn cái dẫn đến phần thưởng cao hơn. Cuối cùng, ILF (Chen et al., 2023a) thu thập dữ liệu cải tiến mã được chú thích bởi con người để huấn luyện một mô hình cải tiến riêng biệt trên đó. Mô hình cải tiến được sử dụng để tạo ra dữ liệu văn bản-thành-mã cho việc tinh chỉnh LLM sinh tạo mã. Không giống như ILF, cách tiếp cận của chúng tôi có khả năng mở rộng hơn vì chúng tôi không yêu cầu chú thích của con người và dữ liệu cá nhân hóa của chúng tôi chứng minh hiệu quả hơn đáng kể so với ILF như chúng tôi khảo sát thực nghiệm trong §5.

2.3 Học tăng cường từ Phản hồi (Con người)
Sau khi ra mắt ChatGPT, việc điều chỉnh các LLM theo sở thích của con người đã thu hút sự chú ý to lớn từ các cộng đồng nghiên cứu. Như một trong những cách tiếp cận có ảnh hưởng nhất trong hướng này, học tăng cường từ phản hồi của con người (RLHF) (Ouyang et al., 2022; Li et al., 2023b), áp dụng một khung actor-critic, trong đó mô hình học sinh được tối ưu hóa để tạo ra các phản hồi nhận được phần thưởng cao hơn từ mô hình phê bình. Trong InstructGPT (Ouyang et al., 2022), phê bình (mô hình phần thưởng) được huấn luyện từ chú thích của con người. Tối ưu hóa Sở thích Trực tiếp (DPO) (Rafailov et al., 2023) bỏ qua nhu cầu huấn luyện một mô hình phần thưởng, bằng cách sử dụng một LLM tham chiếu và các quỹ đạo ngoại tuyến để ước tính phần thưởng. Chain-of-Hindsight (Liu et al., 2023) chuyển đổi các chú thích sở thích của con người thành phản hồi ngôn ngữ tự nhiên đơn giản, và do đó biến tối ưu hóa RL thành sinh tạo có điều kiện. Trong các phương pháp trên, giả định là không có mục tiêu sự thật cơ bản và do đó chúng cố gắng cải thiện LLM dựa trên đánh giá (phê bình) của nhiều đầu ra được tạo ra. Tuy nhiên, việc huấn luyện kiểu RL như vậy sẽ kém hiệu quả và hiệu quả hơn so với tinh chỉnh có giám sát, đặc biệt đối với các nhiệm vụ thách thức với phần thưởng thưa thớt – ví dụ: giải quyết các câu đố toán học hoặc các nhiệm vụ mã hóa. Không giống như các phương pháp này, cách tiếp cận của chúng tôi có thể thu được các đầu ra "sự thật cơ bản" từ một giáo viên cá nhân hóa, do đó tinh chỉnh có giám sát có thể được áp dụng làm cho việc học tập hiệu quả và hiệu quả, ngay cả đối với các nhiệm vụ thách thức như giải quyết các vấn đề mã hóa.

3 Phương pháp
3.1 Chưng cất Tiêu chuẩn
Giả sử một tập dữ liệu các nhiệm vụ sinh tạo mã D={(t, u)} trong đó mỗi vấn đề (hoặc nhiệm vụ) bao gồm một hướng dẫn nhiệm vụ t và một bộ sưu tập kiểm thử đơn vị u. Trong quá trình huấn luyện, chúng ta có quyền truy cập vào một mô hình giáo viên πϕ và một mô hình học sinh πθ. Mục tiêu là chưng cất cách giáo viên giải quyết các nhiệm vụ sinh tạo mã cho mô hình học sinh, trong bối cảnh của D. Đối với mỗi nhiệm vụ (t, u), trước tiên chúng ta truy vấn giáo viên πϕ(t) với hướng dẫn nhiệm vụ, để có được một đoạn mã được tạo ra trực tiếp cϕ. Sau đó, chúng ta thực thi mã được tạo ra cϕ với các trường hợp kiểm thử đơn vị u và nhận phản hồi thực thi f←EXEC (cϕ, u), trong đó hàm EXEC trả về passed nếu mã vượt qua tất cả các kiểm thử đơn vị, nếu không nó trả về một thông báo lỗi từ trình thực thi. Bằng cách lọc ra các nhiệm vụ mà cϕ không vượt qua tất cả các kiểm thử đơn vị (tức là, f̸=passed), chúng ta có được một tập dữ liệu sạch mới DSTAND={(t, u, c)}, trong đó mỗi nhiệm vụ bao gồm một hướng dẫn nhiệm vụ t, một bộ kiểm thử đơn vị u và một mã giải pháp chính xác c.

Sau đó chúng ta tinh chỉnh mô hình học sinh πθ trên {(u, c)} ∼ DSTAND, trong đó đầu vào là hướng dẫn nhiệm vụ u và đầu ra là mã giải pháp tương ứng c. Chúng ta đặt tên cho cách tiếp cận này là STAND.

3.2 Chưng cất Cá nhân hóa
Cách tiếp cận STAND chỉ đơn giản lấy mẫu các ví dụ huấn luyện (hướng dẫn và nhãn) từ phân phối tiên nghiệm của mô hình giáo viên và đưa nó cho học sinh mà không xem xét điều kiện của mô hình học sinh. Lấy cảm hứng từ các nguyên tắc giáo dục hiện đại ủng hộ trải nghiệm học tập tương tác và cá nhân hóa, chúng tôi đề xuất chưng cất cá nhân hóa: thích ứng tài liệu giảng dạy với kiến thức và năng lực hiện tại của học sinh.

--- TRANG 4 ---
Thuật toán 1 chưng cất cá nhân hóa cho sinh tạo mã (PERsD-combined).
1:Đầu vào: Tập dữ liệu DSTAND, LLM học sinh πθ, trình thực thi kiểm thử đơn vị EXEC, mẫu cải tiến Trefine, LLM giáo viên πϕ
2:Drefine←{}▷dữ liệu cải tiến để tinh chỉnh
3:Dcode←{}▷dữ liệu sinh tạo trực tiếp
4:for(t, u, c)∈DSTANDdo
5: cθ←πθ(t) ▷học sinh tạo ra cθ
6: f←EXEC(cθ, u) ▷phản hồi thực thi cho cθ
7: iff̸=passed then
8: // cải tiến cá nhân hóa từ giáo viên
9: crefine←πϕ(t, cθ, f)
10: // tạo hướng dẫn nhiệm vụ cải tiến
11: trefine←Trefine(t, cθ, f)
12: ifEXEC(crefine, u) =passed then
13: Drefine.insert ({trefine, crefine})
14: Dcode.insert ({t, c})
15: end if
16: end if
17: end for
18:πθ∗←FINETUNE (πθ,Drefine +Dcode)

và năng lực. Chúng tôi đề xuất ba biến thể:

PERSD-combined Thuật toán 1 hiển thị các bước chi tiết cho PERSD-combined. Phương pháp này lấy tập dữ liệu chưng cất tiêu chuẩn DSTAND từ §3.1 và trước tiên để mô hình học sinh tạo ra các giải pháp cho mỗi nhiệm vụ. Sau đó nó lọc ra các nhiệm vụ mà mô hình học sinh đã có thể giải quyết chính xác. Đối với các nhiệm vụ còn lại, nó thu được cải tiến cá nhân hóa của giáo viên có điều kiện trên nỗ lực của học sinh và phản hồi lỗi thực thi của nó, và chỉ giữ các nhiệm vụ mà cải tiến của giáo viên là hợp lệ (tức là, vượt qua tất cả các trường hợp kiểm thử đơn vị). Hình 1 minh họa ba bước này.

Đối với tập nhiệm vụ cuối cùng này, chúng tôi tạo ra hai tập dữ liệu: i)Dcode chứa hướng dẫn nhiệm vụ làm đầu vào và câu trả lời trực tiếp của giáo viên làm đầu ra, và ii) Drefine chứa hướng dẫn nhiệm vụ cải tiến làm đầu vào và câu trả lời cải tiến cá nhân hóa làm đầu ra. Hướng dẫn nhiệm vụ cải tiến (dòng 9 trong Thuật toán 1) được tạo ra bằng cách nối hướng dẫn nhiệm vụ t, nỗ lực của học sinh cθ và phản hồi thực thi f của nó với một mẫu cải tiến Trefine (Chi tiết thêm trong Phụ lục C). Hướng dẫn cải tiến như vậy biến sinh tạo mã tiêu chuẩn thành một nhiệm vụ cải tiến mã, dạy học sinh cách cải tiến giải pháp của chính mình. PERSD-combined sau đó tinh chỉnh mô hình học sinh trên Drefine kết hợp với Dcode.

PERSD-refine Tương tự như PERSD-combined, biến thể này tuân theo dòng 1-15 của Thuật toán 1 để thu thập dữ liệu cải tiến Drefine. Tuy nhiên, nó khác với mô hình trên vì nó chỉ sử dụng Drefine để tinh chỉnh mô hình học sinh.

PERsD Biến thể này lấy dữ liệu huấn luyện Drefine từ PERsD-refine và thay thế đầu vào của mỗi điểm dữ liệu từ nhắc cải tiến mã sang hướng dẫn nhiệm vụ gốc. Do đó nó huấn luyện mô hình học sinh với các nhãn cá nhân hóa trên sinh tạo mã.

Để minh họa sự khác biệt giữa cải tiến cá nhân hóa và giải pháp trực tiếp của giáo viên, chúng tôi hiển thị một ví dụ thực trong Hình 2. Phần trên hiển thị cải tiến cá nhân hóa cho nhiệm vụ đã cho, trong khi phần dưới hiển thị sinh tạo trực tiếp của giáo viên cho cùng nhiệm vụ. Lưu ý cách sinh tạo trực tiếp của giáo viên khác biệt đáng kể so với nỗ lực của mô hình học sinh, trong khi cải tiến của giáo viên tuân theo nỗ lực của học sinh và cải thiện dựa trên đó. Chúng tôi đưa ra giả thuyết rằng cải tiến thích ứng như vậy trong đó giáo viên điều chỉnh theo sinh tạo của học sinh, giúp học sinh học tập hiệu quả và hiệu quả hơn, tương tự như cách con người hưởng lợi từ học tập cá nhân hóa.

Hình 2: Ví dụ: (Trên) Cải tiến cá nhân hóa từ nỗ lực của học sinh và phản hồi thực thi; (Dưới) Giải pháp trực tiếp được tạo ra bởi giáo viên có điều kiện trên nhiệm vụ.

3.3 Suy luận Lặp lại
Gọi Dtest ={(t, u)} là tập kiểm thử của chúng ta cho suy luận, trong đó mỗi điểm dữ liệu (t, u) bao gồm một hướng dẫn nhiệm vụ t và một bộ các trường hợp kiểm thử đơn vị ẩn u. Chúng ta cũng giả sử rằng hướng dẫn nhiệm vụ chứa một số trường hợp kiểm thử đơn vị đơn giản trong doc-string của nó (như thường thấy trong các hướng dẫn sinh tạo mã), mà chúng ta có thể trích xuất và định dạng bằng cách sử dụng heuristic dựa trên quy tắc để có được một bộ các trường hợp kiểm thử đơn vị đã thấy useen(Chi tiết thêm trong Phụ lục A).

--- TRANG 5 ---
Đối với suy luận một bước, chúng ta sử dụng cách tiếp cận tiêu chuẩn để đánh giá pass@k. Cụ thể, đối với mỗi nhiệm vụ t, chúng ta truy vấn mô hình n lần với hướng dẫn nhiệm vụ: ciθ←πθ(t) cho i= 1. . . n. Sau đó, theo (Chen et al., 2021), chúng ta ước tính pass@k từ số lần thử vượt qua các trường hợp kiểm thử đơn vị ẩn: EXEC (ciθ, u) =passed.

Suy luận nhiều bước Nếu mô hình πθ đã được huấn luyện để sửa chữa, theo cách tiếp cận của chúng ta trong PERsD-refine hoặc PERsD-combine, và nếu các kiểm thử đơn vị có sẵn trong quá trình suy luận, chúng ta có thể thực hiện suy luận 2 bước: đối với mỗi nỗ lực được tạo ra ciθ trong 1 bước, trước tiên chúng ta nhận phản hồi thực thi fiseen←EXEC (ciθ, useen). Nếu fiseen =passed, chúng ta sử dụng lại nỗ lực gốc làm nỗ lực 2 bước. Nếu không, chúng ta tạo ra một hướng dẫn cải tiến ti←Trefine(t, ciθ, fiseen) theo cách tiếp cận trong PERsD-refine hoặc PERsD-combined, và truy vấn cùng mô hình với hướng dẫn cải tiến cho nỗ lực 2 bước: ciθ,2-step←πθ(ti). Sau đó chúng ta tính pass@k trên các sinh tạo 2 bước tương tự như suy luận 1 bước.

4 Thiết lập Thực nghiệm
4.1 Đường cơ sở
Đường cơ sở đầu tiên là STAND, cách tiếp cận chưng cất tiêu chuẩn được đề cập trong §3.1.

Để đo lường hiệu quả của các nhãn cá nhân hóa một cách định lượng, chúng tôi cũng so sánh với các đường cơ sở chưng cất cá nhân hóa đầu vào, trong đó chỉ các nhiệm vụ đầu vào được chọn theo cách tùy chỉnh cho khả năng của học sinh. Tuy nhiên, các nhãn đầu ra không được cá nhân hóa, vì chúng được lấy từ sinh tạo trực tiếp của giáo viên c thay vì cải tiến cá nhân hóa crefine. Chúng tôi bắt đầu với Dcode từ PERsD-combined và có ba biến thể:

INPD Chúng tôi tinh chỉnh mô hình học sinh πθ trên {(t, c)} ∼ Dcode, trong đó đầu vào là một hướng dẫn nhiệm vụ và đầu ra là một giải pháp mã. Biến thể này được tùy chỉnh hơn so với STAND vì nó lọc ra các nhiệm vụ mà học sinh đã có thể giải quyết chính xác.

INPD-refine Tương tự như PERsD-refine, InpD-refine huấn luyện mô hình học sinh để sửa chữa nỗ lực sai của nó. Sự khác biệt là trong InpD-refine, mã được cải tiến là từ giải pháp trực tiếp của giáo viên c, thay vì cải tiến cá nhân hóa crefine.

INPD-combined Tương tự như PERsD-combined, InpD-combined huấn luyện học sinh về việc sửa chữa câu trả lời của nó cũng như giải quyết trực tiếp nhiệm vụ. Sự khác biệt là trong InpD-combined, các nhãn cho cả cải tiến mã và sinh tạo mã đều được lấy từ giải pháp trực tiếp của giáo viên c.

4.2 Xây dựng Dữ liệu Tiền huấn luyện
Để xây dựng dữ liệu tiền huấn luyện của chúng tôi, chúng tôi áp dụng quy trình thu thập dữ liệu trong code-alpaca(Chaudhary, 2023) và sử dụng một tập hợp 374 nhiệm vụ hạt giống từ MBPP (task-ids 601-974) như nhắc trong ngữ cảnh để truy vấn ChatGPT cho các nhiệm vụ sinh tạo mã mới. Tập hạt giống này tăng khả năng ChatGPT tạo ra mã python.

Thông qua quy trình này, chúng tôi thu được một kho dữ liệu 20K nhiệm vụ sinh tạo mã từ ChatGPT, mỗi nhiệm vụ bao gồm một hướng dẫn nhiệm vụ và mã được tạo ra tương ứng, thường là một hàm python duy nhất. Tiếp theo, chúng tôi hiển thị mỗi instance được tạo ra cho ChatGPT một lần nữa và nhắc nó tạo ra 5 đầu vào trường hợp kiểm thử độc đáo (tức là các giá trị đối số đầu vào) cho hàm python. Sau đó chúng tôi phân tích và định dạng đầu vào trường hợp kiểm thử được tạo ra và thực thi mã được tạo ra trên đó để có được một đầu ra. Do đó, trong số 20K, đối với 14880 instance chúng tôi có thể tạo ra và phân tích thành công 5 đầu vào trường hợp kiểm thử đơn vị và đối với 10172 instance chúng tôi có thể thực thi thành công mã được tạo ra và có được đầu ra trên tất cả 5 đầu vào. Kho dữ liệu cuối cùng này gồm 10K nhiệm vụ sinh tạo mã, mỗi nhiệm vụ bao gồm một hướng dẫn nhiệm vụ và mã được tạo ra tương ứng cùng với 5 đầu vào và đầu ra kiểm thử đơn vị tạo thành tập dữ liệu chưng cất tiêu chuẩn DSTAND của chúng tôi.

Để thu thập dữ liệu chưng cất cá nhân hóa, chúng tôi tuân theo §3.2 để trước tiên yêu cầu mô hình học sinh tạo ra 1 mã đầu ra cho mỗi nhiệm vụ, đặt nhiệt độ lấy mẫu là 0.3. Sau đó chúng tôi đánh giá nỗ lực của học sinh và chỉ giữ các nhiệm vụ với các sinh tạo sai (tức là những cái thất bại trong bất kỳ trường hợp kiểm thử đơn vị nào). Chúng tôi sử dụng điều này để truy vấn ChatGPT cho các cải tiến cá nhân hóa và chỉ giữ lại các cải tiến hợp lệ vượt qua tất cả các kiểm thử đơn vị. Nhắc của chúng tôi cho ChatGPT chứa hướng dẫn nhiệm vụ gốc và mã từ DSTAND cùng với mã được tạo ra bởi mô hình học sinh và phản hồi thực thi (lỗi biên dịch hoặc thất bại kiểm thử đơn vị). Hướng dẫn của chúng tôi cho ChatGPT là tạo ra một giải pháp chính xác sửa chữa các lỗi và gần nhất về mặt ngữ nghĩa với mã của học sinh (Chi tiết thêm trong Phụ lục B). Bảng 3 hiển thị thống kê của quy trình xây dựng dữ liệu cá nhân hóa.

--- TRANG 6 ---
Mô hình học sinh | # Nỗ lực sai bởi học sinh | # Nhiệm vụ cá nhân hóa được xác thực | Chi phí dữ liệu
--- | --- | --- | ---
CodeGen-mono-6B (Nijkamp et al., 2023) | 6.5K | 3.25K | 5.5$
CodeGen-mono-6B (vòng2) | 4K | 1.4K | 4.4$
CodeGen-mono-16B | 6.2K | 2.8K | 6.5$
StarCoder (Li et al., 2023a) | 4.3K | 2.5K | 4.3$

Bảng 3: Thống kê Xây dựng Dữ liệu Cá nhân hóa

4.3 Đánh giá Mô hình
Chúng tôi đánh giá các mô hình của mình trên hai tập dữ liệu: HumanEval(Chen et al., 2021), chứa 164 vấn đề Python, và tập con MBPP(Austin et al., 2021) được làm sạch không có sự trùng lặp với các nhiệm vụ hạt giống MBPP của chúng tôi để thu thập dữ liệu tiền huấn luyện. Điều này tương ứng với các phần test+validation+prompt của MBPP-sanitized và bao gồm 306 vấn đề Python. Chúng tôi sử dụng lấy mẫu nucleus với nhiệt độ 0.2 để tạo ra 20 ứng viên cho mỗi nhiệm vụ để ước tính pass@1, và với nhiệt độ 0.8, 100 ứng viên cho mỗi nhiệm vụ để ước tính pass@5/10/20/50/100.

Đối với suy luận nhiều bước, trước tiên chúng tôi trích xuất các trường hợp kiểm thử đơn vị "đã thấy" từ doc-string của hướng dẫn nhiệm vụ (Chi tiết thêm trong Phụ lục A). Tiếp theo, chúng tôi tạo ra các mẫu đầu ra theo kiểu sinh tạo mã thông thường tạo thành tập hợp các sinh tạo 1 bước cho mỗi instance. Mỗi sinh tạo ứng viên này sau đó được thực thi trên các trường hợp kiểm thử đơn vị "đã thấy" được trích xuất để có được một mã được cải tiến, do đó tạo thành tập hợp các sinh tạo 2 bước.

4.4 Thiết lập Tiền huấn luyện
Đối với tất cả các thí nghiệm với backbone CodeGen-mono-6B, chúng tôi sử dụng kích thước batch hiệu quả là 1024 và tiền huấn luyện trong 20 epoch. Đối với backbone là CodeGen-mono-16B, chúng tôi sử dụng kích thước batch hiệu quả là 1024 và tiền huấn luyện trong 3 epoch, vì việc huấn luyện hội tụ nhanh hơn nhiều so với CodeGen-mono-6B. Đối với PERsD-combine với mô hình StarCoder, chúng tôi sử dụng kích thước batch hiệu quả là 1024 và tiền huấn luyện trong 8 epoch, điều này dẫn đến tổn thất huấn luyện tương tự như CodeGen-mono-16B. Chúng tôi triển khai bằng cách sử dụng HuggingFace transformers(Wolf et al., 2020) và DeepSpeed Zero (Rajbhandari et al., 2020). Tất cả các thí nghiệm được thực hiện trên một cụm 8 GPU A100-40GB.

5 Kết quả Thực nghiệm
5.1 Kết quả Chính
Chúng tôi kiểm tra thực nghiệm giả thuyết rằng chưng cất cá nhân hóa giúp mô hình học sinh học tập hiệu quả hơn, bằng cách so sánh các mô hình PERsD với các phương pháp chưng cất đường cơ sở (InpD, StanD) trong Bảng 4.

Dữ liệu được gắn nhãn cá nhân hóa nói chung tốt hơn dữ liệu tiêu chuẩn So sánh PERsD-combine với InpD-combine, chúng tôi thấy PERsD-combine vượt trội hơn InpD-combine trong tất cả các thiết lập, thường với một biên độ đáng kể (hai backbone, hai tập dữ liệu, hai bước suy luận, 4 chỉ số pass@k). Quan sát tương tự đúng khi so sánh PERsD-refine với InpD-refine (ngoại trừ 2/32 thiết lập), và PERsD với InpD. Do đó, chúng tôi kết luận rằng các biến thể PERsD nói chung tốt hơn đáng kể so với các đối tác InpD của chúng, cung cấp bằng chứng mạnh mẽ rằng các nhãn cá nhân hóa hiệu quả hơn cho mô hình học sinh học hỏi so với các nhãn tiêu chuẩn.

PERsD vượt trội hơn StanD với ít hơn một phần ba dữ liệu của nó Chúng tôi quan sát thấy PERsD vượt trội hơn StanD cho mọi pass@k trên cả backbone CodeGen-mono 16B và 6B trên cả HumanEval và MBPP, mặc dù StanD có 10K dữ liệu và PERsD chỉ có 3.3K và 2.8K ví dụ cho CodeGen-mono-6B và 16B. Ngoại lệ duy nhất là trong thiết lập CodeGen-mono-16B, MBPP, pass@1, trong đó StanD vượt qua PERsD 1.2 điểm. Vì dữ liệu tiền huấn luyện của chúng tôi được xây dựng từ các nhiệm vụ hạt giống được lấy từ MBPP, chúng tôi đưa ra giả thuyết rằng StanD có thể hưởng lợi từ lợi thế không công bằng do có gấp ba lần dữ liệu hơn, làm cho nó dễ bị rò rỉ dữ liệu hơn. Chúng tôi xác minh giả thuyết như vậy thêm trong §5.2. Tóm lại, với PERsD vượt trội hơn StanD trong 15 trên 16 thiết lập trong khi có ít hơn một phần ba dữ liệu, rõ ràng là dữ liệu được gắn nhãn cá nhân hóa làm cho việc học tập hiệu quả hơn.

Suy luận nhiều bước liên tục cải thiện chất lượng câu trả lời Đối với các mô hình PERsD-refine và PERsD-combine, chúng tôi thấy rằng suy luận 2 bước liên tục cải thiện hiệu suất trên HumanEval và MBPP. Điều này cho thấy các mô hình học thành công cách sửa chữa giải pháp của nó dựa trên phản hồi lỗi thực thi. Lưu ý rằng InpD-refine cho độ chính xác kém hơn với suy luận 2 bước trên HumanEval pass@10/20, củng cố lợi thế của dữ liệu được gắn nhãn cá nhân hóa so với dữ liệu được gắn nhãn tiêu chuẩn.

5.2 Phân tích trùng lặp Train-Test
Như quan sát trong Bảng 4, các biến thể PersD có các cải thiện trung bình cao hơn so với các đối tác InpD của chúng, trên HumanEvan so với trên MBPP. Để tìm hiểu sâu hơn, chúng tôi tiến hành phân tích trùng lặp dữ liệu. Đối với mỗi nhiệm vụ kiểm thử, chúng tôi trích xuất nhiệm vụ huấn luyện tương tự nhất và sử dụng GPT-3.5-turbo để chấm điểm tương tự ngữ nghĩa của chúng, với 0 cho biết không có mối quan hệ và 1

--- TRANG 7 ---
(a) Backbone là CodeGen-mono-6B
Phương pháp | #Dữ liệu | Pass@1 | Pass@5 | Pass@10 | Pass@20
--- | --- | --- | --- | --- | ---
 |  | bước=1 | bước=2 | bước=1 | bước=2 | bước=1 | bước=2 | bước=1 | bước=2
HumanEval |  |  |  |  |  |  |  |  | 
StanD | 10K | 32.41 | - | 41.79 | - | 45.67 | - | 49.26 | -
InpD | 3.3K | 31.65 | - | 44.55 | - | 50.72 | - | 56.76 | -
-refine | 3.3K | 29.70 | 29.70 | 43.82 | 41.99 | 51.28 | 47.89 | 58.29 | 53.51
-combined | 6.5K | 30.15 | 32.30 | 42.94 | 45.27 | 47.91 | 50.50 | 52.54 | 55.46
PERsD | 3.3K | 34.63 | - | 49.34 | - | 55.34 | - | 60.41 | -
-refine | 3.3K | 32.35 | 33.35 | 48.69 | 49.35 | 56.07 | 56.87 | 63.60 | 64.76
-combined | 6.5K | 33.81 | 35.53 | 44.64 | 49.67 | 49.96 | 55.67 | 55.23 | 61.21
MBPP |  |  |  |  |  |  |  |  | 
StanD | 10K | 43.11 | - | 55.24 | - | 59.07 | - | 62.51 | -
InpD | 3.3K | 43.59 | - | 55.83 | - | 63.13 | - | 67.34 | -
-refine | 3.3K | 44.44 | 47.81 | 62.25 | 66.43 | 67.61 | 71.44 | 71.68 | 75.22
-combined | 6.5K | 42.69 | 47.25 | 56.70 | 62.17 | 61.39 | 66.49 | 65.46 | 70.22
PERsD | 3.3K | 45.47 | - | 59.90 | - | 64.85 | - | 69.73 | -
-refine | 3.3K | 48.24 | 52.65 | 63.65 | 68.49 | 69.00 | 73.34 | 73.16 | 77.62
-combined | 6.5K | 42.77 | 48.92 | 56.91 | 62.29 | 61.43 | 66.89 | 65.22 | 70.96

(b) Backbone là CodeGen-mono-16B
Phương pháp | #Dữ liệu | Pass@1 | Pass@5 | Pass@10 | Pass@20
--- | --- | --- | --- | --- | ---
 |  | bước=1 | bước=2 | bước=1 | bước=2 | bước=1 | bước=2 | bước=1 | bước=2
HumanEval |  |  |  |  |  |  |  |  | 
StanD | 10K | 33.96 | - | 50.56 | - | 57.69 | - | 63.82 | -
InpD | 2.8K | 36.68 | - | 49.51 | - | 53.85 | - | 57.47 | -
-refine | 2.8K | 30.55 | 31.28 | 48.40 | 48.13 | 55.00 | 54.52 | 61.31 | 60.62
-combined | 5.6K | 34.66 | 36.49 | 50.65 | 53.89 | 56.75 | 60.07 | 62.78 | 65.85
PERsD | 2.8K | 37.74 | - | 56.57 | - | 63.92 | - | 69.97 | -
-refine | 2.8K | 36.77 | 37.99 | 51.86 | 54.23 | 58.07 | 60.92 | 63.17 | 67.13
-combined | 5.6K | 36.40 | 37.74 | 53.57 | 55.80 | 60.81 | 63.37 | 67.3 | 70.50
MBPP |  |  |  |  |  |  |  |  | 
StanD | 10K | 48.90 | - | 62.21 | - | 66.91 | - | 71.33 | -
InpD | 2.8K | 46.27 | - | 58.45 | - | 62.61 | - | 66.43 | -
-refine | 2.8K | 48.79 | 54.87 | 66.89 | 71.32 | 72.24 | 75.71 | 75.82 | 78.84
-combined | 5.6K | 47.39 | 53.59 | 59.14 | 66.38 | 63.48 | 70.76 | 67.10 | 74.35
PERsD | 2.8K | 47.68 | - | 65.80 | - | 71.56 | - | 76.02 | -
-refine | 2.8K | 51.50 | 56.21 | 66.82 | 71.86 | 72.06 | 76.78 | 76.03 | 80.42
-combined | 5.6K | 51.44 | 56.44 | 66.45 | 71.31 | 71.64 | 76.43 | 76.04 | 80.20

Bảng 4: So sánh các mô hình PERsD với StanD & InpD

chỉ ra sự trùng lặp ngữ nghĩa hoàn toàn (chi tiết thêm trong Phụ lục D). Bảng 5 cho thấy nhiều trùng lặp hơn trong MBPP so với HumanEval, và nhiều trùng lặp hơn cho StanD so với PERsD. Sự trùng lặp này có thể là lý do tại sao StanD vượt qua PERsD trong thiết lập 1/16 (CodeGen-mono-16B, MBPP, pass@1), vì StanD có lợi thế không công bằng khi có rò rỉ dữ liệu đáng kể hơn. Ngoài ra, nếu chúng tôi kiểm tra các phương pháp của chúng tôi trên clean-MBPP trong đó các điểm dữ liệu bị rò rỉ được loại bỏ, thì PERsD trở nên gần như ngang bằng với StanD trong thiết lập cụ thể này trong khi có biên độ lớn hơn so với StanD trên 15/16 thiết lập còn lại (từ biên độ trung bình 4.8 điểm lên 5.9 điểm, chi tiết thêm tại Phụ lục E). Tổng cộng, phân tích trùng lặp này, kết hợp với kết quả từ MBPP đã làm sạch, thêm nhấn mạnh những lợi thế của chưng cất cá nhân hóa.

Phương pháp | Backbone | %("rò rỉ") | Tương tự
--- | --- | --- | ---
HumanEval |  |  | 
StanD | 6B,16B | 6.1% | 0.22
PERsD | 6B | 3.6% | 0.18
PERsD | 16B | 3.05% | 0.22
MBPP |  |  | 
StanD | 6B,16B | 18.24% | 0.40
PERsD | 6B | 8.47% | 0.30
PERsD | 16B | 7.49% | 0.30

Bảng 5: Phân tích Trùng lặp Train-Test. 6B/16B biểu thị backbone CodeGen-mono-{6/16}B. %("rò rỉ") biểu thị tỷ lệ phần trăm dữ liệu kiểm thử bị rò rỉ ngữ nghĩa trong dữ liệu huấn luyện. 'Tương tự' đại diện cho điểm tương tự trung bình (phạm vi: 0 đến 1; giá trị cao hơn cho thấy tương tự lớn hơn)

5.3 Hiệu ứng của việc trộn dữ liệu StanD và InpD
Bảng 6 hiển thị nghiên cứu loại bỏ về việc trộn dữ liệu chưng cất tiêu chuẩn với PERsD-refine và InpD-refine: trong khi việc trộn dữ liệu tiêu chuẩn với InpD-refine cải thiện hiệu suất 1 bước của nó trên MBPP và

Phương pháp | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100
--- | --- | --- | --- | --- | --- | ---
 | Step |  |  |  |  | 
HumanEval |  |  |  |  |  | 
StanD + InpD-refine | 1 | 30.59 | 40.04 | 44.20 | 54.23 | 58.54
StanD + InpD-refine* |  | 29.45 | 39.83 | 44.07 | 54.55 | 59.76
StanD + PERsD-refine |  | 32.13 | 43.82 | 48.66 | 59.55 | 64.02
PERsD-refine |  | 32.35 | 48.69 | 56.07 | 72.10 | 77.44
StanD + InpD-refine | 2 | 30.87 | 42.88 | 47.90 | 58.21 | 60.98
StanD + InpD-refine* |  | 30.12 | 42.71 | 47.42 | 58.69 | 64.02
StanD + PERsD-refine |  | 35.00 | 47.89 | 52.96 | 64.36 | 69.51
PERsD-refine |  | 33.35 | 49.35 | 56.87 | 74.13 | 79.88
MBPP |  |  |  |  |  | 
StanD + InpD-refine | 1 | 42.60 | 53.18 | 56.49 | 62.11 | 63.07
StanD + InpD-refine* |  | 44.08 | 54.12 | 57.82 | 64.96 | 66.34
StanD + PERsD-refine |  | 45.63 | 53.20 | 56.38 | 63.02 | 65.36
PERsD-refine |  | 48.24 | 63.65 | 69.00 | 78.16 | 81.70
StanD + InpD-refine | 2 | 46.32 | 58.84 | 62.80 | 69.80 | 71.23
StanD + InpD-refine* |  | 46.92 | 58.18 | 62.03 | 68.82 | 68.95
StanD + PERsD-refine |  | 48.44 | 58.37 | 62.47 | 70.64 | 73.20
PERsD-refine |  | 52.65 | 68.49 | 73.34 | 82.72 | 85.62

Bảng 6: Nghiên cứu loại bỏ về việc trộn StanD, với Backbone là CodeGen-mono 6B. InpD-refine* biểu thị sử dụng tất cả 6.5K nhiệm vụ mà mô hình học sinh mắc lỗi, bao phủ khoảng 3K nhiệm vụ nhiều hơn so với InpD-refine.

gần như duy trì hiệu suất của nó trên các thiết lập khác, việc trộn dữ liệu StanD với PERsD-refine làm giảm đáng kể hiệu suất của nó (ngoại trừ pass@1 inf-step=2 trên HumanEval). Chúng tôi đoán rằng vì StanD có khối lượng dữ liệu lớn hơn nhiều so với PERsD-refine, nó áp đảo việc huấn luyện học sinh trên chưng cất tiêu chuẩn. Tuy nhiên, việc kết hợp với dữ liệu cá nhân hóa đầu vào cân bằng có thể có lợi, như chúng tôi quan sát từ hiệu suất tốt của PERsD-combined trong Bảng 4 trên CodeGen-mono-16B.

Phương pháp | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100
--- | --- | --- | --- | --- | ---
HumanEval |  |  |  |  | 
PERsD | 34.63 | 49.34 | 55.34 | 65.56 | 67.93
PERsD + InpD | 34.88 | 48.35 | 54.06 | 64.88 | 68.90
MBPP |  |  |  |  | 
PERsD | 45.47 | 59.90 | 64.85 | 76.05 | 80.07
PERsD + InpD | 43.84 | 59.02 | 63.77 | 71.69 | 74.84

Bảng 7: Nghiên cứu loại bỏ về PERsD trộn InpD với CodeGen-mono 6B làm backbone

Tương tự, trong Bảng 7 chúng tôi hiển thị một nghiên cứu loại bỏ khác: việc trộn dữ liệu InpD với PERsD gần như duy trì hiệu suất trên HumanEval nhưng giảm trên MBPP. Điều này cho thấy các nhãn cá nhân hóa có chất lượng cao hơn và việc trộn các nhãn không cá nhân hóa cho cùng một nhiệm vụ nói chung làm hại hiệu suất.

--- TRANG 8 ---
Vòng | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100
--- | --- | --- | --- | --- | --- | ---
 | Step |  |  |  |  | 
HumanEval |  |  |  |  |  | 
1 | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73
2 |  | 32.74 | 45.50 | 51.52 | 66.14 | 71.95
1 | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44
2 |  | 36.75 | 49.71 | 56.13 | 70.24 | 75.00
MBPP |  |  |  |  |  | 
1 | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67
2 |  | 45.07 | 57.75 | 62.27 | 70.49 | 72.55
1 | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25
2 |  | 49.59 | 63.43 | 68.30 | 76.00 | 78.10

Bảng 8: Nghiên cứu loại bỏ về chưng cất nhiều vòng trên PERsD-combined với backbone CodeGen-mono 6B

5.4 Chưng cất Nhiều vòng
Sau khi tinh chỉnh mô hình học sinh với dữ liệu chưng cất cá nhân hóa, liệu chúng ta có thể thực hiện một vòng chưng cất cá nhân hóa khác, trên mô hình mới? Chúng tôi hiển thị nghiên cứu loại bỏ như vậy trong Bảng 8. Đáng khích lệ, chúng tôi thấy PERsD-combined vòng-2 nói chung vượt trội hơn PERsD-combined vòng-1 với một biên độ khiêm tốn. Sự cải thiện này cung cấp thêm bằng chứng về lợi ích của học tập cá nhân hóa, ngay cả khi được áp dụng cho các mô hình được huấn luyện với chưng cất cá nhân hóa. Những phát hiện này gợi ý khả năng thú vị của một phiên bản trực tuyến hoặc chủ động của chưng cất cá nhân hóa, trong đó việc thu thập dữ liệu và huấn luyện mô hình xảy ra đồng thời để đảm bảo mỗi batch được cá nhân hóa hoàn toàn và có hiệu quả mẫu cao hơn. Tuy nhiên, chúng tôi sẽ để lại khám phá thú vị như vậy cho công việc tương lai.

5.5 Sử dụng phản hồi cho Suy luận nhiều bước
Để hiểu rõ hơn vai trò của phản hồi thực thi trong quá trình huấn luyện và suy luận nhiều bước, chúng tôi hiển thị một nghiên cứu loại bỏ trong Bảng 9, trong đó chúng tôi so sánh PERsD-combine với một biến thể cụ thể (PERsD-combine*) loại trừ phản hồi trong cả huấn luyện và suy luận. chúng tôi quan sát thấy rằng PERsD-combine* hoạt động tương đương với PERsD-combine trên HumanEval và hơi tốt hơn trên MBPP cho suy luận 1 bước. Tuy nhiên, đối với suy luận 2 bước, PERsD-combine* liên tục hoạt động kém hơn

Phương pháp | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100
--- | --- | --- | --- | --- | --- | ---
 | Step |  |  |  |  | 
HumanEval |  |  |  |  |  | 
PERsD-combine | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73
PERsD-combine* |  | 33.29 | 45.47 | 50.90 | 62.87 | 68.29
PERsD-combine | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44
PERsD-combine* |  | 34.59 | 49.54 | 55.59 | 67.27 | 71.95
MBPP |  |  |  |  |  | 
PERsD-combine | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67
PERsD-combine* |  | 44.76 | 56.95 | 60.85 | 68.67 | 71.57
PERsD-combine | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25
PERsD-combine* |  | 47.83 | 61.28 | 65.54 | 73.03 | 75.49

Bảng 9: Nghiên cứu loại bỏ về việc loại bỏ phản hồi thực thi với backbone CodeGen-mono 6B. PERsD-combine* biểu thị chưng cất cá nhân hóa kết hợp mà không có phản hồi thực thi trong nhắc đầu vào.

PERsD-combine. Kết quả này phù hợp tốt với kỳ vọng của chúng tôi rằng việc sửa chữa mã cần phản hồi thực thi để hướng dẫn việc cải tiến.

5.6 Chưng cất Cá nhân hóa Chéo-Mô hình
Để khảo sát liệu dữ liệu chưng cất cá nhân hóa của một mô hình có thể có lợi cho mô hình khác hay không, chúng tôi tiến hành một nghiên cứu loại bỏ trong Bảng 10 bằng cách sử dụng dữ liệu PERsD-combined của CodeGen-mono-6B để huấn luyện CodeGen-mono-16B. Kết quả cho thấy rằng dữ liệu cá nhân hóa chéo-mô hình như vậy không hoạt động tốt bằng dữ liệu cá nhân hóa thực: dẫn đến việc giảm hiệu suất nhất quán với biên độ lớn. Phát hiện này củng cố quan niệm của chúng tôi rằng dữ liệu học tập nên được điều chỉnh cho mô hình học sinh cụ thể, vì dữ liệu cá nhân hóa phù hợp cho một mô hình có thể không nhất thiết mang lại lợi ích cho các mô hình khác.

Mô hình | Inf | Pass@1 | Pass@5 | Pass@10 | Pass@50 | Pass@100
--- | --- | --- | --- | --- | --- | ---
 | Step |  |  |  |  | 
HumanEval |  |  |  |  |  | 
CodeGen-mono-6B | 1 | 33.81 | 44.64 | 49.96 | 61.75 | 70.73
CodeGen-mono-16B* |  | 32.99 | 47.81 | 54.58 | 69.31 | 73.98
CodeGen-mono-16B |  | 36.40 | 53.57 | 60.81 | 74.64 | 79.88
CodeGen-mono-6B | 2 | 35.53 | 49.67 | 55.67 | 68.16 | 77.44
CodeGen-mono-16B* |  | 35.85 | 51.31 | 58.23 | 74.02 | 76.60
CodeGen-mono-16B |  | 37.74 | 55.80 | 63.37 | 77.14 | 81.10
MBPP |  |  |  |  |  | 
CodeGen-mono-6B | 1 | 42.77 | 56.91 | 61.43 | 68.84 | 70.67
CodeGen-mono-16B* |  | 43.24 | 60.14 | 65.19 | 72.31 | 74.19
CodeGen-mono-16B |  | 51.44 | 66.45 | 71.64 | 80.62 | 82.93
CodeGen-mono-6B | 2 | 48.92 | 62.29 | 66.89 | 75.09 | 77.25
CodeGen-mono-16B* |  | 48.12 | 65.31 | 70.02 | 76.60 | 78.70
CodeGen-mono-16B |  | 56.44 | 71.31 | 76.43 | 84.39 | 86.76

Bảng 10: Nghiên cứu loại bỏ về chưng cất cá nhân hóa chéo-mô hình với PERsD-combined. CodeGen-mono-16B* có nghĩa là dữ liệu chưng cất là từ CodeGen-mono-6B.

5.7 So sánh với các Mô hình Sinh tạo Mã dựa trên Phản hồi khác
So sánh với ILF (Chen et al., 2023a): Để so sánh với ILF, một trong những công trình liên quan gần nhất của chúng tôi, chúng tôi thí nghiệm trên một thiết lập riêng biệt:

--- TRANG 9 ---
bắt đầu với tập dữ liệu MBPP đầy đủ (974 nhiệm vụ) và sử dụng Task-Ids 11-111 làm phần kiểm thử và 863 còn lại làm dữ liệu huấn luyện. Trên tập huấn luyện, mô hình học sinh CodeGen-6B của chúng tôi (giống như ILF) tạo ra các nỗ lực sai trên 562 nhiệm vụ, được hiển thị cho ChatGPT cùng với hướng dẫn nhiệm vụ và phản hồi lỗi thực thi để cuối cùng thu thập 288 nhãn sửa chữa mã cá nhân hóa hợp lệ.

Dữ liệu văn bản-thành-mã MBPP gốc và dữ liệu cải tiến mã cá nhân hóa được thu thập này cho 288 nhiệm vụ

Tập Kiểm thử MBPP
Phương pháp | Chi phí | Pass@1 | Pass@10
--- | --- | --- | ---
ILF | >4K$ | 36 | 68
PERSD | 0.65$ | 46.8 | 67.4
-refine | 0.65$ | 41.8 | 66.8
-combined | 0.65$ | 47.8 | 64.8

Bảng 11: So sánh với ILF

tương ứng tạo thành dữ liệu tinh chỉnh Dcode và Drefine mà chúng tôi huấn luyện các mô hình PERSD và PERSD-refine. Chúng tôi tiếp tục kết hợp Dcode và Drefine để huấn luyện PERSD-combined. Kết quả thực nghiệm của chúng tôi trong Bảng 11 cho thấy tất cả các biến thể PERSD đều vượt trội đáng kể so với ILF 11.8% tại pass@1 với chi phí thấp hơn 1e-4 lần so với ILF, do đó thể hiện sự thiếu khả năng mở rộng của các mô hình kiểu ILF.

So sánh với Self-Edit: Vì Self-Edit (Zhang et al., 2023) sử dụng một mô hình trình soạn thảo mã CodeGen-350M có thể huấn luyện và một mô hình sinh tạo mã cố định, thiết lập thực nghiệm của chúng tôi không thể so sánh trực tiếp với của họ. Tuy nhiên, các mô hình INPD-refine và INPD-combined của chúng tôi thực sự có thể được coi là các đối tác rất gần với một phiên bản Self-Edit với mô hình sinh tạo mã và cải tiến mã chia sẻ và backbone CodeGen-6B. Sự cải thiện hiệu suất nhất quán của các mô hình chưng cất cá nhân hóa so với các mô hình được chưng cất đầu vào trên toàn bộ, ám chỉ triển vọng rằng các mô hình PERSD thực sự hiệu quả hơn so với các mô hình kiểu Self-Edit.

5.8 So sánh với các Mô hình SOTA
Cuối cùng, chúng tôi so sánh các mô hình PERsD-combine với các mô hình tiên tiến mã nguồn mở và mã nguồn đóng trên HumanEval trong Bảng 12. Chúng tôi thấy rằng các phương pháp PERsD-combine có thể cải thiện đáng kể mô hình backbone, với mức tăng hiệu suất 6.2 điểm cho CodeGen-mono 6B (giảm lỗi 8.4%), 5.9 điểm cho CodeGen-mono 16B (giảm lỗi 8.3%) và 12.2 điểm cho StarCoder (giảm lỗi 18.4%). Hơn nữa, StarCoder với PERsD-combined, vượt trội hơn các mô hình mã nguồn mở khác ngoại trừ WizardCoder. Lưu ý

Mô hình | Kích thước mô hình | Pass@1 | Pass@10 | Pass@100
--- | --- | --- | --- | ---
Mô hình mã nguồn đóng |  |  |  | 
LaMDA | 137B | 14.0 | - | 47.3
PaLM | 540B | 26.2 | - | 76.2
Codex | 12B | 28.8 | 46.8 | 72.3
code-cushman-001 | - | 33.5 | 54.3 | 77.4
code-davinci-002 | - | 47.0 | 74.9 | 92.1
GPT-3.5 | - | 48.1 | - | -
phi-1 | 1.3B | 50.6 | - | -
GPT-4 | - | 67.0 | - | -
Mô hình mã nguồn mở |  |  |  | 
CodeGeeX | 13B | 22.9 | 39.6 | 60.9
LLaMA | 65B | 23.7 | - | 79.3
StarCoder | 15B | 33.6 | - | -
CodeGen-mono | 6B | 26.1 | 42.3 | 65.8
CodeGen-mono | 16B | 29.3 | 49.9 | 75.0
InstructCodeT5+ | 16B | 35.0 | 54.5 | 77.9
WizardCoder | 15B | 57.3 | - | -
CodeGen-mono (PERsD-combined) | 6B | 33.8 | 50.0 | 70.7
CodeGen-mono (PERsD-combined) | 16B | 36.4 | 60.8 | 79.9
StarCoder (PERsD-combined) | 15B | 45.8 | 68.3 | 82.3

Bảng 12: Kết quả của pass@k (%) trên HumanEval

rằng mô hình của chúng tôi sử dụng 5K ví dụ dữ liệu trong khi WizardCoder sử dụng 78K. Như đã đề cập trong §2.1, WizardCoder là một cách tiếp cận trực giao có thể được tích hợp vào chưng cất cá nhân hóa.

6 Kết luận
Trong bài báo này, chúng tôi giới thiệu chưng cất cá nhân hóa như một phương pháp thu thập dữ liệu được gắn nhãn tùy chỉnh thích ứng với năng lực của các mô hình học sinh, dẫn đến việc học tập hiệu quả hơn. Chúng tôi đã chứng minh những lợi thế của chưng cất cá nhân hóa so với chưng cất tiêu chuẩn trong lĩnh vực sinh tạo mã, đạt được hiệu suất vượt trội trên cả tập dữ liệu HumanEval và MBPP. Thông qua các nghiên cứu loại bỏ toàn diện, chúng tôi xác nhận rằng chưng cất cá nhân hóa dẫn đến chất lượng dữ liệu cao hơn, hưởng lợi từ chưng cất nhiều vòng, và cho phép các mô hình tận dụng phản hồi thực thi để tự sửa chữa. Chúng tôi tin rằng chưng cất cá nhân hóa đại diện cho một bước tiến thú vị hướng tới việc chưng cất tốt hơn các LLM mã nguồn đóng thành các mô hình mã nguồn mở.

Hạn chế
Trong phần này, chúng tôi thảo luận về một số hạn chế của bài báo này và các hướng tương lai để làm cho nó có giá trị hơn:

Về Quy mô Dữ liệu Để so sánh công bằng, chúng tôi đã thực hiện tất cả các thí nghiệm dựa trên cùng 10K dữ liệu DSTAND (được giới thiệu §4.2) và dữ liệu cá nhân hóa tương ứng được xử lý từ DSTAND có kích thước 2-3K như hiển thị trong Bảng 3. Tuy nhiên, vì chúng tôi đã chứng minh chưng cất cá nhân hóa hỗ trợ

--- TRANG 10 ---
học tập hiệu quả và hiệu quả hơn, sẽ thú vị khi khảo sát chưng cất cá nhân hóa mở rộng tốt như thế nào với kích thước dữ liệu. Ví dụ, nếu chúng ta mở rộng dữ liệu chưng cất cá nhân hóa lên 50K, các phương pháp PERsD sẽ nhận được bao nhiều tăng hiệu suất hơn so với InpD và StanD với việc mở rộng kích thước dữ liệu.

Chưng cất Cá nhân hóa Trực tuyến Như đã thảo luận trong §5.4, việc tiến hành vòng thứ hai chưng cất cá nhân hóa tiếp tục cải thiện một mô hình học sinh đã được huấn luyện với PERsD-combine. Quan sát như vậy gợi ý tiềm năng của một phiên bản trực tuyến của chưng cất cá nhân hóa, thu thập một batch dữ liệu cá nhân hóa một cách tức thì với mô hình giáo viên, sau mỗi bước tối ưu hóa trong quá trình tinh chỉnh. Vì chúng tôi đã chứng minh rằng dữ liệu cá nhân hóa thực sự có lợi hơn dữ liệu tiêu chuẩn hoặc dữ liệu cá nhân hóa chéo-mô hình (§5.6), chưng cất cá nhân hóa trực tuyến như vậy sẽ về nguyên tắc tối đa hóa lợi ích từ chưng cất cá nhân hóa, vì mỗi batch dữ liệu huấn luyện được điều chỉnh hoàn toàn cho mô hình học sinh.

Tài liệu tham khảo
Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, và Charles Sutton. 2021. Program synthesis with large language models. CoRR, abs/2108.07732.

Sahil Chaudhary. 2023. Code alpaca: An instruction-following llama model for code generation. https://github.com/sahil280114/codealpaca.

Angelica Chen, Jérémy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R. Bowman, Kyunghyun Cho, và Ethan Perez. 2023a. Improving code generation by training with natural language feedback.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, và Wojciech Zaremba. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374.

Xinyun Chen, Maxwell Lin, Nathanael Schärli, và Denny Zhou. 2023b. Teaching large language models to self-debug. CoRR, abs/2304.05128.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

Yuxin Jiang, Chunkit Chan, Mingyang Chen, và Wei Wang. 2023. Lion: Adversarial distillation of closed-source large language model. CoRR, abs/2305.12870.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, và Harm de Vries. 2023a. Starcoder: may the source be with you! CoRR, abs/2305.06161.

Zihao Li, Zhuoran Yang, và Mengdi Wang. 2023b. Reinforcement learning with human feedback: Learning dynamic choices via pessimism. CoRR, abs/2305.18438.

Hao Liu, Carmelo Sferrazza, và Pieter Abbeel. 2023. Chain of hindsight aligns language models with feedback. CoRR, abs/2302.02676.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, và Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evol-instruct. CoRR, abs/2306.08568.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,

--- TRANG 11 ---
Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, và Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. CoRR, abs/2303.17651.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. ICLR.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, và Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, và Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. CoRR, abs/2305.18290.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. 2020. Zero: memory optimizations toward training trillion parameter models. In SC, page 20. IEEE/ACM.

Heather Roberts-Mahoney, Alexander J. Means, và Mark J. Garrison. 2016. Netflixing human capital development: personalized learning technology and the corporatization of k-12 education. Journal of Education Policy, 31(4):405–420.

Atikah Shemshack và Jonathan Michael Spector. 2020. A systematic literature review of personalized learning terms. Smart Learning Environments, 7(1):1–20.

Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, và Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, và Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions.

Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, và Yejin Choi. 2022. Generating sequences by learning to self-correct. CoRR, abs/2211.00053.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Association for Computational Linguistics.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin Jiang. 2023a. Wizardlm: Empowering large language models to follow complex instructions. CoRR, abs/2304.12244.

Canwen Xu, Daya Guo, Nan Duan, và Julian McAuley. 2023b. Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196.

Kechi Zhang, Zhuo Li, Jia Li, Ge Li, và Zhi Jin. 2023. Self-edit: Fault-aware code editor for code generation. CoRR, abs/2305.04087.

A Chi tiết trong Đánh giá Mô hình Nhiều bước
Vì các docstring được định dạng kém trong HumanEval, chúng tôi viết một đoạn mã phân tích dựa trên quy tắc đơn giản để trích xuất các trường hợp kiểm thử đơn vị đã thấy của nó. Trung bình mỗi nhiệm vụ, có 2 trường hợp kiểm thử đơn vị đã thấy và 4.2 trường hợp kiểm thử đơn vị chưa thấy. Sự trùng lặp giữa kiểm thử đã thấy và chưa thấy là 11.33%. Đối với MBPP, vì theo thông lệ nhắc hướng dẫn được xây dựng bằng cách lấy mô tả nhiệm vụ và cách sử dụng ví dụ (từ các trường hợp kiểm thử đơn vị) làm một phần của doc-string, chúng tôi coi tất cả các trường hợp kiểm thử đơn vị là "đã thấy" và sử dụng tất cả chúng cho suy luận nhiều bước.

B Mẫu Nhắc ChatGPT cho Chưng cất Cá nhân hóa
Trong Hình 3, chúng tôi hiển thị mẫu nhắc mà chúng tôi sử dụng để truy vấn ChatGPT cho cải tiến cá nhân hóa. Đối với mỗi ví dụ nhiệm vụ, với hướng dẫn nhiệm vụ t, các trường hợp kiểm thử đơn vị u và mã chính xác c, chúng tôi truy vấn ChatGPT API với lịch sử cuộc trò chuyện hai lượt.

Đối với lượt đầu tiên, chúng tôi sử dụng mẫu trong Hình 3a và thay thế «TASK», «HEADER» bằng hướng dẫn nhiệm vụ thực tế t và tiêu đề hàm được trích xuất. Điều này được thêm vào đầu vào của người dùng lượt đầu tiên và mã chính xác c được bao gồm làm đầu ra trợ lý lượt đầu tiên. Đối với lượt thứ hai, chúng tôi sử dụng mẫu trong Hình 3b và thay thế «CODE», «ERROR» bằng nỗ lực của mô hình học sinh và phản hồi thực thi của nó. Điều này được thêm vào đầu vào của người dùng lượt thứ hai và chúng tôi truy vấn ChatGPT với lịch sử cuộc trò chuyện được xây dựng để có đầu ra trợ lý lượt thứ hai làm cải tiến mã cá nhân hóa.

--- TRANG 12 ---
(a) Mẫu Nhắc Lượt-1
(b) Mẫu Nhắc Lượt-2

Hình 3: Mẫu nhắc để truy vấn cải tiến cá nhân hóa. Trên(a): mẫu nhắc cho cuộc trò chuyện lượt đầu tiên, Dưới(b): mẫu nhắc cho cuộc trò chuyện lượt thứ hai.

C Mẫu Nhắc cho Tinh chỉnh Cải tiến Mã
Hình 4 hiển thị mẫu cải tiến Trefine được giới thiệu trong §3.2), được sử dụng để xây dựng nhắc đầu vào cho tinh chỉnh cải tiến mã. chúng tôi thay thế «TASK» bằng hướng dẫn nhiệm vụ, «CODE» bằng nỗ lực sai ban đầu từ học sinh, «ERROR» bằng phản hồi thực thi, và «HEADER» bằng tiêu đề hàm được trích xuất từ hướng dẫn nhiệm vụ.

D Chi tiết trong Phân tích Trùng lặp Dữ liệu
Phần này mô tả các quy trình chi tiết để tiến hành phân tích trùng lặp dữ liệu train-test. Mục tiêu là đánh giá mức độ rò rỉ dữ liệu trong các tập dữ liệu kiểm thử có nguồn gốc từ kho dữ liệu tiền huấn luyện tự xây dựng của chúng tôi.

Hình 4: Mẫu nhắc cho tinh chỉnh cải tiến mã.

Đầu tiên, chúng tôi đã thực hiện khớp chuỗi chính xác và không tìm thấy rò rỉ dữ liệu trong bất kỳ dữ liệu kiểm thử nào (HumanEval/MBPP).

Để đo lường tương tự ngữ nghĩa giữa các nhiệm vụ huấn luyện/kiểm thử, chúng tôi đã làm như sau:

1. Đối với mỗi nhiệm vụ trong kiểm thử (MBPP/HumanEval) chúng tôi lấy hai nhiệm vụ huấn luyện gần nhất (dựa trên tương tự cosine của embedding starcoder & vector tf-idf của mô tả nhiệm vụ).

2. Chúng tôi sử dụng gpt-3.5-turbo-16k để xác định liệu có rò rỉ dữ liệu giữa một instance train và test hay không bằng cách phân loại cặp thành ("rò rỉ", "hơi giống", "hơi không giống", "không liên quan"). Chúng tôi sử dụng một nhắc với hướng dẫn và các ví dụ few-shot được tạo thủ công và yêu cầu gpt-3.5 tạo ra lý luận và phân loại. Chúng tôi kiểm tra thủ công một số ví dụ cho mỗi danh mục để đảm bảo lý luận và phán đoán được thực hiện chính xác và nhất quán.

3. Ánh xạ các danh mục tương tự thành điểm tương tự 0-1 ("rò rỉ" -> 1, "hơi giống" -> 0.75, "hơi không giống" -> 0.25, "không liên quan" -> 0) và hiển thị điểm trung bình và % trường hợp được phân loại là "rò rỉ". Lưu ý rằng StanD & PERsD có 10K & 3K dữ liệu huấn luyện tương ứng nên điểm của chúng khác nhau.

E Kết quả trong MBPP-Cleaned
Trong Phụ lục D, chúng tôi tìm thấy 55 instance dữ liệu có khả năng bị rò rỉ (với điểm tương tự = 1) trong dữ liệu kiểm thử MBPP. Trong phần này, chúng tôi xây dựng một tập dữ liệu MBPP-Cleaned mới, trong đó các điểm dữ liệu bị rò rỉ được loại bỏ (ban đầu 306 vấn đề → 251 vấn đề sau khi lọc). Kết quả trên tập dữ liệu MBPP-Cleaned mới này được hiển thị trong Bảng 13. Từ

--- TRANG 13 ---
kết quả, chúng ta có thể thấy đối với thiết lập CodeGen-mono-16B, pass@1, PERsD trở nên gần như ngang bằng với StanD (từ khoảng cách -1.21 thành -0.17). Đối với 15/16 thiết lập còn lại trên PERsD so với StanD, biên độ trung bình của nó tăng từ 4.8 điểm lên 5.9 điểm. Bên cạnh đó, PERsD-refine trên MBPP-Cleaned cho thấy những cải thiện nhất quán và đáng kể hơn so với InpD-refine, với lợi thế trung bình +0.86 cho suy luận 1 bước, và +1.91 cho suy luận hai bước. Nhìn chung, với dữ liệu kiểm thử trùng lặp được loại bỏ, các phương pháp PERsD cho thấy những lợi thế thậm chí lớn hơn so với các phương pháp StanD hoặc InpD.

(a) Backbone là CodeGen-mono-6B
Phương pháp | #Dữ liệu | Pass@1 | Pass@5 | Pass@10 | Pass@20
--- | --- | --- | --- | --- | ---
 |  | bước=1 | bước=2 | bước=1 | bước=2 | bước=1 | bước=2 | bước=1 | bước=2
MBPP-Cleaned |  |  |  |  |  |  |  |  | 
StanD | 10K | 37.51 | - | 50.89 | - | 55.15 | - | 58.87 | -
InpD | 3.3K | 38.80 | - | 53.91 | - | 58.47 | - | 62.73 | -
-refine | 3.3K | 37.58 | 42.95 | 57.65 | 62.29 | 63.52 | 67.79 | 67.92 | 71.96
-combined | 6.5K | 38.11 | 43.01 | 52.69 | 58.32 | 57.36 | 62.75 | 61.19 | 66.18
PERsD | 3.3K | 41.30 | - | 56.20 | - | 61.86 | - | 67.53 | -
-refine | 3.3K | 43.86 | 47.73 | 59.33 | 64.41 | 65.19 | 69.95 | 69.62 | 74.33
-combined | 6.5K | 38.86 | 43.75 | 52.78 | 57.04 | 57.35 | 61.78 | 61.52 | 66.19

(b) Backbone là CodeGen-mono-16B
Phương pháp | #Dữ liệu | Pass@1 | Pass@5 | Pass@10 | Pass@20
--- | --- | --- | --- | --- | ---
 |  | bước=1 | bước=2 | bước=1 | bước=2 | bước=1 | bước=2 | bước=1 | bước=2
MBPP-Cleaned |  |  |  |  |  |  |  |  | 
StanD | 10K | 43.10 | - | 57.53 | - | 62.92 | - | 68.12 | -
InpD | 2.8K | 40.64 | - | 53.88 | - | 58.82 | - | 62.88 | -
-refine | 2.8K | 43.67 | 49.60 | 63.14 | 68.21 | 69.27 | 73.28 | 73.36 | 76.85
-combined | 5.6K | 41.63 | 47.77 | 54.74 | 62.24 | 59.67 | 67.33 | 63.75 | 71.57
PERsD | 2.8K | 42.93 | - | 62.40 | - | 68.90 | - | 74.10 | -
-refine | 2.8K | 47.73 | 52.63 | 63.62 | 69.21 | 69.84 | 75.17 | 74.90 | 79.69
-combined | 5.6K | 46.33 | 51.67 | 63.46 | 68.65 | 69.49 | 74.26 | 74.53 | 78.83

Bảng 13: So sánh hiệu suất của các mô hình PERsD với StanD & InpD trên MBPP-Cleaned
