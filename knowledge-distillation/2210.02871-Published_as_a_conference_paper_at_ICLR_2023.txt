# 2210.02871.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/knowledge-distillation/2210.02871.pdf
# File size: 865030 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2023
SELF-DISTILLATION FOR
FURTHER PRE-TRAINING OF TRANSFORMERS
Seanie Lee1†Minki Kang1,2Juho Lee1,2Sung Ju Hwang1Kenji Kawaguchi3∗
KAIST1, AITRICS2, National University of Singapore3
{lsnfamily02, zzxc1133, juholee, sjhwang82 }@kaist.ac.kr
kenji@comp.nus.edu.sg
ABSTRACT
The application of pre-training large transformer models on massive amounts of
unlabeled data and fine-tuning them on labeled datasets for diverse downstream
tasks has demonstrated remarkable success in various vision and natural language
processing tasks. However, the direct fine-tuning approach may result in subopti-
mal performance if there exists a significant discrepancy between the pre-training
and fine-tuning domains. To address this issue, some previous studies have pro-
posed further pre-training strategies to continue pre-training the model on the tar-
get unlabeled dataset before fine-tuning. However, these strategies are limited to
language models and may result in overfitting when applied to Vision Transform-
ers. To overcome this limitation, we present a novel approach of self-distillation
as a regularization method for the further pre-training stage. Our method first fur-
ther pre-trains the initial pre-trained model on the target unlabeled data, and then
uses it as a teacher for self-distillation. Then we take the same initial pre-trained
model as a student, and enforce its hidden representations to be close to those
of the teacher while optimizing the student with a masked auto-encoding objec-
tive. Our experiments demonstrate the superiority of self-distillation over relevant
baselines on various benchmark datasets for image and text classification tasks.
Furthermore, we provide a theoretical analysis of our proposed method using a
simplified model to shed light on how self-distillation for further pre-training can
potentially enhance the performance of downstream tasks.
1 I NTRODUCTION
Pre-trained transformer models (Devlin et al., 2019; Brown et al., 2020; Liu et al., 2019; He et al.,
2022) have been effective on various vision and natural language processing tasks. The pre-trained
models learn general representation from a large volume of unlabeled data so that they generalize
well to various downstream tasks when they are fine-tuned on each task with a labeled dataset.
However, in many of real-world applications, it requires a considerable amount of effort to adapt the
pre-trained model to a specific downstream task domain since there exists a significant distributional
discrepancy between data for the pre-training and fine-tuning stage. Moreover, it is difficult to
collect a large amount of labeled data for such specific domains, which renders adaptation of the
pre-trained model to downstream tasks more challenging.
0 10k 20K 30K 40K 50K 60K
Further Pre-training Steps5052545658Test Accuracy
Image Classification on CUB
Self-Distillation
Further Pre-training
Figure 1: Acc. with varying the
number of further pre-training steps.Several works have proposed to tackle the problem of adapting
pre-trained models to a specific domain. A prevalent approach
for adaptation of the pre-trained model is further pre-training
where we continue to update the parameters of the pre-trained
model on additionally curated domain-specific unlabeled data
with self-supervision (Beltagy et al., 2019; Lee et al., 2020),
before fine-tuning it on the target labeled data as depicted in
Figure 2b. Gururangan et al. (2020) also show that further pre-
training only with the target unlabeled data is still effective with-
out any extra data. However, most of the existing further pre-
training approaches have focused on language models, and we find that the further pre-training
∗Corresponding Author†The work was done while the author was an intern at NUS.
1arXiv:2210.02871v3  [cs.CV]  9 Jun 2023

--- PAGE 2 ---
Published as a conference paper at ICLR 2023
Transformer𝑓!!"!#Classifierℎ"(a) Fine-tuning
𝑦=0
Labeled Dataset𝑦=1
(c) Self-distillation in Further Pre-training
Transformer𝑓!∗Classifierℎ"∗(b) Further Pre-training
UnlabeledDataset
Transformer𝑓!%Classifierℎ"
Transformer𝑓!∗Classifierℎ"∗
Unlabeled Dataset
Unlabeled Dataset
Transformer𝑓!&Classifierℎ"𝓛𝑪𝑬𝓛𝑴𝑨𝑬
𝓛𝑴𝑨𝑬+ 𝓛𝑫𝒊𝒔𝒕𝒊𝒍𝒍
𝑦=0
Labeled Dataset𝑦=1
Transformer𝑓!∗Classifierℎ"∗𝓛𝑪𝑬
𝑦=0
𝑦=1𝓛𝑪𝑬Labeled Dataset
Transformer𝑓!!"!#Decoder𝑔#!"!#𝓛𝑴𝑨𝑬TeacherStudent
Transformer𝑓!!"!#Decoder𝑔#!"!#
Transformer𝑓!%Decoder𝑔#%
Figure 2: Concepts. Comparison between methods adapting pre-trained transformers to the target domain. (a)
Fine-tuning without any further pre-training. (b) Further pre-training and fine-tuning. (c) Self-distillation
in further pre-training and fine-tuning.
strategy is not effective for Vision Transformer (ViT) (Dosovitskiy et al., 2021). As shown in Fig-
ure 1, ViT is vulnerable to overfitting and does not generalize well to downstream tasks as when we
continue to pre-train it on the target unlabeled data.
Several regularization methods (Chen et al., 2020a; Gouk et al., 2021; Aghajanyan et al., 2021) have
proposed to tackle the overfitting issue of large pre-trained models, however, they do not consider the
adaptation process such as further pre-training. Instead, they enforce the distance between the final
fine-tuned weight and the pre-trained weight to be small to promote the transfer of the knowledge
acquired from pre-training to downstream tasks for better generalization. However, these regular-
izations hinder the adaptation of pre-trained models to downstream tasks especially when there is a
significant distributional shift between the pre-trained data and target data. It eventually results in
worse generalization than the simple fine-tuning strategy.
To tackle these limitations, we propose self-distillation as a regularization for further pre-training on
a target unlabeled dataset so that we can effectively adapt pre-trained models to the downstream task
of various domains with a limited amount of labeled data. For self-supervision, we focus on masked
auto-encoding for pre-training since it does not depend on any data augmentations, compared to
other self-supervised learning methods (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; Zbontar
et al., 2021; Chen & He, 2021; Caron et al., 2021) which require data augmentations to construct
positive pairs for self-supervised learning objective such as contrastive learning. This is especially
useful when it is hard to define meaningful data augmentations for a target domain.
Specifically, we take the pre-trained model with an encoder fθinitand a decoder gϕinitwhich are pre-
trained on a massive amount of unlabeled data from general domain, and continue to pre-train it with
masked auto-encoding (MAE) (Devlin et al., 2019; He et al., 2022) objective on the target unlabeled
data to obtain fθ0andgϕ0. After that, we set the encoder fθ0as a teacher for self-distillation. Then
we take the copy of the pre-trained model (fθinit, gϕinit)as a student, and match the representations
of the student encoder and those of the teacher encoder while optimizing the student with the MAE
on the target unlabeled data. Finally, we fine-tune the self-distilled student fθ1on the target labeled
data for the downstream task. We illustrate the overview of our method in Figure 2c.
To verify the efficacy of our method, we empirically show that it significantly improves the general-
ization performance of a pre-trained ViT and language model RoBERTA (Liu et al., 2019), and out-
performs the relevant baselines on various image and text classification datasets. Moreover, we the-
oretically analyze the proposed method with a simplified model to understand how self-distillation
for further pre-training can potentially help improve the generalization performance on the target
tasks after fine-tuning.
Our contribution is threefold:
• We propose self-distillation for further pre-training on the target unlabeled dataset, where
we enforce representations of the student to be close to those of the further pre-trained
teacher while training the student with masked-auto encoding objective.
• We theoretically analyze the proposed method with a simplified model to understand how
self-distillation for further pre-training can potentially lead to better generalization perfor-
mance of downstream tasks.
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2023
• We extensively validate our method on various image and text classification datasets with
pre-trained transformers and show that ours outperforms the relevant baselines.
2 R ELATED WORK
Self-Distillation Knowledge distillation is to transfer knowledge of teacher to student by minimiz-
ing a divergence between output of teacher and student (Hinton et al., 2014). When the parameteri-
zation of student and teacher is identical, we call it self-distillation as a special case of the knowledge
distillation. Although there is no new information during self-distillation process, Furlanello et al.
(2018) have shown that the student from self-distillation achieves better generalization performance
than the teacher. A similar phenomenon has been consistently observed in other works (Yang et al.,
2019; Ahn et al., 2019). Several works propose a self-distillation without a pre-trained teacher net-
work (Sun et al., 2019; Zhang et al., 2019; 2022). They add auxiliarly classifiers to intermediate
layers and train the classifiers to minimize divergence between the output of the classifier of the
last layer and that of the auxiliary classifiers. Mobahi et al. (2020) theoretically analyze how self-
distillation induces regularization and reduces overfitting in Hilbert space. However, all of them
focus on self-distillation for supervised learning. Instead, we empirically and theoretically show
that self-distillation for further pre-training with self-supervision leads to better generalization of
downstream tasks after fine-tuning the self-distilled model with target labeled data.
Further Pre-training Lee et al. (2020); Beltagy et al. (2019); Sun et al. (2020) have shown the
success of continual pre-training language model on a large number of corpora collected from target
domain and fine-tuning the model on target labeled dataset. However, it is computationally expen-
sive to further pre-train the model on a large amount of unlabeled text data and it may not be feasible
to collect such a large scale of unlabeled data on certain domains. Instead, Gururangan et al. (2020)
devise a task-adaptive pre-training where we use only target unlabeled data for further pre-training
language model before fine-tuning the model on the target labeled data. To improve the effective-
ness of further pre-training, Kang et al. (2020); Ye et al. (2021) propose learning to mask input for
masked auto-encoding with bilevel optimization, which requires a prohibitive computational cost.
However, all of them solely focus on pre-trained language models and we empirically find that naive
further pre-training is not effective for Vision Transformers.
Regularization for Fine-tuning There are several works proposing regularization for fine-tuning
a pre-trained model. Chen et al. (2020a) propose to modify Adam (Kingma & Ba, 2015) optimizer,
called RecAdam, which enforces the fine-tuned model close to the initial pre-trained model by
minimizing L2distance between fine-tuned and initial pre-trained weight. Similarly, Gouk et al.
(2021) project the fine-tuned weight for every gradient descent update such that it lies within the
sphere centered on the initial pre-trained weights with the distance induced by the norm of maximum
absolute row sums (MARS). Instead of explicitly minimizing the distance, motivated by trust region
theory, Aghajanyan et al. (2021) propose to minimize symmetric KL-divergence between the model
output of an original input and that of the input perturbed by Gaussian noise. However, all of them
do not consider adaptation of pre-trained models to a specific target domain, which results in worse
generalization performance of downstream tasks than a simple fine-tuning strategy.
3 M ETHOD
3.1 P RELIMINARIES
Problem Statement We assume that we are given (θinit, ϕinit)parameters of the neural network
gϕinit◦fθinitwhich is pre-trained on a large volume of unlabeled data with masked auto-encoding
objective, where fθinitis an encoder which extracts hidden representation of an input and gϕinit
is an decoder reconstructing a masked input. Our goal is to fine-tune the pre-trained model fθinit
with a randomly initialized task specific head hωon labeled dataset Dtr={(x(i), y(i))}n
i=1of a
downstream classification task such that the model generalizes well to unseen test dataset Dtest. A
typical approach to achieve this goal is empirical risk minimization as follows:
minimize
θ,ωLCE(θ, ω;Dtr)via algorithm Aas
(θ∗, ω∗) =A(LCE;θinit,Dtr),(1)
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2023
Algorithm 1 Self-Distillation
Require: Unlabeled dataset Du, initialization θinit, ϕinit,
learning rate α∈R≥0, round of self-distillation T′∈
N+, masking probability γ∈(0,1)and batch size B.
1:θ0←Further Pretrain (Du, θinit, ϕinit, α, γ, B )
2:for all t←1, . . . , T′do
3: Initialize θt←θinit andϕt←ϕinit
4: while not converge do
5: Sample a mini-batch {x(j)}B
j=1fromDu
6: for all j←1, . . . , B do
7: Sample a mask z(j)∼pγ,K(z)
8: Z(j)←PK
k=1z(j)
k
9: Get a masked input ˆx(j)withz(j)
10: ℓ1
j← −PK
k=1z(j)
k
Z(j)logpθt,ϕt(x(j)
k|ˆx(j))
11: ℓ2
j←fθt(x(j))−StopGrad (fθ0(x(j)))2
2
12: end for
13: L1←1
BPB
j=1ℓ1
j,L2←1
BPB
j=1ℓ2
j
14: θt←θt−α∂(L1+L2)
∂θ|θ=θt
15: ϕt←ϕt−α∂L1
∂ϕ|ϕ=ϕt
16: end while17: θ0←θt
18:end for19:return θT′Algorithm 2 Further Pretrain
Require: Unlabeled dataset Du, initialization
θinit, ϕinit, learning rate α∈R≥0, mask-
ing probability γ∈(0,1), and batch size B.
1: Initialize θ0←θinit andϕ0←ϕinit
2:while not converge do
3: Sample a mini-batch {x(j)}B
j=1fromDu
4: for all j←1, . . . , B do
5: Sample a mask z(j)∼pγ,T(z)
6: Z(j)←PK
k=1z(j)
k
7: Get a masked input ˆx(j)withz(j)
8: pk←pθ0,ϕ0(x(j)
k|ˆx(j))
9: ℓ1
j← −PK
k=1z(j)
k
Z(j)logpk
10: end for
11: L ←1
BPB
j=1ℓ1
j
12: θ0←θ0−α∂L
∂θ|θ=θ0
13: ϕ0←ϕ0−α∂L
∂ϕ|ϕ=ϕ0
14:end while
15:return θ0
16:
17:
where LCEis a cross-entropy loss and Adenotes a stochastic gradient descent algorithm to minimize
LCEon the dataset Dtrwith the initialization θinit.
Further Pre-training However, the pre-trained model is prone to overfitting when it is fine-tuned
on a small amount of domain-specific labeled data. Gururangan et al. (2020) have shown that further
pre-training, where we continue to pre-train the model gϕinit◦fθiniton the target unlabeled dataset
Du={x(i)}n
i=1and then fine-tune it on Dtr, is effective for improving generalization performance
when there is not enough domain-specific labeled data. Note that Duis the exactly same as Dtr
except that we remove the labels y(i). In this work, we focus on the masked auto-encoding (Devlin
et al., 2019; He et al., 2022) as a pre-training objective function since its generality compared to
other self-supervised methods (Chen et al., 2020b; He et al., 2020; Grill et al., 2020; He et al., 2020;
Chen & He, 2021; Caron et al., 2021) which require well-defined data augmentations to construct
positive pairs for self-supervised learning.
Masked Auto-Encoding We briefly describe the masked auto-encoding objective (Liu et al., 2019;
He et al., 2022) for a language model such as RoBERTA (Liu et al., 2019) and Vision Transformer
(ViT) (Dosovitskiy et al., 2021). Let x(i)= (x(i)
1, . . . , x(i)
K)be a sequence of patches for a image or
tokens for a sentence with length K. Then we independently sample a binary mask from Bernoulli
distribution with probability γfor each x(i)
k, denoted as z(i)= (z(i)
1, . . . , z(i)
K). Ifz(i)
k= 1, then
x(i)
kis replaced with a special “mask” token. Otherwise, we use the same x(i)
kfor a masked input.
Letˆx(i)= (ˆx(i)
1, . . . , ˆx(i)
K)be a masked input and let fθ, gϕbe an encoder and decoder, respectively.
Then the final objective for masked auto-encoding is defined as follows:
LMAE(θ, ϕ;Du) =1
nnX
i=1Ez(i)∼pγ,T(z)"
−KX
k=1z(i)
k
Z(i)·logpθ,ϕ(x(i)
k|ˆx(i))#
, Z(i)=KX
k=1z(i)
k,(2)
where pγ,K(z)denotes a Binomial distribution with its parameters γfor probability that zk= 1and
Kfor the number of trials. Note that the negative log-likelihood is instantiated as cross-entropy loss
for language models or mean square error for Vision Transformers. See Appendix C for more detail.
3.2 S ELF-DISTILLATION FOR FURTHER PRE-TRAINING
Although further pre-training strategy has been effective on text domain (Gururangan et al., 2020;
Lee et al., 2020; Sun et al., 2020), we empirically find that ViT with further pre-training overfits
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2023
the target unlabeled data and does not generalize well to downstream image classification tasks.
In order to tackle the issue, we propose self-distillation as a regularization for further pre-training.
Specifically, given a pre-trained model gϕinit◦fθinit, we first continue to train the model on the
target unlabeled data Duwith the masked auto-encoding objective as described in equation 2 to
obtain the encoder fθ0and decoder gϕ0. We discard the decoder and consider the encoder fθ0as a
teacher for self-distillation. Then we take the copy of the pre-trained initial network gϕinit◦fθinitas
a student and further pre-train the student with masked auto-encoding objective but enforce hidden
representation of the encoder of the student fθinitto be close to that of the teacher fθ0as follows:
(θ1, ϕ1)∈arg min
θ,ϕ(LMAE(θ, ϕ;Du) +LDistill (θ;θ0,Du))
LDistill (θ;θ0,Du) =1
nnX
i=1fθ(x(i))−StopGrad
fθ0(x(i))2
2(3)
where θandϕare initialized with the pre-trained parameters θinit andϕinit, respectively and
StopGrad denotes the stop-gradient operation which does not back-propagate through the in-
put. As described in Algorithm 1, we can repeat this process to perform multiple rounds of self-
distillation (T′>1)where the student of the previous round becomes a teacher and a new student
is initialized with the pre-trained weights θinit andϕinit for the next round. We empirically find
that the first round of self-distillation plays the most significant role in improving the final gener-
alization performance of downstream tasks. Furthermore, theoretical analysis shows that the first
round of self-distillation has the largest impact on regularization. Thus, we perform a single round
of self-distillation for computational efficiency. After self-distillation, we discard the decoder gϕ1
and fine-tune the encoder of the student fθ1along with a randomly initialized task-specific head hω
by minimizing LCE(θ, ω,;Dtr)with the initialization θ1as described in equation 1.
4 T HEORETICAL ANALYSIS
In this section, we analyze how self-distillation affects the final model after fine-tuning in terms of
generalization and regularization. This section proves a generalization bound on the supervised loss
for our method and shows that the generalization bound strictly decreases as the number of self-
distillation increases. Moreover, we show that self-distillation acts as a regularizer on the distance
between the initial weight before further pre-training and the final weight after fine-tuning. The
regularization effect is shown to have the largest impact in the first round of self-distillation, which
suggests that the first round of self-distillation plays a more significant role in the final performance
when compared to the other rounds.
We consider the dynamics of the weight vector wt,τover time τof fine-tuning after trounds of self-
distillation, where w0,0is the result of further pre-training, and wt,0∈minimize wLt(w)is the result
of the self-distillation of trounds with Lt(w) =1
nPn
i=1∥f(xi, w)−f(xi, wt−1,0)∥2
2+λ∥w∥2
2for
some λ >0. After trounds of self-distillation, we consider the dynamics over fine-tuning time τvia
gradient flow (Saxe et al., 2014; Kawaguchi, 2021):dwt,τ
dτ=−∇L (wt,τ),with the initialization wt,0
obtained by the self-distillation where L(w) =1
2Pn
i=1ℓ(w, xi, yi)withℓ(w, x, y ) =∥f(x, w)−
y∥2
2andy∈Rp. Here, the self-distillation and fine-tuning share a same training dataset s=
{(xi, yi)}n
i=1. In this section, to obtain theoretical insights, we consider the regime of d > n and a
simple abstract model, f(x, w) =Wφ(x)∈Rp, with some nonlinear map φand the weight matrix
W∈Rp×dwhere w= vec[ W⊤]∈Rdpandφ(x)∈Rd. Here, vec[W⊤]is a vectorization of the
matrix W⊤. Let us fix the fine-tuning time length Tas1< τ≤T <∞. Since d > n , there
are infinitely many solutions to the problem of minimizing L(w). Thus, each of the finite length T
and the over-parameterization d > n implies that the initialization wt,0at the fine-tuning phase via
self-distillation plays an important role.
Letδ >0andt∈N0. We then define Ft={At(s) :s∈ S} ,where Sis a set of all training
datasets of size nsuch that with probability at least 1−δ, the training dataset sis inS. For each
training dataset s∈ S,At(s) =wt,Tis the final weight vector of the model after trounds of self-
distillation and Ttime of fine-tuning. Let us define the matrix Φ∈Rd×nbyΦij=φ(xj)i. We
assume that Φis of full rank; i.e., rank(Φ) = nsince d≥n. This is typically satisfied because if
rank(Φ) < n, there is some redundancy in the rows of the matrix Φ. Denote by [Ip⊗Φ]∈Rdp×np
the Kronecker product of the identity matrix Ip∈Rp×pand the matrix Φ. We write its singular
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2023
value decomposition by [Ip⊗Φ] = UΣV⊤where U= [u1, u2. . . , u dp]∈Rdp×dpcontains the
left-singular vectors ui∈Rdpfori∈ {1, . . . , dp }andΣ∈Rdp×npis a rectangular diagonal matrix
with Σii=σi∈R≥0fori∈ {1, . . . , np }andσ1≥σ2≥ ··· ≥ σnp≥0. Define Mto be
an upper bound on the loss as ℓ(w, x, y )≤M. Define Rto be an upper bound on the expected
norm of the features as Ex∥φ(x)∥2≤R. We assume that w0,0̸= 0; ifw0,0= 0, then the target
function in the self-distillation phase is always zero as f(xi, w0,0) = 0 for all i, which is unlikely
the case in practice. We define winit∈Rdpto be the weight before further pre-training and define
Y= vec[[ y1, . . . , y n]⊤]∈Rnp.
The following theorem shows that the generalization bound on the supervised loss ℓ(wt,T, x, y)of
the fine-tuning phase strictly decreases as we increase the number tof self-distillation rounds in the
further pre-training phase:
Theorem 1. There exists a constant c(that only depends on M) such that with probability at least
1−δ, the following holds:
Ex,y[ℓ(wt,T, x, y)]≤1
nnX
i=1ℓ(wt,T, xi, yi) +ζ(t)r
4c2R2p
n+Mr
ln(2/δ)
2n, (4)
where the function ζ(t)is strictly decreasing in t∈N0.
The proofs of all results in this section are presented in Appendix A. Moreover, the following theo-
rem shows that the tight upper bound on the distance between the initial weight winitand the final
weight wt,TafterTsteps of fine-tuning (i.e., ∥winit−wt,T∥2) strictly decreases as the number tof
self-distillation rounds increases:
Theorem 2. There is a function ψ:N0→R≥0such that (1) ∥winit−wt,T∥2=ψ(t)for some
winit∈Rdp, (2)∥winit−wt,T∥2≤ψ(t)for all winit∈Rdp, (3) the function ψ(t)is strictly decreas-
ing in t∈N0, (4) the function ψ(t)can be decomposed to ψ(t) =p
G1+ψ1(t) + 1{t= 0}B+G2
with constants G1, G2≥0intwhere ψ1(t)is strictly decreasing in t∈N0andB=Pdp
i=np+1(u⊤
iw0,0)2≥0.
Theorem 2 shows that the self-distillation acts as a regularizer on the distance between the initial
weight winitand the final weight wt,T. Since the Rademacher complexity of a set of vectors is
invariant to a shift by a constant vector, this distance has been shown to control the generalization
bound in previous papers in various models and settings, including deep neural networks (Bartlett &
Mendelson, 2002; Bartlett et al., 2017; Nagarajan & Kolter, 2019). This suggests that self-distillation
helps generalization via a regularization effect on the distance. Moreover, the first round of self-
distillation is expected to have the largest impact based on Theorem 2 since Theorem 2 shows that we
can completely remove the unnecessary component Bofw0,0in the first round of self-distillation.
We have verified these theoretical predictions in the experiments where we show the correlation
between the improvement via self-distillation and the distance that appeared in the generalization
bound in the previous paper (Nagarajan & Kolter, 2019).
5 E XPERIMENT
Dataset For image classification problem, we use six datasets — FGVC Aircraft (Aircraft) (Maji
et al., 2013), Caltech UCSD Birds 200 (CUB) (Wah et al., 2011), Chest X-ray (Kermany et al., 2018),
Describable Textures Dataset (DTD) (Cimpoi et al., 2014), Stanford Dogs (Khosla et al., 2011), and
Oxford 102 Flower (Nilsback & Zisserman, 2008). For text classification problem, we use four
datasets — Chemprot (Kringelum et al., 2016), ACL-ARC (Jurgens et al., 2018), SCIERC (Luan
et al., 2018), and Twitter-Emotion (Mohammad et al., 2018). Please see Appendix D for more detail.
Implementation Detail For the image classification problem, we use Vision Transformer pre-
trained on unlabeled ImageNet dataset with masked auto-encoding (He et al., 2022) and fine-tune
it on the downstream task with AdamW optimizer (Loshchilov & Hutter, 2019) for 10,000 steps
with batch size 32. Regarding further pre-training and self-distillation, we continue to pre-train the
model for 20,000 steps with batch size 64. We evaluate the Vision Transformers with accuracy.
For text classification, following the experimental setup from Gururangan et al. (2020), we use
RoBERTA (Liu et al., 2019) as a backbone network and fine-tune it on the target labeled dataset
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2023
Table 1: Average and standard deviation of accuracy with 5 runs for image classification datasets.
Method Aircraft CUB Chest X-ray DTD Dogs Flower
Fine-tuning 72.33±1.13 55 .55±0.54 77 .15±0.52 67 .56±0.52 62 .53±0.57 88 .78±0.65
RecAdam 70.76±1.25 55 .22±1.29 77 .29±1.32 67 .59±1.03 61 .65±0.92 88 .97±0.44
MARS 72.74±0.57 55 .35±0.73 77 .28±1.80 66 .79±0.90 62 .24±0.96 87 .93±1.21
R3F 72.95±0.46 55 .91±0.79 76 .86±0.97 65 .32±1.25 62 .15±0.48 88 .92±0.78
Further Pre-training 73.38±0.64 55 .72±0.46 77 .79±2.06 65 .55±1.12 62 .34±0.39 88 .63±0.35
Self-Distillation 74.37 ±0.43 58.06 ±0.90 79.68 ±1.05 68.51 ±0.51 63.55 ±0.39 90.28 ±0.44
Table 2: Average and standard deviation of F1 score with 5 runs for text classification datasets.
Method SCIERC ACL-ARC Chemprot Twitter-Emotion
Fine-tuning 76.63±2.06 64 .09±4.13 80 .59±1.15 77 .61±0.83
RecAdam 79.45±1.92 59 .70±2.68 82.73±0.28 78.26±0.88
MARS 74.69±1.25 53 .57±6.64 80 .18±0.92 77 .48±1.69
R3F 75.61±2.49 60 .13±2.55 79 .25±2.16 77 .79±0.81
Further Pre-training 80.32±1.25 69 .73±2.40 82 .33±0.46 78 .71±0.40
Self-Distillation 81.79 ±0.75 73.17 ±2.19 82.87 ±0.35 79.77 ±0.79
with AdamW optimizer for 10 epochs with batch size 32. In terms of further pre-training and self-
distillation, we further pre-train RoBERTA for 100 epochs with batch size 128. We evaluate the
models with macro F1 for SCIERC, ACL-ARC, and Twitter-Emotion dataset, and micro F1 for
Chemprot dataset.
Baselines We compare our method against the following baselines targeting for fine-tuning pre-
trained models. All the models are initialized with the pre-trained weights θinit andϕinit.
1.Fine-tuning : The model fine-tuned on target labeled dataset Dtrwithout any further pre-training
or regularization except dropout and weight decay.
2.RecAdam (Chen et al., 2020a) : The model trained with RecAdam optimizer which is a variant
of Adam optimizer (Kingma & Ba, 2015) and additionally penalizes L2distance between the
fine-tuned and the initial pre-trained weight.
3.MARS (Gouk et al., 2021) : The model trained to minimize the cross-entropy loss along with the
regularization projecting the fine-tuned weight to lie within a sphere centered on the initial pre-
trained weights. For each layer, the distance induced by Maximum Absolute Row Sum (MARS)
matrix norm (max jP
i=1|Wj,i−Uj,i|)is used for the regularization.
4.R3F (Aghajanyan et al., 2021) : The model trained to minimize the cross-entropy loss as well
as symmetric KL-divergence between softmax output of the original input and that of the input
perturbed by Gaussian noise.
5.Further Pre-training (Gururangan et al., 2020) : Task adaptive pre-training where we further
pre-train the model on the unlabeled target dataset Duwith masked auto-encoding objective and
fine-tune it on the target labeled dataset Dtr.
6.Self-Distillation : This is our model which is further pre-trained on unlabeled target dataset Du
with equation 3 and fine-tuned on the target labeled dataset Dtr.
5.1 M AINRESULTS
As shown in Table 1, self-distillation consistently outperforms all the regularization methods and the
further pre-training method on image datasets. Notably, our method significantly improves the per-
formance of the Chest X-ray dataset consisting of grey-scaled images for diagnosis of pneumonia.
In addition, self-distillation effectively tackles the Flower dataset which contains only 2,040 labeled
examples. In contrast, the other baselines do not show consistent improvement across all the image
datasets. For instance, further pre-training is effective for the Aircraft dataset, but significantly de-
grades the test accuracy on the DTD dataset. Regularization methods such as RecAdam, MARS,
and R3F barely improve generalization performance on most datasets or underperform the simple
fine-tuning strategy on certain datasets. This empirical evidence supports that the regularizations
enforcing the fine-tuned models close to the initial pre-trained weight are not effective for adapting
a pre-trained model to the target datasets of specific domains.
Furthermore, as shown in Table 2, we provide additional experimental results for text classification
tasks. Again, self-distillation significantly outperforms all of the baselines across all four datasets,
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2023
1K 2.5K 5K 10K 20K 30K 40K 50K
# of instances020406080Test AccuracyFine-tuning
Further Pre-training
Self-Distillation
Figure 3: Accuracy with varying
the number of training data.Table 3: Ablation on CUB and SCIERC.
Model CUB SCIERC
Full Model 58.06±0.90 81.79 ±0.75
w/oLMAE 57.60±0.81 80 .66±0.62
w/oLDistill 55.72±0.46 80 .32±1.25
Further Pre-train ×2 53 .41±0.75 80 .52±0.98
Prediction-Matching 55.27±1.07 81 .09±1.07
Weight-Matching ( ℓ2) 53.70±0.91 80 .54±1.21
Weight-Matching (MARS) 54.82±0.60 80 .95±0.71
0 2 4
Round of Self-Distillation ( t)88.589.089.590.090.591.0Test Accuracy
Figure 4: Test accuracy with
varying self-distillation round.
except RecAdam in the Chemprot dataset. In contrast to the previous experiment, the further pre-
training method improves the test F1 score of the simple fine-tuning method, yet it still underper-
forms our model. For regularization methods — RecAdam, MARS, and R3F, they do not achieve
consistent improvement across all three datasets. RecAdam moderately improves the F1 score on the
SCIERC and Chemprot dataset but significantly degrades the generalization performance on ACL-
ARC dataset. Both MARS and R3F show poor performance on SCIERC and ACL-ARC datasets,
and their performance slightly is worse than Fine-tuning method on the Chemprot dataset.
Result for Low Resource Data We further perform experiments to show how self-distillation
effectively handles low resources of labeled data. Given a full CIFAR-100 dataset (Krizhevsky
et al., 2009) which contains 50,000 training pairs of an image and corresponding label, we plot
the test accuracy of each model by varying the number of training instances. Note that we also
reduce the number of unlabeled images used for further pre-training or self-distillation. As shown in
Figure 3, self-distillation consistently improves the generalization performance of both fine-tuning
method and the model which is further pre-trained on the images from the CIFAR-100 dataset.
Notably, the gain by self-distillation becomes larger when the models are trained with an extremely
small number of instances. For example, self-distillation achieves 13% and6%improvement of test
accuracy compared to the model with simple fine-tuning when there are 1,000 and 2,500 labeled
examples, respectively. These empirical results verify that self-distillation can effectively adapt the
pre-trained model to the target dataset even if there are extremely small amounts of labeled data.
Ablation Study We perform ablation study to verify the effectiveness of each component of
self-distillation. In Table 3, we show empirical results on both the CUB dataset and SCI-
ERC data set while removing or replacing various components of self-distillation. Firstly, we
remove masked auto-encoding objective LMAEand train the model with only distillation loss
LDistill before fine-tuning. On image dataset CUB, it does not make a significant differ-
ence, however, removing the masked auto-encoding objective degrades the generalization per-
formance of the language model on text classification dataset SCIERC. Alternatively, we re-
move the distillation loss LDistill in equation 3, which results in further pre-training method.
Furthermore, we continue to pre-train the model for twice longer steps as the original further
pre-training method, denoted as Further Pre-train ×2, to show that higher test accuracy of self-
distillation is not a consequence of longer pre-training. Both of the models significantly un-
derperform self-distillation, which shows the effectiveness of the self-distillation loss. Lastly,
we perform experiments for variants of distillation loss LDistill in equation 3. Instead of
matching representation of teacher and student, we enforce the reconstruction of masked in-
puts by teacher and student to be consistent, i.e., minimize θ,ϕ∥gϕ◦fθ(ˆx)−gϕ0◦fθ0(ˆx)∥2
2for
ViT or minimize θ,ϕPT
t=1DKL(pθ0,ϕ0(xt|ˆx)∥pθ,ϕ(xt|ˆx))for RoBERTA, denoted as Prediction-
Matching. Furthermore, we replace the distillation loss with the one minimizing L2or MARS
distance between the parameters of student and teacher, denoted as Weight-Matching. As shown in
Table 3, all these variants are not effective compared to the one minimizing the distance between
hidden representations of the student and teacher.
Multi-Round of Self-Distillation Lastly, we empirically show that the first round of self-
distillation plays the most significant role in improving generalization performance. Specifically,
we fine-tune each model after tround of self-distillation and plot the test accuracy on Oxford 102
Flower dataset, where 0round of self-distillation (t= 0) denotes the model with further pre-training.
As shown in Figure 4, the first round of self-distillation significantly improves the test accuracy of
the model with further pre-training and the gain by self-distillation becomes marginal after the first
round. Considering the extra computational cost and marginal improvement of multi-round self-
distillation, we perform a single round of self-distillation for all the experiments.
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2023
Aircraft CUB DTD Dogs X-ray Flower0.00.51.01.52.02.5Generalization GapFurther Pre-train
Self-Distillation
(a)
Aircraft CUB DTD Dogs X-ray Flower020406080100120140L2distance
Further Pre-train
Self-Distillation (b)
0 1 2 3 4
Round of Self-Distillation ( t)100110120130/bardblθinit−θt,T/bardbl2
 (c)
Figure 5: (a) Generalization gap : Difference between supervised test loss and training loss. (b) Effect of
self-distillation on distance: Distance between the initial pre-trained weights and the final fine-tuned weights
of further pre-training and self-distillation. (c) Effect of multi-round self distillation: Distance between the
initial pre-trained weights and the final fine-tuned weights for each round of self-distillation t∈N+.
5.2 F URTHER ANALYSIS
In this subsection, we present numerical experiments to analyze why self-distillation can potentially
help improve the generalization performance of downstream tasks compared to further pre-training
and empirically show that Theorem 1 and 2 can be extended to deep neural networks — transformers.
(a) Generalization gap: In Figure 5a, we plot the generalization gap, which is test loss minus train-
ing loss on each labeled dataset, of self-distillation and further pre-training method. Self-distillation
improves the generalization gap of the further pre-training method across all the datasets. It is con-
sistent with Theorem 1 showing that self-distillation with a simplified model strictly decreases the
generalization bound on the supervised loss of the fine-tuning stage.
(b) Effect of self-distillation on distance: To empirically validate Theorem 2 about regularization
effects by self-distillation on L2distance between the initial pre-trained weight θinit and the final
weight after fine-tuning, we plot the distance obtained from self-distillation and further pre-training.
Specifically, we compare the distance ∥θinit−θ1,T∥2and∥θinit−θ0,T∥2, where θt,τis the pa-
rameter after tround of self-distillation and τsteps of gradient descent for fine-tuning. As shown
in Figure 5b, self-distillation consistently decreases the distance and the reduced distance correlates
with the better generalization gap in Figure 5a. These empirical results confirm the connection be-
tween the L2distance from the initialization and generalization bound (Nagarajan & Kolter, 2019).
(c) Effect of multi-round self-distillation: Lastly, we empirically verify part of Theorem 2 which
shows that the first round of self-distillation plays the most critical role of regularization on the
L2distance between the initial pre-trained weight θinit and the final weight θt,Tdenoted as the
parameter after tround of self-distillation and Tsteps of gradient descent for fine-tuning on VGG
flower 102 dataset. As shown in Figure 5c, self-distillation significantly decreases the distance at the
first round (t= 1) and the regularization effect on the distance diminishes afterward, where 0round
of self-distillation (t= 0) denotes the model with further pre-training but without self-distillation.
6 C ONCLUSION
To effectively adapt pre-trained transformers to a target domain, we proposed self-distillation as a
regularization for further pre-training. Specifically, we first took the initial pre-trained transformer
and continued to pre-train it with the masked auto-encoding objective on the target unlabeled dataset
and considered the encoder part of the model as a teacher for self-distillation. Then we took the
copy of the same initial pre-trained model as a student and enforced representations of the student
to be close to those of the teacher while optimizing the student with the masked auto-encoding
objective on the target unlabeled dataset. Finally, we fine-tuned the self-distilled student on the
target labeled dataset. Our empirical evaluation on various image and text classification benchmark
datasets showed that self-distillation consistently improved generalization performance compared
to relevant baselines. Lastly, we provided the theoretical analysis of the proposed method with
a simplified model to understand how self-distillation for further pre-training can potentially help
improve the generalization performance of the downstream tasks.
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2023
REPRODUCIBILITY STATEMENT
We use Pytorch (Paszke et al., 2019) and transformers library (Wolf et al., 2020) from Huggingface
to implement all the baselines and our proposed method in the experiments. We have described our
method of self-distillation for further pre-training in Algorithm 1 and specified all the experimental
setup including hyperparameters in Section 5 and Appendix E. For theoretical analysis, we have
provided all the proofs in Appendix A.
ACKNOWLEDGMENTS
This work was supported by Institute of Information & communications Technology Planning &
Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-00075, Artificial In-
telligence Graduate School Program(KAIST)), the Engineering Research Center Program through
the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF-
2018R1A5A1059921), Institute of Information & communications Technology Planning & Evalua-
tion (IITP) grant funded by the Korea government(MSIT) (No. 2021-0-02068, Artificial Intelligence
Innovation Hub), Institute of Information & communications Technology Planning & Evaluation
(IITP) grant funded by the Korea government(MSIT) (No. 2022-0-00184, Development and Study
of AI Technologies to Inexpensively Conform to Evolving Policy on Ethics), Institute of Informa-
tion & communications Technology Planning & Evaluation (IITP) grant funded by the Korea gov-
ernment(MSIT) (No.2022-0-00713), KAIST-NA VER Hypercreative AI Center, and Samsung Elec-
tronics (IO201214-08145-01). This material is based upon work supported by the Google Cloud
Research Credits program with the award (6NW8-CF7K-3AG4-1WH1).
REFERENCES
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal
Gupta. Better fine-tuning by reducing representational collapse. In International Conference on
Learning Representations , 2021.
Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational
information distillation for knowledge transfer. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pp. 9163–9171, 2019.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research , 3(Nov):463–482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. Advances in neural information processing systems , 30, 2017.
Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text.
InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,
pp. 3615–3620, 2019.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of
the IEEE/CVF International Conference on Computer Vision , pp. 9650–9660, 2021.
Sanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che, Ting Liu, and Xiangzhan Yu. Recall and
learn: Fine-tuning deep pretrained language models with less forgetting. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pp. 7870–
7881, 2020a.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning ,
pp. 1597–1607. PMLR, 2020b.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2023
Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 15750–15758, 2021.
M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In
Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of NAACL-HLT , pp. 4171–
4186, 2019.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. In International Conference on Learning Representations , 2021.
Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural networks. In International Conference on Machine Learning , pp. 1607–1616.
PMLR, 2018.
Henry Gouk, Timothy Hospedales, and Massimiliano Pontil. Distance-based regularisation of deep
networks for fine-tuning. In International Conference on Learning Representations , 2021.
Jean-Bastien Grill, Florian Strub, Florent Altch ´e, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural
information processing systems , 33:21271–21284, 2020.
Suchin Gururangan, Ana Marasovi ´c, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pp.
8342–8360, 2020.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition , pp. 9729–9738, 2020.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll ´ar, and Ross Girshick. Masked au-
toencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pp. 16000–16009, 2022.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. NIPS
2014 Deep Learning Workshop , 2014.
David Jurgens, Srijan Kumar, Raine Hoover, Dan McFarland, and Dan Jurafsky. Measuring the
evolution of a scientific field through citation frames. Transactions of the Association for Com-
putational Linguistics , 6:391–406, 2018.
Minki Kang, Moonsu Han, and Sung Ju Hwang. Neural mask generator: Learning to generate
adaptive word maskings for language model adaptation. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Processing (EMNLP) , pp. 6102–6120, 2020.
Kenji Kawaguchi. On the theory of implicit deep learning: Global convergence with implicit layers.
InInternational Conference on Learning Representations , 2021.
Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L
Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al. Identifying medical diag-
noses and treatable diseases by image-based deep learning. Cell, 172(5):1122–1131, 2018.
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-
grained image categorization. In First Workshop on Fine-Grained Visual Categorization, IEEE
Conference on Computer Vision and Pattern Recognition , Colorado Springs, CO, June 2011.
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2023
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural
Information Processing Systems , 33:18661–18673, 2020.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations , 2015.
Jens Kringelum, Sonny Kim Kjaerulff, Søren Brunak, Ole Lund, Tudor I Oprea, and Olivier
Taboureau. Chemprot-3.0: a global chemical biology diseases mapping. Database , 2016, 2016.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. arXiv preprint , 2009.
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jae-
woo Kang. BioBERT: a pre-trained biomedical language representation model for biomedical
text mining. Bioinformatics , 36(4):1234–1240, 2020.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 , 2019.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations , 2019.
Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. Multi-task identification of entities,
relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing , pp. 3219–3232, 2018.
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained
visual classification of aircraft. arXiv preprint arXiv:1306.5151 , 2013.
Andreas Maurer. A vector-contraction inequality for rademacher complexities. In International
Conference on Algorithmic Learning Theory , pp. 3–17. Springer, 2016.
Hossein Mobahi, Mehrdad Farajtabar, and Peter Bartlett. Self-distillation amplifies regularization in
hilbert space. Advances in Neural Information Processing Systems , 33:3351–3361, 2020.
Saif Mohammad, Felipe Bravo-Marquez, Mohammad Salameh, and Svetlana Kiritchenko. Semeval-
2018 task 1: Affect in tweets. In Proceedings of the 12th international workshop on semantic
evaluation , pp. 1–17, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Generalization in deep networks: The role of distance from
initialization. arXiv preprint arXiv:1901.01672 , 2019.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number
of classes. In Indian Conference on Computer Vision, Graphics and Image Processing , Dec 2008.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. Advances in neural information processing systems , 32, 2019.
Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu,
Yi-Ting Chen, Minh-Thang Luong, Yonghui Wu, et al. Combined scaling for open-vocabulary
image classification. arXiv preprint arXiv:2111.10050 , 2021.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. In International Conference on Learning
Representations , 2014.
Dawei Sun, Anbang Yao, Aojun Zhou, and Hao Zhao. Deeply-supervised knowledge synergy.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.
6997–7006, 2019.
Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. ERNIE
2.0: A continual pre-training framework for language understanding. In Proceedings of the AAAI
Conference on Artificial Intelligence , pp. 8968–8975, 2020.
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2023
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-UCSD birds-200-2011. Tech-
nical report, California Institute of Technology, 2011.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, R ´emi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art
natural language processing. In Proceedings of the 2020 conference on empirical methods in
natural language processing: system demonstrations , pp. 38–45, 2020.
Chenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan L Yuille. Training deep neural networks in gen-
erations: A more tolerant teacher educates better students. In Proceedings of the AAAI Conference
on Artificial Intelligence , pp. 5628–5635, 2019.
Qinyuan Ye, Belinda Z Li, Sinong Wang, Benjamin Bolte, Hao Ma, Wen-tau Yih, Xiang Ren, and
Madian Khabsa. On the influence of masking policies in intermediate pre-training. In Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing , pp. 7190–7202,
2021.
Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St ´ephane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. In International Conference on Machine Learning , pp. 12310–
12320. PMLR, 2021.
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your
own teacher: Improve the performance of convolutional neural networks via self distillation. In
Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 3713–3722,
2019.
Linfeng Zhang, Xin Chen, Junbo Zhang, Runpei Dong, and Kaisheng Ma. Contrastive deep super-
vision. In European Conference on Computer Vision , pp. 1–19. Springer, 2022.
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2023
APPENDIX
A P ROOFS
We also define the model output vector ft∈Rn×pby(ft)ij=f(xi, wt)j. For example, f0
is the initial teacher label matrix. Let [n] ={1, . . . , n }. Denote the rank of [Ip⊗Φ]byr=
rank([ Ip⊗Φ])≤np. Define ˜U= [u1, u2. . . , u r]∈Rdp×randPr=I−˜U˜U⊤, which is the
projection matrix onto the null space of ˜U⊤. We first prove the following lemma, which will be used
in the proofs of Theorem 1 and Theorem 2 later:
Lemma 1. For any t∈N0,wt,0=Pr
i=1αi,t˜ui+ 1{t= 0}v, where αi,t=1
σi
1
1+(nλ/σ2
i)t
∈R,
˜ui= ˜yiui∈Rdp,˜yi= (V⊤vec[f0])i∈R, and v=Prw0,0.
Proof of Lemma 1. Define wt:=wt,0fort∈N+. The necessary condition on the solution wtat
steptis that ∇L(wt) = 0 . Thus, by solving ∇L(wt) = 0 forwt, we have that wt= ([Ip⊗Φ][Ip⊗
Φ]⊤+nλI)−1[Ip⊗Φ] vec[ ft−1]. By using the singular value decomposition [Ip⊗Φ] = UΣV⊤,
since UU⊤=U⊤U=IandV⊤V=I, we have that
wt= (UΣΣ⊤U⊤+nλI)−1UΣV⊤vec[ft−1] = (U(ΣΣ⊤+nλI)U⊤)−1UΣV⊤vec[ft−1]
=U(ΣΣ⊤+nλI)−1ΣV⊤vec[ft−1].
Therefore, wt=U(ΣΣ⊤+nλI)−1ΣV⊤vec[ft−1]. Using this and [Ip⊗Φ] = UΣV⊤,
vec[ft] = vec[Φ⊤W⊤
tIp] = [Ip⊗Φ]⊤wt= [Ip⊗Φ]⊤U(ΣΣ⊤+nλI)−1ΣV⊤vec[ft−1]
=VΣ⊤(ΣΣ⊤+nλI)−1ΣV⊤vec[ft−1].
Therefore, vec[ft] =V AV⊤vec[ft−1]where A= Σ⊤(ΣΣ⊤+nλI)−1Σ. Repeating this process
forvec[ft−1], since V⊤V=I,
vec[ft] =V AV⊤V AV⊤···V AV⊤vec[f0] =V AtV⊤vec[f0].
Plugging this equation of vec[ft−1] =V At−1V⊤vec[f0]into the equation of wt=U(ΣΣ⊤+
nλI)−1ΣV⊤vec[ft−1], we have that
wt=U(ΣΣ⊤+nλI)−1ΣV⊤V At−1V⊤vec[f0] =UBAt−1V⊤vec[f0]
where B= (ΣΣ⊤+nλI)−1Σ. Here, we can rewrite the matrix B∈Rdp×npas
B=¯B
0(dp−np)×np
where 0(dp−np)×npis the (dp−np)bynpmatrix with all entries being zero, and ¯B∈Rnp×npis a
diagonal matrix defined by
¯Bii:=σi(σ2
i+nλ)−1.
Using this Bin the above equation of wt=UBAt−1V⊤vec[f0],
wt=U¯B
0(dp−np)×np
At−1V⊤vec[f0]
= [u1u2··· udp]¯BAt−1
0(dp−np)×np
V⊤vec[f0]
=¯U¯BAt−1V⊤vec[f0]
where ¯U= [u1, u2. . . , u np]∈Rdp×np. Since the matrix A= Σ⊤(ΣΣ⊤+nλI)−1Σ∈Rnp×npis
a diagonal matrix with its entry being Aii=σ2
i(σ2
i+nλ)−1,this can be further simplified as
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2023
wt=¯U¯BAt−1V⊤vec[f0]
=¯U
σ1
1
σ2
1+nλ
...
σ1
np
σ2np+nλ

σ2
1
σ2
1+nλ
...
σ2
np
σ2np+nλ
t−1
˜y1
˜y2
...
˜ynp

= [u1u2··· unp]
σ1(σ2
1+nλ)−1(σ2
1(σ2
1+nλ)−1)t−1˜y1
σ2(σ2
2+nλ)−1(σ2
2(σ2
2+nλ)−1)t−1˜y2
...
σnp(σ2
np+nλ)−1(σ2
np(σ2
np+nλ)−1)t−1˜ynp

=npX
i=1σi(σ2
i+nλ)−1(σ2
i(σ2
i+nλ)−1)t−1˜yiui
=rX
i=1σi(σ2
i+nλ)−1(σ2
i(σ2
i+nλ)−1)t−1˜yiui
where the last line follows from the fact that σi(σ2
i+nλ)−1(σ2
i(σ2
i+nλ)−1)t−1= 0for all i > r .
Since σi(σ2
i+nλ)−1(σ2
i(σ2
i+nλ)−1)t−1=σ2t−1
i(σ2
i+nλ)−t=1
σi(σ2
i
σ2
i+nλ)tfori≤r, this
implies that
wt=rX
i=11
σiσ2
i
σ2
i+nλt
˜yiui=rX
i=11
σi1
1 + (nλ/σ2
i)t
˜yiui
Since t∈N+was arbitrary, this holds for any t∈N+. This proves the first statement of the theorem
for any t∈N+. For t= 0, since
˜yi= (V⊤vec[f0])i= (V⊤[Ip⊗Φ]⊤w0,0)i= (V⊤VΣ⊤U⊤w0,0)i= (Σ⊤U⊤w0,0)i=σiu⊤
iw0,0,
we have that
rX
i=11
σi1
1 + (nλ/σ2
i)t
˜yiui= rX
i=1uiu⊤
i!
w0,0=˜U˜U⊤w0,0.
Thus,
w0,0=˜U˜U⊤w0,0+ (I−˜U˜U⊤)w0,0=rX
i=11
σi1
1 + (nλ/σ2
i)t
˜yiui+ (I−˜U˜U⊤)w0,0.
Since (I−˜U˜U⊤)w0,0=Prw0,0, this completes the first statement of the theorem for any t∈N0.
A.1 P ROOF OF THEOREM 1
Proof. Define Z:= [Ip⊗Φ]⊤∈R¯n×¯dwhere ¯n=npand¯d=dp. Then,
L(w) =1
2nX
i=1∥f(xi, w)−yi∥2
2=1
2∥Zw−Y∥2
2
where Y= vec[[ y1, . . . , y n]⊤]∈R¯n. Since ∇L(wt,τ) =Z⊤(Zwt,τ−Y),
dwt,τ
dτ=−Z⊤(Zwt,τ−Y)
Since rank(Φ) = nandd≥n, we have rank( Z) = ¯nby the property of the Kronecker product
with the identity matrix. Since rank( Z) = ¯n, there exists v∈R¯dsuch that Y=Zv. Thus,
dwt,τ
dτ=−Z⊤(Zwt,τ−Zv)
=−Z⊤Z(wt,τ−v)
=−Z⊤Z(wt,τ−v).
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2023
Since Z⊤=UΣV⊤, we have Z⊤Z=UΣΣ⊤U⊤=P¯n
i=1σ2
iuiu⊤
i. Thus,
dwt,τ
dτ=− ¯nX
i=1σ2
iuiu⊤
i!
(wt,τ−v) =−¯nX
i=1σ2
iuiu⊤
i(wt,τ−v).
Since the columns of Uforms the basis of R¯dandw, v∈R¯d, we can write wt,τ=P¯d
k=1c(t,τ)
kuk
andv=P¯d
k=1qkukfor some c(t,τ)
kandqk. Thus,
dwt,τ
dτ=−¯nX
i=1σ2
iuiu⊤
i¯dX
k=1(c(t,τ)
k−qk)uk
=−¯nX
i=1¯dX
k=1σ2
i(c(t,τ)
k−qk)uiu⊤
iuk
=−¯nX
i=1σ2
i(c(t,τ)
i−qi)ui.
Using wt,τ=P¯d
k=1c(t,τ)
kukfor the right-hand side too, we have that
d
dτ¯dX
i=1c(t,τ)
iui=−¯nX
i=1σ2
i(c(t,τ)
i−qi)ui.
This implies that for all i∈ {1, . . . , ¯n},
d
dτc(t,τ)
i=−σ2
i(c(t,τ)
i−qi),
andd
dτc(t,τ)
i= 0for all i /∈ {1, . . . , ¯n}. This can be also seen by the fact thatdwt,τ
dτ=−Z⊤(Zwt,τ−
Zv)withZ⊤=UΣV⊤and thus the dynamics only adds components of uifori∈ {1, . . . , ¯n}, and
not for i /∈ {1, . . . , ¯n}. Thus, for components of uifori /∈ {1, . . . , ¯n}, the initial values stays. In
other words, for i /∈ {1, . . . , ¯n},
c(t,τ)
i=c(t,0)
i.
On the other hand, for i∈ {1, . . . , ¯n}, sinced
dτqi= 0,
d
dτ(c(t,τ)
i−qi) =d
dτc(t,τ)
i=−σ2
i(c(t,τ)
i−qi).
Solving this for (c(t,τ)
i−qi), we have that for i∈ {1, . . . , ¯n},
c(t,τ)
i−qi= (c(t,0)
i−qi)e−σ2
iτ.
This implies that
c(t,τ)
i=qi+ (c(t,0)
i−qi)e−σ2
iτ=qi(1−e−σ2
iτ) +c(t,0)
ie−σ2
iτ.
Combining these with wt,T=P¯d
k=1c(t,T)
kuk,
wt,T=¯dX
i=1c(t,T)
iui=¯nX
i=1qi(1−e−σ2
iT)ui+¯nX
i=1c(t,0)
ie−σ2
iTui+¯dX
i=¯n+1c(t,0)
iui. (5)
Therefore, for any particular s∈ S, since U= [u1, u2. . . , u dp]∈Rdp×dpis an orthogonal matrix,
∥At(s)∥2
2=∥wt,T∥2
2≤¯nX
i=1
qi(1−e−σ2
iT)2
+¯nX
i=1(c(t,0)
i)2e−2σ2
iT+¯dX
i=¯n+1(c(t,0)
i)2. (6)
where qi, σi, and c(t,0)
iall depend on s.
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2023
By using Lemma 4 of (Pham et al., 2021) and taking union bound with P(s /∈ S)≤δ, with
probability at least 1−δ, we have that wt,T∈ Ftand the following holds:
Ex,y[ℓ(wt,T, x, y)]≤1
nnX
i=1ℓ(wt,T, xi, yi) + 2Rn(Ft) +Mr
ln(2/δ)
2n, (7)
where Rn(Ft) =Es,ξ[supw∈Ft1
nPn
i=1ξi∥Wφ(xi)−yi∥2
2],s= ((xi, yi))n
i=1,w= vec[ W⊤], and
ξ1, . . . , ξ nare independent uniform random variables taking values in {−1,1}. By using Corollary
4 of (Maurer, 2016), there exits a constant c(only depending on M) such that,
Rn(Ft)≤c
nEs,ξ[ sup
w∈FtnX
i=1pX
k=1ξikWkφ(xi)]
=c
nEs,ξ[ sup
w∈FtpX
k=1WknX
i=1ξikφ(xi)]
=c
nEs,ξ[ sup
w∈Ftw⊤h]
where Wkis the k-th row of W,ξikare independent uniform random variables taking values in
{−1,1},h= vec[ H]∈Rdp, and H∈Rd×pwithHjk=Pn
i=1ξikφ(xi)j. Thus,
Rn(Ft)≤c
nEs,ξ[ sup
w∈Ft∥w∥2∥h∥2] =c(supw∈Ft∥w∥2)
nEs,ξ[∥h∥2]
Here,
Es,ξ[∥h∥2] =Es,ξvuutdX
j=1pX
k=1 nX
i=1ξikφ(xi)j!2
≤vuutdX
j=1pX
k=1Es,ξ nX
i=1ξikφ(xi)j!2
=vuutdX
j=1pX
k=1EsnX
i=1(φ(xi)j)2 (8)
=vuutpX
k=1nX
i=1EsdX
j=1(φ(xi)j)2
=vuutpX
k=1nX
i=1Es∥φ(xi)∥2
2
≤R√pn
Equation 8 holds since
Es,ξ[(ξikϕ(xi)j)·(ξlkϕ(xl)j)] =Es[ 1{i=l}ϕ(xi)jϕ(xl)j]
for all i, l∈[n].
Thus,
Rn(Ft)≤cR√p(supw∈Ft∥w∥2)√n. (9)
Define
ζt(s):=vuut¯nX
i=1 
qi(1−e−σ2
iT)2+¯nX
i=1(c(t,0)
i)2e−2σ2
iT+¯dX
i=¯n+1(c(t,0)
i)2.
where qi, σi, and c(t,0)
iall depend on s. With this, we define
ζ(t):= sup
s∈Sζt(s).
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2023
Then, by combining equation 6, equation 7, and equation 9, with probability at least 1−δ, the
following holds:
Ex,y[ℓ(wt,T, x, y)]≤1
nnX
i=1ℓ(wt,T, xi, yi) +ζ(t)r
4c2R2p
n+Mr
ln(2/δ)
2n.
Finally, from Lemma 1, for any t∈N0andi∈ {1, . . . , ¯n},
(c(t,0)
i)2= 
1
σi1
1 + (nλ/σ2
i)t
˜yi!2
.
Since1
1+(nλ/σ2
i)<1(because nλ/σ2
i>0), the value of
1
1+(nλ/σ2
i)2t
strictly decreases as t
increases. Since1
σ2
i>0and˜y2
i≥0, this implies that (c(t,0)
i)2is strictly decreasing in t∈N0unless
c(t,0)
i= 0. Moreover, from Lemma 1, we have that
wt,0=¯nX
i=1αi,t˜yiui+ 1{t= 0}(I−˜U˜U⊤)w0,0.
Since{u1, . . . , u ¯d}is a orthonormal basis for R¯dwith inner product ⟨x, y⟩=y⊤x, we get
w0,0=¯dX
i=1(u⊤
iw0,0)ui.
Since ˜U˜U⊤w0,0=P¯n
i=1(u⊤
iw0,0)ui, we have that
(I−˜U˜U⊤)w0,0=¯dX
i=1(u⊤
iw0,0)ui−¯nX
i=1(u⊤
iw0,0)ui=¯dX
i=¯n+1(u⊤
iw0,0)ui,
which implies that the uicomponent to span wt,0fori∈ {¯n+ 1, . . . , ¯d}is only present in (I−
˜U˜U⊤)w0,0. In other words,
wt,0=¯nX
i=1αi,t˜yiui+¯dX
i=¯n+11{t= 0}(u⊤
iw0,0)ui.
Thus, for any t∈N0andi∈ {¯n+ 1, . . . , ¯d}, we have that
(c(t,0)
i)2= 1{t= 0}(u⊤
iw0,0)2.
These implies that ζ(t)is strictly decreasing in t∈N0unless w0,0= 0.
A.2 P ROOF OF THEOREM 2
Proof. In this proof, we continue to use the results and the notation from the proof of Theorem 1.
By using equation 5 in the proof of Theorem 1, we have that
∥winit−wt,T∥2=∥winit−vt∥2,
where
vt=¯nX
i=1qi(1−e−σ2
iT)ui+¯nX
i=1c(t,0)
ie−σ2
iTui+¯dX
i=¯n+1c(t,0)
iui.
Ifwinit=−αvtfor some α >0, then
∥winit−vt∥2=∥vt+αvt∥2
= (1 + α)∥vt∥2
=∥vt∥2+∥αvt∥2
=vuut¯nX
i=1q2
i(1−e−σ2
iT)2+¯nX
i=1(c(t,0)
i)2e−2σ2
iT+¯dX
i=¯n+1(c(t,0)
i)2+∥winit∥2.
18

--- PAGE 19 ---
Published as a conference paper at ICLR 2023
On the other hand, for any winit∈Rdp,
∥winit−vt∥2≤ ∥vt∥2+∥winit∥2
≤vuut¯nX
i=1q2
i(1−e−σ2
iT)2+¯nX
i=1(c(t,0)
i)2e−2σ2
iT+¯dX
i=¯n+1(c(t,0)
i)2+∥winit∥2.
Thus, setting ψ(t)to be the following function satisfies conditions (1) and (2) in the statement:
ψ(t):=vuut¯nX
i=1q2
i(1−e−σ2
iT)2+¯nX
i=1(c(t,0)
i)2e−2σ2
iT+¯dX
i=¯n+1(c(t,0)
i)2+∥winit∥2
Finally, from Lemma 1, for any t∈N0andi∈ {1, . . . , ¯n},
(c(t,0)
i)2= 
1
σi1
1 + (nλ/σ2
i)t
˜yi!2
.
which is strictly decreasing in t∈N0unless c(t,0)
i= 0for all i∈ {1, . . . , ¯n}as shown in the proof
of Theorem 1. Moreover, from Lemma 1, for any t∈N0andi∈ {¯n+ 1, . . . , ¯d},
(c(t,0)
i)2= 1{t= 0}(u⊤
iw0,0)2.
That is,
ψ(t) =vuutG1+ψ1(t) +¯dX
i=¯n+11{t= 0}(u⊤
iw0,0)2+G2,
where
G1:=¯nX
i=1q2
i(1−e−σ2
iT)2,
ψ1(t):=¯nX
i=1 
1
σi1
1 + (nλ/σ2
i)t
˜yi!2
e−2σ2
iT,
and
G2:=∥winit∥2.
Since e−2σ2
iT>0is a constant in tand we have previously shown that
1
σi
1
1+(nλ/σ2
i)t
˜yi2
is strictly decreasing in t∈N0unless w0,0= 0. It implies that both ψ1(t)andψ(t)are strictly
decreasing in t∈N0unless w0,0= 0.
Remark 1. Note that Theorem 2 also shows the distance between the weight of the teacher wt−1,T
and the initial pre-trained weight winitfor all t∈N+. Since the teacher at t′∈N+round of
self-distillation used to be a student of the t′−1round of the self-distillation and Theorem 2 holds
for all non-negative integer t∈N0, the distance between the initial weight and the teacher weight
∥winit−wt−1,T∥2strictly decreases for all t∈N+. For instance, at t= 1, we obtain the following
inequality
∥winit−w0,T∥2≤ψ(0),
where w0,Tis the weight of the initial teacher without self-distillation .
B A DDITIONAL EXPERIMENTS
In this section, we perform additional experiments to better analyze the proposed method, self-
distillation for further pre-training.
19

--- PAGE 20 ---
Published as a conference paper at ICLR 2023
Table 4: Training and test loss.
Method Split Aircraft CUB Chest-Xray DTD Dogs Flower
Further Pre-trainingTrain (log) −9.13±0.09−8.58±0.05−8.10±1.90−3.70±0.37−5.41±0.13−9.70±0.32
Test 1.44±0.03 2 .44±0.02 2 .26±0.22 2 .49±0.08 1 .96±0.02 0 .63±0.04
Self-DistillationTrain (log) −9.17±0.08−8.16±0.31−6.08±0.50−4.08±0.30−5.50±0.06−7.98±1.28
Test 1.36±0.01 2.26 ±0.04 2.08 ±0.17 2.15 ±0.06 1.90 ±0.01 0.50 ±0.03
Table 5: Comparison with supervised con-
trastive learning.
Model X-ray Flower
SupCon 75.30±1.03 76 .70±1.76
Fine-tuning 77.15±0.52 88 .78±0.65
Further Pre-training 77.79±2.06 88 .63±0.35
Self-Distillation 79.68±1.05 79.68 ±0.44Supervised Contrastive Learning For the main exper-
iments on Section 5, we use the same objective function
for pre-training and further pre-training. One may won-
der what happens if we use different objective at further
pre-training stage. In this experiment, we continue to pre-
train the encoder of the pre-trained transformer (He et al.,
2022) with supervised contrastive loss (Khosla et al.,
2020) while fine-tuning a randomly initialized linear classifier with cross-entropy loss. As shown in
Table 5, supervised contrastive learning (SupCon) significantly degrades the generalization perfor-
mance of Vision Transformer on both X-ray and Flower datasets. Based on this experimental result,
we conjecture that the transformer pre-trained with a masked auto-encoding objective might not be
compatible with the contrastive loss and thus we may use the same objective for pre-training and
further pre-training.
Generalization Gap As shown in Figure 5a, self-distillation decreases generalization gap com-
pared to further pre-training. Additionally, we separately report training and test loss in Table 4.
Both of further pre-training and self-distillation reach near zero training loss, but self-distillation
achieves way lower test loss than further pre-training as a consequence of regularization induced by
self-distillation.
Table 6: Down weight-
ing MAE objective.
λ CUB
1.0 58.06±0.90
0.5 57.76±0.17
0.3 58.21±0.42
0.1 57.76±0.33Down Weighting of Masked Auto-Encoding Although we fix both
weight of self-distillation and masked auto-encoding objective to 1 in equa-
tion 3, we vary λ∈(0,1], the weight of masked auto-encoding objective
(λLMAE(θ, ϕ;Du) +LDistill (θ;θ0,Du))and report test accuracy of Vi-
sion Transformer with self-distillation on the CUB dataset. As shown in
Table 6, our proposed method is insensitive to the value of λand thus there
is no benefit to fine-tuning the weight of masked auto-encoding objective.
Extra Training Time To better analyze extra computational cost for further pre-training and
self-distillation, we report training wall clock time for fine-tuning, further pre-training, and self-
distillation, respectively. We train a Vision Transformer (Dosovitskiy et al., 2021) on CUB dataset
with 3090 RTX GPU and Intel(R) Xeon(R) Silver 4210R CPU. It takes 32 minutes and 18 sec-
onds for fine-tuning the transformer. We need an extra 1 hour 29 minutes 33 seconds for further
pre-training and 5 hours 13 minutes 22 seconds for self-distillation.
C M ASKED AUTO-ENCODING
In this section, we describe the masked auto-encoding objective from equation 2 in more detail.
Given a sequence x= (x1, . . . , x K)with length K, we sample mask z= (z1, . . . , z K)from
a Binomial distribution pγ,K with probability for success γ∈(0,1)and the number of trials
K. For each xk, we replace it with the special token “mask” if zk= 1. Otherwise we use the
same xkfor an masked input. Let ˆx= (ˆx1, . . . , ˆxK)be a masked input and let fθ, gϕbe en-
coder and decoder, respectively. We want to compute the log-likelihood of the reconstructed inputPK
k=1zklogpθ,ϕ(xk|ˆx).
For language models, reconstruction of the masked input ˆxkis predicting which token is masked
out of pre-defined vocabulary with its size V, where each token is represented as an integer from
{1, . . . , V }. Thus the conditional probability of xk∈ {1, . . . , V }given ˆxis parameterized as
20

--- PAGE 21 ---
Published as a conference paper at ICLR 2023
follows:
pθ,ϕ(xk|ˆx) =exp(uxk))PV
j=1exp(uj)
where (u1, . . . , u V) =gϕ(hk)∈RV,"| |
h1···hK
| |#
=fθ(ˆx)∈Rh×K.
For ViT, the sequence xconsists of image patches and the reconstruction of the masked input is pre-
dicting pixel values for each masked patches, which is a regression problem. Thus, we parameterize
the conditional probability of a patch xk= (xk,1, . . . , x k,m)∈Rmgiven ˆxas follows:
pθ,ϕ(xk|ˆx) =mY
i=11√
2πσ2exp
−(xk,i−µk,i)2
2σ2
where µk= (µk,1, . . . , µ k,m)∈Rm,"| |
µ1···µK
| |#
=fθ(ˆx)∈Rm×K.
Since σ >0andπare constants with respect to θandϕ,
arg min
θ,ϕ−KX
k=1logpθ,ϕ(xk|ˆx) = arg min
θ,ϕKX
k=1
1
2σ2mX
j=1(xk,j−µk,j)2−mlog1√
2πσ2

= arg min
θ,ϕKX
k=1
mX
j=1(xk,j−µk,j)2

= arg min
θ,ϕKX
k=1∥xk−µk∥2
2.
D D ATASET
We describe statistics of all the image and text classification datasets used for our experiments in
Table 7 and 8.
Table 7: The number of training instances and classes for each image classification dataset.
Aircraft CUB Chest X-ray DTD Dogs Flower
# of instances 6,667 5,594 5,216 4,230 12,000 2,040
# of classes 100 200 2 47 120 102
Table 8: The number of training instances and classes for each text classification dataset.
SCIERC ACL-ARC Chemprot Twitter-Emotion
# of instances 3,219 1,688 4,169 4,230
# of classes 7 6 13 47
21

--- PAGE 22 ---
Published as a conference paper at ICLR 2023
E H YPERPARAMETERS
In Table 9, we summarize all the hyperparameters for Vision Transformer and RoBERTA.
Table 9: Hyperparameters for Vision Transformer and RoBERTA
Hyperparameters Vision Transformer RoBERTA
learning rate for pre-training 1.5·10−41·10−4
learning rate for fine-tuning 1·10−42·10−5
weight decay coefficient 1·10−21·10−2
batch-size for pre-training 64 128
batch-size for fine-tuning 32 16
learning rate scheduler linear-decay linear-decay
fine-tuning steps 10,000 10 epochs
pre-training steps 20,000 100 epochs
round of self-distillation 1 1
22
