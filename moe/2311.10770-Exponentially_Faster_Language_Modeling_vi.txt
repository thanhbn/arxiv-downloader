# 2311.10770.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2311.10770.pdf
# Kích thước tệp: 236064 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mô hình Ngôn ngữ Nhanh Hơn Theo Cấp Số Mũ
Peter Belcak và Roger Wattenhofer
ETH Zürich
{belcak,wattenhofer}@ethz.ch
Tóm tắt
Các mô hình ngôn ngữ thực sự chỉ cần sử dụng một phần theo cấp số mũ các neuron của chúng cho các suy luận riêng lẻ.
Làm bằng chứng, chúng tôi trình bày UltraFastBERT, một biến thể BERT sử dụng 0,3% neuron của nó trong quá trình suy luận trong khi thực hiện ngang tầm với các mô hình BERT tương tự. UltraFastBERT chỉ sử dụng có chọn lọc 12 trong số 4095 neuron cho mỗi lần suy luận lớp.
Điều này đạt được bằng cách thay thế các mạng feedforward bằng các mạng feedforward nhanh (FFFs).
Mặc dù hiện tại không tồn tại triển khai thực sự hiệu quả để khai thác đầy đủ tiềm năng gia tốc của việc thực thi neural có điều kiện, chúng tôi cung cấp mã CPU cấp cao đạt được tăng tốc 78x so với triển khai feedforward tối ưu hóa cơ bản, và một triển khai PyTorch mang lại tăng tốc 40x so với suy luận feedforward theo lô tương đương.
Chúng tôi công bố mã huấn luyện, thiết lập benchmark và trọng số mô hình của chúng tôi.¹

1. Giới thiệu
Các lớp feedforward chứa phần lớn tham số của các mô hình ngôn ngữ lớn (Brown et al., 2020; Anil et al., 2023). Tuy nhiên, không phải tất cả các neuron của chúng cần được tham gia vào việc tính toán đầu ra lớp feedforward tại thời điểm suy luận cho mọi đầu vào.

Để có một bằng chứng dễ tiếp cận chung, chúng tôi trình bày UltraFastBERT, một biến thể của kiến trúc BERT (Devlin et al., 2018) thay thế các lớp feedforward bằng các mạng feedforward nhanh. Về mặt hiệu suất downstream, UltraFastBERT thực hiện ngang tầm với các mô hình giống BERT khác có kích thước tương tự và trải qua các quy trình huấn luyện tương tự. Tuy nhiên, các lớp trung gian của UltraFastBERT nhanh hơn theo cấp số mũ về thiết kế: với một mạng feedforward (FF) và một mạng feedforward nhanh (FFF), mỗi mạng có n neuron, độ phức tạp thời gian của một lượt truyền tiến qua FFF là O(log₂n) thay vì O(n) như đối với FF. Đây là hệ quả của việc FFFs tổ chức các neuron của chúng thành một cây nhị phân cân bằng, và chỉ thực thi một nhánh của cây có điều kiện với đầu vào.

Thực hiện suy luận trên một FFF tương đương với thực hiện phép nhân ma trận có điều kiện (CMM), trong đó các hàng của đầu vào nhân với các cột của trọng số neural từng cái một, và cột trọng số để tiến hành được chọn tùy thuộc vào đầu ra của phép toán tích vô hướng trước đó. Theo cách này, tất cả các neuron chỉ được sử dụng bởi một số đầu vào và không có đầu vào nào cần nhiều hơn chỉ một số ít neuron để được xử lý bởi mạng. Điều này trái ngược với phép nhân ma trận dày đặc (DMM), nằm ở trung tâm của các mạng feedforward truyền thống, và tính toán các tích vô hướng của tất cả các hàng với tất cả các cột.

Không tồn tại triển khai gốc, hiệu quả của phép nhân ma trận có điều kiện, và không có framework học sâu phổ biến nào cung cấp bất kỳ giao diện nào có thể được sử dụng để triển khai nó ngoài một mô phỏng cấp cao. Do đó, chúng tôi cung cấp một tập hợp các triển khai CPU dựa trên các routine nhân ma trận theo lô con trỏ của thư viện BLAS. Trong một phần sau, chúng tôi so sánh giữa các triển khai CPU và GPU ở các cấp độ tối ưu hóa khác nhau và lưu ý rằng mặc dù đã có bằng chứng rõ ràng về sự gia tốc đáng kể, vẫn còn tiềm năng cho nhiều hơn nữa.

Vai trò của attention. Một lượng lớn tài liệu đã giải quyết chủ đề tăng tốc việc thực thi cơ chế attention. Chúng tôi lưu ý rằng đối với một mô hình kích thước BERT-base với kích thước ngữ cảnh tiền huấn luyện thông thường là 128 (Devlin et al., 2018), chi phí suy luận mỗi token của attention của nó đến tất cả các token khác chỉ bằng chút ít hơn chi phí suy luận mạng feedforward 128-neuron. Do đó, chúng tôi để nguyên các lớp attention và chỉ tập trung vào các lớp trung gian chứa các mạng feedforward.

Các điểm so sánh. Các mạng feedforward BERT-base bao gồm 3072 neuron. Điều này không gần với bất kỳ lũy thừa của hai nào, và vì vậy trong thiết kế của UltraFastBERT, chúng tôi làm tròn số này thành 4095 – số nút trong một cây nhị phân cân bằng có độ sâu tối đa 11. Trong khung tham chiếu này, UltraFastBERT chỉ sử dụng 1/256 (0,04%) của 3072 neuron BERT-base cho suy luận. Tuy nhiên, bản thân UltraFastBERT bao gồm 4095 neuron, và do đó sử dụng 1/341 (0,03%) các neuron của nó cho suy luận.

Khi báo cáo hiệu suất mô hình trên các tác vụ downstream trong Phần 3.3, chúng tôi đưa ra cả baseline 3072-neuron và 4095-neuron để hoàn chỉnh.

Tại sao chỉ tăng tốc 78x chứ không phải 341x? Phép nhân ma trận dày đặc là phép toán toán học được tối ưu hóa nhất trong lịch sử máy tính. Một nỗ lực to lớn đã được đầu tư vào việc thiết kế bộ nhớ, chip, tập lệnh và các routine phần mềm thực thi nó nhanh nhất có thể. Nhiều trong số những tiến bộ này đã được – dù vì độ phức tạp của chúng hay vì lợi thế cạnh tranh – giữ bí mật và chỉ được tiếp xúc với người dùng cuối thông qua các giao diện lập trình mạnh mẽ nhưng hạn chế.

Do đó, mặc dù không cần phần cứng mới, chúng tôi vẫn buộc phải dựa vào việc kết hợp các routine đại số tuyến tính cấp cao để triển khai CMM, do đó làm giảm tăng tốc. Chúng tôi mô tả chi tiết điều này trong Phần 3.

Tính tái tạo. Chúng tôi chia sẻ trọng số của mô hình tốt nhất. Mặc dù chúng tôi không cung cấp triển khai PyTorch hoặc TensorFlow hiệu quả của CMM, việc chỉ có 12 neuron được sử dụng trong suy luận của UltraFastBERT có thể được xác minh đơn giản bằng cách che đi đầu ra của tất cả trừ các neuron được chọn, và chúng tôi cung cấp mã cho điều này.

Các điểm chính.
• Chúng tôi trình bày UltraFastBERT, một mô hình giống BERT có 4095 neuron nhưng chỉ sử dụng có chọn lọc 12 (0,03%) cho suy luận.
• Chúng tôi tinh chỉnh UltraFastBERT cho các tác vụ downstream tiêu chuẩn và thấy rằng nó thực hiện ngang tầm với các đối tác BERT của nó.
• Chúng tôi cung cấp một triển khai đơn giản của phép nhân ma trận có điều kiện làm nền tảng cho suy luận mạng feedforward nhanh. Chúng tôi thấy rằng nó dẫn đến tăng tốc 78x so với phép nhân ma trận dày đặc được tối ưu hóa gốc.
• Thông qua UltraFastBERT và những tăng tốc đã đáng kể bởi các triển khai FFF đơn giản, chúng tôi chứng minh tiềm năng đáng kể của việc thực thi neural có điều kiện trong mô hình hóa ngôn ngữ.

2. Mô hình
2.1. Kiến trúc
Điểm khởi đầu kiến trúc của chúng tôi là kiến trúc crammedBERT (Geiping & Goldstein, 2023), mà chúng tôi triển khai từng chữ trong tất cả trừ bản chất của các lớp trung gian. Ở đó, các mạng feedforward có trong các lớp trung gian của bộ mã hóa transformer crammedBERT được thay thế bằng các mạng feedforward nhanh (Belcak & Wattenhofer, 2023).

Chúng tôi thực hiện các thay đổi đơn giản hóa sau đây đối với các mạng feedforward nhanh gốc:
1. Loại bỏ tất cả sự khác biệt giữa các nút lá và nút không phải lá. Đặc biệt, chúng tôi sử dụng cùng một hàm kích hoạt (GeLU) trên tất cả các nút, trang bị tất cả các nút với trọng số đầu ra, và loại bỏ tất cả các bias đầu ra.
2. Cố định kích thước lá thành 1.
3. Cho phép nhiều cây FFF song song. Chúng tôi cho phép nhiều cây FFF cùng tính toán các đầu ra lớp trung gian. Điều này đạt được bằng cách cộng các đầu ra của các cây riêng lẻ và trình bày tổng như đầu ra lớp trung gian.

Chúng tôi ký hiệu một mô hình với K cây có độ sâu D+1 bằng cách thêm hậu tố vào tên mô hình, tức là UltraFastBERT-KxD. Lưu ý rằng để nhất quán với mã suy luận của chúng tôi, chúng tôi coi một cây không có cạnh nào có độ sâu 0 – do đó cây có độ sâu tối đa D có độ sâu D+1. Một mô hình kích thước BERT-base với lớp feedforward truyền thống có độ rộng 3072 chỉ là một trường hợp đặc biệt của UltraFastBERT, cụ thể là UltraFastBERT-3072x0.

Mặc dù chúng tôi chỉ chia sẻ mô hình nhanh nhất, chúng tôi huấn luyện một loạt đầy đủ các mô hình ngày càng sâu hơn và hẹp hơn, bắt đầu từ UltraFastBERT-3072x0 và tiến hành với UltraFastBERT-1536x1, UltraFastBERT-512x2, v.v.

2.2. Huấn luyện
Chúng tôi tuân theo quy trình huấn luyện cuối cùng của crammedBERT (Geiping & Goldstein, 2023), cụ thể là tắt dropout trong tiền huấn luyện và sử dụng lịch trình tỷ lệ học tam giác 1-cycle. Theo mặc định, chúng tôi huấn luyện mọi mô hình trong 1 ngày trên một GPU A6000 duy nhất, ngoại trừ mô hình UltraFastBERT-1x11-long cuối cùng, mà chúng tôi huấn luyện 2 lần lâu hơn sử dụng cùng chế độ để có hiệu suất downstream tốt hơn một chút.

2.3. Hiệu suất Downstream
2.3.1. THIẾT LẬP
Chúng tôi tinh chỉnh tất cả các mô hình UltraFastBERT cho các tác vụ RTE, MRPC, SST, STS-B, MNLI, QQP, QNLI, và CoLA của benchmark GLUE (Wang et al., 2018) và báo cáo điểm đánh giá như trong Geiping & Goldstein (2023) để nhất quán. Tóm lại, cách tiếp cận này tương đương với việc tinh chỉnh trong 5 epoch với tỷ lệ học 4×10⁻⁵ trên tất cả các tác vụ.

--- TRANG 3 ---
Mô hình Ngôn ngữ Nhanh Hơn Theo Cấp Số Mũ

[Bảng 1 với kết quả của các mô hình ngôn ngữ khác nhau trên tập test GLUE-dev. NT biểu thị số neuron có sẵn để huấn luyện, NI/NT tỷ lệ neuron được sử dụng cho một lần suy luận duy nhất.]

Chúng tôi thấy rằng các mô hình UltraFastBERT được tinh chỉnh theo cách này cho CoLA cuối cùng bị thiếu huấn luyện nếu chỉ sử dụng 5 epoch huấn luyện. Do đó, chúng tôi mở rộng số epoch tinh chỉnh CoLA lên 15. Điều này dẫn đến cải thiện ít hoặc không có gì cho các mô hình crammedBERT cơ bản nhưng có tác động đáng kể đến hiệu suất CoLA của UltraFastBERT.

2.3.2. KẾT QUẢ
Kết quả tinh chỉnh của chúng tôi được liệt kê trong Bảng 1.

Chúng tôi thấy rằng các biến thể UltraFastBERT được huấn luyện trong 1 ngày trên một GPU A6000 duy nhất đều giữ lại ít nhất 96,0% hiệu suất dự đoán downstream GLUE của mô hình BERT-base gốc (Devlin et al., 2018). Chúng tôi cũng quan sát rằng hiệu suất giảm với độ sâu tăng của FFFs. Tuy nhiên, lưu ý rằng phần lớn việc giảm hiệu suất do độ sâu tăng chỉ do một tác vụ duy nhất – CoLA. Hành vi này đã được quan sát trước đây trong tài liệu và phù hợp với các công trình khác cố gắng nén hành vi BERT vào các mô hình nhỏ hơn (Sun et al., 2019; Turc et al., 2019; Mukherjee et al., 2021). Nếu chúng tôi bỏ qua CoLA, ít nhất 98,6% hiệu suất dự đoán được bảo tồn bởi tất cả các mô hình UltraFastBERT.

Hơn nữa, chúng tôi thấy rằng ngoài CoLA, mô hình tốt nhất của chúng tôi – UltraFastBERT-1x11-long – thực hiện ngang tầm với mô hình BERT-base gốc trong khi chỉ sử dụng 0,3% neuron của chính nó, tương đương với chỉ 0,4% neuron BERT-base. Chúng tôi công khai trọng số của mô hình này.

3. Suy luận
Nếu mục đích của phần trên là báo cáo phát hiện rằng chỉ cần rất ít neuron cho mỗi lần suy luận, thì mục tiêu của phần này là áp dụng góc nhìn kỹ thuật và phác thảo cách điều này có thể được tận dụng ở mặt triển khai.

Các mạng feedforward nhanh như một phần của các mô hình ngôn ngữ lớn có tiềm năng gia tốc to lớn. Để chỉ ra loại mức độ tăng tốc mà người ta có thể hy vọng, hãy xem GPT-3 (Brown et al., 2020), mô hình ngôn ngữ lớn đầu tiên được ca ngợi rộng rãi vì tính hợp lý của các đầu ra của nó. Các mạng feedforward của mỗi lớp transformer của GPT-3 bao gồm 49152 neuron. Nếu có thể huấn luyện được, mạng này có thể được thay thế bằng một mạng feedforward nhanh có độ sâu tối đa 15, sẽ chứa 65536 neuron nhưng chỉ sử dụng 16 cho suy luận. Điều này tương đương với khoảng 0,03% neuron của GPT-3.

Ở trung tâm của lời hứa này là phép toán nhân ma trận có điều kiện, với mã giả được đưa ra bên dưới, và với những nỗ lực tương lai của chúng tôi tập trung vào việc triển khai hiệu quả của nó.

3.1. Thuật toán
Belcak & Wattenhofer (2023) đưa ra mã giả đệ quy cho suy luận FFF. Chúng tôi liệt kê mã giả cho CMM và suy luận liên tiếp cho FFFs, với các sửa đổi theo Phần 2.1. Trong Thuật toán 1, B biểu thị kích thước lô, H độ rộng đầu vào lớp (chiều ẩn transformer), 2^(D-1) là số neuron, và M*,k, Ml,* biểu thị cột thứ k và hàng thứ l của M, tương ứng. Kết quả của phép so sánh > trong CMM được giả định là một số nguyên ∈ {0,1}.

3.2. Tương thích
Người ta có thể hỏi liệu tính có điều kiện được giới thiệu bởi việc sử dụng CMM có làm cho FFFs không tương thích với các quá trình và phần cứng đã có sẵn cho phép nhân ma trận dày đặc và học sâu nói chung hay không. Tóm lại, câu trả lời là "Không, nó không làm vậy, ngoại trừ một số độ phức tạp caching tăng lên."

CPU DMM đơn luồng như một phần của suy luận feedforward dựa vào việc thực thi tuần tự các lệnh nhân và tích lũy (MAC). Như vậy, CPU, đặc biệt là CPU edge, sẽ được hưởng lợi dễ dàng nhất từ việc thay thế DMM bằng CMM như thấy trong UltraFastBERT, đơn giản là vì ít lần thực thi lệnh MAC cho mỗi phần tử hơn được cần để tính toán đầu ra lớp. Bất chấp việc sử dụng tính có điều kiện rõ ràng, thường được liên kết với phân nhánh trong mã CPU, "phân nhánh neural" thấy trong CMM chỉ biểu hiện như một phép cộng offset bộ nhớ vào các con trỏ liên quan. Do đó, dự đoán nhánh lệnh không bao giờ được tham gia để tạo điều kiện cho tính có điều kiện CMM.

Để tận dụng đầy đủ caching trọng số để tăng tốc truy cập trọng số, CPU có thể cần được gợi ý để chỉ tải các cột liên quan của ma trận trọng số và chỉ một lần một cột. Vì CMM tiếp tục thực hiện các tích vô hướng hàng-cột, xử lý song song vector single-instruction-multiple-data (SIMD) vẫn là một tùy chọn khả thi để tăng tốc các triển khai suy luận cụ thể thiết bị.

Tính toán DMM GPU đa luồng ngầm định sử dụng rộng rãi cách tiếp cận single-instruction-multiple-threads (SIMT) đằng sau các GPU hiện đại bằng cách thực thi cùng các lệnh MAC trong mỗi luồng, chỉ trên các miếng khác nhau của ma trận. Như trên, lưu ý rằng điều này dễ dàng chuyển sang CMM vì tính có điều kiện được biểu diễn bởi việc tiến hành các cột khác nhau của ma trận trọng số chỉ ảnh hưởng đến offset đến bộ nhớ được sử dụng, và không phải lệnh nào, có hay bao nhiều lần các lệnh MAC được thực thi. Tuy nhiên, các triển khai DMM hiệu quả phân phối khối lượng công việc nhân ma trận (các cặp miếng ma trận được nhân) theo cách tối đa hóa việc sử dụng cache phân tán để các truy cập đến bộ nhớ thiết bị toàn cục, chậm hơn đáng kể so với truy cập cache, bị hạn chế. Để đạt được tiềm năng đầy đủ của nó so với baseline DMM, bất kỳ triển khai hiệu quả nào của CMM phải quản lý caching một cách rõ ràng theo cách tối ưu cho việc duyệt cây, chứ không phải nhân ma trận dày đặc được vá. Điều này có thể được thực hiện bằng cách luôn tải trước trọng số của các cây con liên quan hoặc bằng cách sử dụng các chiến lược vá DMM nhưng loại bỏ các kết quả trung gian từ kết quả của lề vá khi không cần thiết. Dù bằng cách nào, vẫn là một thách thức để thực hiện những tối ưu hóa này mà không có kiến thức thân mật (và thường bí mật) về thiết bị đích của triển khai.

3.3. Hiệu suất Suy luận
Chúng tôi so sánh tốc độ của một số triển khai suy luận FF/FFF có sẵn.

Triển khai. Đối với suy luận CPU, chúng tôi sử dụng Math Kernel Library có sẵn như một phần của Intel oneAPI.
• Triển khai Level 1 là triển khai được xây dựng chỉ sử dụng các routine BLAS Level 1 và các phần mở rộng giống BLAS Level 1, cụ thể là tích vô hướng vector-vector và tích scalar-vector.
• Triển khai Level 2 sử dụng các routine BLAS Level 2 theo lô và các phần mở rộng giống BLAS Level 1, cụ thể là nhân ma trận-vector theo lô và tích scalar-vector theo lô.
• Triển khai Level 3 sử dụng nhân ma trận-ma trận BLAS Level 3 (không theo lô). Đây là triển khai CPU nhanh nhất cho FF, nhưng không thể cung cấp triển khai như vậy vào thời điểm này cho FFF do tính thưa thớt cấp vector của CMM không được thư viện hỗ trợ.

Đối với các triển khai GPU, chúng tôi sử dụng kernel PyTorch hoặc kernel CUDA tùy chỉnh.
• Triển khai Native fused sử dụng kernel lớp feedforward fused gốc. Lưu ý rằng đây là triển khai GPU nhanh nhất cho các lớp FF nhưng một lần nữa, không tồn tại kernel như vậy hiện tại cho FFFs do bản chất của CMM.
• Triển khai BMM sử dụng các kernel nhân ma trận theo lô và kích hoạt cho cả FF và FFF. Trong trường hợp FFFs, chúng tôi sử dụng rộng rãi việc sao chép vector ở mỗi bước giảm cây để mô phỏng tính có điều kiện.
• Triển khai Naive CUDA là mã kernel CUDA tùy chỉnh của chúng tôi cho cả FF và FFF, thực hiện DMM/CMM fused và kích hoạt ở cấp độ phần tử vector/ma trận, được thực thi như một phần mở rộng PyTorch.

Phương pháp. Đối với suy luận CPU, chúng tôi thực hiện 250 lượt truyền tiến mỗi mục trên CPU Intel(R) Core(TM) i7-6700HQ dưới Intel MKL v2023.2.0, sử dụng các biến thể 64-bit của tất cả các routine. Chúng tôi báo cáo thời gian trung bình được thực hiện bởi suy luận đơn, lưu ý rằng giá trị của độ lệch chuẩn luôn nằm dưới 2% của trung bình. Đối với suy luận GPU, chúng tôi thực hiện 1000 lượt truyền tiến mỗi mục trên GPU NVIDIA RTX A6000 dưới CUDA v11.7 và PyTorch 2.0.1. Chúng tôi đo thời gian GPU và báo cáo thời gian trung bình được thực hiện, với độ lệch chuẩn một lần nữa dưới 2% của trung bình trong tất cả các trường hợp. Chúng tôi lấy kích thước lô B = 128 × 128 (tương đương với kích thước lô token ngữ cảnh tiền huấn luyện BERT) và chiều ẩn H = 768.

Kết quả. Bảng 2 liệt kê so sánh hiệu suất của các lớp feedforward và fast feedforward như chúng xuất hiện trong BERT-base và UltraFastBERT-1x11. Mỗi cột của bảng liệt kê các tăng tốc triển khai FFF-so-với-FF tương đối khi sử dụng cùng các primitive routine đại số tuyến tính.

Hai mục thiếu trong Bảng 2 là cho các triển khai BLAS Level 3 và Native fused hiện không có sẵn của FFFs.

So sánh thêm. Tất cả các tăng tốc được báo cáo trong Bảng 2 đưa ra "so sánh công bằng", có nghĩa là trong mỗi trường hợp, cả triển khai FF và FFF đều sử dụng chính xác cùng các phép toán đại số tuyến tính nguyên thủy. Người ta cũng có thể quan tâm đến việc biết các triển khai tốt nhất của FFF hiện tại so với các triển khai tốt nhất của FF như thế nào, mặc dù những cái cho FF sử dụng các primitive không có sẵn cho FFF. Trên CPU, các triển khai Level 1 và Level 2 của FFF thực hiện suy luận nhanh hơn 48x và 78x so với triển khai nhanh nhất (Level 3) của FF, tương ứng. Trên GPU, triển khai PyTorch BMM của FFF mang lại tăng tốc 3,15x so với triển khai nhanh nhất (Native fused) của FF.

3.4. Triển vọng tương lai
Những nét chính để bắt đầu triển khai hiệu quả suy luận FFF đã được vẽ như một phần của thư viện PyTorch. Các tensor thưa thớt cấp vector lai, nếu được hỗ trợ đầy đủ cho nhân ma trận đơn lẻ và theo lô, sẽ đủ để triển khai CMM và suy luận FFF như trong Thuật toán 1.

Một triển khai gốc hơn nữa của CMM như một phần của mã Intel MKL/NVIDIA cuBLAS cụ thể thiết bị sẽ có cơ hội thực sự cung cấp đầy đủ lời hứa tăng tốc 341 lần.

4. Kết luận
Chúng tôi trình bày UltraFastBERT, một phiên bản sửa đổi của kiến trúc (crammed)BERT sử dụng feedforward nhanh thay vì mạng feedforward trong các lớp trung gian của nó. UltraFastBERT phục vụ như bằng chứng rằng các mô hình ngôn ngữ lớn thực sự chỉ cần tham gia một phần theo cấp số mũ các tham số của chúng để thực hiện các suy luận riêng lẻ. UltraFastBERT-1x11, mô hình sâu nhất của chúng tôi với lời hứa gia tốc cao nhất, chỉ sử dụng 0,3% neuron của nó trong quá trình suy luận và đã đạt được tăng tốc CPU 78x so với thời gian suy luận của lớp feedforward tương ứng. Với lời hứa tăng tốc lý thuyết 341x ở quy mô các mô hình BERT-base, chúng tôi hy vọng rằng công trình của chúng tôi sẽ truyền cảm hứng cho một nỗ lực triển khai các primitive cho việc thực thi neural có điều kiện như một phần của giao diện lập trình thiết bị.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được dịch từ tiếng Anh]
