# 2306.03900.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2306.03900.pdf
# File size: 5124778 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MODEL SPIDER : Learning to Rank
Pre-Trained Models Efficiently
Yi-Kai Zhang1, Ting-Ji Huang1, Yao-Xiang Ding2, De-Chuan Zhan1, Han-Jia Ye1,B
1State Key Laboratory for Novel Software Technology, Nanjing University
2State Key Lab of CAD & CG, Zhejiang University
{zhangyk,huangtj,zhandc,yehj}@lamda.nju.edu.cn yxding@zju.edu.cn
Abstract
Figuring out which Pre-Trained Model (PTM) from a model zoo fits the target task
is essential to take advantage of plentiful model resources. With the availability
of numerous heterogeneous PTMs from diverse fields, efficiently selecting the
most suitable PTM is challenging due to the time-consuming costs of carrying out
forward or backward passes over all PTMs. In this paper, we propose MODEL
SPIDER , which tokenizes both PTMs and tasks by summarizing their characteristics
into vectors to enable efficient PTM selection. By leveraging the approximated
performance of PTMs on a separate set of training tasks, MODEL SPIDER learns to
construct tokens and measure the fitness score between a model-task pair via their
tokens. The ability to rank relevant PTMs higher than others generalizes to new
tasks. With the top-ranked PTM candidates, we further learn to enrich task tokens
with their PTM-specific semantics to re-rank the PTMs for better selection. MODEL
SPIDER balances efficiency and selection ability , making PTM selection like a
spider preying on a web. MODEL SPIDER demonstrates promising performance in
various configurations of model zoos.
1 Introduction
Fine-tuning Pre-Trained Models (PTMs) on downstream tasks has shown remarkable improvements
in various fields [ 28,20,59,34], making “pre-training →fine-tuning” the de-facto paradigm in many
real-world applications. A model zoo contains diverse PTMs in their architectures and functionali-
ties [ 1,11], but a randomly selected PTM makes their helpfulness for a particular downstream task
vary unpredictably [ 63,55,77]. One important step to take advantage of PTM resources is to identify
the most helpful PTM in a model zoo — estimating and ranking the transferabilities of PTMs — with
the downstream task’s data accurately and efficiently .
Which PTM is the most helpful? A direct answer is to enumerate all PTMs and evaluate the
performance of their corresponding fine-tuned models. However, the high computational cost of
the backward steps in fine-tuning makes this solution impractical. Some existing methods estimate
proxies of transferability with only forward passes based on the target task’s features extracted by
PTMs [ 8,73,53,45,83,21,56,19,72]. Nowadays, a public model zoo often contains hundreds and
thousands of PTMs [ 79]. Then, the computational burden of forward passes will be amplified, let
alone for the time-consuming forward passes of some complicated PTMs. Therefore, the efficiency of
searching helpful PTMs and estimating the transferability should be further emphasized.
In this paper, we propose MODEL SPIDER , the SPecification InDuced Expression and Ranking of
PTMs, for accurate and efficient PTM selection. In detail, we tokenize all PTMs and tasks into
vectors that capture their general properties and the relationship with each other. For example, two
models pre-trained on NABirds [ 30] and Caltech-UCSD Birds [ 76] datasets may have similar abilities
Preprint. Under review.arXiv:2306.03900v1  [cs.LG]  6 Jun 2023

--- PAGE 2 ---
3
 3
 3
2
 2
 2
Tasks
(a) Forward -Based andToken/Specification -Based Model Selection (b) Selection ability & Efficiency Comparison
…
Forward each Pre-trained ModelTransferability
Ranking
Extract•Features & Logits
1…
1
 1
Learned
Tokens
Similarity Measure
× 2=
TasksModel Zoo•w/o forwardTransferability Ranking
Correlation (𝝉𝝉𝒘𝒘)
Time (multiple of acceleration)𝓝𝓝-LEEP
1x 5x 50x0.40.50.6
𝑘𝑘=0𝑘𝑘=3𝑘𝑘=36𝑘𝑘=42
H-ScoreNCE0.7
LEEP
40x𝑘𝑘=6MODEL SPIDER
(Ours)
LogME
…
𝒯𝒯1𝒯𝒯2𝒯𝒯3𝒯𝒯1𝒯𝒯2𝒯𝒯3
Ranked 2ndFigure 1: (a) Two strategies for PTM selection. Related works utilize forward-based features and
corresponding proxies on the target dataset to evaluate transferability. The token/specification-based
approach with learned model-task token reduces the requirement for forwarding pass on each PTM.
(b) The average efficiency (wall-clock time) vsperformance (correlation τw, the higher, the better)
comparison of PTM selection on various datasets. The circle sizes indicate the memory footprint.
Red circles are MODEL SPIDER with different values of the number of PTM-specific features k,
while others are comparison methods. M ODEL SPIDER balances efficiency and accuracy well .
in birds recognition, so that we can associate them with similar tokens. Then the transferability from
a PTM to a task could be approximated by the distance of their tokens without requiring per-PTM
forward pass over the downstream task . The success of MODEL SPIDER depends on two key factors.
First, how to obtain tokens for tasks and PTMs? The token of the most helpful PTM should be close
to the task token w.r.t. some similarity measures. Then, will a general task token weaken the selection
ability since it may ignore specific characteristics of a PTM?
InMODEL SPIDER , we learn to construct tokens with a general encoder and measure the similarity
between tokens with a Transformer module [ 74] in a supervised learning manner . We estimate the
rankings of PTMs in the model zoo for some historical tasks using rank aggregation. By leveraging
the approximated supervision, we pull task tokens close to the top-ranked PTM tokens and push
unhelpful PTM tokens away based on the transformer-measured similarity. We expect that the ability
to tokenize and measure similarity could be generalized to unseen tasks. The difference between
MODEL SPIDER ’s token-based PTM selection with forward-based strategy is illustrated in Figure 1.
The tokens generated by general encoders significantly reduce the PTM search time and improve the
search performance. If the budget allows, we can extract features of the downstream task by carrying
out forward passes over a part of (the top- kranked) PTMs, revealing the specific relationship between
PTMs and the task. We equip our MODEL SPIDER with the ability to incorporate PTM-specific
tokens, which re-ranks the PTMs and further improves the selection results. In summary, MODEL
SPIDER is suitable for different budget requirements, where the general and task-specific tokens make
a flexible trade-off between efficiency and accuracy, given various forward passes. Figure 1 illustrates
a comparison of PTM selection methods w.r.t. both efficiency and accuracy. Our contributions are
•We propose a novel approach MODEL SPIDER to tokenize tasks and PTMs, which is able to rank
PTMs in a model zoo given a downstream task efficiently and accurately.
•MODEL SPIDER learns to tokenize and rank PTMs on a separate training set of tasks, and it can
incorporate task-specific forward results of some PTMs when resource budgets allow.
•The experiments demonstrate that MODEL SPIDER effectively ranks PTMs and achieves significant
improvements on various model zoo configurations.
2 Related Works
Efficient PTM Search with Transferability Assessment. Whether a selected PTM is helpful could
be formulated as the problem measuring the transferability from the source data pre-training the
PTM to the target downstream task [ 12,33,4,62]. The current evaluation of transferability relies
on a forward pass of the PTM on the target task, which generates the PTM-specific features on the
target task. For example, NCE [ 73], LEEP [ 53], LogME [ 83,84], PACTran [ 21], and TransRate [ 32]
estimate negative conditional entropy, log expectation, marginalized likelihood, PAC-Bayesian
bound, mutual information to obtain proxy metric of transferability, respectively. Several extensions
including N-LEEP [ 45] with Gaussian mixture model on top of PTM features, H-Score [ 8] utilizing
2

--- PAGE 3 ---
divergence transition matrix to approximate the transferred log-likelihood, and [ 19,56,67] exploring
correlations between categories of the target task. Auxiliary information such as source clues [ 6,72]
and gradients of PTMs when back propagating with few steps [ 68,58] are also investigated. Although
the transferability assessment methods avoid the time-consuming fine-tuning, the forward costs over
PTMs also become heavier given diverse and complicated pre-trained model zoos.
Relatedness of Task . Whether a PTM gains improvements after fine-tuning on the downstream
task has been verified to depend on the relatedness between tasks both theoretically [ 9,10,48]
and empirically [ 77]. The relatedness could be measured through various ways, such as fully fine-
tuning [ 85], task vectors [ 2], example-based graphs [ 40,23,69], representation-level similarities [ 24,
3], and human prior knowledge [ 36,60]. Instead of utilizing a pre-defined strategy to measure the
relatedness, MODEL SPIDER construct tokens of PTMs/tasks in vector forms and learns a similarity
between them on historical tasks.
Learning to rank predicts the orders of objects usually with a score function [ 35], and the experience
on a training set could be generalized to unseen data [ 5,51]. Additional learned metrics or embeddings
further improve the ranking ability [ 50,14]. The task relatedness can also be modeled as a learning-
to-rank problem, where the preference over one PTM over another could be learned from historical
rankings of PTMs. However, obtaining the supervision on the training set requires complete fine-
tuning over a large number of historical tasks, which either come from a time-consuming transfer
learning experience [ 78] or the output from some specially selected transferability assessment
methods [ 22]. We propose a strong and efficient approximation of the PTM ranking supervision on
the training set tasks, and a novel token-based similarity is applied.
3 Preliminary
We describe the PTM selection problem by assuming all PTMs are classifiers, and the description
could be easily extended to PTMs for other tasks, e.g., regression. Then we discuss several solutions.
3.1 Selecting PTMs from a Model Zoo
Consider we have a target classification task T={(xi, yi)}N
i=1withNlabeled examples, where the
label yiof each instance xicomes from one of the CTclasses. Instead of learning on Tdirectly,
we assume there is a model zoo M={fm=Wm◦ϕm}M
m=1containing MPTMs. A PTM fm
could be decomposed into two components. ϕmis the feature extraction network producing dm-
dimensional features. Wm∈Rdm×Cmis the top-layer classifier which maps a dm-dimensional
feature to the confidence score over Cmclasses.1PTMs in Mare trained on source data across
various domains. Their feature extractors ϕmhave diverse architectures, and the corresponding
classifiers are pre-trained for different sets of objects. In other words, dmandCm′may differ for a
certain pair of mandm′. A widely-used way to take advantage of a PTM fm=Wm◦ϕmin the
target task is to fine-tune the feature extractor together with a randomly initialized classifier over T.
In detail, we minimize the following objective
ˆf=ˆW◦ˆϕ= arg min
f=W◦ϕNX
i=1ℓ(W⊤ϕ(xi), yi|ϕm), (1)
where ϕisinitialized with ϕm. The fine-tuned fmakes prediction with arg maxc∈[C]ˆw⊤
cˆϕ(x).
[C] ={1, . . . , C }andˆwcis the cth column of ˆW. Then, we can rank the helpfulness of PTMs based
on the performance of their fine-tuned models. In other words, we obtain ˆfmfollowing Equation 1
based on the mth PTM fm, then we calculate the averaged accuracy when predicting over an unseen
test set of T(the higher, the better), i.e.,
tϕm→T=Eh
I
y= arg maxc∈[C]ˆfm(x)i
. (2)
tϕm→Tis also named as the transferability , measuring if the feature extractor ϕmin a PTM could
be transferred well to the target task with fine-tuning [ 73,32].I(·)is the indicator function, which
outputs 1if the condition is satisfied. Given tT={tϕm→T}M
m=1,i.e., the transferability for all
PTMs, then we can obtain the ground-truth ranking of all PTMs in the model zoo for task Tand
1We omit the bias term for simplicity.
3

--- PAGE 4 ---
select the top-ranked one. In the PTM selection problem, the goal is to estimate the ranking of all
PTMs for a task Tusing ˆtT={ˆtϕm→T}M
m=1. The evaluation criterion is the similarity between the
predicted ˆtTand the ground-truth tT, typically measured by weighted Kendall’s τw[37]. We omit
the subscript Twhen it is clear from the context.
3.2 Efficiency Matters in PTM Selection
One direct solution to PTM selection is approximating the ground truth tTby fine-tuning all the
PTMs over T, where a validation set should be split from Tto estimate Equation 2. Since fine-tuning
PTM contains multiple forward and backward passes, the computation burden is astronomical.
A forward pass of a certain PTM’s extractor ϕmoverTgenerates the features Φm
T={ϕm(xi)∈
Rdm}(xi,yi)∈T, which is lightweight compared with the backward step. The feature reveals how
examples in Tare distributed from the selected PTM’s view, and a more discriminative feature may
have a higher transfer potential. As mentioned in section 2, the existing transferability assessment
methods estimate tϕm→Tbased on the PTM-specific feature Φm
Tand target labels {yi}N
i=1[53,83,
45,84]. Precise estimation requires a large N, which means we need to collect enough examples to
identify the most helpful PTMs from a model zoo.
While the previous forward-based transferability assessment methods reduce the time cost, selecting
among MPTMs in the model zoo multiplies the forward cost Mtimes, making the estimation of
ˆtcomputationally expensive. Moreover, since forward passes for complicated PTMs take longer,
selecting a PTM efficiently , especially given a large model zoo, is crucial.
4 M ODEL SPIDER
InMODEL SPIDER , we propose to tokenize PTMs and tasks regardless of their complexity, allowing
us to efficiently calculate their relatedness based on a certain similarity measure over their tokens.
These tokens capture general properties and serve as a specification of a model or task, demonstrating
which kinds of tasks a model performs well on or what kind of models a task requires. In this section,
we first introduce the process of obtaining tokens by learning from a training set of tasks, and the
ability to rank PTMs could be generalized to downstream tasks. We then describe the token encoder,
the token-wise similarity measure, and an efficient way to generate supervision during token training.
Finally, we discuss how MODEL SPIDER can be flexible in incorporating forward pass results of
top-ranked PTMs to further improve the token’s semantics and the ranking’s quality.
4.1 Learning to Rank PTMs with Tokens
InMODEL SPIDER , we learn the model tokens {θm}M
m=1, task tokens µ(T), and the similarity
measure sim(·,·)in a supervised learning manner based on a separate training set D. The training set
Ddoes not contain overlapping classes with the downstream task T.
Specifically, we randomly sample training tasks {Ti}fromD. For a given training task Ti, we assume
that we can obtain the ground-truth ranking tTi={tϕm→Ti}M
m=1over the MPTMs, indicating the
helpfulness of each PTM. We will discuss the details of obtaining the supervision tTilater. We then
select PTMs for Tibased on the similarity between the task token µ(Ti)and those MPTM tokens
{θm}M
m=1. We expect the higher the similarity, the more helpful a PTM is for the given task. We
useΘto denote all learnable parameters and optimize Θwith a ranking loss, which minimizes the
discrepancy between the rank ˆtTipredicted by the similarity function and the ground-truth tTi:
min
ΘX
Ti∼Dℓrank
ˆtTi={sim(θm,µ(Ti))}M
m=1,tTi
. (3)
Given t∈RM, we use an operator dsc(·)to index the elements of tin a descending order, i.e.,
∀m < l , we have tdsc(m)⩾tdsc(l).dsc (m)is exactly the index of the PTM with mth largest
ground-truth score. Based on this, we use the following ranking loss:
ℓrank(ˆt,t) =MX
m=1−log 
exp ˆtdsc(m)
PM
l=mexp ˆtdsc(l)!
, (4)
4

--- PAGE 5 ---
…
sim( ,
𝜓𝜓
General
ExtractorModel Tokens
•Random Init. & Learnable
Task Token
PTM -Specific Task Token *
Selected PTM
* [Optional]
Model -Task Space Model -Task Similarity Measure
•Transformer -based
project to
̂𝑡𝑡𝒯𝒯={sim( ,
),
sim( ,),
sim( ,)}
sim( ,task token PTM 
token)
)
Update with
PTM -specific token *•E.g.
PTM 
Ranking:
… > > >
Approximated Supervision
•on few -shot tasks
LogMEH-Score
NCE
RankAggℓrank
Ranking
LossFigure 2: An illustration of MODEL SPIDER . The middle part (b) shows the workflow of MODEL
SPIDER , which involves tokenizing both PTMs and tasks into a shared space. Plot (c) demonstrates
how the model-task similarity calculated based on the tokens helps rank PTMs for a given task. In plot
(a), when the budget allows, MODEL SPIDER can take advantage of PTM-specific features obtained
by performing forward passes of the top- kranked PTMs on some selected tasks. This improves the
quality of task tokens as well as the PTM ranking.
Equation 4 aims to make the whole order of the predicted ˆtTisimilar to the ground-truth tTi. So the
similarity between the task token and the token of a higher-ranked PTM indicated by tTishould be
larger than the similarity with lower-ranked PTM tokens. The underlying intuition is that if a PTM
performs well on certain tasks, it is likely to generalize its ability to related tasks. For example, if a
PTM excels at bird recognition, it may effectively recognize other flying animals.
For a downstream task T, we generate its task token with µ(T), and identify the close PTM tokens
with the learned sim(·,·). Objective Equation 3 also works when the number of examples in a task
is small. By learning to rank PTMs for sampled few-shot tasks ,MODEL SPIDER can rank helpful
models even with limited training data. We will show this ability of M ODEL SPIDER in section 5.
4.2 Tokens for PTM Selection
We encode the general characteristics of tasks and PTMs via two types of tokens.
Model Token. Given a model zoo with MPTMs, we associate a PTM fmwith a token θm∈Rd.
θmencodes rich semantics about the aspects in which fmexcels. Models pre-trained from related
datasets or those with similar functionalities are expected to have similar tokens.
Task Token. ACT-class task T={(xi, yi)}N
i=1contains a set of instances and labels. We would like
to tokenize a task with a mapping µ(·), which outputs a set of vectors µ(T)∈Rd×CT, one for each
class. We implement µwith one additional frozen encoder ψwith an equivalent parameter magnitude
as the PTMs in the model zoo. ψis pre-trained by self-supervised learning methods [ 15,27,43] and
captures the semantics of a broad range of classes. In detail, we extract the features of all instances in
the task Tand take the class centers as the task token:
µ(T) =

1
|I(yi=c)|X
(xi,yi)∈T[ψ(xi)·I(yi=c)]


c∈[C]. (5)
The task token expresses the characteristics of a task, e.g., those tasks with semantically similar
classes may have similar sets of tokens.
Model-Task Similarity. The helpfulness of a PTM w.r.t. a task, i.e., the transferability score, could be
estimated based on the similarity of the model-task token pairs ˆtϕm→T= sim( θm,µ(T)), and the
PTM selection is complemented by embedding the model and tasks into a space and then identifying
close PTM tokens for a task. In MODEL SPIDER , the sim(·,·)is implemented with a one-layer
Transformer [ 74], a self-attention module that enables various inputs. The Transformer consists of
alternating layers of multi-head self-attention, multi-layer perceptron, and layer norm blocks. We set
the input of the Transformer as the union set of model and task tokens z= [θm,µ(T)]∈Rd×(1+C),
then the similarity ˆtϕm→Tbetween model and task tokens is:
sim(θm,µ(T)) = FC (transformer ( z) [0]) , (6)
where [0]is the first output of the Transformer, i.e., the corresponding output of the model token. We
add a Fully Connected (FC) layer to project the intermediate result to a scalar. Learnable parameters
Θ, including {θm}M
m=1, FC, and weights of the Transformer, are trained via objective in Equation 3.
5

--- PAGE 6 ---
4.3 Accelerating Training for M ODEL SPIDER
The training of MODEL SPIDER in Equation 3 requires a large number of (task Ti, PTM ranking
tTi) pairs. Although we could collect enough data for each task, obtaining the ground-truth PTMs
rankings, i.e., the helpfulness order of PTMs for each task, is computationally expensive. In addition,
using some proxies of tTimay weaken the ability of the MODEL SPIDER . We propose a closer
approximation of the ground-truth tTi, which efficiently supervises sampled tasks from D.
Approximated Training Supervision . We take advantage of the fact that existing PTM selection
methods rely on the PTM-specific features Φm
Tito estimate the transferability score w.r.t. Tiand
produce diverse scores. In other words, a PTM will be placed in different positions based on the
scores provided by various methods such as NCE [ 73], LEEP [ 53], and LogME [ 83,84]. Based on
their “relatively good but diverse” ranking results, an intuitive approach to estimate the ground-truth
tTiis to ensemble their multiple ranking results into a stronger single order.
Given{ˆt1
Ti,ˆt2
Ti, . . .}as multiple predicted rankings over MPTMs for a sampled task Ti,i.e., the order
sorted by the estimations of transferability via various methods, we take advantage of Copeland’s ag-
gregation method [ 7,65] to ensemble the orders: ¯tTi={¯tϕm→Ti}M
m=1= RankAgg( {ˆt1
Ti,ˆt2
Ti, . . .}).
Copeland’s aggregation compares each pair of ranking candidates and considers all preferences
to determine which of the two is more preferred. The output ¯tTiacts as a good estimation of the
ground-truth supervision tTi. The aggregated ¯tTiis more accurate than a particular transferability
assessment method, which improves the quality of the supervision in ranking loss in Equation 4.
Sampling Tasks for Training . We assume that the training data Dcontains a large number of
classes with sufficient data. To sample tasks for training, we randomly select a set of classes from D
and choose a subset of their corresponding examples. Benefiting from the supervision estimation
approach RankAgg , we are able to obtain the aggregated ranking ¯tfor any sampled task.
Training Complexity . The training phase in MODEL SPIDER is efficient. First, we pre-extract
features {Φm
D}M
m=1forDwith all PTMs in advance. Then only the computational burden of base
transferability assessment methods, rank aggregation methods, and the optimization of top-layer
parameters are involved. Furthermore, training tasks with the same set of classes share the same ¯tTi.
4.4 Re-ranking with Efficiency-Accuracy Trade-off
The learnable model token captures the PTM’s empirical performance on various fields of training
tasks, which decouples the task token from the PTM. Each model token implicitly expresses the field
in which the PTM excels, so the PTM selection only requires a task token to express the field in
which the task is. In contrast to the general task token µ(Ti), PTM-specific features Φm
Tifor a subset
of PTMs provide rich clues about how those PTMs fit the target examples, which are also used in
related transferability assessment approaches [ 19,56]. We claim that given specific features with a
subset of PTMs when the budget is available, our MODEL SPIDER can re-rank the estimated PTM
order and further improve performance.
Specifically, we extract the PTM-specific task token µm(T)∈Rdm×CTwith the specific features
Φm
Tof the mth PTM as Equation 5. To take account of different values of dmdue to the heterogeneity
of PTMs, we learn a projection P∈Rdm×dfor the mth PTM to align the dimensionality of µm(T)
with the model token. We then replace the general task token µ(T)via the specific one P⊤
mµm(T)
when calculating the similarity with the token θmof the mth PTM. The specific task token may
facilitate obtaining more accurate estimations. During the training process, we dynamically select
a partial set of PTMs and incorporate the specific tokens into the sampled tasks. Thus, the same
Transformer module in Equation 6 can deal with the new type of tokens. To differentiate the general
and specific tokens, we learn two additional d-dimensional embeddings as prompts. The prompts
are added to the input tokens, allowing the transformer to utilize token-type context for a better
ranking process. Notably, µm(T)depends on Φm
T, and the pre-extracted PTM-specific features for
all training tasks make the construction of these specific tokens efficient.
4.5 A Brief Summary of M ODEL SPIDER
MODEL SPIDER learns to rank PTMs with their tokens for a given task, which balances efficiency
and accuracy. During the training, we sample tasks where PTM tokens and the transformer-based
6

--- PAGE 7 ---
Table 1: Performance comparisons of 10baseline approaches and MODEL SPIDER on a model zoo
with10PTMs [ 83]. We measure the performance with Kendall’s [ 37] weighted τw. The downstream
tasks from diverse fields ( 8datasets) are evaluated in a standard manner (all training examples) and a
few-shot manner ( 10examples per class and 30 trials). Specific features of top- 3ranked PTMs are
used in M ODEL SPIDER . We denote the best-performing results in bold.
MethodDownstream Target DatasetMeanAircraft Caltech101 Cars CIFAR10 CIFAR100 DTD Pets SUN397
Standard Evaluation
H-Score [8] 0.328 0 .738 0 .616 0 .797 0 .784 0 .395 0 .610 0 .918 0 .648
NCE [73] 0.501 0 .752 0 .771 0 .694 0 .617 0 .403 0 .696 0 .892 0 .666
LEEP [53] 0.244 0 .014 0 .704 0 .601 0 .620 -0.111 0 .680 0 .509 0 .408
N-LEEP [45] - 0.725 0 .599 0 .622 0 .768 0 .776 0 .074 0 .787 0 .730 0 .454
LogME [83] 0.540 0.666 0 .677 0 .802 0 .798 0 .429 0 .628 0 .870 0 .676
PACTran [21] 0.031 0 .200 0 .665 0 .717 0 .620 -0.236 0 .616 0 .565 0 .397
OTCE [72] - 0.241 -0.011 -0.157 0 .569 0 .573 -0.165 0 .402 0 .218 0 .149
LFC [19] 0.279 -0.165 0 .243 0 .346 0 .418 -0.722 0 .215 -0.344 0 .034
GBC [56] - 0.744 -0.055 -0.265 0 .758 0 .544 -0.102 0 .163 0 .457 0 .095
MODEL SPIDER 0.506 0.761 0.785 0.909 1.000 0.695 0.788 0.954 0.800
Few-Shot Evaluation (10-example per class)
H-Score [8] - 0.014 0 .078 0 .375 0 .018 0 .005 -0.028 -0.006 0 .853 0 .160
NCE [73] 0.273 0 .534 0 .597 0 .267 0 .232 0 .362 0 .352 0 .793 0 .426
LEEP [53] 0.069 -0.038 0 .476 0 .530 0 .471 -0.111 0 .567 0 .468 0 .304
N-LEEP [45] - 0.559 0 .476 0 .743 0 .515 0 .707 0 .027 0 .713 0 .812 0 .429
LogME [83] 0.341 0 .453 0 .497 0 .718 0 .698 0 .407 0 .657 0 .817 0 .574
PACTran [21] 0.136 0 .262 0 .484 0 .631 0 .614 -0.227 0 .701 0 .477 0 .385
OTCE [72] - 0.316 -0.050 -0.127 0 .515 0 .505 -0.168 0 .406 0 .210 0 .123
LFC [19] 0.226 -0.226 -0.235 0 .330 0 .271 -0.669 -0.059 -0.151 -0.064
MODEL SPIDER 0.382 0.711 0.727 0.870 0.977 0.686 0.717 0.933 0.750
similarity are learned. In particular, to enable the model-task similarity to incorporate PTM-specific
features, we replace some of the inputs to the transformer with enriched tokens. We pre-extract
PTM-specific features for all training tasks, then the estimated ground-truth and the specific tokens
could be constructed efficiently. During deployment, we first employ a coarse-grained PTM search
with general tokens. Then we carry out forward passes over the target task only for top- kranked
PTMs , where the obtained PTM-specific task tokens will re-rank the PTMs by taking the distributed
examples with PTM’s features into account.
5 Experiments
We evaluate MODEL SPIDER on two benchmarks: a model zoo comprising heterogeneous models
pre-trained from the same and different datasets. We analyze the influence of key components in
MODEL SPIDER and visualize the ability of a PTM using spider charts based on the learned tokens.
5.1 Evaluation on a Single-Source Model Zoo
Setups. We follow [ 83] and construct a model zoo with 10PTMs pre-trained on ImageNet [ 64]
across five architecture families, i.e.Inception [ 70], ResNet [ 28], DenseNet [ 31], MobileNet [ 66],
and MNASNet [ 71]. We evaluate various methods on 9downstream datasets, i.e.Aircraft [ 47],
Caltech101 [ 26], Cars [ 39], CIFAR10 [ 41], CIFAR100 [ 41], DTD [ 17], Pet [ 57], and SUN397 [ 82]
for classification, UTKFace [86] and dSprites [49] for regression.
Baselines. There are three groups of comparison methods. First are creating a proxy between PTM-
specific features and downstream labels, such as H-Score [ 8], NCE [ 73], LEEP [ 53],N-LEEP [ 45],
LogME [ 83], and PACTran [ 21]. The second are based on the downstream inter-categories features
like OTCE [ 72], Label-Feature Correlation (LFC) [ 19], and GBC [ 56]. Following [ 53] and [ 83], we
equivalently modify NCE and H-Score to the general model selection application.
Evaluations. For the standard evaluation , we follow the official train-test split of each downstream
dataset and utilize all the training samples. In few-shot evaluation , we consider if MODEL SPIDER
can select useful models with limited labeled examples under privacy and resource constraints. We
7

--- PAGE 8 ---
45.47
39.75
34.03
28.31
35.77
32.30
28.83
25.37
32.91
26.99
21.0715.16
-4.17
-4.31
-4.45
-4.58
-3.28
-3.41
-3.53
-3.66
-2.40
-2.85
-3.30-3.75-4.51
-4.54
-4.58-4.61
-3.65
-3.72
-3.78-3.85
-3.10
-3.29
-3.48-3.67-1.99
-2.19
-2.38
-2.57
-1.09
-1.51
-1.93
-2.35
-0.28
-1.08
-1.89-2.690.94
0.92
0.900.89
0.70
0.64
0.59
0.53
1.08
0.79
0.490.20-0.04
-0.05
-0.06-0.07
-0.07
-0.08
-0.09
-0.10
-0.07
-0.09
-0.11-0.13-0.03
-0.02
-0.01
-0.00
0.13
0.09
0.05
0.01
0.43
0.25
0.08
-0.08Pet                               DTD                          Aircraft
DenseNet-201         Inception v3         ResNet-50
76.00            81.35           86.70           76.00          81.35          86.70            76.00           81.35        86.70            76.00          81.35            86.70           76.00           81.35           86.70           76.00        81.35          86.70             76.00          81.35           86.70            76.00          81.35         86.70         76.00 81.35           86.70 -0.04
-2.75
-5.47-8.19
0.19
-0.41-1.13
-1.61
0.11
-0.55
-1.23-1.90-8.16
-15.08-21.99-28.90
13.10
1.40
-10.30-22.00
23.43
3.22
-16.99
-37.20
1e576.00             81.35            86.70           76.00          81.35          86.70            76.00           81.35      86.70            76.00          81.35            86.70           76.00           81.35           86.70           76.00       81.35          86.70             76.00          81.35           86.70            76.00          81.35         86.70         76.00           81.35           86.70 
76.00             81.35            86.70           76.00          81.35          86.70            76.00           81.35      86.70            76.00          81.35            86.70           76.00           81.35           86.70           76.00       81.35          86.70             76.00          81.35           86.70            76.00          81.35         86.70         76.00           81.35           86.70 
1e5
1e5
H-Score ( 0.649)       NCE ( 0.671)         LEEP ( 0.361)          - LEEP ( 0.677)    LogME (0.673)    PACTran (0.345)     LFC ( 0.637)          GBC ( 0.439)         Ours (0.678)  H-Score ( 0.357)       NCE ( 0.254)         LEEP ( 0.200)          - LEEP ( 0.254)    LogME (0.542)    PACTran (0.237)     LFC ( 0.205)          GBC ( 0.147)         Ours (0.549)  H-Score ( 0.067)       NCE ( -0.160)       LEEP ( -0.615)         -LEEP ( -0.103)   LogME (-0.075)  PACTran (-0.602)   LFC ( -0.178)         GBC ( -0.195)        Ours (0.352)  Figure 3: Visualizations when selecting PTMs from a multi-source heterogeneous model zoo (w/ 42
PTMs) on three downstream datasets. Rows represent approaches, and columns represent datasets.
Correlations ( τw) are shown above each subfigure. The horizontal axis denotes transferred accuracy
(w/ fine-tuning), while the vertical axis is the output ranking score. The PTM architectures are
drawn in red, yellow, and green. The bold line and the gray area show the fitted straight line and the
confidence interval for all PTMs. The strong linear correlation suggests superior performance.
sample 10 examples per class from the training set as a “probe set” and report the average results
over30trials. The full results, along with 95% confidence intervals, are presented in the appendix.
Training Details of MODEL SPIDER .We implement the ψwith the pre-trained Swin-B [ 46,43] to
extract the task tokens. M ODEL SPIDER is trained on 832sampled tasks from the mix of 6datasets,
i.e., EuroSAT [ 29], OfficeHome [ 75], PACS [ 44], SmallNORB [ 42], STL10 [ 18] and VLCS [ 25].
MODEL SPIDER utilizes specific features from the top- 3ranked PTMs (out of 10) for downstream
tasks, resulting in a 3-4 times speedup.
Results of Standard and Few-Shot Evaluation. For the standard evaluation shown in Table 1 and
Table 2, MODEL SPIDER outperforms other baselines across datasets, except for Aircraft, which
ranks top- 2. It also demonstrates superior stability and outperforms all the existing approaches in
few-shot scenarios, as displayed in the lower part of Table 1. Consistently ranking and selecting the
correct PTMs, M ODEL SPIDER achieves the highest mean performance among all methods.
5.2 Evaluation on a Multi-Source Model Zoo
Table 2: Performance comparison of regression-
conducted approaches with the same model zoo
and weighted τwmeasurement as in Table 1.
The downstream task is dSprites and UTKFace.
DatasetMethods for Regression Tasks
H-Score LogME GBC Ours
dSprites 0.106 0 .612 -0.283 0.679
UTKFace 0.075 -0.156 0 .052 0.364We construct a large model zoo where 42heteroge-
neous PTMs are pre-trained from multiple datasets.
Setups. PTMs with 3similar magnitude architec-
tures, i.e., Inception V3, ResNet 50, and DenseNet
201, are pre-trained on 14datasets, including ani-
mals [ 30,38], general and 3D objects [ 26,42,41,
39,13], plants [ 54], scene-based [ 82], remote sens-
ing [81,16,29] and multi-domain recognition [ 44].
We evaluate the ability of PTM selection on Air-
craft [47], DTD [17], and Pet [57] datasets.
Training Details. We use the same task token extractor as in subsection 5.1 with 4352 training tasks
sampled from the mix of the above datasets for pre-training the model zoo.
Analysis of Multi-Source Model Zoo. With many PTMs in the model zoo, we first set k= 0and
select PTMs based on general tokens. We visualize the results in Figure 3, with each subfigure showing
the transferred accuracy using the selected PTM with fine-tuning and the predicted ranking score.
A better-performing method will show a more obvious linear correlation. The results demonstrate
thatMODEL SPIDER achieves the optimum in all three datasets. Furthermore, a visualization of
efficiency, the averaged performance over all datasets, and model size on this benchmark with
standard evaluation is shown in Figure 1. The different configurations of kbalance the efficiency
and performance in PTM selection, which “envelope” the results of other methods. These results
8

--- PAGE 9 ---
/uni00000013 /uni00000016
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000033/uni00000037/uni00000030/uni00000010/uni00000056/uni00000053/uni00000048/uni00000046/uni0000004c/uni00000049/uni0000004c/uni00000046/uni00000003/uni00000029/uni00000048/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000b w/uni0000000c
/uni00000013/uni00000011/uni00000017/uni00000014/uni00000013/uni00000011/uni00000018/uni00000014/uni00000013/uni00000011/uni00000018/uni0000001a/uni00000013/uni00000011/uni0000001a/uni0000001c
/uni00000013/uni00000011/uni00000018/uni0000001b/uni00000013/uni00000011/uni00000019/uni0000001c/uni00000013/uni00000011/uni00000019/uni00000019/uni00000013/uni00000011/uni00000019/uni0000001b /uni00000013/uni00000011/uni00000019/uni0000001a/uni00000013/uni00000011/uni0000001a/uni00000019/uni00000013/uni00000011/uni0000001b/uni0000001c/uni00000013/uni00000011/uni0000001c/uni00000018
/uni00000024/uni0000004c/uni00000055/uni00000046/uni00000055/uni00000044/uni00000049/uni00000057
/uni00000026/uni00000044/uni00000055/uni00000056
/uni00000027/uni00000037/uni00000027
/uni00000047/uni00000036/uni00000053/uni00000055/uni0000004c/uni00000057/uni00000048/uni00000056
/uni00000026/uni00000044/uni0000004f/uni00000057/uni00000048/uni00000046/uni0000004b/uni00000014/uni00000013/uni00000014
/uni00000036/uni00000038/uni00000031/uni00000016/uni0000001c/uni0000001a
/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048(a) Number of Used Features vs. Correlation.
Scene Birds3D
ObjectMedical Animals Animals Medical
Scene BirdsRemoteSensingRemoteSensing3D
Object
Pre-Trained on AID Pre-Trained on NABirds (b) Spider chart of which semantic aspects the PTM excels in.
Figure 4: (a): The ablation analysis of how the ranking correlation changes (Y-axis) with more
PTM-specific features (X-axis). (b): Visualization of the PTM’s ability on 6major semantic clusters
of datasets with spider chart. The score on the vertex of the spider chart is the averaged similarities
between a PTM and the task tokens in the cluster. The higher the vertex value, the better a PTM
would perform on that kind of task.
confirm that MODEL SPIDER performs well in complex scenarios, highlighting its ability to select
heterogeneous PTMs in a large model zoo.
5.3 Ablation Studies
We analyze the properties of MODEL SPIDER on some downstream datasets, following the evaluation
of a single-source model zoo in subsection 5.1.
Will RankAgg provide more accurate ground-truth during training? As discussed in subsec-
tion 4.3, MODEL SPIDER is trained on historical tasks and we utilize RankAgg to approximate
accuracy ranking. We investigate if this approximation offers better supervision and if using previous
model selection methods like H-Score or LogME without aggregation is sufficient. The results
in Table 3 include CIFAR10 and averaged results over eight classification datasets. It is evident that
RankAgg provides stronger supervision during M ODEL SPIDER ’s training.
Table 3: The weighted τwofMODEL SPI-
DER variants when the training supervi-
sion is approximated by different methods.
“Mean” denotes the averaged performance
over8downsteam datasets in Table 1.
Method CIFAR10 Mean
w/ H-Score [8] 0.386 0 .642
w/ LogME [83] 0.695 0 .689
w/RankAgg (Ours) 0.845 0.765Will more PTM-specific features help? As mentioned
in subsection 4.4, MODEL SPIDER is able to incor-
porate PTM-specific features — the forward pass of
a PTM over the downstream task – to improve the
ranking scores. When no specific features ( k= 0)
exist, we use the general token to rank PTMs (most effi-
cient). In Figure 4 (a), we show that τwincreases when
MODEL SPIDER receives more PTM-specific features.
It balances the efficiency and accuracy trade-off.
5.4 Interpreting MODEL SPIDER by Spider Chart
An interesting by-product of MODEL SPIDER is that we can visualize the ability of a PTM with a
spider chart, which demonstrates which fields the PTM is good at. We cluster the datasets in our
multi-source model zoo into six major groups. Then, we approximate a PTM’s ability on the six
types of tasks with the averaged similarity between a PTM to the tasks in the cluster. The larger the
similarity, the better the PTM performs on that task. In Figure 4 (b), we find a PTM pre-trained on
AID dataset works well on medical and remote sensing tasks, and a PTM pre-trained on NABirds
dataset shows strong ability on birds and animal recognition. The spider chart will help to explain the
application scenarios of a PTM and help PTM recommendations.
6 Conclusion
The proposed M ODEL SPIDER learns to rank PTMs for existing tasks and can generalize the model
selection ability to unseen tasks, even with few-shot examples. The two-stage pipeline in MODEL
SPIDER enables it to fit the resources adaptively. A task is matched with PTMs efficiently based on
their task-agnostic tokens if the resource is limited. While there is a sufficient resource budget, limited
forward passes are carried out over the candidates of top-ranked PTMs, which re-ranks the candidates
9

--- PAGE 10 ---
via incorporating the detailed fitness between the task and the selected PTMs. The learned tokens
help construct a spider chart for each task, illustrating its relevance with all PTMs. The tokens for
models and tasks act as a kind of specification that matches the main design in Learnware [ 87,88].
References
[1]Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, and Michael Isard. Tensorflow: a system for large-scale machine
learning. In OSDI , 2016.
[2]Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C
Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In ICCV , 2019.
[3]Enric Boix Adserà, Hannah Lawrence, George Stepaniants, and Philippe Rigollet. GULP: a prediction-
based metric between representations. In NeurIPS , 2022.
[4]Andrea Agostinelli, Michal Pándy, Jasper R. R. Uijlings, Thomas Mensink, and Vittorio Ferrari. How
stable are transferability metrics evaluations? In ECCV , 2022.
[5]Nir Ailon and Mehryar Mohri. Preference-based learning to rank. Machine Learning , 80(2-3):189–211,
2010.
[6]David Alvarez-Melis and Nicolò Fusi. Geometric dataset distances via optimal transport. In NeurIPS ,
2020.
[7]Ann Arbor. A reasonable social welfare function. Seminar on Applications of Mathematics to Social
Sciences , 1951.
[8]Yajie Bao, Yang Li, Shao-Lun Huang, Lin Zhang, Lizhong Zheng, Amir Zamir, and Leonidas Guibas. An
information-theoretic approach to transferability in task transfer learning. In ICIP , 2019.
[9]Shai Ben-David and Reba Schuller. Exploiting task relatedness for multiple task learning. In COLT , 2003.
[10] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for
domain adaptation. In NIPS , 2006.
[11] Steiner Benoit, DeVito Zachary, Chintala Soumith, Gross Sam, Paszke Adam, Massa Francisco, Lerer
Adam, Chanan Gregory, Lin Zeming, Yang Edward, Desmaison Alban, Tejani Alykhan, Kopf Andreas,
Bradbury James, Antiga Luca, Raison Martin, Gimelshein Natalia, Chilamkurthy Sasank, Killeen Trevor,
Fang Lu, and Bai Junjie. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS ,
2019.
[12] Daniel Bolya, Rohit Mittapalli, and Judy Hoffman. Scalable diverse model selection for accessible transfer
learning. In NeurIPS 2021 , 2021.
[13] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components
with random forests. In ECCV , 2014.
[14] Fatih Çakir, Kun He, Xide Xia, Brian Kulis, and Stan Sclaroff. Deep metric learning to rank. In CVPR ,
2019.
[15] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In ICML , 2020.
[16] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and
state of the art. Proceedings of IEEE , 105(10):1865–1883, 2017.
[17] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing
textures in the wild. In CVPR , 2014.
[18] Adam Coates, Andrew Y . Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In AISTATS , 2011.
[19] Aditya Deshpande, Alessandro Achille, Avinash Ravichandran, Hao Li, Luca Zancato, Charless Fowlkes,
Rahul Bhotika, Stefano Soatto, and Pietro Perona. A linearized framework and a new benchmark for model
selection for fine-tuning. CoRR , abs/2102.00084, 2021.
[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep
bidirectional transformers for language understanding. In NAACL-HLT , pages 4171–4186, 2019.
10

--- PAGE 11 ---
[21] Nan Ding, Xi Chen, Tomer Levinboim, Soravit Changpinyo, and Radu Soricut. Pactran: Pac-bayesian
metrics for estimating the transferability of pretrained models to classification tasks. In ECCV , 2022.
[22] Yao-Xiang Ding, Xi-Zhu Wu, Kun Zhou, and Zhi-Hua Zhou. Pre-trained model reusability evaluation for
small-data transfer learning. In NeurIPS , 2022.
[23] Kshitij Dwivedi and Gemma Roig. Representation similarity analysis for efficient task taxonomy & transfer
learning. In CVPR , 2019.
[24] Kshitij Dwivedi, Jiahui Huang, Radoslaw Martin Cichy, and Gemma Roig. Duality diagram similarity: A
generic framework for initialization selection in task transfer learning. In ECCV , 2020.
[25] Chen Fang, Ye Xu, and Daniel N. Rockmore. Unbiased metric learning: On the utilization of multiple
datasets and web images for softening bias. In ICCV , 2013.
[26] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:
An incremental bayesian approach tested on 101 object categories. In CVPR Workshops , 2004.
[27] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Ávila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray
Kavukcuoglu, Rémi Munos, and Michal Valko. Bootstrap your own latent - A new approach to self-
supervised learning. In NeurIPS , 2020.
[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
InCVPR , 2016.
[29] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep
learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied
Earth Observations and Remote Sensing , 2019.
[30] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona,
and Serge J. Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The
fine print in fine-grained dataset collection. In CVPR , 2015.
[31] Gao Huang, Zhuang Liu, Kilian Q. Weinberger, and Laurens van der Maaten. Densely connected
convolutional networks. In CVPR , 2017.
[32] Long-Kai Huang, Junzhou Huang, Yu Rong, Qiang Yang, and Ying Wei. Frustratingly easy transferability
estimation. In ICML , 2022.
[33] Shibal Ibrahim, Natalia Ponomareva, and Rahul Mazumder. Newer is not always better: Rethinking
transferability metrics, their peculiarities, stability and performance. CoRR , abs/2110.06893, 2021.
[34] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and
Ser-Nam Lim. Visual prompt tuning. In ECCV , 2022.
[35] Thorsten Joachims. Optimizing search engines using clickthrough data. In SIGKDD , 2002.
[36] Brendan Jou and Shih-Fu Chang. Deep cross residual learning for multitask visual recognition. In ACM
MM, 2016.
[37] Maurice G Kendall. A new measure of rank correlation. Biometrika , 30(1/2):81–93, 1938.
[38] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained
image categorization: Stanford dogs. In CVPR workshop on FGVC , volume 2, 2011.
[39] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13) ,
2013.
[40] Nikolaus Kriegeskorte. Representational similarity analysis – connecting the branches of systems neuro-
science. Frontiers in Systems Neuroscience , 2008.
[41] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical
report, 2009.
[42] Yann LeCun, Fu Jie Huang, and Léon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In CVPR , 2004.
11

--- PAGE 12 ---
[43] Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai, Lu Yuan, and Jianfeng
Gao. Efficient self-supervised vision transformers for representation learning. In ICLR , 2022.
[44] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain
generalization. In ICCV , 2017.
[45] Yandong Li, Xuhui Jia, Ruoxin Sang, Yukun Zhu, Bradley Green, Liqiang Wang, and Boqing Gong.
Ranking neural checkpoints. In CVPR , 2021.
[46] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin
transformer: Hierarchical vision transformer using shifted windows. In ICCV , pages 9992–10002, 2021.
[47] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual
classification of aircraft. CoRR , abs/1306.5151, 2013.
[48] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and
algorithms. In COLT , 2009.
[49] Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testing
sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
[50] Brian McFee and Gert R. G. Lanckriet. Metric learning to rank. In ICML , 2010.
[51] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning . Adaptive
computation and machine learning. MIT Press, 2012.
[52] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y . Ng. Reading digits in
natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised
Feature Learning 2011 , 2011.
[53] Cuong V Nguyen, Tal Hassner, Cedric Archambeau, and Matthias Seeger. Leep: A new measure to
evaluate transferability of learned representations. In ICML , 2020.
[54] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of
classes. In ICVGIP , 2008.
[55] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and
data engineering , 22(10):1345–1359, 2009.
[56] Michal Pándy, Andrea Agostinelli, Jasper R. R. Uijlings, Vittorio Ferrari, and Thomas Mensink. Transfer-
ability estimation using bhattacharyya class separability. In CVPR , 2022.
[57] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V . Jawahar. Cats and dogs. In CVPR , 2012.
[58] Huiyan Qi, Lechao Cheng, Jingjing Chen, Yue Yu, Zunlei Feng, and Yu-Gang Jiang. Transferability
estimation based on principal gradient expectation. CoRR , abs/2211.16299, 2022.
[59] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning
transferable visual models from natural language supervision. In ICML , 2021.
[60] Rajeev Ranjan, Vishal M. Patel, and Rama Chellappa. Hyperface: A deep multi-task learning framework
for face detection, landmark localization, pose estimation, and gender recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 41(1):121–135, 2019.
[61] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremental
classifier and representation learning. In CVPR , pages 5533–5542, 2017.
[62] Cédric Renggli, André Susano Pinto, Luka Rimanic, Joan Puigcerver, Carlos Riquelme, Ce Zhang, and
Mario Lucic. Which model to transfer? finding the needle in the growing haystack. In CVPR , 2022.
[63] Michael T Rosenstein, Zvika Marx, Leslie Pack Kaelbling, and Thomas G Dietterich. To transfer or not to
transfer. In NIPS Workshop on Transfer Learning , volume 898, 2005.
[64] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International Journal of Computer Vision , 115(3):211–252, 2015.
[65] Saari, Donald G., and Vincent R. Merlin. The copeland method: I.: Relationships and the dictionary.
Economic Theory , 8(1):51–76, 1996.
12

--- PAGE 13 ---
[66] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2:
inverted residuals and linear bottlenecks. In CVPR , 2018.
[67] Wenqi Shao, Xun Zhao, Yixiao Ge, Zhaoyang Zhang, Lei Yang, Xiaogang Wang, Ying Shan, and Ping Luo.
Not all models are equal: Predicting model transferability in a self-challenging fisher space. In ECCV ,
2022.
[68] Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, and Mingli Song. Deep model transferability from
attribution maps. In NeurIPS , 2019.
[69] Jie Song, Yixin Chen, Jingwen Ye, Xinchao Wang, Chengchao Shen, Feng Mao, and Mingli Song. Depara:
Deep attribution graph for deep knowledge transferability. In CVPR , 2020.
[70] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the
Inception Architecture for Computer Vision. In CVPR , 2016.
[71] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V .
Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR , 2019.
[72] Yang Tan, Yang Li, and Shao-Lun Huang. OTCE: A transferability metric for cross-domain cross-task
representations. In CVPR , 2021.
[73] Anh Tuan Tran, Cuong V . Nguyen, and Tal Hassner. Transferability and hardness of supervised classifica-
tion tasks. In ICCV , 2019.
[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS , 2017.
[75] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing
network for unsupervised domain adaptation. In CVPR , pages 5385–5394, 2017.
[76] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset.
Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
[77] Zirui Wang, Zihang Dai, Barnabás Póczos, and Jaime Carbonell. Characterizing and avoiding negative
transfer. In CVPR , 2019.
[78] Ying Wei, Yu Zhang, Junzhou Huang, and Qiang Yang. Transfer learning via learning to transfer. In ICML ,
2018.
[79] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric
Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara
Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In EMNLP ,
2020.
[80] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. Listwise approach to learning to rank:
theory and algorithm. In ICML , volume 307, pages 1192–1199, 2008.
[81] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei Zhong, and Liangpei Zhang. Aid: A
benchmark dataset for performance evaluation of aerial scene classification. CoRR , abs/1608.05167, 2016.
[82] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-
scale scene recognition from abbey to zoo. In CVPR , 2010.
[83] Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. Logme: Practical assessment of pre-trained
models for transfer learning. In ICML , 2021.
[84] Kaichao You, Yong Liu, Ziyang Zhang, Jianmin Wang, Michael I Jordan, and Mingsheng Long. Ranking
and tuning pre-trained models: A new paradigm for exploiting model hubs. Journal of Machine Learning
Research , 23:1–47, 2022.
[85] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese.
Taskonomy: Disentangling task transfer learning. In CVPR , 2018.
[86] Zhifei Zhang, Yang Song, and Hairong Qi. Age progression/regression by conditional adversarial autoen-
coder. In CVPR , pages 4352–4360, 2017.
[87] Zhi-Hua Zhou. Learnware: on the future of machine learning. Frontiers Computer Science , 10(4):589–590,
2016.
13

--- PAGE 14 ---
[88] Zhi-Hua Zhou and Zhi-Hao Tan. Learnware: Small models do big. CoRR , abs/2210.03647, 2022.
Supplementary Material
We provide details omitted in the main paper.
•Appendix A: Workflow of MODEL SPIDER , encompassing the construction of model-task
tokens, training, and testing, with the “how to” and “answer” format.
•Appendix B: Experimental setups and implementation details of MODEL SPIDER , especially
the two types of pre-training model zoos utilized in the experimental section.
•Appendix C: Additional experimental results conducted along different dimensions of
robustness analysis.
•Appendix D: Additional datasets descriptions and other details mentioned in the main text.
• Appendix E: Discussions and future exploration of M ODEL SPIDER .
A Details and Discussions of M ODEL SPIDER
In the method section of the main text, we elucidate the comprehensive workflow for training and
testing the deployment of MODEL SPIDER . This process encompasses three main steps , including (1)
the extraction of task tokens, (2) the extraction of model tokens, and (3) the construction of a training
scheme that assesses the ranking of matching between model-task tokens, thereby establishing the
ground-truth rank of the model zoo for a given task. Once these three steps have been accomplished,
the subsequent phase entails training the MODEL SPIDER by leveraging the extracted tokens in
conjunction with the ranked ground-truth information.
In essence, the testing and deployment strategy employed by the MODEL SPIDER framework
epitomizes a balance between flexibility and efficiency. By employing a fixed feature extractor ψ
to acquire tokens pertaining to downstream target tasks, the trained MODEL SPIDER undergoes a
singular inference pass , generating an output quantifying the similarity between each model token
and the downstream task token. It then accomplishes the task of ranking the PTMs.
In the forthcoming sections, we elaborate on the details in the form of “ how to do it ” questions. The
training process of MODEL SPIDER is illustrated in Algorithm 1, while the sampling procedure for
training tasks is elaborated in detail in subsection A.2. Additionally, in subsection A.6, we expound
upon the training strategy of PTM-Specific task tokens. Analogously, the testing process of MODEL
SPIDER is presented in Algorithm 2, and in subsection A.7, we provide a comprehensive exposition
of the entire deployment workflow for ranking pre-trained models.
A.1 How to construct model tokens and task ones
This section supplements the details of subsection 4.2 and subsection 4.4, i.e., the construction of the
model-task tokens, including the enriched PTM-specific ones.
PTM token. The dimension of PTM token, i.e., thedofθ∈Rdis implemented as 1024 . It is a
learnable parameter that is optimized with the training process.
Task token. Theψis implemented by a pre-trained Swin-B-based EsViT [ 46,43] (linked at
https://github.com/microsoft/esvit ), self-supervised learning on the ImageNet-1K [ 64] with
batch size 512. In our experiments, this encoder acts as a wide-field feature extractor and is fixed
without updating. The shape of task token µ(T)∈Rd×CTvaries with the number of categories
of downstream tasks. As mentioned in subsection 4.4, task tokens enriched by the PTM-specific
features are obtained through the forward pass of a PTM. We use another fully connected layer to
project the PTM-specific feature to align with the model token.
A.2 How to sample the training tasks of M ODEL SPIDER
We sample tasks for training MODEL SPIDER from additional datasets that are disjoint from the
downstream tasks. These additional datasets possess notable differences and encompass diverse
domains. Notably, MODEL SPIDER does not require substantial additional data for training. We
14

--- PAGE 15 ---
PTM-1NCE:
LEEP:
LogME :> > >
PTM-2 PTM-3 PTM-4RankAgg> > >
> > >> > >Figure 5: An illustration of the rank aggregation approach to ensemble the ranking of PTMs relying
on diverse transferability assessment methods (three methods depicted in the figure). The PTMs that
outperform more other PTMs should be placed ahead.
sample the training tasks from a diverse pool of datasets. The number and size of the mixed datasets
are controlled within a certain range. For more details, please see Appendix B.
A.3 How to see the relationship between RankAgg and M ODEL SPIDER
We claim that RankAgg proposed by us cannot be considered as a direct baseline method. Firstly,
RankAgg involves a substantial computational overhead when used as a stand-alone method for
ranking PTMs. This is primarily due to the time and memory requirements of computing the base
selection methods. Using RankAgg directly as a baseline would introduce a significant computational
burden. However, we introduce RankAgg as an approximate ground-truth method for pre-computing
in the training part of M ODEL SPIDER . It is more efficient compared to full parameter fine-tuning.
Actually, MODEL SPIDER aims to demonstrate its broad generalization capacity by leveraging
RankAgg to process an independent set of mixed data that has no overlap with the test data. This
independent evaluation showcases the effectiveness of MODEL SPIDER in a real-world scenario and
emphasizes its ability to handle diverse data efficiently. RankAgg itself does not play a role during
the test execution of M ODEL SPIDER .
A.4 How to efficiently approximate the training ground-truth of M ODEL SPIDER
This section complements subsection 4.4, wherein the training and ranking of the model zoo across
multiple datasets are discussed. However, obtaining the ranking for all historical tasks through brute
force is computationally expensive. To mitigate this issue, we introduce a rank aggregation method
denoted as RankAgg , which serves as an approximation of the ground truth ranking.
Existing PTM selection methods rely on the PTM-specific features Φm
Tto estimate the transferability
score. Different methods may have diverse score values — a PTM will be placed in different
positions based on the scores provided by various methods. We empirically observe that some
popular approaches such as NCE [ 73], LEEP [ 53], and LogME [ 83,84] show “good but diverse”
PTM ranking orders, so an intuitive approach to improving the transferability estimation quality is to
ensemble their ranking results to a stronger single order.
As mentioned in subsection 4.3, given {ˆt1,ˆt2, . . . , ˆtA}as multiple rankings over the same set of
MPTMs for a target task T,i.e., the order sorted by the estimations of transferability via various
methods, we take advantage of Copeland’s aggregation method [7, 65] to ensemble the orders.
¯t={¯tϕm→T}M
m=1= RankAgg 
{ˆt1,ˆt2, . . . , ˆtA}
. (7)
Copeland’s aggregation compares each pair of ranking candidates and considers all preferences to
determine which of the two is more preferred as illustrated in Figure 5.
Taking model m, m′as an example, we define the majority relation to express the one-on-one
dominance between these two models. Precisely, assuming that Amapproaches rank model mabove
model m′,i.e.,ˆti,m>ˆti,m′withAm×suchˆti, while the remaining Am′ones do the opposite. Note
thatAm+Am′=A. The m >Mm′just in case Am> A m′, and correspondingly m=Mm′
indicates Am=Am′. In summary, we define the aggregation score for model mas:
¯tϕm→T= #{i|m >Mi}+1
2#{i|m=Mi}, (8)
15

--- PAGE 16 ---
Algorithm 1 The Training Part of the M ODEL SPIDER
1:Input: fixed ψ, learnable parameters Θ, including model token {θm}M
m=1, FC for projection,
and parameters of the transformer-based M ODEL SPIDER
2:Sample training tasks {Ti}from the additional mixed datasets as in subsection A.2
3:Extract and save all task tokensS
i{µ(Ti)}withψ.
4:for all sampled task Tido
5: form= 1toMdo
6: ifthemth PTM-specific features is available (randomly holds) then
7: Derive the PTM-specific task token as mentioned in subsection 4.4.
8:
ˆtϕm→Ti= sim Θ 
θm,P⊤
mµm(Ti)
.
9: else
10: Take model token θmand estimate the similarity of model-token pairs as Eq. 6.
11:
ˆtϕm→Ti= sim Θ(θm,µ(Ti)).
12: end if
13: end for
14: From above for, the estimation scores of M ODEL SPIDER ˆtis conducted.
15: Calculate H-Score, NCE, LEEP, and LogME on Ti.
16: Aggregate on the results of existing approaches to obtain ground-truth ¯tas in subsection 4.3.
17:
¯t={¯tϕm→Ti}M
m=1= RankAgg( {ˆt1,ˆt2, . . .}).
18: Optimize the parameters of M ODEL SPIDER with ranking loss ℓrankw.r.t. the ranking of ¯t.
ℓrank(ˆt,t) =MX
m=1−log 
exp ˆtdsc(m)
PM
l=mexp ˆtdsc(l)!
.
19: Compute ∇Θℓrankand update corresponding parameters with the gradients
20:end for
21:Output: learned Θ, including {θm}M
m=1, FC, and parameters of the M ODEL SPIDER
where #{·}is the size of the set. The aggregation score for a model is the number of others over
which they have a majority preference plus half the number of models with which they have a
preference tie. In our implementation, we aggregate the results of NCE, LEEP, LogME, and H-Score.
RankAgg can become quite time-consuming when calculating PTM ranking scores for the entire
dataset, mainly due to the substantial overhead of computing the base selection methods. In our
experimental setup, we integrate the RankAgg method as a module during the training phase,
enabling us to pre-compute the rankings for each task. The RankAgg may raise the computational
burden if employed directly as a testing baseline. Therefore, we employ RankAgg for the sampled
few-shot tasks to balance ranking accuracy with efficiency and only use it in the training part. Note
thatMODEL SPIDER learns based on the RankAgg results, but is deployed independently of both
it and other baseline methods. Since RankAgg summarizes the PTM generalization capability on
differentiated tasks spanning multiple domains, our model derived from the pre-aggregated rankings
can learn the PTM ranking ability on a broader range of unseen tasks.
A.5 How to learn the similarity of model-task token
This section elaborates on subsection 4.1, i.e., the learning process of MODEL SPIDER , especially
the Transformer based estimation. The Transformer-based module of model-task similarity. The
model-task token is concatenated as a sequence of features. The Transformer based module naturally
16

--- PAGE 17 ---
Algorithm 2 The Downstream Inference Part of M ODEL SPIDER
Input: target task T, fixed ψ, learned Θ
Obtain task token µ(T)withψas Eq. 5.
Estimate similarity of model-token pairs as Eq. 6
ˆt=ˆtϕm→T= sim Θ(θm,µ(T))	M
m=1.
Select top- kPTMs via ˆt=ˆtϕm→T	M
m=1.
Obtain the indexes in descending order via dsc (·).
form= dsc (1) todsc (k)do
Re-construct enriched token µm(T), and update:
ˆtϕm→T= sim Θ 
θm,P⊤
mµm(T)
end for
Output: Rank PTMs with ˆt=ˆtϕm→T	M
m=1
fits and takes such input. Concretely, in operation, transformer ( ·)is formalized as:
transformer ( z) =z+α(Q,K,V=z)
=z+ softmax 
zWQ·(zWK)⊤
√
d!
zWV.(9)
we apply linear projections on the query, key, and values using WQ,WK, and WV, respectively. The
similarity between prototypes is measured by the inner product in the transformed space, which
results in larger weights of the attention head α. Here dis the size of every attention head. The output
of the corresponding position of the model token is forwarding passed through a learnable MLP and
then obtains the fitness estimated score of PTM selection.
The learnable parameters in MODEL SPIDER .To learn a PTM ranker, we optimize Mmodel
tokens {θm}M
m=1, the fully connected layer projection heads of the PTM-specific task tokens Φm
Ti
(mentioned in subsection 4.4) and the transformer-based model-task similarity evaluator sim(·,·),
which is the main mapping and estimation module (mentioned in subsection 4.2).
A.6 How to re-rank with PTM-specific task tokens
As described in subsection 4.4 of the main text, we initially extract generic features using a fixed ψ
and conduct with the invariant task token across all PTMs. These features are used to generate a
coarse-grained ranking by comparing the similarity between each task token and the model token.
However, this ranking is solely based on a standardized task representation and does not account for
the specific task-related information for each individual PTM.
Hence, we propose the re-ranking strategy specifically targeted at the top-k PTMs. During the testing
phase, we leverage the coarse-grained ranking and perform inference on the downstream task with
these top-k PTMs. Such PTM-specific task tokens are worked to update their similarity with the
downstream task, as outlined in Algorithm 2. Notably, in the third line of the algorithm, we conduct a
re-ranking based on the revised similarity scores obtained through this process.
A.7 How to deploy M ODEL SPIDER for testing
For a novel downstream task, we employ the generic feature extractor ψto extract the task token.
We then evaluate the similarity between each PTM in the model zoo and the given downstream task
using the learned model token and a transformer-based MODEL SPIDER . If computational resources
are available, we can leverage the results from the previous round to enhance the ranking process.
Specifically, we can select the top-k PTMs from the previous ranking, extract their features, and apply
the re-ranking approach as described in subsection A.6.
17

--- PAGE 18 ---
B Experimental Setups and Implementation Details
In this section, we introduce the experiment setups and implementation details, including constructing
the pre-trained model zoo and training as well as deploying M ODEL SPIDER .
B.1 Single-source heterogeneous model zoo
Construction of the model zoo. We follow [ 83] and construct a model zoo with 10PTMs pre-trained
on ImageNet [ 64] across 5families of architectures available from PyTorch. Concretely, they are
Inception V1 [ 70], Inception V3 [ 70], ResNet 50 [ 28], ResNet 101 [ 28], ResNet 152 [ 28], DenseNet
121 [ 31], DenseNet 169 [ 31], DenseNet 201 [ 31], MobileNet V2 [ 66], and NASNet-A Mobile [ 71].
The model zoo spans PTMs of multiple parameter quantities. These pre-training models cover most
of the supervised pre-training models the researchers employ.
The downstream tasks. There are 9downstream tasks from various fields, including Aircraft [ 47],
Caltech101 [ 26], Cars [ 39], CIFAR10 [ 41], CIFAR100 [ 41], DTD [ 17], Pets [ 57], and SUN397 [ 82]
for classification, UTKFace [ 86] and dSprites [ 49] for regression. We use official train-test splits on
each dataset and calculate the estimation scores for the baseline approaches on the training part.
Transferred accuracy ranking of PTMs (ground-truth) after fine-tuning downstream tasks.
We follow You et al. [83] to obtain the ground-truth transferability score as well as the rankings
t={tϕm→T}M
m=1(M= 10 ) with careful grid-search of hyper-parameters. Specifically, we
grid search the learning rates ( 7learning rates from 10−1to10−4, logarithmically spaced) and
weight decays ( 7weight decays from 10−6to10−3, logarithmically spaced) to select the best
hyper-parameter on the validation set and compute the accuracy on the downstream test set. The
training and computation of such a ground truth necessitates a substantial investment of over 1K GPU
hours, imposing significant financial and computational burdens. Consequently, the feasibility of
accomplishing this task within the constraints of training M ODEL SPIDER is rendered unattainable.
Sampling details of training tasks. We sample the training tasks from a diverse pool of datasets.
The datasets considered for sampling include EuroSAT, OfficeHome, PACS, SmallNORB, STL10,
and VLCS. To ensure a representative training set, we randomly sample 832tasks from all datasets.
Each task is distributed across 2 to 4 mixed datasets and consists of 100 categories, and for each
category, we randomly select 50 examples. In cases where the number of categories or examples to
be sampled exceeds the specified limits, we select the maximum allowable value.
Discussions. This model zoo covers several classical structures commonly used in deep learning.
The number of model parameters ranges widely, with large application potential. Still, there is also a
situation where PTMs with larger scales tend to perform better in classification tasks and regression
ones, making certain rankings always better on some datasets.
B.2 Multi-source heterogeneous model zoo
Construction of the Model Zoo. As mentioned in the main text, we construct a large model zoo
where 42heterogeneous PTMs are pre-trained from multiple datasets in different domains, including
animals [ 30,38], general and 3D objects [ 26,42,41,39,13], plants [ 54], scene-based [ 82], remote
sensing [ 81,16,29] and multi-domain recognition [ 44]. The concrete datasets are Caltech101 [ 26],
Cars [ 39], CIFAR10 [ 41], CIFAR100 [ 41], SUN397 [ 82], Dogs [ 38], EuroSAT [ 29], Flowers [ 54],
Food [ 13], NABirds [ 30], PACS [ 44], Resisc45 [ 16], SmallNORB [ 42] and SVHN [ 52]. The models’
structures are 3similar parameter-magnitude architectures, i.e., Inception V3 [ 70], ResNet 50 [ 28] and
DenseNet 201 [ 31]. The setting of the multi-source heterogeneous model zoo includes significantly
more pre-training data than the single-source heterogeneous one described above. We pre-train the
models with 3structures on 14datasets mentioned above ( 3×14 = 42 , initialized from the weights
of the corresponding ImageNet pre-trained models).
The downstream tasks. We select 3representative datasets as the downstream test tasks and conduct
the PTM selection methods on them. Concretely, they are Aircraft [ 47], DTD [ 17] and Pets [ 57]. As
outlined in the following description, we obtain the transferred fine-tuning accuracy (ground-truth)
with an equivalent level of hyper-parameters search strategies.
Transferred accuracy ranking (ground-truth). Similarly, we adopt downstream supervised learning
with optimizing by cross-entropy loss. We meticulously conduct a grid-search of hyper-parameters,
18

--- PAGE 19 ---
/uni00000024/uni0000004c/uni00000055/uni00000046/uni00000055/uni00000044/uni00000049/uni00000057 /uni00000026/uni00000044/uni0000004f/uni00000057/uni00000048/uni00000046/uni0000004b/uni00000014/uni00000013/uni00000014 /uni00000026/uni00000044/uni00000055/uni00000056 /uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013 /uni00000026/uni0000002c/uni00000029/uni00000024/uni00000035/uni00000014/uni00000013/uni00000013 /uni00000027/uni00000037/uni00000027 /uni00000033/uni00000048/uni00000057 /uni00000036/uni00000038/uni00000031/uni00000016/uni0000001c/uni0000001a
/uni00000027/uni00000052/uni0000005a/uni00000051/uni00000056/uni00000057/uni00000055/uni00000048/uni00000044/uni00000050/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000bw/uni0000000c
/uni0000002f/uni00000048/uni00000056/uni00000056/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000056
/uni00000036/uni0000004c/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000055/uni00000003
/uni00000032/uni00000058/uni00000055/uni00000056/uni00000003/uni0000000b/uni00000055/uni00000048/uni00000053/uni00000052/uni00000055/uni00000057/uni0000000cFigure 6: Ablation studies on simpler ψand less training tasks. We observed a slight decrease in
performance when employing a weakened fixed feature extractor ψforMODEL SPIDER . Reducing
the diversity of training tasks may conduct in performance degradation on some datasets.
/uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000016/uni00000013 /uni00000017/uni00000013 /uni00000018/uni00000013 /uni00000019/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000048/uni0000005b/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000013/uni00000011/uni0000001a/uni00000018
/uni00000013/uni00000011/uni00000018/uni00000013
/uni00000013/uni00000011/uni00000015/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000bw/uni0000000c
/uni0000002b/uni00000010/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000031/uni00000026/uni00000028
/uni0000002f/uni00000028/uni00000028/uni00000033
/uni00000010/uni0000002f/uni00000028/uni00000028/uni00000033
/uni0000002f/uni00000052/uni0000004a/uni00000030/uni00000028
/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000024/uni0000004a/uni0000004a
(a)on Aircraft
/uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000019/uni00000013
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000052/uni00000049/uni00000003/uni00000048/uni0000005b/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000046/uni0000004f/uni00000044/uni00000056/uni00000056/uni00000013/uni00000011/uni00000018/uni00000013
/uni00000013/uni00000011/uni00000015/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018/uni00000014/uni00000011/uni00000013/uni00000013/uni00000026/uni00000052/uni00000055/uni00000055/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000000bw/uni0000000c
/uni0000002b/uni00000010/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000031/uni00000026/uni00000028
/uni0000002f/uni00000028/uni00000028/uni00000033
/uni00000010/uni0000002f/uni00000028/uni00000028/uni00000033
/uni0000002f/uni00000052/uni0000004a/uni00000030/uni00000028
/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000024/uni0000004a/uni0000004a (b)on Caltech101
Figure 7: Correlation ( τw) given various number of examples per class on (a)Aircraft and (b)
Caltech101. M ODEL SPIDER shows stable and promising results in the low-shot scenario.
such as optimizers, learning rates, and weight decays ( 2optimizers as SGD or Adam, 6learning
rates from 5×10−2to10−4, and 3weight decay values from 5×10−4to10−5, batch size of 128,
and the maximum epoch of 100). For the multi-domain dataset, like PACS [ 44], we set the test set
to the same domain as the training set to reveal the in-domain performance. For the rest, we use
the official train-test splits. We build the model zoo with around 5K GPU hours (on NVIDIA V100
GPUs). Similarly, when dealing with the expanded model zoo, the utilization of rigorous training
methodologies to acquire the requisite ground truth for training M ODEL SPIDER is eschewed.
Sampling details of training tasks. The sampling process for the multi-source heterogeneous model
zoo is consistent with the single-source one mentioned above. In this case, we use the following
datasets as the auxiliary set, i.e., Caltech101, Cars, CIFAR10, CIFAR100, Dogs, EuroSAT, Flowers,
Food, NABirds, PACS, Resisc45, SUN397, and SVHN. We randomly sample 4352 tasks for training.
Discussion. The availability of a multi-source heterogeneous model zoo introduces a wider array
of models with varying structures, effectively covering a broader scope of domain knowledge.
Consequently, this heightened diversity presents an increased difficulty in accurately ranking PTMs.
Particularly, when a substantial gap exists between the characteristics of downstream tasks and the
major PTMs, the ranking accuracy of some baseline methods undergoes a precipitous decline.
C Additional Experimental Results
C.1 Ablation studies on simpler ψand less training tasks
We deploy additional experience with weakened conditions to verify the robustness of MODEL
SPIDER . In Figure 6, we first introduce an attenuated simpler ψ, the additional encoder except for the
PTMs in the model zoo. We import the tiny format pre-trained Swin-Transformer from EsViT (about
this, please refer to subsection A.1 for more details). It has about half the number of parameters. The
results show that although attenuated ψhas only half of the parameters, it can still assist MODEL
SPIDER in expressing task tokens.
We then halve the training tasks to verify the significance of the training part diversity. We find that
except for the performance degradation of the DTD dataset, the others are still flush with performance.
MODEL SPIDER learns the characteristics of different PTM ability dimensions well despite the
absence of training tasks.
19

--- PAGE 20 ---
Table 5: Ablation studies on the performance of MODEL SPIDER when the pre-trained model
repository grows dynamically.
MODEL SPIDER Aircraft Caltech101 Cars CIFAR10 CIFAR100 DTD Pets SUN397 Mean
When the number of PTMs increases
w/ number of 3 0 .545 1 .000 1 .000 1 .000 0 .182 1 .000 1 .000 1 .000 0 .841
increase to 6 0 .573 0 .627 0 .818 0 .905 0 .839 0 .445 0 .888 0 .336 0 .679
increase to 10 0 .568 0 .637 0 .576 0 .797 0 .695 0 .796 0 .573 0 .436 0 .635
C.2 Ablation studies on the influence of training loss
Table 4: The weighted τwofMODEL SPIDER
variants when the training objective is imple-
mented by different loss functions. “Mean” de-
notes the averaged performance over 8 datasets.
Method CIFAR10 Mean
w/ MSE 0.558 0 .526
w/ ListMLE [80] 0.777 0 .735
w/ℓrank(Ours) 0.845 0.765As stated in the main text, the learning process of
MODEL SPIDER incorporates a ranking loss. To as-
sess the efficacy of this selection, alternative regres-
sion or ranking loss functions, such as mean square
error (MSE) and ListMLE [ 80], are employed as
replacements. The outcomes, presented in Table 4,
clearly demonstrate that the presented ranking loss
function surpasses the other alternatives in terms of
both effectiveness and robustness. Notably, when
alternative loss functions are utilized, the overall
performance of MODEL SPIDER experiences a sub-
stantial decline. These findings underscore the indispensable role of the ranking loss function within
the framework of M ODEL SPIDER .
C.3 Ablation studies on the different shots of RankAgg and other baselines
We conduct an ablation analysis to compare RankAgg with several baseline methods on Aircraft and
Caltech101 datasets with respect to the τwof the PTM ranking. We examined the variation of these
metrics and their corresponding confidence intervals (in 95%) as the number of samples per class
(shot) increased. The results, depicted in the provided Figure 7, are based on the average values and
confidence intervals obtained from 30 randomly sampled sets for each shot. Due to computational
constraints, certain baseline methods were omitted from the analysis. Notably, our findings reveal
that the rank aggregation strategy effectively consolidates diverse perspectives on PTM ranking and
consistently surpasses the performance of baselines across almost all shots.
C.4 Ablation studies on the dynamically incremental model zoo
When encountering new PTMs during the model selection task, the previously trained model token
inMODEL SPIDER can be dynamically learned and updated. We employ an incremental learning
approach [ 61] to address this challenge. Specifically, we sample 25% target tasks where the PTM
ranking is closest to the average of all and insert the approximated accuracy of the new PTMs on
them. This newly constructed ranking ground-truths include the correlation between old and new
model tokens, reducing the influence of imbalanced incremental data.
We performed ablation studies to investigate the behavior of MODEL SPIDER as the pre-trained model
zoo dynamically expanded. Our analysis focused on how can MODEL SPIDER could quickly adapt to
newly added PTMs and integrate them into the ranking process. The results in Table 5 demonstrate
that as the size of the model zoo increased from 3to6and then to 10,MODEL SPIDER demonstrated
the ability to incrementally learn the recommended ranking for the new additions to the model zoo.
The incrementally learned ranking for the entire PTM zoo exhibited slightly lower accuracy than
the results of direct training on all PTMs. Nonetheless, MODEL SPIDER consistently maintained an
excellent level of performance.
C.5 Confidence intervals for few-shot setting in Table 1 of the main text
We include the confidence intervals (in 95%) for the few-shot experiments in the respective section of
Table 1 for the main text. These intervals were obtained through 30 repeated trials, providing a robust
estimate of the performance variability in a few-shot manner.
20

--- PAGE 21 ---
Table 6: The confidence interval (in 95%) for few-shot evaluation ( 10examples per class and 30
trials) in Table 1 of the main text. Specific features of Top- 3ranked PTMs are employed.
MethodDownstream Target Dataset
Aircraft Caltech101 Cars CIFAR10 CIFAR100 DTD Pets SUN397
Few-Shot Evaluation (10-example per class)
H-Score [8]-0.014±0.140.078±0.130.375±0.090.018±0.120.005±0.14-0.028±0.12-0.006±0.150.853±0.02
NCE [73] 0.273±0.050.534±0.070.597±0.020.267±0.080.232±0.040.362±0.060.352±0.090.793±0.03
LEEP [53] 0.069±0.04-0.038±0.010.476±0.030.530±0.040.471±0.02-0.111±0.020.567±0.020.468±0.01
N-LEEP [45]-0.559±0.060.476±0.050.743±0.040.515±0.060.707±0.030.027±0.070.713±0.040.812±0.02
LogME [83] 0.341±0.020.453±0.010.497±0.010.718±0.020.698±0.030.407±0.010.657±0.020.817±0.00
PACTran [21]0.136±0.050.262±0.020.484±0.050.631±0.020.614±0.03-0.227±0.030.701±0.030.477±0.03
OTCE [72] -0.316±0.01-0.050±0.00-0.127±0.000.515±0.000.505±0.00-0.168±0.010.406±0.000.210±0.00
LFC [19] 0.226±0.01-0.226±0.01-0.235±0.020.330±0.040.271±0.01-0.669±0.03-0.059±0.04-0.151±0.02
Ours 0.382±0.040.711±0.000.727±0.010.870±0.010.977±0.020.686±0.020.717±0.020.933±0.03
C.6 Illustration of re-ranking with PTM-specific task token
In subsection 4.4, we discuss the learnable model token, which captures the empirical performance of
a PTM across various training tasks. This training scheme serves to decouple the task token from
the forward pass of each PTM. Compared to the task token guided solely by general features, the
PTM-specific task token provides more informative clues. By constructing it with the forwarding
pass of PTM, we can incorporate the source PTM’s adaptation information for downstream tasks.
Our approach allows for the re-ranking of estimated PTM rankings using PTM-specific task tokens.
Since more forward pass consumes more resources, MODEL SPIDER further improves performance
and provides a dynamic resource adaptation option with PTM-specific features.
Illustrated in Figure 8 is an example of model re-ranking in the context of a heterogeneous multi-
source model zoo. The MODEL SPIDER , after extracting PTM-specific task tokens, accomplished
a more precise PTM ranking. We re-construct the PTM-specific task token on the Dogs dataset
pre-trained. Our investigation focuses on the Aircraft downstream dataset, and intriguingly, we
discover that PTMs trained on multi-scenario multi-target datasets possessed inherent advantages
when applied to the aircraft domain. This advantage can be attributed to their generally strong
recognition capabilities for diverse targets. Remarkably, even models pre-trained on the Food dataset
demonstrated exceptional performance on the Aircraft dataset. Despite the notable dissimilarities
between the Food and Aircraft datasets, we conjecture that the Food-pre-trained models not only
exhibit proficiency in recognizing multiple targets, encompassing various food items but also harbor
latent potential for fine-grained recognition within the food domain. Consequently, these PTMs
transfer their fine-grained recognition capacity to the aircraft domain. In contrast, the Dogs dataset,
characterized by a narrow focus on a single biological species, impedes successful transfer to the
Aircraft task.
The substantial disparities between the datasets pose a significant challenge for conventional baseline
methods, often failing to prioritize the Food-pre-trained model. However, MODEL SPIDER success-
fully learns to rank the Food-pre-trained one and, through a meticulous screening process followed
by result re-ranking, MODEL SPIDER identifies that the Caltech101-pre-trained model outperforms
the Dogs-pre-trained one due to its superior multi-target recognition capabilities, thereby exhibiting
enhanced transfer performance.
D More Details
D.1 Comparison of the time consumption and memory footprint (details in Figure 1(c))
Figure 1(c) shows the average efficiency vsperformance comparison over 5baseline approaches
andMODEL SPIDER . The k= 0,k= 3,k= 6,k= 36 , and k= 42 correspond to inference w/o
PTM-specific features, w/ 3,6,36, and 42ones. Following [ 83], we measure the wall-clock time
(second) and memory footprint (MB) with code instrumentation.
21

--- PAGE 22 ---
AircraftPre-trained on
SUN397Pre-trained on
DogsPre-trained on
FoodPre-trained on
Caltech101Model Zoo
Ground -Truth:
Food>SUN397>Caltech101 >DogsDownstream Task
MODEL SPIDER (Ours) :H-Score: Caltech101 >Dogs>SUN397>Food
NCE:  Dogs>Caltech101 > Food>SUN397Baselines:
w/PTM -Specific task token *
Update Dogs -pretrained measure
Bad Result…
𝟒𝟒𝟒𝟒PTMs
86.86%PTM Ranking:
81.97% 80.65% 78.19%
Bad Result
w/o PTM -specific feature :  Food>SUN397>Dogs>Caltech101
Updated
Task Token①
×
OriginalModel Token̂𝑡𝑡𝒯𝒯UpdatedMeasure ②measure score: -1.608                             - 2.363
-2.792  < -2.363 Update
w/ PTM -specific feature :    Food>SUN397> Caltech101 >DogsFigure 8: Illustrative re-ranking example with enhanced ranking through PTM-specific task token.
Table 7: Comparison of the time consumption and memory footprint of fine-tuning, RankAgg ,
different baseline approaches, and M ODEL SPIDER to rank the PTMs.
Approaches Wall-clock Time ( second ) Memory Footprint ( MB)
RankAgg 7,318.06 10,405.32
Fine-tuning (all parameters) 614,497.22 13,872.81
H-Score 2,358.70 9,367.74
NCE 2,196.53 8,121.49
LEEP 2,215.06 8,209.33
N-LEEP 4,963.01 9,850.84
LogME 2,571.99 8,217.80
MODEL SPIDER (w/o PTM-Specific Feature) 52.36 608.01
MODEL SPIDER (w/3PTM-Specific Feature) 105.19 1,386.43
MODEL SPIDER (w/6PTM-Specific Feature) 175.87 1,760.28
MODEL SPIDER (w/36PTM-Specific Feature) 2,180.23 7,989.35
MODEL SPIDER (w/ all ( 42) PTM-Specific Feature) 2,402.77 9,954.09
D.2 Datasets Description
We show the datasets description Table 8 with some examples Figure 9 covered in this paper.
E Discussions
There are two promising directions of MODEL SPIDER . First, MODEL SPIDER exhibits the unique
characteristic of not relying on the forward pass of the model zoo, thereby enabling the evaluation of
task compatibility with classical machine learning models . Then, MODEL SPIDER could be applied
to the case when we use other criteria in addition to fine-tuning performance to measure the fitness
between a model and a task.
22

--- PAGE 23 ---
Aircraft CIFAR10 DTD Cars Caltech101 CIFAR100 STL10
NABirds Resisc 45 Pet AID PACS VLCS SUN397
Flowers CUB 2011 Dogs EuroSAT SmallNORB SVHN Food
Figure 9: Examples of datasets.
Table 8: The number of training images, testing images and classes with the link to download the dataset.
Dataset Training Images Testing Images # Classes URL
Aircraft [47] 6,667 3,333 100 https://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/#aircraft
CIFAR10 [41] 50,000 10,000 10 https://www.cs.toronto.edu/~kriz/cifar.html
CIFAR100 [41] 50,000 10,000 100 https://www.cs.toronto.edu/~kriz/cifar.html
DTD [17] 3,760 1,880 47 https://www.robots.ox.ac.uk/~vgg/data/dtd/
Stanford Cars [39] 8,144 8,041 196 https://ai.stanford.edu/~jkrause/cars/car_dataset.html
Caltech101 [26] 3,060 6,084 101 http://www.vision.caltech.edu/Image_Datasets/Caltech101/
STL10 [18] 5,000 8,000 10 https://cs.stanford.edu/~acoates/stl10/
Oxford Flowers 102 [54] 2040 6149 102 https://www.robots.ox.ac.uk/~vgg/data/flowers/102/
CUB-200 [76] 5994 5793 200 http://www.vision.caltech.edu/visipedia/CUB-200-2011.html
Stanford Dogs [38] 12,000 8,580 120 http://vision.stanford.edu/aditya86/ImageNetDogs/
EuroSAT [29] 21,600 5,400 10 https://github.com/phelber/eurosat
SmallNORB [42] 24,300 24,300 5 https://cs.nyu.edu/~ylclab/data/norb-v1.0-small/
SVHN [52] 73,257 26,032 10 http://ufldl.stanford.edu/housenumbers/
Food-101 [13] 75,750 25,250 101 https://www.tensorflow.org/datasets/catalog/food101
NABirds [30] 23,929 24,633 555 https://dl.allaboutbirds.org/nabirds
NWPU-RESISC45 [16] 25,200 6,300 45 https://www.tensorflow.org/datasets/catalog/resisc45
Oxford-IIIT Pets [57] 3,680 3,669 37 https://www.robots.ox.ac.uk/~vgg/data/pets/
AID [81] 8,000 2,000 30 https://captain-whu.github.io/AID/
PACS [44] 5,446 616 7 https://domaingeneralization.github.io/#data
VLCS [25] 4,690 2,234 5 https://github.com/belaalb/G2DM#download-vlcs
Office-Home [75] 11,231 11,231 65 https://www.hemanthdv.org/officeHomeDataset.html
SUN397 [82] 87,003 21,751 397 https://vision.princeton.edu/projects/2010/SUN/
ImageNet-1K [64] 1,281,167 50,000 1000 http://image-net.org/download
23
