# Adaptive Gating trong Mixture-of-Experts cho Mô hình Ngôn ngữ
Jiamin Li1, Qiang Su1, Yitao Yang2, Yimin Jiang3, Cong Wang1, Hong Xu2
1City University of Hong Kong
2The Chinese University of Hong Kong
3Unaffiliated

Tóm tắt
Các mô hình ngôn ngữ lớn, như ChatGPT của OpenAI, đã thể hiện khả năng hiểu ngôn ngữ đặc biệt trong nhiều nhiệm vụ NLP khác nhau. Mixture-of-experts (MoE) được kích hoạt thưa thớt đã nổi lên như một giải pháp đầy hứa hẹn để mở rộng quy mô mô hình trong khi duy trì số lượng phép tính không đổi. Mô hình MoE hiện tại áp dụng mạng gating cố định nơi mỗi token được tính toán bởi cùng một số lượng expert. Tuy nhiên, cách tiếp cận này mâu thuẫn với trực giác của chúng ta rằng các token trong mỗi chuỗi khác nhau về độ phức tạp ngôn ngữ và do đó yêu cầu chi phí tính toán khác nhau. Ít được thảo luận trong nghiên cứu trước đây về sự cân bằng giữa tính toán mỗi token và hiệu suất mô hình. Bài báo này giới thiệu adaptive gating trong MoE, một chiến lược huấn luyện linh hoạt cho phép các token được xử lý bởi một số lượng expert biến đổi dựa trên phân phối xác suất của expert. Khung được đề xuất bảo toàn tính thưa thớt trong khi cải thiện hiệu quả huấn luyện. Ngoài ra, curriculum learning được tận dụng để giảm thêm thời gian huấn luyện. Các thí nghiệm mở rộng trên các nhiệm vụ NLP đa dạng cho thấy adaptive gating giảm tối đa 22.5% thời gian huấn luyện trong khi duy trì chất lượng suy luận. Hơn nữa, chúng tôi thực hiện phân tích toàn diện về các quyết định định tuyến và trình bày hiểu biết của chúng tôi khi adaptive gating được sử dụng.

1 Giới thiệu
Lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) đã trải qua một cuộc cách mạng đáng chú ý được thúc đẩy bởi những tiến bộ nhanh chóng trong các mô hình ngôn ngữ (Cha; Touvron et al., 2023; Bar; pal). Chúng thể hiện khả năng "nổi lên" được gọi là cho nhiều ứng dụng đa dạng (Wei et al., 2022). Tuy nhiên, khi nhu cầu cho các ứng dụng này tiếp tục tăng, khả năng mở rộng của những mô hình này đặt ra một rào cản ngày càng thách thức do các ràng buộc về tài nguyên tính toán, dung lượng bộ nhớ, băng thông kết nối, v.v. (Pope et al., 2023).

Mixture-of-experts (MoE) được kích hoạt thưa thớt là một paradigm đầy hứa hẹn để giải quyết vấn đề khả năng mở rộng trong khi duy trì số lượng FLOP tính toán không đổi (Lepikhin et al., 2020; Fedus et al., 2021). MoE sử dụng một tập hợp các expert để cùng nhau giải quyết nhiệm vụ học tập. Mỗi đầu vào kích hoạt một tập con các expert, dẫn đến một đồ thị tính toán thay đổi động và thưa thớt. Phương pháp này phân phối hiệu quả việc tính toán giữa các expert, tăng dung lượng mô hình và cải thiện hiệu quả huấn luyện (Du et al., 2022; Rajbhandari et al., 2022). Gần đây, đã có khá nhiều công trình trước đây về cải thiện hiệu suất của Transformer sử dụng MoE (Rajbhandari et al., 2022; Zoph et al., 2022; Chen et al., 2023a; Gale et al., 2022).

Mặc dù MoE có lợi ích về khả năng mở rộng, nó vẫn gặp phải hiệu quả huấn luyện chưa tối ưu. Cụ thể, chúng tôi tập trung vào cơ chế gating chọn các expert cho mỗi token trong công trình này. Các mô hình MoE hiện tại áp dụng top-2 gating cố định trong huấn luyện trong khi sử dụng top-1 gating trong quá trình suy luận để có thời gian phản hồi ngắn hơn. Top-2 gating đòi hỏi gấp đôi chi phí tính toán mỗi token và tăng gấp đôi kích thước truyền dữ liệu của các phép toán all-to-all so với top-1. Tuy nhiên, vẫn chưa rõ liệu top-2 gating có thực sự dẫn đến những cải thiện hiệu suất có thể biện minh cho các chi phí phụ thêm hay không. Do đó, một phân tích toàn diện về sự cân bằng giữa hiệu quả huấn luyện và hiệu suất mô hình ngày càng quan trọng. Thực tế hơn, làm thế nào để xây dựng một mô hình ngôn ngữ MoE cân bằng hiệu quả giữa hiệu quả huấn luyện và hiệu suất, có giá trị quan trọng và sắp xảy ra.

Hướng tới mục tiêu này, chúng tôi trình bày nỗ lực đầu tiên của mình để mô tả và cải thiện hiệu quả của cơ chế gating trong MoE một cách thực nghiệm. Chúng tôi quan sát thấy rằng trên các mô hình và nhiệm vụ khác nhau, một số lượng lớn các token hiển thị các đặc điểm ngôn ngữ đơn giản hoặc một tính năng chi phối duy nhất, cho phép chúng được xử lý hiệu quả chỉ bằng expert top-1.

Quan sát này cho thấy rằng chiến lược top-2 gating hiện tại phát sinh chi phí tính toán không cần thiết cho một số lượng đáng kể các token.

Được thúc đẩy bởi hiểu biết này, chúng tôi tiếp tục giới thiệu adaptive gating trong MoE cho phép các token được xử lý bởi một số lượng expert linh hoạt tùy thuộc vào quyết định gating. Cách tiếp cận của chúng tôi, trái ngược với các mô hình MoE thông thường, bảo toàn tính thưa thớt của các mô hình MoE trong khi tăng cường tính linh hoạt trong xử lý token. Chúng tôi kết hợp một ngưỡng trong mạng gating để thực hiện định tuyến token thích ứng dựa trên phân phối xác suất của expert. Với adaptive gating, phần lớn các token sử dụng top-1 gating đơn giản; top-2 gating được áp dụng có chọn lọc chỉ khi cần thiết và có lợi, do đó giảm đáng kể chi phí tính toán.

Tuy nhiên, hiệu quả huấn luyện không thể đạt được cùng mức cải thiện như chi phí tính toán do thực tế là các token với top-2 gating luôn phát sinh thời gian huấn luyện dài hơn, do đó trở thành nút thắt cổ chai. Do đó, để tăng cường hiệu quả huấn luyện hơn nữa, chúng tôi tận dụng ý tưởng của curriculum learning bằng cách điều chỉnh chiến lược thứ tự của các mẫu dữ liệu huấn luyện.

Chúng tôi thực hiện các thí nghiệm mở rộng trên sáu nhiệm vụ NLP với các mô hình encoder và decoder khác nhau. Kết quả cho thấy cách tiếp cận của chúng tôi có thể giảm hiệu quả thời gian huấn luyện end-to-end tối đa 22.5%, trong khi đạt được chất lượng suy luận tương đương với các mô hình MoE top-2 gating. Hơn nữa, chúng tôi chỉ ra rằng các token được định tuyến đến hai expert được kết hợp với bản chất của mỗi nhiệm vụ NLP. Đối với phân tích tình cảm, đó là những token thể hiện ý kiến trung tính; nhiệm vụ dịch thuật chú ý đến các câu có cấu trúc phức tạp; Hỏi và Trả lời kết nối các từ khóa trong câu hỏi và ngữ cảnh và gán cả hai với top-2 gating; tóm tắt nỗ lực nhiều hơn trong việc hiểu đại từ và tìm các token thể hiện ý tưởng trung tâm; quyết định định tuyến top-2 thay đổi cùng với token được tạo ra trong nhiệm vụ hoàn thành văn bản và các token đối thoại trong nhiệm vụ phản hồi đối thoại sử dụng expert top-2 thường xuyên. Về mặt thực nghiệm, chúng tôi thấy rằng một giá trị ngưỡng nhỏ (tức là 0.1, 0.2) trong adaptive gating có thể dẫn đến hiệu suất tương tự như top-2 gating.

Những đóng góp của chúng tôi như sau:
• Chúng tôi đề xuất adaptive gating trong sơ đồ huấn luyện MoE, cho phép các token được xử lý bởi một số lượng expert linh hoạt.
• Chúng tôi tận dụng curriculum learning để giảm bớt nút thắt cổ chai huấn luyện do thời gian thực thi khác nhau của các token.
• Chúng tôi thực hiện các thí nghiệm mở rộng trên các nhiệm vụ và bộ dữ liệu NLP khác nhau và trình bày phân tích kỹ lưỡng về quyết định gating của các token để chứng minh tính hiệu quả và hiệu suất của adaptive gating.

2 Bối cảnh

2.1 Mixture-of-Experts
Mixture-of-Experts (MoE) đã được áp dụng trong các mô hình mạng nơ-ron sâu khác nhau (Shen et al., 2023; Chen et al., 2023b) và đã cho thấy triển vọng lớn trong việc tăng cường hiệu suất của các mô hình ngôn ngữ. Ví dụ, GShard (Lepikhin et al., 2020) và Switch Transformer (Fedus et al., 2021) mở rộng hiệu quả các mô hình ngôn ngữ dựa trên Transformer với các lớp MoE.

Cụ thể, những mô hình này thường sử dụng một lớp MoE để thay thế lớp mạng feed-forward (FFN). Lớp MoE bao gồm nhiều FFN, mỗi cái hoạt động như một expert, cùng với một mạng gating. Mỗi expert i là một mạng hai lớp được kết nối đầy đủ sử dụng kích hoạt ReLU và với bộ tham số riêng của nó. Đối với một token x cho trước, đầu ra của một expert có thể được định nghĩa là:

FFN_i(x) = ReLU(x·W_i^0)·W_i^1,                    (1)

trong đó W_i^0 và W_i^1 là các trọng số có thể huấn luyện của hai lớp tuyến tính trong expert i.

Mạng gating nhận vector nhúng của mỗi token x và nhân chúng với ma trận có thể huấn luyện W_G của nó. Giá trị gate cho một token cụ thể có thể được xác định thông qua:

R = softmax(x·W_G).                                (2)

Kích hoạt softmax R này chỉ ra trọng số của mỗi expert trong việc xử lý token. Mạng gating sau đó gửi token này đến top-k expert với k kích hoạt cao nhất. Đầu ra cuối cùng của lớp MoE là:

y = Σ_(i∈E) R_i·FFN_i(x),                         (3)

đó là tổng có trọng số của các đầu ra từ (các) expert được chọn E ⊂ {FFN_1, FFN_2...FFN_N}. Bản chất thưa thớt của MoE cải thiện khả năng mở rộng mô hình về kích thước mà không tăng chi phí huấn luyện.

Công trình liên quan. Một số công trình trước đây đã khám phá việc sử dụng hiệu quả gating hoặc lựa chọn expert trong MoE. Aoki et al., 2022; Zhou et al., 2022; Hazimeh et al., 2021; Ma et al., 2018 đề xuất các cách tiếp cận khác nhau để khuyến khích chuyên môn hóa expert. Dai et al., 2022 áp dụng một phân công expert được định nghĩa trước cho mỗi danh mục đầu vào. Roller et al., 2021; Zuo et al., 2021 đề xuất loại bỏ các mạng gating. Zhou et al., 2022 trình bày một cơ chế lựa chọn mới nơi expert chọn token thay vì token chọn expert. Hazimeh et al., 2021 giới thiệu nhiều chính sách định tuyến để tăng cường chuyên môn hóa trong kịch bản đa nhiệm vụ. Roller et al., 2021 sử dụng băm xác định, trong khi Zuo et al., 2021 sử dụng định tuyến ngẫu nhiên. Tuy nhiên, nó có thể dẫn đến kết quả suy luận không nhất quán. Do đó, họ sử dụng một loss được điều chỉnh để phạt sự khác biệt của lựa chọn expert. Tất cả công trình hiện tại áp dụng một khả năng tính toán cố định và bằng nhau cho mỗi token và expert, trong khi chúng tôi xem xét sự cân bằng giữa tính toán và hiệu suất mô hình với adaptive gating.

3 Thiết kế

Bây giờ chúng tôi thảo luận về thiết kế của adaptive gating trong MoE cho huấn luyện.

3.1 Adaptive Gating trong MoE

Quan sát. Chúng tôi trước tiên trình bày các phát hiện thực nghiệm của chúng tôi từ các thí nghiệm với các mô hình MoE cổ điển. Cụ thể, chúng tôi trích xuất các kích hoạt softmax và phân tích phân phối xác suất của lựa chọn expert cho mỗi token trong mạng gating. Hình 1 mô tả các giá trị kích hoạt được chuẩn hóa của bốn token được lấy mẫu trên 16 expert. Chúng tôi thấy rằng đối với token 1 và 4, kích hoạt của expert top-1 và top-2 của chúng rất gần nhau như được hiển thị trong Hình 1a và 1d, trong khi đối với token 2 và 3, một độ lệch đáng kể về phía expert top-1 tồn tại như trong Hình 1b và 1c. Chúng tôi thấy rằng những phân phối thiên lệch đáng kể này chiếm ít nhất 55% tất cả các token trong đánh giá của chúng tôi.

Adaptive gating. Công trình trước đây đã chứng minh rằng các expert MoE chuyên về các khía cạnh ngôn ngữ khác nhau. Dựa trên các phát hiện thực nghiệm của chúng tôi, người ta có thể thấy rằng nhiều token có thể được xử lý hiệu quả bởi một expert duy nhất trong giai đoạn huấn luyện. Để kiểm soát số lượng expert xử lý mỗi token, chúng tôi giới thiệu một tham số ngưỡng, được ký hiệu là T. Nếu sự khác biệt giá trị kích hoạt giữa expert top-1, được ký hiệu là i, và expert top-2, được ký hiệu là j, nằm trong ngưỡng T, chúng tôi coi token đó là yêu cầu cả expert i và expert j để xử lý. Ngược lại, chúng tôi định tuyến token chỉ đến expert top-1.

Load balancing loss. Adaptive gating sử dụng một số lượng expert linh hoạt để xử lý mỗi token. Tuy nhiên, tính linh hoạt này thêm khó khăn bổ sung cho vấn đề cân bằng tải trong huấn luyện nhằm phân phối đều các token giữa tất cả các expert. Vì vẫn quan trọng để ngăn mạng gating quá tập trung vào một số lượng rất nhỏ expert, trong adaptive gating, chúng tôi áp đặt các ràng buộc cân bằng tải mềm trên các quyết định gating top-1, trong khi cho phép các quyết định gating top-2 được huấn luyện mà không có bất kỳ ràng buộc mềm nào. Đó là, loss của mỗi lớp MoE i trở thành:

L_i = E_i Σ_(e∈E) f_e^1 p_e,                      (4)

trong đó f_e^1 là phần các token được gửi đến expert e trong số những token được xử lý bởi gating top-1; p_e là xác suất gating trung bình đến expert e trên tất cả các token trong batch hiện tại, và E_i là số lượng expert ở lớp i giống như trong MoE cổ điển (Fedus et al., 2021).

3.2 Batching

Thách thức. Trong khi adaptive gating cung cấp tiết kiệm tính toán hiệu quả, kiến trúc mô hình Transformer MoE đặt ra một thách thức đáng kể đối với hiệu quả huấn luyện. Cụ thể, có một sự không phù hợp trong độ chi tiết xử lý dữ liệu giữa các expert MoE và lớp Attention. Các expert MoE hoạt động trên các token riêng lẻ, trong khi lớp Attention yêu cầu đầu vào dưới dạng một câu hoàn chỉnh. Do đó, mặc dù thời gian xử lý cho một phần lớn các token được giảm một nửa trong lớp MoE, chúng tôi vẫn cần phải chờ đợi cho đến khi các token còn lại (trong cùng một batch dữ liệu) hoàn thành xử lý top-2 của chúng. Do đó, thời gian bước huấn luyện không thể tận hưởng cùng mức giảm như trong tính toán.

Bảng 1 cho thấy việc giảm tính toán cũng như thời gian chạy lớp MoE thực nghiệm, cả hai đều được chuẩn hóa theo top-2 gating thông thường. Để đơn giản, ở đây chúng tôi buộc một tỷ lệ phần trăm cố định của các token được định tuyến chỉ đến expert top-1 và đo thời gian chạy. Việc giảm thời gian chạy rõ ràng nhỏ hơn nhiều so với tiết kiệm tính toán.

Curriculum learning. Trong adaptive gating, chúng tôi đề xuất kết hợp khái niệm curriculum learning để giải quyết thách thức hiệu quả huấn luyện nêu trên. Curriculum learning (Bengio et al., 2009), như tên gọi ngụ ý, là một paradigm nơi các ví dụ huấn luyện được trình bày cho một mô hình theo thứ tự tăng dần của độ phức tạp. Nó nhằm tăng cường hiệu quả học tập và hiệu suất tổng quát hóa của các mô hình. Bằng cách thiết kế cẩn thận chương trình giảng dạy, mô hình được tiếp xúc với các ví dụ dễ dàng hơn ở các giai đoạn ban đầu, cho phép nó xây dựng một nền tảng vững chắc trước khi giải quyết các khái niệm thách thức hơn. Quá trình học tập dần dần này đã cho thấy kết quả đầy hứa hẹn trong NLP (Wang et al., 2021).

Điều chỉnh thứ tự dữ liệu huấn luyện. Trực giác của chúng tôi là số lượng expert yêu cầu bởi mỗi token có thể là một chỉ số của độ phức tạp token. Do đó, chúng tôi có thể sắp xếp lại dữ liệu huấn luyện theo cách ưu tiên các chuỗi đơn giản hơn trong quá trình huấn luyện mô hình. Ngoài ra, chúng tôi có thể nhóm cùng nhau dữ liệu huấn luyện với mức độ phức tạp tương tự để giảm thiểu hiệu ứng nút thắt cổ chai do các token khó khăn cần expert top-2 gây ra.

Để định lượng độ phức tạp của một mẫu huấn luyện d, chúng tôi định nghĩa một vector độ phức tạp C:

C_d = [r_0^d, r_1^d, ...r_L^d],                    (5)

trong đó L là số lượng lớp MoE trong mô hình, và r_i đại diện cho tỷ lệ các token được xử lý bởi expert top-2 trên độ dài chuỗi (tức là tổng số token trong mẫu dữ liệu d) ở lớp i.

Để xác định thứ tự của dữ liệu huấn luyện, chúng tôi xác định mẫu dữ liệu với ít token nhất được xử lý bởi expert top-2, và tính độ tương tự cosine sử dụng vector độ phức tạp của các mẫu dữ liệu còn lại. Dữ liệu huấn luyện sau đó được sắp xếp lại dựa trên giá trị tương tự này, bắt đầu từ những cái tương tự nhất. Cách tiếp cận này cho phép mô hình dần dần học từ các chuỗi đơn giản hơn và dần dần xử lý các chuỗi phức tạp hơn.

4 Đánh giá

Chúng tôi đánh giá adaptive gating trong MoE trên sáu nhiệm vụ NLP sử dụng các mô hình encoder và decoder khác nhau. Sau đó chúng tôi phân tích quyết định gating để hiểu rõ hơn về tính hiệu quả của adaptive gating.

4.1 Nhiệm vụ và Mô hình

Bảng 2 tóm tắt các chi tiết.

4.2 Baseline

Chúng tôi sử dụng các mô hình Transformer từ HuggingFace và chuyển đổi các lớp FFN thành lớp MoE (Komatsuzaki et al., 2022). Chúng tôi so sánh hiệu quả huấn luyện của adaptive gating với ba baseline sau đây và sau đó đánh giá hiệu suất suy luận với MoE gating top-1.

Dense models. Transformer không có lớp MoE.

Top-2 gating MoE. Mô hình MoE với top-2 gating cho huấn luyện.

Top-1 gating MoE (Switch Transformer). Switch Transformer (Fedus et al., 2021) sử dụng top-1 gating để giảm bớt sự bất ổn trong huấn luyện.

4.3 Cấu hình Huấn luyện

Chúng tôi sử dụng 8 GPU A100, mỗi cái có bộ nhớ 40 GB. Dữ liệu và song song expert được sử dụng cho huấn luyện phân tán. Chúng tôi phân phối các expert đều trên tất cả các GPU. Về các siêu tham số và kiến trúc mô hình, chúng tôi áp dụng các cấu hình mặc định được thiết lập trong các mô hình hiện tại (Wolf et al., 2020; Kwon và Chung, 2023).

Kiến trúc mô hình. BERT-Base có 12 attention head mỗi lớp. Kích thước ẩn là 768 và kích thước trung gian là 3072. Mô hình Transformer có 16 attention head. Kích thước ẩn là 1024 và kích thước trung gian trong các lớp encoder và decoder lần lượt là 8192 và 4096. BART-Large có 16 attention head. Kích thước ẩn là 1024 và kích thước trung gian là 4096. GPT-2 và DialoGPT-medium có 16 attention head. Kích thước ẩn là 1024 và kích thước trung gian là 4096.

Siêu tham số. BERT-Base có kích thước batch là 24 và tỷ lệ học là 0.00003. Số lượng token tối đa cho mô hình dịch thuật là 4096 với tỷ lệ học là 0.0005. Số lượng token tối đa được phép cho BART-Large được đặt là 4096. Tỷ lệ học là 0.00001. Kích thước batch của GPT-2 là 8 với tỷ lệ học là 0.00015. Đối với DialoGPT-medium, kích thước batch và tỷ lệ học là 64 và 0.0001.

Cấu hình MoE. Kích thước tham số của FFN trong mỗi mô hình là giống nhau trong các mô hình Baseline và MoE và chúng tôi đặt số lượng FFN (tức là expert) là 16 cho tất cả các nhiệm vụ được đánh giá. Hệ số của load balancing loss là 0.01. Không có ràng buộc dung lượng được kích hoạt nên không có token nào sẽ bị bỏ. Các tham số expert được khởi tạo ngẫu nhiên. Chúng tôi chuẩn hóa xác suất expert trong adaptive gating và đặt ngưỡng T là 0.1.

4.4 Hiệu suất Tổng thể

Chúng tôi trình bày hiệu suất huấn luyện và suy luận tổng thể trong Bảng 3. Tổng thể, adaptive gating đạt hiệu suất tương đương với các baseline trong khi giảm đáng kể thời gian huấn luyện thậm chí so với top-1 gating. Điều này là do mặc dù top-1 gating tối đa hóa việc tiết kiệm tính toán, nó làm cho huấn luyện khó hội tụ hơn đến cùng giá trị loss, cuối cùng dẫn đến thời gian huấn luyện hơi dài hơn so với top-2 gating trong 4 trong 6 nhiệm vụ chúng tôi chạy. Một phân tích sâu về cách adaptive gating hoạt động liên quan đến mỗi nhiệm vụ được trình bày trong Phần 4.5.

Phân tích tình cảm. Adaptive gating trong MoE vượt trội hơn cả mô hình Dense và top-2 gating MoE trong tất cả các chỉ số. Trong khi trung bình FLOP tính toán mỗi token cao hơn với adaptive gating so với top-1 gating MoE, đại diện cho FLOP tối thiểu có thể trong cấu trúc MoE, adaptive gating yêu cầu ít thời gian huấn luyện hơn và đạt được độ chính xác vượt trội trong giai đoạn suy luận. Điều này nhất quán trên tất cả các nhiệm vụ. Đáng chú ý, chỉ có 11.3% token trong đánh giá của chúng tôi nhận được hai expert, thấp nhất trong tất cả các nhiệm vụ. So với top-2 gating, adaptive gating tập trung vào việc gán nhiều expert hơn cho các token đại diện cho ý kiến trung tính, cho phép một quá trình ra quyết định toàn diện hơn. Ngược lại, các token thể hiện ít hoặc ý kiến rõ ràng được chú ý ít hơn mà không làm giảm độ chính xác.

Dịch thuật. Adaptive gating mang lại hiệu suất tương tự với top-2 gating trong khi giảm thời gian huấn luyện và FLOP mỗi token lần lượt 25.6% và 38.2%. Đáng chú ý, chúng tôi quan sát thấy rằng mạng gating trong adaptive gating thể hiện sự tập trung đặc biệt vào độ phức tạp của cấu trúc câu. Thậm chí các token có vẻ đơn giản về mặt ngôn ngữ có thể liên quan đến hai expert khi chúng xuất hiện trong các câu có cấu trúc và ngữ pháp phức tạp. Tổng thể, 25.6% tất cả các token được huấn luyện được định tuyến đến hai expert.

Hỏi và Trả lời. Thời gian huấn luyện với adaptive gating là 85.7% của top-2 gating. Mặc dù hiệu suất suy luận của nó hơi thấp hơn, nó vẫn vượt trội hơn top-1 gating. Thông qua các thí nghiệm của chúng tôi (tham khảo Phần 4.6), chúng tôi phát hiện ra rằng adaptive gating đạt kết quả tốt nhất khi ngưỡng được đặt là 0.2 cho Hỏi và Trả lời. Quyết định gating bị ảnh hưởng bởi cả ngữ cảnh và câu hỏi cụ thể được đặt ra. Đối với nhiệm vụ này, 16.4% token nhận xử lý top-2.

Tóm tắt. Tóm tắt là nhiệm vụ thách thức nhất trong đánh giá của chúng tôi, vì nó liên quan đến việc xử lý các bài báo dài và giàu thông tin. Adaptive gating mất ít thời gian hơn 11.8% so với top-2 gating. Tuy nhiên, hiệu suất suy luận của nó hơi chậm hơn. Đặc biệt, trong adaptive gating, các token được chọn cho expert top-2 thể hiện các biến thể đáng kể trên các lớp khác nhau. Chúng tôi cung cấp một phân tích chi tiết hơn về quan sát này trong Phần 4.5.

Hoàn thành văn bản. Chúng tôi sử dụng kiến trúc chỉ decoder giống GPT cho nhiệm vụ này. Adaptive gating đạt hiệu suất tương tự như top-2 gating và mô hình Dense trong khi vượt trội hơn top-1 gating. Khi so sánh với top-2 gating, chỉ có 21.8% token dựa vào hai expert, dẫn đến giảm 23.8% trong trung bình FLOP tính toán mỗi token. Việc lựa chọn các token sử dụng hai expert thay đổi đáng kể do bản chất đa dạng của đầu vào.

Phản hồi đối thoại. Phản hồi đối thoại yêu cầu xử lý tinh tế hơn so với tạo văn bản đơn giản, vì nó liên quan đến việc tạo ra các phản hồi trong một vai trò được nhắm mục tiêu dựa trên đầu vào tường thuật và lịch sử đối thoại. Tính thưa thớt được giới thiệu bởi MoE có lợi cho nhiệm vụ này. Tất cả ba cách tiếp cận MoE đều vượt trội hơn mô hình Dense. Trong tất cả các nhiệm vụ được đánh giá, phản hồi đối thoại thể hiện tỷ lệ phần trăm cao nhất, 23.4% token được định tuyến đến hai expert, cho thấy việc sử dụng cao hơn của cơ chế gating top-2 trong tất cả các nhiệm vụ. Khi đánh giá các token, chúng tôi quan sát thấy rằng nhiệm vụ này có thể được xem như một sự kết hợp của tất cả các nhiệm vụ khác được đánh giá.

4.5 Phân tích và Hiểu biết

Trong khi trực quan để hiểu rằng một số token nhỏ (ví dụ, "a", "the", "is") chỉ cần expert top-1 để xử lý, điều này không giải thích đầy đủ cách và tại sao adaptive gating hoạt động trong các nhiệm vụ NLP khác nhau. Do đó, chúng tôi phân tích cách các token được xử lý trong huấn luyện với adaptive gating, và tạo ra khá nhiều quan sát thú vị có thể giúp trả lời tốt hơn câu hỏi này. Theo nghĩa rộng hơn, chúng tôi tin rằng hiểu biết của chúng tôi cũng hữu ích cho việc xây dựng các mô hình ngôn ngữ tốt hơn.

Phân tích tình cảm. Phân tích tình cảm thể hiện tỷ lệ phần trăm thấp nhất của gating top-2 trong tất cả các nhiệm vụ, và tỷ lệ phần trăm ổn định trên các lớp (Hình 2a). Cơ chế gating top-2 tập trung vào hai loại đầu vào chính ở đây. Đầu tiên, nó thường xuyên chọn các token thể hiện ý kiến trung tính hơn vì chúng khó phân loại hơn (Bảng 4). Thứ hai, các token liên quan đến các tuyên bố mỉa mai, phủ định kép, hoặc ý kiến mâu thuẫn cũng thường được định tuyến đến hai expert. Adaptive gating hiệu quả xác định các token này sớm trong mô hình vì chúng tương đối dễ trích xuất, giải thích tỷ lệ phần trăm ổn định trên các lớp. Một trường hợp đặc biệt là khi đầu vào không rõ ràng truyền đạt bất kỳ tình cảm nào. Adaptive gating có xu hướng ban đầu định tuyến tất cả các token đến expert top-1 hoặc top-2 và dần dần thu hẹp xuống các token thông tin hơn. Một ví dụ điển hình của điều này là "as a dentist's waiting room."

Dịch thuật. Chúng tôi kiểm tra kết quả gating top-2 dựa trên sự hiểu biết của chúng tôi về văn bản nguồn. Tỷ lệ phần trăm gating top-2 thay đổi giữa các lớp encoder và decoder, thể hiện sự giảm dần trong các lớp encoder và tăng trong các lớp decoder (Hình 2b). Từ các token được lấy mẫu và thứ tự dữ liệu được điều chỉnh, chúng tôi quan sát thấy rằng các token yêu cầu hai expert thường nằm trong cùng một câu. Điều này dẫn chúng tôi suy ra rằng độ phức tạp của cấu trúc câu ảnh hưởng đến kết quả gating. Trong Bảng 4, chúng tôi trình bày một câu chứa nhiều mệnh đề thường được xử lý bởi expert top-2.

Hỏi và Trả lời. Tỷ lệ phần trăm token top-2 trong các nhiệm vụ hỏi và trả lời dao động trên các lớp (Hình 2c). Đầu tiên, adaptive gating chú ý thêm đến chính câu hỏi. Các từ được liệt kê trong Bảng 4 là một số ví dụ phổ biến. Những token này thường chỉ định phạm vi của câu hỏi hoặc đặt ra các ràng buộc cho câu trả lời. Thứ hai, ở phía ngữ cảnh, các token được định tuyến đến hai expert có liên quan chặt chẽ đến câu hỏi trong đầu vào. Ví dụ, đặt câu hỏi về số và tính toán sẽ dẫn đến gating top-2 trên các số và các đối tượng mà những số đó đề cập đến.

Tóm tắt. Trong tóm tắt, tỷ lệ phần trăm token sử dụng hai expert giảm trong cả lớp encoder và decoder (Hình 2d). Dựa trên phân tích của chúng tôi về các token được lấy mẫu, chúng tôi xác định hai mẫu cho các token có khả năng được định tuyến đến expert top-2. Đầu tiên, các token có nhiều ý nghĩa dựa vào cả chính chúng và ngữ cảnh xung quanh để giải thích cuối cùng. Chúng thường được định tuyến đến hai expert trong các lớp nông. Thứ hai, các token đại từ, vì việc hiểu các từ mà chúng đề cập đến là rất quan trọng để tóm tắt chính xác, sử dụng hai expert trong các lớp sâu hơn. Mẫu này đặc biệt phổ biến trong nhiệm vụ này. Ngoài ra, một số token chính (ví dụ "in conclusion", "however", "in all") chỉ ra sự bắt đầu của ý tưởng trung tâm hoặc ý kiến chính của ngữ cảnh thường được gửi đến hai expert cùng với các token sau.

Hoàn thành văn bản. Hoàn thành văn bản khác với các nhiệm vụ trước đây vì nó là một nhiệm vụ chỉ decoder và tự hồi quy. Kết quả gating trong hoàn thành văn bản bị ảnh hưởng bởi dự đoán hiện tại được tạo ra. Sự tập trung của các token thay đổi một cách động dựa trên dự đoán hiện tại. Thật khó khăn để xác định các loại token cụ thể nhận được hai expert một cách nhất quán. Khi dự đoán một đại từ, ví dụ, sự tập trung chuyển sang tên của các cá nhân. Các mẫu tương tự có thể được quan sát cho số và ngày tháng. Ngoài ra, chúng tôi thấy rằng tỷ lệ phần trăm token được định tuyến đến hai expert có liên quan đến độ dài của chuỗi hiện tại. Các chuỗi dài hơn có tỷ lệ phần trăm gating top-2 cao hơn.

Phản hồi đối thoại. Phản hồi đối thoại, so với hoàn thành văn bản, yêu cầu hiểu biết nhiều hơn về đầu vào tường thuật và lịch sử đối thoại. Chúng tôi thấy rằng nhiều nỗ lực được đặt vào việc xử lý lịch sử đối thoại. Đầu tiên, một sự khác biệt chính là các token có ý nghĩa đối thoại xảy ra thường xuyên hơn nhiều. Những từ này thiếu nội dung thông tin nhưng phục vụ để thể hiện tình cảm giống con người, chẳng hạn như lòng biết ơn và lịch sự. Chúng tôi suy ra rằng việc định tuyến các token này cho hai expert cho thấy có sự khác biệt giữa việc sử dụng đối thoại và văn bản viết và cũng rất quan trọng để học nơi và khi nào những từ này nên được sử dụng. Thứ hai, do bản chất của đối thoại, nhiều cuộc trò chuyện dựa trên các giả định và điều kiện cơ bản. Các token liên quan thường được xử lý với hai token để cải thiện việc hiểu ngữ cảnh. Ví dụ, ví dụ đối thoại được cung cấp trong Bảng 4 được xây dựng dựa trên một kịch bản giả định rằng "Johnathan nói với cha mẹ mình rằng anh ấy là gay" và yêu cầu mô hình trả lời các câu hỏi với điều kiện này.

4.6 Nghiên cứu Ablation

Ngưỡng T trong adaptive gating. Bây giờ chúng tôi thực hiện một nghiên cứu ablation về ngưỡng T được giới thiệu trong adaptive gating. Tăng giá trị ngưỡng dẫn đến một mô hình ít thưa thớt hơn, nơi nhiều token hơn được gán cho cơ chế gating top-2, sau đó tăng FLOP tính toán. Bảng 5 cho thấy hiệu suất suy luận của các nhiệm vụ khác nhau khi ngưỡng tăng từ 0.05 đến 0.5. Khi sử dụng ngưỡng nhỏ là 0.05, cả thời gian huấn luyện và hiệu suất suy luận đều giống với top-1 gating MoE. Mặt khác, đặt ngưỡng là 0.4 không phải lúc nào cũng dẫn đến hiệu suất giống như top-2 gating. Cùng với Bảng 3, chúng tôi phát hiện ra rằng các giá trị ngưỡng 0.1 và 0.2 thường tạo ra sự cân bằng thuận lợi giữa thời gian huấn luyện và hiệu suất suy luận.

Curriculum learning. Về cơ bản, chúng tôi tắt điều chỉnh thứ tự dữ liệu trước mỗi epoch và sử dụng trình tải dữ liệu ngẫu nhiên để cung cấp tập huấn luyện. Chúng tôi trình bày sự suy giảm hiệu suất so với adaptive gating đầy đủ trong Bảng 6. Vì rất có thể có ít nhất một token được định tuyến đến expert top-2, thời gian bước của mỗi lần lặp không thể đạt được cùng mức giảm như FLOP tính toán. Do đó, thời gian huấn luyện end-to-end bị tăng lên đáng kể, với mức tăng trung bình 13.7%. Ngoài ra, ý tưởng của chương trình giảng dạy cũng góp phần cải thiện hiệu suất suy luận. Mức giảm tối đa là 0.21 trong Hỏi và Trả lời khi dữ liệu được cung cấp và huấn luyện một cách ngẫu nhiên.

5 Kết luận

Bài báo này chứng minh tính hiệu quả và linh hoạt của adaptive gating trong các mô hình MoE cho một loạt rộng các nhiệm vụ xử lý ngôn ngữ tự nhiên. Bằng cách điều chỉnh động số lượng expert dựa trên đặc điểm token, chúng tôi đạt được hiệu quả huấn luyện được cải thiện mà không ảnh hưởng đến hiệu suất suy luận. Ngoài ra, việc tích hợp curriculum learning cho phép chúng tôi giải quyết thách thức của thời gian thực thi khác nhau, do đó giảm chi phí huấn luyện. Nghiên cứu của chúng tôi làm sáng tỏ sự cân bằng giữa hiệu quả huấn luyện và hiệu suất mô hình trong các mạng MoE thưa thớt và động, cung cấp những hiểu biết có giá trị cho việc phát triển các mô hình ngôn ngữ có thể mở rộng và thích ứng hơn.

Hạn chế

Adaptive gating trong MoE hiện tại bị giới hạn ở top-k gating, trong đó k có thể là 1 hoặc 2. Điều này được xây dựng dựa trên thực hành phổ biến trong công trình trước đây mở rộng rằng top-2 gating cho thấy kết quả đầy hứa hẹn trong MoE. Đánh giá thêm là cần thiết để xác thực hiệu suất của một loạt rộng hơn các giá trị k. Các thí nghiệm của chúng tôi được thực hiện trên một tập hợp đa dạng các nhiệm vụ và bộ dữ liệu NLP, nhưng điều cần thiết là lưu ý rằng tính hiệu quả và hiệu suất của adaptive MoE có thể thay đổi tùy thuộc vào các đặc điểm nhiệm vụ cụ thể. Các nhiệm vụ khác nhau có thể thể hiện các mẫu và độ phức tạp khác biệt, có thể tác động đến hiệu suất và khả năng tổng quát hóa của cách tiếp cận được đề xuất. Điều tra và đánh giá thêm trên một loạt rộng hơn các nhiệm vụ sẽ cung cấp hiểu biết toàn diện hơn về các hạn chế và khả năng áp dụng của adaptive MoE.

Tuyên bố Đạo đức

Không có vấn đề đạo đức trong công trình này.

Tài liệu tham khảo

[Phần tài liệu tham khảo được giữ nguyên như trong bản gốc]
