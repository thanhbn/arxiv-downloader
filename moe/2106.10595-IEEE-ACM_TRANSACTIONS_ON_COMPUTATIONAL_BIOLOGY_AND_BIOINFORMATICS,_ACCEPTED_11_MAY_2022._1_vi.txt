# 2106.10595.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2106.10595.pdf
# Kích thước tệp: 8335493 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 1
Học đa nhiệm dị thể với sự đa dạng chuyên gia
Raquel Aoki, Frederick Tung, và Gabriel L. Oliveira,
Tóm tắt —Dự đoán nhiều mục tiêu sinh học và y học dị thể là một thách thức đối với các mô hình học sâu truyền thống. Trái ngược với học đơn nhiệm, trong đó một mô hình riêng biệt được huấn luyện cho mỗi mục tiêu, học đa nhiệm (MTL) tối ưu hóa một mô hình duy nhất để dự đoán nhiều mục tiêu liên quan cùng lúc. Để giải quyết thách thức này, chúng tôi đề xuất Hỗn hợp chuyên gia đa cổng với tính độc quyền (MMoEEx). Công trình của chúng tôi nhằm giải quyết bài toán MTL dị thể, trong đó cùng một mô hình tối ưu hóa nhiều nhiệm vụ với các đặc điểm khác nhau. Kịch bản như vậy có thể làm quá tải các phương pháp MTL hiện tại do những thách thức trong việc cân bằng biểu diễn chia sẻ và đặc thù nhiệm vụ và nhu cầu tối ưu hóa các nhiệm vụ với các đường dẫn tối ưu cạnh tranh. Phương pháp của chúng tôi có hai đóng góp chính: thứ nhất, chúng tôi giới thiệu một cách tiếp cận để tạo ra sự đa dạng hơn giữa các chuyên gia, từ đó tạo ra các biểu diễn phù hợp hơn cho việc học MTL dị thể và mất cân bằng cao; thứ hai, chúng tôi áp dụng phương pháp tối ưu hóa hai bước [1, 2] để cân bằng các nhiệm vụ ở cấp độ gradient. Chúng tôi xác thực phương pháp của mình trên ba bộ dữ liệu chuẩn MTL, bao gồm bộ dữ liệu UCI-Census-income, Kho thông tin y tế cho chăm sóc tích cực (MIMIC-III) và PubChem BioAssay (PCBA).
Từ khóa chỉ mục —học đa nhiệm, mạng nơ-ron, hỗn hợp chuyên gia, cân bằng nhiệm vụ
F
1 GIỚI THIỆU
CÁC mô hình học đơn nhiệm (STL) là cách tiếp cận truyền thống nhất trong máy học và đã cực kỳ thành công trong nhiều ứng dụng. Cách tiếp cận này giả định rằng mô hình được yêu cầu đưa ra một mục tiêu dự đoán duy nhất cho một mẫu đầu vào cho trước, chẳng hạn như một nhãn lớp hoặc một giá trị hồi quy. Nếu hai mục tiêu đầu ra được liên kết với cùng một dữ liệu đầu vào, thì hai mô hình độc lập được huấn luyện: một cho mỗi mục tiêu hoặc nhiệm vụ (Xem Hình 1.a).
STL có thể phù hợp cho các tình huống trong đó các nhiệm vụ rất khác nhau và hiệu quả tính toán có thể được bỏ qua. Tuy nhiên, khi các nhiệm vụ có liên quan, các mô hình STL không hiệu quả về tham số [3, 4]. Ngoài ra, trong một số ứng dụng, sự tương tác giữa các nhiệm vụ có thể giúp một mô hình được huấn luyện chung nắm bắt tốt hơn các mẫu chia sẻ mà nếu không sẽ bị bỏ sót bởi việc huấn luyện độc lập. Ví dụ, trong thị giác máy tính, sự tương tác giữa các nhiệm vụ dự đoán dày đặc của phân đoạn ngữ nghĩa (việc gán nhãn lớp ngữ nghĩa cho mỗi pixel trong một hình ảnh) và ước tính độ sâu (dự đoán độ sâu thực tế tại mỗi pixel trong một hình ảnh) có thể được tận dụng để huấn luyện một mạng nơ-ron duy nhất đạt độ chính xác cao hơn trên cả hai nhiệm vụ so với các mạng được huấn luyện độc lập [5].
Trái ngược với STL, học đa nhiệm (MTL) tối ưu hóa một mô hình duy nhất để thực hiện nhiều nhiệm vụ liên quan cùng lúc, nhằm cải thiện khả năng khái quát hóa và hiệu quả tham số qua các nhiệm vụ. Trong trường hợp này, hai hoặc nhiều mục tiêu đầu ra được liên kết với cùng một dữ liệu đầu vào (Xem Hình 1.b, 1.c và 1.d). Học đa nhiệm hiệu quả thường đòi hỏi cân bằng nhiệm vụ để ngăn một hoặc nhiều nhiệm vụ chi phối việc tối ưu hóa, giảm chuyển giao tiêu cực, và tránh overfitting. Các thiết lập MTL tiêu chuẩn thường giả định một tập hợp các nhiệm vụ đồng nhất, ví dụ tất cả các nhiệm vụ đều là phân loại hoặc hồi quy, và thường chúng là dữ liệu không tuần tự. Kịch bản này có thể được hưởng lợi rất nhiều từ các phương pháp MTL với biểu diễn chia sẻ mạnh. Trái lại, học đa nhiệm dị thể được định nghĩa bởi nhiều lớp nhiệm vụ, chẳng hạn như phân loại, hồi quy với đặc điểm nhãn đơn hoặc đa nhãn và dữ liệu thời gian, được tối ưu hóa đồng thời. Thiết lập sau này thực tế hơn nhưng thiếu khám phá sâu hơn. Ma et al. [6] đã đề xuất Multi-gate Mixture-of-Experts (MMoE), một mô hình kết hợp các chuyên gia bằng cách sử dụng các hàm cổng. Trong trường hợp này, mỗi chuyên gia là một hoặc nhiều lớp mạng nơ-ron được chia sẻ giữa các nhiệm vụ.
MMoE có xu hướng khái quát hóa tốt hơn các mô hình khác vì nó tận dụng nhiều đáy chia sẻ (chuyên gia) thay vì sử dụng một kiến trúc duy nhất. Nó cho phép phân bổ tham số động cho các phần chia sẻ và đặc thù nhiệm vụ của mạng, từ đó cải thiện thêm khả năng biểu diễn. Để tận dụng những đặc điểm này và mở rộng nó cho các vấn đề MTL dị thể, chúng tôi giới thiệu MMoEEx.
Mô hình hỗn hợp chuyên gia đa cổng với tính độc quyền (MMoEEx) là một phương pháp hỗn hợp chuyên gia (MMoE) mới cho MTL giúp tăng hiệu suất khái quát hóa của MMoE truyền thống thông qua hai đóng góp chính:
Các chuyên gia trong MMoE truyền thống là đồng nhất, điều này hạn chế sự đa dạng của các biểu diễn được học. Lấy cảm hứng từ học tập tổng hợp, chúng tôi cải thiện khả năng khái quát hóa của MMoE truyền thống bằng cách tạo ra sự đa dạng giữa các chuyên gia. Chúng tôi giới thiệu các điều kiện loại trừ và độc quyền mới, trong đó một số chuyên gia chỉ đóng góp cho một số nhiệm vụ, trong khi các chuyên gia khác được chia sẻ giữa tất cả các nhiệm vụ.
Chúng tôi giới thiệu tối ưu hóa cân bằng nhiệm vụ hai bước ở cấp độ gradient được lấy cảm hứng từ MAML[1]. Điều này cho phép MMoEEx hỗ trợ việc học các nhiệm vụ dị thể không cân bằng, trong đó một số nhiệm vụ có thể dễ bị overfitting hơn, khó học hơn, hoặc hoạt động ở các thang đo mất mát khác nhau.
Chúng tôi xác thực phương pháp đề xuất của mình bằng một tập hợp thí nghiệm rộng lớn. Đầu tiên, chúng tôi khám phá cách MMoEEx hoạt động trên bộ dữ liệu UCI Census-income [6, 7], một bộ dữ liệu chuẩn MTL cho các nhiệm vụ có tính cardinality thấp. Chúng tôi so sánh nó với một số mô hình đa nhiệm hiện đại và cho thấy rằng kỹ thuật của chúng tôi vượt trội hơn các phương pháp khác. Sau đó, chúng tôi đánh giá hiệu suất của MMoEEx trên hai bộ dữ liệu chuẩn y học và sinh học. Kho thông tin y tế cho chăm sóc tích cực (MIMIC-III) [8, 9] là một bộ dữ liệu học đa nhiệm chuỗi thời gian dị thể. Sự kết hợp của các nhiệm vụ thời gian đa nhãn và đơn nhãn với phân loại nhị phân không thời gian làm cho bộ dữ liệu này lý tưởng để đánh giá phương pháp của chúng tôi. Đặc điểm quy mô lớn và mất cân bằng nhiệm vụ cao của bộ dữ liệu cũng cung cấp kịch bản để khai thác tính mạnh mẽ của phương pháp chúng tôi đối với các nhiệm vụ cạnh tranh.
Chúng tôi quan sát thấy những cải thiện đáng kể trong các thang đo AUC so với tất cả các phương pháp so sánh, đặc biệt là kỹ thuật MMoE [6]. Cuối cùng, chúng tôi xác thực MMoEEx trên bộ dữ liệu PubChem BioAssay (PCBA) [10, 11]. PCBA là một bộ dữ liệu không thời gian đồng nhất (chỉ phân loại nhị phân) có tính cardinality nhiệm vụ cao. Trong khi các nhiệm vụ của nó ít thách thức hơn MIMIC-III, nó chứa hơn một trăm nhiệm vụ. Do đó, chúng tôi áp dụng bộ dữ liệu này để đánh giá khả năng mở rộng và các khía cạnh chuyển giao tiêu cực của các phương pháp MTL. Sau khi khám phá tất cả các thiết lập này, kết quả của chúng tôi đã xác nhận hiệu quả của MMoEEx và cho thấy rằng phương pháp của chúng tôi có hiệu suất ngang bằng với hiện đại hiện tại.

--- TRANG 2 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 2
2 CÔNG TRÌNH LIÊN QUAN
Các công trình gần đây trong học sâu cho học đa nhiệm (MTL) có thể được chia thành hai nhóm: những công trình tập trung vào kiến trúc mạng nơ-ron, nghiên cứu gì, khi nào và cách chia sẻ thông tin giữa các nhiệm vụ; và những công trình tập trung vào tối ưu hóa, thường tập trung vào cách cân bằng các nhiệm vụ cạnh tranh, được học chung. Công trình của chúng tôi đóng góp cho cả hai lĩnh vực.
Kiến trúc MTL có thể được chia thành hai nhóm chính, chia sẻ tham số cứng và chia sẻ tham số mềm. Một trong những công trình đầu tiên trong MTL sử dụng chia sẻ tham số cứng. Trong loại kiến trúc này được đề xuất bởi Caruana [12], các lớp dưới của mạng nơ-ron được chia sẻ giữa tất cả các nhiệm vụ, và các lớp trên là đặc thù nhiệm vụ. Một mặt, lợi thế chính của lớp phương pháp này là tính bất biến quy mô đối với số lượng lớn các nhiệm vụ. Mặt khác, với biểu diễn chia sẻ, các đặc trưng kết quả có thể trở nên thiên vị đối với các nhiệm vụ có tín hiệu mạnh.
Nhóm thứ hai của các tôpô MTL có một tập hợp tham số riêng biệt cho mỗi nhiệm vụ. Những phương pháp như vậy được gọi là chia sẻ tham số mềm [13]. Chúng có thể được hiểu là các mạng đơn có cơ chế chia sẻ đặc trưng giữa các bộ mã hóa để tạo ra sự giao thoa thông tin giữa các nhánh. Các phương pháp như Cross-Stitch Network [14] và Multi-gate Mixture of Experts [6] là các ví dụ của chia sẻ tham số mềm dựa trên cơ chế chia sẻ đặc trưng rõ ràng, hỗn hợp chuyên gia hợp nhất đặc trưng và các phương pháp dựa trên attention để giao thoa giữa các nhánh. Misra et al. [14] đã giới thiệu khái niệm chia sẻ tham số mềm trong các phương pháp đa nhiệm sâu bằng cách học một tổ hợp tuyến tính của các bản đồ kích hoạt đầu vào. Tổ hợp tuyến tính (hợp nhất đặc trưng mềm) được học tại mỗi lớp từ cả hai nhiệm vụ. Phương pháp MMoE được đề xuất bởi Ma et al. [6] là một nỗ lực cung cấp cơ chế chia sẻ tham số mềm thông qua cơ chế cổng. Hàm cổng chọn một tập hợp chuyên gia cho mỗi nhiệm vụ trong khi tái sử dụng nó cho nhiều nhiệm vụ, do đó cung cấp chia sẻ đặc trưng. Lợi thế chính của các phương pháp chia sẻ tham số mềm là khả năng học các biểu diễn đặc thù nhiệm vụ và chia sẻ một cách rõ ràng. Tuy nhiên, những mô hình này gặp vấn đề về khả năng mở rộng, vì kích thước của mạng MTL có xu hướng tăng tỉ lệ với số lượng nhiệm vụ. Một khảo sát đầy đủ về các tôpô MTL khác nhau có thể được tìm thấy tại [15, 16].
Những công trình được đề cập trước đó tập trung vào cấu trúc mạng tốt hơn cho MTL. Một vấn đề quan trọng khác của việc học nhiều nhiệm vụ liên quan đến thủ tục tối ưu hóa. Các phương pháp MTL cần cân bằng gradient của nhiều nhiệm vụ để ngăn một hoặc nhiều nhiệm vụ chi phối mạng và tạo ra các dự đoán thiên vị nhiệm vụ. Các phương pháp tối ưu hóa có thể được chia thành các kỹ thuật cân bằng mất mát [11, 17], chuẩn hóa gradient [18] và học meta bất khả tri mô hình [1, 2]. Liu et al. [11] đề xuất một phương pháp cân bằng mất mát dựa trên tỷ lệ mất mát giữa batch đầu tiên và tất cả các batch tiếp theo trong mỗi epoch. Phương pháp được gọi là Loss-Balanced Task Weighting (LBTW) đã cho thấy kết quả hứa hẹn trong việc giảm chuyển giao tiêu cực trong kịch bản 128 nhiệm vụ. Các phương pháp cân bằng mất mát có những nhược điểm chính là tính không tối ưu khi gradient của các nhiệm vụ xung đột hoặc khi một tập hợp nhiệm vụ có độ lớn gradient cao hơn các nhiệm vụ khác. Để giảm thiểu những hạn chế này của các phương pháp dựa trên mất mát, GradNorm [18] và Model-Agnostic Meta-Learning (MAML) cho MTL [1, 2] đã được đề xuất.
Chuẩn hóa gradient được đề xuất bởi Chen et al. [18] nhằm kiểm soát việc huấn luyện thông qua một cơ chế khuyến khích tất cả các nhiệm vụ có độ lớn tương tự. Ngoài ra, mô hình cũng cân bằng tốc độ học các nhiệm vụ. Gần đây hơn, các phương pháp dựa trên meta-learning [1] đã xuất hiện và vượt trội hơn các phương pháp dựa trên mất mát trước đó và các kỹ thuật chuẩn hóa gradient [2]. Lee et al. [2] đề xuất một phương pháp đa bước cập nhật mỗi nhiệm vụ theo cách độc quyền. Phương pháp này không chỉ có thể cung cấp tối ưu hóa nhiệm vụ cân bằng mà còn thúc đẩy các kiến trúc MTL hiện tại.
Các phương pháp meta-learning MTL, mặc dù là lớp phương pháp hiện đại hiện tại, có thể trở nên không thực tế cho các thiết lập với cardinality lớn dựa trên các bước trung gian cần thiết cho tính toán trạng thái nhiệm vụ.
Phương pháp hỗn hợp chuyên gia đa cổng với tính độc quyền (MMoEEx) đề xuất có điểm tương đồng với phương pháp MMoE [6] và MAML [1]. Kỹ thuật của chúng tôi là một phương pháp MMoE, tuy nhiên chúng tôi giới thiệu cơ chế độc quyền cung cấp kích hoạt thưa thớt rõ ràng của mạng, cho phép phương pháp học các đặc trưng đặc thù nhiệm vụ và biểu diễn chia sẻ đồng thời. Chúng tôi cũng giải quyết hạn chế về khả năng mở rộng của các kỹ thuật MMoE với các cổng loại trừ của chúng tôi. Phương pháp của chúng tôi được lấy cảm hứng từ kỹ thuật MAML trong đó chúng tôi nhằm giới thiệu một phương pháp hai bước để cân bằng các nhiệm vụ ở cấp độ gradient cho hỗn hợp chuyên gia.

--- TRANG 3 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 3
Hình 1. Kiến trúc mạng nơ-ron.(a) Học đơn nhiệm, (b) Học đa nhiệm chia sẻ tham số cứng [12], (c) Hỗn hợp chuyên gia đa cổng (MMoE)[6], (d) MMoEEx – phương pháp được đề xuất.
3 PHƯƠNG PHÁP LUẬN
Mạng chia sẻ tham số cứng [12] được hiển thị trong Hình 1.b là một trong những trụ cột của học đa nhiệm. Những mạng này được cấu thành từ một đáy chia sẻ và các nhánh đặc thù nhiệm vụ.
Ma et al. [6] đề xuất rằng một đáy chia sẻ duy nhất có thể không đủ để khái quát cho tất cả các nhiệm vụ trong một ứng dụng, và đề xuất sử dụng nhiều đáy chia sẻ, hoặc những gì họ gọi là chuyên gia. Các chuyên gia được kết hợp bằng cách sử dụng các hàm cổng, và sự kết hợp của chúng được chuyển tiếp đến các tòa tháp. Kiến trúc cuối cùng được gọi là Multi-gate Mixture-of-Experts (MMoE), và được hiển thị trong Hình 1.c. MMoE khái quát hóa tốt hơn so với đối tác chia sẻ tham số cứng truyền thống của nó, nhưng có hai điểm yếu: thứ nhất, nó thiếu cơ chế cân bằng nhiệm vụ; thứ hai, nguồn đa dạng duy nhất giữa các chuyên gia là do khởi tạo ngẫu nhiên. Mặc dù các chuyên gia thực sự có thể đủ đa dạng nếu họ chuyên biệt trong các nhiệm vụ khác nhau, nhưng không có gì đảm bảo rằng điều này sẽ xảy ra trong thực tế. Trong công trình này, chúng tôi đề xuất Multi-gate Mixture-of-Experts với tính độc quyền (MMoEEx) (Hình 1.d), một mô hình tạo ra sự đa dạng hơn giữa các chuyên gia và có thành phần cân bằng nhiệm vụ.
3.1 Cấu trúc
Kiến trúc mạng nơ-ron tuân theo cấu trúc được đề xuất bởi Ma et al. [6] và có thể được chia thành ba phần: cổng, chuyên gia và tòa tháp. Xét một ứng dụng với K nhiệm vụ, dữ liệu đầu vào x∈Rd, hàm cổng gk() được định nghĩa là:
gk(x) = softmax(Wkx); ∀k ∈ {0, ..., K} (1)
trong đó Wk ∈ RE×d là trọng số có thể học và E là số lượng chuyên gia, được người dùng định nghĩa. Các cổng kiểm soát sự đóng góp của mỗi chuyên gia cho mỗi nhiệm vụ.
Các chuyên gia fe(); ∀e ∈ {0, ..., E}, và cài đặt của chúng tôi rất linh hoạt để chấp nhận nhiều kiến trúc chuyên gia, điều này rất cần thiết để làm việc với các ứng dụng có các loại dữ liệu khác nhau. Ví dụ, nếu làm việc với dữ liệu thời gian, các chuyên gia có thể là LSTM, GRU, RNN; đối với dữ liệu không thời gian, các chuyên gia có thể là các lớp dày đặc. Số lượng chuyên gia E được người dùng định nghĩa. Đầu ra của các chuyên gia và cổng được kết hợp như sau:
fk(x) = ∑E e=0 gk(x)fe(x); ∀k ∈ {0, ..., K} (2)
Các fk() là đầu vào cho các tòa tháp, phần đặc thù nhiệm vụ của kiến trúc. Thiết kế của chúng phụ thuộc vào loại dữ liệu và nhiệm vụ. Các tòa tháp hk đưa ra các dự đoán nhiệm vụ như sau:
yk = hk(fk(x)); ∀k ∈ {0, ..., K} (3)
3.2 Đa dạng
Trong học tập tổng hợp, các mô hình có sự đa dạng đáng kể giữa các người học của chúng có xu hướng khái quát hóa tốt hơn. MMoE [6] tận dụng nhiều chuyên gia để đưa ra dự đoán cuối cùng; tuy nhiên, nó chỉ dựa vào khởi tạo ngẫu nhiên để tạo ra sự đa dạng giữa các chuyên gia, và vào kỳ vọng rằng hàm cổng sẽ học cách kết hợp những chuyên gia này. Ở đây chúng tôi đề xuất hai cơ chế để tạo ra sự đa dạng giữa các chuyên gia, được định nghĩa là loại trừ và độc quyền:
Độc quyền: Chúng tôi đặt E chuyên gia được kết nối độc quyền với một nhiệm vụ. Giá trị α ∈ [0, 1] kiểm soát tỷ lệ các chuyên gia sẽ là độc quyền. Nếu α = 1, tất cả chuyên gia đều độc quyền, và nếu α = 0, tất cả chuyên gia đều được chia sẻ (giống như MMoE). Một chuyên gia độc quyền được gán ngẫu nhiên cho một trong các nhiệm vụ Tk, nhưng nhiệm vụ Tk vẫn có thể được liên kết với các chuyên gia độc quyền khác và các chuyên gia chia sẻ.
Loại trừ: Chúng tôi ngẫu nhiên loại trừ các cạnh/kết nối giữa E chuyên gia và các nhiệm vụ. Nếu β = 1, tất cả chuyên gia sẽ có một kết nối được loại bỏ ngẫu nhiên, và nếu β = 0, không có xóa cạnh (giống như MMoE).
Đối với các ứng dụng chỉ có hai nhiệm vụ (K = 2), cơ chế loại trừ và độc quyền là giống nhau. Cơ chế loại trừ có khả năng mở rộng hơn cơ chế độc quyền vì nó không yêu cầu một chuyên gia cho mỗi nhiệm vụ, và do đó, hoạt động tốt trong các ứng dụng có số lượng lớn nhiệm vụ. Đối với một tập hợp nhỏ các nhiệm vụ, cả hai phương pháp đều có kết quả tương tự. MMoEEx, tương tự như MMoE, dựa vào kỳ vọng rằng các hàm cổng sẽ học cách kết hợp các chuyên gia. Phương pháp của chúng tôi tạo ra sự đa dạng hơn bằng cách buộc một số cổng này phải 'đóng' đối với một số chuyên gia, và các cơ chế độc quyền và loại trừ được sử dụng để đóng một phần các cổng. Các cổng không đóng còn lại học cách kết hợp đầu ra của mỗi chuyên gia dựa trên dữ liệu đầu vào, theo Phương trình 1.
Như Phương trình 2 cho thấy, các cổng được sử dụng như trọng số chuyên gia. Do đó, nếu một chuyên gia e ∈ {0, ..., E} độc quyền với một nhiệm vụ k ∈ {0, ..., K}, thì chỉ giá trị gk[e] ≠ 0, và tất cả các cổng khác cho chuyên gia đó đều 'đóng': gm[e] = 0; m ∈ {0, ..., K}; m ≠ k.
3.3 Tối ưu hóa MAML - MTL
Mục tiêu của tối ưu hóa hai bước là cân bằng các nhiệm vụ ở cấp độ gradient. Finn et al. [1] đề xuất Model-agnostic Meta-learning (MAML), một phương pháp tối ưu hóa hai bước ban đầu dự định được sử dụng với học chuyển giao và học vài mẫu do hội tụ nhanh của nó. MAML cũng có tương lai hứa hẹn trong MTL. Lee và Son [2] đầu tiên điều chỉnh MAML cho các ứng dụng học đa nhiệm, cho thấy rằng MAML có thể cân bằng các nhiệm vụ ở cấp độ gradient và mang lại kết quả tốt hơn so với một số phương pháp cân bằng nhiệm vụ hiện có. Ý tưởng cốt lõi là cập nhật tạm thời của MAML tạo ra các mất mát được làm mượt, điều này cũng làm mượt các gradient về hướng và độ lớn.
Trong công trình của chúng tôi, chúng tôi áp dụng MAML. Tuy nhiên, khác với Lee và Son [2], chúng tôi không đóng băng các lớp đặc thù nhiệm vụ trong cập nhật trung gian/bên trong. Mã giả của phương pháp MAML-MTL của chúng tôi được hiển thị trong Thuật toán 1.
Thuật toán 1: MAML-MTL
Lấy mẫu batch X;
loss = 0;
for T trong TASKS do
    Đánh giá LT(f(X));
    Cập nhật tạm thời θ'T ← θ - α∇LT(f(X));
    Tái đánh giá và lưu
    loss = loss + L'T(f_θ'T(X));
Cập nhật θ ← θ - β∇loss;
Hình 2 hiển thị một minh họa của Thuật toán 1 khi chúng ta có hai nhiệm vụ (A và B). Các hộp trắng biểu thị các khối chia sẻ của mạng nơ-ron; các khối vàng và đỏ đặc thù cho nhiệm vụ A và nhiệm vụ B, tương ứng. Bước đầu tiên của MAML là tính toán mất mát hiện tại cho mỗi nhiệm vụ. Một phương pháp tối ưu hóa tiêu chuẩn thông thường sẽ tổng những mất mát này và lan truyền ngược. Tuy nhiên, đây là những mất mát chúng ta muốn làm mượt để cải thiện cân bằng nhiệm vụ. Do đó, chúng tôi thực hiện cập nhật tạm thời trong toàn bộ mạng nơ-ron cho mỗi nhiệm vụ, như được hiển thị trong bước 2. Sau đó, trong bước 3, chúng tôi tái đánh giá các nhiệm vụ sử dụng mạng nơ-ron được cập nhật tạm thời. Những mất mát mới này được làm mượt (về độ lớn và hướng) và được sử dụng trong cập nhật cuối cùng (bước 4).
Phần 4 hiển thị kết quả của các thí nghiệm sử dụng chiến lược tối ưu hóa hai bước của chúng tôi. Một trong những nhược điểm chính của phương pháp này là khả năng mở rộng. Các cập nhật tạm thời đắt đỏ, làm cho việc sử dụng MAML trong các ứng dụng với nhiều nhiệm vụ trở nên không khả thi.

--- TRANG 4 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 4
Hình 2. Minh họa của MAML-MTL (Thuật toán 1) với hai nhiệm vụ. Các khối trắng biểu thị các lớp không đặc thù nhiệm vụ; màu vàng và đỏ biểu thị các khối đặc thù nhiệm vụ. Bước 2 và 3 hiển thị các mô hình được cập nhật tạm thời (một cho mỗi nhiệm vụ). Bước 4 cập nhật tất cả các mô hình với tổng của hai mất mát được tính toán lại.

--- TRANG 5 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 5
4 THÍ NGHIỆM
Trong phần này, chúng tôi phát triển các thí nghiệm để trả lời hai câu hỏi chính để xác thực phương pháp đề xuất của chúng tôi:
1) Phương pháp đề xuất MMoEEx của chúng tôi có kết quả tốt hơn các baseline MTL hiện có, chẳng hạn như MMoE, chia sẻ tham số cứng (shared bottom), Multi-Channel Wise LSTMs (bộ dữ liệu chuỗi thời gian);
2) Phương pháp đề xuất MMoEEx của chúng tôi có kết quả tốt hơn các phương pháp học đơn nhiệm (STL).
Hơn nữa, chúng tôi cũng khám phá một số kết quả phụ, chẳng hạn như ảnh hưởng của độ phức tạp chuyên gia và số lượng chuyên gia đến kết quả, và so sánh sự đa dạng của chuyên gia trong phương pháp đề xuất của chúng tôi và baseline chính của chúng tôi.
4.1 Bộ dữ liệu
Chúng tôi đánh giá hiệu suất của phương pháp của chúng tôi trên ba bộ dữ liệu. Bộ dữ liệu UCI-Census-income [6, 7], cơ sở dữ liệu Kho thông tin y tế cho chăm sóc tích cực (MIMIC-III) [8, 9] và bộ dữ liệu PubChem BioAssay (PCBA) [10]. Một đặc điểm chung giữa tất cả các bộ dữ liệu là sự hiện diện của các nhiệm vụ rất không cân bằng (ít ví dụ tích cực).
Bộ dữ liệu UCI - Census-income[6, 7]
Được trích xuất từ cơ sở dữ liệu điều tra dân số Mỹ năm 1994, có 299,285 câu trả lời và 40 đặc trưng, được trích xuất từ mẫu đơn kinh tế xã hội của người trả lời. Ba nhiệm vụ phân loại nhị phân được khám phá sử dụng bộ dữ liệu này:
1) Thu nhập của người trả lời vượt quá $50K;
2) Tình trạng hôn nhân của người trả lời là "đã từng kết hôn";
3) Học vấn của người trả lời ít nhất là đại học;
Cơ sở dữ liệu Kho thông tin y tế cho chăm sóc tích cực (MIMIC-III) [8, 9]
Cơ sở dữ liệu này được đề xuất bởi Harutyunyan et al. [9] làm bộ dữ liệu chuẩn cho MTL trong dữ liệu chuỗi thời gian. Nó chứa các chỉ số của bệnh nhân từ hơn 40,000 lần lưu trú tại các đơn vị chăm sóc tích cực (ICU). Bộ dữ liệu này có 4 nhiệm vụ: hai nhiệm vụ nhị phân, một nhiệm vụ đa nhãn thời gian, và một phân loại thời gian. Hình 3 hiển thị mạng nơ-ron được áp dụng trong công trình của chúng tôi và nơi mỗi nhiệm vụ được tính toán.
Chúng tôi tuân theo tiền xử lý dữ liệu được giới thiệu bởi Harutyunyan et al. [9], có sẵn tại https://github.com/YerevaNN/mimic3-benchmarks. Để thuận tiện cho việc đánh giá, chúng tôi giữ cùng các nhiệm vụ và cấu trúc nhiệm vụ. Mô tả nhiệm vụ:
1) Dự đoán kiểu hình: đo lường vào cuối thời gian lưu trú, phân loại xem bệnh nhân có 25 tình trạng chăm sóc cấp tính (pT trong Hình 3). Trong thực tế, chúng tôi có 25 nhiệm vụ phân loại nhị phân song song;
2) Dự đoán thời gian lưu trú (LOS): mục tiêu là dự đoán thời gian còn lại trong ICU tại mỗi giờ lưu trú (lt trong Hình 3). Theo Harutyunyan et al. [9], thời gian còn lại được chuyển đổi từ nhiệm vụ hồi quy thành nhiệm vụ đa nhãn. Chúng tôi có 10 lớp, một lớp cho mỗi một trong 8 ngày đầu tiên, giữa 8-15 ngày, và +15 ngày;
3) Dự đoán suy giảm: nhằm dự đoán xem trạng thái của bệnh nhân có bị suy giảm nhanh chóng trong 24 giờ tới hay không. Chúng tôi tuân theo sơ đồ Harutyunyan et al. [9], trong đó do thiếu tiêu chuẩn vàng, nhiệm vụ được định nghĩa lại như dự đoán tử vong trong 24 giờ tới tại mỗi giờ lưu trú ICU, trong thực tế, một phân loại nhị phân thời gian (dt trong Hình 3);
4) Dự đoán tử vong trong viện: phân loại nhị phân vào cuối 48 giờ đầu tiên của bệnh nhân trong lần lưu trú ICU (m48 trong Hình 3);
Cơ sở dữ liệu PubChem BioAssay (PCBA)[10, 11]
Chúng tôi làm việc với một tập con của PCBA, bao gồm 128 nhiệm vụ/mục tiêu sinh học nhị phân và 439,863 mẫu. Mỗi mẫu biểu thị một phân tử, được tiền xử lý sử dụng trích xuất đặc trưng phân tử Circular Fingerprint [11], tạo ra 1024 đặc trưng. Những đặc trưng này được sử dụng để xác định xem hóa chất có ảnh hưởng đến mục tiêu sinh học hay không, ở đây được định nghĩa là các nhiệm vụ của chúng tôi.
4.2 Thiết kế thí nghiệm
Việc chia tách giữa tập huấn luyện, xác thực và kiểm tra giống như được sử dụng bởi các baseline của chúng tôi để đưa ra so sánh công bằng. Đối với UCI-Census, tỷ lệ chia là 66%/17%/17% cho tập huấn luyện/xác thực/kiểm tra, đối với MIMIC-III và PCBA là 70%/15%/15%. Tiền xử lý dữ liệu, tiêu chí mất mát, bộ tối ưu hóa, tham số, và mô tả các thang đo của thí nghiệm của chúng tôi được hiển thị trong Phần 4.3. Thang đo được áp dụng để so sánh kết quả là AUC (Diện tích dưới đường cong) ROC cho các nhiệm vụ nhị phân và Kappa Score cho các nhiệm vụ đa lớp.
4.3 Tính tái tạo thí nghiệm
Chúng tôi đã sử dụng PyTorch trong cài đặt của mình, và mã sẽ được cung cấp tại https://github.com/BorealisAI/MMoEEx-MTL. Chúng tôi đã sử dụng bộ tối ưu hóa Adam với tỷ lệ học 0.001, suy giảm trọng số 0.001, và tỷ lệ học giảm theo hệ số 0.9 mỗi mười epoch. Thang đo được áp dụng để so sánh các mô hình là ROC AUC, với ngoại lệ của nhiệm vụ LOS trên bộ dữ liệu MIMIC-III, là Cohen's kappa Score, một thống kê đo lường sự đồng ý giữa các giá trị quan sát và dự đoán. Chúng tôi đã huấn luyện các mô hình sử dụng tập huấn luyện, và chúng tôi đã sử dụng tổng AUC của nhiệm vụ trong tập xác thực để xác định mô hình tốt nhất, trong đó tổng lớn nhất chỉ ra epoch tốt nhất, và do đó, mô hình tốt nhất. Bảng 1 hiển thị tóm tắt các mô hình được áp dụng để tham khảo trong tương lai.

--- TRANG 6 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 6
BẢNG 1
Kiến trúc mô hình, thông tin huấn luyện, và tham khảo tiền xử lý dữ liệu của bộ dữ liệu cho tính tái tạo thí nghiệm.
Bộ dữ liệu Tiền xử lý Epochs Chuyên gia Mất mát Lớp
UCI-Census Ma et al. [6] 200 12 BCEWithLogitsLoss Chuyên gia: Linear (4) + ReLU, Tòa tháp: Linear (4) + Linear (1)
MIMIC-III Johnson et al. [8], Harutyunyan et al. [9] 50 12 và 32 BCEWithLogitsLoss, CrossEntropyLoss (nhiệm vụ đa nhãn), posweight: pheno = 5, LOS = 1, Decomp = 25, Ihm = 5 Chuyên gia: RNN(128) hoặc GRU(128); Tòa tháp: Linear (16) + Linear (output), trong đó output phụ thuộc vào nhiệm vụ. Ba tòa tháp có dữ liệu chuỗi thời gian, và một chỉ có 24 quan sát đầu tiên của chuỗi thời gian.
PCBA Liu et al. [11] 100 2 hoặc 4 BCEWithLogitsLoss, posweight = 100 Linear (2000) + Dropout (0.25) + ReLU + Linear (2000) + Sigmoid + Linear (2000). Tòa tháp có một lớp Linear (1) cho mỗi nhiệm vụ.

4.4 Nghiên cứu UCI - Census-income
Trong phần này, chúng tôi báo cáo và thảo luận kết quả thí nghiệm trên dữ liệu census-income. Chúng tôi trình bày các thí nghiệm dự đoán thu nhập, tình trạng hôn nhân, và mức độ giáo dục.
Các thí nghiệm bộ dữ liệu thu nhập điều tra dân số, tình trạng hôn nhân và giáo dục được trình bày tại Bảng 2. Chúng tôi có thể quan sát rằng phương pháp của chúng tôi vượt trội hơn tất cả các baseline với ngoại lệ của nhiệm vụ Giáo dục, trong đó phương pháp đơn nhiệm thể hiện cải thiện nhỏ hơn MMoEEx. Chúng tôi lập luận rằng các nhiệm vụ Census đã có mục tiêu tối ưu hóa hơi xung đột và trong tình huống này, phương pháp của chúng tôi phù hợp hơn để cân bằng nhiều nhiệm vụ cạnh tranh.
BẢNG 2
Kết quả trên bộ dữ liệu Thu nhập điều tra dân số/Hôn nhân/Giáo dục. Δ là cải thiện tương đối trung bình. MMoEEx Exclusivity = 0.5.
Phương pháp AUC Thu nhập Tình trạng hôn nhân Giáo dục Δ
Single-Task 88.95 97.48 87.23 -
Shared-Bottom 91.09 97.98 86.99 +0.85%
MMoE 90.86 96.70 86.33 -0.28%
MMoEEx (Của chúng tôi) 92.51 98.47 87.19 +1.65%
Chúng tôi có thể kết luận rằng phương pháp MMoEEx của chúng tôi có thể học tốt hơn nhiều nhiệm vụ khi so sánh với các phương pháp shared bottom tiêu chuẩn và baseline MMoE, do các đóng góp độc quyền và tối ưu hóa đa bước trong công trình của chúng tôi.
4.5 Nghiên cứu MIMIC-III
Bộ dữ liệu MIMIC-III là chuẩn chính cho MTL dị thể với chuỗi thời gian. Bộ dữ liệu bao gồm hỗn hợp các nhiệm vụ thời gian đa nhãn và đơn nhãn và hai nhiệm vụ phân loại nhị phân không thời gian. Các thí nghiệm của chúng tôi sẽ đầu tiên điều tra các lớp hồi quy tốt nhất để được chọn làm chuyên gia cho mô hình MMoEEx. Các phần sau đây hiển thị nghiên cứu ablation về tác động của cardinality chuyên gia cao hơn và đánh giá baseline quy mô đầy đủ của chúng tôi.
4.5.1 Nghiên cứu Ablation các mô-đun hồi quy
Một trong những lựa chọn thiết kế chính cho dự đoán chuỗi thời gian là loại đơn vị hồi quy được triển khai. Mục tiêu của nghiên cứu ablation này là cung cấp phân tích toàn diện về tác động của các lớp hồi quy khác nhau đến phương pháp của chúng tôi. Các lớp được xem xét bao gồm từ RNN tiêu chuẩn [19], LSTM [20] và GRU [21] đến các lớp hồi quy hiện đại như Simple Recurrent Units (SRU) [22] và Independent Recurrent Networks (IndRNN) [23].
BẢNG 3
Kết quả trên nghiên cứu ablation các mô-đun hồi quy MIMIC-III. Tất cả các cấu hình MMoEEx đều tính với 12 chuyên gia dựa trên hạn chế bộ nhớ của các phương pháp như IndRNN và LSTM. MMoEEx Exclusivity = 0.5.
Phương pháp Pheno LOS Decomp Ihm
MMoEEx - SRU [22] 71.00 57.88 96.67 89.95
MMoEEx - IndRNN [23] 67.49 57.11 95.89 91.68
MMoEEx - IndRNNV2 [23] 68.15 54.48 96.50 90.58
MMoEEx - LSTM [20] 73.48 45.99 96.54 90.88
MMoEEx - RNN [19] 73.40 55.56 96.85 91.08
MMoEEx - GRU [21] 74.08 54.48 97.20 91.49
Nghiên cứu ablation các mô-đun hồi quy MIMIC-III được trình bày trong Bảng 3. SRU và IndRNN vượt trội hơn các phương pháp khác từ nhiệm vụ thời gian lưu trú (LOS). Bên cạnh đó MMoEEx với IndRNN cũng là người thực hiện hàng đầu cho nhiệm vụ tử vong trong viện (Ihm). Bên cạnh hiệu suất tốt của SRU và IndRNN cho các nhiệm vụ này, chúng thể hiện hiệu suất mất cân bằng trên tất cả các nhiệm vụ được xem xét và cũng áp đặt gánh nặng về bộ nhớ và thời gian chạy, làm cho khả năng mở rộng của MMoEEx đến số lượng chuyên gia cao hơn không khả thi. Xem xét hiệu suất tổng thể nhiệm vụ, RNN và GRU vượt trội hơn các phương pháp hồi quy so sánh. RNN, ngoài việc là chuyên gia thực hiện hàng đầu, cũng thể hiện dung lượng bộ nhớ thấp nhất và do đó có khả năng cung cấp cho MMoEEx nhiều chuyên gia hơn nếu cần. Từ phần này trở đi, chúng tôi sẽ có MMoEEx với RNN hoặc GRU làm lớp hồi quy của chúng.
4.5.2 Tác động của cardinality chuyên gia
Trong quá trình huấn luyện phương pháp của chúng tôi cho các thí nghiệm MIMIC-III, chúng tôi nhận thấy rằng số lượng chuyên gia lớn hơn, khi kết nối với cơ chế độc quyền của chúng tôi, đã cho kết quả tổng thể tốt hơn. Để khám phá thêm tham số này, chúng tôi đã thực hiện một loạt thí nghiệm mà chúng tôi huấn luyện MMoEEx của chúng tôi với RNN với số lượng chuyên gia từ 12 đến 64 chuyên gia. Chúng tôi chọn RNN làm lớp hồi quy trong thí nghiệm này dựa trên yêu cầu bộ nhớ thấp của nó.
Hình 4 mô tả kết quả của chúng tôi cho bốn nhiệm vụ trên bộ dữ liệu MIMIC-III. Nhiệm vụ LOS là nhiệm vụ tận dụng nhất từ số lượng chuyên gia lớn hơn với cải thiện vượt quá 17 điểm phần trăm hoặc cải thiện tương đối 38 phần trăm. Các nhiệm vụ còn lại ổn định cho cardinality chuyên gia cao hơn. Chúng tôi tin rằng số lượng chuyên gia cao hơn cho phép MMoEEx có biểu diễn tốt hơn cho các nhiệm vụ thách thức khi biểu diễn chia sẻ không được cập nhật với cùng độ lớn do các nhiệm vụ khác đã đạt được sự ổn định. Số lượng 32 chuyên gia đã cho MMoEEx hiệu suất tổng thể và LOS tốt nhất. Kết quả cuối cùng trên MIMIC-III đều sử dụng 32 chuyên gia.
Hình 4. Tác động của cardinality chuyên gia đối với mô hình MMoEEx trên MIMIC-III.
BẢNG 4
Kết quả cuối cùng MIMIC-III. MMoEEx vượt trội hơn tất cả các baseline so sánh với ngoại lệ đối với Pheno nơi công trình của Johnson et al. [8] là phương pháp tốt nhất. Phương pháp của chúng tôi có thể cung cấp cải thiện tương đối vượt quá 40 điểm phần trăm khi so sánh với Multitask channel wise LSTM cho nhiệm vụ LOS. MMoEEx Exclusivity = 0.5.
Phương pháp Pheno LOS Decomp Ihm Δ
MCW-LSTM [8] 77.4 45.0 90.5 87.0 +0.28%
Single Task [8] 77.0 45.0 91.0 86.0 -
Shared Bottom 73.36 30.60 94.12 82.71 -9.28%
MMoE 75.09 54.48 96.20 90.44 +7.36%
MMoEEx - RNN 72.44 63.45 96.82 90.73 +11.74%
MMoEEx - GRU 74.57 60.63 97.03 91.03 +11.00%
4.5.3 Kết quả MIMIC-III
Tập hợp kết quả đầy đủ cho bộ dữ liệu MIMIC-III được trình bày trong Bảng 4. Chúng tôi so sánh phương pháp của chúng tôi với multitask channel wise LSTM (MCW-LSTM) [8], mạng được huấn luyện đơn nhiệm, shared bottom, MMoE [6] và hai biến thể của MMoEEx với RNN và GRU.
MMoEEx vượt trội hơn tất cả các phương pháp so sánh trừ nhiệm vụ Phenotype (Pheno). Đối với cả hai nhiệm vụ chuỗi thời gian (LOS và Decomp), phương pháp của chúng tôi vượt trội hơn tất cả các baseline. Đáng chú ý là đối với nhiệm vụ LOS, đây là nhiệm vụ khó nhất trên MIMIC-III, chúng tôi thể hiện cải thiện tương đối vượt quá 40 điểm phần trăm khi so sánh với multitask channel wise LSTM [8] và hơn 16 điểm phần trăm so với MMoE cho MMoEEx với RNN của chúng tôi. MMoEEx với GRU thể hiện hiệu suất nhiệm vụ cá nhân tốt hơn so với đối tác RNN nhưng với hiệu suất nhiệm vụ LOS thấp hơn.
4.6 Nghiên cứu bộ dữ liệu PubChem BioAssay
BẢNG 5
Kết quả cuối cùng của PCBA. Phương pháp của chúng tôi có kết quả cạnh tranh khi so sánh với các baseline. NT là Chuyển giao tiêu cực, Δ là Cải thiện tương đối trung bình.
Phương pháp AUC trung bình CI(95%) NT Δ
STL[11] 79.9 [78.0, 81.7] - -
MTL[11] 85.7 [84.2, 87.2] 13 +8.5%
Fine Tuning [11] 80.6 [78.7, 82.4] 50 +0.8%
GradNorm[11] 84.0 [82.5, 85.3] 44 +5.1%
RMTL[11] 85.2 [83.7, 86.7] 11 +6.6%
LBTW(λ = 0.1)[11] 85.9 [84.4, 87.3] 13 +7.5%
LBTW(λ = 0.5)[11] 86.3 [84.8, 87.6] 11 +8.0%
Shared Bottom 86.8 [84.6, 87.5] 10 +8.6%
MMoE 85.8 [84.1, 87.1] 15 +7.3%
MMoEEx (của chúng tôi) 85.9 [84.1, 87.1] 13 +7.5%
Bộ dữ liệu PCBA có 128 nhiệm vụ và là chuẩn chính cho khả năng mở rộng và chuyển giao tiêu cực. Tất cả 128 nhiệm vụ đều là các nhiệm vụ phân loại nhị phân, và chúng rất giống nhau. Các thí nghiệm của chúng tôi đầu tiên so sánh phương pháp của chúng tôi với các baseline hiện có trên AUC trung bình của các nhiệm vụ và số lượng nhiệm vụ có chuyển giao tiêu cực. Sau đó, chúng tôi có nghiên cứu ablation thứ hai so sánh phương pháp MMoEEx của chúng tôi với MMoE về số lượng chuyên gia và đánh giá overfitting.
4.6.1 So sánh với các baseline hiện có
Chúng tôi đã sử dụng công trình của Liu et al. [11] làm baseline chính, do sự đa dạng của kỹ thuật được thử nghiệm, chẳng hạn như GradNorm [18], và LBTW [11]. Ngoài ra, chúng tôi đã bao gồm một kỹ thuật shared bottom và MMoE vào các baseline của chúng tôi.
Kiến trúc được áp dụng cho các baseline và chuyên gia rất giống nhau (chi tiết hơn xem Phần 4.3). Đối với ứng dụng này, chúng tôi không sử dụng tối ưu hóa MAML-MTL do các vấn đề về khả năng mở rộng. Do đó, sự khác biệt giữa MMoE và MMoEEx trong ứng dụng này là sự đa dạng của các chuyên gia: tất cả chuyên gia của MMoE được chia sẻ giữa tất cả các nhiệm vụ, so với chỉ một phần của MMoEEx được chia sẻ. Bảng 5 hiển thị kết quả cuối cùng. Chúng tôi áp dụng bốn thang đo để so sánh kết quả của chúng tôi với các baseline: ROC AUC trung bình của tất cả các nhiệm vụ, Độ lệch chuẩn của ROC AUC, Δ, và số lượng chuyển giao tiêu cực (NT). NT được tính toán sử dụng các mô hình học đơn nhiệm, và đếm có bao nhiêu nhiệm vụ có kết quả tệ hơn trong phương pháp học đa nhiệm. Hình 5 hiển thị sự cải thiện của mỗi mô hình so sánh với mô hình STL, trong đó các nhiệm vụ dưới 0 chỉ ra NT.
Xem xét tất cả các baseline, shared bottom được khớp trong nghiên cứu của chúng tôi có kết quả tổng thể tốt nhất (AUC trung bình lớn nhất, NT nhỏ hơn). Sử dụng AUC của các nhiệm vụ, chúng tôi đã xây dựng khoảng tin cậy 95%, được hiển thị trong Bảng 5, từ đó chúng tôi kết luận rằng không có sự khác biệt đáng kể giữa RMTL, MTL, LBTW, Shared Bottom, MMoE, và MMoEEx. Do đó, phương pháp MMoEEx đề xuất của chúng tôi có kết quả cạnh tranh khi so sánh với các baseline khác. Chúng tôi nhấn mạnh rằng LBTW và GradNorm đều tập trung vào cân bằng nhiệm vụ. Tuy nhiên, bộ dữ liệu PCBA có các nhiệm vụ rất giống nhau, gần như làm cho thành phần cân bằng nhiệm vụ không cần thiết. Mô hình shared bottom, ví dụ, không có bất kỳ phương pháp cân bằng nhiệm vụ nào và có hiệu suất tổng thể tốt nhất.
Hình 5. Biểu đồ hiển thị thay đổi AUC cho mỗi nhiệm vụ k ∈ {1, ..., 128} so với AUC học đơn nhiệm (STL). Giá trị dưới 0 chỉ ra chuyển giao tiêu cực. MMoEEx Exclusivity = 0.5.
4.6.2 Tác động của số lượng chuyên gia
Hình 6. So sánh ROC AUC so với số lượng chuyên gia. Chúng tôi thay đổi tham số exclusivity MMoEEx với mức tăng 0.09, bắt đầu với α = 0.42, vì vậy luôn chỉ có hai chuyên gia được chia sẻ.
Chúng tôi cũng thực hiện so sánh trực tiếp giữa phương pháp MMoEEx của chúng tôi và baseline chính MMoE. Trong bộ dữ liệu này, cố định số lượng chuyên gia, phương pháp MMoEEx đề xuất của chúng tôi có ROC AUC trung bình tốt hơn trên tập kiểm tra so với MMoE, như Hình 6 hiển thị. Trong nghiên cứu này, chúng tôi cố định số lượng chuyên gia chia sẻ trong MMoEEx là 2. Với ba chuyên gia, chúng tôi áp dụng α = 0.42, và với mỗi chuyên gia mới được thêm vào, chúng tôi tăng giá trị α lên 0.09. Do đó, với tám chuyên gia, chúng tôi có hai chuyên gia chia sẻ và α = 0.87. Biểu đồ cho thấy rằng việc bao gồm sự đa dạng hơn trên các chuyên gia thông qua độc quyền chuyên gia đã giúp mô hình khái quát hóa tốt hơn trên tập kiểm tra và giảm overfitting.
4.7 Nghiên cứu điểm số đa dạng
Chúng tôi đề xuất một phép đo đa dạng để hỗ trợ tuyên bố của chúng tôi rằng phương pháp MMoEEx của chúng tôi tạo ra sự đa dạng hơn giữa các chuyên gia so với baseline MMoE của chúng tôi. Sự đa dạng giữa các chuyên gia có thể được đo thông qua khoảng cách giữa đầu ra của các chuyên gia fe; ∀e ∈ {0, ..., E}. Xét một cặp chuyên gia i và j, khoảng cách giữa chúng được định nghĩa là:
di,j = √(∑N n=0 (fi(xn) - fj(xn))²) (4)
trong đó N là số lượng mẫu trong bộ dữ liệu, di,j = dj,i, và một ma trận D ∈ RE×E được sử dụng để giữ tất cả các khoảng cách.
Để chia tỷ lệ các khoảng cách thành di,j ∈ [0, 1], chúng tôi chia các mục thô trong ma trận khoảng cách D cho khoảng cách tối đa quan sát được, max(D). Một cặp chuyên gia i, j với di,j = 0 được coi là giống hệt nhau, và các chuyên gia có khoảng cách di,j gần 0 được coi là rất giống nhau; tương tự, các chuyên gia có di,j gần 1 được coi là rất khác nhau. Để so sánh khoảng cách tổng thể giữa các chuyên gia của một mô hình, chúng tôi định nghĩa điểm số đa dạng d là mục trung bình trong D.
Trong phần này, chúng tôi phân tích điểm số đa dạng của MMoE và MMoEEx trong các bộ dữ liệu chuẩn của chúng tôi. Các mô hình MMoE và MMoEEx được so sánh sử dụng cùng bộ dữ liệu có cùng cấu trúc mạng nơ-ron, nhưng MMoEEx sử dụng tối ưu hóa MAML - MTL và có sự đa dạng được thúc đẩy. Các mô hình MMoEEx trong Hình 7 và 8 được tạo với α = 0.5 và độc quyền. Nói cách khác, một nửa số chuyên gia trong các mô hình MMoEEx được gán ngẫu nhiên là độc quyền cho một trong các nhiệm vụ, trong khi kết quả MMoE có α = 0 (tất cả chuyên gia được chia sẻ giữa tất cả các nhiệm vụ).
Hình 7 hiển thị bản đồ nhiệt của các khoảng cách DMMoE và DMMoEEx được tính toán trên tập kiểm tra MIMIC-III với 12 chuyên gia. Bản đồ nhiệt của MMoE có, tổng thể, màu sắc nhạt hơn và điểm số đa dạng nhỏ hơn so với MMoEEx. Hình 8 hiển thị bản đồ nhiệt MMoE và MMoEEx cho bộ dữ liệu PCBA, với 128 nhiệm vụ và 4 chuyên gia. Phương pháp MMoEEx đề xuất của chúng tôi cũng có điểm số đa dạng d lớn hơn và màu sắc đậm hơn.
Tóm lại, phương pháp của chúng tôi hoạt động cực kỳ tốt trên bộ dữ liệu dị thể, MIMIC-III, tăng điểm số đa dạng lên 43.0%. PCBA là một bộ dữ liệu đồng nhất, nhưng thành phần đa dạng vẫn tác động tích cực và tăng điểm số đa dạng lên 7.7%. Cuối cùng, là bộ dữ liệu đồng nhất và đơn giản nhất được áp dụng trong nghiên cứu của chúng tôi, bộ dữ liệu Census là bộ dữ liệu duy nhất không tận dụng đầy đủ sự đa dạng của chuyên gia. Điểm số đa dạng của MMoE là 0.410 so với 0.433 cho mô hình MMoEEx, chỉ là cải thiện 5.6%.
Những kết quả này cho thấy rằng phương pháp của chúng tôi thực sự đã tăng sự đa dạng của chuyên gia trong khi giữ AUC nhiệm vụ tương tự hoặc tốt hơn (Xem Bảng 2, 5 và 4), đây là một trong những mục tiêu chính của chúng tôi.
4.8 Ý nghĩa sinh học
Một phương pháp học MTL có thể cải thiện khả năng khái quát hóa khi các nhiệm vụ có liên quan từ góc độ ML. Từ quan điểm sinh học, điều này có nghĩa là các nhiệm vụ liên quan được hưởng lợi từ việc được mô hình hóa cùng nhau, ngay cả khi chúng là những nhiệm vụ rất khác biệt. Lý do là việc mô hình hóa nhiều nhiệm vụ trong một mô hình duy nhất có thể giúp mạng nơ-ron loại bỏ các mối tương quan giả mạo, làm nổi bật các mẫu quan trọng trong dữ liệu. Để minh họa trực giác này, hãy xem xét cơ sở dữ liệu PCBA và các kết quả của nó được hiển thị trong Bảng 5. Mục tiêu của chúng tôi là dự đoán xem một phân tử cho trước có phản ứng với một hợp chất hóa học hay không. Các phân tử được tiền xử lý sử dụng phương pháp vân tay tròn, tạo ra biểu diễn của cấu trúc phân tử sử dụng các vùng lân cận nguyên tử. Biểu diễn cuối cùng là một mảng phẳng với 1024 đặc trưng cho mỗi phân tử. Sử dụng phương pháp STL, chúng tôi sẽ có một mô hình cho mỗi hợp chất hóa học; đối với mỗi mô hình, chúng tôi sẽ chia cơ sở dữ liệu thành tập huấn luyện/kiểm tra và huấn luyện mô hình. Lý tưởng, mô hình sẽ học nhận diện biểu diễn của các đặc trưng đầu vào, có ý nghĩa cho việc dự đoán phản ứng quan sát được. Điều đó sẽ được lặp lại cho từng một trong 128 hợp chất hóa học, có nghĩa là kiến thức thu được bằng cách dự đoán một hợp chất theo phương pháp STL không bao giờ được tái sử dụng hoặc cải thiện theo thời gian. Mỗi lần, mô hình STL sẽ học mọi thứ từ đầu. Nếu một hợp chất cụ thể không cân bằng trên tập huấn luyện hoặc khó học hơn vì nó có hành vi phức tạp hơn, mạng nơ-ron có thể không nắm bắt và học biểu diễn có ý nghĩa một cách đầy đủ hoặc tạo trọng số quá mức cho các mối tương quan giả mạo. Mặt khác, phương pháp học MTL sẽ cố gắng học biểu diễn có ý nghĩa của tất cả các hợp chất hóa học, và các lớp đặc thù nhiệm vụ sau đó sẽ học biểu diễn cụ thể cho một hợp chất cho trước. Điều đó có nghĩa là phương pháp MTL có nhiều khả năng học biểu diễn tốt hơn của dữ liệu vì có nhiều thông tin hơn (từ nhiều nhiệm vụ/mất mát) để giúp loại trừ các mối tương quan giả mạo và làm nổi bật các mẫu quan trọng. Tuy nhiên, các lớp đặc thù nhiệm vụ cho phép mạng nơ-ron phân biệt các nhiệm vụ, tạo không gian cho biểu diễn cụ thể của hợp chất. Trực giác đó được xác nhận bởi các thí nghiệm của chúng tôi, như Bảng 5 cho thấy: tất cả các phương pháp học MTL đều có hiệu suất tốt hơn so với các mô hình STL.

--- TRANG 7 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 7

--- TRANG 8 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 8

--- TRANG 9 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 9
Hình 7. Bản đồ nhiệt MMoE (d = 0.311) và MMoEEx (d = 0.445) trong bộ dữ liệu MIMIC-III. MMoE có 12 chuyên gia chia sẻ so với 6 chuyên gia chia sẻ và 6 chuyên gia độc quyền trong mô hình MMoEEx. Màu sắc đậm hơn cho thấy sự bất đồng hơn giữa hai chuyên gia và do đó, sự đa dạng hơn. Biểu đồ được tạo ra với 12 thay vì 32 chuyên gia để trực quan hóa tốt hơn các khoảng cách; kết quả cũng đúng trong thiết lập với 32 chuyên gia.
Hình 8. Bản đồ nhiệt MMoE (d = 0.557) và MMoEEx (d = 0.600) cho bộ dữ liệu PCBA. Mô hình MMoEEx có 2 chuyên gia chia sẻ và 2 chuyên gia có loại trừ.

--- TRANG 10 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 10
5 KẾT LUẬN
Chúng tôi đã đề xuất một phương pháp học đa nhiệm mới gọi là Hỗn hợp chuyên gia đa cổng với tính độc quyền (MMoEEx), mở rộng các phương pháp MMoE bằng cách giới thiệu cơ chế độc quyền và loại trừ tạo ra sự đa dạng hơn giữa các chuyên gia, cho phép mạng học các biểu diễn hiệu quả hơn cho MTL dị thể. Chúng tôi cũng giới thiệu một phương pháp tối ưu hóa hai bước gọi là MAML-MTL, cân bằng các nhiệm vụ ở cấp độ gradient và tăng cường khả năng của MMoEEx trong việc tối ưu hóa các nhiệm vụ không cân bằng. Chúng tôi cho thấy rằng phương pháp của chúng tôi có kết quả tốt hơn các baseline trong các thiết lập MTL với các nhiệm vụ dị thể, thường được tìm thấy trong các ứng dụng sinh học. Các thí nghiệm trên các bộ dữ liệu chuẩn sinh học và lâm sàng chứng minh sự thành công của phương pháp đề xuất của chúng tôi trong các thiết lập đồng nhất và dị thể, nơi chúng tôi vượt trội hơn một số baseline hiện đại.
Bên cạnh những đóng góp ở trên, chúng tôi tin rằng phân tích ablation trên MMoEEx có thể mở ra cơ hội khám phá thêm về học đa nhiệm dị thể. Cải thiện đáng kể trên nhiệm vụ khó nhất của bộ dữ liệu MIMIC-III là dấu hiệu của hướng điều tra có thể này. Tối ưu hóa MAML-MTL cũng đang trong giai đoạn sơ khai, và nghiên cứu thêm về cân bằng nhiệm vụ meta-learning có thể mang lại lợi ích lớn cho nghiên cứu MTL. Chúng tôi hy vọng công trình này sẽ truyền cảm hứng cho các nhà nghiên cứu khác điều tra thêm về học đa nhiệm ở cấp độ kiến trúc mạng và tối ưu hóa.

--- TRANG 11 ---
IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, ĐƯỢC CHẤP NHẬN 11 THÁNG 5 NĂM 2022. 11
Raquel Aoki nhận bằng cử nhân Thống kê và thạc sĩ Khoa học máy tính từ Đại học Liên bang Minas Gerais năm 2014 và 2017, tương ứng. Cô hiện đang theo đuổi bằng tiến sĩ Khoa học máy tính tại Đại học Simon Fraser, Canada. Sở thích nghiên cứu của cô bao gồm nhân quả trong sinh học tính toán và học đa nhiệm. Công trình này được phát triển trong thời gian thực tập tại Borealis AI.
Frederick Tung nhận bằng tiến sĩ khoa học máy tính từ Đại học British Columbia năm 2017 và hiện là trưởng nhóm nghiên cứu tại Borealis AI. Sở thích nghiên cứu của ông trong thị giác máy tính và học sâu tiết kiệm tài nguyên.
Gabriel L. Oliveira nhận bằng cử nhân Kỹ thuật máy tính từ Đại học Liên bang Rio Grande năm 2009, thạc sĩ Khoa học máy tính từ Đại học Liên bang Minas Gerais và Đại học Minnesota năm 2012 và 2014, tương ứng. Ông nhận bằng tiến sĩ Khoa học máy tính từ Đại học Freiburg năm 2019. Cùng năm đó, ông gia nhập Borealis AI với tư cách là nhà nghiên cứu máy học. Sở thích nghiên cứu của ông bao gồm học đa nhiệm, học phân phối đuôi dài và mạng nơ-ron động.
