# 2310.10837.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2310.10837.pdf
# File size: 866857 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Approximating Two-Layer Feedforward Networks
for Efficient Transformers
Róbert Csordás1Kazuki Irie2†Jürgen Schmidhuber1,3
1The Swiss AI Lab IDSIA, USI & SUPSI2Harvard University3AI Initiative, KAUST
{robert,juergen}@idsia.ch ,kirie@fas.harvard.edu
Abstract
How to reduce compute and memory require-
ments of neural networks (NNs) without sac-
rificing performance? Many recent works
use sparse Mixtures of Experts (MoEs) to
build resource-efficient large language mod-
els (LMs). Here we introduce several novel
perspectives on MoEs, presenting a general
framework that unifies various methods to ap-
proximate two-layer NNs (e.g., feedforward
blocks of Transformers), including product-
key memories (PKMs). Leveraging insights
from this framework, we propose methods
to improve both MoEs and PKMs. Unlike
prior work that compares MoEs with dense
baselines under the compute-equal condition,
our evaluation condition is parameter-equal ,
which is crucial to properly evaluate LMs. We
show that our MoEs are competitive with the
dense Transformer-XL on both the WikiText-
103 and enwiki8 datasets at two different scales,
while being much more resource-efficient. This
demonstrates that MoEs are relevant not only
to extremely large LMs but also to any-scale
resource-efficient LMs. Our code is public.1
1 Introduction
Despite impressive results recently achieved by
large language models (LLMs; Radford et al.
(2019); Brown et al. (2020); Rae et al. (2021)),
vast resource requirement remains their obvious
limitation. In fact, most existing LLMs, such as
GPT-3 (Brown et al., 2020), cannot be trained, fine-
tuned or even evaluated without access to enormous
compute. Many recent works strive to develop
LLMs that, at least, enable inference with limited
resources (e.g., on consumer hardware), e.g., by
building “smaller” yet capable LMs (Touvron et al.,
2023; Taori et al., 2023; Chiang et al., 2023) or de-
veloping post-training quantization methods (Zafrir
et al., 2019; Dettmers et al., 2022). While these
†Work done at IDSIA.
1https://github.com/robertcsordas/moemethods are gaining popularity, a principled solu-
tion for resource-efficient neural networks (NNs)
remains elusive.
One promising approach explored by several re-
cent works on extremely-large LMs is the sparse
mixture of experts (MoE; Shazeer et al. (2017);
Lewis et al. (2021); Lepikhin et al. (2021); Fedus
et al. (2022); Clark et al. (2022); Chi et al. (2022)).
Unlike their dense counterparts, MoEs only com-
pute a subset of their activations (i.e, only a few
experts ) at each step, offering reduced computation
and memory costs. However, MoEs are not yet
generally adopted as a generic/to-go approach, per-
haps because of certain common beliefs on MoEs:
(1) They are hard to train (involving complex en-
gineering tricks to prevent collapsing), (2) they
are not competitive against their dense counter-
parts with the same number of parameters (in fact,
prior work focuses on FLOP-equal comparison,
“unfairly” comparing MoEs against dense baselines
with many fewer trainable parameters), and finally,
(3) they are reserved for extremely large models
(they are rarely/never considered to further improve
the efficiency of “small” models). Indeed, even
prior works on MoE-based Transformer LMs only
deploy MoEs in a few feedforward blocks; while
ideally, allsuch blocks should benefit from replace-
ment by MoEs. Here we challenge these common
beliefs, and propose novel perspectives on MoEs.
We present MoEs within a unified framework
of methods that approximate two-layer feed-
forward networks, which includes product-key
memories (PKMs; Lample et al. (2019)) and
top-ksparsification. This principled view not
only allows us to conceptually group and compare
MoEs with PKMs, it also provides insights on
design choices for improving these methods. Our
resulting MoE Transformer variant outperforms
our improved PKMs, and performs as well as
or even outperforms the dense baseline, while
using a fraction of its compute for both trainingarXiv:2310.10837v3  [cs.LG]  21 Nov 2023

--- PAGE 2 ---
and inference. Importantly, unlike prior work,
we compare our MoEs with dense baselines with
the same number of total trainable parameters,
which is crucial for proper evaluation in language
modeling. We conduct experiments on the standard
WikiText-103 (at two different model scales) and
Enwik8 datasets. We demonstrate that MoEs are
not limited to extremely-large LMs, but useful as a
generic approach for resource-efficient NNs at any
scale, and in line with the recent trend of improving
“smaller” models (Touvron et al., 2023; Taori et al.,
2023; Chiang et al., 2023). Finally, we release a
CUDA kernel for our MoE layers which allows for
achieving faster wall clock time and large memory
reduction compared to the dense model.2
2 Background
Transformers (Vaswani et al., 2017) have two
main building blocks: the self-attention layer
(Parikh et al., 2016; Cheng et al., 2016; Bahdanau
et al., 2015), and the two-layer feedforward, i.e,
multi-layer perceptron (MLP) block. Acceleration
and memory reduction of the self-attention is rather
well explored (see, e.g., linear attention dating back
to the unnormalised linear Transformers of 1991
(Schmidhuber, 1991; Katharopoulos et al., 2020;
Choromanski et al., 2021; Schlag et al., 2021)), and
very efficient implementations (Dao et al., 2022)
are also available. In constrast, resource-efficient
MLP blocks are still underexplored. This is
our main focus, and it is of particular relevance
today, as the proportion of the total parameter
counts, compute and memory requirements due
to MLP blocks in Transformers is increasing in
ever-growing LLMs.
Letdmodel, dffdenote positive integers. Each
Transformer MLP block consists of one up-
projection layer with a weight matrix W1∈
Rdff×dmodel where typically dff= 4dmodel, and
one down-projection layer with parameters W2∈
Rdmodel×dffthat projects it back to the original size.
Non-linearity (typically ReLU) is applied between
these two layers. That is, an input x∈Rdmodelis
transformed to an output y∈Rdmodelas
u= ReLU ( W1x) (1)
y=W2u (2)
where u∈Rdff, and we omit biases (as well as
batch and time dimensions) for simplicity.
2Our non-expert CUDA implementation still has much
room for further optimization.Alternatively, this layer can be viewed as a key-
value memory accessed by attention (Vaswani et al.
(2017)3,Geva et al. (2021)), where keys and values
are rows and columns of weight matrices W1and
W2:
W1=
k⊺
1
k⊺
2...
k⊺
dff
(3)
W2=
v1v2. . .vdff
 (4)
where ki∈Rdmodel,vi∈Rdmodelfori∈ {1, ..., d ff}.
Then, the output is computed as “attention”:
y=dffX
i=1viReLU( k⊺
ix) =dffX
i=1αivi (5)
where αi= ReLU( k⊺
ix)∈R≥0are the “attention
weights.” Note that αi=u[i]where u[i]∈R
denotes the i-th component of u∈Rdffin Eq. 1.
Unlike the standard self-attention, the MLP block
uses a ReLU activation function (instead of the
softmax) without scaling.
It has been observed that, in practice, only a few
of the factors k⊺
ixare positive (Li et al., 2023; Shen
et al., 2023), making the first layer’s output, i.e., u,
sparse. Concretely, Shen et al. (2023) report that in
a Transformer with dmodel = 256 anddff= 1024 ,
10% of the channels account for 90% of the total
activation mass. We confirm this trend in our own
preliminary study. Fig. 1 shows the average number
of non-zero units in uof size dff= 2053 in our
47M parameter dense model trained on WikiText-
103 (we refer to App. A.2 for more details). The
number is below 200 for all layers. This suggests
that the MLP block can be approximated without a
significant performance loss. Note that this is also
supported by the findings of Zhang et al. (2022).
3 Approximating 2-layer MLPs
Here we present a unified view on methods to
approximate 2-layer MLPs (Sec. 2) that includes
many existing methods such as MoEs (Sec. 3.3)
and PKMs (Sec. 3.2).
3See the appendix “Two feedforward Layers = Attention
over Parameter” in their paper version “arXiv:1706.03762v3.”

--- PAGE 3 ---
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Layer0200400Active channelsFigure 1: Number of active channels in uin our dense
47M parameter model on WikiText-103 (out of 2053
total channels). Standard deviation over all tokens of
the test and validation set.
Preliminaries. Letˆy∈Rdmodeldenote an approx-
imation of y∈Rdmodelin Eq. 5. Let yi∈Rdmodelde-
noteyi=αivifori∈ {1, ..., d ff}. The core idea is
to approximate the sum in Eq. 5, i.e., y=Pdff
i=1yi
by only keeping a subset S ⊂ { 1, ..., d ff}of the
key-value pairs, i.e., ˆy=P
i∈Syi. The intu-
ition of this approximation is as follows. We as-
sume that a good approximation ˆyofyis the
one that minimizes their Euclidean distance e=
||ˆy−y||2
2∈R, which can now be expressed as
e=||P
i∈¯Sαivi||2
2where ¯Sdenotes the comple-
ment of S, i.e., ¯S={1, ..., d ff}\S . Since we have
e=||P
i∈¯Sαivi||2
2≤P
i∈¯Sαi||vi||2
2(triangle in-
equality; where the equality is achieved when vi
are orthogonal), this upper-boundP
i∈¯Sαi||vi||2
2
can be minimized if each term ci=αi||vi||2
2∈R
are small. If we further assume that all value vec-
torsvihave the same norm, the crucial factor for
approximation quality is reduced to the attention
weights αi. In this context, we also call αithe
contribution of key-value pair i.
LetKbe a positive integer. The general idea
of all methods discussed in this work is to keep
Kpairs (ki,vi) whose contribution αiis the high-
est, and ignore other low-contribution pairs. The
goal is to find the best mechanism to select such K
pairs. Here we discuss three variants: Top- Kacti-
vation (Sec. 3.1), Product-Key Memories (PKMs,
Sec. 3.2), and Mixture of Experts (MoEs, Sec. 3.3).
3.1 Top- KActivation Function
The most straightforward implementation of the
approximation described above is the top- Kacti-
vation function:
Ex= arg topk( u, K)⊂ {1, ..., d ff} (6)
ˆy=X
i∈Exαivi (7)
Unfortunately this only saves less than half of the
entire computation: while this allows us to re-
duce computation of Eq. 2, no computation can
be saved in Eq. 1 because full computation of u=ReLU ( W1x)is required for Eq. 6. Going beyond
this requires to also introduce some approximation
to Eq. 6 as in PKMs (Sec. 3.2) and MoEs (Sec. 3.3).
3.2 Product-Key Memories (PKMs)
Product-Key memories (Lample et al., 2019) con-
sist of replacing W1∈Rdff×dmodelin Eq. 1 by two
matrices Wa,Wb∈R√dff×dmodel
2. It slices the in-
put vector x∈Rdmodelinto two halves, xa,xb∈
Rdmodel
2, so that x=xa|xb, where |denotes con-
catenation. The matrix multiplication is then per-
formed on these smaller vectors: ua=Waxaand
ub=Wbxb. Then u∈Rdffis calculated by com-
bining the elements of ua∈R√dffandub∈R√dff
in all possible ways (i.e., Cartesian products), simi-
larly to the outer product, but using addition instead
of multiplication, i.e., for all i∈ {1, ..., d ff},
u[i] =ub[⌊i/p
dff⌋] +ua[imodp
dff](8)
In addition to applying Top- Kat the output as in
Sec 3.1, here Top- Kcan also be used to accelerate
the operation above. By applying Top- Ktoua
andubbefore combining them to compute u, only
theK2<< d ffcomponents of u[i]have to be
calculated, and they are guaranteed to contain the
Kbiggest components of the full u.
In the original formulation (Lample et al., 2019),
PKMs use a softmax activation function, taking in-
spiration from self-attention (Vaswani et al., 2017).
Instead, we’ll show how a non-competing activa-
tion function, such as ReLU is a better choice (see
Sec. 6.2).
3.3 Mixture of Experts (MoE)
LetNE, Gdenote positive integers. MoEs par-
tition dffpairs of ( ki,vi) (see their definition in
Sec. 2) into NEgroups of size Geach, such that
G·NE=dff. This means that the weight matrices
W1∈Rdff×dmodelandW2∈Rdmodel×dff(Eqs. 1-2)
are partitioned into matrices We
1∈Rdff
NE×dmodel
andWe
2∈Rdmodel×dff
NEfore∈ {1, ..., N E},
We
1=
k⊺
eG+1
k⊺
eG+2...
k⊺
(e+1)G
(9)
We
2=
veG+1veG+2. . .v(e+1)G

(10)

--- PAGE 4 ---
The output is computed as:
ˆy=X
e∈ExWe
2s[e] ReLU( We
1x) (11)
where s[e]∈Ris the e-th element of vector
s∈RNEcomputed by an expert scoring func-
tionsel :Rdmodel→RNE(typically s= sel( x) =
softmax( W3x)withW3∈RNE×dmodel), and Ex
denotes a subset of indices {1, ..., N E}result-
ing from the Top- Koperation on s, i.e.,Ex=
arg topk( s, K). Note that in some variants, ad-
ditional re-normalization is applied after Top-K,
so thatP
e∈Exs[e] = 1,s[e]≥0; we define such
an operation as norm topk , see its exact defini-
tion in App. A.14. The efficiency of MoEs comes
from the fact that NE≪dff, thus calculating s
is cheap. Furthermore, GandKare chosen so
thatG∗K≪dff, so the calculation performed by
experts is less expensive than the dense MLP.
Given the notation above, it is straightforward to
see that MoEs can also be viewed as approximating
2-layer MLPs with a trainable component (i.e., the
selection function selto produce s). Similarly to
Eqs. 5 and 7, Eq. 11 can be expressed as:
ˆy=X
e∈ExGX
i=1αeG+is[e]veG+i (12)
where, compared to Eqs. 5 and 7, the “contribution
scores” of key-value pair i(defined in Sec. 3/Pre-
liminaries) have an additional factor s[e]of an
expert group eto which the key-value pair belongs.
The key challenge of MoEs is to learn an ex-
pert selection mechanism/function selabove that
assigns high scores to only a few experts (so that we
can ignore others without sacrificing performance),
while avoiding a well-known issue, called expert
collapsing , where only a few experts are used and
the rest are never selected. To avoid this, some regu-
larization is typically applied to the selection score
sel(x), encouraging more uniform routing of ex-
perts across the whole batch of tokens. We provide
a comprehensive review of MoE variants and their
details in Sec. 4 and our improved version in Sec. 5.
4 Existing MoE variants
Several variations of MoEs have been proposed
with many different details. Here we briefly review
the most popular and representative ones (e.g., we
4In the case of the softmax( ·)activation function, this is
equivalent to applying Top- Kto the logits before softmax.do not cover those that make use of reinforcement
learning for expert routing) before describing
our improved version in Sec. 5. We’ll review
their expert selection function andregularization
method , and highlight their key characteristics.
Sparsely Gated Mixtures of Experts. Shazeer
et al. (2017) have revisited MoEs (Jacobs et al.,
1991; Ivakhnenko and Lapa, 1965) with the Top- K
operation, allowing a reduction in its resource de-
mands. Their method is basically the one described
in Sec. 3.3 (with re-normalization after Top- K)
except that they use a noisy gating function:
sel(x) = softmax(
W3x+N(0,1)·softplus( W4x))(13)
where W4∈RNE×dmodel, the Gaussian noise term
N(0,1)is element-wise and independent for each
channel, and softplus( x) = log(1 + ex). They use
the following auxiliary regularization term for load
balancing,
L= CV X
x∈Bnorm topk(sel( x))!
(14)
where CV(x) =µx
σxis the coefficient of variation
andBis the set of all tokens in the batch.
Key characteristics: The scores are normalized
after the top- Koperation (with K > 1), which is
equivalent to applying top- Kbefore the softmax.
Switch Transformer. Fedus et al. (2022) inte-
grate the MoE above into the Transformer to obtain
their Switch Transformer. In terms of MoE details,
one of Fedus et al. (2022)’s key claims is that top-1
routing is enough. Their selection function is sim-
ply:sel(x) = softmax( W3x), but they propose
a hard load-balancing between experts that run on
different hardware accelerators: At most µ|B|
NEto-
kens are allowed to be routed to an expert, where
µ∈R>0is the capacity factor (typically between 1
and1.5), defining how many times more tokens can
be processed by one expert compared to the ideal
case of uniform routing. Each expert is forbidden
to process more than this number of tokens. For
regularization, the fraction of the tokens f∈RNE
processed by each expert, and the average selection
probability p∈RNEfor each expert are calculated

--- PAGE 5 ---
(K= 1; top-1 is used) as:
fi=1
|B|X
x∈B1{i∈arg topk(sel( x), K)}(15)
p=1
|B|X
x∈Bsel(x) (16)
L=NEf·p (17)
where 1denotes the indicator function (which is
equal to 1if the argument is true, and 0otherwise),
and ·denotes dot product. Intuitively, this serves
as an adaptive regularization that penalizes experts
that are used often with high “weights.” In addition,
they use dropout with a high drop rate ( 40%) in
the experts (but only 10% in the normal layers).
Furthermore, Fedus et al. (2022) also propose to
initialize the experts withq
0.1
G. As we’ll see in
Sec. 5, we use a modified version of this scheme.
Note that applying Top- Kafter softmax encour-
ages collapsing: if the score of the selected ex-
pert is increased, the scores of all other experts are
automatically decreased. This is not the case for
Shazeer et al. (2017): In their method, only the se-
lected experts compete with each other, so if their
presence is beneficial, their score can be increased.
Key characteristics: Note that Top-1 is applied
after the softmax without re-normalization.
BASE layers and S-BASE. Inspired by the rout-
ing strategy and the hard capacity factor of the
Switch Transformer, Lewis et al. (2021) propose
BASE layers. They use top-1 routing and a sigmoid
activation σin the selection function:
sel(x) =σ(W3x) (18)
Now instead of using arg topk , they solve the fol-
lowing linear assignment problem to find the index
ex∈ {1, ..., N E}of the expert to which each input
x∈ B is routed,
maximize
ex∈{1,...,N E},x∈BX
x∈Bsel(x)[ex] (19)
s.t.∀i∈ {1, ..., N E},X
x∈B1{ex==i}=|B|
NE
This guarantees uniform assignment of experts,
which is efficient for multi-accelerator training.
The output is computed using Eq. 11 with Ex=
{ex}(a set with a single element; “top-1”). How-
ever, at inference time, no such balancing is pos-
sible because not all tokens of the sequence areavailable at each step; Ex={arg max (sel( x))}is
used instead. Lewis et al. (2021) show that, while
during training, the routing is enforced to be com-
pletely uniform, during the test time, the distribu-
tion looks exponential (in fact, this is similar to the
Switch Transformer but more balanced for BASE).
The algorithm for solving the linear assignment
problem (Eq. 19) is difficult to implement effi-
ciently on modern accelerators. Clark et al. (2022)
have proposed to use the Sinkhorn algorithm
(Sinkhorn, 1964; Sinkhorn and Knopp, 1967) in-
stead (resulting in a model called Sinkhorn-BASE
or S-BASE), to approximate the solution to this
problem (note that similar routing is independently
discussed by Kool et al. (2021)). They report that
this works well, while being simpler to implement.
Thus, our reimplementation of BASE is S-BASE
using the Sinkhorn algorithm.
Key characteristics: During training, Sinkhorn
iterations are used on scores to obtain a balanced as-
signment. The sigmoid activation is always applied
to compute the weighting score.
Overall , all load-balancing methods above are
rather complex. We propose simpler but effective
approach for MoEs in Sec. 5.
5 Improving Mixture of Experts
Here we present our improved MoE variant, which
we call σ-MoE. We conduct thorough ablation stud-
ies on our design choices in Sec. 6.
σ-MoE Expert Selection Function. Our MoE
make use of the top- Koperation (unlike BASE).
The activation we use on the selection function is
sigmoid (as in Eq. 18 of BASE) instead of softmax
used in Switch Transformer and Sparsely Gated
Mixtures of Experts. This choice is motivated by
the view of MoEs as approximate 2-layer MLPs
(Sec. 3). In fact, softmax introduces competition
between experts. No such competition between
channels is used in the regular 2-layer MLP (i.e.,
there is no constraint on αiin Eq. 5). This suggests
that, in principle, no competition is needed
between terms in the sum of Eq. 12 in the MoE
either, to induce sparsity. It is also well known
to practitioners that softmax as regular activation
negatively affects the trainability of standard MLPs.
Softmax combined with top- Kcan also encourage
expert collapsing: when the selection score of one
expert increases, the score of the others automat-
ically decreases. For all these reasons, we opt for

--- PAGE 6 ---
sigmoid instead of softmax; we experimentally
confirm that this is indeed a good choice.
Additionally, looking at MoEs in this framework
gives us hints on combining them with Top- K
activation (Sec. 3.1) for further acceleration. We
can calculate ue=s[e] ReLU( We
1x)(Eq. 11)
for the selected experts and perform an additional
Top-Kto keep the highest units among them and
set the rest to zero. We leave this for future work.
σ-MoE Initialization. Another design choice
guided by the MLP-approximation view of MoEs
(Sec. 3) is the initialization scheme for experts. Typ-
ically, experts are assumed to be independent, and
the standard deviation of the initialization (Glorot
and Bengio, 2010; He et al., 2015) of We
2is calcu-
lated based on Ginstead of dff. Our experiments
in Sec. 6.3 show that this is sub-optimal.
In contrast, we initialize all weight matrices
identically to the pre-layernorm dense baselines,
not taking in account the smaller size of the indi-
vidual experts, i.e., We
1∼ N(0,q
2
dmodel·nlayers)and
We
2∼ N(0,q
2
dff·nlayers)where nlayers denotes the
number of layers, using dmodel anddffinstead of G.
We also take special care when initializing W3
of the selection function. We initialize it to a
normal distribution with the same standard devi-
ation as We
1, but we also ensure that the rows
ofW3have the same norm5. This can be easily
achieved in practice by initializing the weights to
W′
3∼ N(0,1), rescaling its rows to norm 1, and
then rescaling the whole matrix again to have the
desired standard deviation. Note that each scalar
score in sis the dot product of a row of W3andx.
This initialization method ensures that only the an-
gle between xand the rows of W3initially affects
the score s, rather than an additional random factor
resulting from initialization.
σ-MoE Regularization. As already noted in
Sec. 4, existing regularization methods for load-
balancing are complex (e.g., Switch Transformers
need to deal separately with the actual selection
distribution and the scores, Sparsely Gated Mixture
of Experts needs noise in the selection function).
In contrast, we propose to simply maximize the
entropy of the selection distribution p∈RNEcal-
culated across the entire batch. Intuitively, this is a
5Having rows with different norms would discourage the
use of experts corresponding to rows with small norms, as
their selection score would be low even if the angle of the
selector (row of W3) fully aligns with x.simple way to encourage equal expert usage within
the batch and prevent unnecessary overconfidence
in selecting individual experts. Let Bbe the set
of all tokens in the batch (counting through both
the batch and time dimensions). We introduce the
following regularization term L:
p=1
|B|X
x∈Bsoftmax( W3x) (20)
L=NEX
e=1p[e] logp[e] (21)
Furthermore, we propose to randomly drop com-
plete experts, during training; we refer to this as
expert dropout . Unlike the standard dropout on the
activation level, we do not apply rescaling, i.e.,
sel(x) =(
σ(Wsx)⊙m if training
σ(Wsx) otherwise(22)
where m∈ {0,1}NE,m∼Bernoulli (1−δ),
where δis the dropout rate, and ⊙is the element-
wise product. This prevents the dropped experts
from being selected while not affecting the other
ones. Intuitively, when an expert dropout removes
a popular expert, it forces the less popular ones to
take over. Thus, the chance of them being trained
and improved increases. We experimentally show
that our regularization method (Eq. 21) and expert
dropout (Eq. 22) are both effective despite their
simplicity.
6 Experiments
Our experimental setup is based on Dai et al.
(2019)’s Transformer XL with some modifications:
we use pre-layer norm and reduce the number of
training steps to 100k to reduce the computational
budget. Also, to match the parameter counts be-
tween the baseline and MoEs, we slightly modify
the hyperparameters of the baselines (Dai et al.,
2019). In fact, our MoE CUDA kernel can only
work with dimensions divisible by 4. We round the
original sizes up to the next suitable number, e.g.,
we change dmodel of our 47M-parameter WikiText-
103 model from the original 410 to 412. Further-
more, since MoEs require extra parameters for the
expert selection function, we compensate for these
by increasing the dffof the baseline model to match
the number of parameters. Our modified baseline
model on Enwik8 still has 41M parameters and
performs similarly to the original Transformer XL

--- PAGE 7 ---
(see Tab. 1). For WikiText-103, we use subword
units (Sennrich et al., 2016) using SentencePiece
tokenizer (Kudo and Richardson, 2018) instead of
the word-level vocabulary, to avoid extra tricks re-
quired to reduce the parameter count and compute
requirement resulting from the huge vocabulary
size. On WikiText-103, we consider two different
model sizes: a 47M-parameter one (denoted by
“WT-S” for “small”), and a 262M-parameter one
(“WT-B” for “big”). We refer to Enwik8 as “E8” in
certain tables. For more details, see Appendix B.
For all the methods considered, we use them
inevery MLP block of the model, which is not a
common practice in the literature. Typically, MoE
(or other approximation methods) is used only once
every nthlayer or even only in one layer. This is not
satisfactory since our goal is to find a generally ap-
plicable method that can accelerate all layers across
the whole model. Moreover, this amplifies the dif-
ference between different methods, helping better
illustrate effects of each of the design choices.
6.1 Top- K
We first evaluate the Top- Kmethod (Sec. 3.1).
This standalone evaluation is important as Top- K
is the basis of both the PKM and the MoE
approximations. Tab. 1 shows the results. We
observe that not only Top- Kin the MLP blocks
preserves the performance of Transformers, it even
improves performance. We hypothesize that these
improvements are due to the reduction in feature
interference as described by Elhage et al. (2022).
However, we obviously can not arbitrarily reduce
K; there should be a trade-off between the denois-
ing effect and the capacity of the network. Here,
the optimal value we find is K= 128 orK= 512 .
Table 1: Effects of the top-k activation function on the
perplexity (WikiText-103) and bits/character (Enwik8).
Dataset #params dffK bpc/perplexity
Enwik8 41M 2053 - 1.08
41M 2053 128 1.07
41M 2053 256 1.08
41M 2053 512 1.08
WikiText 103 47M 2053 - 11.81
47M 2053 64 11.86
47M 2053 128 11.74
47M 2053 256 11.74
47M 2053 512 11.68
WikiText 103 262M 4110 - 9.46
262M 4110 128 9.26
262M 4110 256 9.34
262M 4110 512 9.366.2 Product-Key Memory (PKM)
Our view of Sec. 3 suggests using a non-
competitive activation such as ReLU instead of the
softmax used in the original PKM (Lample et al.,
2019). Our experiments confirm the benefits of
this choice (Tab. 2): the performance of the ReLU
variants is much closer to the dense baseline (see
also related findings in Shen et al. (2023)). But
even the best PKM models underperform the dense
baselines, indicating the fundamental limitation of
PKMs. Note that, as stated above, we conduct
a careful comparison between the approximation
method (here, PKM) and the dense baseline using
the same number of parameters. For more results
and details on PKM, we refer to App. A.3.
Table 2: Performance of the parameter-matched PKM
models. We provide more results in Appendix/Tab. 6.
Variant Nonlin WT-S WT-B E8
Dense Baseline ReLU 11.81 9.46 1.08
PKM Softmax 13.96 11.10 1.16
ReLU 12.77 9.98 1.11
6.3 Mixture of Experts (MoE)
Here we evaluate our σ-MoE models (Sec. 5) on
Enwik8 and WikiText-103 as well as two additional
datasets, C4 (Raffel et al., 2020) and the newly
proposed peS2o (Soldaini and Lo, 2023). Given
the large sizes of C4 and peS2o, we cannot afford to
train for a full epoch; we train for 100k steps with
the same hyperparameters as for WikiText-103.
Main results. Tab. 3 shows the main results. Our
σ-MoE models match the performance of their
parameter-equal dense baselines, while achieving
significant memory and compute reduction. These
models use K= 4 forNE= 16 orNE= 32 ,
which is a “moderate” level of sparsity but already
offering significant compute reduction as shown
in the column “% FLOPs”; concrete compute and
memory reduction is further shown in Fig. 2 (see
Appendix A.5 for details). Naturally, there is a
limit on the minimum sparsity level to preserve
good performance of MoEs, which is determined
by several factors. First, we empirically find that
experts with a group size of G < 128generally
degrades performance. Second, our benchmarks
with the Top- Koperation (Tab. 1) and our
ablations (Tab. 10 in the Appendix) show that
the minimum number of simultaneously active

--- PAGE 8 ---
Table 3: Performance of parameter-batched σ-MoEs
on perplexity (WikiText-103, C4 and peS2o) and
bits/character (Enwik8). Ours matches or surpasses the
performance of the dense baselines across all datasets.
Dataset Model #params % FLOPs bpc/ppl
Enwik8 Dense 41M 100.0% 1.08
σ-MoE 41M 25.0% 1.08
WikiText-103 Dense 47M 100.0% 11.81
σ-MoE 47M 25.0% 11.71
WikiText-103 Dense 262M 100.0% 9.46
σ-MoE 262M 12.5% 9.44
C4 Dense 47M 100.0% 23.76
σ-MoE 47M 25.0% 23.25
C4 Dense 262M 100.0% 17.79
σ-MoE 262M 12.5% 17.46
peS2o Dense 47M 100.0% 14.34
σ-MoE 47M 25.0% 14.12
peS2o Dense 262M 100.0% 10.91
σ-MoE 262M 12.5% 10.91
channels G·Kneed to be above a certain critical
threshold (usually around 256-512). Finally, we
match the number of parameters of the baseline
model; this is the last constraint. Under these
constraints, we find that the performance of the
dense baselines can be matched using 25% of the
required FLOPs and memory for activations for our
small models, and 12.5%sparsity for the big one
(note that FLOPs here do not take into account the
linear projection used to select the experts, which
is negligible within the range of NEused here).
Increasing NEand Impact of Sparsity. The
results above demonstrate that our σ-MoEs can
be configured to match the desired performance
with fewer resources. Here we conduct an extra
experiment where we naively increase NE(while
keeping K= 4) from 16 to 128. This increases the
number of parameters to 238M, while keeping the
speed and memory requirements comparable to the
original model (column “WT-S*” in Tab. 4). This
model achieves a test perplexity of 10.37, which
is worse than 9.46 of the 262M dense model (see
Tab. 1). Indeed, even when the parameter count is
matched, there are other bottlenecks that are cru-
cial, e.g., here dmodel is much smaller (412 vs 1024).
We construct another dense baseline by setting ev-
ery hyperparameter like in the 47M model, except
dff, which is set to 16480 to match the number of
parameters of the NE= 128 MoE. This baseline
achieves a perplexity of 10.03: thus, the gap be-
tween the scaled-up MoE and its dense counterpart
256 1024 2048 4096
dmodel0200Time (ms)MLP
MoE
0.00.51.0
Memory (GB)MLP
MoEFigure 2: Execution time and memory usage of a
forward-backward pass of a single MLP and MoE layer.
|B|= 32768 , corresponding to a batch size 64 and
sequence length 512, dmodel indicated on the x-axis,
K= 4,G= 128 ,dff= 4dmodel, and NE=dff
G(fol-
lowing the standard 1x-4x-1x shape). Full/dashed lines
show the execution time/memory consumption, respec-
tively. Shows our mixed-precision Triton implementa-
tion on an RTX 3090 with PyTorch 2.1 and CUDA 12.
still remains significant (10.37 vs 10.03), unlike
with the MoE with moderate sparsity. This indi-
cates the importance of controlling MoE sparsity to
preserve its performance against the dense baseline.
Comparison to Existing MoEs. We also com-
pare our σ-MoE to other MoE variants (Sec. 4),
namely Switch Transformer (Fedus et al., 2022),
S-BASE (Clark et al., 2022)6and the basic softmax
variant. Tab. 4 shows the results for multiple vari-
ants on WikiText-103 and Enwik8. Additionally,
we compare σ-MoE to the most important baselines
on C4 and peS2o in Tab. 5. As Switch Transformer
and S-BASE select only one single expert ( K= 1),
we increase the expert size by a factor of 4 (instead
ofG= 128 in our models, we use G= 512 ), and
we decrease NEby the same factor for fair com-
parison in terms of the parameter count. Neither
of them uses our proposed expert dropout. For
Switch Transformer, we test a variant with stan-
dard dropout in the experts (see App. B for details),
and a version without. We also extend S-BASE to
K= 4, which is similar to ours, except for the bal-
ancing method. Even considering all these cases,
ourσ-MoE outperforms Switch Transformer and
S-BASE. Note that in terms of FLOPs and memory
usage, all MoE variants are equivalent given the
same hyperparameters ( G,dmodel, and K).
Ablation Studies. Finally we conduct ablation
studies of individual design choices (Sec. 5). Tab. 4
shows the results. Standard dropout instead of ex-
pert dropout leads to performance degradation for
most of the cases, except the model with NE= 128
experts. The softmax-based selection functions
6Unlike the original ones, our implementation does not
enforce capacity factor-based hard balancing.

--- PAGE 9 ---
Table 4: Ablation studies. WT-S* is obtained by naively
scaling NEin WT-S. More details in Sec. 6.3 & Tab. 10.
Dataset WT-S WT-S* WT-B E8
# params. (in M) 47 238 262 41
Switch Transformer 12.27 11.24 9.68 1.08
no dropout 11.88 11.10 9.77 1.10
S-BASE ( K=4,G=128) 13.01 10.96 10.50 1.17
K= 1, G= 512 12.32 11.31 9.77 1.32
σ-MoE ( K=4,G=128) 11.59 10.37 9.44 1.08
standard dropout 12.01 10.27 9.53 1.08
softmax (renorm.) 11.89 11.27 9.58 1.09
softmax (no renorm.) 12.05 10.54 9.62 1.09
standard init 11.80 10.59 9.67 1.08
no regularization 11.83 10.41 9.51 1.08
K= 8, G= 64 11.63 10.30 9.58 1.08
K= 2, G= 256 11.84 10.44 9.56 1.09
K= 1, G= 512 11.90 10.83 9.58 1.09
Table 5: Perplexity of σ-MoE compared to parameter-
matched baselines on C4 and peS2o datasets.
Dataset C4 C4 peS2o peS2o
dmodel 412 1024 412 1024
# params 47M 262M 47M 262M
G K
Dense 128 1 23.76 17.79 14.34 10.91
σ-MoE 128 4 23.25 17.46 14.12 10.91
Switch 512 1 24.47 18.29 14.74 11.56
S-BASE 128 4 35.48 18.53 16.61 11.72
(with and without re-re-normalization) consistently
perform worse than our sigmoid one. The same is
true for the standard initialization ; ours is better.
Interestingly, removing all regularization methods
degrades performance but does not entail catas-
trophic collapse even with NE= 128 . We also
examine the best ( G,K) combinations, given a con-
stant number ( G·K) of active pairs ki,vi; we find
a high K= 4works best within this range. Further
analysis of our σ-MoE can be found in App. A.4.
Analyzing expert utilization. A typical failure
mode of MoEs is expert collapse, where only a
few experts are used while others are completely
ignored or underused. Here we conduct an analysis
to evaluate whether various models including ours
are affected by this issue. For each layer, we com-
pute the proportion of the expert selection weights
assigned to each expert ( sel(x)) on the entire
validation set of WikiText-103. We use WT-S*
models from Tab. 4 with 128 experts. A represen-
tative layer is shown in Fig. 3. Models with poor
performance (see Tab. 4), i.e., Switch Transformer
(red) and a “bad” variant of σ-MoE with a softmax
and renormalization “softmax (renom.)” ( green ),
0 16 32 48 64 80 96 112
Expert10−610−410−2Selection proportionσ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)Figure 3: The total proportion of selection weights as-
signed to a given expert (x-axis; sorted by their popu-
larity) on the validation set of Wikitext-103 using the
WT-S* models from Tab. 4. This is for one representa-
tive layer (“Layer 5”; similar plots for other layers are
shown in Fig. 7 in the appendix). The models with poor
performance (Tab. 4), i.e., Switch Transformer ( red) and
σ-MoE with a softmax and renormalization “softmax
(renom.)” ( green ) can be easily identified. In contrast,
the fine performance differences between the rest of the
models do not seem to be due to expert collapse.
can be easily identified: they severely suffer from
the expert collapse problem. The statistics are
rather similar for all other models; the fine per-
formance differences among these models do not
seem to be due to expert collapse. Remarkably, our
entropy-regularized models with expert dropout,
especially σ-MoE, are capable of matching the ex-
pert usage balancing of S-BASE without using the
Sinkhorn activation function. Note that in general,
we do not consider uniform expert activation to be
optimal: we expect expert specialization, and thus
the frequency of their usage should depend on the
occurrence of the task they are performing.
7 Conclusion
Our novel view unifies methods that approximate
2-layer MLPs, such as Top- K, Mixture of
Experts (MoE) and product-key memory (PKM)
methods. While Top- Kby itself provides limited
performance improvements and speedups, further
speedup requires PKM or MoE. A non-competitive
activation function inspired by our unified view
improves both PKM and MoE. Further novel
enhancements of MoEs yield our σ-MoE which
outperforms existing MoEs. Importantly, our
σ-MoE with moderate sparsity matches the
performance of parameter-equal dense baselines
while being much more resource-efficient. Our
new insights improve the training of language
models with limited hardware resources, making
language modeling research more accessible.

--- PAGE 10 ---
Limitations
Our experiments show that if we naively increase
the number of experts, the performance gap be-
tween MoE models and their dense counterparts in-
creases. This indicates the need for careful control
of sparsity and hyper-parameters, which remains a
challenge for MoEs.
Our CUDA kernel is sub-optimal and I/O
limited. However, even in its current form, it
already yields significant performance boosts
and memory reduction. We expect that an expert
CUDA programmer could improve the speed of
our kernel by at least a factor of 2.
We do not consider load balancing between hard-
ware accelerators as is done in Switch Transformers
and S-BASE. Our goal is to make a larger model
fit a single accelerator, or multiple accelerators in
the standard data-parallel training. Our preliminary
experiments suggest that such balancing entails a
performance hit.
We could not reproduce the 277M Enwik8
model of Dai et al. (2019), because we could not
fit the beaseline model on any of our machines.
We tried to use rotary positional encodings with
PyTorch 2.0’s memory-efficient attention to reduce
it’s memory consumption; however, this resulted
in a significant performance degradation (even for
the smaller models).
Our study focuses on end-to-end trainable
MoEs. Other MoE methods (Irie et al., 2018; Li
et al., 2022) that pre-train LMs on disjoint data,
to recombine them later into a single model, are
out-of-scope.
Our study only considers standard Transform-
ers; however, similar acceleration methods are of
utmost importance for shared-layer Transformers,
such as Universal Transformers (Dehghani et al.,
2019) and NDRs (Csordás et al., 2022). In fact,
layer sharing dramatically reduces the number of
parameters. Compensating for this by naively in-
creasing dmodel ordffresults in prohibitively high
memory overhead and slow execution. In contrast,
MoEs allow increasing the number of parameters
without such dramatic drawbacks. We leave shared-
layer MoEs for future work.
Acknowledgements
This research was partially funded by ERC Ad-
vanced grant no: 742870, project AlgoRNN, and
by Swiss National Science Foundation grant no:
200021_192356, project NEUSYM. We are thank-ful for hardware donations from NVIDIA and IBM.
The resources used for this work were partially pro-
vided by Swiss National Supercomputing Centre
(CSCS) project s1154, s1205 and d123. Finally, we
would like to thank Dániel Berényi for his support
with the CUDA kernel development.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In Int. Conf. on Learn-
ing Representations (ICLR) , San Diego, CA, USA.
Tom B Brown et al. 2020. Language models are few-
shot learners. In Proc. Advances in Neural Informa-
tion Processing Systems (NeurIPS) , Virtual only.
Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016.
Long short-term memory-networks for machine read-
ing. In Proc. Conf. on Empirical Methods in Natu-
ral Language Processing (EMNLP) , pages 551–561,
Austin, TX, USA.
Zewen Chi, Li Dong, Shaohan Huang, Damai Dai,
Shuming Ma, Barun Patra, Saksham Singhal, Payal
Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and
Furu Wei. 2022. On the representation collapse of
sparse mixture of experts. In Proc. Advances in Neu-
ral Information Processing Systems (NeurIPS) , New
Orleans, Louisiana, USA.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Krzysztof Marcin Choromanski, Valerii Likhosherstov,
David Dohan, Xingyou Song, Andreea Gane, Tamás
Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz
Mohiuddin, Lukasz Kaiser, David Benjamin Be-
langer, Lucy J. Colwell, and Adrian Weller. 2021.
Rethinking attention with performers. In Int. Conf.
on Learning Representations (ICLR) , Virtual only.
Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur
Mensch, Michela Paganini, Jordan Hoffmann, Bog-
dan Damoc, Blake A. Hechtman, Trevor Cai, Se-
bastian Borgeaud, George van den Driessche, Eliza
Rutherford, Tom Hennigan, Matthew J. Johnson,
Albin Cassirer, Chris Jones, Elena Buchatskaya,
David Budden, Laurent Sifre, Simon Osindero, Oriol
Vinyals, Marc’Aurelio Ranzato, Jack W. Rae, Erich
Elsen, Koray Kavukcuoglu, and Karen Simonyan.
2022. Unified scaling laws for routed language mod-
els. In Proc. Int. Conf. on Machine Learning (ICML) .
Róbert Csordás, Kazuki Irie, and Jürgen Schmidhuber.
2022. The neural data router: Adaptive control flow
in transformers improves systematic generalization.
InInt. Conf. on Learning Representations (ICLR) ,
Virtual only.

--- PAGE 11 ---
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-
bonell, Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-XL: Attentive language models beyond
a fixed-length context. In Proc. Association for Com-
putational Linguistics (ACL) , pages 2978–2988, Flo-
rence, Italy.
Tri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,
and Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
InProc. Advances in Neural Information Processing
Systems (NeurIPS) , New Orleans, Louisiana, USA.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Lukasz Kaiser. 2019. Universal
Transformers. In Int. Conf. on Learning Representa-
tions (ICLR) , New Orleans, LA, USA.
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke
Zettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-
cation for transformers at scale. In Proc. Advances in
Neural Information Processing Systems (NeurIPS) ,
New Orleans, Louisiana, USA.
Nelson Elhage, Tristan Hume, Catherine Olsson,
Nicholas Schiefer, Tom Henighan, Shauna Kravec,
Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain,
Carol Chen, Roger Grosse, Sam McCandlish, Jared
Kaplan, Dario Amodei, Martin Wattenberg, and
Christopher Olah. 2022. Toy models of superpo-
sition. Transformer Circuits Thread .
William Fedus, Barret Zoph, and Noam Shazeer. 2022.
Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. Journal
of Machine Learning Research (JMLR) , 23(1):5232–
5270.
Jerry A Fodor and Zenon W Pylyshyn. 1988. Connec-
tionism and cognitive architecture: A critical analysis.
Cognition , 28(1-2):3–71.
Mor Geva, Roei Schuster, Jonathan Berant, and Omer
Levy. 2021. Transformer feed-forward layers are
key-value memories. In Proc. Conf. on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 5484–5495, Punta Cana, Dominican Republic.
Xavier Glorot and Yoshua Bengio. 2010. Understanding
the difficulty of training deep feedforward neural
networks. In Proc. Int. Conf. on Artificial Intelligence
and Statistics (AISTATS) , pages 249–256, Sardinia,
Italy.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2015. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification.
InProc. IEEE Int. Conf. on Computer Vision (ICCV) ,
pages 1026–1034, Santiago, Chile.
Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia
Bruni. 2020. Compositionality decomposed: How
do neural networks generalise? Journal of Artificial
Intelligence Research , pages 757–795.Kazuki Irie, Shankar Kumar, Michael Nirschl, and Hank
Liao. 2018. RADMM: Recurrent adaptive mixture
model with applications to domain robust language
modeling. In Proc. IEEE Int. Conf. on Acoustics,
Speech and Signal Processing (ICASSP) , pages 6079–
6083, Calgary, Canada.
Alekse ˘ı Grigorievitch Ivakhnenko and Valentin Grig-
orévich Lapa. 1965. Cybernetic Predicting Devices .
CCM Information Corporation.
Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan,
and Geoffrey E. Hinton. 1991. Adaptive mixtures of
local experts. Neural Compututaion , 3(1):79–87.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-
pas, and François Fleuret. 2020. Transformers are
rnns: Fast autoregressive transformers with linear
attention. In Proc. Int. Conf. on Machine Learning
(ICML) , volume 119, pages 5156–5165, Virtual Only.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Int. Conf. on
Learning Representations (ICLR) , San Diego, CA,
USA.
Wouter Kool, Chris J Maddison, and Andriy Mnih. 2021.
Unbiased gradient estimation with balanced assign-
ments for mixtures of experts. In I (Still) Can’t
Believe It’s Not Better Workshop, NeurIPS , Virtual
Only.
Taku Kudo and John Richardson. 2018. Sentencepiece:
A simple and language independent subword tok-
enizer and detokenizer for neural text processing. In
Proc. Conf. on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 66–71, Brussels,
Belgium.
Guillaume Lample, Alexandre Sablayrolles,
Marc’Aurelio Ranzato, Ludovic Denoyer, and
Hervé Jégou. 2019. Large memory layers with prod-
uct keys. In Proc. Advances in Neural Information
Processing Systems (NeurIPS) , pages 8546–8557,
Vancouver, Canada.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,
Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. 2021.
Gshard: Scaling giant models with conditional com-
putation and automatic sharding. In Int. Conf. on
Learning Representations (ICLR) , Virtual only.
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman
Goyal, and Luke Zettlemoyer. 2021. BASE layers:
Simplifying training of large, sparse models. In Proc.
Int. Conf. on Machine Learning (ICML) , volume 139,
pages 6265–6274, Virtual only.
Margaret Li, Suchin Gururangan, Tim Dettmers, Mike
Lewis, Tim Althoff, Noah A Smith, and Luke Zettle-
moyer. 2022. Branch-train-merge: Embarrassingly
parallel training of expert language models. Preprint
arXiv:2208.03306 .

--- PAGE 12 ---
Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang
Li, Ankit Singh Rawat, Sashank J. Reddi, Ke Ye,
Felix Chern, Felix Yu, Ruiqi Guo, and Sanjiv Kumar.
2023. The lazy neuron phenomenon: On emergence
of activation sparsity in transformers. In Int. Conf. on
Learning Representations (ICLR) , Kigali, Rwanda.
Peter Pagin and Dag Westerståhl. 2010. Compositional-
ity I: Definitions and variants. Philosophy Compass ,
5(3):250–264.
Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and
Jakob Uszkoreit. 2016. A decomposable attention
model for natural language inference. In Proc. Conf.
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 2249–2255, Austin, TX, USA.
Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Te-
jani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch:
An imperative style, high-performance deep learning
library. In Proc. Advances in Neural Information Pro-
cessing Systems (NeurIPS) , pages 8024–8035, Van-
couver, Canada.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, H. Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, Eliza Rutherford, Tom Hennigan, Ja-
cob Menick, Albin Cassirer, Richard Powell, George
van den Driessche, Lisa Anne Hendricks, Mari-
beth Rauh, Po-Sen Huang, Amelia Glaese, Jo-
hannes Welbl, Sumanth Dathathri, Saffron Huang,
Jonathan Uesato, John Mellor, Irina Higgins, Antonia
Creswell, Nat McAleese, Amy Wu, Erich Elsen, Sid-
dhant M. Jayakumar, Elena Buchatskaya, David Bud-
den, Esme Sutherland, Karen Simonyan, Michela Pa-
ganini, Laurent Sifre, Lena Martens, Xiang Lorraine
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
Gribovskaya, Domenic Donato, Angeliki Lazaridou,
Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-
poukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-
tiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,
Daniel Toyama, Cyprien de Masson d’Autume, Yujia
Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,
Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris
Jones, James Bradbury, Matthew J. Johnson, Blake A.
Hechtman, Laura Weidinger, Iason Gabriel, William
Isaac, Edward Lockhart, Simon Osindero, Laura
Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,
Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-
ray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling
language models: Methods, analysis & insights from
training gopher. Preprint arXiv:2112.11446 .
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research
(JMLR) , 21:140:1–140:67.
Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber.
2021. Linear transformers are secretly fast weight
programmers. In Proc. Int. Conf. on Machine Learn-
ing (ICML) , volume 139, pages 9355–9366, Virtual
only.
Jürgen Schmidhuber. 1991. Learning to control fast-
weight memories: An alternative to recurrent nets.
Technical Report FKI-147-91, Institut für Informatik,
Technische Universität München.
Rico Sennrich, Barry Haddow, and Alexandra Birch.
2016. Neural machine translation of rare words with
subword units. In Proc. Association for Computa-
tional Linguistics (ACL) , pages 1715–1725, Berlin,
Germany.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. In Int.
Conf. on Learning Representations (ICLR) , Toulon,
France.
Kai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui
Wang, and Jiang Bian. 2023. A study on relu and
softmax in transformer. Preprint arXiv:2302.06461 .
Richard Sinkhorn. 1964. A relationship between arbi-
trary positive matrices and doubly stochastic matri-
ces. The annals of mathematical statistics , 35(2):876–
879.
Richard Sinkhorn and Paul Knopp. 1967. Concerning
nonnegative matrices and doubly stochastic matrices.
Pacific Journal of Mathematics , 21(2):343–348.
Luca Soldaini and Kyle Lo. 2023. peS2o (Pretrain-
ing Efficiently on S2ORC) Dataset. Technical re-
port, Allen Institute for AI. https://github.com/
allenai/pes2o .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. Preprint
arXiv:2302.13971 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Proc. Advances in Neural Information
Processing Systems (NIPS) , pages 5998–6008, Long
Beach, CA, USA.

--- PAGE 13 ---
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019. Q8BERT: quantized 8bit BERT.
InWorkshop on Energy Efficient Machine Learn-
ing and Cognitive Computing - NeurIPS , Vancouver,
Canada.
Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,
Maosong Sun, and Jie Zhou. 2022. MoEfication:
Transformer feed-forward layers are mixtures of ex-
perts. In Proc. Findings of the Association for Com-
putational Linguistics (ACL) , pages 877–890, Dublin,
Ireland.
A Further details and analyses
A.1 Definition of normalised Top- K
Using the setting of Sec. 3.3, we define the normal-
ized top- Koperation as follows:
Ex= arg topk( s, K) (23)
topk(s)[i] =(
s[i]ifi∈ Ex
0 otherwise(24)
norm topk( s) =topk(s)P
itopk(s)[i](25)
A.2 Measuring the Number of Active
Channels in u
In order to explore whether a ( ki-vi) sparsity-
based approach is feasible, we measure the number
of nonzero entries in the up-projected vector uin
our baseline models (which, because of the ReLU
activation function, is the same as the positive en-
tries). We show the results of our 47M model in Fig.
1. Note that dff= 2053 (See Tab. 8) for the same
model, which means that on average only 1-10% of
the channels are active. We show the same analysis
for the 262M model in Fig. 4. Interestingly, the
counts remain the same, even though dff= 4110
for this model. The 41M parameter model on En-
wik8 shows a stark difference in the distribution
of the channels between layers; see Fig. 5. This
suggests that the key factor determining the count
distribution is the dataset, and the size of the model
plays only a secondary role. Fortunately, the spar-
sity is very high for all models considered.
A.3 More Details and Results on PKM
Our PKM (Sec. 3.2) is based on Lample et al.
(2019) with the following basic modifications.
First, we do not use batch normalization (BN). As
Lample et al. (2019) shows that BN is only benefi-
cial for models with a very large memory size, we
remove it as it simplifies inference where the effec-
tive batch size varies over time. Also, we directly
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Layer0200400Active channelsFigure 4: Number of active channels in uin our dense
262M parameter model on Wikitext-103. dff= 4110
for this model, so the sparsity is below ∼5%. Standard
deviation over all tokens of the test and validation set.
1
2
3
4
5
6
7
8
9
10
11
12
Layer0100200300Active channels
Figure 5: Number of active channels in uin our dense
41M parameter model on Enwik8. dff= 2053 for this
model, thus the sparsity is below ∼15%. Standard
deviation over all tokens of the test and validation set.
divide the input vectors into two sub-keys without
an additional projection. Finally, unlike Lample
et al. (2019), we use the same learning rate for all
parts of the network.
In addition to the parameter-equal comparison
of Sec. 6.2, there is another possibly “fair” way of
setting the size of the PKM-based model: match
the number of values (this would result in fewer pa-
rameters because of the key approximation), even
though Elhage et al. (2022) suggest that the keys
typically play a vital role, and reducing their capac-
ity will cause a performance loss. See Tab. 6 for
the corresponding results. Note that, for Enwik8
and Wikitext-103 small, the parameter-equal set-
ting increases the number of sub-keys from 46 to
62 (2116 vs. 3844 values). This helps significantly.
A.4 Further Analyses of Our σ-MoE
We also examine the best ( G,K) given a constant
number ( G·K) of active pairs ki,vi. In this setting,
reducing Kby a factor of m(K′=K
m) involves
increasing G(G′=mG), which, for a constant
number of parameters, reduces NEtoN′
E=NE
m.
The results can be seen in the 2ndblock of Tab. 10.
We find that a higher Kis beneficial. Given this, we
ask the question how the selection distribution of
the models with K > 1is different from selecting
the same experts together and acting as a larger

--- PAGE 14 ---
Table 6: The performance of the PKM model variants. Both value-count and parameter-matched variants are shown.
Additionally, we show the effect of the initialization inspired by our unified view, which is marginal for PKMs.
Variant Setting Nonlinearity WT-S WT-M E8
Dense Baseline ReLU 11.81 9.46 1.08
PKM value-count Softmax 14.11 11.29 1.20
PKM value-count ReLU 13.32 10.16 1.12
PKM # total params. Softmax 13.96 11.10 1.16
PKM # total params. ReLU 12.77 9.98 1.11
PKM + init # total params. ReLU 12.75 9.96 1.11
expert. Are these models combining experts in
more meaningful ways? To test this, we measure
the distribution of experts that are used together
on Wikitext-103 with our 47M MoE model with
K= 4. The result can be seen in Fig. 6: the
network combines experts in a rich way, further
supporting the use of K > 1. Note that, it remains
an open question whether such “compositions” may
help the generalization and compositional behavior
of the network (Fodor and Pylyshyn, 1988; Pagin
and Westerståhl, 2010; Hupkes et al., 2020).
1 5 9 13
Expert ID1
5
9
13Expert ID
0.000.050.100.150.20
Figure 6: Expert co-occurrence in a σ-MoE model with
NE= 16 experts and K= 4. Each row shows the
distribution of experts used together with the one corre-
sponding to the row. Measured on the validation set of
Wikitext-103 in the 3rdlayer of our 47M σ-MoE model.
The other layers and models behave qualitatively the
same.
Detailed Usage Count Analysis. We show the
relative proportion of experts selected for all layers
in Fig. 7. For more details, please refer to Sec. 6.3.
A.5 More on Resource Efficiency
For execution time and memory usage, both the
dense MLP and the MoE layers are linear in dmodel
(Fig. 11), the MLP is linear in dff, and MoE is
linear in G(Fig. 10) and K. For the same number
of parameters (except for the selection network,which is negligible), dmodel =G·NE. However,
both the memory usage and the execution time of
the MoE are almost independent of NE, except for
a small linear factor due to the selection network
(see Fig. 9). Figures 2, 9, 10 and 11 show the actual
measured execution time and memory usage on a
RTX 3090 GPU.
Note that there is no significant difference in
terms of speed and memory usage between differ-
ent MoE variants given the same dmodel,G, and
K. This is because they only differ in the selection
mechanism and regularization, and not in the way
the experts are executed. Since all methods are
configured to have the same number of parameters
as the dense baselines, and Kexperts are used in
parallel, the factor of reduction in both FLOPs and
memory usage is given byK
NE. We show this factor
for all models in Tab. 7.
B Implementation details
We train all of our models for 100k steps with co-
sine learning rate decay, starting from the initial
learning rate of 0.00025 and decaying to 0. We use
the Adam optimizer (Kingma and Ba, 2015) with
default PyTorch parameters (Paszke et al., 2019).
We use gradient clipping with a max gradient norm
of 0.25. We show the other hyperparameters of
our dense models in Tab. 8. We train our models
with an XL memory of the same size as the context
size. However, following Dai et al. (2019), we eval-
uate the models using a longer memory. Unlike
the hyperparameter-tuned memory sizes in Trans-
former XL, we use 4 times the context size (this
approximates the size of the memory by Dai et al.
(2019), while being simple).
The hyperparameters of the MoE models match
those of their dense counterparts with the same
number of parameters, except for the MoE-specific
ones, which are shown in Tab. 9. δdenotes the
expert dropout and γdenotes the regularization

--- PAGE 15 ---
Table 7: The relative amount of FLOPs and memory used by the feedforward block of the MoE transformer
compared to its dense counterpart. The same configurations are shown as in Tab. 10.
Dataset Wikitext 103 Wikitext 103 Wikitext 103 Enwik8
dmodel 412 412 1024 512
# params 47M 237M 262M 41M
G K
σ-MoE (ours) 128 4 25.0% 3.1% 12.5% 25.0%
standard dropout 128 4 25.0% 3.1% 12.5% 25.0%
softmax (after top-k) 128 4 25.0% 3.1% 12.5% 25.0%
softmax (before top-k) 128 4 25.0% 3.1% 12.5% 25.0%
standard init 128 4 25.0% 3.1% 12.5% 25.0%
no reg ( γ= 0, δ= 0) 128 4 25.0% 3.1% 12.5% 25.0%
K= 8, G= 64 64 8 25.0% 3.1% 12.5% 25.0%
K= 2, G= 256 256 2 25.0% 3.1% 12.5% 25.0%
K= 1, G= 512 512 1 25.0% 3.1% 12.5% 25.0%
N′
E= 2NE, G= 64 64 4 12.5% 1.6% - 12.5%
K= 1 128 1 6.2% 0.8% - 6.2%
K= 2 128 2 12.5% 1.6% - 12.5%
K= 8 128 8 50.0% 6.2% - 50.0%
Switch, K= 1, G= 512 512 1 25.0% 3.1% 12.5% 25.0%
no dropout 512 1 25.0% 3.1% 12.5% 25.0%
K= 4, G= 128 128 4 25.0% 3.1% - 25.0%
K= 1, G= 128 128 1 6.2% 0.8% - 6.2%
no dropout 128 1 6.2% 0.8% - 6.2%
S-BASE 128 4 25.0% 3.1% 12.5% 25.0%
K= 1, G= 512 512 1 25.0% 3.1% 12.5% 25.0%
strength used for the loss L(See Eq. 21). For the
non-MoE layers, the same dropout is used as for
the baselines. For Switch Transformers, we use
γ= 0.01with regularization of the form presented
in Eq. 17, following Fedus et al. (2022). The other
variants, including S-BASE, use the regularizer
proposed by us (Eq. 21).
Our small PKM models use 46 subkeys resulting
in462= 2116 values for the dff-matched case
and 62 subkeys (3844 values) for the parameter-
matched case. The PKM equivalent of the 262M
parameter model on Wikitext-103 has 64 subkeys
(4096 values) for the dff-matched and 89 subkeys
(7921 values) for the parameter-matched case. The
PKM models do not use dropout in the PKM layers,
and have 4 heads.
B.1 A Few Words on the CUDA Kernel
We call the key operation for our MoE layers con-
ditional vector-matrix multiplication, or CVMM,
and we define it as follows. Given a batch of vec-
tors,V∈RN×M, where Nis the batch size and
Mis the number of channels, a set of Kmatri-
cesM∈RK×M×Land selection indices S∈{0, ..., K −1}N,CVMM( V,S,M)∈RN×Lis:
CVMM( V,S,M)[n, l] = (26)
M−1X
m=0V[n, m]M[S[n], m, l]
Our CUDA kernel is based on the blog post
developing a matrix multiplication kernel by Si-
mon Boehm ( https://siboehm.com/articles/
22/CUDA-MMM ). However, there are major differ-
ences: unlike standard matrix multiplication, in our
case, different matrices could be used for different
batch elements of the input. In order to be able
to reuse matrices fetched from the global memory
of the GPU, we first do a preprocessing step: we
sort the selection indices, and obtain a reordering
vector. This gives us an ordering of the input and
output batch elements, such that the consecutive
indices are multiplied by the same matrix with high
probability. Fortunately, multiple channels have to
be fetched/written out at once, so this reordering
has minimal overhead. Our kernel has an addi-
tional grid dimension compared to standard ma-
trix multiplication, iterating over the matrix index,
k∈ {0, ..., K −1}. We find that skipping matrices
that do not have any corresponding inputs has min-
imal overhead. To avoid checking all elements of
the reordering vector, we precompute their offsets.

--- PAGE 16 ---
Table 8: Hyperparameters of dense baselines and their MoE counterparts. For the MoE-specific hyperparameters,
please refer to Tab. 9. “SetencePiece” tokenization is used for Wikitext-103, C4 and PES2O datasets, and “Character”
for Enwik8.
Tokenization #params dmodel dffnlayers nheads head size context size batch size dropout lr warmup
SentencePiece 47M 412 2053 16 10 41 256 64 0.1 -
SentencePiece 238M 412 16480 16 10 41 256 64 0.1 -
SentencePiece 262M 1024 4110 18 16 64 512 64 0.2 4000
Character 41M 512 2053 12 8 64 512 32 0.1 -
Table 9: MoE-specific hyperparameters for different model variants. γdenotes the scaler for the load balancing
term in the loss and δis the probability of the expert dropout. The standard, transformer-specific hyperparameters
are the same as for the baselines. Please refer to Tab. 8. “SetencePiece” tokenization is used for Wikitext-103, C4
and PES2O datasets, and “Character” for Enwik8.
Tokenization #params dmodel NE G K δ γ
SentencePiece 47M 412 16 128 4 - 0.001
SentencePiece 237M 412 128 128 4 0.05 0.001
SentencePiece 262M 1024 32 128 4 0.2 0.001
Character 41M 512 16 128 4 0.05 0.0001
Our kernel uses shared memory and register
caching; however, it does not use asynchronous
loads, which makes it I/O bound. It also does not
support tensor cores and mixed precision. The
pre-processing step uses the radix sort from the
CUB library. However, computing the offsets re-
quires counting the number of vectors assigned to
a single matrix. This information, as well as the
offset, which is their sum, are freely available as
sub-results that the radix sort computes anyways;
however, we found no way of extracting it from
the CUB implementation. We estimate that by
implementing a more efficient preprocessing step,
asynchronous loads, and tensor core support, our
kernel can be further accelerated by a factor of two.
Triton kernel. We added a new Triton-based im-
plementation that supports mixed precision. How-
ever, due to Triton limitations, it only supports
V olta or newer GPU generations. For more details,
seehttps://github.com/robertcsordas/moe_
layer .
B.2 Additional Results on MoEs
Additional results of different MoE variants with
more model details are shown in Tab. 10. We repeat
the entries from Tab. 4 for easier comparison.

--- PAGE 17 ---
Table 10: Detailed ablation results. WT-S* is obtained by naively scaling NEin WT-S. More details in Sec. 6.3. We
do not evaluate all versions of the 262M Wikitext-103 model due to its long training time. However, we aim to
include what we believe are the most interesting variants. γ= 0means no regularization applied to the selection
scores (See Eq. 21), δ= 0denotes no expert dropout.
Variant WT-S WT-S* WT-B E8
dmodel 412 412 1024 512
# params 47M 237M 262M 41M
G K
σ-MoE (ours) 128 4 11.59 10.37 9.44 1.08
standard dropout 128 4 12.01 10.27 9.53 1.08
softmax (after top-k) 128 4 11.89 11.27 9.58 1.09
softmax (before top-k) 128 4 12.05 10.54 9.62 1.09
standard init 128 4 11.80 10.59 9.67 1.08
no reg ( γ= 0, δ= 0) 128 4 11.83 10.41 9.51 1.08
K= 8, G= 64 64 8 11.63 10.30 9.58 1.08
K= 2, G= 256 256 2 11.84 10.44 9.56 1.09
K= 1, G= 512 512 1 11.90 10.83 9.58 1.09
N′
E= 2NE, G= 64 64 4 11.81 10.53 - 1.08
K= 1 128 1 12.26 11.30 - 1.09
K= 2 128 2 11.90 10.66 - 1.09
K= 8 128 8 11.58 10.22 - 1.08
Switch, K= 1, G= 512 512 1 12.27 11.24 9.68 1.08
no dropout 512 1 11.88 11.10 9.77 1.10
K= 4, G= 128 128 4 12.05 11.37 - 1.10
K= 1, G= 128 128 1 12.61 11.89 - 1.11
no dropout 128 1 12.35 11.78 - 1.10
S-BASE, K= 4,G= 128 128 4 13.01 10.96 10.50 1.17
K= 1, G= 512 512 1 12.32 11.31 9.77 1.32

--- PAGE 18 ---
0 16 32 48 64 80 96 11210−610−510−410−310−210−1Selection proportionLayer 0
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−610−510−410−310−210−1Layer 1
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−510−410−310−210−1Selection proportionLayer 2
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−710−610−510−410−310−210−1Layer 3
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−510−410−310−210−1Selection proportionLayer 4
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−710−610−510−410−310−210−1Layer 5
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−510−410−310−210−1Selection proportionLayer 6
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−610−510−410−310−210−1Layer 7
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−710−510−310−1Selection proportionLayer 8
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−610−510−410−310−210−1Layer 9
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−510−410−310−210−1Selection proportionLayer 10
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−510−410−310−210−1Layer 11
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−710−610−510−410−310−210−1Selection proportionLayer 12
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 11210−510−410−310−210−1Layer 13
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 112
Expert10−510−410−310−210−1Selection proportionLayer 14
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)
0 16 32 48 64 80 96 112
Expert10−610−510−410−310−210−1Layer 15
σ-MoE
σ-MoE - softmax (no renorm.)
σ-MoE - softmax (renorm.)
Switch Transformer
S-BASE (K=4, G=128)Figure 7: Total proportions of selection weights assigned to a given expert (x-axis) on the validation set of Wikitext-
103 using the WT-S* models from Tab. 4. Experts are sorted by their popularity. All layers are shown (“Layer 5”
is also shown in Fig. 3 in the main text). The models with poor performance can be distinguished easily (Switch
Transformer and σ-MoE with a softmax and renormalization, “softmax (renom.)”). Their poor performance may be
partially explained by expert collapse. In contrast, the fine performance differences between the rest of the models
do not seem to be due to the expert collapse phenomenon.

--- PAGE 19 ---
256 1024 2048 4096
dmodel0100200300Time (ms)MLP
MoE
0.00.51.0
Memory (GB)MLP
MoEFigure 8: Execution time and memory usage of a
forward-backward pass of a single MLP and MoE layer.
|B|= 32768 , corresponding to a batch size 64 and
sequence length 512, dmodel indicated on the x-axis,
K= 4,NE= 32 ,dff= 4dmodel, and G=dff
NE(fol-
lowing the standard 1x-4x-1x shape). Full/dashed lines
show the execution time/memory consumption, respec-
tively. Shows our mixed-precision Triton implementa-
tion on an RTX 3090 with PyTorch 2.1 and CUDA 12.
83264 128 256
Number of experts ( NE)050100Time (ms)MLP
MoE
012
Memory (GB)MLP
MoE
Figure 9: Execution time and memory usage of a
forward-backward pass of a single MLP and MoE layer.
|B|= 32768 , corresponding to a batch size 64 and
sequence length 512, dmodel = 512 ,K= 4 , and
dff=G·NE. Full/dashed lines show the execution
time/memory consumption, respectively. As they are
both linear with similar slopes, they are almost indistin-
guishable. Shows our mixed-precision Triton implemen-
tation on an RTX 3090 with PyTorch 2.1 and CUDA 12.
64 256 512 1024
Expert size ( G)50100Time (ms)MLP
MoE
012
Memory (GB)MLP
MoE
Figure 10: Measured execution time and memory us-
age of a forward-backward pass of a single MLP and
MoE layer. |B|= 32768 , corresponding to the realistic
scenario of a batch size 64 and sequence length 512,
dmodel = 512 ,K= 4,NE= 32 anddff=G·NE.
Full lines show the execution time, and dashed ones
the memory consumption. Because they are both linear
with similar slopes, they are almost indistinguishable.
Shows our mixed-precision Triton implementation on
an RTX 3090 with PyTorch 2.1 and CUDA 12.
128 512 1024 2048
dmodel2040Time (ms)MLP
MoE
0.10.20.3
Memory (GB)MLP
MoEFigure 11: Measured execution time and memory us-
age of a forward-backward pass of a single MLP and
MoE layer. |B|= 32768 , corresponding to the real-
istic scenario of a batch size 64 and sequence length
512,K= 4,NE= 32 ,G= 128 anddff=G·NE.
Full lines show the execution time, and dashed ones
the memory consumption. Shows our mixed-precision
Triton implementation on an RTX 3090 with PyTorch
2.1 and CUDA 12.
