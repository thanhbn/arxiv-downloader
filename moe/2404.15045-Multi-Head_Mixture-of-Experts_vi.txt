# 2404.15045.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2404.15045.pdf
# Kích thước tệp: 8806745 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hỗn hợp Chuyên gia Đa Đầu
Xun Wu1 2Shaohan Huang2Wenhui Wang2Furu Wei2

Tóm tắt
Hỗn hợp Chuyên gia Thưa (SMoE) mở rộng quy mô dung lượng mô hình mà không tăng đáng kể chi phí huấn luyện và suy luận. Tuy nhiên, nó có hai vấn đề: (1) Kích hoạt chuyên gia thấp, trong đó chỉ một tập hợp con nhỏ các chuyên gia được kích hoạt để tối ưu hóa, dẫn đến hiệu suất không tối ưu và hạn chế hiệu quả trong việc học một số lượng lớn chuyên gia trong các tác vụ phức tạp. (2) Thiếu khả năng phân tích chi tiết cho nhiều khái niệm ngữ nghĩa trong từng token riêng lẻ. Trong bài báo này, chúng tôi đề xuất Hỗn hợp Chuyên gia Đa Đầu (MH-MoE). MH-MoE sử dụng cơ chế đa đầu để tách mỗi token đầu vào thành nhiều token con. Sau đó, các token con này được gán cho và xử lý bởi một tập hợp đa dạng các chuyên gia song song, và được tích hợp liền mạch trở lại dạng token gốc. Các hoạt động trên cho phép MH-MoE tập thể tập trung vào thông tin từ các không gian biểu diễn khác nhau trong các chuyên gia khác nhau để làm sâu sắc hơn hiểu biết về ngữ cảnh đồng thời tăng cường đáng kể kích hoạt chuyên gia. Đáng chú ý là MH-MoE của chúng tôi dễ dàng triển khai và tách rời khỏi các khung SMoE khác, khiến nó dễ tích hợp với các khung này để nâng cao hiệu suất. Kết quả thực nghiệm rộng rãi trên ba tác vụ: Mô hình hóa ngôn ngữ tập trung tiếng Anh, Mô hình hóa ngôn ngữ đa ngôn ngữ và tác vụ Mô hình hóa đa phương thức có mặt nạ, chứng minh tính hiệu quả của MH-MoE. Mã nguồn của chúng tôi có sẵn tại https://github.com/yushuiwx/MH-MoE.

1. Giới thiệu
Các mô hình dung lượng lớn, chẳng hạn như Mô hình Ngôn ngữ Lớn (LLM) (Zhao et al., 2023; Pham et al., 2023; Chung et al., 2022; OpenAI, 2023) và Mô hình Đa phương thức Lớn (LMM) (Wang et al., 2022; Peng et al., 2023), đã chứng minh hiệu quả của chúng trên các lĩnh vực và tác vụ khác nhau. Để tiếp tục nâng cao hiệu suất, một phương pháp đáng tin cậy liên quan đến việc mở rộng quy mô các mô hình này bằng cách tăng số lượng tham số (Fedus et al., 2022). Nhưng đối với hầu hết các mô hình dung lượng lớn được kích hoạt dày đặc này (được gọi là mô hình Dày đặc), sử dụng tất cả tham số của chúng để xử lý tất cả đầu vào, kích thước cực lớn của các mô hình này làm giảm đáng kể tốc độ suy luận, hạn chế thêm tính thực tiễn của chúng.

Một thay thế đầy hứa hẹn, tạo điều kiện cho khả năng mở rộng mô hình trong khi giảm thiểu chi phí tính toán nặng nề, nằm trong Hỗn hợp Chuyên gia Thưa (SMoE) (Shazeer et al., 2017b; Du et al., 2021; Chi et al., 2022; Clark et al., 2022).

--- TRANG 2 ---
Hỗn hợp Chuyên gia Đa Đầu

Phân công MH-MoE, Phân công SMoE, Chuyên gia3

Tiếng Anh:
trong chuyến safari động vật hoang dã, tôi đã sử dụng máy ảnh độ phân giải cao để chụp những bức ảnh tuyệt đẹp của sư tử, voi và hươu cao cổ trong môi trường sống tự nhiên của chúng, tạo ra những kỷ niệm khó quên về cuộc phiêu lưu.

Tiếng Ý:
dopo una lunga giornata di lavoro, non vedo l'ora di rilassarmi nella mia camera (có nghĩa là phòng ngủ) da letto, che ho decorato con colori caldi e tessuti morbidi per creare un ambiente accogliente e tranquillo.

camera, Đầu vào, Tokenization & Embedding, Dữ liệu Thị giác, Dữ liệu Ngôn ngữ, Router α, Embeddings h, Token Phân công cho Chuyên gia

hoặc

Có ba người.
(Thiếu quá nhiều chi tiết)

Camera có nghĩa là, Lớp Đa đầu, Router α, h, Chuyên gia3, Chuyên gia2, Chuyên gia1, Chuyên gia4, Trong tiếng Ý, nó có nghĩa là, Trong tiếng Anh, Nó có nghĩa là

Anh ấy ấn tay vào ngực, nói một cách tha thiết về phía bên trái.

Anh ấy giơ một ngón tay, như thể để đặt câu hỏi. h, Lớp Hợp nhất, h

Hình 2. Quy trình làm việc cho MH-MoE trên cả dữ liệu thị giác và ngôn ngữ. Đối với dữ liệu thị giác, các đầu khác nhau được định tuyến đến các chuyên gia khác nhau cố gắng nắm bắt các khía cạnh khác nhau của chi tiết trong các patch và mối quan hệ giữa các patch. Đối với dữ liệu ngôn ngữ, các đầu khác nhau tập trung nắm bắt các bối cảnh khác nhau của từ đồng âm khác nghĩa giữa các ngôn ngữ khác nhau (ví dụ: tiếng Ý và tiếng Anh) hoặc từ đa nghĩa trong một ngôn ngữ duy nhất.

Trái ngược với mô hình Dày đặc, SMoE chứa các mạng nơ-ron truyền thẳng song song (được gọi là chuyên gia) trong mỗi khối xây dựng, và kích hoạt có chiến lược các chuyên gia riêng biệt cho các token đầu vào cụ thể thông qua một bộ định tuyến, từ đó mang lại những cải tiến hiệu quả đáng chú ý. GShard (Lepikhin et al., 2020) mở rộng quy mô mô hình Dày đặc từ 2B lên 600B tham số với chi phí huấn luyện thấp hơn so với mô hình Dày đặc 100B. Và gần đây, Mixtral 8×7B (Jiang et al., 2024), một mô hình SMoE chứa 8 chuyên gia (tổng cộng 7B tham số) được cho là vượt trội hoặc sánh ngang với LLaMA-2 70B (Touvron et al., 2023) và GPT-3.5.

Mặc dù thành công, SMoE có một số nhược điểm: (1) Kích hoạt chuyên gia thấp, có nghĩa là chỉ một tập hợp con nhỏ các chuyên gia được kích hoạt trong quá trình tối ưu hóa và suy luận, ví dụ, tỷ lệ kích hoạt 8,33% được hiển thị trong Hình 1 (a), trong khi phần lớn chúng không được sử dụng chút nào (xem vùng tối). Kết quả là, SMoE không thể sử dụng toàn bộ sức mạnh biểu đạt của các chuyên gia này, đặc biệt là khi số lượng chuyên gia lớn, điều này hạn chế đáng kể hiệu quả và khả năng mở rộng của SMoE. (2) Thiếu khả năng phân tích chi tiết. Các mẫu tokenization hiện tại áp đặt giới hạn đối với khả năng của mô hình để nắm bắt nhiều diễn giải ngữ nghĩa liên quan đến các token riêng lẻ. Trong bối cảnh dữ liệu thị giác, việc chia hình ảnh thành các patch để tokenization có thể bỏ qua các chi tiết hình ảnh tinh hơn khi sử dụng các patch lớn hơn hoặc làm tăng yêu cầu tính toán khi sử dụng các patch nhỏ hơn. Đối với dữ liệu ngôn ngữ, việc tokenization của từ đồng âm khác nghĩa giữa các ngôn ngữ khác nhau hoặc từ đa nghĩa trong một ngôn ngữ duy nhất dẫn đến chúng được biểu diễn bằng các token giống nhau, mặc dù mang ý nghĩa khác biệt. Điều này có thể dẫn đến nhầm lẫn trong các mô hình.

Để giải quyết các vấn đề trên, chúng tôi đề xuất Hỗn hợp Chuyên gia Đa Đầu (MH-MoE). Quy trình làm việc của MH-MoE được minh họa trong Hình 2. Bằng cách sử dụng cơ chế đa đầu để tách mỗi token đầu vào thành nhiều token con và phân phối chúng đến các chuyên gia khác nhau, MH-MoE đạt được kích hoạt chuyên gia dày đặc hơn mà không tăng độ phức tạp tính toán và tham số. Cụ thể, như được hiển thị trong Hình 2, khi được cung cấp một token đầu vào duy nhất, MH-MoE kích hoạt bốn chuyên gia bằng cách tách nó thành bốn token con, trong khi SMoE chỉ kích hoạt một chuyên gia. Hơn nữa, việc phân bổ các token con cho các chuyên gia riêng biệt cho phép mô hình tập trung đồng thời vào thông tin từ các không gian biểu diễn khác nhau trong các chuyên gia khác nhau, đảm bảo hiểu biết chi tiết hơn về sự khác biệt tinh tế trong cả mẫu thị giác và ngôn ngữ. Xem trong Hình 2, các token con được gán cho Chuyên gia 3 và 2 nắm bắt sự hiểu biết chi tiết về hành động của mỗi nhân vật trong một patch hình ảnh, trong khi những token được gán cho Chuyên gia 1 và 4 mô hình hóa rõ ràng ngữ nghĩa của từ đồng âm khác nghĩa 'camera'. Sau khi xử lý chuyên gia, các token con được tích hợp liền mạch trở lại dạng token gốc, từ đó tránh được bất kỳ gánh nặng tính toán bổ sung nào trong các lớp không song song tiếp theo, ví dụ, lớp attention, đồng thời cũng tích hợp thông tin ngữ nghĩa được nắm bắt từ nhiều chuyên gia.

MH-MoE duy trì các điểm mạnh sau: (1) Kích hoạt chuyên gia cao hơn và khả năng mở rộng tốt hơn. MH-MoE có thể giảm bớt vấn đề kích hoạt chuyên gia thấp hơn và tăng cường đáng kể việc sử dụng các chuyên gia lớn hơn bằng cách cho phép tối ưu hóa hầu như tất cả các chuyên gia, ví dụ, đạt được kích hoạt 90,71% trong Hình 1 (a), cho phép mở rộng quy mô dung lượng mô hình hiệu quả hơn. (2) Khả năng hiểu biết chi tiết hơn. Cơ chế đa đầu được áp dụng trong MH-MoE gán các token con cho các chuyên gia khác nhau, cho phép cùng tập trung vào thông tin từ các không gian biểu diễn khác nhau tại các chuyên gia khác nhau, và cuối cùng đạt được khả năng hiểu biết chi tiết tốt hơn. Ví dụ, tham khảo vùng sáng trong Hình 1 (b), nơi các token con được phân phối giữa một tập hợp đa dạng hơn các chuyên gia, tạo điều kiện cho việc nắm bắt thông tin giàu ngữ nghĩa. (3) Tích hợp liền mạch. Việc triển khai MH-MoE cực kỳ đơn giản và tách rời khỏi các phương pháp tối ưu hóa SMoE khác (ví dụ, GShard (Lepikhin et al., 2020)), khiến nó rất dễ tích hợp chúng lại với nhau để đạt được hiệu suất tốt hơn.

Chúng tôi đánh giá MH-MoE được đề xuất trên ba thiết lập huấn luyện trước và tinh chỉnh mô hình: Mô hình hóa ngôn ngữ tập trung tiếng Anh, Mô hình hóa ngôn ngữ đa ngôn ngữ và Mô hình hóa đa phương thức có mặt nạ. Thực nghiệm rộng rãi trên ba tác vụ này chứng minh tính hiệu quả của MH-MoE.

2. Nền tảng
Hỗn hợp Chuyên gia Thưa. Hỗn hợp Chuyên gia Thưa (SMoE) (Shazeer et al., 2017b; Du et al., 2021; Chi et al., 2022; Clark et al., 2022) nâng cao dung lượng mô hình trong khi duy trì nhu cầu tính toán không đổi, do đó đạt được hiệu suất tốt hơn so với các mô hình được kích hoạt dày đặc trên các tác vụ khác nhau (Lepikhin et al., 2021; Kumatani et al., 2021; Zhao et al., 2023; Pham et al., 2023) và nổi lên như một tiến bộ then chốt trong lĩnh vực học sâu.

Khác với các mô hình được kích hoạt dày đặc, mỗi lớp MoE bao gồm N mạng Truyền thẳng độc lập (FFN) {f^FFN_i}^N_{i=0} làm chuyên gia, cùng với một hàm cổng g(·) để mô hình hóa phân phối xác suất cho thấy trọng số trên đầu ra của các chuyên gia này. Đối với biểu diễn ẩn h∈R^d của mỗi token đầu vào, giá trị cổng của việc định tuyến h đến chuyên gia f^FFN_i được ký hiệu là:

g^{f^FFN_i} = exp(h·e_i) / Σ^N_{j=0} exp(h·e_j), (1)

trong đó e_i ký hiệu embedding có thể huấn luyện của chuyên gia thứ i và Σ^N_{i=0} g^{f^FFN_i} = 1. Sau đó, k chuyên gia tương ứng, theo các giá trị cổng top-k, được kích hoạt và đầu ra O của lớp MoE là

O = h + Σ_{i∈Φ} g^{f^FFN_i} · f^FFN_i(h). (2)

trong đó Φ ký hiệu tập hợp chuyên gia được kích hoạt và |Φ| = k.

Cơ chế Định tuyến trong SMoE. Như mô tả ở trên, cơ chế định tuyến được sử dụng phổ biến nhất liên quan đến việc chọn k chuyên gia hàng đầu từ N chuyên gia, trong đó k ≪ N (Shazeer et al., 2017a), ví dụ, k = 2 và N = 2048 trong GShard (Lepikhin et al., 2020). Cơ chế định tuyến như vậy cho phép kết hợp song song hóa dữ liệu và song song hóa chuyên gia. Yang et al. (2021) và Lepikhin et al. (2020) gợi ý rằng các giá trị k lớn hơn thường góp phần vào hiệu suất mô hình tốt hơn. Tuy nhiên, với sự gia tăng giá trị k, việc huấn luyện các mô hình với triển khai định tuyến top-k thông thường trở nên kém hiệu quả hơn nhiều (Lepikhin et al., 2020).

Trong bài báo này, chúng tôi giới thiệu MH-MoE, một cách đơn giản nhưng hiệu quả để làm cho kích hoạt chuyên gia dày đặc hơn mà không tăng độ phức tạp tính toán.

3. Phương pháp
Kiến trúc đầy đủ của MH-MoE có thể được thấy trong Hình 3, MH-MoE giải quyết vấn đề kích hoạt chuyên gia thấp và nhầm lẫn về sự mơ hồ của các token bằng cách áp dụng cơ chế đa đầu để tách mỗi token thành các token con và định tuyến chúng đến các chuyên gia khác nhau để đạt được kích hoạt chuyên gia dày đặc hơn cũng như hiểu biết sâu hơn.

3.1. Hỗn hợp Chuyên gia Đa Đầu
Cụ thể, chúng tôi ký hiệu một chuỗi token đầu vào bằng X∈R^{l×d}, trong đó l là số lượng token và d biểu thị chiều dài của chiều token. Trong MH-MoE, mỗi lớp song song chứa một tập hợp N chuyên gia, mỗi chuyên gia được trình bày dưới dạng {f^FFN_i: R^{d_h} → R^{d_h}}^N_{i=0}, h ký hiệu số lượng đầu trong cơ chế đa đầu, được tách rời khỏi đầu trong lớp tự chú ý đa đầu. Để rõ ràng, chúng tôi chỉ mô tả hoạt động của một lớp MH-MoE duy nhất ở đây.

Đầu tiên, X được chiếu bởi một lớp đa đầu với ma trận tham số W_head ∈ R^{d×d},

X̂ = X · W^T_head (3)

trong đó X̂ ∈ R^{l×d}. Sau đó, mỗi token trong X̂ được tách thành h token con dọc theo chiều token, và các token con này được sắp xếp song song theo chuỗi token gốc, tạo thành một không gian đặc trưng mới Ẍ ∈ R^{(l×h)×d_h} như:

Ẍ = Sₛ(X̂)
= x⁰₀, ..., x⁰_{h-1}, ..., xⁱⱼ, xⁱⱼ₊₁, ..., xˡ_{h-1}, (4)

trong đó hàm Sₛ ký hiệu hoạt động tách token: R^{l×d} → R^{(l×h)×d_h}, và mỗi token con được trình bày dưới dạng xⁱⱼ ∈ R^{d_h}, có nghĩa là nó là token con thứ j được tách từ token thứ i.

Sau đó, tất cả các token con này được đưa vào hàm cổng g(·). Giá trị cổng của việc định tuyến một token con cụ thể xⁱⱼ vào chuyên gia thứ p được tính như

g^{f^FFN_p} = exp(xⁱⱼ · eₚ) / Σ^N_{ξ=0} exp(xⁱⱼ · eξ), (5)

trong đó eₚ ∈ R^{d_h} là embedding có thể học của chuyên gia thứ p. Trong bài báo này, chúng tôi tập trung chủ yếu vào định tuyến top-k, tức là chỉ các chuyên gia có điểm định tuyến top-k lớn nhất được kích hoạt. Φ = Top_k(g^{f^FFN}) ký hiệu tập hợp các chuyên gia được kích hoạt và |Φ| = k. Sau đó xⁱⱼ được xử lý bởi các chuyên gia được kích hoạt này như sau,

oⁱⱼ = xⁱⱼ + Σ_{p∈Φ} g^{f^FFN_p} · f^FFN_p(xⁱⱼ). (6)

Sau đó, tất cả oⁱⱼ thu được được sắp xếp lại theo thứ tự gốc của các token con và được tích hợp lại với nhau như

O = o⁰₀, ..., o⁰_{h-1}, ..., oⁱⱼ, oⁱⱼ₊₁, ..., oˡ_{h-1}, (7)

trong đó O ∈ R^{(l×h)×d_h}. Sau đó, O được chuyển đổi trở lại dạng token gốc bằng hoạt động hợp nhất token Sₘ: R^{(l×h)×d_h} → R^{l×d}:

X̄ = Sₘ(O)ᵀ, (8)

trong đó X̄ ∈ R^{l×d}. Cuối cùng, X̄ được chiếu bởi một lớp hợp nhất với ma trận tham số W_merge ∈ R^{d×d} để tích hợp hiệu quả nhiều đặc trưng oⁱⱼ nắm bắt thông tin chi tiết từ các không gian biểu diễn chuyên gia khác nhau. Hoạt động được trình bày như sau:

Ẍ = X̄ · W^T_merge. (9)

Sau đó chúng ta nhận được đầu ra cuối cùng Ẍ của lớp MH-MoE duy nhất.

Chúng tôi gọi hoạt động tách token (Phương trình 4) và hoạt động hợp nhất token (Phương trình 8) cùng nhau là hoạt động Tách-Hợp nhất Token (TSM). Bằng cách thực hiện các hoạt động nói trên, chúng tôi đã tăng hiệu quả khối lượng trung bình dữ liệu được định tuyến đến một chuyên gia cụ thể với hệ số h, như được chứng minh trong Phương trình 4. Do đó, thành tích này đã dẫn đến kích hoạt chuyên gia dày đặc hơn. Hơn nữa, việc phân bổ các token con cho các chuyên gia riêng biệt trong MH-MoE cho phép chúng tôi tập thể nắm bắt thông tin ngữ nghĩa từ các không gian đặc trưng đa dạng trên các chuyên gia này, từ đó nâng cao khả năng của mô hình để đạt được hiểu biết chi tiết hơn.

Các hoạt động được đề cập ở trên đảm bảo rằng hình dạng của đầu vào và đầu ra trong lớp MH-MoE không thay đổi. Do đó, không có chi phí tính toán bổ sung nào được giới thiệu trong khối tiếp theo. Cụ thể, chúng tôi giới thiệu một siêu tham số β để chia tỷ lệ các chiều trong của mỗi chuyên gia, nhằm cân bằng các tham số được giới thiệu bởi lớp đa đầu và lớp hợp nhất, căn chỉnh các tham số và độ phức tạp tính toán của mô hình với SMoE gốc.

Như mã giả phong cách Pytorch của MH-MoE được hiển thị trong Phụ lục E, MH-MoE được đặc trưng bởi tính đơn giản tổng thể của việc triển khai, đòi hỏi sự sửa đổi tối thiểu đối với việc triển khai SMoE. Ngoài ra, nó được tách rời khỏi các chiến lược tối ưu hóa SMoE khác (Lepikhin et al., 2020; Chi et al., 2022), từ đó tạo điều kiện cho việc tích hợp thuận tiện với các khung SMoE được tối ưu hóa khác để nâng cao hiệu suất.

3.2. Mục tiêu Huấn luyện
Mục tiêu huấn luyện của MH-MoE liên quan đến việc giảm thiểu đồng thời cả tổn thất liên quan đến tác vụ đích và tổn thất cân bằng tải phụ trợ.

Tổn thất cân bằng tải. Như mô tả trong Phần 2, thường có vấn đề mất cân bằng tải chuyên gia (Xie et al., 2023; Lepikhin et al., 2020). Vì vậy, theo (Lepikhin et al., 2020; Fedus et al., 2022), cho tập hợp token con Ẍ (được mô tả trong Phương trình 4) và tần suất tₚ của việc bao nhiêu token con được định tuyến đến chuyên gia thứ p, chúng tôi tính toán tổn thất cân bằng tải L_balance thông qua:

L_balance = N/|Ẍ| · Σ^N_{p=1} Σ_{x^i_j∈Ẍ} tₚ · g^{f^FFN_p}, (10)

trong đó N ký hiệu số lượng chuyên gia, |Ẍ| là số lượng token con chứa trong Ẍ. g^{f^FFN_p} là hàm cổng được mô tả trong Phương trình 5, ký hiệu giá trị cổng của việc định tuyến một token con cụ thể x^i_j vào chuyên gia thứ p.

Tổn thất cụ thể của tác vụ. Thuật ngữ L_task phụ thuộc vào tác vụ cụ thể mà MH-MoE được thiết kế để học. Chẳng hạn, trong quá trình huấn luyện trước, chúng tôi sử dụng tổn thất mô hình hóa ngôn ngữ (Radford et al., 2018), trong khi mô hình dự đoán từ tiếp theo trong một chuỗi.

Vì vậy, mục tiêu huấn luyện tổng thể là tối thiểu hóa:

L = L_task + αL_balance, (11)

trong đó α là hệ số cho cân bằng tải.

4. Thí nghiệm
4.1. Thiết lập Thí nghiệm
Các đường cơ sở so sánh. Chúng tôi bao gồm hai mô hình đường cơ sở để so sánh: (1) Dense, đại diện cho bộ giải mã Transformer mà không có sự kết hợp của các mô-đun song song được kích hoạt thưa thớt (tức là lớp SMoE). (2) X-MoE, là việc triển khai của chúng tôi dựa trên phương pháp được đề xuất bởi Chi et al. (2022). Chúng tôi xây dựng MH-MoE của mình dựa trên X-MoE và sử dụng các thiết lập giống hệt với những thiết lập được sử dụng trong X-MoE. Lưu ý rằng tất cả các mô hình này được huấn luyện trước bằng cùng dữ liệu huấn luyện với MH-MoE, và chúng tôi đảm bảo rằng số lượng tham số của mô hình chúng tôi vẫn nhất quán với hoặc thấp hơn so với X-MoE, đảm bảo so sánh công bằng và công bằng. Phân tích và so sánh chi tiết về độ phức tạp tham số và tính toán có thể được tìm thấy trong Phần 5.3 và Bảng 11.

Dữ liệu huấn luyện trước. Chúng tôi chi tiết dữ liệu huấn luyện trước của MH-MoE, chứng minh hiệu quả của nó trong việc cho phép kích hoạt chuyên gia dày đặc hơn và hiểu biết chi tiết hơn thông qua một loạt thí nghiệm. Các thí nghiệm này được tổ chức thành ba danh mục chủ đề: (1) Đối với các thí nghiệm tập trung tiếng Anh, chúng tôi huấn luyện trước cả các mô hình đường cơ sở và MH-MoE trên bộ dữ liệu RedPajama (Computer, 2023), đây là bộ dữ liệu huấn luyện trước nguồn mở bao gồm các nguồn như Common Crawl, C4 (Raffel et al., 2020), Wikipedia và các bộ dữ liệu được tuyển chọn bổ sung. Việc huấn luyện trước được thực hiện bằng các tác vụ GPT để dự đoán từ tiếp theo trong một chuỗi. (2) Trong bối cảnh đại diện đa ngôn ngữ, chúng tôi huấn luyện trước các mô hình đường cơ sở và MH-MoE trên Wikipedia đa ngôn ngữ, theo phương pháp được mô tả trong XLM (Lample & Conneau, 2019), một lần nữa sử dụng các tác vụ GPT. (3) Đối với lĩnh vực đa phương thức, chúng tôi huấn luyện trước tất cả các đường cơ sở được so sánh và MH-MoE trên tác vụ mô hình hóa đa phương thức có mặt nạ trên cả dữ liệu đơn phương thức và đa phương thức (14M hình ảnh, 160GB tài liệu và 21M cặp hình ảnh-văn bản theo Wang et al. (2022)), và chúng tôi trình bày chi tiết về các dữ liệu huấn luyện trước này trong Phụ lục A.

Kiến trúc mô hình và siêu tham số. Đối với tất cả các thí nghiệm, chúng tôi sử dụng X-MoE Chi et al. (2022) làm kiến trúc backbone để xây dựng MH-MoE của chúng tôi, đã cho thấy hiệu suất tốt hơn so với các mô hình SMoE trước đó như Switch Transformers (Fedus et al., 2022) trên các điểm chuẩn hiểu biết đa ngôn ngữ. Đối với Mô hình hóa Ngôn ngữ tập trung tiếng Anh và Mô hình hóa Ngôn ngữ Đa ngôn ngữ, chúng tôi xây dựng Dense, X-MoE và MH-MoE bằng cách sử dụng bộ giải mã Transformer (Vaswani et al., 2017) (L = 12, H = 768, A = 12) với từ vựng GPT-4 làm kiến trúc backbone. Quy trình huấn luyện trước mất 14 ngày trên 2 Trạm NVIDIA DGX-2. Đối với Mô hình hóa Đa phương thức có Mặt nạ, chúng tôi xây dựng Dense, X-MoE và MH-MoE theo cùng kiến trúc mã hóa Transformer như BEiT v3 (Wang et al., 2022). Quy trình huấn luyện trước mất 4 ngày trên 2 Trạm NVIDIA DGX-2. Đối với tất cả ba tác vụ huấn luyện trước, chúng tôi đặt số lượng đầu h = 4. Thêm chi tiết về kiến trúc và siêu tham số huấn luyện có thể được tìm thấy trong Phụ lục B và C.

4.2. Đánh giá Perplexity
Chúng tôi kiểm tra các đường cong perplexity xác thực cho tất cả các mô hình được huấn luyện trước và các tác vụ huấn luyện trước dưới hai thiết lập chuyên gia (8 chuyên gia và 32 chuyên gia). Các xu hướng perplexity được mô tả trong Hình 4, với các giá trị perplexity cuối cùng được liệt kê trong Bảng 1. Chúng tôi có thể quan sát thấy rằng khi quá trình huấn luyện tiến triển: 1) perplexity của MH-MoE của chúng tôi vẫn thấp hơn so với các đường cơ sở được so sánh, cho thấy việc học hiệu quả hơn; 2) MH-MoE đạt được perplexity thấp nhất trên ba thiết lập thí nghiệm khác biệt; 3) sự gia tăng số lượng chuyên gia dẫn đến sự giảm tương ứng trong perplexity của MH-MoE, cho thấy rằng mô hình có lợi từ khả năng học biểu diễn nâng cao khi có thêm nhiều chuyên gia được kết hợp. Những kết quả này tập thể chứng minh sự vượt trội của MH-MoE về hiệu quả học tập và biểu diễn ngôn ngữ trên nhiều mô hình huấn luyện trước.

4.3. Đánh giá Downstream
Đối với mỗi tác vụ huấn luyện trước, chúng tôi thực hiện đánh giá downstream tương ứng để xác thực hiệu quả của MH-MoE.

Mô hình hóa Ngôn ngữ tập trung tiếng Anh. Chúng tôi đánh giá các mô hình của chúng tôi trên tổng cộng 9 điểm chuẩn zero-shot khác nhau để đánh giá khả năng của chúng trên các tác vụ ngôn ngữ tự nhiên khác nhau như lý luận thông thường, hiểu biết ngôn ngữ tổng quát và hiểu biết kiến thức sử dụng LLM Evaluation Harness (Gao et al., 2023). Như được hiển thị trong Bảng 2, so sánh X-MoE với mô hình Dense, X-MoE cho thấy cải thiện đáng chú ý, cho thấy rằng các mô hình SMoE (ví dụ, X-MoE) có lợi từ dung lượng mô hình lớn. Nhìn chung, đối với tất cả các điểm chuẩn, MH-MoE của chúng tôi đạt được hiệu suất tốt nhất, đạt được mức tăng hiệu suất trung bình 1,1 cho thiết lập 8 chuyên gia và 1,5 cho thiết lập 32 chuyên gia so với X-MoE, chứng minh hiệu quả của cơ chế đa đầu được đề xuất của chúng tôi trong việc mô hình hóa ngôn ngữ tập trung tiếng Anh.

Mô hình hóa Ngôn ngữ Đa ngôn ngữ. Chúng tôi đánh giá các mô hình ngôn ngữ đa ngôn ngữ của chúng tôi trên corpus suy luận ngôn ngữ tự nhiên đa ngôn ngữ (XNLI) (Conneau et al., 2018), đây là phần mở rộng của corpus suy luận đa thể loại NLI (MultiNLI) sang 14 ngôn ngữ. Chúng tôi theo đường ống LLM Evaluation Harness và sử dụng thiết lập zero-shot để đánh giá khả năng đa ngôn ngữ. Bảng 3 trình bày kết quả đánh giá zero-shot trên tác vụ XNLI. Tương tự, X-MoE có lợi từ dung lượng mô hình lớn và cho thấy cải thiện đáng chú ý so với mô hình Dense. Nhìn chung, MH-MoE đạt được hiệu suất tốt nhất, vượt trội X-MoE với mức tăng hiệu suất trung bình 0,6 cho thiết lập 8 chuyên gia và 0,8 cho thiết lập 32 chuyên gia. So sánh MH-MoE với X-MoE, nó cho thấy rằng các mô hình MH-MoE cung cấp mức tăng nhất quán trên các tác vụ downstream, chứng minh hiệu quả của cơ chế đa đầu được đề xuất của chúng tôi trong việc mô hình hóa ngôn ngữ tự nhiên đa ngôn ngữ.

Mô hình hóa Đa phương thức có Mặt nạ. Chúng tôi đánh giá trên các điểm chuẩn hiểu biết và tạo sinh thị giác-ngôn ngữ được sử dụng rộng rãi, bao gồm trả lời câu hỏi thị giác (Goyal et al., 2017), lý luận thị giác (Suhr et al., 2019) và tạo chú thích hình ảnh (Lin et al., 2014). Chúng tôi báo cáo vqa-score trên VQAv2, độ chính xác cho NLVR2. Đối với tạo chú thích hình ảnh COCO, chúng tôi báo cáo BLEU@4 (B@4), METEOR (M), CIDEr (C) và SPICE (S). Bảng 4 trình bày kết quả đánh giá. Đối với tác vụ VQA, MH-MoE vượt trội cả Dense và X-MoE với biên độ lớn, ví dụ, tăng 4,24 và 1,69 điểm trên phần test-dev, tương ứng. Đối với tác vụ lý luận thị giác, MH-MoE đánh bại cả hai đường cơ sở này trên cả phần dev (tăng 1,5 điểm so với X-MoE) và phần test-P (tăng 1,7 điểm so với X-MoE). Đối với tác vụ tạo chú thích hình ảnh, MH-MoE vượt trội X-MoE 4,2%, 10,2%, 9,4% về B@4, M và S, tương ứng. Những kết quả trên cho thấy rằng X-MoE thể hiện khả năng hiểu thông tin thị giác nâng cao, điều này cũng xác thực hiệu quả của cơ chế đa đầu được đề xuất của chúng tôi trong việc nắm bắt thông tin ngữ nghĩa và chi tiết đa dạng trong dữ liệu thị giác.

4.4. Nghiên cứu Ablation
Phần này trình bày phân tích thí nghiệm để chứng minh chức năng của MH-MoE. Trong tất cả các thí nghiệm so sánh, chúng tôi đảm bảo sự bằng nhau về tham số trên các mô hình bằng cách điều chỉnh các chiều trong của các chuyên gia.

Số lượng đầu h. Chúng tôi thực hiện thí nghiệm bằng cách điều chỉnh số lượng đầu (h = 2, 4, 6, 8 và 12) trong MH-MoE. Như được hiển thị trong Bảng 5, chúng tôi thấy rằng trên tất cả các thiết lập của h, mô hình của chúng tôi liên tục vượt trội X-MoE, chứng minh hiệu quả của MH-MoE. Bên cạnh đó, khi giá trị h tăng lên, chúng tôi quan sát thấy một cải thiện ban đầu tiếp theo là sự suy giảm trong hiệu suất của mô hình chúng tôi. Điều này dẫn chúng tôi đến giả thuyết rằng khi h ≤ 6, việc tăng cường hiệu suất có lợi từ cơ chế đa đầu bằng cách kích hoạt số lượng chuyên gia lớn hơn, từ đó tăng cường hiệu quả của mô hình và nắm bắt phạm vi rộng hơn thông tin token chi tiết. Tuy nhiên, khi h tiếp tục tăng vượt quá 6, việc phân chia quá mức các token có thể vô tình làm suy yếu nội dung ngữ nghĩa gốc của chúng, dẫn đến sự giảm hiệu suất của mô hình.

Tác động của các thành phần MH-MoE. Như được hiển thị trong Hình 3 (b), cơ chế đa đầu được sử dụng trong MH-MoE của chúng tôi chủ yếu kết hợp hai thành phần: các lớp Perceptron Đa tầng (MLP), bao gồm lớp đa đầu (Phương trình 3) và lớp hợp nhất (Phương trình 9), và hoạt động Tách-Hợp nhất Token (TSM) (Phương trình 4 và Phương trình 8). Chúng tôi thực hiện phân tích chi tiết về hiệu quả của từng thành phần trong mô hình của chúng tôi, cũng như sự cần thiết của việc tích hợp chúng.

Kết quả được trình bày trong Bảng 6. Một phân tích so sánh giữa Dense so với Dense w/o MLP, cũng như X-MoE so với X-MoE w/MLP, cho thấy rằng việc giới thiệu lớp MLP không nâng cao hiệu suất của mô hình. Tương tự, khi so sánh MH-MoE với MH-MoE w/o MLP, rõ ràng là việc chỉ bao gồm MLP, trong trường hợp không có TS, cũng không mang lại bất kỳ cải thiện nào trong hiệu quả của mô hình. Số lượng tham số của các mô hình được so sánh theo cặp là bằng nhau.

Một quan sát thú vị được thực hiện khi so sánh MH-MoE với MH-MoE w/o TS. Việc giới thiệu Token-Splitting-Merging (TSM) một mình, không có MLP, dẫn đến sự gia tăng nhẹ trong hiệu suất mô hình. Ngược lại, một sự nâng cao đáng kể trong hiệu suất mô hình chỉ đạt được khi cả MLP và TS được kết hợp đồng thời. Chúng tôi đưa ra giả thuyết rằng việc giới thiệu TS, không có sự tích hợp của MLP, kích hoạt nhiều chuyên gia hơn, nhưng việc phân đoạn và hợp nhất của mô hình xuất hiện quá đơn giản và đột ngột trong việc thực hiện. Hạn chế này cản trở khả năng của mô hình để phân đoạn có ý nghĩa các token thành token con và hợp nhất hiệu quả thông tin đa dạng được thu thập từ các không gian chuyên gia khác nhau.

Số lượng lớp MLP. Chúng tôi khám phá tác động của việc thay đổi số lượng lớp (n = 0, 1, 2, 3) trong MLP đối với hiệu suất MH-MoE. Đối với các cấu hình vượt quá một lớp duy nhất, các hàm kích hoạt ReLU đã được kết hợp giữa các lớp MLP để đảm bảo tính phi tuyến của các phép biến đổi. Số lượng tham số của các mô hình được so sánh là bằng nhau. Khi phân tích kết quả trong Bảng 7, chúng tôi quan sát thấy rằng việc tăng số lượng lớp MLP vượt quá một có tác động không đáng kể đến hiệu suất của mô hình. Điều này cho thấy rằng MLP một lớp là đủ để hoàn thành việc phân đoạn và hợp nhất token.

5. Phân tích
5.1. Phân tích Kích hoạt Chuyên gia
Kích hoạt Chuyên gia. Chúng tôi trực quan hóa việc kích hoạt của mỗi chuyên gia thay đổi trên các lớp chuyên gia song song cho X-MoE và MH-MoE tại Hình 5. Có thể quan sát thấy rằng: 1) X-MoE thể hiện phân phối lệch hơn, trong đó một phần đáng kể các chuyên gia vẫn không được kích hoạt mọi lúc. 2) MH-MoE của chúng tôi đạt được kích hoạt chuyên gia dày đặc hơn so với X-MoE, giảm thiểu hiệu quả vấn đề sử dụng chuyên gia thấp. 3) Khi số lượng đầu h tăng lên, tần suất kích hoạt chuyên gia trong MH-MoE cũng tăng lên.

Khả năng mở rộng. Chúng tôi khám phá khả năng mở rộng cho cả X-MoE và MH-MoE bằng cách mở rộng quy mô số lượng chuyên gia từ 8 lên 256 (khoảng 7B tham số). Đối với hiệu suất upstream, như được hiển thị trong Hình 6 (a), với sự gia tăng của các chuyên gia, MH-MoE của chúng tôi có thể mang lại nhiều lợi ích hơn. Đó là bởi vì MH-MoE có thể giảm thiểu vấn đề kích hoạt chuyên gia thấp một cách hiệu quả. Với khả năng này, sự vượt trội của mô hình SMoE quy mô lớn sẽ được phát huy tốt hơn, từ đó đạt được sự cải thiện của giới hạn trên của SMoE với nhiều chuyên gia hơn. Các đường cong perplexity xác thực chi tiết cho các thí nghiệm mở rộng quy mô này có thể được tìm thấy trong Hình 9 tại Phụ lục F. Đối với hiệu suất downstream được hiển thị trong Hình 6 (b), đối với X-MoE, số lượng chuyên gia = 64 là giới hạn trên, có nghĩa là việc tiếp tục tăng số lượng chuyên gia sẽ không mang lại bất kỳ lợi ích nào. MH-MoE của chúng tôi không chỉ có lợi thế hiệu suất so với X-MoE với cùng số lượng chuyên gia, mà còn cải thiện giới hạn trên từ 64 lên 256, điều này chứng minh hiệu quả của khả năng mở rộng của MH-MoE của chúng tôi trên các tác vụ downstream.

5.2. Phân tích khả năng hiểu biết chi tiết
Trong Phần 4, mô hình của chúng tôi xuất sắc trong nhiều tác vụ upstream và downstream, chứng minh khả năng mô hình hóa chi tiết vượt trội, cả cho ngôn ngữ và hình ảnh. Trong phần này, chúng tôi đi sâu vào phân tích chi tiết hơn để xác thực cách cơ chế đa đầu hỗ trợ MH-MoE trong việc nắm bắt thông tin ngữ nghĩa đa dạng và phức tạp thường khó hiểu, ví dụ, từ đa nghĩa và từ đồng âm khác nghĩa (được ký hiệu là token PF) trong ngôn ngữ, và các khu vực giàu ngữ nghĩa trong hình ảnh. Lưu ý rằng đối với dữ liệu ngôn ngữ, chúng tôi đã sử dụng API GPT-4 (OpenAI, 2023) để trích xuất các từ đa nghĩa và từ đồng âm khác nghĩa từ corpus XNLI (Conneau et al., 2018), và lời nhắc tương ứng có thể được tìm thấy trong Bảng 12.

Phân công Chuyên gia trong Token. Đối với dữ liệu ngôn ngữ, chúng tôi tính toán và so sánh mức độ phân kỳ (tức là, số lượng chuyên gia khác nhau mà các token con này được định tuyến đến) của các token con được tách từ token PF và token Không-PF. Chúng tôi thực hiện trên MH-MoE với 8 đầu (h = 8) và biểu diễn sự phân kỳ của mỗi token bằng cách tính toán sự phân kỳ trung bình trên các lớp khác nhau của mô hình. Kết quả, được trình bày trong Hình 7, rõ ràng chứng minh rằng phân phối phân kỳ cho token PF bị lệch đáng kể về phía bên phải khi so sánh với token Không-PF. Điều này cho thấy rằng, trong quá trình suy luận của MH-MoE, các token PF định tuyến các token con của chúng đến số lượng chuyên gia khác nhau lớn hơn, từ đó nắm bắt thông tin ngữ nghĩa đa dạng trái ngược với token Không-PF để mô hình hóa từ đa nghĩa và từ đồng âm khác nghĩa tốt hơn.

Đối với dữ liệu hình ảnh, chúng tôi phân tích cách mức độ phân kỳ của các patch khác nhau phát triển trong quá trình huấn luyện, như được minh họa trong Hình 8. Thú vị là, chúng tôi quan sát thấy rằng khi các bước huấn luyện tăng lên, mức độ phân kỳ tăng dần trong các vùng kết cấu tần số cao (hoặc các vùng với ngữ nghĩa phong phú), trong khi mức độ phân kỳ trong các vùng kết cấu tần số thấp giảm dần. Điều này cho thấy rằng trong quá trình huấn luyện, MH-MoE có xu hướng định tuyến các token từ các khu vực có kết cấu phức tạp đến một loạt chuyên gia đa dạng hơn, từ đó nâng cao hiểu biết chi tiết hơn về ngữ nghĩa trong khu vực đó. Để có thêm ví dụ trực quan, vui lòng tham khảo Hình 10 tại Phụ lục G.

5.3. Phân tích Độ phức tạp & Tham số.
Chúng tôi trình bày phân tích về Độ phức tạp & Tham số cho X-MoE và MH-MoE trong Phụ lục D, để xác thực rằng đối với tất cả thiết lập thí nghiệm, chi phí tính toán và tham số của MH-MoE của chúng tôi đều thấp hơn so với SMoE. Bên cạnh đó, số lượng tham số chi tiết cho tất cả các thí nghiệm và các mô hình có thể so sánh có thể được thấy trong Bảng 11.

6. Kết luận
Trong bài báo này, chúng tôi nghiên cứu cách chúng ta có thể đạt được kích hoạt chuyên gia dày đặc hơn mà không giới thiệu chi phí bổ sung, đồng thời cải thiện khả năng hiểu biết chi tiết. Với Hỗn hợp Chuyên gia Đa Đầu được đề xuất, chúng tôi có thể dễ dàng triển khai chức năng nói trên. Hơn nữa, tính đơn giản của MH-MoE cho phép nó tích hợp với các khung SMoE khác để nâng cao hiệu suất một cách dễ dàng. Kết quả thực nghiệm rộng rãi trên ba tác vụ chứng minh tính hiệu quả của MH-MoE.

7. Tác động rộng hơn
Trong các đường ống NLP trước đây, chiều của các token từ đã được duy trì không đổi một cách thông thường trong cả giai đoạn huấn luyện và suy luận. Chúng tôi là những người đầu tiên cố gắng phân đoạn token bên ngoài mô-đun attention đa đầu, nhằm nâng cao khả năng của mô hình trong một số khía cạnh, bao gồm hiểu biết tinh tế và đa diện hơn về nội dung token cũng như thúc đẩy kiến trúc mạng thưa thớt hơn. Chúng tôi tin rằng đây là một khám phá phản trực giác nhưng đáng giá trong lĩnh vực này.

Tài liệu tham khảo
[Các tài liệu tham khảo được liệt kê từ trang 9-11 với định dạng học thuật tiêu chuẩn]

A. Dữ liệu huấn luyện trước của tác vụ mô hình hóa đa phương thức có mặt nạ
[Nội dung chi tiết về dữ liệu huấn luyện từ trang 12]

B. Siêu tham số mô hình của các tác vụ mô hình hóa ngôn ngữ
[Nội dung chi tiết về siêu tham số từ trang 12]

C. Siêu tham số cho huấn luyện trước
[Nội dung chi tiết về siêu tham số huấn luyện từ trang 12-13]

D. Phân tích Độ phức tạp & Tham số
[Nội dung chi tiết về phân tích từ trang 14-15]

E. Mã phong cách PyTorch
[Nội dung chi tiết về mã nguồn từ trang 16]

F. Trực quan hóa perplexity huấn luyện
[Nội dung chi tiết về trực quan hóa từ trang 17]

G. Trực quan hóa
[Nội dung chi tiết về trực quan hóa từ trang 17-18]
