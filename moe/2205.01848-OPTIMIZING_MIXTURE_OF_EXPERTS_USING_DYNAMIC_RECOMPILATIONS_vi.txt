# 2205.01848.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2205.01848.pdf
# Kích thước file: 1792388 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
TỐI ƯU HOÁ HỖN HỢP CHUYÊN GIA SỬ DỤNG BIÊN DỊCH LẠI ĐỘNG
Ferdinand Kossmann1 Zhihao Jia2 Alex Aiken3

TÓM TẮT
Kiến trúc Hỗn hợp Chuyên gia cho phép các mạng nơ-ron cực lớn bằng cách mở rộng kích thước tham số mô hình độc lập với nhu cầu tính toán (FLOPs). Tuy nhiên, các framework DNN hiện tại không thể hỗ trợ hiệu quả luồng dữ liệu động trong Hỗn hợp Chuyên gia, và các triển khai trên các framework này cần sử dụng các cách giải quyết tạm thời gây ra chi phí đáng kể. Để giải quyết hạn chế của các framework này, chúng tôi trình bày DYNAMOE, một thư viện DNN sử dụng biên dịch lại động để tối ưu hóa và thích ứng việc sử dụng tài nguyên tính toán với nhu cầu động của các mô hình Hỗn hợp Chuyên gia. Đánh giá của chúng tôi cho thấy DYNAMOE đạt được tăng tốc 1.8× và hỗ trợ kích thước mô hình lớn hơn 2.3× khi so sánh với các hệ thống MoE hiện có, ngay cả khi không sử dụng biên dịch lại. Sau đó chúng tôi trình bày các tối ưu hóa bổ sung được kích hoạt bởi biên dịch lại động mang lại tăng tốc thêm 1.7× trong khi đồng thời giảm áp lực bộ nhớ và cải thiện chất lượng mô hình.

1 GIỚI THIỆU
Việc mở rộng tham số mô hình và kích thước mẫu huấn luyện là một trong những con đường hiệu quả nhất hướng tới các mô hình mạnh mẽ hơn (Kaplan et al., 2020; Brown et al., 2020): các kiến trúc nơ-ron đơn giản với nhiều tham số có thể huấn luyện và lượng lớn dữ liệu huấn luyện thường vượt trội hơn các mạng nơ-ron phức tạp hơn được huấn luyện ở quy mô nhỏ hơn (Sutton, 2019). Tuy nhiên, cả việc tăng tham số mô hình và kích thước mẫu đều đưa thêm tính toán, tạo ra nút thắt cổ chai cho việc mở rộng thêm.

Hỗn hợp Chuyên gia (MoE) giải quyết nút thắt cổ chai tính toán này bằng cách cho phép tham số mô hình và nhu cầu tính toán (FLOPs) được mở rộng độc lập. Điều này cho phép mở rộng thêm kích thước tham số mô hình với nhiều bậc độ lớn: các mô hình MoE như Switch transformer chứng minh rằng MoE làm cho việc mở rộng vượt quá tỷ tham số mô hình trở nên khả thi - Switch transformer có 5000× tham số nhiều hơn BERT. ST-MoE hiện đang dẫn đầu hầu hết các benchmark NLP (Zoph et al., 2022) và V-MoE đang đạt hiệu suất dự đoán tốt nhất trên các tác vụ hình ảnh chỉ với một nửa tính toán tại thời điểm suy luận (Riquelme et al., 2021).

Để hỗ trợ kích thước tham số lớn như vậy, MoE chỉ kích hoạt các mô-đun con của mạng cho mỗi mẫu, được gọi là chuyên gia. Chuyên gia nào sử dụng cho một mẫu nhất định được học bởi một mạng gating. Điều này tạo ra luồng dữ liệu động, nơi đầu vào cho một toán tử chỉ được xác định tại thời điểm chạy và thay đổi trong suốt quá trình huấn luyện.

1ETH Zurich 2Carnegie Mellon University 3Stanford. Liên hệ: Ferdinand Kossmann <ferdiko@mit.edu>.

Tuy nhiên, các hệ thống học máy hiện tại vẫn phục vụ cho khối lượng công việc tĩnh và gặp khó khăn với luồng dữ liệu động trong MoE. Ví dụ, các framework như TensorFlow yêu cầu người dùng định nghĩa đồ thị tính toán với kích thước tensor tĩnh không thể thay đổi trong quá trình huấn luyện (Abadi et al., 2015). Để xử lý các biến thể trong số lượng mẫu được gán cho một chuyên gia, người dùng cần sử dụng các cách giải quyết tạm thời dẫn đến chi phí thời gian chạy đáng kể và chất lượng mô hình không tối ưu.

Trong bài báo này, chúng tôi giới thiệu RECOMPILE, một cơ chế đơn giản cho phép thay đổi động kiến trúc mô hình và kích thước tensor của nó trong quá trình huấn luyện. RECOMPILE cho phép hai tối ưu hóa hệ thống chính mà các framework MoE hiện tại thiếu. Đầu tiên, bằng cách sử dụng RECOMPILE, chúng tôi giới thiệu các hệ số dung lượng động thỉnh thoảng thích ứng kích thước tensor của mỗi chuyên gia dựa trên nhu cầu thực tế của chúng, giảm nhu cầu tính toán và yêu cầu bộ nhớ của MoE. Tối ưu hóa thứ hai tận dụng việc gán mẫu của mạng gating hội tụ sớm trong quá trình huấn luyện. Dựa trên quan sát này, chúng tôi giới thiệu caching gán mẫu, lưu trữ các gán mẫu sau khi chúng hội tụ và sử dụng RECOMPILE để loại bỏ phụ thuộc giữa mạng gating và các chuyên gia, dẫn đến nhiều song song hóa hơn.

Để đánh giá RECOMPILE và các tối ưu hóa MoE của chúng tôi, chúng tôi triển khai DYNAMOE, một hệ thống huấn luyện MoE mã nguồn mở với API linh hoạt và hỗ trợ phong phú cho các toán tử phổ biến.

Tóm lại, các đóng góp của chúng tôi là:
• Chúng tôi trình bày DYNAMOE, một triển khai MoE có sẵn công khai. So với các hệ thống MoE công khai tốt nhất, DYNAMOE mang lại tăng tốc lên đến 1.8× trong khi hỗ trợ các mô hình lớn hơn 2.3× trước khi hết bộ nhớ.
• Chúng tôi đề xuất RECOMPILE, một cơ chế có thể được thêm vào bất kỳ framework ML nào để xử lý luồng dữ liệu động trong các mô hình như MoE. Chúng tôi trình bày cách RECOMPILE tự nhiên cho phép hai tối ưu hóa cho MoE mang lại tăng tốc thêm 1.7×. Các tối ưu hóa này không chỉ giảm thời gian chạy mà còn giảm áp lực bộ nhớ và cải thiện chất lượng mô hình.

--- TRANG 2 ---
DynaMoE: Hỗn hợp Chuyên gia được tối ưu hóa động

2 KIẾN THỨC NỀN TẢNG & NGHIÊN CỨU LIÊN QUAN
Chúng tôi thảo luận ngắn gọn về nghiên cứu trước đây về cả các ý tưởng thuật toán cơ bản của MoE cũng như các hệ thống nhằm mở rộng MoE.

2.1 Nghiên cứu thuật toán
Hỗn hợp Chuyên gia được đề xuất ban đầu trong (Jacobs et al., 1991). Ý tưởng chính đằng sau MoE là xây dựng một mạng gồm nhiều mô-đun con, được gọi là chuyên gia. Mỗi chuyên gia nên chuyên biệt hóa về một miền con của đầu vào, giả định rằng giải quyết chỉ miền con đó dễ dàng hơn so với giải quyết toàn bộ vấn đề. Để quyết định chuyên gia nào có thẩm quyền nhất cho một mẫu nhất định, một mạng gating gán trọng số cho dự đoán của mỗi chuyên gia, và dự đoán MoE cuối cùng được đưa ra bởi tổng có trọng số của các dự đoán chuyên gia.

Jacobs et al. (1991) giới thiệu nhiều hàm mất mát để huấn luyện MoE, trong đó chúng tôi trình bày hai: mất mát hợp tác và mất mát đặc tả. Mất mát hợp tác Lcoop được cho bởi Phương trình 1.

Lcoop(ŷ,O,g) = L(ŷ,∑igiOi) (1)

trong đó L là bất kỳ hàm mất mát đặc thù tác vụ nào (ví dụ: categorical cross-entropy), Oi là dự đoán mẫu của chuyên gia thứ i, O = (O1,...,On) là dự đoán mẫu của tất cả chuyên gia, gi đại diện cho trọng số được gán cho chuyên gia thứ i, và ŷ là nhãn của mẫu.

Mất mát hợp tác do đó chỉ là mất mát đặc thù tác vụ giữa vector đích ŷ và đầu ra MoE ∑igiOi.

Lưu ý rằng mất mát hợp tác khuyến khích các chuyên gia hợp tác để tạo ra dự đoán tốt: Gradient đối với mạng của mỗi chuyên gia trong back propagation cũng phụ thuộc vào các dự đoán được thực hiện bởi các chuyên gia khác trong quá trình forward pass.

Thay vào đó, mất mát đặc tả được cho bởi Phương trình 2.

Lspec(ŷ,O,g) = ∑igiL(ŷ,Oi) (2)

Gradient đối với mạng của mỗi chuyên gia do đó chỉ phụ thuộc vào dự đoán của chính chuyên gia đó, và không phụ thuộc vào dự đoán của các chuyên gia khác như với mất mát hợp tác. Kết quả là, mất mát đặc tả cho phép mỗi chuyên gia học độc lập với các chuyên gia khác, vì dự đoán của các chuyên gia khác không ảnh hưởng đến gradient của nó.

DYNAMOE hỗ trợ cả mất mát hợp tác và mất mát đặc tả. Trong khi mất mát hợp tác phổ biến hơn và dễ thêm vào các framework hiện có, chúng tôi thấy rằng mất mát đặc tả mang lại kết quả tốt hơn. Phụ lục A mô tả triển khai mất mát đặc tả của chúng tôi.

Jordan & Jacobs (1993) đầu tiên đề xuất MoE phân cấp, nơi mạng của mỗi chuyên gia cũng có thể là một MoE (điều này có thể tiếp tục đệ quy). Sử dụng kiến trúc MoE phân cấp giảm hệ số nhánh của mỗi mạng gating, điều này đặc biệt mong muốn đối với các mô hình có số lượng lớn chuyên gia.

Shazeer et al. (2017) giới thiệu gating top-k thưa, nơi chỉ những k chuyên gia có trọng số cao nhất được gán bởi mạng gating mới dự đoán trên một mẫu. k do đó phục vụ như một siêu tham số xác định lượng tính toán được thực hiện cho mỗi mẫu và cho phép số lượng FLOPs (được xác định bởi k) mở rộng độc lập với số lượng tham số (được xác định bởi tổng số chuyên gia n). Thuật toán 1 cho thấy mã giả cho forward pass khi sử dụng gating top-k thưa.

Thuật toán 1 Forward pass MoE với gating top-k
Đầu vào: x: mẫu đầu vào
k: số chuyên gia được chọn
G: Mạng Gating
{E1,...,En}: Một pool gồm n chuyên gia
Đầu ra: y: dự đoán MoE cho x
1: score ← G(x)
2: indices ← argmax_k(score)
3: (w1, ..., wk) ← normalize(score[indices])
4: y ← 0 vector trong output shape
5: for i ← 1, ..., k do
6:    e ← indices[i]
7:    yi ← Ee(x)
8:    y += wi * yi
9: end for
10: return y

Gating thưa tạo ra vấn đề là các chuyên gia khác nhau thường nhận được lượng dữ liệu huấn luyện khác nhau. Ở đầu quá trình huấn luyện, một số chuyên gia nhận được nhiều mẫu hơn các chuyên gia khác do khởi tạo ngẫu nhiên. Những chuyên gia này sau đó có nhiều dữ liệu huấn luyện hơn các chuyên gia khác và do đó đưa ra dự đoán tốt hơn, dẫn đến mạng gating gán cho chúng nhiều mẫu hơn nữa. Vòng lặp phản hồi này có thể dẫn đến tất cả các mẫu được gán cho rất ít (thường chỉ k) chuyên gia. Để tránh những tình huống như vậy, nhiều nghiên cứu đề xuất thêm một penalty mất cân bằng vào hàm mất mát (Shazeer et al., 2017; Fedus et al., 2021). Một ví dụ về số hạng cân bằng được đưa ra trong phương trình 3.

--- TRANG 3 ---
DynaMoE: Hỗn hợp Chuyên gia được tối ưu hóa động

2.2 Nghiên cứu hệ thống

GShard (Lepikhin et al., 2020) triển khai một phiên bản phân tán của MoE gating thưa và mở rộng huấn luyện MoE lên 600 tỷ tham số. Hệ thống chứng minh rằng MoE có thể được huấn luyện hiệu quả trên 2048 TPU song song.

Những kết quả này được cải thiện thêm bởi Switch Transformer, đơn giản hóa thuật toán MoE và mở rộng lên 1.6 nghìn tỷ tham số (Fedus et al., 2021). Switch transformer đạt hiệu suất dự đoán tốt nhất trên một số tác vụ ngôn ngữ. Các đơn giản hóa được đề xuất trong Switch Transformer bao gồm gán mỗi mẫu cho một chuyên gia duy nhất (tức là k = 1) và sử dụng số hạng cân bằng đơn giản hóa được đưa ra bởi Phương trình 3. Chúng tôi áp dụng số hạng cân bằng này trong DYNAMOE.

B = λ * n * ∑(i=1 to n) Ti · Gi (3)

trong đó n là tổng số chuyên gia, λ là hệ số trọng số của regularizer, Ti là tỷ lệ token được gán cho chuyên gia thứ i, và Gi là tỷ lệ xác suất mạng gating được gán cho chuyên gia thứ i trên toàn bộ batch.

GShard và hệ thống được sử dụng để huấn luyện Switch Transformer không có sẵn công khai. Có tồn tại một số triển khai mã nguồn mở sử dụng TensorFlow và PyTorch, nhưng những triển khai này gặp phải hiệu suất kém vì chúng không thực hiện bất kỳ sửa đổi nào đối với các framework để hỗ trợ MoE một cách bản địa. Các triển khai này hơn nữa chỉ hoạt động cho một GPU (Rau, 2019; Wang, 2019).

Tensor2Tensor (Vaswani et al., 2018) cung cấp một triển khai MoE Transformer được xây dựng trên TensorFlow, nhưng Tensor2Tensor đã bị deprecated và mô hình MoE hiện tại không hoạt động như dự định.

FastMoE (He et al., 2021) là một hệ thống huấn luyện MoE phân tán dựa trên PyTorch với hỗ trợ cho nhiều GPU. Tuy nhiên, FastMoE thiếu các tính năng quan trọng như hỗ trợ cho các hệ số dung lượng khác 1.0.

Không có nghiên cứu nào ở trên, bao gồm GShard và Switch Transformer, giải quyết hành vi động của MoE, dẫn đến việc sử dụng tài nguyên tính toán không hiệu quả.

3 RECOMPILE

Phần này mô tả cách RECOMPILE hoạt động từ góc nhìn của người dùng. Triển khai RECOMPILE của chúng tôi trên FlexFlow được mô tả trong Phụ lục B. Ngoài ra, chúng tôi cho thấy trong Phụ lục C rằng RECOMPILE cũng có thể được áp dụng cho bất kỳ framework ML nào khác. Phụ lục C cũng cho thấy cách các yêu cầu RECOMPILE hiện tại không được thỏa mãn bởi các framework ML phổ biến như PyTorch và TensorFlow và cách các framework này có thể được hưởng lợi từ việc triển khai một cơ chế như RECOMPILE.

Trong khi RECOMPILE không đặc thù cho MoE và cũng có thể được sử dụng để tối ưu hóa các mô hình khác, bài báo này tập trung vào việc sử dụng RECOMPILE cho MoE. Sau phần giới thiệu chung về RECOMPILE này, Phần 4 trình bày hai tối ưu hóa đặc thù MoE được kích hoạt thông qua RECOMPILE.

Về mặt khái niệm, RECOMPILE bao gồm một cơ chế biên dịch lại đồ thị tính toán từ đầu nhưng chỉ phân bổ lại tài nguyên nếu cần thiết. Ví dụ, khi thay đổi chiều batch của một tensor, bất kỳ vùng bộ nhớ nào chứa weights vẫn không được động đến. Điều này cho phép RECOMPILE tránh chi phí cấm đoán có thể xuất phát từ việc biên dịch lại toàn bộ đồ thị một cách ngây thơ.

RECOMPILE hơn nữa cho phép lập kế hoạch những biên dịch lại như vậy trong khi ngăn chặn chi phí từ các quyết định điều chỉnh đồ thị. Điều này quan trọng vì việc tính toán khi nào và cách thay đổi đồ thị tính toán có thể tốn kém và cần được thực hiện thường xuyên (tức là sau mỗi mini-batch). Thực tế, việc xác định cách thay đổi đồ thị tính toán có thể ví dụ yêu cầu dự đoán của một mạng nơ-ron bổ sung, như được thảo luận trong Phần 6. Để tránh thêm những chi phí như vậy vào thời gian chạy huấn luyện/suy luận tổng thể, RECOMPILE chạy các quyết định điều chỉnh đồ thị trên CPU trong khi quá trình huấn luyện hoặc suy luận của mô hình tiếp tục trên GPU.

Tóm lại, một triển khai RECOMPILE phải thỏa mãn ba yêu cầu sau.

1. Người dùng có thể giám sát các metric mô hình với chi phí không đáng kể. Một metric mô hình có thể là bất cứ thứ gì thông báo cho người dùng về trạng thái hiện tại của mô hình trong quá trình suy luận hoặc huấn luyện, chẳng hạn như mất mát huấn luyện.

2. Người dùng có thể thay đổi đồ thị tính toán với ít chi phí. Khi nào và cách đồ thị được thay đổi được xác định bởi một recompile trigger do người dùng cung cấp, có thể dựa quyết định của nó trên các metric mô hình.

3. Việc thực thi recompile trigger và tính toán ánh xạ phần cứng của đồ thị mới được chồng chéo với suy luận/huấn luyện sử dụng đồ thị hiện tại. Ví dụ: Một người dùng có thể thực hiện một tính toán tốn kém để quyết định cách thay đổi đồ thị hoặc thực hiện một tìm kiếm dài cho một chiến lược song song hóa mới trong khi quá trình huấn luyện mô hình tiếp tục song song.

Bằng cách triển khai một cơ chế RECOMPILE, DYNAMOE do đó cho phép người dùng dễ dàng thay đổi đồ thị tính toán

--- TRANG 4 ---
DynaMoE: Hỗn hợp Chuyên gia được tối ưu hóa động

[Hình 1. Tổng quan về DYNAMOE, một hệ thống MoE sử dụng RECOMPILE để tận dụng caching gán mẫu và hệ số dung lượng động cho tăng tốc, giảm áp lực bộ nhớ và chất lượng mô hình cao hơn.]

dựa trên các sự kiện tùy ý, do người dùng định nghĩa mà không phát sinh bất kỳ chi phí lập lịch nào. DYNAMOE hơn nữa phơi bày các toán tử và tensor có kích thước động tận dụng RECOMPILE và có thể được sử dụng như bất kỳ toán tử hoặc tensor nào khác chỉ trong một dòng code.

4 CÁC TỐI ƯU HÓA ĐƯỢC KÍCH HOẠT BỞI RECOMPILE

Phần này trình bày hai tối ưu hóa cho MoE được kích hoạt tự nhiên thông qua RECOMPILE và chỉ yêu cầu vài dòng code của người dùng. Những tối ưu hóa này cũng được mô tả sơ đồ trong Hình 1.

4.1 Hệ số dung lượng động

Trong suốt quá trình huấn luyện MoE, mạng gating có thể gán số lượng mẫu khác nhau cho một chuyên gia. Các framework MoE hiện tại yêu cầu khai báo các tensor có kích thước cố định khi xây dựng mô hình MoE. Để giải quyết điều này, người dùng hiện tại khai báo các tensor quá lớn (tức là tăng chiều batch) và bỏ các mẫu vẫn không vừa với các tensor (tức là bỏ qua chúng trong back propagation). Kết quả là, khi một mini-batch chứa ít mẫu hơn dung lượng được khai báo cho một chuyên gia, các mục đầu tiên của tensor đầu vào sẽ được sử dụng cho những mẫu này và các mục còn lại sẽ không được sử dụng trong quá trình huấn luyện bằng cách đặt gradient về zero.

Fedus et al. (2021) biểu thị dung lượng chuyên gia là số lượng mẫu mà tensor đầu vào của chuyên gia có thể chứa. Đối với một mô hình MoE với n chuyên gia, nơi k chuyên gia được gán cho mỗi mẫu, một hệ số dung lượng chuyên gia α được sử dụng để tính dung lượng C của mỗi chuyên gia như được hiển thị trong Phương trình 4.

C = α * batch_size * k/n (4)

Lưu ý rằng một hệ số dung lượng chuyên gia cao yêu cầu tính toán bổ sung (FLOPs), bộ nhớ và truyền dữ liệu. Mặt khác, các hệ số dung lượng chuyên gia thấp dẫn đến nhiều mẫu bị bỏ hơn, dẫn đến chất lượng mô hình thấp hơn. Việc định nghĩa hệ số dung lượng chuyên gia do đó liên quan đến sự đánh đổi giữa hiệu suất mô hình và tài nguyên tính toán.

Sử dụng RECOMPILE, chúng tôi có thể cải thiện lớn sự đánh đổi này bằng cách sử dụng các hệ số dung lượng động thích ứng theo dung lượng mà chuyên gia thực sự cần. Để làm như vậy, DYNAMOE đo lường số lượng mẫu mà mạng gating gán cho mỗi chuyên gia và tạo một metric mô hình cho điều đó. Khi yêu cầu dung lượng của chuyên gia thay đổi trong suốt quá trình huấn luyện, hệ số dung lượng của chuyên gia có thể được điều chỉnh tương ứng bằng cách gọi RECOMPILE, cho phép người dùng cả điều chỉnh dung lượng một cách động theo thời gian và điều chỉnh riêng cho từng chuyên gia.

4.2 Caching Gán Mẫu

Trong quá trình huấn luyện MoE, điều quan trọng là các chuyên gia chuyên biệt hóa về các miền con của đầu vào — nếu không, việc tăng kích thước tham số không hiệu quả vì các chuyên gia dư thừa về chức năng và hầu hết các tham số có cùng chức năng đa mục đích. Để một chuyên gia chuyên biệt hóa về một miền con, chuyên gia phải được huấn luyện trên các mẫu chủ yếu là các phần tử của miền con đó. Nếu quá nhiều mẫu từ miền đầu vào còn lại được gán cho chuyên gia, chuyên gia sẽ không chuyên biệt hóa và thay vào đó sẽ cố gắng giảm mất mát cho dự đoán trên toàn bộ miền đầu vào.

Chúng tôi giả thuyết rằng các gán mẫu của mạng gating hội tụ sớm trong quá trình huấn luyện. Một khi mạng gating quyết định gán một miền con nhất định cho một chuyên gia nào đó, quyết định này được củng cố vì chuyên gia chuyên biệt hóa về miền con đó, do đó mang lại hiệu suất tốt hơn so với các chuyên gia khác và khuyến khích mạng gating tiếp tục gán các mẫu của miền con đó cho chuyên gia. Một khi vòng lặp phản hồi này bắt đầu, mạng gating sẽ chủ yếu tinh chỉnh trọng số của các chuyên gia và sẽ chỉ gán lại vài mẫu.

Hội tụ sớm trên các gán mẫu có thể được tận dụng để đạt được mức độ song song hóa mô hình cao hơn. Vì các batch chuyên gia chỉ có thể được hình thành khi các gán mẫu được biết, các forward pass của các chuyên gia chỉ có thể được thực thi khi forward pass của mạng gating đã hoàn thành. Tuy nhiên, nếu chúng ta biết các gán mẫu trước khi mạng gating hoàn thành forward pass của nó, phụ thuộc này có thể được loại bỏ và mạng gating và các chuyên gia có thể thực thi forward pass của chúng song song. Hình 2 cho thấy phụ thuộc "có thể loại bỏ" như các mũi tên đứt nét.

[Hình 2. Toán tử Cache loại bỏ phụ thuộc đứt nét sau khi các gán mẫu đã hội tụ.]

Sử dụng RECOMPILE, chúng tôi có thể phát hiện khi nào các gán mẫu đã hội tụ và loại bỏ phụ thuộc sau đó. Cụ thể hơn, chúng tôi có thể tạo một metric mô hình về số lượng mẫu của một mini-batch được gán lại cho các chuyên gia khác nhau giữa hai epoch huấn luyện liên tiếp. Nếu chỉ có vài mẫu được gán lại, chúng tôi có thể cache các gán mẫu và sử dụng các gán được cache để tạo batch chuyên gia trước khi mạng gating hoàn thành forward pass của nó.

DYNAMOE cung cấp một toán tử Cache nhớ các gán mẫu và tạo một metric ghi lại số lượng mẫu đã được gán lại giữa hai epoch liên tiếp. Toán tử Cache được hiển thị màu đỏ trong Hình 2. Ngay cả khi thực thi forward pass của chuyên gia và mạng gating song song, toán tử Cache sẽ cập nhật các gán mẫu được cache sau khi mạng gating hoàn thành forward pass của nó. Tuy nhiên, những gán mới này sẽ chỉ được sử dụng trong epoch tiếp theo. Do đó, sau khi phát hiện rằng các gán mẫu đã hội tụ, mô hình sẽ sử dụng các gán mẫu của epoch trước để đạt được mức độ song song hóa cao hơn.

Hình 3 cho thấy cách các tác vụ được lập lịch với và không có caching được bật trên bốn GPU. Lưu ý cách các forward pass của mạng gating và các chuyên gia xảy ra song song khi caching được bật.

--- TRANG 5 ---
DynaMoE: Hỗn hợp Chuyên gia được tối ưu hóa động

[Hình 3. GPU 1 thực thi mạng gating, GPU 2-4 thực thi mỗi GPU một chuyên gia. Mỗi hộp đại diện cho việc thực thi một tác vụ. Caching cho phép thực thi các forward pass chuyên gia song song với forward pass mạng gating.]

5 ĐÁNH GIÁ

Chúng tôi đánh giá DYNAMOE theo ba khía cạnh. Phần 5.1 so sánh hiệu suất huấn luyện MoE end-to-end giữa DYNAMOE và các hệ thống MoE hiện có; Phần 5.2 đánh giá các hệ số dung lượng động; và Phần 5.3 đánh giá caching gán mẫu. Chúng tôi tập trung vào việc đánh giá các layer MoE riêng lẻ để giảm ảnh hưởng của các toán tử khác đến thời gian chạy càng nhiều càng tốt.

Tất cả các thí nghiệm được thực hiện trên một máy với bốn GPU NVIDIA Tesla P100 và hai CPU Intel Xeon E5-2640 v4. Trừ khi được nêu khác, tất cả các thí nghiệm được thực hiện trên CIFAR10 (Krizhevsky et al., 2014) sử dụng CNN (LeCun et al., 1989) làm mô-đun gating và chuyên gia.

5.1 Hiệu suất End-to-end

Hình 4 so sánh DYNAMOE với FastMoE, một triển khai MoE mã nguồn mở tốt nhất (He et al., 2021). Để so sánh, một CPU và một GPU đã được sử dụng. Không có tối ưu hóa RECOMPILE nào được bật và đường màu xanh phản ánh hiệu suất DYNAMOE cơ bản.

Benchmark sử dụng cùng kiến trúc mô hình mà FastMoE sử dụng trong script benchmarking trên GitHub (các mô-đun perceptron đa tầng). Chúng tôi tái tạo kiến trúc này trong DYNAMOE và cũng đặt tất cả các siêu tham số khác bằng nhau. Như trong script benchmarking của FastMoE, chúng tôi sử dụng một batch dữ liệu được tạo ngẫu nhiên cố định và đo throughput end-to-end cho huấn luyện qua 10.000 bước. Việc mở rộng kích thước tham số (trục x) được thực hiện bằng cách tăng số lượng chuyên gia n trong mạng.

[Hình 4. DYNAMOE không có tối ưu hóa RECOMPILE nhanh hơn FastMoE lên đến 1.8×. Hơn nữa, DYNAMOE có thể hỗ trợ mô hình lớn hơn 2.3× trước khi hết bộ nhớ (OOM).]

Hình 4 cho thấy DYNAMOE mang lại tăng tốc lên đến 1.8× so với FastMoE trên các kích thước mô hình lớn hơn, đây là những kích thước quan trọng nhất vì mục tiêu của MoE là mở rộng tham số mô hình. DYNAMOE hơn nữa có thể hỗ trợ mô hình lớn hơn 2.3× trước khi hết bộ nhớ (OOM). Chúng tôi suy đoán rằng một phần tăng tốc đến từ việc DYNAMOE sử dụng đồ thị tính toán tĩnh và triển khai các tối ưu hóa tương ứng. Mặt khác, FastMoE nằm trên PyTorch và sử dụng đồ thị tính toán động, điều này không cho phép tất cả các tối ưu hóa này.

Hình 5 hơn nữa cho thấy DYNAMOE mở rộng tuyến tính khi thêm GPU, cả về throughput và kích thước mô hình được hỗ trợ trước khi hết bộ nhớ.

[Hình 5. DYNAMOE mở rộng tuyến tính cả về tăng tốc và kích thước mô hình được hỗ trợ khi thêm nhiều GPU hơn.]

Bây giờ chúng tôi so sánh các tối ưu hóa RECOMPILE với DYNAMOE có các hệ số dung lượng tĩnh khác nhau. Vì FastMoE không hỗ trợ các hệ số dung lượng khác 1.0, chúng tôi bỏ qua FastMoE trong Hình 6.

[Hình 6. Các tối ưu hóa RECOMPILE kết hợp đạt được tăng tốc 1.7× mà không ảnh hưởng đến độ chính xác.]

Hình 6 cho thấy cách các hệ số dung lượng động đạt được độ chính xác tương tự như khi sử dụng hệ số dung lượng tĩnh đảm bảo rằng không có mẫu nào bị bỏ (trong trường hợp này, α = 7.0). Hệ số dung lượng động mang lại tăng tốc 1.4×. Khi caching gán mẫu cũng được bật, tăng tốc tăng lên tổng cộng 1.7× trong khi vẫn đạt được độ chính xác tương tự. Sử dụng cả hai tối ưu hóa, chúng tôi có được throughput giống như hệ số dung lượng tĩnh α = 1.0 nhưng không bị mất chất lượng mô hình.

Chỉ sử dụng tối ưu hóa caching với các hệ số dung lượng tĩnh không mang lại tăng tốc trong các thí nghiệm của chúng tôi, vì thiết lập thí nghiệm không cho phép ánh xạ phần cứng khai thác song song hóa mô hình bổ sung. Các ánh xạ phần cứng được xem xét bị giới hạn trong song song hóa chuyên gia, có nghĩa là mạng gating và bất kỳ mô-đun chuyên gia nào phải hoàn toàn được đặt trên một thiết bị và một mô-đun đơn không thể được phân phối giữa nhiều thiết bị. Khi bỏ ràng buộc này, hoặc chọn một thiết lập thí nghiệm khác (tức là các siêu tham số khác hoặc topology thiết bị khác), caching cũng có thể mang lại tăng tốc với các hệ số dung lượng tĩnh, như chúng tôi cho thấy trong Phần 5.3.

Cuối cùng, chúng tôi so sánh mất mát hợp tác với mất mát đặc tả và cũng xem xét baseline giữ các gán chuyên gia cố định đồng nhất, vì điều này gợi ý nơi hiệu suất tốt hơn đến từ: Các gán chuyên gia tốt hơn hay các chuyên gia tốt hơn.

Hình 7 cho thấy mất mát đặc tả vượt trội đáng kể so với mất mát hợp tác. Hơn nữa, mất mát đặc tả có vẻ là mất mát tốt hơn cho cả các chuyên gia và mạng gating: Mất mát đặc tả vượt trội hơn mất mát hợp tác khi giữ các gán mẫu cố định thành một gán ngẫu nhiên đồng nhất, điều này gợi ý rằng mất mát đặc tả tốt hơn cho huấn luyện chuyên gia độc lập với mạng gating. Hơn nữa, sử dụng các gán mẫu học có tác động lớn hơn khi sử dụng mất mát đặc tả, gợi ý rằng mất mát đặc tả cũng là hàm mất mát tốt hơn cho mạng gating.

[Hình 7. Mất mát đặc tả vượt trội đáng kể so với mất mát hợp tác.]

5.2 Hệ số Dung lượng Động

Các đường màu mờ trong Hình 8 cho thấy tỷ lệ giữa số lượng mẫu được gán cho mỗi chuyên gia và số lượng mẫu trung bình được gán cho một chuyên gia. Ví dụ, nếu chuyên gia E có tỷ lệ 2 tại một điểm nhất định trong quá trình huấn luyện, có nghĩa là mạng gating đã gán gấp đôi số lượng mẫu trung bình cho chuyên gia E tại thời điểm đó.

Hình 8 cho thấy cách số lượng mẫu được gán cho một chuyên gia thay đổi trong suốt quá trình huấn luyện và mất cân bằng. Như đã thảo luận trong Phần 4, một cách phổ biến để xử lý mất cân bằng là xác định một hệ số dung lượng nhân với dung lượng của mỗi chuyên gia, nhưng các hệ số dung lượng lớn hơn tạo ra chi phí lớn hơn trong việc sử dụng bộ nhớ, thời gian chạy và truyền dữ liệu giữa các thiết bị. Tuy nhiên, nếu một chuyên gia không có đủ dung lượng, các mẫu bị bỏ qua trong quá trình huấn luyện, điều này ảnh hưởng đến chất lượng mô hình.

Trong nghiên cứu trước đây, hệ số dung lượng được xác định tĩnh (cố định trong suốt quá trình huấn luyện) và toàn cầu (giống nhau cho tất cả chuyên gia). Tuy nhiên, các hệ số dung lượng tĩnh, toàn cầu không thể nắm bắt được các biến thể trong mất cân bằng trong suốt quá trình huấn luyện, cũng như các biến thể giữa các chuyên gia riêng lẻ. Trái ngược với các hệ số dung lượng tĩnh, toàn cầu, Hình 8 cho thấy cách DYNAMOE sử dụng các hệ số dung lượng khác nhau cho các chuyên gia khác nhau và cũng thay đổi các hệ số dung lượng trong suốt quá trình huấn luyện. Các đường màu sáng cho thấy các hệ số dung lượng mà DYNAMOE sử dụng cho một chuyên gia nhất định tại một điểm nhất định trong quá trình huấn luyện.

[Hình 8. Các hệ số dung lượng điều chỉnh động theo yêu cầu dung lượng thực tế của các chuyên gia.]

Trung bình trên tất cả các chuyên gia, hệ số dung lượng cần thiết để không bỏ mẫu luôn là 1.0 (theo định nghĩa). Tuy nhiên, việc tạo một chính sách RECOMPILE với hệ số dung lượng trung bình 1.0 và không bỏ mẫu là không khả thi: Vì có những biến thể nhỏ trong số lượng mẫu được gán cho một chuyên gia giữa mỗi lần lặp huấn luyện, chính sách này sẽ cần biên dịch lại mọi lần lặp đơn. Vì điều này sẽ loại bỏ bất kỳ tăng tốc nào, việc thiết kế một chính sách RECOMPILE với hệ số dung lượng trung bình nhỏ, ít bỏ mẫu và số lần biên dịch lại chấp nhận được là không đơn giản. Tuy nhiên, điều này giúp ích khi cho phép hệ số dung lượng trung bình trên 1.0. Do đó, sự đánh đổi runtime-accuracy cũng tồn tại khi sử dụng các hệ số dung lượng động. Hình 9 cho thấy các chính sách RECOMPILE với các hệ số dung lượng động trung bình khác nhau cung cấp sự đánh đổi hấp dẫn hơn so với việc sử dụng các hệ số dung lượng tĩnh.

[Hình 9. Các hệ số dung lượng động, cục bộ cung cấp sự đánh đổi runtime-accuracy tốt hơn so với các hệ số tĩnh, toàn cầu.]

5.3 Caching Gán Mẫu

Hình 10 cho thấy các gán mẫu thực sự hội tụ sớm trong quá trình huấn luyện. Trục y mô tả tỷ lệ các mẫu không được gán lại bởi mạng gating giữa hai epoch liên tiếp, đây là tỷ lệ các mẫu được cache đúng cách. Trong thí nghiệm được thực hiện trong Hình 10, chúng tôi bật caching gán mẫu nếu ít nhất 96% mẫu được cache đúng cách; caching được tắt nếu tỷ lệ phần trăm mẫu được cache đúng cách giảm xuống dưới 90%, điều này xảy ra trong một thời gian rất ngắn trong epoch 48. Chúng tôi cũng không bật caching trước epoch 10. Sử dụng chính sách này, caching mang lại tăng tốc 1.15×.

[Hình 10. Trong các giai đoạn với ít mẫu được gán lại, hệ thống chuyển sang sử dụng các gán mẫu được cache để tăng tốc. Hệ thống sẽ phát hiện thêm các giai đoạn với nhiều gán lại và chuyển về không sử dụng các gán được cache.]

Chúng tôi đã quan sát sự hội tụ sớm gán mẫu như trong Hình 10 trên các tập dữ liệu khác nhau và sử dụng các kiến trúc và siêu tham số khác nhau. Tuy nhiên, ngay cả khi các gán mẫu không hội tụ sớm, việc có tối ưu hóa caching bị vô hiệu hóa không có tác động đáng kể đến hiệu suất so với việc không chạy bất kỳ code liên quan đến caching nào.

Hơn nữa, chúng tôi quan sát rằng độ chính xác cuối cùng cũng như sự tiến triển độ chính xác trong suốt quá trình huấn luyện thường không bị ảnh hưởng bởi tối ưu hóa caching. Đối với quá trình huấn luyện được mô tả trong Hình 10, Phụ lục D cho thấy sự tiến triển độ chính xác test của mô hình theo thời gian như một ví dụ về việc tối ưu hóa caching ảnh hưởng ít như thế nào đến sự tiến triển.

6 KẾT LUẬN & NGHIÊN CỨU TƯƠNG LAI

Sử dụng ví dụ về Hỗn hợp Chuyên gia, chúng tôi chứng minh cách RECOMPILE cho phép sử dụng tài nguyên tính toán hiệu quả hơn cho các mô hình với hành vi động. Trong khi thuật toán MoE đặc biệt nhằm giảm chi phí tính toán khi mở rộng mô hình, các triển khai hiện tại thường bao gồm chi phí 2× về FLOPs và việc sử dụng bộ nhớ lớn hơn đáng kể chỉ để xử lý luồng dữ liệu động trong MoE. RECOMPILE không chỉ tiết kiệm thời gian và bộ nhớ mà còn cải thiện chất lượng mô hình vì ít mẫu bị bỏ trong quá trình huấn luyện và vì các mô hình có thể được mở rộng thêm do việc sử dụng tài nguyên hiệu quả hơn.

Mặc dù chúng tôi đã thành công chạy DYNAMOE trên nhiều node, việc mở rộng hệ thống của chúng tôi lên kích thước khổng lồ gồm vài trăm GPU vẫn cần được thử nghiệm. Cũng còn nhiều câu hỏi nghiên cứu tương lai liên quan đến MoE nói chung cũng như RECOMPILE nói riêng. Một số câu hỏi liên quan đến RECOMPILE được phác thảo ở đây:

• Điều chỉnh đồ thị: Không đơn giản để xác định khi nào và cách thay đổi đồ thị tính toán trong quá trình huấn luyện và suy luận. Các chính sách được sử dụng trong các thí nghiệm của chúng tôi tương đối ngây thơ và không thể đảm bảo gần với một số tối ưu, cũng như không thể khái quát hóa tốt cho các lựa chọn siêu tham số và tập dữ liệu khác. RECOMPILE cung cấp một framework nơi các thuật toán tốn kém cũng có thể phục vụ như một chính sách RECOMPILE mà không phát sinh chi phí trong thời gian huấn luyện tổng thể.

• Ánh xạ lại lên phần cứng: Đồ thị tính toán có thể được điều chỉnh nhiều trong suốt quá trình huấn luyện đến mức việc ánh xạ lại đồ thị sử dụng một chiến lược song song hóa khác có thể có lợi. Tuy nhiên, trong khi các điều chỉnh đồ thị tương đối rẻ, việc ánh xạ lại đồ thị có thể tạo ra chi phí lớn và nên được lập kế hoạch cẩn thận. Một giải pháp toàn diện cho chính sách RECOMPILE nên xem xét cả hai, điều chỉnh đồ thị và ánh xạ lại phần cứng, cùng lúc. Chính sách phải xem xét chi phí của việc ánh xạ lại đồ thị (phụ thuộc vào số lượng thành phần đồ thị được ánh xạ lên thiết bị khác) cũng như khả năng ánh xạ đó có lợi theo thời gian.

• Sự bất ổn huấn luyện: Sự bất ổn trong huấn luyện vẫn là một vấn đề của MoE và những tiến bộ theo hướng này cũng có lợi cho các tối ưu hóa RECOMPILE: Trong các giai đoạn không ổn định trong huấn luyện, caching gán mẫu được tắt và do đó không mang lại tăng tốc nào. Hơn nữa, trong thời gian có dao động lớn trong dung lượng chuyên gia, các hệ số dung lượng động sẽ thường xuyên phát hành biên dịch lại có thể có tác động tiêu cực đến tăng tốc trong các trường hợp cực đoan.

--- TRANG 9 ---
DynaMoE: Hỗn hợp Chuyên gia được tối ưu hóa động

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo tiếng Anh được duy trì nguyên bản]

--- TRANG 10 ---
DynaMoE: Hỗn hợp Chuyên gia được tối ưu hóa động

--- TRANG 11 ---
DynaMoE: Hỗn hợp Chuyên gia được tối ưu hóa động

PHỤ LỤC A: MẤT MÁT ĐẶC TẢ NHƯ TOÁN TỬ

Mất mát đặc tả yêu cầu dự đoán của mỗi chuyên gia được chọn và không hoạt động trên đầu ra mô hình thực tế (xem phương trình 2). Nếu k chuyên gia được gán cho mỗi mẫu, mất mát đặc tả do đó yêu cầu k * batch_size dự đoán chuyên gia – và không phải batch_size. Điều này làm cho việc hỗ trợ mất mát đặc tả theo cách tương tự như các hàm mất mát khác được hỗ trợ trở nên không thể.

Khi sử dụng mất mát đặc tả, đầu ra mạng phải bao gồm hai tensor: Các dự đoán chuyên gia được nối cho tính toán mất mát, và các dự đoán chuyên gia được tổng hợp làm đầu ra mô hình thực tế (ví dụ, để tính toán các metric như độ chính xác trên hiệu suất mô hình).

Trong DYNAMOE, các mô hình Mixture of Experts được định nghĩa bằng cách sử dụng ba toán tử đặc thù MoE:

• TopK: Lấy chỉ số và giá trị của k giá trị lớn nhất từ một danh sách n giá trị. Điều này được sử dụng để trích xuất các chuyên gia được chọn từ logit mạng gating.

• GroupBy: Cho một batch dữ liệu và một ánh xạ giữa các mẫu và chuyên gia, GroupBy nhóm các mẫu thành n tensor trong đó tensor thứ i chứa tất cả các mẫu được ánh xạ tới chuyên gia thứ i.

• Aggregate: Cho các dự đoán chuyên gia và trọng số tương ứng của chúng, tạo trung bình có trọng số trên các dự đoán. Trung bình có trọng số này là dự đoán đầu ra của MoE.

Trực quan, việc hỗ trợ mất mát đặc tả có thể được đạt được bằng cách thay đổi toán tử Aggregate để nhận một Boolean β xác định mất mát nào đang được sử dụng. Nếu β là true, mất mát đặc tả được sử dụng và Aggregate có hai đầu ra. Nếu không, mất mát hợp tác được sử dụng và Aggregate chỉ có một đầu ra. Bên dưới, điều này có nghĩa là sẽ có hai toán tử Aggregate khác nhau nhưng điều này sẽ được ẩn khỏi người dùng.

Tuy nhiên, nếu người dùng muốn xây dựng một mô hình phân cấp sử dụng mất mát đặc tả, Aggregate sẽ cần nhận hai đầu vào, các dự đoán chuyên gia được nối và các dự đoán chuyên gia được tổng hợp. Điều này sẽ yêu cầu một Boolean khác và sẽ dẫn đến bốn toán tử Aggregate khác nhau bên dưới. Hơn nữa, API sẽ trở nên khó hiểu vì không tự giải thích cách định nghĩa hai Boolean để biểu đạt chức năng đồ thị mong muốn. Thay vào đó, chúng tôi phơi bày một toán tử AggregateSpec đặc biệt cho người dùng, cho phép xây dựng đồ thị trực quan hơn.

Cả hai toán tử tổng hợp đều có một đầu ra duy nhất, trong đó Aggregate đầu ra các dự đoán chuyên gia được tổng hợp và AggregateSpec đầu ra các dự đoán chuyên gia được nối. Đầu ra của Aggregate tạo thành dự đoán cuối cùng của MoE – có nghĩa là đầu ra của Aggregate được sử dụng làm nhãn suy luận, hoặc để tính toán độ chính xác của mô hình. Khi sử dụng mất mát hợp tác, đầu ra của Aggregate cũng được sử dụng cho tính toán mất mát (xem Hình 12). Tuy nhiên, khi sử dụng mất mát đặc tả, đầu ra của Aggregate chỉ được sử dụng làm đầu ra mô hình nhưng không được sử dụng cho tính toán mất mát, vì tính toán mất mát yêu cầu tất cả dự đoán chuyên gia, tức là đầu ra của AggregateSpec. DYNAMOE cho phép người dùng dễ dàng thêm toán tử AggregateSpec và chỉ định rằng đầu ra của toán tử AggregateSpec nên được sử dụng cho tính toán mất mát. Điều này được mô tả trong Hình 13. Hình 11 hơn nữa mô tả đồ thị của một mô hình phân cấp – lưu ý cách các mất mát thậm chí có thể được trộn trong các mô hình phân cấp và các MoE bên trong có thể sử dụng mất mát hợp tác trong khi các MoE bên ngoài sử dụng mất mát đặc tả.

[Hình 11. Mô hình phân cấp với mất mát đặc tả]

[Hình 12. Mất mát hợp tác]

[Hình 13. Mất mát đặc tả]

Tóm lại, bằng cách phơi bày một toán tử bổ sung cho việc tính toán mất mát đặc tả, chúng tôi cho phép tính linh hoạt (tức là định nghĩa các mô hình phân cấp) trong khi đồng thời không đưa ra các Boolean gây nhầm lẫn không tự giải thích.

PHỤ LỤC B: TRIỂN KHAI RECOMPILE TRÊN FLEXFLOW

FlexFlow (Jia et al., 2018) là một framework DNN phân tán được xây dựng trên hệ thống lập trình Legion (Bauer et al., 2012). Một trong những khía cạnh của thiết kế Legion là nó có tính bất đồng bộ cao, khởi chạy các tác vụ có thể tiềm ẩn trước khi chúng có thể thực sự được thực thi. Điều này tạo ra một frontier khởi chạy dọc theo đó các tác vụ được khởi chạy, và một frontier thực thi dọc theo đó các tác vụ được thực thi. Hình 14 minh họa hai frontier này – lưu ý rằng các tác vụ giữa hai frontier đã được khởi chạy nhưng chưa được thực thi.

[Hình 14. Δlaunch chỉ ra số lần lặp forward/backward giữa frontier khởi chạy và thực thi và là một siêu tham số do người dùng đặt (ở đây, Δlaunch = 1). Như vậy, người dùng có thể kiểm soát chính xác sau bao nhiêu lần lặp sau một sự kiện nhất định việc biên dịch lại sẽ có hiệu lực.]

Khi thay đổi đồ thị tính toán của một mô hình, các thay đổi sẽ chỉ được phản ánh trong các tác vụ chưa được khởi chạy – tất cả các tác vụ đã khởi chạy vẫn sử dụng đồ thị tính toán gốc trước đó. Kết quả là, các điều chỉnh đồ thị tính toán có hiệu lực tại frontier khởi chạy.

Tuy nhiên, các điều chỉnh đồ thị được kích hoạt dựa trên các metric mô hình được tính toán tại frontier thực thi. Điều này tạo ra sự chênh lệch, nơi các điều chỉnh đồ thị chỉ có hiệu lực với một độ trễ nhất định sau khi chúng được kích hoạt.

RECOMPILE tận dụng sự chênh lệch này như một bộ đệm thời gian trong đó việc thực thi trigger RECOMPILE của người dùng và huấn luyện mô hình được chồng chéo. Trong quá trình thực thi trigger RECOMPILE, không có tác vụ mới nào có thể được khởi chạy vì không biết tác vụ nào cần khởi chạy. Không có các tác vụ được đệm nằm giữa hai frontier (được khởi chạy nhưng chưa thực thi), quá trình huấn luyện/suy luận của mô hình do đó cũng sẽ cần bị dừng cho đến khi có thể khởi chạy tác vụ mới (tức là trigger RECOMPILE đã hoàn thành thực thi). Tuy nhiên, miễn là còn một số tác vụ đã khởi chạy để thực thi, việc dừng khởi chạy tác vụ không dừng thực thi tác vụ.

Do đó, điều quan trọng là phải khởi chạy đủ tác vụ trước khi thực thi trigger RECOMPILE, sao cho việc thực thi hàm trigger hoàn thành trước khi tất cả các tác vụ đã khởi chạy được thực thi. Tuy nhiên, khởi chạy quá nhiều tác vụ trước có thể dẫn đến việc điều chỉnh chỉ có hiệu lực sau một độ trễ dài, có thể dẫn đến ít tăng tốc hơn hoặc giảm chất lượng mô hình. Do đó, điều quan trọng là RECOMPILE cho phép người dùng chỉ định số lượng tác vụ cần đệm trước khi thực thi trigger RECOMPILE.

Trong triển khai RECOMPILE của chúng tôi, người dùng chỉ định số lượng tác vụ cần đệm như số lần lặp huấn luyện/suy luận Δlaunch. RECOMPILE đảm bảo rằng Δlaunch lần lặp có giá trị tác vụ được khởi chạy trước khi trigger RECOMPILE được thực thi. Để thực thi rằng chính xác Δlaunch lần lặp huấn luyện/suy luận nằm giữa frontier thực thi và khởi chạy, RECOMPILE duy trì một hàng đợi cho mỗi metric mô hình. Khi khởi chạy một tác vụ trả về metric mô hình, một future cho metric đó được đẩy vào hàng đợi. Future này là một placeholder cho giá trị của metric, sẽ chỉ được biết khi tác vụ đã được thực thi. Khi tác vụ đã được thực thi, RECOMPILE sẽ lấy future tương ứng ra khỏi hàng đợi. Vì metric mô hình được tính toán chính xác một lần cho mỗi lần lặp, độ dài hàng đợi chỉ ra chính xác có bao nhiêu lần lặp huấn luyện/suy luận đã được khởi chạy nhưng chưa thực thi.

RECOMPILE giờ đây có thể đơn giản thực thi rằng độ dài của tất cả các hàng đợi Future tương ứng chính xác với Δlaunch trước khi thực thi trigger RECOMPILE. Nếu hàng đợi quá dài, RECOMPILE đợi cho đến khi đúng số lượng metric mô hình đã được tính toán (hoàn thành thực thi) rồi gọi trigger RECOMPILE với các metric mô hình được tính toán gần đây nhất. Nếu hàng đợi quá ngắn, RECOMPILE đơn giản khởi chạy thêm tác vụ trước khi thực thi trigger. Hàng đợi chỉ quá ngắn trong hai tình huống: Ở đầu và cuối huấn luyện, và khi người dùng tăng Δlaunch. Lưu ý rằng Δlaunch luôn có thể được thay đổi tại bất kỳ thời điểm nào trong runtime.

Trong Hình 14, Δlaunch là 1 lần lặp và DYNAMOE đợi trên Future F1 trước khi nó sẽ tiếp tục khởi chạy tác vụ và cuối cùng thêm Future F3. Trong Hình 14, bảy tác vụ giữa frontier thực thi và khởi chạy tương ứng với một lần lặp, được chỉ ra bằng mũi tên trên có nhãn Δlaunch. Trong khoảnh khắc được mô tả trong Hình 14, tác vụ trả về giá trị F1 vừa được thực thi, có nghĩa là F1 sẽ được lấy ra khỏi hàng đợi và các tác vụ khác sẽ được khởi chạy. Đợi F1 đảm bảo rằng DYNAMOE không khởi chạy nhiều tác vụ hơn Δlaunch = 1 lần lặp phía trước.

PHỤ LỤC C: HỖ TRỢ RECOMPILE TRONG CÁC FRAMEWORK KHÁC

RECOMPILE cũng có thể được triển khai trên các framework ML hiện có khác. Phần này cho thấy rằng những framework này hiện tại không thỏa mãn các yêu cầu được thảo luận trong Phần 3 và do đó cũng sẽ được hưởng lợi từ việc hỗ trợ một cơ chế như RECOMPILE. Để tham khảo, các yêu cầu được nêu lại như sau:

1. Người dùng có thể giám sát các metric mô hình với chi phí không đáng kể. Một metric mô hình có thể là bất cứ thứ gì thông báo cho người dùng về trạng thái hiện tại của mô hình trong quá trình suy luận hoặc huấn luyện, chẳng hạn như mất mát huấn luyện.

2. Người dùng có thể thay đổi đồ thị tính toán với ít chi phí. Khi nào và cách đồ thị được thay đổi được xác định bởi một trigger biên dịch lại do người dùng cung cấp, có thể dựa quyết định của nó trên các metric mô hình.

3. Việc thực thi trigger RECOMPILE và tính toán ánh xạ phần cứng của đồ thị mới được chồng chéo với suy luận/huấn luyện sử dụng đồ thị hiện tại. Ví dụ: Một người dùng có thể thực hiện một tính toán tốn kém để quyết định cách thay đổi đồ thị hoặc thực hiện một tìm kiếm dài cho một chiến lược song song hóa mới trong khi quá trình huấn luyện mô hình tiếp tục song song.

Vì RECOMPILE chỉ yêu cầu khả năng thực hiện các tính toán RECOMPILE trên CPU song song với việc huấn luyện mô hình MoE trên GPU, RECOMPILE có thể được triển khai cho cả đồ thị tính toán tĩnh và động. DYNAMOE chọn triển khai RECOMPILE cho đồ thị tính toán tĩnh, cho phép nó kết hợp điều tốt nhất của cả hai thế giới: Các đồ thị tính toán tĩnh thường nhanh hơn vì chúng cho phép các tối ưu hóa runtime không thể thực hiện được đối với các đồ thị động. Chúng tôi cho thấy DYNAMOE vượt trội hơn một hệ thống MoE sử dụng đồ thị tính toán động của PyTorch. Tuy nhiên, một đồ thị tính toán tĩnh với RECOMPILE vẫn cho phép các điều chỉnh đồ thị động – chúng tôi sẽ sau đó cho thấy rằng những điều chỉnh này thậm chí vượt quá những gì có thể thực hiện được trong các framework như PyTorch.

Trong phần tiếp theo, chúng tôi sẽ xem xét mức độ TensorFlow và PyTorch có thể thỏa mãn ba yêu cầu trong Phần 3. Trong khi các framework phổ biến khác không được thảo luận chi tiết, PyTorch và TensorFlow nên phục vụ như những ví dụ chung về các framework phổ biến nhất sử dụng đồ thị tính toán tĩnh và động tương ứng. Các framework phổ biến khác cũng rơi vào một trong hai danh mục này: Caffe, Jax, Theano, và CNTK sử dụng đồ thị tính toán tĩnh trong khi Chainer và DyNet sử dụng đồ thị tính toán động.

RECOMPILE trong PyTorch. Trong khi PyTorch có thể thỏa mãn hai yêu cầu đầu tiên, nó không thể thỏa mãn yêu cầu cuối cùng.

1. Các metric mô hình có thể được giám sát trực tiếp trong PyTorch do hỗ trợ xây dựng mô hình động của nó. API của PyTorch thậm chí trực quan hơn so với DYNAMOE: trong khi các trigger RECOMPILE và điều chỉnh đồ thị được định nghĩa như các hàm trong DYNAMOE, người dùng có thể đơn giản xen kẽ chúng với code xây dựng đồ thị trong PyTorch.

2. PyTorch hỗ trợ các sửa đổi động của các thành phần đồ thị trong runtime.

3. PyTorch cho phép xen kẽ thực thi đồ thị với thực thi code kích hoạt hoặc thực hiện điều chỉnh đồ thị (trigger RECOMPILE). Vì PyTorch khởi chạy các toán tử GPU bất đồng bộ, PyTorch cũng có thể thực thi các trigger RECOMPILE song song với huấn luyện/suy luận mô hình trên GPU. Tuy nhiên, PyTorch không phơi bày API cho phép các trigger RECOMPILE sử dụng các metric mô hình được tính toán Δlaunch lần lặp huấn luyện/suy luận trước, nơi Δlaunch có thể được điều chỉnh động. Thay vào đó, việc sử dụng các metric mô hình trong trigger RECOMPILE sẽ chèn một phụ thuộc vào giá trị được tính toán trong các lần lặp huấn luyện/suy luận hiện tại, điều này sẽ để lại ít thời gian cho trigger RECOMPILE thực thi trước khi thực thi GPU dừng. Để hỗ trợ các tối ưu hóa RECOMPILE, một cơ chế tương tự như trong phần B do đó sẽ cần được triển khai, ví dụ sử dụng Torch.Futures.

RECOMPILE trong TensorFlow. TensorFlow chỉ thỏa mãn yêu cầu đầu tiên nhưng không thỏa mãn hai yêu cầu sau.

1. TensorFlow cung cấp callback để giám sát các metric mô hình tương tự như DYNAMOE. Tuy nhiên, TensorFlow không hỗ trợ điều chỉnh đồ thị trong hàm callback, dẫn đến không thể hỗ trợ yêu cầu thứ hai.

2. Điều chỉnh đồ thị trong TensorFlow chỉ có thể xảy ra bên ngoài một session. Để thực hiện một biên dịch lại trong quá trình huấn luyện, người dùng cần phải chia quy trình huấn luyện của họ thành nhiều lần chạy của các Session riêng lẻ, giữa đó một biên dịch lại có thể được phát hành. Điều này là không khả thi, vì nó sẽ yêu cầu checkpoint và reload toàn bộ mô hình mỗi khi một session mới được chạy – hỗ trợ các biên dịch lại ở mức granularity thời gian của DYNAMOE, sẽ yêu cầu lưu trữ và loading toàn bộ mô hình sau mỗi lần lặp batch huấn luyện.

3. Thực thi code người dùng tùy chỉnh trong quá trình thực thi đồ thị cũng yêu cầu chia huấn luyện thành nhiều session nhỏ với code người dùng được thực thi giữa các session. Không có khả năng chồng chéo tính toán; thực thi đồ thị dừng lại trong khi code người dùng chạy, và chi phí từ việc sử dụng nhiều session nhỏ sẽ là một penalty hiệu suất bổ sung.

Kết luận, TensorFlow và PyTorch không thỏa mãn bản địa ba yêu cầu áp đặt lên RECOMPILE. Chúng không thể hỗ trợ các tối ưu hóa như những tối ưu hóa được đề xuất trong Phần 4, và sẽ được hưởng lợi từ việc cũng triển khai RECOMPILE.

PHỤ LỤC D: TÁC ĐỘNG CỦA CACHING LÊN ĐỘ CHÍNH XÁC

Hình 15 cho thấy sự tiến triển độ chính xác khi bật caching thích ứng trên các phần lớn của quá trình huấn luyện hoặc vô hiệu hóa nó trên toàn bộ quá trình huấn luyện. Lưu ý rằng trục x mô tả runtime chứ không phải training epoch. Cả hai mô hình đều được chạy cho cùng số lượng epoch huấn luyện.

[Hình 15. Bật caching có ít tác động đến tiến triển huấn luyện và mang lại tăng tốc 1.15×.]
