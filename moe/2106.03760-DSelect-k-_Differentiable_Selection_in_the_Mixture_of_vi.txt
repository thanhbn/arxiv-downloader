# 2106.03760.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2106.03760.pdf
# Kích thước file: 3290911 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
DSelect-k: Lựa chọn có thể vi phân trong Hỗn hợp của các Chuyên gia với Ứng dụng cho Học Đa nhiệm vụ
Hussein Hazimeh1, Zhe Zhao1, Aakanksha Chowdhery1, Maheswaran Sathiamoorthy1
Yihua Chen1,Rahul Mazumder2,Lichan Hong1,Ed H. Chi1
1Google, {hazimeh,zhezhao,chowdhery,nlogn,yhchen,lichan,edchi}@google.com
2Massachusetts Institute of Technology, rahulmaz@mit.edu
Tóm tắt
Kiến trúc Hỗn hợp các Chuyên gia (MoE) đang cho thấy kết quả đầy hứa hẹn trong việc cải thiện chia sẻ tham số trong học đa nhiệm vụ (MTL) và trong việc mở rộng quy mô các mạng nơ-ron có dung lượng cao. Các mô hình MoE tiên tiến nhất sử dụng một "cổng thưa" có thể huấn luyện để chọn một tập con của các chuyên gia cho mỗi ví dụ đầu vào. Mặc dù về mặt khái niệm hấp dẫn, các cổng thưa hiện có, chẳng hạn như Top-k, không trơn tru. Việc thiếu tính trơn tru có thể dẫn đến các vấn đề hội tụ và hiệu suất thống kê khi huấn luyện bằng các phương pháp dựa trên gradient. Trong bài báo này, chúng tôi phát triển DSelect-k: một cổng liên tục có thể vi phân và thưa cho MoE, dựa trên công thức mã hóa nhị phân mới. Cổng có thể được huấn luyện bằng các phương pháp bậc nhất, chẳng hạn như descent gradient ngẫu nhiên, và cung cấp kiểm soát rõ ràng về số lượng chuyên gia để chọn. Chúng tôi chứng minh hiệu quả của DSelect-k trên cả dữ liệu MTL tổng hợp và thực tế với tối đa 128 nhiệm vụ. Các thí nghiệm của chúng tôi chỉ ra rằng DSelect-k có thể đạt được những cải thiện có ý nghĩa thống kê trong dự đoán và lựa chọn chuyên gia so với các cổng MoE phổ biến. Đáng chú ý, trên một hệ thống gợi ý quy mô lớn trong thế giới thực, DSelect-k đạt được hơn 22% cải thiện hiệu suất dự đoán so với Top-k. Chúng tôi cung cấp một triển khai mã nguồn mở của DSelect-k1.

1 Giới thiệu
Hỗn hợp các Chuyên gia (MoE) [14] là cơ sở của nhiều mô hình học sâu tiên tiến nhất. Ví dụ, các lớp dựa trên MoE đang được sử dụng để thực hiện tính toán hiệu quả trong các mạng nơ-ron có dung lượng cao và để cải thiện chia sẻ tham số trong học đa nhiệm vụ (MTL) [33,22,21]. Ở dạng đơn giản nhất, một MoE bao gồm một tập hợp các chuyên gia (mạng nơ-ron) và một cổng có thể huấn luyện. Cổng gán trọng số cho các chuyên gia trên cơ sở từng ví dụ, và MoE xuất ra một kết hợp có trọng số của các chuyên gia. Cơ chế tạo trọng số theo từng ví dụ này cho phép các chuyên gia chuyên biệt hóa trong các phân vùng khác nhau của không gian đầu vào, điều này có tiềm năng cải thiện hiệu suất dự đoán và khả năng diễn giải. Trong Hình 1 (trái), chúng tôi cho thấy một ví dụ về kiến trúc MoE đơn giản có thể được sử dụng như một bộ học độc lập hoặc như một lớp trong mạng nơ-ron.

Tài liệu về MoE truyền thống tập trung vào các cổng dựa trên softmax, trong đó tất cả các chuyên gia được gán trọng số khác không [17]. Để nâng cao hiệu quả tính toán và khả năng diễn giải của các mô hình MoE, các nghiên cứu gần đây sử dụng các cổng thưa chỉ gán trọng số khác không cho một tập con nhỏ của các chuyên gia [1,33,29,21]. Các cổng thưa hiện có không thể vi phân, và các thuật toán học tăng cường thường được sử dụng để huấn luyện [1,29]. Trong một công trình thú vị, [33] đã giới thiệu một cổng thưa mới (cổng Top-k) và đề xuất huấn luyện nó bằng descent gradient ngẫu nhiên (SGD). Khả năng huấn luyện cổng bằng SGD rất hấp dẫn vì nó cho phép huấn luyện đầu cuối đến cuối. Tuy nhiên, cổng Top-k không liên tục, điều này có thể dẫn đến các vấn đề hội tụ trong SGD ảnh hưởng đến hiệu suất thống kê (như chúng tôi chứng minh trong các thí nghiệm của mình).

1https://github.com/google-research/google-research/tree/master/dselect_k_moe
Hội nghị lần thứ 35 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2021).arXiv:2106.03760v3 [cs.LG] 31 Dec 2021

--- TRANG 2 ---
Hình 1: (Trái): Một ví dụ về MoE có thể được sử dụng như một bộ học độc lập hoặc lớp trong mạng nơ-ron. Ở đây "Ei" biểu thị chuyên gia thứ i. (Phải): Một MoE đa cổng để học đồng thời hai nhiệm vụ. "Task i NN" là một mạng nơ-ron tạo ra đầu ra của Nhiệm vụ i.

Trong bài báo này, chúng tôi giới thiệu DSelect-k: một cổng liên tục có thể vi phân và thưa cho MoE. Với một tham số k do người dùng chỉ định, cổng chọn tối đa k trong số n chuyên gia. Kiểm soát rõ ràng về độ thưa này dẫn đến một bài toán tối ưu hóa có ràng buộc về số lượng, điều này là thách thức về mặt tính toán. Để vượt qua thách thức này, chúng tôi đề xuất một công thức lại không có ràng buộc mới tương đương với bài toán gốc. Bài toán được công thức lại sử dụng một sơ đồ mã hóa nhị phân để ngầm thực thi ràng buộc về số lượng. Chúng tôi chứng minh rằng bằng cách làm mượt cẩn thận các biến mã hóa nhị phân, bài toán được công thức lại có thể được tối ưu hóa hiệu quả bằng các phương pháp bậc nhất như SGD. DSelect-k có một lợi thế độc đáo so với các phương pháp hiện có về mặt tính gọn và hiệu quả tính toán. Số lượng tham số được sử dụng bởi DSelect-k là logarit theo số lượng chuyên gia, trái ngược với tuyến tính trong các cổng hiện có như Top-k. Hơn nữa, đầu ra của DSelect-k có thể được tính toán hiệu quả thông qua một biểu thức dạng đóng đơn giản. Ngược lại, các phương pháp có thể vi phân tiên tiến nhất để chọn tập con k ngẫu nhiên và thư giãn Top-k, chẳng hạn như [26,40]2, yêu cầu giải một bài toán con tối ưu hóa (cho mỗi ví dụ đầu vào) để tính đầu ra của cổng.

DSelect-k hỗ trợ hai cơ chế cổng: theo từng ví dụ và tĩnh. Cổng theo từng ví dụ là kỹ thuật cổng cổ điển được sử dụng trong các mô hình MoE, trong đó các trọng số được gán cho các chuyên gia là một hàm của ví dụ đầu vào [14,33]. Trong cổng tĩnh, một tập con các chuyên gia được chọn và các trọng số tương ứng không phụ thuộc vào đầu vào [29]. Dựa trên các thí nghiệm của chúng tôi, mỗi cơ chế cổng có thể vượt trội hơn cái kia trong các thiết lập nhất định. Do đó, chúng tôi nghiên cứu cả hai cơ chế và ủng hộ việc thử nghiệm với từng cơ chế.

MTL là một lĩnh vực quan trọng nơi các mô hình MoE nói chung, và cổng của chúng tôi nói riêng, có thể hữu ích. Mục tiêu của MTL là học nhiều nhiệm vụ đồng thời bằng cách sử dụng một mô hình được chia sẻ. So với học nhiệm vụ đơn lẻ thông thường, MTL có thể đạt được hiệu suất tổng quát hóa tốt hơn thông qua việc khai thác các mối quan hệ nhiệm vụ [4]. Một vấn đề quan trọng trong MTL là cách chia sẻ tham số mô hình giữa các nhiệm vụ [31]. Ví dụ, chia sẻ tham số giữa các nhiệm vụ không liên quan có thể làm giảm hiệu suất. MoE đa cổng [22] là một kiến trúc linh hoạt cho phép học những gì cần chia sẻ giữa các nhiệm vụ. Hình 1 (phải) cho thấy một ví dụ về MoE đa cổng, trong trường hợp đơn giản của hai nhiệm vụ. Ở đây, mỗi nhiệm vụ có cổng riêng của nó để kiểm soát thích ứng mức độ chia sẻ tham số. Trong các thí nghiệm của chúng tôi, chúng tôi nghiên cứu hiệu quả của DSelect-k trong bối cảnh MoE đa cổng.

Đóng góp: Ở mức độ cao, đóng góp chính của chúng tôi là DSelect-k: một cổng liên tục có thể vi phân và thưa mới cho MoE, có thể được huấn luyện trực tiếp bằng các phương pháp bậc nhất. Các đóng góp kỹ thuật của chúng tôi có thể được tóm tắt như sau. (i) Cổng chọn (tối đa) k trong số n chuyên gia, trong đó k là một tham số do người dùng chỉ định. Điều này dẫn đến một bài toán tối ưu hóa có ràng buộc về số lượng đầy thách thức. Để đối phó với thách thức này, chúng tôi phát triển một công thức lại không có ràng buộc mới, và chúng tôi chứng minh rằng nó tương đương với bài toán gốc. Việc công thức lại sử dụng một sơ đồ mã hóa nhị phân ngầm áp đặt ràng buộc về số lượng bằng cách sử dụng các mã nhị phân có thể học. (ii) Để làm cho việc công thức lại không có ràng buộc trơn tru, chúng tôi thư giãn và làm mượt các biến nhị phân. Chúng tôi chứng minh rằng, với việc khởi tạo và chính quy hóa cẩn thận, bài toán kết quả có thể được tối ưu hóa bằng các phương pháp bậc nhất như SGD. (iii) Chúng tôi thực hiện một loạt thí nghiệm trên các bộ dữ liệu MTL tổng hợp và thực tế, cho thấy rằng cổng của chúng tôi có tính cạnh tranh với các cổng tiên tiến nhất về chia sẻ tham số và hiệu suất dự đoán. (iv) Chúng tôi cung cấp một triển khai mã nguồn mở của DSelect-k.

2Các phương pháp này không được thiết kế đặc biệt cho MoE.

--- TRANG 3 ---
1.1 Công trình liên quan

MoE và Tính toán có điều kiện: Kể từ khi MoE được giới thiệu bởi [14], một khối lượng công trình thú vị đã mở rộng và nghiên cứu mô hình này, ví dụ, xem [17,13,16]. Gần đây, các mô hình dựa trên MoE đang cho thấy thành công trong học sâu. Ví dụ, [33] giới thiệu cổng Top-k thưa cho MoE và cho thấy những cải thiện tính toán đáng kể trong các nhiệm vụ dịch máy; chúng tôi thảo luận về các kết nối chính xác với cổng này trong Mục 2. Cổng Top-k cũng đã được sử dụng trong một số mô hình học sâu tiên tiến nhất đã xem xét các nhiệm vụ MTL, ví dụ, [21,28,9]. Công trình của chúng tôi cũng liên quan đến các mô hình tính toán có điều kiện kích hoạt các phần của mạng nơ-ron dựa trên đầu vào [2,1,33,12,36]. Không giống như DSelect-k, các công trình này dựa trên các mô hình không thể vi phân, hoặc các heuristic trong đó các mô hình huấn luyện và suy luận khác nhau.

Lựa chọn tập con k ngẫu nhiên và Thư giãn Top-k: Một dòng công trình liên quan tập trung vào lựa chọn tập con k ngẫu nhiên trong mạng nơ-ron, ví dụ, xem [26,5,39] và các tài liệu tham khảo trong đó. Cụ thể, các công trình này đề xuất các phương pháp có thể vi phân để lấy mẫu các tập con k từ một phân phối phân loại, dựa trên các mở rộng hoặc tổng quát hóa của thủ thuật Gumbel-softmax [23,15]. Tuy nhiên, trong MoE chúng tôi xem xét lựa chọn tập con xác định—tính xác định là một giả định phổ biến trong các mô hình MoE có thể cải thiện khả năng diễn giải và cho phép các triển khai hiệu quả [14,17,33]. Ngược lại, các phương pháp ngẫu nhiên được mô tả ở trên phù hợp trong các ứng dụng có phân phối lấy mẫu cơ bản, chẳng hạn như trong suy luận biến phân [19]. Một công trình liên quan khác là việc thư giãn có thể vi phân của toán tử Top-k được đề xuất bởi [40]. Tất cả các công trình được đề cập ở trên thực hiện huấn luyện dày đặc (tức là, các gradient của tất cả các chuyên gia, ngay cả khi không được chọn, sẽ phải được tính toán trong quá trình lan truyền ngược), trong khi DSelect-k có thể (ở một mức độ nào đó) khai thác tính thưa để tăng tốc độ huấn luyện, như chúng tôi sẽ thảo luận trong Mục 2 và 3. Hơn nữa, khung lựa chọn tập con k ngẫu nhiên trong [26] (bao gồm một số công trình trước đó) và thư giãn Top-k trong [40] yêu cầu giải một bài toán con tối ưu hóa để tính đầu ra cổng—mỗi ví dụ sẽ yêu cầu giải một bài toán con riêng biệt trong thiết lập cổng theo từng ví dụ, điều này có thể tốn kém về mặt tính toán. Ngược lại, đầu ra của DSelect-k được tính toán hiệu quả thông qua một biểu thức dạng đóng.

Các phép biến đổi thưa đến Simplex: Đây là các biến thể thưa của hàm softmax có thể xuất ra các vector xác suất thưa, ví dụ, xem [24,27,6,3]. Mặc dù tương tự như công trình của chúng tôi ở chỗ chúng xuất ra các vector xác suất thưa, các phép biến đổi này không thể kiểm soát mức độ thưa một cách chính xác như DSelect-k (thông qua ràng buộc về số lượng). Do đó, các phép biến đổi này có thể gán cho một số ví dụ hoặc nhiệm vụ các kết hợp thưa và các kết hợp dày đặc khác.

MTL: Trong Phụ lục A, chúng tôi xem xét tài liệu liên quan về MTL.

2 Cổng trong Hỗn hợp các Chuyên gia

Trong mục này, chúng tôi đầu tiên xem xét kiến trúc MoE và các cổng phổ biến, sau đó thảo luận về cách các cổng này so sánh với đề xuất của chúng tôi. Chúng tôi sẽ giả định rằng các đầu vào cho MoE thuộc về một không gian X⊆Rp. Ở dạng đơn giản nhất, MoE bao gồm một tập hợp n chuyên gia (mạng nơ-ron) fi:X→Ru, i∈{1;2;...;n}, và một cổng g:X→Rn gán trọng số cho các chuyên gia. Đầu ra của cổng được giả định là một vector xác suất, tức là, g(x)≥0 và ∑ni=1g(x)i = 1, cho bất kỳ x∈X. Với một ví dụ x∈X, đầu ra tương ứng của MoE là một kết hợp có trọng số của các chuyên gia:

∑ni=1fi(x)g(x)i: (1)

Tiếp theo, chúng tôi thảo luận về hai lựa chọn phổ biến cho cổng g(·) có thể được tối ưu hóa trực tiếp bằng SGD.

Cổng Softmax: Một mô hình cổ điển cho g(x) là cổng softmax: σ(Ax+b), trong đó σ(·) là hàm softmax, A∈Rn×p là một ma trận trọng số có thể huấn luyện, và b∈Rn là một vector bias [17]. Cổng này dày đặc, theo nghĩa là tất cả các chuyên gia được gán xác suất khác không. Lưu ý rằng cổng tĩnh (tức là, cổng không phụ thuộc vào ví dụ đầu vào) có thể được thu được bằng cách đặt A = 0.

Cổng Top-k: Đây là một biến thể thưa của cổng softmax trả về một vector xác suất chỉ có k mục khác không [33]. Cổng Top-k được định nghĩa bởi σ(KeepTopK(Ax+b)), trong đó đối với bất kỳ vector v, KeepTopK(v)i := vi nếu vi nằm trong k phần tử hàng đầu của v, và KeepTopK(v)i := -∞ ngược lại3. Cổng này hấp dẫn về mặt khái niệm vì nó cho phép kiểm soát trực tiếp số lượng chuyên gia để chọn

3Để cân bằng tải giữa các chuyên gia, [33] thêm nhiễu và các bộ chính quy hóa bổ sung vào mô hình.

--- TRANG 4 ---
Hình 2: Trọng số chuyên gia được xuất ra bởi Top-k (trái) và DSelect-k (phải) trong quá trình huấn luyện trên dữ liệu tổng hợp được tạo từ một MoE, dưới cổng tĩnh. Mỗi màu đại diện cho một chuyên gia riêng biệt. Ở đây DSelect-k khôi phục các chuyên gia thực được sử dụng bởi mô hình tạo dữ liệu, trong khi Top-k không khôi phục và thể hiện hành vi dao động. Xem Phụ lục C.2 để biết chi tiết về dữ liệu và thiết lập.

và được huấn luyện bằng SGD. Hơn nữa, cổng Top-k hỗ trợ huấn luyện có điều kiện: trong lan truyền ngược, đối với mỗi ví dụ đầu vào, chỉ cần tính toán các gradient của loss w.r.t. k phần tử hàng đầu. Với một triển khai cẩn thận, huấn luyện có điều kiện có thể dẫn đến tiết kiệm tính toán. Tuy nhiên, cổng Top-k không liên tục, điều này ngụ ý rằng gradient không tồn tại tại một số đầu vào nhất định. Điều này có thể có vấn đề khi huấn luyện được thực hiện bằng các phương pháp dựa trên gradient. Để có cái nhìn sâu sắc hơn, trong Hình 2 (trái), chúng tôi vẽ đồ thị các trọng số chuyên gia được chọn bởi cổng Top-k trong quá trình huấn luyện với SGD. Kết quả chỉ ra một hành vi dao động trong đầu ra của cổng Top-k, có thể được quy cho bản chất không liên tục của nó: một thay đổi nhỏ trong đầu vào có thể dẫn đến "nhảy" trong đầu ra.

So sánh với DSelect-k: Chúng tôi phát triển DSelect-k trong Mục 3. Ở đây chúng tôi trình bày một so sánh cấp cao giữa DSelect-k và Top-k. Tương tự như Top-k, DSelect-k có thể chọn k trong số n chuyên gia và có thể được huấn luyện bằng các phương pháp tối ưu hóa dựa trên gradient. Một lợi thế lớn của DSelect-k so với Top-k là nó có thể vi phân liên tục, điều này dẫn đến việc lựa chọn các chuyên gia ổn định hơn trong quá trình huấn luyện—xem Hình 2 (phải). Trong quá trình suy luận, DSelect-k chỉ cần đánh giá một tập con các chuyên gia, điều này có thể dẫn đến tiết kiệm tính toán. Tuy nhiên, DSelect-k chỉ hỗ trợ huấn luyện có điều kiện một phần. Ở đầu quá trình huấn luyện, nó sử dụng tất cả các chuyên gia có sẵn, vì vậy huấn luyện có điều kiện không thể thực hiện được. Như chúng tôi thảo luận trong Mục 3, sau một thời điểm nhất định trong quá trình huấn luyện, DSelect-k hội tụ đến một tập con nhỏ các chuyên gia, và sau đó huấn luyện có điều kiện trở nên khả thi. Các thí nghiệm của chúng tôi chỉ ra rằng DSelect-k có thể có lợi thế đáng kể so với Top-k về hiệu suất dự đoán và lựa chọn chuyên gia, vì vậy việc hỗ trợ đầy đủ cho huấn luyện có điều kiện trong Top-k dường như có cái giá của hiệu suất thống kê.

3 Cổng có thể vi phân và thưa

Trong mục này, chúng tôi phát triển DSelect-k, cho cả thiết lập cổng tĩnh và theo từng ví dụ. Đầu tiên, chúng tôi giới thiệu thiết lập bài toán và ký hiệu. Để đơn giản hóa việc trình bày, chúng tôi sẽ phát triển cổng cho một nhiệm vụ học có giám sát đơn lẻ, và chúng tôi lưu ý rằng cùng một cổng có thể được sử dụng trong các mô hình MTL. Chúng tôi giả định rằng nhiệm vụ có một không gian đầu vào X⊆Rp, một không gian đầu ra Y, và một hàm loss liên quan ℓ:Y×R→R. Chúng tôi biểu thị tập hợp N ví dụ huấn luyện bởi D = {(xi,yi)∈X×Y}Ni=1. Chúng tôi xem xét một mô hình học được định nghĩa bởi MoE trong Phương trình (1). Để đơn giản, chúng tôi giả định rằng các chuyên gia có giá trị vô hướng và thuộc về một lớp các hàm liên tục H. Chúng tôi giả định rằng số lượng chuyên gia n = 2m cho một số nguyên m—trong Phụ lục B.2, chúng tôi thảo luận về cách cổng có thể được mở rộng cho n tùy ý. Để thuận tiện, với một số nguyên không âm i, chúng tôi biểu thị tập hợp {1;2;...;i} bởi [i].

Trong Mục 3.1, chúng tôi phát triển DSelect-k cho thiết lập cổng tĩnh. Sau đó, trong Mục 3.2, chúng tôi tổng quát hóa nó cho thiết lập theo từng ví dụ.

3.1 DSelect-k cho Cổng tĩnh

Mục tiêu của chúng tôi ở đây là phát triển một cổng tĩnh chọn một kết hợp lồi của tối đa k trong số n chuyên gia. Đầu ra của cổng có thể được coi như một vector xác suất w với tối đa k mục khác không, trong đó wi là trọng số được gán cho chuyên gia fi. Một cách tự nhiên để tối thiểu hóa rủi ro thực nghiệm của mô hình MoE là bằng cách giải bài toán sau:

--- TRANG 5 ---
min
f1,...fn,w
1/N ∑(x,y)∈D ℓ(y, ∑ni=1 fi(x)wi) (2a)

s.t: ||w||0 ≤ k (2b)
     ∑ni=1 wi = 1, w ≥ 0: (2c)

Trong phần trên, chuẩn L0 của w, ||w||0, bằng số lượng các mục khác không trong w. Do đó, ràng buộc về số lượng (2b) đảm bảo rằng cổng chọn tối đa k chuyên gia. Bài toán (2) là một bài toán tối ưu hóa tổ hợp không phù hợp với SGD do ràng buộc về số lượng (2b) và các ràng buộc simplex trong (2c). Trong phần tiếp theo của mục này, chúng tôi đầu tiên biến đổi Bài toán (2) thành một bài toán tối ưu hóa không có ràng buộc tương đương, dựa trên một sơ đồ mã hóa nhị phân. Tuy nhiên, bài toán không có ràng buộc không thể được xử lý trực tiếp bằng SGD do sự hiện diện của các biến nhị phân. Do đó, trong một phép biến đổi thứ hai, chúng tôi làm mượt các biến nhị phân, điều này dẫn đến một bài toán tối ưu hóa phù hợp với SGD.

Lộ trình: Trong Mục 3.1.1, chúng tôi giới thiệu bộ chọn chuyên gia đơn: một cấu trúc để chọn 1 trong số n chuyên gia bằng cách sử dụng mã hóa nhị phân. Trong Mục 3.1.2, chúng tôi tận dụng bộ chọn chuyên gia đơn để biến đổi Bài toán (2) thành một bài toán không có ràng buộc. Sau đó, trong Mục 3.1.3, chúng tôi làm mượt bài toán không có ràng buộc và thảo luận về cách SGD có thể được áp dụng.

3.1.1 Lựa chọn Chuyên gia Đơn bằng Mã hóa Nhị phân

Bộ chọn chuyên gia đơn (viết tắt là bộ chọn) là một cấu trúc cơ bản mà chúng tôi sẽ sử dụng sau này để chuyển đổi Bài toán (2) thành một bài toán tối ưu hóa không có ràng buộc. Ở mức độ cao, bộ chọn chuyên gia đơn chọn chỉ số của 1 trong số n chuyên gia và trả về mã hóa một-nóng của lựa chọn. Ví dụ, trong trường hợp 4 chuyên gia, bộ chọn có thể chọn chuyên gia đầu tiên bằng cách trả về vector nhị phân [1 0 0 0]T. Nói chung, bộ chọn có thể chọn bất kỳ chuyên gia nào, và lựa chọn của nó được xác định bởi một tập hợp các biến mã hóa nhị phân, như chúng tôi sẽ mô tả tiếp theo.

Bộ chọn được tham số hóa bởi m (nhớ lại rằng m = log2 n) biến nhị phân, z1,z2,...,zm, trong đó chúng tôi xem các biến này một cách tập thể như một số nhị phân: zmzm-1...z1. Số nguyên được biểu diễn bởi số nhị phân sau xác định chuyên gia nào sẽ chọn. Chính thức hơn, đặt l là số nguyên được biểu diễn bởi số nhị phân zmzm-1...z1. Bộ chọn là một hàm r:Rm→{0,1}n ánh xạ z := [z1,z2,...,zm]T đến mã hóa một-nóng của số nguyên (l+1). Ví dụ, nếu tất cả các zi đều là 0, thì bộ chọn trả về mã hóa một-nóng của số nguyên 1. Tiếp theo, chúng tôi định nghĩa bộ chọn r(z). Để dễ dàng trình bày, chúng tôi bắt đầu với trường hợp đặc biệt của 4 chuyên gia và sau đó tổng quát hóa cho n chuyên gia.

Trường hợp đặc biệt của 4 chuyên gia: Trong trường hợp này, bộ chọn sử dụng hai biến nhị phân z1 và z2. Đặt l là số nguyên được biểu diễn bởi số nhị phân z2z1. Sau đó, bộ chọn nên trả về mã hóa một-nóng của số nguyên (l+1). Để đạt được điều này, chúng tôi định nghĩa bộ chọn r(z) như sau:

r(z) = [z̄1z̄2; z1z̄2; z̄1z2; z1z2]T (3)

trong đó z̄i := 1-zi. Theo cấu trúc, chính xác một mục trong r(z) là 1 (cụ thể, r(z)l+1 = 1) và phần còn lại của các mục là không. Ví dụ, nếu z1=z2=0, thì r(z)1=1 và r(z)i=0, i∈{2,3,4}.

Trường hợp tổng quát của n chuyên gia: Ở đây chúng tôi tổng quát hóa bộ chọn r(z) cho trường hợp n chuyên gia. Để hỗ trợ việc trình bày, chúng tôi đưa ra định nghĩa sau. Đối với bất kỳ số nguyên không âm l, chúng tôi định nghĩa B(l) là tập hợp các chỉ số của các mục khác không trong biểu diễn nhị phân của l (trong đó chúng tôi giả định rằng bit ít quan trọng nhất được đánh chỉ số bởi 1). Ví dụ, B(0) = ∅, B(1) = {1}, B(2) = {2}, và B(3) = {1,2}. Đối với mọi i∈[n], chúng tôi định nghĩa mục thứ i của r(z) như sau:

r(z)i = ∏j∈B(i-1)(zj) ∏j∈[m]\B(i-1)(1-zj) (4)

Trong phần trên, r(z)i là một tích của m biến nhị phân, bằng 1 khi và chỉ khi số nguyên (i-1) được biểu diễn bởi số nhị phân zmzm-1...z1. Do đó, r(z) trả về mã hóa một-nóng của chỉ số của chuyên gia được chọn. Lưu ý rằng khi n = 4, các định nghĩa (3) và (4) là tương đương.

3.1.2 Lựa chọn Nhiều Chuyên gia thông qua Tối thiểu hóa Không có Ràng buộc

Trong mục này, chúng tôi phát triển một cổng tổ hợp cho phép biến đổi Bài toán (2) thành một bài toán tối ưu hóa không có ràng buộc. Chúng tôi thiết kế cổng này bằng cách tạo k thể hiện của bộ chọn chuyên gia đơn r(·), và sau đó lấy một kết hợp lồi của k thể hiện này. Chính thức hơn, đối với mọi i∈[k], đặt z(i)∈{0,1}m là một vector nhị phân (có thể học), sao cho đầu ra của thể hiện thứ i của bộ chọn là r(z(i)). Đặt Z là một ma trận k×m có hàng thứ i là z(i). Hơn nữa, đặt α∈Rk là một vector các tham số có thể học. Chúng tôi định nghĩa cổng tổ hợp q như sau:

q(α,Z) = ∑ki=1 σ(α)i r(z(i)),

trong đó chúng tôi nhớ lại rằng σ(·) là hàm softmax. Vì đối với mọi i∈[k], r(z(i)) là một vector một-nóng, chúng ta có ||q(α,Z)||0 ≤ k. Hơn nữa, vì các trọng số của các bộ chọn được thu được bằng softmax, chúng ta có q(α,Z) ≥ 0 và ∑ni=1 q(α,Z)i = 1. Do đó, q(α,Z) có cùng cách hiểu như w trong Bài toán (2), mà không yêu cầu bất kỳ ràng buộc nào. Do đó, chúng tôi đề xuất thay thế w trong mục tiêu của Bài toán (2) bằng q(α,Z) và loại bỏ tất cả các ràng buộc. Việc thay thế này dẫn đến một bài toán tối ưu hóa không có ràng buộc tương đương, như chúng tôi phát biểu trong mệnh đề tiếp theo.

Mệnh đề 1. Bài toán (2) tương đương4 với:

min
f1,...fn,α,Z
1/N ∑(x,y)∈D ℓ(y, ∑ni=1 fi(x)q(α,Z)i)

z(i) ∈ {0,1}m, i ∈ [k] (5)

Chứng minh của Mệnh đề 1 trong Phụ lục B.1. Không giống như Bài toán (2), Bài toán (5) không bao gồm bất kỳ ràng buộc nào, ngoài việc yêu cầu các biến nhị phân. Tuy nhiên, các biến nhị phân này không thể được xử lý trực tiếp bằng các phương pháp bậc nhất. Tiếp theo, chúng tôi thảo luận về cách làm mượt các biến nhị phân để thu được một sự thư giãn liên tục của Bài toán (5).

3.1.3 Cổng Mượt

Trong mục này, chúng tôi trình bày một quy trình để làm mượt các biến nhị phân trong Bài toán (5) và thảo luận về cách bài toán kết quả có thể được tối ưu hóa bằng các phương pháp bậc nhất. Quy trình dựa trên hàm smooth-step, mà chúng tôi định nghĩa tiếp theo.

Hàm Smooth-step: Đây là một hàm có thể vi phân liên tục và có dạng S, tương tự về hình dạng với hàm logistic. Tuy nhiên, không giống như hàm logistic, hàm smooth-step có thể xuất ra 0 và 1 chính xác cho các cường độ đủ lớn của đầu vào. Các hàm smooth-step và logistic được mô tả trong Phụ lục B.3. Chính thức hơn, với một tham số tỷ lệ không âm η, hàm smooth-step, Sη:R→R, là một đa thức từng khúc bậc ba được định nghĩa như sau:

Sη(t) = {
0                           if t ≤ -η/2
-2/η³ t³ + 3/(2η) t + 1/2  if -η/2 ≤ t ≤ η/2
1                           if t ≥ η/2

Tham số η kiểm soát độ rộng của vùng phân số (tức là, vùng mà hàm nằm nghiêm ngặt giữa 0 và 1). Lưu ý rằng Sη(t) có thể vi phân liên tục tại tất cả các điểm—điều này theo sau vì tại các điểm biên ±η/2, chúng ta có: S'η(-η/2) = S'η(η/2) = 0. Hàm này đã được sử dụng gần đây cho tính toán có điều kiện trong các cây mềm [11] và phổ biến trong tài liệu đồ họa máy tính [8, 30].

Làm mượt: Chúng tôi thu được DSelect-k từ cổng tổ hợp q(α,Z) bằng cách (i) thư giãn mỗi biến nhị phân trong Z để liên tục trong khoảng (-∞,+∞), tức là, Z∈Rk×m, và (ii) áp dụng hàm smooth-step cho Z theo từng phần tử. Chính thức, DSelect-k là một hàm q̃ được định nghĩa như sau:

q̃(α,Z) := q(α,Sη(Z)) = ∑ki=1 σ(α)i r(Sη(z(i))), (6)

trong đó ma trận Sη(Z) được thu được bằng cách áp dụng Sη(·) cho Z theo từng phần tử. Lưu ý rằng q̃(α,Z) có thể vi phân liên tục nên nó phù hợp với các phương pháp bậc nhất. Nếu Sη(Z) là nhị phân, thì q̃(α,Z)

4Tương đương có nghĩa là hai bài toán có cùng mục tiêu tối ưu, và với một giải pháp tối ưu cho một bài toán, chúng ta có thể xây dựng một giải pháp tối ưu cho bài toán khác.

--- TRANG 6 ---
chọn tối đa k chuyên gia (điều này đúng vì q̃(α,Z) = q(α,Sη(Z)), và từ Mục 3.1.2, q chọn tối đa k chuyên gia khi ma trận mã hóa của nó là nhị phân). Tuy nhiên, khi Sη(Z) có bất kỳ mục không nhị phân nào, thì có thể chọn nhiều hơn k chuyên gia, có nghĩa là ràng buộc về số lượng sẽ không được tôn trọng. Trong phần tiếp theo, chúng tôi thảo luận về cách cổng có thể được tối ưu hóa bằng các phương pháp bậc nhất, đồng thời đảm bảo rằng Sη(Z) hội tụ đến một ma trận nhị phân để ràng buộc về số lượng được thực thi.

Chúng tôi đề xuất sử dụng q̃(α,Z) trong MoE, điều này dẫn đến bài toán tối ưu hóa sau:

min f1,...fn,α,Z 1/N ∑(x,y)∈D ℓ(y, ∑ni=1 fi(x)q̃(α,Z)i): (7)

Bài toán (7) có thể được xem như một sự thư giãn liên tục của Bài toán (5). Nếu các chuyên gia có thể vi phân, thì mục tiêu của Bài toán (7) có thể vi phân. Do đó, chúng tôi đề xuất tối ưu hóa MoE từ đầu đến cuối bằng các phương pháp bậc nhất. Chúng tôi lưu ý rằng q̃(α,Z) sử dụng (k + k log n) tham số có thể học. Ngược lại, các cổng Top-k và softmax (được thảo luận trong Mục 2) sử dụng n tham số. Do đó, đối với k tương đối nhỏ, đề xuất của chúng tôi sử dụng số lượng tham số nhỏ hơn. Tiếp theo, chúng tôi thảo luận về cách các tham số của DSelect-k nên được khởi tạo để đảm bảo rằng nó có thể huấn luyện được.

Khởi tạo: Theo định nghĩa của hàm smooth-step, nếu Sη(Zij) là nhị phân thì S'η(Zij) = 0, và do đó ∂ℓ/∂Zij = 0. Điều này ngụ ý rằng, trong quá trình tối ưu hóa, nếu Sη(Zij) trở thành nhị phân, biến Zij sẽ không được cập nhật trong bất kỳ lần lặp tiếp theo nào. Do đó, chúng ta phải cẩn thận về việc khởi tạo Z. Ví dụ, nếu Z được khởi tạo sao cho Sη(Z) là một ma trận nhị phân thì cổng sẽ không được huấn luyện. Để đảm bảo rằng cổng có thể huấn luyện được, chúng tôi khởi tạo mỗi Zij sao cho 0 < Sη(Zij) < 1. Bằng cách này, các Zij có thể có gradient khác không ở đầu quá trình tối ưu hóa.

Tăng tốc Hội tụ đến Giải pháp Nhị phân: Nhớ lại rằng chúng ta cần Sη(Z) hội tụ đến một ma trận nhị phân, để cổng q̃ tôn trọng ràng buộc về số lượng (tức là, chọn tối đa k chuyên gia). Về mặt thực nghiệm, chúng tôi quan sát rằng nếu bộ tối ưu chạy trong một số lần lặp đủ lớn, thì Sη(Z) thường hội tụ đến một ma trận nhị phân. Tuy nhiên, việc dừng sớm bộ tối ưu có thể được mong muốn trong thực tế vì những cân nhắc tính toán và thống kê, và điều này có thể ngăn Sη(Z) hội tụ. Để khuyến khích hội tụ nhanh hơn đến một Sη(Z) nhị phân, chúng tôi sẽ thêm một bộ chính quy hóa entropy vào Bài toán (7). Mệnh đề sau được cần trước khi chúng tôi giới thiệu bộ chính quy hóa.

Mệnh đề 2. Đối với bất kỳ z∈Rm, α∈Rk, và Z∈Rk×m, r(Sη(z)) và q̃(α,Z) thuộc về simplex xác suất.

Chứng minh của mệnh đề trong Phụ lục B.1. Mệnh đề 2 ngụ ý rằng, trong quá trình huấn luyện, đầu ra của mỗi bộ chọn chuyên gia đơn được sử dụng bởi q̃(α,Z), tức là, r(Sη(z(i))) cho i∈[k], thuộc về simplex xác suất. Lưu ý rằng entropy của mỗi r(Sη(z(i))) được tối thiểu hóa bởi bất kỳ vector mã hóa một-nóng nào. Do đó, đối với mỗi r(Sη(z(i))), chúng tôi thêm một thuật ngữ chính quy hóa entropy khuyến khích hội tụ đến các vector mã hóa một-nóng; tương đương, điều này khuyến khích hội tụ đến một Sη(Z) nhị phân. Cụ thể, chúng tôi giải biến thể chính quy hóa sau của Bài toán (7):

min f1,...fn,α,Z ∑(x,y)∈D [1/N ℓ(y, ∑ni=1 fi(x)q̃(α,Z)i) + λ Ωη(Z)]

trong đó Ωη(Z) := ∑ki=1 h(r(Sη(z(i)))) và h(·) là hàm entropy. Siêu tham số λ là không âm và kiểm soát tốc độ mỗi bộ chọn hội tụ đến mã hóa một-nóng. Trong các thí nghiệm của chúng tôi, chúng tôi điều chỉnh trên một dải giá trị λ. Khi chọn các siêu tham số tốt nhất từ việc điều chỉnh, chúng tôi bỏ qua bất kỳ λ nào có giải pháp tương ứng không có Sη(Z) nhị phân. Trong Phụ lục C.3, chúng tôi báo cáo số bước huấn luyện cần thiết để Sη(Z) hội tụ đến một ma trận nhị phân, trên một số bộ dữ liệu thực tế.

Các lựa chọn thay thế khác để đảm bảo rằng Sη(Z) hội tụ đến một ma trận nhị phân cũng có thể. Một lựa chọn thay thế là chính quy hóa entropy của mỗi mục trong Sη(Z) riêng biệt. Một lựa chọn thay thế khác là làm lạnh tham số η của hàm smooth-step về phía không.

Lựa chọn thay thế dựa trên Softmax cho Mã hóa Nhị phân: Nhớ lại rằng các bộ chọn được đề xuất của chúng tôi trong (6), tức là, r(Sη(z(i))), i∈[k], học các vector một-nóng chính xác (bằng cách sử dụng mã hóa nhị phân). Một lựa chọn thay thế thực tế để học một vector một-nóng là bằng cách sử dụng hàm softmax với làm lạnh nhiệt độ. Về mặt lý thuyết, lựa chọn thay thế này không thể trả về một vector một-nóng, nhưng sau khi huấn luyện, đầu ra softmax có thể được chuyển đổi thành vector một-nóng bằng một heuristic (ví dụ, bằng cách lấy argmax). Trong Phụ lục C.1, chúng tôi thực hiện một nghiên cứu ablation trong đó chúng tôi thay thế các bộ chọn trong DSelect-k bằng các hàm softmax (cùng với làm lạnh nhiệt độ hoặc chính quy hóa entropy).

--- TRANG 7 ---
3.2 DSelect-k cho Cổng theo từng ví dụ

Trong mục này, chúng tôi tổng quát hóa phiên bản tĩnh của DSelect-k, q̃(α,Z), cho thiết lập cổng theo từng ví dụ. Ý tưởng chính là làm cho các tham số α và Z của cổng thành các hàm của đầu vào, sao cho cổng có thể đưa ra quyết định trên cơ sở từng ví dụ. Lưu ý rằng nhiều dạng hàm có thể cho các tham số này. Để đơn giản và dựa trên các thí nghiệm của chúng tôi, chúng tôi chọn làm cho α và Z thành các hàm tuyến tính của ví dụ đầu vào. Chính thức hơn, đặt G∈Rk×p, W(i)∈Rm×p, i∈[k], là một tập hợp các tham số có thể học. Với một ví dụ đầu vào x∈Rp, chúng tôi đặt α = Gx và z(i) = W(i)x trong q̃(α,Z) (để đơn giản hóa việc trình bày, chúng tôi không bao gồm các thuật ngữ bias). Do đó, phiên bản theo từng ví dụ của DSelect-k là một hàm v được định nghĩa như sau:

v(G,W,x) = ∑ki=1 σ(Gx)i r(Sη(W(i)x)):

Trong phần trên, thuật ngữ r(Sη(W(i)x)) đại diện cho bộ chọn chuyên gia đơn thứ i, có đầu ra phụ thuộc vào ví dụ x; do đó các ví dụ khác nhau tự do chọn các chuyên gia khác nhau. Thuật ngữ σ(Gx)i xác định trọng số phụ thuộc đầu vào được gán cho bộ chọn thứ i. Cổng v(G,W,x) có thể vi phân liên tục trong các tham số G và W, vì vậy chúng tôi đề xuất tối ưu hóa nó bằng các phương pháp bậc nhất. Tương tự như trường hợp cổng tĩnh, nếu Sη(W(i)x) là nhị phân cho tất cả i∈[k], thì mỗi r(Sη(W(i)x)) sẽ chọn chính xác một chuyên gia, và ví dụ x sẽ được gán cho tối đa k chuyên gia.

Để khuyến khích Sη(W(i)x), i∈[k] trở thành nhị phân, chúng tôi giới thiệu một bộ chính quy hóa entropy, tương tự về bản chất với cổng tĩnh. Tuy nhiên, không giống như cổng tĩnh, bộ chính quy hóa ở đây nên theo từng ví dụ, sao cho mỗi ví dụ tôn trọng ràng buộc về số lượng. Bởi Mệnh đề 2, đối với bất kỳ i∈[k], r(Sη(W(i)x)) thuộc về simplex xác suất. Do đó, đối với mỗi ví dụ x trong dữ liệu huấn luyện, chúng tôi giới thiệu một thuật ngữ chính quy hóa có dạng: Ωη(W,x) := ∑i∈[k] h(r(Sη(W(i)x))), và tối thiểu hóa hàm mục tiêu sau:

∑(x,y)∈D [1/N ℓ(y, ∑ni=1 fi(x)v(G,W,x)i) + λ Ωη(W,x)],

trong đó λ là một siêu tham số không âm. Tương tự như trường hợp cổng tĩnh, chúng tôi điều chỉnh trên một dải giá trị λ, và chúng tôi chỉ xem xét các lựa chọn λ buộc số lượng chuyên gia được chọn trung bình mỗi ví dụ nhỏ hơn hoặc bằng k. Nếu ứng dụng yêu cầu ràng buộc về số lượng được thỏa mãn nghiêm ngặt cho mọi ví dụ (không chỉ trung bình), thì việc làm lạnh η trong hàm smooth-step về phía không thực thi điều này.

4 Thí nghiệm

Chúng tôi nghiên cứu hiệu suất của DSelect-k trong bối cảnh MTL và so sánh với các cổng tiên tiến nhất và các baseline. Trong phần còn lại của mục này, chúng tôi trình bày các thí nghiệm trên các bộ dữ liệu MTL thực tế sau: MovieLens, Multi-MNIST, Multi-Fashion MNIST, và trên một hệ thống gợi ý quy mô lớn trong thế giới thực. Hơn nữa, trong Phụ lục C, chúng tôi trình bày một thí nghiệm bổ sung trên dữ liệu tổng hợp (với tối đa 128 nhiệm vụ), trong đó chúng tôi nghiên cứu hiệu suất thống kê và thực hiện các nghiên cứu ablation.

Các phương pháp cạnh tranh: Chúng tôi tập trung vào MoE đa cổng, và nghiên cứu các cổng DSelect-k và Top-k trong cả thiết lập cổng tĩnh và theo từng ví dụ. Đối với cổng tĩnh, chúng tôi cũng xem xét một cổng dựa trên Gumbel-softmax [34]—không giống như DSelect-k, cổng này không thể kiểm soát mức độ thưa một cách rõ ràng (xem phần bổ sung để biết chi tiết). Ngoài ra, chúng tôi xem xét hai baseline MTL. Baseline đầu tiên là MoE với cổng softmax (sử dụng tất cả các chuyên gia có sẵn). Baseline thứ hai là mô hình bottom được chia sẻ [4], trong đó tất cả các nhiệm vụ chia sẻ cùng các lớp bottom, sau đó được kết nối với các mạng nơ-ron đặc thù cho nhiệm vụ.

Thiết lập thí nghiệm: Tất cả các mô hình cạnh tranh được triển khai trong TensorFlow 2. Chúng tôi sử dụng Adam [18] và Adagrad [7] để tối ưu hóa, và chúng tôi điều chỉnh các siêu tham số chính bằng tìm kiếm lưới ngẫu nhiên (với trung bình 5 thử nghiệm cho mỗi điểm lưới). Chi tiết đầy đủ về thiết lập trong Phụ lục D.

4.1 MovieLens

Bộ dữ liệu: MovieLens [10] là một bộ dữ liệu gợi ý phim chứa các bản ghi cho 4,000 phim và 6,000 người dùng. Theo [37], đối với mỗi cặp người dùng-phim, chúng tôi xây dựng hai nhiệm vụ. Nhiệm vụ 1 là một bài toán phân loại nhị phân để dự đoán liệu người dùng có xem một bộ phim cụ thể hay không. Nhiệm vụ 2 là một bài toán hồi quy để dự đoán xếp hạng của người dùng (trong {1,2,...,5}) cho một bộ phim đã cho. Chúng tôi sử dụng 1,6 triệu ví dụ để huấn luyện và 200,000 cho mỗi tập validation và testing.

Chi tiết thí nghiệm: Chúng tôi sử dụng cross-entropy và squared error loss cho nhiệm vụ 1 và 2, tương ứng. Chúng tôi tối ưu hóa một trung bình có trọng số của hai loss, tức là, hàm loss cuối cùng là λ(Loss của Nhiệm vụ 1) + (1-λ)(Loss của Nhiệm vụ 2), và chúng tôi báo cáo kết quả cho λ∈{0.1,0.5,0.9}. Cùng một hàm loss cũng được sử dụng để điều chỉnh và testing. Kiến trúc bao gồm một MoE đa cổng với 8 chuyên gia, trong đó mỗi chuyên gia và mạng đặc thù cho nhiệm vụ được cấu thành từ các lớp dense được kích hoạt ReLU. Đối với mỗi λ, chúng tôi điều chỉnh các siêu tham số tối ưu hóa và đặc thù cho cổng, bao gồm số lượng chuyên gia để chọn (tức là, k trong DSelect-k và Top-k). Sau khi điều chỉnh, chúng tôi huấn luyện mỗi mô hình cho 100 lần lặp lại (sử dụng khởi tạo ngẫu nhiên) và báo cáo kết quả trung bình. Để biết chi tiết đầy đủ, xem Phụ lục D.1.

Kết quả: Trong Bảng 1, chúng tôi báo cáo test loss và số lượng chuyên gia được chọn trung bình. Kết quả chỉ ra rằng đối với tất cả các giá trị λ, một trong các cổng DSelect-k của chúng tôi (tĩnh hoặc theo từng ví dụ) vượt trội hơn các phương pháp cạnh tranh, về cả test loss và số lượng chuyên gia được chọn. Trong thiết lập cổng tĩnh, dường như không có người chiến thắng rõ ràng trong ba phương pháp cạnh tranh (Top-k, DSelect-k, và Gumbel Softmax), nhưng chúng tôi lưu ý rằng DSelect-k vượt trội hơn cả Top-k và Gumbel Softmax cho hai trong ba lựa chọn λ. Đáng chú ý, softmax MoE bị vượt trội đồng đều bởi các cổng DSelect-k và Top-k, vì vậy tính thưa trong cổng có vẻ có lợi trên bộ dữ liệu này. Giả thuyết của chúng tôi là softmax MoE đang overfitting và các phương pháp cổng thưa đang giảm thiểu vấn đề này. Trong Bảng C.5 trong phụ lục, chúng tôi cũng báo cáo các metric nhiệm vụ riêng lẻ (loss và accuracy).

λ = 0.1 | λ = 0.5 | λ = 0.9
Loss | Experts | Loss | Experts | Loss | Experts
Tĩnh DSelect-k | 40155 | 2.7 | 38043 | 1.5 | 36902 | 1.3
Top-k | 40124 | 2.0 | 38182 | 2.0 | 36936 | 2.0
Gumbel Softmax | 41713 | 2.7 | 38982 | 2.6 | 36884 | 3.6
Theo từng ví dụ DSelect-k | 40066 | 1.5 | 38233 | 1.2 | 36792 | 1.1
Top-k | 40278 | 2.0 | 38414 | 2.0 | 37413 | 2.0
Baseline Softmax MoE | 40901 | 8.0 | 39603 | 8.0 | 38471 | 8.0
Shared Bottom | 40372 | - | 38682 | - | 36871 | -

Bảng 1: Test loss (với sai số chuẩn) và số lượng chuyên gia được chọn trung bình trên MovieLens. Tham số λ là trọng số của loss Nhiệm vụ 1 (xem văn bản để biết chi tiết). Test loss được nhân với 10^4.

4.2 Multi-MNIST và Multi-Fashion MNIST

Bộ dữ liệu: Chúng tôi xem xét hai bộ dữ liệu phân loại hình ảnh: Multi-MNIST và Multi-Fashion [32], là các biến thể đa nhiệm vụ của bộ dữ liệu MNIST [20] và Fashion MNIST [38]. Chúng tôi xây dựng bộ dữ liệu Multi-MNIST tương tự như [32]: lấy mẫu đồng đều hai hình ảnh từ MNIST và phủ chúng lên nhau, và (ii) dịch chuyển một chữ số về phía góc trên-trái và chữ số khác về phía góc dưới-phải (bằng 4 pixel theo mỗi hướng). Quy trình này dẫn đến các hình ảnh 36×36 với một số chồng chéo giữa các chữ số. Multi-Fashion được xây dựng theo cách tương tự bằng cách phủ các hình ảnh từ bộ dữ liệu Fashion MNIST. Đối với mỗi bộ dữ liệu, chúng tôi xem xét hai nhiệm vụ phân loại: Nhiệm vụ 1 là phân loại mục trên-trái và Nhiệm vụ 2 là phân loại mục dưới-phải. Chúng tôi sử dụng 100,000 ví dụ để huấn luyện, và 20,000 ví dụ cho mỗi tập validation và testing.

Chi tiết thí nghiệm: Chúng tôi sử dụng cross-entropy loss cho mỗi nhiệm vụ và tối ưu hóa tổng của các loss5. Mô hình là một MoE đa cổng với 8 chuyên gia, trong đó mỗi chuyên gia là một mạng nơ-ron tích chập và mỗi mạng đặc thù cho nhiệm vụ được cấu thành từ một số lớp dense. Chúng tôi điều chỉnh các siêu tham số tối ưu hóa và đặc thù cho cổng, bao gồm số lượng chuyên gia để chọn, và sử dụng trung bình của các accuracy nhiệm vụ làm metric điều chỉnh. Sau khi điều chỉnh, chúng tôi huấn luyện mỗi mô hình cho 100 lần lặp lại (sử dụng khởi tạo ngẫu nhiên) và báo cáo kết quả trung bình. Để biết chi tiết đầy đủ, xem Phụ lục D.2.

Kết quả: Trong Bảng 2, chúng tôi báo cáo test accuracy và số lượng chuyên gia được chọn cho bộ dữ liệu Multi-MNIST và Multi-Fashion. Trên Multi-MNIST, DSelect-k (tĩnh) vượt trội hơn Top-k và Gumbel Softmax, về cả accuracy nhiệm vụ và số lượng chuyên gia được chọn. Ví dụ, nó đạt được hơn 1% cải thiện trong accuracy Nhiệm vụ 2 so với Top-k (tĩnh). DSelect-k (tĩnh) gần với hiệu suất của Softmax MoE, nhưng sử dụng ít chuyên gia hơn (~1.7 so với 8 chuyên gia). Ở đây DSelect-k (theo từng ví dụ) không mang lại cải thiện so với biến thể tĩnh (không giống như bộ dữ liệu MovieLens). Trên Multi-Fashion, chúng tôi lại thấy rằng DSelect-k (tĩnh) hoạt động tốt nhất về accuracy.

5Do tính đối xứng trong bài toán, việc gán trọng số bằng nhau cho hai nhiệm vụ là một lựa chọn hợp lý.

--- TRANG 9 ---
Multi-MNIST | Multi-Fashion MNIST
Accuracy 1 | Accuracy 2 | Experts | Accuracy 1 | Accuracy 2 | Experts
Tĩnh DSelect-k | 92.56±0.03 | 90.98±0.04 | 1.7 | 83.78±0.05 | 83.34±0.05 | 1.8
Top-k | 91.93±0.06 | 90.03±0.08 | 4 | 83.44±0.07 | 82.66±0.08 | 4
Gumbel Softmax | 92.3±0.05 | 90.63±0.05 | 1.8 | 83.66±0.07 | 83.28±0.05 | 1.5
Theo từng ví dụ DSelect-k | 92.42±0.03 | 90.7±0.03 | 1.5 | 83.69±0.04 | 83.13±0.04 | 1.5
Top-k | 92.27±0.03 | 90.45±0.03 | 4 | 83.66±0.04 | 83.15±0.04 | 4
Baseline Softmax MoE | 92.61±0.03 | 91.0±0.03 | 8 | 83.48±0.04 | 82.81±0.04 | 8
Shared Bottom | 91.3±0.04 | 89.47±0.04 | - | 82.05±0.05 | 81.37±0.06 | -

Bảng 2: Test accuracy (với sai số chuẩn) và số lượng chuyên gia được chọn trên Multi-MNIST/Fashion.

Nhiệm vụ / Phương pháp | DSelect-k | Top-k
E. Task 1 (AUC) | 0.8103±0.0002 | 0.7481±0.0198
E. Task 2 (AUC) | 0.8161±0.0002 | 0.7624±0.0169
E. Task 3 (RMSE) | 0.2874±0.0002 | 0.3406±0.0180
E. Task 4 (RMSE) | 0.8781±0.0014 | 1.1213±0.0842
E. Task 5 (AUC) | 0.7524±0.0003 | 0.5966±0.0529
S. Task 1 (AUC) | 0.6133±0.0066 | 0.5080±0.0047
S. Task 2 (AUC) | 0.8468±0.0289 | 0.5981±0.0616
S. Task 3 (AUC) | 0.9259±0.0008 | 0.6665±0.0091

Bảng 3: Hiệu suất trung bình (AUC và RMSE) và sai số chuẩn trên một hệ thống gợi ý thế giới thực với 8 nhiệm vụ: "E." và "S." biểu thị các nhiệm vụ engagement và satisfaction, tương ứng.

Hình 3: Trọng số chuyên gia của các cổng DSelect-k trên hệ thống gợi ý.

4.3 Một Hệ thống Gợi ý Quy mô Lớn

Chúng tôi nghiên cứu hiệu suất của DSelect-k và Top-k trong một hệ thống gợi ý nội dung quy mô lớn trong thế giới thực. Hệ thống bao gồm hàng trăm triệu mục độc đáo và hàng tỷ người dùng.

Kiến trúc và Bộ dữ liệu: Hệ thống bao gồm một bộ tạo ứng viên theo sau bởi một mô hình xếp hạng đa nhiệm vụ, và nó áp dụng một framework tương tự như [41,35]. Mô hình xếp hạng đưa ra dự đoán cho 6 nhiệm vụ phân loại và 2 nhiệm vụ hồi quy. Chúng có thể được phân loại thành hai danh mục: (i) các nhiệm vụ engagement (ví dụ, dự đoán nhấp chuột của người dùng, nhấp chuột xấu, thời gian engagement), và (ii) các nhiệm vụ satisfaction (ví dụ, dự đoán các hành vi satisfaction của người dùng như like và dislike). Chúng tôi xây dựng bộ dữ liệu từ các log người dùng của hệ thống (chứa thông tin lịch sử về người dùng và nhãn cho 8 nhiệm vụ). Bộ dữ liệu bao gồm hàng tỷ ví dụ (chúng tôi không báo cáo số chính xác vì lý do bảo mật). Chúng tôi sử dụng phân chia ngẫu nhiên 90/10 cho tập huấn luyện và đánh giá.

Chi tiết Thí nghiệm: Chúng tôi sử dụng cross-entropy và squared error loss cho các nhiệm vụ phân loại và hồi quy, tương ứng. Mô hình xếp hạng dựa trên MoE đa cổng, trong đó mỗi nhiệm vụ sử dụng một cổng tĩnh riêng biệt. MoE sử dụng 8 chuyên gia, mỗi chuyên gia được cấu thành từ các lớp dense. Đối với cả các mô hình dựa trên DSelect-k và Top-k, chúng tôi điều chỉnh learning rate và kiến trúc của các chuyên gia. Sau đó, sử dụng các siêu tham số tốt nhất, chúng tôi huấn luyện các mô hình cuối cùng cho 5 lần lặp lại (sử dụng khởi tạo ngẫu nhiên). Để biết chi tiết bổ sung, xem Phụ lục D.3.

Kết quả: Trong Bảng 3, chúng tôi báo cáo các metric hiệu suất ngoài mẫu cho 8 nhiệm vụ. Kết quả chỉ ra rằng DSelect-k vượt trội hơn Top-k trên tất cả các nhiệm vụ, với những cải thiện rõ rệt nhất trên các nhiệm vụ satisfaction. Trong Hình 3, chúng tôi cho thấy một heatmap của các trọng số chuyên gia được chọn bởi các cổng DSelect-k. Đáng chú ý, đối với DSelect-k, tất cả các nhiệm vụ engagement chia sẻ ít nhất một chuyên gia, và hai trong số các nhiệm vụ satisfaction chia sẻ cùng một chuyên gia.

5 Kết luận

Chúng tôi đã giới thiệu DSelect-k: một cổng liên tục có thể vi phân và thưa cho MoE, có thể được huấn luyện bằng các phương pháp bậc nhất. Với một tham số k do người dùng chỉ định, cổng chọn tối đa k trong số n chuyên gia. Kiểm soát trực tiếp như vậy về mức độ thưa thường được xử lý trong tài liệu bằng cách thêm ràng buộc về số lượng vào bài toán tối ưu hóa. Một trong những ý tưởng chính mà chúng tôi giới thiệu là sơ đồ mã hóa nhị phân cho phép chọn k chuyên gia, mà không yêu cầu bất kỳ ràng buộc nào trong bài toán tối ưu hóa. Chúng tôi đã nghiên cứu hiệu suất của DSelect-k trong các thiết lập MTL, trên cả dữ liệu tổng hợp và thực tế. Các thí nghiệm của chúng tôi chỉ ra rằng DSelect-k có thể đạt được những cải thiện đáng kể trong dự đoán và lựa chọn chuyên gia, so với các cổng MoE tiên tiến nhất và các baseline MTL.

Tác động Xã hội: Các mô hình MoE được sử dụng trong nhiều ứng dụng khác nhau (như đã thảo luận trong phần giới thiệu). DSelect-k có thể cải thiện khả năng diễn giải và hiệu quả của các mô hình MoE, do đó có lợi cho các ứng dụng cơ bản. Chúng tôi không thấy tác động xã hội tiêu cực trực tiếp từ đề xuất của chúng tôi.

--- TRANG 10 ---
Lời cảm ơn: Nghiên cứu được thực hiện khi Hussein Hazimeh làm việc tại Google, và một phần viết bài được thực hiện trong thời gian ông ở MIT. Tại MIT, Hussein Hazimeh và Rahul Mazumder thừa nhận tài trợ nghiên cứu từ Văn phòng Nghiên cứu Hải quân [Grant ONR-N000141812298].

Tài liệu tham khảo

[1] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, và Doina Precup. Conditional computation in neural networks for faster models. CoRR, abs/1511.06297, 2015. URL http://arxiv.org/abs/1511.06297.

[2] Yoshua Bengio, Nicholas Léonard, và Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

[3] Mathieu Blondel, Andre Martins, và Vlad Niculae. Learning classifiers with fenchel-young losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 606–615. PMLR, 2019.

[4] Rich Caruana. Multitask learning. Machine learning, 28(1):41–75, 1997.

[5] Jianbo Chen, Le Song, Martin Wainwright, và Michael Jordan. Learning to explain: An information-theoretic perspective on model interpretation. In International Conference on Machine Learning, pages 883–892. PMLR, 2018.

[6] Gonçalo M Correia, Vlad Niculae, và André FT Martins. Adaptively sparse transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2174–2184, 2019.

[7] John Duchi, Elad Hazan, và Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.

[8] David S Ebert, F Kenton Musgrave, Darwyn Peachey, Ken Perlin, và Steven Worley. Texturing & modeling: a procedural approach. Morgan Kaufmann, 2003.

[9] William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.

[10] F Maxwell Harper và Joseph A Konstan. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4):1–19, 2015.

[11] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, và Rahul Mazumder. The tree ensemble layer: Differentiability meets conditional computation. In Hal Daumé III và Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4138–4148, Virtual, 13–18 Jul 2020. PMLR.

[12] Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie Shotton, Matthew Brown, và Antonio Criminisi. Decision forests, convolutional networks and the models in-between. arXiv preprint arXiv:1603.01250, 2016.

[13] Robert A Jacobs. Bias/variance analyses of mixtures-of-experts architectures. Neural computation, 9(2):369–383, 1997.

[14] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, và Geoffrey E Hinton. Adaptive mixtures of local experts. Neural computation, 3(1):79–87, 1991.

[15] Eric Jang, Shixiang Gu, và Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.

[16] Wenxin Jiang và Martin A Tanner. On the identifiability of mixtures-of-experts. Neural Networks, 12(9):1253–1258, 1999.

[17] Michael I Jordan và Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181–214, 1994.

--- TRANG 11 ---
[18] Diederik P. Kingma và Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio và Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.

[19] Diederik P Kingma và Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

[20] Yann LeCun, Corinna Cortes, và CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, và Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.

[22] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, và Ed H Chi. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1930–1939, 2018.

[23] Chris J Maddison, Andriy Mnih, và Yee Whye Teh. The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.

[24] Andre Martins và Ramon Astudillo. From softmax to sparsemax: A sparse model of attention and multi-label classification. In International Conference on Machine Learning, pages 1614–1623. PMLR, 2016.

[25] Krzysztof Maziarz, Efi Kokiopoulou, Andrea Gesmundo, Luciano Sbaiz, Gabor Bartok, và Jesse Berent. Gumbel-matrix routing for flexible multi-task learning. arXiv preprint arXiv:1910.04915, 2019.

[26] Max Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, và Chris J Maddison. Gradient estimation with stochastic softmax tricks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, và H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 5691–5704. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/3df80af53dce8435cf9ad6c3e7a403fd-Paper.pdf.

[27] Ben Peters, Vlad Niculae, và André FT Martins. Sparse sequence-to-sequence models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1504–1519, 2019.

[28] Prajit Ramachandran và Quoc V Le. Diversity and depth in per-example routing models. In International Conference on Learning Representations, 2018.

[29] Clemens Rosenbaum, Tim Klinger, và Matthew Riemer. Routing networks: Adaptive selection of non-linear functions for multi-task learning. In International Conference on Learning Representations, 2018.

[30] Randi J Rost, Bill Licea-Kane, Dan Ginsburg, John Kessenich, Barthold Lichtenbelt, Hugh Malan, và Mike Weiblen. OpenGL shading language. Pearson Education, 2009.

[31] Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017.

[32] Sara Sabour, Nicholas Frosst, và Geoffrey E Hinton. Dynamic routing between capsules. In Advances in neural information processing systems, pages 3856–3866, 2017.

[33] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, và Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.

--- TRANG 12 ---
[34] Ximeng Sun, Rameswar Panda, Rogerio Feris, và Kate Saenko. Adashare: Learning what to share for efficient deep multi-task learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, và H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 8728–8740. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/634841a6831464b64c072c8510c7f35c-Paper.pdf.

[35] Hongyan Tang, Junning Liu, Ming Zhao, và Xudong Gong. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In Fourteenth ACM Conference on Recommender Systems, pages 269–278, 2020.

[36] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, và Joseph E Gonzalez. Skipnet: Learning dynamic routing in convolutional networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 409–424, 2018.

[37] Yuyan Wang, Zhe Zhao, Bo Dai, Christopher Fifty, Dong Lin, Lichan Hong, và Ed H Chi. Small towers make big differences. arXiv preprint arXiv:2008.05808, 2020.

[38] Han Xiao, Kashif Rasul, và Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

[39] Sang Michael Xie và Stefano Ermon. Reparameterizable subset sampling via continuous relaxations. In International Joint Conference on Artificial Intelligence, 2019.

[40] Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, và Tomas Pfister. Differentiable top-k with optimal transport. Advances in Neural Information Processing Systems, 33, 2020.

[41] Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, và Ed Chi. Recommending what video to watch next: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender Systems, pages 43–51, 2019.

--- TRANG 13 ---
A Công trình liên quan bổ sung

MTL: Trong MTL, các kiến trúc dựa trên học sâu thực hiện chia sẻ tham số mềm, tức là chia sẻ tham số mô hình một phần, đang chứng tỏ hiệu quả trong việc khai thác cả điểm chung và sự khác biệt giữa các nhiệm vụ [31]. Một kiến trúc linh hoạt cho chia sẻ tham số mềm là MoE đa cổng [22]. Chúng tôi sử dụng MoE đa cổng trong các thí nghiệm của mình và so sánh cả cổng thưa và dày đặc—[22] chỉ xem xét các cổng dày đặc. Ngoài ra, một số công trình gần đây đã xem xét các cấu trúc giống cổng để chia sẻ tham số linh hoạt trong MTL. Ví dụ, [34,25] cho mỗi nhiệm vụ sự linh hoạt để sử dụng hoặc bỏ qua các thành phần bên trong mạng nơ-ron. Các quyết định được mô hình hóa bằng các biến ngẫu nhiên nhị phân, và các phân phối xác suất tương ứng được học bằng SGD và thủ thuật Gumbel-softmax [15]. Phương pháp này tương tự như cổng tĩnh, nhưng nó không hỗ trợ cổng theo từng ví dụ. Hơn nữa, số lượng khác không không thể được kiểm soát trực tiếp (ngược lại với cổng của chúng tôi). Công trình của chúng tôi cũng liên quan đến [29] người đã giới thiệu "router" (tương tự như cổng) có thể chọn lớp hoặc thành phần của lớp nào để kích hoạt cho mỗi nhiệm vụ. Các router trong công trình sau không thể vi phân và yêu cầu học tăng cường.

B Chi tiết Phương pháp

B.1 Chứng minh

B.1.1 Chứng minh Mệnh đề 1

Đặt f̄ = {fi}i∈[n]. Để chứng minh tương đương, chúng ta cần thiết lập hai hướng sau: (I) một giải pháp tối ưu (f̄,α,Z) cho Bài toán (5) có thể được sử dụng để xây dựng một giải pháp khả thi (f̄,w) cho Bài toán (2) và cả hai giải pháp có cùng mục tiêu, và (II) một giải pháp tối ưu (f̄,w) cho Bài toán (2) có thể được sử dụng để xây dựng một giải pháp khả thi (f̄,α,Z) cho Bài toán (5) và cả hai giải pháp có cùng mục tiêu. Hướng (I) là hiển nhiên: giải pháp được định nghĩa bởi f̄ = f̄ và w = q(α,Z) là khả thi cho Bài toán (2) và có cùng mục tiêu như (f̄,α,Z).

Tiếp theo, chúng tôi chỉ ra Hướng (II). Đặt s = ||w||0 và biểu thị bởi tj chỉ số của phần tử lớn thứ j trong w, tức là, các mục khác không trong w là wt1 > wt2 > ... > wts. Đối với mọi i∈[s], đặt z(i) thành biểu diễn nhị phân của ti-1. Nếu s < k, thì chúng tôi đặt các z(i) còn lại (chưa đặt) như sau: đối với i∈{s+1,s+2,...,k} đặt z(i) thành biểu diễn nhị phân của ts-1. Bởi cấu trúc này, các chỉ số khác không được chọn bởi r(z(i)), i∈[k] chính xác là các chỉ số khác không của w.

Để xây dựng α, có hai trường hợp cần xem xét: (i) s = k và (ii) s < k. Nếu s = k, thì đặt αi = log(wti) cho i∈[k]. Do đó, σ(α)i = wti cho i∈[k], và do đó q(α,Z) = w. Ngược lại, nếu s < k, thì đặt αi = log(wti) cho i∈[s-1] và αi = log(wts/(k-s)) cho i∈{s,s+1,...,k}. Do đó, đối với i∈[s-1], chúng ta có σ(α)i = wti, tức là, các trọng số của các chỉ số khác không tj, j∈[s-1] trong q(α,Z) bằng với những trọng số trong w. Trọng số được gán cho chỉ số khác không ts trong q(α,Z) là: ∑i∈{s,s+1,...,k} σ(α)i = ∑i∈{s,s+1,...,k} wts/(k-s) = wts. Do đó, q(α,Z) = w. Trong cả (i) và (ii), chúng ta có q(α,Z) = w, vì vậy giải pháp (f̄,α,Z) là khả thi và có cùng mục tiêu như (f̄,w).

B.1.2 Chứng minh Mệnh đề 2

Đầu tiên, chúng tôi sẽ sử dụng quy nạp để chỉ ra rằng r(Sη(z)) thuộc về simplex xác suất. Cụ thể, chúng tôi sẽ chứng minh rằng đối với bất kỳ số nguyên t≥1 và z∈Rt, r(Sη(z)) thuộc về simplex xác suất.

Trường hợp cơ sở của chúng tôi là cho t = 1. Trong trường hợp này, có một biến mã hóa nhị phân đơn z1∈R và 2 chuyên gia. Bộ chọn chuyên gia đơn r(Sη(z1)) được định nghĩa như sau: r(Sη(z1))1 = 1-Sη(z1) và r(Sη(z1))2 = Sη(z1). Hai thuật ngữ sau là không âm và tổng bằng 1. Do đó, r(Sη(z1)) thuộc về simplex xác suất.

Giả thuyết quy nạp của chúng tôi là đối với một số t≥1 và bất kỳ z∈Rt, r(Sη(z)) thuộc về simplex xác suất. Đối với bước quy nạp, chúng ta cần chỉ ra rằng đối với bất kỳ v∈Rt+1, r(Sη(v)) thuộc về simplex xác suất. Từ định nghĩa của r(·), điều sau đây đúng:

r(Sη(v))i = r(Sη([v1,v2,...,vt]T))i(1-Sη(vt+1)) i∈[2t]
r(Sη([v1,v2,...,vt]T))i-2t Sη(vt+1) i∈[2t+1]\[2t] (B.8)

Bởi giả thuyết quy nạp, chúng ta có r(Sη([v1,v2,...,vt]T))i ≥ 0 đối với bất kỳ i. Hơn nữa, Sη(·) là một hàm không âm. Do đó, r(Sη(v))i ≥ 0 đối với bất kỳ i. Còn lại là chỉ ra rằng tổng của các mục trong r(Sη(v)) là 1, mà chúng tôi thiết lập tiếp theo:

∑i=1^(2t+1) r(Sη(v))i = ∑i=1^(2t) r(Sη(v))i + ∑i=2t+1^(2t+1) r(Sη(v))i

(B.8) = ∑i=1^(2t) r(Sη([v1,...,vt]T))i(1-Sη(vt+1)) + ∑i=2t+1^(2t+1) r(Sη([v1,...,vt]T))i-2t Sη(vt+1) (B.9)

Sử dụng phép thay đổi biến, tổng thứ hai trong phần trên có thể được viết lại như sau: ∑i=2t+1^(2t+1) r(Sη([v1,v2,...,vt]T))i-2t Sη(vt+1) = ∑i=1^(2t) r(Sη([v1,v2,...,vt]T))i Sη(vt+1). Thay đẳng thức sau vào (B.9) và đơn giản hóa, chúng ta có ∑i=1^(2t+1) r(Sη(v))i = 1. Do đó, r(Sη(v)) thuộc về simplex xác suất, điều này thiết lập bước quy nạp. Cuối cùng, chúng tôi lưu ý rằng q̃(α,Z) thuộc về simplex xác suất vì nó là một kết hợp lồi của các vector xác suất.

B.2 Mở rộng DSelect-k cho n tùy ý

Giả sử rằng số lượng chuyên gia n không phải là lũy thừa của 2. Để cổng DSelect-k q̃(α,Z) hoạt động trong thiết lập này, chúng ta cần mỗi bộ chọn chuyên gia đơn r được sử dụng bởi cổng có thể xử lý n chuyên gia. Tiếp theo, chúng tôi thảo luận về cách r có thể xử lý n khi nó không phải là lũy thừa của 2. Đặt m là số nguyên nhỏ nhất sao cho n < 2m. Sau đó, chúng tôi coi bài toán như thể có 2m chuyên gia và sử dụng một vector mã hóa nhị phân z∈Rm. Đối với i∈[n], chúng tôi để mục r(z)i là trọng số của chuyên gia i trong MoE. Lưu ý rằng các mục r(z)i, i∈{n+1,n+2,...,2m}, không liên quan đến bất kỳ chuyên gia nào. Để tránh tình huống mà r(z) gán xác suất khác không cho các mục sau, chúng tôi thêm hình phạt sau vào hàm mục tiêu: ∑i∈[n] r(z)i trong đó μ là một tham số không âm được sử dụng để kiểm soát cường độ của hình phạt. Hình phạt này khuyến khích r(z)i, i∈[n] (tức là, các mục liên quan đến n chuyên gia) nhận được nhiều xác suất hơn. Trong các thí nghiệm của chúng tôi, chúng tôi quan sát rằng ∑i∈[n] r(z)i hội tụ về 1, khi μ đủ lớn. Hình phạt được mô tả ở trên là một phần của việc triển khai TensorFlow của chúng tôi cho DSelect-k. Chúng tôi cũng lưu ý rằng có các lựa chọn thay thế tiềm năng khác để xử lý các mục r(z)i, i∈{n+1,n+2,...,2m}, mà không thêm hình phạt vào hàm mục tiêu. Ví dụ, một lựa chọn thay thế là gán ngẫu nhiên mỗi r(z)i, i∈{n+1,n+2,...,2m} cho một trong n chuyên gia.

B.3 Hàm Smooth-step

Trong Hình B.4, chúng tôi vẽ đồ thị hàm smooth-step [11] và hàm logistic L(x) = (1+e^(-6x))^(-1). Lưu ý rằng hàm logistic được chia lại quy mô để có cùng quy mô với hàm smooth-step.

Hình B.4: Các hàm Smooth-step (η = 1) và Logistic.

--- TRANG 15 ---
C Kết quả Thí nghiệm Bổ sung

C.1 Hiệu suất Dự đoán và Lựa chọn Chuyên gia trên Dữ liệu Tổng hợp

Trong thí nghiệm này, chúng tôi nhằm mục đích (i) hiểu cách số lượng chuyên gia và nhiệm vụ ảnh hưởng đến hiệu suất dự đoán và lựa chọn chuyên gia cho các cổng khác nhau, và (ii) định lượng lợi ích từ mã hóa nhị phân trong cổng của chúng tôi thông qua một nghiên cứu ablation. Chúng tôi tập trung vào thiết lập cổng tĩnh, trong đó chúng tôi xem xét các cổng DSelect-k và Top-k, cùng với hai biến thể của cổng DSelect-k được sử dụng cho ablation. Để định lượng tốt hơn hiệu suất lựa chọn chuyên gia và tránh sai lệch mô hình, chúng tôi sử dụng dữ liệu tổng hợp được tạo từ MoE đa cổng. Đầu tiên, chúng tôi mô tả quy trình tạo dữ liệu của chúng tôi.

Tạo Dữ liệu Tổng hợp: Chúng tôi xem xét 128 nhiệm vụ hồi quy, được phân thành bốn nhóm loại trừ lẫn nhau: {Gi}i∈[4], trong đó Gi là tập hợp các chỉ số của các nhiệm vụ trong nhóm i. Như chúng tôi sẽ thảo luận tiếp theo, các nhiệm vụ được xây dựng theo cách mà các nhiệm vụ trong mỗi nhóm có mối liên hệ cao, trong khi các nhiệm vụ giữa các nhóm chỉ có liên quan một cách biên lề. Cấu trúc như vậy mô phỏng các ứng dụng thế giới thực trong đó các nhiệm vụ có thể được phân cụm theo mức độ liên quan.

Mỗi nhóm bao gồm 16 nhiệm vụ được tạo từ một MoE đặc thù cho nhóm. MoE đặc thù cho nhóm bao gồm 4 chuyên gia: {fi}i∈[4]. Mỗi chuyên gia là tổng của 4 đơn vị được kích hoạt ReLU. Đầu ra của mỗi nhiệm vụ trong nhóm là một kết hợp lồi của 4 chuyên gia. Cụ thể, đối với mỗi nhiệm vụ t∈[16] trong nhóm, đặt β(t)∈R4 là một vector trọng số đặc thù cho nhiệm vụ. Sau đó, với một vector đầu vào x, đầu ra của nhiệm vụ t được định nghĩa như sau:

y(t)(x) := ∑i=1^4 (β(t))i fi(x)

Đối với mỗi nhóm, chúng tôi tạo một thể hiện của MoE đặc thù cho nhóm được mô tả ở trên, trong đó chúng tôi khởi tạo tất cả các trọng số ngẫu nhiên và độc lập với các nhóm khác. Cụ thể, chúng tôi lấy mẫu các trọng số của mỗi chuyên gia độc lập từ một phân phối chuẩn tiêu chuẩn. Để khuyến khích sự liên quan giữa các nhiệm vụ trong mỗi nhóm, chúng tôi lấy mẫu các trọng số nhiệm vụ [β(1),β(2)...β(16)] từ một phân phối chuẩn đa biến với trung bình bằng không trong đó chúng tôi đặt tương quan giữa bất kỳ hai trọng số nhiệm vụ nào là 0.8.

Để tạo dữ liệu, chúng tôi lấy mẫu một ma trận dữ liệu X, với 140,000 quan sát và 10 đặc trưng, từ một phân phối chuẩn tiêu chuẩn. Ma trận dữ liệu được chia sẻ bởi tất cả 128 nhiệm vụ và các đầu ra hồi quy được thu được bằng cách sử dụng X làm đầu vào cho mỗi MoE đặc thù cho nhóm. Chúng tôi sử dụng 100,000 quan sát cho tập huấn luyện và 20,000 quan sát cho mỗi tập validation và testing.

Thiết kế Thí nghiệm: Chúng tôi xem xét MoE đa cổng và so sánh các cổng tĩnh sau: cổng DSelect-k, cổng Top-k, và một cổng "ablation" (sẽ được thảo luận sau trong mục này). Mục tiêu của chúng tôi là nghiên cứu cách, đối với mỗi cổng, số lượng nhiệm vụ ảnh hưởng đến hiệu suất dự đoán và lựa chọn chuyên gia. Để đạt được điều này, chúng tôi xem xét 4 bài toán hồi quy, mỗi bài cho một tập con khác nhau của 128 nhiệm vụ; cụ thể, chúng tôi xem xét dự đoán các nhiệm vụ trong (i) G1 (16 nhiệm vụ), (ii) G1∪G2 (32 nhiệm vụ), (iii) G1∪G2∪G3 (64 nhiệm vụ), và (iv) G1∪G2∪G3∪G4 (128 nhiệm vụ). Trong mỗi bài toán trong bốn bài toán, chúng tôi sử dụng MoE đa cổng để dự đoán đầu ra của các nhiệm vụ tương ứng đồng thời. MoE có cùng số lượng chuyên gia được sử dụng để tạo dữ liệu, tức là, nếu T là tổng số nhiệm vụ trong bài toán, MoE bao gồm T/4 chuyên gia, trong đó các chuyên gia tương tự như những chuyên gia được sử dụng trong việc tạo dữ liệu (nhưng có thể huấn luyện được). Mỗi nhiệm vụ được liên kết với một cổng đặc thù cho nhiệm vụ, chọn một kết hợp lồi của 4 trong số T/4 chuyên gia. Lưu ý rằng không giống như kiến trúc được sử dụng để tạo dữ liệu, mỗi cổng nhiệm vụ ở đây được kết nối với tất cả các chuyên gia, ngay cả những chuyên gia thuộc về các nhóm không liên quan. Kiến trúc được sử dụng để tạo dữ liệu có thể được khôi phục nếu các cổng nhiệm vụ giữa các nhóm không chia sẻ chuyên gia, và các cổng nhiệm vụ trong mỗi nhóm chia sẻ cùng 4 chuyên gia. Chúng tôi sử dụng squared error loss để huấn luyện và điều chỉnh.

Ablation: Ngoài việc so sánh các cổng DSelect-k và Top-k, chúng tôi thực hiện một nghiên cứu ablation để hiểu rõ về vai trò của mã hóa nhị phân trong cổng DSelect-k. Nhớ lại rằng trong cổng DSelect-k, chúng tôi đã giới thiệu bộ chọn chuyên gia đơn học một vector mã hóa một-nóng (sử dụng sơ đồ mã hóa nhị phân). Trong tài liệu, một cách phổ biến để học các vector mã hóa một-nóng như vậy là bằng cách sử dụng softmax (với các heuristic bổ sung như làm lạnh nhiệt độ để đảm bảo rằng xác suất tập trung trên một mục). Do đó, trong nghiên cứu ablation của chúng tôi, chúng tôi xem xét cổng DSelect-k q̃(α,Z), và chúng tôi thay thế mỗi bộ chọn chuyên gia đơn r(·) bằng một bộ chọn dựa trên softmax. Chính xác hơn, đặt γ∈Rk và ψ(i)∈Rn, i∈[k], là các vector tham số có thể học. Sau đó, chúng tôi xem xét cổng "ablation" sau: h(γ,ψ) := ∑i=1^k σ(γ)i σ(ψ(i)). Ở đây, σ(γ)i xác định trọng số được gán cho bộ chọn i, và σ(ψ(i)) hoạt động như một thay thế cho bộ chọn chuyên gia đơn r(Sη(z(i))). Để đảm bảo rằng σ(ψ(i)) chọn một chuyên gia đơn (tức là, dẫn đến mã hóa một-nóng), chúng tôi xem xét hai lựa chọn thay thế: (i) làm lạnh nhiệt độ của σ(ψ(i)) trong quá trình huấn luyện6, và (ii) tăng cường mục tiêu với một thuật ngữ chính quy hóa entropy (tương tự như của cổng DSelect-k) để tối thiểu hóa entropy của mỗi σ(ψ(i)). Trong kết quả của chúng tôi, chúng tôi gọi (i) là "Ablation (Annealing)" và (ii) là "Ablation (Entropy)". Lưu ý rằng hai lựa chọn ablation này có thể hội tụ đến mã hóa một-nóng một cách tiệm cận (do bản chất của softmax), trong khi cổng được đề xuất của chúng tôi có thể hội tụ trong một số bước hữu hạn.

Đo lường Hiệu suất Lựa chọn Chuyên gia: Để định lượng sự tương tự giữa các chuyên gia được chọn bởi các nhiệm vụ khác nhau, chúng tôi sử dụng chỉ số Jaccard. Với hai nhiệm vụ, đặt A và B là các tập hợp chuyên gia được chọn bởi nhiệm vụ thứ nhất và thứ hai, tương ứng. Chỉ số Jaccard của hai tập hợp này được định nghĩa bởi: |A∩B|/|A∪B|. Trong các thí nghiệm của chúng tôi, chúng tôi tính toán: (i) chỉ số Jaccard trung bình cho các nhiệm vụ liên quan, và (ii) chỉ số Jaccard trung bình cho các nhiệm vụ không liên quan. Cụ thể, chúng tôi thu được (i) bằng cách tính toán chỉ số Jaccard cho mỗi cặp nhiệm vụ liên quan, và sau đó lấy trung bình. Chúng tôi thu được (ii) bằng cách tính toán chỉ số Jaccard trên tất cả các cặp nhiệm vụ thuộc về các nhóm khác nhau (tức là, các cặp trong cùng nhóm bị bỏ qua), và sau đó lấy trung bình.

Kết quả: Sau khi điều chỉnh, chúng tôi huấn luyện mỗi mô hình cạnh tranh, với các siêu tham số tốt nhất, cho 100 lần lặp lại được khởi tạo ngẫu nhiên. Trong Hình C.5, chúng tôi vẽ đồ thị các thước đo hiệu suất (trung bình qua các lần lặp lại) so với số lượng nhiệm vụ. Trong đồ thị bên trái, chúng tôi báo cáo MSE trên tập test. Trong các đồ thị giữa và phải chúng tôi báo cáo chỉ số Jaccard (trung bình) cho các nhiệm vụ liên quan và không liên quan, tương ứng. Trong hai đồ thị sau, chúng tôi cũng xem xét một cổng ngẫu nhiên chọn 4 chuyên gia một cách đồng đều ngẫu nhiên, và vẽ đồ thị giá trị kỳ vọng của chỉ số Jaccard của nó. Trong Hình C.5 (giữa), một chỉ số lớn hơn tốt hơn vì các nhiệm vụ liên quan sẽ chia sẻ nhiều chuyên gia hơn. Ngược lại, trong Hình C.5 (phải), một chỉ số thấp hơn được ưa thích vì các nhiệm vụ không liên quan sẽ chia sẻ ít chuyên gia hơn. Đối với tất cả các phương pháp, chỉ số Jaccard trong Hình C.5 (giữa) và (phải) giảm theo số lượng nhiệm vụ. Điều này trực quan, vì khi số lượng nhiệm vụ tăng, chúng ta sử dụng nhiều chuyên gia hơn, cho bất kỳ hai cổng nhất định nào nhiều sự linh hoạt hơn trong việc chọn các tập con chuyên gia loại trừ lẫn nhau.

Nhìn chung, kết quả chỉ ra rằng cổng DSelect-k vượt trội đáng kể so với Top-k trong tất cả các thước đo hiệu suất được xem xét, và sự khác biệt trở nên rõ rệt hơn khi số lượng nhiệm vụ tăng. Ví dụ, ở 128 nhiệm vụ, DSelect-k đạt được hơn 40% cải thiện trong MSE và 76% cải thiện trong chỉ số Jaccard cho các nhiệm vụ liên quan, so với Top-k. Cổng DSelect-k cũng vượt trội hơn hai cổng ablation trong đó chúng tôi thay thế mã hóa nhị phân bằng bộ chọn dựa trên Softmax. Cải thiện sau cho thấy rằng sơ đồ mã hóa nhị phân được đề xuất tương đối hiệu quả trong việc chọn các chuyên gia phù hợp. Chúng tôi cũng điều tra hiệu suất kém của cổng Ablation (Entropy), và hóa ra các bộ chọn chuyên gia đơn dựa trên Softmax, tức là, các σ(ψ(i)), có xu hướng chọn cùng một chuyên gia. Cụ thể, chúng tôi đặt k = 4 trong cổng ablation, nhưng nó kết thúc việc chọn 2 chuyên gia trong nhiều lần lặp lại huấn luyện. Ngược lại, các cổng DSelect-k và Top-k chọn 4 chuyên gia.

6Có những trường hợp bệnh lý mà việc làm lạnh nhiệt độ trong softmax sẽ hội tụ đến nhiều hơn một mục khác không. Điều này có thể xảy ra khi nhiều mục trong đầu vào của softmax có cùng giá trị chính xác.

--- TRANG 16 ---
Hình C.5: Hiệu suất dự đoán và lựa chọn chuyên gia trên dữ liệu tổng hợp được tạo từ MoE.

C.2 Trực quan hóa Cổng

C.2.1 MovieLens

Trong Hình C.6, chúng tôi vẽ đồ thị các trọng số chuyên gia trong quá trình huấn luyện trên bộ dữ liệu MovieLens, cho các cổng Top-k và DSelect-k (sau khi điều chỉnh cả hai mô hình). Các đồ thị cho thấy rằng Top-k thể hiện các "nhảy" thường xuyên, trong đó trong một bước huấn luyện duy nhất trọng số của một chuyên gia có thể thay đổi đột ngột từ giá trị khác không thành không. Những nhảy này tiếp tục xảy ra cho đến khi kết thúc huấn luyện (vào khoảng 10^5 bước huấn luyện). Ngược lại, DSelect-k có các chuyển tiếp mượt mà trong quá trình huấn luyện. Chi tiết bổ sung về bộ dữ liệu MovieLens và kiến trúc MoE được sử dụng có thể được tìm thấy trong Mục 4.1 của bài báo.

Hình C.6: Trọng số chuyên gia trong quá trình huấn luyện trên bộ dữ liệu MovieLens. Mỗi màu tương ứng với một chuyên gia riêng biệt. Các đồ thị dành cho các mô hình tốt nhất thu được sau khi điều chỉnh.

C.2.2 Dữ liệu Tổng hợp

Ở đây chúng tôi xem xét một bộ dữ liệu phân loại nhị phân được tạo từ MoE tĩnh bao gồm 4 chuyên gia. Chúng tôi huấn luyện một mô hình MoE khác sử dụng 16 chuyên gia: 4 trong số các chuyên gia này là bản sao (tức là, có cùng trọng số chính xác) của 4 chuyên gia được sử dụng trong việc tạo dữ liệu, và phần còn lại của các chuyên gia được khởi tạo ngẫu nhiên. Chúng tôi đóng băng tất cả các chuyên gia và chỉ huấn luyện trên các tham số cổng. Trong thiết lập đơn giản này, chúng tôi mong đợi cổng có thể khôi phục 4 chuyên gia đã được sử dụng để tạo dữ liệu. Chúng tôi huấn luyện hai mô hình MoE: một dựa trên Top-k và một dựa trên DSelect-k. Sau khi điều chỉnh cả hai mô hình, Top-k chỉ khôi phục được 1 chuyên gia đúng (và mắc 3 lỗi), trong khi mô hình của chúng tôi khôi phục tất cả 4 chuyên gia. Trong Hình C.7 và C.8, chúng tôi vẽ đồ thị các trọng số chuyên gia trong quá trình huấn luyện, cho các cổng Top-k và DSelect-k, tương ứng. Top-k thể hiện hành vi dao động mạnh trong quá trình huấn luyện, trong khi DSelect-k có các chuyển tiếp mượt mà.

Chi tiết bổ sung về tạo dữ liệu và mô hình: Chúng tôi xem xét một MoE "tạo dữ liệu" được khởi tạo ngẫu nhiên với 4 chuyên gia (mỗi chuyên gia là một lớp dense được kích hoạt ReLU với 4 đơn vị). Đầu ra của MoE được thu được bằng cách lấy trung bình của 4 chuyên gia và đưa vào một đơn vị logistic duy nhất. Chúng tôi tạo một ma trận dữ liệu đa biến chuẩn X với 20,000 quan sát và 10 đặc trưng (10,000 quan sát được phân bổ cho mỗi tập huấn luyện và validation). Để tạo nhãn phân loại nhị phân, chúng tôi sử dụng X làm đầu vào cho MoE tạo dữ liệu và áp dụng hàm dấu cho đầu ra tương ứng. Để huấn luyện và điều chỉnh, chúng tôi xem xét kiến trúc MoE với 16 chuyên gia: 4 trong số các chuyên gia này là bản sao của các chuyên gia được sử dụng trong việc tạo dữ liệu, và phần còn lại của các chuyên gia được khởi tạo ngẫu nhiên. Tất cả các chuyên gia đều bị đóng băng (không thể huấn luyện). Một cổng có thể huấn luyện chọn 4 trong số 16 chuyên gia, và kết quả cuối cùng được đưa vào một đơn vị logistic duy nhất. Chúng tôi tối ưu hóa cross-entropy loss bằng Adam với kích thước batch là 256, và điều chỉnh trên learning rate trong {10^(-1), 10^(-2), ..., 10^(-5)}.

--- TRANG 17 ---
Hình C.7: Trọng số chuyên gia trong quá trình huấn luyện trên dữ liệu tổng hợp được tạo từ MoE. Mỗi màu tương ứng với một chuyên gia riêng biệt. Đồ thị bên trái là phiên bản phóng to của đồ thị bên phải. Các đồ thị dành cho mô hình tốt nhất thu được sau khi điều chỉnh.

Hình C.8: Trọng số chuyên gia trong quá trình huấn luyện trên dữ liệu tổng hợp được tạo từ MoE. Mỗi màu tương ứng với một chuyên gia riêng biệt. Đồ thị bên trái là phiên bản phóng to của đồ thị bên phải. Các đồ thị dành cho mô hình tốt nhất thu được sau khi điều chỉnh.

C.3 Hội tụ Cổng và FLOPS

Trong Bảng C.4, chúng tôi báo cáo tỷ lệ phần trăm các bước huấn luyện cần thiết để Sη(Z) hội tụ đến một ma trận nhị phân trong cổng DSelect-k, trên một số bộ dữ liệu thực tế. Những kết quả này dựa trên các mô hình được điều chỉnh đã thảo luận trong Mục 4 của bài báo. Chúng tôi cũng báo cáo số lượng phép toán dấu phẩy động (FLOPS) cần thiết bởi MoE dựa trên DSelect-k so với MoE dựa trên Top-k, trong quá trình huấn luyện. Kết quả chỉ ra rằng số bước huấn luyện cho đến khi hội tụ đến ma trận nhị phân phụ thuộc vào bộ dữ liệu cụ thể: từ chỉ 0.04% trên bộ dữ liệu MovieLens đến 80% trên Multi-Fashion MNIST. Hơn nữa, trên một số bộ dữ liệu (MovieLens với λ = 0.9 và Multi-MNIST), DSelect-k yêu cầu ít FLOPS hơn trong quá trình huấn luyện so với Top-k, tức là, DSelect-k hiệu quả trong huấn luyện có điều kiện (trên các bộ dữ liệu cụ thể này).

MovieLens | Multi-MNIST | Multi-Fashion
λ = 0.1 | λ = 0.5 | λ = 0.9 | | 
% Bước Huấn luyện cho đến Sη(Z) Nhị phân | 11.37 | 9.39 | 0.04 | 42.33 | 80.02
FLOPS(DSelect-k)/FLOPS(Top-k) | 1.5 | 1.2 | 0.6 | 0.8 | 1.2

Bảng C.4: Chúng tôi báo cáo hai thống kê: (i) Tỷ lệ phần trăm các bước huấn luyện cần thiết để cổng DSelect-k hội tụ đến ma trận nhị phân, và (ii) số FLOPS cần thiết bởi MoE dựa trên DSelect-k trong quá trình huấn luyện so với Top-k. Tham số λ kiểm soát trọng số được gán cho loss của nhiệm vụ 1 trong bộ dữ liệu MovieLens—xem Mục 4.1 của bài báo để biết thêm chi tiết.

--- TRANG 18 ---
C.4 MovieLens

Trong Bảng C.5, chúng tôi báo cáo accuracy cho nhiệm vụ 1 (phân loại) và loss cho nhiệm vụ 2 (hồi quy) cho các phương pháp cạnh tranh trên bộ dữ liệu MovieLens.

λ = 0.1 | λ = 0.5 | λ = 0.9
T2 Loss | T1 Accuracy | T2 Loss | T1 Accuracy | T2 Loss | T1 Accuracy
Tĩnh DSelect-k | 40385 | 83.03±0.07 | 39266 | 83.86±0.02 | 39435 | 84.04±0.01
Top-k | 40564 | 84.09±0.05 | 40024 | 84.21±0.02 | 38844 | 84.17±0.02
Gumbel Softmax | 41722 | 80.76±0.04 | 40853 | 83.71±0.01 | 38784 | 84.17±0.02
Theo từng ví dụ DSelect-k | 40307 | 83.03±0.12 | 39817 | 83.92±0.03 | 39321 | 84.07±0.02
Top-k | 40579 | 83.28±0.13 | 39958 | 83.91±0.03 | 39144 | 84.05±0.01
Baseline Softmax MoE | 40471 | 78.73±0.01 | 40283 | 83.56±0.01 | 39703 | 83.86±0.02
Shared Bottom | 39932 | 79.06±0.02 | 38752 | 82.65±0.02 | 39912 | 84.01±0.01

Bảng C.5: Loss test trung bình cho nhiệm vụ 2 (T2) và accuracy cho nhiệm vụ 1 (T1) trên MovieLens (sai số chuẩn được hiển thị bên cạnh mỗi trung bình). Tham số λ xác định trọng số của Loss Nhiệm vụ 1 (xem văn bản để biết chi tiết). Test loss được nhân với 10^4.

D Chi tiết Thí nghiệm

Thiết lập Tính toán: Chúng tôi chạy các thí nghiệm trên một cụm tự động phân bổ tài nguyên tính toán. Chúng tôi không báo cáo thông số kỹ thuật chính xác của cụm vì lý do bảo mật.

Cổng Gumbel-softmax: [34,25] trình bày một phương pháp để học lớp nào trong mạng nơ-ron cần kích hoạt trên cơ sở từng nhiệm vụ. Quyết định chọn mỗi lớp được mô hình hóa bằng một biến ngẫu nhiên nhị phân có phân phối được học bằng thủ thuật Gumbel-softmax. Lưu ý rằng phương pháp sau không xem xét mô hình MoE. Ở đây chúng tôi điều chỉnh phương pháp sau cho MoE; cụ thể chúng tôi xem xét cổng Gumbel-softmax sử dụng các biến nhị phân để xác định chuyên gia nào cần chọn. Với n chuyên gia {fi}i=1^n, cổng này sử dụng n biến ngẫu nhiên nhị phân {Ui}i=1^n, trong đó Ui xác định liệu chuyên gia fi có được chọn hay không. Hơn nữa, cổng sử dụng một vector có thể học bổ sung γ∈Rn xác định trọng số của các chuyên gia. Cụ thể, cổng là một hàm d(γ,U) có thành phần thứ i (đối với bất kỳ i∈[n]) được cho bởi:

d(γ,U)i = σ(γ)i Ui

Để học phân phối của các Ui, chúng tôi sử dụng thủ thuật Gumbel-softmax như được mô tả trong [34]. Hơn nữa, theo [34], chúng tôi thêm hình phạt khuyến khích độ thưa sau vào hàm mục tiêu: ∑i∈[n] log πi, trong đó πi là tham số phân phối Bernoulli của Ui, và μ là một tham số không âm được sử dụng để kiểm soát số lượng khác không được chọn bởi cổng. Lưu ý rằng hình phạt sau không thể kiểm soát trực tiếp số lượng khác không như trong DSelect-k hoặc Top-k.

D.1 MovieLens

Kiến trúc: Đối với các mô hình dựa trên MoE, chúng tôi xem xét kiến trúc MoE đa cổng (xem Hình 1), trong đó mỗi nhiệm vụ được liên kết với một cổng riêng biệt. MoE sử dụng 8 chuyên gia, mỗi chuyên gia là một lớp dense được kích hoạt ReLU với 256 đơn vị, theo sau bởi một lớp dropout (với tỷ lệ dropout là 0.5). Đối với mỗi trong hai nhiệm vụ, kết hợp lồi tương ứng của các chuyên gia được đưa vào một mạng con đặc thù cho nhiệm vụ. Mạng con được cấu thành từ một lớp dense (được kích hoạt ReLU với 256 đơn vị) theo sau bởi một đơn vị duy nhất tạo ra đầu ra cuối cùng của nhiệm vụ. Mô hình shared bottom sử dụng một lớp dense (có số đơn vị là một siêu tham số) được chia sẻ bởi hai nhiệm vụ, theo sau bởi một lớp dropout (với tỷ lệ 0.5). Đối với mỗi nhiệm vụ, đầu ra của lớp được chia sẻ được đưa vào một mạng con đặc thù cho nhiệm vụ (giống như của các mô hình dựa trên MoE).

Siêu tham số và Điều chỉnh: Chúng tôi điều chỉnh mỗi mô hình bằng tìm kiếm lưới ngẫu nhiên, với trung bình 5 thử nghiệm cho mỗi điểm lưới. Chúng tôi sử dụng Adagrad với kích thước batch là 128 và xem xét các siêu tham số và phạm vi sau: Learning Rate: {0.001,0.01,0.1,0.2,0.3}, Epochs: {5,10,20,30,40,50}, k cho Top-k và DSelect-k: {2,4}, λ cho DSelect-k: {0.1,1,10}, η cho smooth-step: {1,10}, Đơn vị trong Shared bottom: {32,256,2048,4096,8192}, μ trong Gumbel-softmax: {10^(-6),10^(-5),...,10}. Đối với Gumbel-softmax, chúng tôi chọn giải pháp tốt nhất có số lượng khác không kỳ vọng nhỏ hơn hoặc bằng 4.

--- TRANG 19 ---
D.2 Multi-MNIST và Multi-Fashion MNIST

Kiến trúc: Các mô hình dựa trên MoE sử dụng MoE đa cổng (như trong Hình 1). Mỗi trong 8 chuyên gia là một CNN được cấu thành (theo thứ tự) bởi: (i) lớp tích chập 1 (kích thước kernel = 5, số filter = 10, được kích hoạt ReLU) theo sau bởi max pooling, (ii) lớp tích chập 2 (kích thước kernel = 5, số filter = 20, được kích hoạt ReLU) theo sau bởi max pooling, và (iii) một stack các lớp dense được kích hoạt ReLU với 50 đơn vị mỗi lớp (số lượng lớp là một siêu tham số). Mạng con đặc thù cho mỗi nhiệm vụ được cấu thành từ một stack 3 lớp dense: hai lớp đầu có 50 đơn vị được kích hoạt ReLU và lớp thứ ba có 10 đơn vị theo sau bởi softmax. Mô hình shared bottom sử dụng CNN được chia sẻ (với cùng kiến trúc như CNN trong MoE). Đối với mỗi nhiệm vụ, đầu ra của CNN được chia sẻ được đưa vào mạng con đặc thù cho nhiệm vụ (giống như của các mô hình dựa trên MoE).

Siêu tham số và Điều chỉnh: Chúng tôi điều chỉnh mỗi mô hình bằng tìm kiếm lưới ngẫu nhiên, với trung bình 5 thử nghiệm cho mỗi điểm lưới. Chúng tôi sử dụng Adam với kích thước batch là 256 và xem xét các siêu tham số và phạm vi sau: Learning Rate: {0.01,0.001,0.0001,0.00001}, Epochs: {25,50,75,100}, k cho Top-k và DSelect-k: {2,4}, η cho smooth-step: {0.1,1,10}, Số lớp dense trong CNN: {1,3,5}, μ trong Gumbel-softmax: {0,10^(-3),10^(-2),10^(-1),1,10,1000}. Đối với Gumbel-softmax, chúng tôi chọn giải pháp tốt nhất có số lượng khác không kỳ vọng nhỏ hơn hoặc bằng 4.

D.3 Hệ thống Gợi ý

Mỗi trong 8 chuyên gia trong MoE bao gồm một stack các lớp dense được kích hoạt ReLU với 256 đơn vị mỗi lớp. Chúng tôi cố định k là 2 trong cả DSelect-k và Top-k. Chúng tôi điều chỉnh trên learning rate và kiến trúc. Đối với cả hai mô hình, việc huấn luyện được chấm dứt khi không có cải thiện đáng kể trong validation loss.

D.4 Dữ liệu Tổng hợp

Chúng tôi điều chỉnh mỗi mô hình bằng tìm kiếm lưới ngẫu nhiên, với trung bình 5 thử nghiệm cho mỗi điểm lưới. Chúng tôi sử dụng Adam với kích thước batch là 256 và xem xét các siêu tham số và phạm vi sau: Learning rate: {0.001,0.01,0.1}, Epochs: {25,50,75,100}, η cho smooth-step: {5,10,15}, λ cho DSelect-k: {0.001,0.005,0.01,0.1}, λ cho Ablation (Entropy): {10^(-6),10^(-5),10^(-4),10^(-3),10^(-2),10^(-1),1,100}. Hơn nữa, đối với Ablation (Annealing), chúng tôi làm lạnh nhiệt độ của softmax bắt đầu từ một siêu tham số s xuống 10^(-16) (các nhiệt độ được phân bố đều trên thang logarit). Chúng tôi điều chỉnh nhiệt độ bắt đầu s trên {10^(-6),10^(-5),5×10^(-5),10^(-4),2.5×10^(-4),5×10^(-4),7.5×10^(-4),10^(-3),5×10^(-3),10^(-2)} (lưu ý rằng một lưới tinh như vậy là cần thiết để làm cho annealing hoạt động cho cổng ablation).
