# HMOE: Mạng chuyên gia trộn dựa trên siêu mạng cho tổng quát hóa miền

Jingang Qu1 Thibault Faney2 Ze Wang1 Patrick Gallinari1,3
Soleiman Yousef2 Jean-Charles de Hemptinne2
Đại học Sorbonne, CNRS, ISIR, 75005 Paris, Pháp1 IFPEN2 Criteo AI Lab, Paris, Pháp3

## Tóm tắt

Do sự dịch chuyển miền, các hệ thống học máy thường khó có thể tổng quát hóa tốt cho các miền mới khác biệt với dữ liệu huấn luyện, đó là vấn đề mà tổng quát hóa miền (DG) nhắm đến giải quyết. Mặc dù đã có nhiều phương pháp DG được đề xuất, hầu hết đều thiếu tính diễn giải và yêu cầu nhãn miền, điều này không có sẵn trong nhiều tình huống thực tế. Bài báo này trình bày một phương pháp DG mới, gọi là HMOE: Mạng chuyên gia trộn dựa trên siêu mạng (MoE), không phụ thuộc vào nhãn miền và có tính diễn giải cao hơn. MoE đã chứng minh hiệu quả trong việc xác định các mẫu không đồng nhất trong dữ liệu. Đối với vấn đề DG, tính không đồng nhất phát sinh chính xác từ sự dịch chuyển miền. HMOE sử dụng siêu mạng lấy vector làm đầu vào để tạo ra trọng số của các chuyên gia, điều này thúc đẩy việc chia sẻ kiến thức giữa các chuyên gia và cho phép khám phá sự tương đồng của chúng trong không gian vector chiều thấp. Chúng tôi đánh giá HMOE so với các phương pháp DG khác dưới một khung đánh giá công bằng - DomainBed. Các thí nghiệm mở rộng của chúng tôi cho thấy HMOE có thể phân tách hiệu quả dữ liệu trộn miền thành các cụm riêng biệt phù hợp với trực giác con người hơn so với nhãn miền gốc một cách đáng ngạc nhiên. Sử dụng thông tin miền tự học, HMOE đạt được kết quả tốt nhất trên hầu hết các tập dữ liệu và vượt trội đáng kể so với các phương pháp DG khác về độ chính xác trung bình trên tất cả các tập dữ liệu.

## 1. Giới thiệu

Tổng quát hóa miền (DG) nhắm đến huấn luyện mô hình trên các miền đã biết để hoạt động tốt trên các miền chưa thấy, điều này rất quan trọng để triển khai mô hình trong các ứng dụng đòi hỏi tính an toàn cao. Trong thập kỷ qua, nhiều thuật toán DG đã được đề xuất [28, 87, 101], tập trung chủ yếu vào việc phát triển các kỹ thuật tăng cường dữ liệu đặc thù cho DG và học các biểu diễn bất biến miền để xây dựng bộ dự đoán có khả năng tổng quát hóa. Tuy nhiên, nhiều thuật toán DG hiệu suất cao phụ thuộc vào nhãn miền để giảm rõ ràng sự khác biệt giữa các miền, hạn chế nghiêm trọng khả năng ứng dụng của chúng trong các tình huống thực tế nơi việc chú thích miền có thể cực kỳ tốn kém. Ngoài ra, các thuật toán DG hiện tại thiếu tính diễn giải và không thể cung cấp cái nhìn sâu sắc về nguyên nhân thành công hoặc thất bại trong việc tổng quát hóa cho các miền mới. Do đó, công trình này nhắm đến phát triển một thuật toán DG mới không yêu cầu nhãn miền và có tính diễn giải cao hơn.

Chúng tôi tuân theo danh pháp được thiết lập bởi [11], gọi DG với nhãn miền là DG vanilla và DG khó khăn hơn không có nhãn miền là DG hợp chất. Công trình này tập trung vào giải quyết DG hợp chất bằng cách suy luận các miền tiềm ẩn từ dữ liệu trộn miền và sử dụng chúng hiệu quả. [6, 14, 58] đã chứng minh rằng việc sử dụng các tập dữ liệu theo miền có thể tạo ra các cận sai số tổng quát hóa thấp hơn về mặt lý thuyết và hiệu suất DG tốt hơn so với việc sử dụng dữ liệu trộn trực tiếp, cho thấy tầm quan trọng của thông tin miền. Hơn nữa, việc khám phá miền tiềm ẩn giúp chúng ta hiểu cách hoạt động của mô hình và tăng cường tính diễn giải. Để làm cho vấn đề có thể giải quyết được, chúng tôi giả định rằng các miền tiềm ẩn là riêng biệt và có thể tách rời.

Trong bài báo này, chúng tôi giới thiệu HMOE: Mạng chuyên gia trộn dựa trên siêu mạng (MoE). MoE là một mô hình học được thiết lập tốt, tập hợp một số chuyên gia bằng cách tính tổng có trọng số các dự đoán của chúng [36, 37], trong đó các trọng số tập hợp, thường được gọi là giá trị cổng, được xác định bởi một cơ chế định tuyến và tổng bằng 1. HMOE tận dụng tính chất chia để trị của MoE, tức là cơ chế định tuyến có thể phân chia mềm không gian đầu vào thành các không gian con một cách không giám sát trong quá trình huấn luyện [92], với mỗi không gian con được gán cho một chuyên gia. Chúng tôi còn kỳ vọng rằng mỗi không gian con được liên kết với một miền tiềm ẩn, cho phép khám phá miền tiềm ẩn. Trong quá trình suy luận, chúng ta có thể so sánh sự tương đồng giữa một miền thử nghiệm chưa thấy và các miền được suy luận dựa trên giá trị cổng, từ đó cải thiện tính diễn giải. [29, 97] đã xác nhận MoE trong thích ứng miền [88] và cho thấy rằng MoE có thể tận dụng tính chuyên môn của từng miền riêng lẻ và giảm thiểu việc chuyển giao kiến thức tiêu cực [75] so với việc sử dụng một mô hình duy nhất để học các miền khác nhau đồng thời.

HMOE sử dụng sáng tạo một mạng neural, gọi là siêu mạng [30], lấy vector làm đầu vào để tạo ra trọng số cho các chuyên gia của MoE. Bằng cách ánh xạ vector sang chuyên gia, siêu mạng cho phép khám phá sự tương đồng của các chuyên gia trong không gian vector chiều thấp, tạo điều kiện thuận lợi cho việc khám phá miền tiềm ẩn. Siêu mạng cũng đóng vai trò như một cầu nối giữa các chuyên gia và cung cấp cho họ một kênh để trao đổi thông tin, từ đó thúc đẩy việc chia sẻ kiến thức.

Việc phân chia mềm nội tại của MoE không phải lúc nào cũng hiệu quả và đôi khi không thể duy trì sự phân chia dữ liệu nhất quán, đặc biệt khi sự phân biệt giữa các miền tiềm ẩn không đáng kể. Để giải quyết vấn đề này, chúng tôi đề xuất một thuật toán định tuyến Top-1 từ dày đặc đến thưa thớt có thể vi phân, buộc các giá trị cổng trở thành one-hot và hội tụ về phân chia cứng. Điều này dẫn đến MoE cổng thưa thớt, cải thiện và ổn định việc khám phá miền tiềm ẩn. Ngoài ra, để tích hợp tốt hơn siêu mạng vào MoE, chúng tôi giới thiệu một không gian nhúng chứa một tập hợp các vector nhúng có thể học tương ứng một-một với các chuyên gia. Không gian nhúng này được cung cấp cho siêu mạng để tạo ra trọng số của các chuyên gia và cũng là một phần của cơ chế định tuyến để tính toán giá trị cổng, từ đó tăng cường tương tác giữa siêu mạng và cơ chế định tuyến.

Chúng tôi cũng đề xuất một mixup nội miền để cải thiện thêm khả năng tổng quát hóa của HMOE. mixup tạo ra các mẫu huấn luyện ảo bằng cách lấy tổ hợp tuyến tính của hai đầu vào được chọn ngẫu nhiên và nhãn của chúng [93], và chúng tôi thực hiện mixup trong mỗi miền tiềm ẩn được suy luận.

Các đóng góp của chúng tôi như sau: (1) Chúng tôi trình bày một phương pháp DG mới - HMOE trong khung MoE, không yêu cầu nhãn miền, cho phép khám phá miền tiềm ẩn, và mang lại tính diễn giải tuyệt vời. (2) HMOE tận dụng siêu mạng để tạo ra trọng số chuyên gia và đạt được MoE cổng thưa thớt. (3) Theo như chúng tôi biết, HMOE là công trình đầu tiên có thể học và sử dụng miền tiềm ẩn một cách đầu cuối. (4) Các thí nghiệm mở rộng được thực hiện để so sánh HMOE với các phương pháp DG khác dưới một khung đánh giá công bằng - DomainBed [28]. HMOE thể hiện hiệu suất tốt nhất trên hầu hết các tập dữ liệu và vượt trội đáng kể so với các phương pháp DG khác về độ chính xác trung bình.

## 2. Công trình liên quan

### 2.1. Tổng quát hóa miền (DG)

Mục tiêu của DG là huấn luyện một bộ dự đoán trên các miền đã biết có thể tổng quát hóa tốt cho các miền chưa thấy.

**DG Vanilla** Hướng công trình đầu tiên là thiết kế các kỹ thuật tăng cường dữ liệu đặc thù cho DG để tăng tính đa dạng và số lượng dữ liệu huấn luyện nhằm cải thiện hiệu suất DG [51, 65, 70, 84, 91, 93, 98, 100]. Các công trình trước đây học các biểu diễn bất biến miền thông qua tối thiểu hóa rủi ro bất biến [1, 2, 41], phương pháp kernel [6, 22, 26, 58], căn chỉnh đặc trưng [25, 47, 56, 57, 61, 63, 76, 78, 86], và huấn luyện đối kháng miền [23, 24, 27, 47, 49]. Một cách tiếp cận khác là tách rời các đặc trưng tiềm ẩn thành các biểu diễn đặc thù cho lớp và đặc thù cho miền [35, 38, 59, 64, 94]. Các mô hình học máy tổng quát cũng được áp dụng cho DG vanilla, như meta-learning [3, 17, 45, 46], học tự giám sát [8, 39], thao tác gradient [34, 66, 72], và tối ưu hóa mạnh mẽ phân phối [41, 67].

**DG Hợp chất** Có một số thuật toán DG không yêu cầu nhãn miền theo thiết kế [11, 34, 48, 56, 59, 94]. Ngoài việc cải thiện hiệu suất DG, việc khám phá miền tiềm ẩn cũng là một nhiệm vụ quan trọng cho DG hợp chất và góp phần vào tính diễn giải tốt hơn. [11, 56] có thể làm điều này nhưng có hai hạn chế chính: (1) Phương pháp của họ tiến hành theo hai giai đoạn: đầu tiên suy luận miền tiềm ẩn từ dữ liệu trộn rồi sau đó giải quyết DG bằng các miền được suy luận, điều này tương tự như DG vanilla. Vấn đề là giai đoạn thứ hai phụ thuộc vào giai đoạn đầu tiên và không thể cung cấp phản hồi để sửa các lỗi có thể có trong việc khám phá miền. (2) Phương pháp của họ giả định rằng sự dịch chuyển miền phát sinh từ sự khác biệt về phong cách để xác định miền tiềm ẩn, điều này không phải lúc nào cũng đúng.

Ngược lại, HMOE được huấn luyện theo cách đầu cuối và tận dụng MoE để khám phá miền tiềm ẩn mà không có thiên hướng cụ thể nào về nguyên nhân của sự dịch chuyển miền.

### 2.2. Siêu mạng

Siêu mạng là một mạng neural tạo ra trọng số của một mạng đích khác. Siêu mạng ban đầu được đề xuất bởi [30] và kể từ đó đã được áp dụng cho các bài toán tối ưu hóa [53, 60], meta-learning [96], học liên tục [7, 85], học đa nhiệm [50, 54, 77], học few-shot [68], và học liên kết [69].

### 2.3. Mạng chuyên gia trộn (MoE)

MoE ban đầu được đề xuất bởi [36, 37] và bao gồm hai thành phần chính: các chuyên gia và một mạng cổng, như được thể hiện trong Hình 1. Đầu ra của MoE là tổng có trọng số của các chuyên gia, với các giá trị cổng được tính toán bởi mạng cổng trên cơ sở từng ví dụ. Trong những năm gần đây, MoE đã thu hút sự chú ý trở lại như một cách để mở rộng quy mô các mô hình học sâu và khai thác phần cứng hiện đại một cách hiệu quả hơn [18, 20, 21, 42, 71, 102]. Trong trường hợp này, MoE thưa thớt được ưa chuộng, định tuyến mỗi ví dụ chỉ đến các chuyên gia có giá trị cổng Top-1 hoặc Top-K.

### 2.4. Ứng dụng của siêu mạng và MoE trong DG

Theo như chúng tôi biết, không có công trình nào đã áp dụng siêu mạng để giải quyết DG trong thị giác máy tính. Gần đây, [83] đã áp dụng siêu mạng cho DG trong xử lý ngôn ngữ tự nhiên (NLP) và đạt được kết quả SOTA trên hai nhiệm vụ DG liên quan đến NLP.

Đối với MoE, [43] đã đề xuất thay thế lớp mạng feed-forward (FFN) của Vision Transformer (ViT) [16] bằng hỗn hợp thưa thớt của các chuyên gia FFN để cải thiện hiệu suất DG. [29, 97] đã áp dụng MoE cho một nhiệm vụ tương tự như DG, cụ thể là thích ứng miền [88], nhưng họ yêu cầu nhãn miền để huấn luyện một chuyên gia cho mỗi miền riêng biệt. [97] tập hợp các đầu ra của chuyên gia thông qua một bộ tập hợp dựa trên transformer, nhưng bộ tập hợp của nó được huấn luyện với các chuyên gia cố định và không thể cung cấp xác suất của chuyên gia, trong khi HMOE có thể làm điều này và có tính diễn giải cao hơn. Ngoài ra, nếu chúng ta coi MoE như một loại phương pháp ensemble, [15, 55, 99] có cùng tinh thần.

## 3. Phương pháp

### 3.1. Thiết lập bài toán

Cho X là không gian đầu vào và Y là không gian đích. Một miền S được đặc trưng bởi một phân phối kết hợp P^s_XY trên X × Y. Trong thiết lập DG vanilla, chúng ta có một tập huấn luyện chứa M miền đã biết, tức là D^V_tr = {D^s}^M_{s=1} với D^s = {(x^s_i, y^s_i, d^s_i)}^{N^s}_{i=1} trong đó (x^s_i, y^s_i) ∼ P^s_XY và d^s_i là chỉ số hoặc nhãn miền. Cũng xem xét một tập dữ liệu thử nghiệm D_te bao gồm các miền chưa biết khác với những miền của D^V_tr.

DG Vanilla nhắm đến huấn luyện một bộ dự đoán mạnh mẽ f: X → Y trên D^V_tr để đạt được sai số dự đoán tối thiểu trên D_te, tức là min_f E_{(x,y)∼D_te}[ℓ(f(x), y)], trong đó ℓ là hàm mất mát.

Công trình của chúng tôi tập trung vào DG hợp chất khó khăn hơn, trong đó tập huấn luyện D_tr = {(x_i, y_i)}^N_{i=1} chứa các miền trộn lẫn và không có chú thích miền. Tuy nhiên, như đã được chứng minh trong [28, 87, 101], các mối quan hệ nội tại giữa các miền đóng vai trò quan trọng trong việc đạt được hiệu suất tổng quát hóa tốt hơn. Do đó, HMOE được đề xuất của chúng tôi được thiết kế để khám phá miền tiềm ẩn bằng cách chia D_tr thành các cụm và tận dụng đầy đủ thông tin miền đã học để hoạt động tốt trên các miền chưa biết.

### 3.2. Kiến trúc tổng thể

Tổng quan về HMOE được minh họa trong Hình 2a. HMOE xử lý đầu vào x thông qua hai đường: đường miền cho việc khám phá miền tiềm ẩn và đường phân loại để huấn luyện một chuyên gia cho mỗi miền tiềm ẩn.

Đường phân loại bắt đầu với một bộ trích xuất đặc trưng h_z để trích xuất các đặc trưng cấp cao từ x, có thể là một mạng đã được huấn luyện trước, như VGG [74], ResNet [32], hoặc ViT [16]. Chúng tôi định nghĩa một không gian nhúng có thể học rời rạc E bao gồm K vector nhúng {e_k ∈ R^D}^K_{k=1} (D đại diện cho chiều nhúng), mỗi vector tương ứng với một chuyên gia phân loại. Các vector này được cung cấp cho một siêu mạng f_h để tạo ra một tập hợp trọng số {θ_k}^K_{k=1}, sau đó tạo thành một tập hợp các chuyên gia {f_c(·; θ_k)}^K_{k=1}. Đầu ra của bộ trích xuất đặc trưng z được chuyển đến các chuyên gia này để tính toán đầu ra tương ứng của chúng, tức là y_k = f_c(z; θ_k).

Đường miền bắt đầu với một bộ mã hóa Domain2Vec (D2V) h_v, chuyển đổi x thành không gian nhúng E và đầu ra v ∈ R^D. Đầu ra v sau đó được so sánh với các vector nhúng thông qua một hàm cổng được định nghĩa trước g(v, E), như được thể hiện trong Hình 2b, để tạo ra một tập hợp xác suất p = {p_k}^K_{k=1}. Đầu ra cuối cùng của HMOE là tổng có trọng số của đầu ra của các chuyên gia như sau:

y = Σ^K_{k=1} p_k y_k = g(h_v(x), E) • [f_c(h_z(x); f_h(e_k))]^K_{k=1}    (1)

### 3.3. Siêu mạng

Chúng tôi sử dụng một siêu mạng f_h lấy một vector e làm đầu vào để tạo ra trọng số cho bộ phân loại f_c. Trong công trình của chúng tôi, cả f_h và f_c đều là MLP. Về bản chất, f_c hoạt động như một placeholder đồ thị tính toán, e là tín hiệu điều kiện, và f_h ánh xạ e thành một hàm. Vai trò của f_h bao gồm: (1) tạo điều kiện thuận lợi cho việc khám phá miền tiềm ẩn, (2) sử dụng nhiều chuyên gia mà không tăng đáng kể tham số, (3) cung cấp một tương tác khác giữa các chuyên gia và cơ chế định tuyến ngoài việc tập hợp các chuyên gia so với MoE cổ điển, và (4) cho phép tổng quát hóa các chuyên gia vượt ra ngoài việc tập hợp (Như chúng ta sẽ thấy sau này, f_h có thể trực tiếp lấy bộ mã hóa D2V làm đầu vào).

### 3.4. Cơ chế định tuyến

#### 3.4.1 Hàm cổng

Để định lượng trách nhiệm của các chuyên gia đối với mỗi ví dụ đầu vào và tập hợp đầu ra của chuyên gia, chúng ta cần tính toán giá trị cổng p. Như được thể hiện trong Hình 2b, dựa trên đầu ra của bộ mã hóa D2V v và không gian nhúng E, chúng tôi định nghĩa một hàm cổng g(v, E) để tính toán p như sau:

d_k = ∥v - e_k∥_2                                    (2a)
s_k = -log(d^2_k + ε)                                (2b)
p_k = exp(s_k) / Σ^K_{j=1} exp(s_j)                  (2c)

trong đó ε là một giá trị nhỏ. Logarithm âm trong Phương trình (2b) được sử dụng để thiết lập một mối tương quan âm giữa d_k và p_k (tức là, d_k càng nhỏ thì p_k càng lớn) và để tái tỷ lệ phi tuyến khoảng cách d (tức là, kéo dài d nhỏ và nén d lớn), làm cho p ít nhạy cảm hơn với d lớn.

#### 3.4.2 Định tuyến Top-1 từ dày đặc đến thưa thớt có thể vi phân

Dựa trên giá trị cổng p, cơ chế định tuyến xác định vị trí và cách định tuyến các ví dụ đầu vào. Một định tuyến nhất quán và liên kết chặt chẽ là rất quan trọng đối với tính ổn định huấn luyện và sự hội tụ của MoE [12]. Để ổn định định tuyến và tăng cường việc khám phá miền tiềm ẩn để nắm bắt sự khác biệt miền ít rõ ràng hơn, MoE cổng thưa thớt được ưa chuộng. Tuy nhiên, các hàm Top-1 hoặc Top-K thường được sử dụng không thể vi phân và có thể gây ra hành vi dao động của giá trị cổng trong quá trình huấn luyện [31]. Để khắc phục hạn chế này, chúng tôi đề xuất một thuật toán định tuyến Top-1 từ dày đặc đến thưa thớt có thể vi phân bằng cách giới thiệu một mất mát entropy trên p như sau:

L_en = E_{(x,y)∼D_tr} [H(g(h_v(x), E))]              (3)

trong đó H(·) biểu thị entropy của một phân phối. Trong thực tế, chúng tôi nhân L_en với γ_en tăng tuyến tính từ 0 đến 1 trong nửa đầu của quá trình huấn luyện và giữ nguyên ở 1 trong nửa sau. Ban đầu, γ_en nhỏ, và khoảng cách giữa v và các vector nhúng gần như bằng nhau, dẫn đến p đồng đều. Do đó, tất cả các chuyên gia có thể được huấn luyện đầy đủ và dần dần trở nên chuyên môn hóa. Trong các giai đoạn sau, L_en buộc p trở thành one-hot dựa trên các chuyên gia chuyên môn hóa.

Do logarithm âm trong Phương trình (2b), bộ mã hóa D2V phải di chuyển về phía một vector nhúng để tối thiểu hóa L_en thay vì di chuyển ra xa khỏi các vector khác.

#### 3.4.3 Cân bằng tải chuyên gia

MoE cổng thưa thớt có thể gặp phải vấn đề tải chuyên gia không cân bằng. Chúng tôi định nghĩa tầm quan trọng của chuyên gia là I(X) = [I_1(X), ···, I_K(X)], trong đó X đại diện cho một batch duy nhất và I_k(X) được chỉ định như tổng các giá trị cổng được gán cho chuyên gia thứ k (tức là, tổng ma trận giá trị cổng trong Hình 1b theo chiều ví dụ). [62] định nghĩa một phân phối P = I(X)/ΣI(X) và sử dụng phân kỳ KL giữa P và phân phối đồng đều U để cân bằng tải chuyên gia, điều này cũng được sử dụng trong công trình của chúng tôi:

L_kl = D_KL(P∥U) = D_KL(I(X)/ΣI(X)∥U)              (4)

### 3.5. Không gian nhúng

Không gian nhúng E đóng vai trò quan trọng trong HMOE. Như chúng ta có thể thấy, các vector nhúng có tác động đến cả việc tạo ra trọng số chuyên gia và cơ chế định tuyến, do đó đóng vai trò như một cầu nối để cân bằng hai phần này. Ngoài ra, các vector nhúng này có thể học được như trọng số của mạng neural và thu hút bộ mã hóa D2V trong quá trình huấn luyện dưới ảnh hưởng của L_en.

### 3.6. Huấn luyện đối kháng lớp trên D2V

Chúng tôi kỳ vọng bộ mã hóa D2V h_v chứa ít thông tin đặc thù cho lớp nhất có thể, điều này đảm bảo rằng HMOE phân chia không gian đầu vào dựa trên sự phân biệt theo miền thay vì các danh mục ngữ nghĩa. Được lấy cảm hứng từ Mạng neural đối kháng miền [24], chúng tôi định nghĩa một bộ phân loại đối kháng f^{ad}_c lấy v làm đầu vào và thêm mất mát sau đây để thực hiện huấn luyện đối kháng lớp trên h_v:

L_ad = E_{(x,y)∼D_tr} [ℓ_ce(f^{ad}_c(GRL(v, λ_grl)), y)]  (5)

trong đó ℓ_ce biểu thị mất mát cross-entropy và GRL đại diện cho lớp đảo ngược gradient, hoạt động như một hàm đồng nhất trong lượt truyền tiến và nhân gradient với -λ_grl trong lượt truyền ngược. Như được gợi ý trong [24], chúng tôi định nghĩa λ_grl như sau:

λ_grl = 2/(1 + exp(-10 × pct_tr)) - 1                  (6)

trong đó pct_tr thay đổi tuyến tính từ 0 đến 1 trong quá trình huấn luyện.

### 3.7. Học có giám sát trên mục tiêu

Chúng tôi cung cấp hai cách để tính toán mất mát có giám sát trên mục tiêu L_y, tức là Tối thiểu hóa rủi ro kinh nghiệm (ERM) [81] và mixup nội miền.

**ERM** Trong thiết lập ERM, mất mát có giám sát trên mục tiêu đơn giản là rủi ro kinh nghiệm trên dữ liệu huấn luyện D_tr:

L_y = E_{(x,y)∼D_tr}[ℓ_ce(ŷ, y)]                      (7)

trong đó ŷ là dự đoán của HMOE, được tính toán bằng Phương trình (1).

**mixup nội miền** mixup huấn luyện mạng neural trên các mẫu ảo được tổng hợp thông qua kết hợp lồi của các cặp mẫu và nhãn của chúng [93]:

x̃ = βx_i + (1-β)x_j                                    (8)
ỹ = βy_i + (1-β)y_j                                    (9)

trong đó β ∼ Beta(α, α) và α điều chỉnh độ mạnh của phép nội suy. mixup có thể được xem như một cách tiếp cận tăng cường dữ liệu được củng cố lý thuyết trong Tối thiểu hóa rủi ro lân cận [10], một nguyên tắc học thay thế cho ERM. [89, 90] đã áp dụng mixup liên miền trộn các mẫu qua các miền khác nhau để học bất biến miền, trong khi mixup nội miền của chúng tôi, như được thể hiện trong Thuật toán 1, thúc đẩy HMOE có dự đoán mượt mà hơn trong vùng lân cận trong mỗi miền, tăng cường khả năng tổng quát hóa và độ mạnh mẽ.

Để thực hiện mixup nội miền mà không có nhãn miền, HMOE bắt đầu với Phương trình (7) rồi chuyển sang Thuật toán 1 cho đến khi L_en < 0.1 cho thấy các miền tiềm ẩn được khám phá và phân cụm một cách hợp lý.

**Thuật toán 1** mixup nội miền
**Yêu cầu:** Một mini-batch B được chia thành các miền riêng biệt cho trước nhãn miền hoặc cụm được xác định bởi giá trị cổng
1: **for** mỗi miền hoặc cụm B_i ∈ B **do**
2:     B̃_i ← mixup(B_i, B_i đã xáo trộn) với β ∼ Beta(α, α)
    ▷ Trộn các mẫu cùng chỉ số giữa B_i và B_i đã xáo trộn
3:     Tính toán rủi ro kinh nghiệm L_i trên B̃_i
4: **end for**
5: L_y ← Trung bình trên tất cả L_i

### 3.8. Học bán/có giám sát trên miền

Do tính chất xác suất của MoE, cho một đầu vào x và các giá trị cổng tương ứng p = {p_k}^K_{k=1}, chúng ta có thể diễn giải p_k như xác suất chọn chuyên gia thứ k E_k cho trước x, tức là p_k = p(E_k|x). Ngoài ra, E_k được cho là liên kết với một miền cụ thể S_m. Do đó, chúng ta có p_k = p(E_k|x) = p(S_m|x). Xem xét một tập dữ liệu với nhãn miền D_d = {(x_i, d_i)}^{N_d}_{i=1} (nhãn lớp không cần thiết) với d_i ∈ {1, ..., M_d}, chúng ta có thể sử dụng D_d như sau:

L_d = E_{(x,d)∼D_d}[ℓ_ce(p, d)]                       (10)

M_d có thể nhỏ hơn K, nhưng điều này không ảnh hưởng đến việc tính toán L_d. Trong trường hợp này, chúng tôi giả định rằng M_d chuyên gia đầu tiên được gán cho M_d miền, trong khi phần còn lại học tự động mà không có thông tin miền. Nếu tất cả nhãn miền được cung cấp, L_d chuyển sang học miền có giám sát.

### 3.9. Huấn luyện và suy luận

Mất mát huấn luyện cuối cùng là:

L = λ_y L_y + λ_en L_en + λ_kl L_kl + λ_ad L_ad + λ_d L_d  (11)

trong đó λ là các siêu tham số đánh đổi để cân bằng các mất mát khác nhau. Thông thường, λ_y được đặt bằng 1 và L_d không được sử dụng cho DG hợp chất không có nhãn miền.

Đối với suy luận, chúng tôi cung cấp hai chế độ: MIX và OOD. MIX đề cập đến hỗn hợp các chuyên gia, được tính toán bằng Phương trình (1). OOD (Out of Domain) sử dụng đầu ra của một bộ phân loại có trọng số được tạo ra bởi siêu mạng trực tiếp lấy bộ mã hóa D2V làm đầu vào. OOD cho phép tổng quát hóa các chuyên gia vượt ra ngoài việc tập hợp.

## 4. Thí nghiệm

Bài báo này tập trung vào phân loại hình ảnh. Tuy nhiên, để minh họa động lực học tập và tính linh hoạt của HMOE, chúng tôi cũng áp dụng nó cho một nhiệm vụ hồi quy đồ chơi để học một hàm một chiều được định nghĩa trên 3 khoảng. HMOE chứng minh hiệu quả trong việc gán một chuyên gia cho mỗi khoảng. Do hạn chế về không gian, chi tiết được đưa vào tài liệu bổ sung. Tiếp theo, chúng tôi đánh giá HMOE so với các thuật toán DG khác trên DomainBed [28].

### 4.1. Tập dữ liệu và đánh giá mô hình

DomainBed cung cấp một codebase thống nhất để triển khai, huấn luyện và đánh giá các thuật toán DG, và tích hợp các tập dữ liệu liên quan đến DG thường được sử dụng. Chúng tôi thí nghiệm trên Colored MNIST (3 miền và 2 lớp) [2], Rotated MNIST (6 miền và 10 lớp) [25], PACS (4 miền và 7 lớp) [44], VLCS (4 miền và 5 lớp) [19], OfficeHome (4 miền và 65 lớp) [82], và TerraIncognita (4 miền và 10 lớp) [4]. Thống kê tập dữ liệu chi tiết và trực quan hóa mẫu được cung cấp trong tài liệu bổ sung.

Đối với việc chọn mô hình và điều chỉnh siêu tham số, DomainBed cung cấp ba tùy chọn, trong đó chúng tôi chọn xác thực miền huấn luyện phân bổ 80% từ mỗi miền huấn luyện cho huấn luyện và phần còn lại cho xác thực. Tùy chọn này phù hợp tốt với DG hợp chất không có quyền truy cập vào nhãn miền và miền thử nghiệm.

### 4.2. Chi tiết triển khai

Đối với CMNIST và RMNIST, chúng tôi sử dụng ConvNet bốn lớp như bộ trích xuất đặc trưng (xem Phụ lục D.1 của [28]). Bộ mã hóa D2V h_v kết nối ConvNet bốn lớp này với một lớp fully-connected (fc) để ánh xạ đến chiều nhúng D.

Đối với các tập dữ liệu khác, chúng tôi sử dụng ResNet-50 được huấn luyện trước trên ImageNet [13] làm bộ trích xuất đặc trưng và đóng băng tất cả các lớp chuẩn hóa batch. Bộ mã hóa D2V h_v nối tiếp 3 lớp conv (64-128-256 đơn vị, stride 2, kernel 4×4, ReLU), hai khối residual (mỗi khối có 2 lớp conv với 256 đơn vị, kernel 3×3, ReLU), và một lớp conv 3×3 với D đơn vị theo sau bởi global average pooling. Chúng tôi sử dụng Instance Normalization [79] với tham số affine có thể học trước tất cả ReLU của h_v.

Đối với tất cả các tập dữ liệu, bộ phân loại f_c là một lớp fc có kích thước đầu vào là kích thước đầu ra của bộ trích xuất đặc trưng (128 cho ConvNet và 2048 cho ResNet-50) và kích thước đầu ra là số lượng lớp. Siêu mạng f_h là một MLP năm lớp với 256-128-64-32 đơn vị ẩn và SiLU [33], và kích thước đầu vào của nó là D và kích thước đầu ra là tổng số tham số có thể học (tức là trọng số và bias) của f_c. Ngoài ra, chúng tôi khởi tạo f_h bằng phương pháp hyperfan [9]. Nếu L_ad được sử dụng, bộ phân loại đối kháng là một MLP ba lớp với 256 đơn vị ẩn và ReLU, và kích thước đầu vào của nó là D và kích thước đầu ra là số lượng lớp. Chúng tôi đặt D = 32 và khởi tạo các vector nhúng với phân phối chuẩn tiêu chuẩn.

Chúng tôi định nghĩa ba biến thể HMOE, bao gồm (1) HMOE-DL: Nhãn miền được cung cấp. Chúng tôi sử dụng L_y được tính toán bằng Phương trình (7) và L_d với λ_y = λ_d = 1 và loại bỏ các mất mát khác, và K là số lượng miền huấn luyện. (2) HMOE-ND: Không có thông tin miền nào. Chúng tôi sử dụng L_y được tính toán bằng Phương trình (7), L_en, L_kl và L_ad với λ_y = λ_en = λ_kl = 1 và λ_ad = 0.1, và chúng tôi cố định K = 3. (3) HMOE-MU: Thiết lập giống như trong HMOE-ND, ngoại trừ việc L_y được tính toán thông qua mixup nội miền (Thuật toán 1) với α = 0.3.

DomainBed huấn luyện tất cả các thuật toán DG với Adam trong 5,000 lần lặp. Đối với Colored và Rotated MNIST / các tập dữ liệu khác, tốc độ học là 0.001 / 5e-5, kích thước batch là 64 / 32 × số lượng miền huấn luyện, và các mô hình được đánh giá trên tập xác thực mỗi 100 / 300 lần lặp. Mỗi thí nghiệm sử dụng một miền của tập dữ liệu làm miền thử nghiệm và huấn luyện thuật toán trên các miền khác, điều này được lặp lại ba lần với các seed ngẫu nhiên khác nhau. Độ chính xác trung bình trên ba lần lặp lại được báo cáo. Các thuật toán DG sử dụng các thiết lập mặc định được định nghĩa trước trong DomainBed. Tất cả các thí nghiệm được thực hiện bằng PyTorch trên nhiều GPU A5000.

### 4.3. Kết quả

Benchmark DomainBed trong [28] đã lỗi thời, và chúng tôi cập nhật nó bằng ResNet-50 được huấn luyện trước được cải tiến (IMAGENET1K-V2) có sẵn trên torchvision. Việc so sánh HMOE với các thuật toán DG khác được thể hiện trong Bảng 1, trong đó DeepAll có nghĩa là học có giám sát vanilla chỉ fine-tune ResNet-50 trên dữ liệu trộn và đóng vai trò như một baseline hiệu suất. Chúng tôi báo cáo độ chính xác trung bình của tất cả các miền thử nghiệm cho mỗi tập dữ liệu. Tham khảo tài liệu bổ sung để biết kết quả chi tiết.

HMOE-MU vượt trội hơn tất cả các thuật toán DG khác về độ chính xác trung bình. Đáng chú ý, các thuật toán được hỗ trợ bởi mixup cho thấy hiệu suất ấn tượng, chứng minh hiệu quả của mixup trong việc tăng cường tổng quát hóa. Cả Mixup [90] (vị trí thứ hai) và SelfReg [39] (vị trí thứ ba) đều áp dụng mixup liên miền để học các biểu diễn bất biến miền. HMOE-ND xếp thứ tư tổng thể, nhưng đứng đầu trong số các thuật toán không có mixup. Ngoài ra, HMOE-ND / MU vượt trội đáng kể so với baseline DeepAll, ngoại trừ trên RMNIST.

Đối với các tập dữ liệu MNIST, hiệu suất tương đương qua các thuật toán, ngoại trừ kết quả xuất sắc của ARM [95]. Các tập dữ liệu khác đặt ra thách thức cao hơn. Ví dụ, VLCS bao gồm hình ảnh chụp thực tế, với sự dịch chuyển miền chủ yếu do thay đổi cảnh và góc nhìn, dẫn đến sự khác biệt thị giác tinh tế giữa các miền. Nhiều thuật toán kém hơn DeepAll trên các tập dữ liệu thách thức này. HMOE-MU đạt được kết quả tốt nhất trên PACS, OfficeHome, và TerraInc, và hiệu suất của nó trên VLCS gần như ngang bằng với kết quả tốt nhất (78.6 vs. 78.9). HMOE-ND cũng hoạt động ấn tượng. Tất cả những phát hiện này xác nhận tính ưu việt của HMOE trong việc giải quyết DG hợp chất.

HMOE-MU vượt trội rõ rệt so với ND. Hình 3a trình bày một so sánh về độ chính xác xác thực / thử nghiệm của chúng trong quá trình huấn luyện. Rõ ràng là độ chính xác của MU tiếp tục cải thiện với việc giới thiệu mixup nội miền khi L_en < 0.1, vì mixup áp đặt các ràng buộc tuyến tính, thúc đẩy tính mượt mà và giảm thiểu overfitting.

Thật thú vị, HMOE-DL thua kém HMOE-ND / MU đáng kể, cho thấy rằng HMOE hoạt động tốt hơn khi sử dụng thông tin miền tự học thay vì dựa vào nhãn miền được cung cấp. Chúng tôi quan sát thấy rằng các miền tiềm ẩn được khám phá bởi HMOE có vẻ trực quan hơn đối với con người so với nhãn miền được cung cấp (Mục 4.4). Hình 3b cho thấy rằng mất mát có giám sát trên miền L_d của HMOE-DL không thể giảm nhanh chóng trên các tập dữ liệu OfficeHome và VLCS. Điều này có thể gợi ý rằng HMOE gặp khó khăn trong việc tiếp thu thông tin nhãn miền, làm phức tạp quá trình học của nó và ảnh hưởng tiêu cực đến hiệu suất DG.

Đối với hai chế độ suy luận, MIX vượt trội hơn OOD trong hầu hết các trường hợp, nhưng OOD có thể được sử dụng để hy sinh một chút độ chính xác cho hiệu quả trong thực tế vì nó hiệu quả hơn về mặt tính toán mà không cần tính toán tất cả các chuyên gia như MIX.

### 4.4. Khám phá miền tiềm ẩn

Chúng tôi sử dụng t-SNE [80] để trực quan hóa đầu ra của bộ mã hóa D2V, như được thể hiện trong Hình 4. Rõ ràng là HMOE-ND phân tách hiệu quả dữ liệu trộn thành các cụm riêng biệt, mỗi cụm hút về một vector nhúng.

Nhãn miền được sử dụng để tô màu dữ liệu để làm nổi bật sự khác biệt giữa chúng và các miền tiềm ẩn được suy luận. Đối với PACS với miền thử nghiệm art (Hình 4a), các miền được suy luận phần lớn phù hợp với nhãn miền, mặc dù một số ảnh được nhóm vào cụm chủ yếu cartoon. Tuy nhiên, với cartoon làm miền thử nghiệm (Hình 4b), dữ liệu không được chia dựa trên art và photo. Hình 4e cho thấy rằng, ngay cả với nhãn miền, HMOE-DL gặp khó khăn trong việc phân tách hoàn toàn art khỏi photo. Đối với TerraInc (Hình 4c), các điểm cùng màu có xu hướng phân cụm với nhau, trong khi đối với OfficeHome (Hình 4d), các màu khác nhau trộn lẫn trong mỗi cụm, làm nổi bật khoảng cách lớn giữa miền được gán nhãn và được suy luận. Hình 4f cũng cho thấy rằng HMOE-DL gặp khó khăn trong việc phân chia dữ liệu, giải thích sự giảm chậm trong L_d cho OfficeHome trong Hình 3b.

Để hiểu trực quan cách HMOE phân biệt giữa các miền, Hình 5 so sánh các miền được gán nhãn và được suy luận bằng các mẫu thị giác. HMOE-ND có vẻ phân chia TerraInc theo độ chiếu sáng và OfficeHome theo độ phức tạp nền, điều này phù hợp hơn với trực giác con người.

Sau phân tích trên, chúng tôi kết luận rằng thành công của HMOE xuất phát từ khả năng tự học kiến thức miền hợp lý hơn. Tuy nhiên, điều này không có nghĩa là nhãn miền được cung cấp là sai. Thông thường có nhiều yếu tố tạo sinh đằng sau quá trình tạo dữ liệu [5], làm cho định nghĩa miền trở nên đa diện. HMOE chỉ đơn giản khám phá một cách phân chia dữ liệu trực quan và dễ hiểu để tăng cường hiệu suất DG của nó.

### 4.5. Nghiên cứu loại bỏ

Vai trò của mixup nội miền đã được xác nhận trước đây. Trong mục này, chúng tôi phân tích đóng góp của các thành phần khác của HMOE thông qua các nghiên cứu loại bỏ, như được thể hiện trong Bảng 2. Chúng tôi sử dụng hệ số silhouette (SC) để đánh giá định lượng việc phân cụm của HMOE về mặt độ compactness và phân tách cụm. SC dao động từ -1 (kém) đến 1 (tốt). Các cụm được xác định bởi giá trị cổng và khoảng cách của chúng được đo bằng đầu ra của bộ mã hóa D2V.

**Định tuyến Top-1 L_en và cân bằng tải chuyên gia L_kl**
Việc sử dụng kết hợp L_en và L_kl dẫn đến phân cụm tốt hơn với SC lớn hơn và thúc đẩy khám phá miền tiềm ẩn. Không có chúng, HMOE dựa vào việc phân chia mềm vốn có của MoE. H6 vượt trội hơn H2 trong hầu hết các trường hợp, điều này có thể cho thấy rằng phân cụm tốt hơn có lợi cho hiệu suất DG. Tuy nhiên, H1 và H5 hoạt động tương tự, có lẽ do sự vắng mặt của L_ad. Chúng tôi thấy rằng L_en không có L_kl gặp phải vấn đề sụp đổ học tập, tức là một số vector nhúng sụp đổ với nhau, dẫn đến sụt giảm độ chính xác. Một ví dụ được thể hiện trong Hình 6c. Điều này chứng minh tầm quan trọng của L_kl.

**Huấn luyện đối kháng lớp L_ad** tăng độ chính xác trong hầu hết các trường hợp, xác minh sự cần thiết của việc lọc thông tin đặc thù cho lớp khỏi bộ mã hóa D2V. H2 và H6 có SC nhỏ hơn H1 và H5 tương ứng, điều này hợp lý vì thông tin lớp vẫn có thể được H1 và H5 sử dụng để phân cụm, nhưng bị giảm bớt phần nào đối với H2 và H6 thông qua L_ad.

### 4.6. Phân tích kinh nghiệm khác

**Ảnh hưởng của K đến việc khám phá miền tiềm ẩn** Trong Hình 6, chúng tôi thử các số lượng vector nhúng K khác nhau. Đối với K = 2, cartoon được hợp nhất vào sketch và photo. Đối với K = 5, sketch và cartoon được chia thành hai cụm con. Tuy nhiên, khi K tăng lên 8 và nhiều hơn mức cần thiết, HMOE gặp khó khăn trong việc gán dữ liệu cho các chuyên gia khác nhau một cách chính xác và gặp phải vấn đề sụp đổ học tập.

**Sử dụng Swin Transformer làm bộ trích xuất đặc trưng** [43] đã nghiên cứu tác động của mạng backbone (tức là bộ trích xuất đặc trưng cho HMOE) đến DG và thấy rằng các backbone dựa trên transformer vượt trội hơn các đối tác dựa trên CNN. Được thúc đẩy bởi điều này, chúng tôi thử Swin Transformer [52] (phiên bản tiny được huấn luyện trước với độ phức tạp tương tự như ResNet-50 và kích thước đầu ra là 768) làm bộ trích xuất đặc trưng (Bảng 3), điều này tăng cường cả DeepAll và HMOE-MU, nhưng cái sau vẫn hoạt động tốt hơn nhiều.

## 5. Kết luận

Bài báo này trình bày một phương pháp DG mới - HMOE, dựa trên Mixture of Experts, sử dụng siêu mạng để tạo ra trọng số của các chuyên gia, không yêu cầu nhãn miền, và cho phép khám phá miền tiềm ẩn. HMOE đạt được hiệu suất SOTA về độ chính xác trung bình trên DomainBed.

Tuy nhiên, vẫn chưa rõ làm thế nào để xác định hiệu quả số lượng chuyên gia hoặc vector nhúng phù hợp để khám phá đầy đủ thông tin miền trong khi tránh sụp đổ học tập. Một giải pháp đầy hứa hẹn mà chúng tôi sẽ khám phá trong công trình tương lai là sử dụng MoE phân cấp có cấu trúc cây để khám phá kiến thức miền phân cấp, trong đó mỗi cấp chỉ chứa một số chuyên gia nhưng số lượng miền được suy luận đa cấp tăng theo cấp số nhân.

Cuối cùng, HMOE linh hoạt và có thể mở rộng, và nó cũng có thể áp dụng cho một loạt các vấn đề rộng rãi ngoài phạm vi của DG bị ảnh hưởng bởi các mẫu không đồng nhất.
