# 2103.13262.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2103.13262.pdf
# Kích thước tệp: 755611 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bản thảo
FASTMOE: HỆ THỐNG HUẤN LUYỆN MIXTURE-OF-EXPERT NHANH

Jiaao Heyz, Jiezhong Qiuyz, Aohan Zengyz, Zhilin Yangz], Jidong Zhaiyz, Jie Tangyz
yĐại học Thanh HoazViện Hàn lâm Trí tuệ Nhân tạo Bắc Kinh (BAAI)]Recurrent AI
fhja20,qiujz16,zah18 g@mails.tsinghua.edu.cn ;
kimi yang@rcrai.com ;fzhaijidong, jietang g@tsinghua.edu.cn

TÓM TẮT
Mixture-of-Expert (MoE) thể hiện tiềm năng mạnh mẽ trong việc mở rộng kích thước của mô hình ngôn ngữ lên hàng nghìn tỷ tham số. Tuy nhiên, việc huấn luyện MoE quy mô nghìn tỷ đòi hỏi đồng thiết kế thuật toán và hệ thống cho một hệ thống huấn luyện phân tán hiệu suất cao được điều chỉnh tốt. Thật không may, nền tảng duy nhất hiện có đáp ứng các yêu cầu phụ thuộc mạnh vào ngăn xếp phần cứng (TPU) và phần mềm (Mesh Tensorflow) của Google, và không mở và có sẵn cho công chúng, đặc biệt là cộng đồng GPU và PyTorch.

Trong bài báo này, chúng tôi trình bày FastMoE, một hệ thống huấn luyện MoE phân tán dựa trên PyTorch với các bộ gia tốc thông thường. Hệ thống cung cấp giao diện phân cấp cho cả thiết kế mô hình linh hoạt và thích ứng dễ dàng với các ứng dụng khác nhau, chẳng hạn như Transformer-XL và Megatron-LM. Khác với việc triển khai trực tiếp các mô hình MoE sử dụng PyTorch, tốc độ huấn luyện được tối ưu hóa cao trong FastMoE bằng các kỹ năng gia tốc hiệu suất cao tinh vi. Hệ thống hỗ trợ đặt các chuyên gia khác nhau trên nhiều GPU qua nhiều nút, cho phép mở rộng số lượng chuyên gia tuyến tính so với số lượng GPU. Mã nguồn của FastMoE có sẵn tại https://github.com/laekov/fastmoe dưới giấy phép Apache-2.

1 GIỚI THIỆU
Sự xuất hiện gần đây của các mô hình ngôn ngữ quy mô lớn, được minh họa bởi BERT (Devlin et al., 2018), GPT-2/-3 (Radford et al., 2019; Brown et al., 2020), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020), GShard (Chen et al., 2020) và Switch Transformer (Fedus et al., 2021), đã thay đổi drastically cảnh quan nghiên cứu xử lý ngôn ngữ tự nhiên, thiết lập lại các đường cơ sở tiên tiến mới trong các benchmark khác nhau như GLUE (Wang et al., 2018) và SuperGLUE (Wang et al., 2019).

Trong số nhiều giải pháp có thể, việc mở rộng kích thước mô hình đã được chứng minh là một trong những cách đơn giản và hiệu quả nhất (Kaplan et al., 2020) hướng tới các mô hình mạnh mẽ hơn. Từ BERT (Devlin et al., 2018) với 340 triệu tham số, đến T5 (Raffel et al., 2020) với 11 tỷ tham số, đến GPT-3 (Brown et al., 2020) với 175 tỷ tham số, kích thước mô hình được mở rộng 500 lần chỉ trong hai năm. Gần đây hơn, GShard (Chen et al., 2020) mở rộng đến số kỷ lục 600 tỷ tham số, nhanh chóng bị vượt qua bởi Switch Transformer (Fedus et al., 2021) với 1,6 nghìn tỷ tham số. Yếu tố chính góp phần vào kích thước mô hình khổng lồ của GShard và Switch Transformer là một kiến trúc mạng nơ-ron mới có tên mixture of experts (MoE) (Shazeer et al., 2017).

Một lớp MoE (có thể tìm thấy ví dụ minh họa trong Hình 1) bao gồm một cổng và một nhóm các chuyên gia. Đối với mỗi đầu vào, chỉ một thiểu số nhỏ các chuyên gia được cổng chọn để tính toán. Kiến trúc đặc biệt của MoE là một con dao hai lưỡi cho huấn luyện phân tán quy mô lớn. Một mặt, do kích hoạt thưa thớt của các chuyên gia, MoE có thể mở rộng kích thước mô hình theo bậc độ lớn mà không tăng đáng kể lượng tính toán (FLOP). Mặt khác, khi mở rộng lên hàng nghìn chuyên gia, mẫu giao tiếp all-to-all không cân bằng của MoE mang lại những thách thức mới cho việc đồng thiết kế thuật toán và hệ thống. Do đó, MoE không thể được hỗ trợ trực tiếp bởi các thư viện học sâu truyền thống như PyTorch (Paszke et al., 2019) và TensorFlow (Abadi et al., 2016).

1arXiv:2103.13262v1 [cs.LG] 24 Mar 2021

--- TRANG 2 ---
Bản thảo
Đầu vàoMạng cổng Điểm số
Bộ chọn chuyên gia Bộ tổng hợp Đầu raMạng chuyên gia 1
Mạng chuyên gia 2
Mạng chuyên gia 3
Mạng chuyên gia n

Hình 1: Ví dụ minh họa về một lớp MoE. Trong ví dụ này, chuyên gia 1 và chuyên gia 3 được cổng chọn để tính toán.

Do thách thức được đặt ra bởi kiến trúc mô hình mới, cả cộng đồng nghiên cứu và ngành công nghiệp đều cần một triển khai MoE hỗ trợ huấn luyện phân tán quy mô lớn. Tuy nhiên, mặc dù tồn tại một số triển khai GPU đơn naïve trong PyTorch (Rau, 2019), hệ thống duy nhất hiện tại hỗ trợ huấn luyện MoE có thể mở rộng dựa trên ngăn xếp phần cứng và phần mềm riêng tư của Google — TPU (Jouppi et al., 2017) và Mesh TensorFlow (Shazeer et al., 2018). Do đó, có nhu cầu cấp thiết phát triển một hệ thống MoE trên phần cứng có sẵn công khai (ví dụ: GPU) và các nền tảng (ví dụ: PyTorch (Paszke et al., 2019)).

Được thúc đẩy bởi mong muốn có được giải pháp dễ sử dụng, linh hoạt, hiệu quả, có thể mở rộng và mã nguồn mở cho huấn luyện MoE quy mô lớn, chúng tôi phát hành FastMoE với các mục tiêu thiết kế sau:

•Dễ sử dụng: cung cấp giao diện thân thiện với người dùng để định nghĩa một lớp MoE, và hỗ trợ liền mạch cho hệ thống huấn luyện mô hình ngôn ngữ phổ biến, Megatron-LM (Shoeybi et al., 2019).

•Linh hoạt: giúp người dùng dễ dàng tùy chỉnh mạng cổng và mạng chuyên gia.

•Hiệu quả: tích hợp lớp feedforward (FFN) được tối ưu hóa cao cho Transformer.

•Có thể mở rộng: hỗ trợ mở rộng kích thước mô hình MoE bằng cách huấn luyện trên nhiều GPU trên nhiều nút.

Khác với triển khai PyTorch GPU đơn trước đây (Rau, 2019), FastMoE tập trung vào hiệu quả và khả năng mở rộng. Các kernel CUDA chuyên dụng được bao gồm trong FastMoE để có hiệu suất cao với các tối ưu hóa chuyên biệt. FastMoE có thể chạy trên nhiều GPU trên nhiều nút sử dụng NCCL (Jeaugey, 2017). Chi tiết của giao tiếp được ẩn khỏi các nhà phát triển mô hình bởi FastMoE. Phương pháp song song mô hình của FastMoE cho phép phân phối các chuyên gia trên các GPU khác nhau, trong khi các phần khác của mô hình vẫn được song song hóa theo chiều batch (song song dữ liệu) hoặc chiều tensor (song song mô hình). Có khả năng kích thước mô hình, tỷ lệ thuận với số lượng chuyên gia, có thể mở rộng theo số lượng GPU được sử dụng để huấn luyện, là chìa khóa để huấn luyện các mô hình quy mô nghìn tỷ.

Trong thí nghiệm của chúng tôi, chúng tôi quan sát thấy rằng FastMoE nhanh hơn so với baseline (Rau, 2019) được triển khai bằng API PyTorch thuần túy trên một GPU đơn. FastMoE cũng cho thấy khả năng mở rộng hợp lý khi chạy trên các nút trên một cụm được kết nối bởi mạng Infiniband. Chúng tôi huấn luyện một mô hình GPT thực với 96 chuyên gia mỗi lớp sử dụng FastMoE phân tán với tốc độ huấn luyện end-to-end đầy hứa hẹn. So với mô hình không MoE có cùng lượng tính toán, hiệu suất của nó được hưởng lợi từ kích thước mô hình được mở rộng mà kiến trúc MoE mang lại.

Bài báo này được tổ chức như sau. Phần 2 giới thiệu nền tảng của MoE và so sánh các hệ thống hiện có. Phần 3 trình bày hệ thống FastMoE một cách chi tiết. Phần 4 giới thiệu các thách thức trong việc đạt được hiệu suất cao và các giải pháp của FastMoE. Phần 5 hiển thị kết quả của các thí nghiệm chứng minh hiệu quả của FastMoE và lợi ích hiệu suất của mô hình MoE sử dụng FastMoE trong huấn luyện. Phần 6 tóm tắt bài báo và chỉ ra hướng công việc tương lai của chúng tôi.

2 MIXTURE-OF-EXPERTS (MOE)

Trong phần này, chúng tôi xem xét kiến trúc của MoE và các hệ thống hiện tại để huấn luyện MoE.

2.1 MOE: CẤU TRÚC MÔ HÌNH

Mixture-of-Expert là viết tắt của các lớp Sparsely-Gated Mixture-of-Experts được đề xuất bởi Shazeer et al. (2017). Một lớp MoE bao gồm nhiều chuyên gia, mỗi chuyên gia có thể là một mạng nơ-ron tùy ý. Ràng buộc duy nhất của các chuyên gia là chúng phải nhận cùng đầu vào và cho đầu ra trong cùng không gian vector. Hình 1 cho thấy một ví dụ chi tiết về một lớp MoE. Một mạng nơ-ron đặc biệt, cụ thể là mạng cổng, được giới thiệu để chấm điểm mỗi chuyên gia đối với một đầu vào cho trước. Theo điểm số, việc lựa chọn các chuyên gia được thực hiện bởi một chính sách, có thể thay đổi từ mô hình này sang mô hình khác. Sau đó, các chuyên gia được chọn, ví dụ chuyên gia 1 và 3 trong ví dụ, được kích hoạt để xử lý mẫu đầu vào. Đầu ra của các chuyên gia, cùng với điểm số, được kết hợp thành đầu ra cuối cùng sử dụng một thuật toán nhất định.

Một cách phổ biến để lựa chọn chuyên gia là chọn các chuyên gia có điểm số k cao nhất. Trong quá trình tổng hợp, điểm số được sử dụng như trọng số cho đầu ra của các chuyên gia để được cộng vào đầu ra tổng thể. Điều này cho phép huấn luyện mạng cổng, vì gradient có thể được lan truyền bởi điểm số. Thuật toán 1 chính thức hóa phương pháp trên.

Thuật toán 1 Tính toán tiến của một lớp MoE với top-k gating.
Yêu cầu: Một nhóm n chuyên gia: fE1; E2;; Eng
Yêu cầu: Cổng G
Yêu cầu: Số lượng chuyên gia k được chọn
1:function MOE(x)
2: score(G(x)
3: indices(ArgMaxk(score)
4: y(zero tensor like x
5: foreach index i2indices do
6: xi(Ei(x)
7: y(scoreixi+y
8: end for
9: return y
10:end function

2.2 CÁC HỆ THỐNG HIỆN TẠI CHO HUẤN LUYỆN MOE

Hệ thống GShard (Chen et al., 2020) triển khai một phiên bản phân tán của mô hình MoE. Nó huấn luyện một mô hình ngôn ngữ trên tối đa 2048 TPU, với 1 chuyên gia mỗi lớp được đặt trên mỗi TPU. Kết quả là, các lớp MoE chứa 2048 tham số nhiều hơn so với một lớp không MoE. Trong Switch Transformer (Fedus et al., 2021), mô hình được mở rộng thêm lên 1,6 nghìn tỷ, cho thấy khả năng mạnh mẽ của hệ thống trong việc hỗ trợ huấn luyện mô hình quy mô lớn. Thật không may, hệ thống này chưa có sẵn công khai. Nó bị ràng buộc mạnh mẽ với cụm TPU, điều này khiến việc tái tạo các thí nghiệm trên các thiết bị thông thường trở nên khó khăn. Ngoài ra, thiết kế của GShard thiếu linh hoạt để sử dụng số lượng và kích thước khác nhau của các chuyên gia với chiến lược sao chép khác nhau.

Trong Tensor2tensor (Vaswani et al., 2018), một mô hình MoE Transformer được cung cấp. Tuy nhiên, triển khai này sử dụng Mesh TensorFlow (Shazeer et al., 2018), không hỗ trợ GPU rất tốt. Để triển khai một FFN trong Transformer, cần hơn 100 dòng code trong TensorFlow với các toán tử einsum phức tạp, khiến việc hiểu cấu trúc và khám phá các cấu trúc mô hình khác dựa trên code trở nên gánh nặng cho các nhà phát triển.

PyTorch (Paszke et al., 2019), như một framework học sâu phổ biến trong số các nhà nghiên cứu, cung cấp phong cách mã hóa trực tiếp và linh hoạt hơn so với TensorFlow. Có những nỗ lực được thực hiện để huấn luyện các mô hình MoE với PyTorch (Rau, 2019). Tuy nhiên, vì cộng đồng PyTorch thiếu các công cụ huấn luyện song song đa chiều, không có triển khai nào dựa trên PyTorch hỗ trợ huấn luyện trên nhiều GPU. Vì mục tiêu cuối cùng của việc áp dụng MoE là huấn luyện các mô hình thậm chí còn lớn hơn, các triển khai dựa trên PyTorch không thể trở thành một ứng viên.

3 FastMoE: THIẾT KẾ HỆ THỐNG

Trong phần này, chúng tôi giới thiệu thiết kế FastMoE với hỗ trợ huấn luyện phân tán.

--- TRANG 3 ---
Bản thảo

3.1 MỘT HỆ THỐNG LINH HOẠT CHO CÁC NHÀ KHÁM PHÁ MÔ HÌNH ĐA DẠNG

Backbone để Chạy Các Mạng Chuyên Gia Tùy Ý. FastMoE hỗ trợ sử dụng mạng tùy ý làm chuyên gia. Giao diện FMoE của FastMoE nhận bất kỳ constructor module mạng nơ-ron nào làm đầu vào, và sao chép module nhiều lần như các instance chuyên gia. Chuyên gia được định nghĩa để nhận một batch các đặc trưng đầu vào liền kề được căn chỉnh, và đầu ra phải theo cùng thứ tự batch. Do đó, việc triển khai module chuyên gia được tách rời khỏi kiến trúc MoE để các nhà phát triển có thể tập trung vào thiết kế mạng chuyên gia của riêng họ.

Để có tính linh hoạt thậm chí mạnh hơn, lớp FMoE chứa một hàm thành viên expert_fn, nơi các module chuyên gia được sử dụng để tiến hành tính toán tiến. Hàm này có thể được overload để có hành vi MoE tùy chỉnh thêm. Ví dụ, trong mạng FMoETransformerMLP, sẽ được đề cập sau trong phần này, danh sách các chuyên gia được thay thế bằng một module được tối ưu hóa đặc biệt áp dụng các chuyên gia song song để giảm thiểu độ trễ cực kỳ.

Hơn nữa, FastMoE hỗ trợ đặt nhiều chuyên gia cùng nhau trên cùng một worker, cho phép không gian cấu hình linh hoạt hơn về số lượng chuyên gia (tức là, số lượng chuyên gia không phải bằng số lượng data parallel), khác với thiết kế của GShard.

Một FFN Được Tối Ưu Hóa Cao cho Transformer. Để hỗ trợ tốt hơn việc huấn luyện Transformer với MoE, FastMoE cung cấp một triển khai FFN tiêu chuẩn và hiệu suất cao (FMoETransformerMLP). Chiến lược tối ưu hóa chi tiết được ẩn khỏi các nhà phát triển. Đặc biệt, khi đặt nhiều chuyên gia trên cùng một worker, một triển khai naïve là lặp qua các chuyên gia này và tiến hành forward tuần tự. Tuy nhiên, đối với một số loại mạng chuyên gia nhất định, có thể khám phá tiềm năng tăng tốc mang lại bởi thực thi song song. Trong FastMoE, chúng tôi chủ yếu tối ưu hóa việc thực thi song song của các lớp fully-connected bằng một module FMoELinear chuyên dụng. Thay vì tính toán các module chuyên gia tuần tự, module chuyên gia được tối ưu hóa đặc biệt duy trì một nhóm tài nguyên phần cứng có sẵn, và áp dụng tính toán chuyên gia song song.

Hỗ Trợ Kiểu Plugin cho PyTorch và Megatron-LM. Tính linh hoạt của FastMoE cho phép thích ứng thuận tiện với các ứng dụng huấn luyện hiện có. Lấy Megatron-LM (Shoeybi et al., 2019) làm ví dụ, một module kiểu plugin được tích hợp trong FastMoE để nhanh chóng thay thế các FFN trong mô hình Megatron-LM gốc bằng các mạng MoE. Như được hiển thị trong listing 1, việc chuyển đổi có thể được thực hiện chỉ bằng 2 dòng code.

Listing 1: Code mẫu để sử dụng FastMoE trong Megatron-LM
from fmoe.megatron import fmoefy
model = fmoefy(model, num_experts=<số lượng chuyên gia mỗi worker>)

Hàm fmoefy có thể tìm các FFN trong các lớp Transformer. Sau đó, một mạng MoE sử dụng FastMoE được tạo ra, đây là một module bao bọc module FMoETransformerMLP để tương thích ở mức giao diện.

3.2 MỞ RỘNG DUNG LƯỢNG MÔ HÌNH THEO CÁCH PHÂN TÁN

Phương Pháp Song Song Mô Hình của FastMoE. Như một trong những cách hiệu quả nhất để mở rộng dung lượng mô hình, khả năng chứa một quần thể chuyên gia lớn và huấn luyện chúng song song được yêu cầu trong nhiều mô hình MoE. Việc xử lý việc truyền dữ liệu phức tạp giữa các GPU và thậm chí qua các nút là khó khăn đối với các nhà phát triển mô hình. Đạt được hiệu suất huấn luyện cao và sử dụng tài nguyên phần cứng tốt đòi hỏi chuyên môn về kiến trúc máy tính và lập trình song song, vượt quá ngăn xếp kỹ thuật của các nhà phát triển mô hình thông thường.

FastMoE hỗ trợ phân phối các chuyên gia trên nhiều worker trên nhiều nút, được gọi là phương pháp song song mô hình trong FastMoE. Chi tiết của việc trao đổi dữ liệu đầu vào được ẩn trong giao diện FMoE. Đối với các nhà phát triển mô hình, họ chỉ cần viết code cho một chuyên gia đơn, và mỗi chuyên gia được cung cấp tất cả dữ liệu đầu vào được thu thập từ tất cả các worker bởi FastMoE. Kết quả là, các nhà phát triển mô hình không phải xem xét chi tiết triển khai về giao tiếp cross-worker.

--- TRANG 4 ---
Bản thảo

Trong thiết kế của FastMoE, khi tính năng phân phối chuyên gia trên các worker được kích hoạt, các hoạt động giao tiếp bổ sung được bao gồm trong tính toán tiến và lùi. Để nhận diện tốt hơn các hoạt động, chúng tôi gọi chúng là các hoạt động trao đổi dữ liệu toàn cục, trái ngược với quá trình xáo trộn dữ liệu cục bộ, sẽ được đề cập trong phần 4.

Một thách thức lớn trong bối cảnh phân tán là tổng số mẫu đầu vào được gán cho tất cả các chuyên gia trên một worker có thể thay đổi rất nhiều. Không thể có số lượng mẫu đến trước khi đầu ra cổng có sẵn. Tuy nhiên, việc phân bổ buffer để đặt các mẫu đầu vào phụ thuộc vào số lượng. Do đó, trước khi trao đổi thực tế các mẫu đầu vào giữa các worker xảy ra sau khi trao đổi thông tin lượng giữa các worker, và phân bổ bộ nhớ theo việc kiểm tra thông tin đếm chuyên gia.

Worker 1mẫu của chuyên gia 0
mẫu của chuyên gia 1
mẫu của chuyên gia 2
mẫu của chuyên gia 0
mẫu của chuyên gia 1
mẫu của chuyên gia 2
mẫu của chuyên gia 0
mẫu của chuyên gia 1
mẫu của chuyên gia 2Worker 0
Worker 2Worker 1# mẫu của chuyên gia 0
# mẫu của chuyên gia 1
# mẫu của chuyên gia 2# mẫu của chuyên gia 0
# mẫu của chuyên gia 1
# mẫu của chuyên gia 2# mẫu của chuyên gia 0
# mẫu của chuyên gia 1
# mẫu của chuyên gia 2Worker 0
Worker 2Worker 1đầu vào của chuyên gia 0
đầu vào của chuyên gia 1
đầu vào của chuyên gia 2đầu vào của chuyên gia 0
đầu vào của chuyên gia 1
đầu vào của chuyên gia 2đầu vào của chuyên gia 0
đầu vào của chuyên gia 1
đầu vào của chuyên gia 2Worker 0
Worker 2Trao đổi
kích thước
Phân bổ
buffer
Phân bổ
buffer
Phân bổ
bufferTrao đổi
dữ liệu

Hình 2: Một ví dụ về các hoạt động toàn cục.

Một ví dụ về các hoạt động toàn cục trong FastMoE được hiển thị trong hình 2. Các worker đầu tiên đếm số lượng mẫu được gán cho mỗi chuyên gia trên mỗi worker. Sau đó, chúng trao đổi kích thước của đầu vào chuyên gia, để tất cả các worker có được số lượng mẫu đầu vào đến, và chúng đến từ đâu. Sau khi các offset của mỗi buffer nhận được tính toán, các worker bắt đầu trao đổi dữ liệu trực tiếp. Đáng chú ý là thống kê của các mẫu đến và đi có thể được tái sử dụng qua toàn bộ quá trình của một iteration huấn luyện.

Module Đồng Bộ Hóa Nhận Biết Tính Không Đồng Nhất. Tính không đồng nhất được giới thiệu vì các phần khác nhau của mạng có thể được sao chép trên các nhóm worker khác nhau. Đó là một thách thức mà module phân tán phải xác định liệu gradient của một tham số có nên được đồng bộ hóa không, và với ai nó được đồng bộ hóa. FastMoE giới thiệu tag nhóm giao tiếp song song dữ liệu trên mỗi tham số để giải quyết vấn đề.

Tag có thể là một trong world, data parallel hoặc none, tương ứng chỉ ra rằng gradient nên được đồng bộ hóa với (1) tất cả các worker khác, (2) các worker trong một nhóm data-parallel trực giao với nhóm model-parallel, hoặc (3) không có worker nào. Ví dụ, mạng cổng được sao chép trên tất cả các worker, bất kể cài đặt model parallel. Lớp attention có thể được chia thành các sub-layer model-parallel, nên tag của nó là data parallel. Mỗi worker phục vụ một số mạng chuyên gia duy nhất, có tag là none. Một module data parallel tùy chỉnh thay vì module distributed data parallel gốc của PyTorch được cung cấp bởi FastMoE, có thể nhận diện các tag và thực hiện đồng bộ hóa chính xác.

4 TỐI ƯU HÓA ĐỂ ĐẠT HIỆU SUẤT CAO

Hiệu suất của tính toán MoE trên một nút đơn là đáng kể, vì nó xác định giới hạn trên lý thuyết của hệ thống mở rộng lên bất kỳ quy mô nào.

--- TRANG 5 ---
Bản thảo

Cách trực quan nhất để tính toán một lớp MoE là cắt batch đầu vào thành các mẫu, và tính toán từng mẫu một. Sau đó, các đặc trưng đầu ra được xếp chồng theo thứ tự gốc. Tuy nhiên, được quan sát thấy rằng việc triển khai một mô hình MoE sử dụng các toán tử PyTorch đơn giản khó có thể đạt được hiệu suất cao. Ít hơn 5% hiệu suất đỉnh của GPU có thể đạt được.

1 2 4 816 64 256 1024 4096 16384 65536
kích thước batch0246810121416TFLOPs

Hình 3: Hiệu suất GeMM của các kích thước bài toán khác nhau sử dụng cuBLAS trên NVIDIA V100.

Không mất tính tổng quát, chúng tôi giả định mạng chuyên gia là một FFN. Lưu ý rằng toán tử chính trong một FFN đến từ các lớp fully-connected, bao gồm một số toán tử GeMM. Khi batch được chia thành các mẫu đơn, hoạt động GeMM bị suy giảm thành GeMV. Hình 3 hiển thị throughput tính toán điểm nổi của một lớp fully-connected ví dụ sử dụng kích thước batch khác nhau. Cho rằng trong các thiết bị tính toán không đồng nhất hiện đại, các toán tử nhân ma trận được tinh chỉnh với các kỹ thuật tiling tinh vi được áp dụng trên tất cả các chiều, không có gì đáng ngạc nhiên khi throughput chỉ có thể tiếp cận đỉnh lý thuyết khi kích thước batch đủ lớn. Điều này dẫn đến nguyên tắc rằng để đạt được hiệu suất cao trong tính toán MoE, các mẫu nên được gom batch để sử dụng đầy đủ tài nguyên phần cứng.

FastMoE gom batch tất cả các mẫu đầu vào đến cùng một chuyên gia lại với nhau. Do giới hạn của biểu diễn dữ liệu, FastMoE thực hiện di chuyển bộ nhớ với một kernel CUDA được phát triển đặc biệt để giảm overhead. Cho trước chỉ số của cổng mà mỗi mẫu sẽ đi tới, quá trình đặt tất cả các mẫu đầu vào đến cùng một cổng trong một không gian bộ nhớ liền kề được gọi là scatter. Tuy nhiên, trong các phần khác của mạng nơ-ron, batch có thể phải được tổ chức theo thứ tự gốc của nó, ví dụ lớp attention trong Transformer. Một hoạt động ngược được thực hiện sau khi các chuyên gia xuất ra một không gian bộ nhớ liền kề khác, tức là đặt các vector đặc trưng được scatter trở lại thứ tự gốc của chúng theo các chỉ số cổng. Quá trình này được ký hiệu là gather trong FastMoE.

đầu vào 0
đầu vào 1
đầu vào 2
đầu vào 3
đầu vào 4
đầu vào 5đầu ra cổng:
0, 1, 2, 1, 1, 2
đầu vào 0
đầu vào 1
đầu vào 2đầu vào 3
đầu vào 4đầu vào 5
Scatterchuyên gia 1chuyên gia 0
chuyên gia 2đầu ra 0
đầu ra 1
đầu ra 2đầu ra 3
đầu ra 4đầu ra 5đầu ra 0
đầu ra 1
đầu ra 2
đầu ra 3
đầu ra 4
đầu ra 5Gather

Hình 4: Một ví dụ về tính toán được sắp xếp lại của một lớp MoE

Quá trình tính toán sắp xếp lại được hiển thị như Hình 4. Khi việc gán từ các mẫu đầu vào đến các chuyên gia cân bằng đủ, mỗi chuyên gia được kỳ vọng có một kích thước batch đầu vào tương đối lớn có thể đạt được sử dụng phần cứng thỏa mãn theo Hình 3. Tuy nhiên, mất cân bằng tải luôn xảy ra do bản chất của việc lấy mẫu ngẫu nhiên của dữ liệu huấn luyện đầu vào. Rất có thể một chuyên gia nhận được rất ít mẫu đầu vào trong hàng triệu iteration huấn luyện. Ngoài ra, vì nhiều chuyên gia được đặt trên một worker, kích thước batch cục bộ của các chuyên gia, trung bình, thống kê thấp hơn so với trong data parallel. FastMoE sử dụng một stream manager tùy chỉnh để đồng thời thực thi tính toán của nhiều chuyên gia để trích xuất lợi ích throughput tiềm năng.

5 ĐÁNH GIÁ

Trong phần này, tốc độ huấn luyện của FastMoE được so sánh với một triển khai MoE PyTorch khác (Rau, 2019) trên một GPU đơn. Chúng tôi cũng báo cáo khả năng mở rộng của FastMoE khi huấn luyện phân tán. Theo hiểu biết tốt nhất của chúng tôi, FastMoE là hệ thống MoE dựa trên PyTorch duy nhất có thể chạy trên các nút và GPU khác nhau. Chúng tôi cũng hiển thị hiệu suất end-to-end của một mô hình MoE Transformer được huấn luyện sử dụng FastMoE.

5.1 THIẾT LẬP THÍ NGHIỆM

Chúng tôi sử dụng các ký hiệu sau để đặc trưng cho tác vụ tính toán: ne chuyên gia được đặt trên mỗi GPU. Mỗi chuyên gia áp dụng hai lớp tuyến tính có kích thước dm×dh và dh×dm tương ứng. Đầu vào chứa nb mẫu. Module cổng chấm điểm độ phù hợp của mỗi mẫu để được xử lý bởi mỗi chuyên gia. Đối với mỗi mẫu đầu vào, các chuyên gia có điểm số k cao nhất được chọn để xử lý mẫu. Ngoài ra, một số vòng warm-up được thực hiện, thực hiện cùng một tính toán nhưng không được tính trong kết quả. Đối với mỗi thí nghiệm, tác vụ được thực thi 16 lần, và thời gian trung bình được sử dụng để tính toán hiệu suất. Các giá trị độ lệch chuẩn của thời gian thực thi cũng được kiểm tra. Tất cả chúng đều không đáng kể.

5.2 TỐC ĐỘ HUẤN LUYỆN TRÊN MỘT GPU ĐƠN

Hiệu suất của FMoETransformerMLP được kiểm tra, hoàn thành tác vụ tương tự với module MoE trong baseline (Rau, 2019), trên một GPU NVIDIA TESLA V100 PCIe. Baseline được triển khai bằng API PyTorch thuần túy với cấu trúc mô hình được mã hóa cứng. Để công bằng trong so sánh, cả hai module đều sử dụng một ma trận được khởi tạo ngẫu nhiên làm trọng số của mạng cổng, bao gồm một lớp fully-connected. Các chuyên gia cũng thực hiện cùng một tính toán.

2 4 8 16 32 64 128
Số lượng chuyên gia01020304050607080Thời gian tính toán / msBaseline fwd
Baseline bwd
FastMoE fwd
FastMoE bwd

Độ trễ được kiểm tra với nb= 4096; dm= 1024; dh= 4096; k= 2.

Hình 5: So sánh thời gian tính toán giữa FastMoE và triển khai baseline.

Như Hình 5 hiển thị, triển khai baseline liên tục chậm hơn FastMoE. Khi số lượng chuyên gia tăng, baseline tiêu tốn nhiều thời gian hơn trong tính toán tiến, trong khi độ trễ của FastMoE vẫn ổn định, nhờ stream manager tùy chỉnh được đề cập trong Phần 4. Xem xét rằng FastMoE được nhắm mục tiêu về huấn luyện, thời gian lùi được xếp chồng lên thời gian tiến. Chúng tôi quan sát thấy rằng FastMoE vượt trội hơn baseline trong tổng thời gian tiêu tốn trong mỗi iteration.

--- TRANG 6 ---
Bản thảo

5.3 KHẢ NĂNG MỞ RỘNG CROSS-GPU VÀ CROSS-NODE

Để kiểm tra hiệu suất của FastMoE mở rộng trên nhiều GPU qua các nút, chúng tôi tiến hành một thí nghiệm trên một cụm 8 nút, với 1 GPU NVIDIA Tesla V100 trên mỗi nút. Cụm được kết nối thông qua một switch Infiniband EDR và 8 card HCA. FLOP của các hoạt động nhân ma trận được tính toán để đại diện cho throughput huấn luyện.

1 2 3 4 5 6 7 8
Số lượng GPU510152025TFLOPs
Tổng throughput
Throughput trung bình mỗi GPU

Throughput được kiểm tra với ne= 4; nb= 4096; dm= 1024; dh= 4096; k= 2.

Hình 6: Khả năng mở rộng của FastMoE trên nhiều GPU trên nhiều nút

Theo kết quả hiển thị trong Hình 6, FastMoE cho thấy khả năng mở rộng qua các nút. Tổng throughput tăng từ 10 TFLOPs đến 25 TFLOPs, khi số lượng GPU tăng từ 2 đến 8, mở rộng dưới tuyến tính. Chúng tôi quan sát thấy rằng khi mở rộng đến 2 GPU, hiệu suất là một nửa so với trên một GPU đơn, điều này cho thấy rằng FastMoE bị giới hạn bởi giao tiếp. Khi nhiều GPU hơn được sử dụng để tính toán, nhiều chuyên gia hơn được giới thiệu, và độ chi tiết của việc trao đổi mẫu đầu vào trở nên nhỏ hơn, làm giảm hiệu quả trong truyền dữ liệu qua mạng.

Kết luận, khả năng mở rộng của FastMoE có thể hỗ trợ huấn luyện mô hình MoE lớn sử dụng nhiều GPU qua nhiều nút với lợi ích hiệu suất. Tuy nhiên, không gian còn lại để tối ưu hóa thêm về throughput.

5.4 LỢI ÍCH HIỆU SUẤT END-TO-END SỬ DỤNG FastMoE

Chúng tôi kiểm tra lợi ích hiệu suất end-to-end sử dụng FastMoE bằng cách huấn luyện một mô hình GPT 12 lớp trên 8 GPU sử dụng Megatron-LM (Shoeybi et al., 2019). Như đề cập trong Phần 3, adapter Megatron của FastMoE được sử dụng cho cấu trúc MoE. Đối với mỗi lớp, 96 chuyên gia được phân phối qua các GPU, tức là 12 chuyên gia được đặt trên mỗi GPU. Đối với mỗi token đầu vào, top 2 chuyên gia có điểm số cao nhất được sử dụng để xử lý nó. dh trong lớp MLP chuyên gia được giảm một nửa để FLOP hợp lệ của mô hình gần như giống hệt nhau, ngoại trừ FLOP bổ sung được giới thiệu bởi cổng, là không đáng kể. Cả mô hình baseline và mô hình MoE đều được huấn luyện trong 70 giờ. Metric lm loss trong huấn luyện chỉ ra xu hướng hội tụ của các mô hình.

Từ Hình 7, chúng tôi quan sát thấy rằng tốc độ huấn luyện của mô hình baseline là khoảng 3× của FastMoE. Vì FastMoE thực hiện nhiều tính toán và giao tiếp hơn, đó là một sự chậm lại hợp lý. May mắn thay, mô hình MoE đạt được loss thấp hơn nhiều với cùng số iteration huấn luyện. Ngoài ra, như một lợi ích từ hiệu quả của FastMoE, mô hình MoE đạt được loss thấp hơn trong cùng thời gian huấn luyện.

0 100000 200000 300000
Bước3.003.253.503.754.004.254.504.755.00lm loss
0 20 40 60
Thời gian / giờBaseline
FastMoE 96 chuyên gia

Các đường tối màu hẹp được làm mượt theo cấp số nhân bởi 0,97 từ đường cong loss gốc, được đại diện bởi các đường cong rộng sáng hơn tương ứng.

Hình 7: Đường cong loss của việc huấn luyện mô hình GPT bằng FastMoE

6 TÓM TẮT VÀ CÔNG VIỆC TƯƠNG LAI

Trong bài báo này, chúng tôi trình bày FastMoE, một hệ thống mã nguồn mở để huấn luyện các mô hình Mixture-of-Experts. Hệ thống dựa trên framework PyTorch phổ biến, và hiện tại hỗ trợ huấn luyện hiệu quả trên GPU. Giao diện thân thiện của nhiều cấp độ được cung cấp cho các người dùng khác nhau để khám phá các khía cạnh khác nhau của kiến trúc MoE. Hiệu suất của FastMoE trên một GPU đơn được tối ưu hóa tốt để khai thác sức mạnh của GPU. FastMoE cũng có thể chạy trên các GPU trên nhiều nút với khả năng mở rộng hợp lý, cho phép mở rộng thêm kích thước mô hình. Lợi thế hiệu suất mô hình thực được quan sát trong thí nghiệm huấn luyện mô hình end-to-end của chúng tôi sử dụng FastMoE.

Chúng tôi vẫn đang làm việc trên FastMoE để có thêm tính năng và huấn luyện nhanh hơn. So với mô hình GShard (Chen et al., 2020), FastMoE thiếu các chức năng để hỗ trợ cân bằng tải giữa các chuyên gia. Công việc về monitor cân bằng tải và hỗ trợ cho loss cân bằng tải đang trong tiến trình. Chúng tôi cũng đang cố gắng làm cho hệ thống thân thiện hơn với người dùng về các tiện ích, chẳng hạn như tải và lưu các mô hình MoE. Hiệu suất trên nhiều GPU đòi hỏi nỗ lực chung từ quan điểm cả tính toán hiệu suất cao và học máy. Bất kỳ đóng góp nào cho dự án mã nguồn mở sẽ được đánh giá cao. Chúng tôi mong muốn sự tham gia của bạn.

--- TRANG 7 ---
Bản thảo

TÀI LIỆU THAM KHẢO

Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In 12th fUSENIXg symposium on operating systems design and implementation (fOSDIg16), pp. 265–283, 2016.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.

Dehao Chen, Dmitry Dima Lepikhin, HyoukJoong Lee, Maxim Krikun, Noam Shazeer, Orhan Firat, Yanping Huang, Yuanzhong Xu, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. 2020.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2021.

Sylvain Jeaugey. Nccl 2.0. In GPU Technology Conference (GTC), 2017.

Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pp. 1–12, 2017.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020.

David Rau. Sparsely-gated mixture-of-experts pytorch implementation, 2019. URL https://github.com/davidmrau/mixture-of-experts.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

--- TRANG 8 ---
Bản thảo

Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorflow: Deep learning for supercomputers. arXiv preprint arXiv:1811.02084, 2018.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053, 2019.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, 2018.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.
