# Tỉa gọn chuyên gia theo tác vụ cho
Hỗn hợp chuyên gia thưa thớt
Tianyu Chen1;2, Shaohan Huang4, Yuan Xie3, Binxing Jiao5, Daxin Jiang5,
Haoyi Zhou1;2, Jianxin Li1;2, Furu Wei4
1BDBC, Đại học Bách Khoa Bắc Kinh, Trung Quốc2Phòng thí nghiệm SKLSDE, Đại học Bách Khoa Bắc Kinh, Trung Quốc
3Viện Âm học của Viện Hàn lâm Khoa học Trung Quốc, Trung Quốc
4Microsoft Research5Nhóm NLP, Microsoft STCA

Tóm tắt
Mô hình Hỗn hợp chuyên gia (MoE) thưa thớt rất mạnh mẽ cho việc tiền huấn luyện quy mô lớn
và đã đạt được kết quả đầy hứa hẹn nhờ khả năng mô hình của nó. Tuy nhiên, với
hàng nghìn tỷ tham số, MoE khó triển khai trên môi trường đám mây hoặc di động.
Việc suy luận của MoE yêu cầu tính song song chuyên gia, điều này không thân thiện với phần cứng
và tốn kém về mặt truyền thông. Đặc biệt đối với các tác vụ hạ lưu có tài nguyên hạn chế,
cấu trúc thưa thớt như vậy phải hy sinh rất nhiều hiệu quả tính toán để có được những cải thiện hiệu suất hạn chế. Trong công trình này, chúng tôi quan sát rằng hầu hết các chuyên gia đóng góp rất ít cho việc tinh chỉnh và suy luận MoE. Chúng tôi tiếp tục đề xuất một phương pháp tổng quát để dần dần loại bỏ các chuyên gia không chuyên nghiệp cho tác vụ hạ lưu mục tiêu, giúp bảo tồn lợi ích của MoE trong khi giảm mô hình MoE thành một mô hình dày đặc chuyên gia đơn lẻ. Các thí nghiệm của chúng tôi cho thấy rằng mô hình chuyên gia đơn lẻ đã tinh chỉnh có thể bảo tồn 99,3% lợi ích từ MoE trên sáu loại tác vụ khác nhau trong khi tận hưởng tốc độ suy luận nhanh gấp 2 lần với chi phí truyền thông miễn phí.

1 Giới thiệu
Hình 1: Đóng góp của các chuyên gia khác nhau trong các tác vụ hạ lưu khác nhau Phân phối của
đóng góp khá có đuôi dài, cho thấy một số chuyên gia "chuyên nghiệp" hơn trong các tác vụ như vậy.

Trong những năm gần đây, việc mở rộng quy mô của các mô hình mạng thần kinh đã mang lại những cải thiện chất lượng đáng kể trong cả lĩnh vực ngôn ngữ Devlin et al. [2019], Brown et al. [2020] và thị giác Dosovitskiy et al. [2020]. Trong khi các mô hình dày đặc lớn đã đạt đến giới hạn về kích thước mô hình và khả năng phần cứng, một hướng nghiên cứu khác đã đề xuất các lớp Hỗn hợp chuyên gia (MoE) cổng thưa thớt như một giải pháp thay thế hiệu quả cho các mô hình dày đặc Lepikhin et al. [2020], Fedus et al. [2021b], Riquelme et al. [2021]. Trong một mô hình MoE cổng thưa thớt vani, mỗi token của chuỗi đầu vào kích hoạt một tập con khác nhau của các chuyên gia, do đó chi phí tính toán cho mỗi token chỉ tỷ lệ thuận với kích thước của mạng con được kích hoạt. Đối với một tác vụ tiền huấn luyện tổng quát, sự phức tạp và đa dạng của mục tiêu tối ưu yêu cầu mỗi mạng con (chuyên gia) đảm nhận một phần chuyên nghiệp của tác vụ con, do đó tăng khả năng mô hình và dẫn đến khả năng tổng quát hóa.

Tuy nhiên, trình độ chuyên môn của các chuyên gia trong mô hình MoE còn chưa được khám phá kỹ lưỡng trong các giai đoạn tinh chỉnh và phục vụ. Khác với một tác vụ tiền huấn luyện tổng quát và phức tạp, tác vụ hạ lưu để tinh chỉnh mô hình MoE luôn cụ thể và có dữ liệu hạn chế. Xét đến sự đa dạng của các chuyên gia trong các mô hình MoE đã tiền huấn luyện, chỉ một tập con các chuyên gia sẽ được kích hoạt tốt, điều này có thể chiếm ưu thế trong đóng góp cho chuỗi đầu vào. Như được mô tả trong Hình 1, ngay cả với cùng một mô hình MoE đã tiền huấn luyện, đóng góp của các chuyên gia trong MNLI, CoLA và SQuAD khá khác nhau. Dựa trên quan sát như vậy, chúng tôi đề xuất một câu hỏi quan trọng: Đối với các tác vụ hạ lưu, liệu chúng ta có thể chọn ra chuyên gia chuyên nghiệp nhất bằng các chiến lược tinh chỉnh nhất định?.

Việc chuyển đổi các mô hình MoE thưa thớt lớn thành mô hình dày đặc chuyên gia đơn lẻ nhỏ bằng phương pháp tỉa gọn chuyên gia theo tác vụ cụ thể có thể mang lại một số lợi thế. Đầu tiên, chuyên gia chuyên nghiệp nhất được chọn cho tác vụ kế thừa kiến thức có thể chuyển giao nhất từ việc tiền huấn luyện MoE, điều này có thể đóng góp vào hiệu suất tốt hơn so với đối tác huấn luyện dày đặc. Thứ hai, mô hình chuyên gia đơn lẻ tránh được tính song song của các chuyên gia, điều này giảm yêu cầu về thiết bị và cắt giảm chi phí truyền thông giữa các thiết bị, do đó tăng hiệu quả suy luận. Cuối cùng, không cần phải nỗ lực chưng cất thêm, mô hình chuyên gia đơn lẻ đã biến mô hình MoE thành kiến trúc đối tác dày đặc, tương thích với nhiều loại tác vụ hạ lưu khác nhau.

Trong công trình này, chúng tôi khám phá một mô hình tổng quát về cách tìm ra chuyên gia chuyên nghiệp nhất trong các mô hình MoE trong giai đoạn tinh chỉnh, nhằm biến mô hình MoE huấn luyện thưa thớt thành đối tác dày đặc chuyên gia đơn lẻ. Chúng tôi đi sâu vào các khu vực chưa được khám phá kỹ lưỡng, bao gồm tiêu chí tỉa gọn và thời điểm loại bỏ các chuyên gia. Trong một kịch bản lý tưởng, chúng tôi muốn huấn luyện hiệu quả một mô hình lớn duy nhất tối đa hóa việc chuyển giao tích cực trong khi mở rộng nút cổ chai về khả năng; đồng thời, chúng tôi muốn tận hưởng lợi ích của các mạng con được kích hoạt thưa thớt cho từng tác vụ tại thời điểm suy luận. Trong thực tế, chúng tôi đặt một ngưỡng loại bỏ sau mỗi k bước của quá trình tinh chỉnh. Nếu điểm trình độ chuyên môn của một chuyên gia không đáp ứng được ngưỡng loại bỏ, chuyên gia đó sẽ bị tỉa gọn. Chúng tôi lặp lại thao tác loại bỏ cho đến khi chỉ còn một chuyên gia duy nhất trong mỗi lớp MoE hoặc quá trình tinh chỉnh đã qua nửa chừng. Thông thường, chuyên gia chuyên nghiệp nhất của tác vụ sẽ chiếm ưu thế trong đóng góp trước khi kết thúc quá trình tinh chỉnh một nửa, do đó chúng tôi phải loại bỏ tất cả các chuyên gia không chuyên nghiệp khác và giữ lại chuyên gia được chọn để tối ưu hóa tiếp theo.

Chúng tôi tóm tắt những đóng góp chính của mình như sau:
• Chúng tôi khám phá mô hình tỉa gọn chuyên gia theo tác vụ cụ thể, một phương pháp tinh chỉnh thân thiện với thiết bị tổng quát cho Hỗn hợp chuyên gia. Chúng tôi cẩn thận loại bỏ các chuyên gia không chuyên nghiệp cho tác vụ hạ lưu và giữ lại chuyên gia chuyên nghiệp nhất để suy luận và phục vụ. Chúng tôi cho thấy rằng chuyên gia được chọn có thể được chuyển giao tích cực và thành công bảo tồn hầu hết lợi ích từ việc tiền huấn luyện MoE.
• Chúng tôi cho thấy rằng các MoE đã tỉa gọn vượt trội hơn các đối tác dày đặc của chúng về các tác vụ NLI, cảm tính, tương tự và QA theo thuật ngữ tuyệt đối. Hơn nữa, tại thời điểm suy luận, các MoE đã tỉa gọn có thể (i) bằng hoặc thậm chí vượt trội hơn các MoE tinh chỉnh tất cả chuyên gia. và (ii) loại bỏ chi phí truyền thông giữa các thiết bị khác nhau.
• Chúng tôi cung cấp trực quan hóa về việc tỉa gọn chuyên gia trong quá trình tinh chỉnh, tiết lộ các mô hình và kết luận đã giúp thúc đẩy các quyết định lựa chọn và có thể cải thiện thêm hiệu suất cuối cùng của các mô hình đã tinh chỉnh.

2 Công trình liên quan
Mô hình Hỗn hợp chuyên gia thưa thớt Các mô hình Hỗn hợp chuyên gia (MoE) thưa thớt cung cấp một cách hiệu quả cao để mở rộng quy mô huấn luyện mạng thần kinh, đã được nghiên cứu gần đây trong Lepikhin et al. [2020], Fedus et al. [2021b], Shazeer et al. [2017]. So với các mô hình dày đặc tiêu chuẩn yêu cầu chi phí tính toán cực kỳ cao trong huấn luyện, các mô hình MoE được chứng minh là có thể xử lý dữ liệu với quy mô lớn hơn nhiều và hội tụ nhanh hơn đáng kể. Một số nghiên cứu gần đây tập trung vào vấn đề gán chuyên gia, chẳng hạn như việc xây dựng phân bổ token-to-expert như một phép gán tuyến tính Lewis et al., định tuyến dựa trên băm Roller et al., và chưng cất trong hàm định tuyến Dai et al. [2022]. Ngoài ra, một số công trình gần đây cho thấy các phương pháp huấn luyện khác nhau cho các mô hình MoE thưa thớt. Dua et al. [2021] phát hiện ra rằng cơ chế làm nóng nhiệt độ và tiền huấn luyện dày đặc có thể cải thiện hiệu suất của các mô hình dịch thuật thưa thớt. Nie et al. [2021] đề xuất một chiến lược cổng từ dày đặc đến thưa thớt cho việc huấn luyện MoE. Một số bài báo đã khám phá hiệu suất của việc tinh chỉnh có giám sát trong các mô hình MoE và đề xuất một số phương pháp để tăng chất lượng tinh chỉnh. Artetxe et al. đã giới thiệu một quy chuẩn chuyên gia để cải thiện việc tinh chỉnh mô hình thưa thớt. Chi et al. [2022] đề xuất một phương pháp giảm chiều và chuẩn hóa L2 để giải quyết vấn đề sụp đổ biểu diễn trong tinh chỉnh. Thay vì cải thiện việc tinh chỉnh của các mô hình MoE, công trình này tập trung vào việc tinh chỉnh trực tiếp các mô hình thưa thớt thành các mô hình dày đặc chuyên gia đơn lẻ, cụ thể là tỉa gọn chuyên gia theo tác vụ cụ thể.

Suy luận hiệu quả trong mô hình MoE Các mô hình MoE đã đạt được kết quả đầy hứa hẹn. Tuy nhiên, chúng không đáp ứng được yêu cầu về hiệu quả suy luận trong giai đoạn tinh chỉnh và phục vụ. Các nghiên cứu gần đây áp dụng chưng cất kiến thức để suy luận nhanh hơn. Rajbhandari et al. đề xuất một mô hình MoE nhỏ hơn (có tên là Mixture-of-Students) với chưng cất kiến thức. Tuy nhiên, MoE thưa thớt làm cho việc suy luận khó khăn vì các lý do sau: nếu tham số của mô hình vượt quá dung lượng bộ nhớ của một thiết bị tăng tốc duy nhất, việc suy luận yêu cầu nhiều thiết bị. Chi phí truyền thông giữa các thiết bị làm tăng thêm chi phí phục vụ tổng thể. Đã có một số công trình áp dụng chưng cất các mô hình MoE thưa thớt thành các mô hình dày đặc Fedus et al. [2021a], Xue et al., Artetxe et al.. Fedus et al. [2021a] trình bày một nghiên cứu về việc chưng cất một mô hình thưa thớt đã tinh chỉnh thành một mô hình dày đặc. Phương pháp chưng cất theo tác vụ cụ thể cần tinh chỉnh các mô hình khác nhau cho các tác vụ khác nhau tương ứng. Công trình này không yêu cầu tinh chỉnh bổ sung trên các mô hình MoE và điều chỉnh trực tiếp các mô hình MoE thưa thớt thành các mô hình dày đặc chuyên gia đơn lẻ.

3 Kiến thức cơ bản
Hãy nhớ lại các mô hình Mixture-of-Expert Transformer được đề xuất bởi Shazeer et al. Shazeer et al. [2017], trong đó lớp MoE nhận một biểu diễn token đầu vào x và sau đó định tuyến token này đến k chuyên gia hàng đầu được xác định tốt nhất được chọn từ một tập hợp {Exp_i(x)}_{i=1}^E của E chuyên gia. Các logit h(x) = W_r x được tạo ra bởi biến bộ định tuyến W_r được chuẩn hóa thông qua một phân phối softmax trên E chuyên gia có sẵn tại lớp đó. Giá trị cổng của chuyên gia i được cho bởi:

g_i(x) = e^{h(x)_i} / Σ_{j=1}^E e^{h(x)_j}  (1)

Các giá trị cổng k hàng đầu được chọn để định tuyến token x. Nếu P là tập hợp các chỉ số k hàng đầu được chọn thì phép tính đầu ra của lớp là tổ hợp tuyến tính có trọng số của phép tính của mỗi chuyên gia trên token theo giá trị cổng:

y = Σ_{i∈P} g_i(x)Exp_i(x)  (2)

Ký hiệu Trong bài báo này, ký hiệu E biểu thị tổng số chuyên gia trong khi e biểu thị id của chuyên gia trong một lớp Mixture-of-Experts. N biểu thị tổng số bước huấn luyện và w biểu thị một cửa sổ huấn luyện cụ thể. Z biểu thị số lượng chuyên gia sống sót trong cửa sổ huấn luyện hiện tại.

Xây dựng bài toán Với dữ liệu được gắn nhãn của một tác vụ hạ lưu và một mô hình MoE đã tiền huấn luyện với một số lớp mixture-of-experts, trong đó mỗi lớp MoE có E chuyên gia, chúng tôi cố gắng tìm ra chuyên gia chuyên nghiệp nhất ê cho tác vụ trong mỗi lớp MoE và tỉa gọn các chuyên gia không chuyên nghiệp khác, trong quá trình tinh chỉnh mô hình MoE trên dữ liệu được gắn nhãn.

4 Phương pháp

4.1 Tổng quan
Chúng tôi minh họa phương pháp của mình trong Hình 2, trong đó chúng tôi không phá vỡ quy trình tiêu chuẩn của việc tinh chỉnh mô hình MoE trên các tác vụ hạ lưu. Thay vào đó, chúng tôi chia N bước huấn luyện của toàn bộ quá trình tinh chỉnh thành E cửa sổ huấn luyện một cách đều nhau. Trong một cửa sổ huấn luyện, chúng tôi sẽ tích lũy điểm trình độ chuyên môn của mỗi chuyên gia khi các biểu diễn của các token đầu vào đi qua lớp MoE. Ở cuối một cửa sổ huấn luyện, chúng tôi quyết định chuyên gia nào không chuyên nghiệp và nên được tỉa gọn. Các chuyên gia đã tỉa gọn sẽ không bao giờ tham gia vào việc gán các token tại các cửa sổ huấn luyện sau đó, cũng sẽ bị loại bỏ khỏi quá trình suy luận. Tại điểm giữa của toàn bộ lịch trình huấn luyện, chúng tôi đưa ra quyết định cuối cùng và chỉ giữ lại chuyên gia chuyên nghiệp nhất trong mỗi lớp MoE và tỉa gọn tất cả các chuyên gia khác. Một khi chuyên gia chuyên nghiệp nhất được chọn, tất cả các token đầu vào sẽ được gán cho chuyên gia đó và giảm mô hình MoE thưa thớt thành đối tác dày đặc.

4.2 Tiêu chí về trình độ chuyên môn của chuyên gia
Điểm quan trọng đầu tiên của phương pháp chúng tôi là đánh giá trình độ chuyên môn của chuyên gia. Lưu ý rằng khi đầu vào với chuỗi token x_s, mô hình MoE sẽ thực hiện hai thao tác liên quan đến lịch trình của các chuyên gia. Thao tác đầu tiên là gán các token, trong đó mỗi token chỉ có thể được gán cho một chuyên gia duy nhất. Thao tác thứ hai là chinh phục kết quả phân chia, sử dụng g_i để tính tổng có trọng số của đầu ra chuyên gia được chọn và xây dựng đầu ra cuối cùng y_s. Trong quá trình tinh chỉnh các mô hình MoE, cả hai thao tác liên quan đến lịch trình chuyên gia đều là tiêu chí về trình độ chuyên môn của chuyên gia. Chúng tôi định nghĩa chúng tương ứng là tỷ lệ trúng R và điểm alpha A.

Trong phương pháp của chúng tôi, chúng tôi chọn A làm tiêu chí mặc định và định nghĩa điểm trình độ chuyên môn của chuyên gia i trong một cửa sổ huấn luyện là C_i:

C_i = Σ_{k=wN/E}^{(w+1)N/E} A_i  (3)

A_i = g_i / Σ_{j=1}^E g_j  (4)

Ở đây, w·N/E, N biểu thị bước bắt đầu của cửa sổ huấn luyện và tổng số bước huấn luyện trong lịch trình tinh chỉnh tương ứng. Cả R_i và A_i đều được tích lũy trong một cửa sổ huấn luyện gồm K bước huấn luyện. Khi có nhiều token được gán cho một chuyên gia i và điểm alpha cổng A_i cao hơn, chuyên gia i càng quan trọng đối với đầu ra cuối cùng y, do đó dẫn đến dự đoán trên tác vụ hạ lưu. Tỷ lệ trúng R_i cao hơn sẽ mang lại nhiều token đầu vào được gán cho chuyên gia i và ít token hơn được tính toán bởi các chuyên gia khác, cũng có thể đưa ra xấp xỉ phù hợp về trình độ chuyên môn của chuyên gia trên các tác vụ hạ lưu.

4.3 Thời điểm loại bỏ các chuyên gia không chuyên nghiệp
Để tìm ra chuyên gia chuyên nghiệp nhất, một phương pháp trực quan là huấn luyện hai lần, chọn chuyên gia trong lần đầu tiên và tinh chỉnh mô hình dày đặc trong lần thứ hai. Tuy nhiên, khác với chiến lược tỉa gọn hoặc chưng cất sau huấn luyện, phương pháp của chúng tôi đang theo đuổi mô hình tinh chỉnh một lần, không phát sinh chi phí tính toán bổ sung sau khi tinh chỉnh mô hình MoE. Trong chiến lược trong-quá-trình-huấn-luyện của chúng tôi, chúng tôi thấy rằng việc quyết định khi nào loại bỏ các chuyên gia không chuyên nghiệp ảnh hưởng rất lớn đến hiệu suất cuối cùng của mô hình đã tinh chỉnh. Về một khía cạnh, nếu chúng tôi tích lũy điểm trình độ chuyên môn C trong một cửa sổ huấn luyện tương đối dài, xấp xỉ của trình độ chuyên môn sẽ đáng tin cậy hơn và gần hơn với toàn bộ tác vụ. Về khía cạnh khác, quyết định sớm loại bỏ các chuyên gia không chuyên nghiệp để lại nhiều không gian huấn luyện để tối ưu hóa chuyên gia chuyên nghiệp được chọn và tránh lãng phí tính toán cho các chuyên gia không chuyên nghiệp. Do đó trong chiến lược một lần của chúng tôi, thời điểm loại bỏ là sự đánh đổi giữa việc lựa chọn chuyên gia chuyên nghiệp phù hợp hơn và việc tối ưu hóa chuyên gia cuối cùng nhiều hơn.

Trong thực tế, chúng tôi đề xuất kết hợp việc loại bỏ dần dần và loại bỏ cưỡng bức. Chúng tôi chia toàn bộ các bước huấn luyện một cách đều nhau thành E cửa sổ và dần dần đưa ra quyết định loại bỏ ở cuối mỗi cửa sổ. Nếu E/2 cửa sổ đã trôi qua và lớp MoE vẫn có hơn một chuyên gia sống sót, chỉ chuyên gia có điểm trình độ chuyên môn cao nhất sẽ được giữ lại.

4.4 Loại bỏ dần dần
Trong quá trình loại bỏ dần dần, chúng tôi có E/2 cơ hội để đưa ra quyết định loại bỏ. Đó là một vấn đề quan trọng trong mỗi quyết định loại bỏ là nên loại bỏ bao nhiêu chuyên gia. Để chọn ra những chuyên gia không chuyên nghiệp nhất trong mỗi cửa sổ huấn luyện, chúng tôi đề xuất một ngưỡng loại bỏ động T. Bất kỳ chuyên gia nào có điểm trình độ chuyên môn dưới ngưỡng T sẽ bị loại bỏ.

T = α/Z  (5)

Ở đây, Z biểu thị số lượng chuyên gia sống sót trong cửa sổ huấn luyện hiện tại. α là một siêu tham số để kiểm soát tốc độ loại bỏ dần dần. α lớn hơn sẽ tăng ngưỡng T và nhiều chuyên gia sẽ bị loại bỏ trong mỗi quyết định. Trong các thí nghiệm của chúng tôi, một α tương đối nhỏ có thể là một mặc định tốt để bảo tồn hiệu suất tối đa của các mô hình MoE.

5 Thí nghiệm
Trong phần này, chúng tôi thiết kế cả các tác vụ cấp độ chuỗi và cấp độ token để đánh giá hiệu suất và hiệu quả của mô hình tỉa gọn chuyên gia theo tác vụ cụ thể của chúng tôi. Chúng tôi cũng cung cấp một thảo luận chi tiết về các siêu tham số ảnh hưởng đến chiến lược của chúng tôi.

5.1 Các tác vụ đánh giá
GLUE Wang et al. [2019], benchmark Đánh giá hiểu ngôn ngữ tổng quát, là một tập hợp các công cụ để đánh giá hiệu suất của các mô hình trên một tập hợp đa dạng các tác vụ NLU hiện có, bao gồm STS-B Cer et al. [2017], CoLA Warstadt et al. [2019], MRPC Dolan and Brockett [2005], RTE Dagan et al. [2005], Haim et al. [2006], Giampiccolo et al. [2007], Bentivogli et al. [2009], SST-2 Socher et al. [2013], QQP, MNLI Williams et al. [2018] và QNLI Rajpurkar et al. [2016]. Theo các công trình trước đó Devlin et al. [2019], Turc et al. [2019], chúng tôi loại trừ tác vụ WNLI khỏi benchmark GLUE. Xét đến kích thước dữ liệu nhỏ của RTE có thể dẫn đến sai lệch đáng kể trong đánh giá hiệu suất, chúng tôi loại trừ kết quả RTE trong Bảng 1, nhưng bao gồm tất cả các tác vụ GLUE trong Bảng 6 Phụ lục.

SQuAD2.0 Rajpurkar et al. [2018], Bộ dữ liệu trả lời câu hỏi Stanford, là một tập hợp các cặp câu hỏi-trả lời được lấy từ các bài viết Wikipedia. Trong SQuAD, các câu trả lời đúng có thể là bất kỳ chuỗi token nào trong các tác vụ đã cho. SQuAD2.0, phiên bản mới nhất, kết hợp 100.000 câu hỏi trong SQuAD1.1 với hơn 50.000 câu hỏi không thể trả lời được viết bởi những người lao động đám đông dưới dạng tương tự như những câu có thể trả lời.

5.2 Cài đặt thí nghiệm
Chúng tôi sử dụng kiến trúc Switch Transformer cơ bản Fedus et al. [2021b] cho tập hợp thí nghiệm này. Để so sánh công bằng, chúng tôi cũng sử dụng một mô hình transformer dày đặc có cùng kiến trúc với Switch-base ngoại trừ các lớp mixture-of-experts. Chúng tôi tiền huấn luyện cả mô hình MoE và mô hình dày đặc trong 125k bước trên BooksCorpus Zhu et al. [2015] và corpus Wikipedia tiếng Anh Foundation, với mục tiêu mô hình hóa ngôn ngữ có mask. Đối với mô hình MoE, chúng tôi sử dụng một lớp MoE gồm 32 chuyên gia trong khối transformer thứ 4 và thứ 8. Cài đặt tiền huấn luyện chi tiết hơn và các siêu tham số mô hình có thể được tìm thấy trong Phụ lục A.

Ở đây chúng tôi bao gồm một số cài đặt tinh chỉnh để so sánh. Tất cả các siêu tham số tinh chỉnh có thể được tìm thấy trong Phụ lục B:

Dense-ft: Tinh chỉnh mô hình dày đặc tiêu chuẩn, dùng để chứng minh lợi ích của MoE và như một baseline mạnh mẽ cho chiến lược tỉa gọn chuyên gia của chúng tôi.

MoE-ft: Phương pháp tinh chỉnh MoE tiêu chuẩn, trong đó tất cả 32 chuyên gia tham gia vào việc tối ưu hóa cho tác vụ hạ lưu và cũng cho suy luận cuối cùng.

Two-pass-staged-drop: Một baseline tỉa gọn hai lần trực quan, trong đó chúng tôi trước tiên tích lũy điểm trình độ chuyên môn của mỗi chuyên gia trong lần tinh chỉnh đầu tiên và giữ lại chuyên gia chuyên nghiệp nhất để tinh chỉnh mô hình MoE một lần nữa.

Two-pass-eager-drop: Một baseline để chứng minh hiệu quả của tiêu chí trình độ chuyên môn của chúng tôi, trong đó chúng tôi chọn chuyên gia chuyên nghiệp nhất dựa trên quy trình của chúng tôi và giữ lại chuyên gia được chọn để tinh chỉnh lại mô hình MoE.

MoE-staged-pruning: Một chiến lược tỉa gọn chuyên gia dần dần theo tác vụ cụ thể, trong đó chúng tôi loại bỏ một chuyên gia không chuyên nghiệp nhất ở cuối mỗi cửa sổ huấn luyện, tạo ra một mô hình chuyên gia đơn lẻ để suy luận.

MoE-eager-pruning: Phương pháp thực hiện tất cả các phần của quy trình tỉa gọn chuyên gia theo tác vụ cụ thể của chúng tôi, sẽ loại bỏ các chuyên gia có điểm trình độ chuyên môn dưới mức tại mỗi cửa sổ huấn luyện và thực hiện loại bỏ cưỡng bức tại lịch trình huấn luyện nửa sau.

5.3 Bảo tồn lợi ích của MoE thành một chuyên gia duy nhất
Bảng 1: Hiệu suất trên các tác vụ GLUE cho các mô hình dày đặc và các mô hình MoE, được đo trên các tập phát triển. Chúng tôi báo cáo kết quả trung bình theo một tập hợp các seed. AVG biểu thị điểm trung bình của tất cả các tác vụ. Các mô hình MoE đã tỉa gọn chuyên gia theo tác vụ cụ thể chỉ với chuyên gia đơn lẻ có thể đạt được khoảng 0,84 điểm GLUE trung bình hơn so với các mô hình dày đặc.

Cài đặt | CoLA | STS-B | MNLI-m | MNLI-mm | SST-2 | QQP | QNLI | MRPC | AVG
Dense-ft | 57.23 | 89.18 | 85.87 | 85.87 | 92.87 | 91.20 | 92.23 | 88.07 | 85.32
MoE-ft | 63.75 | 89.37 | 86.93 | 86.93 | 94.03 | 91.37 | 92.90 | 89.13 | 86.80
Two-pass-staged-drop | 58.32 | 88.64 | 86.30 | 85.90 | 90.00 | 90.50 | 91.40 | 86.90 | 84.75
Two-pass-eager-drop | 60.89 | 89.02 | 86.10 | 86.20 | 92.60 | 90.60 | 91.50 | 87.72 | 85.58
MoE-staged-pruning | 56.20 | 89.00 | 85.33 | 85.33 | 92.33 | 90.07 | 91.97 | 86.83 | 84.63
MoE-eager-pruning | 61.47 | 89.43 | 86.10 | 86.10 | 93.17 | 91.20 | 92.53 | 89.30 | 86.16

Trong Bảng 1, kết quả có thể được chia thành ba nhóm. Nhóm đầu tiên là baseline, trong đó cả mô hình MoE tiền huấn luyện thưa thớt và các đối tác tiền huấn luyện dày đặc của chúng được tinh chỉnh theo cách tiêu chuẩn. Chúng tôi quan sát thấy MoE tận hưởng lợi thế đầy đủ ở tất cả các tác vụ, với điểm GLUE trung bình 1,48 hơn so với các đối tác tiền huấn luyện dày đặc của chúng, điều này có thể nhờ vào khả năng tổng quát hóa mạnh mẽ của việc tiền huấn luyện mixture-of-experts.

Trong nhóm thứ hai, chúng tôi so sánh hai chiến lược tỉa gọn chuyên gia trực quan dựa trên tối ưu hóa hai lần. Với ngân sách tính toán nhiều hơn của một lần khác, chúng tôi có thể chọn chuyên gia chuyên nghiệp nhất trong lần đầu tiên trong khi tối ưu hóa chuyên gia được chọn trong lần thứ hai. Chúng tôi thấy rằng đối với phương pháp loại bỏ theo từng giai đoạn hai lần, một lần khác có thể tăng hiệu suất cuối cùng của mô hình lên 0,12 điểm GLUE trung bình. Đối với việc loại bỏ tích cực hai lần, một lần khác làm tổn hại hiệu suất, cắt giảm điểm GLUE trung bình xuống 0,58. Chúng tôi giả định rằng một lần khác cung cấp nhiều không gian để tối ưu hóa chuyên gia cuối cùng được chọn trong việc loại bỏ theo giai đoạn, điều này có thể là tối ưu phụ trong lần đầu tiên. Tuy nhiên, đối với việc loại bỏ tích cực hai lần, chuyên gia được chọn có thể được tối ưu hóa tốt và có xu hướng overfit trong lần huấn luyện thứ hai.

Trong nhóm cuối cùng, chúng tôi quan tâm đến việc liệu phương pháp của chúng tôi có thể bảo tồn hầu hết lợi thế của mixture-of-experts bằng cách chỉ giữ lại chuyên gia chuyên nghiệp nhất duy nhất hay không. Chúng tôi thấy rằng trong cài đặt tỉa gọn MoE theo giai đoạn, chuyên gia được chọn không thể bảo tồn lợi thế của MoE và chỉ đạt được điểm trung bình 84,63, thấp hơn 0,69 so với đối tác tiền huấn luyện dày đặc. Tuy nhiên, đối với việc tỉa gọn MoE tích cực, chuyên gia được chọn có thể bảo tồn hầu hết lợi ích từ việc tiền huấn luyện MoE và vượt trội hơn các mô hình tiền huấn luyện dày đặc 0,84 điểm GLUE trung bình. Chúng tôi giả định rằng thất bại của chiến lược theo giai đoạn có thể do thiếu đủ thích nghi của chuyên gia cuối cùng được chọn, điều mà chúng tôi sẽ thảo luận thêm trong phần 5.5. Ngoài ra, chúng tôi thấy rằng chuyên gia đơn lẻ được chọn bằng phương pháp tích cực thậm chí có thể vượt quá hiệu suất của các mô hình MoE tinh chỉnh tiêu chuẩn, có 32 chuyên gia, trong các tác vụ STS-B và MRPC. Xét đến khả năng mô hình quá mức của các mô hình MoE, tinh chỉnh với tất cả các chuyên gia có thể không phải lúc nào cũng là mặc định tốt cho các tác vụ hạ lưu có dữ liệu hạn chế.

Bảng 2: Hiệu suất trên tác vụ SQuAD2.0. EM biểu thị độ chính xác của các câu trả lời khớp chính xác.

Cài đặt | SQuAD2.0-F1 | SQuAD2.0-EM
MoE-ft | 80.59 | 77.85
Dense-ft | 79.77 | 77.23
Two-pass-drop | 80.11 | 77.31
Two-pass-eager-drop | 80.33 | 77.40
MoE-staged-pruning | 80.08 | 77.24
MoE-eager-pruning | 80.40 | 77.55

Đối với tác vụ cấp token, chúng tôi trình bày kết quả của SQuAD 2.0 trong Bảng 2. Chúng tôi thấy rằng các mô hình MoE tiền huấn luyện thưa thớt vượt trội hơn các đối tác dày đặc của chúng 0,82 điểm F1 và 0,62 điểm EM. Đối với phương pháp tỉa gọn chuyên gia theo tác vụ cụ thể của chúng tôi, cả chiến lược tỉa gọn theo giai đoạn và tích cực đều thể hiện hiệu suất tương đương hoặc tốt hơn so với baseline dense-ft. Kết quả của SQuAD 2.0 phù hợp với những kết quả trong benchmark GLUE cấp câu, trong đó việc tỉa gọn tích cực vượt trội hơn tỉa gọn theo giai đoạn 0,32 điểm F1 và 0,31 điểm EM. Ngoài ra, tối ưu hóa hai lần cũng vượt trội hơn baseline dense-ft, trong đó two-pass-eager-drop vượt trội hơn dense-ft 0,56 điểm F1 và 0,17 điểm EM. Chúng tôi giả định rằng các tác vụ cấp token nhạy cảm với chuyên gia hơn và phương pháp tỉa gọn chuyên gia của chúng tôi dễ dàng xác định được chuyên gia chuyên nghiệp nhất, dẫn đến hiệu suất tốt hơn.

5.4 Các tiêu chí tỉa gọn khác nhau
Như đã đề cập trong Mục 4.2, cả tỷ lệ trúng R và điểm alpha A đều có thể là đánh giá về trình độ chuyên môn của chuyên gia. Chúng tôi trình bày kết quả của hai tiêu chí tỉa gọn khác nhau này trong Hình 3. Việc tỉa gọn dựa trên R chỉ vượt trội hơn baseline dense-ft trên tác vụ MRPC 0,17 điểm độ chính xác trong khi thực hiện tệ hơn trong năm tác vụ khác. Tuy nhiên, việc tỉa gọn dựa trên A cho thấy hiệu suất tốt hơn nhiều trên tất cả năm tác vụ và đạt hiệu suất tốt hơn so với baseline MoE-ft trên MRPC.

Chúng tôi giả định rằng nhiều token được gán cho một chuyên gia có thể không thể hiện tốt đóng góp của nó vì đầu ra của phép tính chuyên gia trên token x sẽ được tính lại trọng số bởi điểm cổng g_i(x). Do đó A có thể được xem là R "mềm" với đánh giá chính xác hơn về trình độ chuyên môn của chuyên gia.

5.5 Nhìn kỹ hơn vào thời điểm loại bỏ
Một câu hỏi quan trọng liên quan đến việc tỉa gọn chuyên gia theo tác vụ cụ thể là khi nào chúng ta nên đưa ra quyết định cuối cùng để loại bỏ tất cả các chuyên gia khác. Nhớ lại rằng trong các phương pháp của chúng tôi, hai yếu tố có thể ảnh hưởng đến quyết định cuối cùng. Yếu tố đầu tiên là độ dài của cửa sổ huấn luyện và yếu tố thứ hai là ngưỡng loại bỏ.

Như được mô tả trong Hình 4, chúng tôi điều tra xem các cửa sổ huấn luyện nên được chia trong bao lâu để đạt được hiệu suất cuối cùng tốt hơn trên các tác vụ MNLI, SST-2, QQP, QNLI và MRPC. Chúng tôi quan sát thấy rằng đối với các tác vụ MNLI, SST-2, QQP và MRPC, một cửa sổ huấn luyện dài hơn đóng góp vào hiệu suất cuối cùng tốt hơn của chuyên gia được chọn. Trong khi đối với tác vụ QNLI, độ dài cửa sổ huấn luyện là 0,5·N/E cũng có thể đạt được hiệu suất tương đương với cửa sổ huấn luyện tối đa. Chúng tôi giả định rằng đối với hầu hết các tác vụ, các cửa sổ huấn luyện dài hơn có lợi cho việc tăng hiệu suất của chuyên gia được chọn.

Hình 5: So sánh các ngưỡng loại bỏ khác nhau. Trong hình bên trái, chúng tôi minh họa độ chính xác MoE đã tinh chỉnh trên tác vụ MRPC với ngưỡng A khác nhau. Trong hình bên phải, chúng tôi mô tả số lượng chuyên gia sống sót tại một nửa của lịch trình, được ký hiệu là K_half.

Một cách khác để kiểm soát tiến trình của việc tỉa gọn là ngưỡng loại bỏ. Chúng tôi cho thấy hiệu ứng của các ngưỡng loại bỏ khác nhau trong Hình 5. Một ngưỡng loại bỏ thấp hơn có thể dẫn đến tiến trình chậm của việc lựa chọn chuyên gia và nhiều chuyên gia sống sót hơn bị buộc phải loại bỏ tại lịch trình huấn luyện một nửa, nhưng cung cấp quan sát chính xác hơn về đóng góp của các chuyên gia.

Cuối cùng, chúng tôi so sánh thời điểm để đưa ra quyết định chuyên gia cuối cùng trong Hình 6. Đối với phương pháp tỉa gọn theo giai đoạn, chúng tôi thấy rằng một số chuyên gia dần dần chiếm ưu thế trong đóng góp cho đầu ra cuối cùng trong quá trình huấn luyện. Tuy nhiên, ngay cả khi chuyên gia cuối cùng được chọn đã chiếm ưu thế trong đóng góp ở cửa sổ huấn luyện sớm, nó không thể được tối ưu hóa độc lập cho đến cửa sổ huấn luyện cuối cùng, điều này có thể trả lời cho hiệu suất tương đối kém trong giai đoạn suy luận. Dựa trên quan sát như vậy, chúng tôi đề xuất một phương pháp tỉa gọn tích cực hơn, loại bỏ tất cả các chuyên gia không chuyên nghiệp tại một nửa của việc huấn luyện vì chuyên gia được chọn của chúng tôi đã chiếm ưu thế trong đóng góp. Phương pháp này để lại nửa sau của lịch trình huấn luyện để tối ưu hóa chuyên gia được chọn, dẫn đến hiệu suất suy luận tốt hơn so với phương pháp theo giai đoạn.

5.6 Hiệu quả của suy luận
Chúng tôi trình bày tốc độ suy luận của các cài đặt khác nhau trong Hình 7, được đánh giá trên các tác vụ MNLI và SQuAD. Chúng tôi sử dụng token mỗi giây làm thước đo tốc độ suy luận mô hình và sử dụng NVIDIA-V100-32GB để thử nghiệm. Đối với mô hình MoE tinh chỉnh với 32 chuyên gia, chi phí truyền thông bổ sung giữa các thiết bị khác nhau làm giảm rất nhiều tốc độ suy luận, dẫn đến hiệu quả suy luận tệ nhất trong tất cả các cài đặt. Các mô hình chuyên gia đơn lẻ được tối ưu hóa với phương pháp tỉa gọn chuyên gia theo tác vụ cụ thể của chúng tôi, tận hưởng lợi thế hiệu quả suy luận với tốc độ tăng 2x trên tác vụ MNLI và tăng 1,28x trên tác vụ SQuAD. Do các lớp phụ MoE bổ sung trong chuyên gia được chọn, phương pháp của chúng tôi hơi chậm hơn so với hiệu quả suy luận của mô hình dày đặc, đạt 80% tốc độ suy luận của mô hình dày đặc trên các tác vụ MNLI và SQuAD.

6 Kết luận
Hình 7: Tốc độ suy luận của các mô hình tinh chỉnh khác nhau.

Trong bài báo này, chúng tôi đã thảo luận về một mô hình tinh chỉnh thân thiện với suy luận hơn cho các mô hình Mixture-of-experts đã tiền huấn luyện bằng cách tinh chỉnh chuyên gia chuyên nghiệp nhất và loại bỏ các chuyên gia khác, cụ thể là tỉa gọn chuyên gia theo tác vụ cụ thể. Chúng tôi chứng minh thực nghiệm rằng mô hình tinh chỉnh mới này có thể bảo tồn hầu hết lợi ích của các mô hình MoE đã tiền huấn luyện và tốt hơn nhiều so với các đối tác tiền huấn luyện dày đặc, trên cả các tác vụ cấp câu và cấp token. Bằng cách kiểm tra cẩn thận các yếu tố của việc tỉa gọn chuyên gia trên các cài đặt khác nhau, chúng tôi chứng minh sự ưu việt của mô hình tỉa gọn chuyên gia tích cực của chúng tôi so với các giải pháp khả thi khác như tối ưu hóa hai lần hoặc tỉa gọn chuyên gia theo giai đoạn.

Chúng tôi kết luận bằng cách nhấn mạnh rằng các mô hình tỉa gọn chuyên gia theo tác vụ cụ thể thân thiện với suy luận hơn trong khi vẫn giữ được những cải thiện chất lượng của các mô hình MoE là hướng khám phá đầy hứa hẹn trong tương lai.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên như bản gốc]

A Siêu tham số cho tiền huấn luyện
Bảng 3 trình bày các siêu tham số cho tiền huấn luyện. Bảng 4 trình bày các siêu tham số của mô hình MoE của chúng tôi.

Siêu tham số | Giá trị
Bộ tối ưu | Adam
Số bước huấn luyện | 125.000
Kích thước batch | 2.048
Adam ε | 1e-6
Adam β | (0.9, 0.98)
Tỷ lệ học tối đa | 5e-4
Lịch trình tỷ lệ học | Giảm tuyến tính
Số bước khởi động | 10.000
Phân rã trọng số | 0.01
Dropout transformer | 0.1
Trọng số mất mát cân bằng MoE | 1e-2

Bảng 3: Siêu tham số tiền huấn luyện

Siêu tham số | Giá trị
Khối transformer | 12
Kích thước ẩn | 768
Kích thước ẩn bên trong FFN | 3.072
Đầu attention | 12
Số lượng chuyên gia | 32

Bảng 4: Siêu tham số mô hình

B Siêu tham số cho tinh chỉnh
Bảng 5 trình bày các siêu tham số cho tinh chỉnh.

Siêu tham số | CoLA | STS-B | RTE | MNLI | SST-2 | QQP | QNLI | MRPC | SQuAD
Kích thước Batch | [32,16] | [32,16] | [32,16] | [32] | [32] | [32] | [32] | [32,16] | [32]
Seed | [1,2,3] | [1,2,3] | [1,2,3] | [2,3,5] | [2,3,5] | [2,3,5] | [2,3,5] | [1,2,3] | [1,2,3]
Tỷ lệ học | [2,3,4,5]e-5 | [2,3,4,5]e-5 | [2,3,4,5]e-5 | [1,2,3,4]e-5 | [1,2,3,4]e-5 | [1,2,3,4]e-5 | [1,2,3,4]e-5 | [2,3,4,5]e-5 | [2,3,4]e-5
Khởi động | [16, 10] | [16, 10] | [16, 10] | [16] | [16] | [16] | [16] | [16, 10] | [10]
Epochs | [2,3,5,10] | [2,3,5,10] | [2,3,5,10] | [2,3,5] | [2,3,5] | [2,3,5] | [2,3,5] | [2,3,5,10] | [3]

Bảng 5: Siêu tham số cho tinh chỉnh các tác vụ hạ lưu.

C Kết quả khác

Bảng 6: Kết quả GLUE hoàn chỉnh.

Cài đặt | CoLA | STS-B | RTE | MNLI-m | MNLI-mm | SST-2 | QQP | QNLI | MRPC | AVG
Dense-ft | 57.23 | 89.18 | 67.27 | 85.87 | 85.87 | 92.87 | 91.20 | 92.23 | 88.07 | 83.31
MoE-ft | 63.75 | 89.37 | 61.83 | 86.93 | 86.93 | 94.03 | 91.37 | 92.90 | 89.13 | 84.03
Two-pass-staged-drop | 58.32 | 88.64 | 60.32 | 86.30 | 85.90 | 90.00 | 90.50 | 91.40 | 86.90 | 82.03
Two-pass-eager-drop | 60.89 | 89.02 | 60.06 | 86.10 | 86.20 | 92.60 | 90.60 | 91.50 | 87.72 | 82.74
MoE-staged-pruning | 56.20 | 89.00 | 64.53 | 85.33 | 85.33 | 92.33 | 90.07 | 91.97 | 86.83 | 82.40
MoE-eager-pruning | 61.47 | 89.43 | 62.70 | 86.10 | 86.10 | 93.17 | 91.20 | 92.53 | 89.30 | 83.56
