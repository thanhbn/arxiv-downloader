# 2211.00558.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2211.00558.pdf
# File size: 1375602 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. X, NO. Y , MONTH YEAR 1
Contextual Mixture of Experts: Integrating
Knowledge into Predictive Modeling
Francisco Souza, Tim Offermans, Ruud Barendse, Geert Postma, and Jeroen Jansen
Abstract —This work proposes a new data-driven model devised
to integrate process knowledge into its structure to increase the
human-machine synergy in the process industry. The proposed
Contextual Mixture of Experts (cMoE) explicitly uses process
knowledge along the model learning stage to mold the historical
data to represent operators’ context related to the process
through possibility distributions. This model was evaluated in
two real case studies for quality prediction, including a sulfur
recovery unit and a polymerization process. The contextual
mixture of experts was employed to represent different contexts
in both experiments. The results indicate that integrating process
knowledge has increased predictive performance while improving
interpretability by providing insights into the variables affecting
the process’s different regimes.
Index Terms —soft sensors, mixture of experts, multimode
processes, process knowledge, possibility distribution
I. I NTRODUCTION
There is an increasing demand for industrial digitization
toward a more sustainable and greener industrial future. Arti-
ﬁcial intelligence (AI) is at the front of the 4th industrial rev-
olution by redeﬁning decision-making at the operational and
technical levels, allowing faster, data-driven, and automatic
decision-making along the value chain. Also, with further
growth in industrial data infrastructure, many companies are
implementing data-driven predictive models to improve en-
ergy efﬁciencies and industrial sustainability. This can reduce
production costs and environmental impact while increasing
process efﬁciency. In parallel, the new upcoming industrial
revolution (Industry 5.0) aims to leverage human knowledge
and decision-making abilities by strengthening the cooperation
between humans and machines [1]. This new revolution will
require the data-driven models to be explainable by providing
insights into the process to gain the operator’s trust and
increase synergy. The human-machine synergy can be further
enhanced by incorporating operator domain knowledge and
process information into the data-driven models.
Process information can come from various sources, in-
cluding ﬁrst-principle equations and process-speciﬁc charac-
teristics [2], such as process division structure or multiple
operating modes. First-principle models can be combined with
data-driven models within the hybrid AI models framework
[3] or within the informed machine learning framework [4].
© 2022 IEEE. Personal use of this material is permitted. Permission from
IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers
or lists, or reuse of any copyrighted component of this work in other works
F. Souza, T. Offermans, G. Postma and J. Jansen are with
the Radboud University, Institute for Molecules and Materials,
Heyendaalseweg 135, 6525AJ Nijmegen, The Netherland. Emails:
{f.souza, t.offermans, g.j.postma, jj.jansen}@science.ru.nl.For the process division structure, the multiblock modeling
is a common approach for retaining the explainability of
multi-stage processes within a multiblock representation [2].
Multiple operating modes can be caused by a change in
feedstock, operation, seasonality (aka. multimode processes),
or the sequence of phases comprising a batch cycle produc-
tion (aka. multiphase processes) [5]. These modes can be
represented in a multi-model (ensemble) structure [6]. This
work focuses on modeling processes with multiple operating
modes, and the proposed method was created with this in
mind. However, the proposed method is ﬂexible enough to
be applied to other types of processes where process expert
knowledge is available.
The modeling of multiple operating modes processes fol-
lows from rule-based expert systems [7]–[10], clustering [11],
Mixture models (MM) [12]–[14], Gaussian mixture regression
(GMR) [15], [16], or mixture of experts (MoE) [6], [17]
strategies to identify the groups that represent each operational
regime, then combine them according to the process’s regime.
Apart from rule-based expert models, none of the above
works discuss using domain knowledge from operators. In
practice, such methods do not attempt to represent process
characteristics; rather, the goal is to minimize prediction error,
and in some cases, domain knowledge is used only to initialize
the model structure. As a result, despite being accurate, the
model becomes unrepresentative of the process, making it
impossible to understand the effects of the variables in the
various regimes of the process. In fact, the rich process context
available from operators can be valuable to the model. If a
model can reﬂect the operator’s knowledge, it can play an
important role in model acceptance in the industry.
This paper proposes the contextual mixture of experts
(cMoE), a new data-driven model that connects the process’
expert domain knowledge (process context) to the predictive
model. The cMoE gives a holistic perspective to the data-
driven model while still adhering to process correlation and
representing the operators’ process context. The cMoE is
composed of a set of expert and gate models, where each
expert/gate is designed to represent a speciﬁc context of the
process employing possibility distributions. The gates repre-
sent operators’ process context in the model by deﬁning the
boundaries of each contextual region. These three components,
experts and gates, and the operator’s contextual information
form the basis of cMoE. The training procedure in cMoE uses
a learning approach devised to assure that each expert rep-
resents the deﬁned context and, at the same time, generalizes
well for unseen data. To allow model interpretability, the gates
and expert models are linear models. Also, a `1regularization
penalty is applied to the gates and experts learning for aarXiv:2211.00558v1  [cs.LG]  1 Nov 2022

--- PAGE 2 ---
2 IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. X, NO. Y , MONTH YEAR
parsimonious representation of each context model.
The application of regularization in MoE, with linear base
models is not new, and it has been used for dealing with high-
dimensional settings and for feature selection. For example,
[18] investigates the `1penalty and smooth clipped absolute
deviation (Scad) [19] penalties for feature selection in MoE.
In addition, [20] proposed using the `1penalty for MoE in
classiﬁcation applications. The authors in [21], [22] investigate
the theoretical aspect of MoE with `1penalties. In order
to avoid instability in the learning of gating coefﬁcients
(typically followed by a softmax function), the authors in
[23] proposed the use of a proximal-Newton expectation-
maximization (EM). The elastic-net penalty was used by the
authors of [6], who employed a regularization penalty for
the inverse of Hessian matrix along the gates learning. In
the proposed cMoE, the `1penalty is employed to promote
sparsity in the gates and experts. The solution for the gates and
experts follows from the coordinate gradient descent together
with the EM framework, as used in [6]. However, there are
two signiﬁcant differences between the method presented here
and the work of [6]. The gates and experts models are chosen
based on an estimator of the leave-one-out cross-validation
error (LOOCV). The cMoE’s performance is accessed based
on an estimated LOOCV , and this is used in the EM algorithm
as a stop condition and for model selection. Unlike [6], where
regularized Hessian can lead to unstable results, the learning
of the gates in the cMoE is based on the Newton update, with
a step-size parameter added to control the learning rate and
increase model stability.
The proposed method is evaluated in two experiments. The
ﬁrst one is the sulfur recovery unit (SRU) described in [24],
where the goal is to predict the H 2S at the SRU’s output
stream. The operators in that study are more interested in the
H2S peaks as they are related to the undesirable behavior of
SRU unit. The cMoE is then applied for predicting H 2S with
separate representations for peaks and non-peaks components,
allowing the identiﬁcation of causes of the peaks, beyond the
prediction of H 2S. The second case study investigates the
application of cMoE to estimate the acid number in a multi-
phase batch polymerization process. The process knowledge is
presented in an annotated data source for the phase transition.
The cMoE is then utilized to provide a contextual model
for each phase. In all the experiments, the results indicate
that incorporating the operator’s context into the cMoE gives
interpretability and insight to the process and signiﬁcantly
increases the model performance.
This work’s main contributions are as follows: i) the use
of possibility distributions to represent the operator’s expert
knowledge; ii) the development of a new mixture model called
contextual mixture of experts to incorporate the operator’s
expert knowledge from the possibility distributions into the
model structure; iii) the application of a `1penalty to the
gates and experts coefﬁcients to promote sparse solutions. iv)
the development of a leave-one-out error (LOOCV) estimator
for experts and gates and the cMoE model;
The paper is divided as follows. Section II gives the
background for the paper. Section III presents the proposed
model contextual mixture of experts. Section IV presentsexperimental results. Finally, Section VI gives concluding
remarks.
II. P RELIMINARIES
A. Notation
In this paper, ﬁnite random variables are represented by
capital letters and their values by the corresponding low-
ercase letters, e.g. random variable A, and corresponding
valuea. Matrices and vectors are represented by boldface
capital letters, e.g. A= [aij]Ndand boldface lowercase
letters, e.g. a= [a1;:::;ad]T, respectively. The input and
output/target variables are deﬁned as X=fX1;:::;Xdgand
Y, respectively. The variables X1;:::;Xdcan takeNdifferent
values asfxij2Xj:j= 1;:::;d andi= 1;:::;Ng, and
similarly for Yasfyi2Y:i= 1;:::;Ng.
B. Mixture of Experts
The MoE is a modeling framework based on the divide and
conquer principle. It consists of a set of experts and gates,
with the gates deﬁning the boundaries (soft boundaries) and
the experts making predictions within the region assigned by
the gates. The prediction output of a MoE with Cexperts is
^y(xi) =CX
c=1gc(xi)^yc(xi); (1)
where ^yc(xi)is the expert’s predicted output at region c,
andgc(xi)is the gate function that represents the expert’s
boundaries at region c.
The probability distribution function (PDF) of the MoE is
deﬁned as
p(yijxi; 
) =CX
c=1gc(xi;V)p(yijxi; c); (2)
The PDF is the expert cconditional distribution with mean ^yci,
and2
cis the noise variance. The set of expert parameters is
deﬁned as c=fc;2
cg. The gatesgc(xi;V)assigns mixture
proportions to the experts, with constraintsPC
c=1gc(xi) = 1
and0gc(xi)1, and for simplicity gci=gc(xi). The
gates typically follows from the softmax function:
gci= exp 
xT
ivc.CX
k=1exp 
xT
ivk
(3)
where vcis the parameter that governs the gate c, and
V=fv1;:::;vCgis the set of all gates parameters.
The collection of all parameters is deﬁned as 
 =
f1;:::; C;v1;:::;vCg.
From the MoE framework, the parameters in 
are found
trough the maximization of log-likelihood

 = arg max

L(
); (4)
where the log-likelihood for Niid samples is deﬁned as
L(
) = logp(YjX; 
) =PN
i=1logp(yijxi; 
). The solution
of Eq. (4) follows from the expectation-maximization (EM)
algorithm, an iterative procedure that maximizes the log-
likelihood from successive steps. In EM, the p(YjX; 
) is

--- PAGE 3 ---
SOUZA ET. AL: et al. : CONTEXTUAL MIXTURE OF EXPERTS: INTEGRATING KNOWLEDGE INTO PREDICTIVE MODELING 3
treated as the incomplete data distribution. The missing part,
the hidden variables Zare introduced to indicate which expert
cwas responsible to generate the sample i. The complete
distribution is given by
p(yi;zijxi; 
) =CY
c=1gc(xi;V)zcip(yijxi; c)zci(5)
wherezci2f0;1gand for each sample i, all variables zciare
zero, except for a single one. The hidden variable zciindicates
which expert cwas responsible of generating data point i.
Let^
tdenote the estimated parameters at iteration t, the EM
algorithm increases the log-likelihood at each iteration so that
LC(^
t+1)>LC(^
t). It is composed by two main steps, the
expectation (E-step) and maximization step (M-step).
From an initial guess ^
0, the expectation of the complete
log-likelihood (aka Q-function) is computed with respect to
the current estimate ^
t:
a) E-step:Qt(
) = Eh
~LC(
)jZ;^
ti
Qt(
) =NX
i=1CX
c=1t
cilogh
gc(xi;V)p(yijxi; c)i
:(6)
wheret
ciEh
zcijZ;^
ti
is the posteriori distribution of zci
after observing the data X,Y(called responsibilities). The
responsibilities accounts for the probability of expert chas
generated the sample i.
In the M-Step, the new parameters are found by maximizing
theQ-function, as the following
b) M-step:
^
t+1= arg max

Qt(
) (7)
The EM runs until convergence, typically measured by the
Q-function.
C. Possibility distribution
Possibility theory is a framework for representing uncer-
tainty, and ambiguous knowledge, from possibility distribu-
tions [25], [26]. Let 	represent a ﬁnite set of mutually
exclusive events, where the true alternative is unknown. This
lack of information on the true event resumes the uncertainty
in representing the true alternative. A possibility distribution,
deﬁned in the set 	, maps the set of possible events to the
unit interval [0;1],: 	![0;1], with at least (s) = 1
for somes2	, where the function (s)represents the state
of knowledge by the expert about the state of the data, and
(s)stands as the belief of event sbe the true alternative. The
larger(s), the more plausible, i.e. plausible the event sis. It
can also be interpreted as the “degree of belief”; for example,
the “degree of belief” of event sbe true is 0:8. It is assumed
to be possibilistic rather than probabilistic, so distinct events
may simultaneously have a degree of possibility equal to 1.
Given two possibilities distributions 1(s)and2(s), the
possibility distribution 1(s)is said to be more speciﬁc than
2(s), iff1(s)< 2(s)8s2A. Then,1is at least as re-
strictive and informative as 2. In the possibilistic framework,
extreme forms of partial knowledge can be captured, namely:
Process
knowledgeGates
Expert 2 Expert C
 Expert 1 Process data:
Operator
   Fig. 1. MoE representation with Cexperts. Solid lines indicates di-
rect data ﬂow, while dashed lines indicates ﬂow of expert knowl-
edge. The process knowledge is encoded via the possibility distributions
fcontext-1;:::; context-C g.
Complete knowledge : for somes0,(s0) = 1 , and(s) =
0;8s6=s0(onlys0is possible);
Complete ignorance :(s) = 1;8s2	(all events are
possible).
The more speciﬁc is, the more informative it is. The minimal
speciﬁcity principle drives possibility theory [27]. It states
that any now-known impossible hypothesis cannot be ruled
out. It is a principle of minimal commitment, caution, and
information. Essentially, the aim is to maximize possibility
degrees while keeping constraints in mind.
III. C ONTEXTUAL MIXTURE OF EXPERTS
In this section the contextual mixture of experts is pre-
sented. The ﬁrst subsection Sec. III-A describes the model
structure and its goals. Sec. III-B introduces the possibilities
distributions used in the expert knowledge representation. The
learning of the contextual mixture of experts is given in Sec.
III-C.
A. The Model
The structure of the contextual mixture of experts is com-
posed byCexperts, and gates models; this is represented
in Fig. 1. The context here refers to any meaningful process
data characteristic deﬁned by the analyst or derived from any
process information/knowledge. Each context cis encoded
by a possibility distribution c, which is used to represent
the expert/analyst uncertainty knowledge about the respective
context. The analyst inputs each context into the contextual
mixture of experts by incorporating the context into the model
structure and deﬁning each expert model’s expected operating
region. Then, each contextual expert model ^yc(xi)is trained
on the region of context cand makes predictions based on its
domain representation. This contextual approach enables the
prediction to be divided into different components representing
meaningful context speciﬁed by the analyst.
The output prediction of cMoE is given by a weighted
sum of the experts output, given by Eq. (1). In cMoE the
the gatesgc(xi)is the probability of sample xibelonging to
the region of context c. The expert model ^yc(xi)is trained
on the region deﬁned by the context c, and gives the predic-
tion according with its domain representation. For simpliﬁed

--- PAGE 4 ---
4 IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. X, NO. Y , MONTH YEAR
notation, the contribution of each expert model is deﬁned as
^yi=PC
c=1hciwhere the input xiis omitted for clarity, and
hci,gc(xi)^yc(xi), where ,stands for deﬁned to be. For
example, in a 3-phase batch process, the cMoE is set to have
three contexts, each one representing a phase. In such case,
the simpliﬁed representation is given by
^y=hphase1+hphase2+hphase3
From that, each contextual model can be interpreted separately,
or jointly according to the analyst’s needs.
B. Expert Process Knowledge Representation
Here, the expert’s knowledge for each context is represented
by a possibility distribution. For each context c, there is
an associated possibility distribution c(xi)(in shortci),
wherei2	(where 	represents the set of all available
samples). The value of ciindicates the degree of belief of
the sampleipertains to the region of context c. In the case
ofci= 1, sampleiis considered to be fully possible; if
ci= 0, sampleiis considered to be completely impossible
to be part of context c; any value between these two extremes
ci=p; p2]0;1[, can be accredited as partial possibility, to
a degree certainty p, of sample ibelonging to the context c.
Therefore, if ci= 1for allc= 1;:::;C , the sample iwill be
deﬁned as being believed to belong to all contextual regions
(complete ignorance about sample i), whereas if ci= 1 for
a singlec2f1;:::;Cg, the sample iwill be deﬁned as be
being accredited to to be fully certain to belong to context c
(complete knowledge about sample i), while being impossible
to belong the other contexts.
Because expert knowledge’s reliability cannot be fully guar-
anteed for each context, and because reliability is commonly
described with some degree of certainty, for example (80%
sure or 70% certain) [28, Chapter 2], the possibility distribu-
tions used here are intended to account for this uncertainty in
representing expert knowledge of each context. To do so, two
possibility distributions are employed in the experiments, the
-Certain distribution and the -Trapezoidal (fuzzy) distribu-
tion, where andare the degrees of certainty.
a)-Certain possibility distribution: is an imprecise
knowledge distribution with a certainty factor . The available
knowledge about the true alternative is expressed as a subset
A	associated with a certain level of trust 2[0;1],
concerning the occurrence of A. This can be expressed declar-
atively as “Ais certain to degree ”. This type of distribution
has been suggested as [29]:
ci=(
1 ifi2A
1 otherwise(8)
If= 1,cirepresents the characteristic function of A, on
the other extreme if = 0,cirepresents the total ignorance
aboutA. The-Certain possibility distribution is shown in
Fig. 2a.
b)-Trapezoidal epistemic possibility distribution: In
the epistemic (fuzzy like) possibility distribution, each event
has a degree of belief associated with the possibility of an
event. In the epistemic distribution, the available knowledge
/bracehtipupleft/bracehtipdownright/bracehtipdownleft /bracehtipupright
Ax1
1−βπ(x)
a b c d/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Ax1
1−απ(x)
b) a)Fig. 2. Possibilistic distributions, a) -Certain possibility distribution, b) -
Trapezoidal epistemic possibility distribution.
about the true alternative is given as a constrain deﬁned
in terms of “a fuzzy concept” deﬁned in 	. All standard
types deﬁning membership function and representing fuzzy
constraints (i.e., triangular, trapezoidal, Gaussian, etc.) can be
applied to deﬁne epistemic type possibility distributions. In
here, the trapezoidal function is deﬁned by a lower limit a, an
upper limit d, a lower support limit b, and an upper support
limitc, wherea<b<c<d :
ci= max
mini a
b a;1;d i
d c
;
(9)
To account for the unreliable, a certainty factor 2[0;1]is
added, where = 0means a fully unreliable source, and =
1means a fully reliable source. the -Trapezoidal distribution
is shown in Fig. 2b. More possibility distributions are also ﬁted
in this framework; for more possibilities distributions, see [28,
Chapter 2].
C. The Learning
The goal of cMoE is to integrate the expert knowledge
representation (via the possibility distribution) into the model
structure. To constrain the contextual information, the param-
eters in 
are found trough the maximization of the weighted
log-likelihood (WLL) of 
, deﬁned as
LC(
) =NX
i=1logp(yijxi; 
)i(10)
where i= [1i;:::;ci]Tis the contextual weight vec-
tor from the expert knowledge and must be speciﬁed a
priori . The r.h.s power is deﬁned as p(yijxi; 
)i,PC
c=1h
gc(xi;V)p(yijxi; c)ici
:The weighted ML estima-
tion of 
constrains the contextual information into the model
structure, laying down the basis of cMoE framework. The idea
is to down-weight samples that have a low degree of belief to
belong to regions of expert c.
The maximization of the WLL also LC(
)follows from the
EM algorithm. By inserting the contextual weights, the WLL
of the complete data distribution becomes
~LC(
) =NX
i=1logp(yi;zijxi; 
)i; (11)
where the power at the r.h.s is deﬁned as
p(yi;zijxi; 
)i,CY
c=1h
gc(xi;V)zcip(yijxi; c)zciici
:

--- PAGE 5 ---
SOUZA ET. AL: et al. : CONTEXTUAL MIXTURE OF EXPERTS: INTEGRATING KNOWLEDGE INTO PREDICTIVE MODELING 5
Let^
tdenote the estimated parameters at iteration tof the EM
algorithm. The expectation (E-step) and maximization steps
(M-step) for WLL are:
a) E-step: From an initial guess ^
0, the expectation of
the complete WLL (aka Q-function) is computed with respect
to the current estimate ^
t:
Qt(
) =NX
i=1CX
c=1cit
cilogh
gc(xi;V)p(yijxi; c)i
:(12)
where
t
ci=cigc(xi;Vt)p(yijxi; t
c)PC
k=1kigk(xi;Vt)p(yijxi; t
k)
t
ciare the responsibilities of expert cgenerated sample i. It
should be noted that the responsibilities are also a result of the
contextual weights, in the case of ci= 0, the responsibility
of expertcist
ci= 0, indicating that context chas no role in
generating the sample i.
b) M-step: In the M-Step, the new parameters are found
by maximizing the Q-function, as the following
^
t+1= arg max

Qt(
) (13)
TheQ-function is further decomposed to account for the gates
and experts contributions separately, as Qt(
) =Qt
g(V) +
Qt
e(), where
Qt
g(V) =NX
i=1CX
c=1cit
ciloggp(xi;V)
Qt
e() =NX
i=1CX
c=1cit
cilogp(yijxi; c)
The maximization is performed separately for the experts and
gates in the updating phase. Here, the experts and gates models
are linear, despite more complex models being allowed and
easily integrated into this framework.
D. Experts Learning
In the maximization step, the updated experts parameters
are found from the maximization of Qt
e(). The contribution
of the individual experts can be accounted separately:
Qt
e() =CX
c=1Qt
ec(c) =CX
c=1NX
i=1cit
cilogN 
yijxT
ic;2
c
(14)
whereQt
ec()accounts for the contribution of expert c. Hence,
the parameters of expert ccan be updated apart from the
other experts. Equivalently, instead of maximizing Qt
ec(c),
the updated coefﬁcient is found by minimizing the negative of
Qt
ec(c), as
^t+1
c= arg min
c 
1
2NX
i=1cit
ci 
yi xT
ic2!
: (15)
To promote the sparsity of expert ccoefﬁcients, a `1penalty
is added to Eq. (15), leading to
^t+1
c= arg min
c 
1
2NX
i=1cit
ci
yi xT
ic2
+e
cdX
j=1jcjj!
;
(16)wheree
ccontrols the importance of the regularization penalty.
This penalty, also known as least absolute shrinkage and selec-
tion operator (LASSO), drives irrelevant features coefﬁcients
towards zero. This characteristic is suitable for industrial ap-
plications where not all variables are relevant to the prediction,
providing compact models. Under the cMoE, this penalty
will allow the selection of parsimonious models for each
expert, reducing the complexity of the overall model structure.
Together with the contextual information, the LASSO penalty
will provide a relevant set of features for each expert domain,
thus allowing a better interpretation of the model represen-
tation, as well allowing the learning in scenarios with small
number of samples and many features.
The minimization of Eq. (16) will follow from the co-
ordinate gradient descent (CGD). In CGD, each coefﬁcient
is minimized individually at a time. The updated regression
coefﬁcient of variable jand expertcis given as
^t+1
cj=S NX
i=1cit
cixij(yi ~yj
ci);e
c!.NX
i=1cit
cix2
ij;
(17)
where ~yj
ci=Pd
l6=jxilt
clis the ﬁtted value of local expert c,
without the contribution of variable jand theS(z;)is the
soft threshold operator, given by S(z;) =sign(z)(jzj )+.
From Eq. (17) the contextual weight adds a weighting factor
over the responsibilities. In the case where ci= 1 for all
experts, the responsibility will be the primary driving force in
determining the contribution for that speciﬁc sample.
1) Experts Model Selection: The selection of LASSO reg-
ularization can follow from the k-fold cross-validation ( k-CV)
error. However, the k-fold cross-validation for the LASSO
may present potential bias issues in high-dimensional settings.
The reason for bias is that the data change caused by the
splitting into folds affects the results. Allowing kto be
large enough to reduce the bias is one possible solution.
Choosingk=N, for example, results in the leave-one-out
cross-validation (LOOCV) error, an unbiased estimator for the
LASSO error. However, the computation of LOOCV is heavy
as it requires training the model Ntimes, and one solution
is to approximate the LOOCV from the data. Under mild
conditions assumptions [30], the prediction LOOCV of linear
models with LASSO penalty can be approximated from its
active set; the active set is the index of those variables with
nonzero coefﬁcients.
Let the active set of the expert cbeEc=fj2
f1;:::;dgjcj6= 0g. Also, let XEcfor the columns of matrix
Xin the setEc.
The LOOCV of expert ccan be approximated by
CVe
c(e
p) =NX
i=1cit
ci
yi ^y( i)
ci2
(18)
where ^y( i)
ci is the estimated output of expert cwithout sample
i. The estimation of ^y i
cifrom the active set is given by
^y( i)
ci=(^yci [Hc]iiyi)
1 [Hc]ii: (19)

--- PAGE 6 ---
6 IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. X, NO. Y , MONTH YEAR
where [Hc]iiare theith diagonal element of hat-matrix of
expertc, which is deﬁned as
Hc=XEc(XT
Ec cXEc) 1XT
Ec c
where  c=diag(c1t
c1;:::;cnt
cn)is the diagonal matrix
of contextual weights and responsibilities, and XEcis the ac-
tive set subset of matrix X. In here, the inverse (XT
Ec cXEc) 1
is computed via the LU decomposition. If this is not possible,
e.g. due the matrix becoming too large, one could estimate the
validation error from an independent validation set. The value
ofe
pselected is the one that minimizes the value of CVe
M.
Note that this estimator resembles the predicted residual sum
of squares (PRESS), except that only the active set is used
along with the LOOCV estimation for the LASSO.
E. Gates Learning
For the gates, the new updated parameters results from
the maximization of Qt
g(V), or equivalently by minimizing
 Qt
g(V). By expanding the gates contribution, it becomes
Qt
g(V) =NX
i=1"CX
c=1cit
cixT
ivc ilog CX
k=1exp 
xT
ivk!#
;
(20)
wherei=PC
c=1cit
ci. The function Qt
g(V)is concave in
the parameters, and its maximization of Qt
g(V)will follow
the Newton’s method. Let f^vt
cgC
c=1be the current estimates
of gates coefﬁcients, the second-order (quadratic) Taylor ap-
proximation of Eq. (20) around f^vt
cgC
c=1is:
~Qt
g(V) =CX
c=1Qt
gc(vc) +C(f^vt
cgC
c=1) (21)
Qt
gc(vc) = 1
2NX
i=1rci 
zci xT
ivc2; (22)
where ~Qt
g(V)is the second order Taylor approximation of
Qt
g(V),Qt
gc(vc)accounts by the individual contribution of
gatec, andC(fvt
cgC
c=1)is a constant term, while rciandzci
are given by
rci=igci(1 gci); (23)
zci=xT
ivt
c+cit
ci igci
igci(1 gci); (24)
with the gates gcicomputed from Eq. (3), and the parameter
is a magic parameter added to control the Newton update
on the optimization phase.
By adding the LASSO penalty to the gates contribution Eq.
(22), the new gate coefﬁcients are found as the solution of the
following minimization problem
^vt+1
c= arg min
vc2
41
2NX
i=1rci 
zci xT
ivc2+g
cdX
j=1jvcjj3
5:
The gate coefﬁcients are updated from successive local New-
ton steps. It cycles trough all Cgates sequentially, where the
values ofgciare calculated from f^vt
cgC
c=1, and they must
be updated as soon a new ^vt
cis computed. The computationof Eq. (25) must be repeated until the coefﬁcients converge;
usually, few iterations (less than 10) are needed to the reach
convergence.
The solution of Eq. (25) is achieved from the CGD, in which
^vt+1
pj=S NX
i=1rcixij(zci ~zj
ci);g
c!.NX
i=1rcix2
ij (25)
where ~zj
ci=Pd
l6=jzilvt
clis the ﬁtted value of gate c, without
considering variable j.
a) Practicalities in gates update: Along the gates coef-
ﬁcients update, some practical issues must be taken
Care should be taken in the update of Eq. (24), to avoid
coefﬁcients diverging in order to achieve ﬁtted gciof0
or1. Whengciis within= 10 3of1, Eq. (24) is set
tozci=xT
ivt
c, and and the weights in (23) rciare set to
.0is treated similarly.
The use of full Newton step = 1 in optimization Eq.
(25) do not guarantee the converge of coefﬁcients [31].
To avoid this issue, along the experiments was ﬁxed to
= 0:1.
1) Gates Model Selection: Similar to the experts’ proce-
dure, the gates model selection will follow from the estimated
LOOCV error. The predicted gate output without the sample
iis given by
^z( i)
ci= (^zci+ [Mc]iizi).
(1 [Mc]ii) (26)
where Mcis the gate hat matrix at each step of the Newton
update, and is computed as
Mc=XGc(XT
GcRcXGc) 1XT
GcRc (27)
whereGc=fj2f1;:::;Ngjvcj6= 0gis the active set of
gatec, and Rc=diag(rp1;:::;rpn). The LOOCV is then
estimated as
CVg
c(g
c) =NX
i=1rci
zi ^z( i)
ci2
(28)
The gate regularization parameter is selected to minimize the
estimated LOOCV error.
F . EM Stop Condition
In cMoE, the information on the number of experts must
be known a priori , or can be deﬁned by the analyst, so this
is not an issue for the design. The EM algorithm’s condition
stops must be properly deﬁned to avoid overﬁtting or a poorly
chosen model. Because the implementation is based on a set
of linear models, an approximation of LOOCV error is used
to assess the model’s quality and set the EM algorithm’s stop
criteria. The estimated LOOCV for cMoE is given by
CV(
) =1
NNX
i=1 
yi CX
c=1g( i)
ci^y( i)
ci!2
(29)
where ^y( i)
ci is given by Eq. (19) and g( i)
ci is given by
g( i)
ci= exp
z( i)
ci.CX
k=1exp
^z( i)
ki
: (30)

--- PAGE 7 ---
SOUZA ET. AL: et al. : CONTEXTUAL MIXTURE OF EXPERTS: INTEGRATING KNOWLEDGE INTO PREDICTIVE MODELING 7
 
Complete ignorance
region (common
contextual knowledge)Fitted contextual
domain boundaries
by cMoE01
samplePhase 1 Phase 2 Phase 3 
01
samplePhase 1 Phase 2 Phase 3Learning
Fig. 3. Representation of contextual information with uncertainty (left), and
ﬁtted contextual information with cMoE (right).
The performance of cMoE is checked at each iteration by
computing CV (
); it is expected that the estimated LOOCV
CV(
) decreases until a minimum, before beginning to in-
crease continuously. This minimum is found by checking
whether for nititerations, the CV (
) kept only increasing.
If so, the cMoE of nititerations back is considered to
have a global minimum error and is selected as optimized
model, and the algorithm is terminated. The cMoE algorithm
should terminate after nititerations if the error continually
increases, counting from the iteration where CV (
)reaching
its minimum. In the experiments, a value of nit= 6 was
considered.
G. Evaluation of Expert Process Knowledge Integration
In the cMoE model, each gate gcshould reﬂect the knowl-
edge of each context represented by c, and in the prediction
stage, the gates are responsible for automatically determining
which context the process is running in and switching to
the appropriate expert model. In fact, the gcis a probability
counterpart of the possibility distribution c. The weakest
consistency principle [25] leads to a sufﬁcient condition for
checking the consistency between the gate’s probability func-
tiongcand the possibility distribution c. It states that a
probable occurrence must also be possible to some extent, at
least to the same degree. The following inequality can formally
express this:
gci<ci;8i2	 (31)
The possibility distribution is a upper bound for the probability
distribution [29]. Each context possibility distribution cre-
ﬂects the knowledge expert’s uncertainty in a quasi-qualitative
manner that is less restrictive than probability gc. This is
visually represented in Fig. 3, for a hypothetical three phases
process. The left picture shows the contextual information
provided by the analyst, with a complete ignorance region
(common contextual knowledge). The right picture shows the
ﬁtted contextual information with well-deﬁned boundaries,
represented as the gate’s output. In the speciﬁc case where
context information follows the complete ignorance possibility
distributions ( ci= 1;forc= 1;:::;C; andi= 1;:::;N ),
the cMoE reduces to MoE.
To access if the assignment of context cis correct, the
following consistency index of context c(CI;c) is deﬁned:
CI;c=1
NNX
i=1I(gci;ci) (32)
whereI(a;b) = 1 iffab, and 0otherwise. This consistency
index measures the accuracy of the gates’ agreement of contextcwith respect to the consistency principle. To measure the con-
sistency of all contexts in representing the expert knowledge,
the following geometric mean is employed
CI= CY
c=1CI;c!1=C
(33)
IfCI= 1 indicates complete agreement, CI= 0 indicates
complete disagreement, indicating an inability of cMoE to
incorporate the expert knowledge into its structure. In such
cases, the uncertainty in the expert knowledge from the
possibility distributions can also be re-tuned, for example in
-Certain distribution, the certainty parameter can be tuned
from automatic methods. In this case, the should be chosen
using the minimal speciﬁcity principle, i.e. search for the most
informative distribution (lowest ), while keeping a desired
consistency index. This can be stated as
= inff2[0;1] :CI()<g: (34)
where 01is the minimum desired consistency index.
The same reasoning can be applied to the -Trapezoidal
possibility distribution.
IV. E XPERIMENTAL RESULTS
This section presents the experimental results in two indus-
trial case studies. The ﬁrst case study deals with the estimation
H2S at the output stream of a sulfur recover unit described in
[24]. The second case study predicts the acidity number in a
multiphase polymerization reactor.
a) State of art models: The following models were also
implemented along with the experiments for performance
comparison purposes. The MoLE with LASSO penalty [6],
the LASSO regression model, PLS regression model, Gaussian
mixture regression model (GMR), decision tree (TREE), and
the optimally pruned extreme learning machine model (ELM)
[32]. The MoLE and LASSO source code are based on the
MoLE Toolbox available at [6]. The PLS is based on the
author’s own implementation. The GMR implementation is
based on the Netlab Toolbox available at [33]. The TREE
is based on the Matlab implementation of the Statistics and
Machine Learning Toolbox. The ELM were implemented from
the author’s source code, available at [34].
b) Hyper-parameters Selection: The selection of MoLE
and LASSO regularization parameters follows the predicted
LOOCV error described in Sec. III-D1; the LASSO parameter
is denoted as , while the regularization parameters of MoLE
is deﬁned as e
p;g
p, whereas the e;gsuperscript denotes the
expert and gates parameters, respectively, while prefers to
the expert/gate number. The selection of PLS latent variables
Nlatfollows from a 10-fold CV error on the training data.
The selection of the hidden neurons Nneuin the ELM model
follows from the optimization procedure described in [32].
The number of components Ncin the GMR is set equal to
the number of contexts. For the tree, the minimum number of
leaf node observations Nleaf, was set to be Nleaf for both
experiments.

--- PAGE 8 ---
8 IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. X, NO. Y , MONTH YEAR
c) Experimental settings: The predictive performance
is accessed by the root mean square error (RMSE), the
coefﬁcient of determination (R2), and the maximum absolute
error (MAE). The results of the second case study have
been accessed by following a leave-one-batch-out procedure.
The models were trained from all batches except one (to
be used as a test). This procedure was repeated so that all
the batches were used in the testing phase. The performance
metrics were averaged and then reported as the ﬁnal values.
Also, a randomization t-test (from [35]) was used to compare
the cMoE performance (the null hypothesis assumes that the
RMSE of cMoE and the method to be compared are equal; i.e.
equal mean), the p-value is then reported, if p-value<0:05the
null hypothesis is rejected. Along with the training procedure,
the data for the training data was auto-scaled, and the testing
data were re-scaled according to the training parameters (mean
and variance).
A. SRU Unit
The sulfur recovery unit (SRU) unit aims to remove pol-
lutants and recover sulfur from acid gas streams. The SRU
plant takes two acid gas as input, the ﬁrst (MEA gas), from
gas washing plants, rich in H 2S, and the gas from sour water
stripping plants (SWS gas), rich in H 2S and NH 3(ammonia).
The acid gases are burned into reactors, where H 2S is trans-
formed into sulfur after oxidation reaction with air. Gaseous
products from the reaction are cooled down, collected, and
further processed, leading to the formation of water and sulfur.
The remaining gas non-converted to sulfur (less than 5%) is
further processed for a ﬁnal conversion phase. The ﬁnal gas
stream contains H 2S and SO 2, and online analyzers measure
these quantities. The goal is to use the process data to build
a soft sensor to replace the online analyzers when under
maintenance.
Five main variables are collected, X1is the gas ﬂow in MEA
zone,X2is the air ﬂow in MEA zone to the combustion of
MEA gas (set manually by the operators), X3is the airﬂow in
MEA zone regulated by an automatic control loop according
to the output stream gas composition, X4the airﬂow in SWS
zone (set manually by operators), and X5the gas ﬂow in SWS
zone. The target/output is set here to be the H 2S at the end-
tail; the SO 2can also be predicted using the same principle
presented here. Also, according to [24] the operators are more
interested in the models that can predict the H 2S peaks.
There is a total of 10000 samples available. The ﬁrst 5000
were used for training, and the remaining 5000 for test; Fig.
4(a) shows the training data for the SRU dataset, the peaks are
clearly visible. Time-lagged features were designed to account
for the process dynamics. Then, for the ﬁve variables, the time
lags ofXi;t d, for variables i= 1;:::; 5and delays d=
f0;5;7;9gwere considered, resulting in a total of 20 features.
To predict the H 2S, and allow a better understanding of the
causes of the peaks, a cMoE model with three contexts was
designed. The ﬁrst, representing the operator context, accounts
for the peaks. The second context is designed to represent the
non-peaks. The third context represents the remaining process
0 2,500 5,00000.51
SampleH2SSRU train data(a)
00.10.20.30.40.50.60.70.80.9100.20.50.81
αCISRU -αvsCIand LOOCV
12345
LOOCVCI
LOOCV (b)
300 370 400 460 490 58000.30.51
SampleContext weightsSRU contextual information in training data
πpeak
πnpeak
πrem
Ref.
(c)
4,600 4,620 4,640 4,660 4,70000.51
SampleGates outputSRU gates output in test data
gpeak
gnpeak
grem
Ref.
Pred. (d)
X1,t−9
X2,t−9
X3,t−9
X4,t−9
X5,t−9
X1,t−7
X2,t−7
X3,t−7
X4,t−7
X5,t−7
X1,t−5
X2,t−5
X3,t−5
X4,t−5
X5,t−5
X1,t
X2,t
X3,t
X4,t
X5,t−4−2024Gates CoeﬃcientsSRU gates coeﬃcients
Peak
nPeak
(e)
4,600 4,620 4,640 4,670 4,70000.51
SampleX3,tSRU Inﬂuence of variable X3,tin H 2S
gpeak Ref.
X3,t (f)
Fig. 4. SRU dataset (a) training data, (b) , vs consistency index and LOOCV ,
(c) contextual information set in training data, and (d) gates output prediction
in test data, (e) gates coefﬁcients for peaks and npeaks contexts, (f) effect of
variableX3;tin H 2S output.
states that are not accounted for the peaks and non-peaks. The
cMoE model is represented by
^y=hpeak+hnpeak+hrem;
where “npeak” is a typo to non-peaks. Two -Trapezoidal
possibility distributions were designed for peaks context peak,
and non-peaks context npeak, while for the remaining context
rem, a complete ignorance distribution is assumed, i.e. rem=
1;8i2f1;:::; 5000g, as there are no information about the
context. The peaks were selected manually in the training set,
and the peak distribution was designed so that the limits of the
trapezoidal function were deﬁned to guarantee that the highest
values have peaks = 1 peaks. The possibility distribution
for the non-peaks was designed to be complementary to the
context of the peak, with a lower bound deﬁned by the
certainty. A portion of the expert knowledge feed to cMoE
is depicted in Fig. 4(c). From sample 370 400a peak in H 2S
is present (as Ref.). peaks (see Fig. 4(c) samples 370 400, and
460 490). The context of non-peak ( npeak ) was designed
to be complementary to the peaks context. The remaining
context (rem=1) for all samples. The uncertainty was
chosen using the consistency principle described in Eq. (34).
The consistency index and the LOOCV , for different values
of the uncertainty parameter , are shown in Fig. 4(b). The
results show that = 0:3, the selected uncertainty factor, has
the lowest LOOCV , with a consistency index of CI= 0:79.
Figure 4(d), shows the gates output prediction for the peaks
(Gpeak), non-peaks (G npeak ) and remaining (G rem) contexts

--- PAGE 9 ---
SOUZA ET. AL: et al. : CONTEXTUAL MIXTURE OF EXPERTS: INTEGRATING KNOWLEDGE INTO PREDICTIVE MODELING 9
TABLE I
H2SPREDICTION ACCURACY FOR ALL COMPARED MODELS
H2S
cMoE MoLE LASSO PLS GMR TREE ELM
R20.732 0.583 0.085 0.541 0.519 0.001 0.002
RMSE 0.026 0.035 0.053 0.035 0.032 0.087 0.060
MAE 0.340 0.484 0.630 0.519 0.401 0.603 0.662
p-value 1.000 0.001 0.001 0.001 0.001 0.001 0.001
in the test set, for samples 4600 4700 . There are two peaks
in this portion of the test data, between samples 4620 4640 ,
and4640 4660 . The gates of peaks expert G peak follow
the peak pattern by assigning higher contributions to the peak
expert model when the peaks are present. The same behavior
is perceived in the non-peaks gates G npeak , which seems to
work complementary to the peaks component. The remaining
context gates, G rem, seem somewhat oscillating between the
patterns; this seems to be related to a constant operation of
the system. The gates coefﬁcients allow identifying the root
causes of change between the peaks and non-peaks. The gates
coefﬁcients for the peaks, and non-peaks is shown in Fig. 4(e).
The variable X3(marked with a rectangle dashed line) has the
largest difference between the two gates, indicating this is the
main variable that acts on the peaks and non-peaks model
switching. Figure 4(f) shows the variable X3, together with
the H 2S peaks.X3represented the input airﬂow to control
the end tail H 2S. Thus H 2S is a consequence of X3. It seems
that the control system is unstable, and any oscillation in X3
causes a large oscillation in the H 2S. One possible solution to
improve the stability of H 2S and reduce peaks is to improve the
control system. Improvements to the control system can have
a positive environmental impact by lowering H 2S emissions
and/or reducing costs associated with H 2S post-treatment.
The accuracy of the cMoE was compared with the other
state of the art models, and the results are shown in Table
I. The results show that the cMoE outperforms all the other
models with statistical signiﬁcance. Results conﬁrm that con-
straining the model to represent the system’s expected behavior
positively impacts the prediction performance. Table II shows
the parameters obtained for each model in the H 2S experiment.
B. Polymerization
This case study refers to a polymerization batch process for
resin production. The material is loaded into the reactor, which
then undergoes the ﬁve process phases: heating, pressure,
reaction, vacuum, and cooling; most of the phase changes
are triggered manually by the operators. The phase change
is determined by the quality values of the resin, namely the
resin acidity number and the resin viscosity. While a physical
sensor measures the viscosity, the acidity number is measured
three times, one at the vacuum phase and two at the reaction
phase. The objective here is to build a soft sensor to measure
the acidity number online and better understand the variables
that affect the acidity in that two phases.
For this process, there are data for 33batches in the
speciﬁcation, with a total of 17variables measured along the
process; they are described in Table III. As there are three
acidity measurements for each batch, a total of 99samples
0 65110 320 495−100.412
SampleAcidity numberPolymerization - reference and cMoLE prediction
πreaction πvaccum
Ref. Pred.(a)
0 110 49500.20.40.50.81
SampleGates output & context weightsPolymerization - cMoLE Gates prediction
πreaction πvaccum
greaction gvaccum (b)
00.10.20.30.40.50.60.70.80.9100.20.50.81
αCIPolymerization - αvsCIand LOOCV
1234
LOOCVCI
LOOCV
(c)
X1
X2
X3
X4
X5
X6
X7
X8
X9
X10
X11
X12
X13
X14
X15
X16
X17−0.500.50.75Experts CoeﬃcientsPolymerization - Experts Coeﬃcients
Reaction
Vacuum (d)
X1
X2
X3
X4
X5
X6
X7
X8
X9
X10
X11
X12
X13
X14
X15
X16
X17−0.4−0.200.20.4Gates coeﬃcientsPolymerization - Gates Coeﬃcients
Reaction
Vacuum
(e)
0 100 200 300 400 500−2−1012
SampleX2Polymerization - Proﬁle of variable X2
πreaction πvacuum
X2 Ref. (f)
Fig. 5. Polymerization dataset, a) context for reaction and vacuum phases,
together with acidity and prediction by cMoE, b) cMoE gates output, c) , vs
consistency index and LOOCV , d) reaction and vacuum experts coefﬁcients,
e) reaction and vacuum gates coefﬁcients, f) variable X2(temperature oil
return)
is available. The process variables are synchronized with the
acidity number by removing the samples that do not have the
corresponding acidity number values.
Then, a cMoE model with two contexts was designed to
predict acidity and understand the variables that mostly affect
the acidity number. The ﬁrst context represents the reaction
phase, and the second to the vacuum phase. The cMoE model
for acidity prediction is represented by
^y=hreaction +hvacuum
The process knowledge is available as the phase changes
from the operators, between the reaction and vacuum phases.
In this case, the -Certain distribution was designed to repre-
sent the operator’s context of the phases. This is depicted in
Fig. 5(a) for a single batch. There, the two contexts ( reaction
andvacuum ) taken by operators indicate the region of samples
belonging to each phase; the change between phases occurs
at sample 110. The acidity number is also indicated as ‘Ref.’,
measured at samples 65,320, and 495. The uncertainty was
chosen using the consistency principle described in Eq. (34).
The consistency index and the LOOCV , for different values of
the uncertainty parameter , are shown in Fig. 5(c). The results
show that= 0:4has the lowest LOOCV , with a consistency
index ofCI= 0:99. It is worth noting that the LOOCV error is
signiﬁcantly higher for = 0(no uncertainty) when compared
to higher uncertainty >0, indicating that uncertainty plays
a signiﬁcant role in representing process expert knowledge.

--- PAGE 10 ---
10 IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. X, NO. Y , MONTH YEAR
TABLE II
HYPER -PARAMETERS OF THE FITTED H2SMODELS
H2S
cMoE MoLE LASSO PLS GMR TREE ELM
e
peak= 0:00,g
peak= 0:00e
peak= 1:00,g
peak= 1:049
= 1:00Nlat= 16Nc= 3Nleaf = 1Nneu = 160 e
npeak= 1:00,g
npeak= 3:34e
npeak= 12:61,e
npeak= 1:31
e
rem= 1:00,g
rem= 1:00e
rem= 12:83,e
rem= 0:00
TABLE III
VARIABLES OF THE POLYMERIZATION UNIT .
Variable Description Variable Description
X1 Reactor temperature; X 10 Reactor pressure 2;
X2 Temperature oil return; X 11 Pressure reﬂux;
X3 Temperature gas return; X 12 Vacuum presure;
X4 Temperature reﬂux pump; X 13 Flow reﬂux;
X5 Condensator cooling temperature; X 14 Flow oil;
X6 Column temperature; X 15 Lever water;
X7 Reactor temperature mixture; X 16 Level solvent;
X8 Temperature thermal oil ; X 17 Viscosity;
X9 Reactor pressure 1; Y Acidity number.
TABLE IV
ACIDITY PREDICTION ACCURACY FOR ALL COMPARED MODELS
Acidity
cMoE MoLE LASSO PLS GMR TREE ELM
R20.996 0.995 0.995 0.994 0.996 0.994 0.812
RMSE 0.092 0.101 0.101 0.109 0.096 0.120 0.500
MAE 0.134 0.155 0.155 0.166 0.145 0.174 0.768
p-value 1.000 0.342 0.350 0.010 0.669 0.028 0.001
The predictive performance from the leave-one-batch-out
procedure, for the all models compared, is shown in Table
V, in terms of R2, RMSE and MAE. Table V shows the
parameters obtained for ﬁtted models for the ﬁrst fold of
the leave-one-batch-out procedure. The cMoE is statistically
different at a 0:05signiﬁcance level than PLS, TREE, and
ELM and has superior performance to all other models. Also,
when inspecting the gate’s output provided by cMoE, the
cMoE model signiﬁcantly retained the representation of the
initial contextual information and detected the change between
phases as shown by the gate’s output in Figs. 5(b).
The cMoE coefﬁcients for the reaction, and vacuum experts
are shown in Fig. 5(d). The reaction expert has a signiﬁcant
representation of the reaction’s phase of the polymerization
unit. The main variables which were important for the reaction
expert are the oil temperature (X 2, X3), reactor mixture
temperature (X 7), condenser temperature (X 5) and the liquid
viscosity (X 17). The vacuum expert represents a more signif-
icant portion of the vacuum of the polymerization unit. The
most signiﬁcant variable is the viscosity (X 17), as the viscosity
is physically a function of pressure. It is also important on the
gas return temperature (X 3), reactor temperature, and pressure
(X7, X9). These variables are physically related to the equation
of states of the gaseous product within the reactor.
To better understand the phases transition Fig. 5(e) shows
the gates coefﬁcients for the reaction and vacuum contexts.
From there, variable X2, the temperature of oil return, mostly
affects the transition to phases. The variable X2is plotted
in Fig. 5(f). From there, it is possible to check that there is
an intermediate step in which oil temperature drops up to a
minimum before the operator starts the vacuum phase. Theprocess status again, reaching normalization in sample 200.
Also, the oil temperature has two different regimes in the
reaction and vacuum phases.
V. D ISCUSSION
The proposed cMoE model uses possibility distributions to
represent contextual information provided by process operators
and to integrate this expert knowledge into the cMoE model
via the data using the learning procedure. In addition, to assess
how well expert knowledge was integrated into the cMoE
model, a consistency index was deﬁned in Eq. (33). The ﬁrst
case study, a continuous process, was broken down into three
contexts, with the peaks and non-peaks contexts being the most
relevant to operators. As a result, important information and
insights on the status of the control system were obtained by
inspecting the main variables causing the transition between
the peaks and non-peaks contexts. In the second case study, a
multiphase batch process, the information on phase transitions
was the knowledge to be integrated into the model. As a result,
more insights into phase transitions and the impact of each
variable on each phase were realized.
It is worth noting that the linear models in cMoE are
sufﬁcient to integrate the expert knowledge in both case
studies; also worth mentioning that the uncertainty parameters
in both cases studies were reﬁned from the data, together with
an analysis of the consistency index. In cases where the con-
sistency index performs poorly, or one wants to employ a more
informative distribution (i.e., lower values of uncertainty), non-
linear models for gates and experts may be employed as an
alternative to linear models so that the consistency index’s
performance is improved. It would be expected that non-linear
modeling must capture the non-linear behavior of the data that
must be relevant in integrating expert knowledge. Furthermore,
the`1penalty is not required for the linear solution of cMoE;
other penalties, such as `2or`1;2, can also be employed. In
the case of data collinearity, the solution can be obtained by
applying the PLS model to experts and gates, as demonstrated
in [17].
Compared to other models that are interpretable by nature,
such as the Lasso, PLS, DT, and GMR, they lack mechanisms
to integrate expert knowledge. Of course, meaningful relations
and rules can be extracted from these models, but this is still
driven by the data, and if no context is provided, the extraction
of relevant information for more complex relationships, as
in the two case studies presented here, is not possible. The
contextual framework presented here is ﬂexible enough to
allow its implementation in many models, including the GMR
model.

--- PAGE 11 ---
SOUZA ET. AL: et al. : CONTEXTUAL MIXTURE OF EXPERTS: INTEGRATING KNOWLEDGE INTO PREDICTIVE MODELING 11
TABLE V
HYPER -PARAMETERS OF THE FITTED ACIDITY MODELS
Acidity
cMoE MoLE LASSO PLS GMR TREE ELM
e
reaction= 1:00,g
reaction= 1:15e
reaction= 16:90,g
reaction= 0= 31:90Nlat= 5Nc= 2Nleaf = 1Nneu = 106e
vacuum = 21:92,g
vacuum = 2:36e
vacuum = 16:90,e
vacuum = 0
VI. C ONCLUSIONS
In conclusion, this paper proposes the contextual mixture
of experts model, a data-driven model devised to incorporate
operator domain knowledge into its structure. The proposed
approach has been shown to increase predictive performance
while achieving a direct interpretation of process variable
contribution in each regime of the process. This approach
was evaluated on two different problems, demonstrating better
statistical performance than conventional machine learning
models that do not rely on contextual information. The pro-
posed method has strong potential as a stable and explain-
able framework to include contextual information in data-
driven modeling. This is important to help the transition to
Industry 5.0 by increasing the human-machine synergy in the
process industry. Future research could concentrate on non-
linear functions for experts and gates learning to improve
predictive performance, as well as explainable methods for
interpretability.
REFERENCES
[1] S. Nahavandi, “Industry 5.0 – a human–centric solution,” Sustainability ,
vol. 11, no. 16, 2019.
[2] M. S. Reis, G. Gins, and T. J. Rato, “Incorporation of process-speciﬁc
structure in statistical process monitoring: A review,” Journal of Quality
Technology , vol. 51, no. 4, pp. 407–421, 2019.
[3] J. Sansana, M. N. Joswiak, I. Castillo, Z. Wang, R. Rendall, L. H.
Chiang, and M. S. Reis, “Recent trends on hybrid modeling for industry
4.0,” Computers & Chemical Engineering , vol. 151, p. 107365, 2021.
[4] L. von Rueden, S. Mayer, K. Beckh, B. Georgiev, S. Giesselbach,
R. Heese, B. Kirsch, M. Walczak, J. Pfrommer, A. Pick, R. Ramamurthy,
J. Garcke, C. Bauckhage, and J. Schuecker, “Informed machine learning
- a taxonomy and survey of integrating prior knowledge into learning
systems,” IEEE Transactions on Knowledge and Data Engineering , pp.
1–1, 2021.
[5] K. Wang, R. B. Gopaluni, J. Chen, and Z. Song, “Deep learning of
complex batch process data and its application on quality prediction,”
IEEE Transactions on Industrial Informatics , vol. 16, no. 12, pp. 7233–
7242, 2020.
[6] F. Souza, J. Mendes, and R. Araújo, “A regularized mixture of linear
experts for quality prediction in multimode and multiphase industrial
processes,” Applied Sciences , vol. 11, no. 5, 2021.
[7] P. Facco, F. Bezzo, and M. Barolo, “Nearest-neighbor method for the
automatic maintenance of multivariate statistical soft sensors in batch
processing,” Industrial & Engineering Chemistry Research , vol. 49,
no. 5, pp. 2336–2347, 2010.
[8] L. Zhao, C. Zhao, and F. Gao, “Between-mode quality analysis based
multimode batch process quality prediction,” Industrial & Engineering
Chemistry Research , vol. 53, no. 40, pp. 15 629–15 638, 2014.
[9] Y . He, B. Zhu, C. Liu, and J. Zeng, “Quality-related locally weighted
non-gaussian regression based soft sensing for multimode processes,”
Industrial & Engineering Chemistry Research , vol. 57, no. 51, pp.
17 452–17 461, 2018.
[10] X. Shi, Q. Kang, M. Zhou, A. Abusorrah, and J. An, “Soft sensing of
nonlinear and multimode processes based on semi-supervised weighted
gaussian regression,” IEEE Sensors Journal , vol. 20, no. 21, pp. 12 950–
12 960, 2020.
[11] L. Luo, S. Bao, J. Mao, D. Tang, and Z. Gao, “Fuzzy phase partition
and hybrid modeling based quality prediction and process monitoring
methods for multiphase batch processes,” Industrial & Engineering
Chemistry Research , vol. 55, no. 14, pp. 4045–4058, 2016.[12] J. Yu and S. J. Qin, “Multiway gaussian mixture model based multi-
phase batch process monitoring,” Industrial & Engineering Chemistry
Research , vol. 48, no. 18, pp. 8585–8594, 2009.
[13] W. Shao, Z. Ge, Z. Song, and J. Wang, “Semisupervised robust mod-
eling of multimode industrial processes for quality variable prediction
based on student’s tmixture model,” IEEE Transactions on Industrial
Informatics , vol. 16, no. 5, pp. 2965–2976, 2020.
[14] B. Wang, Z. Li, Z. Dai, N. Lawrence, and X. Yan, “Data-driven mode
identiﬁcation and unsupervised fault detection for nonlinear multimode
processes,” IEEE Transactions on Industrial Informatics , vol. 16, no. 6,
pp. 3651–3661, 2020.
[15] W. Shao, Z. Ge, and Z. Song, “Soft-sensor development for processes
with multiple operating modes based on semi-supervised gaussian mix-
ture regression,” IEEE Transactions on Control Systems Technology , pp.
1–13, 2018.
[16] J. Wang, W. Shao, and Z. Song, “Student’s-t mixture regression-based
robust soft sensor development for multimode industrial processes,”
Sensors , vol. 18, no. 11, 2018.
[17] F. A. A. Souza and R. Araújo, “Mixture of partial least squares experts
and application in prediction settings with multiple operating modes,”
Chemometrics and Intelligent Laboratory Systems , vol. 130, pp. 192–
202, January 2014.
[18] A. Khalili, “New estimation and feature selection methods in mixture-
of-experts models,” Canadian Journal of Statistics , vol. 38, no. 4, pp.
519–539, 2010.
[19] J. Fan and R. Li, “Variable selection via nonconcave penalized like-
lihood and its oracle properties,” Journal of the American Statistical
Association , vol. 96, no. 456, pp. 1348–1360, 2001.
[20] B. Peralta and A. Soto, “Embedded local feature selection within mixture
of experts,” Information Sciences , vol. 269, pp. 176 – 187, 2014.
[21] F. Chamroukhi and B. T. Huynh, “Regularized maximum-likelihood
estimation of mixture-of-experts for regression and clustering,” in The
International Joint Conference on Neural Networks (IJCNN) , Rio,
Brazil, July 2018.
[22] T. Nguyen, H. D. Nguyen, F. Chamroukhi, and G. J. McLachlan, “An l1-
oracle inequality for the lasso in mixture-of-experts regression models,”
2020.
[23] B. T. Huynh and F. Chamroukhi, “Estimation and feature selection in
mixtures of generalized linear experts models,” 2019.
[24] L. Fortuna, S. Graziani, and M. G. Xibilia, Soft Sensors for Monitoring
and Control of Industrial Processes . Springer, 2007.
[25] L. A. Zadeh, “Fuzzy sets as a basis for a theory of possibility,” Fuzzy
Sets and Systems , vol. 1, no. 1, pp. 3–28, 1978.
[26] D. Dubois and H. Prade, Possibility Theory: Qualitative and Quantitative
Aspects . Dordrecht: Springer Netherlands, 1998, pp. 169–226.
[27] R. R. YAGER, “Measuring tranquility and anxiety in decision making:
An application of fuzzy sets,” International Journal of General Systems ,
vol. 8, no. 3, pp. 139–146, 1982.
[28] B. Solaiman and É. Bossé, Fundamental Possibilistic Concepts . Cham:
Springer International Publishing, 2019, pp. 13–46.
[29] D. Dubois and H. Prade, “Possibilistic logic — an overview,” in
Computational Logic , ser. Handbook of the History of Logic, J. H.
Siekmann, Ed. North-Holland, 2014, vol. 9, pp. 283–342.
[30] W. Stephenson and T. Broderick, “Approximate cross-validation in
high dimensions with guarantees,” in Proceedings of the Twenty Third
International Conference on Artiﬁcial Intelligence and Statistics , ser.
Proceedings of Machine Learning Research, S. Chiappa and R. Calandra,
Eds., vol. 108. PMLR, 26–28 Aug 2020, pp. 2424–2434.
[31] S. Lee, H. Lee, P. Abbeel, and A. Y . Ng, “Efﬁcient L1 regular-
ized logistic regression,” in Proceedings, The Twenty-First National
Conference on Artiﬁcial Intelligence and the Eighteenth Innovative
Applications of Artiﬁcial Intelligence Conference, July 16-20, 2006,
Boston, Massachusetts, USA . AAAI Press, 2006, pp. 401–408.
[32] Y . Miche, A. Sorjamaa, P. Bas, O. Simula, C. Jutten, and A. Lendasse,
“Op-elm: Optimally pruned extreme learning machine,” IEEE Transac-
tions on Neural Networks , vol. 21, no. 1, pp. 158–162, 2010.

--- PAGE 12 ---
12 IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS, VOL. X, NO. Y , MONTH YEAR
[33] I. Nabney, “Netlab toolbox,” https://www.mathworks.com/matlabcentral/
ﬁleexchange/2654-netlab, 2022, accessed: 2022-10-13.
[34] A. Lendasse, S. A., and Y . Miche, “Op-elm toolbox,” https://research.
cs.aalto.ﬁ//aml/software.shtml, 2022, accessed: 2022-10-13.
[35] H. van der V oet, “Comparing the predictive accuracy of models using
a simple randomization test,” Chemometrics and Intelligent Laboratory
Systems , vol. 25, no. 2, pp. 313–323, 1994.
