# 2310.04361.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2310.04361.pdf
# Kích thước tệp: 4858386 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Khai thác Tính thưa thớt của Kích hoạt với Chuyển đổi
Dense thành Dynamic-k Mixture-of-Experts
Filip Szatkowski∗
IDEAS NCBR
Đại học Công nghệ Warsaw
Bartosz Wójcik∗†
IDEAS NCBR
Đại học Jagiellonian‡
Mikołaj Piórczy ´nski∗
Đại học Công nghệ Warsaw
Simone Scardapane
Đại học Sapienza Rome

Tóm tắt
Các mô hình Transformer có thể gặp phải những hạn chế thực tế do yêu cầu tính toán cao của chúng. Đồng thời, các mô hình như vậy thể hiện tính thưa thớt kích hoạt đáng kể, điều này có thể được tận dụng để giảm chi phí suy luận bằng cách chuyển đổi các phần của mạng thành các lớp Mixture-of-Experts (MoE) tương đương. Bất chấp vai trò quan trọng của tính thưa thớt kích hoạt, tác động của nó đối với quá trình này vẫn chưa được khám phá. Chúng tôi chứng minh rằng hiệu quả của việc chuyển đổi có thể được cải thiện đáng kể bằng cách điều chỉnh tính thưa thớt kích hoạt của mô hình cơ sở một cách phù hợp. Hơn nữa, được thúc đẩy bởi phương sai cao của số lượng neuron được kích hoạt cho các đầu vào khác nhau, chúng tôi giới thiệu một quy tắc lựa chọn expert dynamic-k hiệu quả hơn, điều chỉnh số lượng expert được thực thi trên cơ sở từng token. Để đạt được các khoản tiết kiệm thêm, chúng tôi mở rộng phương pháp này cho các phép chiếu multi-head attention. Cuối cùng, chúng tôi phát triển một triển khai hiệu quả chuyển đổi các khoản tiết kiệm tính toán này thành tốc độ thực tế. Phương pháp được đề xuất, Dense to Dynamic-k Mixture-of-Experts (D2DMoE), vượt trội hơn các phương pháp hiện có trên các nhiệm vụ NLP và thị giác phổ biến, giảm chi phí suy luận lên đến 60% mà không ảnh hưởng đáng kể đến hiệu suất.

1 Giới thiệu
Transformer đã trở thành một kiến trúc mô hình chủ đạo trong các lĩnh vực khác nhau của học sâu như dịch máy [47], mô hình ngôn ngữ [6,31], và thị giác máy tính [7,21]. Hiệu quả rộng rãi của các mô hình Transformer trong nhiều ứng dụng khác nhau có liên quan chặt chẽ đến khả năng mở rộng hiệu quả theo số lượng tham số mô hình [20], thúc đẩy các nhà nghiên cứu huấn luyện các mô hình ngày càng lớn hơn [45,19]. Tuy nhiên, nhu cầu tính toán đáng kể của các mô hình này thường hạn chế việc triển khai trong các môi trường thực tế với tài nguyên hạn chế.

Đồng thời, các mô hình Transformer thể hiện tính thưa thớt kích hoạt đáng kể trong các biểu diễn trung gian của chúng [24], điều này cho thấy rằng hầu hết các tính toán của chúng đều thừa thải. Các phương pháp tính toán có điều kiện có thể giảm các chi phí không cần thiết này bằng cách chỉ sử dụng một tập con các tham số mô hình cho bất kỳ đầu vào nào [14]. Đặc biệt, các lớp Mixture-of-Experts (MoE) [38], bao gồm nhiều expert được thực thi một cách thưa thớt cho bất kỳ token đầu vào nào, là một cách hiệu quả để tách biệt số lượng tham số của mô hình khỏi chi phí tính toán của nó [3]. Như được chỉ ra bởi [52], nhiều mô hình Transformer dense đã được huấn luyện trước có thể được làm hiệu quả hơn bằng cách chuyển đổi các khối FFN của chúng thành các lớp MoE, một quá trình mà họ gọi là MoEfication.

∗Đóng góp ngang nhau
†Tác giả liên hệ: b.wojcik@doctoral.uj.edu.pl
‡Khoa Toán học và Khoa học Máy tính, Trường Tiến sĩ Khoa học Chính xác và Tự nhiên
Preprint. Đang xem xét.arXiv:2310.04361v4 [cs.LG] 12 Nov 2024

--- TRANG 2 ---
Hình 1: Các thành phần chính của D2DMoE: (a) Chúng tôi tăng cường tính thưa thớt kích hoạt trong mô hình cơ sở. (b) Chúng tôi chuyển đổi các lớp FFN trong mô hình thành các lớp MoE với router dự đoán đóng góp của từng expert. (c) Chúng tôi giới thiệu định tuyến dynamic-k lựa chọn các expert để thực thi dựa trên đóng góp dự đoán của chúng.

Đóng góp của bài báo này: Chúng tôi xem xét câu hỏi nghiên cứu sau: cách tối ưu để chuyển đổi một mô hình Transformer chung thành một biến thể thưa thớt tương đương là gì? Chúng tôi xác định một loạt điểm yếu của quá trình MoEfication hạn chế sự cân bằng độ chính xác-độ thưa thớt kết quả, và đề xuất các biện pháp khắc phục tương ứng như sau. Chúng tôi gọi thuật toán kết quả là Dense to Dynamic-k Mixture-of-Experts (D2DMoE) và phác thảo nó trong Hình 1.

1. Đầu tiên, chúng tôi phân tích mối quan hệ giữa tính thưa thớt kích hoạt của mô hình khởi đầu và hiệu quả của mô hình MoE cuối cùng. Chúng tôi chỉ ra rằng các khoản tiết kiệm tính toán có liên quan trực tiếp đến mức độ thưa thớt, và chúng tôi tương ứng thực thi các mức độ thưa thớt kích hoạt cao hơn trước khi chuyển đổi thông qua một quá trình tinh chỉnh nhẹ, dẫn đến sự cân bằng chi phí-hiệu suất được cải thiện đáng kể.

2. Chúng tôi xác định sơ đồ huấn luyện router trong thuật toán MoEfication ban đầu là một hạn chế của quá trình chuyển đổi. Chúng tôi đề xuất khung hóa việc huấn luyện router như một bài toán hồi quy thay vì, do đó các router của chúng tôi trực tiếp dự đoán chuẩn của đầu ra của mỗi expert.

3. Chúng tôi chỉ ra rằng các mô hình Transformer thể hiện phương sai đáng kể của số lượng neuron được kích hoạt, và việc lựa chọn expert top-k tiêu chuẩn trong các lớp MoE là không hiệu quả. Chúng tôi đề xuất một sơ đồ lựa chọn expert dynamic-k thay thế điều chỉnh số lượng expert được kích hoạt trên cơ sở từng token. Phương pháp này cho phép mô hình phân bổ tính toán hiệu quả giữa các đầu vào dễ và khó, tăng hiệu quả tổng thể.

4. Chúng tôi tổng quát hóa phương pháp chuyển đổi cho bất kỳ lớp tuyến tính độc lập nào bao gồm các biến thể MLP có cổng thường được tìm thấy trong các LLM hiện đại [45,42] và các phép chiếu trong các lớp Multi-Head Attention (MHA) (thường chiếm hơn 30% tổng tính toán trong các mô hình Transformer [39]).

Đối với MHA, chúng tôi đề xuất một quy trình thay thế trong đó mỗi lớp dense được thay thế bằng một mô-đun MLP hai lớp được huấn luyện để bắt chước đầu ra của lớp gốc.

Chúng tôi đánh giá D2DMoE trên các benchmark về phân loại văn bản, phân loại hình ảnh và mô hình ngôn ngữ, chứng minh những cải thiện đáng kể trong sự cân bằng chi phí-hiệu suất trong tất cả các trường hợp. D2DMoE đặc biệt phù hợp với phần cứng hiện đại, như được chứng minh bởi triển khai GPU hiệu quả của chúng tôi, mà chúng tôi đóng góp cùng với phương pháp được đề xuất.

2 Động lực
Các mô hình MoE đã thu hút nhiều sự chú ý trong những năm gần đây như một kiến trúc hiệu quả để tách biệt số lượng tham số khỏi chi phí tính toán của các mô hình [56]. Trong một lớp MoE, độ thưa thớt cứng thường được thực thi một cách rõ ràng bằng cách áp dụng phép toán top-k trên các đầu ra của một lớp gating có thể huấn luyện. Tuy nhiên, nhiều công trình gần đây [53,2,30] đã chỉ ra rằng hầu hết các Transformer, khi được huấn luyện ở quy mô lớn, xây dựng các biểu diễn thưa thớt và mô-đun nội tại. Zhang et al. [52] đề xuất tận dụng tính mô-đun tự nhiên này với MoEfication - một phương pháp chuyển đổi các mô hình transformer dense thành các mô hình MoE bằng cách nhóm các trọng số FFN thành các expert và sau đó học các router nhỏ xác định expert nào được kích hoạt. Các mô hình được chuyển đổi với MoEfication có thể bảo toàn hiệu suất của các mô hình dense gốc trong khi chỉ sử dụng một phần chi phí tính toán của chúng. Tuy nhiên, chúng tôi tin rằng quy trình MoEfication không tối ưu, và do đó nhằm mục đích có được các sơ đồ chuyển đổi dense-to-sparse có được sự cân bằng chi phí-hiệu suất tốt hơn.

--- TRANG 3 ---
Hình 2: (a) Tác động của độ thưa thớt đến chuyển đổi MoE cho một mô hình GPT-2 được MoEfied [27] thu được bắt đầu từ các mô hình với các mức độ thưa thớt kích hoạt khác nhau. Việc thưa thớt hóa tương quan với hiệu suất mô hình. (b) Phân bố kích hoạt khác không trong các lớp FFN trong GPT-2-base trên OpenWebText, có và không có giai đoạn thực thi độ thưa thớt. Cả hai mô hình đều thể hiện phương sai đáng kể, và tỷ lệ trung bình-phương sai tăng trong mô hình thưa thớt hóa. (c) Chúng tôi đề xuất khai thác sự biến đổi trong kích hoạt thông qua một quy trình định tuyến dynamic-k điều chỉnh số lượng expert được phân bổ cho một mẫu.

Theo trực giác, một MoE được chuyển đổi từ một mô hình cơ sở thưa thớt hơn sẽ có thể thực hiện chức năng ban đầu bằng cách sử dụng số lượng expert nhỏ hơn. Để xác thực giả thuyết này, chúng tôi thực hiện MoEfication trên các biến thể khác nhau của GPT2-base với các mức độ thưa thớt kích hoạt khác nhau và hiển thị kết quả trong Hình 2a. Như mong đợi, MoEfication hoạt động tốt hơn với các mô hình thưa thớt hơn. Chúng tôi tiếp tục điều tra trung bình và phương sai theo từng token của các neuron khác không trong mô hình cơ sở và mô hình thưa thớt hóa, và hiển thị kết quả trong Hình 2b. Quan sát thấy rằng các lớp khác nhau sử dụng số lượng neuron khác nhau trung bình. Hơn nữa, phương sai của số lượng neuron được kích hoạt khá cao và trở nên đáng kể hơn trong mô hình thưa thớt hóa. Điều này có nghĩa là gating top-k tĩnh như được sử dụng trong MoEfication không tối ưu cho các mô hình được chuyển đổi dense-to-MoE, và một quy tắc phân bổ expert linh hoạt hơn có thể xử lý phương sai cao theo từng token và từng lớp có thể có lợi cho hiệu quả của các mô hình như vậy, như được minh họa tại Hình 2c. Gating dynamic-k như vậy yêu cầu các router dự đoán đóng góp của mỗi expert một cách đáng tin cậy. Chúng tôi quan sát thấy rằng các router thu được thông qua MoEfication không phản ánh chính xác đóng góp này. Hơn nữa, quy trình huấn luyện router của chúng phụ thuộc vào độ thưa thớt nghiêm ngặt của mô hình được đảm bảo bởi hàm kích hoạt ReLU. Do đó, chúng tôi thiết kế một sơ đồ huấn luyện router mới trực tiếp dự đoán đóng góp của mỗi expert và tổng quát hóa cho họ hàm kích hoạt rộng hơn. Chúng tôi kết hợp các thành phần được đề xuất (thực thi độ thưa thớt, định tuyến đóng góp expert, và gating dynamic-k) thành một phương pháp duy nhất mà chúng tôi gọi là Dense to Dynamic-k Mixture-of-Experts (D2DMoE), mà chúng tôi mô tả chi tiết trong Phần tiếp theo.

3 Phương pháp
D2DMoE giảm chi phí tính toán của mô hình bằng cách tách mỗi mô-đun MLP thành một lớp MoE. Trong phần này, chúng tôi mô tả tất cả các thành phần của nó một cách chi tiết. Tổng quan cấp cao về toàn bộ quy trình được trình bày trong Hình 1. Quá trình chuyển đổi có thể được đi trước tùy chọn bởi việc thay thế lớp phép chiếu MHA (Mục 3.5), cho phép chúng tôi áp dụng cùng một pipeline chuyển đổi trên tất cả các mô-đun thay thế.

3.1 Thực thi độ thưa thớt kích hoạt
Chúng tôi mong đợi rằng việc thực thi các mức độ thưa thớt kích hoạt cao hơn có thể cho phép thực thi số lượng expert thậm chí nhỏ hơn, dẫn đến tiết kiệm tính toán tổng thể. Để đạt được điều này, chúng tôi tạo ra độ thưa thớt kích hoạt bằng cách tinh chỉnh mô hình với một số hạng mất mát bổ sung tạo ra độ thưa thớt kích hoạt [11]. Chúng tôi áp dụng điều chỉnh Hoyer bình phương [22, 17] trên các kích hoạt của mô hình:

Ls(x) = 1/L ∑(l=1 to L) (∑i|al_i|)² / ∑i(al_i)²    (1)

4Chúng tôi cung cấp các chi tiết thử nghiệm cho phân tích này trong Mục 4.3 và Phụ lục J.

--- TRANG 4 ---
nơi al_i là vector kích hoạt từ lớp giữa của MLP thứ l cho đầu vào x, và L là tổng số MLP trong mô hình. Tổng thể, mô hình được huấn luyện với hàm chi phí sau:

L(x) = LCE(ŷ, y) + αLs(x)    (2)

nơi LCE là mất mát cross-entropy, và α là siêu tham số kiểm soát cường độ thực thi độ thưa thớt. Chúng tôi thấy rằng các mô hình được huấn luyện trước khôi phục hiệu suất ban đầu chỉ với một phần ngân sách huấn luyện ban đầu (ví dụ 1B token cho GPT2-base hoặc Gemma-2B, ít hơn 1% token được sử dụng để huấn luyện trước).

3.2 Phân cụm expert
Chúng tôi tách các mô-đun MLP hai lớp thành các expert bằng cách sử dụng phương pháp phân cụm tham số được đề xuất bởi Zhang et al. [52]. Giả sử các lớp MLP bao gồm trọng số W1, W2 và bias tương ứng b1, b2, chúng tôi xử lý trọng số của mỗi neuron từ W1 như các đặc trưng và đưa chúng vào thuật toán k-means cân bằng [26] nhóm các neuron với trọng số tương tự lại với nhau. Sau đó, chúng tôi sử dụng các chỉ số cụm kết quả để tách lớp tuyến tính đầu tiên W1, vector bias đầu tiên b1, và lớp tuyến tính thứ hai W2 thành n expert có cùng kích thước. Bias thứ hai b2 không bị ảnh hưởng bởi quy trình này.

Quá trình MoEfication được thiết kế cho các MLP hai lớp tiêu chuẩn [52]. Các LLM gần đây [45,42] đã chuyển sang FFN có cổng, nơi kích hoạt được thực hiện thông qua Gated Linear Unit (GLU) [37], chứa một ma trận trọng số bổ sung cho các phép chiếu cổng. Để điều chỉnh quy trình phân cụm expert được mô tả ở trên cho các lớp FFN có cổng, chúng tôi phân cụm trọng số của ma trận cổng Wg thay vì W1, và sử dụng các chỉ số thu được thông qua việc tách expert trên trọng số cổng để chia trọng số của hai lớp khác. Chúng tôi cung cấp thêm trực giác và chi tiết về phương pháp của chúng tôi cho FFN có cổng trong Phụ lục G.

3.3 Định tuyến đóng góp expert
Trong một mô hình dựa trên MoE tiêu chuẩn, các mạng gating được huấn luyện theo cách đầu cuối đến đầu cuối. Ngược lại với điều này, chúng tôi huấn luyện mỗi mạng gating một cách độc lập. Chúng tôi đề xuất khung hóa bài toán huấn luyện router như một nhiệm vụ hồi quy và trực tiếp dự đoán chuẩn ℓ2 của đầu ra của mỗi expert với router. Chính thức, cho một token đầu vào z, chúng tôi huấn luyện router D2DMoE R để tối thiểu hóa mất mát sau:

Lr(z) = 1/n ∑(i=1 to n) (R(z)i - ∥Ei(z)∥)²    (3)

nơi Ei là expert thứ i. Chúng tôi sử dụng một mạng neural hai lớp nhỏ làm router R và áp dụng hàm kích hoạt giá trị tuyệt đối để đảm bảo đầu ra không âm. Công thức dựa trên hồi quy này vẫn tương thích với việc lựa chọn expert top-k thường được sử dụng, nhưng cho phép phân bổ đóng góp của mỗi expert chính xác hơn, như chúng tôi chỉ ra sau đó trong phần thử nghiệm.

Lưu ý rằng Zhang et al. [52] cũng huấn luyện mỗi mạng định tuyến một cách độc lập, nhưng phương pháp của họ xây dựng nhãn nhân tạo cho mỗi đầu vào, và sau đó huấn luyện router như một bộ phân loại. Chúng tôi thảo luận về sự khác biệt chi tiết trong Phụ lục A.

3.4 Gating dynamic-k
Các lớp MoE thường được sử dụng luôn thực thi top-k expert cho mỗi token, nơi k là một siêu tham số được xác định trước. Điều này có nghĩa là, bất kể độ khó của đầu vào, mô hình dành cùng một lượng tính toán cho mỗi batch [54] hoặc token [38]. Mặc dù điều này có thể phù hợp nếu mô hình được huấn luyện với cùng một hạn chế, nó không tối ưu cho một mô hình được chuyển đổi từ một mô hình dense, như chúng tôi chỉ ra trong Mục 2.

Vì router của chúng tôi trực tiếp dự đoán chuẩn ℓ2 của đầu ra của mỗi expert, chúng tôi đề xuất một phương pháp lựa chọn expert dynamic-k bỏ qua các expert mà router dự đoán chuẩn đầu ra tương đối nhỏ. Cho một vector đầu ra router R(z), chúng tôi lựa chọn một siêu tham số τ ∈ [0,1] và định nghĩa quy tắc lựa chọn expert G cho phần tử thứ i như:

G(z)i = {1 nếu R(z)i ≥ τ · maxR(z)
         {0 nếu R(z)i < τ · maxR(z)    (4)

Lưu ý rằng khi τ tăng, số lượng expert được thực thi và chi phí tính toán tổng thể giảm. Chúng tôi nhấn mạnh rằng sau khi triển khai mô hình τ có thể được điều chỉnh mà không cần huấn luyện lại.

--- TRANG 5 ---
Hình 4: So sánh sự cân bằng FLOPs-hiệu suất của phương pháp chúng tôi và MoEfication [52] trên các benchmark CV và NLP. Chúng tôi cũng bao gồm early-exit (ZTW, [49]) và các baseline token dropping (A-ViT, [51]) cho phân loại. Phương pháp của chúng tôi vượt trội hơn các baseline này qua nhiều ngân sách tính toán.

3.5 Chuyển đổi các lớp dense độc lập
Một lượng đáng kể tính toán trong các mạng neural sâu thường được chi tiêu cho các lớp dense không được theo sau bởi bất kỳ hàm kích hoạt nào. Các phương pháp chuyển đổi Dense-to-sparse-MoE không thể giảm chi phí của các lớp như vậy do thiếu độ thưa thớt kích hoạt. Điều này xác định một giới hạn trên về khả năng tiết kiệm tính toán có thể. Để vượt qua nó, chúng tôi thay thế các lớp dense bằng các MLP nhỏ với chi phí tính toán và số lượng tham số gần như tương tự. Mỗi MLP được huấn luyện để bắt chước đầu ra của lớp dense ban đầu cho cùng một đầu vào bằng cách tối thiểu hóa sai số bình phương trung bình giữa hai (giống như mất mát distillation).

Hình 3: Sơ đồ chuyển đổi phép chiếu Multi-Head Attention.

Trong trường hợp của chúng tôi, đối với kiến trúc Transformer, chúng tôi thay thế các ma trận phép chiếu cùng với bias của chúng trong mỗi lớp MHA, như được mô tả trong Hình 3. Điều này có nghĩa là mô hình cuối cùng có bốn lớp MoE trong lớp MHA và một lớp MoE trong lớp FFN (hoặc thông thường hoặc có cổng) cho mỗi khối Transformer. Lưu ý rằng chúng tôi không sửa đổi việc tính toán của scaled dot-product attention bản thân và sơ đồ này có thể được áp dụng cho bất kỳ lớp dense độc lập nào.

4 Thí nghiệm
Để phân tích tác động của phương pháp chúng tôi, chúng tôi đánh giá hiệu suất của nó trên mô hình ngôn ngữ, phân loại văn bản và phân loại hình ảnh. Chúng tôi thu được đặc tính hiệu suất so với chi phí tính toán cho mỗi phương pháp bằng cách đánh giá các phương pháp với các siêu tham số suy luận khác nhau (hoặc τ được mô tả trong Mục 3.4 cho D2DMoE hoặc số lượng expert k cho MoEfication; chúng tôi đánh dấu chúng trên các đồ thị với các điểm marker). Chúng tôi báo cáo chi phí tính toán của mỗi phương pháp bằng FLOPs, vì nó là một thước đo độc lập với thiết bị đã được chỉ ra có tương quan tốt với độ trễ [27]. Ngoài ra, chúng tôi đo thời gian thực thi wall-clock của một triển khai hiệu quả của phương pháp chúng tôi.

Đối với MoEfication, chúng tôi tuân theo quy trình được mô tả bởi Zhang et al. [52] bằng cách chuyển đổi các hàm kích hoạt của mô hình được huấn luyện trước thành ReLU và sau đó tinh chỉnh mô hình. Trong trường hợp D2DMoE, chúng tôi cũng thay thế các hàm kích hoạt bằng ReLU, ngoại trừ Mục 5.4, nơi chúng tôi chứng minh rằng phương pháp của chúng tôi cũng hoạt động tốt với GELU. Để cung cấp so sánh công bằng, tổng ngân sách dữ liệu huấn luyện luôn giống nhau giữa các phương pháp khác nhau. Xem Phụ lục J để có mô tả chi tiết về thiết lập của chúng tôi. Mã nguồn cho các thí nghiệm của chúng tôi có sẵn tại: https://github.com/bartwojcik/D2DMoE .

4.1 Phân loại hình ảnh
Vision Transformer [7] là một trong những kiến trúc phổ biến nhất trong thị giác máy tính. Vì phương pháp của chúng tôi áp dụng cho bất kỳ mô hình Transformer nào, chúng tôi đánh giá nó trên tập dữ liệu ImageNet-1k [35] phổ biến. Chúng tôi sử dụng một checkpoint ViT-B được huấn luyện trước làm mô hình cơ sở và so sánh D2DMoE với MoEfication về sự cân bằng chi phí tính toán so với độ chính xác. Để so sánh rộng hơn, chúng tôi cũng đánh giá phương pháp early-exit hiện đại Zero-time Waste (ZTW) [49], cũng như việc triển khai lại A-ViT của chúng tôi, một phương pháp token dropping hiệu quả được đề xuất bởi Yin et al. [51]. Kết quả của chúng tôi, được trình bày trong Hình 4a, chứng minh những cải thiện đáng kể từ việc áp dụng phương pháp của chúng tôi so với MoEfication.

--- TRANG 6 ---
Bảng 1: Hiệu suất downstream tương đối của D2DMoE và MoEfication trên tập dữ liệu BoolQ. Phương pháp của chúng tôi chỉ bắt đầu suy giảm ở khoảng 70% ngân sách tính toán, trong khi MoEfication giảm dần.

Ngân sách tính toán | 100% | 90% | 80% | 70% | 60% | 50% | 25% | 10%
MoEfication | 100% | 92.24% | 92.19% | 92.15% | 88.79% | 75.40% | 86.70% | 77.53%
D2DMoE | 100% | 99.68% | 99.37% | 98.69% | 97.60% | 94.34% | 92.75% | 90.89%

4.2 Phân loại văn bản
Chúng tôi đánh giá phương pháp của chúng tôi với BERT-base [6] trên tập dữ liệu CARER [36] chứa các mẫu văn bản được phân loại thành 6 danh mục cảm xúc khác nhau. Chúng tôi so sánh sự cân bằng độ chính xác so với FLOPs cho D2DMoE, MoEfication, và ZTW. Chúng tôi hiển thị kết quả trong Hình 4b. Hiệu suất của MoEfication dần suy giảm và hoàn toàn sụp đổ khi số lượng expert được thực thi tiệm cận về không. Ngược lại, D2DMoE duy trì hiệu suất ban đầu cho một phạm vi rộng các ngân sách tính toán, và việc giảm hiệu suất bắt đầu ở một ngân sách thấp hơn đáng kể. Trong khi early-exiting hoạt động tốt cho các ngân sách thấp nhất, nó thu được kết quả tồi tệ hơn D2DMoE ở ngân sách trung bình và gặp phải sự suy giảm hiệu suất dần dần.

4.3 Mô hình ngôn ngữ
Chúng tôi đánh giá phương pháp của chúng tôi trên mô hình ngôn ngữ và so sánh với MoEfication sử dụng GPT-2-base [31] và Gemma-2B [42]. Chúng tôi khởi tạo các mô hình GPT-2 từ một checkpoint OpenAI có sẵn công khai được huấn luyện trước trên tập dữ liệu WebText nguồn đóng và sử dụng OpenWebText [12] trong tất cả các thí nghiệm của chúng tôi. Đối với Gemma-2B, chúng tôi cũng bắt đầu từ mô hình được huấn luyện trước có sẵn công khai và đánh giá khả năng ngôn ngữ của nó trên tập dữ liệu C4 [32] sau khi tinh chỉnh. Đối với cả hai mô hình, chúng tôi sử dụng khoảng 1B token cho giai đoạn tinh chỉnh (ít hơn 1% chi phí huấn luyện trước ban đầu) và 8-16M token cho huấn luyện router. Chúng tôi báo cáo kết quả trong phần này mà không thay thế phép chiếu MHA, vì nhiệm vụ này rất nhạy cảm với các thay đổi trong các lớp attention, dẫn đến sự suy giảm mất mát đáng chú ý. Để biết thêm chi tiết huấn luyện, xem Phụ lục J.3

Chúng tôi trình bày các mất mát kiểm tra cho D2DMoE và MoEfication ở các ngân sách tính toán khác nhau cho GPT-2-base và Gemma-2B trong Hình 4c và 4d tương ứng. Phương pháp của chúng tôi vượt trội hơn baseline ở mọi ngân sách tính toán. Mất mát của D2DMoE ổn định ở các mức ngân sách cao hơn, trong khi baseline hiển thị kết quả tồi tệ hơn một cách nhất quán bất cứ khi nào chúng tôi giảm ngân sách tính toán. Đáng chú ý, đối với mô hình Gemma-2B lớn hơn, phương pháp của chúng tôi hoạt động tốt cho hầu hết các ngân sách tính toán, trong khi hiệu suất của MoEfication sụp đổ. Sự thất bại của MoEfication có thể được giải thích bởi sự xuất hiện của các kích hoạt lớn trong các mô hình lớn [40], điều này làm cho nó không thể học được định tuyến đáng tin cậy, như chúng tôi phân tích chi tiết hơn trong Phụ lục E.

Chúng tôi cũng cung cấp đánh giá downstream của các mô hình Gemma trên tập dữ liệu BoolQ. Chúng tôi lấy mô hình cơ sở, đạt được 68.40% độ chính xác đánh giá zero-shot, và chuyển đổi nó thành MoE với D2DMoE và MoEfication. Trong Bảng 1, chúng tôi báo cáo độ chính xác tương đối của các mô hình ở các ngân sách tính toán khác nhau. Phương pháp của chúng tôi phần lớn duy trì hiệu suất qua nhiều ngân sách tính toán, trong khi hiệu suất của MoEfication giảm đáng kể. Điều này chỉ ra rằng kết quả loss-vs-FLOPs cho D2DMoE và MoEfication trực tiếp chuyển đổi thành hiệu suất downstream trên các nhiệm vụ ngôn ngữ.

4.4 Độ trễ thực thi
Hình 5: Thời gian thực thi wall-clock của lớp D2DMoE đơn lẻ.

Để bất kỳ phương pháp tăng tốc mô hình nào có ích trong thực tế, nó phải giảm thời gian thực thi suy luận đầu cuối đến đầu cuối trên phần cứng GPU hiện đại. Để đạt được điều này, chúng tôi triển khai forward pass của lớp MoE trong ngôn ngữ trung gian Triton [43], và sử dụng một số tối ưu hóa cho triển khai của chúng tôi, bao gồm mẫu truy cập bộ nhớ hiệu quả, fusion kernel, và auto-tuning cấu hình. Như được đề xuất bởi Tan et al. [41], triển khai của chúng tôi cũng tránh các bản sao không cần thiết khi nhóm token.

Chúng tôi xác minh hiệu suất của triển khai cho một lớp D2DMoE đơn lẻ (24 expert với chiều expert 128) một cách riêng biệt bằng cách so sánh với mô-đun MLP tương ứng (chiều trong 3072) trên GPU NVIDIA A100. Chúng tôi điền một tensor kích thước [256×197×768] (kích thước batch, độ dài chuỗi, và chiều ẩn, tương ứng) với nhiễu Gaussian và sử dụng nó làm đầu vào cho cả hai mô-đun.

--- TRANG 7 ---
Mạng gating của D2DMoE được bao gồm trong các phép đo, nhưng các quyết định được ghi đè với các mẫu từ phân phối Bernoulli, và chúng tôi kiểm soát số lượng expert được thực thi trung bình bằng cách thay đổi xác suất Bernoulli. Kết quả, được trình bày trong Hình 5, chỉ ra rằng triển khai của chúng tôi mở rộng tuyến tính với số lượng expert được thực thi, và có overhead không đáng kể. Phương pháp của chúng tôi có thể nhanh gần ba lần so với MLP tiêu chuẩn trong khi bảo toàn 99% độ chính xác ban đầu. Trong Phụ lục C, chúng tôi cung cấp kết quả đo wall-clock bổ sung cùng với mô tả chi tiết hơn về triển khai của chúng tôi.

4.5 Khả năng tương thích với các kỹ thuật nén mô hình

Hình 6: D2DMoE được áp dụng cho các mô hình được tỉa với CoFi.

Hình 7: D2DMoE được áp dụng cho các mô hình được lượng tử hóa.

Để tăng tốc suy luận, D2DMoE tận dụng độ thưa thớt kích hoạt phụ thuộc đầu vào, một tính chất vốn có của hầu hết mọi mô hình Transformer. Tuy nhiên, tương tác giữa D2DMoE và các kỹ thuật tăng tốc mạng phổ biến khác, chẳng hạn như tỉa [16] hoặc lượng tử hóa [13, 28], không rõ ràng. Chúng tôi đánh giá D2DMoE kết hợp với các kỹ thuật như vậy để chứng minh tính bổ sung của chúng.

Đầu tiên, chúng tôi đánh giá D2DMoE được áp dụng trên các mạng được tỉa với CoFi, một kỹ thuật tỉa có cấu trúc được giới thiệu bởi Xia et al. [50]. CoFi loại bỏ các neuron thừa, attention head, và các lớp con để đạt được tỷ lệ thưa thớt mong muốn, và sau đó tinh chỉnh mạng đã giảm. Chúng tôi đầu tiên tỉa mô hình cơ sở với CoFi đến mức thưa thớt mong muốn, áp dụng D2DMoE cho nó, và sau đó đánh giá cả hai mô hình trên QNLI [48]. Trong Hình 6, chúng tôi chỉ ra rằng D2DMoE thành công tăng tốc suy luận ngay cả trên các mạng được tỉa đến mức thưa thớt cao.

Trong Hình 7, chúng tôi cũng điều tra khả năng ứng dụng của D2DMoE cho các mô hình được lượng tử hóa sử dụng lượng tử hóa động sau huấn luyện từ PyTorch trên BERT được huấn luyện trên tập dữ liệu CARER. Phương pháp của chúng tôi mạnh mẽ với lượng tử hóa 8- và 16-bit và chỉ thể hiện những biến đổi nhỏ trong hiệu suất sau lượng tử hóa. Vì FLOPs không tính đến độ rộng bit, chúng tôi hiển thị các mô hình được lượng tử hóa trong cùng phạm vi FLOPs với mô hình gốc. Trong Phụ lục C, chúng tôi cũng trình bày các phép đo thời gian wall-clock cho D2DMoE được lượng tử hóa.

5 Phân tích
Trong phần này, chúng tôi trình bày chi tiết các thí nghiệm bổ sung cung cấp cái nhìn sâu sắc về hiệu suất của phương pháp chúng tôi. Ngoài ra, trong Phụ lục E chúng tôi phân tích hiệu suất của MoEfication với Gemma, trong Phụ lục F chúng tôi cung cấp kết quả phân tích kiến trúc router, trong Phụ lục H chúng tôi tiến hành các thí nghiệm tương ứng với những thí nghiệm trong Mục 5.5 với hàm GELU, và trong Phụ lục I chúng tôi chỉ ra các hình ảnh hóa bổ sung cho các mẫu kích hoạt expert.

5.1 Mẫu lựa chọn expert
Quy tắc dynamic-k giới thiệu tính biến đổi trong việc phân bổ ngân sách tính toán dọc theo độ sâu mô hình. Để khám phá quy mô của nó, chúng tôi điều tra phân phối số lượng expert được thực thi, có và không có giai đoạn thưa thớt hóa kích hoạt. Trong Hình 8a, chúng tôi hiển thị biểu đồ của số lượng expert được kích hoạt cho mỗi lớp FFN của mô hình BERT-base được huấn luyện trên tập dữ liệu CARER (kết quả bổ sung có sẵn trong phụ lục trong Phụ lục I). Như mong đợi, mô hình với thực thi độ thưa thớt kích hoạt yêu cầu ít expert hơn cho một ngưỡng nhất định. Cả mô hình cơ sở và mô hình thưa thớt hóa đều thể hiện phương sai đáng kể trong số lượng neuron được kích hoạt qua các lớp khác nhau, điều này biện minh cho việc lựa chọn dynamic-k và chỉ ra rằng các cơ chế thích ứng tính toán là quan trọng cho suy luận hiệu quả trong các mô hình dựa trên Transformer.

--- TRANG 8 ---
Hình 8: D2DMoE cho phép phân bổ tính toán động cho mỗi lớp và mỗi đầu vào một cách độc lập. a) Phân phối theo lớp của số lượng expert được thực thi trên tập dữ liệu CARER trong D2DMoE với τ = 0.01 cho mô hình tiêu chuẩn (trên) và mô hình thưa thớt hóa (dưới). Việc thưa thớt hóa dẫn đến số lượng expert được chọn thấp hơn đáng kể. b) Bản đồ tải tính toán của các mẫu ImageNet-1k được chọn cho mô hình ViT-B được chuyển đổi với τ = 0.0025. D2DMoE phân bổ ngân sách tính toán cho các vùng quan trọng về mặt ngữ nghĩa của đầu vào.

D2DMoE cũng cho phép mô hình phân bổ tài nguyên tính toán khác nhau cho các lớp khác nhau. Chúng tôi mong đợi mô hình phân bổ nhiều tính toán hơn cho các token chứa thông tin liên quan đến nhiệm vụ hiện tại. Vì mỗi vị trí token trong mô hình ViT tương ứng với một phần riêng biệt và không chồng chéo của hình ảnh đầu vào, chúng tôi có thể dễ dàng vẽ heatmap để chỉ ra các vùng của hình ảnh mà mô hình chi tiêu ngân sách tính toán. Trong Hình 8b, chúng tôi trình bày phân tích như vậy cho mô hình ViT-B được chuyển đổi. Như mong đợi, định tuyến dynamic-k cho phép mô hình giảm thiểu nỗ lực tính toán được chi tiêu cho các vùng chứa thông tin không quan trọng.

5.2 Nghiên cứu ablation
Vì phương pháp của chúng tôi bao gồm một số bước, tác động tích cực của mỗi bước có thể không rõ ràng. Để chỉ ra tầm quan trọng của mỗi thành phần, chúng tôi thực hiện một nghiên cứu ablation bằng cách thêm từng thành phần vào phương pháp baseline. Chúng tôi lấy mô hình BERT-base và đánh giá các biến thể ablated trong cùng thiết lập như được mô tả trong Mục 4.2. Kết quả của thí nghiệm này được trình bày trong Hình 9a. Như mong đợi, mỗi phiên bản ablated của phương pháp cải thiện so với phiên bản trước đó. Giai đoạn thực thi độ thưa thớt dẫn đến hiệu suất được cải thiện so với MoEfication thuần túy. Mục tiêu huấn luyện router thay thế và phân công expert dynamic-k tiếp tục cải thiện kết quả, nhưng - vì phương pháp chỉ hoạt động trên lớp FFN - chi phí tính toán không thể xuống dưới chi phí của phần còn lại của mô hình. Mở rộng D2DMoE cho các lớp phép chiếu MHA cho phép phương pháp của chúng tôi giảm chi phí tính toán thêm, và phương pháp đầy đủ kết quả duy trì độ chính xác của mô hình gốc với khoảng hai lần ít FLOPs hơn MoEfication.

5.3 Độ thưa thớt kích hoạt mô hình cơ sở
Để biện minh cho giai đoạn thưa thớt hóa kích hoạt được đề xuất của chúng tôi, chúng tôi điều tra tác động của độ thưa thớt kích hoạt của mô hình dense cơ sở đối với hiệu suất MoE thu được với phương pháp của chúng tôi. Chúng tôi tiến hành một nghiên cứu tương tự như được trình bày trong Hình 2a: chúng tôi huấn luyện nhiều mô hình cơ sở với các trọng số mất mát thực thi độ thưa thớt kích hoạt α khác nhau và chuyển đổi chúng thành các mô hình Mixture-of-Experts với phương pháp của chúng tôi. Kết quả, được hiển thị trong Hình 9b, làm nổi bật mối tương quan tích cực giữa độ thưa thớt kích hoạt và hiệu suất của MoE được chuyển đổi, vì độ thưa thớt cao hơn trong mô hình cơ sở luôn chuyển đổi thành hiệu suất tốt hơn cho D2DMoE. Điều này phù hợp với kết quả đã quan sát trước đây cho MoEfication. Tuy nhiên, phương pháp của chúng tôi đạt được kết quả tốt hơn cho mỗi mô hình cơ sở trong tất cả các trường hợp, chứng minh rằng định tuyến hồi quy và lựa chọn dynamic-k sử dụng tốt hơn độ thưa thớt được tạo ra.

5.4 Thưa thớt hóa và sự phụ thuộc vào hàm kích hoạt
Các công trình về độ thưa thớt kích hoạt tập trung phân tích của họ vào các mạng với kích hoạt ReLU, vì các hàm khác (như GELU hoặc SiLU) không đảm bảo độ thưa thớt chính xác. Khi phân tích các mô hình không phải ReLU, các

--- TRANG 9 ---
Hình 9: Các thí nghiệm phân tích với D2DMoE. (a) Tác động của các giai đoạn khác nhau của phương pháp chúng tôi. Mỗi giai đoạn cải thiện so với baseline. (b) Việc thưa thớt hóa cải thiện sự cân bằng chi phí-độ chính xác của mô hình D2DMoE cuối cùng. (c) Việc thưa thớt hóa cho phép chúng tôi áp dụng phương pháp cho mô hình dựa trên GELU mà không có sự sụt giảm đáng kể về hiệu suất. (d) Các expert nhỏ hơn hiển thị hiệu suất thuận lợi và cho phép tiết kiệm tính toán lớn hơn.

các công trình yêu cầu tinh chỉnh với hàm kích hoạt được thay đổi thành ReLU (relufication) [52,27], điều này hạn chế khả năng ứng dụng thực tế của chúng. Chúng tôi giả thuyết rằng relufication không cần thiết và các mô hình với nhiều kích hoạt gần bằng không hoạt động hiệu quả tương tự như các mô hình dựa trên ReLU tiêu chuẩn. Để đánh giá giả thuyết này, chúng tôi mở rộng sơ đồ thực thi độ thưa thớt cho kích hoạt GELU thường được sử dụng bằng cách phạt mô hình cho các giá trị trước kích hoạt lớn hơn một ngưỡng nhất định. Chúng tôi đầu tiên biến đổi các giá trị trước kích hoạt như z′ = max(0, z − d), nơi z là giá trị trước kích hoạt và d là một siêu tham số dịch chuyển. Sau đó, chúng tôi áp dụng mất mát từ Phương trình (1) trên z′. Biến đổi này chỉ phạt các giá trị trước kích hoạt lớn hơn d, và kết quả là, mô hình học tạo ra các giá trị thực tế trở nên không đáng kể sau kích hoạt. Chúng tôi tìm thấy thực nghiệm rằng d = −10 hoạt động tốt với GELU vì đầu ra dưới giá trị này gần bằng không.

Để xác thực giả thuyết của chúng tôi, chúng tôi tuân theo phương pháp từ Mục 4.3 và chúng tôi huấn luyện GPT-2 dựa trên ReLU và GELU có và không có mất mát thực thi độ thưa thớt. Chúng tôi hiển thị kết quả trong Hình 9c. D2DMoE với mô hình dựa trên GELU thưa thớt hóa hoạt động tương tự như mô hình dựa trên ReLU thưa thớt hóa, trong khi hiệu suất của biến thể dựa trên GELU không thưa thớt hóa sụp đổ. Trong các mô hình dựa trên ReLU, việc thưa thớt hóa vẫn tăng cường hiệu suất của D2DMoE, nhưng những cải thiện ít drastic hơn, và hành vi của phương pháp chúng tôi không thay đổi đáng kể như trong trường hợp của GELU. Điều này chỉ ra rằng việc thực thi độ thưa thớt kích hoạt đầy đủ giải phóng mô hình khỏi sự phụ thuộc vào ReLU.

5.5 Tác động của độ chi tiết expert
Một siêu tham số quan trọng trong D2DMoE là việc lựa chọn kích thước expert. Các expert nhỏ hơn có thể cho phép lựa chọn chi tiết hơn các neuron được thực thi, có thể dẫn đến chi phí tính toán thấp hơn. Tuy nhiên, giảm kích thước expert tăng số lượng expert, điều này chuyển đổi thành router lớn hơn, có thể phủ nhận bất kỳ lợi ích tính toán nào. Để nghiên cứu tác động của siêu tham số này đối với phương pháp của chúng tôi, chúng tôi đánh giá D2DMoE trên GPT-2 với các kích thước expert khác nhau, và hiển thị kết quả trong Hình 9d. Chúng tôi quan sát thấy rằng phương pháp của chúng tôi thường hoạt động tốt hơn với các expert nhỏ hơn. Những kết quả này khác với những kết quả được trình bày trong [52], nơi kích thước expert cao hơn đáng kể. Mối tương quan tích cực giữa độ chi tiết và hiệu suất có thể được giải thích bởi các mức độ thưa thớt kích hoạt tăng trong mô hình của chúng tôi, điều này yêu cầu ít neuron được kích hoạt (expert) hơn đáng kể. Như mong đợi, hiệu suất giảm cho lựa chọn cực đoan về kích thước expert bằng 1 do chi phí định tuyến cao hơn đáng kể. Chúng tôi bao gồm kết quả bổ sung cho độ chi tiết expert với kích hoạt GELU trong Phụ lục H.

6 Công trình liên quan
Mixture-of-Experts. Các lớp MoE được giới thiệu như một cách hiệu quả để tăng thêm khả năng của các mạng neural sâu được áp dụng cho các nhiệm vụ NLP, ban đầu trong các mô hình LSTM [38], và sau đó trong Transformer [23]. Kể từ đó, chúng cũng đã được áp dụng cho thị giác máy tính [33,5]. Các lớp MoE đã thu hút sự phổ biến đáng kể chủ yếu do các tính chất mở rộng tuyệt vời của chúng [8,3]. Tuy nhiên, việc huấn luyện các mô hình như vậy là thách thức, chủ yếu vì các quyết định gating phải rời rạc để đảm bảo lựa chọn expert thưa thớt. Các phương pháp huấn luyện khác nhau đã được đề xuất, một số trong số đó bao gồm học tăng cường [1], trọng số đầu ra expert bằng xác suất để cho phép tính toán gradient của router [38], hoặc sử dụng thuật toán Sinkhorn [3]. Một số phương pháp đó cũng gặp phải khả năng mất cân bằng tải và do đó yêu cầu các mất mát phụ trợ hoặc các phương pháp lựa chọn expert thay thế [9,54]. Thú vị là, trong nhiều trường hợp, các hàm định tuyến cố định hoạt động tương tự như các router có thể huấn luyện [34], điều này cho thấy rằng các giải pháp hiện tại phần lớn không tối ưu. Các mô hình MoE cũng có thể được suy ra từ các mô hình dense được huấn luyện trước bằng cách tách các trọng số mô hình thành các expert và huấn luyện độc lập các router cho mỗi lớp [52,57], điều này tránh hầu hết các vấn đề có mặt trong huấn luyện đầu cuối đến đầu cuối.

Độ thưa thớt kích hoạt trong Transformer. Li et al. [24] chỉ ra rằng các mô hình Transformer dựa trên ReLU tạo ra các kích hoạt thưa thớt trong các biểu diễn trung gian của chúng, một hiệu ứng phổ biến qua các kiến trúc, lớp và phương thức. Họ đề xuất một quy tắc đơn giản để chỉ giữ lại các kích hoạt top-k trong mỗi lớp MLP, dẫn đến một mô hình với hiệu suất tương đương. Tương tự, Mirzadeh et al. [27] chứng minh rằng hàm kích hoạt ReLU trong LLM khuyến khích độ thưa thớt kích hoạt tiếp theo có thể được tận dụng để bỏ qua các tính toán thừa. Tuli và Jha [46] tiến thêm một bước và thiết kế một bộ tăng tốc kiến trúc Transformer chuyên dụng cũng khai thác độ thưa thớt kích hoạt, trong khi Liu et al. [25] đề xuất dự đoán cấu trúc độ thưa thớt kích hoạt trong LLM và giảm độ trễ mô hình bằng cách bỏ qua các tính toán thừa. Jaszczur et al. [18] chứng minh rằng có thể huấn luyện các mô hình Transformer từ đầu với một mức độ thưa thớt kích hoạt cố định và thu được hiệu suất tương tự. Cuối cùng, một dòng công trình liên quan tập trung vào độ thưa thớt trong các phân phối attention thay vì các biểu diễn trung gian [4]. Không có phương pháp nào được đề cập ở trên khám phá độ thưa thớt kích hoạt được tạo ra như một cách để tăng lợi ích tính toán, cũng như chúng không giải quyết phương sai của số lượng kích hoạt thưa thớt trên cơ sở từng token.

7 Kết luận
Chúng tôi giới thiệu Dense to Dynamic-k Mixture-of-Experts (D2DMoE), một phương pháp mới tạo ra độ thưa thớt kích hoạt để cải thiện hiệu quả của các mô hình dựa trên Transformer bằng cách chuyển đổi các lớp của chúng thành Mixture-of-Experts (MoE). Chúng tôi chứng minh sự tương tác giữa độ thưa thớt kích hoạt của các mô hình dense và hiệu quả của các MoE được chuyển đổi. Hơn nữa, chúng tôi giới thiệu huấn luyện router dựa trên hồi quy và định tuyến dynamic-k, cho phép phương pháp của chúng tôi sử dụng hiệu quả độ thưa thớt được tạo ra. Cuối cùng, chúng tôi chỉ ra cách các phương pháp chuyển đổi dense-to-sparse-MoE có thể được mở rộng cho các phép chiếu MHA và MLP có cổng. Phương pháp của chúng tôi tương thích với các kiến trúc Transformer hiện có và cải thiện đáng kể so với các sơ đồ chuyển đổi MoE hiện có. Những phát hiện của chúng tôi đóng góp vào các nỗ lực đang diễn ra để làm cho các mô hình Transformer hiệu quả hơn và dễ tiếp cận hơn cho một phạm vi ứng dụng rộng hơn, đặc biệt là trong các môi trường hạn chế tài nguyên.

Hạn chế và Tác động Rộng hơn
Mặc dù D2DMoE hiển thị kết quả hứa hẹn trong việc giảm chi phí tính toán suy luận trong các mô hình Transformer, một số hạn chế cần được thừa nhận. Các giai đoạn thực thi độ thưa thớt và huấn luyện router được đề xuất của chúng tôi yêu cầu thời gian huấn luyện bổ sung. Overhead này, mặc dù nhỏ, phải được xem xét khi đánh giá lợi ích của phương pháp chúng tôi. Hơn nữa, chúng tôi chứng minh hiệu suất được cải thiện so với các phương pháp hiện có trên các nhiệm vụ NLP và CV phổ biến, nhưng phạm vi thí nghiệm của chúng tôi bị hạn chế do truy cập hạn chế vào tài nguyên tính toán. Nghiên cứu thêm là cần thiết để khám phá khả năng ứng dụng của nó cho các mô hình cực lớn.

Công trình của chúng tôi tập trung chủ yếu vào nghiên cứu học máy cơ bản và chúng tôi không thấy bất kỳ rủi ro cụ thể hoặc vấn đề đạo đức nào liên quan đến phương pháp của chúng tôi. Tuy nhiên, chúng tôi nhận ra tiềm năng lạm dụng công nghệ học máy và ủng hộ các thực hành AI có trách nhiệm để giảm thiểu các rủi ro như vậy.

Lời cảm ơn
Filip Szatkowski được hỗ trợ bởi Trung tâm Khoa học Quốc gia (NCP, Ba Lan) Cấp phát số 2022/45/B/ST6/02817. Bartosz Wójcik được hỗ trợ bởi Trung tâm Khoa học Quốc gia (NCP, Ba Lan) Cấp phát số 2023/49/N/ST6/02513. Simone Scardapane được tài trợ một phần bởi cấp phát Sapienza RG123188B3EF6A80 (CENTS). Bài báo này đã được hỗ trợ bởi Chương trình Horizon Europe (HORIZON-CL4-2022-HUMAN-02) dưới dự án "ELIAS: European Lighthouse of AI for Sustainability", GA số 101120237. Vì mục đích Truy cập Mở, các tác giả đã áp dụng giấy phép bản quyền công cộng CC-BY cho bất kỳ phiên bản Bản thảo Được Chấp nhận của Tác giả (AAM) nào phát sinh từ bài nộp này.

Chúng tôi biết ơn cơ sở hạ tầng hiệu suất cao PLGrid của Ba Lan (Trung tâm HPC: ACK Cyfronet AGH, PCSS, CI TASK, WCSS) vì đã cung cấp các cơ sở máy tính và hỗ trợ trong các cấp phát tính toán số PLG/2023/016393, PLG/2023/016321, và PLG/2024/017385.

Tài liệu tham khảo
[1]Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.
[2]Xuanyao Chen, Zhijian Liu, Haotian Tang, Li Yi, Hang Zhao, and Song Han. Sparsevit: Revisiting activation sparsity for efficient high-resolution vision transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2061–2070, June 2023.
[3]Aidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for routed language models. In International Conference on Machine Learning, pages 4057–4086. PMLR, 2022.
[4]Gonçalo M Correia, Vlad Niculae, and André FT Martins. Adaptively sparse transformers. arXiv preprint arXiv:1909.00015, 2019.
[5]Erik Daxberger, Floris Weers, Bowen Zhang, Tom Gunter, Ruoming Pang, Marcin Eichner, Michael Emmersberger, Yinfei Yang, Alexander Toshev, and Xianzhi Du. Mobile v-moes: Scaling down vision transformers via sparse mixture-of-experts. arXiv preprint arXiv:2309.04354, 2023.
[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[7]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.
[8]Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 5547–5569. PMLR, 2022.
[9]William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232–5270, 2022.
[10] Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.
[11] Georgios Georgiadis. Accelerating convolutional neural networks via activation map compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7085–7095, 2019.
[12] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/OpenWebTextCorpus, 2019.
[13] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.
[14] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang. Dynamic neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(11):7436–7456, 2021.

--- TRANG 12 ---
[15] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.
[16] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. The Journal of Machine Learning Research, 22(1):10882–11005, 2021.
[17] Patrik O Hoyer. Non-negative matrix factorization with sparseness constraints. Journal of machine learning research, 5(9), 2004.
[18] Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech Gajewski, Henryk Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers. Advances in Neural Information Processing Systems, 34:9895–9907, 2021.
[19] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.
[20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
[21] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: A survey. ACM computing surveys (CSUR), 54(10s):1–41, 2022.
[22] Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William Leiserson, Sage Moore, Nir Shavit, and Dan Alistarh. Inducing and exploiting activation sparsity for fast inference on deep neural networks. In International Conference on Machine Learning, pages 5533–5543. PMLR, 2020.
[23] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2020.
[24] Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. The lazy neuron phenomenon: On emergence of activation sparsity in transformers. In The Eleventh International Conference on Learning Representations, 2022.
[25] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137–22176. PMLR, 2023.
[26] Mikko I Malinen and Pasi Fränti. Balanced k-means for clustering. In Structural, Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshop, S+ SSPR 2014, Joensuu, Finland, August 20-22, 2014. Proceedings, pages 32–41. Springer, 2014.
[27] Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, and Mehrdad Farajtabar. Relu strikes back: Exploiting activation sparsity in large language models. arXiv preprint arXiv:2310.04564, 2023.
[28] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021.
[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.

--- TRANG 13 ---
[30] Zihan Qiu, Zeyu Huang, and Jie Fu. Unlocking emergent modularity in large language models. In2024 Annual Conference of the North American Chapter of the ACL, 2024.
[31] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
[32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1–67, 2020.
[33] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:8583–8595, 2021.
[34] Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 34:17555–17566, 2021.
[35] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.
[36] Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3687–3697, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1404. URL https://www.aclweb.org/anthology/D18-1404.
[37] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.
[38] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2016.
[39] Zhuoran Song, Yihong Xu, Zhezhi He, Li Jiang, Naifeng Jing, and Xiaoyao Liang. Cp-vit: Cascade vision transformer pruning via progressive sparsity prediction. arXiv preprint arXiv:2203.04570, 2022.
[40] Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language models. arXiv preprint arXiv:2402.17762, 2024.
[41] Shawn Tan, Yikang Shen, Rameswar Panda, and Aaron Courville. Scattered mixture-of-experts implementation. arXiv preprint arXiv:2403.08245, 2024.
[42] Gemma Team. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024.
[43] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 10–19, 2019.
[44] Hugo Touvron, Matthieu Cord, and Hervé Jégou. Deit iii: Revenge of the vit. In European Conference on Computer Vision, pages 516–533. Springer, 2022.
[45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
[46] Shikhar Tuli and Niraj K Jha. Acceltran: A sparsity-aware accelerator for dynamic inference with transformers. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2023.

--- TRANG 14 ---
[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[48] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.
[49] Bartosz Wójcik, Marcin Przewie´ zlikowski, Filip Szatkowski, Maciej Wołczyk, Klaudia Bałazy, Bartłomiej Krzepkowski, Igor Podolak, Jacek Tabor, Marek ´Smieja, and Tomasz Trzci ´nski. Zero time waste in pre-trained early exit neural networks. Neural Networks, 168:580–601, 2023.
[50] Mengzhou Xia, Zexuan Zhong, and Danqi Chen. Structured pruning learns compact and accurate models. arXiv preprint arXiv:2204.00408, 2022.
[51] Hongxu Yin, Arash Vahdat, Jose M Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-vit: Adaptive tokens for efficient vision transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10809–10818, 2022.
[52] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moefication: Transformer feed-forward layers are mixtures of experts. In Findings of the Association for Computational Linguistics: ACL 2022, pages 877–890, 2022.
[53] Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Chaojun Xiao, Xiaozhi Wang, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, and Jie Zhou. Emergent modularity in pre-trained transformers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 4066–4083. Association for Computational Linguistics, July 2023.
[54] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:7103–7114, 2022.
[55] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In The IEEE International Conference on Computer Vision (ICCV), December 2015.
[56] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022.
[57] Simiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, and Weizhu Chen. MoE-BERT: from BERT to mixture-of-experts via importance-guided adaptation. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1610–1623, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.116. URL https://aclanthology.org/2022.naacl-main.116.

--- TRANG 15 ---
A Sự khác biệt giữa huấn luyện router trong D2DMoE và MoEfication
Quy trình huấn luyện router của chúng tôi tương tự như được đề xuất trong MoEfication [52], nhưng mã nguồn của phương pháp được cung cấp bởi các tác giả chứa một mục tiêu định tuyến khác so với được báo cáo trong bài báo. Trong khi bài báo mô tả mục tiêu huấn luyện router của họ là dự đoán tổng các giá trị kích hoạt ReLU trong mỗi expert, mã nguồn sử dụng nhãn dự đoán được tạo từ tổng các kích hoạt trong lớp trung gian chia cho giá trị tối đa trong toàn bộ batch và tối thiểu hóa mất mát binary cross-entropy. Giả sử rằng ak,j là vector kích hoạt trong lớp ẩn của expert j cho mẫu k, việc tạo nhãn cho router của họ có thể được biểu diễn như:

yk,j = (∑i ak,j,i) / (max l,m ∑i al,m,i)    (5)

So với phương pháp của họ, quy trình huấn luyện router trong D2DMoE khác biệt ở nhiều khía cạnh:
• Router của chúng tôi xem xét đầu ra của mỗi expert thay vì nhìn vào các kích hoạt trong các lớp trung gian.
• Thay vì sử dụng các nhãn được tạo nhân tạo dựa trên tổng các giá trị kích hoạt, chúng tôi dự đoán chuẩn ℓ2 của đầu ra. Điều này có lợi ích bổ sung là router của chúng tôi có thể làm việc với các hàm kích hoạt thay thế.
• Router của chúng tôi được huấn luyện với sai số bình phương trung bình thay vì mất mát binary cross-entropy. Đầu ra của router của chúng tôi bị hạn chế ở các giá trị dương, trong khi router MoEfication bị hạn chế ở đầu ra trong [0,1].

Chúng tôi thấy rằng những khác biệt trên chịu trách nhiệm cho hiệu suất được cải thiện của router chúng tôi (xem Hình 9a).

B So sánh FLOPs giữa lớp FFN tiêu chuẩn và MoE dynamic-k
Để so sánh hiệu quả của phương pháp chúng tôi với lớp MLP tiêu chuẩn trong Transformer, chúng tôi ước tính FLOPs trong cả hai mô-đun. Chúng tôi giả sử lớp bao gồm hai biến đổi tuyến tính, với kích thước đầu vào và đầu ra dm và chiều ẩn edm, nơi e là hệ số mở rộng, thường bằng 4 trong các mô hình Transformer tiêu chuẩn. Chúng tôi bỏ qua chi phí không đáng kể của bias và các hàm kích hoạt để đơn giản.

Có thể ước tính chi phí của lớp MLP tính bằng FLOPs, CMLP, như:
CFFN = dm · edm + edm · dm = dm² · 2e.    (6)

Đối với lựa chọn expert dynamic-k với ntotal expert và k expert được chọn cho một đầu vào nhất định, chi phí của forward pass bao gồm chi phí của forward pass thông qua k expert và chi phí của router 2 lớp với chiều đầu vào dm, chiều ẩn dh và chiều đầu ra n. Chi phí của một expert pass đơn có thể được biểu diễn như:

CE = (dm · edm/n + edm/n · dm) = dm² · 2e/n,    (7)

và chi phí định tuyến có thể được ước tính như:
CR = dm · dh + dh · n.    (8)

Do đó, chi phí đầy đủ của dynamic-k Cdynk có thể được ước tính như:
Cdynk = k · CE + CR = dm² · 2ek/n + dh(dm + n),    (9)

và chi phí của phương pháp chúng tôi so với chi phí của MLP tiêu chuẩn có thể được biểu diễn như:
Cdynk/CMLP = (dm² · 2ek/n + dh(dm + n))/(dm² · 2e)    (10)
= k/n + dh(1 + n/dm)/(dm · 2e).    (11)

Hình 10: Tỷ lệ FLOPs giữa lớp expert dynamic-k và MLP hai lớp tiêu chuẩn cho các giá trị khác nhau của tổng số expert n và số expert được chọn k. Chúng tôi giả sử chiều ẩn của router dh dựa trên chiều mô hình dm, và đặt hệ số mở rộng tiêu chuẩn e = 4. Đối với các kích thước router khác nhau, dynamic-k sử dụng ít FLOPs hơn MLP tiêu chuẩn miễn là tổng số expert đủ lớn và số expert được chọn không bằng tổng số expert. Để rõ ràng trong trình bày, chúng tôi vẽ các giá trị rời rạc của k và n như liên tục.

Miễn là số expert được chọn k không tiến tới tổng số expert n và chiều ẩn của router không tiến tới kích thước chiều ẩn dm, tỷ lệ này thấp hơn đáng kể so với một.

Giả sử trường hợp tệ nhất cho số hạng thứ hai (n = edm), chúng tôi có thể ước tính tỷ lệ chi phí như:
k/n + dh/dm · (1 + e)/(2e),    (12)

điều này chỉ ra rằng lựa chọn expert dynamic-k chỉ vượt quá chi phí FLOPs của mạng tiêu chuẩn khi quy tắc dynamic-k chọn hầu hết tất cả expert hoặc số lượng expert trở nên rất cao. Để phân tích chi tiết hơn, chúng tôi tham khảo Hình 10 nơi tỷ lệ chi phí giữa phương pháp chúng tôi và MLP tiêu chuẩn được hiển thị, giả sử các kích thước router khác nhau và e = 4 như tiêu chuẩn cho hầu hết các mô hình Transformer. Trong thực tế, chúng tôi sử dụng dh = 128, vì vậy trong tất cả các thí nghiệm của chúng tôi dm = 6dh.

C Triển khai hiệu quả của D2DMoE

Hình 11: Các phép đo thời gian wall-clock của mô hình ViT-B và mô hình D2DMoE tương ứng.

Trong Listing 1, chúng tôi trình bày pseudocode cho triển khai hiệu quả của forward pass của mô-đun D2DMoE. Chúng tôi bỏ qua pseudocode của kernel của lớp thứ hai vì nó tương tự, nhưng cung cấp mã nguồn đầy đủ trong kho mã của chúng tôi. Lưu ý rằng triển khai của chúng tôi có nhiều điểm có thể được cải thiện để có thêm lợi ích hiệu suất: 1) metadata yêu cầu cho các kernel cũng có thể được tính toán với một kernel chuyên dụng để giảm overhead; 2) các phép toán atomic hiện được sử dụng trong lớp thứ hai để hợp nhất kết quả từ các expert khác nhau, một triển khai thay thế không sử dụng phép toán atomic có thể nhanh hơn; 3) nó có thể được viết lại trong CUDA để sử dụng dynamic parallelism. Chúng tôi để lại những cải tiến này cho công việc tương lai.

Trong bài báo chính, chúng tôi đã trình bày các phép đo thời gian wall-clock của một lớp D2DMoE đơn lẻ. Dưới đây, chúng tôi cũng đảm bảo rằng triển khai của chúng tôi hoạt động và hoạt động tốt khi được sử dụng cho mô hình ViT-B trong đó mỗi FFN được thay thế bằng một mô-đun D2DMoE. Trong Hình 11, chúng tôi đo thời gian xử lý trung bình và độ chính xác của mô hình chúng tôi. Chúng tôi thực hiện các thí nghiệm trên GPU NVIDIA A100 sử dụng kích thước batch là 256. Mỗi điểm trên trục x tương ứng với một ngưỡng τ đơn lẻ và hiển thị thời gian wall-clock xử lý một đầu vào đơn lẻ được tính trung bình trên toàn bộ tập kiểm tra ImageNet-1k. Suy luận động với D2DMoE cung cấp lên đến 30% giảm thời gian xử lý mà không ảnh hưởng đến độ chính xác.

Để chỉ ra rằng D2DMoE cũng giảm độ trễ thực thi của các mô hình được lượng tử hóa, chúng tôi sửa đổi kernel của chúng tôi để xử lý các loại dữ liệu float16 và int8. Trong Bảng 2, chúng tôi thực hiện một thí nghiệm tương tự như trong Hình 5. Chúng tôi lấy mẫu các quyết định gating từ phân phối Bernoulli với xác suất p và đo thời gian thực thi của các expert cho ba biến thể kiểu dữ liệu.

Bảng 2: Các phép đo thời gian wall-clock (µs) của việc thực thi lớp D2DMoE khi sử dụng các kiểu dữ liệu và GPU khác nhau.

GPU | p | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0
RTX 4090 | float32 | 5 | 9 | 13 | 18 | 23 | 28 | 33 | 38 | 42 | 47 | 52
| float16 | 4 | 5 | 7 | 9 | 11 | 14 | 16 | 18 | 21 | 24 | 27
| int8 | 4 | 4 | 5 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14
A100 | float32 | 6 | 9 | 12 | 15 | 19 | 22 | 25 | 28 | 31 | 35 | 38
| float16 | 6 | 7 | 8 | 10 | 11 | 13 | 14 | 16 | 17 | 19 | 21
| int8 | 7 | 8 | 9 | 10 | 11 | 12 | 14 | 15 | 16 | 17 | 19

Kết quả chỉ ra rằng cả độ thưa thớt kích hoạt cao hơn (p thấp hơn) của phương pháp chúng tôi và các kiểu dữ liệu độ chính xác thấp hơn đều bổ sung về mặt giảm thời gian wall-clock. Mặc dù chúng tôi thấy cải thiện nhỏ hơn từ việc sử dụng int8 so với float16 trên A100, chúng tôi cho rằng điều này do sự khác biệt giữa kiến trúc GPU và hỗ trợ phần mềm cho số học độ chính xác thấp.

D Khả năng tương thích với distillation kiến thức

Hình 12: Hiệu suất của D2DMoE được áp dụng trên ViT-S được distill từ mô hình ViT-B lớn hơn.

Trong Mục 4.5, chúng tôi đã chứng minh rằng phương pháp của chúng tôi tương thích với hai kỹ thuật nén mô hình phổ biến: lượng tử hóa và tỉa. Một câu hỏi tự nhiên là liệu phương pháp của chúng tôi có thể được áp dụng hiệu quả cho các mô hình được nén thông qua distillation kiến thức hay không. Vì các mô hình được distill cũng thể hiện độ thưa thớt kích hoạt mà phương pháp chúng tôi dựa vào, D2DMoE nên có thể áp dụng cho các mô hình như vậy. Trong Hình 12, chúng tôi chứng minh kết quả của D2DMoE khi được áp dụng trên mô hình ViT-S, đã được huấn luyện thông qua distillation kiến thức [15] với torchvision ViT-B được sử dụng làm mô hình giáo viên. Chúng tôi thấy rằng D2DMoE cũng có thể giảm chi phí của mô hình nhỏ hơn này.

E Phân tích định tuyến cho các mô hình lớn

Hình 13: So sánh hiệu suất trên Gemma-2B cho MoEfication với định tuyến vanilla và với định tuyến hồi quy của chúng tôi.

Như được trình bày trong Hình 4d, so với các benchmark khác được xem xét, MoEfication hoạt động kém rõ ràng trên mô hình ngôn ngữ với Gemma-2B. Chúng tôi cho rằng điều này do sự xuất hiện của các kích hoạt lớn trong LLM đạt đến một quy mô cụ thể [40]. Các kích hoạt lớn là các ngoại lệ dọc theo các chiều đặc trưng nhất định có độ lớn lớn hơn hàng nghìn lần so với độ lớn của các kích hoạt khác. Mục tiêu huấn luyện của MoEfication được mô tả trong Phương trình (5) sử dụng kích hoạt tối đa trên toàn bộ batch để chuẩn hóa nhãn mục tiêu cho mỗi expert. Khi gặp các giá trị ngoại lệ lớn, những nhãn đó trở nên vô nghĩa một cách hiệu quả, vì các giá trị cho hầu hết các expert trở nên rất gần với không. Trong trường hợp này, router hiệu quả học xuất ra nhãn không cho hầu hết các expert ngoài những nhãn tương ứng với các giá trị ngoại lệ.

So với MoEfication, sơ đồ huấn luyện router của chúng tôi không sử dụng chuẩn hóa như vậy, và do đó nên mạnh mẽ đối với sự xuất hiện của các kích hoạt lớn. Để xác thực điều này, chúng tôi áp dụng MoEfication trên Gemma-2B, nhưng với định tuyến hồi quy của chúng tôi thay vì chiến lược huấn luyện router ban đầu. Chúng tôi so sánh mô hình kết quả với MoEfication vanilla trong Hình 13 và nhận thấy rằng việc thay thế sơ đồ định tuyến đủ để mô hình học được phân công expert hiệu quả, vì mặc dù lựa chọn expert tĩnh và mô hình cơ sở không được thưa thớt hóa, sự cân bằng chi phí-mất mát đã được cải thiện đáng kể. Thí nghiệm đơn giản này chỉ ra rằng mục tiêu định tuyến hồi quy của chúng tôi mạnh mẽ hơn MoEfication khi mở rộng lên các mô hình lớn hơn.

--- TRANG 18 ---
Hình 14: Các ablation bổ sung với kiến trúc router và độ chi tiết expert.

F Kiến trúc router
So với các router tuyến tính tiêu chuẩn được sử dụng trong các mô hình MoE được huấn luyện từ đầu, các router trong MoEfication là MLP 2 lớp. Để đạt được hiệu suất tốt nhất với D2DMoE, chúng tôi so sánh thiết kế tuyến tính với MLP có các kích thước ẩn khác nhau cho BERT-base và GPT-2-base trong Hình 14a và 14b tương ứng. Các router tuyến tính không hoạt động tốt với phương pháp của chúng tôi, và tổng thể một MLP 2 lớp với chiều ẩn 128 dẫn đến hiệu suất tốt nhất cho cả hai mô hình. Lưu ý cách đối với BERT-base, đường cong độ chính xác cho mô hình với chiều ẩn 128 hơi tệ hơn so với các router nhỏ hơn, nhưng đối với nhiệm vụ khó hơn với GPT-2 một router phức tạp hơn là bắt buộc. Theo phân tích này, chúng tôi sử dụng MLP 2 lớp với chiều ẩn 128 cho hầu hết các thí nghiệm trong bài báo, với ngoại lệ duy nhất là mô hình Gemma-2B lớn hơn nơi chúng tôi mở rộng chiều ẩn tương ứng lên 512 để phù hợp với sự tăng chiều mô hình.

G Mở rộng D2DMoE cho các lớp dựa trên GLU

Hình 15: Mở rộng D2DMoE cho Gated MLP.

Để cung cấp trực giác tốt hơn về việc mở rộng phương pháp của chúng tôi cho các MLP gated dựa trên GLU được đề cập trong Mục 3.2, chúng tôi hình ảnh hóa sự khác biệt giữa FFN tiêu chuẩn và Gated FFN và việc áp dụng phương pháp của chúng tôi trong Hình 15. Transformer MLP tiêu chuẩn thực hiện hàm sau:

y(x) = W1A(W2x),    (13)

nơi W1, W2 là các trọng số cho các phép chiếu upscale và downscale và A đại diện cho hàm kích hoạt. Ngược lại, gated MLP có thể được viết như:

y(x) = W1(A(Wgx) ◦ W2x),    (14)

nơi Wg là trọng số cho phép chiếu cổng được thêm vào.

Trực giác đằng sau MoEfication, mà phương pháp của chúng tôi cũng tuân theo cho FFN tiêu chuẩn, là độ thưa thớt của các biểu diễn trung gian, sau kích hoạt xác định độ thưa thớt của biểu diễn đầu ra. Do đó, việc tách expert được thực hiện dựa trên các trọng số của phép chiếu upscale, vì các neuron bằng không trong các kích hoạt upscale cũng sẽ dẫn đến đầu ra bằng không của phép chiếu downscale.

Khi mở rộng D2DMoE cho Gated MLP, trực giác của chúng tôi là các phép chiếu gating xác định độ thưa thớt của tất cả các biểu diễn sau đó, vì cả upscale và downscale đều được nhân với các giá trị gating. Do đó, chúng tôi đề xuất xây dựng các expert thông qua phân cụm được thực hiện trên các trọng số gating Wg và sử dụng các chỉ số thu được thông qua việc tách expert trên các trọng số gating để xây dựng các expert từ W1 và W2. Theo lý luận tương tự, đối với các mô hình dựa trên GLU, chúng tôi cũng thực hiện thực thi độ thưa thớt kích hoạt trên các phép chiếu gating thay vì các phép chiếu upscale như được mô tả ban đầu trong Mục 3.1.

H Kết quả bổ sung với kích thước expert và GELU
Ngoài các thí nghiệm trong Mục 5.5, chúng tôi trình bày kết quả của ablation tương tự được thực hiện trên mô hình GPT-2 thưa thớt hóa với kích hoạt GELU. Kết quả, được trình bày trong Hình 14c, tuân theo cùng một mẫu như trước, điều này hỗ trợ tuyên bố của chúng tôi rằng việc thưa thớt hóa cho phép các mô hình dựa trên GELU hoạt động tương tự như các mô hình dựa trên ReLU.

I Các mẫu kích hoạt expert cho các lớp phép chiếu attention
Theo phân tích cho các lớp FFN được chuyển đổi MoE trong Mục 5.1, chúng tôi trình bày kết quả đầy đủ cho FFN trong Hình 16, và điều tra các mẫu kích hoạt trong các phép chiếu MHA được sửa đổi với phương pháp của chúng tôi trong Hình 17 đến 20. Các mô-đun phép chiếu hiển thị mức độ thưa thớt thấp hơn so với FFN, điều này là mong đợi vì các lớp phép chiếu của chúng tôi có chiều trung gian thấp hơn. Các mẫu phân phối lựa chọn expert trong Q và K cho thấy sự tương tự đáng kể, và các mẫu trong V và các phép chiếu đầu ra cũng tương tự ở mức độ thấp hơn. Phương sai của số lượng expert được chọn trong các phép chiếu MHA cao hơn so với trong các lớp FFN, nhưng nó vẫn tồn tại và phân phối trong một số lớp có vẻ như là lưỡng cực, cung cấp biện minh thêm cho quy tắc lựa chọn dynamic-k.

J Chi tiết huấn luyện và phần cứng
Trong Mục này, chúng tôi mô tả các chi tiết kỹ thuật được sử dụng trong quy trình chuyển đổi D2DMoE. Để có thể tái tạo hoàn toàn, chúng tôi chia sẻ mã nguồn mà chúng tôi đã sử dụng để tiến hành các thí nghiệm. Tất cả các thí nghiệm được thực hiện sử dụng thư viện PyTorch [29] trên GPU NVIDIA A100 và V100 trên các cụm nội bộ. Chúng tôi sử dụng thư viện fvcore để đếm FLOPs của mô hình.

J.1 Phân loại hình ảnh
Tất cả các phương pháp bắt đầu với cùng một ViT-B được huấn luyện trước từ thư viện torchvision và được huấn luyện trên ImageNet-1k sử dụng augmentation được đề xuất bởi Touvron et al. [44]. Chúng tôi sử dụng mixup (0.8), cutmix, label smoothing (0.1), gradient clipping (1.0) và optimizer Adam với lịch học rate cosine không có warm-up. Đối với D2DMoE, chúng tôi thay thế các phép chiếu MHA và huấn luyện các thay thế trong 3 epoch với learning rate ban đầu 0.001 và kích thước batch 128, và sau đó tinh chỉnh mô hình trong 90 epoch với trọng số thực thi độ thưa thớt α = 0.2, learning rate ban đầu 2·10⁻⁵ và kích thước batch 512. Sau đó chúng tôi chuyển đổi các mô-đun thành các lớp MoE, và huấn luyện các mạng gating trong 7 epoch với learning rate ban đầu được đặt thành 0.001 và kích thước batch 128. Chúng tôi huấn luyện ZTW trong 100 epoch tổng cộng, phân bổ 5 epoch cho huấn luyện ensemble, trong khi giữ nguyên các siêu tham số ban đầu khác. Đối với MoEfication, chúng tôi đầu tiên chuyển đổi mô hình được huấn luyện trước thành mô hình dựa trên ReLU và tinh chỉnh trong 90 epoch với learning rate ban đầu 0.0001 và kích thước batch 256. Sau đó chúng tôi tách các trọng số và huấn luyện các router trong 10 epoch với learning rate ban đầu 0.001 và kích thước batch 256.

J.2 Phân loại văn bản
Tất cả các thí nghiệm bắt đầu từ cùng một checkpoint BERT-base được huấn luyện trước. Đối với các phương pháp yêu cầu hàm kích hoạt ReLU, chúng tôi thay thế GELU bằng ReLU và tiếp tục huấn luyện trước mô hình trên các corpus wikipedia [10] và books [55] được kết hợp trong 5000 bước trên 8 GPU sử dụng thiết lập chính từ https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py, kích thước batch mỗi thiết bị 96 và learning rate 5·10⁻⁴. Để thay thế phép chiếu MHA, chúng tôi sử dụng cùng corpus và huấn luyện các mô-đun MLP thay thế trên một GPU đơn lẻ với kích thước batch 128 và learning rate 0.001 trong 3000 bước. Chúng tôi tinh chỉnh các mô hình dense cơ sở trên tập dữ liệu CARER trong 5 epoch với learning rate 2·10⁻⁵. Để thực thi độ thưa thớt trong D2DMoE, chúng tôi sử dụng α tăng tuyến tính từ không lên 0.0001 trong quá trình huấn luyện. Đối với cả MoEfication và D2DMoE, chúng tôi huấn luyện router với kích thước batch 64 và learning rate ban đầu 0.001 trong 5 epoch. Trong tất cả các thí nghiệm, chúng tôi sử dụng optimizer Adam với decay learning rate tuyến tính. Đối với MoEfication, chúng tôi sử dụng kích thước expert 32, đối với D2DMoE chúng tôi sử dụng kích thước expert chi tiết hơn bằng 6. Đối với ZTW, chúng tôi huấn luyện IC trong 5 epoch với kích thước batch 32 và learning rate 0.01.

J.3 Mô hình ngôn ngữ
Chúng tôi dựa mã và siêu tham số cho GPT2-base trên kho nanoGPT được cung cấp tại https://github.com/karpathy/nanoGPT. Chúng tôi khởi tạo mô hình từ https://huggingface.co/openai-community/gpt2. Trong tất cả các thí nghiệm huấn luyện trước, chúng tôi khởi tạo các mô hình từ checkpoint OpenAI có sẵn công khai được huấn luyện trước trên tập dữ liệu WebText nguồn đóng và tinh chỉnh trong số bước cố định 1000 với kích thước batch hiệu quả bằng giá trị trong kho thông qua gradient accumulation. Các giá trị alpha cho thực thi độ thưa thớt có thể được tìm thấy tại Hình 9b. Chúng tôi huấn luyện các router cho D2DMoE và MoEfication trong 2000 bước sử dụng một GPU và điều chỉnh các learning rate cho kích thước expert nhất định từ phạm vi giữa 0.002−0.005. Để huấn luyện router, chúng tôi sử dụng optimizer Adam và scheduler warmup cosine.

Đối với Gemma-2B, chúng tôi bắt đầu từ checkpoint tại https://huggingface.co/google/gemma-2b. Chúng tôi cũng tinh chỉnh mô hình trong 1k bước với kích thước batch hiệu quả 1024, độ dài chuỗi 1024 và optimizer Adam với learning rate 1e-4. Vì chiều ẩn của Gemma lớn hơn nhiều so với các mô hình khác được xem xét, chúng tôi thay đổi chiều ẩn của các router thành 512 cho cả phương pháp của chúng tôi và MoEfication, nhưng giữ nguyên các siêu tham số khác giống như trong phần còn lại của các thí nghiệm. Đối với MoEfication, Gemma, chúng tôi sử dụng 512 expert để có được kích thước expert tương đương với trong bài báo của họ. Đối với phương pháp của chúng tôi, chúng tôi sử dụng 2048 expert. Trong D2DMoE, chúng tôi đặt trọng số thực thi độ thưa thớt thành 0.00003. Chúng tôi huấn luyện các router trong 500 bước với Adam và kích thước batch hiệu quả 16 và sử dụng learning rate 0.001.

Chúng tôi báo cáo kết quả cho mô hình ngôn ngữ mà không có bước thay thế phép chiếu MHA, vì chúng tôi thấy rằng nó đặc biệt nhạy cảm với các thay đổi trong các lớp attention, luôn dẫn đến sự suy giảm mất mát rõ ràng.

K Đóng góp
Filip tích hợp codebase và chạy các thí nghiệm cho GPT-2 và Gemma, thực hiện phân tích độ thưa thớt kích hoạt, và tất cả các phân tích trên các mô hình mô hình ngôn ngữ. Anh đóng góp vào thiết kế gating dynamic-k và đóng vai trò chính trong việc thiết kế các thí nghiệm và viết bài báo.

Bartosz đặt hướng nghiên cứu của dự án và đề xuất sơ đồ định tuyến thay thế, lựa chọn expert dynamic-k, giai đoạn thực thi độ thưa thớt kích hoạt bổ sung cho ReLU và GELU, và việc thay thế các lớp phép chiếu MHA. Anh viết codebase chung cho các thí nghiệm, thực hiện các thí nghiệm ViT-B, triển khai các kernel Triton tùy chỉnh cho triển khai hiệu quả của phương pháp, và cũng đóng vai trò chính trong việc viết và chỉnh sửa bài báo.

Mikołaj làm cho bài báo này có thể bằng cách thực hiện tất cả các thí nghiệm ở các giai đoạn ban đầu của dự án và triển khai MoEfication và nhiều biến thể của phương pháp chúng tôi. Anh thực hiện các thí nghiệm BERT, thực hiện phân tích tương thích độ thưa thớt trọng số, nghiên cứu ablation, và đóng góp vào việc tạo ra bài báo.

Simone cải thiện đáng kể khả năng đọc của bài báo và cung cấp lời khuyên vô giá để sửa đổi nó.

--- TRANG 21 ---
1def forward_triton_atomic(self, x, routing_tensor):
2    # tính toán metadata cần thiết
3    # chia batch thành hai nhóm: được thực thi bởi expert đó hay không
4    # (cho mỗi expert độc lập)
5    sort_indices = routing_tensor.argsort(dim=0, descending=True)
6    # lấy số lượng mẫu được thực thi bởi mỗi expert
7    expert_bincounts = routing_tensor.sum(dim=0)
8    # forward pass thực tế
9    intermediate_acts = MoeFirstLayerImplementation.apply(...)
10   final_out = MoeSecondLayerAtomicImplementation.apply(...)
11   return final_out
12
13class MoeFirstLayerImplementation(torch.autograd.Function):
14   @staticmethod
15   def forward(input, weight, bias, sort_indices, expert_bincounts):
16       ...
17       # một grid các instance kernel chia công việc tính toán
18       # trong nhiều chiều: chiều batch (sample_dim),
19       # chiều đầu ra (expert_dim) và chiều số lượng expert (num_experts)
20       grid = (cdiv(sample_dim, BLOCK_SIZE_BD) *
21               cdiv(expert_dim, BLOCK_SIZE_ED), num_experts)
22       moe_first_kernel[grid](...)
23       ...
24
25@triton.jit
26def moe_first_kernel(x_ptr, ...
27                    weight_ptr, ...
28                    bias_ptr, ...
29                    output_ptr, ...
30                    sort_indices_ptr, ...
31                    expert_bincounts_ptr,
32                    ...,
33                    ):
34   # dựa trên tl.program_id(axis=0), tính toán các chỉ số tile
35   # cho các chiều batch và đầu ra
36   # (được nhóm, column major hoặc row major ordering)
37   pid_bd, pid_ed = ...
38   # các instance kernel và expert có mối quan hệ nhiều-một
39   expert_index = tl.program_id(axis=1)
40   # tải tổng số token được gán cho expert này
41   expert_samples_count = tl.load(expert_bincounts_ptr + expert_index)
42   # tính số lượng instance cần được sử dụng để xử lý tất cả token
43   bd_pids_for_expert = tl.cdiv(expert_samples_count, BLOCK_SIZE_BD)
44   # các instance không có tính toán để thực hiện thoát sớm
45   if pid_bd < bd_pids_for_expert:
46       # tính toán offset sẽ được sử dụng để định địa chỉ dữ liệu trong bộ nhớ
47       offs_bd = ...
48       offs_ed = ...
49       offs_hd = ...
50       # chọn dữ liệu để tải dựa trên sort indices
51       in_data_indices = tl.load(sort_indices_ptr + expert_index * ... + offs_bd * ...)
52       # tính toán địa chỉ bộ nhớ của dữ liệu đầu vào và trọng số
53       # trong quá trình tải, điều này sẽ nhóm các mẫu chỉ cho learner hiện tại
54       x_ptrs = x_ptr + in_data_indices[:, None] * ...
55       w_ptrs = weight_ptr + expert_index * ...
56       # kết quả sẽ được tích lũy trong biến này
57       accumulator = tl.zeros((BLOCK_SIZE_BD, BLOCK_SIZE_ED), dtype=tl.float32)
58       # lặp qua chiều trong cùng
59       for k in range(0, tl.cdiv(hidden_dim, BLOCK_SIZE_HD)):
60           # tải bộ nhớ từ các tile đầu vào và trọng số hiện tại
61           x = tl.load(x_ptrs, mask=..., other=0.0)
62           w = tl.load(w_ptrs, mask=..., other=0.0)
63           # thực hiện phép nhân ma trận cho các tile này và tích lũy
64           # (vì dữ liệu được nhóm, điều này có thể được thực hiện một cách hiệu quả)
65           accumulator += tl.dot(x, w)
66           # tiến các pointer đến tile tiếp theo
67           x_ptrs += BLOCK_SIZE_HD * stride_x_hd
68           w_ptrs += BLOCK_SIZE_HD * stride_weight_hd
69       # tải và thêm bias vào kết quả tích lũy
70       offs_b_ed = ...
71       b_ptrs = bias_ptr + expert_index * ...
72       accumulator += tl.load(b_ptrs, mask=..., other=0.0)
73       # áp dụng hàm kích hoạt trên kết quả
74       if ACTIVATION == 'relu':
75           accumulator = relu(accumulator)
76       ...
77       # tính toán địa chỉ bộ nhớ cho đầu ra
78       offs_out_bd = ...
79       out_ptrs = output_ptr + expert_index * ... + \
80                  offs_out_bd[:, None] * ... + offs_b_ed[None, :] * ...
81       out_mask = ...
82       # lưu trữ kết quả vào bộ nhớ GPU chính
83       tl.store(out_ptrs, accumulator, mask=out_mask)
84

Listing 1: Pseudocode đơn giản hóa của triển khai D2DMoE hiệu quả cho GPU

--- TRANG 22 ---
Hình 16: Phân phối theo lớp của số lượng expert được thực thi trong D2DMoE được huấn luyện trên CARER với các ngưỡng τ khác nhau cho mô hình tiêu chuẩn, không thưa thớt hóa (hàng trên) và mô hình thưa thớt hóa (hàng dưới). Tính biến đổi cao của số lượng đó giải thích các lợi ích tính toán từ việc sử dụng dynamic-k.

Hình 17: Phân phối số lượng expert được thực thi trong mỗi lớp cho các phép chiếu query.

Hình 18: Phân phối số lượng expert được thực thi trong mỗi lớp cho các phép chiếu key.

--- TRANG 23 ---
Hình 19: Phân phối số lượng expert được thực thi trong mỗi lớp cho các phép chiếu value.

Hình 20: Phân phối số lượng expert được thực thi trong mỗi lớp cho các phép chiếu output.
