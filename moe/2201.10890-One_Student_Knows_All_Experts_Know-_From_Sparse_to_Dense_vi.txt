# Một Sinh Viên Biết Tất Cả Những Gì Các Chuyên Gia Biết: Từ Thưa Thớt Đến Dày Đặc

Fuzhao Xue¹, Xiaoxin He¹, Xiaozhe Ren², Yuxuan Lou¹, Yang You¹
¹Khoa Khoa học Máy tính, Đại học Quốc gia Singapore
²Phòng thí nghiệm Bahtera Noah của Huawei

## Tóm tắt

Hệ thống giáo dục con người đào tạo một học sinh bởi nhiều chuyên gia. Mixture-of-experts (MoE) là một kiến trúc thưa thớt mạnh mẽ bao gồm nhiều chuyên gia. Tuy nhiên, mô hình MoE thưa thớt dễ bị quá khớp, khó triển khai, và không thân thiện với phần cứng đối với các nhà thực hành. Trong nghiên cứu này, lấy cảm hứng từ mô hình giáo dục con người, chúng tôi đề xuất một nhiệm vụ mới, tích hợp kiến thức, để có được một mô hình sinh viên dày đặc (OneS) có kiến thức như một MoE thưa thớt. Chúng tôi nghiên cứu nhiệm vụ này bằng cách đề xuất một khung đào tạo tổng quát bao gồm thu thập kiến thức và chưng cất kiến thức. Cụ thể, để thu thập kiến thức chính từ các chuyên gia đã được đào tạo trước khác nhau, chúng tôi đầu tiên nghiên cứu bốn phương pháp thu thập kiến thức có thể khác nhau, tức là tổng, lấy trung bình, Thu thập Kiến thức Top-K (Top-KG), và Thu thập Kiến thức Phân tích Giá trị Đặc biệt (SVD-KG) được đề xuất trong bài báo này. Sau đó chúng tôi tinh chỉnh mô hình sinh viên dày đặc bằng chưng cất kiến thức để bù trừ nhiễu từ việc thu thập. Trên ImageNet, OneS của chúng tôi bảo tồn 61.7% lợi ích từ MoE và đạt được 78.4% độ chính xác top-1 ImageNet chỉ với 15M tham số. Trên bốn tập dữ liệu xử lý ngôn ngữ tự nhiên, OneS có được 88.2% lợi ích MoE và vượt trội hơn đường cơ sở tốt nhất 51.7% sử dụng cùng kiến trúc và dữ liệu đào tạo. Ngoài ra, so với đối tác MoE, OneS có thể đạt được tăng tốc suy luận 3.7× do ít tính toán hơn và kiến trúc thân thiện với phần cứng.

## 1. Giới thiệu

Nhìn lại cách chúng ta trở thành một nhà nghiên cứu, hầu hết mọi người học từ nhiều giáo viên (tức là các chuyên gia). Nghiên cứu hiện tại [2] trong giáo dục cũng cho thấy rằng các chuyên gia từ các môn học khác nhau có thể giúp học sinh đạt được hiểu biết sâu sắc và đào tạo nhiều tài năng hơn. Những học sinh tích hợp kiến thức từ các chuyên gia có thể trở nên có kiến thức như tập hợp các chuyên gia này một cách nhanh chóng. Lấy cảm hứng từ mô hình giáo dục con người như vậy, công trình này tập trung vào việc đào tạo một mô hình học sâu mạnh mẽ bằng cách thu thập kiến thức từ một tập hợp các chuyên gia.

Nghiên cứu gần đây trong học sâu đề xuất mixture-of-experts (MoE), một mạng nơ-ron sâu với nhiều chuyên gia. Mỗi chuyên gia là một mạng nơ-ron con trong toàn bộ mô hình. Ý tưởng chính của MoE là chia để trị nhiệm vụ. MoE khuyến khích mỗi chuyên gia học từ một tập con đầu vào cụ thể cho nhiệm vụ. Đối với mỗi tập con của đầu vào, sẽ chỉ có một mạng con được kích hoạt. Việc tính toán thưa thớt như vậy của MoE cho phép chúng ta mở rộng mô hình lên hàng nghìn tỷ tham số với chi phí tính toán tương đương [8].

Mô hình MoE mạnh mẽ và đạt được kết quả đầy hứa hẹn do dung lượng mô hình lớn nhưng được kích hoạt thưa thớt. Tuy nhiên, MoE dễ bị quá khớp. Chúng ta thường đào tạo trước một MoE trên một tập dữ liệu lớn và sau đó tinh chỉnh nó trên các nhiệm vụ downstream khác nhau. Trong hầu hết các trường hợp, các nhiệm vụ downstream này là vấn đề đích mà chúng ta muốn giải quyết. So với các mô hình dày đặc, nhiều tham số có thể đào tạo hơn và tính toán có điều kiện thưa thớt gây ra quá khớp [14, 27] trong quá trình tinh chỉnh, đặc biệt khi quy mô tập dữ liệu không đủ lớn. Ngoài ra, ngay cả khi chúng ta đào tạo thành công một mô hình MoE, nó vẫn khó triển khai. Đối với MoE với hàng nghìn tỷ tham số, chúng ta cần triển khai các chuyên gia khác nhau trên các thiết bị khác nhau để giảm tiêu thụ bộ nhớ trên thiết bị (ví dụ: GPU, TPU). Thứ ba, mô hình MoE không thân thiện với phần cứng. Song song hóa chuyên gia tốn kém về giao tiếp. Đối với các cụm GPU, thao tác all-to-all quá chậm để mở rộng mô hình MoE. Bên cạnh đó, hàm cổng bao gồm nhiều thao tác để tạo mặt nạ token, chọn k chuyên gia hàng đầu, và thực hiện tổng tích lũy để tìm token-id đi đến mỗi chuyên gia và nhân ma trận thưa thớt [16]. Tất cả các thao tác này đều lãng phí do biểu diễn tensor thưa thớt. Quan trọng hơn, chúng cực kỳ chậm do nhiều lần gọi kernel. Tóm lại, mô hình MoE thưa thớt mạnh mẽ, nhưng tương đối khó sử dụng trong thực tế. Mô hình dày đặc được sử dụng rộng rãi nhưng yếu hơn mô hình thưa thớt với chi phí tính toán tương đương. Vậy, có thể kết hợp điểm mạnh của mô hình thưa thớt và dày đặc để đào tạo một mô hình vừa hiệu quả vừa dễ sử dụng không?

Trong nghiên cứu này, lấy cảm hứng từ mô hình giáo dục con người, chúng tôi đề xuất một nhiệm vụ mới, tức là tích hợp kiến thức. Như một khung đào tạo tổng quát, tích hợp kiến thức bao gồm hai bước, tức là thu thập kiến thức và chưng cất kiến thức. Trong thu thập kiến thức, chúng tôi coi mỗi chuyên gia trong MoE như một chuyên gia trong giáo dục con người. Sinh viên là một mô hình dày đặc, và chúng tôi sẽ thu thập kiến thức từ tất cả các chuyên gia và gán kiến thức cho sinh viên. Để thu thập kiến thức từ các chuyên gia, là công trình đầu tiên tập trung vào nhiệm vụ này, chúng tôi nghiên cứu bốn giải pháp có thể khác nhau, tức là tổng, lấy trung bình, Thu thập Kiến thức Top-K (Top-KG), và Thu thập Kiến thức Phân tích Giá trị Đặc biệt (SVD-KG) được đề xuất trong nghiên cứu này. Đối với Top-KG và SVD-KG, chúng tôi sử dụng chọn lọc Top-K hoặc SVD để trích xuất kiến thức chính từ các chuyên gia khác nhau của một MoE đã được đào tạo trước, và sau đó, chúng tôi khởi tạo các lớp mạng feed-forward (FFN) cho một mô hình dày đặc để xấp xỉ MoE. Để tinh chỉnh thêm mô hình từ nhiễu, chúng tôi sử dụng chưng cất kiến thức [9] để tinh chỉnh sinh viên. Xin lưu ý trong giai đoạn chưng cất kiến thức, chúng tôi sử dụng toàn bộ mô hình MoE để dạy mô hình sinh viên dày đặc. Mô hình sinh viên cuối cùng có cùng kiến trúc với một mô hình dày đặc tiêu chuẩn, nhưng, nó sẽ bao phủ kiến thức của MoE với nhiều chuyên gia và nhiều tham số có thể đào tạo hơn. Khung được mô tả ở trên phù hợp tốt với mô hình giáo dục con người, một sinh viên tích hợp kiến thức từ nhiều chuyên gia để sinh viên có thể học nhanh.

Đóng góp của chúng tôi được tóm tắt như sau:
• Chúng tôi đề xuất một nhiệm vụ mới, tích hợp kiến thức. Mục tiêu là kết hợp tính hiệu quả của mô hình MoE thưa thớt và tính khả dụng của mô hình dày đặc. Theo hiểu biết tốt nhất của chúng tôi, đây là công trình đầu tiên tập trung vào việc học một mô hình dày đặc từ một mô hình MoE đã được đào tạo trước.

• Chúng tôi đề xuất giải quyết tích hợp kiến thức trong hai bước, thu thập kiến thức và chưng cất kiến thức. Để thu thập, chúng tôi đầu tiên nghiên cứu bốn phương pháp thu thập kiến thức có thể khác nhau, tức là tổng, lấy trung bình, Top-KG và SVD-KG được đề xuất trong bài báo này. Top-KG và SVD-KG là các phương pháp mới để trích xuất và hợp nhất kiến thức chính từ các chuyên gia của một MoE đã được đào tạo trước để khởi tạo một mô hình dày đặc.

• Chúng tôi đánh giá khung đào tạo tổng quát của chúng tôi trong các lĩnh vực khác nhau, tức là thị giác máy tính và xử lý ngôn ngữ tự nhiên. Trên ImageNet, so với các đường cơ sở, OneS của chúng tôi bảo tồn thêm 23.1% lợi ích từ MoE. Trên các benchmark xử lý ngôn ngữ tự nhiên, chúng tôi đạt được 88.2% lợi ích MoE chỉ với 46% tham số, và chúng tôi vượt trội hơn các đường cơ sở (ví dụ: Distill, Switch) sử dụng gần như cùng kiến trúc và dữ liệu đào tạo. Ngoài ra, do kiến trúc mô hình thân thiện với phần cứng, OneS có thể đạt được tăng tốc suy luận 3.7× so với đối tác MoE.

## 2. Kiến thức chuẩn bị

### 2.1. Mixture-of-Experts

Mixture-of-experts là một mô hình tính toán có điều kiện điển hình. Trong nghiên cứu này, chúng tôi sử dụng một mô hình MoE đã được đào tạo trước làm giáo viên, và một mô hình dày đặc làm sinh viên để bắt chước mô hình giáo dục con người. Do đó, chúng tôi sẽ xem xét ngắn gọn MoE trước.

Cho một mô hình MoE với E chuyên gia có thể đào tạo và biểu diễn đầu vào x ∈ R^D, đầu ra của mô hình MoE có thể được công thức hóa như [21]:

MoE(x) = Σ(i=1 to E) G(x)_i e_i(x)                                    (1)

trong đó e_i(·) là một phép biến đổi phi tuyến R^D → R^D của chuyên gia thứ i, và G(·) : R^D → R^E là mạng cổng, G(x)_i là trọng số định tuyến của x đến chuyên gia thứ i. Thông thường, cả e(·) và G(·) đều được tham số hóa bằng mạng nơ-ron. Xin lưu ý đầu ra của G(·) nên được kích hoạt bằng hàm softmax:

G(x) = topK(ω(h(x) + ϵ))                                             (2)

trong đó ω là hàm softmax, h(·) là một lớp tuyến tính ánh xạ R^D → R^E, và ϵ ∼ N(0, 1/E²) là nhiễu Gaussian để khám phá định tuyến chuyên gia. Việc chọn lọc top-K là một module chính để kích hoạt mạng con một cách thưa thớt. Chúng ta thường đặt K là 1 hoặc 2 để có chi phí tính toán tương đương với mô hình dày đặc tương ứng.

Khi đào tạo mô hình MoE, nếu chúng ta không có điều chuẩn hóa, hầu hết các token có thể được gửi đến một phần nhỏ các chuyên gia, và các chuyên gia khác nhận được ít token. Việc gán không cân bằng như vậy sẽ dẫn đến hiệu quả thấp hơn và độ chính xác kém hơn [8, 12]. Do đó, để đạt được khối lượng công việc cân bằng cho các chuyên gia khác nhau, chúng ta thường kết hợp bộ định tuyến g(·) với mất mát cân bằng tải [12] L_balance:

L_balance = E Σ(i=1 to E) m_i P_i                                     (3)

trong đó m là một vector và phần tử thứ i của m biểu diễn phần token được gửi đến chuyên gia i:

m_i = 1/N Σ(j=1 to N) k(x_j)_i                                      (4)

trong đó N là số token để định tuyến, k(x_j) là một vector chỉ mục từ hàm top-K. Vì việc tạo vector chỉ mục ở đây không thể vi phân, chúng ta định nghĩa P_i là:

P_i = ω(h(x) + ϵ)_i                                                   (5)

trong đó P_i ≈ g(x) mà không có định tuyến top-K. Khi chúng ta tối thiểu hóa L_balance, chúng ta có thể thấy cả m và P sẽ gần với phân phối đồng đều.

Bộ định tuyến có thể đào tạo ở đây cũng có thể được thay thế bằng các module không thể đào tạo, ví dụ: lớp BASE [13]. Nghiên cứu này tập trung vào việc tích hợp kiến thức từ một MoE đã được đào tạo trước thay vì các biến thể MoE.

### 2.2. Công thức hóa vấn đề

Chúng tôi có hai giai đoạn trong khung tích hợp kiến thức được đề xuất trong nghiên cứu này: (1) thu thập kiến thức từ MoE; (2) chưng cất kiến thức để tinh chỉnh thêm mô hình dày đặc mới (tức là sinh viên). Đối với giai đoạn đầu tiên, cho E chuyên gia {e_1(·), e_2(·), ..., e_E(·)}, chúng tôi cần tối đa hóa kiến thức được bao phủ trong mô hình dày đặc s(·). Chúng tôi sử dụng MoE dựa trên transformer để giới thiệu khung của chúng tôi do tính phổ biến của nó. Cho biểu diễn đầu vào x, trong một khối transformer, mỗi chuyên gia là một FFN, có thể được công thức hóa như:

e_i(x) = f_2^i(σ(f_1^i(x)))                                          (6)

trong đó f_1^i(·) và f_2^i(·) là các phép biến đổi tuyến tính của chuyên gia thứ i, σ(·) là hàm kích hoạt. Đối với sinh viên dày đặc, chúng tôi có cùng kiến trúc nhưng các tham số có thể đào tạo khác nhau:

s(x) = g_2(σ(g_1(x)))                                                 (7)

trong đó σ(·) sẽ là cùng hàm kích hoạt như các chuyên gia. Sự khác biệt duy nhất là các tham số có thể đào tạo trong các phép biến đổi tuyến tính. Sau đó, mục tiêu của chúng tôi là xấp xỉ các tham số có thể đào tạo của g_1 và g_2 theo {f_1^1, ..., f_1^E} và {f_2^1, ..., f_2^E}, tương ứng. Chúng tôi định nghĩa mục tiêu này là thu thập kiến thức từ MoE.

Giai đoạn thứ hai là tinh chỉnh sinh viên dày đặc để tối thiểu hóa sự khác biệt giữa đầu ra của giáo viên và đầu ra của sinh viên. Chúng ta có thể dễ dàng thấy nhiệm vụ này gần với chưng cất kiến thức [9], vì vậy trong bài báo này, chúng tôi theo các phương pháp KD điển hình như giải pháp của chúng tôi.

Mục tiêu của chúng tôi là bảo tồn lợi ích của MoE bằng một sinh viên dày đặc càng nhiều càng tốt. Vì vậy, chúng tôi định nghĩa một số liệu, lợi ích MoE, để đo lường khả năng của một sinh viên dày đặc trong việc tích hợp kiến thức từ đối tác MoE. Lợi ích MoE có thể được viết như:

Lợi ích MoE = (điểm sinh viên - điểm dày đặc) / (điểm MoE - điểm dày đặc)    (8)

trong đó điểm có thể là bất kỳ số liệu nào để đánh giá mô hình. Ví dụ, điểm là độ chính xác cho phân loại hình ảnh. Điểm dày đặc ở đây biểu thị hiệu suất của mô hình dày đặc mà không có tích hợp kiến thức được đề xuất.

## 3. Phương pháp

Nói chung, mục tiêu cuối cùng của nghiên cứu này là có được một mô hình sinh viên dày đặc dễ sử dụng và hiệu quả như MoE thưa thớt. Để đạt được điều này, chúng tôi đề xuất một khung đào tạo tổng quát, tích hợp kiến thức, để tích hợp kiến thức từ giáo viên MoE thưa thớt đến sinh viên dày đặc. Tích hợp kiến thức được đề xuất bao gồm hai giai đoạn: tích hợp kiến thức từ MoE và chưng cất kiến thức để tinh chỉnh sinh viên. Tổng quan về khung đào tạo tổng quát được đề xuất được hiển thị trong Hình 2. Bước đầu tiên là khởi tạo sinh viên dày đặc. Đối với hầu hết các lớp có thể đào tạo (ví dụ: lớp embedding, lớp attention, lớp chuẩn hóa), giáo viên và sinh viên có cùng cấu trúc (Chúng tôi đặt tên các lớp như vậy là các lớp khớp hoàn hảo trong nghiên cứu này.), vì vậy chúng ta có thể sao chép trọng số từ giáo viên theo Switch Transformer [8] trực tiếp. Phần thách thức là lớp MoE. Lớp MoE có nhiều tham số có thể đào tạo hơn đối tác dày đặc với một lớp FFN duy nhất, và mỗi chuyên gia thực sự là một lớp FFN với trọng số và bias độc đáo. Vấn đề cốt lõi là kết hợp kiến thức từ các chuyên gia FFN khác nhau và gán kiến thức cho một FFN duy nhất trong mô hình sinh viên. Để đạt được điều này, chúng tôi nghiên cứu bốn phương pháp thu thập kiến thức có thể khác nhau, tức là tổng, lấy trung bình, Top-KG và SVD-KG. Sau đó, chưng cất kiến thức là tinh chỉnh mô hình đã khởi tạo để cải thiện hiệu suất thêm.

### 3.1. Thu thập Kiến thức từ MoE

Chúng tôi đầu tiên công thức hóa nhiệm vụ KG của chúng tôi. Cho một lớp MoE với E chuyên gia, mục tiêu ở đây là thu thập kiến thức từ tất cả các chuyên gia cho một sinh viên dày đặc. Theo Phương trình 6 và Phương trình 7, mỗi chuyên gia bao gồm hai lớp tuyến tính, và sinh viên chia sẻ cùng cấu trúc mô hình với một chuyên gia duy nhất. Để ngắn gọn, chúng tôi coi mỗi chuyên gia như một phép biến đổi tuyến tính để hiển thị ý tưởng của chúng tôi, có thể được mở rộng dễ dàng đến nhiều lớp tuyến tính. Đối với E lớp tuyến tính {f_1, f_2, ..., f_E}, mỗi lớp tuyến tính f_i(·) : R^(d_1) → R^(d_2) với trọng số W_f^i ∈ R^(d_1×d_2) và bias b_f^i ∈ R^(d_2),

KG(f_1, f_2, ..., f_E) = KG(W_f^1, W_f^2, ..., W_f^E, b_f^1, b_f^2, ..., b_f^E) → (W_g, b_g) = g    (9)

trong đó g(·) : R^(d_1) → R^(d_2) là một lớp tuyến tính với W_g ∈ R^(d_1×d_2) và bias b_g ∈ R^(d_2).

Trước khi hợp nhất các trọng số, chúng tôi đầu tiên khởi tạo b_g từ các chuyên gia khác nhau. Vì nó có ít tham số có thể đào tạo hơn nhiều, chúng tôi đơn giản lấy trung bình vector bias từ các chuyên gia khác nhau:

b_g = (1/E) Σ(i=1 to E) b_f^i                                        (10)

Chúng tôi sử dụng chính sách đơn giản như vậy vì kiến thức được lưu trữ trong bias ít hơn nhiều so với trong trọng số, do ít tham số có thể đào tạo hơn. Chúng tôi chứng minh giả định này bằng thực nghiệm trong Phụ lục E.

Sau khi sao chép trọng số và bias trong các lớp khớp hoàn hảo và lấy trung bình bias trong các lớp MoE, chúng tôi khởi tạo trọng số mô hình sinh viên dày đặc bằng MoE thưa thớt. Là công trình đầu tiên tập trung vào nhiệm vụ này, chúng tôi nghiên cứu bốn phương pháp để thu thập kiến thức, tức là tổng, lấy trung bình, Top-KG và SVD-KG. Hai phương pháp đầu tiên là những phương pháp đơn giản nhất. Chúng tôi cũng đề xuất hai phương pháp mới, Top-KG và SVD-KG để trích xuất kiến thức chính từ các chuyên gia khác nhau của một MoE đã được đào tạo trước.

#### 3.1.1 Tổng và Lấy trung bình

Đối với trọng số trong MoE, chúng tôi đầu tiên xem xét hai phương pháp đơn giản. Phương pháp đầu tiên là tổng:

W_g = Σ(i=1 to E) W_f^i                                               (11)

và phương pháp thứ hai là lấy trung bình:

W_g = (1/E) Σ(i=1 to E) W_f^i                                        (12)

Mặc dù hai phương pháp thu thập này đơn giản, là công trình đầu tiên tập trung vào nhiệm vụ này, chúng tôi nghiên cứu chúng để mở đường cho việc thu thập kiến thức từ các mô hình MoE.

### 3.2. Thu thập Kiến thức Top-K

Chúng tôi cũng đề xuất hai phương pháp mới để thu thập kiến thức. Đối với trọng số, trong MoE, một mô hình tham số quá mức rộng với nhiều tham số có thể đào tạo hơn, việc bao phủ tất cả kiến thức trong một mô hình dày đặc hẹp là thách thức. Do đó, chúng ta phải trích xuất kiến thức chính từ mỗi chuyên gia và sau đó hợp nhất chúng thành một mô hình dày đặc nhỏ duy nhất. Sau đó, câu hỏi là, làm thế nào chúng ta có thể trích xuất kiến thức chính của mỗi ma trận có thể đào tạo (tức là trọng số)? Chúng tôi đầu tiên đề xuất thu thập kiến thức Top-K để trích xuất ma trận con của mỗi chuyên gia. Đối với ma trận trọng số chuyên gia thứ i W_i ∈ R^(d_1×d_2), chúng tôi tính toán chuẩn l2 của mỗi cột như l_i ∈ R^(d_1). Sau đó chúng tôi sử dụng chọn lọc Top-K để chọn K cột của W_i theo l_i, trong đó K = d_2/E. Ma trận được trích xuất W_g^i ∈ R^(d_1×K). Sau đó chúng tôi nối các ma trận được trích xuất từ tất cả các chuyên gia làm khởi tạo sinh viên cuối cùng W_g ∈ R^(d_1×d_2).

Trong thực tế, vì mỗi chuyên gia có hai lớp tuyến tính W_1^i ∈ R^(d_1×d_2) và W_2^i ∈ R^(d_2×d_1), sẽ có sự không khớp cột cho hai ma trận được trích xuất từ cùng một chuyên gia nếu chúng ta chọn các ma trận con của hai ma trận này một cách độc lập. Để giảm thiểu vấn đề này, chúng tôi tính toán chuẩn l2 của mỗi cột trong W_1^i và chuẩn l2 của mỗi hàng trong W_2^i. Tổng của hai vector chuẩn l2 này, tức là l_i ∈ R^(d_1) được đưa vào chọn lọc Top-K và sau đó trích xuất ma trận con.

#### 3.2.1 Thu thập Kiến thức SVD

Chúng tôi nghiên cứu một cách mới khác để trích xuất kiến thức chính từ các chuyên gia. Nén hạng thấp [3] đã cho thấy kết quả đầy hứa hẹn trong việc nắm bắt kiến thức chính, được sử dụng để chuyển đổi một ma trận không có hạng thấp thành một phân tích hạng-k của ma trận trọng số. Ma trận hạng thấp như vậy có thể xấp xỉ kiến thức của toàn bộ ma trận. Trên cơ sở này, chúng ta có thể hợp nhất ma trận hạng thấp dễ dàng hơn bằng cách tái tạo một ma trận hạng cao từ nhiều ma trận hạng thấp. Xin lưu ý, trong nghiên cứu này, việc có được phân tích hạng-k không phải là mục tiêu của chúng tôi. Thay vào đó, phân tích hạng-k chỉ là một bước trung gian của việc phân tích và hợp nhất của chúng tôi. Trong nghiên cứu này, chúng tôi đề xuất sử dụng SVD để trích xuất kiến thức chính và hợp nhất chúng để khởi tạo một ma trận dày đặc khác:

W_f^i = U_f^i S_f^i V_f^i^T ≈ U_f^i_{K_i} S_f^i_{K_i} V_f^i^T_{K_i}      (13)

trong đó U_f^i ∈ R^(d_1×d_1) và V_f^i ∈ R^(d_2×d_2) là các ma trận unitary, S_f^i ∈ R^(d_1×d_2) là một ma trận chéo. Chúng ta thường chọn các phần tử top-K trong S_f^i và sau đó xây dựng U_f^i_{K_i} ∈ R^(d_1×K_i), S_f^i_{K_i} ∈ R^(K_i×K_i) và V_f^i_{K_i} ∈ R^(d_2×K_i) để xấp xỉ W_f^i.

Khi k được cố định, mỗi ma trận có phân tích hạng-k để xấp xỉ ma trận gốc. Tuy nhiên, chúng ta không thể đảm bảo kiến thức chính trong mỗi chuyên gia có thể được bao phủ bởi một phân tích hạng-k cố định. Do đó, chúng tôi định nghĩa một tỷ lệ SVD thích ứng α ∈ (0, 1] để đảm bảo:

σ(S_f^i_{K_i}) ≥ α·σ(S_f^i)                                          (14)

trong đó σ(S_f^i) biểu thị tổng các phần tử chéo của S_f^i. Nếu α = 1, tất cả các hạng sẽ được bảo tồn cho một ma trận đầy đủ hạng. Sau đó chúng tôi thu thập phân tích của mỗi chuyên gia và nối chúng như:

[U_g^t] = [U_f^1_{K_1} : ... : U_f^E_{K_E}];
[S_g] = [S_f^1_{K_1} ... S_f^E_{K_E}];
[V_g] = [V_f^1_{K_1} ... V_f^E_{K_E}]                                (15)

Sau đó chúng ta có thể thu được W_g như:

W_g = U_g S_g V_g^T                                                   (16)

W_g là một ma trận hạng-K_g, trong đó K_g = Σ(i=1 to E) K_i, bao phủ kiến thức chính của mỗi chuyên gia.

Sau SVD-KG, kiến thức đã được tích hợp từ MoE đã được đào tạo trước. Tuy nhiên, trong quá trình thu thập kiến thức, không thể tránh khỏi việc tạo ra nhiễu khi chúng ta loại bỏ tính toán có điều kiện. Phân tích chi tiết về nhiễu được tạo ra trong quá trình thu thập có thể được tìm thấy trong Phụ lục A.

### 3.3. Chưng cất Kiến thức

Để khai thác kiến thức từ nhiễu, chúng tôi áp dụng chưng cất kiến thức mềm [9] để tinh chỉnh sinh viên dày đặc. Chưng cất mềm tối thiểu hóa phân kỳ Kullback-Leibler giữa đầu ra của giáo viên và sinh viên. Mất mát chưng cất tương ứng có thể được viết như:

L_distill^soft = T² L_KL(ω(z_s/T), ω(z_t/T))                        (17)

trong đó ω là hàm softmax, L_KL là mất mát phân kỳ Kullback-Leibler, z_s và z_t là logits của sinh viên và giáo viên, tương ứng, và T là nhiệt độ softmax. Chúng tôi cũng xem xét chưng cất nhãn cứng [24] và so sánh hiệu suất của nó với chưng cất mềm. Vui lòng xem Phụ lục C để biết chi tiết.

### 3.4. Tối ưu hóa

Hàm mất mát cuối cùng của chúng tôi đơn giản:

L_total = λL_main + (1-λ)L_distill                                   (18)

trong đó λ được sử dụng để cân bằng mất mát chính và mất mát chưng cất. Mất mát chính phụ thuộc vào nhiệm vụ. Ví dụ, để phân loại hình ảnh, đó là cross-entropy. Đối với đào tạo trước BERT, nó nên là mất mát mô hình hóa ngôn ngữ có mặt nạ và mất mát dự đoán câu tiếp theo. Mất mát chưng cất ở đây có thể là mất mát chưng cất mềm hoặc mất mát chưng cất nhãn cứng. Vì MoE đã được đào tạo trước của chúng tôi được cố định trong quá trình chưng cất kiến thức, chúng tôi không cần mất mát cân bằng tải của transformer dựa trên MoE.

## 4. Thực nghiệm

### 4.1. Thị giác Máy tính

**Cài đặt thực nghiệm** Để đánh giá khung đào tạo tổng quát của chúng tôi, chúng tôi tiến hành thực nghiệm trên hai lĩnh vực khác nhau, thị giác máy tính và xử lý ngôn ngữ tự nhiên.

**Tập dữ liệu** Đối với thị giác, chúng tôi chọn hai benchmark phân loại hình ảnh được sử dụng rộng rãi, ILSVRC-2012 ImageNet [5] và Cifar10 [10], làm nền tảng để đánh giá khung của chúng tôi về thị giác máy tính. Tập dữ liệu ILSVRC-2012 ImageNet mà chúng tôi sử dụng trong nghiên cứu này có 1k lớp và 1.3M hình ảnh. Chúng tôi ký hiệu nó là ImageNet trong các thực nghiệm sau để ngắn gọn.

**Đường cơ sở** Vì chúng tôi là công trình đầu tiên, theo hiểu biết tốt nhất của chúng tôi, tập trung vào việc tích hợp kiến thức từ một MoE đã được đào tạo trước, chỉ có hai đường cơ sở mạnh hiện tại là khung chưng cất kiến thức được đề xuất trong Meta AI MoE [1] và Switch Transformer [8]. Phương pháp đầu tiên đơn giản khởi tạo mô hình sinh viên dày đặc một cách ngẫu nhiên. Nghiên cứu thứ hai khởi tạo mô hình dày đặc với các trọng số không phải chuyên gia. Nghĩa là, họ đơn giản sao chép lớp có thể được khớp hoàn hảo vào mô hình dày đặc. Đối với các trọng số không thể được khớp (tức là các chuyên gia), họ bỏ qua việc khởi tạo từ MoE và đào tạo các lớp này từ đầu thay vào đó. Trong nghiên cứu của chúng tôi, để ngắn gọn, chúng tôi ký hiệu hai phương pháp này là Distill và Switch, tương ứng. Chúng tôi cũng báo cáo kết quả của Vision Transformer (ViT) trên cùng cài đặt để so sánh hiệu quả tham số.

**Giáo viên** Trong khung đào tạo của chúng tôi, chúng tôi cần một mô hình MoE để khởi tạo mô hình sinh viên dày đặc của chúng tôi (tức là thu thập kiến thức) và thực hiện chưng cất kiến thức. Trong nghiên cứu này, chúng tôi áp dụng WideNet [27] đã được đào tạo trước làm nền tảng. WideNet là một transformer dựa trên MoE chỉ với một khối transformer có thể đào tạo. Khối transformer này sử dụng MoE thay vì lớp FFN để học biểu diễn cục bộ. Trọng tâm chính của bài báo này là xác minh kiến thức trong MoE đã được đào tạo trước có thể được bảo tồn trong sinh viên dày đặc, vì vậy chúng tôi sử dụng WideNet làm mô hình giáo viên để xác minh tính hiệu quả của phương pháp của chúng tôi một cách đơn giản hơn.

**Siêu tham số** Để so sánh công bằng, chúng tôi theo tăng cường dữ liệu được sử dụng trong mô hình giáo viên: tiền xử lý kiểu Inception, Mixup [30], RandAugment [4] và làm mượt nhãn [23, 29]. Chúng tôi sử dụng bộ tối ưu LAMB [28]. Kích thước batch và tỷ lệ học được đặt là 4096 và 0.004, tương ứng. Đối với mô hình giáo viên, tất cả cài đặt của WideNet [27] giống như được báo cáo trong bài báo của họ. Xin lưu ý chúng tôi đóng băng tất cả trọng số có thể đào tạo của mô hình giáo viên (tức là WideNet) trong giai đoạn chưng cất kiến thức của OneS. Đối với siêu tham số chưng cất, chúng tôi đặt λ là 0.25 và nhiệt độ T là 1.0. Áp dụng suy giảm tỷ lệ học tuyến tính.

Chúng tôi cũng tinh chỉnh mô hình sinh viên đã được đào tạo trước của chúng tôi trên Cifar-10. Cài đặt giống như ViT và WideNet. Chúng tôi sử dụng bộ tối ưu SGD với momentum. Theo các nghiên cứu hiện tại, làm mượt nhãn và warm-up được loại bỏ. Vui lòng xem Phụ lục để biết chi tiết đào tạo khác.

#### 4.1.1 Kết quả trên ImageNet

Chúng tôi báo cáo độ chính xác top-1 và lợi ích MoE trên ImageNet trong Bảng 1. Trong bảng này, như chúng tôi đã định nghĩa trong Phương trình 8, lợi ích MoE có nghĩa là mô hình dày đặc đã bảo tồn được bao nhiêu cải thiện, sau tích hợp kiến thức. Đầu tiên, sau khi nghiên cứu bốn phương pháp KG khác nhau, phương pháp tích hợp dựa trên SVD hoạt động tốt nhất. Do đó, chúng tôi đặt phương pháp dựa trên SVD là lựa chọn mặc định trong các thực nghiệm sau. Phương pháp tích hợp dựa trên Top-K hoạt động tương đương với phương pháp dựa trên SVD ở quy mô lớn nhưng hơi tệ hơn ở cấp độ cơ bản. Chúng tôi gợi ý lý do là mô hình lớn có dung lượng lớn hơn và bền vững hơn với việc giảm cột thưa thớt. Ngoài ra, chúng tôi quan sát thấy OneS-L-SVD đạt được 78.4% độ chính xác top-1 trên ImageNet chỉ với 15M tham số. So với Switch-L mạnh nhất, mô hình của chúng tôi có cải thiện 0.6 điểm. So với mô hình giáo viên, OneS-L-SVD vượt trội hơn WideNet-B 0.9% với một nửa số tham số. Là kết quả cuối cùng, OneS-L-SVD đạt được hiệu suất tương đương với ViT-B chỉ với 17% tham số có thể đào tạo. Quan trọng hơn, trong [27], không có MoE, WideNet-L chỉ có thể đạt được 76.9% độ chính xác top-1. OneS của chúng tôi có hoàn toàn cùng kiến trúc như vậy, nhưng chúng tôi có thể đạt được 78.4% độ chính xác. Nghĩa là, OneS-L-SVD của chúng tôi bảo tồn 61.7% cải thiện (tức là lợi ích MoE) từ WideNet. Ngoài ra, OneS-B-SVD của chúng tôi đạt được 57.7 lợi ích MoE, vượt trội hơn đường cơ sở mạnh nhất (tức là Switch) 23.1 điểm. Những kết quả như vậy cho thấy tính hiệu quả của tích hợp kiến thức.

#### 4.1.2 Kết quả trên Cifar10

Chúng tôi tinh chỉnh thêm mô hình sinh viên dày đặc của chúng tôi, OneS trên Cifar10 trong phần này. Như được hiển thị trong Bảng 2, OneS-L của chúng tôi vẫn vượt trội hơn các đường cơ sở của chúng tôi, Switch-B và Switch-L, lần lượt là 0.3% và 0.6%. OneS-L thậm chí có thể đạt được hiệu suất tương đương với WideNet-B với 0.33× tham số có thể đào tạo. OneS-B cũng đạt được hiệu suất tốt hơn Switch-B do thu thập kiến thức. Tóm lại, kết quả trên Cifar10 cho thấy cải thiện của đào tạo trước trên ImageNet có thể lan truyền đến nhiệm vụ downstream.

### 4.2. Xử lý Ngôn ngữ Tự nhiên

**Cài đặt thực nghiệm** Tương tự như thực nghiệm trên các nhiệm vụ thị giác máy tính, chúng tôi vẫn có hai giai đoạn đào tạo trong xử lý ngôn ngữ tự nhiên. Sự khác biệt là, theo các nghiên cứu hiện tại [6, 11, 27], chúng tôi tập trung vào hiệu suất của các nhiệm vụ downstream thay vì đào tạo trước.

**Tập dữ liệu** Chúng tôi sử dụng Wikipedia tiếng Anh [6] và BOOKCORPUS [33] làm kho dữ liệu đào tạo trước. Đối với tinh chỉnh, chúng tôi đánh giá nghiên cứu của chúng tôi trên benchmark General Language Understanding Evaluation (GLUE) [26], hai phiên bản khác nhau của tập dữ liệu Stanford Question Answering (SQuAD) [17, 18]. Đối với thực nghiệm GLUE, chúng tôi báo cáo trung vị trên 5 lần chạy theo các nghiên cứu hiện tại [11, 27].

**Đường cơ sở** Tương tự như thực nghiệm trên thị giác máy tính, chúng tôi vẫn chọn Distill và Switch làm đường cơ sở trực tiếp, mặc dù nghiên cứu của chúng tôi là công trình đầu tiên tập trung vào nhiệm vụ này. Mô hình sinh viên ở đây cũng có cùng kiến trúc với ALBERT ngoại trừ chuẩn hóa lớp riêng lẻ [27]. Do đó, một đường cơ sở khác là ALBERT. Chúng tôi mong đợi OneS của chúng tôi có thể vượt trội hơn ALBERT với gần như cùng kiến trúc, số lượng tham số tương đương, và cùng tập dữ liệu đào tạo trước.

**Siêu tham số** Sau khi khởi tạo, chúng tôi tinh chỉnh thêm OneS bằng kết hợp tuyến tính của mất mát mô hình hóa ngôn ngữ có mặt nạ, mất mát dự đoán thứ tự câu, và mất mát chưng cất kiến thức mềm. Theo [20], chúng tôi chỉ đưa logits của mất mát mô hình hóa ngôn ngữ có mặt nạ vào L_distill. Chúng tôi vẫn đóng băng tất cả trọng số có thể đào tạo của mô hình giáo viên MoE (WideNet) trong giai đoạn đào tạo của OneS. λ được đặt là 0.75, và α là 0.25 trong phần này. Nghiên cứu cắt giảm của các cài đặt này có thể được tìm thấy trong Phụ lục D. Các siêu tham số chi tiết khác có thể được tìm thấy trong Phụ lục B.2.

#### 4.2.1 Kết quả trên benchmark NLU

Sau đào tạo trước, chúng tôi tinh chỉnh OneS của chúng tôi mà không có mất mát chưng cất. Cài đặt như vậy khác với nghiên cứu hiện tại về chưng cất mô hình ngôn ngữ. Lý do là, một trong những mục tiêu của chúng tôi là có được một mô hình dễ sử dụng mà không có định tuyến chuyên gia. Nếu chúng tôi vẫn có một giáo viên MoE, việc tinh chỉnh downstream vẫn yêu cầu thiết kế phần cứng và phần mềm phức tạp cho MoE. Kết quả trên các nhiệm vụ hiểu ngôn ngữ tự nhiên downstream được hiển thị trong Bảng 3. Nói chung, chúng ta có thể quan sát OneS vượt trội hơn ALBERT và các đường cơ sở (tức là Distill và Switch) trên tất cả các nhiệm vụ bằng cách đạt được 88.2% lợi ích MoE. Ví dụ, trên bốn nhiệm vụ, OneS vượt trội hơn Switch 0.42 điểm trung bình. Ngoài ra, chúng tôi đạt được 53.2% và 51.7% lợi ích MoE so với Switch và Distill, tương ứng. Trên một số nhiệm vụ, ví dụ: SQuAD1.1 và SST-2, OneS thậm chí có thể vượt trội hơn mô hình giáo viên MoE, WideNet. Chúng tôi gợi ý rằng mô hình MoE có xu hướng quá khớp trên các tập dữ liệu nhỏ. OneS có kiến thức của MoE nhưng cấu trúc dày đặc, vì vậy lợi ích từ đào tạo trước có thể lan truyền đến các nhiệm vụ downstream dễ dàng hơn.

So với mô hình MoE, điểm mạnh khác của OneS chúng tôi là tốc độ suy luận. Lý do tại sao MoE quá chậm là, mô hình MoE có hàm cổng và toán tử einsum thưa thớt do tính toán có điều kiện, điều này sẽ giảm hiệu quả tính toán. Tuy nhiên, mô hình của chúng tôi có thể đạt được tăng tốc suy luận 3.7×. Xin lưu ý rằng WideNet chỉ sử dụng 2.4× FLOPs ở các lớp MoE. Đối với các lớp khác, WideNet có cùng chi phí tính toán như OneS hoặc ALBERT, vì vậy FLOPs toàn cục ít hơn 2.4× của OneS. Do đó, mặc dù một lý do tại sao OneS có thể đạt được hiệu quả cao như vậy là ít tính toán hơn, lý do quan trọng khác là, mô hình dày đặc thân thiện với phần cứng hơn mô hình MoE thưa thớt.

### 4.3. Nghiên cứu cắt giảm

Chúng tôi tiến hành bốn bộ nghiên cứu cắt giảm trong nghiên cứu này. Bộ đầu tiên là để nghiên cứu đóng góp của thu thập kiến thức và chưng cất kiến thức. Như được hiển thị trong Bảng 4, có sự suy giảm hiệu suất đáng kể khi không có thu thập kiến thức, điều này cho thấy kiến thức được bao gồm trong mô hình thưa thớt đã được đào tạo trước là rất quan trọng để cải thiện hiệu suất của mô hình sinh viên. Đối với mô hình không có KD, trong thực nghiệm này, chúng tôi áp dụng L_main trong Phương trình 18 là hàm mất mát duy nhất. Chúng ta có thể thấy chưng cất kiến thức hữu ích, vì dự đoán của giáo viên có thể hướng dẫn sinh viên khai thác kiến thức trong trọng số thu thập có nhiễu. Ngoài ra, khi mô hình dày đặc không thu thập kiến thức từ MoE, KD cho phép quá trình đào tạo của mô hình lite (tức là OneS-B) ổn định hơn. Đối với mô hình lớn, việc loại bỏ cả thu thập kiến thức và chưng cất kiến thức cũng sẽ làm tổn hại hiệu suất.

Vì chúng tôi tiến hành hai giai đoạn đào tạo trong khung của chúng tôi, tổng số bước đào tạo của OneS nhiều hơn mô hình dày đặc được đào tạo từ đầu mà không có chưng cất. Bộ nghiên cứu cắt giảm thứ hai là để xác minh liệu cải thiện của mô hình của chúng tôi có từ nhiều lần lặp đào tạo hơn hay không. Để đạt được điều này, chúng tôi đào tạo OneS mà không có KG và KD từ đầu cho các epoch đào tạo toàn cục tương đương. Chúng tôi sử dụng OneS-L làm nền tảng cho bộ thực nghiệm này vì chúng tôi quan sát thấy việc đào tạo không ổn định của OneS-B mà không có cả KG và KD. Như được hiển thị trong Hình 3, khi đào tạo với các epoch tương đương, OneS của chúng tôi vượt trội hơn các đường cơ sở với một khoảng cách lớn một cách nhất quán. Ngoài ra, khi mở rộng đến nhiều epoch hơn, WideNet không có MoE ngừng cải thiện, nhưng OneS của chúng tôi vẫn có thể có được lợi ích từ việc đào tạo nhiều hơn. Chúng tôi cũng nghiên cứu hai loại phương pháp chưng cất kiến thức, chưng cất mềm [9] và chưng cất nhãn cứng [24]. Bộ cuối cùng là để cắt giảm tỷ lệ SVD α. Vui lòng xem Phụ lục C và Phụ lục D để biết chi tiết.

## 5. Kết luận và Nghiên cứu Tương lai

Trong bài báo này, lấy cảm hứng từ mô hình giáo dục con người, chúng tôi đề xuất tích hợp kiến thức, một nhiệm vụ mới để kết hợp tính hiệu quả của mô hình MoE và tính khả dụng của mô hình dày đặc. Là công trình đầu tiên tập trung vào nhiệm vụ này, giải pháp của chúng tôi là tích hợp kiến thức trong hai bước (tức là thu thập kiến thức và chưng cất kiến thức). Thu thập kiến thức tập trung vào việc thu thập kiến thức từ MoE đã được đào tạo trước để khởi tạo các mô hình sinh viên dày đặc. Chưng cất kiến thức là để tinh chỉnh thêm mô hình dày đặc. Thực nghiệm cho thấy OneS của chúng tôi đạt được tính hiệu quả và hiệu suất xuất sắc trên các nhiệm vụ thị giác máy tính và xử lý ngôn ngữ tự nhiên. Đáng chú ý là OneS của chúng tôi có thể bảo tồn 88.2% lợi ích từ MoE với 0.42× FLOPs mỗi lớp MoE hoặc FFN, tăng tốc suy luận 3.7×, và 46% tham số có thể đào tạo.

Trong tương lai, chúng tôi dự định khám phá các phương pháp thu thập kiến thức và chưng cất tiên tiến hơn để tích hợp kiến thức của MoE vào một sinh viên dày đặc tốt hơn. Ngoài ra, mặc dù hầu hết các transformer dựa trên MoE gần đây đều sử dụng cùng kiến trúc cho các chuyên gia khác nhau, việc nghiên cứu phương pháp thu thập kiến thức từ các chuyên gia với kiến trúc khác nhau là có giá trị. Cuối cùng, chúng tôi mong đợi điều chỉnh phương pháp của chúng tôi cho mô hình MoE cực kỳ lớn như GLaM [7].
