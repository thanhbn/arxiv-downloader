# 2009.10622.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2009.10622.pdf
# File size: 673876 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Under Review, 2024
Non-asymptotic oracle inequalities for the Lasso in
high-dimensional mixture of experts
TrungTin Nguyen trungtin.nguyen@uq.edu.au
School of Mathematics and Physics, The University of Queensland, St Lucia, QLD 4072, Australia;
Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, Inria Grenoble Rhone-Alpes, 655 av. de
l’Europe, 38335 Montbonnot, France.
Hien D Nguyen h.nguyen5@latrobe.edu.au
School of Computing, Engineering, and Mathematical Sciences, La Trobe University, Bundoora, VIC
3086, Australia; Institute of Mathematics for Industry, Kyushu University, Nishi Ward, Fukuoka
819-0395, Japan.
Faicel Chamroukhi Faicel.chamroukhi@irt-systemx.fr
IRT SystemX, Palaiseau, France.
Geoffrey J McLachlan g.mclachlan@uq.edu.au
School of Mathematics and Physics, The University of Queensland, St Lucia, QLD 4072, Australia.
Abstract
We investigate the estimation properties of the mixture of experts (MoE) model in a
high-dimensional setting, where the number of predictors is much larger than the sample
size, and for which the literature is particularly lacking in theoretical results. We consider
the class of softmax-gated Gaussian MoE (SGMoE) models, defined as MoE models with
softmax gating functions and Gaussian experts, and focus on the theoretical properties of
their l1-regularized estimation via the Lasso. To the best of our knowledge, we are the first
to investigate the l1-regularization properties of SGMoE models from a non-asymptotic
perspective, under the mildest assumptions, namely the boundedness of the parameter
space. We provide a lower bound on the regularization parameter of the Lasso penalty
that ensures non-asymptotic theoretical control of the Kullback–Leibler loss of the Lasso
estimator for SGMoE models. Finally, we carry out a simulation study to empirically
validate our theoretical findings.
Keywords: Mixture of experts; mixture of regressions; penalized maximum likelihood;
l1-oracle inequality; high-dimensional statistics; Lasso.
1. Introduction
1.1. Mixture of experts
MoE models, introduced in Jacobs et al. (1991), are a flexible mixture model construction
for conditional density estimation and prediction. Because of their flexibility and the wealth
of statistical estimation and model selection tools available, they have become widely used
in statistics and machine learning. The MoE model construction allows the mixture weights
(or gating functions) to depend on the explanatory variables (or predictors) together with
the experts (or mixture component densities). This permits the modeling of data arising
from more complex data generating processes than those that can be analyzed using mixture
©2024 T. Nguyen, H.D. Nguyen, F. Chamroukhi & G.J. McLachlan.arXiv:2009.10622v7  [math.ST]  2 Jul 2024

--- PAGE 2 ---
Nguyen Nguyen Chamroukhi McLachlan
models and mixture of regressions models, whose mixing parameters are independent of
the covariates. Finite mixture-type models have also become popular due to their universal
approximation and good convergence rates for parameter and density estimation, which
have been extensively studied in Genovese and Wasserman (2000); Nguyen (2013); Ho and
Nguyen (2016); Nguyen et al. (2020, 2022a). In the same vein, recent results for parameter
and conditional density estimation of MoE models have recently been published in Jiang and
Tanner (1999); Norets (2010); Nguyen et al. (2016, 2019, 2021); Ho et al. (2022); Nguyen
et al. (2023, 2024b,a).
In the context of regression, softmax-gated Gaussian MoE models , which will be referred
to as SGMoE , defined as MoE models with Gaussian experts and softmax gating functions,
are a standard choice and a powerful tool for modeling more complex nonlinear relationships
between response and predictor, arising from different subpopulations. Since each mixture
weight is modeled by a softmax function of the covariates, the dependence on each feature
appears both in the experts and in the gating functions, which allows one to capture
more complex nonlinear relationships between the response and predictors arising from
different subpopulations, compared to mixture of regressions models. This is demonstrated
via numerical experiments in several works such as Chamroukhi and Huynh (2018, 2019);
Montuelle and Le Pennec (2014). The reader is referred to Yuksel et al. (2012); Nguyen and
Chamroukhi (2018) for reviews on this topic. Statistical estimation and variable selection
for MoE models in the high-dimensional regression setting remain challenging. In particular,
from a theoretical point of view, there is a lack of results for MoE models, where the number
of explanatory variables can be much larger than the sample size. In such situations, we
need to reduce the dimension of the problem by looking for the most relevant relationships
to avoid numerical problems while ensuring identifiability.
1.2. Related literature
We focus on the use of the Lasso, originally introduced by Tibshirani (1996), also known
as an l1-penalised maximum likelihood estimator ( l1-PMLE). Using l1-PMLE tends to
produce sparse solutions and can be viewed as a convex surrogate for the non-convex l0-
penalization problem. Relaxation methods have attractive computational and theoretical
properties ( cf.,Fan and Li, 2001). First introduced for the linear regression model, the
Lasso estimator has since been studied and extended to many statistical problems. To deal
with heterogeneous high-dimensional data, several researchers have studied the Lasso for
variable selection in the context of mixture of regression models, see, e.g., Khalili and
Chen (2007); Stadler et al. (2010); Meynet (2013); Devijver (2015); and Lloyd-Jones et al.
(2018). In particular, Stadler et al. (2010) provided an l0-oracle inequality, satisfied by the
Lasso estimators, conditional on the restricted eigenvalue condition, namely that the Fisher
information matrix is positive definite. Furthermore, they have to introduce some margin
conditions to link the Kullback–Leibler (KL) loss function to the l2-norm of the parameters.
Another direction of studying this problem is to look at its l1-regularisation properties; see
e.g., Massart and Meynet (2011); Meynet (2013); Devijver (2015). As indicated by Devijver
(2015), in contrast to results for the l0-penalty, some results for the l1-penalty are valid
without any assumptions, either on the Gram matrix or on the bound.

--- PAGE 3 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
1.3. Main contributions
Our overall contributions in the paper can be summarized as follows:
1.To the best of our knowledge, we are the first to study the l1-regularization properties of
the SGMoE models from a non-asymptotic point of view with the mildest assumptions .
Theorem 1 provides a lower bound on the regularization parameter of the Lasso
penalty that ensures non-asymptotic theoretical control of the KL loss of the l1-PMLE
estimator for SGMoE models. Because this result is non-asymptotic, it is valid when
nis fixed, while the number of predictors pcan grow, with respect to n, and can be
much larger than n.
2.Our non-asymptotic result complements the standard asymptotic results forhigh-
dimensional SGMoE models for feature selection using Lasso-PMLE or more general
PMLE via the Scad penalty function of Khalili (2010). Specifically, Khalili (2010)
proved both consistency in feature selection and√nconsistency of the PMLE in
SGMoE models, but under several strict conditions on the regularity of the true joint
density function and on the choice of tuning parameters. On the contrary, the only
mild assumption we use here to obtain the order of rate convergence of the error upper
bounds in(10)from Theorem 1 is boundedness on the parameter space , which also
appeared in Khalili (2010); Stadler et al. (2010); Meynet (2013); Devijver (2015).
3.We extend non-asymptotic results for mixture of regressions models (Massart and
Meynet, 2011; Meynet, 2013; Devijver, 2015) to the more general SGMoE models
as defined in (1), for which the theoretical analysis of the non-asymptotic result is
challenging , because, in SGMoE models, the dependence on each feature appears both
in the means of the experts and in the gating functions. This requires in particular
non-trivial technical proof we establish in this paper.
4.Our focus in this paper is on a simplified but standard setting in which the expert
component means are linear functions with respect to the explanatory variables.
Despite this linear simplification, the overall SGMoE model captures the non-linearity
of the true regression function thanks to its mixture construction. We believe that
the general techniques we develop here can be extended to more general experts , such
as Gaussian experts with polynomial means (see, e.g., Mendes and Jiang, 2012),
hierarchical MoE for exponential family regression models (Jiang and Tanner, 1999),
and when the covariance matrix is also parameterized as an expert function potentially
depending on the covariates as in Ho et al. (2022).
Notations. Throughout this paper, {1, . . . , n }is abbreviated as [ n] forn∈N⋆={1,2, . . .}.
Here, vec(·) is the vectorization operator that stacks the columns of a matrix into a vector.
We denote the induced p-norm of a matrix βby∥β∥p,p∈ {1,2,∞}, which differs from the
vector norm ∥vec(β)∥p. For a matrix Σ, m(Σ) and M(Σ) denote the smallest and largest
eigenvalues of Σ, respectively. We write N(·;v,Σ)for the multivariate Gaussian density
with mean vand with covariance matrix Σ. Given an arbitrary event Tin some probability
space, we define an indicator function by: IT(ω) = 1 if ω∈ TandIT(ω) = 0 if ω /∈ T.
Paper organization. In Section 2, we discuss the construction and framework of high-
dimensional SGMoE models. In Section 3, we present our main result. Then, we conduct

--- PAGE 4 ---
Nguyen Nguyen Chamroukhi McLachlan
a simulation study to empirically verify our theoretical results in 4. Some conclusions are
given in Section 5. Supplementary material is devoted to proving the technical results.
2. Problem setup
2.1. High-dimensional SGMoE models
In the high-dimensional regression setting, we observe ncouples 
x[n], y[n]
≡(xi, yi)i∈[n]∈
(X × Y )n⊂(Rp×Rq)n, where typically p≫n,xiis fixed and yiis a realization of the
random variable Yi,i∈[n]. We assume that, conditional on x[n],Y[n]are independent and
identically distributed (IID) with conditional PDF s0(·|xi). Our goal is to estimate s0from
the observations using the following K-component SGMoE models:
sψ(y|x) =KX
k=1exp 
γk0+γ⊤
kx
PK
l=1exp 
γl0+γ⊤
lxN(y;βk0+βkx,Σk), (1)
with K∈N⋆and unknown parameters ψ= (γ, β,Σ)≡(γk0, γk, βk0, βk,Σk)k∈[K]in a
parameter space Ψ. For technical reasons, we require that the covariates are fixed and the
boundedness assumptions on the parameter space Ψ.
The explanatory variables x[n]and the number of components Kare both fixed. We
assume that Xis a compact subset of Rpand the observations x[n]are finite. Without loss
of generality, we choose to rescale x, so that ∥x∥∞≤1. Therefore, we can assume that
X= [0,1]p. However, the arguments in our proofs are valid for covariates of on scale. We
assume that there exists positive constants Aγ, Aβ, aΣ, AΣ, such that ψ∈eΨ, where
eΨ =n
ψ∈Ψ|max
k∈[K]sup
x∈X
|γk0|+γ⊤
kx
≤Aγ,max
z∈[q]max
k∈[K]sup
x∈X(|[βk0]z|+|[βkx]z|)≤Aβ,
aΣ≤m 
Σ−1
k
≤M 
Σ−1
k
≤AΣo
. (2)
Collection of SGMoE models. In summary, given (1)and(2), we wish to estimate s0
via the following collection of SGMoE models:
S=n
(x, y)7→sψ(y|x)|ψ∈eΨo
. (3)
In particular, to simplify the proofs, we shall assume that the true conditional PDF s0
belongs to S. That is to say, there exists ψ0=(γ0, β0,Σ0)∈eΨ, such that s0=sψ0. From
hereon in, where there is no confusion, we will use s0andsψ0, interchangeably.
2.2. Minimum contrast estimation
Several loss functions have been introduced into the MLE for MoE models. For example,
by using the identifiability conditions, Nguyen et al. (2023, 2024b,a) established inverse
bounds between Hellinger distance and some Wasserstein distances or Voronoi loss functions
to accurately capture heterogeneous parameter estimation convergence rates for different
classes of MoE models. However, in this paper, our main idea is to consider the Lasso as
anl1-ball model selection procedure, see e.g., Massart and Meynet (2011). Therefore, we

--- PAGE 5 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
follow the framework of minimum contrast estimation , see e.g., Massart (2007, Chapter 1),
Arlot and Celisse (2010), and Barron et al. (1999). In this situation, negative log-likelihood
(NLL) and KL divergence are the natural choices for the density estimation problem.
Average KL divergence. To take into account the structure of conditional PDFs, for
fixed explanatory variables ( xi)1≤i≤n, we consider the following average KL loss function:
KLn(s, t) =1
nnX
i=1KL (s(·|xi), t(·|xi)),for any densities sandt, where (4)
KL(s, t) =(R
Rqln
s(y)
t(y)
s(y)dy, ifsdyis absolutely continuous w.r.t tdy,
+∞, otherwise .(5)
Lasso estimator. Conditioned on (xi)1≤i≤n, the MLE approach suggests estimating s0
by the conditional PDF sψthat minimizes the NLL: −1
nPn
i=1ln (sψ(yi|xi)). However, in
high-dimensional data, we need to regularize the MLE in order to obtain reasonable estimates.
Here, we first consider the l1-PMLE (the Lasso estimator):
bsLasso
λ = arg min
sψ∈Sn
−1
nnX
i=1ln (sψ(yi|xi)) +λ(∥γ∥1+∥vec(β)∥1)o
, (6)
where λ≥0 is a regularization parameter to be tuned, ∥γ∥1=PK
k=1Pp
j=1|γkj|, and
∥vec(β)∥1=PK
k=1Pp
j=1Pq
z=1[βk]z,j. It is worth noting that these two entry-wise l1
norms do not contain scalar γk0and vector βk0bias. These Lasso regularisation terms
encourage sparsity for both gating and expert parameters.
3. Main result
To simplify the statement of Theorem 1, given some constants κ≥148, we first define the
following condition for λand the constant C1nthat appears on the upper risk bounds:
λ≥κK√nC0n, C0n=B0n
qlnnp
ln(2p+ 1) + 1
, (7)
B0n= max ( AΣ,1 +KAG)
1 + 2 q√qAΣ 
5A2
β+ 4AΣlnn
,
C1n=p
2qAγ 
eq/2−1πq/2
Aq/2
Σ+Hs0!
+B0nC2n, (8)
Hs0= maxn
0,lnh
(4π)−q/2Aq/2
Σio
, C2n= 302 qK"
1 +
Aγ+qAβ+q√q
aΣ2#
.(9)
Remark. In(7), we have taken care to make dependencies explicit, not only on the tuning
constant κ, but also on n,p,q, and Kas well as on Aβ, AΣ, AG—all of the quantities that
constrain the parameters of the model. See Section 3.1 for a more detailed description. Note
that both C0nandC1ndepend on the sample size nonly via the lnnterm. Furthermore, Hs0
is related to the negative of the differential entropy of the true unknown conditional density
s0∈S; see the supplementary material for more details. We state our main contribution:
anl1-oracle inequality for the Lasso estimator for SGMoE models via Theorem 1.

--- PAGE 6 ---
Nguyen Nguyen Chamroukhi McLachlan
Theorem 1 ( l1-oracle inequality) Assume that we observe 
x[n], y[n]
∈([0,1]p×Rq)n,
coming from an unknown conditional PDF s0≡sψ0∈S, defined in (3). Given C1nin(8),
ifλsatisfies (7), the Lasso estimator bsLasso
λ, defined in (6), satisfies the l1-oracle inequality:
E
KLn 
s0,bsLasso
λ
≤κ+ 1
κinf
sψ∈Sh
KLn(s0, sψ) +λ(∥γ∥1+∥vec(β)∥1)i
+λ+r
K
nC1n.
(10)
3.1. Discussion and perspectives regarding our oracle inequality
The oracle model and convergence rate for the Lasso estimator. Theorem 1
characterizes the performance of Lasso estimators as l1-PMLEs for SGMoE models. If the
regularization parameter λis properly chosen, the solution to the l1-penalized empirical
risk minimization problem , behaves in a manner comparable to the deterministic Lasso
(the so-called oracle ). This oracle is the solution to the l1-penalized true risk minimization
problem , up to an error term of order λ. Note that the best model , denoted by sψ∗, is defined
as the one with the smallest l1-penalized risk:
inf
sψ∈S[KLn(s0, sψ) +λ(∥γ∥1+∥vec(β)∥1)]. (11)
However, since we do not know true density s0, we cannot select this best model, which we
call the oracle model . In particular, by definition, the oracle is the model in the collection
that minimizes the l1-penalized risk in (11), which is generally assumed to be unknown.
From the oracle inequality of Theorem 1, we conjecture that by constructing a suitable
approximation theory on a good space, we can control this l1-penalized true risk to obtain the
parametric convergence rate of n−1/2for the Lasso estimator. A related work in this direction
is Massart and Meynet (2012), who established convergence rates for the selected Lasso
estimator of Massart and Meynet (2011), for a wide range of function classes described by
the interpolation spaces of Barron et al. (2008). Furthermore, to the best of our knowledge,
Theorem 2.8 of Maugis-Rabusseau and Michel (2013) is the only result in the literature that
investigates the minimax estimator for Gaussian mixture models but for model selection
instead of Lasso. We will leave the non-trivial task of obtaining Lasso extensions of this
result to future work.
Implementable Lasso estimator and data-driven regularization parameter λ.Note
that Theorem 1 ensures that there exists a sufficiently large λfor which the estimate has
good properties, but does not give an explicit value for λ. However, we at least give the lower
bound on the value of λvia the bound λ≥κC(p, q, n, K ), where κ≥148, although this
value is obviously conservative. Moreover, it is important to note that the Lasso estimator
that appears in Theorem 1 has already been implemented in practice. Indeed, when the
l2-penalties are given zero weight in Khalili (2010) and Chamroukhi and Huynh (2018,
2019), their penalty functions and a recent result from Huynh and Chamroukhi (2019) for
generalized linear expert models belong to our framework and the l1-oracle inequality from
Theorem 1 provides further theoretical insight for these Lasso estimators. In particular,
possible solutions for calibrating the tuning parameter λof the penalty from the data are
the BIC (Schwarz, 1978) used in Chamroukhi and Huynh (2018, 2019) and the generalized
cross-validation (Stone, 1974) utilized in Khalili (2010); Khalili and Chen (2007). Given the

--- PAGE 7 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
available literature, such computational strategies and numerical simulations for real data
sets will not be considered and discussed further here.
Dependency on p,q,n, and Kin the lower bound of λ.Note that we recover the same
dependence of the formp
ln (2p+ 1) as for the homogeneous linear regression in Stadler
et al. (2010) and of the formp
ln (2p+ 1) (ln n)2/√nfor the mixture of regressions models
in Meynet (2013). On the contrary, the dependence on qfor the mixture of multivariate
Gaussian regression models in Devijver (2015) has form q2+q, while here we get the form
q2√q. The main reason is that the class Sof the SGMoE model is larger, and we use a
different technique to evaluate the upper bound of the uniform norm of the gradient for each
element in S. In the lower bound of λin(7), we can potentially get the factor dependence
K2instead of Kas in Meynet (2013) and Devijver (2015). This can be explained by the
fact that we used a different technique to handle the more complex model when dealing
with the upper bound on the uniform norm of the gradient of lnsψ, forsψ∈S. We refer to
Meynet (2013, Remark 5.8) for some data sets where the dependence on Kcan be reduced
to the order of√
Kfor the mixture Gaussian regression models. The determination of
optimal rates for such problems is still open. Furthermore, the dependence on nfor the
homogeneous linear regression in Stadler et al. (2010) is of the order of n−1/2, while here we
have an additional (lnn)2factor. In fact, the same situation can be found in the l1-oracle
inequalities of Meynet (2013) and Devijver (2015). As explained in Meynet (2013), the use
of nonlinear KL information leads to a scenario where the linearity arguments developed in
Stadler et al. (2010) with the quadratic loss function cannot be exploited. Instead, we need
to use the entropy arguments to for our model, which leads to an additional (ln n)2factor.
Multiplicative upper bound constant. It is worth noting that the constant 1 + κ−1
appearing in the upper bounds of Theorem 1 cannot be reduced to 1, which is in fact the
same situation as the constant from C1from Montuelle and Le Pennec (2014, Theorem 1).
Note that this problem also occurred in the l1-oracle inequalities of Meynet (2013), and
Devijver (2015). Deriving an oracle inequality such that 1 + κ−1can be replaced by 1 for
the KL loss is still an open problem.
Model misspecification. In Theorem 1, when s0/∈S, by letting n→ ∞ , due to the large
bias from the first upper bound term, the total error upper bound of (10) converges to

1 +1
κ
inf
sψ∈Sh
lim
n→∞KLn(s0, sψ) +λ(∥γ∥1+∥vec(β)∥1)i
+λ,
which may be large. The same conclusion holds for Theorem 2 when s0/∈ ∪ m∈N⋆Sm.
Nevertheless, as we consider SGMoE models, some recent universal approximation results,
see,e.g., Nguyen et al. (2016, 2019, 2020, 2021, 2022a), imply that if we take a sufficiently
large number of mixture components K, that is sufficiently large class S, we can approximate
a broad class of conditional PDFs, and thus the term on the right hand side is small for K
sufficiently large. This improves the error bound even when s0/∈S.
3.2. Comparison with the state-of-the-art
Standard asymptotic results with variable selection. Theorem 1 complements the
standard asymptotic results for high-dimensional SGMoE models via feature selection using
Lasso, as well asthe more general PMLE via the Scad penalty function of Khalili (2010).

--- PAGE 8 ---
Nguyen Nguyen Chamroukhi McLachlan
By extending the theoretical developments for mixture of regressions models in Khalili and
Chen (2007), standard asymptotic theorems for SGMoE are established in Khalili (2010).
Then, under several strict regularity conditions on the true joint density function and the
choice of tuning parameter, the PMLE from Khalili (2010), using the Scad penalty function
from Fan and Li (2001) instead of Lasso, is proved to be both consistent in feature selection
and maintains√nconsistency. On the contrary, the only assumption used to obtain the
n−1/2convergence rate of the error upper bound in (10)from Theorem 1 is boundedness
on the parameter space. In fact, this boundedness condition is also required by Khalili
(2010). Furthermore, we work directly on conditional PDFs with fixed covariates rather
than focusing on joint PDFs as in Khalili (2010); Khalili and Chen (2007). In future work,
we shall investigate whether our proof technique used in this paper can be adapted to the
problem of estimating joint PDFs when the predictors are random variables.
Boundedness assumptions on the parameter space. It is worth noting that our
boundedness assumptions also appeared in Stadler et al. (2010); Meynet (2013); Devijver
(2015). They are quite natural when working with MLE (Maugis and Michel, 2011), at least
when considering the problem of the unboundedness of the likelihood at the boundaries of
the parameter space (Redner and Walker, 1984; McLachlan and Peel, 2000), and to prevent
the likelihood from diverging.
3.3. Proof of Theorem 1
Proof sketch of Theorem 1. At a high level, the main idea here is to study the Lasso
estimator, Theorem 1, as a solution of the l1-ball PMLE, Theorem 2, which is defined later
in this section. The proof of Theorem 2 can be deduced from Propositions 3 and 4, which
deal with the cases for small and large values of Yand are proved in the supplementary
materials.
l1-ball PMLE. We need to define the l1-ball PMLE for the statement of Theorem 2. For
this, by restricting S, to a suitable l1-ball of the parameters γ, βon the definition of S, we
define a collection of l1-ball models Sm, where m∈N⋆is a radius of the l1-ball, as follows:
Sm={sψ∈S| ∥γ∥1+∥vec(β)∥1≤m}. (12)
Then for some ηm≥0, letbsmbe a ηm-log-likelihood estimator (LLE) in Sm, defined as:
−1
nnX
i=1ln (bsm(yi|xi))≤inf
sm∈Sm 
−1
nnX
i=1ln (sm(yi|xi))!
+ηm. (13)
As is always the case, it is not sufficient to use the LLE of the estimate in each model as a
criterion. It underestimates the risk of the estimate and the result is a choice of model that
is too complex. In the context of the PMLE, by adding an appropriate penalty pen(m), one
hopes to create a trade-off between good data fit and model complexity. Suppose that for
allm∈N⋆, the penalty function satisfies pen(m) =λm, where λwill be determined as in
(7). Then, for some η≥0, an l1-ball PMLE is defined as bsbm, where bmsatisfies
−1
nnX
i=1ln (bsbm(yi|xi)) + pen( bm)≤inf
m∈N⋆ 
−1
nnX
i=1ln (bsm(yi|xi)) + pen( m)!
+η. (14)

--- PAGE 9 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Note that the error terms ηmandηare necessary to avoid any existence problems, e.g., the
infimum may not be reached. Roughly speaking, the Ekeland variational principle states
that for any extended-valued lower semicontinuous function, which is bounded below, one
can add a small perturbation to ensure the existence of the minimum, see e.g., Borwein and
Zhu (2004). This framework is also used in Montuelle and Le Pennec (2014), and Nguyen
et al. (2022b). Next, we state an l1-ball model selection via Theorem 2.
Theorem 2 ( l1-ball model selection) Assume that 
x[n], y[n]
∈([0,1]p×Rq)n, come
from an unknown conditional PDF s0≡sψ0∈S, defined in (3). Given C1nin(8), ifλ
satisfies (7), the l1-ball PMLE bsbm, defined in (14), satisfies the oracle inequality:
E[KLn(s0,bsbm)]≤
1 +1
κ
inf
m∈N⋆
inf
sm∈SmKLn(s0, sm) +pen(m) +ηm
+η+r
K
nC1n.
(15)
Proof of Theorem 1. Letλ >0 and define bmto be the smallest integer such that bsLasso
λ
belongs to Sbm,i.e.,bm:=ψ[1,2]
1
≤ψ[1,2]
1+ 1. Then using the definition of bm,(6),
(12), and S=S
m∈N⋆Sm, we get
−1
nnX
i=1ln 
bsLasso
λ(yi|xi)
+λbm≤ −1
nnX
i=1ln 
bsLasso
λ(yi|xi)
+λψ[1,2]
1+ 1
= inf
sψ∈S 
−1
nnX
i=1ln (sψ(yi|xi)) +λψ[1,2]
1!
+λ
= inf
m∈N⋆ 
inf
sψ∈Sm 
−1
nnX
i=1ln (sψ(yi|xi)) +λψ[1,2]
1!!
+λ
≤inf
m∈N⋆ 
inf
sm∈Sm 
−1
nnX
i=1ln (sm(yi|xi)) +λm!!
+λ,
which implies
−1
nnX
i=1ln 
bsLasso
λ(yi|xi)
+ pen( bm)≤inf
m∈N⋆ 
−1
nnX
i=1ln (bsm(yi|xi)) + pen( m)!
+η
with pen(m) =λm, η =λ, andbsmis aηm-log-likelihood minimizer in Sm, with ηm≥0
defined by (13). Thus, bsLasso
λsatisfies (14) with bsLasso
λ≡bsbm,i.e.,
−1
nnX
i=1ln (bsbm(yi|xi)) + pen( bm)≤inf
m∈N⋆ 
−1
nnX
i=1ln (bsm(yi|xi)) + pen( m)!
+η.
Then, Theorem 2 implies that if
λ≥κKB 0n√n
qlnnp
ln(2p+ 1) + 1
,
B0n= max ( AΣ,1 +KAG)
1 + 2 q√qAΣ 
5A2
β+ 4AΣlnn
,

--- PAGE 10 ---
Nguyen Nguyen Chamroukhi McLachlan
for some absolute constants κ≥148, Theorem 1 holds as required.
Proof of Theorem 2. Given any Mn>0, this can be done by introducing an event Tand
a space Fmas follows:
T=
max
i∈[n]∥Yi∥∞= max
i∈[n]max
z∈[q]|[Yi]z| ≤Mn
,TC=
max
i∈[n]∥Yi∥∞= max
i∈[n]max
z∈[q]|[Yi]z|> M n
,
Fm=
fm=−lnsm
s0
= ln( s0)−ln(sm), sm∈Sm
.
Conditional on {xi}i∈[n], letY′
[n]|x[n]≡(Y′
i|xi)i∈[n]be IID random samples from Yarising
from the conditional PDF s0(·|xi), i∈[n]. They are independence copies of the sample
Y[n]|x[n]. By taking into account the definition of average KL loss function from (4), the
conditional expectation property, we obtain
KLn(s0,bsbm) =EY′
[n]|x[n]"
1
nnX
i=1bfbm(Yi|xi)|T#
IT+EY′
[n]|x[n]"
1
nnX
i=1bfbm(Yi|xi)|TC#
ITC
≡(KL n(s0,bsbm)|T)IT+ 
KLn(s0,bsbm)|TC
ITc. (16)
From now on, when there is no confusion, the expectation of (16) is written as follows:
E[KLn(s0,bsbm)] =EY[n][(KL n(s0,bsbm)|T)IT] +EY[n] 
KLn(s0,bsbm)|TC
ITc
≡E[KLn(s0,bsbm)IT] +E[KLn(s0,bsbm)ITC]. (17)
Therefore, on the basis of the above remark (17), Theorem 2 is proved by obtaining the
upper bound for each of the following terms using Propositions 3 and 4:
E[KLn(s0,bsbm)] =E[KLn(s0,bsbm)IT] +E[KLn(s0,bsbm)ITC].
Given some constants κ≥148, we need to define the following condition for λ, given
some constant Mn>0:
λ≥κK√nC3n, C3n=Bn
qlnnp
ln(2p+ 1) + 1
, (18)
Bn= max ( AΣ,1 +KAG)h
1 +q√q(Mn+Aβ)2AΣi
. (19)
Proposition 3 (Small values of Y)Assume that 
x[n], y[n]
∈([0,1]p×Rq)ncomes from
an unknown conditional PDF s0≡sψ0∈Sdefined in (3). Given C2nandBndefined as in
(9)and(19), respectively, if λsatisfies (18), the l1-ball PMLE bsbm, defined in (14), satisfies:
E[KLn(s0,bsbm)IT]≤ 
1 +κ−1
inf
m∈N⋆
inf
sm∈SmKLn(s0, sm) +pen(m) +ηm
+η+r
K
nBnC2n.
Proposition 4 (Large values of Y)Consider s0,T, andbsbmas defined in Proposition 3
andHs0as defined in (9). Then,
E[KLn(s0,bsbm)ITC]≤ 
eq/2−1πq/2
Aq/2
Σ+Hs0!
p
2KnqA γe−M2n−2MnAβ
4AΣ.

--- PAGE 11 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Proposition 3 constitutes our important technical contribution. Via Lemma 5, the main idea
to prove Proposition 3 is to control the following deviation on the event T:
sup
fm∈Fm|νn(−fm)| ≡ sup
fm∈Fm1
nnX
i=1{fm(Yi|xi)−E[fm(Yi|xi)]}, (20)
Fm=
fm=−lnsm
s0
= ln( s0)−ln(sm), sm∈Sm
. (21)
Lemma 5 (Control of deviation) For each m′∈N⋆, let
∆m′=m′p
ln(2p+ 1) ln n+ 2√
K
Aγ+qAβ+q√q
aΣ
. (22)
Then, on the event T, for all m′∈N⋆, and for all t >0, with probability greater than 1−e−t,
sup
fm′∈Fm′|νn(−fm′)| ≤4KBn√n
37q∆m′+√
2
Aγ+qAβ+q√q
aΣ√
t
. (23)
The proof of Lemma 5 appears in the supplementary material and follows the arguments
developed in the proof of Massart (2007, Theorem 7.11). The proof of Proposition 3 is in the
spirit of Vapnik’s method of structural risk minimization, first established in Vapnik (1982)
and briefly summarized in Section 8.2 of Massart (2007). In particular, we use concentration
inequalities combined with symmetrization arguments to obtain an upper bound on the
empirical process in expectation from (20). Our technique combines Vapnik’s structural
risk minimization paradigm ( e.g., Vapnik, 1982) and model selection theory for conditional
density estimation ( e.g., Cohen and Le Pennec, 2011), which extend the density estimation
results of Massart (2007).
4. Numerical experiments
In this section, we empirically validate the convergence rate of the error upper bound in (10)
from Theorem 1 in our SGMoE models. For simplicity, we only perform a simulation study
to illustrate the convergence rates when X ⊂Rp, p = 6, and Y ⊂Rq, q = 1. All the
following simulations were performed in R 4.3.2 on a standard Unix machine. We construct
simulated data sets sampling from the true conditional density, s0, belongs to the class of
SGMoE models S:
s0(y|x) =2X
k=1exp 
γ0k0+γ⊤
0kx
P2
l=1exp 
γ0l0+γ⊤
0lxN(y;β0k0+β0kx,Σ0k).
Here, the true parameters for the true SGMoE model are given by:
(γ010, γ01)⊤= (1,2,0,0,−1,0,0)⊤; Σ 01= Σ 02= 1;
(β010, β01)⊤= (0,0,1.5,0,0,0,1)⊤; (β020, β02)⊤= (0,1,−1.5,0,0,2,0)⊤.
Here we implement the l1-PMLE using the EM algorithm for the SGMoE model with
coordinate ascent algorithm for updating the gating network, following the strategy of
Chamroukhi and Huynh (2018, 2019).

--- PAGE 12 ---
Nguyen Nguyen Chamroukhi McLachlan
We want to empirically validate the convergence rate of the error upper bound in
terms of the KL divergence, which cannot be exactly calculated in the case of Gaussian
mixtures. Thus, we assess the divergence through a Monte Carlo simulation, given that
we have knowledge of the true density. It is important to mention that the variability in
this randomized approximation has been shown to be minimal in practice, a fact that is
corroborated by the numerical experiments conducted by Nguyen et al. (2022b); Montuelle
and Le Pennec (2014). Specifically, we calculate the Monte Carlo approximation for the
average KL divergence KL n(s0,bsLasso
λ) as described below:
1
nnX
i=1KL 
s0(·|xi),bsLasso
λ(·|xi)
≈1
nnynX
i=1nyX
j=1lns0(yij|xi)
bsLasso
λ(yij|xi)
.
Here ( yij)j∈[ny]are drawn from s0(·|xi). Then E
KLn 
s0,bsLasso
λ
is approximated again by
averaging over ntMonte Carlo trials. Therefore, the simulated data used for approximation
can be written as ( xi, yij)twith i∈[n], j∈[ny], t∈[nt]. Figure 1 illustrates that the error
decreases with order C1n√
K/√n, as predicted theoretically in Theorem 1, as the sample
sizenincreases when applying the penalty based on our criterion.
0.0050.0100.030
1000 3000 10000 30000
Sample sizeMean of Kullback Leibler distanceError decay
E[AKL]
linear regression of E[AKL]
n −> c/sqrt(n)
Figure 1: Average KL divergence between the true and selected densities based on the Lasso
estimator, represented in a log-log scale, using 100 different choices of sample size
nbetween 1000 and 32000 over nt= 20 trials and ny= 30. A free least-square
regression with confidence interval and a regression with slope −1/2 were added
to stress the two different behavior for each graph.

--- PAGE 13 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
5. Conclusions
To the best of our knowledge, we are the first to establish an l1-oracle inequality for SGMoE
models from a non-asymptotic perspective, under the mildest assumptions, namely the
boundedness of the parameter space, which provides a lower bound for the regularization
parameter of the Lasso, while ensuring non-asymptotic theoretical control of the KL loss of
the estimator. We further conduct a simulation study to empirically confirm our theoretical
results. We believe that our contribution assists in further popularizing MoE models by
providing a theoretical basis for their application to heterogeneous high-dimensional data.
References
Sylvain Arlot and Alain Celisse. A survey of cross-validation procedures for model selection.
Statistics Surveys , 4:40–79, January 2010.
Andrew Barron, Lucien Birg´ e, and Pascal Massart. Risk bounds for model selection via
penalization. Probability theory and related fields , 113:301–413, 1999.
Andrew R. Barron, Albert Cohen, Wolfgang Dahmen, and Ronald A. DeVore. Approximation
and learning by greedy algorithms. The Annals of Statistics , 36(1):64 – 94, 2008.
Jonathan M Borwein and Qiji J Zhu. Techniques of Variational Analysis . Springer New
York, 2004.
Faicel Chamroukhi and Bao Tuyen Huynh. Regularized maximum-likelihood estimation of
mixture-of-experts for regression and clustering. In 2018 International Joint Conference
on Neural Networks (IJCNN) , pages 1–8, 2018.
Faicel Chamroukhi and Bao Tuyen Huynh. Regularized maximum likelihood estimation
and feature selection in mixtures-of-experts models. Journal de la Soci´ et´ e Fran¸ caise de
Statistique , 160(1):57–85, 2019.
S X Cohen and Erwan Le Pennec. Conditional density estimation by penalized likelihood
model selection and applications. Technical report, INRIA , 2011.
Thomas M Cover. Elements of information theory . John Wiley & Sons, 1999.
Emilie Devijver. An l1-oracle inequality for the Lasso in multivariate finite mixture of
multivariate Gaussian regression models. ESAIM: PS , 19:649–670, 2015.
Johannes Jisse Duistermaat and Johan AC Kolk. Multidimensional real analysis I: differen-
tiation , volume 86. Cambridge University Press, 2004.
Jianqing Fan and Runze Li. Variable selection via nonconcave penalized likelihood and its
oracle properties. Journal of the American statistical Association , 96(456):1348–1360,
2001.
Christopher R Genovese and Larry Wasserman. Rates of convergence for the Gaussian
mixture sieve. The Annals of Statistics , 28(4):1105–1127, aug 2000.

--- PAGE 14 ---
Nguyen Nguyen Chamroukhi McLachlan
Gene H Golub and Charles F Van Loan. Matrix computations , volume 3. JHU press, 2012.
Nhat Ho and XuanLong Nguyen. Convergence rates of parameter estimation for some weakly
identifiable finite mixtures. The Annals of Statistics , 44(6):2726 – 2755, 2016.
Nhat Ho, Chiao-Yu Yang, and Michael I. Jordan. Convergence Rates for Gaussian Mixtures
of Experts. Journal of Machine Learning Research , 23(323):1–81, 2022.
Roger A Horn and Charles R Johnson. Matrix analysis . Cambridge University Press, 2012.
Bao Tuyen Huynh and Faicel Chamroukhi. Estimation and feature selection in mixtures of
generalized linear experts models. arXiv preprint arXiv:1907.06994 , 2019.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive
mixtures of local experts. Neural computation , 3(1):79–87, 1991.
J L W V Jensen. Sur les fonctions convexes et les in´ egalit´ es entre les valeurs moyennes. Acta
Mathematica , 30(1):175–193, 1906.
Wenxin Jiang and Martin A Tanner. Hierarchical mixtures-of-experts for exponential
family regression models: approximation and maximum likelihood estimation. Annals of
Statistics , pages 987–1011, 1999.
Abbas Khalili. New estimation and feature selection methods in mixture-of-experts models.
Canadian Journal of Statistics , 38(4):519–539, 2010.
Abbas Khalili and Jiahua Chen. Variable selection in finite mixture of regression models.
Journal of the american Statistical association , 102(479):1025–1038, 2007.
Luke R Lloyd-Jones, Hien D Nguyen, and Geoffrey J McLachlan. A globally convergent
algorithm for lasso-penalized mixture of linear regression models. Computational Statistics
& Data Analysis , 119:19–38, 2018.
Jan R Magnus and Heinz Neudecker. Matrix differential calculus with applications in
statistics and econometrics . John Wiley & Sons, 2019.
Masud Mansuripur. Introduction to information theory . Prentice-Hall, Inc., 1987.
Pascal Massart. Concentration Inequalities and Model Selection: Ecole d’Et´ e de Probabilit´ es
de Saint-Flour XXXIII-2003 . Springer, 2007.
Pascal Massart and Caroline Meynet. The Lasso as an l1-ball model selection procedure.
Electronic Journal of Statistics , 5:669 – 687, 2011.
Pascal Massart and Caroline Meynet. Some Rates of Convergence for the Selected Lasso
Estimator. In Algorithmic Learning Theory , pages 17–33, Berlin, Heidelberg, 2012. ISBN
978-3-642-34106-9.
Cathy Maugis and Bertrand Michel. A non asymptotic penalized criterion for gaussian
mixture model selection. ESAIM: Probability and Statistics , 15:41–68, 2011.

--- PAGE 15 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Cathy Maugis-Rabusseau and Bertrand Michel. Adaptive density estimation for clustering
with Gaussian mixtures. ESAIM: Probability and Statistics , 17:698–724, 2013.
G J McLachlan and D Peel. Finite Mixture Models . John Wiley & Sons, 2000.
Eduardo F Mendes and Wenxin Jiang. On convergence rates of mixtures of polynomial
experts. Neural computation , 24(11):3025–3051, 2012.
C Meynet. An l1-oracle inequality for the Lasso in finite mixture Gaussian regression models.
ESAIM: Probability and Statistics , 17:650–671, 2013.
Lucie Montuelle and Erwan Le Pennec. Mixture of Gaussian regressions model with logistic
weights, a penalized maximum likelihood approach. Electronic Journal of Statistics , 8(1):
1661–1695, 2014.
Hien D Nguyen and Faicel Chamroukhi. Practical and theoretical aspects of mixture-of-
experts modeling: An overview. Wiley Interdisciplinary Reviews: Data Mining and
Knowledge Discovery , 8(4):e1246, 2018.
Hien D Nguyen, Luke R Lloyd-Jones, and Geoffrey J McLachlan. A universal approximation
theorem for mixture-of-experts models. Neural computation , 28(12):2585–2593, 2016.
Hien D Nguyen, Faicel Chamroukhi, and Florence Forbes. Approximation results regarding
the multiple-output Gaussian gated mixture of linear experts model. Neurocomputing ,
366:208–214, 2019. ISSN 0925-2312.
Hien D Nguyen, TrungTin Nguyen, Faicel Chamroukhi, and Geoffrey John McLachlan.
Approximations of conditional probability density functions in Lebesgue spaces via mixture
of experts models. Journal of Statistical Distributions and Applications , 8(1):13, 2021.
Huy Nguyen, TrungTin Nguyen, and Nhat Ho. Demystifying Softmax Gating Function
in Gaussian Mixture of Experts. In Thirty-seventh Conference on Neural Information
Processing Systems , 2023.
Huy Nguyen, Pedram Akbarian, TrungTin Nguyen, and Nhat Ho. A General Theory for
Softmax Gating Multinomial Logistic Mixture of Experts. In Forty-first International
Conference on Machine Learning , 2024a.
Huy Nguyen, TrungTin Nguyen, Khai Nguyen, and Nhat Ho. Towards Convergence Rates
for Parameter Estimation in Gaussian-gated Mixture of Experts. In Proceedings of The
27th International Conference on Artificial Intelligence and Statistics , volume 238, pages
2683–2691, May 2024b.
TrungTin Nguyen, Hien D. Nguyen, Faicel Chamroukhi, and Geoffrey J. McLachlan. Ap-
proximation by finite mixtures of continuous density functions that vanish at infinity.
Cogent Mathematics & Statistics , 7(1):1750861, January 2020.
TrungTin Nguyen, Faicel Chamroukhi, Hien D. Nguyen, and Geoffrey J. McLachlan. Ap-
proximation of probability density functions via location-scale finite mixtures in Lebesgue
spaces. Communications in Statistics - Theory and Methods , pages 1–12, May 2022a.

--- PAGE 16 ---
Nguyen Nguyen Chamroukhi McLachlan
TrungTin Nguyen, Hien Duy Nguyen, Faicel Chamroukhi, and Florence Forbes. A non-
asymptotic approach for model selection via penalization in high-dimensional mixture of
experts models. Electronic Journal of Statistics , 16(2):4742 – 4822, 2022b.
XuanLong Nguyen. Convergence of latent mixing measures in finite and infinite mixture
models. The Annals of Statistics , 41(1):370–400, 2013.
Andriy Norets. Approximation of conditional densities by smooth mixtures of regressions.
The Annals of Statistics , 38(3):1733 – 1766, 2010.
Richard A Redner and Homer F Walker. Mixture densities, maximum likelihood and the
EM algorithm. SIAM review , 26(2):195–239, 1984.
Gideon Schwarz. Estimating the dimension of a model. The Annals of Statistics , 6(2):
461–464, 1978.
N Stadler, P Buhlmann, and S van de Geer. l1-penalization for mixture regression models.
TEST , 19:209–256, 2010.
M. Stone. Cross-Validatory Choice and Assessment of Statistical Predictions. Journal of the
Royal Statistical Society: Series B (Methodological) , 36(2):111–133, 1974.
Robert Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal
Statistical Society: Series B (Methodological) , 58(1):267–288, 1996.
AW Van Der Vaart and JA Wellner. Weak Convergence and Empirical Processes: With
Applications to Statistics Springer Series in Statistics , volume 58. Springer, 1996.
Vladimir Vapnik. Estimation of Dependences Based on Empirical Data (Springer Series in
Statistics) . Springer-Verlag, 1982.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint , volume 48.
Cambridge University Press, 2019.
Christopher K Williams and Carl Edward Rasmussen. Gaussian processes for machine
learning , volume 2. MIT press Cambridge, MA, 2006.
S E Yuksel, J N Wilson, and P D Gader. Twenty Years of Mixture of Experts. IEEE
Transactions on Neural Networks and Learning Systems , 23(8):1177–1193, 2012.

--- PAGE 17 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Supplement to “Non-asymptotic oracle inequalities for the
Lasso in high-dimensional mixture of experts” In this supplement, we
provide proofs for Theorem 2,and Propositions 3 and 4 in Appendices A.1, A.2 and A.3,
respectively. We then provide proofs for the remaining lemmas and provide further related
technical results in Appendices B and C, respectively.
Appendix A. Proof of main results
First, it is important to note that the negative of the differential entropy (see, e.g., Mansuripur
(1987, Chapter 9)) of the true unknown conditional density s0∈S, defined in (3), is finite,
see more in Lemma 6, which is proved in Appendix B.3.
Lemma 6 There exists a constant Hs0= max {0,lnCs0}, where Cs0= (4π)−q/2Aq/2
Σ,s.t.
max
0,sup
x∈XZ
Rqln (s0(y|x))s0(y|x)dy
≤Hs0<∞. (24)
We then introduce some definitions and notations that we shall use in the proofs.
Additional notation
For any measurable function f:Rq→R, consider its empirical norm
∥f∥n:=vuut1
nnX
i=1f2(Yi|xi),
and its conditional expectation
EY|X=x[f] :=E[f(Y|X)|X=x] =Z
Rqf(y|x)s0(y|x)dy.
Furthermore, we also define its empirical process
Pn(f) :=1
nnX
i=1f(Yi|xi), (25)
with expectation
P(f) =1
nnX
i=1EYi|Xi=xi[f(Yi|Xi)|Xi=xi] =1
nnX
i=1Z
Rqf(y|xi)s0(y|xi)dy, (26)
and the recentered process
νn(f) :=Pn(f)−P(f) =1
nnX
i=1
f(Yi|xi)−Z
Rqf(y|xi)s0(y|xi)dy
. (27)

--- PAGE 18 ---
Nguyen Nguyen Chamroukhi McLachlan
For all m∈N⋆, recall that we consider the model
Sm={sψ∈S|∥γ∥1+∥vec(β)∥1≤m} ≡n
sψ∈S|ψ[1,2]
1≤mo
,
Fm=
fm=−lnsm
s0
= ln( s0)−ln(sm), sm∈Sm
.
By using the basic properties of the infimum: for every ϵ >0, there exists xϵ∈A, such
that xϵ<infA+ϵ. Then let δKL>0 for all m∈N⋆, and let ηm≥0. It holds that there
exist two functions bsmandsminSm, such that
Pn(−lnbsm)≤inf
sm∈SmPn(−lnsm) +ηm,and (28)
KLn(s0,sm)≤inf
sm∈SmKLn(s0, sm) +δKL. (29)
Define
bfm:=−lnbsm
s0
,andfm:=−lnsm
s0
. (30)
Letη≥0 and fix m∈N⋆. Further, define
cM(m) =
m′∈N⋆|Pn(−lnbsm′) + pen( m′)≤Pn(−lnbsm) + pen( m) +η	
.(31)
A.1. Proof of Theorem 2
LetMn>0 and κ≥148. Assume that, for all m∈N⋆, the penalty function satisfies
pen(m) =λm, with
λ≥κKBn√n
qlnnp
ln(2p+ 1) + 1
. (32)
We derive, from Propositions 3 and 4, that any penalized likelihood estimator bsbmwithbm,
satisfying
−1
nnX
i=1ln (bsbm(yi|xi)) + pen( bm)≤inf
m∈N⋆ 
−1
nnX
i=1ln (bsm(yi|xi)) + pen( m)!
+η,
for some η≥0, yields
E[KLn(s0,bsbm)] =E[KLn(s0,bsbm)IT] +E[KLn(s0,bsbm)ITc]
≤ 
1 +κ−1
inf
m∈N⋆
inf
sm∈SmKLn(s0, sm) + pen( m) +ηm
+302K3/2qBn√n 
1 +
Aγ+qAβ+q√q
aΣ2!
+η
+ 
eq/2−1πq/2
Aq/2
Σ+Hs0!
p
2KnqA γe−M2n−2MnAβ
4AΣ. (33)

--- PAGE 19 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
To obtain inequality (15), it only remains to optimize the inequality (33), with respect Mn.
Since the two terms depending on Mn, in(33), have opposite monotonicity with respect to
Mn, we are looking for a value of Mnsuch that these two terms are the same order with
respect to n. Consider the positive solution Mn=Aβ+q
A2
β+ 4AΣlnnof the equation
X(X−2Aβ)
4AΣ−lnn= 0. Then, on the one hand,
e−M2n−2MnAβ
4AΣ√n=e−lnn√n=1√n.
On the other hand, using the inequality ( a+b)2≤2(a2+b2), we have
Bn= max ( AΣ,1 +KAG)
1 +q√q(Mn+Aβ)2AΣ
= max ( AΣ,1 +KAG)
1 +q√qAΣ
2Aβ+q
A2
β+ 4AΣlnn2
≤max ( AΣ,1 +KAG) 
1 + 2 q√qAΣ 
5A2
β+ 4AΣlnn
,
hence (33) implies (15).
A.2. Proof of Proposition 3
For every m′∈cM(m), from (31), (30), and (28), we obtain
Pn
bfm′
+ pen( m′) =Pn(ln(s0)−ln (bsm′)) + pen( m′) (using (30))
≤Pn(ln(s0)−ln (bsm)) + pen( m) +η(using (31))
≤Pn(ln(s0)−ln (sm)) +ηm+ pen( m) +η
(using (28) with sminSmand linearity of Pn)
=Pn 
fm
+ pen( m) +ηm+η(using (30)) .
By the definition of recentered process, νn(·), in (27), it holds that
P
bfm′
+ pen( m′)≤P 
fm
+ pen( m) +νn 
fm
−νn
bfm′
+η+ηm.
Taking into account (4) and (25), we obtain
KLn(s0,bsm′) =1
nnX
i=1Z
Rqlns0(y|xi)
bsm′(y|xi)
s0(y|xi)dy=1
nnX
i=1Z
Rqbfm′(y|xi)s0(y|xi)dy(using (30))
=P
bfm′
(using (26)) .
Similarly, we also obtain KL n(s0,sm) =P 
fm
. Hence, (29) implies that
KLn(s0,bsm′) + pen( m′)≤KLn(s0,sm) + pen( m) +νn 
fm
−νn
bfm′
+η+ηm
≤inf
sm∈SmKLn(s0, sm) + pen( m) +νn 
fm
−νn
bfm′
+ηm+δKL+η. (34)

--- PAGE 20 ---
Nguyen Nguyen Chamroukhi McLachlan
All that remains is to control the deviation of −νn
bfm′
=νn
−bfm′
. To handle the
randomness of bfm′, we shall control the deviation of supfm′∈Fm′νn(−fm′), since bfm′∈Fm′.
Such control is provided by Lemma 5. From (34)and(23), we derive that on the event T,
for all m∈N⋆, and t >0, with probability larger than 1 −e−t,
KLn(s0,bsm′) + pen( m′)≤inf
sm∈SmKLn(s0, sm) + pen( m) +νn 
fm
−νn
bfm′
+ηm+δKL+η
≤inf
sm∈SmKLn(s0, sm) + pen( m) +νn 
fm
+ηm+δKL+η
+4KBn√n
37q∆m′+√
2
Aγ+qAβ+q√q
aΣ√
t
≤inf
sm∈SmKLn(s0, sm) + pen( m) +νn 
fm
+ηm+δKL+η
+4KBn√n"
37q∆m′+1
2
Aγ+qAβ+q√q
aΣ2
+t#
,ifm′∈cM(m).
(35)
Here, we use the fact that bfm′∈Fm′and get the last inequality using the fact that
2ab≤a2+b2forb=√
t, and a=
Aγ+qAβ+q√q
aΣ
/√
2.
It remains to sum up the tail bounds (35) over all possible values of m∈N⋆and
m′∈cM(m). To get an inequality valid on a set of high probability, we need to adequately
choose the value of the parameter t, depending on m∈N⋆andm′∈cM(m). Let z >0, for
allm∈N⋆andm′∈cM(m), and apply (35)to obtain t=z+m+m′. Then, on the event
T, for all m∈N⋆, with probability larger than 1 −e−(z+m+m′),
KLn(s0,bsm′) + pen( m′)≤inf
sm∈SmKLn(s0, sm) + pen( m) +νn 
fm
+ηm+δKL+η
+4KBn√n"
37q∆m′+1
2
Aγ+qAβ+q√q
aΣ2
+ 
z+m+m′#
,
ifm′∈cM(m). (36)
Here, (36) is equivalent to
KLn(s0,bsm′)−νn 
fm
≤inf
sm∈SmKLn(s0, sm) +
pen(m) +4KBn√nm
+ηm+δKL+η
+4KBn√n 
37q∆m′+m′
−pen(m′)
+4KBn√n"
1
2
Aγ+qAβ+q√q
aΣ2
+z#
. (37)
Note that with probability larger than 1 −e−z,(36)holds simultaneously for all m∈N⋆
andm′∈cM(m). Indeed, by defining the event
∩(m,m′)∈N⋆×cM(m)Ωm,m′={w:w∈Ω such that the event in (36) holds },

--- PAGE 21 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
it holds that, on the event T,
P
∩(m,m′)∈N⋆×cM(m)Ωm,m′
= 1−P
∪(m,m′)∈N⋆×cM(m)ΩC
m,m′
≥1−X
(m,m′)∈N⋆×cM(m)P 
ΩC
m,m′
≥1−X
(m,m′)∈N⋆×cM(m)e−(z+m+m′)
≥1−X
(m,m′)∈N⋆×N⋆e−(z+m+m′)
= 1−e−z X
m∈N⋆e−m!2
≥1−e−z,
where we get the last inequality by using the the geometric series
∞X
m=1 
e−1m=∞X
m=0 
e−1m−1 =1
1−e−1−1 =e
e−1−1 =1
e−1<1.
Taking into account (22), we get
KLn(s0,bsm′)−νn 
fm
≤inf
sm∈SmKLn(s0, sm) +
pen(m) +4KBn√nm
+ηm+δKL+η
+4KBn√n
37qlnnp
ln(2p+ 1) + 1
m′−pen(m′)
+4KBn√n"
1
2
Aγ+qAβ+q√q
aΣ2
+ 74q√
K
Aγ+qAβ+q√q
aΣ
+z#
.
(38)
Now, let κ≥1 and assume that pen( m) =λm, for all m∈N⋆with
λ≥κ4KBn√n
37qlnnp
ln(2p+ 1) + 1
. (39)

--- PAGE 22 ---
Nguyen Nguyen Chamroukhi McLachlan
Then, (38) implies
KLn(s0,bsm′)−νn 
fm
≤inf
sm∈SmKLn(s0, sm) +
λm+4KBn√nm
+ηm+δKL+η
+
4KBn√n
37qlnnp
ln(2p+ 1) + 1
| {z }
≤λκ−1m′−λm′

+4KBn√n"
1
2
Aγ+qAβ+q√q
aΣ2
+ 74q√
K
Aγ+qAβ+q√q
aΣ
+z#
≤inf
sm∈SmKLn(s0, sm) +
pen(m) +4KBn√nm
|{z}
≤κ−1pen(m)
+ηm+δKL+η
+
λκ−1m′−λm′
| {z }
≤0
+4KBn√n"
1
2
Aγ+qAβ+q√q
aΣ2
+ 74q√
K
Aγ+qAβ+q√q
aΣ
+z#
≤inf
sm∈SmKLn(s0, sm) + 
1 +κ−1
pen(m) +ηm+δKL+η
+4KBn√n"
1
2
Aγ+qAβ+q√q
aΣ2
+ 74q√
K
Aγ+qAβ+q√q
aΣ
+z#
.
Next, using the inequality 2 ab≤β−1a2+β−1b2fora=√
K,b=K
Aγ+qAβ+q√q
aΣ
, and
β=√
K, and the fact that K≤K3/2, for all K∈N⋆, it follows that
KLn(s0,bsm′)−νn 
fm
≤inf
sm∈SmKLn(s0, sm) + 
1 +κ−1
pen(m) +ηm+δKL+η
+4Bn√n"
qK3/2
2
Aγ+qAβ+q√q
aΣ2
+ 74q√
KK
Aγ+qAβ+q√q
aΣ
| {z }
37q×2ab+Kz#
≤inf
sm∈SmKLn(s0, sm) + 
1 +κ−1
pen(m) +ηm+δKL+η
+4Bn√n"
37qK1/2+75qK3/2
2
Aγ+qAβ+q√q
aΣ2
+Kz#
. (40)

--- PAGE 23 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
By(14)and(31),bmbelongs to cM(m), for all m∈N⋆, so we deduce from (40)that on the
eventT, for all z >0, with probability greater than 1 −e−z,
KLn(s0,bsbm)−νn 
fm
≤inf
m∈N⋆
inf
sm∈SmKLn(s0, sm) + 
1 +κ−1
pen(m) +ηm
+η+δKL
+4Bn√n"
37qK1/2+75qK3/2
2
Aγ+qAβ+q√q
aΣ2
+Kz#
.
(41)
Note that for any non-negative random variable Zand any a >0,E[Z]=aR
z≥0P(Z >
az)dz. Indeed, if we let t=az, then dz=adtand
aZ
z≥0P(Z > az )dz=aZ∞
0Z∞
azfZ(u)dudz =Z∞
0Z∞
tfZ(u)dudt =Z∞
0Zu
0fZ(u)dtdu
=Z∞
0fZ(u)Zu
0dtdu =Z∞
0fZ(u)udu=E[Z]. (42)
Then, we define the following random variable w.r.t. the random response Y[n]:= (Yi)i∈[n]:
Z:= KL n(s0,bsbm)−νn 
fm
−
inf
m∈N⋆
inf
sm∈SmKLn(s0, sm) + 
1 +κ−1
pen(m) +ηm
+η+δKL
−4Bn√n"
37qK1/2+75qK3/2
2
Aγ+qAβ+q√q
aΣ2#
.
Then by (41), on the even T, it holds that P(Z≤az)≥1−e−zand if Z≤0 then
P(Z < az ) = 1≥1−e−z, for all z >0, where a=4BnK√n>0. Therefore, it is sufficient to
consider Z≥0 and it holds that P(Z > az |T)≤e−z. In this case, by (42)and the fact that
P(T)≤1, it holds that
EY[n][ZIT] =P(T)EY[n][Z|T]≤EY[n][Z|T]≤aZ
z≥0e−zdz=a. (43)
Then, by integrating (41) over z >0 using (43), the fact that
EY[n]
νn 
fm
=EY[n]
Pn 
fm
−EY[n]
P 
fm
= 0, (44)

--- PAGE 24 ---
Nguyen Nguyen Chamroukhi McLachlan
δKL>0 can be chosen arbitrary small, and EY[n][IT] =P(T)≤1, we obtain that
E[KLn(s0,bsbm)IT]≤
inf
m∈N⋆
inf
sm∈SmKLn(s0, sm) + 
1 +κ−1
pen(m) +ηm
+η
EY[n][IT]
+4Bn√n"
37qK1/2+75qK3/2
2
Aγ+qAβ+q√q
aΣ2
+K#
EY[n][IT]
≤inf
m∈N⋆
inf
sm∈SmKLn(s0, sm) + 
1 +κ−1
pen(m) +ηm
+η
+4Bn√n"
37qK3/2+75qK3/2
2
Aγ+qAβ+q√q
aΣ2
+qK3/2#
≤inf
m∈N⋆
inf
sm∈SmKLn(s0, sm) + 
1 +κ−1
pen(m) +ηm
+η
+302K3/2qBn√n 
1 +
Aγ+qAβ+q√q
aΣ2!
. (45)
A.3. Proof of Proposition 4
By the Cauchy-Schwarz inequality,
E[KLn(s0,bsbm)ITC]≤q
E
KL2
n(s0,bsbm)q
E
I2
TC
=q
E
KL2
n(s0,bsbm)q
P(TC).(46)
We seek to bound the two terms of the right-hand side of (46).
For the first term, let us bound KL (s0(·|x), sψ(·|x)), for all sψ∈Sandx∈ X. Let
sψ∈Sandx∈ X. Then, we obtain
KL (s0(·|x), sψ(·|x)) =Z
Rqlns0(y|x)
sψ(y|x)
s0(y|x)dy
=Z
Rqln (s0(y|x))s0(y|x)dy−Z
Rqln (sψ(y|x))s0(y|x)dy
≤ −Z
Rqln (sψ(y|x))s0(y|x)dy+Hs0,∀x∈ X(using (24)) .(47)
Since
aG:=exp (−Aγ)PK
l=1exp (Aγ)≤sup
x∈X,γ∈eΓexp 
γk0+γ⊤
kx
PK
l=1exp 
γl0+γ⊤
lx≤exp (Aγ)PK
l=1exp (−Aγ)=:AG,
there exists deterministic positive constants aG, AG, such that
aG≤sup
x∈X,γ∈eΓgk(x;γ)≤AG. (48)
Here, the softmax gating function gk(x;γ) is described as
gk(x;γ) =exp (wk(x))PK
l=1exp (wl(x)), wk(x) =γk0+γ⊤
kx, γ=
γk0, γ⊤
k
k∈[K]∈Γ =R(p+1)K.(49)

--- PAGE 25 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Thus, for all y∈Rq,
ln (sψ(y|x))s0(y|x)
≥ln"KX
k=1aGdet(Σ−1
k)1/2
(2π)q/2exp
−
y⊤Σ−1
ky+ (βk0+βkx)⊤Σ−1
k(βk0+βkx)#
×KX
k=1aGdet(Σ−1
0,k)1/2
(2π)q/2exp
−
y⊤Σ−1
0,ky+ (β0,k0+β0,kx)⊤Σ−1
0,k(β0,k0+β0,kx)

using (48) and −(a−b)⊤A(a−b)/2≥ −(a⊤Aa+b⊤Ab),e.g., a=y, b=βk0+βkx,A= Σ k
≥ln"KX
k=1aGaq/2
Σ
(2π)q/2exp
−
y⊤Σ−1
ky+ (βk0+βkx)⊤Σ−1
k(βk0+βkx)#
×KX
k=1aGaq/2
Σ
(2π)q/2exp
−
y⊤Σ−1
0,ky+ (β0,k0+β0,kx)⊤Σ−1
0,k(β0,k0+β0,kx)
(using (2))
≥ln"
KaGaq/2
Σ
(2π)q/2exp
−
y⊤y+qA2
β
AΣ#
×KaGaq/2
Σ
(2π)q/2exp
−
y⊤y+qA2
β
AΣ
(using (2)) ,
(50)
where, in the last inequality, we use the fact that for all u∈Rq. By using the eigenvalue
decomposition of Σ 1=P⊤DP,
u⊤Σ1u=u⊤P⊤DPu≤ ∥Pu∥2≤M(D)∥Pu∥2
2≤AΣ∥u∥2
2≤AΣq∥u∥2
∞,
where in the last inequality, we used the fact that (94). Therefore, setting u=√2AΣyand
h(t) =tlnt, for all t∈R, and noticing that h(t)≥h 
e−1
=−e−1, for all t∈R, and from
(47) and (50), we get that
KL (s0(·|x), sψ(·|x))−Hs0
≤ −Z
Rq"
ln"
Kaγaq/2
Σ
(2π)q/2exp
−
y⊤y+qA2
β
AΣ#
Kaγaq/2
Σ
(2π)q/2exp
−
y⊤y+qA2
β
AΣ!
dy
=−Kaγaq/2
Σe−qA2
βAΣ
(2AΣ)q/2Z
Rq"
ln 
Kaγaq/2
Σ
(2π)q/2!
−qA2
βAΣ−u⊤u
2#
e−u⊤u
2
(2π)q/2du
=−Kaγaq/2
Σe−qA2
βAΣ
(2AΣ)q/2EU""
ln 
Kaγaq/2
Σ
(2π)q/2!
−qA2
βAΣ−U⊤U
2##
(with U∼ N q(0,Iq))
=−Kaγaq/2
Σe−qA2
βAΣ
(2AΣ)q/2"
ln 
Kaγaq/2
Σ
(2π)q/2!
−qA2
βAΣ−q
2#
=−Kaγaq/2
Σe−qA2
βAΣ−q
2
(2π)q/2(AΣ)q/2eq/2πq/2ln 
Kaγaq/2
Σe−qA2
βAΣ−q
2
(2π)q/2!
≤eq/2−1πq/2
Aq/2
Σ, (51)
where we used the fact that tln(t)≥ −e−1, for all t∈R.

--- PAGE 26 ---
Nguyen Nguyen Chamroukhi McLachlan
Then, for all sψ∈S,
KLn(s0, sψ) =1
nnX
i=1KL (s0(·|xi), sψ(.|xi))≤eq/2−1πq/2
Aq/2
Σ+Hs0,
and note that bsbm∈S, thus
q
E
KL2
n(s0,bsbm)
≤eq/2−1πq/2
Aq/2
Σ+Hs0. (52)
We now provide an upper bound for P 
TC
:
P 
TC
≤nX
i=1P(∥Yi∥∞> M n). (53)
For all i∈[n],
Yi|xi∼KX
k=1gk(xi;γ)Nq(βk0+βkxi,Σk),
so we see from (53) that we need to provide an upper bound on P(|Yx|> M n), with
Yx∼KX
k=1gk(x;γ)Nq(βk0+βkx,Σk), x∈ X.
First, using Chernoff’s inequality for a centered Gaussian variable (see Lemma 23), and the
fact that ψbelongs to the bounded space eΨ(defined by (2)), and thatPK
k=1gk(x;γ)= 1,

--- PAGE 27 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
we get
P(∥Yx∥∞> M n)
=KX
k=1gk(x;γ)
(2π)q/2det(Σ k)1/2Z
{∥y∥∞>Mn}exp 
−(y−(βk0+βkx))⊤Σ−1
k(y−(βk0+βkx))
2!
dy
=KX
k=1gk(x;γ)P 
∥Yx,k∥∞> M n
≤KX
k=1gk(x;γ)qX
z=1P [Yx,k]z> M n
=KX
k=1gk(x;γ)qX
z=1 
P 
[Yx,k]z<−Mn
+P 
[Yx,k]z> M n
=KX
k=1gk(x;γ)qX
z=1 
P 
U >Mn−[βk0+βkx]z
[Σk]1/2
z,z!
+P 
U <−Mn−[βk0+βkx]z
[Σk]1/2
z,z!!
=KX
k=1gk(x;γ)qX
z=1 
P 
U >Mn−[βk0+βkx]z
[Σk]1/2
z,z!
+P 
U >Mn+ [βk0+βkx]z
[Σk]1/2
z,z!!
≤KX
k=1gk(x;γ)qX
z=1
e−1
2 
Mn−[βk0+βkx]z
[Σk]1/2
z,z!2
+e−1
2 
Mn+[βk0+βkx]z
[Σk]1/2
z,z!2
(using Lemma 23, (105))
≤2KX
k=1gk(x;γ)qX
z=1e−1
2 
Mn−|[βk0+βkx]z|
[Σk]1/2
z,z!2
≤2KX
k=1gk(x;γ)qX
z=1e−1
2M2n−2Mn|[βk0+βkx]z|+|[βk0+βkx]|2
z
[Σk]z,z
≤2KAγqe−M2n−2MnAβ
2AΣ, (54)
where
Yx,k∼ N q(βk0+βkx,Σk),[Yx,k]z∼ N
[βk0+βkx]z,[Σk]z,z
,andU=[Yx,k]z−[βx]z
[Σk]1/2
z,z∼ N(0,1),
and using the facts that e−1
2|[βk0+βkx]|2
z
AΣ≤1 and max 1≤z≤q[Σk]z,z≤ ∥Σk∥2=M(Σk)=
m 
Σ−1
k
≤AΣ. We derive from (53) and (54) that
P(Tc)≤2KnqA γe−M2n−2MnAβ
2AΣ, (55)
and finally from (46), (52), and (55), we obtain
E[KLn(s0,bsbm)ITC]≤ 
eq/2−1πq/2
Aq/2
Σ+Hs0!
p
2KnqA γe−M2n−2MnAβ
4AΣ. (56)

--- PAGE 28 ---
Nguyen Nguyen Chamroukhi McLachlan
Appendix B. Proofs of technical lemmas
B.1. Proof of Lemma 5
Letm∈N⋆, on the event T, to control the deviation
sup
fm∈Fm|νn(−fm)|= sup
fm∈Fm1
nnX
i=1{fm(Yi|xi)−E[fm(Yi|xi)]}, (57)
we shall use concentration and symmetrization arguments. We shall first use the following
concentration inequality, which is an adaption of Wainwright (2019, Theorem 4.10).
Lemma 7 (Theorem 4.10 from Wainwright (2019)) LetZ1, . . . , Z nbe independent
random variables with values in some space Zand let Fbe a class of integrable real-valued
functions with domain on Z. Assume that
sup
f∈F∥f∥∞≤Rnfor some non-random constants Rn<∞. (58)
Then, for all t >0,
P 
sup
f∈F1
nnX
i=1[f(Zi)−E[f(Zi)]]>E"
sup
f∈F1
nnX
i=1[f(Zi)−E[f(Zi)]]#
+ 2√
2Rnr
t
n!
≤e−t.
(59)
That is, with probability greater than 1−et,
sup
f∈F1
nnX
i=1[f(Zi)−E[f(Zi)]]≤E"
sup
f∈F1
nnX
i=1[f(Zi)−E[f(Zi)]]#
+ 2√
2Rnr
t
n(60)
Then, we propose to bound E
supf∈F1
nPn
i=1[f(Zi)−E[f(Zi)]]
due to the following
symmetrization argument. The proof of this result can be found in Van Der Vaart and
Wellner (1996).
Lemma 8 (See Lemma 2.3.6 in Van Der Vaart and Wellner (1996)) LetZ1, . . . , Z n
be independent random variables with values in some space Zand let Fbe a class of real-
valued functions on Z. Let (ϵ1, . . . , ϵ n)be a Rademacher sequence independent of (Z1, . . . , Z n).
Then,
E"
sup
f∈F1
nnX
i=1[f(Zi)−E[f(Zi)]]#
≤2E"
sup
f∈F1
nnX
i=1ϵif(Zi)#
. (61)
From (61), the problem is to provide an upper bound on
E"
sup
f∈F1
nnX
i=1ϵif(Zi)#
.
To do so, we shall apply the following lemma, which is adapted from Lemma 6.1 in Massart
(2007).

--- PAGE 29 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Lemma 9 (See Lemma 6.1 in Massart (2007)) LetZ1, . . . , Z nbe independent random
variables with values in some space Zand let Fbe a class of real-valued functions on Z.
Let(ϵ1, . . . , ϵ n)be a Rademacher sequence, independent of (Z1, . . . , Z n). Define Rn, a
non-random constant, such that
sup
f∈F∥f∥n≤Rn. (62)
Then, for all S∈N⋆,
E"
sup
f∈F1
nnX
i=1ϵif(Zi)#
≤Rn 
6√nSX
s=12−sq
ln [1 + M(2−sRn,F,∥.∥n)] + 2−S!
,(63)
where M(δ,F,∥.∥n)stands for the δ-packing number (see Definition 20) of the set of functions
F, equipped with the metric induced by the norm ∥·∥n.
We are now able to prove Lemma 5. Indeed, given any fixed values x1, . . . , x n∈ X, in order
to control supfm∈Fm|νn(−fm)|ITfrom the (57), we would like to apply Lemmas 7–9. On
the one hand, we see from (62)that we need an upper bound of supfm∈Fm∥fm∥∞IT. On
the other hand, we see from (63)that on the event T, we need to bound the entropy of the
set of functions Fm, equipped with the metric induced by the norm ∥·∥n. Such bounds are
provided by the two following lemmas.
Recall that given Mn>0, we considered the event
T=
max
i∈[n]∥Yi∥∞= max
i∈[n]max
z∈[q]|[Yi]z| ≤Mn
, (64)
letBn= max ( AΣ,1 +KAG)
1 +q√q(Mn+Aβ)2AΣ
.
Lemma 10 On the event T, for all m∈N⋆,
sup
fm∈Fm∥fm∥∞≤2KBn
Aγ+qAβ+q√q
aΣ
=:Rn. (65)
Proof of Lemma 10. See Appendix B.2.1.
Lemma 11 Letδ >0andm∈N⋆. On the event T, we have the following upper bound of
theδ-packing number of the set of functions Fm, equipped with the metric induced by the
norm ∥·∥n:
M(δ, Fm,∥·∥n)≤(2p+ 1)72B2nq2K2m2
δ2
1 +18BnKqA β
δK
1 +18BnKAγ
δK
1 +18BnKq√q
aΣδK
.
Proof of Lemma 11. See Appendix B.2.2.
Lemma 12 (Lemma 5.9 from Meynet (2013)) Letδ >0and(xij)i∈[n];j=1,...,p∈Rnp.
There exists a family Bof(2p+ 1)∥x∥2
max,n/δ2vectors in Rp, such that for all β∈Rp, with
∥β∥1≤1, where ∥x∥2
max,n=1
nPn
i=1max j∈{1,...,p}x2
ij, there exists β′∈ B, such that
1
nnX
i=1
pX
j=1 
βj−β′
j
xij
2
≤δ2.

--- PAGE 30 ---
Nguyen Nguyen Chamroukhi McLachlan
Proof of Lemma 12. See in the proof of Meynet (2013, Lemma 5.9).
Via the upper bounds provided in Lemmas 10 and 11, we can apply Lemma 9 to get an
upper bound of
E"
sup
fm∈Fm1
nnX
i=1ϵifm(Yi|xi)#
on the event T. (66)
In order to provide such an upper bound, Lemmas 7 and 9 can be utilized via defining a
suitable class of integrable real-valued functions as follows:
F:=
f:=fmI|fm|≤Rn:fm∈Fm,	
, Z i:=Yi|xi,∀i∈[n]. (67)
Indeed, by definition, it holds that
sup
f∈F∥f∥n≤sup
f∈F∥f∥∞= sup
f∈Fsup
z∈Z|f(z)|= sup
fm∈Fmsup
z∈Zfm(z)I|fm(z)|≤Rn≤Rn. (68)
Note that the last inequality is valid since if |fm(z)| ≤Rnthenfm(z)I|fm(z)|≤Rn=
|fm(z)| ≤Rn. Otherwise, if |fm(z)|> R n, thenfm(z)I|fm(z)|≤Rn=|fm(z)×0|= 0≤Rn.
We thus obtain the following results.
Lemma 13 Letm∈N⋆, consider (ϵ1, . . . , ϵ n), a Rademacher sequence independent of
(Y1, . . . , Y n). Then, on the event T, it holds that
E"
sup
fm∈Fm1
nnX
i=1ϵifm(Yi|xi)#
≤74KBnq√n∆m,where
∆m:=mp
ln(2p+ 1) ln n+ 2√
K
Aγ+qAβ+q√q
aΣ
. (69)
Proof of Lemma 13. See Appendix B.2.3.
We now return to the proof of the Lemma 5.

--- PAGE 31 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Finally, on the event T, using (67) and Lemma 10, for all m∈N⋆andt >0, with
probability greater than 1 −e−t, we obtain
sup
fm∈Fm|νn(−fm)|= sup
f∈F1
nnX
i=1{f(Zi)−E[f(Zi)]}(70)
≤E"
sup
f∈F1
nnX
i=1[f(Zi)−E[f(Zi)]]#
+ 2√
2Rnr
t
n(using Lemma 7)
(71)
≤E"
sup
f∈F1
nnX
i=1[f(Zi)−E[f(Zi)]]#
+ 2√
2Rnr
t
n(since ≤1) (72)
≤2E"
sup
f∈F1
nnX
i=1ϵif(Zi)#
+ 2√
2Rnr
t
n(Lemma 8) (73)
= 2E"
sup
fm∈Fm1
nnX
i=1ϵifm(Yi|xi)#
+ 2√
2Rnr
t
n(74)
≤148KBnq√n∆m+ 4√
2KBn
Aγ+qAβ+q√q
aΣr
t
n(75)

using Lemma 13 and Rn= 2KBn
Aγ+qAβ+q√q
aΣ
(76)
≤4KBn√n
37q∆m+√
2
Aγ+qAβ+q√q
aΣ√
t
. (77)
B.2. Proofs of Lemmas 10–13
The proofs of Lemmas 10–11 require an upper bound on the uniform norm of the gradient
of lnsψ, forsψ∈S. We begin by providing such an upper bound.
Lemma 14 Given sψ, as described in (3), it holds that
sup
x∈Xsup
ψ∈eΨ∂ln (sψ(·|x))
∂ψ
∞≤G(·),
G:Rq∋y7→G(y) = max ( AΣ,1 +KAG)
1 +q√q(∥y∥∞+Aβ)2AΣ
.(78)

--- PAGE 32 ---
Nguyen Nguyen Chamroukhi McLachlan
Proof of Lemma 14. Letsψ∈S, with ψ=(γ, β,Σ). From now on, we consider any
x∈ X, any y∈Rq, and any k∈[K]. We can write
ln (sψ(y|x)) = ln KX
k=1gk(x;γ)N(y;βk0+βkx,Σk)!
= ln KX
k=1fk(x, y)!
,
gk(x;γ) =exp (wk(x))PK
l=1exp (wl(x)), wk(x) =γk0+γ⊤
kx,
N(y;βk0+βkx,Σk) =1
(2π)q/2det(Σ k)1/2exp 
−(y−(βk0+βkx))⊤Σ−1
k(y−(βk0+βkx))
2!
,
fk(x, y) =gk(x;γ)N(y;βk0+βkx,Σk)
=gk(x;γ)
(2π)q/2det(Σ k)1/2exp
−1
2(y−(βk0+βkx))⊤Σ−1
k(y−(βk0+βkx))
.
By using the chain rule, for all l∈[K],
∂ln (sψ(y|x))
∂γl0=KX
k=1fk(x, y)
gk(x;γ)PK
k=1fk(x, y)∂gk(x;γ)
∂wl(x)∂wl(x)
∂γl0|{z}
=1,and
∂ln (sψ(y|x))
∂ 
γ⊤
lx=KX
k=1fk(x, y)
gk(x;γ)PK
k=1fk(x, y)∂gk(x;γ)
∂wl(x)∂wl(x)
∂ 
γ⊤
lx
|{z}
=1.
Furthermore,
∂gk(x;γ)
∂wl(x)=∂
∂wl(x) 
exp (wk(x))PK
l=1exp (wl(x))!
=δlkexp (wk(x))PK
l=1exp (wl(x))−exp (wk(x))PK
l=1exp (wl(x))exp (wl(x))PK
l=1exp (wl(x))=gk(x;γ) (δlk−gl(x;γ)),
where δlk=(
1 if l=k,
0 if l̸=k.
Therefore, we obtain
∂ln (sψ(y|x))
∂ 
γ⊤
lx=∂ln (sψ(y|x))
∂γl0=KX
k=1fk(x, y)
gk(x;γ)PK
k=1fk(x, y)gk(x;γ) (δlk−gl(x;γ))
=KX
k=1fk(x, y)PK
k=1fk(x, y)(δlk−gl(x;γ))≤KX
k=1(δlk−gl(x;γ))
=1−KX
k=1gl(x;γ)=|1−Kgl(x;γ)| ≤1 +Kgl(x;γ)≤1 +KAG(using (48)) .

--- PAGE 33 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Similarly, by using the fact that ψbelongs to the bounded space eΨ,fl(x, y)/PK
k=1fk(x, y)≤
1,
∂ln (sψ(y|x))
∂βl0
∞=∂ln (sψ(y|x))
∂(βlx)
∞
=fl(x, y)PK
k=1fk(x, y)∂
∂(βl0+βlx)
−1
2(y−(βl0+βlx))⊤Σ−1
l(y−(βl0+βlx))
∞
≤∂
∂(βl0+βlx)
−1
2(y−(βl0+βlx))⊤Σ−1
l(y−(βl0+βlx))
∞
=Σ−1
l(y−(βl0+βlx))
∞≤Σ−1
l
∞∥(y−(βl0+βlx))∥∞(using (95))
≤√qΣ−1
l
2(∥y∥∞+∥βl0+βlx∥∞) (using (100))
≤√qM 
Σ−1
l
(∥y∥∞+∥βl0+βlx∥∞) (using (99))
≤√qAΣ(∥y∥∞+Aβ) (using (2)) .
Now, we need to calculate the gradient w.r.t. to the covariance matrices of the Gaussian
experts. To do this, we need the following result: given any l∈[K],vl=βl0+βlx, it holds
that
∂
∂ΣlN(x;vl,Σl) =N(x;vl,Σl)1
2h
Σ−1
l(x−vl) (x−vl)⊤Σ−1
l− 
Σ−1
l⊤i
| {z }
T(x,vl,Σl), (79)
noting that
∂
∂Σl
(x−vl)⊤Σ−1
l(x−vl)
=−Σ−1
l(x−vl) (x−vl)⊤Σ−1
l(using Lemma 16) , (80)
∂
∂Σl(det(Σ l)) = det(Σ l) 
Σ−1
l⊤(using Jacobi formula, Lemma 17) .(81)
For any l∈[K],
∂ln (sψ(y|x))
∂
[Σl]z1,z2≤∂ln (sψ(y|x))
∂Σl
2(using (99))
=fl(x, y)PK
k=1fk(x, y)∂
∂Σl
−1
2(y−(βl0+βlx))⊤Σ−1
l(y−(βl0+βlx))
2
≤∂
∂Σl
−1
2(y−(βl0+βlx))⊤Σ−1
l(y−(βl0+βlx))
2
=1
2Σ−1
l(y−(βl0+βlx)) (y−(βl0+βlx))⊤Σ−1
l− 
Σ−1
l⊤
2(using (79))
≤1
2h
AΣ+√q(y−(βl0+βlx)) (y−(βl0+βlx))⊤
∞A2
Σi
(using (100))
≤1
2h
AΣ+q√q(∥y∥∞+Aβ)2A2
Σi
(using (2)) ,

--- PAGE 34 ---
Nguyen Nguyen Chamroukhi McLachlan
where, in the last inequality given a=y−(βl0+βlx), we use the fact that
aa⊤
∞= max
1≤i≤qqX
j=1[aa⊤]i,j= max
1≤i≤qqX
j=1|aiaj|= max
1≤i≤q|ai|qX
j=1|aj| ≤q∥a∥2
∞.
Thus,
sup
x∈Xsup
ψ∈eΨ∂ln (sψ(y|x))
∂ψ
∞
≤max"
1 +KAG,√q(∥y∥∞+Aβ)AΣ,1
2h
AΣ+q√q(∥y∥∞+Aβ)2A2
Σi#
≤max"
1 +KAG,max ( AΣ,1)
1 +q√q(∥y∥∞+Aβ)2AΣ#
≤max ( AΣ,1 +KAG)
1 +q√q(∥y∥∞+Aβ)2AΣ
=:G(y),
where we use the fact that
√q(∥y∥∞+Aβ)AΣ=:θ≤1 +θ2= 1 + q(∥y∥∞+Aβ)2A2
Σ
≤max ( AΣ,1)
1 +q√q(∥y∥∞+Aβ)2AΣ
.
B.2.1. Proof of Lemma 10
Letm∈N⋆andfm∈Fm. By (21), there exists sm∈Sm, such that fm=−ln (sm/s0).
For all x∈ X, letψ(x) = 
γk0, γ⊤
kx, βk0, βkx,Σk
k∈[K]be the parameters of sm(·|x). In our
case, we approximate f(ψ) =ln (sψ(yi|xi))around ψ0(xi) by the n= 0thdegree Taylor
polynomial of f(ψ). That is,
|ln (sm(yi|xi))−ln (s0(yi|xi))|=:|f(ψ)−f(ψ0)|=|R0(ψ)|(defined in Lemma 24)
≤sup
x∈Xsup
ψ∈eΨ∂ln (sψ(yi|x))
∂ψ
∞∥ψ(xi)−ψ0(xi)∥1.

--- PAGE 35 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
First applying Taylor’s inequality and then Lemma 14 on the event T. For all i∈[n], it
holds that
|fm(yi|xi)|IT=|ln (sm(yi|xi))−ln (s0(yi|xi))|IT≤sup
x∈Xsup
ψ∈eΨ∂ln (sψ(yi|x))
∂ψ
∞∥ψ(xi)−ψ0(xi)∥1IT
≤max ( AΣ,1 +KAG)
1 +q√q(Mn+Aβ)2AΣ
| {z }
=:Bn∥ψ(xi)−ψ0(xi)∥1(using Lemma 14)
≤BnKX
k=1
|γk0−γ0,k0|+γ⊤
kxi−γ⊤
0,kxi+∥βk0−β0,k0∥1+∥βkxi−β0,kxi∥1+∥vec (Σ k−Σ0,k)∥1
≤2BnKX
k=1
|γk0|+γ⊤
kxi+∥βk0∥1+∥βkxi∥1+q∥Σk∥1
(using (97))
≤2KBn(Aγ+q∥βk0∥∞+q∥βkxi∥∞+q√q∥Σk∥2) (using (2), (92), (93), (101))
≤2KBn
Aγ+qAβ+q√q
aΣ
(using (2)) .
Therefore, on the event T,
sup
fm∈Fm∥fm∥∞≤2KBn
Aγ+qAβ+q√q
aΣ
=:Rn.
B.2.2. Proof of Lemma 11
Letm∈N⋆,f[1]
m∈Fm, and x∈[0,1]p. By (21), there exists s[1]
m∈Sm, such that
f[1]
m=−ln
s[1]
m/s0
. Introduce the notation s[2]
m∈Sandf[2]
m=−ln
s[2]
m/s0
. Let
ψ[1](x) =
γ[1]
k0, γ[1]
kx, β[1]
k0, β[1]
kx,Σ[1]
k
k∈[K],andψ[2](x) =
γ[2]
k0, γ[2]
kx, β[2]
k0, β[2]
kx,Σ[2]
k
k∈[K],
be the parameters of the PDFs s[1]
m(·|x)ands[2]
m(·|x), respectively. By applying Taylor’s
inequality and then Lemma 14 on the event T, for all i∈[n], it holds that
f[1]
m(yi|xi)−f[2]
m(yi|xi)=ln
s[1]
m(yi|xi)
−ln
s[2]
m(yi|xi)
≤sup
x∈Xsup
ψ∈eΨ∂ln (sψ(yi|x))
∂ψψ[1](xi)−ψ[1](xi)
1(using Taylor’s inequality in Lemma 24)
≤max ( AΣ, C(p, K))
1 +q√q(Mn+Aβ)2AΣ
| {z }
Bnψ[1](xi)−ψ[2](xi)
1(using Lemma 14)
≤BnKX
k=1 γ[1]
k0−γ[2]
k0+γ[1]⊤
kxi−γ[2]⊤
kxi
+β[1]
k0−β[2]
k0
1+β[1]
kxi−β[2]
kxi
1+vec
Σ[1]
k−Σ[2]
k
1!
.

--- PAGE 36 ---
Nguyen Nguyen Chamroukhi McLachlan
By the Cauchy-Schwarz inequality, (Pm
i=1ai)2≤mPm
i=1a2
i(m∈N⋆), we get
f[1]
m(yi|xi)−f[2]
m(yi|xi)2
≤3B2
n
 KX
k=1γ[1]⊤
kxi−γ[2]⊤
kxi!2
+ KX
k=1qX
z=1h
β[1]
kxii
z−h
β[2]
kxii
z!2

+ 3B2
nβ[1]
0−β[2]
0
1+γ[1]
0−γ[2]
0
1+vec
Σ[1]−Σ[2]
12
≤3B2
n
KKX
k=1
pX
j=1γ[1]⊤
kjxij−pX
j=1γ[2]⊤
kjxij
2
KqKX
k=1qX
z=1
pX
j=1h
β[1]
ki
z,jxij−pX
j=1h
β[2]
ki
z,jxij
2

+ 3B2
nβ[1]
0−β[2]
0
1+γ[1]
0−γ[2]
0
1+vec
Σ[1]−Σ[2]
12
,
and
f[1]
m−f[2]
m2
n=1
nnX
i=1f[1]
m(yi|xi)−f[2]
m(yi|xi)2
≤3B2
nKKX
k=11
nnX
i=1
pX
j=1γ[1]
kjxij−pX
j=1γ[2]
kjxij
2
| {z }
=:a
+ 3B2
nKqKX
k=1qX
z=11
nnX
i=1
pX
j=1h
β[1]
ki
z,jxij−pX
j=1h
β[2]
ki
z,jxij
2
| {z }
=:b
+ 3B2
nβ[1]
0−β[2]
0
1+γ[1]
0−γ[2]
0
1+vec
Σ[1]−Σ[2]
12
.
So, for all δ >0, if
a≤δ2/ 
36B2
n
, b≤δ2/ 
36B2
n
,β[1]
0−β[2]
0
1≤δ/(18Bn),
γ[1]
0−γ[2]
0
1≤δ/(18Bn),andvec
Σ[1]−Σ[2]
1≤δ/(18Bn),
thenf[1]
m−f[2]
m2
n≤δ2/4. To bound aandb, we can write
a=Km2KX
k=11
nnX
i=1
pX
j=1γ[1]
kj
mxij−pX
j=1γ[2]
kj
mxij
2
,and
b=Kqm2KX
k=1qX
z=11
nnX
i=1
pX
j=1h
β[1]
ki
z,j
mxij−pX
j=1h
β[2]
ki
z,j
mxij
2
.

--- PAGE 37 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Then, we apply Lemma 12 to obtainγ[1]
k,.
m=
γ[1]
kj
m
j∈[q]andh
β[1]
ki
z,.
m= h
β[1]
ki
z,j
m!
j∈[q],
for all k∈[K], z∈[q]. Since s[1]
m∈Sm, and using (12), we haveγ[1]
k≤mand
vec
β[1]
k
1≤m, which leads toPp
j=1γ[1]
kj
m≤1 andPq
z=1Pp
j=1h
β[1]
ki
z,j
m≤1, respec-
tively. Furthermore, given x∈ X= [0,1]p, we have ∥x∥2
max,n= 1. Thus, there exist families
Aof(2p+ 1)36B2
nK2m2/δ2vectors and Bof(2p+ 1)16B2
nq2K2m2/δ2vectors of Rp, such that
for all k∈[K],z∈[q],γ[1]
k,., andh
β[1]
ki
z,., there exist γ[1]
k,.∈ Aandh
β[2]
ki
z,.∈ B, such that
1
nnX
i=1
pX
j=1γ[1]
kj
mxij−pX
j=1γ[2]
kj
mxij
2
≤δ2
36B2nK2m2,and
1
nnX
i=1
pX
j=1h
β[1]
ki
z,j
mxij−pX
j=1h
β[2]
ki
z,j
mxij
2
≤δ2
36B2nq2K2m2,
which leads to a≤δ2/36B2
nandb≤δ2/36B2
n. Moreover, (2) leads to
β[1]
0
1=KX
k=1β[1]
0k
1≤Kqβ[1]
0k
∞≤KqA β(using (92)) ,
γ[1]
0
1=KX
k=1γ[1]
0k≤KAγ,vec
Σ[1]
1≤Kq√q
aΣ.
Therefore, on the event T,
M(δ, Fm,∥·∥n)≤N(δ/2, Fm,∥·∥n) (using Lemma 22)
≤card(A) card( B)Nδ
18Bn, BK
1(KqA β),∥·∥1
×Nδ
18Bn, BK
1(KAγ),∥·∥1
Nδ
18Bn, BK
1Kq√q
aΣ
,∥·∥1
≤(2p+ 1)72B2nq2K2m2
δ2
1 +18BnKqA β
δK
1 +18BnKAγ
δK
1 +18BnKq√q
aΣδK
.
B.2.3. Proof of Lemma 13
Letm∈N⋆. From Lemma 10, on the event T, it holds that
sup
fm∈Fm∥fm∥n≤2KBn
Aγ+qAβ+q√q
aΣ
=:Rn. (82)

--- PAGE 38 ---
Nguyen Nguyen Chamroukhi McLachlan
From Lemma 11, on the event Tfor all S∈N⋆, with δ= 2−sRn,
SX
s=12−sq
ln [1 + M(2−sRn, Fm,∥·∥n)]≤SX
s=12−sq
ln [2M(δ, Fm,∥·∥n)]
≤SX
s=12−s"
√
ln 2 +6√
2BnqKm
δp
ln (2p+ 1)
+s
Kln
1 +18BnKqA β
δ
1 +18BnKAγ
δ
1 +18BnKq√q
aΣδ#
≤SX
s=12−s"
√
ln 2 +2s6√
2BnqKm
Rnp
ln (2p+ 1)
+s
Kln
1 +2s18BnKqA β
Rn
1 +2s18BnKAγ
Rn
1 +2s18BnKq√q
aΣRn#
.(83)
Notice from (82), that Rn≥2KBnmax
Aγ, qAβ,q√q
aΣ
. Moreover, it holds that 1 ≤2s+3,
andPS
s=12−s= 1−2−S≤1,PS
s=1(√e/2)s≤√e/(2−√e), and since for all s∈N⋆, es≥s,
and thus 2−s√s≤(√e/2)s. Therefore, from (83):
SX
s=12−sq
ln [1 + M(2−sRn, Fm,∥·∥n)]
≤SX
s=12−s"
√
ln 2 +2s6√
2BnqKm
Rnp
ln (2p+ 1) +p
Kln [(2s+132) (2s+132) (2s+132)]#
=SX
s=12−s"
√
ln 2 +2s6√
2BnqKm
Rnp
ln(2p+ 1) +√
Kp
3 ((s+ 1) ln 2 + 2 ln 3)#
≤6√
2BnKqm
Rnp
ln(2p+ 1)S+√
K√
3 ln 2SX
s=12−s√s+√
ln 2
1 +√
3K
+√
6 ln 3K
≤6√
2BnKqm
Rnp
ln(2p+ 1)S+√
K√
3 ln 2SX
s=1√e
2s
+√
ln 2
1 +√
3K
+√
6 ln 3K
≤6√
2BnqKm
Rnp
ln(2p+ 1)S+√
Kln 2 √
3e
2−√e+ 1 +√
3 +r
6 ln 3
ln 2!
| {z }
=:C1. (84)

--- PAGE 39 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Then, for all S∈N⋆, on the event T:
E"
sup
fm∈Fm1
nnX
i=1ϵifm(Zi)#
=E"
sup
fm∈Fm1
nnX
i=1ϵifm(Zi)I|fm(Zi)|≤Rn#
(using Lemma 10)
=E"
sup
f∈F1
nnX
i=1ϵif(Zi)#
≤Rn 
6√nSX
s=12−sq
ln [1 + M(2−sRn,F,∥.∥n)] + 2−S!
(using Lemma 9)
≤Rn"
6√n 
6√
2BnKmq
Rnp
ln(2p+ 1)S+√
Kln 2C1!
+ 2−S#
(using Lemma 84) .(85)
We choose S=lnn/ln2 so that the two terms depending on Sin(85)are of the same order.
In particular, for this value of S, 2−S≤1/n, and we deduce from (85)and(82)that on the
eventT,
E"
sup
fm∈Fm1
nnX
i=1ϵifm(Zi)#
≤36√
2BnKmq√np
ln(2p+ 1)lnn
ln 2+ 2KBn
Aγ+qAβ+q√q
aΣ 
6√
ln 2C1√
K√n+1
n!
≤BnKmq√np
ln(2p+ 1) ln n36√
2
ln 2|{z}
≈73.45+K√
K√nBn
Aγ+qAβ+q√q
aΣ
2
6√
ln 2C1+ 1
| {z }
≈141.32
<74KBn√n
mqp
ln(2p+ 1) ln n+ 2√
K
Aγ+qAβ+q√q
aΣ
.
B.3. Proof of Lemma 6
Since ln(z) is concave in z, Jensen’s inequality (see e.g. Jensen (1906); Cover (1999)) implies
that ln (EZ[Z])≥EZ[ln (Z)], where Zis a random variable. Thus, for all x∈ X, Jensen’s
inequality and Lemma 15 lead us the following upper bound
Z
Rqln (s0(y|x))s0(y|x)dy≤KX
k=1gk(x;γ0) lnZ
Rqs0(y|x)N(y;v0k(x),Σ0k)dy
≤KX
k=1gk(x;γ0) ln"KX
l=1gl(x;γ0)Cs0#
= lnCs0<∞,
where Cs0= (4π)−q/2Aq/2
Σ,(using Lemma 15). (86)
Therefore, we obtain
max
0,sup
x∈XZ
Rqln (s0(y|x))s0(y|x)dy
≤max{0,lnCs0}=:Hs0<∞.
Next, we state the following important Lemma 15, which is used in the proof of Lemma 6.

--- PAGE 40 ---
Nguyen Nguyen Chamroukhi McLachlan
Lemma 15 There exists a positive constant Cs0:=(4π)−q/2Aq/2
Σ,0< C s0<∞, such that
for all k∈[K], l∈[L],
Z
RqN(y;v0l(x),Σ0l)N(y;v0k(x),Σ0k)dy < C s0,∀x∈ X. (87)
Proof of Lemma 15. Firstly, for all k∈[K], l∈[L], given
clk(x) =Clk
Σ−1
0lv0l(x) + Σ−1
0kv0k(x)
, C lk= 
Σ−1
0l+ Σ−1
0k−1,
Lemma 25 leads to
Z
Rq[N(y;v0l(x),Σ0l)N(y;v0k(x),Σ0k)]dy=Z−1
lkZ
RqN(y;clk(x), Clk)dy
| {z }
=1,where
= (2π)−q/2det (Σ 0l+ Σ 0k)−1/2exp
−1
2(v0l(x)−v0k(x))⊤(Σ0l+ Σ 0k)−1(v0l(x)−v0k(x))
(88)
Next, since the determinant is the product of the eigenvalues, counted with multiplicity, and
Weyl’s inequality, see e.g., Lemma 26, for all k∈[K], l∈[L], we have
det (Σ 0l+ Σ 0k)≥[m(Σ0l+ Σ 0k)]q
≥[m(Σ0l) +m(Σ0k)]q(using (108) from Lemma 26)
=h
M 
Σ−1
0l−1+M 
Σ−1
0k−1iq
≥ 
2A−1
Σq(using boundedness assumptions in (2)) .
Therefore, for all k∈[K], l∈[L], it holds that
det (Σ 0l+ Σ 0k)−1/2≤2−q/2(AΣ)q/2(using boundedness assumptions in (2)) . (89)
Since (Σ 0l+ Σ 0k)−1is a positive definite matrix, it holds that
(v0l(x)−v0k(x))⊤(Σ0l+ Σ 0k)−1(v0l(x)−v0k(x))≥0,∀x∈ X, l∈[L], k∈[K].
Then, since the exponential function is increasing, ∀x∈ X, l∈[L], k∈[K],we have
exp
−1
2(v0l(x)−v0k(x))⊤(Σ0l+ Σ 0k)−1(v0l(x)−v0k(x))
≤exp(0) = 1 . (90)
Finally, from (88), (89) and (90), we obtain
Z
Rq[N(y;v0l(x),Σ0l)N(y;v0k(x),Σ0k)]dy≤(2π)−q/22−q/2Aq/2
Σ= (4π)−q/2Aq/2
Σ=:Cs0<∞.

--- PAGE 41 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Appendix C. Further Technical results
We denote the vector space of all q-by-qreal matrices by Rq×q(q∈N⋆):
A∈Rq×q⇐⇒A= (ai,j) =
a1,1··· a1,q
......
aq,1··· aq,q
, ai,j∈R.
If a capital letter is used to denote a matrix ( e.g., A, B), then the corresponding lower-case
letter with subscript i, jrefers to the (i, j)th entry ( e.g., ai,j, bi,j). When required, we also
designate the elements of a matrix with the notation [A]i,jorA(i, j). Denote the q-by-q
identity and zero matrices by I qand0q, respectively.
Lemma 16 (Derivative of quadratic form, cf., Magnus and Neudecker (2019))
Assume that Xandaare non-singular matrix in Rq×qand vector in Rq×1, respectively.
Then
∂a⊤X−1a
∂X=−X−1aa⊤X−1.
Lemma 17 (Jacobi’s formula, cf., Theorem 8.1 from Magnus and Neudecker (2019))
IfXis a differentiable map from the real numbers to q-by-qmatrices,
d
dtdet (X(t)) =tr
Adj (X(t))dX(t)
dt
,∂det (X)
∂X= (Adj ( X))⊤= det ( X) 
X−1⊤.
Lemma 18 (Operator induced p-norm) We recall an operator (induced) p-norms of a
matrix A∈Rq×q(q∈N⋆, p∈ {1,2,∞}),
∥A∥p= max
x̸=0∥Ax∥p
∥x∥p= max
x̸=0A 
x
∥x∥p!
p= max
∥x∥p=1∥Ax∥p, (91)
where for all x∈Rq,
∥x∥∞≤ ∥x∥1=qX
i=1|xi| ≤q∥x∥∞, (92)
∥x∥2= qX
i=1|xi|2!1
2
=
x⊤x1
2≤ ∥x∥1≤√q∥x∥2,and (93)
∥x∥∞= max
1≤i≤q|xi| ≤ ∥x∥2≤√q∥x∥∞. (94)
Lemma 19 (Some matrix p-norm properties, Golub and Van Loan (2012)) By def-
inition, we always have the important property that for every A∈Rq×qandx∈Rq,
∥Ax∥p≤ ∥A∥p∥x∥p, (95)

--- PAGE 42 ---
Nguyen Nguyen Chamroukhi McLachlan
and every induced p-norm is submultiplicative, i.e.,for every A∈Rq×qandB∈Rq×q,
∥AB∥p≤ ∥A∥p∥B∥p. (96)
In particular, it holds that
∥A∥1= max
1≤j≤qqX
i=1|aij| ≤qX
j=1qX
i=1|aij|:=∥vec(A)∥1≤q∥A∥1, (97)
∥vec(A)∥∞:= max
1≤i≤q,1≤j≤q|aij| ≤ ∥A∥∞= max
1≤j≤qqX
i=1|aij| ≤q∥vec(A)∥∞, (98)
∥vec(A)∥∞≤ ∥A∥2=λmax(A)≤q∥vec(A)∥∞, (99)
where λmaxis the largest eigenvalue of a positive definite symmetric matrix A. The p-norms,
when p∈ {1,2,∞}, satisfy
1√q∥A∥∞≤ ∥A∥2≤√q∥A∥∞, (100)
1√q∥A∥1≤ ∥A∥2≤√q∥A∥1. (101)
Given δ >0, we need to define the δ-packing number and δ-covering number.
Definition 20 ( δ-packing number, cf., Definition 5.4 from Wainwright (2019))
Let(F,∥·∥)be a normed space and let G ⊂ F . With (gi)i=1,...,m∈ G,{g1, . . . , g m}is an
δ-packing of Gof size m∈N⋆, if∥gi−gj∥> δ,∀i̸=j, i, j∈ {1, . . . , m }, or equivalently,Tn
i=1B(gi, δ/2)=∅. Upon defining δ-packing, we can measure the maximal number of
disjoint closed balls with radius δ/2that can be “packed” into G. This number is called the
δ-packing number and is defined as
M(δ,G,∥·∥) := max {m∈N⋆:∃δ-packing of Gof size m}. (102)
Definition 21 ( δ-covering number, cf., Definition 5.1 from Wainwright (2019))
Let(F,∥·∥)be a normed space and let G ⊂ F . With (gi)i∈[n]∈ G,{g1, . . . , g n}is an δ-
covering of Gof size nifG ⊂ ∪n
i=1B(gi, δ), or equivalently, ∀g∈ G,∃isuch that ∥g−gi∥ ≤δ.
Upon defining the δ-covering, we can measure the minimal number of closed balls with radius
δ, which is necessary to cover G. This number is called the δ-covering number and is defined
as
N(δ,G,∥·∥) := min {n∈N⋆:∃δ-covering of Gof size n}. (103)
The covering entropy (metric entropy) is defined as follows H∥.∥(δ,G) = ln ( N(δ,G,∥·∥)).
The relation between the packing number and the covering number is described in the
following lemma.
Lemma 22 (Lemma 5.5 from Wainwright (2019)) Let(F,∥·∥)be a normed space and
letG ⊂ F . Then
M(2δ,G,∥·∥)≤N(δ,G,∥·∥)≤M(δ,G,∥·∥).

--- PAGE 43 ---
Non-asymptotic oracle inequalities for the Lasso in high-dimensional MoE
Lemma 23 (Chernoff’s inequality, e.g., Chapter 2 in Wainwright (2019)) Assume
that the random variable has a moment generating function in a neighborhood of zero, mean-
ing that there is some constant b >0such that the function φ(λ) =E
eλ(U−µ)
exists
for all λ≤ |b|. In such a case, we may apply Markov’s inequality to the random variable
Y=eλ(U−µ), thereby obtaining the upper bound
P(U−µ≥a) =P
eλ(U−µ)≥eλt
≤E
eλ(U−µ)
eλt.
Optimizing our choice of λso as to obtain the tightest result yields the Chernoff bound
ln (P(U−µ≥a))≤sup
λ∈[0,b]n
λt−ln
Eh
eλ(U−µ)io
. (104)
In particular, if U∼ N(µ, σ)is a Gaussian random variable with mean µand variance σ2.
By a straightforward calculation, we find that Uhas the moment generating function
Eh
eλUi
=eµλ+σ2λ2
2,valid for all λ∈R.
Substituting this expression into the optimization problem defining the optimized Chernoff
bound (104) , we obtain
sup
λ≥0n
λt−ln
Eh
eλ(U−µ)io
= sup
λ≥0
λt−σ2λ2
2
=−t2
2σ2,
where we have taken derivatives in order to find the optimum of this quadratic function. So,
(104) leads to
P(X≥µ+t)≤e−t2
2σ2,for all t≥0. (105)
Recall that a multi-index α=(α1, . . . , α p), αi∈N⋆,∀i∈ {1, . . . , p }is an p-tuple of non-
negative integers. Let
|α|=pX
i=1αi, α! =pY
i=1αi!, xα=pY
i=1xαi
i, x∈Rp, ∂αf=∂α1
1∂α2
2···∂αpp=∂|α|f
∂xα1
1∂xα2
2···∂xαpp.
The number |α|is called the order ordegree ofα. Thus, the order of αis the same as the
order of xαas a monomial or the order of ∂αas a partial derivative.
Lemma 24 (Multivariate Taylor’s Theorem from Duistermaat and Kolk (2004))
Suppose f:Rp7→Ris in the class Ck+1, of continuously differentiable functions, on an
open convex set S. Ifa∈Sanda+h∈S, then
f(a+h) =X
|α|≤k∂αf(a)
α!hα+Ra,k(h),
where the remainder is given in Lagrange’s form by
Ra,k(h) =X
|α|=k+1∂αf(a+ch)hα
α!for some c∈(0,1),

--- PAGE 44 ---
Nguyen Nguyen Chamroukhi McLachlan
or in integral form by
Ra,k(h) = (k+ 1)X
|α|=k+1hα
α!Z1
0(1−t)k∂αf(a+th)dt.
In particular, we can estimate the remainder term if |∂αf(x)| ≤Mforx∈Sand|α|=k+1,
|Ra,k(h)| ≤M
(k+ 1)!∥h∥k+1
1,∥h∥1=pX
i=1|hi|.
Recall that the multivariate Gaussian (or Normal) distribution has a joint density given by
N(y;µ; Σ) = (2 π)−q/2det (Σ)−1/2exp
−1
2(y−µ)⊤Σ−1(y−µ)
, (106)
where µis the mean vector (of length q) and Σ is the symmetric, positive definite covariance
matrix (of size q×q). Then, we have the following well-known Gaussian identity, see more
in Lemma 25, which is proved in Equation (A.7) from Williams and Rasmussen (2006).
Lemma 25 (Product of two Gaussians) The product of two Gaussians gives another
(un-normalized) Gaussian
N(y;a, A)N(y;b, B) =Z−1N(y;c, C),where, (107)
c=C 
A−1a+B−1b
andC= 
A−1+B−1−1,
Z−1= (2π)−q/2det (A+B)−1/2exp
−1
2(a−b)⊤(A+B)−1(a−b)
.
We recall the following inequality of Hermann Weyl, see e.g., Horn and Johnson (2012,
Theorem 4.3.1)
Lemma 26 (Weyl’s inequality) LetA, B∈Rq×qbe Hermitian and let the respective
eigenvalues of A, B, and A+Bbe{λi(A)}i∈[q],{λi(B)}i∈[q], and {λi(A+B)}i∈[q], each
algebraically nondecreasing order as:
m(A) =λ1(A)≤λ2(A)≤. . .≤λq(A) =M(A).
Then, for each i∈[q],
λi(A+B)≤λi+j(A) +λq−j(B), j∈ {0} ∪[q−i], λi−j+1(A) +λj(B)≤λi(A+B), j∈[i].
In particular, we have
M(A+B)≤M(A) +M(B), m(A+B)≥m(A) +m(B). (108)
