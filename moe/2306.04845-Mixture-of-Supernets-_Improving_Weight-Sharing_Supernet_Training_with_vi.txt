# 2306.04845.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2306.04845.pdf
# Kích thước tệp: 1572977 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mixture-of-Supernets: Cải thiện việc Huấn luyện Supernet Chia sẻ Trọng số với
Mixture-of-Experts Định tuyến theo Kiến trúc
Ganesh Jawaharµ♡*Haichuan Yang∞Yunyang Xiong∞Zechun Liu∞
Dilin Wang∞Fei Sun∞Meng Li∞Aasish Pappu∞Barlas Oguz∞
Muhammad Abdul-Mageedµ♢Laks V .S. Lakshmananµ
Raghuraman Krishnamoorthi∞Vikas Chandra∞
µUniversity of British Columbia∞Meta♢MBZUAI♡Google DeepMind
ganeshjwhr@gmail.com, {haichuan, yunyang, zechunliu, wdilin, feisun, aasish, barlaso}@meta.com
meng.li@pku.edu.cn, muhammad.mageed@ubc.ca, laks@cs.ubc.ca, {raghuraman, vchandra}@meta.com

Tóm tắt
Các supernet chia sẻ trọng số rất quan trọng trong việc ước tính hiệu suất của các framework tìm kiếm kiến trúc mạng nơ-ron (NAS) tiên tiến. Mặc dù có khả năng tạo ra nhiều subnetwork đa dạng mà không cần huấn luyện lại, chất lượng của các subnetwork này không được đảm bảo do việc chia sẻ trọng số. Trong các tác vụ NLP như dịch máy và mô hình hóa ngôn ngữ được tiền huấn luyện, có khoảng cách hiệu suất đáng kể giữa supernet và huấn luyện từ đầu cho cùng một kiến trúc mô hình, đòi hỏi phải huấn luyện lại sau khi xác định được kiến trúc tối ưu.

Nghiên cứu này giới thiệu một giải pháp gọi là mixture-of-supernets, một công thức hóa supernet tổng quát tận dụng mixture-of-experts (MoE) để tăng cường khả năng biểu đạt của mô hình supernet với chi phí huấn luyện tối thiểu. Khác với các supernet truyền thống, phương pháp này sử dụng cơ chế định tuyến dựa trên kiến trúc, cho phép chia sẻ trọng số gián tiếp giữa các subnetwork. Việc tùy chỉnh trọng số này cho các kiến trúc cụ thể, được học thông qua gradient descent, giảm thiểu thời gian huấn luyện lại, cải thiện đáng kể hiệu quả huấn luyện trong NLP. Phương pháp đề xuất đạt hiệu suất tốt nhất (SoTA) trong NAS cho các mô hình dịch máy nhanh, thể hiện sự cân bằng tốt hơn giữa độ trễ-BLEU so với HAT, framework NAS SoTA cho dịch máy. Hơn nữa, nó vượt trội trong NAS để xây dựng các mô hình BERT hiệu quả bộ nhớ và bất khả tri tác vụ, vượt qua NAS-BERT và AutoDistil trên nhiều kích thước mô hình khác nhau. Mã nguồn có thể tìm thấy tại: https://github.com/UBC-NLP/MoS .

1 Giới thiệu
Tìm kiếm kiến trúc mạng nơ-ron (NAS) tự động hóa việc thiết kế các kiến trúc chất lượng cao cho các tác vụ xử lý ngôn ngữ tự nhiên (NLP) đồng thời đáp ứng các ràng buộc hiệu quả được chỉ định (Wang et al., 2020a; Xu et al., 2021, 2022a). NAS thường được xử lý như tối ưu hóa hộp đen (Zoph et al., 2018; Pham et al., 2018), nhưng việc có được độ chính xác tốt nhất đòi hỏi huấn luyện và đánh giá lặp đi lặp lại, điều này không thực tế với các tập dữ liệu lớn. Để giải quyết vấn đề này, việc chia sẻ trọng số được áp dụng thông qua một supernet, trong đó các subnetwork đại diện cho các kiến trúc mô hình khác nhau (Pham et al., 2018).

Các nghiên cứu gần đây chứng minh việc sử dụng trực tiếp các subnetwork thành công cho phân loại hình ảnh với hiệu suất tương đương việc huấn luyện từ đầu (Cai et al., 2020; Yu et al., 2020). Tuy nhiên, việc áp dụng cách tiếp cận supernet này cho các tác vụ NLP thách thức hơn, tiết lộ khoảng cách hiệu suất đáng kể khi sử dụng trực tiếp các subnetwork. Điều này phù hợp với các công trình NAS gần đây trong NLP (Wang et al., 2020a; Xu et al., 2021), giải quyết khoảng cách bằng cách huấn luyện lại hoặc tinh chỉnh các ứng viên kiến trúc được xác định. Tình huống này tạo ra những bất định về tính tối ưu của các kiến trúc được chọn và đòi hỏi huấn luyện lặp đi lặp lại để có được độ chính xác cuối cùng trên mặt phẳng Pareto, tức là các mô hình tốt nhất cho các ngân sách hiệu quả khác nhau (ví dụ: kích thước mô hình hoặc độ trễ suy luận). Công trình này nhằm tăng cường cơ chế chia sẻ trọng số giữa các subnetwork để giảm thiểu khoảng cách hiệu suất quan sát được trong các tác vụ NLP.

Supernet chia sẻ trọng số được huấn luyện bằng cách lấy mẫu lặp đi lặp lại các kiến trúc từ không gian tìm kiếm và huấn luyện các trọng số cụ thể của chúng từ supernet. Việc chia sẻ trọng số tiêu chuẩn (Yu et al., 2020; Cai et al., 2020) bao gồm việc trực tiếp trích xuất một số nơ-ron đầu ra đầu tiên để tạo ra một subnetwork nhỏ hơn (xem Hình 1 (a)), đặt ra hai thách thức do khả năng mô hình hạn chế. Thứ nhất, supernet áp đặt việc chia sẻ trọng số nghiêm ngặt giữa các kiến trúc, gây ra co-adaptation (Bender et al., 2018; Zhao et al., 2021) và xung đột gradient (Gong et al., 2021). Ví dụ, trong việc chia sẻ trọng số tiêu chuẩn, nếu một mô hình 5M tham số là một subnetwork của mô hình 90M tham số, thì 5M trọng số được chia sẻ trực tiếp. Tuy nhiên, các trọng số chia sẻ tối ưu cho mô hình 5M có thể không tối ưu cho mô hình 90M, dẫn đến xung đột gradient đáng kể trong quá trình tối ưu hóa (Gong et al., 2021). Thứ hai, khả năng tổng thể của supernet bị hạn chế bởi các tham số của một mạng nơ-ron sâu đơn lẻ (DNN), tức là subnetwork lớn nhất trong không gian tìm kiếm. Tuy nhiên, khi xử lý một số lượng subnetwork có khả năng rất lớn (ví dụ: hàng tỷ), việc dựa vào một tập trọng số đơn lẻ để tham số hóa tất cả chúng có thể không đủ (Zhao et al., 2021).

Để giải quyết những thách thức này, chúng tôi đề xuất một framework Mixture-of-Supernets (MoS). MoS cho phép trích xuất trọng số cụ thể theo kiến trúc, cho phép các kiến trúc nhỏ hơn tránh chia sẻ một số nơ-ron đầu ra với các kiến trúc lớn hơn. Ngoài ra, nó phân bổ khả năng lớn mà không bị hạn chế bởi số lượng tham số trong một DNN đơn lẻ. MoS bao gồm hai biến thể: layer-wise MoS, trong đó các ma trận trọng số cụ thể theo kiến trúc được xây dựng dựa trên các kết hợp có trọng số của các ma trận trọng số chuyên gia ở cấp độ tập hợp nơ-ron, và neuron-wise MoS, hoạt động ở cấp độ các nơ-ron riêng lẻ trong mỗi ma trận trọng số chuyên gia. Phương pháp NAS đề xuất của chúng tôi chứng minh hiệu quả trong việc xây dựng các mô hình BERT bất khả tri tác vụ hiệu quả (Devlin et al., 2019) và các mô hình dịch máy (MT). Đối với BERT hiệu quả, supernet tốt nhất của chúng tôi vượt trội hơn SuperShaper (Ganesan et al., 2021) 0.85 điểm GLUE, vượt qua NAS-BERT (Xu et al., 2021) và AutoDistil (Xu et al., 2022a) trong nhiều kích thước mô hình khác nhau (≤50M tham số). So với HAT (Wang et al., 2020a), supernet hàng đầu của chúng tôi giảm khoảng cách giữa supernet và mô hình độc lập 26.5%, cung cấp mặt phẳng Pareto vượt trội cho sự cân bằng độ trễ-BLEU (100 đến 200ms), và giảm các bước cần thiết để đóng khoảng cách 39.8%. Bảng 1 tóm tắt việc tiết kiệm thời gian và cải thiện BLEU của các supernet MoS cho tác vụ WMT'14 En-De.

Chúng tôi tóm tắt các đóng góp chính:
1. Chúng tôi đề xuất một công thức hóa tổng quát các phương pháp chia sẻ trọng số, bao gồm việc chia sẻ trọng số trực tiếp (ví dụ: mạng once-for-all (Cai et al., 2020), BigNAS (Yu et al., 2020)) và chia sẻ trọng số linh hoạt (ví dụ: few-shot NAS (Zhao et al., 2021)). Công thức hóa này tăng cường sức mạnh biểu đạt của supernet.
2. Chúng tôi áp dụng khái niệm MoE để tăng cường khả năng mô hình. Trọng số của mô hình được tạo ra động dựa trên kiến trúc subnetwork được kích hoạt. Sau huấn luyện, MoE có thể được chuyển đổi thành các mô hình tĩnh tương đương vì các supernet của chúng tôi chỉ phụ thuộc vào kiến trúc subnetwork cố định sau huấn luyện.
3. Các thí nghiệm của chúng tôi cho thấy các supernet của chúng tôi đạt kết quả NAS SoTA trong việc xây dựng các mô hình BERT bất khả tri tác vụ hiệu quả và các mô hình MT.

--- TRANG 2 ---
(a) Tiêu chuẩn
(b) Layer-wise Mixture-of-Supernet
(c) Neuron-wise Mixture-of-Supernet
Hình 1: Lựa chọn các lớp tuyến tính cho huấn luyện supernet. Chiều dài và chiều cao của các khối 'Linear' tương ứng với số lượng tính năng đầu vào và đầu ra của supernet. Các phần được làm nổi bật bằng màu xanh tương ứng với các trọng số cụ thể theo kiến trúc được trích xuất từ supernet. Các cường độ màu xanh khác nhau trong các khối 'Linear' của mixture-of-supernet tương ứng với các điểm số alignment khác nhau được tạo ra bởi router.

Bảng 1: Tiết kiệm thời gian tổng thể và cải thiện BLEU trung bình của các supernet MoS so với HAT để tính toán mặt phẳng Pareto (ràng buộc độ trễ: 100ms, 150ms, 200ms) cho tác vụ WMT'14 En-De. Thời gian tổng thể (giờ NVIDIA V100 đơn lẻ) bao gồm thời gian huấn luyện supernet, thời gian tìm kiếm và thời gian huấn luyện bổ sung cho các kiến trúc tối ưu. BLEU trung bình là trung bình của điểm số BLEU của các kiến trúc trong mặt phẳng Pareto (xem Bảng 5 cho điểm số cá nhân). Các supernet MoS tạo ra các kiến trúc có sự cân bằng độ trễ-BLEU tốt hơn HAT và tiết kiệm tổng thể ít nhất 20% giờ GPU so với HAT.

mô hình có thể không tối ưu cho mô hình 90M, dẫn đến xung đột gradient đáng kể trong quá trình tối ưu hóa (Gong et al., 2021). Thứ hai, khả năng tổng thể của supernet bị hạn chế bởi các tham số của một mạng nơ-ron sâu đơn lẻ (DNN), tức là subnetwork lớn nhất trong không gian tìm kiếm. Tuy nhiên, khi xử lý một số lượng subnetwork có khả năng rất lớn (ví dụ: hàng tỷ), việc dựa vào một tập trọng số đơn lẻ để tham số hóa tất cả chúng có thể không đủ (Zhao et al., 2021).

Để giải quyết những thách thức này, chúng tôi đề xuất một framework Mixture-of-Supernets (MoS). MoS cho phép trích xuất trọng số cụ thể theo kiến trúc, cho phép các kiến trúc nhỏ hơn tránh chia sẻ một số nơ-ron đầu ra với các kiến trúc lớn hơn. Ngoài ra, nó phân bổ khả năng lớn mà không bị hạn chế bởi số lượng tham số trong một DNN đơn lẻ. MoS bao gồm hai biến thể: layer-wise MoS, trong đó các ma trận trọng số cụ thể theo kiến trúc được xây dựng dựa trên các kết hợp có trọng số của các ma trận trọng số chuyên gia ở cấp độ tập hợp nơ-ron, và neuron-wise MoS, hoạt động ở cấp độ các nơ-ron riêng lẻ trong mỗi ma trận trọng số chuyên gia. Phương pháp NAS đề xuất của chúng tôi chứng minh hiệu quả trong việc xây dựng các mô hình BERT bất khả tri tác vụ hiệu quả (Devlin et al., 2019) và các mô hình dịch máy (MT). Đối với BERT hiệu quả, supernet tốt nhất của chúng tôi vượt trội hơn SuperShaper (Ganesan et al., 2021) 0.85 điểm GLUE, vượt qua NAS-BERT (Xu et al., 2021) và AutoDistil (Xu et al., 2022a) trong nhiều kích thước mô hình khác nhau (≤50M tham số). So với HAT (Wang et al., 2020a), supernet hàng đầu của chúng tôi giảm khoảng cách giữa supernet và mô hình độc lập 26.5%, cung cấp mặt phẳng Pareto vượt trội cho sự cân bằng độ trễ-BLEU (100 đến 200ms), và giảm các bước cần thiết để đóng khoảng cách 39.8%. Bảng 1 tóm tắt việc tiết kiệm thời gian và cải thiện BLEU của các supernet MoS cho tác vụ WMT'14 En-De.

Chúng tôi tóm tắt các đóng góp chính:
1. Chúng tôi đề xuất một công thức hóa tổng quát các phương pháp chia sẻ trọng số, bao gồm việc chia sẻ trọng số trực tiếp (ví dụ: mạng once-for-all (Cai et al., 2020), BigNAS (Yu et al., 2020)) và chia sẻ trọng số linh hoạt (ví dụ: few-shot NAS (Zhao et al., 2021)). Công thức hóa này tăng cường sức mạnh biểu đạt của supernet.
2. Chúng tôi áp dụng khái niệm MoE để tăng cường khả năng mô hình. Trọng số của mô hình được tạo ra động dựa trên kiến trúc subnetwork được kích hoạt. Sau huấn luyện, MoE có thể được chuyển đổi thành các mô hình tĩnh tương đương vì các supernet của chúng tôi chỉ phụ thuộc vào kiến trúc subnetwork cố định sau huấn luyện.

--- TRANG 3 ---
có thể được chuyển đổi thành các mô hình tĩnh tương đương vì các supernet của chúng tôi chỉ phụ thuộc vào kiến trúc subnetwork cố định sau huấn luyện.
3. Các thí nghiệm của chúng tôi cho thấy các supernet của chúng tôi đạt kết quả NAS SoTA trong việc xây dựng các mô hình BERT bất khả tri tác vụ hiệu quả và các mô hình MT.

2 Supernet - Cơ bản
Một supernet, sử dụng chia sẻ trọng số, tham số hóa trọng số cho hàng triệu kiến trúc, cung cấp dự đoán hiệu suất nhanh và giảm đáng kể chi phí tìm kiếm NAS. Mục tiêu huấn luyện có thể được hình thức hóa như sau. Gọi Xtr là phân phối dữ liệu huấn luyện. Gọi x,y là mẫu huấn luyện và nhãn tương ứng, tức là x, y∼ Xtr. Gọi arand là một kiến trúc được lấy mẫu đều từ không gian tìm kiếm A. Gọi farand là subnetwork với kiến trúc arand, và f được tham số hóa bởi trọng số mô hình supernet W. Khi đó, mục tiêu huấn luyện của supernet có thể được cho bởi:
min W Ex,y∼Xtr Earand∼A[L(farand(x;W), y)]. (1)

Công thức được đề cập được gọi là tối ưu hóa single path one-shot (SPOS) (Guo et al., 2020) cho huấn luyện supernet. Một kỹ thuật phổ biến khác là sandwich training (Yu et al., 2020), trong đó các kiến trúc lớn nhất (abig), nhỏ nhất (asmall), và được lấy mẫu đều (arand) từ không gian tìm kiếm được tối ưu hóa cùng nhau.

3 Mixture-of-Supernets
Các supernet hiện tại thường có khả năng mô hình hạn chế để trích xuất trọng số cụ thể theo kiến trúc. Để đơn giản, giả sử hàm mô hình fa(x;W) là một lớp được kết nối đầy đủ (đầu ra o=Wx, bỏ qua bias để ngắn gọn), trong đó x∈nin×1, W∈nout×nin, và o∈nout×1. nin và nout tương ứng với số lượng tính năng đầu vào và đầu ra. Khi đó, trọng số (Wa∈nouta×nin) cụ thể cho kiến trúc a với nouta tính năng đầu ra thường được trích xuất bằng cách lấy nouta hàng đầu tiên (như được hiển thị trong Hình 1 (a)) từ trọng số supernet W. Giả sử một người lấy mẫu hai kiến trúc (a và b) từ không gian tìm kiếm với số lượng tính năng đầu ra nouta và noutb tương ứng. Khi đó, trọng số tương ứng với kiến trúc có số lượng tính năng đầu ra nhỏ nhất sẽ là tập con của kiến trúc khác, chia sẻ chính xác |nouta−noutb| tính năng đầu ra đầu tiên. Kỹ thuật trích xuất trọng số này áp đặt việc chia sẻ trọng số nghiêm ngặt giữa các kiến trúc, bất kể thông tin kiến trúc toàn cục của chúng (ví dụ: các tính năng khác nhau trong các lớp khác). Ví dụ, các kiến trúc a và b có thể có khả năng rất khác nhau (ví dụ: 5M vs 90M tham số). Kiến trúc nhỏ hơn (ví dụ: 5M) phải chia sẻ tất cả trọng số với kiến trúc lớn hơn (ví dụ: 90M), và supernet (được mô hình hóa bởi fa(x;W)) không thể phân bổ trọng số cụ thể cho kiến trúc nhỏ hơn. Một vấn đề khác với fa(x;W) là khả năng tổng thể của supernet bị hạn chế bởi các tham số trong subnetwork lớn nhất (W) trong không gian tìm kiếm. Tuy nhiên, những trọng số supernet W này phải tham số hóa nhiều subnetwork đa dạng. Điều này thể hiện hạn chế cơ bản của cơ chế chia sẻ trọng số tiêu chuẩn. Phần 3.1 đề xuất một cải tiến để vượt qua hạn chế này, được thực hiện thông qua hai phương pháp (Layer-wise MoS, Phần 3.2, Neuron-wise MoS, Phần 3.3), phù hợp để tích hợp vào Transformers (xem Phần 3.4).

3.1 Hàm Mô hình Tổng quát
Chúng ta có thể cải tiến hàm fa(x;W) thành dạng tổng quát g(x, a;E), nhận 2 đầu vào: dữ liệu đầu vào x, và kiến trúc được kích hoạt a. E bao gồm các tham số có thể học được của g. Khi đó, mục tiêu huấn luyện của supernet đề xuất trở thành:
min E Ex,y∼Xtr Earand∼A[L(g(x, arand;E), y)]. (2)

Đối với cơ chế chia sẻ trọng số tiêu chuẩn được đề cập ở trên, E=W và hàm g chỉ sử dụng a để thực hiện thao tác "cắt tỉa" trên ma trận trọng số W, và chuyển tiếp subnetwork. Để tiếp tục giảm thiểu mục tiêu phương trình 2, việc tăng cường khả năng của hàm mô hình g là một cách tiếp cận tiềm năng. Tuy nhiên, các phương pháp thông thường như thêm lớp ẩn hoặc nơ-ron là không thực tế ở đây vì kiến trúc subnetwork cuối cùng của ánh xạ x tới fa(x;W) không thể bị thay đổi. Công trình này giới thiệu khái niệm Mixture-of-Experts (MoE) (Fedus et al., 2022) để tăng cường khả năng của g. Cụ thể, chúng tôi tạo ra động trọng số Wa cho một kiến trúc cụ thể a bằng cách định tuyến đến các ma trận trọng số nhất định từ một tập hợp trọng số chuyên gia. Chúng tôi gọi supernet dựa trên MoE định tuyến theo kiến trúc này là Mixture-of-Supernets (MoS) và thiết kế hai cơ chế định tuyến cho hàm g(x, a;E). Do thiếu không gian, thuật toán chi tiết cho huấn luyện và tìm kiếm supernet được hiển thị trong A.2.

3.2 Layer-wise MoS
Giả sử có m (số lượng chuyên gia) ma trận trọng số duy nhất ({Ei∈ Rnoutbig×ninbig}mi=1, hoặc trọng số chuyên gia), là các tham số có thể học được. Để đơn giản, chúng tôi chỉ sử dụng một lớp tuyến tính đơn lẻ làm ví dụ. Đối với một kiến trúc a với nouta tính năng đầu ra, chúng tôi đề xuất layer-wise MoS tính toán trọng số cụ thể cho kiến trúc a (tức là Wa∈ Rnouta×nin) bằng cách thực hiện kết hợp có trọng số của trọng số chuyên gia, Wa=∑i αia Eia. Ở đây, Eia∈ Rnouta×nin tương ứng với việc trích xuất hàng đầu tiêu chuẩn từ trọng số chuyên gia thứ i. Vector alignment (αa∈[0,1]m, ∑i αia = 1) nắm bắt điểm số alignment của kiến trúc a đối với mỗi chuyên gia (ma trận trọng số). Chúng tôi mã hóa kiến trúc a thành vector số Enc(a)∈ Rnenc×1 (ví dụ: danh sách số lượng tính năng đầu ra cho các lớp khác nhau), và áp dụng router r(·) có thể học được (một MLP với softmax) để tạo ra các điểm số như vậy, tức là αa=r(Enc(a)). Do đó, hàm mô hình tổng quát cho lớp tuyến tính (như được hiển thị trong Hình 1 (b)) có thể được định nghĩa như sau (bỏ qua bias để ngắn gọn):
g(x, a;E) =Wax=∑i r(Enc(a))i Eia x. (3)

Router r(·) điều chỉnh mức độ chia sẻ trọng số giữa hai kiến trúc thông qua điều chỉnh điểm số alignment (αa). Ví dụ, nếu m= 2 và a là một subnetwork của kiến trúc b, supernet có thể phân bổ trọng số cụ thể cho kiến trúc nhỏ hơn a bằng cách đặt αa= (1,0) và αb= (0,1). Trong kịch bản này, g(x, a;E) độc quyền sử dụng trọng số từ E1, và g(x, b;E) sử dụng trọng số từ E2, cho phép cập nhật E1 và E2 hướng tới loss từ kiến trúc a và b mà không xung đột. Đáng chú ý là few-shot NAS (Zhao et al., 2021) là một trường hợp đặc biệt của framework của chúng tôi khi router r dựa trên quy tắc. Hơn nữa, g(·) hoạt động như một MoE, tăng cường sức mạnh biểu đạt và giảm mục tiêu phương trình 2. Khi huấn luyện supernet hoàn thành, đối với một kiến trúc a cho trước, điểm số αa=r(Enc(a)) có thể được tạo ra ngoại tuyến. Trọng số chuyên gia thu gọn, giảm số lượng tham số cho kiến trúc a thành nouta×nina. Layer-wise MoS dẫn đến mức độ chia sẻ trọng số thấp hơn giữa các kiến trúc có kích thước khác nhau, được chứng minh bởi khoảng cách Jensen-Shannon cao hơn giữa các vector xác suất alignment của chúng so với các kiến trúc có kích thước tương tự. Tham khảo A.1.1 để biết thêm chi tiết.

3.3 Neuron-wise MoS
Layer-wise MoS sử dụng thiết lập MoE tiêu chuẩn, trong đó mỗi chuyên gia là một lớp/mô-đun tuyến tính. Router xác định sự kết hợp của các chuyên gia để sử dụng cho việc chuyển tiếp đầu vào x dựa trên a. Trong thiết lập này, mức độ tự do để tạo trọng số là m, và số lượng tham số tăng lên m× |W|, với |W| là các tham số trong supernet tiêu chuẩn. Do đó, một m đủ lớn cần thiết để linh hoạt trong việc tạo trọng số subnetwork, nhưng nó cũng đưa quá nhiều tham số vào supernet, làm cho layer-wise MoS thách thức để huấn luyện. Để giải quyết vấn đề này, chúng tôi chọn độ chi tiết nhỏ hơn của trọng số để đại diện cho mỗi chuyên gia, sử dụng các nơ-ron trong DNN làm chuyên gia. Về mặt ma trận trọng số, neuron-wise MoS đại diện cho một chuyên gia riêng lẻ bằng một hàng, trong khi layer-wise MoS sử dụng toàn bộ ma trận trọng số. Đối với neuron-wise MoS, đầu ra router βa=r(·)∈[0,1]noutbig×m cho mỗi lớp, và tổng của mỗi hàng trong βa là 1. Tương tự như layer-wise MoS, chúng tôi sử dụng MLP để tạo ra ma trận noutbig×m và áp dụng softmax trên mỗi hàng. Chúng tôi hình thức hóa hàm g(x, a;E) cho neuron-wise MoS như sau:
Wa=∑i diag(βia)Eia, (4)

trong đó diag(β) xây dựng ma trận đường chéo noutbig×noutbig bằng cách đặt β trên đường chéo, và βia là cột thứ i của βa. Ei vẫn là ma trận noutbig×nin như trong layer-wise MoS. So với layer-wise MoS, neuron-wise MoS cung cấp tính linh hoạt tăng cường (m×nouta thay vì chỉ m) để quản lý mức độ chia sẻ trọng số giữa các kiến trúc khác nhau, với số lượng tham số vẫn tỷ lệ thuận với m. Neuron-wise MoS cung cấp kiểm soát tinh tế hơn đối với việc chia sẻ trọng số giữa các subnetwork. Xung đột gradient, được tính toán sử dụng độ tương tự cosine giữa gradient supernet và gradient subnet nhỏ nhất theo NASVIT (Gong et al., 2021), là thấp nhất đối với neuron-wise MoS so với layer-wise MoS và HAT, được thể hiện bởi độ tương tự cosine cao nhất (xem A.1.2).

3.4 Thêm g(x, a;E) vào Transformer
MoS có thể thích ứng với một lớp tuyến tính đơn, nhiều lớp tuyến tính và các lớp được tham số hóa khác (ví dụ: layer-norm). Cho rằng lớp tuyến tính chiếm ưu thế về số lượng tham số, chúng tôi áp dụng cách tiếp cận được sử dụng trong hầu hết công trình MoE (Fedus et al., 2022). Chúng tôi áp dụng MoS vào Transformer dựa trên chia sẻ trọng số tiêu chuẩn (fa(x;W)) bằng cách thay thế hai lớp tuyến tính trong mỗi khối mạng feed-forward bằng g(x, a;E).

4 Thí nghiệm - BERT Hiệu quả
4.1 Thiết lập Thí nghiệm
Chúng tôi khám phá việc áp dụng supernet đề xuất trong việc xây dựng các mô hình BERT bất khả tri tác vụ hiệu quả (Devlin et al., 2019), tập trung vào tác vụ pretraining BERT. Điều này bao gồm việc pretraining một mô hình ngôn ngữ từ đầu để học các biểu diễn văn bản bất khả tri tác vụ sử dụng mục tiêu masked language modeling. Mô hình BERT được pretrained sau đó được finetuned trên nhiều tác vụ NLP downstream khác nhau. Trọng tâm là xây dựng các mô hình BERT có độ chính xác cao nhưng nhỏ (ví dụ: 5M−50M tham số). Cả supernet BERT và các mô hình độc lập đều được pretrained từ đầu trên Wikipedia và Books Corpus (Zhu et al., 2015). Đánh giá hiệu suất được thực hiện bằng cách finetuning trên bảy tác vụ từ benchmark GLUE (Wang et al., 2018), được chọn bởi AutoDistil (Xu et al., 2022a). Mã hóa kiến trúc, tiền xử lý dữ liệu, cài đặt pretraining và cài đặt finetuning được chi tiết trong A.4.1. Các mô hình baseline bao gồm các mô hình độc lập và supernet tiêu chuẩn được đề xuất trong SuperShaper (Ganesan et al., 2021). Các mô hình đề xuất của chúng tôi là layer-wise và neuron-wise MoS. Tất cả các supernet trải qua sandwich training. Tham số m và chiều ẩn của router được đặt lần lượt là 2 và 128 cho các supernet MoS.

4.2 Khoảng cách giữa Supernet và standalone
Để điều tra khoảng cách giữa supernet và standalone, không gian tìm kiếm được dẫn xuất từ SuperShaper (Ganesan et al., 2021), bao gồm các kiến trúc BERT chỉ khác nhau về kích thước ẩn ở mỗi lớp (120, 240, 360, 480, 540, 600, 768) với số lượng lớp cố định (12) và số đầu attention (12). Không gian tìm kiếm này bao gồm khoảng 14 tỷ kiến trúc. Chúng tôi kiểm tra khoảng cách giữa supernet và mô hình độc lập cho kiến trúc mô hình hàng đầu từ mặt phẳng Pareto của Supernet (Sandwich) (Ganesan et al., 2021). Bảng 2 minh họa hiệu suất benchmark GLUE của huấn luyện độc lập cho kiến trúc (ngân sách pretraining 1x, tương đương với 2048 batch size * 125.000 steps) cũng như trọng số cụ thể theo kiến trúc từ các supernet khác nhau (0 steps pretraining bổ sung; tức là chỉ pretraining supernet). MoS (layer-wise hoặc neuron-wise) thu hẹp khoảng cách giữa supernet cụ thể tác vụ và hiệu suất độc lập cho 6 trong 7 tác vụ, bao gồm MNLI, một tác vụ được sử dụng rộng rãi để đánh giá các mô hình ngôn ngữ pretrained (Liu et al., 2019; Xu et al., 2022b). Khoảng cách GLUE trung bình giữa mô hình độc lập và supernet tiêu chuẩn là 0.13 điểm. Đáng chú ý, với các tính chất tùy chỉnh và biểu đạt, layer-wise và neuron-wise MoS cải thiện đáng kể huấn luyện độc lập lần lượt 0.75 và 0.85 điểm GLUE trung bình. Bảng 2 chứng minh rằng MoS áp đặt chi phí tính toán dưới 22% cho BERT, dẫn đến tối thiểu 0.8 cải thiện GLUE trung bình so với supernet tiêu chuẩn. Chi phí này có thể không đáng kể, vì nó đại diện cho một khoản đầu tư một lần loại bỏ nhu cầu huấn luyện bổ sung sau quá trình tìm kiếm.

4.3 So sánh với NAS SoTA
Các framework NAS SoTA để xây dựng mô hình BERT bất khả tri tác vụ là NAS-BERT (Xu et al., 2021) và AutoDistil (Xu et al., 2022a). Pipeline NAS-BERT bao gồm: (1) huấn luyện supernet (với stack Transformer chứa multi-head attention, feed-forward network [FFN], và các lớp tích chập ở các vị trí tùy ý), (2) tìm kiếm dựa trên loss distillation (bất khả tri tác vụ), và (3) pretraining kiến trúc tốt nhất từ đầu (ngân sách pretraining 1x, tương đương với 2048 batch size * 125.000 steps). Bước thứ ba cần được lặp lại cho mỗi thay đổi ràng buộc và thay đổi phần cứng, phát sinh chi phí đáng kể. Pipeline AutoDistil bao gồm: (1) xây dựng K không gian tìm kiếm và huấn luyện supernet cho mỗi không gian tìm kiếm độc lập, (2a) chế độ tìm kiếm agnostic: tìm kiếm dựa trên loss self-attention distillation (bất khả tri tác vụ), (2b) chế độ tìm kiếm proxy: tìm kiếm dựa trên điểm số validation MNLI, và (3) trích xuất trọng số cụ thể theo kiến trúc từ supernet mà không cần huấn luyện bổ sung. Bước đầu tiên có thể tốn kém vì pretraining K supernet đòi hỏi tính toán và bộ nhớ huấn luyện gấp K lần, so với huấn luyện một supernet đơn lẻ. Chế độ tìm kiếm proxy có thể ưu ái AutoDistil một cách không công bằng, vì nó finetunes tất cả kiến trúc trong không gian tìm kiếm của nó trên MNLI và sử dụng điểm số MNLI để xếp hạng. Để so sánh công bằng với SoTA, tác vụ MNLI được loại trừ khỏi đánh giá.

Pipeline NAS đề xuất của chúng tôi giải quyết những thách thức trong NAS-BERT và AutoDistil. So với NAS SoTA, không gian tìm kiếm của chúng tôi bao gồm các kiến trúc BERT với các lớp Transformer đồng nhất: kích thước ẩn (120 đến 768 với bước tăng 12), attention heads (6, 12), tỷ lệ chiều ẩn FFN trung gian (2, 2.5, 3, 3.5, 4). Không gian tìm kiếm này bao gồm 550 kiến trúc, tương tự như AutoDistil. Supernet dựa trên neuron-wise MoS, và tìm kiếm sử dụng perplexity (bất khả tri tác vụ) để xếp hạng kiến trúc. Khác với NAS-BERT, trọng số kiến trúc cuối cùng của chúng tôi được trích xuất trực tiếp từ supernet mà không cần pretraining bổ sung. Khác với AutoDistil, pipeline của chúng tôi chỉ pretrain một supernet, giảm đáng kể tính toán và bộ nhớ huấn luyện. Chúng tôi chỉ sử dụng các thước đo bất khả tri tác vụ để tìm kiếm, tương tự như cài đặt agnostic của AutoDistil. Bảng 3 so sánh supernet neuron-wise MoS với NAS-BERT và AutoDistil cho nhiều kích thước mô hình khác nhau. Hiệu suất NAS-BERT và AutoDistil được lấy từ các bài báo tương ứng. Về GLUE trung bình, pipeline của chúng tôi cải thiện so với NAS-BERT cho các kích thước mô hình 5M, 10M và 30M, không có huấn luyện bổ sung (tiết kiệm 100% tính toán huấn luyện bổ sung, tương đương với 2048 batch size * 125.000 steps). Về GLUE trung bình, pipeline của chúng tôi: (i) vượt qua AutoDistil-proxy cho các kích thước mô hình 6.88M và 50M với lần lượt ít hơn 1.88M và 0.1M tham số, và (ii) vượt trội cả AutoDistil-proxy và AutoDistil-agnostic cho kích thước mô hình 26M. Ngoài việc đạt kết quả SoTA, phương pháp của chúng tôi giảm đáng kể khối lượng công việc nặng nề của việc huấn luyện nhiều mô hình trong huấn luyện lại subnetwork (NAS-BERT) hoặc huấn luyện supernet (AutoDistil).

--- TRANG 7 ---
5 Thí nghiệm - MT Hiệu quả
5.1 Thiết lập thí nghiệm
Chúng tôi thảo luận về việc áp dụng các supernet đề xuất để xây dựng các mô hình MT hiệu quả theo thiết lập của Hardware-aware Transformers (HAT (Wang et al., 2020a)), framework NAS SoTA cho các mô hình MT với sự cân bằng độ trễ-BLEU tốt. Tập trung vào các benchmark WMT'14 En-De, WMT'14 En-Fr và WMT'19 En-De, chúng tôi duy trì mã hóa kiến trúc nhất quán và cài đặt huấn luyện cho các mô hình supernet và độc lập (chi tiết trong A.5.2). Các supernet baseline bao gồm HAT và Supernet (Sandwich). Các supernet đề xuất là Layer-wise MoS và Neuron-wise MoS, cả hai đều sử dụng sandwich training, với m và chiều ẩn của router được đặt lần lượt là 2 và 128. Tham khảo A.5.8 cho lý do chọn 'm'.

5.2 Khoảng cách giữa Supernet và standalone
Trong không gian tìm kiếm của HAT gồm 6M kiến trúc encoder-decoder, có các tham số linh hoạt như kích thước embedding (512 hoặc 640), các lớp decoder (1 đến 6), attention heads self/cross (4 hoặc 8), và số lượng lớp encoder hàng đầu cho attention decoder (1 đến 3), các supernet tốt nên thể hiện sai số tuyệt đối trung bình (MAE) tối thiểu và tương quan xếp hạng cao giữa hiệu suất supernet và độc lập cho một kiến trúc cho trước. Bảng 4 trình bày MAE và tương quan xếp hạng Kendall cho 15 kiến trúc ngẫu nhiên, thể hiện rằng sandwich training tạo ra MAE và chất lượng xếp hạng tốt hơn so với HAT. Trong khi các supernet đề xuất của chúng tôi đạt chất lượng xếp hạng tương đương cho WMT'14 En-Fr và WMT'19 En-De, và hơi kém hơn cho WMT'14 En-De, chúng thể hiện MAE tối thiểu trên tất cả các tác vụ. Đặc biệt, neuron-wise MoS đạt được cải thiện MAE đáng kể, gợi ý ít bước huấn luyện bổ sung cần thiết hơn để làm MAE trở nên không đáng kể (như được chi tiết trong Phần 5.4). Biểu đồ hiệu suất supernet và độc lập cho thấy neuron-wise MoS vượt trội với hầu hết các kiến trúc hiệu suất hàng đầu (xem A.5.3). Chi phí huấn luyện cho MoS nói chung là không đáng kể, ví dụ, với WMT'14 En-De, huấn luyện supernet mất 248 giờ, với neuron-wise MoS và layer-wise MoS cần lần lượt 14 và 18 giờ bổ sung (ít hơn 8% chi phí, xem Phần A.5.10 để biết chi tiết).

5.3 So sánh với NAS SoTA
Mặt phẳng Pareto từ supernet được thu được sử dụng thuật toán tìm kiếm evolutionary tận dụng supernet để nhanh chóng xác định các kiến trúc ứng viên hiệu suất hàng đầu và một bộ ước tính độ trễ để nhanh chóng loại bỏ các ứng viên có độ trễ vượt quá ngưỡng. Cài đặt cho thuật toán tìm kiếm evolutionary và bộ ước tính độ trễ được chi tiết trong A.5.4. Ba ngưỡng độ trễ được khám phá: 100ms, 150ms và 200ms. Bảng 5 minh họa sự cân bằng giữa độ trễ và hiệu suất supernet cho các mô hình trong mặt phẳng Pareto từ các supernet khác nhau. So với HAT, các supernet đề xuất đạt được BLEU cao hơn đáng kể một cách nhất quán cho mỗi ngưỡng độ trễ trên tất cả các tập dữ liệu, nhấn mạnh tầm quan trọng của việc chuyên biệt hóa kiến trúc và khả năng biểu đạt của supernet. Xem A.5.6 để biết tính nhất quán của những xu hướng này trên các seed khác nhau.

5.4 Huấn luyện bổ sung để đóng khoảng cách
Các supernet đề xuất giảm thiểu đáng kể khoảng cách giữa MAE supernet và độc lập (như đã thảo luận trong Phần 5.2), tuy nhiên khoảng cách vẫn còn không thể bỏ qua. Việc đóng khoảng cách cho một kiến trúc bao gồm trích xuất trọng số cụ thể theo kiến trúc từ supernet và tiến hành huấn luyện bổ sung cho đến khi đạt được hiệu suất độc lập (đạt được khoảng cách bằng 0). Một supernet hiệu quả nên yêu cầu số lượng tối thiểu các bước bổ sung và thời gian để các kiến trúc được trích xuất đóng khoảng cách. Trong bối cảnh huấn luyện bổ sung, chúng tôi đánh giá BLEU test cho mỗi kiến trúc sau mỗi 10K bước, dừng lại khi BLEU test khớp hoặc vượt quá BLEU test của mô hình độc lập. Bảng 6 trình bày số lượng bước huấn luyện bổ sung trung bình cần thiết cho tất cả các mô hình trên mặt phẳng Pareto từ mỗi supernet để đóng khoảng cách. So với HAT, layer-wise MoS đạt được giảm ấn tượng từ 9% đến 51% trong các bước huấn luyện, trong khi neuron-wise MoS mang lại giảm đáng kể nhất từ 21% đến 60%. Đối với tác vụ WMT'14 En-Fr, cả hai supernet MoS đều cần ít nhất 2.7% thời gian hơn HAT để đạt BLEU SoTA trên các ràng buộc khác nhau. Những kết quả này nhấn mạnh tầm quan trọng của việc chuyên biệt hóa kiến trúc và khả năng biểu đạt của supernet trong việc cải thiện đáng kể hiệu quả huấn luyện của các subnet được trích xuất từ supernet.

5.5 So sánh với AutoMoE
Mặc dù AutoMoE (Jawahar et al., 2023) và MoS theo đuổi các mục tiêu khác nhau (như đã thảo luận trong Phụ lục A.3), chúng tôi tiến hành so sánh điểm số BLEU supernet của HAT, AutoMoE và MoS dưới ràng buộc độ trễ 200 ms trên GPU NVIDIA V100 trên ba benchmark WMT. Bảng 7 cho thấy rằng MoS nhất quán vượt trội hơn AutoMoE và HAT trên tất cả các tập dữ liệu. Thú vị là, AutoMoE thua kém HAT, gợi ý sự khác biệt tiềm năng giữa hiệu suất supernet và mô hình độc lập của AutoMoE.

6 Công trình Liên quan
Trong phần này, chúng tôi tóm tắt ngắn gọn nghiên cứu hiện tại về NAS trong NLP. Evolved Transformer (ET) (So et al., 2019) là một công trình ban đầu khám phá NAS cho các mô hình MT hiệu quả. Nó sử dụng tìm kiếm evolutionary và phân bổ động tài nguyên huấn luyện cho các ứng viên hứa hẹn. HAT (Wang et al., 2020a) giới thiệu supernet chia sẻ trọng số như một bộ ước tính hiệu suất, phân bổ chi phí huấn luyện cho các đánh giá ứng viên MT trong tìm kiếm evolutionary. NAS-BERT (Xu et al., 2021) phân chia mô hình BERT-Base thành các khối và huấn luyện supernet chia sẻ trọng số để distill mỗi khối. NAS-BERT sử dụng progressive shrinking trong quá trình huấn luyện supernet để loại bỏ các ứng viên ít hứa hẹn, xác định các kiến trúc hàng đầu cho mỗi ràng buộc hiệu quả một cách nhanh chóng. Tuy nhiên, NAS-BERT đòi hỏi pretraining kiến trúc hàng đầu từ đầu cho mỗi thay đổi ràng buộc, phát sinh chi phí cao. SuperShaper (Ganesan et al., 2021) pretrain supernet chia sẻ trọng số cho BERT sử dụng mục tiêu masked language modeling với sandwich training. AutoDistil (Xu et al., 2022a) sử dụng few-shot NAS (Zhao et al., 2021): xây dựng K không gian tìm kiếm của các kiến trúc BERT không chồng chéo và huấn luyện supernet BERT chia sẻ trọng số cho mỗi không gian tìm kiếm. Tìm kiếm dựa trên loss self-attention distillation với BERT-Base (tìm kiếm bất khả tri tác vụ) và điểm số MNLI (tìm kiếm proxy). AutoMoE (Jawahar et al., 2023) tăng cường không gian tìm kiếm của HAT với các mô hình mixture-of-expert để thiết kế các mô hình dịch thuật hiệu quả. Tham khảo A.3 cho những khác biệt chính giữa framework của chúng tôi và framework AutoMoE.

Trong cộng đồng computer vision, K-shot NAS (Su et al., 2021) tạo ra trọng số cho mỗi subnet như một kết hợp lồi của các trọng số supernet khác nhau trong một từ điển sử dụng simplex code. Trong khi K-shot NAS có những điểm tương đồng với layer-wise MoS, có những khác biệt quan trọng. K-shot NAS huấn luyện bộ tạo mã kiến trúc và supernet lặp đi lặp lại do khó khăn huấn luyện, trong khi layer-wise MoS huấn luyện tất cả các thành phần của nó cùng nhau. K-shot NAS đã được áp dụng cụ thể cho các kiến trúc tích chập cho các tác vụ phân loại hình ảnh. Tuy nhiên, nó đưa ra quá nhiều tham số với sự gia tăng số lượng supernet (K), một mối quan tâm được giảm bởi neuron-wise MoS do việc chuyên biệt hóa trọng số chi tiết của nó. Trong công trình của chúng tôi, chúng tôi tập trung vào các tác vụ NLP và các baseline liên quan, lưu ý rằng supernet trong NLP có xu hướng tụt hậu đáng kể so với các mô hình độc lập về mặt hiệu suất. Ngoài ra, các tác giả của K-shot NAS chưa phát hành mã để tái tạo kết quả của họ, ngăn cản việc đánh giá trực tiếp so với phương pháp của họ.

--- TRANG 9 ---
7 Kết luận
Chúng tôi đã giới thiệu Mixture-of-Supernets, một công thức hóa nhằm tăng cường sức mạnh biểu đạt của supernet. Bằng cách áp dụng ý tưởng MoE, chúng tôi đã chứng minh khả năng tạo ra trọng số linh hoạt cho subnetwork. Thông qua các đánh giá toàn diện để xây dựng các mô hình BERT và MT hiệu quả, các supernet của chúng tôi đã thể hiện khả năng: (i) giảm thiểu thời gian huấn luyện lại, từ đó cải thiện đáng kể hiệu quả NAS, và (ii) tạo ra các kiến trúc chất lượng cao đáp ứng các ràng buộc do người dùng xác định.

8 Hạn chế
Các hạn chế của công trình này như sau:
1. Việc áp dụng Mixture-of-Supernet (MoS) vào các benchmark phổ biến trong NLP, tập trung vào dịch máy hiệu quả và BERT, mang lại những hiểu biết có giá trị. Một hướng tương lai có tác động tiềm năng có thể bao gồm việc mở rộng ứng dụng của MoS để xây dựng các mô hình ngôn ngữ decoder-only autoregressive hiệu quả, như GPT-4 (OpenAI, 2023).
2. Việc giới thiệu kiến trúc MoE có khả năng cần ngân sách huấn luyện nhiều hơn. Trong công trình của chúng tôi, chúng tôi không sử dụng số lượng lớn các vòng lặp huấn luyện để so sánh công bằng và việc cố định số lượng trọng số chuyên gia (m) thành 2 hoạt động tốt. Chúng tôi sẽ điều tra tiềm năng đầy đủ của các supernet đề xuất bằng cách kết hợp ngân sách huấn luyện lớn hơn (ví dụ: ≥200K bước) và số lượng trọng số chuyên gia lớn hơn (ví dụ: ≥16 trọng số chuyên gia) trong công việc tương lai.
3. Do yêu cầu tính toán cao để pretraining BERT, chúng tôi chỉ điều tra khoảng cách giữa supernet và mô hình độc lập cho mô hình hàng đầu từ mặt phẳng Pareto của Supernet (Sandwich) (xem Bảng 2). Sẽ thú vị khi khám phá khoảng cách này cho một số lượng lớn hơn các kiến trúc từ không gian tìm kiếm, như được thể hiện trong Bảng 4 cho các tác vụ MT.

Lời cảm ơn
MAM ghi nhận sự hỗ trợ từ Canada Research Chairs (CRC), Natural Sciences and Engineering Research Council of Canada (NSERC; RGPIN-2018-04267), Canadian Foundation for Innovation (CFI; 37771), và Digital Research Alliance of Canada. Nghiên cứu của Lakshmanan được hỗ trợ một phần bởi một khoản tài trợ từ NSERC (Canada). Chúng tôi đã sử dụng ChatGPT để diễn đạt lại và kiểm tra ngữ pháp của bài báo.

Tài liệu tham khảo
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, và Quoc Le. 2018. Understanding and simplifying one-shot architecture search. Trong International conference on machine learning, trang 550–559. PMLR.

Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, và Song Han. 2020. Once for All: Train One Network and Specialize it for Efficient Deployment. Trong International Conference on Learning Representations.

--- TRANG 10 ---
Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

William Fedus, Barret Zoph, và Noam Shazeer. 2022. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Journal of Machine Learning Research, 23(120):1–39.

Vinod Ganesan, Gowtham Ramesh, và Pratyush Kumar. 2021. SuperShaper: Task-Agnostic Super Pre-training of BERT Models with Variable Hidden Dimensions. CoRR, abs/2110.04711.

Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian, Vikas Chandra, et al. 2021. NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training. Trong International Conference on Learning Representations.

Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, và Jian Sun. 2020. Single Path One-Shot Neural Architecture Search with Uniform Sampling. Trong Computer Vision – ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVI, trang 544–560, Berlin, Heidelberg. Springer-Verlag.

Peter Izsak, Moshe Berchansky, và Omer Levy. 2021. How to train BERT with an academic budget. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 10644–10652, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, Laks Lakshmanan, V.S., Ahmed Hassan Awadallah, Sebastien Bubeck, và Jianfeng Gao. 2023. AutoMoE: Heterogeneous mixture-of-experts with adaptive computation for efficient neural machine translation. Trong Findings of the Association for Computational Linguistics: ACL 2023, trang 9116–9132, Toronto, Canada. Association for Computational Linguistics.

Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, và Qun Liu. 2020. TinyBERT: Distilling BERT for Natural Language Understanding. Trong Findings of the Association for Computational Linguistics: EMNLP 2020, trang 4163–4174, Online. Association for Computational Linguistics.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. Cite arxiv:1907.11692.

OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.

Kishore Papineni, Salim Roukos, Todd Ward, và Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. Trong Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, trang 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, và Jeff Dean. 2018. Efficient neural architecture search via parameters sharing. Trong International conference on machine learning, trang 4095–4104. PMLR.

Matt Post. 2018. A Call for Clarity in Reporting BLEU Scores. Trong Proceedings of the Third Conference on Machine Translation: Research Papers, trang 186–191, Brussels, Belgium. Association for Computational Linguistics.

David So, Quoc Le, và Chen Liang. 2019. The Evolved Transformer. Trong Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, trang 5877–5886. PMLR.

Xiu Su, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, Changshui Zhang, và Chang Xu. 2021. K-shot NAS: Learnable Weight-Sharing for NAS with K-shot Supernets. Trong Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, trang 9880–9890. PMLR.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. Trong Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, trang 353–355, Brussels, Belgium. Association for Computational Linguistics.

Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, và Song Han. 2020a. HAT: Hardware-aware transformers for efficient natural language processing. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 7675–7688, Online. Association for Computational Linguistics.

Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, và Ming Zhou. 2020b. MINILM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. Trong Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20.

Dongkuan Xu, Subhabrata Mukherjee, Xiaodong Liu, Debadeepta Dey, Wenhui Wang, Xiang Zhang, Ahmed Hassan Awadallah, và Jianfeng Gao. 2022a. Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models. Trong Advances in Neural Information Processing Systems.

--- TRANG 11 ---
Jin Xu, Xu Tan, Renqian Luo, Kaitao Song, Jian Li, Tao Qin, và Tie-Yan Liu. 2021. NAS-BERT: Task-Agnostic and Adaptive-Size BERT Compression with Neural Architecture Search. Trong Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '21, trang 1933–1943, New York, NY, USA. Association for Computing Machinery.

Jin Xu, Xu Tan, Kaitao Song, Renqian Luo, Yichong Leng, Tao Qin, Tie-Yan Liu, và Jian Li. 2022b. Analyzing and Mitigating Interference in Neural Architecture Search. Trong Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, trang 24646–24662. PMLR.

Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, và Quoc Le. 2020. BigNAS: Scaling up Neural Architecture Search with Big Single-Stage Models. Trong Computer Vision – ECCV 2020, trang 702–717, Cham. Springer International Publishing.

Yiyang Zhao, Linnan Wang, Yuandong Tian, Rodrigo Fonseca, và Tian Guo. 2021. Few-Shot Neural Architecture Search. Trong Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, trang 12707–12718. PMLR.

Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, và Sanja Fidler. 2015. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. Trong The IEEE International Conference on Computer Vision (ICCV).

Barret Zoph, Vijay Vasudevan, Jonathon Shlens, và Quoc V Le. 2018. Learning transferable architectures for scalable image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 8697–8710.

A Phụ lục
A.1 Phân tích Chia sẻ Trọng số và Xung đột Gradient
A.1.1 Khoảng cách Jensen-Shannon của vector alignment như thước đo chia sẻ trọng số
Chúng tôi sử dụng khoảng cách Jensen-Shannon của vector alignment được tạo ra bởi Layer-wise MoS cho hai kiến trúc như một proxy để định lượng mức độ chia sẻ trọng số. Lý tưởng nhất, khoảng cách Jensen-Shannon càng thấp thì mức độ chia sẻ trọng số càng cao và ngược lại. Chúng tôi thí nghiệm với hai kiến trúc 23M tham số (Smallest A và Smallest B) và hai kiến trúc 118M tham số (Largest A và Largest B). Từ Bảng 8, rõ ràng là Layer-wise MoS tạo ra mức độ chia sẻ trọng số thấp giữa các kiến trúc có kích thước khác nhau được thể hiện bởi khoảng cách Jensen-Shannon cao hơn giữa các vector alignment của chúng. Mặt khác, có mức độ chia sẻ trọng số cao giữa các kiến trúc có kích thước tương tự trong đó khoảng cách Jensen-Shannon thấp đáng kể.

A.1.2 Độ tương tự cosine giữa gradient supernet và gradient subnet nhỏ nhất như thước đo xung đột gradient
Chúng tôi tính toán xung đột gradient sử dụng độ tương tự cosine giữa gradient supernet và gradient subnet nhỏ nhất, theo công trình NASVIT (Gong et al., 2021). Trong Bảng 9, chúng tôi cho thấy rằng Neuron-wise MoS có xung đột gradient thấp nhất so với Layer-wise MoS và HAT, được thể hiện bởi độ tương tự cosine cao nhất.

A.2 Thuật toán chi tiết cho Huấn luyện Supernet và Tìm kiếm
A.2.1 Thuật toán huấn luyện supernet
Thuật toán chi tiết cho huấn luyện supernet được thể hiện trong Thuật toán 1.

A.2.2 Thuật toán tìm kiếm
Thuật toán chi tiết cho tìm kiếm được thể hiện trong Thuật toán 2.

A.3 So sánh với công trình AutoMoE
Mục tiêu: Cho một không gian tìm kiếm của các mô hình dense và mixture-of-expert, mục tiêu của framework AutoMoE (Jawahar et al., 2023) là tìm kiếm các kiến trúc mô hình hiệu suất cao thỏa mãn các ràng buộc hiệu quả do người dùng định nghĩa. Các kiến trúc cuối cùng có thể là các mô hình dense hoặc mixture-of-expert. Mặt khác, cho một không gian tìm kiếm chỉ gồm các mô hình dense, mục tiêu của framework Mixture-of-Supernets là tìm kiếm các kiến trúc mô hình dense hiệu suất cao thỏa mãn các ràng buộc hiệu quả do người dùng định nghĩa. Kiến trúc cuối cùng chỉ có thể là một mô hình dense. Ngoài ra, framework MoS giảm thiểu tính toán huấn luyện lại cần thiết để kiến trúc được tìm kiếm tiếp cận hiệu suất độc lập. Framework MoS thiết kế supernet với việc chia sẻ trọng số linh hoạt và khả năng cao. Mặt khác, supernet cơ bản của framework AutoMoE gặp vấn đề chia sẻ trọng số nghiêm ngặt và khả năng hạn chế.

Ứng dụng của mixture-of-experts: Ứng dụng chính của ý tưởng mixture-of-experts bởi framework AutoMoE là tăng cường không gian tìm kiếm NAS tiêu chuẩn của các mô hình dense với các mô hình mixture-of-experts. Để thực hiện điều này, framework AutoMoE sửa đổi supernet chia sẻ trọng số tiêu chuẩn để hỗ trợ tạo trọng số cho các mô hình mixture-of-expert. Mặt khác, framework Mixture-of-Supernets sử dụng thiết kế mixture-of-expert để: (i) tăng khả năng của supernet chia sẻ trọng số tiêu chuẩn và (ii) tùy chỉnh trọng số cho mỗi kiến trúc. Sau huấn luyện, trọng số chuyên gia được thu gọn để tạo ra một trọng số đơn lẻ cho kiến trúc dense.

Đặc tả Router: Router cơ bản của framework AutoMoE nhận token embedding làm đầu vào, đầu ra là phân phối xác suất trên các chuyên gia, và truyền token embedding tới các chuyên gia top-k. Mặt khác, router cơ bản của framework Mixture-of-Supernets nhận architecture embedding làm đầu vào, đầu ra là phân phối xác suất trên các chuyên gia (layer-wise MoS) / nơ-ron (neuron-wise MoS), sử dụng phân phối xác suất để kết hợp TẤT CẢ trọng số chuyên gia thành một trọng số đơn lẻ, và truyền token embedding tới trọng số đơn lẻ (tất cả chuyên gia).

A.4 Thí nghiệm Bổ sung - BERT Hiệu quả
A.4.1 Cài đặt pretraining / finetuning BERT
Dữ liệu pretraining: Dữ liệu pretraining bao gồm văn bản từ Wikipedia và Books Corpus (Zhu et al., 2015). Chúng tôi sử dụng các script tiền xử lý dữ liệu được cung cấp bởi Izsak et al. để xây dựng văn bản đã được token hóa.

Cài đặt pretraining supernet và độc lập: Cài đặt pretraining cho các mô hình supernet và độc lập được lấy từ SuperShaper (Ganesan et al., 2021): batch size 2048, độ dài sequence tối đa 128, bước huấn luyện 125K, learning rate 5e−4, weight decay 0.01, và warmup steps 10K (0 cho độc lập). Đối với các thí nghiệm với không gian tìm kiếm từ SuperShaper (Ganesan et al., 2021) (Phần 4.2), mã hóa kiến trúc a là danh sách kích thước ẩn ở mỗi lớp của kiến trúc (12 phần tử vì supernet là mô hình 12 lớp). Đối với các thí nghiệm với không gian tìm kiếm ngang hàng với AutoDistil (Xu et al., 2022a) (Phần 4.3), mã hóa kiến trúc a là danh sách bốn siêu tham số elastic của kiến trúc BERT đồng nhất: số lượng lớp, kích thước ẩn của tất cả các lớp, tỷ lệ mở rộng mạng feed-forward (FFN) của tất cả các lớp và số lượng attention heads của tất cả các lớp (xem Bảng 10 cho các kiến trúc BERT đồng nhất mẫu).

Cài đặt finetuning: Chúng tôi đánh giá hiệu suất của mô hình BERT bằng cách finetuning trên mỗi trong số bảy tác vụ (được chọn bởi AutoDistil (Xu et al., 2022a)) trong benchmark GLUE (Wang et al., 2018). Thước đo đánh giá là độ chính xác trung bình (hệ số tương quan Matthews chỉ cho CoLA) trên tất cả các tác vụ (trung bình GLUE). Cài đặt finetuning được lấy từ bài báo BERT (Devlin et al., 2019): learning rate từ {5e−5, 3e−5, 2e−5}, batch size từ {16, 32}, và epochs từ {2, 3, 4}.

A.4.2 Đường cong học tập cho các biến thể supernet BERT
Hình 2 hiển thị các bước huấn luyện so với validation MLM loss (đường cong học tập) cho mô hình BERT độc lập và các biến thể BERT dựa trên supernet khác nhau. Mô hình độc lập và supernet được so sánh cho kiến trúc lớn nhất (big) và kiến trúc nhỏ nhất (small) từ không gian tìm kiếm của SuperShaper (Ganesan et al., 2021). Đối với kiến trúc lớn nhất, mô hình độc lập hoạt động tốt nhất. Đối với kiến trúc nhỏ nhất, mô hình độc lập bị vượt trội bởi tất cả các biến thể supernet. Trong cả hai trường hợp, các supernet đề xuất (đặc biệt là neuron-wise MoS) hoạt động tốt hơn nhiều so với supernet tiêu chuẩn.

A.4.3 So sánh kiến trúc của Neuron-wise MoS vs. AutoDistil
Bảng 10 hiển thị so sánh kiến trúc BERT được thiết kế bởi neuron-wise MoS đề xuất của chúng tôi với AutoDistil.

A.4.4 So sánh công bằng của Neuron-wise MoS w.r.t SoTA với MNLI
Chúng tôi so sánh neuron-wise MoS với NAS-BERT và AutoDistil (agnostic) cho các kích thước mô hình khác nhau (≤50M tham số) dựa trên hiệu suất validation GLUE. Trong Bảng 11, chúng tôi bao gồm kết quả trên tác vụ MNLI. Để so sánh công bằng, chúng tôi loại bỏ AutoDistil (proxy), sử dụng trực tiếp tác vụ MNLI để lựa chọn kiến trúc. Neuron-wise MoS cải thiện so với các baseline trong tất cả kích thước mô hình, về mặt GLUE trung bình. Đối với tác vụ MNLI, neuron-wise MoS cải thiện so với các baseline trong hầu hết kích thước mô hình.

A.4.5 Kết quả BERT với các random seed khác nhau
Bảng 12 hiển thị kết quả BERT trên CoLA và RTE với các random seed khác nhau. Layer-wise MoS nhất quán tăng cường hiệu suất so với các baseline trong RTE và giảm hiệu suất so với các baseline trong CoLA cho cả hai seed. Kiến trúc BERT (67M tham số) tương ứng với mô hình hàng đầu từ mặt phẳng Pareto của Supernet (Sandwich) trong không gian tìm kiếm của SuperShaper (nhất quán với Bảng 2).

A.5 Thí nghiệm Bổ sung - Dịch máy Hiệu quả
A.5.1 Dữ liệu benchmark dịch máy
Bảng 13 hiển thị thống kê của ba tập dữ liệu dịch máy: WMT'14 En-De, WMT'14 En-Fr, và WMT'19 En-De.

A.5.2 Cài đặt huấn luyện và thước đo
Cài đặt huấn luyện cho cả mô hình supernet và độc lập đều giống nhau: 40K bước huấn luyện, bộ tối ưu hóa Adam, bộ lập lịch learning rate cosine, và warmup của learning rate từ 10−7 đến 10−3 với cosine annealing. Checkpoint tốt nhất được chọn dựa trên validation loss, trong khi hiệu suất của mô hình MT được đánh giá dựa trên BLEU. Kích thước beam là bốn với length penalty 0.6. Mã hóa kiến trúc a là danh sách 10 giá trị sau:
1. Chiều embedding encoder tương ứng với chiều embedding của encoder.
2. Số lớp encoder tương ứng với số lượng lớp encoder.
3. Chiều trung gian FFN encoder trung bình tương ứng với trung bình của chiều trung gian FFN trên các lớp encoder.
4. Attention heads tự encoder trung bình tương ứng với trung bình của số lượng attention heads tự trên các lớp encoder.
5. Chiều embedding decoder tương ứng với chiều embedding của decoder.
6. Số lớp decoder tương ứng với số lượng lớp decoder.
7. Chiều Trung gian FFN Decoder trung bình tương ứng với trung bình của chiều trung gian FFN trên các lớp decoder.
8. Attention heads tự decoder trung bình tương ứng với trung bình của số lượng attention heads tự trên các lớp decoder.
9. Attention heads cross decoder trung bình tương ứng với trung bình của số lượng attention heads cross trên các lớp decoder.
10. Attention encoder decoder tùy ý trung bình tương ứng với số lượng trung bình của các lớp encoder được attention bởi cross-attention heads trong mỗi lớp decoder (-1 có nghĩa là chỉ attend tới lớp cuối cùng, 1 có nghĩa là attend tới hai lớp cuối cùng, 2 có nghĩa là attend tới ba lớp cuối cùng).

--- TRANG 17 ---
A.5.3 Biểu đồ hiệu suất Supernet vs. Standalone
Hình 3 hiển thị hiệu suất supernet so với độc lập cho 15 kiến trúc được lấy mẫu ngẫu nhiên trên tất cả ba tác vụ. Neuron-wise MoS vượt trội với hầu hết tất cả các kiến trúc hiệu suất hàng đầu (≥26.5 và ≥42.5 BLEU độc lập cho WMT'14 En-De và WMT'19 En-De tương ứng), cho thấy rằng các mô hình đặc biệt trong mặt phẳng Pareto có thể được hưởng lợi to lớn từ việc chuyên biệt hóa cấp độ nơ-ron.

A.5.4 Cài đặt HAT
Tìm kiếm evolutionary: Cài đặt cho thuật toán tìm kiếm evolutionary bao gồm: 30 vòng lặp, kích thước population 125, population cha mẹ 25, population crossover 50, và population mutation 50 với xác suất mutation 0.3.

Bộ ước tính độ trễ: Bộ ước tính độ trễ được phát triển theo hai giai đoạn. Đầu tiên, tập dữ liệu độ trễ được xây dựng bằng cách đo độ trễ của 2000 kiến trúc được lấy mẫu ngẫu nhiên trực tiếp trên phần cứng do người dùng định nghĩa (GPU NVIDIA V100). Độ trễ là thời gian để dịch một câu nguồn thành câu đích (độ dài câu nguồn và đích mỗi câu 30 token). Đối với mỗi kiến trúc, 300 phép đo độ trễ được thực hiện, các ngoại lệ (10% hàng đầu và 10% hàng dưới) được loại bỏ, và phần còn lại (80%) được tính trung bình. Thứ hai, bộ ước tính độ trễ là một bộ hồi quy dựa trên mạng nơ-ron đa lớp 3 lớp, được huấn luyện sử dụng mã hóa và độ trễ của kiến trúc làm tính năng và nhãn tương ứng.

A.5.5 Các bước huấn luyện bổ sung để đóng khoảng cách vs. hiệu suất
Hình 4, Hình 5, và Hình 6 hiển thị các bước huấn luyện bổ sung so với BLEU cho các ràng buộc độ trễ khác nhau trên tác vụ WMT'14 En-De, WMT'14 En-Fr và WMT'19 En-De tương ứng.

A.5.6 Tìm kiếm Evolutionary - Tính ổn định
Chúng tôi nghiên cứu tác động khởi tạo đối với tính ổn định của mặt phẳng Pareto được xuất ra bởi tìm kiếm evolutionary cho các supernet khác nhau. Bảng 14 hiển thị BLEU và độ trễ được lấy mẫu (trực tiếp) của các mô hình trong mặt phẳng Pareto cho các seed khác nhau trên tác vụ WMT'14 En-Fr. Sự khác biệt về độ trễ và BLEU trên các seed chủ yếu là tối thiểu. Kết quả này làm nổi bật rằng mặt phẳng Pareto được xuất ra bởi tìm kiếm evolutionary phần lớn ổn định cho tất cả các biến thể supernet.

A.5.7 Tác động của hàm router khác nhau
Bảng 15 hiển thị tác động của việc thay đổi số lượng lớp ẩn trong hàm router cho neuron-wise MoS trên tác vụ WMT'14 En-De. Hai lớp ẩn cung cấp lượng khả năng router phù hợp, trong khi việc thêm nhiều lớp ẩn hơn dẫn đến sự suy giảm hiệu suất ổn định.

A.5.8 Tác động của việc tăng số lượng trọng số chuyên gia 'm'
Bảng 16 hiển thị tác động của việc tăng số lượng trọng số chuyên gia 'm' cho tác vụ WMT'14 En-Fr, trong đó kiến trúc cho tất cả các supernet là kiến trúc hàng đầu từ mặt phẳng Pareto của HAT cho ràng buộc độ trễ 200ms. Dưới ngân sách huấn luyện tiêu chuẩn (40K bước cho MT), hiệu suất của layer-wise MoS dường như không cải thiện bằng cách tăng 'm' từ 2 lên 4. Việc tăng 'm' đưa vào quá nhiều tham số, có thể đòi hỏi sự gia tăng đáng kể trong ngân sách huấn luyện (ví dụ: gấp 2 lần số bước huấn luyện hơn ngân sách huấn luyện tiêu chuẩn). Để so sánh công bằng với tài liệu hiện tại, chúng tôi sử dụng ngân sách huấn luyện tiêu chuẩn cho tất cả các thí nghiệm. Chúng tôi sẽ điều tra tiềm năng đầy đủ của các supernet đề xuất bằng cách kết hợp ngân sách huấn luyện lớn hơn (ví dụ: ≥200K bước) và số lượng trọng số chuyên gia lớn hơn (ví dụ: ≥16 trọng số chuyên gia) trong công việc tương lai.

A.5.9 SacreBLEU vs. BLEU
Chúng tôi sử dụng BLEU tiêu chuẩn (Papineni et al., 2002) để định lượng hiệu suất của supernet theo HAT để so sánh công bằng. Trong Bảng 17, chúng tôi cũng thí nghiệm với SacreBLEU (Post, 2018), trong đó xu hướng tương tự của MoS mang lại hiệu suất tốt hơn cho một ràng buộc độ trễ cho trước vẫn đúng.

A.5.10 Phân tích việc tiết kiệm thời gian tổng thể
Bảng 18 hiển thị phân tích việc tiết kiệm thời gian tổng thể của các supernet MoS so với HAT để tính toán mặt phẳng Pareto cho tác vụ WMT'14 En-De. Các ràng buộc độ trễ bao gồm 100ms, 150ms, 200ms. MoS tiết kiệm tổng thể ít nhất 20% giờ GPU so với HAT, nhờ tiết kiệm đáng kể trong thời gian huấn luyện bổ sung (45%-51%).

A.5.11 Codebase
Chúng tôi chia sẻ codebase tại: https://github.com/UBC-NLP/MoS, có thể được sử dụng để tái tạo tất cả kết quả trong bài báo này. Đối với cả benchmark đánh giá BERT và dịch máy, chúng tôi thêm một tệp README chứa các hướng dẫn sau: (i) thiết lập môi trường (ví dụ: các phụ thuộc phần mềm), (ii) tải dữ liệu, (iii) huấn luyện supernet, (iv) tìm kiếm, và (v) huấn luyện lại subnet.

--- TRANG 20 ---
[Bảng 18 và nội dung tiếp theo với các bảng và số liệu thống kê chi tiết về việc tiết kiệm thời gian và so sánh hiệu suất]
