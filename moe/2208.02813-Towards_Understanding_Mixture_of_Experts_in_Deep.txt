# 2208.02813.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2208.02813.pdf
# File size: 3687621 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Towards Understanding Mixture of Experts in Deep
Learning
Zixiang Chenâˆ—and Yihe Dengâ€ and Yue Wuâ€¡and Quanquan GuÂ§and Yuanzhi LiÂ¶
Abstract
The Mixture-of-Experts (MoE) layer, a sparsely-activated model controlled by a router, has
achieved great success in deep learning. However, the understanding of such architecture remains
elusive. In this paper, we formally study how the MoE layer improves the performance of neural
network learning and why the mixture model will not collapse into a single model. Our empirical
results suggest that the cluster structure of the underlying problem and the non-linearity of the
expert are pivotal to the success of MoE. To further understand this, we consider a challenging
classication problem with intrinsic cluster structures, which is hard to learn using a single
expert. Yet with the MoE layer, by choosing the experts as two-layer nonlinear convolutional
neural networks (CNNs), we show that the problem can be learned successfully. Furthermore,
our theory shows that the router can learn the cluster-center features, which helps divide the
input complex problem into simpler linear classication sub-problems that individual experts
can conquer. To our knowledge, this is the rst result towards formally understanding the
mechanism of the MoE layer for deep learning.
1 Introduction
The Mixture-of-Expert (MoE) structure (Jacobs et al., 1991; Jordan and Jacobs, 1994) is a classic
design that substantially scales up the model capacity and only introduces small computation
overhead. In recent years, the MoE layer (Eigen et al., 2013; Shazeer et al., 2017), which is an
extension of the MoE model to deep neural networks, has achieved remarkable success in deep
learning. Generally speaking, an MoE layer contains many experts that share the same network
architecture and are trained by the same algorithm, with a gating (or routing) function that routes
individual inputs to a few experts among all the candidates. Through the sparse gating function,
the router in the MoE layer can route each input to the top- K(K2) best experts (Shazeer et al.,
2017), or the single ( K= 1) best expert (Fedus et al., 2021). This routing scheme only costs the
computation of Kexperts for a new input, which enjoys fast inference time.
âˆ—Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail:
chenzx19@cs.ucla.edu
â€ Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail:
yihedeng@cs.ucla.edu
â€¡Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail: :
ywu@cs.ucla.edu
Â§Department of Computer Science, University of California, Los Angeles, CA 90095, USA; e-mail:
qgu@cs.ucla.edu
Â¶Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA, USA; email:
yuanzhil@andrew.cmu.edu
1arXiv:2208.02813v1  [cs.LG]  4 Aug 2022

--- PAGE 2 ---
Despite the great empirical success of the MoE layer, the theoretical understanding of such
architecture is still elusive. In practice, all experts have the same structure, initialized from the same
weight distribution (Fedus et al., 2021) and are trained with the same optimization conguration.
The router is also initialized to dispatch the data uniformly. It is unclear why the experts can
diverge to dierent functions that are specialized to make predictions for dierent inputs, and why
the router can automatically learn to dispatch data, especially when they are all trained using
simple local search algorithms such as gradient descent. Therefore, we aim to answer the following
questions:
Why do the experts in MoE diversify instead of collapsing into a single model? And how can the
router learn to dispatch the data to the right expert?
In this paper, in order to answer the above question, we consider the natural \mixture of
classication" data distribution with cluster structure and theoretically study the behavior and
benet of the MoE layer. We focus on the simplest setting of the mixture of linear classication,
where the data distribution has multiple clusters, and each cluster uses separate (linear) feature
vectors to represent the labels. In detail, we consider the data generated as a combination of feature
patches, cluster patches, and noise patches (See Denition 3.1 for more details). We study training
an MoE layer based on the data generated from the \mixture of classication" distribution using
gradient descent, where each expert is chosen to be a two-layer CNN. The main contributions of
this paper are summarized as follows:
â€¢We rst prove a negative result (Theorem 4.1) that any single expert, such as two-layer CNNs
with arbitrary activation function, cannot achieve a test accuracy of more than 87 :5% on our
data distribution.
â€¢Empirically, we found that the mixture of linear experts performs better than the single expert
but is still signicantly worse than the mixture of non-linear experts. Figure 1 provides such a
result in a special case of our data distribution with four clusters. Although a mixture of linear
models can represent the labeling function of this data distribution with 100% accuracy, it fails
to learn so after training . We can see that the underlying cluster structure cannot be recovered
by the mixture of linear experts, and neither the router nor the experts are diversied enough
after training. In contrast, the mixture of non-linear experts can correctly recover the cluster
structure and diversify.
â€¢Motivated by the negative result and the experiment on the toy data, we study a sparsely-gated
MoE model with two-layer CNNs trained by gradient descent. We prove that this MoE model
can achieve nearly 100% test accuracy eciently (Theorem 4.2).
â€¢Along with the result on the test accuracy, we formally prove that each expert of the sparsely-
gated MoE model will be specialized to a specic portion of the data (i.e., at least one cluster),
which is determined by the initialization of the weights. In the meantime, the router can learn
the cluster-center features and route the input data to the right experts.
â€¢Finally, we also conduct extensive experiments on both synthetic and real datasets to corroborate
our theory.
Notation. We use lower case letters, lower case bold face letters, and upper case bold face letters
to denote scalars, vectors, and matrices respectively. We denote a union of disjoint sets ( Ai:i2I)
byti2IAi. For a vector x, we usekxk2to denote its Euclidean norm. For a matrix W, we use
kWkFto denote its Frobenius norm. Given two sequences fxngandfyng, we denote xn=O(yn) if
jxnjC1jynjfor some absolute positive constant C1,xn= 
(yn) ifjxnjC2jynjfor some absolute
2

--- PAGE 3 ---
Initialization Training FinishedMixture of nonlinear experts
Mixture of linear expertsFigure 1: Visualization of the training of MoE with nonlinear expert and linear expert .
Dierent colors denote router's dispatch to dierent experts. The lines denote the decision boundary
of the MoE model. The data points are visualized on 2d space via t-SNE (Van der Maaten and
Hinton, 2008). The MoE architecture follows section 3 where nonlinear experts use activation
function(z) =z3. For this visualization, we let the expert number M= 4 and cluster number
K= 4. We generate n= 1;600 data points from the distribution illustrated in Section 3 with
2(0:5;2),2(1;2),2(1;2), andp= 1. More details of the visualization are discussed in
Appendix A.
positive constant C2, andxn= (yn) ifC3jynjjxnjC4jynjfor some absolute constants C3;C4>
0. We also use eO() to hide logarithmic factors of dinO(). Additionally, we denote xn= poly(yn)
ifxn=O(yD
n) for some positive constant D, andxn= polylog(yn) ifxn= poly(log( yn)). We also
denote byxn=o(yn) if limn!1xn=yn= 0. Finally we use [ N] to denote the index set f1;:::;Ng.
2 Related Work
Mixture of Experts Model. The mixture of experts model (Jacobs et al., 1991; Jordan and
Jacobs, 1994) has long been studied in the machine learning community. These MoE models are
based on various base expert models such as support vector machine (Collobert et al., 2002) ,
Gaussian processes (Tresp, 2001), or hidden Markov models (Jordan et al., 1997). In order to
increase the model capacity to deal with the complex vision and speech data, Eigen et al. (2013)
extended the MoE structure to the deep neural networks, and proposed a deep MoE model composed
of multiple layers of routers and experts. Shazeer et al. (2017) simplied the MoE layer by making
the output of the gating function sparse for each example, which greatly improves the training
stability and reduces the computational cost. Since then, the MoE layer with dierent base neural
network structures (Shazeer et al., 2017; Dauphin et al., 2017; Vaswani et al., 2017) has been
proposed and achieved tremendous successes in a variety of language tasks. Very recently, Fedus
et al. (2021) improved the performance of the MoE layer by routing one example to only a single
expert instead of Kexperts, which further reduces the routing computation while preserving the
model quality.
3

--- PAGE 4 ---
Mixture of Linear Regressions/Classications. In this paper, we consider a \mixture of
classication" model. This type of models can be dated back to (De Veaux, 1989; Jordan and
Jacobs, 1994; Faria and Soromenho, 2010) and has been applied to many tasks including object
recognition (Quattoni et al., 2004) human action recognition (Wang and Mori, 2009), and machine
translation (Liang et al., 2006). In order to learn the unknown parameters for mixture of linear
regressions/classication model, (Anandkumar et al., 2012; Hsu et al., 2012; Chaganty and Liang,
2013; Anandkumar et al., 2014; Li and Liang, 2018) studies the method of moments and tensor
factorization. Another line of work studies specic algorithms such as Expectation-Maximization
(EM) algorithm (Khalili and Chen, 2007; Yi et al., 2014; Balakrishnan et al., 2017; Wang et al.,
2015).
Theoretical Understanding of Deep Learning. In recent years, great eorts have been made to
establish the theoretical foundation of deep learning. A series of studies have proved the convergence
(Jacot et al., 2018; Li and Liang, 2018; Du et al., 2019; Allen-Zhu et al., 2019b; Zou et al., 2018)
and generalization (Allen-Zhu et al., 2019a; Arora et al., 2019a,b; Cao and Gu, 2019) guarantees
in the so-called \neural tangent kernel" (NTK) regime, where the parameters stay close to the
initialization, and the neural network function is approximately linear in its parameters. A recent
line of works (Allen-Zhu and Li, 2019; Bai and Lee, 2019; Allen-Zhu and Li, 2020a,b,c; Li et al.,
2020; Cao et al., 2022; Zou et al., 2021; Wen and Li, 2021) studied the learning dynamic of neural
networks beyond the NTK regime. It is worthwhile to mention that our analysis of the MoE model
is also beyond the NTK regime.
3 Problem Setting and Preliminaries
We consider an MoE layer with each expert being a two-layer CNN trained by gradient descent
(GD) overnindependent training examples f(xi;yi)gn
i=1generated from a data distribution D. In
this section, we will rst introduce our data model D, and then explain our neural network model
and the details of the training algorithm.
3.1 Data distribution
We consider a binary classication problem over P-patch inputs, where each patch has ddimensions.
In particular, each labeled data is represented by ( x;y), where input x= (x(1);x(2);:::;x(P))2
(Rd)Pis a collection of Ppatches and y2f 1gis the data label. We consider data generated from
Kclusters. Each cluster k2[K] has a label signal vector vkand a cluster-center signal vector ck
withkvkk2=kckk2= 1. For simplicity, we assume that all the signals fvkgk2[K][fckgk2[K]are
orthogonal with each other.
Denition 3.1. A data pair ( x;y)2(Rd)Pf 1gis generated from the distribution Das follows.
â€¢Uniformly draw a pair ( k;k0) withk6=k0fromf1;:::;Kg.
â€¢Generate the label y2f 1guniformly, generate a Rademacher random variable 2f 1g.
â€¢Independently generate random variables ;; from distribution D;D;D. In this paper, we
assume there exists absolute constants C1;C2such that almost surely 0 <C 1;;C2.
â€¢Generate xas a collection of Ppatches: x= (x(1);x(2);:::;x(P))2(Rd)P, where
{ Feature signal. One and only one patch is given by yvk.
{ Cluster-center signal. One and only one patch is given by ck.
{ Feature noise. One and only one patch is given by vk0.
4

--- PAGE 5 ---
{ Random noise. The rest of the P 3 patches are Gaussian noises that are independently
drawn from N(0;(2
p=d)Id) wherepis an absolute constant.
How to learn this type of data? Since the positions of signals and noises are not specied
in Denition 3.1, it is natural to use the CNNs structure that applies the same function to each
patch. We point out that the strength of the feature noises could be as large as the strength of
the feature signals . As we will see later in Theorem 4.1, this classication problem is hard to
learn with a single expert, such as any two-layer CNNs (any activation function with any number
of neurons). However, such a classication problem has an intrinsic clustering structure that may
be utilized to achieve better performance. Examples can be divided into Kclusters[k2[K]
kbased
on the cluster-center signals: an example ( x;y)2
kif and only if at least one patch of xaligns
withck. It is not dicult to show that the binary classication sub-problem over 
 kcan be easily
solved by an individual expert. We expect the MoE can learn this data cluster structure from the
cluster-center signals.
Signicance of our result. Although this data can be learned by existing works on a mixture of
linear classiers with sophisticated algorithms (Anandkumar et al., 2012; Hsu et al., 2012; Chaganty
and Liang, 2013), the focus of our paper is training a mixture of nonlinear neural networks, a
more practical model used in real applications. When an MoE is trained by variants of gradient
descent, we show that the experts automatically learn to specialize on each cluster , while the router
automatically learns to dispatch the data to the experts according to their specialty . Although from
a representation point of view, it is not hard to see that the concept class can be represented by
MoEs, our result is very signicant as we prove that gradient descent from random initialization
can nd a good MoE with non-linear experts eciently. To make our results even more compelling,
we empirically show that MoE with linear experts, despite also being able to represent the concept
class, cannot be trained to nd a good classier eciently.
3.2 Structure of the MoE layer
An MoE layer consists of a set of M\expert networks" f1;:::;fM, and a gating network which is
generally set to be linear (Shazeer et al., 2017; Fedus et al., 2021). Denote by fm(x;W) the output
of them-th expert network with input xand parameter W. Dene an M-dimensional vector
h(x;) =P
p2[P]>x(p)as the output of the gating network parameterized by = [1;:::; M]2
RdM. The output Fof the MoE layer can be written as follows:
F(x;;W) =P
m2Txm(x;)fm(x;W);
whereTx[M] is a set of selected indices and m(x;)'s are route gate values given by
m(x;) =exp(hm(x;))PM
m0=1exp(hm0(x;));8m2[M]:
Expert Model. In practice, one often uses nonlinear neural networks as experts in the MoE layer.
In fact, we found that the non-linearity of the expert is essential for the success of the MoE layer
(see Section 6). For m-th expert, we consider a convolution neural network as follows:
fm(x;W) =P
j2[J]PP
p=1 
hwm;j;x(p)i
; (3.1)
5

--- PAGE 6 ---
where wm;j2Rdis the weight vector of the j-th lter (i.e., neuron) in the m-th expert, Jis
the number of lters (i.e., neurons). We denote Wm= [wm;1;:::;wm;J]2RdJas the weight
matrix of the m-th expert and further let W=fWmgm2[M]as the collection of expert weight
matrices. For nonlinear CNN, we consider the cubic activation function (z) =z3, which is one of
the simplest nonlinear activation functions (Vecci et al., 1998). We also include the experiment for
other activation functions such as RELU in Appendix Table 7.
Top-1 Routing Model. A simple choice of the selection set Txwould be the whole experts
setTx= [M] (Jordan and Jacobs, 1994), which is the case for the so-called soft-routing model.
However, it would be time consuming to use soft-routing in deep learning. In this paper, we consider
\switch routing", which is introduced by Fedus et al. (2021) to make the gating network sparse
and save the computation time. For each input x, instead of using all the experts, we only pick
one expert from [ M], i.e.,jTxj= 1. In particular, we choose Tx= argmaxmfhm(x;)g.
Figure 2: Illustration of an MoE layer. For each
input x, the router will only select one expert to per-
form computations. The choice is based on the output
of the gating network (dotted line). The expert layer
returns the output of the selected expert (gray box)
multiplied by the route gate value (softmax of the gat-
ing function output).Algorithm 1 Gradient descent with
random initialization
Require: Number of iterations T, ex-
pert learning rate , router learning
rater, initialization scale 0, train-
ing setS=f(xi;yi)gn
i=1.
1:Generate each entry of W(0)indepen-
dently from N(0;2
0).
2:Initialize each entry of (0)as zero.
3:fort= 0;2;:::;T 1do
4: Generate each entry of r(t)inde-
pendently from Unif[0,1].
5: Update W(t+1)as in (3.4).
6: Update (t+1)as in (3.5).
7:end for
8:return ((T);W(T)).
3.3 Training Algorithm
Given the training data S=f(xi;yi)gn
i=1, we train Fwith gradient descent to minimize the
following empirical loss function:
L(;W) =1
nPn
i=1` 
yiF(xi;;W)
; (3.2)
where`is the logistic loss dened as `(z) = log(1 + exp( z)). We initialize (0)to be zero and
initialize each entry of W(0)by i.i.dN(0;2
0). Zero initialization of the gating network is widely
used in MoE training. As discussed in Shazeer et al. (2017), it can help avoid out-of-memory errors
and initialize the network in a state of approximately equal expert load (see (5.1) for the denition
of expert load).
Instead of directly using the gradient of empirical loss (3.2) to update weights, we add pertur-
bation to the router and use the gradient of the perturbed empirical loss to update the weights.
In particular, the training example xiwill be distributed to argmaxmfhm(xi;(t)) +r(t)
m;iginstead,
wherefr(t)
m;igm2[M];i2[n]are random noises. Adding noise term is a widely used training strategy
for sparsely-gated MoE layer (Shazeer et al., 2017; Fedus et al., 2021), which can encourage explo-
6

--- PAGE 7 ---
ration across the experts and stabilize the MoE training. In this paper, we draw fr(t)
m;igm2[M];i2[n]
independently from the uniform distribution Unif[0 ;1] and denotes its collection as r(t). Therefore,
the perturbed empirical loss at iteration tcan be written as
L(t)((t);W(t)) =1
nPn
i=1` 
yimi;t(xi;(t))fmi;t(xi;W(t))
; (3.3)
wheremi;t= argmaxmfhm(xi;(t)) +r(t)
m;ig. Starting from the initialization W(0), the gradient
descent update rule for the experts is
W(t+1)
m =W(t)
m rWmL(t)((t);W(t))=krWmL(t)((t);W(t))kF;8m2[M]; (3.4)
where >0 is the expert learning rate. Starting from the initialization (0), the gradient update
rule for the gating network is
(t+1)
m =(t)
m rrmL(t)((t);W(t));8m2[M]; (3.5)
wherer>0 is the router learning rate. In practice, the experts are trained by Adam ( ?) to make
sure they have similar learning speeds. Here we use a normalized gradient which can be viewed as
a simpler alternative to Adam (Jelassi et al., 2021).
4 Main Results
In this section, we will present our main results. We rst provide a negative result for learning with
a single expert.
Theorem 4.1 (Single expert performs poorly) .SupposeD=Din Denition 3.1, then any
function with the form F(x) =PP
p=1f(x(p)) will get large test error P(x;y)D 
yF(x)0
1=8.
Theorem 4.1 indicates that if the feature noise has the same strength as the feature signal
i.e.,D=D, any two-layer CNNs with the form F(x) =P
j2[J]ajP
p2[P](w>
jx(p)+bj) can't
perform well on the classication problem dened in Denition 3.1 where can be any activation
function. Theorem 4.1 also shows that a simple ensemble of the experts may not improve the
performance because the ensemble of the two-layer CNNs is still in the form of the function dened
in Theorem 4.1.
As a comparison, the following theorem gives the learning guarantees for training an MoE layer
that follows the structure dened in Section 3.2 with cubic activation function.
Theorem 4.2 (Nonlinear MoE performs well) .Suppose the training data size n= 
(d). Choose
experts number M= (KlogKlog logd), lter size J= (logMlog logd), initialization scale
02[d 1=3;d 0:01], learning rate =eO(0);r= (M2). Then with probability at least 1  o(1),
Algorithm 1 is able to output ( (T);W(T)) withinT=eO( 1) iterations such that the non-linear
MoE dened in Section 3.2 satises
â€¢Training error is zero, i.e., yiF(xi;(T);W(T))>0;8i2[n].
â€¢Test error is nearly zero, i.e., P(x;y)D 
yF(x;(T);W(T))0
=o(1).
More importantly, the experts can be divided into a disjoint union of Knon-empty sets [ M] =
tk2[K]Mkand
7

--- PAGE 8 ---
â€¢(Each expert is good on one cluster) Each expert m2Mkperforms good on the cluster 
 k,
P(x;y)D(yfm(x;W(T))0j(x;y)2
k) =o(1).
â€¢(Router only distributes example to good expert) With probability at least 1  o(1), an example
x2
kwill be routed to one of the experts in Mk.
Theorem 4.2 shows that a non-linear MoE performs well on the classication problem in De-
nition 3.1. In addition, the router will learn the cluster structure and divide the problem into K
simpler sub-problems, each of which is associated with one cluster. In particular, each cluster will
be classied accurately by a subset of experts. On the other hand, each expert will perform well
on at least one cluster.
Furthermore, together with Theorem 4.1, Theorem 4.2 suggests that there exist problem in-
stances in Denition 3.1 (i.e., D=D) such that an MoE provably outperforms a single expert.
5 Overview of Key Techniques
A successful MoE layer needs to ensure that the router can learn the cluster-center features and
divide the complex problem in Denition 3.1 into simpler linear classication sub-problems that
individual experts can conquer. Finding such a gating network is dicult because this problem is
highly non-convex. In the following, we will introduce the main diculties in analyzing the MoE
layer and the corresponding key techniques to overcome those barriers.
Main Diculty 1: Discontinuities in Routing. Compared with the traditional soft-routing
model, the sparse routing model saves computation and greatly reduces the inference time. How-
ever, this form of sparsity also causes discontinuities in routing (Shazeer et al., 2017). In fact, even
a small perturbation of the gating network outputs h(x;) +may change the router behavior
drastically if the second largest gating network output is close to the largest gating network output.
Key Technique 1: Stability by Smoothing. We point out that the noise term added to the
gating network output ensures a smooth transition between dierent routing behavior, which makes
the router more stable. This is proved in the following lemma.
Lemma 5.1. Leth;bh2RMto be the output of the gating network and frmgM
m=1to be the noise
independently drawn from Unif[0,1]. Denote p;bp2RMto be the probability that experts get
routed, i.e., pm=P(argmaxm02[M]fhm0+rm0g=m),bpm=P(argmaxm02[M]fbhm0+rm0g=m).
Then we have that kp bpk1M2kh bhk1.
Lemma 5.1 implies that when the change of the gating network outputs at iteration tandt0
is small, i.e.,kh(x;(t)) h(x;(t0))k1, the router behavior will be similar. So adding noise
provides a smooth transition from time ttot0. It is also worth noting that is zero initialized. So
h(x;(0)) = 0 and thus each expert gets routed with the same probability pm= 1=Mby symmetric
property. Therefore, at the early of the training when kh(x;(t)) h(x;(0))k1is small, router
will almost uniformly pick one expert from [ M], which helps exploration across experts.
Main Diculty 2: No \Real" Expert. At the beginning of the training, the gating network
is zero, and the experts are randomly initialized. Thus it is hard for the router to learn the right
features because all the experts look the same: they share the same network architecture and are
trained by the same algorithm. The only dierence would be the initialization. Moreover, if the
router makes a mistake at the beginning of the training, the experts may amplify the mistake
because the experts will be trained based on mistakenly dispatched data.
8

--- PAGE 9 ---
Key Technique 2: Experts from Exploration. Motivated by the key technique 1, we intro-
duce an exploration stage to the analysis of MoE layer during which the router almost uniformly
picks one expert from [ M]. This stage starts at t= 0 and ends at T1=b 10:5
0cT=eO( 1)
and the gating network remains nearly unchanged kh(x;(t)) h(x;(0))k1=O(1:5
0). Be-
cause the experts are treated almost equally during exploration stage, we can show that the
experts become specialized to some specic task only based on the initialization. In particu-
lar, the experts set [ M] can be divided into Knonempty disjoint sets [ M] =tkMk, where
Mk:=fmjargmaxk02[K];j2[J]hvk0;w(0)
m;ji=kg. For nonlinear MoE with cubic activation func-
tion, the following lemma further shows that experts in dierent set Mkwill diverge at the end of
the exploration stage.
Lemma 5.2. Under the same condition as in Theorem 4.2, with probability at least 1  o(1), the
following equations hold for all expert m2Mk,
P(x;y)D 
yfm(x;W(T1)
0(x;y)2
k
=o(1);
P(x;y)D 
yfm(x;W(T1))0(x;y)2
k0
= 
 
1=K
;8k06=k:
Lemma 5.2 implies that, at the end of the exploration stage, the expert m2Mkcan achieve
nearly zero test error on the cluster 
 kbut high test error on the other clusters 
 k0;k06=k.
Main Diculty 3: Expert Load Imbalance. Given the training data set S=f(xi;yi)gn
i=1,
the load of expert mat iteratetis dened as
Load(t)
m=P
i2[n]P(mi;t=m); (5.1)
where P(mi;t=m) is probability that the input xibeing routed to expert mat iteration t. Eigen
et al. (2013) rst described the load imbalance issues in the training of the MoE layer. The gating
network may converge to a state where it always produces large Load(t)
mfor the same few experts.
This imbalance in expert load is self-reinforcing, as the favored experts are trained more rapidly
and thus are selected even more frequently by the router (Shazeer et al., 2017; Fedus et al., 2021).
Expert load imbalance issue not only causes memory and performance problems in practice, but
also impedes the theoretical analysis of the expert training.
Key Technique 3: Normalized Gradient Descent. Lemma 5.2 shows that the experts will
diverge intotk2[K]Mk. Normalized gradient descent can help dierent experts in the same Mk
being trained at the same speed regardless the imbalance load caused by the router. Because the
self-reinforcing circle no longer exists, we can prove that the router will treat dierent experts in the
sameMkalmost equally and dispatch almost the same amount of data to them (See Section E.2 in
Appendix for detail). This Load imbalance issue can be further avoided by adding load balancing
loss (Eigen et al., 2013; Shazeer et al., 2017; Fedus et al., 2021), or advanced MoE layer structure
such as BASE Layers (Lewis et al., 2021; Dua et al., 2021) and Hash Layers (Roller et al., 2021).
Road Map: Here we provide the road map of the proof of Theorem 4.2 and the full proof is
presented in Appendix E. The training process can be decomposed into several stages. The rst
stage is called Exploration stage . During this stage, the experts will diverge into Kprofessional
groupstK
k=1Mk= [M]. In particular, we will show that Mkis not empty for all k2[K]. Besides,
for allm2Mk,fmis a good classier over 
 k. The second stage is called router learning stage .
During this stage, the router will learn to dispatch x2
kto one of the experts in Mk. Finally,
we will give the generalization analysis for the MoEs from the previous two stages.
9

--- PAGE 10 ---
6 Experiments
Setting 1:2(0:5;2),2(1;2),2(0:5;3);p= 1
Test accuracy (%) Dispatch Entropy
Single (linear) 68 :71 NA
Single (nonlinear) 79 :48 NA
MoE (linear) 92 :992:11 1 :3000:044
MoE (nonlinear) 99:460:55 0:0980:087
Setting 2:2(0:5;2),2(1;2),2(0:5;3),p= 2
Test accuracy (%) Dispatch Entropy
Single (linear) 60 :59 NA
Single (nonlinear) 72 :29 NA
MoE (linear) 88 :481:96 1 :2940:036
MoE (nonlinear) 98:091:27 0:1710:103
Table 1: Comparison between MoE (linear) and MoE
(nonlinear) in our setting. We report results of top-1 gating
with noise for both linear and nonlinear models. Over ten
random experiments, we report the average value standard
deviation for both test accuracy and dispatch entropy.
/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013
/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000028/uni00000053/uni00000052/uni00000046/uni0000004b/uni00000056/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001b/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000015/uni00000014/uni00000011/uni00000017/uni00000028/uni00000051/uni00000057/uni00000055/uni00000052/uni00000053/uni0000005c/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000010/uni00000014
/uni00000031/uni00000052/uni00000051/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000010/uni00000014
/uni0000002f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000010/uni00000015
/uni00000031/uni00000052/uni00000051/uni0000004f/uni0000004c/uni00000051/uni00000048/uni00000044/uni00000055/uni00000010/uni00000015Figure 3: Illustration of router
dispatch entropy. We demon-
strate the change of entropy of
MoE during training on the syn-
thetic data. MoE (linear)-1 and
MoE (nonlinear)-1 refer to Setting
1 in Table 1. MoE (linear)-2 and
MoE (nonlinear)-2 refer to Setting
2 in Table 1.
6.1 Synthetic-data Experiments
Datasets. We generate 16 ;000 training examples and 16 ;000 test examples from the data distri-
bution dened in Denition 3.1 with cluster number K= 4 , patch number P= 4 and dimension
d= 50. We randomly shue the order of the patches of xafter we generate data ( x;y). We
consider two parameter settings: 1. Uniform(0:5;2),Uniform(1;2),Uniform(0:5;3)
andp= 1; 2.Uniform(0:5;2),Uniform(1;2),Uniform(0:5;3) andp= 2. Note that
Theorem 4.1 shows that when andfollow the same distribution, neither single linear expert or
single nonlinear expert can give good performance. Here we consider a more general and dicult
setting when andare from dierent distributions.
Models. We consider the performances of single linear CNN, single nonlinear CNN, linear MoE,
and nonlinear MoE. The single nonlinear CNN architecture follows (3.1) with cubic activation
function, while single linear CNN follows (3.1) with identity activation function. For both linear
and nonlinear MoEs, we consider a mixture of 8 experts with each expert being a single linear
CNN or a single nonlinear CNN. Finally, we train single models with gradient descent and train
the MoEs with Algorithm 1. We run 10 random experiments and report the average accuracy with
standard deviation.
Evaluation. To evaluate how well the router learned the underlying cluster structure of the data,
we dene the entropy of the router's dispatch as follows. Denote by nk;mthe number of data in
clusterKthat are dispatched to expert m. The total number of data dispatched to expert mis
nm=PK
k=1nk;mand the total number of data is n=PK
k=1PM
m=1nk;m. The dispatch entropy is
10

--- PAGE 11 ---
CIFAR-10 (%) CIFAR-10-Rotate (%)
CNNSingle 80 :680:45 76 :781:79
MoE 80 :310:62 79:601:25
MobileNetV2Single 92 :450:25 85 :762:91
MoE 92 :230:72 89:852:54
ResNet18Single 95 :510:31 88 :230:96
MoE 95 :320:68 92:602:01
Table 2: Comparison between MoE and single model on CIFAR-10 and CIFAR-10-Rotate datasets.
We report the average test accuracy over 10 random experiments the standard deviation.
then dened as
entropy = PM
m=1;nm6=0nm
nPK
k=1nk;m
nmlog nk;m
nm
: (6.1)
When each expert receives the data from at most one cluster, the dispatch entropy will be zero.
And a uniform dispatch will result in the maximum dispatch entropy.
As shown in Table 1, the linear MoE does not perform as well as the nonlinear MoE in Setting 1,
with around 6% less test accuracy and much higher variance. With stronger random noise (Setting
2), the dierence between the nonlinear MoE and linear MoE becomes even more signicant. We
also observe that the nal dispatch entropy of nonlinear MoE is nearly zero while that of the
linear MoE is large. In Figure 3, we further demonstrate the change of dispatch entropy during
the training process. The dispatch entropy of nonlinear MoE signicantly decreases, while that of
linear MoE remains large. Such a phenomenon indicates that the nonlinear MoE can successfully
learn the underlying cluster structure of the data while the linear MoE fails to do so.
6.2 Real-data Experiments
We further conduct experiments on real image datasets and demonstrate the importance of the
clustering data structure to the MoE layer in deep neural networks.
Datasets. We consider the CIFAR-10 dataset (Krizhevsky, 2009) and the 10-class classication
task. Furthermore, we create a CIFAR-10-Rotate dataset that has a strong underlying cluster
structure that is independent of its labeling function. Specically, we rotate the images by 30
degrees and merge the rotated dataset with the original one. The task is to predict if the image is
rotated, which is a binary classication problem. We deem that some of the classes in CIFAR-10
form underlying clusters in CIFAR-10-Rotate. In Appendix A, we explain in detail how we generate
CIFAR-10-Rotate and present some specic examples.
Models. For the MoE, we consider a mixture of 4 experts with a linear gating network. For the
expert/single model architectures, we consider a CNN with 2 convolutional layers (architecture de-
tails are illustrated in Appendix A.) For a more thorough evaluation, we also consider expert/single
models with architecture including MobileNetV2 (Sandler et al., 2018) and ResNet18 (He et al.,
2016). The training process of MoE also follows Algorithm 1.
The experiment results are shown in Table 2, where we compare single and mixture models
of dierent architectures over CIFAR-10 and CIFAR-10-Rotate datasets. We observe that the
improvement of MoEs over single models diers largely on the dierent datasets. On CIFAR-10,
11

--- PAGE 12 ---
the performance of MoEs is very close to the single models. However, on the CIFAR-10-Rotate
dataset, we can observe a signicant performance improvement from single models to MoEs. Such
results indicate the advantage of MoE over single models depends on the task and the cluster
structure of the data.
7 Conclusion and Future Work
In this work, we formally study the mechanism of the Mixture of Experts (MoE) layer for deep
learning. To our knowledge, we provide the rst theoretical result toward understanding how the
MoE layer works in deep learning. Our empirical evidence reveals that the cluster structure of
the data plays an important role in the success of the MoE layer. Motivated by these empirical
observations, we study a data distribution with cluster structure and show that Mixture-of-Experts
provably improves the test accuracy of a single expert of two-layer CNNs.
There are several important future directions. First, our current results are for CNNs. It
is interesting to extend our results to other neural network architectures, such as transformers.
Second, our data distribution is motivated by the classication problem of image data. We plan to
extend our analysis to other types of data (e.g., natural language data).
A Experiment Details
A.1 Visualization
In the visualization of Figure 1, MoE (linear) and MoE (nonlinear) are trained according to Algo-
rithm 1 by normalized gradient descent with learning rate 0 :001 and gradient descent with learning
rate 0:1. According to Denition 3.1, we set K= 4,P= 4 andd= 50 and choose 2(0:5;2),
2(1;2),2(1;2) andp= 1, and generate 3 ;200 data examples. We consider mixture of M= 4
experts for both MoE (linear) and MoE (nonlinear). For each expert, we set the number of neu-
rons/lters J= 16. We train MoEs on 1 ;600 data examples and visualize classication result and
decision boundary on the remaining 1 ;600 examples. The data examples are visualized via t-SNE
(Van der Maaten and Hinton, 2008). When visualizing the data points and decision boundary on
the 2d space, we increase the magnitude of random noise patch by 3 so that the positive/negative
examples and decision boundaries can be better viewed.
A.2 Synthetic-data Experiments
Synthetic-data experiment setup. For the experiments on synthetic data, we generate the data
according to Denition 3.1 with K= 4,P= 4 andd= 50. We consider four parameter settings:
â€¢Uniform(0:5;2),Uniform(1;2),Uniform(0:5;3) andp= 1;
â€¢Uniform(0:5;2),Uniform(1;2),Uniform(0:5;3) andp= 2;
â€¢Uniform(0:5;2),Uniform(1;2),Uniform(0:5;2) andp= 1;
â€¢Uniform(0:5;2),Uniform(1;2),Uniform(0:5;2) andp= 2.
We consider mixture of M= 8 experts for all MoEs and J= 16 neurons/lters for all experts. For
single models, we consider J= 128 neurons/lters. We train MoEs using Algorithm 1. Specically,
12

--- PAGE 13 ---
Setting 1:2(0:5;2),2(1;2),2(0:5;3);p= 1
Test accuracy (%) Dispatch Entropy Number of Filters
Single (linear) 68 :71 NA 128
Single (linear) 67 :63 NA 512
Single (nonlinear) 79 :48 NA 128
Single (nonlinear) 78 :18 NA 512
MoE (linear) 92 :992:11 1 :3000:044 128 (16*8)
MoE (nonlinear) 99:460:55 0:0980:087 128 (16*8)
Setting 2:2(0:5;2),2(1;2),2(0:5;3),p= 2
Test accuracy (%) Dispatch Entropy Number of Filters
Single (linear) 60 :59 NA 128
Single (linear) 63 :04 NA 512
Single (nonlinear) 72 :29 NA 128
Single (nonlinear) 52 :09 NA 512
MoE (linear) 88 :481:96 1 :2940:036 128 (16*8)
MoE (nonlinear) 98:091:27 0:1710:103 128 (16*8)
Setting 3:2(0:5;2),2(1;2),2(0:5;2);p= 1
Test accuracy (%) Dispatch Entropy Number of Filters
Single (linear) 74 :81 NA 128
Single (linear) 74 :54 NA 512
Single (nonlinear) 72 :69 NA 128
Single (nonlinear) 67 :78 NA 512
MoE (linear) 95 :931:34 1 :1600:100 128 (16*8)
MoE (nonlinear) 99:990:02 0:0080:011 128 (16*8)
Setting 4:2(0:5;2),2(1;2),2(0:5;2),p= 2
Test accuracy (%) Dispatch Entropy Number of Filters
Single (linear) 74 :63 NA 128
Single (linear) 72 :98 NA 512
Single (nonlinear) 68 :60 NA 128
Single (nonlinear) 61 :65 NA 512
MoE (linear) 93 :301:48 1 :1600:155 128 (16*8)
MoE (nonlinear) 98:921:18 0:0890:120 128 (16*8)
Table 3: Comparison between MoE (linear) and MoE (nonlinear) in our setting. We
report results of top-1 gating with noise for both linear and nonlinear models. Over ten random
experiments, we report the average value standard deviation for both test accuracy and dispatch
entropy.
13

--- PAGE 14 ---
Expert number 1 2 3 4 5 6 7 8
Initial dispatch 1921 2032 1963 1969 2075 1980 2027 2033
Final dispatch 0 3979 4009 0 0 3971 0 4041
Cluster 1 0 0 0 0 0 3971 0 0
Cluster 2 0 0 4009 0 0 0 0 0
Cluster 3 0 0 0 0 0 0 0 4041
Cluster 4 0 3979 0 0 0 0 0 0
Table 4: Dispatch details of MoE (nonlinear) with test accuracy 100%.
we train the experts by normalized gradient descent with learning rate 0 :001 and the gating network
by gradient descent with learning rate 0 :1. We train single linear/nonlinear models by Adam ( ?)
to achieve the best performance, with learning rate 0 :01 and weight decay 5e-4 for single nonlinear
model and learning rate 0 :003 and weight decay 5 e 4 for single linear model.
Synthetic-data experiment results. In Table 3, we present the empirical results of single linear
CNN, single nonlinear CNN, linear MoE, and nonlinear MoE under settings 3 and 4, where and
follow the same distribution as we assumed in theoretical analysis. Furthermore, we report the
total number of lters for both single CNNs and a mixture of CNNs, where the lter size (equal
to 50) is the same for all single models and experts. For linear and nonlinear MoE, there are 16
lters for each of the 8 experts, and therefore 128 lters in total. Note that in the synthetic-data
experiment in the main paper, we let the number of lters of single models be the same as MoEs
(128). Here, we additionally report the performances of single models with 512 lters, and see
if increasing the model size of single models can beat MoE. From Table 3, we observe that: 1.
single models perform poorly in all settings; 2. linear MoEs do not perform as well as nonlinear
MoEs. Specically, the nal dispatch entropy of nonlinear MoEs is nearly zero while the dispatch
entropy of linear MoEs is consistently larger under settings 1-4. This indicates that nonlinear MoEs
successfully uncover the underlying cluster structure while linear MoEs fail to do so. In addition,
we can see that even larger single models cannot beat linear MoEs or nonlinear MoEs. This is
consistent with Theorem 4 :1, where a single model fails under such data distribution regardless of
its model size. Notably, by comparing the results in Table 1 and Table 3, we can see that a single
nonlinear model suers from overtting as we increase the number of lters.
Router dispatch examples. We demonstrate specic examples of router dispatch for MoE
(nonlinear) and MoE (linear). The examples of initial and nal router dispatch for MoE (nonlinear)
are shown in Table 4 and Table 5. Under the dispatch for nonlinear MoE, each expert is given
either no data or data that comes from one cluster only. The entropy of such dispatch is thus 0.
The test accuracy of MoE trained under such a dispatch is either 100% or very close to 100%, as
the expert can be easily trained on the data from one cluster only. An example of the nal dispatch
for MoE (linear) is shown in Table 6, where clusters are not well separated and an expert gets data
from dierent clusters. The test accuracy under such dispatch is lower (90 :61%).
MoE during training. We further provide gures that illustrate the growth of the inner products
between expert/router weights and feature/center signals during training. Specically, since each
expert has multiple neurons, we plot the max absolute value of the inner product over the neurons of
each expert. In Figure 4, we demonstrate the training process of MoE (nonlinear), and in Figure 5,
we demonstrate the training process of MoE (linear). The data is the same as setting 1 in Table 1,
14

--- PAGE 15 ---
Expert number 1 2 3 4 5 6 7 8
Initial dispatch 1978 2028 2018 1968 2000 2046 2000 1962
Final dispatch 3987 4 3975 6 0 1308 4009 2711
Cluster 1 0 0 3971 0 0 0 0 0
Cluster 2 0 0 0 0 0 4 4005 0
Cluster 3 8 4 4 6 0 1304 4 2711
Cluster 4 3979 0 0 0 0 0 0 0
Table 5: Dispatch details of MoE (nonlinear) with test accuracy 99 :95%.
Expert number 1 2 3 4 5 6 7 8
Initial dispatch 1969 2037 1983 2007 1949 1905 2053 2097
Final dispatch 136 2708 6969 5311 27 87 4 758
Cluster 1 0 630 1629 1298 27 87 4 296
Cluster 2 136 1107 1884 651 0 0 0 231
Cluster 3 0 594 1976 1471 0 0 0 0
Cluster 4 0 377 1480 1891 0 0 0 231
Table 6: Dispatch details of MoE (linear) with test accuracy 90 :61%.
with2(0:5;2),2(1;2),2(0:5;3) andp= 1. We can observe that, in the top left sub-
gure of Figure 4 for MoE (nonlinear), the max inner products between expert weight and feature
signals exhibit a property that each expert picks up one feature signal quickly. Similarly, as shown
in the bottom right sub-gure, the router picks up the corresponding center signal. Meanwhile,
the nonlinear experts almost do not learn center signals and the magnitude of the inner products
between router weight and feature signals remain small. However, for MoE (linear), as shown in
the top two sub-gures of Figure 5, an expert does not learn a specic feature signal, but instead
learns multiple feature and center signals. Moreover, as demonstrated in the bottom sub-gures
of Figure 5, the magnitude of the inner products between router weight and feature signals can be
even larger than the inner products between router weight and center signals.
Verication of Theorem 4.1. In Table 7, we provide the performances of single models with
dierent activation functions under setting 3, where ;2(1;2) follow the same distribution. In
Table 8, we further report the performances of single models with dierent activation functions
under setting 1 and setting 2. Empirically, even when anddo not share the same distribution,
single models still fail. Note that, for Tables 7 and 8, the numbers of lters for single models are
128.
Load balancing loss. In Table 9, we present the results of linear MoE with load balancing loss
and directly compare it with nonlinear MoE without load balancing loss. Load balancing loss
guarantees that the experts receive similar amount of data and prevents MoE from activating only
one or few experts. However, on the data distribution that we study, load balancing loss is not the
key to the success of MoE: the single experts cannot perform well on the entire data distribution
and must diverge to learn dierent labeling functions with respect to each cluster.
15

--- PAGE 16 ---
Inner product between expert weight and feature signal Inner product between expert weight and center signal
Inner product between router weight and feature signal Inner product between router weight and center signalFigure 4: Mixture of nonlinear experts. Growth of inner product between expert/router weight
and center/feature vector.
Activation Optimal Accuracy (%) Test Accuracy (%)
Linear 87 :50% 74 :81%
Cubic 87 :50% 72 :69%
Relu 87 :50% 73 :45%
Celu 87 :50% 76 :91%
Gelu 87 :50% 74 :01%
Tanh 87 :50% 74 :76%
Table 7: Verication of Theorem 4.1 (single expert performs poorly) . Test accuracy of
single linear/nonlinear models with dierent activation functions. Data is generated according to
Denition 3.1 with ;2(1;2),2(1;2) andp= 1.
16

--- PAGE 17 ---
Inner product between expert weight and feature signal Inner product between expert weight and center signal
Inner product between router weight and feature signal Inner product between router weight and center signal
Figure 5: Mixture of linear experts. Growth of inner product between expert/router weight
and center/feature vector.
Activation Setting 1 Setting 2
Linear 68 :71% 60:59%
Cubic 79 :48% 72:29%
Relu 72 :28% 80:12%
Celu 81 :75% 78:99%
Gelu 79 :04% 82:01%
Tanh 81 :72% 81:03%
Table 8: Single expert performs poorly (setting 1&2). Test accuracy of single lin-
ear/nonlinear models with dierent activation functions. Data is generated according to De-
nition 3.1 with 2(0:5;2),2(1;2),2(0:5;3);p= 1 for setting 1. And we have 2(0:5;2),
2(1;2),2(0:5;3);p= 1 for setting 2.
17

--- PAGE 18 ---
Linear MoE with Load Balancing Nonlinear MoE without Load Balancing
Setting 1 93 :811:02 99:460:55
Setting 2 89 :202:20 98:091:27
Setting 3 95 :120:58 99:990:02
Setting 4 92 :501:55 98:921:18
Table 9: Load balancing loss. We report the results for linear MoE with load balancing loss and
compare them with our previous results on nonlinear MoE without load balancing loss. Over ten
random experiments, we report the average test accuracy (%) standard deviation. Setting 1-4
follows the data distribution introduced above.
A.3 Experiments on Image Data
Rotation Crop Resize Gaussian Blur
Figure 6: Examples of the CIFAR-10-Rotate dataset. Both the original image and the
rotated image are processed in the same way, where we crop the image to (24 ;24), resize to (32 ;32)
and apply random Gaussian blur.
Datasets. We consider CIFAR-10 (Krizhevsky, 2009) with the 10-class classication task, which
contains 50 ;000 training examples and 10 ;000 testing examples. For CIFAR-10-Rotate, we design
a binary classication task by copying and rotating all images by 30 degree and let the model
predict if an image is rotated. In Figure 6, we demonstrate the positive and negative examples
of CIFAR-10-Rotate. Specically, we crop the rotated images to (24 ;24), and resize to (32 ;32)
for model architectures that are designed on image size (32 ;32). And we further apply random
Gaussian noise to all images to avoid the models taking advantage of image resolutions.
Models. For the simple CNN model, we consider CNN with 2 convolutional layers, both with
kernel size 3 and ReLU activation followed by max pooling with size 2 and a fully connected layer.
The number of lters of each convolutional layer is respectively 64, 128.
CIFAR-10 Setup. For real-data experiments on CIFAR-10, we apply the commonly used trans-
forms on CIFAR-10 before each forward pass: random horizontal ips and random crops (padding
the images on all sides with 4 pixels and randomly cropping to (32 ;32)). And as conventionally,
18

--- PAGE 19 ---
we normalize the data by channel. We train the single CNN model with SGD of learning rate 0 :01,
momentum 0 :9 and weight decay 5e-4. And we train single MobileNetV2 and single ResNet18 with
SGD of learning rate 0 :1, momentum 0 :9 and weight decay 5e-4 to achieve the best performances.
We train MoEs according to Algorithm 1, with normalized gradient descent on the experts and
SGD on the gating networks. Specically, for MoE (ResNet18) and MoE (MobileNetV2), we use
normalized gradient descent of learning rate 0 :1 and SGD of learning rate 1e-4, both with mo-
mentum 0:9 and weight decay of 5e-4. For MoE (CNN), we use normalized gradient descent of
learning rate 0 :01 and SGD of learning rate 1e-4, both with momentum 0 :9 and weight decay of
5e-4. We consider top-1 gating with noise and load balancing loss for MoE on both datasets, where
the multiplicative coecient of load balancing loss is set at 1e-3. All models are trained for 200
epochs to achieve convergence.
CIFAR-10-Rotate Setup. For experiments on CIFAR10-Rotate, the data is normalized by
channel as the same as in CIFAR-10 before each forward pass. We train the single CNN, single
MobileNetV2 and single ResNet18 by SGD with learning rate 0 :01, momentum 0 :9 and weight
decay 5e-4 to achieve the best performances. And we train MoEs by Algorithm 1 with normalized
gradient descent learning rate 0 :01 on the experts and with SGD of learning rate 1e-4 on the gating
networks, both with momentum 0 :9 and weight decay of 5e-4. We consider top-1 gating with noise
and load balancing loss for MoE on both datasets, where the multiplicative coecient for load
balancing loss is set at 1e-3. All models are trained for 50 epochs to achieve convergence.
Visualization. In Figure 7, we visualize the latent embedding learned by MoEs (ResNet18) for
the 10-class classication task in CIFAR-10 as well as the binary classication task in CIFAR-10-
Rotate. We visualize the data with the same label yto see if cluster structures exist within each
class. For CIFAR-10, we choose y= 1 ("car"), and plot the latent embedding of data with y= 1
using t-SNE on the left subgure, which does not show an salient cluster structure. For CIFAR-
10-Rotate, we choose y= 1 ("rotated") and visualize the data with y= 1 in the middle subgure.
Here, we can observe a clear clustering structure even though the class signal is not provided during
training. We take a step further to investigate what is in each cluster in the right subgure. We
can observe that most of the examples in the \frog" class fall into one cluster, while examples of
\ship" class mostly fall into the other cluster.
y=1 (car) y=1 (rotated) (frog, ship)
Figure 7: Visualization of the latent embedding on CIFAR-10 and CIFAR-10-Rotate with xed
labely. The left gure denotes the visualization of CIFAR-10 when label yis xed to be 1 (car).
The central gure represents the visualization of CIFAR-10-Rotate when label yis xed to be 1
(rotated). On the right gure, red denotes that the data is from the ship class, and blue denotes
that the data is from the frog class.
19

--- PAGE 20 ---
Single MoE
Accuracy 74 :13% 76:22%
Table 10: The test accuracy of the single classier vs. MoE classier.
Expert 1 Expert 2 Expert 3 Expert 4
English 1 ;374 3;745 2;999 31;882
French 23;470 3;335 13;182 13
Russian 833 9;405 7;723 39
Table 11: The nal router dispatch details with regard to the linguistic source of the test data.
1.0
 0.5
 0.0 0.5 1.00.6
0.4
0.2
0.00.20.40.60.8
Figure 8: The distribution of text embedding of the multilingual sentiment analysis dataset. The
embedding is generated by the pre-trained BERT multilingual base model and visualized on 2d
space using t-SNE. Each color denotes a linguistic source, including English, French, and Russian.
A.4 Experiments on Language Data
Here we provide a simple example of how MoE would work for multilingual tasks. We gather
multilingual sentiment analysis data from the source of English (Sentiment140 (Go et al., 2009))
which is randomly sub-sampled to 200 ;000 examples, Russian (RuReviews (Smetanin and Komarov,
2019)) which contains 90 ;000 examples, and French (Blard, 2020) which contains 200 ;000 examples.
We randomly split the dataset into 80% training data and 20% test data. We use a pre-trained
BERT multilingual base model (Devlin et al., 2018) to generate text embedding for each text
and train 1-layer neural network with cubic activation as the single model. For MoE, we still let
M= 4 with each expert sharing the same architecture as the single model. In Figure 8, we show
the visualization of the text embeddings in the 2d space via t-SNE, where each color denotes a
linguistic source, with representing a positive example and representing a negative example.
Data from dierent linguistic sources naturally form dierent clusters. And within each cluster,
positive and negative data exist.
In Table 10, we demonstrate the test accuracy of a single classier and MoE on the multilingual
sentiment analysis dataset. And in Table 11, we show the nal router dispatch details of MoE to
20

--- PAGE 21 ---
each expert with regard to the linguistic source of the text. Notably, MoE learned to distribute
examples largely according to the original language.
B Proof of Theorem 4.1
Because we are using CNNs as experts, dierent ordering of the patches won't aect the value of
F(x). So for ( x;y) drawn fromDin Denition 3.1, we can assume that the rst patch x(1)is feature
signal, the second patch x(2)is cluster-center signal, the third patch x(3)is feature noise. The other
patches x(p);p4 are random noises. Therefore, we can rewrite x= [yvk;ck;vk0;], where
= [4;:::; P] is a Gaussian matrix of size Rd(P 3).
Proof of Theorem 4.1. Conditioned on the event that y= , points ([yvk;ck; yvk0;];y), 
[ yvk;ck;yvk0;]; y
, 
[yvk0;ck0; yvk;];y
, 
[ yvk0;ck0;yvk;]; y
follow the
same distribution because andfollow the same distribution, and yand yfollow the same
distribution. Therefore, we have
4P 
yF(x)0j= y
=E
1(yF([yvk;ck; yvk0;])0)| {z }
I1+1( yF([ yvk;ck;yvk0;])0)| {z }
I2
+1(yF([yvk0;ck0; yvk;])0)| {z }
I3+1( yF([ yvk0;ck0;yvk;])0)
| {z }
I4:
It is easy to verify the following fact

yF([yvk;ck; yvk0;])
+
 yF([ yvk;ck;yvk0;])
+
yF([yvk0;ck0; yvk;])
+
 yF([ yvk0;ck0;yvk;])
=
yf(yvk) +yf(ck) +yf( yvk0) +PX
p=4yf(p)
+
 yf( yvk) yf(ck) yf(yvk0) PX
p=4yf(p)
+
yf(yvk0) +yf(ck0) +yf( yvk) +PX
p=4yf(p)
+
 yf( yvk0) yf(ck0) yf(yvk) PX
p=4yf(p)
= 0:
By pigeonhole principle, at least one of I1;I2;I3;I4is non-zero. This further implies that 4 P 
yF(x)
0j= y
1. Applying P(= y) = 1=2, we have that
P 
yF(x)0
P 
yF(x)0)j= y
P(= y)1=8;
21

--- PAGE 22 ---
which completes the proof.
C Smoothed Router
In this section, we will show that the noise term provides a smooth transition between dierent
routing behavior. All the results in this section is independent from our NN structure and its
initialization. We rst present a general version of Lemma 5.1 with its proof.
Lemma C.1 (Extension of Lemma 5.1) .Leth;bh2RMto be the output of the gating network and
frmgM
m=1to be the noise independently drawn from Dr. Denote p;bp2RMto be the probability
that experts get routed, i.e., pm=P(argmaxm02[M]fhm0+rm0g=m),bpm=P(argmaxm02[M]fbhm0+
rm0g=m). Suppose the probability density function of Dris bounded by , Then we have that
kp bpk1(M2)kh bhk1.
Proof. Given random variable frmgM
m=1, let us rst consider the event that argmaxmfhm+rmg6=
argmaxmfbhm+rmg. Letm1= argmaxmfhm+rmgandm2= argmaxmfbhm+rmg, then we have
that
hm1+rm1hm2+rm2;bhm2+rm2bhm1+rm1;
which implies that
bhm2 bhm1rm1 rm2hm2 hm1: (C.1)
DeneC(m1;m2) = (bhm2 bhm1+hm2 hm1)=2, then (C.1) implies that
jrm1 rm2 C(m1;m2)jjbhm2 bhm1 hm2+hm1j=2kbh hk1: (C.2)
Therefore, we have that,
P(argmax
mfhm+rmg6= argmax
mfbhm+rmg)
P(9m16=m22[M];s.t.jrm1 rm2 C(m1;m2)jkbh hk1)
X
m1<m 2P 
jrm1 rm2 C(m1;m2)jkbh hk1
=X
m1<m 2Eh
P 
rm2+C(m1;m2) kbh hk1rm1rm2+C(m1;m2) +kbh hk1rm2i
(M2)kbh hk1;
where the rst inequality is by (C.2), the second inequality is by union bound and the last inequality
is due to the fact that the probability density function of rm1is bounded by . Then we have that
fori2[M],
jpi bpijE
1 
argmax
mfbhm+rmg=i
 1 
argmax
mfhm+rmg=i
E1 
argmax
mfbhm+rmg=i
 1 
argmax
mfhm+rmg=i
22

--- PAGE 23 ---
P 
argmax
mfbhm+rmg6= argmax
mfhm+rmg
(M2)kbh hk1;
which completes the proof.
Remark C.2. A widely used choice of Drin Lemma C.1 is uniform noise Unif[a, b], in which
case the density function can be upper bounded by 1 =(b a). Another widely used choice of Dris
Gaussian noiseN(0;2
r), in which case the density function can be upper bounded by 1 =(rp
2).
Increase the range of uniform noise or increase the variance of the Gaussian noise will result in
a smaller density function upper bound and a smoother behavior of routing. In our paper, we
consider unif[0,1] for simplicity, in which case the the density function can be upper bounded by 1
(= 1).
The following Lemma shows that when two gate network outputs are close, the router will
distribute the examples to those corresponding experts with nearly the same probability.
Lemma C.3. Leth2RMbe the output of the gating network and frmgM
m=1be the noise in-
dependently drawn from Unif[0,1]. Denote the probability that experts get routed by p, i.e.,
pm=P(argmaxm0fhm0+rm0g=m). Then we have that
jpm pm0jM2jhm hm0j:
Proof. Constructbhas copy of hand permute its m;m0-th element. Denote the corresponding
probability vector as bp. Then it is obviously that jpm pm0j=kp bpk1andjhm hm0j=kbh hk1.
Applying Lemma 5.1 completes the proof.
The following lemma shows that the router won't route examples to the experts with small
gating network outputs, which saves computation and improves the performance.
Lemma C.4. Suppose the noise frmgM
m=1are independently drawn from Unif[0,1] and hm(x;)
maxm0hm0(x;) 1, example xwill not get routed to expert m.
Proof. Becausehm(x;)maxm0hm0(x;) 1 implies that for any Uniform noise frm0gm02[M]
we have that
hm(x;) +rmmax
m0hm0(x;)max
m0fhm0(x;) +rm0g;
where the rst inequality is by rm1, the second inequality is by rm00;8m02[M].
D Initialization of the Model
Before we look into the detailed proof of Theorem 4.2, let us rst discuss some basic properties of
the data distribution and our MoE model. For simplicity of notation, we simplify ( xi;yi)2
kas
i2
k.
Training Data Set Property. Because we are using CNNs as experts, dierent ordering of the
patches won't aect the value of F(x). So for ( x;y) drawn fromDin Denition 3.1, we can assume
that the rst patch x(1)is feature signal, the second patch x(2)is cluster-center signal, the third
patch x(3)is feature noise. The other patches x(p);p4 are random noises. Therefore, we can
23

--- PAGE 24 ---
rewrite x= [yvk;ck;vk0;], where = [4;:::; P] is a Gaussian matrix of size Rd(P 3).
According to the type of the feature noise, we further divide 
 kinto 
k=[
k;k0based on the
feature noise, i.e. x2
k;k0ifx= [yvk;ck;vk0;]. To better characterize the router training,
we need to break down 
 k;k0into 
+
k;k0and 
 
k;k0. Denote by 
+
k;k0the set thatfyi=iji2
k;k0g,
by 
 
k;k0the set thatfyi= iji2
k;k0g.
Lemma D.1. With probability at least 1  , the following properties hold for all k2[K],
X
i2
kyi3
i=eO(pn);X
i2
k3
i=E[3]n=K +eO(pn);X
i2
kyii3
i=eO(pn); (D.1)
X
i2
+
k;k0yii=eO(pn);X
i2
 
k;k0yii=eO(pn);X
i2
+
k;k0ii=eO(pn); (D.2)
X
i2
 
k;k0ii=eO(pn);X
i2
ki=E[]n=K +eO(pn): (D.3)
Proof. Fixk2[K], by Hoeding's inequality we have that with probability at least 1  =8K,
X
i2
kyi3
i=nX
i=1yi3
i1 
(xi;yi)2
k
=eO(pn);
where the last equality is by the fact that the expectation of y31 
(x;y)2
k
is zero. Fix k2[K],
by Hoeding's inequality we have that with probability at least 1  =8K,
X
i2
k3
i=nX
i=13
i1 
(xi;yi)2
k
=nE[3]
K+eO(pn);
where the last equality is by the fact that the expectation of 31 
(x;y)2
k
isE[3]=K. Fix
k2[K], by Hoeding's inequality we have that with probability at least 1  =8K,
X
i2
kyii3
i=nX
i=1yii3
i1 
(xi;yi)2
k
=eO(pn);
where the last equality is by the fact that the expectation of y31 
(x;y)2
k
is zero. Now we
have proved the bounds in (D.1). We can get other bounds in (D.2) and (D.3) similarly. Applying
union bound over [ K] completes the proof.
Lemma D.2. Suppose that d= 
(log(4nP= )), with probability at least 1  , the following
inequalities hold for all i2[n];k2[K];p4,
â€¢ki;pk2=O(1),
â€¢hvk;i;pieO(d 1=2),hck;i;pieO(d 1=2),hi;p;i0;p0ieO(d 1=2),8(i0;p0)6= (i;p).
24

--- PAGE 25 ---
Proof of Lemma D.2. By Bernstein's inequality, with probability at least 1  =(2nP) we have
ki;pk2
2 2
pO(2
pp
d 1log(4nP= )):
Therefore, as long as d= 
(log(4nP= )), we haveki;pk2
22. Moreover, clearly hi;p;i0;p0ihas
mean zero,8(i;p)6= (i0;p0). Then by Bernstein's inequality, with probability at least 1  =(6n2P2)
we have
jhi;p;i0;p0ij22
pp
d 1log(12n2P2=):
Similarly,hvk;i;piandhck;i;pihave mean zero. Then by Bernstein's inequality, with probability
at least 1 =(3nPK ) we have
jhi;p;vkij2pp
d 1log(6nPK= );jhi;p;ckij2pp
d 1log(6nPK= ):
Applying a union bound completes the proof.
MoE Initialization Property.
We divide the experts into Ksets based on the initialization.
Denition D.3. Fix expertm2[M], denote (k
m;j
m) = argmaxj;khvk;w(0)
m;ji. Fix cluster k2[K],
denote the profession experts set as Mk=fmjk
m=kg.
Lemma D.4. ForM(Klog(K=)),J(log(M=)), the following inequalities hold with
probability at least 1  .
â€¢max (j;k)6=(jm;km)hw(0)
m;j;vki 
1 = 
3MJ2K2)
hw(0)
m;jm;vkmifor allm2[M]
â€¢hw(0)
m;jm;vkmi0:010for allm2[M].
â€¢jMkj1 for allk2[K].
Proof. Recall that wm;jN(0;2
0Id). Notice that signals v1;:::;vKare orthogonal. Given xed
m2[M], we have thatfhw(0)
m;j;vkijj2[J];k2[K]gare independent and individually draw from
N(0;2
0) we have that
P(hw(0)
m;j;vki<0:010)<0:9:
Therefore, we have that
P(max
j;khw(0)
m;j;vki<0:010)<0:9KJ:
Therefore, as long as J(K 1log(M=)), xm2[M] we can guarantee that with probability
at least 1 =(3M),
max
j;khw(0)
m;j;vki>0:010:
TakeG==(3MJ2K2), by Lemma F.1 we have that with probability at least 1  =(3M),
max
(j;k)6=(jm;km)hw(0)
m;j;vki(1 G)hw(0)
m;jm;vkmi:
25

--- PAGE 26 ---
By the symmetric property, we have that for all k2[K];m2[M],
P(k=k
m) =K 1:
Therefore, the probability that jMkjat least include one element is as follows,
P(jMkj1)1 (1 K 1)M:
By union bound we get that
P(jMkj1;8k)1 K(1 K 1)M1 Kexp( M=K )1 =3;
where the last inequality is by condition MKlog(3K=). Therefore, with probability at least
1 =3,jMkj1;8k.
Applying Union bound, we have that with probability at least 1  ,
max
(j;k)6=(jm;km)hw(0)
m;j;vki 
1 = 
3MJ2K2)
hw(0)
m;jm;vkmi;
hw(0)
m;jm;vkmi0:010;8m2[M];
jMkj1;8k2[K]:
Lemma D.5. Suppose the conclusions in Lemma D.2 hold, then with probability at least 1  we
have thatjhw(0)
m;j;vijeO(0) for all v2fvkgk2[K][fckgk2[K][fi;pgi2[n];p2[P 3];m2[M];j2[J].
Proof. Fixv2fvkgk2[K][fckgk2[K][fi;pgi2[n];p2[P 3];m2[M];j2[J], we have thathw(0)
m;j;vi
N(0;2
0kvk2
2) andkvk2=O(1). Therefore, with probability at least 1  =(nPMJ ) we have that
jhw(0)
m;j;vijeO(0). Applying union bound completes the proof.
E Proof of Theorem 4.2
In this section we always assume that the conditions in Theorem 4.2 holds. It is easy to show that
all the conclusions in this section D hold with probability at least 1  O(1=logd). The results in
this section hold when all the conclusions in Section D hold. For simplicity of notation, we simplify
(xi;yi)2
k;k0asi2
k;k0, and`0(yimi;t(xi;(t))fmi;t(xi;W(t))) as`0
i;t.
Recall that at iteration t, data xiis routed to the expert mi;t. Heremi;tshould be interpreted
as a random variable. The gradient of MoE model at iteration tcan thus be computed as follows
rmL(t)=1
nX
i;p1(mi;t=m)`0
i;tmi;t(xi;(t))(1 mi;t(xi;(t)))yifmi;t(xi;W(t))x(p)
i
 1
nX
i;p1(mi;t6=m)`0
i;tmi;t(xi;(t))m(xi;(t))yifmi;t(xi;W(t))x(p)
i
=1
nX
i;p1(mi;t=m)`0
i;tmi;t(xi;(t))yifmi;t(xi;W(t))x(p)
i
26

--- PAGE 27 ---
 1
nX
i;p`0
i;tmi;t(xi;(t))m(xi;(t))yifmi;t(xi;W(t))x(p)
i; (E.1)
rwm;jL(t)=1
nX
i;p1(mi;t=m)`0
i;tm(xi;(t))yi0(hw(t)
m;j;x(p)
ii)x(p)
i: (E.2)
Following lemma shows implicit regularity in the gating network training.
Lemma E.1. For allt0, we have thatPM
m=1rmL(t)=0and thusP
m(t)
m=P
m(0)
m. In
particular, when is zero initialized, thenP
m(t)
m= 0
Proof. We rst write out the gradient of mfor allm2[M],
rmL(t)=1
nX
i2[n];p2[P]1(mi;t=m)`0
i;tmi;t(xi;(t))yifmi;t(xi;W(t))x(p)
i
 1
nX
i2[n];p2[P]`0
i;tmi;t(xi;(t))m(xi;(t))yifmi;t(xi;W(t))x(p)
i:
Take summation from m= 1 tom=M, then we have
MX
m=1rmL(t)=1
nX
i2[n];p2[P]`0
i;tmi;t(xi;(t))yifmi;t(xi;W(t))x(p)
i
 1
nX
i2[n];p2[P]`0
i;tmi;t(xi;(t))yifmi;t(xi;W(t))x(p)
i
= 0:
Notice that the gradient at iteration tin (E.1) and (E.2) is depend on the random variable mi;t,
the following lemma shows that it can be approximated by its expectation.
Lemma E.2. With probability at least 1  1=d, for all the vector v2fvkgk2[K][fckgk2[K],
m2[M],j2[J], we have the following equations hold jhrmL(t);vi E[hrmL(t);vi]j=
eO(n 1=2(0+t)3),jhrwm;jL(t);vi E[hrwm;jL(t);vi]j=eO(n 1=2(0+t)2), for alltd100.
HereE[hrwm;jL(t);vi] andE[hrmL(t);vi] can be computed as follows,
E[hrmL(t);vi] =1
nX
i;pP(mi;t=m)`0
i;tm(xi;(t))yifm(xi;W(t))hx(p)
i;vi
 1
nX
i;p;m0P(mi;t=m0)`0
i;tm0(xi;(t))m(xi;(t))yifm0(xi;W(t))hx(p)
i;vi
E[hrwm;jL(t);vi] =1
nX
i;pP(mi;t=m)`0
i;tm(xi;(t))yi0(hw(t)
m;j;x(p)
ii)hx(p)
i;vi:
Proof. Because we are using normalized gradient descent, kw(t)
m;j w(0)
m;jk2O(t) and thus by
27

--- PAGE 28 ---
Lemma D.5 we have jhw(t)
m;j;x(p)
iijeO(0+t). Therefore,
hrwm;jL(t);vi=1
nX
iX
p1(mi;t=m)`0
i;tm(xi;(t))yi0(hw(t)
m;j;x(p)
ii)hx(p)
i;vi
| {z }
Ai;
whereAiare independent random variables with jAij eO 
(0+t)2
. Applying Hoeding's
inequality gives that with probability at least 1  1=(4d101MJK ) we have thatjhrwm;jL(t);vi 
E[hrwm;jL(t);vi]j=eO(n 1=2(0+t)2). Applying union bound gives that with probability at least
1 1=(2d),jhrwm;jL(t);vi E[hrwm;jL(t);vi]j=eO(n 1=2(0+t)2);8m2[M];j2[J];td100.
Similarly, we can prove jhrmL(t);vi E[hrmL(t);vi]j=eO(n 1=2(0+t)3).
E.1 Exploration Stage
DenoteT1=b 10:5
0c. The rst stage ends when t=T1. During the rst stage training, we can
prove that the neural network parameter maintains the following property.
Lemma E.3. For alltT1, we have the following properties hold,
â€¢hw(t)
m;j;vki=O(0:5
0);hw(t)
m;j;cki=O(0:5
0);hw(t)
m;j;i;pi=eO(0:5
0),
â€¢fm(xi;W(t)) =eO(1:5
0),
â€¢j`0
i;t 1=2jeO(1:5
0),
â€¢k(t)
mk2eO(1:5
0),
â€¢kh(xi;(t))k1=eO(1:5
0),m(xi;(t)) =M 1+eO(1:5
0),
for allm2[M];k2[k];i2[n];p4.
Proof. The rst property is obvious since kw(t)
m;j w(0)
m;jk2O(T1) =O(0:5
0) and thus
jfm(xi;W(t))jX
p2[P]X
j2[J]j(hw(t)
m;j;x(p)
ii)j=eO(1:5
0):
Then we show that the loss derivative is close to 1 =2 during this stage.
Lets=yimi;t(xi;(t))fmi;t(xi;W(t)), then we have that jsj=eO(1:5
0) and
`0
i;t 1
2=1
es+ 1 1=2(i)
jsj=eO(1:5
0);
where (i) can be proved by considering jsj1 andjsj>1.
Now we prove the fourth bullet in Lemma E.3. Because jfmj=eO(1:5
0), we can upper bound
the gradient of the gating network by
krmL(t)k2=1
nX
i;p1(mi;t=m)`0
i;tmi;t(xi;(t))yifmi;t(xi;W(t))x(p)
i
28

--- PAGE 29 ---
 1
nX
i;p`0
i;tmi;t(xi;(t))m(xi;(t))yifmi;t(xi;W(t))x(p)
i
2:
=eO(1:5
0);
where the last inequality is due to j`0
i;tj1,m;mi;t2[0;1] andkx(p)
ik2=O(1). This further
implies that
k(t)
mk2=k(t)
m (0)
mk2eO(1:5
0tr) =eO(1:5
0);
where the last inequality is by r= (M2). The proof of kh(xi;(t))k1O(1:5
0) and
m(xi;(t)) =M 1+O(1:5
0) are straight forward given k(t)
mk2=eO(1:5
0).
We will rst investigate the property of the router.
Lemma E.4. maxm2[M]jP(mi;t=m) 1=Mj=eO(1:5
0) for alltT1,i2[n] andm2[M].
Proof. By Lemma E.3 we have that kh(xi;(t))k1eO(1:5
0). Lemma 5.1 further implies that
max
m2[M]jP(mi;t=m) 1=Mj=eO(1:5
0):
Lemma E.5. We have following gradient update rules hold for the experts,
hrwm;jL(t);vki= E[3] +eO(d 0:005)
2KM20(hw(t)
m;j;vki) +eO(2:5
0);
hrwm;jL(t);cki=eO(d 0:005)0(hw(t)
m;j;cki) +eO(2:5
0);
hrwm;jL(t);i;pi=eO(d 0:005)0(hw(t)
m;j;i;pi) +eO(2:5
0)
for alltT1;j2[J];k2[K];m2[M];p4. Besides, we have the following gradient norm upper
bound holds
krwm;jL(t)k2X
k2[K]E[3] +eO(d 0:005)
2KM20(hw(t)
m;j;vki) +X
k2[K]eO(d 0:005)0(hw(t)
m;j;cki)
+X
i2[n];p4eO(d 0:005)0(hw(t)
m;j;i;pi) +eO(2:5
0)
for alltT1;j2[J];m2[M].
Proof. The experts gradient can be computed as follows,
rwm;jL(t)=1
nX
i2[n];p2[P]1(mi;t=m)`0
i;tfm(xi;W(t))m(xi;(t))yi0(hw(t)
m;j;x(p)
ii)x(p)
i:
We rst compute the inner product hrwm;jL(t);cki. By Lemma E.2, we have that jhrwm;jL(t);cki 
29

--- PAGE 30 ---
E[hrwm;jL(t);cki]j=eO(n 1=20)eO(2:5
0).
E[hrwm;jL(t);cki] = 1
nX
i2
kP(mi;t=m)`0
i;tm(xi;(t))0(hw(t)
m;j;cki)yi3
ikckk2
2
 1
nX
i2[n];p4P(mi;t=m)`0
i;tm(xi;(t))0(hw(t)
m;j;i;pi)yihck;i;pi
=
 1
2nMX
i2
kyi3
iP(mi;t=m) +eO(1:5
0)
0(hw(t)
m;j;cki) +eO(2:5
0)
=eO(n 1=2+1:5
0)0(hw(t)
m;j;cki) +eO(2:5
0)
=eO(d 0:005)0(hw(t)
m;j;cki) +eO(2:5
0)
where the second equality is due to Lemma E.3 and D.2, the third equality is due to Lemma E.4,
the last equality is by the choice of nand0. Next we compute the inner product hrwm;jL;vki.
By Lemma E.2, we have that jhrwm;jL(t);vki E[hrwm;jL(t);vki]j=eO(n 1=20)eO(2:5
0).
E[hrwm;jL(t);vki] = 1
nX
i2
kP(mi;t=m)`0
i;tm(xi;(t))0(hw(t)
m;j;vki)3
ikvkk2
2
 1
nX
k06=kX
i2
k0;kP(mi;t=m)`0
i;tm(xi;(t))0(hw(t)
m;j;vki)3
iyiikvkk2
2
 1
nX
i2[n];p4P(mi;t=m)`0
i;tm(xi;(t))0(hw(t)
m;j;i;pi)yihvk;i;pi
=
 1
2nMX
i2
kP(mi;t=m)3
i 1
2nMX
i2
k0;kP(mi;t=m)3
iyii+O(1:5
0)

0(hw(t)
m;j;cki) +eO(2:5
0)
= 
E[3] +eO(n 1=2+1:5
0)
0(hw(t)
m;j;vki) +eO(2:5
0)
=E[3]
2KM2+eO(d 0:005)
0(hw(t)
m;j;vki) +eO(2:5
0)
where the second equality is due to Lemma E.3 and D.2, the third equality is due to Lemma E.4,
the last equality is by the choice of nand0. Finally we compute the inner product hrwm;jL;i;pi
as follows
hrwm;jL(t);i;pi= 1
n1(mi;t=m)`0
i;tm(xi;(t))0(hw(t)
m;j;i;pi)ki;pk2
2+eO(0d 1=2)
=eOki;pk2
2
n
0(hw(t)
m;j;i;pi) +eO(0d 1=2)
=eO(d 0:005)0(hw(t)
m;j;i;pi) +eO(2:5
0);
where the rst equality is due to Lemma D.2, second equality is due to j`0
i;tj1;m2[0;1] and
the third equality is due to Lemma D.2 and our choice of n;0. Based on previous results, let B
30

--- PAGE 31 ---
be the projection matrix on the linear space spanned by fvkgk2[K][fckgk2[K]. We can verify that
krwm;jL(t)k2kBrwm;jL(t)k2+k(I B)rwm;jL(t)k2
X
k2[K]E[3] +eO(d 0:005)
2KM20(hw(t)
m;j;vki) +X
k2[K]eO(d 0:005)0(hw(t)
m;j;cki)
+X
i2[n];p4eO(d 0:005)0(hw(t)
m;j;i;pi) +eO(2:5
0):
Because we use normalized gradient descent, all the experts get trained at the same speed.
Following lemma shows that expert mwill focus on the signal vkm.
Lemma E.6. For allm2[M] andtT1, we have following inequalities hold,
hw(t)
m;jm;vkmi=O(0:5
0);
hw(t)
m;j;vki=eO(0);8(j;k)6= (j
m;k
m);
hw(t)
m;j;cki=eO(0);8j2[J];k2[K];
hw(t)
m;j;i;pi=eO(0);8j2[J];i2[n];p4:
Proof. FortT1, the update rule of every expert could be written as,
hw(t+1)
m;j;vki=hw(t)
m;j;vki+
krWmL(t)kF3E[3] +eO(d 0:005)
2KM2hw(t)
m;j;vki2+eO(2:5
0)
;
hw(t+1)
m;j;i;pi=hw(t)
m;j;i;pi+
krWmL(t)kFeO(d 0:005)hw(t)
m;j;i;pi2+eO(2:5
0)
;
hw(t+1)
m;j;cki=hw(t)
m;j;cki+
krWmL(t)kFeO(d 0:005)hw(t)
m;j;cki2+eO(2:5
0)
: (E.3)
FortT1, we have thathw(t)
m;j;vkmiO(0:5
0). By comparing the update rule of hw(t)
m;j;vkmi
and other inner product presented in (E.3) , We can prove that hw(t)
m;j;vkmiwill grow to 0:5
0while
other inner product still remain nearly unchanged.
Comparison with hw(t)
m;j;vki. Consider k6=k
m. We want to get an upper bound of hw(t)
m;j;vki,
so without loss of generality we can assume hw(t)
m;j;vki= 
(0). Since0d 0:01, we have that
hw(t)
m;j;vki2+eO(2:5
0) = (1 +eO(d 0:005))hw(t)
m;j;vki2. Therefore, we have that
hw(t+1)
m;j;vkmi=hw(t)
m;j;vkmi+
krWmL(t)kF3E[3] +eO(d 0:005)
2KM2hw(t)
m;j;vkmi2; (E.4)
hw(t+1)
m;j;vki=hw(t)
m;j;vki+
krWmL(t)kF3E[3] +eO(d 0:005)
2KM2hw(t)
m;j;vki2: (E.5)
Applying Lemma F.2 by choosing Ct= (3E[3] +eO(d 0:005))=(2KM2krWmL(t)kF),S= 1 +
eO(d 0:005),G= 1=(3 log(d)M2) and verifyinghw(0)
m;vkmiS(1 +G 1)hw(0)
m;vki(events in Sec-
tion D hold), we have that hw(t)
m;j;vkiO(G 10) =eO(0).
31

--- PAGE 32 ---
Comparison with hw(t)
m;j;cki.We want to get an upper bound of hw(t)
m;j;cki, so without loss of
generality we can assume hw(t)
m;j;vki= 
(0). Because 0d 0:01, one can easily show that
hw(t+1)
m;j;vkmi=hw(t)
m;j;vkmi+
krWmL(t)kF3E[3] +eO(d 0:005)
2KM2hw(t)
m;j;vkmi2;
hw(t+1)
m;j;ckihw(t)
m;j;cki+
krWmL(t)kFeO(d 0:01)hw(t)
m;j;cki2:
Again, applying Lemma F.2 by choosing Ct= (3E[3] +eO(d 0:005))=(2KM2krWmL(t)kF),S=
eO(d 0:01),G= 2 and verifying hw(0)
m;vkmiS(1 +G 1)hw(0)
m;cki(events in Section D hold), we
have thathw(t);vkiO(G 10) =eO(0).
Comparison with hw(t)
m;j;i;pi. The proof is exact the same as the one with ck.
Denote the iteration T(m)as the rst time that krWmL(t)kF1:8
0. Then Following lemma
gives an upper bound of T(m)for allm2M .
Lemma E.7. For allm2[M], we have that T(m)=eO( 10:8
0) and thusT(m)<0:01T1. Besides,
for allTm<tT1we have that
hrwm;jmL(t);vkmi(1 0:1
0)krWmL(t)kF:
Proof. Let projection matrix B=vkmv>
km2Rdd, then we can divide the gradient into two
orthogonal part
krwm;jmL(t)k2=kBrwm;jmL(t)+ (I B)rwm;jmL(t)k2
kBrwm;jmL(t)k2+k(I B)rwm;jmL(t)k2
Recall that
rwm;jmL(t)=1
nX
i;p1(mi;t=m)`0
i;tm(xi;(t))yi0(hw(t)
m;jm;x(p)
ii)x(p)
i;
So we have that
k(I B)rwm;jmL(t)k2=1
nX
i;p1(mi;t=m)`0
i;tm(xi;(t))yi0(hw(t)
m;jm;x(p)
ii)(I B)x(p)
i
2
1
nX
i;p0(hw(t)
m;jm;x(p)
ii)(I B)x(p)
i
2
eO(2
0);
where the rst inequality is by j`0
i;tj1;m2[0;1] and the second equality is because
1. when x(p)
ialign with vkm, (I B)x(p)
i=0.
2. when x(p)
idoesn't align with vkm,hw(t)
m;jm;x(p)
ii=eO(0).
32

--- PAGE 33 ---
Therefore, we have that
krwm;jmL(t)k2kBrwm;jmL(t)k2+eO(2
0) =hrwm;jmL(t);vkmi+eO(2
0):
We next compute the gradient of the neuron wm;j;j6=j
m,
krwm;jL(t)k2=1
nX
i;p1(mi;t=m)`0
i;tm(xi;(t))yi0(hw(t)
m;j;x(p)
ii)x(p)
i
2=eO(2
0); (E.6)
where the inequality is by hw(t)
m;j;x(p)
ii=eO(0);8j6=j
mwhich is due to Lemma E.6. Now we can
upper bound the gradient norm,
krWmL(t)kFX
j2[J]krwm;jL(t)k2kr wm;jmL(t)k2+eO(2
0): (E.7)
WhenkrWmL(t)kF1:8
0, it is obviously that
hrwm;jmL;vkmikr wm;jmL(t)k2 eO(2
0)kr WmL(t)kF eO(2
0)(1 0:1
0)krWmL(t)kF;
where the rst inequality is by (E.6) and the second inequality is by (E.7). Now let us give an
upper bound for T(m). During the period tT(m),krWmL(t)kF< 1:8
0. On the one hand, by
Lemma E.5 we have that
krWmL(t)k2 hr wm;jL(t);vkmi=3E[3] eO(d 0:005)
2KM2[hw(t)
m;jm;vkmi]2 eO(2:5
0)
which implies that the inner product hw(t)
m;jm;vkmieO(0:9
0). On the other hand, by Lemma E.6
we have that
hw(t+1)
m;jm;vkmihw(t)
m;jm;vkmi+
krWmL(t)kF(1
KM2)hw(t)
m;jm;vkmi2
hw(t)
m;jm;vkmi+ 
KM21:8
0
hw(t)
m;jm;vkmi2
hw(t)
m;jm;vkmi+ 
KM20:8
0
hw(t)
m;jm;vkmi;
where last inequality is by hw(t)
m;jm;vkmi  0:10. Therefore, we have that the inner product
hw(t)
m;j;vkmigrows exponentially and will reach eO(0:9
0) withineO( 10:8
0) iterations.
Recall that T1=b 10:5
0c, following Lemma shows that the expert m2[M] only learns one
feature during the rst stage,
Lemma E.8. For alltT1;m2[M], we have that
hw(t)
m;jm;vkmi=O(0:5
0);
hw(t)
m;j;vki=eO(0);8(j;k)6= (j
m;k
m);
hw(t)
m;j;cki=eO(0);8j2[J];k2[K];
33

--- PAGE 34 ---
hw(t)
m;j;i;pi=eO(0);8j2[J];i2[n];p4:
Besideshw(t)
m;jm;vkmi(1 0:1
0)t, for alltT1=2.
Proof. By Lemma E.7, we have T(m)=eO( 10:8
0)< 0:2
0T1. Notice thathrwm;jmL(t);vki
(1 0:1
0)krWmL(t)kF, for allTmtT1. Therefore, we have that
hw(t+1)
m;jm;vkmihw(t)
m;jm;vkmi+ (1 0:1
0);8TmtT1;
which implieshw(t)
m;jm;vkmi(1 O(0:1
0))t;8tT1=2. Finally, applying Lemma E.6 completes
the proof.
E.2 Router Learning Stage
DenoteT2=b 1M 2c, The second stage ends when t=T2. Given x= [yvk;ck;vk0;],
we denote by  x= [0;ck;0;:::;0] the one only keeps cluster-center signal and denote by bx=
[yvk;0;vk0;0] the one that only keeps feature signal and feature noise.
For allT1tT2, we will show that the router only focuses on the cluster-center signals and
the experts only focus on the feature signals, i.e., we will prove that jfm(xi;W(t)) fm(bxi;W(t))j
andkh(xi;(t)) h(xi;(t))k1are small. In particular, We claim that for all T1tT2,
following proposition holds.
Proposition E.9. For allT1tT2, following inequalities hold,
jfm(xi;W(t)) fm(bxi;W(t))jO(d 0:001);8m2[M];i2[n]; (E.8)
kh(xi;(t)) h(xi;(t))k1O(d 0:001);8i2[n]; (E.9)
P(mi;t=m);m(xi;(t)) = 
(1=M);8m2[M];i2
km: (E.10)
Proposition E.9 implies that expert will only focus on the label signal and router will only focus
on the cluster-center signal. We will prove Proposition E.9 by induction. Before we move into the
detailed proof of Proposition E.9, we will rst prove some important lemmas.
Lemma E.10. For allT1tT2, the neural network parameter maintains following property.
â€¢jfm(xi;W(t))j=O(1);8m2[M],
â€¢mi;t(xi;(t)) = 
(1=M),8i2[n].
Proof. Because we use normalized gradient descent, the rst bullet would be quite straight forward.
jfm(xi;W(t))j=X
j2[J]X
p2[P](hw(t)
m;j;x(p)
ii)(i)=O(1);
where (i) is bykw(t)
m;j w(0)
m;jk2=O(T2) =O(M 2) and x(p)
i=O(1).
34

--- PAGE 35 ---
Now we prove the second bullet. By Lemma C.4, we have that hmi;t(x;)maxmhm(x;) 1,
which implies that
mi;t(xi;(t)) =exp(hmi;t(xi;(t)))P
mexp(hm(x;(t)))exp(hmi;t(xi;(t)))
Mmaxmexp(hm(x;(t)))1
eM:
Lemma E.11. Denote= maxikh(xi;) h(xi;)k1and let the random variable  mi;tbe
expert that get routed if we use the gating network output h(xi;(t)) instead. Then we have
following inequalities,
jm(xi;) m(xi;)j=O();8m2[M];i2[n];: (E.11)
jP(mi;t=m) P( mi;t=m)j=O(M2);8m2[M];i2[n]: (E.12)
Proof. By denition of , we have thatkh(xi;(t)) h(xi;(t))k1. Then applying
Lemma 5.1 gives jP(mi;t=m) P( mk;t=m)j=eO();8m2[M];i2[n], which completes
the proof for (E.12).
Next we prove (E.11), which needs more eort. For all i2[n], we have
m(xi;) =m(xi;) exp(hm(xi;) hm(xi;))P
m0m0(xi;) exp(hm0(xi;) hm0(xi;)):
Letm0= exp(hm0(xi;) hm0(xi;)) = 1 +O(). Then for suciently small , we have that
m00:5 . Then we can further compute
jm(xi;(t)) m(xi;)j=m(xi;)mP
m0m0(xi;)m0 1
=m(xi;)jP
m0m0(xi;)(m0 m)jP
m0m0(xi;)m0
m(xi;)P
m0m0(xi;)jm0 mjP
m0m0(xi;)m0
O();
where the last inequality is by jm0 mj O(),m(xi;)1 andP
m0m0(xi;)m0
[P
m0m0(xi;)]=2 = 0:5.
Following Lemma implies that the pattern learned by experts during the rst stage won't change
in the second stage.
Lemma E.12. Suppose (E.8), (E.9), (E.10) hold for all t2[T1;T][T1;T2 1], then we have
following inequalities hold for all t2[T1;T+ 1],
hw(t)
m;jm;vkmi(1 O(0:1
0))t;
hw(t)
m;j;vki=eO(0);8(j;k)6= (j
m;k
m);
hw(t)
m;j;cki=eO(0);8j2[J];k2[K];
hw(t)
m;j;i;pi=eO(0);8j2[J];k2[K];i2[n];p4:
35

--- PAGE 36 ---
Proof. Most of the proof exactly follows the proof in the rst stage, so we only list some key steps
here. Recall that
rwm;jL(t)=1
nX
i;p1(mi;t=m)`0
i;tm(xi;(t))yi0(hw(t)
m;j;x(p)
ii)x(p)
i:
In the proof of Lemma E.5, we do Taylor expansion at the zero point. Now we will do Taylor
expansion at fm(bxi;W) and(xi;) as follows,
jm(xi;(t))fm(xi;W(t)) m(xi;(t))fm(bxi;W(t))j
jm(xi;(t))[fm(xi;W(t)) fm(bxi;W(t))]j+j[m(xi;(t)) m(xi;(t))]fm(xi;W(t))j
jfm(xi;W(t)) fm(bxi;W(t))j+O(jm(xi;(t)) m(xi;(t))j)
O(d 0:001);
where the rst inequality is by triangle inequality, the second inequality is by m(xi;(t))1 and
jfm(xi;W(t))j=O(1) in Lemma E.10, the third inequality is by (E.8), (E.9) and (E.11).
Then follow the proof of Lemma E.5, we have that
E[hrwm;jL(t);vkmi] = 1
nX
i2
kmP(mi;t=m)`0
i;tm(xi;(t))0(hw(t)
m;j;vkmi)3
ikvkmk2
2
 1
nX
i2
k0;kmP(mi;t=m)`0
i;tm(xi;(t))0(hw(t)
m;j;vkmi)3
iyiikvkmk2
2
 1
nX
i;pP(mi;t=m)`0
i;tm(xi;(t))0(hw(t)
m;j;i;pi)yihvkm;i;pi
=
 e1
nX
i2
kmP(mi;t=m)3
i e1
nX
i2
k0;kmP(mi;t=m)3
iyii
+O(d 0:001)
0(hw(t)
m;j;vkmi) +eO(d 1=2)
(i)= e(1)0(hw(t)
m;j;vkmi);
where (i) is due to (E.10): P(mi;t=m)(1=M),8i2
km;m2[M]. Again follow Lemma E.5
and Lemma E.6, we further have that
hrwm;jL(t);vki= e(1)[hw(t)
m;j;vki]2;
hrwm;jL(t);cki=eO(1)[hw(t)
m;j;cki]2;
hrwm;jL(t);i;pi=eO(1)[hw(t)
m;j;i;pi]2:
Thus for all T1tT, the update rule of every expert could be written as,
hw(t+1)
m;j;vkmi=hw(t)
m;j;vkmi+e(1)
krWmL(t)kFhw(t)
m;j;vkmi2
hw(t+1)
m;j;vki=hw(t)
m;j;vki+eO(1)
krWmL(t)kFhw(t)
m;j;vki2
36

--- PAGE 37 ---
hw(t+1);i;pi=hw(t);i;pi+eO(1)
krWmL(t)kFhw(t);i;pi2
hw(t+1)
m;j;cki=hw(t)
m;j;cki+eO(1)
krWmL(t)kFhw(t)
m;j;cki2:
By the rst stage of training we have that hw(T1)
m;j;vkmi= (0:5
0), while others remains eO(0).
Then we can use Lemma F.2, by choosing S=e(1) andG= 2, then we have that
hw(t)
m;j;vkmi=O(1):
hw(t)
m;j;vki=eO(0);8k6=k
m:
hw(t)
m;j;cki=eO(0):
hw(t);i;pi=eO(0):
Then following Lemma E.7 and E.8, we can prove that for all T1tT+ 1,m2[M],
hw(t)
m;jm;vkmi(1 O(0:1
0))t;
hw(t)
m;j;vki=eO(0);8(j;k)6= (j
m;k
m);
hw(t)
m;j;cki=eO(0);8j2[J];k2[K];
hw(t)
m;j;i;pi=eO(0);8j2[J];i2[n];p4:
By the result of expert training we have following results
Lemma E.13. Suppose (E.8), (E.9), (E.10) hold for all t2[T1;T][T1;T2 1], then we have
thatjfm(xi;W(t)) fm(bxi;W(t))j=eO(3
0) for allm2[M] andi2[n],t2[T1;T+ 1]. Besides,
yifm(bxi;W(t)) =X
j2[J]h
3
i(hw(t)
m;j;vki) +3
i(hw(t)
m;j;vk0i)i
;8i2
+
k;k0;m2[M];
yifm(bxi;W(t)) =X
j2[J]h
3
i(hw(t)
m;j;vki) 3
i(hw(t)
m;j;vk0i)i
;8i2
 
k;k0;m2[M]:
Proof. For alli2
k, we have that
fm(xi;W(t)) fm(bxi;W(t))X
j2[J](hw(t)
m;j;cki)+X
j2[J];p4(hw(t)
m;j;i;pi)
O(J)max
k;j(hw(t)
m;j;cki) +O(J)max
i;j;pj(hw(t)
m;j;i;pi)j
=eO(3
0);
where the rst inequality is by triangle inequality and the last equality is by Lemma E.12.
Next we will show that router only focus on the cluster-center signal rather than the label signal
during the router training.
37

--- PAGE 38 ---
Lemma E.14. Suppose (E.8), (E.9), (E.10) hold for all t2[T1;T][T1;T2 1], then we have
thatkh(xi;(t)) h(xi;(t))k1=eO(d 0:005) hold for all i2[n] andt2[T1;T+ 1]. Besides, we
have that max m;kjh(t)
m;vkij;maxm;i;pjh(t)
m;i;pij=eO(d 0:005) for allt2[T1;T+ 1].
Proof. Recall the denition of in Lemma E.11, we need to show that (t)=eO(d 0:005) for all
t2[T1;T+ 1]. We rst prove following router parameter update rules,
hrmL(t);vki=O((t)K2) +eO(d 0:005);hrmL(t);i;pi=eO(d 0:005); (E.13)
for allT1tT,m2[M],k2[K],i2[n] andp4.
Consider the inner product of the router gradient and the feature vector and we have
E[
rmL(t);vk
]
=1
nX
i2
kP(mi;t=m)`0
i;tyim(xi;(t))fm(xi;W(t))yii
| {z }
I1
+1
nX
i2
k0;kP(mi;t=m)`0
i;tyim(xi;(t))fm(xi;W(t))ii
| {z }
I2
 1
nX
i2
k;m02[M]P(mi;t=m0)`0
i;tyim0(xi;(t))m(xi;(t))fm0(xi;W(t))yii
| {z }
I3
 1
nX
i2
k0;k;m02[M]P(mi;t=m0)`0
i;tm0(xi;(t))yim(xi;(t))fm0(xi;W(t))ii
| {z }
I4
+1
nX
i2[n];p4P(mi;t=m)`0
i;tyim(xi;(t))fm(xi;W(t))hx(p)
i;vki
| {z }
I5
 1
nX
i2[n];p4;m02[M]P(mi;t=m0)`0
i;tyim0(xi;(t))m(xi;(t))fm0(xi;W(t))hx(p)
i;vki
| {z }
I6:(E.14)
Denoteyim(xi;(t))fm(bxi;W(t));8i2
+
k;k0byF+
k;k0. We next show that the output of the MoE
multiplied by label: yim(xi;(t))fm(xi;W);8i2
+
k;k0can be approximated by F+
k;k0.
jm(xi;(t))fm(xi;W(t)) m(xi;(t))fm(bxi;W(t))j
j[m(xi;(t)) m(xi;(t))]fm(xi;W(t))j+jm(xi;(t))[fm(xi;W(t)) fm(bxi;W(t))]j
O(jm(xi;(t)) m(xi;(t))j) +jfm(xi;W(t)) fm(bxi;W(t))j
O((t)) +eO(3
0);
where the rst inequality is by triangle inequality, the second inequality is by m(xi;(t))1 and
jfm(xi;W(t))j=O(1) in Lemma E.10, the third inequality is by (E.11) and Lemma E.13.
38

--- PAGE 39 ---
Similarly, denote yim(xi;(t))fm(bxi;W(t));i2
 
k;k0byF 
k;k0and we can show that value
yim(xi;(t))fm(xi;W(t));8i2
 
k;k0can be approximated by F 
k;k0. Now we can bound I1as
follows,
I1=X
k06=k`0(Fk;k0+)F+
k;k0
nX
i2
+
k;k0
P(mi;t=m)yii+O((t))
+eO(3
0)
+X
k06=k`0(Fk;k0 )F 
k;k0
nX
i2
 
k;k0
P(mi;t=m)yii+O((t))
+eO(3
0)
(i)=X
k06=k`0(Fk;k0+)F+
k;k0
nX
i2
+
k;k0
P( mi;t=m)yii+O(M2(t))
+eO(3
0)
+X
k06=k`0(Fk;k0 )F 
k;k0
nX
i2
 
k;k0
P( mi;t=m)yii+O(M2(t))
+eO(3
0)
(ii)=O(M2(t)) +eO(n 1=2+3
0)
=O(M2(t)) +eO(d 0:005)
where (i) is due to (E.12) and (ii) is byP
i2
+
k;k0yi=eO(pn) andP
i2
 
k;k0yi=eO(pn) in
Lemma D.1. Similarly we can prove that I2;I3;I4=O(M2(t)) +eO(d 0:005). Sincehx(p)
i;vii=
eO(d 1=2);8p4,m;mi;t1 andfmi;t=O(1), we can upper bound I5;I6byeO(d 1=2). Plugging
those bounds into the gradient computation (E.14) gives
E[
rmL(t);vk
] =O(M2(t)) +eO(d 0:005):
We nally consider the alignment between router gradient and noise

rmL(t);i0;p0
=1
nX
i2[n];p41(mi;t=m)`0
i;tyimi;t(xi;(t))fmi;t(xi;W(t))hx(p)
i;i0;p0i
 1
nX
i2[n];p4`0
i;tyimi;t(xi;(t))m(xi;(t))fmi;t(xi;W(t))hx(p)
i;i0;p0i:
(i)=eO1
n
+eO(d 1=2)
(ii)=eO(d 1=2);
where the (i) is by considering the cases ( i0;p0) =i;pandi0;p06=i;prespectively and (ii) is due to
our choice of n. Now, we have completed the proof of (E.13).
Plugging the gradient estimation (E.13) in to the gradient update rule for the gating network
(3.5) gives
max
m;kjh(t+1)
m;vkijmax
m;kjh(t)
m;vkij+O(rM2(t)) +eO(rd 0:005) (E.15)
max
m;i;pjh(t+1)
m;i;pijmax
m;i;pjh(t)
m;i;pij+eO(rd 0:005) (E.16)
39

--- PAGE 40 ---
Combining (E.15) and (E.16), we have that there exist C1=O(M2) andC2=eO(d 0:005) such
that(t+1)(t)+C1r(t)+C2r. Therefore, we have that
(t+1)+C 1
1C2(1 +C1r)[(t)+C 1
1C2]
exp(C1r)[(t)+C 1
1C2];
where the last inequality is due to exp( z)1 +zfor allz2R. Then we further have that
(t)exp(C1rt)[(0)+C 1
1C2]exp(C1r 1M 2)[(0)+C 1
1C2] =eO(d 0:005);
where the last equality is by r= (M2).
Dene  := maxk2[K]maxm;m02Mkmax (xi;yi)2
kjhm(xi;) hm0(xi;)j, which measures
the bias of the router towards dierent experts in the same Mk. Following Lemma shows that the
router will treats professional experts equally when  is small.
Lemma E.15. For allt0, we have that following inequality holds,
max
k2[K]max
m;m02Mkmax
(xi;yi)2
kjm0(xi;(t)) m(xi;(t))j2(t);
max
k2[K]max
m;m02Mkmax
(xi;yi)2
kjP(mi;t=m) P(mi;t=m0)j=O(M2)(t):
Proof. By Lemma C.3, we directly have that
jP(mi;t=m) P(mi;t=m0)jO(M2)jhm(xi;(t)) hm0(xi;(t))j:
Then, we prove that
jm0(xi;) m(xi;)j2jhm(xi;(t)) hm0(xi;(t))j: (E.17)
Whenjhm(xi;(t)) hm0(xi;(t))j1, it is obvious that (E.17) is true. When jhm(xi;(t)) 
hm0(xi;(t))j1 we have that
jm0(xi;) m(xi;)j=exp(hm(xi;(t))) exp(hm0(xi;(t)))P
m00exp 
hm00(xi;(t))
=exp(hm0(xi;(t)))P
m00exp 
hm00(xi;(t))jexp(hm(xi;(t)) hm0(xi;(t))) 1j
2jhm(xi;(t)) hm0(xi;(t))j;
which completes the proof of (E.17).
Notice that the gating network is initialized to be zero, so we have  = 0 at initialization.
We can further show that  =O 
1=poly(d)
during the training up to time T=eO( 1).
Lemma E.16. Suppose (E.8), (E.9), (E.10) hold for all t2[T1;T][T1;T2 1], then we have
that (t)eO(d 0:001) holds for all t2[T1;T+ 1].
Proof. One of the key observation is the similarity of the m-th and the m0-th expert in the same
expert classMk. Lemma E.12 implies that max i2
kjfm(xi;W(t)) fm0(xi;W(t))j=eO(0:1
0)
eO(d 0:001).
40

--- PAGE 41 ---
Another key observe is that, we only need to focus on the k thcluster-center signal. Lemma E.14
implies that,
(t)= max
k2[K]max
m;m02Mkmax
(xi;yi)2
kjhm(xi;) hm0(xi;(t))j
max
k2[K]max
m;m02Mkmax
(xi;yi)2
kjhm(xi;(t)) hm0(xi;(t))j+ 2(t)
= max
k2[K]max
m;m02Mkjhm m0;ickij+ 2(t)
C2max
k2[K]max
m;m02Mkjhm m0;ckij+ 2(t);
where the rst inequality is by Lemma E.14 and the second inequality is by iC2. We now prove
that following gradient dierence is small

rmL(t) r m0L(t);ck
(i)=1
nX
i2[n]X
p2[P]P(mi;t=m)`0
i;tm(xi;(t))yifm(xi;W(t))hx(p)
i;cki
 1
nX
i2[n]X
p2[P]P(mi;t=m0)`0
i;tm0(xi;(t))yifm0(xi;W(t))hx(p)
i;cki
+1
nX
i2
kX
p2[P]X
m002[M][m0(xi;(t)) m(xi;(t))]P(mi;t=m00)`0
i;tm00(xi;(t))
yifm00(xi;W)hx(p)
i;cki+eO(d 0:001)
=O1
nX
i2
k[P(mi;t=m0) P(mi;t=m)]j`0
i;tm(xi;)iyifm(xi;W(t))j+eO(d 0:001)
+O(1) max
i2
kjm0(xi;(t)) m(xi;(t))j+O(1) max
i2
kjfm(xi;W(t)) fm0(xi;W(t))j
=O(1)jP(mi;t=m0) P(mi;t=m)]j+O(1) max
i2
kjm0(xi;(t)) m(xi;(t))j
+O(1) max
i2
kjfm(xi;W(t)) fm0(xi;W(t))j+eO(d 0:001)
(ii)=O(M2(t)) +eO(d 0:001);
where the (i) is by Lemma E.2 and (ii) is by Lemma E.15. It further implies that (t+1)
O(rM2)(t)+eO(rd 0:001). Following previous proof of , we have that (T+1)=eO(d 0:001).
Together with the key technique 1, we can infer that each expert m2Mkwill get nearly the
same load as other experts in Mk. Since  keeps increasing during the training, it cannot be
bounded if we allow the total number of iterations goes to innity in Algorithm 1. This is the
reason that we require early stopping in Theorem 4.2, which we believe can be waived by adding
load balancing loss (Eigen et al., 2013; Shazeer et al., 2017; Fedus et al., 2021), or advanced MoE
layer structure such as BASE Layers (Lewis et al., 2021; Dua et al., 2021) and Hash Layers (Roller
et al., 2021).
Lemma E.17. Suppose (E.8), (E.9), (E.10) hold for all t2[T1;T][T1;T2 1], then for m =2Mk
41

--- PAGE 42 ---
andt2[T1;T] , ifh(t)
m;ckimaxm0h(t)
m0;cki 1 we have that
hrmL(t);cki
3t3
KM3
+eO(d 0:005):
Proof. The expectation of the inner product hrmL(t);ckican be computed as follows,
E[hrmL(t);cki] =1
nX
i;pP(mi;t=m)`0
i;tm(xi;(t))yifm(xi;W(t))hx(p)
i;cki
 1
nX
i;p;m0P(mi;t=m0)`0
i;tm0(xi;(t))m(xi;(t))yifm0(xi;W(t))hx(p)
i;cki
(i)=1
nX
i2
kP(mi;t=m)`0
i;tm(xi;(t))iyifm(xi;W(t)) +eO(d 0:005)
 1
nX
i2
kX
m02[M]P(mi;t=m0)`0
i;tm0(xi;(t))m(xi;(t))iyifm0(xi;W):
(E.18)
where (i) is due to jhi;p;ckij=eO(d 0:5).
We can rewrite the inner product (E.18) as follows,
E[
rmL(t);ck
] =1
nX
i2
kP(mi;t=m)`0
i;tm(xi;(t))iyifm(xi;W(t)) +eO(d 0:005)
 1
nX
i2
kX
m02[M]P(mi;t=m0)`0
i;tm0(xi;(t))m(xi;(t))iyifm0(xi;W)
=1
nX
i2
kP(mi;t=m)`0
i;tm(xi;(t))yiifm(xi;W(t))
| {z }
I1+eO(d 0:005)
 1
nX
i2
k;m02MkP(mi;t=m0)`0
i;tm0(xi;(t))m(xi;(t))iyifm0(xi;W(t))
| {z }
I2
(E.19)
 1
nX
i2
k;m0=2MkP(mi;t=m0)`0
i;tm0(xi;(t))m(xi;(t))iyifm0(xi;W(t))
| {z }
I3:
(E.20)
To calculate I1;I2;I3, let's rst lower bound I2. We now consider the case that m62Mk;m02Mk.
Becauseh(t)
m;ckimaxm0h(t)
m;cki 1, we can easily prove that m(xi;(t)) = 
(1=M);8i2
k.
Then we have that
I2= 1
nX
i2
k;m02MkP(mi;t=m0)`0
i;tm0(xi;(t))m(xi;(t))iyifm0(xi;W(t))
42

--- PAGE 43 ---

3t3
nM3X
i2
k;m02Mki

3t3
KM3
;
where the rst inequality is by m0(xi;(t)) = 
(1=M),P(mi;t=m0)(1=M),8i2
km;m2
[M],yifm0(xi;W(t)) =3t3(1 O(0:1
0)) and`0= (1) for all i2
k;m02 Mkdue to
Proposition E.9 and Lemma E.12, and the last inequality is by jMkj1 in Lemma D.4 andP
i2
ki= 
(n=K) in Lemma D.1.
Then we consider the case that m;m062Mk. Applying Taylor expansion of `0
i;t= 1=2+O(J3t3)
gives
1
nX
i2
kP(mi;t=m)`0
i;tm(xi;(t))yiifm(xi;W(t))
=1
2nX
i2
kP(mi;t=m)m(xi;(t))yiifm(xi;W(t)) +O 
J26t6
=1
2nX
k0X
i2
+
k;k0P(mi;t=m)m(xi;(t))yiifm(xi;W(t)) +O 
J26t6
+1
2nX
k0X
i2
 
k;k0P(mi;t=m)m(xi;(t))yiifm(xi;W(t))
=O(J26t6) +eO(d 0:005): (E.21)
where the last inequality is by the technique we have used before in Lemma E.16. By (E.21), we
can get upper bound jI1j;jI3jbyO(J26t6) +eO(d 0:005).
Plugging the bound of I1;I2;I3into (E.20) gives,

rmL(t);ck

3t3
KM3
+O(J26t6) +eO(d 0:005)

3t3
KM3
+eO(d 0:005);
where the last inequality is by tT2=b 1M 2c.
Now we can claim that Proposition E.9 is true and we summarize the results as follow lemma.
Lemma E.18. For allT1tT2, we have Proposition E.9 holds. Besides, we have that
h(T2)
m;ckimaxm02[M]h(T2)
m0;cki 
(K 1M 9) for allm =2Mk..
Proof. We will rst use induction to prove Proposition E.9. It is worth noting that proposition E.9
is true at the beginning of the second stage t=T1. Suppose (E.8), (E.9), (E.10) hold for all
t2[T1;T][T1;T2 1], we next verify that they also hold for t2[T1;T+ 1]. Lemma E.13
shows that (E.8) holds for t2[T1;T+ 1]. Lemma E.14 further shows that (E.8) holds for t2
[T1;T+ 1]. Therefore, we only need to verify whether (E.10) holds for t2[T1;T+ 1]. Therefore,
for each pair i2
k,m2Mk, we need to estimate the gap between expert mand the expert
with best performance hm(xi;(t)) maxm0hm0(xi;(t)). By Lemma E.17 and Lemma E.14, we
43

--- PAGE 44 ---
can induce that hm(xi;(t)) is small therefore cannot be the largest one. Thus hm(xi;(t)) 
maxm0hm0(xi;(t)) =hm(xi;(t)) maxm0hm0(xi;(t))(t)eO(d 0:001). Therefore, by
Lemma C.3 we have (E.10) holds. Now we have veried that (E.10) also holds for t2[T1;T+ 1],
which completes the induction for Lemma E.9.
Finally, we carefully characterize the value of h(t)
m;cki, forr 1= (M2) andm =2Mk. If
h(t)
m;ckimaxm0h(t)
m0;cki 1, by Lemma E.17 we have that
h(t+1)
m;ckih(t)
m;cki r3t3
KM3
+eO(rd 0:005)0: (E.22)
If there exists tT2 1 such thath(t+1)
m;ckimaxm0h(t)
m0;cki 1, clearly we have that h(T2)
m;cki
 
(K 1M 9) sinceh(t)
m;ckiwill keep decreasing as long as h(t+1)
m;cki  1 and our step size
r= (M2)is small enough. If h(t+1)
m;ckimaxm0h(t)
m0;cki 1 holds for all tT2 1, take
telescope sum of (E.22) from t= 0 tot=T2 1 gives that
h(T2)
m;ckih(0)
m;cki T2 1X
s=0r3s3
KM3
+eO(d 0:005)
(i)= T2 1X
s=0r3s3
KM3
+eO(d 0:005)
(ii)= r3T4
2
KM3
+eO(d 0:005)
 
(K 1M 9);
where the (i) is by (0)
m= 0 and (ii) is byPn 1
i=0i3=n2(n 1)2=4 and the last inequality is due to
T2=b 1M 2candr= (M2). Now we have proved that h(T2)
m;cki  
(K 1M 9) for all
m =2Mk. Finally, by Lemma E.1 we have that
max
m02[M]h(T2)
m0;cki1
mX
m02[M]h(T2)
m0;cki= 0:
Therefore, we have that h(T2)
m;cki  
(K 1M 9)maxm02[M]h(T2)
m0;cki 
(K 1M 9), which
completes the proof.
E.3 Generalization Results
In this section, we will present the detailed proof of Lemma 5.2 and Theorem 4.2 based on analysis
in the previous stages.
Proof of Lemma 5.2. We consider the m-th expert in the MoE layer, suppose that m2Mk. Then if
we draw a new sample ( x;y)2
k. Without loss of generality, we assume x= [yvk;ck;vk0;].
By Lemma E.8, we have already get the bound for inner product between weights and feature
signal, cluster-center signal and feature noise. However, we need to recalculate the bound of the
inner product between weights and random noises because we have fresh random noises i.i.d drawn
fromN(0;(2
p=d)Id). Notice that we use normalized gradient descent for expert with step size ,
44

--- PAGE 45 ---
so we have that
kw(T1)
m;j w(0)
m;jk2T1=O(0:5
0):
Therefore, by triangle inequality we have that kw(T1)
m;jk2kw(0)
m;jk2+O(0:5
0)eO(0p
d). Because
the inner product hw(t)
m;j;pifollows the distribution N(0;(2
p=d)kw(T1)
m;jk2
2), we have that with
probability at least 1  1=(dPMJ ),
jhw(T1)
m;j;pij=O(pd 1=2kw(t)
m;jk2log(dPMJ ))eO(0):
Applying Union bound for m2[M];j2[J];p4 gives that, with probability at least 1  1=d,
jhw(T1)
m;j;pij=eO(0);8m2[M];j2[J];p4: (E.23)
Now under the event that (E.23) holds, we have that
yfm(x;W(t)) =yX
j2[J]X
p2[P](hwm;j;x(p)i)
=y(hwm;jm;yvki) +yX
(j;p)6=(jm;1)(hwm;j;x(p)i)
C3
1(1 0:1
0)31:5
0 eO(3
0)

(1:5
0);
where the rst inequality is due to (E.3). Because (E.23) holds holds with probability at least
1 1=d, so we have prove that
P(x;y)D 
yfm(x;W(T1)
0(x;y)2
k
1=d:
On the other hand, if we draw a new sample ( x;y)2
k0;k06=k. Then we consider the special
set 
 
k0;k
k0where feature noise is vkand the sign of the feature noise is not equal to the label
y. Without loss of generality, we assume it as x= [yvk0;ck0; yvk;]. Then under the event
that (E.23) holds, we have that
yfm(x;W(t)) =yX
j2[J]X
p2[P](hwm;j;x(p)i)
=y(hwm;jm; yvki) +yX
(j;p)6=(jm;3)(hwm;j;x(p)i)
 C3
1(1 0:1
0)31:5
0+eO(3
0)
 
(1:5
0);
where the rst inequality is due to (E.3). Because (E.23) holds holds with probability at least
1 1=d, so we have prove that
P(x;y)D 
yfm(x;W(T1)
0(x;y)2
 
k0;k
1 1=d:
45

--- PAGE 46 ---
Then we further have that
P(x;y)D 
yfm(x;W(T1)
0(x;y)2
k0
P(x;y)D 
yfm(x;W(T1)
0(x;y)2
 
k0;k
P(x;y)D 
(x;y)2
 
k0;k(x;y)2
k0

(1=K);
which completes the proof.
Proof of Theorem 4.2. We will give the prove for T=T2, i.e., at the end of the second stage.
Test Error is small. We rst prove the following result for the experts. For all expert m2Mk,
we have that
P(x;y)D 
yfm(x;W(T)
0(x;y)2
k
=o(1): (E.24)
The proof of is similar to the proof of Lemma 5.2. We consider the m-th expert in the MoE layer,
suppose that m2Mk. Then if we draw a new sample ( x;y)2
k. Without loss of generality, we
assume x= [yvk;ck;vk0;]. By Lemma E.8, we have already get the bound for inner product
between weights and feature signal, cluster-center signal and feature noise. However, we need to
recalculate the bound of the inner product between weights and random noises because we have
fresh random noises i.i.d drawn from N(0;(2
p=d)Id). Notice that we use normalized gradient
descent with step size , so we have that
kw(T)
m;j w(0)
m;jk2T=eO(1):
Therefore, by triangle inequality we have that kw(T)
m;jk2kw(0)
m;jk2+eO(1)eO(0p
d). Because the
inner producthw(t)
m;j;pifollows the distribution N(0;(2
p=d)kw(T)
m;jk2
2), with probability at least
1 1=(dPMJ ) we have that ,
jhw(T)
m;j;pij=O(pd 1=2kw(t)
m;jk2log(dPMJ ))eO(0):
Applying Union bound for m2[M];j2[J];p4 gives that, with probability at least 1  1=d,
jhw(T)
m;j;pij=eO(0);8m2[M];j2[J];p4: (E.25)
Now, under the event that (E.25) holds, we have that
yfm(x;W(T)) =yX
j2[J]X
p2[P](hw(T)
m;j;x(p)i)
=y(hw(T)
m;jm;yvki) +yX
(j;p)6=(jm;1)(hw(T)
m;j;x(p)i)
C3
1(1 0:1
0)3M 4 eO(3
0)
=e
(1);
where the rst inequality is by Lemma E.12. Because (E.25) holds with probability at least 1  1=d,
46

--- PAGE 47 ---
so we have prove that
P(x;y)D 
yfm(x;W(T)
0(x;y)2
k
1=d:
We then prove that, with probability at least 1  o(1), an example x2
kwill be routed to
one of the experts in Mk. For x= [yvk;ck;vk0;], we need to check that hm(x;(T))<
maxm0hm0(x;(T));8m62Mk. By Lemma E.18, we know that h(T)
m;ckimaxm0h(T)
m0;cki 
 
(K 1M 9). Further by Lemma E.14, we have that max m;kjh(T)
m;vkij=O(d 0:001). Again to
calculate test error, we need to give an upper bound h(T)
m;pi, where pis a fresh noise drawn from
N(0;(2
p=d)Id). We can upper bound the gradient of the gating network by
krmL(t)k2=1
nX
i;p1(mi;t=m)`0
i;tmi;t(xi;(t))yifmi;t(xi;W(t))x(p)
i
 1
nX
i;p`0
i;tmi;t(xi;(t))m(xi;(t))yifmi;t(xi;W(t))x(p)
i
2:
=eO(1);
where the last inequality is due to j`0
i;tj1,m;mi;t2[0;1] andkx(p)
ik2=O(1). This further
implies that
k(T)
mk2=k(T)
m (0)
mk2eO(tr)eO( 1r) =eO(1);
where the last inequality is by r= (M2). Because the inner product h(T)
m;pifollows the
distributionN(0;(2
p=d)k(T)
mk2
2), we have that with probability at least 1  1=(dPM ),
jh(T)
m;pij=O(pd 1=2k(T)
mk2log(dPM ))eO(d 1=2):
Applying Union bound for m2[M];p4 gives that, with probability at least 1  1=d,
jh(T)
m;pij=eO(d 1=2);8m2[M];p4: (E.26)
Now, under the event that (E.26) holds, we have that
hm(x;(T)) max
m0hm0(x;(T))
h(T)
m;cki max
m0h(T)
m0;cki+ 4 max
m;kjh(T)
m;vkij+ 4Pmax
m;pjh(T)
m;pij
 
(K 1M 9) +eO(d 0:001)
<0:
Because (E.26) holds holds with probability at least 1  1=d, so we have prove that with probability
at least 1 1=d, an example x2
kwill be routed to one of the experts in Mk.
Training Error is zero. The prove for training error is much easier, because we no longer need
to deal with the fresh noises and we no longer need to use high probability bound for those inner
products with fresh noises. That's the reason we can get exactly zero training error. We rst prove
47

--- PAGE 48 ---
the following result for the experts. For all expert m2Mk, we have that
yifm(xi;W(T)
0;8i2
k:
Without loss of generality, we assume that the feature patch appears in x(1)
i. By Lemma E.12,
we have that for all i2
k
yifm(xi;W(T)) =yiX
j2[J]X
p2[P](hw(T)
m;j;x(p)
ii)
=yi(hw(T)
m;jm;yivki) +yiX
(j;p)6=(jm;1)(hw(T)
m;j;x(p)i)
C3
1(1 0:1
0)3M 4 eO(3
0)
>0;
where the rst inequality is Lemma E.12. We then prove that, and example ( xi;yi)2
 will be
routed to one of the experts in Mk. Suppose the m-th expert is not in Mk. We only need to check
the value of hm(xi;(T))<maxm0hm0(xi;(T)), which is straight forward by Lemma E.18 and
Lemma E.14.
F Auxiliary Lemmas
Lemma F.1. LetfamgM
m=1are the random variable i.i.d. drawn from N(0;1). Dene the non-
increasing sequence of famgM
m=1asa(1):::a(M). Then we have that
P(a(2)(1 G)a(1))GM2
Proof. Let 	 be the CDF of N(0;1) and letbe the PDF ofN(0;2
0). Then we have that,
P(a(2)(1 G)a(1))
=Z
a(1):::a(M)1(a(2)(1 G)a(1))M!m(a(m))da
=Z
a(1)a(2)1(a(2)(1 G)a(1))M(M 1)(a(1))(a(2))	(a(2))M 2da(1)da(2)
Z
a(1)a(2)1(a(2)(1 G)a(1))M(M 1)(a(1))1p
2da(1)da(2)
=Z
a(1)0GM(M 1)p
2a(1)(a(1))da(1)
GM2:
For normalized gradient descent we have following lemma,
48

--- PAGE 49 ---
Lemma F.2 (Lemma C.19 Allen-Zhu and Li 2020c) .Letfxt;ytgt=1;::be two positive sequences
that satisfy
xt+1xt+Ctx2
t
yt+1yt+SCty2
t;
andjxt+1 xtj2+jyt+1 ytj22. Supposex0;y0=o(1);x0y0S(1 +G),
minfG2x0
log(A=x 0);G2y0
log(1=G)g:
Then we have for all A > x 0, letTxbe the rst iteration such that xtA, then we have yTx
O(y0G 1).
Proof. We only need to replace O(Aq 1) in the proof of Lemma C.19 by O(), because we use
normalized gradient descent, i.e, Ctx2
t1. For completeness, we present the whole poof here.
for allg= 0;1;2;:::;, letTgbe the rst iteration such that xt(1+)gx0, letbbe the smallest
integer such that (1+ )bx0A. For simplicity of notation, we replace xtwithAwheneverxtA.
Then by the denition of Tg, we have that
X
t2[Tg;Tg+1)Ct[(1 +)gx0]2xTg+1 xTg(1 +)gx0+O();
where the last inequality holds because we are using normalized gradient descent, i.e., max tjxt+1 
xtj. This implies that
X
t2[Tg;Tg+1)Ct
(1 +)g1
x0+O()
x2
0:
Recall that bis the smallest integer such that (1 + )bx0A, so we can calculate
X
t0;xtACtb 1X
g=0
(1 +)g1
x0
+O()
x2
0b=1 +
x0+O()b
x2
01 +
x0+O() log(A=x 0)
x2
0log(1 +)
LetTxbe the rst iteration tin whichxtA. Then we have that
TxX
t=0Ct1 +
x0+O() log(A=x 0)
x2
0: (F.1)
On the other hand, let A0=G 1y0and b' be the smallest integer such that (1 + )b0x0A0. For
simplicity of notation, we replace ytwithA0whenytA0. Then letT0
gbe the rst iteration such
thatyt(1 +)gy0, then we have that
X
t2[T0g;T0
g+1)SCt[(1 +)g+1x0](q 1)yT0
g+1 yT0g(1 +)gy0 O():
49

--- PAGE 50 ---
Therefore, we have that
X
t2[T0g;T0
g+1)SCt
(1 +)g(1 +)21
y0 O()
y2
0:
Recall that b0is the smallest integer such that (1 + )b0y0A0. wo we have that
X
t0;xtASCtb0 2X
g=0
(1 +)g(1 +)21
y0 O()b0
y2
0
LetTybe the rst iteration tin whichytA0, so we can calculate
TyX
t=0SCt1 O(+G)
y0 O() log(A0=y0)
y2
0: (F.2)
Compare (F.1) and (F.2). Choosing =GandminfG2x0
log(A=x 0);G2y0
log(1=G)g, together with x0
y0S(1 +G)
References
Allen-Zhu, Z. andLi, Y. (2019). What can ResNet learn eciently, going beyond kernels? In
Advances in Neural Information Processing Systems .
Allen-Zhu, Z. andLi, Y. (2020a). Backward feature correction: How deep learning performs
deep learning. arXiv preprint arXiv:2001.04413 .
Allen-Zhu, Z. andLi, Y. (2020b). Feature purication: How adversarial training performs robust
deep learning. arXiv preprint arXiv:2005.10190 .
Allen-Zhu, Z. andLi, Y. (2020c). Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816 .
Allen-Zhu, Z. ,Li, Y. andLiang, Y. (2019a). Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in Neural Information Processing Systems .
Allen-Zhu, Z. ,Li, Y. andSong, Z. (2019b). A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning .
Anandkumar, A. ,Ge, R. ,Hsu, D. ,Kakade, S. M. andTelgarsky, M. (2014). Tensor
decompositions for learning latent variable models. Journal of machine learning research 15
2773{2832.
Anandkumar, A. ,Hsu, D. andKakade, S. M. (2012). A method of moments for mixture
models and hidden markov models. In Conference on Learning Theory . JMLR Workshop and
Conference Proceedings.
50

--- PAGE 51 ---
Arora, S. ,Du, S. ,Hu, W. ,Li, Z. andWang, R. (2019a). Fine-grained analysis of optimization
and generalization for overparameterized two-layer neural networks. In International Conference
on Machine Learning .
Arora, S. ,Du, S. S. ,Hu, W. ,Li, Z. ,Salakhutdinov, R. andWang, R. (2019b). On exact
computation with an innitely wide neural net. In Advances in Neural Information Processing
Systems .
Bai, Y. andLee, J. D. (2019). Beyond linearization: On quadratic and higher-order approxima-
tion of wide neural networks. arXiv preprint arXiv:1910.01619 .
Balakrishnan, S. ,Wainwright, M. J. andYu, B. (2017). Statistical guarantees for the em
algorithm: From population to sample-based analysis. The Annals of Statistics 4577{120.
Blard, T. (2020). French sentiment analysis with bert. https://github.com/TheophileBlard/
french-sentiment-analysis-with-bert .
Cao, Y. ,Chen, Z. ,Belkin, M. andGu, Q. (2022). Benign overtting in two-layer convolutional
neural networks. arXiv preprint arXiv:2202.06526 .
Cao, Y. andGu, Q. (2019). Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems .
Chaganty, A. T. andLiang, P. (2013). Spectral experts for estimating mixtures of linear
regressions. In International Conference on Machine Learning . PMLR.
Collobert, R. ,Bengio, S. andBengio, Y. (2002). A parallel mixture of svms for very large
scale problems. Neural computation 141105{1114.
Dauphin, Y. N. ,Fan, A. ,Auli, M. andGrangier, D. (2017). Language modeling with gated
convolutional networks. In International conference on machine learning . PMLR.
De Veaux, R. D. (1989). Mixtures of linear regressions. Computational Statistics & Data Analysis
8227{245.
Devlin, J. ,Chang, M. ,Lee, K. andToutanova, K. (2018). BERT: pre-training of deep
bidirectional transformers for language understanding. CoRR abs/1810.04805 .
Du, S. S. ,Zhai, X. ,Poczos, B. andSingh, A. (2019). Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations .
Dua, D. ,Bhosale, S. ,Goswami, V. ,Cross, J. ,Lewis, M. andFan, A. (2021). Tricks for
training sparse translation models. arXiv preprint arXiv:2110.08246 .
Eigen, D. ,Ranzato, M. andSutskever, I. (2013). Learning factored representations in a deep
mixture of experts. arXiv preprint arXiv:1312.4314 .
Faria, S. andSoromenho, G. (2010). Fitting mixtures of linear regressions. Journal of Statistical
Computation and Simulation 80201{225.
Fedus, W. ,Zoph, B. andShazeer, N. (2021). Switch transformers: Scaling to trillion parameter
models with simple and ecient sparsity. arXiv preprint arXiv:2101.03961 .
51

--- PAGE 52 ---
Go, A. ,Bhayani, R. andHuang, L. (2009). Twitter sentiment classication using distant
supervision. CS224N project report, Stanford 12009.
He, K. ,Zhang, X. ,Ren, S. andSun, J. (2016). Deep residual learning for image recognition.
InProceedings of the IEEE conference on computer vision and pattern recognition .
Hsu, D. J. ,Kakade, S. M. andLiang, P. S. (2012). Identiability and unmixing of latent parse
trees. Advances in neural information processing systems 25.
Jacobs, R. A. ,Jordan, M. I. ,Nowlan, S. J. andHinton, G. E. (1991). Adaptive mixtures
of local experts. Neural computation 379{87.
Jacot, A. ,Gabriel, F. andHongler, C. (2018). Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems .
Jelassi, S. ,Mensch, A. ,Gidel, G. andLi, Y. (2021). Adam is no better than normalized sgd:
Dissecting how adaptivity improves gan performance .
Jordan, M. I. ,Ghahramani, Z. andSaul, L. K. (1997). Hidden markov decision trees. Advances
in neural information processing systems 501{507.
Jordan, M. I. andJacobs, R. A. (1994). Hierarchical mixtures of experts and the em algorithm.
Neural computation 6181{214.
Khalili, A. andChen, J. (2007). Variable selection in nite mixture of regression models. Journal
of the american Statistical association 1021025{1038.
Krizhevsky, A. (2009). Learning multiple layers of features from tiny images. Tech. rep.
Lewis, M. ,Bhosale, S. ,Dettmers, T. ,Goyal, N. andZettlemoyer, L. (2021). Base layers:
Simplifying training of large, sparse models. In International Conference on Machine Learning .
PMLR.
Li, Y. andLiang, Y. (2018). Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems .
Li, Y. ,Ma, T. andZhang, H. R. (2020). Learning over-parametrized two-layer neural networks
beyond ntk. In Conference on learning theory . PMLR.
Liang, P. ,Bouchard-C ^ote, A. ,Klein, D. andTaskar, B. (2006). An end-to-end discrim-
inative approach to machine translation. In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meeting of the Association for Computational
Linguistics .
Quattoni, A. ,Collins, M. andDarrell, T. (2004). Conditional random elds for object
recognition. Advances in neural information processing systems 17.
Roller, S. ,Sukhbaatar, S. ,Weston, J. et al. (2021). Hash layers for large sparse models.
Advances in Neural Information Processing Systems 3417555{17566.
52

--- PAGE 53 ---
Sandler, M. ,Howard, A. ,Zhu, M. ,Zhmoginov, A. andChen, L.-C. (2018). Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer
vision and pattern recognition .
Shazeer, N. ,Mirhoseini, A. ,Maziarz, K. ,Davis, A. ,Le, Q. ,Hinton, G. andDean, J.
(2017). Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538 .
Smetanin, S. andKomarov, M. (2019). Sentiment analysis of product reviews in russian using
convolutional neural networks. In 2019 IEEE 21st conference on business informatics (CBI) ,
vol. 1. IEEE.
Tresp, V. (2001). Mixtures of gaussian processes. Advances in neural information processing
systems 654{660.
Van der Maaten, L. andHinton, G. (2008). Visualizing data using t-sne. Journal of machine
learning research 9.
Vaswani, A. ,Shazeer, N. ,Parmar, N. ,Uszkoreit, J. ,Jones, L. ,Gomez, A. N. ,Kaiser,
 L.andPolosukhin, I. (2017). Attention is all you need. In Advances in neural information
processing systems .
Vecci, L. ,Piazza, F. andUncini, A. (1998). Learning and approximation capabilities of adaptive
spline activation function neural networks. Neural Networks 11259{270.
Wang, Y. andMori, G. (2009). Max-margin hidden conditional random elds for human action
recognition. In 2009 IEEE Conference on Computer Vision and Pattern Recognition . IEEE.
Wang, Z. ,Gu, Q. ,Ning, Y. andLiu, H. (2015). High dimensional em algorithm: Statistical
optimization and asymptotic normality. Advances in neural information processing systems 28.
Wen, Z. andLi, Y. (2021). Toward understanding the feature learning process of self-supervised
contrastive learning. In International Conference on Machine Learning . PMLR.
Yi, X. ,Caramanis, C. andSanghavi, S. (2014). Alternating minimization for mixed linear
regression. In International Conference on Machine Learning . PMLR.
Zou, D. ,Cao, Y. ,Li, Y. andGu, Q. (2021). Understanding the generalization of adam in
learning neural networks with proper regularization. arXiv preprint arXiv:2108.11371 .
Zou, D. ,Cao, Y. ,Zhou, D. andGu, Q. (2018). Stochastic gradient descent optimizes over-
parameterized deep relu networks. arXiv preprint arXiv:1811.08888 .
53
