# Mở khóa Tính mô-đun nổi lên trong các Mô hình Ngôn ngữ Lớn

12Zihan Qiu*†,3Zeyu Huang∗,1Jie Fu‡
1CSE, HKUST2IIIS, Tsinghua University,
3ILCC, University of Edinburgh
qzh11628@gmail.com, zeyu.huang@ed.ac.uk, jiefu@ust.hk

Tóm tắt
Mạng Nơ-ron Mô-đun (MNN) thể hiện nhiều ưu điểm khác nhau so với các mô hình nguyên khối. Các MNN hiện có thường là tường minh: kiến trúc mô-đun của chúng được định nghĩa trước, với các mô-đun riêng lẻ được kỳ vọng thực hiện các chức năng riêng biệt. Các công trình gần đây tiết lộ rằng có tồn tại tính mô-đun tiềm ẩn trong các transformer được huấn luyện trước tiêu chuẩn, cụ thể là Tính mô-đun nổi lên. Chúng chỉ ra rằng các cấu trúc mô-đun như vậy tự phát xuất hiện trong giai đoạn huấn luyện trước sớm. Mặc dù có lợi ích của tính mô-đun, hầu hết các Mô hình Ngôn ngữ (LM) vẫn được xử lý như các mô hình nguyên khối trong mô hình huấn luyện trước và tinh chỉnh, với tính mô-đun nổi lên của chúng bị khóa và chưa được tận dụng. Trong công trình này, tập trung vào việc mở khóa tính mô-đun nổi lên trong LM, chúng tôi chứng minh rằng các LM tiêu chuẩn có thể được tinh chỉnh như các đối tác Mixture-of-Expert (MoE) mà không cần thêm bất kỳ tham số nào. Các MoE như vậy được dẫn xuất từ tính mô-đun nổi lên và được gọi là MoE Nổi lên (EMoE). Các thí nghiệm của chúng tôi chứng minh rằng tinh chỉnh EMoE cải thiện hiệu quả khả năng tổng quát hóa trong và ngoài miền đối với nhiệm vụ hạ lưu so với tinh chỉnh vani. Phân tích và nghiên cứu khử từng phần của chúng tôi tiếp tục minh họa rằng nó mạnh mẽ với các cấu hình khác nhau và có thể mở rộng lên Mô hình Ngôn ngữ Lớn (tức là Llama2-7B và Llama-30B). Mã nguồn có sẵn tại repo này.

1 Giới thiệu
Tính mô-đun thu hút sự chú ý đáng kể từ cộng đồng trí tuệ nhân tạo (Auda và Kamel, 1999). Mạng nơ-ron với thiết kế mô-đun, được gọi là Mạng Nơ-ron Mô-đun (MNN), đã thể hiện một loạt ưu điểm, bao gồm khả năng thích ứng (Shen et al., 2023b), hiệu quả dữ liệu (Bengio et al., 2020), và khả năng tổng quát hóa tốt hơn (Goyal và Bengio, 2020; Weiss et al., 2022). Các MNN điển hình thường là mô-đun một cách tường minh. Chúng có cấu trúc mô-đun được định nghĩa trước và được kỳ vọng đạt được giải pháp chia để trị cho nhiệm vụ đã cho. Trong số các MNN khác nhau, Mixture-of-Experts (MoE) sử dụng chiến lược tính toán có điều kiện nơi các mô-đun con khác nhau - được gọi là chuyên gia - được kỳ vọng được kích hoạt bởi các loại đầu vào khác nhau. MoE đạt thành công đáng kể trong nhiều lĩnh vực khác nhau (Fedus et al., 2022; Shen et al., 2023a; Chen et al., 2023b; Mustafa et al., 2022; Bao et al., 2022) trong thời đại transformer quy mô lớn, và do đó chúng là kiến trúc mạng nơ-ron mô-đun được sử dụng rộng rãi.

Ngoài MNN tường minh, một số nghiên cứu phát hiện ra rằng các cấu trúc mô-đun tự phát xuất hiện trong quá trình huấn luyện, không chỉ trong CNN hoặc LSTM quy mô nhỏ (Csordás et al., 2021; Agarwala et al., 2021), mà cả trong các mô hình transformer được huấn luyện trước quy mô lớn. Zhang et al. (2022b); Li et al. (2022) tiết lộ các mẫu kích hoạt thưa thớt đáng chú ý trong Mạng Feed-Forward (FFN) trong các mô hình transformer được huấn luyện trước. Họ phát hiện ra rằng trong T5-Base (Raffel et al., 2020) và ViT-B16, chỉ có 3.0% và 6.3% nơ-ron được kích hoạt trong một quá trình truyền tiến, tương ứng. Hơn nữa, Zhang et al. (2023) sử dụng các nhiệm vụ ngữ nghĩa và thâm dụng tri thức được chọn thủ công để thăm dò bản chất của các nơ-ron trong FFN. Họ quan sát thấy mối tương quan mạnh mẽ giữa kích hoạt nơ-ron và các nhiệm vụ cụ thể, tiếp tục khám phá việc nhóm nơ-ron dựa trên chức năng rõ ràng của mô hình T5 được huấn luyện trước (các nơ-ron có chức năng tương tự thường được đồng kích hoạt). Họ tóm tắt hiện tượng này là Tính mô-đun Nổi lên (EM).

Mặc dù tính mô-đun nổi lên, các mô hình ngôn ngữ được huấn luyện trước thường được xử lý như các mô hình nguyên khối trong mô hình huấn luyện trước và tinh chỉnh tiêu chuẩn. Tự nhiên ta đặt câu hỏi liệu EM của chúng và các cải tiến tiềm năng mang lại bởi EM có bị khóa trong quá trình này hay không.

Trong bài báo này, chúng tôi ủng hộ việc mở khóa EM trong các mô hình ngôn ngữ được huấn luyện trước có thể mang lại cải tiến tổng quát hóa cho các nhiệm vụ hạ lưu. Cụ thể, chúng tôi chia một số tầng FFN của mô hình gốc thành các tầng MoE. MoE được dẫn xuất theo EM trong tầng đó và có thể được coi là sự bên ngoài hóa của EM. Do đó, mô hình MoE thu được được gọi là MoE Nổi lên (EMoE). Sau đó chúng tôi tinh chỉnh mô hình EMoE thu được để điều tra liệu việc mở khóa EM có khuyến khích hiệu suất nhiệm vụ hạ lưu hay không.

Chúng tôi xác nhận các phát hiện thực nghiệm với nhiều mô hình, benchmark đánh giá, phương pháp tinh chỉnh khác nhau (tinh chỉnh hiệu quả tham số và tinh chỉnh đầy đủ). Chúng tôi thấy rằng tinh chỉnh EMoE đạt hiệu suất tổng quát hóa mạnh hơn so với tinh chỉnh vani trong nhiều thiết lập thí nghiệm khác nhau, chứng minh rằng việc mở khóa EM của LM tăng cường khả năng tổng quát hóa hạ lưu của các mô hình. Chúng tôi cung cấp phân tích toàn diện cho EMoE: 1) Trước tiên chúng tôi xác nhận rằng EMoE thực sự mở khóa EM trong các mô hình ngôn ngữ được huấn luyện trước bằng cách thể hiện việc lựa chọn chuyên gia cụ thể theo nhiệm vụ. 2) Sau đó chúng tôi tiết lộ rằng EMoE cải thiện việc cập nhật tham số trong quá trình tinh chỉnh và thậm chí có thể bị loại bỏ sau đó. Chúng tôi muốn nhấn mạnh rằng thuộc tính này cải thiện thêm tính thực tế của EMoE vì kiến trúc mô hình không thay đổi trước và sau tinh chỉnh. Trong khi đó, các nghiên cứu khử từng phần của chúng tôi cho thấy độ mạnh mẽ của EMoE đối với các cấu hình siêu tham số khác nhau. 3) Chúng tôi cũng kết luận rằng EMoE có thể che dấu các nơ-ron có hiệu ứng chuyển giao tiêu cực. Chúng tôi hy vọng các khám phá nghiên cứu của chúng tôi có thể mang lại những hiểu biết mới và phục vụ như một nỗ lực thí dụ hướng tới việc mở khóa thêm EM của LLM.

2 Phương pháp luận

2.1 Kiến thức cơ bản

Transformer FFN là Bộ nhớ Key-Value. Tầng FFN trong khối transformer thường bao gồm trọng số K∈Rh×d, V∈Rd×h, trong đó h là kích thước embedding và d là chiều của tầng ẩn (thường d = 4h), và một hàm kích hoạt phi tuyến σ(·). Đối với đầu vào x∈Rh, đầu ra y∈Rh có thể được tính như Phương trình 1:

y = FFN(x;K,V) = σ(x·K)·V. (1)

Cụ thể hơn, đối với mỗi cột K:,i và hàng Vi,:, Phương trình 1 có thể được viết lại như:

y = σ(x·K)·V = Σ(i=1 đến h) σ(x·K:,i)·Vi,: (2)

Theo Geva et al. (2021, 2022); Huang et al. (2023), chúng tôi coi các cột trong K là các vectơ khóa và các hàng trong V là các vectơ giá trị, đầu ra của mạng FFN có thể được xem như tổng có trọng số của các vectơ giá trị dựa trên điểm kích hoạt σ(x·K). Đối với phần còn lại của bài báo này, chúng tôi đề cập đến một cặp bộ nhớ khóa-giá trị sử dụng nơ-ron và việc đồng kích hoạt các nơ-ron sử dụng tính mô-đun.

Mixture-of-Experts Trong transformer, MoE thường được thực hiện bằng cách thay thế FFN gốc bằng một nhóm FFN song song và giới thiệu mô-đun cổng. Giả sử có N chuyên gia: {FFNn(·;Kn,Vn)|n∈[1,N]}, cổng g(·;G,k), được định nghĩa với tham số G và số nguyên k, là để ánh xạ đầu vào x tới phân phối điểm của chuyên gia g(x;G,k)∈RN. Thông thường, g được thực hiện với một tầng tuyến tính đơn giản theo sau bởi hàm softmax và hàm Top-k. Cho x∈Rh, đầu ra y∈Rh có thể được tóm tắt như tổng có trọng số của đầu ra từ tất cả chuyên gia:

y = Σ(n∈N) gn(x;G,k) FFNn(x;Kn,Vn) (3)

Khi k cho Top-K nhỏ hơn N, chỉ có một nhóm con chuyên gia tham gia vào tính toán, được gọi là MoE thưa thớt.

2.2 Mixture-of-Expert Nổi lên

Theo Phương trình 2 và Phương trình 3, chúng tôi thấy rằng FFN bên trong giống MoE nếu chúng ta coi khóa như mô-đun cổng và giá trị như nhóm chuyên gia, điều này truyền cảm hứng cho chúng tôi chuyển đổi FFN hiện có thành MoE thưa thớt để mở khóa tiềm năng mô-đun của nó. Vì mục tiêu nghiên cứu của chúng tôi tập trung chủ yếu vào EM, một phương pháp ưu tiên để bên ngoài hóa EM thành MoE thưa thớt không nên giới thiệu tham số, huấn luyện và dữ liệu bổ sung, điều này có thể dẫn đến độ thiên lệch không thực tế hoặc không mong muốn. Do đó, sau khi chia các nơ-ron gốc thành các nhóm khác nhau để xây dựng các chuyên gia khác nhau, trung bình của các vectơ khóa của mỗi nhóm được tính toán để tạo thành mô-đun cổng. Chi tiết được mô tả và minh họa trong Hình 1.

Xây dựng Chuyên gia dựa trên Clustering. Chúng tôi mục tiêu đảm bảo rằng các nơ-ron có xu hướng được đồng kích hoạt được chia vào cùng một nhóm. Vì các nơ-ron có vectơ khóa tương tự có xu hướng được đồng kích hoạt theo Phương trình 2, chúng tôi chia chúng thành các chuyên gia riêng biệt bằng cách cluster các vectơ khóa của chúng. Cụ thể, cho một tầng FFN FFN(·;K,V), chúng tôi thực hiện clustering có ràng buộc (Malinen và Fränti, 2014) (chi tiết thêm trong Phụ lục A.0.1) để phân chia tất cả vectơ khóa K thành N chuyên gia trung bình. Ký hiệu chỉ số của các khóa trong nhóm thứ i là Ei⊂[d], đối với ∀j∈Ei, chúng tôi trích xuất nơ-ron (K:,j,Vj,:) để tạo thành chuyên gia thứ i FFN(·;Ki,Vi) như mô tả trong Hình 1(b). Sau đó, tính toán của mỗi chuyên gia tuân theo Phương trình 1.

Avg-k Gating. Như đã thảo luận, chúng tôi không muốn giới thiệu các tham số có thể huấn luyện bổ sung khi bên ngoài hóa EM. Do đó, chúng tôi xây dựng mô-đun cổng bằng cách tính trung bình khóa của mỗi chuyên gia, điều này sẽ định tuyến đầu vào x tới các chuyên gia có xu hướng có điểm kích hoạt lớn hơn và do đó đóng góp nhiều hơn vào đầu ra của mô hình. Hàm cổng thường được thực hiện bởi một tầng đơn G∈Rh×N, trong trường hợp avg-k gating, trọng số trong cột thứ i G:,i có thể được tính như sau:

G:,i = Avg(Ki,dim=0). (4)

Và sau đó điểm cổng cho chuyên gia thứ i là:

gi(x;G,k) = {1 nếu i∈Top-K(x·G;k), 0 ngược lại} (5)

trong đó Top-K(·;k) trả về chỉ số của k phần tử lớn nhất của đầu vào cho theo. Vì điểm cổng là trung bình của điểm kích hoạt (trước σ(·)) của các nơ-ron trong chuyên gia đó:

x·G:,i = x·Avg(Ki) = (N/d)Σ(j∈Ei) x·Ki:,j = (N/d)Σj aj. (6)

một giá trị lớn hơn của điểm cổng gi ngụ ý nhiều khóa được kích hoạt hơn trong chuyên gia tương ứng. Do đó, chuyên gia có thể đóng góp nhiều hơn vào đầu ra y cho đầu vào x. Trong quá trình tinh chỉnh hạ lưu, trọng số cổng được liên kết với tham số FFN sử dụng Phương trình 4.

3 Thí nghiệm

Cấu hình: Trước tiên chúng tôi đánh giá EMoE sử dụng các mô hình dòng BERT và GPT2. Chúng tôi sử dụng GLUE (Wang et al., 2019b) và GLUE-X (Yang et al., 2023) để đánh giá hiệu suất trong miền (ID) và ngoài miền (OOD) của mô hình được tinh chỉnh, tương ứng. Chúng tôi chủ yếu trình bày kết quả thí nghiệm khi sử dụng LoRA (Hu et al., 2022) để tinh chỉnh các mô hình ngôn ngữ được huấn luyện trước vì hai lý do: (1) với quy mô ngày càng tăng của các mô hình được huấn luyện trước, tinh chỉnh hiệu quả tham số (Houlsby et al., 2019) có thể mở rộng lên các mô hình ngôn ngữ rất lớn và do đó trở nên thực tế hơn. (2) trọng số LoRA tiêu chuẩn được thêm vào mỗi khối self-attention, và các tham số trong FFN sẽ không được cập nhật, cho phép chúng tôi điều tra liệu việc tận dụng EM, ngay cả khi không tinh chỉnh tham số của EMoE, có thể mang lại cải tiến hay không. Chúng tôi trình bày kết quả tinh chỉnh đầy đủ EMoE trong Phụ lục B.1. Ngoài ra, chúng tôi mở rộng EMoE trên Llama2-7B và Llama-30B (Touvron et al., 2023) để xác nhận thêm. Chúng tôi tinh chỉnh hướng dẫn các mô hình trên Alpaca và kiểm tra chúng trên benchmark MMLU (Hendrycks et al., 2021). Để biết chi tiết thêm về bộ dữ liệu, thước đo đánh giá và chi phí tính toán, vui lòng tham khảo Phụ lục D và E.

Baseline Các baseline của chúng tôi bao gồm (1) tinh chỉnh LoRA vani: thêm trọng số LoRA vào các phép chiếu q và v trong tầng attention; (2) GMoE (Li et al., 2023): Thay vì chia tách, GMoE sao chép tầng FFN và huấn luyện tầng cổng mới để giới thiệu cấu trúc MoE từ các khối transformer thứ hai cuối và thứ tư cuối trong các mô hình được huấn luyện trước gốc. Vì GMoE sao chép FFN của mô hình được huấn luyện trước để tạo thành MoE, nó không hiệu quả nếu MoE được giới thiệu không được tinh chỉnh. Do đó, chúng tôi tiến hành thí nghiệm với tinh chỉnh LoRA cho GMoE và tinh chỉnh khối transformer nơi FFN gốc được thay thế. (3) EMoE-learn: một phương pháp khử từng phần, nơi hàm cổng được học (giống như GMoE) trong quá trình tinh chỉnh. Điều này giúp chúng tôi hiểu rõ hơn về tác động của avg-k gating.

Siêu tham số: những tham số không liên quan đến MoE (ví dụ: tỷ lệ học, kích thước batch) vẫn nhất quán với baseline. Theo Li et al. (2023), chúng tôi thay thế FFN gốc bằng tầng EMoE trong {hai tầng chẵn cuối cùng, một tầng chẵn cuối cùng}. Tìm kiếm siêu tham số tương đương được tiến hành cho cả GMoE và EMoE cho số lượng chuyên gia N và top-k: GMoE khám phá N trong {4, 8} và top-k trong {1, 2}; đối với EMoE, N được cố định ở 64, với top-k được khám phá trong {16, 32}. Lý do cơ bản cho N và top-k khác nhau trong GMoE và EMoE là EMoE phân giải FFN gốc thành chuyên gia, và tổng tham số của tất cả chuyên gia vẫn giống như FFN gốc. Ngược lại, GMoE sao chép FFN gốc từ kiến trúc MoE; N và top-k lớn hơn mà họ chọn, càng nhiều tham số và chi phí tính toán mà họ giới thiệu. Nghiên cứu khử từng phần của chúng tôi chỉ ra rằng trong khi tìm kiếm siêu tham số cẩn thận hơn có thể mang lại hiệu suất vượt trội, việc tuân thủ tỷ lệ top-k/N = 0.25 hoặc 0.5 cho EMoE liên tục mang lại cải tiến so với tinh chỉnh vani.

Thước đo Đánh giá Tất cả thí nghiệm ngoại trừ những thí nghiệm mở rộng đều được lặp lại ba lần, và trung bình được trình bày trong phần chính. Kết quả đầy đủ trong Phụ lục F.1. Đối với thước đo OOD, chúng tôi theo GLUE-X (Yang et al., 2023) và sử dụng thứ hạng Friedman (Friedman, 1940) rank_f = (1/n)Σ(i=1 đến n) rank_i. Đối với mỗi phương pháp dưới cùng một backbone, rank_i được tạo ra dựa trên kết quả tốt nhất và trung bình của mỗi bộ dữ liệu. Đối với 13 nhiệm vụ OOD được sử dụng trong GLUE-X, mỗi phương pháp tạo ra 26 giá trị rank_i. Kết quả OOD được trình bày trong Bảng 1 đại diện cho trung bình của tất cả các giá trị rank_i này. Kết quả gốc của mỗi nhiệm vụ có thể được tìm thấy trong Phụ lục F.1.

Kết quả với BERT và GPT2 Theo Bảng 1: (1) EMoE thể hiện cải tiến so với tinh chỉnh LoRA vani. Đặc biệt, EMoE cũng đạt kết quả tương đương với GMoE trên BERT-large và vượt trội GPT2-XL với ít tham số có thể học hơn nhiều. (2) Trong khi kết quả EMoE-learn tốt hơn EMoE trong một số nhiệm vụ (STSB, QNLI), EMoE thể hiện sự ổn định cao hơn EMoE-learn và mang lại kết quả tổng thể vượt trội. (3) Cấu trúc MoE cải thiện hiệu suất OOD (GMoE, EMoE, EMoE-learn vs. LoRA).

Kết quả với Llama Không giống như GMoE, EMoE không giới thiệu bất kỳ tham số hoặc quy trình có thể huấn luyện bổ sung nào để chúng tôi có thể mở rộng EMoE lên các mô hình kích thước 7B và 30B để xác nhận thêm hiệu quả của nó. Từ Bảng 2: 1. EMoE liên tục mang lại cải tiến so với tinh chỉnh LoRA với tính toán bổ sung không đáng kể. 2. Vì EMoE không giới thiệu bất kỳ tham số có thể huấn luyện bổ sung nào, việc lựa chọn tỷ lệ K và N vẫn lớn (có thể thậm chí lớn hơn) và có thể áp dụng trong các mô hình quy mô lớn hơn. Trong khi hiệu suất thay đổi khi sử dụng N và K khác nhau, EMoE liên tục vượt trội so với tinh chỉnh LoRA vani. Điều này cho thấy rằng mặc dù các siêu tham số bổ sung N và K được giới thiệu, chúng không dẫn đến thách thức về khả năng sử dụng. Vui lòng tham khảo Phần 5 để biết thêm kết quả khử từng phần về siêu tham số.

Kết quả Bổ sung Hơn nữa, chúng tôi tiến hành thí nghiệm với tinh chỉnh đầy đủ dưới các thiết lập đánh giá toàn diện: 1. Benchmark OOD thị giác Domainbed (Gulrajani và Lopez-Paz, 2021), tinh chỉnh đầy đủ ViT-Small (22M) và ViT-Base (86M). 2. Benchmark GLUE, tinh chỉnh đầy đủ BERT-Base, BERT-Large, và GPT2-Small. 3. Tinh chỉnh đầy đủ Llama2-7B. Chúng tôi trình bày kết quả và phân tích chi tiết trong Phụ lục B.1. Theo kết quả, EMoE mang lại cải tiến nhất quán so với tinh chỉnh đầy đủ tiêu chuẩn qua nhiều nhiệm vụ và quy mô mô hình khác nhau. Trên benchmark OOD thị giác Domainbed, kiểm soát nghiêm ngặt thước đo đánh giá, EMoE đạt kết quả tương đương với GMoE tiên tiến. Hơn nữa, khi được tinh chỉnh đầy đủ với Alpaca, EMoE thể hiện cải tiến đáng chú ý 1.58 trên MMLU so với baseline tinh chỉnh tiêu chuẩn. Những phát hiện này nhấn mạnh hiệu quả của EMoE trong việc nâng cao hiệu suất mô hình qua nhiều kiến trúc và nhiệm vụ khác nhau.

4 Phân tích

4.1 EMoE có Mở khóa Tính mô-đun Nổi lên không?

Trước tiên chúng tôi điều tra liệu việc phân vùng clustering dựa trên vectơ khóa đơn giản có thể nắm bắt được mẫu mô-đun của kích hoạt nơ-ron hay không. Điểm kích hoạt của các nơ-ron khác nhau trên các đầu vào khác nhau được hiển thị trực quan trong Hình 2 trái và giữa. Trước clustering (Hình 2 trái), việc kích hoạt các nơ-ron có vẻ ngẫu nhiên. Sau khi sắp xếp lại các điểm kích hoạt đó theo phân vùng EMoE (Hình 2 giữa), chúng tôi quan sát thấy rằng chỉ có một phần các nơ-ron được sử dụng thường xuyên trong nhiệm vụ này, và các cluster kích hoạt rõ ràng xuất hiện. Điều này chứng minh rằng clustering dựa trên vectơ khóa có thể phân giải các thành phần mô-đun trong mô hình tiêu chuẩn. Sau đó chúng tôi đi sâu vào việc sử dụng chuyên gia qua các nhiệm vụ khác nhau trong EMoE. Bản đồ nhiệt giữa nhiệm vụ và chuyên gia (Hình 2 phải) cho thấy rằng việc sử dụng chuyên gia khác nhau giữa các nhiệm vụ, trong khi một số chuyên gia quan trọng gần như được sử dụng bởi tất cả 8 nhiệm vụ. Ví dụ, chuyên gia 19, được sử dụng nhiều trong QQP và MNLI, chỉ được kích hoạt một chút trong MRPC và RTE. Ngược lại, chuyên gia 33 được kích hoạt thường xuyên bởi tất cả 8 nhiệm vụ.

4.2 EMoE cải thiện hiệu suất tinh chỉnh như thế nào?

Mặc dù EMoE đạt cải tiến đáng chú ý trong cả kịch bản ID và OOD, việc đơn giản chuyển đổi FFN được huấn luyện trước thành MoE trước khi tinh chỉnh có thể mang lại những lợi ích như vậy là không hiển nhiên, đặc biệt khi sử dụng tinh chỉnh LoRA, nơi phần cổng và chuyên gia không được cập nhật. Chúng tôi điều tra cơ chế đằng sau hiệu suất tăng cường.

Chúng tôi sử dụng BERT-Large làm mô hình backbone.

EMoE có lợi cho việc học trọng số LoRA thay vì ảnh hưởng đến suy luận. EMoE và mô hình tiêu chuẩn chỉ khác nhau trong kích hoạt FFN. Những khác biệt như vậy có thể (1) tác động trực tiếp đến đầu ra trong quá trình suy luận và (2) ảnh hưởng đến việc cập nhật tham số trong quá trình huấn luyện. Dựa trên điều này, chúng tôi đề xuất hai biến thể và so sánh chúng với tinh chỉnh LoRA vani và EMoE: (a) LoRA2EMoE: Sử dụng LoRA để tinh chỉnh mô hình gốc và chia nó thành EMoE trong quá trình suy luận. Nếu nó vượt trội so với tinh chỉnh LoRA vani, chúng ta có thể suy luận rằng EMoE chủ yếu tác động đến suy luận mô hình. (b) EMoE2LoRA: Sử dụng LoRA để tinh chỉnh mô hình EMoE và hợp nhất chuyên gia thành FFN gốc trong quá trình suy luận. Nếu không có thay đổi nào xảy ra, điều này ngụ ý rằng EMoE chủ yếu cải thiện việc cập nhật tham số của giai đoạn tinh chỉnh.

Theo Hình 3, việc thực hiện kích hoạt thưa thớt trong quá trình kiểm tra không đóng góp vào tổng quát hóa tốt hơn trung bình (LoRA2EMoE). Tuy nhiên, khi hợp nhất EMoE trở lại FFN gốc sau tinh chỉnh, hiệu suất vẫn tốt hơn đáng kể so với tinh chỉnh LoRA vani và gần như giống hệt với EMoE (EMoE2LoRA và EMoE). Vui lòng tham khảo Phụ lục F.1 Bảng 16 để biết kết quả đầy đủ. Nhờ thuộc tính này, chúng ta có thể sử dụng EMoE trong quá trình tinh chỉnh và sau đó chuyển đổi các mô hình thành mô hình tiêu chuẩn. Điều này cho phép mô hình tận hưởng lợi ích của EM mà không có bất kỳ thay đổi nào đối với việc triển khai, điều này cải thiện tính thực tế của EMoE trong thời đại LLM. Chúng tôi tiếp tục xác nhận điều này trên các mô hình dòng Llama và các phát hiện là nhất quán. Do đó, trong Bảng 2, chúng tôi báo cáo kết quả sử dụng EMoE để tinh chỉnh và kiến trúc Llama tiêu chuẩn để đánh giá. Và chúng tôi nhấn mạnh rằng thuộc tính này cũng có thể được áp dụng trong thiết lập tinh chỉnh đầy đủ, như được minh họa trong Phụ lục B.1 Bảng 7.

EMoE che dấu các nơ-ron có tác động chuyển giao tiêu cực. Sự khác biệt duy nhất giữa EMoE và tinh chỉnh LoRA vani là EMoE chặn một số nơ-ron được kích hoạt trong quá trình huấn luyện bằng việc lựa chọn chuyên gia Top-k. Dựa trên điều này, chúng tôi đưa ra giả thuyết rằng tác động của EMoE bắt nguồn từ việc ngăn chặn chuyển giao tri thức tiêu cực từ các nơ-ron bị chặn. Do đó, chúng tôi điều tra liệu có tồn tại những chuyển giao tiêu cực như vậy hay không. Cụ thể, chúng tôi nghiên cứu các biến thể lựa chọn chuyên gia sau: (1) Bottom-k: chọn k chuyên gia có điểm thấp nhất; (2) Not-top-k: chọn N-k chuyên gia không thuộc top-k chuyên gia. Các biến thể này được đánh giá qua sáu nhiệm vụ từ GLUE. Kết quả trung bình trong Hình 4. Kết quả đầy đủ có thể được tìm thấy trong Phụ lục F.1 Bảng 17. Kết quả tinh chỉnh LoRA với lựa chọn chuyên gia Bottom-k và Not-top-k kém hơn tinh chỉnh LoRA vani, trong khi Top-K vượt trội. Chúng tôi cũng kiểm tra tỷ lệ kích hoạt nơ-ron (số lượng nơ-ron được kích hoạt trong chuyên gia được chọn so với số lượng trong FFN) qua các biến thể này. Kết quả đầy đủ có thể được tìm thấy trong Phụ lục F.2 Bảng 21. Tỷ lệ kích hoạt cho ba biến thể này là khoảng 0.43 cho Top-k, 0.57 cho Not-top-k, và 0.12 cho Bottom-k, tương ứng. Đáng chú ý, Not-top-k tụt hậu đáng kể so với Top-k, mặc dù nó liên quan và kích hoạt nhiều nơ-ron hơn, cho thấy sự sụt giảm hiệu suất có liên quan nhiều hơn đến thuộc tính của các nơ-ron. Điều này tiếp tục chứng thực rằng các nơ-ron bị che dấu có tác động chuyển giao tiêu cực.

Được truyền cảm hứng từ điều này, chúng tôi chọn một thiết lập học đa nhiệm vụ nơi chuyển giao tiêu cực có thể rõ ràng hơn. Chúng tôi áp dụng cơ sở mã từ ATTEMPT (Asai et al., 2022). Đối với kịch bản ID, chúng tôi theo ATTEMPT và chọn sáu nhiệm vụ nhỏ. Đối với kịch bản OOD, chúng tôi huấn luyện các mô hình trên hai bộ dữ liệu suy luận ngôn ngữ tự nhiên (NLI) lớn hơn và kiểm tra chúng trên bốn bộ dữ liệu NLI từ các miền khác nhau. Tất cả siêu tham số không liên quan đến MoE đều nhất quán với baseline. Chúng tôi liệt kê các siêu tham số liên quan đến MoE và kết quả trung bình trong Bảng 3. Chi tiết thêm về thiết lập thí nghiệm và kết quả trong Phụ lục B.2. Chúng ta có thể quan sát thấy rằng EMoE thể hiện cải tiến đáng kể hơn so với baseline. Trong thiết lập ID, cải tiến cao nhất đạt 7.56, ngay cả khi xem xét hiệu suất trung bình qua sáu nhiệm vụ. Trong thiết lập OOD, kết quả OOD trung bình cao nhất qua bốn bộ dữ liệu cũng cải thiện 1.58.

5 Nghiên cứu Khử từng phần

Tính Thưa thớt và Tính mô-đun Để cung cấp bằng chứng thêm rằng cải tiến của EMoE bắt nguồn từ việc tận dụng tính năng mô-đun hơn là chỉ kích hoạt thưa thớt hoặc kiến trúc MoE, chúng tôi so sánh kết quả của (1) xây dựng chuyên gia dựa trên clustering vectơ khóa và (2) xây dựng ngẫu nhiên. Chúng tôi sử dụng cùng thiết lập của Phần 4.2. Những thay đổi tương đối trong kết quả trung bình so với tinh chỉnh vani được hiển thị trong Bảng 4, trong khi kết quả đầy đủ có thể được tìm thấy trong phụ lục F.1, Bảng 17. Đáng chú ý là trong khi cluster top-k thể hiện cải tiến đáng kể so với tiêu chuẩn, random top-k ngược lại kém hơn tinh chỉnh vani. Điều này cho thấy rằng xây dựng ngẫu nhiên có thể tác động tiêu cực đến cổng, và chỉ cấu trúc MoE bản thân không thể mang lại cải tiến. Hơn nữa, khi chọn trọng số với chuyển giao tiêu cực dưới lựa chọn bottom-k, quan sát thấy rằng cluster bottom-k cũng đạt kết quả thấp hơn. Tóm lại, các phương pháp dựa trên clustering có thể bên ngoài hóa tính mô-đun tiềm ẩn trong mô hình ngôn ngữ được huấn luyện trước. Trong các khung thích hợp như MoE, tính mô-đun như vậy có thể tạo thuận lợi cho tinh chỉnh hạ lưu.

Phương pháp Xây dựng MoE Trong EMoE, các phương pháp xây dựng chuyên gia và phương pháp cổng là then chốt. Để hiểu rõ hơn về việc chia tách so với nhân bản FFN, avg-k gating so với learned gating, chúng tôi trực quan hóa việc lựa chọn chuyên gia của GPT2-XL được mô-đun hóa trong quá trình tinh chỉnh trên 6 nhiệm vụ với 16 chuyên gia. Trong Hình 5, chúng tôi thể hiện kết quả cho bộ dữ liệu lớn nhất QNLI trong số đó. Kết quả đầy đủ có sẵn trong Phụ lục G. Quan sát của chúng tôi là: (1) Cả avg-k gating và learned gating đều hội tụ, như được chỉ ra bởi nửa dưới của các đồ thị. (2) avg-k gating ổn định hơn learned gating. Như được hiển thị trong Hình 5 (a) và (b), việc lựa chọn chuyên gia chỉ thay đổi một chút trong quá trình tinh chỉnh. Điều này có thể giảm thiểu sự kém hiệu quả dữ liệu từ sự không nhất quán cổng qua các giai đoạn huấn luyện khác nhau (Zuo et al., 2022a). (3) EMoE, với các tham số chuyên gia được khởi tạo khác nhau, thể hiện cân bằng tải tốt hơn GMoE (chuyên gia 11 trong Hình 5 c được chọn thường xuyên hơn nhiều so với các chuyên gia khác). Trong GMoE, tất cả chuyên gia chia sẻ khởi tạo giống hệt nhau; trong EMoE, các chuyên gia được dẫn xuất từ FFN với EM. Điều này cũng cho thấy khởi tạo tốt có thể tạo thuận lợi cho việc học MoE, như được chỉ ra bởi Nie et al. (2021).

Hình 5 (a) và Phụ lục G chứng minh rằng một số chuyên gia trong EMoE hầu như không được chọn trong quá trình tinh chỉnh. Điều này có sự tương đồng với pruning, dẫn đến lập luận rằng cải tiến của EMoE có thể bắt nguồn từ pruning. Trong bối cảnh này, tiếp tục khám phá từ 4.2 về việc kiểm tra EMoE trong quá trình huấn luyện và suy luận, chúng tôi bổ sung hai khử từng phần dưới cùng thiết lập: 1. Training-Pruning: Huấn luyện EMoE (N=64, top-k=32), pruning chuyên gia có tần suất lựa chọn thấp hơn, và đánh giá mô hình đã được pruning. 2. Pruning-Training: pruning chuyên gia có tần suất lựa chọn thấp hơn của mô hình mới theo bước 1, sau đó huấn luyện và đánh giá mô hình đã được pruning. Kết quả thí nghiệm được chi tiết trong Bảng 22. Các phát hiện chính của chúng tôi là: 1. Khi các chuyên gia được chọn thưa thớt bị pruning sau tinh chỉnh EMoE, hiệu suất liên tục vượt trội so với baseline LoRA (ngay cả khi chỉ 1/16 chuyên gia được sử dụng). Điều này nhấn mạnh rằng tác động của EMoE được thể hiện trong giai đoạn tinh chỉnh, không phải suy luận. 2. Trong thiết lập pruning-training tối ưu, hiệu suất vượt trội so với baseline nhưng thua kém đáng kể so với EMoE. Điều này cho thấy rằng cải tiến của EMoE không chỉ do pruning. So với EMoE chọn chuyên gia cho mỗi token, pruning che dấu cùng các nơ-ron cho tất cả token. Do đó, pruning cũng có thể được coi là EMoE cấp nhiệm vụ, do đó ít hiệu quả hơn. Cuối cùng, điều quan trọng cần nhấn mạnh là trong học đa nhiệm vụ, các phương pháp pruning gặp khó khăn trong việc xác định các nơ-ron cần pruning, trong khi EMoE thể hiện hiệu quả đáng kể trong các kịch bản này.

Cấu hình MoE Ngoài các thiết lập được chi tiết trong phần kết quả chính, dựa trên N = 64 chuyên gia và top-k ∈ {16,32,48}, chúng tôi cũng trình bày các kịch bản cụ thể nơi N∈{16,32}, và top-k thay đổi trong {2,4,8,16} trong Hình 6. Đáng chú ý, trong mỗi thiết lập này, (1) EMoE liên tục vượt trội so với mô hình tiêu chuẩn, minh họa độ mạnh mẽ của nó đối với siêu tham số. (2) Trung bình, avg-k gating thể hiện hiệu suất vượt trội hơn learned gating. Mặc dù learned gating (EMoE-learn) vượt trội so với avg-k gating trong một số thiết lập cụ thể (Hình 6 (b) và (e)). Điều này nhất quán với kết quả trước đó trong Phần 3. Về số lượng tầng EMoE nên được giới thiệu, các phát hiện của chúng tôi phù hợp với những phát hiện trong GMoE, cho thấy rằng chỉ có một số lượng hạn chế các tầng có thể được chuyển đổi thành tầng EMoE. Nếu quá nhiều tầng EMoE được giới thiệu, hiệu suất suy giảm. Lấy GPT2-XL (48 tầng) làm ví dụ, khi giới thiệu EMoE mỗi hai tầng trong nửa sau, hiệu suất trung bình qua 5 nhiệm vụ GLUE (79.36) khớp với mô hình tiêu chuẩn (78.87). Tuy nhiên, khi áp dụng EMoE cho mỗi 2 tầng cho toàn bộ mô hình, hiệu suất tụt hậu một chút so với mô hình tiêu chuẩn (78.17) nhưng vượt trội so với EMoE-learn (75.87). Đối với cấu hình bổ sung, vui lòng tham khảo phụ lục F.2 Bảng 19.

6 Công trình Liên quan

Các công trình liên quan nhất đến công trình của chúng tôi là giới thiệu tính mô-đun dựa trên các mô hình được huấn luyện trước có sẵn. Ví dụ, GMoE (Li et al., 2023) và Upcycling (Komatsuzaki et al., 2023) sao chép FFN từ mô hình transformer đã huấn luyện để tạo thành kiến trúc MoE. Cấu trúc mô-đun của chúng được giới thiệu bằng cách nhân bản các mô-đun FFN hiện có, để lại EM trong FFN được huấn luyện trước chưa được khám phá. MoEfication (Zhang et al., 2022b) và MoEBert (Zuo et al., 2022b) khám phá EM trong mô hình. Họ tìm cách cải thiện hiệu quả suy luận bằng cách phân giải tầng FFN gốc thành MoE thưa thớt. Họ sử dụng kích hoạt chuyên gia thưa thớt để giảm chi phí suy luận và không đề cập đến việc EM ảnh hưởng đến hiệu suất của mô hình trong tinh chỉnh hạ lưu như thế nào. Phương pháp của chúng tôi để chia FFN được áp dụng từ một phương pháp đơn giản trong các nghiên cứu khử từng phần của bài báo MoEfication (Zhang et al., 2022a): xây dựng chuyên gia dựa trên clustering và avg-k gating. Chúng tôi thực nghiệm thấy rằng một phương pháp đơn giản như vậy có thể xác nhận các cải tiến mang lại bởi EM và trả lời các câu hỏi nghiên cứu của chúng tôi. Do đó chúng tôi để lại các phương pháp tinh vi hơn cho các công trình tương lai. Vui lòng tham khảo Phụ lục C để so sánh chi tiết EMoE với các công trình liên quan đó.

7 Kết luận

Trong công trình này, chúng tôi xác nhận rằng việc mở khóa EM trong LM tiêu chuẩn cải thiện hiệu suất ID và OOD của nhiệm vụ hạ lưu. EMoE có thể mang lại lợi ích này mà không thêm bất kỳ tham số nào, chi phí huấn luyện đáng kể, hoặc bất kỳ thay đổi nào đối với việc triển khai, điều này cải thiện tính thực tế của nó trong thời đại LLM. Một lý do có thể là cấu trúc mô-đun có thể giảm thiểu tác động chuyển giao tiêu cực có mặt trong LM. Chúng tôi hy vọng các phát hiện của chúng tôi có thể làm sâu sắc thêm sự hiểu biết về tính mô-đun của mạng nơ-ron, tiếp tục giúp cộng đồng phát triển các kiến trúc mạng nơ-ron mô-đun tinh vi hơn và sử dụng các LM hiện có.

8 Hạn chế

Mục tiêu chính của chúng tôi là điều tra tiện ích của EM, và do đó, chúng tôi chủ yếu áp dụng các kỹ thuật từ MoEfication để phân giải. Chúng tôi khuyến khích nghiên cứu thêm để đề xuất các thuật toán cải tiến để khai thác EM. Các phát hiện nghiên cứu của chúng tôi chưa được xác nhận trên các nhiệm vụ thách thức hơn (ví dụ: Lý luận Toán học (Imani et al., 2023)). Trong khi phân tích của chúng tôi chủ yếu được tiến hành trên các mô hình có số lượng tham số tối đa 1.5B, chúng tôi xác nhận khả năng mở rộng của EMoE lên Llama-30B.
