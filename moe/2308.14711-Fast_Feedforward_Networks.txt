# 2308.14711.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2308.14711.pdf
# File size: 905220 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Fast Feedforward Networks
Peter Belcak and Roger Wattenhofer
ETH Z ¨urich
{belcak,wattenhofer }@ethz.ch
Abstract
We break the linear link between the layer size and its infer-
ence cost by introducing the fast feedforward1(FFF) archi-
tecture, a log-time alternative to feedforward networks.
We demonstrate that FFFs are up to 220x faster than feedfor-
ward networks, up to 6x faster than mixture-of-experts net-
works, and exhibit better training properties than mixtures of
experts thanks to noiseless conditional execution.
Pushing FFFs to the limit, we show that they can use as little
as 1% of layer neurons for inference in vision transformers
while preserving 94.2% of predictive performance.
Introduction
The feedforward layer is a parameter-heavy building block
of transformer models (Vaswani et al. 2017). Growing to
tens of thousands of hidden neurons in recent years, the cost
of feedforward layer inference is now in the sights of those
seeking to make large models faster.
It has been recognized that in very large networks, only
a small portion of the feedforward hidden neurons plays a
role in determining the output for any single input, and that
it is possible to design networks that are modular in order to
utilize this fact (Bengio et al. 2015).
The most recent work on the modularization of feedfor-
ward layers aims at architectural designs that implicitly en-
courage sparsity (Shazeer et al. 2017; Lepikhin et al. 2020;
Fedus, Zoph, and Shazeer 2022). They share the common
approach of subdividing the feedforward layer into separate
blocks of neurons – “experts” – and training a gating layer
to determine the mixture of experts to be used in the forward
pass. Inference acceleration is then achieved by using only
the best-scoring kblocks, or a variant thereof. This approach
scales down the inference time by a constant but remains lin-
ear in the width of the feedforward layer. Moreover, it relies
on noisy gating to allow for load balancing among the ex-
perts, complicating training and encouraging duplicity.
Outline. We introduce the Fast Feedforward (FFF) archi-
tecture – a peer of the feedforward (FF) architecture that
accesses blocks of its neurons in logarithmic time. FFF di-
vides the input space into disjoint regions by means of a dif-
ferentiable binary tree and performs simultaneous learning
1https://github.com/pbelcak/fastfeedforward
Figure 1: A fast feedforward network set in comparison to its
peers. Bottom. Illustrations of the resulting regionalization
of the input space and varying boundary hardness.
of the region boundaries and the neural blocks assigned to
these regions. This is achieved by tree-conditional execution
of neurons: a small subset of node neurons is set apart to
choose what mixtures of leaf neuron blocks are to be com-
puted to produce the final output (Figure 1). As the training
progresses, the region boundaries harden, and the mixtures
tend toward selecting only one log-time-accessible leaf.arXiv:2308.14711v2  [cs.LG]  18 Sep 2023

--- PAGE 2 ---
Formally, let f:DI→ D Obe the learning target.
The naive approach is to train a feedforward layer (FF) F
of width wto approximate fonDI, i.e.F≈DIf.
The mixture-of-experts (MoE) approach (Shazeer et al.
2017) is to choose an expert width ethat does not hin-
der performance, and then train separate expert blocks
E1, . . . , E ⌈w/e⌉of neurons mixed by the partially random-
ized output of a gating network of width g=⌈w/e⌉. The
learning target fis then approximated on DIunder the mix-
ture of kbest-scoring experts, i.e. m1Eb1+...+mkEbk≈DI
f. For the corresponding FF network of width w, a MoE net-
work with g=⌈w/e⌉experts uses keneurons for inference
at the mixture overhead of g.
Fast feedforward networks (FFFs) are designed to lever-
age the fact that different regions of the input space activate
different sets of neurons in wide networks. FFFs of depth d
jointly learn a tree partition R1, . . . , R 2dof the input space
determined by their nodes, and 2dsmall leaf feedforward
networks L1, . . . , L 2dof width ℓ, which are trained so that
eachLiapproximates the partial target function f|RionRi,
i.e.Li≈Rif. Crucially, an FFF with depth dand leaf width
ℓcan use all 2dℓhidden neurons to learn f, but requires only
one leaf of width ℓto compute its output for any ι∈ DI, and
does so at the lookup overhead of only dneurons.
To draw a comparison, the leaves Liare to FFFs what
experts Ejare to MoEs, but the FFF tree network is re-
gionalizing the input space rather than noisily voting on ex-
pert prowess. For the corresponding feedforward network
of width wand a choice of leaf size ℓ, one can choose
d= log2⌈w/ℓ⌉and access the leaf in O(d) =O(logw)
instead of O(g) =O(w)time. Coincidentally, the FFF ap-
proach also happens to represent a differentiable relaxation
of the classical notion of k-d trees (Bentley 1975).
The process enabling the fragmentation of a large feedfor-
ward layer to a number of smaller leaf layers while preserv-
ing its predictive performance is hardening . In training, the
nodes of FFF recursively perform a soft choice ⟨1−p, p⟩
over the outputs of their two children (a “mixture of child
experts” if we must), and based on the final loss incurred,
the optimizer updates both the weights of the children and
the weights of the parent that computed the choice. During
inference, a single hard decision (i.e. “proceed to left/right
child”) is made depending on the rounding result [p]at each
node. Hardening is the process of nodes learning boundaries
such that the soft choices they make for individual inputs go
from indecisive mixtures (e.g. ⟨49,51⟩) toward more deci-
sive ones (e.g. ⟨3,97⟩). We observe that in settings where
representation power is plentiful (i.e. wide leaves and deep
FFFs), the process often takes place on its own. In settings
where more representational power may be warranted (e.g.
wide vision transformers with FFF leaf bottlenecks), this
process either takes place but stalls prematurely, or takes
place at a very low rate, and we choose to encourage it with
the addition of a hardening loss .
Thanks to hardening, the performance of the soft training
setting carries over to inference. As a byproduct, the learned
regions can also be used as a partition of the input space for
interpretability, surgical model editing, catastrophic forget-
ting mitigation, reduction of replay data budget, etc..User manual. Suppose that you have an existing architec-
ture featuring a FF layer of width wand want to replace it
with a FFF layer.
Case 1: “I want faster inference”. Choose ℓ << w such
thatℓfits your target inference budget, and then experiment
with different depths d≥log2(w/ℓ)to achieve the desired
performance. Note that ℓstill needs to be large enough to be
able to learn the partial function on its region, and that the
final training width 2dℓmight end up being larger than w.
Case 2: “I want a partition of the input space”. Choose
dsuch that 2dmeets your expectation on the number of
regions. Then experiment with ℓ≥w2−dfor best perfor-
mance. Note that ℓagain needs to be large enough to be able
to learn the partial function on its region, and that for large d
you might have to actively counter the effects of overfrage-
mentation.
Contributions.
1. We introduce the fast feedforward (FFF) architecture, a
peer to the feedforward (FF) architecture that uses only a
log-time-accessible fraction of its neurons at any point.
2. We investigate the effect of leaf size and depth on the
predictive performance of FFFs as models in their own
right and show that in sufficiently large settings, FFFs
give performance comparable to FFs of the same train-
ing width while carrying out the inference significantly
faster. We further show that FFFs deliver better memo-
rization and generalization performance than the FFs of
the same inference size.
3. We compare the FFF architecture against the mixture-of-
experts (MoE) approach in terms of their predictive per-
formance and inference speed as the number of blocks
increases, and support the claimed advantages of the de-
sign experimentally.
4. We demonstrate that FFFs can be feasibly used in place
of FFs as parts of larger, more complex architectures such
as transformers.
The text culminates into a comparison with related work.
Algorithm
Denote the network input, output dimension dimI,dimO.
Denote ⟨a, b, c⟩-feedforward network a feedforward net-
work with ainputs, bneurons, and coutputs.
Notice that we override the terminology designed for
multi-layer networks and talk of only one set of neurons that
has both input and output weights. For example, we would
refer to the BERT-base feedforward layer with input dimen-
sion 768, 3072 hidden neurons, and 768 output neurons as
to the feedforward layer with 3072 neurons, each with 768
inputs and 768 outputs. This greatly simplifies our presenta-
tion.
Definition. A fast feedforward network of depth d≥0,
node size n, and leaf size ℓis a pair ⟨N,L⟩.
N:={N0,0, . . . , N d−1,2d−1−1}is the set of node
⟨dimI, n,1⟩-feedforward networks with an addi-
tional sigmoidal activation on the output. These nodes

--- PAGE 3 ---
Algorithm 1: FFF forward pass.
Input: Input sample ι∈ DI, the root node N0,0
Output: Output ∈ DOof the FFF for ι
Function FO R W A R D T 
ι,Nm,n
:
ifNm,n∈ L then
return Nm,n(ι)
else
cm,n←Nm,n(ι)
return cm,nFO R W A R D T 
ι, Nm+1,2n+1
+ (1−cm,n)FO R W A R D T 
ι, Nm+1,2n
end
Function FO R W A R D I 
ι,Nm,n
:
ifNm,n∈ L then
return Nm,n(ι)
else
cm,n←Nm,n(ι)
ifcm,n≥1
2then
return FO R W A R D I 
ι, Nm+1,2n+1
else
return FO R W A R D I 
ι, Nm+1,2n
end
end
form a balanced differentiable binary tree such that
Nm+1,2n, Nm+1,2n+1are the children of Nm,n.
L :={Nd,0, . . . , N d,2d−1}is the set of leaf
⟨dimI, ℓ,dimO⟩-feedforward networks. All weights are
trainable by default and the forward pass is governed by the
fully deterministic Algorithm 1.
The nodes of FFF are arranged in a differentiable dimI-d
tree that makes a soft choice over the leaves in the form of
a stochastic vector c. In training, FFF performs a mixture of
experts over all leaves in L, with the choice weights of the
mixture ccomputed by ascending through the tree from the
root node N0,0(cf. F ORWARD T). During inference, the de-
cision at each node is taken to be the closer of {0,1}, and
the forward pass algorithm proceeds from the root, always
choosing only one branch depending on the local node deci-
sion (cf. F ORWARD I).
Regions of responsibility and their boundaries. The tree
component of the FFF yields a partition of the input space.
Each leaf is responsible for exactly one region of this parti-
tion, even though during training, its prediction on its own
region of responsibility is mixed with the predictions of
other leaves. The boundaries between the individual regions
are determined by the node networks.
In the case when n= 1 and there is no activation on the
node network but the head sigmoid, the boundary is the ac-
tivation plane of the hidden neuron. The norm of the plane’s
normal vector (=weights of the neuron) affects how quickly
the sigmoid goes from 0to1around the boundary (cf. Fig-
ure 1 bottom-left). This determines how clearly the bound-
ary is defined.Hardening. For the predictive performance of
FORWARD Tto carry over to F ORWARD I, one must
not lose predictive information when rounding the choice
scores. This loss of information is minimal when the
boundary decisions have been properly hardened (cf. Intro-
duction). As hinted at above, hardening at a node generally
does not have to involve adjustment to the boundary as
a manifold in space – progressive uniform rescaling of
boundary coefficients (i.e. squashing of the final sigmoid
toward the step function to make the boundary more clearly
defined) suffices.
Interpreting the node choice scores as Bernoulli probabili-
ties, hardening can be tracked by monitoring the batch mean
of entropies of the choices scores at each node. In our exper-
imentation, we found that rounding choice pairs ⟨1−p, p⟩
with entropies below 0.10tends to lead to only very modest
deviations from the F ORWARD Tperformance. For situations
where the hardening of node decisions does not occur to a
sufficient extent on its own, hardening can be encouraged by
the addition of hardening loss .
LetLpredbe the loss due to the outputs of FFF. Then one
can take the total loss to be Ltotal:=Lpred+hLharden with
Lharden :=X
ι∈BX
N∈NH(N(ι)),
where B ⊆ D Iis a batch of samples, H(p)the entropy of a
Bernoulli random variable, and hthe training hyperparame-
ter controlling the effect of the hardening loss.
Overfragmentation. If pushed to the extreme, allowing
fast feedforward networks to learn too many hard bound-
aries leads to overfragmentation – a phenomenon in which
the network divides the input space into exponentially many
disjoint regions and learns to approximate parts of the learn-
ing target in a way that is too specific for each region. Over-
fragmentation has two direct consequences: localized over-
fitting and the “shrinking batch problem”.
Localized overfitting denotes a tail process that occurs
once the model has sufficiently hardened, in which the re-
gion boundaries are no longer flexible and certain leaves
learn to overfit the training data on their regions of respon-
sibility. This is because they stop receiving meaningfully
large gradient updates from the neighboring regions, but
may be responsible for handling test samples that are not
well understood by the training data for their region. Local-
ized overfitting manifests itself just like classical overfitting
– the validation performance ceases to improve or deterio-
rates while learning on the training set continues. It can be
mitigated by randomized child transpositions – the soft de-
cisions ⟨1−p, p⟩at each node can be randomly transposed
with some low probability into ⟨p,1−p⟩. This is to expose
their children to the training data of neighboring regions in
order to aid generalization performance and does to some
extent already happen for soft boundaries, but it becomes
rare as the boundaries harden.
The FFF variant of the shrinking batch problem is also a
result of the leaf region boundaries hardening, and it arises in
situations when the batch size becomes too small for the par-
tition of the input space learned by the FFF tree. If the par-

--- PAGE 4 ---
tition of the input space is too finely grained and the bound-
aries hardened, each leaf ends up receiving meaningful gra-
dient updates from only a small fraction of the training sam-
ples, resulting in inaccurate gradient descent progress. Batch
shrinking leads to poor learning performance (e.g. low train-
ing set accuracy, early stalling, chaotic development) but can
be mitigated – naively with larger batch sizes, gradient accu-
mulation, and smaller learning rates; in full generality with
localized optimization.
We consider the complexity of our algorithms in terms of the
parameters d, n, ℓ . Note that we found n= 1to suffice in all
our experiments, but we keep nin for the sake of generality.
Training complexity. The training forward pass
FORWARD Tascends through dlevels of the tree, passing
through node neurons to compute the final choice vector cin
O((2d−1)n)time. Then, the leaf outputs are computed and
mixed by cinO(2dℓ)time. This means O(2d(ℓ+n)−n)
time for the forward pass, and a (d+ 1) -step backward pass
back to the decision on the root.
From the implementation standpoint, we express the as-
cent through the tree as a single loop making didentical
batched computations, and then perform the final leaf for-
ward pass and expert mixture.
Inference complexity. FORWARD Iascends through the
FFF tree in dsteps, always executing exactly one node net-
work. Then, it performs inference on one leaf, leading to
O(dn+ℓ)time.
In terms of the implementation, the ascent from the
root through the tree is executed as a batched computa-
tion of an indexed set of weights and biases (multiply-and-
accumulate), comparison of the logit to 0, and advancing of
the index depending on the result of the comparison.
In our experience of using ahead-of-time compilation for
CUDA, the selective indexing of weights for node decisions
manifested itself in the native code as a simple offset in the
data load for batched matrix multiplication, having only a
small constant implementation overhead on the hardware
level when compared to feedforward layers.
Size and width. Fast feedfoward networks consist of neu-
rons of two types: node and leaf neurons. For clarity and to
make direct comparisons to the corresponding feedforward
networks, we distinguish between variants of network size
and width.
An FFF with d,n,ℓas in the definition has training size of
(2d−1)n+2dℓ– these are all the neurons of the network, and
they are all affected by optimization. It further has inference
sizeofdn+ℓ, as these are the neurons engaged to produce
inference output by F ORWARD I.
However, only the neurons of leaves produce output, with
the node neurons being involved solely in the computation
of the mixture of the outputs of individual leaves. Therefore,
we say that the FFF has training width of2dℓand inference
width ℓ. Note that the FFF with all weights of node networks
set to 0is equivalent to a vanilla feedforward network with
2dℓneurons (up to a uniform rescaling of the output weights,
which is learnable). We refer to the difference between the
training/inference size and width as overhead .Experiments
We conduct a number of experiments to (1) explore the ef-
fects of assigning neurons with learnable regions of influ-
ence, (2) compare the predictive performance and speed of
FFFs to that of MoEs, and (3) assess the feasibility of FFFs
as parts of deeper architectures. The task of each experiment
is image classification, and we evaluate the classification ac-
curacy of the softmax of output logits in the usual way. For
FFFs, we measure the accuracy of making “hard” decisions
at every node (i.e. we use F ORWARD I).
For each dataset considered, we use the designated train-
ing and test sets as provided. We further split the full training
set9 : 1 into training and validation subsets.
To compare the qualities of individual models, we mea-
sure four quantities.
Memorization accuracy ( MA).Interpreting (fast) feed-
forward networks as model memories (Bau et al. 2020), we
measure their ability to learn the training set by computing
the accuracy of overfitted networks on the training set. That
is, we train networks until their accuracy in training stops
improving, and then run a test on the training data. A result-
ingMAof 100% means that the network has successfully
memorized all the predictions for the training data.
Generalization accuracy ( GA).Treating (fast) feedfor-
ward networks as predictive models in their own right, we
measure their ability to correctly predict the classes of pre-
viously unseen samples in the test set. For this, we train net-
works until their validation accuracy stops improving, and
use the best model in terms of the validation accuracy for
evaluation.
Inference time and speedup. Our own implementa-
tion of the FFF algorithms is provided through pip
install fastfeedforward and on GitHub1. We
compile our implementation of F ORWARD Ifor NVIDIA
A100 GPUs using PyTorch 2.0.1 model compilation in the
reduce-overhead mode. We then run each model 104
times with batch size 2048 on a single NVIDIA A100 GPU.
Where relevant for comparison, we report the mean infer-
ence time t•per single forward pass under repeated trials, to-
gether with its standard deviation. We further report speedup
– the fraction tFF/tFFF , where tFFF is the mean inference
time for the given FFF model and tFFis the mean infer-
ence time for the vanilla feedforward network of the same
training width. Simply put, speedup says how much faster
the FFF was than the FF with the same number of neurons
available for making predictions in training, measured using
this choice of software and hardware. The means and devia-
tions for speedups are in the appendix.
Explorative evaluation
To examine the nature of fast feedforward networks as an al-
ternative to feedforward networks, we measure the effect of
their parameters on their predictive performance and speed
in the context.
Evaluation with training counterparts
Subject. We investigate the relationship between the config-
uration of leaf size, depth, and training width and the memo-

--- PAGE 5 ---
Model USPS
16 32 64 128
MA GA speedup MA GA speedup MA GA speedup MA GA speedup
vanilla FF 100.0 93.1 1.00x 100.0 93.7 1.00x 100.0 94.1 1.00x 100.0 94.2 1.00xFFFℓ= 8 99.3 92.2 1.07x 99.2 91.8 1.16x 99.2 92.3 1.53x 99.5 92.1 2.56x
ℓ= 4 94.1 87.6 0.98x 97.2 89.5 1.08x 97.6 90.6 1.56x 97.1 90.3 2.40x
ℓ= 2 92.0 85.5 0.90x 93.4 86.4 1.07x 90.6 84.4 1.39x 94.3 88.1 2.22x
ℓ= 1 83.4 77.0 0.85x 77.3 74.2 0.99x 79.2 77.1 1.34x 81.4 77.8 2.12x
Model MNIST
vanilla FF 98.0 95.2 1.00x 100.0 96.6 1.00x 100.0 97.7 1.00x 100.0 98.1 1.00xFFFℓ= 8 94.6 93.1 1.13x 96.5 93.9 1.50x 97.7 94.2 2.20x 99.3 94.9 3.39x
ℓ= 4 91.6 90.8 1.33x 96.2 93.1 1.35x 96.7 93.3 2.33x 97.6 93.6 3.29x
ℓ= 2 92.1 90.3 1.19x 94.0 91.4 1.48x 95.2 92.1 2.33x 96.2 92.4 3.47x
ℓ= 1 91.7 89.9 1.04x 94.4 92.0 1.26x 94.5 91.4 1.91x 94.1 92.0 3.93x
Model FashionMNIST
vanilla FF 91.0 86.4 1.00x 94.8 87.8 1.00x 98.5 89.0 1.00x 99.3 89.6 1.00xFFFℓ= 8 86.7 84.2 1.34x 87.8 85.2 1.44x 88.8 85.2 2.02x 90.5 86.1 3.78x
ℓ= 4 86.4 83.3 1.27x 86.6 84.5 1.32x 89.1 85.1 2.02x 89.0 85.4 3.41x
ℓ= 2 84.5 83.0 1.24x 85.4 82.9 1.34x 87.2 84.1 2.01x 87.3 84.3 3.28x
ℓ= 1 79.7 78.4 1.04x 79.4 77.8 1.29x 79.9 79.5 1.90x 78.7 77.7 2.92x
Table 1: The results of the explorative experimentation on FFFs. Reading top-to-bottom shows the effect of decreasing the leaf
size and correspondingly increasing the depth. Left-to-right: The effect of increasing the training width and model depth while
keeping the leaf size constant. Diagonally bottom-left-to-top-right: The effect of keeping the depth constant while increasing
the leaf size and training width. Emphasis andemphasis mark the best speedups per ℓ, dataset and dataset, respectively.
rization and generalization performance of fast feedforward
networks. For each FFF, we also consider the performance
of a FF of the same training width. We make the compar-
ison with them having the same training width rather than
training size since only leaf neurons are directly involved in
computing the classification prediction for the inputs given.
Method. We train fast feedforward networks for train-
ing widths w= 16 ,32,64,128, leaf sizes ℓ= 1,2,4,
datasets USPS (Hull 1994), MNIST (LeCun, Cortes, and
Burges 2010), and FashionMNIST (Xiao, Rasul, and V oll-
graf 2017). For each w, ℓ configuration we compute the
depth as log2(w/ℓ). The set of widths has been chosen on
purpose: notice that any fast feedforward network with w, ℓ
as above has inference width smaller than 16– the narrow-
est of our configurations. For each width, we further train a
vanilla feedforward network as a baseline.
We feed the networks flattened images. For ease of com-
parison, we use batch size 256 and pure SGD optimization
with learning rate of 0.2irrespective of the size or the depth
of the networks, but we note that deeper FFFs have bene-
fited from larger batch sizes and smaller learning rates. We
engage the hardening loss with scale parameter h= 3.0. We
execute 10 runs for each configuration, and since this is an
evaluation of architectural limits, we report the performance
of the best model. Means and deviations are in the appendix.Discussion. Our results are listed in Table 1. A visualiza-
tion of the hardening process can be found in Figure 5 of
the appendix. The general observations are that each of: in-
creasing width, increasing leaf size, and increasing leaf size
while keeping the depth constant; universally help memo-
rization and generalization performance. We make several
specific observations in relation to our contributions.
FFFs perform comparably to FFs. For sufficiently large
widths and depths on USPS and MNIST, fast feedforward
networks are only slightly (2-3%) worse than vanilla feed-
forward networks. Coincidentally, these are also the config-
urations in which FFFs deliver the best inference speed im-
provements over classical feedforward layers as measured
on our hardware.
Notice the performances of the FFFs with w= 128 , ℓ= 8
across datasets relative to FFs with w= 16 . The perfor-
mance is remarkably close and even exceeds that of FFs, all
that while the inference size of these FFFs ( 12) remains be-
low that of the FFs ( 16).
On FashionMNIST we observe the same trends but note
that an FFF beyond our testing range ( w= 512 , ℓ= 8)
was eventually necessary to bring FFFs close ( MA=97.1,
GA=88.1) to the performance of FFs.
Speedup increases with width. The wider and deeper
networks become while keeping the leaf size constant, the
more significant the inference speed improvement delivered
by the fast feedforward architecture.

--- PAGE 6 ---
Figure 2: A visualization of the comparison of memorization and generalization performance of fast feedforward ( d=2,6) and
feedforward ( d=0) networks. Horizontally: the inference size in neurons. Vertically: accuracy.
Constant width leads to a speed-performance trade-
off.If the training width is kept fixed, there is clearly a
trade-off between increasing depth (and therefore increasing
speed) and performance (cf. Table 1 top-to-bottom).
Increasing the depth of the network while keeping the
training width constant results in a gradual decrease in per-
formance due to the FFF having smaller leaves, hence our
note in the “User manual” (cf. Introduction). Later we will
see that in deeper architectures the trade-off in constant-
width networks between inference speed and performance
decrease due to small leaf size appears to be lessened in
multi-layer architectures.
Large-depth small-leaf networks exhibit overfragmen-
tation. We observe (USPS, FashionMNIST) that decreasing
the leaf size while increasing depth (keeping the width con-
stant) leads to quickly worsening memorization and general-
ization performance. The difference is particularly stark with
greater depths ( ℓ=1,2, w=64, 128). This is well explained by
overfragmentation, since especially for ℓ= 1, w= 128 ,
each leaf receives only 2samples per batch on average. The
results on USPS for ℓ=1,w=16,32 are a model example of
this: we see that FFF with ℓ=1,d=4 delivers MAof83.4,
but that its deeper cousin ℓ=1,d=5 – which has more of the
same-sized leaves at its disposal – yields MAof only 77.3.
TL;DR. FFFs give predictive performance comparable to
FFs of the same training width, are faster as the training
width increases, and if pushed to the limit exhibit speed-
performance trade-offs and overfragmentation.
Evaluation with inference counterparts
Subject. Similarly to the experimentation above with the
main consideration for training width, we now evaluate fastfeedforward networks of varying depths and leaf sizes with
respect to their inference size. We make the inference size
the point of comparison with feedforward networks since,
unlike the inference width, it is directly proportional to the
computational cost of inference.
Method. We train fast feedforward networks for ℓ=
2,4,6,8,16,32,d= 2,6, datasets SVHN (Netzer et al.
2011), CIFAR10, CIFAR100 (Krizhevsky, Hinton et al.
2009), and for each ℓ, dconfiguration we compute the in-
ference size as ℓ+d. For each inference size, we further
train a vanilla feedforward network as a baseline.
We train the networks with all parameters as above except
h, where we do not engage the hardening loss ( h= 0) as
we found that the hardening tended to occur on its own. We
execute 10 runs for each configuration and report the best
performances.
Discussion. Our results are sparse because of all the dif-
ferent possible sums of leaf size and depth and are shown
in Figure 2. Aligned with intuition, we observe that bigger
depth and larger leaf sizes lead to better memorization and
generalization performance.
Further, FFFs outperform FFs of the same inference
size. FFFs of varying depths and sizes consistently outper-
form the FFs with widths equal to the FFF inference sizes,
both in terms of MAandGA. In terms of MA, the difference
is stark and grows with the depth and leaf size. In terms of
GA, FFFs initially gain an edge over FFs, but later the per-
formances of all models unite toward plateauing out at the
limit of the naive, single-layer feedforward-only approach.
TL;DR. FFFs deliver performance more readily than FFs
of the same inference width.

--- PAGE 7 ---
Width Model
feedforward mixture-of-experts ( e=16,k=2) fast feedforward ( ℓ=32)
MA ETT GA ETT MA ETT GA ETT MA ETT GA ETT
w= 64 87.2 307 49.3 55 57.8 5354 29.4 4880 85.8 302 45.9 22
w= 128 95.5 200 51.5 46 62.0 6074 33.6 938 90.1 305 45.5 22
w= 256 99.9 105 52.0 48 62.4 2001 33.9 372 91.2 244 44.4 17
w= 512 99.9 85 52.4 31 65.4 3834 34.5 315 96.2 175 43.7 10
w= 1024 99.9 82 53.0 21 65.3 1575 35.2 327 96.0 180 41.3 9
Table 2: The results of the comparison of feedforward, mixture-of-experts, and fast feedforward networks, for various training
widths. The inference width is fixed to 32 for mixture-of-experts and fast feedforward networks. The ETT columns to the right
of metric columns list the “epochs to train”, i.e. the number of training epochs that have elapsed until the score to the left was
observed.
Comparative evaluation
The direct contender architecture to fast feedforward, com-
ing along with its own set of design parameters, is the
mixture-of-expert layer, which we take in its original form
(Shazeer et al. 2017).
Predictive performance comparison
Subject. We compare FFFs against MoEs and FFs of vary-
ing training widths in terms of their predictive performance.
We keep the leaf and expert width constant and focus on the
ability of the architectures to deliver good memorization and
generalization properties as well as on the training compute
necessary to reach those properties.
Method. We experiment on the unaugmented CIFAR10
dataset. We train feedforward, mixture-of-experts, and fast
feedforward networks of increasing size so that they always
agree in the training width. To keep the inference width the
same, we set the leaf width to 32and expert width to 16
with always engaging k= 2experts. Note that while single-
expert networks can be used for inference, they are not able
to propagate gradients to the gating network (cf. Shazeer
et al. (2017)). We take widths w= 26,27, . . . , 210, which
correspond to 16- to 64-expert MoE networks and FFFs of
depths 1 to 5. To encourage importance equality and load
balancing in MoEs, we set wimportance =wload= 0.1in
line with previous work. To encourage FFF hardening, we
useh= 3.0. We train all models width batch size 4096 for
7000 epochs at most, with early stopping after 350 epochs
where no improvement in the respective validation accura-
cies is seen. All models have been trained with the Adam op-
timizer, learning rate of 0.001, with the learning rate halving
on 250-epoch training accuracy plateaus.
Discussion. The results are listed in Table 2. On the outset,
we observe that the MAandGAbenefit from larger training
width across all models except GAon FFFs, where we see
the unmitigated localized overfitting negatively affecting the
performance.
FFFs are the fastest to deliver MAandGA.We see that
the FFFs are the fastest (in terms of ETT) to deliver both
MAandGA, but, consistently with our previous explorative
evaluation, deliver slightly lower MAthan FFs of the sametraining width, and suffer from localized overfitting with the
increasing depth.
FFFs outperform MoEs of the same training width.
We observe that FFFs consistently deliver better MAand
GAscores than the MoE networks of the same training
width. We further see that they do so at ETTs smaller by an
order of magnitude. We attribute this difference mainly to
the learnably controlled noise introduced to the expert mix-
ture computation to aid load balancing and generalization.
Without the noise, however, MoE networks would overfit to
learn only with a handful of experts. We also experimented
with varying values of wimportance andwload, but we found
those to be broadly detrimental to the load balancing effort.
Our final values of batch importance and load were consis-
tent with those arrived at in Shazeer et al. (2017).
TL;DR. FFFs deliver representational power more readily
than the MoEs of equal training widths.
Inference speed comparison
Subject. Since the operations involved in the computation
of the expert/leaf network output are the same, the difference
in inference speed between mixture-of-experts and fast feed-
forward networks comes solely from the functioning of the
gating/lookup mechanism. We therefore keep the expert/leaf
width constant and measure the time needed to execute in-
ference forward passes of feedforward, mixture-of-experts,
and fast feedforward networks across repeated trials for in-
creasing numbers of experts/leaves (i.e. wider and wider net-
works).
To add realism, we simulate the conditions of a BERT-
base (Devlin et al. 2018) feedforward layer, setting the input
and output widths of all neurons to 768each.
Method. We consider FF models of width 32×21to
32×25, where we highlight the 32 neuron blocks for di-
rect comparison with the other models. We further evaluate
MoE models with expert width e= 32 and21to215ex-
perts, and FFF models with leaf width e= 32 and depths
1to15. To eliminate the effect of the mixture computation
on our measurements, we keep e=ℓand set k= 1, even
though this MoE parameter configuration is not trainable (cf.

--- PAGE 8 ---
Figure 3: A visualization of the performance measure-
ment results. The horizontal axis denotes the number of
blocks/experts/leaves and is scaled logarithmically, the point
values are the mean inference times per single forward pass,
and the error bars show the standard deviation.
above). When measuring, each model performs inference on
BERT-base inputs with batch size 256 exactly 20000 times.
Discussion. The inference speed measurement results are
visualized in Figures 3–4. We observe that both MoEs and
FFFs offer significant acceleration to the inference speed
when compared to the FF baseline. However, Figure 4,
shows the clear tendency of MoE model inference time to
grow exponentially with the exponent of the expert count, in
stark contrast with the linear relationship between the two
exhibited by the FFF models. This is fully aligned with our
theoretical analysis of the inference time complexity of the
two architectures.
TL;DR. We have experimentally confirmed the exponen-
tial difference between the inference time complexity of the
MoE’s and FFF’s internal mechanism.
Fast feedforward layers as building blocks
Subject. We demonstrate that fast feedforward networks
can be used as layers in place of standard feedforward layers
in the transformer architecture, thus giving it a significant
performance boost. Previously, we noted that leaf sizes that
are too small for a given problem may lead to the occurrence
of overfragmentation. Here we push our experimental setup
to the limit in terms of leaf size and investigate the effect of
overfragmentation in a deep transformer.
Method. We experiment on the CIFAR10 dataset of
32x32-pixel 3-channel images, with random horizontal, ver-
tical flipping, and random linear augmentations (translate,
rotate, scale). As models, we use 4-layer vision transform-
ers with patch size 4, hidden dimension 128, input dropout
0.1, and no layer dropout.
We consider vision transformers with their feedforward
layers replaced by fast feedforward layers of training width
Figure 4: A close-up on the visualization of the performance
measurement results. The axes and values are as in Figure 3.
128, and a baseline vision transformer with feedforward lay-
ers of width w= 128 . The fast feedforward layers have leaf
sizeℓ= 1,2,4,8,16,32and depths log2(w/ℓ). We try three
levels of hardening: h= 5,10,∞, where ∞denotes that the
FFF tree has been effectively frozen from the beginning (i.e.
the boundaries are not trainable). We use Adam optimizer
with the initial learning rate of 4e−4and learning rate halv-
ing on 50-epoch validation accuracy plateaus. For each ℓ,d-
configuration, we report the generalization performance of
the best model and the measured speedup at the feedforward
layers (not the whole transformer).
Discussion. The results of our experimentation can be
seen in Table 3. A visualization of the hardening process
across the layers of the transformer can be found in Figure 6
of the appendix. In line with our assessment of the algorithm
complexity, the measured speedup at the feedforward layers
increases with decreasing leaf size. Further:
Single-neuron FFFs suffice. We observe that even fast
feedforward layers with inference width 1are sufficient for
the vision transformer to deliver reasonable performance,
with relative decrease in performance of only 5.8%.
The effects of overfragmentation are suppressed. We
observe that the generalization performance of GAsuffers
only relatively mildly due to the increase in depth and de-
crease in leaf size, which is in stark contrast with the results
of Table 1. We attribute this to the depth of the transformer
and take it as an encouraging sign of the feasibility of FFFs
as replacements for FFs.
TL;DR. Fast feedforward layers can deliver inference ac-
celeration in vision transformers over feedforward layers of
the same training width at the cost of a small performance
decrease.
Related work
Our work overlaps with the research efforts in two areas of
inference acceleration.

--- PAGE 9 ---
Model Property
depth training width training size inference width inference size speedup GA
FF w= 128 – 128 128 (100%) 128 (100%) 128 (100%) 1.00x 84.7fast FFℓ= 32 2 128 131 (102%) 32 (25%) 34 (27%) 2.44x 83.6
ℓ= 16 3 128 135 (105%) 16 (13%) 19 (15%) 2.80x 83.2
ℓ= 8 4 128 143 (112%) 8 (6%) 12 (9%) 3.29x 82.8
ℓ= 4 5 128 159 (124%) 4 (3%) 9 (7%) 3.39x 81.6
ℓ= 2 6 128 191 (149%) 2 (1%) 8 (6%) 3.47x 80.1
ℓ= 1 7 128 255 (199%) 1 (1%) 8 (6%) 3.93x 79.8
Table 3: The results of the testing of vision transformers leveraging feedforward and fast feedforward layers. All sizes are given
in neurons. Bracketed percentages describe quantities relative to their counterparts in the vanilla feedforward layers. GAis the
generalization accuracy of the fully trained vision transformer and “speedup” gives the performance improvement over vanilla
feedforward layers in our testing setup.
Conditional execution. Although nowadays largely inac-
tive due to the tendency to move away from custom archi-
tectures, modified designs for MLP and CNN models were
proposed to allow for their partial execution where possible.
A number of methods were proposed (Davis and Arel
2013; Bengio, L ´eonard, and Courville 2013; Bengio et al.
2015; Almahairi et al. 2016) to learn either policy distribu-
tions or additional controlling neural components to decide
which blocks of layer neurons to execute during forward
pass.
In comparison, fast feedforward networks completely
conceal the learning of leaf regions from the user (save from
the hyperparameter hif used) and come in an inference-
ready form once trained, requiring no adjustment when in-
cluded as a part of transformers.
In a notable generalization of this line of work to deep ar-
chitectures, Ioannou et al. (2016) proposed an approach peer
to deep convolutional neural networks that learns to route
the input through a sequence of chosen intermediate layers.
While our method quickly routes the signal to a single-leaf
feedforward neural network, it draws no comparison to deep
networks.
Modular “mixture-of-experts” models. Very large mod-
els practically demand modularity. The most straightforward
way to modularize large transformer models in order to re-
duce their inference cost is to subdivide their feedforward
layers into nblocks of neurons, and then train a controlling
classifier to choose which block to involve in forward pass.
This is usually done by training a wide softmax-activated
linear layer to produce a stochastic vector of mixture scores
to be applied to the outputs per block in order to produce the
final output. Several variants of this method have been pro-
posed and tested across a variety of large models (Shazeer
et al. 2017; Lepikhin et al. 2020; Fedus, Zoph, and Shazeer
2022).
We thoroughly compare fast feedforward to mixture-of-
expert networks in earlier sections. To briefly summarise,
the mixture-of-experts approach reduces the layer inference
width by a factor of n/k, where kis the number of best-
scoring blocks to engage in inference, but requires O(n)
time to select the kblocks, and often relies on controlledrandomization to avoid the formation of a strong preference
for only a handful experts. This holds true even when mul-
tiple layers of expert mixers are introduced. In direct com-
parison, a fast feedforward network of depth d= log2n
reduces the inference by a factor of nand requires only
O(d) =O(logn)time to decide on which leaf to use. Ad-
mittedly, to compensate for the effect of having only one
leaf to make the decision, the leaves of the fast feedforward
layer might have to be slightly wider than the blocks of the
corresponding mixture-of-experts layer.
Regionalization. An additional advantage of FFF over all
of the related work is that there is a direct correspondence
between parts of the network used in inference and alge-
braically identifiable regions of the input space. This can be
leveraged to mitigate catastrophic forgetting when editing
models and to significantly reduce replay data budgets by
applying the learned partition of the input space to partition
the training data.
References
Almahairi, A.; Ballas, N.; Cooijmans, T.; Zheng, Y .;
Larochelle, H.; and Courville, A. 2016. Dynamic capacity
networks. In International Conference on Machine Learn-
ing, 2549–2558. PMLR.
Bau, D.; Liu, S.; Wang, T.; Zhu, J.-Y .; and Torralba, A.
2020. Rewriting a deep generative model. In Computer
Vision–ECCV 2020: 16th European Conference, Glasgow,
UK, August 23–28, 2020, Proceedings, Part I 16 , 351–369.
Springer.
Bengio, E.; Bacon, P.-L.; Pineau, J.; and Precup, D. 2015.
Conditional computation in neural networks for faster mod-
els.arXiv preprint arXiv:1511.06297 .
Bengio, Y .; L ´eonard, N.; and Courville, A. 2013. Estimat-
ing or propagating gradients through stochastic neurons for
conditional computation. arXiv preprint arXiv:1308.3432 .
Bentley, J. L. 1975. Multidimensional binary search trees
used for associative searching. Communications of the ACM ,
18(9): 509–517.

--- PAGE 10 ---
Davis, A.; and Arel, I. 2013. Low-rank approximations
for conditional feedforward computation in deep neural net-
works. arXiv preprint arXiv:1312.4461 .
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.
Bert: Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805 .
Fedus, W.; Zoph, B.; and Shazeer, N. 2022. Switch trans-
formers: Scaling to trillion parameter models with simple
and efficient sparsity. The Journal of Machine Learning Re-
search , 23(1): 5232–5270.
Hull, J. J. 1994. A database for handwritten text recogni-
tion research. IEEE Transactions on pattern analysis and
machine intelligence , 16(5): 550–554.
Ioannou, Y .; Robertson, D.; Zikic, D.; Kontschieder, P.;
Shotton, J.; Brown, M.; and Criminisi, A. 2016. Decision
forests, convolutional networks and the models in-between.
arXiv preprint arXiv:1603.01250 .
Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple
layers of features from tiny images.
LeCun, Y .; Cortes, C.; and Burges, C. 2010. MNIST
handwritten digit database. ATT Labs [Online]. Available:
http://yann.lecun.com/exdb/mnist , 2.
Lepikhin, D.; Lee, H.; Xu, Y .; Chen, D.; Firat, O.; Huang,
Y .; Krikun, M.; Shazeer, N.; and Chen, Z. 2020. Gshard:
Scaling giant models with conditional computation and au-
tomatic sharding. arXiv preprint arXiv:2006.16668 .
Netzer, Y .; Wang, T.; Coates, A.; Bissacco, A.; Wu, B.; and
Ng, A. Y . 2011. Reading digits in natural images with unsu-
pervised feature learning.
Shazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.;
Hinton, G.; and Dean, J. 2017. Outrageously large neu-
ral networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538 .
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. Advances in neural information pro-
cessing systems , 30.
Xiao, H.; Rasul, K.; and V ollgraf, R. 2017. Fashion-mnist:
a novel image dataset for benchmarking machine learning
algorithms. arXiv preprint arXiv:1708.07747 .

--- PAGE 11 ---
Extended results of Table 1
Table 4 lists the means and standard deviations of our explo-
rative evaluation of fast feedforward networks in comparison
with feedforward networks.
On top of the observations made in the main text, we
see that the variance of MAandGAwhen performing re-
peated runs increases with the decreasing leaf size and is
most clearly seen for small training widths.
Visualization of the hardening process
To provide some insight and intuition into the hardening of
the node decision boundaries, we track the batch mean en-
tropies across the nodes of fast feedforward layers and report
on their evolution as training progresses visually.
Figure 5 shows the evolution of the batched mean entropy
for three models of varying depths whose training was ter-
minated by early stopping on validation F ORWARD Iaccu-
racy plateau. We observe that by keeping the dataset (and
therefore the complexity of the input space) and leaf size
constant, the batch mean accuracy of deep layers converges
faster for deeper networks. We attribute this simply to the
fact that the deeper the fast feedforward network in this ex-
periment, the more leaves were available to selectively cater
to sub-spaces of the input space, allowing the networks to
make cleaner separations between the regions learned.
Figure 6 reports the evolution of batched mean entropies
per layer for a 4-layer visual transformer model with fast
feedforward networks in place of the vanilla feedforward
networks. We notice that early into the training, entropies of
the layers closer to the input (i.e. the earlier/lower layers) are
faster to converge. However, we see that the entropy of the
second layer then ceases to meaningfully decrease, and that
the entropies of the first layer actually begin to climb. This
can be explained by hardened decision boundaries showing
signs of bottleneck behavior in higher layers of the trans-
former, thus necessitating that the lower layers contribute
with learned representations of higher quality.
Interpreting the stalling or climbing of the entropy as a
sign of the need for more representational power, we note
across multiple experiments that the ideal shape for a vision
transformer, even on a relatively simple dataset such as CI-
FAR10, appears to be more toward the shape of a pyramid
than a uniform-width line of layers.
Relationship to multi-gating MoE networks
More recent literature on mixture-of-experts networks (Lep-
ikhin et al. 2020; Fedus, Zoph, and Shazeer 2022) suggests
the use of a hierarchical, two-layer gating structure. Save
from the inherent randomness of the gating network selec-
tion, this can be seen as a small step towards FFFs. However,
FFFs attend to clearly specified regions of space rather than
being noisily voted in as the most appropriate experts for a
given input (which we have shown gives them much better
training and inference properties), and further, even under
such an approach, the inference time reduction of mixture-
of-expert networks remains constant in the width of the tar-
get network.
Figure 5: The evolution of batched mean decision entropy in
a fast feedforward layer with ℓ= 8, d= 2,3,4, h= 3.0,
trained on the MNIST dataset.
Figure 6: The evolution of batched mean decision en-
tropies across a vision 4-layer transformer of dimension 128
equipped with fast feedforward layers of ℓ= 32, d= 2, h=
0.10, trained on the CIFAR10 dataset.
Code
We provide our implementations as a Python package built
on top of the PyTorch framework.
Just use pip install fastfeedforward . Docu-
mentation is provided with the code.

--- PAGE 12 ---
Model USPS
16 32 64 128
MA GA time MA GA time MA GA time MA GA time
vanilla FF 100.0 ± 0.0 93.1 ± 0.4 0.13 ± 0.02ms 100.0 ± 0.0 93.7 ± 0.4 0.16 ± 0.01ms 100.0 ± 0.0 94.1 ± 0.3 0.24 ± 0.02ms 100.0 ± 0.0 94.2 ± 0.3 0.41 ± 0.03msfast FFℓ= 8 99.3 ± 0.5 92.2 ± 0.4 0.12 ± 0.03ms 99.2 ± 0.5 91.8 ± 0.6 0.14 ± 0.03ms 99.2 ± 0.4 92.3 ± 0.7 0.16 ± 0.03ms 99.5 ± 0.8 92.1 ± 0.4 0.16 ± 0.02ms
ℓ= 4 94.1 ± 0.5 87.6 ± 0.7 0.13 ± 0.03ms 97.2 ± 3.4 89.5 ± 2.0 0.15 ± 0.03ms 97.6 ± 2.3 90.6 ± 1.4 0.16 ± 0.02ms 97.1 ± 1.3 90.3 ± 1.2 0.17 ± 0.02ms
ℓ= 2 92.0 ± 9.2 85.5 ± 8.0 0.14 ± 0.03ms 93.4 ± 10.0 86.4 ± 8.7 0.15 ± 0.02ms 90.6 ± 7.7 84.4 ± 6.0 0.18 ± 0.03ms 94.3 ± 8.6 88.1 ± 7.1 0.18 ± 0.03ms
ℓ= 1 83.4 ± 12.1 77.0 ± 11.1 0.15 ± 0.02ms 77.3 ± 5.8 74.2 ± 5.0 0.16 ± 0.01ms 79.2 ± 8.1 77.1 ± 7.1 0.18 ± 0.02ms 81.4 ± 9.2 77.8 ± 8.3 0.19 ± 0.02ms
Model MNIST
vanilla FF 98.0 ± 0.9 95.2 ± 0.5 0.34 ± 0.11ms 100.0 ± 0.0 96.6 ± 0.2 0.42 ± 0.06ms 100.0 ± 0.0 97.7 ± 0.2 0.69 ± 0.10ms 100.0 ± 0.0 98.1 ± 0.1 1.13 ± 0.06msfast FFℓ= 8 94.6 ± 19.5 93.1 ± 16.6 0.30 ± 0.11ms 96.5 ± 2.3 93.9 ± 1.2 0.28 ± 0.05ms 97.7 ± 4.3 94.2 ± 2.4 0.31 ± 0.06ms 99.3 ± 1.0 94.9 ± 0.6 0.33 ± 0.08ms
ℓ= 4 91.6 ± 29.3 90.8 ± 27.2 0.26 ± 0.07ms 96.2 ± 24.3 93.1 ± 23.9 0.31 ± 0.09ms 96.7 ± 1.0 93.3 ± 0.6 0.30 ± 0.06ms 97.6 ± 0.6 93.6 ± 0.5 0.34 ± 0.08ms
ℓ= 2 92.1 ± 7.3 90.3 ± 5.6 0.28 ± 0.08ms 94.0 ± 1.4 91.4 ± 1.0 0.28 ± 0.05ms 95.2 ± 1.8 92.1 ± 1.2 0.30 ± 0.07ms 96.2 ± 1.4 92.4 ± 0.6 0.32 ± 0.06ms
ℓ= 1 91.7 ± 7.4 89.9 ± 6.4 0.33 ± 0.11ms 94.4 ± 3.5 92.0 ± 3.1 0.33 ± 0.07ms 94.5 ± 1.8 91.4 ± 1.1 0.36 ± 0.09ms 94.1 ± 0.9 92.0 ± 0.7 0.29 ± 0.03ms
Model FashionMNIST
vanilla FF 91.0 ± 0.7 86.4 ± 0.4 0.34 ± 0.10ms 94.8 ± 0.9 87.8 ± 0.2 0.42 ± 0.07ms 98.5 ± 0.8 89.0 ± 0.4 0.64 ± 0.07ms 99.3 ± 0.4 89.6 ± 0.2 1.13 ± 0.05msfast FFℓ= 8 86.7 ± 12.1 84.2 ± 10.9 0.26 ± 0.07ms 87.8 ± 17.6 85.2 ± 16.1 0.29 ± 0.10ms 88.8 ± 5.6 85.2 ± 3.8 0.32 ± 0.05ms 90.5 ± 1.7 86.1 ± 1.0 0.30 ± 0.06ms
ℓ= 4 84.5 ± 25.0 83.0 ± 24.5 0.27 ± 0.11ms 86.6 ± 8.6 84.5 ± 6.4 0.32 ± 0.08ms 89.1 ± 3.5 85.1 ± 2.3 0.32 ± 0.07ms 89.0 ± 0.7 85.4 ± 0.7 0.33 ± 0.07ms
ℓ= 2 83.6 ± 21.0 82.5 ± 11.0 0.28 ± 0.10ms 85.4 ± 8.4 82.9 ± 6.5 0.32 ± 0.09ms 87.2 ± 7.1 84.1 ± 5.9 0.32 ± 0.06ms 85.3 ± 5.2 81.5 ± 3.7 0.35 ± 0.08ms
ℓ= 1 86.4 ± 9.0 83.3 ± 8.0 0.33 ± 0.09ms 79.4 ± 6.2 77.8 ± 5.5 0.33 ± 0.07ms 79.9 ± 3.5 79.5 ± 3.7 0.34 ± 0.08ms 78.7 ± 4.6 77.7 ± 3.8 0.39 ± 0.08ms
Table 4: The detailed results of the explorative experimentation on FFFs. Reading top-to-bottom shows the effect of decreasing the leaf size and correspondingly
increasing the depth. Left-to-right: The effect of increasing the training width and model depth while keeping the leaf size constant. Diagonally bottom-left-to-top-
right: The effect of keeping the depth constant while increasing the leaf size and training width.
