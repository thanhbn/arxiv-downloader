# 2309.04354.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2309.04354.pdf
# Kích thước file: 1394365 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Mobile V-MoEs: Thu nhỏ Vision Transformers
thông qua Sparse Mixture-of-Experts
Erik Daxberger* Floris Weers Bowen Zhang Tom Gunter Ruoming Pang
Marcin Eichner Michael Emmersberger Yinfei Yang Alexander Toshev Xianzhi Du
Apple

Tóm tắt
Các mô hình Sparse Mixture-of-Experts (MoEs) gần đây đã
trở nên phổ biến nhờ khả năng tách biệt kích thước mô hình
khỏi hiệu quả suy luận bằng cách chỉ kích hoạt một tập con
nhỏ các tham số mô hình cho bất kỳ token đầu vào nào. Như
vậy, sparse MoEs đã cho phép khả năng mở rộng chưa từng
có, dẫn đến những thành công to lớn trên các lĩnh vực như
xử lý ngôn ngữ tự nhiên và thị giác máy tính. Trong nghiên
cứu này, chúng tôi thay vào đó khám phá việc sử dụng sparse
MoEs để thu nhỏ Vision Transformers (ViTs) nhằm làm cho
chúng hấp dẫn hơn cho các ứng dụng thị giác có hạn chế tài
nguyên. Để đạt được điều này, chúng tôi đề xuất một thiết kế
MoE đơn giản và thân thiện với thiết bị di động, trong đó
toàn bộ hình ảnh thay vì các patch riêng lẻ được định tuyến
đến các experts. Chúng tôi cũng đề xuất một quy trình huấn
luyện MoE ổn định sử dụng thông tin siêu lớp để hướng dẫn
router. Chúng tôi chứng minh thực nghiệm rằng các Mobile
Vision MoEs (V-MoEs) sparse của chúng tôi có thể đạt được
sự cân bằng tốt hơn giữa hiệu suất và hiệu quả so với các
ViTs dense tương ứng. Ví dụ, đối với mô hình ViT-Tiny,
Mobile V-MoE của chúng tôi vượt trội hơn mô hình dense
tương ứng 3.39% trên ImageNet-1k. Đối với biến thể ViT
nhỏ hơn với chi phí suy luận chỉ 54M FLOPs, MoE của
chúng tôi đạt được cải thiện 4.66%.

1. Giới thiệu
Sự cân bằng giữa hiệu suất và hiệu quả của các mạng
nơ-ron (NNs) vẫn là một thách thức, đặc biệt trong các
môi trường có hạn chế tài nguyên tính toán. Gần đây,
các mô hình sparsely-gated Mixture-of-Experts (sparse
MoEs) đã trở nên phổ biến vì chúng cung cấp một giải
pháp hứa hẹn cho vấn đề này bằng cách cho phép tách
biệt kích thước mô hình khỏi hiệu quả suy luận [3]. MoEs
là các NNs được phân vùng thành các "experts", được
huấn luyện cùng với một router để chuyên môn hóa trên
các tập con của dữ liệu. Trong MoEs, mỗi đầu vào chỉ
được xử lý bởi một tập con nhỏ các tham số mô hình
(còn gọi là tính toán có điều kiện). Ngược lại, các mô
hình dense truyền thống kích hoạt tất cả tham số cho
mỗi đầu vào.

*Thư từ liên hệ: Erik Daxberger, edaxberger@apple.com .

[THIS IS FIGURE: Biểu đồ thể hiện Accuracy vs. FLOPs cho ViTs có kích thước khác nhau, với các labels như 12×384, 6×384, 12×192 (ViT-Tiny), 6×192, 6×96, 6×64, so sánh giữa Mobile V-MoE và Dense ViT]

Hình 1. Accuracy vs. FLOPs cho ViTs có kích thước khác nhau. Các
labels (ví dụ 12×192, tức là ViT-Tiny) biểu thị số lượng lớp ViT
(ví dụ 12) và kích thước embedding ẩn (ví dụ 192). Các sparse
MoEs vượt trội hơn các baseline dense tương ứng trên các quy mô
mô hình khác nhau. Hình 3a liệt kê tất cả kết quả số.

Sparse MoEs được phổ biến trong deep learning bởi [16],
đã giới thiệu các sparse MoE-layers như các thành phần
thay thế trực tiếp cho các lớp NN tiêu chuẩn. Hầu hết
các MoEs gần đây đều dựa trên Transformer [19], xử lý
các token đầu vào riêng lẻ; theo đó, các MoEs gần đây
cũng định tuyến các token đầu vào riêng lẻ đến experts,
tức là các image patches trong trường hợp Vision Trans-
formers (ViTs) [2, 13] (xem Hình 2b). Tính toán có điều
kiện như được triển khai bởi sparse MoEs đã cho phép
huấn luyện các Transformers với kích thước chưa từng
có [4]. Kết quả là, MoEs đã đạt được những thành công
ấn tượng trên nhiều lĩnh vực khác nhau bao gồm ngôn ngữ
[4, 10], thị giác [13], giọng nói [20] và học đa phương
thức [12], và hiện đang giữ kết quả tốt nhất trên nhiều
benchmark [21].

Khả năng tăng dung lượng mô hình trong khi giữ chi phí
suy luận thấp cũng hấp dẫn đối với các bài toán thị giác
có hạn chế tài nguyên. Trong khi Transformers đang ngày
càng được thiết lập như kiến trúc chuẩn de-facto cho
mô hình hóa thị giác quy mô lớn [2, 13], hầu như tất cả
các mô hình thân thiện với thiết bị di động vẫn sử dụng
convolutions do hiệu quả của chúng [1,5,6,11,15,18].
Như vậy, tính toán có điều kiện có thể giúp các mô hình
dựa trên attention giảm khoảng cách với các mô hình
convolutional trong chế độ quy mô nhỏ.

Tuy nhiên, các MoEs dựa trên Transformer chưa được
khám phá cho các môi trường có hạn chế tài nguyên;
điều này có thể do hai điểm yếu chính của các MoEs
được phổ biến gần đây [16].

Thứ nhất, trong khi per-token routing tăng tính linh hoạt
để học đường dẫn tính toán tối ưu qua mô hình, nó làm
cho suy luận không hiệu quả, vì nhiều (hoặc thậm chí
tất cả) experts cần được tải cho một hình ảnh đầu vào
duy nhất. Thứ hai, các MoEs gần đây huấn luyện các
routers cùng với phần còn lại của mô hình theo cách
end-to-end. Để tránh sụp đổ xuống chỉ một vài experts
trong khi bỏ qua tất cả các experts khác, người ta cần
sử dụng các cơ chế cân bằng tải [3] như các auxiliary
losses chuyên dụng [16]. Tuy nhiên, các mục tiêu tối ưu
hóa phức tạp kết quả thường dẫn đến bất ổn / phân kỳ
trong huấn luyện [4, 10, 12, 21].

Trong nghiên cứu này, chúng tôi khảo sát tiềm năng của
sparse MoEs để thu nhỏ ViTs cho các ứng dụng thị giác
có hạn chế tài nguyên thông qua một thiết kế MoE và
quy trình huấn luyện giải quyết các vấn đề nêu trên.
Đóng góp của chúng tôi là:

1. Chúng tôi đề xuất một thiết kế sparse MoE đơn giản,
thân thiện với thiết bị di động, trong đó một router
duy nhất gán toàn bộ hình ảnh (thay vì các image
patches) cho các experts (xem Hình 2c).

2. Chúng tôi phát triển một quy trình huấn luyện đơn
giản nhưng mạnh mẽ, trong đó sự mất cân bằng
expert được tránh bằng cách tận dụng các siêu lớp
ngữ nghĩa để hướng dẫn huấn luyện router.

3. Chúng tôi chứng minh thực nghiệm rằng phương pháp
sparse MoE được đề xuất cho phép chúng tôi thu nhỏ
các mô hình ViT bằng cách cải thiện sự cân bằng hiệu
suất vs. hiệu quả của chúng.

2. Thu nhỏ ViTs thông qua sparse MoEs

2.1. Tính toán có điều kiện với sparse MoEs

Một MoE triển khai tính toán có điều kiện bằng cách
kích hoạt các tập con khác nhau của một NN (gọi là
experts) cho các đầu vào khác nhau. Chúng tôi xem xét
một lớp MoE với E experts như

MoE (x) = ∑(i=1 to E) g(x)ᵢeᵢ(x), (1)

trong đó x∈R^D là đầu vào của lớp, eᵢ:R^D→R^D
là hàm được tính toán bởi expert i, và g:R^D→R^E
là hàm routing tính toán trọng số phụ thuộc đầu vào
cho mỗi expert [16]. Trong một MoE dựa trên ViT,
mỗi expert eᵢ được tham số hóa bởi một multi-layer
perceptron (MLP) riêng biệt trong lớp ViT, trong khi
các phần khác được chia sẻ giữa các experts (xem Hình
2d). Chúng tôi sử dụng hàm routing

g(x) = TOP_k(softmax (Wx )), (2)

trong đó phép toán TOP_k(x) đặt tất cả các phần tử
của x về zero trừ những phần tử có k giá trị lớn nhất
[13]. Trong một sparse MoE, chúng ta có k≪E, do đó
chúng ta chỉ cần tải và tính toán k experts có trọng số
routing lớn nhất. Điều này cho phép chúng ta mở rộng
dung lượng mô hình tổng thể (xác định bởi E) mà không
tăng chi phí suy luận (xác định bởi k).

2.2. MoEs hiệu quả và mạnh mẽ cho ViTs quy mô nhỏ

Per-image routing. Các sparse MoEs quy mô lớn gần
đây sử dụng per-patch routing (tức là các đầu vào x
là các image patches riêng lẻ). Điều này thường yêu
cầu một số lượng lớn hơn các experts được kích hoạt
cho mỗi hình ảnh. Ví dụ, [13] cho thấy rằng trong MoE
của họ với per-patch routing, "hầu hết các hình ảnh
sử dụng – tổng hợp bằng cách gộp tất cả các patches
của chúng – hầu hết các experts" [13, Phụ lục E.3].
Do đó, per-patch routing có thể tăng overhead tính toán
và bộ nhớ của cơ chế routing và giảm hiệu quả tổng
thể của mô hình. Thay vào đó, chúng tôi đề xuất sử dụng
per-image routing (tức là, các đầu vào x là toàn bộ hình
ảnh) để giảm số lượng experts được kích hoạt mỗi hình
ảnh, như cũng được thực hiện trong các nghiên cứu
sớm về MoEs [7, 9].

Super-class-based routing. Các nghiên cứu trước đây
về sparse MoEs huấn luyện router end-to-end cùng với
các experts và backbone ViT dense, để cho phép mô hình
học phân công tối ưu từ đầu vào đến experts dựa trên
dữ liệu [13]. Trong khi việc học cơ chế routing tối ưu
từ đầu có thể dẫn đến hiệu suất cải thiện, nó thường
dẫn đến bất ổn huấn luyện và sụp đổ expert, trong đó
hầu hết các đầu vào được định tuyến đến chỉ một tập
con nhỏ các experts, trong khi tất cả các experts khác
bị bỏ qua trong quá trình huấn luyện [3].

Do đó, một auxiliary loss bổ sung thường được yêu cầu
để đảm bảo cân bằng tải giữa các experts khác nhau,
điều này có thể tăng độ phức tạp của quá trình huấn
luyện [3].

Ngược lại, chúng tôi đề xuất nhóm các lớp của tập dữ
liệu thành các siêu lớp và rõ ràng huấn luyện router
để mỗi expert chuyên môn hóa trên một siêu lớp. Để
đạt được điều này, chúng tôi thêm một cross-entropy
loss L_g bổ sung giữa đầu ra router g(x) trong Phương
trình (2) và các nhãn siêu lớp ground truth vào regular
classification loss L_C để có được weighted loss tổng
thể L=L_C+λL_g (chúng tôi sử dụng λ= 0.3 trong các
thí nghiệm của chúng tôi, mà chúng tôi thấy hoạt động
tốt). Một phân chia siêu lớp như vậy thường được cung
cấp sẵn với tập dữ liệu (ví dụ cho CIFAR-10/100 hoặc
MS-COCO). Nếu một tập dữ liệu không có phân chia
siêu lớp, chúng ta có thể dễ dàng có được một phân
chia như sau: 1) chúng ta đầu tiên huấn luyện một mô
hình baseline dense trên tập dữ liệu; 2) sau đó chúng
ta tính toán confusion matrix của mô hình trên một
tập validation tách biệt; 3) cuối cùng chúng ta xây dựng
một confusion graph từ confusion matrix và áp dụng
một thuật toán graph clustering để có được phân chia
siêu lớp [8]. Phương pháp này khuyến khích các siêu
lớp chứa các hình ảnh có liên quan về mặt ngữ nghĩa
mà mô hình thường nhầm lẫn. Trực quan, bằng cách
cho phép các experts MoE khác nhau chuyên môn hóa
trên các cụm dữ liệu ngữ nghĩa khác nhau, hiệu suất
trên các lớp bị nhầm lẫn cao sẽ được cải thiện. Chúng
tôi sử dụng phương pháp này trong các thí nghiệm của
chúng tôi trên ImageNet-1k, tính toán confusion matrix
thông qua mô hình dense ViT-S/16. Phân chia siêu lớp
kết quả cho E= 10 experts được thể hiện trong Bảng 1;
các siêu lớp chứa các lớp liên quan về mặt ngữ nghĩa.

--- TRANG 2 ---

[THIS IS FIGURE: Minh họa kiến trúc mô hình với 4 phần: (a) Dense ViT, (b) Regular V-MoE, (c) Mobile V-MoE, (d) Layer types]

Hình 2. Kiến trúc mô hình. (a) Mô hình baseline dense ViT sử dụng các lớp dense ViT xuyên suốt. (b) Regular sparse V-MoE với các routers per-patch theo lớp. (c) Thiết kế sparse Mobile V-MoE được đề xuất của chúng tôi với một router per-image duy nhất. Trong cả (b) và (c), các lớp dense ViT được theo sau bởi các lớp MoE-ViT (ở đây, k= 1 trong số E= 3 experts được kích hoạt mỗi đầu vào). (d) Khác với các lớp dense ViT [19], các lớp MoE-ViT có một MLP riêng biệt cho mỗi expert (được đi trước bởi một router) trong khi tất cả các phần khác của lớp được chia sẻ giữa tất cả các experts [13].

[THIS IS TABLE: Bảng 1 - Phân chia siêu lớp cho E= 10]
ID | Lớp | Siêu lớp
0 | boxer, pug, Rottweiler | chó
1 | orangutan, weasel, panda | động vật có vú khác
2 | toucan, flamingo, ostrich | chim
3 | eel, scorpion, hammerhead | động vật khác
4 | minivan, ambulance, taxi | phương tiện đất
5 | submarine, canoe, pirate | phương tiện biển
6 | guacamole, hotdog, banana | thức ăn
7 | backpack, pyjama, kimono | quần áo
8 | monitor, iPod, photocopier | thiết bị công nghệ
9 | xylophone, harp, trumpet | nhạc cụ

Bảng 1. Phân chia siêu lớp cho E= 10. Đối với mỗi siêu lớp, chúng tôi liệt kê ba tên lớp được chọn ngẫu nhiên (hóa ra có liên quan về mặt ngữ nghĩa) cùng với một tên siêu lớp có thể.

3. Thí nghiệm

Giờ chúng tôi trình bày kết quả thực nghiệm trên benchmark phân loại ImageNet-1k chuẩn [14]. Chúng tôi huấn luyện tất cả các mô hình từ đầu trên tập huấn luyện ImageNet-1k gồm 1.28M hình ảnh, và sau đó đánh giá top-1 accuracy của chúng trên tập validation tách biệt gồm 50K hình ảnh. Trong Mục 3.1, chúng tôi đầu tiên đánh giá sparse Mobile V-MoE được đề xuất trên một loạt các quy mô mô hình và cho thấy rằng chúng đạt được sự cân bằng hiệu suất vs. hiệu quả tốt hơn so với các baseline ViT dense tương ứng. Trong Mục 3.2, chúng tôi sau đó tiến hành một số nghiên cứu ablation để hiểu rõ hơn về các tính chất của thiết kế mô hình sparse MoE và quy trình huấn luyện được đề xuất.

3.1. Accuracy vs. efficiency trên các quy mô ViT

Chúng tôi xem xét các mô hình ViT (cả MoEs và các baseline dense tương ứng) có kích thước khác nhau bằng cách mở rộng tổng số lớp (chúng tôi sử dụng 12, 9 hoặc 6) và kích thước embedding ẩn (chúng tôi sử dụng 384, 192, 96 hoặc 64). Số lượng đầu multi-head self-attention là (6, 3, 3, 2) cho các kích thước embedding ẩn khác nhau. Kích thước embedding của MLP là 4× kích thước embedding ẩn, như thực hành phổ biến. Chúng tôi sử dụng E= 10 experts tổng cộng cho MoE, trong đó k= 1 được kích hoạt mỗi hình ảnh đầu vào. Các MoE của chúng tôi bao gồm L= 2 lớp MoE-ViT được đi trước bởi (10, 7 hoặc 4) lớp dense ViT (xem Hình 2c). Chúng tôi sử dụng patch size 32×32 cho tất cả các mô hình. Điều này là vì patch size có hiệu quả kiểm soát sự cân bằng giữa FLOPs và số lượng tham số mô hình: vì chúng tôi nhắm đến tối ưu hóa cho FLOPs, một patch size lớn hơn (dẫn đến số lượng patches ít hơn) là có lợi. Chúng tôi cũng đã thử sử dụng patch size nhỏ hơn 16×16, trong đó các xu hướng kết quả về cơ bản là giống nhau (nhưng trong đó số lượng FLOPs cao hơn so với dung lượng mô hình và do đó accuracy). Đối với các ViTs có kích thước ẩn 384 và 192, chúng tôi sử dụng công thức huấn luyện DeiT [17], trong khi đối với kích thước ẩn 96 và 64, chúng tôi sử dụng công thức huấn luyện ViT chuẩn [2] để tránh underfitting. Hình 1 và 3a so sánh top-1 validation accuracy vs. FLOPs. Các Mobile V-MoEs của chúng tôi vượt trội hơn các baseline ViT dense tương ứng trên tất cả các kích thước mô hình.

3.2. Nghiên cứu ablation

Chúng tôi huấn luyện DeiT-Tiny [17] (12 lớp tổng cộng, 192 embedding size, 16×16 patch size) với k= 1 trong số E= 10 experts mỗi đầu vào, và với L= 2 lớp MoE (trừ khi có ghi chú khác); baseline ViT dense đạt 70.79% accuracy.

Tổng số experts E. Chúng tôi xem xét các độ rộng khác nhau của MoE, tức là số lượng experts E khác nhau (và do đó các siêu lớp), dao động từ E= 5 đến E= 20. Chúng tôi báo cáo cả accuracy của toàn bộ mô hình MoE (tức là trên tác vụ phân loại 1,000-way), cũng như accuracy của router (tức là trên tác vụ siêu phân loại E-way). Hình 3b cho thấy rằng hiệu suất tổng thể cải thiện đến E= 10, từ điểm đó trở đi nó trì trệ. Accuracy của router cũng giảm vượt quá E= 10 do độ khó tăng của bài toán siêu phân loại E-way.

Số lớp MoE L. Chúng tôi xem xét các độ sâu khác nhau của MoE, tức là số lượng lớp MoE L khác nhau, dao động từ L= 1 đến L= 8 (trong tổng số 12 lớp ViT). Chúng tôi lại báo cáo cả accuracy của MoE đầy đủ và router. Hình 3c cho thấy rằng hiệu suất tổng thể đạt đỉnh tại L= 2, và giảm nhanh cho L lớn hơn. Điều này là do accuracy của router, giảm với L tăng vì router nhận được ít thông tin hơn (từ 12−L lớp ViT).

Số experts k mỗi hình ảnh. Chúng tôi thay đổi số lượng experts k được kích hoạt mỗi hình ảnh. Chúng tôi so sánh với các baseline dense sử dụng MLP với kích thước ẩn được mở rộng k lần để khớp với FLOPs suy luận của MoE. Hình 3d cho thấy rằng k= 1 và k= 2 hoạt động tốt nhất (so với baseline dense), với delta hiệu suất giảm cho k lớn hơn.

Các chiến lược routing. Chúng tôi so sánh super-class per-image routing ngữ nghĩa được đề xuất vs. end-to-end-learned routing (cả per-image và per-token) và một baseline với các siêu lớp ngẫu nhiên (cho k=2). Hình 3e cho thấy rằng phương pháp của chúng tôi (Hình 2c) tốt hơn, ngoại trừ learned per-token routing (như trong regular V-MoE [13], Hình 2b), tuy nhiên cần kích hoạt nhiều experts hơn và do đó nhiều tham số mô hình hơn cho mỗi hình ảnh đầu vào (lên đến 11.05M, vs. 6.31M cho chúng tôi).

--- TRANG 3 ---

[THIS IS TABLE: Bảng kết quả thực nghiệm với nhiều phần:]
Model | FLOPs | Top-1 Accuracy
      |       | Dense | MoE | ∆
12×384| 2297M | 71.88 | 74.23 | +2.35
9×384 | 1752M | 69.94 | 72.47 | +2.53
6×384 | 1207M | 63.21 | 66.91 | +3.70
12×192| 1618M | 59.51 | 62.90 | +3.39
9×192 | 478M  | 56.50 | 59.52 | +3.02
6×192 | 338M  | 51.18 | 55.69 | +4.51
12×96 | 176M  | 53.79 | 55.39 | +1.60
9×96  | 140M  | 51.27 | 52.99 | +1.72
6×96  | 103M  | 46.54 | 50.28 | +3.74
12×64 | 88M   | 42.90 | 46.07 | +3.17
9×64  | 71M   | 40.46 | 43.95 | +3.49
6×64  | 54M   | 36.64 | 41.30 | +4.66

(a) Accuracy vs. efficiency trên các quy mô ViT.

[Additional tables showing experiments with different parameters E, L, k, and routing strategies...]

Hình 3. Kết quả thực nghiệm. (a) Các Mobile V-MoEs của chúng tôi vượt trội hơn các ViTs dense tương ứng trên các quy mô mô hình. Tên mô hình (ví dụ 12×192) biểu thị số lượng lớp (12) và kích thước embedding (192). (b-e) Các nghiên cứu ablation sử dụng DeiT-Ti/16 [17], với k= 1, E= 10, L= 2 theo mặc định. Sự cân bằng hiệu suất vs. hiệu quả tốt nhất được đạt với (b) E= 10 experts tổng cộng, (c) L= 2 lớp MoE (trong tổng số 12 lớp), (d) k= 1 hoặc k= 2 experts được kích hoạt mỗi hình ảnh, (e) super-class routing ngữ nghĩa của chúng tôi; các thiết lập được sử dụng trong (a) được in đậm.

4. Kết luận và nghiên cứu tương lai

Chúng tôi đã chứng minh rằng sparse MoEs có thể cải thiện sự cân bằng hiệu suất vs. hiệu quả so với các ViTs dense, trong nỗ lực làm cho ViTs phù hợp hơn cho các ứng dụng có hạn chế tài nguyên. Trong tương lai, chúng tôi nhắm đến áp dụng thiết kế MoE của chúng tôi cho các mô hình thân thiện với thiết bị di động hơn ViTs, ví dụ như các CNNs nhẹ như MobileNets [5,6,15] hoặc các hybrid ViT-CNN [1, 11, 18]. Chúng tôi cũng nhắm đến xem xét các tác vụ thị giác khác, ví dụ như phát hiện đối tượng. Cuối cùng, chúng tôi nhắm đến có được các phép đo latency thực tế trên thiết bị cho tất cả các mô hình.

--- TRANG 4 ---
[Danh sách tài liệu tham khảo từ [1] đến [21]]
