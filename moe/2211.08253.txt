# 2211.08253.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2211.08253.pdf
# File size: 2672653 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization
Jingang Qu1Thibault Faney2Ze Wang1Patrick Gallinari1,3
Soleiman Yousef2Jean-Charles de Hemptinne2
Sorbonne Université, CNRS, ISIR, 75005 Paris, France1IFPEN2Criteo AI Lab, Paris, France3
Abstract
Due to domain shifts, machine learning systems typically
struggle to generalize well to new domains that differ from
those of training data, which is what domain generalization
(DG) aims to address. Although a variety of DG methods
have been proposed, most of them fall short in interpretabil-
ity and require domain labels, which are not available in
many real-world scenarios. This paper presents a novel
DG method, called HMOE: Hypernetwork-based Mixture
of Experts (MoE), which does not rely on domain labels
and is more interpretable. MoE proves effective in identi-
fying heterogeneous patterns in data. For the DG problem,
heterogeneity arises exactly from domain shifts. HMOE
employs hypernetworks taking vectors as input to generate
the weights of experts, which promotes knowledge sharing
among experts and enables the exploration of their simi-
larities in a low-dimensional vector space. We benchmark
HMOE against other DG methods under a fair evaluation
framework – DomainBed. Our extensive experiments show
that HMOE can effectively separate mixed-domain data
into distinct clusters that are surprisingly more consistent
with human intuition than original domain labels. Using
self-learned domain information, HMOE achieves state-of-
the-art results on most datasets and significantly surpasses
other DG methods in average accuracy across all datasets.
1. Introduction
Domain generalization (DG) aims to train models on known
domains to perform well on unseen domains, which is cru-
cial for deploying models in safety-critical applications.
Over the past decade, a variety of DG algorithms have
been proposed [28, 87, 101], focusing primarily on devel-
oping DG-specific data augmentation techniques and learn-
ing domain-invariant representations to build generalizable
predictors. However, many high-performing DG algorithms
rely on domain labels to explicitly reduce inter-domain dif-
ferences, severely limiting their applicability in real-worldscenarios where domain annotation may be prohibitively
expensive. Additionally, current DG algorithms lack inter-
pretability and cannot provide insight into the causes of suc-
cess or failure in generalizing to new domains. Therefore,
this work aims to develop a novel DG algorithm that does
not require domain labels and is more interpretable.
We follow the nomenclature established by [11], which
refers to DG with domain labels as vanilla DG and the
more challenging DG without domain labels as compound
DG. This work focuses on addressing compound DG by
inferring latent domains from mixed-domain data and us-
ing them effectively. [6, 14, 58] demonstrated that using
domain-wise datasets can theoretically yield lower general-
ization error bounds and better DG performance compared
to using mixed data directly, indicating the importance of
domain information. Furthermore, latent domain discovery
helps us understand the workings of models and enhances
interpretability. To make the problem tractable, we assume
that latent domains are distinct and separable.
In this paper, we introduce HMOE: Hypernetwork-based
Mixture of Experts ( MoE ). MoE is a well-established learn-
ing paradigm that aggregates a number of experts by calcu-
lating the weighted sum of their predictions [36, 37], where
the aggregation weights, commonly referred to as gate val-
ues, are determined by a routing mechanism and add up to
1. HMOE capitalizes on MoE’s divide and conquer prop-
erty, that is, the routing mechanism can softly partition the
input space into subspaces in an unsupervised manner dur-
ing training [92], with each subspace assigned to an expert.
We further expect that each subspace is associated with a
latent domain, enabling latent domain discovery. During
inference, we can compare the similarities between an un-
seen test domain and the inferred domains based on gate
values, hence improving interpretability. [29, 97] have vali-
dated MoE in domain adaptation [88] and showed that MoE
can leverage the specialty of individual domain and allevi-
ate negative knowledge transfer [75] compared to using a
single model to learn different domains concurrently.
HMOE innovatively uses a neural network, called hyper-
network [30], which takes vectors as input to generate the
weights for MoE’s experts. By mapping vectors to experts,
1arXiv:2211.08253v3  [cs.LG]  14 Nov 2023

--- PAGE 2 ---
hypernetworks enable the exploration of experts’ similar-
ities in a low-dimensional vector space, facilitating latent
domain discovery. Hypernetworks also serve as a bridge
between experts and provide them a channel to exchange
information, thereby promoting knowledge sharing.
MoE’s intrinsic soft partitioning is not always effective
and sometimes fails to maintain a consistent data division,
especially when the distinction between latent domains is
not significant. To address this issue, we propose a dif-
ferentiable dense-to-sparse Top-1 routing algorithm, which
forces gate values to become one-hot and converges to hard
partitioning. This leads to sparse-gated MoE, which im-
proves and stabilizes latent domain discovery. In addition,
to better incorporate hypernetworks into MoE, we introduce
an embedding space that contains a set of learnable embed-
ding vectors corresponding one-to-one with experts. This
embedding space is fed to hypernetworks to generate the
weights of experts and is also part of the routing mecha-
nism to compute gate values, thus enhancing the interaction
between hypernetworks and the routing mechanism.
We also propose an intra-domain mixup to further im-
prove the generalization ability of HMOE. mixup creates
virtual training samples by taking a linear combination of
two randomly chosen inputs and their labels [93], and we
perform mixup within each inferred latent domain.
Our contributions are as follows: (1) We present a novel
DG method – HMOE within the framework of MoE, that
does not require domain labels, enables latent domain dis-
covery, and offers excellent interpretability. (2) HMOE
leverages hypernetworks to generate expert weights and
achieves sparse-gated MoE. (3) As far as we know, HMOE
is the first work that can jointly learn and use latent do-
mains in an end-to-end way. (4) Extensive experiments are
conducted to compare HMOE with other DG methods un-
der a fair evaluation framework – DomainBed [28]. HMOE
exhibits state-of-the-art performance on most datasets and
greatly outperforms other DG methods in average accuracy.
2. Related Work
2.1. Domain Generalization (DG)
The goal of DG is to train a predictor on known domains
that can generalize well to unseen domains.
Vanilla DG The first line of work is to design DG-
specific data augmentation techniques to increase the di-
versity and quantity of training data to improve DG per-
formance [51, 65, 70, 84, 91, 93, 98, 100]. Previous work
learned domain-invariant representations through invariant
risk minimization [1, 2, 41], kernel methods [6, 22, 26, 58],
feature alignment [25, 47, 56, 57, 61, 63, 76, 78, 86], and
domain-adversarial training [23, 24, 27, 47, 49]. Another
approach is to disentangle latent features into class-specific
and domain-specific representations [35, 38, 59, 64, 94].General machine learning paradigms were also applied to
vanilla DG, such as meta-learning [3, 17, 45, 46], self-
supervised learning [8, 39], gradient manipulation [34, 66,
72], and distributionally robust optimization [41, 67].
Compound DG There are some DG algorithms that do
not require domain labels by design [11, 34, 48, 56, 59, 94].
Besides improving DG performance, latent domain discov-
ery is also an important task for compound DG and con-
tributes to better interpretability. [11, 56] can do this but
have two main limitations: (1) Their methods proceed in
two phases: first infer latent domains from mixed data and
then deal with DG using the inferred domains, which is sim-
ilar to vanilla DG. The problem is that the second phase de-
pends on the first and cannot provide some feedback to cor-
rect possible errors in domain discovery. (2) Their methods
assume that domain shift arises from stylistic differences to
identify latent domains, which does not always hold.
On the contrary, HMOE is trained in an end-to-end man-
ner and leverages MoE to discover latent domains without
an explicit induced bias on the cause of domain shift.
2.2. Hypernetworks
A hypernetwork is a neural network that generates the
weights of another target network. Hypernetworks were ini-
tially proposed by [30] and have since been applied to opti-
mization problems [53, 60], meta-learning [96], continuous
learning [7, 85], multi-task learning [50, 54, 77], few-shot
learning [68], and federated learning [69].
2.3. Mixture of Experts (MoE)
Gate Network Expert 1 Expert 2 Expert 
(a) Classical MoE
Experts0.57 0.18 0.05 0.21
0.1 1 0.27 0.59 0.02
0.09 0.35 0.13 0.74
1 1 1 1Input examples (b) Gate value matrix
Figure 1. (a) MoE calculates the weighted sum of experts’ outputs.
(b) Gate values are determined by a gate network.
MoE was originally proposed by [36, 37] and consists
of two main components: experts and a gate network, as
shown in Fig. 1. The output of MoE is the weighted sum of
experts, with gate values calculated by the gate network on a
per-example basis. In recent years, MoE has regained atten-
tion as a way to scale up deep learning models and more ef-
ficiently harness modern hardware [18, 20, 21, 42, 71, 102].
In this case, sparse MoE is preferred, which routes each ex-
ample only to the experts with Top-1 or Top-K gate values.
2

--- PAGE 3 ---
2.4. Application of Hypernetworks and MoE in DG
As far as we know, no work has applied hypernetworks to
solve DG in computer vision. Recently, [83] applied hyper-
networks to DG in natural language processing (NLP) and
achieved SOTA results on two NLP-related DG tasks.
As for MoE, [43] proposed replacing feed-forward net-
work layer (FFN) of Vision Transformer (ViT) [16] with a
sparse mixture of FFN experts to improve DG performance.
[29, 97] applied MoE to a task similar to DG, namely do-
main adaptation [88], but they require domain labels to train
an expert for each domain separately. [97] aggregates the
outputs of experts via a transformer-based aggregator, but
its aggregator is trained with fixed experts and cannot pro-
vide probabilities of experts, while HMOE can do this and
is more interpretable. In addition, if we regard MoE as a
kind of ensemble method, [15, 55, 99] share the same spirit.
3. Method
3.1. Problem Setting
LetXdenote an input space and Ya target space. A do-
main Sis characterized by a joint distribution Ps
XYon
X × Y . In vanilla DG setting, we have a training set
containing Mknown domains, i.e.,DV
tr={Ds}M
s=1with
Ds={(xs
i, ys
i, ds
i)}Ns
i=1where (xs
i, ys
i)∼Ps
XYandds
iis
the domain index or label. Also consider a test dataset Dte
composed of unknown domains different from those of DV
tr.
Vanilla DG aims to train a robust predictor f:X → Y
onDV
trto achieve a minimum predictive error on Dte,i.e.,
minfE(x,y)∼Dte[ℓ(f(x), y)], where ℓis the loss function.
Our work focuses on the more difficult compound DG,
for which the training set Dtr={(xi, yi)}N
i=1contains
mixed domains and has no domain annotation. However,
as demonstrated in [28, 87, 101], intrinsic inter-domain re-
lationships play a key role in obtaining better generalization
performance. Therefore, our proposed HMOE is designed
to discover latent domains by dividing Dtrinto clusters and
to fully leverage the learned domain information in order to
perform well on unknown domains.
3.2. Overall Architecture
An overview of HMOE is illustrated in Fig. 2a. HMOE
processes input xthrough two paths: the domain path for
latent domain discovery and the classifier path to train an
expert for each latent domain.
The classifier path begins with a featurizer hzto extract
high-level features from x, which can be a pretrained net-
work, such as VGG [74], ResNet [32], or ViT [16]. We
define a discrete learnable embedding space Econsisting
ofKembedding vectors {ek∈RD}K
k=1(Drepresents the
embedding dimension), each corresponding to a classifier
expert. These vectors are fed into a hypernetwork fhto
generate a set of weights {θk}K
k=1, which further form a setof experts {fc(:;θk)}K
k=1. The output of the featurizer zis
passed to these experts to compute their corresponding out-
puts, that is, yk=fc(z;θk).
The domain path begins with a Domain2Vec (D2V) en-
coder hv, which transforms xinto the embedding space E
and outputs v∈RD. The output vis then compared with
the embedding vectors through a predefined gate function
g(v,E), as shown in Fig. 2b, to produce a set of probabilities
p={pk}K
k=1. The final output of HMOE is the weighted
sum of the outputs of experts as follows:
y=KX
k=1pkyk=g(hv(x),E)•[fc(hz(x);fh(ek))]K
k=1(1)
3.3. Hypernetworks
We employ a hypernetwork fhtaking a vector eas input to
produce weights for classifier fc. In our work, both fhand
fcare MLPs. Essentially, fcacts as a computational graph
placeholder, eis a conditioning signal, and fhmaps eto a
function. The roles of fhinclude: (1) easing latent domain
discovery, (2) using many experts without a major increase
in parameters, (3) offering another interaction between ex-
perts and the routing mechanism besides the aggregation of
experts compared to the classical MoE, and (4) enabling the
generalization of experts beyond aggregation (As we will
see later, fhcan directly take the D2V encoder as input).
3.4. Routing Mechanism
3.4.1 Gate Function
To quantify the responsibilities of experts for each input ex-
ample and to aggregate experts’ outputs, we need to calcu-
late gate values p. As shown in Fig. 2b, based on the output
of the D2V encoder vand the embedding space E, we define
a gate function g(v,E)to calculate pas follows:
dk=∥v−ek∥2 (2a)
sk=−log(d2
k+ϵ) (2b)
pk=exp(sk)PK
j=1exp(sj)(2c)
where ϵis a small value. The negative logarithm in Eq. (2b)
is used to establish a negative correlation between dkand
pk(i.e., the smaller dk, the larger pk) and to nonlinearly
rescale the distance d(i.e., stretch small dand squeeze great
d), which makes pless sensitive to large d.
3.4.2 Differentiable Dense-to-Sparse Top-1 Routing
Based on gate values p, the routing mechanism determines
where and how to route input examples. A consistent and
cohesive routing is crucial to the training stability and con-
vergence of MoE [12]. In order to stabilize the routing and
3

--- PAGE 4 ---
Featurizer  D2V
EncoderGate
Function  
 
Embedding
Space  
  
Classifier Output  Gate values  
Mixed data  
HyperNetwork  (a) An overview of HMOE
Gate Function  
Euclidean distance 
Softmax (b) Gate function
Figure 2. (a) In the upper branch ( i.e., the domain path), the input goes through the D2V encoder into the embedding space, and a predefined
gate function calculates gate values. In the lower branch ( i.e., the classifier path), the hypernetwork takes the embedding vectors as input
to create a set of classifiers. The final output is the weighted sum of the outputs of classifiers. (b) The gate function determines gate values
based on the distances between the output of the D2V encoder and embedding vectors. Smaller distances yield greater gate values.
enhance latent domain discovery to capture less obvious do-
main differences, sparse-gated MoE is preferable. However,
the commonly used Top-1 or Top-K functions are not dif-
ferentiable and may cause oscillatory behavior of gate val-
ues during training [31]. To overcome this limitation, we
propose a differentiable dense-to-sparse Top-1 routing al-
gorithm by introducing an entropy loss on pas follows:
Len=E(x,y)∼Dtr
H 
g(hv(x),E)
(3)
where H(·)denotes the entropy of a distribution. In prac-
tice, we multiply Lenbyγenthat linearly increases from
0 to 1 in the first half of training and remains at 1 in the
second. Early on, γenis small, and the distances between
vand the embedding vectors are almost the same, leading
to a uniform p. Therefore, all experts can be fully trained
and gradually become specialized. In the later stages, Len
forces pto become one-hot based on specialized experts.
Due to the negative logarithm in Eq. (2b), the D2V en-
coder has to move towards one embedding vector to mini-
mizeLeninstead of moving away from others.
3.4.3 Expert Load Balancing
Sparse-gated MoE may suffer from an unbalanced expert
load. We define the importance of experts as I(X) =
[I1(X),···, IK(X)], where Xrepresents a single batch
andIk(X)is specified as the sum of gate values assigned
to the kth expert ( i.e., sum the gate value matrix in Fig. 1b
along the example dimension). [62] defines a distribution
P=I(X)/PI(X)and uses the KL-divergence betweenPand the uniform distribution Uto balance the expert load,
which is also used in our work:
Lkl=DKL(P∥U) =DKLI(X)PI(X)∥U
(4)
3.5. Embedding Space
The embedding space Eplays a key role in HMOE. As
we can see, the embedding vectors have an effect on both
the generation of expert weights and the routing mecha-
nism, thus serving as a bridge to balance these two parts.
In addition, these embedding vectors are learnable like the
weights of neural networks and attract the D2V encoder
during training under the influence of Len.
3.6. Class-Adversarial Training on D2V
We expect the D2V encoder hvto contain as little class-
specific information as possible, which ensures that HMOE
partitions the input space based on domain-wise distinc-
tion rather than semantic categories. Inspired by Domain-
Adversarial Neural Networks [24], we define an adversarial
classifier fad
ctaking vas input and add the following loss to
perform class-adversarial training on hv:
Lad=E(x,y)∼Dtr
ℓce(fad
c(GRL (v, λgrl)), y)
(5)
where ℓcedenotes the cross-entropy loss and GRL repre-
sents the gradient reversal layer, which acts as an identity
function in the forward pass and multiplies the gradient by
−λgrlin the backward pass. As suggested in [24], we de-
fineλgrlas follows:
λgrl= 2/(1 + exp( −10×pcttr))−1 (6)
4

--- PAGE 5 ---
where pcttrvaries linearly from 0 to 1 during training.
3.7. Supervised Learning on Targets
We provide two ways to calculate the supervised loss on tar-
getsLy, that is, Empirical Risk Minimization (ERM) [81]
and the intra-domain mixup .
ERM In the setting of ERM, the supervised loss on tar-
gets is simply the empirical risk on the training data Dtr:
Ly=E(x,y)∼Dtr[ℓce(ˆy, y)] (7)
where ˆyis the prediction of HMOE, as calculated by Eq. (1).
Intra-domain mixup mixup trains a neural network on
virtual samples synthesized through convex combinations
of pairs of samples and their labels [93]:
˜x=βxi+ (1−β)xj (8)
˜y=βyi+ (1−β)yj (9)
where β∼Beta(α, α)andαadjusts interpolation strength.
mixup can be seen as a data augmentation approach theoret-
ically grounded in Vicinal Risk Minimization [10], which
is an alternative learning principle to ERM. [89, 90] applied
the inter-domain mixup mixing samples across different
domains for domain-invariant learning, whereas our intra-
domain mixup , as shown in Algorithm 1, prompts HMOE
for smoother predictions in neighborhood within each do-
main, enhancing its generalization and robustness.
To perform the intra-domain mixup without domain la-
bels, HMOE starts with Eq. (7) and then switches to Al-
gorithm 1 until Len<0.1indicating latent domains are
reasonably discovered and clustered.
Algorithm 1 intra-domain mixup
Require: A mini-batch Bsplit into distinct domains given
domain labels or clusters identified by gate values
1:for each domain or cluster Bi∈ B do
2:eBi←mixup (Bi,shuffled Bi)withβ∼Beta(α, α)
▷Mix same-index samples between Biand shuffled Bi
3: Compute the empirical risk LioneBi
4:end for
5:Ly←Average over all Li
3.8. Semi-/supervised Learning on Domains
Due to the probabilistic nature of MoE, given an input x
and the corresponding gate values p={pk}K
k=1, we can
interpret pkas the probability of selecting the kth expert
Ekgiven x,i.e.,p(Ek|x). In addition, Ekis thought to be
associated with a specific domain Sm. Therefore, we get
pk=p(Ek|x) =p(Sm|x). Consider a dataset with domain
labelsDd={(xi, di)}Nd
i=1(class labels are not necessary)
withdi∈ {1, . . . , M d}, we can make use of Ddas follows:
Ld=E(x,d)∼Dd[ℓce(p, d)] (10)Mdmay be smaller than K, but this has no bearing on the
calculation of Ld. In this case, we assume that the first Md
experts are assigned to Mddomains, while the rest learn
autonomously without domain information. If all domain
labels are given, Ldshifts to supervised domain learning.
3.9. Training and Inference
The final training loss is:
L=λyLy+λenLen+λklLkl+λadLad+λdLd(11)
where λare trade-off hyper-parameters to balance different
losses. Generally, λyis set to 1 and Ldis not used for com-
pound DG without domain labels.
For inference, we offer two modes: MIX and OOD. MIX
refers to the mixture of experts, as calculated by Eq. (1).
OOD1(Out of Domain) uses the output of a classifier whose
weights are generated by the hypernetwork directly taking
the D2V encoder as input. OOD enables the generalization
of experts beyond aggregation.
4. Experiments
This paper focuses on image classification. However, to il-
lustrate HMOE’s learning dynamics and versatility, we also
apply it to a toy regression task to learn a one-dimensional
function defined on 3 intervals. HMOE proves effective in
assigning an expert to each interval. Due to space limits, de-
tails are in the supplementary material . Next, we evaluate
HMOE against other DG algorithms on DomainBed [28].
4.1. Datasets and Model Evaluation
DomainBed offers a unified codebase to implement, train,
and evaluate DG algorithms, and integrates commonly used
DG-related datasets. We experiment on Colored MNIST
(3 domains and 2 classes) [2], Rotated MNIST (6 domains
and 10 classes) [25], PACS (4 domains and 7 classes) [44],
VLCS (4 domains and 5 classes) [19], OfficeHome (4 do-
mains and 65 classes) [82], and TerraIncognita (4 domains
and 10 classes) [4]. Detailed dataset statistics and sample
visualization are provided in the supplementary material .
For model selection and hyper-parameter tuning, Do-
mainBed offers three options, of which we choose the
training-domain validation that allocates 80% from each
training domain for training and the rest for validation. This
option aligns well with compound DG without access to do-
main labels and test domains.
4.2. Implementation Details
For CMNIST and RMNIST, we use a four-layer ConvNet as
the featurizer (see Appendix D.1 of [28]). The D2V encoder
hvconnects this four-layer ConvNet to a fully-connected
(fc) layer in order to map to the embedding dimension D.
1OOD is efficiently realized using PyTorch-based JAX-like functorch .
5

--- PAGE 6 ---
For other datasets, we use ResNet-50 pretrained on Im-
ageNet [13] as the featurizer and freeze all batch normal-
ization layers. The D2V encoder hvcascades 3 conv layers
(64-128-256 units, stride 2, 4×4kernels, ReLU), two resid-
ual blocks (each has 2 conv layers with 256 units, 3×3ker-
nels, ReLU), and a 3×3conv layer with Dunits followed
by global average pooling. We use Instance Normalization
[79] with learnable affine parameters before all ReLU of hv.
For all datasets, the classifier fcis a fc layer whose in-
put size is the featurizer’s output size (128 for ConvNet
and 2048 for ResNet-50) and output size is the number of
classes. The hypernetwork fhis a five-layer MLP with 256-
128-64-32 hidden units and SiLU [33], and its input size is
Dand output size is the total number of learnable parame-
ters ( i.e., weights and biases) of fc. In addition, we initial-
izefhusing the hyperfan method [9]. If Ladis used, the
adversarial classifier is a three-layer MLP with 256 hidden
units and ReLU, and its input size is Dand output size is the
number of classes. We set D= 32 and initialize embedding
vectors with the standard normal distribution.
We define three HMOE variants, including (1) HMOE-
DL: Domain labels are provided. We use Lycalculated by
Eq. (7) and Ldwithλy=λd= 1and discard other losses,
andKis the number of training domains. (2) HMOE-ND :
No domain information is available. We use Lycalculated
by Eq. (7), Len,LklandLadwithλy=λen=λkl= 1and
λad= 0.1, and we fix K= 3. (3) HMOE-MU : The setting
is the same as in HMOE-ND, except that Lyis calculated
via the intra-domain mixup (Algorithm 1) with α= 0.3.
DomainBed trains all DG algorithms with Adam for
5,000 iterations. For Colored and Rotated MNIST / other
datasets, the learning rate is 0.001 / 5e-5, the batch size is
64 / 32 ×the number of training domains, and models are
evaluated on the validation set every 100 / 300 iterations.
Each experiment uses one domain of a dataset as the test do-
main and trains algorithms on the others, which is repeated
three times with different random seeds. The average accu-
racy over three replicates is reported. DG algorithms use the
default settings predefined in DomainBed. All experiments
are conducted using PyTorch on multiple A5000 GPUs.
4.3. Results
The DomainBed benchmark in [28] has been outdated,
and we update it using an improved pretrained ResNet-50
(IMAGENET1K-V2) available on torchvision. The com-
parison of HMOE against other DG algorithms is shown in
Tab. 1, where DeepAll means the vanilla supervised learn-
ing that just fine-tunes ResNet-50 on mixed data and serves
as a performance baseline. We report the average accuracy
of all test domains for each dataset. Refer to the supple-
mentary material for detailed results.
HMOE-MU outperforms all other DG algorithms in av-
erage accuracy. Notably, mixup -powered algorithms showimpressive performance, proving the effectiveness of mixup
in enhancing generalization. Both Mixup [90] (second
place) and SelfReg [39] (third place) adopted the inter-
domain mixup to learn domain-invariant representations.
HMOE-ND ranks fourth overall, but is the top among algo-
rithms without mixup . In addition, HMOE-ND / MU largely
surpass the DeepAll baseline, except on RMNIST.
For MNIST datasets, performance is comparable across
algorithms, except for the outstanding results of ARM [95].
Other datasets pose higher challenges. For instance, VLCS
comprises real photo images, with the domain shift primar-
ily caused by changes in scene and perspective, leading
to subtle visual differences between domains. Many algo-
rithms are inferior to DeepAll on these challenging datasets.
HMOE-MU achieves state-of-the-art results on PACS, Of-
ficeHome, and TerraInc, and its performance on VLCS is
nearly on par with the best result (78.6 vs. 78.9). HMOE-
ND also performs impressively. All these findings validate
the superiority of HMOE in addressing compound DG.
HMOE-MU markedly surpasses ND. Fig. 3a presents a
comparison of their validation / test accuracy during train-
ing. It is evident that the accuracy of MU continues to
improve with the introduction of intra-domain mixup upon
Len<0.1, because mixup imposes linearity constraints,
which prompts smoothness and mitigates overfitting.
Interestingly, HMOE-DL lags behind HMOE-ND / MU
significantly, indicating that HMOE performs better when
using self-learned domain information rather than relying
on provided domain labels. We observe that the latent
domains discovered by HMOE seem to be more human-
intuitive than given domain labels (Sec. 4.4). Fig. 3b shows
that the supervised loss on domains Ldof HMOE-DL fails
to decrease rapidly on OfficeHome and VLCS datasets.
This could suggest that HMOE struggles to assimilate do-
main label information, which complicates its learning pro-
cess and negatively affects its DG performance.
0 2000 4000
# iterations0.40.60.8AccuracyHMOE-ND
HMOE-MU
0.00.10.51.0
en
(a) HMOE-ND vs.MU
0 2000 4000
# iterations0.00.51.0d
Office
VLCST erra
PACS (b) Average of Ldper dataset
Figure 3. Losses over iterations. (a) ND / MU on OfficeHome
with clipart as the test domain. The upper / lower curves of ND /
MU represent their validation / test accuracy, respectively.
For two inference modes, MIX outperforms OOD in
most cases, but OOD can be used to sacrifice a little ac-
curacy for efficiency in practice because it is more compu-
tationally efficient without computing all experts like MIX.
6

--- PAGE 7 ---
Algorithm M CMNIST RMNIST VLCS PACS OfficeHome TerraInc Avg Ranking
w/ Domain Labels
Mixup [90] ✓ 51.9±0.1 97.6±0.1 78.7±0.1 86.6±0.1 71.6±0.2 51.4±0.4 72.97 2
CORAL [76] 51.4 ±0.1 98.0±0.0 78.1±0.2 86.7±0.4 72.2±0.2 48.9±0.5 72.55 5
VREx [41] 52.2 ±0.1 97.8±0.0 77.3±0.2 86.0±0.7 69.8±0.1 51.8±0.4 72.48 6
Fish [73] 51.5 ±0.1 97.9±0.1 78.1±0.0 86.9±0.9 68.7±0.1 51.0±0.7 72.35 7
ARM [95] 55.6 ±0.3 98.1±0.0 78.0±0.6 85.7±0.8 66.5±0.4 48.5±0.4 72.05 9
MTL [6] 51.5 ±0.2 97.8±0.0 77.3±0.3 85.5±0.2 68.4±0.5 51.3±0.6 71.97 10
GroupDRO [67] 52.1 ±0.0 97.8±0.0 77.8±0.6 85.0±0.8 68.3±0.3 49.6±0.5 71.77 11
MLDG [45] 44.2 ±4.6 97.8±0.0 76.6±0.2 87.1±0.1 68.3±0.3 49.9±1.1 70.65 15
MMD [47] 38.5 ±0.8 98.0±0.0 77.4±0.9 84.2±0.1 69.1±0.0 50.0±1.2 69.53 16
DANN [24] 51.8 ±0.1 97.7±0.0 75.6±0.6 77.0±1.4 66.5±0.3 42.5±2.6 68.52 17
IRM [2] 41.3 ±0.9 87.3±0.4 78.3±1.1 82.1±0.7 64.9±0.3 50.8±1.1 67.45 18
HMOE-DLMIX 51.5 ±0.1 94.1±0.5 77.0±0.4 85.5±0.6 68.9±0.6 49.6±0.2 71.70 14
OOD 57.0±3.9 93.3±0.5 77.9±0.3 85.1±0.8 67.9±0.3 48.3±0.4 71.58
w/o Domain Labels
SelfReg [39] ✓ 51.4±0.1 98.0±0.0 78.9±0.3 86.1±0.3 71.3±0.2 51.5±0.3 72.87 3
SagNet [59] 51.8 ±0.1 98.0±0.0 77.7±0.3 86.2±0.4 69.3±0.2 50.7±0.5 72.28 8
RSC [34] 51.5 ±0.2 97.5±0.1 78.8±0.3 87.0±0.4 65.5±0.9 49.1±1.0 71.57 12
DeepAll [81] 51.4 ±0.1 97.8±0.1 77.5±0.2 85.8±0.4 68.5±0.2 47.7±0.9 71.45 13
HMOE-NDMIX 51.8 ±0.1 97.5±0.1 78.1±0.3 86.6±0.3 69.7±0.2 52.5±0.3 72.70 4
OOD 51.8±0.1 97.5±0.1 78.0±0.4 86.9±0.2 69.0±0.2 51.1±1.4 72.38
HMOE-MU ✓MIX 51.7 ±0.2 97.6±0.1 78.6±0.0 88.0±0.3 72.5±0.1 52.8±0.9 73.53 1
OOD 51.6±0.2 97.6±0.1 78.8±0.3 87.0±1.0 72.4±0.1 52.1±0.9 73.25
Table 1. Domain generalization results on DomainBed. We format first, second and worse than DeepAll results. Mdenotes the inter/intra-
mixup . The performance of HMOE is evidenced by MIX. OOD is only for comparison with MIX and does not participate in the ranking.
4.4. Latent Domain Discovery
We employ t-SNE [80] to visualize the output of the D2V
encoder, as shown in Fig. 4. It is evident that HMOE-ND
effectively separates the mixed data into distinct clusters,
each gravitating towards an embedding vector.
Domain labels are used to color data to highlight the dif-
ferences between them and inferred latent domains. For
PACS with the arttest (Fig. 4a), inferred domains largely
align with domain labels, although some photos are grouped
into the cartoon-predominant cluster. However, with car-
toon as the test domain (Fig. 4b), data is not split based on
artandphoto . Fig. 4e shows that, even with domain labels,
HMOE-DL struggles to fully separate artfrom photo . For
TerraInc (Fig. 4c), points of the same color tend to cluster
together, whereas for OfficeHome (Fig. 4d), different col-
ors intermix within each cluster, highlighting the big gap
between labeled and inferred domains. Fig. 4f also shows
that HMOE-DL has difficulty in data partitioning, explain-
ing the slow decrease in Ldfor OfficeHome in Fig. 3b.
To intuitively understand how HMOE distinguishes be-
tween domains, Fig. 5 compares labeled and inferred do-
mains using visual samples. HMOE-ND seems to partition
TerraInc by illumination and OfficeHome by background
complexity, which aligns more with human intuition.
After the above analysis, we conclude that the success of
HMOE stems from its ability to self-learn more reasonabledomain knowledge. However, this does not mean that given
domain labels are erroneous. There are typically multiple
generative factors behind the data-generating process [5],
rendering the definition of domains multifaceted. HMOE
simply discovers an intuitive and digestible way of data par-
titioning in order to enhance its DG performance.
4.5. Ablation Studies
The role of the intra-domain mixup has been validated be-
fore. In this section, we analyze the contribution of other
components of HMOE through ablation studies, as shown
in Tab. 2. We use the silhouette coefficient (SC) to quanti-
tatively evaluate the clustering of HMOE in terms of cluster
compactness and separation. SC ranges from -1 (poor) to 1
(good). Clusters are identified by gate values and their dis-
tances are measured using the output of the D2V encoder.
Name LenLklLadVLCS PACS Office TerraInc Avg. SC
H1 - - - 78.0 86.8 68.4 50.5 0.37
H2 - - ✓ 77.8 86.9 69.1 51.2 0.27
H3✓ - - 77.3 84.8 69.0 48.2 Collapse
H4✓ -✓ 77.8 86.3 68.6 49.2 Collapse
H5✓ ✓ - 77.7 86.8 68.7 50.5 0.65
H6✓ ✓ ✓ 78.1 86.6 69.7 52.5 0.60
Table 2. Ablation studies for HMOE-ND ( ✓means the corre-
sponding loss is used and SC denotes the silhouette coefficient.)
7

--- PAGE 8 ---
Cartoon
PhotoSketch
Art(a) PACS-ND (Art)
Art
PhotoSketch
Cartoon (b) PACS-ND (Cartoon)
L100
L38L46
L43 (c) TerraInc-ND (L43)
Art
ProductReal
Clipart (d) Office-ND (Clipart)
Art
PhotoSketch
Cartoon (e) PACS-DL (Cartoon)
Art
ProductReal
Clipart (f) Office-DL (Clipart)
Figure 4. The t-SNE visualization of the output of the D2V encoder. The suffixes in captions denote HMOE-DL / ND, with the test domain
in parentheses. Red squares are embedding vectors, black triangles are 20 samples randomly drawn from the test domain, and other dots
are training domains. The silhouette coefficients are 0.7, 0.68, 0.48, 0.66, 0.46, and 0.36 for Figs. 4a to 4f, respectively.
Cluster 0L100
 L38
 L43
 L46
Cluster 1
 Cluster 2
(a) TerraInc-ND (L43)
Cluster 0Art
 Clipart
 Product
 Real
Cluster 1
 Cluster 2
 (b) Office-ND (Clipart)
Figure 5. Compare domain labels and HMOE-ND clusters
Top-1 routing Lenand expert load balancing Lkl
The joint use of LenandLklleads to better clustering with
greater SC and promotes latent domain discovery. With-
out them, HMOE relies on the inherent soft partitioning of
MoE. H6 outperforms H2 mostly, which could indicate that
better clustering benefits DG performance. However, H1
and H5 perform similarly, probably due to the absence of
Lad. We find that Lenwithout Lklsuffers from the learning
collapse problem, i.e., some embedding vectors collapse to-
gether, leading to a drop in accuracy. An example is shown
in Fig. 6c. This demonstrates the importance of Lkl.
Class-adversarial training Ladboosts accuracy in most
cases, verifying the necessity of filtering out class-specific
information from the D2V encoder. H2 and H6 have smaller
SC than H1 and H5, respectively, which is reasonable since
class information can still be used by H1 and H5 for clus-
tering, but is somewhat diminished for H2 and H6 via Lad.
4.6. More Empirical Analysis
Effect of Kon latent domain discovery In Fig. 6, we
try different numbers of embedding vectors K. ForK= 2,
cartoon is merged into sketch andphoto . ForK= 5,sketch
andcartoon are split into two sub-clusters. However, when
Kincreases to 8 and is much more than necessary, HMOE
has difficulty in assigning data to different experts correctly
and suffers from the learning collapse problem.
Use Swin Transformer as featurizer [43] investigated
the impact of the backbone network ( i.e., the featurizer
for HMOE) on DG and found that transformer-based back-
Cartoon
PhotoSketch
Art(a)K= 2
Cartoon
PhotoSketch
Art (b)K= 5
Cartoon
PhotoSketch
Art (c)K= 8
Figure 6. HMOE-ND using different Kfor PACS (Art)
bones outperform CNN-based counterparts. Motivated by
this, we try Swin Transformer [52] (pretrained tiny version
with similar complexity to ResNet-50 and its output size is
768) as featurizer (Tab. 3), which enhances both DeepAll
and HMOE-MU, but the latter still performs much better.
VLCS PACS OfficeHome TerraInc
DeepAll 79.7 86.5 71.9 52.9
HMOE-MU 79.8 88.1 74.6 54.7
Table 3. Use Swin Transformer as featurizer of HMOE
5. Conclusion
This paper presents a novel DG method – HMOE, which is
based on Mixture of Experts, uses hypernetworks to gener-
ate the weights of experts, does not require domain labels,
and enables latent domain discovery. HMOE achieves the
SOTA performance in average accuracy on DomainBed.
However, it remains unclear how to effectively determine
an appropriate number of experts or embedding vectors to
fully explore domain information while avoiding the learn-
ing collapse. A promising solution that we will explore in
future work is to use tree-structured hierarchical MoE to
discover hierarchical domain knowledge, where each level
contains only a number of experts but the number of multi-
level inferred domains grows exponentially.
Finally, HMOE is versatile and scalable, and it should
also be applicable to a wide range of problems beyond the
scope of DG that are troubled by heterogeneous patterns.
8

--- PAGE 9 ---
HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization
Supplementary Material
A. Toy Regression Problem
In the paper, we employ HMOE to address the domain generalization problem in image classification. In fact, HMOE is
equally applicable to other problems troubled by heterogeneous patterns. To demonstrate the versatility of HMOE, we apply
it to a toy regression task, aiming to learn a one-dimensional function defined over three intervals. Through this toy problem,
we can also more intuitively understand the learning dynamics of HMOE, including the evolution of the gating mechanism
and how experts become specialized gradually.
We use the function y= sin(4 πx)to generate 10, 20, and 30 data points uniformly in three intervals: (0,0.5),(1,1.5), and
(2,2.5), respectively, as shown in Fig. 7a. Unequal data points are used to simulate a naturally unbalanced expert load. These
three intervals represent three source domains, and we see if HMOE can generalize well in the regions between intervals.
HMOE uses three embedding vectors of dimension D= 8, which are initialized using the standard normal distribution.
All networks of HMOE are MLPs with 32 hidden units. The featurizer is a three-layer MLP whose input size is 1 and output
size is 32. The encoder is a three-layer whose input size is 1 and output size is D. The classifier is a two-layer MLP whose
input size is 32 and output size is 1. The hypernetwork is a four-layer MLP whose input size is Dand output size is the total
number of learnable parameters ( i.e., weights and biases) of the classifier. All MLPs use the SiLU activation function [33]
except the output layers. In addition, Ly(use MSE as the loss function), Len, andLklare used with λy=λen=λkl= 1,
and HMOE is trained using Adam [40] with learning rate 0.001over20,000epochs.
The evolution of the experts’ outputs and gate values with respect to training epochs is depicted in Fig. 7a. From this,
we can observe that three experts compete with each other and progressively delineate their respective positions. Notably,
HMOE manages to identify three intervals even in the face of imbalanced data. After training, we compare two different
inference modes, as shown in Fig. 7b. They all coincide well with the training points. MIX seems to perform better in
the regions between intervals, while OOD presents an unexpected peak. Overall, HMOE demonstrates an ability to detect
heterogeneous patterns within data.
2.5
0.02.5Experts
epochs=0
E1E2E3
epochs=1000
 epochs=2000
 epochs=20000
0 201Gate valuesE1E2E3
0 2 0 2 0 2
(a) Experts’ outputs and gate values during training
0 22.5
0.02.5
MIX
0 2
OOD (b) Two inference modes after training
Figure 7. A toy regression problem. We generate some data points using the function y= sin(4 πx)in three intervals and fit HMOE with
three embedding vectors to these points. HMOE well identifies three intervals and experts also become specialized.
1

--- PAGE 10 ---
B. Description and visualization of datasets of DomainBed
Dataset Domains # of classes # of samples Image size
+90% +80% -90%
Colored MNIST [2]
 2 70,000 (2, 28, 28)
(degree of correlation between color and label)
0◦15◦30◦45◦60◦75◦
Rotated MNIST [25]
 10 70,000 (1, 28, 28)
Caltech101 LabelMe SUN09 VOC2007
VLCS [19]
 5 10,729 (3, 224, 224)
Art Cartoon Photo Sketch
PACS [44]
 7 9,991 (3, 224, 224)
Art Clipart Product Photo
OfficeHome [82]
 65 15,588 (3, 224, 224)
L100 L38 L43 L46
TerraIncognita [4]
 10 24,788 (3, 224, 224)
(camera trap location)
Table 4. Description and visualization of datasets used in our experiments (Adapted from [28])
C. Detailed domain generalization results
We detail the domain generalization results for each dataset, and we format first, second and worse than DeepAll results.
Algorithm +90% +80% -90% Avg Ranking
w/ Domain Labels
Mixup [90] 72.3 ±0.1 73.1±0.0 10.4±0.1 51.9 4
CORAL [76] 71.3 ±0.3 73.0±0.2 9.9±0.0 51.4 13
VREx [41] 73.1 ±0.3 73.7±0.3 10.0±0.1 52.2 2
Fish [73] 71.3 ±0.1 73.1±0.2 10.2±0.1 51.5 9
ARM [95] 81.7 ±0.5 74.8±1.1 10.3±0.2 55.6 1
MTL [6] 71.6 ±0.3 72.9±0.3 10.2±0.0 51.5 10
GroupDRO [67] 73.0 ±0.1 73.0±0.4 10.2±0.3 52.1 3
MLDG [45] 37.5 ±9.9 56.4±5.2 38.8±8.1 44.2 16
MMD [47] 53.9 ±2.7 51.6±0.8 10.1±0.1 38.5 18
DANN [24] 72.5 ±0.1 72.7±0.2 10.1±0.1 51.8 5
IRM [2] 57.0 ±2.7 57.2±4.9 9.7±0.0 41.3 17
HMOE-DL 71.5 ±0.4 72.9±0.1 10.2±0.0 51.5 11
w/o Domain Labels
SelfReg [39] 71.1 ±0.3 73.0±0.0 10.1±0.2 51.4 14
SagNet [59] 72.2 ±0.0 73.3±0.3 10.0±0.1 51.8 6
RSC [34] 72.1 ±0.3 72.3±0.8 10.1±0.1 51.5 12
DeepAll [81] 71.6 ±0.1 72.7±0.2 10.0±0.1 51.4 15
HMOE-ND 71.8 ±0.1 73.0±0.1 10.5±0.2 51.8 7
HMOE-MU 71.7 ±0.4 73.0±0.3 10.3±0.1 51.7 8
Table 5. Domain generalization results on Colored MNIST
2

--- PAGE 11 ---
Algorithm 0 15 30 45 60 75 Avg Ranking
w/ Domain Labels
Mixup [90] 93.8 ±0.1 98.8±0.1 99.0±0.0 99.1±0.1 98.9±0.0 95.9±0.2 97.6 13
CORAL [76] 95.8 ±0.2 98.5±0.1 99.1±0.0 99.0±0.1 99.1±0.0 96.6±0.1 98.0 2
VREx [41] 95.5 ±0.1 98.3±0.2 98.9±0.1 98.9±0.0 98.9±0.0 96.4±0.1 97.8 7
Fish [73] 95.5 ±0.4 98.7±0.0 99.0±0.0 99.1±0.1 98.9±0.0 96.3±0.3 97.9 6
ARM [95] 95.9 ±0.1 98.8±0.0 98.9±0.1 99.1±0.0 98.9±0.0 96.2±0.1 98.1 1
MTL [6] 95.2 ±0.2 98.6±0.1 99.1±0.0 98.9±0.1 98.8±0.1 96.1±0.1 97.8 8
GroupDRO [67] 94.9 ±0.2 98.6±0.1 98.9±0.0 99.0±0.1 99.0±0.0 96.3±0.1 97.8 9
MLDG [45] 95.3 ±0.1 98.5±0.1 99.0±0.0 99.0±0.0 98.9±0.1 96.1±0.1 97.8 10
MMD [47] 95.8 ±0.3 98.8±0.0 99.0±0.1 98.9±0.0 99.0±0.0 96.2±0.1 98.0 3
DANN [24] 95.9 ±0.1 98.5±0.1 98.6±0.0 98.8±0.0 98.7±0.0 95.6±0.1 97.7 12
IRM [2] 81.9 ±2.4 88.1±4.2 93.2±0.6 91.3±2.8 93.1±0.7 76.0±0.7 87.3 18
HMOE-DL 87.7 ±1.3 93.3±2.2 98.2±0.3 98.6±0.0 98.2±0.2 88.8±1.5 94.1 17
w/o Domain Labels
SelfReg [39] 95.7 ±0.1 98.7±0.0 99.0±0.0 99.2±0.0 99.1±0.0 96.5±0.1 98.0 4
SagNet [59] 95.1 ±0.3 98.8±0.0 99.1±0.0 99.1±0.1 99.0±0.0 96.7±0.1 98.0 5
RSC [34] 94.0 ±0.3 98.3±0.1 99.0±0.0 98.9±0.0 98.9±0.0 95.9±0.1 97.5 15
DeepAll [81] 95.0 ±0.4 98.5±0.2 99.0±0.0 99.1±0.0 98.9±0.0 96.2±0.1 97.8 11
HMOE-ND 94.5 ±0.1 98.5±0.1 98.8±0.0 98.7±0.0 98.7±0.1 95.7±0.3 97.5 16
HMOE-MU 94.6 ±0.3 98.8±0.0 98.9±0.1 98.8±0.0 98.8±0.1 95.6±0.2 97.6 14
Table 6. Domain generalization results on Rotated MNIST
Algorithm Caltech101 LabelMe SUN09 VOC2007 Avg Ranking
w/ Domain Labels
Mixup [90] 98.2 ±0.3 64.8±0.3 74.9±0.2 76.9±1.0 78.7 3
CORAL [76] 97.2 ±0.4 65.8±0.4 74.0±0.3 75.4±0.8 78.1 6
VREx [41] 96.1 ±0.5 64.8±1.2 72.6±0.5 75.5±1.0 77.3 14
Fish [73] 96.8 ±0.5 64.5±0.3 74.9±0.3 76.1±1.0 78.1 7
ARM [95] 97.0 ±0.2 65.9±1.4 73.0±0.1 76.2±1.4 78.0 9
MTL [6] 96.3 ±0.1 64.5±0.3 72.6±0.5 75.6±0.9 77.3 15
GroupDRO [67] 97.1 ±0.3 65.9±0.7 72.4±1.7 75.8±0.4 77.8 10
MLDG [45] 96.9 ±0.6 61.5±0.8 71.7±0.7 76.5±0.2 76.6 17
MMD [47] 96.9 ±0.5 64.2±1.9 71.7±0.9 76.6±1.8 77.4 13
DANN [24] 95.8 ±1.0 65.1±0.7 68.1±2.4 73.5±0.7 75.6 18
IRM [2] 96.8 ±0.3 64.6±1.2 75.2±0.8 76.6±3.4 78.3 5
HMOE-DL 95.5 ±1.4 63.5±0.5 73.8±1.0 75.0±1.5 77.0 16
w/o Domain Labels
SelfReg [39] 97.6 ±0.4 65.2±0.2 75.5±0.2 77.1±0.7 78.9 1
SagNet [59] 96.8 ±0.1 63.0±1.0 72.3±0.2 78.7±1.1 77.7 11
RSC [34] 96.7 ±0.9 64.7±0.7 76.4±0.6 77.4±0.8 78.8 2
DeepAll [81] 95.0 ±0.5 65.4±1.0 72.0±1.2 77.7±0.3 77.5 12
HMOE-ND 96.8 ±0.5 64.7±0.5 75.0±0.1 76.1±1.5 78.1 8
HMOE-MU 97.1 ±0.2 64.6±0.7 74.9±0.4 77.9±0.3 78.6 4
Table 7. Domain generalization results on VLCS
3

--- PAGE 12 ---
Algorithm Art Cartoon Photo Sketch Avg Ranking
w/ Domain Labels
Mixup [90] 88.1 ±0.3 81.7±1.0 98.1±0.1 78.6±1.6 86.6 6
CORAL [76] 87.8 ±0.9 82.7±0.9 98.0±0.1 78.4±1.8 86.7 5
VREx [41] 86.5 ±2.0 79.2±0.9 97.7±0.3 80.6±1.2 86.0 10
Fish [73] 86.0 ±1.8 83.1±0.3 98.1±0.3 80.5±2.3 86.9 4
ARM [95] 86.2 ±1.2 81.5±0.7 97.2±0.3 77.9±1.1 85.7 12
MTL [6] 88.4 ±0.8 80.7±1.2 97.8±0.2 75.2±1.8 85.5 13
GroupDRO [67] 86.3 ±1.9 81.0±0.6 97.8±0.1 74.9±2.0 85.0 15
MLDG [45] 90.7 ±0.3 80.4±0.4 97.9±0.1 79.5±0.8 87.1 2
MMD [47] 87.0 ±0.4 79.6±0.9 97.4±0.3 72.6±1.8 84.2 16
DANN [24] 79.4 ±1.9 74.7±0.9 97.0±1.1 57.1±7.0 77.0 18
IRM [2] 84.8 ±1.8 73.9±1.9 98.6±0.1 71.3±1.0 82.1 17
HMOE-DL 87.5 ±1.4 78.9±1.3 97.6±0.1 77.9±1.3 85.5 14
w/o Domain Labels
SelfReg [39] 86.8 ±2.0 82.3±0.8 97.6±0.2 77.8±0.8 86.1 9
SagNet [59] 85.3 ±2.0 81.8±1.6 97.7±0.3 79.8±0.8 86.2 8
RSC [34] 86.6 ±1.2 82.4±0.4 97.4±0.3 81.6±0.7 87.0 3
DeepAll [81] 86.4 ±1.2 81.7±0.6 97.5±0.3 77.7±1.8 85.8 11
HMOE-ND 87.1 ±0.7 81.7±0.9 97.7±0.1 79.9±1.1 86.6 7
HMOE-MU 89.6 ±0.5 81.2±1.0 98.3±0.2 82.9±1.6 88.0 1
Table 8. Domain generalization results on PACS
Algorithm Art Clipart Product Real Avg Ranking
w/ Domain Labels
Mixup [90] 68.1 ±0.8 55.9±0.8 80.3±0.1 82.0±0.3 71.6 3
CORAL [76] 69.9 ±0.7 56.8±0.1 80.5±0.4 81.7±0.2 72.2 2
VREx [41] 66.4 ±0.8 54.0±0.4 78.2±0.2 80.6±0.2 69.8 5
Fish [73] 64.3 ±0.3 53.0±0.4 78.1±0.1 79.4±0.7 68.7 10
ARM [95] 60.4 ±0.2 52.2±0.6 75.6±0.6 77.9±0.3 66.5 15
MTL [6] 64.3 ±0.7 52.1±1.3 78.5±0.1 78.6±0.1 68.4 12
GroupDRO [67] 63.7 ±0.8 52.9±0.8 77.6±0.2 78.8±0.3 68.3 13
MLDG [45] 64.2 ±0.8 52.7±0.9 78.4±0.8 78.1±0.2 68.3 14
MMD [47] 65.6 ±0.3 53.7±0.5 77.8±0.1 79.4±0.1 69.1 8
DANN [24] 62.0 ±0.9 49.7±1.8 76.1±0.5 78.2±0.4 66.5 16
IRM [2] 60.4 ±0.4 49.6±1.0 73.2±0.8 76.2±0.5 64.9 18
HMOE-DL 64.8 ±0.7 53.0±1.4 78.6±0.3 79.0±0.3 68.9 9
w/o Domain Labels
SelfReg [39] 68.0 ±0.4 55.7±0.4 79.7±0.2 81.9±0.6 71.3 4
SagNet [59] 63.7 ±0.9 54.6±0.2 78.2±0.2 80.7±0.4 69.3 7
RSC [34] 60.7 ±1.4 51.4±0.3 74.8±1.1 75.1±1.3 65.5 17
DeepAll [81] 64.7 ±0.6 52.2±1.0 77.4±0.2 79.8±0.2 68.5 11
HMOE-ND 65.6 ±0.1 54.7±0.6 78.8±0.2 79.9±0.3 69.7 6
HMOE-MU 68.7 ±0.6 57.7±0.4 81.0±0.2 82.6±0.4 72.5 1
Table 9. Domain generalization results on OfficeHome
4

--- PAGE 13 ---
Algorithm L100 L38 L43 L46 Avg Ranking
w/ Domain Labels
Mixup [90] 68.3 ±2.0 43.9±0.4 56.9±1.5 36.6±0.5 51.4 5
CORAL [76] 52.9 ±3.7 46.8±1.4 59.5±0.4 36.3±0.9 48.9 15
VREx [41] 60.7 ±1.7 44.8±1.2 58.9±1.4 42.6±1.3 51.8 3
Fish [73] 55.7 ±2.2 46.9±2.5 59.9±0.4 41.3±2.1 51.0 7
ARM [95] 56.0 ±3.1 44.3±1.4 54.9±0.3 38.6±0.6 48.5 16
MTL [6] 55.1 ±0.8 51.3±2.3 57.8±0.8 41.2±2.1 51.3 6
GroupDRO [67] 51.9 ±2.9 45.4±1.8 60.8±0.7 40.2±0.3 49.6 12
MLDG [45] 57.6 ±3.3 46.2±1.2 58.4±0.7 37.5±0.8 49.9 11
MMD [47] 61.0 ±2.7 43.2±0.6 57.5±1.5 38.3±2.2 50.0 10
DANN [24] 48.8 ±1.1 38.1±3.9 44.1±4.4 38.9±2.4 42.5 18
IRM [2] 49.4 ±4.3 47.6±2.4 58.4±1.6 47.8±1.5 50.8 8
HMOE-DL 56.1 ±1.9 48.1±1.2 57.7±0.8 36.5±1.3 49.6 13
w/o Domain Labels
SelfReg [39] 59.0 ±2.4 46.0±1.1 59.6±1.7 41.5±1.1 51.5 4
SagNet [59] 59.6 ±1.3 46.3±1.1 59.8±0.7 37.2±1.6 50.7 9
RSC [34] 51.7 ±6.4 46.4±0.7 59.1±0.9 39.2±1.1 49.1 14
DeepAll [81] 50.0 ±3.4 42.3±1.6 58.5±1.0 39.9±2.3 47.7 17
HMOE-ND 60.7 ±3.6 53.2±1.5 56.7±1.2 39.6±0.3 52.5 2
HMOE-MU 67.3 ±1.0 43.4±1.4 57.4±0.7 43.0±2.8 52.8 1
Table 10. Domain generalization results on TerraInc
5

--- PAGE 14 ---
References
[1] Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet, Yoshua Bengio, Ioannis Mitliagkas, and Irina
Rish. Invariance principle meets information bottleneck for out-of-distribution generalization. Advances in Neural Information
Processing Systems , 34:3438–3450, 2021. 2
[2] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint
arXiv:1907.02893 , 2019. 2, 5, 7, 3, 4
[3] Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain generalization using meta-regularization.
Advances in neural information processing systems , 31, 2018. 2
[4] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European Conference on
Computer Vision (ECCV) , pages 456–473, 2018. 5, 2
[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions
on pattern analysis and machine intelligence , 35(8):1798–1828, 2013. 7
[6] Gilles Blanchard, Aniket Anand Deshmukh, Ürun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal
transfer learning. The Journal of Machine Learning Research , 22(1):46–100, 2021. 1, 2, 7, 3, 4, 5
[7] Dhanajit Brahma, Vinay Kumar Verma, and Piyush Rai. Hypernetworks for Continual Semi-Supervised Learning. arXiv preprint
arXiv:2110.01856 , 2021. 2
[8] Fabio M. Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving
jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2229–2238, 2019.
2
[9] Oscar Chang, Lampros Flokas, and Hod Lipson. Principled weight initialization for hypernetworks. In International Conference on
Learning Representations , 2019. 6
[10] Olivier Chapelle, Jason Weston, Léon Bottou, and Vladimir Vapnik. Vicinal risk minimization. Advances in neural information
processing systems , 13, 2000. 5
[11] Chaoqi Chen, Jiongcheng Li, Xiaoguang Han, Xiaoqing Liu, and Yizhou Yu. Compound Domain Generalization via Meta-
Knowledge Encoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7119–7129,
2022. 1, 2
[12] Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei. StableMoE: Stable routing strategy for
mixture of experts. arXiv preprint arXiv:2204.08396 , 2022. 3
[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In
2009 IEEE Conference on Computer Vision and Pattern Recognition , pages 248–255. Ieee, 2009. 6
[14] Aniket Anand Deshmukh, Yunwen Lei, Srinagesh Sharma, Urun Dogan, James W. Cutler, and Clayton Scott. A generalization error
bound for multi-class domain generalization. arXiv preprint arXiv:1905.10392 , 2019. 1
[15] Antonio D’Innocente and Barbara Caputo. Domain generalization with domain-specific aggregation modules. In German Confer-
ence on Pattern Recognition , pages 187–198. Springer, 2018. 3
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,
Matthias Minderer, Georg Heigold, and Sylvain Gelly. An image is worth 16x16 words: Transformers for image recognition at scale.
arXiv preprint arXiv:2010.11929 , 2020. 3
[17] Qi Dou, Daniel Coelho de Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model-agnostic learning
of semantic features. Advances in Neural Information Processing Systems , 32, 2019. 2
[18] Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei
Yu, and Orhan Firat. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine
Learning , pages 5547–5569. PMLR, 2022. 2
[19] Chen Fang, Ye Xu, and Daniel N. Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for
softening bias. In Proceedings of the IEEE International Conference on Computer Vision , pages 1657–1664, 2013. 5, 2
[20] William Fedus, Barret Zoph, and Noam Shazeer. Switch Transformers: Scaling to Trillion Parameter Models with Simple and
Efficient Sparsity . 2021. 2
[21] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667 ,
2022. 2
[22] Chuang Gan, Tianbao Yang, and Boqing Gong. Learning attributes equals multi-source domain generalization. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition , pages 87–97, 2016. 2
[23] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International Conference on
Machine Learning , pages 1180–1189. PMLR, 2015. 2
[24] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and
Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research , 17(1):2096–2030,
2016. 2, 4, 7, 3, 5
6

--- PAGE 15 ---
[25] Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with
multi-task autoencoders. In Proceedings of the IEEE International Conference on Computer Vision , pages 2551–2559, 2015. 2, 5
[26] Muhammad Ghifary, David Balduzzi, W. Bastiaan Kleijn, and Mengjie Zhang. Scatter component analysis: A unified framework for
domain adaptation and domain generalization. IEEE transactions on pattern analysis and machine intelligence , 39(7):1414–1430,
2016. 2
[27] Rui Gong, Wen Li, Yuhua Chen, and Luc Van Gool. Dlow: Domain flow for adaptation and generalization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 2477–2486, 2019. 2
[28] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. arXiv preprint arXiv:2007.01434 , 2020. 1, 2, 3, 5,
6
[29] Jiang Guo, Darsh J. Shah, and Regina Barzilay. Multi-source domain adaptation with mixture of experts. arXiv preprint
arXiv:1809.02256 , 2018. 1, 3
[30] David Ha, Andrew Dai, and Quoc V . Le. Hypernetworks. arXiv preprint arXiv:1609.09106 , 2016. 1, 2
[31] Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder, Lichan Hong,
and Ed Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning. Advances in
Neural Information Processing Systems , 34:29335–29347, 2021. 4
[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016. 3
[33] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016. 6, 1
[34] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In European
Conference on Computer Vision , pages 124–140. Springer, 2020. 2, 7, 3, 4, 5
[35] Maximilian Ilse, Jakub M Tomczak, Christos Louizos, and Max Welling. Diva: Domain invariant variational autoencoders. In
Medical Imaging with Deep Learning , pages 322–348. PMLR, 2020. 2
[36] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural
computation , 3(1):79–87, 1991. 1, 2
[37] Michael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural computation , 6(2):181–214,
1994. 1, 2
[38] Aditya Khosla, Tinghui Zhou, Tomasz Malisiewicz, Alexei A. Efros, and Antonio Torralba. Undoing the damage of dataset bias. In
European Conference on Computer Vision , pages 158–171. Springer, 2012. 2
[39] Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervised contrastive regularization for
domain generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 9619–9628, 2021. 2,
6, 7, 3, 4, 5
[40] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014. 1
[41] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron
Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning , pages
5815–5826. PMLR, 2021. 2, 7, 3, 4, 5
[42] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer,
and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint
arXiv:2006.16668 , 2020. 2
[43] Bo Li, Jingkang Yang, Jiawei Ren, Yezhen Wang, and Ziwei Liu. Sparse Fusion Mixture-of-Experts are Domain Generalizable
Learners. arXiv preprint arXiv:2206.04046 , 2022. 3, 8
[44] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain generalization. In Proceedings
of the IEEE International Conference on Computer Vision , pages 5542–5550, 2017. 5, 2
[45] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization. In
Proceedings of the AAAI Conference on Artificial Intelligence , 2018. 2, 7, 3, 4, 5
[46] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, and Timothy M. Hospedales. Episodic training for domain general-
ization. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1446–1455, 2019. 2
[47] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition , pages 5400–5409, 2018. 2, 7, 3, 4, 5
[48] Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M. Hospedales. A simple feature augmentation for domain
generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 8886–8895, 2021. 2
[49] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization
via conditional invariant adversarial networks. In Proceedings of the European Conference on Computer Vision (ECCV) , pages
624–639, 2018. 2
[50] Xi Lin, Zhiyuan Yang, Qingfu Zhang, and Sam Kwong. Controllable pareto multi-task learning. arXiv preprint arXiv:2010.06313 ,
2020. 2
[51] Alexander H. Liu, Yen-Cheng Liu, Yu-Ying Yeh, and Yu-Chiang Frank Wang. A unified feature disentangler for multi-domain
image translation and manipulation. Advances in neural information processing systems , 31, 2018. 2
7

--- PAGE 16 ---
[52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical
vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages
10012–10022, 2021. 8
[53] Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through hypernetworks. arXiv preprint
arXiv:1802.09419 , 2018. 2
[54] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficient multi-task fine-tuning
for transformers via shared hypernetworks. arXiv preprint arXiv:2106.04489 , 2021. 2
[55] Massimiliano Mancini, Samuel Rota Bulo, Barbara Caputo, and Elisa Ricci. Best sources forward: Domain generalization through
source-specific nets. In 2018 25th IEEE International Conference on Image Processing (ICIP) , pages 1353–1357. IEEE, 2018. 3
[56] Toshihiko Matsuura and Tatsuya Harada. Domain generalization using a mixture of multiple latent domains. In Proceedings of the
AAAI Conference on Artificial Intelligence , pages 11749–11756, 2020. 2
[57] Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gianfranco Doretto. Unified deep supervised domain adaptation and
generalization. In Proceedings of the IEEE International Conference on Computer Vision , pages 5715–5725, 2017. 2
[58] Krikamol Muandet, David Balduzzi, and Bernhard Schölkopf. Domain generalization via invariant feature representation. In
International Conference on Machine Learning , pages 10–18. PMLR, 2013. 1, 2
[59] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domain gap by reducing style bias.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8690–8699, 2021. 2, 7, 3, 4, 5
[60] Aviv Navon, Aviv Shamsian, Gal Chechik, and Ethan Fetaya. Learning the pareto front with hypernetworks. arXiv preprint
arXiv:2010.04104 , 2020. 2
[61] Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE
transactions on neural networks , 22(2):199–210, 2010. 2
[62] Svetlana Pavlitskaya, Christian Hubschneider, Lukas Struppek, and J. Marius Zöllner. Balancing Expert Utilization in Mixture-of-
Experts Layers Embedded in CNNs. arXiv preprint arXiv:2204.10598 , 2022. 4
[63] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain
adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 1406–1415, 2019. 2
[64] Xingchao Peng, Zijun Huang, Ximeng Sun, and Kate Saenko. Domain agnostic learning with disentangled representations. In
International Conference on Machine Learning , pages 5102–5112. PMLR, 2019. 2
[65] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 12556–12565, 2020. 2
[66] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution generalization.
InInternational Conference on Machine Learning , pages 18347–18377. PMLR, 2022. 2
[67] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally Robust Neural Networks for Group
Shifts: On the Importance of Regularization for Worst-Case Generalization, 2020. 2, 7, 3, 4, 5
[68] Marcin Sendera, Marcin Przewi˛ e´ zlikowski, Konrad Karanowski, Maciej Zi˛ eba, Jacek Tabor, and Przemysław Spurek. Hypershot:
Few-shot learning by kernel hypernetworks. arXiv preprint arXiv:2203.11378 , 2022. 2
[69] Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning using hypernetworks. In International
Conference on Machine Learning , pages 9489–9502. PMLR, 2021. 2
[70] Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita Sarawagi. Generalizing across
domains via cross-gradient training. arXiv preprint arXiv:1804.10745 , 2018. 2
[71] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large
neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538 , 2017. 2
[72] Yuge Shi, Jeffrey Seely, Philip HS Torr, N. Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching
for domain generalization. arXiv preprint arXiv:2104.09937 , 2021. 2
[73] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for
domain generalization. arXiv preprint arXiv:2104.09937 , 2021. 7, 2, 3, 4, 5
[74] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint
arXiv:1409.1556 , 2014. 3
[75] Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese. Which tasks should be learned
together in multi-task learning? In International Conference on Machine Learning , pages 9120–9132. PMLR, 2020. 1
[76] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European Conference on
Computer Vision , pages 443–450. Springer, 2016. 2, 7, 3, 4, 5
[77] Yi Tay, Zhe Zhao, Dara Bahri, Don Metzler, and Da-Cheng Juan. Hypergrid transformers: Towards a single model for multiple
tasks. 2021. 2
[78] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain
invariance. arXiv preprint arXiv:1412.3474 , 2014. 2
[79] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv
preprint arXiv:1607.08022 , 2016. 6
8

--- PAGE 17 ---
[80] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine learning research , 9(11), 2008. 7
[81] Vladimir Vapnik. The Nature of Statistical Learning Theory . Springer science & business media, 1999. 5, 7, 2, 3, 4
[82] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised
domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 5018–5027, 2017.
5, 2
[83] Tomer V olk, Eyal Ben-David, Ohad Amosy, Gal Chechik, and Roi Reichart. Example-based hypernetworks for out-of-distribution
generalization. arXiv preprint arXiv:2203.14276 , 2022. 3
[84] Riccardo V olpi, Hongseok Namkoong, Ozan Sener, John C. Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen
domains via adversarial data augmentation. Advances in neural information processing systems , 31, 2018. 2
[85] Johannes V on Oswald, Christian Henning, João Sacramento, and Benjamin F. Grewe. Continual learning with hypernetworks. arXiv
preprint arXiv:1906.00695 , 2019. 2
[86] Jindong Wang, Wenjie Feng, Yiqiang Chen, Han Yu, Meiyu Huang, and Philip S. Yu. Visual domain adaptation with manifold
embedded distribution alignment. In Proceedings of the 26th ACM International Conference on Multimedia , pages 402–410, 2018.
2
[87] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Gener-
alizing to unseen domains: A survey on domain generalization. IEEE Transactions on Knowledge and Data Engineering , 2022. 1,
3
[88] Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing , 312:135–153, 2018. 1, 3
[89] Minghao Xu, Jian Zhang, Bingbing Ni, Teng Li, Chengjie Wang, Qi Tian, and Wenjun Zhang. Adversarial domain adaptation with
domain mixup. In Proceedings of the AAAI conference on artificial intelligence , pages 6502–6509, 2020. 5
[90] Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain adaptation with mixup training. arXiv
preprint arXiv:2001.00677 , 2020. 5, 6, 7, 2, 3, 4
[91] Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, and Boqing Gong. Domain randomization
and pyramid consistency: Simulation-to-real generalization without accessing target domain data. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 2100–2110, 2019. 2
[92] Seniha Esen Yuksel, Joseph N. Wilson, and Paul D. Gader. Twenty years of mixture of experts. IEEE transactions on neural
networks and learning systems , 23(8):1177–1193, 2012. 1
[93] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. Mixup: Beyond empirical risk minimization. arXiv
preprint arXiv:1710.09412 , 2017. 2, 5
[94] Hanlin Zhang, Yi-Fan Zhang, Weiyang Liu, Adrian Weller, Bernhard Schölkopf, and Eric P Xing. Towards principled disentangle-
ment for domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages
8024–8034, 2022. 2
[95] Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk minimization:
Learning to adapt to domain shift. Advances in Neural Information Processing Systems , 34:23664–23678, 2021. 6, 7, 2, 3, 4, 5
[96] Dominic Zhao, Johannes von Oswald, Seijin Kobayashi, João Sacramento, and Benjamin F. Grewe. Meta-learning via hypernet-
works. 2020. 2
[97] Tao Zhong, Zhixiang Chi, Li Gu, Yang Wang, Yuanhao Yu, and Jin Tang. Meta-DMoE: Adapting to Domain Shift by Meta-
Distillation from Mixture-of-Experts. arXiv preprint arXiv:2210.03885 , 2022. 1, 3
[98] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domains for domain generalization.
InEuropean Conference on Computer Vision , pages 561–578. Springer, 2020. 2
[99] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain adaptive ensemble learning. IEEE Transactions on Image Pro-
cessing , 30:8008–8018, 2021. 3
[100] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. arXiv preprint arXiv:2104.02008 ,
2021. 2
[101] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. IEEE Transactions on
Pattern Analysis and Machine Intelligence , 2022. 1, 3
[102] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. Designing
effective sparse expert models. arXiv preprint arXiv:2202.08906 , 2022. 2
9
