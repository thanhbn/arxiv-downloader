# Học cách Định tuyến giữa các Chuyên gia Chuyên biệt để Tổng quát hóa Zero-Shot
Mohammed Muqeeth¹ Haokun Liu² ³ Yufan Liu⁴ Colin Raffel² ³

Tóm tắt
Gần đây, đã có sự gia tăng rộng rãi của các mô hình ngôn ngữ "chuyên gia" được chuyên biệt hóa cho một nhiệm vụ hoặc lĩnh vực cụ thể thông qua việc tinh chỉnh hiệu quả tham số. Làm thế nào chúng ta có thể tái sử dụng các bộ sưu tập lớn các mô hình ngôn ngữ chuyên gia để cải thiện khả năng tổng quát hóa zero-shot cho các nhiệm vụ chưa thấy? Trong công trình này, chúng tôi đề xuất Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), phương pháp học cách định tuyến giữa các mô-đun chuyên biệt được tạo ra thông qua việc tinh chỉnh hiệu quả tham số. Khác với các phương pháp trước đây học cách định tuyến giữa các mô hình chuyên biệt, PHATGOOSE khám phá khả năng rằng tổng quát hóa zero-shot sẽ được cải thiện nếu các chuyên gia khác nhau có thể được chọn một cách thích ứng cho từng token và tại từng lớp trong mô hình. Quan trọng là, phương pháp của chúng tôi là post-hoc - nó không yêu cầu truy cập đồng thời vào các bộ dữ liệu được sử dụng để tạo ra các mô hình chuyên biệt và chỉ yêu cầu một lượng tính toán bổ sung khiêm tốn sau khi mỗi mô hình chuyên gia được huấn luyện. Trong các thí nghiệm bao gồm một loạt các bộ sưu tập mô hình chuyên biệt và các benchmark tổng quát hóa zero-shot, chúng tôi thấy rằng PHATGOOSE vượt trội so với các phương pháp định tuyến post-hoc trong quá khứ và, trong một số trường hợp, vượt trội so với việc huấn luyện đa nhiệm vụ rõ ràng (đòi hỏi truy cập dữ liệu đồng thời). Để hiểu rõ hơn về chiến lược định tuyến được học bởi PHATGOOSE, chúng tôi thực hiện các thí nghiệm định tính để xác nhận rằng hiệu suất của PHATGOOSE bắt nguồn từ khả năng thực hiện định tuyến theo token và theo mô-đun. Chúng tôi phát hành toàn bộ mã nguồn để hỗ trợ các nghiên cứu tương lai về cải thiện tổng quát hóa zero-shot bằng cách tái sử dụng các chuyên gia chuyên biệt.

¹MIT-IBM ²University of Toronto ³Vector Institute ⁴University of North Carolina at Chapel Hill. Liên hệ: Mohammed Muqeeth <muqeeth101@gmail.com>, Haokun Liu <haokun-liu412@gmail.com>, Colin Raffel <craffel@gmail.com>.

Hình 1. Hiệu suất trung bình của các phương pháp huấn luyện đa nhiệm vụ và định tuyến chuyên gia khác nhau khi sử dụng cùng các nhiệm vụ held-in và held-out như T0 (Sanh et al., 2021). Đáng chú ý, phương pháp đề xuất của chúng tôi PHATGOOSE vượt trội so với tất cả các phương pháp tái sử dụng chuyên gia trong quá khứ cũng như việc huấn luyện đa nhiệm vụ rõ ràng (đòi hỏi truy cập dữ liệu đồng thời) và gần như khớp với hiệu suất của một sơ đồ định tuyến oracle. Xem Mục 4 để biết thêm chi tiết. Kết quả số chính xác cho tất cả các phương pháp có thể được tìm thấy trong Bảng 3.

1 Giới thiệu

Sự có sẵn của các mô hình ngôn ngữ được huấn luyện trước có hiệu suất tốt đã dẫn đến sự gia tăng các mô hình "chuyên gia" được tinh chỉnh chuyên biệt cho một nhiệm vụ hoặc lĩnh vực cụ thể. Nhiều mô hình chuyên gia này được tạo ra thông qua các kỹ thuật tinh chỉnh hiệu quả tham số (PEFT) (Ding et al., 2022; Lialin et al., 2023; He et al., 2021), tạo ra một mô hình được tinh chỉnh bằng cách thêm các "mô-đun" nhỏ (như Low-Rank Adapters (Hu et al., 2021) hoặc (IA)³ vectors (Liu et al., 2022)) chỉ giới thiệu hoặc sửa đổi một số lượng nhỏ tham số. Các mô-đun PEFT chuyên biệt có thể được chia sẻ dễ dàng do kích thước nhỏ, điều này đã dẫn đến việc phân phối ngày càng nhiều adapters trên các nền tảng khác nhau - ví dụ, tại thời điểm viết bài, hơn 17.000 adapters dựa trên thư viện peft (Mangrulkar et al., 2022) đã được tải lên Hugging Face Model Hub. Sự có sẵn của các mô-đun PEFT này làm cho việc điều chỉnh một mô hình được huấn luyện trước nhất định cho một nhiệm vụ hoặc lĩnh vực cụ thể trở nên rẻ và dễ dàng.

Trong khi đó, các mô hình ngôn ngữ lớn cực kỳ lớn (LLMs) hiện đang được coi là hệ thống AI "đa năng" có thể thực hiện bất kỳ nhiệm vụ nào mà không cần bất kỳ huấn luyện hoặc thích ứng cụ thể cho nhiệm vụ nào. Cách tiếp cận này bắt nguồn từ quan sát rằng LLMs thường thể hiện khả năng tổng quát hóa zero-shot mạnh mẽ, tức là khả năng thực hiện các nhiệm vụ mới mà chúng không được huấn luyện rõ ràng. Khả năng tổng quát hóa zero-shot như vậy thường được cải thiện thông qua việc tinh chỉnh đa nhiệm vụ quy mô lớn (còn gọi là "instruction tuning") (Sanh et al., 2021; Wei et al., 2021; Mishra et al., 2022). Việc dựa vào tổng quát hóa zero-shot đứng trong sự tương phản rõ rệt với cách tiếp cận huấn luyện các mô hình chuyên biệt cho từng nhiệm vụ (thông qua PEFT hoặc cách khác) đã đề cập.

Sự hấp dẫn của các mô hình ngôn ngữ đa năng và sự gia tăng của các mô hình chuyên biệt dựa trên PEFT đặt ra một câu hỏi tự nhiên: Liệu chúng ta có thể tận dụng một bộ sưu tập lớn các mô-đun chuyên biệt để cải thiện khả năng tổng quát hóa zero-shot của một mô hình ngôn ngữ cơ sở? Cách tiếp cận như vậy hấp dẫn vì nhiều lý do: Đầu tiên, nó sẽ cung cấp một con đường để phát triển phi tập trung các mô hình ngôn ngữ tổng quát, điều mà nếu không sẽ đòi hỏi một lượng lớn tính toán tập trung (Kaplan et al., 2020; Hoffmann et al., 2022). Ngoài ra, nó sẽ cung cấp một cách để tái sử dụng nỗ lực và tính toán rộng rãi đã được chi tiêu để tạo ra các mô hình chuyên biệt. Chúng ta có thể hy vọng rằng cách tiếp cận như vậy có thể thành công dựa trên bằng chứng rộng rãi rằng huấn luyện đa nhiệm vụ cải thiện tổng quát hóa zero-shot (Sanh et al., 2021; Wei et al., 2021; Mishra et al., 2022), và việc kết hợp các mô hình chuyên biệt có thể được xem là một hình thức học đa nhiệm vụ không yêu cầu truy cập dữ liệu đồng thời.

Để giải quyết vấn đề này, hầu hết các công trình trước đây (Jang et al., 2023; Belofsky, 2023; Durbin, 2024; Maxine, 2023) học một chiến lược định tuyến post-hoc bằng cách so sánh embedding của truy vấn đầu vào với embedding trung bình của các ví dụ trong mỗi bộ dữ liệu được sử dụng để huấn luyện mỗi chuyên gia. Các phương pháp như vậy ngầm giả định rằng có một chuyên gia duy nhất phù hợp cho truy vấn và hy vọng rằng thuật toán truy xuất có thể xác định chính xác chuyên gia tốt nhất này. Tuy nhiên, Jang et al. (2023) đã chỉ ra rằng các cách tiếp cận như vậy thua kém so với một router "oracle" luôn chọn chuyên gia tốt nhất cho một truy vấn nhất định. Để khám phá các cách tiếp cận định tuyến thay thế, chúng tôi lưu ý rằng nhiều phương pháp PEFT thường chèn các mô-đun có thể huấn luyện nhỏ ở nhiều nơi trong mô hình (ví dụ tại mỗi ma trận trọng số (Hu et al., 2021)). Trong khi đó, nhiều mô hình Mixture-of-Experts có cổng thưa thớt đưa ra quyết định định tuyến riêng biệt cho từng token (Shazeer et al., 2016; Fedus et al., 2022; Du et al., 2022). Trong công trình này, do đó chúng tôi khám phá góc độ cải thiện tổng quát hóa zero-shot thông qua định tuyến thích ứng theo token và theo mô-đun. Bằng cách làm như vậy, mô hình tổng hợp có thể có khả năng tổng quát hóa tốt hơn cho các nhiệm vụ mới bằng cách sử dụng các khả năng chuyên gia khác nhau ở các giai đoạn khác nhau và/hoặc cho các token khác nhau. Ngoài ra, hiệu suất zero-shot sẽ không bị hạn chế bởi mô hình chuyên biệt tốt nhất duy nhất và khả năng truy xuất chính xác nó.

Dựa trên lập luận này, chúng tôi giới thiệu Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), một phương pháp post-hoc cho phép tổng quát hóa zero-shot giữa các mô hình chuyên biệt. PHATGOOSE tái sử dụng các mô-đun PEFT bằng cách giới thiệu một bước tính toán bổ sung không tốn kém sau khi huấn luyện mô hình dựa trên PEFT. Cụ thể, toàn bộ mô hình (bao gồm các mô-đun PEFT mới được giới thiệu) được đóng băng và một cổng theo mô-đun được huấn luyện. Cổng này (có tham số được chia sẻ qua các vị trí chuỗi) bao gồm một lớp tuyến tính theo sau là một hàm phi tuyến sigmoid xác định liệu kích hoạt tại một vị trí chuỗi nhất định có nên được đưa vào mô-đun hay không. Việc huấn luyện cổng này chỉ yêu cầu một lượng tính toán bổ sung nhỏ so với việc thực hiện PEFT. Các cổng cho mọi mô-đun qua các mô hình chuyên biệt sau đó được kết hợp để xác định cách định tuyến các token khác nhau đến các mô-đun khác nhau trong quá trình suy luận sử dụng chiến lược định tuyến "top-k" tiêu chuẩn (Shazeer et al., 2016; Lepikhin et al., 2020; Du et al., 2022).

Để kiểm tra hiệu quả của PHATGOOSE, chúng tôi đã điều chỉnh các mô hình họ T5 để cải thiện tổng quát hóa zero-shot trên các benchmark tiêu chuẩn. Đáng chú ý, chúng tôi thấy rằng PHATGOOSE không chỉ vượt trội so với các phương pháp trước đây liên quan đến việc hợp nhất các chuyên gia hoặc truy xuất một chuyên gia duy nhất mà còn có thể vượt trội so với huấn luyện đa nhiệm vụ rõ ràng trong một số trường hợp. Trong phân tích định tính, chúng tôi thấy rằng PHATGOOSE sử dụng một tập hợp đa dạng các mô-đun để thực hiện một nhiệm vụ nhất định, do đó kết hợp khả năng từ nhiều mô hình chuyên biệt và, trong một số trường hợp, tạo ra hiệu suất tốt hơn so với mô hình chuyên gia có hiệu suất tốt nhất duy nhất. Nhìn chung, công trình của chúng tôi đặt nền móng cho một khung làm việc mới đầy hứa hẹn cho việc phát triển phi tập trung các hệ thống AI tổng quát.

2 Phát triển phi tập trung các mô hình zero-shot

Mục tiêu của chúng tôi trong công trình này là cho phép các đóng góp viên cá nhân cải thiện khả năng tổng quát hóa zero-shot của mô hình một cách tập thể bằng cách chia sẻ các mô-đun PEFT chuyên biệt. Cụ thể, chúng tôi xác định khung bài toán như sau:

1. Chúng tôi giả định rằng các đóng góp viên cá nhân lấy một mô hình cơ sở và thực hiện PEFT trên nhiệm vụ cụ thể mà họ quan tâm. Vì PEFT thường có chi phí tính toán và truyền thông thấp hơn so với việc tinh chỉnh mô hình đầy đủ, việc sử dụng PEFT làm cho việc tham gia và đóng góp dễ dàng hơn.

2. Chúng tôi giả định một phương pháp PEFT giới thiệu các mô-đun khắp mô hình - ví dụ, như được thảo luận thêm trong Mục 3, LoRA (Hu et al., 2021) giới thiệu một cập nhật thứ hạng thấp tại mỗi lớp tuyến tính trong mô hình. Chúng tôi gọi mỗi cập nhật này là một "mô-đun".

3. Chúng tôi muốn tránh đặt các ràng buộc lên các đóng góp viên hoặc yêu cầu họ thực hiện một lượng công việc bổ sung lớn ngoài việc huấn luyện mô hình dựa trên PEFT.

4. Theo thông lệ tiêu chuẩn, các đóng góp viên chỉ chia sẻ các tham số được huấn luyện (ví dụ, các mô-đun PEFT), không phải bộ dữ liệu được sử dụng để tinh chỉnh. Do đó, chúng tôi không cho phép truy cập đồng thời vào các bộ dữ liệu bất kỳ lúc nào, và tất cả việc huấn luyện trên một bộ dữ liệu cụ thể phải được thực hiện bởi một đóng góp viên duy nhất.

5. Chúng tôi nhằm sử dụng bộ sưu tập các mô-đun PEFT để cải thiện hiệu suất zero-shot trên các nhiệm vụ chưa thấy, tức là trên các nhiệm vụ không có mô hình dựa trên PEFT chuyên biệt cũng như không có bộ dữ liệu huấn luyện. Điều này phản ánh tiêu chuẩn đánh giá hiện tại và trường hợp sử dụng chủ đạo của LLMs.

6. Chúng tôi không nhằm cải thiện hiệu suất trên các nhiệm vụ held-in (tức là những nhiệm vụ mà chúng tôi có mô hình dựa trên PEFT chuyên biệt) vì chúng tôi luôn có thể duy trì hiệu suất trên một nhiệm vụ held-in nhất định bằng cách sử dụng chuyên gia chuyên biệt cho nhiệm vụ đó.

Khung bài toán này đặt ra nhiều thách thức. Đầu tiên, trong khi các chuyên gia được huấn luyện độc lập, chúng ta phải xác định một cách để làm cho chúng hoạt động cùng nhau để cải thiện hiệu suất trên các nhiệm vụ chưa thấy. Thứ hai, chúng ta nhằm chi tiêu ít tính toán bổ sung nhất có thể, điều này loại trừ các phương pháp liên quan đến việc huấn luyện đáng kể sau khi các mô-đun chuyên gia được tạo ra. Cuối cùng, trong đánh giá zero-shot, mô hình cần xác định định tuyến chỉ từ thông tin trong một ví dụ đầu vào duy nhất. Yêu cầu cuối cùng này khác với các công trình trước đây giả định truy cập vào bộ dữ liệu huấn luyện nhiệm vụ đích để chuyển giao kiến thức từ bộ sưu tập các mô hình chuyên biệt (Huang et al., 2023; Wu et al., 2023; Pfeiffer et al., 2020; Caccia et al., 2023; Shnitzer et al., 2023), mà chúng tôi không so sánh trực tiếp.

Theo hiểu biết tốt nhất của chúng tôi, hầu hết các phương pháp được đề xuất trước đây áp dụng cho khung bài toán của chúng tôi nhằm chọn một mô hình chuyên biệt duy nhất dựa trên các thuộc tính của truy vấn đầu vào. Ngoại lệ duy nhất mà chúng tôi biết là phương pháp Arrow đồng thời của Ostapenko et al. (2024), xây dựng router sử dụng thống kê của chính các tham số chuyên gia. Chúng tôi so sánh với Arrow trong các thí nghiệm của mình trong Mục 4. Trong số các phương pháp định tuyến dựa trên đầu vào, một lớp phương pháp (ví dụ (Jang et al., 2023; Belofsky, 2023; Durbin, 2024; Maxine, 2023)) định tuyến đến một mô hình chuyên gia duy nhất bằng cách so sánh embedding của truy vấn đầu vào (Reimers & Gurevych, 2019) với embedding trung bình của các điểm dữ liệu được sử dụng để huấn luyện mỗi chuyên gia. Chúng tôi coi lớp phương pháp này là baseline chính để so sánh. Một lớp phương pháp bổ sung (ví dụ (Durbin, 2024; Liu, 2024)) tận dụng một LLM đa năng bên ngoài (ví dụ GPT-4) và chọn mô hình nào để định tuyến bằng cách xây dựng một truy vấn văn bản hỏi mô hình nào nên sử dụng (ví dụ "Tôi có một mô hình tiếng Pháp và một mô hình tiếng Anh. Tôi nên sử dụng mô hình nào cho truy vấn: combien pèse une pomme?"). Vì việc truy vấn mô hình đa năng (có thể phù hợp để xử lý chính truy vấn đó) phát sinh chi phí đáng kể, và vì cách tiếp cận này chưa được định nghĩa hoặc đánh giá nghiêm ngặt bên ngoài bối cảnh các dự án nguồn mở, chúng tôi không đưa nó vào làm baseline. Cuối cùng, Lu et al. (2023) gần đây đã đề xuất "Zooter", định tuyến giữa các mô hình tổng quát bằng cách huấn luyện một bộ phân loại để dự đoán mô hình nào sẽ tạo ra sinh ra thưởng cao nhất theo mô hình thưởng phụ trợ. Trong khi mục tiêu của Zooter liên quan đến khung bài toán của chúng tôi, thực tế là nó liên quan đến việc chưng cất tập trung một mô hình thưởng phù hợp thành một bộ phân loại và tập trung vào các mô hình tổng quát hơn là chuyên biệt đã khiến chúng tôi loại trừ nó làm baseline.

3 Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts

Để tóm tắt lại, mục tiêu của chúng tôi trong công trình này là phát triển một phương pháp có thể áp dụng trong khung bài toán của chúng tôi - tức là, nó tái sử dụng các mô-đun PEFT từ các đóng góp viên cá nhân để cải thiện tổng quát hóa zero-shot mà không yêu cầu công việc bổ sung đáng kể hoặc truy cập đồng thời vào các bộ dữ liệu của các đóng góp viên. Ngoài ra, chúng tôi nhằm phát triển một phương pháp tuân theo giả thuyết rằng học một chiến lược định tuyến theo token và theo mô-đun có thể đạt được tổng quát hóa zero-shot tốt hơn so với chiến lược định tuyến chọn một mô hình chuyên gia duy nhất cho tất cả các token. Phương pháp đề xuất của chúng tôi, Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), đạt được những mục tiêu này bằng cách yêu cầu các đóng góp viên huấn luyện một cổng cụ thể cho nhiệm vụ và sau đó sử dụng các tham số của mỗi cổng để thực hiện định tuyến top-k rời rạc. Quá trình này được minh họa trong Hình 2.

Bây giờ chúng tôi trình bày chi tiết các bước cụ thể này. Để cụ thể (và phù hợp với khung thí nghiệm của chúng tôi trong Mục 4), chúng tôi xem xét trường hợp các đóng góp viên thực hiện PEFT sử dụng LoRA (Hu et al., 2021). Chúng tôi nhấn mạnh rằng PHATGOOSE có thể áp dụng cho bất kỳ phương pháp PEFT nào giới thiệu các mô-đun có thể huấn luyện khắp mô hình (ví dụ (IA)³ (Liu et al., 2022), Adapters (Houlsby et al., 2019), v.v.), nhưng chúng tôi xem xét LoRA do tính phổ biến và sử dụng rộng rãi của nó. LoRA sửa đổi đầu ra của mỗi lớp tuyến tính Wut với các tham số mô hình cơ sở W∈Rd×n cho kích hoạt đầu vào thứ t ut∈Rn thành Wut+BAut trong đó A∈Rr×n và B∈Rd×r là các tham số có thể huấn luyện trong khi W vẫn đóng băng trong quá trình tinh chỉnh. Bằng cách làm như vậy, một "mô-đun" (bao gồm một cặp ma trận B và A) được giới thiệu tại mỗi lớp tuyến tính của mô hình.

Sau khi huấn luyện các mô-đun PEFT trên bộ dữ liệu của họ, đóng góp viên thêm một lớp cổng sigmoid phía trước mỗi mô-đun PEFT và huấn luyện cổng (và chỉ cổng thôi, với tất cả các tham số khác cố định) trong một số bước tương đối nhỏ (100 trong các thí nghiệm của chúng tôi) sử dụng cùng bộ dữ liệu và mục tiêu đã được sử dụng để huấn luyện mô-đun PEFT. Cổng, được chia sẻ qua tất cả các vị trí chuỗi, xác định liệu một kích hoạt nhất định có sử dụng mô-đun PEFT hay không. Trong ví dụ LoRA, một lớp tuyến tính trở thành Wut+BAut σ(vTut) trong đó v∈Rn là vector cổng có thể huấn luyện được khởi tạo bằng toàn bộ số không và W, B, và A đều bị đóng băng.

Khi các đóng góp viên chia sẻ các mô-đun PEFT và vector cổng được huấn luyện, PHATGOOSE xây dựng các router từ các vector cổng được huấn luyện va, vb, vc, ... để thực hiện định tuyến top-k trong quá trình suy luận. Một router riêng biệt được tạo tại mỗi lớp nơi các mô-đun được giới thiệu để PHATGOOSE có thể thực hiện định tuyến theo mô-đun. Cụ thể, trước tiên chúng tôi chuẩn hóa (tức là trừ trung bình và chia cho độ lệch chuẩn qua các chiều) cả vector cổng (ký hiệu bằng v̄a, v̄b, ...) và một kích hoạt nhất định (ký hiệu ūt) cho mục đích định tuyến. Sau đó, chúng tôi gán cho mỗi mô-đun một điểm bằng cách tính toán độ tương tự cosine giữa ūt và vector định tuyến của nó. Sau đó chúng tôi định tuyến ut đến các mô-đun có k điểm cao nhất và, như trong Shazeer et al. (2016); Du et al. (2022); Lepikhin et al. (2020), tái cân bằng đầu ra của chúng bằng trọng số được chuẩn hóa softmax. Cụ thể hơn, trong quá trình suy luận, PHATGOOSE trước tiên tính toán ái lực αt,z giữa mô-đun PEFT z và kích hoạt ut là v̄Tzūt. Sau đó, PHATGOOSE tập hợp Et, tập hợp các mô-đun PEFT top-k cho một kích hoạt nhất định, bằng cách tính toán Et=top-k(αt,a, αt,b, ...). Sau đó, trọng số tỷ lệ cho mỗi mô-đun được tính toán bằng wt=softmax({αt,z/√n, z∈Et}) trong đó việc tỷ lệ bằng 1/√n được bao gồm như cách thông thường để tránh bão hòa softmax khi được cung cấp tích vô hướng của các vector chuẩn hóa (Vaswani et al., 2017). Cuối cùng, đầu ra của lớp tuyến tính cho kích hoạt ut được tính toán là Wut+∑z∈Et wt,z BzAzut.

Tại sao các cổng sigmoid, được huấn luyện với phần còn lại của mô hình cố định, lại hữu ích cho định tuyến kiểu top-k trong quá trình suy luận? Chúng tôi mong đợi rằng vector cổng cho một mô-đun PEFT nhất định học để liên kết với các đặc điểm của kích hoạt liên quan đến nhiệm vụ mà mô-đun PEFT được huấn luyện. Việc kết hợp các cổng từ nhiều mô-đun PEFT được huấn luyện trên các nhiệm vụ khác nhau sau đó sẽ định tuyến dựa trên mức độ liên quan của mô-đun PEFT tương ứng đối với một kích hoạt đầu vào nhất định. Chúng tôi lưu ý rằng việc cố định phần còn lại của mô hình trong quá trình huấn luyện cổng ngăn chặn phần còn lại của mô hình coadapt với cổng. Để xác thực cách tiếp cận này, chúng tôi xem xét hai cách thay thế để tạo vector cổng: Đầu tiên, trong Mục 4, chúng tôi xem xét một baseline trong đó các vector router được tính toán là kích hoạt trung bình trên một bộ dữ liệu nhất định, và thứ hai, trong Phụ lục A, chúng tôi xem xét việc huấn luyện chung các mô-đun PEFT và cổng trong một bước. Huấn luyện cổng sau khi mô-đun PEFT đã được huấn luyện và đóng băng cuối cùng dẫn đến hiệu suất tốt hơn so với cả hai lựa chọn thay thế này.

Để nhấn mạnh lại, PHATGOOSE có thể tái sử dụng các mô-đun PEFT từ các đóng góp viên mà không yêu cầu các bộ dữ liệu mà các mô-đun được huấn luyện và không phát sinh chi phí bổ sung đáng kể. Tuy nhiên, điều quan trọng là phải thừa nhận rằng PHATGOOSE yêu cầu một bước huấn luyện bổ sung cho các cổng. Mặc dù có yêu cầu này, chúng tôi thấy rằng cổng có thể được huấn luyện chỉ trong 100 lần lặp sử dụng chính xác cùng bộ dữ liệu, mục tiêu và siêu tham số như huấn luyện mô-đun PEFT, và do đó áp đặt gánh nặng bổ sung tối thiểu cho mỗi đóng góp viên.

4 Thí nghiệm

Sau khi giới thiệu PHATGOOSE, bây giờ chúng tôi chuyển sang xác thực thực nghiệm cách tiếp cận của chúng tôi. Chúng tôi tập trung vào khung được sử dụng rộng rãi của việc cải thiện tổng quát hóa zero-shot trong các mô hình T5. Các thí nghiệm của chúng tôi xem xét hai nhóm chuyên gia khác nhau và ba benchmark tổng quát hóa zero-shot khác nhau.

4.1 Thiết lập

Sanh et al. (2021) thấy rằng huấn luyện đa nhiệm vụ rõ ràng của T5 (Raffel et al., 2020) trên một bộ sưu tập các bộ dữ liệu được nhắc tạo ra một mô hình có hiệu suất zero-shot mạnh mẽ trên các nhiệm vụ chưa thấy. Điều này đã trở thành một khung thí nghiệm phổ biến để đánh giá tổng quát hóa zero-shot (ví dụ (Chung et al., 2022; Longpre et al., 2023; Jang et al., 2023; Zhou et al., 2022), v.v.), vì vậy chúng tôi áp dụng nó trong nghiên cứu của mình. Cụ thể, làm mô hình cơ sở, chúng tôi sử dụng LM-adapted T5.1.1 XL (Lester et al., 2021), một biến thể 3B-tham số của mô hình ngôn ngữ T5 (Raffel et al., 2020) đã trải qua thêm 100K bước huấn luyện sử dụng mục tiêu mô hình hóa ngôn ngữ tiêu chuẩn trên bộ dữ liệu C4.

Để tạo nhóm các mô-đun chuyên gia để định tuyến giữa, chúng tôi xem xét hai bộ sưu tập bộ dữ liệu. Đối với bộ thứ nhất ("T0 Held-In"), chúng tôi sử dụng cùng tập hợp 36 bộ dữ liệu và nhiệm vụ được nhắc held-in đã được sử dụng để huấn luyện T0 (Sanh et al., 2021). Đối với bộ thứ hai ("FLAN"), chúng tôi xem xét bộ sưu tập FLAN lớn các bộ dữ liệu được nhắc (Longpre et al., 2023). Bộ sưu tập FLAN mở rộng T0 Held-in với các bộ dữ liệu được nhắc zero-shot và few-shot từ SuperGLUE (Wang et al., 2019), Super Natural Instructions (Wang et al., 2022b), các bộ dữ liệu hội thoại, và các bộ dữ liệu Chain-of-Thought (Wei et al., 2022). Chúng tôi chỉ xem xét những bộ dữ liệu có định dạng nhắc zero-shot, dẫn đến tổng cộng 166 mô hình chuyên biệt từ Bộ sưu tập FLAN.

Đối với các mô-đun PEFT của chúng tôi, chúng tôi tập trung vào Low-Rank Adapters (LoRAs, Hu et al., 2021), nhưng lưu ý rằng không có gì về PHATGOOSE yêu cầu sử dụng LoRA và chúng tôi mong đợi nó sẽ có thể áp dụng như nhau cho các kiến trúc mô-đun khác (ví dụ (IA)³ (Liu et al., 2022), Adapters (Houlsby et al., 2019), v.v.). Chúng tôi huấn luyện một mô-đun PEFT duy nhất cho mỗi bộ dữ liệu trong một trong hai bộ sưu tập bộ dữ liệu, dẫn đến hai thiết lập với 36 hoặc 166 mô hình chuyên gia cho T0 Held-In và FLAN tương ứng.

Chúng tôi xem xét ba benchmark tổng quát hóa zero-shot để đánh giá. Đối với bộ thứ nhất ("T0HO"), chúng tôi sử dụng cùng các bộ dữ liệu held-out được sử dụng để đánh giá T0 (Sanh et al., 2021). Vì bộ sưu tập FLAN bao gồm các bộ dữ liệu held-out từ T0, chúng tôi không đánh giá trên T0HO khi sử dụng nhóm chuyên gia FLAN. Đối với bộ thứ hai và thứ ba, chúng tôi xem xét hai biến thể của BIG-bench (BIG-bench authors, 2023), một bộ sưu tập các bộ dữ liệu được cộng đồng tuyển chọn đo lường các khả năng khác nhau của mô hình như lý luận, sáng tạo, thiên vị, v.v. Cụ thể, chúng tôi đánh giá trên BIG-bench Hard (Suzgun et al., 2022) và BIG-bench Lite (BIG-bench authors, 2023). BIG-Bench Hard (BBH) là một bộ sưu tập 23 bộ dữ liệu mà các mô hình hiện đại thực hiện đáng kể tệ hơn so với con người. BIG-Bench Lite (BBL) bao gồm 24 bộ dữ liệu đa dạng được coi là proxy nhẹ cho benchmark BIG-Bench đầy đủ. Vì tokenizer T5 không thể tokenize một số bộ dữ liệu trong bộ sưu tập BIG-bench, chúng tôi loại trừ chúng trong quá trình đánh giá (thảo luận được cung cấp trong B). Trong tất cả các trường hợp, chúng tôi lấy nguồn các bộ dữ liệu từ Hugging Face Hub.

Mặc dù PHATGOOSE không yêu cầu các đóng góp viên khác nhau sử dụng cùng siêu tham số, để đơn giản chúng tôi huấn luyện rank r=16 LoRAs trên mỗi bộ dữ liệu trong 1000 bước trên các batch với tối đa 1024 chuỗi độ dài-512 sử dụng optimizer AdamW (Loshchilov & Hutter, 2017) với tỷ lệ học 5e-3 và tỷ lệ warmup 0.06. Chúng tôi thực hiện lựa chọn checkpoint trên bước validation với độ chi tiết 100 bước. Đối với PHATGOOSE, sau khi huấn luyện mỗi mô-đun, chúng tôi đóng băng tất cả các tham số và huấn luyện vector cổng thêm 100 bước với cùng siêu tham số. Theo thông lệ tiêu chuẩn trong các công trình trước đây (Shazeer et al., 2016; Du et al., 2022; Lepikhin et al., 2020), chúng tôi sử dụng k=2 cho định tuyến top-k.

4.2 Baselines

Chúng tôi so sánh với các baseline khác nhau tương tự tái sử dụng các mô-đun chuyên gia để cải thiện tổng quát hóa zero-shot.

Retrieval Nhiều công trình trước đây đã xem xét việc truy xuất một mô hình chuyên gia cho một truy vấn nhất định bằng cách so sánh embedding của truy vấn (Reimers & Gurevych, 2019) với embedding của các ví dụ được sử dụng để huấn luyện mỗi chuyên gia (Jang et al., 2023; Maxine, 2023; Durbin, 2024; Belofsky, 2023). Các công trình trước đây đã khác nhau một chút, nhưng chúng tôi thấy hầu hết các chi tiết triển khai tương đối không quan trọng và dựa triển khai của chúng tôi trên Jang et al. (2023). Cụ thể, chúng tôi nhúng văn bản sử dụng mô hình MiniLM-L6-v2 (Reimers & Gurevych, 2019), như được sử dụng bởi Jang et al. (2023); Maxine (2023); Belofsky (2023). Chúng tôi lưu trữ embedding cho 1000 ví dụ ngẫu nhiên từ mỗi bộ dữ liệu được sử dụng để huấn luyện mỗi mô hình chuyên gia. Sau đó chúng tôi định tuyến mỗi truy vấn đến chuyên gia tương ứng với ví dụ có embedding có độ tương tự cosine cao nhất với embedding của truy vấn.

Average Activation Vì baseline Retrieval thực hiện định tuyến theo ví dụ và mô hình (thay vì theo token và mô-đun như PHATGOOSE), chúng tôi thiết kế thêm một baseline để so sánh trực tiếp hơn với PHATGOOSE. Cụ thể, chúng tôi xem xét một biến thể của PHATGOOSE trong đó chúng tôi thay thế mỗi vector cổng đã học (ví dụ va) với kích hoạt trung bình (tức là trung bình của u1, u2, ...) trên bộ dữ liệu được sử dụng để huấn luyện một mô-đun chuyên gia nhất định. Để tính toán trung bình, chúng tôi sử dụng cùng 1000 ví dụ ngẫu nhiên từ mỗi bộ dữ liệu như được sử dụng bởi phương pháp Retrieval.

Arrow Ostapenko et al. (2024) định tuyến giữa các mô-đun chuyên gia bằng cách xây dựng vector cổng từ chính các mô-đun chuyên gia. Cụ thể, phương pháp này giả định các mô-đun là các chuyên gia LoRA và sử dụng vector kỳ dị phải đầu tiên của tích ngoài của cập nhật LoRA BA làm vector cổng. Mỗi đầu vào sau đó được định tuyến dựa trên phân phối xác suất được tính toán sử dụng các điểm được đưa ra bởi tích vô hướng tuyệt đối giữa biểu diễn của đầu vào và các vector cổng. Chúng tôi sử dụng định tuyến top-k với k=2 trong phương pháp này để có cùng tính toán suy luận như PHATGOOSE.

Merged Experts Merging (Matena & Raffel, 2022; Choshen et al., 2022), liên quan đến việc tính trung bình các tham số của các mô hình hoặc mô-đun khác nhau để tạo ra một mô hình tổng hợp duy nhất, cung cấp một cách khác để tái sử dụng mô hình. Gần đây, Ostapenko et al. (2023) đã chứng minh rằng tính toán trung bình đồng đều của các mô-đun PEFT có thể cải thiện tổng quát hóa zero-shot. Cách tiếp cận như vậy có thể được xem là một hình thức cực đoan của học đa nhiệm vụ phân tán hoặc liên kết (Smith et al., 2017) với một bước federated averaging duy nhất (McMahan et al., 2017) ở cuối quá trình huấn luyện. Chúng tôi bao gồm merging làm baseline bằng cách tính toán trung bình không có trọng số đơn giản của tất cả các chuyên gia LoRA trong nhóm. Theo Ostapenko et al. (2003), chúng tôi tính trung bình sau khi tính toán mỗi tích ngoài LoRA để các mô-đun được hợp nhất có thể có rank cao hơn r. Lưu ý rằng merging chỉ có thể áp dụng khi các mô-đun chuyên gia cá nhân có cùng kiến trúc, trong khi PHATGOOSE về nguyên tắc có thể được sử dụng với các mô-đun chuyên gia không đồng nhất.

Ngoài các phương pháp baseline trên thỏa mãn khung bài toán của chúng tôi (Mục 2), chúng tôi so sánh với một số baseline bổ sung vi phạm khung bài toán của chúng tôi nhưng tuy nhiên cung cấp một điểm so sánh hữu ích.

Multitask Huấn luyện đa nhiệm vụ rõ ràng yêu cầu truy cập đồng thời vào bộ dữ liệu của mỗi chuyên gia và do đó vi phạm khung bài toán của chúng tôi. Tuy nhiên, với việc huấn luyện đa nhiệm vụ là một cách rộng rãi và hiệu quả để cải thiện tổng quát hóa zero-shot (Sanh et al., 2021; Wei et al., 2021), chúng tôi đưa nó vào làm baseline. Chúng tôi thiếu tài nguyên tính toán để huấn luyện các mô hình đa nhiệm vụ của riêng mình, vì vậy chúng tôi sử dụng các mô hình công khai thay thế. Đối với nhóm bộ dữ liệu T0 Held-In, chúng tôi so sánh mô hình T0-3B đã được huấn luyện trên cùng bộ sưu tập bộ dữ liệu (Sanh et al., 2021). Đối với FLAN, thật không may không có mô hình công khai nào được huấn luyện trên cùng bộ dữ liệu chúng tôi xem xét. Mô hình được huấn luyện trên hỗn hợp bộ dữ liệu tương tự nhất là FLAN-T5 XL, bao gồm một bộ sưu tập bộ dữ liệu khác (và không công khai). Chúng tôi báo cáo hiệu suất của FLAN-T5 XL để tham khảo nhưng nhấn mạnh rằng nó không nên được so sánh trực tiếp.

Oracle Như được xem xét trong (Jang et al., 2023), chúng tôi xem xét một sơ đồ định tuyến "oracle" chọn chuyên gia chuyên biệt từ nhóm có hiệu suất cao nhất trên một bộ dữ liệu đánh giá nhất định. Sơ đồ định tuyến như vậy không phải là zero-shot và phục vụ như một giới hạn trên cho hiệu suất của các cách tiếp cận kiểu retrieval.

Best Individual Cũng được xem xét trong (Jang et al., 2023), chúng tôi tìm chuyên gia duy nhất có hiệu suất trung bình cao nhất trên tất cả các bộ dữ liệu đánh giá. Cách tiếp cận này, cũng không phải là zero-shot, phục vụ như hiệu suất trường hợp tốt nhất của một sơ đồ định tuyến suy biến luôn chọn cùng một chuyên gia cho tất cả đầu vào.

4.3 Kết quả

Hiệu suất của PHATGOOSE và các baseline chúng tôi mô tả ở trên trên tất cả các tổ hợp nhóm chuyên gia/benchmark zero-shot được trình bày trong Bảng 1.

Trong thiết lập T0 Held-In, PHATGOOSE nổi bật vượt trội đáng kể so với các phương pháp trước đây. Sự cải thiện đặc biệt lớn trên các nhiệm vụ T0 Held-Out, nơi PHATGOOSE gần như khớp với hiệu suất của định tuyến Oracle không-zero-shot. Đáng chú ý, chúng tôi thấy rằng PHATGOOSE luôn khớp hoặc vượt trội so với baseline Multitask T0-3B, mặc dù được huấn luyện theo cách phi tập trung (tức là không có truy cập dữ liệu đồng thời). PHATGOOSE có hiệu suất thấp hơn một chút (0.4%) so với Merged Experts trong BBH nhưng vượt trội hơn 11% trên T0HO và 1.5% trên BBL.

Khi mở rộng nhóm chuyên gia từ 36 chuyên gia trong T0 Held-in đến 166 chuyên gia trong thiết lập FLAN, PHATGOOSE vượt trội so với tất cả các phương pháp định tuyến khác trên cả BBL và BBH. Tuy nhiên, khoảng cách giữa các phương pháp định tuyến và định tuyến oracle nói chung lớn hơn trong thiết lập FLAN. Ngoài ra, hiệu suất của tất cả các phương pháp định tuyến giảm trên BBL khi mở rộng nhóm chuyên gia. Để hiểu rõ hơn về hành vi này, chúng tôi lưu ý rằng hiệu suất của PHATGOOSE trên các nhiệm vụ yêu cầu lý luận logic (ví dụ Object Counting, Ruin Names, Track Shuffled Objects, Operators, và Winowhy) có xu hướng tăng khi nhóm chuyên gia được mở rộng, trong khi hiệu suất trên các nhiệm vụ nặng về kiến thức (Conlang Translation, Known Unknown, Hindu Knowledge, và Novel Concepts) có xu hướng giảm. Điều này có thể là do các nhiệm vụ thâm dụng kiến thức yêu cầu các chuyên gia đã ghi nhớ thông tin nhất định (có thể ít phổ biến hơn trong nhóm chuyên gia lớn hơn) trong khi các nhiệm vụ dựa trên lý luận hưởng lợi từ việc kết hợp nhiều kỹ năng hơn từ nhiều chuyên gia hơn. Trong mọi trường hợp, hành vi này nổi bật tầm quan trọng của nghiên cứu tương lai về học định tuyến post-hoc giữa các chuyên gia chuyên biệt cho tổng quát hóa zero-shot.

Ngoài những hiểu biết cụ thể được đề cập ở trên, chúng tôi nhấn mạnh rằng PHATGOOSE luôn và đáng kể vượt trội so với Retrieval và Arrow (chỉ có các công trình trước đây về định tuyến post-hoc cho tổng quát hóa zero-shot). Bước huấn luyện cổng trong PHATGOOSE là thiết yếu vì các phương pháp như Arrow xây dựng cổng sử dụng các mô-đun chuyên gia và Average Activation (một biến thể của PHATGOOSE tránh bước huấn luyện cổng) kém hiệu quả so với PHATGOOSE. Ngoài ra, Retrieval - thiếu cơ chế để kết hợp các chuyên gia khác nhau - luôn kém hiệu quả so với các phương pháp định tuyến thích ứng như PHATGOOSE và Average Activation trên tất cả các đánh giá. Điều này cho thấy rằng khả năng kết hợp kiến thức từ các chuyên gia chuyên biệt có thể có lợi khi học định tuyến post-hoc cho tổng quát hóa zero-shot.

4.4 Phân tích định tính

Sau khi thiết lập hiệu suất mạnh mẽ của PHATGOOSE, bây giờ chúng tôi thực hiện một nghiên cứu định tính để hiểu rõ hơn về lợi ích của định tuyến thích ứng theo token và theo mô-đun. Cụ thể, chúng tôi đo lường liệu sự liên kết giữa định tuyến đã học của PHATGOOSE và định tuyến Oracle có tương quan với hiệu suất của PHATGOOSE hay không. Với một bộ dữ liệu đánh giá cụ thể, chúng tôi tính toán phân phối định tuyến cho PHATGOOSE bằng cách tính trung bình xác suất định tuyến của tất cả các token trong bộ dữ liệu qua tất cả các mô-đun. Sau đó chúng tôi định lượng sự liên kết giữa định tuyến đã học của PHATGOOSE và Oracle sử dụng divergence KL giữa các phân phối định tuyến tương ứng của chúng. Sau đó, chúng tôi tính toán tương quan giữa divergence KL của các phân phối định tuyến và hiệu suất của PHATGOOSE qua tất cả các bộ dữ liệu trong đánh giá của chúng tôi để xác định liệu thành công của PHATGOOSE có thể được quy cho sự liên kết với định tuyến Oracle hay không. Chúng tôi thấy hệ số tương quan Pearson là -0.2, chỉ ra ít hoặc không có tương quan giữa hiệu suất của PHATGOOSE và sự liên kết của nó với Oracle. Điều này cho thấy rằng PHATGOOSE tìm thấy các chiến lược định tuyến hiệu quả khác biệt so với định tuyến Oracle.

Để khám phá các chiến lược như vậy, trong Hình 3 chúng tôi cung cấp một biểu diễn trực quan của phân phối định tuyến của PHATGOOSE cho hai bộ dữ liệu, nổi bật các trường hợp mà hiệu suất của PHATGOOSE khớp hoặc vượt trội so với định tuyến Oracle. Biểu đồ minh họa phân phối định tuyến qua tập hợp 36 mô-đun T0 Held-In tại mỗi lớp khắp mô hình. Đối với bộ dữ liệu Story Cloze, chúng tôi nhận thấy rằng PHATGOOSE thường định tuyến đến cùng mô-đun như Oracle trong các lớp encoder, nhưng sử dụng chiến lược định tuyến đa dạng hơn trong decoder. Trên CB, PHATGOOSE gần như không bao giờ định tuyến đến mô-đun Oracle nhưng tuy nhiên vượt trội so với định tuyến Oracle 10%. Điều này có thể được quy cho khả năng của PHATGOOSE kết hợp hiệu quả các khả năng của nhiều chuyên gia, do đó tăng cường tổng quát hóa.

5 Nghiên cứu liên quan

Định tuyến giữa các LLM Nghiên cứu gần đây của Shnitzer et al. (2023) và Lu et al. (2023) xem xét vấn đề chọn LLM tổng quát nào để định tuyến truy vấn. Shnitzer et al. (2023) huấn luyện một bộ phân loại nhị phân cho mỗi LLM để dự đoán tính chính xác của phản hồi của nó đối với một đầu vào, cho phép lựa chọn LLM chính xác trong quá trình suy luận. Lu et al. (2023) huấn luyện một router để phân phối truy vấn giữa các LLM, được thông báo bởi các xếp hạng mô hình thưởng được chưng cất, do đó tránh việc kích hoạt tất cả LLM cho mỗi truy vấn. Trong công trình của chúng tôi, thay vào đó chúng tôi tập trung vào định tuyến giữa các mô hình chuyên biệt.

Tái sử dụng mô-đun cho học few-shot Trái ngược với tập trung vào tổng quát hóa zero-shot của chúng tôi, một số nghiên cứu đã xem xét việc tái sử dụng các mô-đun chuyên biệt cho học few-shot trên một bộ dữ liệu được gán nhãn nhỏ. LoRAHub (Huang et al., 2023) sử dụng optimizer hộp đen để học trọng số tích hợp các mô-đun LoRA chuyên biệt cho một nhiệm vụ few-shot. Ngược lại, Wu et al. (2024) sử dụng các ví dụ few-shot để học một hàm cổng với router có thể huấn luyện, đạt hiệu suất tương đương với LoRAHub. Wu et al. (2023) huấn luyện các chuyên gia cụ thể cho nhiệm vụ, sau đó sử dụng embedding nhiệm vụ dựa trên đường chéo của ma trận thông tin Fisher để truy xuất, tính trung bình và huấn luyện các mô-đun từ top-k nhiệm vụ tương tự nhất với một nhiệm vụ đích. Pfeiffer et al. (2020) độc lập học các adapter cho mỗi nhiệm vụ, sau đó sử dụng mô-đun kết hợp kiến thức để kết hợp các adapter ở các lớp khác nhau, vượt trội so với huấn luyện đa nhiệm vụ độc lập và tinh chỉnh mô hình đầy đủ trên 16 nhiệm vụ hiểu ngôn ngữ tự nhiên. Gou et al. (2023) huấn luyện LoRA cho các cluster dữ liệu và một LoRA đa năng cho toàn bộ bộ dữ liệu, tăng cường tổng quát hóa cho các chỉ dẫn chưa thấy. Shah et al. (2023) huấn luyện các LoRA nội dung và phong cách độc lập, sử dụng các vector hợp nhất để tối thiểu hóa can thiệp, và kết hợp LoRA bằng cách sử dụng dữ liệu huấn luyện từ cả hai lĩnh vực. Wang et al. (2023) hợp nhất các mô hình SAM và CLIP để tạo ra SAM-CLIP cho phân đoạn dựa trên ngôn ngữ. Vì tất cả các nghiên cứu này sử dụng các bộ dữ liệu nhiệm vụ đích được gán nhãn, chúng tôi loại trừ chúng khỏi so sánh.

Hợp nhất các mô hình chuyên gia Model merging (Choshen et al., 2022; Wortsman et al., 2022; Ramé et al., 2022; Matena & Raffel, 2022; Ilharco et al., 2022; Yadav et al., 2023; Tam et al., 2023; Jin et al., 2022; Yang et al., 2023) nhằm kết hợp các khả năng của các mô hình được huấn luyện trên các nhiệm vụ hoặc lĩnh vực khác nhau thành một mô hình duy nhất. Nhiều phương pháp merging dựa trên một bộ dữ liệu để tính toán thống kê hoặc điều chỉnh siêu tham số, vì vậy chúng tôi tập trung vào so sánh với việc tính trung bình tham số đơn giản trong các thí nghiệm của chúng tôi (vẫn là một phương pháp rộng rãi). Ngoài ra, các phương pháp merging hiện đại thường kém hiệu quả so với huấn luyện đa nhiệm vụ (Tam et al., 2023; Ilharco et al., 2022). Merging cũng đã được sử dụng như một thành phần của các hệ thống nhằm cho phép tổng quát hóa zero-shot. Ví dụ, Chronopoulou et al. (2023) hợp nhất các adapter nhiệm vụ và ngôn ngữ riêng biệt để cho phép tổng quát hóa đa ngôn ngữ.

Tinh chỉnh đa nhiệm vụ cho tổng quát hóa zero-shot Trong học đa nhiệm vụ, một mô hình được huấn luyện đồng thời trên một bộ sưu tập các bộ dữ liệu từ các nhiệm vụ khác nhau. Học đa nhiệm vụ nói chung giả định truy cập vào tất cả các bộ dữ liệu cùng một lúc, khác với trọng tâm của công trình chúng tôi. Trong trường hợp đơn giản nhất, một mô hình cơ sở được tinh chỉnh trên một hỗn hợp đa nhiệm vụ của các bộ dữ liệu. Học đa nhiệm vụ như vậy đã được nhất quán để cải thiện tổng quát hóa zero-shot trên các nhiệm vụ chưa thấy (Sanh et al., 2021; Chung et al., 2022; Wei et al., 2021).

Các mô hình mixture-of-expert đa nhiệm vụ Thay vào đó, nhiều nghiên cứu gần đây đã khám phá việc huấn luyện các mô hình kiểu mixture-of-experts trên các hỗn hợp đa nhiệm vụ. Trong các mô hình như vậy, một router chọn các chuyên gia tốt nhất cho một đầu vào nhất định, và cả router và các chuyên gia đều được huấn luyện sử dụng tất cả các bộ dữ liệu cùng một lúc. Các nghiên cứu như Muqeeth et al. (2023); Zadouri et al. (2023); Wang et al. (2022a) huấn luyện một hệ thống định tuyến mỗi ví dụ giữa một tập hợp các chuyên gia và đã chứng minh hiệu suất cải thiện trên các nhiệm vụ chưa thấy. Thay vào đó, Ponti et al. (2023) huấn luyện một ma trận kỹ năng học phân bổ một nhiệm vụ cho một tập hợp các kỹ năng, với mỗi kỹ năng là một mô-đun hiệu quả tham số. Để thích ứng với một nhiệm vụ few-shot mới, họ tinh chỉnh cả ma trận kỹ năng và các chuyên gia. Caccia et al. (2023) cho thấy rằng việc chia các tham số chuyên gia thành các khối và định tuyến giữa các khối này hiệu quả hơn so với chỉ định tuyến giữa một tập hợp các chuyên gia. Họ cũng thấy rằng chỉ tinh chỉnh router cho một thích ứng few-shot hoạt động gần như tốt như huấn luyện cả chuyên gia và router trong khi hiệu quả hơn. Gupta et al. (2022) huấn luyện một router riêng biệt cho mỗi nhiệm vụ, đó là một mạng cổng nhận biết nhiệm vụ. Đối với một nhiệm vụ mới, họ chọn một router từ một nhiệm vụ tương tự dựa trên kiến thức lĩnh vực và sử dụng nó để định tuyến các ví dụ từ nhiệm vụ mới. Ye et al. (2022) huấn luyện một nhóm nhỏ các chuyên gia, mỗi chuyên gia là một lớp transformer hoàn chỉnh, với một router chọn các chuyên gia khác nhau cho mỗi lớp dựa trên biểu diễn nhiệm vụ được rút ra từ embedding trung bình của các ví dụ bộ dữ liệu được mã hóa sử dụng BART encoder (Lewis et al., 2019). Cách tiếp cận này cho phép router chọn hiệu quả các chuyên gia phù hợp nhất cho các nhiệm vụ chưa thấy bằng cách tận dụng các biểu diễn cụ thể cho nhiệm vụ. Các nghiên cứu trước đây về các mô hình mixture-of-experts đa nhiệm vụ này có một số điểm tương đồng với khung bài toán của chúng tôi nhưng cuối cùng dựa vào truy cập dữ liệu đồng thời. Tuy nhiên, chúng tôi lạc quan rằng những hiểu biết có thể được chia sẻ giữa các khung bổ sung này.

6 Kết luận

Trong bài báo này, chúng tôi đã giới thiệu Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE). PHATGOOSE cung cấp một cách để tái sử dụng các mô-đun chuyên gia được tạo ra thông qua huấn luyện hiệu quả tham số để cải thiện tổng quát hóa zero-shot của một mô hình cơ sở. Cụ thể, PHATGOOSE yêu cầu các đóng góp viên thực hiện một bước tính toán bổ sung không tốn kém liên quan đến việc huấn luyện một cổng sigmoid cho mỗi mô-đun. Các tham số của các cổng này được kết hợp để tạo ra một router top-k giữa các mô-đun. Trong các thí nghiệm trên khung được sử dụng rộng rãi của việc cải thiện tổng quát hóa zero-shot của các mô hình họ T5, chúng tôi thấy rằng PHATGOOSE nói chung vượt trội so với các phương pháp khác học các chiến lược định tuyến post-hoc giữa các mô-đun chuyên biệt và thường xuyên khớp hoặc vượt trội so với huấn luyện đa nhiệm vụ rõ ràng. Chúng tôi cũng phân tích định tính định tuyến được học bởi PHATGOOSE và thấy rằng nó có thể học các chiến lược định tuyến hiệu quả khác biệt so với chiến lược Oracle đơn giản định tuyến đến mô-đun đạt hiệu suất tốt nhất trên một nhiệm vụ nhất định.

Công trình của chúng tôi, và khung bài toán đề xuất của chúng tôi, mở ra các hướng cho nghiên cứu tương lai về phát triển mô hình cộng tác phi tập trung. Đầu tiên, trong khi chúng tôi tập trung vào khung tiêu chuẩn của việc điều chỉnh các mô hình họ T5 để tổng quát hóa zero-shot tốt hơn, chúng tôi sẽ quan tâm đến việc áp dụng PHATGOOSE cho các Transformer chỉ decoder đã trở nên rộng rãi trong việc phát triển LLM. Thứ hai, trong khi nghiên cứu của chúng tôi tập trung vào các mô-đun dựa trên LoRA với cùng rank (Hu et al., 2021), PHATGOOSE có thể áp dụng cho một loạt rộng các kiến trúc mô-đun, bao gồm các trường hợp mà các mô-đun không nhất thiết phải chia sẻ kiến trúc. Khám phá các kiến trúc mô-đun PEFT khác nhau (như Adapters (Houlsby et al., 2019) và (IA)³ (Liu et al., 2022)) và định tuyến giữa các mô hình không đồng nhất có thể cải thiện hiệu suất và hiệu quả của PHATGOOSE. Cuối cùng, chúng tôi lưu ý lại rằng không có chiến lược định tuyến post-hoc nào mà chúng tôi xem xét thể hiện mức tăng nhất quán khi tăng kích thước của bộ sưu tập mô-đun. Mô hình này phản ánh xu hướng được chú ý trong huấn luyện đa nhiệm vụ rõ ràng, nơi các mô hình thể hiện hiệu suất mạnh mẽ trên một số bộ dữ liệu nhất định trong khi kém hiệu quả trên những bộ khác (Sanh et al., 2021; Chung et al., 2022). Giải quyết điều này có thể đóng góp đáng kể vào việc phát triển các mô hình không chỉ học liên tục mà còn thể hiện tổng quát hóa zero-shot tăng cường cho các nhiệm vụ chưa thấy khi nhóm chuyên gia mở rộng. Nhìn chung, chúng tôi lạc quan rằng nghiên cứu tương lai sẽ nghiên cứu và xây dựng trên những vấn đề này và cho phép một mô hình mới cho việc phát triển mô hình.

Tuyên bố tác động

Bài báo này trình bày nghiên cứu có mục tiêu thúc đẩy lĩnh vực Học máy. Có nhiều hậu quả xã hội tiềm năng của công trình chúng tôi, không có gì chúng tôi cảm thấy phải được nêu bật cụ thể ở đây.

Lời cảm ơn

Cảm ơn Derek Tam về phản hồi về bản thảo của bài báo này. Công trình này được hỗ trợ bởi NSF-AI Engage Institute DRL-2112635.
