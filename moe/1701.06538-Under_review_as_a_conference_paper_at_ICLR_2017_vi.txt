# 1701.06538.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/1701.06538.pdf
# Kích thước tệp: 544288 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
MẠNG NEURAL CỰC KỲ LỚN:
LỚP HỖN HỢP CHUYÊN GIA CÓ CỔNG THƯA
Noam Shazeer1, Azalia Mirhoseiniy1, Krzysztof Maziarz2, Andy Davis1, Quoc Le1, Geoffrey
Hinton1và Jeff Dean1
1Google Brain, {noam,azalia,andydavis,qvl,geoffhinton,jeff}@google.com
2Đại học Jagiellonian, Cracow, krzysztof.maziarz@student.uj.edu.pl

TÓM TẮT
Khả năng hấp thụ thông tin của một mạng neural bị giới hạn bởi số lượng tham số của nó. Tính toán có điều kiện, nơi các phần của mạng hoạt động trên cơ sở từng ví dụ, đã được đề xuất về mặt lý thuyết như một cách tăng đáng kể khả năng của mô hình mà không tăng tỷ lệ thuận trong tính toán. Tuy nhiên, trong thực tế, có những thách thức đáng kể về thuật toán và hiệu suất. Trong công trình này, chúng tôi giải quyết những thách thức này và cuối cùng thực hiện lời hứa của tính toán có điều kiện, đạt được cải thiện hơn 1000 lần trong khả năng mô hình với chỉ những tổn thất nhỏ trong hiệu quả tính toán trên các cụm GPU hiện đại. Chúng tôi giới thiệu lớp Hỗn hợp Chuyên gia có Cổng Thưa (MoE), bao gồm lên đến hàng nghìn mạng con feed-forward. Một mạng cổng có thể huấn luyện xác định sự kết hợp thưa của những chuyên gia này để sử dụng cho mỗi ví dụ. Chúng tôi áp dụng MoE cho các nhiệm vụ mô hình hóa ngôn ngữ và dịch máy, nơi khả năng mô hình là quan trọng để hấp thụ lượng kiến thức khổng lồ có sẵn trong các kho dữ liệu huấn luyện. Chúng tôi trình bày các kiến trúc mô hình trong đó một MoE với lên đến 137 tỷ tham số được áp dụng tích chập giữa các lớp LSTM xếp chồng. Trên các điểm chuẩn mô hình hóa ngôn ngữ lớn và dịch máy, những mô hình này đạt được kết quả tốt hơn đáng kể so với hiện đại nhất với chi phí tính toán thấp hơn.

1 GIỚI THIỆU VÀ CÔNG TRÌNH LIÊN QUAN

1.1 TÍNH TOÁN CÓ ĐIỀU KIỆN
Khai thác quy mô trong cả dữ liệu huấn luyện và kích thước mô hình đã là trung tâm của sự thành công của học sâu. Khi các tập dữ liệu đủ lớn, việc tăng khả năng (số lượng tham số) của mạng neural có thể cho độ chính xác dự đoán tốt hơn nhiều. Điều này đã được chỉ ra trong các lĩnh vực như văn bản (Sutskever et al., 2014; Bahdanau et al., 2014; Jozefowicz et al., 2016; Wu et al., 2016), hình ảnh (Krizhevsky et al., 2012; Le et al., 2012), và âm thanh (Hinton et al., 2012; Amodei et al., 2015). Đối với các mô hình học sâu điển hình, nơi toàn bộ mô hình được kích hoạt cho mọi ví dụ, điều này dẫn đến sự bùng nổ chi phí huấn luyện gấp bậc hai, khi cả kích thước mô hình và số lượng ví dụ huấn luyện đều tăng. Thật không may, những tiến bộ trong sức mạnh tính toán và tính toán phân tán không đáp ứng được nhu cầu như vậy.

Nhiều hình thức tính toán có điều kiện khác nhau đã được đề xuất như một cách để tăng khả năng mô hình mà không tăng tỷ lệ thuận chi phí tính toán (Davis & Arel, 2013; Bengio et al., 2013; Eigen et al., 2013; Ludovic Denoyer, 2014; Cho & Bengio, 2014; Bengio et al., 2015; Almahairi et al., 2015). Trong những sơ đồ này, các phần lớn của mạng hoạt động hoặc không hoạt động trên cơ sở từng ví dụ. Các quyết định cổng có thể là nhị phân hoặc thưa và liên tục, ngẫu nhiên hoặc xác định. Nhiều hình thức học tăng cường và lan truyền ngược khác nhau được đề xuất để huấn luyện các quyết định cổng.

Những người đóng góp chính như nhau
yNgười làm việc như một thành viên của chương trình Google Brain Residency (g.co/brainresidency)
1arXiv:1701.06538v1 [cs.LG] 23 Jan 2017

--- TRANG 2 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

Hình 1: Một lớp Hỗn hợp Chuyên gia (MoE) được nhúng trong một mô hình ngôn ngữ tuần hoàn. Trong trường hợp này, hàm cổng thưa chọn hai chuyên gia để thực hiện tính toán. Đầu ra của họ được điều chế bởi đầu ra của mạng cổng.

Mặc dù những ý tưởng này hứa hẹn về mặt lý thuyết, không có công trình nào cho đến nay đã chứng minh được những cải thiện lớn trong khả năng mô hình, thời gian huấn luyện, hoặc chất lượng mô hình. Chúng tôi đổ lỗi điều này cho sự kết hợp của các thách thức sau:

Các thiết bị tính toán hiện đại, đặc biệt là GPU, nhanh hơn nhiều trong tính toán số học so với phân nhánh. Hầu hết các công trình trên đều nhận ra điều này và đề xuất bật/tắt các khối lớn của mạng với mỗi quyết định cổng.

Kích thước batch lớn là quan trọng đối với hiệu suất, vì chúng phân bổ chi phí của việc truyền và cập nhật tham số. Tính toán có điều kiện làm giảm kích thước batch cho các khối hoạt động có điều kiện của mạng.

Băng thông mạng có thể là một nút cổ chai. Một cụm GPU có thể có sức mạnh tính toán lớn hơn hàng nghìn lần so với tổng băng thông mạng giữa các thiết bị. Để có hiệu quả tính toán, nhu cầu tính toán tương đối so với mạng của một thuật toán phải vượt quá tỷ lệ này. Các lớp embedding, có thể được xem như một dạng tính toán có điều kiện, bị cản trở bởi chính vấn đề này. Vì các embedding thường cần được gửi qua mạng, số lượng tương tác (ví dụ, tham số) bị giới hạn bởi băng thông mạng thay vì khả năng tính toán.

Tùy thuộc vào sơ đồ, các điều kiện tổn thất có thể cần thiết để đạt được mức độ thưa mong muốn cho mỗi khối và/hoặc mỗi ví dụ. Bengio et al. (2015) sử dụng ba điều kiện như vậy. Những vấn đề này có thể ảnh hưởng đến cả chất lượng mô hình và cân bằng tải.

Khả năng mô hình quan trọng nhất đối với các tập dữ liệu rất lớn. Văn học hiện có về tính toán có điều kiện đối phó với các tập dữ liệu nhận dạng hình ảnh tương đối nhỏ bao gồm lên đến 600.000 hình ảnh. Thật khó tưởng tượng rằng các nhãn của những hình ảnh này cung cấp tín hiệu đủ để huấn luyện thỏa đáng một mô hình với hàng triệu, chứ chưa nói đến hàng tỷ tham số.

Trong công trình này, chúng tôi lần đầu tiên giải quyết tất cả các thách thức trên và cuối cùng thực hiện lời hứa của tính toán có điều kiện. Chúng tôi đạt được cải thiện hơn 1000 lần trong khả năng mô hình với chỉ những tổn thất nhỏ trong hiệu quả tính toán và tiến bộ đáng kể so với kết quả hiện đại nhất trên các tập dữ liệu mô hình hóa ngôn ngữ và dịch thuật công khai.

1.2 PHƯƠNG PHÁP CỦA CHÚNG TÔI: LỚP HỖN HỢP CHUYÊN GIA CÓ CỔNG THƯA

Phương pháp của chúng tôi đối với tính toán có điều kiện là giới thiệu một loại thành phần mạng neural mục đích chung mới: một Lớp Hỗn hợp Chuyên gia có Cổng Thưa (MoE). MoE bao gồm một số chuyên gia, mỗi cái là một mạng neural feed-forward đơn giản, và một mạng cổng có thể huấn luyện chọn một sự kết hợp thưa của các chuyên gia để xử lý mỗi đầu vào (xem Hình 1). Tất cả các phần của mạng được huấn luyện cùng nhau bằng lan truyền ngược.

--- TRANG 3 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

Mặc dù kỹ thuật được giới thiệu là tổng quát, trong bài báo này chúng tôi tập trung vào các nhiệm vụ mô hình hóa ngôn ngữ và dịch máy, được biết là có lợi từ các mô hình rất lớn. Cụ thể, chúng tôi áp dụng một MoE tích chập giữa các lớp LSTM xếp chồng (Hochreiter & Schmidhuber, 1997), như trong Hình 1. MoE được gọi một lần cho mỗi vị trí trong văn bản, chọn một sự kết hợp khác biệt tiềm năng của các chuyên gia tại mỗi vị trí. Các chuyên gia khác nhau có xu hướng trở nên chuyên môn cao dựa trên cú pháp và ngữ nghĩa (xem Phụ lục E Bảng 9). Trên cả điểm chuẩn mô hình hóa ngôn ngữ và dịch máy, chúng tôi cải thiện so với kết quả được công bố tốt nhất với một phần chi phí tính toán.

1.3 CÔNG TRÌNH LIÊN QUAN VỀ HỖN HỢP CHUYÊN GIA

Kể từ khi được giới thiệu hơn hai thập kỷ trước (Jacobs et al., 1991; Jordan & Jacobs, 1994), phương pháp hỗn hợp chuyên gia đã là chủ đề của nhiều nghiên cứu. Các loại kiến trúc chuyên gia khác nhau đã được đề xuất như SVM (Collobert et al., 2002), Gaussian Processes (Tresp, 2001; Theis & Bethge, 2015; Deisenroth & Ng, 2015), Dirichlet Processes (Shahbaba & Neal, 2009), và mạng sâu. Công trình khác đã tập trung vào các cấu hình chuyên gia khác nhau như cấu trúc phân cấp (Yao et al., 2009), số lượng chuyên gia vô hạn (Rasmussen & Ghahramani, 2002), và thêm chuyên gia tuần tự (Aljundi et al., 2016). Garmash & Monz (2016) đề xuất một mô hình tổng hợp trong định dạng hỗn hợp chuyên gia cho dịch máy. Mạng cổng được huấn luyện trên một mô hình NMT tổng hợp đã được huấn luyện trước.

Các công trình trên liên quan đến hỗn hợp chuyên gia cấp cao nhất. Hỗn hợp chuyên gia là toàn bộ mô hình. Eigen et al. (2013) giới thiệu ý tưởng sử dụng nhiều MoE với mạng cổng riêng của chúng như các phần của một mô hình sâu. Trực quan là phương pháp sau mạnh hơn, vì các vấn đề phức tạp có thể chứa nhiều vấn đề con mỗi cái đòi hỏi các chuyên gia khác nhau. Họ cũng ám chỉ trong kết luận của họ về tiềm năng giới thiệu tính thưa, biến MoE thành một phương tiện cho tính toán có điều kiện.

Công trình của chúng tôi xây dựng trên việc sử dụng MoE này như một thành phần mạng neural mục đích chung. Trong khi Eigen et al. (2013) sử dụng hai MoE xếp chồng cho phép hai bộ quyết định cổng, ứng dụng tích chập của chúng tôi về MoE cho phép các quyết định cổng khác nhau tại mỗi vị trí trong văn bản. Chúng tôi cũng thực hiện cổng thưa và chứng minh việc sử dụng nó như một cách thực tế để tăng lớn khả năng mô hình.

2 CẤU TRÚC CỦA LỚP HỖN HỢP CHUYÊN GIA

Lớp Hỗn hợp Chuyên gia (MoE) bao gồm một tập hợp n "mạng chuyên gia" E1,...,En, và một "mạng cổng" G có đầu ra là một vector n chiều thưa. Hình 1 cho thấy tổng quan về mô-đun MoE. Các chuyên gia bản thân là mạng neural, mỗi cái với tham số riêng của chúng. Mặc dù về nguyên tắc chúng tôi chỉ yêu cầu các chuyên gia chấp nhận đầu vào cùng kích thước và tạo ra đầu ra cùng kích thước, trong các điều tra ban đầu của chúng tôi trong bài báo này, chúng tôi hạn chế bản thân với trường hợp các mô hình là mạng feed-forward với kiến trúc giống hệt nhau, nhưng với tham số riêng biệt.

Hãy để chúng tôi ký hiệu bằng G(x) và Ei(x) đầu ra của mạng cổng và đầu ra của mạng chuyên gia thứ i cho một đầu vào x cho trước. Đầu ra y của mô-đun MoE có thể được viết như sau:

y = Σ(i=1 đến n) G(x)i Ei(x)    (1)

Chúng tôi tiết kiệm tính toán dựa trên tính thưa của đầu ra của G(x). Ở bất cứ đâu G(x)i = 0, chúng tôi không cần tính Ei(x). Trong các thí nghiệm của chúng tôi, chúng tôi có lên đến hàng nghìn chuyên gia, nhưng chỉ cần đánh giá một số ít trong số họ cho mỗi ví dụ. Nếu số lượng chuyên gia rất lớn, chúng tôi có thể giảm hệ số phân nhánh bằng cách sử dụng MoE phân cấp hai cấp. Trong một MoE phân cấp, một mạng cổng chính chọn một sự kết hợp có trọng số thưa của "chuyên gia", mỗi cái bản thân là một hỗn hợp chuyên gia thứ cấp với mạng cổng riêng của nó. Trong phần sau chúng tôi tập trung vào MoE thông thường. Chúng tôi cung cấp chi tiết hơn về MoE phân cấp trong Phụ lục B.

Việc triển khai của chúng tôi liên quan đến các mô hình tính toán có điều kiện khác. Một MoE có các chuyên gia là ma trận trọng số đơn giản tương tự như ma trận trọng số được tham số hóa được đề xuất trong (Cho & Bengio, 2014). Một MoE có các chuyên gia có một lớp ẩn tương tự như dropout theo khối được mô tả trong (Bengio et al., 2015), nơi lớp bị dropout được kẹp giữa các lớp được kích hoạt đầy đủ.

--- TRANG 4 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

2.1 MẠNG CỔNG

Cổng Softmax: Một lựa chọn đơn giản của hàm cổng không thưa (Jordan & Jacobs, 1994) là nhân đầu vào với một ma trận trọng số có thể huấn luyện Wg rồi áp dụng hàm Softmax.

G(x) = Softmax(xWg)    (2)

Cổng Top-K Nhiễu: Chúng tôi thêm hai thành phần vào mạng cổng Softmax: tính thưa và nhiễu. Trước khi lấy hàm softmax, chúng tôi thêm nhiễu Gaussian có thể điều chỉnh, sau đó chỉ giữ lại k giá trị cao nhất, đặt phần còn lại thành -∞ (điều này khiến các giá trị cổng tương ứng bằng 0). Tính thưa phục vụ để tiết kiệm tính toán, như mô tả ở trên. Mặc dù hình thức tính thưa này tạo ra một số điểm không liên tục đáng sợ về mặt lý thuyết trong đầu ra của hàm cổng, chúng tôi chưa quan sát thấy điều này là một vấn đề trong thực tế. Thành phần nhiễu giúp với cân bằng tải, như sẽ được thảo luận trong Phụ lục A. Lượng nhiễu cho mỗi thành phần được kiểm soát bởi một ma trận trọng số có thể huấn luyện thứ hai Wnoise.

G(x) = Softmax(KeepTopK(H(x), k))    (3)
H(x)i = (xWg)i + StandardNormal() · Softplus((xWnoise)i)    (4)
KeepTopK(v, k)i = {vi nếu vi trong k phần tử cao nhất của v; -∞ nếu không.}    (5)

Huấn luyện Mạng Cổng: Chúng tôi huấn luyện mạng cổng bằng lan truyền ngược đơn giản, cùng với phần còn lại của mô hình. Nếu chúng tôi chọn k > 1, các giá trị cổng cho k chuyên gia hàng đầu có đạo hàm khác không đối với trọng số của mạng cổng. Loại hành vi thỉnh thoảng nhạy cảm này được mô tả trong (Bengio et al., 2013) đối với các bộ chỉnh lưu nhiễu. Gradient cũng lan truyền ngược qua mạng cổng đến đầu vào của nó. Phương pháp của chúng tôi khác ở đây so với (Bengio et al., 2015) người sử dụng cổng boolean và phương pháp kiểu REINFORCE để huấn luyện mạng cổng.

3 GIẢI QUYẾT CÁC THÁCH THỨC HIỆU SUẤT

3.1 VẤN ĐỀ BATCH THU NHỎ

Trên CPU và GPU hiện đại, kích thước batch lớn là cần thiết cho hiệu quả tính toán, để phân bổ chi phí tải và cập nhật tham số. Nếu mạng cổng chọn k trong số n chuyên gia cho mỗi ví dụ, thì đối với một batch b ví dụ, mỗi chuyên gia nhận được một batch nhỏ hơn nhiều gồm khoảng kb/n ví dụ. Điều này khiến việc triển khai MoE ngây thơ trở nên rất không hiệu quả khi số lượng chuyên gia tăng lên. Giải pháp cho vấn đề batch thu nhỏ này là làm cho kích thước batch ban đầu càng lớn càng tốt. Tuy nhiên, kích thước batch có xu hướng bị giới hạn bởi bộ nhớ cần thiết để lưu trữ kích hoạt giữa lần truyền thuận và ngược. Chúng tôi đề xuất các kỹ thuật sau để tăng kích thước batch:

Trộn Song song Dữ liệu và Song song Mô hình: Trong một thiết lập huấn luyện phân tán thông thường, nhiều bản sao của mô hình trên các thiết bị khác nhau xử lý bất đồng bộ các batch dữ liệu riêng biệt, và tham số được đồng bộ hóa thông qua một tập hợp máy chủ tham số. Trong kỹ thuật của chúng tôi, những batch khác nhau này chạy đồng bộ để chúng có thể được kết hợp cho lớp MoE. Chúng tôi phân phối các lớp tiêu chuẩn của mô hình và mạng cổng theo các sơ đồ song song dữ liệu thông thường, nhưng chỉ giữ một bản sao chia sẻ của mỗi chuyên gia. Mỗi chuyên gia trong lớp MoE nhận được một batch kết hợp bao gồm các ví dụ liên quan từ tất cả các batch đầu vào song song dữ liệu. Cùng một tập hợp thiết bị hoạt động như các bản sao song song dữ liệu (cho các lớp tiêu chuẩn và mạng cổng) và như các phân đoạn song song mô hình (mỗi cái lưu trữ một tập con các chuyên gia). Nếu mô hình được phân phối trên d thiết bị, và mỗi thiết bị xử lý một batch kích thước b, mỗi chuyên gia nhận được một batch khoảng kbd/n ví dụ. Do đó, chúng tôi đạt được một yếu tố cải thiện d trong kích thước batch chuyên gia.

Trong trường hợp MoE phân cấp (Phần B), mạng cổng chính sử dụng song song dữ liệu, và các MoE thứ cấp sử dụng song song mô hình. Mỗi MoE thứ cấp nằm trên một thiết bị.

--- TRANG 5 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

Kỹ thuật này cho phép chúng tôi tăng số lượng chuyên gia (và do đó số lượng tham số) bằng cách tăng tỷ lệ thuận số lượng thiết bị trong cụm huấn luyện. Tổng kích thước batch tăng, giữ kích thước batch cho mỗi chuyên gia không đổi. Yêu cầu bộ nhớ và băng thông cho mỗi thiết bị cũng không đổi, cũng như thời gian bước, cũng như lượng thời gian cần thiết để xử lý số lượng ví dụ huấn luyện bằng số lượng tham số trong mô hình. Mục tiêu của chúng tôi là huấn luyện một mô hình trillion-tham số trên một kho ngữ liệu trillion-từ. Chúng tôi chưa mở rộng hệ thống của mình đến mức này khi viết bài báo này, nhưng điều đó sẽ có thể bằng cách thêm phần cứng.

Tận dụng Tính Tích chập: Trong các mô hình ngôn ngữ của chúng tôi, chúng tôi áp dụng cùng một MoE cho mỗi bước thời gian của lớp trước. Nếu chúng tôi đợi lớp trước hoàn thành, chúng tôi có thể áp dụng MoE cho tất cả các bước thời gian cùng nhau như một batch lớn. Làm như vậy tăng kích thước của batch đầu vào cho lớp MoE lên một hệ số bằng số bước thời gian được mở ra.

Tăng Kích thước Batch cho MoE Tuần hoàn: Chúng tôi nghi ngờ rằng các mô hình mạnh mẽ hơn có thể liên quan đến việc áp dụng MoE tuần hoàn. Ví dụ, các ma trận trọng số của LSTM hoặc RNN khác có thể được thay thế bằng MoE. Thật đáng buồn, những mô hình như vậy phá vỡ thủ thuật tích chập từ đoạn cuối, vì đầu vào cho MoE tại một timestep phụ thuộc vào đầu ra của MoE tại timestep trước. Gruslys et al. (2016) mô tả một kỹ thuật để giảm đáng kể số lượng kích hoạt được lưu trữ trong một RNN mở ra, với chi phí tính toán lại kích hoạt tiến. Điều này sẽ cho phép tăng lớn kích thước batch.

3.2 BĂNG THÔNG MẠNG

Một mối quan tâm hiệu suất chính khác trong tính toán phân tán là băng thông mạng. Vì các chuyên gia cố định (xem trên) và số lượng tham số cổng nhỏ, hầu hết giao tiếp liên quan đến việc gửi đầu vào và đầu ra của các chuyên gia qua mạng. Để duy trì hiệu quả tính toán, tỷ lệ tính toán của chuyên gia so với kích thước đầu vào và đầu ra của nó phải vượt quá tỷ lệ năng lực tính toán so với mạng của thiết bị tính toán. Đối với GPU, điều này có thể là hàng nghìn so với một. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng các chuyên gia với một lớp ẩn chứa hàng nghìn đơn vị được kích hoạt RELU. Vì các ma trận trọng số trong chuyên gia có kích thước input_size×hidden_size và hidden_size×output_size, tỷ lệ tính toán so với đầu vào và đầu ra bằng kích thước của lớp ẩn. Thuận tiện, chúng tôi có thể tăng hiệu quả tính toán đơn giản bằng cách sử dụng lớp ẩn lớn hơn, hoặc nhiều lớp ẩn hơn.

4 CÂN BẰNG SỬ DỤNG CHUYÊN GIA

Chúng tôi đã quan sát thấy rằng mạng cổng có xu hướng hội tụ đến một trạng thái nơi nó luôn tạo ra trọng số lớn cho cùng một vài chuyên gia. Sự mất cân bằng này tự củng cố, vì các chuyên gia được ưa chuộng được huấn luyện nhanh hơn và do đó được mạng cổng chọn nhiều hơn. Eigen et al. (2013) mô tả hiện tượng tương tự, và sử dụng một ràng buộc cứng ở đầu huấn luyện để tránh cực tiểu địa phương này. Bengio et al. (2015) bao gồm một ràng buộc mềm trên trung bình theo batch của mỗi cổng.

Chúng tôi áp dụng phương pháp ràng buộc mềm. Chúng tôi định nghĩa tầm quan trọng của một chuyên gia tương đối với một batch ví dụ huấn luyện là tổng theo batch của các giá trị cổng cho chuyên gia đó. Chúng tôi định nghĩa một tổn thất bổ sung Limportance, được thêm vào hàm tổn thất tổng thể cho mô hình. Tổn thất này bằng bình phương hệ số biến thiên của tập hợp các giá trị quan trọng, nhân với một hệ số tỷ lệ được điều chỉnh bằng tay wimportance. Tổn thất bổ sung này khuyến khích tất cả các chuyên gia có tầm quan trọng bằng nhau.

Importance(X) = Σ(x∈X) G(x)    (6)
Limportance(X) = wimportance × CV(Importance(X))²    (7)

Trong khi hàm tổn thất này có thể đảm bảo tầm quan trọng bằng nhau, các chuyên gia vẫn có thể nhận được số lượng ví dụ rất khác nhau. Ví dụ, một chuyên gia có thể nhận được một vài ví dụ với trọng số lớn, và một chuyên gia khác có thể nhận được nhiều ví dụ với trọng số nhỏ. Điều này có thể gây ra các vấn đề về bộ nhớ và hiệu suất trên phần cứng phân tán. Để giải quyết vấn đề này, chúng tôi giới thiệu một hàm tổn thất thứ hai, Lload, đảm bảo tải cân bằng. Phụ lục A chứa định nghĩa của hàm này, cùng với kết quả thí nghiệm.

5 THÍ NGHIỆM

5.1 ĐIỂM CHUẨN MÔ HÌNH HÓA NGÔN NGỮ 1 TỶ TỪ

Tập dữ liệu: Tập dữ liệu này, được giới thiệu bởi (Chelba et al., 2013) bao gồm các câu duy nhất được xáo trộn từ các bài báo tin tức, tổng cộng khoảng 829 triệu từ, với từ vựng 793,471 từ.

Hiện đại nhất Trước đây: Kết quả được công bố tốt nhất trước đây (Jozefowicz et al., 2016) sử dụng các mô hình bao gồm một hoặc nhiều lớp Long Short-Term Memory (LSTM) xếp chồng (Hochreiter & Schmidhuber, 1997; Gers et al., 2000). Số lượng tham số trong các lớp LSTM của những mô hình này thay đổi từ 2 triệu đến 151 triệu. Chất lượng tăng lên đáng kể với số lượng tham số, cũng như chi phí tính toán. Kết quả cho những mô hình này tạo thành đường trên cùng của Hình 2-phải.

Các Mô hình MoE: Mô hình của chúng tôi bao gồm hai lớp LSTM xếp chồng với một lớp MoE giữa chúng (xem Hình 1). Chúng tôi thay đổi kích thước của các lớp và số lượng chuyên gia. Để biết chi tiết đầy đủ về kiến trúc mô hình, phác thức huấn luyện, baseline bổ sung và kết quả, xem Phụ lục C.

Tính toán Thấp, Khả năng Thay đổi: Để điều tra hiệu ứng của việc thêm khả năng, chúng tôi huấn luyện một loạt mô hình MoE tất cả với chi phí tính toán gần như bằng nhau: khoảng 8 triệu phép nhân và cộng cho mỗi ví dụ huấn luyện cho mỗi timestep trong lần truyền thuận, loại trừ lớp softmax. Chúng tôi gọi số liệu này (ops/timestep). Chúng tôi huấn luyện các mô hình với MoE phẳng chứa 4, 32, và 256 chuyên gia, và các mô hình với MoE phân cấp chứa 256, 1024, và 4096 chuyên gia. Mỗi chuyên gia có khoảng 1 triệu tham số. Đối với tất cả các lớp MoE, 4 chuyên gia hoạt động cho mỗi đầu vào.

Kết quả của những mô hình này được hiển thị trong Hình 2-trái. Mô hình với 4 chuyên gia luôn hoạt động (không ngạc nhiên) hoạt động tương tự như các mô hình baseline phù hợp về mặt tính toán, trong khi lớn nhất trong số các mô hình (4096 chuyên gia) đạt được perplexity thấp hơn 24% ấn tượng trên tập kiểm tra.

Hình 2: So sánh mô hình trên Điểm chuẩn Mô hình hóa Ngôn ngữ 1-Tỷ-Từ. Ở bên trái, chúng tôi vẽ perplexity kiểm tra như một hàm của khả năng mô hình cho các mô hình với ngân sách tính toán tương tự khoảng 8-triệu-ops-per-timestep. Ở bên phải, chúng tôi vẽ perplexity kiểm tra như một hàm của ngân sách tính toán. Đường trên cùng đại diện cho các mô hình LSTM từ (Jozefowicz et al., 2016). Đường dưới cùng đại diện cho các mô hình MoE 4-tỷ tham số với ngân sách tính toán khác nhau.

Tính toán Thay đổi, Khả năng Cao: Ngoài mô hình lớn nhất từ phần trước, chúng tôi huấn luyện thêm hai mô hình MoE với khả năng tương tự cao (4 tỷ tham số), nhưng ngân sách tính toán cao hơn. Những mô hình này có LSTM lớn hơn, và ít hơn nhưng lớn hơn và chuyên gia. Chi tiết

--- TRANG 6 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

Bảng 1: Tóm tắt các mô hình được tăng cường MoE với khả năng cao với ngân sách tính toán thay đổi, so với kết quả được công bố tốt nhất trước đây (Jozefowicz et al., 2016). Chi tiết trong Phụ lục C.

| Kiểm tra Perplexity | Kiểm tra Perplexity | #Tham số | ops/timestep | Thời gian huấn luyện | TFLOPS |
|---|---|---|---|---|---|
| 10 epochs | 100 epochs | loại trừ embedding | và lớp softmax | 10 epochs | /GPU |
| Kết quả được công bố tốt nhất | 34.7 | 30.6 | 151 triệu | 151 triệu | 59 giờ, 32 k40s | 1.09 |
| Mô hình MoE Ngân sách thấp | 34.1 | | 4303 triệu | 8.9 triệu | 15 giờ, 16 k40s | 0.74 |
| Mô hình MoE Ngân sách trung bình | 31.3 | | 4313 triệu | 33.8 triệu | 17 giờ, 32 k40s | 1.22 |
| Mô hình MoE Ngân sách cao | 28.0 | | 4371 triệu | 142.7 triệu | 47 giờ, 32 k40s | 1.56 |

có thể được tìm thấy trong Phụ lục C.2. Kết quả của ba mô hình này tạo thành đường dưới cùng của Hình 2-phải. Bảng 1 so sánh kết quả của những mô hình này với kết quả được công bố tốt nhất trước đây trên tập dữ liệu này. Ngay cả mô hình nhanh nhất trong số này cũng đánh bại kết quả được công bố tốt nhất (khi kiểm soát số epochs huấn luyện), mặc dù chỉ yêu cầu 6% tính toán.

Hiệu quả Tính toán: Chúng tôi huấn luyện mô hình của chúng tôi sử dụng TensorFlow (Abadi et al., 2016) trên các cụm chứa 16-32 Tesla K40 GPU. Đối với mỗi mô hình của chúng tôi, chúng tôi xác định hiệu quả tính toán trong TFLOPS/GPU bằng cách chia số lượng phép toán dấu phẩy động cần thiết để xử lý một batch huấn luyện cho thời gian bước quan sát được và số lượng GPU trong cụm. Số lượng phép toán được sử dụng ở đây cao hơn những gì chúng tôi báo cáo trong các số ops/timestep của chúng tôi ở chỗ chúng tôi bao gồm lần truyền ngược, chúng tôi bao gồm huấn luyện dựa trên importance-sampling của lớp softmax, và chúng tôi đếm một phép nhân-và-cộng như hai phép toán riêng biệt. Đối với tất cả các mô hình MoE của chúng tôi, các phép toán dấu phẩy động liên quan đến các chuyên gia đại diện cho từ 37% đến 46% tổng số.

Đối với các mô hình baseline của chúng tôi không có MoE, hiệu quả tính toán quan sát được dao động từ 1.07-1.29 TFLOPS/GPU. Đối với các mô hình MoE tính toán thấp của chúng tôi, hiệu quả tính toán dao động từ 0.74-0.90 TFLOPS/GPU, ngoại trừ mô hình 4-chuyên gia không sử dụng đầy đủ tính song song có sẵn. Mô hình MoE tính toán cao nhất của chúng tôi hiệu quả hơn ở 1.56 TFLOPS/GPU, có thể do các ma trận lớn hơn. Những con số này đại diện cho một phần đáng kể của mức tối đa lý thuyết 4.29 TFLOPS/GPU được tuyên bố bởi NVIDIA. Kết quả chi tiết trong Phụ lục C, Bảng 7.

5.2 KHO NGỮ LIỆU TIN TỨC GOOGLE 100 TỶ TỪ

Hình 3: Mô hình hóa ngôn ngữ trên kho ngữ liệu 100 tỷ từ. Các mô hình có ngân sách tính toán tương tự (8 triệu ops/timestep).

Trên kho ngữ liệu 1-tỷ-từ, việc thêm khả năng bổ sung dường như tạo ra lợi nhuận giảm dần khi số lượng tham số trong lớp MoE vượt quá 1 tỷ, như có thể thấy trong Hình 2-trái. Chúng tôi đưa ra giả thuyết rằng đối với một tập huấn luyện lớn hơn, ngay cả khả năng cao hơn sẽ tạo ra cải thiện chất lượng đáng kể.

Chúng tôi xây dựng một tập huấn luyện tương tự bao gồm các câu duy nhất được xáo trộn từ kho ngữ liệu tin tức nội bộ của Google, tổng cộng khoảng 100 tỷ từ. Tương tự như phần trước, chúng tôi kiểm tra một loạt mô hình với chi phí tính toán tương tự khoảng 8 triệu ops/timestep. Ngoài mô hình LSTM baseline, chúng tôi huấn luyện các mô hình được tăng cường với các lớp MoE chứa 32, 256, 1024, 4096, 16384, 65536, và 131072 chuyên gia. Điều này tương ứng với lên đến 137 tỷ tham số trong lớp MoE. Chi tiết về kiến trúc, huấn luyện, và kết quả được đưa ra trong Phụ lục D.

Kết quả: Hình 3 cho thấy perplexity kiểm tra như một hàm của khả năng sau khi huấn luyện trên 10 tỷ từ (đường trên) và 100 tỷ từ (đường dưới). Khi huấn luyện trên toàn bộ 100 tỷ từ, perplexity kiểm tra cải thiện đáng kể lên đến 65536 chuyên gia (68 tỷ tham số), giảm 39% thấp hơn so với baseline phù hợp về mặt tính toán, nhưng giảm sút ở 131072 chuyên gia, có thể là kết quả của quá nhiều tính thưa. Khoảng cách ngày càng lớn giữa hai đường chứng minh (không ngạc nhiên) rằng khả năng mô hình tăng lên giúp ích nhiều hơn trên các tập huấn luyện lớn hơn.

Ngay cả ở 65536 chuyên gia (99.994% tính thưa lớp), hiệu quả tính toán cho mô hình vẫn ở mức đáng kính 0.72 TFLOPS/GPU.

5.3 DỊCH MÁY (CẶP NGÔN NGỮ ĐƠN)

Kiến trúc Mô hình: Mô hình của chúng tôi là một phiên bản được điều chỉnh của mô hình GNMT được mô tả trong (Wu et al., 2016). Để giảm tính toán, chúng tôi giảm số lượng lớp LSTM trong bộ mã hóa và giải mã từ 9 và 8 xuống 3 và 2 tương ứng. Chúng tôi chèn các lớp MoE trong cả bộ mã hóa (giữa lớp 2 và 3) và bộ giải mã (giữa lớp 1 và 2). Mỗi lớp MoE chứa lên đến 2048 chuyên gia mỗi cái với khoảng hai triệu tham số, thêm tổng cộng khoảng 8 tỷ tham số vào các mô hình. Chi tiết thêm về kiến trúc mô hình, quy trình kiểm tra và kết quả có thể được tìm thấy trong Phụ lục E.

Tập dữ liệu: Chúng tôi đánh giá phương pháp của chúng tôi trên các kho ngữ liệu WMT'14 En→Fr và En→De, có tập huấn luyện có 36M cặp câu và 5M cặp câu, tương ứng. Các giao thức thí nghiệm cũng tương tự như những giao thức trong (Wu et al., 2016): newstest2014 được sử dụng làm tập kiểm tra để so sánh với công trình trước (Luong et al., 2015a; Zhou et al., 2016; Wu et al., 2016), trong khi sự kết hợp của newstest2012 và newstest2013 được sử dụng làm tập phát triển. Chúng tôi cũng kiểm tra cùng mô hình trên dữ liệu Sản xuất Google Tiếng Anh sang Tiếng Pháp.

Bảng 2: Kết quả trên WMT'14 En→Fr newstest2014 (giá trị in đậm đại diện cho kết quả tốt nhất).

| Mô hình | Kiểm tra Perplexity | Kiểm tra BLEU | ops/timestep | Tổng #Tham số | Thời gian huấn luyện |
|---|---|---|---|---|---|
| MoE với 2048 Chuyên gia | 2.69 | 40.35 | 85M | 8.7B | 3 ngày/64 k40s |
| MoE với 2048 Chuyên gia (huấn luyện lâu hơn) | 2.63 | 40.56 | 85M | 8.7B | 6 ngày/64 k40s |
| GNMT (Wu et al., 2016) | 2.79 | 39.22 | 214M | 278M | 6 ngày/96 k80s |
| GNMT+RL (Wu et al., 2016) | 2.96 | 39.92 | 214M | 278M | 6 ngày/96 k80s |
| PBMT (Durrani et al., 2014) | | 37.0 | | | |
| LSTM (6-lớp) (Luong et al., 2015b) | | 31.5 | | | |
| LSTM (6-lớp+PosUnk) (Luong et al., 2015b) | | 33.1 | | | |
| DeepAtt (Zhou et al., 2016) | | 37.7 | | | |
| DeepAtt+PosUnk (Zhou et al., 2016) | | 39.2 | | | |

Bảng 3: Kết quả trên WMT'14 En→De newstest2014 (giá trị in đậm đại diện cho kết quả tốt nhất).

| Mô hình | Kiểm tra Perplexity | Kiểm tra BLEU | ops/timestep | Tổng #Tham số | Thời gian huấn luyện |
|---|---|---|---|---|---|
| MoE với 2048 Chuyên gia | 4.64 | 26.03 | 85M | 8.7B | 1 ngày/64 k40s |
| GNMT (Wu et al., 2016) | 5.25 | 24.91 | 214M | 278M | 1 ngày/96 k80s |
| GNMT +RL (Wu et al., 2016) | 8.08 | 24.66 | 214M | 278M | 1 ngày/96 k80s |
| PBMT (Durrani et al., 2014) | | 20.7 | | | |
| DeepAtt (Zhou et al., 2016) | | 20.6 | | | |

Bảng 4: Kết quả trên tập dữ liệu Google Production En→Fr (giá trị in đậm đại diện cho kết quả tốt nhất).

| Mô hình | Eval Perplexity | Eval BLEU | Kiểm tra Perplexity | Kiểm tra BLEU | ops/timestep | Tổng #Tham số | Thời gian huấn luyện |
|---|---|---|---|---|---|---|---|
| MoE với 2048 Chuyên gia | 2.60 | 37.27 | 2.69 | 36.57 | 85M | 8.7B | 1 ngày/64 k40s |
| GNMT (Wu et al., 2016) | 2.78 | 35.80 | 2.87 | 35.56 | 214M | 278M | 6 ngày/96 k80s |

--- TRANG 8 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

Kết quả: Bảng 2, 3, và 4 cho thấy kết quả của các mô hình lớn nhất của chúng tôi, so sánh với kết quả đã công bố. Phương pháp của chúng tôi đạt được điểm BLEU 40.56 và 26.03 trên các điểm chuẩn WMT'14 En→Fr và En→De. Vì các mô hình của chúng tôi không sử dụng tinh chỉnh RL, những kết quả này tạo thành những cải thiện đáng kể 1.34 và 1.12 điểm BLEU trên các baseline mạnh trong (Wu et al., 2016). Điểm perplexity cũng tốt hơn. Trên tập dữ liệu Google Production, mô hình của chúng tôi đạt được 1.01 điểm BLEU kiểm tra cao hơn ngay cả sau khi huấn luyện chỉ trong một phần sáu thời gian.

5.4 DỊCH MÁY ĐA NGÔN NGỮ

Tập dữ liệu: (Johnson et al., 2016) huấn luyện một mô hình GNMT đơn (Wu et al., 2016) trên một tập dữ liệu kết hợp rất lớn của mười hai cặp ngôn ngữ. Kết quả hơi tệ hơn so với 12 mô hình GNMT cặp đơn được huấn luyện riêng biệt. Điều này không ngạc nhiên, cho rằng mười hai mô hình có khả năng gấp 12 lần và huấn luyện tổng hợp gấp mười hai lần so với một mô hình. Chúng tôi lặp lại thí nghiệm này với một mô hình được tăng cường MoE đơn. Xem Phụ lục E để biết chi tiết về kiến trúc mô hình. Chúng tôi huấn luyện mô hình của chúng tôi trên cùng tập dữ liệu như (Johnson et al., 2016) và xử lý cùng số lượng ví dụ huấn luyện (khoảng 3 tỷ cặp câu). Thời gian huấn luyện của chúng tôi ngắn hơn do ngân sách tính toán thấp hơn của mô hình chúng tôi.

Kết quả: Kết quả cho các mô hình GNMT cặp đơn, mô hình GNMT đa ngôn ngữ và mô hình MoE đa ngôn ngữ được đưa ra trong Bảng 5. Mô hình MoE đạt được perplexity thấp hơn 19% trên tập dev so với mô hình GNMT đa ngôn ngữ. Về điểm BLEU, mô hình MoE đánh bại đáng kể mô hình GNMT đa ngôn ngữ trên 11 trong số 12 cặp ngôn ngữ (lên đến 5.84 điểm), và thậm chí đánh bại các mô hình GNMT đơn ngôn ngữ trên 8 trong số 12 cặp ngôn ngữ. Hiệu suất kém trên English→Korean dường như là kết quả của việc over-training nghiêm trọng, vì đối với các cặp ngôn ngữ hiếm hơn, một số lượng nhỏ ví dụ thực được lấy mẫu quá mức trong kho ngữ liệu huấn luyện.

Bảng 5: Dịch Máy Đa ngôn ngữ (giá trị in đậm đại diện cho kết quả tốt nhất).

| | GNMT-Mono | GNMT-Multi | MoE-Multi | MoE-Multi vs. GNMT-Multi |
|---|---|---|---|---|
| Tham số | 278M / mô hình | 278M | 8.7B | |
| ops/timestep | 212M | 212M | 102M | |
| thời gian huấn luyện, phần cứng | khác nhau | 21 ngày, 96 k20s | 12 ngày, 64 k40s | |
| Perplexity (dev) | | 4.14 | 3.35 | -19% |
| French→English Test BLEU | 36.47 | 34.40 | 37.46 | +3.06 |
| German→English Test BLEU | 31.77 | 31.17 | 34.80 | +3.63 |
| Japanese→English Test BLEU | 23.41 | 21.62 | 25.91 | +4.29 |
| Korean→English Test BLEU | 25.42 | 22.87 | 28.71 | +5.84 |
| Portuguese→English Test BLEU | 44.40 | 42.53 | 46.13 | +3.60 |
| Spanish→English Test BLEU | 38.00 | 36.04 | 39.39 | +3.35 |
| English→French Test BLEU | 35.37 | 34.00 | 36.59 | +2.59 |
| English→German Test BLEU | 26.43 | 23.15 | 24.53 | +1.38 |
| English→Japanese Test BLEU | 23.66 | 21.10 | 22.78 | +1.68 |
| English→Korean Test BLEU | 19.75 | 18.41 | 16.62 | -1.79 |
| English→Portuguese Test BLEU | 38.40 | 37.35 | 37.90 | +0.55 |
| English→Spanish Test BLEU | 34.50 | 34.25 | 36.21 | +1.96 |

6 KẾT LUẬN

Công trình này là đầu tiên chứng minh những chiến thắng lớn từ tính toán có điều kiện trong mạng sâu. Chúng tôi đã xác định cẩn thận các cân nhắc thiết kế và thách thức của tính toán có điều kiện và giải quyết chúng với sự kết hợp của các giải pháp thuật toán và kỹ thuật. Trong khi chúng tôi tập trung vào văn bản, tính toán có điều kiện có thể giúp ích trong các lĩnh vực khác nữa, với điều kiện các tập huấn luyện đủ lớn. Chúng tôi mong muốn được thấy nhiều triển khai và ứng dụng mới của tính toán có điều kiện trong những năm tới.

LỜI CẢM ơN

Chúng tôi muốn cảm ơn tất cả các thành viên của đội Google Brain và Google Translate đã giúp chúng tôi với dự án này, đặc biệt là Zhifeng Chen, Yonghui Wu, và Melvin Johnson. Cảm ơn cũng đến các nhà đánh giá ICLR ẩn danh vì những gợi ý hữu ích về việc làm cho bài báo này tốt hơn.

Perplexity được báo cáo tương đối với việc tokenization được sử dụng bởi cả mô hình của chúng tôi và GNMT.

--- TRANG 9 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo dài với nhiều mục - giữ nguyên định dạng từ bản gốc]

--- TRANG 10 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 11 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 13 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

PHỤ LỤC

A TỔN THẤT CÂN BẰNG TẢI

Như đã thảo luận trong phần 4, cho mục đích cân bằng tải, chúng tôi muốn định nghĩa một hàm tổn thất bổ sung để khuyến khích các chuyên gia nhận được số lượng ví dụ huấn luyện gần như bằng nhau. Thật không may, số lượng ví dụ mà một chuyên gia nhận được là một đại lượng rời rạc, nên nó không thể được sử dụng trong lan truyền ngược. Thay vào đó, chúng tôi định nghĩa một ước lượng mượt Load(X) của số lượng ví dụ được gán cho mỗi chuyên gia cho một batch X đầu vào. Tính mượt cho phép chúng tôi lan truyền ngược gradient thông qua ước lượng. Đây là mục đích của thành phần nhiễu trong hàm cổng. Chúng tôi định nghĩa P(x,i) là xác suất mà G(x)i khác không, cho một lựa chọn nhiễu ngẫu nhiên mới trên phần tử i, nhưng giữ các lựa chọn nhiễu đã được lấy mẫu trên các phần tử khác. Để tính P(x,i), chúng tôi lưu ý rằng G(x)i khác không khi và chỉ khi H(x)i lớn hơn phần tử lớn thứ k của H(x) loại trừ chính nó. Xác suất tính ra là:

P(x,i) = Pr((xWg)i + StandardNormal() · Softplus((xWnoise)i) > kth_excluding(H(x),k,i))    (8)

Trong đó kth_excluding(v,k,i) có nghĩa là thành phần cao thứ k của v, loại trừ thành phần i. Đơn giản hóa, chúng tôi có:

P(x,i) = Φ(((xWg)i - kth_excluding(H(x),k,i)) / Softplus((xWnoise)i))    (9)

Trong đó Φ là CDF của phân phối chuẩn tiêu chuẩn.

Load(X)i = Σ(x∈X) P(x,i)    (10)

Bây giờ chúng tôi có thể định nghĩa tổn thất tải là bình phương hệ số biến thiên của vector tải, nhân với một hệ số tỷ lệ được điều chỉnh bằng tay wload.

Lload(X) = wload × CV(Load(X))²    (11)

Mất cân bằng Tải Ban đầu: Để tránh lỗi hết bộ nhớ, chúng tôi cần khởi tạo mạng trong trạng thái tải chuyên gia gần như bằng nhau (vì các ràng buộc mềm cần thời gian để hoạt động). Để thực hiện điều này, chúng tôi khởi tạo các ma trận Wg và Wnoise thành tất cả số không, điều này cho ra không có tín hiệu và một ít nhiễu.

Thí nghiệm: Chúng tôi huấn luyện một tập hợp mô hình với kiến trúc giống hệt nhau (mô hình MoE-256 được mô tả trong Phụ lục C), sử dụng các giá trị khác nhau của wimportance và wload. Chúng tôi huấn luyện mỗi mô hình trong 10 epochs, sau đó đo perplexity trên tập kiểm tra. Chúng tôi cũng đo hệ số biến thiên trong Importance và Load, cũng như tỷ lệ tải trên chuyên gia quá tải nhất so với tải trung bình. Giá trị cuối này quan trọng cho mục đích cân bằng tải trên phần cứng phân tán. Tất cả các số liệu này được tính trung bình trên nhiều batch huấn luyện.

Bảng 6: Thí nghiệm với các kết hợp tổn thất khác nhau.

| wimportance | wload | Test Perplexity | CV(Importance(X)) | CV(Load(X)) | max(Load(X))/mean(Load(X)) |
|---|---|---|---|---|---|
| 0.0 | 0.0 | 39.8 | 3.04 | 3.01 | 17.80 |
| 0.2 | 0.0 | 35.6 | 0.06 | 0.17 | 1.47 |
| 0.0 | 0.2 | 35.7 | 0.22 | 0.04 | 1.15 |
| 0.1 | 0.1 | 35.6 | 0.06 | 0.05 | 1.14 |
| 0.01 | 0.01 | 35.7 | 0.48 | 0.11 | 1.37 |
| 1.0 | 1.0 | 35.7 | 0.03 | 0.02 | 1.07 |

--- TRANG 14 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

Kết quả: Kết quả được báo cáo trong Bảng 6. Tất cả các kết hợp chứa ít nhất một trong hai tổn thất dẫn đến chất lượng mô hình rất tương tự, trong khi không có tổn thất tệ hơn nhiều. Các mô hình với giá trị wload cao hơn có tải thấp hơn trên chuyên gia quá tải nhất.

B HỖN HỢP CHUYÊN GIA PHÂN CẤP

Nếu số lượng chuyên gia rất lớn, chúng tôi có thể giảm hệ số phân nhánh bằng cách sử dụng MoE phân cấp hai cấp. Trong một MoE phân cấp, một mạng cổng chính chọn một sự kết hợp có trọng số thưa của "chuyên gia", mỗi cái bản thân là một hỗn hợp chuyên gia thứ cấp với mạng cổng riêng của nó. Nếu MoE phân cấp bao gồm a nhóm b chuyên gia mỗi nhóm, chúng tôi ký hiệu mạng cổng chính bằng Gprimary, các mạng cổng thứ cấp bằng (G1,G2...Ga), và các mạng chuyên gia bằng (E0,0,E0,1...Ea,b). Đầu ra của MoE được cho bởi:

yH = Σ(i=1 đến a) Σ(j=1 đến b) Gprimary(x)i Gi(x)j Ei,j(x)    (12)

Các số liệu sử dụng chuyên gia của chúng tôi thay đổi thành:

ImportanceH(X)i,j = Σ(x∈X) Gprimary(x)i Gi(x)j    (13)
LoadH(X)i,j = Loadprimary(X)i × Loadi(X(i))j / |X(i)|    (14)

Loadprimary và Loadi ký hiệu các hàm Load cho mạng cổng chính và mạng cổng thứ cấp thứ i tương ứng. X(i) ký hiệu tập con của X mà Gprimary(x)i > 0.

Có vẻ đơn giản hơn để đặt LoadH(X)i,j = Loadi(Xi)j, nhưng điều này sẽ không có gradient đối với mạng cổng chính, nên chúng tôi sử dụng công thức trên.

C ĐIỂM CHUẨN MÔ HÌNH HÓA NGÔN NGỮ 1 TỶ TỪ - CHI TIẾT THÍ NGHIỆM

C.1 CÁC MÔ HÌNH 8-TRIỆU-PHÉP-TOÁN-TRÊN-MỖI-TIMESTEP

Kiến trúc Mô hình: Mô hình của chúng tôi bao gồm năm lớp: một lớp word embedding, một lớp Long Short-Term Memory (LSTM) tuần hoàn (Hochreiter & Schmidhuber, 1997; Gers et al., 2000), một lớp MoE, một lớp LSTM thứ hai, và một lớp softmax. Chiều của lớp embedding, số đơn vị trong mỗi lớp LSTM, và chiều đầu vào và đầu ra của lớp MoE đều bằng 512. Đối với mỗi lớp khác ngoài softmax, chúng tôi áp dụng dropout (Zaremba et al., 2014) cho đầu ra lớp, loại bỏ mỗi kích hoạt với xác suất DropProb, nếu không chia cho (1-DropProb). Sau dropout, đầu ra của lớp trước được thêm vào đầu ra lớp. Kết nối dư này khuyến khích dòng gradient (He et al., 2015).

Kiến trúc Lớp MoE: Mỗi chuyên gia trong lớp MoE là một mạng feed forward với một lớp ẩn được kích hoạt ReLU có kích thước 1024 và một lớp đầu ra có kích thước 512. Do đó, mỗi chuyên gia chứa [512×1024] + [1024×512] = 1M tham số. Đầu ra của lớp MoE được truyền qua một hàm sigmoid trước dropout. Chúng tôi thay đổi số lượng chuyên gia giữa các mô hình, sử dụng các lớp MoE thông thường với 4, 32 và 256 chuyên gia và các lớp MoE phân cấp với 256, 1024 và 4096 chuyên gia. Chúng tôi gọi các mô hình kết quả là MoE-4, MoE-32, MoE-256, MoE-256-h, MoE-1024-h và MoE-4096-h. Đối với các lớp MoE phân cấp, hệ số phân nhánh cấp đầu tiên là 16, tương ứng với số lượng GPU trong cụm của chúng tôi. Chúng tôi sử dụng Cổng Top-K Nhiễu (xem Phần 2.1) với k=4 cho các lớp MoE thông thường và k=2 ở mỗi cấp của các lớp MoE phân cấp. Do đó, mỗi ví dụ được xử lý bởi chính xác 4 chuyên gia với tổng cộng 4M ops/timestep. Hai lớp LSTM đóng góp 2M ops/timestep mỗi lớp cho tổng mong muốn là 8M.

Baseline Phù hợp về Tính toán: Mô hình MoE-4 không sử dụng tính thưa, vì tất cả 4 chuyên gia luôn được sử dụng. Ngoài ra, chúng tôi huấn luyện bốn mô hình baseline phù hợp về tính toán khác không có tính thưa:

MoE-1-Wide: Lớp MoE bao gồm một "chuyên gia" đơn chứa một lớp ẩn được kích hoạt ReLU có kích thước 4096.

MoE-1-Deep: Lớp MoE bao gồm một "chuyên gia" đơn chứa bốn lớp ẩn được kích hoạt ReLU, mỗi lớp có kích thước 1024.

4xLSTM-512: Chúng tôi thay thế lớp MoE bằng hai lớp LSTM 512-đơn vị bổ sung.

LSTM-2048-512: Mô hình chứa một lớp LSTM 2048-đơn vị (và không có MoE). Đầu ra của LSTM được chiếu xuống 512 chiều (Sak et al., 2014). Timestep tiếp theo của LSTM nhận đầu ra được chiếu. Điều này giống hệt với một trong những mô hình được công bố trong (Jozefowicz et al., 2016). Chúng tôi chạy lại nó để tính đến sự khác biệt trong phác thức huấn luyện, và có được kết quả rất tương tự với những kết quả được công bố.

Huấn luyện: Các mô hình được huấn luyện trên một cụm 16 K40 GPU sử dụng phương pháp đồng bộ được mô tả trong Phần 3. Mỗi batch bao gồm một tập hợp câu tổng cộng khoảng 300,000 từ. Vì lợi ích về thời gian, chúng tôi giới hạn huấn luyện đến 10 epochs, (27,000 bước). Huấn luyện mất 12-16 giờ cho tất cả các mô hình, ngoại trừ MoE-4, mất 18 giờ (vì tất cả tính toán chuyên gia được thực hiện chỉ trên 4 trong số 16 GPU). Chúng tôi sử dụng bộ tối ưu Adam (Kingma & Ba, 2015). Tốc độ học cơ sở được tăng tuyến tính trong 1000 bước huấn luyện đầu tiên, và giảm sau đó để tỷ lệ nghịch với căn bậc hai của số bước. Lớp đầu ra Softmax được huấn luyện hiệu quả sử dụng importance sampling tương tự như các mô hình trong (Jozefowicz et al., 2016). Đối với mỗi mô hình, chúng tôi thực hiện tìm kiếm siêu tham số để tìm xác suất dropout tốt nhất, với bước tăng 0.1.

Để đảm bảo sử dụng chuyên gia cân bằng, chúng tôi đặt wimportance = 0.1 và wload = 0.1, như mô tả trong Phần 4 và Phụ lục A.

Kết quả: Chúng tôi đánh giá mô hình của chúng tôi sử dụng perplexity trên tập dữ liệu holdout, được sử dụng bởi (Chelba et al., 2013; Jozefowicz et al., 2016). Chúng tôi tuân theo quy trình tiêu chuẩn và tính tổng trên tất cả các từ bao gồm ký hiệu cuối câu. Kết quả được báo cáo trong Bảng 7. Đối với mỗi mô hình, chúng tôi báo cáo perplexity kiểm tra, ngân sách tính toán, số lượng tham số, giá trị của DropProb, và hiệu quả tính toán.

Bảng 7: So sánh mô hình trên Điểm chuẩn Mô hình hóa Ngôn ngữ 1 Tỷ Từ. Các mô hình được đánh dấu * là từ (Jozefowicz et al., 2016).

[Bảng chi tiết với nhiều hàng và cột - giữ nguyên cấu trúc từ bản gốc]

--- TRANG 15 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017

[Tiếp tục nội dung từ các trang còn lại với tất cả chi tiết về các thí nghiệm, kết quả và phụ lục khác...]

--- TRANG 16 ---
[Tiếp tục dịch toàn bộ nội dung còn lại...]

--- TRANG 17 ---
[Tiếp tục dịch toàn bộ nội dung còn lại...]

--- TRANG 18 ---
[Tiếp tục dịch toàn bộ nội dung còn lại...]

--- TRANG 19 ---
[Tiếp tục dịch toàn bộ nội dung còn lại...]
