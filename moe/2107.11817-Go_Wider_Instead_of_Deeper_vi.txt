# 2107.11817.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2107.11817.pdf
# Kích thước file: 2378435 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Mở rộng thay vì đi sâu hơn
Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, Yang You
Khoa Khoa học Máy tính, Đại học Quốc gia Singapore, Singapore
ff.xue,ziji.shig@u.nus.edu, weifutao2019@gmail.com, yuxuanlou@u.nus.edu, fliuyong,youyg@comp.nus.edu.sg
Tóm tắt
Nhiều khối transformer với kết nối dư gần đây đã đạt được kết quả ấn tượng trên các tác vụ khác nhau. Để đạt được hiệu suất tốt hơn với ít tham số có thể huấn luyện hơn, các phương pháp gần đây được đề xuất để giảm độ sâu bằng cách chia sẻ tham số hoặc nén mô hình theo chiều sâu. Tuy nhiên, khả năng mô hình hóa yếu làm hạn chế hiệu suất của chúng. Ngược lại, việc mở rộng theo chiều rộng bằng cách thêm nhiều ma trận và tham số có thể huấn luyện sẽ tạo ra một mô hình khổng lồ đòi hỏi song song hóa tiên tiến để huấn luyện và suy luận.

Trong bài báo này, chúng tôi đề xuất một khung hiệu quả về tham số, mở rộng thay vì đi sâu hơn. Cụ thể, theo các nghiên cứu hiện có, chúng tôi áp dụng chia sẻ tham số để nén theo chiều sâu. Tuy nhiên, việc triển khai như vậy sẽ hạn chế hiệu suất. Để tối đa hóa khả năng mô hình hóa, chúng tôi mở rộng theo chiều rộng mô hình bằng cách thay thế mạng truyền thẳng (FFN) bằng hỗn hợp các chuyên gia (MoE). Qua các khối transformer, thay vì chia sẻ các lớp chuẩn hóa, chúng tôi đề xuất sử dụng các layernorm riêng lẻ để biến đổi các biểu diễn ngữ nghĩa khác nhau theo cách hiệu quả về tham số hơn. Để đánh giá khung có thể cắm và chạy của chúng tôi, chúng tôi thiết kế WideNet và tiến hành các thí nghiệm toàn diện trên các bộ đánh giá thị giác máy tính và xử lý ngôn ngữ tự nhiên phổ biến. Trên ImageNet-1K, mô hình tốt nhất của chúng tôi vượt trội hơn Vision Transformer (ViT) 1,5% với 0,72× tham số có thể huấn luyện. Sử dụng 0,46× và 0,13× tham số, WideNet của chúng tôi vẫn có thể vượt qua ViT và ViT-MoE lần lượt 0,8% và 2,1%. Trên bốn bộ dữ liệu xử lý ngôn ngữ tự nhiên, WideNet vượt trội hơn ALBERT 1,8% trung bình và vượt qua BERT sử dụng tham số hóa nhúng nhân tử 0,8% với ít tham số hơn.¹

Giới thiệu
Các mô hình dựa trên Transformer đã đạt được kết quả đầy hứa hẹn trên các tác vụ khác nhau (ví dụ: Q&A (Qu et al. 2019; Yang et al. 2020), trích xuất quan hệ (Xue et al. 2020b,a; Zhou et al. 2020)). Để cải thiện thêm tính hiệu quả và hiệu suất của transformer, có hai hướng suy nghĩ để triển khai các tham số có thể huấn luyện. Hướng suy nghĩ đầu tiên là mở rộng transformer theo chiều rộng đến nhiều tham số có thể huấn luyện hơn (ví dụ: Switch Transformer (Fedus, Zoph, and Shazeer 2021), ViT-MoE (Riquelme et al. 2021)). Các mô hình thưa thớt này có thể mở rộng đến các mô hình cực kỳ lớn với FLOPs tương đương thông qua tính toán có điều kiện thưa thớt. Hướng suy nghĩ khác là giảm các tham số có thể huấn luyện cho một mô hình nhẹ. Để đạt được điều này, một số nghiên cứu đề xuất tái sử dụng các tham số có thể huấn luyện qua các khối transformer (ví dụ: Universal Transformer (Dehghani et al. 2018) và ALBERT (Lan et al. 2019)). Nén mô hình (Xu et al. 2020; Sun et al. 2019) cũng có thể làm cho transformer hiệu quả về tham số hơn.

¹Chúng tôi sẽ phát hành mã của mình khi được chấp nhận.

Hai phương pháp hiện có đều có những hạn chế riêng. Đối với các mô hình khổng lồ, một phương pháp điển hình và hiệu quả để mở rộng các tham số có thể huấn luyện là thay thế một phần của lớp mạng truyền thẳng (FFN) trong các khối transformer bằng các lớp MoE. Trong mỗi lớp MoE, để tinh chỉnh một biểu diễn token đơn, chỉ một vài chuyên gia được kích hoạt, vì vậy transformer dựa trên MoE có FLOPs tương đương với transformer thường. Tuy nhiên, trong quá trình huấn luyện và suy luận, chúng ta được yêu cầu sử dụng song song hóa tiên tiến (ví dụ: tensor (Shoeybi et al. 2019), sequence (Li et al. 2021), pipeline (Huang et al. 2018) và expert parallelism (Lepikhin et al. 2020)) để giữ các mô hình này trên TPU hoặc GPU. Ngoài ra, hiệu suất không thể cải thiện tuyến tính trong quá trình mở rộng. Một hạn chế khác là tính thưa thớt của các mô hình dựa trên MoE không thể mở rộng tốt trên các bộ dữ liệu tương đối nhỏ. Chúng tôi sẽ thảo luận lý do cho hiện tượng này trong các phần sau. Đối với các mô hình nhỏ, mặc dù chúng có thể giảm đáng kể các tham số có thể huấn luyện bằng cách giảm độ sâu, hiệu suất của các mô hình nông hơn này vẫn dưới các transformer gốc. Các mô hình nhỏ hơn này được xây dựng bằng cách nén mô hình gốc theo chiều sâu nên tất cả các khối transformer chia sẻ cùng một kiến thức. Cấu trúc như vậy gây ra sự mất mát không thể tránh khỏi của khả năng mô hình.

Trong bài báo này, chúng tôi trình bày một khung triển khai tham số triển khai các tham số có thể huấn luyện hiệu quả hơn: mở rộng thay vì đi sâu hơn. Sau đó, chúng tôi triển khai nó trên transformer và đặt tên là WideNet. Cụ thể, trước tiên chúng tôi sử dụng chia sẻ tham số theo chiều sâu để giảm độ sâu. Do mất mát khả năng mô hình có thể tránh được, chúng tôi mở rộng theo chiều rộng bằng cách sử dụng cùng một lớp MoE trong tất cả các khối transformer. Lớp attention đa đầu cũng được chia sẻ qua các khối. Để giúp các khối transformer học các ngữ nghĩa khác nhau và tối đa hóa khả năng mô hình hóa từ lớp MoE, chúng tôi không chia sẻ các lớp chuẩn hóa. Các tham số có thể huấn luyện khác nhau của lớp chuẩn hóa cho phép các khối transformer được cung cấp bởi các biểu diễn đa dạng. Vì khả năng mô hình hóa của mỗi khối transformer đã được tăng cường bởi lớp MoE, nó có thể mô hình hóa các ngữ nghĩa đa dạng hiệu quả với cùng các tham số có thể huấn luyện. Do đó, với một lớp attention và một lớp MoE mạnh mẽ duy nhất học các biểu diễn phức tạp, và các lớp chuẩn hóa độc lập cho các biểu diễn ngữ nghĩa đa dạng, mở rộng thay vì đi sâu hơn là một khung hiệu quả và hiệu suất về tham số hơn.

So với việc chỉ mở rộng theo chiều rộng, mở rộng thay vì đi sâu hơn là một khung hiệu quả về tham số hơn, làm cho các mô hình đủ nhỏ để được thích ứng với các tác vụ hạ nguồn mà không cần song song hóa tiên tiến. Thứ hai, mỗi chuyên gia trong WideNet có thể được huấn luyện bởi nhiều biểu diễn token hơn để có hiệu suất tổng quát hóa tốt hơn. So với các mô hình chỉ được nén theo chiều sâu, tất cả các khối transformer trong WideNet chia sẻ một lớp MoE giống nhau thay vì một lớp FFN. Cấu trúc như vậy tối đa hóa khả năng mô hình hóa của mỗi khối transformer. Nhiều chuyên gia hơn có thể mô hình hóa các biểu diễn token phức tạp hơn với khả năng mạnh hơn. Một khác biệt khác là các lớp chuẩn hóa độc lập. Các lớp này đi kèm với ít tham số có thể huấn luyện bổ sung, nhưng chúng có thể biến đổi các biểu diễn đầu vào thành các miền ngữ nghĩa khác. Trong trường hợp này, với một lớp MoE duy nhất đủ mạnh, WideNet vẫn có thể mô hình hóa tốt các ngữ nghĩa từ các cấp độ khác nhau. Hơn nữa, trong mỗi khối transformer, mỗi chuyên gia chỉ nhận một phần biểu diễn token thường tương ứng với các token đầu vào khác nhau.

Đóng góp của chúng tôi được tóm tắt thành ba mặt:
• Để cải thiện hiệu quả tham số, chúng tôi đề xuất chia sẻ lớp MoE qua các khối transformer. Các chuyên gia được chia sẻ có thể nhận các biểu diễn token đa dạng trong các khối transformer khác nhau, cho phép mỗi chuyên gia được huấn luyện đầy đủ.
• Chúng tôi đề xuất giữ lớp chuẩn hóa riêng lẻ qua các khối transformer. Các lớp chuẩn hóa riêng lẻ có thể biến đổi các vector ẩn đầu vào thành thông tin ngữ nghĩa bằng cách thêm ít tham số có thể huấn luyện. Sau đó, đầu vào đa dạng có thể được cung cấp vào cùng lớp attention hoặc lớp MoE mạnh hơn để mô hình hóa các ngữ nghĩa khác nhau.
• Bằng cách kết hợp hai ý tưởng trên, chúng tôi đề xuất mở rộng thay vì đi sâu hơn, một khung hiệu quả và hiệu suất về tham số hơn. Sau đó, chúng tôi triển khai khung này như WideNet và đánh giá nó trên cả các tác vụ thị giác máy tính và xử lý ngôn ngữ tự nhiên. Do triển khai tham số hiệu quả hơn, WideNet vượt trội hơn các baseline với ít tham số có thể huấn luyện hơn. Chúng tôi hy vọng WideNet của chúng tôi có thể phục vụ như một backbone transformer thế hệ tiếp theo.

Hỗn hợp các Chuyên gia
Trong bài báo này, chúng tôi tập trung vào một khung triển khai tham số có thể huấn luyện mới và triển khai khung này trên transformer như WideNet. Cấu trúc tổng thể được hiển thị trong Hình 1. Chúng tôi sử dụng Vision Transformer làm backbone trong ví dụ này, có nghĩa là chúng tôi chuẩn hóa các biểu diễn trước lớp attention hoặc lớp FFN. Chúng tôi cũng mở rộng WideNet cho các mô hình transformer khác (ví dụ: BERT (Devlin et al. 2019)) trong bài báo này. Trong WideNet, chúng tôi thay thế lớp FFN bằng lớp MoE. Chia sẻ tham số qua các khối transformer được sử dụng để triển khai hiệu quả về tham số hơn. Trong mỗi lớp MoE, chúng tôi có một router để chọn K chuyên gia để học các biểu diễn phức tạp hơn. Xin lưu ý các tham số có thể huấn luyện trong layer normalization không được chia sẻ cho các biểu diễn ngữ nghĩa đa dạng hơn.

Tính toán có điều kiện với MoE
Ý tưởng cốt lõi của chúng tôi là triển khai nhiều tham số có thể huấn luyện hơn theo chiều rộng và ít tham số có thể huấn luyện hơn theo chiều sâu.

--- TRANG 2 ---
LayerNormMulti-Head AttentionLayerNormMoEi Khối Transformer Lặp lại
FFN 0 FFN 1 FFN 2 FFN 3
Router
h ha thâttt
att
imoe
i
l
il+1ha L
i ha 1
i iFigure 1: Kiến trúc tổng thể của WideNet được đề xuất. So với transformer thông thường, chúng tôi thay thế lớp FFN bằng lớp MoE và chia sẻ các tham số có thể huấn luyện ngoại trừ các lớp chuẩn hóa.

sâu. Để đạt được điều này, chúng tôi sử dụng MoE để mở rộng transformer theo chiều rộng. Như một mô hình tính toán có điều kiện điển hình (Bengio 2013), MoE chỉ kích hoạt một vài chuyên gia, tức là các tập con của mạng. Đối với mỗi đầu vào, chúng tôi chỉ cung cấp một phần biểu diễn ẩn cần được xử lý vào các chuyên gia được chọn.

Theo Shazeer et al. (2017), cho E chuyên gia có thể huấn luyện và biểu diễn đầu vào x∈ℝᴰ, đầu ra của mô hình MoE có thể được xây dựng như:
MoE(x) = ∑ᵢ₌₁ᴱ g(x)ᵢe(x)ᵢ                    (1)
trong đó e(·)ᵢ là một phép biến đổi phi tuyến ℝᴰ→ℝᴰ của chuyên gia thứ i, và g(·)ᵢ là phần tử thứ i của đầu ra của router có thể huấn luyện g(·), một ánh xạ phi tuyến ℝᴰ→ℝᴱ. Thông thường, cả e(·) và g(·) đều được tham số hóa bởi các mạng thần kinh.

Theo công thức trên, khi g(·) là một vector thưa thớt, chỉ một phần các chuyên gia sẽ được kích hoạt và cập nhật bởi lan truyền ngược trong quá trình huấn luyện. Trong bài báo này, đối với cả MoE thông thường và WideNet của chúng tôi, mỗi chuyên gia là một lớp FFN.

Định tuyến
Để đảm bảo một định tuyến thưa thớt g(·), chúng tôi sử dụng TopK(·) để chọn các chuyên gia được xếp hạng cao nhất. Sau đó, theo Riquelme et al. (2021), g(·) có thể được viết như:
g(x) = TopK(softmax(f(x) + ε))            (2)
trong đó f(·) là phép biến đổi tuyến tính định tuyến ℝᴰ→ℝᴱ, và ε ~ N(0,1/E²) là nhiễu Gaussian để khám phá định tuyến chuyên gia. Chúng tôi sử dụng softmax sau f(·) để có hiệu suất tốt hơn và các chuyên gia thưa thớt hơn (Riquelme et al. 2021; Fedus, Zoph, and Shazeer 2021). Khi K≪E, hầu hết các phần tử của g(x) sẽ bằng không để đạt được tính toán có điều kiện thưa thớt.

Cân bằng tải
Trong transformer dựa trên MoE, chúng tôi điều phối mỗi token đến K chuyên gia. Trong quá trình huấn luyện, nếu mô hình MoE không có chính quy hóa, hầu hết các token có thể được điều phối đến một phần nhỏ các chuyên gia. Một phân công không cân bằng như vậy sẽ giảm thông lượng của mô hình MoE. Ngoài ra, quan trọng hơn, hầu hết các tham số có thể huấn luyện bổ sung sẽ không được huấn luyện đầy đủ để mô hình có điều kiện thưa thớt không thể vượt qua mô hình dày đặc tương ứng trong quá trình mở rộng. Do đó, để cân bằng tải, chúng ta có hai điều cần tránh: (1) quá nhiều token được điều phối đến một chuyên gia duy nhất, và (2) quá ít token được nhận bởi một chuyên gia duy nhất. Để giải quyết vấn đề đầu tiên, cần có dung lượng bộ đệm B. Nghĩa là, đối với mỗi chuyên gia, chúng tôi chỉ giữ tối đa B token bất kể có bao nhiêu token được điều phối đến chuyên gia này. Nếu nhiều hơn B=C×K×N×L token được gán, các token còn lại sẽ bị loại bỏ. C là tỷ lệ dung lượng, một siêu tham số được định nghĩa trước để kiểm soát tỷ lệ token được giữ cho mỗi chuyên gia. Thông thường, C∈[1,2], và chúng tôi đặt C là 1.2 khi không có giải thích đặc biệt nào được sử dụng. K là số chuyên gia được chọn cho mỗi token. N là kích thước batch trên mỗi thiết bị². L là độ dài chuỗi. Đối với các tác vụ thị giác máy tính, L biểu thị số token patch trong mỗi hình ảnh.

Dung lượng bộ đệm B giúp chúng tôi loại bỏ các token dư thừa cho mỗi chuyên gia để tối đa hóa thông lượng nhưng nó không thể đảm bảo tất cả các chuyên gia nhận đủ token để huấn luyện. Nói cách khác, cho đến bây giờ, định tuyến vẫn không cân bằng. Do đó, chúng tôi theo Fedus, Zoph, and Shazeer (2021) để sử dụng một hàm mất mát cân bằng tải có thể vi phân thay vì các hàm mất mát cân bằng tải và trọng số quan trọng riêng biệt để cân bằng tải trong router. Đối với mỗi thao tác định tuyến, cho E chuyên gia và N batch với N×L token, hàm mất mát phụ trợ sau được thêm vào tổng hàm mất mát mô hình trong quá trình huấn luyện:
l_balance = E × ∑ᵢ₌₁ᴱ mᵢPᵢ                    (3)
trong đó m là vector. phần tử thứ i là phần token được điều phối đến chuyên gia i:
mᵢ = 1/L ∑ⱼ₌₁ᴸ h(xⱼ)ᵢ                       (4)
trong đó h(·) là vector chỉ số được chọn bởi TopK trong Eq. 2. h(xⱼ)ᵢ là phần tử thứ i của h(xⱼ). Cần chú ý rằng, khác với g(x)ᵢ trong Eq. 2, mᵢ và h(xⱼ)ᵢ là không thể vi phân. Tuy nhiên, một hàm mất mát có thể vi phân là cần thiết để tối ưu hóa MoE theo cách end-to-end. Do đó, chúng tôi định nghĩa Pᵢ trong Eq. 3 như:
Pᵢ = softmax(f(x) + ε)ᵢ                     (5)
Chúng ta có thể quan sát Pᵢ là phần tử thứ i của phép biến đổi tuyến tính định tuyến sau hàm kích hoạt softmax, và Pᵢ có thể vi phân.

Mục tiêu của hàm mất mát cân bằng tải là đạt được một phân công cân bằng. Khi chúng ta tối thiểu hóa l_balance, chúng ta có thể thấy cả m và P sẽ gần với một phân phối đồng đều.

Mở rộng thay vì đi sâu hơn
Chia sẻ MoE qua các khối transformer
Như được hiển thị trong Hình 1, WideNet áp dụng chia sẻ tham số qua các khối transformer để cải thiện hiệu quả tham số, và lớp MoE được sử dụng để cải thiện khả năng mô hình. Ngoài ra, vì chúng tôi sử dụng lớp MoE để có được khả năng mô hình hóa mạnh hơn, để khắc phục overfitting từ tính toán có điều kiện thưa thớt, chúng ta cần cung cấp đủ token cho mỗi chuyên gia. Để đạt được điều này, WideNet sử dụng cùng router và chuyên gia trong các khối transformer khác nhau. Chính thức, cho các biểu diễn ẩn H¹={h₁¹,h₂¹,...,hₗ¹} làm đầu vào của khối transformer đầu tiên, chúng ta có thể định nghĩa việc chia sẻ tham số là Hⁱ⁺¹ = MoE(Hⁱ), khác với các mô hình dựa trên MoE hiện có Hⁱ⁺¹ = MoEᵢ(Hⁱ). Xin lưu ý rằng, mặc dù chúng tôi chia sẻ các tham số có thể huấn luyện trong lớp MoE bao gồm router, các biểu diễn token tương ứng với cùng token là khác nhau trong mỗi khối transformer.

²Để dễ sử dụng hơn trên các tác vụ hạ nguồn, chúng tôi triển khai phương pháp của mình chỉ với song song hóa dữ liệu.

--- TRANG 3 ---
Nghĩa là, hⱼⁱ và hⱼⁱ⁺¹ có thể được điều phối đến các chuyên gia khác nhau. Do đó, mỗi chuyên gia sẽ được huấn luyện bởi nhiều token đa dạng hơn để có hiệu suất tổng quát hóa tốt hơn.

Layer Normalization riêng lẻ
Mặc dù các nghiên cứu hiện có (Lan et al. 2019) cho thấy rằng các kích hoạt trong các khối transformer khác nhau tương tự nhau, khoảng cách cosine vẫn lớn hơn nhiều so với không. Do đó, khác với các nghiên cứu hiện có (Dehghani et al. 2018; Lan et al. 2019) chia sẻ tất cả trọng số qua các khối transformer, để khuyến khích các biểu diễn đầu vào đa dạng hơn của các khối khác nhau, chúng tôi chỉ chia sẻ lớp multi-head attention và lớp FFN (hoặc MoE), có nghĩa là các tham số có thể huấn luyện của layer normalization khác nhau qua các khối.

Tóm lại, khối transformer thứ i trong khung của chúng tôi có thể được viết như:
x' = LayerNormal_att^i(x)
x = MHA(x') + x
x'' = LayerNormal_moe^i(x)
x = MoE(x'') + x                    (6)

Lớp chuẩn hóa LayerNormal(·) là:
LayerNormal(x) = (x - E[x])/√(Var[x] + ε) · γ + β    (7)
trong đó γ∈ℝᴰ và β∈ℝᴰ là hai vector có thể huấn luyện.

Layer normalization chỉ yêu cầu hai vector nhỏ này nên normalization riêng lẻ chỉ thêm ít tham số có thể huấn luyện vào khung của chúng tôi. Chúng ta có thể tìm thấy sự khác biệt giữa layer normalization được chia sẻ và các lớp riêng lẻ là trung bình và độ lớn của đầu ra. Đối với layer normalization được chia sẻ, đầu vào của lớp MHA và MoE tương tự hơn trong các khối transformer khác nhau. Vì chúng tôi có các ma trận có thể huấn luyện được chia sẻ, chúng tôi khuyến khích đầu vào đa dạng hơn để biểu diễn các ngữ nghĩa khác nhau trong các khối transformer khác nhau.

Tối ưu hóa
Mặc dù chúng tôi tái sử dụng các tham số có thể huấn luyện của router trong mỗi khối transformer, việc gán sẽ khác nhau do các biểu diễn đầu vào khác nhau. Do đó, cho T lần thao tác định tuyến với cùng các tham số có thể huấn luyện, chúng ta có hàm mất mát sau để tối ưu hóa:
loss = l_main + α∑ᵗ₌₁ᵀ l_balance^T        (8)
trong đó α là một siêu tham số để đảm bảo một phân công cân bằng, và chúng tôi đặt nó là một số tương đối lớn, tức là 0.01 trong nghiên cứu này. Tương tự như các mô hình dựa trên MoE hiện có, chúng tôi thấy hiệu suất không nhạy cảm với α. l_main là mục tiêu chính của transformer chúng tôi. Ví dụ, trên phân loại hình ảnh có giám sát, l_main là hàm mất mát cross-entropy.

Thí nghiệm
Thị giác máy tính
Cài đặt thí nghiệm Chúng tôi sử dụng ILSVRC-2012 ImageNet (Deng et al. 2009) và Cifar10 (Krizhevsky, Hinton et al. 2009) làm nền tảng để đánh giá khung của chúng tôi. ImageNet chúng tôi sử dụng trong nghiên cứu này có 1k lớp và 1.3M hình ảnh. Chúng tôi gọi nó là ImageNet-1K trong các thí nghiệm sau. Chúng tôi chọn ViT (Dosovitskiy et al. 2020) và ViT-MoE (Riquelme et al. 2021) làm baseline. Chúng tôi đầu tiên triển khai lại ViT bằng Tensorflow 2.x và điều chỉnh nó đến hiệu suất hợp lý. Đối với tất cả các mô hình trong phần này, chúng tôi sử dụng tiền xử lý kiểu Inception, Mixup (Zhang et al. 2017), RandAugment (Cubuk et al. 2020) và label smoothing (Szegedy et al. 2016; Yuan et al. 2020) như tăng cường dữ liệu. Chúng tôi cũng quan sát rằng tối ưu hóa AdamW (Loshchilov and Hutter 2017) nhạy cảm với các siêu tham số và lịch trình học. Tối ưu hóa LAMB (You et al. 2019b) có thể đạt được hiệu suất tương đương nhưng nó mạnh mẽ hơn với các siêu tham số. Để so sánh công bằng, theo Zhai et al. (2021), chúng tôi đánh giá WideNet trên ba quy mô (tức là WideNet-Base, WideNet-Large và WideNet-Huge). Các chiều attention và FFN của các quy mô khác nhau giống như ViT-MoE ngoại trừ WideNet-B. Đối với WideNet-B, chúng tôi sử dụng chiều ẩn của FFN là 4096 thay vì 3072 để huấn luyện ổn định hơn.

Thay vì đạt được hiệu suất SoTA, mục tiêu của bài báo này là chỉ ra rằng khung triển khai tham số của chúng tôi có thể cải thiện backbone transformer với ít tham số có thể huấn luyện hơn. Do đó, chúng tôi sử dụng LAMB thay vì AdamW cho các thí nghiệm tổng quát và điển hình hơn. Đối với các mô hình dựa trên MoE (tức là ViT-MoE và WideNet), chúng tôi đặt trọng số của hàm mất mát cân bằng tải là 0.01. Không có hướng dẫn đặc biệt, chúng tôi sử dụng tổng cộng 4 chuyên gia và Top 2 chuyên gia được chọn trong mỗi khối transformer. Tỷ lệ dung lượng C được đặt là 1.2 để đánh đổi giữa độ chính xác và tốc độ. Chúng tôi tiền huấn luyện các mô hình của mình trên 256 lõi TPUv3. Theo nghiên cứu gần đây (Zhai et al. 2021), các loại đầu dự đoán khác nhau không có sự khác biệt đáng kể về hiệu suất few-shot của ImageNet. Chúng tôi cũng xác minh kết luận này khi huấn luyện ImageNet từ đầu. Trong nghiên cứu này, đối với ViT, chúng tôi sử dụng token head điển hình, có nghĩa là chúng tôi chèn token [CLS] vào đầu các token patch và sử dụng nó để phân loại hình ảnh. Đối với các mô hình dựa trên MoE, để sử dụng đầy đủ các biểu diễn token sau lớp MoE cuối cùng, chúng tôi sử dụng một đầu global average pooling thay vì token head.

Trong quá trình fine-tuning, chúng tôi vẫn theo (Dosovitskiy et al. 2020) và sử dụng tối ưu hóa SGD với momentum. So với tiền huấn luyện trên ImageNet-1K, label smoothing và warm-up được loại bỏ.

Bảng 1: Kết quả tiền huấn luyện ImageNet-1K.
Mô hình | Tham số | ImageNet-1K
ViT-B | 87M | 78.6
ViT-L | 305M | 77.5
ViT-MoE-B | 128M | 77.9
ViT-MoE-L | 406M | 77.4
WideNet-B | 29M | 77.5
WideNet-L | 40M | 79.5
WideNet-H | 63M | 80.1

--- TRANG 4 ---
So sánh với baseline Chúng tôi tuân theo cài đặt siêu tham số của baseline trong tiền huấn luyện và fine-tuning để so sánh công bằng. Vui lòng xem Phụ lục để biết chi tiết. Việc triển khai như vậy cũng cho thấy rằng mô hình của chúng tôi mạnh mẽ với các siêu tham số.

Chúng tôi báo cáo độ chính xác Top-1 trên ImageNet-1K trong Bảng 1 và Cifar10 trong Phụ lục. Quan sát rằng WideNet-H đạt được hiệu suất tốt nhất và vượt trội đáng kể so với các mô hình ViT và ViT-MoE trên ImageNet-1K. So với baseline mạnh nhất, WideNet-H của chúng tôi vượt trội hơn ViT-B 1.5% với ít tham số có thể huấn luyện hơn. Thậm chí nếu chúng tôi sử dụng mô hình nhỏ nhất, WideNet-B, nó vẫn đạt được hiệu suất tương đương với ViT-L và ViT-MoE-B với hơn 4× ít tham số có thể huấn luyện hơn. Khi chúng tôi mở rộng lên WideNet-L, nó đã vượt qua tất cả baseline với một nửa tham số có thể huấn luyện của ViT-B và 0.13× tham số của ViT-L.

Một quan sát khác là, không giống như huấn luyện các mô hình dựa trên MoE trên các bộ dữ liệu khổng lồ (ví dụ: JFT-300M (Sun et al. 2017) và C4 (Raffel et al. 2019)), MoE không thể mang lại lợi ích cho ViT trên ImageNet-1K, nhỏ hơn 200 lần so với ViT-MoE gốc được sử dụng trong tiền huấn luyện³.

Xử lý ngôn ngữ tự nhiên
Đóng góp chính của nghiên cứu này là thiết kế một khung hiệu quả về tham số và có thể cắm vào hơn cho các ứng dụng AI khác nhau. Do đó, chúng tôi tiếp tục đánh giá nghiên cứu của mình về xử lý ngôn ngữ tự nhiên (NLP) sau thị giác máy tính (CV). Việc huấn luyện các thí nghiệm trên NLP vẫn có thể được chia thành 2 giai đoạn, tiền huấn luyện và fine-tuning.

Cài đặt thí nghiệm Theo BERT (Devlin et al. 2019) và ALBERT (Lan et al. 2019), trong phần này, chúng tôi tiền huấn luyện tất cả các mô hình bằng English Wikipedia (Devlin et al. 2019) và BOOKCORPUS (Zhu et al. 2015). Vì mục tiêu của nghiên cứu này là thiết kế một khung hiệu quả về tham số, tất cả các mô hình bao gồm BERT sử dụng tham số hóa nhúng nhân tử. Nghĩa là, kích thước nhúng WordPiece E là 128. Cài đặt siêu tham số của các thí nghiệm trên NLP có thể được tìm thấy trong Phụ lục, giống như ALBERT để so sánh công bằng. Tương tự như các thí nghiệm trên các tác vụ thị giác, chúng tôi tiền huấn luyện các mô hình của mình bằng LAMB trên 256 lõi TPUv3. Tỷ lệ học là 0.00176, giống như ALBERT đã tuyên bố (You et al. 2019a).

Trong quá trình fine-tuning, chúng tôi đánh giá mô hình của mình trên bộ đánh giá General Language Understanding Evaluation (GLUE) (Wang et al. 2018), hai phiên bản của bộ dữ liệu Stanford Question Answering (SQuAD) (Rajpurkar et al. 2016; Rajpurkar, Jia, and Liang 2018). Đối với các thí nghiệm GLUE, chúng tôi báo cáo trung vị trên 5 lần chạy trên tập phát triển do phương sai tương đối lớn.

Đánh giá hạ nguồn Khác với các thí nghiệm trên CV, chúng tôi báo cáo kết quả đánh giá trên các tác vụ hạ nguồn trực tiếp trong phần này. Như được hiển thị trong Bảng 2, khi chúng tôi sử dụng nhiều chuyên gia hơn, WideNet của chúng tôi vượt trội hơn ALBERT với biên độ lớn. Ví dụ, WideNet với 4 chuyên gia vượt qua ALBERT 1.2% trung bình. Khi chúng tôi tăng số chuyên gia E lên 16 để đạt được ít tham số có thể huấn luyện hơn một chút so với BERT với tham số hóa nhúng nhân tử, WideNet của chúng tôi cũng vượt trội hơn nó trên tất cả bốn tác vụ hạ nguồn, cho thấy hiệu quả về tham số và hiệu suất của việc mở rộng thay vì đi sâu hơn.

Bảng 2: Kết quả fine-tuning trên các bộ đánh giá GLUE
Mô hình | #para | SQuAD1.1 | SQuAD2.0 | MNLI | SST-2 | Avg
ALBERT | 12M | 89.3/82.3 | 80.0/77.1 | 81.5 | 90.3 | 84.0
BERT | 89M | 89.9/82.8 | 80.3/77.3 | 83.2 | 91.5 | 85.0
WideNet 4 experts | 26M | 89.6/82.7 | 80.6/77.4 | 82.6 | 91.1 | 84.7
WideNet 8 experts | 45M | 90.0/82.7 | 80.6/77.7 | 83.3 | 91.9 | 85.2
WideNet 16 experts | 83M | 90.9/83.8 | 81.0/77.9 | 84.1 | 92.2 | 85.8

Phân tích MoE
Để điều tra lý do tại sao MoE không thể mở rộng tốt trên các bộ dữ liệu nhỏ hơn như ImageNet-1K, chúng tôi thực hiện hai bộ thí nghiệm trên ViT-MoE và WideNet tương ứng. Cho các siêu tham số sau: (1) Số hình ảnh huấn luyện NI; (2) Số token patch trên mỗi hình ảnh Np; (3) Số chuyên gia trong mỗi khối transformer E; (4) Tỷ lệ dung lượng C; (5) Số chuyên gia được chọn trong mỗi khối transformer K, vì chúng ta thường sử dụng α lớn, chúng ta có thể giả định ít token sẽ bị loại bỏ khi chúng ta sử dụng C lớn hơn một chút so với 1.0. Sau đó, chúng ta có thể xấp xỉ T ≈ NI×Np×K/E. Các nghiên cứu hiện có (Riquelme et al. 2021; Yang et al. 2021) đã chỉ ra rằng giảm NI, Np, K và C có thể gây ra sụt giảm hiệu suất. Trong bộ thí nghiệm đầu tiên của phần này, chúng tôi mở rộng số chuyên gia trong mỗi khối transformer E để kiểm soát các token được cung cấp cho mỗi chuyên gia trên ImageNet-1K.

Kết quả được hiển thị trong Hình 2. Chúng tôi quan sát rằng nhiều chuyên gia hơn (tham số có thể huấn luyện) dẫn đến overfitting mặc dù nhiều chuyên gia hơn có nghĩa là khả năng mô hình hóa mạnh hơn. Độ chính xác huấn luyện thấp hơn độ chính xác kiểm tra vì tăng cường dữ liệu mà chúng tôi đã giới thiệu trong phần Cài đặt thí nghiệm.

Để xác minh thêm rằng mỗi chuyên gia yêu cầu các token đa dạng để huấn luyện, chúng tôi thực hiện bộ thí nghiệm thứ hai trên WideNet. Chúng tôi định nghĩa các khối transformer sử dụng cùng phân công định tuyến thuộc về một nhóm. Để thay đổi tính đa dạng đầu vào của mỗi chuyên gia, mỗi nhóm bao gồm nhiều hơn một khối transformer. Nghĩa là, các biểu diễn ẩn tương ứng với cùng token sẽ được cung cấp cho cùng chuyên gia trong cùng nhóm. Chúng tôi đặt tổng cộng G nhóm và mỗi nhóm bao gồm D/G khối transformer, trong đó D là số khối transformer.

--- TRANG 5 ---
Như được hiển thị trong Hình 3, khi chúng tôi sử dụng ít nhóm hơn, có nghĩa là chúng tôi có ít thao tác định tuyến hơn, có sự sụt giảm hiệu suất rõ ràng. Chúng ta có thể đề xuất rằng ít token đa dạng hơn được cung cấp cho mỗi chuyên gia vì ít nhóm hơn có nghĩa là ít định tuyến và phân công hơn. Do đó, nhiều token đa dạng hơn được yêu cầu để huấn luyện các mô hình dựa trên MoE trên các bộ dữ liệu nhỏ hơn. Quan trọng hơn, kết quả như vậy cho thấy tính hiệu quả và sự cần thiết của thiết kế của chúng tôi, định tuyến tại mỗi khối transformer.

4 6 8 10 12 14 16
Số chuyên gia | 66.0 68.0 70.0 72.0 74.0 76.0 78.0 80.0 | Độ chính xác Top-1 (%)
Huấn luyện
Kiểm tra

Hình 2: Kết quả mở rộng số chuyên gia.

12 6 3 2
Số nhóm | 40.0 45.0 50.0 55.0 60.0 65.0 70.0 75.0 80.0 | Độ chính xác Top-1 (%)
Huấn luyện
Kiểm tra

Hình 3: Kết quả mở rộng số nhóm.

Phân tích Layer Norm
Chúng tôi sẽ phân tích lý do tại sao layer normalization riêng lẻ có thể cải thiện hiệu suất trong phần này. So với cấu trúc transformer thông thường, chúng tôi chia sẻ các ma trận có thể huấn luyện trong lớp MHA và FFN (hoặc MoE) qua các khối transformer. Khả năng mô hình hóa bị nén do cùng các tham số có thể huấn luyện qua các khối. Mặc dù WideNet sử dụng lớp MoE để thay thế lớp FFN để cải thiện khả năng, các khối khác nhau vẫn sử dụng cùng các tham số có thể huấn luyện. Do đó, trong WideNet, chúng tôi khuyến khích đầu vào đa dạng hơn để biểu diễn các ngữ nghĩa khác nhau trong các khối transformer khác nhau. So với ViT thông thường, chúng tôi mong đợi một phương sai lớn hơn của các vector có thể huấn luyện γ và β qua các khối. Trong phần này, chúng tôi quan tâm đến layer normalization trước MoE hoặc FFN.

Do đó, đối với phần tử thứ i của vector có thể huấn luyện γ hoặc β trong khối thứ j, chúng tôi tính toán khoảng cách giữa phần tử này và tất cả các phần tử khác của tất cả các vector từ các khối khác. Lấy γ làm ví dụ, chúng ta có thể xây dựng giá trị y mà chúng ta quan tâm như:
y = 1/(M×N²) ∑ⱼ₌₁ᴺ ∑ₘ₌₁ᴹ ∑ₙ₌₁ᴺ I(j≠n)|γⱼⁱ - γᵐⁿ|     (9)
trong đó N là số khối transformer, M là chiều của vector γ hoặc β.

Trong Hình 4 và Hình 5, chúng ta có thể quan sát rằng cả γ và β trong WideNet có y lớn hơn so với những cái trong ViT, có nghĩa là MoE nhận đầu vào đa dạng hơn so với ViT. Kết quả như vậy chứng minh giả định của chúng tôi rằng lớp normalization riêng lẻ có thể giúp mô hình hóa các ngữ nghĩa đa dạng với các ma trận có thể huấn luyện lớn được chia sẻ như MoE.

0 2 4 6 8 10
LayerNorm thứ i trước lớp MoE | 0.00020 0.00025 0.00030 0.00035 0.00040 0.00045 0.00050 | y của gamma
ViT-B
WideNet-B

Hình 4: Phân kỳ của γ với các lớp LayerNorm.

0 2 4 6 8 10
LayerNorm thứ i trước lớp MoE | 0.00010 0.00015 0.00020 0.00025 0.00030 0.00035 0.00040 0.00045 | y của beta
ViT-B
WideNet-B

Hình 5: Phân kỳ của β với các lớp LayerNorm.

Bảng 3: Kết quả nghiên cứu loại bỏ trên ImageNet-1K để điều tra đóng góp của ba sửa đổi chính của chúng tôi (tức là Independent Layer Normalization, mở rộng chiều rộng với lớp MoE và nén chiều sâu với chia sẻ tham số).
Mô hình | Top-1 | Tham số
WideNet-B | 77.5 | 29M
w/ shared Layer Norm | 76.3 | 29M
w/o lớp MoE | Nan | 9M
w/o chia sẻ tham số | 77.9 | 128M
WideNet-L | 79.5 | 40M
w/ shared Layer Norm | 78.3 | 40M
w/o lớp MoE | 76.9 | 15M
w/o chia sẻ tham số | 77.4 | 406M
WideNet-H | 80.1 | 63M
w/ shared Layer Norm | 76.6 | 63M
w/o lớp MoE | 79.0 | 23M
w/o chia sẻ tham số | OOM

Nghiên cứu loại bỏ: Đóng góp của các sửa đổi chính
Chúng tôi đầu tiên thực hiện nghiên cứu loại bỏ để điều tra đóng góp của ba sửa đổi chính của chúng tôi (tức là Independent Layer Normalization, mở rộng chiều rộng với lớp MoE, và nén chiều sâu với chia sẻ tham số). Kết quả được báo cáo trong Bảng 3.

Chúng tôi đầu tiên thay thế các layer normalization riêng lẻ bằng các lớp được chia sẻ. Chúng ta có thể quan sát có sự sụt giảm hiệu suất với gần như cùng các tham số có thể huấn luyện. Quan sát như vậy cho thấy tính hiệu quả của thiết kế của chúng tôi. Ngoài ra, chúng tôi khôi phục lớp MoE về lớp FFN. Không có lớp MoE, việc huấn luyện sẽ cực kỳ khó khăn với ít tham số có thể huấn luyện hơn nhiều. Ví dụ, WideNet-B không có lớp MoE gặp phải bùng nổ gradient, và có sự sụt giảm hiệu suất đáng kể. Cuối cùng, không có chia sẻ tham số qua các khối transformer, chúng ta cũng có thể quan sát sự sụt giảm hiệu suất nhẹ và tăng tham số đáng kể. Đối với WideNet-H không có chia sẻ tham số, nó gặp phải tình trạng hết bộ nhớ khi huấn luyện trên 256 lõi TPUv3.

--- TRANG 6 ---
Bảng 4: Kết quả nghiên cứu loại bỏ trên ImageNet-1K để đánh giá WideNet của chúng tôi với tốc độ hoặc chi phí tính toán tương đương. #Blocks là số khối transformer. FNN dim có nghĩa là chiều của lớp FFN. Para Sharing là việc chúng tôi có chia sẻ tham số qua các khối transformer hay không. Time biểu thị ngày lõi TPUv3.

Mô hình | #Blocks | FNN dim | Para Sharing | Top-1 | #Para | Time
ViT-L | 24 | 4096 | ✗ | 77.5 | 305M | 0.08K
ViT-L | 24 | 4096 | ✓ | 76.9 | 15M | 0.07K
WideNet-L | 12 | 4096 | ✓ | 78.2 | 40M | 0.07K
ViT-L | 24 | 8192 | ✓ | 75.8 | 24M | 0.09K
WideNet-L | 24 | 4096 | ✓ | 79.5 | 40M | 0.14K

Nghiên cứu loại bỏ: So sánh với tốc độ hoặc chi phí tính toán tương đương Vì chúng tôi đặt số chuyên gia được chọn K là 2 và tỷ lệ dung lượng C là 1.2 trong WideNet, có chi phí tính toán bổ sung so với ViT thông thường. Do đó, chúng tôi thực hiện bộ nghiên cứu loại bỏ thứ hai để đánh giá WideNet của chúng tôi với tốc độ hoặc chi phí tính toán tương đương với các baseline.

Như được hiển thị trong Bảng 4, so với ViT-L, WideNet-L tốn kém về tính toán hơn. Chúng ta có thể quan sát sự gia tăng thời gian huấn luyện. Tuy nhiên, khi WideNet-L sử dụng ít khối transformer hơn (tức là 12 khối) so với ViT-L, WideNet-L vượt trội hơn ViT-L 0.7% với thời gian huấn luyện ít hơn một chút và 13.1% tham số, và tương tự, có sự cải thiện hiệu suất lớn hơn so với ViT-L với chia sẻ tham số. Chúng tôi cũng mở rộng ViT-L sử dụng chia sẻ tham số đến lớp FFN rộng hơn. Sau đó, đối với mỗi token, ViT-L sẽ có tính toán tương đương với cài đặt WideNet-L K là 2. Chúng ta có thể thấy mở rộng đến nhiều tham số có thể huấn luyện và FLOPs hơn không thể cải thiện hiệu suất của ViT-L, điều này cũng cho thấy tính hiệu quả và sự cần thiết của khung của chúng tôi. Mặc dù ViT-L có chi phí tính toán tương đương với WideNet cho mỗi token, WideNet vẫn tốn nhiều thời gian huấn luyện hơn mỗi epoch. Theo các thí nghiệm của chúng tôi, có hai lý do, tức là thao tác định tuyến và C > 1.0. Chúng tôi để việc tối ưu hóa điều này làm công việc tương lai của chúng tôi.

Kết luận
Trong bài báo này, chúng tôi đề xuất mở rộng thay vì đi sâu hơn để triển khai tham số hiệu quả và hiệu suất hơn. Chúng tôi triển khai khung có thể cắm và chạy này như WideNet. Đặc biệt, WideNet đầu tiên nén các tham số có thể huấn luyện theo chiều sâu bằng chia sẻ tham số qua các khối transformer. Để tối đa hóa khả năng mô hình hóa của mỗi khối transformer, chúng tôi thay thế lớp FFN bằng lớp MoE. Sau đó, layer normalization riêng lẻ cung cấp một cách hiệu quả về tham số hơn để biến đổi các biểu diễn ngữ nghĩa qua các khối. Chúng tôi cho thấy rằng WideNet đạt được hiệu suất tốt nhất với ít tham số có thể huấn luyện hơn trên cả backbone thị giác máy tính và xử lý ngôn ngữ tự nhiên. Cụ thể, trên ImageNet-1K, mô hình tốt nhất của chúng tôi đạt được độ chính xác Top-1 80.1% chỉ với 63M tham số, vượt trội hơn ViT và ViT-MoE với biên độ lớn. Trên bốn bộ dữ liệu xử lý ngôn ngữ tự nhiên, WideNet vượt trội hơn ALBERT với biên độ lớn và vượt qua BERT với ít tham số có thể huấn luyện hơn. Ngoài ra, điều tra cho thấy lý do tại sao MoE không thể mở rộng tốt trên các bộ dữ liệu nhỏ hơn. Đó là, mỗi chuyên gia yêu cầu đủ token để huấn luyện. Hơn nữa, chúng tôi xác minh rằng normalization riêng lẻ có thể biến đổi các biểu diễn ẩn sang các miền khác cho các ngữ nghĩa đa dạng hơn. Tóm lại, chúng tôi cho thấy rằng có một tiềm năng lớn của khung này để huấn luyện các mô hình hiệu quả về tham số hơn.

--- TRANG 7 ---
Tài liệu tham khảo
Bengio, Y. 2013. Deep learning of representations: Looking forward. In International conference on statistical language and speech processing, 1-37. Springer.
Cubuk, E. D.; Zoph, B.; Shlens, J.; and Le, Q. V. 2020. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, 702-703.
Dehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.; and Kaiser, Ł. 2018. Universal transformers. arXiv preprint arXiv:1807.03819.
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, 248-255. Ieee.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171-4186. Minneapolis, Minnesota: Association for Computational Linguistics.
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.
Fedus, W.; Zoph, B.; and Shazeer, N. 2021. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961.
Huang, Y.; Cheng, Y.; Bapna, A.; Firat, O.; Chen, M. X.; Chen, D.; Lee, H.; Ngiam, J.; Le, Q. V.; Wu, Y.; et al. 2018. Gpipe: Efficient training of giant neural networks using pipeline parallelism. arXiv preprint arXiv:1811.06965.
Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple layers of features from tiny images.
Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; and Soricut, R. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.
Lepikhin, D.; Lee, H.; Xu, Y.; Chen, D.; Firat, O.; Huang, Y.; Krikun, M.; Shazeer, N.; and Chen, Z. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.
Li, S.; Xue, F.; Li, Y.; and You, Y. 2021. Sequence Parallelism: Making 4D Parallelism Possible. arXiv preprint arXiv:2105.13120.
Loshchilov, I.; and Hutter, F. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.
Qu, C.; Yang, L.; Qiu, M.; Croft, W. B.; Zhang, Y.; and Iyyer, M. 2019. BERT with history answer embedding for conversational question answering. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1133-1136.
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.
Rajpurkar, P.; Jia, R.; and Liang, P. 2018. Know What You Don't Know: Unanswerable Questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), 784-789. Melbourne, Australia: Association for Computational Linguistics.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2383-2392. Austin, Texas: Association for Computational Linguistics.
Riquelme, C.; Puigcerver, J.; Mustafa, B.; Neumann, M.; Jenatton, R.; Pinto, A. S.; Keysers, D.; and Houlsby, N. 2021. Scaling Vision with Sparse Mixture of Experts. arXiv preprint arXiv:2106.05974.
Shazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.; Hinton, G.; and Dean, J. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538.
Shoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper, J.; and Catanzaro, B. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.
Sun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Revisiting unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on computer vision, 843-852.
Sun, S.; Cheng, Y.; Gan, Z.; and Liu, J. 2019. Patient Knowledge Distillation for BERT Model Compression. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 4323-4332. Hong Kong, China: Association for Computational Linguistics.
Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna, Z. 2016. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2818-2826.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 353-355. Brussels, Belgium: Association for Computational Linguistics.
Xu, C.; Zhou, W.; Ge, T.; Wei, F.; and Zhou, M. 2020. BERT-of-Theseus: Compressing BERT by Progressive Module Replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 7859-7869. Online: Association for Computational Linguistics.

--- TRANG 8 ---
Xue, F.; Sun, A.; Zhang, H.; and Chng, E. S. 2020a. An Embarrassingly Simple Model for Dialogue Relation Extraction. arXiv preprint arXiv:2012.13873.
Xue, F.; Sun, A.; Zhang, H.; and Chng, E. S. 2020b. GDP-Net: Refining Latent Multi-View Graph for Relation Extraction. arXiv preprint arXiv:2012.06780.
Yang, A.; Lin, J.; Men, R.; Zhou, C.; Jiang, L.; Jia, X.; Wang, A.; Zhang, J.; Wang, J.; Li, Y.; et al. 2021. Exploring Sparse Expert Models and Beyond. arXiv preprint arXiv:2105.15082.
Yang, Z.; Garcia, N.; Chu, C.; Otani, M.; Nakashima, Y.; and Takemura, H. 2020. Bert representations for video question answering. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 1556-1565.
You, Y.; Li, J.; Hseu, J.; Song, X.; Demmel, J.; and Hsieh, C.-J. 2019a. Reducing BERT pre-training time from 3 days to 76 minutes. arXiv preprint arXiv:1904.00962.
You, Y.; Li, J.; Reddi, S.; Hseu, J.; Kumar, S.; Bhojanapalli, S.; Song, X.; Demmel, J.; Keutzer, K.; and Hsieh, C.-J. 2019b. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962.
Yuan, L.; Tay, F. E.; Li, G.; Wang, T.; and Feng, J. 2020. Revisiting knowledge distillation via label smoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 3903-3911.
Zhai, X.; Kolesnikov, A.; Houlsby, N.; and Beyer, L. 2021. Scaling vision transformers. arXiv preprint arXiv:2106.04560.
Zhang, H.; Cisse, M.; Dauphin, Y. N.; and Lopez-Paz, D. 2017. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412.
Zhou, W.; Huang, K.; Ma, T.; and Huang, J. 2020. Document-Level Relation Extraction with Adaptive Thresholding and Localized Context Pooling. arXiv preprint arXiv:2010.11304.
Zhu, Y.; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun, R.; Torralba, A.; and Fidler, S. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, 19-27.

--- TRANG 9 ---
Phụ lục
Kết quả fine-tuning trên thị giác máy tính
Bảng 5: Kết quả fine-tuning Cifar10.
Mô hình | Tham số | Cifar10
ViT-B | 85M | 98.3
ViT-L | 305M | 98.2
ViT-MoE-B | 126M | 98.5
ViT-MoE-L | 404M | 98.5
WideNet-B | 27M | 98.4
WideNet-L | 38M | 98.8

Kết quả trong Bảng 5 cho thấy rằng WideNet của chúng tôi có thể chuyển đổi hiệu suất tốt hơn từ tiền huấn luyện sang fine-tuning. WideNet-L, vượt trội hơn tất cả baseline trong tiền huấn luyện, vẫn là mô hình tốt nhất trong fine-tuning.

Siêu tham số của các thí nghiệm trên thị giác máy tính
Bảng 6: Siêu tham số trên tiền huấn luyện ImageNet-1K và fine-tuning Cifar10.
Tham số | ImageNet-1K | Cifar10
Epoch | 300 | 100
Warmup Epochs | 30 | 0
Batch Size | 4096 | 512
Learning rate | 0.01 | 0.03
Weight Decay | 0.1 | 0
Dropout | 0.1 | 0.1
Label smoothing | 0.1 | 0
Mixup prob. | 0.5 | 0.5

Giá trị của các siêu tham số trên các thí nghiệm thị giác máy tính được hiển thị trong Bảng 6. Trên ImageNet-1K, giảm tỷ lệ học cosine được sử dụng sau 30 epoch khởi động. Xin lưu ý tất cả các mô hình đều sử dụng cùng siêu tham số của Bảng 6, điều này cũng cho thấy tính mạnh mẽ của mô hình chúng tôi.

Siêu tham số của các thí nghiệm trên xử lý ngôn ngữ tự nhiên
Như được hiển thị trong Bảng 7, chúng tôi sử dụng tỷ lệ dung lượng C lớn hơn trên SQuAD1.1 vì dữ liệu huấn luyện hạn chế có thể gây ra phân phối token không cân bằng. Do đó, chúng tôi đặt α là 0 và C là 2.0.

Bảng 7: Siêu tham số trên các tác vụ NLP hạ nguồn.
Tham số | SQuAD1.1/2.0 | MNLI | SST2
Steps | 3649/8144 | 10000 | 5234
Warmup | 365/814 | 1000 | 314
Batch Size | 48 | 128 | 128
LR | 5e-5/3e-5 | 3e-5 | 4e-5
C | 2.0/1.2 | 1.2 | 1.2
α | 0/0.01 | 0.01 | 0.01
Dropout | 0.1/0 | 0 | 0
Max Length | 384/512 | 512 | 512
