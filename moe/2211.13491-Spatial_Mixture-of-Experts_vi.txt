# Spatial Mixture-of-Experts

Nikoli Dryden
ETH Zürich
ndryden@ethz.ch

Torsten Hoeﬂer
ETH Zürich
htor@inf.ethz.ch

Tóm tắt

Nhiều dữ liệu có sự phụ thuộc cơ bản vào vị trí không gian; có thể là thời tiết trên Trái Đất, một mô phỏng trên lưới, hoặc một hình ảnh đã được đăng ký. Tuy nhiên, đặc trưng này hiếm khi được tận dụng và vi phạm các giả định phổ biến được đưa ra bởi nhiều lớp mạng neural, chẳng hạn như tính bất biến tịnh tiến. Hơn nữa, nhiều nghiên cứu có tích hợp tính địa phương không thể nắm bắt được cấu trúc chi tiết. Để giải quyết vấn đề này, chúng tôi giới thiệu lớp Spatial Mixture-of-Experts (SMoE), một lớp gated thưa thớt học cấu trúc không gian trong miền đầu vào và định tuyến các chuyên gia ở mức độ chi tiết để sử dụng nó. Chúng tôi cũng phát triển các kỹ thuật mới để huấn luyện SMoE, bao gồm mất mát định tuyến tự giám sát và giảm chấn lỗi chuyên gia. Cuối cùng, chúng tôi cho thấy kết quả mạnh mẽ cho SMoE trên nhiều nhiệm vụ và thiết lập kết quả state-of-the-art mới cho dự báo thời tiết tầm trung và hậu xử lý dự báo thời tiết tổng hợp.

1 Giới thiệu

Nhiều tập dữ liệu thể hiện cấu trúc cơ bản dựa trên vị trí, trong đó giá trị tại một điểm phụ thuộc vào vị trí của điểm đó. Ví dụ, dự báo thời tiết [78] phụ thuộc vào vị trí của chúng trên Trái Đất; nhiều mô phỏng khoa học được tham số hóa trên lưới cơ bản [9]; dữ liệu (ví dụ: khuôn mặt) có thể được căn chỉnh cụ thể [91]; hoặc nó có thể đúng gần đúng, như trong hình ảnh tự nhiên với các đối tượng ở trung tâm. Hơn nữa, các nhiệm vụ trên dữ liệu như vậy thường là các nhiệm vụ hồi quy dày đặc, chẳng hạn như dự báo thời tiết vài ngày trong tương lai. Nhiều kiến trúc đã được áp dụng thành công cho các nhiệm vụ như vậy. Mạng neural tích chập (CNN) [80] và transformer [74] cho thấy triển vọng cho dự báo thời tiết tầm trung. Mạng kết nối cục bộ (LCN), sử dụng bộ lọc độc lập thay vì chia sẻ tại mỗi điểm, đã được áp dụng cho hậu xử lý thời tiết [38], nhận dạng khuôn mặt [46,77,91] và các nhiệm vụ khác [18,71], đặc biệt để học các đặc trưng cục bộ. Kết nối cục bộ hạng thấp (LRLCN) [30] nới lỏng tính bất biến tịnh tiến của CNN trong khi yêu cầu ít tham số hơn LCN. Các phương pháp khác, chẳng hạn như CoordConv [65], thêm một thiên hướng quy nạp bổ sung bằng cách cung cấp tọa độ đầu vào rõ ràng.

Tuy nhiên, đối với các nhiệm vụ trên dữ liệu có cấu trúc dựa trên vị trí, các phương pháp trước đây bị hạn chế bởi các giới hạn khác nhau. Tích chập giả định rằng dữ liệu có tính bất biến tịnh tiến, điều này không đúng đối với các nhiệm vụ như vậy [46,91]. Các phương pháp như LCN yêu cầu nhiều tham số. Nhiều kiến trúc đã được thiết kế cho các nhiệm vụ phân loại và không thể hoạt động tốt trên hồi quy vì chúng hoạt động ở mức độ chi tiết quá thô và không thể nắm bắt các chi tiết chính. Điều này hạn chế khả năng áp dụng của chúng cho các nhiệm vụ quan trọng, chẳng hạn như dự báo thời tiết tầm trung hoặc mô phỏng khí hậu. Thực tế, trên một nhiệm vụ khuếch tán nhiệt đơn giản với độ khuếch tán phụ thuộc vào vị trí (xem §3.1), nhiều phương pháp hoàn toàn không học được sự phụ thuộc vào vị trí và thay vào đó hội tụ về độ khuếch tán "trung bình".

Để giải quyết vấn đề này, chúng tôi giới thiệu một lớp mạng neural mới, lớp Spatial Mixture-of-Experts (SMoE) (§2). Một SMoE sử dụng hàm gating đã học để lựa chọn thưa thớt và định tuyến từ một tập hợp các chuyên gia chung đến mỗi vị trí không gian (ví dụ: pixel) trong một mẫu đầu vào. Điều này cho phép các chuyên gia chuyên môn hóa vào các đặc tính độc đáo của các "vùng" khác nhau trong dữ liệu và dễ dàng diễn giải bằng cách phân tích cổng. SMoE yêu cầu giả định rằng tất cả các mẫu trong tập dữ liệu có cấu trúc không gian cơ bản tương tự, nhưng điều này thường đúng (ít nhất là gần đúng) đối với nhiều tập dữ liệu, chẳng hạn như thời tiết, trong đó mỗi ví dụ nằm trên cùng một lưới vĩ độ/kinh độ. Đối với hàm gating SMoE, chúng tôi giới thiệu định tuyến tensor, một hàm định tuyến đơn giản và rẻ tiền có thể học hiệu quả cấu trúc không gian này.

Chúng tôi cũng thấy rằng các phương pháp huấn luyện end-to-end, như trong mixture-of-experts (MoE) tiêu chuẩn [82,86], không nắm bắt tốt sự phụ thuộc vào vị trí. Thay vào đó, chúng tôi giới thiệu mất mát phân loại định tuyến riêng biệt (§2.2) để huấn luyện cổng, một mất mát tự giám sát cung cấp tín hiệu học tập tốt hơn cho cổng về các định tuyến không chính xác. Chúng tôi cũng giới thiệu giảm chấn lỗi chuyên gia (§2.3), giới hạn cập nhật chuyên gia từ việc định tuyến sai và cải thiện hiệu suất hơn nữa. Cả hai phương pháp đều dựa vào việc trích xuất thông tin từ tín hiệu lỗi (gradient của đầu ra lớp) mà nếu không sẽ không mang tính thông tin. Hình 1 cung cấp tổng quan về SMoE và so sánh định tính với các nghiên cứu liên quan quan trọng. Với các phương pháp này, SMoE có thể học cấu trúc chi tiết phụ thuộc vào vị trí mà các kiến trúc khác bỏ lỡ và do đó mang lại hiệu suất tốt hơn nhiều.

SMoE có liên quan và lấy cảm hứng từ các nghiên cứu trước đây về MoE (ví dụ: [82,86,102]), nhưng có những khác biệt chính. Các MoE hiện có sử dụng định tuyến thô ở mức mẫu [99], token [86] hoặc patch [82], và các chuyên gia tạo ra đầu ra thô (ví dụ: toàn bộ mẫu hoặc kênh) trong khi SMoE định tuyến ở mức chi tiết, per-pixel. Định tuyến chi tiết quan trọng để cho phép các chuyên gia chuyên môn hóa vào thông tin quy mô nhỏ, điều này có thể rất quan trọng [74,84]. Các MoE như vậy cũng thường nhằm mục đích tăng khả năng mô hình trong khi giữ chi phí tính toán không đổi, trong khi mục tiêu của SMoE là nắm bắt các phụ thuộc không gian. Các nghiên cứu khác đã nhằm mục đích kết hợp sự phụ thuộc không gian, chẳng hạn như LRLCN [30], học các trọng số phụ thuộc vào nội dung và vị trí để kết hợp một tập hợp các bộ lọc cơ sở. Tuy nhiên, chúng tôi thấy rằng các phương pháp trước đây này không nắm bắt được các đặc trưng chi tiết cần thiết cho hiệu suất tốt trên các nhiệm vụ hồi quy dày đặc (§3.1).

Chúng tôi tiến hành thí nghiệm trên một số tập dữ liệu benchmark với SMoE và thực hiện các nghiên cứu ablation rộng rãi về các quyết định thiết kế SMoE (§3). Sử dụng một tập dữ liệu khuếch tán nhiệt đơn giản, chúng tôi thể hiện các hạn chế của các mô hình khác và sức mạnh của SMoE trong một tình huống có kiểm soát. Sau đó, chúng tôi áp dụng SMoE cho dự báo thời tiết tầm trung và vượt trội so với state-of-the-art [53,80] trên WeatherBench [78]; và thiết lập state-of-the-art mới cho hậu xử lý dự báo thời tiết tổng hợp trên tập dữ liệu ENS-10 [8]. Cuối cùng, chúng tôi cho thấy rằng SMoE cũng có thể được áp dụng cho các nhiệm vụ phân loại hình ảnh, nơi chúng tôi bằng hoặc vượt trội so với LRLCN trong khi sử dụng ít tham số hơn.

Mã nguồn của chúng tôi có sẵn tại https://github.com/spcl/smoe.

1.1 Nghiên cứu liên quan

Mixture-of-Experts và tính toán có điều kiện. Trong khi MoE đã được sử dụng từ lâu [15,49,52], kể từ khi deep learning ra đời, đã có nhiều nghiên cứu trong việc áp dụng MoE, tính toán có điều kiện và định tuyến động cho DNN [1,4,5,7,10-12,19-21,23,25,27,29,33,34,39,51,59,60,70,75,76,82,83,86,99,102,107]. Thường thì mục tiêu là tăng khả năng mô hình mà không tăng tương ứng chi phí tính toán và các mô hình sử dụng định tuyến thô. Đáng chú ý, các MoE định tuyến ở mức mẫu (ví dụ: [25,29,70,83]), token (ví dụ: [33,59,86]) hoặc patch (ví dụ: [82]); thường được huấn luyện với các mất mát phụ trợ rộng rãi; và thường nhắm mục tiêu các nhiệm vụ phân loại. Định tuyến thô này, đặc biệt, có nghĩa là có cơ hội hạn chế cho các chuyên gia chuyên môn hóa. Các MoE đặc biệt cho các nhiệm vụ thị giác cũng thường hoạt động ở mức mẫu (ví dụ: [1,4,39,99,102]) và sử dụng các chuyên gia để chuyên môn hóa lựa chọn bộ lọc hoặc kênh. Ngược lại, SMoE của chúng tôi tạo ra một thiên hướng quy nạp thông qua định tuyến chi tiết, phụ thuộc vào vị trí cho các vấn đề có cấu trúc cơ bản như vậy; thường không cần các mất mát phụ trợ ngoài mất mát phân loại định tuyến; và hoạt động tốt cho các nhiệm vụ hồi quy dày đặc.

Cấu trúc không gian cục bộ. Các mạng kết nối cục bộ (tức là bộ lọc tích chập không chia sẻ trọng số) từng thành công trong các nhiệm vụ thị giác như nhận dạng khuôn mặt [18,46,77,91]. Các nghiên cứu khác kết hợp chia sẻ trọng số tuần hoàn hoặc theo ô [37,71,105]. Tuy nhiên, các nghiên cứu gần đây thường sử dụng độc quyền tích chập, đã được tìm thấy là hoạt động tốt hơn [72] trong khi yêu cầu ít tham số hơn đáng kể. CoordConv [65] cung cấp rõ ràng tọa độ pixel như đầu vào. Do cấu trúc của chúng, một lớp tích chập hoặc CoordConv đơn lẻ không thể học các kích hoạt thưa thớt phụ thuộc vào vị trí như SMoE, và các mạng nhiều lớp cho cải thiện hạn chế trong thực tế. Các mạng kết nối cục bộ hạng thấp [30] học kết hợp một số lượng nhỏ các bộ lọc cơ sở theo cách phụ thuộc vào vị trí và đầu vào. Các bộ kết hợp này thiếu tính thưa thớt và áp dụng tất cả các bộ lọc cơ sở tại mỗi điểm, và được chuẩn hóa softmax, có thể dẫn đến một vài bộ lọc thống trị, hạn chế tính đa dạng. Các nghiên cứu riêng biệt đã sử dụng attention để cung cấp tỷ lệ phụ thuộc vào vị trí hoặc tích hợp ngữ cảnh [35,50,64,93,95,100]. Điều này đã đỉnh cao trong vision transformer [26] và các biến thể [28,62,67,96,97], sử dụng các cơ chế attention chuyên biệt để tích hợp thông tin không gian. Các nghiên cứu khác sử dụng Markov [61,66] hoặc trường ngẫu nhiên có điều kiện [16,43,106], hoặc các phương pháp dựa trên đồ thị [17,98,104], để học các phụ thuộc tầm xa. Các nghiên cứu bổ sung đã nghiên cứu việc kết hợp các tính chất bất biến vào mạng neural (ví dụ: [13]).

Tính thưa thớt. Việc sử dụng định tuyến thưa thớt của chúng tôi giống với các nghiên cứu về tính thưa thớt trong DNN nói chung [45]. Nhiều phương pháp học một mặt nạ chi tiết để xác định trọng số nào cần giữ hoặc cắt tỉa (ví dụ: [63,90]), trong khi các phương pháp khác làm thưa thớt các kích hoạt (ví dụ: [3,47,56,68,69]). Các nghiên cứu này sử dụng tính thưa thớt để cải thiện hiệu suất runtime, thường tại thời gian suy luận.

Dự báo thời tiết. Dự báo thời tiết tầm trung, hoặc dự báo khoảng ba đến bảy ngày trước [6], có tầm quan trọng xã hội rộng rãi [57]. Đã có nhiều quan tâm trong việc áp dụng deep learning cho nhiệm vụ này [85], và WeatherBench [78] phục vụ như một benchmark cộng đồng. Deep learning cũng đã được áp dụng thành công cho các nhiệm vụ liên quan của hậu xử lý tổng hợp [38,79] và now-casting (dự báo vài giờ trước) [2,31,81,87-89]. CNN tiêu chuẩn hiện là state-of-the-art trên WeatherBench [80] (mặc dù graph neural network cho thấy triển vọng trên dữ liệu tương tự [53]) và có tính cạnh tranh trên các nhiệm vụ hậu xử lý [8]. SMoE có thể tận dụng cấu trúc chi tiết, phụ thuộc vào vị trí rộng rãi trong dữ liệu thời tiết để cải thiện hiệu suất.

2 Spatial Mixture of Experts

Bây giờ chúng tôi giới thiệu lớp Spatial Mixture-of-Experts (SMoE). Một SMoE sử dụng hàm gating đã học (§2.1) để lựa chọn các chuyên gia cụ thể từ một tập hợp chung để áp dụng tại mỗi điểm (ví dụ: pixel) trong đầu vào. Cổng học cấu trúc không gian cơ bản của dữ liệu và định tuyến các chuyên gia cụ thể đến các "vùng" khác nhau, cho phép chúng chuyên môn hóa. Một SMoE dựa trên giả định rằng cấu trúc không gian tương tự trên tất cả các mẫu trong tập dữ liệu. Chúng tôi cũng giả định dữ liệu nằm trên lưới Cartesian (tức là grid), mặc dù điều này có thể được tổng quát hóa. Để huấn luyện SMoE, chúng tôi giới thiệu mất mát phân loại định tuyến tự giám sát (§2.2) và giảm chấn lỗi chuyên gia (§2.3), điều mà chúng tôi thấy cần thiết để đạt được hiệu suất tốt nhất. Hình 2 cung cấp tổng quan về SMoE và huấn luyện của chúng.

Trước tiên chúng tôi định nghĩa lớp SMoE một cách đầy đủ tổng quát, sau đó thảo luận về cài đặt cụ thể mà chúng tôi sử dụng. (Xem §3.1 cho ablation.) Gọi x ∈ R^(C×H×W) là một mẫu đầu vào (chúng tôi giả định dữ liệu 2D cho đơn giản). Lớp SMoE bao gồm một tập hợp E các chuyên gia, trong đó E ≤ |E| sẽ được chọn tại mỗi điểm, và một hàm gating G: R^(C×H×W) → R^(|E|×H×W). Hàm gating có chính xác E không-zero tại mỗi điểm không gian (H×W), tương ứng với các chuyên gia được chọn tại điểm đó. Mỗi chuyên gia e ∈ E được áp dụng tại các điểm trong x nơi nó đã được chọn và có thể bao gồm ngữ cảnh bổ sung từ x (ví dụ: các điểm xung quanh nếu e là tích chập), và tạo ra đầu ra có kích thước F. Đầu ra của mỗi chuyên gia tại một điểm sau đó được nối để tạo thành chiều kênh đầu ra (có kích thước E×F) của SMoE.

Chính xác hơn, gọi gather_I() chọn chỉ các mục đầu vào nơi I khác không. Khi đó một SMoE là:
y = gather_{G(x)} (G(x) ⊙ [e_1(x); ...; e_{|E|}(x)]),
trong đó ⊙ là phép nhân theo từng phần tử, [e; ...] xếp các tensor, và y ∈ R^(E×F×H×W). Khi E < |E|, hàm gating tạo ra tính thưa thớt đáng kể; điều này cho phép một SMoE tính toán các chuyên gia chỉ tại các điểm nơi chúng được áp dụng, tránh hầu hết tính toán.

Công thức này cho ra một SMoE có trọng số, trong đó đầu ra chuyên gia được chia tỷ lệ bởi hàm gating. Một thay thế, hiệu quả hơn một chút và có thể dễ diễn giải hơn, là SMoE không trọng số, trong trường hợp này các chuyên gia không được chia tỷ lệ, và gather chỉ sử dụng G(x) để chọn chuyên gia.

Trong nghiên cứu này, chúng tôi tập trung vào một lớp SMoE đơn giản nhưng mạnh mẽ sử dụng các bộ lọc tích chập làm chuyên gia:
y_i = G(x)_i ⊙ Σ_{c∈[C]} w_{i,c} ⋆ x_c,
trong đó ⋆ là tương quan chéo, i ∈ [|E|], và chúng tôi đã bỏ qua gather cho đơn giản.

2.1 Hàm Gating

Hàm gating G trong một SMoE là quan trọng để học sự phụ thuộc vào vị trí cơ bản của dữ liệu. Trong khi G có thể là bất kỳ hàm nào, một cổng tốt nên tương đối rẻ tiền. Chúng tôi theo các nghiên cứu trước đây về MoE (ví dụ: [82,86]) và sử dụng định tuyến top-E để chọn chuyên gia và làm thưa thớt cổng: G(x) = top_E(g(x)), trong đó g(x) ∈ R^(|E|×H×W) là một lớp gating có thể học và top_E chọn E mục lớn nhất dọc theo chiều đầu tiên (chuyên gia) và đặt các mục còn lại về 0. Tuy nhiên, chúng tôi thấy rằng việc chuẩn hóa hàm gating bằng softmax, như thường được thực hiện, là không cần thiết.

Có nhiều lựa chọn cho lớp gating. Nhiều nghiên cứu MoE đã sử dụng MLP [82,86], nhưng chúng tôi thấy điều này không hoạt động tốt. Tích chập hoặc CoordConv [65] cũng không hoạt động tốt (§3.1). Thay vào đó, chúng tôi thấy rằng một lớp gating định tuyến tensor đơn giản hoạt động tốt nhất. Với điều này, g(x) = D ∈ R^(|E|×H×W), trong đó D là một tensor có thể học mã hóa trực tiếp sự phụ thuộc vào vị trí cơ bản và cấu trúc định tuyến mà không phụ thuộc vào đầu vào. Hơn nữa, D sử dụng một tham số cho mỗi chuyên gia cho mỗi vị trí và không yêu cầu tính toán nào ngoài cập nhật optimizer, làm cho nó trở thành lựa chọn hiệu quả.

Chúng tôi khởi tạo D bằng phân phối đều trên [-3√|E|/EF, 3√|E|/EF], tương ứng với khởi tạo đều Kaiming với fan-in EF/|E| [41]. Tuy nhiên, trong nhiều trường hợp thực tế, một số dữ liệu về vị trí có thể được biết (ví dụ: liệu đó có phải là đất liền hay biển khi thực hiện dự báo thời tiết). Trong những trường hợp như vậy, D có thể được khởi tạo dựa trên dữ liệu này để phân nhóm chuyên gia trước. Điều này cho phép hàm gating hưởng lợi từ kiến thức trước trong khi vẫn có thể điều chỉnh định tuyến. Ngoài ra, một mạng có thể chứa nhiều lớp SMoE, trong trường hợp đó chúng ta có thể chia sẻ D giữa các lớp có cùng chiều không gian và số lượng chuyên gia, làm giảm overhead của cổng.

Cuối cùng, nhiều công thức MoE sử dụng một số mất mát phụ trợ để đảm bảo các cổng học chính sách định tuyến tốt và tránh sụp đổ mode (ví dụ: [10,82,86]). Khi huấn luyện với mất mát phân loại định tuyến mà chúng tôi đề xuất, chúng tôi không thấy cần thiết các mất mát phụ trợ bổ sung.

2.2 Huấn luyện và Mất mát Phân loại Định tuyến

Huấn luyện MoE thường được thực hiện end-to-end, với các chuyên gia và cổng học đồng thời, và gradient thưa thớt dựa trên định tuyến. Chúng tôi thấy điều này không dẫn đến hiệu suất tốt với SMoE và định tuyến tensor, đặc biệt là trên các nhiệm vụ hồi quy, vì cổng không học tốt: nó hiếm khi thay đổi quyết định định tuyến từ khởi tạo. Chúng tôi giả thuyết rằng điều này do "mismatch" trong gradient trên các nhiệm vụ hồi quy, trong đó chúng mang tính thông tin cho chuyên gia nhưng không cho cổng, bởi vì hồi quy nhằm mục đích tạo ra dự đoán liên tục trên cả giá trị dương và âm, trong khi chọn chuyên gia yêu cầu ngưỡng (xem §B). Để giải quyết vấn đề này, chúng tôi huấn luyện cổng với một hàm mất mát riêng biệt, tự giám sát, mất mát phân loại định tuyến (RC). Ý tưởng chính là xem xét định tuyến như một nhiệm vụ phân loại dày đặc, đa nhãn: chọn các chuyên gia "đúng" tại mỗi điểm (cf. phân đoạn ngữ nghĩa). Mất mát RC thực hiện chính xác điều này và huấn luyện cổng bằng cách xây dựng các nhãn thích hợp. Điều này cũng giúp tránh sụp đổ mode, nơi chỉ một số lượng nhỏ chuyên gia được chọn (xem §3.2).

Để xây dựng các nhãn này, chúng ta cần xác định liệu cổng có chọn tập hợp chuyên gia đúng tại mỗi điểm hay không. Tuy nhiên, thông tin như vậy không có sẵn trực tiếp. Thay vào đó, chúng tôi sử dụng tín hiệu lỗi vào lớp SMoE (tức là gradient của đầu ra lớp w.r.t. mất mát) như một proxy và nói rằng định tuyến không chính xác khi tín hiệu lỗi có độ lớn lớn cho chuyên gia tại điểm đó, vì điều này sẽ ngụ ý một cập nhật gradient lớn tương ứng. Hơn nữa, trong trường hợp mất mát mean-square error L (thường được sử dụng cho hồi quy), tín hiệu lỗi có thể mã hóa trực tiếp lỗi dự đoán. Gọi X là dự đoán, Y là giá trị thực và N là số phần tử trong X. Khi đó tín hiệu lỗi là:
dL/dX = d/dX [1/N Σ(X-Y)²] = 2/N(X-Y).
Do đó, tín hiệu lỗi chỉ đơn giản là lỗi (được chia tỷ lệ, có dấu) của dự đoán. Trong khi quan hệ chính xác này ngừng tồn tại khi backpropagation tiếp tục, trực giác đằng sau độ lớn tín hiệu lỗi vẫn còn.

Bây giờ chúng tôi định nghĩa mất mát phân loại định tuyến. Cho tín hiệu lỗi ε vào một lớp SMoE và một phân vị lỗi q (một siêu tham số), chúng tôi nói rằng việc chọn chuyên gia tại một điểm là không chính xác nếu ε tại chuyên gia và điểm đó lớn hơn phân vị thứ q của ε, và đúng nếu ngược lại. Chúng tôi sử dụng phân vị vì chúng độc lập với tỷ lệ của ε, có thể khó xác định và thay đổi trong quá trình huấn luyện. Sau đó chúng tôi xây dựng nhãn cho mỗi điểm như sau: Các chuyên gia không được chọn bắt đầu với nhãn 0. Một chuyên gia được chọn đúng có nhãn 1. Cuối cùng, nếu một chuyên gia được chọn không chính xác, nhãn của nó là 0 và chúng tôi thêm 1/(|E| - E) vào giá trị nhãn của mỗi chuyên gia không được chọn (lưu ý |E| - E là số chuyên gia không được chọn). Điều này tương ứng với tiên nghiệm đều rằng chuyên gia đúng có thể là bất kỳ chuyên gia nào không được chọn. Với các nhãn này, mất mát RC cho một cổng sau đó là mất mát binary cross-entropy của đầu ra cổng.

2.3 Giảm chấn Lỗi Chuyên gia

Trong khi mất mát RC cho phép cổng SMoE được huấn luyện trực tiếp dựa trên việc liệu nó có định tuyến đúng hay không, các chuyên gia được định tuyến không chính xác vẫn thực hiện cập nhật gradient dựa trên định tuyến này. Điều này dẫn đến việc các chuyên gia cập nhật để cải thiện hiệu suất của chúng cho các vị trí mà chúng có thể không được áp dụng trong các lần lặp huấn luyện tương lai sau khi định tuyến thay đổi. Để giảm thiểu điều này, chúng tôi đề xuất giảm chấn lỗi chuyên gia, trong đó phần tín hiệu lỗi tương ứng với định tuyến không chính xác được giảm chấn để hạn chế cập nhật không chính xác. Chúng tôi thấy rằng điều này có thể cải thiện hiệu suất SMoE và giảm thời gian hội tụ.

Giảm chấn lỗi chuyên gia tương tự như mất mát RC, và chúng tôi phân loại định tuyến không chính xác theo cách tương tự. Sau đó chúng tôi chia tỷ lệ tín hiệu lỗi vào các chuyên gia bằng một hệ số không đổi tại mỗi điểm nơi định tuyến không chính xác. Điều này sẽ hạn chế độ lớn của cập nhật được thực hiện để phản hồi lại việc định tuyến sai.

2.4 Cài đặt Thực tế

Bây giờ chúng tôi thảo luận về cài đặt của một lớp SMoE, chủ yếu tập trung vào SMoE đơn giản với các bộ lọc tích chập mà chúng tôi sử dụng. Trong một cài đặt lý tưởng, chi phí flop của một SMoE là chi phí của cổng cộng với chi phí áp dụng các chuyên gia được chọn. Khi sử dụng cổng định tuyến tensor, không có flop trong cổng. Các flop từ việc áp dụng các chuyên gia tích chập được chọn tương đương với một lớp tích chập tiêu chuẩn với số bộ lọc bằng số chuyên gia được chọn. Do đó, SMoE khá hiệu quả về mặt flop. Tuy nhiên, các nghiên cứu gần đây đã chỉ ra rằng di chuyển dữ liệu cũng quan trọng đối với hiệu suất [48]. Vì điều này, chúng tôi không mong đợi ngay cả các cài đặt được tối ưu hóa tốt cũng sẽ khớp với runtime của một lớp tích chập do định tuyến thưa thớt, tính cục bộ hạn chế trong việc truy cập các bộ lọc chuyên gia và các hoạt động khác, mặc dù nghiên cứu về tính thưa thớt được tăng tốc phần cứng [22] sẽ mang lại lợi ích. Mặc dù vậy, trong quá trình suy luận các tối ưu hóa bổ sung có thể có sẵn vì cổng định tuyến tensor không phụ thuộc vào đầu vào và các tính toán có thể được sắp xếp lại để tối đa hóa tính cục bộ.

Thật không may, việc cài đặt hiệu quả các mẫu truy cập bất thường trong SMoE là thách thức trong các framework Python tiêu chuẩn, và có thể yêu cầu các kernel tính toán tùy chỉnh. Thay vào đó, chúng tôi chọn cài đặt naive trong PyTorch [73] nơi chúng tôi áp dụng tất cả các chuyên gia tại tất cả các điểm và sau đó sử dụng các hoạt động gather và scatter để cài đặt định tuyến thưa thớt. Các chuyên gia được nối theo thứ tự điểm định tuyến được sắp xếp (tức là như được đưa ra bởi G(x)); trong khi điều này có thể dẫn đến thứ tự kênh thay đổi trong quá trình huấn luyện, chúng tôi thấy nó có tác động ít (xem §3.2). Chúng tôi thấy cài đặt này đủ ngay cả cho các nhiệm vụ quy mô lớn.

3 Thí nghiệm

Bây giờ chúng tôi trình bày kết quả thí nghiệm sử dụng SMoE và nhiều mô hình baseline, cũng như ablation của các thành phần SMoE khác nhau. Đầu tiên, chúng tôi mô tả một tập dữ liệu khuếch tán nhiệt phụ thuộc vào vị trí đơn giản, mà chúng tôi sử dụng để nghiên cứu khả năng của các kiến trúc khác nhau trong việc học sự phụ thuộc vào vị trí trong môi trường có kiểm soát (§3.1). Sau đó chúng tôi trình bày kết quả trên thử thách dự báo thời tiết tầm trung WeatherBench [78] (§3.2) và tập dữ liệu ENS-10 [8] cho hậu xử lý dự báo thời tiết tổng hợp (§3.3), nơi SMoE thiết lập kết quả hiệu suất state-of-the-art mới. Cuối cùng, chúng tôi cho thấy kết quả trên một số nhiệm vụ phân loại hình ảnh, minh họa sự rộng rãi của khả năng áp dụng SMoE ngay cả trong các tình huống không có sự phụ thuộc vào vị trí nghiêm ngặt (§3.4).

Tất cả kết quả đều được chạy bằng PyTorch [73] phiên bản 1.11 trên một cluster lớn với GPU V100 16 GB. Chúng tôi tóm tắt các chi tiết huấn luyện trong suốt phần này và cung cấp chi tiết đầy đủ trong §A. Chúng tôi sử dụng các cài đặt PyTorch hiện có khi có sẵn và tự cài đặt các phương pháp khác. Trừ khi được ghi chú, tất cả kết quả SMoE đều sử dụng các chuyên gia kernel tích chập 3×3, cổng định tuyến tensor không trọng số, mất mát RC, giảm chấn lỗi chuyên gia, phân vị lỗi q = 0.7 và hệ số giảm chấn 0.1. Chúng tôi báo cáo trung bình và độ lệch chuẩn (mean±std) trên mười lần chạy cho tất cả thí nghiệm, ngoại trừ WeatherBench và ImageNet, sử dụng ba lần chạy. Tổng cộng, chúng tôi sử dụng khoảng 30k giờ GPU cho thí nghiệm.

3.1 Khuếch tán Nhiệt phụ thuộc vào Vị trí

Để nghiên cứu sự phụ thuộc vào vị trí một cách có kiểm soát, chúng tôi tạo ra một tập dữ liệu khuếch tán nhiệt với độ khuếch tán phụ thuộc vào vị trí. Nhiệm vụ là dự đoán nhiệt độ sẽ như thế nào ở bước thời gian tiếp theo, cho trước nhiệt độ hiện tại tại mỗi điểm. Bởi vì chúng tôi biết chính xác các độ khuếch tán và phân phối của chúng trong dữ liệu, việc xác định mô hình đã học nhiệm vụ này tốt như thế nào là dễ dàng. Trước tiên chúng tôi mô tả tập dữ liệu và tạo ra nó chi tiết, sau đó thảo luận về kết quả và nghiên cứu ablation về các thành phần SMoE.

Tập dữ liệu. Tập dữ liệu bao gồm một bản đồ vùng, nơi mỗi vị trí được gán một loại, và mỗi loại tương ứng với một độ khuếch tán nhiệt khác nhau. Bản đồ vùng và phân nhóm độ khuếch tán sau đó được cố định cho tập dữ liệu. Chúng tôi tạo ra bản đồ vùng ngẫu nhiên bằng một phương pháp gắn kết ưu tiên đơn giản. Điều này xác định sự phụ thuộc vào vị trí mà chúng tôi muốn học. Để tạo ra các mẫu, trước tiên chúng tôi phân phối ngẫu nhiên các giọt nhiệt trên miền, sau đó áp dụng các stencil khuếch tán năm điểm tại mỗi điểm, sử dụng độ khuếch tán của loại vùng cho mỗi điểm. Để đơn giản, chúng tôi sử dụng điều kiện biên zero. Quá trình này được lặp lại để tạo ra nhiều bước thời gian từ trạng thái bắt đầu đã cho. Tập dữ liệu sau đó bao gồm các bước thời gian được tạo ra cho nhiều trạng thái bắt đầu khác nhau.

Nếu một mô hình có thể học các stencil khuếch tán và sự tương ứng vị trí-độ khuếch tán, nó có thể dự đoán chính xác bước thời gian tiếp theo. Hơn nữa, các stencil độ khuếch tán cũng đơn giản và chính xác tương ứng với một kernel tích chập 3×3 không có tính phi tuyến.

Tập dữ liệu cụ thể mà chúng tôi sử dụng bao gồm 100.000 mẫu 64×64, với 1.000 trạng thái ban đầu phát triển cho 100 bước thời gian mỗi cái. Thêm nhiều mẫu hơn không thay đổi kết quả đáng kể. Có ba loại vùng, với độ khuếch tán 0.25, 0.025 và 0.0025. Hình 3 (giữa) cho thấy bản đồ vùng và các stencil khuếch tán. Chúng tôi báo cáo kết quả sử dụng "% trong vòng 1%", tỷ lệ phần trăm vị trí trong một mẫu trong vòng 1% lỗi tương đối của giá trị thực, vì điều này có thể diễn giải được hơn so với lỗi bình phương trung bình.

Kết quả. Hình 3 (trái) cho thấy kết quả trên tập dữ liệu khuếch tán nhiệt cho SMoE và một số baseline: CNN, LCN, lớp fully-connected (FC), CoordConv [65], CondConv [102], LRLCN [30], vision transformer (ViT) [26] và vision MoE (V-MoE) [82]. Đối với các lớp LCN và FC, chúng tôi chỉ sử dụng một lớp duy nhất vì các lớp bổ sung không cho thấy lợi ích. Convolution, CoordConv, CondConv và LRLCN là mạng tốt nhất được tìm thấy trong một tập hợp có tối đa ba lớp, kernel 3×3, 12 bộ lọc mỗi lớp, batchnorm và kích hoạt ReLU. ViT và V-MoE sử dụng một khối transformer với kích thước patch 4×4, chiều nhúng 128 và bốn head. LRLCN sử dụng ba bộ lọc cơ sở và một bộ kết hợp phụ thuộc vào đầu vào. Chúng tôi cũng thử trọng số kết hợp không chia sẻ không phụ thuộc vào đầu vào (LRLCN-ND). V-MoE chọn một chuyên gia từ một tập hợp ba. SMoE của chúng tôi sử dụng một lớp duy nhất với ba chuyên gia và chọn E = 1 chuyên gia mỗi điểm. Tất cả các mô hình đều được huấn luyện với kích thước batch 32, Adam [54] với tốc độ học 0.001 (giảm 10× sau khi không cải thiện validation trong 15 epoch) và dừng sớm sau khi không cải thiện trong 30 epoch. Điều chỉnh siêu tham số bổ sung không cải thiện kết quả đáng kể. Chúng tôi báo cáo các tham số SMoE như tham số chuyên gia + tham số cổng.

SMoE đạt hiệu suất hoàn hảo trên tập dữ liệu này. Hơn nữa, bằng cách kiểm tra định tuyến đã học và các chuyên gia (Hình 3, phải), chúng ta có thể thấy rằng nó đã thực sự học đúng các stencil khuếch tán và sự phụ thuộc vào vị trí. LCN cũng đạt được điều này, nhưng yêu cầu gấp 3× tham số hơn và yêu cầu 110 epoch để hội tụ (so với 8 cho SMoE). Các lớp fully-connected không học được dữ liệu tốt, có lẽ do thách thức tối ưu hóa rất nhiều tham số. Các phương pháp khác đều hội tụ ở mức từ 91 đến 94% trong vòng 1%. Kiểm tra dự đoán và trọng số của chúng, chúng tôi quan sát thấy rằng chúng dường như không học được sự phụ thuộc vào vị trí của các độ khuếch tán và thay vào đó hội tụ về việc dự đoán với độ khuếch tán "trung bình" trên miền. Chúng tôi cũng thử các mạng tích chập lớn hơn (sâu hơn và/hoặc rộng hơn), nhưng hiệu suất không cải thiện. Các phương pháp MoE (CondConv và V-MoE) cũng thất bại theo cách này, vì các chuyên gia thô của chúng không thể chuyên môn hóa. Hơn nữa, LRLCN-ND thất bại theo cách này, mặc dù kiến trúc của nó tương tự như SMoE khi có một kênh đầu ra (kết hợp phụ thuộc vào vị trí, có trọng số softmax của ba kernel cơ sở). Chúng tôi tin rằng LRLCN-ND thể hiện "mismatch" gradient tương tự như đã thảo luận trước đây (§2.2).

Bây giờ chúng tôi thảo luận về một số ablation khác nhau của kiến trúc và thiết kế SMoE.

Điều gì nếu cấu hình chuyên gia "đúng" không được biết? Trong khi trong các thí nghiệm trên, chúng tôi có thể chọn cấu hình chuyên gia SMoE (số lượng chuyên gia, số chuyên gia được chọn, kích thước bộ lọc chuyên gia) để nó vừa cần thiết vừa đủ để học nhiệm vụ, trong nhiều tình huống thông tin này có thể không có sẵn. Chúng tôi xem xét các cấu hình SMoE thay thế thay đổi từng tham số này: ① sử dụng sáu chuyên gia; ② các chuyên gia với kernel 5×5; ③ và chọn hai chuyên gia mỗi vị trí từ tổng số sáu. Đối với trường hợp ③, chúng tôi cộng hai kênh đầu ra SMoE lại với nhau.

Trong cả ba trường hợp, SMoE đạt 100.00% trong vòng 1% trên nhiệm vụ khuếch tán nhiệt. Trong ①, chúng tôi thấy rằng chúng học các stencil khuếch tán trùng lặp và vẫn định tuyến chúng một cách thích hợp. ② học stencil năm điểm cộng với biên gần-zero, do đó gần như giống hệt với kernel 3×3. Cuối cùng, ③ học các stencil khuếch tán cộng lại với nhau để tạo ra độ khuếch tán đúng. Do đó, chúng ta có thể thấy rằng SMoE mạnh mẽ và thích ứng tốt với các lựa chọn kiến trúc này.

Mất mát RC và giảm chấn lỗi chuyên gia. Bảng 1 cho thấy kết quả huấn luyện SMoE có và không có mất mát phân loại định tuyến (§2.2) và giảm chấn lỗi chuyên gia (§2.3). Không có mất mát RC, hiệu suất SMoE ngang bằng với các baseline khác trong Hình 3, nhưng một khi nó được thêm vào, hiệu suất cải thiện đáng kể khi hàm gating bây giờ học sự phụ thuộc vào vị trí trong dữ liệu. Thêm giảm chấn lỗi chuyên gia cải thiện hiệu suất và hội tụ hơn nữa bằng cách hạn chế tác động của định tuyến sai cổng đối với việc học chuyên gia. Tuy nhiên, chỉ giảm chấn một mình mang lại ít lợi ích, vì nó không cải thiện việc học cổng. Các kết quả này cho thấy rằng những cải tiến này rất quan trọng cho hiệu suất tốt.

Hàm gating. Bảng 2 cho thấy hiệu suất của các hàm gating khác nhau (§2.1) trên SMoE. Chúng tôi xem xét sáu tùy chọn: Một lớp fully-connected đơn lẻ (như thường được sử dụng trong MoE [82,86]); một lớp tích chập, CoordConv [65] hoặc LCN 3×3 đơn lẻ; một cổng với ba lớp CoordConv với batchnorm và ReLU; và cổng định tuyến tensor của chúng tôi. Khi huấn luyện, chúng tôi cũng xem xét các mất mát phụ trợ và các phương pháp khác để cải thiện hiệu suất (xem §C) và báo cáo kết quả tốt nhất. Định tuyến tensor của chúng tôi mang lại hiệu suất tốt nhất. LCN hoạt động tốt thứ hai, có lẽ vì nó cũng sử dụng các tham số riêng biệt cho mỗi vị trí, nhưng sử dụng gấp 9× tham số và yêu cầu tính toán đáng kể. Các phương pháp khác dường như không thể nắm bắt hiệu quả sự phụ thuộc vào vị trí.

Các ablation khác. Chúng tôi thực hiện một số nghiên cứu ablation bổ sung trong §C, bao gồm sử dụng các mất mát phụ trợ và nhiễu định tuyến trong quá trình huấn luyện, chuẩn hóa định tuyến và các hàm chuyên gia.

3.2 Dự báo Thời tiết Tầm trung

Bây giờ chúng tôi thảo luận về kết quả trên benchmark dự báo thời tiết tầm trung WeatherBench [78]. Benchmark này sử dụng tập dữ liệu tái phân tích ERA5 [44], với dữ liệu thời tiết toàn cầu theo giờ cho 1979–2018. Chúng tôi sử dụng tập con dữ liệu được đề xuất bởi Rasp et al. [78] ở độ phân giải 5.625° (32×64 điểm lưới) và huấn luyện trên dữ liệu từ 1979–2015, validate trên 2016 và báo cáo kết quả thử nghiệm cho 2017–2018. Nếu không thì chúng tôi theo phương pháp huấn luyện của Rasp và Thuerey [80]. Các đại lượng mục tiêu cần dự đoán là geopotential tại 500 hPa (Z500) và nhiệt độ tại 850 hPa (T850) với thời gian dẫn trước ba và năm ngày. Chúng tôi báo cáo kết quả sử dụng root-mean-square error (RMSE) có trọng số vĩ độ.

Như một baseline, chúng tôi sử dụng kiến trúc ResNet [42] được giới thiệu bởi Rasp và Thuerey [80], hiện báo cáo kết quả tốt nhất trên WeatherBench. Kiến trúc này bao gồm 19 khối residual mỗi khối có hai lớp [3×3 convolution → LeakyReLU → batchnorm → dropout], cộng với một lớp tích chập 7×7 ban đầu. Tất cả các tích chập trừ cái cuối cùng có 128 bộ lọc. Chúng tôi xem xét ba baseline bổ sung. Đầu tiên giống hệt như trên, nhưng với gấp đôi số bộ lọc (256) trong mỗi tích chập. Thứ hai, chúng tôi thay thế các tích chập 3×3 bằng các lớp LRLCN [30]. Cuối cùng, chúng tôi sử dụng ViT bốn lớp [26] với kích thước patch 2×2, chiều ẩn 1024 và tám head (cấu hình hoạt động tốt nhất).

Chúng tôi thích ứng Rasp và Thuerey ResNet để sử dụng SMoE với ba cấu hình: thêm một lớp SMoE sau tích chập đầu tiên; thêm một lớp SMoE sau tích chập cuối cùng; và thay thế tất cả các tích chập 3×3 bằng các lớp SMoE. Mỗi SMoE chọn cùng số chuyên gia như lớp gốc có bộ lọc và có gấp đôi số chuyên gia (tức là |E| = 256, E = 128). Chúng tôi cũng chia sẻ cổng định tuyến tensor qua tất cả các lớp SMoE có cùng số chuyên gia, vì vậy overhead của nó là tối thiểu.

Bởi vì dữ liệu thời tiết nằm trên một lưới cố định với sự phụ thuộc vào vị trí cơ bản (Trái Đất), chúng tôi mong đợi SMoE mang lại một số lợi ích bằng cách chuyên môn hóa vào các đặc tính của các vùng khác nhau. Trong Bảng 3, chúng tôi quan sát thấy rằng điều này thực sự đúng. Thêm SMoE cải thiện kết quả trong tất cả các tình huống, với sự cải thiện đáng kể nhất đến từ việc thay thế tất cả các tích chập 3×3 bằng SMoE. Điều này thể hiện lợi thế của việc kết hợp các thiên hướng phụ thuộc vào vị trí thích hợp. ResNet rộng hơn mang lại cải thiện hạn chế (phù hợp với kết quả được báo cáo bởi Rasp và Thuerey [80]). Các bộ lọc phụ thuộc vào vị trí của LRLCN cải thiện so với ResNet, nhưng không thể sánh với SMoE. Chúng tôi không thể đạt được hiệu suất tốt với ViT, nhưng đã quan sát thấy rằng chúng rất nhạy cảm với kích thước patch.

Kết hợp kiến thức trước vào cổng. Trong khi bản chất chính xác của sự phụ thuộc vào vị trí của dữ liệu này không được biết, chúng tôi có một tiên nghiệm rộng về một số khía cạnh của nó, chẳng hạn như liệu một điểm có phải là đất liền hay biển. Thông tin này có thể được kết hợp vào SMoE bằng cách khởi tạo cổng định tuyến tensor để thiên hướng định tuyến về các chuyên gia khác nhau. Để thực hiện điều này, chúng tôi sử dụng mặt nạ đất-biển từ ERA5 để khởi tạo cổng định tuyến các vị trí đất liền về một nửa số chuyên gia và các vị trí biển về nửa còn lại. Lưu ý điều này không cố định định tuyến, vì cổng có thể điều chỉnh khi nó học. Hơn nữa, mặt nạ đất-biển đã được bao gồm trong dữ liệu đầu vào, vì vậy tất cả các mô hình đã có quyền truy cập vào thông tin này.

Kết quả với điều này nằm trong dòng "+gate prior" của Bảng 3 và hoạt động tốt nhất. Cấu hình này thiết lập state-of-the-art mới cho WeatherBench khi không sử dụng dữ liệu bổ sung. Thực tế, nó gần như khớp với hiệu suất của một ResNet với 150 năm dữ liệu pretrain bổ sung từ mô phỏng khí hậu [80]. Chúng tôi cũng thử cấu hình nơi cổng được khởi tạo ngẫu nhiên và cố định thay vì học ("rand fixed gate init"). Điều này hoạt động tệ hơn so với baseline của chúng tôi, vì mạng không thể thích ứng lựa chọn định tuyến của nó, và mỗi chuyên gia thấy ít điểm hơn trong mỗi mẫu so với một mạng tiêu chuẩn, dẫn đến ít học hỏn hơn. Do đó, việc học hàm định tuyến cực kỳ quan trọng đối với hiệu suất tốt.

Dữ liệu bổ sung. Theo Rasp và Thuerey [80], chúng tôi sử dụng 150 năm dữ liệu từ mô hình khí hậu MPI-ESSM-HR từ kho lưu trữ CMIP6 [32] để pretrain cấu hình SMoE tốt nhất của chúng tôi, sau đó được fine-tune trên dữ liệu ERA5 như trên. Điều này vượt trội đáng kể so với cả SMoE không pretrain và ResNet pretrain của Rasp và Thuerey. Chúng tôi kết hợp thêm dữ liệu để đẩy hiệu suất hơn nữa bằng cách thêm dữ liệu ERA5 từ phần mở rộng ngược gần đây nhất (1959–1979), tăng kích thước tập dữ liệu khoảng 50%. Điều này cho thấy kết quả được cải thiện; tuy nhiên, chúng tôi nghi ngờ hiệu suất đang bão hòa do độ phân giải không gian thô của dữ liệu. Do đó chúng tôi huấn luyện cấu hình cuối cùng với dữ liệu độ phân giải cao hơn (1.4°). Sử dụng điều này, SMoE của chúng tôi vượt trội đáng kể so với state-of-the-art trên WeatherBench; thực tế, hiệu suất của nó trên T850 rất gần với Hệ thống Dự báo Tích hợp hoạt động [78]. Kết quả của chúng tôi cũng có tính cạnh tranh với của Keisler [53], mặc dù chúng không so sánh trực tiếp được (do, ví dụ, độ phân giải dữ liệu khác nhau).

Sụp đổ Mode. Nhiều MoE gặp phải sụp đổ chuyên gia hoặc mode (ví dụ: [10,82,86]), nơi chỉ một số lượng nhỏ chuyên gia được chọn. Điều này thường được tránh với nhiễu định tuyến và/hoặc các mất mát "cân bằng tải" phụ trợ. Trên tập dữ liệu khuếch tán nhiệt, chúng tôi thấy các mất mát này không mang lại lợi ích (§C). Chúng tôi cũng không quan sát thấy sụp đổ mode trong SMoE trên WeatherBench. Với mất mát RC, chúng tôi huấn luyện cổng trực tiếp, cập nhật trọng số định tuyến về các chuyên gia khác sau sai lầm, và do đó tránh các vấn đề như vậy.

Thứ tự lựa chọn chuyên gia. Trong quá trình huấn luyện, thứ tự các chuyên gia được nối có thể thay đổi (do thay đổi trong điểm định tuyến tương đối hoặc chọn các chuyên gia khác nhau), điều này sẽ tác động đến thứ tự các kênh được thấy bởi các lớp tiếp theo. Khi huấn luyện trên WeatherBench, chúng tôi thấy điều này không có tác động đáng kể: thứ tự chuyên gia ổn định sớm, cho phép các lớp hoạt động trên các biểu diễn ổn định. Hơn nữa, hầu hết "hoán đổi" xảy ra giữa các chuyên gia có độ tin cậy thấp, vì vậy được giới hạn trong một tập con các kênh.

3.3 Hậu xử lý Dự báo Thời tiết Tổng hợp

Các hệ thống dự báo thời tiết số thường sử dụng tổng hợp các mô phỏng để định lượng độ không chắc chắn và cải thiện chất lượng dự báo [14]. Tuy nhiên, các tổng hợp như vậy thường thể hiện thiên hướng hệ thống [92], và việc sửa chữa chúng cải thiện kỹ năng dự báo [14,85,101], một nhiệm vụ mà deep learning đã cho thấy triển vọng [38,79]. Chúng tôi sử dụng tập dữ liệu ENS-10 [8], bao gồm hai mươi năm (1998–2017) dữ liệu reforecast toàn cầu [40] ở độ phân giải không gian 0.5°. Chúng tôi theo thiết lập benchmarking của Ashkboos et al. [8] và sửa chữa dự đoán cho Z500, T850 và nhiệt độ 2 mét (T2m) tại thời gian dẫn trước 48 giờ sử dụng cả năm và mười thành viên tổng hợp. Chúng tôi báo cáo kết quả sử dụng điểm xác suất xếp hạng liên tục (CRPS) và CRPS có trọng số sự kiện cực đoan (EECRPS).

Chúng tôi thích ứng mô hình U-Net từ các baseline ENS-10, vì nó mang lại hiệu suất tốt và hoạt động trên dữ liệu toàn cầu (các phương pháp khác sử dụng patch). Tương tự như phương pháp của chúng tôi cho WeatherBench, chúng tôi thay thế mỗi tích chập 3×3 bằng một SMoE với gấp bốn lần số chuyên gia so với lớp gốc và chọn cùng số chuyên gia như lớp gốc có bộ lọc. Chúng tôi chia sẻ cổng định tuyến tensor giữa tất cả các lớp có cùng chiều không gian và số chuyên gia, ngoại trừ các thân encoder và decoder cũng sử dụng cổng riêng biệt. Như baseline, chúng tôi sử dụng kiến trúc U-Net gốc và Ensemble Model Output Statistics (EMOS) [36], một phương pháp hậu xử lý tiêu chuẩn.

Chúng tôi quan sát trong Bảng 5 rằng, tương tự như WeatherBench, SMoE mang lại cải thiện đáng kể trong kỹ năng dự báo qua tất cả các tình huống và thiết lập state-of-the-art mới cho sửa chữa dự đoán trên tập dữ liệu ENS-10. Điều này cũng chứng minh rằng SMoE có thể mở rộng đến miền không gian rất lớn được sử dụng bởi dữ liệu ENS-10 và vẫn học được sự phụ thuộc vào vị trí thích hợp.

3.4 Phân loại Hình ảnh

Cuối cùng, chúng tôi trình bày kết quả trên một số nhiệm vụ phân loại hình ảnh; chúng tôi tập trung ở đây vào ImageNet-1k [24] và thảo luận về kết quả trên các tập dữ liệu bổ sung trong §D. Trong khi các tập dữ liệu này không có cấu trúc phụ thuộc vào vị trí nghiêm ngặt, việc nới lỏng tính bất biến tịnh tiến nghiêm ngặt của tích chập có thể mang lại lợi ích và cho phép so sánh trực tiếp với Elsayed et al. [30]. Chúng tôi theo phương pháp thí nghiệm của họ và huấn luyện bằng công thức của Vryniotis [94]. Chúng tôi hoặc chèn một lớp SMoE sau lớp tích chập đầu tiên hoặc cuối cùng của ResNet-50 [42] hoặc thay thế tất cả các tích chập 3×3 bằng các lớp SMoE. SMoE của chúng tôi có gấp đôi số chuyên gia so với lớp tích chập gốc và chọn một nửa số đó, để giữ chiều đầu ra không đổi. Các lớp gating được chia sẻ giữa tất cả các khối có kích thước bằng nhau. Để so sánh, chúng tôi cũng huấn luyện ResNet-50 với tất cả các tích chập 3×3 được thay thế bằng các lớp LRLCN [30]; và một Wide ResNet-50-2 [103], có tham số so sánh với SMoE.

Bảng 4 cho thấy SMoE vượt trội so với LRLCN khi chúng tôi thay thế tất cả các tích chập 3×3, trong khi sử dụng 56% tham số. Tuy nhiên, một wide ResNet hoạt động tốt nhất tổng thể. Tuy nhiên, điều này cho thấy rằng phân loại ImageNet thực sự hưởng lợi từ việc nới lỏng tính bất biến tịnh tiến.

4 Thảo luận

Chúng tôi đã trình bày lớp Spatial Mixture-of-Experts, một lớp mới học các phụ thuộc vào vị trí cơ bản trong dữ liệu và sau đó sử dụng định tuyến chi tiết để chuyên môn hóa các chuyên gia cho các khu vực khác nhau. Chúng tôi cũng giới thiệu mất mát phân loại định tuyến và giảm chấn lỗi chuyên gia, cho phép SMoE hoạt động tốt trên các nhiệm vụ hồi quy dày đặc. Các nghiên cứu trước đây cho thấy hiệu quả hạn chế trên các nhiệm vụ này: Hoặc nó không nắm bắt sự phụ thuộc vào vị trí (ví dụ: tích chập) hoặc nó hoạt động ở mức thô (ví dụ: MoE tiêu chuẩn). Bằng cách vượt qua những thách thức này, chúng tôi cho thấy một khả năng mới cho mạng neural và thiết lập state-of-the-art mới cho dự báo thời tiết tầm trung và hậu xử lý tổng hợp.

Nhiều vấn đề khác có tầm quan trọng xã hội rộng rãi có cấu trúc không gian tương tự, đặc biệt là trong các lĩnh vực khoa học [9], và chúng tôi mong đợi SMoE có thể áp dụng cho chúng. Tuy nhiên, các nhiệm vụ như nhận dạng khuôn mặt và giám sát cũng từng cho thấy lợi ích từ những cải tiến như vậy [91] và SMoE do đó nên được sử dụng một cách cẩn thận.

SMoE cho thấy rằng việc học sự phụ thuộc vào vị trí là một thiên hướng quy nạp mạnh mẽ cho các loại dữ liệu nhất định, và có nhiều hướng nghiên cứu hơn nữa. Hai lĩnh vực chính đặc biệt quan tâm là phát triển các cài đặt được cải thiện cho định tuyến chi tiết, thưa thớt; và tổng quát hóa SMoE từ hoạt động trên lưới sang đồ thị tổng quát, điều này sẽ cho phép chúng được áp dụng cho nhiều nhiệm vụ bổ sung.

Lời cảm ơn và Tiết lộ Tài trợ

Chúng tôi cảm ơn các thành viên của SPCL tại ETH Zürich, và Peter Dueben và Mat Chantry của ECMWF, cho các cuộc thảo luận hữu ích; và các reviewer ẩn danh cho các gợi ý và phản hồi của họ. Công việc này đã nhận được tài trợ từ European High-Performance Computing Joint Undertaking (JU) theo thỏa thuận tài trợ số 955513 (MAELSTROM), và từ Huawei. N.D. nhận hỗ trợ từ ETH Postdoctoral Fellowship. Chúng tôi cảm ơn Swiss National Supercomputing Center (CSCS) và Livermore Computing cho cơ sở hạ tầng tính toán.
