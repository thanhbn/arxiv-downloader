# 2311.10768.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2311.10768.pdf
# Kích thước tệp: 396386 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mô hình Ngôn ngữ Tăng cường Bộ nhớ thông qua
Hỗn hợp Chuyên gia Từ
Cicero Nogueira dos Santos, James Lee-Thorp, Isaac Noble
Chung-Ching Chang, David Uthus
Google Research
cicerons,jamesleethorp,isaacn,ccchang,duthus@google.com
Tóm tắt
Việc mở rộng số lượng tham số của các mô hình ngôn ngữ đã được chứng minh là một phương pháp hiệu quả để cải thiện hiệu suất. Đối với các mô hình dày đặc, việc tăng kích thước mô hình sẽ làm tăng tỷ lệ thuận lượng tính toán của mô hình. Trong nghiên cứu này, chúng tôi tìm cách tách biệt mạnh mẽ năng lực học tập và FLOPs thông qua các mô hình kiểu Hỗn hợp Chuyên gia (MoE) với các hàm định tuyến dựa trên từ vựng giàu kiến thức quy mô lớn và các chuyên gia. Phương pháp đề xuất của chúng tôi, được gọi là Hỗn hợp Chuyên gia Từ (MoWE), có thể được xem như một mô hình tăng cường bộ nhớ, nơi một tập hợp lớn các chuyên gia dành riêng cho từ đóng vai trò như một bộ nhớ thưa thớt. Chúng tôi chứng minh rằng MoWE hoạt động tốt hơn đáng kể so với họ mô hình T5 với số lượng FLOPs tương tự trong nhiều tác vụ NLP khác nhau. Ngoài ra, MoWE vượt trội so với các mô hình MoE thông thường trên các tác vụ đòi hỏi kiến thức chuyên sâu và có hiệu suất tương tự với các phương pháp tăng cường bộ nhớ phức tạp hơn thường yêu cầu triệu hồi các cơ chế tùy chỉnh để tìm kiếm bộ nhớ thưa thớt.

1 Giới thiệu
Việc tăng số lượng tham số của các mô hình ngôn ngữ đã là động lực chính dẫn đến chất lượng mô hình được cải thiện (Raffel et al., 2020; Kaplan et al., 2020; Brown et al., 2020). Điều này đặc biệt rõ ràng trên các tác vụ đòi hỏi kiến thức chuyên sâu, chẳng hạn như TriviaQA (Joshi et al., 2017), nơi các mô hình ngôn ngữ có nhiều tham số và năng lực học tập hơn được hưởng lợi từ việc hấp thụ kiến thức thế giới từ dữ liệu tiền huấn luyện của chúng (Chowdhery et al., 2022; Touvron et al., 2023). Tuy nhiên, việc tăng kích thước mô hình cũng làm tăng chi phí chạy mô hình.

Trong nghiên cứu này, chúng tôi xây dựng dựa trên mô hình Hỗn hợp Chuyên gia (MoE) để thiết kế một kiến trúc mạng thần kinh tận hưởng lợi ích chất lượng từ việc mở rộng số lượng tham số nhưng vẫn hiệu quả về FLOPs và độ trễ. Phương pháp đề xuất của chúng tôi, mà chúng tôi gọi là Hỗn hợp Chuyên gia Từ (MoWE), tuân theo hai nguyên tắc thiết kế: (1) một số lượng rất lớn các chuyên gia (hàng chục nghìn thay vì 32 đến 128 thường được sử dụng trong MoEs) mà (2) là "dành riêng cho từ" – tức là, chúng được liên kết với một từ vựng giàu kiến thức quy mô lớn thông qua hàm định tuyến cố định. Lớp MoWE cốt lõi được minh họa trong Hình 2. Các mô hình MoWE là các mô hình tăng cường bộ nhớ, nơi tập hợp lớn các chuyên gia từ (MLPs nhỏ) đóng vai trò như một bộ nhớ thưa thớt được tích hợp liền mạch vào khung xương mô hình chính.

Chúng tôi chứng minh thực nghiệm rằng MoWE vượt trội đáng kể so với các mô hình T5 (Raffel et al., 2020) với số lượng FLOPs tương đương trên nhiều tác vụ NLP khác nhau. Tập trung vào các tác vụ đòi hỏi kiến thức chuyên sâu như TriviaQA (Joshi et al., 2017) và WebQuestions (Berant et al., 2013), chúng tôi chỉ ra rằng MoWE kích thước "Base" vượt trội so với T5-XL và MoWE "Large" vượt trội so với các mô hình T5-XXL (xem Hình 1), trong khi nhanh hơn ít nhất 4.3x và 6.6x trong huấn luyện, tương ứng. MoWE vượt trội so với các mô hình MoE thông thường (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022) trên các tác vụ đòi hỏi kiến thức chuyên sâu, trong khi đạt hiệu suất tương đương trên các bộ tác vụ NLP như SuperGLUE (Wang et al., 2019a). Ngoài ra, MoWE cũng đạt hiệu suất tương đương hoặc vượt trội

--- TRANG 2 ---
Hình 2: Lớp MoWE: Chúng tôi thay thế lớp FFN trong một tập con các khối Transformer bằng Lớp MoWE, là một lớp thưa thớt xử lý các token sử dụng nhiều chuyên gia (FFNs). Mỗi token đầu vào được xử lý bởi một chuyên gia duy nhất được chọn dựa trên id token đầu vào (tại vị trí chuỗi tương ứng) trong từ vựng định tuyến.

so với các mô hình tăng cường kiến thức được đề xuất gần đây (Févry et al., 2020; de Jong et al., 2022), trong khi tránh triệu hồi bất kỳ cơ chế tùy chỉnh nào để tìm kiếm bộ nhớ thưa thớt.

Tóm lại, các đóng góp chính của nghiên cứu này là:

• Chúng tôi đề xuất một kiến trúc mạng thần kinh mới kết hợp hiệu quả giữa tính hiệu quả của các mô hình thưa thớt với sức mạnh của các mô hình ngôn ngữ lớn để ghi nhớ và truy xuất kiến thức thế giới; xem Bảng 4 để có cái nhìn sơ bộ về cách sử dụng các bộ nhớ này.

• Chúng tôi giới thiệu các từ vựng phụ trợ rất lớn để thực hiện định tuyến.

• Chúng tôi đề xuất và xác thực một chiến lược mới để huấn luyện hiệu quả các mô hình MoE với: (1) hàng trăm nghìn chuyên gia và (2) phân công token rất không cân bằng giữa các chuyên gia.

• Đối với các tác vụ đòi hỏi kiến thức chuyên sâu như trả lời câu hỏi và xác minh tuyên bố, chúng tôi trình bày các mô hình thưa thớt hiệu quả mới vượt trội so với các mô hình dày đặc lớn hơn, chậm hơn đáng kể sử dụng nhiều FLOPs hơn một bậc độ lớn.

2 Hỗn hợp Chuyên gia Từ

2.1 Nền tảng Hỗn hợp Chuyên gia (MoE)

Các kiến trúc MoE dựa trên Transformer (Lepikhin et al., 2020; Du et al., 2022; Fedus et al., 2022) được triển khai bằng cách thay thế lớp Mạng Truyền tiếp (FFN) dày đặc trong một tập con các khối Transformer bằng một lớp thưa thớt của các chuyên gia. Thay vì sử dụng một FFN duy nhất để xử lý tất cả đầu vào, lớp thưa thớt sử dụng một tập hợp các FFNs (các chuyên gia). Mỗi biểu diễn token được xử lý bởi một chuyên gia duy nhất (top-1) hoặc một tập con (top-k) của các chuyên gia. Hứa hẹn trong các mô hình MoE là tăng đáng kể số lượng tham số trong mạng mà không tăng đáng kể lượng tính toán.

Các triển khai MoE phổ biến thay thế mỗi lớp FFN khác của kiến trúc Transformer bằng một lớp thưa thớt chứa từ 32 đến 128 chuyên gia (Lepikhin et al., 2020; Du et al., 2022; Fedus et al., 2022). Các token được phân công cho các chuyên gia cụ thể bởi một hàm định tuyến được học cùng với các tham số còn lại của mạng. Do tính chất của việc phân công một chiều của các token cho các chuyên gia, việc huấn luyện hàm định tuyến khó khăn và thường được thực hiện gián tiếp bằng cách tái tỉ lệ đầu ra chuyên gia theo xác suất phân công (độ "tin cậy của bộ định tuyến") rằng một token nhất định nên được phân công cho một chuyên gia cụ thể.

2.2 Kiến trúc Hỗn hợp Chuyên gia Từ (MoWE)

Tương tự như các mô hình MoE, MoWE là một kiến trúc dựa trên Transformer (Vaswani et al., 2017) nơi lớp FFN của một tập con các khối Transformer được thay thế bằng Lớp MoWE, là một lớp thưa thớt xử lý các token sử dụng một nhóm chuyên gia (FFNs). Trong một lớp MoWE, biểu diễn token tại vị trí i được xử lý bởi một chuyên gia duy nhất được chọn dựa trên id, trong từ vựng định tuyến, của token chuỗi đầu vào tương ứng tại vị trí i. Hình 2 minh họa một lớp MoWE.

Các quyết định định tuyến được điều khiển bởi một từ vựng phụ trợ lớn. Có hai tokenization của đầu vào: (1) tokenization mặc định là tokenization thông thường xác định các token đầu vào và embedding của chúng; và (2) tokenization định tuyến, được thực hiện sử dụng một từ vựng định tuyến phụ trợ lớn (được giới thiệu trong Mục 2.4). Các id token thu được từ tokenization định tuyến được gọi là id định tuyến. Trong một lớp MoWE, định tuyến bao gồm việc ánh xạ các id định tuyến tới các id chuyên gia thông qua một hàm băm. Trong trường hợp cực đoan khi mỗi từ trong từ vựng định tuyến có chuyên gia riêng, id định tuyến tương ứng trực tiếp với id chuyên gia, như được minh họa trong Hình 2.

Tầm quan trọng của một nhóm chuyên gia lớn. Một lớp MoWE sử dụng hàng chục hoặc hàng trăm nghìn chuyên gia, thường nhỏ hơn (chiều MLP nhỏ hơn) so với lớp FFN dày đặc thông thường. Mục tiêu của việc sử dụng một số lượng lớn chuyên gia là khuyến khích chuyên môn hóa. Với số lượng chuyên gia cực lớn, mỗi từ trong từ vựng định tuyến được phân công cho chuyên gia riêng của nó. Tuy nhiên, chúng tôi thấy rằng việc có ít chuyên gia hơn so với số mục từ vựng và chia sẻ một số chuyên gia giữa nhiều id định tuyến là hiệu quả hơn (cả về mặt bộ nhớ và tín hiệu huấn luyện). Tuy nhiên, một token với id nhất định luôn được định tuyến tới cùng một chuyên gia.

Nghiên cứu gần đây cho thấy rằng các Transformer hoạt động như các bộ nhớ key-value (Geva et al., 2021; Dai et al., 2022; Zhang et al., 2022), và kiến thức thực tế dường như được lưu trữ trong các FFN (Dai et al., 2022; Meng et al., 2022). Chúng tôi phỏng đoán rằng từ vựng định tuyến lớn và số lượng chuyên gia liên quan lớn khuyến khích thêm lớp MoWE hoạt động như một bộ nhớ thưa thớt. Chúng tôi thấy rằng việc sử dụng các từ hoàn chỉnh thay vì các phần từ (xem Mục 2.4) để thực hiện định tuyến là một thiên hướng quy nạp mạnh mẽ giúp các chuyên gia dễ dàng chuyên môn hóa trên các từ cụ thể hơn. Ví dụ, chuyên gia cho từ "Turing" sẽ chỉ được kích hoạt khi từ đó xuất hiện trong đầu vào, và do đó sẽ được chuyên môn hóa về nội dung cùng xuất hiện với từ đó. Bằng cách sử dụng các bộ nhớ key-value dành riêng cho từ (chuyên gia từ), hy vọng của chúng tôi là MoWE có thể giúp mô hình dễ dàng lưu trữ và truy xuất thông tin về những từ đó hơn.

2.3 Vượt qua Thách thức của việc Sử dụng Hàng chục Nghìn Chuyên gia

Hầu hết các mô hình MoE quy mô lớn được triển khai sử dụng chiến lược song song chương trình đơn, dữ liệu đa (SPMD); xem, ví dụ, (Lepikhin et al., 2020). Dữ liệu và chuyên gia được phân chia chung giữa các thiết bị. Dữ liệu ban đầu trên thiết bị x nhưng được phân công, bởi hàm định tuyến, cho một chuyên gia trên thiết bị y phải được chuyển giữa các thiết bị thông qua giao tiếp tất cả-tới-tất cả. Dưới mô hình chương trình đơn trên các bộ gia tốc hiện đại, các chuyên gia gửi và nhận cùng một lượng dữ liệu và thực hiện cùng một lượng tính toán (cùng hình dạng mảng trên mỗi thiết bị). Việc triển khai MoWE hiệu quả sử dụng SPMD thông thường gặp một số thách thức chính: (1) Số lượng chuyên gia lớn mang lại chi phí không thực tế về mặt giao tiếp tất cả-tới-tất cả. (2) Tần suất từ tuân theo phân bố giống Zipfian. Bản chất không cân bằng của định tuyến được điều khiển bởi từ vựng yêu cầu các chuyên gia từ khác nhau xử lý nhiều token hơn các chuyên gia khác theo từng bậc độ lớn. Chúng tôi đề xuất một chiến lược mới vượt qua những thách thức này và cho phép triển khai hiệu quả lớp MoWE. Phương pháp của chúng tôi chứa ba thành phần chính:

Khối Chuyên gia. Chúng tôi nhóm các chuyên gia thành các khối được phân chia giữa các thiết bị. Giao tiếp tất cả-tới-tất cả chỉ được thực hiện giữa các khối thay vì giữa các chuyên gia. Miễn là chúng tôi giữ số lượng khối chuyên gia đủ nhỏ, chúng tôi có thể tăng số lượng chuyên gia mà không tăng chi phí giao tiếp tất cả-tới-tất cả. Ví dụ, nếu chúng tôi sử dụng 128 khối với 256 chuyên gia mỗi khối, chúng tôi có 32768 chuyên gia. Chúng tôi có thể sử dụng các khối chuyên gia vì hàm định tuyến cố định định trước khối nào, và chuyên gia nào trong khối, sẽ xử lý một token nhất định.

Phân nhóm Tần suất. Để vượt qua phân bố tần suất từ không cân bằng, chúng tôi tính tần suất của các từ trong một mẫu 2B token từ dữ liệu tiền huấn luyện và sau đó chia từ vựng định tuyến thành k nhóm, nơi các từ trong mỗi nhóm có tần suất gần như tương tự. Mỗi nhóm sau đó được xử lý bởi một tập hợp riêng biệt của các khối chuyên gia. Về mặt khái niệm, k lớp MoWE được thực thi song song. Với phương pháp này, các chuyên gia trong các nhóm khác nhau có thể có kích thước khác nhau hoặc thậm chí kiến trúc khác nhau và có thể hỗ trợ khả năng token khác nhau (xử lý số lượng token khác nhau).

Định tuyến Phân cấp. Cho một batch token, bước đầu tiên là định tuyến chúng tới các nhóm tần suất. Tiếp theo, trong mỗi nhóm, mỗi token được định tuyến tới khối chuyên gia chứa chuyên gia được phân công. Cuối cùng, trong khối, mỗi token được định tuyến tới và xử lý bởi một chuyên gia thực tế. Vì các quyết định định tuyến dựa hoàn toàn trên các id định tuyến (tĩnh), việc phân công token cho chuyên gia được biết trước và đường dẫn đầy đủ qua cây định tuyến phân cấp trở nên trivial. Hình 3 minh họa quá trình này.

Chiến lược đề xuất của chúng tôi cho phép chúng tôi tiền huấn luyện các mô hình MoWE-Base với tới 1 triệu chuyên gia (nhỏ) sử dụng 16 v3 TPUs. Chúng tôi không quan sát thấy bất kỳ sự bất ổn trong huấn luyện nào (ví dụ: gradient bùng nổ) thường được báo cáo trong tiền huấn luyện của các mô hình MoE thông thường (Zoph et al., 2022); chúng tôi nghi ngờ đây là một tác dụng phụ hữu ích của sơ đồ định tuyến cố định của chúng tôi.

2.4 Từ vựng Định tuyến Giàu Kiến thức

Một chiến lược đơn giản để xây dựng từ vựng định tuyến lớn bao gồm việc sử dụng tập dữ liệu tiền huấn luyện để huấn luyện một tokenizer SentencePiece từ vựng lớn (Kudo và Richardson, 2018). Tuy nhiên, các thí nghiệm ban đầu chỉ ra rằng phương pháp này không tối ưu vì nhiều từ trong từ vựng hóa ra không mang thông tin – nhiều từ chỉ là biến thể của dạng của các từ khác. Để xây dựng một từ vựng định tuyến giàu kiến thức chứa nhiều token thông tin hơn, chúng tôi rút ra từ vựng từ một tập dữ liệu giàu kiến thức như sau:

(1) Bắt đầu với tập hợp tất cả tên thực thể và quan hệ xuất hiện trong một bản dump Wikidata.
(2) Chữ thường và tách mỗi tên sử dụng khoảng trắng và regex để loại bỏ dấu câu.
(3) Sắp xếp các token dựa trên tần suất của chúng trong tập dữ liệu C4 (Raffel et al., 2020) (phiên bản 2.2.0), là tập dữ liệu tiền huấn luyện của chúng tôi.
(4) Chọn 1M token hàng đầu để tạo thành từ vựng định tuyến của chúng tôi.

Chiến lược này tăng khả năng phần lớn các mục trong từ vựng là tên (từ đơn) – tức là, các thuật ngữ mà chúng tôi muốn lưu trữ kiến thức về chúng. Ví dụ, tokenization với từ vựng T5.1.1 32K chia từ "mathematician" thành 5 token ("math", "e", "m", "a", "tician"), trong khi từ vựng định tuyến 1M của chúng tôi giữ nó như một token duy nhất; xem thêm Hình 2. Lý tưởng nhất, hai tokenization nên được căn chỉnh như trong hình, nhưng ràng buộc cứng duy nhất là mỗi token từ tokenization mặc định (xác định chuỗi đầu vào) cần có một id định tuyến. Phụ lục D hiển thị thêm mẫu của các từ hàng đầu trong từ vựng định tuyến.

Cuối cùng, để cho phép (a) tra cứu hiệu quả các id định tuyến và (b) việc sử dụng lớp MoWE trong các tình huống tự hồi quy nơi thông thường chỉ phần đầu của từ được biết, chúng tôi xấp xỉ tokenization định tuyến sử dụng một phép toán băm. Cụ thể hơn, chúng tôi sử dụng các bước sau:

• Ngoại tuyến: (1) chúng tôi mở rộng từ vựng phụ trợ bằng cách nối từ vựng T5 32K mặc định vào nó. (2) chúng tôi tokenize mỗi mục trong từ vựng phụ trợ sử dụng tokenizer mặc định và xây dựng một bảng băm nơi khóa là chuỗi các id token (mặc định) và giá trị là id định tuyến (một số tuần tự).

• Trực tuyến: cho một chuỗi đầu vào được tokenize s gồm n id token {t1, t2, ..., tn}, chúng tôi tạo id định tuyến của token ti bằng cách đầu tiên tra cứu trong bảng băm tất cả các chuỗi con {ti-k, ..., ti} cho k∈[0,8], và áp dụng id định tuyến của chuỗi con lớn nhất.

3 Thiết lập Thí nghiệm

3.1 Tác vụ và Tập dữ liệu

Chúng tôi trình bày kết quả trên một loạt các tác vụ NLP. Điều đó nói, vì mục tiêu chính của chúng tôi là đánh giá hiệu suất của MoWE trên các tác vụ đòi hỏi kiến thức chuyên sâu, chúng tôi tập trung phân tích vào các tác vụ trả lời câu hỏi sách đóng: TriviaQA (Joshi et al., 2017), WebQuestions (Berant et al., 2013) và Natural Questions (Kwiatkowski et al., 2019). Như trong Roberts et al. (2020), mô hình của chúng tôi không có quyền truy cập vào kiến thức/văn bản bên ngoài trong quá trình tinh chỉnh và suy luận. Tương tự như Lee et al. (2019); Roberts et al. (2020), chúng tôi thực hiện đánh giá bằng cách giữ lại 10% của tập huấn luyện

--- TRANG 5 ---
Model TriviaQA WebQuestions Natural Questions FEVER SuperGLUE Tỷ lệ Thời gian Huấn luyện
so với T5.1.1-Base
T5.1.1-Base 24.2 28.2 25.7 61.3 77.2 1.0
MoWE-Base 39.4 35.7 29.6 66.3 83.5 2.0
T5.1.1-Large 28.2 29.5 27.3 63.0 85.1 3.1
MoWE-Large 44.8 38.8 31.9 68.5 87.4 4.0
T5.1.1-XL 36.0 32.4 29.5 65.9 88.5 8.6
T5.1.1-XXL 42.9 35.6 32.8 67.5 89.9 26.4

Bảng 1: So sánh các mô hình MoWE và T5.1.1 trên năm tác vụ xử lý ngôn ngữ khác nhau. Chúng tôi sử dụng khớp chính xác cho TriviaQA, WebQuestions và Natural Questions. Chúng tôi sử dụng độ chính xác cho FEVER và trung bình pha trộn của độ chính xác và điểm F1 cho bộ SuperGLUE như trong (Raffel et al., 2020). Kết quả T5.1.1 cho TriviaQA, WebQuestions và Natural Questions từ (Roberts et al., 2020). Đối với mỗi mô hình, chúng tôi cũng báo cáo thời gian huấn luyện tương đối so với T5.1.1-Base.; được ước tính bằng cách chạy mỗi mô hình với kích thước batch 256 và độ dài chuỗi đầu vào (đầu ra) 512 (62) trên 64 v3 TPUs – lát cắt nhỏ nhất có thể vừa T5-XXL với 256 ví dụ. Lưu ý rằng điều này có thể đánh giá thấp tốc độ của các mô hình nhỏ hơn, vốn sẽ có hiệu quả sử dụng tốt hơn trên ít thiết bị hơn.

làm tập validation; các mô hình được tinh chỉnh trên 90% còn lại của dữ liệu.

Chúng tôi cũng kiểm tra hiệu suất của MoWE cho tác vụ xác minh tuyên bố sử dụng tập dữ liệu FEVER (Thorne et al., 2018), chứa các tập validation và test riêng biệt. Cuối cùng, để so sánh kết quả của chúng tôi với các mô hình Transformer MoE cổ điển (Lepikhin et al., 2020), chúng tôi áp dụng MoWE cho benchmark SuperGLUE (Wang et al., 2019b). Chúng tôi tiền huấn luyện tất cả các mô hình trên tập dữ liệu C4 (Raffel et al., 2020), phiên bản 2.2.0.

3.2 Thiết lập MoWE và Siêu tham số

Theo các mô hình MoE encoder-decoder dựa trên Transformer phổ biến (Fedus et al., 2022) và tiên tiến (Zoph et al., 2022), chúng tôi sử dụng T5.1.1 làm khung xương cho các mô hình MoWE của chúng tôi.

Kết quả chính của chúng tôi từ một kiến trúc có bốn lớp MoWE – hai trong encoder và hai trong decoder, và mỗi lớp MoWE chứa 32K chuyên gia. Chúng tôi sử dụng bốn lớp MoWE vì chúng cung cấp độ chính xác tốt mà không hy sinh hiệu suất tính toán do chi phí định tuyến (xem Phụ lục 8). Chúng tôi đặt các lớp MoWE gần giữa encoder (decoder) để đảm bảo rằng: (1) các lớp MoWE nhận được biểu diễn của token đã được ngữ cảnh hóa một phần; (2) sau lớp MoWE, vẫn còn nhiều Khối Transformer có thể hưởng lợi từ đầu ra của lớp đó. Các tham số được chia sẻ giữa tất cả các lớp MoWE với mục tiêu sau: (1) nó làm cho lớp MoWE giống hơn với một bộ nhớ được truy cập tại các điểm khác nhau của mạng; (2) chúng tôi có thể giữ tổng số tham số thưa thớt tương đối thấp mà không cần giảm tổng số và kích thước của các chuyên gia. Ngoài ra, kết quả thực nghiệm cho thấy việc chia sẻ tham số giữa các lớp MoWE dẫn đến hiệu suất tốt hơn. Từ vựng định tuyến có 2^20 (~1M) mục và được xây dựng như mô tả trong Mục 2.4. Các mô hình MoWE-Base và MoWE-Large có 31B và 45.5B tham số, tương ứng. Xem Phụ lục A để biết thêm chi tiết.

Tiền huấn luyện được thực hiện sử dụng cùng phương pháp che span được sử dụng trong T5 (Raffel et al., 2020). Theo các mô hình T5, kết quả chính của chúng tôi sử dụng các mô hình MoWE được tiền huấn luyện cho khoảng 1 nghìn tỷ token – 1M bước, với kích thước batch 2048 và độ dài chuỗi đầu vào 512 token; độ dài chuỗi đích là 114. Chúng tôi sử dụng cùng siêu tham số tiền huấn luyện của T5.1.1, và sử dụng 64 TPUs v3 cho tiền huấn luyện.

Trong quá trình tinh chỉnh cho các tác vụ downstream, chúng tôi đóng băng tất cả các chuyên gia MoWE để tránh cả overfitting và quên thảm khốc kiến thức thu được trong quá trình tiền huấn luyện (Xem Phụ lục B.0.3 cho các thí nghiệm ablation). Đây là một sự khác biệt quan trọng so với các mô hình MoE, vốn tinh chỉnh các chuyên gia cho các tác vụ downstream. Siêu tham số chính mà chúng tôi điều chỉnh trong quá trình tinh chỉnh là tỷ lệ học. Chúng tôi chỉ sử dụng mất mát cross-entropy; không sử dụng mất mát phụ trợ bổ sung.

4 Kết quả Thí nghiệm và Thảo luận

4.1 So sánh với T5.1.1

Trong Bảng 1, chúng tôi tóm tắt kết quả MoWE trên 5 tác vụ NLP khác nhau và cùng với các mô hình T5.1.1. MoWE-Base và MoWE-Large vượt trội so với T5.1.1-Base và T5.1.1-Large, tương ứng, trên tất cả năm tác vụ. Có cải thiện đáng kể về hiệu suất cho các tác vụ đòi hỏi kiến thức chuyên sâu – đặc biệt cho TriviaQA, WebQuestions và FEVER. Trên

--- TRANG 6 ---
Model # lớp thưa thớt # chuyên gia trên mỗi lớp Chiều MLP chuyên gia trung bình # tham số Tham số được chia sẻ? TQA WQ NQ SG
MoE-Top2 12 32 2048 2B Không 26.5 27.7 25.8 80.2
MoWE 4 8K 141 2B Có 29.8 31.6 26.0 81.2
MoE-Top2 12 512 2048 29.2B Không 36.2 31.6 28.5 83.5
MoWE 4 32K 577 31B Có 39.4 35.7 29.6 83.5

Bảng 2: So sánh MoWE-Base với các mô hình MoE thông thường trên TriviaQA (TQA), WebQuestions (WQ), Natural Questions (NQ) và SuperGLUE (SG). Các mô hình MoE-Top2 dựa trên GShard Top-2 MoE Transformer chuẩn (Lepikhin et al., 2020).

TriviaQA, MoWE-Base vượt trội so với T5.1.1-Base 15.2 điểm về khớp chính xác, tương ứng với cải thiện 62.8%. Trên cùng tập dữ liệu, MoWE-Large vượt trội so với T5.1.1-Large khoảng 16.6 điểm.

Đáng chú ý, MoWE-Base vượt trội so với T5.1.1-XL trên tất cả các tác vụ đòi hỏi kiến thức chuyên sâu, trong khi đạt được tăng tốc huấn luyện tương đối 4.3x. Tương tự, trên các tác vụ tương tự, MoWE-Large vượt trội hoặc có kết quả cạnh tranh với T5.1.1-XXL, trong khi đạt được tăng tốc huấn luyện tương đối 6.6x.

4.2 So sánh với MoE Thông thường

Bảng 2 so sánh các mô hình MoWE với GShard Top-2 MoE Transformer chuẩn (Lepikhin et al., 2020). Chúng tôi sử dụng T5-Base làm khung xương cho tất cả các mô hình trong Bảng 2, do đó chúng có # FLOPS tương tự với T5-Base. Tất cả các mô hình trong bảng được huấn luyện cho 1M bước với kích thước batch 2048. Bảng 2 cũng nêu bật một số khác biệt kiến trúc giữa MoWE và MoE thông thường. MoE thông thường sử dụng số lượng lớp thưa thớt lớn hơn, mỗi lớp có số lượng chuyên gia nhỏ và không có chia sẻ tham số giữa các lớp. Trong MoWE, vì các chuyên gia được gắn với từ vựng định tuyến và chúng tôi muốn khuyến khích chuyên môn hóa chuyên gia, chúng tôi sử dụng số lượng lớn chuyên gia. Việc chia sẻ tham số chuyên gia giữa các lớp MoWE cho phép sử dụng số lượng lớn chuyên gia mà không làm bùng nổ tổng số tham số.

Trong phần trên của Bảng 2, chúng tôi so sánh MoWE với kiến trúc MoE-Top2 điển hình trong đó mỗi lớp khác là thưa thớt và mỗi lớp thưa thớt chứa 32 chuyên gia, tạo ra mô hình 2B tham số; xem Phụ lục C để biết chi tiết. Để so sánh công bằng MoWE với mô hình này, chúng tôi đã tạo một phiên bản MoWE-Base chứa 2B tham số bằng cách giảm số lượng chuyên gia từ 32K xuống 8K và giảm kích thước chuyên gia; xem Phụ lục A.1 để biết chi tiết. Ở quy mô 2B, MoWE vượt trội so với MoE-Top2 cho tất cả bốn tác vụ. Trong phần dưới của Bảng 2, chúng tôi so sánh mô hình MoWE-Base 31B với phiên bản MoE-Top2 sử dụng 512 chuyên gia mỗi lớp thưa thớt và chứa 29.2B tham số. MoWE hoạt động tốt hơn đáng kể trên các tác vụ đòi hỏi kiến thức chuyên sâu, trong khi đạt hiệu suất tương tự trên SuperGLUE. Chúng tôi tin rằng hiệu suất vượt trội của MoWE cho các tác vụ đòi hỏi kiến thức chuyên sâu đến từ chiến lược sử dụng từ vựng giàu kiến thức lớn để thực hiện định tuyến, như được khám phá thêm trong các thí nghiệm ablation được trình bày trong Mục 4.5.

4.3 Lớp MoWE là một Bộ nhớ Thưa thớt

Chúng tôi thực hiện một thí nghiệm để đánh giá mức độ mà một mô hình MoWE dựa vào lớp MoWE để thực hiện tác vụ TriviaQA. Cụ thể, chúng tôi quan tâm đến việc đo lường tác động của việc vô hiệu hóa các chuyên gia của các từ liên quan khi mô hình đang tạo ra câu trả lời. Sau đó chúng tôi tinh chỉnh mô hình này trong một trong hai chế độ: (1) tất cả chuyên gia được kích hoạt: đây là thiết lập tinh chỉnh và suy luận thông thường của chúng tôi nơi tất cả các token đầu vào được xử lý bởi các chuyên gia tương ứng trong lớp MoWE; (2) một số chuyên gia bị vô hiệu hóa: chúng tôi vô hiệu hóa các chuyên gia của các token có id định tuyến >32K trong quá trình tinh chỉnh và suy luận. Chúng tôi đặt ngưỡng là 32K vì 32K id định tuyến đầu tiên tương ứng gần đúng với các token thường xuyên và ít hướng kiến thức thu được từ việc nối từ vựng mặc định với từ vựng phụ trợ (xem Mục 2.4 để biết thêm chi tiết).

Vô hiệu hóa Chuyên gia Có lựa chọn TriviaQA EM
Không 35.1
Có 25.6

Bảng 3: Tác động lên khớp chính xác TriviaQA của việc vô hiệu hóa các chuyên gia của các token có id định tuyến >32K.

Bảng 3 hiển thị hiệu suất của MoWE cho các thiết lập (1) và (2). Có sự sụt giảm đáng kể 9 điểm EM khi các chuyên gia của các từ có id định tuyến >32K bị vô hiệu hóa. Kết quả này cho thấy rằng

--- TRANG 7 ---
Câu hỏi Vô hiệu hóa chuyên gia của các từ được làm nổi bật khi tạo ra câu trả lời?
Không Có
Tên của album đầu tiên của Adele là gì? 19 Addiction
Ai đã kế nhiệm William Taft làm Tổng thống Mỹ? Woodrow Wilson James Garfield
Quinsy ảnh hưởng đến bộ phận nào của cơ thể con người? Amidan Chân
Quốc gia nào sẽ đăng cai FIFA World Cup 2022? Qatar Brazil
Vệ tinh chính của Sao Hải Vương là gì? Triton Thiên Vương
Tên của chính khách và nhà văn Ý Machiavelli là gì? Niccolo Francois
Almeria, Merlot, và Waltham Cross là loại trái cây nào? Nho Táo

Bảng 4: Ví dụ về các câu hỏi TriviaQA và câu trả lời tương ứng từ hai cấu hình của mô hình MoWE-Base đã được tiền huấn luyện tùy thuộc vào việc chúng tôi có vô hiệu hóa chuyên gia tương ứng với id định tuyến của các từ được làm nổi bật hay không. Câu trả lời được tạo ra bởi mô hình có thể thay đổi hoàn toàn (từ đúng sang sai trong những trường hợp này) chỉ bằng cách đơn giản vô hiệu hóa chuyên gia MoWE của một từ liên quan duy nhất. Trong thí nghiệm này, mô hình MoWE có một lớp MoWE duy nhất được đặt trong encoder và chứa 32K chuyên gia.

các mô hình MoWE phụ thuộc rất nhiều vào các chuyên gia của các từ có trong từ vựng giàu kiến thức của chúng tôi. Trong Bảng 4, chúng tôi hiển thị một số ví dụ được chọn về các câu hỏi và câu trả lời tương ứng cho hai thiết lập. Việc vô hiệu hóa một chuyên gia duy nhất khiến mô hình trả lời câu hỏi theo cách hoàn toàn khác. Đối với mô hình MoWE được sử dụng trong thí nghiệm này, một chuyên gia duy nhất đại diện cho chỉ 0.33% của tổng số tham số được kích hoạt ước tính. Lưu ý rằng, vì lớp MoWE được đóng băng trong quá trình tinh chỉnh, tất cả kiến thức được tận dụng trong tác vụ downstream đều đến từ corpus tiền huấn luyện. Những kết quả này cho thấy rằng (ít nhất một phần) kiến thức thế giới tiền huấn luyện cần thiết để trả lời một số câu hỏi được lưu trữ trong các chuyên gia bị vô hiệu hóa.

4.4 So sánh với các mô hình Tăng cường Bộ nhớ

Trong phần này, chúng tôi so sánh hiệu suất của MoWE với các mô hình tăng cường bộ nhớ được đề xuất gần đây: Entities as Experts (EaE) (Févry et al., 2020) và Transformer Over Mention Encodings (TOME) (de Jong et al., 2022) trên hai tác vụ đòi hỏi kiến thức chuyên sâu. Những mô hình này được tiền huấn luyện trên dữ liệu Wikipedia sử dụng các mất mát nhận biết thực thể, và thành phần bộ nhớ của chúng tập trung chủ yếu vào lĩnh vực đó. Để làm cho các mô hình MoWE chuyên biệt hơn một chút trên lĩnh vực Wikipedia, vốn được biết là có lợi cho các tác vụ như TriviaQA, chúng tôi đã làm theo (Roberts et al., 2020) và sử dụng dữ liệu Salient Spam Masking (SSM) từ (Guu et al., 2020) để thực hiện thêm 40K bước tiền huấn luyện.

Chúng tôi tóm tắt kết quả thí nghiệm trong Bảng 5. Mô hình MoWE-Base vượt trội so với EaE trên cả hai tập dữ liệu. Mô hình MoWE-Large vượt trội so với cả hai baseline trên FEVER và có hiệu suất tương tự hoặc cạnh tranh với các mô hình TOME trên TriviaQA.

Các mô hình EaE và TOME có thể được xem là các giải pháp tùy chỉnh hơn cho những tác vụ này. Ví dụ, EaE và TOME giải quyết TriviaQA như một tác vụ liên kết thực thể, nơi một tập đóng 1M thực thể Wikipedia được sử dụng để xếp hạng. Ngược lại, MoWE thực hiện tạo ra câu trả lời mở, linh hoạt hơn nhưng cũng thách thức hơn. Ngoài ra, cả EaE và TOME đều sử dụng các quy trình huấn luyện chuyên biệt, bao gồm thêm các hàm mất mát bổ sung và phân đoạn thực thể hoặc cụm danh từ, và yêu cầu các công cụ k-nn để tìm kiếm các embedding liên quan trong bộ nhớ của chúng. Trong các mô hình MoWE, "bộ nhớ thưa thớt" được tích hợp vào khung xương mô hình và được truy cập liền mạch như bất kỳ tham số mô hình nào khác. Kết quả là, MoWE có thể được huấn luyện theo cách tương tự như mô hình T5 mà không cần công cụ/mô hình bên ngoài.

Model TQA FEVER
EaE 43.2 66.1 / 63.6
TOME 1 50.8 70.5 / 67.8
TOME 2 54.6 71.1 / 68.1
MoWE-Base + SSM 44.9 69.1 / 66.9
MoWE-Large + SSM 50.2 70.5 / 68.7

Bảng 5: So sánh MoWE với EaE và TOME. Kết quả cho cả hai mô hình từ (de Jong et al., 2022). Kết quả cho TQA là dev, trong khi FEVER là dev/test. TOME 1 sử dụng hai lớp bộ nhớ và TOME 2 sử dụng hai.

--- TRANG 8 ---
4.5 Hiệu quả của Từ vựng Định tuyến Hướng Kiến thức

Trong phần này, chúng tôi hiển thị bằng chứng để hỗ trợ phỏng đoán của chúng tôi rằng định tuyến với từ vựng giàu kiến thức lớn dẫn đến hiệu suất tốt hơn bằng cách thay đổi kích thước của từ vựng định tuyến. Đối với các thí nghiệm trong phần này, chúng tôi sử dụng cấu hình mô hình MoWE baseline với khung xương T5.1.1-Base cố định với 32K chuyên gia, tạo ra 15.5B tham số thưa thớt. Đối với các từ vựng nhỏ hơn 1M, chúng tôi sử dụng K từ hàng đầu (theo tần suất trong tập dữ liệu C4) từ từ vựng định tuyến 1M của chúng tôi được mô tả trong Mục 2.4. Chúng tôi báo cáo kết quả chủ yếu trên các tập dữ liệu TriviaQA và Natural Questions và chúng tôi sử dụng metric F1 thay vì khớp chính xác vì nó ít nhiễu hơn một chút và làm nổi bật xu hướng rõ ràng hơn.

Hình 4 cho thấy kết quả được cải thiện dần dần khi chúng tôi tăng từ vựng định tuyến. Những cải thiện này rõ ràng hơn khi huấn luyện lâu hơn; xem Hình 5. Khi chúng tôi tăng kích thước từ vựng định tuyến, chúng tôi tăng thiên hướng quy nạp dựa trên từ vựng được tiêm vào mô hình qua hàm định tuyến. Đối với TriviaQA, có cải thiện ~2 điểm F1 khi sử dụng từ vựng định tuyến có kích thước trên 262K. Xem Phụ lục B để biết thêm các thí nghiệm ablation về số lượng chuyên gia được sử dụng.

32K 65K 131K 262K 524K 1M
34 34.5 35 35.5 36
34.3 34.7 34.9 35.6
35.2 35.3

Kích thước Từ vựng Định tuyến
TriviaQA F1

Hình 4: Hiệu suất trên TriviaQA với các kích thước từ vựng định tuyến khác nhau. Những mô hình này được tiền huấn luyện cho 200K bước huấn luyện.

5 Công trình Liên quan

Các mô hình Hỗn hợp Chuyên gia (MoE) kích hoạt thưa thớt (Shazeer et al., 2017) tăng số lượng tham số với chi phí tính toán tăng dưới tuyến tính (FLOPs) bằng cách kích hoạt thưa thớt các module ("chuyên gia"). Gần đây, các mô hình MoE dựa trên Transformer đã đạt được hiệu suất tiên tiến và lợi ích hiệu quả trong ngôn ngữ (Lepikhin et al., 2020; Fedus et al., 2022; Du et al., 2022; Artetxe et al., 2021; Zoph et al., 2022), thị giác (Riquelme et al., 2021) và đa phương thức (Mustafa et al., 2022).

Trái ngược với các mô hình MoE nói trên, MoWE sử dụng hàng chục nghìn chuyên gia; Du et al. (2022), ví dụ, thấy hiệu suất giảm dần trong các mô hình MoE của họ vượt quá khoảng 64 hoặc 128 chuyên gia. Để hỗ trợ nhiều chuyên gia hơn, MoWE sử dụng sơ đồ định tuyến cố định, không giống như các mô hình vanilla đều dựa vào các cơ chế định tuyến top-k học được để phân công token → chuyên gia, hoặc Zhou et al. (2022) sử dụng phân công chuyên gia → token top-k học được. Hàm định tuyến MoWE phân công token cho các chuyên gia riêng lẻ dựa trên id token của chúng trong từ vựng phụ trợ. Điều này gợi nhớ đến Hash Layers (Roller et al., 2021), phân công token cho chuyên gia dựa trên phân nhóm băm cố định, với sự khác biệt là nhiều id token khác nhau, dựa trên từ vựng embedding, được phân nhóm cùng nhau và phân công cho các chuyên gia riêng lẻ. Là hệ quả của việc tăng số lượng chuyên gia, chúng tôi đóng băng các chuyên gia MoWE trong quá trình tinh chỉnh để tránh cả overfitting và quên thảm khốc kiến thức thu được trong quá trình tiền huấn luyện.

Trong các triển khai SPMD MoE tiêu chuẩn, các chuyên gia có bộ đệm khả năng cố định và do đó chỉ có thể xử lý một phần cố định của các token đầu vào, vì vậy hầu hết các mô hình định tuyến top-k triệu hồi mất mát cân bằng tải phụ trợ (Shazeer et al., 2017) để khuyến khích phân bố đều token giữa các chuyên gia. Vì định tuyến được cố định, các bộ đệm khả năng chuyên gia MoWE có thể được định kích thước theo tần suất token mong đợi. Nghiên cứu gần đây, như Gale et al. (2023) nới lỏng các ràng buộc bộ đệm chuyên gia với các "khối" bộ đệm chuyên gia biến đổi.

Các mô hình MoWE bắc cầu khoảng cách giữa các mô hình MoE và các mô hình Tăng cường Bộ nhớ, như Mention Memory (de Jong et al., 2022), FILM

32K 262K 1M
32 34 36 38 40 42
37.2 39.1 39
32.8 33.9 34.3

Kích thước Từ vựng Định tuyến
F1
TriviaQA Natural Questions

Hình 5: Hiệu suất trên TriviaQA và Natural Questions với các kích thước từ vựng định tuyến khác nhau. Những mô hình này được tiền huấn luyện 1M bước huấn luyện (dài hơn Hình 4).

--- TRANG 9 ---
(Verga et al., 2021), Entities as Experts (Févry et al., 2020) và Knowledge Prompts (dos Santos et al., 2022), gọi một ngân hàng bộ nhớ khi xử lý đầu vào. Các mô hình bộ nhớ đã chứng minh hiệu quả trong các tác vụ đòi hỏi kiến thức chuyên sâu nhưng có thể có một số nhược điểm: (1) Chúng thường yêu cầu quy trình huấn luyện chuyên biệt, khác với các mô hình dày đặc, để học cách sử dụng hiệu quả bộ nhớ "bên ngoài". (2) Dữ liệu huấn luyện thường rất cụ thể về lĩnh vực (hầu hết các trường hợp tập trung vào Wikipedia) và kết quả là, mỗi mô hình chỉ có thể được áp dụng cho các tác vụ hưởng lợi từ dữ liệu đó.

Mặt khác, MoWE đơn giản để huấn luyện – không có mất mát bổ sung và không cần học tìm kiếm bộ nhớ. Nó tích hợp liền mạch với mô hình vì không cần thực hiện tìm kiếm sử dụng công cụ kiểu nearest neighbor trong quá trình suy luận hoặc huấn luyện; định tuyến được xác định trước tránh hoàn toàn việc tìm kiếm này. Các mô hình MoWE có thể được huấn luyện trên dữ liệu tiền huấn luyện chung (C4 trong trường hợp của chúng tôi). Liên kết giữa các mô hình tăng cường bộ nhớ và MoWE, là các thực thể được mã hóa vào mô hình khi được xác định với các chuyên gia cụ thể. Tuy nhiên, không giống như các mô hình bộ nhớ, các chuyên gia/thực thể là các mạng thần kinh nhỏ chứ không phải embedding.

6 Kết luận

Chúng tôi đã trình bày MoWE, một kiến trúc mạng thần kinh mới nội suy giữa hiệu quả của các mô hình MoE kích hoạt thưa thớt dựa trên phép nhân ma trận và các mô hình tăng cường bộ nhớ. Các mô hình MoWE đặc biệt hiệu quả tại các tác vụ đòi hỏi kiến thức chuyên sâu yêu cầu ghi nhớ và truy xuất kiến thức thế giới. Công trình của chúng tôi mang đến những phát hiện mới quan trọng về việc sử dụng các hàm định tuyến hướng từ vựng trong MoE, và hy vọng mời gọi nghiên cứu tương lai về các chuyên gia từ.

Tài liệu tham khảo

Oshin Agarwal, Heming Ge, Siamak Shakeri, và Rami Al-Rfou. 2021. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 3554–3565, Online. Association for Computational Linguistics.

Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. 2021. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684.

Jonathan Berant, Andrew Chou, Roy Frostig, và Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, trang 1533–1544, Seattle, Washington, USA. Association for Computational Linguistics.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, và Qiao Zhang. 2018. JAX: composable transformations of Python+NumPy programs.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Language models are few-shot learners.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.

Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, và Furu Wei. 2022. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 8493–8502, Dublin, Ireland. Association for Computational Linguistics.

Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, và William W. Cohen. 2022. Mention

--- TRANG 10 ---
memory: incorporating textual knowledge into transformers through entity mention attention. In International Conference on Learning Representations.

Cicero Nogueira dos Santos, Zhe Dong, Daniel Cer, John Nham, Siamak Shakeri, Jianmo Ni, và Yun hsuan Sung. 2022. Knowledge prompts: Injecting world knowledge into language models through soft prompts.

Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, trang 5547–5569. PMLR.

William Fedus, Barret Zoph, và Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.

Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, và Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 4937–4951, Online. Association for Computational Linguistics.

Trevor Gale, Deepak Narayanan, Cliff Young, và Matei Zaharia. 2023. Megablocks: Efficient sparse training with mixture-of-experts. Proceedings of Machine Learning and Systems, 5.

Mor Geva, Roei Schuster, Jonathan Berant, và Omer Levy. 2021. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 5484–5495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Ming-Wei Chang. 2020. REALM: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909.

Mandar Joshi, Eunsol Choi, Daniel Weld, và Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 1601–1611, Vancouver, Canada. Association for Computational Linguistics.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. 2020. Scaling laws for neural language models.

Taku Kudo và John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, trang 66–71, Brussels, Belgium. Association for Computational Linguistics.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, và Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466.

Kenton Lee, Ming-Wei Chang, và Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, trang 6086–6096, Florence, Italy. Association for Computational Linguistics.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, và Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.

Kevin Meng, David Bau, Alex J Andonian, và Yonatan Belinkov. 2022. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems.

Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, và Neil Houlsby. 2022. Multimodal contrastive learning with limoe: the language-image mixture of experts. arXiv preprint arXiv:2206.02770.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.

Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, và Neil Houlsby. 2021. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:8583–8595.

Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. 2022. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 13.

Adam Roberts, Colin Raffel, và Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 5418–5426, Online. Association for Computational Linguistics.

Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, và Jason E Weston. 2021. Hash layers for large sparse models. In Advances in Neural Information Processing Systems.

--- TRANG 11 ---
Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, và Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations.

James Thorne, Andreas Vlachos, Christos Christodoulopoulos, và Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), trang 809–819, New Orleans, Louisiana. Association for Computational Linguistics.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. 2023. Llama: Open and efficient foundation language models.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS'17, trang 6000–6010, Red Hook, NY, USA. Curran Associates Inc.

Pat Verga, Haitian Sun, Livio Baldini Soares, và William Cohen. 2021. Adaptable and interpretable neural MemoryOver symbolic knowledge. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 3678–3691, Online. Association for Computational Linguistics.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R. Bowman. 2019b. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. Curran Associates Inc., Red Hook, NY, USA.

Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, và Colin Raffel. 2022. Byt5: Towards a token-free future with pre-trained byte-to-byte models.

Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, và Jie Zhou. 2022. MoEfication: Transformer feed-forward layers are mixtures of experts. In Findings of the Association for Computational Linguistics: ACL 2022, trang 877–890, Dublin, Ireland. Association for Computational Linguistics.

Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le, và James Laudon. 2022. Mixture-of-experts with expert choice routing.

Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, và William Fedus. 2022. St-moe: Designing stable and transferable sparse expert models.

A Thiết lập MoWE

Các thí nghiệm chính của chúng tôi trên MoWE-Base và MoWE-Large sử dụng kiến trúc với bốn lớp MoWE tổng cộng. Hai trong encoder và hai trong decoder và tham số được chia sẻ giữa tất cả các lớp MoWE. Những lớp này được đặt tại các khối Transformer 5 và 10 trong mô hình Base, và tại các khối 9 và 17 trong mô hình large. Chúng tôi đặt các lớp MoWE hướng về giữa encoder (decoder) vì: (1) chúng nhận được biểu diễn của token đã được ngữ cảnh hóa một phần; (2) sau lớp MoWE, vẫn còn nhiều Khối Transformer có thể hưởng lợi từ đầu ra của lớp đó. Trừ khi có thông báo khác, mỗi lớp MoWE chứa 32K chuyên gia và từ vựng định tuyến có ~1M mục.

Triển khai hiện tại của MoWE được viết bằng Jax (Bradbury et al., 2018) trên khung T5X (Roberts et al., 2022).

Một số cấu hình bổ sung được cung cấp trong các phần sau.

A.1 Cấu hình của Nhóm Tần suất, Khối Chuyên gia và Chuyên gia

Chúng tôi chia từ vựng thành bốn nhóm tần suất khác nhau. Tần suất token được tính toán sử dụng một mẫu từ tập dữ liệu tiền huấn luyện của chúng tôi. Lớp MoWE không xử lý 16 token thường xuyên nhất trong từ vựng định tuyến, tức là những id token đó không bao giờ được định tuyến tới một chuyên gia. Những token này là dấu câu và các từ không nội dung khác và chúng tôi ước tính chúng có thể đại diện cho tới 28% các token trong một batch. Điều này tăng tốc thời gian huấn luyện và không làm tổn hại hiệu suất downstream, vì những token này không phải là từ nội dung. Cấu hình của bốn nhóm tần suất được mô tả trong Bảng 6. Sử dụng cấu hình này, chúng tôi có được mô hình với ~31B tham số trong trường hợp mô hình Base và ~45.5B tham số thưa thớt trong trường hợp mô hình Large. Sự khác biệt về số lượng tham số là do việc sử dụng các chiều hình chiếu MLP khác nhau (xem Bảng 6) và kích thước token embedding, là 768 trong Base và 1024 trong Large.

Lưu ý trong Bảng 6 rằng đối với các nhóm 1 đến 3, chúng tôi sử dụng một chuyên gia mỗi token. Trong cấu hình này, trong nhóm 4, các chuyên gia được chia sẻ cho nhiều token. Nhóm này chứa chủ yếu các token tần suất thấp, là đa số trong từ vựng. Ngoài ra, do số lượng lớn chuyên gia trong nhóm này, các Khối Chuyên gia được triển khai như bảng tra cứu. Mặc dù chúng tôi tin rằng cấu hình hiện tại không tối ưu và có thể được cải thiện, nó đã tạo ra các mô hình hiệu quả.

Trong Bảng 7, chúng tôi chi tiết cấu hình của bốn nhóm tần suất và số lượng chuyên gia tương ứng và kích thước cho mô hình MoWE-Base với 2B tham số mà chúng tôi đề cập trong Mục 4.2.

A.2 Siêu tham số bổ sung

Để tiền huấn luyện các mô hình MoWE, chúng tôi sử dụng các siêu tham số T5x mặc định cho T5.1.1. Trừ khi có đề cập khác, tiền huấn luyện được thực hiện cho khoảng 1 nghìn tỷ token – 1M bước, với kích thước batch 2048 và độ dài chuỗi đầu vào 512 token; độ dài chuỗi đích là 114.

Đối với tác vụ downstream, chúng tôi thường sử dụng kích thước batch 256 hoặc 512. Đối với hầu hết các tập dữ liệu, tỷ lệ học 1e-4 và tỷ lệ dropout 0.05 cho kết quả tốt nhất. Ngoại lệ chính là các tập dữ liệu SuperGLUE và Fever, hoạt động tốt hơn với LR giữa 1e-3 và 5e-4.

B Thí nghiệm ablation bổ sung

Trong phần này, chúng tôi trình bày các thí nghiệm ablation bổ sung về các lựa chọn kiến trúc khác nhau của MoWE. Trong tất cả các thí nghiệm, chúng tôi tiền huấn luyện các mô hình cho 200K bước.

B.0.1 Tác động của Số lượng Chuyên gia

Chúng tôi trình bày hai thí nghiệm bổ sung về cách số lượng chuyên gia ảnh hưởng đến hiệu suất MoWE. Đầu tiên, chúng tôi kiểm tra tác động của việc thay đổi số lượng chuyên gia giữa 16K, 32K và 64K trong khi giữ cố định từ vựng định tuyến ở kích thước 1M và kích thước mô hình ở 15.5B. Trong Hình 6, chúng tôi thấy rằng 32K chuyên gia dường như là điểm tối ưu về số lượng chuyên gia cho MoWE. Việc sử dụng số lượng lớn hơn các chuyên gia nhỏ hơn là tốt hơn vì hiệu quả hơn về bộ nhớ và cũng tăng tốc triển khai bảng tra cứu của chúng tôi của các Khối Chuyên gia trong nhóm tần suất 4.

16k 32k 64k
34.6 34.8 35 35.2 35.4
35.2 35.3
34.7

Tổng Số Chuyên gia
TriviaQA F1

Hình 6: Hiệu suất trên TriviaQA của các mô hình MoWE-baseline khác nhau nơi chúng tôi cố định từ vựng định tuyến ở kích thước 1M và thay đổi số lượng chuyên gia.

32K 65K 131K 262K 524K 1M
33 34 34.5
34.2 34.1 34
33.7
32.6

Số Chuyên gia == Kích thước Từ vựng Định tuyến
TriviaQA F1

Hình 7: Hiệu suất trên TriviaQA của các mô hình MoWE-baseline khác nhau nơi số lượng chuyên gia khớp với từ vựng định tuyến.

Tiếp theo, chúng tôi đánh giá hiệu suất MoWE khi chúng tôi tăng số lượng chuyên gia để khớp với kích thước của từ vựng định tuyến lớn. Chúng tôi giữ tổng số tham số thưa thớt cố định bằng cách giảm kích thước của các chuyên gia trong mỗi thí nghiệm. Do đó, khi sử dụng 1M chuyên gia, chiều MLP của mỗi chuyên gia là 8, trong khi chiều hình chiếu MLP khi sử dụng 32K chuyên gia là 256. Theo hiểu biết của chúng tôi, đây là lần đầu tiên mô hình MoE dựa trên Transformer được huấn luyện với tới một triệu chuyên gia, chứng minh rằng các giải pháp đề xuất của chúng tôi để triển khai MoWE khá mạnh mẽ.

Trong Hình 7, chúng tôi hiển thị kết quả cho việc tăng MoWE-baseline lên tới 1M chuyên gia. Chúng tôi thấy sự suy giảm hiệu suất dần dần khi khớp số lượng chuyên gia với kích thước từ vựng. Chúng tôi tin rằng điều này chủ yếu do hai yếu tố: (1) số lượng cập nhật huấn luyện mà mỗi chuyên gia nhận được trở nên ngày càng thưa thớt; (2) kích thước của các chuyên gia được giảm.

B.0.2 Tác động của số lượng Lớp MoWE

Trong Bảng 8, chúng tôi hiển thị tác động của việc sử dụng số lượng Lớp MoWE khác nhau trong encoder và decoder. Tất cả các mô hình được huấn luyện cho 200K bước. Chúng tôi có thể thấy trong Bảng 8 rằng việc đi từ một lên hai lớp trong encoder mang lại cải thiện đáng kể về EM (31.0 -> 31.6). Tuy nhiên, việc đi từ 2 lên 3 lớp không mang lại cải thiện về EM. Việc thêm các lớp MoWE vào decoder cải thiện hiệu suất, đặc biệt khi sử dụng 2 lớp trong encoder.

# Lớp MoWE EM F1
Encoder Decoder
1 0 31.0 36.3
2 0 31.6 36.9
3 0 31.5 37.1
1 1 31.4 36.7
2 1 32.4 37.5
2 2 33.1 38.4

Bảng 8: Tác động của số lượng lớp MoWE trên TriviaQA cho mô hình Base. Tham số chuyên gia được chia sẻ giữa các lớp MoWE trong encoder.

Trong Bảng 9, chúng tôi hiển thị kết quả về việc sử dụng kích thước chuyên gia khác nhau trong mỗi trong số bốn kích thước bucket. Một lớp MoWE duy nhất được sử dụng, và nó được đặt trong encoder. Chúng tôi bắt đầu với cấu hình nơi các chuyên gia trong Bucket 1 có các chuyên gia với chiều MLP 512, và tuần tự giảm một nửa giá trị cho bucket liên tiếp tiếp theo. Điều này tạo ra mô hình với 3.9B tham số thưa thớt, có hiệu suất trên TriviaQA được trình bày trong hàng đầu tiên của Bảng 9. Trong các hàng tiếp theo, chúng tôi liên tiếp tăng gấp đôi kích thước chuyên gia trong mỗi bucket, gấp đôi tổng số tham số thưa thớt. Có cải thiện nhất quán 1 điểm EM khi tăng gấp đôi kích thước mô hình. Chúng tôi tin rằng sự tăng sẽ lớn hơn nếu chúng tôi tiền huấn luyện mô hình cho 1M bước thay vì 200K bước.

# tham số thưa thớt Chiều MLP của Chuyên gia trong mỗi Nhóm Tần suất EM F1
B1 B2 B3 B4
3.9B 512 256 128 64 28.5 33.7
7.8B 1024 512 256 128 29.6 34.8
15.5B 2048 1024 512 256 30.0 35.3
31.0B 2048 2048 1024 512 31.0 36.3

Bảng 9: Tác động trên TriviaQA EM và F1 của việc sử dụng kích thước chuyên gia khác nhau trong bốn bucket khác nhau.

B.0.3 Đóng băng vs Không đóng băng Chuyên gia Trong Tinh chỉnh

MoWE-Base trên TriviaQA đạt EM 37.7 khi đóng băng các chuyên gia trong tinh chỉnh. Khi chúng tôi cho phép cập nhật các chuyên gia trong tinh chỉnh, EM giảm 5 điểm xuống 33.5.

C Metrics và Thiết lập Baseline

Chúng tôi sử dụng các metric sau trong thí nghiệm của chúng tôi: đối với TriviaQA, WebQuestions và Natural Question, chúng tôi chủ yếu báo cáo kết quả theo Khớp Chính xác (EM), ngoại trừ một số thí nghiệm ablation, nơi chúng tôi báo cáo kết quả theo F1. Đối với tập dữ liệu Fever, chúng tôi báo cáo độ chính xác trong cả tập validation và test. Đối với SuperGLUE, theo các nghiên cứu trước (Raffel et al., 2020; Xue et al., 2022), chúng tôi tinh chỉnh các mô hình MoWE trên hỗn hợp của tất cả các tác vụ trong benchmark, chọn kết quả tốt nhất mỗi tác vụ và trình bày điểm trung bình tập validation trên tất cả các tác vụ.

Chúng tôi sử dụng triển khai MoE-Top2 từ khung T5x trong các thí nghiệm so sánh của chúng tôi. Các lớp dày đặc và thưa thớt được xen kẽ, tạo ra tổng cộng 12 lớp thưa thớt: 6 trong encoder và 6 trong decoder. Chúng tôi sử dụng định tuyến Top-2 và hầu hết các siêu tham số là mặc định, ngoại trừ dropout chuyên gia (0.3) và tỷ lệ học trong tinh chỉnh, mà chúng tôi đặt thành 5e-4 cho các tác vụ QA và Fever. Đối với SuperGLUE, chúng tôi làm theo khuyến nghị từ bài báo ST-MoE và sử dụng tỷ lệ học lớn hơn (1e-3) và kích thước batch nhỏ (256), ngoại trừ mô hình với 512 chuyên gia, mà chúng tôi sử dụng kích thước batch 512. https://github.com/google-research/t5x/

D Ví dụ về Các Mục từ Từ vựng Giàu Kiến thức

50 từ hàng đầu, theo tần suất trong C4, trong từ vựng định tuyến: 'isn', 'aren', '. . . ', '3d', '1st', 'whilst', 'copyright', 'creates', '2nd', 'tells', 'adds', 'wet', '3rd', ' ·', 'likes', 'filling', 'yours', ' ˆ', 'accordance', '4th', 'amongst', 'sees', '20th', 'mp3', '5th', 'woods', '19th', 'tx', 'toy', 'solely', 'thinks', '21st', 'sits', 'asks', '10th', 'receives', 'worlds', '6th', 'singles', 'blues', 'tops', 'inn', 'lean', 'mills', '7th', 'ranges', 'bears', 'newer', '8th', 'node'.

Trong 50 từ hàng đầu theo tần suất, chúng tôi vẫn thấy nhiều từ là biến thể của các từ phổ biến, như "sees". Tuy nhiên, chất lượng của từ vựng được cải thiện đáng kể sau này trong xếp hạng. Ví dụ, đây là 50 từ hàng đầu sau vị trí 6000 trong 1M: 'consignment', 'billboards', 'primal', 'discrepancy', 'callback', 'freeware', 'horticulture', 'jb', 's8', 'aspirants', 'commemorative', 'brisk', 'arched', 'pondering', 'fluff', 'diwali', 'landline', 'wilder', 'apocalyptic', 'patchwork', 'airs', 'stagnant', '412', 'watery', 'hospitalization', 'mccoy', 'serbian', 'paprika', 'headsets', 'deserts', 'pulley', 'orthopaedic', 'disparity', 'egyptians', 'painfully', 'kenyan', 'bale', 'condemnation', 'deportation', 'incline', 'perfumes', 'undergraduates', 'favoured', 'pvp', 'bbb', 'lyons', 'fremont', 'eurozone', 'afl', 'monogram'.

Chắc chắn có thể làm nhiều việc hơn để cải thiện từ vựng định tuyến, nhưng chúng tôi muốn giữ nó đơn giản cho các thí nghiệm của chúng tôi.
