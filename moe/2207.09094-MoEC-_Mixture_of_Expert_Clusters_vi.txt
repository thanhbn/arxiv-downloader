# MoEC: Hỗn hợp các Cụm Chuyên gia

Yuan Xie, Shaohan Huang, Tianyu Chen, Furu Wei
Microsoft Research Asia, Trung Quốc
fv-yuanxie, shaohanh, v-tianyuchen, fuwei g@microsoft.com

## Tóm tắt
Hỗn hợp Chuyên gia Thưa thớt (MoE) đã nhận được sự quan tâm lớn do khả năng mở rộng đầy hứa hẹn với chi phí tính toán có thể chấp nhận được. MoE chuyển đổi các lớp dày đặc thành các chuyên gia thưa thớt, và sử dụng mạng định tuyến có cổng để kích hoạt các chuyên gia một cách có điều kiện. Tuy nhiên, khi số lượng chuyên gia tăng lên, MoE với các tham số khổng lồ gặp phải vấn đề overfitting và phân bổ dữ liệu thưa thớt. Những vấn đề này đặc biệt nghiêm trọng đối với các nhiệm vụ có dữ liệu hạn chế, do đó cản trở tiến trình cải thiện hiệu suất của các mô hình MoE bằng cách mở rộng quy mô. Trong nghiên cứu này, chúng tôi đề xuất Hỗn hợp các Cụm Chuyên gia — một phương pháp tổng quát để cho phép các lớp chuyên gia học được kiến thức đa dạng và phù hợp hơn bằng cách áp đặt các ràng buộc dựa trên phương sai ở giai đoạn định tuyến. Chúng tôi tiếp tục đề xuất một chiến lược dropout chuyên gia ở cấp độ cụm được thiết kế đặc biệt cho cấu trúc cụm chuyên gia. Các thí nghiệm của chúng tôi cho thấy MoEC có thể cải thiện hiệu suất trên các nhiệm vụ dịch máy và hiểu ngôn ngữ tự nhiên, và nâng cao giới hạn trên của hiệu suất để mở rộng quy mô chuyên gia trong điều kiện dữ liệu hạn chế. Chúng tôi cũng xác minh rằng MoEC đóng vai trò tích cực trong việc giảm thiểu overfitting và phân bổ dữ liệu thưa thớt.

## Giới thiệu
Mở rộng quy mô khả năng mô hình đã cho thấy triển vọng đạt được hiệu suất tốt hơn trên nhiều nhiệm vụ khác nhau, bao gồm hiểu ngôn ngữ tự nhiên (Brown et al. 2020; Raffel et al. 2019) và học biểu diễn thị giác (Dosovitskiy et al. 2020; Bao, Dong, and Wei 2021). Sự tăng trưởng liên tục về kích thước mô hình và tham số mang lại chi phí tính toán cao hơn, trong khi các mô hình dày đặc lớn gần như đã chạm đến giới hạn khả năng phần cứng. Trong việc theo đuổi hiệu quả tính toán tốt hơn, Hỗn hợp Chuyên gia thưa thớt (MoE) được đề xuất như một giải pháp thay thế hiệu quả cho các mô hình dày đặc (Lepikhin et al. 2020; Fedus, Zoph, and Shazeer 2021; Riquelme et al. 2021; Lewis et al. 2021). Đối với các transformer MoE có cổng thưa thớt, lớp con mạng feed-forward (FFN) sẽ được thay thế bằng một tập hợp các chuyên gia với các tham số độc lập. Tính thưa thớt của MoE được tạo ra bởi các chuyên gia và mạng định tuyến có cổng. Mạng định tuyến có cổng sẽ tính toán điểm định tuyến giữa các token đầu vào và mỗi chuyên gia và kích hoạt các chuyên gia có điểm định tuyến top-k. Hầu hết các chuyên gia sẽ không được kích hoạt, do đó tạo thành cấu trúc thưa thớt. Vì chi phí tính toán chỉ tỷ lệ thuận với mạng con top-k được kích hoạt, các mô hình MoE được kích hoạt thưa thớt có thể mở rộng các tham số mô hình mà không tăng đáng kể chi phí tính toán. Với chi phí tính toán có thể chấp nhận được, các mô hình MoE có thể đạt được hiệu suất tốt hơn các mô hình dày đặc trên nhiều nhiệm vụ khác nhau như dịch máy thần kinh (Lewis et al. 2019; Conneau and Lample 2019; Lepikhin et al. 2020), nhận dạng hình ảnh (Riquelme et al. 2021) và nhận dạng giọng nói (Kumatani et al. 2021).

Các nghiên cứu gần đây đã đạt được sự đồng thuận rằng càng nhiều chuyên gia có nghĩa là càng nhiều tham số và khả năng mô hình lớn, điều này luôn mang lại cải thiện. Tuy nhiên, một số nghiên cứu cho thấy rằng nhiều tham số có thể huấn luyện hơn và tính toán có điều kiện thưa thớt có thể gây ra overfitting (Xue et al. 2021; Lou et al. 2021; Xue et al. 2022), đặc biệt là đối với các nhiệm vụ hạ nguồn với dữ liệu hạn chế. Như được mô tả trong Hình 1, khi số lượng chuyên gia tăng lên, overfitting dần trở nên rõ ràng trong nhiệm vụ dịch máy. Hơn nữa, chúng tôi phát hiện rằng việc mở rộng kích thước của MoE sẽ không luôn dẫn đến cải thiện. Dường như tồn tại một giới hạn trên về hiệu suất khi mở rộng quy mô chuyên gia với dữ liệu hạn chế.

Hơn nữa, chúng tôi phát hiện một hiện tượng không hợp lý trong Hình 1: MoE 64 chuyên gia với nhiều tham số hơn và khả năng mô hình lớn hơn lại có loss huấn luyện cao hơn MoE 32 chuyên gia. Điều này ngụ ý rằng MoE quy mô lớn không chỉ gặp phải overfitting mà còn có các vấn đề ẩn khác ảnh hưởng đến quá trình huấn luyện. Theo phân tích của chúng tôi, xác suất mỗi chuyên gia nhận được một token giảm tỷ lệ thuận khi số lượng chuyên gia tăng lên. Với cùng một lượng dữ liệu, mỗi chuyên gia sẽ nhận được ít mẫu đa dạng hơn. Điều này có thể ảnh hưởng đến việc học đầy đủ của các lớp chuyên gia. Dữ liệu không đủ để phù hợp với nhiều tham số hơn cũng là nguyên nhân chính gây ra overfitting. Do đó, chúng tôi muốn khám phá những cách mà các chuyên gia có thể nhận được các mẫu đa dạng và học được kiến thức phong phú, từ đó giảm thiểu overfitting và phân bổ dữ liệu thưa thớt.

Trong nghiên cứu này, chúng tôi đề xuất Hỗn hợp các Cụm Chuyên gia (MoEC), một chiến lược tối ưu hóa tổng quát cho các mô hình MoE. Chúng tôi thu hẹp xác suất định tuyến giữa các chuyên gia lân cận để tạo thành cấu trúc chuyên gia được nhóm cụm. Thiên hướng quy nạp mong đợi rằng độ tương tự của các chuyên gia trong cụm cao trong khi độ tương tự của các chuyên gia giữa các cụm thấp. Các chuyên gia trong một cụm có xu hướng với các token có trạng thái ẩn tương tự và có thể "chia sẻ" các token tương tự. Hơn nữa, chúng tôi đề xuất một chiến lược dropout chuyên gia ở cấp độ cụm cho cấu trúc cụm chuyên gia. Một số chuyên gia trong cụm sẽ được loại bỏ ngẫu nhiên, các chuyên gia bị loại bỏ sẽ không tham gia vào giai đoạn định tuyến. Các chuyên gia được kích hoạt sẽ được chọn từ các chuyên gia còn lại trong cụm. Việc thực hiện dropout trong các cụm sẽ làm cho các token luôn được gửi đến các chuyên gia phù hợp, bất kể dropout ngẫu nhiên như thế nào.

Chúng tôi đánh giá MoEC của chúng tôi trên các nhiệm vụ dịch máy và hiểu ngôn ngữ tự nhiên. Kết quả thí nghiệm cho thấy MoEC vượt trội hơn các mô hình dày đặc và các mô hình MoE cơ sở. Điều này chỉ ra rằng MoEC giữ lại những ưu điểm của cấu trúc thưa thớt của MoE, và giảm thiểu các vấn đề overfitting và phân bổ dữ liệu thưa thớt.

Những đóng góp của chúng tôi được tóm tắt như sau:
• Chúng tôi chỉ ra các vấn đề overfitting và phân bổ dữ liệu thưa thớt cho các mô hình MoE quy mô lớn, và các chuyên gia nhận được ít mẫu đa dạng hơn có thể là nguyên nhân chung của cả hai vấn đề.
• Chúng tôi đề xuất xây dựng các cụm chuyên gia bằng các ràng buộc dựa trên phương sai, cho phép các chuyên gia nhận được một tập hợp các token tương tự đa dạng hơn. Chúng tôi cũng thực hiện dropout chuyên gia ở cấp độ cụm như một phương pháp điều chuẩn.
• Chúng tôi tiến hành thí nghiệm trên các nhiệm vụ dịch thuật và hiểu ngôn ngữ tự nhiên. MoEC có thể cải thiện hiệu suất và giảm thiểu các vấn đề gây ra bởi việc mở rộng quy mô chuyên gia.
• Chúng tôi phát hiện rằng tồn tại một giới hạn trên về hiệu suất khi mở rộng quy mô các mô hình MoE với dữ liệu hạn chế, và MoEC có thể nâng cao giới hạn trên đó.

## Nghiên cứu liên quan
Trong bối cảnh của các kiến trúc học sâu hiện đại, việc mở rộng quy mô transformer bằng cách sử dụng Hỗn hợp Chuyên gia thưa thớt (MoE) được chứng minh là hiệu quả để đạt được hiệu suất tốt nhất trên nhiều nhiệm vụ NLP và CV khác nhau (Shazeer et al. 2017; Lepikhin et al. 2020; Riquelme et al. 2021; Fedus, Zoph, and Shazeer 2021). So với các transformer dày đặc, một mô hình MoE chứa nhiều chuyên gia (mạng feed-forward) và một router để chọn top-k chuyên gia cho các token đầu vào. Nó tăng khả năng mô hình bằng tính toán có điều kiện như vậy trong khi duy trì hiệu quả tính toán. Để tiếp tục khám phá tiềm năng của MoE, một số nghiên cứu tập trung vào thuật toán gán router (Lewis et al. 2021; Roller et al. 2021; Dai et al. 2022). Bên cạnh đó, một số nghiên cứu tập trung vào việc tối ưu hóa các phương pháp huấn luyện cho các mô hình MoE. Dua et al. (2021) áp dụng cơ chế gia nhiệt nhiệt độ cho các mô hình MoE thưa thớt trong nhiệm vụ dịch thuật. Chi et al. (2022) đề xuất giảm chiều để ước tính điểm định tuyến giữa các token và chuyên gia trên một siêu cầu chiều thấp. Nghiên cứu của chúng tôi cũng được đề xuất để tối ưu hóa mô hình MoE. Thay vì thay đổi cấu trúc mô hình và chiến lược định tuyến, MoEC thiết lập các cụm chuyên gia, cho phép các chuyên gia được gán một tập hợp các token tương tự đa dạng hơn.

Mặc dù các mô hình MoE đã đạt được kết quả đầy hứa hẹn, chúng được chứng minh có các vấn đề overfitting (Fedus, Zoph, and Shazeer 2021; Wu et al. 2022; Xue et al. 2022) trên các nhiệm vụ hạ nguồn với dữ liệu hạn chế. Để giảm thiểu overfitting, một số nghiên cứu sử dụng chưng cất kiến thức để chưng cất các mô hình MoE thành các mô hình MoE kích thước nhỏ hoặc các mô hình dày đặc (Xue et al. 2022; Dai et al. 2022). Một cách tiếp cận khác là áp dụng chiến lược dropout trong quá trình huấn luyện. Fedus, Zoph, and Shazeer (2021) đặt tỷ lệ dropout nhỏ ở các lớp không phải chuyên gia và tỷ lệ dropout lớn hơn ở các lớp chuyên gia. Liu et al. (2022) đề xuất gating dropout, cho phép một số token bỏ qua mạng định tuyến có cổng và ở lại máy cục bộ của chúng để giảm giao tiếp giữa các máy. Trong nghiên cứu của chúng tôi, chúng tôi đề xuất dropout chuyên gia ở cấp độ cụm. Các chuyên gia được chọn ngẫu nhiên trong cụm sẽ bị loại bỏ để chúng không tham gia vào giai đoạn định tuyến.

## Kiến thức nền tảng
Để xây dựng các transformer MoE, thông thường người ta thay thế các lớp con mạng feed-forward (FFN) bằng một tập hợp các chuyên gia. Các chuyên gia có cùng cấu trúc với lớp FFN trong mô hình transformer dày đặc. Chúng tôi ký hiệu biểu diễn ẩn của token đầu vào x là h, và embedding cho chuyên gia thứ i là ei. Router tính toán điểm định tuyến si = h^T ei để so sánh độ tương tự giữa h và tập hợp các chuyên gia E. Sau đó, router sử dụng một hàm cổng Φ() để tính toán giá trị có cổng của chuyên gia i.

Φi = {
    exp(si) / Σ(j=1 to E) exp(sj), softmax gating
    1 / (1 + exp(-si)), sigmoid gating
}                                                    (1)

Hàm cổng Φi biểu diễn xác suất gửi token đầu vào đến chuyên gia i. Giá trị có cổng top-k được sử dụng để gửi token x theo Φi. Các mạng chuyên gia k tương ứng được kích hoạt có điều kiện. Chúng tôi ký hiệu tập hợp các chỉ số top-k đã chọn là K.

y = Σ(i∈K) Φi Ei(x)                                 (2)

trong đó Ei(x) là mạng chuyên gia thứ i, là một mạng feed-forward. Đầu ra của mạng định tuyến có cổng là tổ hợp tuyến tính có trọng số của phép tính của mỗi chuyên gia trên token theo giá trị cổng.

## Phương pháp
Trong nghiên cứu này, mục tiêu của chúng tôi là cung cấp cho các chuyên gia quyền truy cập vào nhiều mẫu huấn luyện đa dạng hơn, từ đó học được kiến thức phong phú và giảm thiểu overfitting cũng như phân bổ dữ liệu thưa thớt. Chúng tôi thu hẹp xác suất định tuyến giữa các chuyên gia lân cận để tạo thành cấu trúc chuyên gia được nhóm cụm. Chúng tôi áp dụng loss phân cụm dựa trên phương sai để thực hiện các ràng buộc. Sau đó, chúng tôi tiếp tục đề xuất một chiến lược dropout chuyên gia ở cấp độ cụm. Trong nghiên cứu của chúng tôi, chúng tôi sử dụng cổng top-1. Chỉ có chuyên gia có điểm định tuyến lớn nhất được kích hoạt. Và chúng tôi chọn softmax làm hàm kích hoạt của mình. Các chuyên gia trong một cụm sẽ được phân phối trên cùng một thiết bị để giảm chi phí giao tiếp.

### Hỗn hợp các Cụm Chuyên gia
Chúng tôi minh họa MoEC (Hỗn hợp các Cụm Chuyên gia) của chúng tôi trong Hình 2. Đối với MoE thông thường, xác suất định tuyến của các token sẽ không bị ràng buộc. Router sẽ luôn gửi các token đầu vào đến các chuyên gia phù hợp nhất của chúng, trong khi các token tương tự khác có ít cơ hội được chọn. Khi mở rộng quy mô số lượng chuyên gia, phân phối dữ liệu thưa thớt sẽ khiến mỗi chuyên gia nhận được ít token đa dạng hơn. Lớp chuyên gia không thể được huấn luyện đầy đủ. Ngoài ra, lượng dữ liệu không đủ để phù hợp với số lượng tham số ngày càng tăng, điều này cũng là lý do chính gây ra overfitting. Để giải quyết các vấn đề của MoE thông thường, MoEC của chúng tôi cho phép mỗi chuyên gia nhận được nhiều token phong phú và đa dạng hơn. Chúng tôi áp đặt các ràng buộc dựa trên phương sai ở giai đoạn định tuyến, nhằm mục đích làm cho các chuyên gia lân cận có xác suất định tuyến tương tự cho các token đầu vào, từ đó tạo thành các cụm chuyên gia có xu hướng với các token có trạng thái ẩn tương tự. Trong MoEC, các chuyên gia sẽ nhận được một tập hợp các token đầu vào tương tự đa dạng hơn bằng cách "chia sẻ" token đầu vào với các chuyên gia khác trong cụm.

So với các nghiên cứu trước đây liên quan đến MoE, mục tiêu huấn luyện của chúng tôi thêm một thuật ngữ bổ sung - loss cụm. Mục tiêu huấn luyện tổng thể là tối thiểu hóa:

L = Ltask + Lbalance + Lcluster                     (3)

Ltask được xác định bởi nhiệm vụ cụ thể. Trong nghiên cứu của chúng tôi, chúng tôi sử dụng loss cross-entropy được làm mượt nhãn cho dịch máy thần kinh, loss mô hình hóa ngôn ngữ có mặt nạ cho mô hình ngôn ngữ tiền huấn luyện, và loss log-likelihood âm (NLL loss) hoặc loss bình phương trung bình (MSE loss) cho các nhiệm vụ GLUE. Trong phần tiếp theo, chúng tôi sẽ giới thiệu Lbalance và Lcluster.

**Loss Cân bằng Tải.** Trong quá trình huấn luyện, tồn tại vấn đề mất cân bằng tải giữa các chuyên gia (Shazeer et al. 2017; Lepikhin et al. 2020): Hầu hết các token được gửi đến một số lượng nhỏ chuyên gia, trong khi nhiều chuyên gia khác không được huấn luyện đầy đủ. Bên cạnh đó, việc gán không cân bằng sẽ dẫn đến tắc nghẽn tính toán cao trong lớp MoE và do đó hạn chế hiệu quả tính toán. Chúng tôi làm theo nghiên cứu trong (Fedus, Zoph, and Shazeer 2021) và thêm loss cân bằng vào mục tiêu huấn luyện để khuyến khích tải cân bằng giữa các chuyên gia. Với N chuyên gia được lập chỉ mục bởi i = 1 đến N, loss cân bằng được tính như sau:

Lbalance = N Σ(i=1 to N) fi pi                     (4)

trong đó fi là tỷ lệ token được gửi đến chuyên gia i. Chúng tôi ký hiệu số lượng token được gửi đến chuyên gia thứ i là Counti. Với một batch B có T token, fi = Counti / T. pi là tỷ lệ xác suất định tuyến được phân bổ cho chuyên gia i trong batch B. Nó được tính bằng cách lấy trung bình xác suất định tuyến token x đến chuyên gia i trong batch B.

pi = (1/T) Σ(x∈B) Φi(x)                           (5)

trong đó Φi(x) là hàm cổng được mô tả trong Phương trình 1, biểu diễn xác suất gửi token x đến chuyên gia i. Loss cân bằng trong Phương trình 4 khuyến khích định tuyến đồng đều vì nó sẽ được tối thiểu hóa dưới phân phối đồng đều. Để kiểm soát tác động của loss cân bằng trong quá trình huấn luyện, một siêu tham số α được áp dụng như một hệ số nhân cho loss. Trong toàn bộ nghiên cứu này, chúng tôi sử dụng α = 10^-2 đủ lớn để đảm bảo cân bằng tải trong khi đủ nhỏ để không áp đảo mục tiêu cross-entropy chính.

**Loss Phân cụm.**
Trong nghiên cứu của chúng tôi, chúng tôi phát hiện rằng việc phân bổ dữ liệu thưa thớt cản trở nghiêm trọng việc huấn luyện đầy đủ các lớp MoE và làm trầm trọng thêm overfitting. Để cho phép các chuyên gia nhận được các token phong phú và đa dạng để giảm thiểu tác động của phân bổ thưa thớt, chúng tôi thiết kế loss phân cụm. Loss này được thiết kế để ràng buộc một số chuyên gia liền kề để chúng sẽ chia sẻ xác suất định tuyến tương tự với các token, từ đó tạo thành phân phối giống như cụm. Đối với các token đầu vào ban đầu được gửi đến các chuyên gia phù hợp nhất, loss phân cụm sẽ cung cấp cho chúng nhiều cơ hội hơn để truy cập các chuyên gia khác trong cụm. Kết quả là, các chuyên gia sẽ được gán một tập hợp các token tương tự đa dạng hơn, từ đó giảm thiểu vấn đề phân bổ thưa thớt.

Trong các mô hình MoE với N chuyên gia, loss phân cụm sẽ hướng dẫn các chuyên gia tạo thành m cụm (m nên chia hết cho N), và mỗi cụm chứa L = N/m chuyên gia. Chúng tôi sử dụng E_i^j để biểu diễn chuyên gia thứ j trong cụm thứ i, trong khi p_i^j biểu diễn xác suất định tuyến được phân bổ cho E_i^j (i = 0, 1, ..., m-1; j = 0, 1, ..., L-1). Theo kích thước và số lượng cụm, p_i^0, p_i^1, ..., p_i^(L-1) sẽ tạo thành một ma trận một chiều P̃_i ∈ R^L để biểu diễn xác suất định tuyến của L chuyên gia trong cụm thứ i, và chúng tôi ký hiệu giá trị trung bình của chúng là p̄_i. Chúng tôi định nghĩa loss phân cụm như sau:

Lclustering = N × (Cintra - λ × Cinter)
            = N × (Σ(i=0 to m-1) σ(P̃_i) / m - λ × (max{p̄_i} - max_2{p̄_i}) / max{p̄_i})   (6)

Như có thể thấy từ Phương trình 6, loss phân cụm chủ yếu bao gồm hai phần: ràng buộc trong cụm dựa trên phương sai Cintra và ràng buộc giữa các cụm dựa trên sự khác biệt Cinter.

σ(P̃_i) = [(p_i^0 - p̄_i)^2 + (p_i^1 - p̄_i)^2 + ... + (p_i^(L-1) - p̄_i)^2] / L biểu diễn phương sai của xác suất định tuyến trong cụm thứ i. Chúng tôi tính toán phương sai trung bình của m cụm như ràng buộc trong cụm Cintra, sẽ được tối thiểu hóa khi xác suất định tuyến của các chuyên gia trong cùng một cụm được cân bằng.

Bên cạnh đó, chúng tôi sử dụng Cinter để đo lường sự khác biệt xác suất giữa cụm được gửi và cụm tối ưu phụ. max{} có nghĩa là giá trị tối đa của p̄_i (i = 0, 1, ..., m-1) và max_2{} có nghĩa là giá trị tối đa thứ hai. Cinter sẽ được tối thiểu hóa khi xác suất một token được gửi đến cụm tối ưu phụ thấp. λ là hệ số được sử dụng để kiểm soát giá trị của Cinter. Khi chúng tôi đặt λ = 0, sự khác biệt xác suất giữa các cụm sẽ không được xem xét. Chúng tôi cũng có thể đặt λ thành giá trị khác không để kích hoạt Cinter. Chúng tôi sẽ tiến hành thí nghiệm và phân tích sâu về nó trong chương Thí nghiệm.

Để tối thiểu hóa loss phân cụm, phân phối xác suất trong cụm nên đồng đều, và sự khác biệt xác suất giữa các cụm nên rõ ràng hơn (tùy chọn). Trong các bước huấn luyện ban đầu, phương sai giữa các chuyên gia sẽ rất cao, vì vậy loss phân cụm sẽ thống trị việc tối ưu hóa và hướng dẫn việc hình thành nhanh chóng các cụm chuyên gia. Khi phương sai trong cụm ổn định, loss phân cụm sẽ trở nên tương đối nhỏ để duy trì các cụm chuyên gia. Tương tự như thực hành trong loss cân bằng, một siêu tham số β được áp dụng. Giá trị của β nên tương đối nhỏ, vì β lớn có nghĩa là ràng buộc phân cụm mạnh, do đó làm cho các chuyên gia trong cụm quá tương tự nhau. Điều này sẽ khiến các chuyên gia này mất đi đặc điểm riêng, và những đóng góp của nhiều chuyên gia tương tự chỉ xấp xỉ bằng một chuyên gia. Trong nghiên cứu của chúng tôi, chúng tôi đặt giá trị của β là 10^-2 theo mặc định. Thí nghiệm về việc chọn giá trị β có thể được tìm thấy trong Phụ lục A.

### Dropout chuyên gia ở cấp độ cụm
Khi áp dụng các mô hình MoE quy mô lớn trên các nhiệm vụ có dữ liệu hạn chế, các vấn đề overfitting xuất hiện một cách tự nhiên. Các nghiên cứu trước đây liên quan đến MoE (Raffel et al. 2019; Fedus, Zoph, and Shazeer 2021) đã sử dụng dropout (Srivastava et al. 2014) ở mỗi lớp để ngăn chặn overfitting. Ở đây, dropout chuyên gia ở cấp độ cụm hoạt động như một kỹ thuật điều chuẩn hoàn toàn khác với dropout truyền thống. Nó không loại bỏ tham số, mà loại bỏ một số chuyên gia trong cụm, điều này làm cho việc gửi token trở nên ngẫu nhiên hơn.

**Thực hiện trong các cụm.** Đầu tiên, dropout chuyên gia ở cấp độ cụm của chúng tôi hoạt động ở giai đoạn định tuyến, vì vậy nó sẽ chỉ được thực hiện ở các lớp chuyên gia. Đối với các chuyên gia trong một cụm, chúng tôi loại bỏ ngẫu nhiên một số trong chúng bằng cách xóa id chuyên gia khỏi danh sách chuyên gia ứng cử viên khi tính toán xác suất định tuyến. Do đó, các chuyên gia tương ứng sẽ bị bỏ qua trong giai đoạn định tuyến. Giả sử tỷ lệ dropout là ρ, chỉ có N(1-ρ) chuyên gia còn lại sẽ tham gia vào việc tính toán xác suất định tuyến trong quá trình huấn luyện. Chiều của ma trận P sẽ giảm từ R^N xuống R^(N(1-ρ)). Tất cả các cụm thực hiện dropout đồng thời. Điều này cho phép các token có nhiều cơ hội hơn để được gửi đến các chuyên gia khác trong cùng một cụm, thay vì được gửi đi liên tục đến chuyên gia có xác suất cao nhất. Từ một góc độ khác, mỗi chuyên gia sẽ nhận được nhiều token đa dạng hơn mà không cần thêm dữ liệu huấn luyện.

**Dropout chuyên gia ở cấp độ cụm so với Dropout chuyên gia truyền thống.**
Dropout chuyên gia truyền thống được khuyến nghị trong Fedus, Zoph, and Shazeer (2021). Đó là một kỹ thuật dropout (Srivastava et al. 2014) để điều chuẩn các mô hình MoE, hoạt động trên lớp feed-forward để giảm overfitting gây ra bởi quá nhiều tham số. Bằng cách đặt tỷ lệ dropout tương đối nhỏ ở các lớp không phải chuyên gia (0.1), dropout chuyên gia tăng tỷ lệ dropout bằng một lượng rõ ràng ở phép tính feed-forward trung gian tại mỗi lớp chuyên gia (0.4). Dropout chuyên gia của chúng tôi hoạt động hoàn toàn khác với nó. Chúng tôi thực hiện dropout ngẫu nhiên trên danh sách ứng cử viên của các chuyên gia trong giai đoạn định tuyến. Nó không giảm số lượng tham số trong quá trình huấn luyện mà phân bổ token một cách đa dạng và linh hoạt hơn. Trong khi dropout chuyên gia truyền thống thường được sử dụng để tinh chỉnh trên các nhiệm vụ hạ nguồn, dropout chuyên gia ở cấp độ cụm của chúng tôi là một cơ chế điều chuẩn tổng quát với tính tổng quát mạnh. Ngoài ra, dropout của chúng tôi có thể được áp dụng cùng với dropout chuyên gia của Fedus, và chúng có thể hoạt động cùng nhau để cải thiện hiệu suất của MoE.

**Tại sao cấp độ cụm tốt hơn?**
Việc nghĩ rằng dropout chuyên gia có thể được thực hiện ở cấp độ toàn cục là điều tự nhiên, điều này cung cấp nhiều cơ hội hơn cho các token để truy cập các chuyên gia tối ưu phụ khác. Nhưng đối với dropout chuyên gia ở cấp độ toàn cục, như được hiển thị trong Hình 3, nếu một dropout ngẫu nhiên tình cờ loại bỏ các chuyên gia phù hợp, các token có thể được gửi đến các chuyên gia ít liên quan hơn. Việc gửi không phù hợp có thể tác động tiêu cực đến việc học của các chuyên gia.

Trong MoEC, chúng tôi giải quyết vấn đề này bằng cách khai thác cấu trúc giống như cụm và thiết kế một dropout chuyên gia ở cấp độ cụm. Dropout ở cấp độ cụm có thể cung cấp cho các token tùy chọn được gửi lại một cách ngẫu nhiên trong khi giới hạn kết quả định tuyến trong một phạm vi hợp lý hơn. Bất kể dropout được thực hiện ngẫu nhiên như thế nào, các token sẽ luôn được gửi đến các chuyên gia có xác suất định tuyến tương tự.

## Thí nghiệm
Chúng tôi đặt tên mô hình của mình là MoEC (Hỗn hợp các Cụm Chuyên gia) và đánh giá hiệu suất trên các nhiệm vụ dịch máy song ngữ và hiểu ngôn ngữ tự nhiên. Chúng tôi sử dụng mô hình X-MoE từ Chi et al. (2022) làm kiến trúc xương sống, đã cho thấy hiệu suất tốt hơn các mô hình MoE trước đây như Switch Transformers (Fedus, Zoph, and Shazeer 2021) trên các benchmark hiểu đa ngôn ngữ được sử dụng rộng rãi.

### Bộ dữ liệu đánh giá
**WMT 2014 English-to-German** Ninth Workshop on Statistical Machine Translation (WMT 2014) phát hành một bộ sưu tập các bộ dữ liệu được sử dụng trong các nhiệm vụ chung bao gồm dịch máy. Chúng tôi thêm dữ liệu news-commentary-v12 bổ sung từ WMT-17 cho huấn luyện và xác thực. Tổng dữ liệu huấn luyện chứa 3.96M cặp câu English-to-German.

**GLUE** General Language Understanding Evaluation (Wang et al. 2018) benchmark là một bộ sưu tập các công cụ để đánh giá hiệu suất của các mô hình trên một tập hợp đa dạng các nhiệm vụ NLU hiện có, bao gồm MNLI (Williams, Nangia, and Bowman 2017), CoLA (Warstadt, Singh, and Bowman 2019), SST-2 (Socher et al. 2013), QQP, QNLI (Rajpurkar et al. 2016), MRPC (Dolan and Brockett 2005) và STS-B (Cer et al. 2017). Chúng tôi không thực hiện thí nghiệm trên RTE vì nghiên cứu trước đây (Chen et al. 2022) đã chứng minh rằng MoE không phù hợp cho nhiệm vụ này. Đáng chú ý là chúng tôi sẽ tiền huấn luyện mô hình của mình trên BooksCorpus (Zhu et al. 2015) và corpus English Wikipedia (Foundation) trong 120k bước trước khi tinh chỉnh trên các nhiệm vụ GLUE.

### Thiết lập thí nghiệm
**Kiến trúc mô hình** Đối với MoEC của chúng tôi và tất cả các mô hình cơ sở, chúng tôi tuân theo các cài đặt được khuyến nghị trong (Vaswani et al. 2017) và sử dụng Transformer-big làm kiến trúc xương sống thống nhất trên nhiệm vụ dịch WMT 2014 English-German. Đối với các nhiệm vụ GLUE, chúng tôi sử dụng Transformer-base làm kiến trúc xương sống.

Đối với các lớp MoE, chúng tôi áp dụng mô hình MoE 64 chuyên gia với 3 lớp con FFN trong khối encoder thứ 3 và khối decoder thứ 3. Các siêu tham số mô hình chi tiết hơn có thể được tìm thấy trong Phụ lục B.

**Baseline**
Chúng tôi tiến hành hai baseline trong các thí nghiệm của mình. Đầu tiên là dense transformer (Vaswani et al. 2017). Đối với cái khác, chúng tôi tuân theo nghiên cứu trong (Chi et al. 2022) và áp dụng X-MoE làm MoE baseline của chúng tôi. Nó có thể phục vụ như một baseline mạnh cho thấy hiệu suất tốt hơn Switch Transformer (Fedus, Zoph, and Shazeer 2021) trên các benchmark hiểu đa ngôn ngữ được sử dụng rộng rãi. Baseline MoE ước tính điểm định tuyến giữa các token và chuyên gia trên một siêu cầu chiều thấp và thêm một vô hướng nhiệt độ có thể học được trong hàm cổng. Để so sánh công bằng, hai phương pháp baseline được xây dựng với cùng cài đặt như MoEC, có thể được tìm thấy trong Phụ lục B.

**Siêu tham số MoEC** Đối với MoEC, một số siêu tham số duy nhất được giới thiệu. Đối với loss phân cụm, chúng tôi đặt β thành 10^-2 theo kết quả thí nghiệm (xem Phụ lục A) và đặt λ = 0 theo mặc định. Đối với kích thước cụm (số lượng chuyên gia trong một cụm) và tỷ lệ dropout chuyên gia, chúng tôi sẽ có các thí nghiệm liên quan chi tiết trong các phần sau.

**Siêu tham số huấn luyện** Để so sánh công bằng, mô hình dày đặc, mô hình MoE baseline và mô hình MoEC chia sẻ cùng các siêu tham số huấn luyện. Tất cả các mô hình được huấn luyện với bộ tối ưu Adam (Kingma and Ba 2014) (β1 = 0.9, β2 = 0.98). Tỷ lệ học được đặt 5e-4 với 4000 bước warm-up và bộ lập lịch căn bậc hai nghịch đảo (Raffel et al. 2019). Kích thước batch, bước huấn luyện và tỷ lệ dropout được đặt bởi các nhiệm vụ khác nhau, được ghi lại trong Phụ lục C.

### Kết quả thí nghiệm
Chúng tôi huấn luyện các mô hình dày đặc, MoE baseline và MoEC trên một số nhiệm vụ đánh giá được sử dụng rộng rãi, và kết quả được hiển thị trong Bảng 1. So với các mô hình dày đặc, các mô hình MoE thể hiện cải thiện hiệu suất đáng kể, được hưởng lợi từ khả năng mô hình lớn. Bên cạnh đó, MoEC có thể mang lại cải thiện đáng chú ý so với MoE baseline mà không áp dụng chiến lược dropout cho các chuyên gia. Trên WMT-14, nó cung cấp một sự tăng điểm BLEU 1.62. Ưu thế có thể được quy cho phân phối được nhóm cụm của các chuyên gia, điều này trao cho các chuyên gia nhiều mẫu huấn luyện đa dạng và phù hợp hơn. Hơn nữa, với việc áp dụng chiến lược dropout chuyên gia ở cấp độ cụm, hiệu suất của MoEC sẽ được cải thiện thêm.

Như được hiển thị trong Hình 4, MoE baseline gặp phải overfitting nghiêm trọng trên WMT-14, trong khi MoEC của chúng tôi cho thấy khả năng xuất sắc trong việc giảm thiểu overfitting. Hiện tượng overfitting trên tập xác thực gần như được loại bỏ, và loss xác thực tương đối thấp hơn. Điều này cho thấy rằng khi MoEC của chúng tôi giải quyết việc phân bổ dữ liệu thưa thớt, mỗi chuyên gia có thể nhận được nhiều mẫu huấn luyện phong phú và đa dạng hơn. Bằng cách này, dữ liệu huấn luyện của mỗi chuyên gia được giữ đầy đủ, từ đó giảm thiểu hiện tượng overfitting. Hơn nữa, chúng tôi phát hiện rằng MoEC hội tụ chậm hơn một chút. Điều này là do mỗi chuyên gia cần học từ nhiều mẫu huấn luyện đa dạng hơn, điều này cần nhiều bước hơn để cho phép chuyên gia được huấn luyện đầy đủ.

### Phân tích chi tiết về các cụm chuyên gia
Tiếp theo, chúng tôi tiến hành phân tích chi tiết về các cụm chuyên gia. Hình 5 cho thấy tỷ lệ token được gửi đến cụm 0 (chuyên gia 0-3) trong quá trình huấn luyện và suy luận. Trong quá trình huấn luyện, các chuyên gia trong cụm 0 nhận được các token đầu vào tương tự, được ảnh hưởng bởi loss cân bằng và loss phân cụm. Trong quá trình suy luận, xác suất định tuyến của các chuyên gia trong cụm thay đổi, điều này chỉ ra rằng chúng vẫn giữ lại đặc điểm riêng của mình. Chúng học được kiến thức tinh vi hơn, đây là lợi thế của nhiều chuyên gia tương tự so với một chuyên gia duy nhất. Đối với WMT14, điểm BLEU của MoE với 16 chuyên gia là 30.49, trong khi điểm BLEU của MoE với 16 cụm (kích thước cụm = 4) là 32.16. Điều này cho thấy nhiều chuyên gia tương tự có lợi thế rõ ràng so với một chuyên gia duy nhất.

Kích thước cụm cũng có tác động quan trọng đến việc học của MoEC, vì vậy chúng tôi tiến hành thí nghiệm trên các kích thước cụm khác nhau. Như được mô tả trong Bảng 2, hiệu suất tốt nhất được đạt được khi kích thước cụm = 8. So với MoE baseline với 64 chuyên gia, các cụm chuyên gia có thể mang lại khoảng 1.62 điểm BLEU cải thiện. Khi kích thước cụm tương đối nhỏ, dữ liệu được chia sẻ giữa các chuyên gia sẽ ít hơn, và sự cải thiện do MoEC mang lại sẽ không được khai thác đầy đủ. Như một trường hợp đặc biệt, khi kích thước cụm = 1, một chuyên gia duy nhất không thể được gọi là một cụm, và MoEC tương đương với MoE baseline. Khi kích thước cụm lớn, dữ liệu được chia sẻ giữa các chuyên gia sẽ tăng, nhưng độ tương tự và tương quan của các dữ liệu này sẽ trở nên thấp hơn, điều này sẽ dẫn đến tác động bất lợi đến "tính chuyên môn" của mỗi chuyên gia. Khi chúng tôi mở rộng kích thước cụm lên 16, hiệu suất của MoEC thậm chí còn thấp hơn MoE baseline, điều này có nghĩa là kích thước cụm quá lớn sẽ ức chế lợi thế của cấu trúc MoE và làm hại hiệu suất.

### Dropout chuyên gia: Cấp độ cụm so với cấp độ toàn cục
Trong Bảng 3, chúng tôi thí nghiệm trên WMT-14 với tỷ lệ dropout chuyên gia ở cấp độ cụm. Chúng tôi phát hiện rằng dropout ở cấp độ cụm có thể nâng cao hiệu suất tổng quát hóa của MoEC. Phương pháp điều chuẩn như vậy có thể mang lại 0.29 điểm BLEU cải thiện cho MoEC. Kết quả thí nghiệm cho thấy 0.5 là lựa chọn tốt cho tỷ lệ dropout. Bên cạnh đó, rõ ràng là dropout chuyên gia ở cấp độ toàn cục sẽ làm hại hiệu suất.

Đối với dropout chuyên gia ở cấp độ cụm, khi loại bỏ chuyên gia phù hợp nhất cho các token đầu vào, quyết định định tuyến vẫn sẽ được thực hiện giữa các chuyên gia còn lại trong cụm. Bất kể các chuyên gia bị loại bỏ được chọn như thế nào, sẽ luôn có các chuyên gia còn lại trong mỗi cụm. Điều này đảm bảo rằng các chuyên gia phù hợp luôn có sẵn. Nhưng đối với cấp độ toàn cục, do phân phối ngẫu nhiên của các chuyên gia, nếu tất cả các chuyên gia phù hợp đều bị loại bỏ, token sẽ được định tuyến đến một chuyên gia không phù hợp. Điều này có thể khiến các chuyên gia bị phân tâm bởi dữ liệu có độ liên quan thấp, do đó tác động tiêu cực đến việc học kiến thức. Lấy Hình 3 làm ví dụ đơn giản (với cài đặt tỷ lệ dropout là 0.5). Đối với dropout chuyên gia ở cấp độ toàn cục, khi cả chuyên gia 1 và chuyên gia 2 đều bị loại bỏ, thì Hn sẽ chỉ được gửi đến chuyên gia 3 hoặc chuyên gia 4. Việc phân bổ không phù hợp này có thể làm hại hiệu suất của mô hình.

### Vai trò của hệ số ràng buộc giữa cụm Cinter
Chúng tôi tiếp tục khám phá xem hệ số ràng buộc giữa cụm Cinter (trong Phương trình 6) có giúp cải thiện hiệu suất hay không. Như được mô tả trong Hình 6, khi dropout = 0.75 hoặc kích thước cụm = 4, đặt λ thành 1 sẽ có kết quả tốt hơn. Trong các trường hợp khác, tốt hơn là không áp dụng các ràng buộc giữa cụm bằng cách đặt λ thành 0.

Khi có đủ chuyên gia trong cụm, tốt hơn là không sử dụng ràng buộc giữa cụm bằng cách đặt λ thành 0. Các ràng buộc trong cụm đã làm cho các chuyên gia khác trong cụm có xác suất định tuyến cao hơn, trong khi các ràng buộc giữa cụm sẽ tiếp tục mở rộng khoảng cách xác suất định tuyến giữa các cụm. Điều này sẽ khiến entropy của phân phối xác suất định tuyến quá nhỏ, điều này không có lợi cho việc học của mạng có cổng.

Chúng tôi phát hiện rằng ràng buộc giữa cụm sẽ có lợi cho MoEC khi kích thước cụm nhỏ hoặc tỷ lệ dropout chuyên gia cao. Trong trường hợp này, số lượng chuyên gia trong cụm nhỏ, và chỉ riêng ràng buộc trong cụm không đủ để tạo thành phân phối xác suất định tuyến hợp lý toàn cục, vì vậy cần sự hỗ trợ của các ràng buộc giữa các cụm.

### Nâng cao giới hạn trên của MoE
Nói chung, số lượng chuyên gia cao hơn có nghĩa là khả năng mô hình cao hơn và hiệu suất tốt hơn. Tuy nhiên, đối với các nhiệm vụ có dữ liệu hạn chế, tồn tại một giới hạn trên về hiệu suất khi mở rộng quy mô các mô hình MoE. Chúng tôi đi sâu vào khả năng của MoEC để nâng cao giới hạn trên. Như được hiển thị trong Bảng 4, đối với MoE baseline, chuyên gia = 32 là giới hạn trên, có nghĩa là tiếp tục tăng số lượng chuyên gia sẽ không mang lại lợi ích gì cho mô hình. MoEC của chúng tôi không chỉ có lợi thế về hiệu suất so với MoE baseline với cùng số lượng chuyên gia mà còn cải thiện giới hạn trên từ 32 lên 64.

Với sự gia tăng của các chuyên gia, MoEC của chúng tôi có thể mang lại nhiều lợi ích hơn. Điều này là do MoEC có thể thể hiện đầy đủ khả năng đầy hứa hẹn của mình để giải quyết các vấn đề overfitting nghiêm trọng và phân bổ thưa thớt. Với việc giảm thiểu hai vấn đề trên, ưu thế của mô hình MoE quy mô lớn sẽ được phát huy tốt hơn, từ đó đạt được sự cải thiện về giới hạn trên của các mô hình MoE. Với sự hỗ trợ của MoEC, chúng tôi có thể thử xây dựng các mô hình thưa thớt với nhiều chuyên gia hơn.

## Kết luận
Trong nghiên cứu của chúng tôi, chúng tôi chỉ ra các vấn đề overfitting và phân bổ dữ liệu thưa thớt của các mô hình MoE quy mô lớn và đề xuất một chiến lược huấn luyện mới - MoEC để chuyển đổi các chuyên gia thành các cụm. Mỗi chuyên gia có thể nhận được nhiều mẫu huấn luyện phong phú và đa dạng hơn. Bằng cách này, dữ liệu huấn luyện của mỗi chuyên gia được giữ đầy đủ, từ đó giảm thiểu overfitting. Chúng tôi cũng đề xuất dropout chuyên gia ở cấp độ cụm để thực hiện điều chuẩn. Chúng tôi tiến hành thí nghiệm trên các nhiệm vụ dịch máy và hiểu ngôn ngữ tự nhiên. Kết quả thí nghiệm cho thấy MoEC có thể cải thiện hiệu suất và giảm thiểu các vấn đề gây ra bởi việc mở rộng quy mô chuyên gia mà không thay đổi cấu trúc mô hình và chiến lược định tuyến. Ưu thế của mô hình MoE quy mô lớn sẽ được phát huy tốt hơn bởi MoEC, từ đó nâng cao giới hạn trên của các mô hình MoE. Với sự hỗ trợ của MoEC, chúng tôi có thể thử xây dựng các mô hình thưa thớt với nhiều chuyên gia hơn.

## Tài liệu tham khảo
[Danh sách tài liệu tham khảo giữ nguyên định dạng gốc]

## Phụ lục

### A Lựa chọn giá trị của β
Bảng 5 trình bày các thí nghiệm về việc chọn giá trị tốt nhất của β. MoEC hoạt động tốt nhất khi β được đặt thành 1e-2. Và khi giá trị beta quá lớn, hiệu suất của MoEC giảm đáng kể, điều này xác nhận phân tích của chúng tôi trong văn bản chính. Dựa trên kết quả, chúng tôi đặt giá trị của β là 10^-2 làm mặc định trong tất cả các thí nghiệm ở trên.

### B Tham số kiến trúc
Bảng 6 trình bày các tham số kiến trúc cho các nhiệm vụ khác nhau.

### C Siêu tham số huấn luyện
Bảng 7 trình bày các siêu tham số huấn luyện cho WMT-14 và tiền huấn luyện. Bảng 8 trình bày các siêu tham số huấn luyện trên các nhiệm vụ GLUE hạ nguồn.
