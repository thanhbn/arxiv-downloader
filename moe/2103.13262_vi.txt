# 2103.13262.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2103.13262.pdf
# Kích thước tệp: 755611 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo
FASTMOE: MỘT HỆ THỐNG HUẤN LUYỆN MIXTURE-OF-EXPERT NHANH
Jiaao Heyz, Jiezhong Qiuyz, Aohan Zengyz, Zhilin Yangz], Jidong Zhaiyz, Jie Tangyz
yTrường Đại học Thanh HoazViện Trí tuệ Nhân tạo Bắc Kinh (BAAI)]Recurrent AI
fhja20,qiujz16,zah18 g@mails.tsinghua.edu.cn ;
kimi yang@rcrai.com ;fzhaijidong, jietang g@tsinghua.edu.cn
TÓM TẮT
Mixture-of-Expert (MoE) thể hiện tiềm năng mạnh mẽ trong việc mở rộng kích thước mô hình ngôn ngữ lên hàng nghìn tỷ tham số. Tuy nhiên, huấn luyện MoE quy mô nghìn tỷ đòi hỏi thiết kế đồng bộ thuật toán và hệ thống cho một hệ thống huấn luyện phân tán hiệu suất cao được điều chỉnh tốt. Thật không may, nền tảng duy nhất hiện có đáp ứng các yêu cầu phụ thuộc mạnh vào ngăn xếp phần cứng (TPU) và phần mềm (Mesh Tensorflow) của Google, và không mở cũng như không có sẵn cho công chúng, đặc biệt là cộng đồng GPU và PyTorch.

Trong bài báo này, chúng tôi trình bày FastMoE, một hệ thống huấn luyện MoE phân tán dựa trên PyTorch với các bộ tăng tốc thông thường. Hệ thống cung cấp giao diện phân cấp cho cả thiết kế mô hình linh hoạt và thích ứng dễ dàng với các ứng dụng khác nhau, chẳng hạn như Transformer-XL và Megatron-LM. Khác với việc triển khai trực tiếp các mô hình MoE sử dụng PyTorch, tốc độ huấn luyện được tối ưu hóa cao trong FastMoE bằng các kỹ thuật tăng tốc hiệu suất cao tinh vi. Hệ thống hỗ trợ đặt các chuyên gia khác nhau trên nhiều GPU trên nhiều nút, cho phép mở rộng số lượng chuyên gia tuyến tính theo số lượng GPU. Mã nguồn của FastMoE có sẵn tại https://github.com/laekov/fastmoe dưới giấy phép Apache-2.

1 GIỚI THIỆU
Sự xuất hiện gần đây của các mô hình ngôn ngữ quy mô lớn, được minh họa bởi BERT (Devlin et al., 2018), GPT-2/-3 (Radford et al., 2019; Brown et al., 2020), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020), GShard (Chen et al., 2020) và Switch Transformer (Fedus et al., 2021), đã thay đổi hoàn toàn bối cảnh nghiên cứu xử lý ngôn ngữ tự nhiên, thiết lập lại các đường cơ sở tiên tiến mới trong các điểm chuẩn khác nhau như GLUE (Wang et al., 2018) và SuperGLUE (Wang et al., 2019).

Trong số nhiều giải pháp có thể, việc mở rộng kích thước mô hình đã được chứng minh là một trong những cách đơn giản và hiệu quả nhất (Kaplan et al., 2020) hướng tới các mô hình mạnh mẽ hơn. Từ BERT (Devlin et al., 2018) với 340 triệu tham số, đến T5 (Raffel et al., 2020) với 11 tỷ tham số, đến GPT-3 (Brown et al., 2020) với 175 tỷ tham số, kích thước mô hình được mở rộng 500 lần chỉ trong hai năm. Gần đây hơn, GShard (Chen et al., 2020) mở rộng lên con số kỷ lục 600 tỷ tham số, nhanh chóng bị phá vỡ bởi Switch Transformer (Fedus et al., 2021) với 1,6 nghìn tỷ tham số. Người đóng góp chính cho kích thước mô hình khổng lồ của GShard và Switch Transformer là một kiến trúc mạng neural mới có tên mixture of experts (MoE) (Shazeer et al., 2017).

Một tầng MoE (một ví dụ minh họa có thể được tìm thấy trong Hình 1) bao gồm một cổng và một nhóm các chuyên gia. Đối với mỗi đầu vào, chỉ một thiểu số nhỏ các chuyên gia được cổng chọn để tính toán. Kiến trúc đặc biệt của MoE là một con dao hai lưỡi cho huấn luyện phân tán quy mô lớn. Một mặt, do kích hoạt thưa thớt của các chuyên gia, MoE có thể mở rộng kích thước mô hình theo bậc độ lớn mà không tăng đáng kể lượng tính toán (FLOPs). Mặt khác, khi mở rộng lên hàng nghìn chuyên gia, mô hình giao tiếp all-to-all không cân bằng của MoE mang lại những thách thức mới cho việc thiết kế đồng bộ thuật toán và hệ thống. Do đó, MoE không thể được hỗ trợ trực tiếp bởi các thư viện học sâu truyền thống như PyTorch (Paszke et al., 2019) và TensorFlow (Abadi et al., 2016).

1arXiv:2103.13262v1  [cs.LG]  24 Mar 2021

--- TRANG 2 ---
Bản thảo
Đầu vàoMạng cổng Điểm số
Bộ chọn chuyên gia Bộ tổng hợp Đầu raMạng chuyên gia 1
Mạng chuyên gia 2
Mạng chuyên gia 3
Mạng chuyên gia n
Hình 1: Một ví dụ minh họa về tầng MoE. Trong ví dụ này, chuyên gia 1 và chuyên gia 3 được cổng chọn để tính toán.

Do thách thức được nêu ra bởi kiến trúc mô hình mới, cả cộng đồng nghiên cứu và ngành công nghiệp đều cần một triển khai MoE hỗ trợ huấn luyện phân tán quy mô lớn. Tuy nhiên, mặc dù tồn tại một số triển khai GPU đơn naive trong PyTorch (Rau, 2019), hệ thống duy nhất hiện tại hỗ trợ huấn luyện MoE có thể mở rộng dựa trên ngăn xếp phần cứng và phần mềm riêng tư của Google — TPU (Jouppi et al., 2017) và Mesh TensorFlow (Shazeer et al., 2018). Do đó, có nhu cầu cấp thiết phát triển một hệ thống MoE trên phần cứng có sẵn công khai (ví dụ: GPU) và nền tảng (ví dụ: PyTorch (Paszke et al., 2019)).

Được thúc đẩy bởi mong muốn có được giải pháp dễ sử dụng, linh hoạt, hiệu quả, có thể mở rộng và mã nguồn mở cho huấn luyện MoE quy mô lớn, chúng tôi phát hành FastMoE với các mục tiêu thiết kế sau:

•Dễ sử dụng: cung cấp giao diện thân thiện với người dùng để định nghĩa tầng MoE, và hỗ trợ liền mạch cho hệ thống huấn luyện mô hình ngôn ngữ phổ biến, Megatron-LM (Shoeybi et al., 2019).

•Linh hoạt: làm cho người dùng dễ dàng tùy chỉnh mạng cổng và mạng chuyên gia.

•Hiệu quả: tích hợp tầng feedforward (FFN) được tối ưu hóa cao cho Transformer.

•Có thể mở rộng: hỗ trợ mở rộng kích thước mô hình MoE bằng cách huấn luyện trên nhiều GPU trên nhiều nút.

Khác với triển khai PyTorch GPU đơn trước đây (Rau, 2019), FastMoE tập trung vào hiệu quả và khả năng mở rộng. Các kernel CUDA chuyên dụng được bao gồm trong FastMoE để đạt hiệu suất cao với các tối ưu hóa chuyên biệt. FastMoE có thể chạy trên nhiều GPU trên nhiều nút sử dụng NCCL (Jeaugey, 2017). Chi tiết giao tiếp được ẩn khỏi các nhà phát triển mô hình bởi FastMoE. Phương pháp song song mô hình của FastMoE cho phép phân phối các chuyên gia trên các GPU khác nhau, trong khi các phần khác của mô hình vẫn được song song hóa theo chiều batch (song song dữ liệu) hoặc chiều tensor (song song mô hình). Có khả năng kích thước mô hình, tỷ lệ thuận với số lượng chuyên gia, có thể mở rộng với số lượng GPU được sử dụng để huấn luyện, là chìa khóa để huấn luyện các mô hình quy mô nghìn tỷ.

Trong thí nghiệm của chúng tôi, chúng tôi quan sát thấy FastMoE nhanh hơn so với đường cơ sở (Rau, 2019) được triển khai bằng API PyTorch thuần túy trên một GPU đơn. FastMoE cũng cho thấy khả năng mở rộng hợp lý khi chạy trên các nút trong một cụm kết nối bằng mạng Infiniband. Chúng tôi huấn luyện một mô hình GPT thực với 96 chuyên gia mỗi tầng sử dụng FastMoE phân tán với tốc độ huấn luyện end-to-end đầy hứa hẹn. So với mô hình không MoE có cùng lượng tính toán, hiệu suất của nó được hưởng lợi từ kích thước mô hình được mở rộng mà kiến trúc MoE mang lại.

Bài báo này được tổ chức như sau. Phần 2 giới thiệu bối cảnh của MoE và so sánh các hệ thống hiện có. Phần 3 trình bày hệ thống FastMoE chi tiết. Phần 4 giới thiệu các thách thức của việc đạt hiệu suất cao và các giải pháp của FastMoE. Phần 5 hiển thị kết quả các thí nghiệm chứng minh hiệu quả của FastMoE và lợi ích hiệu suất của mô hình MoE sử dụng FastMoE trong huấn luyện. Phần 6 tóm tắt bài báo và chỉ ra hướng công việc tương lai của chúng tôi.

2 MIXTURE-OF-EXPERTS (MOE)
Trong phần này, chúng tôi xem xét kiến trúc của MoE và các hệ thống hiện tại để huấn luyện MoE.

2.1 MOE: CẤU TRÚC MÔ HÌNH
Mixture-of-Expert là viết tắt của Sparsely-Gated Mixture-of-Experts layers được đề xuất bởi Shazeer et al. (2017). Một tầng MoE bao gồm nhiều chuyên gia, mỗi chuyên gia có thể là một mạng neural tùy ý. Ràng buộc duy nhất của các chuyên gia là chúng phải nhận cùng đầu vào và cho đầu ra trong cùng không gian vector. Hình 1 hiển thị một ví dụ chi tiết về tầng MoE. Một mạng neural đặc biệt, gọi là mạng cổng, được giới thiệu để chấm điểm mỗi chuyên gia trên một đầu vào cho trước. Theo điểm số, việc lựa chọn chuyên gia được thực hiện bởi một chính sách, có thể khác nhau từ mô hình này sang mô hình khác. Sau đó, các chuyên gia được chọn, ví dụ chuyên gia 1 và 3 trong ví dụ, được kích hoạt để xử lý mẫu đầu vào. Các đầu ra của chuyên gia, cùng với điểm số, được kết hợp thành đầu ra cuối cùng sử dụng một thuật toán nhất định.

Một cách phổ biến của việc lựa chọn chuyên gia là chọn các chuyên gia có k điểm số cao nhất. Trong quá trình tổng hợp, điểm số được sử dụng làm trọng số cho đầu ra của các chuyên gia để được cộng vào đầu ra tổng thể. Điều này cho phép huấn luyện mạng cổng, vì gradient có thể được lan truyền qua điểm số. Thuật toán 1 hình thức hóa phương pháp trên.

Thuật toán 1 Tính toán tiến của tầng MoE với cổng top-k.
Yêu cầu: Một nhóm n chuyên gia: {E1; E2; ...; En}
Yêu cầu: Cổng G
Yêu cầu: Số lượng chuyên gia k được chọn
1: function MOE(x)
2: score ← G(x)
3: indices ← ArgMaxk(score)
4: y ← zero tensor like x
5: foreach index i ∈ indices do
6: xi ← Ei(x)
7: y ← scorei * xi + y
8: end for
9: return y
10: end function

2.2 CÁC HỆ THỐNG HIỆN TẠI CHO HUẤN LUYỆN MOE
Hệ thống GShard (Chen et al., 2020) triển khai một phiên bản phân tán của mô hình MoE. Nó huấn luyện một mô hình ngôn ngữ trên tới 2048 TPU, với 1 chuyên gia mỗi tầng được đặt trên mỗi TPU. Kết quả là, các tầng MoE chứa nhiều hơn 2048 lần tham số so với tầng không MoE. Trong Switch Transformer (Fedus et al., 2021), mô hình được mở rộng thêm lên 1,6 nghìn tỷ, cho thấy khả năng mạnh mẽ của hệ thống để hỗ trợ huấn luyện mô hình ở quy mô lớn. Thật không may, hệ thống này chưa có sẵn công khai. Nó gắn liền chặt chẽ với cụm TPU, điều này khiến việc tái tạo các thí nghiệm trên các thiết bị thông thường trở nên khó khăn. Ngoài ra, thiết kế của GShard thiếu tính linh hoạt để sử dụng số lượng và kích thước chuyên gia khác nhau với chiến lược sao chép khác nhau.

Trong Tensor2tensor (Vaswani et al., 2018), một mô hình MoE Transformer được cung cấp. Tuy nhiên, triển khai này sử dụng Mesh TensorFlow (Shazeer et al., 2018), không hỗ trợ GPU rất tốt. Để triển khai một FFN trong Transformer, cần hơn 100 dòng code trong TensorFlow với các toán tử einsum phức tạp, gây khó khăn cho các nhà phát triển hiểu cấu trúc và khám phá các cấu trúc mô hình khác dựa trên code.

PyTorch (Paszke et al., 2019), như một framework học sâu phổ biến trong giới nghiên cứu, cung cấp phong cách coding trực tiếp hơn và tính linh hoạt hơn so với TensorFlow. Những nỗ lực được thực hiện để huấn luyện các mô hình MoE với PyTorch (Rau, 2019). Tuy nhiên, vì cộng đồng PyTorch thiếu các công cụ huấn luyện song song đa chiều, không có triển khai nào dựa trên PyTorch hỗ trợ huấn luyện trên nhiều GPU. Vì mục tiêu cuối cùng của việc áp dụng MoE là huấn luyện các mô hình thậm chí còn lớn hơn, các triển khai dựa trên PyTorch không thể trở thành ứng cử viên.

3 FastMoE: THIẾT KẾ HỆ THỐNG
Trong phần này, chúng tôi giới thiệu thiết kế FastMoE của chúng tôi với hỗ trợ huấn luyện phân tán.

--- TRANG 4 ---
Bản thảo

3.1 MỘT HỆ THỐNG LINH HOẠT CHO CÁC NHÀ KHÁM PHÁ MÔ HÌNH ĐA DẠNG
Xương sống để Chạy Mạng Chuyên gia Tùy ý. FastMoE hỗ trợ sử dụng mạng tùy ý làm chuyên gia. Giao diện FMoE của FastMoE nhận bất kỳ constructor module mạng neural nào làm đầu vào, và sao chép module nhiều lần làm các thể hiện chuyên gia. Chuyên gia được định nghĩa để nhận một batch các đặc trưng đầu vào liền kề được căn chỉnh, và đầu ra phải theo cùng thứ tự batch. Do đó, việc triển khai module chuyên gia được tách rời khỏi kiến trúc MoE để các nhà phát triển có thể tập trung vào thiết kế mạng chuyên gia của riêng họ.

Để có tính linh hoạt thậm chí mạnh hơn, lớp FMoE chứa một hàm thành viên expert_fn, nơi các module chuyên gia được sử dụng để thực hiện tính toán tiến. Hàm này có thể được overload để có hành vi MoE tùy chỉnh thêm. Ví dụ, trong mạng FMoETransformerMLP, sẽ được đề cập sau trong phần này, danh sách các chuyên gia được thay thế bằng một module được tối ưu hóa đặc biệt áp dụng các chuyên gia song song để giảm cực kỳ độ trễ.

Hơn nữa, FastMoE hỗ trợ đặt nhiều chuyên gia cùng nhau trên cùng một worker, cho phép không gian cấu hình linh hoạt hơn về số lượng chuyên gia (tức là, số lượng chuyên gia không phải bằng số lượng data parallel), khác với thiết kế của GShard.

Một FFN Được Tối ưu hóa Cao cho Transformer. Để hỗ trợ tốt hơn việc huấn luyện Transformer với MoE, FastMoE cung cấp một triển khai FFN chuẩn và hiệu suất cao (FMoETransformerMLP). Chiến lược tối ưu hóa chi tiết được ẩn khỏi các nhà phát triển. Đặc biệt, khi đặt nhiều chuyên gia trên cùng một worker, một triển khai naive là lặp qua các chuyên gia này và thực hiện forward tuần tự. Tuy nhiên, đối với một số loại mạng chuyên gia nhất định, có thể khám phá tiềm năng tăng tốc mang lại bởi việc thực thi song song. Trong FastMoE, chúng tôi chủ yếu tối ưu hóa việc thực thi song song của các tầng fully-connected bằng một module FMoELinear chuyên dụng. Thay vì tính toán các module chuyên gia tuần tự, module chuyên gia được tối ưu hóa đặc biệt duy trì một pool các tài nguyên phần cứng có sẵn, và áp dụng tính toán chuyên gia song song.

Hỗ trợ Kiểu Plugin cho PyTorch và Megatron-LM. Tính linh hoạt của FastMoE cho phép thích ứng thuận tiện với các ứng dụng huấn luyện hiện có. Lấy Megatron-LM (Shoeybi et al., 2019) làm ví dụ, một module kiểu plugin được tích hợp trong FastMoE để nhanh chóng thay thế các FFN trong mô hình Megatron-LM gốc bằng mạng MoE. Như được hiển thị trong listing 1, việc chuyển đổi có thể được thực hiện chỉ bằng 2 dòng code.

Listing 1: Code mẫu để sử dụng FastMoE trong Megatron-LM
from fmoe.megatron import fmoefy
model = fmoefy(model, num_experts=<số lượng chuyên gia mỗi worker>)

Hàm fmoefy có thể tìm các FFN trong các tầng Transformer. Sau đó, một mạng MoE sử dụng FastMoE được tạo ra, đó là một module bao bọc module FMoETransformerMLP để tương thích ở cấp độ giao diện.

3.2 MỞ RỘNG DUNG LƯỢNG MÔ HÌNH PHÂN TÁN
Phương pháp Song song Mô hình của FastMoE. Như một trong những cách hiệu quả nhất để mở rộng dung lượng mô hình, khả năng chứa một số lượng chuyên gia lớn và huấn luyện chúng song song được yêu cầu trong nhiều mô hình MoE. Việc xử lý việc truyền dữ liệu phức tạp giữa các GPU và thậm chí qua các nút là khó khăn đối với các nhà phát triển mô hình. Đạt được hiệu suất huấn luyện cao và sử dụng tài nguyên phần cứng tốt đòi hỏi chuyên môn về kiến trúc máy tính và lập trình song song, vượt quá ngăn xếp kỹ thuật của các nhà phát triển mô hình thông thường.

FastMoE hỗ trợ phân phối các chuyên gia trên nhiều worker trên nhiều nút, được gọi là phương pháp song song mô hình trong FastMoE. Chi tiết trao đổi dữ liệu đầu vào được ẩn trong giao diện FMoE. Đối với các nhà phát triển mô hình, họ chỉ cần viết code cho một chuyên gia đơn, và mỗi chuyên gia được cung cấp tất cả dữ liệu đầu vào được thu thập từ tất cả worker bởi FastMoE. Kết quả là, các nhà phát triển mô hình không phải xem xét chi tiết triển khai về giao tiếp cross-worker.

--- TRANG 5 ---
Bản thảo
Trong thiết kế của FastMoE, khi tính năng phân phối chuyên gia trên các worker được kích hoạt, các phép toán giao tiếp bổ sung được bao gồm trong tính toán tiến và lùi. Để xác định tốt hơn các phép toán, chúng tôi gọi chúng là phép toán trao đổi dữ liệu toàn cục, trái ngược với quá trình xáo trộn dữ liệu cục bộ, sẽ được đề cập trong phần 4.

Một thách thức chính trong bối cảnh phân tán là tổng số mẫu đầu vào được gán cho tất cả chuyên gia trên một worker có thể khác nhau rất nhiều. Không thể có số lượng mẫu đến trước khi đầu ra cổng có sẵn. Tuy nhiên, việc phân bổ buffer để đặt các mẫu đầu vào phụ thuộc vào số lượng. Do đó, trước khi trao đổi thực tế các mẫu đầu vào giữa các worker xảy ra sau khi trao đổi thông tin số lượng giữa các worker, và phân bổ bộ nhớ theo kiểm tra thông tin đếm chuyên gia.

Worker 1mẫu của chuyên gia 0
mẫu của chuyên gia 1
mẫu của chuyên gia 2
mẫu của chuyên gia 0
mẫu của chuyên gia 1
mẫu của chuyên gia 2
mẫu của chuyên gia 0
mẫu của chuyên gia 1
mẫu của chuyên gia 2Worker 0
Worker 2Worker 1# mẫu của chuyên gia 0
# mẫu của chuyên gia 1
# mẫu của chuyên gia 2# mẫu của chuyên gia 0
# mẫu của chuyên gia 1
# mẫu của chuyên gia 2# mẫu của chuyên gia 0
# mẫu của chuyên gia 1
# mẫu của chuyên gia 2Worker 0
Worker 2Worker 1đầu vào của chuyên gia 0
đầu vào của chuyên gia 1
đầu vào của chuyên gia 2đầu vào của chuyên gia 0
đầu vào của chuyên gia 1
đầu vào của chuyên gia 2đầu vào của chuyên gia 0
đầu vào của chuyên gia 1
đầu vào của chuyên gia 2Worker 0
Worker 2Trao đổi
kích thước
Phân bổ
buffer
Phân bổ
buffer
Phân bổ
bufferTrao đổi
dữ liệu

Hình 2: Một ví dụ về các phép toán toàn cục.

Một ví dụ về các phép toán toàn cục trong FastMoE được hiển thị trong hình 2. Các worker đầu tiên đếm số lượng mẫu được gán cho mỗi chuyên gia trên mỗi worker. Sau đó, họ trao đổi kích thước của đầu vào chuyên gia, để tất cả worker nhận được số lượng mẫu đầu vào đến và chúng đến từ đâu. Sau khi offset của mỗi buffer nhận được tính toán, các worker bắt đầu trao đổi dữ liệu trực tiếp. Điều đáng chú ý là thống kê của các mẫu đến và đi có thể được tái sử dụng trong toàn bộ quá trình của một lần lặp huấn luyện.

Module Đồng bộ Nhận biết Tính Dị tốc. Tính dị tốc được giới thiệu khi các phần khác nhau của mạng có thể được sao chép trên các nhóm worker khác nhau. Đó là một thách thức mà module phân tán phải xác định liệu gradient của một tham số có nên được đồng bộ hay không, và với ai nó được đồng bộ. FastMoE giới thiệu tag nhóm giao tiếp song song dữ liệu trên mỗi tham số để giải quyết vấn đề.

Tag có thể là một trong số world, data_parallel hoặc none, tương ứng chỉ ra rằng gradient nên được đồng bộ với (1) tất cả worker khác, (2) các worker trong nhóm data-parallel trực giao với nhóm model-parallel, hoặc (3) không có worker nào. Ví dụ, mạng cổng được sao chép trên tất cả worker, bất kể cài đặt model parallel. Tầng attention có thể được chia thành các sub-layer model-parallel, vì vậy tag của nó là data_parallel. Mỗi worker phục vụ một số mạng chuyên gia độc đáo, có tag là none. Một module data parallel tùy chỉnh thay vì module distributed data parallel gốc của PyTorch được cung cấp bởi FastMoE, có thể xác định các tag và thực hiện đồng bộ chính xác.

4 TỐI ƯU HÓA ĐỂ ĐẠT HIỆU SUẤT CAO
Hiệu suất của tính toán MoE trên một nút đơn là quan trọng, vì nó xác định giới hạn trên lý thuyết của hệ thống mở rộng lên bất kỳ quy mô nào.

--- TRANG 6 ---
Bản thảo
Cách trực quan nhất để tính toán một tầng MoE là cắt batch đầu vào thành các mẫu, và tính toán từng mẫu một. Sau đó, các đặc trưng đầu ra được xếp chồng theo thứ tự gốc. Tuy nhiên, được quan sát rằng việc triển khai mô hình MoE sử dụng các toán tử PyTorch đơn giản khó có thể đạt hiệu suất cao. Ít hơn 5% hiệu suất đỉnh của GPU có thể đạt được.

1 2 4 816 64 256 1024 4096 16384 65536
kích thước batch0246810121416TFLOPs

Hình 3: Hiệu suất GeMM của các kích thước bài toán khác nhau sử dụng cuBLAS trên NVIDIA V100.

Không mất tính tổng quát, chúng tôi giả định mạng chuyên gia là một FFN. Lưu ý rằng toán tử chính trong FFN là từ các tầng fully-connected, bao gồm một số toán tử GeMM. Khi batch được chia thành các mẫu đơn, phép toán GeMM bị suy giảm thành GeMV. Hình 3 hiển thị thông lượng tính toán floating-point của một tầng fully-connected ví dụ sử dụng kích thước batch khác nhau. Với việc trong các thiết bị tính toán dị tốc hiện đại, các toán tử nhân ma trận được điều chỉnh tinh vi với các kỹ thuật tiling được áp dụng trên tất cả chiều, không có gì ngạc nhiên khi thông lượng chỉ có thể tiếp cận đỉnh lý thuyết khi kích thước batch đủ lớn. Điều này dẫn đến nguyên tắc rằng để đạt hiệu suất cao trong tính toán MoE, các mẫu nên được batch để sử dụng đầy đủ tài nguyên phần cứng.

FastMoE batch tất cả mẫu đầu vào đến cùng một chuyên gia lại với nhau. Do giới hạn của biểu diễn dữ liệu, FastMoE thực hiện di chuyển bộ nhớ với kernel CUDA được phát triển đặc biệt để giảm overhead. Với chỉ số của cổng mà mỗi mẫu sẽ đi đến, quá trình đặt tất cả mẫu đầu vào đến cùng cổng trong một không gian bộ nhớ liền kề được gọi là scatter. Tuy nhiên, trong các phần khác của mạng neural, batch có thể phải được tổ chức theo thứ tự gốc của nó, ví dụ, tầng attention trong Transformer. Một phép toán ngược được thực hiện sau khi các chuyên gia xuất ra một không gian bộ nhớ liền kề khác, tức là đặt các vector đặc trưng scattered trở lại thứ tự gốc của chúng theo chỉ số cổng. Quá trình này được ký hiệu là gather trong FastMoE.

đầu vào 0
đầu vào 1
đầu vào 2
đầu vào 3
đầu vào 4
đầu vào 5đầu ra cổng:
0, 1, 2, 1, 1, 2
đầu vào 0
đầu vào 1
đầu vào 2đầu vào 3
đầu vào 4đầu vào 5
Scatterchuyên gia 1chuyên gia 0
chuyên gia 2đầu ra 0
đầu ra 1
đầu ra 2đầu ra 3
đầu ra 4đầu ra 5đầu ra 0
đầu ra 1
đầu ra 2
đầu ra 3
đầu ra 4
đầu ra 5Gather

Hình 4: Một ví dụ về tính toán được sắp xếp lại của tầng MoE

Quá trình tính toán sắp xếp lại được hiển thị như Hình 4. Khi việc gán từ mẫu đầu vào đến chuyên gia đủ cân bằng, mỗi chuyên gia được mong đợi có kích thước batch đầu vào tương đối lớn có thể đạt mức sử dụng phần cứng thỏa mãn theo Hình 3. Tuy nhiên, mất cân bằng tải luôn xảy ra vì bản chất của việc lấy mẫu ngẫu nhiên của dữ liệu huấn luyện đầu vào. Rất có thể một chuyên gia nhận rất ít mẫu đầu vào trong hàng triệu lần lặp huấn luyện. Ngoài ra, vì nhiều chuyên gia được đặt trên một worker, kích thước batch cục bộ của các chuyên gia, trung bình, thống kê thấp hơn so với trong data parallel. FastMoE sử dụng một stream manager tùy chỉnh để đồng thời thực thi tính toán của nhiều chuyên gia để trích xuất lợi ích thông lượng tiềm năng.

5 ĐÁNH GIÁ
Trong phần này, tốc độ huấn luyện của FastMoE được so sánh với một triển khai PyTorch MoE khác (Rau, 2019) trên một GPU đơn. Chúng tôi cũng báo cáo khả năng mở rộng của FastMoE khi huấn luyện phân tán. Theo hiểu biết tốt nhất của chúng tôi, FastMoE là hệ thống MoE dựa trên PyTorch duy nhất có thể chạy trên các nút và GPU khác nhau. Chúng tôi cũng hiển thị hiệu suất end-to-end của mô hình MoE Transformer được huấn luyện sử dụng FastMoE.

5.1 THIẾT LẬP THÍ NGHIỆM
Chúng tôi sử dụng các ký hiệu sau để đặc trưng cho tác vụ tính toán: ne chuyên gia được đặt trên mỗi GPU. Mỗi chuyên gia áp dụng hai tầng tuyến tính có kích thước dm×dh và dh×dm tương ứng. Đầu vào chứa nb mẫu. Module cổng chấm điểm độ phù hợp của mỗi mẫu để được xử lý bởi mỗi chuyên gia. Đối với mỗi mẫu đầu vào, các chuyên gia có k điểm số cao nhất được chọn để xử lý mẫu. Ngoài ra, một số vòng warm-up được thực hiện, thực hiện cùng tính toán nhưng không được tính trong kết quả. Đối với mỗi thí nghiệm, tác vụ được thực thi 16 lần, và thời gian trung bình được sử dụng để tính toán hiệu suất. Các giá trị độ lệch chuẩn của thời gian thực thi cũng được kiểm tra. Tất cả đều không đáng kể.

5.2 TỐC ĐỘ HUẤN LUYỆN TRÊN MỘT GPU ĐƠN
Hiệu suất của FMoETransformerMLP được kiểm tra, hoàn thành tác vụ tương tự với module MoE trong đường cơ sở (Rau, 2019), trên GPU NVIDIA TESLA V100 PCIe. Đường cơ sở được triển khai bằng API PyTorch thuần túy với cấu trúc mô hình hard-coded. Để công bằng trong so sánh, cả hai module đều sử dụng ma trận được khởi tạo ngẫu nhiên làm trọng số của mạng cổng, bao gồm một tầng fully-connected. Các chuyên gia cũng thực hiện cùng tính toán.

2 4 8 16 32 64 128
Số lượng chuyên gia01020304050607080Thời gian tính toán / msBaseline fwd
Baseline bwd
FastMoE fwd
FastMoE bwd
Độ trễ được kiểm tra với nb= 4096 ; dm= 1024 ; dh= 4096 ; k= 2.

Hình 5: So sánh thời gian tính toán giữa FastMoE và triển khai baseline.

Như Hình 5 hiển thị, triển khai baseline liên tục chậm hơn FastMoE. Khi số lượng chuyên gia tăng, baseline tốn nhiều thời gian hơn trong tính toán tiến, trong khi độ trễ của FastMoE vẫn ổn định, nhờ vào stream manager tùy chỉnh được đề cập trong Phần 4. Xét rằng FastMoE nhắm mục tiêu vào huấn luyện, thời gian backward được xếp chồng lên thời gian forward. Chúng tôi quan sát rằng FastMoE vượt trội hơn baseline trong tổng thời gian dành cho mỗi lần lặp.

--- TRANG 8 ---
Bản thảo

5.3 KHẢ NĂNG MỞ RỘNG CROSS-GPU VÀ CROSS-NODE
Để kiểm tra hiệu suất của FastMoE mở rộng trên nhiều GPU qua các nút, chúng tôi thực hiện một thí nghiệm trên một cụm 8 nút, với 1 GPU NVIDIA Tesla V100 trên mỗi nút. Cụm được kết nối qua switch Infiniband EDR và 8 card HCA. FLOPs của các phép toán nhân ma trận được tính toán để đại diện cho thông lượng huấn luyện.

1 2 3 4 5 6 7 8
Số lượng GPU510152025TFLOPs
Tổng thông lượng
Thông lượng trung bình mỗi GPU
Thông lượng được kiểm tra với ne= 4; nb= 4096 ; dm= 1024 ; dh= 4096 ; k= 2.

Hình 6: Khả năng mở rộng của FastMoE trên nhiều GPU trên nhiều nút

Theo kết quả được hiển thị trong Hình 6, FastMoE cho thấy khả năng mở rộng qua các nút. Thông lượng tổng thể tăng từ 10 TFLOPs lên 25 TFLOPs, khi số lượng GPU tăng từ 2 lên 8, mở rộng dưới tuyến tính. Chúng tôi quan sát thấy khi mở rộng lên 2 GPU, hiệu suất bằng một nửa so với trên một GPU đơn, điều này cho thấy FastMoE bị giới hạn bởi giao tiếp. Khi nhiều GPU được sử dụng cho tính toán, nhiều chuyên gia được giới thiệu, và độ chi tiết của việc trao đổi mẫu đầu vào trở nên nhỏ hơn, làm giảm hiệu quả trong truyền dữ liệu qua mạng.

Kết luận, khả năng mở rộng của FastMoE có thể hỗ trợ huấn luyện mô hình MoE lớn sử dụng nhiều GPU qua nhiều nút với lợi ích hiệu suất. Tuy nhiên, vẫn còn không gian để tối ưu hóa thêm về thông lượng.

5.4 HIỆU SUẤT END-TO-END SỬ DỤNG FastMoE
Chúng tôi kiểm tra lợi ích hiệu suất end-to-end sử dụng FastMoE bằng cách huấn luyện mô hình GPT 12 tầng trên 8 GPU sử dụng Megatron-LM (Shoeybi et al., 2019). Như được đề cập trong Phần 3, adapter Megatron của FastMoE được sử dụng cho cấu trúc MoE. Đối với mỗi tầng, 96 chuyên gia được phân phối trên các GPU, tức là 12 chuyên gia được đặt trên mỗi GPU. Đối với mỗi token đầu vào, top 2 chuyên gia có điểm số cao nhất được sử dụng để xử lý nó. dh trong tầng MLP chuyên gia được giảm một nửa để FLOPs hữu hiệu của mô hình gần như giống hệt nhau, ngoại trừ FLOPs bổ sung được giới thiệu bởi cổng, không đáng kể. Cả mô hình baseline và mô hình MoE đều được huấn luyện trong 70 giờ. Metric lm_loss trong huấn luyện chỉ ra xu hướng hội tụ của các mô hình.

Từ Hình 7, chúng tôi quan sát thấy rằng tốc độ huấn luyện của mô hình baseline khoảng 3× so với FastMoE. Vì FastMoE thực hiện nhiều tính toán và giao tiếp hơn, đây là sự chậm lại hợp lý. May mắn thay, mô hình MoE đạt được loss thấp hơn nhiều với cùng số lần lặp huấn luyện. Ngoài ra, như một lợi ích từ hiệu quả của FastMoE, mô hình MoE đạt được loss thấp hơn trong cùng thời gian huấn luyện.

6 TÓM TẮT VÀ CÔNG VIỆC TƯƠNG LAI
Trong bài báo này, chúng tôi trình bày FastMoE, một hệ thống mã nguồn mở để huấn luyện các mô hình Mixture-of-Experts. Hệ thống dựa trên framework PyTorch phổ biến, và hiện tại hỗ trợ huấn luyện hiệu quả trên GPU. Giao diện thân thiện của nhiều cấp độ được cung cấp cho các người dùng khác nhau để khám phá các khía cạnh khác nhau của kiến trúc MoE. Hiệu suất của FastMoE trên một GPU đơn được tối ưu hóa tốt để

--- TRANG 9 ---
Bản thảo

0 100000 200000 300000
Bước3.003.253.503.754.004.254.504.755.00lm loss
0 20 40 60
Thời gian / giờBaseline
FastMoE 96 chuyên gia
Các đường tối màu hẹp được làm mượt theo cấp số nhân bởi 0,97 từ đường cong loss gốc, được biểu diễn bởi các đường cong sáng rộng tương ứng.

Hình 7: Đường cong loss của việc huấn luyện mô hình GPT bằng FastMoE

khai thác sức mạnh của GPU. FastMoE cũng có thể chạy trên các GPU trên nhiều nút với khả năng mở rộng hợp lý, cho phép mở rộng thêm kích thước mô hình. Lợi thế hiệu suất mô hình thực được quan sát trong thí nghiệm huấn luyện mô hình end-to-end của chúng tôi sử dụng FastMoE.

Chúng tôi vẫn đang làm việc trên FastMoE để có thêm tính năng và huấn luyện nhanh hơn. So với mô hình GShard (Chen et al., 2020), FastMoE thiếu các chức năng để hỗ trợ cân bằng tải giữa các chuyên gia. Công việc về monitor cân bằng tải và hỗ trợ cho loss cân bằng tải đang được tiến hành. Chúng tôi cũng đang cố gắng làm cho hệ thống thân thiện hơn với người dùng về các tiện ích, chẳng hạn như loading và saving các mô hình MoE. Hiệu suất trên nhiều GPU đòi hỏi nỗ lực chung từ quan điểm của cả tính toán hiệu suất cao và machine learning. Bất kỳ đóng góp nào cho dự án mã nguồn mở sẽ được đánh giá cao. Chúng tôi mong đợi sự tham gia của bạn.

--- TRANG 10 ---
Bản thảo

TÀI LIỆU THAM KHẢO
Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16), pp. 265–283, 2016.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.

Dehao Chen, Dmitry Dima Lepikhin, HyoukJoong Lee, Maxim Krikun, Noam Shazeer, Orhan Firat, Yanping Huang, Yuanzhong Xu, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. 2020.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2021.

Sylvain Jeaugey. Nccl 2.0. In GPU Technology Conference (GTC), 2017.

Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensor processing unit. In Proceedings of the 44th annual international symposium on computer architecture, pp. 1–12, 2017.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1–67, 2020.

David Rau. Sparsely-gated mixture-of-experts pytorch implementation, 2019. URL https://github.com/davidmrau/mixture-of-experts.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

--- TRANG 11 ---
Bản thảo

Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorflow: Deep learning for supercomputers. arXiv preprint arXiv:1811.02084, 2018.

Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053, 2019.

Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353–355, 2018.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019.

Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.
