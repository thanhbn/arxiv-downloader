# 2402.01739.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2402.01739.pdf
# Kích thước tệp: 1304825 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
OpenMoE: Nỗ lực Sơ khai về Mô hình Ngôn ngữ
Mixture-of-Experts Mở
Fuzhao Xue1†Zian Zheng1Yao Fu2Jinjie Ni1Zangwei Zheng1
Wangchunshu Zhou3Yang You1
1Đại học Quốc gia Singapore
2Đại học Edinburgh
3ETH Zurich

Tóm tắt
Để giúp cộng đồng mã nguồn mở hiểu rõ hơn về các mô hình ngôn ngữ lớn (LLMs) dựa trên Mixture-of-Experts (MoE), chúng tôi huấn luyện và phát hành OpenMoE, một loạt các mô hình MoE LLM chỉ giải mã, hoàn toàn mã nguồn mở và có thể tái tạo, từ 650M đến 34B tham số và được huấn luyện trên hơn 1T token. Nghiên cứu của chúng tôi xác nhận rằng các LLM dựa trên MoE có thể cung cấp sự đánh đổi hiệu quả chi phí thuận lợi hơn so với các LLM dày đặc, làm nổi bật tiềm năng hiệu quả cho việc phát triển LLM trong tương lai.

Một đóng góp quan trọng khác của nghiên cứu này là phân tích sâu về các cơ chế định tuyến trong các mô hình OpenMoE của chúng tôi, dẫn đến ba phát hiện quan trọng: Chuyên môn hóa Độc lập Ngữ cảnh, Học Định tuyến Sớm, và Rơi-về-cuối. Chúng tôi phát hiện rằng các quyết định định tuyến trong mô hình MoE chủ yếu dựa trên ID token, với mức độ liên quan ngữ cảnh tối thiểu. Việc gán token-to-expert được xác định sớm trong giai đoạn tiền huấn luyện và vẫn phần lớn không thay đổi. Định tuyến không hoàn hảo này có thể dẫn đến suy giảm hiệu suất, đặc biệt trong các tác vụ tuần tự như cuộc trò chuyện nhiều lượt, nơi các token xuất hiện muộn hơn trong chuỗi có nhiều khả năng bị loại bỏ. Cuối cùng, chúng tôi suy ngẫm lại thiết kế của mình dựa trên các quan sát và phân tích nói trên. Để tạo điều kiện cho việc phát triển MoE LLM trong tương lai, chúng tôi đề xuất các chiến lược tiềm năng để giảm thiểu các vấn đề chúng tôi tìm thấy và cải thiện hơn nữa thiết kế MoE LLM có sẵn.³

†Email: f.xue@u.nus.edu
³https://github.com/XueFuzhao/OpenMoE
Preprint.arXiv:2402.01739v2 [cs.CL] 27 Mar 2024

--- TRANG 2 ---
1 Giới thiệu
Mô hình Ngôn ngữ Lớn (LLM) đã thể hiện hiệu suất đáng chú ý trên các tác vụ NLP khác nhau [29, 37], và thậm chí đã trở thành một phần của cuộc sống hàng ngày của chúng ta thông qua các ứng dụng chatbot như ChatGPT, Bard, và Copilot. Tuy nhiên, LLM tốn kém về mặt tính toán trong cả huấn luyện và suy luận. Khi LLM ngày càng trở nên phổ biến, việc nâng cao hiệu suất của chúng mà không tăng tỷ lệ thuận tài nguyên tính toán là một thách thức quan trọng. Để đối phó với thách thức này, Fedus et al. [15] và Riquelme et al. [38] đã đề xuất Mixture-of-Experts (MoE) để mở rộng các tham số có thể huấn luyện của transformer với ít chi phí tính toán bổ sung. Những tiến bộ gần đây trong các mô hình ngôn ngữ dựa trên MoE, như GLaM [14] và ST-MoE [61] đã chứng minh hiệu suất vượt trội trong các tác vụ khác nhau. Tuy nhiên, trước khi phát hành OpenMoE, có ít mô hình ngôn ngữ MoE mã nguồn mở được huấn luyện với các bộ dữ liệu đa dạng cấp nghìn tỷ.

Trong công trình này, chúng tôi đặt ra ba mục tiêu chính: (1) Cung cấp giải pháp nỗ lực đầu tiên chi tiết để huấn luyện mô hình MoE chỉ giải mã trong khuôn khổ hiện có của việc huấn luyện LLM. (2) Thực hiện phân tích sâu về các cơ chế định tuyến MoE, từ đó cung cấp cho cộng đồng nghiên cứu những hiểu biết sâu sắc hơn về hành vi và những hạn chế tiềm năng của các LLM dựa trên MoE. (3) Mở đường cho việc phát triển MoE LLM trong tương lai. Thông qua nỗ lực sơ khai này, chúng tôi nhằm mục đích kích thích và tăng tốc sự phát triển của cộng đồng MoE mã nguồn mở.

Phát hành OpenMoE. Đầu tiên, chúng tôi phát hành OpenMoE, một loạt các LLM dựa trên MoE mã nguồn mở, bao gồm: (1) OpenMoE-Base/16E: một mô hình nhỏ với 0.65B tham số cho mục đích gỡ lỗi. 16E có nghĩa là 16 expert cho mỗi lớp MoE; (2) OpenMoE-8B/32E: biến thể này có 8B tham số tổng cộng, kích hoạt khoảng 2B tham số mỗi token trong các khối Transformer, và được tiền huấn luyện trên hơn 1 nghìn tỷ token; (3) OpenMoE-8B/32E-Chat, một phiên bản chat của OpenMoE-8B/32E, được tinh chỉnh với tập con 100K của bộ dữ liệu WildChat [2]; (4) OpenMoE-34B/32E: một mô hình quy mô lớn hơn, kích hoạt 6B tham số mỗi token trong các khối Transformer và được huấn luyện với 200B token, phục vụ như một minh chứng cho khả năng mở rộng của phương pháp của chúng tôi. Cấu hình chi tiết có thể được tìm thấy trong Phụ lục B.

Các mô hình OpenMoE-8B/32E của chúng tôi đạt được hiệu suất tương đương với OpenLLaMA-3B [19] và TinyLLaMA-1.1B [56], hai LLM dày đặc mở sử dụng chi phí huấn luyện cao hơn. Đáng chú ý, trên MT-Bench [58], OpenMoE-8B/32E-Chat vượt trội hơn hai LLM dày đặc một cách đáng kể trong cuộc trò chuyện một lượt. Ngoài ra, chúng tôi phát hành 5 checkpoint trung gian của OpenMoE-8B/32E, mỗi checkpoint được huấn luyện với 200B token nhiều hơn so với checkpoint trước đó, để hỗ trợ và khuyến khích nghiên cứu trong tương lai. Phần 2 và 3 sẽ thảo luận về thiết kế, chi tiết huấn luyện, và kết quả đánh giá của OpenMoE.

Khám phá Các Chiến lược Huấn luyện Tiên tiến. Như một phần của nỗ lực nghiên cứu, chúng tôi cam kết khám phá các Kỹ thuật tiên tiến hơn trong huấn luyện LLM: (1) Khác với thực hành thông thường của việc huấn luyện mô hình trên dữ liệu nội bộ hoặc dữ liệu mã nguồn mở chủ yếu văn bản, chúng tôi huấn luyện OpenMoE với tỷ lệ đáng kể của mã, chiếm đến 52.25% trong giai đoạn đầu của tiền huấn luyện; (2) Vượt ra ngoài mục tiêu huấn luyện dự đoán token tiếp theo thông thường, chúng tôi điều tra mục tiêu huấn luyện UL2 [47], được thúc đẩy bởi hiệu quả đã được chứng minh trong công việc trước đó [1] và sự phù hợp tốt với dữ liệu mã hóa [5]. Chúng tôi thừa nhận rằng hiệu suất của mô hình, mặc dù chấp nhận được, không vượt quá đáng kể kỳ vọng của chúng tôi, có thể do một số lựa chọn thiết kế không tối ưu. Tuy nhiên, chúng tôi tin rằng công việc khám phá này mang lại giá trị đáng kể cho cộng đồng mã nguồn mở, đặc biệt trong việc đánh giá tiềm năng và hiệu quả của những kỹ thuật ít được khám phá này.

Nghiên cứu Định tuyến MoE Sâu sắc. Mặc dù MoE hiệu quả, vẫn thiếu nghiên cứu về lý do tại sao MoE hoạt động tốt. Từ cấp độ cao, MoE giới thiệu nhiều tham số có thể huấn luyện hơn so với đối tác dày đặc của nó. Để giữ FLOPs cố định khi mở rộng số lượng tham số, MoE áp dụng một lớp định tuyến gán mỗi token một cách thưa thớt và thích ứng cho một vài expert. Quá trình lựa chọn expert thưa thớt này rất quan trọng đối với chức năng của MoE. Thật không may, mặc dù có các tài liệu hiện tại trực quan hóa ngắn gọn quyết định định tuyến [28, 33, 38, 42, 61], chúng ta vẫn không có hiểu biết rõ ràng về cách router hoạt động và cách quyết định định tuyến ảnh hưởng đến kết quả trong các mô hình MoE, đặc biệt đối với các LLM hậu ChatGPT được huấn luyện trên hỗn hợp các bộ dữ liệu từ các lĩnh vực đa dạng. Trong công trình này, chúng tôi nghiên cứu vấn đề này dựa trên các phân loại khác nhau, bao gồm lĩnh vực, ngôn ngữ, tác vụ, và token. Các phát hiện chính của chúng tôi như sau: (1) Chuyên môn hóa Độc lập Ngữ cảnh: MoE có xu hướng đơn giản nhóm các token dựa trên ngữ nghĩa cấp token tương tự, ngụ ý rằng, bất kể ngữ cảnh, một token nhất định có nhiều khả năng được định tuyến đến một expert nhất định; (2) Học Định tuyến Sớm: Chuyên môn hóa định tuyến ID Token được thiết lập sớm trong tiền huấn luyện và vẫn phần lớn cố định, dẫn đến các token được xử lý nhất quán bởi cùng các expert trong suốt quá trình huấn luyện; (3) Rơi-về-cuối:

--- TRANG 3 ---
Vì mỗi expert có dung lượng tối đa cố định, các token xuất hiện muộn hơn trong chuỗi phải đối mặt với rủi ro cao hơn bị loại bỏ nếu expert đã ở dung lượng tối đa. Vấn đề này nghiêm trọng hơn trong các bộ dữ liệu instruction-tuning. Những bộ dữ liệu này thường thể hiện khoảng cách lĩnh vực so với dữ liệu tiền huấn luyện, có nghĩa là các chiến lược gán token cân bằng được thiết lập và củng cố trong tiền huấn luyện sớm có thể không hiệu quả tương tự trong các tình huống instruction-tuning. Điều này đáng lo ngại vì dữ liệu instruction đóng vai trò quan trọng trong việc triển khai LLM cho các ứng dụng thực tế. Phần 4 thảo luận chi tiết về các hiện tượng trên.

Suy ngẫm lại Những Sai lầm của Chúng tôi và Đề xuất Các Giải pháp Tiềm năng. Nhìn lại, dự án của chúng tôi gặp phải một số sai lầm và đưa ra những quyết định không tối ưu (ví dụ, hỗn hợp dữ liệu tích cực), như được chi tiết trong Phần 5. Như một nỗ lực mã nguồn mở sớm, chúng tôi tin rằng việc chia sẻ những kinh nghiệm và hiểu biết này rất quan trọng, có lẽ còn quan trọng hơn việc chỉ tập trung vào các chiến lược thành công. Dựa trên các phát hiện thực nghiệm của chúng tôi trong quá trình huấn luyện và phân tích trực quan hóa tiếp theo (Phần 4), chúng tôi đã phát triển một bộ các giải pháp tiềm năng. Chúng tôi chân thành hy vọng những hiểu biết này có thể giúp cộng đồng phát triển các mô hình tốt hơn trong tương lai.

Cấu trúc của bài báo này phản ánh vòng đời của dự án OpenMoE, bao gồm tất cả các giai đoạn của nó. Điều này bao gồm thiết kế ban đầu (Phần 2), huấn luyện và đánh giá (Phần 3), phân tích sâu (Phần 4), và suy ngẫm lại dự án OpenMoE (Phần 5).

2 Thiết kế OpenMoE
Đầu tiên, chúng tôi giới thiệu thiết kế được khởi tạo của các mô hình OpenMoE liên quan đến dữ liệu tiền huấn luyện, kiến trúc mô hình, mục tiêu huấn luyện, và dữ liệu tinh chỉnh có giám sát.

2.1 Bộ dữ liệu Tiền huấn luyện: Nhiều Mã hơn Bình thường
Các LLM hiện đại thường được huấn luyện bằng sự kết hợp các bộ dữ liệu từ các lĩnh vực khác nhau, tức là hỗn hợp dữ liệu [7, 9, 21, 36, 49]. Ngoại trừ các LLM được tùy chỉnh hướng đến lập trình (ví dụ, StarCoder [30], CodeLLaMA [40]), dữ liệu tiền huấn luyện của hầu hết các mô hình hiện tại được chi phối bởi dữ liệu văn bản. Ví dụ, tỷ lệ lấy mẫu của bộ dữ liệu GitHub chỉ là 4.5% cho LLaMA [49]. Tuy nhiên, chúng tôi cho rằng dữ liệu mã rất quan trọng vì hai lý do. Đầu tiên, dữ liệu mã có khả năng cải thiện khả năng suy luận phức tạp với chuỗi suy nghĩ [16]. Quan trọng hơn, khác với ngôn ngữ tự nhiên, đôi khi mơ hồ và dễ hiểu sai, mã luôn chính xác. Điều này cho phép mã trở thành ngôn ngữ hiệu quả hơn để máy móc truyền đạt thông tin một cách ngắn gọn mà không hiểu sai giữa các tác nhân AI (hiện thực hóa) khác nhau, và kết quả là, mã có tiềm năng lớn để chi phối giao tiếp LLM trong các ứng dụng thực tế. Do đó, chúng tôi thiết kế một hỗn hợp dữ liệu tiền huấn luyện chủ yếu mã hóa hơn. Như được hiển thị trong Bảng 1, chúng tôi trích xuất 50% dữ liệu từ RedPajama [11] và 50% dữ liệu từ phiên bản sao chép của The Stack [25]. Kết quả thực nghiệm của chúng tôi cho thấy rằng hỗn hợp dữ liệu phiên bản I có thể hơi tích cực trong tỷ lệ mã của nó. Chúng tôi khắc phục những vấn đề này ở giai đoạn sau của tiền huấn luyện, vui lòng xem Phần 3.2 sau để biết chi tiết.

Bảng 1: Ba phiên bản hỗn hợp dữ liệu tiền huấn luyện OpenMoE.
Phiên bản I Phiên bản II Phiên bản III
Mô hình OpenMoE-Base, OpenMoE-8B/32E OpenMoE-34B/32E
Thời kỳ trước 780B token →sau 780B token từ đầu đến cuối
Bộ dữ liệu Tỷ lệ Lấy mẫu
RedPajama 50.0% 83.5% 67.5%
C4 7.50% 15.0% 15.0%
Wikipedia 2.25% 6.50% 4.50%
Stackexchange 1.00% 2.50% 1.00%
ArXiv 1.25% 4.50% 4.50%
Books 2.25% 6.50% 4.50%
GitHub 2.25% 5.00% 5.00%
Commoncrawl 33.5% 43.5% 33.0%
Wikipedia-en 0.00% 6.50% 2.50%
The Stack Dedup 50.0% 10.0% 30.0%

--- TRANG 4 ---
2.2 Kiến trúc Mô hình: ST-MoE chỉ Giải mã
Tokenizer. Chúng tôi áp dụng tokenizer umT5 [10] với kích thước từ vựng 256K vì hai lý do: (1) tokenizer umT5 với từ vựng đa ngôn ngữ lớn hỗ trợ ngôn ngữ ít tài nguyên tốt hơn so với các tokenizer sử dụng từ vựng nhỏ (ví dụ, tokenizer LLaMA với từ vựng 32K); (2) so với một số tokenizer cũ, như BERT [24] và tokenizer T5 [37], tokenizer umT5 có tính năng byte fallback để hỗ trợ token ngoài từ vựng tốt hơn.

Định tuyến Token-choice. Chúng tôi thường tuân theo ST-MoE [61] cho kiến trúc mô hình và thiết kế định tuyến để đảm bảo tính ổn định huấn luyện, điều này cực kỳ quan trọng khi huấn luyện các mô hình lớn hơn. Cho E expert có thể huấn luyện và biểu diễn đầu vào x∈RD, đầu ra của mô hình MoE có thể được công thức hóa là:

MoE(x) = ∑(i=1 to E) g(x)_i e_i(x), (1)

trong đó e_i(·) là một phép biến đổi phi tuyến RD→RD của expert thứ i, và g(·)_i là phần tử thứ i của đầu ra của router có thể huấn luyện g(·), một ánh xạ phi tuyến RD→RE. Thông thường, cả e(·) và g(·) đều được tham số hóa bởi mạng nơ-ron. Vui lòng lưu ý rằng mỗi expert là một lớp FFN thay vì một mô hình Transformer hoàn chỉnh trong hầu hết các mô hình Transformer dựa trên MoE, bao gồm cả mô hình của chúng tôi.

Lựa chọn Top-2. Theo công thức trên, khi g(·) là một vector thưa thớt, chỉ một phần các expert sẽ được kích hoạt và cập nhật bằng lan truyền ngược trong quá trình huấn luyện. Chúng tôi đặt lớp gating như một lựa chọn top-K như:

g(x) = TopK(softmax(f(x))), (2)

trong đó f(·) là phép biến đổi tuyến tính định tuyến RD→RE. Khi K≪E, hầu hết các phần tử của g(x) sẽ bằng không để đạt được tính toán có điều kiện thưa thớt. Chúng tôi đặt K=2 theo Zoph et al. [61].

MoE Dư. Mỗi khối Transformer vanilla có thể được viết như:
x′ = LayerNorm^att_i(x),
x = MHA(x′) + x,
x′′ = LayerNorm^ffn_i(x),
x = FFN(x′′) + x, (3)

Trong OpenMoE, đối với mỗi khối Transformer dựa trên MoE, chúng tôi sử dụng một lớp MoE dư để đảm bảo rằng một lớp FFN cố định luôn được kích hoạt cho mọi token. Đó là:
x′ = LayerNorm^att_i(x),
x = MHA(x′) + x,
x′′ = LayerNorm^ffn_i(x),
x = MoE(x′′) + FFN(x′′) + x, (4)

Lưu ý chúng tôi sử dụng các khối Transformer dựa trên MoE theo cách xen kẽ thay vì đặt MoE trong mọi khối Transformer. Trong cài đặt của chúng tôi, chúng tôi sử dụng MoE mỗi 4 lớp trong OpenMoE-Base/16E và OpenMoE 34B/32E và sử dụng MoE mỗi 6 lớp cho OpenMoE-8B/32E. Cài đặt này được lấy cảm hứng từ các phát hiện trong ViT-MoE [38], tức là sử dụng MoE mọi lớp giới thiệu thêm chi phí tính toán trong quá trình định tuyến, và sau đó gây ra sự đánh đổi hiệu quả chi phí tệ hơn so với việc sử dụng MoE xen kẽ.

Load Balance Loss và Router Z-loss. ST-MoE [61] tuân theo Shazeer et al. [42], sử dụng MoE load balance loss để đảm bảo số lượng token cân bằng được gán cho các expert khác nhau để các mô hình MoE có thể đạt được sự song song tốt hơn. Đối với mỗi hoạt động định tuyến, cho E expert và N batch với B=NL token, loss phụ trợ sau được thêm vào tổng loss mô hình trong quá trình huấn luyện:

L_b = E · ∑(i=1 to E) m_i · P_i, (5)

trong đó m là một vector, P_i là softmax(f(x))_i biểu thị ID expert. Phần tử thứ i là phần của token được gửi đến expert i:

m_i = (1/B) ∑(j=1 to B) h(x_j)_i, (6)

--- TRANG 5 ---
Bảng 2: Cấu hình mixture-of-denoisers của UL2, μ là độ dài span trung bình và r là tỷ lệ mặt nạ.
Mục tiêu Huấn luyện Tỷ lệ phần trăm
PrefixLM, r=0.5 50%
SpanCorrupt
μ=3, r=0.15 10%
μ=8, r=0.15 10%
μ=3, r=0.5 10%
μ=8, r=0.5 10%
μ=64, r=0.5 10%

trong đó h(·) là một vector chỉ số được chọn bởi TopK trong Eq. 2. h(x_j)_i là phần tử thứ i của h(x_j). Có thể nhận thấy rằng, khác với g(x)_i trong Eq. 2, m_i và h(x_j)_i là không thể vi phân. Tuy nhiên, một hàm loss có thể vi phân là cần thiết để tối ưu hóa MoE theo cách end-to-end, vì vậy chúng tôi sử dụng điểm định tuyến softmax(f(x)) trong Eq. 2 (tức là P_i trong Eq. 5) để làm cho quyết định định tuyến có thể vi phân và sau đó có thể học được.

Ngoài load balance loss, Zoph et al. [61] đề xuất router z-loss để huấn luyện MoE ổn định hơn:

L_z(x) = (1/B) ∑(i=1 to B) [log ∑(j=1 to E) e^(x^(i)_j)]^2 (7)

Router z-loss này có thể phạt các logit lớn đầu vào vào mạng gating và khuyến khích độ lớn tuyệt đối của các số nhỏ để có thể giảm lỗi làm tròn trong các lớp MoE. Vui lòng tham khảo bài báo ST-MoE [61] để có giải thích chi tiết.

Tổng hợp, loss huấn luyện cuối cùng của chúng tôi có thể được viết là:
L = L_CE + L_b + L_z (8)
trong đó L_CE là cross-entropy loss trong tiền huấn luyện mô hình ngôn ngữ.

2.3 Mục tiêu Huấn luyện: UL2 và CasualLM
Thay vì áp dụng mô hình ngôn ngữ casual (CasualLM) vanilla trực tiếp, chúng tôi khám phá UL2 [46], một mục tiêu tiền huấn luyện mô hình ngôn ngữ đa dạng hơn kết hợp span corruption (SpanCorrupt) và prefix language modeling (PrefixLM) [37]. Đáng chú ý là SpanCorrupt trong UL2 đa dạng hơn SpanCorrupt vanilla vì nó trộn các độ dài span và tỷ lệ corruption khác nhau. Chúng tôi có hai lý do để khám phá UL2 trong OpenMoE. Đầu tiên, UL2 đã cho thấy kết quả hứa hẹn trong PaLM-2 [1]. Quan trọng hơn, việc che token tích cực rất giống với tác vụ hoàn thành mã trong thế giới thực, như Copilot. Bavarian et al. [5] cũng phát hiện rằng mục tiêu filling-in-the-middle (FiM) tương tự có thể mô hình hóa mã tốt hơn so với mục tiêu huấn luyện vanilla. Vì chúng tôi sử dụng nhiều mã hơn trong hỗn hợp dữ liệu tiền huấn luyện, việc thích ứng UL2 bao gồm FiM là lựa chọn hợp lý hơn theo trực giác. Cấu hình mục tiêu huấn luyện UL2 chi tiết của chúng tôi được hiển thị trong Bảng 2. Chúng tôi chỉ sử dụng 20% tỷ lệ mặt nạ thấp (r=0.15) vì có ít token đầu ra hơn trong quá trình huấn luyện, điều này có thể làm chậm việc học. Chúng tôi cũng sử dụng nhiều PrefixLM hơn cài đặt UL2 mặc định vì chúng tôi nghĩ khả năng zero-shot và in-context learning được tăng cường bởi huấn luyện PrefixLM là quan trọng. Chúng tôi gặp một số khó khăn khi huấn luyện với UL2 trong OpenMoE, điều này sẽ được thảo luận trong Phần 3.2.

2.4 Tinh chỉnh Có giám sát
Mặc dù alignment không phải là trọng tâm của dự án OpenMoE này, chúng tôi vẫn tiến hành tinh chỉnh có giám sát (SFT) với một tập con của bộ dữ liệu WildChat mã nguồn mở [2] để tăng cường khả năng tuân theo hướng dẫn và nghiên cứu hành vi của mô hình MoE trước và sau SFT. Chúng tôi chỉ chọn các cặp instruction-response từ GPT-4 trong WildChat vì thiếu tài nguyên tính toán ở giai đoạn cuối của việc phát triển OpenMoE. Tập con bao gồm 58K cuộc trò chuyện và mỗi cuộc trò chuyện bao gồm trung bình 1.8 lượt.

--- TRANG 6 ---
Bảng 3: Nghiên cứu ablation với OpenMoE-Base/16E trên zero-shot TriviaQA [23].
Phương pháp EM F1
OpenMoE 1.4 4.5
w/o MoE 0.1 0.3
w/o UL2 (chỉ PrefixLM) 0.0 0.0
w/o dữ liệu Code 0.7 1.1
w/ tokenizer LLaMA 2.2 5.7

[Hình 1: So sánh validation loss và accuracy trên các bộ dữ liệu tiền huấn luyện khác nhau. Chúng ta có thể quan sát thấy các mô hình dễ đạt độ chính xác cao hơn và loss thấp hơn trên dữ liệu mã.]

2.5 Các Thiết kế Khác
Theo các LLM gần đây, chúng tôi áp dụng RoPE [45] cho position embedding và SwiGLU [41] cho hàm kích hoạt cho FFN trong cả các khối Transformer dày đặc và MoE. Cấu hình mô hình chi tiết hơn và các siêu tham số huấn luyện cho các mô hình OpenMoE có thể được tìm thấy trong Phụ lục B. Chúng tôi áp dụng data parallelism, tensor parallelism [43, 52], và expert parallelism [27] để huấn luyện các mô hình ở quy mô lớn. Chúng tôi huấn luyện các mô hình OpenMoE trên Google Cloud TPU với 64 đến 512 chip v3 tùy thuộc vào tính khả dụng.

3 Huấn luyện OpenMoE
3.1 Nghiên cứu Ablation
Như một đánh giá ban đầu về các quyết định thiết kế của chúng tôi, chúng tôi đã tiến hành nghiên cứu ablation sử dụng mô hình OpenMoE-Base/16E. Quan trọng là phải lưu ý rằng mặc dù những kết quả này cung cấp những hiểu biết sớm, chúng tôi không thể chắc chắn về khả năng tổng quát hóa của chúng cho các mô hình lớn hơn, chủ yếu do các ràng buộc tài nguyên tính toán ngăn cản ablation quy mô lớn hơn.

Các phát hiện của chúng tôi chỉ ra rằng một số yếu tố — phương pháp MoE, mục tiêu huấn luyện UL2, và việc tăng cường nhấn mạnh dữ liệu mã — tất cả đều đóng góp tích cực vào hiệu suất của phiên bản cơ sở trong các tác vụ zero-shot TriviaQA. Mô hình sử dụng tokenizer LLaMA [49] vượt trội hơn mô hình với tokenizer umT5. Kết quả này được coi là chấp nhận được, mặc dù kích thước từ vựng lớn hơn có thể làm suy giảm hiệu suất một chút. Chúng tôi tin rằng việc hỗ trợ các ngôn ngữ ít tài nguyên là quan trọng, vì các mô hình nền tảng nên có thể tiếp cận và có lợi cho đối tượng toàn cầu đa dạng. Sau kiểm tra tính hợp lệ này, chúng tôi tiến hành mở rộng OpenMoE lên OpenMoE-8B/32E.

Chúng tôi cũng tiến hành nghiên cứu ablation để so sánh tiến độ học dữ liệu từ các lĩnh vực khác nhau. Như được hiển thị trong Hình 1, chúng ta có thể quan sát thấy các mô hình dễ đạt độ chính xác cao hơn và loss thấp hơn trên dữ liệu mã. Trên Github, mặc dù mô hình của chúng tôi nhỏ, nó vẫn có thể đạt được hơn 80% độ chính xác dự đoán token. Chúng tôi suy luận rằng điều này là do phân phối token long-tail trong dữ liệu mã. Ví dụ, một số lượng lớn token trong mã là "\n" và "\t", tương đối dễ dự đoán hơn.

--- TRANG 7 ---
[Hình 2: Độ chính xác dự đoán token của các mô hình OpenMoE. OpenMoE-8B/32E sử dụng UL2 trước bước 390K và quay lại CasualLM sau bước 790K. OpenMoE-34B/32E sử dụng UL2 cho đến 50B token.]

3.2 Tiến độ Huấn luyện
Bão hòa UL2 Trong quá trình huấn luyện, chúng tôi phát hiện rằng, mặc dù UL2 có thể giúp mô hình học nhanh hơn ở giai đoạn đầu của huấn luyện, nó dễ bão hòa ở giai đoạn huấn luyện sau của OpenMoE-8B/32E. Như được hiển thị trong Hình 2, nếu chúng ta phóng to, chúng ta có thể thấy rằng OpenMoE-8B/32E cải thiện rất chậm từ bước 35K đến 39K. Chúng tôi đề xuất rằng điều này có thể là do, mặc dù UL2 đa dạng hơn, SpanCorrupt vẫn tương đối dễ so với CasualLM. Do đó, chúng tôi quay lại CasualLM sau bước 390K (780B) token. Ngoài ra, vì dữ liệu mã phù hợp tốt hơn với UL2 và hỗn hợp dữ liệu mã ban đầu của chúng tôi tương đối tích cực, chúng tôi cũng giảm tỷ lệ lấy mẫu dữ liệu mã xuống 15%. Hỗn hợp dữ liệu phiên bản thứ hai được báo cáo trong Bảng 1.

Rõ ràng, trong Hình 2, sau 780B token, có một sự giảm đáng kể trong độ chính xác dự đoán token sau bước 390K cho OpenMoE-8B/32E. Điều này được gây ra bởi mục tiêu CasualLM khó hơn và ít dữ liệu mã dễ hơn. Lưu ý rằng, mặc dù chúng tôi gặp vấn đề bão hòa ở giai đoạn sau của huấn luyện OpenMoE-8B/32E, chúng tôi nghĩ rằng chương trình giảng dạy từ dễ đến khó như vậy có thể hữu ích cho huấn luyện LLM. Do đó, chúng tôi vẫn thích ứng UL2 cho 25K bước (50B token) trong OpenMoE-34B/32E. Chúng tôi sử dụng hỗn hợp dữ liệu nặng mã tương đối vừa phải trong OpenMoE-34B/32E. Như được hiển thị trong Bảng 1, chúng tôi sử dụng tổng cộng 35% dữ liệu mã. Do hạn chế tài nguyên tính toán, chúng tôi huấn luyện OpenMoE-34B/32E chỉ với 200B token để xác minh khả năng mở rộng của nó. Chúng tôi để việc huấn luyện OpenMoE quy mô lớn với nhiều token hơn như công việc tương lai nếu có thể.

3.3 Đánh giá trên Benchmark
3.3.1 Đánh giá Mô hình Thô
Trước hết, chúng tôi nhấn mạnh rằng chúng tôi hoàn toàn không hack các benchmark và tiền huấn luyện hoàn toàn trên các bộ dữ liệu mã nguồn mở được đề cập ở trên. Vì mô hình của chúng tôi tương đối nhỏ về mặt ngân sách huấn luyện, chúng tôi chủ yếu đánh giá mô hình thô trên các benchmark đã được thiết lập nhưng không quá khó, tức là TriviaQA [23], HumanEval [8], WMT16-En-Ro [6], BigBench-Lite (24 tác vụ) [4], và một tập con của bộ sưu tập lm-evaluation-harness [18] với 13 tác vụ. Đối với các benchmark phổ biến nhưng tương đối thách thức như 5-shot MMLU [20], OpenMoE-8B/32E của chúng tôi đạt được khoảng 26.2% độ chính xác, có nghĩa là mô hình gần như đoán ngẫu nhiên từ bốn lựa chọn. Chúng tôi chủ yếu so sánh với các mô hình mã nguồn mở với chi phí huấn luyện cao hơn, tức là TinyLLaMA-1.1B [56] và OpenLLaMA-3B [19]. Trên BigBench-Lite, chúng tôi cũng so sánh với GPT-3 [7], Big-G [4] và Big-G-Sparse [4]. Big-G và Big-G-Sparse là hai bộ mô hình Transformer nội bộ của Google được đánh giá trên BigBench-Lite, và các mô hình Big-G-Sparse là Transformer dựa trên MoE.

Chúng tôi đầu tiên báo cáo kết quả của chúng tôi trên Commonsense QA (TriviaQA), Coding (HumanEval), và Low-resource Machine Translation (WMT16 En-Ro). Chúng tôi nghĩ rằng ba benchmark này có ý nghĩa đối với chúng tôi vì (1) Commonsense là để kiểm tra xem OpenMoE có thể ghi nhớ thêm commonsense cho lợi thế mở rộng tham số hiệu quả của nó không; (2) Coding quan trọng vì việc sử dụng phổ biến trong việc giải quyết các lời nhắc liên quan đến mã hóa, LLM như tác nhân, và AI hiện thực hóa; (3) Low-resource Machine Translation quan trọng vì chúng tôi muốn chia sẻ lợi ích của các mô hình nền tảng cho mọi người trên

--- TRANG 8 ---
Bảng 4: Kết quả trên TriviaQA (Exact Match). Chúng tôi cũng báo cáo số lượng token huấn luyện từ Wikipedia vì các câu hỏi commonsense trong TriviaQA có mối liên quan tương đối gần với dữ liệu Wikipedia.
Mô hình Act. Params Total Tokens Text Tokens Wiki Tokens TriviaQA
TinyLLaMA-1.1B 0.9B 3.0T 2.1T 75B 11.2
OpenLLaMA-3B 2.9B 1.0T 991B 24B 29.7
OpenMoE-8B/32E 2.1B 1.1T 644B 58B 32.7
OpenMoE-34B/32E 6.4B 0.2T 130B 14B 31.3

Bảng 5: Kết quả trên HumanEval (Pass@1). Chúng tôi cũng báo cáo số lượng token huấn luyện từ lĩnh vực mã (dữ liệu The Stack và GitHub).
Mô hình Act. Params Total Tokens Code Tokens HumanEval
TinyLLaMA-1.1B 0.9B 3.0T 900B 9.1
OpenLLaMA-3B 2.9B 1.0T 59B 0
OpenMoE-8B/32E 2.1B 1.1T 456B 9.8
OpenMoE-34B/32E 6.4B 0.2T 70B 10.3

trái đất. Như được hiển thị trong Bảng 4, OpenMoE-8B/32E vượt trội hơn các baseline rõ ràng với chi phí huấn luyện ít hơn (Tham số Kích hoạt × Tổng Token Huấn luyện). Ngoài ra, xin lưu ý rằng TinyLLaMA-1.1B hoạt động kém đáng kể so với các mô hình khác trên TriviaQA mặc dù có chi phí huấn luyện tương đương với OpenLLaMA-3B. Do đó, điều này làm nổi bật tầm quan trọng của số lượng tham số để giữ kiến thức trong LLM, điều này cũng chỉ ra ý nghĩa của việc sử dụng MoE.

Trong Bảng 5, các mô hình OpenMoE đạt được hiệu suất tốt hơn so với baseline. OpenMoE-34B/32E chỉ sử dụng 70B dữ liệu mã, trong khi nó vẫn hoạt động tương đối tốt trên HumanEval, điều này cho thấy khả năng mở rộng của OpenMoE, mặc dù chúng tôi không có đủ tài nguyên tính toán để huấn luyện nó cho đến cuối. OpenLLaMA-3B gặp khó khăn trên HumanEval vì các khoảng trắng liên tiếp được coi là một, trái với cú pháp Python[34].

Bảng 6 cho thấy kết quả của chúng tôi trên tác vụ dịch WMT16 En-Ro. Lưu ý rằng mô hình của chúng tôi không bao gồm nhiều dữ liệu đa ngôn ngữ một cách có chủ ý. Hầu hết dữ liệu đa ngôn ngữ nên từ phiên bản đa ngôn ngữ của Wikipedia trong RedPajama, cũng được sử dụng trong TinyLLaMA-1.1B và OpenLLaMA-3B. Tuy nhiên, các mô hình OpenMoE vẫn cho thấy kết quả tốt hơn so với baseline, điều này có thể làm nổi bật tầm quan trọng của tokenizer umT5.

[Hình 3: Kết quả trên BigBench-Lite. Chi phí tương đối được tính dựa trên việc nhân các tham số kích hoạt trong Transformer và số lượng token huấn luyện. Kích thước các chấm màu biểu thị số lượng tham số kích hoạt, và kích thước bóng biểu thị số lượng tổng tham số cho các mô hình MoE.]

--- TRANG 9 ---
Bảng 6: Kết quả trên WMT16 En-Ro (điểm BLEU). Chúng tôi cũng báo cáo số lượng token đa ngôn ngữ rõ ràng trong bộ dữ liệu tiền huấn luyện, tức là phiên bản đa ngôn ngữ của Wikipedia từ bộ dữ liệu RedPajama.
Mô hình Act. Params Total Tokens Multi-lingual Tokens WMT16 En-Ro
TinyLLaMA-1.1B 0.9B 3.0T 75B 2.6
OpenLLaMA-3B 2.9B 1.0T 24B 1.9
OpenMoE-8B/32E 2.1B 1.1T 38B 3.1
OpenMoE-34B/32E 6.4B 0.2T 9B 3.4

Bảng 7: Đánh giá OpenMoE-8B/32E trên lm-evaluation-harness. Kết quả của OpenLLaMA từ trang chủ của nó, chỉ cung cấp hai chữ số hiệu quả.
Dataset TinyLLaMA-1.1B OpenLLaMA-3B OpenMoE-8B/32E
ANLI-R1 34.2 33.0 32.7
ANLI-R2 32.4 36.0 33.2
ANLI-R3 35.1 38.0 33.9
HellaSwag 59.2 52.0 45.5
WinoGrande 59.1 63.0 60.3
PIQA 73.3 77.0 74.2
ARC-Easy 55.2 68.0 64.1
ARC-Challenge 30.1 34.0 30.3
Boolq 57.8 66.0 61.2
TruthfulQA 37.6 35.0 36.0
OpenbookQA 21.8 26.0 24.6
RTE 51.9 55.0 53.4
WiC 50.1 50.0 49.8
Trung bình 45.9 48.7 46.1

Trong Hình 3, chi phí tương đối được tính dựa trên việc nhân các tham số kích hoạt (Act. Params) trong các khối Transformer và số lượng token huấn luyện. Kích thước các chấm màu biểu thị số lượng tham số kích hoạt, và kích thước bóng biểu thị số lượng tổng tham số cho các mô hình MoE. Chúng ta có thể quan sát thấy OpenMoE đạt được sự đánh đổi hiệu quả chi phí tốt hơn trên BigBench-Lite, về cả chi phí huấn luyện và suy luận.

Chúng tôi cũng đánh giá OpenMoE trên 13 tác vụ từ bộ sưu tập LM-Evaluation-Harness. Như được hiển thị trong Bảng 7, cả OpenMoE và TinyLLaMA đều hoạt động kém hơn OpenLLaMA. Tuy nhiên, điểm số đạt được bởi OpenMOE là chấp nhận được. Chúng tôi đề xuất rằng tỷ lệ lấy mẫu cao ban đầu trên dữ liệu mã có thể gây hại cho kết quả trên các benchmark chủ yếu văn bản này, đây là một trong những vấn đề chúng tôi sẽ thảo luận trong Phần 5.

3.3.2 Đánh giá Mô hình Chat
Chúng tôi tiếp tục đánh giá mô hình của chúng tôi trên MTBench, một benchmark ChatBot được thiết lập có thể kiểm tra các mô hình một cách toàn diện. Chúng tôi báo cáo cả kết quả một lượt và nhiều lượt trong Hình 4a và Bảng 8. Chúng ta có thể quan sát thấy OpenMoE vượt trội hơn baseline với biên độ lớn trong kết quả một lượt, đặc biệt trên các tác vụ mã hóa. Tuy nhiên, hiệu suất của OpenMoE giảm nhiều hơn ở lượt thứ hai, dẫn đến kết quả nhiều lượt tệ hơn trong Hình 4b. Chúng tôi phát hiện rằng điều này có thể do token drop của chuỗi dài. Vui lòng xem Phần 4 sau để có phân tích chi tiết.

4 Phân tích OpenMoE
Chúng tôi thường nghĩ MoE là một cách hiệu quả để mở rộng tham số với ngân sách tính toán cố định. Tuy nhiên, chúng ta ít hiểu biết về những gì các expert trong MoE chuyên môn hóa. Trong phần này, chúng tôi tiến hành phân tích sâu về OpenMoE trong nhiều khía cạnh để nghiên cứu hành vi định tuyến.

--- TRANG 10 ---
[Hình 4: Đánh giá OpenMoE trên MTBench. (a) Kết quả một lượt. (b) Kết quả nhiều lượt.]

Bảng 8: Điểm số trung bình trên MT-Bench.
Mô hình MT-Bench Lượt 1 MT-Bench Lượt 2 MT-Bench Trung bình
GPT-J-6B (0.4T) 2.51 2.35 2.43
TinyLLaMA-1.1B (3T) 4.08 2.54 3.31
OpenLLaMA-3B (1T) 4.36 3.62 3.99
OpenMoE-8B/32E (1.1T) 4.69 3.26 3.98

4.1 Các Expert Chuyên môn hóa về gì?
MoE có chuyên môn hóa ở cấp độ lĩnh vực không? Chúng tôi đầu tiên trực quan hóa quyết định định tuyến của các token từ các tập con khác nhau trong bộ dữ liệu RedPajama. Lưu ý rằng tất cả kết quả trực quan hóa đều từ lớp MoE thứ ba theo mặc định vì chúng tôi không quan sát thấy sự khác biệt đáng kể giữa các lớp. Chúng ta có thể quan sát thấy các token từ các tập con khác nhau (tức là lĩnh vực) được phân phối đồng nhất trên biểu đồ. Đó là, mặc dù E21 hơi thích token mã và E10 thích sách một chút, hầu hết expert trong MoE không được chuyên môn hóa dựa trên lĩnh vực.

MoE có chuyên môn hóa ở cấp độ ngôn ngữ không? Chúng tôi tiến hành nghiên cứu dữ liệu hạt mịn hơn để kiểm tra xem MoE có chuyên môn hóa trong các ngôn ngữ lập trình khác nhau và ngôn ngữ tự nhiên không. Trong Hình 6, chúng tôi so sánh 4 ngôn ngữ lập trình khác nhau, tức là Assembly, Blitzmax, Java, và Python. Tương tự như cấp độ lĩnh vực,

[Hình 5: Trực quan hóa quyết định định tuyến trên bộ dữ liệu RedPajama. Ei biểu thị tỷ lệ token được định tuyến đến expert thứ i.]

--- TRANG 11 ---
[Hình 6: Trực quan hóa quyết định định tuyến trên bộ dữ liệu TheStack. Ei biểu thị tỷ lệ token được định tuyến đến expert thứ i.]

[Hình 7: Trực quan hóa quyết định định tuyến trên TED-Parallel-Corpus bao gồm 12 ngôn ngữ, tức là ar (Ả Rập), de (Đức), es (Tây Ban Nha), fr (Pháp), he (Do Thái), it (Ý), ja (Nhật), ko (Hàn), nl (Hà Lan), ru (Nga), zh-cn (Trung Quốc Giản thể), zh-tw (Trung Quốc Phồn thể), Ei biểu thị tỷ lệ token được định tuyến đến expert thứ i.]

thậm chí đối với Assembly và Blitzmax, tức là hai ngôn ngữ ít tài nguyên so với Java và Python, chúng vẫn không thể hiện chuyên môn hóa expert đáng kể.

Chúng tôi tiếp tục nghiên cứu chuyên môn hóa expert trên các ngôn ngữ tự nhiên khác nhau. Chúng tôi áp dụng một corpus song song đa ngôn ngữ, tức là TED-Parallel-Corpus⁴ làm nền tảng. Trong Hình 7, chúng tôi phát hiện rằng có sự chuyên môn hóa tương đối rõ ràng giữa các expert khác nhau. Ví dụ, zh-cn (Trung Quốc Giản thể) và zh-tw (Trung Quốc Phồn thể) đều có sở thích mạnh cho E5 và E16; ja (Nhật), và ko (Hàn) đều thích E14.

MoE có chuyên môn hóa ở cấp độ tác vụ không? Dựa trên các phát hiện trên, dữ liệu hạt mịn hơn có quan sát chuyên môn hóa expert rõ ràng hơn. Sau đó chúng tôi trực quan hóa quyết định định tuyến trên dữ liệu cuộc trò chuyện MT-Bench trong Hình 8. Chúng ta có thể thấy sự chuyên môn hóa tương tự như trên, đặc biệt đối với dữ liệu toán học. Chúng tôi đề xuất rằng lý do chính là các tác vụ toán học bao gồm nhiều token đặc biệt hơn so với các tác vụ khác.

MoE có chuyên môn hóa trong Position ID không? Router trong MoE đưa ra quyết định dựa trên biểu diễn token. Biểu diễn token từ token embedding và position embedding. Do đó chúng tôi

⁴https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus

--- TRANG 12 ---
[Hình 8: Trực quan hóa quyết định định tuyến trên MT-Bench. Chúng tôi áp dụng lịch sử cuộc trò chuyện khi đánh giá OpenMoE MT-Bench làm nguồn dữ liệu trực quan hóa. Ei biểu thị tỷ lệ token được định tuyến đến expert thứ i.]

[Hình 9: Trực quan hóa quyết định định tuyến tại các Position ID khác nhau. Ei biểu thị tỷ lệ token được định tuyến đến expert thứ i. (a) ID token được lấy mẫu đồng nhất. (b) ID token liên tiếp.]

trực quan hóa các quyết định định tuyến trên các vị trí khác nhau trong Hình 9a và Hình 9b. Chúng ta có thể quan sát: (1) thực sự có một số chuyên môn hóa trong các Position ID khác nhau; (2) các vị trí liên tiếp thích các expert tương tự, như E10 và E19 trong Hình 9b.

MoE có chuyên môn hóa trong Token ID không? Vì chúng tôi đang sử dụng tokenizer umT5, các token từ các ngôn ngữ khác nhau thường có Token ID khác nhau. Do đó, chúng tôi tiếp tục nghiên cứu xem router trong MoE có chủ yếu đưa ra quyết định dựa trên Token ID không. Chúng tôi trực quan hóa các quyết định định tuyến của một vài token đại diện trong Hình 10. Tất cả các token này cho thấy sự chuyên môn hóa rất mạnh chỉ trên một vài expert. Đây là một phát hiện rất thú vị vì các token có cùng Token ID có ngữ cảnh rất đa dạng trong các câu khác nhau. Ví dụ, token "ed" có thể là hậu tố của nhiều từ khác nhau, ví dụ "preferred", và "led". Token "an" cũng có thể là một phần của "an apple" hoặc "another". Tuy nhiên, tất cả các token này đều có chuyên môn hóa rất mạnh chỉ trên một vài expert cố định. Điều đó có nghĩa là, MoE đơn giản định tuyến dựa trên Token ID thay vì ngữ nghĩa cấp cao. Chúng tôi đặt tên cho quan sát này là Chuyên môn hóa Độc lập Ngữ cảnh trong các phần sau. Để xác minh rằng Chuyên môn hóa Độc lập Ngữ cảnh cũng tồn tại cho các Token ID khác, chúng tôi vẽ độ lệch chuẩn quyết định định tuyến trong Phụ lục E.

--- TRANG 13 ---
[Hình 10: Trực quan hóa quyết định định tuyến tại các Token ID khác nhau. Ei biểu thị tỷ lệ token được định tuyến đến expert thứ i.]

Bảng 9: Token Hàng đầu được chọn bởi mỗi expert.
Expert ID Token Hàng đầu
0 \n ,' ,' ,s ,- ,$ ,y ,_ , ,2
1 \n ,1 , ,2 ,\\ ,S ,. ,- ,C ,{
21 , ,and , ,. ,\n , = ,\t ,the , ,n
30 } ,ed , d ,have , ing , , ,has , s ," ,had
31 to , can , s ,of , ing , will , not , e ,ed , would

4.2 Nghiên cứu Chuyên môn hóa Token
Các expert có nhóm các token tương tự không? Như chúng tôi đã thảo luận ở trên, các token có cùng Token ID luôn được định tuyến đến cùng một expert bất kể ngữ cảnh là gì, tức là Chuyên môn hóa Độc lập Ngữ cảnh. Do đó chúng tôi điều tra xem các expert có thích các Token ID tương ứng với các token có ngữ nghĩa cấp thấp tương tự không. Chúng tôi liệt kê 10 token yêu thích hàng đầu cho mỗi expert trong Bảng 9. Chúng ta có thể quan sát thấy các token tương tự được nhóm trong các expert. Ví dụ, "can", "will", và "would" đều trong expert 31. "have", "has", và "had" đều được bao gồm trong expert 30. Trực quan hóa này cũng có thể giải thích nhiều quan sát ở trên. Một ví dụ là, trong hầu hết các hình trên, chúng ta có thể thấy hầu hết dữ liệu mã hóa và toán học thích expert 21. Ở đây nó tiết lộ lý do thực sự. Expert 21 có sở thích mạnh cho "=", "and", và "\n", xuất hiện thường xuyên hơn trong toán và mã.

Mô hình học chuyên môn hóa khi nào? Theo Chuyên môn hóa Độc lập Ngữ cảnh được quan sát ở trên, mô hình không học cách định tuyến dựa trên ngữ nghĩa cấp cao. Do đó, chúng tôi đặt ra một câu hỏi khác, mô hình học và cố định quyết định định tuyến cho các token khi nào? Chúng tôi so sánh các quyết định định tuyến của các checkpoint trung gian OpenMoE khác nhau trong Hình 11a và Hình 11b. Chúng ta có thể thấy rằng các sở thích expert gần như hoàn toàn trùng lặp cho các checkpoint khác nhau, có nghĩa là mô hình đã bắt đầu cố định định tuyến của nó ở giai đoạn rất sớm của huấn luyện. Ngay cả khi chúng tôi thay đổi hỗn hợp dữ liệu huấn luyện (từ 52.25% mã thành 20% mã) và mục tiêu huấn luyện (từ UL2 thành CasualLM), quyết định định tuyến vẫn cố định. Chúng tôi suy luận rằng lý do là, khi token thường được gán cho một expert cụ thể, loss sẽ tăng lên rất nhiều nếu token được gửi đến expert khác chưa thấy, điều này đẩy mô hình gán token trở lại expert ban đầu. Do đó, định tuyến có thể đã được học ở giai đoạn warmup hoặc tương tự, và được giữ nguyên trong suốt giai đoạn huấn luyện tiếp theo.

4.3 Token Drop Trong Định tuyến
Rơi-về-cuối Trong các mô hình MoE, chúng ta thường đặt dung lượng tối đa C được định trước cho mọi expert để đảm bảo khối lượng công việc cân bằng, có nghĩa là mỗi expert không thể xử lý nhiều hơn C token.

--- TRANG 14 ---
[Hình 11: Trực quan hóa quyết định định tuyến ID token của các checkpoint trung gian khác nhau. Ei biểu thị tỷ lệ token được định tuyến đến expert thứ i. (a) Quyết định định tuyến token "ed" của các checkpoint trung gian khác nhau. (b) Quyết định định tuyến token "an" của các checkpoint trung gian khác nhau.]

[Hình 12: So sánh tỷ lệ token bị loại bỏ tại các Position ID khác nhau. (a) Các bộ dữ liệu khác nhau. (b) Trước và sau tinh chỉnh có giám sát.]

Điều này có thể đảm bảo thông lượng khi huấn luyện và triển khai mô hình MoE với expert parallelism, tức là phân phối các expert khác nhau cho các GPU khác nhau. Tuy nhiên, điều này cũng sẽ giới thiệu một vấn đề, các token sau sẽ bị loại bỏ nếu các token trước đã lấp đầy expert. Trong kiến trúc MoE chỉ giải mã, do tính chất tự hồi quy, các token sau trong chuỗi có thể bị loại bỏ nhiều hơn. Ví dụ, nếu một expert thích token "\n", và một chuỗi bắt đầu với nhiều "\n" và cũng có nhiều "\n" trong đầu ra tiếp theo được tạo, expert sẽ được lấp đầy với token "\n" nhanh chóng và tất cả các token khác xuất hiện sau đó, nên được gán cho expert này, sẽ bị loại bỏ. Để xác minh điều này, chúng tôi trực quan hóa tỷ lệ token bị loại bỏ tại các Position ID khác nhau. Như được hiển thị trong Hình 12a, các bộ dữ liệu tiền huấn luyện chung, ví dụ RedPajama và TheStack đạt được gán token cân bằng, chỉ có một tỷ lệ nhỏ token bị loại bỏ, ngay cả đối với Position ID sau 1500. Tuy nhiên, đối với các bộ dữ liệu đa ngôn ngữ và instruction-following, một tỷ lệ lớn token bị loại bỏ. Chúng tôi đề xuất lý do là, như chúng tôi đã thảo luận ở trên, quyết định định tuyến được cố định ở giai đoạn đầu của huấn luyện và không thay đổi nữa, vì vậy cân bằng tải cũng được đạt được dựa trên bộ dữ liệu tiền huấn luyện. Dữ liệu instruction following có thể được xem như một loại dữ liệu ngoài lĩnh vực (OOD) của router MoE, điều này sẽ gây ra gán token không cân bằng để nhiều token xuất hiện sau đó sẽ bị loại bỏ.

Tinh chỉnh có giám sát với dữ liệu instruction-following có thể giảm thiểu vấn đề Rơi-về-cuối này không? Vì vấn đề Rơi-về-cuối chủ yếu do dữ liệu OOD gây ra, tự nhiên nghĩ và nghiên cứu xem có thể chuyển đổi dữ liệu instruction-following thành dữ liệu trong lĩnh vực bằng cách điều chỉnh MoE với bộ dữ liệu instruction không. Do đó, chúng tôi so sánh các mô hình trước và sau

--- TRANG 15 ---
[Hình 13: Trực quan hóa quyết định định tuyến tại các Token ID khác nhau. (a) Mixtral-8×7B (b) Deepseek-MoE-16B]

tinh chỉnh có giám sát trong Hình 12b. Chúng ta có thể thấy các mô hình không có sự khác biệt đáng kể trong vấn đề Rơi-về-cuối. Điều này phù hợp tốt với hiểu biết của chúng tôi ở trên, tức là hành vi định tuyến được học và cố định ở giai đoạn rất sớm của tiền huấn luyện LLM.

4.4 Nghiên cứu Các Mô hình MoE Khác
Bối cảnh Trong phần này, chúng tôi điều tra xem các vấn đề chúng tôi tìm thấy ở trên có tồn tại trong các LLM dựa trên MoE khác, tức là Mixtral và DeepSeek-MoE không. Cả Mixtral và Deepseek-MoE đều được huấn luyện bằng định tuyến token dropless, có nghĩa là các mô hình này sẽ không loại bỏ token ngay cả khi khối lượng công việc khác nhau không cân bằng. Thiết kế này ổn nếu mô hình của chúng tôi không quá lớn sau khi áp dụng một số thủ thuật triển khai như Megablock [17], có thể xử lý khối lượng công việc không cân bằng tốt hơn nếu các expert trên cùng GPU. Tuy nhiên, các thủ thuật triển khai như Megablock không thể hoạt động hiệu quả khi chỉ có một expert trên một GPU, và thật không may, điều này xảy ra với MoE LLM rất lớn (ví dụ một MoE kiểu GPT với hơn 2T tham số). Xem xét rằng kích thước bộ nhớ GPU không tăng nhanh như trước, việc có khối lượng công việc cân bằng cho mỗi expert vẫn cực kỳ quan trọng đối với huấn luyện mô hình MoE lớn hiệu quả.

Chuyên môn hóa Độc lập Ngữ cảnh Chúng tôi trực quan hóa chuyên môn hóa token ID của Mixtral và Deepseek-MoE trong Hình 13a và 13b. Chúng tôi phát hiện, tương tự như OpenMoE, Deepseek-MoE có Chuyên môn hóa Độc lập Ngữ cảnh rõ ràng, nhưng Mixtral thì không. Chúng tôi đề xuất rằng lý do là, theo blog này⁵, Mixtral có thể được finetuned dựa trên checkpoint dày đặc Mistral-7B, tức là MoE upcycling [26], thay vì huấn luyện từ đầu như deepseek-MoE và OpenMoE. Vì các expert trong Mixtral rất giống nhau, có lý khi có sự chuyên môn hóa tương đối yếu trong mô hình MoE của họ, và đồng thời, vì mô hình đã học ngữ nghĩa cấp cao khi chuyển đổi LLM dày đặc thành MoE LLM, ít có khả năng phát triển Chuyên môn hóa Độc lập Ngữ cảnh. Do đó, chúng tôi đề xuất rằng Chuyên môn hóa Độc lập Ngữ cảnh là một vấn đề chỉ dành cho huấn luyện MoE từ đầu. Tuy nhiên, như chúng tôi đã thảo luận trong bài báo của chúng tôi, MoE hiệu quả hơn trong huấn luyện thay vì suy luận, vẫn rất mong muốn nghiên cứu cách tránh Chuyên môn hóa Độc lập Ngữ cảnh khi huấn luyện MoE từ đầu hoặc chuyển đổi LLM dày đặc thành MoE ở giai đoạn đầu của huấn luyện. Một giải pháp khả thi có thể là, đầu tiên huấn luyện một LLM dày đặc nửa chín (có thể sử dụng 20% token tiền huấn luyện hoặc tương tự), và sau đó chuyển đổi LLM dày đặc thành MoE thông qua MoE upcycling. Sau đó chúng ta có thể huấn luyện MoE với 80% token huấn luyện còn lại để đảm bảo sự đánh đổi hiệu quả chi phí tốt hơn.

Học Định tuyến Sớm Vì Mixtral và Deepseek-MoE không có checkpoint trung gian mã nguồn mở, chúng tôi không thể nghiên cứu vấn đề này trên các mô hình này.

Rơi-về-cuối Như đã đề cập trước đây, Mixtral và Deepseek-MoE không có cơ chế token drop. Tuy nhiên, điều này không thân thiện với expert parallelism, đặc biệt đối với MoE-LLM rất lớn

⁵https://x.com/tianle_cai/status/1734188749117153684?s=20

--- TRANG 16 ---
[Hình 14: So sánh tỷ lệ token bị loại bỏ tại các Position ID khác nhau. (a) Mixtral-8×7B (b) DeepSeek-MoE-16B]

với tham số cấp nghìn tỷ, mặc dù ổn nếu mô hình tương đối nhỏ (<100B). Do đó, chúng tôi vẫn nghiên cứu xem có vấn đề Rơi-về-cuối trong Mixtral và Deepseek-MoE không bằng cách thủ công thêm cơ chế token drop khi có quá nhiều token được định tuyến đến một expert. Như được hiển thị trong Hình 14a và Hình 14b, có token drop rõ ràng tại các token sau trong chuỗi đầu vào, có nghĩa là Rơi-về-cuối là một vấn đề đối với tất cả các MoE LLM này.

5 Suy ngẫm lại OpenMoE
Làm việc trên dự án này là một hành trình dài đối với các tác giả. Chúng tôi thực sự đã mắc một số sai lầm trong quá trình thiết kế và phát triển, nhưng chúng tôi cũng đạt được một số hiểu biết mới trong phân tích. Do đó chúng tôi viết ra mọi thứ chúng tôi tìm thấy mà không có bất kỳ dè dặt nào trong bài báo này để giúp các nhà thực hành tương lai. Sau đó, trong phần này, chúng tôi thảo luận về cách huấn luyện một mô hình tốt hơn trong tương lai, đây là những điểm chính quan trọng nhất của công việc chúng tôi.

Chúng ta nên sử dụng bao nhiều mã? Thành thật mà nói, chúng tôi không có câu trả lời rất chính xác. Tiến hành nghiên cứu ablation cực kỳ tốn kém vì chi phí tiền huấn luyện LLM ở quy mô lớn. Kết luận cũng có thể phụ thuộc mạnh vào kích thước mô hình và chất lượng dữ liệu. Tuy nhiên, theo quan sát của chúng tôi, hơn 50% mã có vẻ quá tích cực có thể gây hại cho khả năng trên các tác vụ văn bản, nhưng xem xét tầm quan trọng của việc viết mã, chúng tôi đề xuất sử dụng khoảng 30% mã như chúng tôi đã sử dụng trong OpenMoE-34B/32E.

Lựa chọn Tokenizer Từ vựng tokenizer lớn của chúng tôi giới thiệu chi phí tính toán ở lớp đầu ra cuối cùng sau các khối Transformer. Mặc dù chi phí này sẽ trở nên tương đối nhỏ sau khi mở rộng mô hình Transformer, vẫn có giá trị để làm cho lựa chọn tokenizer thông minh hơn. Chúng tôi tiến hành phân tích định lượng tokenizer với các bộ dữ liệu chúng tôi sử dụng trong Phần 4. Như được hiển thị trong Bảng 10, tokenizer umT5 thực sự tốt hơn nhiều so với tokenizer LLaMA trên bộ dữ liệu đa ngôn ngữ, đặc biệt trên ngôn ngữ ít tài nguyên. Nó cũng hơi tốt hơn LLaMA trên dữ liệu instruction-following. Tuy nhiên, nó không phù hợp với kỳ vọng của chúng tôi rằng nó có thể tiết kiệm nhiều token hơn cho dữ liệu mã. Ngoài ra, chúng tôi quan sát thấy việc sử dụng token trong cả hai tokenizer đều được phân phối long-tail cực kỳ, điều này chỉ ra rằng có không gian lớn để cải thiện tokenizer và các thuật toán tiếp theo. Như chúng ta biết, học từ dữ liệu long-tailed khó [57]. Vì chúng tôi chỉ có một ít dữ liệu đa ngôn ngữ trong hỗn hợp dữ liệu tiền huấn luyện, chi phí tính toán dự đoán logits của những token ít tài nguyên đó bị lãng phí. Dựa trên lựa chọn không tối ưu của chúng tôi, chúng tôi cũng cần một benchmark tokenizer vững chắc, điều này sẽ giúp mọi người đánh giá tokenizer một cách có hệ thống. Và sau đó chúng ta có thể chọn tokenizer tốt nhất trước khi huấn luyện mô hình.

Kiến trúc MoE Hiệu quả hơn Theo quan sát của chúng tôi, định tuyến MoE gần như độc lập ngữ cảnh (tức là Chuyên môn hóa Độc lập Ngữ cảnh), chúng tôi đề xuất rằng chúng ta có thể (1) loại bỏ router có thể huấn luyện sau giai đoạn warmup; (2) áp dụng lớp Transformer song song [9, 50] tính toán lớp FFN dựa trên đầu vào trực tiếp thay vì sử dụng đầu ra của lớp attention; (3) chồng chéo tính toán lớp attention

--- TRANG 17 ---
Bảng 10: So sánh tokenizer umT5 và tokenizer LLaMA trên các tập con được trích xuất từ các bộ dữ liệu khác nhau. Vocab used biểu thị số lượng ID token được kích hoạt khi tokenize toàn bộ tập con. umT5/LLaMA có nghĩa là, khi tokenize cùng một tập con, tỷ lệ số lượng token được tạo bởi umT5 và LLaMA.

Dataset Subset LLaMA Tokenizer umT5 Tokenizer umT5/LLaMA
#Tokens Vocab Used #Tokens Vocab Used

RedPajama arxiv 125,339 8,327 131,059 8,762 1.046
book 137,972 11,603 131,072 15,202 0.950
c4 28,592 5,439 26,428 5,554 0.924
cc 78,450 8,738 73,403 9,927 0.936
github 54,707 4,769 59,732 4,539 1.092
stackexchange 40,659 4,714 43,195 4,317 1.062
wikipedia 37,406 7,179 30,555 8,748 0.817

TheStack assembly 49,143 3,066 50,738 3,130 1.032
blitzmax 78,259 4,200 80,658 4,209 1.031
java 64,236 4,229 69,902 3,905 1.088
python 66,243 5,095 70,795 4,799 1.069

MTBench writing 6,062 1,700 5,786 1,535 0.954
roleplay 4,309 1,291 4,076 1,172 0.946
reasoning 2,369 478 2,309 429 0.975
math 5,163 290 5,154 282 0.998
coding 4,955 651 5,256 631 1.061
extraction 7,058 1,376 6,817 1,234 0.966
stem 4,783 1,151 4,527 1,039 0.946
humanities 6,398 1,451 5,946 1,320 0.929

Multi-lingual ar 256,952 187 88,406 8,037 0.344
TED de 103,270 4,880 80,593 8,470 0.780
es 101,212 4,745 78,713 8,519 0.778
fr 115,057 5,156 95,978 8,164 0.834
he 242,446 239 86,891 4,074 0.358
it 109,591 4,593 84,201 8,833 0.768
ja 144,825 931 63,491 6,860 0.438
ko 257,107 596 106,770 2,736 0.415
nl 102,703 4,234 75,084 7,540 0.731
ru 107,144 2,502 74,445 9,658 0.695
zh-cn 149,581 1,058 88,107 3,611 0.589
zh-tw 173,415 1,107 93,693 3,619 0.540

và giao tiếp all-to-all lớp MoE. (1) và (3) sẽ cải thiện sử dụng phần cứng và (2) có thể kích hoạt (3) mà không giảm hiệu suất khi mở rộng [9].

Trộn dữ liệu instruction-following trong warmup tiền huấn luyện để kiểm soát cân bằng tải và giảm thiểu Rơi-về-cuối. Theo kết quả của chúng tôi trên MT-Bench nhiều lượt, rất quan trọng để giảm thiểu vấn đề Rơi-về-cuối. Để làm điều này, chìa khóa là làm cho MoE đạt được cân bằng tải trên dữ liệu instruction-following. Một lần nữa, vì MoE học và cố định hành vi định tuyến ở giai đoạn đầu của tiền huấn luyện, một giải pháp đơn giản là trộn dữ liệu instruction-tuning vào corpus tiền huấn luyện trong warmup. Việc trộn dữ liệu này không phải để align mô hình học cách tuân theo hướng dẫn. Thay vào đó, chúng tôi hy vọng mô hình đạt được định tuyến token cân bằng trên dữ liệu instruction-tuning, điều này mở đường cho trường hợp sử dụng cuối cùng của chúng tôi về LLM.

6 Kết luận
Trong công trình này, chúng tôi khám phá cách huấn luyện MoE cho các cộng đồng mã nguồn mở. Chúng tôi đạt được kết quả tích cực xác nhận hiệu quả của MoE-based LLM trong giai đoạn hậu ChatGPT. Chúng tôi tiết lộ tất cả chi tiết, và mô hình của chúng tôi hoàn toàn có thể tái tạo với mã và dữ liệu mã nguồn mở. Quan trọng hơn, chúng tôi tiến hành phân tích sâu về MoE-based LLM của chúng tôi và tìm thấy "Chuyên môn hóa Độc lập Ngữ cảnh", "Học Định tuyến Sớm" và "Rơi-về-cuối" quan trọng. Chúng tôi cũng suy ngẫm lại những sai lầm chúng tôi đã mắc và đề xuất các giải pháp có thể cho các nhà phát triển tương lai. Chúng tôi chân thành hy vọng công việc này có thể giúp cộng đồng mã nguồn mở hiểu rõ hơn về các mô hình MoE. Chúc tất cả mọi điều tốt đẹp!

Tài liệu tham khảo
[1] R. Anil et al., "Palm 2 technical report," arXiv preprint arXiv:2305.10403, 2023.
[2] Anonymous, "(inthe)wildchat: 570k chatGPT interaction logs in the wild," in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https://openreview.net/forum?id=Bl8u7ZRlbM.
[3] M. Artetxe et al., "Efficient large scale language modeling with mixtures of experts," arXiv preprint arXiv:2112.10684, 2021.
[4] B.-b. authors, "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models," Transactions on Machine Learning Research, 2023, ISSN: 2835-8856. [Online]. Available: https://openreview.net/forum?id=uyTL5Bvosj.
[5] M. Bavarian et al., "Efficient training of language models to fill in the middle," arXiv preprint arXiv:2207.14255, 2022.
[6] O. r. Bojar et al., "Findings of the 2016 conference on machine translation," in Proceedings of the First Conference on Machine Translation, Berlin, Germany: Association for Computational Linguistics, Aug. 2016, pp. 131–198. [Online]. Available: http://www.aclweb.org/anthology/W/W16/W16-2301.
[7] T. B. Brown et al., "Language models are few-shot learners," arXiv preprint arXiv:2005.14165, 2020.
[8] M. Chen et al., "Evaluating large language models trained on code," 2021. arXiv: 2107.03374 [cs.LG].
[9] A. Chowdhery et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[10] H. W. Chung et al., "Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining," arXiv preprint arXiv:2304.09151, 2023.
[11] T. Computer, Redpajama: An open source recipe to reproduce llama training dataset, 2023. [Online]. Available: https://github.com/togethercomputer/RedPajama-Data.
[12] D. Dai et al., "Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models," arXiv preprint arXiv:2401.06066, 2024.
[13] A. Dosovitskiy et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929, 2020.
[14] N. Du et al., "Glam: Efficient scaling of language models with mixture-of-experts," in International Conference on Machine Learning, PMLR, 2022, pp. 5547–5569.
[15] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," J. Mach. Learn. Res, vol. 23, pp. 1–40, 2021.
[16] H. Fu Yao; Peng and T. Khot, "How does gpt obtain its ability? tracing emergent abilities of language models to their sources," Yao Fu's Notion, Dec. 2022. [Online]. Available: https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1.
[17] T. Gale, D. Narayanan, C. Young, and M. Zaharia, "Megablocks: Efficient sparse training with mixture-of-experts," Proceedings of Machine Learning and Systems, vol. 5, 2023.
[18] L. Gao et al., A framework for few-shot language model evaluation, version v0.4.0, Dec. 2023. DOI:10.5281/zenodo.10256836. [Online]. Available: https://zenodo.org/records/10256836.
[19] X. Geng and H. Liu, Openllama: An open reproduction of llama, May 2023. [Online]. Available: https://github.com/openlm-research/open_llama.
[20] D. Hendrycks et al., "Measuring massive multitask language understanding," arXiv preprint arXiv:2009.03300, 2020.

--- TRANG 21 ---
[21] J. Hoffmann et al., "Training compute-optimal large language models," arXiv preprint arXiv:2203.15556, 2022.
[22] A. Q. Jiang et al., "Mixtral of experts," arXiv preprint arXiv:2401.04088, 2024.

--- TRANG 19 ---
[23] M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension," in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), R. Barzilay and M.-Y. Kan, Eds., Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601–1611. DOI:10.18653/v1/P17-1147. [Online]. Available: https://aclanthology.org/P17-1147.
[24] J. D. M.-W. C. Kenton and L. K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," in Proceedings of naacL-HLT, vol. 1, 2019, p. 2.
[25] D. Kocetkov et al., "The stack: 3 tb of permissively licensed source code," Preprint, 2022.
[26] A. Komatsuzaki et al., "Sparse upcycling: Training mixture-of-experts from dense checkpoints," in The Eleventh International Conference on Learning Representations, 2023. [Online]. Available: https://openreview.net/forum?id=T5nUQDrM4u.
[27] D. Lepikhin et al., "Gshard: Scaling giant models with conditional computation and automatic sharding," arXiv preprint arXiv:2006.16668, 2020.
[28] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer, "Base layers: Simplifying training of large, sparse models," in International Conference on Machine Learning, PMLR, 2021, pp. 6265–6274.
[29] J. Li, Z. Zhang, and H. Zhao, "Self-prompting large language models for open-domain qa," arXiv preprint arXiv:2212.08635, 2022.
[30] R. Li et al., "Starcoder: May the source be with you!" arXiv preprint arXiv:2305.06161, 2023.
[31] Y. Liu et al., "Roberta: A robustly optimized bert pretraining approach," arXiv preprint arXiv:1907.11692, 2019.
[32] Y. Lou, F. Xue, Z. Zheng, and Y. You, "Cross-token modeling with conditional computation," arXiv preprint arXiv:2109.02008, 2021.
[33] B. Mustafa, C. Riquelme, J. Puigcerver, R. Jenatton, and N. Houlsby, "Multimodal contrastive learning with limoe: The language-image mixture of experts," Advances in Neural Information Processing Systems, vol. 35, pp. 9564–9576, 2022.
[34] E. Nijkamp et al., "Xgen-7b technical report," arXiv preprint arXiv:2309.03450, 2023.
[35] J. Puigcerver, C. Riquelme, B. Mustafa, and N. Houlsby, "From sparse to soft mixtures of experts," arXiv preprint arXiv:2308.00951, 2023.
[36] J. W. Rae et al., "Scaling language models: Methods, analysis & insights from training gopher," arXiv preprint arXiv:2112.11446, 2021.
[37] C. Raffel et al., "Exploring the limits of transfer learning with a unified text-to-text transformer," Journal of Machine Learning Research, vol. 21, no. 140, pp. 1–67, 2020. [Online]. Available: http://jmlr.org/papers/v21/20-074.html.
[38] C. Riquelme et al., "Scaling vision with sparse mixture of experts," Advances in Neural Information Processing Systems, vol. 34, pp. 8583–8595, 2021.
[39] S. Roller, S. Sukhbaatar, J. Weston, et al., "Hash layers for large sparse models," Advances in Neural Information Processing Systems, vol. 34, pp. 17555–17566, 2021.
[40] B. Roziere et al., "Code llama: Open foundation models for code," arXiv preprint arXiv:2308.12950, 2023.
[41] N. Shazeer, "Glu variants improve transformer," arXiv preprint arXiv:2002.05202, 2020.
[42] N. Shazeer et al., "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," arXiv preprint arXiv:1701.06538, 2017.
[43] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," arXiv preprint arXiv:1909.08053, 2019.
[44] L. Soldaini et al., "Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research," arXiv preprint, 2023.
[45] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, "Roformer: Enhanced transformer with rotary position embedding," Neurocomputing, vol. 568, p. 127063, 2024.
[46] Y. Tay et al., "Ul2: Unifying language learning paradigms," in The Eleventh International Conference on Learning Representations, 2022.
[47] Y. Tay et al., "Unifying language learning paradigms," arXiv preprint arXiv:2205.05131, 2022.

--- TRANG 20 ---
[48] L.-M. Team, Llama-moe: Building mixture-of-experts from llama with continual pre-training, Dec. 2023. [Online]. Available: https://github.com/pjlab-sys4nlp/llama-moe.
[49] H. Touvron et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.
[50] B. Wang and A. Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model, https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
[51] G. Wenzek et al., "CCNet: Extracting high quality monolingual datasets from web crawl data," English, in Proceedings of the Twelfth Language Resources and Evaluation Conference, N. Calzolari et al., Eds., Marseille, France: European Language Resources Association, May 2020, pp. 4003–4012, ISBN: 979-10-95546-34-4. [Online]. Available: https://aclanthology.org/2020.lrec-1.494.
[52] Y. Xu et al., "Gspmd: General and scalable parallelization for ml computation graphs," arXiv preprint arXiv:2105.04663, 2021.
[53] F. Xue, X. He, X. Ren, Y. Lou, and Y. You, "One student knows all experts know: From sparse to dense," arXiv preprint arXiv:2201.10890, 2022.
[54] F. Xue, Z. Shi, F. Wei, Y. Lou, Y. Liu, and Y. You, "Go wider instead of deeper," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, 2022, pp. 8779–8787.
[55] P. Yu et al., "Efficient language modeling with sparse all-mlp," arXiv preprint arXiv:2203.06850, 2022.
[56] P. Zhang, G. Zeng, T. Wang, and W. Lu, Tinyllama: An open-source small language model, 2024. arXiv: 2401.02385 [cs.CL].
[57] Y. Zhang, B. Kang, B. Hooi, S. Yan, and J. Feng, "Deep long-tailed learning: A survey," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.
[58] L. Zheng et al., "Judging llm-as-a-judge with mt-bench and chatbot arena," arXiv preprint arXiv:2306.05685, 2023.
[59] Y. Zhou et al., "Brainformers: Trading simplicity for efficiency," in International Conference on Machine Learning, PMLR, 2023, pp. 42531–42542.
[60] Y. Zhou et al., "Mixture-of-experts with expert choice routing," Advances in Neural Information Processing Systems, vol. 35, pp. 7103–7114, 2022.
[61] B. Zoph et al., "St-moe: Designing stable and transferable sparse expert models," URL https://arxiv.org/abs/2202.08906, 2022.

--- TRANG 21 ---
Phụ lục
A Câu hỏi Thường gặp
Chúng tôi liệt kê các câu hỏi có thể thường gặp và câu trả lời point-to-point⁵ như sau:

A.1 Tại sao không hiển thị chuyên môn hóa token của các checkpoint ở giai đoạn warmup?
Chúng tôi không mong đợi rằng định tuyến sẽ được học và cố định sớm như vậy. Trong quá trình huấn luyện, do hạn chế quota lưu trữ, chúng tôi chỉ giữ các checkpoint mỗi 200B token.

A.2 Tại sao không so sánh với các mô hình MoE mở tiên tiến như Mixtral và DeepSeek-MoE?
Đầu tiên, mô hình của chúng tôi được công bố và phát hành sớm hơn Mistral hơn 4 tháng và thậm chí nhiều hơn so với DeepSeek-MoE. Thứ hai, khác với các mô hình sử dụng dữ liệu huấn luyện nội bộ, mô hình của chúng tôi hoàn toàn minh bạch. Chúng tôi cũng tiết lộ tất cả chi tiết và mã để đảm bảo mọi người có thể huấn luyện một mô hình OpenMoE có thể so sánh từ đầu.

A.3 Tại sao không sử dụng MoE upcycling?
MoE hiệu quả hơn trong huấn luyện thay vì suy luận, vì sự song song tốt hơn được tạo ra bởi kích thước batch lớn. Xây dựng MoE trên các LLM dày đặc là cách thông minh và nhanh hơn để có được mô hình MoE, nhưng không phải là cách hiệu quả hơn từ góc độ dài hạn. Thay vào đó, có lẽ chưng cất MoE thành mô hình dày đặc [53] sẽ hữu ích nếu có ít suy giảm hiệu suất.

A.4 Tại sao không sử dụng optimizer AdamW và Cosine Learning Rate Schedule?
Chúng tôi áp dụng optimizer Adafactor và lịch trình learning rate Inverse Square Root theo ST-MoE [61]. Chúng tôi đã thử Optimizer AdamW nhưng phát hiện rằng điều đó sẽ giới thiệu các vấn đề không ổn định (tức là NAN loss) thường xuyên, có thể giới thiệu một lượng đáng kể sweep siêu tham số. Xem xét tài nguyên tính toán hạn chế mà chúng tôi có, chúng tôi quyết định đơn giản tuân theo lịch trình learning rate được nghiên cứu kỹ từ ST-MoE [61].

A.5 Tại sao không sử dụng các bộ dữ liệu tốt hơn và lớn hơn?
Khi khởi động dự án này vào tháng 5 năm 2023, chỉ có một vài bộ dữ liệu tiền huấn luyện mã nguồn mở có sẵn. Tuy nhiên, quy mô và chất lượng của các bộ dữ liệu tiền huấn luyện mã nguồn mở đang ngày càng tốt hơn. Ví dụ, Soldaini et al. [44] đã phát hành 3T token với việc làm sạch cẩn thận. Computer [11] cũng đã phát hành một bộ dữ liệu khổng lồ với tổng cộng 30T token. Chúng tôi tin rằng huấn luyện trên dữ liệu tốt hơn trong tương lai sẽ cải thiện hiệu suất LLM nói chung với biên độ lớn.

--- TRANG 22 ---
B Siêu tham số
Bảng 11: Cấu hình Mô hình. H là kích thước ẩn. "Layout" có nghĩa là cách sử dụng lớp MoE. Ví dụ, "Every 4" có nghĩa là chúng tôi sử dụng một lớp MoE cho mỗi 4 khối transformer. HFFN là kích thước trung gian FFN. NHead và HHead là số lượng attention head và chiều attention head. L là số lượng lớp. #Param là tổng tham số. #ActParam là số lượng tham số chúng tôi sử dụng để xử lý mỗi token trong các khối Transformer. #ActParam w/ E là tổng của #ActParam và số lượng tham số trong lớp token embedding.

Mô hình Layout H HFFN NHead HHead L #Param #ActParam w/ E #ActParam
OpenMoE-Base/16E Every 4 768 3072 12 64 12 650M 339M 142M
OpenMoE-8B/32E Every 6 2048 8192 24 128 24 8.7B 2.6B 2.1B
OpenMoE-34B/32E Every 4 3072 12288 24 128 32 34B 6.8B 6.0B
TinyLLaMA - 2048 5632 32 64 22 1.0B 1.0B 0.9B
OpenLLaMA-3B - 3200 8640 32 64 26 3.0B 3.0B 2.9B
LLaMA-7B - 4096 11008 32 128 32 6.6B 6.4B 6.4B

Đối với OpenMoE-8B/32E, chúng tôi đặt chiều head là 128 thay vì 64, có thể quá lớn cho một mô hình sử dụng 2B tham số Transformer kích hoạt. Chúng tôi đề xuất rằng sử dụng 64 có thể tạo ra sự đánh đổi hiệu quả chi phí tốt hơn so với của chúng tôi. Đối với số lượng tham số trong bảng trên, vì hầu hết tham số trong các khối Transformer đều từ lớp attention và lớp FFN, chúng tôi chỉ tính các tham số có thể huấn luyện từ hai cái này để đơn giản.

Bảng 12: Siêu tham số huấn luyện OpenMoE.
Base/16E 8B/32E 34B/32E
Optimizer Adafactor
Batch Size 128 2048 2048
Training Steps 500K 500K 100K
Peak Learning Rate 0.01
Learning Rate Schedule Inverse Square Root Decay
Warmup Steps 10K
Sequence Length 2048
Load Balance Loss Weight 0.01
Z-Loss Weight 0.001
Router Z-Loss Weight 0.0001

Khác với các LLM hiện tại được huấn luyện với AdamW, chúng tôi sử dụng Adafactor, một optimizer tiết kiệm bộ nhớ hơn. Mặc dù nó hoạt động hơi tệ hơn AdamW với cùng số bước huấn luyện, hiệu quả bộ nhớ cho phép chúng tôi sử dụng ít song song mô hình hơn và nhiều song song dữ liệu hơn. Trong trường hợp này, sử dụng Adafactor làm cho huấn luyện của chúng tôi rẻ hơn so với sử dụng AdamW để huấn luyện cùng mô hình trên cùng dữ liệu. Tuy nhiên, chúng tôi nhấn mạnh rằng biên của khoảng cách này không rõ ràng vì nó phụ thuộc nhiều vào phần cứng và kích thước mô hình. Đối với cơ sở hạ tầng của chúng tôi, tức là TPUv3, khoảng cách này nên tương đối lớn hơn do bộ nhớ on-chip hạn chế (16 GB mỗi core).

--- TRANG 23 ---
C Công trình Liên quan
C.1 Trước OpenMoE
MoE không mới. Một nỗ lực đại diện sớm là, Shazeer et al. [42] nhúng lớp MoE vào mô hình ngôn ngữ hồi quy. Do khả năng mở rộng của kiến trúc Transformer, GShard [27] tích hợp MoE vào lớp Transformer và sử dụng expert parallelism để huấn luyện Transformer dựa trên MoE ở quy mô lớn. Switch Transformer [15] là MoE-based LM mã nguồn mở sớm nhất theo hiểu biết tốt nhất của chúng tôi, sử dụng kiến trúc encoder-decoder và được huấn luyện với bộ dữ liệu C4 [37]. Do thành công của Switch Transformer trên tiền huấn luyện quy mô lớn, MoE nhận được nhiều sự chú ý hơn, và các thuật toán định tuyến tiên tiến hơn được phát minh. Ví dụ, BASE Layers [28] xây dựng phân bổ token-to-expert như một bài toán gán tuyến tính, cho phép gán tối ưu trong đó mỗi expert nhận được số lượng token bằng nhau. Roller et al. [39] đơn giản sửa đổi lớp feedforward để hash thành các bộ trọng số khác nhau tùy thuộc vào token hiện tại và đạt được kết quả hứa hẹn so với định tuyến dựa trên học. Khác với định tuyến dựa trên Token ở trên, Zhou et al. [60] đề xuất để các expert chọn token yêu thích của họ, tức là Expert-Choice Routing. Expert-choice Routing đạt được gán token cân bằng hơn và sự đánh đổi hiệu quả chi phí tốt hơn.

Ngoài thuật toán định tuyến, cũng có một số công việc tập trung vào việc mở rộng MoE hiệu quả. Artetxe et al. [3] huấn luyện các mô hình MoE của họ chủ yếu trên các bộ dữ liệu được sử dụng trong RoBERTa [31] và CC100 [51] (tổng cộng 112B token). GaLM [14] tiếp tục mở rộng mô hình MoE chỉ giải mã với bộ dữ liệu chất lượng cao nội bộ với 1.6T token. Brainformer [59] đề xuất một tìm kiếm tiến hóa để khám phá các thuộc tính MoE, ví dụ cách tốt nhất để xen kẽ các lớp và dung lượng lớp, khi nào hợp nhất các lớp, và khi nào chuyên môn hóa các lớp với các mô-đun MoE và cho thấy hiệu quả của nó ở các quy mô khác nhau.

Ngoài mô hình ngôn ngữ, Vision Transformer (ViT) [13] cũng có thể được tăng cường bằng kiến trúc MoE. ViT-MoE [38] xác minh khả năng mở rộng của MoE trên các mô hình ViT. WideNet [54] chia sẻ các khối Transformer dựa trên MoE với layer normalization riêng lẻ để đạt được hiệu quả tham số tốt hơn. SoftMoE [35] tiếp tục cải thiện thuật toán định tuyến bằng cách áp dụng lựa chọn token mềm, không chỉ giữ hiệu quả mà còn ổn định gradient định tuyến. Cũng có một số nỗ lực dành riêng để bao gồm MoE vào kiến trúc non-Transformer, ví dụ Sparse-MLP [32] cho computer vision và s-MoE cho mô hình ngôn ngữ [55].

C.2 Sau OpenMoE
Bảng 13: Timeline MoE LLM mã nguồn mở. Chúng tôi sử dụng ngày phát hành mô hình làm khóa để sắp xếp các MoE LLM mã nguồn mở. Dataset Size là số lượng token trong bộ dữ liệu tiền huấn luyện, tức là số lượng token cho một epoch. LLaMA-MoE được tiếp tục tiền huấn luyện trên các mô hình họ LLaMA có sẵn. Chúng tôi chỉ tính bộ dữ liệu huấn luyện tiếp tục của nó.

Tên Mô hình Dataset Size Reproducible Release Date
Switch Transformer [15] 156B Yes Feb 2021
Meta-MoE [3] 112B Yes Dec 2021
OpenMoE (Của chúng tôi) 1.1T Yes Aug 2023
Mixtral of Experts [22] Unknown No Dec 2023
LLaMA-MoE [48] 200B Yes Dec 2023
DeepSeek-MoE [12] 2T No Jan 2024

Chúng tôi đã phát hành mô hình và triển khai của chúng tôi sớm hơn nhiều so với việc viết báo cáo này. Như được hiển thị trong Bảng 13, sau khi phát hành của chúng tôi, có một số mô hình mã nguồn mở một phần được phát hành, ví dụ Mixtral [22] và Deepseek-MoE [12]. Như chúng tôi biết, những mô hình này tốt hơn đáng kể về mặt kết quả cuối cùng. Tuy nhiên, vì những mô hình này được huấn luyện với dữ liệu nội bộ, chúng tôi không biết những gì đã xảy ra. Chúng tôi tin rằng, mặc dù kết quả của chúng tôi không tuyệt vời như vậy, tính chất hoàn toàn mã nguồn mở và phân tích sâu đều có ý nghĩa đối với cộng đồng.

--- TRANG 24 ---
D Kết quả BigBench-Lite
Bảng 14: Kết quả BigBench-Lite chi tiết. Lưu ý rằng BIG-G-sparse 8B là một mô hình MoE với tổng cộng 60B tham số.

Mô hình BIG-G 8B BIG-G-sparse 8B GPT-3 6B OpenMoE-8B
auto_debugging 0.0 0.0 0.0 17.65
bbq_lite_json 58.63 46.13 49.85 42.67
code_line_description 4.66 2.44 20.18 2.44
conceptual_combinations −2.16 1.07 −3.36 0.81
conlang_translation 31.38 33.25 37.92 36.93
emoji_movie 3.75 7.5 −5.0 3.75
formal_fallacies_syllogisms_negation 0.78 −0.39 −0.8 −0.56
hindu_knowledge 12.44 8.63 19.29 16.24
known_unknowns −34.78 −4.35 −8.7 −13.04
language_identification 1.39 −0.33 1.66 1.77
linguistics_puzzles 0.0 0.0 0.0 0.05
logic_grid_puzzle −2.45 0.01 −0.28 0.89
logical_deduction 1.38 4.2 1.05 0.09
misconceptions_russian −34.69 −38.78 −34.69 −38.78
novel_concepts 10.16 14.06 17.97 6.25
operators 10.48 16.67 20.0 20.48
parsinlu_reading_comprehension 0.0 0.0 0.0 11.97
play_dialog_same_or_different 12.5 4.69 −3.8 1.1
repeat_copy_logic 0.0 6.25 0.0 3.12
strange_stories −7.15 −4.77 9.54 14.52
strategyqa 7.23 8.4 −3.8 3.36
symbol_interpretation 6.06 0.13 4.17 2.65
vitaminc_fact_verification 6.25 1.27 −3.2 21.34
winowhy 4.69 5.27 11.6 10.14
Trung bình 3.77 4.63 5.40 6.93

--- TRANG 25 ---
E Độ lệch chuẩn Quyết định Định tuyến

[Hình 15: Độ lệch chuẩn quyết định định tuyến tại các position ID khác nhau.]

[Hình 16: Độ lệch chuẩn quyết định định tuyến tại các token ID khác nhau. Chúng tôi chỉ lấy các token ID có hơn 128 token, vì các token cực kỳ ít tài nguyên luôn có độ lệch chuẩn quyết định định tuyến lớn. Các token ID không bao giờ xuất hiện cũng hoàn toàn không có phương sai.]

Trong Hình 15 và 16, chúng ta có thể thấy rõ ràng rằng các token ID có độ lệch chuẩn lớn hơn về quyết định định tuyến so với position ID. Ngoài ra, hầu hết token ID có độ lệch chuẩn tương đối lớn, có nghĩa là hầu hết các token ID có Định tuyến Độc lập Ngữ cảnh.

--- TRANG 26 ---
F Lựa chọn Token Hàng đầu bởi Expert
Bảng 15: Token Hàng đầu được chọn bởi mỗi expert.

Expert ID Token Hàng đầu
0 "\n", "'", "'", "s", "-", "$", "y", "_", " ", "2"
1 "\n", "1", " ", "2", "\\", "S", ".", "-", "C", "{"
2 "in", ".", "2", "1", "0", "\n", " ", "3", "_", "4"
3 "s", ")", "a", "\n", "which", "es", ");", "}", "\\", "e"
4 "\n", ".", "0", "the", ",", "_", "that", "1", "as", "ˆ"
5 " ", "\n", "s", "2", "a", "on", "ter", "*", "\\", "all"
6 "the", ",", ".", "a", "to", "of", " ", "s", "de", "\n"
7 ",", "and", "\n", ":", "_", " ", "0", "on", "at", "{"
8 "(", ".", "that", "s", " ", ",", "C", "which", "of", "G"
9 "(", "this", "2", "\n", "\\", " ", "3", "also", "I", "1", ","
10 "\n", ".", "and", "\r", ").", ";", "\t", ":", "?", "The"
11 "to", "1", "the", "2", "0", "s", "for", "t", "3", "\n"
12 "the", ",", "$", "to", "in", "?", "as", "that", "In", "who"
13 "in", "/", "0", "\n", "with", "-", " ", "{", "of", "2"
14 "is", ".", "are", "be", "was", "s", "\n", ",", "has", "not"
15 "of", "\n", "_", "s", " ", ".", "S", "the", "for", "\\"
16 "cite", ",", "\n", ".", "{", "s", "'", "ing", "data", "\\$", "\t"
17 "the", ".", "\n", "The", "0", "1", "as", "of", "5", "2"
18 "-", "{", "for", "(", "_", " ", "$", "(", "\n", "}"
19 " ", "and", "in", "to", ",", "of", "or", "\n", "by", "$"
20 "\n", "the", "$", "a", "0", "}", "this", "1", "s", "9", " "
21 ",", "and", " ", ".", "\n", "=", "\t", "the", " ", "n"
22 "the", "\n", ")", ",", "his", "their", "s", "\"", ",", "i"
23 ".", "\n", ",", "*", "<pad>", "Cha", "i", "!", "our", "/"
24 "a", "with", "}", "in", ")'", ":", "an", "1", "\n", "at"
25 "\\", "the", ".", "of", "er", ", ", "s", "ter", "book", "model"
26 "\n", ", ", ".", "a", "<pad>", "s", "de", "al", "-"
27 "the", "'", "I", "The", ", ", "it", "we", "he", "a", "x"
28 ", ", "ly", "{", "_{", "new", "-", "ed", "more", "\n", "d"
29 ", ", ".", "of", ";", "by", ",:", "\n", "to", "from", "("
30 "}", "ed", "d", "have", "ing", ", ", "has", "s", "\"", "had"
31 "to", "can", "s", "of", "ing", "will", "not", "e", "ed", "would"
