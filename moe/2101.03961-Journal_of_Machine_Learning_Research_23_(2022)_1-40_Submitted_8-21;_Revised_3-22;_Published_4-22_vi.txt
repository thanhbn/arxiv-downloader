# 2101.03961.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2101.03961.pdf
# Kích thước tệp: 1304157 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Journal of Machine Learning Research 23 (2022) 1-40 Đã nộp 8/21; Đã sửa đổi 3/22; Đã xuất bản 4/22
Switch Transformers: Mở rộng quy mô đến các mô hình nghìn tỷ tham số
với Sparsity đơn giản và hiệu quả
William Fedus
liamfedus@google.com
Barret Zoph*
barretzoph@google.com
Noam Shazeer
noam@google.com
Google, Mountain View, CA 94043, USA
Biên tập viên: Alexander Clark
Tóm tắt
Trong deep learning, các mô hình thường sử dụng lại các tham số giống nhau cho tất cả đầu vào. Các mô hình Mixture of Experts (MoE) phá vỡ điều này và thay vào đó chọn các tham số khác nhau cho từng ví dụ đầu vào. Kết quả là một mô hình được kích hoạt thưa thớt - với số lượng tham số cực lớn - nhưng chi phí tính toán không đổi. Tuy nhiên, mặc dù có một số thành công đáng chú ý của MoE, việc áp dụng rộng rãi đã bị cản trở bởi độ phức tạp, chi phí giao tiếp và tính không ổn định trong huấn luyện. Chúng tôi giải quyết những vấn đề này với việc giới thiệu Switch Transformer. Chúng tôi đơn giản hóa thuật toán định tuyến MoE và thiết kế các mô hình cải tiến trực quan với chi phí giao tiếp và tính toán giảm. Các kỹ thuật huấn luyện được đề xuất của chúng tôi giảm thiểu tính không ổn định, và chúng tôi cho thấy các mô hình thưa thớt lớn có thể được huấn luyện, lần đầu tiên, với các định dạng độ chính xác thấp hơn (bfloat16). Chúng tôi thiết kế các mô hình dựa trên T5-Base và T5-Large (Raffel et al., 2019) để đạt được tăng tốc độ tiền huấn luyện lên đến 7x với cùng tài nguyên tính toán. Những cải tiến này mở rộng sang cài đặt đa ngôn ngữ nơi chúng tôi đo được lợi ích so với phiên bản mT5-Base trên tất cả 101 ngôn ngữ. Cuối cùng, chúng tôi thúc đẩy quy mô hiện tại của các mô hình ngôn ngữ bằng cách tiền huấn luyện các mô hình lên đến nghìn tỷ tham số trên "Colossal Clean Crawled Corpus", và đạt được tăng tốc 4x so với mô hình T5-XXL.12

Từ khóa: mixture-of-experts, xử lý ngôn ngữ tự nhiên, sparsity, machine learning quy mô lớn, distributed computing

*. Đóng góp bằng nhau.
1. Mã JAX cho Switch Transformer và tất cả các checkpoint mô hình có sẵn tại https://github.com/google-research/t5x
2. Mã TensorFlow cho Switch Transformer có sẵn tại https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/moe.py

©2022 William Fedus, Barret Zoph và Noam Shazeer.
Giấy phép: CC-BY 4.0, xem https://creativecommons.org/licenses/by/4.0/ . Yêu cầu ghi công được cung cấp tại http://jmlr.org/papers/v23/21-0998.html .arXiv:2101.03961v3 [cs.LG] 16 Jun 2022

--- TRANG 2 ---
Fedus, Zoph và Shazeer
Mục lục
1 Giới thiệu 3
2 Switch Transformer 4
2.1 Đơn giản hóa Định tuyến thưa thớt . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
2.2 Định tuyến thưa thớt hiệu quả . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.3 Kết hợp tất cả lại: Switch Transformer . . . . . . . . . . . . . . . . . . . . . 8
2.4 Kỹ thuật huấn luyện và tinh chỉnh cải tiến . . . . . . . . . . . . . . . . . . . . . . 8
3 Tính chất mở rộng quy mô 11
3.1 Kết quả mở rộng quy mô trên cơ sở bước . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3.2 Kết quả mở rộng quy mô trên cơ sở thời gian . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.3 Mở rộng quy mô so với mô hình Dense lớn hơn . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4 Kết quả Downstream 14
4.1 Tinh chỉnh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
4.2 Chưng cất . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4.3 Học đa ngôn ngữ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5 Thiết kế mô hình với Song song dữ liệu, mô hình và chuyên gia 18
5.1 Song song dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.2 Song song mô hình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
5.3 Song song mô hình và dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
5.4 Song song chuyên gia và dữ liệu . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
5.5 Song song chuyên gia, mô hình và dữ liệu . . . . . . . . . . . . . . . . . . . . . . 22
5.6 Hướng tới các mô hình nghìn tỷ tham số . . . . . . . . . . . . . . . . . . . . . . . 22
6 Công việc liên quan 24
7 Thảo luận 25
8 Công việc tương lai 26
9 Kết luận 27
A Switch cho Attention 27
B Ngăn chặn việc bỏ Token với No-Token-Left-Behind 29
C Khuyến khích khám phá trên các chuyên gia 29
D Switch Transformers trong các chế độ tính toán thấp hơn 29
E Mối quan hệ giữa hiệu suất mô hình Upstream và Downstream 32
F Mã giả cho Switch Transformers 33
2

--- TRANG 3 ---
Switch Transformers
1. Giới thiệu
Huấn luyện quy mô lớn đã là một con đường hiệu quả hướng tới các mô hình ngôn ngữ thần kinh linh hoạt và mạnh mẽ (Radford et al., 2018; Kaplan et al., 2020; Brown et al., 2020). Các kiến trúc đơn giản - được hỗ trợ bởi ngân sách tính toán dồi dào, kích thước tập dữ liệu và số lượng tham số - vượt qua các thuật toán phức tạp hơn (Sutton, 2019). Một cách tiếp cận được theo đuổi trong Radford et al. (2018); Raffel et al. (2019); Brown et al. (2020) mở rộng kích thước mô hình của một Transformer được kích hoạt dày đặc (Vaswani et al., 2017). Mặc dù hiệu quả, nó cũng cực kỳ thâm dụng tính toán (Strubell et al., 2019). Được truyền cảm hứng bởi thành công của quy mô mô hình, nhưng tìm kiếm hiệu quả tính toán lớn hơn, chúng tôi thay vào đó đề xuất một mô hình chuyên gia được kích hoạt thưa thớt: Switch Transformer. Trong trường hợp của chúng tôi, sparsity đến từ việc kích hoạt một tập con các trọng số mạng thần kinh cho mỗi ví dụ đầu vào.

[Biểu đồ cho thấy các tính chất scaling và hiệu quả mẫu của Switch Transformers]

Huấn luyện thưa thớt là một lĩnh vực nghiên cứu và kỹ thuật tích cực (Gray et al., 2017; Gale et al., 2020), nhưng tính đến hôm nay, các thư viện machine learning và bộ gia tốc phần cứng vẫn phục vụ cho phép nhân ma trận dày đặc. Để có một thuật toán thưa thớt hiệu quả, chúng tôi bắt đầu với paradigm Mixture-of-Expert (MoE) (Jacobs et al., 1991; Jordan và Jacobs, 1994; Shazeer et al., 2017), và đơn giản hóa nó để mang lại tính ổn định huấn luyện và lợi ích tính toán. Các mô hình MoE đã có những thành công đáng chú ý trong dịch máy (Shazeer et al., 2017, 2018; Lepikhin et al., 2020), tuy nhiên, việc áp dụng rộng rãi bị cản trở bởi độ phức tạp, chi phí giao tiếp và tính không ổn định huấn luyện.

Chúng tôi giải quyết những vấn đề này, và sau đó vượt ra ngoài dịch thuật, để thấy rằng lớp thuật toán này có giá trị rộng rãi trong ngôn ngữ tự nhiên. Chúng tôi đo được scaling vượt trội trên một tập đa dạng các nhiệm vụ ngôn ngữ tự nhiên và trên ba chế độ trong NLP: tiền huấn luyện, tinh chỉnh và huấn luyện đa nhiệm vụ. Trong khi công việc này tập trung vào quy mô, chúng tôi cũng cho thấy rằng kiến trúc Switch Transformer không chỉ xuất sắc trong lĩnh vực siêu máy tính, mà còn có lợi ngay cả với chỉ một vài lõi tính toán. Hơn nữa, các mô hình thưa thớt lớn của chúng tôi có thể được chưng cất (Hinton et al., 2015) thành các phiên bản dày đặc nhỏ trong khi vẫn bảo tồn 30% lợi ích chất lượng của mô hình thưa thớt. Đóng góp của chúng tôi như sau:

• Kiến trúc Switch Transformer, đơn giản hóa và cải thiện so với Mixture of Experts.

• Tính chất scaling và benchmark so với mô hình T5 được điều chỉnh mạnh mẽ (Raffel et al., 2019) nơi chúng tôi đo được tăng tốc tiền huấn luyện 7x+ trong khi vẫn sử dụng cùng FLOPS mỗi token. Chúng tôi tiếp tục cho thấy các cải tiến duy trì ngay cả với tài nguyên tính toán hạn chế, sử dụng ít nhất hai chuyên gia.

• Chưng cất thành công các mô hình tiền huấn luyện thưa thớt và tinh chỉnh chuyên biệt thành các mô hình dày đặc nhỏ. Chúng tôi giảm kích thước mô hình lên đến 99% trong khi bảo tồn 30% lợi ích chất lượng của giáo viên thưa thớt lớn.

• Kỹ thuật tiền huấn luyện và tinh chỉnh cải tiến: (1) huấn luyện độ chính xác chọn lọc cho phép huấn luyện với độ chính xác bfloat16 thấp hơn (2) một sơ đồ khởi tạo cho phép scaling đến số lượng chuyên gia lớn hơn và (3) tăng regularization chuyên gia cải thiện tinh chỉnh mô hình thưa thớt và huấn luyện đa nhiệm vụ.

• Một phép đo các lợi ích tiền huấn luyện trên dữ liệu đa ngôn ngữ nơi chúng tôi tìm thấy cải tiến toàn cầu trên tất cả 101 ngôn ngữ và với 91% ngôn ngữ được hưởng lợi từ tăng tốc 4x+ so với baseline mT5 (Xue et al., 2020).

• Một sự gia tăng quy mô của các mô hình ngôn ngữ thần kinh đạt được bằng cách kết hợp hiệu quả song song dữ liệu, mô hình và chuyên gia để tạo ra các mô hình với lên đến một nghìn tỷ tham số. Các mô hình này cải thiện tốc độ tiền huấn luyện của baseline T5-XXL được điều chỉnh mạnh mẽ 4x.

2. Switch Transformer
Nguyên tắc thiết kế hướng dẫn cho Switch Transformers là tối đa hóa số lượng tham số của một mô hình Transformer (Vaswani et al., 2017) một cách đơn giản và hiệu quả tính toán. Lợi ích của quy mô đã được nghiên cứu toàn diện trong Kaplan et al. (2020) đã khám phá ra scaling theo luật lũy thừa với kích thước mô hình, kích thước tập dữ liệu và ngân sách tính toán. Quan trọng là, công việc này ủng hộ huấn luyện các mô hình lớn trên lượng dữ liệu tương đối nhỏ như cách tiếp cận tối ưu tính toán.

Lưu ý những kết quả này, chúng tôi điều tra một trục thứ tư: tăng số lượng tham số trong khi giữ các phép toán dấu phẩy động (FLOPs) mỗi ví dụ không đổi. Giả thuyết của chúng tôi là số lượng tham số, độc lập với tổng phép tính được thực hiện, là một trục quan trọng riêng biệt để scaling. Chúng tôi đạt được điều này bằng cách thiết kế một mô hình được kích hoạt thưa thớt sử dụng hiệu quả phần cứng được thiết kế cho phép nhân ma trận dày đặc như GPU và TPU. Công việc của chúng tôi ở đây tập trung vào kiến trúc TPU, nhưng lớp mô hình này có thể được huấn luyện tương tự trên các cluster GPU. Trong thiết lập huấn luyện phân tán của chúng tôi, các lớp được kích hoạt thưa thớt của chúng tôi chia các trọng số duy nhất trên các thiết bị khác nhau. Do đó, trọng số của mô hình tăng theo số lượng thiết bị, tất cả trong khi duy trì dấu chân bộ nhớ và tính toán có thể quản lý trên mỗi thiết bị.

4

--- TRANG 5 ---
Switch Transformers

[Hình minh họa về một khối encoder Switch Transformer với mô tả cách thức hoạt động của routing]

2.1 Đơn giản hóa Định tuyến thưa thớt

Định tuyến Mixture of Expert. Shazeer et al. (2017) đề xuất một lớp Mixture-of-Experts (MoE) ngôn ngữ tự nhiên lấy biểu diễn token x làm đầu vào và sau đó định tuyến điều này đến k chuyên gia tốt nhất được xác định, được chọn từ một tập {Ei(x)}N i=1 của N chuyên gia. Biến router Wr tạo ra logits h(x) = Wrx được chuẩn hóa qua phân phối softmax trên N chuyên gia có sẵn tại lớp đó. Giá trị gate cho chuyên gia i được cho bởi,

pi(x) = eh(x)i / ΣN j eh(x)j. (1)

Các giá trị gate k hàng đầu được chọn để định tuyến token x. Nếu T là tập các chỉ số k hàng đầu được chọn thì tính toán đầu ra của lớp là tổ hợp tuyến tính có trọng số của tính toán mỗi chuyên gia trên token bởi giá trị gate,

y = Σ i∈T pi(x)Ei(x). (2)

Switch Routing: Suy nghĩ lại về Mixture-of-Experts. Shazeer et al. (2017) phỏng đoán rằng định tuyến đến k>1 chuyên gia là cần thiết để có gradient không tầm thường đến các hàm định tuyến. Các tác giả trực giác rằng học cách định tuyến sẽ không hoạt động mà không có khả năng so sánh ít nhất hai chuyên gia. Ramachandran và Le (2018) đi xa hơn để nghiên cứu quyết định top-k và thấy rằng các giá trị k cao hơn trong các lớp thấp hơn trong mô hình là quan trọng đối với các mô hình có nhiều lớp định tuyến. Trái ngược với những ý tưởng này, chúng tôi thay vào đó sử dụng một chiến lược đơn giản hóa nơi chúng tôi định tuyến chỉ đến một chuyên gia duy nhất. Chúng tôi cho thấy sự đơn giản hóa này bảo tồn chất lượng mô hình, giảm tính toán định tuyến và hoạt động tốt hơn. Chiến lược định tuyến k=1 này sau này được gọi là một lớp Switch. Lưu ý rằng đối với cả MoE và Switch Routing, giá trị gate pi(x) trong Phương trình 2 cho phép khả năng vi phân của router.

Lợi ích cho lớp Switch là ba lần: (1) Tính toán router được giảm vì chúng ta chỉ định tuyến một token đến một chuyên gia duy nhất. (2) Kích thước batch (khả năng chuyên gia) của mỗi chuyên gia có thể được giảm ít nhất một nửa vì mỗi token chỉ được định tuyến đến một chuyên gia duy nhất. (3) Việc triển khai định tuyến được đơn giản hóa và chi phí giao tiếp được giảm. Hình 3 cho thấy một ví dụ về định tuyến với các hệ số khả năng chuyên gia khác nhau.

[Hình minh họa về động lực định tuyến token với các mô tả về Expert Capacity và Capacity Factor]

2.2 Định tuyến thưa thớt hiệu quả

Chúng tôi sử dụng Mesh-TensorFlow (MTF) (Shazeer et al., 2018) là một thư viện, với ngữ nghĩa và API tương tự như TensorFlow (Abadi et al., 2016) giúp thuận tiện cho các kiến trúc dữ liệu phân tán hiệu quả và song song mô hình. Nó thực hiện điều này bằng cách trừu tượng hóa tập hợp vật lý các lõi thành một lưới logic của các bộ xử lý. Tensors và tính toán sau đó có thể được chia sẻ theo các chiều được đặt tên, tạo điều kiện dễ dàng phân vùng các mô hình qua các chiều. Chúng tôi thiết kế mô hình của mình với TPU trong tâm trí, đòi hỏi các kích thước được khai báo tĩnh. Dưới đây chúng tôi mô tả việc triển khai Switch Transformer phân tán của chúng tôi.

Triển khai Switch phân tán. Tất cả các hình dạng tensor của chúng tôi được xác định tĩnh tại thời điểm biên dịch, nhưng tính toán của chúng tôi là động do các quyết định định tuyến khi huấn luyện và suy luận. Vì điều này, một cân nhắc kỹ thuật quan trọng là cách đặt khả năng chuyên gia. Khả năng chuyên gia - số lượng token mỗi chuyên gia tính toán - được đặt bằng cách chia đều số lượng token trong batch qua số lượng chuyên gia, và sau đó mở rộng thêm bằng một hệ số khả năng,

khả năng chuyên gia = tokens mỗi batch / số lượng chuyên gia × hệ số khả năng. (3)

Một hệ số khả năng lớn hơn 1.0 tạo ra bộ đệm bổ sung để đáp ứng khi các token không được cân bằng hoàn hảo qua các chuyên gia. Nếu quá nhiều token được định tuyến đến một chuyên gia (được gọi sau này là các token bị loại bỏ), tính toán được bỏ qua và biểu diễn token được truyền trực tiếp đến lớp tiếp theo thông qua kết nối dư. Việc tăng khả năng chuyên gia không phải không có nhược điểm, tuy nhiên, vì các giá trị cao sẽ dẫn đến tính toán và bộ nhớ lãng phí. Sự đánh đổi này được giải thích trong Hình 3. Thực nghiệm chúng tôi thấy việc đảm bảo tỷ lệ token bị loại bỏ thấp hơn là quan trọng cho việc scaling các mô hình chuyên gia thưa thớt. Trong suốt các thí nghiệm của chúng tôi, chúng tôi không nhận thấy bất kỳ sự phụ thuộc nào vào số lượng chuyên gia cho số lượng token bị loại bỏ (thường <1%). Sử dụng loss cân bằng tải phụ trợ (phần tiếp theo) với hệ số đủ cao đảm bảo cân bằng tải tốt. Chúng tôi nghiên cứu tác động mà những quyết định thiết kế này có đối với chất lượng và tốc độ mô hình trong Bảng 1.

Một Loss cân bằng tải có thể vi phân. Để khuyến khích tải cân bằng qua các chuyên gia, chúng tôi thêm một loss phụ trợ (Shazeer et al., 2017, 2018; Lepikhin et al., 2020). Như trong Shazeer et al. (2018); Lepikhin et al. (2020), Switch Transformers đơn giản hóa thiết kế gốc trong Shazeer et al. (2017) có các loss cân bằng tải và trọng số quan trọng riêng biệt. Đối với mỗi lớp Switch, loss phụ trợ này được thêm vào tổng loss mô hình trong quá trình huấn luyện. Cho N chuyên gia được lập chỉ mục bởi i = 1 đến N và một batch B với T tokens, loss phụ trợ được tính như tích vô hướng có scaling giữa các vector f và P,

loss = N Σ^N i=1 fi Pi (4)

trong đó fi là phần của token được gửi đến chuyên gia i,

fi = 1/T Σ x∈B 1{argmax p(x) = i} (5)

và Pi là phần của xác suất router được phân bổ cho chuyên gia i,

Pi = 1/T Σ x∈B pi(x). (6)

Vì chúng ta tìm kiếm định tuyến đồng đều của batch token qua N chuyên gia, chúng ta mong muốn cả hai vector có giá trị 1/N. Loss phụ trợ của Phương trình 4 khuyến khích định tuyến đồng đều vì nó được tối thiểu hóa dưới phân phối đồng đều. Mục tiêu cũng có thể được vi phân vì vector P có thể vi phân, nhưng vector f thì không. Loss cuối cùng được nhân với số lượng chuyên gia N để giữ loss không đổi khi số lượng chuyên gia thay đổi vì dưới định tuyến đồng đều Σ^N i=1(fi Pi) = Σ^N i=1(1/N × 1/N) = 1/N. Cuối cùng, một siêu tham số là một hệ số nhân cho những loss phụ trợ này; trong suốt công việc này chúng tôi sử dụng α = 10^-2 đủ lớn để đảm bảo cân bằng tải trong khi đủ nhỏ để không áp đảo mục tiêu cross-entropy chính. Chúng tôi quét các phạm vi siêu tham số α từ 10^-1 đến 10^-5 theo lũy thừa của 10 và thấy 10^-2 cân bằng tải nhanh chóng mà không can thiệp vào loss huấn luyện.

2.3 Kết hợp tất cả lại: Switch Transformer

Thử nghiệm đầu tiên của chúng tôi về Switch Transformer bắt đầu với tiền huấn luyện trên "Colossal Clean Crawled Corpus" (C4), được giới thiệu trong (Raffel et al., 2019). Đối với mục tiêu tiền huấn luyện của chúng tôi, chúng tôi sử dụng một nhiệm vụ mô hình hóa ngôn ngữ có che (Taylor, 1953; Fedus et al., 2018; Devlin et al., 2018) nơi mô hình được huấn luyện để dự đoán các token bị thiếu. Trong cài đặt tiền huấn luyện của chúng tôi, như được xác định trong Raffel et al. (2019) là tối ưu, chúng tôi loại bỏ 15% token và sau đó thay thế chuỗi bị che bằng một token sentinel duy nhất. Để so sánh các mô hình của chúng tôi, chúng tôi ghi lại negative log perplexity. Trong suốt tất cả các bảng trong bài báo, "↑" chỉ ra rằng giá trị cao hơn cho metric đó là tốt hơn và ngược lại cho "↓". Một so sánh của tất cả các mô hình được nghiên cứu trong công việc này có trong Bảng 9.

Một so sánh trực tiếp giữa Switch Transformer và MoE Transformer được trình bày trong Bảng 1. Mô hình Switch Transformer của chúng tôi được FLOP-matched với 'T5-Base' (Raffel et al., 2019) (cùng lượng tính toán mỗi token được áp dụng). MoE Transformer, sử dụng định tuyến top-2, có hai chuyên gia mỗi cái áp dụng một FFN riêng biệt cho mỗi token và do đó FLOPS của nó lớn hơn. Tất cả các mô hình được huấn luyện cùng số bước trên phần cứng giống hệt nhau. Lưu ý rằng mô hình MoE đi từ hệ số khả năng 2.0 đến 1.25 thực sự chậm lại (840 đến 790) trong thiết lập thí nghiệm trên, điều này không mong đợi.

Chúng tôi nêu bật ba phát hiện chính từ Bảng 1: (1) Switch Transformers vượt trội cả các mô hình dày đặc được điều chỉnh cẩn thận và MoE Transformers trên cơ sở tốc độ-chất lượng. Đối với một lượng tính toán và thời gian wall-clock cố định, Switch Transformers đạt được kết quả tốt nhất. (2) Switch Transformer có dấu chân tính toán nhỏ hơn so với đối tác MoE. Nếu chúng ta tăng kích thước của nó để phù hợp với tốc độ huấn luyện của MoE Transformer, chúng ta thấy điều này vượt trội tất cả các mô hình MoE và Dense trên cơ sở mỗi bước cũng như. (3) Switch Transformers hoạt động tốt hơn ở các hệ số khả năng thấp hơn (1.0, 1.25). Khả năng chuyên gia nhỏ hơn là biểu thị của kịch bản trong chế độ mô hình lớn nơi bộ nhớ mô hình rất khan hiếm và hệ số khả năng sẽ muốn được làm nhỏ nhất có thể.

2.4 Kỹ thuật huấn luyện và tinh chỉnh cải tiến

Các mô hình chuyên gia thưa thớt có thể gây ra khó khăn huấn luyện so với Transformer vanilla. Tính không ổn định có thể xảy ra vì các quyết định hard-switching (định tuyến) tại mỗi lớp này. Hơn nữa, các định dạng độ chính xác thấp như bfloat16 (Wang và Kanwar, 2019) có thể làm trầm trọng thêm các vấn đề trong tính toán softmax cho router của chúng tôi. Chúng tôi mô tả các khó khăn huấn luyện ở đây và các phương pháp chúng tôi sử dụng để khắc phục chúng để đạt được huấn luyện ổn định và có thể scaling.

Độ chính xác chọn lọc với các mô hình thưa thớt lớn. Tính không ổn định mô hình cản trở khả năng huấn luyện sử dụng độ chính xác bfloat16 hiệu quả, và kết quả là, Lepikhin et al. (2020) huấn luyện với độ chính xác float32 trong suốt MoE Transformer của họ. Tuy nhiên, chúng tôi cho thấy rằng bằng cách thay vào đó chọn lọc cast đến độ chính xác float32 trong một phần địa phương của mô hình, tính ổn định có thể đạt được, mà không phải chịu chi phí giao tiếp đắt đỏ của tensors float32. Kỹ thuật này phù hợp với các chiến lược huấn luyện độ chính xác hỗn hợp hiện đại nơi các phần nhất định của mô hình và cập nhật gradient được thực hiện ở độ chính xác cao hơn Micikevicius et al. (2017). Bảng 2 cho thấy cách tiếp cận của chúng tôi cho phép tốc độ gần bằng với huấn luyện bfloat16 trong khi mang lại tính ổn định huấn luyện của float32.

Để đạt được điều này, chúng tôi cast đầu vào router đến độ chính xác float32. Hàm router lấy các token làm đầu vào và tạo ra các tensor dispatch và combine được sử dụng cho việc lựa chọn và tái kết hợp tính toán chuyên gia (tham khảo Code Block 15 trong Phụ lục để biết chi tiết). Quan trọng là, độ chính xác float32 chỉ được sử dụng trong thân của hàm router - trên các tính toán cục bộ cho thiết bị đó. Vì các tensor dispatch và combine kết quả được recast đến độ chính xác bfloat16 ở cuối hàm, không có tensor float32 đắt đỏ nào được broadcast qua các hoạt động giao tiếp all-to-all, nhưng chúng ta vẫn hưởng lợi từ tính ổn định tăng của float32.

[Bảng 2 về Selective precision cho thấy so sánh chất lượng và tốc độ]

Khởi tạo tham số nhỏ hơn cho tính ổn định. Khởi tạo thích hợp là quan trọng để huấn luyện thành công trong deep learning và chúng tôi đặc biệt quan sát điều này là đúng đối với Switch Transformer. Chúng tôi khởi tạo các ma trận trọng số của mình bằng cách rút các phần tử từ một phân phối chuẩn bị cắt với mean μ = 0 và độ lệch chuẩn σ = √(s/n) trong đó s là một siêu tham số scale và n là số lượng đơn vị đầu vào trong tensor trọng số (ví dụ fan-in).

Như một biện pháp khắc phục bổ sung cho tính không ổn định, chúng tôi khuyến nghị giảm scale khởi tạo Transformer mặc định s = 1.0 bằng một hệ số 10. Điều này cả cải thiện chất lượng và giảm khả năng huấn luyện không ổn định trong các thí nghiệm của chúng tôi. Bảng 3 đo sự cải thiện của chất lượng mô hình và giảm phương sai sớm trong huấn luyện. Chúng tôi thấy rằng chất lượng mô hình trung bình, được đo bằng Neg. Log Perp., được cải thiện đáng kể và có phương sai giảm đáng kể qua các lần chạy. Hơn nữa, cùng sơ đồ khởi tạo này có hiệu quả rộng rãi cho các mô hình kéo dài qua nhiều bậc độ lớn. Chúng tôi sử dụng cùng cách tiếp cận để huấn luyện ổn định các mô hình nhỏ như baseline 223M tham số của chúng tôi đến các mô hình khổng lồ vượt quá một nghìn tỷ tham số.

[Bảng 3 về Reduced initialization scale cải thiện stability]

Regularizing các mô hình thưa thớt lớn. Bài báo của chúng tôi xem xét cách tiếp cận NLP phổ biến là tiền huấn luyện trên một corpus lớn tiếp theo bởi tinh chỉnh trên các nhiệm vụ downstream nhỏ hơn như tóm tắt hoặc trả lời câu hỏi. Một vấn đề tự nhiên phát sinh là overfitting vì nhiều nhiệm vụ tinh chỉnh có rất ít ví dụ. Trong quá trình tinh chỉnh các Transformer tiêu chuẩn, Raffel et al. (2019) sử dụng dropout (Srivastava et al., 2014) tại mỗi lớp để ngăn chặn overfitting. Switch Transformers của chúng tôi có đáng kể nhiều tham số hơn so với baseline dày đặc được FLOP matched, có thể dẫn đến overfitting nghiêm trọng hơn trên những nhiệm vụ downstream nhỏ hơn này.

[Bảng 4 về Fine-tuning regularization results]

Do đó chúng tôi đề xuất một cách đơn giản để giảm thiểu vấn đề này trong quá trình tinh chỉnh: tăng dropout bên trong các chuyên gia, mà chúng tôi đặt tên là expert dropout. Trong quá trình tinh chỉnh, chúng tôi đơn giản tăng tỷ lệ dropout bằng một lượng đáng kể chỉ tại tính toán feed-forward tạm thời tại mỗi lớp chuyên gia. Bảng 4 có kết quả cho giao thức expert dropout của chúng tôi. Chúng tôi quan sát rằng đơn giản tăng dropout qua tất cả các lớp dẫn đến hiệu suất tồi tệ hơn. Tuy nhiên, đặt tỷ lệ dropout nhỏ hơn (0.1) tại các lớp không phải chuyên gia và tỷ lệ dropout lớn hơn nhiều (0.4) tại các lớp chuyên gia dẫn đến cải thiện hiệu suất trên bốn nhiệm vụ downstream nhỏ hơn.

3. Tính chất mở rộng quy mô

Chúng tôi trình bày một nghiên cứu về các tính chất scaling của kiến trúc Switch Transformer trong quá trình tiền huấn luyện. Theo Kaplan et al. (2020), chúng tôi xem xét một chế độ nơi mô hình không bị nghẽn cổ chai bởi ngân sách tính toán hoặc lượng dữ liệu. Để tránh nghẽn cổ chai dữ liệu, chúng tôi sử dụng corpus C4 lớn với hơn 180B target tokens (Raffel et al., 2019) và chúng tôi huấn luyện cho đến khi quan sát được diminishing returns.

Số lượng chuyên gia là chiều hiệu quả nhất để scaling mô hình của chúng tôi. Tăng các chuyên gia giữ chi phí tính toán gần như cố định vì mô hình chỉ chọn một chuyên gia mỗi token, bất kể số lượng chuyên gia để chọn từ đó. Router phải tính toán một phân phối xác suất trên nhiều chuyên gia hơn, tuy nhiên, đây là một tính toán nhẹ với chi phí O(d_model × num_experts) trong đó d_model là chiều embedding của tokens được truyền giữa các lớp. Trong phần này, chúng tôi xem xét các tính chất scaling trên cơ sở bước và cơ sở thời gian với ngân sách tính toán cố định.

3.1 Kết quả mở rộng quy mô trên cơ sở bước

Hình 4 cho thấy lợi ích scaling nhất quán với số lượng chuyên gia khi huấn luyện tất cả các mô hình cho một số bước cố định. Chúng tôi quan sát một xu hướng rõ ràng: khi giữ FLOPS mỗi token cố định, có nhiều tham số (chuyên gia) hơn tăng tốc huấn luyện. Hình bên trái cho thấy các tính chất scaling nhất quán (với FLOPS mỗi token cố định) giữa các tham số mô hình thưa thớt và test loss. Điều này tiết lộ lợi thế của scaling dọc theo trục bổ sung này của các tham số mô hình thưa thớt. Hình bên phải của chúng tôi đo hiệu quả mẫu của một biến thể mô hình dày đặc và bốn biến thể thưa thớt được FLOP-matched. Chúng tôi thấy rằng tăng số lượng chuyên gia dẫn đến các mô hình hiệu quả mẫu hơn. Mô hình Switch-Base 64 expert của chúng tôi đạt được cùng hiệu suất của mô hình T5-Base tại bước 60k tại bước 450k, đó là tăng tốc 7.5x về thời gian bước. Ngoài ra, phù hợp với các phát hiện của Kaplan et al. (2020), chúng tôi thấy rằng các mô hình lớn hơn cũng hiệu quả mẫu hơn - học nhanh hơn cho một số lượng tokens quan sát cố định.

[Hình 4: Scaling properties của Switch Transformer với hai plots về hiệu suất scaling]

3.2 Kết quả mở rộng quy mô trên cơ sở thời gian

Hình 4 cho thấy rằng trên cơ sở bước, khi chúng ta tăng số lượng chuyên gia, hiệu suất liên tục cải thiện. Trong khi các mô hình của chúng tôi có gần như cùng lượng FLOPS mỗi token như baseline, Switch Transformers của chúng tôi phải chịu chi phí giao tiếp bổ sung qua các thiết bị cũng như tính toán bổ sung của cơ chế định tuyến. Do đó, hiệu quả mẫu tăng quan sát trên cơ sở bước không nhất thiết dịch sang chất lượng mô hình tốt hơn được đo bằng wall-clock. Điều này đặt ra câu hỏi:

Đối với thời gian huấn luyện cố định và ngân sách tính toán, người ta nên huấn luyện một mô hình dày đặc hay thưa thớt?

[Hình 5: Speed advantage của Switch Transformer cho thấy 7x speedup]

Hình 5 và 6 giải quyết câu hỏi này. Hình 5 đo chất lượng mô hình tiền huấn luyện như một hàm của thời gian. Đối với thời gian huấn luyện cố định và ngân sách tính toán, Switch Transformers mang lại tăng tốc đáng kể. Trong cài đặt này, mô hình Switch-Base 64 expert của chúng tôi huấn luyện trong một phần bảy thời gian mà T5-Base sẽ cần để có perplexity tương tự.

3.3 Mở rộng quy mô so với mô hình Dense lớn hơn

Phân tích trên cho thấy rằng một mô hình dày đặc được matched tính toán bị vượt qua bởi đối tác Switch của nó. Hình 6 xem xét một kịch bản khác: điều gì nếu chúng ta thay vào đó đã phân bổ tài nguyên của mình cho một mô hình dày đặc lớn hơn? Chúng tôi làm điều đó bây giờ, đo Switch-Base so với baseline mạnh tiếp theo, T5-Large. Nhưng mặc dù T5-Large áp dụng 3.5x FLOPS mỗi token hơn, Switch-Base vẫn hiệu quả mẫu hơn và mang lại tăng tốc 2.5x. Hơn nữa, nhiều lợi ích hơn có thể có được đơn giản bằng cách thiết kế một phiên bản thưa thớt mới, lớn hơn, Switch-Large, được FLOP-matched với T5-Large. Chúng tôi làm điều này và cho thấy scaling và tinh chỉnh vượt trội trong phần tiếp theo.

[Hình 6: Scaling Transformer models với Switch layers hoặc với standard dense model scaling]

4. Kết quả Downstream

Phần 3 đã chứng minh các tính chất scaling vượt trội trong khi tiền huấn luyện, nhưng bây giờ chúng tôi xác thực rằng những lợi ích này dịch sang khả năng học ngôn ngữ cải thiện trên các nhiệm vụ downstream. Chúng tôi bắt đầu bằng tinh chỉnh trên một tập đa dạng các nhiệm vụ NLP. Tiếp theo chúng tôi nghiên cứu giảm dấu chân bộ nhớ của các mô hình thưa thớt của chúng tôi hơn 90% bằng cách chưng cất thành các baseline dày đặc nhỏ - và dễ dàng triển khai. Cuối cùng, chúng tôi kết thúc phần này bằng việc đo các cải tiến trong cài đặt đa nhiệm vụ, đa ngôn ngữ, nơi chúng tôi cho thấy rằng Switch Transformers là những người học đa nhiệm vụ mạnh mẽ, cải thiện so với mô hình T5-base đa ngôn ngữ trên tất cả 101 ngôn ngữ.

4.1 Tinh chỉnh

Baseline và các mô hình Switch được sử dụng để tinh chỉnh. Baseline của chúng tôi là mô hình T5-Base 223M tham số được điều chỉnh cao và mô hình T5-Large 739M tham số (Raffel et al., 2019). Đối với cả hai phiên bản, chúng tôi thiết kế một Switch Transformer được FLOP-matched, với nhiều tham số hơn, được tóm tắt trong Bảng 9. Baseline của chúng tôi hơi khác so với những trong Raffel et al. (2019) vì chúng tôi tiền huấn luyện trên một corpus C4 cải tiến loại bỏ trùng lặp văn bản intra-example và do đó tăng hiệu quả như một nhiệm vụ tiền huấn luyện Lee et al. (2021). Trong giao thức của chúng tôi, chúng tôi tiền huấn luyện với 2^20 (1,048,576) tokens mỗi batch cho 550k bước tổng cộng 576B tokens. Sau đó chúng tôi tinh chỉnh trên một tập đa dạng các nhiệm vụ sử dụng tỷ lệ dropout 0.1 cho tất cả các lớp trừ các lớp Switch, sử dụng tỷ lệ dropout 0.4 (xem Bảng 4). Chúng tôi tinh chỉnh sử dụng batch-size 1M cho 16k bước và đối với mỗi nhiệm vụ, chúng tôi đánh giá chất lượng mô hình mỗi 200 bước và báo cáo hiệu suất đỉnh được tính trên tập validation.

Nhiệm vụ và tập dữ liệu tinh chỉnh. Chúng tôi chọn các nhiệm vụ thăm dò khả năng ngôn ngữ bao gồm trả lời câu hỏi, tóm tắt và kiến thức về thế giới. Các benchmark ngôn ngữ GLUE (Wang et al., 2018) và SuperGLUE (Wang et al., 2019) được xử lý như hỗn hợp composite với tất cả các nhiệm vụ được trộn theo tỷ lệ với lượng tokens có trong mỗi cái. Những benchmark này bao gồm các nhiệm vụ đòi hỏi phân tích sentiment (SST-2), phân biệt nghĩa từ (WIC), similarity câu (MRPC, STS-B, QQP), suy luận ngôn ngữ tự nhiên (MNLI, QNLI, RTE, CB), trả lời câu hỏi (MultiRC, RECORD, BoolQ), phân giải coreference (WNLI, WSC) và hoàn thành câu (COPA) và acceptability câu (CoLA). Các tập dữ liệu CNNDM (Hermann et al., 2015) và BBC XSum (Narayan et al., 2018) được sử dụng để đo khả năng tóm tắt bài báo. Trả lời câu hỏi được thăm dò với tập dữ liệu SQuAD (Rajpurkar et al., 2016) và ARC Reasoning Challenge (Clark et al., 2018). Và như trong Roberts et al. (2020), chúng tôi đánh giá kiến thức của các mô hình của chúng tôi bằng cách tinh chỉnh trên ba tập dữ liệu trả lời câu hỏi closed-book: Natural Questions (Kwiatkowski et al., 2019), Web Questions (Berant et al., 2013) và Trivia QA (Joshi et al., 2017). Closed-book đề cập đến các câu hỏi được đặt ra mà không có tài liệu tham khảo hoặc ngữ cảnh bổ sung. Để đánh giá lý luận common sense của mô hình, chúng tôi đánh giá nó trên Winogrande Schema Challenge (Sakaguchi et al., 2020). Và cuối cùng, chúng tôi kiểm tra khả năng suy luận ngôn ngữ tự nhiên của mô hình trên Adversarial NLI Benchmark (Nie et al., 2019).

Metrics tinh chỉnh. Các metric đánh giá sau được sử dụng trong suốt bài báo: Chúng tôi báo cáo điểm trung bình trên tất cả các subtask cho GLUE và SuperGLUE. Metric Rouge-2 được sử dụng cho cả CNNDM và XSum. Trong SQuAD và các nhiệm vụ closed book (Web, Natural, và Trivia Questions) chúng tôi báo cáo phần trăm câu trả lời khớp chính xác với target (tham khảo Roberts et al. (2020) để biết thêm chi tiết và thiếu sót của phép đo này). Cuối cùng, trong ARC Easy, ARC Challenge, ANLI, và Winogrande chúng tôi báo cáo độ chính xác của các phản hồi được tạo ra.

Kết quả tinh chỉnh. Chúng tôi quan sát những cải thiện downstream đáng kể trên nhiều nhiệm vụ ngôn ngữ tự nhiên. Những cải thiện đáng chú ý đến từ SuperGLUE, nơi chúng tôi thấy các biến thể Switch được FLOP-matched cải thiện 4.4 và 2 điểm phần trăm so với baseline T5-Base và T5-Large, tương ứng cũng như cải thiện lớn trong Winogrande, closed book Trivia QA, và XSum. Trong nghiên cứu tinh chỉnh của chúng tôi, các nhiệm vụ duy nhất nơi chúng tôi không quan sát được lợi ích là trên các tập dữ liệu AI2 Reasoning Challenge (ARC) nơi T5-Base vượt trội Switch-Base trên tập dữ liệu challenge và T5-Large vượt trội Switch-Large trên tập dữ liệu easy. Được lấy như một tổng thể, chúng tôi quan sát những cải thiện đáng kể trải dài cả nhiệm vụ lý luận và kiến thức nặng. Điều này xác thực kiến trúc của chúng tôi, không chỉ như một cái tiền huấn luyện tốt, mà có thể dịch những cải thiện chất lượng sang các nhiệm vụ downstream qua tinh chỉnh.

[Bảng 5: Fine-tuning results cho thấy so sánh giữa T5 baselines và Switch models]

4.2 Chưng cất

Triển khai các mạng thần kinh khổng lồ với hàng tỷ, hoặc nghìn tỷ, tham số là bất tiện. Để giảm thiểu điều này, chúng tôi nghiên cứu chưng cất (Hinton et al., 2015) các mô hình thưa thớt lớn thành các mô hình dày đặc nhỏ. Công việc tương lai có thể bổ sung nghiên cứu chưng cất các mô hình lớn thành các mô hình thưa thớt nhỏ hơn.

Kỹ thuật chưng cất. Trong Bảng 6 chúng tôi nghiên cứu nhiều kỹ thuật chưng cất. Những kỹ thuật này được xây dựng từ Sanh et al. (2019), những người nghiên cứu các phương pháp chưng cất cho các mô hình BERT. Chúng tôi thấy rằng khởi tạo mô hình dày đặc với các trọng số không phải chuyên gia mang lại một cải thiện khiêm tốn. Điều này có thể vì tất cả các mô hình được FLOP matched, vì vậy các lớp không phải chuyên gia sẽ có cùng chiều. Vì các lớp chuyên gia thường chỉ được thêm vào mỗi hoặc mỗi lớp FFN khác trong một Transformer, điều này cho phép nhiều trọng số được khởi tạo với các tham số đã huấn luyện. Hơn nữa, chúng tôi quan sát một cải thiện chưng cất sử dụng hỗn hợp 0.25 cho xác suất giáo viên và 0.75 cho nhãn ground truth. Bằng cách kết hợp cả hai kỹ thuật, chúng tôi bảo tồn 30% lợi ích chất lượng từ các mô hình thưa thớt lớn hơn với chỉ 1/20 số tham số. Lợi ích chất lượng đề cập đến phần trăm của sự khác biệt chất lượng giữa Switch-Base (Giáo viên) và T5-Base (Học sinh). Do đó, lợi ích chất lượng 100% có nghĩa là Học sinh bằng hiệu suất của Giáo viên.

[Bảng 6: Distilling Switch Transformers for Language Modeling]

Tỷ lệ nén có thể đạt được. Sử dụng kỹ thuật chưng cất tốt nhất của chúng tôi được mô tả trong Bảng 6, chúng tôi chưng cất một variety wide của các mô hình thưa thớt thành các mô hình dày đặc. Chúng tôi chưng cất các phiên bản Switch-Base, quét qua số lượng chuyên gia tăng, tương ứng với việc thay đổi giữa 1.1B đến 14.7B tham số. Thông qua chưng cất, chúng tôi có thể bảo tồn 37% lợi ích chất lượng của mô hình 1.1B tham số trong khi nén 82%. Ở mức cực đoan, nơi chúng tôi nén mô hình 99%, chúng tôi vẫn có thể duy trì 28% cải thiện chất lượng của giáo viên.

Chưng cất một mô hình được tinh chỉnh. Chúng tôi kết thúc điều này với một nghiên cứu về chưng cất một mô hình thưa thớt được tinh chỉnh thành một mô hình dày đặc. Bảng 8 cho thấy kết quả chưng cất một mô hình Switch-Base 7.4B tham số, được tinh chỉnh trên nhiệm vụ SuperGLUE, thành T5-Base 223M. Tương tự như kết quả tiền huấn luyện của chúng tôi, chúng tôi thấy chúng tôi có thể bảo tồn 30% lợi ích của mô hình thưa thớt khi chưng cất thành một biến thể dày đặc được FLOP matched. Một con đường tương lai tiềm năng, không được xem xét ở đây, có thể kiểm tra các chuyên gia cụ thể được sử dụng cho các nhiệm vụ tinh chỉnh và trích xuất chúng để đạt được nén mô hình tốt hơn.

[Bảng 7 và 8 về Distillation compression rates và Distilling a fine-tuned SuperGLUE model]

4.3 Học đa ngôn ngữ

Trong tập thí nghiệm downstream cuối cùng của chúng tôi, chúng tôi đo các đánh đổi chất lượng mô hình và tốc độ trong khi tiền huấn luyện trên hỗn hợp 101 ngôn ngữ khác nhau. Chúng tôi xây dựng và benchmark từ công việc gần đây của mT5 (Xue et al., 2020), một phần mở rộng đa ngôn ngữ cho T5. Chúng tôi tiền huấn luyện trên biến thể đa ngôn ngữ của tập dữ liệu Common Crawl (mC4) trải dài 101 ngôn ngữ được giới thiệu trong mT5, nhưng do các biến thể script trong một số ngôn ngữ nhất định, hỗn hợp chứa 107 nhiệm vụ.

Trong Hình 7 chúng tôi vẽ cải thiện chất lượng trong negative log perplexity cho tất cả ngôn ngữ của một mô hình Switch được FLOP-matched, mSwitch-Base với biến thể T5 base, mT5-Base. Sau khi tiền huấn luyện cả hai phiên bản cho 1M bước, chúng tôi thấy rằng trên tất cả 101 ngôn ngữ được xem xét, Switch Transformer tăng negative log perplexity cuối cùng so với baseline. Trong Hình 8, chúng tôi trình bày một view khác và bây giờ histogram tăng tốc mỗi bước của việc sử dụng Switch Transformer so với mT5-Base. Chúng tôi thấy tăng tốc trung bình so với mT5-Base là 5x và 91% ngôn ngữ đạt được ít nhất tăng tốc 4x. Điều này trình bày bằng chứng rằng Switch Transformers là những người học đa nhiệm vụ và đa ngôn ngữ hiệu quả.

[Hình 7 và 8: Multilingual pre-training on 101 languages]

5. Thiết kế mô hình với Song song dữ liệu, mô hình và chuyên gia

Việc tùy ý tăng số lượng chuyên gia có diminishing returns (Hình 4). Ở đây chúng tôi mô tả các chiến lược scaling bổ sung. Cách phổ biến để scale một Transformer là tăng các chiều song song, như d_model hoặc d_ff. Điều này tăng cả tham số và tính toán được thực hiện và cuối cùng bị giới hạn bởi bộ nhớ mỗi accelerator. Một khi nó vượt quá kích thước bộ nhớ của accelerator, model-parallelism single program multiple data (SPMD) có thể được sử dụng. Phần này nghiên cứu các đánh đổi của việc kết hợp song song dữ liệu, mô hình và chuyên gia.

Xem xét lại lớp Feed-Forward Network (FFN). Chúng tôi sử dụng lớp FFN như một ví dụ về cách song song dữ liệu, mô hình và chuyên gia hoạt động trong Mesh TensorFlow (Shazeer et al., 2018) và xem xét nó một cách ngắn gọn ở đây. Chúng tôi giả định B tokens trong batch, mỗi cái có chiều d_model. Cả đầu vào (x) và đầu ra (y) của FFN có kích thước [B, d_model] và trung gian (h) có kích thước [B, d_ff] trong đó d_ff thường lớn hơn d_model nhiều lần. Trong FFN, trung gian là h = xW_in và sau đó đầu ra của lớp là y = ReLU(h)W_out. Do đó W_in và W_out được áp dụng độc lập cho mỗi token và có kích thước [d_model, d_ff] và [d_ff, d_model].

Chúng tôi mô tả hai khía cạnh của phân vùng: cách các trọng số và batch dữ liệu chia qua các lõi, được mô tả trong Hình 9. Chúng tôi ký hiệu tất cả các lõi có sẵn là N mà Mesh TensorFlow sau đó có thể remap thành một lưới logic đa chiều của các bộ xử lý. Ở đây chúng tôi tạo một lưới logic hai chiều, với một chiều đại diện cho số cách cho data-parallel sharding (n) và cái khác, model-parallel sharding (m). Tổng lõi phải bằng các cách để shard qua cả song song dữ liệu và mô hình, ví dụ N = n × m.

Để shard lớp qua các lõi, các tensor chứa batch B tokens đó được shard qua n lõi song song dữ liệu, vì vậy mỗi lõi chứa B/n tokens. Tensors và variables với d_ff sau đó được shard qua m lõi song song mô hình. Đối với các biến thể với các lớp chuyên gia, chúng tôi xem xét E chuyên gia, mỗi cái có thể xử lý lên đến C tokens.

[Bảng thuật ngữ với các định nghĩa]

5.1 Song song dữ liệu

Khi huấn luyện các mô hình song song dữ liệu, đây là tiêu chuẩn cho huấn luyện phân tán, thì tất cả các lõi được phân bổ cho chiều song song dữ liệu hoặc n = N; m = 1. Điều này có lợi thế là không cần giao tiếp cho đến khi toàn bộ forward và backward pass hoàn thành và gradients cần được tổng hợp qua tất cả các lõi. Điều này tương ứng với cột ngoài cùng bên trái của Hình 9.

5.2 Song song mô hình

Bây giờ chúng tôi xem xét một kịch bản nơi tất cả các lõi được phân bổ độc quyền cho chiều song song mô hình và vì vậy n = 1; m = N. Bây giờ tất cả các lõi phải giữ toàn bộ B tokens và mỗi lõi sẽ chứa một slice duy nhất của các trọng số. Đối với mỗi forward và backward pass, một chi phí giao tiếp bây giờ được phát sinh. Mỗi lõi gửi một tensor [B, d_model] để tính toán phép nhân ma trận thứ hai ReLU(h)W_out vì chiều d_ff được phân vùng và phải được tổng lại. Như một quy tắc chung, bất cứ khi nào một chiều được phân vùng qua các lõi phải được tổng, thì một hoạt động all-reduce được thêm vào cho cả forward và backward pass. Điều này trái ngược với song song dữ liệu thuần túy nơi all-reduce chỉ xảy ra ở cuối toàn bộ forward và backward pass.

[Hình 9: Data and weight partitioning strategies]

5.3 Song song mô hình và dữ liệu

Việc trộn cả song song mô hình và dữ liệu cho các mô hình quy mô lớn là phổ biến, đã được thực hiện trong các mô hình T5 lớn nhất (Raffel et al., 2019; Xue et al., 2020) và trong GPT-3 (Brown et al., 2020). Với tổng N = n × m lõi, bây giờ mỗi lõi sẽ chịu trách nhiệm cho B/n tokens và d_ff/m của cả trọng số và activation trung gian. Trong forward và backward pass mỗi lõi giao tiếp một tensor kích thước [B/n, d_model] trong một hoạt động all-reduce.

5.4 Song song chuyên gia và dữ liệu

Tiếp theo chúng tôi mô tả chiến lược phân vùng cho song song chuyên gia và dữ liệu. Switch Transformers sẽ phân bổ tất cả các lõi của chúng cho chiều phân vùng dữ liệu n, cũng sẽ tương ứng với số lượng chuyên gia trong mô hình. Đối với mỗi token mỗi lõi, một router cục bộ tính toán assignments cho các chuyên gia. Đầu ra là một ma trận binary kích thước [n, B/n, E, C] được phân vùng qua chiều đầu tiên và xác định assignment chuyên gia. Ma trận binary này sau đó được sử dụng để làm gather qua phép nhân ma trận với tensor đầu vào [n, B/n, d_model].

einsum([n, B/n, d_model], [n, B/n, E, C], dimension=[B/n]) (7)

dẫn đến tensor cuối cùng có hình dạng [n, E, C, d_model], được shard qua chiều đầu tiên. Vì mỗi lõi có chuyên gia riêng của nó, chúng tôi thực hiện giao tiếp all-to-all kích thước [E, C, d_model] để bây giờ shard chiều E thay vì chiều n. Có các chi phí giao tiếp bổ sung của tensors bfloat16 kích thước ECd_model trong forward pass để tương tự nhận tokens từ mỗi chuyên gia nằm trên các lõi khác nhau. Xem Phụ lục F để có phân tích chi tiết về mã phân vùng chuyên gia.

5.5 Song song chuyên gia, mô hình và dữ liệu

Trong thiết kế mô hình tốt nhất của chúng tôi, chúng tôi tìm cách cân bằng FLOPS mỗi token và số lượng tham số. Khi chúng tôi scale số lượng chuyên gia, chúng tôi tăng số lượng tham số, nhưng không thay đổi FLOPS mỗi token. Để tăng FLOPS, chúng tôi cũng phải tăng chiều d_ff (cũng tăng tham số, nhưng với tốc độ chậm hơn). Điều này trình bày một sự đánh đổi: khi chúng tôi tăng d_ff chúng tôi sẽ hết bộ nhớ mỗi lõi, sau đó cần thiết việc tăng m. Nhưng vì chúng tôi có số lõi cố định N, và N = n × m, chúng tôi phải giảm n, buộc sử dụng batch-size nhỏ hơn (để giữ tokens mỗi lõi không đổi).

Khi kết hợp cả song song mô hình và chuyên gia, chúng tôi sẽ có chi phí giao tiếp all-to-all từ định tuyến tokens đến các chuyên gia đúng cùng với các giao tiếp all-reduce bên trong từ song song mô hình. Cân bằng FLOPS, chi phí giao tiếp và bộ nhớ mỗi lõi trở nên khá phức tạp khi kết hợp cả ba phương pháp nơi mapping tốt nhất được xác định thực nghiệm. Xem phân tích thêm của chúng tôi trong phần 5.6 về cách số lượng chuyên gia ảnh hưởng đến hiệu suất downstream cũng như.

5.6 Hướng tới các mô hình nghìn tỷ tham số

Kết hợp song song chuyên gia, mô hình và dữ liệu, chúng tôi thiết kế hai mô hình Switch Transformer lớn, một với 395 tỷ và 1.6 nghìn tỷ tham số, tương ứng. Chúng tôi nghiên cứu cách các mô hình này hoạt động trên cả tiền huấn luyện upstream như các mô hình ngôn ngữ và hiệu suất tinh chỉnh downstream của chúng. Các tham số, FLOPS mỗi sequence và siêu tham số của hai mô hình khác nhau được liệt kê dưới đây trong Bảng 9. Các siêu tham số tiêu chuẩn của Transformer, bao gồm d_model, d_ff, d_kv, số heads và số lớp được mô tả, cũng như một tính năng ít phổ biến hơn, FFN GEGLU, đề cập đến một biến thể của lớp FFN nơi ma trận mở rộng được thay thế bằng hai tập trọng số được kết hợp phi tuyến (Shazeer, 2020).

Mô hình Switch-C được thiết kế chỉ sử dụng song song chuyên gia, và không có song song mô hình, như mô tả trước đó trong Phần 5.4. Kết quả là, các siêu tham số kiểm soát chiều rộng, chiều sâu, số heads, và như vậy, đều nhỏ hơn nhiều so với mô hình T5-XXL. Ngược lại, Switch-XXL được FLOP-matched với mô hình T5-XXL, cho phép các chiều lớn hơn của các siêu tham số, nhưng với chi phí của các chi phí giao tiếp bổ sung được gây ra bởi song song mô hình (xem Phần 5.5 để biết thêm chi tiết).

[Bảng 9: Switch model design and pre-training performance]

Hiệu quả mẫu so với T5-XXL. Trong hai cột cuối cùng của Bảng 9 chúng tôi ghi lại negative log perplexity trên corpus C4 sau 250k và 500k bước, tương ứng. Sau 250k bước, chúng tôi thấy cả hai biến thể Switch Transformer cải thiện so với negative log perplexity của phiên bản T5-XXL hơn 0.061. Để contextualize tầm quan trọng của một gap 0.061, chúng tôi lưu ý rằng mô hình T5-XXL phải huấn luyện thêm 250k bước để tăng 0.052. Gap tiếp tục tăng với huấn luyện bổ sung, với mô hình Switch-XXL vượt trội T5-XXL 0.087 vào 500k bước.

Tính không ổn định huấn luyện. Tuy nhiên, như mô tả trong phần giới thiệu, các mô hình thưa thớt lớn có thể không ổn định, và khi chúng tôi tăng quy mô, chúng tôi gặp phải một số vấn đề thể thức. Chúng tôi thấy rằng mô hình Switch-C lớn hơn, với 1.6T tham số và 2048 chuyên gia, không thể hiện tính không ổn định huấn luyện nào cả. Thay vào đó, phiên bản Switch XXL, với FLOPS mỗi sequence lớn hơn gần 10x, đôi khi không ổn định. Kết quả là, mặc dù đây là mô hình tốt hơn của chúng tôi trên cơ sở bước, chúng tôi không tiền huấn luyện đầy đủ 1M bước, phù hợp với các kết quả cuối cùng được báo cáo của T5 (Raffel et al., 2019).

Hiệu suất tinh chỉnh lý luận. Như một đánh giá sơ bộ về chất lượng mô hình, chúng tôi sử dụng một mô hình Switch-XXL được tiền huấn luyện một phần trên 503B tokens, hoặc khoảng một nửa văn bản được sử dụng bởi mô hình T5-XXL. Sử dụng checkpoint này, chúng tôi tiến hành huấn luyện đa nhiệm vụ cho hiệu quả, nơi tất cả các nhiệm vụ được học cùng nhau, thay vì được tinh chỉnh cá nhân. Chúng tôi thấy rằng độ chính xác SQuAD trên tập validation tăng lên 89.7 so với state-of-the-art là 91.3. Tiếp theo, điểm test SuperGLUE trung bình được ghi lại ở 87.5 so với phiên bản T5 đạt được điểm 89.3 so với state-of-the-art là 90.0 (Wang et al., 2019). Trên ANLI (Nie et al., 2019), Switch XXL cải thiện so với state-of-the-art trước đó để có độ chính xác 65.7 so với tốt nhất trước đó là 49.4 (Yang et al., 2020). Chúng tôi lưu ý rằng trong khi Switch-XXL có state-of-the-art Neg. Log Perp. trên nhiệm vụ tiền huấn luyện upstream, các lợi ích của nó chưa hoàn toàn dịch sang hiệu suất downstream SOTA. Chúng tôi nghiên cứu vấn đề này thêm trong Phụ lục E.

Hiệu suất tinh chỉnh dựa trên kiến thức. Cuối cùng, chúng tôi cũng tiến hành một kiểm tra sớm về kiến thức của mô hình với ba nhiệm vụ dựa trên kiến thức closed-book: Natural Questions, WebQuestions và TriviaQA, mà không có tiền huấn luyện bổ sung sử dụng Salient Span Masking (Guu et al., 2020). Trong cả ba trường hợp, chúng tôi quan sát những cải thiện so với mô hình T5-XXL state-of-the-art trước đó (không có SSM). Natural Questions exact match tăng lên 34.4 so với tốt nhất trước đó là 32.8, Web Questions tăng lên 41.0 so với 37.2, và TriviaQA tăng lên 47.5 so với 42.9.

Tóm lại, mặc dù huấn luyện trên ít hơn một nửa dữ liệu của các mô hình khác, chúng tôi đã thấy chất lượng mô hình có thể so sánh, và đôi khi state-of-the-art. Hiện tại, Switch Transformer dịch những lợi ích upstream đáng kể tốt hơn sang các nhiệm vụ dựa trên kiến thức, hơn là các nhiệm vụ lý luận (xem Phụ lục E). Trích xuất hiệu suất tinh chỉnh mạnh hơn từ các mô hình chuyên gia lớn là một câu hỏi nghiên cứu tích cực, và perplexity tiền huấn luyện chỉ ra những cải thiện tương lai nên có thể.

6. Công việc liên quan

Tầm quan trọng của quy mô trong các mạng thần kinh được công nhận rộng rãi và một số cách tiếp cận đã được đề xuất. Các công việc gần đây đã scale các mô hình đến hàng tỷ tham số thông qua việc sử dụng song song mô hình (ví dụ chia các trọng số và tensor qua nhiều lõi) (Shazeer et al., 2018; Rajbhandari et al., 2019; Raffel et al., 2019; Brown et al., 2020; Shoeybi et al., 2019). Thay vào đó, Harlap et al. (2018); Huang et al. (2019) đề xuất sử dụng song song mô hình dựa trên pipeline, nơi các lớp khác nhau được chia qua các thiết bị và micro-batches được pipeline đến các lớp khác nhau. Cuối cùng, Product Key networks (Lample et al., 2019) được đề xuất để scale up khả năng của các mạng thần kinh bằng cách lookup các embeddings có thể học được dựa trên biểu diễn token đến một lớp nhất định.

Công việc của chúng tôi nghiên cứu một mô hình cụ thể trong một lớp các phương pháp thực hiện tính toán có điều kiện, nơi các quyết định tính toán được đưa ra động dựa trên đầu vào. Cho và Bengio (2014) đề xuất việc chọn trọng số một cách thích nghi dựa trên các bit patterns nhất định xảy ra trong hidden-states của mô hình. Eigen et al. (2013) xây dựng các lớp chuyên gia xếp chồng với phép nhân ma trận dày đặc và activations ReLU và cho thấy kết quả hứa hẹn trên jittered MNIST và monotone speech. Trong computer vision Puigcerver et al. (2020) manually route các token dựa trên các lớp semantic trong quá trình tiền huấn luyện upstream và sau đó chọn các chuyên gia liên quan để được sử dụng theo nhiệm vụ downstream.

Mixture of Experts (MoE), trong bối cảnh các kiến trúc deep learning hiện đại, được chứng minh hiệu quả trong Shazeer et al. (2017). Công việc đó thêm một lớp MoE được xếp chồng giữa các lớp LSTM (Hochreiter và Schmidhuber, 1997), và các token được định tuyến riêng biệt đến các tổ hợp chuyên gia. Điều này dẫn đến kết quả state-of-the-art trong các benchmark modeling ngôn ngữ và dịch máy. Lớp MoE được giới thiệu lại vào kiến trúc Transformer bởi thư viện Mesh TensorFlow (Shazeer et al., 2018) nơi các lớp MoE được giới thiệu như một thay thế của các lớp FFN, tuy nhiên, không có kết quả NLP đi kèm. Gần đây hơn, thông qua những tiến bộ trong cơ sở hạ tầng machine learning, GShard (Lepikhin et al., 2020), mở rộng compiler XLA, sử dụng MoE Transformer để cải thiện đáng kể dịch máy qua 100 ngôn ngữ. Cuối cùng Fan et al. (2021) chọn một chiến lược MoE deterministic khác để chia các tham số mô hình thành các nhóm ngôn ngữ không overlap.

Sparsity dọc theo chiều độ dài sequence (L) trong các patterns attention Transformer đã là một kỹ thuật thành công để giảm độ phức tạp attention từ O(L^2) (Child et al., 2019; Correia et al., 2019; Sukhbaatar et al., 2019; Kitaev et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020). Điều này đã cho phép học các sequence dài hơn so với trước đó có thể. Phiên bản này của Switch Transformer không sử dụng attention sparsity, nhưng những kỹ thuật này là bổ sung, và, như công việc tương lai, chúng có thể được kết hợp để có thể cải thiện việc học trên các nhiệm vụ đòi hỏi ngữ cảnh dài.

7. Thảo luận

Chúng tôi đặt ra và thảo luận các câu hỏi về Switch Transformer, và các mô hình chuyên gia thưa thớt nói chung, nơi sparsity đề cập đến trọng số, không phải trên patterns attention.

Switch Transformer có tốt hơn không chỉ vì số lượng tham số thuần túy? Có, và theo thiết kế! Tham số, độc lập với tổng FLOPS được sử dụng, là một trục hữu ích để scale các mô hình ngôn ngữ thần kinh. Các mô hình lớn đã được chỉ ra toàn diện để hoạt động tốt hơn (Kaplan et al., 2020). Nhưng trong trường hợp này, mô hình của chúng tôi hiệu quả mẫu hơn và nhanh hơn trong khi sử dụng cùng tài nguyên tính toán.

Tôi không có quyền truy cập vào siêu máy tính - điều này vẫn hữu ích cho tôi không? Mặc dù công việc này tập trung vào các mô hình cực lớn, chúng tôi cũng thấy rằng các mô hình với ít nhất hai chuyên gia cải thiện hiệu suất trong khi dễ dàng phù hợp trong các ràng buộc bộ nhớ của GPU hoặc TPU thường có sẵn (chi tiết trong Phụ lục D). Do đó chúng tôi tin rằng các kỹ thuật của chúng tôi hữu ích trong cài đặt quy mô nhỏ.

Các mô hình thưa thớt có vượt trội các mô hình dày đặc trên đường cong Pareto tốc độ-độ chính xác không? Có. Qua một variety wide của các kích thước mô hình khác nhau, các mô hình thưa thớt vượt trội các mô hình dày đặc mỗi bước và trên thời gian wall clock. Các thí nghiệm kiểm soát của chúng tôi cho thấy với một lượng tính toán và thời gian cố định, các mô hình thưa thớt vượt trội các mô hình dày đặc.

Tôi không thể triển khai một mô hình nghìn tỷ tham số - chúng ta có thể thu nhỏ những mô hình này không? Chúng tôi không thể bảo tồn hoàn toàn chất lượng mô hình, nhưng tỷ lệ nén 10 đến 100x có thể đạt được bằng cách chưng cất các mô hình thưa thớt của chúng tôi thành các mô hình dày đặc trong khi đạt được 30% lợi ích chất lượng của mô hình chuyên gia.

Tại sao sử dụng Switch Transformer thay vì một mô hình dày đặc song song mô hình? Trên cơ sở thời gian, Switch Transformers có thể hiệu quả hơn nhiều so với các mô hình dày đặc với các tham số được shard (Hình 6). Ngoài ra, chúng tôi chỉ ra rằng quyết định này không loại trừ lẫn nhau - chúng tôi có thể, và làm, sử dụng song song mô hình trong Switch Transformers, tăng FLOPS mỗi token, nhưng phải chịu sự chậm lại của song song mô hình thông thường.

Tại sao các mô hình thưa thớt không được sử dụng rộng rãi rồi? Động lực để thử các mô hình thưa thớt đã bị cản trở bởi thành công lớn của việc scaling các mô hình dày đặc (thành công của điều này được thúc đẩy một phần bởi co-adaptation với phần cứng deep learning như được lập luận trong Hooker (2020)). Hơn nữa, các mô hình thưa thớt đã phải chịu nhiều vấn đề bao gồm (1) độ phức tạp mô hình, (2) khó khăn huấn luyện, và (3) chi phí giao tiếp. Switch Transformer thực hiện bước tiến để giảm thiểu những vấn đề này.

8. Công việc tương lai

Bài báo này đặt ra một kiến trúc đơn giản hóa, các thủ tục huấn luyện cải tiến, và một nghiên cứu về cách các mô hình thưa thớt scale. Tuy nhiên, vẫn còn nhiều hướng tương lai mở mà chúng tôi mô tả ngắn gọn ở đây:

1. Một thách thức đáng kể là cải thiện thêm tính ổn định huấn luyện cho các mô hình lớn nhất. Trong khi các kỹ thuật ổn định của chúng tôi hiệu quả cho các mô hình Switch-Base, Switch-Large và Switch-C của chúng tôi (không quan sát tính không ổn định), chúng không đủ cho Switch-XXL. Chúng tôi đã thực hiện những bước đầu hướng tới việc ổn định những mô hình này, mà chúng tôi nghĩ có thể hữu ích chung cho các mô hình lớn, bao gồm sử dụng các regularizers để cải thiện tính ổn định và các hình thức thích nghi của gradient clipping, nhưng điều này vẫn chưa được giải quyết.

2. Nói chung chúng tôi thấy rằng chất lượng tiền huấn luyện cải thiện dẫn đến kết quả downstream tốt hơn (Phụ lục E), mặc dù chúng tôi đôi khi gặp các anomalies nổi bật. Ví dụ, mặc dù perplexities tương tự khi modeling tập dữ liệu C4, mô hình Switch-C 1.6T tham số chỉ đạt được điểm exact match 87.7 trong SQuAD, so sánh không thuận lợi với 89.6 cho mô hình Switch-XXL nhỏ hơn. Một sự khác biệt đáng chú ý là mô hình Switch-XXL áp dụng 10x FLOPS mỗi token so với mô hình Switch-C, mặc dù nó có 4x ít tham số duy nhất hơn (395B vs 1.6T). Điều này gợi ý một sự phụ thuộc được hiểu kém giữa chất lượng tinh chỉnh, FLOPS mỗi token và số lượng tham số.

3. Thực hiện một nghiên cứu toàn diện về các mối quan hệ scaling để hướng dẫn thiết kế các kiến trúc pha trộn song song dữ liệu, mô hình và chuyên gia. Lý tưởng, cho các thông số kỹ thuật của một cấu hình phần cứng (tính toán, bộ nhớ, giao tiếp) người ta có thể thiết kế nhanh hơn một mô hình tối ưu. Và, ngược lại, điều này cũng có thể giúp trong thiết kế phần cứng tương lai.

4. Công việc của chúng tôi nằm trong gia đình các thuật toán tính toán thích nghi. Cách tiếp cận của chúng tôi luôn sử dụng các chuyên gia giống hệt nhau, đồng nhất, nhưng các thiết kế tương lai (được tạo điều kiện bởi cơ sở hạ tầng linh hoạt hơn) có thể hỗ trợ các chuyên gia không đồng nhất. Điều này sẽ cho phép thích nghi linh hoạt hơn bằng cách định tuyến đến các chuyên gia lớn hơn khi cần nhiều tính toán hơn - có lẽ cho các ví dụ khó hơn.

5. Điều tra các lớp chuyên gia bên ngoài lớp FFN của Transformer. Chúng tôi tìm thấy bằng chứng sơ bộ rằng điều này tương tự có thể cải thiện chất lượng mô hình. Trong Phụ lục A, chúng tôi báo cáo cải thiện chất lượng thêm những cái này vào bên trong các lớp Self-Attention, nơi lớp của chúng tôi thay thế các ma trận trọng số tạo ra Q, K, V. Tuy nhiên, do tính không ổn định huấn luyện với định dạng bfloat16, chúng tôi thay vào đó để điều này như một lĩnh vực cho công việc tương lai.

6. Kiểm tra Switch Transformer trong các modalities mới và qua các mạng đa modal khác nhau. Chúng tôi cho đến nay chỉ xem xét ngôn ngữ, nhưng chúng tôi tin rằng sparsity mô hình có thể tương tự cung cấp lợi thế trong các modalities mới, cũng như các mạng đa modal.

Danh sách này có thể dễ dàng được mở rộng, nhưng chúng tôi hy vọng điều này đưa ra hương vị cho các loại thách thức mà chúng tôi đang suy nghĩ và những gì chúng tôi nghi ngờ là những hướng tương lai hứa hẹn.

9. Kết luận

Switch Transformers là những người học ngôn ngữ tự nhiên có thể scale và hiệu quả. Chúng tôi đơn giản hóa Mixture of Experts để tạo ra một kiến trúc dễ hiểu, ổn định để huấn luyện và hiệu quả mẫu hơn rất nhiều so với các mô hình dày đặc có kích thước tương đương. Chúng tôi thấy rằng những mô hình này xuất sắc qua một tập đa dạng các nhiệm vụ ngôn ngữ tự nhiên và trong các chế độ huấn luyện khác nhau, bao gồm tiền huấn luyện, tinh chỉnh và huấn luyện đa nhiệm vụ. Những tiến bộ này làm cho việc huấn luyện các mô hình với hàng trăm tỷ đến nghìn tỷ tham số trở nên khả thi và đạt được tăng tốc đáng kể so với các baseline T5 dày đặc. Chúng tôi hy vọng công việc của chúng tôi thúc đẩy các mô hình thưa thớt như một kiến trúc hiệu quả và khuyến khích các nhà nghiên cứu và thực hành viên xem xét những mô hình linh hoạt này trong các nhiệm vụ ngôn ngữ tự nhiên, và hơn thế nữa.

Lời cảm ơn

Các tác giả muốn cảm ơn Margaret Li đã cung cấp những tháng insight chính vào các cải tiến thuật toán và đề xuất cho các nghiên cứu thực nghiệm. Hugo Larochelle cho lời khuyên khôn ngoan và nhận xét làm rõ trên bản thảo, Irwan Bello cho nhận xét chi tiết và sửa đổi cẩn thận, Colin Raffel và Adam Roberts cho lời khuyên kịp thời về các mô hình ngôn ngữ thần kinh và code-base T5, Yoshua Bengio cho lời khuyên và khuyến khích trong nghiên cứu về tính toán thích nghi, Jascha Sohl-dickstein cho những hướng mới thú vị để ổn định các mô hình quy mô lớn mới và sửa đổi bài báo, và Google Brain Team cho các thảo luận hữu ích về bài báo. Blake Hechtman đã cung cấp sự giúp đỡ vô giá trong profiling và cải thiện hiệu suất huấn luyện của các mô hình của chúng tôi.

A. Switch cho Attention

Shazeer et al. (2018); Lepikhin et al. (2020) thiết kế MoE Transformers (Shazeer et al., 2017) bằng cách thêm các lớp MoE vào các tính toán feedforward network (FFN) dày đặc của Transformer. Tương tự, công việc của chúng tôi cũng thay thế lớp FFN trong Transformer, nhưng chúng tôi khám phá ngắn gọn ở đây một thiết kế thay thế. Chúng tôi thêm các lớp Switch vào các lớp Transformer Self-Attention. Để làm điều đó, chúng tôi thay thế các ma trận trọng số có thể huấn luyện tạo ra queries, keys và values bằng các lớp Switch như thấy trong Hình 10.

[Hình 10: Switch layers in attention]

Bảng 10 ghi lại chất lượng sau một số bước cố định cũng như thời gian huấn luyện cho một số biến thể. Mặc dù chúng tôi thấy những cải thiện, chúng tôi cũng thấy những lớp này không ổn định hơn khi sử dụng độ chính xác bfloat16 và do đó chúng tôi không bao gồm chúng trong biến thể cuối cùng. Tuy nhiên, khi những lớp này huấn luyện ổn định, chúng tôi tin rằng kết quả tích cực sơ bộ gợi ý một hướng tương lai hứa hẹn.

[Bảng 10: Switch attention layer results]

B. Ngăn chặn việc bỏ Token với No-Token-Left-Behind

Do các ràng buộc phần mềm trên các accelerators TPU, hình dạng của Tensors của chúng tôi phải được có kích thước tĩnh. Kết quả là, mỗi chuyên gia có khả năng hữu hạn và cố định để xử lý biểu diễn token. Tuy nhiên, điều này trình bày một vấn đề cho mô hình của chúng tôi định tuyến tokens động tại run-time có thể dẫn đến phân phối không đều qua các chuyên gia. Nếu số lượng tokens gửi đến một chuyên gia ít hơn khả năng chuyên gia, thì tính toán có thể đơn giản được padded - một việc sử dụng không hiệu quả của phần cứng, nhưng đúng về mặt toán học. Tuy nhiên, khi số lượng tokens gửi đến một chuyên gia lớn hơn khả năng của nó (expert overflow), một giao thức cần thiết để xử lý điều này. Lepikhin et al. (2020) thích nghi một mô hình Mixture-of-Expert và giải quyết expert overflow bằng cách truyền biểu diễn của nó đến lớp tiếp theo mà không xử lý thông qua kết nối dư mà chúng tôi cũng theo.

Chúng tôi nghi ngờ rằng việc không có tính toán được áp dụng cho tokens có thể rất lãng phí, đặc biệt vì nếu có overflow trên một chuyên gia, có nghĩa là chuyên gia khác sẽ có khả năng bổ sung. Với trực giác này chúng tôi tạo ra No-Token-Left-Behind, lặp lại định tuyến lại bất kỳ tokens nào lúc đầu được định tuyến đến một chuyên gia đang overflow. Hình 11 cho thấy một mô tả đồ họa của phương pháp này, sẽ cho phép chúng tôi đảm bảo hầu như không có tokens nào sẽ bị loại bỏ trong quá trình huấn luyện và suy luận. Chúng tôi giả thuyết rằng điều này có thể cải thiện hiệu suất và ổn định thêm huấn luyện, nhưng chúng tôi không thấy lợi ích thực nghiệm. Chúng tôi nghi ngờ rằng một khi mạng học các associations giữa các tokens khác nhau và chuyên gia, nếu association này được thay đổi (ví dụ gửi một token đến chuyên gia cao thứ hai của nó) thì hiệu suất có thể bị suy giảm.

[Hình 11: Diagram of the No-Token-Left-Behind Routing]

C. Khuyến khích khám phá trên các chuyên gia

Tại mỗi lớp chuyên gia, router xác định chuyên gia nào để gửi token. Đây là một quyết định rời rạc qua các chuyên gia có sẵn, có điều kiện trên thông tin về biểu diễn token. Dựa trên biểu diễn token đến, router xác định chuyên gia tốt nhất, tuy nhiên, nó không nhận thông tin counterfactual về cách nó sẽ làm tốt khi chọn một chuyên gia thay thế. Như trong reinforcement learning, một dilemma exploration-exploitation cổ điển phát sinh (Sutton và Barto, 2018). Những vấn đề này đã được lưu ý tương tự và giải quyết khác nhau bởi Rosenbaum et al. (2017) đã chứng minh thành công trong multi-task learning. Cài đặt cụ thể này khớp nhất với contextual bandit (Robbins, 1952). Việc lựa chọn chuyên gia hàng đầu một cách xác định luôn tương đương với một chiến lược exploitative - chúng tôi xem xét cân bằng exploration để tìm kiếm assignment chuyên gia tốt hơn.

Để giới thiệu exploration, chúng tôi xem xét một số cách tiếp cận: 1) deterministic hoặc argmax 2) sampling từ phân phối softmax 3) input dropout trên biểu diễn đến 4) multiplicative jitter noise trên biểu diễn đến. Tác động kết quả đến chất lượng mô hình được báo cáo trong Bảng 11. Trong suốt công việc này, chúng tôi sử dụng input jitter để inject noise vì chúng tôi thấy nó hoạt động thực nghiệm tốt nhất.

[Bảng 11: Router Exploration Strategies]

D. Switch Transformers trong các chế độ tính toán thấp hơn

Switch Transformer cũng là một kiến trúc hiệu quả ở quy mô nhỏ cũng như trong các chế độ với hàng nghìn lõi và nghìn tỷ tham số. Nhiều thí nghiệm trước đây của chúng tôi ở quy mô 10B+ parameter models, nhưng chúng tôi cho thấy trong Hình 12 ít nhất 2 chuyên gia tạo ra lợi ích hấp dẫn so với đối tác được FLOP-matched. Ngay cả khi một siêu máy tính không sẵn có, huấn luyện Switch Transformers với 2, 4, hoặc 8 chuyên gia (như chúng tôi thường khuyến nghị một chuyên gia mỗi lõi) dẫn đến những cải thiện vững chắc so với baselines T5 dày đặc.

[Hình 12: Switch Transformer with few experts]

E. Mối quan hệ giữa hiệu suất mô hình Upstream và Downstream

Không có đảm bảo rằng chất lượng của một mô hình trên một mục tiêu tiền huấn luyện sẽ dịch sang kết quả nhiệm vụ downstream. Hình 13 trình bày sự tương quan của chất lượng mô hình upstream, cho cả mô hình dày đặc và Switch, trên nhiệm vụ tiền huấn luyện C4 với hai phép đo nhiệm vụ downstream: hiệu suất SuperGLUE trung bình và điểm TriviaQA. Chúng tôi chọn hai nhiệm vụ này vì một thăm dò lý luận của mô hình và cái khác kiến thức thực tế.

[Hình 13: Upstream pre-trained quality to downstream model quality]

Chúng tôi thấy một tương quan nhất quán, chỉ ra rằng đối với cả baseline và Switch models, tiền huấn luyện cải thiện dẫn đến kết quả downstream tốt hơn. Ngoài ra, đối với một perplexity upstream cố định chúng tôi thấy rằng cả Switch và dense models hoạt động tương tự trong chế độ kích thước mô hình nhỏ đến trung bình. Tuy nhiên, trong chế độ mô hình lớn nhất (T5-11B/T5-XXL) các mô hình Switch lớn nhất của chúng tôi, như đề cập trong Phần 5.6, không luôn dịch perplexity upstream của chúng tốt sang tinh chỉnh downstream trên nhiệm vụ SuperGLUE. Điều này đảm bảo điều tra và nghiên cứu tương lai để hoàn toàn thực hiện tiềm năng của các mô hình thưa thớt. Hiểu động lực tinh chỉnh với các mô hình chuyên gia rất phức tạp và phụ thuộc vào regularization, load-balancing, và các siêu tham số tinh chỉnh.

F. Mã giả cho Switch Transformers

Mã giả cho Switch Transformers trong Mesh TensorFlow (Shazeer et al., 2018). Không có song song mô hình được sử dụng cho code dưới đây (xem 5.4 để biết thêm chi tiết).

[Hình 14, 15, 16: Pseudo code cho load balance loss, router, và switch layer trong Mesh TensorFlow]

Tài liệu tham khảo

[Danh sách đầy đủ các tài liệu tham khảo từ trang 36-40]
