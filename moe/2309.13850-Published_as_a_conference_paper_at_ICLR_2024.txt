# 2309.13850.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2309.13850.pdf
# File size: 1015173 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2024
STATISTICAL PERSPECTIVE OF TOP-K S PARSE SOFT-
MAX GATING MIXTURE OF EXPERTS
Huy Nguyen
Department of Statistics and Data Sciences
The University of Texas at Austin
Austin, TX 78712
huynm@utexas.eduPedram Akbarian
Department of Electrical and Computer Engineering
The University of Texas at Austin
Austin, TX 78712
akbarian@utexas.edu
Fanqi Yan
Department of Computer Science
The University of Texas at Austin
Austin, TX 78712
fanqi.yan@utexas.eduNhat Ho
Department of Statistics and Data Sciences
The University of Texas at Austin
Austin, TX 78712
minhnhat@utexas.edu
ABSTRACT
Top-K sparse softmax gating mixture of experts has been widely used for scaling
up massive deep-learning architectures without increasing the computational cost.
Despite its popularity in real-world applications, the theoretical understanding of
that gating function has remained an open problem. The main challenge comes
from the structure of the top-K sparse softmax gating function, which partitions
the input space into multiple regions with distinct behaviors. By focusing on a
Gaussian mixture of experts, we establish theoretical results on the effects of the
top-K sparse softmax gating function on both density and parameter estimations.
Our results hinge upon defining novel loss functions among parameters to capture
different behaviors of the input regions. When the true number of experts k∗
is known, we demonstrate that the convergence rates of density and parameter
estimations are both parametric on the sample size. However, when k∗becomes
unknown and the true model is over-specified by a Gaussian mixture of kexperts
where k > k ∗, our findings suggest that the number of experts selected from
the top-K sparse softmax gating function must exceed the total cardinality of a
certain number of V oronoi cells associated with the true parameters to guarantee
the convergence of the density estimation. Moreover, while the density estimation
rate remains parametric under this setting, the parameter estimation rates become
substantially slow due to an intrinsic interaction between the softmax gating and
expert functions.
1 I NTRODUCTION
Introduced by Jacobs et al. (1991) and Jordan & Jacobs (1994), the mixture of experts (MoE) has
been known as a statistical machine learning design that leverages softmax gating functions to blend
different expert networks together in order to establish a more intricate model. Recently, there has
been a huge interest in a variant of this model called top-K sparse softmax gating MoE, which
activates only the best Kexpert networks for each input based on sparse gating functions (Shazeer
et al., 2017; Fedus et al., 2022a; Chen et al., 2023). Thus, this surrogate can be seen as a form of
conditional computation (Bengio, 2013; Cho & Bengio, 2014) since it significantly scales up the
model capacity while avoiding a proportional increase in the computational cost. These benefits
have been empirically demonstrated in several deep learning applications, including natural language
processing (Lepikhin et al., 2021; Du et al., 2022; Fedus et al., 2022b; Zhou et al., 2023; Pham et al.,
1arXiv:2309.13850v2  [stat.ML]  23 Feb 2024

--- PAGE 2 ---
Published as a conference paper at ICLR 2024
2024), speech recognition (Peng et al., 1996; Gulati et al., 2020; You et al., 2022), computer vision
(Dosovitskiy et al., 2021; Riquelme et al., 2021; Liang et al., 2022; Bao et al., 2022), multi-task
learning (Hazimeh et al., 2021; Gupta et al., 2022) and other applications (Rives et al., 2021; Chow
et al., 2023; Li et al., 2023; Han et al., 2024). Nevertheless, to the best of our knowledge, the full
theoretical analysis of the top-K sparse softmax gating function has remained missing in the literature.
In this paper, we aim to shed new light on the theoretical understanding of that gating function from a
statistical perspective via the convergence analysis of maximum likelihood estimation (MLE) under
the top-K sparse softmax gating Gaussian MoE.
There have been previous efforts to study the parameter estimation problem in Gaussian MoE models.
Firstly, Ho et al. (2022) utilized the generalized Wasserstein loss functions Villani (2003; 2008) to
derive the rates for estimating parameters in the input-free gating Gaussian MoE. They pointed out
that due to an interaction among expert parameters, these rates became increasingly slow when the
number of extra experts rose. Subsequently, Nguyen et al. (2023) and Nguyen et al. (2024) took
into account the Gaussian MoE with softmax gating and Gaussian gating functions, respectively.
Since these gating functions depended on the input, another interaction between gating parameters
and expert parameters arose. Therefore, they designed V oronoi loss functions to capture these
interactions and observe that the convergence rates for parameter estimation under these settings can
be characterized by the solvability of systems of polynomial equations (Sturmfels, 2002).
Turning to the top-K sparse softmax gating Gaussian MoE, the convergence analysis of the MLE,
however, becomes substantially challenging due to the sophisticated structure of the top-K sparse
softmax gating function compared to those of softmax gating and Gaussian gating functions. To
comprehend these obstacles better, let us introduce the formal formulation of that model.
Problem setup. Suppose that (X1, Y1), . . . , (Xn, Yn)∈Rd×Rare i.i.d. samples of size nfrom
the top-K sparse softmax gating Gaussian MoE of order k∗with the conditional density function
gG∗(Y|X) =k∗X
i=1Softmax(TopK(( β∗
1i)⊤X, K ;β∗
0i))·f(Y|(a∗
i)⊤X+b∗
i, σ∗
i), (1)
where G∗:=Pk∗
i=1exp(β∗
0i)δ(β∗
1i,a∗
i,b∗
i,σ∗
i)is a true but unknown mixing measure of order k∗(i.e.,
a linear combination of k∗Dirac measures δ) associated with true parameters (β∗
0i, β∗
1i, a∗
i, b∗
i, σ∗
i)
fori∈ {1,2, . . . , k ∗}. Here, h1(X, a, b ) := a⊤X+bis referred to as an expert function, while
we denote f(·|µ, σ)as an univariate Gaussian density function with mean µand variance σ(The
results for other settings of f(·|µ, σ)are in Appendix E). Additionally, we define for any vectors
v= (v1, . . . , v k∗)andu= (u1, . . . , u k∗)inRk∗thatSoftmax( vi) := exp( vi)/Pk∗
j=1exp(vj)and
TopK( vi, K;ui) :=vi+ui, ifviis in the top Kelements of v;
−∞, otherwise .
More specifically, before applying the softmax function in equation (1), we keep only the top Kvalues
of(β∗
11)⊤X, . . . , (β∗
1k∗)⊤Xand their corresponding bias vectors among β∗
01, . . . , β∗
0k∗, while we set
the rest to −∞ to make their gating values vanish. An instance of the top-K sparse softmax gating
function is given in equation (3). Furthermore, linear expert functions considered in equation (1)are
only for the simplicity of presentation. With similar proof techniques, the results in this work can
be extended to general settings of the expert functions, including deep neural networks. In order to
obtain an estimate of G∗, we use the following maximum likelihood estimation (MLE):
bGn∈arg max
G1
nnX
i=1log(gG(Yi|Xi)). (2)
Under the exact-specified settings, i.e., when the true number of expert k∗is known, the maximum
in equation (2)is subject to the set of all mixing measures of order k∗denoted by Ek∗(Ω) := {G=Pk∗
i=1exp(β0i)δ(β1i,ai,bi,σi): (β0i, β1i, ai, bi, σi)∈Ω}. On the other hand, under the over-specified
settings, i.e., when k∗is unknown and the true model is over-specified by a Gaussian mixture of k
experts where k > k ∗, the maximum is subject to the set of all mixing measures of order at most k,
i.e.,Ok(Ω) := {G=Pk′
i=1exp(β0i)δ(β1i,ai,bi,σi): 1≤k′≤k,(β0i, β1i, ai, bi, σi)∈Ω}.
Universal assumptions. For the sake of theory, we impose four main assumptions on the parameters:
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2024
(U.1) Convergence of MLE: To ensure the convergence of parameter estimation, we assume that the
parameter space Ωis compact subset of R×Rd×Rd×R×R+, while the input space Xis bounded.
(U.2) Identifiability: Next, we assume that β∗
1k∗=0dandβ∗
0k∗= 0so that the top-K sparse softmax
gating Gaussian mixture of experts is identifiable. Under that assumption, we show in Appendix C
that if GandG′are two mixing measures such that gG(Y|X) =gG′(Y|X)for almost surely (X, Y),
then it follows that that G≡G′. Without that assumption, the result that G≡G′does not hold,
which leads to unncessarily complicated loss functions (see [Proposition 1,Nguyen et al. (2023)]).
(U.3) Distinct Experts: To guarantee that experts in the mixture (1)are different from each other, we
assume that parameters (a∗
1, b∗
1, σ∗
1), . . . , (a∗
k∗, b∗
k∗, σ∗
k∗)are pairwise distinct.
(U.4) Input-dependent Gating Functions: To make sure that the gating functions depend on
the input X, we assume that at least one among parameters β∗
11, . . . , β∗
1k∗is different from zero.
Otherwise, the gating functions would be independent of the input X, which simplifies the problem
significantly. In particular, the model (1)would reduce to an input-free gating Gaussian mixture of
experts, which was already studied in Ho et al. (2022).
Challenge discussion. In our convergence analysis, there are two main challenges attributed to the
structure of the top-K sparse softmax gating function. (1)First, since this gating function divides the
input space into multiple regions and each of which has different convergence behavior of density
estimation, there could be a mismatch between the values of the top-K sparse softmax gating function
in the density estimation gbGnand in the true density gG∗(see Figure 1). (2)Second, under the over-
specified settings, each component of G∗could be fitted by at least two components of bGn. Therefore,
choosing only the best Kexperts in the formulation of gbGn(Y|X)is insufficient to estimate the
true density gG∗(Y|X). As a result, it is of great importance to figure out the minimum number of
experts selected in the top-K sparse softmax gating function necessary for ensuring the convergence
of density estimation.
Contributions. In this work, we provide rigorous statistical guarantees for density estimation and
parameter estimation in the top-K sparse softmax gating Gaussian MoE under both the exact-specified
and over-specified settings. Our contributions are two-fold and can be summarized as follows (see
also Table 1):
1. Exact-specified settings: When the true number of experts k∗is known, we point out that
the density estimation gbGnconverges to the true density gG∗under the Hellinger distance hat the
parametric rate, that is, EX[h(gbGn(·|X), gG∗(·|X))] =eO(n−1/2). Then, we propose a novel V oronoi
metric D1defined in equation (5)to characterize the parameter estimation rates. By establishing
the Hellinger lower bound EX[h(gG(·|X), gG∗(·|X))]≳D1(G, G∗)for any mixing measure G∈
Ek∗(Θ), we arrive at another bound D1(bGn, G∗) =eO(n−1/2), which indicates that the rates for
estimating true parameters exp(β∗
0i), β∗
1i, a∗
i, b∗
i, σ∗
iare of the optimal order eO(n−1/2).
2. Over-specified settings: When k∗is unknown and the true model is over-specified by a Gaussian
ofkexperts where k > k ∗, we demonstrate that the density estimation gbGnconverges to the true
density gG∗only if the number of experts chosen from gbGn, denoted by K, is greater than the
total cardinality of a certain number of V oronoi cells generated by the support of G∗. Given these
results, the density estimation rate is shown to remain parametric on the sample size. Additionally,
by designing a novel V oronoi loss function D2in equation (8)to capture an interaction between
parameters of the softmax gating and expert functions, we prove that the MLE bGnconverges to
the true mixing measure G∗at a rate of eO(n−1/2). Then, it follows from the formulation of D2
that the estimation rates for true parameters β∗
1j, a∗
j, b∗
j, σ∗
jdepend on the solvability of a system of
polynomial equations arising from the previous interaction, and turn out to be significantly slow.
High-level proof ideas. Following from the challenge discussion, to ensure the convergence of
density estimation under the exact-specified settings (resp. over-specified settings), we show that
the input regions divided by the true gating functions match those divided by the estimated gating
functions in Lemma 1 (resp. Lemma 2). Then, we leverage fundamental results on density estimation
for M-estimators in (van de Geer, 2000) to derive the parametric density estimation rate under the
Hellinger distance in Theorem 1 (resp. Theorem 3). Regarding the parameter estimation problem,
a key step is to decompose the density discrepancy gbGn(Y|X)−gG∗(Y|X)into a combination of
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2024
Table 1: Summary of density estimation and parameter estimation rates under the top-K sparse
softmax gating Gaussian MoE model. Here, the function ¯r(·)stands for the solvability of the system
of polynomial equations (7), andCjis a V oronoi cell defined in equation (4). If|Cj|= 2, then we
have¯r(|Cj|) = 4 . Meanwhile, if |Cj|= 3, then we get ¯r(|Cj|) = 6 .
Setting gG∗(Y|X)exp(β∗
0j) β∗
1j, b∗
j a∗
j, σ∗
j
Exact-specified O(n−1/2)O(n−1/2) O(n−1/2) O(n−1/2)
Over-specified O(n−1/2)O(n−1/2)O(n−1/2¯r(|Cj|))O(n−1/¯r(|Cj|))
1n1*2n2*3n3*
Figure 1: Illustration of two partitions of the input space with respect to the TopK function in the
density estimation gbGn(left) and the true density gG∗(right) under the exact-specified settings when
k∗= 3andK= 1. Here, the regions labelled as 1nand1∗contain X∈ X such that (bβn
11)⊤Xand
(β∗
11)⊤Xare the top-1 elements of ((bβn
1i)⊤X)3
i=1and((β∗
1i)⊤X)3
i=1, respectively. Other regions
are defined similarly. Assume that bβn
1i→β∗
1iasn→ ∞ for any i∈ {1,2,3}, then the regions
1n,2n,3nshould respectively match their counterparts 1∗,2∗,3∗to guarantee the convergence of
gbGntogG∗. Lemma 1 reads that this property holds when the sample size nis sufficiently large.
linearly independent terms. Thus, when the density estimation gbGn(Y|X)converges to the true
density gG∗(Y|X), the coefficients in that combination also tend to zero, which leads to our desired
parameter estimation rates in Theorem 2 (resp. Theorem 4).
Organization. The rest of our paper is organized as follows. In Section 2, we establish the
convergence rates of density estimation and parameter estimation for the top-K sparse softmax gating
Gaussian MoE under the exact-specified settings, while the results for the over-specified settings are
presented in Section 3. Subsequently, we provide practical implications of those results in Section 4
before concluding the paper and discussing some future directions in Section 5. Finally, full proofs,
numerical experiments and additional results are deferred to the supplementary material.
Notations. For any natural numbers m≥n, we denote [n] :={1,2, . . . , n }and m
n
:=m!
n!(m−n!).
Next, for any vector u, v∈Rd, we let |u|:=Pd
i=1ui,u! := u1!. . . u d!,uv:=uv1
1. . . uvd
dand denote ∥u∥as its 2-norm value. Meanwhile, we define |A|as the cardinality of some set
A. Then, for any two probability densities f1andf2dominated by the Lebesgue measure ν,
we denote V(f1, f2) :=1
2R
|f1−f2|dνas their Total Variation distance, whereas h2(f1, f2) :=
1
2R
(√f1−√f2)2dνrepresents the squared Hellinger distance. Finally, for any two sequences
of positive real numbers (an)and(bn), the notations an=O(bn)andan≲bnboth stand for
an≤Cbnfor all n∈Nfor some constant C >0, while the notation an=eO(bn)indicates that the
previous inequality may depend on some logarithmic term.
2 E XACT -SPECIFIED SETTINGS
In this section, we characterize respectively the convergence rates of density estimation and parameter
estimation in the top-K sparse softmax gating Gaussian MoE under the exact-specified settings,
namely when the true number of experts k∗is known.
To begin with, let us introduce some essential notations and key results to deal with the top-K sparse
softmax gating function. It can be seen from equation (1)that whether (a∗
i)⊤X+b∗
ibelongs to the top
Kexperts in the true density gG∗(Y|X)or not is determined based on the ranking of (β∗
1i)⊤X+β∗
0i,
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2024
which depends on the input X. Additionally, it is also worth noting that there are a total of q:= k∗
K
different potential choices of top Kexperts. Thus, we first partition the input space Xintoqregions
in order to specify the top Kexperts and obtain an according representation of gG∗(Y|X)in each
region. In particular, for each ℓ∈[q], we denote {ℓ1, ℓ2, . . . , ℓ K}as an K-element subset of [k∗]and
{ℓK+1, . . . , ℓ k∗}:= [k∗]\ {ℓ1, ℓ2, . . . , ℓ K}. Then, the ℓ-th region of Xis defined as
X∗
ℓ:=n
x∈ X: (β∗
1i)⊤x≥(β∗
1i′)⊤x,∀i∈ {ℓ1, . . . , ℓ K}, i′∈ {ℓK+1, . . . , ℓ k∗}o
,
for any ℓ∈[q]. From this definition, we observe that if X∈ X∗
ℓfor some ℓ∈[q]such that
{ℓ1, . . . , ℓ K}= [K], then it follows that TopK(( β∗
1i)⊤X, K ;β∗
0i) = (β∗
1i)⊤X+β∗
0ifor any i∈[K].
As a result, (a∗
1)⊤X+b∗
1, . . . , (a∗
K)⊤X+b∗
Kbecome the top Kexperts, and the true conditional
density gG∗(Y|X)is reduced to:
gG∗(Y|X) =KX
i=1exp(( β∗
1i)⊤X+β∗
0i)PK
j=1exp(( β∗
1j)⊤X+β∗
0j)·f(Y|(a∗
i)⊤X+b∗
i, σ∗
i). (3)
Subsequently, we assume that the MLE bGntakes the form bGn:=Pk∗
i=1exp(bβn
0i)δ(bβn
1i,ban
i,bbn
i,bσn
i),
where the MLE’s component bωn
i:= (bβn
0i,bβn
1i,ban
i,bbn
i,bσn
i)converges to the true component ω∗
i:=
(β∗
0i, β∗
1i, a∗
i, b∗
i, σ∗
i)for any i∈[k∗]. We figure out in the following lemma a relation between the
values of the TopK function in gG∗(Y|X)andgbGn(Y|X):
Lemma 1. For any i∈[k∗], letβ1i, β∗
1i∈Rdsuch that ∥β1i−β∗
1i∥ ≤ηifor some sufficiently small
ηi>0. Then, for any ℓ∈[q], unless the set X∗
ℓhas measure zero, we obtain that X∗
ℓ=Xℓwhere
Xℓ:={x∈ X: (β1i)⊤x≥(β1i′)⊤x,∀i∈ {ℓ1, . . . , ℓ K}, i′∈ {ℓK+1, . . . , ℓ k∗}}.
Proof of Lemma 1 is in Appendix A.3. This lemma indicates that for almost surely X,
TopK(( β∗
1i)⊤X, K ;β∗
0i) = (β∗
1i)⊤X+β∗
0iis equivalent to TopK(( bβn
1i)⊤X, K ;bβn
0i) = (bβn
1i)⊤X+
bβn
0i, for any i∈[k∗]and sufficiently large n. Based on this property, we provide in Theorem 1 the
rate for estimating the true conditional density function gG∗:
Theorem 1 (Density estimation rate) .Given the MLE bGndefined in equation (2), the convergence
rate of the conditional density estimation gbGnto the true conditional density gG∗under the exact-
specified settings is illustrated by the following inequality:
P
EX[h(gbGn(·|X), gG∗(·|X))]> Cp
log(n)/n
≲n−c,
where C >0andc >0are some universal constants that depend on ΩandK.
Proof of Theorem 1 can be found in Appendix A.1. It follows from the result of Theorem 1 that
the conditional density estimation gbGnconverges to its true counterpart gG∗under the Hellinger
distance at the parametric rate of order eO(n−1/2). This rate plays a critical role in establishing the
convergence rates of parameter estimation. In particular, if we are able to derive the following lower
bound: EX[h(gG(·|X), gG∗(·|X))]≳D1(G, G∗)for any mixing measure G∈ Ek∗(Ω), where the
metric D1will be defined in equation (5), we will achieve our desired parameter estimation rates.
Before going into further details, let us introduce the formulation of V oronoi metric D1that we use
for our convergence analysis under the exact-specified settings.
Voronoi metric. Given an arbitrary mixing measure Gwithk′components, we distribute those
components to the following V oronoi cells generated by the components of G∗Manole & Ho (2022):
Cj≡ Cj(G) :={i∈[k′] :∥θi−θ∗
j∥ ≤ ∥ θi−θ∗
j′∥,∀j′̸=j}, (4)
where θi:= (β1i, ai, bi, σi)andθ∗
j:= (β∗
1j, a∗
j, b∗
j, σ∗
j)for any j∈[k∗]. Recall that under the
exact-specified settings, the MLE bGnbelongs to the set Ek∗(Ω). Therefore, we consider k′=k∗in
this case. Then, the V oronoi metric D1is defined as follows:
D1(G, G∗) := max
{ℓj}K
j=1⊂[k∗]KX
j=1"X
i∈Cℓjexp(β0i)
∥∆β1iℓj∥+∥∆aiℓj∥+∥∆biℓj∥+∥∆σiℓj∥
+X
i∈Cℓjexp(β0i)−exp(β∗
0ℓj)#
. (5)
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2024
Here, we denote ∆β1iℓi:=β1i−β∗
1ℓj,∆aiℓj:=ai−a∗
ℓj,∆biℓj:=bi−b∗
ℓjand∆σiℓj:=σi−σ∗
ℓj.
Additionally, the above maximum operator helps capture the behaviors of all input regions separated
by the top-K sparse softmax gating function. Now, we are ready to characterize the convergence rates
of parameter estimation in the top-K sparse softmax gating Gaussian MoE.
Theorem 2 (Parameter estimation rate) .Under the exact-specified settings, the Hellinger lower bound
EX[h(gG(·|X), gG∗(·|X))]≳D1(G, G∗)holds for any mixing measure G∈ Ek∗(Ω). Consequently,
we can find a universal constant C1>0depending only on G∗,ΩandKsuch that
P
D1(bGn, G∗)> C1p
log(n)/n
≲n−c1,
where c1>0is a universal constant that depends only on Ω.
Proof of Theorem 2 is in Appendix A.2. This theorem reveals that the V oronoi metric D1between the
MLEbGnand the true mixing measure G∗, i.e.D1(bGn, G∗), vanishes at the parametric rate of order
eO(n−1/2). As a result, the rates for estimating ground-truth parameters exp(β∗
0i), β∗
1i, a∗
i, b∗
i, σ∗
iare
optimal at eO(n−1/2)for any i∈[k∗].
3 O VER-SPECIFIED SETTINGS
In this section, we continue to carry out the same convergence analysis for the top-K sparse softmax
gating Gaussian MoE as in Section 2 but under the over-specificed settings, that is, when the true
number of experts k∗becomes unknown.
Recall that under the over-specified settings, we look for the MLE bGnwithin Ok(Ω), i.e. the set of all
mixing measures with at most kcomponents, where k > k ∗. Thus, there could be some components
(β∗
1i, a∗
i, b∗
i, σ∗
i)of the true mixing measure G∗approximated by at least two fitted components
(bβn
1i,ban
i,bbn
i,bσn
i)of the MLE bGn. Moreover, since the true density gG∗(Y|X)is associated with
topKexperts and each of which corresponds to one component of G∗, we need to select more
thanKexperts in the formulation of density estimation gbGn(Y|X)to guarantee its convergence to
gG∗(Y|X). In particular, for any mixing measure G=Pk′
i=1exp(β0i)δ(β1i,ai,bi,σi)∈ O k(Ω), let
us consider a new formulation of conditional density used for estimating the true density under the
over-specified settings as follows:
gG(Y|X) :=k′X
i=1Softmax(TopK(( β1i)⊤X,K;β0i))·f(Y|(ai)⊤X+bi, σi).
Here, gG(Y|X)is equipped with top- Ksparse softmax gating, where 1≤K≤k′is fixed and
might be different from K. Additionally, the definition of MLE in equation (2)also changes
accordingly to this new density function. Then, we demonstrate in Proposition 1 an interesting
phenomenon that the density estimation gbGnconverges to gG∗under the Hellinger distance only if
K≥max{ℓ1,...,ℓK}⊂[k∗]PK
j=1|Cℓj|, where Cℓjis a V oronoi cell defined in equation (4).
Proposition 1. IfK < max{ℓ1,...,ℓK}⊂[k∗]PK
j=1|Cℓj|, then the following inequality holds true:
inf
G∈Ok(Ω)EX[h(gG(·|X), gG∗(·|X))]>0.
Proof of Proposition 1 is deferred to Appendix B.3. Following from the result of Proposition 1, we
will consider only the regime when max{ℓj}K
j=1⊂[k∗]PK
j=1|Cℓj| ≤K≤kin the rest of this section
to ensure the convergence of density estimation. As the number of experts chosen in the density
estimation changes from KtoK, it is necessary to partition the input space Xintoq:= k
K
regions.
More specifically, for any ℓ∈[q], we denote {ℓ1,ℓ2, . . . , ℓK}as an K-element subset of [k]and
{ℓK+1, . . . , ℓk}:= [k]\ {ℓ1,ℓ2, . . . , ℓK}. Then, we define the ℓ-th region of Xas follows:
Xℓ:={x∈ X: (β1i)⊤x≥(β1i′)⊤x,∀i∈ {ℓ1, . . . , ℓK}, i′∈ {ℓK+1, . . . , ℓk}}.
Inspired by the result of Lemma 1, we continue to present in Lemma 2 a relation between the values
of the TopK functions in the density estimation gbGnand the true density gG∗.
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2024
Lemma 2. For any j∈[k∗]andi∈ Cj, letβ1i, β∗
1j∈Rdthat satisfy ∥β1i−β∗
1j∥ ≤ηjfor some
sufficiently small ηj>0. Additionally, for max{ℓj}K
j=1⊂[k∗]PK
j=1|Cℓj| ≤K≤k, we assume that
there exist ℓ∈[q]andℓ∈[q]such that {ℓ1, . . . , ℓK}=Cℓ1∪. . .∪ CℓK. Then, if the set X∗
ℓdoes not
have measure zero, we achieve that X∗
ℓ=Xℓ.
Proof of Lemma 2 is in Appendix B.4. Different from Lemma 1, we need to impose on Lemma 2 an
assumption that there exist indices ℓ∈[q]andℓ∈[q]that satisfy {ℓ1, . . . , ℓK}=Cℓ1∪. . .∪ CℓK.
This assumption means that each component (bβn
1i,ban
i,bbn
i,bσn
i)corresponding to the top Kexperts
ingbGnmust converge to some true component which corresponds to the top Kexperts in gG∗.
Consequently, for X∈ X∗
ℓ, ifTopK(( β∗
1j)⊤X, K ;β∗
0j) = ( β∗
1j)⊤X+β∗
0jholds true, then we
achieve that TopK(( bβn
1i)⊤X,K;bβn
0i) = (bβn
1i)⊤X+bβn
0iand vice versa, for any j∈[k∗]andi∈ Cj.
Given the result of Lemma 2, we are now able to derive the convergence rate of the density estimation
gbGnto its true counterpart gG∗under the over-specified settings in Theorem 3.
Theorem 3 (Density estimation rate) .Under the over-specified settings, the conditional density
estimation gbGnconverges to the true density gG∗under the Hellinger distance at the following rate:
P
EX[h(gbGn(·|X), gG∗(·|X))]> C′p
log(n)/n
≲n−c′,
where C′>0andc′>0are some universal constants that depend on ΩandK.
Proof of Theorem 3 is in Appendix B.1. Although there is a modification in the number of experts
chosen from gbGn, Theorem 3 verifies that the convergence rate of this density estimation to gG∗under
the over-specified settings remains the same as that under the exact-specified settings, which is of
ordereO(n−1/2). Subsequently, we will leverage this result for our convergence analysis of parameter
estimation under the over-specified settings, which requires us to design a new V oronoi metric.
Voronoi metric. Regarding the top-K sparse softmax gating function, challenges comes not only
from the TopK function but also from the Softmax function. In particular, there is an intrinsic
interaction between the numerators of softmax weights and the expert functions in the Gaussian
density. Moreover, Gaussian density parameters also interacts with each other. These two interactions
are respectively illustrated by the following partial differential equations (PDEs):
∂2s(X, Y)
∂β1∂b=∂s(X, Y)
∂a;∂s(X, Y)
∂σ=1
2·∂2s(X, Y)
∂b2, (6)
where we denote s(X, Y) := exp( β⊤
1X)·f(Y|a⊤X+b, σ). These PDEs arise when we decompose
the density difference gbGn(Y|X)−gG∗(Y|X)into a linear combination of linearly independent
terms using Taylor expansions. However, the above PDEs indicates that derivative terms which
admit the forms in equation (6)are linearly dependent. Therefore, we have to group these terms
by taking the summation of their coefficients, and then arrive at our desired linear combination
of linearly independent elements. Consequently, when gbGn(Y|X)−gG∗(Y|X)converges to zero,
all the coefficients in this combination also tend to zero, which leads to the following system of
polynomial equations with unknown variables {z1j, z2j, z3j, z4j, z5j}m
i=1:
mX
i=1X
(α1,α2,α3,α4)∈Jη1,η2z2
5izα1
1izα2
2izα3
3izα4
4i
α1!α2!α3!α4!= 0, (7)
for all (η1, η2)∈Nd×Nsuch that 0≤ |η1| ≤r,0≤η2≤r− |η1|and|η1|+η2≥1, where
Jη1,η2:={(α1, α2, α3, α4)∈Nd×Nd×N×N:α1+α2=η1,|α2|+α3+α4=η2}.
Here, a solution to the above system is called non-trivial if all of variables z5iare different from zero,
whereas at least one among variables z3iis non-zero. For any m≥2, letr(m)be the smallest natural
number rsuch that the above system does not have any non-trivial solution. In a general case when
d≥1andm≥2, it is non-trivial to determine the exact value of r(m)Sturmfels (2002). However,
when mis small, Nguyen et al. (2023) show that r(2) = 4 andr(3) = 6 . Additionally, since r(m)is
a monotonically increasing function of m, they also conjecture that r(m) = 2 mfor any m≥2and
d≥1.
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2024
Given the above results, we design a V oronoi metric D2to capture the convergence rates of parameter
estimation in the top-K sparse softmax gating Gaussian MoE under the over-specified settings as
D2(G, G∗) := max
{ℓj}K
j=1⊂[k∗](X
j∈[K],
|Cℓj|=1X
i∈Cℓjexp(β0i)h
∥∆β1iℓj∥+∥∆aiℓj∥+|∆biℓj|+|∆σiℓj|i
+X
j∈[K],
|Cℓj|>1X
i∈Cℓjexp(β0i)h
∥∆β1iℓj∥¯r(|Cℓj|)+∥∆aiℓj∥¯r(|Cℓj|)
2+|∆biℓj|¯r(|Cℓj|)+|∆σiℓj|¯r(|Cℓj|)
2i
+KX
j=1X
i∈Cℓjexp(β0i)−exp(β∗
0ℓj))
,(8)
for any mixing measure G∈ Ok(Ω). The above maximum operator allows us to capture the behaviors
of all input regions partitioned by the top-K sparse softmax gating function in gG∗. Then, we show in
the following theorem that parameter estimation rates vary with the values of the function r(·).
Theorem 4 (Parameter estimation rate) .Under the over-specified settings, the Hellinger lower
bound EX[h(gG(·|X), gG∗(·|X))]≳D2(G, G∗)holds for any mixing measure G∈ O k(Ω). As a
consequence, we can find a universal constant C2>0depending only on G∗,ΩandKsuch that
P
D2(bGn, G∗)> C2p
log(n)/n
≲n−c2,
where c2>0is a universal constant that depends only on Ω.
Proof of Theorem 4 is in Appendix B.2. A few remarks on this theorem are in order.
(i)Under the over-specified settings, the MLE bGnconverges to the true mixing measure G∗at the
rate of order eO(n−1/2)under the V oronoi metric D2. Let us denote Cn
j=Cj(bGn), then this result
indicates that the estimation rates for ground-truth parameters exp(β∗
0j), β∗
1j, a∗
j, b∗
j, σ∗
jfitted by only
one component, i.e. |Cn
j|= 1, remain the same at eO(n−1/2)as those in the exact-specified settings.
(ii)However, for ground-truth parameters which are approximated by at least two components, i.e.
|Cn
j|>1, the rates for estimating them depend on their corresponding V oronoi cells, and become
significantly low when the cardinality of those V oronoi cells increase. In particular, both β∗
1jandb∗
j
admit the estimation rate of order eO(n−1/2r(|Cn
j|)). Meanwhile, the convergence rates of estimating
a∗
jandσ∗
jare of the same order O(n−1/r(|Cn
j|)). For instance, assume that (β∗
1j, a∗
j, b∗
j, σ∗
j)has three
fitted components, which means that |Cn
j|= 3and therefore, r(|Cn
j|) = 6 . Then, the estimation rates
forβ∗
1j, b∗
janda∗
j, σ∗
jareeO(n−1/12)andeO(n−1/6), respectively.
4 P RACTICAL IMPLICATIONS
In this section, we present three main practical implications of our theoretical results for the use of
top-K sparse softmax gating function in mixture of experts as follows:
1. No trade-off between model capacity and model performance: In the top-K sparse softmax
gating Gaussian mixture of experts, since the gating function is discontinuous and only a portion of
experts are activated for each input to scale up the model capacity, the parameter estimation rates
under that model are expected to be slower than those under the dense softmax gating Gaussian
mixture of experts (Nguyen et al., 2023). However, from our theories it turns out that the parameter
estimation rates under these two models are the same under both the exact-specified and over-specified
settings. As a result, we point out that using the top-K sparse softmax gating function allows us to
scale up the model capacity without sacrificing the computational cost as well as the convergence
rates of parameter and density estimation.
2. Favourable gating function: As mentioned in Section 3, due to an intrinsic interaction between
gating parameters and expert parameters via the first PDE in equation (6), the rates for estimating
those parameters under the over-specified settings are determined by the solvability of the system of
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2024
polynomial equations (7), which are significantly slow. However, if we use the top-1 sparse softmax
gating function, i.e. activating only a single expert for each input, then the gating value is either one
or zero. As a result, the previous interaction no longer occurs, which helps improve the parameter
estimation rates. This partially accounts for the efficacy of top-1 sparse softmax gating mixture of
experts in scaling up deep learning architectures (see (Fedus et al., 2022b)).
3. True/ Minimal number of experts: Another challenge is to choose the true/ minimal number
of experts, which can be partially addressed using theories developed from the paper. In particular,
suppose that the MLE bGnconsists of ˆkncomponents. When the sample size ngoes to infinity, every
V oronoi cell among Cn
1, . . . ,Cn
k∗contains at least one element. Since the total number of elements
of those V oronoi cells is ˆkn, the maximum cardinality of a V oronoi cell turns out to be ˆkn−k∗+ 1.
This maximum value is attained when there is exactly one ground-truth component (β∗
1j, a∗
j, b∗
j, σ∗
j)
fitted by more than one component. An instance for this scenario is when |Cn
1|=ˆkn−k∗+ 1and
|Cn
2|=. . .=|Cn
k∗|= 1. Under this setting, the first true parameters β∗
11, b∗
1anda∗
1, σ∗
1achieve their
worst possible estimation rates of order eO(n−1/2r(ˆkn−k∗+1))andeO(n−1/r(ˆkn−k∗+1)), respectively,
which become significantly slow when the difference ˆkn−k∗increases. As a consequence, the
estimated number of experts ˆknshould not be very large compared to the true number of experts k∗.
5 C ONCLUSION AND FUTURE DIRECTIONS
In this paper, we provide a convergence analysis for density estimation and parameter estimation
in the top-K sparse softmax gating Gaussian MoE. To overcome the complex structure of top-K
sparse softmax gating function, we first establish a connection between the outputs of TopK function
associated with the density estimation and the true density in each input region partitioned by
the gating function, and then construct novel V oronoi loss functions among parameters to capture
different behaviors of these input regions. Under the exact-specified settings, we show that the rates
for estimating the true density and true parameters are both parametric on the sample size. On the
other hand, although the density estimation rate remains parametric under the over-specified settings,
the parameter estimation rates witness a sharp drop because of an interaction between the softmax
gating and expert functions.
Based on the results of this paper, there are a few potential future directions. Firstly, as we mentioned
in Section 4, a question of how to estimate the true number of experts k∗and the number of experts
selected in the top-K sparse softmax gating function Knaturally arises from this work. Since the
parameter estimation rates under the over-specified settings decrease proportionately to the number
of fitted experts k, one possible approach to estimating k∗is to reduce kuntil these rates reach
the optimal order eO(n−1/2). That reduction can be done by merging close estimated parameters
within their convergence rates to the true parameters (Guha et al., 2021) or by penalizing the log-
likelihood function of the top-K sparse softmax gating Gaussian MoE using the differences among
the parameters (Manole & Khalili, 2021). As a result, the number of experts chosen in the density
estimation Kalso decreases accordingly and approximates the value of K. Secondly, the theoretical
results established in the paper are under the assumption that the data are assumed to be generated
from a top-K sparse softmax gating Gaussian MoE, which can be violated in real-world settings when
the data are not necessarily generated from that model. Under those misspecified settings, the MLE
converges to a mixing measure G∈arg minG∈Ok(Ω)KL(P(Y|X)||gG(Y|X))where P(Y|X)is
the true conditional distribution of Ygiven Xand KL stands for the Kullback-Leibler divergence.
As the space Ok(Ω)is non-convex, the existence of Gis not unique. Furthermore, the current
analysis of the MLE under the misspecified settings of statistical models is mostly conducted when
the function space is convex (van de Geer, 2000), which is inapplicable to the current non-convex
misspecified settings. Thus, it is necessary to develop a new analysis and a new metric to establish
the convergence rate of the MLE to the set of G. Finally, since the log-likelihood function of the
top-K sparse softmax gating Gaussian MoE is highly non-concave, the EM algorithm is used to
approximate the MLE. While the statistical guarantee of the EM has been established for vanilla
Gaussian mixture models (Balakrishnan et al., 2017; Dwivedi et al., 2020b;a), to the best of our
knowledge such guarantee has not been studied in the setting of top-K sparse softmax gating Gaussian
MoE. A potential approach to this problem is to utilize the population-to-sample analysis that has
been widely used in previous works to study the EM algorithm Balakrishnan et al. (2017); Ho et al.
(2023). We leave that direction for the future work.
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2024
ACKNOWLEDGEMENTS
NH acknowledges support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of
Machine Learning.
REFERENCES
S. Balakrishnan, M. J. Wainwright, and B. Yu. Statistical guarantees for the EM algorithm: From
population to sample-based analysis. Annals of Statistics , 45:77–120, 2017.
H. Bao, W. Wang, L. Dong, Q. Liu, O-K. Mohammed, K. Aggarwal, S. Som, S. Piao, and F. Wei.
VLMo: Unified vision-language pre-training with mixture-of-modality-experts. In Advances in
Neural Information Processing Systems , 2022.
Y . Bengio. Deep learning of representations: Looking forward. In International Conference on
Statistical Language and Speech Processing , 2013.
Faicel Chamroukhi, Allou Sam ´e, G´erard Govaert, and Patrice Aknin. Time series modeling by a
regression approach based on a latent process. Neural Networks , 22(5-6):593–602, 2009. Publisher:
Elsevier.
T. Chen, Z. Zhang, A. Jaiswal, S. Liu, and Z. Wang. Sparse moe as the new dropout: Scaling
dense and self-slimmable transformers. In The Eleventh International Conference on Learning
Representations , 2023.
K. Cho and Y . Bengio. Exponentially increasing the capacity-to-computation ratio for conditional
computation in deep learning. arxiv preprint arxiv 1406.7362 , 2014.
Y . Chow, A. Tulepbergenov, O. Nachum, D. Gupta, M. Ryu, M. Ghavamzadeh, and C. Boutilier.
A mixture-of-expert approach to rl-based dialogue management. In The Eleventh International
Conference on Learning Representations , 2023.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data Via the
EM Algorithm. Journal of the Royal Statistical Society: Series B (Methodological) , 39(1):1–22,
September 1977. ISSN 0035-9246.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16
words: Transformers for image recognition at scale. In International Conference on Learning
Representations , 2021.
N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun, Y . Zhou, A. Yu, O. Firat,
B. Zoph, L. Fedus, M. Bosma, Z. Zhou, T. Wang, E. Wang, K. Webster, M. Pellat, K. Robinson,
K. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. Le, Y . Wu, Z. Chen, and C. Cui. Glam:
Efficient scaling of language models with mixture-of-experts. In ICML , 2022.
R. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan, and B. Yu. Sharp analysis of
expectation-maximization for weakly identifiable models. International Conference on Artificial
Intelligence and Statistics (AISTATS) , 2020a.
R. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan, and B. Yu. Singularity, misspecifi-
cation, and the convergence rate of EM. Annals of Statistics , 44:2726–2755, 2020b.
W. Fedus, J. Dean, and B. Zoph. A review of sparse expert models in deep learning. arxiv preprint
arxiv 2209.01667 , 2022a.
W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with
simple and efficient sparsity. Journal of Machine Learning Research , 23:1–39, 2022b.
A. Guha, N. Ho, and XL. Nguyen. On posterior contraction of parameters and interpretability in
Bayesian mixture modeling. Bernoulli , 27(4):2159–2188, 2021.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2024
A. Gulati, J. Qin, C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y . Wu, and
R. Pang. Conformer: Convolution-augmented Transformer for Speech Recognition. In Proc.
Interspeech 2020 , pp. 5036–5040, 2020. doi: 10.21437/Interspeech.2020-3015.
S. Gupta, S. Mukherjee, K. Subudhi, E. Gonzalez, D. Jose, A. Awadallah, and J. Gao. Sparsely
activated mixture-of-experts are robust multi-task learners. arXiv preprint arxiv 2204.0768 , 2022.
X. Han, H. Nguyen, C. Harris, N. Ho, and S. Saria. Fusemoe: Mixture-of-experts transformers for
fleximodal fusion. arXiv preprint arXiv:2402.03226 , 2024.
H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoorthy, Y . Chen, R. Mazumder, L. Hong, and Ed H.
Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task
learning. In NeurIPS , 2021.
N. Ho and X. Nguyen. On strong identifiability and convergence rates of parameter estimation in
finite mixtures. Electronic Journal of Statistics , 10:271–307, 2016.
N. Ho, C. Y . Yang, and M. I. Jordan. Convergence rates for Gaussian mixtures of experts. Journal of
Machine Learning Research , 23(323):1–81, 2022.
N. Ho, K. Khamaru, R. Dwivedi, M. J. Wainwright, M. I. Jordan, and B. Yu. Instability, computational
efficiency and statistical accuracy. Journal of Machine Learning Research , 2023.
R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.
Neural Computation , 3, 1991.
M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural
Computation , 6:181–214, 1994.
J. Kwon and C. Caramanis. EM converges for a mixture of many linear regressions. In Silvia
Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference
on Artificial Intelligence and Statistics , volume 108 of Proceedings of Machine Learning Research ,
pp. 1727–1736. PMLR, 26–28 Aug 2020.
J. Kwon, W. Qian, C. Caramanis, Y . Chen, and D. Davis. Global convergence of the em algorithm
for mixtures of two component linear regression. In Alina Beygelzimer and Daniel Hsu (eds.),
Proceedings of the Thirty-Second Conference on Learning Theory , volume 99 of Proceedings of
Machine Learning Research , pp. 2055–2110. PMLR, 25–28 Jun 2019.
J. Kwon, N. Ho, and C. Caramanis. On the minimax optimality of the em algorithm for learning two-
component mixed linear regression. In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings
of The 24th International Conference on Artificial Intelligence and Statistics , volume 130 of
Proceedings of Machine Learning Research , pp. 1405–1413. PMLR, 13–15 Apr 2021.
D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen. GShard:
Scaling Giant Models with Conditional Computation and Automatic Sharding. In International
Conference on Learning Representations , 2021.
B. Li, Y . Shen, J. Yang, Y . Wang, J. Ren, T. Che, J. Zhang, and Z. Liu. Sparse mixture-of-experts are
domain generalizable learners. In The Eleventh International Conference on Learning Representa-
tions , 2023.
H. Liang, Z. Fan, R. Sarkar, Z. Jiang, T. Chen, K. Zou, Y . Cheng, C. Hao, and Z. Wang. M3ViT:
Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator
Co-design. In NeurIPS , 2022.
T. Manole and N. Ho. Refined convergence rates for maximum likelihood estimation under finite
mixture models. In Proceedings of the 39th International Conference on Machine Learning ,
volume 162 of Proceedings of Machine Learning Research , pp. 14979–15006. PMLR, 17–23 Jul
2022.
T. Manole and A. Khalili. Estimating the number of components in finite mixture models via the
group-sort-fuse procedure. The Annals of Statistics , 49(6):3043–3069, 2021.
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2024
H. Nguyen, TT. Nguyen, and N. Ho. Demystifying softmax gating function in Gaussian mixture of
experts. In Advances in Neural Information Processing Systems , 2023.
H. Nguyen, TT. Nguyen, K. Nguyen, and N. Ho. Towards convergence rates for parameter estimation
in Gaussian-gated mixture of experts. In Proceedings of The 27th International Conference on
Artificial Intelligence and Statistics , 2024.
F. Peng, R. Jacobs, and M. Tanner. Bayesian Inference in Mixtures-of-Experts and Hierarchical
Mixtures-of-Experts Models With an Application to Speech Recognition. Journal of the American
Statistical Association , 91(435):953–960, 1996. ISSN 01621459.
Q. Pham, G. Do, H. Nguyen, TT. Nguyen, C. Liu, M. Sartipi, B. T. Nguyen, S. Ramasamy, X. Li,
S. Hoi, and N. Ho. Competesmoe – effective training of sparse mixture of experts via competition.
arXiv preprint arXiv:2402.02526 , 2024.
C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pint, D. Keysers, and
N. Houlsby. Scaling vision with sparse mixture of experts. In Advances in Neural Information
Processing Systems , volume 34, pp. 8583–8595. Curran Associates, Inc., 2021.
A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L. Zitnick, J. Ma, and
R. Fergus. Biological structure and function emerge from scaling unsupervised learning to 250
million protein sequences. Proceedings of the National Academy of Sciences , 118, 2021.
N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large
neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on
Learning Representations , 2017.
B. Sturmfels. Solving Systems of Polynomial Equations . Providence, R.I, 2002.
H. Teicher. On the mixture of distributions. Annals of Statistics , 31:55–73, 1960.
H. Teicher. Identifiability of mixtures. Annals of Statistics , 32:244–248, 1961.
H. Teicher. Identifiability of finite mixtures. Ann. Math. Statist. , 32:1265–1269, 1963.
S. van de Geer. Empirical Processes in M-estimation . Cambridge University Press, 2000.
C. Villani. Topics in Optimal Transportation . American Mathematical Society, 2003.
C. Villani. Optimal transport: Old and New . Springer, 2008.
Z. You, S. Feng, D. Su, and D. Yu. Speechmoe2: Mixture-of-experts model with improved routing.
InIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp.
7217–7221, 2022.
Y . Zhou, N. Du, Y . Huang, D. Peng, C. Lan, D. Huang, S. Shakeri, D. So, A. Dai, Y . Lu, Z. Chen,
Q. Le, C. Cui, J. Laudon, and J. Dean. Brainformers: Trading simplicity for efficiency. In
International Conference on Machine Learning , pp. 42531–42542. PMLR, 2023.
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2024
In this supplementary material, we first provide rigorous proofs for all results under the exact-specified
settings in Appendix A, while those for the over-specified settings are then presented in Appendix B.
Next, we study the identifiability of the top-K sparse softmax gating Gaussian mixture of experts
(MoE) in Appendix C. We then carry out several numerical experiments in Appendix D to empirically
justify our theoretical results. Finally, we establish the theories for parameter and density estimation
beyond the settings of top-K sparse softmax Gaussian MoE in Appendix E.
A P ROOF FOR RESULTS UNDER THE EXACT -SPECIFIED SETTINGS
In this appendix, we present the proofs for Theorem 1 in Appendix A.1, while that for Theorem 2 is
then given in Appendix A.2. Lastly, the proof of Lemma 1 is provided in Appendix A.3.
A.1 P ROOF OF THEOREM 1
In this appendix, we conduct a convergence analysis for density estimation in the top-K sparse
softmax gating Gaussian MoE using proof techniques in (van de Geer, 2000). For that purpose, it is
necessary to introduce some essential notations and key results first.
LetPk∗(Θ) := {gG(Y|X) :G∈ Ek∗(Ω)}be the set of all conditional density functions of mixing
measures in Ek∗(Ω). Next, we denote by N(ε,Pk∗(Ω),∥ · ∥1)the covering number of metric space
(Pk∗(Ω),∥ · ∥ 1). Meanwhile, HB(ε,Pk∗(Ω), h)represents for the bracketing entropy of Pk∗(Ω)
under the Hellinger distance. Then, we provide in the following lemma the upper bounds of those
terms.
Lemma 3. IfΩis a bounded set, then the following inequalities hold for any 0< η < 1/2:
(i)logN(η,Pk∗(Ω),∥ · ∥1)≲log(1/η);
(ii)HB(η,Pk∗(Ω), h)≲log(1/η).
Proof of Lemma 3 is in Appendix A.1.2. Subsequently, we denote
ePk∗(Ω) := {g(G+G∗)/2(Y|X) :G∈ Ek∗(Ω)};
eP1/2
k∗(Ω) := {g1/2
(G+G∗)/2(Y|X) :G∈ Ek∗(Ω)}.
In addition, for each δ >0, we define a Hellinger ball centered around the conditional density
function gG∗(Y|X)and met with the set eP1/2
k∗(Ω)as
eP1/2
k∗(Ω, δ) :={g1/2∈eP1/2
k∗(Ω) : h(g, gG∗)≤δ}.
To capture the size of the above Hellinger ball, van de Geer (2000) suggest using the following
quantity:
JB(δ,eP1/2
k∗(Ω, δ)) :=Zδ
δ2/213H1/2
B(t,eP1/2
k∗(Ω, t),∥ · ∥)dt∨δ, (9)
where t∨δ:= max {t, δ}. Given those notations, let us recall a standard result for density estimation
in van de Geer (2000).
Lemma 4 (Theorem 7.4, van de Geer (2000)) .TakeΨ(δ)≥ JB(δ,eP1/2
k∗(Ω, δ))such that Ψ(δ)/δ2is
a non-increasing function of δ. Then, for some sequence (δn)and universal constant cwhich satisfy√nδ2
n≥cΨ(δ), we obtain that
P
EXh
h(gbGn(·|X), gG∗(·|X))i
> δ
≤cexp(−nδ2/c2),
for any δ≥δn
Proof of Lemma 4 can be found in van de Geer (2000). Now, we are ready to provide the proof for
convergence rate of density estimation in Theorem 1 in Appendix A.1.1.
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2024
A.1.1 M AINPROOF
It is worth noting that for any t >0, we have
HB(t,eP1/2
k∗(Ω, t),∥ · ∥)≤HB(t,Pk∗(Ω, t), h).
Then, the integral in equation (9) is upper bounded as follows:
JB(δ,eP1/2
k∗(Ω, δ))≤Zδ
δ2/213H1/2
B(t,Pk∗(Ω, t), h)dt∨δ≲Zδ
δ2/213log(1/t)dt∨δ, (10)
where the second inequality follows from part (ii) of Lemma 3.
As a result, by choosing Ψ(δ) =δ·p
log(1/δ), we can verify that Ψ(δ)/δ2is a non-increasing
function of δ. Furthermore, the inequality in equation (10) indicates that Ψ(δ)≥ JB(δ,eP1/2
k∗(Ω, δ)).
Next, let us consider a sequence (δn)defined as δn:=p
log(n)/n. This sequence can be validated
to satisfy the condition√nδ2
n≥cΨ(δ)for some universal constant c. Therefore, by Lemma 4, we
reach the conclusion of Theorem 1:
P
EX[h(gbGn(·|X), gG∗(·|X))]> Cp
log(n)/n
≲n−c,
for some universal constant Cdepending only on Ω.
A.1.2 P ROOF OF LEMMA 3
Part (i). In this part, we will derive the following upper bound for the covering number of metric
space (Pk∗(Ω),∥ · ∥1)for any 0< η < 1/2given the bounded set Ω:
logN(η,Pk∗(Ω),∥ · ∥1)≲log(1/η).
To begin with, we define Θ :={(a, b, σ )∈Rd×R×R+: (β0, β1, a, b, σ )∈Ω}. Note that Ω
is a bounded set, then Θalso admits this property. Thus, there exists an η-cover of Θ, denoted by
Θη. Additionally, we also define ∆ :={(β0, β1)∈R×Rd: (β0, β1, a, b, σ )∈Ω}, and ∆ηbe an
η-cover of ∆. Then, it can be validated that |Θη| ≤ O (η−(d+1)k∗)and|∆η| ≤ O (η−(d+3)k∗).
Subsequently, for each G=Pk∗
i=1exp(β0i)δ(β1i,ai,bi,σi)∈ E k∗(Ω), we take into account
two other mixing measures. The first measure is G′=Pk∗
i=1exp(β0i)δ(β1i,ai,bi,σi), where
(ai,bi,σi)∈Θηis the closest points to (ai, bi, σi)in this set for all i∈[k∗]. The second one
isG:=Pk∗
i=1exp(β0i)δ(β1i,ai,bi,σi)in which (β0i,β1i)∈∆ηfor any i∈[k∗]. Next, let us define
T:={gG∈ Pk∗(Ω) : ( β0i,β1i)∈∆η,(ai,bi,σi)∈Θη,∀i∈[k∗]},
then it is obvious that gG∈ T. Now, we will show that Tis anη-cover of metric space (Pk∗(Ω),∥·∥1)
with a note that it is not necessarily the smallest cover. Indeed, according to the triangle inequality,
∥gG−gG∥1≤ ∥gG−gG′∥1+∥gG′−gG∥1. (11)
The first term in the right hand side can be upper bounded as follows:
∥gG−gG′∥1≤k∗X
i=1Z
X×Yf(Y|a⊤
iX+bi, σi)−f(Y|a⊤
iX+bi,σi)d(X, Y)
≲k∗X
i=1Z
X×Y
∥ai−ai∥+∥bi−bi∥+∥σi−σi∥
d(X, Y)
=k∗X
i=1
∥ai−ai∥+∥bi−bi∥+∥σi−σi∥
≲η. (12)
Next, we will also demonstrate that ∥gG′−gG∥1≲η. For that purpose, let us consider q:= k
K
K-element subsets of {1, . . . , k }, which are assumed to take the form {ℓ1, ℓ2, . . . , ℓ K}for any ℓ∈[q].
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2024
Additionally, we also denote {ℓK+1, . . . , ℓ k}:={1, . . . , k } \ {ℓ1, . . . , ℓ K}for any ℓ∈[q]. Then,
we define
Xℓ:={x∈ X:β⊤
1ix≥β⊤
1i′x:i∈ {ℓ1, . . . , ℓ K}, i′∈ {ℓK+1, . . . , ℓ k∗}},
eXℓ:={x∈ X:β⊤
1ix≥β⊤
1i′x:i∈ {ℓ1, . . . , ℓ K}, i′∈ {ℓK+1, . . . , ℓ k∗}}.
By using the same arguments as in the proof of Lemma 1 in Appendix A.3, we achieve that either
Xℓ=eXℓorXℓhas measure zero for any ℓ∈[q]. As the Softmax function is differentiable, it is a
Lipschitz function with some Lipschitz constant L≥0. Since Xis a bounded set, we may assume
that∥X∥ ≤Bfor any X∈ X. Next, we denote
πℓ(X) :=
β⊤
1ℓix+β⊤
0ℓiK
i=1; πℓ(X) :=
β⊤
1ℓix+β⊤
0ℓiK
i=1,
for any K-element subset {ℓ1, . . . ℓ K}of{1, . . . , k ∗}. Then, we get
∥Softmax( πℓ(X))−Softmax( πℓ(X))∥ ≤L· ∥πℓ(X)−πℓ(X)∥
≤L·KX
i=1
∥β1ℓi−β1ℓi∥ · ∥X∥+|β0ℓi−β0ℓi|
≤L·KX
i=1
ηB+η
≲η.
Back to the proof for ∥gG′−gG∥1≲η, it follows from the above results that
∥gG′−gG∥1=Z
X×Y|gG′(Y|X)−gG(Y|X)|d(X, Y)
≤qX
ℓ=1Z
Xℓ×Y|gG′(Y|X)−gG(Y|X)|d(X, Y)
≤qX
ℓ=1Z
Xℓ×YKX
i=1Softmax( πℓ(X)i)−Softmax( πℓ(X)i)·f(Y|a⊤
ℓiX+bℓi,σℓi)d(X, Y)
≲η, (13)
From the results in equations (11),(12) and(13), we deduce that ∥gG−gG∥1≲η. This implies that
Tis anη-cover of the metric space (Pk∗(Ω),∥ · ∥1). Consequently, we achieve that
N(η,Pk∗(Ω),∥ · ∥1)≲|∆η| × |Θη| ≤ O (1/η(d+3)k),
which induces the conclusion of this part
logN(η,Pk∗(Ω),∥ · ∥1)≲log(1/η).
Part (ii). Moving to this part, we will provide an upper bound for the bracketing entropy of Pk∗(Ω)
under the Hellinger distance:
HB(η,Pk∗(Ω), h)≲log(1/η).
Recall that ΘandXare bounded sets, we can find positive constants −γ≤a⊤X+b≤γand
u1≤σ≤u2. Let us define
Q(Y|X) :=(
1√2πu1exp
−Y2
8u2
,for|Y| ≥2γ
1√2πu1, for|Y|<2γ
Then, it can be validated that f(Y|a⊤X+b, σ)≤Q(X, Y)for any (X, Y)∈ X × Y .
Next, let τ≤ηwhich will be chosen later and {g1, . . . , g N}be an τ-cover of metric space
(Pk∗(Ω),∥ · ∥ 1)with the covering number N:=N(τ,Pk∗(Ω),∥ · ∥ 1). Additionally, we also
consider brackets of the form [ΨL
i(Y|X),ΨU
i(Y|X)]where
ΨL
i(Y|X) := max {gi(Y|X)−τ,0}
ΨU
i(Y|X) := max {gi(Y|X) +τ, Q(Y|X)}.
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2024
Then, we can check that Pk∗(Ω)⊆SN
i=1[ΨL
i(Y|X),ΨU
i(Y|X)]andΨU
i(Y|X)−ΨL
i(Y|X)≤
min{2η, Q(Y|X)}. LetS:= max {2γ,√8u2}log(1/τ), we have for any i∈[N]that
∥ΨU
i−ΨL
i∥1=Z
|Y|<2γ[ΨU
i(Y|X)−ΨL
i(Y|X)] dXdY+Z
|Y|≥2γ[ΨU
i(Y|X)−ΨL
i(Y|X)] dXdY
≤Sτ+ exp
−S2
2u2
≤S′τ,
where S′is some positive constant. This inequality indicates that
HB(S′τ,Pk∗(Ω),∥ · ∥1)≤logN(τ,Pk∗(Ω),∥ · ∥1)≤log(1/τ).
By setting τ=η/S′, we obtain that HB(η,Pk∗(Ω),∥·∥1)≲log(1/η). Finally, since the norm ∥·∥1
is upper bounded by the Hellinger distance, we reach the conclusion of this part:
HB(η,Pk∗(Ω), h)≲log(1/η).
Hence, the proof is completed.
A.2 P ROOF OF THEOREM 2
Since the Hellinger distance is lower bounded by the Total Variation distance, that is h≥V, we will
prove the following Total Variation lower bound:
EX[V(gG(·|X), gG∗(·|X))]≳D1(G, G∗),
which is then respectively broken into local part and global part as follows:
inf
G∈Ek∗(Ω):D1(G,G∗)≤ε′EX[V(gG(·|X), gG∗(·|X))]
D1(G, G∗)>0, (14)
inf
G∈Ek∗(Ω):D1(G,G∗)>ε′EX[V(gG(·|X), gG∗(·|X))]
D1(G, G∗)>0, (15)
for some constant ε′>0.
Proof of claim (14): It is sufficient to show that
lim
ε→0inf
G∈Ek∗(Ω):D1(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]
D1(G, G∗)>0.
Assume that this inequality does not hold, then since the number of experts k∗is known in this case,
there exists a sequence of mixing measure Gn:=Pk∗
i=1exp(βn
0i)δ(βn
1i,an
i,bn
i,σn
i)∈ Ek∗(Ω)such
that both D1(Gn, G∗)andEX[V(gGn(·|X), gG∗(·|X))]/D1(Gn, G∗)approach zero as ntends to
infinity. Now, we define
Cn
j=Cj(Gn) :={i∈[k∗] :∥ωn
i−ω∗
j∥ ≤ ∥ ωn
i−ω∗
s∥,∀s̸=j},
for any j∈[k∗]ask∗V oronoi cells with respect to the mixing measure Gn, where we denote
ωn
i:= (βn
1i, an
i, bn
i, σn
i)andω∗
j:= (β∗
1j, a∗
j, b∗
j, σ∗
j). As we use asymptotic arguments in this proof,
we can assume without loss of generality (WLOG) that these V oronoi cells does not depend on n,
that is, Cj=Cn
j. Next, it follows from the hypothesis D1(Gn, G∗)→0asn→ ∞ that each V oronoi
cell contains only one element. Thus, we continue to assume WLOG that Cj={j}for any j∈[k∗],
which implies that (βn
1j, an
j, bn
j, σn
j)→(β∗
1j, a∗
j, b∗
j, σ∗
j)andexp(βn
0j)→exp(β∗
0j)asn→ ∞ .
Subsequently, to specify the top Kselection in the formulations of gGn(Y|X)andgG∗(Y|X), we
divide the covariate space Xinto some subsets in two ways. In particular, we first consider q:= k∗
K
different K-element subsets of [k∗], which are assumed to take the form {ℓ1, . . . , ℓ K}, forℓ∈[q].
Additionally, we denote {ℓK+1, . . . , ℓ k∗}:= [k∗]\ {ℓ1, . . . , ℓ K}. Then, we define for each ℓ∈[q]
two following subsets of X:
Xn
ℓ:=n
x∈ X: (βn
1j)⊤x≥(βn
1j′)⊤x:∀j∈ {ℓ1, . . . , ℓ K}, j′∈ {ℓK+1, . . . , ℓ k∗}o
,
X∗
ℓ:=n
x∈ X: (β∗
1j)⊤x≥(β∗
1j′)⊤x:∀j∈ {ℓ1, . . . , ℓ K}, j′∈ {ℓK+1, . . . , ℓ k∗}o
.
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2024
Since (βn
0j, βn
1j)→(β∗
0j, β∗
1j)asn→ ∞ for any j∈[k∗], we have for any arbitrarily small ηj>0
that∥βn
1j−β∗
1j∥ ≤ηjand|βn
0j−β∗
0j| ≤ηjfor sufficiently large n. By applying Lemma 1, we
obtain that Xn
ℓ=X∗
ℓfor any ℓ∈[q]for sufficiently large n. WLOG, we assume that
D1(Gn, G∗) =KX
i=1h
exp(βn
0i)
∥∆βn
1i∥+∥∆an
i∥+∥∆bn
i∥+∥∆σn
i∥
+exp(βn
0i)−exp(β∗
0i)i
,
where we denote ∆βn
1i:=βn
1i−β∗
1i,∆an
i:=an
i−a∗
i,∆bn
i:=bn
i−b∗
iand∆σn
i:=σn
i−σ∗
i.
Letℓ∈[q]such that {ℓ1, . . . , ℓ K}={1, . . . , K }. Then, for almost surely (X, Y)∈ X∗
ℓ× Y, we
can rewrite the conditional densities gGn(Y|X)andgG∗(Y|X)as
gGn(Y|X) =KX
i=1exp(( βn
1i)⊤X+βn
0i)PK
j=1exp(( βn
1j)⊤X+βn
0j)·f(Y|(an
i)⊤X+bn
i, σn
i),
gG∗(Y|X) =KX
i=1exp(( β∗
1i)⊤X+β∗
0i)PK
j=1exp(( β∗
1j)⊤X+β∗
0j)·f(Y|(a∗
i)⊤X+b∗
i, σ∗
i).
Now, we break the rest of our arguments into three steps:
Step 1 - Taylor expansion :
In this step, we take into account Hn:=hPK
i=1exp(( β∗
1i)⊤X+β∗
0i)i
·[gGn(Y|X)−gG∗(Y|X)].
Then, Hncan be represented as follows:
Hn=KX
i=1exp(βn
0i)h
exp(( βn
1i)⊤X)f(Y|(an
i)⊤X+bn
i, σn
i)−exp(( β∗
1i)⊤X)f(Y|(a∗
i)⊤X+b∗
i, σ∗
i)i
−KX
i=1exp(βn
0i)h
exp(( βn
1i)⊤X)gGn(Y|X)−exp(( β∗
1i)⊤X)gGn(Y|X)i
+KX
i=1h
exp(βn
0i)−exp(β∗
0i)ih
exp(( β∗
1i)⊤X)f(Y|(a∗
i)⊤X+b∗
i, σ∗
i)−exp(( β∗
1i)⊤X)gGn(Y|X)i
.
By applying the first-order Taylor expansion to the first term in the above representation, which is
denoted by An, we get that
An=KX
i=1X
|α|=1exp(βn
0i)
α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4
×Xα1+α2exp(( β∗
1i)⊤X)·∂|α2|+α3+α4f
∂h|α2|+α3
1 ∂σα4(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +R1(X, Y),
where R1(X, Y)is a Taylor remainder that satisfies R1(X, Y)/D′
1(X, Y)→0asn→ ∞ . Recall
thatfis the univariate Gaussian density, then we have
∂α4f
∂σα4(Y|(a∗
i)⊤X+b∗
i, σ∗
i) =1
2α4·∂2α4f
∂h2α4
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i),
which leads to
An=KX
i=1X
|α|=1exp(βn
0i)
2α4α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4
×Xα1+α2exp(( β∗
1i)⊤X)·∂|α2|+α3+2α4f
∂h|α2|+α3+2α4
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +R1(X, Y)
=KX
i=12X
|η1|+η2=1X
α∈Jη1,η2exp(βn
0i)
2α4α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4
×Xη1exp(( β∗
1i)⊤X)·∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +R1(X, Y), (16)
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2024
where we denote η1=α1+α2∈Nd,η2=|α2|+α3+ 2α4∈Nand an index set
Jη1,η2:={(αi)4
i=1∈Nd×Nd×N×N:α1+α2=η1, α3+ 2α4=η2− |α2|}. (17)
By arguing in a similar fashion for the second term in the representation of Hn, we also get that
Bn:=−KX
i=1X
|γ|=1exp(βn
0i)
γ!(∆βn
1i)·Xγexp(( βn
1i)⊤X)gGn(Y|X) +R2(X, Y),
where R2(X, Y)is a Taylor remainder such that R2(X, Y)/D1(Gn, G∗)→0asn→ ∞ . Putting
the above results together, we rewrite the quantity Hnas follows:
Hn=KX
i=1X
0≤|η1|+η2≤2Un
i,η1,η2·Xη1exp(( β∗
1i)⊤X)∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i)
+KX
i=1X
0≤|γ|≤1Wn
i,γ·Xγexp(( β∗
1i)⊤X)gGn(Y|X) +R1(X, Y) +R2(X, Y), (18)
in which we respectively define for each i∈[K]that
Un
i,η1,η2:=X
α∈Jη1,η2exp(βn
0i)
2α4α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4,
Wn
i,γ:=−exp(βn
0i)
γ!(∆βn
1i)γ,
for any (η1, η2)̸= (0d,0)and|γ| ̸=0d. Otherwise, Un
i,0d,0=−Wn
i,0d:= exp( βn
0i)−exp(β∗
0i).
Step 2 - Non-vanishing coefficients :
Moving to the second step, we will show that not all the ratios Un
i,η1,η2/D1(Gn, G∗)tend to zero
asngoes to infinity. Assume by contrary that all of them approach zero when n→ ∞ , then for
(η1, η2) = (0d,0), it follows that
1
D1(Gn, G∗)·KX
i=1exp(βn
0i)−exp(β∗
0i)=KX
i=1|Un
j,η1,η2|
D1(Gn, G∗)→0. (19)
Additionally, for tuples (η1, η2)where η1∈ {e1, e2, . . . , e d}withej:= (0 , . . . , 0,1|{z}
j−th,0, . . . , 0)
andη2= 0, we get
1
D1(Gn, G∗)·KX
i=1exp(βn
0i)∥∆βn
1i∥1=KX
i=1|Un
j,η1,η2|
D1(Gn, G∗)→0.
By using similar arguments, we end up having
1
D1(Gn, G∗)·KX
i=1exp(βn
0i)h
∥∆βn
1i∥1+∥∆an
i∥1+|∆bn
i|+|∆σn
i|i
→0.
Due to the topological equivalence between norm-1 and norm-2, the above limit implies that
1
D1(Gn, G∗)·KX
i=1exp(βn
0i)h
∥∆βn
1i∥+∥∆an
i∥+|∆bn
i|+|∆σn
i|i
→0. (20)
Combine equation (19) with equation (20), we deduce that D1(Gn, G∗)/D1(Gn, G∗)→0, which is
a contradiction. Consequently, at least one among the ratios Un
i,η1,η2/D1(Gn, G∗)does not vanish as
ntends to infinity.
Step 3 - Fatou’s contradiction :
Let us denote by mnthe maximum of the absolute values of Un
i,η1,η2/D1(Gn, G∗)and
Wn
i,γ/D1(Gn, G∗). It follows from the result achieved in Step 2 that 1/mn̸→ ∞ .
18

--- PAGE 19 ---
Published as a conference paper at ICLR 2024
Recall from the hypothesis that EX[V(gGn(·|X), gG∗(·|X))]/D1(Gn, G∗)→0asn→ ∞ . Thus,
by the Fatou’s lemma, we have
0 = lim
n→∞EX[V(gGn(·|X), gG∗(·|X))]
D1(Gn, G∗)≥1
2·Z
lim inf
n→∞|gGn(Y|X)−gG∗(Y|X)|
D1(Gn, G∗)dXdY.
This result indicates that |gGn(Y|X)−gG∗(Y|X)|/D1(Gn, G∗)tends to zero as ngoes to infinity
for almost surely (X, Y). As a result, it follows that
lim
n→∞Hn
mnD1(Gn, G∗)= lim
n→∞|gGn(Y|X)−gG∗(Y|X)|
mnD1(Gn, G∗)= 0.
Next, let us denote Un
i,η1,η2/[mnD1(Gn, G∗)]→τi,η1,η2andWn
i,γ/[mnD1(Gn, G∗)]→κi,γwith
a note that at least one among them is non-zero. From the formulation of Hnin equation (18), we
deduce that
KX
i=1X
0≤|η1|+η2≤2τi,η1,η2·Xη1exp(( β∗
1i)⊤X)∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i)
+KX
i=1X
0≤|γ|≤1κi,γ·Xγexp(( β∗
1i)⊤X)gG∗(Y|X) = 0 , (21)
for almost surely (X, Y). This equation is equivalent to
KX
i=1X
0≤|η1|≤1
X
0≤η2≤2−|γ|τi,η1,η2∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +κi,η1gG∗(Y|X)

×Xη1exp(( β∗
1i)⊤X) = 0 ,
for almost surely (X, Y). Note that β∗
11, . . . , β∗
1Kadmits pair-wise different values, then
{exp(( β∗
1i)⊤X) :i∈[K]}is a linearly independent set, which leads to
X
0≤|η1|≤1
X
0≤η2≤2−|γ|τi,η1,η2∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +κi,η1gG∗(Y|X)
Xη1= 0,
for any i∈[K]for almost surely (X, Y). It is clear that the left hand side of the above equation is a
polynomial of Xbelonging to the compact set X. As a result, we get that
X
0≤η2≤2−|γ|τi,η1,η2∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +κi,η1gG∗(Y|X) = 0 ,
for any i∈[K],0≤ |η1| ≤1and almost surely (X, Y). Since (a∗
1, b∗
1, σ∗
1), . . . , (a∗
K, b∗
K, σ∗
K)have
pair-wise distinct values, those of ((a∗
1)⊤X+b∗
1, σ∗
1), . . . , ((a∗
K)⊤X+b∗
K, σ∗
K)are also pair-wise
different. Thus, the setn
∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i), gG∗(Y|X) :i∈[K]o
is linearly independent.
Consequently, we obtain that τi,η1,η2=κi,γ= 0for any i∈[K],0≤ |η1|+η2≤2and0≤ |γ| ≤1,
which contradicts the fact that at least one among these terms is different from zero.
Hence, we can find some constant ε′>0such that
inf
G∈Ek∗(Ω):D1(G,G∗)≤ε′EX[V(gG(·|X), gG∗(·|X))]
D1(G, G∗)>0.
Proof of claim (15): Assume by contrary that this claim is not true, then we can seek a sequence
G′
n∈ Ek∗(Ω)such that D1(G′
n, G∗)> ε′and
lim
n→∞EX[V(gG′n(·|X), gG∗(·|X))]
D1(G′n, G∗)= 0,
which directly implies that EX[V(gG′n(·|X), gG∗(·|X))]→0asn→ ∞ . Recall that Ωis a compact
set, therefore, we can replace the sequence G′
nby one of its subsequences that converges to a mixing
measure G′∈ Ek∗(Ω). Since D1(G′
n, G∗)> ε′, this result induces that D1(G′, G∗)> ε′.
19

--- PAGE 20 ---
Published as a conference paper at ICLR 2024
Subsequently, by means of the Fatou’s lemma, we achieve that
0 = lim
n→∞EX[2V(gG′n(·|X), gG∗(·|X))]≥Z
lim inf
n→∞gG′n(Y|X)−gG∗(Y|X)dµ(Y)ν(X).
It follows that gG′(Y|X) =gG∗(Y|X)for almost surely (X, Y). From Proposition 2, we know that
the top-K sparse softmax gating Gaussian mixture of experts is identifiable, thus, we obtain that
G′≡G∗. As a consequence, D1(G′, G∗) = 0 , contradicting the fact that D1(G′, G∗)> ε′>0.
Hence, the proof is completed.
A.3 P ROOF OF LEMMA 1
Letηi=Miε, where εis some fixed positive constant and Miwill be chosen later. For an arbitrary
ℓ∈[q], since XandΩare bounded sets, there exists some constant c∗
ℓ≥0such that
min
x,i,i′h
(β∗
1i)⊤x−(β∗
1i′)⊤xi
=c∗
ℓε, (22)
where the minimum is subject to x∈ X∗
ℓ, i∈ {ℓ1, . . . , ℓ K}andi′∈ {ℓK+1, . . . , ℓ k∗}. We will
point out that c∗
ℓ>0. Assume by contrary that c∗
ℓ= 0. For x∈ X∗
ℓ, we may assume for any
1≤i < j≤k∗that
(β∗
1ℓi)⊤x≥(β∗
1ℓj)⊤x.
Since c∗
ℓ= 0, it follows from equation (22) that (β∗
1ℓK)⊤x−(β∗
1ℓK+1)⊤x= 0, or equivalently
(β∗
1ℓK−β∗
1ℓK+1)⊤x= 0.
In other words, X∗
ℓis a subset of
Z:={x∈ X: (β∗
1ℓK−β∗
1ℓK+1)⊤x= 0}.
Since β1ℓK−β1ℓK+1̸=0dand the distribution of Xis continuous, it follows that the set Zhas
measure zero. Since X∗
ℓ⊆ Z , we can conclude that X∗
ℓalso has measure zero, which contradicts the
hypothesis of Lemma 1. Therefore, we must have c∗
ℓ>0.
AsXis a bounded set, we assume that ∥x∥ ≤Bfor any x∈ X. Letx∈ X∗
ℓ, then we have for any
i∈ {ℓ1, . . . , ℓ K}andi′∈ {ℓK+1, . . . , ℓ k∗}that
β⊤
1ix= (β1i−β∗
1i)⊤x+ (β∗
1i)⊤x
≥ −MiεB+ (β∗
1i′)⊤x+c∗
ℓε
=−MiεB+c∗
ℓε+ (β∗
1i′−β1i′)⊤x+β⊤
1i′x
≥ −2MiεB+ +c∗
ℓε+β⊤
1i′x.
By setting Mi≤c∗
ℓ
2B, we get that x∈ Xℓ, which means that X∗
ℓ⊆ Xℓ. Similarly, assume that there
exists some constant cℓ≥0that satisfies
min
x,i,i′h
(β∗
1i)⊤x−(β∗
1i′)⊤xi
=c∗
ℓε.
Here, the above minimum is subject to x∈ X ℓ,i∈ {ℓ1, . . . , ℓ K}andi′∈ {ℓK+1, . . . , ℓ k∗}. If
Mi≤cℓ
2B, then we also receive that Xℓ⊆ X∗
ℓ.
Hence, if we set Mi=1
2Bmin{c∗
ℓ, cℓ}, we reach the conclusion that X∗
ℓ=Xℓ.
B P ROOF FOR RESULTS UNDER OVER-SPECIFIED SETTINGS
In this appendix, we first provide the proofs of Theorem B.1 and Theorem 4 in Appendix B.1 and
Appendix B.2, respectively. Subsequently, we present the proof for Proposition 1 in Appendix B.3,
while that for Lemma 2 is put in Appendix B.4.
20

--- PAGE 21 ---
Published as a conference paper at ICLR 2024
B.1 P ROOF OF THEOREM 3
In this appendix, we follow proof techniques presented in Appendix A.1 to demonstrate the result of
Theorem 3. Recall that under the over-specified settings, the MLE bGnbelongs to the set of all mixing
measures with at most k > k ∗components, i.e. Ok(Ω). Interestingly, if we can adapt the result of
part (i) of Lemma 3 to the over-specified settings, then other results presented in Appendix A.1 will
also hold true. Therefore, our main goal is to derive following bound for any 0< η < 1/2under the
over-specified settings:
logN(η,Pk(Ω),∥ · ∥1)≲log(1/η),
where Pk(Ω) := {gG(Y|X) :G∈ O k(Ω)}. For ease of presentation, we will reuse the notations
defined in Appendix A.1 with Ek∗(Ω)being replaced by Ok(Ω). Now, let us recall necessary notations
for this proof.
Firstly, we define Θ ={(a, b, σ )∈Rd×R×R+: (β0, β1, a, b, σ )∈Ω}, and Θηis anη-cover of
Θ. Additionally, we also denote ∆ :={(β0, β1)∈Rd×R: (β0, β1, a, b, σ )∈Ω}, and ∆ηbe an
η-cover of ∆. Next, for each mixing measure G=Pk
i=1exp(β0i)δ(β1i,ai,bi,σi)∈ Ok(Ω), we denote
G′=Pk
i=1exp(β0i)δ(β1i,ai,bi,σi)in which (ai,bi,σi)∈Θηis the closest point to (ai, bi, σi)in this
set for any i∈[k]. We also consider another mixing measure G:=Pk
i=1exp(β0i)δ(β1i,ai,bi,σi)∈
Ok(Ω)where (β0i,β1i)∈∆ηis the closest point to (β0i, β1i)in this set for any i∈[k].
Subsequently, we define
L:={gG∈ Pk(Ω) : ( β0i,β1i)∈∆η,(ai,bi,σi)∈Θη}.
We demonstrate that Lis anη-cover of the metric space (Pk(Ω),∥ · ∥1), that is, for any gG∈ Pk(Ω),
there exists a density gG∈ L such that ∥gG−gG∥1≤η. By the triangle inequality, we have
∥gG−gG∥1≤ ∥gG−gG′∥1+∥gG′−gG∥1. (23)
From the formulation of G′, we get that
∥gG−gG′∥1≤kX
i=1Z
X×Yf(Y|a⊤
iX+bi, σi)−f(Y|a⊤
iX+bi,σi)d(X, Y)
≲kX
i=1Z
X×Y
∥ai−ai∥+|bi−bi|+|σi−σi|
d(X, Y)
≲η (24)
Based on inequalities in equations (23) and(24), it is sufficient to show that ∥gG′−gG∥1≲η. For
anyℓ∈[q], let us define
Xℓ:={x∈ X: (β1i)⊤x≥(β1i′)⊤x,∀i∈ {ℓ1, . . . , ℓK}, i′∈ {ℓK+1, . . . , ℓk}},
X′
ℓ:={x∈ X: (β1i)⊤x≥(β1i′)⊤x,∀i∈ {ℓ1, . . . , ℓK}, i′∈ {ℓK+1, . . . , ℓk}}.
Since the Softmax function is differentiable, it is a Lipschitz function with some Lipschitz constant
L≥0. Assume that ∥X∥ ≤Bfor any X∈ X and denote
πℓ(X) :=
β⊤
1ℓix+β⊤
0ℓiK
i=1; πℓ(X) :=
β⊤
1ℓix+β⊤
0ℓiK
i=1,
for any K-element subset {ℓ1, . . .ℓK}of{1, . . . , k }. Then, we have
∥Softmax( πℓ(X))−Softmax( πℓ(X))∥ ≤L· ∥πℓ(X)−πℓ(X)∥
≤L·KX
i=1
∥β1ℓi−β1ℓi∥ · ∥X∥+|β0ℓi−β0ℓi|
≤L·KX
i=1
ηB+η
≲η.
21

--- PAGE 22 ---
Published as a conference paper at ICLR 2024
By arguing similarly to the proof of Lemma 2 in Appendix B.4, we receive that either Xℓ=X′
ℓor
Xℓhas measure zero for any ℓ∈[q]. As a result, we deduce that
∥gG′−gG∗∥1≤qX
ℓ=1Z
Xℓ×Y|gG′(Y|X)−gG(Y|X)|d(X, Y)
≤qX
ℓ=1Z
Xℓ×YKX
i=1Softmax( πℓ(X)i)−Softmax( πℓ(X)i)·f(Y|a⊤
ℓiX+bℓi,σℓi)d(X, Y)
≲η.
Thus,Lis anη-cover of the metric space (Pk(Ω),∥ · ∥1), which implies that
N(η,Pk(Ω),∥ · ∥1)≲|∆η| × |Θη| ≤ O (η−(d+1)k)× O(η−(d+3)k) =O(η−(2d+4)k). (25)
Hence, logN(η,Pk(Ω),∥ · ∥1)≲log(1/η).
B.2 P ROOF OF THEOREM 4
Similar to the proof of Theorem 2 in Appendix A, our objective here is also to derive the Total
Variation lower bound adapted to the over-fitted settings:
EX[V(gG(·|X), gG∗(·|X))]≳D2(G, G∗).
Since the global part of the above inequality can be argued in the same fashion as in Appendix A, we
will focus only on demonstrating the following local part via the proof by contradiction method:
lim
ε→0inf
G∈Ok(Θ):D2(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]
D2(G, G∗)>0. (26)
Assume that the above claim does not hold true, then we can find a sequence of mixing
measures Gn:=Pkn
i=1exp(βn
0i)δ(βn
1i,an
i,bn
i,σn
i)∈ O k(Ω) such that both D2(Gn, G∗)and
EX[V(gGn(·|X), gG∗(·|X))]/D2(Gn, G∗)vanish when ngoes to infinity. Additionally, by abuse of
notation, we reuse the set of V oronoi cells Cj, forj∈[k∗], defined in Appendix A. Due to the limit
D2(Gn, G∗)→0asn→ ∞ , it follows that for any j∈[k∗], we haveP
i∈Cjexp(βn
0i)→exp(β∗
0j)
and(βn
1i, an
i, bn
i, σn
i)→(β∗
1j, a∗
j, b∗
j, σ∗
j)for all i∈ Cj. WLOG, we may assume that
D2(Gn, G∗) =X
j∈[K],
|Cj|>1X
i∈Cjexp(βn
0i)h
∥∆βn
1ij∥¯r(|Cj|)+∥∆an
ij∥¯r(|Cj|)
2+|∆bn
ij|¯r(|Cj|)+|∆σn
ij|¯r(|Cj|)
2i
+X
j∈[K],
|Cj|=1X
i∈Cjexp(βn
0i)h
∥∆βn
1ij∥+∥∆an
ij∥+|∆bn
ij|+|∆σn
ij|i
+KX
j=1X
i∈Cjexp(βn
0i)−exp(β∗
0j).
Regarding the top- Kselection in the conditional density gG∗, we partition the covariate space Xin
a similar fashion to Appendix A. More specifically, we consider q= k∗
K
subsets {ℓ1, . . . , ℓ K}of
{1, . . . , k ∗}for any ℓ∈[q], and denote {ℓK+1, . . . , ℓ k∗}:= [k∗]\ {ℓ1, . . . , ℓ K}. Then, we define
X∗
ℓ:=n
x∈ X: (β∗
1j)⊤x≥(β∗
1j′)⊤x,∀j∈ {ℓ1, . . . , ℓ K}, j′∈ {ℓK+1, . . . , ℓ k∗}o
,
for any ℓ∈[q]. On the other hand, we need to introduce a new partition method of the covariate
space for the weight selection in the conditional density gGn. In particular, let K∈Nsuch that
max{ℓj}K
j=1⊂[k∗]PK
j=1|Cℓj| ≤K≤kandq:= k
K
. Then, for any ℓ∈[q], we denote (ℓ1, . . . , ℓk)
as a subset of [k]and{ℓK+1, . . . , ℓk}:= [k]\ {ℓ1, . . . , ℓK}. Additionally, we define
Xn
ℓ:=n
x∈ X: (βn
1i)⊤x≥(βn
1i′)⊤x,∀i∈ {ℓ1, . . . , ℓK}, i′∈ {ℓK+1, . . . , ℓk}o
.
22

--- PAGE 23 ---
Published as a conference paper at ICLR 2024
LetX∈ X∗
ℓfor some ℓ∈[q]such that {ℓ1, . . . , ℓ K}={1, . . . , K }. If{ℓ1, . . .ℓK} ̸=C1∪. . .∪CK
for any ℓ∈[q], then EX[V(gGn(·|X), gG∗(·|X))]/D2(Gn, G∗)̸→0asntends to infinity. This
contradicts the fact that this term must approach zero. Therefore, we only need to consider the
scenario when there exists ℓ∈[q]such that {ℓ1, . . .ℓK}=C1∪. . .∪ CK. Recall that we have
(βn
0i, βn
1i)→(β∗
0j, β∗
1j)asn→ ∞ for any j∈[k∗]andi∈ Cj. Thus, for any arbitrarily small
ηj>0, we have that ∥βn
1i−β∗
1j∥ ≤ηjand|βn
0i−β∗
0j| ≤ηjfor sufficiently large n. Then, it follows
from Lemma 2 that X∗
ℓ=Xn
ℓfor sufficiently large n. This result indicates that X∈ Xn
ℓ.
Then, we can represent the conditional densities gG∗(Y|X)andgGn(Y|X)for any sufficiently large
nas follows:
gG∗(Y|X) =KX
j=1exp(( β∗
1j)⊤X+β∗
0j)
PK
j′=1exp(( β∗
1j′)⊤X+β∗
0j′)·f(Y|(a∗
j)⊤X+b∗
j, σ∗
j),
gGn(Y|X) =KX
j=1X
i∈Cjexp(( βn
1i)⊤X+βn
0i)PK
j′=1P
i′∈Cj′exp(( βn
1i′)⊤X+βn
0i′)·f(Y|(an
i)⊤X+bn
i, σn
i).
Now, we reuse the three-step framework in Appendix A.
Step 1 - Taylor expansion :
Firstly, by abuse of notations, let us consider the quantity
Hn:=hKX
j=1exp(( β∗
1j)⊤X+β∗
0j)i
·[gGn(Y|X)−gG∗(Y|X)].
Similar to Step 1 in Appendix A, we can express this term as
Hn=KX
j=1X
i∈Cjexp(βn
0i)h
exp(( βn
1i)⊤X)f(Y|(an
i)⊤X+bn
i, σn
i)−exp(( β∗
1j)⊤X)f(Y|(a∗
j)⊤X+b∗
j, σ∗
j)i
−KX
j=1X
i∈Cjexp(βn
0i)h
exp(( βn
1i)⊤X)gGn(Y|X)−exp(( β∗
1j)⊤X)gGn(Y|X)i
+KX
j=1hX
i∈Cjexp(βn
0i)−exp(β∗
0j)ih
exp(( β∗
1j)⊤X)f(Y|(a∗
i)⊤X+b∗
i, σ∗
i)−exp(( β∗
1j)⊤X)gGn(Y|X)i
:=An+Bn+En.
Next, we proceed to decompose Anbased on the cardinality of the V oronoi cells as follows:
An=X
j:|Cj|=1X
i∈Cjexp(βn
0i)h
exp(( βn
1i)⊤X)f(Y|(an
i)⊤X+bn
i, σn
i)−exp(( β∗
1j)⊤X)f(Y|(a∗
j)⊤X+b∗
j, σ∗
j)i
+X
j:|Cj|>1X
i∈Cjexp(βn
0i)h
exp(( βn
1i)⊤X)f(Y|(an
i)⊤X+bn
i, σn
i)−exp(( β∗
1j)⊤X)f(Y|(a∗
j)⊤X+b∗
j, σ∗
j)i
.
By applying the Taylor expansions of order 1 and ¯r(|Cj|)to the first and second terms of An,
respectively, and following the derivation in equation (16), we arrive at
An=X
j:|Cj|=1X
i∈CjX
1≤|η1|+η2≤2X
α∈Jη1,η2exp(βn
0i)
2α4α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4
×Xη1exp(( β∗
1i)⊤X)·∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +R3(X, Y)
+X
j:|Cj|>1X
i∈CjX
1≤|η1|+η2≤2¯r(|Cj|)X
α∈Jη1,η2exp(βn
0i)
2α4α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4
×Xη1exp(( β∗
1i)⊤X)·∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +R4(X, Y),
23

--- PAGE 24 ---
Published as a conference paper at ICLR 2024
where the set Jη1,η2is defined in equation (17) while Ri(X, Y)is a Taylor remainder such that
Ri(X, Y)/D2(Gn, G∗)→0asn→ ∞ fori∈ {3,4}. Similarly, we also decompose Bnaccording
to the V oronoi cells as Anbut then invoke the Taylor expansions of order 1 and 2 to the first term and
the second term, respectively. In particular, we get
Bn=−X
j:|Cj|=1X
i∈CjX
|γ|=1exp(βn
0i)
γ!(∆βn
1i)·Xγexp(( βn
1i)⊤X)gGn(Y|X) +R5(X, Y)
−X
j:|Cj|>1X
i∈CjX
1≤|γ|≤2exp(βn
0i)
γ!(∆βn
1i)·Xγexp(( βn
1i)⊤X)gGn(Y|X) +R6(X, Y),
where R5(X, Y)andR6(X, Y)are Taylor remainders such that their ratios over D2(Gn, G∗)ap-
proach zero as n→ ∞ . Subsequently, let us define
Sn
j,η1,η2:=X
i∈CjX
α∈Jη1,η2exp(βn
0i)
2α4α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4,
Tn
j,γ:=X
i∈Cjexp(βn
0i)
γ!(∆βn
1i)γ,
for any (η1, η2)̸= (0d,0)and|γ| ̸=0d, while for (η1, η2) = (0d,0)we set
Sn
i,0d,0=−Tn
i,0d:=X
i∈Cjexp(βn
0i)−exp(β∗
0i).
As a consequence, it follows that
Hn=KX
j=12¯r(|Cj|)X
|η1|+η2=0Sn
j,η1,η2·Xη1exp(( β∗
1i)⊤X)·∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i)
+KX
j=11+1{|Cj|>1}X
|γ|=0Tn
j,γ·Xγexp(( βn
1i)⊤X)gGn(Y|X) +R5(X, Y) +R6(X, Y). (27)
Step 2 - Non-vanishing coefficients :
In this step, we will prove by contradiction that at least one among the ratios Sn
j,η1,η2/D2(Gn, G∗)
does not converge to zero as n→ ∞ . Assume that all these terms go to zero, then by employing
arguments for deriving equations (19) and (20), we get that
1
D2(Gn, G∗)·hKX
j=1X
i∈Cjexp(βn
0i)−exp(β∗
0j)
+X
j:|Cj|=1X
i∈Cjexp(βn
0i)
∥∆βn
1ij∥+∥∆an
ij∥+|∆bn
ij|+|∆σn
ij|i
→0.
Combine this limit with the representation of D2(Gn, G∗), we have that
1
D2(Gn, G∗)·X
j:|Cj|>1X
i∈Cjexp(βn
0i)
∥∆βn
1ij∥¯r(|Cj|)+∥∆an
ij∥¯r(|Cj|)
2+|∆bn
ij|¯r(|Cj|)+|∆σn
ij|¯r(|Cj|)
2
̸→0.
This result implies that we can find some index j′∈[K] :|Cj′|>1that satisfies
1
D2(Gn, G∗)·X
i∈Cj′exp(βn
0i)
∥∆βn
1ij′∥¯r(|Cj′|)+∥∆an
ij′∥¯r(|Cj′|)
2+|∆bn
ij′|¯r(|Cj′|)+|∆σn
ij′|¯r(|Cj′|)
2
̸→0.
For simplicity, we may assume that j′= 1. Since Sn
1,η1,η2/D2(Gn, G∗)vanishes as n→ ∞ for any
(η1, η2)∈Nd×Nsuch that 1≤ |η1|+η2≤¯r(|Cj|), we divide this term by the left hand side of the
above equation and achieve that
P
i∈C1P
α∈Jη1,η2exp(βn
0i)
2α4α!(∆βn
1i1)α1(∆an
i1)α2(∆bn
i1)α3(∆σn
i1)α4
P
i∈C1exp(βn
0i)
∥∆bn
i1∥¯r(|C1|)+∥∆an
i1∥¯r(|C1|)
2+|∆bn
i1|¯r(|C1|)+|∆σn
i1|¯r(|C1|)
2→0, (28)
24

--- PAGE 25 ---
Published as a conference paper at ICLR 2024
for any (η1, η2)∈Nd×Nsuch that 1≤ |η1|+η2≤¯r(|C1|).
Subsequently, we define Mn:= max {∥∆βn
1i1∥,∥∆an
i1∥1/2,|∆bn
i1|,|∆σn
i1|1/2:i∈ C1}andpn:=
max{exp(βn
0i) :i∈ C1}. As a result, the sequence exp(βn
0i)/pnis bounded, which indicates that
we can substitute it with its subsequence that admits a positive limit z2
5i:= lim n→∞exp(βn
0i)/pn.
Therefore, at least one among the limits z2
5iequals to one. Furthermore, we also denote
(∆βn
1i1)/Mn→z1i,(∆an
i1)/Mn→z2i,(∆bn
i1)/Mn→z3i,(∆σn
i1)/(2Mn)→z4i.
From the above definition, it follows that at least one among the limits z1i, z2i, z3iandz4iequals to
either 1 or −1. By dividing both the numerator and the denominator of the term in equation (28) by
pnM|η1|+η2n , we arrive at the following system of polynomial equations:
X
i∈C1X
α∈Jη1,η2z2
5izα1
1izα2
2izα3
3izα4
4i
α1!α2!α3!α4!= 0,
for all (η1, η2)∈Nd×N: 1≤ |η1|+η2≤¯r(|C1|). Nevertheless, from the definition of ¯r(|C1|),
we know that the above system does not admit any non-trivial solutions, which is a contradiction.
Consequently, not all the ratios Sn
j,η1,η2/D2(Gn, G∗)tend to zero as ngoes to infinity.
Step 3 - Fatou’s contradiction :
It follows from the hypothesis that EX[V(gGn(·|X), gG∗(·|X))]/D2(Gn, G∗)→0asn→ ∞ .
Then, by applying the Fatou’s lemma, we get
0 = lim
n→∞EX[V(gGn(·|X), gG∗(·|X))]
D2(Gn, G∗)=1
2·Z
lim inf
n→∞|gGn(Y|X)−gG∗(Y|X)|
D2(Gn, G∗)dXdY,
which implies that |gGn(Y|X)−gG∗(Y|X)|/D2(Gn, G∗)→0asn→ ∞ for almost surely (X, Y).
Next, we define mnas the maximum of the absolute values of Sn
j,η1,η2/D2(Gn,G∗). It follows from
Step 2 that 1/mn̸→ ∞ . Moreover, by arguing in the same way as in Step 3 in Appendix A, we
receive that
Hn/[mnD2(Gn, G∗)]→0 (29)
asn→ ∞ . By abuse of notations, let us denote
Sn
j,η1,η2/[mnD2(Gn, G∗)]→τj,η1,η2,
Tn
j,γ/[mnD2(Gn, G∗)]→κj,γ.
Here, at least one among τj,η1,η2, κj,γis non-zero. Then, by putting the results in equations (27) and
(29) together, we get
KX
i=12¯r(|Cj|)X
|η1|+η2=0τi,η1,η2·Xη1exp(( β∗
1i)⊤X)∂η2f
∂hη2
1(Y|(a∗
i)⊤X+b∗
i, σ∗
i)
+KX
i=11+1{|Cj|>1}X
|γ|=0κi,γ·Xγexp(( β∗
1i)⊤X)gG∗(Y|X) = 0 .
Arguing in a similar fashion as in Step 3 of Appendix A, we obtain that τj,η1,η2=κj,γ= 0for any
j∈[K],0≤ |η1|+η2≤2¯r(|Cj|)and0≤ |γ| ≤1 +1{|Cj|>1}. This contradicts the fact that at
least one among them is non-zero. Hence, the proof is completed.
B.3 P ROOF OF PROPOSITION 1
Since the Hellinger distance is lower bounded by the Total Variation distance, i.e. h≥V, it is
sufficient to show that
inf
G∈Ok(Ω)EX[V(gG(·|X), gG∗(·|X))]>0.
25

--- PAGE 26 ---
Published as a conference paper at ICLR 2024
For that purpose, we first demonstrate that
lim
ε→0inf
G∈Ok(Ω):D2(G,G∗)≤εEX[V(g(·|X), gG∗(·|X))]>0. (30)
Assume by contrary that the above claim is not true, then we can find a sequence Gn=Pkn
i=1exp(βn
0i)δ(βn
1i,an
i,bn
i,σn
i)∈ Ok(Ω)that satisfies D2(Gn, G∗)→0and
EX[V(gGn(·|X), gG∗(·|X))]→0
when ntends to infinity. By applying the Fatou’s lemma, we have
0 = lim
n→∞EX[V(gGn(·|X), gG∗(·|X))]
≥1
2Z
X×Ylim inf
n→∞|gGn(Y|X)−gG∗(Y|X)|d(X, Y). (31)
The above results indicates that gGn(Y|X)−gG∗(Y|X)→0asn→ ∞ for almost surely (X, Y).
WLOG, we may assume that
max
{ℓ1,...,ℓK}KX
j=1|Cℓj|=|C1|+|C2|+. . .+|CK|.
Let us consider X∈ X∗
ℓ, where ℓ∈[q]such that {ℓ1, . . . , ℓ K}={1, . . . , K }. Since D2(Gn, G∗)
converges to zero, it follows that (βn
1i, an
i, bn
i, σn
i)→(β∗
1j, a∗
j, b∗
j, σ∗
j)andP
i∈Cjexp(βn
0i)→
exp(β∗
0j)for any i∈ Cjandj∈[k∗]. Thus, we must have that X∈Xℓfor some ℓ∈[q]such that
{ℓ1, . . . , ℓK}=C1∪. . .∪ CK. Otherwise, gGn(Y|X)−gG∗(Y|X)̸→0, which is a contradiction.
However, as K <PK
j=1|Cj|, the fact that {ℓ1, . . . , ℓK}=C1∪. . .∪ CKcannot occur. Therefore,
we reach the claim in equation (30). Consequently, there exists some positive constant ε′such that
inf
G∈Ok(Ω):D2(G,G∗)≤ε′EX[V(gG(·|X), gG∗(·|X))]>0.
Given the above result, it suffices to point out that
inf
G∈Ok(Ω):D2(G,G∗)>ε′EX[V(gG(·|X), gG∗(·|X))]>0. (32)
We continue to use the proof by contradiction method here. In particular, assume that the inequal-
ity(32) does not hold, then there exists a sequence of mixing measures G′
n∈ O k(Ω)such that
D2(G′
n, G∗)> ε′and
EX[V(gG′n(·|X), gG∗(·|X))]→0.
By invoking the Fatou’s lemma as in equation (31), we get that gG′n(Y|X)−gG∗(Y|X)→0
asn→ ∞ for almost surely (X, Y). Since Ωis a compact set, we can substitute (Gn)with its
subsequence which converges to some mixing measure G′∈ O k(Ω). Then, the previous limit
implies that gG′(Y|X) =gG∗(Y|X)for almost surely (X, Y). From the result of Proposition 2 in
Appendix C, we know that the top-K sparse softmax gating Gaussian MoE is identifiable. Therefore,
we obtain that G′≡G∗, or equivalently, D2(G′, G∗) = 0
On the other hand, due to the hypothesis D2(G′
n, G∗)> ε′for any n∈N, we also get that
D2(G′, G∗)> ε′>0, which contradicts the previous result. Hence we reach the claim in equa-
tion (32) and totally completes the proof.
B.4 P ROOF OF LEMMA 2
Letηj=Mjε, where εis some fixed positive constant and Miwill be chosen later. As XandΩare
bounded sets, we can find some constant c∗
ℓ≥0such that
min
x,j,j′h
(β∗
1j)⊤x−(β∗
1j′)⊤xi
=c∗
ℓε,
where the above minimum is subject to x∈ X∗
ℓ, j∈ {ℓ1, . . . , ℓ K}andj′∈ {ℓK+1, . . . , ℓ k∗}. By
arguing similarly to the proof of Lemma 1 in Appendix A.3, we deduce that c∗
ℓ>0.
26

--- PAGE 27 ---
Published as a conference paper at ICLR 2024
SinceXis a bounded set, we may assume that ∥x∥ ≤Bfor any x∈ X. Let x∈ X∗
ℓandℓ∈[q]
such that {ℓ1, . . . , ℓK}=Cℓ1∪. . .∪ CℓK. Then, for any i∈ {ℓ1, . . . , ℓK}andi′∈ {ℓK+1, . . . , ℓk},
we have that
β⊤
1ix= (β1i−β∗
1j)⊤x+ (β∗
1j)⊤x
≥ −MiεB+ (β∗
1j′)⊤x+c∗
ℓε
=−MiεB+c∗
ℓε+ (β∗
1j′−β1i′)⊤x+β⊤
1i′x
≥ −2MiεB+c∗
ℓε+β⊤
1i′x,
where j∈ {ℓ1, . . . , ℓ K}andj′∈ {ℓK+1, . . . , ℓ k∗}such that i∈ Cjandi′∈ Cj′. IfMj≤c∗
ℓ
2B, then
we get that x∈ Xℓ, which leads to X∗
ℓ⊆Xℓ.
Analogously, assume that there exists some constant cℓ≥0such that
min
x,j,j′h
(β∗
1j)⊤x−(β∗
1j′)⊤xi
=c∗
ℓε,
where the minimum is subject to x∈Xℓ,i∈ {ℓ1, . . . , ℓK}andi′∈ {ℓK+1, . . . , ℓk}. Then, if
Mj≤cℓ
2B, then we receive that Xℓ⊆ X∗
ℓ.
As a consequence, by setting Mj=1
2Bmin{c∗
ℓ, cℓ}, we achieve the conclusion that Xℓ=X∗
ℓ.
C I DENTIFIABILITY OF THE TOP-K S PARSE SOFTMAX GATING GAUSSIAN
MIXTURE OF EXPERTS
In this appendix, we study the identifiability of the top-K sparse softmax gating Gaussian MoE, which
plays an essential role in ensuring the convergence of the MLE bGnto the true mixing measure G∗
under V oronoi loss functions.
Proposition 2 (Identifiability) .LetGandG′be two arbitrary mixing measures in Ok(Θ). Suppose
that the equation gG(Y|X) =gG′(Y|X)holds for almost surely (X, Y)∈ X × Y , then it follows
thatG≡G′.
Proof of Proposition 2. First, we assume that two mixing measures GandG′take the follow-
ing forms: G=Pk
i=1exp(β0i)δ(β1i,ai,bi,σi)andG′=Pk′
i=1exp(β′
0i)δ(β′
1i,a′
i,b′
i,σ′
i). Recall that
gG(Y|X) =gG′(Y|X)for almost surely (X, Y), then we have
kX
i=1Softmax(TopK(( β1i)⊤X, K ;β0i))·f(Y|a⊤
iX+bi, σi)
=k′X
i=1Softmax(TopK(( β′
1i)⊤X, K ;β′
0i))·f(Y|(a′
i)⊤+b′
i, σ′
i). (33)
Due to the identifiability of the location-scale Gaussian mixtures Teicher (1960; 1961; 1963), we get
thatk=k′and
n
Softmax(TopK(( β1i)⊤X, K ;β0i)) :i∈[k]o
≡n
Softmax(TopK(( β′
1i)⊤X, K ;β′
0i)) :i∈[k]o
,
for almost surely X. WLOG, we may assume that
Softmax(TopK(( β1i)⊤X, K ;β0i)) = Softmax(TopK(( β′
1i)⊤X, K ;β′
0i)), (34)
for almost surely Xfor any i∈[k]. Since the Softmax function is invariant to translations, it follows
from equation (34) thatβ1i=β′
1i+v1andβ0i=β′
0i+v0for some v1∈Rdandv0∈R. Notably,
from the assumption of the model, we have β1k=β′
1k=0dandβ0k=β′
0k= 0, which implies that
v1=0dandv0= 0. As a result, we obtain that β1i=β′
1iandβ0i=β′
0ifor any i∈[k].
27

--- PAGE 28 ---
Published as a conference paper at ICLR 2024
Let us consider X∈ Xℓwhere ℓ∈[q]such that {ℓ1, . . . , ℓ K}={1, . . . , K }. Then, equation (33)
can be rewritten as
KX
i=1exp(β0i) exp( β⊤
1iX)f(Y|a⊤
iX+bi, σi) =KX
i=1exp(β0i) exp( β⊤
1iX)f(Y|(a′
i)⊤X+b′
i, σ′
i),
(35)
for almost surely (X, Y). Next, we denote J1, J2, . . . , J mas a partition of the index set [k], where
m≤k, such that exp(β0i) = exp( β0i′)for any i, i′∈Jjandj∈[m]. On the other hand, when
iandi′do not belong to the same set Jj, we let exp(β0i)̸= exp( β0i′). Thus, we can reformulate
equation (35) as
mX
j=1X
i∈Jjexp(β0i) exp( β⊤
1iX)f(Y|a⊤
iX+bi, σi)
=mX
j=1X
i∈Jjexp(β0i) exp( β⊤
1iX)f(Y|(a′
i)⊤X+b′
i, σ′
i),
for almost surely (X, Y). This results leads to {((ai)⊤X+bi, σi) :i∈Jj} ≡ { ((a′
i)⊤X+b′
i, σ′
i) :
i∈Jj}, for almost surely Xfor any j∈[m]. Therefore, we have
{(ai, bi, σi) :i∈Jj} ≡ { (a′
i, b′
i, σ′
i) :i∈Jj},
for any j∈[m]. As a consequence,
G=mX
j=1X
i∈Jjexp(β0i)δ(β1i,ai,bi,σi)=mX
j=1X
i∈Jjexp(β′
0i)δ(β′
1i,a′
i,b′
i,σ′
i)=G′.
Hence, we reach the conclusion of this proposition.
D N UMERICAL EXPERIMENTS
In this appendix, we conduct a few numerical experiments to illustrate the theoretical convergence
rates of the MLE bGnto the true mixing measure G∗under both the exact-specified and the over-
specified settings.
D.1 E XPERIMENTAL SETUP
Synthetic Data. First, we assume that the true mixing measure G∗=Pk∗
i=1exp(β∗
0i)δ(β∗
1i,a∗
i,b∗
i,σ∗
i)
is of order k∗= 2and associated with the following ground-truth parameters:
β∗
01=−8, β∗
11= 25, a∗
1=−20, b∗
1= 15, σ∗
1= 0.3,
β∗
02= 0, β∗
12= 0, a∗
2= 20, b∗
2=−5, σ∗
2= 0.4.
Then, we generate i.i.d samples {(Xi, Yi)}n
i=1by first sampling Xi’s from the uniform distribution
Uniform[0 ,1]and then sampling Yi’s from the true conditional density gG∗(Y|X)of top-K sparse
softmax gating Gaussian mixture of experts (MoE) given in equation (1). In Figure 2, we visualize
the relationship between XandYwhen the numbers of experts chosen from gG∗(Y|X)areK= 1
(Figure 2a) and K= 2(Figure 2b), respectively. However, throughout the following experiments,
we will consider only the scenario when K= 1, that is, we choose the best expert from the true
conditional density gG∗(Y|X).
Maximum Likelihood Estimation (MLE). A popular approach to determining the MLE bGnfor
each set of samples is to use the EM algorithm Dempster et al. (1977). However, since there are not
any closed-form expressions for updating the gating parameters β0i, β1iin the maximization steps,
we have to leverage an EM-based numerical scheme, which was previously used in Chamroukhi et al.
(2009). In particular, we utilize a simple coordinate gradient descent algorithm in the maximization
steps. Additionally, we select the convergence criterion of ϵ= 10−6and run a maximum of 2000
EM iterations.
28

--- PAGE 29 ---
Published as a conference paper at ICLR 2024
0.00 0.25 0.50 0.75 1.00
x7.5
5.0
2.5
0.02.55.07.510.0y
(a)K= 1
0.00 0.25 0.50 0.75 1.00
x5
051015y (b)K= 2
Figure 2: A visual representation showcasing the relationship between XandY, along with their
respective marginal distributions when K= 1andK= 2.
Initialization. For each k∈ {k∗, k∗+ 1}, we randomly distribute elements of the set {1,2, ..., k}
intok∗different V oronoi cells C1,C2, . . . ,Ck∗, each contains at least one element. Moreover, we
repeat this process for each replication. Subsequently, for each j∈[k∗], we initialize parameters
β1iby sampling from a Gaussian distribution centered around its true counterpart β∗
1jwith a small
variance, where i∈ Cj. Other parameters β0i, ai, bi, σiare also initialized in a similar fashion.
D.2 E XACT -SPECIFIED SETTINGS
Under the exact-specified settings, we conduct 40 sample generations for each configuration, across
a spectrum of 200 different sample sizes nranging from 102to105. It can be seen from Figure 3a
that the MLE bGnempirically converges to the true mixing measure G∗under the V oronoi metric D1
at the rate of order eO(n−1/2), which perfectly matches the theoretical parametric convergence rate
established in Theorem 2.
D.3 O VER-SPECIFIED SETTINGS
Under the over-specified settings, we continue to generate 40 samples of size nfor each setting,
given 100 different choices of sample size n∈[102,105]. As discussed in Section 3, to guarantee
the convergence of density estimation to the true density, we need to select K= 2 experts from
the density estimation. As far as we know, existing works, namely Kwon et al. (2019); Kwon &
Caramanis (2020); Kwon et al. (2021), only focus on the global convergence of the EM algorithm for
parameter estimation under the input-free gating MoE, while that under the top-K sparse softmax
gating MoE has remained poorly understood. Additionally, it is worth noting that the sample size
must be sufficiently large so that the empirical convergence rate of the MLE returned by the EM
algorithm aligns with the theoretical rate of order eO(n−1/2)derived in Theorem 4.
E A DDITIONAL RESULTS
In this appendix, we study the convergence rates of parameter estimation under the model (1)when f
is a probability density function of an arbitrary location-scale distribution. For that purpose, we first
characterize the family of probability density functions of location-scale distributions
F:={f(Y|h1(X, a, b ), σ) : (a, b, σ )∈Θ}, (36)
29

--- PAGE 30 ---
Published as a conference paper at ICLR 2024
6 8 10
log(sample size)3
2
1
01log(loss)3.6n0.54588
1(Gn,G*)
(a) Exact-specified settings with K= 1
10.0 10.5 11.0 11.5
log(sample size)2
0246log(loss)6.2n0.53253
2(Gn,G*)
 (b) Over-specified settings with K= 1andK= 2
Figure 3: Log-log scaled plots illustrating simulation results under the exact-specified and the over-
specified settings. We analyze the MLE bGnacross 40 independent samples, spanning sample sizes
from 102to105. The blue curves depict the mean discrepancy between the MLE bGnand the true
mixing measure G∗, accompanied by error bars signifying two empirical standard deviations under
the exact-specified settings. Additionally, an orange dash-dotted line represents the least-squares
fitted linear regression line for these data points.
where h1(X, a, b ) :=a⊤X+bstands for the location, σdenotes the scale and Θis a compact subset
ofRd×R×R+, based on the following notion of strong identifiability, which was previously studied
in Manole & Ho (2022) and Ho & Nguyen (2016):
Definition 1 (Strong Identifiability) .We say that the family Fis strongly identifiable if the probability
density function f(Y|h1(X, a, b ), σ)is twice differentiable w.r.t its parameters and the following
assumption holds true:
For any k≥1andkpairwise different tuples (a1, b1, σ1), . . . , (ak, bk, σk)∈Θ, if there exist real
coefficients α(i)
ℓ1,ℓ2, fori∈[k∗]and0≤ℓ1+ℓ2≤2, such that
kX
i=12X
ℓ1+ℓ2=0α(i)
ℓ1,ℓ2·∂ℓ1+ℓ2f
∂hℓ1
1∂σℓ2(Y|h1(X, a i, bi), σ(X, σ i)) = 0 ,
for almost surely (X, Y), then we obtain that α(i)
ℓ1,ℓ2= 0for any i∈[k∗]and0≤ℓ1+ℓ2≤2.
Example 1. The families of Student’s t-distributions and Laplace distributions are strongly identifi-
able, while the family of location-scale Gaussian distributions is not.
In high level, we need to establish the Total Variation lower bound EX[V(gG(·|X), gG∗(·|X))]≳
D(G, G∗)for any G∈ O k(Ω). Then, this bound together with the density estimation rate in
Theorem 1 (resp. Theorem 3) leads to the parameter estimation rates in Theorem 2 (resp. Theorem 4).
Here, the key step is to decompose the difference gbGn(Y|X)−gG∗(Y|X)into a combination of
linearly independent terms using Taylor expansions. Therefore, we have to involve the above notion
of strong identifiability, and separate our convergence analysis based on that notion.
Subsequently, since the convergence rates of maximum likelihood estimation when Fis a family of
location-scale Gaussian distributions, which is not strongly identifiable, have already been studied in
Section 2 and Section 3, we will focus on the scenario when the family Fis strongly identifiable
in the sequel. Under that assumption, the density f(Y|h1, σ)is twice differentiable in (h1, σ),
therefore, it is also Lipschitz continuous. As a consequence, the density estimation rates under both
the exact-specified and over-specified in Theorem 1 and Theorem 3 still hold true. Therefore, we aim
to establish the parameter estimation rates under those settings in Appendix E.1 and Appendix E.2,
respectively.
30

--- PAGE 31 ---
Published as a conference paper at ICLR 2024
E.1 E XACT -SPECIFIED SETTINGS
In this appendix, by using the V oronoi loss function D1(G, G∗)defined in equation (5), we demon-
strate in the following theorem that the rates for estimating true parameters exp(β∗
0i), β∗
1i, a∗
i, b∗
i, σ∗
i
are of parametric order eO(n−1/2), which totally match those in Theorem 2.
Theorem 5. Under the exact-specified settings, if the family Fis strongly identifiable, then the
Hellinger lower bound EX[h(gG(·|X), gG∗(·|X))]≳D1(G, G∗)holds for any mixing measure
G∈ Ek∗(Ω). Consequently, we can find a universal constant C3>0depending only on G∗,Ωand
Ksuch that
P
D1(bGn, G∗)> C3p
log(n)/n
≲n−c3,
where c3>0is a universal constant that depends only on Ω.
Proof of Theorem 5 is in Appendix E.3.1.
E.2 O VER-SPECIFIED SETTINGS
In this appendix, we capture the convergence rates of parameter estimation under the over-specified
settings when the family Fis strongly identifiable.
Voronoi metric. It is worth noting that when Fis strongly identifiable, the interaction among expert
parameters in the second PDE (6)no longer holds true. As a result, it is not necessary to involve the
solvability of the system (7)in the formulation of the V oronoi loss as in equation (8). Instead, let us
consider the V oronoi loss function D3(G, G∗)defined as
D3(G, G∗) := max
{ℓj}K
j=1⊂[k∗](X
j∈[K],
|Cℓj|=1X
i∈Cℓjexp(β0i)h
∥∆β1iℓj∥+∥∆aiℓj∥+|∆biℓj|+|∆σiℓj|i
+X
j∈[K],
|Cℓj|>1X
i∈Cℓjexp(β0i)h
∥∆β1iℓj∥2+∥∆aiℓj∥2+|∆biℓj|2+|∆σiℓj|2i
+KX
j=1X
i∈Cℓjexp(β0i)−exp(β∗
0ℓj))
,(37)
for any mixing measure G∈ O k(Ω). Equipped with loss function, we are ready to illustrate the
convergence behavior of maximum likelihood estimation in the following theorem:
Theorem 6. Under the over-specified settings, if the family Fis strongly identifiable, then the
Hellinger lower bound EX[h(gG(·|X), gG∗(·|X))]≳D2(G, G∗)holds for any mixing measure
G∈ Ok(Ω). As a consequence, we can find a universal constant C4>0depending only on G∗,Ω
andKsuch that
P
D3(bGn, G∗)> C4p
log(n)/n
≲n−c4,
where c4>0is a universal constant that depends only on Ω.
Proof of Theorem 6 is in Appendix E.3.2. Theorem 6 indicates that true parameters β∗
1i, a∗
i, b∗
i, σ∗
i,
which are fitted by a single component, share the same estimation rates of order eO(n−1/2)as those
in Theorem 4. By contrast, the estimation rates for true parameters β∗
1i, a∗
i, b∗
i, σ∗
i, which are fitted by
more than one component, are of order eO(n−1/4). Notably, these rates are no longer determined by
the solvability of the system (7). Thus, they are significantly faster than those in Theorem 4. The
main reason for this improvement is when Fis strongly identifiable, the interaction among expert
parameters via the second PDE in equation (6) does not occur.
31

--- PAGE 32 ---
Published as a conference paper at ICLR 2024
E.3 P ROOFS OF ADDITIONAL RESULTS
E.3.1 P ROOF OF THEOREM 5
Following from the result of Theorem 1 and since the Hellinger distance is lower bounded by the Total
Variation distance, i.e. h≥V, it is sufficient to demonstrate for any mixing measure G∈ Ok(Θ)that
EX[V(gG(·|X), gG∗(·|X))]≳D1(G, G∗),
which is then respectively broken into local part and global part as follows:
inf
G∈Ek∗(Ω):D1(G,G∗)≤ε′EX[V(gG(·|X), gG∗(·|X))]
D1(G, G∗)>0, (38)
inf
G∈Ek∗(Ω):D1(G,G∗)>ε′EX[V(gG(·|X), gG∗(·|X))]
D1(G, G∗)>0, (39)
for some constant ε′>0. Subsequently, we will prove only the local part (38), while the proof of the
global part (39) can be done similarly to that in Appendix A.2.
Proof of claim (38): It is sufficient to show that
lim
ε→0inf
G∈Ek∗(Ω):D1(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]
D1(G, G∗)>0.
Assume that this inequality does not hold, then since the number of experts k∗is known in this case,
there exists a sequence of mixing measure Gn:=Pk∗
i=1exp(βn
0i)δ(βn
1i,an
i,bn
i,σn
i)∈ Ek∗(Ω)such
that both D1(Gn, G∗)andEX[V(gGn(·|X), gG∗(·|X))]/D1(Gn, G∗)approach zero as ntends to
infinity. Since D1(Gn, G∗)→0asn→ ∞ , each V oronoi cell contains only one element. Thus,
we may assume WLOG that Cj={j}for any j∈[k∗], which implies that (βn
1j, an
j, bn
j, σn
j)→
(β∗
1j, a∗
j, b∗
j, σ∗
j)andexp(βn
0j)→exp(β∗
0j)asn→ ∞ . WLOG, we assume that
D1(Gn, G∗) =KX
i=1h
exp(βn
0i)
∥∆βn
1i∥+∥∆an
i∥+∥∆bn
i∥+∥∆σn
i∥
+exp(βn
0i)−exp(β∗
0i)i
,
where we denote ∆βn
1i:=βn
1i−β∗
1i,∆an
i:=an
i−a∗
i,∆bn
i:=bn
i−b∗
iand∆σn
i:=σn
i−σ∗
i.
Subsequently, by arguing in the same fashion as in Appendix A.2, we obtain that Xn
ℓ=X∗
ℓ, where
Xn
ℓ:=n
x∈ X: (βn
1j)⊤x≥(βn
1j′)⊤x:∀j∈ {ℓ1, . . . , ℓ K}, j′∈ {ℓK+1, . . . , ℓ k∗}o
,
X∗
ℓ:=n
x∈ X: (β∗
1j)⊤x≥(β∗
1j′)⊤x:∀j∈ {ℓ1, . . . , ℓ K}, j′∈ {ℓK+1, . . . , ℓ k∗}o
,
for any ℓ∈[q]for sufficiently large n.
Letℓ∈[q]such that {ℓ1, . . . , ℓ K}={1, . . . , K }. Then, for almost surely (X, Y)∈ X∗
ℓ× Y, we
can rewrite the conditional densities gGn(Y|X)andgG∗(Y|X)as
gGn(Y|X) =KX
i=1exp(( βn
1i)⊤X+βn
0i)PK
j=1exp(( βn
1j)⊤X+βn
0j)·f(Y|(an
i)⊤X+bn
i, σn
i),
gG∗(Y|X) =KX
i=1exp(( β∗
1i)⊤X+β∗
0i)PK
j=1exp(( β∗
1j)⊤X+β∗
0j)·f(Y|(a∗
i)⊤X+b∗
i, σ∗
i).
Now, we break the rest of our arguments into three steps:
Step 1 - Taylor expansion :
32

--- PAGE 33 ---
Published as a conference paper at ICLR 2024
In this step, we take into account Hn:=hPK
i=1exp(( β∗
1i)⊤X+β∗
0i)i
·[gGn(Y|X)−gG∗(Y|X)].
Note that the quantity Hncan be represented as follows:
Hn=KX
i=1exp(βn
0i)h
exp(( βn
1i)⊤X)f(Y|(an
i)⊤X+bn
i, σn
i)−exp(( β∗
1i)⊤X)f(Y|(a∗
i)⊤X+b∗
i, σ∗
i)i
−KX
i=1exp(βn
0i)h
exp(( βn
1i)⊤X)gGn(Y|X)−exp(( β∗
1i)⊤X)gGn(Y|X)i
+KX
i=1h
exp(βn
0i)−exp(β∗
0i)ih
exp(( β∗
1i)⊤X+β∗
0i)f(Y|(a∗
i)⊤X+b∗
i, σ∗
i)−exp(( β∗
1i)⊤X)gGn(Y|X)i
.
By applying the first-order Taylor expansion to the first term in the above representation, which is
denoted by An, we get that
An=KX
i=1X
|α|=1exp(βn
0i)
α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4
×Xα1+α2exp(( β∗
1i)⊤X)·∂|α2|+α3+α4f
∂h|α2|+α3
1 ∂σα4(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +R1(X, Y),
where R1(X, Y)is a Taylor remainder that satisfies R1(X, Y)/D1(X, Y)→0asn→ ∞ . Let
η1=α1+α2∈Nd,η2=|α2|+α3∈Nandη3=α4∈N, then we can rewrite Anas follows:
An=KX
i=11X
η3=02−η3X
|η1|+η2=1−η3X
α∈Iη1,η2,η3exp(βn
0i)
α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4
×Xη1exp(( β∗
1i)⊤X)·∂η2+η3f
∂hη2
1∂ση3(Y|(a∗
i)⊤X+b∗
i, σ∗
i) +R1(X, Y),(40)
where we define
Iη1,η2,η3:={(αi)4
i=1∈Nd×Nd×N×N:α1+α2=η1,|α2|+α3=η2, α4=η3}.(41)
By arguing in a similar fashion for the second term in the representation of Hn, we also get that
Bn:=−KX
i=1X
|γ|=1exp(βn
0i)
γ!(∆βn
1i)·Xγexp(( βn
1i)⊤X)gGn(Y|X) +R2(X, Y),
where R2(X, Y)is a Taylor remainder such that R2(X, Y)/D1(Gn, G∗)→0asn→ ∞ . Putting
the above results together, we rewrite the quantity Hnas follows:
Hn=KX
i=11X
η3=02−η3X
|η1|+η2=0Un
i,η1,η2,η3·Xη1exp(( β∗
1i)⊤X)∂η2+η3f
∂hη2
1∂ση3(Y|(a∗
i)⊤X+b∗
i, σ∗
i)
+KX
i=1X
0≤|γ|≤1Wn
i,γ·Xγexp(( β∗
1i)⊤X)gGn(Y|X) +R1(X, Y) +R2(X, Y), (42)
in which we respectively define for each i∈[K]that
Un
i,η1,η2,η3:=X
α∈Iη1,η2,η3exp(βn
0i)
α!·(∆βn
1i)α1(∆an
i)α2(∆bn
i)α3(∆σn
i)α4,
Wn
i,γ:=−exp(βn
0i)
γ!(∆βn
1i)γ,
for any (η1, η2, η3)̸= (0d,0,0)and|γ| ̸=0d. Additionally, Un
i,0d,0,0=−Wn
i,0d:= exp( βn
0i)−
exp(β∗
0i).
33

--- PAGE 34 ---
Published as a conference paper at ICLR 2024
Step 2 - Non-vanishing coefficients :
Moving to the second step, we will show that not all the ratios Un
i,η1,η2,η3/D1(Gn, G∗)tend to zero
asngoes to infinity. Assume by contrary that all of them approach zero when n→ ∞ , then for
(η1, η2, η3) = (0d,0,0), it follows that
1
D1(Gn, G∗)·KX
i=1exp(βn
0i)−exp(β∗
0i)=KX
i=1|Un
j,η1,η2,η3|
D1(Gn, G∗)→0. (43)
Additionally, for tuples (η1, η2, η3)where η1∈ {e1, e2, . . . , e d}withej:= (0 , . . . , 0,1|{z}
j−th,0, . . . , 0)
andη2=η3= 0, we get
1
D1(Gn, G∗)·KX
i=1exp(βn
0i)∥∆βn
1i∥1=KX
i=1X
η1∈{e1,...,ed}|Un
j,η1,0,0|
D1(Gn, G∗)→0.
By using similar arguments, we end up with
1
D1(Gn, G∗)·KX
i=1exp(βn
0i)h
∥∆βn
1i∥1+∥∆an
i∥1+|∆bn
i|+|∆σn
i|i
→0.
Due to the topological equivalence between norm-1 and norm-2, the above limit implies that
1
D1(Gn, G∗)·KX
i=1exp(βn
0i)h
∥∆βn
1i∥+∥∆an
i∥+|∆bn
i|+|∆σn
i|i
→0. (44)
Combine equation (43) with equation (44), we deduce that D1(Gn, G∗)/D1(Gn, G∗)→0, which is
a contradiction. Consequently, at least one among the ratios Un
i,η1,η2,η3/D1(Gn, G∗)does not vanish
asntends to infinity.
Step 3 - Fatou’s contradiction :
Let us denote by mnthe maximum of the absolute values of Un
i,η1,η2,η3/D1(Gn, G∗)and
Wn
i,γ/D1(Gn, G∗). It follows from the result achieved in Step 2 that 1/mn̸→ ∞ .
Recall from the hypothesis that EX[V(gGn(·|X), gG∗(·|X))]/D1(Gn, G∗)→0asn→ ∞ . Thus,
by the Fatou’s lemma, we have
0 = lim
n→∞EX[V(gGn(·|X), gG∗(·|X))]
D1(Gn, G∗)=1
2·Z
lim inf
n→∞|gGn(Y|X)−gG∗(Y|X)|
D1(Gn, G∗)dXdY.
This result indicates that |gGn(Y|X)−gG∗(Y|X)|/D1(Gn, G∗)tends to zero as ngoes to infinity
for almost surely (X, Y). As a result, it follows that
lim
n→∞Hn
mnD(Gn, G∗)=hKX
i=1exp(( β∗
1i)⊤X+β∗
0i)i
·lim
n→∞|gGn(Y|X)−gG∗(Y|X)|
mnD1(Gn, G∗)= 0.
Next, let us denote Un
i,η1,η2,η3/[mnD1(Gn, G∗)]→τi,η1,η2,η3andWn
i,γ/[mnD1(Gn, G∗)]→κi,γ
with a note that at least one among them is non-zero. From the formulation of Hnin equation (42),
we deduce that
KX
i=11X
η3=02−η3X
|η1|+η2=0τi,η1,η2,η3·Xη1exp(( β∗
1i)⊤X)∂η2+η3f
∂hη2
1∂ση3(Y|(a∗
i)⊤X+b∗
i, σ∗
i)
+KX
i=1X
0≤|γ|≤1κi,γ·Xγexp(( β∗
1i)⊤X)gG∗(Y|X) = 0 ,
for almost surely (X, Y). This equation is equivalent to
1X
|η1|=0hKX
i=12−|η1|X
η2+η3=0τi,η1,η2,η3exp(( β∗
1i)⊤X)∂η2+η3f
∂hη2
1∂ση3(Y|(a∗
i)⊤X+b∗
i, σ∗
i)
+κi,η1exp(( β∗
1i)⊤X)gG∗(Y|X)i
×Xη1= 0,
34

--- PAGE 35 ---
Published as a conference paper at ICLR 2024
for almost surely (X, Y). It is clear that the left hand side of the above equation is a polynomial of X
belonging to the compact set X. As a result, we get that
KX
i=12−|η1|X
η2+η3=0τi,η1,η2,η3exp(( β∗
1i)⊤X)∂η2+η3f
∂hη2
1∂ση3(Y|(a∗
i)⊤X+b∗
i, σ∗
i)
+κi,η1gG∗(Y|X) exp(( β∗
1i)⊤X) = 0 ,
for any i∈[K],0≤ |η1| ≤1and almost surely (X, Y). Since (a∗
1, b∗
1, σ∗
1), . . . , (a∗
K, b∗
K, σ∗
K)have
pair-wise distinct values and the family Fis strongly identifiable, the set
n∂η2+η3f
∂hη2
1∂ση3(Y|(a∗
i)⊤X+b∗
i, σ∗
i) :i∈[K],0≤η2+η3≤2− |η1|o
is linearly independent w.r.t (X, Y). Consequently, we obtain that τi,η1,η2,η3=κi,η1= 0for any
i∈[K],0≤ |η1| ≤1and0≤η2+η3≤2− |η1|, which contradicts the fact that at least one among
these terms is different from zero.
Hence, we reach the desired conclusion.
E.3.2 P ROOF OF THEOREM 6
Similar to the proof of Theorem 5 in Appendix E.3.1, our objective here is also to derive the local
part of the following Total Variation lower bound:
EX[V(gG(·|X), gG∗(·|X))]≳D3(G, G∗),
for any G∈ Ok(Θ). In particular, we aim to show that
lim
ε→0inf
G∈Ok(Θ):D3(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]
D2(G, G∗)>0. (45)
Assume that the above claim does not hold true, then we can find a sequence of mixing
measures Gn:=Pkn
i=1exp(βn
0i)δ(βn
1i,an
i,bn
i,σn
i)∈ O k(Ω) such that both D3(Gn, G∗)and
EX[V(gGn(·|X), gG∗(·|X))]/D3(Gn, G∗)vanish when ngoes to infinity. Then, it follows that
for any j∈[k∗], we haveP
i∈Cjexp(βn
0i)→exp(β∗
0j)and(βn
1i, an
i, bn
i, σn
i)→(β∗
1j, a∗
j, b∗
j, σ∗
j)for
alli∈ Cj. WLOG, we may assume that
D3(Gn, G∗) =X
j∈[K],
|Cj|>1X
i∈Cjexp(βn
0i)h
∥∆βn
1ij∥2+∥∆an
ij∥2+|∆bn
ij|2+|∆σn
ij|2i
+X
j∈[K],
|Cj|=1X
i∈Cjexp(βn
0i)h
∥∆βn
1ij∥+∥∆an
ij∥+|∆bn
ij|+|∆σn
ij|i
+KX
j=1X
i∈Cjexp(βn
0i)−exp(β∗
0j).
Subsequently, let X∈ X∗
ℓfor some ℓ∈[q]such that {ℓ1, . . . , ℓ K}={1, . . . , K }, where
X∗
ℓ:=n
x∈ X: (β∗
1j)⊤x≥(β∗
1j′)⊤x,∀j∈ {ℓ1, . . . , ℓ K}, j′∈ {ℓK+1, . . . , ℓ k∗}o
.
Then, for any ℓ∈[q], we denote (ℓ1, . . . , ℓk)as a permutation of (1, . . . , k )and
Xn
ℓ:=n
x∈ X: (βn
1i)⊤x≥(βn
1i′)⊤x,∀i∈ {ℓ1, . . . , ℓK}, i′∈ {ℓK+1, . . . , ℓk}o
.
If{ℓ1, . . .ℓK} ̸=C1∪. . .∪ CKfor any ℓ∈[q], then V(gGn(·|X), gG∗(·|X))/D3(Gn, G∗)̸→0as
ntends to infinity. This contradicts the fact that this term must approach zero. Therefore, we only
need to consider the scenario when there exists ℓ∈[q]such that {ℓ1, . . .ℓK}=C1∪. . .∪ CK. By
using the same arguments as in Appendix B.2, we obtain that X∈ Xn
ℓ.
35

--- PAGE 36 ---
Published as a conference paper at ICLR 2024
Then, we can represent the conditional densities gG∗(Y|X)andgGn(Y|X)for any sufficiently large
nas follows:
gG∗(Y|X) =KX
j=1exp(( β∗
1j)⊤X+β∗
0j)
PK
j′=1exp(( β∗
1j′)⊤X+β∗
0j′)·f(Y|(a∗
j)⊤X+b∗
j, σ∗
j),
gGn(Y|X) =KX
j=1X
i∈Cjexp(( βn
1i)⊤X+βn
0i)PK
j′=1P
i′∈Cj′exp(( βn
1i′)⊤X+βn
0i′)·f(Y|(an
i)⊤X+bn
i, σn
i).
Now, we reuse the three-step framework in Appendix E.3.1.
Step 1 - Taylor expansion :
Firstly, by abuse of notations, let us consider the quantity
Hn:=hKX
j=1exp(( β∗
1j)⊤X+β∗
0j)i
·[gGn(Y|X)−gG∗(Y|X)].
Similar to Step 1 in Appendix A, we can express this term as
Hn=KX
j=1X
i∈Cjexp(βn
0i)h
exp(( βn
1i)⊤X)f(Y|(an
i)⊤X+bn
i, σn
i)−exp(( β∗
1j)⊤X)f(Y|(a∗
j)⊤X+b∗
j, σ∗
j)i
−KX
j=1X
i∈Cjexp(βn
0i)h
exp(( βn
1i)⊤X)gGn(Y|X)−exp(( β∗
1j)⊤X)gGn(Y|X)i
+KX
j=1hX
i∈Cjexp(βn
0i)−exp(β∗
0j)ih
exp(( β∗
1j)⊤X)f(Y|(a∗
i)⊤X+b∗
i, σ∗
i)−exp(( β∗
1j)⊤X)gGn(Y|X)i
:=An+Bn+En.
Next, we proceed to decompose Anbased on the cardinality of the V oronoi cells as follows:
An=X
j:|Cj|=1X
i∈Cjexp(βn
0i)h
exp(( βn
1i)⊤X)f(Y|(an
i)⊤X+bn
i, σn
i)−exp(( β∗
1i)⊤X)f(Y|(a∗
i)⊤X+b∗
i, σ∗
i)i
+X
j:|Cj|>1X
i∈Cjexp(βn
0i)h
exp(( βn
1i)⊤X)f(Y|(an
i)⊤X+bn
i, σn
i)−exp(( β∗
1i)⊤X)f(Y|(a∗
i)⊤X+b∗
i, σ∗
i)i
.
By applying the Taylor expansions of first and second orders to the first and second terms of An,
respectively, and following the derivation in equation (40), we arrive at
An=X
j:|Cj|=1X
i∈Cj1X
η3=02−η3X
|η1|+η2=1−η3X
α∈Iη1,η2,η3exp(βn
0i)
α!·(∆βn
1ij)α1(∆an
ij)α2(∆bn
ij)α3(∆σn
i)α4
×Xη1exp(( β∗
1j)⊤X)·∂η2+η3f
∂hη2
1∂ση3(Y|(a∗
j)⊤X+b∗
j, σ∗
j) +R3(X, Y)
+X
j:|Cj|>1X
i∈Cj2X
η3=04−η3X
|η1|+η2=1−1{η3>0}X
α∈Iη1,η2,η3exp(βn
0i)
α!·(∆βn
1ij)α1(∆an
ij)α2(∆bn
ij)α3(∆σn
ij)α4
×Xη1exp(( β∗
1j)⊤X)·∂η2+η3f
∂hη2
1∂ση3(Y|(a∗
j)⊤X+b∗
j, σ∗
j) +R4(X, Y),
where the set Iη1,η2,η3is defined in equation (41) while Ri(X, Y)is a Taylor remainder such that
Ri(X, Y)/D3(Gn, G∗)→0asn→ ∞ fori∈ {3,4}. Similarly, we also decompose Bnas
Bn=−X
j:|Cj|=1X
i∈CjX
|γ|=1exp(βn
0i)
γ!(∆βn
1i)·Xγexp(( β∗
1j)⊤X)gGn(Y|X) +R5(X, Y)
−X
j:|Cj|>1X
i∈CjX
1≤|γ|≤2exp(βn
0i)
γ!(∆βn
1i)·Xγexp(( β∗
1j)⊤X)gGn(Y|X) +R6(X, Y),
36

--- PAGE 37 ---
Published as a conference paper at ICLR 2024
where R5(X, Y)andR6(X, Y)are Taylor remainders such that their ratios over D3(Gn, G∗)ap-
proach zero as n→ ∞ . Subsequently, let us define
Sn
j,η1,η2,η3:=X
i∈CjX
α∈Iη1,η2,η3exp(βn
0i)
α!·(∆βn
1ij)α1(∆an
ij)α2(∆bn
ij)α3(∆σn
ij)α4,
Tn
j,γ:=−X
i∈Cjexp(βn
0i)
γ!(∆βn
1ij)γ=−Sn
j,γ,0,0,
for any (η1, η2, η3)̸= (0d,0,0)and|γ| ̸=0d, while for (η1, η2, η3) = (0d,0)we set
Sn
i,0d,0=−Tn
i,0d:=X
i∈Cjexp(βn
0i)−exp(β∗
0i).
As a consequence, it follows that
Hn=KX
j=11+1{|Cj|>1}X
η3=02(1+ 1{|Cj|>1})−η3X
|η1|+η2=0Sn
j,η1,η2,η3·Xη1exp(( β∗
1j)⊤X)·∂η2f
∂hη2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j)
+KX
j=11+1{|Cj|>1}X
|γ|=0Tn
j,γ·Xγexp(( β∗
1j)⊤X)gGn(Y|X) +R5(X, Y) +R6(X, Y). (46)
Step 2 - Non-vanishing coefficients :
In this step, we will prove by contradiction that at least one among the ratios Sn
j,η1,η2,η3/D3(Gn, G∗)
does not converge to zero as n→ ∞ . Assume that all these terms go to zero, then by employing
arguments for deriving equations (43) and (44), we get that
1
D3(Gn, G∗)·hKX
j=1X
i∈Cjexp(βn
0i)−exp(β∗
0j)
+X
j:|Cj|=1X
i∈Cjexp(βn
0i)
∥∆βn
1ij∥+∥∆an
ij∥+|∆bn
ij|+|∆σn
ij|i
→0.
Next, let ej:= (0 , . . . , 0,1|{z}
j−th,0, . . . , 0)for any j∈[d]. Then, we have
1
D3(Gn, G∗)·KX
i=1exp(βn
0i)∥∆βn
1ij∥2=KX
i=1X
η1∈{2e1,...,2ed}|Un
j,η1,0,0|
D3(Gn, G∗)→0.
Similarly, we also get that
1
D3(Gn, G∗)·KX
i=1exp(βn
0i)∥∆bn
ij∥2→0,1
D3(Gn, G∗)·KX
i=1exp(βn
0i)∥∆σn
ij∥2→0
Moreover, note that
1
D3(Gn, G∗)·KX
i=1exp(βn
0i)∥∆an
ij∥2=KX
i=1X
η1∈{2e1,...,2ed}|Un
j,η1,2,0|
D3(Gn, G∗)→0.
Gathering all the above limits, we obtain that 1 =D3(Gn, G∗)/D3(Gn, G∗)→0asn→ ∞ , which
is a contradiction. Thus, at least one among the terms Sn
j,η1,η2,η3/D3(Gn, G∗)does not converge to
zero as n→ ∞
Step 3 - Fatou’s contradiction :
It follows from the hypothesis that EX[V(gGn(·|X), gG∗(·|X))]/D3(Gn, G∗)→0asn→ ∞ .
Then, by applying the Fatou’s lemma, we get
0 = lim
n→∞EX[V(gGn(·|X), gG∗(·|X))]
D3(Gn, G∗)=1
2·Z
lim inf
n→∞|gGn(Y|X)−gG∗(Y|X)|
D3(Gn, G∗)dXdY,
37

--- PAGE 38 ---
Published as a conference paper at ICLR 2024
which implies that |gGn(Y|X)−gG∗(Y|X)|/D3(Gn, G∗)→0asn→ ∞ for almost surely (X, Y).
Next, we define mnas the maximum of the absolute values of Sn
j,η1,η2/D3(Gn,G∗). It follows from
Step 2 that 1/mn̸→ ∞ . Moreover, by arguing in the same way as in Step 3 in Appendix E.1, we
receive that
Hn/[mnD3(Gn, G∗)]→0 (47)
asn→ ∞ . By abuse of notations, let us denote
Sn
j,η1,η2,η3/[mnD3(Gn, G∗)]→τj,η1,η2,η3
Here, at least one among τj,η1,η2,η3is non-zero. Then, by putting the results in equations (46) and
(47) together, we get
KX
j=11+1{|Cj|>1}X
η3=02(1+ 1{|Cj|>1})−η3X
|η1|+η2=0τj,η1,η2,η3·Xη1exp(( β∗
1j)⊤X)∂η2+η3f
∂hη2
1∂ση3(Y|(a∗
j)⊤X+b∗
j, σ∗
j)
+KX
j=11+1{|Cj|>1}X
|γ|=0−τn
j,γ,0,0·Xγexp(( β∗
1j)⊤X)gG∗(Y|X) = 0 ,
for almost surely (X, Y). Arguing in a similar fashion as in Step 3 of Appendix E.1, we obtain that
τj,η1,η2,η3= 0for any j∈[K],0≤ |η1|+η2+η3≤2(1 + 1{|Cj|>1})and0≤ |γ| ≤1 +1{|Cj|>1}.
This contradicts the fact that at least one among them is non-zero. Hence, the proof is completed.
38
