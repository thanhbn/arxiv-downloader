# Hướng tới Hiểu biết về Hỗn hợp Chuyên gia trong Học sâu

Zixiang Chen∗và Yihe Deng†và Yue Wu‡và Quanquan Gu§và Yuanzhi Li¶

Tóm tắt

Lớp Hỗn hợp Chuyên gia (MoE), một mô hình được kích hoạt thưa thớt được điều khiển bởi một bộ định tuyến, đã đạt được thành công lớn trong học sâu. Tuy nhiên, việc hiểu biết về kiến trúc này vẫn còn khó nắm bắt. Trong bài báo này, chúng tôi nghiên cứu chính thức về cách lớp MoE cải thiện hiệu suất học của mạng nơ-ron và tại sao mô hình hỗn hợp không sụp đổ thành một mô hình duy nhất. Kết quả thực nghiệm của chúng tôi cho thấy rằng cấu trúc cụm của bài toán cơ bản và tính phi tuyến của chuyên gia là then chốt cho thành công của MoE. Để hiểu rõ hơn điều này, chúng tôi xem xét một bài toán phân loại thách thức với các cấu trúc cụm nội tại, khó học bằng một chuyên gia duy nhất. Tuy nhiên với lớp MoE, bằng cách chọn các chuyên gia là mạng nơ-ron tích chập phi tuyến hai lớp (CNN), chúng tôi chỉ ra rằng bài toán có thể được học thành công. Hơn nữa, lý thuyết của chúng tôi cho thấy rằng bộ định tuyến có thể học các đặc trưng trung tâm cụm, giúp chia bài toán đầu vào phức tạp thành các bài toán phân loại tuyến tính đơn giản hơn mà các chuyên gia riêng lẻ có thể chinh phục. Theo hiểu biết của chúng tôi, đây là kết quả đầu tiên hướng tới việc hiểu chính thức cơ chế của lớp MoE cho học sâu.

1 Giới thiệu

Cấu trúc Hỗn hợp Chuyên gia (MoE) (Jacobs et al., 1991; Jordan và Jacobs, 1994) là một thiết kế cổ điển mở rộng đáng kể khả năng mô hình và chỉ đưa ra chi phí tính toán nhỏ. Trong những năm gần đây, lớp MoE (Eigen et al., 2013; Shazeer et al., 2017), là một phần mở rộng của mô hình MoE cho mạng nơ-ron sâu, đã đạt được thành công đáng kể trong học sâu. Nói chung, một lớp MoE chứa nhiều chuyên gia có chung kiến trúc mạng và được huấn luyện bởi cùng một thuật toán, với một hàm cổng (hoặc định tuyến) định tuyến từng đầu vào tới một vài chuyên gia trong số tất cả các ứng viên. Thông qua hàm cổng thưa thớt, bộ định tuyến trong lớp MoE có thể định tuyến mỗi đầu vào tới K (K≥2) chuyên gia tốt nhất (Shazeer et al., 2017), hoặc chuyên gia tốt nhất duy nhất (K=1) (Fedus et al., 2021). Sơ đồ định tuyến này chỉ tốn chi phí tính toán của K chuyên gia cho một đầu vào mới, có thời gian suy luận nhanh.

Bất chấp thành công thực nghiệm lớn của lớp MoE, việc hiểu lý thuyết về kiến trúc này vẫn còn khó nắm bắt. Trong thực tế, tất cả các chuyên gia có cùng cấu trúc, được khởi tạo từ cùng phân phối trọng số (Fedus et al., 2021) và được huấn luyện với cùng cấu hình tối ưu hóa. Bộ định tuyến cũng được khởi tạo để phân phối dữ liệu đồng đều. Không rõ tại sao các chuyên gia có thể phân kỳ thành các hàm khác nhau chuyên môn hóa để đưa ra dự đoán cho các đầu vào khác nhau, và tại sao bộ định tuyến có thể tự động học cách phân phối dữ liệu, đặc biệt khi tất cả đều được huấn luyện bằng các thuật toán tìm kiếm cục bộ đơn giản như gradient descent. Do đó, chúng tôi nhằm trả lời các câu hỏi sau:

Tại sao các chuyên gia trong MoE đa dạng hóa thay vì sụp đổ thành một mô hình duy nhất? Và bộ định tuyến có thể học cách phân phối dữ liệu cho chuyên gia đúng như thế nào?

Trong bài báo này, để trả lời câu hỏi trên, chúng tôi xem xét phân phối dữ liệu "hỗn hợp phân loại" tự nhiên với cấu trúc cụm và nghiên cứu lý thuyết về hành vi và lợi ích của lớp MoE. Chúng tôi tập trung vào thiết lập đơn giản nhất của hỗn hợp phân loại tuyến tính, nơi phân phối dữ liệu có nhiều cụm, và mỗi cụm sử dụng các vector đặc trưng (tuyến tính) riêng biệt để biểu diễn nhãn. Cụ thể, chúng tôi xem xét dữ liệu được tạo ra như một sự kết hợp của các vùng đặc trưng, vùng cụm, và vùng nhiễu (Xem Định nghĩa 3.1 để biết chi tiết). Chúng tôi nghiên cứu huấn luyện lớp MoE dựa trên dữ liệu được tạo từ phân phối "hỗn hợp phân loại" bằng gradient descent, trong đó mỗi chuyên gia được chọn là CNN hai lớp. Các đóng góp chính của bài báo này được tóm tắt như sau:

• Chúng tôi trước tiên chứng minh một kết quả tiêu cực (Định lý 4.1) rằng bất kỳ chuyên gia đơn lẻ nào, như CNN hai lớp với hàm kích hoạt tùy ý, không thể đạt được độ chính xác kiểm tra hơn 87,5% trên phân phối dữ liệu của chúng tôi.

• Thực nghiệm, chúng tôi thấy rằng hỗn hợp các chuyên gia tuyến tính hoạt động tốt hơn chuyên gia đơn lẻ nhưng vẫn kém đáng kể so với hỗn hợp các chuyên gia phi tuyến. Hình 1 cung cấp kết quả như vậy trong trường hợp đặc biệt của phân phối dữ liệu với bốn cụm. Mặc dù hỗn hợp các mô hình tuyến tính có thể biểu diễn hàm gắn nhãn của phân phối dữ liệu này với độ chính xác 100%, nó không thể học được sau khi huấn luyện. Chúng ta có thể thấy rằng cấu trúc cụm cơ bản không thể được phục hồi bởi hỗn hợp các chuyên gia tuyến tính, và cả bộ định tuyến lẫn các chuyên gia đều không đa dạng hóa đủ sau khi huấn luyện. Ngược lại, hỗn hợp các chuyên gia phi tuyến có thể phục hồi đúng cấu trúc cụm và đa dạng hóa.

• Được thúc đẩy bởi kết quả tiêu cực và thí nghiệm trên dữ liệu mẫu, chúng tôi nghiên cứu một mô hình MoE có cổng thưa thớt với CNN hai lớp được huấn luyện bởi gradient descent. Chúng tôi chứng minh rằng mô hình MoE này có thể đạt được gần 100% độ chính xác kiểm tra một cách hiệu quả (Định lý 4.2).

• Cùng với kết quả về độ chính xác kiểm tra, chúng tôi chứng minh chính thức rằng mỗi chuyên gia của mô hình MoE có cổng thưa thớt sẽ được chuyên môn hóa cho một phần cụ thể của dữ liệu (tức là, ít nhất một cụm), được xác định bởi việc khởi tạo trọng số. Trong khi đó, bộ định tuyến có thể học các đặc trưng trung tâm cụm và định tuyến dữ liệu đầu vào tới các chuyên gia đúng.

• Cuối cùng, chúng tôi cũng tiến hành các thí nghiệm mở rộng trên cả bộ dữ liệu tổng hợp và thực tế để chứng thực lý thuyết của chúng tôi.

Ký hiệu. Chúng tôi sử dụng chữ thường, chữ in đậm thường, và chữ in đậm hoa để ký hiệu các đại lượng vô hướng, vector, và ma trận tương ứng. Chúng tôi ký hiệu hợp của các tập rời rạc (Ai:i∈I) bằng ⊔i∈IAi. Đối với vector x, chúng tôi sử dụng ∥x∥2 để ký hiệu chuẩn Euclidean. Đối với ma trận W, chúng tôi sử dụng ∥W∥F để ký hiệu chuẩn Frobenius. Với hai dãy {xn} và {yn}, chúng tôi ký hiệu xn=O(yn) nếu |xn|≤C1|yn| với hằng số dương tuyệt đối C1, xn=Ω(yn) nếu |xn|≥C2|yn| với hằng số dương tuyệt đối C2, và xn=Θ(yn) nếu C3|yn|≤|xn|≤C4|yn| với các hằng số tuyệt đối C3,C4>0. Chúng tôi cũng sử dụng Õ() để ẩn các thừa số logarit của d trong O(). Thêm vào đó, chúng tôi ký hiệu xn=poly(yn) nếu xn=O(ynD) với hằng số dương D, và xn=polylog(yn) nếu xn=poly(log(yn)). Chúng tôi cũng ký hiệu bằng xn=o(yn) nếu lim_{n→∞} xn/yn = 0. Cuối cùng chúng tôi sử dụng [N] để ký hiệu tập chỉ số {1,...,N}.

2 Công trình liên quan

Mô hình Hỗn hợp Chuyên gia. Mô hình hỗn hợp chuyên gia (Jacobs et al., 1991; Jordan và Jacobs, 1994) đã được nghiên cứu lâu dài trong cộng đồng học máy. Các mô hình MoE này dựa trên nhiều mô hình chuyên gia cơ sở khác nhau như máy vector hỗ trợ (Collobert et al., 2002), tiến trình Gauss (Tresp, 2001), hoặc mô hình Markov ẩn (Jordan et al., 1997). Để tăng khả năng mô hình xử lý dữ liệu thị giác và giọng nói phức tạp, Eigen et al. (2013) đã mở rộng cấu trúc MoE cho mạng nơ-ron sâu, và đề xuất một mô hình MoE sâu bao gồm nhiều lớp bộ định tuyến và chuyên gia. Shazeer et al. (2017) đã đơn giản hóa lớp MoE bằng cách làm cho đầu ra của hàm cổng thưa thớt cho mỗi ví dụ, cải thiện đáng kể tính ổn định huấn luyện và giảm chi phí tính toán. Kể từ đó, lớp MoE với các cấu trúc mạng nơ-ron cơ sở khác nhau (Shazeer et al., 2017; Dauphin et al., 2017; Vaswani et al., 2017) đã được đề xuất và đạt được thành công to lớn trong nhiều tác vụ ngôn ngữ. Rất gần đây, Fedus et al. (2021) đã cải thiện hiệu suất của lớp MoE bằng cách định tuyến một ví dụ chỉ tới một chuyên gia duy nhất thay vì K chuyên gia, tiếp tục giảm tính toán định tuyến đồng thời bảo toàn chất lượng mô hình.

Hỗn hợp Hồi quy/Phân loại Tuyến tính. Trong bài báo này, chúng tôi xem xét mô hình "hỗn hợp phân loại". Loại mô hình này có thể truy nguyên từ (De Veaux, 1989; Jordan và Jacobs, 1994; Faria và Soromenho, 2010) và đã được áp dụng cho nhiều tác vụ bao gồm nhận dạng đối tượng (Quattoni et al., 2004), nhận dạng hành động con người (Wang và Mori, 2009), và dịch máy (Liang et al., 2006). Để học các tham số chưa biết cho mô hình hỗn hợp hồi quy/phân loại tuyến tính, (Anandkumar et al., 2012; Hsu et al., 2012; Chaganty và Liang, 2013; Anandkumar et al., 2014; Li và Liang, 2018) nghiên cứu phương pháp moment và phân tích tensor. Một hướng nghiên cứu khác tập trung vào các thuật toán cụ thể như thuật toán Expectation-Maximization (EM) (Khalili và Chen, 2007; Yi et al., 2014; Balakrishnan et al., 2017; Wang et al., 2015).

Hiểu biết Lý thuyết về Học sâu. Trong những năm gần đây, nhiều nỗ lực lớn đã được thực hiện để thiết lập nền tảng lý thuyết của học sâu. Một loạt nghiên cứu đã chứng minh các bảo đảm hội tụ (Jacot et al., 2018; Li và Liang, 2018; Du et al., 2019; Allen-Zhu et al., 2019b; Zou et al., 2018) và tổng quát hóa (Allen-Zhu et al., 2019a; Arora et al., 2019a,b; Cao và Gu, 2019) trong chế độ "neural tangent kernel" (NTK) được gọi là, nơi các tham số ở gần khởi tạo, và hàm mạng nơ-ron gần như tuyến tính theo các tham số. Một hướng nghiên cứu gần đây (Allen-Zhu và Li, 2019; Bai và Lee, 2019; Allen-Zhu và Li, 2020a,b,c; Li et al., 2020; Cao et al., 2022; Zou et al., 2021; Wen và Li, 2021) nghiên cứu động lực học mạng nơ-ron ngoài chế độ NTK. Đáng chú ý rằng phân tích mô hình MoE của chúng tôi cũng ngoài chế độ NTK.

3 Thiết lập Bài toán và Kiến thức Chuẩn bị

Chúng tôi xem xét một lớp MoE với mỗi chuyên gia là CNN hai lớp được huấn luyện bởi gradient descent (GD) trên n ví dụ huấn luyện độc lập {(xi,yi)}_{i=1}^n được tạo từ phân phối dữ liệu D. Trong phần này, chúng tôi sẽ trước tiên giới thiệu mô hình dữ liệu D, sau đó giải thích mô hình mạng nơ-ron và chi tiết của thuật toán huấn luyện.

3.1 Phân phối dữ liệu

Chúng tôi xem xét một bài toán phân loại nhị phân trên đầu vào P-vùng, trong đó mỗi vùng có d chiều. Cụ thể, mỗi dữ liệu có nhãn được biểu diễn bởi (x,y), trong đó đầu vào x=(x^{(1)},x^{(2)},...,x^{(P)})∈(ℝ^d)^P là tập hợp P vùng và y∈{±1} là nhãn dữ liệu. Chúng tôi xem xét dữ liệu được tạo từ K cụm. Mỗi cụm k∈[K] có một vector tín hiệu nhãn v_k và một vector tín hiệu trung tâm cụm c_k với ∥v_k∥_2=∥c_k∥_2=1. Để đơn giản, chúng tôi giả định rằng tất cả các tín hiệu {v_k}_{k∈[K]}∪{c_k}_{k∈[K]} trực giao với nhau.

Định nghĩa 3.1. Một cặp dữ liệu (x,y)∈(ℝ^d)^P×{±1} được tạo từ phân phối D như sau:
• Rút đồng đều một cặp (k,k') với k≠k' từ {1,...,K}.
• Tạo nhãn y∈{±1} đồng đều, tạo một biến ngẫu nhiên Rademacher ε∈{±1}.
• Tạo độc lập các biến ngẫu nhiên α,β,γ từ phân phối D_α,D_β,D_γ. Trong bài báo này, chúng tôi giả định tồn tại các hằng số tuyệt đối C_1,C_2 sao cho hầu chắc chắn 0<C_1≤α,β,γ≤C_2.
• Tạo x như tập hợp P vùng: x=(x^{(1)},x^{(2)},...,x^{(P)})∈(ℝ^d)^P, trong đó
{ Tín hiệu đặc trưng. Một và chỉ một vùng được cho bởi αyv_k.
{ Tín hiệu trung tâm cụm. Một và chỉ một vùng được cho bởi βc_k.
{ Nhiễu đặc trưng. Một và chỉ một vùng được cho bởi γεv_{k'}.
{ Nhiễu ngẫu nhiên. Phần còn lại của P−3 vùng là nhiễu Gauss được rút độc lập từ N(0,(σ^2_p/d)I_d) trong đó σ_p là hằng số tuyệt đối.

Cách học loại dữ liệu này? Vì vị trí của tín hiệu và nhiễu không được chỉ định trong Định nghĩa 3.1, việc sử dụng cấu trúc CNN áp dụng cùng một hàm cho mỗi vùng là tự nhiên. Chúng tôi chỉ ra rằng cường độ của nhiễu đặc trưng γε có thể lớn bằng cường độ của tín hiệu đặc trưng α. Như chúng ta sẽ thấy sau trong Định lý 4.1, bài toán phân loại này khó học với một chuyên gia duy nhất, như bất kỳ CNN hai lớp nào (bất kỳ hàm kích hoạt nào với bất kỳ số nơ-ron nào). Tuy nhiên, bài toán phân loại như vậy có cấu trúc cụm nội tại có thể được sử dụng để đạt được hiệu suất tốt hơn. Các ví dụ có thể được chia thành K cụm ⊔_{k∈[K]}Ω_k dựa trên các tín hiệu trung tâm cụm: một ví dụ (x,y)∈Ω_k nếu và chỉ nếu ít nhất một vùng của x căn chỉnh với c_k. Không khó để chỉ ra rằng bài toán phân loại nhị phân con trên Ω_k có thể dễ dàng được giải quyết bởi một chuyên gia riêng lẻ. Chúng tôi mong đợi MoE có thể học cấu trúc cụm dữ liệu này từ các tín hiệu trung tâm cụm.

Ý nghĩa của kết quả chúng tôi. Mặc dù dữ liệu này có thể được học bởi các công trình hiện có về hỗn hợp các bộ phân loại tuyến tính với các thuật toán tinh vi (Anandkumar et al., 2012; Hsu et al., 2012; Chaganty và Liang, 2013), trọng tâm của bài báo chúng tôi là huấn luyện hỗn hợp các mạng nơ-ron phi tuyến, một mô hình thực tế hơn được sử dụng trong các ứng dụng thực tế. Khi MoE được huấn luyện bởi các biến thể của gradient descent, chúng tôi chỉ ra rằng các chuyên gia tự động học chuyên môn hóa trên mỗi cụm, trong khi bộ định tuyến tự động học phân phối dữ liệu cho các chuyên gia theo chuyên môn của họ. Mặc dù từ góc độ biểu diễn, không khó để thấy rằng lớp khái niệm có thể được biểu diễn bởi MoE, kết quả của chúng tôi rất có ý nghĩa vì chúng tôi chứng minh rằng gradient descent từ khởi tạo ngẫu nhiên có thể tìm thấy một MoE tốt với các chuyên gia phi tuyến một cách hiệu quả. Để làm cho kết quả của chúng tôi thậm chí còn thuyết phục hơn, chúng tôi chỉ ra thực nghiệm rằng MoE với các chuyên gia tuyến tính, mặc dù cũng có thể biểu diễn lớp khái niệm, không thể được huấn luyện để tìm thấy một bộ phân loại tốt một cách hiệu quả.

3.2 Cấu trúc của lớp MoE

Một lớp MoE bao gồm một tập M "mạng chuyên gia" f_1,...,f_M, và một mạng cổng thường được đặt là tuyến tính (Shazeer et al., 2017; Fedus et al., 2021). Ký hiệu f_m(x,W) là đầu ra của mạng chuyên gia thứ m với đầu vào x và tham số W. Định nghĩa một vector M chiều h(x,θ)=∑_{p∈[P]}θ^⊤x^{(p)} là đầu ra của mạng cổng được tham số hóa bởi θ=[θ_1,...,θ_M]∈ℝ^{d×M}. Đầu ra F của lớp MoE có thể được viết như sau:

F(x,θ,W)=∑_{m∈T_x}π_m(x,θ)f_m(x,W),

trong đó T_x⊆[M] là tập các chỉ số được chọn và π_m(x,θ) là giá trị cổng định tuyến cho bởi

π_m(x,θ)=\frac{\exp(h_m(x,θ))}{\sum_{m'=1}^M\exp(h_{m'}(x,θ))}, ∀m∈[M].

Mô hình Chuyên gia. Trong thực tế, người ta thường sử dụng mạng nơ-ron phi tuyến làm chuyên gia trong lớp MoE. Thực tế, chúng tôi thấy rằng tính phi tuyến của chuyên gia là thiết yếu cho thành công của lớp MoE (xem Phần 6). Đối với chuyên gia thứ m, chúng tôi xem xét một mạng nơ-ron tích chập như sau:

f_m(x,W)=∑_{j∈[J]}∑_{p=1}^P σ(⟨w_{m,j},x^{(p)}⟩),

trong đó w_{m,j}∈ℝ^d là vector trọng số của bộ lọc thứ j (tức là, nơ-ron) trong chuyên gia thứ m, J là số bộ lọc (tức là, nơ-ron). Chúng tôi ký hiệu W_m=[w_{m,1},...,w_{m,J}]∈ℝ^{d×J} là ma trận trọng số của chuyên gia thứ m và tiếp tục đặt W={W_m}_{m∈[M]} là tập hợp các ma trận trọng số chuyên gia. Đối với CNN phi tuyến, chúng tôi xem xét hàm kích hoạt lập phương σ(z)=z^3, là một trong những hàm kích hoạt phi tuyến đơn giản nhất (Vecci et al., 1998). Chúng tôi cũng bao gồm thí nghiệm cho các hàm kích hoạt khác như RELU trong Phụ lục Bảng 7.

Mô hình Định tuyến Top-1. Một lựa chọn đơn giản của tập chọn T_x sẽ là toàn bộ tập chuyên gia T_x=[M] (Jordan và Jacobs, 1994), đây là trường hợp cho mô hình định tuyến mềm được gọi là. Tuy nhiên, việc sử dụng định tuyến mềm sẽ tốn thời gian trong học sâu. Trong bài báo này, chúng tôi xem xét "định tuyến chuyển mạch", được giới thiệu bởi Fedus et al. (2021) để làm cho mạng cổng thưa thớt và tiết kiệm thời gian tính toán. Đối với mỗi đầu vào x, thay vì sử dụng tất cả các chuyên gia, chúng tôi chỉ chọn một chuyên gia từ [M], tức là, |T_x|=1. Cụ thể, chúng tôi chọn T_x={argmax_m{h_m(x,θ)}}.

3.3 Thuật toán Huấn luyện

Cho tập dữ liệu huấn luyện S={(x_i,y_i)}_{i=1}^n, chúng tôi huấn luyện F với gradient descent để tối thiểu hóa hàm mất mát thực nghiệm sau:

L(θ,W)=\frac{1}{n}∑_{i=1}^n ℓ(y_i F(x_i,θ,W)),

trong đó ℓ là mất mát logistic được định nghĩa là ℓ(z)=log(1+exp(-z)). Chúng tôi khởi tạo θ^{(0)} bằng không và khởi tạo mỗi phần tử của W^{(0)} bằng i.i.d N(0,σ_0^2). Việc khởi tạo bằng không của mạng cổng được sử dụng rộng rãi trong huấn luyện MoE. Như được thảo luận trong Shazeer et al. (2017), nó có thể giúp tránh lỗi hết bộ nhớ và khởi tạo mạng trong trạng thái tải chuyên gia gần như bằng nhau (xem (5.1) cho định nghĩa tải chuyên gia).

Thay vì trực tiếp sử dụng gradient của mất mát thực nghiệm (3.2) để cập nhật trọng số, chúng tôi thêm nhiễu vào bộ định tuyến và sử dụng gradient của mất mát thực nghiệm bị nhiễu để cập nhật trọng số. Cụ thể, ví dụ huấn luyện x_i sẽ được phân phối tới argmax_m{h_m(x_i,θ^{(t)})+r_m,i^{(t)}} thay vì, trong đó {r_m,i^{(t)}}_{m∈[M],i∈[n]} là nhiễu ngẫu nhiên. Việc thêm thành phần nhiễu là một chiến lược huấn luyện được sử dụng rộng rãi cho lớp MoE có cổng thưa thớt (Shazeer et al., 2017; Fedus et al., 2021), có thể khuyến khích khám phá trên các chuyên gia và ổn định huấn luyện MoE. Trong bài báo này, chúng tôi rút {r_m,i^{(t)}}_{m∈[M],i∈[n]} độc lập từ phân phối đồng đều Unif[0,1] và ký hiệu tập hợp của nó là r^{(t)}. Do đó, mất mát thực nghiệm bị nhiễu tại lần lặp t có thể được viết là

L^{(t)}(θ^{(t)},W^{(t)})=\frac{1}{n}∑_{i=1}^n ℓ(y_i π_{m_{i,t}}(x_i,θ^{(t)})f_{m_{i,t}}(x_i,W^{(t)})),

trong đó m_{i,t}=argmax_m{h_m(x_i,θ^{(t)})+r_m,i^{(t)}}. Bắt đầu từ khởi tạo W^{(0)}, quy tắc cập nhật gradient descent cho các chuyên gia là

W_m^{(t+1)}=W_m^{(t)}-η_e\frac{∇_{W_m}L^{(t)}(θ^{(t)},W^{(t)})}{∥∇_{W_m}L^{(t)}(θ^{(t)},W^{(t)})∥_F}, ∀m∈[M],

trong đó η_e>0 là tốc độ học chuyên gia. Bắt đầu từ khởi tạo θ^{(0)}, quy tắc cập nhật gradient cho mạng cổng là

θ_m^{(t+1)}=θ_m^{(t)}-η_r∇_{θ_m}L^{(t)}(θ^{(t)},W^{(t)}), ∀m∈[M],

trong đó η_r>0 là tốc độ học bộ định tuyến. Trong thực tế, các chuyên gia được huấn luyện bởi Adam để đảm bảo họ có tốc độ học tương tự. Ở đây chúng tôi sử dụng gradient chuẩn hóa có thể được xem là một giải pháp thay thế đơn giản hơn cho Adam (Jelassi et al., 2021).

4 Kết quả Chính

Trong phần này, chúng tôi sẽ trình bày các kết quả chính. Chúng tôi trước tiên cung cấp một kết quả tiêu cực cho việc học với một chuyên gia duy nhất.

Định lý 4.1 (Chuyên gia đơn lẻ hoạt động kém). Giả sử D_α=D_γ trong Định nghĩa 3.1, thì bất kỳ hàm nào có dạng F(x)=∑_{p∈[P]}f(x^{(p)}) sẽ có lỗi kiểm tra lớn ℙ_{(x,y)∼D}[yF(x)≤0]≥1/8.

Định lý 4.1 chỉ ra rằng nếu nhiễu đặc trưng có cùng cường độ với tín hiệu đặc trưng tức là D_α=D_γ, bất kỳ CNN hai lớp nào có dạng F(x)=∑_{j∈[J]}a_j∑_{p∈[P]}σ(w_j^⊤x^{(p)}+b_j) không thể hoạt động tốt trên bài toán phân loại được định nghĩa trong Định nghĩa 3.1 trong đó σ có thể là bất kỳ hàm kích hoạt nào. Định lý 4.1 cũng chỉ ra rằng một tập hợp đơn giản của các chuyên gia có thể không cải thiện hiệu suất bởi vì tập hợp của các CNN hai lớp vẫn có dạng hàm được định nghĩa trong Định lý 4.1.

Để so sánh, định lý sau đưa ra các bảo đảm học cho việc huấn luyện lớp MoE tuân theo cấu trúc được định nghĩa trong Phần 3.2 với hàm kích hoạt lập phương.

Định lý 4.2 (MoE phi tuyến hoạt động tốt). Giả sử kích thước dữ liệu huấn luyện n=Ω̃(d). Chọn số chuyên gia M=Ω̃(K log K log log d), kích thước bộ lọc J=Ω̃(log M log log d), thang đo khởi tạo σ_0∈[d^{-1/3},d^{-0.01}], tốc độ học η_e=Õ(σ_0), η_r=Θ(σ_0/M^2). Thì với xác suất ít nhất 1-o(1), Thuật toán 1 có thể xuất ra (θ^{(T)},W^{(T)}) trong T=Õ(1) lần lặp sao cho MoE phi tuyến được định nghĩa trong Phần 3.2 thỏa mãn

• Lỗi huấn luyện bằng không, tức là, y_i F(x_i,θ^{(T)},W^{(T)})>0, ∀i∈[n].
• Lỗi kiểm tra gần như bằng không, tức là, ℙ_{(x,y)∼D}[yF(x,θ^{(T)},W^{(T)})≤0]=o(1).

Quan trọng hơn, các chuyên gia có thể được chia thành một hợp rời rạc của K tập không rỗng [M]=⊔_{k∈[K]}M_k và

• (Mỗi chuyên gia tốt trên một cụm) Mỗi chuyên gia m∈M_k hoạt động tốt trên cụm Ω_k, ℙ_{(x,y)∼D}[yf_m(x,W^{(T)})≤0|(x,y)∈Ω_k]=o(1).
• (Bộ định tuyến chỉ phân phối ví dụ cho chuyên gia tốt) Với xác suất ít nhất 1-o(1), một ví dụ x∈Ω_k sẽ được định tuyến tới một trong các chuyên gia trong M_k.

Định lý 4.2 chỉ ra rằng MoE phi tuyến hoạt động tốt trên bài toán phân loại trong Định nghĩa 3.1. Ngoài ra, bộ định tuyến sẽ học cấu trúc cụm và chia bài toán thành K bài toán phụ đơn giản hơn, mỗi bài toán được liên kết với một cụm. Cụ thể, mỗi cụm sẽ được phân loại chính xác bởi một tập con các chuyên gia. Mặt khác, mỗi chuyên gia sẽ hoạt động tốt trên ít nhất một cụm.

Hơn nữa, cùng với Định lý 4.1, Định lý 4.2 gợi ý rằng tồn tại các trường hợp bài toán trong Định nghĩa 3.1 (tức là, D_α=D_γ) sao cho MoE có thể chứng minh vượt trội hơn một chuyên gia duy nhất.

5 Tổng quan về Các Kỹ thuật Chính

Một lớp MoE thành công cần đảm bảo rằng bộ định tuyến có thể học các đặc trưng trung tâm cụm và chia bài toán phức tạp trong Định nghĩa 3.1 thành các bài toán phân loại tuyến tính đơn giản hơn mà các chuyên gia riêng lẻ có thể chinh phục. Việc tìm ra mạng cổng như vậy là khó khăn vì bài toán này rất phi lồi. Trong phần sau, chúng tôi sẽ giới thiệu những khó khăn chính trong việc phân tích lớp MoE và các kỹ thuật chính tương ứng để vượt qua những rào cản đó.

Khó khăn Chính 1: Sự Gián đoạn trong Định tuyến. So với mô hình định tuyến mềm truyền thống, mô hình định tuyến thưa thớt tiết kiệm tính toán và giảm đáng kể thời gian suy luận. Tuy nhiên, hình thức thưa thớt này cũng gây ra sự gián đoạn trong định tuyến (Shazeer et al., 2017). Thực tế, ngay cả một nhiễu nhỏ của đầu ra mạng cổng h(x,θ)+ε có thể thay đổi hành vi bộ định tuyến một cách đáng kể nếu đầu ra mạng cổng lớn thứ hai gần với đầu ra mạng cổng lớn nhất.

Kỹ thuật Chính 1: Ổn định bằng Làm mịn. Chúng tôi chỉ ra rằng thành phần nhiễu được thêm vào đầu ra mạng cổng đảm bảo một chuyển đổi mượt mà giữa các hành vi định tuyến khác nhau, làm cho bộ định tuyến ổn định hơn. Điều này được chứng minh trong bổ đề sau.

Bổ đề 5.1. Đặt h,ĥ∈ℝ^M là đầu ra của mạng cổng và {r_m}_{m=1}^M là nhiễu được rút độc lập từ Unif[0,1]. Ký hiệu p,p̂∈ℝ^M là xác suất các chuyên gia được định tuyến, tức là, p_m=ℙ(argmax_{m'∈[M]}{h_{m'}+r_{m'}}=m), p̂_m=ℙ(argmax_{m'∈[M]}{ĥ_{m'}+r_{m'}}=m). Thì chúng ta có ∥p-p̂∥_1≤M^2∥ĥ-h∥_1.

Bổ đề 5.1 hàm ý rằng khi thay đổi của đầu ra mạng cổng tại lần lặp t và t' là nhỏ, tức là, ∥h(x,θ^{(t)})-h(x,θ^{(t')})∥_1≤ε, hành vi bộ định tuyến sẽ tương tự. Vì vậy việc thêm nhiễu cung cấp một chuyển đổi mượt mà từ thời gian t đến t'. Cũng đáng chú ý rằng θ được khởi tạo bằng không. Vì vậy h(x,θ^{(0)})=0 và do đó mỗi chuyên gia được định tuyến với cùng xác suất p_m=1/M bởi tính chất đối xứng. Do đó, ở đầu huấn luyện khi ∥h(x,θ^{(t)})-h(x,θ^{(0)})∥_1 nhỏ, bộ định tuyến sẽ gần như đồng đều chọn một chuyên gia từ [M], giúp khám phá trên các chuyên gia.

Khó khăn Chính 2: Không có "Chuyên gia Thực". Ở đầu huấn luyện, mạng cổng bằng không, và các chuyên gia được khởi tạo ngẫu nhiên. Do đó khó để bộ định tuyến học các đặc trưng đúng vì tất cả các chuyên gia trông giống nhau: họ chia sẻ cùng kiến trúc mạng và được huấn luyện bởi cùng thuật toán. Sự khác biệt duy nhất sẽ là khởi tạo. Hơn nữa, nếu bộ định tuyến mắc lỗi ở đầu huấn luyện, các chuyên gia có thể khuếch đại lỗi vì các chuyên gia sẽ được huấn luyện dựa trên dữ liệu được phân phối sai.

Kỹ thuật Chính 2: Chuyên gia từ Khám phá. Được thúc đẩy bởi kỹ thuật chính 1, chúng tôi giới thiệu giai đoạn khám phá vào phân tích lớp MoE trong đó bộ định tuyến gần như đồng đều chọn một chuyên gia từ [M]. Giai đoạn này bắt đầu tại t=0 và kết thúc tại T_1=⌊σ_0^{-0.5}⌋≪T=Õ(1) và mạng cổng vẫn gần như không thay đổi ∥h(x,θ^{(t)})-h(x,θ^{(0)})∥_1=O(σ_0^{1.5}). Vì các chuyên gia được đối xử gần như bằng nhau trong giai đoạn khám phá, chúng tôi có thể chỉ ra rằng các chuyên gia trở nên chuyên môn hóa cho một số tác vụ cụ thể chỉ dựa trên khởi tạo. Cụ thể, tập chuyên gia [M] có thể được chia thành K tập không rỗng rời rạc [M]=⊔_k M_k, trong đó M_k:={m|argmax_{k'∈[K],j∈[J]}⟨v_{k'},w_{m,j}^{(0)}⟩=k}. Đối với MoE phi tuyến với hàm kích hoạt lập phương, bổ đề sau tiếp tục chỉ ra rằng các chuyên gia trong tập M_k khác nhau sẽ phân kỳ ở cuối giai đoạn khám phá.

Bổ đề 5.2. Dưới cùng điều kiện như trong Định lý 4.2, với xác suất ít nhất 1-o(1), các phương trình sau đúng cho tất cả chuyên gia m∈M_k,

ℙ_{(x,y)∼D}[yf_m(x,W^{(T_1)})≤0|(x,y)∈Ω_k]=o(1),

ℙ_{(x,y)∼D}[yf_m(x,W^{(T_1)})≤0|(x,y)∈Ω_{k'}]=Θ(1/K), ∀k'≠k.

Bổ đề 5.2 hàm ý rằng, ở cuối giai đoạn khám phá, chuyên gia m∈M_k có thể đạt được lỗi kiểm tra gần như bằng không trên cụm Ω_k nhưng lỗi kiểm tra cao trên các cụm khác Ω_{k'}, k'≠k.

Khó khăn Chính 3: Mất cân bằng Tải Chuyên gia. Cho tập dữ liệu huấn luyện S={(x_i,y_i)}_{i=1}^n, tải của chuyên gia m tại lần lặp t được định nghĩa là

Load_m^{(t)}=∑_{i∈[n]}ℙ(m_{i,t}=m),

trong đó ℙ(m_{i,t}=m) là xác suất đầu vào x_i được định tuyến tới chuyên gia m tại lần lặp t. Eigen et al. (2013) đầu tiên mô tả các vấn đề mất cân bằng tải trong huấn luyện lớp MoE. Mạng cổng có thể hội tụ tới một trạng thái nơi nó luôn tạo ra Load_m^{(t)} lớn cho cùng một vài chuyên gia. Sự mất cân bằng tải chuyên gia này tự củng cố, vì các chuyên gia được ưa chuộng được huấn luyện nhanh hơn và do đó được bộ định tuyến chọn thường xuyên hơn (Shazeer et al., 2017; Fedus et al., 2021). Vấn đề mất cân bằng tải chuyên gia không chỉ gây ra vấn đề bộ nhớ và hiệu suất trong thực tế, mà còn cản trở phân tích lý thuyết của việc huấn luyện chuyên gia.

Kỹ thuật Chính 3: Gradient Descent Chuẩn hóa. Bổ đề 5.2 chỉ ra rằng các chuyên gia sẽ phân kỳ thành ⊔_{k∈[K]}M_k. Gradient descent chuẩn hóa có thể giúp các chuyên gia khác nhau trong cùng M_k được huấn luyện với tốc độ như nhau bất kể sự mất cân bằng tải do bộ định tuyến gây ra. Vì vòng tự củng cố không còn tồn tại, chúng tôi có thể chứng minh rằng bộ định tuyến sẽ đối xử với các chuyên gia khác nhau trong cùng M_k gần như bằng nhau và phân phối gần như cùng lượng dữ liệu cho họ (Xem Phần E.2 trong Phụ lục để biết chi tiết). Vấn đề mất cân bằng tải này có thể được tránh thêm bằng cách thêm mất mát cân bằng tải (Eigen et al., 2013; Shazeer et al., 2017; Fedus et al., 2021), hoặc cấu trúc lớp MoE tiên tiến như BASE Layers (Lewis et al., 2021; Dua et al., 2021) và Hash Layers (Roller et al., 2021).

Lộ trình: Ở đây chúng tôi cung cấp lộ trình chứng minh Định lý 4.2 và chứng minh đầy đủ được trình bày trong Phụ lục E. Quá trình huấn luyện có thể được phân tách thành nhiều giai đoạn. Giai đoạn đầu tiên được gọi là giai đoạn Khám phá. Trong giai đoạn này, các chuyên gia sẽ phân kỳ thành K nhóm chuyên nghiệp ⊔_{k=1}^K M_k=[M]. Cụ thể, chúng tôi sẽ chỉ ra rằng M_k không rỗng cho tất cả k∈[K]. Ngoài ra, cho tất cả m∈M_k, f_m là một bộ phân loại tốt trên Ω_k. Giai đoạn thứ hai được gọi là giai đoạn học bộ định tuyến. Trong giai đoạn này, bộ định tuyến sẽ học phân phối x∈Ω_k tới một trong các chuyên gia trong M_k. Cuối cùng, chúng tôi sẽ đưa ra phân tích tổng quát hóa cho các MoE từ hai giai đoạn trước.

6 Thí nghiệm

Thiết lập 1: α∼Uniform(0.5,2), β∼Uniform(1,2), γ∼Uniform(0.5,3); σ_p=1

Độ chính xác kiểm tra (%) | Entropy Phân phối
--- | ---
Đơn lẻ (tuyến tính) | 68.71 | NA
Đơn lẻ (phi tuyến) | 79.48 | NA
MoE (tuyến tính) | 92.99±2.11 | 1.300±0.044
MoE (phi tuyến) | 99.46±0.55 | 0.098±0.087

Thiết lập 2: α∼Uniform(0.5,2), β∼Uniform(1,2), γ∼Uniform(0.5,3), σ_p=2

Độ chính xác kiểm tra (%) | Entropy Phân phối
--- | ---
Đơn lẻ (tuyến tính) | 60.59 | NA
Đơn lẻ (phi tuyến) | 72.29 | NA
MoE (tuyến tính) | 88.48±1.96 | 1.294±0.036
MoE (phi tuyến) | 98.09±1.27 | 0.171±0.103

Bảng 1: So sánh giữa MoE (tuyến tính) và MoE (phi tuyến) trong thiết lập của chúng tôi. Chúng tôi báo cáo kết quả của cổng top-1 với nhiễu cho cả mô hình tuyến tính và phi tuyến. Trên mười thí nghiệm ngẫu nhiên, chúng tôi báo cáo giá trị trung bình ± độ lệch chuẩn cho cả độ chính xác kiểm tra và entropy phân phối.

6.1 Thí nghiệm Dữ liệu Tổng hợp

Bộ dữ liệu. Chúng tôi tạo 16,000 ví dụ huấn luyện và 16,000 ví dụ kiểm tra từ phân phối dữ liệu được định nghĩa trong Định nghĩa 3.1 với số cụm K=4, số vùng P=4 và chiều d=50. Chúng tôi xáo trộn ngẫu nhiên thứ tự các vùng của x sau khi chúng tôi tạo dữ liệu (x,y). Chúng tôi xem xét hai thiết lập tham số: 1. α∼Uniform(0.5,2), β∼Uniform(1,2), γ∼Uniform(0.5,3) và σ_p=1; 2. α∼Uniform(0.5,2), β∼Uniform(1,2), γ∼Uniform(0.5,3) và σ_p=2. Lưu ý rằng Định lý 4.1 chỉ ra rằng khi α và γ tuân theo cùng phân phối, cả chuyên gia tuyến tính đơn lẻ hay chuyên gia phi tuyến đơn lẻ đều không thể đưa ra hiệu suất tốt. Ở đây chúng tôi xem xét một thiết lập tổng quát và khó khăn hơn khi α và γ từ các phân phối khác nhau.

Mô hình. Chúng tôi xem xét hiệu suất của CNN tuyến tính đơn lẻ, CNN phi tuyến đơn lẻ, MoE tuyến tính, và MoE phi tuyến. Kiến trúc CNN phi tuyến đơn lẻ tuân theo (3.1) với hàm kích hoạt lập phương, trong khi CNN tuyến tính đơn lẻ tuân theo (3.1) với hàm kích hoạt đồng nhất. Cho cả MoE tuyến tính và phi tuyến, chúng tôi xem xét hỗn hợp 8 chuyên gia với mỗi chuyên gia là một CNN tuyến tính đơn lẻ hoặc một CNN phi tuyến đơn lẻ. Cuối cùng, chúng tôi huấn luyện các mô hình đơn lẻ với gradient descent và huấn luyện các MoE với Thuật toán 1. Chúng tôi chạy 10 thí nghiệm ngẫu nhiên và báo cáo độ chính xác trung bình với độ lệch chuẩn.

Đánh giá. Để đánh giá mức độ bộ định tuyến học được cấu trúc cụm cơ bản của dữ liệu, chúng tôi định nghĩa entropy của phân phối bộ định tuyến như sau. Ký hiệu n_{k,m} là số dữ liệu trong cụm k được phân phối tới chuyên gia m. Tổng số dữ liệu được phân phối tới chuyên gia m là n_m=∑_{k=1}^K n_{k,m} và tổng số dữ liệu là n=∑_{k=1}^K ∑_{m=1}^M n_{k,m}. Entropy phân phối sau đó được định nghĩa là

entropy = -∑_{m=1,n_m≠0}^M \frac{n_m}{n} ∑_{k=1}^K \frac{n_{k,m}}{n_m} log \frac{n_{k,m}}{n_m}.

Khi mỗi chuyên gia nhận dữ liệu từ nhiều nhất một cụm, entropy phân phối sẽ bằng không. Và một phân phối đồng đều sẽ dẫn đến entropy phân phối tối đa.

Như được chỉ ra trong Bảng 1, MoE tuyến tính không hoạt động tốt bằng MoE phi tuyến trong Thiết lập 1, với khoảng 6% độ chính xác kiểm tra ít hơn và phương sai cao hơn nhiều. Với nhiễu ngẫu nhiên mạnh hơn (Thiết lập 2), sự khác biệt giữa MoE phi tuyến và MoE tuyến tính trở nên thậm chí còn đáng kể hơn. Chúng tôi cũng quan sát thấy rằng entropy phân phối cuối cùng của MoE phi tuyến gần như bằng không trong khi của MoE tuyến tính là lớn. Trong Hình 3, chúng tôi tiếp tục thể hiện sự thay đổi của entropy phân phối trong quá trình huấn luyện. Entropy phân phối của MoE phi tuyến giảm đáng kể, trong khi của MoE tuyến tính vẫn lớn. Hiện tượng như vậy chỉ ra rằng MoE phi tuyến có thể học thành công cấu trúc cụm cơ bản của dữ liệu trong khi MoE tuyến tính không làm được.

6.2 Thí nghiệm Dữ liệu Thực tế

Chúng tôi tiếp tục tiến hành thí nghiệm trên bộ dữ liệu hình ảnh thực tế và thể hiện tầm quan trọng của cấu trúc dữ liệu cụm đối với lớp MoE trong mạng nơ-ron sâu.

Bộ dữ liệu. Chúng tôi xem xét bộ dữ liệu CIFAR-10 (Krizhevsky, 2009) và tác vụ phân loại 10 lớp. Hơn nữa, chúng tôi tạo ra một bộ dữ liệu CIFAR-10-Rotate có cấu trúc cụm cơ bản mạnh mẽ độc lập với hàm gắn nhãn của nó. Cụ thể, chúng tôi xoay các hình ảnh 30 độ và hợp nhất bộ dữ liệu đã xoay với bộ dữ liệu gốc. Nhiệm vụ là dự đoán xem hình ảnh có bị xoay hay không, đây là một bài toán phân loại nhị phân. Chúng tôi coi rằng một số lớp trong CIFAR-10 tạo thành các cụm cơ bản trong CIFAR-10-Rotate. Trong Phụ lục A, chúng tôi giải thích chi tiết cách chúng tôi tạo ra CIFAR-10-Rotate và trình bày một số ví dụ cụ thể.

Mô hình. Đối với MoE, chúng tôi xem xét hỗn hợp 4 chuyên gia với mạng cổng tuyến tính. Đối với các kiến trúc chuyên gia/mô hình đơn lẻ, chúng tôi xem xét CNN với 2 lớp tích chập (chi tiết kiến trúc được minh họa trong Phụ lục A). Để đánh giá kỹ lưỡng hơn, chúng tôi cũng xem xét các mô hình chuyên gia/đơn lẻ với kiến trúc bao gồm MobileNetV2 (Sandler et al., 2018) và ResNet18 (He et al., 2016). Quá trình huấn luyện MoE cũng tuân theo Thuật toán 1.

Kết quả thí nghiệm được chỉ ra trong Bảng 2, nơi chúng tôi so sánh các mô hình đơn lẻ và hỗn hợp của các kiến trúc khác nhau trên bộ dữ liệu CIFAR-10 và CIFAR-10-Rotate. Chúng tôi quan sát thấy rằng sự cải thiện của MoE so với các mô hình đơn lẻ khác nhau rất nhiều trên các bộ dữ liệu khác nhau. Trên CIFAR-10, hiệu suất của MoE rất gần với các mô hình đơn lẻ. Tuy nhiên, trên bộ dữ liệu CIFAR-10-Rotate, chúng ta có thể quan sát thấy sự cải thiện hiệu suất đáng kể từ các mô hình đơn lẻ đến MoE. Những kết quả như vậy chỉ ra lợi thế của MoE so với các mô hình đơn lẻ phụ thuộc vào tác vụ và cấu trúc cụm của dữ liệu.

CIFAR-10 (%) | CIFAR-10-Rotate (%)
--- | ---
CNN Đơn lẻ | 80.68±0.45 | 76.78±1.79
MoE | 80.31±0.62 | 79.60±1.25
MobileNetV2 Đơn lẻ | 92.45±0.25 | 85.76±2.91
MoE | 92.23±0.72 | 89.85±2.54
ResNet18 Đơn lẻ | 95.51±0.31 | 88.23±0.96
MoE | 95.32±0.68 | 92.60±2.01

Bảng 2: So sánh giữa MoE và mô hình đơn lẻ trên bộ dữ liệu CIFAR-10 và CIFAR-10-Rotate. Chúng tôi báo cáo độ chính xác kiểm tra trung bình trên 10 thí nghiệm ngẫu nhiên ± độ lệch chuẩn.

7 Kết luận và Hướng nghiên cứu tương lai

Trong công trình này, chúng tôi nghiên cứu chính thức cơ chế của lớp Hỗn hợp Chuyên gia (MoE) cho học sâu. Theo hiểu biết của chúng tôi, chúng tôi cung cấp kết quả lý thuyết đầu tiên hướng tới việc hiểu cách lớp MoE hoạt động trong học sâu. Bằng chứng thực nghiệm của chúng tôi tiết lộ rằng cấu trúc cụm của dữ liệu đóng vai trò quan trọng trong thành công của lớp MoE. Được thúc đẩy bởi những quan sát thực nghiệm này, chúng tôi nghiên cứu một phân phối dữ liệu với cấu trúc cụm và chỉ ra rằng Hỗn hợp Chuyên gia có thể chứng minh cải thiện độ chính xác kiểm tra của một chuyên gia đơn lẻ là CNN hai lớp.

Có một số hướng nghiên cứu tương lai quan trọng. Thứ nhất, kết quả hiện tại của chúng tôi dành cho CNN. Thú vị khi mở rộng kết quả của chúng tôi cho các kiến trúc mạng nơ-ron khác, như transformer. Thứ hai, phân phối dữ liệu của chúng tôi được thúc đẩy bởi bài toán phân loại dữ liệu hình ảnh. Chúng tôi dự định mở rộng phân tích của chúng tôi cho các loại dữ liệu khác (ví dụ, dữ liệu ngôn ngữ tự nhiên).
