# 2405.16836.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2405.16836.pdf
# File size: 357367 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Enhancing Fast Feed Forward Networks with
Load Balancing and a Master Leaf Node
Andreas Charalampopoulos1
andcharalamp@gmail.comNikolas Chatzis1
chatznikolas@gmail.com
Foivos Ntoulas-Panagiotopoulos1
foivosdoulas@hotmail.grCharilaos Papaioannou1
cpapaioan@mail.ntua.grAlexandros Potamianos1
potam@central.ntua.gr
1School of ECE, National Technical University of Athens, Greece
Abstract
Fast feedforward networks (FFFs) are a class of neural networks that exploit the
observation that different regions of the input space activate distinct subsets of
neurons in wide networks. FFFs partition the input space into separate sections
using a differentiable binary tree of neurons and during inference descend the
binary tree in order to improve computational efficiency. Inspired by Mixture of
Experts (MoE) research, we propose the incorporation of load balancing and Master
Leaf techniques into the FFF architecture to improve performance and simplify
the training process. We reproduce experiments found in literature and present
results on FFF models enhanced using these techniques. The proposed architecture
and training recipe achieves up to 16.3% and 3% absolute classification accuracy
increase in training and test accuracy, respectively, compared to the original FFF
architecture. Additionally, we observe a smaller variance in the results compared
to those reported in prior research. These findings demonstrate the potential of
integrating MoE-inspired techniques into FFFs for developing more accurate and
efficient models.
1 Introduction
Recently, models with billions of parameters have had great success in generative artificial intelligence
applications [ 1,2,3]. But alongside those impressive results, came the burdensome computational
complexity of the FeedForward (FF) layer inference, which is especially present in Transformers[ 4].
It has been observed that in wide FF layers, different parts of the input domain activate distinct sets
of neurons; this observation can be leveraged to design more efficient models[ 5]. As a result the
idea of achieving better computational efficiency from sparsely-activated models has gained much
attention[6, 7].
Mixture of Experts (MoE) is an early attempt to take advantage of this sparsity, and continues to
be a topic of interest [ 8,9,10]. Recent work on sparsely-activated architectures includes Fast Feed
Forward networks (FFF) [11]. The authors in [ 12,11] indicate that FFFs can be used instead of
vanilla FF and MoE architectures in transformers and Large Language Models (LLM) without
incurring any significant loss in accuracy, while realizing a considerable speed-up during inference.
Inference acceleration in FFFs is achieved through a tree-conditional activation of neurons.
While trying to reproduce experiments from [ 11], we verified that FFFs suffer from training instability.
This can be also inferred from the large variance in results that are reported also in Table 5 of [ 11],
where the variance among identical training runs is high. Further we observed that certain subtreesarXiv:2405.16836v1  [cs.LG]  27 May 2024

--- PAGE 2 ---
in the FFF architecture were activated significantly more than others during inference, i.e., there
was significant imbalance on the utilization of the FFF. To address these two issues and motivated
by the MoE literature [ 13], we propose two modifications to the FFF architecture: 1) introducing
load balancing to better utilize all FFF subtrees, and 2) adding a master leaf node in parallel to
the FFF topology that contributes to the output with a constant mixture coefficient, so that input
sequences that cause “wider” neural activation patterns can be better serviced. We show that the
proposed enhancements improve classification performance on the MNIST and FashionMNIST
datasets. Further we show that the enhanced FFFs achieve better overall training stability compared
to vanilla FFFs.
Our contributions can be summarized as follows:
1.We propose an enhanced FFF architecture (eFFF) that incorporates a load balancing term at
the loss function and a master leaf node that gets linearly mixed with the FFF output.
2.We provide experimental validation on the MNIST and FashionMNIST datasets showing
that the proposed method yields better classification accuracy both during training and
testing, and leads to more stable training runs (reduced variance). Further, we perform
ablation experiments showing the contribution of each proposed enhancement.
3.We also provide all the code necessary to reproduce our experiments in the following GitHub
repository2.
2 Related Work
The importance of inference speedup in feedforward neural networks is widely recognised and
several approaches have been proposed. Recent works have successfully managed to reduce the
feedforward layer inference time. The Mixture of Experts (MoE) approach, as explored in Shazeer et
al. (2017) [ 9], has demonstrated its effectiveness towards inference speedup. MoE involves dividing
the feedforward layer into distinct sets of neurons known as “experts”, with a gating layer trained to
select which mixture of experts to utilize during the forward pass. This method enhances inference
speed by utilizing only the top-performing kblocks, or a similar variation thereof. It effectively
reduces inference time by a constant factor while maintaining a linear relationship with the width of
the feedforward layer. However, it depends on noisy gating to balance the load among the experts,
adding complexity to the training process and encouraging redundancy.
In [11], the authors introduced the Fast Feedforward (FFF) architecture as an alternative to the
feedforward (FF) architecture. FFF operates by accessing blocks of its neurons in logarithmic time,
offering improved efficiency. It accomplishes this by dividing the input space into separate regions
using a differentiable binary tree, simultaneously learning the boundaries of these regions and the
neural blocks assigned to them. Neurons are executed conditionally based on the tree structure during
inference: a subset of node neurons determines the mixtures of leaf neuron blocks required to generate
the final output. Further in [ 11,12], the authors demonstrate that FFFs can be up to 220 times faster
than feedforward networks and up to 6 times faster than mixture-of-experts networks. Additionally,
the authors claim that FFFs exhibit superior training properties compared to mixture-of-experts
networks due to their noiseless conditional execution approach.
In this paper, we utilize the concept of load balancing, previously introduced in MoE[ 10,14,8], to
ensure a balanced load across FFF’s leaves, aiming to improve training stability. In the context of
MoE,[ 9] an additional term in the loss function is introduced, in order to encourage experts to receive
roughly equal numbers of training examples. This idea proves to be significant for load balancing
purposes on distributed hardware.
Furthermore, we propose mixing the FFF’s output with that of another neural network with much
fewer neurons. We call this network “master leaf” as it is similar to the leaves of FFF. The weight of
the output of the master leaf is set to be a trainable parameter. Inspiration for this was drawn from[ 15],
where authors proposed enhancing MoE performance by integrating a base network alongside the
selected expert. This is shown to not only improves model accuracy, but also provides an early exit
output during inference, reducing computational redundancy for “easier” samples. Additionally,
computational efficiency is achieved by reusing early layers of the base model as inputs to the gate
and the experts.
2https://github.com/AndreasCharalamp/fastfeedforward-experiments
2

--- PAGE 3 ---
3 Method
3.1 FFF architecture
Fast feedforward networks (FFFs) are designed to capitalize on the phenomenon wherein different
parts of the input domain activate distinct sets of neurons in wide networks. FFFs partition the input
space into separate sections using a differentiable binary tree, enabling the concurrent learning of
both the boundaries delineating these sections and the neural units associated with them. This is
accomplished through the tree-conditional activation of neurons: a designated subset of node neurons
determines the combinations of leaf neuron blocks to be computed for generating the output.
3.2 Training Process
The nodes are arranged in a differentiable tree that makes a soft choice over the leaves in the form of
a stochastic vector. In training, FFF performs a mixture of experts over all leaves in L, where Lis the
set of leaves, with the weights of the mixture computed by ascending through the tree from the root
node. During inference, the decision at each node is taken to be the greatest weight, and the forward
pass algorithm proceeds from the root, always choosing only one branch depending on the local
node decision. All leaves are simple Feed-Forward (FF) networks with one hidden layer of width ℓ,
and ReLU (Rectified Linear Unit) activation function. The nodes of the tree are simple neurons that
use sigmoid activation function. Following the notation of [ 11] we will refer to the total number of
neurons in each model (excluding the tree nodes in an FFF) as the training width and will denote it
asw. The number of neurons of each leaf will be denoted by ℓand we will call it leaf width . The
output of an FFF during training is of the following form:
FFF train(x) =X
1≤i≤|L|li(x)ci(x),(1)
whereP
1≤i≤|L|ci(x) = 1 ,|L|is the number of leaves, ℓi(x)is the output of leaf iandci(x)is the
mixture coefficient of leaf icomputed as the product of the edges in the path from the root to each
leaflias shown in Fig. 1.
Figure 1: Visualization of FFF training for tree depth 2.
3

--- PAGE 4 ---
During inference the output is computed by taking hard decisions at each level of the hierarchy
resulting in only c∗of the cibeing 1 and the rest being 0, i.e.,
FFF inference (x) =l∗(x), (2)
where l∗is the leaf that we end up on, following the edges of greater value. This way, even though
2d·ℓ+ 2d−1neurons are used for training, where dis the depth of the tree, only ℓ+d−1are used
for inference.
In [11] the following loss function is used:
L=Lpred+h L harden,
where Lpredis the task cross entropy loss, Lharden is a term that pushed the decisions at each level of
the tree to be either 0 or 1 and his the training hyperparameter controlling the effect of the hardening.
Specifically, Lharden is defined as:
Lharden =X
i∈BX
N∈NH(N(i)),
whereBis a batch of samples, Nis the set of tree nodes of the FFF, H(p)the entropy of a Bernoulli
random variable p. This extra term is needed so that all edges of the tree have values close to 1or0
for all inputs. The hardening term is important because the FFF is trained to output predictions in the
form of a weighted sum of its leaves, while during inference we make hard 0 vs 1 decision while
descending the tree. In order for inference output FFF inference (x)to be as close as possible to training
ouput FFF train(x)(see Eqs. (1) and (2) above) we aim for all cito be near 0and only c∗to be close
to 1.
Thus, through the hardening term, we seek to force the weight of leaf l∗to be close to 1and the
weights of the rest of the leaves to be close to 0.
3.3 Load Balancing
During our training trials with FFFs we noted that they are highly sensitive to poor initialization of
weights. This is evident from the significant variability in test accuracy observed across multiple runs
of the same training procedure. Similar challenges are also noted in [11], particularly in the Table 4
in the Appendix, where accuracy variations are documented. To elaborate further, the loss function
does not promote a wide usage of the leaves. Consequently, during training, if a leaf is assigned to a
region of little relevance, it is likely to complete the training process without effectively capturing
any meaningful representation.
To tackle this, we study how this problem was addressed in MoE architectures. Following the idea
from [10] we propose to add the following term into the loss function:
Lbalance = 2dX
i∈leavesfiPi,
where fiis the fraction of the inputs dispatched to leaf liandPi=1
|B|P
x∈Bci(x)is the sum of
the coefficients of each leaf ion the current batch B.The term Lbalance is minimized when the load is
evenly balanced on all leaves. The resulting total loss L′is now
L′=Lpred+h L harden +α L balance ,
where αis a hyperparameter controlling the effect of the load balancing term.
3.4 Master Leaf
Inspired from [ 15], we experiment with the addition of an extra neural component. Instead of allowing
each partition set of the input space to be processed exclusively by independent sets of neurons
(leaves) during inference, we provide an additional set of neurons which contributes to the output for
all inputs, and not only a subset of them like the rest of the leaves. We introduce a master leaf, that
contributes to the final output with a factor k. During training, the output of the new architecture is
formulated as follows:
FFF ML Train(x) =kX
1≤i≤|L|li(x)ci(x) + (1 −k)ML(x),
4

--- PAGE 5 ---
where|L|is the number of the leaves, ℓi(x)is the output of leaf i,ci(x)is the mixture coefficient of
leafi,MLis the output of the master leaf and kis a trainable parameter with 0< k < 1. This linear
fusion method is further elucidated in Fig. 2.
Figure 2: Visualization of FFF training with master leaf architecture.
During inference , the output of the new architecture is formulated as follows:
FFF ML Inference (x) =k ℓ∗(x) + (1 −k)ML(x),
where ℓ∗(x)is the output of the leaf with the greatest mixture coefficient c∗(x).
The master leaf undergoes training concurrently with the FFF on the entire dataset. Each FFF leaf
is tasked with handling a distinct subset of the input space. Consequently, the introduction of the
master leaf enriches the “localized” output of a leaf through the incorporation of the well-trained
feedforward network output1.
4 Experimental Setup
We conduct a series of experiments to investigate the benefits in performance resulting from:
(1) the inclusion of the load balancing term in the loss function and
(2) the integration of the output of an FFF with the master leaf output, as described above.
1The master leaf output can be calculated in parallel with the output of the leaf chosen from the FFF.
Consequently, with proper implementation, it should not significantly affect inference speed.
5

--- PAGE 6 ---
Building upon the foundation laid in [ 11], we adopt training and test accuracy as our evaluation metrics
to facilitate direct comparison with the literature. Each experiment focuses on image classification,
with classification accuracy assessed through the softmax of output logits in the usual way. Results
are reported on the MNIST and FashionMNIST image classification databases. The reader can refer
to [11] for details on the database and experimental setup, which are mirrored here.
4.1 Experiments 1 and 2: Load Balancing
In order to investigate the effect of load balancing we reproduce the experiment from Table 1 in
[11] (referred henceforth as baseline ) and compare the performance when using the load balancing
term in the loss function (referred henceforth as balanced ). We report classification accuracy on the
MNIST and FashionMNIST datasets for the following sets of parameters in experiment 1: leaf width
l∈ {8,4,2,1}and training width w= 16 . We train for 300epochs with learning rate lr= 0.001,
loss hyperparameters h= 1, α= 1and another 300epochs with lr= 0.001, h= 3, α= 0. We use
the Adam optimizer and early stopping (if no increase in loss is observed over 50 epochs).
Additionally in experiment 2, we explore cases for the FashionMNIST database where training width
isw= 128 ,l∈ {8,4,2,1}and also l∈ {64,32,16}that were not included in the initial study. This
allows us to observe the accuracy attained when the leaf size approaches that of a simple feedforward
network.
We perform 10 training runs and report best accuracy and worst accuracy in Tables 1 and 2.
4.2 Experiment 3: Master Leaf with Load Balancing
Next, we investigate the performance of Master Leaf architecture on the MNIST dataset. For this
experiment we fix the master leaf size at 8and also include the load balancing term in the loss
function (henceforth referred to as “ master leaf + balanced ”). Training takes place for 200epochs
withlr= 0.001, h= 1, α= 1 and another 100epochs with lr= 0.001, h= 3, α= 0. We train
using the Adam optimizer and early stopping (if no increase is observed for over 50epochs). We
perform 5 training runs and report best accuracy and worst accuracy in Table 3.
We publish the parameters for all trained models in our GitHub repository (see link in Introduction).
5 Experimental Results
5.1 Experiment 1: Load Balancing
The results for the baseline FFF model as reported in [ 11] and the load balanced FFF model are
shown in Table 1 for the MNIST and FashionMNIST datasets. The load balanced FFF model with
the proposed training strategy outperforms the baseline in all settings. Specifically, we observe an
increase in training accuracy up to 16.3%absolute, achieved for ℓ= 1for FashionMNIST, while
the test accuracy exhibits a maximum increase of 3.0%, achieved for ℓ= 4 for FashionMNIST.
The average absolute training accuracy improvement for MNIST is 2.3%that translates to 27%
relative error reduction. Test accuracy improvement is small typically 0.5%absolute for MNIST, but
consistent and significant for FashionMNIST on average 2.2%absolute and 10% relative error rate
reduction.
Moreover, it is apparent that accuracy variability among training runs has diminished by 4 to 5 times
on average for both training and testing when using load balancing. However, accuracy variability
remains significantly higher than for vanilla FFs. We believe variance in deep models remained high
because asking our model to partition MNIST and FashionMNIST into w= 16 meaningful regions
might lead to overfragmentation of the input space, as explained in [ 11]. One last thing to note is
that the load balancing term appears to introduce overfitting especially for deeper models, i.e., the
training accuracy improves faster than the test accuracy.
5.2 Experiment 2: Load Balancing with Larger Training and Leaf Width
The increase in accuracy is made more apparent via Table 2 where we present results also for w= 128
case for FashionMNIST and also for deeper models.
6

--- PAGE 7 ---
MNIST
w= 16
train accuracy test accuracy
baseline balanced baseline balanced
vanilla FF 98.0±0.9 - 95.2±0.5 -
ℓ= 8 94.6±19.5 94.6±7.0 93.1±16.693.5±6.1
ℓ= 4 91.6±29.3 94.2±3.9 90.8±27.291.3±8.9
ℓ= 2 92.1±7.3 95.0±1.7 90.3±5.6 91.0±2.7
ℓ= 1 91.7±7.4 95.2±3.0 89.9±6.4 89.0±8.1
FashionMNIST
w= 16
train accuracy test accuracy
baseline balanced baseline balanced
vanilla FF 91.0±0.7 - 86.4±0.4 -
ℓ= 8 86.7±12.1 90±1.5 84.2±10.986.1±1.1
ℓ= 4 86.4±25.0 89.5±0.6 83.3±24.5 85.8±0.9
ℓ= 2 84.5±21.091.2±1.4 83.0±11.0 85.4±2.5
ℓ= 1 79.7±9.0 92.7±1.6 78.4±8.0 80.3±9.1
Table 1: Training and test image classification accuracy of baseline and load balanced models on
MNIST and FashionMNIST. wis the training width, ℓis the leaf width. Results with grey background
are copied from [ 11] for comparison. x±ymeans that, from the 10 training runs best accuracy was
xand worst was x−y.
FashionMNIST
w= 16 w= 128
train accuracy test accuracy train accuracy test accuracy
baseline balanced baseline balanced baseline balanced baseline balanced
vanilla FF 91.0 - 86.4 - 99.3 - 89.6 -
ℓ= 64 - - - - 95.6 97.0 88.8 88.9
ℓ= 32 - - - - 93.1 96.5 87.9 88.2
ℓ= 16 - - - - 92.5 94.3 87.1 87.5
ℓ= 8 86.7 90.0 84.2 86.1 90.5 92.8 86.1 86.7
ℓ= 4 86.4 89.5 83.3 85.8 89.0 89.6 85.4 85.8
ℓ= 2 84.5 91.2 83.0 85.4 87.3 88.3 84.3 84.8
ℓ= 1 79.7 92.7 78.4 80.3 78.7 84.5 77.7 79.9
Table 2: Training and test accuracy attained with load balancing (baseline vs balanced) for the
FashionMNIST database and for larger training and leaf widths. Baseline results that are copied from
[11] are highlighted in grey, baseline results for ℓ= 16,32,64are our own.
We observe that load balancing improves the accuracy over the baseline FFF model for all setups.
Forℓ∈ {1,2}we observe that we have better results for the w= 16 case rather than w= 128
probably due to input space overfragmentation mentioned before. Results could be further improved
if we harden our models for more epochs. Note that load balancing provides consistent accuracy
improvement even for best performing deep models. The more the leaves in the model, the harder it
is to find a good partition of the input space without using load balancing.
5.3 Experiment 3: Master Leaf and Load Balancing
Results on MNIST when adding a master leaf node of size 8are shown in Table 3. Compared to the
baseline and Table 1 performance (using load balancing only) we see significant improvement on
training accuracy both for w= 16 andw= 128 . Test accuracy also improves in the vast majority
of the cases. As expected, the improvement is greater for w= 16 than for w= 128 , typically 3.8%
vs1.3%absolute accuracy improvement, respectively. Additionally, adding the master leaf further
reduces the performance variability among runs bringing it to reasonable levels comparable to vanilla
FF for w= 16 . Overall, mixing the output of an FFF with the output of a simple neural network is a
very promising direction.
7

--- PAGE 8 ---
MNIST
w= 16
train accuracy test accuracy
baseline master leaf + balanced baseline master leaf + balanced
vanilla FF 98.0±0.9 - 95.2±0.5 -
ℓ= 8 94.6±19.5 96.7±1.4 93.1±16.6 94.8±0.5
ℓ= 4 91.6±29.3 96.7±1.6 90.8±27.2 94.7±2.0
ℓ= 2 92.1±7.3 97.2±1.5 90.3±5.6 94.1±1.1
ℓ= 1 91.7±7.4 97.3±0.9 89.9±6.4 93.8±1.8
w= 128
train accuracy test accuracy
baseline master leaf + balanced baseline master leaf + balanced
vanilla FF 100±0.0 - 98.1±0.1 -
ℓ= 8 99.3±1.0 100±0.0 94.9±0.6 95.1±0.3
ℓ= 4 97.6±0.6 99.8±0.5 93.6±0.5 95.0±1.8
ℓ= 2 96.2±1.4 99.7±2.6 92.4±0.6 93.7±3.1
ℓ= 1 94.1±0.9 99.7±0.7 92.0±0.7 91.6±10.1
Table 3: Training and test accuracy attained with master leaf models also using the load balancing
loss term for the MNIST database. wis the training width, ℓis the leaf width. Baseline results copied
from [ 11] are highlighted in grey. x±ymeans that from the 5 training runs best accuracy was xand
worst was x−y.
6 Conclusions
We enhanced the FFF architecture proposed in [ 11] with a load balancing loss term and a master
leaf node achieving consistently improved accuracy for the MNIST and FashionMNIST image
classification tasks. Particularly noteworthy is the increase in accuracy for deep FFFs. Equally
noteworthy is the reduction in accuracy variability across our training runs. This result underscores
the robustness conferred by the incorporation of the load balancing term and master leaf architecture
into FFFs. The main conclusions from the 3 experiments and proposed future directions are discussed
next:
1.Experiment 1 results confirm our belief that the largely varying test accuracy are caused
by unbalanced trees. Adding the load balancing term in our training we achieve better leaf
utilization resulting in increased robustness.
2.Experiment 2 results indicate that we can achieve significantly better performance using
leaf balance, as we surpassed [ 11] best accuracy for all w, ℓ. Thus, we believe it is worth
evaluating the performance using more training epochs and a larger range of parameters to
fully explore the potential of the method.
3.Experiment 3 results show that the master leaf architecture outperforms FFF models, in
terms of test and train accuracy, for all cases investigated. Expanding these experiments to
other datasets and exploring various values of master leaf width holds significant potential
for further performance improvements.
Limitations: We did not explore fully the (hyper-) parameter space due to computational resource
limitations; it is possible that results can be further improved via parameter tuning.
Acknowledgements
We wish to thank the authors of [ 11] for their guidance on the FFF implementation. This work was
part of a term project for the Pattern Recognition class of the ECE curriculum at NTUA.
8

--- PAGE 9 ---
References
[1]A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding
by generative pre-training,” preprint online: https://cdn.openai.com/research-covers/language-
unsupervised/language_understanding_paper.pdf , 2018.
[2]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child,
A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,
B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei,
“Language models are few-shot learners,” arXiv preprint arXiv: 2005.14165 , 2020.
[3]J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford,
J. Wu, and D. Amodei, “Scaling laws for neural language models,” arXiv preprint arXiv:
2001.08361 , 2020.
[4]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I. Polosukhin, “Attention is all you need,” arXiv preprint arXiv: 1706.03762 , 2023.
[5]E. Bengio, P.-L. Bacon, J. Pineau, and D. Precup, “Conditional computation in neural networks
for faster models,” arXiv preprint arXiv: 1511.06297 , 2016.
[6]S. Gray, A. Radford, and D. P. Kingma, “GPU kernels for block-sparse weights.” online:
https://openai.com/research/block-sparse-gpu-kernels , 2017.
[7]T. Gale, M. Zaharia, C. Young, and E. Elsen, “Sparse GPU kernels for deep learning,” arXiv
preprint arXiv: 2006.10901 , 2020.
[8]D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen,
“GShard: Scaling giant models with conditional computation and automatic sharding,” arXiv
preprint arXiv: 2006.16668 , 2020.
[9]N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, “Outrageously
large neural networks: The sparsely-gated mixture-of-experts layer,” arXiv preprint arXiv:
1701.06538 , 2017.
[10] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to trillion parameter models
with simple and efficient sparsity,” arXiv preprint arXiv: 2101.03961 , 2021.
[11] P. Belcak and R. Wattenhofer, “Fast feedforward networks,” arXiv preprint arXiv: 2308.14711 ,
2023.
[12] ——, “Exponentially faster language modelling,” arXiv preprint arXiv: 2311.10770 , 2023.
[13] D. Dai, C. Deng, C. Zhao, R. X. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y . Wu, Z. Xie,
Y . K. Li, P. Huang, F. Luo, C. Ruan, Z. Sui, and W. Liang, “DeepSeekMoE: Towards ultimate
expert specialization in mixture-of-experts language models,” arXiv preprint arXiv: 2401.06066 ,
2024.
[14] N. Shazeer, Y . Cheng, N. Parmar, D. Tran, A. Vaswani, P. Koanantakool, P. Hawkins, H. Lee,
M. Hong, C. Young, R. Sepassi, and B. Hechtman, “Mesh-TensorFlow: Deep learning for
supercomputers,” arXiv preprint arXiv: 1811.02084 , 2018.
[15] A. Royer, I. Karmanov, A. Skliar, B. E. Bejnordi, and T. Blankevoort, “Revisiting single-gated
mixtures of experts,” arXiv preprint arXiv: 2304.05497 , 2023.
9
