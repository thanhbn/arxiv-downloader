# MEGA BLOCKS: HUẤN LUYỆN THƯA HIỆU QUẢ VỚI HỖN HỢP CHUYÊN GIA

Trevor Gale1 Deepak Narayanan2 Cliff Young3 Matei Zaharia1

TÓM TẮT
Chúng tôi trình bày MegaBlocks, một hệ thống để huấn luyện Mixture-of-Experts (MoE) hiệu quả trên GPU. Hệ thống của chúng tôi được thúc đẩy bởi các hạn chế của các framework hiện tại, vốn hạn chế việc định tuyến động trong các lớp MoE để thỏa mãn các ràng buộc của phần mềm và phần cứng hiện có. Các công thức này buộc phải đánh đổi giữa chất lượng mô hình và hiệu suất phần cứng, khi người dùng phải chọn giữa việc loại bỏ các token khỏi tính toán hoặc lãng phí tính toán và bộ nhớ cho padding. Để giải quyết những hạn chế này, chúng tôi tái công thức hóa tính toán MoE theo các phép toán thưa khối và phát triển các kernel GPU thưa khối mới xử lý hiệu quả tính động có trong MoE. Phương pháp của chúng tôi không bao giờ loại bỏ token và ánh xạ hiệu quả lên phần cứng hiện đại, cho phép tăng tốc huấn luyện từ đầu đến cuối lên đến 40% so với MoE được huấn luyện với thư viện Tutel tiên tiến và 2.4× so với DNN được huấn luyện với framework Megatron-LM được tối ưu hóa cao.

1 GIỚI THIỆU
Khai thác tính thưa trong trọng số, activation và dữ liệu đầu vào của mạng nơ-ron sâu (DNN) là một kỹ thuật hiệu quả để giảm lượng tính toán cần thiết để đạt được chất lượng mô hình nhất định (Han et al., 2015; Gale et al., 2019). Thập kỷ qua đã chứng kiến tiến bộ đáng kể trong các thuật toán và phần mềm hiệu suất cao để làm cho tính thưa trở nên hữu ích trong thực tế (Gray et al., 2017; Narang et al., 2017; Kalchbrenner et al., 2018; Elsen et al., 2020; Gale et al., 2020). Một lĩnh vực vẫn là thách thức đối với tính thưa là huấn luyện mô hình trên các bộ gia tốc. DNN thường được huấn luyện trên các bộ gia tốc phần cứng như GPU (NVIDIA, 2020) và TPU (Jouppi et al., 2017), khai thác tính quy tắc của tính toán dày đặc để mang lại hiệu suất cao. Do đó, tính toán thưa tinh vi kém hiệu quả hơn trên các bộ xử lý này. Để cho phép tính toán hiệu quả trên các bộ gia tốc, có thể áp đặt cấu trúc lên các ma trận thưa (Narang et al., 2017; Gray et al., 2017; Yao et al., 2019).

Một lớp mô hình mới nổi với tính thưa có cấu trúc cơ bản là Mixture-of-Experts (MoE) (Shazeer et al., 2017). Mỗi lớp trong MoE là một tập hợp các chuyên gia, bản thân chúng là các DNN nhỏ. Khi dữ liệu được truyền qua các lớp MoE, mỗi token được định tuyến động đến một tập con các chuyên gia để tính toán. Bằng cách khai thác tính toán thưa này, MoE đã giảm thời gian huấn luyện lên đến 4× cho các ứng dụng trong xử lý ngôn ngữ tự nhiên và thị giác máy tính (Artetxe et al., 2021; Riquelme et al., 2021). Những lợi ích này đã chuyển thành các mức độ quy mô mới cho việc huấn luyện mô hình, đẩy kích thước mô hình vượt qua 1 nghìn tỷ tham số (Artetxe et al., 2021; Du et al., 2021; Fedus et al., 2022).

Thách thức trong việc tính toán MoE hiệu quả là xử lý việc định tuyến động và tính toán mất cân bằng tải, những yếu tố cơ bản của các kiến trúc này. Tuy nhiên, phần cứng và phần mềm hiện có cho deep learning khiến việc đáp ứng thách thức này trở nên khó khăn. Ví dụ, TPU và trình biên dịch XLA của chúng yêu cầu tất cả các hình dạng tensor phải được biết tĩnh và thường gặp khó khăn với các phép toán tinh vi như scatter và gather (Fedus et al., 2022). Các ràng buộc này khiến việc triển khai MoE trực tiếp trên TPU trở nên khó khăn. Mặc dù GPU linh hoạt hơn, tính toán thưa trong MoE không ánh xạ sạch sẽ đến các primitives phần mềm được hỗ trợ trong các framework và thư viện chính.

Các framework tiên tiến cho huấn luyện MoE tránh những thách thức này bằng cách đặt các ràng buộc cứng nhắc lên việc định tuyến MoE. Để loại bỏ tính động khỏi tính toán, tập hợp các token được ánh xạ đến mỗi chuyên gia được cắt bớt hoặc đệm đến một kích thước do người dùng chỉ định (Lepikhin et al., 2020; Fedus et al., 2022; Hwang et al., 2022). Công thức procrustean này tạo ra sự đánh đổi giữa chất lượng mô hình và hiệu suất phần cứng, khi người dùng phải quyết định có loại bỏ token hay lãng phí tính toán và bộ nhớ cho padding. Quyết định này thường được thực hiện thông qua điều chỉnh hyperparameter, làm tăng độ phức tạp của việc sử dụng MoE.

Để giải quyết những thách thức này, chúng tôi phát triển một phương pháp định tuyến và tính toán MoE dựa trên các primitives thưa. Phương pháp của chúng tôi không bao giờ loại bỏ token và ánh xạ hiệu quả lên GPU hiện đại, cho phép tăng tốc huấn luyện từ đầu đến cuối lên đến 40% và 2.4× so với các framework tiên tiến cho huấn luyện MoE và DNN, tương ứng. Chúng tôi đóng góp các điểm cụ thể sau:

• Chúng tôi chỉ ra cách tính toán trong lớp MoE có thể được biểu thị dưới dạng các phép toán thưa khối để phù hợp với việc gán không cân bằng các token cho chuyên gia. Chúng tôi sử dụng công thức này để huấn luyện dropless-MoE (dMoE).

• Chúng tôi phát triển các kernel GPU hiệu suất cao cho tích ma trận thưa khối để xử lý hiệu quả tính toán MoE động. Kernel của chúng tôi sử dụng hai kỹ thuật, mã hóa blocked-CSR-COO và transpose indices, để cho phép tích ma trận hiệu quả với đầu vào và đầu ra thưa theo thứ tự chuyển vị hoặc không chuyển vị.

Chúng tôi đã triển khai những kỹ thuật này trong một hệ thống gọi là MegaBlocks, được xây dựng trên thư viện Megatron-LM tiên tiến để huấn luyện các mô hình Transformer (Shoeybi et al., 2019). Chúng tôi đánh giá hệ thống thông qua cả microbenchmark và huấn luyện từ đầu đến cuối của các mô hình ngôn ngữ Transformer.

2 NỀN TẢNG: CÁC LỚP MOE

Các lớp MoE được tạo thành từ nhiều chuyên gia, bản thân chúng là các mạng nơ-ron nhỏ. Mỗi token được định tuyến động đến một tập con các chuyên gia để tính toán dựa trên điểm số được tính bởi một router. Các chuyên gia thường được định nghĩa là các perceptron đa lớp nhỏ (MLP). Thông thường token được gửi đến một số lượng nhỏ chuyên gia, thường từ 1 đến 4 (Fedus et al., 2022).

Các lớp MoE thường được xen kẽ với các lớp DNN khác và thường được sử dụng để thay thế các lớp mạng feed-forward (FFN) trong Transformer (Shazeer et al., 2017; Fedus et al., 2022). Kiến trúc lai này đã thể hiện kết quả mạnh mẽ trên cả các tác vụ ngôn ngữ tự nhiên và thị giác (Du et al., 2021; Riquelme et al., 2021). Người ta đoán rằng những cải thiện này là kết quả của việc các chuyên gia chuyên môn hóa cho các phần khác nhau của phân phối dữ liệu (Shazeer et al., 2017).

Chúng tôi minh họa một lớp MoE trong Hình 1 và mô tả chi tiết trong phần còn lại của mục này.

2.1 Định tuyến

Giai đoạn đầu tiên của lớp MoE là router, có trách nhiệm xác định việc gán token cho chuyên gia. Ngoài việc gán chuyên gia, router MoE cũng tạo ra xác suất cho mỗi phép gán phản ánh độ tin cậy của ánh xạ. Những trọng số này được mã hóa dưới dạng ma trận điểm số cho mỗi cặp token-chuyên gia, được sử dụng để kết hợp tuyến tính các đầu ra chuyên gia top-k cho mỗi token (xem §2.4).

Kiểu định tuyến MoE phổ biến nhất là learned router được đề xuất bởi Shazeer et al. (2017). Trong router này, các token được chiếu từ hidden size elements thành num experts scores bằng cách nhân với ma trận trọng số được học cùng với các tham số mô hình khác. Các điểm số được chuẩn hóa bằng softmax và các quyết định định tuyến được thực hiện bằng cách chọn tham lam các chuyên gia có điểm số top-k cho mỗi token.

2.2 Hoán vị

Các triển khai MoE tiên tiến nhằm tính toán tất cả các lớp chuyên gia song song để sử dụng hiệu quả tính song song có sẵn trên GPU và TPU (Lepikhin et al., 2020; Fedus et al., 2022; Hwang et al., 2022). Primitive tiêu chuẩn được sử dụng bởi các triển khai là batched matrix multiplication, tính toán một tập hợp các tích ma trận có cùng hình dạng (xem Hình 3A). Tuy nhiên, việc ánh xạ tính toán MoE vào primitive này không phải là tầm thường. Để tuân thủ các ràng buộc hình dạng của batched matrix multiplication, các chuyên gia phải được ràng buộc có ma trận trọng số cùng hình dạng và số lượng token được gán cho mỗi chuyên gia phải bằng nhau. Ràng buộc sau đặc biệt có vấn đề vì thuật toán định tuyến học được mô tả ở trên không đảm bảo việc gán cân bằng tải các token cho chuyên gia.

Để thỏa mãn ràng buộc này, các nghiên cứu trước đây đã định nghĩa một expert capacity cố định, là số lượng token mà mỗi chuyên gia có thể được gán (Lepikhin et al. (2020); Fedus et al. (2022)). Nếu số lượng token được gán cho một chuyên gia vượt quá dung lượng của nó, các token thừa sẽ bị loại bỏ. Tức là, chúng không được chuyển đến bất kỳ chuyên gia nào để tính toán và mô hình dựa vào kết nối residual để tái giới thiệu biểu diễn của các token bị loại bỏ sau lớp MoE. Nếu một lớp chuyên gia không được gán đủ token để lấp đầy dung lượng của nó, tập hợp token của nó sẽ được đệm để lấp đầy không gian còn lại. Expert capacity thường được chỉ định theo hyperparameter capacity factor, là một multiplier trên số lượng token dự kiến sẽ được gán cho mỗi chuyên gia dưới phân phối đồng đều hoàn hảo:

expert capacity = num tokens / num experts × capacity factor

Capacity factor có thể được coi là một tham số giảm khả năng loại bỏ token. Hyperparameter này đại diện cho sự đánh đổi giữa tính toán bổ sung và chất lượng mô hình. Do đó, mong muốn giảm thiểu lượng mất cân bằng tải trong việc gán token cho chuyên gia. Cơ chế điển hình để làm như vậy là auxiliary load balancing losses, khuyến khích router tạo ra phép gán cân bằng (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022). Những loss này cũng giúp đảm bảo rằng tất cả các chuyên gia thấy số lượng token tương tự trong quá trình huấn luyện. Điều này được cho là quan trọng để tránh các trạng thái thoái hóa khi một số chuyên gia được gán zero token và ngừng nhận gradient updates (Zhou et al., 2022).

Ngoài việc cho phép tính toán batched các lớp chuyên gia, những ràng buộc này cho phép tất cả các hình dạng tensor được biết tĩnh, điều này được yêu cầu bởi TPU và XLA.

2.3 Tính toán

Khi dữ liệu đã được hoán vị, các chuyên gia có thể được tính toán song song. Đối với các mô hình mà chuyên gia là MLP, điều này đòi hỏi tính toán từng lớp cho tất cả chuyên gia sử dụng batched matrix multiplication. Đối với chuyên gia convolutional, các lớp có thể được tính toán bằng grouped convolutions.

2.4 Hoán vị ngược

Sau khi các chuyên gia được tính toán, các vector đặc trưng kết quả được hoán vị ngược sao cho thứ tự của chúng khớp với đầu vào của lớp. Bước cuối cùng trong tính toán MoE là scale các token đầu ra bằng điểm số mà chúng được gán cho chuyên gia tương ứng. Khi token được định tuyến đến nhiều hơn một chuyên gia, những kết quả có trọng số này được cộng lại để tạo ra đầu ra lớp cuối cùng cho mỗi token.

3 ĐỘNG LỰC: LOẠI BỎ TOKEN TRONG MOE

Mặc dù sử dụng load balancing losses, các nghiên cứu trước đây đã chỉ ra rằng định tuyến token vẫn rất mất cân bằng (Hwang et al., 2022). Để định lượng tác động của việc loại bỏ token lên chất lượng mô hình, chúng tôi đã huấn luyện các mô hình ngôn ngữ MoE trên The Pile (Gao et al., 2020) với một loạt capacity factors. Chúng tôi huấn luyện Transformer MoE tương tự như những mô hình được sử dụng bởi Fedus et al. (2022), trong đó mỗi mô hình là một Transformer với các lớp FFN được thay thế bằng các lớp MoE 64-expert trong đó mỗi chuyên gia là MLP 2-lớp khớp với kích thước FFN gốc. Chúng tôi sử dụng định tuyến top-1 và dựa kích thước mô hình MoE của chúng tôi trên mô hình Transformer-Small được mô tả trong Bảng 1. Tất cả các mô hình được huấn luyện sử dụng tokenization từ GPT2 (Radford et al., 2019) cho 10B token với sequence length 1024, optimizer Adam, và các cài đặt learning rate và gradient clipping từ Shoeybi et al. (2019). Chúng tôi huấn luyện tất cả các mô hình trên một GPU A100 duy nhất với batch size 512 sequences. Chúng tôi huấn luyện MoE với capacity factor 1, 1.5, và 2 cũng như kỹ thuật dynamic capacity factor được đề xuất bởi Tutel (Hwang et al., 2022), trong đó capacity factor được đặt động đến giá trị tối thiểu để tránh loại bỏ token. Làm baseline, chúng tôi huấn luyện các mô hình Transformer tiêu chuẩn trên một loạt kích thước. Tất cả các mô hình Transformer và MoE có vocabulary size 51200, sequence length 1024 và attention head size 64. Các cấu hình mô hình của chúng tôi được tóm tắt trong Bảng 1 và kết quả của các thí nghiệm được hiển thị trong Hình 2.

Đối với những mô hình này, chúng tôi quan sát thấy rằng tác động của việc loại bỏ token là đáng kể. Trong khi MoE với capacity factor 1 đạt được giảm 0.15 validation loss, MoE tránh loại bỏ token cung cấp giảm 0.26, lớn hơn 1.73× so với lợi ích của mô hình trước và đủ để vượt quá chất lượng của Transformer-Medium.

Trong khi loại bỏ token giảm chất lượng mô hình, việc tăng capacity factor đi kèm với chi phí tính toán và bộ nhớ bổ sung. Trong ví dụ này, các phép toán MoE-layer tăng hơn 2× để tránh loại bỏ token. Hwang et al. (2022) chỉ ra rằng một số MoE yêu cầu capacity factors cao đến 11× để tránh loại bỏ token, và các mô hình khác mà capacity factor cần thiết để tránh loại bỏ token tăng vọt không thể dự đoán trong quá trình huấn luyện. Ngoài overhead tính toán của việc tăng capacity factor, phải điều chỉnh một hyperparameter bổ sung có thể tăng đáng kể số lượng mô hình cần được huấn luyện cho một tác vụ mục tiêu. Điều này đặc biệt cồng kềnh đối với mạng nơ-ron lớn, nơi chi phí huấn luyện một mô hình duy nhất có thể lên đến hàng trăm nghìn đô la (MosaicML, 2022). Có thể là kết quả của điều này, một số nghiên cứu lớn về MoE đã từ chối khám phá các capacity factors khác nhau (Artetxe et al., 2021; Clark et al., 2022).

4 KHÔNG BỎ LẠI TOKEN-NÀO VỚI BLOCK SPARSITY

Mục này mô tả cách chúng tôi công thức hóa tính toán lớp MoE theo tính toán thưa khối để tránh loại bỏ token. Động lực sử dụng primitives thưa khối để biểu thị tính toán MoE là đa dạng. Trước tiên, như chúng tôi chỉ ra dưới đây, ma trận thưa khối là cách tự nhiên và linh hoạt để mô tả tính toán động và mất cân bằng tải trong MoE. Thứ hai, block sparsity ánh xạ hiệu quả lên các bộ gia tốc phần cứng được xây dựng xung quanh systolic array matrix multipliers như GPU và TPU. Do độ chi tiết thô của các chuyên gia MoE, chúng ta có thể chọn kích thước khối cho triển khai của mình đủ lớn để cho phép tính toán đạt được phân số cao của throughput thiết bị đỉnh. Cuối cùng, các kernel thưa khối như matrix multiplication và convolution là primitives tổng quát hữu ích trên nhiều ứng dụng (Narang et al., 2017; Gray et al., 2017; Child et al., 2019; Elsen et al., 2020). Điều này làm cho đầu tư vào kernel hiệu suất cao trở nên thực tế hơn, vì công việc có thể được phân bổ qua các tác vụ mục tiêu. Chúng ta có thể tương tự đầu tư vào kernel batched matrix multiplication kích thước biến đổi, nhưng tiện ích của điều này sẽ được giới hạn trong các kiến trúc MoE như chúng được thiết kế ngày nay.

Ngoài những cân nhắc này, công thức thưa khối của MoE phơi bày một quan điểm mới về những thuật toán này như một dạng activation sparsity động, có cấu trúc. Quan điểm này rút ra những điểm tương đồng với nhiều tài liệu về thuật toán huấn luyện thưa và mở ra cơ hội cải thiện thêm MoE với những hiểu biết từ lĩnh vực liền kề này.

Sơ bộ: Ký hiệu Tích Ma trận Thưa. Trong phần còn lại của bài báo này, chúng tôi thường đề cập đến phép nhân ma trận trong đó một trong ba ma trận (hai đầu vào và một đầu ra) là thưa và những cái khác là dày đặc. Chúng tôi mượn ký hiệu từ Triton (Tillet et al., 2019) để mô tả những phép toán khác nhau này. Mỗi phép toán được mô tả bằng chuỗi ba ký tự trong đó mỗi ký tự là "S" cho thưa hoặc "D" cho dày đặc. Thứ tự ký tự là đầu ra, theo sau là đầu vào trái theo sau là đầu vào phải. Ví dụ, tích của hai ma trận dày đặc với đầu ra thưa là "SDD", cũng được gọi là sampled dense-dense matrix multiplication (SDDMM). Ký hiệu này hữu ích để phân biệt các phép toán như DSD và DDS, là các dạng khác nhau của sparse matrix-dense matrix multiplication (SpMM). Superscript "T" chỉ ra chuyển vị của các đối số đầu vào. Ví dụ, SDDT chỉ ra SDD trong đó ma trận đầu vào phải được chuyển vị.

4.1 Tính toán Chuyên gia với Block Sparsity

Hiểu biết chính đằng sau phương pháp của chúng tôi được hiển thị trong Hình 3. Thay vì phương pháp hiện hành tính toán các chuyên gia trong lớp MoE sử dụng batched matrix multiplication, chúng ta có thể tương đương tính toán các chuyên gia như một SDD trong đó ma trận thưa đầu ra có cấu trúc block diagonal, như hiển thị trong Hình 3B. Trong công thức này, cho phép việc gán không cân bằng tải các token cho chuyên gia tương tự như cho phép các khối trong ma trận block diagonal có số hàng biến đổi. Để đạt được điều này, chúng tôi đề xuất tính toán mỗi khối như nhiều khối kích thước cố định nhỏ hơn sử dụng block-sparse matrix multiplication, như hiển thị trong Hình 3C. Để xây dựng chuyên gia đa lớp, chúng ta có thể lặp lại giữa các phép toán SDD và DSD (xem Hình 6).

Trong công thức này, chúng ta cũng có thể nới lỏng ràng buộc về số cột trong mỗi khối để xây dựng các lớp MoE với các chuyên gia kích thước biến đổi, như hiển thị trong Hình 3C. Mặc dù đây là hướng thú vị cho nghiên cứu tương lai, chúng tôi không khám phá những cấu hình này vì cần thêm nghiên cứu để xác định cách khả năng này có thể được sử dụng để tăng hiệu quả.

Với các khối đủ lớn, block-sparse matrix multiplication có khả năng đạt được phân số cao của throughput đỉnh trên GPU hiện đại (Gray et al., 2017; NVIDIA, 2021). Tính thưa hạt thô trong MoE phù hợp với yêu cầu này - trong các mô hình Transformer sử dụng các lớp MoE FFN, số cột trong các khối hiển thị trong Hình 3B tương ứng với ffn_hidden_size, thường từ 1024 đến 8192 (Vaswani et al., 2017; Radford et al., 2019; Brown et al., 2020). Số hàng trong những khối này tương ứng với số lượng token được gán cho mỗi chuyên gia, dự kiến bằng số token chia cho số chuyên gia dưới phân phối đồng đều. Điều này có thể dao động từ vài nghìn đến hàng chục nghìn token mỗi chuyên gia (Lepikhin et al., 2020; Artetxe et al., 2021; Fedus et al., 2022). Những khối hạt thô này lớn hơn nhiều lần so với kích thước tile lớn nhất được sử dụng cho kernel matrix multiplication dày đặc, mang lại cho chúng ta sự linh hoạt để chọn kích thước khối có thể khớp với throughput của chúng.

5 MEGA BLOCKS: MỘT FRAMEWORK CHO HUẤN LUYỆN MOE HIỆU QUẢ

Chúng tôi triển khai các kỹ thuật của mình trong một hệ thống gọi là MegaBlocks, xây dựng trên Megatron-LM (Shoeybi et al., 2019) và PyTorch (Paszke et al., 2019). Ngoài các lớp dropless-MoE (dMoE) hiệu suất cao, hệ thống của chúng tôi hỗ trợ huấn luyện phân tán MoE với cả data và expert model parallelism (Fedus et al., 2022).

Mục này thảo luận về thiết kế triển khai dMoE của chúng tôi, bao gồm các kernel thưa khối và các cân nhắc khác để xây dựng hệ thống hiệu quả. §5.1.1 thảo luận về các hạn chế của kernel thưa khối hiện có. §5.1.2 phân tích các hiệu ứng của kích thước khối lên hiệu suất tích thưa khối. §5.1.3 mô tả định dạng ma trận thưa blocked-CSR-COO lai của chúng tôi, cho phép tích ma trận hiệu quả với các toán hạng đầu vào và đầu ra thưa. §5.1.4 giới thiệu transpose indices như một cơ chế để lặp hiệu quả qua các ma trận thưa khối theo thứ tự chuyển vị. Cuối cùng, §5.2 thảo luận về định tuyến và hoán vị hiệu quả cho dMoE.

Sơ bộ: Nhân Ma trận trên GPU. Các kernel nhân ma trận trên GPU khai thác tiling, trong đó ma trận đầu ra được chia thành các khối hai chiều có kích thước tĩnh (NVIDIA, 2022c). Việc tính toán những tile này có thể được song song hóa, và các tile riêng lẻ có thể được định kích thước để đánh đổi cường độ số học và tính song song. Nhóm thread được gán cho một tile được gọi là threadblock.

5.1 Kernel Thưa Khối Hiệu quả cho MoE

Để huấn luyện MoE với kernel thưa khối, chúng ta cần primitives cho forward và backward pass. Xem xét một lớp MoE FFN trong đó mỗi chuyên gia là MLP 2-lớp. Đối với cấu hình này, forward pass yêu cầu phép toán SDD theo sau là DSD (Hình 6). Đối với backward pass, chúng ta tính toán SDDT và DSTD cho gradient dữ liệu và gradient trọng số lớp thứ hai, tương ứng, theo sau là DSDT và DDTS cho gradient dữ liệu và gradient trọng số lớp đầu tiên, tương ứng.

5.1.1 Primitives Thưa Khối Hiện có

Chúng tôi xem xét hai thư viện hiện có cho block-sparse matrix multiplication trên GPU: NVIDIA cuSPARSE (NVIDIA, 2022b) và Triton Blocksparse (Tillet et al., 2019). cuSPARSE hỗ trợ định dạng ma trận thưa blocked-ELL cho DSD. Tuy nhiên, tính đến CUDA 11.8, phép toán này không hỗ trợ chuyển vị của đầu vào ma trận thưa. cuSPARSE cũng không cung cấp primitive SDD với ma trận blocked-ELL. Ngoài những hạn chế này, định dạng blocked-ELL yêu cầu tất cả các hàng trong ma trận thưa có cùng số lượng nonzero, điều này sẽ đánh bại mục tiêu hỗ trợ ma trận mất cân bằng tải của chúng tôi. Blocksparse hỗ trợ SDD, DSD, và DDS cũng như tất cả các kết hợp của đầu vào chuyển vị và không chuyển vị. Tuy nhiên, những primitives này giả định rằng topology của ma trận thưa không thay đổi giữa các lần gọi. API thư viện nhận bitmask mô tả toán hạng thưa và sau đó tiền tính toán look-up tables và nhóm khối để tăng tốc tính toán. Đối với use case của chúng tôi, topology ma trận thưa thay đổi qua mỗi lần lặp huấn luyện và mỗi lớp MoE trong mô hình. Để sử dụng Blocksparse, chúng tôi phải trả chi phí của những bước tiền xử lý này lặp đi lặp lại.

Dựa trên phân tích này, chúng tôi chọn viết primitives thưa khối của riêng mình để điều chỉnh chúng cho tính động của tính toán chuyên gia MoE. Chúng tôi triển khai các phép toán SDD, DSD, và DDS nhắm mục tiêu GPU NVIDIA. Kernel của chúng tôi hỗ trợ tất cả các kết hợp của đầu vào chuyển vị và không chuyển vị. Phần còn lại của mục này chi tiết thiết kế và triển khai kernel của chúng tôi.

5.1.2 Chọn Kích thước Khối cho MoE

Để sử dụng GPU hiện đại hiệu quả, chúng ta muốn sử dụng các khối thưa có đủ cường độ số học để giữ các đơn vị nhân ma trận bận rộn. Các khối lớn cũng mong muốn để phân bổ chi phí lưu trữ và vận hành trên metadata ma trận thưa, vì metadata như column indices chỉ cần được giữ cho mỗi khối nonzero.

Để chọn kích thước khối mục tiêu, chúng tôi nghiên cứu hiệu suất của kernel nhân ma trận dày đặc từ NVIDIA CUTLASS (NVIDIA, 2022c) với các kích thước tile khác nhau. Chúng tôi benchmark mixed-precision (FP16 + FP32 accumulation) matrix multiplication trên ma trận vuông với độ dài cạnh lũy thừa của 2 từ 512 đến 16384 và mọi tập hợp kích thước tile được hỗ trợ trong CUTLASS. Đối với tile hình chữ nhật, chúng tôi chỉ hiển thị những cấu hình mà kích thước tile đầu tiên lớn hơn vì chúng tôi thấy những cấu hình này vượt trội nhẹ so với thứ tự thay thế cho những vấn đề này. Chúng tôi chạy tất cả benchmark trên GPU A100 SXM4 80GB với CUDA 11.5 và CUTLASS 2.5. Những benchmark này được hiển thị trong Hình 4.

Qua những benchmark này, chúng tôi quan sát thấy rằng tile 128x128 liên tục hoạt động ngang bằng hoặc tốt hơn so với các cấu hình khác. Theo kinh nghiệm, chúng tôi quan sát thấy rằng cấu hình tương tự này thường được chọn bởi NVIDIA cuBLAS (NVIDIA, 2022a) cho các mô hình Transformer dày đặc mà chúng tôi nghiên cứu. Dựa trên phân tích này, chúng tôi chọn sử dụng block sparsity 128x128. Mặc dù kích thước tile của block-sparse matrix multiplication và kích thước khối trong ma trận thưa không cần bằng nhau, chúng tôi thấy rằng đối với các khối 128x128, kích thước tile hoạt động cao nhất trong workload của chúng tôi cũng là 128x128.

Để triển khai kernel của chúng tôi, chúng tôi mở rộng CUTLASS (NVIDIA, 2022c) để hỗ trợ ma trận thưa khối và tái sử dụng máy móc của chúng cho matrix multiplication hiệu suất cao với các kiểu dữ liệu và kiến trúc GPU khác nhau.

5.1.3 Tính toán Đầu ra Thưa với Blocked-CSR-COO Lai

Chúng tôi sử dụng blocked compressed sparse row (BCSR) làm định dạng ma trận thưa chính. BCSR làm cho việc lặp qua nonzero trong một hàng trở nên đơn giản, điều này cần thiết cho các phép toán như DSD và DDST. Việc lặp qua các khối cũng có overhead tối thiểu với BCSR, vì việc xác định vị trí của khối trong ma trận chỉ yêu cầu một lần load column index của nó. Chúng tôi thảo luận về phương pháp lặp hiệu quả qua nonzero trong một cột với định dạng này trong §5.1.4.

Một thách thức với ma trận thưa BCSR là tính toán hiệu quả các phép toán SDD song song. Khi kernel launch, mỗi threadblock cần xác định hàng và cột của khối đầu ra của nó để biết cần hàng và cột nào của ma trận đầu vào để tính toán nó. Vì BCSR chỉ mã hóa column indices cho mỗi khối, việc xác định row index của khối nonzero yêu cầu tìm kiếm qua row offsets. Một giải pháp cho vấn đề này là launch số lượng threadblock tối đa có thể cần để tính toán mỗi hàng của đầu ra nếu nó hoàn toàn dày đặc. Khi khởi động, mỗi threadblock có thể kiểm tra xem column offset của nó có nằm ngoài phạm vi số lượng nonzero trong hàng của nó không và return nếu không có công việc để làm. Gale et al. (2020) chỉ ra rằng overhead được tạo ra bởi việc launch threadblock thừa là không đáng kể đối với ma trận thưa vừa phải (50 - 90% zero). Chúng tôi thử nghiệm với phương pháp này nhưng quan sát thấy rằng đối với MoE, chi phí launch những threadblock không sử dụng này là đáng kể, đặc biệt là đối với các mô hình có số lượng chuyên gia cao nơi mức độ thưa trong ma trận thưa khối rất cao.

Để song song hóa SDD hiệu quả, chúng tôi bổ sung materialize row indices cho mỗi khối nonzero để threadblock có thể tra cứu tầm thường tọa độ của các khối thưa trong ma trận đầu ra. Lưu trữ được yêu cầu cho metadata bổ sung này không đáng kể vì chúng tôi chỉ cần lưu trữ một index cho mỗi 16384 giá trị nonzero trong khối 128x128. Ngay cả với metadata bổ sung này, chúng tôi duy trì thứ tự theo hàng của các khối nonzero để ma trận có thể được vận hành như BCSR hoặc blocked coordinate format (BCOO). Chúng tôi minh họa mã hóa blocked-CSR-COO lai này trong Hình 5.

5.1.4 Chuyển vị Ma trận Thưa Khối với Transpose Indices

Tính toán forward và backward pass để huấn luyện mô hình yêu cầu chuyển vị ma trận thưa. Tuy nhiên, việc lặp qua ma trận BCSR theo thứ tự chuyển vị yêu cầu tìm kiếm qua mỗi hàng để xác định xem khối trong cột mục tiêu có nonzero không (Bulu c ¸et al., 2009). Chúng ta có thể materialize một phiên bản chuyển vị của ma trận thưa một cách rõ ràng, nhưng điều này sẽ phát sinh chi phí runtime và lưu trữ vì tất cả các giá trị nonzero trong ma trận cần được sao chép. Để cho phép lặp hiệu quả qua ma trận BCSR theo thứ tự chuyển vị, chúng tôi xây dựng metadata cho ma trận chuyển vị nhưng không chuyển vị rõ ràng các giá trị nonzero. Thay vào đó, chúng tôi xây dựng một mảng indices, một cho mỗi khối nonzero, được lưu trữ theo thứ tự chuyển vị và chứa offset của mỗi khối nonzero trong bộ nhớ. Metadata bổ sung này cho phép lặp hiệu quả qua ma trận theo thứ tự chuyển vị với một lớp indirection, như hiển thị trong Hình 5.

Ý tưởng này tương tự như secondary index trong cơ sở dữ liệu, cho phép truy cập hiệu quả đến các mục theo thứ tự khác với primary index. Tương tự như mã hóa Blocked-CSR-COO lai của chúng tôi, kỹ thuật này dựa trên thực tế rằng lưu trữ và tính toán rẻ hơn nhiều lần đối với metadata so với giá trị nonzero nhờ kích thước khối lớn của chúng tôi.

5.2 Định tuyến và Hoán vị Hiệu quả

Như hiện tại được triển khai, các kernel block-sparse matrix multiplication của chúng tôi yêu cầu số lượng token được gán cho mỗi chuyên gia phải là bội số của kích thước khối. Để tuân thủ ràng buộc này, chúng tôi pad mỗi nhóm token bằng zero đến bội số gần nhất của 128 và fuse phép toán này vào các kernel hoán vị tùy chỉnh. Chúng tôi có thể loại bỏ ràng buộc này bằng cách hỗ trợ các khối một phần ở rìa của bài toán tương tự như cách matrix multiplication xử lý ma trận không chia hết cho kích thước tile. Tuy nhiên, tác động hiệu suất của tính năng này sẽ tối thiểu vì chúng tôi mong đợi số lượng token được gán cho mỗi chuyên gia sẽ là hàng nghìn hoặc hàng chục nghìn.

Khi các phép gán chuyên gia đã được tính toán bởi router, chúng tôi tạo metadata cho ma trận thưa khối sử dụng kernel CUDA tùy chỉnh. Chúng tôi cũng xây dựng metadata chuyển vị tại thời điểm này để phân bổ chi phí qua nhiều block-sparse matrix multiplication sử dụng nó qua tính toán forward và backward.

6 THÍ NGHIỆM

Mục này phân tích hiệu suất của hệ thống chúng tôi so với các thư viện tiên tiến, Microsoft Tutel (Hwang et al., 2022) và NVIDIA Megatron-LM (Shoeybi et al., 2019), để huấn luyện Transformer MoE và Transformer tiêu chuẩn tương ứng. Để đảm bảo so sánh công bằng, chúng tôi mở rộng Megatron-LM để hỗ trợ bổ sung huấn luyện MoE sử dụng lớp MoE của Tutel. Tất cả các thí nghiệm được thực hiện trên GPU NVIDIA A100 SXM4 80GB với CUDA 11.5, CUTLASS 2.5 và sử dụng mixed-precision training (Micikevicius et al., 2018) như được triển khai trong Megatron-LM.

6.1 Huấn luyện MoE Không Loại bỏ Token

Để đánh giá hiệu quả của kỹ thuật tránh loại bỏ token, chúng tôi so sánh với phương pháp dMoE được đề xuất bởi Hwang et al. (2022) trong đó capacity factor được đặt động đến giá trị tối thiểu tránh loại bỏ token.

Chúng tôi huấn luyện các mô hình ngôn ngữ Transformer decoder-only trên The Pile (Gao et al., 2020) với cùng hyperparameters được mô tả trong §3. Đối với Transformer MoE, chúng tôi huấn luyện các mô hình được mở rộng từ mô hình XS, Small, và Medium của chúng tôi với mỗi lớp FFN được thay thế bằng các lớp MoE 64-expert sử dụng định tuyến top-1. Chúng tôi cũng huấn luyện các mô hình Transformer tiêu chuẩn từ 46M đến 1.3B tham số, tương đương với Transformer-Base (Vaswani et al., 2017) đến GPT3-XL (Brown et al., 2020), làm baseline dày đặc. Chúng tôi huấn luyện tất cả các mô hình trên 8 GPU A100 SXM4 80GB sử dụng 8-way expert model parallelism cho các lớp MoE và data parallelism cho tất cả các lớp khác. Chúng tôi sử dụng gradient accumulation cho tất cả các mô hình và huấn luyện với batch size 512 sequences và micro batch size lớn nhất không hết bộ nhớ (Narayanan et al., 2021a). Các cấu hình mô hình của chúng tôi được tóm tắt trong Bảng 1 và 2. Đối với mỗi mô hình, chúng tôi báo cáo thời gian huấn luyện từ đầu đến cuối và loss cuối cùng đạt được trên validation set trong Hình 7.

So với phương pháp dựa trên padding phổ biến để tránh loại bỏ token, kỹ thuật tính toán MoE thích nghi với block sparsity của chúng tôi cho phép tăng tốc huấn luyện từ đầu đến cuối 1.38×, 2.0× và 4.35× cho MoE-XS, MoE-Small, và MoE-Medium, tương ứng. Ngoài overhead tính toán, phương pháp dựa trên padding được triển khai trong Tutel tăng đáng kể lượng bộ nhớ cần thiết để lưu trữ activation trong các lớp MoE. Điều này đặc biệt có vấn đề vì MoE đã yêu cầu nhiều lần lưu trữ hơn cho ma trận trọng số lớn của chúng so với Transformer tiêu chuẩn. Đối với những mô hình này, chúng tôi quan sát thấy sự gia tăng sử dụng bộ nhớ này giảm micro batch size tối đa mà Tutel có thể sử dụng 2×, 4×, và 8× so với MegaBlocks cho MoE-XS, MoE-Small, và MoE-Medium, tương ứng. Điều này lại tăng thời gian huấn luyện do giảm hiệu suất phần cứng. Kết quả là, chúng tôi quan sát thấy lợi thế của MegaBlocks so với Tutel tăng theo kích thước mô hình. Micro batch size được sử dụng cho mỗi cấu hình mô hình được hiển thị trong Bảng 3.

So với các mô hình Transformer được huấn luyện với Megatron-LM, dMoE được huấn luyện với MegaBlocks giảm thời gian huấn luyện cần thiết để đạt validation loss nhất định 1.8×-2.4×. Sự biến thiên trong so sánh này chủ yếu là kết quả của việc sử dụng bộ nhớ trọng số tăng của các mô hình MoE, buộc MegaBlocks phải sử dụng micro batch size nhỏ hơn 2× cho MoE-Medium so với mô hình Transformer tương tự. Những kết quả này nêu bật tầm quan trọng của việc giảm sử dụng bộ nhớ trong MoE như một hướng cho nghiên cứu tương lai.

Đối với những mô hình Transformer này, chúng tôi quan sát thấy rằng Megatron-LM duy trì từ 21% đến 48% throughput đỉnh 2.5 petaFLOP của hệ thống 8-GPU này với hiệu quả tăng theo kích thước mô hình. Tăng tốc đạt được bởi MegaBlocks so với framework tiên tiến này chứng minh hiệu quả của hệ thống chúng tôi và hiệu quả của MoE.

6.2 Huấn luyện MoE với Loại bỏ Token

Chúng tôi cũng so sánh các mô hình dMoE của chúng tôi với MoE loại bỏ token được huấn luyện với Tutel. Để tìm các cấu hình hiệu quả nhất, chúng tôi huấn luyện các mô hình MoE-XS, MoE-Small và MoE-Medium với capacity factors 1, 1.5, và 2 với tổng cộng 9 mô hình bổ sung. Đối với những cấu hình này, tất cả các mô hình MoE loại bỏ token đều có thể sử dụng cùng micro batch size như dMoE tương tự mà không hết bộ nhớ GPU. Chúng tôi báo cáo thời gian huấn luyện từ đầu đến cuối và validation loss cho những mô hình này cùng với kết quả dMoE và Transformer tiêu chuẩn của chúng tôi trong Hình 8. Việc so sánh MoE và dMoE cho cùng độ chính xác là không tầm thường vì loại bỏ token làm giảm chất lượng mô hình. Đối với mỗi dMoE, chúng tôi ước tính runtime của MoE sẽ đạt được cùng validation loss bằng cách so sánh với điểm tương đương loss trên Pareto frontier MoE.

Ngay cả với capacity factor hiệu quả nhất cho mỗi MoE, dMoE được huấn luyện với MegaBlocks giảm thời gian huấn luyện cần thiết để đạt validation loss nhất định 1.38×, 1.37× và 1.18× cho MoE-XS, MoE-Small và MoE-Medium, tương ứng. Ngoài giảm đáng kể thời gian huấn luyện từ đầu đến cuối, hệ thống của chúng tôi giảm chi phí sử dụng MoE bằng cách giảm số lượng hyperparameters cần được điều chỉnh lại cho mỗi mô hình và tác vụ. Những tiết kiệm tính toán này có thể lại được áp dụng để khám phá các tham số khác để cải thiện thêm chất lượng mô hình.

Đối với MoE-Medium, chúng tôi quan sát một số mất hiệu quả trong triển khai của chúng tôi do micro batch size tương đối nhỏ có thể được sử dụng trong khi vừa với bộ nhớ GPU giới hạn. Đối với batch size nhỏ, kích thước tile nhỏ hơn (ví dụ, 64x128 hoặc 64x64) trong kernel thưa khối của chúng tôi có thể cải thiện hiệu suất bằng cách giảm lượng tính toán lãng phí khi kích thước bài toán không chia hết cho 128. Một hướng khác để tăng hiệu quả là giảm sử dụng bộ nhớ trên mỗi thiết bị sao cho có thể sử dụng batch size lớn hơn, thông qua song song hóa trên nhiều thiết bị hơn hoặc các kỹ thuật như selective recomputation (Korthikanti et al., 2022).

6.3 Hiệu suất Block-Sparse Matrix Multiplication

Để đánh giá chất lượng kernel block-sparse matrix multiplication của chúng tôi, chúng tôi benchmark các cấu hình bài toán được sử dụng trong huấn luyện mô hình MoE-XS, MoE-Small và MoE-Medium và so sánh với cuBLAS batched matrix multiplication. Điều này bao gồm forward pass, backward weights, và backward data operations cho hai lớp trong mỗi lớp FFN. Tổng cộng, chúng tôi benchmark 18 bài toán - 6 bài toán cho mỗi trong 3 mô hình. Để cho phép so sánh với batched matrix multiplication, chúng tôi benchmark mỗi bài toán với phân phối đồng đều token cho chuyên gia và cùng micro batch size được liệt kê trong Bảng 3. Những benchmark này có thể được xem như ablation đánh giá overhead sẽ được đưa ra nếu người ta sử dụng kernel thưa khối của chúng tôi để triển khai MoE loại bỏ token tiêu chuẩn. Đối với mỗi bài toán, chúng tôi tính trung bình throughput qua 100 lần thực hiện. Chúng tôi không bao gồm thời gian lấy để xây dựng metadata ma trận thưa trong những benchmark này vì những phép toán này phân bổ qua tất cả 6 bài toán trong một lớp FNN. Kết quả của những benchmark này được hiển thị trong Hình 9.

Trên những bài toán này, chúng tôi quan sát thấy rằng kernel thưa khối của chúng tôi có thể đạt được 98.6% throughput của cuBLAS với độ lệch chuẩn 4%. Throughput tương đối tối đa là 104% và tối thiểu là 91%. Nhìn chung, kernel của chúng tôi vượt trội nhẹ so với cuBLAS trên một nửa số bài toán và kém hơn nhẹ trên nửa còn lại. Trong khi benchmark CUTLASS, chúng tôi quan sát thấy rằng việc thay đổi thứ tự tính toán các tile của ma trận đầu ra có thể thay đổi throughput của phép toán lên đến 10% do hiệu ứng L2 caching. Chúng tôi tin rằng hầu hết sự khác biệt hiệu suất trong những kết quả này có thể được quy cho việc sắp xếp lại tính toán xảy ra với ma trận thưa khối, mặc dù cần điều tra thêm.

Một trường hợp chúng tôi lưu ý overhead bổ sung là trong các phép toán DSTD được sử dụng để tính toán gradient trọng số. Vì chúng tôi sử dụng secondary index để lặp qua toán hạng thưa theo thứ tự chuyển vị, các mẫu truy cập khi lặp qua ma trận này thể hiện ít locality không gian điều này lại giảm throughput của phép toán tổng thể. Mặc dù đây là vấn đề thú vị để nghiên cứu thêm, tác động tổng thể lên hiệu suất mô hình tối thiểu do cơ hội cải thiện giới hạn (<10%) kết hợp với lượng runtime từ đầu đến cuối tương đối nhỏ mà hai phép toán này đại diện.

7 CÔNG TRÌNH LIÊN QUAN

Định tuyến MoE. Các thuật toán định tuyến cải tiến cho MoE là một lĩnh vực nghiên cứu tích cực. Các lớp BASE công thức hóa định tuyến MoE như một bài toán gán tuyến tính cố gắng tối đa hóa các ái lực token-chuyên gia tổng hợp dưới ràng buộc của phép gán cân bằng hoàn hảo (Lewis et al., 2021). Phương pháp này đảm bảo không có token nào bị loại bỏ bằng cách định tuyến lại token đến các chuyên gia khác khi cần thiết. Clark et al. (2022) thấy rằng các lớp BASE có thể phát sinh overhead runtime đáng kể và đề xuất một phiên bản xấp xỉ sử dụng thuật toán Sinkhorn. Vì xấp xỉ của họ không còn đảm bảo tránh loại bỏ token, Clark et al. (2022) sử dụng capacity factor 2 cho tất cả các thí nghiệm. Các kỹ thuật khác đã được đề xuất để quyết định tĩnh ánh xạ token đến chuyên gia trước dựa trên hash functions (Roller et al., 2021). Tuy nhiên, Clark et al. (2022) quan sát thấy rằng phương pháp này không hoạt động tốt như các thuật toán định tuyến khác mà họ nghiên cứu. Gần đây hơn, Zhou et al. (2022) đề xuất đảo ngược bài toán định tuyến sao cho mỗi chuyên gia chọn các token có điểm số top-k của nó. Mặc dù điều này đảm bảo phép gán cân bằng tải các token cho chuyên gia, phương pháp này vẫn chịu loại bỏ token vì cùng một token có thể được chọn bởi nhiều chuyên gia. Chúng tôi mong đợi rằng các thuật toán định tuyến cải tiến bổ sung cho phương pháp tính toán chuyên gia hiệu quả và linh hoạt của chúng tôi. Khám phá cách những phương pháp này có thể được kết hợp là một hướng thú vị cho nghiên cứu tương lai.

MoE Hiệu suất Cao. Để mở rộng huấn luyện MoE, Tutel triển khai các primitive giao tiếp phân tán được tối ưu hóa cho MoE và các kỹ thuật để ẩn chi phí giao tiếp của expert model parallelism (Hwang et al., 2022). He et al. (2022) đề xuất FasterMoE, một hệ thống để huấn luyện phân tán MoE dựa trên các chiến lược giao tiếp hiệu quả và thay đổi thuật toán định tuyến MoE để tránh tắc nghẽn mạng. Triển khai của chúng tôi có thể hưởng lợi bổ sung từ những kỹ thuật này, đặc biệt là cho huấn luyện phân tán quy mô lớn.

Kernel Thưa. Các định dạng ma trận thưa cho phép truy cập chuyển vị hiệu quả được nghiên cứu kỹ (Bulu c ¸et al., 2009; Smith & Karypis, 2015; Li et al., 2018). Khám phá cách những định dạng này có thể được điều chỉnh cho block sparsity lớn trên GPU hiện đại là một hướng thú vị cho nghiên cứu tương lai.

8 KẾT LUẬN

Chúng tôi giới thiệu MegaBlocks, một hệ thống để huấn luyện MoE hiệu quả trên GPU. Hệ thống của chúng tôi dựa trên tái công thức hóa MoE theo các phép toán thưa khối và kernel GPU thưa khối mới xử lý hiệu quả tính động có trong MoE. Phương pháp của chúng tôi không bao giờ loại bỏ token và ánh xạ hiệu quả lên các bộ gia tốc phần cứng hiện đại, cho phép tăng tốc huấn luyện từ đầu đến cuối lên đến 40% so với MoE được huấn luyện với thư viện Tutel tiên tiến và 2.4× so với DNN được huấn luyện với framework Megatron-LM được tối ưu hóa cao.
