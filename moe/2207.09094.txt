# 2207.09094.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2207.09094.pdf
# File size: 3172551 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MoEC: Mixture of Expert Clusters
Yuan Xie, Shaohan Huang, Tianyu Chen, Furu Wei
Microsoft Research Asia, China
fv-yuanxie, shaohanh, v-tianyuchen, fuwei g@microsoft.com
Abstract
Sparsely Mixture of Experts (MoE) has received great interest
due to its promising scaling capability with affordable com-
putational overhead. MoE converts dense layers into sparse
experts, and utilizes a gated routing network to make experts
conditionally activated. However, as the number of experts
grows, MoE with outrageous parameters suffers from overﬁt-
ting and sparse data allocation. Such problems are especially
severe on tasks with limited data, thus hindering the progress
for MoE models to improve performance by scaling up. In
this work, we propose Mixture of Expert Clusters — a general
approach to enable expert layers to learn more diverse and ap-
propriate knowledge by imposing variance-based constraints
on the routing stage. We further propose a cluster-level ex-
pert dropout strategy speciﬁcally designed for the expert clus-
ter structure. Our experiments reveal that MoEC could im-
prove performance on machine translation and natural lan-
guage understanding tasks, and raise the performance upper
bound for scaling up experts under limited data. We also ver-
ify that MoEC plays a positive role in mitigating overﬁtting
and sparse data allocation.
Introduction
Scaling up the model capacity has shown to be promising
to achieve better performance on a variety of tasks, includ-
ing natural language understanding (Brown et al. 2020; Raf-
fel et al. 2019) and visual representation learning (Dosovit-
skiy et al. 2020; Bao, Dong, and Wei 2021). The continued
growth in model size and parameters brings higher compu-
tational cost, while large dense models have almost hit the
boundary of hardware capacity. In pursuit of better compu-
tational efﬁciency, sparse Mixture-of-Experts (MoE) is pro-
posed as an efﬁcient alternative to dense models (Lepikhin
et al. 2020; Fedus, Zoph, and Shazeer 2021; Riquelme et al.
2021; Lewis et al. 2021). For the sparsely-gated MoE trans-
formers, the feed-forward network (FFN) sub-layer will be
replaced by a set of experts with independent parameters.
The sparsity of MoE is brought by experts and the gated
routing network. The gated routing network will calculate
the routing score between input tokens and each expert and
activate experts with top-k routing scores. Most experts will
not be activated, thus forming a sparse structure. Since the
Copyright © 2022, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.computation cost is only proportional to the activated top-k
sub-network, sparsely activated MoE models could scale up
model parameters without signiﬁcantly increasing computa-
tional cost. With affordable computational overhead, MoE
models could achieve better performance than dense models
on various tasks such as neural machine translation (Lewis
et al. 2019; Conneau and Lample 2019; Lepikhin et al.
2020),image recognition (Riquelme et al. 2021) and speech
recognition (Kumatani et al. 2021).
Recent studies have reached a consensus that more ex-
perts mean more parameters and large model capacity,
which always bring improvements. However, some studies
show that more trainable parameters and sparse conditional
computation may introduce overﬁtting (Xue et al. 2021; Lou
et al. 2021; Xue et al. 2022), especially for downstream tasks
with limited data. As depicted in Figure 1, as the number of
experts grows, overﬁtting gradually becomes apparent in the
machine translation task. Moreover, we ﬁnd that enlarging
the size of the MoE will not always lead to improvement.
There seems to exist a performance upper bound of scaling
up experts with limited data.
Moreover, we ﬁnd an unreasonable phenomenon in Fig-
ure 1: 64-expert MoE with more parameters and larger
model capacity has higher training loss than 32-expert MoE.
It implies that large-scale MoE not only suffers from overﬁt-
ting, but also has other hidden problems that affect training.
According to our analysis, the probability of each expert get-
ting a token reduces proportionally as the number of experts
grows. With the same data, each expert will get less diverse
samples. It may affect the sufﬁcient learning of expert lay-
ers. Insufﬁcient data to match more parameters is also a ma-
jor cause of overﬁtting. Therefore, we want to explore ways
in which experts could get diverse samples and learn abun-
dant knowledge, thereby alleviating overﬁtting and sparse
data allocation.
In this work, we propose Mixture of Expert Clusters
(MoEC), a general optimizing strategy for MoE models.
We close the routing probability among neighbor experts
to form the clustered expert structure. The inductive bias
expects that the similarity of intra-cluster experts is high
while the similarity of inter-cluster experts is low. Experts
within a cluster are prone to tokens with similar hidden
states and could “share” similar tokens. Moreover, we pro-
pose a cluster-level expert dropout strategy for the expertarXiv:2207.09094v1  [cs.CL]  19 Jul 2022

--- PAGE 2 ---
Figure 1: A simple demonstration of loss curves of MoE
models on WMT-14 English-to-German translation task. We
show the loss curve of MoE baseline models with different
experts. The value in the box represents the minimum loss.
cluster structure. Several experts in the cluster will be ran-
domly dropped, the dropped experts will not participate in
the routing stage. The activated experts will be selected from
the remaining experts in the cluster. Implementing dropout
within clusters will make tokens always dispatched to suit-
able experts, no matter how random the dropout is.
We evaluate our MoEC on machine translation and natu-
ral language understanding tasks. Experiment results show
that MoEC outperforms dense models and baseline MoE
models. It indicates that MoEC retains the advantages of the
sparse structure of MoE, and alleviates overﬁtting and sparse
data allocation problems.
Our contributions are summarized as follows:
• We point out the overﬁtting and sparse data allocation
problems for large-scale MoE models, and experts get-
ting less diverse samples could be the common cause of
both problems.
• We propose to build expert clusters by variance-based
constraints, which allows experts to get a more diverse
set of similar tokens. We also implement cluster-level ex-
pert dropout as a regularization method.
• We conduct experiments on translation and natural lan-
guage understanding tasks. MoEC could improve perfor-
mance and alleviate problems caused by scaling up ex-
perts.
• We ﬁnd that there exists a performance upper bound for
scaling up MoE models with limited data, and MoEC
could raise the upper bound.
Related Work
In the context of modern deep learning architectures, scal-
ing up transformers using sparse Mixture of Experts (MoE)
is proven to be effective to achieve state-of-the-art perfor-
mance on various NLP and CV tasks (Shazeer et al. 2017;
Lepikhin et al. 2020; Riquelme et al. 2021; Fedus, Zoph, andShazeer 2021). Compared with dense transformers, an MoE
model contains several experts (feed-forward networks), and
a router to select top-k experts for input tokens. It increases
the model capacity by such conditional computation while
maintaining computational efﬁciency. To future explore the
potential of MoE, some studies focus on router assignment
algorithm (Lewis et al. 2021; Roller et al. 2021; Dai et al.
2022). Besides, some work focus on optimizing training
methods for MoE models. Dua et al. (2021) applied a tem-
perature heating mechanism for sparse MoE models on the
translation task. Chi et al. (2022) proposed a dimension re-
duction to estimate the routing scores between tokens and
experts on a low-dimensional hyper-sphere. Our work is also
proposed to optimize the MoE model. Instead of changing
the model structure and routing strategy, MoEC establishes
expert clusters, which allows experts to be assigned a more
diverse set of similar tokens.
Although MoE models have achieved promising results,
they are proven to have overﬁtting problems (Fedus, Zoph,
and Shazeer 2021; Wu et al. 2022; Xue et al. 2022) on down-
stream tasks with limited data. To mitigate overﬁtting, some
works use knowledge distillation to distill MoE models into
small-sized MoE models or dense models (Xue et al. 2022;
Dai et al. 2022). Another approach is to apply the dropout
strategy during training. Fedus, Zoph, and Shazeer (2021)
set a small dropout rate at non-expert layers and a larger
dropout rate at expert layers. Liu et al. (2022) propose gat-
ing dropout, which allows some tokens to ignore the gated
routing network and stay at their local machines to reduce
cross-machine communication. In our work, we propose the
cluster-level expert dropout. Randomly selected experts in
the cluster will be dropped so that they will not participate
in the routing stage.
Preliminary
To build MoE transformers, it is a common practice to re-
place feed-forward network (FFN) sub-layers with a set of
experts. The experts share the same structure with the FFN
layer in the dense transformer model. We denote the hidden
representation of input token xash, and the embedding for
thei-th expert as ei. The router computes the routing score
si=hTeito compare the similarity between hand the set
of expertsE. Then, the router utilizes a gating function ()
to compute the gated value of expert i.
i=8
>>><
>>>:exp(si)PE
j=1exp(sj);softmaxgating
1
1 +exp( si);sigmoidgating(1)
The gating function irepresents the probability of dis-
patching input token to expert i. The top-k gated-value is
used for dispatching the token xaccording to i. The corre-
sponding k expert networks are conditionally activated. We
denote the set of selected top-k indices as K.
y=X
i2KiEi(x) (2)

--- PAGE 3 ---
Figure 2: Illustration of a conventional MoE layer and our
proposed MoEC layer. The similarity between hidden states
Hiis represented by the color.
whereEi(x)is thei-th expert network, which is a feed-
forward network. The output of the gated routing network is
the linearly weighted combination of each expert’s compu-
tation on the token by the gate value.
Method
In this work, our goal is to give experts access to more di-
verse training samples, thus learning abundant knowledge
and mitigating overﬁtting and sparse data allocation. We
close the routing probability among neighbor experts to
form the clustered expert structure. We apply the variance-
based clustering loss to implement constraints. Then, we fur-
ther propose a cluster-level expert dropout strategy. In our
work, we use top-1 gating. Only the expert with the largest
routing score is activated. And we choose softmax as our ac-
tivation function. Experts in a cluster will be distributed on
the same device to reduce communication costs.
Mixture of Expert Clusters
We illustrated our MoEC (Mixture of Expert Clusters) in
Figure 2. For conventional MoE, the routing probability of
tokens will not be constrained. The router will always dis-
patch input tokens to their best-matched experts, while other
similar tokens have little chance of being selected. When
scaling up the number of experts, the sparse data distribution
will cause each expert to get less diverse tokens. The expert
layer could not get adequately trained. Also, the amount of
data is insufﬁcient to match the growing number of parame-
ters, which is also the main reason for overﬁtting. In order to
solve the problems of conventional MoE, our MoEC allows
each expert to get more rich and diverse tokens. We impose
variance-based constraints on the routing stage, aiming to
make neighbor experts have similar routing probabilities for
input tokens, thus forming expert clusters prone to tokens
with similar hidden states. In MoEC, experts will get a more
diverse set of similar input tokens by “sharing” input tokens
with other experts in the cluster.
Compared with previous work related to MoE, our train-
ing objective added an extra term - cluster loss. The overall
training objective is to minimize:L=Ltask+Lbalance +Lcluster (3)
Ltaskis determined by the speciﬁc task. In our work, we
employ the label smoothed cross-entropy loss for neural ma-
chine translation, masked language modeling loss for pre-
training language model, and negative log-likelihood loss
(NLL loss) or mean-squared loss (MSE loss) for GLUE
tasks. In the following, we will introduce Lbalance and
Lcluster .
Load Balancing Loss. During training, there exists a load
imbalance issue between experts (Shazeer et al. 2017; Lep-
ikhin et al. 2020): Most tokens are dispatched to a small
number of experts, while many other experts do not get suf-
ﬁciently trained at all. Besides, imbalanced assignments will
result in a high computational bottleneck in the MoE layer
and thus limit the computational efﬁciency. We follow the
work in (Fedus, Zoph, and Shazeer 2021) and add the bal-
ance loss to the training objective to encourage a balanced
load across experts. Given Nexperts indexed by i=1 toN,
the balance loss is computed as follows:
Lbalance =NNX
i=1fipi (4)
wherefiis the fraction of tokens dispatching to expert i.
We denote the number of tokens dispatched to the i-th expert
asCounti. Given a batch BwithTtokens,fi=Counti=T.
piis the fraction of the routing probability allocated for ex-
pertiin the batch B. It is calculated by averaging the prob-
ability of routing token xto expertiin the batch B.
pi=1
TX
x2Bi(x) (5)
wherei(x)is the gating function depicted in Equation 1,
which represents the probability of dispatching token xto
experti. The balance loss in Equation 4 encourages uniform
routing since it would be minimized under a uniform distri-
bution. To control the impact of balance loss in the training
process, a hyper-parameter is applied as a multiplicative
coefﬁcient for the loss. Throughout this work, we use an
= 10 2which was sufﬁciently large to ensure load bal-
ancing while small enough not to overwhelm the primary
cross-entropy objective.
Clustering Loss.
In our work, we ﬁnd the sparse allocation of data severely
hinders the adequate training of MoE layers and exacerbates
overﬁtting. In order to allow experts to get rich and diverse
tokens to mitigate the impact of sparse allocation, we design
the clustering loss. This loss is designed to constrain certain
adjacent experts so that they will share similar routing prob-
abilities to tokens, thus forming a cluster-like distribution.
For input tokens originally dispatched to the best-matched
experts, clustering loss will give them more opportunities to
access other experts in the cluster. As a result, experts will
be assigned a more diverse set of similar tokens, thus allevi-
ating the problem of sparse allocation.
In MoE models with Nexperts, the clustering loss will
guide experts to form mclusters (mshould be divisible

--- PAGE 4 ---
byN), and each cluster contains L=N
mexperts. We use
Ej
ito represent the j-th expert in the i-th cluster, while pj
i
represents the routing probability allocated for Ej
i(i=
0;1;:::;m 1;j= 0;1;:::;L 1). According to the size
and number of clusters, p0
i;p1
i;:::;pL 1
iwill compose a one-
dimensional matrix ~Pi2RLto represent the routing proba-
bilities of the Lexperts in the i-th cluster, and we denote the
mean value of them as pi. We deﬁne the clustering loss as
follows:
Lclustering =NCintraCinter
=NPm 1
i=0(~Pi)
me maxfpig max2fpig
maxfpig
(6)
As can be seen from Equation 6, clustering loss is mainly
composed of two parts: the variance-based intra-cluster con-
straint Cintra and the difference-based inter-cluster con-
straint Cinter .
(~Pi) =(p0
i pi)2+(p1
i pi)2+:::+(pL 1
i pi)2]
Lrepresents the
variance of the routing probability in the i-th cluster. We
compute the mean variance of mclusters as the intra-cluster
constraint Cintra , which will be minimized when the routing
probabilities of experts within the same cluster are balanced.
Besides, we use Cinter to measure the probability dif-
ference between the dispatched cluster and the sub-optimal
cluster. maxfgmeans the max value of pi(i=0,1,...,m-1)
andmax 2fgmeans the second max value. Cinter will be
minimized when the probability of a token being dispatched
to a suboptimal cluster is low. is the coefﬁcient used to
control the value of Cinter . When we set = 0, the proba-
bility difference between clusters will not be considered. We
could also set to a non-zero value to activate Cinter . We
will conduct in-depth experiments and analysis on it in the
Experiments chapter.
To minimize clustering loss, the probability distribution
within the cluster should be uniform, and the probability dif-
ference between the clusters should be more apparent (op-
tional). In the initial training steps, the variance among ex-
perts will be very high, so the clustering loss will dominate
the optimization and guide the rapid formation of expert
clusters. When the intra-cluster variance is stable, the clus-
tering loss will become relatively small to maintain the ex-
pert clusters. Similar to the practice in balance loss, a hyper-
parameteris applied. The value of the should be rel-
atively small, because a large means a strong clustering
constraint, thus making experts in the cluster too similar. It
will cause these experts to lose their characteristics, and the
contributions of multiple similar experts are only approxi-
mately equal to one expert. In our work, we set the value
ofas10 2by default. Experiments on the selection of 
values could be found in Appendix A.
Cluster-level expert dropout
When applying large-scale MoE models on tasks with lim-
ited data, over-ﬁtting issues naturally arise. Previous MoE-
related work (Raffel et al. 2019; Fedus, Zoph, and Shazeer
2021) used dropout (Srivastava et al. 2014) at each layer to
Figure 3: Illustration of global-level expert dropout and
cluster-level expert dropout. The similarity between hidden
statesHiis represented by the color.
prevent overﬁtting. Here, cluster-level expert dropout acts as
a regularization technique completely different from tradi-
tional dropout. It does not drop parameters, but drops some
experts in the cluster, which makes the dispatching of tokens
more random.
Implementation in clusters. First, our cluster-level ex-
pert dropout works at the routing stage, so it will only be
implemented at expert layers. For experts in a cluster, we
randomly drop some of them by deleting the expert ids from
the candidate expert list when calculating the routing prob-
ability. Thus, the corresponding experts will be ignored in
the routing stage. Assume the dropout rate as , only the re-
mainingN(1 )experts will participate in the calculation
of routing probability during training. The dimension of the
matrixPwill decrease from RNtoRN(1 ). All clusters
implement the dropout simultaneously. It allows tokens to
have more opportunities to be dispatched to other experts in
the same cluster, instead of being repeatedly dispatched to
the expert with the highest probability. From another per-
spective, each expert will receive more diverse tokens with-
out adding training data.
Cluster-level expert dropout vs Traditional expert
dropout.
Traditional expert dropout is recommended in Fedus,
Zoph, and Shazeer (2021). It is a dropout technique (Srivas-
tava et al. 2014) to regularize MoE models, which acts on the
feed-forward layer to reduce overﬁtting caused by too many
parameters. By setting a relatively small dropout rate at non-
expert layers (0.1), expert dropout increases the dropout rate
by an explicit amount at the interim feed-forward computa-
tion at each expert layer (0.4). Our expert dropout acts com-
pletely different from it. We perform random dropout on the
candidate list of experts during the routing stage. It does not
reduce the number of parameters during training but allo-
cates tokens more diversely and ﬂexibly. While traditional
expert dropout is usually used for ﬁne-tuning on downstream
tasks, our cluster-level expert dropout is a general regular-
ization mechanism with strong generality. In addition, our
dropout can be applied together with Fedus’ expert dropout,
and they can work together to improve the performance of
MoE.

--- PAGE 5 ---
Why cluster-level is better?
It is natural to think that expert dropout could be imple-
mented at the global level, which provides more opportu-
nities for tokens to access other sub-optimal experts. But
for global-level expert dropout, as shown in Figure 3, if a
random dropout happens to drop suitable experts, tokens
may be dispatched to less relevant experts. Inappropriate dis-
patching may negatively impact the learning of experts.
In MoEC, We address this problem by exploiting the
cluster-like structure and design a cluster-level expert
dropout. Cluster-level dropout could give tokens the option
to be randomly re-dispatched while conﬁning the routing re-
sults to a more reasonable range. No matter how random the
dropout is implemented, tokens will always be dispatched to
experts with similar routing probability.
Experiments
We name our model MoEC (Mixture of Expert Clusters),
and evaluate the performance on bilingual machine transla-
tion and natural language understanding tasks. We use the
X-MoE model from Chi et al. (2022) as our backbone ar-
chitecture, which has shown better performance than prior
MoE models such as Switch Transformers (Fedus, Zoph,
and Shazeer 2021) on widely-used cross-lingual understand-
ing benchmarks.
Evaluation Dataset
WMT 2014 English-to-German Ninth Workshop on Sta-
tistical Machine Translation (WMT 2014) releases a collec-
tion of datasets used in shared tasks including machine trans-
lation. We add additional news-commentary-v12 data from
WMT-17 for training and validation. The total training data
contains 3.96M English-to-German sentence pairs.
GLUE General Language Understanding Evaluation (Wang
et al. 2018) benchmark is a collection of tools for evaluating
the performance of models across a diverse set of existing
NLU tasks, including MNLI (Williams, Nangia, and Bow-
man 2017), CoLA (Warstadt, Singh, and Bowman 2019),
SST-2 (Socher et al. 2013), QQP, QNLI (Rajpurkar et al.
2016), MRPC (Dolan and Brockett 2005) and STS-B (Cer
et al. 2017). We do not perform experiments on RTE be-
cause previous work (Chen et al. 2022) demonstrated that
MoE is not suitable for this task. It is worth mentioning that
we will pre-train our model on the BooksCorpus (Zhu et al.
2015) and English Wikipedia corpus (Foundation) for 120k
steps before ﬁne-tuning on GLUE tasks.
Experiments Setup
Model Architecture For our MoEC and all baseline mod-
els, we follow the recommended settings in (Vaswani et al.
2017) and use Transformer-big as the uniﬁed backbone ar-
chitecture on WMT 2014 English-German translation task.
For GLUE tasks, we use Transformer-base as the backbone
architecture.
For MoE layers, we apply the 64-expert MoE model with
3 FFN sub-layers in the 3rd encoder block and 3rd decoder
block. A more detailed model hyper-parameters could be
found in Appendix B.Baselines
We conduct two baselines in our experiments. The ﬁrst is
dense transformer (Vaswani et al. 2017). For another, we
follow the work in (Chi et al. 2022) and apply X-MoE as
ourMoE baseline . It could serve as a strong baseline that
shows better performance than Switch Transformer (Fedus,
Zoph, and Shazeer 2021) on widely-used cross-lingual un-
derstanding benchmarks. The MoE baseline estimates rout-
ing scores between tokens and experts on a low-dimensional
hypersphere and adds a learnable temperature scalar in the
gating function. For a fair comparison, the two baseline
methods are built with the same setting as MoEC, which
could be found in Appendix B.
MoEC Hyper-parameters For MoEC, several unique
hyper-parameters are introduced. For clustering loss, we set
to10 2according to the experiment results (see Appendix
A) and set= 0by default. For cluster size (the number of
experts in a cluster) and expert dropout rate, we will have
detailed related experiments in the following sections.
Training Hyper-parameters For a fair comparison, the
dense model, MoE baseline model, and MoEC model share
the same training hyper-parameters. All models are trained
with the Adam optimizer (Kingma and Ba 2014) ( 1=
0:9;2= 0:98). The learning rate is set 5e 4with 4000
warm-up steps and inverse square root scheduler (Raffel
et al. 2019). Batch size, training steps, and dropout rate are
set by different tasks, which are recorded in Appendix C.
Experiments results
We train dense models, baseline MoE and MoEC models
on several widely-used evaluation tasks, and the results are
shown in Table 1. Compared with dense models, MoE mod-
els exhibit signiﬁcant performance improvements, which
beneﬁt from the large model capacity. Besides, MoEC could
bring notable improvement over the MoE baseline without
applying the dropout strategy to experts. On WMT-14, it
gives a 1.62 BLUE score boost. The advantage could be at-
tributed to the clustered distribution of experts, which en-
dows experts with more diverse and appropriate training
samples. Moreover, with the application of the cluster-level
expert dropout strategy, the performance of MoEC will be
further improved.
As shown in Figure 4, the MoE baseline severely suf-
fers from overﬁtting on WMT-14, while our MoEC shows
excellent ability to mitigate overﬁtting. The overﬁtting phe-
nomenon on the validation set is almost eliminated, and the
validation loss is relatively lower. It shows that when our
MoEC solves the sparse allocation of data, each expert could
get more abundant and diverse training samples. In this way,
the training data of each expert is kept sufﬁcient, thereby
alleviating the phenomenon of overﬁtting. Furthermore, we
found that MoEC converges slightly slower. It is due to the
fact that each expert needs to learn from more diverse train-
ing samples, which takes more steps to allow the expert to
get sufﬁciently trained.
Detailed analysis of expert clusters
Next, we conduct a detailed analysis of expert clusters. Fig-
ure 5 shows the fraction of tokens dispatched to cluster 0

--- PAGE 6 ---
Table 1: The performance on machine translation and GLUE tasks for baselines and MoEC models. WMT-14 is measured
on the test set, while GLUE tasks are measured on the development sets. We report the average results by a set of seeds (see
Appendix C). All experiments are conducted with 64 experts.
NMT GLUE Tasks
WMT14 En-De MNLI CoLA SST-2 QQP QNLI MRPC STS-B GLUE Avg
Dense 27.10 85.97 57.10 92.87 91.20 92.23 87.50 89.18 85.16
MoE Baseline 30.59 87.27 75.60 93.30 91.37 92.33 86.30 88.28 87.78
MoEC (w/o expert dropout) 32.21 87.37 75.93 93.43 91.45 92.40 88.07 89.11 88.25
MoEC 32.50 87.37 76.80 93.37 91.40 92.45 88.23 89.24 88.41
Figure 4: Loss curves on the WMT-14 validation set. All
experiments are conducted with 64 experts for a fair com-
parison. The validation loss that rises with increasing train-
ing steps indicates the overﬁtting phenomenon. Our MoEC
shows excellent ability to mitigate overﬁtting.
(expert 03) during training and inference. During training,
the experts in cluster 0 get similar input tokens, which are
affected by balance loss and clustering loss. During infer-
ence, the routing probabilities of experts in the cluster vary,
which indicates that they still retain their own characteris-
tics. They learn more ﬁne-grained knowledge, which is the
advantage of multiple similar experts compared to a single
expert. For WMT14, the BLUE score of MoE with 16 ex-
perts is 30.49, while the BLUE score of MoE with 16 clus-
ters (cluster size=4) is 32.16. It shows that multiple similar
experts have an obvious advantage over a single expert.
The cluster size also has a critical impact on the learning
of MoEC, so we conduct experiments on different cluster
sizes. As depicted in Table 2, the best performance is ob-
tained when cluster size = 8. Compared to the MoE base-
line with 64 experts, expert clusters could bring about a 1.62
BLUE scores improvement. When the cluster size is rel-
atively small, the data shared among experts will be less,
and the improvement brought by MoEC will not be fully
exploited. As a special case, when cluster size=1, a single
expert could not be called a cluster, and MoEC is equiva-
lent to MoE baseline. When the cluster size is large, the data
shared among experts will increase, but the similarity and
correlation of these data will become lower, which will lead
to an adverse impact on the “professionalism” of each ex-
pert. When we expand the cluster size to 16, the performance
Figure 5: Fraction of tokens dispatched to Expert 0 3 (i.e.fi
mentioned above) of 64-expert MoEC (cluster size = 4) dur-
ing training and inference. The graph on the left represents
the fraction of tokens dispatched to cluster 0 during training,
while the right shows the fraction of tokens dispatched to
cluster 0 during inference.
of MoEC is even lower than that of the MoE baseline, which
means that an excessively large cluster size will suppress the
advantages of MoE structure and hurt the performance.
Table 2: The performance of MoEC with different cluster
sizes on WMT-14. All experiments were conducted with 64
experts. For a fair comparison, all methods do not employ
the dropout on experts.
Cluster size Number of clusters BLEU
1 64 30.59
4 16 32.16
8 8 32.21
16 4 29.98
Expert dropout: Cluster-level vs global-level
In Table 3, we experiment on WMT-14 with the cluster-level
expert dropout rate. We ﬁnd that cluster-level dropout could
enhance the generalization performance of MoEC. Such a
regularization method could bring a 0.29 BLUE scores im-
provement for MoEC. Experimental results show that 0.5 is
a good choice for the dropout rate. Besides, it is obvious that
global-level expert dropout will hurt the performance.

--- PAGE 7 ---
Table 3: Cluster-level vs global-level expert dropout on
WMT-14. All experiments are conducted on the 64-expert
MoEC and cluster size = 8. Under this setting, the BLUE
score of MoEC without expert dropout is 32.21.
Dropout rate cluster-level global-level
0 32.21 32.21
0.25 32.32 31.88
0.5 32.50 31.53
0.75 32.02 29.73
For cluster-level expert dropout, when dropping the best-
matched expert for input tokens, the routing decision will
still be made among the rest experts in the cluster. Regard-
less of how the dropped experts are selected, there will al-
ways be experts left in each cluster. It ensures that suitable
experts are always available. But for the global-level one,
due to the random distribution of experts, if all matched
experts are dropped, the token will be routed to an inap-
propriate expert. It could cause experts to be distracted by
low-relevant data, thus negatively impacting the learning of
knowledge. Take Figure 3 as a simple example (with set-
ting the dropout rate to 0.5). For global-level expert dropout,
when both expert1 and expert2 are dropped, then Hnwill
only be dispatched to expert3 or expert4. This inappropriate
allocation could hurt the performance of the model.
Role of the inter-cluster constraint coefﬁcient Cinter
We further explore whether the inter-cluster constraint co-
efﬁcient Cinter (in Equation 6) will help improve perfor-
mance. As depicted in Figure 6, when dropout=0.75 or clus-
ter size=4, setting to 1 will get better results. In other cases,
it is better not to apply inter-cluster constraints by setting 
to 0.
When there are sufﬁcient experts in the cluster, it is better
not to use the inter-cluster constraint by setting to 0. Intra-
cluster constraints have already made other experts in the
cluster have higher routing probabilities, while inter-cluster
constraints will further widen the routing probability gap be-
tween clusters. This will cause the entropy of the routing
probability distribution to be too small, which is not con-
ducive to the learning of the gated network.
We ﬁnd that the inter-cluster constraint will beneﬁt MoEC
when the cluster size is small or the expert dropout rate is
high. In this case, the number of experts in the cluster is
small, and the intra-cluster constraint alone is not enough to
form a globally reasonable routing probability distribution,
so the assistance of constraints between clusters is needed.
Raising the upper bound of MoE
In general, a higher number of experts means higher model
capacity and better performance. However, for tasks with
limited data, there exists a performance upper bound on scal-
ing up MoE models. We take a deep dive into the ability of
MoEC to raise the upper bound. As shown in Table 4, for the
MoE baseline, expert = 32 is the upper bound, which means
that continuing to increase the number of experts will not
Figure 6: Two sets of experiments on the inter-cluster con-
straint coefﬁcient Cinter . All experiments are performed on
WMT14 En-De. The ﬁgure on the left is about experiments
with different expert dropout rates (cluster size=8), and The
ﬁgure on the right is about experiments with different cluster
sizes (without expert dropout).
Table 4: Results of scaling up MoEC.
Expert num MoE baseline MoEC Beneﬁts
16 30.49 30.50 +0.01
32 30.81 30.84 +0.03
64 30.59 32.50 +1.91
128 30.21 32.40 +2.19
bring any gain to the model. Our MoEC not only has a per-
formance advantage over the MoE baseline with the same
number of experts, but also improves the upper bound from
32 to 64.
With the increase of experts, our MoEC could bring more
gains. It is because MoEC could fully show its promis-
ing ability to solve severe overﬁtting and sparse allocation
problems. With the mitigation of the above two problems,
the superiority of the large-scale MoE model will be bet-
ter exerted, thereby achieving the improvement of the upper
bound of MoE models. With the help of MoEC, we could
try to build sparse models with more experts.
Conclusion
In our work, we point out the overﬁtting and the sparse data
allocation problems of large-scale MoE models and pro-
pose a novel training strategy - MoEC to convert experts
into clusters. Each expert could get more abundant and di-
verse training samples. In this way, the training data of each
expert is kept sufﬁcient, thereby alleviating overﬁtting. We
also propose the cluster-level expert dropout to realize regu-
larization. We conduct experiments on machine translation
and natural language understanding tasks. Experiment re-
sults show MoEC could improve performance and allevi-
ate problems caused by scaling up experts without changing
the model structure and routing strategy. The superiority of
the large-scale MoE model will be better exerted by MoEC,
thereby raising the upper bound of MoE models. With the
help of MoEC, we could try to build sparse models with
more experts.

--- PAGE 8 ---
References
Bao, H.; Dong, L.; and Wei, F. 2021. Beit: Bert pre-training
of image transformers. arXiv preprint arXiv:2106.08254 .
Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,
A.; et al. 2020. Language models are few-shot learners. Ad-
vances in neural information processing systems , 33: 1877–
1901.
Cer, D.; Diab, M.; Agirre, E.; Lopez-Gazpio, I.; and Specia,
L. 2017. Semeval-2017 task 1: Semantic textual similarity-
multilingual and cross-lingual focused evaluation. arXiv
preprint arXiv:1708.00055 .
Chen, T.; Huang, S.; Xie, Y .; Jiao, B.; Jiang, D.; Zhou,
H.; Li, J.; and Wei, F. 2022. Task-Speciﬁc Expert
Pruning for Sparse Mixture-of-Experts. arXiv preprint
arXiv:2206.00277 .
Chi, Z.; Dong, L.; Huang, S.; Dai, D.; Ma, S.; Patra, B.;
Singhal, S.; Bajaj, P.; Song, X.; and Wei, F. 2022. On
the Representation Collapse of Sparse Mixture of Experts.
arXiv preprint arXiv:2204.09179 .
Conneau, A.; and Lample, G. 2019. Cross-lingual language
model pretraining. Advances in neural information process-
ing systems , 32.
Dai, D.; Dong, L.; Ma, S.; Zheng, B.; Sui, Z.; Chang, B.;
and Wei, F. 2022. StableMoE: Stable routing strategy for
mixture of experts. arXiv preprint arXiv:2204.08396 .
Dolan, B.; and Brockett, C. 2005. Automatically construct-
ing a corpus of sentential paraphrases. In Third International
Workshop on Paraphrasing (IWP2005) .
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 .
Dua, D.; Bhosale, S.; Goswami, V .; Cross, J.; Lewis, M.;
and Fan, A. 2021. Tricks for Training Sparse Translation
Models. arXiv preprint arXiv:2110.08246 .
Fedus, W.; Zoph, B.; and Shazeer, N. 2021. Switch trans-
formers: Scaling to trillion parameter models with simple
and efﬁcient sparsity. arXiv preprint arXiv:2101.03961 .
Foundation, W. ???? Wikimedia Downloads.
Kingma, D. P.; and Ba, J. 2014. Adam: A method for
stochastic optimization. arXiv preprint arXiv:1412.6980 .
Kumatani, K.; Gmyr, R.; Salinas, F. C.; Liu, L.; Zuo, W.;
Patel, D.; Sun, E.; and Shi, Y . 2021. Building a great multi-
lingual teacher with sparsely-gated mixture of experts for
speech recognition. arXiv preprint arXiv:2112.05820 .
Lepikhin, D.; Lee, H.; Xu, Y .; Chen, D.; Firat, O.; Huang,
Y .; Krikun, M.; Shazeer, N.; and Chen, Z. 2020. Gshard:
Scaling giant models with conditional computation and au-
tomatic sharding. arXiv preprint arXiv:2006.16668 .
Lewis, M.; Bhosale, S.; Dettmers, T.; Goyal, N.; and Zettle-
moyer, L. 2021. Base layers: Simplifying training of large,
sparse models. In International Conference on Machine
Learning , 6265–6274. PMLR.Lewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-
hamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.
2019. Bart: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461 .
Liu, R.; Kim, Y . J.; Muzio, A.; Mozafari, B.; and Awadalla,
H. H. 2022. Gating Dropout: Communication-efﬁcient
Regularization for Sparsely Activated Transformers. arXiv
preprint arXiv:2205.14336 .
Lou, Y .; Xue, F.; Zheng, Z.; and You, Y . 2021. Sparse-mlp: A
fully-mlp architecture with conditional computation. arXiv
preprint arXiv:2109.02008 .
Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Explor-
ing the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683 .
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
Squad: 100,000+ questions for machine comprehension of
text. arXiv preprint arXiv:1606.05250 .
Riquelme, C.; Puigcerver, J.; Mustafa, B.; Neumann, M.; Je-
natton, R.; Susano Pinto, A.; Keysers, D.; and Houlsby, N.
2021. Scaling vision with sparse mixture of experts. Ad-
vances in Neural Information Processing Systems , 34.
Roller, S.; Sukhbaatar, S.; Weston, J.; et al. 2021. Hash lay-
ers for large sparse models. Advances in Neural Information
Processing Systems , 34.
Shazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.;
Hinton, G.; and Dean, J. 2017. Outrageously large neu-
ral networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538 .
Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,
C. D.; Ng, A. Y .; and Potts, C. 2013. Recursive deep models
for semantic compositionality over a sentiment treebank. In
Proceedings of the 2013 conference on empirical methods in
natural language processing , 1631–1642.
Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and
Salakhutdinov, R. 2014. Dropout: a simple way to prevent
neural networks from overﬁtting. The journal of machine
learning research , 15(1): 1929–1958.
Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-
tention is all you need. Advances in neural information pro-
cessing systems , 30.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. R. 2018. GLUE: A multi-task benchmark and
analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461 .
Warstadt, A.; Singh, A.; and Bowman, S. R. 2019. Neural
network acceptability judgments. Transactions of the Asso-
ciation for Computational Linguistics , 7: 625–641.
Williams, A.; Nangia, N.; and Bowman, S. R. 2017. A
broad-coverage challenge corpus for sentence understand-
ing through inference. arXiv preprint arXiv:1704.05426 .
Wu, L.; Liu, M.; Chen, Y .; Chen, D.; Dai, X.; and Yuan,
L. 2022. Residual Mixture of Experts. arXiv preprint
arXiv:2204.09636 .

--- PAGE 9 ---
Xue, F.; He, X.; Ren, X.; Lou, Y .; and You, Y . 2022. One
Student Knows All Experts Know: From Sparse to Dense.
arXiv preprint arXiv:2201.10890 .
Xue, F.; Shi, Z.; Wei, F.; Lou, Y .; Liu, Y .; and You,
Y . 2021. Go wider instead of deeper. arXiv preprint
arXiv:2107.11817 .
Zhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,
R.; Torralba, A.; and Fidler, S. 2015. Aligning books and
movies: Towards story-like visual explanations by watching
movies and reading books. In Proceedings of the IEEE in-
ternational conference on computer vision , 19–27.
Appendix
A Selection of the value of 
Table 5: The performance of MoEC with different coef-
ﬁcients on WMT-14. All experiments are conducted with 64
experts. The cluster sizes=8, and expert dropout rate=0.25.
Value ofMoEC
1e-3 32.21
5e-3 32.17
1e-2 32.32
5e-2 31.21
Table 5 presents the experiments on selecting the best
value of. MoEC works best when is set to 1e-2. And
when the beta value is too large, the performance of MoEC
drops signiﬁcantly, which conﬁrms our analysis in the main
text. Based on the results, we uniformly set the value of as
10 2as a default in all experiments above.
B Architecture parameters
Table 6 presents the architecture parameters for different
tasks.
Table 6: Architecture parameters for all tasks
- WMT-14 En-De Pre-train&GLUE
Transformer blocks 12 12
Attention heads 16 12
Encoder/Decoder embedding 1024 768
FFN embedding 4096 3072
Experts [16,32,64,128] [16,32,64,128]
Routing dimension [8,16,32,64] [8,16,32,64]
MoE layers 2 1
Sub-layers 3 3
C Training hyper-parameters
Table 7 presents the training hyper-parameters for WMT-
14 and pre-training. Table 8 presents the training hyper-
parameters on downstream GLUE tasks.Table 7: Training hyper-parameters for all tasks
- WMT-14 En-De Pre-train
Optimizer Adam Adam
Adam 1e-6 1e-6
Adam (0.9,0.98) (0.9,0.98)
Training Steps 32k 125k
Batch size 8k 2k
Maximum learning rate 5e-4 5e-4
Learning Rate Scheduler inverse sqrt inverse sqrt
Warmup steps 4k 4k
Weight decay 0 0.01
Dropout 0.3 0.1
Attention dropout 0.1 0
Gradient Clip Norm 0.1 0.1
Label smoothing 0.1 -
Capacity factor 2 2
MoE dropout 0.4 0
MoE activation dropout 0.1 0
balancing coefﬁcient  0.01 0.01
Table 8: Training hyper-parameters for GLUE.
Hyper-parameters MNLI SST-2 QQP QNLI CoLA STS-B MRPC RTE
Batch Size 32 32 32 32 32 32 32 32
Epochs [3,5] [3,5] [3,5] [3,5] [3,5,10] [10,15,20] [5,10,15,20] [3,5,10]
Learning rate [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5 [1,2,4]e-5
Warm up 16 16 16 16 16 16 16 16
Seed [1,2,3] [1,2,3] [1,2,3] [1,2,3] [1,2,3] [2,42,123] [2,42,123] [1,2,3]
