# 2210.04995.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2210.04995.pdf
# File size: 1088725 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
FEAMOE: Fair, Explainable and Adaptive Mixture
of Experts
Shubham Sharma
University of Texas at Austin
shubham_sharma@utexas.eduJette Henderson
CognitiveScale
jhenderson@cognitivescale.com
Joydeep Ghosh
University of Texas at Austin
jghosh@utexas.edu
Abstract
Three key properties that are desired of trustworthy machine learning models
deployed in high-stakes environments are fairness, explainability, and an ability to
account for various kinds of "drift". While drifts in model accuracy, for example
due to covariate shift, have been widely investigated, drifts in fairness metrics
over time remain largely unexplored. In this paper, we propose FEAMOE, a
novel "mixture-of-experts" inspired framework aimed at learning fairer, more
explainable/interpretable models that can also rapidly adjust to drifts in both the
accuracy and the fairness of a classiﬁer. We illustrate our framework for three
popular fairness measures and demonstrate how drift can be handled with respect
to these fairness constraints. Experiments on multiple datasets show that our
framework as applied to a mixture of linear experts is able to perform comparably
to neural networks in terms of accuracy while producing fairer models. We then
use the large-scale HMDA dataset and show that while various models trained on
HMDA demonstrate drift with respect to both accuracy and fairness, FEAMOE
can ably handle these drifts with respect to all the considered fairness measures
and maintain model accuracy as well. We also prove that the proposed framework
allows for producing fast Shapley value explanations, which makes computationally
efﬁcient feature attribution based explanations of model decisions readily available
via FEAMOE.
1 Introduction
The ﬁeld of responsible artiﬁcial intelligence has several desiderata that are motivated by regulations
such as the General Data Protected Regulation [ 9]. These include: ensuring that an AI model is
non-discriminatory and transparent; individuals subject to model decisions should have access to
explanations that point a path towards recourse; and models should adapt to any changes in the
characteristics of the data post-deployment so as to maintain their quality and trustworthiness.
Most approaches towards the mitigation of any form of bias assume a static classiﬁer. A practitioner
decides on some deﬁnition of fairness, trains a model that attempts to enforce this notion of fairness
and then deploys the model. Many of the fairness deﬁnitions are based on model outcomes or
on error rates (the gap between true and/or false positive rates) that are associated with different
subgroups speciﬁed by a protected attribute. The goal is to reduce the difference between these error
rates across relevant subgroups. For example, average odds difference [ 4] is a measure signifying
equalized odds and is given by the sum of the differences in both true positive and false positive rates
between two groups, scaled by a factor of 0.5. Equality of opportunity and demographic parity [ 3]
Preprint. Under review.arXiv:2210.04995v1  [cs.LG]  10 Oct 2022

--- PAGE 2 ---
are also popular deﬁnitions of fairness. Recently, fairness in terms of a gap of recourse has been
proposed, where recourse is deﬁned as the ability to obtain a positive outcome from the model [ 34].
While the suitability of a fairness measure is application dependent [ 26,3], demographic parity and
equalized odds remain the most popularly used, and the need for recourse gap-based fairness is being
increasingly recognized [18].
However, static models can encounter drift once deployed, as the statistical properties of real data
often change over time. This can lead to deteriorating performance. Model drift can occur when the
properties of the target variable change (concept drift) or when the input data distribution changes, or
both. The performance of models has largely been measured through accuracy-based metrics such
as misclassiﬁcation rates, F-score or AUC. [ 37]. However, a model trained in the past and found
to be fair at training time may act unfairly for data in the present. Addressing drift with respect to
fairness in addition to accuracy has remained largely unexplored though it is an important aspect of
trustworthy AI in practice.
Explainability of individual model outcomes is another principal concern for trustworthy ML. Among
many methods of explanations in terms of feature attribution, [ 6], the SHAP approach based on Shap-
ley values is particularly popular as it enjoys several axiomatic guarantees [21]. While computation
of SHAP values is fast for linear and tree-based models, it can be very slow for neural networks
and several other model types, especially when the data has a large numbers of features or when a
large number of explanations are required [ 27]. This poses a barrier to deployments that demand fast
explanations in real-time, production settings.
In this paper, we address these fairness, data/model drift, and explainability concerns by proposing
FEAMOE: Fair, Explainable and Adaptive Mixture of Experts, an incrementally grown mixture of
experts (MOE) with fairness constraints. In the standard mixture of experts setup, each expert is
a machine learning model, and so is the gating network. The gating network learns to assign an
input-dependent weight gu(x)to theuthexpert for input x, and the ﬁnal output of the model is a
weighted combination of the outputs of each expert. Hence, each expert contributes differently for
every data point towards the ﬁnal outcome, which is a key difference from standard ensembles.
Many types of MOE’s exist in the literature [ 40] - the architecture is not standard. For FEAMOE, we
chose this family, with some novel modiﬁcations described later, for three main reasons: 1) Suitable
regularization penalties that promote fairness can be readily incorporated into the loss function. 2)
Online learning is possible, so changes in the data can be tracked. Crucially, since localized changes
in data distribution post-deployment may impact only one or a few experts, the other experts may not
need to be adjusted, making the experts localized and only loosely coupled. This allows for handling
drift and avoiding catastrophic forgetting, which is a prime concern in widely used neural network
models [ 31]. 3) Simpler models can be used to ﬁt a more complex problem in the mixture of experts,
as each model needs to ﬁt well in only a limited part of the input space. In particular, even linear
models, which provide very fast SHAP explanations, can be used. The overall mixture of experts,
even with such simple base models (the "experts") often has predictive power that is comparable to a
single complex model such as a neural network, as shown by our experiments as well as in many
previous studies [40].
A motivating toy example of why FEAMOE is needed and how it works is shown in Figure 1.
Consider a linear binary classiﬁer (1a) that has perfect accuracy. The colors represent the ground
truth labels, and green is the positive (desired) class label. The circles are the privileged group and
diamonds are the underprivileged group. As can be seen in the ﬁgure, more diamonds receive a
negative outcome and more circles receive a positive outcome. Consider new data that arrives for
predictions. This classiﬁer (1b) not only misclassiﬁes individuals but also gives more underprivileged
individuals that were actually in the positive class a negative outcome, hence inducing bias with
respect to equalized odds. There is drift with respect to accuracy and fairness. A more complex
model (1c) such as a neural network, if retrained, may handle some of these concerns but would be
less explainable.
FEAMOE can address these imperative concerns, as shown in 1d. Trained in an online manner, a
new linear model is added (i.e., an expert) once the new data arrives. The gating network dictates
which region each expert operates in (shown by the blue and pink colors), and FEAMOE is able
to adapt automatically with respect to accuracy and fairness. This dynamic framework enables the
overall model to be fairer, adjust to drift, maintain accuracy, while also remaining explainable since
the decision boundary is locally linear.
2

--- PAGE 3 ---
(a)
 (b)
 (c)
 (d)
Figure 1: A toy example demonstrating the need and use of FEAMOE. The color of every datapoint
corresponds to the original class label. Diamonds represent the underprivileged group and circles
represent the privileged group. (a) Represents a perfectly accurate linear classiﬁer, (b) represents the
same classiﬁer mis-classifying new data points and inducing bias (drift), (c) represents an alternate
non-linear model that corrects for drift but has a complex decision boundary and (d) represents
FEAMOE where the blue and pink regions show the regions of operation for each of the two experts,
separated by the gating network
We show how three fairness constraints–demographic parity, equalized odds, and burden-based
fairness–can be incorporated into the mixture of experts training procedure in order to encourage
ﬁtting fairer models (according to these measures). We use these three popular fairness measures as
illustrative examples to demonstrate the effectiveness of FEAMOE, but our method can be adapted to
incorporate other fairness constraints as well. We then describe a new algorithm for training to account
for drift, where the drift in question can be due to accuracy or fairness. We show experimentally that
by using a set of logistic regression experts, the accuracy of the mixture is comparable to using a
complex model like a neural network. Additionally, we show we can efﬁciently compute Shapley
value explanations when explanations for every individual expert can be computed quickly. To the
best of our knowledge, this is the ﬁrst work that addresses the problem of drift with respect to fairness
in a large-scale real world dataset. We then introduce a framework that can ﬂexibly adapt to drifts in
both fairness and accuracy with the added beneﬁt of delivering explanations quickly, while comparing
to the less explainable neural network model class trained in online mode.
The key contributions of this work are: a mixture of experts framework that can incorporate multiple
fairness constraints, a method to handle drift, where drift can be with respect to accuracy or fairness,
empirical evidence of the presence of drift with respect to fairness in a real-world, large-scale dataset,
a theoretical proof that FEAMOE leads to the generation of fast explanations given a suitable choice
of experts, and extensive experimentation on three datasets to show that our method has predictive
performance similar to neural networks while being fairer, handling different types of drift, and
generating faster explanations.
2 Related Work
The mixture of experts (MOE) [ 16,39] represent a class of co-operative ensemble models; detailed
surveys on their design and use can be found in [ 40] and [ 25]. Very recently, the deep learning
community has started recognizing and leveraging several advantageous properties that MOE’s have
for efﬁcient design of complex, multi-purpose learners [ 30]. This paper contributes to this expanding
literature by proposing a new algorithm to train this model class to account for both fairness and drift,
and by also adding an explainability module.
Fairness in machine learning is a growing ﬁeld of research [ 14]. Mitigating biases in models can
be done through pre-processing, in-processing, or post-processing techniques. A description of
these techniques can be found in [ 4]. In-processing techniques for fairness have been gaining
traction [ 41,26,34]. However, there is limited work on investigating the usefulness of ensemble
models in dealing with biases. [ 13] show that an ensemble of fair classiﬁers is guaranteed to be
fair for several different measures of fairness, an ensemble of unfair classiﬁers can still achieve
fair outcomes, and an ensemble of classiﬁers can achieve better accuracy-fairness trade-offs than a
single classiﬁer. However, they neither provide experimental evidence nor discuss speciﬁc methods
to incorporate fairness into ensemble learning. [ 24] develop a method to learn to defer in the case
of unfair predictions. [ 5] use an AdaBoost framework to build a fairer model. [ 28] use adaptive
3

--- PAGE 4 ---
random forest classiﬁers to account for fairness in online learning, only considering the statistical
parity deﬁnition of fairness.
Accounting for drift is a widely explored problem, and is now appearing in commercial products
as well (e.g. model monitoring is a key part of MLOPs) as ML solutions get deployed in business
environments. Details on many such approaches can be found in [ 12,20]. Among these approaches,
the one that comes closest to ours is [ 37] which uses a committee of decision trees to account for drift.
However, ensuring fairness in the presence of drift remains an open problem. [ 7] is a very recent
work on achieving a fairer model by building a set of classiﬁers in the presence of prior distribution
shifts. The method is built for a shift between the training and test distributions, and not for online
learning.
There are many ways to explain a machine learning model [ 8,27]. In this paper, we focus on Shapley
values-based explanations, which are widely used in practical applications [ 6]. [22] propose the
computation of Shapley values for tree ensembles, which is a faster way to get Shapley values than
through the more broadly applicable method, KernelShap [ 21]. We show that in FEAMOE, the Shap
values for the overall model are just a data-dependent linear combination of the values from individual
experts. Thus the mixture approach does not add any signiﬁcant complexity to the computation of
feature attribution scores.
3 Theory
We ﬁrst summarize the original mixture of experts framework and then describe the addition of
fairness constraints. Then, we introduce the algorithm to detect and mitigate data drift when the data
input is sequential (online learning). Thereafter, we show how using the proposed mixture of experts
architecture leads to computing faster Shapley value explanations for the overall non-linear model.
Mixture of Experts (MoE) [ 16] is a technique where multiple experts (learners) can be used to
softly divide the problem space into regions. A gating network decides which expert to weigh most
heavily for each input region. Learning thus consists of the following: 1) learning the parameters of
individual learners and 2) learning the parameters of the gating network. Both the gating network
and every expert have access to the input x. The gating network has one output gifor every expert
i. The output vector is the weighted (by the gating network outputs) mean of the expert outputs:
y(x) =Pm
i=1gi(x)yi(x). Consistent with [ 16], the error associated with training the mixture of
experts for case jfor an accurate prediction is given by: Ej
acc= logP
igj
ie 1
2jjdj yj
ijj2,where
yj
iis the output vector of expert ion casej,gj
iis the proportional contribution of expert ito the
combined output vector, and djis the desired output vector.
3.1 Fairness Constraints
In this paper, we incorporate three diverse fairness deﬁnitions into the mixture of experts framework:
demographic parity only depends on the model outcome, equalized odds is conditioned on the
ground-truth label, and burden-based fairness depends on the distance of the input to the boundary.
These three popular deﬁnitions have been chosen as illustrative metrics; our approach can be readily
extended to several other fairness metrics as well.
For simplicity, we consider a binary classiﬁcation setting with a binary protected attribute (our
approach readily extends to multi-class and multi-protected attribute problems, where a protected
attribute is a feature such as race or gender). Let yj
i= 1be the positive outcome. Let A= 0and
A= 1represent the underprivileged and privileged protected attribute groups, respectively. For a
given dataset D, letDadrepresent all individuals that belong to the protected attribute group aand
original class label d.
Statistical parity difference (SPD), which is a measure of demographic parity, measures the difference
between the probability of getting a positive outcome between protected attribute groups [ 4,35].
LetD0be the set of individuals in the underprivileged group and D1be the set of individuals in
the privileged group. Inspired by [ 36], the associated penalty for demographic parity for case j
is:Ej
SPD = 1[j2D0](1 P
igiyj
i) + 1[j2D1](P
igiyj
i):The idea behind this term is that
individuals belonging to the underprivileged group predicted as getting a negative outcome are
4

--- PAGE 5 ---
Algorithm 1 Learning FEAMOE
Inputs: data X, labelsY
Hyperparameters: k,1,2,3
s= 1
# Selectkpointsfxgk
l=1X
X(s)=Xnfxgk
l=1
# Learn the ﬁrst expert, ms
# Initializews, weights of ms
forjinf1;:::;kgdo
# Take gradient steps to minimize MoE loss:
wj
s=wj 1
s rEj
acc
end for
1= 0;2= 0, and3= 0
whileX(s)is not empty do
s+=1
1=1+ 1;2=2+ 2;3=3+ 3
# Selectkpointsfxgk
l=1X(s)
X(s)=X(s)nfxgk
l=1
# Learn the subsequent expert, ms
Initializews
forjinf1;:::;skgdo
# Take gradient steps to minimize Equation 1
forlinf1;::;sgdo
wj
s=wj 1
s rEj
acc 1rESPD 2rEj
AOD 3rEj
Burdenend for
end for
end while
assigned a higher penalty. Similarly, individuals belonging to the privileged group predicted to have a
positive outcome are assigned a higher penalty, thereby encouraging an SPD value closer to zero.
Average odds difference (AOD), which is a measure of equalized odds, measures the difference in
true and false rates between protected attribute groups. Details on the measure can be found in [ 4,35].
The associated penalty for equalized odds is:
Ej
AOD = 1[j2D01](1 P
igiyj
i) + 1[j2D11](1 P
igiyj
i) + 1[j2D10](P
igiyj
i) +
1[j2D00](P
igiyj
i):This term encourages the true and false positive rate gaps between groups to
reduce by conditioning the indicator function on the ground truth label in addition to the protected
attribute (as was in the demographic parity formulation).
Burden for a protected attribute group is a measure of the ability to obtain recourse for individ-
uals in that group. As shown in [ 34], burden-based fairness can be calculated as: Burden =
jExjA=0[d(x;B)] ExjA=1[d(x;B)]j,whered(x;B)represents the distance to the boundary for a
given xthat is classiﬁed as being in the negative class. Then, the associated penalty for burden based
fairness is:Ej
Burden=jExjA=0[d(x;B)] ExjA=1[d(x;B)]j:
The overall loss for case jis then given by:
Ej
MOE =Ej
acc+1Ej
SPD +2Ej
AOD +3Ej
Burden(1)
3.2 Data Drift and the FEAMOE algorithm
Data Drift means that the statistical properties of the data, embodied in the underlying joint distribution
of independent and dependent variables, can change over time, often in unforeseen ways. The change
could be in the class priors, the class conditional distributions (concept drift), in the distribution of
the independent variables etc. Drift can cause the model to become less accurate as time passes.
However, drift can also cause other properties associated with the model to change, such as fairness.
We develop an algorithm that can handle drift with respect to both accuracy and fairness.
5

--- PAGE 6 ---
Consider an online learning setup where input data points are observed sequentially. The algorithm to
learn FEAMOE (Algorithm 1) is as follows: start with a single model. Begin to train with data points
(using stochastic gradient descent) and train the current model for a certain number of data points
kusing onlyEacc(equation 2). After kpoints, introduce a new logistic regression model and train
the mixture of experts with a softmax gating function using the loss in Equation 1. Simultaneously,
introduce the fairness penalties. Then, continue training for the next kpoints, and then add another
expert. As more experts are added, gradually increase the hyperparameters ( ’ values) associated
with the three fairness losses. This process is continued until all the available data is seen.
The motivation behind this training scheme is two-fold: in beginning with the accuracy penalty
for the ﬁrst expert, we ensure that the fairness measures do not interfere with training an accurate
classiﬁer, since high weights on the fairness terms would result in a less accurate classiﬁer (as shown
in experiments). Then, we slowly increase the weights on the fairness penalties with the goal that for
individuals that are classiﬁed unfairly with respect to these group fairness measures, another expert
takes over in this data regime to train for these individuals over time. This is because the mixture of
experts framework allows some or all of the experts to learn on different regions of the data. Secondly,
the algorithm allows us to account for drift, both with respect to the accuracy of the classiﬁer and the
fairness, since our framework allows for fairness constraints. If there is a change in the statistical
properties of the data that impacts any of the loss terms, the mixture of experts adapts to this change
over time through the addition of experts.
3.3 Fast Shapley value explanations
A prominent class of feature attribution methods is based on Shapley values from cooperative game
theory [ 33]. Details about Shapley value explanations can be found in [ 23], [38], and [ 1]. While
computing Shapley values for a linear model is fast, doing so for non-linear models with methods like
KernelShap [ 21] requires approximations and methods that cause the overall computation to become
slow [ 27,1]. Another method, TreeShap, [ 22] works only for tree models. Though the mixture of
experts model proposed is non-linear, as the individual experts are linear, the theorem below shows
how to compute them for the whole model quickly and efﬁciently.
Consider a mixture of experts model with mexperts. Let j(m(x)) be the Shapley value associated
with expertmfor featurejfor an input instance x.
Theorem 1 For a mixture of experts model, the Shapley value for a given instance xand featurej
for the model prediction is given by:
j(y(x)) =mX
i=1gi(x)j(m(x)) (2)
The proof is provided in the appendix. This result shows that the Shapley value for a given feature and
input instance for the mixture of linear experts is a linear combination of the Shapley values of the
feature and that input instance from every expert, weighted by the gating network’s assigned weights
for that input. This means that so long as the Shap values for individual models can be quickly
computed (as is the case for linear/logistic regression, decision trees, XGBoost), the FEAMOE
system-level Shap computation is also very quick. In this paper, we illustrate FEAMOE using logistic
regression experts, so this desirable property holds, even though the mixture model is able to construct
non-linear models of arbitrary complexity by including as many logistic regression-based experts as
needed.
4 Experiments and Results
Experiments are performed using the mixture of experts with logistic regression experts and a
softmax gating function. We implement the logistic regression models using scikit learn with default
parameters. We show that using logistic regression experts within the MOE produces accuracies
similar to using appropriately sized neural networks while allowing for the generation of faster
explanations. All neural networks are multilayer perceptrons. There are two sets of experiments,
highlighting different aspects of FEAMOE:
(a)Fairness Study . We use two classiﬁcation datasets that exhibit bias and, despite having some
6

--- PAGE 7 ---
Table 1: Names of the models compared in ﬁgure 2, based on the class of models (Mixture of Experts
(MOE) or Neural Network (NN)) and type of fairness constraints (None, SPD, AOD, Burden, or all).
Experiments on models marked x are in the appendix
Type None SPD AOD Burden All
MOE MOE FEAMOE1 FEAMOE2 FEAMOE3 FEAMOE
NN NN x x x FairNN
(a)
 (b)
(c)
 (d)
Figure 2: Results for the UCI Adult Dataset on different fairness constraints being incorporated.
Details on model names are provided in Table 1. A higher accuracy is better and a lower bias is better
known issues [ 11], are very well studied in the fairness community: UCI Adult [ 19] and COMPAS
[29]. Note that these datasets are not inherently streaming; so they are being used only for fairness
studies rather than for handling of drifts. Gender is considered as the protected attribute for UCI
Adult. A two layer multilayer perceptron with 30 hidden units in each layer was trained for the
UCI Adult dataset. Additional or larger hidden layers, or ensemble methods such as xgboost do not
provide extra beneﬁt for these two tabular datasets, and hence are omitted for comparison purposes.
Experiments on the COMPAS dataset are in the appendix.
(b)Drift Study . The large HMDA (Home Mortgage Disclosure Act) dataset [15] reﬂects data from
multiple years, with the underlying data statistics varying considerably over the years, so it is suitable
for drift studies. Gender is the protected attribute. A ﬁve layer multilayer perceptron with 50 hidden
units in each layer is trained for the HMDA dataset as the baseline neural network. Experiments
on a synthetic streaming version of the UCI Adult dataset are in the appendix. Data sequencing
for training is described later; for now we mention that batch techniques (e.g. standard tree based
models), including popular ensemble techniques, that make multiple passes over data sampled from
the entire time span, cannot be deployed as we are studying effects of drift over time.
First, we use UCI Adult to demonstrate the effects of incorporating the proposed fairness constraints
on the mixture of experts models. Similar results for the COMPAS dataset are provided in the
appendix. We then show that the HMDA dataset demonstrates drift with respect to both fairness and
accuracy, and that FEAMOE can adapt to such drifts. Comparisons are made to neural networks
(both with and without fairness constraints), which is the state-of-the-art model class for accuracy-
based performance across these datasets in all experiments. Experiments on faster Shapley value
explanations are in the appendix.
7

--- PAGE 8 ---
(a)
 (b)
(c)
 (d)
Figure 3: Comparison of Drift Handling on the HMDA dataset. Lower bias values are better. Results
with error bars (excluded here for readability) are provided in the appendix. 1) Blue: baseline
neural networks (ﬁxed neural network) trained without fairness constraints on a previous year (20XX,
indicated by x-axis; 2016 and 2017 are the "future" years) and not updated with new data; 2) Orange:
(fair and) trainable neural network: Neural networks with fairness constraints incorporated; also
updated with streaming data from the "future" years and 3) Green: FEAMOE, also update with a
single pass on streaming data from the "future" years. Note that several popular models (including
ensembles such as XGBoost) are not being considered as by default they will need to make multiple
passes over the dataset and are really not designed for streaming (single-look) applications.
4.1 Fairness Constraints
Experiments are performed on UCI Adult in seven different regimes based on model types and
fairness constraints. Details of these regimes are in Table 1. We use our training algorithm such that
experts are added every 4000 data points for the UCI Adult dataset. Hyperparameters associated
with the fairness constraints are incremented in levels of 0.02 per expert for the UCI Adult dataset.
The parameters are found using grid search and vary based on dataset size and extent of prevalent
bias (details in appendix). The results are averaged across ﬁve runs. We report the accuracy and the
absolute value of the three fairness measures (for consistency in interpreting results across fairness
measures). We provide comparisons to other methods for bias reduction [10, 34, 2] in the appendix.
The results are shown in Figure 2. The accuracy across different model types remains similar for the
UCI Adult dataset, but using just a neural network with fairness constraints works poorly, as shown in
Figure 2a. As seen in 2b,c,d, the fairness measures also work well even in isolation from each other
That is, in trying to improve based on just one measure, the other measures also improve. In this
regard, the burden-based fairness measure (FEAMOE3) has the best effect; just using burden-based
fairness alone helps signiﬁcantly improve the other fairness measures while maintaining reasonable
accuracy. This behavior agrees with the observations in [ 34]. The fair neural network (FairNN)
performs worse for demographic parity and equalized odds compared to FEAMOE. We hypothesize
that this happens because our learning process slowly induces fairness with every expert, as opposed
to training the whole architecture in one go. Overall, FEAMOE signiﬁcantly reduces all three forms
of biases while maintaining accuracy.
8

--- PAGE 9 ---
4.2 Real-World Drift: The HMDA dataset
We ﬁrst demonstrate that the HMDA dataset exhibits drift across years, and then show FEAMOE’s
effectiveness in handling it. The HMDA dataset has millions of records of individuals spanning
several years. It contains consumer characteristics; the target variable indicates whether a consumer
received a mortgage. While this dataset considered as a whole has been previously shown to exhibit
bias, there is no investigation into how such bias varies across the years. First, to quantify drift in this
dataset in both fairness and accuracy, we trained one neural network per year from 2007 to 2017, each
on 100,000 random samples in that year, and tested each of these networks on data from the years
2016-2017 (Once trained, these models, which we call ﬁxed neural networks, cannot be updated).
The results are shown in Figure 7 by the blue points. In general, the farther the training data is away
from the test year the more the accuracy and fairness measures degrade (i.e., accuracy decreases and
fairness differences increase). Also training a single model on a dataset of the same size but sampled
uniformly over all the previous years does not help either as the data is non-stationary. To the best of
our knowledge, this is so far the most detailed study on fairness drift over time, made on an openly
accessible, real-world, large-scale dataset.
We now study how fairness aware neural networks with online updates compare with FEAMOE in
their ability to handle drift. Note that for our setting, we cannot use certain models such as popular
ensemble approaches (XGBoost etc), that need multiple passes through new data in batch mode after
the initial model is built and deployed. For FEAMOE, we train models for each year separately and
consider each of those as one expert. Then, we add on experts based on a single pass on new data
(from the 2016-2017 years). We compare this to neural networks (with fairness constraints) that are
also trained on each of the past years in batch mode, and then updated online with a single pass on the
new data. The FEAMOE results are shown in Figure 7 by the green points, and the trainable neural
networks results by orange points. FEAMOE is noticeably better at maintaining good accuracy and
keeping lower bias across all bias metrics, pretty much irrespective on how old the original model was,
even when compared to adaptive neural networks. We believe that the loosely coupled architecture
and adaptive model complexity is key to FEAMOE’s success in handling drift. It is also worth noting
that using the FEAMOE architecture provides for much faster Shapley value explanations compared
to the trainable neural network (as shown in experiments in the supplementary material).
4.3 Additional Experiments and Findings
We performed several more experiments to validate the effectiveness of using FEAMOE. We summa-
rize the ﬁndings here; further details are in the appendix. Irrespective of dataset size, using neural
networks as experts instead of logistic regression experts in the mixture had negligible effects on the
accuracy. We also tried variants of the proposed learning method, such as introducing experts based on
individual fairness-based performance saturation instead of adding experts at (predetermined) regular
intervals, achieving good performance. Having more experts with a smaller increment on fairness
constraints was also considered and leads to similar performance as having fewer experts with a tuned
increment on the fairness hyperparameters across datasets. Further details on hyperparameters can
also be found in the appendix. Experiments on fast Shapley value explanations are in the appendix.
Also, experiments on the COMPAS dataset for the fairness study, and on the synthetic streaming
version of the UCI Adult dataset for the drift study can be found in the appendix.
5 Conclusion and Future Work
We propose FEAMOE: a novel mixture of experts architecture and learning framework that can better
maintain the fairness of a model in the face of data drift. We show how three fairness constraints can
be incorporated into this framework. These constraints are illustrative, as one can use alternative
constraints instead. We prove that by using this mixture of experts, Shapley value explanations can
be computed efﬁciently even though the overall model is non-linear. Experiments are performed on
three datasets to demonstrate the various properties and effectiveness of FEAMOE. In particular,
we identiﬁed a large-scale, real-world dataset that induces drift with respect to fairness over time in
non-adaptive models, and show that our framework can adequately address this challenge. Given
the usefulness of the FEAMOE formulation, we would now like to extend it to incorporate other
potential forms of drift, such as those that cause changes in adversarial robustness.
9

--- PAGE 10 ---
References
[1]Kjersti Aas, Martin Jullum, and Anders Løland. 2019. Explaining individual predictions when
features are dependent: More accurate approximations to Shapley values. arXiv preprint
arXiv:1903.10464 (2019).
[2]Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík, John Langford, and Hanna Wallach. 2018.
A reductions approach to fair classiﬁcation. arXiv preprint arXiv:1803.02453 (2018).
[3]Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2018. Fairness and Machine Learning .
fairmlbook.org. http://www.fairmlbook.org .
[4]Rachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde,
Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic,
et al.2018. AI fairness 360: An extensible toolkit for detecting, understanding, and mitigating
unwanted algorithmic bias. arXiv preprint arXiv:1810.01943 (2018).
[5]Dheeraj Bhaskaruni, Hui Hu, and Chao Lan. 2019. Improving Prediction Fairness via Model
Ensemble. In 2019 IEEE 31st International Conference on Tools with Artiﬁcial Intelligence
(ICTAI) . IEEE, 1810–1814.
[6]Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep
Ghosh, Ruchir Puri, José MF Moura, and Peter Eckersley. 2020. Explainable machine learn-
ing in deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and
Transparency . 648–657.
[7]Arpita Biswas and Suvam Mukherjee. 2020. Ensuring Fairness under Prior Probability Shifts.
arXiv preprint arXiv:2005.03474 (2020).
[8]Nadia Burkart and Marco F Huber. 2021. A survey on the explainability of supervised machine
learning. Journal of Artiﬁcial Intelligence Research 70 (2021), 245–317.
[9]Michael Butterworth. 2018. The ICO and artiﬁcial intelligence: The role of fairness in the
GDPR framework. Computer Law & Security Review 34, 2 (2018), 257–268.
[10] Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and
Kush R Varshney. 2017. Optimized pre-processing for discrimination prevention. In Advances
in Neural Information Processing Systems . 3992–4001.
[11] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. 2021. Retiring adult: New
datasets for fair machine learning. Advances in Neural Information Processing Systems 34
(2021).
[12] João Gama, Indr ˙e Žliobait ˙e, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia.
2014. A survey on concept drift adaptation. ACM computing surveys (CSUR) 46, 4 (2014),
1–37.
[13] Nina Grgi ´c-Hla ˇca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian Weller. 2017.
On fairness, diversity and randomness in algorithmic decision making. arXiv preprint
arXiv:1706.10208 (2017).
[14] Philipp Hacker. 2018. Teaching fairness to artiﬁcial intelligence: Existing and novel strategies
against algorithmic discrimination under EU law. Common Market Law Review 55, 4 (2018),
1143–1185.
[15] HMDA. [n. d.]. HMDA dataset. https: // www. consumerfinance. gov/
data-research/ hmda/ historic-data ([n. d.]).
[16] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive
mixtures of local experts. Neural computation 3, 1 (1991), 79–87.
[17] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. 2012. Fairness-aware
classiﬁer with prejudice remover regularizer. In Joint European Conference on Machine Learn-
ing and Knowledge Discovery in Databases . Springer, 35–50.
10

--- PAGE 11 ---
[18] Amir-Hossein Karimi, Gilles Barthe, Bernhard Schölkopf, and Isabel Valera. 2020. A survey
of algorithmic recourse: deﬁnitions, formulations, solutions, and prospects. arXiv preprint
arXiv:2010.04050 (2020).
[19] Ron Kohavi. 1996. Scaling up the accuracy of naive-bayes classiﬁers: A decision-tree hybrid..
InKdd, V ol. 96. Citeseer, 202–207.
[20] Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. 2018. Learning
under concept drift: A review. IEEE Transactions on Knowledge and Data Engineering 31, 12
(2018), 2346–2363.
[21] Scott Lundberg and Su-In Lee. 2017. A uniﬁed approach to interpreting model predictions.
arXiv preprint arXiv:1705.07874 (2017).
[22] Scott M Lundberg, Gabriel G Erion, and Su-In Lee. 2018. Consistent individualized feature
attribution for tree ensembles. arXiv preprint arXiv:1802.03888 (2018).
[23] Scott M Lundberg and Su-In Lee. 2017. A Uniﬁed Approach to Interpreting Model Predictions.
InAdvances in Neural Information Processing Systems 30 (NeurIPS 2017) , I. Guyon, U. V .
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.). Curran
Associates, Inc., 4765–4774.
[24] David Madras, Toniann Pitassi, and Richard Zemel. 2017. Predict responsibly: improving
fairness and accuracy by learning to defer. arXiv preprint arXiv:1711.06664 (2017).
[25] Saeed Masoudnia and Reza Ebrahimpour. 2014. Mixture of experts: a literature survey. Artiﬁcial
Intelligence Review 42, 2 (2014), 275–293.
[26] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2019.
A survey on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635 (2019).
[27] Christoph Molnar. 2019. Interpretable Machine Learning .https://christophm.github.
io/interpretable-ml-book/ .
[28] Wolfgang Nejdl. [n. d.]. FARF: A Fair and Adaptive Random Forests Classiﬁer. In Advances in
Knowledge Discovery and Data Mining: 25th Paciﬁc-Asia Conference, PAKDD 2021, Virtual
Event, May 11–14, 2021, Proceedings, Part II . Springer Nature, 245.
[29] ProPublica. [n. d.]. ProPublica COMPAS. https: // www. propublica. org/ datastore/
dataset/ compas-recidivism-risk-score-data-and-analysis ([n. d.]).
[30] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, An-
dré Susano Pinto, Daniel Keysers, and Neil Houlsby. 2021. Scaling Vision with Sparse Mixture
of Experts. arXiv preprint arXiv:2106.05974 (2021).
[31] Anthony Robins. 1995. Catastrophic forgetting, rehearsal and pseudorehearsal. Connection
Science 7, 2 (1995), 123–146.
[32] Lloyd S Shapley. 1953. Stochastic games. Proceedings of the national academy of sciences 39,
10 (1953), 1095–1100.
[33] Lloyd S Shapley. 1953. A Value for n-Person Games. In Contributions to the Theory of Games
II. 307–317.
[34] Shubham Sharma, Alan H Gee, David Paydarfar, and Joydeep Ghosh. 2020. FaiR-N: Fair and
Robust Neural Networks for Structured Data. arXiv preprint arXiv:2010.06113 (2020).
[35] Shubham Sharma, Yunfeng Zhang, Jesús M Ríos Aliaga, Djallel Bouneffouf, Vinod Muthusamy,
and Kush R Varshney. 2020. Data Augmentation for Discrimination Prevention and Bias
Disambiguation. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society .
358–364.
[36] Dylan Slack, Sorelle A Friedler, and Emile Givental. 2020. Fairness warnings and fair-MAML:
learning fairly with minimal data. In Proceedings of the 2020 Conference on Fairness, Account-
ability, and Transparency . 200–209.
11

--- PAGE 12 ---
[37] Kenneth O Stanley. 2003. Learning concept drift with a committee of decision trees. Informe
técnico: UT-AI-TR-03-302, Department of Computer Sciences, University of Texas at Austin,
USA (2003).
[38] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks.
InProceedings of the 34th International Conference on Machine Learning-Volume 70 (ICML
2017) . Journal of Machine Learning Research, 3319–3328.
[39] Lei Xu, Michael Jordan, and Geoffrey E Hinton. 1994. An alternative model for mixtures of
experts. Advances in neural information processing systems 7 (1994), 633–640.
[40] Seniha Esen Yuksel, Joseph N Wilson, and Paul D Gader. 2012. Twenty years of mixture of
experts. IEEE transactions on neural networks and learning systems 23, 8 (2012), 1177–1193.
[41] Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. 2018. Mitigating unwanted biases
with adversarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and
Society . ACM, 335–340.
12

--- PAGE 13 ---
A Supplementary Material
We present the supplementary material for FEAMOE. The key items provided are:
• Proof of Theorem 1
• Experiments on models marked x in Table 1 of the paper
•Additional details on experiment setups and ﬁnding hyperparameters to reproduce experi-
ments
• COMPAS dataset experiments
• Experiments on synthetic drift using the UCI Adult dataset
• HMDA experiments with error bars
• Experiments on fast Shapley value explanations
• Experiments on comparison to existing fairness mitigation strategies
•Experiments on comparison of accuracy of using neural network experts to logistic regression
experts
• Individual fairness based experiments
• Experiments on using more experts with less increments on the fairness constraints
• Example Shapley value explanations
A.1 Proof of Theorem 1
Consider a mixture of experts model with mexperts. Let j(m(x)) be the Shapley value associated
with expertmfor featurejfor an input instance x.
For a mixture of experts model, the Shapley value for a given instance xand featurejfor the model
prediction is given by:
j(y(x)) =mX
i=1gi(x)j(m(x)) (3)
For any given input x, the prediction for the mixture of experts is simply a weighted combination of
the predictions from individual experts. Using the linearity property of Shapley values [32]:
j(y(x)) =j(g1(x)y1(x) +g2(x)y2(x) +:::+gm(x)ym(x))
=j(g1(x)y1(x)) +j(g2(x)y2(x)) +:::+j(gm(x)ym(x))
=g1(x)j(y1(x)) +g2(x)j(y2(x)) +:::+gm(x)j(ym(x))
=mX
i=1gi(x)j(m(x))(4)
A.2 Experiments on Neural Network Models from Table 1
Results for fairness constraints incorporated independently in neural network models are shown in
Figure 4. As we can see, the fairness constraints work even with neural networks, but when compared
to results provided in the main draft, they do not work as well as with the equivalent FEAMOE
models (FEAMOE1, FEAMOE2, and FEAMOE3 respectively).
A.3 Additional Details on Experiments
To study the effect of changing the number of experts on performance, we vary the number of data
pointskafter which an expert is added for the UCI Adult dataset for a ﬁxed increment in the fairness
associated hyperparameters, and study the effect. For example, to obtain one expert (i.e. simply a
logistic regression model), k=n, wherenis the size of the dataset.
The results are shown in Table 7. When the number of experts is large ( k=100) with a ﬁxed increment
of 0.02 for each fairness measure, the fairness weights start dominating signiﬁcantly with just a few
13

--- PAGE 14 ---
Figure 4: Results for the UCI Adult Dataset on different fairness constraints being incorporated for
neural network models. A higher accuracy is better and a lower fairness value is better
Table 2: Results on varying the number of data points kafter which an expert is added for the UCI
Adult dataset. Here the increase in the weights of fairness penalties per expert added is ﬁxed at 0.02.
Performance k= 100k= 1000k= 4000k= 10000k= 41018
Accuracy 46.34 74.77 79.20 83.1 77.54
SPD 0.002 0.007 0.06 0.14 0.26
AOD 0.002 0.03 0.07 0.24 0.31
Burden 0.01 0.02 0.038 0.25 0.28
experts, causing the model to become inaccurate. With fewer experts for the same increment of each
fairness measure weight, the accuracy starts to improve. For k= 4000 , which represents 11 experts,
the model learned is reasonably accurate and fairer. Decreasing the number of experts further results
in a more accurate classiﬁer to a point, but fairness measures suffer since the weight associated with
them is very low. Finally, just having one logistic regression model with no constraint on fairness ( k
= 41018, which is the total number of data points) results in a poor model in performance, since this
model is just a linear model. This highlights the importance of tuning the number of experts to ﬁnd a
balance between fairness and accuracy for this framework.
Experiments are performed using the mixture of experts with logistic regression experts and a softmax
gating function. We consider three datasets that exhibit bias: UCI Adult [ 19], COMPAS [ 29], and
the HMDA (Home Mortgage Disclosure Act) datasets [ 15]. The UCI Adult dataset pertains to the
ﬁnancial domain, where the goal is to predict a class of income ( >50k is the positive outcome).
Gender is considered as the protected attribute. The COMPAS dataset is a classiﬁcation dataset where
the prediction variable is the likelihood to re-offend. Race is considered as the protected attribute. The
HMDA dataset is used to predict if an individual receives a mortgage or not. Race is considered as
the protected attribute. The dataset exists across several years and has a huge number of data entries
in every year. We use the UCI Adult and COMPAS dataset to demonstrate the effects of incorporating
the proposed fairness constraints on the mixture of experts models. The UCI Adult dataset is used
to show the ability to handle drift when drift with respect to fairness is synthetically induced. The
HMDA dataset is used to show a real world application of our approach towards handling concept
drift. Experiments were done on a standard laptop with an Intel i5 processor and no GPU’s were
used.
14

--- PAGE 15 ---
(a)
 (b)
(c)
 (d)
Figure 5: Results for the COMPAS dataset on different fairness constraints being incorporated.
Details on model names are provided in Table 1. A higher accuracy is better and a lower fairness
measure value is better
The neural networks used to compare with FEAMOE were 2 hidden layer neural networks with 30
neurons in each layer for the UCI Adult and COMPAS datasets, and 5 hidden layer neural networks
with 50 neurons in each layer for the HMDA dataset. The number of experts in FEAMOE depends
on the number of data points considered after which an expert is added. The number of data points,
along with the hyperparameters for every fairness constraint can be found using grid search. Through
extensive experimentation, we found that the number of points after which experts are added should
be around one-tenth of the expected data size if data is sequential or dataset size if the whole dataset is
available. The fairness constraints-based hyperparameters (increments) should not be beyond a factor
of 0.1 otherwise the models have poor accuracy. Also, since the burden constraint works reasonably
well to also help with SPD and AOD, only that constraint can be considered to avoid tuning for four
different hyperparameters. We provide code for a basic notebook implementing mixture of experts
with fairness constraints. All experiments are simple variations of this notebook.
A.4 COMPAS dataset experiments
Compared to the UCI Adult dataset, the COMPAS dataset (results in Figure 5) has more of an
accuracy drop with the inclusion of fairness constraints (Figure 5a), due to an inherent fairness
accuracy trade-off, as has been shown before [ 10,35]. This can be attributed to this dataset having
a prejudice-based bias or label bias as opposed to the UCI Adult dataset which has a sampling-
based bias [ 35]. It can also be observed that for this dataset, there is an inherent trade-off between
demographic parity and equalized odds (Figure 5b,c). In trying to improve just one, the other worsens
(FEAMOE1 and FEAMOE2). The burden-based constraint performs similarly to the effect it had for
the UCI Adult dataset (Figure 5d). Since burden-based fairness depends on the distance of points
to the boundary, a classiﬁer may be able to maintain its accuracy while changing the distance to
points to reduce the recourse gap. This effect can be seen through the COMPAS dataset: the accuracy
does not drop signiﬁcantly when only the burden constraint is used. However, including all three
constraints (FEAMOE) produces a model that is fairer than standard models and reasonably accurate,
thereby highlighting the importance of the inclusion of all three constraints in the objective.
15

--- PAGE 16 ---
(a)
 (b)
(c)
 (d)
Figure 6: Results on the ability to handle drifts of different magnitudes. When a lower percentage of
data points are considered from the new world dataset (which is the low level of augmentation from
[35]), less drift occurs. However, with an increase in data points from the sorted new world dataset
(sorted in decreasing order of how realistic points are to the original dataset), there is more drift, both
with respect to accuracy and fairness. Higher accuracy is better and lower fairness value is better.
Fixed neural network is trained on the original dataset without fairness constraints and tested on the
new world dataset subsets. Trainable neural network is trained on the original dataset with fairness
constraints, and is retrained in online mode with the subsets from the new world dataset
A.5 Experiments on synthetic drift using the UCI Adult dataset
To test FEAMOE on drift that could occur due to a change in the characteristics of groups within a
dataset, we perform experiments on datasets synthetically generated based on the UCI Adult dataset.
We perform experiments on synthetically induced drift to show that our method can handle drifts of
different magnitudes.
To synthetically induce drift, we generate a dataset using the UCI Adult dataset where the binary
protected attribute gender is ﬂipped, as in [ 35]. We call this the new world dataset. This new dataset
has more females belonging that receive an income of greater than 50k per year. To assess the ability
to handle drifts of different magnitudes, we consider sequentially chosen subsets from the new world
dataset where the data is sorted in order of realism as in [ 35]. In [ 35], realism of a data point is
determined by the distance between the data point in the new dataset with cluster centers (found
using k-means) from the original dataset. For example, we ﬁrst consider only the ﬁrst 10% of the
most realistic new world data, which is the 10% augmentation level in [ 35]. The most realistic subset
would induce the least drift (slow drift), and as the distance of the synthetic subsets to the original
dataset increases, the drift is more pronounced. As a baseline, we train a neural network (without
any fairness constraint) on the original UCI Adult dataset. We call this the ﬁxed neural network.
Then, we test this network on data from different subsets of the new world data. In testing this model,
we go from the most to least realistic subsets (i.e. less drift to more drift) and track the accuracy
and the three fairness measures introduced earlier. We compare this model to training our adaptive
mixture of logistic regression experts framework on these subsets. We also compare to training the
16

--- PAGE 17 ---
Table 3: Results on comparing KernelShap with using LinearShap for individual experts and com-
bining them using Theorem 1 (FEAMOE) for 100 instances across three datasets. Two variants of
KernelShap are considered, with varying the number of samples nallowed to form a combination of
features for computation. Time is measured in seconds
Dataset FEAMOE Time KernelShap Time n= 500 KernelShap Time n= 2000
UCI Adult 34 379 941
COMPAS 32 246 805
HMDA 38 314 887
original neural network in online mode with fairness constraints on these subsets i.e. adapting a
neural network with fairness constraints. We call this the trainable neural network.
The results are shown in Figure 6. Both the accuracy and fairness measures remain consistently poor
for the baseline neural network (ﬁxed neural network), and they degrade as subsets from less realistic
portions of the new world data are evaluated, demonstrating that drift with respect to both accuracy
and fairness is possible when test data has drifted from the training data. We need frameworks that
can adapt to this. To implement FEAMOE, we train a logistic regression model with the original
UCI Adult dataset, and then add experts for the new subsets based on Algorithm 1. FEAMOE can
account for different levels of drift. The accuracy of our model remains fairly constant, and the bias
is signiﬁcantly lower compared to the baseline model. Our method is also able to adapt better to
maintain accuracy and all fairness measures, compared to a neural network that can be trained with
the new data points and with fairness constraints (i.e. the trainable neural network).
A.6 HMDA experiments with error bars
We ﬁrst demonstrate that the HMDA dataset exhibits drift across years, and then show FEAMOE’s
effectiveness in handling it. The HMDA dataset has millions of records of individuals spanning
several years. It contains consumer characteristics; the target variable indicates whether a consumer
received a mortgage. While this dataset considered as a whole has been previously shown to exhibit
bias, there is no investigation into how such bias varies across the years. First, to quantify drift in this
dataset in both fairness and accuracy, we trained one neural network per year from 2007 to 2017, each
on 100,000 random samples in that year, and tested each of these networks on data from the years
2016-2017 (Once trained, these models, which we call ﬁxed neural networks, cannot be updated).
The results are shown in Figure 7 by the blue points. In general, the farther the training data is away
from the test year the more the accuracy and fairness measures degrade (i.e., accuracy decreases and
fairness differences increase). Also training a single model on a dataset of the same size but sampled
uniformly over all the previous years does not help either as the data is non-stationary. To the best of
our knowledge, this is so far the most detailed study on fairness drift over time, made on an openly
accessible, real-world, large-scale dataset.
We now study how fairness aware neural networks with online updates compare with FEAMOE in
their ability to handle drift. Note that for our setting, we cannot use certain models such as popular
ensemble approaches (XGBoost etc), that need multiple passes through new data in batch mode after
the initial model is built and deployed. For FEAMOE, we train models for each year separately
and consider each of those as one expert. Then, we add on experts based on a single pass on new
data (from the 2016-2017 years). We compare this to neural networks (with fairness constraints)
that are also trained on each of the past years in batch mode, and then updated online with a single
pass on the new data. The FEAMOE results are shown in Figure 7 by the green points, and the
trainable neural networks results by orange points. FEAMOE is noticeably better at maintaining
good accuracy and keeping lower bias across all bias metrics, pretty much irrespective on how old the
original model was, even when compared to adaptive neural networks. We believe that the loosely
coupled architecture and adaptive model complexity is key to FEAMOE’s success in handling drift.
It is also worth noting that using the FEAMOE architecture provides for much faster Shapley value
explanations compared to the trainable neural network (as shown in experiments later).
17

--- PAGE 18 ---
(a)
 (b)
(c)
 (d)
Figure 7: Comparison of Drift Handling on the HMDA dataset. Lower bias values are better. Results
with error bars included. 1) Blue: baseline neural networks (ﬁxed neural network) trained without
fairness constraints on a previous year (20XX, indicated by x-axis; 2016 and 2017 are the "future"
years) and not updated with new data; 2) Orange: (fair and) trainable neural network: Neural networks
with fairness constraints incorporated; also updated with streaming data from the "future" years and
3) Green: FEAMOE, also update with a single pass on streaming data from the "future" years. Note
that several popular models (including ensembles such as XGBoost) are not being considered as
by default they will need to make multiple passes over the dataset and are really not designed for
streaming (single-look) applications.
A.7 Shapley Value Explanations
As an added beneﬁt, by using a mixture of experts with logistic regression experts, we can generate
exact explanations in a computationally efﬁcient manner. We generate Shap values for each inde-
pendent expert in FEAMOE using LinearShap from the SHAP explainer [ 21] and then calculate the
ﬁnal Shapley value using Theorem 1. We compare FEAMOE’s explanation computation time to
KernelShap. We do this for 100 instances across the three datasets. We compare the two methods
based on the time taken across the 100 samples. Table 3 shows the results. As we can see, KernelShap
takes signiﬁcantly more time than using FEAMOE to compute the ﬁnal Shapley values. When the
allowed feature combinations are increased towards the computation of the KernelShap approximation
(i.e.nis increased to produce explanations closer to exact Shapley values), it takes even more time.
Hence, our method generates explanations much faster and can scale better for large numbers of
features and observations. This is particularly useful in a production setting that requires a huge
number of explanations quickly.
A.8 Comparison to other fairness mitigation methods
We compare using the fairness constraints used in FEAMOE to existing state of the art bias mitigation
methods for neural networks for the UCI Adult Dataset. The results are shown in Table 4. As we can
see, FEAMOE produces accurate and fair models across all bias measures. Other methods optimize
on only one or two of the measures, but we still compare across all measures to validate that the
constraints introduced in FEAMOE can sufﬁciently reduce bias and while maintaining accuracy.
18

--- PAGE 19 ---
Method Accuracy SPD AOD Burden
NN 82.02 0.19 0.29 0.29
FEAMOE 83.24 0.06 0.07 0.03
FaiR-N-4 83.75 0.12 0.11 0.02
P.R.y78.14 0.09 0.14 0.32
O.P.P.y77.62 0.10 0.08 0.16
DataAugy78.57 0.07 0.06 0.11
RedAppy81.98 0.10 0.08 0.21
Table 4: Comparison of models (averaged across ﬁve runs) trained on the Adult dataset. NN is a
neural network trained without any fairness constraints, FaiR-N-4 is the model from [ 34],y: prejudice
remover (P.R.) [17], optimized pre-processing (O.P.P.) [10], DataAug [35], and RedApp [2]
A.9 Using neural network experts in FEAMOE
Instead of using logistic regression experts, neural network experts can also be used, with the
disadvantage of not being able to get fast explanations. However, we compare using neural network
experts with using logistic regression experts to see if there is a difference in performance for the
HMDA dataset for data from the year 2017.
The results are shown in Table 6. The table shows how across accuracy and fairness constraints,
neural network experts and logistic regression perform comparably. While the number of logistic
regression experts needed is more than the number of neural network experts needed for similar
performance, using logistic regression models comes with the advantage of providing explainability.
Performance NN LR (FEAMOE)
Accuracy 84.21 84.08
SPD 0.08 0.07
AOD 0.078 0.081
Burden 0.08 0.08
Number of Experts 8 13
Table 5: Results on using neural network experts and compared to using logistic regression experts
A.10 Experiment on using individual fairness
To use a performance saturation based expert growing mechanism instead of growing based on the
number of data points, we tested for individual fairness and only added an expert when a certain
number of data points ndid not satisfy individual fairness. Also, we add a constant penalty to the
original mixture of experts accuracy loss such that it is 0 when the point is individually fair and is 0.4
(can be changed) when the point is not individually fair. We tested individual fairness by ﬂipping
the protected attribute and evaluating the model: if the ﬂipped data point has the same prediction as
the original data point, this input data point satisﬁes individual fairness, otherwise it does not. When
npoints do not satisfy individual fairness, we add another expert. (Note that this is a naive method
to check for individual fairness, however any other mechanism can be adopted to test for individual
fairness).
The results on accuracy and individual fairness for FEAMOE for the COMPAS dataset are shown in
Table 6, where individual fairness is measured as the fraction of points that do not satisfy individual
fairness in the dataset and compared to a neural network. As we can see, FEAMOE is still reasonably
accurate, while ensuring that the classiﬁer is more individually fair.
Performance NN FEAMOE
Accuracy 66.18 64.13
Individual Fairness 0.38 0.09
Table 6: Results on using individual fairness in FEAMOE
19

--- PAGE 20 ---
(a)
 (b)
Figure 8: Shapley value explanations for two different data points using the mixture of experts model
by ﬁnding independent Shapley values and then using Theorem 1. KernelShap for a neural network
model by using all feature combinations showed similar results, while taking much longer to compute.
A.11 Including more experts
We show that using more experts (i.e. adding each expert after less number of data points) but with
smaller changes in fairness hyperparameters (increments to 1,2, and3). For the COMPAS
dataset, we add experts after every 40 data points with a fairness increment of 0.002 with every expert
added. The results are in Table 7. As we can see, the results are comparable to the COMPAS dataset
results in the main draft, showing that we can have more experts with smaller fairness constraint
increments to produce a similar FEAMOE model.
Performance k= 40
Accuracy 63.77
SPD 0.05
AOD 0.13
Burden 0.05
Table 7: Results on the COMPAS dataset with k= 40 and fairness increments of 0.002
A.12 Example Shapley value explanations
Some example Shapley value explanations based decision plots for individual test points for the UCI
Adult dataset are shown in Figure 8. We ﬁnd independent Shapley values for data points for every
expert using the setup provided in https://slundberg.github.io/shap/notebooks/plots/decision_plot.html
and then use Theorem 1 to ﬁnd the ﬁnal Shapley values. KernelShap used on neural networks for the
same data points had similar results, while taking signiﬁcantly longer to compute
20
