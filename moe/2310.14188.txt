# 2310.14188.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2310.14188.pdf
# File size: 952433 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Huy Nguyen1Pedram Akbarian2TrungTin Nguyen3 4Nhat Ho1
Abstract
Mixture-of-experts (MoE) model incorporates the
power of multiple submodels via gating functions
to achieve greater performance in numerous re-
gression and classification applications. From
a theoretical perspective, while there have been
previous attempts to comprehend the behavior of
that model under the regression settings through
the convergence analysis of maximum likelihood
estimation in the Gaussian MoE model, such anal-
ysis under the setting of a classification problem
has remained missing in the literature. We close
this gap by establishing the convergence rates
of density estimation and parameter estimation
in the softmax gating multinomial logistic MoE
model. Notably, when part of the expert param-
eters vanish, these rates are shown to be slower
than polynomial rates owing to an inherent in-
teraction between the softmax gating and expert
functions via partial differential equations. To ad-
dress this issue, we propose using a novel class
of modified softmax gating functions which trans-
form the input before delivering them to the gat-
ing functions. As a result, the previous interaction
disappears and the parameter estimation rates are
significantly improved.
1. Introduction
Mixture of experts (MoE) (Jacobs et al., 1991; Jordan &
Jacobs, 1994) is a statistical machine learning model which
aggregates several submodels represented by expert net-
works associated with standard softmax gating functions.
Due to its modular and flexible structure, there has been
a surge of interests in leveraging the MoE model in vari-
ous fields, including large language models (Shazeer et al.,
1Department of Statistics and Data Sciences2Department of
Electrical and Computer Engineering, The University of Texas at
Austin3School of Mathematics and Physics, The University of
Queensland4Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP,
LJK. Correspondence to: Huy Nguyen <huynm@utexas.edu>.
Proceedings of the 41stInternational Conference on Machine
Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).2017; Lepikhin et al., 2021; Zhou et al., 2022; Du et al.,
2022; Fedus et al., 2022; Zhou et al., 2023; Do et al., 2023),
computer vision (Deleforge et al., 2015; Riquelme et al.,
2021; Dosovitskiy et al., 2021; Bao et al., 2022; Liang et al.,
2022), speech recognition (Peng et al., 1996; Gulati et al.,
2020; You et al., 2022), reinforcement learning (Ren et al.,
2021; Chow et al., 2023) and multi-task learning (Hazimeh
et al., 2021; Gupta et al., 2022; Chen et al., 2023). De-
spite its recent progress in the aforementioned applications,
the theoretical comprehension of the MoE model has been
found relatively restricted in the literature.
In general, that model can be formulated as either a re-
gression problem, namely when the distribution of MoE
outputs is continuous, or a classification problem, i.e., when
the MoE outputs follow a discrete distribution. Under the
setting of a regression problem, there are some previous
works attempting to theoretically understand the MoE model
through the convergence analysis of maximum likelihood
estimation (MLE) by focusing on Gaussian MoE equipped
with different types of gating functions. First, (Ho et al.,
2022) established the convergence rates of parameter estima-
tion in the covariate-free gating Gaussian MoE model using
the generalized Wasserstein loss function (Villani, 2003;
2008). In that paper, they discovered that those parameter
estimation rates were significantly slow owing to an interac-
tion between expert parameters via some partial differential
equations (PDEs). Next, (Nguyen et al., 2024c) considered
the Gaussian density gating Gaussian MoE model. Since
that gating function depended on the covariates, there was
another interaction between the parameters of the gating
function and the expert function. To this end, the authors
proposed V oronoi loss functions to capture those interac-
tions and demonstrate that the convergence rates of parame-
ter estimation were determined by the solvability of a system
of polynomial equations. Subsequently, the Gaussian MoE
models with softmax gating function and top-K sparse soft-
max gating function were investigated in (Nguyen et al.,
2023) and (Nguyen et al., 2024b), respectively. Due to the
non-linearity and sophisticated structures of those gating
functions, the parameter estimation rates were shown to
vary with the solvability of a more complex system of poly-
nomial equations than that in (Nguyen et al., 2024c), and
became substantially slow when the number of fitted experts
increased.
1arXiv:2310.14188v2  [stat.ML]  24 Jun 2024

--- PAGE 2 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
On the other hand, such theoretical analysis of the MoE
model under the setting of a classification problem has re-
mained poorly understood, to the best of our knowledge.
Therefore, the main objective of our paper is to provide new
insights on the convergence behavior of MLE in the softmax
gating multinomial logistic MoE model (Chen et al., 1999;
Yuksel & Gader, 2010; Huynh & Chamroukhi, 2019; Pham
& Chamroukhi, 2022). Before going into further details, it
is necessary to introduce the formulation of that model and
associated assumptions.
Problem Setup. In this paper, we assume that the output
Y∈ {1,2, . . . , K }is a discrete response variable, where
K∈N, while X∈ X is an covariate vector having an effect
onY, in which Xis a compact subset of Rd. Next, the data
points (X1, Y1),(X2, Y2), . . . , (Xn, Yn)are independently
drawn from the standard softmax gating multinomial logistic
mixture of experts of order k∗, which admits the conditional
probability function gG∗(Y=s|X)defined for any s∈
{1,2, . . . , K }as follows:
k∗X
i=1Softmax(( β∗
1i)⊤X+β∗
0i)×f(Y=s|X;a∗
i, b∗
i)
:=k∗X
i=1exp(( β∗
1i)⊤X+β∗
0i)
Pk∗
j=1exp(( β∗
1j)⊤X+β∗
0j)
×exp(a∗
is+ (b∗
is)⊤X)PK
ℓ=1exp(a∗
iℓ+ (b∗
iℓ)⊤X). (1)
Here, each expert f(·|X;a∗
i, b∗
i)is a multinomial logistic
regression with parameters a∗
i:= (a∗
i1, . . . , a∗
iK)∈RKand
b∗
i:= (b∗
i1, . . . , b∗
iK)∈Rd×K. Meanwhile, β∗
1i∈Rdand
β∗
0i∈Rare referred to as gating parameters. Additionally,
G∗:=Pk∗
i=1exp(β∗
0i)δ(β∗
1i,a∗
i,b∗
i)denotes a true yet un-
known mixing measure , that is, a combination of Dirac mea-
sures δassociated with true parameters (β∗
0i, β∗
1i, a∗
i, b∗
i)∈
Θ, where Θis a bounded subset of R×Rd×RK×Rd×K.
Lastly, we define for any vector v= (v1, . . . , v k∗)∈Rk∗
thatSoftmax( vi) := exp( vi)/Pk∗
j=1exp(vj).
It is worth noting that if we translate β∗
1itoβ∗
1i+t1and
β∗
0itoβ∗
0i+t0, then the values of the standard softmax
gating function do not change. This implies that gating
parameters β∗
1i, β∗
0iare only identifiable up to some trans-
lation. To alleviate this problem, we assume without loss
of generality (WLOG) that β∗
1k∗=0dandβ∗
0k∗= 0. Sim-
ilarly, we also assume that a∗
iK= 0 andb∗
iK=0dfor
anyi∈ {1,2, . . . , k ∗}. Furthermore, at least one among
β∗
11, β∗
12, . . . , β∗
1k∗must be non-zero so that the softmax gat-
ing function depends on the covariate X. Lastly, we let
(a∗
i, b∗
i), fori∈ {1,2, . . . , k ∗}, be pairwise distinct to en-
sure that the epxert functions are different from each other.
Maximum Likelihood Estimation. Regarding the parame-
ter estimation problem in the standard softmax gating multi-nomial logistic MoE, we propose using the maximum like-
lihood method in this work. However, as the true number
of experts k∗is generally unknown in practice, it is neces-
sary to consider an over-specified setting where we fit the
true model with a mixture of kmultinomial logistic experts,
where k > k ∗. In particular, the maximum likelihood es-
timation (MLE) of the true mixing measure G∗is given
by:
bGn∈arg max
G∈Ok(Θ)nX
i=1log(gG(Yi|Xi)), (2)
where Ok(Θ) denotes the set of all mixing mea-
sures with at most kcomponents of the form G=Pk′
i=1exp(β0i)δ(β1i,ai,bi), in which 1≤k′≤kand
(β0i, β1i, ai, bi)∈Θ.
Main Challenges. To characterize the parameter estimation
rates, we need to decompose the difference gbGn(Y|X)−
gG∗(Y|X)into a linear combination of linearly independent
terms using Taylor expansions. Then, when the density esti-
mation gbGn(Y|X)converges to the true density gG∗(Y|X),
all the associated coefficients, which involve the discrep-
ancies between true parameters and their estimations, also
tend to zero. Consequently, we achieve our desired pa-
rameter estimation rates based on the density estimation
rate. However, when part of the expert parameters van-
ishes, there is an interaction between the numerator of the
standard softmax gating function and the expert function,
which induces several challenges in the density decomposi-
tion. In particular, let us denote u(Y=s|X;β1i, ai, bi) :=
exp(( β1i)⊤X)f(Y=s|X;ai, bi). If there exists an index
i∈[k∗]such that b∗
iℓ=0dfor any ℓ∈[K], then the
aforementioned interaction is expressed via the following
PDE:
∂u
∂β1i(Y=s|X;β∗
1i, a∗
i, b∗
i)
=Ca∗
i·∂u
∂bis(Y=s|X;β∗
1i, a∗
i, b∗
i),(3)
where Ca∗
i>0is a constant depending only on a∗
i. The
above PDE shows that there are a number of linearly de-
pendent derivative terms in the Taylor expansion. Then,
we have to incorporate these terms by taking the summa-
tion of their associated coefficients in order to form a linear
combination of linearly independent terms. Therefore, the
structure of resulting coefficients becomes complex, which
makes the parameter estimation rates slower than polyno-
mial rates. This finding indicates that the standard softmax
gating function might hurt the performance of the model
in equation (1)despite its widespread use in the literature
(Jacobs et al., 1991; Jordan & Jacobs, 1994; Nguyen et al.,
2023).
Contribution. Following the above challenge discussion,
we construct a generic V oronoi-based metric Dramong
2

--- PAGE 3 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Table 1. Summary of density estimation rates and parameter estimation rates in the multinomial logistic MoE model with standard and
modified softmax gating functions. Here, we refer to exact-specified parameters β∗
1j, a∗
j, b∗
jas those fitted by exactly one component (i.e.
|Cj|= 1), while over-specified parameters are approximated by more than one component (i.e. |Cj|>1), where Cjis a V oronoi cell
defined in Section 2.
GatingParameter
SettingDensity Exact-specified Parameters Over-specified Parameters
Standard
SoftmaxRegime 1 eO(n−1/2) eO(n−1/2) eO(n−1/4)
Regime 2 eO(n−1/2) slower than eO(n−1/2r)for any r≥1
Modified
SoftmaxAny regime eO(n−1/2) eO(n−1/2) eO(n−1/4)
parameters in equation (5)to capture the convergence be-
havior of the MLE bGnto the true mixing measure G∗in the
standard softmax gating multinomial logistic MoE model.
Moreover, we also design a novel class of modified softmax
gating functions which improves the convergence rate of
the MLE. Our contributions are two-fold and can be sum-
marized as follows (see also Table 1):
1. Standard Softmax Gating Function. With the standard
softmax gating function, we demonstrate that the density
estimation gbGnconverges to the true density gG∗under
the Hellinger distance hat the parametric rate of order
eO(n−1/2). Next, we consider the parameter estimation
problem under two following complement regimes of the
expert parameters b∗
i:
(i) Regime 1: For any i∈ {1,2, . . . , k ∗}, we can find an
index ℓ∈[K−1]that satisfies b∗
iℓ̸=0d. Under this
regime, the PDE in equation (3)does not hold and there is
no interaction between the standard softmax gating and the
expert functions. By deriving the Hellinger lower bound
EX[h(gG(·|X), gG∗(·|X))]≳D2(G, G∗)for any mixing
measure G∈ O k(Θ)in Theorem 3.1, we obtain that the
rates for estimating over-specified parameters β∗
1i,a∗
iandb∗
i,
i.e. those whose V oronoi cells have more than one element,
are identical of order eO(n−1/4). At the same time, the
estimation rates for their exact-specified parameters, namely
those whose V oronoi cells have exactly one element, are
substantially faster of order eO(n−1/2).
(ii) Regime 2: There exists an index i∈ {1,2, . . . , k ∗}such
thatb∗
iℓ=0dfor any ℓ∈[K−1]. Since the PDE (3)holds
true under this regime, an interaction between the standard
softmax gating and the expert functions occurs. Then, we
demonstrate in Theorem 3.3 that the minimax lower bound
infGn∈Ok(Θ)supG∈Ok(Θ)\Ok∗−1(Θ)EpG[Dr(Gn, G)]≳
n−1/2holds for any r≥1. This bound together with the
formulation of Drindicate that true parameters β∗
1i,a∗
iand
b∗
ienjoy much worse rates than those in Regime 1. Remark-
ably, those for over-specified parameters are even slower
than polynomial rates.2. Modified Softmax Gating Function. To resolve the
aforementioned downsides of the standard softmax gating
function towards the convergence rates of parameter esti-
mation, we propose using the following modified softmax
gating functions:
Softmax(( β∗
1i)⊤M(X) +β∗
0i)
:=exp(( β∗
1i)⊤M(X) +β∗
0i)
Pk∗
j=1exp(( β∗
1j)⊤M(X) +β∗
0j),
for any i∈ {1,2, . . . , k ∗}where M:Rd→Rdis a
bounded function such that the setn
Xp[M(X)]q:p, q∈
Nd,0≤ |p|+|q| ≤2o
is linearly independent for almost
surely X. These assumptions guarantee that the PDE in
equation (3)does not hold for any values of expert parame-
tersb∗
i, and therefore, the previous interaction between the
gating and expert functions disappears. As a consequence,
we show in Theorem 4.4 that the parameter estimations un-
der the modified softmax gating multinomial logistic MoE
model share the same convergence behavior as those in
Regime 1 regardless of the values of expert parameters b∗
i.
Practical Implications. Below are three practical implica-
tions from our convergence analysis of the softmax gating
multinomial logistic MoE:
1. Expert parameter collapse: It is worth noting that
the parameter estimation rates when the expert parame-
ters collapse are significantly slow, and could be of order
O(1/log(n))(see Table 1). Therefore, during the training
process, if ones observe that the model convergence be-
comes abnormally slow, or the updated loss values almost
remain unchanged, then it is highly likely that the expert
parameter collapse occurs.
2. Model design: Despite the widespread use of the stan-
dard softmax gate in the practical applications of MoE mod-
els, the insights from our theory indicates that this gate is not
always beneficial for the model performance, particularly
when the expert parameter collapse happens. Therefore,
3

--- PAGE 4 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
our analysis suggests using a novel modified softmax gate
which helps stabilize the training process regardless of the
parameter collapse.
3. Expert selection: The practical problem of selecting
important experts can benefit from the modified softmax
gate. In particular, in many applications of MoE models,
there are some experts which do not play an essential role
in learning, and even become redundant. In such scenario,
we would like the gate to put more weights on the important
experts. However, if the input magnitude is huge, then the
weight distribution will become uniform, which is undesir-
able. To address this issue, practitioners can use an input
normalization function, i.e. M(X) =X
||X||, in the softmax
weights so that the input magnitude remains unchanged of
one. Another possible option is M(X) =sigmoid (X),
which allows the magnitude of the input to vary between 0
and 1.
Outline. The remainder of the paper is organized in the fol-
lowing way. In Section 2, we verify the identiability of the
standard softmax gating multinomial logistic MoE model
and establish the parametric density estimation rate under
that model prior to introducing a novel class of V oronoi loss
functions used for the parameter estimation problem. Sub-
sequently, we characterize the parameter estimation rates
under the multinomial logistic MoE model equipped with
the standard softmax gating function and the modified soft-
max gating function in Section 3 and Section 4, respectively.
Finally, we conclude the paper in Section 5. Meanwhile,
full proofs, additional results and a simulation study are
relegated to the Appendices.
Notations. First, we let [n]stand for the set {1,2, . . . , n }
for any number n∈N. Next, for any vector u, v∈Nd,
we denote |v|:=v1+v2+. . .+vd,vu:=vu1
1vu2
2. . . vud
d
andv! :=v1!v2!. . . v d!, whereas ∥v∥represents for its 2-
norm value. Meanwhile, for any set S, we denote |S|as its
cardinality. Additionally, for any two probability density
functions p, qdominated by the Lebesgue measure µ, we de-
fineV(p, q) :=1
2R
|p−q|dµas the Total Variation distance
between them, and h(p, q) :=
1
2R
|√p−√q|2dµ1/2
as
their Hellinger distance. Lastly, for any two sequences (sn)
and(tn)inR+, the notations sn=O(tn)andsn≲tn
means there exists some constant C > 0independent of
nsuch that sn≤Ctnfor sufficiently large n∈N, while
the notation sn=eO(tn)suggests that the aforementioned
inequality might hold up to some logarithmic function of n.
2. Preliminaries
In this section, we study the identifiability of the standard
softmax gating multinomial logistic MoE model and the con-
vergence behavior of density estimation under that model.
Then, we further highlight the necessity for V oronoi-basedloss functions to determine parameter estimation rates accu-
rately.
Firstly, we show that the standard softmax gating multino-
mial logistic MoE model is identifiable.
Proposition 2.1 (Identifiability) .Given two mixing mea-
sures GandG′inOk(Θ), ifgG(Y|X) =gG′(Y|X)holds
true for almost surely (X, Y), then G≡G′.
The proof of Proposition 2.1 is in Appendix C.1. This result
ensures the convergence of the MLE bGnto the true mixing
measure G∗as long as the conditional density gbGncon-
verges to gG∗for almost surely (X, Y). Next, we character-
ize the density estimation rate under the Hellinger distance
in the following proposition:
Proposition 2.2 (Density Estimation Rate) .With the MLE
bGndefined in equation (2), the convergence rate of the
density estimation gbGnto the true density gG∗is given by:
P
EX[h(gbGn(·|X), gG∗(·|X))]> C 0p
log(n)/n
≲exp(−c0log(n)),(4)
where C0andc0are universal positive constants that de-
pend only on Θ.
The proof of Proposition 2.2 is in Appendix B.1. It is evident
from the bound in equation (4)that the rate for estimating
gG∗is parametric of order eO(n−1/2). Therefore, if we are
able to construct a metric Damong parameters that satis-
fies the inequality EX[h(gG(·|X), gG∗(·|X))]≳D(G, G∗)
for any mixing measure G∈ O k(Θ), then the MLE bGn
will also converge to the true mixing measure G∗at the
parametric rate of order eO(n−1/2).
Voronoi-based Loss: As the MLE bGnis constrained to
have more components than its true counterpart, there ex-
ists a true component approximated by at least two com-
ponents while others could be fitted by exactly one com-
ponent. This possibly leads to different estimation rates
among the true parameters, which requires us to design
novel loss functions tailored to this observation. To this
end, let us recall a notion of V oronoi cells, which was
previously studied in (Manole & Ho, 2022). In partic-
ular, for any mixing measure G∈ O k(Θ), a V oronoi
cell of Ggenerated by θ∗
j:= (β∗
1j, a∗
j, b∗
j)is defined as
Cj≡ Cj(G) :={i∈[k] :∥θi−θ∗
j∥ ≤ ∥ θi−θ∗
ℓ∥,∀ℓ̸=j}
for any j∈[k∗], where θi:= (β1i, ai, bi). Here, the cardi-
nality of each V oronoi cell Cjindicates the number of fitted
components approximating the true component θ∗
j. Based
on these V oronoi cells, we define a generic V oronoi-based
metric of order r≥1between any two mixing measures,
4

--- PAGE 5 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
denoted by Dr(G, G∗), as follows:
Dr(G, G∗) :=k∗X
j=1X
i∈Cjexp(β0i)−exp(β∗
0j),
+X
j:|Cj|>1,
i∈Cjexp(β0i)h
∥∆β1ij∥r+K−1X
ℓ=1(|∆aijℓ|r+∥∆bijℓ∥r)i
+X
j:|Cj|=1,
i∈Cjexp(β0i)h
∥∆β1ij∥+K−1X
ℓ=1(|∆aijℓ|+∥∆bijℓ∥)i
,
(5)
where ∆β1ij:=β1i−β∗
1j,∆aijℓ:=aiℓ−a∗
jℓand
∆bijℓ:=biℓ−b∗
jℓ. The above V oronoi loss function allows
us to capture precisely the distinct convergence behaviors of
parameter estimations under the multinomial logistic MoE
model with (modified) softmax gating function.
3. Standard Softmax Gating Multinomial
Logistic MoE
In this section, we present the convergence analysis of pa-
rameter estimation in the standard softmax gating multino-
mial logistic MoE model.
Given the parametric density estimation rate in Proposi-
tion 2.2, we observe that if the Hellinger lower bound
EX[h(gbGn(·|X), gG∗(·|X))]≳Dr(bGn, G∗),
holds true for some r≥1, then the convergence rate of
the MLE bGnto the true mixing measure G∗would also be
parametric of order eO(n−1/2). Therefore, our goal is to
establish the above bound. Since the Hellinger distance is
lower bounded by the Total Variation distance, i.e., h≥V,
it is sufficient to show that
EX[V(gbGn(·|X), gG∗(·|X))]≳Dr(bGn, G∗).(6)
Then, we consider the term V(gbGn(·|X), gG∗(·|X))which
involves the density discrepancy gbGn(Y|X)−gG∗(Y|X).
We aim to decompose this discrepancy into a linear combina-
tion of linearly independent terms using Taylor expansion so
that when gbGnconverges to gG∗, the associated coefficients
involving the parameter differences bβn
1i−β∗
1j,ban
i−a∗
jand
bbn
i−b∗
jalso tend to zero, where (bβn
1i,ban
i,bbn
i)are compo-
nents of bGn. For that purpose, we first rewrite the previous
density discrepancy in terms of
u(Y|X;bβn
1i,ban
i,bbn
i)−u(Y|X;β∗
1j, a∗
j, b∗
j),
where u(Y|X;β1i, ai, bi) = exp(( β1i)⊤X)f(Y|X;ai, bi).
Next, we apply the Taylor expansion to the functionu(Y|X;bβn
1i,ban
i,bbn
i)about the point (β∗
1j, a∗
j, b∗
j). In this
step, we notice that if there exists j∈[k∗]such that
b∗
jℓ=0dfor any ℓ∈[K−1], then the gating parame-
ters interact with the expert parameters via the following
PDEs:
∂u
∂β1j(Y=s|X;β∗
1j, a∗
j, b∗
j)
=Ca∗
j·∂u
∂bjs(Y=s|X;β∗
1j, a∗
j, b∗
j),(7)
for any s∈[K], where Ca∗
j>0is some constant depending
only on a∗
j. This interaction indicates that there are several
linearly dependent derivative terms in the previous Taylor
expansion, which induces a number of challenges in rep-
resenting the density discrepancy gbGn(Y|X)−gG∗(Y|X)
as a linear combination of linearly independent terms. On
the other hand, if for any j∈[k∗], we can find an index
ℓ∈[K−1]such that b∗
jℓ̸=0d, then the PDEs inequa-
tion (7)no longer hold true and the previous interaction
does not occur, which facilitates the decomposition of the
density discrepancy.
For those reasons, we will characterize the parameter esti-
mation rates under two following complement regimes of
expert parameters b∗
jin Section 3.1 and Section 3.2, respec-
tively:
•Regime 1: For any j∈[k∗], there exists an index
ℓ∈[K−1]such that b∗
jℓ̸=0d;
•Regime 2: There exists an index j∈[k∗]such that
b∗
jℓ=0dfor any ℓ∈[K−1].
3.1. Regime 1 of Expert Parameters
Under this regime, it can be verified that the PDEs in equa-
tion (7)are no longer valid for any j∈[k∗]. Then, we
provide in the following theorem the convergence rate of
the MLE bGnto its true counterpart G∗under the V oronoi
loss function D2in equation (5).
Theorem 3.1 (Parameter Estimation Rate) .Suppose that
the assumption of Regime 1 holds true, then we achieve the
Hellinger lower bound
EX[h(gG(·|X), gG∗(·|X))]≳D2(G, G∗)
for any mixing measure G∈ O k(Θ). Therefore, combin-
ing this bound with Proposition 2.2 leads to the following
convergence rate of the MLE bGn:
P
D2(bGn, G∗)> C 1p
log(n)/n
≲exp(−c1log(n)),
where C1>0is a constant depending on ΘandG∗, while
the constant c1>0depends only on Θ.
5

--- PAGE 6 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Proof of Theorem 3.1 is in Appendix A.1. A few comments
regarding this result are in order.
(i) Theorem 3.1 suggests that the MLE bGnconverges to
the true mixing measure G∗under the loss function D2
at the parametric rate of order eO(n−1/2). Based on the
formulation of D2in equation (5), it follows that exact-
specified parameters β∗
1j, a∗
j, b∗
j, whose V oronoi cells Cj
have exactly one element, share the same estimation rate of
ordereO(n−1/2).
(ii) On the other hand, for over-specified parameters
β∗
1j, a∗
j, b∗
jwhose V oronoi cells Cjhave more than one ele-
ment, the rates for estimating them are slower, standing at
ordereO(n−1/4). Nevertheless, these rates are substantially
faster than their counterparts under the softmax gating Gaus-
sian MoE model studied in (Nguyen et al., 2023), which
decreases monotonically with the cardinality of V oronoi
cells.
3.2. Regime 2 of Expert Parameters
Under this regime, we can find an index j∈[k∗]that satis-
fiesb∗
jℓ= 0for any ℓ∈[K−1]. By simple calculations, we
can validate that the PDEs in equation (7)hold true. This
result leads to so many linearly dependent terms among the
derivatives of the function uw.r.t its parameters that the
density discrepancy gbGn(Y|X)−gG∗(Y|X)cannot be de-
composed into a linear combination of linearly independent
elements as our expectation. As a consequence, we demon-
strate in the following proposition that the Total Variation
lower bound in equation (6) no longer holds under Regime
2.
Proposition 3.2. Suppose that the assumption of Regime 2
is satisfied, then we obtain that
inf
G∈Ok(Θ),
Dr(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]/Dr(G, G∗)→0,
asε→0, for any r≥1.
Proof of Proposition 3.2 is in Appendix A.2.2. Based on
the above result, we establish a minimax lower bound for
estimating the true mixing measure G∗in Theorem 3.3.
Theorem 3.3 (Minimax Lower Bound) .Under Regime 2,
the following minimax lower bound of estimating G∗holds
true for any r≥1:
inf
Gn∈Ok(Θ)sup
G∈Ok(Θ)\Ok∗−1(Θ)EgG[Dr(Gn, G)]≳n−1/2.
Here,EgGrepresents the expectation taken with respect to
the product measure with mixture density gn
G.
Proof of Theorem 3.3 can be found in Appendix A.2. This
result together with the formulation of Drin equation (5)indicates that the rates for estimating exact-specified param-
etersβ∗
1j, a∗
j, b∗
jare not better than eO(n−1/2), while those
for their over-specified counterparts are even slower than
any polynomial rates.
3.3. Proof Sketches
In this section, we provide proof sketches for both Theo-
rem 3.1 and Theorem 3.3.
3.3.1. P ROOF SKETCH OF THEOREM 3.1
Due to the inequality h≥V, it suffices to show the To-
tal Variation lower bound EX[V(gG(·|X), gG∗(·|X)]≳
D2(G, G∗)for any G∈ O k(Θ). We divide this problem
into two parts as follows:
Local structure. In this part, we need to show that
lim
ε→0inf
G∈Ok(Θ):
D2(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]
D2(G, G∗)>0.
Assume by contrary that there exists Gn∈ O k(Θ)
such that EX[V(gGn(·|X), gG∗(·|X))]/D2(Gn, G∗)and
D2(Gn, G∗)both converge to zero.
Step 1. In this step, we use Taylor expansions to decom-
pose the quantity Tn(s) :=hPk∗
j=1exp(( β∗
1j)⊤X+β∗
0j)i
·
[gGn(Y=s|X)−gG∗(Y=s|X)]into a linear combina-
tion of linearly independent terms as
Tn(s) =k∗X
j=1ωjEj(Y=s|X) +R(X, Y),
where R(X, Y)is a Taylor remainder such that
R(X, Y)/D2(Gn, G∗)→0.
Step 2. In this step, we show by contradiction that not all
the ratios ωj/D2(Gn, G∗)tend to zero as n→ ∞ . Assume
that all of them converge to zero, then we deduce that 1 =
D2(Gn, G∗)/D2(Gn, G∗)→0, which is a contradiction.
Therefore, at least one among the ratios wj/D2(Gn, G∗)
does not vanish.
Step 3. Since EX[V(gGn(·|X), gG∗(·|X))]/D2(Gn, G∗)
converges to zero, then by applying the Fatou’s lemma, we
deduce that Tn(s)/D2(Gn, G∗)→0asn→ ∞ . Note
thatTn(s)is written as a linear combination of linearly
independent terms, thus, all the associated coefficients in
that combination go to zero, which contradicts the result in
Step 2. Hence, the proof of the local structure is completed.
Global Structure. In this part, we demonstrate that
inf
G∈Ok(Θ),
D2(G,G∗)>εEX[V(gG(·|X), gG∗(·|X))]
D2(G, G∗)>0.
6

--- PAGE 7 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Assume that this claim is not true. Then, we can
find a mixing measure G′∈ O k(Θ) such that
EX[V(gG(·|X), gG∗(·|X))] = 0 . By the Fatou’s lemma,
we obtain that gG′(Y=s|X) =gG∗(Y=s|X)for any
s∈[K]for almost surely X. It follows from Proposition 2.2
thatG′≡G. This result implies that D2(G′, G∗) = 0 ,
which contradicts the constraint D2(G′, G∗)> ε > 0.
Hence, the proof sketch is completed.
3.3.2. P ROOF SKETCH OF THEOREM 3.3
LetM1be some fixed positive constant. Following from the
result of Proposition 3.2, for any suffciently small ε >0,
we can seek a mixing measure G′
∗∈ O k(Θ) such that
Dr(G′
∗, G∗) = 2 εandEX[V(gG′∗(·|X), gG∗(·|X))]≤
M1ε. Note that for any sequence of mixing measures
Gn∈ Ok(Θ), we have
2 max
G∈{G∗,G′
∗}EgG[Dr(Gn, G)]
≥EgG∗[Dr(Gn, G∗)] +EgG′∗[Dr(Gn, G′
∗)].
Moreover, since Drsatisfies the weak triangle inequality,
there exists some constant M2>0such that
Dr(Gn, G∗) +Dr(Gn, G′
∗)≥M2Dr(G∗, G′
∗) = 2 M2ε.
Then, by leveraging the Le Cam’s minimax lower bound
approach (Yu, 1997), we get that
max
G∈{G∗,G′∗}EgG[Dr(Gn, G)]
≥M2ε(1−EX[V(gn
G∗(·|X), gn
G′∗(·|X))])
≥M2εh
1−q
1−(1−M2
1ε2)ni
.
By setting ε=n−1/2/M1, we obtain that
max
G∈{G∗,G′∗}EgG[Dr(Gn, G)]≳n−1/2,
for any sequence Gn∈ O k(Θ). Furthermore, since
{G∗, G′
∗}is a subset of Ok(Θ)\ Ok∗−1(Θ), we reach the
conclusion of Theorem 3.3.
4. Modified Softmax Gating Multinomial
Logistic MoE
In this section, we propose a novel class of modified softmax
gating functions to resolve the interaction between gating
parameters and expert parameters via the PDE (7)which
leads to significantly slow parameter estimation rates. Then,
we also capture the convergence rates of parameter estima-
tion under the multinomial logistic MoE model with that
those gating functions.
First of all, let us introduce the formulation of the modified
softmax gating multinomial logistic MoE model.Problem Setup. Suppose that the i.i.d samples
(X1, Y1),(X2, Y2), . . . , (Xn, Yn)∈ X × [K]are drawn
from the multinomial logistic MoE model of order k∗with
modified softmax gating function, whose conditional den-
sity function egG∗(Y=s|X)is given by
k∗X
i=1Softmax(( β∗
1i)⊤M(X) +β∗
0i)×f(Y=s|X;a∗
i, b∗
i)
:=k∗X
i=1exp(( β∗
1i)⊤M(X) +β∗
0i)
Pk∗
j=1exp(( β∗
1j)⊤M(X) +β∗
0j)
×exp(a∗
is+ (b∗
is)⊤X)PK
ℓ=1exp(a∗
iℓ+ (b∗
iℓ)⊤X), (8)
for any s∈[K], where Mis a function defined in Defini-
tion 4.1. Here, we reuse all the assumptions imposed on
the model in equation (1)unless stating otherwise. In the
above model, we would transform the covariate Xusing the
function Mfirst rather than routing it directly to the softmax
gating function as in the model (1). This transformation step
allows us to overcome the interaction between gating pa-
rameters and expert parameters emerging from Section 3.2,
which we will discuss below.
Definition 4.1 (Modified Function) .LetM:X →Rdbe
a bounded function such that the following set is linearly
independent for almost surely X:
n
Xp[M(X)]q:p, q∈Nd,0≤ |p|+|q| ≤2o
.(9)
To understand the above condition better, we provide below
both intuitive and technical explanations for it.
Intuitively, Under the multinomial logistic MoE model with
the standard softmax gate, we observe an interaction among
the gating parameters β1and the expert parameters bwhen a
fraction of expert parameters vanish (see equation (3). This
interaction mainly accounts for the slow parameter estima-
tion rates, which could be as slow as O(1/log(n)). We
realize that the previous interaction occurs as parameters
β1andbare both associated with the input Xin the con-
dition density in equation (1). To address this issue, we
propose transforming the input Xin the softmax gate by
the function M. Then, in the modified conditional density
in equation (8),β1is associated with M(X), while bis
still with X. However, to eliminate the parameter interac-
tion completely, we need to impose an assumption of linear
independence between the input Xand its transformation
M(X)in Definition 4.1.
Technically, the set in equation (9)is assumed to be linearly
independent for almost surely Xto guarantee there does
not exist any constant Ca∗
idepending only a∗
isuch that the
7

--- PAGE 8 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
PDEs
∂u
∂β1j(Y=s|X;β∗
1j, a∗
j, b∗
j)
=Ca∗
j·∂u
∂bjs(Y=s|X;β∗
1j, a∗
j, b∗
j),
fors∈[K], do not hold under any setting of the expert
parameters b∗
i. As a result, the interaction between gating
parameters and expert parameters mentioned in the Regime
2 does not appear in the modified softmax gating multino-
mial logistic MoE model. Next, we provide below a few
valid instances of the function M.
Example. It can verified that all the following element-wise
functions satisfy the condition in Definition 4.1: M(X) =
tanh( X),M(X) = cos( X)andM(X) =Xmform≥3.
Additionally, Mcould also be a normalization function,
i.e.M(X) =X
||X||. The practical problem of selecting
important experts can benefit from this choice of function
M. In particular, in many applications of MoE models,
there are some experts which do not play an essential role
in learning, and even become redundant. In such scenario,
we would like the gate to put more weights on the important
experts. However, if the input magnitude is huge, then the
weight distribution will become uniform, which is undesir-
able. Therefore, the input normalization function M, which
helps remain the input magnitude of one, is beneficial in this
case. Regarding the parameter estimation problem, since
the function Mhelps remove the parameter interaction, the
parameter estimation rates are ensured to be of polynomial
orders under any parameter settings (see Table 1). Another
potential option is M(X) =sigmoid (X), which allows
the magnitude of the input to vary between 0 and 1.
Maximum Likelihood Estimation. Due to the modification
of the conditional density in equation (8), the formulation
of MLE of the true mixing measure G∗under the modi-
fied softmax gating multinomial logistic MoE model also
changes as follows:
eGn∈arg max
G∈Ok(Θ)nX
i=1log(egG(Yi|Xi)). (10)
Next, we validate the identifiability of the modified softmax
gating multinomial logistic MoE model in the following
proposition.
Proposition 4.2 (Identifiability) .Assume that GandG′are
two mixing measures in Ok(Θ). IfegG(Y|X) =egG′(Y|X)
holds true for almost surely (X, Y), then we obtain that
G≡G′.
Proof of Proposition 4.2 is in Appendix B.1.2. This result
points out that the modified softmax gating multinomial
logistic MoE model is still identifiable. In other words, ifthe density estimation egeGnconverges to the true density
egG∗, then the MLE eGnalso approaches its true counterpart
G∗.
Now, we are ready to derive the convergence rate of density
estimation egeGnto the true density egG∗under the Hellinger
distance.
Proposition 4.3 (Density Estimation Rate) .With the MLE
eGndefined in equation (10), the density estimation egeGnconverges to the true density egG∗at the following rate:
P
EX[h(egeGn(·|X),egG∗(·|X))]> C 2p
log(n)/n
≲exp(−c2log(n)), (11)
where C2andc2are universal positive constants that de-
pend only on Θ.
Proof of Proposition 4.3 is in Appendix B.2. It follows
from the above proposition that the density estimation rate
under the modified softmax gating multinomial logistic MoE
model is parametric of order eO(n−1/2). This result matches
the rate for estimating the true density under the multinomial
logistic MoE model with standard softmax gating function
in Proposition 2.2. Based on this observation, we then
capture the convergence behavior of the MLE eGnin the
following theorem.
Theorem 4.4 (Parameter Estimation Rate) .The following
Hellinger lower bound holds true for any mixing measure
G∈ Ok(Θ):
EX[h(egG(·|X),egG∗(·|X))]≳D2(G, G∗).
This lower bound together with Proposition 4.3 imply that
there exists a constant C3>0depending on ΘandG∗such
that
P(D2(eGn, G∗)> C 3p
log(n)/n)≲exp(−c3log(n)),
where c3>0is a constant that depends only on Θ.
Proof of Theorem 4.4 is deferred to Appendix A.3. This
theorem reveals that the MLE eGnconverges to the true
mixing measure G∗under the V oronoi loss function D2at
a rate of order eO(n−1/2). From the definition of D2in
equation (5), we deduce that the exact-specified parameters
β∗
1j, a∗
j, b∗
j, which are fitted by exactly one component, en-
joy the same estimation rate of eO(n−1/2), whereas those
for their over-specified counterparts, i.e. those fitted by at
least two components, are of order eO(n−1/4). It is worth
emphasizing that these rates remain stable regardless of any
values of the expert parameters b∗
i. This highlights the bene-
fits of modified softmax gating functions over the standard
softmax gating in the multinomial logistic MoE model.
In summary, replacing the standard softmax gating function
with its modified versions in the multinomial logistic MoE
8

--- PAGE 9 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
model remains the identifiability of that model and the para-
metric density estimation rate as well as ensures the stability
of parameter estimation rates under any settings of the ex-
pert parameters (see Table 1). As a consequence, we can
conclude that the modified softmax gating functions outper-
form their standard counterpart in the parameter estimation
problem under the multinomial logistic MoE model.
5. Discussion
In this paper, we investigate the convergence behavior of
maximum likelihood estimation in the softmax gating multi-
nomial logistic mixture of experts under the over-specified
settings. For that purpose, we design a novel generic
V oronoi loss function, and then discover that the rates for
estimating the true density and exact-specified true param-
eters are all parametric on the sample size, while those for
over-specified true parameters are slightly slower. However,
when part of the expert parameters vanishes, the estimation
rates for the over-specified true parameters experience a sur-
prisingly slow rates due to an intrinsic interaction between
the parameters of gating and expert functions. To tackle
this issue, we propose a novel class of modified softmax
gating functions that not only keep the identifiability of the
multinomial logistic mixture of experts and the parametric
density estimation rate unchanged, but also stabilize the
parameter estimation rates irrespective of the collapse of
expert parameters. This highlights the advantages of using
modified softmax gating functions over the standard soft-
max gating function in the parameter estimation problem of
the multinomial logistic MoE model.
Technical novelty. Compared to previous works, our paper
is technically novel in terms of the following aspects:
(1) Covariate-dependent gate: We consider softmax gate
whose value depends on the covariate X, while (Ho et al.,
2022) use covariate-free weights, which are significantly
simpler. Thus, in Step 1 of our proofs of Theorems 3.1 and
4.4, if we apply Taylor expansions directly to the density
discrepancy gGn(Y|X)−gG∗(Y|X)as in (Ho et al., 2022),
we cannot represent that discrepancy as a combination of
elements from some linearly independent set, which is a
key step. Therefore, we have to take the product of the
softmax’s denominator and the density discrepancy, denoted
byTn(s) :=hPk∗
j=1exp(( β∗
1j)⊤X+β∗
0j)i
·[gGn(Y=
s|X)−gG∗(Y=s|X)]. Then, we decompose Tn(s)
such that it includes two functions exp(( βn
1i)⊤X)f(Y=
s|X;an
i, bn
i)andexp(( βn
1i)⊤X)gGn(Y=s|X)(see equa-
tion(14)). Then, we need to apply two Taylor expansions to
those functions rather than only one as in (Ho et al., 2022).
(2) Minimax lower bound: A key step to derive polynomial
rates for estimating parameters (even in extreme cases) in(Nguyen et al., 2023; 2024b) is to establish Total Variation
lower bounds EX[V(gG(·|X), gG∗(·|X))]≳D(G, G∗),
where Dis a loss function. However, we show in Proposi-
tion 3.2 that such bound does not hold true under the Regime
2 due to the PDEs in equation (3). Thus, we provide a mini-
max lower bound in Theorem 3.3 to show that the parameter
estimation rates are slower than any polynomial rates, and
therefore, could be as slow as O(1/log(n)). As far as we
are concerned, this phenomenon has never been observed in
previous works.
(3) Non-trivial solution for rate improvement: (Ho et al.,
2022; Nguyen et al., 2023; 2024b) characterized the slowest
rates of the same form O(n−1/2¯r), for some ¯r≥4, with-
out proposing any solutions to improve them. By contrast,
although the slowest rate in our work is even slower than
any polynomial rates, we provide a non-trivial modified
gating function in Section 4 to alleviate that issue. More
importantly, since that modified gate generally helps address
interactions among gating and expert parameters, it can be
applied to (Ho et al., 2022; Nguyen et al., 2023; 2024b)
(up to some changes of the conditions of function M(·)in
Definition 4.1) for rate improvement. As shown in Table 1,
the modified softmax gate enhances the estimation rates
fromO(n−1/2¯r)to at least O(n−1/4)under any parameter
settings. To the best of our knowledge, such flexible and
effective solution had never been proposed in the literature.
Limitation and future directions. In this paper, we
consider a well-specified setting when the data are as-
sumed to be sampled from a softmax gating multino-
mial logistic MoE. However, in practice, the data are
not necessarily generated from that model, which we re-
fer to as the misspecified setting. Under the misspec-
ified setting, the MLE converges to a mixing measure
eG∈arg minG∈Ok(Θ)KL(P(Y|X)||gG(Y|X)), where
P(Y|X)is the true conditional distribution of Ygiven X
and KL stands for the Kullback-Leibler divergence. As
the space Ok(Θ)is non-convex, the existence of eGis not
unique. Furthermore, the current analysis of the MLE un-
der the misspecified setting of statistical models is mostly
conducted when the function space is convex (van de Geer,
2000). Thus, it is necessary to develop new technical tools
to establish the convergence rate of the MLE under the non-
convex misspecified setting. Since this is beyond the scope
of our work, we leave it for future development. Another
potential direction is to comprehend the effects of the tem-
perature parameter (Nie et al., 2022; Nguyen et al., 2024a),
which controls the softmax weight distribution and the spar-
sity of the MoE, on the convergence of parameter estimation
under the softmax gating multinomial logistic MoE. This
direction has remained unexplored in the literature.
9

--- PAGE 10 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Acknowledgements
NH acknowledges support from the NSF IFML 2019844
and the NSF AI Institute for Foundations of Machine Learn-
ing.
Impact Statement
This paper presents work whose goal is to advance the field
of Machine Learning. Given the theoretical nature of the
paper, we believe that there are no potential societal conse-
quences of our work.
References
Bao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O.-K.,
Aggarwal, K., Som, S., Piao, S., and Wei, F. VLMo:
Unified vision-language pre-training with mixture-of-
modality-experts. In Advances in Neural Information
Processing Systems , 2022. (Cited on page 1.)
Chen, K., Xu, L., and Chi, H. Improved learning algorithms
for mixture of experts in multiclass classification. Neural
Networks , 12(9):1229–1252, 1999. ISSN 0893-6080. doi:
https://doi.org/10.1016/S0893-6080(99)00043-X. (Cited
on page 2.)
Chen, Z., Wang, P., Ma, L., Wong, K. K., and Wu, Q. Mod-
squad: Designing mixtures of experts as modular multi-
task learners. In CVPR , 2023. (Cited on page 1.)
Chow, Y ., Tulepbergenov, A., Nachum, O., Gupta, D., Ryu,
M., Ghavamzadeh, M., and Boutilier, C. A mixture-
of-expert approach to RL-based dialogue management.
InThe Eleventh International Conference on Learning
Representations , 2023. URL https://openreview.
net/forum?id=4FBUihxz5nm .(Cited on page 1.)
Deleforge, A., Forbes, F., and Horaud, R. High-dimensional
regression with Gaussian mixtures and partially-latent
response variables. Statistics and Computing , 25(5):
893–911, 2015. ISSN 1573-1375. doi: 10.1007/
s11222-014-9461-5. URL https://doi.org/10.
1007/s11222-014-9461-5 .(Cited on page 1.)
Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum
Likelihood from Incomplete Data Via the EM Algorithm.
Journal of the Royal Statistical Society: Series B (Method-
ological) , 39(1):1–22, September 1977. ISSN 0035-9246.
(Cited on page 29.)
Do, T., Le, H., Nguyen, T., Pham, Q., Nguyen, B., Doan, T.,
Liu, C., Ramasamy, S., Li, X., and HOI, S. HyperRouter:
Towards efficient training and inference of sparse mixture
of experts. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing , Sin-
gapore, December 2023. Association for Computational
Linguistics. (Cited on page 1.)
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Min-
derer, M., Heigold, G., Gelly, S., Uszkoreit, J.,
and Houlsby, N. An image is worth 16x16 words:
Transformers for image recognition at scale. In In-
ternational Conference on Learning Representations ,
2021. URL https://openreview.net/forum?
id=YicbFdNTTy .(Cited on page 1.)
Du, N., Huang, Y ., Dai, A. M., Tong, S., Lepikhin, D., Xu,
Y ., Krikun, M., Zhou, Y ., Yu, A., Firat, O., Zoph, B.,
Fedus, L., Bosma, M., Zhou, Z., Wang, T., Wang, E.,
Webster, K., Pellat, M., Robinson, K., Meier-Hellstern,
K., Duke, T., Dixon, L., Zhang, K., Le, Q., Wu, Y ., Chen,
Z., and Cui, C. Glam: Efficient scaling of language
models with mixture-of-experts. In ICML , 2022. (Cited
on page 1.)
Fedus, W., Zoph, B., and Shazeer, N. Switch transformers:
Scaling to trillion parameter models with simple and ef-
ficient sparsity. Journal of Machine Learning Research ,
23:1–39, 2022. (Cited on page 1.)
Grün, B. and Leisch, F. Identifiability of finite mixtures of
multinomial logit models with varying and fixed effects.
Journal of Classification , 25(2):225–247, 2008. (Cited on
pages 28 and 29.)
Gulati, A., Qin, J., Chiu, C., Parmar, N., Zhang, Y ., Yu,
J., Han, W., Wang, S., Zhang, Z., Wu, Y ., and Pang,
R. Conformer: Convolution-augmented transformer for
speech recognition. In Proc. Interspeech 2020 , pp. 5036–
5040, 2020. doi: 10.21437/Interspeech.2020-3015. (Cited
on page 1.)
Gupta, S., Mukherjee, S., Subudhi, K., Gonzalez, E., Jose,
D., Awadallah, A., and Gao, J. Sparsely activated mixture-
of-experts are robust multi-task learners. arXiv preprint
arxiv 2204.0768 , 2022. (Cited on page 1.)
Hazimeh, H., Zhao, Z., Chowdhery, A., Sathiamoorthy, M.,
Chen, Y ., Mazumder, R., Hong, L., and Chi, E. H. Dselect-
k: Differentiable selection in the mixture of experts with
applications to multi-task learning. In NeurIPS , 2021.
(Cited on page 1.)
Ho, N., Yang, C. Y ., and Jordan, M. I. Convergence rates
for Gaussian mixtures of experts. Journal of Machine
Learning Research , 23(323):1–81, 2022. (Cited on pages 1
and 9.)
Huynh, B. and Chamroukhi, F. Estimation and feature
selection in mixtures of generalized linear experts models.
arXiv preprint arXiv:1907.06994 , 2019. (Cited on page 2.)
10

--- PAGE 11 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E.
Adaptive mixtures of local experts. Neural Computation ,
3, 1991. (Cited on pages 1 and 2.)
Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of
experts and the EM algorithm. Neural Computation , 6:
181–214, 1994. (Cited on pages 1 and 2.)
Kwon, J. and Caramanis, C. EM converges for a mixture
of many linear regressions. In Proceedings of the Twenty
Third International Conference on Artificial Intelligence
and Statistics , volume 108 of Proceedings of Machine
Learning Research , pp. 1727–1736. PMLR, 26–28 Aug
2020. (Cited on page 30.)
Kwon, J., Qian, W., Caramanis, C., Chen, Y ., and Davis,
D. Global convergence of the em algorithm for mix-
tures of two component linear regression. In Proceed-
ings of the Thirty-Second Conference on Learning The-
ory, volume 99 of Proceedings of Machine Learning
Research , pp. 2055–2110. PMLR, 25–28 Jun 2019. (Cited
on page 30.)
Kwon, J., Ho, N., and Caramanis, C. On the minimax opti-
mality of the em algorithm for learning two-component
mixed linear regression. In Proceedings of The 24th
International Conference on Artificial Intelligence and
Statistics , volume 130 of Proceedings of Machine Learn-
ing Research , pp. 1405–1413. PMLR, 13–15 Apr 2021.
(Cited on page 30.)
Lepikhin, D., Lee, H., Xu, Y ., Chen, D., Firat, O., Huang, Y .,
Krikun, M., Shazeer, N., and Chen, Z. GShard: Scaling
giant models with conditional computation and automatic
sharding. In International Conference on Learning Rep-
resentations , 2021. (Cited on page 1.)
Liang, H., Fan, Z., Sarkar, R., Jiang, Z., Chen, T., Zou, K.,
Cheng, Y ., Hao, C., and Wang, Z. M3ViT: Mixture-of-
experts vision transformer for efficient multi-task learning
with model-accelerator co-design. In NeurIPS , 2022.
(Cited on page 1.)
Manole, T. and Ho, N. Refined convergence rates for max-
imum likelihood estimation under finite mixture mod-
els. In Proceedings of the 39th International Conference
on Machine Learning , volume 162 of Proceedings of
Machine Learning Research , pp. 14979–15006. PMLR,
17–23 Jul 2022. (Cited on page 4.)
Nguyen, H., Nguyen, T., and Ho, N. Demystifying softmax
gating function in Gaussian mixture of experts. In Ad-
vances in Neural Information Processing Systems , 2023.
(Cited on pages 1, 2, 6, and 9.)
Nguyen, H., Akbarian, P., and Ho, N. Is temperature sample
efficient for softmax Gaussian mixture of experts? In
Proceedings of the ICML , 2024a. (Cited on page 9.)Nguyen, H., Akbarian, P., Yan, F., and Ho, N. Statistical per-
spective of top-k sparse softmax gating mixture of experts.
InInternational Conference on Learning Representations ,
2024b. (Cited on pages 1 and 9.)
Nguyen, H., Nguyen, T., Nguyen, K., and Ho, N. Towards
convergence rates for parameter estimation in Gaussian-
gated mixture of experts. In Proceedings of The 27th
International Conference on Artificial Intelligence and
Statistics , 2024c. (Cited on page 1.)
Nie, X., Miao, X., Cao, S., Ma, L., Liu, Q., Xue, J., Miao,
Y ., Liu, Y ., Yang, Z., and Cui, B. Evomoe: An evolu-
tional mixture-of-experts training framework via dense-
to-sparse gate. arXiv preprint arXiv:2112.14397 , 2022.
(Cited on page 9.)
Peng, F., Jacobs, R., and Tanner, M. Bayesian inference in
mixtures-of-experts and hierarchical mixtures-of-experts
models with an application to speech recognition. Journal
of the American Statistical Association , 91(435):953–960,
1996. ISSN 01621459. (Cited on page 1.)
Pham, N. and Chamroukhi, F. Functional mixture-of-experts
for classification. In 53èmes Journées de Statistique de
la Société Française de Statistique (SFdS) , 2022. (Cited
on page 2.)
Ren, J., Li, Y ., Ding, Z., Pan, W., and Dong, H. Probabilis-
tic mixture-of-experts for efficient deep reinforcement
learning. arxiv preprint arxiv 2104.09122 , 2021. (Cited
on page 1.)
Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Je-
natton, R., Pint, A. S., Keysers, D., and Houlsby, N. Scal-
ing vision with sparse mixture of experts. In Advances
in Neural Information Processing Systems , volume 34,
pp. 8583–8595. Curran Associates, Inc., 2021. (Cited on
page 1.)
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,
Q., Hinton, G., and Dean, J. Outrageously large neural
networks: The sparsely-gated mixture-of-experts layer.
InInternational Conference on Learning Representations ,
2017. (Cited on page 1.)
van de Geer, S. Empirical Processes in M-estimation . Cam-
bridge University Press, 2000. (Cited on pages 9, 25,
and 27.)
Villani, C. Topics in Optimal Transportation . American
Mathematical Society, 2003. (Cited on page 1.)
Villani, C. Optimal transport: Old and New . Springer, 2008.
(Cited on page 1.)
11

--- PAGE 12 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
You, Z., Feng, S., Su, D., and Yu, D. Speechmoe2: Mixture-
of-experts model with improved routing. In IEEE In-
ternational Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pp. 7217–7221, 2022. (Cited on
page 1.)
Yu, B. Assouad, Fano, and Le Cam. Festschrift for Lucien
Le Cam , pp. 423–435, 1997. (Cited on page 7.)
Yuksel, S. and Gader, P. Variational mixture of experts for
classification with applications to landmine detection. In
2010 20th International Conference on Pattern Recogni-
tion, pp. 2981–2984, 2010. doi: 10.1109/ICPR.2010.730.
(Cited on page 2.)
Zhou, Y ., Lei, T., Liu, H., Du, N., Huang, Y ., Zhao, V .,
Dai, A. M., Chen, Z., Le, Q., and Laudon, J. Mixture-of-
experts with expert choice routing. In Advances in Neural
Information Processing Systems , volume 35, pp. 7103–
7114. Curran Associates, Inc., 2022. (Cited on page 1.)
Zhou, Y ., Du, N., Huang, Y ., Peng, D., Lan, C., Huang, D.,
Shakeri, S., So, D., Dai, A., Lu, Y ., Chen, Z., Le, Q.,
Cui, C., Laudon, J., and Dean, J. Brainformers: Trading
simplicity for efficiency. In International Conference
on Machine Learning , pp. 42531–42542. PMLR, 2023.
(Cited on page 1.)
12

--- PAGE 13 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Supplementary Material for “A General Theory for Softmax Gating
Multinomial Logistic Mixture of Experts”
In this supplementary material, we first present rigorous proofs for results regarding the convergence rates of parameter
estimation under the (modified) softmax gating multinomial logistic mixture of experts in Appendix A, while those for
density estimation are then provided in Appendix B. Next, we theoretically verify the identifiability of the proposed models
in Appendix C. Lastly, we conduct several experiments in Appendix D to empirically validate our theoretical results.
A. Proof for Convergence Rates of Parameter Estimation
In this appendix, we provide proofs for Theorem 3.1 and Theorem 3.3 in Appendix A.1 and Appendix A.2, respectively.
Meanwhile, the proof of Theorem 4.4 can be found in Appendix A.3.
A.1. Proof of Theorem 3.1
To reach the conclusion in Theorem 3.1, we need to show that
inf
G∈Ok(Θ)EX[V(gG(·|X), gG∗(·|X))]/D2(G, G∗)>0. (12)
Local Structure : Firstly, we prove by contradiction the local structure of inequality (12), which is given by
lim
ε→0inf
G∈Ok(Θ),
D2(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]/D2(G, G∗)>0. (13)
Assume that this claim does not hold true, then there exists a sequence of mixing measures Gn:=Pkn
i=1exp(βn
0i)δ(βn
1i,an
i,bn
i)∈ Ok(Θ)such that both EX[V(gGn(·|X), gG∗(·|X))]/D2(Gn, G∗)andD2(Gn, G∗)approach
zero as ntends to infinity. In addition, we define the set of V oronoi cells generated by the support of G∗for this sequence as
follows:
Cn
j:={i∈[kn] :∥θn
i−θ∗
j∥ ≤ ∥ θn
i−θ∗
ℓ∥,∀ℓ̸=j},
where θn
i:= (βn
1i, an
i, bn
i)andθ∗
j:= (β∗
1j, a∗
i, b∗
i)for any j∈[k∗]. Recall that β∗
1k∗=0dis known, thus we set βn
1i=0d
for any i∈ Ck∗. Similarly, note that (a∗
jK, b∗
jK) = (0 ,0d)are known for any j∈[k∗], we also let (an
iK, bn
iK) = (0 ,0d)
for any i∈kn. As our proof argument is asymptotic, we can assume without loss of generality (WLOG) that the above
V oronoi cells are independent of n, that is, Cj=Cn
jfor any j∈[k∗]. Next, since D2(Gn, G∗)→0asn→ ∞ , it follows
from the formulation of D2in equation (5)thatβn
1i→β∗
1j,(an
iℓ, bn
iℓ)→(a∗
jℓ, b∗
jℓ)andP
i∈Cn
jexp(βn
0i)→exp(β∗
0j)for
anyℓ∈[K],i∈ Cjandj∈[k∗]when n→ ∞ . Subsequently, we divide our remaining arguments into three steps as below.
Step 1. In this step, we use Taylor expansions to decompose the following quantity:
Tn(s) :=hk∗X
j=1exp(( β∗
1j)⊤X+β∗
0j)i
·h
gGn(Y=s|X)−gG∗(Y=s|X)i
.
Denote u(Y=s|X;β1i, ai, bi) := exp( β⊤
1iX)·f(Y|X;ais, bis)andv(Y=s|X;β1i) = exp( β⊤
1iX)gGn(Y=s|X)for
anys∈[K], we have
Tn(s) =k∗X
j=1X
i∈Cjexp(βn
0i)h
u(Y=s|X;βn
1i, an
i, bn
i)−u(Y=s|X;β∗
1j, a∗
j, b∗
j)i
−k∗X
j=1X
i∈Cjexp(βn
0i)h
v(Y=s|X;βn
1i)−v(Y=s|X;β∗
1j)i
+k∗X
j=1X
i∈Cjexp(βn
0i)−exp(β∗
0j)h
u(Y=s|X;β∗
1j, a∗
j, b∗
j)−v(Y=s|X;β∗
1j)i
:=An−Bn+En. (14)
13

--- PAGE 14 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Given the above formulations of AnandBn, we continue to decompose them into two smaller terms based on the cardinality
of V oronoi cells Cj. In particular,
An=X
j:|Cj|=1X
i∈Cjexp(βn
0i)h
u(Y=s|X;βn
1i, an
i, bn
i)−u(Y=s|X;β∗
1j, a∗
j, b∗
j)i
+X
j:|Cj|>1X
i∈Cjexp(βn
0i)h
u(Y=s|X;βn
1i, an
i, bn
i)−u(Y=s|X;β∗
1j, a∗
j, b∗
j)i
=An,1+An,2.
Next, let us denote hℓ(X, a ℓ, bℓ) :=aℓ+b⊤
ℓXfor any ℓ∈[K], then
f(Y=s|X;ai, bi) =exp(ais+b⊤
isX)PK
ℓ=1exp(aiℓ+b⊤
iℓX)=exp(hs(X, a is, bis))PK
ℓ=1exp(hℓ(X, a iℓ, biℓ)).
By means of the first-order Taylor expansion, An,1can be represented as
An,1=X
j:|Cj|=1X
i∈Cjexp(βn
0i)X
|α|=11
α!(∆βn
1ij)K−1Y
ℓ=1(∆an
ijℓ)α2ℓ(∆bn
ijℓ)α3ℓ∂|α|u(Y=s|X;β∗
1j, a∗
j, b∗
j)
∂βα1
1jQK−1
ℓ=1∂aα2ℓ
jℓ∂bα3ℓ
jℓ+R1(X, Y).
Here, α:= (α1, α21, . . . , α 2(K−1), α31, . . . , α 3(K−1)), where α1, α3ℓ∈Ndandα2ℓ∈Nfor any ℓ∈[K−1]. Additionally,
R1(X, Y)is a Taylor remainder such that R1(X, Y)/D2(Gn, G∗)→0asn→ ∞ . From the formulation of u, we have
An,1=X
j:|Cj|=1X
i∈Cjexp(βn
0i)X
|α|=11
α!(∆βn
1ij)K−1Y
ℓ=1(∆an
ijℓ)α2ℓ(∆bn
ijℓ)α3ℓ
×Xα1+PK−1
ℓ=1α3ℓexp(( β∗
1j)⊤X)∂PK−1
ℓ=1(α2ℓ+|α3ℓ|)f
∂hα21+|α31|
1 . . . ∂hα2(K−1)+|α3(K−1)|
K−1(Y=s|X;a∗
j, b∗
j) +R1(X, Y).
Letq1=α1+PK−1
ℓ=1α3ℓ∈Nd,q2= (q2ℓ)K−1
ℓ=1:= (α2ℓ+|α3ℓ|)K−1
ℓ=1∈NK−1and
Iq1,q2:=(
α= (α1, α21, . . . , α 2(K−1), α31, . . . , α 3(K−1)) :α1+K−1X
ℓ=1α3ℓ=q1,(α2ℓ+|α3ℓ|)K−1
ℓ=1=q2)
, (15)
we can rewrite An,1as
An,1=X
j:|Cj|=12X
|q1|+|q2|=1X
i∈CjX
α∈Iq1,q21
α!(∆βn
1ij)α1K−1Y
ℓ=1(∆an
ijℓ)α2ℓ(∆bn
ijℓ)α3ℓ
×Xq1exp(( β∗
1j)⊤X)∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j) +R1(X, Y).
Similarly, by means of second order Taylor expansion, An,2is expressed as follows:
An,2=X
j:|Cj|>14X
|q1|+|q2|=1X
i∈CjX
α∈Iq1,q21
α!(∆βn
1ij)α1K−1Y
ℓ=1(∆an
ijℓ)α2ℓ(∆bn
ijℓ)α3ℓ
×Xq1exp(( β∗
1j)⊤X)∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j) +R2(X, Y),
where R2(X, Y)is also a Taylor remainder such that R2(X, Y)/D2(Gn, G∗)→0asn→ ∞ . By employing the same
arguments for decomposing AntoBn, we get
Bn=X
j:|Cj|=1X
|γ|=1X
i∈Cjexp(βn
0i)
γ!(∆βn
1ij)γ×Xγexp(( β∗
1j)⊤X)gGn(Y=s|X) +R3(X, Y)
+X
j:|Cj|>12X
|γ|=1X
i∈Cjexp(βn
0i)
γ!(∆βn
1ij)γ×Xγexp(( β∗
1j)⊤X)gGn(Y=s|X) +R4(X, Y).
14

--- PAGE 15 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Putting the above results together, we obtain that
Tn(s) =k∗X
j=12+2·1{|Cj|>1}X
|q1|+|q2|=0Un
q1,q2(j)×Xq1exp(( β∗
1j)⊤X)∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j)
+k∗X
j=11+1{|Cj|>1}X
|γ|=0Wn
γ(j)×Xγexp(( β∗
1j)⊤X)gGn(Y=s|X) +R(X, Y),
where R(X, Y)is the sum of Taylor remainders such that R(X, Y)/D2(X, Y)→0asn→ ∞ and
Un
q1,q2(j) =(P
i∈CjP
α∈Iq1,q2exp(βn
0i)
α!(∆βn
1ij)α1QK−1
ℓ=1(∆an
ijℓ)α2ℓ(∆bn
ijℓ)α3ℓ,(q1, q2)̸= (0d,0K−1),
P
i∈Cjexp(βn
0i)−exp(β∗
0j), (q1, q2) = (0d,0K−1),
and
Wn
γ(j) =(
−P
i∈Cjexp(βn
0i)
γ!(∆βn
1ij)γ, |γ| ̸=0d,
−P
i∈Cjexp(βn
0i) + exp( β∗
0j),|γ|=0d,
for any j∈[k∗].
Step 2. In this step, we will prove by contradiction that at least one among Un
q1,q2(j)/D2(Gn, G∗)andWn
γ(j)/D2(Gn, G∗)
does not vanish as ntends to infinity. Assume that all of them approach zero, then by taking the summation
of|Un
q1,q2(j)|/D2(Gn, G∗)forj∈[k∗] :|Cj|= 1 ,q1∈ {e1, e2, . . . , e d}andq2=0K−1, where ei:=
(0, . . . , 0,1|{z}
i-th,0, . . . , 0)∈Rd, we achieve that
1
D2(Gn, G∗)·X
j:|Cj|=1X
i∈Cjexp(βn
0i)· ∥∆βn
1ij∥1→0. (16)
Similarly, for q1=0dandq2∈ {e′
1, e′
2, . . . , e′
K−1}, where e′
ℓ:= (0 , . . . , 0,1|{z}
ℓ-th,0, . . . , 0)∈RK−1, we have
1
D2(Gn, G∗)·X
j:|Cj|=1X
i∈CjK−1X
ℓ=1exp(βn
0i)· |∆an
ijℓ| →0. (17)
On the other hand, for q1∈ {e1, e2, . . . , e d}andq2∈ {e′
1, e′
2, . . . , e′
K−1}, we obtain that
1
D2(Gn, G∗)·X
j:|Cj|=1X
i∈CjK−1X
ℓ=1exp(βn
0i)· ∥∆bn
ijℓ∥1→0. (18)
Combine the limits in equations (16), (17) and (18), we get
1
D2(Gn, G∗)·X
j:|Cj|=1X
i∈Cjexp(βn
0i)·h
∥∆βn
1ij∥1+K−1X
ℓ=1(|∆an
ijℓ|+∥∆bn
ijℓ∥1)i
→0.
Due to the topological equivalence between 1-norm and 2-norm, the above limit is equivalent to
1
D2(Gn, G∗)·X
j:|Cj|=1X
i∈Cjexp(βn
0i)·h
∥∆βn
1ij∥+K−1X
ℓ=1(|∆an
ijℓ|+∥∆bn
ijℓ∥)i
→0. (19)
Next, we consider the summation of |Un
q1,q2(j)|/D2(Gn, G∗)forj∈[k∗] :|Cj|>1,q1∈ {2e1,2e2, . . . , 2ed}and
q2=0K−1, which leads to
1
D2(Gn, G∗)·X
j:|Cj|>1X
i∈Cjexp(βn
0i)· ∥∆βn
1ij∥2→0. (20)
15

--- PAGE 16 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Forq1=0dandq2∈ {2e′
1,2e′
2, . . . , 2e′
K−1}, we have
1
D2(Gn, G∗)·X
j:|Cj|>1X
i∈Cjexp(βn
0i)·K−1X
ℓ=1|∆an
ijℓ|2→0. (21)
Meanwhile, for q1∈ {2e1,2e2, . . . , 2ed}andq2∈ {2e′
1,2e′
2, . . . , 2e′
K−1}, we get
1
D2(Gn, G∗)·X
j:|Cj|>1X
i∈Cjexp(βn
0i)·K−1X
ℓ=1∥∆bn
ijℓ∥2→0. (22)
It follows from equations (20), (21) and (22) that
1
D2(Gn, G∗)·X
j:|Cj|>1X
i∈Cjexp(βn
0i)·h
∥∆βn
1ij∥2+K−1X
ℓ=1(|∆an
ijℓ|2+∥∆bn
ijℓ∥2)i
→0. (23)
Note that
k∗X
j=1|Un
0d,0K−1(j)|
D2(Gn, G∗)=1
D2(Gn, G∗)·k∗X
j=1X
i∈Cjexp(βn
0i)−exp(β∗
0j)→0. (24)
By taking the sum of limits in equations (19),(23) and(24), we deduce that 1 =D2(Gn, G∗)/D2(Gn, G∗)→0as
n→ ∞ , which is a contradiction. Thus, at least one among the limits of Un
q1,q2(j)/D2(Gn, G∗)andWn
γ(j)/D2(Gn, G∗)
is non-zero.
Step 3. Finally, we will leverage Fatou’s lemma to point out a contradiction to the result in Step 2.
Let us denote by mnthe maximum of the absolute values of Un
q1,q2(j)/D2(Gn, G∗)andWn
γ(j)/D2(Gn, G∗)forj∈[k∗],
0≤ |q1|+|q2| ≤2 + 2·1{|Cj|>1}and0≤ |γ| ≤1 +1{|Cj|>1}. Then, it follows from the Fatou’s lemma that
0 = lim
n→∞EX[2V(gGn(·|X), gG∗(·|X))]
mnD2(Gn, G∗)≥ZKX
s=1lim inf
n→∞|gGn(Y=s|X)−gG∗(Y=s|X)|
mnD2(Gn, G∗)dX≥0.
As a result, we get that |gGn(Y=s|X)−gG∗(Y=s|X)|/[mnD2(Gn, G∗)]converges to zero, which implies that
Tn(s)/[mnD2(Gn, G∗)]→0asn→ ∞ for any s∈[K]and almost surely X. Let Un
q1,q2(j)/[mnD2(Gn, G∗)]→
τq1,q2(j)andWn
γ(j)→ηγ(j)asnapproaches infinity, then the previous result indicates that
k∗X
j=12+2·1{|Cj|>1}X
|q1|+|q2|=0τq1,q2(j)×Xq1exp(( β∗
1j)⊤X)∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j)
+k∗X
j=11+1{|Cj|>1}X
|γ|=0ηγ(j)×Xγexp(( β∗
1j)⊤X)gG∗(Y=s|X) = 0 . (25)
for any s∈[K]and almost surely X. Here, at least one among τq1,q2(j)andηγ(j)is different from zero. Assume the set
F: =(
Xq1exp(( β∗
1j)⊤X)∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j) :j∈[k∗],0≤ |q1|+|q2| ≤2 + 2·1{|Cj|>1})
∪n
Xγexp(( β∗
1j)⊤X)gG∗(Y=s|X) :j∈[k∗],0≤ |γ| ≤1 +1{|Cj|>1}o
(26)
is linearly independent, we deduce that τq1,q2(j) =ηγ(j) = 0 for any j∈[k∗],0≤ |q1|+|q2| ≤2 + 2·1{|Cj|>1}and
0≤ |γ| ≤1 +1{|Cj|>1}, which is a contradiction.
16

--- PAGE 17 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Thus, it suffices to show that Fis a linearly independent set to attain the local inequality in equation (13). In particular,
assume that equation (25) holds true for any s∈[K]and almost surely X, we will show that τq1,q2(j) =ηγ(j) = 0 for any
j∈[k∗],0≤ |q1|+|q2| ≤2 + 2·1{|Cj|>1}and0≤ |γ| ≤1 +1{|Cj|>1}. Firstly, we rewrite this equation as follows:
k∗X
j=11+1{|Cj|>1}X
|ω|=0"2+2·1{|Cj|>1}−|ω|X
|q2|=0τq1,q2(j)∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j) +ηω(j)gG∗(Y=s|X)#
×Xωexp(( β∗
1j)⊤X) = 0 ,
for any s∈[K]and almost surely X. Note that β∗
11, β∗
12, . . . , β∗
1k∗arek∗different values, therefore, it can be seen that
{Xωexp(( β∗
1j)⊤X) :j∈[k∗],0≤ |ω| ≤1 +1{|Cj|>1}}is a linearly independent set. As a result, we obtain that
2+2·1{|Cj|>1}−|ω|X
|q2|=0τq1,q2(j)∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j) +ηω(j)gG∗(Y=s|X) = 0 ,
for any j∈[k∗],0≤ |ω| ≤1 +1{|Cj|>1},s∈[K]and almost surely X. Similarly, the following set is linearly independent:
(
∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j), gG∗(Y=s|X) : 0≤ |q2| ≤2 + 2·1{|Cj|>1}− |ω|)
,
we achieve that τq1,q2(j) =ηγ(j) = 0 for any j∈[k∗],0≤ |q1|+|q2| ≤2 + 2·1{|Cj|>1}and0≤ |γ| ≤1 +1{|Cj|>1},
which completes the proof of local structure.
Global Structure : As the local inequality in equation (13) holds true, there exists a positive constant ε′that satisfies
inf
G∈Ok(Θ),
D2(G,G∗)≤ε′EX[V(gG(·|X), gG∗(·|X))]/D2(G, G∗)>0.
Therefore, it is sufficient to demonstrate that
inf
G∈Ok(Θ),
D2(G,G∗)>ε′EX[V(gG(·|X), gG∗(·|X))]/D2(G, G∗)>0. (27)
Assume by contrary that this inequality does not hold, then we can find a sequence G′
n∈ Ok(Θ)such that D2(G′
n, G∗)> ε′
andEX[V(gG′n(·|X), gG∗(·|X))]→0asntends to infinity. It is worth noting that Θis a compact set, therefore, we can
replace G′
nby its subsequence which converges to some mixing measure G′∈ Ok(Θ). Thus, we get that D2(G′, G∗)> ε′.
On the other hand, according to Fatou’s lemma,
0 = lim
n→∞EX[2V(gG′n(·|X), gG∗(·|X))]
D2(G′n, G∗)≥ZKX
s=1lim inf
n→∞|gG′n(Y=s|X)−gG∗(Y=s|X)|dX.
Consequently, it follows that
ZKX
s=1|gG′(Y=s|X)−gG∗(Y=s|X)|dX= 0,
which means that gG′(Y=s|X) =gG∗(Y=s|X)for any s∈[K]for almost surely X. Recall that the softmax
gating multinomial logistic mixture of experts is identifiable, we deduce that G′≡G, which contradicts the results that
D2(G′, G∗)> ε′. Hence, we obtain the global inequality in equation (27) and complete the proof.
A.2. Proof of Theorem 3.3
In Appendix A.2.1, we present the proof of Theorem 3.3 given the result of Proposition 3.2. Then, we provide the proof of
Proposition 3.2 in Appendix A.2.2.
17

--- PAGE 18 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
A.2.1. M AINPROOF
Given a fixed constant M1>0and a sufficiently small ε > 0that we will choose later, it follows from the
result of Proposition 3.2 that there exists a mixing measure G′
∗∈ O k(Θ) that satisfies Dr(G′
∗, G∗) = 2 εand
EX[V(gG′∗(·|X), gG∗(·|X))≤M1ε. Note that for any sequence Gn∈ Ok(Θ), we have
2 max
G∈{G′∗,G∗}EgG[Dr(Gn, G)]≥EgG∗[Dr(Gn, G∗)] +EgG′∗[Dr(Gn, G′
∗)],
where EgGdenotes the expectation taken with respect to the product measure with density gn
G. Moreover, since the loss Dr
satisfies the weak triangle inequality, we can find a constant M2>0such that
Dr(Gn, G∗) +Dr(Gn, G′
∗)≥M2Dr(G∗, G′
∗) = 2 M2ε.
As a result, we observe that
max
G∈{G∗,G′∗}EgG[Dr(Gn, G)]≥1
2
EgG∗[Dr(Gn, G∗)] +EgG′∗[Dr(Gn, G′
∗)]
≥M2ε·inf
f1,f2
EgG∗[f1] +EgG′∗[f2]
.
Here, f1andf2in the above infimum are measurable functions in terms of X1, X2, . . . , X nthat satisfy f1+f2= 1. By
the definition of Total Variation distance, the above infimum value is equal to 1−EX[V(gn
G∗(·|X), gn
G′∗(·|X))]. Therefore,
we obtain that
max
G∈{G∗,G′∗}EpG[Dr(Gn, G)]≥M2ε
1−EX[V(gn
G∗(·|X), gn
G′∗(·|X))]
≥M2εh
1−q
1−(1−M2
1ε2)ni
.
By choosing ε=n−1/2/M1, we have M2
1ε2=1
n, which implies that
sup
G∈Ok(Θ)\Ok∗−1(Θ)EgG[Dr(Gn, G)]≥ max
G∈{G∗,G′∗}EgG[Dr(Gn, G)]≳n−1/2,
for any mixing measure Gn∈ Ok(Θ). Hence, we reach the conclusion of Theorem 3.3, which says that
inf
Gn∈Ok(Θ)sup
G∈Ok(Θ)\Ok∗−1(Θ)EgG[Dr(Gn, G)]≳n−1/2,
for any r≥1.
A.2.2. P ROOF OF PROPOSITION 3.2
From the conditions of Regime 2, we may assume without loss of generality that b∗
1ℓ=0dfor any ℓ∈[K−1].
To reach the conclusion of Proposition 3.2, we need to find a sequence Gn∈ O k(Θ) such that Dr(Gn, G∗)
andV(pGn, pG∗)/Dr(Gn, G∗)both tend to zero when napproaches infinity. For that purpose, we choose Gn=Pk∗+1
i=1exp(βn
0i)δ(βn
1i,an
11,...,an
1(K−1),bn
11,...,bn
1(K−1))where
•exp(βn
01) = exp( βn
02) =1
2exp(β∗
01)−tn
2,exp(βn
0i) = exp( β∗
0(i−1))for any 3≤i≤k∗+ 1;
•βn
11=βn
12=β∗
11+cn1d,βn
1i=β∗
1(i−1)for any 3≤i≤k∗+ 1;
•an
1ℓ=an
2ℓ=a∗
1ℓ+cn,an
iℓ=a∗
(i−1)ℓfor any ℓ∈[K−1]and3≤i≤k∗+ 1;
•bn
1ℓ=bn
2ℓ=b∗
1ℓ,bn
iℓ=b∗
(i−1)ℓfor any ℓ∈[K−1]and3≤i≤k∗+ 1,
where tn, cn>0will be chosen later such that tn→0andcn=O(tn)asn→ ∞ . Then, it can be verified that
Dr(Gn, G∗) = (K−1)h
exp(β∗
01)−tni
cr
n(1 +dr/2) +tn. (28)
18

--- PAGE 19 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Now, we will show that EX[V(gGn(·|X), gG∗(·|X))]/Dr(Gn, G∗)vanishes as n→ ∞ . Let us reconsider the quantity
Tn(s) =hPk∗
j=1exp(( β∗
1j)⊤X+β∗
0j)i
·h
gGn(Y=s|X)−gG∗(Y=s|X)i
in equation (14) under the above setting of
Gnas follows:
Tn(s) =2X
i=1exp(βn
0i)[u(Y=s|X;βn
1i, an
i, bn
i)−u(Y=s|X;β∗
11, a∗
1, b∗
1)]
−2X
i=1exp(βn
0i)[v(Y=s|X;βn
1i)−v(Y=s|X;β∗
11)]
+ 2X
i=1exp(βn
0i)−exp(β∗
01)!
[u(Y=s|X;β∗
11, a∗
1, b∗
1)−v(Y=s|X;β∗
11)]
:=An−Bn+En,
where u(Y=s|X;β1, a, b) := exp( β⊤
1X)f(Y=s|X;a, b)andv(Y=s|X;β1) = exp( β⊤
1X)gGn(Y=s|X)for any
s∈[K]. Recall that we denote hℓ(X, a ℓ, bℓ) :=aℓ+b⊤
ℓXfor any ℓ∈[K]and
f(Y=s|X;ai, bi) =exp(ais+b⊤
isX)PK
ℓ=1exp(aiℓ+b⊤
iℓX)=exp(hs(X, a is, bis))PK
ℓ=1exp(hℓ(X, a iℓ, biℓ)).
Then, by means of Taylor expansion up to order r, we have
An=2X
i=1exp(βn
0i)rX
|α|=11
α!(βn
1i−β∗
11)α1K−1Y
ℓ=1(an
iℓ−a∗
1ℓ)α2ℓ(bn
iℓ−b∗
1ℓ)α3ℓ·Xα1+PK−1
ℓ=1α3ℓexp(( β∗
11)⊤X)
×∂PK−1
ℓ=1(α2ℓ+|α3ℓ|)f
∂hα21+|α31|
1 . . . ∂hα2(K−1)+|α3(K−1)|
K−1(Y=s|h∗
11, . . . , h∗
1K) +R5(X, Y),
Here, R5(X, Y)is a Taylor remainder that satisfies R5(X, Y)/Dr(Gn, G∗)→0asn→ ∞ . Since b∗
1ℓ=0dfor any
ℓ∈[K−1], the derivatives of fwith respect to h1, . . . , h K−1in the above representation of Anare constants depending
on(α2ℓ)K−1
ℓ=1and(α3ℓ)K−1
ℓ=1. Therefore, we can denote them as Cα2,α3.
Additionally, since βn
1i−β∗
11=0dandbn
iℓ−b∗
1ℓ=0dfor any ℓ∈[K−1]andi∈ {1,2}, we can let α1=0d,α3ℓ=0d,
and rewrite Anas follows:
An=rX
|α1|+|α2|=12X
i=1exp(βn
0i)
α1!α2!(βn
1i−β∗
11)α1K−1Y
ℓ=1(an
iℓ−a∗
1ℓ)α2ℓXα1exp(( β∗
11)⊤X)Cα2,0+R5(X, Y)
=rX
|α1|+|α2|=1exp(β∗
01)−tn
α1!α2!·c|α1|+|α2|
n ·Xα1exp(( β∗
11)⊤X)Cα2,0+R5(X, Y).
Similarly, by applying the Taylor expansion up to order rtoBn, we have
Bn=rX
|γ|=12X
i=11
γ!exp(βn
0i)(βn
1i−β∗
11)γ·Xγexp(( β∗
11)⊤X)gGn(Y=s|X) +R6(X, Y)
=rX
|γ|=1exp(β∗
01)−tn
γ!·c|γ|
n·Xγexp(( β∗
11)⊤X)gGn(Y=s|X) +R6(X, Y),
where R6(X, Y)is a Taylor remainder such that R6(X, Y)/Dr(Gn, G∗)→0asn→ ∞ .
Now, we aim to demonstrate that (An+En,1)/Dr(Gn, G∗)→0and(Bn+En,2)/Dr(Gn, G∗)→0asn→ ∞ , where
19

--- PAGE 20 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
we define
En,1:= 2X
i=1exp(βn
0i)−exp(β∗
01)!
u(Y=s|X;β∗
11, a∗
1, b∗
1),
En,2:= 2X
i=1exp(βn
0i)−exp(β∗
01)!
v(Y=s|X;β∗
11),
in which u(Y=s|X;β∗
11, a∗
1, b∗
1) = exp(( β∗
11)⊤X)f(Y=s|X;a∗
1, b∗
1) = exp(( β∗
11)⊤X)C0,0, and 0< C 0,0<1.
Part 1. Prove that (An+En,1)/Dr(Gn, G∗)→0asn→ ∞ .
SinceXis a bounded set, we assume that ∥X∥ ≤Bfor some positive constant M. Then, we can verify that
0≤An+En,1−R5(X, Y)≤rX
|α1|+|α2|=0Ln
α1,α2exp(( β∗
11)⊤X),
where we denote
Ln
α1,α2:=

exp(β∗
01)−tn
α1!α2!·c|α1|+|α2|
n ·B|α1|Cα2,0, |α1|+|α2|>0,
−tnC0,0, |α1|+|α2|= 0.
Note that for α1, α2such that 2≤ |α1|+|α2| ≤r, ascn=O(tn), we have Ln
α1,α2/Dr(Gn, G∗)→0asn→ ∞ . Now,
we show thatP1
|α1|+|α2|=0Ln
α1,α2= 0. Indeed, we have
1X
|α1|+|α2|=0Ln
α1,α2= [exp( β∗
01)−tn]·(C0,0B+N)cn−C0,0tn,
where N:=P
|α2|=1Cα2,0. By setting tn=B
nNandcn=1
nNexp(β∗
01)−B, the above sum reduces to zero. Thus, we get
that
[An+En,1−R5(X, Y)]/Dr(Gn, G∗)→0,
asn→ ∞ . Recall that R5(X, Y)/Dr(Gn, G∗)→0, we deduce that (An+En,1)/Dr(Gn, G∗)→0asn→ ∞ .
Part 2. Prove that (Bn+En,2)/Dr(Gn, G∗)→0asn→ ∞ .
It is worth noting that
0≤Bn+En,2−R6(X, Y)≤rX
|γ|=0Jn
γexp(( β∗
11)⊤X)gGn(Y=s|X),
where we denote
Jn
γ:=

exp(β∗
01)−tn
γ!·c|γ|
nB|γ|, |γ|>0,
−tn, |γ|= 0.
For2≤ |γ| ≤r, we have that Jn
γ/Dr(Gn, G∗)→0asn→ ∞ . Additionally, we have
1X
|γ|=0Jn
γ= [exp( β∗
01)−tn]·cn·B−tn= 0.
As a result, we get that
[Bn+En,2−R6(X, Y)]/Dr(Gn, G∗)→0,
20

--- PAGE 21 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
asn→ ∞ . Since R6(X, Y)/Dr(Gn, G∗)→0, we obtain that (Bn+En,2)/Dr(Gn, G∗)→0asn→ ∞ .
Putting the results of Part 1 and Part 2 together, we obtain that
Tn(s)
Dr(Gn, G∗)=An−Bn+En
Dr(Gn, G∗)→0,
which indicates that EX[V(gGn(·|X), gG∗(·|X))]/Dr(Gn, G∗)→0asn→ ∞ . Furthermore, it follows from equation (28)
thatDr(Gn, G∗)→0asn→ ∞ . Hence, we reach the conclusion of Proposition 3.2.
A.3. Proof of Theorem 4.4
To reach the desired conclusion in Theorem 4.4, we need to show the following key inequality:
inf
G∈Ok(Θ)EX[V(egG(·|X),egG∗(·|X))]/D2(G, G∗)>0, (29)
which is then divided into two parts named local structure and global structure. Since the global structure can be argued
similarly to that in Appendix A.1 with a note that the modified softmax gating multinomial logistic MoE model is identifiable
(see Proposition 4.2), the proof for it is omitted in this appendix.
Local Structure: For this part, we use the proof by contradiction method to show that
lim
ε→0inf
G∈Ok(Θ),
D2(G,G∗)≤εEX[V(egG(·|X),egG∗(·|X))]/D2(G, G∗)>0. (30)
Assume that this local inequality does not hold, then by utilizing some derivations in Appendix A.1, we proceed the
three-step framework as follows:
Step 1. First of all, by deriving in the same fashion as in equation (14), we get the following decomposition of eTn(s) :=hPk∗
j=1exp(( β∗
1j)⊤X+β∗
0j)i
·h
egGn(Y=s|X)−egG∗(Y=s|X)i
:
eTn(s) =k∗X
j=1X
i∈Cjexp(βn
0i)h
eu(Y=s|X;βn
1i, an
i, bn
i)−eu(Y=s|X;β∗
1j, a∗
j, b∗
j)i
−k∗X
j=1X
i∈Cjexp(βn
0i)h
ev(Y=s|X;βn
1i)−ev(Y=s|X;β∗
1j)i
+k∗X
j=1X
i∈Cjexp(βn
0i)−exp(β∗
0j)h
eu(Y=s|X;β∗
1j, a∗
j, b∗
j)−ev(Y=s|X;β∗
1j)i
:=An−Bn+En,
where we define
eu(Y=s|X;β1i, ai, bi) := exp( β⊤
1iM(X))·f(Y=s|X;ai, bi),
ev(Y=s|X;β1i) := exp( β⊤
1iM(X))·gGn(Y=s|X).
for any s∈[K]. Next, we will apply first order and second order Taylor expansions to two terms in the following sum:
An=X
j:|Cj|=1X
i∈Cjexp(βn
0i)h
eu(Y=s|X;βn
1i, an
i, bn
i)−eu(Y=s|X;β∗
1j, a∗
j, b∗
j)i
+X
j:|Cj|>1X
i∈Cjexp(βn
0i)h
eu(Y=s|X;βn
1i, an
i, bn
i)−eu(Y=s|X;β∗
1j, a∗
j, b∗
j)i
:=An,1+An,2.
21

--- PAGE 22 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
For the first term, we get that
An,1=X
j:|Cj|>1X
i∈Cjexp(βn
0i)X
|α|=11
α!(∆βn
1ij)α1K−1Y
ℓ=1(∆an
ijℓ)α2ℓ(∆bn
ijℓ)α3ℓ·[M(X)]α1XPK−1
ℓ=1α3ℓ
×exp(( β∗
1j)⊤M(X))·∂PK−1
ℓ=1(α2ℓ+|α3ℓ|)f
∂hα21+|α31|
1 . . . ∂hα2(K−1)+|α3(K−1)|
K−1(Y=s|X;a∗
j, b∗
j) +eR1(X, Y),
whereeR1(X, Y)is a Taylor remainder such that eR1(X, Y)/D2(Gn, G∗)→0asn→ ∞ . Let q2= (α2ℓ+|α3ℓ|)K−1
ℓ=1∈
NK−1,q3=PK−1
ℓ=1α3ℓ∈Ndandq4=α1∈Nd, we rewrite An,1as
An,1=X
j:|Cj|=1X
|q4|+|q2|=1|q2|X
|q3|=0X
i∈CjX
α∈Iq2,q3,q4exp(βn
0i)
α!(∆βn
1ij)α1K−1Y
ℓ=1(∆an
ijℓ)α2ℓ(∆bn
ijℓ)α3ℓ
×[M(X)]q4Xq3exp(( β∗
1j)⊤M(X))·∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j) +eR1(X, Y),
where we define
Iq2,q3,q4:=n
α= (α1, α21, . . . , α 2(K−1), α31, . . . , α 3(K−1)) :α1=q4,K−1X
ℓ=1α3ℓ=q3,(α2ℓ+|α3ℓ|)K−1
ℓ=1=q2o
.
For the second term An,2, we have
An,2=X
j:|Cj|>12X
|q4|+|q2|=1|q2|X
|q3|=0X
i∈CjX
α∈Iq2,q3,q4exp(βn
0i)
α!(∆βn
1ij)α1K−1Y
ℓ=1(∆an
ijℓ)α2ℓ(∆bn
ijℓ)α3ℓ
×[M(X)]q4Xq3exp(( β∗
1j)⊤M(X))·∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j) +eR2(X, Y),
whereeR2(X, Y)is a Taylor remainder such that eR2(X, Y)/D2(Gn, G∗)→0when n→ ∞ .
Meanwhile, by arguing similarly, we can decompose Bnas
Bn=X
j:|Cj|=1X
|γ|=1X
i∈Cjexp(βn
0i)
γ!(∆βn
1ij)γ×[M(X)]γexp(( β∗
1j)⊤M(X))gGn(Y=s|X) +eR3(X, Y)
+X
j:|Cj|>12X
|γ|=1X
i∈Cjexp(βn
0i)
γ!(∆βn
1ij)γ×[M(X)]γexp(( β∗
1j)⊤M(X))gGn(Y=s|X) +eR4(X, Y),
whereeR3(X, Y)andeR4(X, Y)are Taylor remainders such that their ratios to D2(Gn, G∗)vanish as napproaches infinity.
Combine the above results, we deduce that eTn(s)can be represented as follows:
eTn(s) =k∗X
j=11+1{|Cj|>1}X
|q4|+|q2|=0|q2|X
|q3|=0Zn
q2,q3,q4(j)×[M(X)]q4Xq3exp(( β∗
1j)⊤M(X))∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j)
+k∗X
j=11+1{|Cj|>1}X
|γ|=0Wn
γ(j)×[M(X)]γexp(( β∗
1j)⊤M(X))gGn(Y=s|X) +eR(X, Y), (31)
whereeR(X, Y)is the sum of Taylor remainders such that eR(X, Y)/D2(Gn, G∗)→0asn→ ∞ and
Zn
q2,q3,q4(j) =(P
i∈CjP
α∈Iq2,q3,q4exp(βn
0i)
α!(∆βn
1ij)α1QK−1
ℓ=1(∆an
ijℓ)α2ℓ(∆bn
ijℓ)α3ℓ,(q2, q3, q4)̸= (0K−1,0d,0d),
P
i∈Cjexp(βn
0i)−exp(β∗
0j), (q2, q3, q4) = (0K−1,0d,0d),
22

--- PAGE 23 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
and
Wn
γ(j) =(
−P
i∈Cjexp(βn
0i)
γ!(∆βn
1ij)γ, |γ| ̸=0d,
−P
i∈Cjexp(βn
0i) + exp( β∗
0j),|γ|=0d,
for any j∈[k∗].
Step 2. Subsequently, we will show that at least one among Zn
q2,q3,q4(j)/D2(Gn, G∗)does not approach zero as ntends to
infinity. Assume by contrary that all of them vanish as n→ ∞ , then we consider some typical tuples (q2, q3, q4). Firstly, by
taking the sum of Zn
q2,q3,q4(j)forj∈[k∗] :|Cj|>1(resp. j∈[k∗] :|Cj|= 1) and (q2, q3, q4)∈ {(0K−1,0d,2ei) :i∈
[d]}(resp. (q2, q3, q4)∈ {(0K−1,0d, ei) :i∈[d]}) where ei:= (0 , . . . , 0,1|{z}
i-th,0, . . . , 0)∈Rd, we get
1
D2(Gn, G∗)·X
j:|Cj|>1X
i∈Cjexp(βn
0i)∥∆βn
1ij∥2→0,
1
D2(Gn, G∗)·X
j:|Cj|=1X
i∈Cjexp(βn
0i)∥∆βn
1ij∥ →0. (32)
For(q2, q3, q4)∈ {(0K−1,2ei,0d) :i∈[d]}(resp. (q2, q3, q4)∈ {(0K−1, ei,0d) :i∈[d]}) where , we obtain that
1
D2(Gn, G∗)·X
j:|Cj|>1X
i∈Cjexp(βn
0i)K−1X
ℓ=1∥∆bn
ijℓ∥2→0,
1
D2(Gn, G∗)·X
j:|Cj|=1X
i∈Cjexp(βn
0i)K−1X
ℓ=1∥∆bn
ijℓ∥ →0. (33)
On the other hand, for (q2, q3, q4)∈ {(e′
ℓ,0d,0d) :ℓ∈[K−1]}(resp. (q2, q3, q4)∈ {(e′
ℓ,0d,0d) :ℓ∈[K−1]}) where
e′
ℓ:= (0 , . . . , 0,1|{z}
ℓ-th,0, . . . , 0)∈RK−1, we have
1
D2(Gn, G∗)·X
j:|Cj|>1X
i∈Cjexp(βn
0i)K−1X
ℓ=1∥∆an
ijℓ∥2→0, (34)
1
D2(Gn, G∗)·X
j:|Cj|=1X
i∈Cjexp(βn
0i)K−1X
ℓ=1∥∆an
ijℓ∥ →0. (35)
Additionally, when (q2, q3, q4) = (0K−1,0d,0d), it follows that
k∗X
j=1|Zn
0K−1,0d,0d(j)|
D2(Gn, G∗)=1
D2(Gn, G∗)·k∗X
j=1X
i∈Cjexp(βn
0i)−exp(β∗
0j)→0. (36)
It is induced from the limits in equations (32),(33),(34) and(36) that1 =D2(Gn, G∗)/D2(Gn, G∗)→0when n→ ∞ ,
which is a contradiction. Thus, at least one among Zn
q2,q3,q4(j)/D2(Gn, G∗)does not go to zero as n→ ∞ .
Step 3. Now, we denote emnas the maximum of the absolute values of Zn
q2,q3,q4(j)/D2(Gn, G∗)andWn
γ(j)/D2(Gn, G∗)
for any j∈[k∗],0≤ |q2|+|q4| ≤1 +1{|Cj|>1},0≤ |q3| ≤ |q2|and0≤ |γ| ≤1 +1{|Cj|>1}. Since at least one among
Zn
q2,q3,q4(j)/D2(Gn, G∗)does not go to zero as n→ ∞ , we deduce that emn̸→0, and therefore, 1/emn̸→ ∞ . Then, we
denote
Zn
q2,q3,q4(j)/[mnD2(Gn, G∗)]→eτq2,q3,q4(j)
Wn
γ(j)/D2(Gn, G∗)→eηγ(j)
23

--- PAGE 24 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
asn→ ∞ . Here, at least one among eτq2,q3,q4(j)forj∈[k∗],0≤ |q2|+|q4| ≤1 +1{|Cj|>1}and0≤ |q3| ≤ |q2|is
non-zero. By invoking the Fatou’s lemma, we have that
0 = lim
n→∞EX[2V(egGn(·|X),egG∗(·|X))]
mnD2(Gn, G∗)≥ZKX
s=1lim inf
n→∞|egGn(Y=s|X)−egG∗(Y=s|X)|
mnD2(Gn, G∗)dX≥0,
which indicates that [egGn(Y=s|X)−egG∗(Y=s|X)]/[emnD2(Gn, G∗)]tends to zero as ngoes to infinity for any s∈[K]
and almost surely X. This result is equivalent to
eTn(s)/[emnD2(Gn, G∗)]→0, (37)
asn→ ∞ , for any s∈[K]. Putting the results in equations (31) and (37) together, we have
k∗X
j=11+1{|Cj|>1}X
|q4|+|q2|=0|q2|X
|q3|=0eτq2,q3,q4(j)×[M(X)]q4Xq3exp(( β∗
1j)⊤M(X))∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j)
+k∗X
j=11+1{|Cj|>1}X
|γ|=0eηγ(j)×[M(X)]γexp(( β∗
1j)⊤M(X))gG∗(Y=s|X) = 0 . (38)
Regime 1: For any j∈[k∗], there exists an index ℓ∈[K−1]such that b∗
jℓ̸=0d.
By using the same arguments for proving the set Fin equation (26) is linearly independent for almost surely X, we get that
the following set also admits that property:
(
[M(X)]q4Xq3exp(( β∗
1j)⊤M(X))∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j),
[M(X)]γexp(( β∗
1j)⊤M(X))gG∗(Y=s|X) :j∈[k∗],0≤ |q2|+|q4|,|γ| ≤1 +1{|Cj|>1},0≤ |q3| ≤ |q2|)
.
As a result, it follows that eτq2,q3,q4(j) =eηγ(j) = 0 for any j∈[k∗],0≤ |q2|+|q4|,|γ| ≤1+1{|Cj|>1}and0≤ |q3| ≤ |q2|,
which contradicts the fact that at least one among eτq2,q3,q4(j)is different from zero.
Regime 2: There exists an index j∈[k∗]such that b∗
jℓ=0dfor any ℓ∈[K−1].
Note that equation (38) can be rewritten as
k∗X
j=1P(j)(X) exp(( β∗
1j)⊤M(X)) +k∗X
j=1Q(j)(X) exp(( β∗
1j)⊤M(X))gG∗(Y=s|X) = 0 ,
where we define
P(j)(X) :=1+1{|Cj|>1}X
|q4|+|q2|=0|q2|X
|q3|=0eτq2,q3,q4(j)Xq3[M(X)]q4·∂|q2|f
∂hq21
1. . . ∂hq2(K−1)
K−1(Y=s|X;a∗
j, b∗
j),
Q(j)(X) :=1+1{|Cj|>1}X
|γ|=0eηγ(j)[M(X)]γ.
Since the following set is linearly independent for almost surely X:
n
exp(( β∗
1j)⊤M(X)),exp(( β∗
1j)⊤M(X))gG∗(Y=s|X) :j∈[k∗]o
,
we achieve that P(j)(X) =Q(j)(X) = 0 for any j∈[k∗]for almost surely X. Then, it follows from the formulation of
P(j)(X)that
1+1{|Cj|>1}X
|q4|+|q2|=0|q2|X
|q3|=0eτq2,q3,q4(j)Xq3[M(X)]q4= 0,
24

--- PAGE 25 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
for any j∈[k∗]for almost surely X. It can be seen from the above equation that 0≤ |q3|+|q4| ≤ | q2|+|q4| ≤
1 +1{|Cj|>1}≤2. Moreover, by definition of function M(see Definition 4.1), the setn
Xp[M(X)]q:p, q∈Nd,0≤
|p|+|q| ≤2o
is linearly independent for almost surely X. As a consequence, we achieve that eτq2,q3,q4(j) = 0 for any
j∈[k∗],0≤ |q2|+|q4|,|γ| ≤1 +1{|Cj|>1}and0≤ |q3| ≤ |q2|, contradicting the fact that at least one among eτq2,q3,q4(j)
is non-zero.
Combine the results of the above two regimes, we reach the bound in equation (30).
B. Proofs for Convergence Rates of Density Estimation
In this appendix, we present the proof of Propostion 2.2 in Appendix B.1, while that for Proposition 4.3 is given in
Appendix B.2.
B.1. Proof of Proposition 2.2
In this appendix, we will firstly introduce key results on density estimation with MLE which are mainly based on (van de
Geer, 2000), and then provide the proof of Proposition 2.2 at the end.
B.1.1. K EYRESULTS
To begin with, it is necessary to define some notations that will be used in our presentation. First, we define Pk(Θ) :=
{gG(Y|X) :G∈ O k(Θ)}as the set of conditional density functions of all mixing measures belonging to Ok(Θ). In
addition, let N(ε,Pk(Θ),∥ · ∥ 1)be the covering number (van de Geer, 2000) of metric space (Pk(Θ),∥ · ∥ 1)while
HB(ε,Pk(Θ), h)be the bracketing entropy (van de Geer, 2000) of Pk(Θ)under the Hellinger distance. Then, the following
result gives us the upper bound of these quantities:
Lemma B.1. For any bounded set Θandε∈(0,1/2), we have
(i)logN(ε,Pk(Θ),∥ · ∥ 1)≲log(1/ε);
(ii)HB(ε,Pk(Θ), h)≲log(1/ε).
Proof of Lemma B.1. Part (i) Firstly, we define Ω :={(a, b)∈RK×Rd×K: (β0, β1, a, b)∈Θ}. Since Θis a compact
set, then Ωis also compact. Therefore, Ωadmits an ε-cover of size T2denoted by Ωε. In addition, we also define
∆ :={(β0, β1)∈R×Rd: (β0, β1, a, b)∈Θ}, and ∆εas an ε-cover of ∆. It can be validated that
|Ωε|≲O(ε−K(d+1)k),|∆ε|≲O(ε−(d+1)k).
Next, given a mixing measure G=Pk′
i=1exp(β0i)δ(β1i,ai,bi)∈ O k(Θ), where k′∈[k], we define eG:=Pk′
i=1exp(β0i)δ(β1i,ai,bi)in which (ai,bi)∈Ωεsuch that it is the closet point to (ai, bi)for any i∈[k′]. Addition-
ally, we also consider the mixing measure G:=Pk′
i=1exp(β0i)δ(β1i,ai,bi), where (β0i,β1i)∈∆εis the closest point to
(β0i, β1i). By this construction, it can be justified that gG∈ H, where we define
H:=
gG∈ Pk(Θ) : ( β0i,β1i)∈∆ε,(ai,bi)∈Ωε,∀i∈[k]	
.
Now, we show that His anε-cover of the metric space (Pk(Ω),∥ ·∥1)but not necessarily the smallest one. For that purpose,
we aim to find a bound for the term ∥gG−gG∥1. According to the triangle inequality, we have
∥gG−gG∥1≤ ∥gG−geG∥1+∥geG−gG∥1.
25

--- PAGE 26 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Regarding the first term in the above right hand side,
∥geG−gG∥1=KX
s=1Z
XgG(Y|X)−geG(Y|X)dX
=KX
s=1Z
Xk′X
i=1Softmax( β⊤
1iX+β0i)·f(Y=s|X;ai, bi)−f(Y=s|X;ai,bi)dX
≤KX
s=1Z
Xk′X
i=1Softmax( ais+b⊤
isX)−Softmax( ais+b⊤
isX)dX.
Since Softmax is a differentiable function, it is also a L-Lipschitz function where L >0. Additionally, as Xis a bounded
function, we may assume that ∥X∥ ≤Bfor some constant B >0. As a result, we have
∥geG−gG∥1≤KX
s=1k′X
i=1L·
|ais−ais|+∥X∥ · ∥bis−bis∥
≤Kk′L·(ε+Bε)≲ε.
Similarly, the second term is bounded as
∥geG−gG∥1=KX
s=1Z
Xk′X
i=1Softmax( β⊤
1iX+β0i)−Softmax( β⊤
1iX+β0i)·f(Y|X;ais,bis)dX
≤KX
s=1k′X
i=1L·
∥β1i−β1i∥ · ∥X∥+|β0i−β0i|
≤Kk′L(Bε+ε)≲ε.
Consequently, we get ∥gG−gG∥1≲ε. This result implies that His anε-cover of the metric space (Pk(Θ),∥ · ∥1). Then, it
follows from the definition of the covering number that
N(ε,Pk(Θ),∥ · ∥ 1)≤ |H| =|Θε| × |∆ε|=O(ε−K(d+1)k)× O(ε−(d+1)k) =O(ε−(K+1)(d+1)k),
which is equivalent to logN(ε,Pk(Θ),∥ · ∥ 1)≤log(1/ε).
Part (ii) Given ε >0and let η≤εthat we will choose later. Assume that Pk(Θ)has an η-cover denoted by {p1, p2, . . . , p N}
where N:=N(η,Pk(Θ),∥ · ∥ 1). Next, we start to construct brackets of the form [Li(Y|X), Ui(Y|X)]for all i∈[N]as
below:
Li(Y|X) := max {pi(Y|X)−η,0},
Ui(Y|X) := max {pi(Y|X) +η,1}.
By this construction, we can verify that Pk(Θ)⊂SN
i=1[Li(Y|X), Ui(Y|X)]andUi(Y|X)−Li(Y|X)≤min{2η,1}.
Additionally, we also have
∥Ui(·|X)−Li(·|X)∥1=KX
ℓ=1[Ui(Y=ℓ|X)−Li(Y=ℓ|X)]≤2Kη.
By definition, since HB(2Kη,Pk(Θ),∥ · ∥ 1)is the logarithm of the smallest number of brackets of size 2Kηrequired for
covering Pk(Θ), we obtain that
HB(2Kη,Pk(Θ),∥ · ∥ 1)≤logN(η,Pk(Θ),∥ · ∥ 1)≤log(1/η),
where the last inequality follows from the result of Part (i). Thus, by choosing η=ε/(2K), we have HB(ε,Pk(Θ),∥·∥1)≲
log(1/ε). Furthermore, as h≤ ∥ · ∥ 1, we achieve the desired conclusion:
HB(ε,Pk(Θ), h)≲log(1/ε).
Hence, the proof is completed.
26

--- PAGE 27 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Subsequently, we denote
Pk(Θ) := {g(G+G∗)/2(Y|X) :G∈ Ok(Θ)},
P1/2
k(Θ) := {g1/2
(G+G∗)/2(Y|X) :G∈ Ok(Θ)}.
For any ξ >0, the Hellinger ball centered around the true conditional density gG∗(Y|X)and intersected with the set
P1/2
k(Θ)is defined as
P1/2
k(Θ, ξ) :={g1/2∈Pk(Θ) :EX[h(g(·|X), gG∗(·|X))]≤ξ}.
Moreover, Geer et al. (van de Geer, 2000) proposes the following term to capture the size of the Hellinger ball P1/2
k(Θ, ξ):
JB(ξ,P1/2
k(Θ, ξ)) :=Zξ
ξ2/213H1/2
B(t,P1/2
k(Θ, t),∥ · ∥)dt∨ξ, (39)
where t∨ξ:= max {t, ξ}. Now, let us recall below an important result regarding the density estimation rate in (van de Geer,
2000) with adapted notations of this paper.
Lemma B.2 (Theorem 7.4, (van de Geer, 2000)) .TakeΨ(ξ)≥ JB(ξ,P1/2
k(Θ, ξ))such that Ψ(ξ)/ξ2is a non-increasing
function of ξ. Then, given a universal constant cand a sequence (ξn)that satisfies√nξ2
n≥cΨ(ξn), we get
P
EX[h(gbGn(·|X), gG∗(·|X))]> ξ
≤cexp
−nξ2
c2
,
for any ξ≥ξn.
The proof of this Lemma is in (van de Geer, 2000).
B.1.2. M AINPROOF
It is worth noting that HB(t,P1/2
k(Θ, t),∥ · ∥)≤HB(t,Pk(Θ), h)for any t >0. Then, equation (39) indicates that
JB(ξ,P1/2
k(Θ, ξ))≤Zξ
ξ2/213H1/2
B(t,P1/2
k(Θ, t), h)dt∨ξ≲Zξ
ξ2/213log(1/t)dt∨ξ,
where the second inequality follows from part (ii) of Lemma B.1. By setting Ψ(ξ) =ξp
log(1/ξ)such that Ψ(ξ)≥
JB(ξ,P1/2
k(Θ, ξ))andξn=ξp
log(1/ξ), Lemma B.2 gives us that
P
EX[h(gbGn(·|X), gG∗(·|X))]> ξ
≤cexp
−nξ2
c2
,
where Candcare universal positive constants depending only on Θ. Hence, the proof is completed.
B.2. Proof of Proposition 4.3
From Definition 4.1, since M(X)is a bounded function of X, the arguments presented in Appendix B.1 still hold under the
modified softmax gating multinomial logistic mixture of experts.
C. Proofs for the Identifiablity of the (Modified) Softmax Gating Multinomial Logistic MoE
In this appendix, we provide the proofs of Proposition 2.1 and Proposition 4.2 in Appendix C.1 and Appendix C.2,
respectively.
27

--- PAGE 28 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
C.1. Proof of Proposition 2.1
From the assumption of Proposition 2.1, the following equation holds for any s∈[K]and almost surely X∈ X:
kX
i=1exp(( β1i)⊤X+β0i)
Pk
j=1exp(( β1i)⊤X+β0i)·exp(ais+ (bis)⊤X)PK
ℓ=1exp(aiℓ+ (biℓ)⊤X)
=k′X
i=1exp(( β′
1i)⊤X+β′
0i)
Pk′
j=1exp(( β′
1i)⊤X+β′
0i)·exp(a′
is+ (b′
is)⊤X)PK
ℓ=1exp(a′
iℓ+ (b′
iℓ)⊤X). (40)
According to (Grün & Leisch, 2008), the multinomial logistic mixtures are identifiable, which implies that two mixing
measures GandG′share the number of experts and the gating set of the mixing measure, i.e. k=k′and
(
exp(( β1i)⊤X+β0i)
Pk
j=1exp(( β1i)⊤X+β0i):i∈[k])
≡(
exp(( β′
1i)⊤X+β′
0i)
Pk
j=1exp(( β′
1i)⊤X+β′
0i):i∈[k])
,
for almost surely X∈ X. WLOG, we assume that
exp(( β1i)⊤X+β0i)
Pk
j=1exp(( β1i)⊤X+β0i)=exp(( β′
1i)⊤X+β′
0i)
Pk
j=1exp(( β′
1i)⊤X+β′
0i),
for any i∈[k]. Asβ1k=β′
1k=0dandβ0k=β′
0k= 0, the above result leads to β1i=β′
1iandβ0i=β′
0ifor all i∈[k].
Thus, the equation (40) becomes
kX
i=1exp(β0i)u(Y=s|X;β1i, ai, bi) =kX
i=1exp(β0i)u(Y=s|X;β1i, a′
i, b′
i), (41)
for any s∈[K]and almost surely X∈ X, where u(Y=s|X;β1i, ai, bi) := exp( β⊤
1iX)·exp(ais+ (bis)⊤X)PK
ℓ=1exp(aiℓ+ (biℓ)⊤X)and
ai= (ai1, ai2, . . . , a iK),bi= (bi1, bi2, . . . , b iK).
Subsequently, we will consider rsubsets of the set [k], denoted by S1, S2, . . . , S rthat satisfy the following property:
exp(β0i) = exp( β0i′)for any i, i′∈Stfort∈[r]. Therefore, we can rewrite equation (41) as
rX
t=1X
i∈Stexp(β0i)u(Y=s|X;β1i, ai, bi) =rX
t=1X
i∈Stexp(β0i)u(Y=s|X;β1i, a′
i, b′
i),
for any s∈[K]and almost surely X∈ X . It follows from the above equation that for each t∈[r], we get {(aiℓ+
(biℓ)⊤X)K
ℓ=1:i∈St} ≡ { (a′
iℓ+ (b′
iℓ)⊤X)K
ℓ=1:i∈St}for almost surely X∈ X. This leads to
n
(ai1, . . . , a iK, bi1, . . . , b iK) :i∈Sto
≡n
(a′
i1, . . . , a′
iK, b′
i1, . . . , b′
iK) :i∈Sto
.
Again, we may assume WLOG that (ai1, . . . , a iK, bi1, . . . , b iK) = (a′
i1, . . . , a′
iK, b′
i1, . . . , b′
iK)for any i∈St. As a result,
we obtain that
rX
t=1X
i∈Stexp(β0i)δ(β1i,ai1,...,a iK,bi1,...,biK)=rX
t=1X
i∈Stexp(β′
0i)δ(β′
1i,a′
i1,...,a′
iK,b′
i1,...,b′
iK).
In other words, we achieve that G≡G′, which completes the proof.
C.2. Proof of Proposition 4.2
According to the assumption of Proposition 4.2, the following equation holds for any s∈[K]and almost surely X∈ X:
kX
i=1exp(( β1i)⊤M(X) +β0i)
Pk
j=1exp(( β1i)⊤M(X) +β0i)·exp(ais+ (bis)⊤X)PK
ℓ=1exp(aiℓ+ (biℓ)⊤X)
=k′X
i=1exp(( β′
1i)⊤M(X) +β′
0i)
Pk′
j=1exp(( β′
1i)⊤M(X) +β′
0i)·exp(a′
is+ (b′
is)⊤X)PK
ℓ=1exp(a′
iℓ+ (b′
iℓ)⊤X). (42)
28

--- PAGE 29 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Since the multinomial logistic mixtures are identifiable (see (Grün & Leisch, 2008)), two mixing measures GandG′admit
the same number of experts and the same gating set, i.e. k=k′and
(
exp(( β1i)⊤M(X) +β0i)
Pk
j=1exp(( β1i)⊤M(X) +β0i):i∈[k])
≡(
exp(( β′
1i)⊤M(X) +β′
0i)
Pk
j=1exp(( β′
1i)⊤M(X) +β′
0i):i∈[k])
,
for almost surely X∈ X. WLOG, we assume that
exp(( β1i)⊤X+β0i)
Pk
j=1exp(( β1i)⊤X+β0i)=exp(( β′
1i)⊤X+β′
0i)
Pk
j=1exp(( β′
1i)⊤X+β′
0i),
for any i∈[k]. From the Definition 4.1, we know that M(X)is a bounded function of X. Moreover, since β1k=β′
1k=0d
andβ0k=β′
0k= 0, the above result implies that β1i=β′
1iandβ0i=β′
0ifor all i∈[k]. Therefore, the equation (42) can
be reformulated as follows:
kX
i=1exp(β0i)u(Y=s|X;β1i, ai, bi) =kX
i=1exp(β0i)u(Y=s|X;β1i, a′
i, b′
i),
for any s∈[K]and almost surely X∈ X, where u(Y=s|X;β1i, ai, bi) := exp( β⊤
1iX)·exp(ais+ (bis)⊤X)PK
ℓ=1exp(aiℓ+ (biℓ)⊤X)and
ai= (ai1, ai2, . . . , a iK),bi= (bi1, bi2, . . . , b iK). Then, we can apply the arguments used in Appendix C.1 to deduce that
G≡G′.
D. Simulation Studies
In this appendix, we carry out several numerical experiments to empirically verify our theoretical results regarding the
convergence rates of maximum likelihood estimation in the standard softmax gating multinomial logistic MoE model under
the Regime 1 in Appendix D.1. Meanwhile, under the Regime 2, we aim to empirically demonstrate the benefits of using
modified softmax gating functions over the standard softmax gating function in the parameter estimation problem of the
multinomial logistic MoE model in Appendix D.2.
D.1. Regime 1
Synthetic Data. We first sample the covariates Xfrom the uniform distribution over [0,1]. Then, we draw the response Y
from the following conditional density gG∗(Y=s|X)of a softmax gating binomial logistic mixture of k∗= 2experts:
gG∗(Y=s|X) :=2X
i=1exp(( β∗
1i)⊤X+β∗
0i)P2
j=1exp(( β∗
1j)⊤X+β∗
0j)×exp(a∗
is+ (b∗
is)⊤X)PK
ℓ=1exp(a∗
iℓ+ (b∗
iℓ)⊤X), (43)
fors∈[K], where K= 2. Here, the true mixing measure G∗=P2
i=1exp(β∗
0i)δ(β∗
1i,a∗
i1,a∗
i2,b∗
i1,b∗
i2)consists of k∗= 2
components with parameters given in Table 2, which satisfy the assumptions of Regime 1.
Gating parameters Expert parameters
Class 1 Class 2
i= 1 ( β∗
01, β∗
11) = (1 ,3) ( a∗
11, b∗
11) = (−1,2) ( a∗
12, b∗
12) = (0 ,0)
i= 2 ( β∗
02, β∗
12) = (0 ,0) ( a∗
21, b∗
21) = (1 ,−1) ( a∗
22, b∗
22) = (0 ,0)
Table 2. True parameters under the Regime 1.
Initialization. We then compute the MLE bGnw.r.t. with the number of components k∈ {k∗+ 1, k∗+ 2}for each sample
using the EM algorithm (Dempster et al., 1977) with convergence criterion ε= 10−6and2000 maximum EM iterations.
29

--- PAGE 30 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
For each k∈ {k∗+ 1, k∗+ 2}, we randomly assign elements of the set {1,2, ..., k}intok∗distinct sets S1, S2, . . . , S k∗,
ensuring that each set contains at least one element. Moreover, we repeat this process for each replication. Following this,
for each i∈[k∗], we initialize the parameters by sampling from a Gaussian distribution with a mean centered around its true
counterpart and a small variance. For each of 200different choices of sample size nbetween nmin= 104andnmax= 105,
we generate 40samples of size n. All the code for our simulation study was written in Python 3.9 on a standard Unix
machine.
Empirical Convergence Rates. Subsequently, we report the empirical means of the discrepancy D2between bGnandG∗,
and the choices of kunder the Regime 1 in Figure 1. It can be observed from Figures 1a and 1b that the empirical vanishing
rates of the average discrepancy D2(bGn, G∗)are of orders eO(n−0.48)andeO(n−0.43)when k= 3andk= 4, respectively.
These rates are slightly slower than the theoretical rate of order eO(n−0.5)in Theorem 3.1. The main reason is that there
has been only theoretical guarantee of global convergence for the parameter estimation under the mixture of experts with
covariate-free gating function (see (Kwon et al., 2021; Kwon & Caramanis, 2020; Kwon et al., 2019)), while that for the
softmax gating mixture of experts has remained missing in the literature. In order for the empirical vanishing rate to match
the theoretical one, the sample size nmust be large enough to compensate for the global convergence problem.
10.0 10.5 11.0 11.5
log(sample size)1
01234log(loss)4.8n0.48195
2(Gn,G*)
(a) Over-specified setting with k= 3experts
10.25 10.50 10.75 11.00 11.25 11.50
log(sample size)0.00.51.01.52.02.5log(loss)5.0n0.43467
2(Gn,G*)
 (b) Over-specified setting with k= 4experts
Figure 1. Two log-log scaled plots for the empirical convergence rates of the MLE bGnwhen the true model in equation (43) is over-
specified by a softmax gating binomial logistic mixture of k= 3andk= 4experts, respectively. In these figures, the empirical means of
the discrepancy D2(bGn, G∗)are illustrated by the blue curves, while the oranges dash-dotted lines represent for the least-squares fitted
linear regression lines.
D.2. Regime 2
Synthetic Data. We also generate the covariates Xfrom the uniform distribution over [0,1]. For the multinomial logistic
MoE model with standard softmax gating function, we draw the response Yfrom the conditional density gG∗(Y=s|X)
given in equation (43), while for the modified softmax gating function, we sample Yfrom the following conditional density:
egG∗(Y=s|X) :=2X
i=1exp(( β∗
1i)⊤M(X) +β∗
0i)P2
j=1exp(( β∗
1j)⊤M(X) +β∗
0j)×exp(a∗
is+ (b∗
is)⊤X)PK
ℓ=1exp(a∗
iℓ+ (b∗
iℓ)⊤X), (44)
where we consider the standard softmax gating function M(X) =XandM(X) = sigmoid( X). Next, while we keep the
formulation of the true mixing measure G∗=P2
i=1exp(β∗
0i)δ(β∗
1i,a∗
i1,a∗
i2,b∗
i1,b∗
i2), the parameter values are slightly changed
to satisfy the assumptions of Regime 2. In particular, we set b∗
21=b∗
22= 0, while other parameters remains the same. More
details can be found in Table 3.
D.2.1. E MPIRICAL CONVERGENCE RATES OF THE VORONOI -BASED LOSS
Initialization. We first initialize fitted parameters in the same fashion as those in Appendix D.1.
Standard Gating Function. Fork= 3we choose 35different values of sample size nbetween nmin≈37×103and
nmax= 105, while for k= 4we select 28different choices of sample size nbetween nmin≈50×103andnmax= 105.
30

--- PAGE 31 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
Gating parameters Expert parameters
Class 1 Class 2
i= 1 ( β∗
01, β∗
11) = (1 ,3) ( a∗
11, b∗
11) = (−1,2) ( a∗
12, b∗
12) = (0 ,0)
i= 2 ( β∗
02, β∗
12) = (0 ,0) ( a∗
21, b∗
21) = (1 ,0) ( a∗
22, b∗
22) = (0 ,0)
Table 3. True parameters under the Regime 2.
105 4×1046×104
Sample Size101LossD2(/hatwideGn,G∗)
1028.7n−0.34
(a) Over-specified setting with k= 3experts,
M(X) =X
105 5×1046×1047×1048×1049×104
Sample Size101102LossD2(/hatwideGn,G∗)
998.8n−0.21(b) Over-specified setting with k= 4experts,
M(X) =X
104105
Sample Size100101Loss
137.3n−0.43
D2(/hatwideGn,G∗)
(c) Over-specified setting with k= 3experts,
M(X) = sigmoid( X).
104105
Sample Size100101Loss
918.3n−0.49
D2(/hatwideGn,G∗)(d) Over-specified setting with k= 4experts,
M(X) = sigmoid( X).
Figure 2. log-log scaled plots for the empirical convergence rates of the MLE bGnwhen the true model in equation (44) is over-specified
by a softmax gating binomial logistic mixture with M(X) =XandM(X) = sigmoid( X)ofk= 3andk= 4experts, respectively. In
these figures, the empirical means of the discrepancy D2(bGn, G∗)are illustrated by the blue curves, while the oranges dash-dotted lines
represent for the least-squares fitted linear regression lines.
In both cases we generate the corresponding samples of size n.
Modified Gating Function. We employ M(X) = sigmoid( X)and conduct 40 experiments for each sample size, covering
a spectrum of 20 different sample sizes ranging from 104to105.
Empirical Convergence Rates. Subsequently, we report the empirical means of the discrepancy D2between bGnandG∗,
and the choices of kunder the Regime 2 for standard gating function and modified gating function in Figure 2. It can be
observed from Figures 2a and 2b that the empirical vanishing rates of the average discrepancy D2(bGn, G∗)are of orders
eO(n−0.34)andeO(n−0.21)when k= 3andk= 4, respectively. These rates are slightly slower than the theoretical rate
of order eO(n−1/2r)for some r≥1in Theorem 3.3. Moreover, for M(X) = sigmoid( X)as illustrated in Figures 2c and
31

--- PAGE 32 ---
A General Theory for Softmax Gating Multinomial Logistic Mixture of Experts
2d the utilization of M(X) = sigmoid( X)in the gating function resulted in an enhanced convergence rate of eO(n−1/2)
for both k= 3andk= 4. The main reasons are that the literature lacks a theoretical guarantee of global convergence for
parameter estimation under the softmax mixture of experts, and that the MLE bGn(which is the standard softmax mixture
function) takes a very long time to converge to the true mixture measure G∗even when we use 2000 maximum number of
EM iterations, see more in Appendices D.1 and D.2.2.
D.2.2. E MPIRICAL CONVERGENCE RATES OF THE EM A LGORITHM
Initialization. We assume that the MLEs bGn(w.r.t. the standard softmax gating function) and eGn(w.r.t. the modified
softmax gating functions) have k= 3components. Next, we initialize fitted parameters in the same fashion as those in
Appendix D.1. With the sample size n= 104, we run the EM algorithm for N= 200 iterations, and compute the negative
log-likelihood value at each iteration.
100101102
Number of EM Iteration0.680
0.678
0.676
0.674
0.672
0.670
Negative Log-LikelihoodStandard
M(x)=cosx
M(x)=sinx
M(x)=log|x|
Figure 3. Empirical convergence rates of the EM algorithm with the standard softmax gating function and three different modified softmax
gating functions with M(X)∈ {sin(X),cos(X),log(|X|)}. The y-axis indicates the negative log-likelihood, while the x-axis illustrates
the number of EM iterations.
Negative Log-likelihood. It can be seen from Figure 3 that the negative log-likelihood corresponding to the modified
softmax gating function with M(X) = cos( X)experiences a sharp drop after 200 iterations. Meanwhile, those for
M(X) = sin( X)andM(X) = log( |X|)decline at a nearly same rate but slower than that for M(X) = cos( X). On the
other hand, the negative log-likelihood corresponding to the standard softmax gating function remains almost unchanged.
Those observations suggest that it would take a very long time for the MLE bGn(w.r.t. the standard softmax gating
function) to converge to the true mixing measure G∗. By contrast, if we use the modified softmax gating functions with
M(X)∈ {sin(X),cos(X),log(|X|)}, the convergence rates of the corresponding MLE eGntoG∗would be substantially
faster. As a consequence, this figure highlights the advantages of using the modified softmax gating functions over the
standard softmax gating function in the parameter estimation of the multinomial logistic MoE model.
32
