# 2202.01169.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2202.01169.pdf
# File size: 1715249 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
UNIFIED SCALING LAWS FOR ROUTED LANGUAGE MODELS
Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch
Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtmanz, Trevor Cai, Sebastian Borgeaud,
George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnsonz, Katie Millican,
Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero,
Oriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu, Karen Simonyan
DeepMind Google Researchz
ABSTRACT
The performance of a language model has been shown to be effectively modeled as a power-law in
its parameter count. Here we study the scaling behaviors of Routing Networks : architectures that
conditionally use only a subset of their parameters while processing an input. For these models,
parameter count and computational requirement form two independent axes along which an increase
leads to better performance. In this work we derive and justify scaling laws deﬁned on these two
variables which generalize those known for standard language models and describe the performance
of a wide range of routing architectures trained via three different techniques. Afterwards we provide
two applications of these laws: ﬁrst deriving an Effective Parameter Count along which all models
scale at the same rate, and then using the scaling coefﬁcients to give a quantitative comparison of the
three routing techniques considered. Our analysis derives from an extensive evaluation of Routing
Networks across ﬁve orders of magnitude of size, including models with hundreds of experts and
hundreds of billions of parameters.
1 Introduction
It is a commonly held belief that increasing the size of a neural network leads to better performance, especially when
training on large and diverse real-world datasets. This vague and debated notion has become increasingly justiﬁed as
large empirical studies have shown that the performance of models on many interesting classes of problems are well
understood as power-laws; where a multiplicative increase in model size leads to an additive reduction in the model’s
loss [Kaplan et al., 2020, Hernandez et al., 2021, Henighan et al., 2020, Rosenfeld et al., 2019]. These relationships are
not well understood, but a key implication is that a sequence of small1models can be used both to infer the performance
of models many times more powerful, but also to provide global information about the scalability of an architecture.
Enter Routing Networks: models with the unusual property that each input interacts with only a subset of the network’s
parameters — chosen independently for each datapoint [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]. For
a Routing Network, the number of parameters is nearly independent from the computational cost of processing a
datapoint. This bifurcates the deﬁnition of size and prevents a scaling law in parameters alone from fully describing the
model class. Speciﬁc Routing Networks have been trained successfully at large scales [Fedus et al., 2021, Du et al.,
2021, Artetxe et al., 2021], but the general scaling behavior is not well understood. In this work we analyze the behavior
of routed language models so that we might infer the scaling laws that describe their performance.
Correspondence to aidan.b.clark@gmail.com, diegolascasas@deepmind.com. All afﬁliation to DeepMind unless noted.
*Shared ﬁrst authorship.
1Measured as training or inference ﬂoating point operations, devices or time required, ﬁnancial cost, carbon emissions, etc.arXiv:2202.01169v2  [cs.CL]  9 Feb 2022

--- PAGE 2 ---
Uniﬁed Scaling Laws for Routed Language Models
12481632 128 512
Expert Count2.02.42.62.83.03.2Validation Loss15M 
25M 
55M 
130M 
370M 
1.3B a) Predicting Loss for Varying Expert Count
1.3B 370M 130M55M25M 5M
Dense Model Size1248163264128256512Expert Countb) Curves of Constant Loss Value
100M 1B 10B
Effective Parameter Count2.02.42.62.83.03.2Validation Loss
c) Unified Model ScalingS-BASE RL-R Hash Dense LM
Figure 1: (a)The performance achieved by Routing Networks when varying the number of experts for a ﬁxed dense
model size is described by a bilinear function (Eq. 1), (b)whose level curves indicate how to trade model size with
expert count to maintain a ﬁxed performance, (c)and which can be manipulated to align dense and routed model
performance under a shared power law.
Key contributions. We analyze three different techniques for training Routing Networks, detailed in §3:
Sinkhorn- BASE , a sparse mixture-of-experts ( SMOE ) approach modifying BASE [Lewis et al., 2021]; non-parametric
HASH Layers [Roller et al., 2021]; and routing via Reinforcement Learning ( RL-R). With models up to 200 billion
parameters, we observe the following:
1. Routing improves the performance of language models across all sizes and variants attempted (see Fig. 1).
2.Training a Routing Network with RL (§3.3), a technique used in early routing work [Bengio et al., 2013], is of
comparable effectiveness to state-of-the-art techniques.
3.The performance of all Routing Networks is accurately described by scaling laws in the number of experts and in
the underlying dense model size (§4) which generalize those from Kaplan et al. [2020].
4.These laws can be restated in terms of parameter count and inference compute, capturing an even wider set of
routing architectures under a shared ﬁt (§4.4).
5.They further imply an Effective Parameter Count : a mapping equating the performance and scaling for both dense
and routed networks (§5).
2 Background
We ﬁrst review the language modelling problem and existing scaling laws before discussing the process of routing a
neural network and how it is applied to language models.
Language modelling. We consider the problem of autoregressively predicting natural language, a task with consistent
and predictable scaling characteristics across many orders of magnitude [Henighan et al., 2020, Kaplan et al., 2020].
The objective is to maximize the likelihood of a sequence of tokens P(x1;:::;xT)factored auto-regressively as
p(x1;:::;xT) =QT
ip(xijxj<i). Our primary metric of performance is the negative log-likelihood of a validation
dataset whose statistics match the training distribution. We focus on this validation loss, but brieﬂy consider zero-shot
transfer to other tasks in App. E.
Scaling laws for large-scale data. We train on a multi-trillion-token compendium of English language text compris-
ing documents from the internet alongside open-source text datasets, details of which are given in Rae et al. [2021]. In
this setting Kaplan et al. [2020] argue that the converged performance of a model trained on a dataset of inﬁnite size is a
power-law in the model’s parameter count N. Our dataset is not inﬁnite, but its size – and the lack of any observed
overﬁtting – make this a reasonable approximation. We consider the ﬁnal (and best) evaluation value as the converged
value, though this is also an approximation which is discussed further in App. F.
2.1 Routing Networks
Power-law scaling implies the performance of a language model increases with size, but so too does the compute needed
to train the model. This undesirable connection between size and computation motivates a search for architectures
wherein the two are disentangled. Routing Networks are one such class of model: a type of neural network that
incorporates a speciﬁc ﬂavor of conditional computation. In a Routing Network, each input (e.g., a token of text) is
2

--- PAGE 3 ---
Uniﬁed Scaling Laws for Routed Language Models
transformed into an output while only interacting with a ﬁxed subset of the network’s parameters – dynamically selected
based on the input itself. Many sparsely-activated networks have this property, but here we exclusively study the layout
based on Sparse Mixtures of Experts [Shazeer et al., 2017] where multiple sub-components of a deep neural network
(i.e., several layers) are independently converted to routed equivalents and jointly trained with the rest of the network.
Routing a single layer. The core idea of a routed layer is that multiple versions of the parameters are kept, and a
per-input decision on which version to use is made. To route a layer finEways, we start by creating Eseparate
versions of the parameters (f1;:::Eg) wherefusing thei-th version of the parameters ( fi,fi) is termed the i-th
Expert . To determine which expert to pick given the input, we introduce an additional router function:RM![1;E]
associated to the layer, typically a small network itself, with parameters '. The routed form hoffis then given by
h(x),f(x)(x). When performance increases with E, routing gives a method by which to improve a neural network
with minimal computational increase (corresponding only to the compute needed by (x)).
We also consider the K-way routed generalization, where the router outputs a set of integers as () :RM![1;E]K,
and we set the output of the layer to be the sum of the outputs of each expert, namely h(x),P
i2(x)fi(x). We
default toK= 1, but revisit this in §4.4.
Routed Transformers We apply routing to a decoder-only Transformer [Vaswani et al., 2017] to measure the scaling
properties that result: an architecture chosen due to its state-of-the-art performance. Details of the baseline architecture
we use are in App. A. We will refer to non-routed Transformers as dense models, in opposition to Routed Transformers
which sparsely activate some of their parameters. Our conversion to a Routed Transformer is the same as is used in
prior work [Lepikhin et al., 2020, Fedus et al., 2021]. Namely, we apply routing to every other set of feedforward
components (FFWs) of the Transformer, sub-components that act on each timestep independently. Though different
layers can have different numbers of experts, here all routed layers share the same number of experts E, and we will
refer to the network as being routedEways .
Model size and inference cost. We useNto indicate a network’s dense model size : the number of parameters any
one input interacts with. This is in opposition to P: the total number of parameters. For a dense model, P=N,
whereas for a Routing Network Pis roughly proportional to NE, with factors that depend on details of the routing
architecture (§4.4). Except for a small overhead due to running the routers, the cost F(in TeraFLOPs) of executing a
Routed Transformer is the same as its dense equivalent.
Training Details. All models are trained on TPUs with JAX [Bradbury et al., 2018] using a combination of data,
expert (see App. C) and sharding parallelism [Shoeybi et al., 2019]. Models were trained with a sequence length of
2048 and batch size of 256for 250,000 steps, i.e. 130 billion tokens, regardless of N. This is an important detail, and
we discuss some of the implications in App. F. All were optimized with AdamW [Loshchilov and Hutter, 2018] and
ZeRO Stage 1 was used to shard the optimizer state [Rajbhandari et al., 2020]. Appendix A contains further details.
3 Routing Techniques
If the beneﬁt of Routing Networks is the decoupling of parameter capacity from network cost, the fundamental difﬁculty
is in effectively learning the parameters 'of the router given the non-differentiability of its output. Much research
in Routing Networks has therefore focused on techniques for learning '. A major ﬁnding of this work is that three
notably different techniques of training Routing Networks are effectively described by the same scaling laws. We now
introduce and contextualize these three methods.
3.1 Sparse Mixture-of-Experts via Weighting
Sparse Mixture-of-Experts ( SMOE ) methods [Shazeer et al., 2017] solve the problem of non-differentiability by reusing
the probability of expert selection as a scalar multiplier on that expert’s output, guaranteeing a gradient passed to the
logits of selected experts despite the the non-differentiability of sampling from those logits. Formally, the router is
given as(x) = topk(Wx+b), whereWx+bis an unnormalized distribution over [1;E]from which the experts
corresponding to the top Kvalues are selected. In the ﬁnal output of the routed layer, the normalized logits are reused as
gating weights , i.e. the ﬁnal output of the routed layer is h(x) =P
i2(x)gi(x)fi(x)whereg(x) = softmax( Wx+b).
Though this formulation supplies a gradient to '= (W;b), it represents changes to the scalar multiplier and does not
directly correspond to optimizing expert selection. This method is nevertheless effective, and can be seen as a sparse
3

--- PAGE 4 ---
Uniﬁed Scaling Laws for Routed Language Models
124832128 512
Expert Count2.02.42.62.83.03.2Validation Loss15M
25M
55M
130M
370M
1.3BS-BASE
1248 32 128 512
Expert CountRL-R
1248 32 128 512
Expert CountHash
1248 32 128 512
Expert CountComparisons
Figure 2: Validation losses with ﬁts from Equation 1 plotted as a dotted line for S-BASE ,HASH and RL-Rrespectively.
On the right, the prediction curves for all model sizes and all techniques overlapping to show relative performance. Fits
to Eq. (7) are overlaid in grey.
approximation to dense mixture of experts models [Eigen et al., 2014, Jacobs et al., 1991] where the likelihood of
skipping an expert is inversely proportional to the value of its scalar gate gi.
It was conjectured that SMOE s require (K>2)-way routing to produce effective gradients in the routers [Shazeer et al.,
2017], and many attempts at incorporating routing into large Transformers use K= 2[Lepikhin et al., 2020, Du et al.,
2021]. However recently this has been challenged, and stable modiﬁcations have been proposed for K= 1; namely the
Switch Transformer [Fedus et al., 2021]. Most SMOE s, including Switch, are reliant on auxiliary balancing losses which
encourage the router output (x)to be more uniform across minibatches of inputs. To improve on this, BASE [Lewis
et al., 2021] post-processes the router output with a Hungarian Matching algorithm that re-assigns expert selections to
ensure that all experts are selected evenly.
Our implementation of BASE replaces the Hungarian Matching with a regularized Optimal Transport formulation
[Cuturi, 2013] using the Sinkhorn algorithm as an approximate matching step during expert selection. This substantially
improves routing efﬁciency on accelerated hardware (details in §B.2.1). We call the resulting method Sinkhorn- BASE
(S-BASE ), and use it as the representative of SMOE methods, as early tests showed the beneﬁt of its balancing mechanism.
3.2 Input-based Deterministic Hash Routing
An alternative approach eschews extra parameters completely and represents as a ﬁxed function of the input. This is
the concept pioneered by HASH Layers [Roller et al., 2021] which circumvents the need to simultaneously learn 'and
. Our implementation takes the token ID assigned to the input by the SentencePiece tokenizer [Kudo and Richardson,
2018] and uses the remainder of it divided by Eas the expert selection. See §B.4 for details.
3.3 Routing via Reinforcement Learning
Finally, we re-analyze a technique that optimizes the router via Reinforcement Learning (a class of methods we call
RL-R), which was proposed in early work on neural conditional computation [Bengio et al., 2013, 2016, Bengio, 2017,
Denoyer and Gallinari, 2014]. In this approach each router is seen as a policy whose actions are the selection of an
expert in each routed layer and whose observations are the activations passed to that router. After completing the
forward pass, the probability the Routed Transformer assigns to the correct output token can be used as a reward,
maximization of which is equivalent to minimization of NLL. To jointly train the experts and the router, we minimize
a composite loss formed with the language modelling loss and a policy-gradient term [Sutton et al., 2000] using the
selected set of experts as actions. We highlight that the optimal expert selection is dependent not only on the input
activations but on the parameters of the rest of the network. This disrupts the theoretical underpinning, crucial to RL,
that this is a Markov Decision Process. Nevertheless, it has been observed that this theoretical issue does not affect the
practicality of the method [Rosenbaum et al., 2019].
Relative to SMOE ,RL-Rbeneﬁts from directly optimizing actions to improve the language modelling loss. However
this absence of bias comes with complications, especially the high variance of the gradient [Rosenbaum et al., 2019,
Denoyer and Gallinari, 2014]. We use REINFORCE with a learned baseline [Williams, 1992, Sutton and Barto, 2018] to
address this issue, so that improving the policy means increasing the likelihood of selecting experts which lead to a
better than average next token prediction. As with SMOE , we ﬁnd it useful to add a balancing term. To our knowledge,
we are the ﬁrst to experiment routing with Reinforcement Learning on large Transformer-based language models—we
therefore explore key ablations in Appendix B.3.
4

--- PAGE 5 ---
Uniﬁed Scaling Laws for Routed Language Models
Table 1: Leave-One-Out RMSLE Fit in (N;E ). The last row is
computed for each model size independently; this gives an lower
bound of the error of any joint scaling law.
Llog-log prediction Eq. S-BASE RL -R HASH
Separably linear in N,E (5) 80e-4 90e-4 90e-4
Bilinear in (N;E ) (7) 60e-4 57e-4 60e-4
Bilin. + saturat. in (N;E )(1) 58e-4 56e-4 56e-4
Per-Nﬁts in (E) (4) 46e-4 29e-4 19e-4Table 2: Dense scaling values (see also App. F).
NNc
Ours 0:078 3:568e13
Kaplan et al. [2020] 0:076 8:8e13
4 Scaling Behavior at Convergence
Our main hypothesis is that the converged log-loss of a Routing Network is bilinear in the terms logNandlogbE,
where ^Eis a saturating transformation of E. Speciﬁcally, we ﬁt the 6-parameter scaling law:
logL(N;E ),alogN+blogbE+clogNlogbE+d (1)
where1
bE,1
E 1 +
1
Estart 1
Emax 1+1
Emax:
We can generalize this law across a wider range of routing architectures by a change of variables, using the model
inference cost Fand the total number of parameters P, as:
logL(F;B),alogF+blogbB+clogFlogbB+d; (2)
whereB,P
FandB!^Bis the same saturating transform as E!^E. Before justifying Equation (1), we validate its
candidacy by ﬁtting it to empirical data obtained on a large sweep of models. This sweep consists of a Routing Network
trained for each of the three techniques described in §3: across six model sizes (described in Table 4) while varying E
across [2;4;8;16;32;64;128;256;512]. This totals 168 different models, including dense baselines.
The observed losses for each model are shown in Fig. 2(a-c). We ﬁt Eq. (1)to each routing method and plot predictions
for ﬁxed values of Nas dotted lines. The goodness-of-ﬁt across all methods is apparent, as is the clear behavior that
increasingEleads to a reduction in validation loss. Fig. 2(d) plots the relative predictions for all three techniques,
clearly showing that S-BASE performs best across all model sizes, followed by RL-R, followed by HASH (see §5.3). The
remainder of this section justiﬁes the chosen functional forms (1)and(2); ﬁrst supposing independent power laws in N
andE(§4.1), then introducing a multiplicative interaction (§4.2) and saturation in the second term (§4.3), followed by
a change of variables (§4.4). The beneﬁt gained by this progression of ﬁts can be seen in Table 1. Notations are recalled
in Fig. 3.
4.1 Separable Scaling Laws in Model Size and Experts
Kaplan et al. [2020] argue that the converged performance of a dense model with Nparameters can be modelled
accurately as the two-parameter power law
logL(N),alogN+d; i.e.L(N) =Nc
NN
(3)
whereN, aandNc,10d= a. We can re-estimate these coefﬁcients from the performance of our own dense
models, leading to estimations in Table 2. The similarity of Nis a reassuring sanity check (there are differences in
dataset, vocabulary, tokenization and model which effect Nc).
An immediate hypothesis is that for all values of N, scaling inEobeys a similar power law:
logLN(E),blogE+d0(4)
BecauseLN(1) =L(N)(a fact we will call dense equivalence ), (3) and (4) can be combined into:
logLN(E),alogN+blogE+d; (5)
5

--- PAGE 6 ---
Uniﬁed Scaling Laws for Routed Language Models
1.3B 370M 130M 55M 25M
Base model size-0.02
-0.025
-0.03Slope b(N)
S-BASE
RL-R
Hash
N Parameter Count in Base Model
E Number of Experts
P Total Number of Parameters
F Compute per Inference (in TeraFLOPs)
B Parameter Utilization Ratio
N EPC: The Effective Parameter Count
Figure 3: Left:b(N)increases with N.Right : Notations.
124832128 512
Expert CountValidation LossS-BASE
124832128 512
Expert Count RL-R
124832128 512
Expert Count Hash
Figure 4: Fits for S-BASE ,RL-Rand HASH . Dashed lines are solutions to Eq. (4)withb(N)given by Table 6 while
dotted lines are solutions to Eq. (7). Solutions for Eq. (5)are in grey. The separable solution fails to account for
decreasing performance given by expert scaling.
corresponding to the multiplicative separated power law:
LN(E) =10d=a
Na1
Eb
(6)
If Eq. (4)ﬁts observed data for any Nwe can proceed with an assumption that scaling in Eobeys a power-law for ﬁxed
N. Observing a constant bacrossNwould allow to ﬁt Eq. (5) to models ranging across NandEsimultaneously.
Fitting. The ﬁrst hypothesis is easily tested and conﬁrmed to a reasonable degree. We ﬁt Eq. (4)for each technique
and value of Nseparately, plotted as colored lines in Fig. 4. The values of bare shown in Fig. 3.
We observe that b(N)isincreasing withN(values listed in Table 6), corresponding to a reduction in beneﬁt from
routing as size increases, with a slope that is approximately linear in logN(Fig. 3). Eq. (5)requires that bremains
ﬁxed across N; therefore we expect it to poorly predict model performance. We can attempt a ﬁt nevertheless: plotted
in grey in Fig. 4. Qualitatively, this mis-predicts some validation losses by over 0.2, particularly overestimating the
performance at large NandE. As reported in Table 1, the ﬁt has held-out RMSLE values greater than 80e-4.
4.2 Quadratic Interaction in NandE
This motivates us to introduce a simple extension: that of a multiplicative interaction between logNandlogE. This is
conveniently the exact function which leads to bscaling with logNand takes the following form:
logL(N;E ),+alogN+blogE+clogNlogE+d (7)
This function has the property that the log-log slope in both NandEare afﬁne in the logarithm of the other variable. In
other words, with EorNﬁxed, the performance Lscales withNorEfollowing (3) and (4) with slopes given by:
a(E), @logL
@logN=a+clog(E) (8)
b(N), @logL
@logE=b+clog(N);
b(N)matches the behavior reported in Table 6. A transposed table, ﬁtting sets of models with ﬁxed Eand changing
N, can be found to match the behavior predicted by a(E)(see Table 8). There are two symmetric non-logarithmic
6

--- PAGE 7 ---
Uniﬁed Scaling Laws for Routed Language Models
55M 370M 870M
Dense Model Size1864256Expert Count
100 1000
TeraFLOPs110Param. Utililzation RatioK=1
K=2
K=4
55M 370M 870M
Dense Model Size1864256Expert Count
100 1000
TeraFLOPs110Param. Utilization RatioR=0.25
R=0.5
R=1.0
Figure 5: Level curves for Equation (1)and Equation (2)onS-BASE forK2f1;2;4g(left two),R2f1:0;0:5;0:25g
(right two). Scaling laws in (N;E )differ for models with different values of (K;R): indicated by non-overlapping
level-curves. A change of variables to (F;P)leads to almost-overlapping functions: allowing the same ﬁts to be reused
across changes in the routing architecture.
representations of (7), useful for comparison to (6):
L(N;E ) =10d=a
Na1
Eb+clog(N)
; (9a)
=10d=b
Eb1
Na+clog(E)
: (9b)
Fitting. Fitting the bilinear (7)instead of (5)substantially reduces the prediction error for large N(Table 1, Eq. (5)
vs Eq. (7)), as displayed in Fig. 4 (dotted lines match the dashed ones, where the grey separable ﬁt doesn’t). We verify
dense equivalence: Na, whileNcexp(d=a), and thus the law (7)gives similar prediction to the reference
law (3) for dense models. Predictions for ﬁxed Nare visualized as grey lines in Fig. 2.
Interpretation. In Eq. (7), whencis positive, the expert improvement slope b(N)reduces with model size N. All
three routing techniques considered therefore predict diminishing improvements from routing when increasing scale .
However, the scaling of S-BASE is predicted (and seen) to be substantially better. When designing a new technique, we
can ﬁt (7)and predict a better scaling behavior if the ﬁtted cis lower than with other techniques. A clear goal for future
work in routing techniques should be to ﬁnd a method with scaling coefﬁcient c0.
4.3 Bounded Scaling in E
Equation (5)models scaling in Eas a power law. For both small and large values of E, there are reasons to
expect some deviation. If a routing technique degrades with E(for instance, the variance of gradients in RL-Rwill
increase), performance for large Emight be worse than predicted. On the other hand, ﬁxed overhead (e.g., interference
from auxiliary losses) might worsen scaling for low values of E, counter-intuitively leading to better than expected
performance. Both phenomena appear clearly in Fig. 2. We seek to model this saturation such that the limit behavior in
Eis bounded on both sides. We choose the following transformation, but discuss in §5.1 a number of implications
which are independent of the speciﬁc saturating form used:
1
bE,1
E Emin+
1
Estart 1
Emax 1+1
Emax: (10)
This is constructed so that we have ^E(Emin) =Estart, while ^E!EmaxasE!1 . We ﬁxEmin= 1, indicating the
lower bound of meaningful expert counts. ^Ecan be seen as a thresholded version of E: increasing past Emaxwill give
7

--- PAGE 8 ---
Uniﬁed Scaling Laws for Routed Language Models
improvement, but not following a power law. Similarly, when Estart>1,^E >E for small values of E. Practically, the
ﬁt is the same over a wide range of different thresholding functions.
Fitting. Solving Equation (1), equal to Eq. (7)withE!^E, is complicated by its non-convexity. We ﬁnd the
coefﬁcients (a;b;c;d;E start;Emax)as the best of repeated solutions provided by the L-BFGS-B algorithm [Byrd et al.,
1995]. Fig. 2 shows ﬁtted curves from these equations; coefﬁcients are reported in Table 3.
Interpretation. Relative to using the simple bilinear law (7), ﬁtting Eq. (1)improves prediction for the lowest and
highest values of Econsidered. Crucially, while the deviation from a power-law (and therefore improvement in RMSLE)
is relatively minor for the values of Econsidered, the deviation is nonetheless clear (seen best looking at the raw losses
in Fig. 21). We believe it is important to model this saturation because (as argued in §5.2) the limit behavior of model
performance as Nincreases is substantially different when bounded, with important properties that are independent of
Emax. We further hypothesize that future work, able to test still larger values of E, will see a more quantitative beneﬁt
from including these terms. This can be already observed in Fig. 20 when noting that the law (7)does not over and
under estimate the performance for E=f2;4;256;512gas it does in Fig. 4. Level curves of Eq. (1)enumerate the
f(N;E )gwhich are predicted to achieve ﬁxed performance, as visualized in Fig 1(b). This demonstrates of the power
of routing: a model with N= 5MandE= 128 equals the performance of a model with N= 55MandE= 1,which
requires over ten times more compute per inference.
4.4 Generalizing Across Architecture Variants
The models trained so far use ﬁxed choices for two key details of routing: the number of experts executed per-datapoint
Kand the frequency of routed layers across depth R(previously set at 1 and 0:5, respectively). For any selected value
ofKandRwe may ﬁt Eq. (1)to observed performance, but since these variables are independent of NandE, we
do not expect the same coefﬁcients to remain valid across values of KandR. To allow for a uniﬁed scaling law, we
modify Eq. (1)to use terms in F, the TeraFLOPs required per forward pass, and in the ratio B,P
FwherePis the
total number of parameters. Speciﬁcally, Fis motivated by the approximation from Kaplan et al. [2020] that F= 2N.
B, the parameter utilization ratio , is an afﬁne function of E, close to linear when most parameters lie in the routed
components of the model.
Using (F;B)instead of (N;E )(and setting Eminto1
2) results in Eq. (2). To show the advantage of this change of
variables we conduct two experiments: varying Kacrossf1;2;4gandRacrossf0:25;0:5;1:0g. In both cases, we vary
E2f8;64;256gandN2f15M;370M;870Mg.
Fitting. Eq.(2)predicts the scaling behavior of models as well as Eq. (1)for a given routing architecture, as indicated
in Fig. 24. The beneﬁt of the change of variables is seen most clearly in Fig. 5, which plots contours of ﬁxed loss value
as functions of (N;E )and of (F;B). For varying (K;R), the loss surface as a function of NandEchanges: meaning
a joint ﬁt would be inaccurate. Plotted as functions of (F;B), the loss surface is almost the same, suggesting a shared ﬁt
between all three methods (see Fig. 25 and Fig. 26 for joint ﬁts for KandRrespectively). We highlight that R= 0:25
deviates slightly. Plausible explanations are discussed in §D.4. The possibility to use a shared ﬁt indicates a singular
takeaway: the architectural details KandRlittle affect the scaling behavior of a Routing Network. The loss of the
network can thus be predicted based only on inference ﬂops Fand total number of parameters P.
5 Scaling Law Applications
Next we provide two applications of the scaling laws presented. We re-emphasize that all values are only valid at the
speciﬁc token count all models were trained at: 130B. App. F provides evidence that our analysis, if not the numerical
values, are nevertheless robust to token count.
5.1 Effective Parameter Equivalence
We leverage Eq. (1)to compute the size Nof a dense model giving the same performance as a Routing Network.
Speciﬁcally, we solve for L(N;1) =L(N;E ), yielding
N,(N)(^E)=(Estart)
^E=E startb=(Estart)
(11)
Here(E) =a+clogE. Given a model with NandE, we call Nthat model’s Effective Parameter Count (orEPC).
Eq. (1) predicts that the performance of all models increases as a power law in this variable
logL(N;E ) =alogN(N;E ) +d: (12)
8

--- PAGE 9 ---
Uniﬁed Scaling Laws for Routed Language Models
Table 3: Solutions to Eq. (1).
a b c d EstartEmax
S-BASE -0.082 -0.108 0.009 1.104 1.847 314.478
RL-R -0.083 -0.126 0.012 1.111 1.880 469.982
HASH -0.087 -0.136 0.012 1.157 4.175 477.741
100M 1B 10B 100B 1T
Base model size100M1B10B100B1TMaximum eff.
parameter countNcutoff
NcutoffS-BASE
RL-R
Hash
Dense
Routing improvement
Figure 6: Maximum effective parameter count as a function of base model size. Routing helps until a certain size
Ncutoff, that varies strongly between methods ( S-BASE being the best)
The result of plotting all models as a function of Nis shown in Fig. 1(c): a good ﬁt across four orders of magnitude.
Scaling in terms of Nresults in a unifying power law: valid for dense and routed language models alike.
5.2 Routing Behavior for Large N
EPCleads to a better grasp of the behavior of routing as Nincreases. Of immediate interest is Ncutoff: the value of N
where N(N;E )6N. For largerN, routing will not improve performance. This is easily found to obey logNcutoff=b
c.
Ncutoff equals 937B,85Band83BforS-BASE ,RL-Rand HASH respectively. These values are highly dependent on the
number of tokens seen, and Ncutoff is expected to increase with increased numbers of tokens.
Next we consider Nmax(N),maxEN(N;E ), i.e. the maximal effective parameter count that a routing network can
reach. Eq. (11) predicts that logNis an afﬁne function of logNfor any ﬁxed E, and Nmax(N) =NforN >N cutoff.
Therefore logNmaxis piecewise-afﬁne in logN, as displayed in Fig. 6:
8N6Ncutoff= 10 b
c;Nmax(N) =N(N;E max);
8N>Ncutoff;Nmax(N) =N: (13)
Note that Nmaxis continuous near Ncutoff, since for all E,N(Ncutoff;E) =Ncutoff. Moreover, the slope of Nmax()
forN6Ncutoff is positive whenever Emax6Estart10 a=c, which is true for our coefﬁcients. In this setting Nmax()
is a non-decreasing function of N. Therefore for any routing network where N <N cutoff,N6Nmax(N)6Ncutoff,
meaning routing will never let you train a model more powerful than Ncutoff. Note that despite this value not depending
onEmax, its existence crucially depends on the saturating transformation: without it Nmaxis unbounded.
5.3 Comparative Analysis
Kaplan et al. [2020] use scaling laws to encapsulate and contrast the behavior of entire model classes. Here we mirror
this analysis by using the scaling laws we have proposed to summarize the relative behavior of the three routing
techniques considered. We make four concrete observations:
•S-BASE consistently outperforms RL-Rand HASH , though RL-Ris very competitive at smaller N.
•All routing techniques suffer from reducing efﬁcacy as Nincreases. Amongst the three techniques, S-BASE
scales best: the ﬁtted parameter cis lowest.
•For smallN,RL-Rand S-BASE scale similarly with expert count and better than HASH (as indicated by
computing the effective expert slope b(N) =b+clogN).
•HASH and RL-Rmaintain power-law behavior for longer than S-BASE (largerEmax). However they suffer
from more interference ( c); leading to worse performance for most model sizes.
•HASH has large initial overhead (bigger Estart), clearly visible as a more obvious curvature at small E.
9

--- PAGE 10 ---
Uniﬁed Scaling Laws for Routed Language Models
For a practitioner interested in applying routing techniques, we conclude with some recommendations:
1. Use routing when training any model with N61.3B.
2.S-BASE is a good default routing algorithm. RL-Rwill sometimes match S-BASE in performance but is less
robust and scalable (§D.1).
3. Target using E2f64;128gexperts. Larger values will continue to improve, but with diminishing returns.
4. UseK=1experts. Route layers at frequency 0:56R61; lower frequency reduces performance.
5. Future routing research should focus on the terms candEmax; indicative of limits to arbitrary scaling.
6.New routing techniques must be validated at multiple values of NandEwhen comparing with prior work.
Results on single sizes cannot be extrapolated.
6 Related Work
In studying the empirical aspects of scaling, this work follows Kaplan et al. [2020]; which triggered much research
including Henighan et al. [2020], Hernandez et al. [2021] and Ghorbani et al. [2021]. The underlying theory is less
understood, but there is some exploration of this space including Hutter [2021] and Bahri et al. [2021].
These studies, and ours, are mutually reliant on a large corpus of work improving the scalability of Transformers. This
includes models like GPT-2 [Radford et al., 2019], GPT-3 [Brown et al., 2020], Jurassic-1 [Lieber et al., 2021] and
Gopher [Rae et al., 2021], as well as work improving the ability of these models to be efﬁciently parallelized across
multiple devices, including Shoeybi et al. [2019], Narayanan et al. [2019], Kim et al. [2021] and Xu et al. [2021].
Parallel to all this has been a long study of Routing Networks; a term introduced by Rosenbaum et al. [2018] but
developed extensively in the literature as Conditional Computation [Bengio et al., 2013, 2016, Bengio, 2017, Denoyer
and Gallinari, 2014] and Mixture of Experts [Jacobs et al., 1991, Collobert et al., 2003, Eigen et al., 2014]. The
framework is sometimes further generalized, seen as per-example architecture search in Ramachandran and Le [2018]
or as a graph problem in Denoyer and Gallinari [2014]. Routing was popularized for large scale training by Shazeer
et al. [2017], and furthered by work including GShard [Lepikhin et al., 2020], Switch Transformer [Fedus et al., 2021]
and GLaM [Du et al., 2021]. In this vein, Artetxe et al. [2021] undertake a comparative analysis of dense networks and
SMOE s withE= 512 that aligns with our results. Finally, the core routing architecture is still being improved. Nie et al.
[2021] adapt Kthrough training where Hazimeh et al. [2021] learn it via a differentiable loss. Ramachandran and Le
[2018] increase Kthrough depth and encourage architectural diversity across experts. Caccia et al. [2021] grows E
throughout training and Rajbhandari et al. [2022] propose networks where Echanges with depth.
7 Conclusion
Using conditional computation to scale neural networks has long been a research goal, and methods based on Routing
Networks have been increasing in popularity. Here we have introduced a scaling law (Eq. (1)) that models the behavior
of these networks. This scaling law predicts that, for all models considered, introducing routing into a language model
improves performance. That improvement follows a power-law in the number of experts Ethat diminishes with
model sizeN, and can be further generalized across routing architectures with Eq. (2). These scaling laws quantify
the differences between three different routing techniques and lead to a single scalar (Eq. (11)) that simultaneously
describes the performance of routed and dense models alike.
This work provides an empirical framework with which to analyze future innovations in routing. We hope the
overwhelming evidence we provide towards the beneﬁts of routing encourage it to be more rapidly adopted as a
powerful tool for model improvement, whose scaling characteristics align with traditional methods of scaling (in depth
and width) and which will remain beneﬁcial up to models with base model size greater than 900 billion parameters.
10

--- PAGE 11 ---
Uniﬁed Scaling Laws for Routed Language Models
Acknowledgments
We would like to thank Marc’Aurelio Ranzato, Nando de Freitas, Jacob Menick and Andy Brock for useful comments
and feedback on early drafts of this paper. The infrastructure needed to train these models wouldn’t have been possible
without the dedicated work of the JAX and XLA teams, especially Peter Hawkins, Roy Frostig and James Bradbury
who all were crucial in the development of the routing software.
References
Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du,
Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines,
Louis Martin, Xing Zhou, Punit Singh Koura, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa
Kozareva, and Ves Stoyanov. Efﬁcient Large Scale Language Modeling with Mixtures of Experts. arXiv:2112.10684 ,
2021.
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural scaling laws.
arXiv:2102.06701 , 2021.
Emmanuel Bengio. On Reinforcement Learning for Deep Neural Architectures: Conditional Computation with
Stochastic Computation Policies . McGill University (Canada), 2017.
Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks
for faster models. In International Conference on Learning Representations , 2016.
Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic
neurons for conditional computation. CoRR , abs/1308.3432, 2013.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, and Skye
Wanderman-Milne. Jax: composable transformations of Python + NumPy programs, 2018.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv:2005.14165 ,
2020.
Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited memory algorithm for bound constrained
optimization. SIAM Journal on scientiﬁc computing , 16(5):1190–1208, 1995.
Lucas Caccia, Jing Xu, Myle Ott, Marc’Aurelio Ranzato, and Ludovic Denoyer. On anytime learning at macroscale.
arXiv:2106.09563 , 2021.
Ronan Collobert, Yoshua Bengio, and Samy Bengio. Scaling large learning problems with hard parallel mixtures.
International Journal of Pattern Recognition and Artiﬁcial Intelligence , 17(03):349–365, 2003.
Curation. Curation corpus base, 2020.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in Neural Information
Processing Systems , 2013.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL:
Attentive language models beyond a ﬁxed-length context. In Annual Meeting of the Association for Computational
Linguistics , pages 2978–2988, 2019.
Ludovic Denoyer and Patrick Gallinari. Deep sequential neural networks. In NIPS Deep Learning Workshop , 2014.
Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou,
Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang,
Kellie Webster, Marie Pellat, Kevin Robinson, Kathy Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V
Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. Glam: Efﬁcient scaling of language models with mixture-of-experts.
arXiv 2112.06905 , 2021.
David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep mixture of experts.
ICLR Workshop , 2014.
11

--- PAGE 12 ---
Uniﬁed Scaling Laws for Routed Language Models
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple
and efﬁcient sparsity. arXiv:2101.03961 , 2021.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish
Thite, Noa Nabeshima, et al. The pile: An 800GB dataset of diverse text for language modeling. arXiv:2101.00027 ,
2020.
Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and
Colin Cherry. Scaling laws for neural machine translation. arXiv:2109.07740 , 2021.
Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen, Rahul Mazumder,
Lichan Hong, and Ed H Chi. Dselect-k: Differentiable selection in the mixture of experts with applications to
multi-task learning. Advances in Neural Information Processing Systems , 2021.
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B Brown,
Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv:2010.14701 , 2020.
Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. arXiv:2102.01293 ,
2021.
Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. CoRR , 2019.
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam,
Quoc V Le, Yonghui Wu, et al. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism.
Advances in Neural Information Processing Systems , 2019.
Marcus Hutter. Learning curve theory. arXiv:2102.04074 , 2021.
Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts.
Neural computation , 3(1):79–87, 1991.
L. Kantorovitch. On the Translocation of Masses. Management Science , 5(1):1–4, 1958.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec
Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv:2001.08361 , 2020.
Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu, Amr Hendy, Samyam
Rajbhandari, Yuxiong He, and Hany Hassan Awadalla. Scalable and efﬁcient MoE training for multitask multilingual
models. CoRR , abs/2109.10465, 2021.
Paul Knopp and Richard Sinkhorn. Concerning nonnegative matrices and doubly stochastic matrices. Paciﬁc Journal of
Mathematics , 21(2):343–348, 1967.
Wouter Kool, Chris J. Maddison, and Andriy Mnih. Unbiased gradient estimation with balanced assignments for
mixtures of experts. CoRR , abs/2109.11817, 2021.
Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer
for neural text processing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing:
System Demonstrations , 2018.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In
International Conference on Learning Representations , 2020.
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. BASE layers: Simplifying training
of large, sparse models. In International Conference on Machine Learning , 2021.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. White Paper.
AI21 Labs , 2021.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning
Representations , 2018.
Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training.
arXiv:1812.06162 , 2018.
12

--- PAGE 13 ---
Uniﬁed Scaling Laws for Routed Language Models
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. International
Conference on Learning Representations , 2016.
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R. Devanur, Gregory R. Ganger, Phillip B.
Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the
ACM Symposium on Operating Systems Principles , 2019.
Xiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao, Zichao Yang, Zhi Yang, and Bin Cui.
Dense-to-sparse gate for mixture-of-experts. arXiv:2112.14397 , 2021.
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle,
Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring a broad
discourse context. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , 2016.
Gabriel Peyré and Marco Cuturi. Computational Optimal Transport . Foundations and Trends in Machine Learning,
2019.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog , 2019.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah
Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard
Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes
Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat
McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland,
Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida
Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,
Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan
Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura
Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals,
Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling
Language Models: Methods, Analysis & Insights from Training Gopher. arXiv:2112.11446 , 2021.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,
and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. Journal of Machine
Learning Research , 21:1–67, 2020.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory optimizations toward training
trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage
and Analysis , pages 1–16, 2020.
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan,
Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power
next-generation AI scale. arXiv 2201.05596 , 2022.
Prajit Ramachandran and Quoc V Le. Diversity and depth in per-example routing models. In International Conference
on Learning Representations , 2018.
Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse models.
arXiv:2106.04426 , 2021.
Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection of non-linear functions
for multi-task learning. In International Conference on Learning Representations , 2018.
Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, and Tim Klinger. Routing networks and the challenges of
modular and compositional computation. arXiv:1904.12774 , 2019.
Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction of the generalization
error across scales. In International Conference on Learning Representations , 2019.
13

--- PAGE 14 ---
Uniﬁed Scaling Laws for Routed Language Models
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.
Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on
Learning Representations , 2017.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-LM:
Training multi-billion parameter language models using model parallelism. arXiv:1909.08053 , 2019.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforce-
ment learning with function approximation. In Advances in neural information processing systems , 2000.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems , 2017.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine
learning , 8(3):229–256, 1992.
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry
Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: General and scalable parallelization for ml computation
graphs. arXiv:2105.04663 , 2021.
A Architecture
Our Transformer [Vaswani et al., 2017] is based on the architecture in [Radford et al., 2019] with relative positional
encodings [Dai et al., 2019]. Text is tokenized via SentencePiece [Kudo and Richardson, 2018] with 32;000tokens
and a byte-level backoff. We use Megatron-style FFW sharding [Shoeybi et al., 2019] where useful. Parameters are
stored in bﬂoat16 but all optimizer statistics are kept in ﬂoat32. As a result, the activations of the language models
are calculated in bﬂoat16 (though we explicitly upcast to perform all operations involving a softmax, including the
Attention Block and Router, in full ﬂoat32 precision). This is crucial to maintain stability on larger models [Fedus et al.,
2021, Rae et al., 2021]. The learning rate starts at 1e-7 and decays to 2e-5 with a cosine decay rate over the entire
250;000steps, after an initial warmup phase ramping up to 2e-4 in the ﬁrst 1500 steps.
We use seven different model sizes, with names and architectures speciﬁed in the following table. The width of the
hidden layer dwis ﬁxed at four times the width of the activations dmodel , and we use the same dimension for keys and
values.
Namedmodelnlayersnheads K/V size Actual # Params
15M 512 6 8 32 16;527;360
25M 512 8 8 64 27;279;360
55M 640 10 12 64 57;369;600
130M 896 12 16 64 132;163;584
370M 1536 12 12 128 368;123;904
870M 2048 16 16 128 872;546;304
1.3B 2048 24 16 128 1;308;819;456
Table 4: Model deﬁnitions used throughout this work.
The number of models we trained was too large to practically include multiple runs of each model with different seeds.
To give an idea of the potential error introduced by random chance, we trained all three routing techniques with 3
different seeds on a 130M model for 100;000steps with 8and256experts (along with a dense baseline). Results
are shown in Fig. 7. Different seeds (which inﬂuence not only parameter initialization but Expert Parallelism – see
Appendix C) lead to extremely minimal model divergence after an initial transitory period, with different seeds diverging
by no more than 0:01before 100;000steps. This is a close match to the 0:02error mentioned in [Kaplan et al., 2020]2.
B Detailed Routing Techniques
Here we detail aspects of the routing techniques crucial to their implementation and provide comparisons to key
alternatives.
2Anecdotally, throughout the development of this work we used 0:02as the cutoff to denote statistical signiﬁcance.
14

--- PAGE 15 ---
Uniﬁed Scaling Laws for Routed Language Models
Figure 7: Training curves color-coded by random seed for 1, 8 and 256 experts and the step-wise maximum disagreement
between runs.
B.1 Balancing Losses
We encourage uniform routing in both our SMoE and RL-R methods with the differentiable load balancing loss adapted
from the mean square auxiliary loss in Shazeer et al. [2017] and introduced in Lepikhin et al. [2020], Fedus et al. [2021].
LB=EEX
e=1mege
N(14)
Wheremeis the mean gate per expert:
me=1
NX
x2Bpe(x) (15)
Andgeis the gating decision per expert:
ge=X
x2B1fargmaxp(x);eg (16)
Forxin batchBof sizeNand policyp(x) = softmax( Wpx+bp). There are two cases where the selected experts
may not be the ones used: in S-BASE after the Sinkhorn redistribution step (see §B.2.1) and when experts are skipped
due to load-balancing (see §C.2). In both cases, the balancing loss is applied to the original gating decisions made by
the policy. We found that the auxiliary loss is less effective if post-balancing experts were considered.
B.2 SMoE with Sinkhorn redistribution ( S-BASE )
Our implementation of S-BASE differs from that proposed in Lewis et al. [2021] in two ways. First, we replace the
auction algorithm for re-assigning expect selections with a continuous rebalancing process implemented via a Sinkhorn
algorithm [Cuturi, 2013, Peyré and Cuturi, 2019]. Second, we add a shufﬂing step, similar to Lewis et al. [2021], before
computing the optimal assignment via Sinkhorn per-device (as opposed to across all devices as done in Lewis et al.
[2021]). In addition, we did not use any input jitter on the activations sent to as we did not see a noticeable effect. This
is in line with BASE but differs from recommendations in other SMOE papers [Lepikhin et al., 2020, Fedus et al., 2021].
B.2.1 Sinkhorn Redistribution
We rebalance expert selections using a Sinkhorn layer applied on top of the router logits, an idea that was explored
independently in parallel by Kool et al. [2021]. This is substantially more efﬁcient on our accelerator cluster than a hard
matching algorithm. We consider H2RTdthe intermediary embeddings of the networks before the application of a
routed layer (folded on the batch and time axes of respective sizes bandt, withT,bt). Those are fed to the linear
router, which output a logits matrix Li=HiW+b2RTe. HereEis the number of experts, and W2RdEand
b2REare the router parameters. From these logits, SMOE and RL-Rcomputes expert selection probabilities by
applying a softmax operation along the expert axis. In doing this, we compute selection probabilities for each input
separately, without taking into consideration any capacity constraints on expert, forcing us to introduce load-balancing
later (§C.2). We seek a proper way to integrate constraints in a mathematically grounded framework.
Mathematically, is obtained by solving a simple problem with constraints: each input must, on average, prefer exactly
one expert. This is made clear by the variational formulation of the softmax:
2RTE,[softmax(Li)]i2[1;T]= argmax
>0;
8i2[T];P
j2[E]pij=1;h; Li H() (17)
15

--- PAGE 16 ---
Uniﬁed Scaling Laws for Routed Language Models
whereHis the Shannon entropy of the matrix , i.e.H(),PT
i=1PE
j=1pijlogpij, and []denotes horizontal
stacking. This variational formulation offers a natural alternative to incorporate extra constraints. For ideal performance,
each expert should be assigned the same number of tokens on average B=T
E. We therefore add Eadditional
constraints:
n
8j2[E];TX
i=1pij=Bo
; (18)
which yields the doubly constrained regularized linear problem
2RTE,argmaxh; Li H(); (19)
under the constraints8
<
:>0;
8i2[T];PE
j=1pij=1
T;
8j2[E];PT
i=1pij=1
E
that we recognize as the regularized Kantorovich problem of optimal transport [Kantorovitch, 1958, Cuturi, 2013].
We solve this problem using the Sinkhorn algorithm [Knopp and Sinkhorn, 1967], that takes the logit matrix L2RTE
and returns a soft-assignment matrix 2RTE. The Sinkhorn algorithm solves Eq. (19) by alternated ascent in the
dual (see Peyré and Cuturi [2019] for details). Starting from f0= 02RTandg0= 02RE, we set
8i2[T]; (ft+1)i= log1
EEX
j=1exp(Lij (gt)j); (20)
8j2[E]; (gt+1)j= log1
TTX
i=1exp(Lij (ft+1)i):
These updates converge towards an optimal couple (f;g), such that
 =1
TEexp(L+fg) (21)
is the solution to Eq. (19), where (fg)ij,fi+gjfor alli;j2[T][E]. As detailed below, we early stop the
iterations (20) by measuring the primal violation of constraints in L1norm, i.e. when
EX
j=1TX
i=1(t)ij 1
E+TX
i=1EX
j=1(t)ij 1
T6etol (22)
Once the plan is computed, we greedily select, for each token, the device with highest device-selection probability,
effectively applying an argmax operation on top of the Sinkhorn logits to form a transportation plan projection.
Comparison to S-BASE and performance. Compared to using an exact (early-stopped) auction algorithm as Lewis
et al. [2021], the complexity of the Sinkhorn algorithm is in O(NE)versusO((NE)3=2), and its update are
well adapted to batch computations on TPU/GPU. In contrast, the auction algorithm must be run on CPU as it is a
greedy per-coordinate algorithm; it becomes a computational bottleneck applied to models with many routed layers.
Replacing the softmax output by an regularized optimal transport plan is very naturally interpreted as adding a balancing
distribution constraint to the softmax operator. Using an auction algorithm on top of the softmax assignment does not
have this property.
Moreover, the Sinkhorn algorithm can be halted before it has fully converged with a proper tolerance parameter (22)
where Lewis et al. [2021] uses a hard number of iterations. We ﬁnd an error tolerance of etol= 10 2gives consistently
good performance. In practice we observe an end-to-end model overhead of 1%to3%compared to Switch (the
same routing technique without this reassignment). This computational offset is negligible compared to the per-step
performance gain. Without the rebalancing step, Switch is very sensitive to balancing loss hyperparameters (as noted in
Lewis et al. [2021]) whereas S-BASE maintains uniform routing decisions with improved performance and robustness
while varying EandN.
B.2.2 Shufﬂing Tokens
Similar to Lewis et al. [2021], we shufﬂe router inputs across workers by ﬁrst computing a random permutation of the
inputs and sending the tth row of the batch to the btE
Tcth worker. We found that this shufﬂing stage was necessary to
16

--- PAGE 17 ---
Uniﬁed Scaling Laws for Routed Language Models
64 128 256 512
Expert Count2.22.42.62.83.03.2Validation Loss
25M 
130M 
1.3B RLR-B
RLR-G
RLR-S
Figure 8: The RLR-B method consistently outperforms RLR-G and RLR-S across scales. We found that Nucleus
Sampling gives a signiﬁcant improvement over Greedy Reinforce. However, performance is slightly improved by
adding a learned baseline.
prevent training from becoming unstable at larger scales. Our hypothesis is that the re-assignment provides a subtle
side channel through which information can be propagated backwards in time, and this can be abused by larger models
resulting in the validation loss diverging during training. Adding a shufﬂing stage ameliorates this issue by introducing
a large number of irrelevant elements to the rebalancing process, making it harder to infer behavior of future inputs.
Further work is needed to conﬁrm this theory, but the introduction of the shufﬂing step does eliminate this performance
degradation.
B.3 Routing with Reinforcement Learning ( RL-R)
We will ﬁrst describe a naive REINFORCE [Williams, 1992] implementation of routing, then describe possible
extensions and improvements which lead to the form used in the main text as RL-R.
Our implementation of REINFORCE uses the balancing loss in Equation 14 and a policy gradient loss:
L=1
NNX
i=1logiRi (23)
WhereRiis the reward for each sequence in the batch of size N and is the normalized expert preferences output by a
linear transformation as in SMOE . The proper thing is for , the selected experts, to be samples from the distribution ,
but we found that this substantially degraded performance at larger scales. This phenomenon can be attributed towards
unwanted interference, where exploratory steps for which turn out to be unnecessary lead to bad gradient updates
to the rest of the network [Rosenbaum et al., 2019]. We therefore consider a greedy selection method, where router
outputs are selected as (x) =TopK (softmax (Wpx+bp)).
While sampling (even when tuning softmax temperature) decreased the performance of the model, we would nevertheless
like to regain some of its exploratory power. To ameliorate this, we can use Nucleus Sampling [Holtzman et al., 2019],
which samples from the top- pset of experts E(p).
P0(e) =P(e)=p0ife2E(p);
0 otherwise.(24)
WhereE(p)is the smallest set of experts such that:
X
e2E(p)P(e)>p (25)
This eliminates the possibility of selecting experts with very low likelihood, while still introducing some randomness.
It is important to emphasize that this introduces a distributional shift to the samples, which can be corrected with
off-policy correction methods such as Importance Sampling.
17

--- PAGE 18 ---
Uniﬁed Scaling Laws for Routed Language Models
An alternative improvement is to learn an additional baseline function for each router. This method has an additional
entropy regularization loss and computes advantages Ai=Ri bifor the learned baseline bi:
L=1
NNX
i=1logpiAi 1
NNX
i=1logpipi+1
NNX
i=1vi (26)
Where we use the Huber Loss to calculate the value loss vi.
vi=1
2(Ri bi)2ifjRi bij6;
(jRi bij 1
2) otherwise.(27)
We numerate three RL-Rvariants below:
•Greedy REINFORCE (RLR-G) . REINFORCE selecting the top- kexperts and no additional auxiliary losses.
•Nucleus-sampled REINFORCE (RLR-S) . REINFORCE using nucleus sampling to eliminate less reliable
expert selections and reduce noise in the policy gradient update. In this method we sample from the top- p
truncated distribution. Nucleus sampling at a ﬁxed top- pscales well with increasing the number of experts.
•REINFORCE with baseline (RLR-B) . Our RL method which stabilizes training with a learned baseline and
a policy entropy regularization loss. We learn a baseline with a value function that has a single hidden layer of
sizedmodel
8.
Table 5 details the hyperparameters chosen for each RL-Rvariant and Fig. 8 contains validation losses across a number
of models. Note that the entropy loss is negative to encourage a more concentrated policy, and the weight must be tuned
jointly with the load balancing loss to keep routing balanced. This is in line with Bengio et al. [2016], who also use two
loss terms to both encourage early specialization and expert diversity. Additionally, since the policy entropy loss has a
similar effect to nucleus sampling, we did not see an improvement from including both regularization methods. RLR-B
consistently performed the best, especially with regards to scalability in EandN. For that reason we selected it as our
prime example, and refer to it as RL-Relsewhere.
Table 5: Selected hyperparameters for RL-Rvariants.
Hyperparameter RLR-G RLR-S RLR-B
Policy entropy weight 0. 0. -5e-4
Load balancing weight 1. 1. 1.
Policy gradient weight 1e-1 1e-1 1e-2
Nucleus top- p - 0.9 1.
Value weight - - 1e-2
Value hidden layers - - 1
Value loss type - - Huber
B.4 Hash layers ( HASH )
HASH is simple compared to RL-RorS-BASE , but is highly reliant on the particular choice of hashing function. Many
functions rely on knowing the integer ID which the tokenizer assigns to each unique token (characters, bytes, subwords,
etc.). Roller et al. [2021] describe multiple alternative functions, including pre-computing expert assignments for each
token using a greedy assignment based on the frequency counts of the token on the training set. They do not observe any
improvement in terms of perplexity relative to simpler random assignments of token to expert, but argue that balanced
hashing has better properties for distributed training.
Our implementation uses a simple modular hashing function, namely the token index modulo the number of experts.
Tokens are indexed by our tokenizer in an order that is roughly ordered by their underlying frequencies in the training
dataset, which means this strategy will be more balanced than an arbitrarily random assignment, while simpler to
implement than fully balanced hashing. We note that poor balancing with increasing expert count is to some extent
inevitable for any routing technique that deﬁnes one-to-one mappings between tokens and experts, assuming a bounded
Expert Capacity (see Section C.2), as it becomes progressively harder to assign high frequency tokens into a bigger
number of smaller buckets due to the tokens’ heavy-tailed distribution. This can be seen in Fig. 9.
18

--- PAGE 19 ---
Uniﬁed Scaling Laws for Routed Language Models
0 2 4 6 8
# Experts = 8103
102
101
Normalized token frequency
greedy (0.0% overflow)
modulo (0.0% overflow)
random (0.0% overflow)
0 20 40 60
# Experts = 64
greedy (0.8% overflow)
modulo (5.0% overflow)
random (7.2% overflow)
0 100 200 300 400 500
# Experts = 512
greedy (18.9% overflow)
modulo (21.5% overflow)
random (23.4% overflow)
Figure 9: HASH becomes less balanced as Eincreases. Here we compare three hash routing strategies using the
token frequency in our validation set. The lines represent the amount of tokens sent to each expert, ordered from most
subscribed to least subscribed. The dotted line represents the point where tokens are likely to overﬂow under our
bounded Expert Capacity setup ( C= 2).greedy implements Balanced assignment as described in Roller et al. [2021],
where the per-token frequency tables are pre-computed and tokens are assigned to the most empty expert ordered by
frequency; random assigns each token to a random expert; and modulo uses the technique described in this paper.
Note that (a) the token distribution is different from the one used by the tokenizer and (b) this simulation is based on
marginal token frequencies, not batches of sequences. The greedy strategy does improve the workload for the mid
range (E= 64) , but not signiﬁcantly for low ( E= 8) or high (E= 512 ) numbers of experts. modulo provides a
modest improvement over random .
C Distributed Routing Details
Here we describe the key aspects of Routing relevant to training on large clusters. We note there are several libraries
available for supporting large-scale Routing, including DeepSpeed [Kim et al., 2021, Rajbhandari et al., 2022] and
GSPMD [Xu et al., 2021]. Unfortunately these were incompatible with our preexisting infrastructure.
C.1 Expert Parallelism
We brieﬂy review parallelism techniques, building up to Expert Parallelism, a technique for efﬁciently distributing
parameters over an accelerator cluster. For a more in-depth exposition we recommend Lewis et al. [2021], Lepikhin
et al. [2020] or Rajbhandari et al. [2022]. In a fully data-parallel world, every device has an identical copy of all
parameters and a different input batch X. Each device executes a forward and backward pass on Xand (usually)
does a synchronous all-reduce across all devices on the gradients to . This is effective, but requires one copy of for
each device, wasteful when jjis large.
The general class of techniques known as Model Parallelism reduce this duplication by having any individual device
store only a subset of the entire model parameters. This reduction in memory comes with a cost: no longer can a single
device take an input and produce the model’s output; that device no longer contains all of . Most techniques therefore
require some additional synchronization or data exchange.
Sharding Parallelism [Shoeybi et al., 2019] takes advantage of a mathematical property present both in 2-layer-MLPs
and a Transformer’s attention blocks: namely, that the output can be represented as the sum of Ncomponents, where
each component applies the same functional form with independent weights on the same input. Shoeybi et al. [2019]
contains more details, but a simpliﬁed example can be given for a matrix multiplication where we observe the effect
of splitting a matrix into columnwise sub-matrices: Wx= [W1;:::;WN]x=PN
iWix. The effect of applying this
technique such that each device has a separate subcolumn is to prevent the duplication of the weight matrices (which
consist of the vast majority of ). The disadvantage is that all devices must see the same input, meaning the total
throughput of data on the cluster has been reduced N-fold. In addition, the sum described above is actually now a sum
across devices, which introduces additional communication overhead.
Expert Parallelism takes further advantage of the structure of a routed layer to similarly reduce the necessity of parameter
duplication while avoiding the need to duplicate data between devices. In particular, rather than duplicating experts
across all devices, each device contains only a subset of the experts which are not replicated anywhere else. Different
devices still see different inputs. The key motivation is that a given input xnever needs to interact with the parameters
19

--- PAGE 20 ---
Uniﬁed Scaling Laws for Routed Language Models
corresponding to experts which the router did not send xto. Therefore, a single input xneed only be present on a single
device (the one which contains the experts which the router selected for x) to produce the correct output. In order to
produce an output, the router selects an expert for all inputs and an additional data-exchange is introduced which sends
all inputs to the device which contains the requested experts. Each device then processes the inputs it was sent, then
returns all inputs to their original devices. Crucially, a roughly uniform router distribution leads to an evenly balanced
computation across devices. This allows routed layers to be stored across a cluster with no duplicated data and without
a reduction in data throughput. The downside is that this data exchange required across devices is generally more costly
than the cross-device-sum required by sharding. More details are given in Lewis et al. [2021]. Previous work [Fedus
et al., 2021] suggests using one expert per device. We believe this to be an implementation detail dependent on many
aspects of the infrastructure in use. For us, typically using 4or8local experts per device gave good performance.
All of Data, Sharding and Expert parallelism can be applied simultaneously. We use all three methods at will, selecting
the combination which works fastest for a given cluster structure and model size. There are still more variations of
model parallelism, notably Pipeline Parallelism [Narayanan et al., 2019, Huang et al., 2019], which we do not use.
C.2 Load Balancing
This at-will changing of parallelism techniques is dependent on the parallelism not affecting the output of the model.
This is generally true, but expert parallelism brings in one complicating factor: load balancing . In the description above,
we emphasized that a roughly-uniform router (averaged over a minibatch) will send the same number of inputs to each
device (we will call the expected value BS avg). However, in the worst case all inputs on all devices might select the
same expert, and therefore need to be sent to a single device. If memory is pre-allocated to accommodate this worse
case, then each device must have enough free memory to potentially store the entire global batch size: prohibitive for
large clusters.
The most common solution is to specify a capacity factor C, and only allocate space for BS avgCtokens. When an
expert is oversubscribed tokens are dropped at random until no experts are exceeding capacity. Having C > 1is useful
during training to prevent unnecessarily large numbers of tokens from being dropped. We set C= 2for all experiments
(though during evaluation we always allow all tokens to be routed to the desired expert). This strategy works well for
the Transformer architecture due to its residual connections – dropping a token means skipping that transformer block.
As long as the amount of dropped tokens is kept at a reasonable bound, it does not impact learning.
An optimization we support is allowing an oversubscribed expert to use the memory allocated by an undersubscribed
expert on the same device. This reduces the average number of tokens which are skipped, but does so at the minor cost
of introducing an interaction between tokens being skipped and the speciﬁc co-habitation of experts on devices. In
practice we do not ﬁnd this to have a large effect. We note that the rebalancing used in S-BASE substantially ameliorates
the load balancing problem by attempting to force all experts to be assigned the same number of tokens. However
because we use the approximate Sinkhorn algorithm, not a hard matching algorithm, over-subscription still happens
(though at a much reduced rate) and so these steps are still taken.
D Architectural Variations
Throughout this work we have focused on a narrow subset of possible Routing Net architectures, which we believe are
representative of recent work on large scale Routing Nets [Roller et al., 2021, Fedus et al., 2021, Lewis et al., 2021,
Shazeer et al., 2017, Artetxe et al., 2021, Lepikhin et al., 2020]. However, we also experimented with many variations
of these architectures, some of which we highlight now in more depth.
D.1 Robustness to hyper-parameter changes
We evaluated the robustness of S-BASE and RL-Rto changes in hyperparameters in Fig. 10. We focus on E= 512 due
to anecdotal experience that the largest performance variance occurred at this scale. RL-Ris found to be highly sensitive
to the hyperparameters in Table 5, especially the choice of balancing weight. In addition, changes to the policy entropy
weight can lead to unbalanced routers when the balancing weight is not tuned jointly.
Unlike Switch which has been shown to be sensitive to the choice of balancing loss [Roller et al., 2021], S-BASE is
robust to changes in balancing weight for values of 1e 3to 1. S-BASE also has competitive performance without
a balancing loss, but training is less stable. Additionally, Switch has higher expert oversubscription rates even when
tuning the balancing weight.
20

--- PAGE 21 ---
Uniﬁed Scaling Laws for Routed Language Models
0 50k 100k 150k 200k 250k
Steps2.42.62.833.2Validation LossHash
RL-R
S-BASE
Figure 10: Hyperparameter sensitivity at 512E 55M. For RL-R, hyperparameter selection has the largest impact on
model performance of the three methods. The top performing RL-Rmodels outperform HASH and are comparable with
S-BASE . However, non-optimal RL-Rconﬁgurations perform worse than the other two methods.
0.03 0.0625 0.125 0.25 0.5 1.0
Routing Frequency2.02.22.42.62.83.03.2Validation Loss
55M 
370M 
870M 
0.03 0.0625 0.125 0.25 0.5 1.0
Routing Frequency2.02.22.42.62.83.03.2
0.03 0.0625 0.125 0.25 0.5 1.0
Routing Frequency2.02.22.42.62.83.03.2
S-BASE
Hash
Figure 11: The model performance improves with increasing routing frequency for S-BASE , while HASH ﬂattens at
higher frequencies for 8E (left), 64E (middle) and 256E (right).
D.2 Varying Routing Frequencies
All of our models thus far have been routed every other layer with experts which are single FFWs [Lepikhin et al.,
2020, Fedus et al., 2021]. However, Lewis et al. [2021], Roller et al. [2021] explored stacking FFWs in the experts and
placingNrouted layers atL
N+1:::NL
N+1. We consider the performance impact of alternative routing frequencies, varying
the frequency R=N
Land placing routed layers atL
N:::NL
N.
We compare routing every layer to routing at frequencies R2f1
2;1
4;1
Lg. For routing a single layer we chose the
second to last layer [Roller et al., 2021], but consider routing atL
2in subsection D.4. S-BASE scales well with routing
frequency, but HASH degrades in performance as shown in Fig. 11. At a single routed layer, HASH has the lowest
validation loss across model sizes.
D.3 Varying the Routing Policy
Motivated by the improved scaling results for S-BASE , we investigate whether learning a routing policy becomes more
beneﬁcial as the frequency of routers increases.
Shared routing decisions. In Fig. 12, the routing decisions are made at the ﬁrst routed layer and shared across layers,
which keeps the number of routers constant as Rincreases. As HASH selects experts based on the token index at the
input layer, its routing function is unchanged for this variant. S-BASE and HASH have similar losses for shared routing
decisions, whereas S-BASE improves when learning to route at each expert layer.
Permuting the hash function. Conversely, we tested a variant of HASH where the hash function at each router uses
a static permutation of the input tokens to select the experts. This allows tokens to be routed to the same expert at
21

--- PAGE 22 ---
Uniﬁed Scaling Laws for Routed Language Models
0.125 0.25 0.5 1.0
Routing Frequency2.42.52.62.72.82.9Validation Loss
64E64E
256E256ES-BASE
Hash
Shared S-BASE
Figure 12: Shared expert selections across layers has a large effect on performance for S-BASE (in grey) at 25M. S-BASE
scales similarly to HASH in the single router case.
1 8 64 256
Expert Count2.02.22.42.62.83.03.2Validation Loss
55M 
370M 
870M S-BASE
Hash
(a)
10k 50k 100k
Steps2.32.42.52.6Validation LossS-BASE
3x S-BASE
Hash
3x Hash (b)
Figure 13: (a)S-BASE and HASH scale similarly when routing a single layer at L 1.(b)We see similar performance
forS-BASE and HASH at 32E 1.3B when routing atL
2with three FFWs per expert. However, S-BASE performance is
improved for interleaving three routed layers.
some layers without having the same hash. We found that performance was unchanged for this variant, suggesting that
increasing the number of possible routing paths does not necessarily impact performance for static policies.
These router variants suggest that methods which can adapt to each expert layer will outperform static policies. Further
work is needed in analyzing how policies can more effectively learn to route across layers.
D.4 Routing a Single Layer
We analyzed the scaling behavior of HASH and S-BASE when only routing a single layer. We observed that the routing
gains forR=1
Ldeviated from higher frequencies, which also impacted R=1
4to a lesser degree. We attribute this
performance regression to the suboptimal behavior of the ﬁrst routed layer. In both cases the total number of routers is
low, and the ﬁrst layer has a larger impact on overall performance than at higher routing frequencies. For R=1
Lthe
complexity of routing is reduced and a simpler routing method can reach competitive performance. HASH and S-BASE
have similar performance across expert counts in this case, as shown in Fig. 13.
We also compared routing a single layer atL
2with three FFWs per expert to three evenly spaced routed layers in Fig. 13.
Similar to the results shown in [Roller et al., 2021], three evenly spaced routed layers has slightly better performance
than three stacked FFWs for a 32E 1.3B model. We also found that S-BASE beneﬁts more from interleaving the routed
and dense layers, which is consistent with our routing frequency results.
22

--- PAGE 23 ---
Uniﬁed Scaling Laws for Routed Language Models
108109
Parameter Count2.22.42.62.83.0Validation Loss
1020
Total FLOPS
1
2
4
Dense
S-Base(64E)
Figure 14: Example of scaling curves for a dense model and a S-BASE (64E) model. When looking at performance per
parameter, higher values of K are always better. But lower values of K are generally more ﬂop efﬁcient, and achieve a
better loss for a given FLOP budget.
D.5 Varying number of experts per datapoint
In this work we have focused on routing each datapoint to a single expert at all routing layers, i.e.for the case where
K= 1. However, SMoE models have historically routed datapoints to more than one expert [Shazeer et al., 2017,
Lepikhin et al., 2020, Ramachandran and Le, 2018]. Increasing Kincurs in extra computation on the experts, but this
additional computation may be helpful for the end result, reﬂecting in better loss. Moreover, routing a datapoint through
more experts means each expert gets to see more data for each forward pass, which may speed up training. For these
reasons, it is not obvious that K= 1is the best setup. Section 4.4 investigated this and argued both that the generalized
formula Equation (2)can accommodate such cases and also that the resulting ﬁts show no substantial difference in
performance for K. However we explore this variance more in Fig. 14: plotting both scaling curves for varying values
ofKas well as plotting the loss in terms of F. Higher values of Kinvariably yield better performance per step, but
they are not necessarily more ﬂop efﬁcient. In fact, K= 1is always in the pareto front. We can verify that this holds
for varying numbers of experts.
Note that this difference in ﬂop-efﬁciency is not only theoretical, and is also followed by increased communication
costs when using expert parallelism. We observed in practice that reducing K by half amounted to close to 2x speedup
in inference and training.
E Effects of scaling strategy on Zero-shot Transfer
There is a strong relationship between the validation loss we have been discussing and the downstream performance
of models and speciﬁc tasks [Kaplan et al., 2020]. However, recent work has shown that this relationship is not as
straightforward for large Routing Networks, and individual tasks can beneﬁt more or less from expert scaling. For
example, Artetxe et al. [2021] show a narrowing performance gap between a SMOE Routing Network with E= 512
and its dense equivalent, with more marked improvement from routing in some tasks like HellaSwag andPIQA than in
in tasks like Winogrande andReCoRD . Likewise, Fedus et al. [2021] shows that Switch beneﬁts more from scale better
inTrivaQA than in SuperGlue .
A detailed analysis of the scaling properties of Routing Networks and how that transfers to downstream tasks merits
dedicated work. Here we start the conversation by looking at zero-shot transfer on a set of well known downstream tasks:
LAMBADA [Paperno et al., 2016], The Pile [Gao et al., 2020], Curation Corpus [Curation, 2020], WikiText-103 [Merity
et al., 2016] and C4[Raffel et al., 2020].
We estimate the scaling coefﬁcients individually for each task and routing technique. For simplicity of interpretation we
ignore the bounded scaling term and focus on the bilinear ﬁt on Eq. 7. The coefﬁcients can be seen in Table H. We
expect that scaling in both NandEwill improve the downstream performance. The key question revolves around
understanding changes in the relative magnitude of a,bandcas we move from task to task.
Viewing Table H it is immediately clear that the individual scaling coefﬁcients vary greatly across tasks, i.e.different
tasks have different relative gains at Zero-Shot performance as we move to larger scales. This can be better shown in
Fig. 15, where all coefﬁcients are displayed in a single plot. The variation across tasks are not the same for aandb. e.g.
23

--- PAGE 24 ---
Uniﬁed Scaling Laws for Routed Language Models
0.175
 0.150
 0.125
 0.100
 0.075
Expert Scaling (b)0.200
0.175
0.150
0.125
0.100
0.075
Size Scaling (a)Validation Set
0.175
 0.150
 0.125
 0.100
 0.075
Expert Scaling (b)LAMBADA
0.175
 0.150
 0.125
 0.100
 0.075
Expert Scaling (b)The Pile
0.175
 0.150
 0.125
 0.100
 0.075
Expert Scaling (b)CC
0.175
 0.150
 0.125
 0.100
 0.075
Expert Scaling (b)WikiText-103
0.175
 0.150
 0.125
 0.100
 0.075
Expert Scaling (b)C4
Hash
RL-R
S-Base
Figure 15: Individual scaling coefﬁcients for different tasks and routing techniques, compared to the coefﬁcients
estimated in the validtion set. Different techniques also scale differently depending on the task, but this also depends on
the interaction term (see Fig. 16)
WikiText-103 has higher values for band lower for awhen compared to the validation set. This means that even though
tasks see monotonic improvement in performance by scaling through either adding more experts or increasing the base
model size, some tasks beneﬁt more and some less from which method is used.
For a more complete picture, we can account for the NandEinteraction coefﬁcient cby incorporating it into one of
the scaling coefﬁcients – by holding the other quantity ﬁxed – which leads to a(E)andb(N)(see Section 4.2). This
can be seen in Fig. 16 for varying values of NandE.
We see that S-BASE tends to dominate with lower coefﬁcients at higher values of EandN(due to its smaller interaction
term relative to scaling terms), but this varies across tasks. For example, RL-Rshows better b(N)for most values of N
in LAMBADA, until it is overtaken by S-BASE atN= 410M, but S-BASE is always superior in C4. Moreover, the
ordering is not consistent between HASH and RL-Racross tasks, even though they often do not cross. This all means it
is difﬁcult to establish superior performance of a routing technique without looking at a variety of tasks and scales.
We often want to compare Routing Networks with a dense baseline with the same performance on the validation set,
and see how this changes on downstream tasks. We can use these parameters in a simpliﬁed version of the Effective
Parameter Count (EPC, Equation 11), by assuming Estart = 1andEmax=1, such that ^E=E. First, we note that
since the coefﬁcients vary greatly across tasks, each task will have a different EPC for the same network conﬁguration .
Moreover, the effects of scaling by varying EandNwill vary across tasks. Say we have a routing net of size NwithE
experts and we want to increase its base model size by a factor of xwhile keeping the same number of experts. The
effect on Nin this case will be a multiplication by xa(E)=a(1)=x1+clogE. Sincecvaries per task, the improvement
achieved by increasing the base model size will also be task dependent.
Say we have a routing net of size NwithEexperts and we want to increase its base model size by a factor of xwhile
keeping the same number of experts. The effect on Nin this case will be a multiplication by xa(E)=a(1)=x1+c
alogE.
Sincec
avaries per task, the improvement achieved by increasing the base model size will also be task dependent.
For example, the EPCvalidation for N=110M, E=32 is 370M, but EPClambada for the same model is 284M, while EPC
pileis 535M. The key implication here is not only do the values change, but their slopes are different. This means that
downstream tasks must be analyzed carefully: a practitioner could scale a model via routing expecting some overarching
improvement, but get a much diminished (or enhanced!) improvement on speciﬁc downstream tasks, depending on
their speciﬁc values of bandc.
F On Convergence, or Lack Thereof
Here we digress on two important details, both focusing on token count. First we argue that discussing converged
performance of large transformers on modern and massive text datasets is probably a misnomer; scaling analyses should
focus on optimal performance at a ﬁxed number of tokens. Second, we provide evidence arguing against a proposed
equation in Kaplan et al. [2020] (Eq. (1.6)).
F.1 Convergence on Large Datasets
There are two cases where the converged performance of a model can be clearly deﬁned. The ﬁrst is when continued
training of the model produces no improved results (even analyzed at logarithmic scale in the number of tokens), the
second is when continued training leads to reduced validation performance: overﬁtting.
Our models exhibit neither behavior. No overﬁtting is seen even for our largest models, likely due to the complexity
and size of the dataset used. Furthermore, despite being trained for 130 billion tokens, not even our smallest models
24

--- PAGE 25 ---
Uniﬁed Scaling Laws for Routed Language Models
1 8 64 512
# Experts (E)0.080
0.075
0.070
0.065
0.060
0.055
a(E)
Validation Set
1 8 64 512
# Experts (E)0.21
0.20
0.19
0.18
0.17
LAMBADA
1 8 64 512
# Experts (E)0.11
0.10
0.09
0.08
The Pile
1 8 64 512
# Experts (E)0.100
0.095
0.090
0.085
0.080
0.075
0.070
CC
1 8 64 512
# Experts (E)0.090
0.085
0.080
0.075
WikiText-103
1 8 64 512
# Experts (E)0.070
0.065
0.060
0.055
0.050
0.045
C4
Hash
RL-R
S-Base
25M 55M 130M 370M 1.3B
Base Model Size (N)0.035
0.030
0.025
0.020
b(N)
25M 55M 130M 370M 1.3B
Base Model Size (N)0.07
0.06
0.05
0.04
0.03
25M 55M 130M 370M 1.3B
Base Model Size (N)0.060
0.055
0.050
0.045
0.040
0.035
25M 55M 130M 370M 1.3B
Base Model Size (N)0.030
0.025
0.020
0.015
0.010
25M 55M 130M 370M 1.3B
Base Model Size (N)0.0350
0.0325
0.0300
0.0275
0.0250
0.0225
0.0200
25M 55M 130M 370M 1.3B
Base Model Size (N)0.0300
0.0275
0.0250
0.0225
0.0200
0.0175
0.0150
0.0125
Hash
RL-R
S-Base
25M 55M 130M 370M 1.3B
Base Model Size (N)1864512# Experts (E)
25M 55M 130M 370M 1.3B
Base Model Size (N)25M 55M 130M 370M 1.3B
Base Model Size (N)25M 55M 130M 370M 1.3B
Base Model Size (N)25M 55M 130M 370M 1.3B
Base Model Size (N)25M 55M 130M 370M 1.3B
Base Model Size (N)
Figure 16: Estimated scaling coefﬁcent for Zero-shot performance across different datasets. Top half: The coefﬁcient
for increasing Ewhile keeping Nﬁxed for varying values of Nat different downstream tasks. Middle: The coefﬁcient
for increasing Nvarying values of E.
10B 100B 1T
Token Count2.42.62.83.03.23.4Validation Loss
10B 100B 1T
Token Count0.030
0.028
0.026
0.024
0.022
0.020
b: Slope of Expert Improvement1-Experts
64-Experts
128-Experts
512-Experts
75000 Steps Trained
250000 Steps Trained
1000000 Steps Trained
Figure 17: On the left: validation performance over time for 15Mmodels trained with different expert counts and over
three different lengths of training. On the right, the coefﬁcient bfrom ﬁtting Eq. (4), representing scaling from Eacross
intermediate values.
have saturated. We push this envelope even further: training two additional sets of 15Mmodels with 1,64,128and512
experts. The ﬁrst set is trained for just 75;000steps, and the second for 1;000;000steps: four times more data (half a
trillion tokens). We highlight that this involves corresponding changes to the cosine cycle decay. We exclusively train
HASH models, both due to limits in the number of extra models we were able to train and also because it has the largest
value ofEmax.
Results from these models are plotted in Fig. 17 (left). 15Mwith no routing, the smallest model we train as part of this
work, is still far from having saturated its performance capability. Indeed, training for 4x longer further reduces the
validation loss by 0:05. This pattern continues, and is exacerbated, when increasing the expert count: the same model
withE= 512 gets a 0:07reduction in loss from 4x more tokens.
25

--- PAGE 26 ---
Uniﬁed Scaling Laws for Routed Language Models
(a)
 (b)
 (c)
Figure 18: a)Values ofafound for dense models across training. b)RMSE for these same ﬁts. c)Three attempts to ﬁt
Eq.(29). In black the standard ﬁt. In orange and grey ﬁts only using and ignoring the ﬁnal 150;000steps respectively.
It is clear then that the very smallest models considered have yet to converge. The same is certainly true for larger ones,
and probably more so. If 500 billion tokens is a lower bound to the convergence point of 15M, the analysis in Kaplan
et al. [2020] would predict needing trillions of tokens to converge 1.3B : much more than what was used to train some
of the largest language models yet created [Brown et al., 2020]. For large, complex text datasets of the scale used to
train large language models, convergence is not a proper criteria.
F.2 Performance Qualiﬁed on Token Count
Rather than claiming analysis at a non-observed point of convergence, we emphasize that the scaling behavior we have
described in this work is valid only as a function of a particular number of steps (or tokens). At each point, we can
deﬁne instantaneous values of scaling coefﬁcients, with the values from all models taken at Ssteps3.
In fact, the situation is more complicated that simply conditioning our scaling coefﬁcients on token count. We can see
this by plotting b, the scaling coefﬁcient for changes in expert-count in Fig. 17(right). An immediate observation is that
the values of bare non-constant, supporting the need to qualify scaling on token count. A second, more substantial
point, is that these values are not uniquely deﬁned by token count. For a given number of tokens, the scaling behavior
of three different sets of models is completely different, dependent on how far into the learning rate schedule those sets
of models were. We note that this behavior is suggested by experiments in Kaplan et al. [2020] (App. D.6).
Attempting to ﬁnd the full set of parameters on which these scaling terms depend is beyond the scope of this work. We
highlight just the importance of insuring that all variables possible are matched when comparing values to calculate
scaling coefﬁcients.
F.3 Performance Modeled as L(N, S)
We conclude by highlighting one implication of the fact that scaling coefﬁcients are dependent on token count. We
analyze only the dense models trained as part of this work, and calculate values of ain Equation (3)for all dense models
trained as part of the primary sweep across all step counts; plotted in Fig. F.3(a) with RMSE values plotted in Fig. F.3(b).
First, it is important to emphasize that the ﬁts remain good throughout S(after an initial period of transience). Namely,
though the slope is different, the validation losses for a given intermediate Sfollow a power law about as well as they
do later in training (if anything, more so). Second, the estimated coefﬁcients aare clearly monotonically increasing
withS.
[Kaplan et al., 2020] propose (Eq. 1.6) a uniﬁed prediction of the loss achieved by a model with size Ntraining forS
steps:
L(N;S) =Nc
NN
+Sc
SS
(28)
3This sidesteps the issue of critical batch size [McCandlish et al., 2018, Kaplan et al., 2020], consideration of which requires a
substantially larger sweep of models. Future work estimating the critical batch size will likely lead to better model ﬁts.
26

--- PAGE 27 ---
Uniﬁed Scaling Laws for Routed Language Models
15M 25M 55M 130M 370M 870M 1.3B 7B
Dense Model Size2.02.42.62.83.03.2Validation Loss
RL-R
Dense
Figure 19: RL-Rperformance for 64E continues to scale well compared to dense up to 7B base model size.
This comes with the subtlety that Smust be deﬁned as the number of steps when training at the averaged critical
batch size , where our models are trained with a ﬁxed batch size. This means a proper analysis must use Sminwith
S=S(1 +Bc
BL B) 1for constants BcandB. It is important to highlight however that Smin, as described in Kaplan
et al. [2020], should be independent of N. This implies that@
@N(L(N;S))is independent of S, or in log-log space:
@log(L(N;S))
@N
N=10N= N (29)
This prediction of constant scale is in concrete opposition to the increasing value seen in Fig. F.3(a). We can furthermore
check that this functional form cannot be obviously ﬁt to our learning curves, with examples show in Fig. F.3(c).
There are subtle differences between training setups, and we do not want to claim our experiments wholly disprove the
conjecture in [Kaplan et al., 2020]. However, the results in Fig. F.3 motivate us to assume that Eq. (F.3) cannot be used
to model our speciﬁc training curves. A consequence of this is that we can also no longer conclude Equation B.5 from
[Kaplan et al., 2020], that:
L(Neff(C);C) = (1 +N
S)L(Neff;1) (30)
With this equation, we might be able to lower-bound true converged performance (which we have not seen in our
models) by inference from compute-efﬁcient performance, which has been achieved by the majority of our models.
G Large Scale Routing Behavior, Coefﬁcient Sensitivity, and Future Work
Our analysis predicts that larger values of Ewill continue to improve performance, especially for small models, at a
diminishing rate. §5.2 also predicts that routing will continue to help with increasing Nfor at least one, if not two orders
of magnitude larger base model size. Practical compute limitations prevented our sweep from exploring these regimes,
and there are interesting unanswered questions in the limit of these two variables. In particular, exact predictions of
Ncutoff are highly dependent on the precise value of b, where error in the second decimal place shifts predicted values by
orders of magnitude (not surprising, as it is the slope of a line in log-log space).
We believe exploring the limit behavior of NandE, especially arriving at a more precise value of b, is crucial.
Anecdotally, we can report the results of one experiment: a large RL-Rmodel with N > 7;000;000, providing a
rough upper bound for error in bfor RL-R. In particular, we trained a model with dmodel = 4096 ,nlayers = 32 ,
nheads = 32;E= 64 and K/V size of 128. There are some important eccentricities of this model which affect its match
to the ﬁts described in this work: it was trained with a batch size of 1024 for 100k steps with a policy gradient weight of
1e-1 and balancing weight of 1e-1. Other training details are consistent with Section 2.1.
The performance of this model, relative to a dense model of the same size and also to a number of smaller models,
is plotted in Fig. 19 evaluated at 100B tokens. The changes described above prevent the analysis in this work from
accurately predicting this model’s performance, but one key feature remains: the routed 7B model substantially
outperforms the baseline. This is of particular interest since just a 0:01decrease inbwould predict an Ncutoff at9B,
27

--- PAGE 28 ---
Uniﬁed Scaling Laws for Routed Language Models
meaning we would already be close to the regime where routing would cease to work. Nevertheless, at this value
routing is clearly still a major improvement, and our estimate of bis unlikely to be a substantial overshoot.
While the differences between this model and those analyzed in the paper make concrete extrapolation impossible, it
shows that routing techniques still maintain competitive improvements at almost an order of magnitude larger value of
Nthan analyzed and it is unlikely the scaling coefﬁcients measured in this work substantially overestimate the routing
technique’s scalability. We encourage future work probing the limits of routing networks, both in NandE, to better
understand their properties and provide more accurate predictions of their scaling coefﬁcients.
H Extra Plots and Tables
This section contains some helpful visualizations and data which are not included in the main text.
Table 6: Values of b(N)with hold-out RMSEs in parentheses.
15M 25M 130M 370M 1.3B
S-BASE -0.035 -0.031 -0.029 -0.024 -0.019
(0.035) (0.019) (0.017) (0.014) (0.012)
RL-R -0.033 -0.031 -0.027 -0.022 -0.016
(0.016) (0.013) (0.013) (0.014) (0.009)
HASH -0.031 -0.029 -0.025 -0.021 -0.015
(0.039) (0.029) (0.023) (0.016) (0.011)
a b c d
S-BASE 0.079 0.088 0.007 1.072
RL-R 0.080 0.105 0.010 1.076
HASH 0.081 0.097 0.009 1.086
Table 7: Fits to Equation (7).
S-BASE RL-R HashLayers
4 0.077 0.075 0.077
8 0.073 0.073 0.075
32 0.070 0.067 0.069
64 0.066 0.063 0.066
128 0.064 0.060 0.063
256 0.058 0.056 0.059
512 0.060 0.053 0.056
Table 8: Values of a(E)for different values of E
.
124832128 512
Expert Count Hash
Figure 20: Afﬁne ﬁts for HASH with a shared slope in grey.
28

--- PAGE 29 ---
Uniﬁed Scaling Laws for Routed Language Models
1248163264128256512
Expert Count2.02.42.62.83.03.2Validation Loss
1091011
Total Parameters100101102
Parameter Utilization Ratio
Figure 21: The validation loss for all S-BASE models plotted as a function of expert count (left), the total number of
parameters (center) and the ratio of parameters to TeraFLOPs per inference (right).
1248163264128256512
Expert Count2.02.42.62.83.03.2Validation Loss
1091011
Total Parameters100101102
Parameter Utilization Ratio
Figure 22: The validation loss for all RL-R models plotted as a function of expert count (left), the total number of
parameters (center) and the ratio of parameters to TeraFLOPs per inference (right).
1248163264128256512
Expert Count2.42.62.83.03.2Validation Loss
1091011
Total Parameters100101102
Parameter Utilization Ratio
Figure 23: The validation loss for all HashLayer models plotted as a function of expert count (left), the total number of
parameters (center) and the ratio of parameters to TeraFLOPs per inference (right).
1248163264128256512
Expert Count22.42.83.2Validation Loss
1248163264128256512
Expert Count22.42.83.2
1 10 100
Parameter Utilization Ratio22.42.83.2
1 10 100
Parameter Utilization Ratio22.42.83.2S-BASE RL-R
Figure 24: Fitting S-BASE and RL-Rwith Eq. (1) and Eq. (2).
29

--- PAGE 30 ---
Uniﬁed Scaling Laws for Routed Language Models
1 10
Parameter Utilization Ratio2.02.42.62.83.03.2Validation Loss
1 10
Parameter Utilization Ratio2.02.42.62.83.03.2
1
2
4
Figure 25: Joint ﬁts to Equation (2) for K2f1;2;4g.
1 10 100
Parameter Utilization Ratio2.02.42.62.83.03.2Validation Loss
1 10 100
Parameter Utilization Ratio2.02.42.62.83.03.2
0.25
0.5
1.0
Figure 26: Joint ﬁts to Equation (2) for R2f0:25;0:5;1:0g.
30

--- PAGE 31 ---
Uniﬁed Scaling Laws for Routed Language Models
a b c d RMSE
policy dataset
Dense Validation Set -0.078 1.063 0.014
LAMBADA -0.203 1.952 0.039
The Pile -0.102 1.239 0.020
CC -0.097 1.133 0.041
WikiText-103 -0.090 1.172 0.015
C4 -0.066 1.009 0.014
Hash Validation Set -0.082 -0.102 0.009 1.102 0.022
LAMBADA -0.213 -0.167 0.015 2.049 0.051
The Pile -0.111 -0.161 0.014 1.325 0.023
CC -0.101 -0.101 0.010 1.177 0.045
WikiText-103 -0.093 -0.086 0.007 1.208 0.027
C4 -0.070 -0.088 0.008 1.045 0.021
S-Base Validation Set -0.081 -0.092 0.008 1.086 0.025
LAMBADA -0.211 -0.152 0.012 2.020 0.048
The Pile -0.110 -0.117 0.008 1.309 0.028
CC -0.100 -0.101 0.010 1.154 0.050
WikiText-103 -0.092 -0.074 0.005 1.194 0.025
C4 -0.068 -0.081 0.007 1.031 0.024
RL-R Validation Set -0.081 -0.107 0.010 1.090 0.022
LAMBADA -0.212 -0.190 0.016 2.030 0.051
The Pile -0.110 -0.149 0.012 1.320 0.030
CC -0.100 -0.113 0.011 1.156 0.045
WikiText-103 -0.092 -0.091 0.008 1.195 0.023
C4 -0.069 -0.092 0.009 1.033 0.022
Table 9: Scaling coefﬁcients for different downstream tasks.
31
