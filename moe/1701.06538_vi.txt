# 1701.06538.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/1701.06538.pdf
# Kích thước file: 544288 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
MẠNG NEURAL CỰC KỲ LỚN:
LỚP HỖN HỢP CHUYÊN GIA CỔNG THƯA
Noam Shazeer1, Azalia Mirhoseiniy1, Krzysztof Maziarz2, Andy Davis1, Quoc Le1, Geoffrey
Hinton1và Jeff Dean1
1Google Brain, {noam,azalia,andydavis,qvl,geoffhinton,jeff}@google.com
2Đại học Jagiellonian, Cracow, krzysztof.maziarz@student.uj.edu.pl
TÓM TẮT
Khả năng hấp thụ thông tin của mạng neural bị giới hạn bởi số lượng tham số của nó. Tính toán có điều kiện, trong đó các phần của mạng hoạt động trên cơ sở từng ví dụ, đã được đề xuất về mặt lý thuyết như một cách để tăng đáng kể dung lượng mô hình mà không cần tăng tỷ lệ thuận tính toán. Tuy nhiên, trong thực tế, có những thách thức đáng kể về thuật toán và hiệu suất. Trong công trình này, chúng tôi giải quyết những thách thức này và cuối cùng thực hiện được lời hứa của tính toán có điều kiện, đạt được cải thiện hơn 1000 lần về dung lượng mô hình chỉ với những tổn thất nhỏ về hiệu quả tính toán trên các cụm GPU hiện đại. Chúng tôi giới thiệu lớp Hỗn hợp Chuyên gia Cổng Thưa (MoE), bao gồm hàng nghìn mạng con feed-forward. Một mạng cổng có thể huấn luyện xác định một kết hợp thưa của các chuyên gia này để sử dụng cho mỗi ví dụ. Chúng tôi áp dụng MoE cho các tác vụ mô hình hóa ngôn ngữ và dịch máy, nơi dung lượng mô hình là rất quan trọng để hấp thụ lượng kiến thức khổng lồ có sẵn trong các corpus huấn luyện. Chúng tôi trình bày các kiến trúc mô hình trong đó MoE với lên đến 137 tỷ tham số được áp dụng theo cách convolutional giữa các lớp LSTM xếp chồng. Trên các benchmark mô hình hóa ngôn ngữ lớn và dịch máy, những mô hình này đạt được kết quả tốt hơn đáng kể so với hiện tại tốt nhất với chi phí tính toán thấp hơn.

1 GIỚI THIỆU VÀ CÔNG TRÌNH LIÊN QUAN
1.1 TÍNH TOÁN CÓ ĐIỀU KIỆN
Khai thác quy mô trong cả dữ liệu huấn luyện và kích thước mô hình đã là trung tâm của thành công của học sâu. Khi các tập dữ liệu đủ lớn, việc tăng dung lượng (số lượng tham số) của mạng neural có thể mang lại độ chính xác dự đoán tốt hơn nhiều. Điều này đã được chứng minh trong các lĩnh vực như văn bản (Sutskever et al., 2014; Bahdanau et al., 2014; Jozefowicz et al., 2016; Wu et al., 2016), hình ảnh (Krizhevsky et al., 2012; Le et al., 2012), và âm thanh (Hinton et al., 2012; Amodei et al., 2015). Đối với các mô hình học sâu điển hình, nơi toàn bộ mô hình được kích hoạt cho mọi ví dụ, điều này dẫn đến sự gia tăng gần như bình phương trong chi phí huấn luyện, khi cả kích thước mô hình và số lượng ví dụ huấn luyện đều tăng. Thật không may, những tiến bộ trong sức mạnh tính toán và tính toán phân tán không đủ để đáp ứng nhu cầu như vậy.

Các hình thức khác nhau của tính toán có điều kiện đã được đề xuất như một cách để tăng dung lượng mô hình mà không cần tăng tỷ lệ thuận chi phí tính toán (Davis & Arel, 2013; Bengio et al., 2013; Eigen et al., 2013; Ludovic Denoyer, 2014; Cho & Bengio, 2014; Bengio et al., 2015; Almahairi et al., 2015). Trong các sơ đồ này, những phần lớn của mạng hoạt động hoặc không hoạt động trên cơ sở từng ví dụ. Các quyết định cổng có thể là nhị phân hoặc thưa và liên tục, ngẫu nhiên hoặc xác định. Các hình thức khác nhau của học tăng cường và lan truyền ngược được đề xuất để huấn luyện các quyết định cổng.

Đóng góp bằng nhau chính
yTác phẩm được thực hiện với tư cách là thành viên của chương trình Google Brain Residency (g.co/brainresidency)
1arXiv:1701.06538v1 [cs.LG] 23 Jan 2017

--- TRANG 2 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Hình 1: Một lớp Hỗn hợp Chuyên gia (MoE) được nhúng trong một mô hình ngôn ngữ tái phát. Trong trường hợp này, hàm cổng thưa chọn hai chuyên gia để thực hiện tính toán. Đầu ra của chúng được điều chỉnh bởi đầu ra của mạng cổng.

Mặc dù những ý tưởng này hứa hẹn về mặt lý thuyết, cho đến nay chưa có công trình nào chứng minh được những cải thiện lớn về dung lượng mô hình, thời gian huấn luyện, hoặc chất lượng mô hình. Chúng tôi đổ lỗi cho điều này là do sự kết hợp của những thách thức sau:

Các thiết bị tính toán hiện đại, đặc biệt là GPU, nhanh hơn nhiều trong việc tính toán số học so với phân nhánh. Hầu hết các công trình trên đều nhận ra điều này và đề xuất bật/tắt các khối lớn của mạng với mỗi quyết định cổng.

Kích thước batch lớn là rất quan trọng cho hiệu suất, vì chúng phân bổ chi phí của việc truyền và cập nhật tham số. Tính toán có điều kiện làm giảm kích thước batch cho các khối hoạt động có điều kiện của mạng.

Băng thông mạng có thể là nút thắt cổ chai. Một cụm GPU có thể có sức mạnh tính toán lớn hơn hàng nghìn lần so với tổng băng thông mạng liên thiết bị. Để có hiệu quả tính toán, tỷ lệ tương đối giữa nhu cầu tính toán và mạng của một thuật toán phải vượt quá tỷ lệ này. Các lớp embedding, có thể được coi như một hình thức tính toán có điều kiện, bị cản trở bởi chính vấn đề này. Vì các embedding thường cần được gửi qua mạng, số lượng tương tác (ví dụ, tham số) bị giới hạn bởi băng thông mạng thay vì khả năng tính toán.

Tùy thuộc vào sơ đồ, các hạng mục mất mát có thể cần thiết để đạt được mức độ thưa mong muốn cho mỗi khối và/hoặc mỗi ví dụ. Bengio et al. (2015) sử dụng ba hạng mục như vậy. Những vấn đề này có thể ảnh hưởng đến cả chất lượng mô hình và cân bằng tải.

Dung lượng mô hình quan trọng nhất đối với các tập dữ liệu rất lớn. Tài liệu hiện có về tính toán có điều kiện xử lý các tập dữ liệu nhận dạng hình ảnh tương đối nhỏ gồm tối đa 600.000 hình ảnh. Khó có thể tưởng tượng rằng các nhãn của những hình ảnh này cung cấp đủ tín hiệu để huấn luyện đầy đủ một mô hình có hàng triệu, chứ đừng nói đến hàng tỷ tham số.

Trong công trình này, lần đầu tiên chúng tôi giải quyết tất cả các thách thức trên và cuối cùng thực hiện được lời hứa của tính toán có điều kiện. Chúng tôi đạt được cải thiện hơn 1000 lần về dung lượng mô hình chỉ với những tổn thất nhỏ về hiệu quả tính toán và tiến bộ đáng kể trong kết quả hiện tại tốt nhất trên các tập dữ liệu mô hình hóa ngôn ngữ và dịch thuật công khai.

1.2 PHƯƠNG PHÁP CỦA CHÚNG TÔI: LỚP HỖN HỢP CHUYÊN GIA CỔNG THƯA
Phương pháp tiếp cận tính toán có điều kiện của chúng tôi là giới thiệu một loại thành phần mạng neural mục đích chung mới: Lớp Hỗn hợp Chuyên gia Cổng Thưa (MoE). MoE bao gồm một số chuyên gia, mỗi chuyên gia là một mạng neural feed-forward đơn giản, và một mạng cổng có thể huấn luyện chọn một kết hợp thưa của các chuyên gia để xử lý từng đầu vào (xem Hình 1). Tất cả các phần của mạng được huấn luyện cùng nhau bằng lan truyền ngược.

2

--- TRANG 3 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Mặc dù kỹ thuật được giới thiệu là chung chung, trong bài báo này chúng tôi tập trung vào các tác vụ mô hình hóa ngôn ngữ và dịch máy, được biết là có lợi từ các mô hình rất lớn. Cụ thể, chúng tôi áp dụng MoE theo cách convolutional giữa các lớp LSTM xếp chồng (Hochreiter & Schmidhuber, 1997), như trong Hình 1. MoE được gọi một lần cho mỗi vị trí trong văn bản, chọn một kết hợp khác nhau có khả năng của các chuyên gia tại mỗi vị trí. Các chuyên gia khác nhau có xu hướng trở nên chuyên môn hóa cao dựa trên cú pháp và ngữ nghĩa (xem Phụ lục E Bảng 9). Trên cả benchmark mô hình hóa ngôn ngữ và dịch máy, chúng tôi cải thiện kết quả được công bố tốt nhất với một phần chi phí tính toán.

1.3 CÔNG TRÌNH LIÊN QUAN VỀ HỖN HỢP CHUYÊN GIA
Kể từ khi được giới thiệu hơn hai thập kỷ trước (Jacobs et al., 1991; Jordan & Jacobs, 1994), phương pháp hỗn hợp chuyên gia đã là chủ đề của nhiều nghiên cứu. Các loại kiến trúc chuyên gia khác nhau đã được đề xuất như SVM (Collobert et al., 2002), Quy trình Gaussian (Tresp, 2001; Theis & Bethge, 2015; Deisenroth & Ng, 2015), Quy trình Dirichlet (Shahbaba & Neal, 2009), và mạng sâu. Các công trình khác tập trung vào các cấu hình chuyên gia khác nhau như cấu trúc phân cấp (Yao et al., 2009), số lượng chuyên gia vô hạn (Rasmussen & Ghahramani, 2002), và thêm chuyên gia tuần tự (Aljundi et al., 2016). Garmash & Monz (2016) đề xuất một mô hình tập hợp dưới định dạng hỗn hợp chuyên gia cho dịch máy. Mạng cổng được huấn luyện trên một mô hình NMT tập hợp đã được huấn luyện trước.

Các công trình trên liên quan đến hỗn hợp chuyên gia cấp cao nhất. Hỗn hợp chuyên gia là toàn bộ mô hình. Eigen et al. (2013) giới thiệu ý tưởng sử dụng nhiều MoE với mạng cổng riêng của chúng như các phần của mô hình sâu. Trực quan là phương pháp sau mạnh mẽ hơn, vì các vấn đề phức tạp có thể chứa nhiều vấn đề con mỗi cái yêu cầu các chuyên gia khác nhau. Họ cũng ám chỉ trong kết luận của mình về tiềm năng giới thiệu tính thưa, biến MoE thành một phương tiện cho tính toán tính toán.

Công trình của chúng tôi dựa trên việc sử dụng MoE này như một thành phần mạng neural mục đích chung. Trong khi Eigen et al. (2013) sử dụng hai MoE xếp chồng cho phép hai tập hợp quyết định cổng, ứng dụng convolutional MoE của chúng tôi cho phép các quyết định cổng khác nhau tại mỗi vị trí trong văn bản. Chúng tôi cũng thực hiện cổng thưa và chứng minh việc sử dụng nó như một cách thực tế để tăng lớn dung lượng mô hình.

2 CẤU TRÚC CỦA LỚP HỖN HỢP CHUYÊN GIA
Lớp Hỗn hợp Chuyên gia (MoE) bao gồm một tập hợp n "mạng chuyên gia" E1, ..., En, và một "mạng cổng" G có đầu ra là một vector thưa n chiều. Hình 1 cho thấy tổng quan về module MoE. Các chuyên gia tự chúng là các mạng neural, mỗi cái có tham số riêng của mình. Mặc dù về nguyên tắc chúng tôi chỉ yêu cầu các chuyên gia chấp nhận các đầu vào có cùng kích thước và tạo ra các đầu ra có cùng kích thước, trong các điều tra ban đầu của chúng tôi trong bài báo này, chúng tôi giới hạn bản thân trong trường hợp các mô hình là mạng feed-forward với kiến trúc giống hệt nhau, nhưng với các tham số riêng biệt.

Hãy ký hiệu G(x) và Ei(x) là đầu ra của mạng cổng và đầu ra của mạng chuyên gia thứ i cho một đầu vào x cho trước. Đầu ra y của module MoE có thể được viết như sau:

y = Σ(i=1 to n) G(x)i Ei(x) (1)

Chúng tôi tiết kiệm tính toán dựa trên tính thưa của đầu ra G(x). Bất cứ khi nào G(x)i = 0, chúng tôi không cần tính Ei(x). Trong các thí nghiệm của chúng tôi, chúng tôi có lên đến hàng nghìn chuyên gia, nhưng chỉ cần đánh giá một số ít trong số chúng cho mỗi ví dụ. Nếu số lượng chuyên gia rất lớn, chúng tôi có thể giảm yếu tố phân nhánh bằng cách sử dụng MoE phân cấp hai cấp. Trong MoE phân cấp, một mạng cổng chính chọn một kết hợp có trọng số thưa của "chuyên gia", mỗi cái tự nó là một hỗn hợp chuyên gia thứ cấp với mạng cổng riêng. Sau đây chúng tôi tập trung vào MoE thông thường. Chúng tôi cung cấp thêm chi tiết về MoE phân cấp trong Phụ lục B.

Việc triển khai của chúng tôi liên quan đến các mô hình tính toán có điều kiện khác. MoE có các chuyên gia là ma trận trọng số đơn giản tương tự như ma trận trọng số được tham số hóa được đề xuất trong (Cho & Bengio, 2014). MoE có các chuyên gia có một lớp ẩn tương tự như dropout theo khối được mô tả trong (Bengio et al., 2015), nơi lớp bị loại bỏ được kẹp giữa các lớp hoạt động đầy đủ.

3

--- TRANG 4 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
2.1 MẠNG CỔNG
Cổng Softmax: Một lựa chọn đơn giản của hàm cổng không thưa (Jordan & Jacobs, 1994) là nhân đầu vào với ma trận trọng số có thể huấn luyện Wg và sau đó áp dụng hàm Softmax.

G(x) = Softmax(xWg) (2)

Cổng Top-K Nhiễu: Chúng tôi thêm hai thành phần vào mạng cổng Softmax: tính thưa và nhiễu. Trước khi thực hiện hàm softmax, chúng tôi thêm nhiễu Gaussian có thể điều chỉnh, sau đó chỉ giữ lại k giá trị cao nhất, đặt phần còn lại thành -∞ (điều này khiến các giá trị cổng tương ứng bằng 0). Tính thưa phục vụ để tiết kiệm tính toán, như được mô tả ở trên. Trong khi hình thức tính thưa này tạo ra một số gián đoạn đáng sợ về mặt lý thuyết trong đầu ra của hàm cổng, chúng tôi chưa quan sát thấy điều này là một vấn đề trong thực tế. Hạng mục nhiễu giúp cân bằng tải, như sẽ được thảo luận trong Phụ lục A. Lượng nhiễu cho mỗi thành phần được kiểm soát bởi ma trận trọng số có thể huấn luyện thứ hai Wnoise.

G(x) = Softmax(KeepTopK(H(x), k)) (3)
H(x)i = (xWg)i + StandardNormal() × Softplus((xWnoise)i) (4)
KeepTopK(v, k)i = {vi nếu vi trong k phần tử hàng đầu của v, -∞ nếu không} (5)

Huấn luyện Mạng Cổng: Chúng tôi huấn luyện mạng cổng bằng lan truyền ngược đơn giản, cùng với phần còn lại của mô hình. Nếu chúng tôi chọn k > 1, các giá trị cổng cho k chuyên gia hàng đầu có đạo hàm khác không đối với trọng số của mạng cổng. Loại hành vi thỉnh thoảng nhạy cảm này được mô tả trong (Bengio et al., 2013) liên quan đến các rectifier nhiễu. Gradient cũng lan truyền ngược qua mạng cổng đến các đầu vào của nó. Phương pháp của chúng tôi khác ở đây so với (Bengio et al., 2015) người sử dụng cổng boolean và phương pháp kiểu REINFORCE để huấn luyện mạng cổng.

3 GIẢI QUYẾT CÁC THÁCH THỨC HIỆU SUẤT
3.1 VẤN ĐỀ BATCH THU HẸP
Trên CPU và GPU hiện đại, kích thước batch lớn là cần thiết cho hiệu quả tính toán, để phân bổ chi phí tải và cập nhật tham số. Nếu mạng cổng chọn k trong số n chuyên gia cho mỗi ví dụ, thì đối với một batch gồm b ví dụ, mỗi chuyên gia nhận được một batch nhỏ hơn nhiều gồm khoảng kb/n ví dụ. Điều này khiến việc triển khai MoE ngây thơ trở nên rất không hiệu quả khi số lượng chuyên gia tăng. Giải pháp cho vấn đề batch thu hẹp này là làm cho kích thước batch ban đầu càng lớn càng tốt. Tuy nhiên, kích thước batch có xu hướng bị giới hạn bởi bộ nhớ cần thiết để lưu trữ các kích hoạt giữa các lần chuyển tiếp và ngược. Chúng tôi đề xuất các kỹ thuật sau để tăng kích thước batch:

Trộn Song song Dữ liệu và Song song Mô hình: Trong một thiết lập huấn luyện phân tán thông thường, nhiều bản sao của mô hình trên các thiết bị khác nhau xử lý bất đồng bộ các batch dữ liệu riêng biệt, và các tham số được đồng bộ hóa thông qua một tập hợp các máy chủ tham số. Trong kỹ thuật của chúng tôi, những batch khác nhau này chạy đồng bộ để chúng có thể được kết hợp cho lớp MoE. Chúng tôi phân phối các lớp tiêu chuẩn của mô hình và mạng cổng theo các sơ đồ song song dữ liệu thông thường, nhưng chỉ giữ một bản sao chung của mỗi chuyên gia. Mỗi chuyên gia trong lớp MoE nhận được một batch kết hợp bao gồm các ví dụ liên quan từ tất cả các batch đầu vào song song dữ liệu. Cùng một tập hợp thiết bị hoạt động như các bản sao song song dữ liệu (cho các lớp tiêu chuẩn và mạng cổng) và như các phân đoạn song song mô hình (mỗi cái lưu trữ một tập con các chuyên gia). Nếu mô hình được phân phối trên d thiết bị, và mỗi thiết bị xử lý một batch có kích thước b, mỗi chuyên gia nhận được một batch gồm khoảng kbd/n ví dụ. Do đó, chúng tôi đạt được cải thiện d lần về kích thước batch chuyên gia.

Trong trường hợp MoE phân cấp (Phần B), mạng cổng chính sử dụng song song dữ liệu, và các MoE thứ cấp sử dụng song song mô hình. Mỗi MoE thứ cấp nằm trên một thiết bị.

4

--- TRANG 5 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Kỹ thuật này cho phép chúng tôi tăng số lượng chuyên gia (và do đó số lượng tham số) bằng cách tăng tỷ lệ thuận số lượng thiết bị trong cụm huấn luyện. Tổng kích thước batch tăng, giữ nguyên kích thước batch cho mỗi chuyên gia. Yêu cầu bộ nhớ và băng thông cho mỗi thiết bị cũng vẫn không đổi, thời gian bước cũng vậy, cũng như lượng thời gian cần thiết để xử lý một số ví dụ huấn luyện bằng số lượng tham số trong mô hình. Mục tiêu của chúng tôi là huấn luyện một mô hình nghìn tỷ tham số trên một corpus nghìn tỷ từ. Chúng tôi chưa mở rộng hệ thống của mình đến mức này vào thời điểm viết bài báo này, nhưng điều đó có thể thực hiện được bằng cách thêm nhiều phần cứng hơn.

Tận dụng Tính Convolutional: Trong các mô hình ngôn ngữ của chúng tôi, chúng tôi áp dụng cùng một MoE cho mỗi bước thời gian của lớp trước. Nếu chúng tôi chờ lớp trước hoàn thành, chúng tôi có thể áp dụng MoE cho tất cả các bước thời gian cùng nhau như một batch lớn. Làm như vậy tăng kích thước batch đầu vào cho lớp MoE bằng một yếu tố bằng số bước thời gian được unroll.

Tăng Kích thước Batch cho MoE Tái phát: Chúng tôi nghi ngờ rằng các mô hình mạnh mẽ hơn có thể liên quan đến việc áp dụng MoE một cách tái phát. Ví dụ, ma trận trọng số của LSTM hoặc RNN khác có thể được thay thế bằng MoE. Đáng buồn, những mô hình như vậy phá vỡ thủ thuật convolutional từ đoạn cuối, vì đầu vào của MoE tại một timestep phụ thuộc vào đầu ra của MoE tại timestep trước. Gruslys et al. (2016) mô tả một kỹ thuật để giảm đáng kể số lượng kích hoạt được lưu trữ trong một RNN unrolled, với chi phí tính toán lại các kích hoạt chuyển tiếp. Điều này sẽ cho phép tăng lớn kích thước batch.

3.2 BĂNG THÔNG MẠNG
Một mối quan tâm hiệu suất chính khác trong tính toán phân tán là băng thông mạng. Vì các chuyên gia là tĩnh (xem trên) và số lượng tham số cổng là nhỏ, hầu hết giao tiếp liên quan đến việc gửi đầu vào và đầu ra của các chuyên gia qua mạng. Để duy trì hiệu quả tính toán, tỷ lệ tính toán của chuyên gia với kích thước đầu vào và đầu ra của nó phải vượt quá tỷ lệ khả năng tính toán với mạng của thiết bị tính toán. Đối với GPU, điều này có thể là hàng nghìn lần. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng các chuyên gia có một lớp ẩn chứa hàng nghìn đơn vị kích hoạt RELU. Vì các ma trận trọng số trong chuyên gia có kích thước input_size × hidden_size và hidden_size × output_size, tỷ lệ tính toán với đầu vào và đầu ra bằng kích thước của lớp ẩn. Thuận tiện, chúng tôi có thể tăng hiệu quả tính toán đơn giản bằng cách sử dụng lớp ẩn lớn hơn, hoặc nhiều lớp ẩn hơn.

4 CÂN BẰNG SỬ DỤNG CHUYÊN GIA
Chúng tôi đã quan sát rằng mạng cổng có xu hướng hội tụ đến trạng thái mà nó luôn tạo ra trọng số lớn cho cùng một vài chuyên gia. Sự mất cân bằng này tự củng cố, vì các chuyên gia được ưa chuộng được huấn luyện nhanh hơn và do đó được mạng cổng chọn nhiều hơn. Eigen et al. (2013) mô tả hiện tượng tương tự, và sử dụng ràng buộc cứng ở đầu huấn luyện để tránh cực tiểu địa phương này. Bengio et al. (2015) bao gồm một ràng buộc mềm về trung bình theo batch của mỗi cổng.

Chúng tôi theo phương pháp ràng buộc mềm. Chúng tôi định nghĩa tầm quan trọng của một chuyên gia liên quan đến một batch ví dụ huấn luyện là tổng theo batch của các giá trị cổng cho chuyên gia đó. Chúng tôi định nghĩa một mất mát bổ sung Limportance, được thêm vào hàm mất mát tổng thể cho mô hình. Mất mát này bằng bình phương của hệ số biến thiên của tập hợp các giá trị tầm quan trọng, nhân với một yếu tố tỷ lệ được điều chỉnh thủ công wimportance. Mất mát bổ sung này khuyến khích tất cả các chuyên gia có tầm quan trọng bằng nhau.

Importance(X) = Σ(x∈X) G(x) (6)
Limportance(X) = wimportance × CV(Importance(X))² (7)

1Bengio et al. (2015) cũng bao gồm hai mất mát bổ sung. Một mất mát kiểm soát tính thưa theo từng ví dụ, mà chúng tôi không cần vì nó được thực thi bởi giá trị cố định của k. Mất mát thứ ba khuyến khích đa dạng hóa các giá trị cổng. Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng các giá trị cổng tự nhiên đa dạng hóa khi các chuyên gia chuyên môn hóa (trong một chu kỳ đạo đức), và chúng tôi không cần thực thi đa dạng hóa các giá trị cổng.

5

--- TRANG 6 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Mặc dù hàm mất mát này có thể đảm bảo tầm quan trọng bằng nhau, các chuyên gia vẫn có thể nhận được số lượng ví dụ rất khác nhau. Ví dụ, một chuyên gia có thể nhận được một vài ví dụ với trọng số lớn, và một chuyên gia khác có thể nhận được nhiều ví dụ với trọng số nhỏ. Điều này có thể gây ra vấn đề về bộ nhớ và hiệu suất trên phần cứng phân tán. Để giải quyết vấn đề này, chúng tôi giới thiệu hàm mất mát thứ hai, Lload, đảm bảo tải cân bằng. Phụ lục A chứa định nghĩa của hàm này, cùng với kết quả thí nghiệm.

5 CÁC THÍ NGHIỆM
5.1 BENCHMARK MÔ HÌNH HÓA NGÔN NGỮ 1 TỶ TỪ
Tập dữ liệu: Tập dữ liệu này, được giới thiệu bởi (Chelba et al., 2013) bao gồm các câu duy nhất được xáo trộn từ các bài báo tin tức, tổng cộng khoảng 829 triệu từ, với từ vựng 793.471 từ.

Hiện tại Tốt nhất Trước đây: Kết quả được công bố tốt nhất trước đây (Jozefowicz et al., 2016) sử dụng các mô hình bao gồm một hoặc nhiều lớp Long Short-Term Memory (LSTM) xếp chồng (Hochreiter & Schmidhuber, 1997; Gers et al., 2000). Số lượng tham số trong các lớp LSTM của những mô hình này thay đổi từ 2 triệu đến 151 triệu. Chất lượng tăng đáng kể với số lượng tham số, cũng như chi phí tính toán. Kết quả cho những mô hình này tạo thành đường trên cùng của Hình 2-phải.

Các Mô hình MoE: Các mô hình của chúng tôi bao gồm hai lớp LSTM xếp chồng với một lớp MoE giữa chúng (xem Hình 1). Chúng tôi thay đổi kích thước của các lớp và số lượng chuyên gia. Để biết chi tiết đầy đủ về kiến trúc mô hình, chế độ huấn luyện, các baseline bổ sung và kết quả, xem Phụ lục C.

Tính toán Thấp, Dung lượng Đa dạng: Để điều tra hiệu ứng của việc thêm dung lượng, chúng tôi huấn luyện một loạt mô hình MoE tất cả với chi phí tính toán gần như bằng nhau: khoảng 8 triệu phép nhân-và-cộng mỗi ví dụ huấn luyện mỗi timestep trong lần chuyển tiếp, loại trừ lớp softmax. Chúng tôi gọi metric này là (ops/timestep). Chúng tôi huấn luyện các mô hình với MoE phẳng chứa 4, 32, và 256 chuyên gia, và các mô hình với MoE phân cấp chứa 256, 1024, và 4096 chuyên gia. Mỗi chuyên gia có khoảng 1 triệu tham số. Đối với tất cả các lớp MoE, 4 chuyên gia hoạt động cho mỗi đầu vào.

Kết quả của những mô hình này được hiển thị trong Hình 2-trái. Mô hình với 4 chuyên gia luôn hoạt động thực hiện (không ngạc nhiên) tương tự như các mô hình baseline được khớp tính toán, trong khi mô hình lớn nhất (4096 chuyên gia) đạt được 24% perplexity thấp hơn đáng kể trên tập kiểm tra.

Hình 2: So sánh mô hình trên Benchmark Mô hình hóa Ngôn ngữ 1-Tỷ-Từ. Ở bên trái, chúng tôi vẽ perplexity kiểm tra như một hàm của dung lượng mô hình cho các mô hình với ngân sách tính toán tương tự khoảng 8-triệu-ops-per-timestep. Ở bên phải, chúng tôi vẽ perplexity kiểm tra như một hàm của ngân sách tính toán. Đường trên cùng đại diện cho các mô hình LSTM từ (Jozefowicz et al., 2016). Đường dưới cùng đại diện cho các mô hình MoE 4-tỷ tham số với các ngân sách tính toán khác nhau.

Tính toán Đa dạng, Dung lượng Cao: Ngoài mô hình lớn nhất từ phần trước, chúng tôi huấn luyện thêm hai mô hình MoE với dung lượng cao tương tự (4 tỷ tham số), nhưng ngân sách tính toán cao hơn. Những mô hình này có LSTM lớn hơn, và ít nhưng lớn hơn và chuyên gia. Chi tiết

6

--- TRANG 7 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Bảng 1: Tóm tắt các mô hình MoE tăng cường dung lượng cao với ngân sách tính toán khác nhau, so với kết quả được công bố tốt nhất trước đây (Jozefowicz et al., 2016). Chi tiết trong Phụ lục C.

Test Test #Parameters ops/timestep Training TFLOPS
Perplexity Perplexity excluding embedding Time /GPU
10 epochs 100 epochs and softmax layers 10 epochs
Best Published Results 34.7 30.6 151 million 151 million 59 hours, 32 k40s 1.09
Low-Budget MoE Model 34.1 4303 million 8.9 million 15 hours, 16 k40s 0.74
Medium-Budget MoE Model 31.3 4313 million 33.8 million 17 hours, 32 k40s 1.22
High-Budget MoE Model 28.0 4371 million 142.7 million 47 hours, 32 k40s 1.56

có thể được tìm thấy trong Phụ lục C.2. Kết quả của ba mô hình này tạo thành đường dưới cùng của Hình 2-phải. Bảng 1 so sánh kết quả của những mô hình này với kết quả được công bố tốt nhất trước đây trên tập dữ liệu này. Ngay cả mô hình nhanh nhất trong số này cũng đánh bại kết quả được công bố tốt nhất (khi kiểm soát số epoch huấn luyện), mặc dù chỉ yêu cầu 6% tính toán.

Hiệu quả Tính toán: Chúng tôi huấn luyện các mô hình của mình bằng TensorFlow (Abadi et al., 2016) trên các cụm chứa 16-32 Tesla K40 GPU. Đối với mỗi mô hình của chúng tôi, chúng tôi xác định hiệu quả tính toán trong TFLOPS/GPU bằng cách chia số phép toán dấu phẩy động cần thiết để xử lý một batch huấn luyện cho thời gian bước quan sát được và số GPU trong cụm. Số lượng phép toán được sử dụng ở đây cao hơn những gì chúng tôi báo cáo trong số ops/timestep của chúng tôi ở chỗ chúng tôi bao gồm lần chuyển ngược, chúng tôi bao gồm huấn luyện dựa trên lấy mẫu tầm quan trọng của lớp softmax, và chúng tôi đếm nhân-và-cộng như hai phép toán riêng biệt. Đối với tất cả các mô hình MoE của chúng tôi, các phép toán dấu phẩy động liên quan đến các chuyên gia đại diện cho 37% đến 46% tổng số.

Đối với các mô hình baseline của chúng tôi không có MoE, hiệu quả tính toán quan sát được dao động từ 1.07-1.29 TFLOPS/GPU. Đối với các mô hình MoE tính toán thấp của chúng tôi, hiệu quả tính toán dao động từ 0.74-0.90 TFLOPS/GPU, ngoại trừ mô hình 4-chuyên gia không tận dụng đầy đủ tính song song có sẵn. Mô hình MoE tính toán cao nhất của chúng tôi hiệu quả hơn ở 1.56 TFLOPS/GPU, có thể do các ma trận lớn hơn. Những con số này đại diện cho một phần đáng kể của tối đa lý thuyết 4.29 TFLOPS/GPU được NVIDIA tuyên bố. Kết quả chi tiết trong Phụ lục C, Bảng 7.

5.2 CORPUS TIN TỨC GOOGLE 100 TỶ TỪ

Hình 3: Mô hình hóa ngôn ngữ trên corpus 100 tỷ từ. Các mô hình có ngân sách tính toán tương tự (8 triệu ops/timestep).

Trên corpus 1-tỷ-từ, việc thêm dung lượng bổ sung dường như tạo ra lợi nhuận giảm dần khi số lượng tham số trong lớp MoE vượt quá 1 tỷ, như có thể thấy trong Hình 2-trái. Chúng tôi giả thuyết rằng đối với tập huấn luyện lớn hơn, thậm chí dung lượng cao hơn sẽ tạo ra cải thiện chất lượng đáng kể.

Chúng tôi xây dựng một tập huấn luyện tương tự bao gồm các câu duy nhất được xáo trộn từ corpus tin tức nội bộ của Google, tổng cộng khoảng 100 tỷ từ. Tương tự như phần trước, chúng tôi kiểm tra một loạt mô hình với chi phí tính toán tương tự khoảng 8 triệu ops/timestep. Ngoài mô hình LSTM baseline, chúng tôi huấn luyện các mô hình được tăng cường với các lớp MoE chứa 32, 256, 1024, 4096, 16384, 65536, và 131072 chuyên gia. Điều này tương ứng với lên đến 137 tỷ tham số trong lớp MoE. Chi tiết về kiến trúc, huấn luyện, và kết quả được đưa ra trong Phụ lục D.

Kết quả: Hình 3 cho thấy perplexity kiểm tra như một hàm của dung lượng sau khi huấn luyện trên 10 tỷ từ (đường trên cùng) và 100 tỷ từ (đường dưới cùng). Khi huấn luyện trên toàn bộ 100 tỷ từ, perplexity kiểm tra cải thiện đáng kể lên đến 65536 chuyên gia (68 tỷ tham số), giảm 39% so với baseline được khớp tính toán, nhưng giảm sút ở 131072 chuyên gia, có thể là kết quả của quá nhiều tính thưa. Khoảng cách mở rộng giữa hai đường chứng minh (không ngạc nhiên) rằng dung lượng mô hình tăng giúp ích nhiều hơn trên các tập huấn luyện lớn hơn.

Ngay cả ở 65536 chuyên gia (99.994% tính thưa lớp), hiệu quả tính toán cho mô hình vẫn giữ ở mức đáng kính 0.72 TFLOPS/GPU.

5.3 DỊCH MÁY (CẶP NGÔN NGỮ ĐƠN)
Kiến trúc Mô hình: Mô hình của chúng tôi là phiên bản được sửa đổi của mô hình GNMT được mô tả trong (Wu et al., 2016). Để giảm tính toán, chúng tôi giảm số lượng lớp LSTM trong encoder và decoder từ 9 và 8 xuống còn 3 và 2 tương ứng. Chúng tôi chèn các lớp MoE trong cả encoder (giữa lớp 2 và 3) và decoder (giữa lớp 1 và 2). Mỗi lớp MoE chứa lên đến 2048 chuyên gia mỗi cái có khoảng hai triệu tham số, thêm tổng cộng khoảng 8 tỷ tham số vào các mô hình. Chi tiết thêm về kiến trúc mô hình, quy trình kiểm tra và kết quả có thể được tìm thấy trong Phụ lục E.

Tập dữ liệu: Chúng tôi đánh giá phương pháp của mình trên các corpus WMT'14 En→Fr và En→De, có tập huấn luyện lần lượt có 36M cặp câu và 5M cặp câu. Các giao thức thí nghiệm cũng tương tự như trong (Wu et al., 2016): newstest2014 được sử dụng làm tập kiểm tra để so sánh với công trình trước (Luong et al., 2015a; Zhou et al., 2016; Wu et al., 2016), trong khi sự kết hợp của newstest2012 và newstest2013 được sử dụng làm tập phát triển. Chúng tôi cũng kiểm tra cùng mô hình trên dữ liệu Sản xuất Tiếng Anh sang Tiếng Pháp của Google.

Bảng 2: Kết quả trên WMT'14 En→Fr newstest2014 (giá trị in đậm đại diện cho kết quả tốt nhất).

Model Test Test ops/timenstep Total Training
Perplexity BLEU #Parameters Time
MoE with 2048 Experts 2.69 40.35 85M 8.7B 3 days/64 k40s
MoE with 2048 Experts (longer training) 2.63 40.56 85M 8.7B 6 days/64 k40s
GNMT (Wu et al., 2016) 2.79 39.22 214M 278M 6 days/96 k80s
GNMT+RL (Wu et al., 2016) 2.96 39.92 214M 278M 6 days/96 k80s
PBMT (Durrani et al., 2014) 37.0
LSTM (6-layer) (Luong et al., 2015b) 31.5
LSTM (6-layer+PosUnk) (Luong et al., 2015b) 33.1
DeepAtt (Zhou et al., 2016) 37.7
DeepAtt+PosUnk (Zhou et al., 2016) 39.2

Bảng 3: Kết quả trên WMT'14 En→De newstest2014 (giá trị in đậm đại diện cho kết quả tốt nhất).

Model Test Test ops/timestep Total Training
Perplexity BLEU #Parameters Time
MoE with 2048 Experts 4.64 26.03 85M 8.7B 1 day/64 k40s
GNMT (Wu et al., 2016) 5.25 24.91 214M 278M 1 day/96 k80s
GNMT +RL (Wu et al., 2016) 8.08 24.66 214M 278M 1 day/96 k80s
PBMT (Durrani et al., 2014) 20.7
DeepAtt (Zhou et al., 2016) 20.6

Bảng 4: Kết quả trên tập dữ liệu Google Production En→Fr (giá trị in đậm đại diện cho kết quả tốt nhất).

Model Eval Eval Test Test ops/timestep Total Training
Perplexity BLEU Perplexity BLEU #Parameters Time
MoE with 2048 Experts 2.60 37.27 2.69 36.57 85M 8.7B 1 day/64 k40s
GNMT (Wu et al., 2016) 2.78 35.80 2.87 35.56 214M 278M 6 days/96 k80s

8

--- TRANG 9 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Kết quả: Bảng 2, 3, và 4 cho thấy kết quả của các mô hình lớn nhất của chúng tôi, so sánh với kết quả đã được công bố. Phương pháp của chúng tôi đạt được điểm BLEU 40.56 và 26.03 trên các benchmark WMT'14 En→Fr và En→De. Vì các mô hình của chúng tôi không sử dụng tinh chỉnh RL, những kết quả này tạo nên những cải thiện đáng kể 1.34 và 1.12 điểm BLEU trên các baseline mạnh trong (Wu et al., 2016). Điểm perplexity cũng tốt hơn.2 Trên tập dữ liệu Google Production, mô hình của chúng tôi đạt được 1.01 điểm BLEU kiểm tra cao hơn ngay cả sau khi huấn luyện chỉ trong một phần sáu thời gian.

5.4 DỊCH MÁY ĐA NGÔN NGỮ
Tập dữ liệu: (Johnson et al., 2016) huấn luyện một mô hình GNMT đơn (Wu et al., 2016) trên một tập dữ liệu kết hợp rất lớn của mười hai cặp ngôn ngữ. Kết quả hơi tệ hơn so với 12 mô hình GNMT đơn cặp được huấn luyện riêng biệt. Điều này không ngạc nhiên, xét rằng mười hai mô hình có dung lượng gấp 12 lần và tổng huấn luyện gấp mười hai lần so với một mô hình. Chúng tôi lặp lại thí nghiệm này với một mô hình MoE tăng cường đơn. Xem Phụ lục E để biết chi tiết về kiến trúc mô hình. Chúng tôi huấn luyện mô hình của mình trên cùng tập dữ liệu như (Johnson et al., 2016) và xử lý cùng số lượng ví dụ huấn luyện (khoảng 3 tỷ cặp câu). Thời gian huấn luyện của chúng tôi ngắn hơn do ngân sách tính toán thấp hơn của mô hình chúng tôi.

Kết quả: Kết quả cho các mô hình GNMT đơn cặp, mô hình GNMT đa ngôn ngữ và mô hình MoE đa ngôn ngữ được đưa ra trong Bảng 5. Mô hình MoE đạt được perplexity thấp hơn 19% trên tập dev so với mô hình GNMT đa ngôn ngữ. Về điểm BLEU, mô hình MoE đánh bại đáng kể mô hình GNMT đa ngôn ngữ trên 11 trong số 12 cặp ngôn ngữ (nhiều nhất là 5.84 điểm), và thậm chí đánh bại các mô hình GNMT đơn ngôn ngữ trên 8 trong số 12 cặp ngôn ngữ. Hiệu suất kém trên Tiếng Anh→Tiếng Hàn dường như là kết quả của overtraining nghiêm trọng, vì đối với các cặp ngôn ngữ hiếm hơn, một số lượng nhỏ ví dụ thực được lấy mẫu quá mức trong corpus huấn luyện.

Bảng 5: Dịch Máy Đa ngôn ngữ (giá trị in đậm đại diện cho kết quả tốt nhất).

GNMT-Mono GNMT-Multi MoE-Multi MoE-Multi vs.
GNMT-Multi
Parameters 278M / model 278M 8.7B
ops/timestep 212M 212M 102M
training time, hardware various 21 days, 96 k20s 12 days, 64 k40s
Perplexity (dev) 4.14 3.35 -19%
French→English Test BLEU 36.47 34.40 37.46 +3.06
German→English Test BLEU 31.77 31.17 34.80 +3.63
Japanese→English Test BLEU 23.41 21.62 25.91 +4.29
Korean→English Test BLEU 25.42 22.87 28.71 +5.84
Portuguese→English Test BLEU 44.40 42.53 46.13 +3.60
Spanish→English Test BLEU 38.00 36.04 39.39 +3.35
English→French Test BLEU 35.37 34.00 36.59 +2.59
English→German Test BLEU 26.43 23.15 24.53 +1.38
English→Japanese Test BLEU 23.66 21.10 22.78 +1.68
English→Korean Test BLEU 19.75 18.41 16.62 -1.79
English→Portuguese Test BLEU 38.40 37.35 37.90 +0.55
English→Spanish Test BLEU 34.50 34.25 36.21 +1.96

6 KẾT LUẬN
Công trình này là công trình đầu tiên chứng minh những chiến thắng lớn từ tính toán có điều kiện trong mạng sâu. Chúng tôi đã cẩn thận xác định các cân nhắc thiết kế và thách thức của tính toán có điều kiện và giải quyết chúng với sự kết hợp của các giải pháp thuật toán và kỹ thuật. Mặc dù chúng tôi tập trung vào văn bản, tính toán có điều kiện có thể giúp ích trong các lĩnh vực khác, với điều kiện là có tập huấn luyện đủ lớn. Chúng tôi mong muốn thấy nhiều triển khai và ứng dụng mới lạ của tính toán có điều kiện trong những năm tới.

LỜI CẢM ƠN
Chúng tôi muốn cảm ơn tất cả các thành viên của nhóm Google Brain và Google Translate đã giúp chúng tôi trong dự án này, đặc biệt là Zhifeng Chen, Yonghui Wu, và Melvin Johnson. Cảm ơn cũng gửi đến các nhà đánh giá ẩn danh ICLR của chúng tôi vì những gợi ý hữu ích để làm cho bài báo này tốt hơn.

2Perplexity được báo cáo liên quan đến tokenization được sử dụng bởi cả mô hình của chúng tôi và GNMT.

9

--- TRANG 10 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
TÀI LIỆU THAM KHẢO
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Gregory S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian J. Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Józefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Gordon Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, và Xiaoqiang Zheng. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. CoRR, abs/1603.04467, 2016. URL http://arxiv.org/abs/1603.04467.

Rahaf Aljundi, Punarjay Chakravarty, và Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. CoRR, abs/1611.06194, 2016. URL http://arxiv.org/abs/1611.06194.

A. Almahairi, N. Ballas, T. Cooijmans, Y. Zheng, H. Larochelle, và A. Courville. Dynamic Capacity Networks. ArXiv e-prints, November 2015.

Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Y. Hannun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Sherjil Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, và Zhenyao Zhu. Deep speech 2: End-to-end speech recognition in english and mandarin. arXiv preprint arXiv:1512.02595, 2015.

Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.

Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, và Doina Precup. Conditional computation in neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.

Yoshua Bengio, Nicholas Léonard, và Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, và Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013.

K. Cho và Y. Bengio. Exponentially Increasing the Capacity-to-Computation Ratio for Conditional Computation in Deep Learning. ArXiv e-prints, June 2014.

Ronan Collobert, Samy Bengio, và Yoshua Bengio. A parallel mixture of SVMs for very large scale problems. Neural Computing, 2002.

Andrew Davis và Itamar Arel. Low-rank approximations for conditional feedforward computation in deep neural networks. arXiv preprint arXiv:1312.4461, 2013.

Marc Peter Deisenroth và Jun Wei Ng. Distributed Gaussian processes. In ICML, 2015.

John Duchi, Elad Hazan, và Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization, 2010.

Nadir Durrani, Barry Haddow, Philipp Koehn, và Kenneth Heafield. Edinburgh's phrase-based machine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation, 2014.

David Eigen, Marc'Aurelio Ranzato, và Ilya Sutskever. Learning factored representations in a deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.

Ekaterina Garmash và Christof Monz. Ensemble learning for multi-source neural machine translation. In staff.science.uva.nl/c.monz, 2016.

10

--- TRANG 11 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Felix A. Gers, Jürgen A. Schmidhuber, và Fred A. Cummins. Learning to forget: Continual prediction with lstm. Neural Computation, 2000.

Audrunas Gruslys, Rémi Munos, Ivo Danihelka, Marc Lanctot, và Alex Graves. Memory-efficient backpropagation through time. CoRR, abs/1606.03401, 2016. URL http://arxiv.org/abs/1606.03401.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. IEEE Conference on Computer Vision and Pattern Recognition, 2015.

Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 2012.

Sepp Hochreiter và Jürgen Schmidhuber. Long short-term memory. Neural Computation, 1997.

Sergey Ioffe và Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.

Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, và Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computing, 1991.

Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda B. Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, và Jeffrey Dean. Google's multilingual neural machine translation system: Enabling zero-shot translation. CoRR, abs/1611.04558, 2016. URL http://arxiv.org/abs/1611.04558.

Michael I. Jordan và Robert A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural Computing, 1994.

Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, và Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.

Diederik Kingma và Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.

Reinhard Kneser và Hermann. Ney. Improved backing-off for m-gram language modeling., 1995.

Alex Krizhevsky, Ilya Sutskever, và Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.

Quoc V. Le, Marc'Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado, Jeffrey Dean, và Andrew Y. Ng. Building high-level features using large scale unsupervised learning. In ICML, 2012.

Patrick Gallinari Ludovic Denoyer. Deep sequential neural network. arXiv preprint arXiv:1410.0510, 2014.

Minh-Thang Luong, Hieu Pham, và Christopher D. Manning. Effective approaches to attention-based neural machine translation. EMNLP, 2015a.

Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, và Wojciech Zaremba. Addressing the rare word problem in neural machine translation. ACL, 2015b.

Carl Edward Rasmussen và Zoubin Ghahramani. Infinite mixtures of Gaussian process experts. NIPS, 2002.

Hasim Sak, Andrew W Senior, và Françoise Beaufays. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In INTERSPEECH, pp. 338–342, 2014.

Mike Schuster và Kaisuke Nakajima. Japanese and Korean voice search. ICASSP, 2012.

Babak Shahbaba và Radford Neal. Nonlinear models using dirichlet process mixtures. JMLR, 2009.

11

--- TRANG 12 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Ilya Sutskever, Oriol Vinyals, và Quoc V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014.

Lucas Theis và Matthias Bethge. Generative image modeling using spatial LSTMs. In NIPS, 2015.

Volker Tresp. Mixtures of Gaussian Processes. In NIPS, 2001.

Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, và Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.

Bangpeng Yao, Dirk Walther, Diane Beck, và Li Fei-fei. Hierarchical mixture of classification experts uncovers interactions between brain regions. In NIPS. 2009.

Wojciech Zaremba, Ilya Sutskever, và Oriol Vinyals. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329, 2014.

Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, và Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. arXiv preprint arXiv:1606.04199, 2016.

12

--- TRANG 13 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
PHỤ LỤC

A MẤT MÁT CÂN BẰNG TẢI
Như đã thảo luận trong phần 4, cho mục đích cân bằng tải, chúng tôi muốn định nghĩa một hàm mất mát bổ sung để khuyến khích các chuyên gia nhận được số lượng ví dụ huấn luyện gần bằng nhau. Thật không may, số lượng ví dụ mà một chuyên gia nhận được là một đại lượng rời rạc, nên nó không thể được sử dụng trong lan truyền ngược. Thay vào đó, chúng tôi định nghĩa một estimator mịn Load(X) của số lượng ví dụ được gán cho mỗi chuyên gia cho một batch X đầu vào. Tính mịn cho phép chúng tôi lan truyền gradient qua estimator. Đây là mục đích của hạng mục nhiễu trong hàm cổng. Chúng tôi định nghĩa P(x,i) là xác suất G(x)i khác không, cho một lựa chọn ngẫu nhiên mới của nhiễu trên phần tử i, nhưng giữ các lựa chọn nhiễu đã được lấy mẫu trên các phần tử khác. Để tính P(x,i), chúng tôi lưu ý rằng G(x)i khác không khi và chỉ khi H(x)i lớn hơn phần tử lớn thứ k của H(x) loại trừ chính nó. Xác suất tính ra là:

P(x,i) = Pr((xWg)i + StandardNormal() × Softplus((xWnoise)i) > kth_excluding(H(x), k, i)) (8)

Trong đó kth_excluding(v, k, i) có nghĩa là thành phần cao thứ k của v, loại trừ thành phần i. Đơn giản hóa, chúng tôi được:

P(x,i) = Φ((xWg)i - kth_excluding(H(x), k, i) / Softplus((xWnoise)i)) (9)

Trong đó Φ là CDF của phân phối chuẩn tắc.

Load(X)i = Σ(x∈X) P(x,i) (10)

Bây giờ chúng tôi có thể định nghĩa mất mát tải là bình phương của hệ số biến thiên của vector tải, nhân với một yếu tố tỷ lệ được điều chỉnh thủ công wload.

Lload(X) = wload × CV(Load(X))² (11)

Mất cân bằng Tải Ban đầu: Để tránh lỗi hết bộ nhớ, chúng tôi cần khởi tạo mạng trong trạng thái tải chuyên gia gần như bằng nhau (vì các ràng buộc mềm cần một chút thời gian để hoạt động). Để thực hiện điều này, chúng tôi khởi tạo các ma trận Wg và Wnoise thành tất cả số không, tạo ra không có tín hiệu và một chút nhiễu.

Thí nghiệm: Chúng tôi huấn luyện một tập hợp các mô hình với kiến trúc giống hệt nhau (mô hình MoE-256 được mô tả trong Phụ lục C), sử dụng các giá trị khác nhau của wimportance và wload. Chúng tôi huấn luyện mỗi mô hình trong 10 epoch, sau đó đo perplexity trên tập kiểm tra. Chúng tôi cũng đo các hệ số biến thiên trong Importance và Load, cũng như tỷ lệ tải trên chuyên gia quá tải nhất so với tải trung bình. Giá trị cuối cùng này có ý nghĩa cho mục đích cân bằng tải trên phần cứng phân tán. Tất cả các metric này được tính trung bình trên nhiều batch huấn luyện.

Bảng 6: Thí nghiệm với các kết hợp mất mát khác nhau.

wimportance wload Test Perplexity CV(Importance(X)) CV(Load(X)) max(Load(X))/mean(Load(X))
0.0 0.0 39.8 3.04 3.01 17.80
0.2 0.0 35.6 0.06 0.17 1.47
0.0 0.2 35.7 0.22 0.04 1.15
0.1 0.1 35.6 0.06 0.05 1.14
0.01 0.01 35.7 0.48 0.11 1.37
1.0 1.0 35.7 0.03 0.02 1.07

13

--- TRANG 14 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Kết quả: Kết quả được báo cáo trong Bảng 6. Tất cả các kết hợp chứa ít nhất một trong hai mất mát dẫn đến chất lượng mô hình rất tương tự, trong khi không có mất mát thì tệ hơn nhiều. Các mô hình với giá trị wload cao hơn có tải thấp hơn trên chuyên gia quá tải nhất.

B HỖN HỢP CHUYÊN GIA PHÂN CẤP
Nếu số lượng chuyên gia rất lớn, chúng tôi có thể giảm yếu tố phân nhánh bằng cách sử dụng MoE phân cấp hai cấp. Trong MoE phân cấp, một mạng cổng chính chọn một kết hợp có trọng số thưa của "chuyên gia", mỗi cái tự nó là một hỗn hợp chuyên gia thứ cấp với mạng cổng riêng của nó.³ Nếu MoE phân cấp bao gồm a nhóm b chuyên gia mỗi nhóm, chúng tôi ký hiệu mạng cổng chính bằng Gprimary, các mạng cổng thứ cấp bằng (G1, G2, ..., Ga), và các mạng chuyên gia bằng (E0,0, E0,1, ..., Ea,b). Đầu ra của MoE được cho bởi:

yH = Σ(i=1 to a) Σ(j=1 to b) Gprimary(x)i × Gi(x)j × Ei,j(x) (12)

Các metric sử dụng chuyên gia của chúng tôi thay đổi thành như sau:

ImportanceH(X)i,j = Σ(x∈X) Gprimary(x)i × Gi(x)j (13)

LoadH(X)i,j = Loadprimary(X)i × Loadi(X(i))j / |X(i)| (14)

Loadprimary và Loadi ký hiệu các hàm Load cho mạng cổng chính và mạng cổng thứ cấp thứ i tương ứng. X(i) ký hiệu tập con của X mà Gprimary(x)i > 0.

Có vẻ đơn giản hơn để cho LoadH(X)i,j = Loadi(Xi)j, nhưng điều này sẽ không có gradient đối với mạng cổng chính, nên chúng tôi sử dụng công thức ở trên.

C BENCHMARK MÔ HÌNH HÓA NGÔN NGỮ 1 TỶ TỪ - CHI TIẾT THÍ NGHIỆM
C.1 CÁC MÔ HÌNH 8-TRIỆU-PHÉP-TOÁN-MỖI-TIMESTEP

Kiến trúc Mô hình: Mô hình của chúng tôi bao gồm năm lớp: một lớp embedding từ, một lớp Long Short-Term Memory (LSTM) tái phát (Hochreiter & Schmidhuber, 1997; Gers et al., 2000), một lớp MoE, một lớp LSTM thứ hai, và một lớp softmax. Chiều của lớp embedding, số đơn vị trong mỗi lớp LSTM, và chiều đầu vào và đầu ra của lớp MoE đều bằng 512. Đối với mọi lớp ngoại trừ softmax, chúng tôi áp dụng dropout (Zaremba et al., 2014) cho đầu ra lớp, loại bỏ mỗi kích hoạt với xác suất DropProb, nếu không thì chia cho (1-DropProb). Sau dropout, đầu ra của lớp trước được cộng vào đầu ra lớp. Kết nối dư này khuyến khích dòng gradient (He et al., 2015).

Kiến trúc Lớp MoE: Mỗi chuyên gia trong lớp MoE là một mạng feed forward với một lớp ẩn kích hoạt ReLU có kích thước 1024 và một lớp đầu ra có kích thước 512. Do đó, mỗi chuyên gia chứa [512×1024] + [1024×512] = 1M tham số. Đầu ra của lớp MoE được truyền qua hàm sigmoid trước dropout. Chúng tôi thay đổi số lượng chuyên gia giữa các mô hình, sử dụng các lớp MoE thông thường với 4, 32 và 256 chuyên gia và các lớp MoE phân cấp với 256, 1024 và 4096 chuyên gia. Chúng tôi gọi các mô hình kết quả là MoE-4, MoE-32, MoE-256, MoE-256-h, MoE-1024-h và MoE-4096-h. Đối với các lớp MoE phân cấp, yếu tố phân nhánh cấp đầu tiên là 16, tương ứng với số GPU trong cụm của chúng tôi. Chúng tôi sử dụng Noisy-Top-K Gating (xem Phần 2.1) với k=4 cho các lớp MoE thông thường và k=2 ở mỗi cấp của các lớp MoE phân cấp. Do đó, mỗi ví dụ được xử lý bởi chính xác 4 chuyên gia với tổng cộng 4M ops/timestep. Hai lớp LSTM đóng góp 2M ops/timestep mỗi lớp cho tổng mong muốn là 8M.

Baseline Khớp Tính toán: Mô hình MoE-4 không sử dụng tính thưa, vì tất cả 4 chuyên gia luôn được sử dụng. Ngoài ra, chúng tôi huấn luyện thêm bốn mô hình baseline khớp tính toán không có tính thưa:

MoE-1-Wide: Lớp MoE bao gồm một "chuyên gia" đơn chứa một lớp ẩn kích hoạt ReLU có kích thước 4096.

MoE-1-Deep: Lớp MoE bao gồm một "chuyên gia" đơn chứa bốn lớp ẩn kích hoạt ReLU, mỗi lớp có kích thước 1024.

4xLSTM-512: Chúng tôi thay thế lớp MoE bằng hai lớp LSTM 512-đơn vị bổ sung.

LSTM-2048-512: Mô hình chứa một lớp LSTM 2048-đơn vị (và không có MoE). Đầu ra của LSTM được chiếu xuống 512 chiều (Sak et al., 2014). Timestep tiếp theo của LSTM nhận đầu ra được chiếu. Điều này giống hệt với một trong những mô hình được công bố trong (Jozefowicz et al., 2016). Chúng tôi chạy lại nó để tính đến sự khác biệt trong chế độ huấn luyện, và thu được kết quả rất tương tự với những kết quả đã được công bố.

Huấn luyện: Các mô hình được huấn luyện trên một cụm 16 K40 GPU sử dụng phương pháp đồng bộ được mô tả trong Phần 3. Mỗi batch bao gồm một tập hợp các câu tổng cộng khoảng 300.000 từ. Vì thời gian, chúng tôi giới hạn huấn luyện ở 10 epoch, (27.000 bước). Huấn luyện mất 12-16 giờ cho tất cả các mô hình, ngoại trừ MoE-4, mất 18 giờ (vì tất cả tính toán chuyên gia được thực hiện chỉ trên 4 trong số 16 GPU). Chúng tôi sử dụng optimizer Adam (Kingma & Ba, 2015). Tỷ lệ học cơ bản được tăng tuyến tính trong 1000 bước huấn luyện đầu tiên, và giảm sau đó để tỷ lệ thuận với nghịch đảo căn bậc hai của số bước. Lớp đầu ra Softmax được huấn luyện hiệu quả sử dụng lấy mẫu tầm quan trọng tương tự như các mô hình trong (Jozefowicz et al., 2016). Đối với mỗi mô hình, chúng tôi thực hiện tìm kiếm siêu tham số để tìm xác suất dropout tốt nhất, theo bước nhảy 0.1.

Để đảm bảo sử dụng chuyên gia cân bằng, chúng tôi đặt wimportance = 0.1 và wload = 0.1, như được mô tả trong Phần 4 và Phụ lục A.

Kết quả: Chúng tôi đánh giá mô hình của mình bằng perplexity trên tập dữ liệu holdout, được sử dụng bởi (Chelba et al., 2013; Jozefowicz et al., 2016). Chúng tôi theo quy trình tiêu chuẩn và tính tổng trên tất cả các từ bao gồm ký hiệu kết thúc câu. Kết quả được báo cáo trong Bảng 7. Đối với mỗi mô hình, chúng tôi báo cáo perplexity kiểm tra, ngân sách tính toán, số lượng tham số, giá trị DropProb, và hiệu quả tính toán.

Bảng 7: So sánh mô hình trên Benchmark Mô hình hóa Ngôn ngữ 1 Tỷ Từ. Các mô hình được đánh dấu * từ (Jozefowicz et al., 2016).

Model Test Test ops/timestep #Params excluding Total Drop TFLOPS
Perplexity Perplexity (millions) embed. & softmax #Params Prob per GPU
10 epochs (final) (millions) (billions) (observed)
Kneser-Ney 5-gram* 67.6 0.00001 1.8
LSTM-512-512* 54.1 2.4 2.4 0.8 0.1
LSTM-1024-512* 48.2 4.7 4.7 0.8 0.1
LSTM-2048-512* 45.0 43.7 9.4 9.4 0.8 0.1 0.61
LSTM-2048-512 44.7 9.4 9.4 0.8 0.1 1.21
4xLSTM-512 46.0 8.4 8.4 0.8 0.1 1.07
MoE-1-Wide 46.1 8.4 8.4 0.8 0.1 1.29
MoE-1-Deep 45.7 8.4 8.4 0.8 0.1 1.29
MoE-4 45.0 8.4 8.4 0.8 0.1 0.52
MoE-32 39.7 8.4 37.8 0.9 0.1 0.87
MoE-256 35.7 8.6 272.9 1.1 0.1 0.81
MoE-256-h 36.0 8.4 272.9 1.1 0.1 0.89
MoE-1024-h 34.6 8.5 1079.0 1.9 0.2 0.90
MoE-4096-h 34.1 8.9 4303.4 5.1 0.2 0.74
2xLSTM-8192-1024* 34.7 30.6 151.0 151.0 1.8 0.25 1.09
MoE-34M 31.3 33.8 4313.9 6.0 0.3 1.22
MoE-143M 28.0 142.7 4371.1 6.0 0.4 1.56

15

--- TRANG 16 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
C.2 CÁC MÔ HÌNH ĐẮT TIỀN HƠN
Chúng tôi chạy thêm hai mô hình (MoE-34M và MoE-143M) để điều tra hiệu ứng của việc thêm nhiều tính toán hơn khi có lớp MoE lớn. Những mô hình này có ngân sách tính toán 34M và 143M ops/timestep. Tương tự như các mô hình trên, những mô hình này sử dụng lớp MoE giữa hai lớp LSTM. Chiều của lớp embedding, và chiều đầu vào và đầu ra của lớp MoE được đặt thành 1024 thay vì 512. Đối với MoE-34M, các lớp LSTM có 1024 đơn vị. Đối với MoE-143M, các lớp LSTM có 4096 đơn vị và một chiếu đầu ra có kích thước 1024 (Sak et al., 2014). MoE-34M sử dụng lớp MoE phân cấp với 1024 chuyên gia, mỗi cái có lớp ẩn kích thước 2048. MoE-143M sử dụng lớp MoE phân cấp với 256 chuyên gia, mỗi cái có lớp ẩn kích thước 8192. Cả hai mô hình đều có 4B tham số trong các lớp MoE. Chúng tôi tìm kiếm DropProb tốt nhất cho mỗi mô hình, và huấn luyện mỗi mô hình trong 10 epoch.

Hai mô hình đạt được perplexity kiểm tra 31.3 và 28.0 tương ứng, cho thấy rằng ngay cả khi có MoE lớn, nhiều tính toán hơn vẫn hữu ích. Kết quả được báo cáo ở cuối Bảng 7. Mô hình lớn hơn trong hai mô hình có ngân sách tính toán tương tự như mô hình được công bố tốt nhất từ tài liệu, và thời gian huấn luyện tương tự. So sánh sau 10 epoch, mô hình của chúng tôi có perplexity kiểm tra thấp hơn 18%.

D CORPUS TIN TỨC GOOGLE 100 TỶ TỪ - CHI TIẾT THÍ NGHIỆM
Kiến trúc Mô hình: Các mô hình tương tự về cấu trúc với các mô hình 8-triệu-phép-toán-mỗi-timestep được mô tả trong phần trước. Chúng tôi thay đổi số lượng chuyên gia giữa các mô hình, sử dụng lớp MoE thông thường với 32 chuyên gia và các lớp MoE phân cấp với 256, 1024, 4096, 16384, 65536 và 131072 chuyên gia. Đối với các lớp MoE phân cấp, các yếu tố phân nhánh cấp đầu tiên lần lượt là 32, 32, 64, 128, 256 và 256.

Huấn luyện: Các mô hình được huấn luyện trên cụm 32 Tesla K40 GPU, ngoại trừ hai mô hình cuối, được huấn luyện trên các cụm 64 và 128 GPU để có đủ bộ nhớ cho tất cả các tham số. Đối với tất cả các mô hình, kích thước batch huấn luyện khoảng 2.5 triệu từ. Các mô hình được huấn luyện một lần qua khoảng 100 tỷ từ.

Chúng tôi triển khai một số tối ưu hóa bộ nhớ để khớp lên đến 1 tỷ tham số trên mỗi GPU. Đầu tiên, chúng tôi không lưu trữ các kích hoạt của các lớp ẩn của các chuyên gia, mà thay vào đó tính toán lại chúng trong lần chuyển ngược. Thứ hai, chúng tôi sửa đổi optimizer trên các tham số chuyên gia để yêu cầu ít lưu trữ phụ trợ hơn:

Optimizer Adam (Kingma & Ba, 2015) giữ các ước lượng moment thứ nhất và thứ hai của gradient theo tham số. Điều này làm tăng gấp ba lần bộ nhớ yêu cầu. Để tránh giữ ước lượng moment thứ nhất, chúng tôi đặt β1 = 0. Để giảm kích thước của ước lượng moment thứ hai, chúng tôi thay thế nó bằng xấp xỉ được phân tích. Đối với ma trận tham số, thay vì duy trì ma trận đầy đủ các ước lượng moment thứ hai, chúng tôi duy trì các vector trung bình theo hàng và theo cột của ma trận đó. Ở mỗi bước, ma trận ước lượng được lấy là tích ngoài của hai vector đó chia cho trung bình của một trong hai. Kỹ thuật này có thể được áp dụng tương tự cho Adagrad (Duchi et al., 2010).

Bảng 8: So sánh mô hình trên Tập dữ liệu Tin tức Google 100 Tỷ Từ

Model Test Test ops/timestep #Params excluding Total TFLOPS
Perplexity Perplexity (millions) embed. & softmax #Params per GPU
.1 epochs 1 epoch (millions) (billions) (observed)
Kneser-Ney 5-gram 67.1 45.3 0.00001 76.0
4xLSTM-512 54.5 47.0 8.4 8.4 0.1 1.23
MoE-32 48.5 40.4 8.4 37.8 0.1 0.83
MoE-256-h 42.8 35.3 8.4 272.9 0.4 1.11
MoE-1024-h 40.3 32.7 8.5 1079.0 1.2 1.14
MoE-4096-h 38.9 30.9 8.6 4303.4 4.4 1.07
MoE-16384-h 38.2 29.7 8.8 17201.0 17.3 0.96
MoE-65536-h 38.2 28.9 9.2 68791.0 68.9 0.72
MoE-131072-h 39.8 29.2 9.7 137577.6 137.7 0.30

Kết quả: Chúng tôi đánh giá mô hình của mình bằng perplexity trên tập dữ liệu holdout. Kết quả được báo cáo trong Bảng 8. Perplexity sau 100 tỷ từ huấn luyện thấp hơn 39% cho mô hình MoE 68-tỷ-tham số so với mô hình baseline. Đáng chú ý là hiệu quả tính toán đo được của mô hình lớn nhất (0.30 TFLOPS/GPU) rất thấp so với các mô hình khác. Điều này có thể là kết quả của việc, cho mục đích so sánh với các mô hình khác, chúng tôi không tăng kích thước batch huấn luyện tỷ lệ thuận với số GPU. Để so sánh, chúng tôi bao gồm kết quả cho mô hình baseline khớp tính toán bao gồm 4 LSTM, và cho mô hình 5-gram không được cắt tỉa với làm mịn Kneser-Ney (Kneser & Ney, 1995).⁴

E DỊCH MÁY - CHI TIẾT THÍ NGHIỆM
Kiến trúc Mô hình cho Các Mô hình MoE Cặp Ngôn ngữ Đơn: Mô hình của chúng tôi là phiên bản được sửa đổi của mô hình GNMT được mô tả trong (Wu et al., 2016). Để giảm tính toán, chúng tôi giảm số lượng lớp LSTM trong encoder và decoder từ 9 và 8 xuống còn 3 và 2 tương ứng. Chúng tôi chèn các lớp MoE trong cả encoder (giữa lớp 2 và 3) và decoder (giữa lớp 1 và 2). Chúng tôi sử dụng cơ chế attention giữa encoder và decoder, với LSTM decoder đầu tiên nhận đầu ra từ và cung cấp đầu vào cho attention⁵. Tất cả các lớp trong mô hình của chúng tôi có chiều đầu vào và đầu ra là 512. Các lớp LSTM của chúng tôi có 2048 đơn vị ẩn, với chiếu đầu ra 512 chiều. Chúng tôi thêm kết nối dư xung quanh tất cả các lớp LSTM và MoE để khuyến khích dòng gradient (He et al., 2015). Tương tự như GNMT, để xử lý hiệu quả các từ hiếm, chúng tôi sử dụng các đơn vị từ phụ (còn được gọi là "wordpieces") (Schuster & Nakajima, 2012) cho đầu vào và đầu ra trong hệ thống của chúng tôi.

Chúng tôi sử dụng từ vựng nguồn và đích chung gồm 32K wordpiece. Chúng tôi cũng sử dụng cùng kỹ thuật tìm kiếm beam như được đề xuất trong (Wu et al., 2016).

Chúng tôi huấn luyện các mô hình với số lượng chuyên gia khác nhau trong các lớp MoE. Ngoài mô hình baseline không có lớp MoE, chúng tôi huấn luyện các mô hình với các lớp MoE phẳng chứa 32 chuyên gia, và các mô hình với các lớp MoE phân cấp chứa 512 và 2048 chuyên gia. Các lớp MoE phẳng sử dụng k=4 và các mô hình MoE phân cấp sử dụng k=2 ở mỗi cấp của mạng cổng. Do đó, mỗi đầu vào được xử lý bởi chính xác 4 chuyên gia trong mỗi lớp MoE. Mỗi chuyên gia trong lớp MoE là mạng feed forward với một lớp ẩn có kích thước 2048 và kích hoạt ReLU. Do đó, mỗi chuyên gia chứa [512×2048] + [2048×512] = 2M tham số. Đầu ra của lớp MoE được truyền qua hàm sigmoid. Chúng tôi sử dụng hàm cổng cân bằng nghiêm ngặt được mô tả trong Phụ lục F.

Kiến trúc Mô hình cho Mô hình MoE Đa ngôn ngữ: Chúng tôi sử dụng cùng kiến trúc mô hình như cho các mô hình cặp ngôn ngữ đơn, với các ngoại lệ sau: Chúng tôi sử dụng noisy-top-k gating như được mô tả trong Phần 2.1, không phải sơ đồ từ Phụ lục F. Các lớp MoE trong encoder và decoder là MoE không phân cấp với n=512 chuyên gia, và k=2. Mỗi chuyên gia có lớp ẩn lớn hơn có kích thước 8192. Điều này tăng gấp đôi lượng tính toán trong các lớp MoE, nâng ngân sách tính toán của toàn bộ mô hình từ 85M lên 102M ops/timestep.

Huấn luyện: Chúng tôi huấn luyện các mạng của mình bằng optimizer Adam (Kingma & Ba, 2015). Tỷ lệ học cơ bản được tăng tuyến tính trong 2000 bước huấn luyện đầu tiên, giữ không đổi trong 8000 bước bổ sung, và giảm sau đó để tỷ lệ thuận với nghịch đảo căn bậc hai của số bước. Đối với các mô hình cặp ngôn ngữ đơn, tương tự như (Wu et al., 2016), chúng tôi áp dụng dropout (Zaremba et al., 2014) cho đầu ra của tất cả các lớp embedding, LSTM và MoE, sử dụng DropProb = 0.4. Huấn luyện được thực hiện đồng bộ trên cụm lên đến 64 GPU như được mô tả trong phần 3. Mỗi batch huấn luyện bao gồm một tập hợp các cặp câu chứa khoảng 16000 từ trên mỗi GPU.

Để đảm bảo sử dụng chuyên gia cân bằng, chúng tôi đặt wimportance = 0.01 và wload = 0.01, như được mô tả trong Phần 4 và Phụ lục A.

Metric: Chúng tôi đánh giá các mô hình của mình bằng perplexity và metric điểm BLEU tiêu chuẩn. Chúng tôi báo cáo điểm BLEU được tokenize như được tính toán bởi script multi-bleu.pl, được tải xuống từ việc triển khai công khai của Moses (trên Github), cũng được sử dụng trong (Luong et al., 2015a).

⁴Mặc dù kích thước ban đầu của corpus là 130 tỷ từ, các mô hình neural được huấn luyện tối đa 100 tỷ từ. Các mô hình Kneser-Ney 5-gram được báo cáo được huấn luyện trên 13 tỷ và 130 tỷ từ tương ứng, mang lại cho chúng một lợi thế nhỏ so với các kết quả được báo cáo khác.
⁵Vì lý do hiệu suất, chúng tôi sử dụng hàm attention hơi khác so với hàm được mô tả trong (Wu et al., 2016) - Xem Phụ lục G

17

--- TRANG 18 ---
Đang được xem xét như một bài báo hội nghị tại ICLR 2017
Kết quả: Bảng 2, 3 và 4 trong Phần 5.3 cho thấy so sánh kết quả của chúng tôi với các phương pháp được công bố khác. Hình 4 cho thấy perplexity kiểm tra như một hàm của số từ trong (của dữ liệu huấn luyện) câu nguồn được xử lý cho các mô hình với số lượng chuyên gia khác nhau. Như có thể thấy từ Hình, khi chúng tôi tăng số lượng chuyên gia để tiếp cận 2048, perplexity kiểm tra của mô hình của chúng tôi tiếp tục cải thiện.

[Có hai biểu đồ hiển thị perplexity theo số từ nguồn được xử lý cho WMT'14 En→Fr (trái) và Google Production En→Fr (phải)]

Hình 4: Perplexity trên các tập dữ liệu WMT'14 En→Fr (trái) và Google Production En→Fr (phải) như một hàm của số từ được xử lý. Sự khác biệt lớn giữa các mô hình ở đầu huấn luyện là do kích thước batch khác nhau. Tất cả các mô hình phát sinh cùng ngân sách tính toán (85M ops/timestep) ngoại trừ mô hình không có chuyên gia.

Chúng tôi thấy rằng các chuyên gia thực sự trở nên chuyên môn hóa cao bằng cú pháp và/hoặc ngữ nghĩa, như có thể thấy trong Bảng 9. Ví dụ, một chuyên gia được sử dụng khi mạo từ không xác định "a" giới thiệu tân ngữ trực tiếp trong cụm động từ chỉ tầm quan trọng hoặc lãnh đạo.

Bảng 9: Ngữ cảnh tương ứng với một vài trong số 2048 chuyên gia trong lớp MoE trong phần encoder của mô hình dịch WMT'14 En→Fr. Đối với mỗi chuyên gia i, chúng tôi sắp xếp các đầu vào trong batch huấn luyện theo thứ tự giảm dần của G(x)i, và hiển thị các từ xung quanh các vị trí tương ứng trong các câu đầu vào.

Expert 381 Expert 752 Expert 2004
... with researchers , ... ... plays acore ... ... with rapidly growing ...
... to innovation . ... plays acritical ... ... under static conditions ...
... tics researchers . ... provides alegislative ... ... to swift ly ...
... the generation of ... ... play aleading ... ... to dras tically ...
... technology innovations is ... ... assume aleadership ... ... the rapid and ...
... technological innovations , ... ... plays acentral ... ... the fastest ...
... support innovation throughout ... ... taken aleading ... ... the Quick Method ...
... role innovation will ... ... established areconciliation ... ... rec urrent ) ...
... research scienti st ... ... played avital ... ... provides quick access ...
... promoting innovation where ... ... have acentral ... ... of volatile organic ...
... ... ...

F CỔNG CÂN BẰNG NGHIÊM NGẶT
Do một số đặc thù trong cơ sở hạ tầng của chúng tôi đã được sửa từ đó, vào thời điểm chúng tôi chạy một số thí nghiệm dịch máy, các mô hình của chúng tôi chạy nhanh hơn nếu mọi chuyên gia nhận được chính xác cùng kích thước batch. Để phù hợp với điều này, chúng tôi sử dụng hàm cổng khác mà chúng tôi mô tả dưới đây.

Nhớ lại rằng chúng tôi định nghĩa hàm cổng softmax là:

G(x) = Softmax(xWg) (15)

Cổng Thưa (công thức thay thế): Để có được vector cổng thưa, chúng tôi nhân G(x) theo từng thành phần với mặt nạ thưa M(G(x)) và chuẩn hóa đầu ra. Bản thân mặt nạ là hàm của G(x) và chỉ định chuyên gia nào được gán cho mỗi ví dụ đầu vào:

G'(x)i = G(x)i × M(G(x))i / Σ(j=1 to n) G(x)j × M(G(x))j (16)

Mặt nạ Top-K: Để triển khai cổng top-k trong công thức này, chúng tôi sẽ cho M(v) = TopK(v, k), trong đó:

TopK(v, k)i = {1 nếu vi trong k phần tử hàng đầu của v, 0 nếu không} (17)

Mặt nạ Theo Batch: Để buộc mỗi chuyên gia nhận được chính xác cùng số lượng ví dụ, chúng tôi giới thiệu hàm mặt nạ thay thế, Mbatchwise(X, m), hoạt động trên các batch vector đầu vào. Thay vì giữ k giá trị hàng đầu cho mỗi ví dụ, chúng tôi giữ m giá trị hàng đầu cho mỗi chuyên gia trên batch huấn luyện, trong đó m = k|X|/n, để mỗi ví dụ được gửi đến trung bình k chuyên gia.

Mbatchwise(X, m)j,i = {1 nếu Xj,i trong m giá trị hàng đầu cho chuyên gia i, 0 nếu không} (18)

Như các thí nghiệm của chúng tôi gợi ý và cũng được quan sát trong (Ioffe & Szegedy, 2015), việc sử dụng hàm theo batch trong huấn luyện (như Mbatchwise) yêu cầu sửa đổi suy luận khi chúng tôi có thể không có batch lớn các ví dụ. Giải pháp của chúng tôi cho điều này là huấn luyện một vector T các giá trị ngưỡng cho mỗi chuyên gia để xấp xỉ hiệu ứng của mặt nạ theo batch. Chúng tôi sử dụng mặt nạ sau tại thời gian suy luận:

Mthreshold(x, T)i = {1 nếu xi > Ti, 0 nếu không} (19)

Để học các giá trị ngưỡng, chúng tôi áp dụng mất mát bổ sung tại thời gian huấn luyện được tối thiểu hóa khi mặt nạ theo batch và mặt nạ ngưỡng giống hệt nhau.

Lbatchwise(X, T, m) = Σ(j=1 to |X|) Σ(i=1 to n) (Mthreshold(x, T)i - Mbatchwise(X, m)j,i) × (Xj,i - Ti) (20)

G HÀM ATTENTION
Cơ chế attention được mô tả trong GNMT (Wu et al., 2016) liên quan đến "Hàm Attention" A(xi, yj) đã học nhận "vector nguồn" xi và "vector đích" yj, và phải được tính toán cho mọi bước thời gian nguồn i và bước thời gian đích j. Trong GNMT, hàm attention được triển khai như một mạng neural feed forward với lớp ẩn có kích thước n. Nó có thể được biểu diễn như:

AGNMT(xi, yj) = Σ(d=1 to n) Vd × tanh((xiU)d + (yjW)d) (21)

Trong đó U và W là các ma trận trọng số có thể huấn luyện và V là vector trọng số có thể huấn luyện.

Vì lý do hiệu suất, trong các mô hình của chúng tôi, chúng tôi sử dụng hàm attention hơi khác:

A(xi, yj) = Σ(d=1 to n) Vd × tanh((xiU)d) × tanh((yjW)d) (22)

Với hàm attention của chúng tôi, chúng tôi có thể đồng thời tính toán hàm attention trên nhiều bước thời gian nguồn và nhiều bước thời gian đích sử dụng các phép nhân ma trận được tối ưu. Chúng tôi thấy rất ít khác biệt về chất lượng giữa hai hàm.

19
