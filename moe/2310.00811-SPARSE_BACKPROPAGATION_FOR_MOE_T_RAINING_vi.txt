# 2310.00811.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2310.00811.pdf
# Kích thước tệp: 550563 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
BACKPROPAGATION THƯA THỚT CHO HUẤN LUYỆN MOE
Liyuan Liu§Jianfeng Gao§Weizhu Chen‡
§Microsoft Research ‡Microsoft Azure AI
{lucliu, jfgao, wzchen }@microsoft.com
TÓM TẮT
Một đặc điểm xác định của các mô hình Mixture-of-Expert (MoE) là khả năng thực hiện tính toán thưa thớt thông qua định tuyến chuyên gia, dẫn đến khả năng mở rộng đáng kể. Tuy nhiên, backpropagation, nền tảng của deep learning, yêu cầu tính toán dày đặc, do đó gây ra những thách thức trong tính toán gradient MoE. Tại đây, chúng tôi giới thiệu SparseMixer, một bộ ước lượng gradient có thể mở rộng nhằm thu hẹp khoảng cách giữa backpropagation và định tuyến chuyên gia thưa thớt. Không giống như huấn luyện MoE điển hình có chiến lược bỏ qua một số hạng gradient vì mục đích tính toán thưa thớt và khả năng mở rộng, SparseMixer cung cấp các xấp xỉ gradient có thể mở rộng cho những hạng này, cho phép ước lượng gradient đáng tin cậy trong huấn luyện MoE. Dựa trên khung ODE số học, SparseMixer khai thác phương pháp trung điểm, một bộ giải ODE bậc hai, để cung cấp xấp xỉ gradient chính xác với chi phí tính toán không đáng kể. Áp dụng SparseMixer cho Switch Transformer trên cả nhiệm vụ pre-training và dịch máy, SparseMixer thể hiện tăng hiệu suất đáng kể, tăng tốc hội tụ huấn luyện lên đến 2 lần1.

1 GIỚI THIỆU
Thành công đáng kể của pre-training quy mô lớn qua các ứng dụng khác nhau đã nhấn mạnh nhu cầu cấp thiết về các mô hình có thể mở rộng và khả thi về mặt kinh tế (Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023). Những tiến bộ gần đây trong mạng kích hoạt thưa thớt, được biết đến rộng rãi là Mixture-of-Experts (MoE), đã thu hút sự quan tâm rộng rãi (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021; Riquelme et al., 2021; Mustafa et al., 2022). Không giống như các mạng truyền thống kích hoạt dày đặc tất cả các mô-đun cho mọi đầu vào, MoE chọn lọc kích hoạt các phần của mô-đun cho các đầu vào cụ thể thông qua một quá trình gọi là định tuyến chuyên gia, dẫn đến cải thiện hiệu quả đáng kể.

Tuy nhiên, sự tăng hiệu quả này đi kèm với chi phí: ước lượng gradient trong MoE trở nên khó khăn do định tuyến chuyên gia. Cụ thể, hàm định tuyến, về bản chất là rời rạc, tạo ra các đầu ra không khả vi. Trong khi đó, backpropagation, nền tảng của deep learning, dựa vào quy tắc chuỗi, khiến nó chỉ tương thích với các hàm khả vi (Rosenblatt, 1957; Bengio et al., 2013), và không thể được áp dụng trực tiếp cho tính toán gradient của định tuyến chuyên gia.

Nhiều phương pháp đã xuất hiện để thu hẹp khoảng cách giữa rời rạc và backpropagation, và hầu hết chúng đều dựa trên Straight-Through (ST) (Rosenblatt, 1957; Bengio et al., 2013; Jang et al., 2017; Liu et al., 2023). Thật không may, tất cả các bộ ước lượng ST hiện có đều không tương thích với MoE, vì chúng yêu cầu kích hoạt tất cả các chuyên gia để tính toán gradient, do đó loại bỏ tất cả các cải thiện hiệu quả của MoE. Do đó, huấn luyện MoE điển hình có chiến lược bỏ qua tính toán gradient cho định tuyến, đánh đổi một số tín hiệu huấn luyện để có tính toán thưa thớt. Bất chấp khả năng mở rộng được mang lại bởi tính toán thưa thớt, sự đánh đổi này có thể dẫn đến hội tụ chậm và các mô hình được huấn luyện không đúng cách.

Giải pháp của chúng tôi cho tình huống khó xử này là SparseMixer—một phương pháp mới được thiết kế để hòa giải sự chia cách giữa định tuyến MoE thưa thớt và backpropagation. Lấy cảm hứng từ các phương pháp số học cho phương trình vi phân thường (ODE), SparseMixer cung cấp xấp xỉ gradient đáng tin cậy cho định tuyến chuyên gia, ngay cả khi chỉ một tập con các chuyên gia được kích hoạt. Hơn nữa, để cung cấp xấp xỉ gradient chính xác với chi phí tính toán không đáng kể, chúng tôi tích hợp phương pháp trung điểm, một bộ giải ODE bậc hai, để khớp với khai triển Taylor của gradient đến bậc hai mà không cần ma trận Hessian hoặc các đạo hàm bậc hai khác.

1Các triển khai có sẵn tại https://github.com/microsoft/SparseMixer/.

--- TRANG 2 ---
Chúng tôi áp dụng SparseMixer cho Switch Transformer trên cả pre-training và dịch máy neural. SparseMixer không chỉ tăng tốc hội tụ huấn luyện lên đến hai lần mà còn hỗ trợ MoE với định tuyến chuyên gia được huấn luyện đúng cách. Đáng chú ý, trong khi Switch Transformer kém hiệu quả hơn mô hình dày đặc trong cả ba thiết lập pre-training, việc tích hợp SparseMixer như một bộ ước lượng gradient cho phép các mô hình MoE kết quả vượt trội liên tục so với mô hình dày đặc.

2 CÔNG TRÌNH LIÊN QUAN VÀ KIẾN THỨC CƠ SỞ
Mixture-of-Expert cho Transformer. Ý tưởng về các mô hình Mixture-of-Expert bắt nguồn từ Jacobs et al. (1991) và Jordan & Jacobs (1994), tích hợp nhiều mạng riêng biệt và sử dụng từng mạng để xử lý một tập con riêng biệt của các trường hợp huấn luyện. Gần đây, nhiều nỗ lực đã được thực hiện để tận dụng ý tưởng này để mở rộng các mô hình ngôn ngữ lớn (Shazeer et al., 2017; Lepikhin et al., 2020; Lewis et al., 2021; Fedus et al., 2021).

Để giữ mọi thứ đơn giản, chúng tôi sẽ đầu tiên tập trung vào thiết lập đơn giản hóa của lớp Switch Transformer (Fedus et al., 2021). Sau đó chúng tôi sẽ thảo luận về sự khác biệt của nó với Switch Transformer và các điều chỉnh cần thiết trong Phần 3.5, trong khi thuật toán kết quả có thể dễ dàng mở rộng sang các thiết kế MoE khác. Xét một tập hợp N chuyên gia, {fi(x)}N i=1, giá trị cổng của chuyên gia i được tính với hàm softmax là πi=softmax(θ)i=exp(θi)Pn j=1exp(θj), trong đó θ=Wr·x. Với i∈[1,···,N], chúng tôi đánh dấu biểu diễn one-hot của nó là Ii∈RN×1, có phần tử bằng 1 nếu nó là phần tử thứ i hoặc bằng 0 nếu khác. Gọi D là một biến ngẫu nhiên rời rạc và D∈{I1,···,IN}. Lưu ý rằng D được lấy mẫu là D∼π trong quá trình huấn luyện, và được tính là D←arg maxIiπIi trong quá trình suy luận.

Khi đó, đầu ra cuối cùng của lớp MoE này là y=πDfD(x). Đánh dấu các phần khác của mạng neural là một hàm khả vi g:Rn→R, chúng tôi nhắm mục tiêu tối thiểu hóa:

min WrL(Wr),trong đó L(Wr)=ED∼softmax(Wrx)[g(πDfD(x))]=X DπD·g(πDfD(x)).(1)

Tính toán Gradient cho Định tuyến Chuyên gia. Để đơn giản, chúng tôi đánh dấu ∂L(Wr) ∂Wr là ∇0+∇1:

∂L ∂Wr:=∇0+∇1,trong đó ∇0=X Iig(πIifIi(x))∂πIi ∂Wr và ∇1=X IiπIi∂g(πIifIi(x)) ∂Wr.(2)

Dễ dàng nhận thấy rằng ∇1 có thể được tính toán đáng tin cậy qua backpropagation. Tuy nhiên, ∇0 khó ước lượng đáng tin cậy trong thực hành huấn luyện MoE điển hình. Trong nghiên cứu này, chúng tôi tập trung thảo luận về ∇0.

[Hình 1: Đường cong huấn luyện của Switch Transformer trên WMT'14 En-De.]

--- TRANG 3 ---
REINFORCE (Williams, 1992) là không thiên vị (tức là, E[∇REINFORCE]=∇0) và chỉ yêu cầu phân phối của biến rời rạc phải khả vi (tức là, không có backpropagation qua g):

∇REINFORCE:=g(πDfD(x))∂logπD ∂Wr.(3)

Mặc dù bộ ước lượng ∇REINFORCE không thiên vị, nó có xu hướng có phương sai cao một cách cấm đoán, đặc biệt đối với các mạng có nguồn ngẫu nhiên khác (tức là, dropout hoặc các biến ngẫu nhiên độc lập khác). Gần đây, các nỗ lực đã được thực hiện để giảm phương sai của REINFORCE (Gu et al., 2016; Tucker et al., 2017; Grathwohl et al., 2018; Shi et al., 2022). Tuy nhiên, người ta thấy rằng các bộ ước lượng kiểu REINFORCE không hoạt động tốt trong huấn luyện MoE (Kool et al., 2021).

Straight-Through. Mặc dù ∇REINFORCE không thiên vị, nó xử lý mạng còn lại (g) như một hộp đen và chỉ tận dụng thông tin bậc không của g. Trong thực tế, một họ bộ ước lượng phổ biến, Straight-Through (ST), tận dụng thông tin bậc một của g (lưu ý rằng g là một vô hướng và g′ là một vector), đã được chứng minh đạt được hiệu suất tốt hơn trong các thiết lập phức tạp hơn (Liu et al., 2023). ST tính toán backpropagation "qua" một surrogate coi hàm không khả vi (ví dụ, việc lấy mẫu D) như một hàm đồng nhất (Rosenblatt, 1957; Bengio et al., 2013; Jang et al., 2017; Liu et al., 2023). Trong thiết lập MoE của chúng tôi, ST xử lý việc lấy mẫu D như một hàm đồng nhất và ước lượng gradient là:

b∇ST:=∂g(πDfD(x)) ∂πDfD(x)∂P iDiπIifIi(x) ∂D∂πD ∂Wr.(4)

Một chiến lược thay thế là thực hiện relaxation biến ngẫu nhiên cụ thể (Maddison et al., 2014; Jang et al., 2017). Người ta quan sát thấy rằng việc lấy mẫu D có thể được tham số hóa lại sử dụng biến ngẫu nhiên Gumbel tại giới hạn nhiệt độ không của softmax có nhiệt độ (Gumbel, 1954):

D=lim τ→0Sτ,trong đó Sτ=softmax τ(θ+G),Gi là i.i.d., và Gi∼Gumbel(0,1).

Straight-Through Gumbel-Softmax (STGS) xử lý giới hạn nhiệt độ không như hàm đồng nhất trong quá trình backpropagation:

b∇STGS:=∂g(πDfD(x)) ∂πDfD(x)∂P iSτ,iπIifIi(x) ∂Sτ∂Sτ ∂Wr.(5)

Mặc dù E[b∇ST] đã được thiết lập chính thức như một xấp xỉ bậc một của ∇0 (Liu et al., 2023), áp dụng các bộ ước lượng ST đòi hỏi nhu cầu tính toán fi(x) cho tất cả i∈{I1,···,IN}, tức là, các đầu ra từ tất cả các chuyên gia. Ví dụ, trong Phương trình 4, chúng ta có ∂P iDiπIifIi(x) ∂D= diag(P iDiπIifIi(x)), liên quan đến việc tính toán {fI1(x),···,fIN(x)}. Về cơ bản, tính toán tất cả fIi biến MoE thành một mạng được kích hoạt dày đặc. Do đó, sử dụng các bộ ước lượng kiểu ST làm suy yếu tính toán thưa thớt, về cơ bản cản trở việc mở rộng các mô hình MoE.

Thực hành Huấn luyện MoE. Do tất cả những thách thức này, thực hành huấn luyện MoE hiện tại đánh đổi một số tín hiệu huấn luyện để có khả năng mở rộng. Cụ thể, ∇0 được bỏ qua một cách có chiến lược trong tính toán gradient (giá trị của ∇0 được đặt thành 0), và chỉ ∇1 được sử dụng cho huấn luyện mô hình (Fedus et al., 2021). Mặc dù thành công của thực hành này, vẫn chưa rõ về tác động của việc bỏ qua ∇0, cách tiến hành huấn luyện chỉ với một phần gradient, và liệu gradient descent có còn hiệu quả sau khi bỏ qua ∇0. Trong nghiên cứu này, chúng tôi nhắm mục tiêu thu hẹp khoảng cách giữa backpropagation và định tuyến chuyên gia bằng cách cung cấp xấp xỉ có thể mở rộng và đáng tin cậy của ∇0 và mở rộng các mô hình MoE mà không bỏ qua ∇0.

3 TỪ RỜI RẠC ĐẾN THƯA THỚT: SPARSE MIXER

Mặc dù các bộ ước lượng ST đã thu hẹp khoảng cách giữa các biến rời rạc và backpropagation, chúng yêu cầu mạng phải được kích hoạt dày đặc. Ở đây, chúng tôi đầu tiên thảo luận về hạn chế nội tại của các bộ ước lượng ST. Sau đó, chúng tôi vượt ra ngoài rời rạc và thu hẹp khoảng cách giữa định tuyến chuyên gia thưa thớt và backpropagation.

3.1 TẠI SAO CÁC BỘ ƯỚC LƯỢNG ST HIỆN CÓ KHÔNG THỂ MỞ RỘNG?

Nhắm mục tiêu xấp xỉ gradient cho các biến rời rạc trong trường hợp đa thức tổng quát, chúng tôi thiết lập chính thức rằng E[b∇ST] là một xấp xỉ bậc một của ∇0 trong Liu et al. (2023). Để thảo luận ST

--- TRANG 4 ---
trong thiết lập tổng quát, chúng tôi tham số hóa lại mạng chuyên gia y←πDfD như một hàm của các biến rời rạc, được đánh dấu là y←h(D)=P iDiπIifIi. Khi đó, chúng ta có2:

∇0=X Ii(h(Ii)−E[h])∂πIi ∂Wr=X IiX IjπIj(h(Ii)−h(Ij))∂πIi ∂Wr.(6)

Cụ thể, xấp xỉ h(Ii)−h(Ij) là h′(Ij)·(Ii−Ij), xấp xỉ gradient kết quả sẽ có cùng dạng với E[b∇ST] (Liu et al., 2023). Trong phân tích số học, xấp xỉ này được biết đến như phương pháp Euler tiến (giới thiệu ngắn gọn trong Phụ lục A), có độ chính xác bậc một. Trong Liu et al. (2023), chúng tôi cũng khám phá các bộ giải ODE bậc cao hơn để xấp xỉ h(Ii)−h(Ij) tốt hơn. Tuy nhiên, tất cả những nỗ lực này đều yêu cầu mạng phải được kích hoạt dày đặc, vì ∂h ∂D=P IiDiπIifIi đòi hỏi tính toán {fI1,···,fIN}. Nói cách khác, mặc dù những bộ ước lượng ST này thu hẹp khoảng cách giữa rời rạc và backpropagation, các tính toán của chúng là dày đặc thay vì thưa thớt, cản trở việc áp dụng chúng vào huấn luyện MoE.

3.2 XẤP XỈ GRADIENT ĐỊNH TUYẾN CHUYÊN GIA: BACKPROPAGATION TRỞ NÊN THƯA THỚT

Để thu hẹp khoảng cách giữa định tuyến MoE thưa thớt và backpropagation, chúng ta cần xấp xỉ ∇0 mà không yêu cầu đầu ra từ tất cả các chuyên gia. Trong nghiên cứu của chúng tôi, chúng tôi trình bày một khung mới để vượt ra ngoài ST và thu hẹp khoảng cách giữa backpropagation và định tuyến chuyên gia thưa thớt.

Xấp xỉ Gradient cho Định tuyến Chuyên gia. Ở đây, chúng tôi bắt đầu bằng cách giới thiệu bộ ước lượng gradient đơn giản nhất, tức là, b∇SparseMixer-1st, trong đó

b∇SparseMixer-1st:=∂g(πDfD(x)) ∂Wr.

Tương tự như E[b∇ST], E[b∇SparseMixer-1st] là một xấp xỉ bậc một của ∇0. Để chứng minh điều này, chúng tôi có một cách tiếp cận thay thế để viết lại ∇0:

∇0=X Ii(g(πIifIi)−g(0))∂πIi ∂Wr.(7)

Áp dụng phương pháp Euler, chúng tôi ước lượng g(πIifIi)−g(0) là g′(πIifIi)·πIifIi. Khi đó, chúng ta có:

∇0forward Euler≈X Iig′(πIifIi)·πIifIi·∂πIi ∂Wr=ED∼π[∂g(πDfD(x)) ∂Wr]=E[b∇SparseMixer-1st].

Xấp xỉ Gradient cho Các Biến Rời rạc Tổng quát. Để so sánh với các bộ ước lượng ST hiện có, chúng tôi áp dụng b∇SparseMixer-1st cho trường hợp tổng quát. Tương tự Phương trình 7, chúng ta có

∇0=X Ii(h(Ii)−h(0))∂πIi ∂Wrforward Euler≈X Iih′(Ii)·Ii·∂πIi ∂Wr.(8)

Đáng chú ý, xấp xỉ bậc một của Phương trình 8 chỉ yêu cầu đầu ra của một chuyên gia, tức là,

h′(Ii)·Ii=X IjDIj·πIj·fIj·Ii=πIjfIj.(9)

Nói cách khác, Phương trình 8, lấy h(0) làm baseline, tận dụng biểu diễn one-hot Ii để giảm yêu cầu tính toán của các chuyên gia không được kích hoạt, do đó đạt được tính toán thưa thớt. Trong khi đó, xấp xỉ bậc một của Phương trình 6, tức là, h′(Ij)·(Ii−Ij), có hạng h′(Ij)·Ii và yêu cầu tính toán dày đặc.

Tóm lại, cả b∇ST và b∇SparseMixer-1st đều áp dụng phương pháp Euler tiến và đạt được độ chính xác bậc một. Đồng thời, b∇SparseMixer-1st chỉ yêu cầu đầu ra của một chuyên gia do đó không hy sinh khả năng mở rộng, trong khi b∇ST yêu cầu đầu ra của tất cả các chuyên gia.

2Thường được gọi là baseline subtraction. Lưu ý P iE[g]∂πIi ∂Wr=E[g]∂P IiπIi ∂Wr=E[g]∂1 ∂Wr=0.

--- TRANG 5 ---
3.3 ĐẠT ĐƯỢC ĐỘ CHÍNH XÁC BẬC HAI VỚI PHƯƠNG PHÁP TRUNG ĐIỂM

Tài liệu về các phương pháp số học cho phương trình vi phân cho thấy rằng có thể đạt được độ chính xác bậc cao hơn mà không cần tính toán các đạo hàm bậc cao hơn. Để cung cấp xấp xỉ gradient chính xác, chúng tôi sử dụng phương pháp ODE bậc hai, phương pháp trung điểm (giới thiệu ngắn gọn trong Phụ lục A). Cụ thể, b∇SparseMixer-2rd là một xấp xỉ bậc hai của ∇, trong đó

b∇SparseMixer-2rd:=2·∂g(πDfD(x) 2) ∂Wr.

Để chứng minh mối liên hệ giữa b∇SparseMixer-2rd và phương pháp trung điểm, chúng tôi sử dụng phương pháp trung điểm để xấp xỉ g(πIifIi)−g(0) là g′(πIifIi 2)·πIifIi, cũng chỉ yêu cầu đầu ra của một chuyên gia. Tương tự, dễ dàng nhận thấy:

∇0mid−point≈X Iig′(πIifIi 2)·πIifIi·∂πIi ∂Wr=ED∼π[2·∂g(πDfD(x) 2) ∂Wr]=E[b∇SparseMixer-2rd].

Đáng chú ý, có thể sử dụng các bộ giải ODE tiên tiến hơn như RKF4 và xấp xỉ ∇0 với độ chính xác bậc cao hơn (Fehlberg, 1969). Trong các thí nghiệm của chúng tôi, chúng tôi quan sát thấy rằng phương pháp trung điểm đủ chính xác và quyết định duy trì phương pháp trung điểm vì tính đơn giản.

3.4 CÂN BẰNG HUẤN LUYỆN ROUTER VÀ HUẤN LUYỆN CHUYÊN GIA

So với b∇SparseMixer-1st, b∇SparseMixer-2rd cung cấp ước lượng gradient tốt hơn cho huấn luyện router. Tuy nhiên, b∇SparseMixer-2rd gây ra khó khăn bổ sung cho huấn luyện chuyên gia. Cụ thể, b∇SparseMixer-2rd yêu cầu thay đổi đầu ra MoE từ y←πDfD(x) thành y←πDfD(x) 2, dẫn đến khoảng cách giữa huấn luyện (y←πDfD(x) 2) và suy luận (y←πDfD(x)). Như thảo luận trong Phần 4.3, khoảng cách như vậy tạo ra những trở ngại đáng kể cho huấn luyện MoE.

Trong khi đó, D được gán là D←arg maxIiπIi trong quá trình suy luận, thay vì được lấy mẫu từ π. Do đó, sẽ đủ để đóng khoảng cách bằng cách chỉ áp dụng b∇SparseMixer-2rd khi D̸=arg maxIiπIi. Tương ứng, chúng tôi đề xuất SparseMixer để cân bằng huấn luyện router và huấn luyện chuyên gia:

b∇SparseMixer:=(1−δD)b∇SparseMixer-2rd+δDb∇SparseMixer-1st, trong đó δD=(1,nếu D=arg max IiπIi 0,nếu khác.

Hiệu quả Tính toán của SparseMixer. b∇SparseMixer không yêu cầu Hessian hoặc các đạo hàm bậc hai khác, do đó có chi phí tính toán không đáng kể (các xác minh thực nghiệm được thảo luận trong Phần 4.4). Đồng thời, tương tự như b∇ST, thuật toán được đề xuất của chúng tôi có thể dễ dàng tích hợp với thư viện phổ biến như PyTorch, khiến nó dễ tích hợp với các thuật toán hiện có.

3.5 TỪ MOE ĐƠN GIẢN HÓA ĐẾN SWITCH TRANSFORMER

Như đã đề cập trong Phần 2, mô hình MoE của chúng tôi là một Switch Transformer đơn giản hóa. Ở đây, chúng tôi đầu tiên thảo luận về sự khác biệt giữa thiết lập đơn giản hóa của chúng tôi và Switch Transformer, rồi chuyển sang các sửa đổi cần thiết để áp dụng SparseMixer cho Switch Transformer.

Thảo luận về Sự khác biệt Thiết lập. Sự khác biệt giữa thiết lập đơn giản hóa của chúng tôi và Switch Transformer là việc lấy mẫu D. Cụ thể, trong thiết lập đơn giản hóa của chúng tôi, chúng tôi giả định D được lấy mẫu từ π; trong Switch Transformer, D được lấy mẫu là:

D=arg max Ii(θIi·uIi), trong đó uIi iid∼Uniform(1−r,1+r).(10)

Như thảo luận trong Fedus et al. (2021), việc lấy mẫu D trực tiếp từ π dẫn đến suy giảm hiệu suất đáng kể (cũng được thảo luận trong Phần 4.2). Trong khi đó, trong Switch Transformer, phân phối của D không có dạng phân tích và do đó không có gradient phân tích, khiến SparseMixer không trực tiếp

--- TRANG 6 ---
[Hình 2: Đường cong huấn luyện của Switch Transformer trên huấn luyện ELECTRA-base.]

áp dụng được. Trong các thí nghiệm của chúng tôi, chúng tôi triển khai một quy trình lấy mẫu có thể vi phân như lấy mẫu từ π, trong khi chia sẻ một số tính chất quan trọng với Switch Transformer.

Tính chất Lấy mẫu của Switch Transformer. Ở đây, chúng tôi đánh dấu θ∗:=maxIiθIi. Khi đó, trong Switch Transformer, Ii sẽ không bao giờ được lấy mẫu nếu θ∗−θIi>r·(|θ∗|+|θIi|). Nói cách khác, phân phối của D trong Switch Transformer được che mặt nạ: xác suất nhỏ sẽ giảm trực tiếp xuống không một khi logit tương ứng chạm ngưỡng. Trong các thí nghiệm của chúng tôi, chúng tôi quan sát thấy rằng phân phối thưa thớt như vậy đóng vai trò quan trọng trong thành công của MoE (như được trình bày chi tiết trong Phần 4.2).

Áp dụng SparseMixer cho Switch Transformer. Tương ứng, chúng tôi thay đổi tính toán π từ πi=softmax(θ)i=exp(θi)Pn j=1exp(θj), thành πi=exp(θi)·∆i Pn j=1exp(θj)·∆j, trong đó ∆j=δ(θ∗−θIi≤r·(|θ∗|+|θIi|)). Nói cách khác, chúng tôi áp dụng một mặt nạ cho hàm softmax, để chỉ lấy mẫu từ các chuyên gia không bị che mặt nạ bởi Switch Transformer.

Ngoài ra, vì giá trị của π sẽ khác sau khi áp dụng mặt nạ (điều này ảnh hưởng đến độ lớn gradient của các thành phần khác), chúng tôi tiếp tục thay đổi đầu ra của lớp MoE từ πD·fD(x) thành ω·πD·fD(x), trong đó ω có thể huấn luyện và được khởi tạo là vector 1. Trực quan, ω có thể được xem như một sự thích ứng về tốc độ học để huấn luyện mạng chuyên gia. Lưu ý rằng, ω có thể được tham số hóa lại vào lớp feedforward sau khi huấn luyện.

4 THÍ NGHIỆM

Ở đây, chúng tôi tiến hành thí nghiệm trên cả nhiệm vụ pre-training và dịch máy neural. Chúng tôi tuân thủ chặt chẽ thiết lập thí nghiệm của nghiên cứu hiện có. Do hạn chế về tài nguyên tính toán, chúng tôi để các siêu tham số liên quan đến MoE không được điều chỉnh trong tất cả các thiết lập, tức là, jitter (tức là r trong Phương trình 10) được đặt thành 0.1 và tỷ lệ cho mất mát cân bằng tải được đặt thành 0.01 (Fedus et al., 2021). Các cấu hình thí nghiệm chi tiết được trình bày trong Phụ lục B.

4.1 ÁP DỤNG SPARSE MIXER TRÊN SWITCH TRANSFORMER

NMT trên WMT'14 En-De. Chúng tôi trực quan hóa đường cong huấn luyện trong Hình 1 và tóm tắt điểm BLEU trong Bảng 1. Về cả tốc độ hội tụ và hiệu suất cuối cùng, Switch+SparseMixer vượt trội liên tục so với Switch trong tất cả năm thiết lập. Đáng chú ý, Switch+SparseMixer phù hợp với hiệu suất huấn luyện của Switch với khoảng 50% ít cập nhật huấn luyện hơn khi N∈{4,6,8} và khoảng 40% ít cập nhật huấn luyện hơn khi N∈{2,16}.

Bảng 1: Điểm BLEU trên WMT'14 En-De (N đề cập đến số lượng chuyên gia).

[Bảng hiển thị kết quả so sánh giữa Dense Transformer-base và các mô hình Mixture-of-Expert với số lượng chuyên gia khác nhau]

--- TRANG 7 ---
Bảng 2: Kết quả trên tập phát triển GLUE. S đề cập đến Switch và S+S đề cập đến Switch+SparseMixer. AVG là điểm trung bình qua tám nhiệm vụ.

[Bảng hiển thị kết quả chi tiết trên các nhiệm vụ GLUE]

[Hình 3: So sánh giữa Switch Transformer và Switch Transformer không có Scaling.]

Chúng ta có thể quan sát thấy rằng, với nhiều chuyên gia hơn, các mô hình MoE đạt được mất mát huấn luyện thấp hơn với điểm BLEU tệ hơn. Cụ thể, mặc dù Switch Transformer đạt được hiệu suất huấn luyện tốt hơn, hiệu suất cuối cùng của nó (điểm BLEU) không bao giờ vượt trội so với mô hình Dense, bất kể nó có bao nhiêu chuyên gia. Chúng tôi tin rằng nó cần nhiều dữ liệu hơn để phát huy đầy đủ tiềm năng của MoE và đề xuất hiện tượng này cho thấy rằng các mô hình MoE dễ bị overfitting (Zuo et al., 2022).

Trong khi đó, không thay đổi siêu tham số hoặc kiến trúc mô hình, hiệu suất downstream của Switch + SparseMixer vượt trội so với cả Dense và Switch, khi N∈{2,4}. Cụ thể, SparseMixer cải thiện hiệu suất của Switch từ 28.17 lên 28.72 (khi N=2) và từ 28.05 lên 28.61 (khi N=4). Hiện tượng này ngụ ý rằng, với sự giúp đỡ của SparseMixer, một bộ ước lượng gradient tốt, MoE học được một định tuyến chuyên gia tổng quát hóa tốt hơn.

Pre-training. Theo các công trình trước đây (Dong et al., 2023), chúng tôi trực quan hóa đường cong huấn luyện trong Hình 2 và tóm tắt kết quả fine-tuning trong Bảng 2. Về cả tốc độ hội tụ và hiệu suất downstream, Switch+SparseMixer vượt trội liên tục so với Switch trong tất cả các thiết lập. Ngoài ra, tương tự như các thí nghiệm về dịch máy, chúng tôi quan sát thấy rằng các mô hình MoE dễ overfit hơn và cả hai thiết lập đều đạt hiệu suất downstream tốt nhất với hai chuyên gia.

Ngoài ra, đáng chú ý rằng, trong khi Switch Transformer chỉ vượt trội so với mô hình dense khi số lượng chuyên gia được đặt thành 2, Switch + SparseMixer vượt trội liên tục so với mô hình Dense trong tất cả bốn thiết lập. Hiện tượng này tiếp tục xác minh trực giác của chúng tôi rằng SparseMixer hỗ trợ các mô hình MoE với huấn luyện router chuyên gia tốt hơn, do đó có mô hình kết quả tổng quát hóa tốt hơn.

4.2 THẢO LUẬN

Ở đây, chúng tôi tiến hành thí nghiệm để thảo luận về mô hình hóa lớp MoE của chúng tôi như trong Phần 2.

Tầm quan trọng của việc Scaling Đầu ra Chuyên gia với Mạng Gating. Một chi tiết thiết kế quan trọng của MoE là scale đầu ra của mạng chuyên gia với mạng gating. Cụ thể, đầu ra của lớp MoE được tính là y←πDfD(x), thay vì y←fD(x). Thiết kế scaling này rất hỗ trợ việc suy dẫn SparseMixer trong Phần 3, và truyền cảm hứng cho việc giới thiệu ω (thảo luận thêm trong Phần 4.3). Ở đây, chúng tôi chứng minh thực nghiệm rằng thiết kế scaling này cũng đóng vai trò quan trọng trong Switch Transformer.

Cụ thể, chúng tôi tiến hành thí nghiệm với một biến thể của Switch Transformer, tức là, Switch w.o. Scaling, đặt đầu ra của lớp MoE là y←fD(x). Chúng tôi áp dụng biến thể Switch này trên WMT'14 En-De và trực quan hóa đường cong huấn luyện trong Hình 3. Switch (y←πDfD(x)) vượt trội đáng kể so với biến thể này (y←fD(x)). Ngoài ra, chúng ta có thể quan sát thấy rằng, khi số lượng chuyên gia

--- TRANG 8 ---
[Hình 4: So sánh giữa SparseMixer và SparseMixer không áp dụng mặt nạ cho việc lấy mẫu.]

[Hình 5: So sánh giữa SparseMixer và SparseMixer-2rd.]

được đặt thành 6, sử dụng biến thể này sẽ dẫn đến bất ổn huấn luyện bổ sung, điều này tiếp tục chứng minh tầm quan trọng của thiết kế scaling.

Tầm quan trọng của việc Áp dụng Mặt nạ cho Softmax. Trong Phần 3.5, chúng tôi xác định rằng việc lấy mẫu trong Switch Transformer đóng vai trò quan trọng trong thành công của Switch Transformer. Như thảo luận trong Fedus et al. (2021), việc sử dụng trực tiếp lấy mẫu softmax sẽ dẫn đến hiệu suất kém hơn.

Ở đây, chúng tôi chứng minh rằng việc lấy mẫu softmax có mặt nạ này cũng đóng vai trò quan trọng trong Switch + SparseMixer. Cụ thể, chúng tôi tiến hành thí nghiệm với một biến thể của SparseMixer, tức là, SparseMixer w.o. Mask, tính toán πi←softmax(θ)i. Chúng tôi áp dụng SparseMixer w.o. Mask trên WMT'14 En-De và trực quan hóa đường cong huấn luyện trong Hình 4. SparseMixer (πi←exp(θi)·∆i Pn j=1exp(θj)·∆j) vượt trội đáng kể so với biến thể này (y←softmax(θ)i). Ngoài ra, chúng ta có thể quan sát thấy rằng, khi số lượng chuyên gia được đặt thành 6, sử dụng biến thể này sẽ dẫn đến bất ổn huấn luyện bổ sung, điều này tiếp tục chứng minh tầm quan trọng của việc áp dụng mặt nạ cho softmax.

4.3 ABLATION

Ở đây, chúng tôi tiến hành thí nghiệm để thảo luận về các chi tiết thiết kế của SparseMixer.

Tầm quan trọng của việc Cân bằng Học Chuyên gia và Học Định tuyến. Trong khi SparseMixer-2rd cung cấp xấp xỉ gradient tốt hơn cho định tuyến chuyên gia, nó tạo ra khoảng cách giữa huấn luyện và suy luận. Để chứng minh tầm quan trọng của việc cân bằng huấn luyện router và huấn luyện chuyên gia, chúng tôi tiến hành thí nghiệm áp dụng SparseMixer-2rd trên WMT'14 En-De. Như được trực quan hóa trong Hình 5, SparseMixer vượt trội liên tục so với SparseMixer-2rd trong tất cả các trường hợp. Ngoài ra, SparseMixer-2rd thể hiện bất ổn huấn luyện khi đặt số lượng chuyên gia thành 2.

Phương pháp Trung điểm và Scaling ω. Để hiểu rõ hơn về lợi ích của việc giới thiệu ω (như trong Phần 3.5) và so sánh với SparseMixer-1st, chúng tôi tiến hành nghiên cứu ablation bổ sung trên WMT'14 En-De. Cụ thể, chúng tôi xem xét hai biến thể SparseMixer:

• ablation-1 loại bỏ ω khỏi SparseMixer (tức là, thay đổi đầu ra của Switch + SparseMixer từ ω·πD·fD(x) thành πD·fD(x)).
• ablation-2 tiếp tục thay thế phương pháp trung điểm bằng phương pháp Euler tiến trong SparseMixer-ablation-1, tức là, b∇SparseMixer-1st được sử dụng làm bộ ước lượng gradient và ω được loại bỏ.

Chúng tôi áp dụng hai biến thể này cho WMT'14 En-De và trực quan hóa đường cong huấn luyện của chúng trong Hình 1. Kết quả tiếp tục xác minh trực giác của chúng tôi rằng ω hỗ trợ huấn luyện MoE bằng cách giảm thiểu tác động của việc áp dụng mặt nạ. Ngoài ra, nó cho thấy rằng việc tích hợp phương pháp trung điểm giúp xấp xỉ gradient định tuyến chuyên gia tốt hơn.

--- TRANG 9 ---
Bảng 3: Chi phí Thời gian Huấn luyện Trung bình (s/cập nhật). N đề cập đến số lượng chuyên gia.

[Bảng hiển thị thời gian huấn luyện cho WMT'14 En-De và Pre-training]

4.4 HIỆU QUẢ

Chúng tôi tóm tắt chi phí thời gian trung bình mỗi cập nhật trong Bảng 3. Switch+SparseMixer đạt được chi phí thời gian trung bình giống hệt với Switch trong tất cả tám thiết lập. Điều này cho thấy rằng chi phí tính toán của SparseMixer là không đáng kể.

5 KẾT LUẬN

Trong nghiên cứu này, chúng tôi trình bày SparseMixer để vượt ra ngoài rời rạc và thu hẹp khoảng cách giữa định tuyến MoE thưa thớt và backpropagation. Bắt nguồn từ khung ODE số học, SparseMixer khai thác phương pháp trung điểm, một bộ giải ODE bậc hai, để cung cấp xấp xỉ gradient chính xác với chi phí tính toán không đáng kể. Trong các thí nghiệm của chúng tôi về cả nhiệm vụ dịch máy neural và pre-training, SparseMixer không chỉ tăng tốc hội tụ huấn luyện lên đến hai lần mà còn hỗ trợ MoE với định tuyến chuyên gia được huấn luyện đúng cách. Đáng chú ý, trong khi Switch Transformer kém hiệu quả hơn mô hình dense trong cả ba thiết lập pre-training, việc tích hợp SparseMixer như một bộ ước lượng gradient cho phép các mô hình MoE kết quả vượt trội liên tục so với mô hình dense.

Có nhiều hướng thú vị để khám phá trong tương lai. Trong khi phương pháp của chúng tôi dựa trên các bộ giải ODE bậc một và bậc hai, sẽ thú vị khi áp dụng các bộ giải ODE bậc cao hơn và thậm chí các bộ giải ODE thích ứng như RKF4 (Fehlberg, 1969). Ngoài ra, vì nghiên cứu của chúng tôi mở đường cho việc thiết kế xấp xỉ gradient cho huấn luyện MOE có thể mở rộng, chúng tôi có kế hoạch tiếp tục cải thiện thiết kế kiến trúc của các mô hình MoE. Cuối cùng, vì chúng tôi quan sát thấy rằng các mô hình MoE dễ overfit hơn, chúng tôi có kế hoạch nghiên cứu định luật scaling của các mô hình thưa thớt và hỗ trợ pre-training quy mô lớn.

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được dịch sang tiếng Việt]

A PHƯƠNG PHÁP EULER TIẾN VÀ PHƯƠNG PHÁP TRUNG ĐIỂM

[Phần phụ lục A được dịch sang tiếng Việt]

B THIẾT LẬP THÍ NGHIỆM

[Phần phụ lục B được dịch sang tiếng Việt]
