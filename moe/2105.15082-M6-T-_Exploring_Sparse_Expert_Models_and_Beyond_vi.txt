# 2105.15082.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2105.15082.pdf
# Kích thước tệp: 2322235 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
M6-T: Khám phá Các Mô hình Chuyên gia Thưa thớt và Hơn thế nữa
An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia
Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu
Jingren Zhou, Hongxia Yangy
Alibaba Group
{ya235025,junyang.ljy,menrui.mr,ericzhou.zc,jiangle.jl,
xianyan.xianyanjia,wangang.wa,wanglin.zj,jiamang.wang,jiufeng.ly,
di.zhangd,weilin.lw,jingren.zhou,yang.yhx}@alibaba-inc.com
xide.ql@taobao.com
Tóm tắt
Các mô hình Mixture-of-Experts (MoE) có thể đạt được kết quả hứa hẹn với lượng tham số cực lớn nhưng chi phí tính toán không đổi, và do đó nó đã trở thành xu hướng trong việc mở rộng mô hình. Tuy nhiên, vẫn là một bí ẩn về cách các lớp MoE mang lại cải thiện chất lượng bằng cách tận dụng các tham số với kích hoạt thưa thớt. Trong công trình này, chúng tôi nghiên cứu một số yếu tố chính trong các mô hình chuyên gia thưa thớt. Chúng tôi quan sát thấy rằng mất cân bằng tải có thể không phải là một vấn đề đáng kể ảnh hưởng đến chất lượng mô hình, trái với quan điểm của các nghiên cứu gần đây [11;6;12], trong khi số lượng chuyên gia được kích hoạt thưa thớt k và dung lượng chuyên gia C trong định tuyến top-k có thể tạo ra sự khác biệt đáng kể trong bối cảnh này. Hơn nữa, chúng tôi tiến thêm một bước để đề xuất một phương pháp đơn giản gọi là nguyên mẫu chuyên gia (expert prototyping) chia các chuyên gia thành các nguyên mẫu khác nhau và áp dụng định tuyến k top-1. Chiến lược này cải thiện chất lượng mô hình nhưng duy trì chi phí tính toán không đổi, và khám phá thêm của chúng tôi về các mô hình quy mô cực lớn phản ánh rằng nó hiệu quả hơn trong việc huấn luyện các mô hình lớn hơn. Chúng tôi đẩy quy mô mô hình lên hơn 1 nghìn tỷ tham số và triển khai nó chỉ trên 480 GPU NVIDIA V100-32GB, so sánh với các SOTA gần đây [11;6] trên 2048 lõi TPU. Mô hình khổng lồ được đề xuất đạt được tăng tốc đáng kể trong hội tụ so với baseline cùng kích thước.

1 Giới thiệu
Tiền huấn luyện quy mô lớn đã chứng minh thành công to lớn trên nhiều lĩnh vực, đặc biệt là xử lý ngôn ngữ tự nhiên [4;21;32;23;2]. Các nghiên cứu gần đây đã cho thấy rằng việc mở rộng kích thước mô hình có thể mang lại cải thiện chất lượng đáng kể trong hiệu suất nhiệm vụ hạ nguồn [32;23;2], và chất lượng mô hình tỷ lệ theo luật lũy thừa với kích thước dữ liệu, quy mô mô hình và lượng tính toán [10]. Điều này có thể được mở rộng sang lĩnh vực học biểu diễn đa phương thức [18;3], nơi các mô hình với số lượng tham số cực lớn [27;15] có thể đạt được hiệu suất xuất sắc trong hiểu và tạo sinh đa phương thức. Tuy nhiên, huấn luyện các mô hình dày đặc tốn kém về mặt tính toán và khó huấn luyện chúng trên dữ liệu quy mô siêu lớn với tài nguyên tính toán hạn chế.

Lấy cảm hứng từ thành công của Mixture-of-Experts (MoE) [30; 26], một số nghiên cứu gần đây đã tập trung vào việc huấn luyện các mô hình chuyên gia thưa thớt quy mô lớn với hiệu quả huấn luyện cao [11;6;15;8]. Một lớp MoE bao gồm nhiều chuyên gia và do đó có dung lượng mô hình lớn. Mỗi tính toán tiến truyền định tuyến một token đến k chuyên gia từ N, trong đó k≪N. Cơ chế định tuyến như vậy cho phép kết hợp song song dữ liệu và song song chuyên gia. Các nghiên cứu trước đây [11;6] cho thấy rằng nó có thể đạt được tăng tốc hiệu suất rõ ràng với cùng tài nguyên tính toán. Tuy nhiên, huấn luyện các mô hình MoE quy mô lớn như vậy có thể cực kỳ khó khăn do nhiều yếu tố, ví dụ: thách thức hệ thống về giao tiếp và mất cân bằng tải, và thách thức thuật toán về bất ổn định huấn luyện, v.v.

Bản thảo. Đang xem xét.arXiv:2105.15082v5  [cs.LG]  9 Aug 2021

--- TRANG 2 ---
Trong công trình này, chúng tôi tiến hành phân tích các mô hình MoE gần đây để tìm ra yếu tố nào ảnh hưởng đến chất lượng mô hình và hiệu quả huấn luyện. Chúng tôi nghiên cứu một số yếu tố liên quan đến các mô hình chuyên gia thưa thớt, bao gồm cân bằng tải, chiến lược định tuyến top-k, attention MoE, v.v. Phân tích của chúng tôi chứng minh rằng mất cân bằng tải không phải là một vấn đề đáng kể ảnh hưởng đến chất lượng mô hình, trong khi số lượng chuyên gia được kích hoạt thưa thớt k và dung lượng chuyên gia C trong định tuyến top-k tạo ra sự khác biệt lớn trong bối cảnh này, và các giá trị k lớn hơn thường đóng góp vào hiệu suất mô hình tốt hơn. Tuy nhiên, với sự gia tăng giá trị k, việc huấn luyện mô hình với triển khai định tuyến top-k thông thường [11] trở nên kém hiệu quả hơn nhiều.

Để khám phá thêm, chúng tôi mở rộng thí nghiệm sang các mô hình quy mô lớn với hơn 10 và 100 tỷ tham số tương ứng. Các phát hiện và đề xuất của chúng tôi có thể áp dụng cho các mô hình quy mô cực lớn, và kết quả cho thấy rằng nguyên mẫu chuyên gia có lợi thế đáng kể hơn trong việc huấn luyện các mô hình lớn hơn. Để tiến xa hơn nữa, chúng tôi đẩy quy mô mô hình lên hơn 1 nghìn tỷ tham số và thành công triển khai nó chỉ trên 480 GPU NVIDIA V100-32GB, so sánh với các SOTA gần đây trên 2048 lõi TPU [11;6]. Chúng tôi cho thấy rằng mô hình 1 nghìn tỷ tham số vượt trội hơn baseline có quy mô mô hình tương tự, và đạt được khoảng 5 lần tăng tốc trong hội tụ huấn luyện.

Tóm lại, đóng góp của chúng tôi như sau:
• Chúng tôi khám phá các yếu tố chính bên trong các mô hình MoE, và tìm ra rằng mất cân bằng tải có thể không phải là một vấn đề đáng kể ảnh hưởng đến chất lượng mô hình trong bối cảnh này, trong khi số lượng chuyên gia được kích hoạt thưa thớt k và dung lượng chuyên gia C trong định tuyến top-k có tác động đáng kể đến hiệu suất mô hình.

• Chúng tôi đề xuất một phương pháp đơn giản gọi là nguyên mẫu chuyên gia chia các chuyên gia thành k nguyên mẫu và áp dụng kích hoạt k top-1 với hiệu quả tương tự định tuyến top-1. Chiến lược này cải thiện chất lượng mô hình nhưng duy trì chi phí tính toán không đổi, và nó thậm chí còn hiệu quả hơn trong việc huấn luyện các mô hình quy mô lớn hơn.

• Chúng tôi tiến mô hình đến 1 nghìn tỷ tham số và thành công triển khai nó chỉ trên 480 GPU NVIDIA V100-32GB, so sánh với các SOTA gần đây trên 2048 lõi TPU. Mô hình nguyên mẫu chuyên gia 1 nghìn tỷ tham số vượt trội hơn baseline cùng quy mô và đạt được tăng tốc đáng kể trong hội tụ.

2 Các Mô hình Chuyên gia Thưa thớt
Các mô hình chuyên gia thưa thớt được coi là một phương pháp hứa hẹn cho việc mở rộng mô hình với hiệu quả huấn luyện cao. Huấn luyện các mô hình dày đặc đòi hỏi chi phí tính toán cực cao, trong khi các mô hình chuyên gia thưa thớt có thể hội tụ nhanh hơn đáng kể vì nó lặp lại trên nhiều dữ liệu hơn với hiệu quả cao hơn nhiều trên cơ sở thời gian. Chế độ huấn luyện mô hình quy mô lớn như vậy cũng thân thiện với môi trường hơn. Trong thiết lập này, một lượng lớn trọng số mô hình được phân phối cho các worker khác nhau do kiến trúc của các lớp MoE, và MoE cho phép tăng tham số trong khi giữ chi phí tính toán không đổi [11; 6].

Mixture-of-Experts về bản chất là một thuật toán định tuyến định tuyến các token đến k chuyên gia được chỉ định từ N (trong đó k≪N) để tính toán tiến truyền. Nó cho phép song song chuyên gia để các chuyên gia xử lý các token đầu vào được chỉ định bởi các hàm cổng đồng thời. Các mạng chuyên gia, mỗi mạng là một perceptron đa lớp, được phân phối trên các worker. Chúng tôi sử dụng một hàm cổng chỉ định k từ N chuyên gia cho một biểu diễn token đầu vào x. Các chuyên gia được chọn xử lý biểu diễn với tính toán tiến truyền và giảm kết quả của chúng bằng tổng có trọng số dựa trên các giá trị cổng:

~x=∑(i=1 đến k) piEi(x); p = softmax(g); g = topk(softmax(Wgx)); (1)

trong đó Ei đề cập đến chuyên gia thứ i. Các thuật toán MoE điển hình nhất cho các mô hình chuyên gia thưa thớt quy mô lớn là Switch Transformer và GShard [6;11]. Sự khác biệt chính của chúng nằm ở lựa chọn top-k, áp dụng lựa chọn top-1 và top-2 tương ứng. Switch Transformer [6] lưu ý rằng định tuyến một token chỉ đến 1 chuyên gia là hiệu quả trong việc bảo tồn chất lượng mô hình và giảm độ phức tạp tính toán, trái với ý tưởng của Shazeer et al. [30] rằng k nên lớn hơn 1 để có gradient không tầm thường cho các hàm định tuyến.

Điều tạo ra sự khác biệt trong hiệu suất và chất lượng mô hình là triển khai thực tế của các chuyên gia phân tán. Song song mô hình cho phép phân vùng một tensor lớn trên các worker, và

--- TRANG 3 ---
Hình 1: Đường cong phát triển của các hệ số biến thiên cv tại các lớp khác nhau. Ở đây chúng tôi minh họa sự phát triển của cv của baseline và auxiliary loss tại tất cả các lớp. Chúng tôi cũng minh họa đường cong log perplexity huấn luyện của chúng (xem đường đứt nét màu đen). Auxiliary loss giúp mô hình đạt được tải tính toán cân bằng cao tại mọi lớp, nhưng sự cân bằng cao hơn đã không được chuyển thành chất lượng mô hình cao hơn. Ngược lại, hành vi cân bằng tải trong mô hình MoE vanilla là kỳ lạ. Mặc dù cv tại tất cả các lớp giảm ở đầu, nhưng một số trong số chúng thậm chí tăng lên giá trị cao sau đó.

triển khai của chúng tôi thậm chí cho phép nhiều chuyên gia trên một worker giống hệt nhau, thay vì một chuyên gia mỗi worker. Do tính chất động của định tuyến top-k, nó có thể gây ra hiệu quả thấp nếu mất cân bằng tải nghiêm trọng xảy ra. Một triển khai tiêu chuẩn để giải quyết vấn đề là thiết lập dung lượng chuyên gia [11;6;31], được định nghĩa là:

C = α·k·T/N; (2)

trong đó T đề cập đến số lượng token trong một batch và α đề cập đến hệ số dung lượng thường lớn hơn 1.0. Hệ số dung lượng lớn hơn có thể cung cấp thêm buffer cho dung lượng chuyên gia. Các token được phân phối cho các chuyên gia với các thao tác điều phối all-to-all. Các biểu diễn token được xử lý bởi các chuyên gia được chọn được kết hợp với giao tiếp all-to-all về các worker gốc của chúng. Trong điều kiện vượt quá dung lượng, các biểu diễn token bỏ qua tính toán tiến truyền bằng kết nối dư. Đối với các chuyên gia vẫn còn chỗ trong dung lượng của họ sau khi điều phối, chúng tôi thêm các token đệm để lấp đầy. Do đó, chi phí tính toán và giao tiếp có mối quan hệ tích cực với số lượng chuyên gia N và dung lượng chuyên gia C. Dung lượng chuyên gia thấp có thể gây ra một lượng đáng kể token bị loại bỏ nếu có mất cân bằng tải trên các chuyên gia, nhưng tăng dung lượng chuyên gia sẽ tương ứng tăng chi phí tính toán và giao tiếp.

3 Khám phá các Mô hình MoE
Để nghiên cứu toàn diện các mô hình MoE, chúng tôi tiến hành một loạt thí nghiệm về tiền huấn luyện đa phương thức theo thực hành của Lin et al. [15], và chúng tôi đánh giá các thiết lập khác nhau, bao gồm các phương pháp định tuyến, dung lượng, attention MoE, v.v. Cụ thể, chúng tôi tiền huấn luyện mô hình MoE trên M6-Corpus, và đánh giá hiệu suất mô hình trên image captioning zero-shot trên dataset E-commerce IC [15]. Chúng tôi cung cấp thêm chi tiết thí nghiệm trong phụ lục.

3.1 Phát triển Cân bằng Tải
Các nghiên cứu gần đây chỉ ra tầm quan trọng của định tuyến cân bằng [6;11;12], và minh họa tầm quan trọng của các phương pháp cân bằng như auxiliary expert balancing loss. Chúng tôi đầu tiên tiến hành thí nghiệm trên các mô hình MoE có và không có auxiliary differentiable load balancing loss tương ứng. Chúng tôi tiền huấn luyện cả hai mô hình trong 500k bước và so sánh hiệu suất upstream và downstream của chúng.

Kết quả thí nghiệm cho thấy rằng mô hình có auxiliary loss thậm chí hoạt động tệ hơn trong cả đánh giá upstream của training log perplexity (2.694 vs. 2.645) và đánh giá perplexity của nhiệm vụ downstream

--- TRANG 4 ---
	
	


	
	


a. Định tuyến Top-k b. Định tuyến k Top-1 cho Nguyên mẫu Chuyên gia
Hình 2: Minh họa định tuyến top-2 và định tuyến 2 top-1 cho nguyên mẫu chuyên gia. Trong định tuyến top-2, router chọn top-2 từ tất cả các chuyên gia và gửi biểu diễn token đến những chuyên gia đó. Trong định tuyến 2 top-1 cho nguyên mẫu chuyên gia, các chuyên gia đầu tiên được nhóm thành 2 nguyên mẫu, và có một router cho mỗi nguyên mẫu. Tương tự, mỗi router chọn chuyên gia top-1 và gửi biểu diễn token qua nó. Đầu ra từ mỗi nguyên mẫu được tổng theo từng phần tử cho đầu ra cuối cùng.

của image captioning (9.97 vs. 9.72). Những quan sát này khích lệ chúng tôi nghiên cứu thêm về mối quan hệ giữa cân bằng tải và chất lượng mô hình.

Chúng tôi đánh giá mức độ cân bằng tải tính toán của các chuyên gia tại mọi lớp. Theo Shazeer et al. [30], chúng tôi cũng sử dụng hệ số biến thiên để đánh giá. Chúng tôi định nghĩa hệ số biến thiên cho tải tính toán hiệu quả là của số lượng token thực được tính toán bởi các chuyên gia cv = σ(T)/μ(T), trong đó T đề cập đến các token được tính toán bởi các chuyên gia. Lưu ý rằng trong triển khai, các chuyên gia vẫn có buffer nên lấp đầy các token đệm trong dung lượng của họ. Chúng tôi không bao gồm phần tính toán này trong đánh giá cân bằng tải tính toán. Metric này phản ánh mức độ đồng nhất của việc gán token. Hơn nữa, chúng tôi tập trung vào sự phát triển của cv trong quá trình huấn luyện để đánh giá sự thay đổi của việc gán tải.

Chúng tôi minh họa kết quả trong Hình 1. Đối với tất cả các lớp, mất cân bằng tải đáng kể tồn tại ở giai đoạn ban đầu theo các giá trị cv cao. Đáng chú ý, giá trị thường cao hơn ở các lớp trên cùng. Đối với mô hình được huấn luyện với auxiliary expert load balancing loss, cv tại tất cả các lớp giảm mạnh ở giai đoạn ban đầu xuống giá trị thấp khoảng 0.3 biểu thị tải tính toán cân bằng cao,¹ và chúng trở nên ổn định sau đó. Tuy nhiên, tải tính toán khá khác biệt đối với mô hình MoE không có auxiliary loss. Mặc dù cv tại tất cả các lớp giảm ở đầu, nhưng chúng không đạt được giá trị thấp biểu thị tải tính toán cân bằng cao. Ngoại trừ điều đó, một số thậm chí tăng lên giá trị cao sau đó. Những hiện tượng này phản ánh sự tồn tại của mất cân bằng tải. Mặc dù auxiliary loss có lợi thế trong cân bằng tải chuyên gia, lợi thế như vậy đã không được chuyển thành những lợi thế trong hiệu suất upstream và downstream, như đã đề cập ở trên.

3.2 Tác động của Kích hoạt Thưa thớt Top-k
Công trình trước đây [6] chỉ ra rằng top-1 là đủ cho chất lượng mô hình cao so với lựa chọn top-k với k>1, trong khi nó có lợi thế đáng kể về hiệu quả tính toán, nhưng

¹ Chúng tôi quan sát thủ công số lượng token mà các chuyên gia nhận được và thấy rằng tải tính toán cân bằng cao

--- TRANG 5 ---
Log Perplexity
Bước Huấn luyện
1 Top-1
2 Top-1 & Dung lượng k×4
Top-1 & Dung lượng k×2
Top-1 & Dung lượng 1×4
Top-1 & Dung lượng 1×

Bước Huấn luyện
Top-1
Top-2 & Dung lượng k×
Top-4 & Dung lượng k×
Top-2 & Dung lượng 1×
Top-4 & Dung lượng 1×

Hình 3: Hiệu suất mô hình với các thiết lập định tuyến top-k khác nhau. Bên trái minh họa hiệu suất của định tuyến top-k với các giá trị k khác nhau. Điều này chứng minh rằng các giá trị k lớn hơn ngay cả với dung lượng nhỏ mang lại nhiều lợi ích hơn, nhưng khoảng cách giữa top-2 và top-4 nhỏ hơn nhiều so với giữa top-1 và top-2. Bên phải minh họa hiệu suất của định tuyến k top-1 cho nhóm chuyên gia. Tương tự, định tuyến k top-1 với k>1 vượt trội đáng kể so với baseline, ngay cả với dung lượng nhỏ.

Bảng 1: FLOPs của các mô hình với các chiến lược định tuyến top-k khác nhau. Chúng tôi báo cáo hiệu quả tính toán (GFLOPs) của định tuyến top-k và nhóm chuyên gia k top-1 với các giá trị k khác nhau. "Dung lượng k" đề cập đến thiết lập tiêu chuẩn của dung lượng chuyên gia dựa trên giá trị k, trong đó C = α·k·T/N, trong khi "Dung lượng 1" đề cập đến thiết lập dung lượng chuyên gia của định tuyến top-1 cho tất cả các mô hình, trong đó C = α·1·T/N. Kết quả cho thấy rằng trong trường hợp dung lượng chuyên gia hạn chế, các mô hình với các chiến lược khác nhau có FLOPs tính toán tương tự.

Top-1  Top-2  Top-4  2 Top-1  4 Top-1
Dung lượng k  2733.53  3614.12  5619.01  3614.00  5618.54
Dung lượng 1  2733.53  2733.60  2733.67  2733.54  2733.57

kết quả thí nghiệm trong Lewis et al. [12] cho thấy rằng top-2 vẫn vượt trội hơn lựa chọn top-1. Ở đây chúng tôi tiến hành thí nghiệm để nghiên cứu thêm cách các phương pháp top-k ảnh hưởng đến chất lượng mô hình.

Giá trị k xác định số lượng chuyên gia để đi qua cho mỗi token. Chúng tôi đánh giá hiệu suất mô hình trong các điều kiện k ∈ {1; 2; 4}. Lưu ý rằng như đã đề cập trong Mục 2, dung lượng chuyên gia C = α·k·T/N nên khác nhau cho các giá trị k khác nhau, và do đó độ phức tạp tính toán của chúng thực sự khác nhau. Để đánh giá hiệu suất của các phương pháp định tuyến khác nhau trong điều kiện dung lượng chuyên gia giống hệt nhau, chúng tôi thí nghiệm các phương pháp định tuyến top-k trong đó k>1 với dung lượng đồng nhất C = α·1·T/N, được ký hiệu là "Dung lượng 1" trong các hình và bảng. Ngược lại, chúng tôi ký hiệu thiết lập ban đầu của dung lượng chuyên gia C = α·k·T/N là "Dung lượng k". Chúng tôi báo cáo FLOPs tính toán của các mô hình với các chiến lược định tuyến top-k khác nhau trong các điều kiện dung lượng chuyên gia tiêu chuẩn và hạn chế.² Bảng 1 chứng minh rằng các giá trị k lớn hơn dẫn đến độ phức tạp tính toán cao hơn, trừ khi dung lượng chuyên gia C bị hạn chế.

Biểu đồ bên trái trong Hình 3 minh họa tác động của lựa chọn k đến hội tụ mô hình. Chúng tôi thấy rằng cả kích hoạt thưa thớt top-2 và top-4 đều có thể vượt trội hơn top-1. Ngay cả khi C bị hạn chế, biểu thị độ phức tạp tính toán tương tự, các phương pháp định tuyến với k>1 vẫn vượt trội đáng kể so với baseline MoE. Chúng tôi giả định rằng các token có thể được xử lý bởi các chuyên gia khác nhau hoạt động khác nhau. Tuy nhiên, thú vị khi thấy lợi nhuận giảm dần rằng khoảng cách giữa top-2 và top-4 nhỏ hơn nhiều so với giữa top-1 và top-2, đặc biệt trong điều kiện dung lượng chuyên gia hạn chế. Ngoài ra, do thao tác "argmax" lặp trong định tuyến top-k, hiệu quả giảm mạnh khi k tăng, như được minh họa trong Bảng 2. Có thể đạt được sự cân bằng hoàn hảo giữa hiệu quả và hiệu suất bằng cách chọn giá trị k thích hợp.

² Chúng tôi báo cáo FLOPs của một GPU duy nhất, được đếm bởi trình profiler Tensorflow.

--- TRANG 6 ---
Bảng 2: Tốc độ của các mô hình với các chiến lược định tuyến top-k khác nhau (ms/bước). Chúng tôi báo cáo tốc độ huấn luyện của các mô hình với các chiến lược định tuyến khác nhau. "Base" đề cập đến mô hình với 1.5 tỷ tham số, và "10B" đề cập đến mô hình quy mô lớn với hơn 10 tỷ tham số. Các mô hình được triển khai với dung lượng chuyên gia hạn chế C = α·1·T/N.

Top-1   Top-2   Top-4   2 Top-1   4 Top-1
Base   214.3   218.2   305.3   220.1   225.3
10B    462.2   493.0   514.2   466.9   473.9

3.3 k Top-1 VS. Top-k
Phân tích trước đây chỉ ra rằng k chuyên gia đóng vai trò khác nhau và vượt trội hơn một chuyên gia duy nhất. Tuy nhiên, thao tác "argmax" lặp trong định tuyến top-k gây ra sự không hiệu quả tính toán. Lấy kích hoạt thưa thớt top-2 làm ví dụ. Nó thực hiện thao tác "argmax" hai lần để chọn các chuyên gia top-2. k lớn hơn có thể gây ra hiệu quả thấp hơn trong tính toán. Chúng tôi đánh giá hiệu quả tính toán của top-1, 2 và 4, như được hiển thị trong Bảng 2. Đối với cả mô hình "Base" với 1.5 tỷ tham số và mô hình "10B" với hơn 10 tỷ tham số, định tuyến top-k trong đó k>1 đặc biệt khi k=4 chậm hơn đáng kể so với top-1.

Để giải quyết vấn đề hiệu quả, chúng tôi đề xuất một phương pháp đơn giản gọi là nguyên mẫu chuyên gia, chia các chuyên gia thành k nguyên mẫu. Trong mỗi tính toán tiến truyền, mỗi token được gửi đến k nguyên mẫu, và trong mỗi nguyên mẫu, nó được xử lý bởi chuyên gia trong mỗi nhóm được chọn bởi định tuyến top-k' trong đó trong hầu hết các trường hợp k'=1 để phân tích. Các đầu ra của nguyên mẫu được kết hợp tuyến tính:

y = Σ(i=1 đến k) Σ(j=1 đến m) pijEij(x); (3)

trong đó m đề cập đến số lượng chuyên gia bên trong một nhóm. Phương pháp này tránh thao tác argmax lặp, nhưng thay vào đó, nó tạo ra k đầu ra theo cách song song. So sánh với top-k, chúng tôi cũng gọi nó là k top-1. Chúng tôi thấy rằng sự gia tăng k không gây ra giảm đáng kể tốc độ huấn luyện, như được hiển thị trong Bảng 2.

Chúng tôi đánh giá chất lượng mô hình của nguyên mẫu chuyên gia k top-1. Biểu đồ bên phải của Hình 3 chứng minh rằng nhóm chuyên gia với k>1 có thể đạt được hiệu suất tốt hơn so với kích hoạt thưa thớt top-1. Tuy nhiên, cải thiện trong bối cảnh dung lượng chuyên gia hạn chế nhỏ hơn nhiều so với trong bối cảnh dung lượng chuyên gia tiêu chuẩn. Chúng tôi thấy rằng vấn đề này có thể được giảm thiểu đáng kể trong việc huấn luyện các mô hình quy mô lớn, và chúng tôi để vấn đề này cho Mục 4.

3.4 Nguyên mẫu Chuyên gia và MoE Attention
Hầu hết các công trình [30;26;11;6] thay thế FFN bằng MoE. Fedus et al. [6] chỉ ra một thiết kế thay thế thay thế các biến đổi tuyến tính trong attention bằng MoE. Lưu ý rằng có 4 biến đổi tuyến tính cho Q, K, V và đầu ra, và chúng có thể được xem như FFN một lớp không có kích hoạt phi tuyến tính. Do đó có thể thay thế chúng bằng MoE, và chúng tôi gọi nó là MoE attention trong bài báo này. Các thí nghiệm sơ bộ trong Fedus et al. [6] chỉ ra rằng attention như vậy có thể mang lại lợi ích cho chất lượng mô hình, nhưng nó gặp phải bất ổn định huấn luyện. Chúng tôi triển khai MoE attention vào mô hình của chúng tôi, và chúng tôi bảo tồn lớp MoE cho FFN.

Đáng ngạc nhiên, theo biểu đồ bên trái trong Hình 4, chúng tôi đạt được một phát hiện khác rằng MoE attention ảnh hưởng tiêu cực đến hiệu suất mô hình, và việc huấn luyện của nó trở nên khó khăn hơn và thậm chí gây ra phân kỳ. Việc kết hợp MoE kích hoạt thưa thớt với tính toán tương quan dựa trên attention có thể làm tăng cao độ khó khăn huấn luyện. Tuy nhiên, nguyên mẫu chuyên gia k top-1 có thể có lợi cho việc huấn luyện MoE attention. Nguyên mẫu chuyên gia với k>1 giúp các mô hình hội tụ, mặc dù tốc độ hội tụ vẫn chậm hơn so với baseline MoE. Hơn nữa, chúng tôi đánh giá tác động của MoE attention trong thiết lập của các mô hình sâu hơn. Cụ thể, chúng tôi tăng số lượng lớp lên 4 lần thành 20, nhưng chúng tôi giảm số lượng chuyên gia xuống 8, để không có sự khác biệt đáng kể về số lượng tham số giữa các mô hình nông và sâu. Chúng tôi minh họa kết quả trong biểu đồ bên phải của Hình 4. Nó cho thấy rằng các mô hình sâu hơn với MoE attention hoạt động tốt hơn và không phân kỳ, nhưng vẫn hoạt động tệ hơn so với baseline MoE. Vẫn trong trường hợp này, nhóm chuyên gia k top-1

--- TRANG 7 ---
Log Perplexity
Bước Huấn luyện
MoE Attention 1 Top-1
MoE Attention 4 Top-1
MoE FFN 1 Top-1

Log Perplexity
Bước Huấn luyện
MoE Attention 1 Top-1
MoE Attention 4 Top-1
MoE FFN 1 Top-1

Hình 4: Hiệu suất mô hình với MoE attention. Biểu đồ bên trái và bên phải đề cập đến hiệu suất của các mô hình nông và sâu tương ứng. Bên trái minh họa hiệu suất của định tuyến k top-1 với các giá trị k khác nhau và thiết kế mô hình cho các mô hình MoE. Khác với các thí nghiệm sơ bộ trong Fedus et al. [6], MoE attention có tác động tiêu cực đến hiệu suất mô hình. Bên phải minh họa hiệu suất của MoE attention trong thiết lập của các mô hình sâu hơn. Tương tự, mô hình MoE attention vẫn hoạt động tệ hơn so với baseline MoE.

Bảng 3: Đánh giá PPL trên E-commerce IC [15]. Theo Radford et al. [21], chúng tôi không thực hiện bất kỳ fine-tuning nào cho bất kỳ kết quả nào trong số này. "Dung lượng k" đề cập đến thiết lập tiêu chuẩn của dung lượng chuyên gia dựa trên giá trị k, trong đó C = α·k·T/N, trong khi "Dung lượng 1" đề cập đến thiết lập dung lượng chuyên gia của định tuyến top-1 cho tất cả các mô hình, trong đó C = α·1·T/N.

Kích thước Mô hình  Dung lượng  Top-1  Top-2  Top-4  2 Top-1  4 Top-1
Base              Dung lượng k   9.72   8.77   8.28   8.73    8.39
Base              Dung lượng 1   9.72   8.84   8.52   9.24    9.41

có lợi ích lớn cho chất lượng mô hình, phù hợp với quan sát đã đề cập ở trên. Điều này cho thấy hiệu quả của nguyên mẫu chuyên gia k top-1.

3.5 Xác thực trên Các Nhiệm vụ Downstream
Chúng tôi tiến hành thí nghiệm về image captioning và chi tiết của các thiết lập thí nghiệm được minh họa trong phụ lục. Chúng tôi minh họa kết quả thí nghiệm của các mô hình MoE với các thiết lập khác nhau cho định tuyến và dung lượng chuyên gia trong Bảng 3. Có thể thấy rằng hiệu suất upstream thường có thể được chuyển sang các nhiệm vụ downstream, cho thấy rằng training log perplexity upstream có thể là một chỉ số cho hiệu suất downstream. Với dung lượng chuyên gia "Dung lượng k", cả định tuyến top-k và nguyên mẫu chuyên gia k top-1 trong đó k>1 đều vượt trội đáng kể so với baseline. Tuy nhiên với dung lượng hạn chế "Dung lượng 1", nguyên mẫu chuyên gia k top-1 có lợi thế nhỏ hơn so với baseline compared với định tuyến top-k. Điều này có thể do vấn đề thoái hóa mạng. Tuy nhiên, vấn đề này được giảm thiểu hiệu quả trong việc huấn luyện mô hình quy mô lớn, như được minh họa trong Mục 4.

4 Tăng vọt lên Nghìn tỷ Tham số
Trong mục này, chúng tôi chứng minh rằng các phát hiện và đề xuất của chúng tôi cũng có thể áp dụng cho các mô hình tiền huấn luyện quy mô lớn, và cuối cùng chúng tôi tiến mô hình lên 1 nghìn tỷ tham số chỉ trên 480 GPU NVIDIA, so sánh với SOTA gần đây trên 2048 lõi TPU [6].

Chúng tôi xác thực các phát hiện của chúng tôi trên các mô hình quy mô cực lớn. Chúng tôi mở rộng kích thước mô hình lên 10 và 100 tỷ tham số tương ứng. Để đơn giản, chúng tôi xác thực nguyên mẫu chuyên gia k top-1 trên các mô hình 10B và 100B tham số. Chúng tôi báo cáo training log perplexity cho hiệu suất upstream của họ. Hình 5 minh họa hiệu suất của họ, và có thể thấy rằng đối với cả mô hình 10B và 100B, nguyên mẫu chuyên gia vẫn có lợi thế so với baseline MoE, và tương tự trong cả hai bối cảnh k lớn hơn cho nguyên mẫu chuyên gia có thể tiếp tục có lợi cho chất lượng mô hình. Ngoài ra, về

--- TRANG 8 ---
Log Perplexity
Bước Huấn luyện
1 Top-1
2 Top-1
4 Top-1

Log Perplexity
Bước Huấn luyện
1 Top-1
2 Top-1
4 Top-1

Hình 5: Hiệu suất của các mô hình 10B và 100B với các chiến lược định tuyến khác nhau. Trong cả hai trường hợp, nguyên mẫu chuyên gia hoạt động tốt hơn so với baseline MoE, và các giá trị k lớn hơn tiếp tục có lợi cho chất lượng mô hình.

Bảng 4: Đánh giá PPL của các mô hình "10B" trên E-commerce IC [15]. Lưu ý rằng các mô hình "10B" với hơn 10 tỷ tham số được huấn luyện với dung lượng chuyên gia hạn chế trong đó C = α·1·T/N.

Kích thước Mô hình  Top-1  Top-2  2 Top-1
10B               6.97   5.73   5.64

hiệu suất downstream, Bảng 3 cho thấy rằng đối với mô hình quy mô lớn "10B", 2 Top-1 có thể đạt hiệu suất tương tự định tuyến Top-2 với dung lượng chuyên gia hạn chế. Điều này cho thấy hiệu quả của nguyên mẫu chuyên gia trong việc huấn luyện các mô hình quy mô lớn hơn.

Dựa trên các phát hiện và đề xuất đã đề cập ở trên, chúng tôi tiến tới xây dựng một mô hình quy mô cực lớn với hơn 1 nghìn tỷ tham số. Do tài nguyên tính toán hạn chế, chúng tôi cố gắng tìm ra giải pháp để triển khai mô hình 1 nghìn tỷ tham số chỉ trên 480 GPU NVIDIA V100-32GB.

Cụ thể hơn, chúng tôi triển khai mô hình của chúng tôi trên một cluster các worker GPU đơn được kết nối bởi mạng RDMA với băng thông 100Gb. Để tiết kiệm việc sử dụng bộ nhớ, chúng tôi thay vào đó chuyển sang Adafactor [29] để tối ưu hóa vì chi phí bộ nhớ dưới tuyến tính của nó. Tuy nhiên, có một loạt các vấn đề lẻ tẻ liên quan đến bất ổn định huấn luyện. Qua thử và sai, chúng tôi thấy rằng việc huấn luyện mô hình như vậy rất nhạy cảm với learning rate, đặc biệt khi được huấn luyện với Adafactor. Chúng tôi không sử dụng mặc định 0.01 do phân kỳ, mà thay vào đó, chúng tôi sử dụng 0.005 để cân bằng giữa tính ổn định huấn luyện và tốc độ hội tụ. Ngoài ra, chúng tôi thấy rằng việc giảm giá trị tuyệt đối của trọng số được khởi tạo là thiết yếu, điều này cũng được minh họa trong Fedus et al. [6]. Chúng tôi cụ thể giảm khởi tạo BERT, một phân phối chuẩn bị cắt với μ=0 và σ=0.02, bằng hệ số 10.

Chúng tôi đầu tiên đánh giá chất lượng của các mô hình với các tham số khác nhau nhưng FLOPs tính toán tương tự bằng cách quan sát training log perplexity. Chúng tôi so sánh hiệu suất của các mô hình baseline MoE với 100 tỷ, 250 tỷ tham số và 1 nghìn tỷ tham số, và chúng tôi quan sát rằng kết quả chứng minh luật mở rộng rằng các mô hình với dung lượng lớn hơn hoạt động tốt hơn, như được minh họa trong Hình 6. Sau đó chúng tôi triển khai cả mô hình baseline MoE 1 nghìn tỷ tham số và mô hình MoE nguyên mẫu chuyên gia của chúng tôi.³ Vẫn từ Hình 6, chúng tôi có thể tìm ra rằng đề xuất có lợi thế mạnh so với mô hình so sánh với 1 nghìn tỷ tham số. Chúng tôi quan sát thấy tăng tốc đáng kể trong hội tụ, trong đó phương pháp của chúng tôi nhanh hơn khoảng 5 lần so với baseline. Tuy nhiên, cả hai mô hình đều có FLOPs tính toán tương tự, điều này chứng minh rằng phương pháp của chúng tôi đạt được sự cân bằng tốt hơn nhiều giữa hiệu quả tính toán và chất lượng mô hình.

³ Do tài nguyên tính toán hạn chế và bất ổn định trong hệ thống và phần cứng, mô hình nguyên mẫu chuyên gia nghìn tỷ tham số chỉ được huấn luyện trong 30k bước.

--- TRANG 9 ---
Log Perplexity
Bước Huấn luyện
100 tỷ
250 tỷ
1 nghìn tỷ
1 nghìn tỷ & nguyên mẫu

Tăng tốc 5x

Hình 6: Hiệu suất của các mô hình baseline với 100 tỷ, 250 tỷ và 1 nghìn tỷ tham số, cũng như mô hình 1 nghìn tỷ tham số với nguyên mẫu chuyên gia. Các đường cong phản ánh luật mở rộng, và cũng chứng minh lợi thế của nguyên mẫu chuyên gia cho các mô hình khổng lồ.

5 Công trình liên quan
Tiền huấn luyện đã đạt được thành công lớn trong những năm này, và gần đây nó đã trở thành thực hành phổ biến trong xử lý ngôn ngữ tự nhiên [20;4;22;36;16;5]. Trong lĩnh vực học biểu diễn đa phương thức, tiền huấn luyện cũng đã trở nên quan trọng và đẩy giới hạn hiệu suất mô hình trong các nhiệm vụ downstream [18;33;19;3;7;14;37;13;38]. Các nghiên cứu gần đây [10] chứng minh luật lũy thừa của quy mô mô hình và hiệu suất. Với sự phát triển nhanh chóng trong huấn luyện phân tán và song song [32;24;28;25], chúng ta đã chứng kiến sự bùng nổ của các nghiên cứu trong tiền huấn luyện quy mô cực lớn trong cả xử lý ngôn ngữ tự nhiên [2;32] và tiền huấn luyện đa phương thức [27;15] và cũng là hiệu suất state-of-the-art mới trong hai năm gần đây. Mặc dù các mô hình dày đặc quy mô cực lớn rất hiệu quả đặc biệt trong bối cảnh few-shot learning [2], một số nhà nghiên cứu đã chuyển sang các mô hình chuyên gia thưa thớt để tiền huấn luyện quy mô lớn hiệu quả. Lấy cảm hứng từ thành công của Mixture-of-Experts [30;26;31], các nghiên cứu gần đây [11;6] mở rộng kích thước mô hình lên hơn nghìn tỷ tham số và tận dụng đầy đủ lợi thế của TPUs để xây dựng các mô hình chuyên gia thưa thớt với Mesh-Tensorflow [31]. Họ chứng minh rằng các mô hình chuyên gia thưa thớt có thể hoạt động tốt hơn nhiều so với các mô hình dày đặc với cùng FLOPs tính toán nhưng chi phí tính toán của họ tương tự. Một loạt các công trình tiếp theo thành công triển khai các mô hình chuyên gia thưa thớt trên NVIDIA GPU [15;12]. Trong công trình này, chúng tôi theo thực hành của Lin et al. [15] và triển khai các mô hình của chúng tôi trên framework học phân tán Whale [35].

6 Kết luận
Trong công trình này, chúng tôi khám phá các yếu tố bên trong các mô hình chuyên gia thưa thớt và nghiên cứu cách chúng ảnh hưởng đến chất lượng mô hình và hiệu quả tính toán. Chúng tôi tìm ra rằng mất cân bằng tải có thể không phải là một vấn đề đáng kể ảnh hưởng đến chất lượng mô hình, nhưng số lượng chuyên gia được kích hoạt k và dung lượng chuyên gia C đóng vai trò quan trọng trong việc huấn luyện các mô hình MoE. Một phương pháp đơn giản gọi là nguyên mẫu chuyên gia chia các chuyên gia thành các nguyên mẫu khác nhau và áp dụng định tuyến k top-1 có thể giúp mô hình đạt được hiệu suất cải thiện trong khi giữ chi phí tính toán không đổi. Chúng tôi mở rộng loạt thí nghiệm của chúng tôi sang các mô hình quy mô cực lớn với hơn 10 và 100 tỷ tham số và chứng minh hiệu quả của nguyên mẫu chuyên gia trong việc huấn luyện các mô hình quy mô cực lớn. Cuối cùng, chúng tôi thành công triển khai mô hình 1 nghìn tỷ tham số chỉ trên 480 GPU NVIDIA V100-32GB, so sánh với các SOTA gần đây được triển khai trên 2048 lõi TPU. Chúng tôi cho thấy rằng phương pháp đơn giản của chúng tôi có thể cải thiện hiệu suất của các mô hình chuyên gia thưa thớt 1 nghìn tỷ tham số một cách hiệu quả và giúp chúng đạt được tăng tốc đáng kể trong hội tụ.

--- TRANG 10 ---
Tài liệu tham khảo
[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and X. Zheng. Tensorflow: A system for large-scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016, pages 265–283, 2016.

[2] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

[3] Y. Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y. Cheng, and J. Liu. UNITER: universal image-text representation learning. In ECCV 2020, pages 104–120, 2020.

[4] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT 2019, pages 4171–4186, 2019.

[5] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y. Wang, J. Gao, M. Zhou, and H. Hon. Unified language model pre-training for natural language understanding and generation. In NeurIPS 2019, pages 13042–13054, 2019.

[6] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https://arxiv.org/abs/2101.03961.

[7] Z. Gan, Y.-C. Chen, L. Li, C. Zhu, Y. Cheng, and J. Liu. Large-scale adversarial training for vision-and-language representation learning. In NeurIPS 2020, 2020.

[8] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang. Fastmoe: A fast mixture-of-expert training system. CoRR, abs/2103.13262, 2021.

[9] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR 2016, pages 770–778, 2016.

[10] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. CoRR, abs/2001.08361, 2020.

[11] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.

[12] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer. BASE layers: Simplifying training of large, sparse models. CoRR, abs/2103.16716, 2021.

[13] W. Li, C. Gao, G. Niu, X. Xiao, H. Liu, J. Liu, H. Wu, and H. Wang. UNIMO: towards unified-modal understanding and generation via cross-modal contrastive learning. CoRR, abs/2012.15409, 2020.

[14] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, Y. Choi, and J. Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. CoRR, abs/2004.06165, 2020.

[15] J. Lin, R. Men, A. Yang, C. Zhou, M. Ding, Y. Zhang, P. Wang, A. Wang, L. Jiang, X. Jia, J. Zhang, J. Zhang, X. Zou, Z. Li, X. Deng, J. Liu, J. Xue, H. Zhou, J. Ma, J. Yu, Y. Li, W. Lin, J. Zhou, J. Tang, and H. Yang. M6: A chinese multimodal pretrainer. CoRR, abs/2103.00823, 2021.

[16] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.

--- TRANG 11 ---
[17] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR 2019, 2019.

[18] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS 2019, pages 13–23, 2019.

[19] J. Lu, V. Goswami, M. Rohrbach, D. Parikh, and S. Lee. 12-in-1: Multi-task vision and language representation learning. CoRR, abs/1912.02315, 2019.

[20] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In NAACL-HLT 2018, pages 2227–2237, 2018.

[21] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised multitask learners.

[22] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf, 2018.

[23] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. CoRR, abs/1910.10683, 2019.

[24] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He. Zero: Memory optimization towards training a trillion parameter models. arXiv preprint arXiv:1910.02054, 2019.

[25] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y. He. Zero-infinity: Breaking the GPU memory wall for extreme scale deep learning. CoRR, abs/2104.07857, 2021.

[26] P. Ramachandran and Q. V. Le. Diversity and depth in per-example routing models. In 7th International Conference on Learning Representations, ICLR 2019, 2019.

[27] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation, 2021.

[28] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He. Zero-offload: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840, 2021.

[29] N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, pages 4603–4611, 2018.

[30] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

[31] N. Shazeer, Y. Cheng, N. Parmar, D. Tran, A. Vaswani, P. Koanantakool, P. Hawkins, H. Lee, M. Hong, C. Young, R. Sepassi, and B. A. Hechtman. Mesh-tensorflow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, pages 10435–10444, 2018.

[32] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.

[33] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai. Vl-bert: Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019.

[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In NeurIPS 2017, pages 5998–6008, 2017.

[35] A. Wang, X. Jia, L. Jiang, J. Zhang, Y. Li, and W. Lin. Whale: A unified distributed training framework. arXiv preprint arXiv:2011.09208, 2020.

--- TRANG 12 ---
[36] Z. Yang, Z. Dai, Y. Yang, J. G. Carbonell, R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. In NeurIPS 2019, pages 5754–5764, 2019.

[37] F. Yu, J. Tang, W. Yin, Y. Sun, H. Tian, H. Wu, and H. Wang. Ernie-vil: Knowledge enhanced vision-language representations through scene graph. arXiv preprint arXiv:2006.16934, 2020.

[38] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao. Vinvl: Making visual representations matter in vision-language models. CoRR, abs/2101.00529, 2021.

--- TRANG 13 ---
A Phụ lục
A.1 Tiền huấn luyện Đa phương thức và Đánh giá Downstream
Trong thực tế, chúng tôi theo Lin et al. [15] sử dụng một mô hình tiền huấn luyện đa phương thức quy mô cực lớn với kiến trúc MoE bằng tiếng Trung. Cụ thể, chúng tôi tiền huấn luyện một mô hình trên các cặp ảnh-văn bản từ dataset M6-Corpus [15]. Trong tiền huấn luyện đa phương thức, mô hình tiền huấn luyện nhận các đầu vào của một cặp ảnh và văn bản liên quan làm đầu vào và tạo ra các biểu diễn cấp cao với các lớp Transformer [34]. Trong các thí nghiệm của chúng tôi, chúng tôi đầu tiên biến đổi một ảnh đầu vào thành các đặc trưng patch bằng cách chia nó thành các patch 4×4 và trích xuất các đặc trưng patch với một ResNet đã được huấn luyện [9]. Chúng tôi làm phẳng các đặc trưng patch của ảnh đầu vào thành một chuỗi biểu diễn và nối chúng với word embeddings của chuỗi văn bản ngắn hơn 128 từ. Sau đó chúng tôi xây dựng một trình trích xuất đặc trưng với nhiều lớp transformer bao gồm self attention và feed-forward neural networks (FFN). Đáng chú ý, để tích hợp MoE vào kiến trúc mô hình, chúng tôi thay thế FFN bằng MoE, trong đó FFN làm chuyên gia được phân phối trên các worker. Chúng tôi tiền huấn luyện mô hình với nhiệm vụ image captioning, trong đó mô hình học tạo từ theo cách tự hồi quy dựa trên ngữ cảnh trước đó bao gồm các đặc trưng patch.

Để đánh giá toàn diện hiệu suất của các phương pháp, chúng tôi tiến hành thí nghiệm về image captioning bằng tiếng Trung, và chúng tôi theo Lin et al. [15] sử dụng dataset E-commerce IC. Chúng tôi tập trung vào khả năng language modeling của mô hình tiền huấn luyện, và do đó chúng tôi sử dụng teacher forcing và đánh giá hiệu suất bằng perplexity (PPL).

A.2 Thiết lập Thí nghiệm
Để khám phá, chúng tôi nghiên cứu các thiết lập khác nhau cho cả hai mô hình. Ở đây chúng tôi chỉ ra các cấu hình chính của thiết lập thí nghiệm và chúng tôi minh họa chi tiết trong Bảng 5. Theo BERT-Chinese [4], chúng tôi sử dụng cùng từ vựng với 21128 từ con. Theo thực hành của Raffel et al. [23], chúng tôi sử dụng cùng hidden size 1024 cho tất cả các mô hình. Chúng tôi thường mở rộng kích thước mô hình bằng cách tăng số lượng lớp, intermediate size, cũng như số lượng chuyên gia. Đối với thiết lập attention, số lượng attention head là 16 và attention head size là 64. Đối với khởi tạo, chúng tôi sử dụng khởi tạo BERT với μ=0 và σ=0.02 cho hầu hết các trường hợp, và chúng tôi sử dụng khởi tạo với độ lệch chuẩn nhỏ hơn 0.002 cho mô hình 1T. Về dung lượng chuyên gia, chúng tôi thường sử dụng hệ số dung lượng α=1.25 để có thêm buffer. Batch size trên mỗi GPU là 8 và tổng batch size bằng tích của batch size trên mỗi GPU và số lượng GPU. Chúng tôi sử dụng optimizer AdamW [17] để tối ưu hóa ngoại trừ mô hình 1T trong đó chúng tôi sử dụng Adafactor [29] thay thế. Learning rate cho AdamW là 8e-5, và cho Adafactor là 5e-3. Chúng tôi sử dụng lịch trình warmup với warmup step là 500. Dropout rate cho FFN và attention là 0.1. Chúng tôi sử dụng huấn luyện mixed precision cho giao tiếp FP16 cho tất cả các mô hình ngoại trừ mô hình 1T do vấn đề bất ổn định huấn luyện.

Bảng 5: Siêu tham số để tiền huấn luyện các mô hình MoE.
Hparam         base    10B     100B    1T
Hidden size    1024    1024    1024    1024
Intermediate size 4096 4096    4096    21248
Number of layers  5    10      24      24
Number of attention heads 16 16 16    16
Attention head size 64  64     64      64
Initializer range 0.02 0.02   0.02    0.002
Number of experts 32   128     512     960
Number of GPUs    8    16      128     480
Optimizer      AdamW  AdamW   AdamW   Adafactor
Learning rate  8e-5   8e-5    8e-5    5e-3
Mixed precision X     X       X       ✗
FP16 communication X  X       X       ✗
Params         1.4B   10.8B   103.2B  1002.7B

Chúng tôi triển khai các thí nghiệm của chúng tôi trên Tensorflow 1.15 [1]. Khác với triển khai ban đầu của Switch và GShard với Mesh-Tensorflow [31], chúng tôi triển khai mô hình tiền huấn luyện đa phương thức với framework Whale [35], cho phép song song dữ liệu, mô hình và chuyên gia trên NVIDIA GPU.

--- TRANG 14 ---
A.3 Mã giả cho Nguyên mẫu Chuyên gia
Các mã giả cho lớp MoE và nguyên mẫu chuyên gia được đề xuất trong Whale được cung cấp trong Hình 7 và Hình 8 tương ứng. Bảng 6 minh họa ký hiệu của các chiều tensor cụ thể.

Bảng 6: Bảng ký hiệu cho mã giả.
Biến    Định nghĩa
D       Số lượng worker
d       Số lượng GPU trên mỗi worker (d=1 trong bài báo này)
E       Số lượng tổng chuyên gia
e       Số lượng chuyên gia trên mỗi worker (e*D=E)
C       Dung lượng trên mỗi chuyên gia
M       Kích thước mô hình (giống như hidden size, giống như embedding size)
I       Intermediate size
B       Batch Size trên mỗi GPU
L       Sequence Length
T       Số lượng token (T=B*L)
Z       Số lượng nguyên mẫu
F       Số lượng chuyên gia trên mỗi nguyên mẫu (Z*F=E)

Lượng Giao tiếp All-to-All Có hai thao tác giao tiếp all-to-all trong mỗi lớp MoE FFN trong một quá trình lan truyền tiến (một cho dispatch_inputs và một cho outputs trong mã giả). Trong quá trình giao tiếp, mỗi entry của tensor được giao tiếp đi qua một worker một lần. Do đó, tổng lượng giao tiếp, là O(EdCM) + O(eDCM) = O(EdCM) = O(ECM), phụ thuộc vào số lượng chuyên gia, dung lượng và kích thước mô hình.

Lượng Tính toán Tổng lượng tính toán trong lớp MoE FFN chủ yếu được chi phối bởi hai phép nhân ma trận, biến đổi tensor đầu vào từ hidden size thành intermediate size và sau đó ngược lại. Tổng tính toán của hai phép nhân ma trận này là O(DeCMI) + O(DeCIM) = O(ECMI). Trong profiling của chúng tôi trên mô hình MoE quy mô 1T, hai thao tác này chiếm khoảng 98% tổng FLOPs tiến truyền của lớp MoE FFN.

--- TRANG 15 ---
Hình 7: Mã giả của lớp MoE Transformer trong Whale.

--- TRANG 16 ---
Hình 8: Mã giả của Nguyên mẫu Chuyên gia.
