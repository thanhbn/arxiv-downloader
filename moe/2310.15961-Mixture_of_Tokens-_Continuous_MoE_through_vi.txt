# 2310.15961.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2310.15961.pdf
# Kích thước tệp: 1085768 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hỗn hợp Token: MoE Liên tục thông qua
Tổng hợp Chéo Ví dụ
Szymon Antoniak∗
IDEAS NCBR
Đại học WarsawMichał Krutul∗
IDEAS NCBR
Đại học WarsawMaciej Pióro
IDEAS NCBR
Viện Hàn lâm Khoa học Ba Lan
Jakub Krajewski
IDEAS NCBR
Đại học WarsawJan Ludziejewski
IDEAS NCBR
Đại học WarsawKamil Ciebiera
IDEAS NCBR
Đại học WarsawKrystian Król
IDEAS NCBR
Đại học Warsaw
Tomasz Odrzygó´ zd´ z
IDEAS NCBRMarek Cygan
Đại học Warsaw
NomagicSebastian Jaszczur∗
IDEAS NCBR
Đại học Warsaw

Tóm tắt
Các mô hình Hỗn hợp Chuyên gia (MoE) dựa trên kiến trúc Transformer đang đẩy xa ranh giới của các tác vụ ngôn ngữ và thị giác. Sức hấp dẫn của các mô hình này nằm ở khả năng tăng đáng kể số lượng tham số mà không có sự gia tăng tương ứng về FLOPs. Hầu hết các mô hình MoE được áp dụng rộng rãi đều không liên tục đối với tham số của chúng - thường được gọi là thưa thớt. Cùng lúc đó, các thiết kế MoE liên tục hiện có hoặc tụt hậu so với các đối tác thưa thớt hoặc không tương thích với giải mã tự hồi quy. Được thúc đẩy bởi quan sát rằng việc thích ứng các phương pháp hoàn toàn liên tục đã là xu hướng bao quát trong học sâu, chúng tôi phát triển Hỗn hợp Token (MoT), một kiến trúc đơn giản, liên tục có khả năng mở rộng số lượng tham số tương tự như các mô hình MoE thưa thớt. Khác với các phương pháp thông thường, MoT gán các hỗn hợp token từ các ví dụ khác nhau cho mỗi chuyên gia. Kiến trúc này hoàn toàn tương thích với huấn luyện và tạo sinh tự hồi quy. Các mô hình tốt nhất của chúng tôi không chỉ đạt được sự gia tăng 3× về tốc độ huấn luyện so với các mô hình Transformer dày đặc trong tiền huấn luyện ngôn ngữ mà còn khớp với hiệu suất của các kiến trúc MoE tiên tiến nhất. Ngoài ra, một mối liên hệ chặt chẽ giữa MoT và MoE được chứng minh thông qua một kỹ thuật mới mà chúng tôi gọi là điều chỉnh chuyển tiếp.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) dựa trên Transformer tạo thành một trong những lĩnh vực tích cực nhất trong AI, thể hiện hiệu suất ở mức con người trong nhiều tác vụ khác nhau, bao gồm dịch thuật, hiểu ngôn ngữ, lý luận và tạo mã [OpenAI et al., 2023, Anil et al., 2023, Chowdhery et al., 2022]. Kích thước khổng lồ của tất cả các mô hình ngôn ngữ tiên tiến nhất là không thể thiếu đối với thành công của chúng, với số lượng tham số đạt tới hàng chục hoặc thậm chí hàng trăm tỷ. Hiện tượng này phù hợp với Kaplan et al. [2020] và Hoffmann et al. [2022], trong đó nghiên cứu sau đề xuất rằng kích thước mô hình tối ưu tăng tỷ lệ thuận với ngân sách tính toán có sẵn. Cho rằng hiệu quả phần cứng đã tăng đều đặn trong thập kỷ qua [Sevilla et al., 2022, Rasch et al., 2023], những phát hiện này gợi ý rằng việc mở rộng sẽ vẫn là một thành phần quan trọng của việc huấn luyện các mô hình ngày càng có khả năng.

∗Những người đóng góp cốt lõi. Các đóng góp chi tiết của tác giả được liệt kê trong Phụ lục C.
Bản thảo. Đang xem xét.arXiv:2310.15961v2 [cs.CL] 24 Sep 2024

--- TRANG 2 ---
Hình 1: Hỗn hợp Token: Mỗi chuyên gia nhận một hỗn hợp token độc nhất trong nhóm. Trọng số hỗn hợp được xác định bởi bộ điều khiển, là một lớp kết nối đầy đủ (được bỏ qua để rõ ràng). Đối với một token nhất định, bản cập nhật của nó là một kết hợp tuyến tính của các đầu ra chuyên gia, với các hệ số bằng trọng số hỗn hợp gốc của token đó cho mỗi chuyên gia.

Tuy nhiên, việc mở rộng mô hình bao giờ cũng đi kèm với chi phí. Các mô hình lớn hơn thực hiện nhiều Phép tính Điểm nổi (FLOPs) hơn cho mỗi token, dẫn đến việc cả huấn luyện và suy luận trở nên chậm hơn và đắt đỏ hơn [Samsi et al., 2023, McDonald et al., 2022]. Các kiến trúc Hỗn hợp Chuyên gia [Jacobs et al., 1991] cung cấp một giải pháp thay thế hấp dẫn cho các Transformer tiêu chuẩn bằng cách tăng mạnh số lượng tham số. Ý tưởng cốt lõi là có nhiều chuyên gia, mỗi chuyên gia chuyên về một phần khác nhau của không gian đầu vào. Hiện tại, các mô hình tiên tiến nhất dựa trên MoE tận dụng độ thưa thớt bằng cách chỉ kích hoạt một phần tham số cho mỗi token [Sanseviero et al., 2023]. Điều này cho phép các mạng tăng số lượng tham số lên một bậc độ lớn trong khi giữ cho FLOPs mỗi token gần như không đổi. Trong công trình này, chúng tôi sử dụng MoE để chỉ các kiến trúc Hỗn hợp Chuyên gia thưa thớt trừ khi được nêu rõ khác.

Độ thưa thớt nêu trên được thực hiện bằng một bộ định tuyến, một mạng nhỏ chọn các chuyên gia tốt nhất cho mỗi token. Điều này làm cho đầu ra của một lớp MoE trở nên không liên tục đối với các tham số của nó, vì chỉ một tập con các chuyên gia được chọn cho mỗi token (thường được thực hiện bằng phép toán top-k rời rạc). Sự không liên tục và những biến động kết quả của các quyết định của bộ định tuyến đã được chứng minh là làm hại hiệu quả huấn luyện [Dai et al., 2022, Chi et al., 2022] và được giả định là nguồn gốc của tính không ổn định huấn luyện trong các mô hình MoE lớn [Mustafa et al., 2022, Puigcerver et al., 2023]. Mặt khác, các kiến trúc MoE liên tục hiện có liên quan đến những đánh đổi, bao gồm không khả năng mở rộng [Muqeeth et al., 2023, Hazimeh et al., 2021], hoặc không tương thích với giải mã tự hồi quy [Puigcerver et al., 2023].

Bài báo này giới thiệu Hỗn hợp Token, một kiến trúc Transformer mới, liên tục có liên quan chặt chẽ đến Hỗn hợp Chuyên gia thưa thớt. Tương tự như MoE, nó có khả năng hỗ trợ số lượng tham số lớn mà không gây ra chi phí đáng kể về FLOPs. Ý tưởng cốt lõi đằng sau thiết kế của chúng tôi là để mỗi chuyên gia xử lý không phải các token riêng lẻ một cách riêng biệt mà là biểu diễn kết hợp của chúng.

Kỹ thuật này tạo ra một mô hình liên tục tránh được phép toán top-k. Nó không đòi hỏi các kỹ thuật bổ sung thường được yêu cầu trong các thiết kế MoE hiện có (cả thưa thớt và liên tục), chẳng hạn như hàm mất cân bằng tải, tính toán các giải pháp cho các bài toán tối ưu, hoặc các lịch trình huấn luyện không đồng nhất [Hazimeh et al., 2021, Jaszczur et al., 2021, Dai et al., 2022]. Nó có khả năng mở rộng số lượng tham số giống như MoE thưa thớt và tương thích với mô hình ngôn ngữ và tạo sinh tự hồi quy.

--- TRANG 3 ---
Những đóng góp của chúng tôi như sau:
• Kiến trúc Hỗn hợp Token, một kiến trúc Hỗn hợp Chuyên gia liên tục có thể mở rộng xử lý chung các token từ các ví dụ khác nhau.
• Đánh giá chuẩn trên tiền huấn luyện ngôn ngữ tự hồi quy so với các kiến trúc Hỗn hợp Chuyên gia thưa thớt, cũng như đường cơ sở Transformer tiêu chuẩn, so với đó MoT thể hiện tăng tốc 3×.
• Phân tích các thuộc tính mở rộng của các mô hình Hỗn hợp Chuyên gia trên nhiều quy mô.
• Cải thiện hiệu suất trong huấn luyện độ chính xác thấp so với Hỗn hợp Chuyên gia.
• Một kỹ thuật mới gọi là điều chỉnh chuyển tiếp, cho phép mô hình Hỗn hợp Token được tiền huấn luyện sử dụng suy luận MoE thưa thớt sau khi được điều chỉnh, với một phần chi phí tiền huấn luyện.

2 Bối cảnh và Công trình Liên quan
Trong phần này, chúng tôi cung cấp tổng quan về các phương pháp liên quan đến công trình của chúng tôi và thảo luận về sự khác biệt giữa các thiết kế MoE khác nhau. Chúng tôi sẽ giới thiệu kiến trúc Hỗn hợp Token trong Phần 3 và cung cấp so sánh chi tiết giữa MoT và các phương pháp liên quan trong Phần 3.3.

2.1 Mô hình Ngôn ngữ Lớn
Việc mở rộng Transformer đã được chứng minh là một yếu tố quan trọng trong việc đạt được kết quả tiên tiến nhất trong các tác vụ ngôn ngữ và thị giác [Kaplan et al., 2020, Zhai et al., 2022], với số lượng tham số lớn nhất được tiết lộ trong các mô hình dày đặc đạt tới hàng trăm tỷ tham số [Brown et al., 2020, Chowdhery et al., 2022, Anil et al., 2023]. Những mô hình lớn này thể hiện khả năng ấn tượng không có trong các đối tác nhỏ hơn [Wei et al., 2022]. Kaplan et al. [2020] và Hoffmann et al. [2022] đã chỉ ra rằng hiệu suất mô hình cuối cùng có thể dự đoán được và tương quan trực tiếp với kích thước mô hình và lượng dữ liệu huấn luyện. Tuy nhiên, việc tăng kích thước mô hình làm tăng nhu cầu tài nguyên tính toán trong cả huấn luyện và suy luận [Rae et al., 2022].

2.2 Hỗn hợp Chuyên gia
Hỗn hợp Chuyên gia (MoE) lần đầu tiên được giới thiệu bởi Jacobs et al. [1991] như một mạng nơ-ron giống như ensemble bao gồm các mạng con riêng biệt gọi là chuyên gia. Thiết kế ban đầu sử dụng một mạng gating để chọn một phân công mềm các chuyên gia cho mỗi đầu vào. Trong bối cảnh Học sâu, khái niệm về lớp MoE được giới thiệu trong Eigen et al. [2014]. Shazeer et al. [2017] đã kết hợp phiên bản thưa thớt của các lớp MoE với LSTM để huấn luyện một mô hình với hơn 100 tỷ tham số, chưa từng có vào thời điểm đó. Thiết kế, tương tự như các mô hình MoE tiên tiến nhất hiện nay, sử dụng một mạng định tuyến nhỏ để quyết định k chuyên gia tốt nhất cho mỗi đầu vào. Bằng cách chỉ chọn một tập con các chuyên gia, họ có thể tăng kích thước mạng trong khi giữ cho FLOPs mỗi token gần như không đổi. Transformer lần đầu tiên được kết hợp với các lớp MoE trong Lepikhin et al. [2020], nơi nó thay thế lớp feed-forward. Thiết kế được đơn giản hóa hơn nữa trong Fedus et al. [2022], huấn luyện một mô hình với 1,6 nghìn tỷ tham số với định tuyến top-1. Kể từ đó, một số nghiên cứu đã điều tra các thiết kế MoE thưa thớt khác nhau [Du et al., 2022, Jiang et al., 2024, Zhou et al., 2022, Roller et al., 2021, Lewis et al., 2021]. Một phân tích toàn diện về các thuộc tính mở rộng của các kiến trúc MoE thưa thớt có thể được tìm thấy trong Clark et al. [2022].

2.3 Hỗn hợp Chuyên gia Liên tục
Các kiến trúc liên tục đóng vai trò quan trọng trong lĩnh vực này do tính linh hoạt và hiệu quả của chúng. Hazimeh et al. [2021] là những người tiên phong trong việc giới thiệu chúng trong MoE bằng cách trình bày các kỹ thuật liên tục để tính toán mã hóa của việc lựa chọn một chuyên gia. Trong một phương pháp khác, Muqeeth et al. [2023] đã đề xuất một phương pháp nơi họ hợp nhất các chuyên gia dựa trên trọng số của mạng định tuyến. Trong một tiến bộ gần đây, Puigcerver et al. [2023] đã đề xuất một biến thể liên tục của MoE cho Vision Transformer, nơi các patch chỉ được trộn trong mỗi hình ảnh.

--- TRANG 4 ---
Hình 2: (Trái) Sơ đồ của một lớp feed-forward tiêu chuẩn có trong kiến trúc Transformer: mỗi token được xử lý bằng cùng một MLP, độc lập với các token khác. (Phải) Sơ đồ của một lớp Token Choice, nơi mỗi token quyết định chuyên gia nào để chọn. Theo cách này, các chuyên gia khác nhau xử lý một số lượng token khác nhau. Nếu một chuyên gia được chọn bởi quá nhiều token, một phần của các token sẽ bị loại bỏ — chúng không nhận được cập nhật nào.

2.4 Từ Phương pháp Cứng đến Mềm
Từ thời điểm đầu của lĩnh vực Học sâu, chúng ta thấy các chuyển động từ các hàm rời rạc sang các hàm liên tục. Perceptron đầu tiên [McCulloch and Pitts, 1943] sử dụng kích hoạt "tất cả hoặc không", được cho là để phù hợp với logic mệnh đề. Điều này sau đó được cải thiện bằng các hàm kích hoạt mềm, cho phép gradient descent và các mạng nơ-ron đa lớp. Tương tự, attention mềm, được giới thiệu trong Bahdanau et al. [2016], cho phép RNN nhìn vào đầu vào tùy ý từ quá khứ trong khi duy trì khả năng học cách chọn lọc bằng gradient descent tiêu chuẩn. Điều này tương phản với attention cứng, đòi hỏi, ví dụ, các kỹ thuật học tăng cường. Trong khi attention cứng có thể hoạt động ngang bằng với attention mềm [Xu et al., 2015, Zohourianshahzadi and Kalita, 2021], attention mềm, với sự đơn giản trong huấn luyện, đã trình bày những đánh đổi tốt hơn và sau đó được sử dụng làm khối xây dựng cơ bản của Transformer [Vaswani et al., 2017].

Hỗn hợp Chuyên gia, được giới thiệu vào Học sâu bởi Shazeer et al. [2017], có vẻ như là một hàm rời rạc tự nhiên — sau tất cả, chuyên gia hoặc xử lý một token nhất định hoặc không. Tuy nhiên, giống như sự chuyển đổi từ attention cứng sang mềm, một chuyên gia trong MoE có thể "chú ý" đến một hỗn hợp token, được lấy làm trung bình có trọng số. Điều này dẫn đến một mô hình mượt mà, liên tục và cho phép huấn luyện ổn định hơn.

3 Hỗn hợp Token
Mục tiêu của công trình này là tạo ra một kiến trúc hiệu quả, liên tục giữ lại khả năng mở rộng của Hỗn hợp Chuyên gia trong khi bỏ qua phép toán top-k giới hạn sự tiếp xúc của token với các chuyên gia khác nhau. Một cách trực quan để đạt được điều này là định tuyến tất cả các token đến tất cả các chuyên gia, nhưng phương pháp này không khả thi về mặt tính toán cho tiền huấn luyện quy mô lớn. Để chống lại ràng buộc này, phương pháp được khám phá trong công trình này xem xét điều gì xảy ra không với một token riêng lẻ mà với toàn bộ nhóm token thay vào đó.

Đóng góp chính của công trình này là quan sát rằng việc cho phép một chuyên gia tự động tạo ra một biểu diễn liên tục của toàn bộ nhóm token nhẹ hơn để xử lý so với mỗi token riêng lẻ mang lại kết quả tích cực.

Cụ thể hơn, trong thiết kế của chúng tôi, một batch đầu vào được chia thành các nhóm token, và mỗi nhóm được xử lý độc lập. Cho một nhóm và một chuyên gia duy nhất, một trọng số vô hướng được tạo ra cho mỗi token. Các trọng số sau đó được chuẩn hóa và sử dụng để tính toán một kết hợp tuyến tính của các token, được sử dụng làm đầu vào của chuyên gia. Các đầu ra của chuyên gia được sử dụng để cập nhật token như sau: đối với mỗi token đầu vào, bản cập nhật của nó là một kết hợp tuyến tính của các đầu ra chuyên gia, với trọng số hỗn hợp của token cho mỗi chuyên gia làm hệ số². Sơ đồ của phương pháp chúng tôi được trình bày trong Hình 1.

Để hiểu tại sao phương pháp này có thể mở rộng, sẽ hữu ích khi xem xét mối quan hệ giữa số lượng token trong một nhóm và số lượng chuyên gia. Về cơ bản, nếu hai con số này bằng nhau, tổng tính toán

²Các tác giả lưu ý rằng một lớp MoT cho phép triển khai vector hóa hiệu quả, nơi tất cả các tính toán có ý nghĩa được thực hiện bằng phép nhân ma trận theo batch.

--- TRANG 5 ---
Thuật toán 1 Lớp Hỗn hợp Token
1: foreach E in experts do:
2:   weightsE = Softmax(Linear(tokens))
3:   mix = Σi tokeni * weightsi,E
4:   outputE = E(mix)
5: foreach i do
6:   foreach E do
7:     updatei = ΣE outputE * weightsi,E

được thực hiện bởi các chuyên gia là giống như trong trường hợp định tuyến top-1. Điều này cho phép MoT hưởng lợi từ cùng một việc mở rộng tham số như thấy trong MoE, mà chúng tôi xác nhận một cách thực nghiệm trong Phần 4.2.

3.1 Nhiều hỗn hợp hơn cho mỗi chuyên gia
Dựa trên thiết kế được mô tả ở trên, chúng tôi thử nghiệm với việc đưa nhiều hơn một hỗn hợp vào mỗi chuyên gia. Nếu được thực hiện mà không có bất kỳ sửa đổi nào khác, điều này có nghĩa là tăng tuyến tính chi phí tính toán cho mỗi hỗn hợp bổ sung được xử lý. Để tránh chi phí bổ sung này, MoT sử dụng nhiều chuyên gia hơn, nhưng mỗi chuyên gia có kích thước ẩn giảm tỷ lệ. Theo cách này, mỗi hỗn hợp được xử lý bởi một chuyên gia nhỏ, và tổng số tham số của lớp, cũng như số lượng FLOPs được sử dụng bởi tất cả các chuyên gia, giữ nguyên. Chúng tôi thấy thiết kế này mang lại những cải thiện nhất quán khi số lượng hỗn hợp được xử lý tăng lên.

3.2 Nhóm Token trong MoT
Câu hỏi về cách các nhóm token được quyết định trong một batch là rất quan trọng để tương thích với huấn luyện và suy luận tự hồi quy. Insight chính là các token từ cùng một chuỗi không thể được đặt trong một nhóm, vì phép toán trộn sẽ dẫn đến rò rỉ thông tin. Do hạn chế này, MoT nhóm các token từ các ví dụ khác nhau dựa trên vị trí của chúng trong chuỗi. Do đó, tất cả các token trong một nhóm có cùng vị trí trong các chuỗi tương ứng của chúng. Như đã đề cập trước đó, để giữ cho số lượng FLOPs mỗi token không đổi, sự gia tăng số lượng chuyên gia có nghĩa là sự gia tăng bằng nhau về kích thước nhóm. Một minh họa về cách nhóm được thực hiện trong một batch token được hiển thị trong Hình 3.

nhạc là nhạc tôi sẽ
là tất cả là tất cả cầu thang có thể
họ ra họ ra vào
thức ăn cần thức ăn cần Thiên đàng tốt
Nếu Chú ý Nếu Chú ý mua Cơ hội
Nếu Chú ý Cơ hội mua Cơ hội sẽ cầu thang có thể vào Thiên đàng tốt

Hình 3: Mỗi nhóm bao gồm các token có cùng vị trí trong một chuỗi. Trong ví dụ này, kích thước nhóm là 2. Lưu ý rằng kích thước nhóm tối đa có thể bằng với kích thước batch.

--- TRANG 6 ---
3.3 So sánh với các Kiến trúc Hỗn hợp Chuyên gia khác
Mở rộng. Kỹ thuật được trình bày trong Hazimeh et al. [2021] dựa trên một bộ định tuyến top-k thưa thớt có thể vi phân liên tục, đây là một lợi thế lớn so với gating top-k thông thường. Tuy nhiên, phương pháp này đòi hỏi rằng tất cả các chuyên gia phải được sử dụng trong một phần của quá trình huấn luyện, khiến nó trở nên tốn kém về mặt tính toán đối với các mô hình có số lượng chuyên gia lớn. Kiến trúc dựa trên việc hợp nhất các chuyên gia được đề xuất trong Muqeeth et al. [2023] cũng trình bày một giải pháp thay thế hấp dẫn, liên tục cho gating top-k, nhưng chi phí của việc hợp nhất tất cả các chuyên gia một lần nữa tăng tuyến tính với số lượng chuyên gia. Để chống lại điều này, kỹ thuật được áp dụng một lần cho mỗi chuỗi, điều này hạn chế sức mạnh biểu đạt của mô hình cuối cùng.

Tính ổn định huấn luyện. Lepikhin et al. [2020] báo cáo sự không ổn định trong quá trình huấn luyện các mô hình MoE lớn, bắt nguồn từ sự không chính xác khi tính toán trọng số định tuyến ở độ chính xác thấp. Để ổn định quá trình huấn luyện, họ đã phải sử dụng độ chính xác đầy đủ. Fedus et al. [2022] đã đạt được tiến bộ trong việc sử dụng độ chính xác hỗn hợp khi huấn luyện MoE bằng cách sử dụng độ chính xác cao có chọn lọc cho gating. Khi so sánh Lepikhin et al. [2020], Fedus et al. [2022] với MoT, một lợi thế của kỹ thuật chúng tôi xuất hiện - nó mạnh mẽ hơn đối với huấn luyện ở độ chính xác thấp hơn so với các phương pháp khác. Chúng tôi suy đoán rằng điều này là do cơ chế hợp nhất ít dễ bị lỗi làm tròn hơn so với gating trong MoE thưa thớt.

Loại bỏ token. Loại bỏ token là một hiện tượng nơi các token không nhận được cập nhật từ bất kỳ chuyên gia nào. Điều này có thể xảy ra vì chuyên gia được chọn bởi quá nhiều token trong một batch [Fedus et al., 2022, Lepikhin et al., 2020, Zoph et al., 2022] hoặc, trong trường hợp định tuyến chuyên gia đến token, khi một token không được chọn bởi bất kỳ chuyên gia nào [Zhou et al., 2022]. Các kỹ thuật hiện có chống lại hiện tượng này cung cấp một giải pháp một phần, nhưng vấn đề vẫn còn. Ngược lại, các token trong MoT là một phần của mỗi hỗn hợp được tạo ra trong nhóm của chúng; do đó, chúng luôn nhận được cập nhật.

Giải mã tự hồi quy. Hỗn hợp Token dựa trên khái niệm hợp nhất các token trước khi được xử lý bởi một chuyên gia. Một thiết kế chỉ encoder có tính chất tương tự được trình bày trong công trình đồng thời [Puigcerver et al., 2023]. Kỹ thuật này dựa trên việc hợp nhất các patch trong một hình ảnh cho các mô hình thị giác. Sự khác biệt quan trọng giữa kỹ thuật này và MoT là MoT tương thích với huấn luyện và suy luận tự hồi quy.

4 Thí nghiệm
Trọng tâm của công trình này là điều tra hiệu quả của Token trong mô hình ngôn ngữ tự hồi quy. Để đo lường chất lượng mô hình, chúng tôi tiền huấn luyện các mô hình cho một số lượng token cố định và so sánh độ phức tạp cuối cùng theo văn học MoE hiện có [Du et al., 2022, Fedus et al., 2022]. Trong tất cả các thí nghiệm, các mô hình được huấn luyện trên bộ dữ liệu C4 [Raffel et al., 2023] và sử dụng bộ tokenizer GPT-2. Trừ khi được chỉ định khác, chúng tôi sử dụng độ chính xác hỗn hợp, nơi tất cả các tính toán nặng được thực hiện trong bfloat16, trong khi trạng thái optimizer và trọng số chính được giữ ở độ chính xác đầy đủ. Để nghiên cứu tính ổn định của mô hình, chúng tôi thử nghiệm với việc huấn luyện hoàn toàn ở độ chính xác giảm.

Kết quả chính của chúng tôi là sự tăng tốc đáng kể của các mô hình MoT so với Transformer dày đặc (Hình 7) và kết quả có thể so sánh với MoE thưa thớt (Hình 6). Tiếp theo là phân tích các thuộc tính mở rộng của kiến trúc MoT đối với số lượng tham số (Hình 4) và số lượng hỗn hợp gửi đến mỗi chuyên gia (Hình 5). Chúng tôi điều tra hiệu suất của mô hình ở độ chính xác thấp để mô phỏng sự không ổn định huấn luyện và thấy rằng MoT ít dễ bị các sự không ổn định xuất phát từ huấn luyện độ chính xác thấp. Cuối cùng, chúng tôi cho thấy mối liên hệ giữa MoT và MoE bằng cách chi một phần bổ sung tính toán tiền huấn luyện để hiệu quả chuyển đổi một mô hình MoT thành mô hình Token Choice (Phần 4.4).

4.1 Kiến trúc Mô hình
Cơ sở của các thí nghiệm của chúng tôi là một Transformer chỉ decoder dựa trên GPT-2 [Radford et al., 2019]. Chúng tôi thử nghiệm trên hai quy mô mô hình: một mô hình Medium 77M và một mô hình Base 162M (tham khảo Phụ lục A để biết siêu tham số và chi tiết huấn luyện). Để sau đó có được mô hình Hỗn hợp Token, chúng tôi thay thế nửa sau của các lớp feed-forward trong Transformer bằng các lớp MoT. Tương tự như các mô hình MoE, FLOPs và số lượng tham số trong MoT được tách rời. Chúng tôi ký hiệu kiến trúc mô hình bằng đối tác dày đặc của nó về số lượng FLOPs và, riêng biệt, số lượng chuyên gia (hoặc tương đương, kích thước nhóm). Theo đó, một mô hình MoT-Medium/32E là một mô hình sử dụng cùng số lượng FLOPS như một mô hình Transformer Medium (77M) nhưng sử dụng 32 chuyên gia trong các lớp MoT.

--- TRANG 7 ---
Về thiết kế được mô tả trong Phần 3.1, Medium/32E/4 biểu thị một mô hình sử dụng các lớp MoT với 32·4 chuyên gia nhỏ có tổng số tham số giống như 32 chuyên gia thông thường.

Ngoài việc sử dụng Transformer làm đường cơ sở, chúng tôi cũng so sánh với Token Choice [Fedus et al., 2022] và Expert Choice [Zhou et al., 2022] như các đường cơ sở MoE thưa thớt. Vì Expert Choice nhạy cảm với kích thước batch, để tránh sự khác biệt giữa huấn luyện và suy luận, chúng tôi nhóm các token trước khi định tuyến trong việc huấn luyện các mô hình Expert Choice.

4.2 Kết quả Mở rộng
Các mô hình Hỗn hợp Token thể hiện các thuộc tính mở rộng mạnh mẽ đối với số lượng tham số. Như thấy trong Hình 4, việc tăng số lượng chuyên gia trong các lớp MoT trong khi sử dụng cùng ngân sách tính toán mang lại những cải thiện nhất quán. Tất cả các mô hình MoT là một cải thiện nghiêm ngặt so với Transformer. Hình này cũng có một thí nghiệm loại bỏ, nơi các trọng số trộn được cố định thành 1/n, với n là kích thước nhóm. Điều này tương ứng với một chiến lược trộn đồng đều; hiệu suất của mô hình đó rõ ràng bị ảnh hưởng, xác nhận rằng các lớp MoT học được các chiến lược trộn không tầm thường.

Hình 4: Mở rộng đối với số lượng tham số. Cũng được trình bày là đường cơ sở Transformer và một mô hình MoT với chiến lược định tuyến đồng đều, không thể học được.

Số lượng hỗn hợp token tăng lên được mô tả trong Phần 3.1 là một trục mở rộng khác cho các mô hình MoT, một lần nữa thể hiện những cải thiện nhất quán. Chúng tôi giả thuyết rằng hiện tượng này là do hai cơ chế: thứ nhất, mô hình đơn giản là có tính biểu đạt cao hơn với số lượng lớn hơn các chuyên gia nhỏ hơn, và thứ hai, mô hình có thể phân bổ sự tập trung của mình (trọng số trộn) linh hoạt hơn cho các token quan trọng hơn trong khi giảm các cập nhật cho những token tầm thường.

Hình 5: Mở rộng đối với số lượng hỗn hợp token.

--- TRANG 8 ---
4.3 So sánh với Transformer và MoE Thưa thớt
Quan trọng nhất, hiệu suất của Hỗn hợp Token có thể so sánh với các đường cơ sở Hỗn hợp Chuyên gia mạnh mẽ (Hình 6). Số lượng hỗn hợp tăng lên cho phép nó cạnh tranh với cả kiến trúc Expert Choice và Token Choice. Vì định tuyến thưa thớt được giả thuyết góp phần vào sự không ổn định huấn luyện trong các mô hình thưa thớt lớn, Hỗn hợp Token, với tính liên tục, trình bày một giải pháp thay thế hứa hẹn. Để điều tra sự không ổn định huấn luyện ở quy mô mà chúng tôi thử nghiệm, chúng tôi đã huấn luyện các mô hình hoàn toàn trong bfloat16, trái ngược với độ chính xác hỗn hợp được sử dụng trong tất cả các thí nghiệm khác. Kết quả xác nhận rằng MoT kháng cự hơn đối với huấn luyện độ chính xác thấp: khi độ chính xác của huấn luyện giảm, hiệu suất của Expert Choice giảm xuống dưới Hỗn hợp Token, mặc dù trước đây đạt được độ phức tạp tốt hơn khi sử dụng độ chính xác hỗn hợp. Chúng tôi thấy đây là bằng chứng về tiềm năng của kiến trúc cho huấn luyện ổn định ở quy mô mô hình cao hơn. Xem Bảng 1 để biết chi tiết.

Hình 6: So sánh các kiến trúc MoT và sMoE. Số lượng chuyên gia nhỏ hơn tăng lên cho phép MoT khớp với hiệu suất của mô hình sMoE tốt nhất. Do các ràng buộc tính toán, các mô hình được huấn luyện trong 100K bước.

Cuối cùng, chúng tôi kết hợp các phát hiện của mình về các thuộc tính mở rộng MoT để huấn luyện mô hình MoT hiệu quả nhất và so sánh nó với đường cơ sở Transformer (Hình 7). Kết quả là một mô hình đạt được mất mát cuối cùng của đường cơ sở chỉ trong một phần ba số bước huấn luyện. Điều này đại diện cho một cải thiện 3× về ngân sách tính toán.

Hình 7: Mô hình MoT tốt nhất của chúng tôi đạt được mất mát cuối cùng của đường cơ sở chỉ trong 33% ngân sách tính toán.

--- TRANG 9 ---
Hình 8: Điều chỉnh chuyển tiếp: 150K bước đầu tiên của mô hình được thực hiện bằng kiến trúc Hỗn hợp Token. Sau đó, một mô hình Token Choice mới được khởi tạo với các trọng số lấy từ mô hình MoT, và mô hình huấn luyện thêm 15K bước để phục hồi hiệu suất. Sự tăng đột biến trong mất mát là do sự thay đổi đột ngột của kiến trúc.

Bảng 1: So sánh mất mát kết quả huấn luyện độ chính xác thấp hơn. MoT hoạt động tốt hơn trong môi trường chỉ bfloat16. Tỷ lệ học được điều chỉnh riêng biệt ở độ chính xác thấp hơn cho cả Expert Choice và MoT. Kết quả trung bình trên 3 hạt giống ngẫu nhiên.

MoT-Medium/32E Expert Choice-Medium/32E
Độ chính xác hỗn hợp 3.442 (±0.002) 3.420 (±0.002)
Chỉ bf16 3.661 (±0.007) 3.728 (±0.044)

4.4 Điều chỉnh Chuyển tiếp
Hỗn hợp Token gặp phải một nhược điểm chung với MoE, đó là nó không hỗ trợ suy luận không có batch. Đây là hệ quả trực tiếp của thiết kế của nó — trong lượt truyền tiến, nó nhóm một số token từ các ví dụ khác nhau trong batch. Với việc áp dụng ngày càng tăng của các Mô hình Ngôn ngữ Lớn đối với phần cứng tiêu dùng [Touvron et al., 2023, Cerisara, 2023], việc thiếu hỗ trợ có thể làm vô hiệu hóa việc áp dụng rộng rãi hơn của kiến trúc. Trong khi Hỗn hợp Token với kích thước nhóm một về mặt kỹ thuật là có thể, để giữ cho FLOPs không đổi, lớp sẽ cần giảm một cách tầm thường thành một MLP Transformer tiêu chuẩn.

Để giải quyết vấn đề này, chúng tôi chứng minh rằng các trọng số được học bởi Hỗn hợp Token có thể được sử dụng để khởi tạo trực tiếp một mô hình Token Choice với cùng thông số kỹ thuật (số lượng chuyên gia và kích thước chuyên gia). Lớp chịu trách nhiệm tạo ra trọng số trộn được sử dụng để khởi tạo bộ định tuyến thưa thớt. Để giảm thiểu sự khác biệt hiệu suất do sự thay đổi kiến trúc này, chúng tôi huấn luyện toàn bộ mô hình mới (không có trọng số nào bị đóng băng) trong 10% tổng số bước tiền huấn luyện của mô hình gốc để phục hồi hiệu suất của mô hình gốc (được đo bằng mất mát đánh giá). Chúng tôi gọi kỹ thuật này là điều chỉnh chuyển tiếp. Theo cách này, có thể huấn luyện với Hỗn hợp Token và tận hưởng tạo sinh không có batch tại thời điểm suy luận. Chúng tôi giả thuyết rằng quy trình này sẽ đặc biệt hấp dẫn trong các thiết lập nơi các phần của mô hình không thể được huấn luyện ở độ chính xác cao hơn, chẳng hạn như trên phần cứng chuyên dụng, độ chính xác thấp. Kết quả được trình bày trong Hình 8.

5 Hạn chế và Hướng Công việc Tương lai
Với hiệu suất mạnh mẽ của MoT trên các mô hình cỡ trung bình, một bước tiếp theo rõ ràng là huấn luyện các mô hình lớn hơn. Điều này mang lại cơ hội để xác thực kết quả ổn định trên các mô hình lớn hơn, nơi sự không ổn định huấn luyện phổ biến hơn.

Như với hầu hết các mô hình Hỗn hợp Chuyên gia, dung lượng bộ nhớ của các lớp MoT là đáng kể. Các mô hình được mở rộng đòi hỏi lượng lớn RAM trên phần cứng chuyên dụng để huấn luyện, làm cho việc áp dụng chúng trở nên đắt đỏ. Theo hướng này, một hướng tương lai hấp dẫn sẽ là điều tra chưng cất mô hình với các mô hình Hỗn hợp Token.

Cuối cùng, cả huấn luyện và suy luận với MoT trộn các ví dụ khác nhau trong một batch duy nhất. Việc trộn các token từ các chuỗi khác nhau này và sự cần thiết phải thực hiện suy luận theo batch có thể không mong muốn trong một số trường hợp sử dụng. Trong khi thực hiện suy luận không có batch luôn không hiệu quả với LLM, vì thông lượng bộ nhớ để truy cập trọng số mô hình trở thành nút thắt cổ chai, suy luận không có batch vẫn tìm thấy các cách sử dụng của nó. Trong khi điều chỉnh chuyển tiếp giải quyết vấn đề này, việc khám phá các chiến lược suy luận khác nhau có thể mang lại những hiểu biết mới.

6 Kết luận
Trong công trình này, chúng tôi đã trình bày Hỗn hợp Token, một kiến trúc Hỗn hợp Chuyên gia liên tục mới tương thích với giải mã tự hồi quy. Kiến trúc này mở rộng đến kích thước mô hình tương tự như Hỗn hợp Chuyên gia thưa thớt, khớp với hiệu suất của nó, và kháng cự hơn đối với sự không ổn định huấn luyện do huấn luyện độ chính xác thấp hơn. Hơn nữa, chúng tôi giới thiệu điều chỉnh chuyển tiếp, một kỹ thuật để khởi tạo mô hình MoE với một mô hình MoE được tiền huấn luyện khác của kiến trúc khác nhau, và chỉ ra rằng mô hình mới đạt được hiệu suất của mô hình gốc sử dụng một phần của ngân sách tính toán.

Lời cảm ơn
Chúng tôi muốn bày tỏ lòng biết ơn chân thành đến Piotr Padlewski và Tomasz Trzci´nski cho phản hồi chung và Dagmara Rudzi´nska cho sự hỗ trợ vô giá với thiết kế đồ họa.

Công trình này được tài trợ bởi IDEAS NCBR, cũng cung cấp tài nguyên tính toán đáng kể. Marek Cygan được hỗ trợ một phần bởi tài trợ NCBiR POIR.01.01.01-00-0392/17-00. Nghiên cứu được hỗ trợ bởi cơ sở hạ tầng PL-Grid (tài trợ PLG/2023/016148). Chúng tôi cũng được hưởng lợi từ cụm Entropy (được lưu trữ tại Khoa Toán học, Tin học và Cơ học của Đại học Warsaw) được tài trợ bởi NVIDIA, Intel, các khoản tài trợ Trung tâm Khoa học Quốc gia Ba Lan UMO-2017/26/E/ST6/00622 và 2022/45/N/ST6/02222, và ERC Starting Grant TOTAL.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được giữ nguyên như trong bản gốc]

--- TRANG 10 ---
[Phần tài liệu tham khảo tiếp tục...]

--- TRANG 11 ---
[Phần tài liệu tham khảo tiếp tục...]

--- TRANG 12 ---
[Phần tài liệu tham khảo tiếp tục...]

--- TRANG 13 ---
[Phần tài liệu tham khảo tiếp tục...]

--- TRANG 14 ---
[Phần tài liệu tham khảo tiếp tục...]

--- TRANG 15 ---
A Siêu tham số Huấn luyện
Tất cả các mô hình được huấn luyện bằng độ chính xác hỗn hợp trừ khi được nêu rõ. Chúng tôi huấn luyện tất cả các thí nghiệm với kích thước batch là 256 và độ dài ngữ cảnh là 256 trong 150K bước huấn luyện (trừ khi được nêu rõ), dẫn đến tổng cộng 10B token huấn luyện. Chúng tôi sử dụng optimizer AdamW với các siêu tham số mặc định. Khi cần thiết, chúng tôi sử dụng phương pháp Fully Sharded Data Parallel từ PyTorch để song song hóa quá trình huấn luyện trên nhiều máy. Tỷ lệ học được điều chỉnh riêng biệt tùy thuộc vào kích thước mô hình và kiến trúc. Tỷ lệ học tối ưu cho Transformer là 1e-3 cho Medium và 4e-4 cho các mô hình Base, trong khi cho cả MoT và MoE, chúng là 7e-4 cho Medium và 2e-4 cho Base.

Bảng 2: Siêu tham số huấn luyện. Bảng cung cấp các mô hình ví dụ được trình bày trong các thí nghiệm. Tất cả các mô hình còn lại có thể được suy ra từ bảng này.

Mô hình Chuyên gia Kích thước Kích thước Tổng Khối dmodel dff #att.
         chuyên gia nhóm   tham số              heads
Transformer-Medium - - - 77M 8 512 2048 8
MoT-Medium/32E 32 2048 32 336M 8 512 - 8
MoT-Medium/32E/8 256 256 32 337M 8 512 - 8
Transformer-Base - - - 162M 12 768 3072 12
MoT-Base/32E 32 3072 32 520M 12 768 - 12
MoT-Base/64E/16 1024 192 64 977M 12 768 - 12

B Khả năng Tái tạo
Mã và các tệp cấu hình được sử dụng để tạo ra các kết quả được mô tả trong công trình này có sẵn trong kho lưu trữ công cộng của chúng tôi tại https://github.com/llm-random/llm-random.

C Đóng góp
Szymon triển khai PoC và các biến thể khác nhau của MoT, cùng với việc chạy thí nghiệm và tối ưu hóa. Michał triển khai và thử nghiệm với các thiết kế MoT khác nhau và đóng góp vào thiết kế và triển khai cơ sở hạ tầng. Sebastian cung cấp ý tưởng ban đầu, trực giác nghiên cứu, và giám sát dự án trực tiếp. Maciej chịu trách nhiệm cho các phần đánh giá và kỹ thuật đáng kể. Jakub triển khai các đường cơ sở MoE, Jan ổn định quá trình huấn luyện Hỗn hợp Chuyên gia, trong khi cả hai đều giúp với việc điều chỉnh siêu tham số MoE. Tomasz tư vấn ý tưởng và giúp với cơ sở hạ tầng cụm. Kamil và Krystian đóng góp vào kỹ thuật chung. Mọi người ở trên đều đóng góp vào cơ sở hạ tầng của dự án. Marek cung cấp lời khuyên khoa học và giám sát cấp cao.
