# Hash Layers Cho Các Mô Hình Thưa Lớn
Stephen Roller Sainbayar Sukhbaatar Arthur Szlam Jason Weston
Facebook AI Research

## Tóm tắt
Chúng tôi nghiên cứu việc huấn luyện các lớp thưa sử dụng các tham số khác nhau cho các đầu vào khác nhau dựa trên hashing trong các mô hình Transformer lớn. Cụ thể, chúng tôi sửa đổi lớp feedforward để hash đến các tập trọng số khác nhau tùy thuộc vào token hiện tại, trên tất cả các token trong chuỗi. Chúng tôi chỉ ra rằng quy trình này hoặc vượt trội hơn hoặc cạnh tranh với các phương pháp mixture-of-expert học-để-định-tuyến như Switch Transformers và BASE Layers, trong khi không yêu cầu tham số định tuyến hoặc các số hạng bổ sung trong hàm mục tiêu như loss cân bằng tải, và không có thuật toán gán phức tạp. Chúng tôi nghiên cứu hiệu suất của các kỹ thuật hashing khác nhau, kích thước hash và đặc trưng đầu vào, và chỉ ra rằng các hash cân bằng và ngẫu nhiên tập trung vào các đặc trưng cục bộ nhất hoạt động tốt nhất, so với việc học các cluster hoặc sử dụng ngữ cảnh tầm xa hơn. Chúng tôi chỉ ra phương pháp của chúng tôi hoạt động tốt cả trên các tác vụ mô hình hóa ngôn ngữ lớn và đối thoại, và trên các tác vụ fine-tuning hạ lưu.

## 1 Giới thiệu
Các nghiên cứu gần đây về các mô hình Transformer đã cho thấy một xu hướng rõ ràng hướng tới cải thiện với quy mô trong dữ liệu và kích thước mô hình [1], phản ánh cùng xu hướng trong Machine Learning nói chung. Tuy nhiên, khi được kiến trúc một cách naïve, các mô hình lớn hơn (về số lượng tham số) chậm hơn để huấn luyện và đánh giá; và ở quy mô cực đại, với các hệ thống máy tính hiện tại, đòi hỏi kỹ thuật phức tạp để tạo điều kiện giao tiếp giữa các worker. Để giải quyết những thách thức này, các nhà nghiên cứu đã nghiên cứu các mô hình Mixtures-of-Experts (MoE) [2,3,4,5,6,7,8], nơi một "gater" định tuyến tính toán thông qua một tập con thưa của các trọng số của mô hình (các "expert modules"). Cụ thể trong bối cảnh Transformers cho Xử lý Ngôn ngữ Tự nhiên (NLP), các phương pháp gần đây đã dẫn đến hiệu suất tối ưu trong mô hình hóa ngôn ngữ [8]. Các mô hình MoE cho phép tăng số lượng tham số trong mô hình trong khi giữ ổn định số lượng tính toán ảnh hưởng đến một mẫu nhất định.

Một thành phần chính của mô hình MoE là chiến lược định tuyến (gating). Trong khi các mô hình MoE có thể có lợi thế tính toán trên mỗi tham số so với mô hình dense, chúng có thể kém mạnh mẽ về chức năng trên mỗi tham số. Một chiến lược định tuyến kém có thể dẫn đến các expert modules không được chuyên môn hóa đúng cách (về cơ bản tạo ra một mô hình ensemble ngẫu nhiên); hoặc chuyên môn hóa quá mức, sử dụng hàm gán dữ liệu để overfit. Trong khi đó, chính chiến lược định tuyến phải hiệu quả.

Một phương pháp tiêu chuẩn là huấn luyện một lớp trọng số đưa ra quyết định định tuyến dựa trên đầu vào của lớp được định tuyến. Theo cách cổ điển, điều này có thể được thực hiện với softmax trên việc lựa chọn các expert modules, và fitted thông qua backpropagation. Tuy nhiên, một softmax dense yêu cầu tất cả các expert modules chạy trên tất cả các điểm dữ liệu tại thời gian huấn luyện, điều này làm mất đi lợi ích tiết kiệm tính toán. Một số công trình đã chỉ ra rằng tính thưa có thể được duy trì trong quá trình huấn luyện, ví dụ [9,7,8,10]. Đặc biệt, Switch Transformers [8] chọn expert hàng đầu cho mỗi token sử dụng softmax trên trạng thái ẩn của token, nhưng yêu cầu một số hạng cân bằng tải trong hàm mục tiêu hoặc chúng có thể trở nên mất cân bằng hoặc suy thoái, cho kết quả kém. BASE Layers [10] sử dụng thuật toán gán tuyến tính để cố gắng giải quyết cùng vấn đề.

Trong công trình này, chúng tôi mô tả một chiến lược định tuyến đơn giản, thưa, hiệu quả dựa trên hashing các input tokens có hiệu quả trong bối cảnh Transformers-for-NLP. Chúng tôi chỉ ra phương pháp này hiệu quả trên một số dataset, so sánh thuận lợi với cả Switch Transformers và BASE Layers. Vì chiến lược định tuyến không yêu cầu tham số bổ sung, không thay đổi hàm mục tiêu hoặc thuật toán gán, tính đơn giản của nó có nghĩa là nó mạnh mẽ, nhanh và dễ thực hiện. Chúng tôi cung cấp phân tích chi tiết để giải thích tại sao phương pháp của chúng tôi hoạt động, và trong những điều kiện nào. Cho rằng khi huấn luyện các mô hình rất lớn, người ta thường chỉ có một cơ hội với ngân sách tính toán yêu cầu, và các nhà thí nghiệm sẽ không thể thử nhiều lựa chọn tham số, do đó chúng tôi ủng hộ phương pháp của chúng tôi như một ứng viên mạnh mẽ cho bối cảnh như vậy.

## 2 Nền tảng
Hãy trước tiên giới thiệu bối cảnh Mixture-of-Experts nơi chúng tôi áp dụng chiến lược định tuyến dựa trên hash của mình. Chúng tôi sử dụng cùng bối cảnh như [11,8,10] nơi một feedforward network (FFN) trong Transformer được thay thế bằng phiên bản MoE của nó. Cho một chuỗi đầu vào đã tokenized {x1; x2; : : : ; xT} của T tokens, một biểu diễn cho mỗi token được tính toán song song bởi một Transformer tiêu chuẩn [12]

h^L_1, h^L_2, : : : , h^L_T = TRANSFORMER(x1, x2, : : : , xT). (1)

Transformer bao gồm L layers tính toán các trạng thái ẩn cuối cùng cho mỗi token, và mỗi layer được cấu thành từ các sublayers self-attention và FFN, nơi FFNs là các mạng hoàn toàn kết nối hai lớp

h^l_t = SelfAttn(h^{l-1}_t) h^l_t = FFN(h^l_t). (2)

Ở đây chúng tôi bỏ qua skip-connections và normalization để ngắn gọn. Sau đó chúng tôi có thể thay thế một hoặc nhiều sublayers FFN bằng các expert modules. Thay thế FNN tại layer l bằng K expert FFNs, đầu ra của chúng sau đó được trộn với một hàm gating g():

h^l_t = FFN(h^l_t) → h^l_t = ∑_{i=1}^K g_i(h^l_t)FFN_i(h^l_t); t = 1, : : : , T, (3)

nơi quan trọng là mỗi token được định tuyến đến một mixture khác nhau của các experts, vì hàm gating phụ thuộc vào trạng thái ẩn cụ thể h^l_t của token.

Các phương pháp MoE thưa giả định các giá trị gating g_i thường bằng không, vì vậy chỉ một vài experts cần được tính toán để hiệu quả tốt hơn. Vì các expert FFNs không chia sẻ tham số, số lượng tham số tăng với K trong khi lượng tính toán trên mỗi input token giữ nguyên nếu MoE FFN chỉ định tuyến đến một expert duy nhất, và tính toán g_i rẻ. Trong khi điều này cho phép huấn luyện các mô hình dung lượng lớn với ngân sách tính toán nhỏ, tối ưu hóa g_i trong bối cảnh thưa có thể khó khăn.

## 3 Phương pháp
Trong bài báo này, chúng tôi đề xuất một cơ chế gating đơn giản đặc biệt hiệu quả vì chỉ có một expert hoạt động, và nó không có tham số mạng định tuyến để học. Công trình gần đây [11,8,10] phải học các tham số xác định việc định tuyến đến các expert modules dựa trên các trạng thái ẩn, phải được tối ưu hóa cùng với chính các trọng số expert. Điều này có thể gây khó khăn vì trong quá trình huấn luyện thành viên cho mỗi expert đang thay đổi trong khi nó đang cố gắng học ánh xạ cho những thành viên đó. Thay vào đó, chúng tôi ủng hộ một ánh xạ cố định đến các experts. Cụ thể, bằng cách hash các tokens thành một số lượng buckets cố định, mỗi bucket tương ứng với một expert:

h^l_t = FFN_{hash(x_t)}(h^l_t); t = 1, : : : , T. (4)

Trong khi FFN vẫn nhận trạng thái ẩn h^l_t làm đầu vào, hàm định tuyến của chúng tôi sử dụng token đầu vào gốc x_t thay vì trạng thái ẩn, xem Hình 1 để mô tả đồ họa. Chúng tôi tự do lựa chọn từ các hàm hash khả dĩ khác nhau, mà chúng tôi sẽ xem xét dưới đây. Tuy nhiên, cho mục đích huấn luyện, hàm hash được cố định trước, và theo cách này, cơ chế định tuyến của chúng tôi không yêu cầu huấn luyện và không có tham số điều chỉnh.

### 3.1 Hàm Hash
Các hàm hash đã được sử dụng từ lâu trong khắp Khoa học Máy tính [13], và có thể có nhiều dạng khác nhau. Trong công trình của chúng tôi, chúng tôi thường sử dụng các hàm hash được tính toán trước, sử dụng bảng tra cứu trong quá trình học - được tính toán trước - để ánh xạ tokens đến các expert modules.

Chúng tôi xem xét một số loại hàm hash như các lựa chọn khả dĩ để định tuyến tokens đến các expert modules. Đơn giản nhất là Random Hash, trong đó chúng tôi gán mỗi token cho một expert cố định, ngẫu nhiên tại khởi tạo. Do phân phối Zipfian của tần suất token, điều này tự nhiên tạo ra sự mất cân bằng giữa các expert modules khác nhau. Vì việc cân bằng đã được chỉ ra trước đây là quan trọng để huấn luyện các mô hình MoE [8,10], chúng tôi cũng xem xét Balanced assignment. Trong phương pháp này, chúng tôi xây dựng bảng tra cứu trước khi huấn luyện mô hình sử dụng phân phối dữ liệu huấn luyện bằng cách gán một cách tham lam các tokens thường xuyên nhất vào các buckets trống nhất. Cấu trúc gán kết quả cân bằng hơn đáng kể so với Random Hashing, nhưng không hoàn hảo, vì tần suất của một số tokens vượt quá phân phối lý tưởng.

Random và Balanced hashing khai thác inductive bias của các mô hình auto-regressive và hash trên input token, nhưng chúng tôi cũng xem xét các khả năng khác: Bigram Hash sử dụng token hiện tại và trước đó (x_{t-1}, x_t) thay vì chỉ token hiện tại, trong khi Previous Token Hash sử dụng token trước đó x_{t-1}, bỏ qua đầu vào hiện tại. Chúng tôi cũng xem xét một kiểm tra thực tế hash dựa trên Position trong chuỗi, mà chúng tôi mong đợi sẽ có ít tác động, vì các vị trí tuyệt đối mang ít thông tin trong ngôn ngữ tự nhiên. Mỗi hàm hash này được sử dụng để đánh giá giá trị của thông tin được định tuyến trong phân tích thí nghiệm tiếp theo của chúng tôi.

Như một baseline trên, chúng tôi cũng đánh giá sử dụng Oracle Future Hash, hash dựa trên output token x_{t+1}, thay vì input token. Oracle Hash này kiểm tra mức độ mạnh mẽ của các quyết định định tuyến trong việc giải quyết một tác vụ. Tương tự, chúng tôi cũng xem xét Predicted Future Token Hash, sử dụng một Transformer baseline để đưa ra dự đoán về output token, và sau đó hash trên dự đoán này.

Clustered Hashes Dựa trên trực giác rằng các tokens tương tự có thể muốn được định tuyến đến cùng một expert, chúng tôi cũng thí nghiệm với Clustered Hashes. Chúng tôi có được các clusters bằng cách thực hiện k-means clustering với số lượng clusters cố định sử dụng token embeddings từ một mô hình Transformer baseline. Mỗi expert được gán một centroid, và các tokens được gán cho cluster gần nhất của chúng.

Dispersed Hashes Chúng tôi cũng xem xét giả thuyết ngược lại: rằng các tokens tương tự nên được đặt ở các buckets khác nhau, nơi giả định là các tokens rất tương tự cần các phân biệt tinh tế đòi hỏi dung lượng mô hình nhiều hơn (do đó gán cho các experts khác nhau). Để làm điều này, chúng tôi sử dụng cùng các k-means clusters như trước, nhưng phân phối tất cả các tokens trong mỗi cluster đều nhau qua tất cả các buckets.

### 3.2 MultiHash Layers
Trong phương pháp FFN MoE tiêu chuẩn, tất cả K expert modules có các tham số độc lập, nhưng ở đây chúng tôi xem xét một tùy chọn khác. Biết rằng trong tài liệu hashing, nhiều hashes có thể cung cấp các phân bổ tốt hơn trong nhiều ngữ cảnh [14]. Chúng tôi xem xét các sơ đồ như vậy trong ngữ cảnh định tuyến thưa. Hãy giả sử chúng ta được cho N hàm hashing khác nhau, và cho một input token x chúng ta tính toán các hashes này, ký hiệu là k_m = hash_m(x); m = 1, : : : , N. Giả sử expert FFN thông thường là một hàm B(relu(A(h))) nơi A: R^d → R^D và B: R^D → R^d, chúng tôi chia các linear layers thành N segments, A_m: R^d → R^{D/N} và B_m: R^D → R^{d/N}. Sau đó chúng tôi tính toán:

v = relu([A_{k_1}(h); : : : ; A_{k_N}(h)]) FFN^{MH}(h) = [B_{k_1}(v); : : : ; B_{k_N}(v)].

Tức là, sử dụng hashing để chọn các tham số chúng ta sẽ sử dụng cho mỗi segment, và sau đó nối chúng lại với nhau. Lợi thế là bây giờ chúng ta không còn phụ thuộc vào chất lượng của một hàm hash duy nhất, mà có nhiều cơ hội để tạo ra các phân vùng chất lượng tốt. Điều này có lẽ cũng có thể được coi là tương tự với quá trình multi-head attention đã được sử dụng trong Transformers.

## 4 Công trình liên quan
Các mô hình MoE thưa, nơi chỉ một vài expert modules hoạt động cho bất kỳ đầu vào nào, đặc biệt trong ngữ cảnh NLP, đã được nghiên cứu gần đây trong [6,11]. Trong các công trình này, gating được học thông qua backpropagation, có lẽ với một regularizer để khuyến khích cân bằng tải giữa các experts. [8] cho thấy rằng các mô hình trong [11] có thể được huấn luyện thành công với mỗi đầu vào được gán cho chính xác một expert. Một phương pháp khác như vậy cho Transformers, nơi việc định tuyến được học thông qua giải quyết bài toán gán tuyến tính, được nghiên cứu trong [10]. [?] sử dụng một phương pháp khác, nơi product keys cho phép tìm kiếm nearest neighbor để chọn tham số. Tổng quát hơn, việc sử dụng MoE để trade-off thời gian tính toán (với chi phí phân mảnh dữ liệu có thể) có lịch sử lâu dài, xem ví dụ [3, 7].

Phương pháp trong công trình này khác với tất cả những phương pháp này ở chỗ các assignments không sử dụng học tập gì cả, và thay vào đó tận dụng các inductive biases có thể trong bối cảnh ngôn ngữ tự nhiên. Đặc biệt, chúng tôi sử dụng thực tế rằng n-grams bản thân chúng là các language models tốt [15]. Do đó công trình này liên quan đến công trình trước đây cố gắng kết hợp các neural và n-gram language models [16, 17, 18, 19, 20, 21].

Công trình của chúng tôi cũng liên quan đến feature hashing trong các linear models và kernel methods [22,23], nơi các word hoặc n-gram features được hash để cung cấp một không gian đặc trưng chiều thấp mới. [22] chỉ ra rằng khi thực hiện feature hashing như vậy, sự tương tác giữa các random subspaces là không đáng kể với xác suất cao. [?] sử dụng hashing để nén các neural networks, thay vì tăng tham số của chúng như chúng tôi làm ở đây. Công trình về long-context Transformers gần đây đã sử dụng các kỹ thuật hashing để tăng tốc truy cập đến lịch sử token tầm xa thông qua các mẫu self-attention thưa, đặc biệt trong Routing Transformers [24] và Reformer [25]. Ngược lại, công trình của chúng tôi sử dụng hashing để truy cập một tập lớn các tham số thông qua định tuyến thưa, thay vì truy cập thưa đến các input features.

## 5 Thí nghiệm

### 5.1 Tác vụ
Pushshift.io Reddit Chúng tôi sử dụng một biến thể của các thảo luận Reddit, cũng đã được sử dụng trong một số nghiên cứu hiện có, xem ví dụ [26,27,28,29]. Theo [30], chúng tôi sử dụng một dataset Reddit đã tồn tại trước đó được trích xuất và thu thập bởi bên thứ ba và được cung cấp trên pushshift.io [31], huấn luyện để tạo ra một comment có điều kiện trên toàn bộ thread dẫn đến comment, trải dài 1.5B ví dụ huấn luyện. Chúng tôi sử dụng cùng từ điển BPE như [32], bao gồm 8008 tokens.

RoBERTa+cc100en Data Chúng tôi sử dụng cùng dữ liệu được sử dụng để huấn luyện BASE [10], bao gồm khoảng 100B tokens, kết hợp các corpora được sử dụng trong RoBERTa [33] với tập con tiếng Anh của corpus CC100 [34]. Từ điển GPT2, có kích thước 51200, được sử dụng để tokenization. Cho các thí nghiệm seq2seq của chúng tôi, chúng tôi sắp xếp dữ liệu này chia theo câu để dự đoán turn tiếp theo. Chúng tôi coi nó như tác vụ mô hình hóa ngôn ngữ ban đầu trong các thí nghiệm so sánh với BASE [10].

Wikitext-103 Wikitext-103 là một benchmark mô hình hóa ngôn ngữ nhỏ hơn [35] bao gồm một bộ sưu tập các bài viết Wikipedia hơn 100 triệu tokens, và một kích thước từ vựng cố định 270K tokens được cung cấp. Chúng tôi xem điều này như một tác vụ seq2seq trong các thí nghiệm của chúng tôi, một lần nữa chia theo câu.

Downstream BST tasks Cuối cùng, chúng tôi sử dụng các tác vụ đối thoại Blended Skill Talk (BST) được sử dụng trong [32] sau pre-training pushshift.io Reddit để đánh giá hiệu suất fine-tuning của các mô hình dense vs. sparse.

### 5.2 Thiết lập thí nghiệm
Seq2Seq Setup Phần lớn các thí nghiệm của chúng tôi được thực hiện trong nền tảng ParlAI¹ sử dụng một framework Transformer encoder-decoder. Chúng tôi đầu tiên huấn luyện một số Transformers tiêu chuẩn (dense), với

¹http://parl.ai

2 encoder layers và 11 hoặc 22 decoder layers, theo cấu trúc trong [32] để huấn luyện trên pushshift.io Reddit. Chúng tôi gọi cái có 11 layers và embedding size d = 1024 và FFN hidden layer size D = 4096 là Baseline Transformer của chúng tôi. Chúng tôi cũng huấn luyện một mô hình "Wider" với D = 6144, và một mô hình "Deeper" với 22 decoder layers, và D = 4096. Mô hình Baseline có 222M tham số, và "Wider" và "Deeper" được chọn để cả hai đều có 755M tham số mỗi cái. Các mô hình này được so sánh với các phương pháp Hash Layer được chi tiết trong section 3 và với Switch Transformers có cùng kích thước và cài đặt. Load balancing cho Switch được tối ưu hóa trên validation set. Cho cả Hash và Switch, chúng tôi sử dụng kích thước Transformer "Baseline" được chi tiết ở trên như kiến trúc mà chúng tôi thêm các sparse routing layers bằng cách thay thế một hoặc nhiều dense layers gốc. Tất cả các thí nghiệm được chạy cho 100k updates; một bảng hyperparameters được cung cấp trong subsection B.1.

BASE Comparison Trong khi phần lớn phân tích của chúng tôi diễn ra trong thiết lập được mô tả ở trên với các mô hình lên đến 1.28B tham số, để kiểm tra các phương pháp của chúng tôi ở quy mô trên các mô hình sparse lớn hơn, chúng tôi áp dụng thiết lập BASE Layer [10] và code base² thay vào đó nơi chúng tôi so sánh các mô hình Hash và BASE Layer 4.5B tham số. Thiết lập này sử dụng các pure language models thay vì thiết lập Seq2Seq ở trên. Chúng tôi sử dụng kiến trúc, dữ liệu (RoBERTa+cc100en), và hyperparameters trực tiếp từ [10], sử dụng hoặc một single sparse routing layer bao gồm 3 stacked FFNs (D = 8192) trên middle layer của một mạng 25 layer, hoặc 3 routing layers được phân bố đều trong mạng. Để so sánh trực tiếp với BASE, chúng tôi giữ tất cả hyperparameters cố định và chỉ thay đổi phương pháp định tuyến; chúng tôi sử dụng balanced assignment Hash Layer trong trường hợp này. Chúng tôi huấn luyện cho đến khi đạt được 40k steps. Một bảng hyperparameters được cung cấp trong subsection B.2.

²Được cung cấp trong Fairseq [36].

### 5.3 Kết quả và Phân tích

#### 5.3.1 So sánh giữa Hash, Switch và Dense models
Hash vs. Switch routing trên một layer duy nhất Chúng tôi đầu tiên so sánh một Hash layer (với balanced hash) với một Switch layer, trên một Transformer dense khác, nơi sparse routing được thực hiện trên layer 7 của decoder. Cả hai phương pháp sử dụng 64 expert FFNs với tổng cộng 751M tham số. Kết quả trên pushshift.io Reddit được đưa ra trong Bảng 1 (hàng 4 và 5) và trên dữ liệu RoBERTa+cc100en trong Bảng 2 (hàng 2 và 3). Chúng tôi thấy Hash Layers vượt trội hơn Switch trên cả hai datasets khoảng 0.4-0.5 perplexity.

Dense vs. Sparse Models Cả mô hình Hash và Switch sparse đều vượt trội hơn dense Baseline (222M tham số) mà chúng dựa trên, cũng như Wider Transformer (755M tham số). Tuy nhiên, Deeper Transformer (755M tham số) vượt trội hơn các mô hình sparse có số lượng tham số tương tự. Tuy nhiên, chúng tôi lưu ý rằng do tính dense thay vì conditional compute, nó chậm hơn về tốc độ inference. Chúng tôi thấy đây là xu hướng chung: các mô hình dense tốt có thể có được nhiều sức mạnh hơn từ cùng số lượng tham số so với các mô hình sparse. Tuy nhiên, các mô hình sparse, mặc dù lãng phí hơn về bộ nhớ, cho perplexity tốt hơn với cùng tốc độ (tức là, chúng ta nên so sánh với Baseline Transformer trong trường hợp này, có khoảng cùng lượng tính toán).

Hash layer module size Chúng tôi tiến hành cùng các thí nghiệm pushshift.io Reddit như trên, nhưng thay đổi số lượng expert modules trong cả Hash và Switch. Tăng từ 64 lên 128 modules (tổng cộng 1.28B tham số) thấy một cải thiện thậm chí lớn hơn của Hash so với Switch (khoảng 0.6 perplexity), xem Bảng 1 (hàng 6 và 7), và Hình 2 (trái). Thử các số lượng modules nhỏ hơn, 16 và 32, và vẽ tất cả kết quả trong Hình 3 (trái) chúng tôi thấy rằng với số lượng modules nhỏ, Hash và Switch hoạt động tương tự, nhưng khoảng cách trở nên lớn hơn khi số lượng modules tăng. Với số lượng modules nhỏ, chúng tôi giả thuyết rằng học để định tuyến, như Switch làm, sẽ quan trọng hơn để hoạt động hiệu quả với những lựa chọn đó, nhưng với số lượng modules lớn hơn nhiều lựa chọn định tuyến có thể hoạt động. Do đó, Hash layers có thể hoạt động tốt trong bối cảnh đó, và học để định tuyến trở nên ít quan trọng hơn.

Hash layer position Chúng tôi cũng thí nghiệm để tìm vị trí tốt nhất theo layer cho sparse routing diễn ra. Trong Hình 3 (phải) chúng tôi vẽ perplexity cho 64 module Hash Layer, đặt trên các layers khác nhau của decoder. Chúng tôi thấy rằng các layers sau hoạt động tốt hơn, nhưng thậm chí lựa chọn hoạt động tệ nhất (layer 1) vẫn hoạt động tốt so với các baselines khác: tốt như Switch Transformers sử dụng các layers sau thực tế. Chúng tôi lưu ý rằng phân tích BASE Layers [10] cho thấy xu hướng tương tự rằng các layers sau hoạt động tốt. Giả thuyết rằng conditional compute cho khả năng tạo ra các chuyên môn hóa tinh tế, điều đó theo logic rằng đáng để tạo ra những phân biệt đó sau khi các đặc trưng rõ ràng hơn đã được trích xuất trước. Chúng tôi sẽ quay lại lập luận này trong các thí nghiệm sau.

Multi-layer routing Chúng tôi đánh giá việc đặt sparse routing mỗi layer khác, 16 modules khác nhau mỗi cái trong Bảng 1 (hàng 8-10). Switch và Hash hoạt động tương tự trong bối cảnh này, với Switch vượt trội với lựa chọn tối ưu của 0.1 load balancing (23.00 vs. 23.21), và cùng hiệu suất (23.19) cho tham số balancing 0.01. Cho các kết quả của Hình 3 (trái), số lượng modules nhỏ trong trường hợp này có thể làm hiệu suất gần nhau.

Downstream fine-tuning Chúng tôi so sánh một số mô hình pushshift.io Reddit cho mục tiêu fine-tuning trên các tác vụ downstream. Chúng tôi thí nghiệm với hoặc fine-tuning toàn bộ mô hình, hoặc đóng băng một số phần của mô hình trong quá trình fine-tuning, cũng như thay đổi load balancing cho Switch tại thời gian fine-tune. Kết quả được đưa ra trong Phụ lục A. Chúng tôi thấy rằng các kết quả fine-tune thường đồng ý với hiệu suất gốc trên tác vụ pre-training pushshift.io Reddit, và thứ tự các phương pháp được giữ nguyên. Hash vượt trội hơn Switch một chút, cả hai đều vượt trội hơn mô hình Baseline, và các mô hình dense lớn hơn hoạt động tốt hơn, như mong đợi. Đóng băng các phần của mô hình thường làm tổn hại fine-tuning, trừ khi phần được đóng băng là phần sparse của mô hình. Có vẻ trong trường hợp đó chỉ fine-tuning các phần dense của mô hình là đủ để có hiệu suất tốt. Chỉ tuning phần sparse của mô hình, mặt khác, làm tổn hại hiệu suất, có lẽ vì phần lớn dung lượng của mô hình nằm ở đó.

#### 5.3.2 Phân tích hàm Hash
Chúng tôi đánh giá các lựa chọn khác nhau của hàm hashing được chi tiết trong subsection 3.1. Kết quả tổng thể được đưa ra trong Bảng 3 trên dataset pushshift.io Reddit sử dụng 64 module Hash Layer.

Random và Balanced Hash Functions Chúng tôi thấy rằng fixed random assignment (hàng 3) và balanced assignment (hàng 2) hoạt động tương tự tốt về perplexity (23.22 vs. 23.16 valid perplexity). Tuy nhiên, balanced assignment, như tên gọi cho thấy, cân bằng hơn, xem Hình 4, có thể làm cho nó hiệu quả hơn về các sơ đồ huấn luyện phân tán.

Clustering Hash Functions Thú vị, việc sử dụng cluster based hashes ("Token clustering", hàng 4) hoạt động rõ ràng tệ hơn so với randomized hashes (23.90 vs. 23.22). Chúng tôi giả thuyết rằng nếu mục tiêu của conditional computation là tạo ra các phân biệt tinh tế, thì những phân biệt đó có nhiều khả năng xuất hiện giữa các tokens trong cùng cluster, do đó chúng nên ở các hashes khác nhau (các phần của biểu đồ tính toán), không phải cùng một cái. Chúng tôi cung cấp bằng chứng một phần cho điều này bằng cách hash trong các token clusters thay vào đó ("Dispersed Hash", hàng 5), điều này khôi phục hiệu suất tương tự với random hashes (23.17 vs. 23.22). Chúng tôi lưu ý rằng các phương pháp learn-to-route như Switch Transformers và BASE sử dụng các hàm đơn giản của trạng thái ẩn để thực hiện định tuyến, thường cung cấp các expert modules được cluster [10], do đó có thể là một bất lợi cho những phương pháp đó.

Position-based Hash Function Chúng tôi tiến hành thí nghiệm hash chỉ dựa trên vị trí chuỗi. Chúng tôi coi thí nghiệm này như một kiểm tra thực tế, chúng tôi không mong đợi việc chọn conditional compute dựa trên vị trí trong chuỗi đầu ra sẽ giúp ích. Thực sự, hóa ra điều này không tốt hơn baseline Transformer dense. Do đó có vẻ như việc định tuyến dựa trên nội dung đầu vào quan trọng hơn nhiều.

Bigram Hash Function Hashing dựa trên hai tokens cuối (bigrams) hoạt động tệ hơn so với chỉ sử dụng token cuối (24.19 vs. 23.16). Chúng tôi giả thuyết có hai lý do cho điều này: (1) trước tiên, token cuối rõ ràng là quan trọng nhất, và bigrams thêm một đặc trưng ít liên quan hơn; (2) điều này tạo ra quá nhiều hashes, hoạt động kém hơn. Các thí nghiệm tiếp theo sẽ giúp kiểm tra những tuyên bố này.

Previous Token Hashing Hashing dựa trên token trước đó rõ ràng tệ hơn so với sử dụng token hiện tại (24.16 vs. 23.16), và cho hiệu suất tương tự với việc sử dụng bigrams, giúp xác nhận phần đầu của giả thuyết bigram ở trên.

Dictionary size Chúng tôi thực hiện thí nghiệm trên Wikitext-103 trong hai bối cảnh: sử dụng từ điển đã cho của 267k tokens, hoặc sử dụng từ điển 8k chúng tôi sử dụng trong các thí nghiệm pushshift.io Reddit, theo [32]. Kết quả, so sánh với Switch và một baseline Transformer, được đưa ra trong Bảng 4. Chúng tôi thấy rằng Hash hoạt động tốt cho từ điển nhỏ, vượt trội hơn Switch một chút. Tuy nhiên, trên từ điển lớn hơn, nó hoạt động tệ hơn Switch. Vì đây là cùng dữ liệu nhưng chỉ tokenization đã thay đổi, chúng tôi kết luận rằng hashing được tạo ra từ từ điển nhỏ hơn dễ học hơn, giúp xác nhận phần thứ hai của giả thuyết bigram ở trên.

Oracle Future Token Hashing Chúng tôi đánh giá hashing sử dụng oracle next token được dự đoán. Điều này mang lại perplexity 1.9. Sử dụng thông tin oracle chỉ để chọn giữa các modules đủ để về cơ bản giải quyết một tác vụ.

Predicted Future Token Hashing Kết quả cuối đặt ra câu hỏi: nếu chúng ta có thể dự đoán token tiếp theo, và hash dựa trên dự đoán đó thay vào đó – liệu nó có tốt hơn hash trên token hiện tại không? Do đó chúng tôi đã thử hash sử dụng Baseline Transformer để dự đoán labels, mang lại perplexity 25.02 – thực tế không đánh bại chính Baseline. Có vẻ như bias của các dự đoán token hạn chế khả năng của sparse routing để cải thiện.

Multi-hashing Chúng tôi đánh giá kỹ thuật multi-hashing được mô tả trong subsection 3.2. Kết quả được đưa ra trong Phụ lục A, so sánh với Switch và standard hashing. Mặc dù cùng số lượng tham số được sử dụng trong tất cả các trường hợp, chúng tôi thấy cải thiện khi chia hash thành 2, 4 hoặc 8 hashes khác nhau so với một hash duy nhất, với kết quả cải thiện đều đặn cho cả 16 hoặc 32 modules.

#### 5.3.3 Phân tích Switch Transformer
Switch load balancing Chúng tôi chỉ ra hiệu suất của Switch cho các giá trị khác nhau của tham số load balancing trên pushshift.io Reddit trong Phụ lục A. Rõ ràng việc lựa chọn tham số quan trọng, với kết quả thay đổi trong khoảng 1 perplexity point.

Switch với Token-based Routing Cho phân tích oracle và predicted token hashing của chúng tôi trong subsection 5.3.2, chúng tôi giả thuyết rằng các biểu diễn ẩn trong các layers của Transformer, bị bias hướng về các dự đoán của mô hình, có thể không tối ưu cho định tuyến. Do đó chúng tôi thí nghiệm với một hybrid giữa Switch và Hash Layers: trên sparse layer, thay vì sử dụng hidden state như Switch router input, chúng tôi sử dụng token hiện tại thay vào đó. Để chuyển đổi token thành vector, chúng tôi sử dụng một lookup table bổ sung, tức là, một tập các tham số có thể học được có kích thước bằng từ điển. Các tham số này độc lập với hidden state và chỉ được router sử dụng để học route tốt nhất. Kết quả được đưa ra trong Bảng 6. Chúng tôi thấy điều này mang lại một số cải thiện nhỏ cho Switch với 64 và 128 modules trên một layer duy nhất, khẳng định tính hữu ích của token-based routing.

#### 5.3.4 So sánh với BASE Layers
Tiếp theo chúng tôi so sánh với BASE Layers. Sử dụng code base BASE Layer, chúng tôi thực hiện Hash Layers trong chính xác cùng thiết lập, chỉ thay đổi phương pháp định tuyến, và để nguyên mọi thứ khác. Hình 2 (phải) cho thấy kết quả so sánh Hash với BASE cho các mô hình 4.5B tham số. Qua toàn bộ run, chúng tôi thấy rằng Hash vượt trội hơn BASE tại mỗi training step. Trong các phần đầu của training, Hash có lẽ sẽ có lợi thế trong việc có thể chuyên môn hóa các expert modules sớm hơn, trong khi BASE phải học membership cho mỗi expert modules. Sau trong training, BASE trở nên hơi không ổn định có lẽ khi các expert assignments thay đổi, trong khi hiệu suất Hash tiếp tục cải thiện một cách mượt mà. Ngoài ra, để chứng minh Hash Layers vẫn hiệu quả khi được xếp chồng, chúng tôi huấn luyện một mô hình với 3 Hash Layers (sử dụng random hashes), nhưng ít tham số hơn cho mỗi expert module để tổng tham số vẫn không đổi ở 4.5B (xem subsection B.2). Chúng tôi thấy rằng việc sử dụng nhiều Hash Layers mang lại cải thiện nhỏ nhưng nhất quán, cho thấy Hash Layers sẽ hiệu quả ở độ sâu thậm chí nhiều hơn.

Ngoài các cải thiện hiệu suất so với BASE, chúng tôi cũng thấy rằng Hash Layers hiệu quả hơn trong tổng tính toán. Đặc biệt, BASE yêu cầu hai all-to-all communications: đầu tiên de-correlates batches để làm cho assignment balancing ngẫu nhiên hơn, và thứ hai routes states đến expert được gán của chúng. Vì Hash Layers sử dụng các assignments cố định, được tính toán trước, chúng tránh bước de-correlation. Trong thực tế, chúng tôi thấy điều này mang lại cải thiện khoảng 11% trong updates-per-second. Khi số lượng expert layers tăng, sự khác biệt này sẽ trở nên cường điệu hơn.

## 6 Kết luận
Chúng tôi đã giới thiệu một phương pháp đơn giản và hiệu quả cho các mô hình sparse trong bối cảnh Transformers-for-NLP dựa trên hash layers. Chúng tôi chỉ ra trên nhiều datasets khác nhau và với phân tích trong các bối cảnh khác nhau rằng phương pháp này có tính cạnh tranh cao với các phương pháp hiện có như Switch Transformers và BASE Layers, trong khi mạnh mẽ và đơn giản hơn nhiều – không yêu cầu tham số học bổ sung, thuật toán assignment hoặc thay đổi hàm mục tiêu. Cho rằng các nhà nghiên cứu thường chỉ có một cơ hội để huấn luyện các mô hình rất lớn, điều này làm cho phương pháp của chúng tôi trở thành một ứng viên mạnh mẽ cho các runs như vậy.

Trong khi các thí nghiệm của chúng tôi mở rộng lên đến 4.5B tham số, chúng tôi không đạt đến quy mô của các công trình công nghiệp lớn như [8], và chúng tôi hy vọng thấy công trình tương lai tiến hành các thí nghiệm như vậy. Cuối cùng, cho rằng phương pháp định tuyến của chúng tôi không có học tập, kết quả của chúng tôi có lẽ gợi ý rằng không có phương pháp hiện tại nào đang định tuyến đặc biệt tốt. Do đó chúng tôi tin rằng learning-to-route nên tiếp tục là nghiên cứu của công trình tương lai, và coi công trình của chúng tôi là một baseline mạnh mẽ cho nghiên cứu như vậy.

[Phần References, Additional Results, Hyperparameters, Computational Resources và Societal Impact sẽ được dịch tương tự với cùng cấu trúc và chi tiết]
