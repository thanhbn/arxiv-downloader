# 2310.01334.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2310.01334.pdf
# Kích thước tệp: 1206255 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
MERGE , T HEN COMPRESS : D EMYSTIFY EFFICIENT
SM OEWITH HINTS FROM ITSROUTING POLICY
Pingzhi Li1Zhenyu Zhang2Prateek Yadav1Yi-Lin Sung1Yu Cheng3
Mohit Bansal1Tianlong Chen1,4,5
1The University of North Carolina at Chapel Hill2The University of Texas at Austin
3The Chinese University of Hong Kong4MIT5Harvard University
{pingzhi,praty,ylsung,mbansal,tianlong }@cs.unc.edu
zhenyu.zhang@utexas.edu chengyu@cse.cuhk.edu.hk
TÓM TẮT
Mixture-of-Experts (SMoE) được kích hoạt thưa thớt đã cho thấy tiềm năng mở rộng khả năng học tập của các mạng nơ-ron, tuy nhiên, chúng có các vấn đề như: ( a)Sử dụng bộ nhớ cao, do sự sao chép các lớp mạng thành nhiều bản sao như các chuyên gia; và ( b)Dư thừa trong các chuyên gia, do các chính sách định tuyến dựa trên học tập phổ biến gặp phải vấn đề sụp đổ đại diện. Do đó, các mô hình SMoE thông thường không hiệu quả về bộ nhớ và không thể mở rộng, đặc biệt cho các kịch bản downstream bị hạn chế tài nguyên. Trong bài báo này, chúng tôi đặt câu hỏi: Liệu chúng ta có thể tạo ra một mô hình SMoE nhỏ gọn bằng cách hợp nhất thông tin chuyên gia không? Công thức tốt nhất để hợp nhất nhiều chuyên gia thành ít chuyên gia hơn nhưng có kiến thức nhiều hơn là gì? Điều tra ban đầu của chúng tôi tiết lộ rằng các phương pháp hợp nhất mô hình thông thường không hiệu quả trong việc hợp nhất chuyên gia như vậy cho SMoE. Các lý do tiềm năng là: ( 1) thông tin dư thừa che lấp các chuyên gia quan trọng; ( 2) thiếu hoán vị nơ-ron thích hợp cho mỗi chuyên gia để đưa tất cả chúng vào sự căn chỉnh. Để giải quyết những thách thức này, chúng tôi đề xuất một thuật toán hợp nhất mới cho SMoE, tức là,M-SMoE , tận dụng thống kê định tuyến để hướng dẫn việc hợp nhất chuyên gia. Cụ thể, nó bắt đầu với việc căn chỉnh hoán vị nơ-ron cho các chuyên gia; sau đó, các chuyên gia thống trị và "thành viên nhóm" của họ được hình thành dựa trên các chính sách định tuyến; cuối cùng, mọi nhóm chuyên gia được hợp nhất thành một chuyên gia duy nhất bằng cách sử dụng tần suất kích hoạt của mỗi chuyên gia làm trọng số cho việc hợp nhất, do đó làm giảm tác động của các chuyên gia không đáng kể. Hơn nữa, chúng tôi rút ra một quan sát thú vị rằng việc hợp nhất được đề xuất của chúng tôi thúc đẩy một chiều thấp trong không gian trọng số của chuyên gia đã hợp nhất, tự nhiên mở đường cho việc nén bổ sung. Do đó, phương pháp cuối cùng của chúng tôi, MC-SMoE (tức là, Merge, then Compress SMoE), tiếp tục phân tách các chuyên gia đã hợp nhất thành các lựa chọn thay thế có hạng thấp và thưa thớt cấu trúc. Các thí nghiệm rộng rãi trên 8 bộ dữ liệu chuẩn xác nhận hiệu quả của các đề xuất của chúng tôi. Ví dụ, MC-SMoE của chúng tôi đạt được đến 80% bộ nhớ và giảm 20% FLOPs, với hầu như không có sự mất mát về hiệu suất.1
1 GIỚI THIỆU
0.5 1.0 1.5 2.0
Kích thước mô hình (B)556065Độ chính xác (%)MC-SMoE
M-SMoE
SMoE
Baselines
Hình 1: Độ chính xác ( %) trên
COPA với switch-base-32 SMoE.
MC-SMoE đạt đến 80% tiết kiệm
bộ nhớ với chỉ một sự thỏa hiệp
không đáng kể về hiệu suất.Transformers (Vaswani et al., 2023) đã trở thành kiến trúc mạng
de facto trong các kịch bản xử lý ngôn ngữ tự nhiên (NLP) khác nhau
(Devlin et al., 2019; Yang et al., 2019; Liu
et al., 2019; Raffel et al., 2020; Fedus et al., 2022; Wei et al.,
2022), và thậm chí cho các ứng dụng thị giác máy tính (Dosovitskiy
et al., 2021; Touvron et al., 2021; Mao et al., 2022; Zheng
et al., 2021; Liu et al., 2021). Ngày nay, số lượng tham số
của các mô hình như vậy thường được đo bằng tỷ hơn là
triệu. Điều này chủ yếu là do một số quy luật mở rộng thực nghiệm
nhất định (Kaplan et al., 2020) tiết lộ mối quan hệ luật lũy thừa giữa
chất lượng mô hình cuối cùng và lượng {dữ liệu, khả năng mô hình,
và thời gian tính toán }. Thật không may, điều này đặt ra các yêu cầu
không khả thi cho tài nguyên tính toán, ví dụ, huấn luyện một
mô hình dựa trên GPT (Brown et al., 2020) thường dẫn đến hàng nghìn
1Mã nguồn của chúng tôi được cung cấp tại https://github.com/UNITES-Lab/MC-SMoE.
1arXiv:2310.01334v2  [cs.LG]  14 Mar 2024

--- TRANG 2 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
ngày GPU. Sparse Mixture-of-Experts (SMoE) (Shazeer et al., 2017) sau đó được đề xuất
để cắt giảm chi phí tính toán trong khi cho phép mở rộng hiệu quả khả năng mạng. Đối với
dự đoán của một đầu vào cho trước, nó tận dụng tính toán có điều kiện phụ thuộc vào đầu vào để kích hoạt thưa thớt
(tức là, định tuyến) các phần mô hình có liên quan ( tức là, chuyên gia). Do đó, số lượng tham số/khả năng
mạng có thể được khuếch đại với chi phí huấn luyện bổ sung tối thiểu. Ví dụ, Fedus et al. (2022) mở rộng
mô hình T5-Base (Raffel et al., 2020) dense thành mô hình Switch-Base SMoE lớn hơn 35×, với
khoảng cùng FLOPS huấn luyện.
Tuy nhiên, một số hạn chế quan trọng vẫn tồn tại trong SMoE để mở rộng khả năng của các mô hình ngôn ngữ lớn.
Thứ nhất, SMoE đánh đổi không gian lấy FLOPs2, điều này giới thiệu chi phí bộ nhớ đáng kể
và hạn chế việc sử dụng thực tế của nó trong các nền tảng hạn chế tài nguyên thế giới thực, đặc biệt cho
triển khai và suy luận downstream. Thứ hai, SMoE có sự sử dụng kém khả năng của nó . Chính sách
định tuyến dựa trên học tập phổ biến trong SMoE gặp phải các vấn đề sụp đổ đại diện, vì
nó khuyến khích các embedding token được nhóm xung quanh các centroid chuyên gia (Chi et al., 2022) và
dẫn đến các chuyên gia dư thừa (Mittal et al., 2022; Chen et al., 2022). Một nghiên cứu gần đây (Chen et al.,
2023) cũng chỉ ra một quan sát tương tự rằng "khả năng hiệu quả" trong các SMoE thông thường là
thấp. Để giải quyết những nhược điểm này và phát huy hết sức mạnh của SMoE, một giải pháp có thể là
hợp nhất thông tin từ các chuyên gia không đáng kể, nhằm thiết lập một SMoE nhỏ gọn hơn mà không
làm tổn hại hiệu suất. Tuy nhiên, việc kết hợp naively các cơ chế hợp nhất mô hình hiện có dẫn đến
kết quả kém trong các kịch bản SMoE, như được chứng minh trong các nghiên cứu ban đầu của chúng tôi trong Phần 4.2.
Các lý do tiềm năng có thể là: ①Các chuyên gia quan trọng dễ bị che khuất bởi thông tin dư thừa
trong quá trình hợp nhất, ②Các chuyên gia thường được khởi tạo và huấn luyện cùng với các quỹ đạo tối ưu hóa
đa dạng, do đó một hoán vị chuyên gia có thể đóng vai trò quan trọng trong việc đưa chúng vào
sự căn chỉnh (Ainsworth et al., 2022). Những thách thức chính này thúc đẩy chúng tôi đặt câu hỏi:
(Q)Làm thế nào để hợp nhất hiệu quả các chuyên gia dư thừa của SMoE thành một số ít được chọn
mà không hy sinh kiến thức quan trọng?
Trong bài báo này, chúng tôi nghiên cứu có hệ thống câu hỏi nghiên cứu (Q) trên, và nhắm mục tiêu một SMoE
nhỏ gọn và chất lượng cao trên các kịch bản fine-tuning/suy luận downstream. Chúng tôi khám phá ra rằng
các chính sách định tuyến từ SMoE chứa các "manh mối" cho việc hợp nhất chuyên gia hiệu quả . Cụ thể, ( 1) tần suất
kích hoạt của các chuyên gia chỉ ra việc sử dụng của nó và có thể được coi là một proxy tuyệt vời cho
tầm quan trọng của nó. Nó cho phép một cách tự động để xác định bao nhiêu và chuyên gia nào nên được giữ lại
trong mỗi lớp SMoE; ( 2) Quyết định định tuyến đo lường mức độ tương tự của các chuyên gia với nhau,
về mặt liên quan đến các mẫu đầu vào cho trước. Nó giúp liên kết các chuyên gia dư thừa với
các chuyên gia thống trị khác nhau. Dựa trên những hiểu biết này, chúng tôi đề xuất một phương pháp M-SMoE mới cho
việc hợp nhất SMoE. Hơn nữa, chúng tôi thấy rằng các chuyên gia đã hợp nhất từ M-SMoE nằm trong một không gian
tham số chiều thấp, điều này dường như gợi ý rằng một việc hợp nhất thích hợp làm giảm các tín hiệu trọng số
nhiễu tiềm năng (Han et al., 2016). Chúng tôi sử dụng lợi ích bổ sung này của việc hợp nhất chuyên gia để thiết kế
phương pháp MC-SMoE (Merge, then Compress SMoE) của chúng tôi tích hợp một cách hữu cơ các kỹ thuật phân tách
hạng thấp để nén chuyên gia thêm. Các đóng góp chính của chúng tôi như sau:
• Chúng tôi đề xuất một khung mới MC-SMoE ,tức là, Merge, then Compress SMoE, cho hiệu quả SMoE
tại các kịch bản downstream, bao gồm fine-tuning và đánh giá zero-shot.
• Chúng tôi thiết kế một phương pháp hợp nhất sáng tạo ( M-SMoE ) dựa trên hướng dẫn từ
các chính sách định tuyến. Cụ thể, nó bắt đầu với một sự căn chỉnh hoán vị tùy chỉnh cho các chuyên gia, sau đó
xác định các chuyên gia thống trị toàn cầu cùng với "thành viên nhóm" của họ trong các
lớp SMoE, và kết luận với một trung bình có trọng số theo tần suất kích hoạt của họ.
• Chúng tôi quan sát thấy rằng các chuyên gia kết quả từ M-SMoE vốn dĩ thể hiện một chiều
trọng số thấp hơn . Hiện tượng thú vị này mở đường cho việc nén bổ sung, cho phép
phương pháp MC-SMoE của chúng tôi tiếp tục tăng cường hiệu quả bộ nhớ và tham số.
• Các thí nghiệm rộng rãi trên tám bộ dữ liệu chuẩn xác nhận hiệu quả của MC-SMoE của chúng tôi .
Một ví dụ được trình bày trong Hình 1. Đáng chú ý, M-SMoE mang lại đến 60% giảm
chi phí bộ nhớ với hiệu suất thậm chí còn được cải thiện một chút. MC-SMoE đạt được đến 80%
giảm bộ nhớ và 20% FLOPs, với chỉ những giảm hiệu suất cận biên.
2FLOPs có nghĩa là số phép toán dấu phẩy động trên giây. Lưu ý rằng thiết kế thông thường của SMoE không
nhất thiết mang lại lợi ích thời gian chạy. Thay vào đó, để giảm thiểu các chi phí độ trễ bổ sung từ định tuyến và
chuyên gia đa dạng, nó thường yêu cầu song song hóa chuyên môn (Rajbhandari et al., 2022; Fedus et al., 2022; He et al., 2021;
2022) và thiết kế phần cứng (Fan et al., 2022).
2

--- TRANG 3 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
......
Chính sách định tuyến 
...Chuyên gia 1 Chuyên gia 2 Chuyên gia N Chuyên gia k ... ...
Token
Embeddings 
(a) Định tuyến chuyên gia của SMoEChỉ số chuyên giaTần suất định tuyến
Nhóm
Chuyên gia thống trị
Chuyên gia không thống trị
......Lớp SMoE 1
Lớp SMoE 2
Lớp SMoE 3
Lớp SMoE M-1
Lớp SMoE M
Hợp nhất chuyên gia nhận thức tần suất
......Lớp hợp nhất 1
Lớp hợp nhất 2
Lớp hợp nhất 3
Lớp hợp nhất M-1
Lớp hợp nhất M(b) Gợi ý định tuyến của hợp nhất SMoEChuyên gia i Chuyên gia jChuyên gia hợp nhất k
(c) Hợp nhất khuyến khích chiều thấp Nén
Hình 2: Tổng quan về pipeline MC-SMoE được đề xuất của chúng tôi. ( a) Trong SMoE thông thường, mỗi token
embedding được hướng đến một số lượng nhỏ chuyên gia liên quan. ( b)Chính sách định tuyến truyền cảm hứng cho việc hợp nhất chuyên gia .
Qua tất cả các lớp SMoE, M-SMoE xác định các chuyên gia được kích hoạt thường xuyên nhất làm chuyên gia thống trị, nhóm
các chuyên gia không thống trị khác, và sau đó hợp nhất chúng trong mỗi nhóm theo cách có trọng số tần suất. ( c)
Sau khi hợp nhất, không gian trọng số của các chuyên gia kết quả có xu hướng thể hiện chiều thấp hơn, mở đường cho
việc nén bổ sung. Nó làm rõ thiết kế MC-SMoE của chúng tôi .
2 NGHIÊN CỨU LIÊN QUAN
Sparse Mixture-of-Experts (SMoE). Lợi ích của việc mở rộng quy mô mô hình được thừa nhận rộng rãi,
thường cung cấp khả năng học tập tăng cường và khả năng tổng quát hóa nâng cao (Brown et al., 2020;
Kaplan et al., 2020; Chung et al., 2022; Chowdhery et al., 2022). SMoE là một cách tiếp cận hiệu quả để
huấn luyện các mô hình lớn hơn với chi phí bổ sung không đáng kể, đã được nghiên cứu rộng rãi trong Shazeer
et al. (2017); Lepikhin et al. (2021); Fedus et al. (2022). Các mô hình SMoE kích hoạt các phần khác nhau của
mô hình cho các token đầu vào khác nhau thay vì sử dụng toàn bộ tham số mạng. Ví dụ,
GShard (Lepikhin et al., 2021), một mô hình SMoE mở rộng mô hình dựa trên Transformer từ 2B lên
600B tham số với chi phí huấn luyện thấp hơn mô hình dense 100B. Gần đây, Fedus et al.
(2022) tạo ra một mô hình SMoE dựa trên T5 (Raffel et al., 2020) với hàng nghìn tỷ tham số.
Mối quan tâm về hiệu quả trong SMoE và các giải pháp hiện có. Các mô hình SMoE yêu cầu bộ nhớ lớn để
chứa các chuyên gia, hơn nữa, nhiều chuyên gia có sử dụng thấp trong quá trình suy luận. Để giải quyết điều này, Chen
et al. (2022); Kim et al. (2021); Koishekenov et al. (2023) cắt tỉa các chuyên gia dựa trên việc sử dụng của họ
để tiết kiệm bộ nhớ, tuy nhiên, điều này dẫn đến hiệu suất thấp hơn. Ngược lại, Gao et al. (2022) sử dụng
phương pháp phân tách tensor để chia sẻ các tham số tensor trung tâm qua các chuyên gia và giữ
các tensor phụ khác nhau cho mỗi chuyên gia. Hơn nữa, một số công trình sử dụng chưng cất kiến thức (KD)
(Rajbhandari et al., 2022; Artetxe et al., 2022; Fedus et al., 2022) để tạo ra một mô hình dense nhỏ hơn
hoặc mô hình SMoE với ít lớp hơn. Tuy nhiên, chúng cũng bỏ qua sự dư thừa hiện có
trong các lớp SMoE. Hơn nữa, Yadav et al. (2023a) cho thấy rằng các chuyên gia có thể được nén đến mức
rất lớn mà không mất hiệu suất.
Hợp nhất mô hình trong các mô hình ngôn ngữ. Sự phong phú của các mô hình mã nguồn mở đòi hỏi việc
khai thác các mô hình hiện có này để tạo ra những mô hình vượt trội. Ensemble mạng (Zhu et al., 2019; Ortega
et al., 2022) nổi lên như một giải pháp trực quan, tuy nhiên, gánh nặng tính toán của nó trong quá trình suy luận
tăng tỷ lệ thuận với việc bao gồm nhiều mô hình hơn. Văn học gần đây ngày càng
nhấn mạnh khái niệm hợp nhất mô hình (Yadav et al., 2023b; Cai et al., 2023; Ilharco et al., 2022b;
Matena & Raffel, 2022; Jin et al., 2022; Don-Yehiya et al., 2022; Rame et al., 2023). Tuy nhiên, hầu hết
các nghiên cứu này giả định rằng các mô hình được hợp nhất có nguồn gốc từ cùng một khởi tạo (Yadav et al.,
2023b; Ilharco et al., 2022a; Wortsman et al., 2022), thu hẹp nhóm các mô hình nguồn tiềm năng
phù hợp cho việc hợp nhất. Tuy nhiên, giả định này có thể không áp dụng cho các mô hình SMoE. Thông thường,
các chuyên gia khác nhau trong SMoE bắt đầu với các khởi tạo tham số ngẫu nhiên riêng biệt, và mỗi chuyên gia
3

--- TRANG 4 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Hình 3: Phân phối tần suất kích hoạt chuyên gia trong mô hình switch-base-32, bao gồm 12
lớp SMoE với 32 chuyên gia mỗi lớp. Phần trên của heatmap là lớp MoE đầu tiên trong khi phần dưới là
lớp cuối cùng. Hai nhiệm vụ bên trái, COPA và SQuAD, được đặc trưng bởi các lời nhắc tạo câu trả lời. Hai
nhiệm vụ bên phải, WikiQA và SST2, được đặc trưng bởi các lời nhắc chọn câu trả lời. Các mô hình SMoE được fine-tuned trên
các nhiệm vụ chọn câu trả lời thể hiện một phân phối nghiêng hơn trong các lớp decoder transformer của chúng, trong đó một
phần đáng kể các chuyên gia vẫn không hoạt động suốt thời gian.
được tối ưu hóa với chỉ một tập con của dữ liệu huấn luyện, như được xác định bởi các mạng định tuyến. Những
đặc điểm này làm cho nhiệm vụ hợp nhất các chuyên gia trong SMoE trở nên thách thức hơn.
Để giải quyết những thách thức này, nhiều nghiên cứu tìm đến kết nối mode (Draxler et al., 2018;
Frankle et al., 2020; Freeman & Bruna, 2016; Garipov et al., 2018) như một thước đo để đo lường sự phức
tạp của việc hợp nhất giữa hai chuyên gia. Tiền đề cơ bản là các mô hình trong cùng một loss basin
có thể hợp nhất được. Ngoài ra, một số công trình sử dụng bất biến hoán vị (Ainsworth et al.,
2022; Jordan et al., 2022; Pe ˜na et al., 2023) để chuyển các mô hình trong các error basin khác nhau vào cùng một
mà không ảnh hưởng đến chức năng của chúng. Jolicoeur-Martineau et al. (2023) áp dụng các thuật ngữ chính quy hóa
trong quá trình huấn luyện để nâng cao khả năng hợp nhất của các mô hình, và Gueta et al. (2023) phân tích có hệ thống
cách các nhiệm vụ huấn luyện, bộ dữ liệu và công thức ảnh hưởng đến độ khó của việc hợp nhất. Một công trình
đồng thời, SMEAR (Muqeeth et al., 2023) hợp nhất động các chuyên gia khác nhau thành một trong
quá trình huấn luyện để tránh định tuyến rời rạc. Lưu ý rằng phương pháp này không cung cấp bất kỳ
giảm bộ nhớ nào và đòi hỏi duy trì toàn bộ SMoE trong quá trình suy luận.
3 PHƯƠNG PHÁP LUẬN
Trong phần này, chúng tôi trình bày chi tiết về phương pháp MC-SMoE được đề xuất. Phần 3.1 giới thiệu
kỹ thuật hợp nhất chuyên gia M-SMoE và cách nó được hướng dẫn bởi chính sách định tuyến. Trong Phần 3.2, chúng tôi
minh họa lợi ích bổ sung của các chuyên gia đã hợp nhất và cách nó dẫn đến việc nén thêm. Toàn bộ
quy trình của MC-SMoE được cung cấp ở cuối trong Thuật toán 1.
3.1 CHÍNH SÁCH ĐỊNH TUYẾN HƯỚNG DẪN HỢP NHẤT CHUYÊN GIA
Căn chỉnh hoán vị chuyên gia. Phương pháp M-SMoE của chúng tôi bắt đầu với việc căn chỉnh các hoán vị trọng số
chuyên gia vì việc hợp nhất mà không có nó có thể dẫn đến việc kết hợp kém các
nơ-ron không khớp. Trong trường hợp của chúng tôi, các chuyên gia mục tiêu hoạt động trong cùng không gian đầu vào-đầu ra,
điều này làm cho việc hợp nhất trở nên khả thi hơn. Các chuyên gia là các mạng feed-forward 2 lớp, trong đó Winvà
Woutbiểu thị hai ma trận trọng số của các lớp đầu vào và đầu ra, tương ứng. xlà vector đầu vào và
act(·)đại diện cho hàm kích hoạt. Khi đó, một mạng feed-forward được định nghĩa là một ánh xạ
F:x→Wout(act(Winx)). Ainsworth et al. (2022) cho chúng ta biết rằng đối với bất kỳ ma trận hoán vị tùy ý
P, phương trình sau Wout(act(Winx)) = WoutPT(act(PWinx))luôn đúng. Nói cách khác, P
bảo toàn hàm F.
Chúng tôi tuân theo tối ưu hóa khớp trọng số trong Ainsworth et al. (2022) để căn chỉnh các chuyên gia mà không
thay đổi các chức năng của chúng. Ví dụ, cho hai chuyên gia EivàEjvới các ma trận trọng số Wivà
Wj, nó cố gắng định vị PivàPjtối ưu bằng cách tối thiểu hóa khoảng cách ℓ2giữa các
trọng số hoán vị tương ứng W′
ivàW′
j. Chi tiết được bao gồm trong A2. Quá trình này cung cấp một bước đầu tiên
có lợi cho việc hợp nhất.
Các chính sách định tuyến phản ánh sự tương tự chuyên gia. Một trong những thách thức chính trong việc hợp nhất
chuyên gia SMoE đến từ chuyên môn hóa chuyên gia (Mittal et al., 2022) được nuôi dưỡng trong quá trình huấn luyện
chung của các chuyên gia và bộ định tuyến. Mặc dù sự sụp đổ đại diện xảy ra (Chi et al., 2022) và
sự dư thừa lớn tồn tại giữa các chuyên gia, Hình 3 chứng minh rằng việc sử dụng một số (nhiều hơn một) chuyên gia
lớn hơn đáng kể so với phần còn lại. Do đó, việc hợp nhất tất cả các chuyên gia trong một lớp SMoE thành
một chuyên gia dense duy nhất là thách thức. Thay vào đó, chúng tôi chia chúng thành nhiều nhóm
dựa trên sự tương tự của chúng, và giữ tất cả các chuyên gia thống trị (được sử dụng nhiều nhất) để bảo toàn hiệu suất. Để
4

--- TRANG 5 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Hình 4: Các chuyên gia có thể nén được hơn sau khi hợp nhất. Chúng tôi tính toán tỷ lệ thay đổi stable-rank trung bình
(sau−trước
trước) của tất cả các chuyên gia thống trị trong mỗi lớp của mô hình switch-base-32 SMoE, phản ánh
sự khác biệt trước và sau khi hợp nhất. Những giá trị chủ yếu âm này trong các lớp SMoE nhấn mạnh một
chiều thấp hơn đạt được thông qua quá trình hợp nhất.
đáp ứng mục tiêu, phương pháp M-SMoE của chúng tôi khai thác hướng dẫn ngầm từ chính sách định tuyến của SMoE: ( 1)
Các hàng tương tự (kênh đầu ra) trong ma trận trọng số router có xu hướng đưa các token đầu vào tương tự đến các chuyên gia
tương ứng của chúng, thúc đẩy các chuyên gia này được huấn luyện theo cách tương tự; ( 2) Một cách trực quan, các chuyên gia
tương tự có xu hướng thể hiện các logit router tương tự qua phần lớn các token đầu vào. Dựa trên điều này,
chúng tôi có thể sử dụng các hàng trong ma trận trọng số router hoặc vector logit router có được từ một batch
token đầu vào, để đo lường sự tương tự chuyên gia. Các so sánh chi tiết được cung cấp trong Phần 4.3 và chúng tôi
mô tả cái vượt trội ở đây, tức là, router logits, và để lại cái khác cho Phụ lục A2. Cụ thể,
sự tương tự Sim(·,·)giữa các chuyên gia EivàEjtrong một lớp SMoE được tính bằng:
H=Wr(XT),Sim(Ei,Ej) =cosine (Hi,∗,Hj,∗), (1)
trong đó Xlà một embedding đầu vào, Wrlà trọng số router, Hi,∗vàHj,∗là các vector hàng trong logits H.
Các chuyên gia thống trị, nhóm chuyên gia, và hợp nhất dựa trên tần suất. Dựa trên việc sử dụng
chuyên gia như được mô tả trong Hình 3, trước tiên chúng tôi coi các chuyên gia hoạt động thường xuyên nhất làm các chuyên gia thống trị .
Việc sử dụng chuyên gia như vậy được tính bằng cách đầu vào và định tuyến một tập con được chọn ngẫu nhiên của
dữ liệu huấn luyện. Sau đó, như được chứng minh trong Hình 2 ( b), mỗi chuyên gia không thống trị hướng về
và tham gia nhóm do chuyên gia thống trị tương tự nhất của nó dẫn đầu , sử dụng hàm tương tự được định nghĩa bởi
Phương trình 1. Sau khi nhóm, mỗi nhóm bao gồm một vài chuyên gia không thống trị và một chuyên gia thống trị.
Cuối cùng, đối với một nhóm kchuyên gia {E1,···,Ek}, một hợp nhất dựa trên tần suất được thực hiện như sau:
Emerged =Pk
i=1αiEiPk
i=1αi, (2)
trong đó αilà tần suất sử dụng của chuyên gia Ei. Sự vượt trội của việc nhấn mạnh các chuyên gia thống trị
được chi tiết và xác thực trong nghiên cứu ablation của chúng tôi (Phần 4.3).
Tỷ lệ hợp nhất thích ứng theo lớp. Như được hiển thị trong Hình 3, tần suất được kích hoạt của mỗi
chuyên gia khác nhau qua các lớp SMoE khác nhau, gợi ý một số lượng đa dạng các chuyên gia thống trị và
các nhóm tương ứng. Để xem xét hiện tượng này, chúng tôi chuẩn hóa các tần suất trong mỗi
lớp SMoE và chọn các chuyên gia thống trị theo cách toàn cầu qua tất cả các lớp3. Lấy một trường hợp
cực đoan làm ví dụ, nếu định tuyến chuyên gia là đồng đều trong một lớp SMoE, thì tất cả các chuyên gia sẽ được
coi là thống trị, phản ánh trực giác của chúng tôi.
3.2 HỢP NHẤT KHUYẾN KHÍCH PHÂN TÁCH CHUYÊN GIA
Hợp nhất khuyến khích trọng số hạng thấp. Chúng tôi quan sát thấy rằng M-SMoE thúc đẩy một chiều
thấp hơn trong không gian trọng số của các chuyên gia đã hợp nhất, tự nhiên tạo điều kiện cho việc nén bổ sung. Chúng tôi
áp dụng thước đo từ Wang et al. (2023) để đo lường hạng của các không gian trọng số. Thước đo này đã
được chứng minh là thực tế vì nó chủ yếu không bị ảnh hưởng bởi các giá trị đơn lẻ nhỏ, cung cấp một
ước tính hạng cho ma trận trọng số Wtừ một lớp mạng. Nó được định nghĩa dưới đây:
stable-rank (σ) =Σiσ2
i
maxσ2
i, (3)
trong đó σbiểu thị vector giá trị đơn lẻ của W. Hình 4 trình bày một số trường hợp tỷ lệ thay đổi stable-rank
của các SMoE được fine-tuned trên các nhiệm vụ khác nhau. Chúng tôi đo lường sự thay đổi của stable-rank 
sau khi hợp nhất bằng cách tính tỷ lệ của sự khác biệt với giá trị ban đầu của nó. Chúng tôi thấy rằng
tỷ lệ thay đổi stable-rank trung bình của tất cả các chuyên gia là liên tục không dương, tức là stable-rank
giảm, trên hầu hết các lớp SMoE, sau khi hợp nhất. Điều này truyền cảm hứng cho chúng tôi tiến hành nén sau hợp nhất,
như được minh họa trong Hình 2 ( c).
3Để đảm bảo tính ổn định tính toán, chúng tôi điều chỉnh tần suất của chuyên gia hoạt động nhiều nhất trong mỗi lớp SMoE
về 1.0. Bằng cách này, ít nhất một chuyên gia sẽ được gắn nhãn là thống trị . Tuy nhiên, các thí nghiệm của chúng tôi cho thấy rằng
luôn có ít nhất hai chuyên gia thống trị trong mỗi lớp SMoE.
5

--- TRANG 6 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Nén sau hợp nhất của MC-SMoE .Để tận hưởng lợi ích bổ sung từ việc hợp nhất, chúng tôi điều chỉnh
các phương pháp phân tách SoTA trước đó (Chen et al., 2021; Li et al., 2023) cho SMoE, và đề xuất
một thuật toán nâng cấp MC-SMoE để có thêm hiệu quả bộ nhớ và tham số. Cụ thể, ma trận trọng số W
của một chuyên gia đã hợp nhất được phân tách thành UV+S. Ở đây, tích của U∈Rd1×rvà
V∈Rr×d2đại diện cho một xấp xỉ hạng thấp, trong đó rlà một hạng nhỏ hơn nhiều so với
chiều đầy đủ của W.Schứa phần không nhất quán của các trọng số trong W, và sẽ được cắt tỉa thêm
theo cách cấu trúc. Một điểm số quan trọng của trọng số si,jlà I(si,j) =|si,j· ∇si,jL|,
trong đó Lchỉ ra mục tiêu huấn luyện của SMoE. Để cắt giảm S, các cột trọng số với
điểm số tích lũy thấp nhấtP
iI(si,j)sẽ được loại bỏ, điều này được xác định qua tất cả các trọng số Svà
tự nhiên dẫn đến tỷ lệ nén thích ứng theo lớp. Như một tóm tắt, Thuật toán 1 trình bày
quy trình đầy đủ của khung MC-SMoE được đề xuất của chúng tôi.
Thuật toán 1 Các quy trình tổng thể của MC-SMoE .
1:Khởi tạo: Một mô hình Mvớillớp SMoE, bộ dữ liệu huấn luyện Tvớibtoken, tổng số chuyên gia gốc n, và số lượng chuyên gia còn lại k.
2: Cho H∈Rl×b×nvàA∈Rl×nbiểu thị logit routervàtần suất kích hoạt , tương ứng
3: Cho Dđại diện cho tập hợp các chuyên gia thống trị
4:H,A←forward (M,T);D ← top(k,row-normalize (A))
5:forlớp t= 1, . . . , l do
6: forchuyên gia i= 2, . . . ,n
ldo
7: Et
i←weight-matching (Et
i,Et
1) ▷Căn chỉnh hoán vị chuyên gia
8: end for
9: Q(i):=argmaxj∈Dtcosine (Ht,∗,i,Ht,∗,j) ▷Phân nhóm nhãn
10: ford∈ Dtdo
11: G ← { i| Q(i) == d};Et
d←P
i∈GAt,iEt
iP
i∈GAt,i▷Hợp nhất dựa trên tần suất kích hoạt
12: Et
d→Ut
dVt
d+St
d ▷Sau đó nén
13: end for
14: fori /∈ D do
15: Loại bỏ Et
ikhỏiM
16: end for
17:end for
18:Return: Một SMoE nhỏ gọn được tạo ra từ MC-SMoE .
4 THÍ NGHIỆM
4.1 CHI TIẾT TRIỂN KHAI
Bảng 1: Hai mô hình SMoE và các checkpoint mô hình
dense tương ứng của chúng. act-size : số lượng tham số
được kích hoạt cho mỗi token, size: tổng số tham số, l:
số lượng lớp transformer, h: chiều ẩn, e: số lượng chuyên gia, arch:
loại kiến trúc transformer.
Mô hình Định danh act-size size l h e arch
t5-base 220M220M12 768 1 enc-dec
switch-base-32 220M 2.0B12 768 32 enc-dec
fairseq-dense-125m 125M125M12 768 1 dec
fairseq-moe-15b 125M 15B12 768 512 decBộ dữ liệu và backbone mạng. Các thí nghiệm của chúng tôi
áp dụng hai họ mô hình ngôn ngữ lớn mã nguồn mở với các
biến thể SMoE của chúng: ( a) Switch Transformers (Fedus et al.,
2022) và ( b) Các mô hình SMoE dựa trên GPT của Meta (Artetxe et al., 2022). Một tóm tắt về các
cấu hình mô hình cụ thể được cung cấp trong Bảng 1. Chúng tôi sử dụng tám nhiệm vụ NLP phổ biến cho
fine-tuning và đánh giá có giám sát: SST-
2 (Socher et al., 2013) cho phân loại cảm xúc, MRPC (Dolan & Brockett, 2005) cho nhận diện paraphrase, MultiRC (Khashabi et al., 2018) cho QA trắc nghiệm, COPA (Gor-
don et al., 2012) cho hoàn thành câu, WinoGrande (Sakaguchi et al., 2019) cho giải quyết hội nghị,
SQuAD v1.1 (Rajpurkar et al., 2016) cho QA trích xuất, WikiQA (Yang et al., 2015)
và HotpotQA (Yang et al., 2018) cho QA sách đóng. Đối với đánh giá zero-shot, chúng tôi chọn ba
bộ dữ liệu chuẩn đại diện: MRPC trong GLUE (Wang et al., 2019), WinoGrande cho lý luận, và
OpenBookQA (Mihaylov et al., 2018) cho QA.
Baseline so sánh. Chúng tôi so sánh các đề xuất của chúng tôi với sáubaseline bao gồm hai phương pháp cắt tỉa và
bốn phương pháp hợp nhất. Thứ nhất, chúng tôi xem xét phương pháp cắt tỉa chuyên gia "task-specific" từ Chen
6

--- TRANG 7 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Bảng 2: Đánh giá hiệu suất trên mô hình switch-base-32 với 32 chuyên gia trong mỗi lớp SMoE,
cũng như mô hình dense so sánh t5-base . Chúng tôi thấy rằng lớp SMoE đầu tiên có tác động sâu sắc
lên hiệu suất của mô hình, và việc hợp nhất nó dẫn đến sự suy giảm hiệu suất đáng kể hơn
so với các lớp khác. Do đó, đối với tất cả các cơ chế hợp nhất/nén, lớp SMoE đầu tiên được
bỏ qua theo Ma et al. (2023), và nó duy trì trung bình 8 chuyên gia trong các lớp SMoE khác.
Chúng tôi báo cáo exact-match/F1-score cho SQuAD và HotpotQA, F1-score cho MultiRC, và độ chính xác
cho các nhiệm vụ khác. Đối với mỗi nhiệm vụ, chúng tôi làm nổi bật hiệu suất tốt nhất trên tất cả baseline bằng màu xanh , và
đánh dấu hiệu suất không tệ hơn SMoE đầy đủ bằng chữ đậm .
Phương pháp Kích thước mô hình TFLOPs SST-2 MRPC MultiRC COPA WinoGrande SQuAD WikiQA HotpotQA
Dense 220M 4.65 94.61 88 .97 74 .25 58 .00 58 .72 63 .65/83.76 96 .12 66 .13/83.45
Full SMoE 2.0B 4.65 95.75 90 .20 76 .19 68 .00 61 .80 65 .39/85.81 96.45 67 .55/84.60
Pruning 733M 4.65 94.50 88 .97 75 .13 63 .00 61 .64 64 .80/85.13 96 .27 67 .39/84.56
Task-Specific 733M 4.65 91.28 82 .04 53 .63 52 .00 58 .56 54 .40/78.00 95 .24 64 .70/82.76
Averaging 733M 4.65 92.66 88 .73 74 .04 62 .00 59 .59 64 .49/84.75 96 .19 67 .36/84.61
ZipIt 733M 4.65 93.12 91.18 75.26 65 .00 60 .38 65 .01/85.06 96 .05 67.59/84.70
REPAIR 733M 4.65 92.89 90.44 74.44 65 .00 61 .48 64 .67/84.84 96 .27 67.67/84.77
Git Re-basin 733M 4.65 93.35 88 .24 74 .25 65 .00 59 .25 64 .61/84.92 96 .23 67 .29/84.46
M-SMoE 733M 4.65 94.50 90.69 75.57 68.00 61.80 65.66/85.49 96.34 67.91/84.83
MC-SMoE 381M 3.83 93.35 89.22 73.98 67.00 59.52 65.41/85.30 96.08 67.64/84.77
et al. (2022), việc loại bỏ dần các chuyên gia không hoạt động trong quá trình fine-tuning. Ngoài ra, chúng tôi đánh giá
việc cắt tỉa một lần các chuyên gia không thống trị như một kiểm tra tính hợp lý. Thứ hai, do thiếu công trình trước đây
về hợp nhất chuyên gia, chúng tôi trực tiếp thích ứng các phương pháp hợp nhất Averaging (Choshen et al., 2022), ZipIt (Stoica et al.,
2023), REPAIR (Jordan et al., 2022) và Git Re-basin (Ainsworth et al., 2022) với các kịch bản SMoE của chúng tôi
như baseline mạnh để so sánh.
Chi tiết huấn luyện và đánh giá. Đối với các mô hình encoder-decoder, bao gồm mô hình switch-base-32
SMoE và mô hình t5-base dense, chúng tôi báo cáo kết quả fine-tuning có giám sát. Đối với mỗi nhiệm vụ,
trước tiên chúng tôi tiến hành tìm kiếm siêu tham số toàn diện. Điều này bao gồm kích thước batch từ {8,
16,32,64}, tốc độ học từ {3×10−4,1×10−4,3×10−5,1×10−5}, và số epoch kéo dài
{3,5,10,20}, để xác định các mô hình fine-tuned tối ưu. Các siêu tham số fine-tuning khác được cố định, như
được hiển thị trong Bảng A15 Phụ lục. Sau khi hợp nhất và nén, chúng tôi tiến hành fine-tune
mô hình được nén để khôi phục hiệu suất của nó. Hơn nữa, chúng tôi áp dụng chưng cất kiến thức (KD) để
buộc các mô hình M-SMoE vàMC-SMoE bắt chước các đầu ra được tạo ra bởi mô hình SMoE đầy đủ
trên bộ dữ liệu huấn luyện. Các siêu tham số trong loss KD được thêm vào được cố định cho tất cả các nhiệm vụ, vui lòng
tham khảo Phụ lục A2 để biết thêm chi tiết. Đối với các mô hình chỉ decoder, bao gồm mô hình fairseq-moe-
15bSMoE và mô hình fairseq-dense-125m dense, chúng tôi báo cáo kết quả zero-shot, tức là không
trải qua bất kỳ huấn luyện nào nữa. Đối với giai đoạn nén trong MC-SMoE , chúng tôi đặt tỷ lệ thưa thớt là
0.1và hệ số hạng thấp là 32, theo Li et al. (2023). Kích thước mô hình và số lượng tera
floating point operations (TFLOPs) được báo cáo để đo lường hiệu quả. TFLOPs được đánh giá
bằng một batch của 64 mẫu đầu tiên trong bộ dữ liệu SQuAD, với độ dài chuỗi đầu vào là 329và
độ dài chuỗi mục tiêu là 13. Tất cả các thí nghiệm được tiến hành với PyTorch và DeepSpeed trên
NVIDIA A 100và A 6000 .
4.2 HIỆU SUẤT CẠNH TRANH VÀ HIỆU QUẢ VƯỢT TRỘI CỦA MC-SM OE
Bảng 2 trình bày các so sánh hiệu suất giữa M-SMoE ,MC-SMoE , và tám baseline theo cách
fine-tuning có giám sát trên bộ dữ liệu {SST2, MRPC, MultiRC, COPA, WinoGrande, SQuaD, WikiQA,
HotpotQA }. Lưu ý rằng tất cả các phương pháp được so sánh kích hoạt cùng số lượng tham số.
Từ Bảng 2, các quan sát sau có thể được rút ra: ❶M-SMoE đạt được 60% giảm bộ nhớ
trong khi duy trì hiệu suất trên {MRPC, COPA, WinoGrande, SQuAD, HotpotQA }, và thậm chí
thu được cải thiện hiệu suất bổ sung {0.49,0.25,0.41}(%) trên {MRPC, SQuAD, HotpotQA }
so với mô hình SMoE đầy đủ , tương ứng. Mặc dù M-SMoE cho thấy một giảm cận biên trong hiệu suất
cho hiệu quả bộ nhớ trên các bộ dữ liệu chuẩn {SST2, MultiRC, WikiQA }, tuy nhiên, nó vẫn vượt trội
tất cả các baseline cắt tỉa và hợp nhất khác. Những kết quả ấn tượng này xác nhận sự vượt trội của
M-SMoE của chúng tôi trong việc hợp nhất các chuyên gia dư thừa. ❷MC-SMoE được thực hiện trên đỉnh của việc hợp nhất chuyên gia từ M-SMoE . Mô hình kết quả đạt được đến 80% trong bộ nhớ và 20% trong tiết kiệm FLOPs,
trong khi sự suy giảm hiệu suất vẫn dưới 1%trên{MRPC, COPA, SQuAD, WikiQA,
HotpotQA }.❸Ngoài ra, các so sánh học tập zero-shot giữa chúng tôi và baseline với
mô hình fairseq-moe-15b SMoE và mô hình fairseq-dense-125m dense được bao gồm trong Phụ lục A1.1.
7

--- TRANG 8 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
4.3 NGHIÊN CỨU ABLATION VÀ ĐIỀU TRA BỔ SUNG
Bảng 3: So sánh giữa tỷ lệ hợp nhất Uniform
vàAdaptive (của chúng tôi) với mô hình
switch-base-32 trên bốn bộ dữ liệu.
Tỷ lệ hợp nhất Uniform Adaptive
MultiRC 74.48 75.57
COPA 63.00 68.00
MRPC 90.44 90.69
SQuAD 64.36/84.56 65.66/85.49Ablation về các thiết kế tỷ lệ hợp nhất khác nhau. Để kiểm tra
xem tỷ lệ hợp nhất thích ứng của chúng tôi có hiệu quả hay không,
chúng tôi tiến hành nghiên cứu ablation về các tỷ lệ hợp nhất khác nhau,
tức là,uniform (tỷ lệ không đổi mỗi lớp) so với adaptive(của chúng tôi).
Kết quả thí nghiệm được tạo ra với backbone switch-base-32 trên
bốn bộ dữ liệu, như được hiển thị trong Bảng 3. Tỷ lệ thích ứng của chúng tôi
thể hiện một lợi thế nhất quán về hiệu suất hợp nhất,
so với tỷ lệ đồng đều. Điều này nằm trong kỳ vọng vì nghiên cứu
ban đầu trong Hình 3 tiết lộ rằng số lượng chuyên gia được sử dụng
thường xuyên là khác nhau qua các khối transformer khác nhau.
Bảng 4: So sánh giữa router-logits (của chúng tôi) và
bảy hàm tương tự khác cho việc nhóm chuyên gia.
Đại diện MultiRC COPA MRPC SQuAD
Random 74.69 62 .00 89 .95 64 .97/84.96
Expert-weight 75.29 63 .00 89.46 64 .98/85.18
Expert-weight-feature 74.96 62 .00 89 .95 64 .98/85.19
Expert-gradient 75.50 59 .00 89 .22 64 .93/85.01
Expert-feature 74.74 60 .00 89 .95 65 .03/85.21
Expert-feature.abs 75.20 65 .00 89 .22 64 .90/85.15
Router-weight 75.01 59 .00 88 .73 64 .99/85.02
Router-logits (Của chúng tôi) 75.57 68.00 90.69 65.66/85.49Ablation về các phương pháp nhóm khác nhau.
Một thành phần quan trọng của khung M-SMoE
của chúng tôi là tính toán sự tương tự giữa các
chuyên gia bằng router output logits, tức là router-logits ,
điều này trực tiếp xác định trạng thái nhóm của
chúng. Ở đây, chúng tôi tiến hành nghiên cứu ablation
để so sánh router-logits của chúng tôi với bảy
hàm tương tự khác: ( i)random , tạo ra một vector
ngẫu nhiên cho mỗi chuyên gia; (ii)expert-weight ,
sử dụng trọng số được làm phẳng của mạng feed-forward
của mỗi chuyên gia; ( iii)expert-weight-feature , tận dụng
tích của trọng số chuyên gia và chuẩn L2 của các đặc trưng
liên quan; ( iv)expert-gradient , sử dụng các gradient
được làm phẳng của mạng feed-forward của mỗi chuyên gia; ( v)expert-feature , áp dụng các trạng thái ẩn
đầu vào trung bình của mỗi chuyên gia; ( vi)expert-feature.abs , sử dụng trung bình của các giá trị tuyệt đối
của các trạng thái ẩn đầu vào của mỗi chuyên gia; ( vii)router-weight , áp dụng vector hàng tương ứng từ
ma trận trọng số router; và ( viii)router-logits của chúng tôi , sử dụng vector logits đầu ra router tương ứng
với chuyên gia sau khi đưa một batch vào mô hình SMoE. Kết quả thí nghiệm với mô hình switch-base-32
qua bốn bộ dữ liệu được trình bày trong Bảng 4. Chúng tôi quan sát thấy rằng router-logits của chúng tôi
vượt trội nhất quán so với tất cả các biến thể tương tự khác. Sức mạnh của router-logits nằm ở khả năng
phản ánh trực tiếp phân phối quyết định định tuyến của các mẫu đầu vào. Trong quá trình huấn luyện, các chuyên gia
với quyết định định tuyến tương tự được tối ưu hóa với một tập con dữ liệu tương tự, dẫn đến sự dư thừa tiềm năng.
Bảng 5: So sánh giữa fine-tuning
M-SMoE không có và có (của chúng tôi) KD
với mô hình switch-base-32.
Phương pháp không có kD có kD
MultiRC 74.77 75.57
COPA 64.00 68.00
MRPC 89.22 90.69
SQuAD 63.25/84.03 65.66/85.49Đóng góp từ chưng cất kiến thức. Chưng cất kiến thức (KD)
đã được chứng minh là hiệu quả trong việc kế thừa thông tin
từ các mô hình lớn. Do đó, theo mặc định chúng tôi sử dụng KD
cho tất cả các SMoE đã hợp nhất và nén , bao gồm
M-SMoE ,MC-SMoE của chúng tôi, và tất cả baseline. Để hiển thị
đóng góp của nó, chúng tôi thực hiện nghiên cứu ablation so sánh
M-SMoE với và không có việc bao gồm loss KD trong quá trình
fine-tuning. Kết quả thí nghiệm được trình bày trong Bảng 5,
với mô hình switch-base-32 SMoE qua bốn bộ dữ liệu, nhấn mạnh
các lợi thế có được từ việc áp dụng KD.
Bảng 6: So sánh giữa M-SMoE
không có và có căn chỉnh hoán vị (PA)
với mô hình switch-base-32.
Phương pháp M-SMoE không có PA M-SMoE có PA
MultiRC 74.84 75.57
COPA 66.00 68.00
MRPC 89.95 90.69
SQuAD 64.73/84.73 65.66/85.49Đóng góp từ căn chỉnh hoán vị chuyên gia.
Xem xét một chuyên gia với hai lớp feed-forward với
một chiều trung gian là d, có d!loại khả năng
hoán vị để khớp và hợp nhất hai chuyên gia.
Tiếp theo, chúng tôi trình bày một nghiên cứu ablation để
so sánh M-SMoE với và không có căn chỉnh để đánh giá
hiệu quả của căn chỉnh hoán vị chuyên gia. Trong Bảng 6,
chúng tôi trình bày kết quả với mô hình switch-base-32 SMoE
trên bốn bộ dữ liệu. Nó chứng minh một cải thiện hiệu suất
rõ ràng khi áp dụng căn chỉnh hoán vị chuyên gia trước khi hợp nhất.
Do đó, không có căn chỉnh hoán vị thích hợp, việc hợp nhất
chuyên gia có thể dẫn đến một sự kết hợp kém của các nơ-ron không khớp.
8

--- TRANG 9 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Bảng 7: So sánh giữa M-SMoE chỉ hợp nhất,
C-SMoE chỉ nén, và MC-SMoE hợp nhất
và sau đó nén. Thí nghiệm được tiến hành với
mô hình switch-base-32. Chúng tôi làm nổi bật hiệu suất
tốt hơn giữa C-SMoE vàMC-SMoE bằng chữ đậm cho mỗi nhiệm vụ.
Phương pháp SMoE M-SMoE C-SMoE MC-SMoE
Kích thước mô hình 2.0B 733M 570M 381M
TFLOPs 4.65 4.65 3.83 3.83
COPA 68.00 68.00 64.00 67.00
MRPC 90.20 90.69 88.97 89.22
SQuAD 65.39/85.81 65.66/85.49 64.78/84.93 65.41/85.30Tác động của hợp nhất so với phân tách.
Để định lượng lợi ích bổ sung của chiều thấp
phát sinh từ M-SMoE , chúng tôi xem xét
các tác động của việc hợp nhất chuyên gia và
nén SMoE riêng biệt. Chúng tôi xem xét việc
đánh giá ba nhiệm vụ sử dụng mô hình
switch-base-32 SMoE và so sánh M-SMoE
chỉ hợp nhất chuyên gia, C-SMoE chỉ nén,
và với MC-SMoE thực hiện cả việc hợp nhất
và nén. Từ Bảng 7, chúng tôi quan sát:
❶M-SMoE giảm kích thước mô hình trong khi duy trì hoặc tăng cường hiệu suất. Ngược lại, C-SMoE
(tức là, chỉ nén) dẫn đến giảm hiệu suất đáng kể. Điều này gợi ý rằng hợp nhất là một
lựa chọn vượt trội để theo đuổi hiệu quả bộ nhớ và duy trì chất lượng mô hình. ❷Sự thành công của M-SMoE
mở đường cho việc nén thêm. Điều này được hỗ trợ bởi MC-SMoE vượt trội C-SMoE
với thậm chí ít số lượng tham số hơn.
Bảng 8: So sánh giữa các chiến lược trung bình khác nhau
củaUniform ,Fisher-weighted vàFrequency-weighted (của chúng tôi),
được đánh giá với các mô hình switch-base-32 SMoE.
Phương pháp Uniform Fisher-weighted Frequency-weighted
MultiRC 75.11 73 .77 75.57
COPA 64.00 65 .00 68.00
MRPC 89.95 89 .46 90.69
SQuAD 64.55/84.85 63 .99/84.44 65.66/85.49Ablation về các chiến lược hợp nhất khác nhau.
Để kiểm tra hiệu quả của việc hợp nhất chuyên gia
nhận thức tần suất được đề xuất của chúng tôi, một
nghiên cứu ablation về các chiến lược hợp nhất khác nhau
là cần thiết. Cụ thể, chúng tôi nghiên cứu uniform
(Wortsman et al., 2022), fisher-weighted (Matena & Raf-
fel, 2022), và frequency-weighted (của chúng tôi) các
phương pháp hợp nhất với mô hình switch-base-32
qua bốn bộ dữ liệu. Như được chi tiết trong Bảng 8,
chúng tôi thấy rằng hợp nhất có trọng số tần suất của chúng tôi
nhất quán đạt hiệu suất tốt nhất. Một lý do có thể là việc hợp nhất dựa trên tần suất kích hoạt
làm giảm tác động của các chuyên gia ít quan trọng hơn. Ngược lại, cách tiếp cận uniform
có xu hướng đưa ra sự nổi bật không phù hợp cho thông tin dư thừa, che lấp các chuyên gia quan trọng
trong quá trình hợp nhất. Đối với chiến lược hợp nhất fisher-weighted, dựa vào độ lớn gradient
để tái trọng số chuyên gia, không khá trúng mục tiêu, vì trong trường hợp của chúng tôi, các chuyên gia đã
được tiền huấn luyện tốt trước khi hợp nhất.
Hình 5: Tỷ lệ tham số còn lại sau khi nén thêm
các chuyên gia thống trị từMC-SMoE .Trực quan hóa các SMoE nhỏ gọn
từ MC-SMoE .Chúng tôi trực quan hóa
phân phối của các chuyên gia thống trị trong
mô hình switch-base-32 SMoE được tạo ra bởi
M-SMoE , và các phiên bản nén của chúng
từ MC-SMoE trong Hình 5. Mỗi ô lưới
biểu thị một chuyên gia thống trị , và màu
tối hơn chỉ ra nhiều tham số còn lại hơn
trong chuyên gia đó. Các lớp SMoE muộn hơn,
ở phía dưới của heatmap, dường như có thể
hợp nhất và nén được hơn.
5 KẾT LUẬN
Sparse Mixture-of-Experts (SMoE) là một khung hứa hẹn để mở rộng khả năng mô hình, tận hưởng
FLOPs huấn luyện và suy luận gần như không thay đổi với chi phí của việc tăng chi phí bộ nhớ đáng kể.
Các yêu cầu bộ nhớ và sự dư thừa chuyên gia hạn chế mạnh mẽ việc sử dụng thực tế của nó.
Trong công trình này, chúng tôi đề xuất một phương pháp hợp nhất SMoE sáng tạo, tức là,M-SMoE , dựa trên các gợi ý
từ các chính sách định tuyến, để hợp nhất thông tin chuyên gia thành ít hơn nhưng có kiến thức hơn.
Hơn nữa, các chuyên gia đã hợp nhất như vậy được chứng minh là có thể nén được hơn. các phương pháp MC-SMoE
được đề xuất của chúng tôi, theo đuổi hiệu quả bộ nhớ và tham số vượt trội với hiệu suất cạnh tranh. Chúng tôi tiến hành
các thí nghiệm toàn diện để hỗ trợ hiệu quả của các đề xuất của chúng tôi. Các công trình tương lai chủ yếu
nằm trong việc mở rộng các kịch bản đa phương thức và thiết kế đồng bộ với các nền tảng phần cứng.
9

--- TRANG 10 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
6 TUYÊN BỐ TÁI SẢN XUẤT
Để khuyến khích tái sản xuất, chúng tôi đã cung cấp mã nguồn của chúng tôi tại kho GitHub của chúng tôi,
https://github.com/UNITES-Lab/MC-SMoE, bao gồm tiền xử lý dữ liệu, hợp nhất/-
nén/cắt tỉa SMoE, và các script đánh giá. Chi tiết siêu tham số được cung cấp trong Phụ
lục A2 và mã giả chi tiết về việc hợp nhất chuyên gia SMoE được cung cấp trong Phụ lục A3.
Chúng tôi cũng cung cấp Thuật toán 1 rõ ràng và súc tích cho pipeline MC-SMoE của chúng tôi.
TÀI LIỆU THAM KHẢO
Samuel K Ainsworth, Jonathan Hayase, và Siddhartha Srinivasa. Git re-basin: Merging models
modulo permutation symmetries. arXiv preprint arXiv:2209.04836 , 2022.
Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria
Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui
Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo,
Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, và Ves Stoyanov. Efficient large
scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684 , 2022.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,
Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,
Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, và Dario Amodei. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165 , 2020.
Ruisi Cai, Zhenyu Zhang, và Zhangyang Wang. Robust weight signatures: Gaining robustness as
easy as patching weights? arXiv preprint arXiv:2302.12480 , 2023.
Tianlong Chen, Zhenyu Zhang, Ajay Jaiswal, Shiwei Liu, và Zhangyang Wang. Sparse moe as the
new dropout: Scaling dense and self-slimmable transformers. arXiv preprint arXiv:2303.01610 ,
2023.
Tianyu Chen, Shaohan Huang, Yuan Xie, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li,
và Furu Wei. Task-specific expert pruning for sparse mixture-of-experts. arXiv preprint
arXiv:2206.00277 , 2022.
Xuxi Chen, Tianlong Chen, Yu Cheng, Weizhu Chen, Zhangyang Wang, và Ahmed Hassan
Awadallah. Dsee: Dually sparsity-embedded efficient tuning of pre-trained language models.
arXiv preprint arXiv:2111.00160 , 2021.
Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,
Payal Bajaj, Xia Song, và Furu Wei. On the representation collapse of sparse mixture of experts.
arXiv preprint arXiv:2204.09179 , 2022.
Leshem Choshen, Elad Venezian, Noam Slonim, và Yoav Katz. Fusing finetuned models for better
pretraining. arXiv preprint arXiv:2204.03044 , 2022.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. Palm: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311 , 2022.
10

--- TRANG 11 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan
Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,
Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel-
lat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao,
Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,
Adam Roberts, Denny Zhou, Quoc V . Le, và Jason Wei. Scaling instruction-finetuned language
models. arXiv preprint arXiv:2210.11416 , 2022.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. ArXiv , abs/1810.04805, 2019.
William B. Dolan và Chris Brockett. Automatically constructing a corpus of sentential paraphrases.
InProceedings of the Third International Workshop on Paraphrasing (IWP2005) , 2005. URL
https://aclanthology.org/I05-5002 .
Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, và Leshem
Choshen. Cold fusion: Collaborative descent for distributed multitask finetuning. arXiv preprint
arXiv:2212.01378 , 2022.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at
scale. ArXiv , abs/2010.11929, 2021.
Felix Draxler, Kambis Veschgini, Manfred Salmhofer, và Fred Hamprecht. Essentially no barriers
in neural network energy landscape. In International conference on machine learning , pp. 1309–
1318. PMLR, 2018.
Zhiwen Fan, Rishov Sarkar, Ziyu Jiang, Tianlong Chen, Kai Zou, Yu Cheng, Cong Hao, Zhangyang
Wang, et al. M3vit: Mixture-of-experts vision transformer for efficient multi-task learning with
model-accelerator co-design. Advances in Neural Information Processing Systems , 35:28441–
28457, 2022.
William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. Journal of Machine Learning Research , 23(120):1–39,
2022. URL http://jmlr.org/papers/v23/21-0998.html .
Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, và Michael Carbin. Linear mode con-
nectivity and the lottery ticket hypothesis. In International Conference on Machine Learning , pp.
3259–3269. PMLR, 2020.
C Daniel Freeman và Joan Bruna. Topology and geometry of half-rectified network optimization.
arXiv preprint arXiv:1611.01540 , 2016.
Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, và Ji-Rong Wen. Parameter-
efficient mixture-of-experts architecture for pre-trained language models. arXiv preprint
arXiv:2203.01104 , 2022.
Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, và Andrew G Wilson. Loss
surfaces, mode connectivity, and fast ensembling of dnns. Advances in neural information pro-
cessing systems , 31, 2018.
Andrew Gordon, Zornitsa Kozareva, và Melissa Roemmele. SemEval-2012 task 7: Choice of
plausible alternatives: An evaluation of commonsense causal reasoning. In *SEM 2012: The First
Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main
conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop
on Semantic Evaluation (SemEval 2012) , pp. 394–398, Montr ´eal, Canada, 2012. Association for
Computational Linguistics. URL https://aclanthology.org/S12-1052 .
Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, và Leshem Choshen.
Knowledge is a region in weight space for fine-tuned language models. arXiv preprint
arXiv:2302.04863 , 2023.
11

--- TRANG 12 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Song Han, Huizi Mao, và William J. Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 , 2016.
Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, và Jie Tang. Fastmoe: A fast
mixture-of-expert training system. arXiv preprint arXiv:2103.13262 , 2021.
Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, và Qin Li.
Fastermoe: Modeling and optimizing training of large-scale dynamic pre-trained models. In
Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel
Programming , PPoPP '22, pp. 120–134, New York, NY , USA, 2022. Association for Com-
puting Machinery. ISBN 9781450392044. doi: 10.1145/3503221.3508418. URL https:
//doi.org/10.1145/3503221.3508418 .
Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt,
Hannaneh Hajishirzi, và Ali Farhadi. Editing models with task arithmetic. arXiv preprint
arXiv:2212.04089 , 2022a.
Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Si-
mon Kornblith, Ali Farhadi, và Ludwig Schmidt. Patching open-vocabulary models by interpo-
lating weights. Advances in Neural Information Processing Systems , 35:29262–29277, 2022b.
Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, và Pengxiang Cheng. Dataless knowledge fusion by
merging weights of language models. arXiv preprint arXiv:2212.09849 , 2022.
Alexia Jolicoeur-Martineau, Emy Gervais, Kilian Fatras, Yan Zhang, và Simon Lacoste-Julien.
Population parameter averaging (papa). arXiv preprint arXiv:2304.03094 , 2023.
Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, và Behnam Neyshabur. Repair: Renor-
malizing permuted activations for interpolation repair. arXiv preprint arXiv:2211.08403 , 2022.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361 , 2020.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, và Dan Roth. Looking be-
yond the surface: A challenge set for reading comprehension over multiple sentences. In Proceed-
ings of the 2018 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp. 252–262, New Or-
leans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1023.
URL https://aclanthology.org/N18-1023 .
Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu,
Amr Hendy, Samyam Rajbhandari, Yuxiong He, và Hany Hassan Awadalla. Scalable and effi-
cient moe training for multitask multilingual models. arXiv preprint arXiv:2109.10465 , 2021.
Yeskendir Koishekenov, Alexandre Berard, và Vassilina Nikoulina. Memory-efficient NLLB-200:
Language-specific expert pruning of a massively multilingual machine translation model. In Anna
Rogers, Jordan Boyd-Graber, và Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 3567–3585,
Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.
acl-long.198. URL https://aclanthology.org/2023.acl-long.198 .
Harold W. Kuhn. The Hungarian Method for the Assignment Problem. Naval Research Logistics
Quarterly , 2(1–2):83–97, March 1955. doi: 10.1002/nav.3800020109.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, và Zhifeng Chen. {GS}hard: Scaling giant models with condi-
tional computation and automatic sharding. In International Conference on Learning Represen-
tations , 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb .
Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, và Tuo Zhao.
Losparse: Structured compression of large language models based on low-rank and sparse ap-
proximation. arXiv preprint arXiv:2306.11222 , 2023.
12

--- TRANG 13 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 , 2019.
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, và Baining Guo.
Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of
the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 10012–10022, October
2021.
Xinyin Ma, Gongfan Fang, và Xinchao Wang. Llm-pruner: On the structural pruning of large
language models. arXiv preprint arXiv:2305.11627 , 2023.
Zhiyuan Mao, Ajay Jaiswal, Zhangyang Wang, và Stanley H. Chan. Single frame atmospheric
turbulence mitigation: A benchmark study and a new physics-inspired transformer model. ArXiv ,
abs/2207.10040, 2022.
Michael S Matena và Colin A Raffel. Merging models with fisher-weighted averaging. Advances
in Neural Information Processing Systems , 35:17703–17716, 2022.
Todor Mihaylov, Peter Clark, Tushar Khot, và Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. In EMNLP , 2018.
Sarthak Mittal, Yoshua Bengio, và Guillaume Lajoie. Is a modular architecture enough? Advances
in Neural Information Processing Systems , 35:28747–28760, 2022.
Mohammed Muqeeth, Haokun Liu, và Colin Raffel. Soft merging of experts with adaptive routing.
arXiv preprint arXiv:2306.03745 , 2023.
Xiaonan Nie, Pinxue Zhao, Xupeng Miao, Tong Zhao, và Bin Cui. Hetumoe: An efficient trillion-
scale mixture-of-expert distributed training system. arXiv preprint arXiv:2203.14685 , 2022.
Luis A Ortega, Rafael Caba ˜nas, và Andres Masegosa. Diversity and generalization in neural net-
work ensembles. In International Conference on Artificial Intelligence and Statistics , pp. 11720–
11743. PMLR, 2022.
Fidel A Guerrero Pe ˜na, Heitor Rapela Medeiros, Thomas Dubail, Masih Aminbeidokhti, Eric
Granger, và Marco Pedersoli. Re-basin via implicit sinkhorn differentiation. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 20237–20246,
2023.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683 , 2020.
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Am-
mar Ahmad Awan, Jeff Rasley, và Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts
inference and training to power next-generation ai scale. arXiv preprint arXiv:2201.05596 , 2022.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. SQuAD: 100,000+ ques-
tions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing , pp. 2383–2392, Austin, Texas, 2016. Association for
Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https://aclanthology.
org/D16-1264 .
Alexandre Rame, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, L ´eon Bottou, và David Lopez-Paz.
Model ratatouille: Recycling diverse models for out-of-distribution generalization. arXiv preprint
arXiv:2212.10445 , 2023.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, và Yejin Choi. Winogrande: An adver-
sarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641 , 2019.
Victor Sanh, Thomas Wolf, và Alexander M. Rush. Movement pruning: Adaptive sparsity by
fine-tuning, 2020.
13

--- TRANG 14 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
và Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538 , 2017.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,
và Christopher Potts. Recursive deep models for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing , pp. 1631–1642, Seattle, Washington, USA, 2013. Association for Computational Lin-
guistics. URL https://aclanthology.org/D13-1170 .
George Stoica, Daniel Bolya, Jakob Bjorner, Taylor Hearn, và Judy Hoffman. Zipit! merging
models from different tasks without training. arXiv preprint arXiv:2305.03053 , 2023.
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, và
Herv'e J'egou. Training data-efficient image transformers & distillation through attention. In
ICML , 2021.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762 ,
2023.
Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Courna-
peau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, St ´efan J. van der
Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nel-
son, Eric Jones, Robert Kern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore,
Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,
Charles R. Harris, Anne M. Archibald, Ant ˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mul-
bregt, và SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing
in Python. Nature Methods , 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R. Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv
preprint arXiv:1804.07461 , 2019.
Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, và Dim-
itris Papailiopoulos. Cuttlefish: Low-rank model training without all the tuning. arXiv preprint
arXiv:2305.02538 , 2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, và Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903 , 2022.
Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,
Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model
soups: averaging weights of multiple fine-tuned models improves accuracy without increasing
inference time. In International Conference on Machine Learning , pp. 23965–23998. PMLR,
2022.
Prateek Yadav, Leshem Choshen, Colin Raffel, và Mohit Bansal. Compeft: Compression for
communicating parameter efficient updates via sparsification and quantization, 2023a.
Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, và Mohit Bansal. Resolving interfer-
ence when merging models. In NeurIPS , New Orleans, USA, 2023b. Proceedings of Machine
Learning Research.
Yi Yang, Wen-tau Yih, và Christopher Meek. WikiQA: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-
guage Processing , pp. 2013–2018, Lisbon, Portugal, 2015. Association for Computational Lin-
guistics. doi: 10.18653/v1/D15-1237. URL https://aclanthology.org/D15-1237 .
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, và
Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question an-
swering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language
14

--- TRANG 15 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Processing , pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Com-
putational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/
D18-1259 .
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, và Quoc V Le.
Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural
information processing systems , 32, 2019.
Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, và Moshe Wasserblat. Prune once for all:
Sparse pre-trained language models, 2021.
Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng He, Weizhu Chen, và
Tuo Zhao. Platon: Pruning large transformer models with upper confidence bound of weight
importance, 2022.
Minghang Zheng, Peng Gao, Renrui Zhang, Xiaogang Wang, Hongsheng Li, và Hao Dong. End-
to-end object detection with adaptive clustering transformer. ArXiv , abs/2011.09315, 2021.
Michael Zhu và Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for
model compression, 2017.
Shilin Zhu, Xin Dong, và Hao Su. Binary ensemble neural network: More bits per network or
more networks per bit? In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , June 2019.
15

--- TRANG 16 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
PHỤ LỤC
A1 KẾT QUẢ THÍ NGHIỆM THÊM
A1.1 KẾT QUẢ ĐÁNH GIÁ ZERO-SHOT
Chúng tôi so sánh M-SMoE ,MC-SMoE được đề xuất của chúng tôi với việc cắt tỉa một lần các chuyên gia không thống trị và
phương pháp cắt tỉa chuyên gia "task-specific", theo cách học zero-shot. M-SMoE của chúng tôi liên tục
vượt trội các phương pháp baseline, như được hiển thị trong Bảng A9. Hiệu suất có thể được cải thiện thêm
nếu chỉ chúng tôi có thể fine-tune các router, do M-SMoE của chúng tôi tận dụng cao thông tin định tuyến trong
giai đoạn hợp nhất.
Bảng A9: Đánh giá hiệu suất trên mô hình fairseq-moe-15b với 512 chuyên gia trong mỗi lớp SMoE, cũng như
mô hình dense so sánh fairseq-dense-125m . Khác với mô hình switch-base-32 được fine-tuned,
chúng tôi áp dụng các phương pháp cắt tỉa/hợp nhất trên mọi lớp SMoE ở đây và duy trì trung bình 16 chuyên gia. Chúng tôi
làm nổi bật hiệu suất tốt nhất trên tất cả baseline bằng chữ đậm .
Phương pháp Kích thước mô hình TFLOPs MRPC OpenBookQA WinoGrande
Dense 125M 5.08 37.75 34 .00 49 .49
Full SMoE 15B 5.08 60.54 36 .80 51 .78
Pruning 552M 5.08 52.20 30 .60 48 .46
Task-Specific 552M 5.08 40.19 23 .60 48 .38
M-SMoE 552M 5.08 52.69 34.40 50.43
MC-SMoE 166M 4.45 47.55 34.60 49.09
A1.2 THẢO LUẬN VỀ HIỆU SUẤT VÀ HẠN CHẾ
Hạn chế về độ trễ Mặc dù các mô hình {dense, SMoE, M-SMoE ,MC-SMoE }chia sẻ cùng
TFLOPs lý thuyết, chúng không nhất thiết tạo ra cùng độ trễ. Điều này là do thiết kế thông thường của SMoE
trong thế giới thực gặp phải các chi phí độ trễ bổ sung đáng kể được giới thiệu bởi định tuyến (Nie
et al., 2022). M-SMoE vàMC-SMoE được đề xuất của chúng tôi đạt được hiệu quả bộ nhớ và TFLOPs ấn tượng
cho SMoE. Tuy nhiên, chúng không cải thiện độ trễ. Lý tưởng, quá trình hợp nhất được cho là
giảm số lượng lớp được quản lý bởi bộ phân loại router do việc giảm số lượng
chuyên gia trong mỗi lớp. Tuy nhiên, trong triển khai thực tế, chúng tôi gặp phải một thách thức: việc tạo ra một
router mới một cách rõ ràng cho các chuyên gia đã hợp nhất là không tầm thường. Để giải quyết vấn đề này, chúng tôi áp dụng
chiến lược sau như được hiển thị trong Phụ lục A3: trong mỗi nhóm, chúng tôi giữ lại một chuyên gia đại diện và
để các router khác hướng về chuyên gia đại diện này. Tuy nhiên, tất cả các quyết định định tuyến như vậy vào nhóm này
sẽ được hướng đến một chuyên gia đã hợp nhất mới duy nhất. Điều này ngụ ý rằng, mặc dù số lượng chuyên gia
giảm, số lượng lớp được quản lý bởi router vẫn không đổi, tức là chi phí độ trễ định tuyến
vẫn không đổi. Do đó, nếu chúng tôi quản lý cắt tỉa các kênh đầu ra router mà không ảnh hưởng đến
chức năng của nó, chúng tôi có thể nhận ra một cải thiện đáng chú ý trong hiệu quả độ trễ.
Để kiểm tra hiệu quả tiềm năng từ việc cắt tỉa router trên M-SMoE , chúng tôi tiến hành thí nghiệm
với backbone switch-base-32 trên kích thước batch {32, 256, 512 }và so sánh độ trễ suy luận của
bốn mô hình này: ①dense,②SMoE, ③M-SMoE ,④M-SMoE w.pruning router. Đáng chú ý, kết quả
trong Bảng A10 qua ba cài đặt kích thước batch chứng minh thứ tự độ trễ của ②≈③>④>①. Điều này
chỉ ra hạn chế về độ trễ và khuyến khích công việc tương lai cho việc cắt tỉa router.
Bảng A10: Phân tích độ trễ của mô hình switch-base-32 trên suy luận nhiệm vụ SQuAD với BF16.
Mô hìnhBSZ=32 BSZ=256 BSZ=512
TFLOPs Độ trễ (s) TFLOPs Độ trễ (s) TFLOPs Độ trễ (s)
Dense 2.33 0 .08 25.51 0 .85 59.32 2 .33
Full SMoE-32 2.33 0 .18 25.51 1 .02 59.32 2 .50
M-SMoE-8 2.33 0 .17 25.51 0 .99 59.32 2 .48
M-SMoE-8 w.pruning router 2.33 0.13 25.51 0.93 59.32 2.38
Chuyên môn hóa tiềm năng cho triển khai suy luận Trước tiên chúng tôi trình bày một nghiên cứu toàn diện về
chi phí suy luận của các mô hình SMoE đầy đủ, M-SMoE, và MC-SMoE của chúng tôi, tập trung vào cả
hiệu quả tính toán và bộ nhớ. Nghiên cứu của chúng tôi bao gồm độ trễ, thông lượng, và FLOPs cho
các khía cạnh tính toán, cùng với kích thước mô hình và chi phí bộ nhớ cho các khía cạnh bộ nhớ. Như được hiển thị trong
Bảng A11, các kết quả được gạch chân chứng minh lợi ích suy luận cận biên từ M-SMoE , điều này
xác nhận phân tích của chúng tôi trong đoạn đầu tiên của Phụ lục A1.2. Mặt khác, thông lượng
củaMC-SMoE thấp hơn so với M-SMoE , mặc dù nó tiêu thụ ít bộ nhớ và FLOPs hơn. Điều này
là do thiếu phần mềm hỗ trợ ma trận thưa chuyên biệt hoặc phần cứng cho MC-SMoE , điều này khuyến khích
công việc tương lai của chúng tôi.
Bảng A11: Đánh giá hiệu quả tính toán và bộ nhớ trên các mô hình SMoE đầy đủ, M-SMoE , và MC-SMoE
của chúng tôi mà không có triển khai chuyên biệt . Tất cả hiệu suất được tạo ra bằng cách sử dụng cùng kích thước đầu vào, bao gồm
{thông lượng (token mỗi ms), độ trễ (ms), GFLOPs, bộ nhớ, kích thước mô hình (số lượng tham số) }
Mô hình Thông lượng Độ trễ GFLOPs Bộ nhớ Kích thước mô hình
Full SMoE 4.47 114 .3 232 3895 MB 2.0B
M-SMoE 4.82 106.2 232 1456 MB 733M
MC-SMoE 2.71 189.0 186 715MB 381M
Tuy nhiên, tồn tại tăng tốc lý thuyết. Điều này là do, trong triển khai SMoE thông thường, quá trình định tuyến
bao gồm hai nhược điểm về thông lượng: (1) một biến đổi bố cục của các tensor (để sắp xếp các token nhắm đến
cùng chuyên gia vào một bộ đệm bộ nhớ liên tục) và phép toán nghịch đảo của nó
(Nie et al., 2022), và (2) chia nhỏ một phép toán GEMM khối ma trận lớn thành nhiều
phép toán GEMM khối ma trận nhỏ hơn (mỗi cái tương ứng với một chuyên gia cá nhân), dẫn đến
việc sử dụng kém hiệu quả hơn các lợi thế của phần cứng tính toán hiện đại. Những yếu tố này dẫn đến
giảm thông lượng thực tế cho tính toán được kích hoạt thưa thớt trong SMoE khi số lượng
chuyên gia tăng, một chủ đề vẫn là một khu vực mở cho nghiên cứu (Nie et al., 2022) và được đánh dấu để
khám phá trong các nghiên cứu tương lai của chúng tôi. Trong khi M-SMoE của chúng tôi đối mặt với thách thức đầu tiên do
khó khăn của việc cắt tỉa các kênh đầu ra của router, chúng tôi có khả năng tối ưu hóa tốc độ suy luận từ
thách thức thứ hai.
Chúng tôi tiến hành một đánh giá mở rộng về chi phí tính toán và bộ nhớ cho một thiết kế suy luận chuyên biệt.
Cách tiếp cận của chúng tôi bao gồm thu thập các token được định tuyến đến tất cả chuyên gia của một nhóm và xử lý
chúng thông qua một chuyên gia duy nhất, tận dụng các trọng số được chia sẻ trong nhóm. Chiến lược này được
thiết kế để tận dụng khả năng xử lý song song của các bộ tăng tốc phần cứng, thường là
GPU. Các kết quả được gạch chân được trình bày trong Bảng A12 rõ ràng minh họa hiệu suất thông lượng và độ trễ
nâng cao của các mô hình M-SMoE vàMC-SMoE của chúng tôi sau khi triển khai kỹ thuật tối ưu hóa này.
Chúng tôi tin rằng những kết quả ban đầu hứa hẹn này sẽ thúc đẩy việc khám phá và nghiên cứu bổ sung.
Bảng A12: Đánh giá hiệu quả tính toán và bộ nhớ trên các mô hình SMoE đầy đủ, M-SMoE , và MC-SMoE
của chúng tôi với triển khai chuyên biệt . Tất cả hiệu suất được tạo ra bằng cách sử dụng cùng kích thước đầu vào, bao gồm
{thông lượng (token mỗi ms), độ trễ (ms), GFLOPs, bộ nhớ, kích thước mô hình (số lượng tham số) }
Mô hình Thông lượng Độ trễ GFLOPs Bộ nhớ Kích thước mô hình
Full SMoE 4.47 114 .3 232 3895 MB 2.0B
M-SMoE 7.91 64.7 232 1456 MB 733M
MC-SMoE 6.27 81.6 186 715MB 381M
A1.3 THẢO LUẬN VỀ CHI PHÍ TÍNH TOÁN CỦA M-SM OE
Chúng tôi trình bày một phân tích chi phí tính toán chi tiết cho mỗi giai đoạn của quy trình hợp nhất của chúng tôi.
Cách tiếp cận hợp nhất M-SMoE bao gồm ba giai đoạn chính: ①căn chỉnh hoán vị chuyên gia,
②nhóm chuyên gia, và ③hợp nhất trọng số chuyên gia. Để bắt đầu, việc căn chỉnh hoán vị chuyên gia được
thực hiện riêng biệt trong mỗi lớp SMoE, dẫn đến việc các chi phí tính toán có
tương quan tuyến tính với số lượng lớp SMoE. Thứ hai, việc nhóm chuyên gia bao gồm suy luận mô hình để
đánh giá tần suất kích hoạt và logit router, tiếp theo là tính toán sự tương tự cặp đôi giữa
các chuyên gia. Do tính toán kích hoạt thưa thớt vốn có trong SMoE, chi phí suy luận của mô hình
vẫn không thay đổi bất kể số lượng lớp SMoE, dẫn đến việc tính toán sự tương tự
trong mỗi lớp SMoE là những người đóng góp chính cho sự tăng tuyến tính trong chi phí tính toán.
Giai đoạn cuối cùng, hợp nhất trọng số chuyên gia trong mỗi lớp SMoE, cũng thêm vào sự tăng tuyến tính
này trong các yêu cầu tính toán. Tóm lại, trong khi một số khía cạnh của cách tiếp cận của chúng tôi duy trì một
tải tính toán không đổi, phân tích chi phí tổng thể của chúng tôi chỉ ra một xu hướng tăng tuyến tính trong những
yêu cầu này.
A17

--- TRANG 18 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
Để xác thực phân tích của chúng tôi, chúng tôi tiến hành thí nghiệm bổ sung cho chi phí tính toán của việc hợp nhất.
Chúng tôi đánh giá chi phí thời gian tính toán của mô hình switch-base-32 cho ①căn chỉnh hoán vị chuyên gia,
②nhóm chuyên gia, và ③hợp nhất trọng số chuyên gia tương ứng. Chúng tôi duy trì một số lượng không đổi ( 24) tổng số
lớp Transformer trong khi thay đổi số lượng lớp SMoE. Kết quả được hiển thị trong
Bảng A13 xác nhận phân tích của chúng tôi, chỉ ra rằng nút thắt cổ chai chính về chi phí thời gian
nằm ở việc căn chỉnh hoán vị chuyên gia, trong khi phần lớn chi phí bộ nhớ được quy cho
suy luận mô hình.
Bảng A13: Chi phí tính toán của phương pháp hợp nhất M-SMoE của chúng tôi, được đánh giá với switch-base-32 trên
nhiệm vụ COPA. Chúng tôi duy trì tổng số lớp Transformer không đổi là 24và thay đổi số lượng lớp SMoE từ 2đến12. Ba giai đoạn chính của M-SMoE được đánh giá riêng biệt, bao gồm Căn chỉnh hoán vị (PA), Nhóm chuyên gia (EG), và Hợp nhất trọng số (WM).
Giai đoạn Thước đo SMoE- 2SMoE- 4SMoE- 6SMoE- 8SMoE- 10 SMoE- 12
PAChí phí thời gian 2.35min 4.61min 6.54min 8.40min 10.30min 12.30min
Chi phí bộ nhớ 1.23GB 2.36GB 3.48GB 4.61GB 5.73GB 6.86GB
EGChí phí thời gian 8.0s 8.2s 8.2s 8.3s 8.2s 8.2s
Chi phí bộ nhớ 4.19GB 5.29GB 6.39GB 7.48GB 8.58GB 9.68GB
WMChí phí thời gian 23ms 44ms 66ms 87ms 109ms 139ms
Chi phí bộ nhớ 1.33GB 1.83GB 2.32GB 2.82GB 3.31GB 3.81GB
A1.4 SO SÁNH GIỮA CÁC LỊCH TRÌNH TỶ LỆ CẮT TỈA KHÁC NHAU
Bảng A14: Đánh giá hiệu suất MC-SMoE
trên mô hình switch-base-32 với
lịch trình tỷ lệ cắt tỉa {linear, quadratic, cubic (của chúng tôi) }.
Chúng tôi làm nổi bật hiệu suất tốt nhất
trên tất cả baseline bằng chữ đậm .
Phương pháp COPA MultiRC
Linear 59.00 73 .83
Quadratic 61.00 73 .92
Cubic (của chúng tôi) 67.00 73 .98Phương pháp nén của chúng tôi cho MC-SMoE sử dụng lịch trình cubic
của tỷ lệ cắt tỉa, được áp dụng rộng rãi trong nhiều phương pháp hiện có
(Zhang et al., 2022; Zhu & Gupta, 2017; Sanh et al., 2020; Zafrir et al., 2021).
Chúng tôi tiến hành thí nghiệm so sánh mở rộng với hai lịch trình
tỷ lệ cắt tỉa khác, bao gồm lịch trình linear và quadratic,
trên các nhiệm vụ COPA và MultiRC. Kết quả, được hiển thị trong A14,
minh họa thứ tự hiệu suất củacubic (của chúng tôi) >quadratic >linear schedules.
Điều này có thể là do, trong các giai đoạn đầu của cắt tỉa, một
lịch trình cắt tỉa tích cực ít có khả năng mất thông tin hữu ích trong các trọng số;
trong khi điều ngược lại xảy ra trong các giai đoạn muộn hơn của cắt tỉa.
A2 CHI TIẾT KỸ THUẬT THÊM
Siêu tham số fine-tuning có giám sát Ngoài {kích thước batch, tốc độ học, số epoch }
thay đổi cho mỗi nhiệm vụ, chúng tôi giữ các siêu tham số fine-tuning khác cố định cho tất cả
các nhiệm vụ. Chúng được hiển thị trong Bảng A15.
Bảng A15: Siêu tham số fine-tuning của mô hình switch-base-32.
Siêu tham số Giá trị
Optimizer A DAM W
Adam ϵ 1e−6
Adam β (0.9,0.98)
Bước khởi động 16
Phân rã trọng số 0.01
Bộ lập lịch LR L INEAR DECAY
KDα 0.2
KDT 2.0
Chi tiết trong học zero-shot Chúng tôi đánh giá các cách tiếp cận và baseline của chúng tôi với mô hình fairseq-moe-
15btrong cài đặt học zero-shot. Cụ thể, chúng tôi sử dụng mô hình ngôn ngữ để chấm điểm riêng biệt
từng lựa chọn nhãn, và chọn cái có điểm số cao nhất làm dự đoán. Mặc dù chúng tôi
sử dụng các bộ huấn luyện, chúng chỉ được kết hợp khi cần thiết trong việc hợp nhất/nén, chẳng hạn như
A18

--- TRANG 19 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
khi tính toán tần suất sử dụng chuyên gia. Tóm lại, không có tối ưu hóa nào xảy ra ở bất kỳ giai đoạn nào
trong quá trình, tức là không có fine-tuning nào cả.
Siêu tham số nén ĐốivớiM-SMoE , chúng tôi chọn ngẫu nhiên 256 mẫu từ dữ liệu huấn luyện
để tính toán cả tần suất sử dụng chuyên gia và sự tương tự router-logits cho tất cả các nhiệm vụ. Đối với
giai đoạn nén trong MC-SMoE , theo Li et al. (2023), chúng tôi áp dụng bộ lập lịch tỷ lệ cắt tỉa cubic
để kiểm soát quá trình Spruning:
Pt=

1 0 ≤t <Ti,
PT+ (1− PT)
1−t−Ti−Tf
T −T i−Tf3
Ti≤t <T − T f,
PT o.w.,
trong đó Tlà tổng số bước. Tilà số lượng bước khởi động ban đầu. Tflà số lượng bước
làm lạnh cuối cùng. Chúng tôi đặt Tlà10000 ,Tilà400vàTjlà1600cho tất cả các nhiệm vụ.
Chưng cất kiến thức Trong đoạn này chúng tôi minh họa chi tiết về chưng cất kiến thức (KD)
được áp dụng trong cài đặt fine-tuning có giám sát trên tất cả các mô hình SMoE đã hợp nhất và nén để
phục hồi hiệu suất, bao gồm M-SMoE ,MC-SMoE của chúng tôi và tất cả baseline. Mục tiêu là buộc chúng,
tức là các học sinh, bắt chước các đầu ra từ mô hình SMoE đầy đủ, tức là giáo viên. Cụ thể, mục tiêu
huấn luyện có thể được công thức hóa như:
min
ΘE(x,y)∼D[L(x; Θ) + αLKD(x; Θ)],
trong đó giá trị của αđược cố định ở 0.2cho tất cả các nhiệm vụ. Llà loss cross-entropy giữa dự đoán
và các nhãn cứng cho trước, LKDlà loss KL divergence giữa dự đoán và các nhãn mềm của mô hình SMoE đầy đủ:
LKD=KLh
P
y|x; Θ(full)
∥ P(y|x; Θ)i
.
Hơn nữa, chúng tôi sử dụng nhiệt độ Ttrong KL divergence để kiểm soát độ mượt của phân phối
đầu ra cho cả mô hình học sinh và giáo viên, được định nghĩa là:
pi= exp( zi/T),
trong đó zilà điểm số logit cho lớp j, và Tđược cố định ở 2cho tất cả các nhiệm vụ.
Hàm tương tự router-weight Chúng tôi cung cấp mô tả chi tiết về hàm tương tự router-weight
trong đoạn này, điều này kém hơn so với router-logits được áp dụng của chúng tôi trong Phần 3.1.
Cụ thể, sự tương tự Sim(·,·)giữa các chuyên gia EivàEjtrong một lớp SMoE được tính bằng:
Sim(Ei,Ej) =cosine (Wi,∗
r,Wj,∗
r),
trong đó Wrlà trọng số router, và Wi,∗
rvàWj,∗
rlà các vector hàng trong đó.
Căn chỉnh hoán vị chuyên gia Chúng tôi cung cấp mô tả chi tiết về căn chỉnh hoán vị chuyên gia
của chúng tôi ở đây.
Đầu tiên, chúng tôi giới thiệu ma trận hoán vị P, là một ma trận vuông trong đó mỗi hàng và cột
có chính xác một phần tử là 1, với tất cả các phần tử khác là 0. Nó bảo tồn chức năng của
chuyên gia, một mạng feed-forward bao gồm hai lớp tuyến tính Win,Wout, và một hàm kích hoạt
act(·). Điều này là do phương trình Wout(act( Winx)) = WoutPT(act( PWinx))luôn đúng.
Thứ hai, chúng tôi tối thiểu hóa khoảng cách L2 giữa hai chuyên gia để căn chỉnh chúng. Xem xét các trọng số lớp đầu tiên,
được ký hiệu là Win, mỗi hàng của nó tương ứng với một đặc trưng ẩn cá nhân. Giả sử hai
hàng của ma trận này giống hệt nhau; trong trường hợp đó, chúng sẽ tạo ra cùng một đặc trưng, bỏ qua
bất kỳ bias nào hiện tại. Hơn nữa, nếu chúng tôi có [W(1)
in]i,:tương tự như [W(2)
in]j,:, nó hợp lý theo sau rằng các nơ-ron i
vàjsẽ có một kết nối hoặc liên kết. Áp dụng khái niệm này vào lớp thứ hai, Wout, quan sát này
dẫn chúng tôi đến xem xét một cách tiếp cận tối ưu hóa:
argminP


vec([ W(1)
in,W(1)
out])−vec([ PW(2)
in,W(2)
outPT])


2
= argmaxPD
W(1)
in,PW(2)
inE
F+D
W(1)
out,W(2)
outPTE
F
Cuối cùng, việc tối ưu hóa này tạo thành một "linear assignment problem" (LAP), có thể được giải
hiệu quả và thực tế bởi Hungarian Algorithm (Kuhn, 1955). Mã giả theo phong cách Python được
bao gồm trong A3.
A19

--- TRANG 20 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
A3 CHI TIẾT TRIỂN KHAI THÊM
Chúng tôi hiển thị một số mã giả để chứng minh việc triển khai M-SMoE được đề xuất của chúng tôi theo
phong cách giống PyTorch.
Chi tiết về hợp nhất chuyên gia trong một lớp Feed-Forward SMoE Trong các thí nghiệm của chúng tôi, bước cuối cùng
của việc hợp nhất bao gồm việc thay thế một chuyên gia trong một nhóm bằng trọng số có được. Thay vì cắt tỉa
các chuyên gia khác, chúng tôi chuyển hướng những cái còn lại trong nhóm đó đến chuyên gia mới được thay thế.
Việc triển khai này đảm bảo rằng chức năng định tuyến vẫn nhất quán. Dưới đây là mã giả theo phong cách PyTorch:
def merge_ffn_experts(
ffn: SwitchTransformersSparseMLP,
group_labels: torch.LongTensor,
usage_frequencies: torch.FloatTensor,
) -> SwitchTransformersSparseMLP:
# Mỗi chuyên gia có một nhãn nhóm và một tần suất sử dụng
assert len(group_labels) == len(usage_frequencies) == len(ffn.experts)
for label in group_labels.unique():
expert_indices = torch.where(group_labels == label)[0]
with torch.no_grad():
# Bước 1. Tính toán trung bình có trọng số tần suất sử dụng
fc1_weight = torch.sum(torch.stack(
[ffn.experts[f"expert_{expert_idx}"].fc1.weight *
usage_frequencies[expert_idx] for expert_idx in
expert_indices], dim=0
), dim=0) / torch.sum(usage_frequencies[expert_indices], dim=0)
fc2_weight = torch.sum(torch.stack(
[ffn.experts[f"expert_{expert_idx}"].fc2.weight *
usage_frequencies[expert_idx] for expert_idx in
expert_indices], dim=0
), dim=0) / torch.sum(usage_frequencies[expert_indices], dim=0)
# Bước 2. Sao chép trọng số vào chuyên gia đầu tiên trong nhóm
first_expert = ffn.experts[f"expert_{expert_indices[0]}"]
first_expert.fc1.weight.copy_(fc1_weight)
first_expert.fc2.weight.copy_(fc2_weight)
# Bước 3. Chuyển hướng các chuyên gia đã hợp nhất khác đến cái đầu tiên
for expert_idx in expert_indices[1:]:
ffn.experts[f"expert_{expert_idx}"] =
ffn.experts[f"expert_{expert_indices[0]}"]
return ffn
Chi tiết về giải quyết căn chỉnh hoán vị chuyên gia Ma trận hoán vị tối ưu để
căn chỉnh hai chuyên gia được tính bằng cách tối thiểu hóa khoảng cách L2 giữa các ma trận trọng số chuyên gia,
điều này tạo thành một linear assignment problem. Chúng tôi sử dụng SciPy (Virtanen et al., 2020) để giải quyết
vấn đề tối ưu hóa này, và mã giả theo phong cách Python được hiển thị dưới đây:
def compute_switch_permutation_by_weight_matching(
reference_mlp: SwitchTransformersDenseActDense,
target_mlp: SwitchTransformersDenseActDense,
) -> torch.Tensor:
lsa_cost_matrix = torch.mm(
reference_mlp.wi.weight.data, target_mlp.wi.weight.data.t()
) + torch.mm(
reference_mlp.wo.weight.data.t(), target_mlp.wo.weight.data)
_, perm = linear_sum_assignment(
lsa_cost_matrix.cpu().numpy(), maximize=True)
return torch.from_numpy(perm).to(lsa_cost_matrix.device)
A20

--- TRANG 21 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
A4 KẾT QUẢ THÍ NGHIỆM BỔ SUNG
A4.1 KẾT QUẢ NHÓM CỦA M-SM OE
Chúng tôi cung cấp kết quả nhóm chuyên gia của mô hình switch-base-32 trên tất cả tám nhiệm vụ bao gồm {SST2,
MRPC, MultiRC, COPA, WinoGrande, SQuAD, WikiQA, HotpotQA }ở đây, như được hiển thị trong Hình
A6 A7 A8 A9 A10 A11 A12 A13 tương ứng.
1 2 3 4 5 6 7 8 9 10 11
Chỉ số lớp02468101214Chỉ số nhóm
Nhóm chuyên gia trong các lớp SMoE - SST2
Hình A6: Kết quả nhóm chuyên gia của mô hình switch-base-32 trên nhiệm vụ SST2.
1 2 3 4 5 6 7 8 9 10 11
Chỉ số lớp02468101214Chỉ số nhóm
Nhóm chuyên gia trong các lớp SMoE - MRPC
Hình A7: Kết quả nhóm chuyên gia của mô hình switch-base-32 trên nhiệm vụ MRPC.
1 2 3 4 5 6 7 8 9 10 11
Chỉ số lớp024681012141618Chỉ số nhóm
Nhóm chuyên gia trong các lớp SMoE - MultiRC
Hình A8: Kết quả nhóm chuyên gia của mô hình switch-base-32 trên nhiệm vụ MultiRC.
A21

--- TRANG 22 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
1 2 3 4 5 6 7 8 9 10 11
Chỉ số lớp0246810Chỉ số nhóm
Nhóm chuyên gia trong các lớp SMoE - COPA
Hình A9: Kết quả nhóm chuyên gia của mô hình switch-base-32 trên nhiệm vụ COPA.
1 2 3 4 5 6 7 8 9 10 11
Chỉ số lớp024681012141618Chỉ số nhóm
Nhóm chuyên gia trong các lớp SMoE - WinoGrande
Hình A10: Kết quả nhóm chuyên gia của mô hình switch-base-32 trên nhiệm vụ WinoGrande.
1 2 3 4 5 6 7 8 9 10 11
Chỉ số lớp024681012Chỉ số nhóm
Nhóm chuyên gia trong các lớp SMoE - SQuAD
Hình A11: Kết quả nhóm chuyên gia của mô hình switch-base-32 trên nhiệm vụ SQuAD.
1 2 3 4 5 6 7 8 9 10 11
Chỉ số lớp0246810121416Chỉ số nhóm
Nhóm chuyên gia trong các lớp SMoE - WikiQA
Hình A12: Kết quả nhóm chuyên gia của mô hình switch-base-32 trên nhiệm vụ WikiQA.
A22

--- TRANG 23 ---
Được công bố như một bài báo hội nghị tại ICLR 2024
1 2 3 4 5 6 7 8 9 10 11
Chỉ số lớp024681012Chỉ số nhóm
Nhóm chuyên gia trong các lớp SMoE - HotpotQA
Hình A13: Kết quả nhóm chuyên gia của mô hình switch-base-32 trên nhiệm vụ HotpotQA.
A23
