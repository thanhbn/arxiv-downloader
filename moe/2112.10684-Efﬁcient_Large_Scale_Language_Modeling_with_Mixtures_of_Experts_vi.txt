# Mô hình Ngôn ngữ Lớn Hiệu quả với Hỗn hợp Chuyên gia

Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer,
Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li,
Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura,
Brian O'Horo, Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, Ves Stoyanov
Meta AI

## Tóm tắt

Các lớp Hỗn hợp Chuyên gia (MoEs) cho phép mở rộng quy mô hiệu quả của các mô hình ngôn ngữ thông qua tính toán có điều kiện. Bài báo này trình bày một nghiên cứu thực nghiệm chi tiết về cách các mô hình ngôn ngữ MoE tự hồi quy mở rộng quy mô so với các mô hình dày đặc trong nhiều tình huống khác nhau: mô hình hóa ngôn ngữ trong và ngoài miền, khởi tạo zero-shot và few-shot, và tinh chỉnh full-shot. Ngoại trừ việc tinh chỉnh, chúng tôi thấy MoEs hiệu quả hơn đáng kể về mặt tính toán. Với ngân sách huấn luyện khiêm tốn hơn, MoEs có thể đạt hiệu suất tương đương với các mô hình dày đặc sử dụng ít tính toán hơn 4 lần. Khoảng cách này thu hẹp ở quy mô lớn, nhưng mô hình MoE lớn nhất của chúng tôi (1.1T tham số) vẫn liên tục vượt trội so với mô hình dày đặc tương đương về tính toán (6.7B tham số). Nhìn chung, khoảng cách hiệu suất này thay đổi rất nhiều giữa các tác vụ và miền, cho thấy rằng các mô hình MoE và dày đặc tổng quát hóa khác nhau theo những cách đáng được nghiên cứu trong tương lai. Chúng tôi cung cấp công khai mã nguồn và các mô hình để sử dụng cho nghiên cứu.

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LMs) đạt được độ chính xác và khả năng tổng quát hóa đáng kể khi được tinh chỉnh cho các tác vụ NLP (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Raffel et al., 2020). Chúng cũng có khả năng học zero-shot và few-shot (Brown et al., 2020), với khả năng tổng quát hóa cho các tác vụ không được thấy trong quá trình huấn luyện. Một cách đáng tin cậy để cải thiện độ chính xác của LM trong tất cả các tình huống này là bằng cách mở rộng quy mô: tăng số lượng tham số và lượng tính toán được sử dụng trong quá trình huấn luyện và suy luận (Raffel et al., 2020; Brown et al., 2020; Fedus et al., 2021). Trên thực tế, một số thuộc tính tổng quát hóa chỉ xuất hiện trong các mô hình rất lớn, bao gồm cả việc cải thiện đáng kể học zero-shot và few-shot (Brown et al., 2020).

Tuy nhiên, sự tăng trưởng tương ứng trong tài nguyên tính toán cần thiết để huấn luyện các mô hình ngôn ngữ tiên tiến là một rào cản đối với nhiều người trong cộng đồng nghiên cứu (Schwartz et al., 2019). Cũng có mối lo ngại về chi phí môi trường liên quan đến việc huấn luyện và triển khai các mô hình như vậy (Strubell et al., 2019; Gupta et al., 2021; Bender et al., 2021; Patterson et al., 2021) thúc đẩy nghiên cứu về các thiết kế mô hình hiệu quả hơn (Lepikhin et al., 2021; Fedus et al., 2021; Lewis et al., 2021).

Các mô hình thưa thớt cho phép tăng số lượng tham số có thể học được mà không có chi phí tính toán liên quan. Ví dụ, hỗn hợp chuyên gia có cổng thưa thớt (MoE) (Lepikhin et al., 2021) đã được sử dụng thành công cho mô hình hóa ngôn ngữ và dịch máy (Lepikhin et al., 2021; Lewis et al., 2021; Roller et al., 2021), nhưng vẫn chưa được chứng minh hiệu quả cho tinh chỉnh (Fedus et al., 2021) cũng như học zero-shot và few-shot. Chúng tôi giả thuyết rằng các mô hình thưa thớt có độ chính xác tương đương với các mô hình dày đặc nhưng với dấu chân tính toán thấp hơn nhiều. Để đo lường khẳng định này, chúng tôi huấn luyện các mô hình ngôn ngữ MoE và dày đặc truyền thống có kích thước từ vài trăm triệu tham số đến hơn một nghìn tỷ tham số và trình bày so sánh thực nghiệm cẩn thận của các mô hình này trên các tác vụ hạ lưu trong các tình huống zero-shot, few-shot và được giám sát đầy đủ.

Như được hiển thị trong Hình 1, chúng tôi thấy rằng các mô hình MoE thực sự có thể đạt được hiệu suất tác vụ hạ lưu tương tự như các mô hình dày đặc với một phần nhỏ tính toán. Đối với các mô hình có ngân sách tính toán tương đối khiêm tốn, một mô hình MoE có thể hoạt động ngang bằng với một mô hình dày đặc yêu cầu gần bốn lần tính toán. Hiệu suất tác vụ hạ lưu cải thiện theo quy mô cho cả mô hình MoE và mô hình dày đặc. Trong khi chúng tôi quan sát thấy rằng khoảng cách hiệu suất thu hẹp khi chúng tôi tăng kích thước mô hình, ngay cả ở ngân sách tính toán lớn hơn (5000 GPU ngày), mô hình MoE lớn nhất của chúng tôi (1.1T tham số) vẫn vượt trội so với mô hình dày đặc với chi phí tính toán tương tự (6.7B tham số). Chúng tôi tiếp tục so sánh và đối chiếu hiệu suất của các mô hình dày đặc và thưa thớt với các đặc trưng tính toán tương tự và quan sát một số biến đổi hiệu suất giữa các tác vụ và miền, cho thấy đây là một lĩnh vực thú vị cho nghiên cứu tương lai. Tóm lại, đóng góp của chúng tôi là:

• Chúng tôi trình bày một nghiên cứu toàn diện về các mô hình thưa thớt cho học zero và few-shot ở quy mô lớn;
• Chúng tôi chứng minh rằng ngay cả ở quy mô lớn, các mô hình MoE thưa thớt có thể mang lại hiệu suất zero và few-shot cạnh tranh với một phần nhỏ tính toán cho huấn luyện và suy luận mô hình;
• Chúng tôi quan sát một số khác biệt trong cách các mô hình dày đặc và thưa thớt tổng quát hóa ở quy mô lớn cho thấy hành vi bổ sung có thể là một hướng nghiên cứu tương lai thú vị.

## 2 Bối cảnh và Công trình Liên quan

### 2.1 Mô hình Ngôn ngữ Lớn / GPT-3

Tiến bộ trong lĩnh vực NLP được thúc đẩy bởi các Mô hình Ngôn ngữ (LMs) ngày càng lớn được huấn luyện trước trên các tập dữ liệu văn bản lớn. Trong khi nhiều biến thể đã được đề xuất, các LM như vậy chủ yếu dựa trên kiến trúc transformer (Vaswani et al., 2017). Các mô hình được huấn luyện trước bằng cách ẩn các phần của đầu vào: dự đoán từ tiếp theo tuần tự từ trái sang phải, che từ trong văn bản (Devlin et al., 2019; Liu et al., 2019), hoặc làm nhiễu và/hoặc che các đoạn (Lewis et al., 2020; Raffel et al., 2020). Các mô hình kết quả có thể được nhanh chóng thích ứng để thực hiện các tác vụ mới với độ chính xác cao bằng cách tinh chỉnh trên dữ liệu được giám sát (Devlin et al., 2019; Liu et al., 2019).

Gần đây, GPT-3 (Brown et al., 2020) đã chứng minh rằng các LM lớn có thể thực hiện học zero-shot và few-shot mà không cần tinh chỉnh thông qua học trong ngữ cảnh. Đáng chú ý, nhiều hành vi học trong ngữ cảnh zero-shot và few-shot này xuất hiện hoặc khuếch đại ở quy mô lớn. Đồng thời với công trình của chúng tôi, Rae et al. (2022) và Smith et al. (2022) tiếp tục khám phá việc mở rộng quy mô các mô hình ngôn ngữ dày đặc.

### 2.2 Mô hình thưa thớt

Một nhược điểm của việc mở rộng quy mô mô hình dày đặc là nó ngày càng tốn kém về mặt tính toán. Để tăng dung lượng mô hình hiệu quả hơn, các chiến lược tính toán có điều kiện đã được phát triển (Bengio et al., 2013; Davis và Arel, 2013; Cho và Bengio, 2014; Bengio et al., 2015), trong đó mỗi đầu vào kích hoạt một tập con của mô hình. Công trình gần đây (Lewis et al., 2021; Lepikhin et al., 2021; Fedus et al., 2021; Fan et al., 2021) đã nghiên cứu các chiến lược tính toán có điều kiện khác nhau hoạt động tốt với các mô hình Transformer cho các tác vụ ngôn ngữ tự nhiên. Trong công trình này, chúng tôi tập trung vào các mô hình Hỗn hợp Chuyên gia Có Cổng Thưa thớt (MoE) (Shazeer et al., 2017; Lepikhin et al., 2021). Các mô hình MoE thưa thớt thay thế khối mạng feedforward dày đặc trong mỗi lớp Transformer xen kẽ bằng một lớp MoE. Lớp MoE có một cổng định tuyến học cách ánh xạ token nào đến tập hợp chuyên gia nào (chúng tôi sử dụng top-2 chuyên gia). Để đảm bảo khả năng mở rộng và hiệu quả huấn luyện, cũng phổ biến khi bao gồm một thuật ngữ mất mát cổng có trọng số như trong Lepikhin et al. (2021) vào mất mát entropy chéo để khuyến khích các token được phân phối đều cho các chuyên gia. Đồng thời với công trình của chúng tôi, Du et al. (2021), Rajbhandari et al. (2022) và Clark et al. (2022) cũng nghiên cứu về mở rộng quy mô MoE.

### 2.3 Học Zero-shot và Few-shot

Các công trình gần đây (Schick và Schütze, 2021a; Radford et al., 2019) đã trực tiếp đánh giá LM trên các tác vụ chưa thấy một cách thành công (học zero-shot), bằng cách chuyển đổi đầu vào tác vụ của họ thành các tác vụ hoàn thiện prompt theo kiểu cloze. Điều này trái ngược với phương pháp truyền thống là bổ sung LM với các đầu cụ thể theo tác vụ, tiếp theo là tinh chỉnh được giám sát (Devlin et al., 2019; Raffel et al., 2020). Tiếp theo, GPT-3 (Brown et al., 2020) đã chứng minh rằng việc khởi tạo LM với một vài ví dụ đầu vào-đầu ra (học few-shot) trước khi gợi ý cẩn thận có thể cải thiện hiệu suất tác vụ, tăng theo quy mô mô hình mà không cần tinh chỉnh nào, và điều này đã tạo ra các nguồn tài nguyên mới cho kỹ thuật prompt (Bach et al., 2022). Trong bài báo này, chúng tôi đối chiếu hiệu suất zero-shot, few-shot và tinh chỉnh được giám sát đầy đủ của các mô hình dày đặc và MoE. Cuối cùng, Schick và Schütze (2021b) thực hiện học few-shot bằng cách tinh chỉnh few-shot sử dụng huấn luyện khai thác mẫu, có hiệu quả có thể được cải thiện bằng cách thực hiện tinh chỉnh một phần của một số lượng nhỏ tham số cụ thể theo tác vụ bổ sung thay vì (Lester et al., 2021; Li và Liang, 2021; Houlsby et al., 2019).

### 2.4 Huấn luyện quy mô lớn

Nhiều mô hình chúng tôi xem xét trong công trình này quá lớn để được huấn luyện bằng các kỹ thuật song song dữ liệu tiêu chuẩn, vì việc lưu trữ tham số sẽ vượt quá bộ nhớ có thể sử dụng của một GPU duy nhất. Chúng tôi áp dụng một số kỹ thuật để làm cho các mô hình này khả thi để huấn luyện, bao gồm huấn luyện FP16 thuần túy, checkpoint kích hoạt và huấn luyện song song dữ liệu được chia sẻ hoàn toàn. Các kỹ thuật này được mô tả chi tiết hơn trong Phụ lục F.

## 3 Thiết lập Thực nghiệm

### 3.1 Mô hình

Chúng tôi huấn luyện các mô hình transformer tự hồi quy (chỉ bộ giải mã) khớp gần với các kích thước và kiến trúc được khám phá trong Brown et al. (2020). Kích thước mô hình được tóm tắt trong Bảng 1. Chúng tôi sử dụng các khối transformer tiền chuẩn hóa (Baevski và Auli, 2019; Child et al., 2019) và kích hoạt GELU (Hendrycks và Gimpel, 2016). Chúng tôi khác với Brown et al. (2020) trong hai cách: (1) chúng tôi chỉ sử dụng attention dày đặc, trong khi họ xen kẽ giữa attention dày đặc và thưa thớt cục bộ có băng tần; và (2) chúng tôi huấn luyện các mô hình của chúng tôi với embedding vị trí sinusoidal, theo Shortformer (Press et al., 2020).

Chúng tôi cũng huấn luyện các mô hình MoE phản chiếu cấu hình mô hình dày đặc của chúng tôi (xem tập cột thứ ba trong Bảng 1), để các so sánh được khớp gần đúng về số lượng phép toán dấu phẩy động (FLOPs). Các mô hình MoE của chúng tôi theo thiết kế được đề xuất trong Lepikhin et al. (2021) với các lớp dày đặc và chuyên gia xen kẽ và lựa chọn chuyên gia top-2. Chúng tôi sử dụng 512 chuyên gia trong mỗi lớp chuyên gia (E = 512). Mỗi chuyên gia có dung lượng CB/E token, trong đó C là hệ số dung lượng mà chúng tôi đặt thành 2 và B là tổng kích thước batch tính bằng token. Dung lượng đề cập đến số lượng token tối đa được định tuyến đến mỗi chuyên gia. Một khi một chuyên gia đạt dung lượng cho một batch nhất định, các token bổ sung được coi là "tràn" với các biểu diễn của chúng được truyền qua thông qua kết nối dư.

Fedus et al. (2021) báo cáo tính không ổn định khi huấn luyện các mô hình MoE lớn và đề xuất tái quy mô trọng số mô hình ban đầu, điều mà chúng tôi không thấy cần thiết. Thay vào đó, chúng tôi quan sát rằng các tham số chuyên gia có kích thước batch nhỏ hơn E lần so với các tham số dày đặc (song song dữ liệu) và tái quy mô gradient chuyên gia tương ứng bằng hệ số 1/√E. Việc tái quy mô này phù hợp với lý thuyết cho thấy rằng việc tăng E lần kích thước batch nên đi kèm với việc tăng √E tỷ lệ học (Krizhevsky, 2014).

Theo Brown et al. (2020), chúng tôi huấn luyện các mô hình của chúng tôi trong 300B token với kích thước ngữ cảnh (độ dài chuỗi) là 2048 token. Kích thước batch và tỷ lệ học được đặt theo kích thước mô hình theo Brown et al. (2020). Chúng tôi làm ấm tuyến tính tỷ lệ học từ 0 trong 375M token đầu tiên và phân rã tuyến tính trở lại 0 trong các token còn lại. Chúng tôi sử dụng bộ tối ưu Adam (Kingma và Ba, 2015) với β₁ = 0.9, β₂ = 0.98, ε = 10⁻⁸, weight decay 0.01 và dropout 0.1.

Chúng tôi huấn luyện các mô hình của chúng tôi trong PyTorch (Paszke et al., 2017) sử dụng FAIRSEQ (Ott et al., 2019).

### 3.2 Dữ liệu huấn luyện trước

Chúng tôi huấn luyện trước các mô hình của chúng tôi trên sự kết hợp của sáu tập dữ liệu tiếng Anh, bao gồm năm tập dữ liệu được sử dụng để huấn luyện trước RoBERTa (Liu et al., 2019) và tập con tiếng Anh của CC100, tổng cộng 112B token tương ứng với 453GB:

• BookCorpus (Zhu et al., 2019) bao gồm hơn 10K sách chưa xuất bản (4GB);
• English Wikipedia, loại trừ danh sách, bảng và tiêu đề (12GB);
• CC-News (Nagel, 2016) chứa 63 triệu bài báo tiếng Anh được thu thập từ tháng 9 năm 2016 đến tháng 2 năm 2019 (76GB);
• OpenWebText (Gokaslan và Cohen, 2019), một tái tạo nguồn mở của tập dữ liệu WebText được sử dụng để huấn luyện GPT-2 (38GB);
• CC-Stories (Trinh và Le, 2018) chứa một tập con của dữ liệu CommonCrawl được lọc để phù hợp với phong cách giống câu chuyện của các lược đồ Winograd (31GB);
• English CC100 (Wenzek et al., 2020), một tập dữ liệu được trích xuất từ các ảnh chụp CommonCrawl từ tháng 1 năm 2018 đến tháng 12 năm 2018, được lọc để phù hợp với phong cách của Wikipedia (292GB).

Chúng tôi mã hóa dữ liệu của chúng tôi bằng cách sử dụng cùng Mã hóa Cặp Byte (BPE) như GPT-2 (Radford et al., 2019) và RoBERTa (Liu et al., 2019) với từ vựng 50K đơn vị từ phụ.

### 3.3 Đánh giá

Chúng tôi đánh giá các mô hình về perplexity trong miền và ngoài miền của chúng, cũng như hiệu suất tác vụ hạ lưu.

#### 3.3.1 Đánh giá Perplexity

Trước tiên, chúng tôi đánh giá các mô hình của chúng tôi về khả năng dự đoán token tiếp theo trong một chuỗi được đo bằng perplexity. Tương tự như huấn luyện, chúng tôi nối tất cả các tài liệu trong một tập dữ liệu nhất định bằng cách sử dụng các dòng trống làm dấu phân cách, chia chuỗi kết quả thành các khối 2048 token không chồng chéo và chấm điểm mỗi khối một cách độc lập.

Chúng tôi đánh giá và báo cáo perplexity trong cả hai tình huống trong miền và ngoài miền. Trong miền, chúng tôi lấy mẫu một tập con giữ lại của dữ liệu huấn luyện trước kết hợp (§3.2). Đối với ngoài miền, chúng tôi sử dụng dữ liệu từ The Pile (Gao et al., 2021), một tập dữ liệu công khai kết hợp dữ liệu từ 22 nguồn đa dạng (ví dụ: ArXiv, Github, OpenSubtitles, v.v.). Chúng tôi báo cáo perplexities trên tập kiểm tra chính thức của mỗi tập con riêng lẻ, cũng như trung bình trên tất cả các tập con.

#### 3.3.2 Đánh giá Hạ lưu

Chúng tôi nhắm mục tiêu các mô hình có thể thực hiện tốt các tác vụ hạ lưu. Công trình gần đây cho thấy rằng hiệu suất perplexity tốt không phải lúc nào cũng phù hợp với hiệu suất tốt trên các tác vụ hạ lưu (Tay et al., 2021). Do đó, chúng tôi đánh giá các mô hình của chúng tôi tương ứng.

**Benchmark.** Chúng tôi đánh giá các mô hình của chúng tôi trên một tập con các tác vụ được xem xét trong Brown et al. (2020). Vì hiệu suất GPT-3 thay đổi rất nhiều giữa các tác vụ và kích thước mô hình, chúng tôi tập trung vào các tác vụ mà GPT-3 đã chứng minh được những cải thiện nhất quán từ việc mở rộng quy mô, hoặc những cải thiện nhất quán từ tình huống zero-shot sang few-shot.

**Few-shot:** chúng tôi sử dụng WinoGrande (Sakaguchi et al., 2020), StoryCloze (Mostafazadeh et al., 2016) và OpenBookQA (Mihaylov et al., 2018), là những tác vụ không tạo sinh duy nhất mà Brown et al. (2020) báo cáo những cải thiện có ý nghĩa so với zero-shot ở quy mô của chúng tôi. Chúng tôi loại trừ SuperGLUE, vì chúng tôi không thể tái tạo kết quả được báo cáo trong Brown et al. (2020) bằng cách sử dụng API GPT-3 công khai.

**Zero-shot:** ngoài 3 tác vụ few-shot, chúng tôi đánh giá trên ReCoRD (Zhang et al., 2018), HellaSwag (Zellers et al., 2019) và PIQA (Bisk et al., 2020). Brown et al. (2020) báo cáo kết quả mạnh mẽ và cải thiện đơn điệu từ việc mở rộng quy mô trên các tác vụ này.

**Giao thức đánh giá.** Theo Brown et al. (2020), chúng tôi báo cáo kết quả trên tập phát triển cho tất cả các tác vụ ngoại trừ OpenBookQA và StoryCloze, mà chúng tôi sử dụng tập kiểm tra. Đối với học few-shot, chúng tôi báo cáo kết quả trung bình trên 25 lần chạy, lấy mẫu ngẫu nhiên một tập các ví dụ few-shot khác nhau từ tập huấn luyện mỗi lần. Đối với khởi tạo, chúng tôi tiếp tục xáo trộn các ví dụ few-shot cho mỗi thực thể kiểm tra. Theo Brown et al. (2020), chúng tôi sử dụng k=50 ví dụ few-shot cho WinoGrande, k=70 cho StoryCloze và k=100 cho OpenBookQA. Trong các trường hợp mà điều này vượt quá độ dài ngữ cảnh tối đa cho mô hình, chúng tôi cắt ngắn prompt giữ số lượng tối đa các ví dụ đầy đủ phù hợp.

**Baseline.** Chúng tôi so sánh với các số liệu GPT-3 đã công bố (Brown et al., 2020) làm baseline chính của chúng tôi. Để xác thực khung thực nghiệm của chúng tôi, chúng tôi cũng đánh giá GPT-3 tận dụng API OpenAI bằng cách sử dụng mã đánh giá và cài đặt của chúng tôi. Thật không may, sự tương ứng giữa kích thước mô hình và tên mô hình trong API OpenAI không được công bố. Chúng tôi theo các công trình đã xuất bản khác (Gao et al., 2021) và đoán sự tương ứng dựa trên kết quả của chúng tôi từ API công khai so với kết quả trong Brown et al. (2020) (xem §4.2.1).

**Phương pháp.** Chúng tôi so sánh cả phương pháp dựa trên khởi tạo và tinh chỉnh.

• **Khởi tạo:** Chúng tôi sử dụng một mô hình ngôn ngữ để chấm điểm riêng biệt từng lựa chọn nhãn bằng cách sử dụng cùng các mẫu như Brown et al. (2020), và chọn cái có điểm cao nhất. Đối với học few-shot, chúng tôi sử dụng một dòng mới duy nhất để phân tách các ví dụ. Hàm chấm điểm của chúng tôi theo mô tả trong Brown et al. (2020):

- Đối với **WinoGrande**, chúng tôi lấy log-likelihood của hậu tố chung của các ứng viên khác nhau.
- Đối với **OpenBookQA**, chúng tôi chuẩn hóa bằng xác suất vô điều kiện của mỗi ứng viên bằng cách lấy p(completion|context)/p(completion|answer_context), trong đó chúng tôi sử dụng chuỗi "Answer: " làm answer_context.
- Đối với **ReCoRD**, chúng tôi lấy tổng của log-probabilities từng token.
- Đối với **tất cả các tác vụ khác**, chúng tôi lấy trung bình của log-probabilities từng token, bỏ qua tiền tố chung của các ứng viên khác nhau.

• **Tinh chỉnh:** Mặc dù tinh chỉnh được giám sát của các LM được huấn luyện trước trên dữ liệu huấn luyện cụ thể theo tác vụ, D, yêu cầu cập nhật và lưu trữ tất cả các tham số mô hình cho mỗi tác vụ, quá trình này thường tạo ra những cải thiện hiệu suất cụ thể theo tác vụ đáng kể. Chúng tôi đối chiếu hiệu suất tinh chỉnh của các mô hình thưa thớt và các đối tác dày đặc của chúng theo (Radford et al., 2018), áp dụng một lớp tuyến tính cụ thể theo tác vụ bổ sung Wy trên biểu diễn từ khối transformer cuối cùng cho mỗi ứng viên đầu vào riêng biệt, tiếp theo là một lớp softmax. Chúng tôi tinh chỉnh tất cả các tham số mô hình sử dụng toàn bộ tập huấn luyện (học được giám sát đầy đủ). Ngoài các tác vụ zero-shot của chúng tôi, chúng tôi cũng đánh giá trên 3 tác vụ phân loại được sử dụng rộng rãi: BoolQ (Clark et al., 2019), MNLI (Williams et al., 2018) và SST-2 (Socher et al., 2013). Thêm chi tiết trong Phụ lục B.

#### 3.3.3 Hệ số tăng tốc MoE

Chúng tôi giả thuyết rằng các mô hình thưa thớt có thể đạt được hiệu suất tương đương với ngân sách tính toán nhỏ hơn. Như vậy, việc đo lường mức độ hiệu quả hơn của MoE trong việc đạt được một mức hiệu suất cụ thể so với các mô hình dày đặc là có thông tin. Chúng tôi ước tính bao nhiêu FLOP c(t) mà mô hình cần để đạt được hiệu suất t trong một tác vụ cụ thể (được đo bằng perplexity cho mô hình hóa ngôn ngữ và độ chính xác cho các tác vụ hạ lưu) sử dụng một mô hình MoE hoặc một mô hình dày đặc. Cho rằng chúng tôi chỉ có các quan sát rời rạc, chúng tôi ước tính các giá trị bị thiếu chính xác bằng cách nội suy trên thang logarit như sau:

c(t) = exp(log c_lo(t) + r(log c_hi(t) - log c_lo(t)))

trong đó r = (t - t_lo)/(t_hi - t_lo), t_lo và t_hi là hiệu suất gần nhất với t từ các mô hình có sẵn trong khi thấp hơn và cao hơn t, tương ứng, và c_lo(t) và c_hi(t) là chi phí huấn luyện tương ứng của chúng tính bằng ZFLOP.

Việc nội suy cho chúng tôi các mức hiệu suất phù hợp cho các mô hình dày đặc và MoE. Chúng tôi sử dụng chúng để tính hệ số tăng tốc MoE c_dense(t)/c_moe(t). Ví dụ, nếu một mô hình dày đặc yêu cầu 20 ZFLOP đạt được hiệu suất 90% trên một tác vụ nhất định và một mô hình MoE yêu cầu 5 ZFLOP đạt được cùng hiệu suất, thì công thức tạo ra hệ số tiết kiệm là 4. Chúng tôi hình dung đường cong tiết kiệm sử dụng c_dense(t) trong trục x, điều này cho phép chúng tôi đối chiếu tăng tốc trong các tác vụ khác nhau ở thang độ tương đương.

## 4 Kết quả và Phân tích

### 4.1 Perplexity mô hình hóa ngôn ngữ

Chúng tôi báo cáo kết quả perplexity của chúng tôi trong Hình 2, và hình dung các đường cong tăng tốc trong các tập con đại diện của The Pile (Gao et al., 2021) trong Hình 3a. Tham khảo Phụ lục A để biết kết quả đầy đủ cho tất cả 22 tập con của The Pile.

Chúng tôi quan sát thấy rằng tất cả các mô hình MoE đều vượt trội so với các đối tác dày đặc của chúng trong tất cả các tập dữ liệu, nhưng lợi thế của chúng thay đổi rất nhiều giữa các miền và mô hình. MoE hiệu quả nhất khi được đánh giá trong miền, nơi chúng có thể phù hợp với hiệu suất của các mô hình dày đặc được huấn luyện với nhiều tính toán hơn 8-16 lần (xem Hình 1). Sự cải thiện khiêm tốn hơn trong các tình huống ngoài miền, mang lại tăng tốc 2-4 trên The Pile. Điều này được phản ánh trong Hình 2, nơi khoảng cách giữa các đường cong MoE và dày đặc nhỏ hơn đáng kể trong các tình huống ngoài miền.

Hơn nữa, lợi thế của MoE so với các mô hình dày đặc giảm ở quy mô: MoE cần ít tính toán hơn 4 lần để phù hợp với hiệu suất của các mô hình dày đặc được huấn luyện với 2-6 ZFLOP, nhưng tăng tốc là 2 đối với các mô hình dày đặc được huấn luyện với 30 ZFLOP.

Chúng tôi cũng quan sát sự khác biệt lớn giữa các tập con của The Pile, tương ứng với các miền khác nhau. Như được hiển thị trong Hình 3a, MoE có được những tăng tốc lớn nhất trong các tập con gần nhất với kho dữ liệu huấn luyện (ví dụ: CommonCrawl). Những lợi ích hiệu quả khiêm tốn hơn nhưng vẫn đáng kể đối với các miền khác như ArXiv và OpenSubtitles. Mô hình MoE lớn nhất của chúng tôi chỉ vượt trội so với đối tác dày đặc của nó trên DM Mathematics (7.63 so với 7.66 perplexity), có thể cho là rất khác biệt so với miền huấn luyện.

### 4.2 Đánh giá tác vụ hạ lưu

#### 4.2.1 Học Zero-shot

Chúng tôi báo cáo kết quả zero-shot trong Bảng 2, và hình dung cách các họ mô hình khác nhau mở rộng quy mô trong Hình 4. Các mô hình dày đặc của chúng tôi hoạt động ngang bằng với các đối tác GPT-3 của chúng. Điều này nhất quán giữa các tác vụ khác nhau, với các mô hình của chúng tôi làm tốt hơn một chút trung bình. Do đó, chúng tôi có thể phù hợp với Brown et al. (2020) bất chấp một số khác biệt đáng chú ý trong thiết lập của chúng tôi (ví dụ: kho dữ liệu huấn luyện khác nhau), thiết lập một baseline vững chắc để đánh giá các mô hình MoE trên các tác vụ hạ lưu. Tương tự, khi sử dụng mã của chúng tôi để đánh giá backend API GPT-3 mạnh nhất (davinci), chúng tôi có được những con số tái tạo những số liệu được báo cáo trong bài báo gốc cho mô hình lớn nhất của họ, điều này củng cố rằng cài đặt đánh giá của chúng tôi tương đương với Brown et al. (2020).

Như với mô hình hóa ngôn ngữ, MoE vượt trội so với các đối tác dày đặc của chúng cho tất cả các tập dữ liệu và kích thước mô hình. Nhưng, một lần nữa, chúng tôi thấy lợi thế thu hẹp ở quy mô như được minh họa trong Hình 4. Tương tự như những khác biệt miền trong mô hình hóa ngôn ngữ, chúng tôi quan sát những khác biệt giữa các tác vụ hạ lưu. Như được hiển thị trong Hình 3b, MoE có được những tăng tốc đáng kể trong các tác vụ nhất định như HellaSwag và PIQA, nhưng sự cải thiện này khiêm tốn hơn trong các tác vụ khác như ReCoRD và Winogrande.

#### 4.2.2 Học Few-shot

Chúng tôi báo cáo kết quả few-shot của chúng tôi trong Bảng 3 và vẽ sự cải thiện tương ứng so với zero-shot trong Hình 5.

Các baseline dày đặc của chúng tôi hoạt động ngang bằng hoặc hơi tốt hơn GPT-3. Chúng tôi quan sát thấy rằng sự cải thiện so với zero-shot lớn hơn đối với các mô hình lớn hơn, hỗ trợ thêm rằng các khả năng nhất định trong các mô hình ngôn ngữ xuất hiện ở quy mô (Brown et al., 2020). Cuối cùng, chúng tôi thấy rằng các mô hình MoE lớn hơn của chúng tôi cũng hưởng lợi từ học few-shot, vượt trội so với các đối tác dày đặc của chúng trong tất cả các điều kiện. Tuy nhiên, những cải thiện từ zero-shot sang few-shot nhỏ hơn đối với các mô hình MoE so với các đối tác dày đặc của chúng. Ví dụ, trung bình cho mô hình dày đặc 6.7B cải thiện 3.6 điểm lên 69.3 từ zero-shot sang few-shot, trong khi mô hình 1.1T tương ứng cải thiện 2.3 điểm cho 70.1.

#### 4.2.3 Tinh chỉnh Được giám sát

Bảng 4 đối chiếu hiệu suất tinh chỉnh đầy đủ của các mô hình MoE với các đối tác dày đặc của chúng trên 8 tập dữ liệu, sử dụng độ chính xác zero-shot làm baseline để tham khảo. Chúng tôi không tinh chỉnh các mô hình dày đặc 6.7B và 13B và các mô hình MoE 1.1T, do nhu cầu tài nguyên cao của chúng. Như mong đợi, tinh chỉnh được giám sát mang lại lợi ích hiệu suất đáng kể cho tất cả các mô hình dày đặc trên tất cả các tập dữ liệu, so với hiệu suất zero-shot. Ngược lại, mặc dù tinh chỉnh các mô hình MoE tạo ra lợi ích đáng kể cho Storycloze, BoolQ, SST-2, MNLI và một số cải thiện trên OpenBookQA, nó dẫn đến hiệu suất tệ hơn cho HellaSwag, PIQA và Winogrande. Đối với các trường hợp mà chúng tôi thấy cải thiện, độ chính xác của các mô hình MoE được tinh chỉnh tiếp cận với các mô hình dày đặc tương ứng của chúng.

Đối với so sánh này, chúng tôi tinh chỉnh các mô hình MoE chính xác như chúng tôi làm với các mô hình dày đặc. Trong khi các mô hình MoE có thể hưởng lợi từ các phương pháp tinh chỉnh thay thế, ví dụ, tinh chỉnh có chọn lọc các tham số chuyên gia hoặc không chuyên gia, chúng tôi để việc khám phá như vậy cho công trình tương lai.

## 5 Kết luận

Chúng tôi trình bày kết quả cho việc mở rộng quy mô các Mô hình Ngôn ngữ thưa thớt lên đến 1.1T tham số. Chúng tôi quan sát thấy rằng cho đến quy mô này, các mô hình thưa thớt cung cấp hiệu suất tốt hơn so với đánh đổi tính toán khi so sánh với các đối tác dày đặc của chúng cho mô hình hóa ngôn ngữ, học zero- và few-shot. Trong khi khoảng cách bắt đầu thu hẹp ở quy mô, mô hình thưa thớt lớn nhất của chúng tôi vượt trội so với đối tác dày đặc của nó trong đó cái sau yêu cầu gấp đôi tính toán. Những kết quả này xác nhận rằng các mô hình MoE thưa thớt có thể cung cấp một thay thế cho các kiến trúc dày đặc được sử dụng rộng rãi tiết kiệm tính toán và giảm tiêu thụ năng lượng mô hình.

## Cân nhắc Đạo đức

Công trình trước đây (Sheng et al., 2019; Bordia và Bowman, 2019; Nadeem et al., 2021; de Vassimon Manela et al., 2021) đã quan sát thấy rằng các mô hình ngôn ngữ hấp thụ thiên lệch và độc tính được thể hiện trong dữ liệu huấn luyện. Để hiểu rõ hơn về những tác hại tiềm ẩn của các mô hình của chúng tôi về mặt này, chúng tôi đã đánh giá chúng trên StereoSet (Nadeem et al., 2021) và CrowS-Pairs (Nangia et al., 2020), và báo cáo kết quả của chúng tôi trong Phụ lục C. Kết quả của chúng tôi cho thấy rằng tỷ lệ phần trăm thiên lệch và khuôn mẫu trong các mô hình dày đặc và MoE là tương đương, đặc biệt là ở quy mô. Hơn nữa, nói chung, chúng tôi lưu ý hiệu suất tệ hơn (nhiều thiên lệch/khuôn mẫu hơn) ở quy mô lớn hơn. Quan sát này chỉ ra cần nhiều nghiên cứu hơn để giảm thiểu hành vi như vậy. Tuy nhiên, một cách trực quan, chúng tôi tin rằng các mô hình thưa thớt có thể vốn dĩ có thể kiểm soát hơn - ví dụ thiết kế các chuyên gia cụ thể - so với các mô hình dày đặc. Chúng tôi để dành dòng điều tra này cho nghiên cứu tương lai.

Một mối quan tâm khác của việc mở rộng quy mô các mô hình ngôn ngữ là việc sử dụng năng lượng và tác động môi trường liên quan cần thiết để huấn luyện, mà chúng tôi thảo luận chi tiết trong Phụ lục D. Tuy nhiên, công trình của chúng tôi cho thấy rằng MoE có thể là một thay thế hiệu quả hơn về mặt tính toán so với các mô hình dày đặc truyền thống, điều này có thể giảm bớt tác động môi trường của các nỗ lực mở rộng quy mô trong tương lai. Hơn nữa, bằng cách phát hành tất cả các mô hình ngôn ngữ được huấn luyện trước của chúng tôi, chúng tôi tin rằng chúng tôi đã giảm bớt một số gánh nặng khám phá cho cộng đồng và môi trường, cho phép bù đắp hiệu quả hơn cho các nhà nghiên cứu khác.

Theo tinh thần minh bạch và cho phép khả năng sao chép và trách nhiệm tối đa, chúng tôi bao gồm thẻ dữ liệu và mô hình cùng với mã của chúng tôi.

## Hạn chế

Nghiên cứu của chúng tôi chỉ giới hạn ở một cấu hình MoE cụ thể. Cụ thể, các mô hình thưa thớt và dày đặc của chúng tôi sử dụng cùng các siêu tham số và cấu trúc mô hình, theo sát GPT-3. Tuy nhiên, có thể cấu hình này không tối ưu để mở rộng quy mô các mô hình MoE. Tương tự, chúng tôi không khám phá các siêu tham số cụ thể của MoE khác nhau, chẳng hạn như số lượng chuyên gia. Cuối cùng, trong khi công trình của chúng tôi tiết lộ rằng khoảng cách hiệu suất giữa các mô hình MoE và dày đặc thay đổi rất nhiều giữa các tác vụ và miền, các yếu tố cụ thể khiến các tác vụ nhất định thuận lợi hơn cho MoE vẫn chưa rõ ràng.
