# Lớp Mixture-of-Expert được Cổng Thưa cho Khả năng Giải thích CNN

Svetlana Pavlitska1;2, Christian Hubschneider1;2, Lukas Struppek2, J. Marius Zöllner1;2
1Trung tâm Nghiên cứu FZI về Công nghệ Thông tin
2Viện Công nghệ Karlsruhe (KIT)
Karlsruhe, Đức
pavlitska@fzi.de

Tóm tắt — Các lớp Mixture of Expert (MoE) được cổng thưa gần đây đã được áp dụng thành công để mở rộng quy mô các transformer lớn, đặc biệt là cho các tác vụ mô hình hóa ngôn ngữ. Một tác dụng phụ thú vị của các lớp MoE thưa là chúng mang lại khả năng giải thích bẩm sinh cho mô hình thông qua việc chuyên môn hóa chuyên gia tự nhiên. Trong nghiên cứu này, chúng tôi áp dụng các lớp MoE thưa cho CNN trong các tác vụ thị giác máy tính và phân tích tác động kết quả đến khả năng giải thích mô hình. Để ổn định việc huấn luyện MoE, chúng tôi trình bày cả phương pháp ràng buộc mềm và cứng. Với ràng buộc cứng, trọng số của một số chuyên gia được phép trở thành không, trong khi ràng buộc mềm cân bằng sự đóng góp của các chuyên gia bằng một hàm mất mát phụ trợ bổ sung. Kết quả là, ràng buộc mềm xử lý việc sử dụng chuyên gia tốt hơn và hỗ trợ quá trình chuyên môn hóa chuyên gia, trong khi ràng buộc cứng duy trì các chuyên gia tổng quát hơn và tăng hiệu suất tổng thể của mô hình. Các phát hiện của chúng tôi cho thấy các chuyên gia có thể ngầm tập trung vào các tiểu miền riêng lẻ của không gian đầu vào. Ví dụ, các chuyên gia được huấn luyện cho phân loại hình ảnh CIFAR-100 chuyên môn hóa trong việc nhận dạng các miền khác nhau như hoa hoặc động vật mà không cần nhóm dữ liệu trước đó. Các thí nghiệm với RetinaNet và tập dữ liệu COCO cũng chỉ ra rằng các chuyên gia phát hiện đối tượng cũng có thể chuyên môn hóa trong việc phát hiện các đối tượng có kích thước khác nhau.

Từ khóa chỉ mục — hỗn hợp các chuyên gia, khả năng giải thích

I. GIỚI THIỆU

Các lớp Mixture of Expert (MoE) thưa gần đây đã trở nên phổ biến nhờ khả năng mở rộng quy mô mô hình lên hàng tỷ và gần đây thậm chí lên hàng nghìn tỷ tham số [1]–[3]. Tuy nhiên, trọng tâm gần như chỉ tập trung vào các mô hình transformer cho các tác vụ mô hình hóa ngôn ngữ. Trong nghiên cứu này, chúng tôi chèn các lớp MoE vào mạng neural tích chập (CNN) và áp dụng phương pháp này cho các tác vụ thị giác máy tính cơ bản là phân loại hình ảnh và phát hiện đối tượng. Để giải quyết vấn đề nổi tiếng về việc huấn luyện chuyên gia không ổn định, chúng tôi trình bày các ràng buộc mềm và cứng, khuyến khích việc sử dụng chuyên gia cân bằng. Các mô hình cũng cung cấp một siêu tham số bổ sung để điều chỉnh số lượng chuyên gia hoạt động trong mỗi lần truyền xuôi và do đó, độ phức tạp tính toán.

Khả năng giải thích mô hình bẩm sinh là một tác dụng phụ của việc nhúng các lớp MoE vào kiến trúc mô hình. Đối với các tác vụ mô hình hóa ngôn ngữ, các chuyên gia đã được chứng minh là chủ yếu chuyên môn hóa về các khái niệm nông [2], [4], [5]. Tất cả các nghiên cứu trước đây đều dựa trên transformer. Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên nghiên cứu tác động của các lớp MoE thưa, được nhúng trong CNN, đến khả năng giải thích mô hình trong các tác vụ thị giác máy tính. Do trường tiếp nhận lớn hơn, các chuyên gia có thể tập trung vào các khái niệm ngữ nghĩa cấp cao hơn.

Các đóng góp của chúng tôi có thể được tóm tắt như sau:
• Chúng tôi áp dụng khái niệm của các lớp MoE thưa, chủ yếu được sử dụng trong transformer cho đến nay, cho CNN.
• Chúng tôi phân tích ngữ nghĩa của các chuyên gia đã học và đánh giá tác động của các ràng buộc khác nhau để cân bằng chuyên môn hóa tải đến cường độ khả năng giải thích của mô hình.
• Chúng tôi đánh giá khái niệm trên hai tác vụ riêng biệt: phân loại hình ảnh và phát hiện đối tượng.

II. NGHIÊN CỨU LIÊN QUAN

A. MoE Thưa

Các mô hình mixture-of-expert cổ điển, được giới thiệu bởi Jacobs et al. [6], bao gồm một số lượng thay đổi của các mô hình chuyên gia và một cổng duy nhất để kết hợp các đầu ra chuyên gia. Eigen et al. [7] đã phát triển ý tưởng sử dụng MoE như các thành phần con của mô hình với các cổng được học riêng lẻ. Điều này cho phép chia sẻ các phần còn lại của kiến trúc và cho phép nhiều lớp MoE trong một kiến trúc duy nhất. Việc chuyển sang tính toán có điều kiện thông qua kích hoạt chuyên gia thưa đầu tiên được khám phá bởi Shazeer et al. [8] sử dụng mạng LSTM. Bằng cách kích hoạt một số lượng cố định các chuyên gia, có thể tách biệt số lượng tham số khỏi FLOP cần thiết cho suy luận thông qua MoE.

Fedus et al. [1] đã tiến thêm một bước về phía kích hoạt thưa và cho thấy rằng có thể huấn luyện các mô hình MoE dựa trên transformer trong khi chỉ kích hoạt một chuyên gia duy nhất, dẫn đến các lớp MoE chỉ thêm ít chi phí tính toán. Một số nghiên cứu [9], [10] đã khám phá chế độ chuyên gia đơn lẻ kể từ đó, và Zopf et al. [2] đã đưa ra các hướng dẫn để thiết kế MoE thưa hiệu quả sử dụng transformer.

Do đó, MoE thưa đã trở nên phổ biến cho các mô hình ngôn ngữ khổng lồ [8], [11], [12]. Những cải thiện hiệu suất đạt được thông qua MoE thưa làm cho việc huấn luyện các mô hình này trở nên khả thi, mặc dù chỉ trên các cụm lớn của GPU mạnh mẽ.

Trong thị giác máy tính, việc áp dụng MoE thưa đã tập trung vào các mô hình transformer [9], [13], [14]. Ngược lại, trong nghiên cứu của chúng tôi, chúng tôi xem xét CNN, vẫn là kiến trúc phổ biến nhất trong lĩnh vực thị giác máy tính.

Mặc dù các MoE thông thường ở cấp mô hình đã được đánh giá thành công trên CNN [15]–[19], việc nhúng các lớp MoE vào CNN đã nhận được ít sự chú ý hơn đáng kể. Một trong số ít các nghiên cứu là của Yang et al. [20], nơi các kernel của các lớp tích chập được kích hoạt trên cơ sở từng ví dụ. Wang et al. [21] đã khám phá một phương pháp định tuyến động sâu cho CNN. Trong mô hình DeepMoE của họ, mỗi lớp tích chập được thay thế bằng một lớp MoE, và một mạng cổng đa đầu chọn và cân bằng lại các kênh trong mỗi lớp tích chập. DeepMoE được huấn luyện từ đầu đến cuối, nó đã chứng minh mức tăng 1000 lần trong dung lượng mô hình trong khi duy trì hiệu quả tính toán với chỉ những mất mát nhỏ.

Khác với nghiên cứu của Wang et al. [21], chúng tôi đề xuất các chuyên gia ở cấp khối dư và phân tích mức độ mà các biểu diễn tiềm ẩn được học bởi mô hình tương ứng với các khái niệm có thể hiểu được bởi con người.

B. Cân bằng Việc Sử dụng Chuyên gia

Một vấn đề MoE đã biết là sự tập trung của cổng vào một tập con nhỏ của tất cả các chuyên gia có sẵn. Các trọng số được gán cho các chuyên gia khác là vĩnh viễn bằng không hoặc nhỏ không đáng kể. Bởi vì một số chuyên gia hoạt động tốt hơn trong các lần lặp đầu tiên, cổng tăng xác suất kích hoạt của họ. Do đó, những chuyên gia ít này cải thiện trên mức trung bình, và cổng gán trọng số thậm chí cao hơn cho họ. Quá trình tự củng cố này tiếp tục, sao cho bộ tối ưu hóa kết thúc trong một cực tiểu địa phương kém [7].

Eigen et al. [7] đã đề xuất một ràng buộc cứng trên các phân công cổng tương đối cho mỗi chuyên gia được áp dụng trong quá trình huấn luyện. Đối với điều này, các trọng số được gán cho mỗi chuyên gia được tổng hợp trên tất cả các mẫu huấn luyện. Nếu giá trị này vượt quá hiệu suất trung bình trong các lần lặp đầu tiên, cổng tăng xác suất kích hoạt của họ bằng không. Các trọng số dương còn lại được tính toán lại bằng softmax để duy trì một kết hợp lồi của các chuyên gia. Các ràng buộc cứng khác đã được đề xuất dựa trên dung lượng chuyên gia [1], [4], [9] để khuyến khích việc sử dụng tất cả các chuyên gia nhưng cũng để đảm bảo việc sử dụng hiệu quả phần cứng có sẵn.

Shazeer et al. [8] đã trình bày một phương pháp ràng buộc mềm giới thiệu một hàm mất mát tầm quan trọng phụ trợ (xem bên dưới), khuyến khích tầm quan trọng bằng nhau cho tất cả các chuyên gia trong quá trình huấn luyện. Số lượng mẫu huấn luyện trên mỗi chuyên gia vẫn có thể thay đổi vì tầm quan trọng được sử dụng thay vì số lượng mẫu trung bình trên mỗi chuyên gia.

Một hàm mất mát phụ trợ khác cho cân bằng tải đã được đề xuất bởi Lepikhin et al. [11] – nó hạn chế số lượng token được xử lý bởi một chuyên gia với một ngưỡng. Hàm mất mát phụ trợ trong mô hình switch transformer [1] nhằm mục đích phân phối đồng đều của một batch token.

C. Khả năng Giải thích thông qua MoE Thưa

Chúng tôi đề cập đến khả năng giải thích như một khả năng mô hình để giải thích hoặc cung cấp ý nghĩa cho con người một cách có thể hiểu được [22]. Ngữ nghĩa hoặc khả năng giải thích trực quan của mô hình là rất quan trọng để đảm bảo rằng con người có thể tin tưởng vào các dự đoán của nó [23]. Trong số rất nhiều phương pháp khả năng giải thích [24], MoE ưu tiên những phương pháp mở hộp đen và phân tích các biểu diễn trung gian được học bởi mô hình.

Trong các thí nghiệm đầu tiên của Shazeer et al. [8] về các tác vụ mô hình hóa ngôn ngữ, các chuyên gia đã chuyên môn hóa về cú pháp và/hoặc ngữ nghĩa. Các tác giả cung cấp các ví dụ về chuyên môn hóa cho ba chuyên gia được chọn: một chuyên gia được sử dụng cho từ innovation, một chuyên gia khác cho mạo từ a, và một chuyên gia khác về khái niệm nhanh chóng, hành động nhanh.

Lewis et al. [4] đã chỉ ra rằng các chuyên gia chuyên môn hóa trong thông tin cú pháp cục bộ rất cụ thể: các chuyên gia đã học các cụm số, từ viết tắt, đại từ sở hữu, v.v. Không có chuyên môn hóa ở cấp độ ngữ nghĩa nào được quan sát thấy.

Hành vi tương tự đã được mô tả trong nghiên cứu sau của Zoph et al. [2]. Các chuyên gia được phát hiện chuyên môn hóa về dấu câu, mạo từ, liên từ, danh từ riêng và số. Thú vị là, ngay cả trong trường hợp các mô hình thưa đa ngôn ngữ, không có chuyên môn hóa trong ngôn ngữ nào được quan sát thấy. Thay vào đó, các chuyên gia tiếp tục tập trung vào cùng những khái niệm nông như dấu câu, mạo từ hoặc số. Mustafa et al. [5] đã khám phá các mô hình được kích hoạt thưa đa phương thức. Các chuyên gia văn bản chuyên môn hóa về danh từ và tính từ, trong khi các chuyên gia hình ảnh chuyên môn hóa về các khái niệm ngữ nghĩa như bộ phận cơ thể, kết cấu, động vật, thực phẩm và cửa.

Nghiên cứu về visual sparse transformer bởi Riquelme et al. [13] gần gũi nhất với nghiên cứu của chúng tôi vì nó liên quan đến dữ liệu hình ảnh. Ở đây, các chuyên gia chuyên môn hóa trong việc phân biệt giữa các tập lớp nhỏ. Tương quan chuyên gia-lớp chỉ mạnh mẽ cho một vài lớp cuối cùng, trong khi không có chuyên môn hóa chuyên gia nào được quan sát thấy cho các lớp đầu tiên. Nghiên cứu khác với visual transformer bởi Wu et al. [25] đã chứng minh chuyên môn hóa chuyên gia trên các lớp ImageNet.

Trong trường hợp visual transformer, các patch hình ảnh là token, được định tuyến đến các chuyên gia. Trong nghiên cứu của chúng tôi, việc định tuyến được thực hiện ở cấp độ toàn bộ hình ảnh, điều này dẫn đến chuyên môn hóa ngữ nghĩa ở cấp độ hình ảnh.

III. PHƯƠNG PHÁP

Phương pháp của chúng tôi bao gồm ba thành phần: (1) nhúng các lớp MoE thưa trong CNN, (2) cân bằng việc sử dụng chuyên gia thông qua ràng buộc, và (3) phân tích chuyên môn hóa chuyên gia tiết lộ các khái niệm được học trong các lớp MoE.

A. MoE được Cổng Thưa cho CNN

Chúng tôi đề xuất một phương pháp nhúng các lớp MoE thưa vào CNN với mục tiêu đạt được độ phức tạp tính toán tương tự như baseline về mặt tham số. Không mất tính tổng quát, chúng tôi xem xét một CNN bao gồm các khối dư [26] (xem Hình 1). Kiến trúc ResBlock-MoE được đề xuất sử dụng một khối dư hoàn chỉnh làm chuyên gia của nó. Lớp MoE đóng gói nhiều bản sao của khối được kích hoạt và trộn bằng một cổng. Chúng tôi thí nghiệm với việc chèn các lớp MoE ở các vị trí khác nhau trong kiến trúc mô hình.

Chúng tôi xem xét hai loại cổng (xem Hình 2): cổng GAP-FC bao gồm một lớp GAP theo sau bởi một lớp kết nối đầy đủ duy nhất, trong khi Conv-GAP-FC bổ sung chứa một lớp tích chập, có thể sử dụng thông tin cục bộ chi tiết được mã hóa trong các tính năng đầu vào.

B. Ràng buộc để Cân bằng Việc Sử dụng Chuyên gia

Một cách chính thức, một MoE bao gồm một tập hợp N chuyên gia E1,...,EN. Đối với một đầu vào x cho trước, mỗi chuyên gia Ei tạo ra một đầu ra ei(x). Cổng tính toán một vector trọng số G(x) = [g1(x),...,gN(x)]. Đầu ra MoE cuối cùng là một tổng có trọng số của các đầu ra chuyên gia: FMoE(x) = ∑(i=1 to N) gi(x)ei(x). Để đo lường việc sử dụng chuyên gia, chúng tôi định nghĩa một vector tầm quan trọng I(X) = ∑(x∈X) G(x) cho mỗi batch mẫu huấn luyện X, và tầm quan trọng của một chuyên gia đơn lẻ Ei là Ii(X) = ∑(x∈X) gi(x) [8].

Chúng tôi đề cập đến vấn đề việc sử dụng chuyên gia MoE không cân bằng như các chuyên gia chết, tương tự như dying ReLU [27]. Chúng tôi coi một chuyên gia là chết nếu nó nhận được ít hơn 1% tầm quan trọng trung bình trên tập kiểm tra. Để giảm thiểu vấn đề, chúng tôi đề xuất một ràng buộc mềm và hai ràng buộc cứng.

Ràng buộc Cứng: được thúc đẩy bởi nghiên cứu của Eigen et al. [7], chúng tôi đề xuất hai ràng buộc cứng về tầm quan trọng. Cả hai ràng buộc cứng chỉ hoạt động trong quá trình huấn luyện và vô hiệu hóa các chuyên gia cho toàn bộ batch.

Chúng tôi ký hiệu tầm quan trọng trung bình của batch X là I(X) và định nghĩa tầm quan trọng tương đối của chuyên gia Ei cho X như sau:

I^rel_i(X) = (Ii(X) - I(X)) / I(X)     (1)

Trong ràng buộc tầm quan trọng tương đối, trọng số chuyên gia bị đặt về không cho một batch, nếu tầm quan trọng tương đối chạy của chuyên gia này vượt quá ngưỡng được xác định trước m_rel:

gi(Xt) = 0 ⟺ (1/t) ∑(t'=1 to t) I^rel_i(Xt') > m_rel     (2)

Đối với ràng buộc tầm quan trọng trung bình, chúng tôi định nghĩa tầm quan trọng trung bình được gán cho chuyên gia Ei lên đến bước thời gian t:

Īi(Xt) = (1/t) ∑(t'=1 to t) Ii(Xt')/|Xt'|     (3)

Trong ràng buộc này, trọng số chuyên gia bị đặt về không khi tầm quan trọng trung bình cho chuyên gia này vượt quá tầm quan trọng trung bình của batch bằng một ngưỡng được xác định trước m_mean:

gi(Xt) = 0 ⟺ Īi(Xt) - Ī(Xt) > m_mean     (4)

Ràng buộc tầm quan trọng tương đối tập trung mạnh hơn vào quá khứ gần, trong khi phương pháp tầm quan trọng trung bình có cái nhìn tổng thể, với tất cả các giá trị tầm quan trọng quá khứ có cùng tác động đến ràng buộc.

Ràng buộc Mềm: phương pháp ràng buộc mềm đầu tiên, ban đầu được đề xuất bởi Shazeer et al. [8], là một hàm mất mát tầm quan trọng phụ trợ L_imp, theo Phương trình 5. Nó sử dụng hệ số bình phương của biến thiên của vector tầm quan trọng I(X) cho batch X và một yếu tố trọng số w_imp:

L_imp = w_imp * CV(I(X))^2 = w_imp * (σ(I(X))/Ī(X))^2     (5)

Chúng tôi đề xuất một ràng buộc mềm khác có cái nhìn xác suất về tầm quan trọng chuyên gia. Đối với điều này, chúng tôi diễn giải một MoE như một mô hình xác suất trong đó xác suất lớp được biên hóa trên việc lựa chọn chuyên gia. Mỗi trọng số gi(x) do đó là xác suất p(Ei|x) để chọn một chuyên gia cụ thể cho một đầu vào cho trước, và các đầu ra của mỗi chuyên gia ei(x) định lượng xác suất p(c|Ei,x) của mỗi lớp c ∈ C. Đầu ra MoE sau đó được định nghĩa như sau:

FMoE(x) = ∑(i=1 to N) p(Ei|x)p(c|Ei,x) = p(c|x)     (6)

Chúng tôi diễn giải đầu ra cổng như một phân phối xác suất rời rạc P với xác suất P(Ei|X) cho chuyên gia Ei được chọn cho đầu vào X (X là biến ngẫu nhiên cho đầu vào x).

Về mặt kỳ vọng, cổng nên gán cho mỗi chuyên gia Ei cùng một trọng số trung bình, bằng E_X[P(Ei|X)] = 1/N. Việc gán trọng số kỳ vọng do đó tương ứng với một phân phối đồng đều rời rạc Q với xác suất Q(Ei|X) = Q(Ei) = 1/N.

Chúng tôi định nghĩa một hàm mất mát KL-divergence phụ trợ L_KL là KL-divergence D_KL(P||Q) giữa P và Q, được cân bằng bởi siêu tham số w_KL. Xác suất P(Ei|X=X) = Ii(X)/|X| được tính toán như tầm quan trọng trung bình trên mỗi mẫu trong batch X.

L_KL sau đó được định nghĩa như sau:

L_KL = w_KL * D_KL(P||Q) = w_KL * ∑(i=1 to N) P(Ei|X)ln(P(Ei|X)/Q(Ei))
     = w_KL * ∑(i=1 to N) (Ii(X)/|X|)ln((Ii(X)*N)/|X|)     (7)

L_imp phạt bất đẳng thức trong phân phối tầm quan trọng khó hơn L_KL, do đó đạt được việc sử dụng chuyên gia bằng nhau. Mặt khác, L_KL dẫn đến phương sai cao hơn trong việc sử dụng chuyên gia nhưng vẫn tránh được các chuyên gia chết.

IV. THÍ NGHIỆM VỚI MOE THƯA CHO PHÂN LOẠI HÌNH ẢNH

A. Thiết lập Thí nghiệm

Chúng tôi sử dụng kiến trúc ResNet-18 [26] bao gồm bốn khối ResNet, trong khi các lớp tích chập và pooling đầu tiên được thay thế bằng một tích chập 3x3 duy nhất để điều chỉnh cho độ phân giải đầu vào thấp hơn. Chúng tôi chạy thí nghiệm trên tập dữ liệu CIFAR-100 [28]. Các chuyên gia MoE tuân theo kiến trúc của các khối dư, nhưng chúng tôi điều chỉnh số lượng bộ lọc của mỗi chuyên gia theo cách bottleneck, do đó giảm số lượng tham số để duy trì độ phức tạp tính toán tương đương.

Chúng tôi cũng thêm một shortcut projection bổ sung kết nối đầu vào lớp MoE với đầu ra của nó. Chúng tôi đánh giá việc nhúng MoE được cổng thưa trong mỗi vị trí có thể trong ResNet-18. Vì các biểu diễn được học bởi CNN phát triển từ các lớp trước đến sau, chúng tôi mong đợi lớp MoE học các khái niệm có thể hiểu được bởi con người ở mức độ lớn hơn khi được chèn vào các lớp sau.

Chúng tôi huấn luyện các mô hình với 4 chuyên gia và đặt số lượng chuyên gia hoạt động là k = 2. Chúng tôi đặt trọng số cho các hàm mất mát phụ trợ là w_imp = w_KL = 0.5, và ngưỡng cho các ràng buộc cứng là m_rel = 0.5 và m_mean = 0.3.

Tất cả các thí nghiệm được thực hiện bằng GPU NVIDIA GeForce RTX 2080 TI. Các mô hình được huấn luyện với kích thước batch 128 trong 150 epoch với bộ tối ưu hóa Adam [29]. Tất cả các thí nghiệm được lặp lại ba lần, và các giá trị trung bình được báo cáo.

B. Hiệu suất và Việc Sử dụng Chuyên gia

Chuyên gia chết: các ràng buộc mềm đã thành công trong việc giảm thiểu cái chết của chuyên gia (xem Bảng I). Cả hai hàm mất mát phụ trợ đều giảm thiểu biến thiên trong tầm quan trọng trên mỗi chuyên gia, trong khi phương sai cao hơn trong số lượng mẫu trên mỗi chuyên gia được quan sát thấy cho hàm mất mát KL-divergence. Tuy nhiên, các ràng buộc cứng đã chứng minh kết quả tệ hơn. Các mô hình tầm quan trọng trung bình không thể giữ tất cả các chuyên gia sống trong một mô hình duy nhất, trong khi các mô hình tầm quan trọng tương đối không có chuyên gia chết cho các mô hình với 4 chuyên gia. Ngoài ra, việc tăng số lượng chuyên gia lên 10 dẫn đến nhiều phương sai hơn trong việc sử dụng chuyên gia.

Độ chính xác và chi phí tính toán: trong thiết lập của chúng tôi, việc nhúng các lớp MoE vào mô hình nhằm mục đích tăng cường độ mạnh khả năng giải thích, không phải đánh bại hiệu suất baseline. Trong số các mô hình được đánh giá, kết quả tốt nhất đạt được với ràng buộc tầm quan trọng trung bình (xem Bảng II). Do đó, việc lựa chọn ràng buộc cung cấp một sự đánh đổi rõ ràng giữa hiệu suất tổng thể và cái chết của các chuyên gia.

Trong tất cả các mô hình, chúng tôi giữ số lượng chuyên gia hoạt động k = 2 để duy trì ngân sách tính toán tương đương với baseline. Chúng tôi đo ngân sách tính toán bằng các phép toán nhân-tích lũy (MAC), và baseline đạt 0.56 GMac. Mặc dù việc tăng số lượng chuyên gia hoạt động k dẫn đến hiệu suất tốt hơn, mỗi chuyên gia được kích hoạt bổ sung thêm khoảng 0.06 đến 0.08 GMac tùy thuộc vào vị trí lớp. Với k = 3, mô hình tổng thể đạt trung bình 0.63 GMac, với k = 4 đã là 0.7 GMac. Do đó k kiểm soát sự đánh đổi giữa độ chính xác và độ phức tạp tính toán.

Tác động của kiến trúc cổng: chúng tôi đã đánh giá việc thay thế cổng GAP-FC bằng cổng Conv-GAP-FC, ví dụ cho ResBlock-MoE với 4 chuyên gia ở vị trí 4 và hàm mất mát tầm quan trọng. Mô hình này đạt độ chính xác 72.42±0.27, đánh bại mô hình GAP-FC tương ứng (71.95±0.39), nhưng không đánh bại baseline (72.62±0.29). Các mô hình ràng buộc cứng sử dụng cổng Conv-GAP-FC chịu tổn thương nặng nề từ chuyên gia chết, ngay cả với tốc độ học giảm.

C. Khả năng Giải thích thông qua MoE được Cổng Thưa

Phân vùng tập dữ liệu theo cổng: đánh giá trực quan của logit cổng, được vẽ bằng t-SNE (xem Hình 3) cho thấy, MoE ở vị trí 1 dẫn đến việc gán dựa trên màu sắc chủ đạo của hình ảnh đầu vào, trong khi đối với vị trí 4, sự khác biệt mờ nhạt hơn nhiều. Hơn nữa, các cấu trúc có thể nhìn thấy ít quan trọng hơn cho các mô hình ràng buộc mềm, so với trường hợp ràng buộc cứng.

Việc gán mẫu kết quả cho các chuyên gia khác nhau tiết lộ những khác biệt nổi bật hơn giữa các ràng buộc (xem Hình 4). Đối với các lớp trước, cổng chia dữ liệu thành 2 tiểu miền chính, trong khi đối với vị trí 4, cổng thay đổi nhiều hơn giữa các kết hợp chuyên gia khác nhau. Do đó, việc gán trọng số trong các lớp MoE sâu hơn dựa nhiều hơn vào các tính năng cấp cao và dẫn đến sự khác biệt mạnh mẽ hơn. Đối với các mô hình ràng buộc cứng, các cấu trúc có thể nhìn thấy ít quan trọng hơn.

Một cổng Conv-GAP-FC phức tạp hơn phân chia các phân công rõ ràng hơn. Do đó cổng tạo ra các vector trọng số ít mơ hồ hơn và chọn chuyên gia một cách chắc chắn.

Chuyên môn hóa chuyên gia: để phân tích chuyên môn hóa ngầm của các chuyên gia trên các tiểu miền khác biệt của không gian đầu vào, chúng tôi chỉ kích hoạt một chuyên gia cụ thể trong quá trình đánh giá và gán tất cả trọng số cho nó. Sau đó chúng tôi phân tích các lớp nhận được trọng số lớn nhất trong mỗi chuyên gia trong quá trình đánh giá (Bảng III). Chúng tôi có thể quan sát các cụm lớp khác biệt lặp lại cho các mô hình khác nhau, ví dụ: hoa, động vật biển, cây cối và đồ nội thất. Do đó các lớp MoE học cách phân cụm tự nhiên các khái niệm được biểu diễn trong hình ảnh đầu vào. Các cụm lặp lại được hình thành bất kể kiến trúc cổng và ràng buộc, nhưng việc chèn các lớp MoE ở các vị trí sâu hơn trong mạng dẫn đến trọng số lớn hơn được gán cho các chuyên gia, cho thấy chuyên môn hóa tốt hơn.

Hơn nữa, chúng tôi xác định rằng cổng chọn chuyên gia hoạt động tốt nhất cho các hình ảnh của cụm tương ứng. Đối với điều này, chúng tôi trích xuất kết quả cho các lớp trong đó mỗi chuyên gia được gán trọng số cao nhất và thấp nhất. MoE đầy đủ hoạt động trong 73 trên 100 lớp ít nhất cũng tốt như các chuyên gia tốt nhất của nó trong miền này. Do đó cổng có thể xác định chuyên gia cho các miền khác biệt và hỗ trợ thích hợp các chuyên gia có hiệu suất kém hơn. Nó do đó có thể kết hợp một cách hợp lý các bản đồ tính năng đầu ra để cải thiện các dự đoán tổng thể.

Việc sử dụng chuyên gia vs. độ chính xác: tiếp theo, chúng tôi đánh giá tương quan giữa trọng số trung bình được gán cho một chuyên gia (thưa và không thưa, tức là với tất cả các chuyên gia được kích hoạt) và độ chính xác kiểm tra của từng chuyên gia trên mỗi lớp. Chúng tôi cũng đánh giá tương quan giữa độ chính xác và số lượng kích hoạt trên mỗi chuyên gia. Kết quả (xem Bảng IV) cho thấy một mối quan hệ mạnh mẽ cho mô hình ràng buộc mềm. Đối với trường hợp ràng buộc cứng, các chuyên gia được tổng quát hóa hơn và không thể hiện biến thiên hiệu suất lớn trên các lớp khác nhau, cổng không dựa vào cùng các chuyên gia cho một lớp nhất định.

V. THÍ NGHIỆM VỚI MOE THƯA CHO PHÁT HIỆN ĐỐI TƯỢNG

A. Thiết lập Thí nghiệm

Chúng tôi sử dụng RetinaNet [30] được huấn luyện trước với backbone ResNet-50 làm baseline, và tập dữ liệu COCO [31]. Chúng tôi huấn luyện tất cả các mô hình sử dụng α = 2 và γ = 0.25 cho focal loss.

Chúng tôi nhúng các lớp MoE thưa theo hai cách: (1) 2Block-MoE: bằng cách thay thế các subnet hồi quy và phân loại bằng hai khối MoE riêng biệt, và (2) SingleMoE: với một cổng duy nhất được chia sẻ giữa regressor và classifier. Chúng tôi giữ trọng số backbone đóng băng trong quá trình huấn luyện. Chúng tôi cũng huấn luyện các mô hình với trọng số không đóng băng nhưng không quan sát thấy cải thiện hiệu suất. Các ràng buộc tầm quan trọng trung bình không được bao gồm trong đánh giá, vì các mô hình chịu tổn thương nặng nề từ các vấn đề chuyên gia chết.

Cổng là Conv-GAP-FC (xem Hình 2b). Tất cả các chuyên gia được khởi tạo bằng phương pháp Kaiming [32] để học các tính năng đa dạng hơn. Các yếu tố trọng số được giảm xuống w_imp = w_KL = 0.25 để đảm bảo việc sử dụng chuyên gia tốt hơn cho các lớp MoE sâu hơn. Ngoài ra, chúng tôi đặt m_rel = 0.3 để tránh chuyên gia chết. Chúng tôi huấn luyện các mô hình với 4 chuyên gia và đặt k = 2.

B. Đánh giá

Hiệu suất: các mô hình ràng buộc cứng hoạt động tốt hơn một chút so với các mô hình ràng buộc mềm, mặc dù không có mô hình nào với các lớp MoE vượt trội hơn baseline (xem Bảng V). Chúng tôi cũng quan sát thấy chỉ có một sự sụt giảm hiệu suất nhỏ cho mô hình Single so với hai cổng riêng biệt.

Chuyên môn hóa của 2Block-MoE trong regressor: để có cái nhìn sâu sắc về các quyết định trọng số của BBox regressor, trước tiên chúng tôi phân tích hành vi một cách trực quan. Đối với điều này, chúng tôi giữ k_cls = 2 chuyên gia hoạt động trong MoE classifier và chỉ phân tích MoE regressor.

So sánh trực quan của các bounding box (xem Hình 5) cho một hình ảnh được chọn cho thấy, tất cả các chuyên gia đều có vấn đề ước tính ranh giới đối tượng chính xác cho một tư thế không điển hình, trong khi các dự đoán trên một đối tượng có góc nhìn trước rõ ràng thay đổi ít hơn. Cổng không phải lúc nào cũng có thể chọn chuyên gia tốt nhất duy nhất. Tuy nhiên, các chuyên gia không hoạt động sẽ cả hai đều ước tính quá mức phần dưới của bounding box, trong khi các chuyên gia được chọn cả hai đều dự đoán phần dưới chặt chẽ. Các dự đoán được thực hiện bởi các chuyên gia khác biệt trong mô hình ràng buộc cứng thay đổi ít hơn so với các mô hình được huấn luyện với ràng buộc mềm.

Để phân tích hành vi của các chuyên gia khác biệt trong regressor, chúng tôi tính toán trọng số trung bình được gán cho mỗi chuyên gia hồi quy trong quá trình đánh giá và chia nhỏ việc gán trọng số thành các cấp độ feature map khác nhau từ P3 đến P7. Trên mỗi cấp độ feature map, mạng gating tính toán các trọng số riêng biệt và do đó chọn chuyên gia độc lập với các cấp độ khác. Chúng tôi tính toán việc gán trọng số trung bình trên các vector trọng số thưa sau khi chọn k = 2 chuyên gia hoạt động (xem Bảng VI).

Chúng tôi cũng minh họa kết quả trong Hình 6 bằng cách vẽ các bounding box dự đoán cho một số hình ảnh với các đối tượng có quy mô khác nhau. Kết quả cho thấy mạng gating chọn chuyên gia tùy thuộc vào các cấp độ feature map.

Mạng gating ràng buộc cứng sử dụng ràng buộc tầm quan trọng tương đối gán trọng số theo cách đa dạng hơn (xem Bảng VI). Tuy nhiên, các chuyên gia có xu hướng được sử dụng chủ yếu cho các cấp độ feature map khác nhau. Chúng tôi kết luận rằng các chuyên gia khác biệt trong trường hợp ràng buộc cứng hoạt động tốt hơn vì họ được kích hoạt trên một số lượng lớn hơn các cấp độ feature map khác nhau và được huấn luyện để tính toán bounding box trên các quy mô khác nhau. Expert 1 là chuyên gia duy nhất được sử dụng chủ yếu để phát hiện các đối tượng nhỏ.

Nhìn chung, chúng tôi kết luận rằng mạng gating chọn chuyên gia chủ yếu tùy thuộc vào đầu vào của các cấp độ feature map khác nhau. Mặc dù chúng tôi không áp dụng các ràng buộc theo cấp độ, các chuyên gia hoạt động tương tự chính xác khi phát hiện các đối tượng có quy mô khác nhau ở các cấp độ feature map khác nhau. Cải thiện có thể được thực hiện bằng cách huấn luyện các chuyên gia cụ thể trên các tập dữ liệu, đặc biệt là cho các đối tượng nhỏ hoặc lớn.

Chuyên môn hóa của 2Block-MoE trong classifier: chúng tôi phân tích hành vi của các chuyên gia classifier tương tự bằng cách giữ k_reg = 2 và gán tất cả trọng số trong MoE classifier cho một chuyên gia cụ thể (xem Bảng VII). Chúng tôi quan sát thấy các chuyên gia classifier thay đổi ở mức độ lớn hơn so với các chuyên gia regressor. Đối với mô hình với hàm mất mát KL-divergence, Expert 1 nổi bật và hoạt động tệ hơn đáng kể so với các chuyên gia khác. Chuyên gia được sử dụng chủ yếu trên cấp độ feature map P7 với khoảng 90% việc gán trọng số. Do đó, chuyên gia chuyên môn hóa trong việc phát hiện các đối tượng lớn trên kích thước feature map cụ thể này. Sự khác biệt giữa các chuyên gia khác trong mô hình này vẫn nhỏ nhưng hơi lớn hơn so với các chuyên gia regressor. Các chuyên gia trong mô hình ràng buộc cứng hoạt động tốt hơn một chút riêng lẻ và gần hơn với mô hình MoE sử dụng k_cls = 2 chuyên gia.

Việc gán trọng số trong MoE classifier tương tự như MoE regressor, sự khác biệt phát sinh cho các cấp độ P6 và P7. Trong khi mạng gating trong MoE regressor tập trung vào hai chuyên gia trên mỗi cấp độ, mạng gating của classifier sử dụng ba chuyên gia cho P6 và chủ yếu là một chuyên gia cho P7. Mô hình ràng buộc cứng cũng gán trọng số đa dạng hơn cho các chuyên gia của nó trong MoE classifier.

Hình 7 cho thấy các dự đoán của các chuyên gia classifier khác biệt cho một hình ảnh mẫu. Chúng tôi đặt k_reg = 2 và vẽ các dự đoán sử dụng các chuyên gia phân loại khác biệt và so sánh các dự đoán của chuyên gia với mô hình MoE đầy đủ trên cùng một hình ảnh đầu vào. Lưu ý rằng tất cả các chuyên gia có thể phát hiện các đối tượng liên quan một cách chính xác nhưng thay đổi trong độ tin cậy của họ. Expert 2 và 4 cũng dự đoán sai các đối tượng bổ sung.

Nhìn chung, các lớp MoE được nhúng trong các subnet classifier và regressor, các đơn vị quyết định trong mạng, cho phép chúng tôi có cái nhìn sâu sắc về các quá trình quyết định của mô hình. Chúng tôi có thể phân tích riêng biệt các dự đoán của mỗi chuyên gia và so sánh chúng với dự đoán MoE kết quả. Các thí nghiệm của chúng tôi đã chứng minh rằng các chuyên gia khác biệt chuyên môn hóa trong việc phát hiện các đối tượng có kích thước cụ thể.

Tác động của huấn luyện trước: cuối cùng, chúng tôi điều tra hành vi của các mô hình MoE sử dụng trọng số chuyên gia được huấn luyện trước. Đối với điều này, chúng tôi tái sử dụng trọng số baseline và thêm nhiễu Gaussian để thực thi chuyên môn hóa chuyên gia. Chúng tôi cũng huấn luyện các mô hình Conv4 bằng cách chỉ thay thế các lớp tích chập thứ 4 trong các subnet regressor và classifier bằng một lớp MoE. Để so sánh, chúng tôi cũng huấn luyện các mô hình Conv4 mà không có huấn luyện trước.

Cả hai mô hình với trọng số được huấn luyện trước đều vượt trội hơn baseline một chút và cũng vượt trội hơn các mô hình được huấn luyện từ đầu, trong khi các mô hình ràng buộc mềm cho thấy những cải thiện lớn hơn (xem Bảng V). Việc phân tích gán trọng số (xem Bảng VIII) đã cho thấy một lần nữa, việc sử dụng chuyên gia chủ yếu phụ thuộc vào các cấp độ feature map. Tuy nhiên, việc sử dụng chuyên gia được phân tán nhiều hơn, và các ranh giới cấp độ ít cứng nhắc hơn. Chúng tôi giả định rằng đối với thời gian huấn luyện dài hơn, mạng gating sẽ tham chiếu mạnh mẽ hơn đến một hoặc hai chuyên gia trên mỗi cấp độ, có thể so sánh với các mô hình MoE khác. Đối với MoE classifier, các trọng số được gán cũng được phân phối tốt hơn giữa các cấp độ feature map khác nhau.

Nhìn chung, việc sử dụng trọng số được huấn luyện trước giúp tăng hiệu suất so với huấn luyện từ đầu, nhưng cản trở chuyên môn hóa chuyên gia. Do đó, quá trình ra quyết định của mô hình trở nên ít minh bạch hơn và ít có thể giải thích hơn. Điều này một lần nữa nhấn mạnh sự tồn tại của sự đánh đổi giữa khả năng giải thích và chuyên môn hóa chuyên gia và hiệu suất mô hình.

VI. KẾT LUẬN

Trong nghiên cứu này, chúng tôi đã áp dụng các lớp MoE được cổng thưa cho CNN trong các tác vụ thị giác máy tính với mục tiêu tăng khả năng giải thích mô hình. Chúng tôi đã trình bày các ràng buộc để giảm thiểu vấn đề chuyên gia chết, giải quyết vấn đề từ các góc độ khác nhau và dẫn đến hành vi MoE khác nhau. Phân tích của chúng tôi đã tiết lộ một số mối liên hệ giữa các ràng buộc được đề xuất một mặt, và hiệu suất mô hình cũng như khả năng giải thích mặt khác. Ràng buộc cứng dẫn đến hiệu suất tổng thể tốt hơn và các chuyên gia tổng quát, mặc dù ràng buộc tầm quan trọng trung bình đặc biệt dễ bị vấn đề chuyên gia chết. Ràng buộc mềm, về mặt khác, dẫn đến chuyên môn hóa chuyên gia tốt hơn. Do đó việc sử dụng ràng buộc giúp kiểm soát sự tương tác giữa hiệu suất mô hình, tính ổn định huấn luyện và chuyên môn hóa chuyên gia.

Các thí nghiệm của chúng tôi đã tiết lộ khả năng giải thích bẩm sinh cho hai tác vụ thị giác máy tính được đánh giá. Đối với tác vụ phân loại hình ảnh, các chuyên gia tập trung vào các nhóm lớp khác biệt lặp lại, trong khi đối với phát hiện đối tượng, họ chuyên môn hóa trong các đối tượng có kích thước khác biệt. Chúng tôi hy vọng rằng những hiểu biết của chúng tôi mở đường cho nghiên cứu thêm về khả năng giải thích của mạng neural sâu.

LỜI CẢM ƠN

Nghiên cứu dẫn đến những kết quả này được tài trợ bởi Bộ Kinh tế và Hành động Khí hậu Liên bang Đức trong dự án "KI Absicherung" (tài trợ 19A19005W) và bởi KASTEL Security Research Labs. Các tác giả muốn cảm ơn tập đoàn vì sự hợp tác thành công.
