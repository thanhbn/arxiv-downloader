# 2405.16836.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2405.16836.pdf
# Kích thước tệp: 357367 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tăng Cường Mạng Feedforward Nhanh với
Cân Bằng Tải và Nút Lá Chính
Andreas Charalampopoulos1
andcharalamp@gmail.comNikolas Chatzis1
chatznikolas@gmail.com
Foivos Ntoulas-Panagiotopoulos1
foivosdoulas@hotmail.grCharilaos Papaioannou1
cpapaioan@mail.ntua.grAlexandros Potamianos1
potam@central.ntua.gr
1Trường ECE, Đại học Kỹ thuật Quốc gia Athens, Hy Lạp
Tóm tắt
Mạng feedforward nhanh (FFF) là một lớp mạng nơ-ron khai thác quan s찰 rằng các vùng khác nhau của không gian đầu vào kích hoạt các tập con riêng biệt của nơ-ron trong mạng rộng. FFF phân vùng không gian đầu vào thành các phần riêng biệt bằng cách sử dụng cây nhị phân khả vi các nơ-ron và trong quá trình suy luận đi xuống cây nhị phân để cải thiện hiệu quả tính toán. Lấy cảm hứng từ nghiên cứu về Hỗn hợp Chuyên gia (MoE), chúng tôi đề xuất việc kết hợp các kỹ thuật cân bằng tải và Lá Chính vào kiến trúc FFF để cải thiện hiệu suất và đơn giản hóa quá trình huấn luyện. Chúng tôi tái hiện các thí nghiệm được tìm thấy trong tài liệu và trình bày kết quả về các mô hình FFF được tăng cường bằng những kỹ thuật này. Kiến trúc được đề xuất và công thức huấn luyện đạt được tới 16,3% và 3% tăng độ chính xác phân loại tuyệt đối lần lượt trong độ chính xác huấn luyện và kiểm tra, so với kiến trúc FFF gốc. Ngoài ra, chúng tôi quan sát thấy phương sai nhỏ hơn trong các kết quả so với những gì được báo cáo trong nghiên cứu trước đó. Những phát hiện này chứng minh tiềm năng của việc tích hợp các kỹ thuật lấy cảm hứng từ MoE vào FFF để phát triển các mô hình chính xác và hiệu quả hơn.

1 Giới thiệu
Gần đây, các mô hình với hàng tỷ tham số đã có thành công lớn trong các ứng dụng trí tuệ nhân tạo tạo sinh [1,2,3]. Nhưng cùng với những kết quả ấn tượng đó, đến gánh nặng độ phức tạp tính toán của suy luận lớp FeedForward (FF), đặc biệt có mặt trong Transformers [4]. Người ta quan sát thấy rằng trong các lớp FF rộng, các phần khác nhau của miền đầu vào kích hoạt các tập hợp nơ-ron riêng biệt; quan sát này có thể được tận dụng để thiết kế các mô hình hiệu quả hơn [5]. Kết quả là ý tưởng về việc đạt được hiệu quả tính toán tốt hơn từ các mô hình được kích hoạt thưa thớt đã thu hút nhiều sự chú ý [6, 7].

Hỗn hợp Chuyên gia (MoE) là một nỗ lực sớm để tận dụng sự thưa thớt này, và tiếp tục là một chủ đề được quan tâm [8,9,10]. Công việc gần đây về các kiến trúc được kích hoạt thưa thớt bao gồm mạng Fast Feed Forward (FFF) [11]. Các tác giả trong [12,11] chỉ ra rằng FFF có thể được sử dụng thay cho kiến trúc FF vani và MoE trong transformers và Mô hình Ngôn ngữ Lớn (LLM) mà không gây ra bất kỳ mất mát đáng kể nào về độ chính xác, đồng thời thực hiện tăng tốc đáng kể trong quá trình suy luận. Tăng tốc suy luận trong FFF đạt được thông qua việc kích hoạt nơ-ron có điều kiện theo cây.

Trong khi cố gắng tái hiện các thí nghiệm từ [11], chúng tôi đã xác minh rằng FFF gặp vấn đề bất ổn trong huấn luyện. Điều này cũng có thể được suy ra từ phương sai lớn trong các kết quả cũng được báo cáo trong Bảng 5 của [11], trong đó phương sai giữa các lần chạy huấn luyện giống hệt nhau là cao. Hơn nữa, chúng tôi quan sát thấy rằng một số cây con trong kiến trúc FFF được kích hoạt nhiều hơn đáng kể so với những cây khác trong quá trình suy luận, tức là có sự mất cân bằng đáng kể trong việc sử dụng FFF. Để giải quyết hai vấn đề này và được thúc đẩy bởi tài liệu MoE [13], chúng tôi đề xuất hai thay đổi đối với kiến trúc FFF: 1) giới thiệu cân bằng tải để sử dụng tốt hơn tất cả các cây con FFF, và 2) thêm nút lá chính song song với cấu trúc FFF góp phần vào đầu ra với hệ số hỗn hợp không đổi, để các chuỗi đầu vào gây ra các mẫu kích hoạt nơ-ron "rộng hơn" có thể được phục vụ tốt hơn. Chúng tôi chỉ ra rằng các cải tiến được đề xuất cải thiện hiệu suất phân loại trên các bộ dữ liệu MNIST và FashionMNIST. Hơn nữa, chúng tôi chỉ ra rằng các FFF tăng cường đạt được sự ổn định huấn luyện tổng thể tốt hơn so với FFF vani.

Đóng góp của chúng tôi có thể được tóm tắt như sau:
1. Chúng tôi đề xuất một kiến trúc FFF tăng cường (eFFF) kết hợp một thuật ngữ cân bằng tải ở hàm mất mát và một nút lá chính được trộn tuyến tính với đầu ra FFF.
2. Chúng tôi cung cấp xác thực thực nghiệm trên các bộ dữ liệu MNIST và FashionMNIST cho thấy rằng phương pháp đề xuất mang lại độ chính xác phân loại tốt hơn cả trong huấn luyện và kiểm tra, và dẫn đến các lần chạy huấn luyện ổn định hơn (phương sai giảm). Hơn nữa, chúng tôi thực hiện các thí nghiệm ablation cho thấy sự đóng góp của từng cải tiến được đề xuất.
3. Chúng tôi cũng cung cấp tất cả mã cần thiết để tái hiện các thí nghiệm của chúng tôi trong kho lưu trữ GitHub sau2.

2 Công trình liên quan
Tầm quan trọng của tăng tốc suy luận trong mạng nơ-ron feedforward được công nhận rộng rãi và một số phương pháp đã được đề xuất. Các công trình gần đây đã thành công trong việc giảm thời gian suy luận lớp feedforward. Phương pháp Hỗn hợp Chuyên gia (MoE), như được khám phá trong Shazeer et al. (2017) [9], đã chứng minh hiệu quả của nó đối với việc tăng tốc suy luận. MoE liên quan đến việc chia lớp feedforward thành các tập hợp nơ-ron riêng biệt được gọi là "chuyên gia", với một lớp cổng được huấn luyện để chọn hỗn hợp chuyên gia nào sẽ sử dụng trong quá trình chuyển tiếp. Phương pháp này tăng cường tốc độ suy luận bằng cách chỉ sử dụng các khối k hiệu suất hàng đầu, hoặc một biến thể tương tự của nó. Nó hiệu quả giảm thời gian suy luận bằng một hệ số không đổi trong khi duy trì mối quan hệ tuyến tính với độ rộng của lớp feedforward. Tuy nhiên, nó phụ thuộc vào cổng nhiễu để cân bằng tải giữa các chuyên gia, thêm độ phức tạp vào quá trình huấn luyện và khuyến khích sự dư thừa.

Trong [11], các tác giả giới thiệu kiến trúc Fast Feedforward (FFF) như một giải pháp thay thế cho kiến trúc feedforward (FF). FFF hoạt động bằng cách truy cập các khối nơ-ron của nó trong thời gian logarit, mang lại hiệu quả cải thiện. Nó thực hiện điều này bằng cách chia không gian đầu vào thành các vùng riêng biệt sử dụng cây nhị phân khả vi, đồng thời học các ranh giới của những vùng này và các khối nơ-ron được gán cho chúng. Nơ-ron được thực thi có điều kiện dựa trên cấu trúc cây trong quá trình suy luận: một tập con của nút nơ-ron xác định các hỗn hợp của khối nơ-ron lá cần thiết để tạo ra đầu ra cuối cùng. Hơn nữa trong [11,12], các tác giả chứng minh rằng FFF có thể nhanh hơn mạng feedforward tới 220 lần và nhanh hơn mạng hỗn hợp chuyên gia tới 6 lần. Ngoài ra, các tác giả khẳng định rằng FFF thể hiện tính chất huấn luyện vượt trội so với mạng hỗn hợp chuyên gia do phương pháp thực thi có điều kiện không nhiễu của chúng.

Trong bài báo này, chúng tôi sử dụng khái niệm cân bằng tải, trước đây được giới thiệu trong MoE [10,14,8], để đảm bảo tải cân bằng trên các lá của FFF, nhằm cải thiện sự ổn định huấn luyện. Trong bối cảnh MoE, [9] một thuật ngữ bổ sung trong hàm mất mát được giới thiệu, để khuyến khích các chuyên gia nhận khoảng bằng nhau các ví dụ huấn luyện. Ý tưởng này chứng minh là có ý nghĩa cho mục đích cân bằng tải trên phần cứng phân tán.

Hơn nữa, chúng tôi đề xuất trộn đầu ra của FFF với đầu ra của một mạng nơ-ron khác có ít nơ-ron hơn nhiều. Chúng tôi gọi mạng này là "lá chính" vì nó tương tự như các lá của FFF. Trọng số của đầu ra của lá chính được đặt là một tham số có thể huấn luyện. Cảm hứng cho điều này được rút ra từ [15], nơi các tác giả đề xuất tăng cường hiệu suất MoE bằng cách tích hợp mạng cơ sở cùng với chuyên gia được chọn. Điều này được chỉ ra không chỉ cải thiện độ chính xác của mô hình, mà còn cung cấp một đầu ra thoát sớm trong quá trình suy luận, giảm sự dư thừa tính toán cho các mẫu "dễ dàng" hơn. Ngoài ra, hiệu quả tính toán được đạt được bằng cách tái sử dụng các lớp sớm của mô hình cơ sở làm đầu vào cho cổng và các chuyên gia.

2https://github.com/AndreasCharalamp/fastfeedforward-experiments

--- TRANG 3 ---
3 Phương pháp
3.1 Kiến trúc FFF
Mạng feedforward nhanh (FFF) được thiết kế để tận dụng hiện tượng mà các phần khác nhau của miền đầu vào kích hoạt các tập con riêng biệt của nơ-ron trong mạng rộng. FFF phân vùng không gian đầu vào thành các phần riêng biệt bằng cách sử dụng cây nhị phân khả vi, cho phép học đồng thời cả ranh giới phân định các phần này và các đơn vị nơ-ron liên quan đến chúng. Điều này được thực hiện thông qua việc kích hoạt nơ-ron có điều kiện theo cây: một tập con được chỉ định của nút nơ-ron xác định các kết hợp của khối nơ-ron lá được tính toán để tạo ra đầu ra.

3.2 Quá trình Huấn luyện
Các nút được sắp xếp trong một cây khả vi tạo ra lựa chọn mềm trên các lá dưới dạng một vector ngẫu nhiên. Trong huấn luyện, FFF thực hiện một hỗn hợp chuyên gia trên tất cả các lá trong L, trong đó L là tập hợp các lá, với các trọng số của hỗn hợp được tính toán bằng cách leo lên cây từ nút gốc. Trong quá trình suy luận, quyết định tại mỗi nút được lấy là trọng số lớn nhất, và thuật toán chuyển tiếp tiến hành từ gốc, luôn chọn chỉ một nhánh tùy thuộc vào quyết định nút địa phương. Tất cả các lá là mạng Feed-Forward (FF) đơn giản với một lớp ẩn có độ rộng ℓ, và hàm kích hoạt ReLU (Rectified Linear Unit). Các nút của cây là các nơ-ron đơn giản sử dụng hàm kích hoạt sigmoid. Theo ký hiệu của [11] chúng tôi sẽ gọi tổng số nơ-ron trong mỗi mô hình (loại trừ các nút cây trong FFF) là độ rộng huấn luyện và sẽ ký hiệu nó là w. Số lượng nơ-ron của mỗi lá sẽ được ký hiệu bởi ℓ và chúng tôi sẽ gọi nó là độ rộng lá. Đầu ra của FFF trong quá trình huấn luyện có dạng sau:

FFF train(x) = Σ(1≤i≤|L|) li(x)ci(x),                    (1)

trong đó Σ(1≤i≤|L|) ci(x) = 1, |L| là số lượng lá, ℓi(x) là đầu ra của lá i và ci(x) là hệ số hỗn hợp của lá i được tính toán như tích của các cạnh trong đường dẫn từ gốc đến mỗi lá li như được hiển thị trong Hình 1.

Hình 1: Trực quan hóa huấn luyện FFF cho độ sâu cây 2.

--- TRANG 4 ---
Trong quá trình suy luận, đầu ra được tính toán bằng cách đưa ra quyết định cứng tại mỗi cấp của hệ thống phân cấp dẫn đến chỉ c* của ci là 1 và phần còn lại là 0, tức là,

FFF inference(x) = l*(x),                                (2)

trong đó l* là lá mà chúng ta kết thúc, theo các cạnh có giá trị lớn hơn. Theo cách này, mặc dù 2^d·ℓ + 2^(d-1) nơ-ron được sử dụng để huấn luyện, trong đó d là độ sâu của cây, chỉ ℓ + d - 1 được sử dụng để suy luận.

Trong [11] hàm mất mát sau được sử dụng:

L = Lpred + h Lharden,

trong đó Lpred là mất mát entropy chéo của nhiệm vụ, Lharden là một thuật ngữ đẩy các quyết định tại mỗi cấp của cây là 0 hoặc 1 và h là siêu tham số huấn luyện kiểm soát hiệu ứng của việc cứng hóa. Cụ thể, Lharden được định nghĩa là:

Lharden = Σ(i∈B) Σ(N∈N) H(N(i)),

trong đó B là một batch mẫu, N là tập hợp các nút cây của FFF, H(p) là entropy của một biến ngẫu nhiên Bernoulli p. Thuật ngữ bổ sung này là cần thiết để tất cả các cạnh của cây có giá trị gần 1 hoặc 0 cho tất cả đầu vào. Thuật ngữ cứng hóa quan trọng vì FFF được huấn luyện để xuất ra dự đoán dưới dạng tổng có trọng số của các lá, trong khi suy luận chúng ta đưa ra quyết định cứng 0 vs 1 khi đi xuống cây. Để đầu ra suy luận FFF_inference(x) gần nhất có thể với đầu ra huấn luyện FFF_train(x) (xem Eqs. (1) và (2) ở trên) chúng ta nhắm đến tất cả ci gần 0 và chỉ c* gần 1.

Do đó, thông qua thuật ngữ cứng hóa, chúng ta tìm cách buộc trọng số của lá l* gần 1 và trọng số của phần còn lại của các lá gần 0.

3.3 Cân bằng Tải
Trong các thử nghiệm huấn luyện của chúng tôi với FFF, chúng tôi lưu ý rằng chúng rất nhạy cảm với việc khởi tạo trọng số kém. Điều này rõ ràng từ sự biến thiên đáng kể trong độ chính xác kiểm tra quan sát được qua nhiều lần chạy của cùng một thủ tục huấn luyện. Những thách thức tương tự cũng được ghi nhận trong [11], đặc biệt trong Bảng 4 trong Phụ lục, nơi các biến thiên độ chính xác được ghi lại. Để giải thích thêm, hàm mất mát không thúc đẩy việc sử dụng rộng rãi các lá. Do đó, trong quá trình huấn luyện, nếu một lá được gán cho một vùng ít liên quan, nó có khả năng hoàn thành quá trình huấn luyện mà không nắm bắt hiệu quả bất kỳ biểu diễn có ý nghĩa nào.

Để giải quyết điều này, chúng tôi nghiên cứu cách vấn đề này được giải quyết trong kiến trúc MoE. Theo ý tưởng từ [10] chúng tôi đề xuất thêm thuật ngữ sau vào hàm mất mát:

Lbalance = 2^d Σ(i∈leaves) fi Pi,

trong đó fi là tỷ lệ đầu vào được gửi đến lá li và Pi = 1/|B| Σ(x∈B) ci(x) là tổng các hệ số của mỗi lá i trên batch hiện tại B. Thuật ngữ Lbalance được tối thiểu hóa khi tải được cân bằng đều trên tất cả các lá. Tổng mất mát L' bây giờ là

L' = Lpred + h Lharden + α Lbalance,

trong đó α là một siêu tham số kiểm soát hiệu ứng của thuật ngữ cân bằng tải.

3.4 Lá Chính
Lấy cảm hứng từ [15], chúng tôi thử nghiệm với việc thêm một thành phần nơ-ron bổ sung. Thay vì cho phép mỗi tập hợp phân vùng của không gian đầu vào được xử lý độc quyền bởi các tập hợp nơ-ron độc lập (lá) trong quá trình suy luận, chúng tôi cung cấp một tập hợp nơ-ron bổ sung góp phần vào đầu ra cho tất cả đầu vào, và không chỉ một tập con của chúng như phần còn lại của các lá. Chúng tôi giới thiệu một lá chính, góp phần vào đầu ra cuối cùng với một hệ số k. Trong quá trình huấn luyện, đầu ra của kiến trúc mới được công thức hóa như sau:

FFF_ML_Train(x) = k Σ(1≤i≤|L|) li(x)ci(x) + (1-k)ML(x),

--- TRANG 5 ---
trong đó |L| là số lượng lá, ℓi(x) là đầu ra của lá i, ci(x) là hệ số hỗn hợp của lá i, ML là đầu ra của lá chính và k là một tham số có thể huấn luyện với 0 < k < 1. Phương pháp hợp nhất tuyến tính này được làm rõ thêm trong Hình 2.

Hình 2: Trực quan hóa huấn luyện FFF với kiến trúc lá chính.

Trong quá trình suy luận, đầu ra của kiến trúc mới được công thức hóa như sau:

FFF_ML_Inference(x) = k ℓ*(x) + (1-k)ML(x),

trong đó ℓ*(x) là đầu ra của lá với hệ số hỗn hợp lớn nhất c*(x).

Lá chính trải qua huấn luyện đồng thời với FFF trên toàn bộ bộ dữ liệu. Mỗi lá FFF được giao nhiệm vụ xử lý một tập con riêng biệt của không gian đầu vào. Do đó, việc giới thiệu lá chính làm phong phú thêm đầu ra "cục bộ hóa" của một lá thông qua việc kết hợp đầu ra mạng feedforward được huấn luyện tốt¹.

4 Thiết lập Thí nghiệm
Chúng tôi tiến hành một loạt thí nghiệm để điều tra lợi ích về hiệu suất từ:
(1) việc bao gồm thuật ngữ cân bằng tải trong hàm mất mát và
(2) việc tích hợp đầu ra của FFF với đầu ra lá chính, như được mô tả ở trên.

¹Đầu ra lá chính có thể được tính toán song song với đầu ra của lá được chọn từ FFF. Do đó, với việc thực hiện thích hợp, nó không nên ảnh hưởng đáng kể đến tốc độ suy luận.

--- TRANG 6 ---
Xây dựng trên nền tảng được đặt trong [11], chúng tôi áp dụng độ chính xác huấn luyện và kiểm tra làm các thước đo đánh giá của chúng tôi để thuận tiện cho việc so sánh trực tiếp với tài liệu. Mỗi thí nghiệm tập trung vào phân loại hình ảnh, với độ chính xác phân loại được đánh giá thông qua softmax của logits đầu ra theo cách thông thường. Kết quả được báo cáo trên các cơ sở dữ liệu phân loại hình ảnh MNIST và FashionMNIST. Người đọc có thể tham khảo [11] để biết chi tiết về cơ sở dữ liệu và thiết lập thí nghiệm, được phản ánh ở đây.

4.1 Thí nghiệm 1 và 2: Cân bằng Tải
Để điều tra hiệu ứng của cân bằng tải, chúng tôi tái hiện thí nghiệm từ Bảng 1 trong [11] (gọi là baseline từ đây) và so sánh hiệu suất khi sử dụng thuật ngữ cân bằng tải trong hàm mất mát (gọi là balanced từ đây). Chúng tôi báo cáo độ chính xác phân loại trên các bộ dữ liệu MNIST và FashionMNIST cho các tập hợp tham số sau trong thí nghiệm 1: độ rộng lá l ∈ {8,4,2,1} và độ rộng huấn luyện w = 16. Chúng tôi huấn luyện trong 300 epoch với tỷ lệ học lr = 0.001, siêu tham số mất mát h = 1, α = 1 và 300 epoch khác với lr = 0.001, h = 3, α = 0. Chúng tôi sử dụng bộ tối ưu hóa Adam và dừng sớm (nếu không quan sát thấy tăng mất mát trong 50 epoch).

Ngoài ra trong thí nghiệm 2, chúng tôi khám phá các trường hợp cho cơ sở dữ liệu FashionMNIST trong đó độ rộng huấn luyện là w = 128, l ∈ {8,4,2,1} và cũng l ∈ {64,32,16} không được bao gồm trong nghiên cứu ban đầu. Điều này cho phép chúng tôi quan sát độ chính xác đạt được khi kích thước lá tiến gần đến một mạng feedforward đơn giản.

Chúng tôi thực hiện 10 lần chạy huấn luyện và báo cáo độ chính xác tốt nhất và độ chính xác tệ nhất trong Bảng 1 và 2.

4.2 Thí nghiệm 3: Lá Chính với Cân bằng Tải
Tiếp theo, chúng tôi điều tra hiệu suất của kiến trúc Lá Chính trên bộ dữ liệu MNIST. Cho thí nghiệm này, chúng tôi cố định kích thước lá chính ở 8 và cũng bao gồm thuật ngữ cân bằng tải trong hàm mất mát (từ đây được gọi là "master leaf + balanced"). Huấn luyện diễn ra trong 200 epoch với lr = 0.001, h = 1, α = 1 và 100 epoch khác với lr = 0.001, h = 3, α = 0. Chúng tôi huấn luyện bằng bộ tối ưu hóa Adam và dừng sớm (nếu không quan sát thấy tăng trong 50 epoch). Chúng tôi thực hiện 5 lần chạy huấn luyện và báo cáo độ chính xác tốt nhất và độ chính xác tệ nhất trong Bảng 3.

Chúng tôi công bố các tham số cho tất cả các mô hình được huấn luyện trong kho lưu trữ GitHub của chúng tôi (xem liên kết trong Giới thiệu).

5 Kết quả Thí nghiệm
5.1 Thí nghiệm 1: Cân bằng Tải
Kết quả cho mô hình FFF baseline như được báo cáo trong [11] và mô hình FFF cân bằng tải được hiển thị trong Bảng 1 cho các bộ dữ liệu MNIST và FashionMNIST. Mô hình FFF cân bằng tải với chiến lược huấn luyện được đề xuất vượt trội hơn baseline trong tất cả các cài đặt. Cụ thể, chúng tôi quan sát thấy tăng độ chính xác huấn luyện lên đến 16,3% tuyệt đối, đạt được cho ℓ = 1 cho FashionMNIST, trong khi độ chính xác kiểm tra thể hiện tăng tối đa 3,0%, đạt được cho ℓ = 4 cho FashionMNIST.

Cải thiện độ chính xác huấn luyện tuyệt đối trung bình cho MNIST là 2,3% chuyển thành giảm 27% lỗi tương đối. Cải thiện độ chính xác kiểm tra thường nhỏ 0,5% tuyệt đối cho MNIST, nhưng nhất quán và đáng kể cho FashionMNIST trung bình 2,2% tuyệt đối và giảm 10% tỷ lệ lỗi tương đối.

Hơn nữa, rõ ràng là biến thiên độ chính xác giữa các lần chạy huấn luyện đã giảm 4 đến 5 lần trung bình cho cả huấn luyện và kiểm tra khi sử dụng cân bằng tải. Tuy nhiên, biến thiên độ chính xác vẫn cao hơn đáng kể so với FF vani. Chúng tôi tin rằng phương sai trong các mô hình sâu vẫn cao vì yêu cầu mô hình của chúng tôi phân vùng MNIST và FashionMNIST thành w = 16 vùng có ý nghĩa có thể dẫn đến phân mảnh quá mức của không gian đầu vào, như được giải thích trong [11]. Một điều cuối cùng cần lưu ý là thuật ngữ cân bằng tải dường như gây ra overfitting đặc biệt cho các mô hình sâu hơn, tức là độ chính xác huấn luyện cải thiện nhanh hơn độ chính xác kiểm tra.

5.2 Thí nghiệm 2: Cân bằng Tải với Độ Rộng Huấn luyện và Lá Lớn hơn
Sự tăng độ chính xác được làm rõ hơn qua Bảng 2 nơi chúng tôi trình bày kết quả cũng cho trường hợp w = 128 cho FashionMNIST và cũng cho các mô hình sâu hơn.

--- TRANG 7 ---
MNIST
w = 16
độ chính xác huấn luyện     độ chính xác kiểm tra
baseline    balanced        baseline    balanced
vanilla FF  98.0±0.9  -     95.2±0.5  -
ℓ = 8      94.6±19.5  94.6±7.0   93.1±16.6  93.5±6.1
ℓ = 4      91.6±29.3  94.2±3.9   90.8±27.2  91.3±8.9
ℓ = 2      92.1±7.3   95.0±1.7   90.3±5.6   91.0±2.7
ℓ = 1      91.7±7.4   95.2±3.0   89.9±6.4   89.0±8.1

FashionMNIST
w = 16
độ chính xác huấn luyện     độ chính xác kiểm tra
baseline    balanced        baseline    balanced
vanilla FF  91.0±0.7  -     86.4±0.4  -
ℓ = 8      86.7±12.1  90±1.5     84.2±10.9  86.1±1.1
ℓ = 4      86.4±25.0  89.5±0.6   83.3±24.5  85.8±0.9
ℓ = 2      84.5±21.0  91.2±1.4   83.0±11.0  85.4±2.5
ℓ = 1      79.7±9.0   92.7±1.6   78.4±8.0   80.3±9.1

Bảng 1: Độ chính xác phân loại hình ảnh huấn luyện và kiểm tra của các mô hình baseline và cân bằng tải trên MNIST và FashionMNIST. w là độ rộng huấn luyện, ℓ là độ rộng lá. Kết quả có nền xám được sao chép từ [11] để so sánh. x±y có nghĩa là, từ 10 lần chạy huấn luyện, độ chính xác tốt nhất là x và tệ nhất là x-y.

FashionMNIST
w = 16                      w = 128
độ chính xác huấn luyện     độ chính xác kiểm tra     độ chính xác huấn luyện     độ chính xác kiểm tra
baseline    balanced        baseline    balanced      baseline    balanced        baseline    balanced
vanilla FF  91.0  -         86.4  -                  99.3  -                    89.6  -
ℓ = 64     -    -           -    -                   95.6  97.0                 88.8  88.9
ℓ = 32     -    -           -    -                   93.1  96.5                 87.9  88.2
ℓ = 16     -    -           -    -                   92.5  94.3                 87.1  87.5
ℓ = 8      86.7  90.0       84.2  86.1               90.5  92.8                 86.1  86.7
ℓ = 4      86.4  89.5       83.3  85.8               89.0  89.6                 85.4  85.8
ℓ = 2      84.5  91.2       83.0  85.4               87.3  88.3                 84.3  84.8
ℓ = 1      79.7  92.7       78.4  80.3               78.7  84.5                 77.7  79.9

Bảng 2: Độ chính xác huấn luyện và kiểm tra đạt được với cân bằng tải (baseline vs balanced) cho cơ sở dữ liệu FashionMNIST và cho độ rộng huấn luyện và lá lớn hơn. Kết quả baseline được sao chép từ [11] được tô sáng màu xám, kết quả baseline cho ℓ = 16,32,64 là của chúng tôi.

Chúng tôi quan sát thấy rằng cân bằng tải cải thiện độ chính xác so với mô hình FFF baseline cho tất cả các thiết lập. Đối với ℓ ∈ {1,2} chúng tôi quan sát thấy rằng chúng tôi có kết quả tốt hơn cho trường hợp w = 16 thay vì w = 128 có thể do phân mảnh quá mức không gian đầu vào được đề cập trước đó. Kết quả có thể được cải thiện thêm nếu chúng tôi cứng hóa các mô hình của chúng tôi thêm nhiều epoch. Lưu ý rằng cân bằng tải cung cấp cải thiện độ chính xác nhất quán ngay cả đối với các mô hình sâu hiệu suất tốt nhất. Càng nhiều lá trong mô hình, càng khó tìm ra phân vùng tốt của không gian đầu vào mà không sử dụng cân bằng tải.

5.3 Thí nghiệm 3: Lá Chính và Cân bằng Tải
Kết quả trên MNIST khi thêm nút lá chính kích thước 8 được hiển thị trong Bảng 3. So với baseline và hiệu suất Bảng 1 (chỉ sử dụng cân bằng tải) chúng tôi thấy cải thiện đáng kể về độ chính xác huấn luyện cho cả w = 16 và w = 128. Độ chính xác kiểm tra cũng cải thiện trong phần lớn các trường hợp. Như dự kiến, cải thiện lớn hơn đối với w = 16 so với w = 128, thường là 3,8% so với 1,3% cải thiện độ chính xác tuyệt đối, tương ứng. Ngoài ra, việc thêm lá chính làm giảm thêm biến thiên hiệu suất giữa các lần chạy đưa nó đến mức độ hợp lý so sánh với FF vani cho w = 16. Nhìn chung, việc trộn đầu ra của FFF với đầu ra của một mạng nơ-ron đơn giản là một hướng rất triển vọng.

--- TRANG 8 ---
MNIST
w = 16
độ chính xác huấn luyện                độ chính xác kiểm tra
baseline    master leaf + balanced    baseline    master leaf + balanced
vanilla FF  98.0±0.9  -              95.2±0.5  -
ℓ = 8      94.6±19.5  96.7±1.4       93.1±16.6  94.8±0.5
ℓ = 4      91.6±29.3  96.7±1.6       90.8±27.2  94.7±2.0
ℓ = 2      92.1±7.3   97.2±1.5       90.3±5.6   94.1±1.1
ℓ = 1      91.7±7.4   97.3±0.9       89.9±6.4   93.8±1.8

w = 128
độ chính xác huấn luyện                độ chính xác kiểm tra
baseline    master leaf + balanced    baseline    master leaf + balanced
vanilla FF  100±0.0  -               98.1±0.1  -
ℓ = 8      99.3±1.0  100±0.0         94.9±0.6  95.1±0.3
ℓ = 4      97.6±0.6  99.8±0.5        93.6±0.5  95.0±1.8
ℓ = 2      96.2±1.4  99.7±2.6        92.4±0.6  93.7±3.1
ℓ = 1      94.1±0.9  99.7±0.7        92.0±0.7  91.6±10.1

Bảng 3: Độ chính xác huấn luyện và kiểm tra đạt được với các mô hình lá chính cũng sử dụng thuật ngữ mất mát cân bằng tải cho cơ sở dữ liệu MNIST. w là độ rộng huấn luyện, ℓ là độ rộng lá. Kết quả baseline được sao chép từ [11] được tô sáng màu xám. x±y có nghĩa là từ 5 lần chạy huấn luyện, độ chính xác tốt nhất là x và tệ nhất là x-y.

6 Kết luận
Chúng tôi đã tăng cường kiến trúc FFF được đề xuất trong [11] với một thuật ngữ mất mát cân bằng tải và một nút lá chính đạt được độ chính xác cải thiện nhất quán cho các nhiệm vụ phân loại hình ảnh MNIST và FashionMNIST. Đặc biệt đáng chú ý là sự tăng độ chính xác đối với FFF sâu. Cũng đáng chú ý như nhau là sự giảm biến thiên độ chính xác trong các lần chạy huấn luyện của chúng tôi. Kết quả này nhấn mạnh tính mạnh mẽ được tạo ra bởi việc kết hợp thuật ngữ cân bằng tải và kiến trúc lá chính vào FFF. Các kết luận chính từ 3 thí nghiệm và các hướng tương lai được đề xuất được thảo luận tiếp theo:

1. Kết quả Thí nghiệm 1 xác nhận niềm tin của chúng tôi rằng độ chính xác kiểm tra biến thiên lớn được gây ra bởi cây không cân bằng. Thêm thuật ngữ cân bằng tải trong huấn luyện của chúng tôi, chúng tôi đạt được sử dụng lá tốt hơn dẫn đến tăng tính mạnh mẽ.

2. Kết quả Thí nghiệm 2 chỉ ra rằng chúng tôi có thể đạt được hiệu suất tốt hơn đáng kể bằng cách sử dụng cân bằng lá, khi chúng tôi vượt qua [11] độ chính xác tốt nhất cho tất cả w, ℓ. Do đó, chúng tôi tin rằng đáng để đánh giá hiệu suất bằng cách sử dụng nhiều epoch huấn luyện hơn và phạm vi tham số lớn hơn để khám phá đầy đủ tiềm năng của phương pháp.

3. Kết quả Thí nghiệm 3 cho thấy rằng kiến trúc lá chính vượt trội hơn các mô hình FFF, về độ chính xác kiểm tra và huấn luyện, cho tất cả các trường hợp được điều tra. Mở rộng những thí nghiệm này sang các bộ dữ liệu khác và khám phá các giá trị khác nhau của độ rộng lá chính có tiềm năng đáng kể cho cải thiện hiệu suất thêm.

Hạn chế: Chúng tôi không khám phá đầy đủ không gian (siêu-) tham số do hạn chế tài nguyên tính toán; có thể kết quả có thể được cải thiện thêm thông qua điều chỉnh tham số.

Lời cảm ơn
Chúng tôi muốn cảm ơn các tác giả của [11] vì hướng dẫn về việc thực hiện FFF. Công việc này là một phần của dự án học kỳ cho lớp Nhận dạng Mẫu của chương trình ECE tại NTUA.

--- TRANG 9 ---
Tài liệu tham khảo
[1] A. Radford, K. Narasimhan, T. Salimans, và I. Sutskever, "Improving language understanding by generative pre-training," preprint online: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf , 2018.
[2] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, và D. Amodei, "Language models are few-shot learners," arXiv preprint arXiv: 2005.14165 , 2020.
[3] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, và D. Amodei, "Scaling laws for neural language models," arXiv preprint arXiv: 2001.08361 , 2020.
[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, và I. Polosukhin, "Attention is all you need," arXiv preprint arXiv: 1706.03762 , 2023.
[5] E. Bengio, P.-L. Bacon, J. Pineau, và D. Precup, "Conditional computation in neural networks for faster models," arXiv preprint arXiv: 1511.06297 , 2016.
[6] S. Gray, A. Radford, và D. P. Kingma, "GPU kernels for block-sparse weights." online: https://openai.com/research/block-sparse-gpu-kernels , 2017.
[7] T. Gale, M. Zaharia, C. Young, và E. Elsen, "Sparse GPU kernels for deep learning," arXiv preprint arXiv: 2006.10901 , 2020.
[8] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, và Z. Chen, "GShard: Scaling giant models with conditional computation and automatic sharding," arXiv preprint arXiv: 2006.16668 , 2020.
[9] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, và J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," arXiv preprint arXiv: 1701.06538 , 2017.
[10] W. Fedus, B. Zoph, và N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," arXiv preprint arXiv: 2101.03961 , 2021.
[11] P. Belcak và R. Wattenhofer, "Fast feedforward networks," arXiv preprint arXiv: 2308.14711 , 2023.
[12] ——, "Exponentially faster language modelling," arXiv preprint arXiv: 2311.10770 , 2023.
[13] D. Dai, C. Deng, C. Zhao, R. X. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, Z. Xie, Y. K. Li, P. Huang, F. Luo, C. Ruan, Z. Sui, và W. Liang, "DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models," arXiv preprint arXiv: 2401.06066 , 2024.
[14] N. Shazeer, Y. Cheng, N. Parmar, D. Tran, A. Vaswani, P. Koanantakool, P. Hawkins, H. Lee, M. Hong, C. Young, R. Sepassi, và B. Hechtman, "Mesh-TensorFlow: Deep learning for supercomputers," arXiv preprint arXiv: 1811.02084 , 2018.
[15] A. Royer, I. Karmanov, A. Skliar, B. E. Bejnordi, và T. Blankevoort, "Revisiting single-gated mixtures of experts," arXiv preprint arXiv: 2304.05497 , 2023.
