# 2205.01848.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2205.01848.pdf
# File size: 1792388 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
OPTIMIZING MIXTURE OF EXPERTS USING DYNAMIC RECOMPILATIONS
Ferdinand Kossmann1Zhihao Jia2Alex Aiken3
ABSTRACT
The Mixture of Experts architecture allows for outrageously large neural networks by scaling model parameter
size independently from computational demand (FLOPs). However, current DNN frameworks cannot effectively
support the dynamic data ﬂow in Mixture of Experts, and implementations on top of these frameworks need to
use workarounds that introduce signiﬁcant overheads. To address the limitation of these frameworks, we present
DYNAMOE, a DNN library that uses dynamic recompilations to optimize and adapt the use of computational
resources to the dynamic needs of Mixture of Experts models. Our evaluation shows that DYNAMOEachieves a
1.8×speedup and supports 2.3×larger model sizes when compared to existing MoE systems, even when not
using recompilations. We then present further optimizations enabled by dynamic recompilations that yield an
additional 1.7×speedup while simultaneously reducing memory pressure and improving model quality.
1 I NTRODUCTION
The scaling of model parameters and training sample size
is one of the most effective paths towards more powerful
models (Kaplan et al., 2020; Brown et al., 2020): simple
neural architectures with many trainable parameters and
large amounts of training data generally outperform more
complicated neural networks that are trained at a smaller
scale (Sutton, 2019). However, both the increase of model
parameters and sample size introduce additional computa-
tion, imposing a bottleneck to further scaling.
Mixture of Experts (MoE) addresses this computational bot-
tleneck by allowing model parameters and computational
demand (FLOPs) to be scaled independently. This allows
to further scale the model parameter size by several orders
of magnitude: MoE models like Switch transformer demon-
strate that MoEs make scaling beyond billion model pa-
rameters feasible — Switch transformer hereby has 5000 ×
more parameters than BERT. ST-MoE is currently leading
most NLP benchmarks (Zoph et al., 2022) and V-MoE is
matching the state-of-the-art predictive performance on im-
age tasks with as little as half of the compute at inference
time (Riquelme et al., 2021).
In order to support such large parameter sizes, MoEs only ac-
tivate submodules of the network for each sample, so-called
experts . Which experts to use for a given sample is learned
by a so-called gating network . This introduces dynamic data
ﬂow, where the inputs to an operator are only determined at
runtime and change over the course of training.
1ETH Zurich2Carnegie Mellon University3Stanford. Corre-
spondence to: Ferdinand Kossmann <ferdiko@mit.edu>.Current machine learning systems however still cater to
static workloads and struggle with the dynamic data ﬂow
in MoEs. For example, frameworks such as TensorFlow
require a user to deﬁne computation graphs with static tensor
sizes that cannot be changed during training (Abadi et al.,
2015). To deal with variations in the amount of samples that
are assigned to an expert, users need to employ workarounds
that lead to signiﬁcant runtime overheads and suboptimal
model quality.
In this paper, we introduce RECOMPILE , a simple mecha-
nism that enables dynamic changes to a model architecture
and its tensor sizes during training. RECOMPILE enables
two key systems optimizations that current MoE frameworks
lack. First, by using RECOMPILE , we introduce dynamic
capacity factors that sporadically adapt the tensor sizes of
each expert based on their actual need, reducing the compu-
tational demand and memory requirement of the MoE. The
second optimization leverages that the sample assignments
of the gating network converge early in training. Based on
this observation, we introduce sample assignment caching ,
which caches the sample assignments after their conver-
gence and uses RECOMPILE to eliminate the dependency
between the gating network and the experts, resulting in
more parallelism.
To evaluate RECOMPILE and our MoE optimizations, we im-
plement D YNAMOE, an open-source MoE training system
with a ﬂexible API and rich support for popular operators.
In summary, our contributions are:
•We present DYNAMOE, a publicly available imple-
mentation of MoE. Compared to state-of-the-art public
MoE systems, DYNAMOEdelivers speed ups of uparXiv:2205.01848v2  [cs.LG]  2 Aug 2022

--- PAGE 2 ---
DynaMoE: Dynamically optimized Mixture of Experts
to1.8×while supporting 2.3×larger models before
running out of memory.
•We propose RECOMPILE , a mechanism that can be
added to any ML framework to handle the dynamic
data ﬂow in models such as MoEs. We showcase how
RECOMPILE naturally enables two optimizations for
MoEs that yield an additional speedup of 1.7×. These
optimizations not only reduce runtime but also reduce
memory pressure and improve model quality.
2 B ACKGROUND & R ELATED WORK
We brieﬂy discuss previous work on both the algorithmic
ideas underlying MoEs as well as systems that aim to scale
MoEs.
2.1 Algorithmic work
Mixture of Experts was originally proposed in (Jacobs et al.,
1991). The key idea behind MoEs is to build a network
consisting of multiple submodules, so-called experts . Each
expert should hereby specialize on a subdomain of the input,
assuming that solving only that subproblem is easier than
solving the whole problem altogether. To decide which
experts are the most competent for a given sample, a gating
network assigns weights to each expert’s prediction, and the
ﬁnal MoE prediction is given by the weighted sum of the
expert predictions.
Jacobs et al. (1991) introduce multiple loss functions for
training an MoE, among which we present two: cooperation
lossandspeciﬁcation loss . The cooperation loss Lcoop is
given by Equation 1.
Lcoop(ˆy,O,g ) =L(ˆy,/summationdisplay
igiOi) (1)
whereLis any task-speciﬁc loss function (e.g. categorical
cross-entropy), Oiis the sample prediction of the i-th expert,
O= (O1,...,O n)are the sample predictions of all experts,
girepresents the weight assigned to the i-th expert, and ˆyis
the sample’s label.
The cooperation loss is therefore just the task-speciﬁc loss
between the target vector ˆyand the MoE output/summationtext
igiOi.
Note that the cooperation loss incentivizes experts to coop-
eratively produce a good prediction: The gradients to each
expert’s network in back propagation also depend on the
predictions made by the other experts during the forward
pass.
Alternatively, the speciﬁcation loss is given by Equation 2.
Lspec(ˆy,O,g ) =/summationdisplay
igiL(ˆy,O i) (2)The gradients to each expert’s network are therefore only
dependent on the expert’s own prediction, and not on other
experts’ predictions as with the cooperation loss. As a result,
the speciﬁcation loss allows each expert to learn independent
from other experts, since other experts’ predictions do not
affect its gradients.
DYNAMOEsupports both the cooperation and the speciﬁca-
tion loss. While the cooperation loss is more common and
easier to add to existing frameworks, we ﬁnd that the speci-
ﬁcation loss delivers better results. Appendix A describes
our implementation of the speciﬁcation loss.
Jordan & Jacobs (1993) ﬁrst propose hierarchical MoEs,
where each expert’s network can also be an MoE itself (this
can be continued recursively). Using a hierarchical MoE
architecture reduces the branching factor of each gating
network, which is particularly desirable for models with a
large number of experts.
Shazeer et al. (2017) introduce sparse top-kgating, where
only thosekexperts predict on a sample, to whom the gating
network assigned the highest weights. ktherefore serves as a
hyperparameter that determines the amount of computation
that is performed for each sample and allows the number
of FLOPs (determined by k) to scale independent from the
number of parameters (determined by the total number of
expertsn). Algorithm 1 shows pseudo-code for the forward
pass when using sparse top- kgating.
Algorithm 1 MoE forward pass with top-k gating
Input:x: input sample
k: the number of experts to be selected
G: Gating network
{E1,...,E n}: A pool of nexperts
Output:y: MoE prediction for x
1:score←G(x)
2:indices←argmax k(score)
3:(w1, ...,wk)←normalize( score[indices] )
4:y←0 vector in output shape
5:fori←1, ...,kdo
6:e←indices[i]
7:yi←Ee(x)
8:y+=wi∗yi
9:end for
10:returny
Sparse gating introduces the problem that different experts
typically receive different amounts of training data. In the
beginning of the training, some experts receive more sam-
ples than others due to random initialization. These experts
then have more training data than the others and thus deliver
better predictions, which results in the gating network as-
signing them even more samples. This feedback loop can

--- PAGE 3 ---
DynaMoE: Dynamically optimized Mixture of Experts
lead to all samples being assigned to very few (often only
k) experts. To avoid such situations, various work suggests
to add an imbalance penalty to the loss function (Shazeer
et al., 2017; Fedus et al., 2021). An example of a balance
term is given in equation 3.
2.2 Systems work
GShard (Lepikhin et al., 2020) implements a distributed
version of sparsely gated MoEs and scales MoE training to
600 billion parameters. The system demonstrates that MoEs
can be trained efﬁciently on 2048 TPUs in parallel.
These results were further improved by Switch Transformer,
which simpliﬁes the MoE algorithm and scales to 1.6 tril-
lion parameters (Fedus et al., 2021). Switch transformer
achieves the state-of-the-art predictive performance on sev-
eral language tasks. The simpliﬁcations proposed in Switch
Transformer include assigning each sample to a single ex-
pert (i.e.,k= 1) and using a simpliﬁed balance term given
by Equation 3. We adopt this balance term in D YNAMOE.
B=λ∗n∗n/summationdisplay
i=1Ti·Gi (3)
wherenis the total number of experts, λis a weighting
factor of the regularizer, Tiis the fraction of tokens assigned
to thei-th expert, and Giis the fraction of gating network
probability assigned to the i-th expert over the entire batch.
GShard and the system used to train Switch Transformer
are not publicly available. There exist several open-source
implementations using TensorFlow and PyTorch, but these
implementations suffer from poor performance as they do
not make any modiﬁcations to the frameworks to support
MoEs natively. The implementations furthermore only work
for a single GPU (Rau, 2019; Wang, 2019).
Tensor2Tensor (Vaswani et al., 2018) provides an implemen-
tation of an MoE Transformer built on top of TensorFlow,
but Tensor2Tensor has been deprecated and the MoE model
does currently not function as intended.
FastMoE (He et al., 2021) is a distributed MoE training
system based on PyTorch with support for multiple GPUs.
However, FastMoE lacks crucial features such as support
for capacity factors different from 1.0.
None of the above work, including GShard and Switch
Transformer, addresses the dynamic behaviour of MoEs,
which leads to an inefﬁcient use of computational resources.
3RECOMPILE
This section describes how RECOMPILE works from the
user’s perspective. Our implementation of RECOMPILE ontop of FlexFlow is described in Appendix B. Additionally,
we show in Appendix C that RECOMPILE can be applied
to any other ML framework as well. Appendix C also
shows how the RECOMPILE requirements are currently not
fulﬁlled by popular ML frameworks such as PyTorch and
TensorFlow and how these frameworks can thus beneﬁt
from implementing a mechanism like RECOMPILE .
While RECOMPILE is not speciﬁc to MoEs and can also
be used to optimize other models, this paper focuses on
using RECOMPILE for MoEs. After this section’s general
introduction to RECOMPILE , Section 4 presents two MoE-
speciﬁc optimizations that are enabled through RECOMPILE .
Conceptually, RECOMPILE comprises of a mechanism that
compiles the computation graph again from scratch but only
reallocates resources where required. For example, when
changing the batch dimension of a tensor, any memory
region containing weights remains untouched. This allows
RECOMPILE to avoid prohibitive overheads that would result
from naively recompiling the entire graph.
RECOMPILE further allows to plan such recompilations
while preventing overheads from the graph adjustment deci-
sions. This is important because computing when and how
to alter the computation graph can be expensive and needs
to be carried out frequently (i.e. after each mini-batch). In
fact, determining how to alter the computation graph could
for example require the prediction of an additional neural
network, as discussed in Section 6. To avoid adding such
overheads to the overall training/inference runtime, RECOM -
PILE runs graph adjustment decisions on CPU while the
training or inference of the model continues on GPU.
In summary, an implementation of RECOMPILE must satisfy
the following three requirements.
1.Users can monitor model metrics with negligible over-
head. A model metric can be anything that informs
the user about the current state of the model during
inference or training, such as training loss.
2.Users can alter the computation graph with little over-
head. When and how the graph is altered is determined
by a user-provided recompile trigger , which may base
its decision on model metrics.
3.Executing the recompile trigger and computing the
hardware mapping of the new graph is overlapped with
inference/training using the current graph. For exam-
ple: A user can conduct an expensive computation to
decide how to alter the graph or conduct a long search
for a new parallelization strategy while model training
continues in parallel.
By implementing a RECOMPILE mechanism, DYNAMOE
therefore allows users to easily alter the computation graph

--- PAGE 4 ---
DynaMoE: Dynamically optimized Mixture of Experts
Gating 
network
Expert 
network
Expert
networkAggregateComputational
graph
Parallelization 
strategy searchInitialization
Allocation / 
initialization
Forward pass / 
backward passExecution
recompile 
trigger
Capacity 
factorsAssignment 
caching
Only ever halt execution for
initialization of new graph
...recompile
𝛼=0.7
1.3...0.7
1.3...
recompile
Figure 1. Overview of DYNAMOE, an MoE system the uses RECOMPILE to leverage sample assignment caching and dynamic capacity
factors for speed up, reduced memory pressure, and higher model quality.
based on arbitrary, user-deﬁned events without incurring
any scheduling overheads. DYNAMOEfurther exposes op-
erators and dynamically-sized tensors that leverage RECOM -
PILE and can be used like any other operators or tensors in
only one line of code.
4RECOMPILE -ENABLED OPTIMIZATIONS
This section presents two optimizations for MoE that are
naturally enabled through RECOMPILE and only require few
lines of user code. These optimizations are also schemati-
cally depicted in Figure 1.
4.1 Dynamic capacity factors
Over the course of MoE training, the gating network may
assign varying numbers of samples to an expert. Current
MoE frameworks require the declaration of ﬁxed-sized ten-
sors when constructing an MoE model. To deal with this,
users currently declare overly big tensors (i.e. increase the
batch dimension) and drop samples that still don’t ﬁt into
the tensors (i.e. ignore them during back propagation). As a
result, when a mini-batch contains fewer samples than the
declared capacity for an expert, the ﬁrst entries of the input
tensor will be used for these samples and the remaining
entries will be unused during training by setting gradients
to zeroes.
Fedus et al. (2021) denotes the expert capacity to be the
number of samples that an expert’s input tensor can hold.
For an MoE model with nexperts, where kexperts are
assigned to each sample, an expert capacity factor αis used
to compute each expert’s capacity Cas shown in Equation
4.
C=α∗batch _size∗k/n (4)
Note that a high expert capacity factor requires additionalcomputation (FLOPs), memory and data transfer. On the
other hand, low expert capacity factors lead to more sam-
ples being dropped which leads to a lower model quality.
Deﬁning the expert capacity factor thus involves a trade off
between model performance and computational resources.
Using RECOMPILE , we can largely improve this trade off by
using dynamic capacity factors that adapt according to the
capacity that an expert actually needs. To do so, DYNAMOE
measures how many samples the gating network assigns
to each expert and creates a model metric for that. As
the expert’s capacity requirement changes over the course
of training, the expert’s capacity factor can be adjusted
accordingly by calling RECOMPILE , which allows users to
both adjust the capacity dynamically over time and tailor it
separately to each expert.
4.2 Sample Assignment Caching
During the training of MoEs, it is critical that experts spe-
cialize to subdomains of the input — otherwise, the increase
in parameter size is not as effective since experts are redun-
dant in functionality and most parameters have the same,
general-purpose function. For an expert to specialize to
a subdomain, the expert must be trained on samples that
predominantly are elements of that subdomain. If too many
samples from the remaining input domain are assigned to
the expert, the expert will not specialize and will instead
attempt to reduce the loss for predictions on the entire input
domain.
We hypothesize that the sample assignments of the gating
network converge early in training. Once the gating network
has decided to assign a certain subdomain to some expert,
this decision is reinforced because the expert specializes
on that subdomain, thus delivering better performance on
it than other experts and incentivizing the gating network
to keep assigning samples of that subdomain to the expert.
Once this feedback loop begins, the gating network will

--- PAGE 5 ---
DynaMoE: Dynamically optimized Mixture of Experts
mostly ﬁne tune the weighting of experts and will only
reassign few samples.
Early convergence on sample assignments can be leveraged
to achieve a higher degree of model parallelism. Since
expert batches can only be formed once the sample assign-
ments are known, the forward passes of the experts can only
be executed once the forward pass of the gating network
has ﬁnished. However, if we know the sample assignments
before the gating network has ﬁnished its forward pass, this
dependency can be eliminated and the gating network and
experts can execute their forward passes in parallel. Figure
2 shows the “disposable” dependency as dashed arrows.
Gating 
networkExpert 
network
Expert 
networkAggregateCache
CacheLoss
Metrics
Figure 2. TheCache operator eliminates the dashed dependency
after the sample assignments have converged.
Using RECOMPILE , we can detect when the sample assign-
ments have converged and eliminate the dependency there-
after. More speciﬁcally, we can create a model metric on
how many samples of a mini-batch are reassigned to dif-
ferent experts between two consecutive training epochs. If
only a few samples are reassigned, we can cache the sample
assignments and use the cached assignments to create expert
batches before the gating network has ﬁnished its forward
pass.
DYNAMOEprovides a Cache operator that remembers
sample assignments and creates a metric recording how
many samples have been reassigned between two consecu-
tive epochs. The Cache operator is shown in red in Figure 2.
Even when executing expert and gating net forward passes
in parallel, the Cache operator will update the cached sam-
ple assignments after the gating network has ﬁnished its
forward pass. However, these new assignments will only
be used in the next epoch. Thus, after detecting that the
sample assignments have converged, the model will use the
previous epoch’s sample assignments to achieve a higher
degree of parallelism.
Figure 3 shows how tasks are scheduled with and without
caching enabled on four GPUs. Note how the forward passes
of the gating network and the experts happen in parallel
when caching is enabled.
5 E VALUATION
We evaluate DYNAMOEin three aspects. Section 5.1 com-
pares the end-to-end MoE training performance between
DYNAMOEand existing MoE systems; Section 5.2 eval-
Forward pass Backward passCaching disabled:
Caching enabled:
Forward pass Backward passGPU 1
GPU 2
GPU 3
GPU 4
GPU 1
GPU 2
GPU 3
GPU 4Figure 3. GPU 1 executes the gating network, GPUs 2-4 execute
one expert each. Each box represents the execution of a task.
Caching allows to execute the expert forward passes in parallel to
the gating net forward pass.
uates dynamic capacity factors; and Section 5.3 evaluates
sample assignment caching. We hereby focus on evaluating
individual MoE layers to reduce the inﬂuence of further
operators on runtime as much as possible.
All experiments were conducted on a machine with four
NVIDIA Tesla P100 GPUs and two Intel Xeon E5-2640
v4 CPUs. Unless otherwise stated, all experiments are con-
ducted on CIFAR10 (Krizhevsky et al., 2014) using CNNs
(LeCun et al., 1989) as gating and expert modules.
5.1 End-to-end Performance
Figure 4 compares DYNAMOEagainst FastMoE, a state-of-
the-art open-source MoE implementation (He et al., 2021).
For the comparison, one CPU and one GPU were used. No
RECOMPILE optimizations are enabled and the blue line
reﬂects the baseline D YNAMOE performance.
The benchmark uses the same model architecture that Fast-
MoE uses in their benchmarking script on GitHub (multi-
layer perceptron modules). We recreated this architecture in
DYNAMOEand also set all further hyperparameters equal.
As in FastMoE’s benchmarking script, we used a ﬁxed, ran-
domly generated data batch and measure the end-to-end
throughput for training over 10,000 steps. The scaling of
the parameter size (x-axis) is done by increasing the number

--- PAGE 6 ---
DynaMoE: Dynamically optimized Mixture of Experts
of expertsnin the network.
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75
model parameter count 1e9010002000300040005000samples / s2.3x1.2x
1.5x
1.7x
1.8x
1.7xFastMoE
OOMFastMoE
OOMDynaMoE (without recompile)
FastMoE
Figure 4. DYNAMOEwithout RECOMPILE optimizations is up
to1.8×faster than FastMoE. Furthermore, DYNAMOEis able
to support 2.3×bigger models before running out of memory
(OOM).
Figure 4 shows that DYNAMOEyields a speed up of up
to1.8×over FastMoE on the larger model sizes, which
are the ones that matter most given the goal of MoEs is
to scale model parameters. DYNAMOEwas furthermore
able to support 2.3×larger models before running out of
memory (OOM). We speculate that part of the speed up
comes from DYNAMOEusing a static computation graph
and implementing corresponding optimizations. FastMoE
on the other hand sits on top of PyTorch and uses a dynamic
computation graph, which does not allow for all of these
optimizations.
Figure 5 further shows that DYNAMOEscales linearly when
adding GPUs, both in terms of throughput and model size
that is supported before running out of memory.
1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
model parameter count 1e940006000800010000120001400016000samples / sDynaMoE with 1 GPU
DynaMoE with 2 GPUs
DynaMoE with 3 GPUs
DynaMoE with 4 GPUs
Figure 5. DYNAMOEscales linearly in both the speed up and the
supported model size when adding more GPUs.We now compare the RECOMPILE optimizations against
DYNAMOEwith varying static capacity factors. Since Fast-
MoE does not support capacity factors different from 1.0,
we omit FastMoE in Figure 6.
0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
normalized runtime3234363840424446accuracy (%)1.4x
1.7x
No recompile optimizations
Dynamic capacity factor
Caching & dynamic capcity factor
Figure 6. The combined RECOMPILE optimizations achieve 1.7×
speed up while not affecting the accuracy.
Figure 6 shows how the dynamic capacity factors achieve
the same accuracy as when using a static capacity factor that
ensures that no samples are dropped (in this case, α= 7.0).
The dynamic capacity factor hereby yields a speed up of
1.4×. When sample assignment caching is also enabled, the
speed up increases to a total of 1.7×while still achieving
the same accuracy. Using both optimizations, we get the
same throughput as a static capacity factor of α= 1.0but
without the losses in model quality.
Using only the caching optimization with static capacity fac-
tors doesn’t yield a speed up in our experiments, because the
experimental setup did not allow for a hardware mapping
that exploits additional model parallelism. The hardware
mappings considered were limited to expert parallelism,
which means that the gating network and any expert module
must entirely be placed onto one device and a single module
cannot be distributed among several devices. When reliev-
ing this constraint, or choosing a different experimental set
up (i.e. different hyperparameters or different device topol-
ogy), caching can also yield speed ups with static capacity
factors, as we show in Section 5.3.
Finally, we compare the cooperation loss with the speciﬁca-
tion loss and also look at the baseline of keeping the expert
assignments uniformly ﬁxed, as this hints at where the better
performance is coming from: Better expert assignments or
better experts.
Figure 7 shows that the speciﬁcation loss signiﬁcantly out-
performs the cooperation loss. Furthermore, the speciﬁca-
tion loss seems to be the better loss for both the experts and

--- PAGE 7 ---
DynaMoE: Dynamically optimized Mixture of Experts
0 119 238 357 476 595 714 833 952 1071 1190
epochs102030405060accuracy (%)
Specification loss
Specification loss with
fixed unif. expert assignment
Cooperation loss
Cooperation loss with
fixed unif. expert assignment
Figure 7. The speciﬁcation loss signiﬁcantly outperforms the co-
operation loss.
the gating network by themselves: The speciﬁcation loss
outperforms the cooperation loss when keeping the sample
assignments ﬁxed to a uniform random assignment, which
suggests that the speciﬁcation loss is better for expert train-
ing independent of the gating network. Furthermore, using
learned sample assignments had a greater impact when us-
ing speciﬁcation loss, which suggests that the speciﬁcation
loss is also a better loss function for the gating network.1
5.2 Dynamic Capacity Factors
The lines with a dim color in Figure 8 show the ratio between
the number of samples that are assigned to each expert and
the average number of samples assigned to an expert. For
example, if expert Ehas a ratio of 2 at a given point in
training, it means that the gating network assigned twice the
average number of samples to expert Eat that point in time.
Figure 8 shows how the number of samples assigned to an
expert varies over the course of training and is imbalanced.
As discussed in Section 4, a common way to deal with
imbalance is to determine a capacity factor that multiplies
each expert’s capacity, but bigger capacity factors hereby
introduce a larger overhead in memory usage, runtime and
inter-device data transfer. However, if an expert doesn’t
have enough capacity, samples are ignored during training,
which affects model quality.
In previous work, the capacity factor is determined statically
(ﬁxed over the course of training) and globally (the same
for all experts). However, static, global capacity factors
can neither capture the variations in imbalance over the
course of training, nor the variations between individual
experts. In contrast to static, global capacity factors, Figure
8 shows how DYNAMOEuses different capacity factors for
1It is unexpected that for cooperation loss, learned gating net
assignments did not outperform uniform random assignments.
0 50 100 150 200 250 300 350 400 450 500
epochs0.00.51.01.52.02.53.03.54.0capacity factor in placeExpert 1
Expert 2
Expert 3
Expert 4Expert 5
Expert 6
Expert 7
Expert 8Figure 8. The capacity factors dynamically adjust to the actual
capacity requirements of the experts.
different experts and also changes the capacity factors over
the course of training. The lines in bright color show the
capacity factors that DYNAMOEused for a given expert at
a given point in training.
On average across all experts, the needed capacity factor
to not drop samples is always 1.0 (by deﬁnition). However,
creating a RECOMPILE policy with an average capacity fac-
tor of 1.0 and no sample drops is infeasible: Since there
are slight variations in the number of samples assigned to
an expert between each training iteration, this policy would
need to recompile every single iteration. Since this would
eliminate any speed up, designing a RECOMPILE policy
with small average capacity factor, few sample drops and a
tolerable number of recompilations is non-trivial. It helps
however, to allow the average capacity factor to be above
1.0. Thus the runtime-accuracy trade off is also present
when using dynamic capacity factors. Figure 9 shows that
RECOMPILE policies with different average dynamic capac-
ity factors offer a more attractive trade off in comparison to
using static capacity factors.
5.3 Sample Assignment Caching
Figure 10 shows that sample assignments do in fact converge
early in training. The y-axis depicts the fraction of samples
that have not been reassigned by the gating network between
two consecutive epochs, which is the fraction of samples that
have been correctly cached. In the experiment conducted in
Figure 10, we switched sample assignment caching on if at
least 96% of the samples were correctly cached; caching is
switched off if the percentage of correctly cached samples
drops below 90%, which happened for a very short period
during epoch 48. We additionally did not switch caching
on before epoch 10. Using this policy, caching yielded a
speedup of 1.15×.

--- PAGE 8 ---
DynaMoE: Dynamically optimized Mixture of Experts
0.75 0.80 0.85 0.90 0.95 1.00
normalized runtime4950515253545556accuracy (%)
=1.0
=2.0
=3.0
=4.0
=1.0
=1.6
Static, global capacity factor
Dynamic, local capacity factors
Figure 9. Dynamic, local capacity factors offer a better runtime-
accuracy trade off than static, global ones.
We have observed early sample assignment convergence as
in Figure 10 on different data sets and using different ar-
chitectures and hyperparameters. However, even if sample
assignments should not converge early, having the caching
optimization disabled did not have a signiﬁcant impact on
performance in comparison to not running any caching re-
lated code at all.
0 14 28 42 56 70 84 98 112 126 140
epochs0.00.20.40.60.81.0ratio corr. cached samplesCaching disabled
Threshold disable caching
Threshold enable caching
Caching never enabled
Caching opportunistically
enabled
Figure 10. During phases with few samples being reassigned, the
system switches to using cached sample assignments for speed up.
The system will further detect phases with much reassignment and
switch back to not using cached assignments.
Furthermore, we observed that the ﬁnal accuracy as well as
the accuracy progression over the course of training is gener-
ally unaffected by the caching optimization. For the training
depicted in Figure 10, Appendix D shows the model’s test
accuracy progression over time as an example of how little
the caching optimization affects the progression.6 C ONCLUSION & F UTURE WORK
Using the example of Mixture of Experts, we demonstrate
how RECOMPILE allows for more efﬁcient use of computa-
tional resources for models with dynamic behaviour. While
the MoE algorithm speciﬁcally aims at reducing computa-
tional cost when scaling models, current implementations
often include overheads of 2×in terms of FLOPs and a
signiﬁcantly larger memory usage just to deal with the dy-
namic data ﬂow in MoEs. RECOMPILE hereby not only
saves time and memory but also improves the model qual-
ity because fewer samples are dropped during training and
because models can be scaled even further due to the more
efﬁcient resource use.
Although we have successfully run DYNAMOEon multiple
nodes, scaling our system to huge sizes consisting of several
100s of GPUs remains to be tried out. There also remain
many future research questions regarding MoEs in general
as well as RECOMPILE in particular. Some questions related
toRECOMPILE are outlined here:
•Graph adjustments: It’s not trivial to determine when
and how to alter the computation graph during training
and inference. The policies used in our experiments are
relatively naive and can neither guarantee to be close
to some optimum, nor can they generalize well to other
hyperparameter and data set choices. RECOMPILE pro-
vides a framework where also expensive algorithms
can serve as a RECOMPILE policy without incurring
overheads in the overall training time.
•Remapping onto hardware: The computation graph
might be adjusted so much over the course of training
that remapping the graph using a different paralleliza-
tion strategy can pay off. However, while graph ad-
justments are relatively cheap, remapping the graph
can induce large overheads and should be planned care-
fully. A holistic solution for a RECOMPILE policy
should consider both, graph adjustments and hardware
remappings, at the same time. The policy must hereby
consider the expense of remapping the graph (which
depends on how many graph components are mapped
to a different device) as well as the likelihood of that
mapping paying off over time.
•Training instabilities: Instabilities in training remain
a problem of MoEs and advances in this direction also
beneﬁt the RECOMPILE optimizations: During unsta-
ble phases in training, sample assignment caching is
switched off and therefore doesn’t yield any speed
up. Furthermore, during times of large ﬂuctuations
in the expert capacities, dynamic capacity factors will
frequently issue recompilations which could have a
negative effect on the speed up in extreme cases.

--- PAGE 9 ---
DynaMoE: Dynamically optimized Mixture of Experts
REFERENCES
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z.,
Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M.,
Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,
M., Jia, Y ., Jozefowicz, R., Kaiser, L., Kudlur, M., Lev-
enberg, J., Mané, D., Monga, R., Moore, S., Murray, D.,
Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever,
I., Talwar, K., Tucker, P., Vanhoucke, V ., Vasudevan,
V ., Viégas, F., Vinyals, O., Warden, P., Wattenberg, M.,
Wicke, M., Yu, Y ., and Zheng, X. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015.
URLhttps://www.tensorflow.org/ . Software
available from tensorﬂow.org.
Bauer, M., Treichler, S., Slaughter, E., and Aiken, A. Le-
gion: Expressing locality and independence with logical
regions. In the International Conference for High Per-
formance Computing, Networking, Storage and Analysis ,
2012.
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,
C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,
Wanderman-Milne, S., and Zhang, Q. JAX: composable
transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax .
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,
J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
S., Radford, A., Sutskever, I., and Amodei, D. Language
models are few-shot learners, 2020.
Fedus, W., Zoph, B., and Shazeer, N. Switch transform-
ers: Scaling to trillion parameter models with simple and
efﬁcient sparsity. 2021.
He, J., Qiu, J., Zeng, A., Yang, Z., Zhai, J., and Tang, J.
Fastmoe: A fast mixture-of-expert training system, 2021.
Jacobs, R., Jordan, M., Nowlan, S., and Hinton, G. Adaptive
mixture of local expert. Neural Computation , 3:78–88,
02 1991. doi: 10.1162/neco.1991.3.1.79.
Jia, Y ., Shelhamer, E., Donahue, J., Karayev, S., Long, J.,
Girshick, R., Guadarrama, S., and Darrell, T. Caffe:
Convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093 , 2014.
Jia, Z., Zaharia, M., and Aiken, A. Beyond data and model
parallelism for deep neural networks, 2018.
Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y ., Dean, J.,
Shazeer, N., and Fedus, W. ST-MoE: Designing Stable
and Transferable Sparse Expert Models, 2022.Jordan, M. and Jacobs, R. Hierarchical mixtures of ex-
perts and the em algorithm. In Proceedings of 1993
International Conference on Neural Networks (IJCNN-
93-Nagoya, Japan) , volume 2, pp. 1339–1344 vol.2, 1993.
doi: 10.1109/IJCNN.1993.716791.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and
Amodei, D. Scaling laws for neural language models,
2020.
Krizhevsky, A., Nair, V ., and Hinton, G. The CIFAR10
dataset, 2014. http://www.cs.toronto.edu/
kriz/cifar.html (accessed on 1 Oct 2021).
LeCun, Y ., Boser, B., Denker, J. S., Henderson, D., Howard,
R. E., Hubbard, W., and Jackel, L. D. Backpropagation
applied to handwritten zip code recognition. Neural Com-
putation , 1(4):541–551, 1989. doi: 10.1162/neco.1989.1.
4.541.
Lepikhin, D., Lee, H., Xu, Y ., Chen, D., Firat, O., Huang, Y .,
Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling
giant models with conditional computation and automatic
sharding, 2020.
Neubig, G., Dyer, C., Goldberg, Y ., Matthews, A.,
Ammar, W., Anastasopoulos, A., Ballesteros, M.,
Chiang, D., Clothiaux, D., Cohn, T., Duh, K.,
Faruqui, M., Gan, C., Garrette, D., Ji, Y ., Kong, L.,
Kuncoro, A., Kumar, G., Malaviya, C., Michel, P.,
Oda, Y ., Richardson, M., Saphra, N., Swayamdipta,
S., and Yin, P. Dynet: The dynamic neural net-
work toolkit. CoRR , abs/1701.03980, 2017. URL
http://dblp.uni-trier.de/db/journals/
corr/corr1701.html#NeubigDGMAABCCC17 .
Rau, D. Sparsely-gated mixture-of-experts pytorch
implementation, 2019. https://github.com/
davidmrau/mixture-of-experts (accessed on
24 Aug 2021).
Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M.,
Jenatton, R., Pinto, A. S., Keysers, D., and Houlsby, N.
Scaling vision with sparse mixture of experts, 2021.
Seide, F. and Agarwal, A. Cntk: Microsoft’s open-
source deep-learning toolkit. KDD ’16, pp. 2135,
New York, NY , USA, 2016. Association for Comput-
ing Machinery. ISBN 9781450342322. doi: 10.1145/
2939672.2945397. URL https://doi.org/10.
1145/2939672.2945397 .
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,
Q., Hinton, G., and Dean, J. Outrageously large neural
networks: The sparsely-gated mixture-of-experts layer,
2017.

--- PAGE 10 ---
DynaMoE: Dynamically optimized Mixture of Experts
Sutton, R. The bitter lesson, 2019. http:
//www.incompleteideas.net/IncIdeas/
BitterLesson.html (accessed on 9 Sept 2021).
TensorFlow. Keras callbacks with tensorﬂow.
https://www.tensorflow.org/api_docs/
python/tf/keras/callbacks/Callback .
Theano Development Team. Theano: A Python framework
for fast computation of mathematical expressions. arXiv
e-prints , abs/1605.02688, May 2016. URL http://
arxiv.org/abs/1605.02688 .
Tokui, S., Okuta, R., Akiba, T., Niitani, Y ., Ogawa, T.,
Saito, S., Suzuki, S., Uenishi, K., V ogel, B., and Ya-
mazaki Vincent, H. Chainer: A deep learning frame-
work for accelerating the research cycle. In Proceed-
ings of the 25th ACM SIGKDD International Confer-
ence on Knowledge Discovery & Data Mining , KDD ’19,
pp. 2002–2011, New York, NY , USA, 2019. Associa-
tion for Computing Machinery. ISBN 9781450362016.
doi: 10.1145/3292500.3330756. URL https://doi.
org/10.1145/3292500.3330756 .
Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez,
A., Gouws, S., Jones, L., Kaiser, Ł., Kalchbrenner, N.,
Parmar, N., Sepassi, R., Shazeer, N., and Uszkoreit, J.
Tensor2Tensor for neural machine translation. In Pro-
ceedings of the 13th Conference of the Association for
Machine Translation in the Americas (Volume 1: Re-
search Track) , pp. 193–199, Boston, MA, March 2018.
Association for Machine Translation in the Americas.
URLhttps://aclanthology.org/W18-1819 .
Wang, A., Pruksachatkun, Y ., Nangia, N., Singh, A.,
Michael, J., Hill, F., Levy, O., and Bowman, S. R.
Superglue: A stickier benchmark for general-purpose
language understanding systems, 2020. Leader-
board: https://super.gluebenchmark.com/
leaderboard/ (accessed on 26 Jan 2022).
Wang, P. Sparsely-gated mixture-of-experts pytorch
implementation, 2019. https://github.com/
lucidrains/mixture-of-experts (accessed
on 24 Aug 2021).A S PECIFICATION LOSS AS OPERATOR
The speciﬁcation loss requires the prediction of each chosen
expert and does not operate on the actual model output (see
equation 2). If kexperts are assigned to each sample, the
speciﬁcation loss thus requires k∗batch _size expert predic-
tions – and not batch _size many. This makes it impossible
to support the speciﬁcation loss in the same way that other
loss functions are supported.
When using the speciﬁcation loss, the network output must
consists of two tensors: The concatenated expert predictions
for the loss computation, and the aggregated expert predic-
tions as the actual model output (for example, to compute
metrics such as accuracy on the model performance).
InDYNAMOE, Mixutre of Experts models are deﬁned using
three MoE speciﬁc operators:
•TopK : Get indices and values of the biggest kvalues
from a list of nvalues. This is used to extract the
chosen experts from the gating net logits.
•GroupBy : Given a data batch and a mapping betwen
samples and experts, GroupBy groups the samples
intontensors where the i-th tensor contains all samples
that are mapped to the i-th expert.
•Aggregate : Given the expert predictions and their re-
spective weighting, create the weighted average over
the predictions. This weighted average is the output
prediction of the MoE.
Intuitively, supporting the speciﬁcation loss can be achieved
by altering the Aggregate operator to take a Boolean β
that determines which loss is being used. If βis true, the
speciﬁcation loss is used and Aggregate has two outputs.
Else, the cooperation loss is used and Aggregate only
has one output. Under the hood, this would mean that there
are two different Aggregate operators but this would be
hidden to the user.
However, if the user wants to build a hierarchical model
using the speciﬁcation loss, Aggregate would need to
take two inputs, the concatenated expert predictions and the
aggregated expert predictions. This would require another
Boolean and would lead to four different Aggregate op-
erators under the hood. Furthermore, the API would get
confusing as it is not self-explanatory how to deﬁne the two
Booleans to express the wanted graph functionality. Instead,
we expose a special AggregateSpec operator to the user,
which allows for a more intuitive graph construction.
Both aggregate operators have a single output, where
Aggregate outputs the aggregated expert predictions and
AggregateSpec outputs the concatenated expert predic-
tions. Aggregate ’s output makes up for the MoE’s ﬁnal

--- PAGE 11 ---
DynaMoE: Dynamically optimized Mixture of Experts
Gating 
networkTopK GroupBy
AggregateLoss
MetricsAggregate 
SpecGating 
networkTopK GroupByExpert 
network
Expert 
networkAggregateAggregate 
Spec
Gating 
networkTopK GroupByExpert 
network
Expert 
networkAggregateAggregate 
Spec𝑘′∗𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒
𝑘∗𝑘′∗𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒
𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒
Figure 11. Hierarchical model with speciﬁcation loss
output predictions — that means that Aggregate ’s output
is used as the inferred labels, or to compute the model’s
accuracy. When using the cooperation loss, Aggregate ’s
output is also used for the loss computation (see Figure 12).
When using the speciﬁcation loss however, Aggregate ’s
output is only used as the model output but is not used for
the loss computation, since the loss computation requires all
expert predicitions, i.e. AggregateSpec ’s output. DY-
NAMOElets users easily add an AggregateSpec op-
erator and specify that the AggregateSpec operator’s
output should be used for the loss computation. This is
depicted in Figure 13. Figure 11 further depicts the graph of
a hierarchical model – note how losses can even be mixed
in hierarchical models and the inner MoEs could use a co-
operation loss while the outer ones use a speciﬁcation loss.
Gating 
networkTopK GroupByExpert 
network
Expert 
network
Expert 
networkAggregateLoss
Metrics𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒
Figure 12. Cooperation loss
Gating 
networkTopK GroupByExpert 
network
Expert 
network
Expert 
networkAggregateLoss
MetricsAggregate
Spec𝑘∗𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒
𝑏𝑎𝑡𝑐ℎ_𝑠𝑖𝑧𝑒
Figure 13. Speciﬁcation lossIn conclusion, by exposing an additional operator for the
computation of the speciﬁcation loss, we allow for ﬂexi-
bility (i.e. deﬁning hierarchical models) while at the same
time not introducing confusing Booleans that are not self-
explanatory.
B RECOMPILE IMPLEMENTATION ON TOP
OFFLEXFLOW
FlexFlow (Jia et al., 2018) is a distributed DNN framework
built on top of the Legion programming system (Bauer et al.,
2012). One of the aspects of the Legion design is that it is
highly asynchronous, launching tasks potentially well ahead
of when they can be actually executed. This gives rise to
alaunch frontier along which tasks are launched, and an
execution frontier along which tasks are executed. Figure
14 visualizes these two frontiers – note that tasks between
the two frontiers have been launched but haven’t yet been
executed.
Recompile
T
T T
TT
TTExecution frontier Launch frontier
Future queue:
∆𝑙𝑎𝑢𝑛𝑐ℎT
T∆𝑙𝑎𝑢𝑛𝑐ℎ
F1 F2T
Figure 14. ∆launch indicates the number of forward/backward it-
erations between the launch and execution frontiers and is a hy-
perparameter set by users (here, ∆launch = 1). Like this, users
can precisely control how many iterations after a certain event the
recompilation should take effect.
When changing the computation graph of a model, the

--- PAGE 12 ---
DynaMoE: Dynamically optimized Mixture of Experts
changes will only be reﬂected in tasks that haven’t been
launched yet — all launched tasks still use the previous
original computation graph. As a result, adjustments to a
computation graph take effect at the launch frontier.
However, graph adjustments are triggered based on the
model metrics computed at the execution frontier. This
creates a disparity, where graph adjustments only take effect
with some delay after they are triggered.
RECOMPILE leverages this disparity as a time buffer during
which the execution of the user’s RECOMPILE trigger and
model training are overlapped. During the execution of the
RECOMPILE trigger, no new tasks can be launched since it
is unknown what tasks to launch. Without the buffered tasks
that reside between the two frontiers (are launched but not
executed), the model’s training/inference would thus also
need to be stalled until new tasks can be launched (i.e. the
RECOMPILE trigger has ﬁnished execution). However, as
long as some launched tasks remain to be executed, stalling
task launching does not stall task execution.
Therefore, it is crucial to launch enough tasks before exe-
cuting a RECOMPILE trigger, such that the execution of the
trigger function ﬁnishes before all launched tasks have been
executed. However, launching too many tasks in advance
can result in the adjustment only taking effect after a long
delay, which could lead to less speed up or decreased model
quality. It is therefore critical that RECOMPILE permits
users to specify how many tasks to buffer before executing
aRECOMPILE trigger.
In our RECOMPILE implementation, users specify the num-
ber of tasks to buffer as a number of training/inference
iterations ∆launch .RECOMPILE ensures that ∆launch iter-
ations worth of tasks are launched before a RECOMPILE
trigger is executed. To enforce that exactly ∆launch train-
ing/inference iterations lie between the execution and launch
frontiers, RECOMPILE maintains a queue for each model
metric. When launching a task that returns a model metric, a
future for that metric is pushed to the queue. This future is a
placeholder for the metric’s value, which will only be known
once the task has been executed. Once the task has been
executed, RECOMPILE will pop the corresponding future
from the queue. Since a model metric is computed exactly
once per iteration, the queue’s length exactly indicates how
many training/inference iterations have been launched but
not executed.
RECOMPILE can now simply enforce that the length of all
Future queues corresponds exactly to ∆launch before ex-
ecuting the RECOMPILE trigger. If the queue is too long,
RECOMPILE waits until the right amount of model metrics
have been computed (ﬁnished execution) and then calls
the RECOMPILE trigger with the most recently computed
model metrics. If the queue is too short, RECOMPILE simplylaunches more tasks before executing the trigger. The queue
is only too short in two scenarios: In the beginning and end
of training, and when the user increases ∆launch . Note that
∆launch can always be changed at any point during runtime.
In Figure 14, ∆launch is 1 iteration and DYNAMOEwaits
on Future F1before it will continue to launch tasks and
eventually add Future F3. In Figure 14, the seven tasks
between the execution and the launch frontier correspond
to one iteration, which is indicated by the top arrow la-
beled with ∆launch . In the moment depicted in Figure 14,
the task returning the value of F1has just been executed,
which means that F1will be popped from the queue and
further tasks will be launched. Waiting on F1made sure that
DYNAMOEdoesn’t launch more tasks than ∆launch = 1
iterations ahead.
C S UPPORT FOR RECOMPILE IN OTHER
FRAMEWORKS
RECOMPILE can also be implemented on top of other exist-
ing ML frameworks. This section shows that these frame-
works currently do not fulﬁll the requirements discussed
in Section 3 and would thus also beneﬁt from supporting
a mechanism like RECOMPILE . For reference, the require-
ments are again stated in the following:
1.Users can monitor model metrics with negligible over-
head. A model metric can be anything that informs
the user about the current state of the model during
inference or training, such as training loss.
2.Users can alter the computation graph with little over-
head. When and how the graph is altered is determined
by a user-provided recompile trigger , which may base
its decision on model metrics.
3.Executing the RECOMPILE trigger and computing the
hardware mapping of the new graph is overlapped with
inference/training using the current graph. For exam-
ple: A user can conduct an expensive computation to
decide how to alter the graph or conduct a long search
for a new parallelization strategy while model training
continues in parallel.
Since RECOMPILE only requires the ability to perform RE-
COMPILE computations on CPUs in parallel with the train-
ing of an MoE model on GPUs, RECOMPILE can be im-
plemented for both static and dynamic computation graphs.
DYNAMOEchooses to implement RECOMPILE for a static
computation graph, which lets it combine the best of both
worlds: Static computation graphs are generally faster since
they allow for runtime optimizations that are impossible for
dynamic graphs. We show that DYNAMOEoutperforms
an MoE system that uses PyTorch’s dynamic computation

--- PAGE 13 ---
DynaMoE: Dynamically optimized Mixture of Experts
graph. Nevertheless, a static computation graph with RE-
COMPILE still allows for dynamic graph adjustments — we
will later show that these adjustments even go beyond what
is possible in frameworks like PyTorch.
In the following, we will consider to what extent Tensor-
Flow2and PyTorch are able to fulﬁll the three requirements
in Section 3. While other popular frameworks are not dis-
cussed in detail, PyTorch and TensorFlow should serve as
general examples of the most popular frameworks using a
static and a dynamic computation graph respecitively. Other
popular frameworks also fall into one of these two cate-
gories: Caffe, Jax, Theano, and CNTK employ static com-
putation graphs while Chainer and DyNet employ dynamic
computation graphs (Jia et al., 2014; Bradbury et al., 2018;
Theano Development Team, 2016; Seide & Agarwal, 2016;
Tokui et al., 2019; Neubig et al., 2017).
RECOMPILE in PyTorch. While PyTorch is able to fulﬁll
the ﬁrst two requirements, it fails to fulﬁll the last one.
1.Model metrics can directly be monitored in PyTorch
due to its support for dynamic model construction. Py-
Torch’s API is hereby even more intuitive than the one
ofDYNAMOE: while RECOMPILE triggers and graph
adjustments are deﬁned as functions in DYNAMOE,
users can simply interleave them with the graph con-
struction code in PyTorch.
2.PyTorch supports dynamic modiﬁcations of graph com-
ponents during runtime.
3.PyTorch allows to interleave the graph execution with
execution of code that triggers or performs graph
adjustments ( RECOMPILE trigger). Since PyTorch
launches GPU operators asynchronously, PyTorch can
also execute RECOMPILE triggers in parallel to model
training/inference on GPU. However, PyTorch does
not expose an API that allows RECOMPILE triggers to
use model metrics that are computed ∆launch train-
ing/inference iterations before, where ∆launch can be
adjusted dynamically. Instead, using model metrics
in the RECOMPILE trigger would insert a dependency
to a value computed in the current training/inference
iterations, which would leave little time for the RECOM -
PILE trigger to execute before GPU execution stalls. In
order to support RECOMPILE optimizations, a similar
mechanism as in section B would therefore need to be
implemented, for example using Torch.Futures .
RECOMPILE in TensorFlow. TensorFlow only fulﬁlls
the ﬁrst requirement but fails to fulﬁll the latter two.
2Without eager execution1.TensorFlow provides callbacks for monitoring model
metrics (TensorFlow) in a similar way as DYNAMOE.
However, TensorFlow does not support graph adjust-
ments in a callback function, resulting in inability to
support the second requirement.
2.Graph adjustments in TensorFlow can only occur out-
side of a session. In order to perform a recompilation
during training, users need to split their training proce-
dure into multiple runs of individual Sessions between
which a recompilation can be issued. This is infeasi-
ble, since it would require checkpointing and reloading
the entire model every time a new session is run —
supporting recompilations at the temporal granularity
ofDYNAMOE, would require storing and loading the
entire model after every training batch iteration.
3.Executing custom user code during graph execution
also requires splitting training into many small sessions
with user code executed between sessions. There is
no ability to overlap computation; graph execution
comes to a halt while user code runs, and the overhead
from using many small sessions would be an additional
performance penalty.
In conclusion, TensorFlow and PyTorch do not natively
fulﬁll the three requirements imposed on RECOMPILE . They
can thus not support optimizations like the ones proposed
in Section 4, and would beneﬁt from also implementing
RECOMPILE .
D E FFECT OF CACHING ON ACCURACY
Figure 15 shows the accuracy progression when adaptively
enabling caching over large parts of the training or disabling
it over all of the training. Note that the x-axis depicts run-
time and not traiing epochs. Both models were run for the
same amount of training epochs.
0.0 0.2 0.4 0.6 0.8 1.0
normalized runtime141618202224262830accuracy (%)1.15x
Caching disabled
Caching never enabled
Caching opportunistically
enabled
Figure 15. Enabling caching has little effect on the training
progress and yields a 1.15×speed up.
