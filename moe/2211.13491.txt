# 2211.13491.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2211.13491.pdf
# File size: 807764 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Spatial Mixture-of-Experts
Nikoli Dryden
ETH Z√ºrich
ndryden@ethz.chTorsten HoeÔ¨Çer
ETH Z√ºrich
htor@inf.ethz.ch
Abstract
Many data have an underlying dependence on spatial location; it may be weather
on the Earth, a simulation on a mesh, or a registered image. Yet this feature
is rarely taken advantage of, and violates common assumptions made by many
neural network layers, such as translation equivariance. Further, many works that
do incorporate locality fail to capture Ô¨Åne-grained structure. To address this, we
introduce the Spatial Mixture-of-Experts ( SM OE) layer, a sparsely-gated layer that
learns spatial structure in the input domain and routes experts at a Ô¨Åne-grained
level to utilize it. We also develop new techniques to train SM OEs, including a
self-supervised routing loss and damping expert errors. Finally, we show strong
results for SM OEs on numerous tasks, and set new state-of-the-art results for
medium-range weather prediction and post-processing ensemble weather forecasts.
1 Introduction
Many datasets exhibit an underlying, location-based structure, where the value at a point depends
on where that point is. For example, weather predictions [ 78] depend on their location on Earth;
many scientiÔ¨Åc simulations are parameterized on an underlying mesh [ 9]; data (e.g., faces) may be
speciÔ¨Åcally aligned [ 91]; or it may approximately hold, as in natural images with centered objects.
Further, tasks on such data are often dense regression tasks, such as predicting weather several days in
the future. Numerous architectures have been successfully applied to such tasks. Convolutional neural
networks (CNNs) [ 80] and transformers [ 74] show promise for medium-range weather prediction.
Locally-connected networks (LCNs), which use independent, rather than shared, Ô¨Ålters at each
point, have been applied to weather post-processing [ 38], face recognition [ 46,77,91], and other
tasks [ 18,71], speciÔ¨Åcally to learn local features. Low-rank local connectivity (LRLCN) [ 30] relaxes
the translation equivariance of CNNs while requiring fewer parameters than LCNs. Other approaches,
such as CoordConv [65], add an additional inductive bias by providing explicit input coordinates.
However, for tasks on data with location-based structure, prior approaches suffer from various
limitations. Convolution assumes that data is translation equivariant, which does not hold for such
tasks [ 46,91]. Approaches like LCNs require many parameters. Many architectures have been
designed for classiÔ¨Åcation tasks and fail to perform well on regression because they opererate at too
coarse granularity and are unable to capture key details. This limits their applicability to important
tasks, such as medium-range weather prediction or climate simulations. Indeed, on a simple heat
diffusion task with location-dependent diffusivities (see ¬ß3.1), many approaches do not learn the
location dependence at all and instead converge to an ‚Äúaverage‚Äù diffusivity.
To address this, we introduce a novel neural network layer, the Spatial Mixture-of-Experts ( SM OE)
layer (¬ß2). An SM OEuses a learned gating function to sparsely select and route from a shared set of
experts to each spatial location (e.g., pixel) in an input sample. This enables experts to specialize to
the unique characteristics of different ‚Äúregions‚Äù within the data and easy interpretability by analyzing
the gate. SM OEs require the assumption that all samples in the dataset have a similar underlying
spatial structure, but this often holds (at least approximately) for many datasets, such as weather,
where each example is on the same latitude/longitude grid. For the SM OEgating function, we
introduce tensor routing , a simple and cheap routing function that effectively learns this spatial
36th Conference on Neural Information Processing Systems (NeurIPS 2022).arXiv:2211.13491v1  [cs.LG]  24 Nov 2022

--- PAGE 2 ---
Inductive biasParametersFully-connectedConvolutionCoordConv
LocallyconnectedSpatialMixture-of-Experts(this work)Tiled LCNLow-rank locallyconnectedSMoEwith same experts at every location
SMoEwith different experts at every location(Not to scale)Convolution:Locally-connected:Low-rank locally-connected:Spatial Mixture-of-Experts:Filters
Filtersùõº!ùõº"ùõº#ùõº$ùõº%ùõº&ùõΩ!ùõΩ"ùõΩ#ùõΩ$ùõΩ%ùõΩ&Basis filtersLearnedcombinersùõº#and ùõΩ#
ExpertsLearned gateSMoEslearn fine-grained routing to locationsMixture-of-Experts:Spatial Mixture-of-Experts:ExpertsGateGateExperts‚Ä¢Coarse-grain routing‚Ä¢>Capacity for constant compute‚Ä¢Classification‚Ä¢Fine-grain routing‚Ä¢Learn location-dependent functions‚Ä¢RegressionInputs
InputFigure 1: Comparison of SMoEs to other networks. Left: Qualitative comparison of parameter counts and
inductive biases. Center : Pattern of Ô¨Ålter application among models. Right : Routing in MoEs and SM OEs.
structure (¬ß2.1). We also Ô¨Ånd that end-to-end training approaches, as in standard mixture-of-experts
(MoEs) [ 82,86], do not capture location-dependence well. Instead, we introduce a separate routing
classiÔ¨Åcation loss (¬ß2.2) to train the gate, a self-supervised loss which provides a better learning
signal to the gate about incorrect routings. We also introduce expert error damping (¬ß2.3), which
limits expert updates from misroutings, and further improves performance. Both methods rely on
extracting information from the error signal (gradient of the layer output) which would otherwise be
uninformative. Figure 1 provides an overview of SM OEs and a qualitative comparison to important
related work. With these methods, SM OEs are able to learn Ô¨Åne-grain, location-dependent structure
which other architectures miss, and consequentially deliver much better performance.
SM OEs are related to and inspired by prior work in MoEs (e.g., [ 82,86,102]), but there are key
differences. Existing MoEs use coarse-grained routing at the sample [ 99], token [ 86], or patch [ 82]
level, and experts produce coarse output (e.g., entire samples or channels) while SM OEs route at
a Ô¨Åne-grained, per-pixel level. Fine-grained routing is important for enabling experts to specialize
to Ô¨Åne-scale information, which may be crucial [ 74,84]. Such MoEs also typically aim to increase
model capacity while keeping compute costs constant, whereas the goal of SM OEs is to capture
spatial dependencies. Other work has aimed to incorporate spatial dependence, such as LRLCNs [ 30],
which learn content- and location-dependent weights to combine a set of basis Ô¨Ålters. However,
we found that these prior methods failed to capture the Ô¨Åne-grained features necessary for good
performance on dense regression tasks (¬ß3.1).
We conduct experiments on several benchmark datasets with SM OEs and conduct extensive ablation
studies of SM OEdesign decisions (¬ß3). Using a simple heat diffusion dataset, we showcase the
limitations of other models and the power of SM OEs in a controlled situation. We then apply
SM OEs to medium-range weather prediction and outperform the state-of-the-art [ 53,80] on Weath-
erBench [ 78]; and set a new state-of-the-art for post-processing ensemble weather forecasts on the
ENS-10 dataset [ 8]. Finally, we show that SM OEs can also be applied to image classiÔ¨Åcation tasks,
where we match or outperform LRLCNs while using fewer parameters.
Our code is available at https://github.com/spcl/smoe .
1.1 Related Work
Mixture-of-Experts and conditional computation. While MoEs have long been used [ 15,49,
52], since the advent of deep learning, there has been much work in applying MoEs, conditional
computation, and dynamic routing to DNNs [ 1,4,5,7,10‚Äì12,19‚Äì21,23,25,27,29,33,34,39,51,
59,60,70,75,76,82,83,86,99,102,107]. Often, the goal is to increase model capacity without
a corresponding increase in compute costs and models use coarse-grained routing. Notably, MoEs
route at the sample (e.g., [ 25,29,70,83]), token (e.g., [ 33,59,86]), or patch (e.g., [ 82]) level; are
typically trained with extensive auxiliary losses; and often target classiÔ¨Åcation tasks. This coarse
routing, in particular, means there is limited opportunity for experts to specialize. MoEs speciÔ¨Åcally
for vision tasks also typically operate at a sample-level (e.g., [ 1,4,39,99,102]) and use experts
to specialize Ô¨Ålter or channel choices. In contrast, our SM OEs induce an inductive bias via Ô¨Åne-
grained, location-dependent routing for problems with such underlying structure; typically do not
need auxiliary losses beyond the routing classiÔ¨Åcation loss; and work well for dense regression tasks.
Local spatial structure. Locally connected networks (i.e., convolution Ô¨Ålters without weight sharing)
were historically successful in vision tasks such as facial recognition [ 18,46,77,91]. Further
2

--- PAGE 3 ---
InputExperts ‚Ñ∞ùêªùëäGate ùê∫, tensor routingùíü‚àº‚Ñ∞√óùêª√óùëä(¬ß2.1)Select ùê∏=1experts per pixelApply experts at locationstop$sparse routing (¬ß2)MSE lossùêø=1ùëÅ%ùëã‚àíùëå!Error signalùëëùêøùëëùëã=2ùëÅ(ùëã‚àíùëå)Routing Classification Loss (¬ß2.2)Identify incorrect routings with error signal magnitude‚úì‚úò‚úì‚úìConstruct labels101-12010-120000Compute LossBinary cross-entropyExpert Error Damping (¬ß2.3)Incorrect routings‚úì‚úò‚úì‚úìDamp error signals for incorrect routingsùõº11
ForwardBackwardùê∂=1inputchannelsùê∏‚ãÖùêπ=1output channels‚ù∂‚ù∑‚ù∏Figure 2: Overview of SM OEarchitecture and training using a mean-square error loss and an example input.
¬∂Each location in an input is routed to Eexperts from a setEbased on a gate G(¬ß¬ß2 and 2.1). ¬∑The gate is
trained using a routing classiÔ¨Åcation loss, which identiÔ¨Åes incorrect routings based on the error signal (¬ß2.2).
¬∏The error signal to experts is damped for locations they were incorrectly routed to (¬ß2.3).
work incorporated periodic or tiled weight sharing [ 37,71,105]. However, more recent work often
exclusively uses convolution, which has been found to perform better [ 72] while requiring signiÔ¨Åcantly
fewer parameters. CoordConv [ 65] explicitly provides pixel coordinates as input. Because of their
structure, a single convolution or CoordConv layer cannot learn location-dependent sparse activations
asSM OEs do, and multi-layer networks give limited improvements in practice. Low-rank locally
connected networks [ 30] learn to combine a small number of basis Ô¨Ålters in a location- and input-
dependent manner. These combiners lack sparsity and apply all basis Ô¨Ålters at each point, and are
softmax-normalized, which can result in a few Ô¨Ålters dominating, limiting diversity. Separate work
has used attention to provide position-dependent scaling or integrate context [ 35,50,64,93,95,100].
This has culminated in vision transformers [ 26] and variants [ 28,62,67,96,97], which use specialized
attention mechanisms to integrate spatial information. Other work uses Markov [ 61,66] or conditional
random Ô¨Åelds [ 16,43,106], or graph-based methods [ 17,98,104], to learn long-range dependencies.
Additional work has studied incorporating equivariance properties into neural networks (e.g., [13]).
Sparsity. Our use of sparse routing resembles work on sparsity in DNNs in general [ 45]. Many
approaches learn a Ô¨Åne-grained mask to identify which weights to keep or prune (e.g., [ 63,90]),
while other approaches sparsify activations (e.g., [ 3,47,56,68,69]). These works use sparsity to
improve runtime performance, typically at inference time.
Weather prediction. Medium-range weather prediction, or forecasting roughly three to seven days in
advance [ 6], is of broad societal import [ 57]. There has been much interest in applying deep learning
to this task [ 85], and WeatherBench [ 78] serves as a community benchmark. Deep learning has also
been successfully applied to the related tasks of ensemble post-processing [ 38,79] and now-casting
(forecasting a few hours in advance) [ 2,31,81,87‚Äì89]. Standard CNNs are currently state-of-the-art
on WeatherBench [ 80] (although graph neural networks show promise on similar data [ 53]) and are
competitive on post-processing tasks [ 8].SM OEs can take advantage of the extensive Ô¨Åne-grained,
location dependent structure in weather data for improved performance.
2 Spatial Mixture of Experts
We now introduce the Spatial Mixture-of-Experts ( SM OE) layer. An SM OEuses a learned gating
function (¬ß2.1) to select speciÔ¨Åc experts from a shared set to apply at each point (e.g., pixel) in an
input. The gate learns the underlying spatial structure of the data, and routes speciÔ¨Åc experts to
different ‚Äúregions‚Äù, allowing them to specialize. An SM OEis predicated on the assumption that the
spatial structure is similar across all samples in a dataset. We also assume data are on a Cartesian
mesh (i.e., grid), although this could be generalized. To train SM OEs, we introduce a self-supervised
3

--- PAGE 4 ---
routing classiÔ¨Åcation loss (¬ß2.2) and expert error damping (¬ß2.3), which we found essential for
achieving the best performance. Figure 2 provides an overview of SM OEs and their training.
We Ô¨Årst deÔ¨Åne the SM OElayer in full generality, then discuss the particular implementation we use.
(See ¬ß3.1 for ablations.) Let x2RCHWbe an input sample (we assume 2D data for simplicity).
TheSM OElayer consists of a set Eof experts, of which EjEj will be selected at each point, and
a gating function G:RCHW!RjEjHW. The gating function has exactly Enonzeros at each
spatial point ( HW), which correspond to the selected experts at that point. Each expert e2Eis
applied at the points in xwhere it has been selected, and may include additional context from x(e.g.,
surrounding points if eis convolution), and produces an output of size F. The outputs of each expert
at a point are then concatenated to form the output channel dimension (of size EF) of the SM OE.
More precisely, let gatherI()select only the input entries where Iis nonzero. Then an SM OE is:
y= gatherG(x) 
G(x)[e1(x);: : :;ejEj(x)]
;
whereis element-wise multiplication, [e;: : :]stacks tensors, and y2REFHW. When EjEj ,
the gating function induces signiÔ¨Åcant sparsity; this allows an SM OEto compute experts only at the
points where they are to be applied, avoiding most computation.
This formulation yields a weighted SM OE, where the expert outputs are scaled by the gating function.
An alternative, which is slightly more efÔ¨Åcient and can be more readily interpretable, is an unweighted
SM OE, in which case experts are not scaled, and the gather only uses G(x)to select experts.
In this work, we focus on a simple, yet powerful, SM OE layer using convolution Ô¨Ålters as experts:
yi=G(x)iX
c2[C]wi;c? xc;
where ?is cross-correlation, i2[jEj], and we have elided the gather for simplicity.
2.1 Gating Functions
The gating function Gin an SM OEis critical for learning the underlying location-dependence
of the data. While Gcan be any function, a good gate should be relatively cheap. We follow
prior work on MoEs (e.g., [ 82,86]) and use top- Erouting to select experts and sparsify the gate:
G(x) = topE(g(x)), where g(x)2RjEjHWis a learnable gating layer and topEselects the E
largest entries along the Ô¨Årst (expert) dimension and sets the remaining entries to 0. However, we
found that normalizing the gating function using softmax, as is typically done, was not necessary.
There are many options for the gating layer. Many MoE works have used MLPs [ 82,86], but we found
this did not perform well. Convolution or CoordConv [ 65] also did not perform well (¬ß3.1). Instead,
we Ô¨Ånd that a simple tensor routing gating layer worked best. With this, g(x) =D2RjEjHW,
whereDis a learnable tensor that directly encodes the underlying location dependence and routing
structure without depending on the input. Further, Duses one parameter per expert per location and
requires no computation beyond optimizer updates, making it an efÔ¨Åcient choice.
We initializeDusing a uniform distribution over [ 3jEj=EF;3jEj=EF], which corresponds to a Kaiming
uniform initialization with fan-inEF=jEj[41]. However, in many practical cases, some data about
locations may be known (e.g., whether it is land or sea when doing weather prediction). In such cases,
Dcan be initialized based on this data to assign groups of experts in advance. This allows the gating
function to beneÔ¨Åt from prior knowledge while still being able to adjust the routing. Additionally, a
network may contain many SM OElayers, in which case we can share Dbetween layers that have the
same spatial dimensions and number of experts, which reduces the overhead of the gate.
Finally, many MoE formulations use a number of auxiliary losses to ensure gates learn good routing
policies and avoid mode collapse (e.g., [ 10,82,86]). When training with the routing classiÔ¨Åcation
loss we propose, we did not Ô¨Ånd additional auxiliary losses to be necessary.
2.2 Training and the Routing ClassiÔ¨Åcation Loss
Training MoEs is typically done end-to-end, with the experts and gate learning simultaneously,
and sparse gradients based on the routing. We found this did not lead to good performance with
SM OEs and tensor routing, particularly on regression tasks, as the gate did not learn well: it rarely
changed its routing decisions from initialization. We hypothesize that this is due to a ‚Äúmismatch‚Äù in
4

--- PAGE 5 ---
the gradients on regression tasks, where they are informative for experts but not the gate, because
regression aims to make a continuous prediction over both positive and negative values, whereas
selecting an expert requires a threshold (see ¬ßB). To address this, we train the gate with a separate,
self-supervised loss function, the routing classiÔ¨Åcation (RC) loss. The key idea is to consider routing
as a dense, multi-label classiÔ¨Åcation task: selecting the ‚Äúcorrect‚Äù experts at each point (cf. semantic
segmentation). The RC loss does exactly this, and trains the gate by constructing appropriate labels.
This also helps avoid mode collapse, where only a small number of experts are selected (see ¬ß3.2).
In order to construct these labels, we need to determine whether the gate selected the correct set of
experts at each point. However, such information is not directly available. Instead, we use the error
signal into the SM OElayer (i.e., the gradient of the layer output w.r.t. the loss) as a proxy, and say
that the routing was incorrect when the error signal has a large magnitude for the expert at that point,
as this will imply a correspondingly large gradient update. Further, in the case of a mean-square error
lossL(commonly used for regression), the error signal can directly encode the prediction error. Let
Xbe the predictions, Ythe true values, and Nthe number of elements in X. Then the error signal is:
dL
dX=d
dX1
NX
(X Y)2=2
N(X Y):
Hence, the error signal is simply the (scaled, signed) error of the predictions. While this exact relation
ceases to hold as backpropagation proceeds, the intuition behind the error signal magnitude remains.
We now deÔ¨Åne the routing classiÔ¨Åcation loss. Given the error signal "into an SM OElayer, and an
error quantile q(a hyperparameter), we say that selecting an expert at a point was incorrect if "at
that expert and point is greater than the qth quantile of ", and correct otherwise. We use quantiles as
they are independent of "‚Äôs scale, which may be hard to determine and change during training. We
then construct the labels for each point as follows: Unselected experts start with label 0. A correctly
selected expert has label 1. Finally, if an expert was incorrectly selected, its label is 0and we add
1=(jEj  E)to the label value of each unselected expert (note jEj  Eis the number of unselected
experts). This corresponds to a uniform prior that the correct expert could be any unselected expert.
With these labels, the RC loss for a gate is then the binary cross-entropy loss of the gate output.
2.3 Expert Error Damping
While the RC loss enables an SM OEgate to be trained directly based on whether it routed correctly,
experts that were incorrectly routed to still perform gradient updates based on this routing. This
results in experts updating to improve their performance for locations they may not be applied at in
future training iterations after routing changes. To mitigate this, we propose expert error damping ,
where the portion of an error signal that corresponds to incorrect routings is damped to limit incorrect
updates. We Ô¨Ånd that this can improve SM OE performance and reduce convergence time.
Expert error damping is similar to the RC loss, and we classify incorrect routings in the same way.
We then scale the error signal into the experts by a constant factor at each point where the routing
was incorrect. This will limit the magnitude of the update made in response to the misrouting.
2.4 Practical Implementation
We now discuss the implementation of an SM OElayer, primarily focusing on the simple SM OE
with convolution Ô¨Ålters we use. In an ideal implementation, the Ô¨Çop cost of an SM OEis the cost of
the gate plus the cost of applying the selected experts. When using a tensor routing gate, there are
no Ô¨Çops in the gate. The Ô¨Çops from applying the selected convolutional experts is equivalent to a
standard convolutional layer with a number of Ô¨Ålters equal to the number of selected experts. Hence,
SM OEs are quite efÔ¨Åcient Ô¨Çop-wise. However, recent work has shown that data movement is also
critical to performance [ 48]. Because of this, we do not expect even well-optimized implementations
to match the runtime of a convolution layer due to the sparse routing, limited locality in accessing
expert Ô¨Ålters, and other operations, although work on hardware-accelerated sparsity [ 22] should offer
beneÔ¨Åts. Despite this, during inference additional optimizations may be available because the tensor
routing gate does not depend on the input and computations could be reordered to maximize locality.
Unfortunately, efÔ¨Åciently implementing the irregular access patterns in an SM OEis challenging in
standard Python frameworks, and likely requires custom compute kernels. Instead we opt for a na√Øve
implementation in PyTorch [ 73] where we apply all experts at all points and then use gather and
scatter operations to implement sparse routing. Experts are concatenated in sorted routing score
5

--- PAGE 6 ---
Model #Params Epochs % within 1%
Convolution 146 102 91.30:2
CoordConv [65] 56 50 91.10:2
CondConv [102] 200 120 91.20:3
LRLCN [30] 516 27 91.80:5
LRLCN-ND [30] 12315 35 91.50:3
LCN 36764 110 100.00
FC 16.7M 250 14.21:2
ViT [26] 200k 67 93.50:1
V-MoE [82] 470k 74 93.80:3
SMoE 27+12288 8 100.00
True region mask & diffusion stencilsSMoErouting map & expert weights
Figure 3: Heat diffusion results. Left:SM OEand baseline performance. Center : Dataset region map and diffu-
sion stencils. Right : SM OE routing map and experts. SM OEs learn correct stencils and location-dependence.
order (i.e., as given by G(x)); while this can result in channel orders changing during training, we
Ô¨Ånd it has little impact (see ¬ß3.2). We Ô¨Ånd this implementation sufÔ¨Åcient even for large-scale tasks.
3 Experiments
We now present experimental results using SM OEs and a variety of baseline models, as well as
ablations of different SM OEcomponents. First, we describe a simple location-dependent heat
diffusion dataset, which we use to study the ability of different architectures to learn location
dependence in a controlled environment (¬ß3.1). We then present results on the WeatherBench [ 78]
medium-range weather forecasting challenge (¬ß3.2) and the ENS-10 [ 8] dataset for post-processing
ensemble weather forecasts (¬ß3.3), where SM OEs set new state-of-the-art performance results. Lastly,
we show results on several image classiÔ¨Åcation tasks, illustrating the breadth of SM OEapplicability
even in situations without strict location-dependence (¬ß3.4).
All results were run using PyTorch [ 73] version 1.11 on a large cluster with 16 GB V100 GPUs. We
summarize training details throughout this section, and provide full details in ¬ßA. We use existing
PyTorch implementations when available, and implemented methods ourselves otherwise. Unless
noted, all SM OEresults used 33convolution kernel experts, unweighted tensor routing gates, RC
loss, expert error damping, error quantile q= 0:7, and damping factor 0:1. We report the mean
and standard deviation (meanstd) over ten runs for all experiments, except for WeatherBench and
ImageNet, which used three runs. In total, we used about 30k GPU hours for experiments.
3.1 Location-Dependent Heat Diffusion
To study location-dependence in a controlled manner, we generated a heat diffusion dataset with
location-dependent diffusivities. The task is to predict what the heat will be at the next timestep,
given the current heat at each point. Because we know the exact diffusivities and their distribution in
the data, it is easy to identify how well a model has learned this task. We Ô¨Årst describe the dataset
and its generation in detail, then discuss results and ablation studies on SM OE components.
Dataset. The dataset consists of a region map, where each location is assigned a type, and each type
corresponds to a different heat diffusivity. The region map and diffusivity assignment are then Ô¨Åxed
for the dataset. We generate the region map randomly using a simple preferential attachment method.
This deÔ¨Ånes the location-dependence that we wish to learn. To generate samples, we Ô¨Årst randomly
distribute drops of heat across the domain, then apply Ô¨Åve-point diffusion stencils at each point, using
the diffusivity of the region type for each point. For simplicity, we use zero boundary conditions.
This process is iterated to generate many timesteps from the given starting state. The dataset then
consists of the generated timesteps for many different starting states.
If a model is able to learn the diffusion stencils and the location-diffusivity correspondence, it
can exactly predict the next timestep. Further, the diffusivity stencils are also simple, and exactly
correspond to a 33convolution kernel with no nonlinearity.
The particular dataset we use consists of 100;000 6464samples, with 1;000initial states evolved
for 100 timesteps each. Adding more samples did not signiÔ¨Åcantly change results. There are three
region types, with diffusivities 0:25,0:025, and 0:0025 . Figure 3 (center) shows the region map and
6

--- PAGE 7 ---
Table 1: Effect of RC loss and expert error
damping on the heat diffusion dataset.
RC loss Damping Epochs % within 1%
7 7 14 91.50:4
7 X 16 91.60:2
X 7 15 96.70:4
X X 8 100.00Table 2: Effect of different SM OE gate functions
on the heat diffusion dataset.
Gate #Params % within 1%
Fully-connected 50M 85.25:7
33convolution 27 91.30:3
33CoordConv [65] 81 91.50:4
33LCN 110592 96.31:2
33CoordConv3 255 91.60:2
Tensor routing 12288 100.00
diffusion stencils. We report results using ‚Äú% within 1%‚Äù, the percentage of locations in a sample that
are within 1% relative error of the true value, as this is more interpretable than a mean-square error.
Results. Figure 3 (left) shows results on the heat diffusion dataset for SM OEs and a number of
baselines: CNNs, LCNs, fully-connected (FC) layers, CoordConv [ 65], CondConv [ 102], LRL-
CNs [ 30], vision transformers (ViT) [ 26], and vision MoEs (V-MoE) [ 82]. For the LCN and FC
layers, we use only a single layer as additional ones showed no beneÔ¨Åt. Convolution, CoordConv,
CondConv, and LRLCN are the best network found among a set with up to three layers, 33kernels,
12 Ô¨Ålters per layer, batchnorm, and ReLU activations. ViT and V-MoE use one transformer block
with a patch size of 44, an embedding dimension of 128, and four heads. LRLCNs use three basis
Ô¨Ålters and an input-dependent combiner. We also tried unshared combining weights with no input
dependence (LRLCN-ND). V-MoEs select one expert from a set of three. Our SM OEs use a single
layer with three experts and select E= 1expert per point. All models were trained with batch size
32, Adam [ 54] with a learning rate of 0:001(decayed by 10after no validation improvement for 15
epochs), and early stopping after no improvement for 30 epochs. Additional hyperparameter tuning
did not signiÔ¨Åcantly improve results. We report SM OE parameters as expert +gate parameters.
SM OEs achieve perfect performance on this dataset. Further, by examining the learned routing
and experts (Fig. 3, right), we can see that it has indeed correctly learned the diffusion stencils and
location-dependence. LCNs also achieve this, but require 3more parameters, and require 110
epochs to converge (versus 8 for SM OEs). Fully-connected layers failed to learn the data well, likely
due to the challenge of optimizing so many parameters. Other methods all converge to between 91
and 94% within 1%. Examining their predictions and weights, we observe that they do not appear to
have learned the location-dependence of the diffusivities, and instead converged to predicting with
an ‚Äúaverage‚Äù diffusivity across the domain. We also tried larger (deeper and/or wider) convolutional
networks, but performance did not improve. MoE methods (CondConv and V-MoE) also fail in
this manner, as their coarse-grained experts are unable to specialize. Further, the LRLCN-ND
fails in this manner, despite its architecture being similar to an SM OEwhen there is one output
channel (a location-dependent, softmax-weighted combination of three basis kernels). We believe the
LRLCN-ND exhibits a similar gradient ‚Äúmismatch‚Äù as discussed earlier (¬ß2.2).
We now discuss a number of different ablations of the SM OE architecture and design.
What if the ‚Äúright‚Äù expert conÔ¨Åguration is not known? While in the above experiments, we
were able to select the SM OEexpert conÔ¨Åguration (number of experts, number of selected experts,
expert Ô¨Ålter size) so that it is both necessary and sufÔ¨Åcient to learn the task, in many situations this
information may not be available. We considered alternative SM OEconÔ¨Ågurations varying each of
these parameters: ¬∂using six experts; ¬∑experts with 55kernels; ¬∏and selecting two experts per
location from six total. For case ¬∏, we summed the two SM OE output channels together.
In all three cases, the SM OEachieved 100.00% within 1% on the heat diffusion task. In ¬∂, we
found that they learned duplicate diffusion stencils and still routed them appropriately. ¬∑learned the
Ô¨Åve-point stencil plus a boundary of near-zero values, thus being nearly identical to the 33kernel.
Finally, ¬∏learned diffusion stencils that summed together to produce the correct diffusivity. Thus,
we can see that SM OEs are robust and adapt well to these sorts of architecture choices.
RC loss and expert error damping. Table 1 shows results for training SM OEs with and without
our routing classiÔ¨Åcation loss (¬ß2.2) and expert error damping (¬ß2.3). Without the RC loss, SM OE
performance is at par with other baselines in Fig. 3, but once it is added, performance improves sig-
niÔ¨Åcantly as the gating function now learns the location-dependency in the data. Adding expert error
damping further improves performance and convergence by limiting the impact of gate misroutings
7

--- PAGE 8 ---
Table 3: WeatherBench [ 78] results (latitude-weighted RMSE).
ModelZ500 [ m2s 2] T850 [ K]
3 days 5 days 3 days 5 days
Rasp and Thuerey [80] 3162:45633:11.800:022.840:03
√•2wide 3102:05552:81.760:032.780:01
LRLCN [30] 2901:45491:91.730:032.790:01
ViT ( 22) [26] 4382:86383:12.240:042.880:03
SM OE after Ô¨Årst layer 3051:95562:21.770:012.800:03
Last layer SM OE 2982:65533:21.730:022.780:04
33convs!SM OE 2782:05301:81.690:012.650:01
√•+gate prior 2701:95252:01.660:022.600:01
√•rand Ô¨Åxed gate init 3283:75724:11.890:082.960:05
R&T [80] (pretrained) 2671:85002:41.660:032.430:02
SM OE (pretrained) 2532:14881:71.570:022.340:02
√•+extra ERA5 2321:54401:21.460:022.190:01
√•+ 1:41981:83822:01.420:002.060:02Table 4: ImageNet [24] validation accuracy.
Model Top‚Äì1 % Top‚Äì5 %
ResNet-50 [42, 94] 80.830:0495.390:03
LRLCN [30] 80.900:0295.410:05
SM OE after Ô¨Årst layer 80.850:0595.400:01
Last layer SM OE 80.910:0495.420:03
33convs!SM OE 81.330:0395.520:01
Wide ResNet-50-2 [103] 81.760:0395.740:02
on expert learning. However, damping on its own offers little beneÔ¨Åt, as it does not improve gate
learning. These results show that these reÔ¨Ånements are critical for good performance.
Gating function. Table 2 shows the performance of different gating functions (¬ß2.1) on the SM OE.
We consider six options: A single fully-connected layer (as is commonly used in MoEs [ 82,86]); a
single 33convolution, CoordConv [ 65], or LCN layer; a gate with three CoordConv layers with
batchnorm and ReLU; and our tensor routing gate. When training, we also considered auxiliary losses
and other methods for improving performance (see ¬ßC) and report the best result. Our tensor routing
offers the best performance. An LCN performs second-best, likely because it also uses separate
parameters per location, but uses 9as many parameters and requires signiÔ¨Åcant computation. Other
methods do not appear able to effectively capture location-dependence.
Other ablations. We conduct a number of additional ablation studies in ¬ßC, including using auxiliary
losses and routing noise during training, routing normalizations, and expert functions.
3.2 Medium-Range Weather Prediction
We now discuss results on the WeatherBench [ 78] medium-range weather forecasting benchmark.
This benchmark uses the ERA5 reanalysis dataset [ 44], with hourly global weather data for 1979‚Äì
2018. We use the data subset suggested by Rasp et al . [78] at 5.625 ¬∞resolution ( 3264grid points)
and train on data from 1979‚Äì2015, validate on 2016, and report test results for 2017‚Äì2018. We
otherwise follow the training methodology of Rasp and Thuerey [80]. The target quantities to predict
are geopotential at 500 hPa (Z500) and temperature at 850 hPa (T850) with a three- and Ô¨Åve-day
lead time. We report results using latitude-weighted root-mean-square error (RMSE).
As a baseline, we use the ResNet architecture [ 42] introduced by Rasp and Thuerey [80], which
currently reports the best results on WeatherBench. This architecture consists of 19 residual blocks
each with two [ 33convolution!LeakyReLU!batchnorm!dropout] layers, plus an initial
77convolution layer. All convolutions but the last have 128 Ô¨Ålters. We consider three additional
baselines. The Ô¨Årst is identical to the above, but with twice as many Ô¨Ålters (256) in each convolution.
Second, we replace 33convolutions with LRLCN [ 30] layers. Finally, we use a four-layer ViT [ 26]
with patch size 22, hidden dimension 1024, and eight heads (the best performing conÔ¨Åguration).
We adapt the Rasp and Thuerey ResNet to use SM OEs with three conÔ¨Ågurations: adding an SM OE
layer after the Ô¨Årst convolution; adding an SM OElayer after the Ô¨Ånal convolution; and replacing all
33convolutions with SM OElayers. Each SM OEselects the same number of experts as the original
layer had Ô¨Ålters, and has twice as many experts (i.e., jEj= 256 ,E= 128 ). We also share the tensor
routing gate across all SM OE layers with the same number of experts, so its overhead is minimal.
Because the weather data is on a Ô¨Åxed grid with underlying location-dependence (the Earth), we
expect SM OEs to convey some beneÔ¨Åt by specializing to the characteristics of different regions. In
Table 3, we observe that this is indeed the case. Adding SM OEs improves results in all situations,
with the most signiÔ¨Åcant improvement coming through replacing all 33convolutions with SM OEs.
This showcases the advantage of incorporating appropriate location-dependent biases. Wider ResNets
8

--- PAGE 9 ---
Table 5: Results for prediction correction on the ENS-10 [8] dataset for ensemble post-processing.
Metric ModelZ500 [ m2s 2] T850 [ K] T2m [ K]
5-ENS 10-ENS 5-ENS 10-ENS 5-ENS 10-ENS
EMOS 79.120:1278.800:210.7210:010.7060:040.7200:000.7110:03
U-Net 76.540:2076.180:120.6850:000.6700:010.6570:010.6440:01CRPSSM OE68.940:1467.430:120.6120:010.5900:020.6010:020.5940:02
EMOS 29.210:1829.020:130.2470:000.2450:020.2440:000.2410:02
U-Net 27.780:1127.550:190.2300:010.2290:010.2250:000.2200:01EECRPSSM OE23.790:2023.100:160.2070:030.1970:030.1990:010.1900:02
offer limited improvement (in line with results reported by Rasp and Thuerey [80]). The location-
dependent Ô¨Ålters of LRLCNs improve over ResNets, but fail to match SM OEs. We were unable to
achieve good performance with ViTs, but did observe that they are highly sensitive to patch size.
Incorporating prior knowledge into gates. While the exact nature of the location-dependence of
this data is unknown, we do have a broad prior on some aspects of it, such as whether a point is land
or sea. This information can be incorporated into an SM OEby initializing the tensor routing gate to
bias routing to different experts. To this end, we use the land-sea mask from ERA5 to initialize the
gate to route land locations to half the experts and sea locations to the other half. Note this does not
Ô¨Åx the routing, as the gate is able to adjust as it learns. Further, the land-sea mask is already included
in the input data, so all models already had access to this information.
Results with this are in the ‚Äú +gate prior‚Äù line of Table 3, and perform best. This conÔ¨Åguration sets a
new state-of-the-art for WeatherBench when not using additional data. Indeed, it nearly matches the
performance of a ResNet with 150 years of additional pretraining data from climate simulations [ 80].
We also tried a conÔ¨Åguration where the gate was initialized randomly and Ô¨Åxed rather than learned
(‚Äúrand Ô¨Åxed gate init‚Äù). This performs worse than our baseline, as the network cannot adapt its routing
choices, and each expert sees even fewer points in each sample than a standard network, resulting in
less learning. Thus, learning the routing function is critically important to good performance.
Additional data. Following Rasp and Thuerey [80], we use 150 years of data from the MPI-ESSM-
HR climate model from the CMIP6 archive [ 32] to pretrain our best SM OEconÔ¨Åguration, which was
then Ô¨Åne-tuned on ERA5 data as above. This signiÔ¨Åcantly outperforms both our SM OEs without
pretraining and Rasp and Thuerey‚Äôs pretrained ResNet. We incorporated more data to further push
the performance by adding ERA5 data from the most recent back extension (1959‚Äì1979), increasing
the dataset size by about 50%. This shows improved results; however, we suspect performance is
saturating due to the coarse spatial resolution of the data. We therefore trained a Ô¨Ånal conÔ¨Åguration
with higher resolution ( 1:4) data. Using this, our SM OEssigniÔ¨Åcantly outperform the state-of-the-art
on WeatherBench ; indeed, its performance on T850 is very close to that of the operational Integrated
Forecast System [ 78]. Our results are also competitive with those of Keisler [53], although they are
not directly comparable (due to, e.g., different data resolutions).
Mode collapse. Many MoEs suffer from expert or mode collapse (e.g., [ 10,82,86]), where only a
small number of experts are selected. This is typically avoided with routing noise and/or auxiliary
‚Äúload balance‚Äù losses. On the heat diffusion dataset, we found these losses to offer no beneÔ¨Åt (¬ßC).
We also did not observe mode collapse in SM OEs on WeatherBench. With the RC loss, we directly
train the gate, updating routing weights toward other experts after mistakes, and so avoid such issues.
Expert selection order. During training, the order experts are concatenated may change (due to
changes in relative routing scores, or selecting different experts), which will impact the order of
channels seen by subsequent layers. When training on WeatherBench, we found this not to have a
signiÔ¨Åcant impact: expert order stabilizes early, allowing layers to operate on stable representations.
Further, most ‚Äúswapping‚Äù occurs among low-conÔ¨Ådence experts, so is limited to a subset of channels.
3.3 Post-Processing Ensemble Weather Forecasts
Numerical weather prediction systems typically utilize ensembles of simulations in order to quantify
uncertainty and improve forecast quality [ 14]. However, such ensembles typically exhibit systematic
biases [ 92], and correcting them improves forecast skill [ 14,85,101], a task for which deep learning
has shown promise [ 38,79]. We use the ENS-10 dataset [ 8], which consists of twenty years (1998‚Äì
9

--- PAGE 10 ---
2017) of global reforecast [ 40] data at 0:5spatial resolution. We follow the benchmarking setup of
Ashkboos et al . [8], and correct predictions for Z500, T850, and 2 meter temperature (T2m) at a 48
hour lead-time using both Ô¨Åve and ten ensemble members. We report results using continuous ranked
probability score (CRPS) and extreme event weighted CRPS (EECRPS).
We adapt the U-Net model from the ENS-10 baselines, as it delivers good performance and operates
on global data (other methods use patches). Similar to our approach for WeatherBench, we replace
each33convolution with an SM OEwith four times as many experts as the original layer, and select
the same number of experts as the original layer had Ô¨Ålters. We share tensor routing gates between all
layers with the same spatial dimensions and number of experts, with the exception that the encoder
and decoder trunks also use separate gates. As baselines, we use the original U-Net architecture and
Ensemble Model Output Statistics (EMOS) [36], a standard post-processing method.
We observe in Table 5 that, similar to WeatherBench, SM OEs offer signiÔ¨Åcant improvements in
forecast skill across all situations, and set a new state-of-the-art for prediction correction on the
ENS-10 dataset. This also demonstrates that SM OEs are able to scale to the very large spatial domain
used by the ENS-10 data and still learn the appropriate location dependence.
3.4 Image ClassiÔ¨Åcation
Lastly, we present results on several image classiÔ¨Åcation tasks; we focus here on ImageNet-1k [ 24]
and discuss results on additional datasets in ¬ßD. While these datasets do not have a strict location-
dependent structure, relaxing the strict translation equivariance of convolutions can bring beneÔ¨Åts,
and enables a direct comparison with Elsayed et al . [30] . We follow their experimental methodology
and train using the recipe of Vryniotis [94]. We either insert an SM OElayer after the Ô¨Årst or last
convolutional layer of ResNet-50 [ 42] or replace all 33convolutions with SM OElayers. Our
SM OEs have twice as many experts as the original convolution layer and select half of them, to
keep output dimensions constant. Gating layers are shared among all equally-sized blocks. For
comparison, we also train ResNet-50 with all 33convolutions replaced by LRLCN [ 30] layers; and
a Wide ResNet-50-2 [103], which has comparable parameters to SM OEs.
Table 4 shows that SM OEs outperform LRLCNs when we replace all 33convolutions, while using
56% of the parameters. However, a wide ResNet performs best overall. Nevertheless, this shows that
ImageNet classiÔ¨Åcation does indeed beneÔ¨Åt from relaxing translation equivariance.
4 Discussion
We presented the Spatial Mixture-of-Experts layer, a novel layer that learns underlying location
dependencies in data and then uses Ô¨Åne-grained routing to specialize experts to different areas. We
also introduce a routing classiÔ¨Åcation loss and expert error damping, which enable SM OEs to perform
well on dense regression tasks. Prior work shows limited effectiveness on these tasks: Either it does
not capture location-dependence (e.g., convolutions) or it operates at a coarse-grained level (e.g.,
standard MoEs). By overcoming these challenges, we show a new capability for neural networks,
and set new state-of-the-arts for medium-range weather prediction and ensemble post-processing.
Many other problems of broad societal import have a similar spatial structure, particularly in scientiÔ¨Åc
domains [ 9], and we expect SM OEs to be applicable to them. However, tasks such as facial
recognition and surveillance have also historically shown beneÔ¨Åt from such improvements [ 91] and
SM OEs should therefore be used with care.
SM OEs show that learning location-dependence is a powerful inductive bias for certain types of data,
and there are many avenues for further study. Two key areas of particular interest are to develop
improved implementations for Ô¨Åne-grained, sparse routing; and to generalize SM OEs from operating
on grids to general graphs, which would enable them to be applied to many additional tasks.
Acknowledgements and Disclosure of Funding
We thank the members of SPCL at ETH Z√ºrich
 , and Peter Dueben and Mat Chantry of ECMWF, for helpful
discussions; and the anonymous reviewers for their suggestions and feedback. This work has received funding
from the European High-Performance Computing Joint Undertaking (JU) under grant agreement No. 955513
(MAELSTROM), and from Huawei. N.D. received support from the ETH Postdoctoral Fellowship. We thank
the Swiss National Supercomputing Center (CSCS) and Livermore Computing for computing infrastructure.
10

--- PAGE 11 ---
References
[1]Alhabib Abbas and Yiannis Andreopoulos. 2020. Biased mixtures of experts: Enabling computer
vision inference under data transfer limitations. IEEE Transactions on Image Processing 29 (2020).
arXiv:2008.09662 [cs.LG]
[2]Shreya Agrawal, Luke Barrington, Carla Bromberg, John Burge, Cenk Gazen, and Jason Hickey. 2019.
Machine learning for precipitation nowcasting from radar images. (2019). arXiv:1912.12132 [cs.CV]
[3]Subutai Ahmad and Luiz Scheinkman. 2019. How can we be so dense? The beneÔ¨Åts of using highly
sparse representations. (2019). arXiv:1903.11257 [cs.LG]
[4]Karim Ahmed, Mohammad Haris Baig, and Lorenzo Torresani. 2016. Network of experts for large-scale
image categorization. In European Conference on Computer Vision (ECCV) . arXiv:1604.06119 [cs.CV]
[5]Amjad Almahairi, Nicolas Ballas, Tim Cooijmans, Yin Zheng, Hugo Larochelle, and Aaron Courville.
2016. Dynamic capacity networks. In International Conference on Machine Learning (ICML) .
arXiv:1511.07838 [cs.LG]
[6]American Meteorological Society. 2022. Medium-range forecast. https://glossary.ametsoc.org/
wiki/Medium-range_forecast
[7]Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,
Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al .2021. EfÔ¨Åcient Large Scale Language Modeling
with Mixtures of Experts. arXiv:2112.10684 [cs.CL]
[8]Saleh Ashkboos, Langwen Huang, Nikoli Dryden, Tal Ben-Nun, Peter Dueben, Lukas Gianinazzi, Luca
Kummer, and Torsten HoeÔ¨Çer. 2022. ENS-10: A Dataset For Post-Processing Ensemble Weather Forecast.
Proceedings of the Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks .
arXiv:2206.14786 [cs.LG]
[9]Nathan Baker, Frank Alexander, Timo Bremer, Aric Hagberg, Yannis Kevrekidis, Habib Najm, Manish
Parashar, Abani Patra, James Sethian, Stefan Wild, et al .2019. Workshop report on basic research needs
for scientiÔ¨Åc machine learning: Core technologies for artiÔ¨Åcial intelligence . Technical Report. United
States Department of Energy, OfÔ¨Åce of Science.
[10] Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. 2016. Conditional computation
in neural networks for faster models. In International Conference on Learning Representations Workshops
(ICLR-W) . arXiv:1511.06297 [cs.LG]
[11] Yoshua Bengio. 2013. Deep learning of representations: Looking forward. In International conference on
statistical language and speech processing . arXiv:1305.0445 [cs.LG]
[12] Yoshua Bengio, Nicholas L√©onard, and Aaron Courville. 2013. Estimating or propagating gradients
through stochastic neurons for conditional computation. (2013). arXiv:1308.3432 [cs.LG]
[13] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veli Àáckovi ¬¥c. 2021. Geometric deep learning:
Grids, groups, graphs, geodesics, and gauges. (2021). arXiv:2104.13478 [cs.LG]
[14] Roberto Buizza and David Richardson. 2017. 25 years of ensemble forecasting at ECMWF. ECMWF
Newsletter (2017). Issue 153. https://www.ecmwf.int/node/18198
[15] Ke Chen, Lei Xu, and Huisheng Chi. 1999. Improved learning algorithms for mixture of experts in
multiclass classiÔ¨Åcation. Neural networks 12, 9 (1999).
[16] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. 2017.
DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully
connected CRFs. IEEE transactions on pattern analysis and machine intelligence 40, 4 (2017).
arXiv:1606.00915 [cs.CV]
[17] Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Yan Shuicheng, Jiashi Feng, and Yannis Kalantidis.
2019. Graph-based global reasoning networks. In Conference on Computer Vision and Pattern Recognition
(CVPR) . arXiv:1811.12814 [cs.CV]
[18] Yu-hsin Chen, Ignacio Lopez Moreno, Tara Sainath, Mirk√≥ Visontai, Raziel Alvarez, and Carolina Parada.
2015. Locally-connected and convolutional neural networks for small footprint speaker recognition. In
INTERSPEECH .
11

--- PAGE 12 ---
[19] Zhourong Chen, Yang Li, Samy Bengio, and Si Si. 2019. You look twice: GaterNet for dy-
namic Ô¨Ålter selection in CNNs. In Conference on Computer Vision and Pattern Recognition (CVPR) .
arXiv:1811.11205 [cs.CV]
[20] Kyunghyun Cho and Yoshua Bengio. 2014. Exponentially increasing the capacity-to-computation ratio
for conditional computation in deep learning. (2014). arXiv:1406.7362 [stat.ML]
[21] Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,
Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al .2022. UniÔ¨Åed Scaling Laws for
Routed Language Models. arXiv:2202.01169 [cs.CL]
[22] Shail Dave, Riyadh Baghdadi, Tony Nowatzki, Sasikanth Avancha, Aviral Shrivastava, and Baoxin Li.
2021. Hardware acceleration of sparse and irregular tensor computations of ML models: A survey and
insights. Proc. IEEE 109, 10 (2021). arXiv:2007.00864 [cs.AR]
[23] Andrew Davis and Itamar Arel. 2013. Low-rank approximations for conditional feedforward computation
in deep neural networks. In International Conference on Learning Representations Workshops (ICLR-W) .
arXiv:1312.4461 [cs.LG]
[24] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale
hierarchical image database. In Conference on computer vision and pattern recognition (CVPR) .
[25] Ludovic Denoyer and Patrick Gallinari. 2014. Deep sequential neural network. (2014).
arXiv:1410.0510 [cs.LG]
[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al .2021. An image
is worth 16x16 words: Transformers for image recognition at scale. In International Conference on
Learning Representations (ICLR) . arXiv:2010.11929 [cs.CV]
[27] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al .2021. GLaM: EfÔ¨Åcient Scaling of Language Models with
Mixture-of-Experts. arXiv:2112.06905 [cs.CL]
[28] St√©phane d‚ÄôAscoli, Hugo Touvron, Matthew L Leavitt, Ari S Morcos, Giulio Biroli, and Levent Sagun.
2021. ConViT: Improving vision transformers with soft convolutional inductive biases. In International
Conference on Machine Learning (ICML) . arXiv:2103.10697 [cs.CV]
[29] David Eigen, Marc‚ÄôAurelio Ranzato, and Ilya Sutskever. 2013. Learning factored representations in a
deep mixture of experts. (2013). arXiv:1312.4314 [cs.LG]
[30] Gamaleldin Elsayed, Prajit Ramachandran, Jonathon Shlens, and Simon Kornblith. 2020. Revisiting
spatial invariance with low-rank local connectivity. In International Conference on Machine Learning
(ICML) . arXiv:2002.02959 [cs.CV]
[31] Lasse Espeholt, Shreya Agrawal, Casper S√∏nderby, Manoj Kumar, Jonathan Heek, Carla Bromberg,
Cenk Gazen, Jason Hickey, Aaron Bell, and Nal Kalchbrenner. 2021. Skillful Twelve Hour Precipitation
Forecasts using Large Context Neural Networks. (2021). arXiv:2111.07470 [cs.LG]
[32] Veronika Eyring, Sandrine Bony, Gerald A Meehl, Catherine A Senior, Bjorn Stevens, Ronald J Stouffer,
and Karl E Taylor. 2016. Overview of the Coupled Model Intercomparison Project Phase 6 (CMIP6)
experimental design and organization. GeoscientiÔ¨Åc Model Development 9, 5 (2016).
[33] William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch transformers: Scaling to trillion parameter
models with simple and efÔ¨Åcient sparsity. arXiv:2101.03961 [cs.LG]
[34] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A Rusu, Alexander
Pritzel, and Daan Wierstra. 2017. PathNet: Evolution channels gradient descent in super neural networks.
arXiv preprint arXiv:1701.08734 (2017). arXiv:1701.08734 [cs.NE]
[35] Hiroshi Fukui, Tsubasa Hirakawa, Takayoshi Yamashita, and Hironobu Fujiyoshi. 2019. Attention branch
network: Learning of attention mechanism for visual explanation. In Conference on computer vision and
pattern recognition (CVPR) . arXiv:1812.10025 [cs.CV]
[36] Tilmann Gneiting, Adrian E Raftery, Anton H Westveld III, and Tom Goldman. 2005. Calibrated
probabilistic forecasting using ensemble model output statistics and minimum CRPS estimation. Monthly
Weather Review 133, 5 (2005).
12

--- PAGE 13 ---
[37] Karo Gregor and Yann LeCun. 2010. Emergence of complex-like cells in a temporal product network
with local receptive Ô¨Åelds. arXiv preprint arXiv:1006.0448 (2010). arXiv:1006.0448 [cs.NE]
[38] Peter Gr√∂nquist, Chengyuan Yao, Tal Ben-Nun, Nikoli Dryden, Peter Dueben, Shigang Li, and Torsten
HoeÔ¨Çer. 2021. Deep learning for post-processing ensemble weather forecasts. Philosophical Transactions
of the Royal Society A 379, 2194 (2021). arXiv:2005.08748 [cs.LG]
[39] Sam Gross, Marc‚ÄôAurelio Ranzato, and Arthur Szlam. 2017. Hard mixtures of experts for large
scale weakly supervised vision. In Conference on Computer Vision and Pattern Recognition (CVPR) .
arXiv:1704.06363 [cs.CV]
[40] Thomas M. Hamill, Jeffrey S. Whitaker, and Steven L. Mullen. 2006. Reforecasts: An Important Dataset
for Improving Weather Predictions. Bulletin of the American Meteorological Society 87, 1 (2006).
[41] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectiÔ¨Åers: Surpassing
human-level performance on ImageNet classiÔ¨Åcation. In International conference on computer vision
(ICCV) . arXiv:1502.01852 [cs.CV]
[42] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog-
nition. In Conference on computer vision and pattern recognition (CVPR) . arXiv:1512.03385 [cs.CV]
[43] Xuming He, Richard S Zemel, and Miguel A Carreira-Perpin√°n. 2004. Multiscale conditional random
Ô¨Åelds for image labeling. In Conference on Computer Vision and Pattern Recognition (CVPR) .
[44] Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andr√°s Hor√°nyi, Joaqu√≠n Mu√±oz-Sabater,
Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, et al .2020. The ERA5 global reanalysis.
Quarterly Journal of the Royal Meteorological Society 146, 730 (2020).
[45] Torsten HoeÔ¨Çer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 2021. Sparsity in Deep
Learning: Pruning and growth for efÔ¨Åcient inference and training in neural networks. Journal of Machine
Learning Research 22, 241 (2021). arXiv:2102.00554 [cs.LG]
[46] Gary B Huang, Honglak Lee, and Erik Learned-Miller. 2012. Learning hierarchical representations for
face veriÔ¨Åcation with convolutional deep belief networks. In Conference on Computer Vision and Pattern
Recognition (CVPR) .
[47] Kevin Lee Hunter, Lawrence Spracklen, and Subutai Ahmad. 2021. Two Sparsities Are Better Than One:
Unlocking the Performance BeneÔ¨Åts of Sparse-Sparse Networks. (2021). arXiv:2112.13896 [cs.LG]
[48] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten HoeÔ¨Çer. 2021. Data movement is
all you need: A case study on optimizing transformers. In Conference on Machine Learning and Systems
(MLSys) . arXiv:2007.00072 [cs.LG]
[49] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of
local experts. Neural computation 3, 1 (1991).
[50] Saumya Jetley, Nicholas A Lord, Namhoon Lee, and Philip HS Torr. 2018. Learn to pay attention. In
International Conference on Learning Representations (ICLR) . arXiv:1804.02391 [cs.CV]
[51] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. 2016. Dynamic Ô¨Ålter networks. Advances
in neural information processing systems (NeurIPS) (2016). arXiv:1605.09673 [cs.LG]
[52] Michael I Jordan and Robert A Jacobs. 1994. Hierarchical mixtures of experts and the EM algorithm.
Neural computation 6, 2 (1994).
[53] Ryan Keisler. 2022. Forecasting Global Weather with Graph Neural Networks. (2022).
arXiv:2202.07575 [physics.ao-ph]
[54] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR) . arXiv:1412.6980 [cs.LG]
[55] Alex Krizhevsky. 2009. Learning multiple layers of features from tiny images. (2009).
[56] Mark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin, William
Leiserson, Sage Moore, Nir Shavit, and Dan Alistarh. 2020. Inducing and exploiting activation sparsity
for fast inference on deep neural networks. In International Conference on Machine Learning (ICML) .
[57] Jeffrey K Lazo, Rebecca E Morss, and Julie L Demuth. 2009. 300 billion served: Sources, perceptions,
uses, and values of weather forecasts. Bulletin of the American Meteorological Society 90, 6 (2009).
13

--- PAGE 14 ---
[58] Yann LeCun, L√©on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied
to document recognition. Proc. IEEE 86, 11 (1998).
[59] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. GShard: Scaling giant models with con-
ditional computation and automatic sharding. In International Conference on Learning Representations .
arXiv:2006.16668 [cs.CL]
[60] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. 2021. BASE layers:
Simplifying training of large, sparse models. In International Conference on Machine Learning (ICML) .
arXiv:2103.16716 [cs.CL]
[61] Chuan Li and Michael Wand. 2016. Combining Markov random Ô¨Åelds and convolutional neural
networks for image synthesis. In Conference on Computer Vision and Pattern Recognition (CVPR) .
arXiv:1601.04589 [cs.CV]
[62] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. 2021. SwinIR:
Image restoration using swin transformer. In International Conference on Computer Vision (ICCV) .
arXiv:2108.10257 [eess.IV]
[63] Tao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. 2020. Dynamic
model pruning with feedback. In International Conference on Learning Representations (ICLR) .
arXiv:2006.07253 [cs.LG]
[64] Drew Linsley, Dan Shiebler, Sven Eberhardt, and Thomas Serre. 2019. Learning what and where to
attend. In International Conference on Learning Representations (ICLR) . arXiv:1805.08819 [cs.CV]
[65] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason
Yosinski. 2018. An intriguing failing of convolutional neural networks and the CoordConv solution. In
Advances in Neural Information Processing Systems (NeurIPS) . arXiv:1807.03247 [cs.CV]
[66] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and Xiaoou Tang. 2017. Deep learning Markov
random Ô¨Åeld for semantic segmentation. IEEE transactions on pattern analysis and machine intelligence
40, 8 (2017). arXiv:1606.07230 [cs.CV]
[67] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021.
Swin transformer: Hierarchical vision transformer using shifted windows. In International Conference on
Computer Vision (ICCV) . arXiv:2103.14030 [cs.CV]
[68] E Majani, Ruth Erlanson, and Yaser Abu-Mostafa. 1988. On the K-winners-take-all network. In Advances
in neural information processing systems (NeurIPS) .
[69] Alireza Makhzani and Brendan J Frey. 2015. Winner-take-all autoencoders. In Advances in neural
information processing systems (NeurIPS) . arXiv:1409.2752 [cs.LG]
[70] Mason McGill and Pietro Perona. 2017. Deciding how to decide: Dynamic routing in artiÔ¨Åcial neural
networks. In International Conference on Machine Learning (ICML) . arXiv:1703.06217 [stat.ML]
[71] Jiquan Ngiam, Zhenghao Chen, Daniel Chia, Pang Koh, Quoc Le, and Andrew Ng. 2010. Tiled
convolutional neural networks. Advances in Neural Information Processing Systems (NeurIPS) (2010).
[72] Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A AbolaÔ¨Åa,
Jeffrey Pennington, and Jascha Sohl-Dickstein. 2019. Bayesian deep convolutional networks with
many channels are gaussian processes. In International Conference on Learning Representations (ICLR) .
arXiv:1810.05148 [stat.ML]
[73] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang,
Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie
Bai, and Soumith Chintala. 2019. PyTorch: An imperative style, high-performance deep learning library.
Advances in neural information processing systems (NeurIPS) . arXiv:1912.01703 [cs.LG]
[74] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza
Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al .[n.d.]. FourCastNet:
A global data-driven high-resolution weather model using adaptive fourier neural operators. ([n. d.]).
arXiv:2202.11214
14

--- PAGE 15 ---
[75] Svetlana Pavlitskaya, Christian Hubschneider, Michael Weber, Ruby Moritz, Fabian Huger, Peter Schlicht,
and Marius Zollner. 2020. Using mixture of expert models to gain insights into semantic segmentation. In
Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) .
[76] Prajit Ramachandran and Quoc V Le. 2018. Diversity and depth in per-example routing models. In
International Conference on Learning Representations (ICLR) .
[77] Marc‚ÄôAurelio Ranzato, Joshua Susskind, V olodymyr Mnih, and Geoffrey Hinton. 2011. On deep
generative models with applications to recognition. In Conference on Computer Vision and Pattern
Recognition (CVPR) .
[78] Stephan Rasp, Peter D Dueben, Sebastian Scher, Jonathan A Weyn, Soukayna Mouatadid, and Nils
Thuerey. 2020. WeatherBench: a benchmark data set for data-driven weather forecasting. Journal of
Advances in Modeling Earth Systems 12, 11 (2020). arXiv:2002.00469 [physics.ao-ph]
[79] Stephan Rasp and Sebastian Lerch. 2018. Neural networks for postprocessing ensemble weather forecasts.
Monthly Weather Review 146, 11 (2018). arXiv:1805.09091 [stat.ML]
[80] Stephan Rasp and Nils Thuerey. 2021. Data-driven medium-range weather prediction with a ResNet
pretrained on climate simulations: A new model for WeatherBench. Journal of Advances in Modeling
Earth Systems 13, 2 (2021). arXiv:2008.08626 [physics.ao-ph]
[81] Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan
Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, et al .2021. Skilful precipitation
nowcasting using deep generative models of radar. Nature 597, 7878 (2021). arXiv:2104.00954 [cs.LG]
[82] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr√© Susano
Pinto, Daniel Keysers, and Neil Houlsby. 2021. Scaling Vision with Sparse Mixture of Experts. In
Advances in Neural Information Processing Systems (NeurIPS) . arXiv:2106.05974 [cs.CV]
[83] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. 2018. Routing networks: Adaptive selection of
non-linear functions for multi-task learning. In International Conference on Learning Representations
(ICLR) . arXiv:1711.01239 [cs.LG]
[84] Christoph Sch√§r, Oliver Fuhrer, Andrea Arteaga, Nikolina Ban, Christophe Charpilloz, Salvatore Di Giro-
lamo, Laureline Hentgen, Torsten HoeÔ¨Çer, Xavier Lapillonne, David Leutwyler, et al .2020. Kilometer-
scale climate models: Prospects and challenges. Bulletin of the American Meteorological Society 101, 5
(2020).
[85] MG Schultz, Clara Betancourt, Bing Gong, Felix Kleinert, Michael Langguth, LH Leufen, Amirpasha
Mozaffari, and Scarlet Stadtler. 2021. Can deep learning beat numerical weather prediction? Philosophical
Transactions of the Royal Society A 379, 2194 (2021).
[86] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In
International Conference on Learning Representations (ICLR) . arXiv:1701.06538 [cs.LG]
[87] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. 2015.
Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In Advances
in neural information processing systems (NeurIPS) . arXiv:1506.04214 [cs.CV]
[88] Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun
Woo. 2017. Deep learning for precipitation nowcasting: A benchmark and a new model. In Advances in
neural information processing systems (NeurIPS) . arXiv:1706.03458 [cs.CV]
[89] Casper Kaae S√∏nderby, Lasse Espeholt, Jonathan Heek, Mostafa Dehghani, Avital Oliver, Tim Salimans,
Shreya Agrawal, Jason Hickey, and Nal Kalchbrenner. 2020. MetNet: A neural weather model for
precipitation forecasting. (2020). arXiv:2003.12140 [cs.LG]
[90] Suraj Srinivas, Akshayvarun Subramanya, and R Venkatesh Babu. 2017. Training sparse neu-
ral networks. In Conference on computer vision and pattern recognition workshops (CVPRW) .
arXiv:1611.06694 [cs.CV]
[91] Yaniv Taigman, Ming Yang, Marc‚ÄôAurelio Ranzato, and Lior Wolf. 2014. DeepFace: Closing the gap to
human-level performance in face veriÔ¨Åcation. In Conference on Computer Vision and Pattern Recognition
(CVPR) .
[92] Zoltan Toth and Eugenia Kalnay. 1993. Ensemble forecasting at NMC: The generation of perturbations.
Bulletin of the American Meteorological Society 74, 12 (1993).
15

--- PAGE 16 ---
[93] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon
Shlens. 2021. Scaling local self-attention for parameter efÔ¨Åcient visual backbones. In Conference on
Computer Vision and Pattern Recognition (CVPR) . arXiv:2103.12731 [cs.CV]
[94] Vasilis Vryniotis. 2021. How to Train State-of-the-Art Models Us-
ing TorchVision‚Äôs Latest Primitives. https://pytorch.org/blog/
how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/
[95] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and
Xiaoou Tang. 2017. Residual attention network for image classiÔ¨Åcation. In Conference on computer
vision and pattern recognition (CVPR) . arXiv:1704.06904 [cs.CV]
[96] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,
and Ling Shao. 2021. Pyramid vision transformer: A versatile backbone for dense prediction without
convolutions. In International Conference on Computer Vision (ICCV) . arXiv:2102.12122 [cs.CV]
[97] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and
Ling Shao. 2022. PVT v2: Improved baselines with Pyramid Vision Transformer. Computational Visual
Media (2022). arXiv:2106.13797 [cs.CV]
[98] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local neural networks. In
Conference on computer vision and pattern recognition (CVPR) . arXiv:1711.07971 [cs.CV]
[99] Xin Wang, Fisher Yu, Lisa Dunlap, Yi-An Ma, Ruth Wang, Azalia Mirhoseini, Trevor Darrell, and
Joseph E Gonzalez. 2020. Deep mixture of experts via shallow embedding. In Uncertainty in ArtiÔ¨Åcial
Intelligence . arXiv:1806.01531 [cs.CV]
[100] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. 2018. CBAM: Convolutional block
attention module. In European conference on computer vision (ECCV) . arXiv:1807.06521 [cs.CV]
[101] World Meteorological Organization. 2021. Guidelines on Ensemble Prediction System Postprocessing,
2021 edition . Technical Report. World Meteorological Organization.
[102] Brandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. 2019. CondConv: Conditionally
parameterized convolutions for efÔ¨Åcient inference. In Advances in Neural Information Processing Systems
(NeurIPS) . arXiv:1904.04971 [cs.CV]
[103] Sergey Zagoruyko and Nikos Komodakis. 2016. Wide residual networks. In British Machine Vision
Conference (BMVC) . arXiv:1605.07146 [cs.CV]
[104] Songyang Zhang, Xuming He, and Shipeng Yan. 2019. LatentGNN: Learning efÔ¨Åcient non-
local relations for visual recognition. In International Conference on Machine Learning (ICML) .
arXiv:1905.11634 [cs.CV]
[105] Kaili Zhao, Wen-Sheng Chu, and Honggang Zhang. 2016. Deep region and multi-label learning for facial
action unit detection. In Conference on computer vision and pattern recognition (CVPR) .
[106] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Dalong
Du, Chang Huang, and Philip HS Torr. 2015. Conditional random Ô¨Åelds as recurrent neural networks. In
International Conference on Computer Vision (ICCV) .
[107] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and
William Fedus. 2022. Designing effective sparse expert models. arXiv:2202.08906 [cs.CL]
16

--- PAGE 17 ---
Table B.1: SM OE performance when initializing the gate or experts either randomly or perfectly.
Gate Experts% within 1%
Initialization Frozen? Initialization Frozen?
Random 7 Random 7 91.50:4
Random 7 Perfect 7 91.40:4
Random 7 Perfect X 80.90:2
Perfect 7 Random 7 96.00:3*
Perfect X Random 7 100.00
Perfect 7 Perfect 7 100.00
*Unlike other conÔ¨Ågurations, after reaching this performance, the model perfor-
mance degraded and converged to 91.60:3% within 1%.
A Training Details
Here we provide additional details on the training setups for experiments in ¬ß3.
A.1 Medium-Range Weather Prediction
We follow the training setup of Rasp and Thuerey [80]. We use a batch size of 64, the Adam [ 54]
optimizer with an initial learning rate of of 0:001, which we divide by a factor of 10if the validation
loss has not improved for two epochs to a minimum learning rate of 1e-6, and early stopping if the
validation loss does not improve for Ô¨Åve epochs.
Data consists of geoptential, temperature, the u-vcomponent of wind, speciÔ¨Åc humidity, top-of-
atmosphere incident solar radiation, 2-meter temperature, 6-hourly accumulated precipitation, a
land-sea mask, orography, and the latitude/longitude. Multi-level variables are given at seven vertical
levels (50, 250, 500, 600, 700, 800, and 925 hPa). We use three timesteps as input context, consisting
of(t 12; t 6; t 0)hours. Data are concatenated together channel-wise to form network input.
A.2 Post-Processing Ensemble Weather Forecasts
We follow the training setup of Ashkboos et al . [8]. All models were trained for ten epochs with
Adam [ 54] with a learning rate of 10 5and a batch size of 8for the U-Net and SM OE, and batch
size1for EMOS.
The ENS-10 data consists of temperature, geopotential, speciÔ¨Åc humidity, vertical velocity, divergence,
and the uandvwind components at eleven vertical levels (10, 50, 100, 200, 300, 400, 500, 700, 850,
925, and 1000 hPa); and the sea surface temperature, total column water, total column water vapor,
convective precipitation, mean sea level pressure, total cloud cover, 10 m uandvwind components,
2 mtemperature, total precipitation, and skin temperature at surface on a single surface level. The
ground truth data used is ERA5 data speciÔ¨Åed similarly to the ENS-10 reforecast data.
A.3 Image ClassiÔ¨Åcation
Our ResNet training recipe is adapted directly from that of Vryniotis [94]. We use a batch size of 128
per GPU (and a total batch size of 8192) and trained for 600 epochs using the SGD optimizer with
a learning rate of 0.5 (and a linear warmup over Ô¨Åve epochs) and momentum (factor 0.9), a cosine
annealing learning rate schedule, weight decay factor 2e-5, and elastic model averaging.
B Routing Challenges
We have empirically observed that, without using the routing classiÔ¨Åcation loss (¬ß2.2) and instead
training a gate and experts with the same loss, good gating functions are challenging to learn,
particularly for regression tasks. We hypothesize that this is due to a gradient ‚Äúmismatch‚Äù, where
gradients are informative for experts but not the gate. Here we Ô¨Årst present a simple ablation that
gives some evidence that this is the case, then discuss a possible explanation.
In Table B.1, we show results for training an SM OEon the heat diffusion task, with a setup identical
to that of ¬ß3.1, but without the RC loss or expert error damping, when initializing the gate and/or
experts to be either random or to a perfect, correct initialization. (Note that such an initialization is
17

--- PAGE 18 ---
Table C.1: Effect of different SM OE routing normalizations.
Routing Normalization % within 1%
Softmax 100.00
Abs. value 90.20:3
Softmax + Abs. value 100.00
None 100.00
Table C.2: Effect of aux. losses and noise on SM OEs
with the RC loss and expert error damping.
Importance Load*Spatial Agreement Noise % within 1%
7 7 7 7 100.00
X 7 7 7 100.00
X 7 7 X 95.70:2
X X 7 X 95.70:3
7 X 7 X 95.70:2
7 7 7 X 95.70:4
X 7 X 7 100.00
X 7 X X 95.60:5
X X X X 95.70:3
7 X X X 95.70:1
7 7 X X 95.70:2
*The load loss is only deÔ¨Åned when there is routing noise.Table C.3: Effect of aux. losses and noise on SM OEs
without the RC loss and expert error damping.
Importance Load*Spatial Agreement Noise % within 1%
7 7 7 7 91.50:4
X 7 7 7 90.80:6
X 7 7 X 74.00:2
X X 7 X 73.90:5
7 X 7 X 74.20:3
7 7 7 X 73.90:5
X 7 X 7 91.30:2
X 7 X X 74.10:2
X X X X 73.70:3
7 X X X 74.00:4
7 7 X X 73.90:3
known because of how the heat diffusion dataset is generated.) We can see that a random initialization
for both does not perform well, while a perfect initialization achieves perfect accuracy. However,
a perfect initialization of the experts alone does not result in good performance. Indeed, freezing
expert weights at their perfect initialization performs worse , as the gate is unable to learn to route
each location to the correct expert. Initializing the gate perfectly results in better performance, but
with continued training, the quality of the gate degrades; if the gate is instead frozen, the SM OE
easily converges to perfect accuracy. We thus see that the gating function appears to be harder to
learn and that when training end-to-end with a single loss, the gradients result in bad gate updates.
We hypothesize that this occurs because the gate is attempting to learn a classiÔ¨Åcation task (‚Äúwhich
expert(s) should be selected at a point?‚Äù), but the experts are learning a regression task with mean-
square error. In such a setting, the sign of a gradient is not informative about whether a routing
decision was correct, as there is no thresholding for the experts, but there is for the gate (due to the
top-Erouting). Hence, training an SM OE with a single loss is likely to lead to poor performance.
C Additional SM OE Ablation Studies
Here we present additional ablations on SM OEconÔ¨Ågurations. All results use the standard conÔ¨Ågura-
tion in ¬ß3 except where noted. We also tried gating functions other than tensor routing, but did not
see improved performance.
Routing normalization. We compare different approaches for normalizing the gating function (i.e.,
normalizing g(x), see ¬ß2.1) in Table C.1. We use no normalization, as that is both simplest and
performs the best.
Auxiliary losses. We compare different MoE auxiliary routing losses, plus using routing noise, in
Table C.2. We deÔ¨Åne these auxiliary losses precisely in ¬ßC.1. We can see that auxiliary losses
neither help nor hurt SM OEperformance; however, adding noise results in signiÔ¨Åcant performance
degradation. In Table C.3, we show SM OEtraining with different auxiliary losses and routing noise
when notusing the RC loss and expert error damping. Auxiliary losses in this setting offer no
improvement over baseline performance, showing they cannot replace the RC loss.
Expert function. We compare between using standard convolution and CoordConv [ 65] as the
SM OE experts in Table C.4.
Weighted SM OEs.We compare between weighted and unweighted SM OEs in Table C.5. Both
formulations perform the same, so we prefer the simpler, unweighted version; however, on other
tasks, weighted SM OEs may perform better.
18

--- PAGE 19 ---
Table C.4: Effect of different
SM OE expert functions.
Expert function % within 1%
Convolution 100.00
CoordConv [65] 96.50:3Table C.5: Weighted versus
unweighted SM OEs.
Weighted? % within 1%
X 100.00
7 100.00
C.1 Auxiliary Losses
Here we describe the auxiliary losses we explored. The importance and load losses follow the
formulation of Riquelme et al . [82] ; the spatial agreement loss is a novel auxiliary loss we explored.
Note that these losses typically aim to enforce a balanced usage of experts; while this may be
appropriate for other MoEs, it is notnecessarily useful for SM OEs, which may have experts that
specialize to rare location types.
C.1.1 Importance Loss
The importance loss encourages the gate to assign equal importance to each expert, where the
importance is the (normalized) routing weight for an expert eover a batch X:
Impe(X) =X
x2XG(x)e:
Then the importance loss is the square of the coefÔ¨Åcient of variation of the expert importances:
LImp(X) =std(Imp( X))
mean(Imp( X))2
:
C.1.2 Load Loss
The load loss attempts to further encourage balanced usage of experts. However, as this quantity is
not differentiable, we instead use the probability of an expert ebeing selected if only the routing
noise were resampled. Let Tbe the threshold above which experts were selected at a point (i.e., the
Eth maximum routing score) during the original forward pass. Then the probability of expert ebeing
above this threshold if we resample the original noise "to be"0is
pe(x) =P(G(x)e+"0T) =P("0T G(x)e):
We then deÔ¨Åne the load over a batch Xas
Load e(X) =X
x2Xpe(x)
and Ô¨Ånally the load loss as the squared coefÔ¨Åcient of variation:
LLoad(X) =std(Load( X))
mean(Load( X))2
:
C.1.3 Spatial Agreement Loss
We introduce the spatial agreement loss, which is conceptually similar to the above losses, to
encourage the gate to select the same experts at a given location across samples in a batch. This
loss,LSA, is the standard deviation of the routing weights for each expert at each point, averaged
across each point, and then summed over experts. We considered using the coefÔ¨Åcient of variation
rather than standard deviation for this, but found that the mean was often very close to zero, making
optimization challenging.
C.1.4 Final Auxiliary Loss
We weight each used auxiliary loss equally, then add them to the Ô¨Ånal network loss, scaled by 0:01,
as done in Riquelme et al . [82] . We did not Ô¨Ånd the Ô¨Ånal performance to be sensitive to the choice of
scaling factor.
D Additional Image ClassiÔ¨Åcation Results
We report image classiÔ¨Åcation results for SM OEs on two additional tasks, MNIST [ 58] (Table D.1)
and CIFAR10 [ 55] (Table D.2). We follow the experimental setup of Elsayed et al . [30] for these
19

--- PAGE 20 ---
Table D.1: MNIST results.
Model Accuracy %
Convolution 98.860:02
LCN 99.090:02
CoordConv [65] 99.400:03
Wide convolution 99.150:01
LRLCN [30] 99.490:01
SM OE 99.540:01Table D.2: CIFAR10 results.
Model Accuracy %
Convolution 78.830:07
LCN 72.030:10
CoordConv [65] 80.980:09
Wide convolution 82.430:13
LRLCN [30] 84.850:08
SM OE 85.050:07
datasets, and use a network of three convolution layers, 64 channels per layer, and 33Ô¨Ålters, with
batch normalization and ReLU activations. The Ô¨Ånal classiÔ¨Åcation layer consists of global average
pooling followed by a linear layer. Models used a mini-batch size of 512 and otherwise followed our
standard training setup.
For our SM OE, we replaced all convolution layers with SM OEs with shared gates and 128 experts
per layer (selecting 64). For the LRLCN, we replaced all layers with LRLCNs with spatial rank 4,
which we found to perform best. The LCN network consisted of two convolutions followed by a third
LCN layer, which performed best in Elsayed et al. [30].
20
