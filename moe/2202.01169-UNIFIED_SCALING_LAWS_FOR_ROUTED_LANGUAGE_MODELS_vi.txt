# 2202.01169.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2202.01169.pdf
# Kích thước tệp: 1715249 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
LUẬT TỶ LỆ THỐNG NHẤT CHO CÁC MÔ HÌNH NGÔN NGỮ ĐỊNH TUYẾN
Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch
Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtmanz, Trevor Cai, Sebastian Borgeaud,
George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnsonz, Katie Millican,
Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero,
Oriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu, Karen Simonyan
DeepMind Google Researchz

TÓM TẮT
Hiệu suất của một mô hình ngôn ngữ đã được chứng minh là có thể mô hình hóa hiệu quả như một luật lũy thừa theo số lượng tham số của nó. Ở đây chúng tôi nghiên cứu hành vi tỷ lệ của Mạng Định tuyến: các kiến trúc chỉ sử dụng có điều kiện một tập con tham số của chúng khi xử lý một đầu vào. Đối với những mô hình này, số lượng tham số và yêu cầu tính toán tạo thành hai trục độc lập mà việc tăng trên đó dẫn đến hiệu suất tốt hơn. Trong công trình này, chúng tôi suy ra và biện minh các luật tỷ lệ được xác định trên hai biến này nhằm tổng quát hóa những luật đã biết cho các mô hình ngôn ngữ tiêu chuẩn và mô tả hiệu suất của một loạt rộng các kiến trúc định tuyến được huấn luyện qua ba kỹ thuật khác nhau. Sau đó, chúng tôi cung cấp hai ứng dụng của những luật này: đầu tiên là suy ra Số Tham Số Hiệu quả mà tất cả các mô hình tỷ lệ với cùng một tốc độ, và sau đó sử dụng các hệ số tỷ lệ để đưa ra so sánh định lượng về ba kỹ thuật định tuyến được xem xét. Phân tích của chúng tôi được suy ra từ một đánh giá mở rộng về Mạng Định tuyến trên năm bậc độ lớn về kích thước, bao gồm các mô hình với hàng trăm chuyên gia và hàng trăm tỷ tham số.

1 Giới thiệu
Một niềm tin phổ biến là tăng kích thước của mạng nơ-ron dẫn đến hiệu suất tốt hơn, đặc biệt là khi huấn luyện trên các tập dữ liệu lớn và đa dạng trong thế giới thực. Khái niệm mơ hồ và gây tranh cãi này đã ngày càng được chứng minh khi các nghiên cứu thực nghiệm lớn cho thấy hiệu suất của các mô hình trên nhiều lớp bài toán thú vị được hiểu rõ như các luật lũy thừa; trong đó việc tăng kích thước mô hình nhân tính dẫn đến giảm tổn thất cộng tính của mô hình [Kaplan et al., 2020, Hernandez et al., 2021, Henighan et al., 2020, Rosenfeld et al., 2019]. Những mối quan hệ này không được hiểu rõ, nhưng một hàm ý chính là một chuỗi các mô hình nhỏ có thể được sử dụng để suy ra hiệu suất của các mô hình mạnh hơn nhiều lần, đồng thời cung cấp thông tin toàn cục về khả năng mở rộng của một kiến trúc.

Giới thiệu Mạng Định tuyến: các mô hình có tính chất bất thường là mỗi đầu vào chỉ tương tác với một tập con tham số của mạng — được chọn độc lập cho mỗi điểm dữ liệu [Bengio et al., 2016, 2013, Denoyer and Gallinari, 2014]. Đối với Mạng Định tuyến, số lượng tham số gần như độc lập với chi phí tính toán để xử lý một điểm dữ liệu. Điều này phân tách định nghĩa về kích thước và ngăn cản luật tỷ lệ chỉ theo tham số mô tả đầy đủ lớp mô hình. Các Mạng Định tuyến cụ thể đã được huấn luyện thành công ở quy mô lớn [Fedus et al., 2021, Du et al., 2021, Artetxe et al., 2021], nhưng hành vi tỷ lệ tổng quát không được hiểu rõ. Trong công trình này, chúng tôi phân tích hành vi của các mô hình ngôn ngữ định tuyến để có thể suy ra các luật tỷ lệ mô tả hiệu suất của chúng.

Liên hệ với aidan.b.clark@gmail.com, diegolascasas@deepmind.com. Tất cả thuộc DeepMind trừ khi có ghi chú.
*Tác giả đầu tiên chung.
¹Đo bằng hoạt động điểm nổi huấn luyện hoặc suy luận, thiết bị hoặc thời gian cần thiết, chi phí tài chính, lượng khí thải carbon, v.v.arXiv:2202.01169v2 [cs.CL] 9 Feb 2022

--- TRANG 2 ---
Luật Tỷ lệ Thống nhất cho Mô hình Ngôn ngữ Định tuyến

[Biểu đồ hiển thị dự đoán tổn thất khi thay đổi số lượng chuyên gia, đường cong giá trị tổn thất không đổi, và tỷ lệ mô hình thống nhất]

Hình 1: (a) Hiệu suất đạt được bởi Mạng Định tuyến khi thay đổi số lượng chuyên gia cho kích thước mô hình dày đặc cố định được mô tả bởi hàm song tuyến tính (Eq. 1), (b) có các đường mức chỉ ra cách đánh đổi kích thước mô hình với số lượng chuyên gia để duy trì hiệu suất cố định, (c) và có thể được thao tác để căn chỉnh hiệu suất mô hình dày đặc và định tuyến dưới luật lũy thừa chung.

Đóng góp chính. Chúng tôi phân tích ba kỹ thuật khác nhau để huấn luyện Mạng Định tuyến, được mô tả chi tiết trong §3: Sinkhorn-BASE, phương pháp mixture-of-experts thưa thớt (SMOE) cải tiến BASE [Lewis et al., 2021]; Hash Layers không tham số [Roller et al., 2021]; và định tuyến qua Reinforcement Learning (RL-R). Với các mô hình lên đến 200 tỷ tham số, chúng tôi quan sát những điều sau:

1. Định tuyến cải thiện hiệu suất của các mô hình ngôn ngữ trên tất cả kích thước và biến thể được thử nghiệm (xem Hình 1).
2. Huấn luyện Mạng Định tuyến với RL (§3.3), một kỹ thuật được sử dụng trong các công trình định tuyến ban đầu [Bengio et al., 2013], có hiệu quả tương đương với các kỹ thuật tiên tiến.
3. Hiệu suất của tất cả Mạng Định tuyến được mô tả chính xác bởi các luật tỷ lệ theo số lượng chuyên gia và kích thước mô hình dày đặc cơ bản (§4) nhằm tổng quát hóa những luật từ Kaplan et al. [2020].
4. Những luật này có thể được phát biểu lại theo số lượng tham số và tính toán suy luận, nắm bắt một tập hợp rộng hơn các kiến trúc định tuyến dưới một phù hợp chung (§4.4).
5. Chúng còn hàm ý Số Tham Số Hiệu quả: một ánh xạ cân bằng hiệu suất và tỷ lệ cho cả mạng dày đặc và định tuyến (§5).

2 Bối cảnh
Trước tiên chúng tôi xem xét bài toán mô hình hóa ngôn ngữ và các luật tỷ lệ hiện có trước khi thảo luận về quá trình định tuyến mạng nơ-ron và cách áp dụng vào các mô hình ngôn ngữ.

Mô hình hóa ngôn ngữ. Chúng tôi xem xét bài toán dự đoán ngôn ngữ tự nhiên một cách tự hồi quy, một nhiệm vụ có đặc điểm tỷ lệ nhất quán và có thể dự đoán được trên nhiều bậc độ lớn [Henighan et al., 2020, Kaplan et al., 2020]. Mục tiêu là tối đa hóa khả năng xảy ra của một chuỗi token P(x₁;...;xₜ) được phân tích tự hồi quy như p(x₁;...;xₜ) = ∏ᵢₜ p(xᵢ|xⱼ<ᵢ). Số liệu hiệu suất chính của chúng tôi là log-likelihood âm của tập dữ liệu xác thực có thống kê khớp với phân phối huấn luyện. Chúng tôi tập trung vào tổn thất xác thực này, nhưng xem xét ngắn gọn việc chuyển giao zero-shot sang các nhiệm vụ khác trong Phụ lục E.

Luật tỷ lệ cho dữ liệu quy mô lớn. Chúng tôi huấn luyện trên tập hợp văn bản tiếng Anh đa nghìn tỷ token bao gồm tài liệu từ internet cùng với các tập dữ liệu văn bản mã nguồn mở, chi tiết được đưa ra trong Rae et al. [2021]. Trong bối cảnh này, Kaplan et al. [2020] lập luận rằng hiệu suất hội tụ của mô hình được huấn luyện trên tập dữ liệu có kích thước vô hạn là luật lũy thừa theo số lượng tham số N của mô hình. Tập dữ liệu của chúng tôi không vô hạn, nhưng kích thước của nó – và việc thiếu bất kỳ hiện tượng overfitting nào được quan sát – khiến điều này trở thành xấp xỉ hợp lý. Chúng tôi xem xét giá trị đánh giá cuối cùng (và tốt nhất) làm giá trị hội tụ, mặc dù điều này cũng là một xấp xỉ được thảo luận thêm trong Phụ lục F.

2.1 Mạng Định tuyến
Tỷ lệ luật lũy thừa hàm ý hiệu suất của mô hình ngôn ngữ tăng theo kích thước, nhưng chi phí tính toán để huấn luyện mô hình cũng tăng theo. Kết nối không mong muốn giữa kích thước và tính toán này thúc đẩy tìm kiếm các kiến trúc trong đó hai yếu tố này được tách rời. Mạng Định tuyến là một lớp mô hình như vậy: một loại mạng nơ-ron kết hợp một hương vị cụ thể của tính toán có điều kiện. Trong Mạng Định tuyến, mỗi đầu vào (ví dụ, một token văn bản) được chuyển đổi thành đầu ra trong khi chỉ tương tác với một tập con cố định của tham số mạng – được lựa chọn động dựa trên chính đầu vào đó. Nhiều mạng kích hoạt thưa thớt có tính chất này, nhưng ở đây chúng tôi độc quyền nghiên cứu bố cục dựa trên Sparse Mixtures of Experts [Shazeer et al., 2017] trong đó nhiều thành phần con của mạng nơ-ron sâu (tức là, một số lớp) được chuyển đổi độc lập thành các tương đương định tuyến và được huấn luyện chung với phần còn lại của mạng.

Định tuyến một lớp đơn. Ý tưởng cốt lõi của lớp định tuyến là giữ nhiều phiên bản của tham số, và quyết định cho mỗi đầu vào sử dụng phiên bản nào. Để định tuyến một lớp f theo E cách, chúng ta bắt đầu bằng cách tạo E phiên bản riêng biệt của tham số (f₁;...fₑ) trong đó việc sử dụng phiên bản thứ i của tham số (fᵢ) được gọi là Chuyên gia thứ i. Để xác định chuyên gia nào chọn với đầu vào cho trước, chúng ta giới thiệu hàm định tuyến bổ sung: R: ℝᴹ → [1,E] liên kết với lớp, thường là một mạng nhỏ, với tham số φ. Dạng định tuyến h của f sau đó được cho bởi h(x) ≜ f_{R(x)}(x). Khi hiệu suất tăng với E, định tuyến cung cấp phương pháp để cải thiện mạng nơ-ron với tăng tính toán tối thiểu (chỉ tương ứng với tính toán cần thiết bởi R(x)).

Chúng tôi cũng xem xét tổng quát hóa định tuyến K-way, trong đó bộ định tuyến xuất ra một tập hợp số nguyên như R(·): ℝᴹ → [1,E]ᴷ, và chúng ta đặt đầu ra của lớp là tổng đầu ra của mỗi chuyên gia, cụ thể h(x) ≜ ∑ᵢ∈R(x) fᵢ(x). Chúng tôi mặc định K = 1, nhưng xem xét lại điều này trong §4.4.

Transformer Định tuyến. Chúng tôi áp dụng định tuyến cho Transformer chỉ giải mã [Vaswani et al., 2017] để đo các tính chất tỷ lệ kết quả: một kiến trúc được chọn do hiệu suất tiên tiến. Chi tiết kiến trúc baseline mà chúng tôi sử dụng được nêu trong Phụ lục A. Chúng tôi sẽ gọi các Transformer không định tuyến là các mô hình dày đặc, trái ngược với Transformer Định tuyến kích hoạt thưa thớt một số tham số của chúng. Việc chuyển đổi thành Transformer Định tuyến của chúng tôi giống như được sử dụng trong các công trình trước [Lepikhin et al., 2020, Fedus et al., 2021]. Cụ thể, chúng tôi áp dụng định tuyến cho mỗi tập thành phần feedforward khác (FFW) của Transformer, các thành phần con hoạt động trên từng timestep độc lập. Mặc dù các lớp khác nhau có thể có số lượng chuyên gia khác nhau, ở đây tất cả các lớp định tuyến chia sẻ cùng số lượng chuyên gia E, và chúng tôi sẽ gọi mạng là định tuyến E-way.

Kích thước mô hình và chi phí suy luận. Chúng tôi sử dụng N để biểu thị kích thước mô hình dày đặc của mạng: số lượng tham số mà bất kỳ đầu vào nào tương tác với. Điều này trái ngược với P: tổng số tham số. Đối với mô hình dày đặc, P = N, trong khi đối với Mạng Định tuyến P tương tỷ lệ với N·E, với các hệ số phụ thuộc vào chi tiết kiến trúc định tuyến (§4.4). Ngoại trừ overhead nhỏ do chạy các bộ định tuyến, chi phí F (theo TeraFLOPs) thực thi Transformer Định tuyến giống như tương đương dày đặc của nó.

Chi tiết Huấn luyện. Tất cả mô hình được huấn luyện trên TPU với JAX [Bradbury et al., 2018] sử dụng kết hợp song song dữ liệu, chuyên gia (xem Phụ lục C) và sharding [Shoeybi et al., 2019]. Các mô hình được huấn luyện với độ dài chuỗi 2048 và kích thước batch 256 trong 250.000 bước, tức 130 tỷ token, bất kể N. Đây là chi tiết quan trọng, và chúng tôi thảo luận một số hàm ý trong Phụ lục F. Tất cả được tối ưu hóa với AdamW [Loshchilov and Hutter, 2018] và ZeRO Stage 1 được sử dụng để chia sẻ trạng thái optimizer [Rajbhandari et al., 2020]. Phụ lục A chứa chi tiết thêm.

3 Kỹ thuật Định tuyến
Nếu lợi ích của Mạng Định tuyến là tách rời khả năng tham số khỏi chi phí mạng, thì khó khăn cơ bản là học hiệu quả tham số φ của bộ định tuyến với tính không khả vi của đầu ra của nó. Do đó, nhiều nghiên cứu trong Mạng Định tuyến tập trung vào các kỹ thuật để học φ. Một phát hiện chính của công trình này là ba kỹ thuật đáng chú ý khác nhau để huấn luyện Mạng Định tuyến được mô tả hiệu quả bởi cùng các luật tỷ lệ. Bây giờ chúng tôi giới thiệu và bối cảnh hóa ba phương pháp này.

3.1 Sparse Mixture-of-Experts thông qua Weighting
Các phương pháp Sparse Mixture-of-Experts (SMOE) [Shazeer et al., 2017] giải quyết vấn đề không khả vi bằng cách sử dụng lại xác suất lựa chọn chuyên gia làm nhân tỷ lệ vô hướng trên đầu ra của chuyên gia đó, đảm bảo gradient được truyền cho các logit của chuyên gia được chọn dù có tính không khả vi của việc lấy mẫu từ những logit đó. Chính thức, bộ định tuyến được cho như R(x) = topk(Wx + b), trong đó Wx + b là phân phối không chuẩn hóa trên [1,E] từ đó các chuyên gia tương ứng với K giá trị cao nhất được chọn. Trong đầu ra cuối cùng của lớp định tuyến, các logit chuẩn hóa được sử dụng lại làm trọng số gating, tức đầu ra cuối cùng của lớp định tuyến là h(x) = ∑ᵢ∈R(x) gᵢ(x)fᵢ(x) trong đó g(x) = softmax(Wx + b).

Mặc dù công thức này cung cấp gradient cho φ = (W,b), nó đại diện cho thay đổi trong nhân tỷ lệ vô hướng và không trực tiếp tương ứng với tối ưu hóa lựa chọn chuyên gia. Phương pháp này vẫn hiệu quả, và có thể được xem như xấp xỉ thưa thớt cho các mô hình mixture of experts dày đặc [Eigen et al., 2014, Jacobs et al., 1991] trong đó khả năng bỏ qua chuyên gia tỷ lệ nghịch với giá trị gate vô hướng gᵢ của nó.

Đã được phỏng đoán rằng SMOE yêu cầu định tuyến (K>2)-way để tạo gradient hiệu quả trong các bộ định tuyến [Shazeer et al., 2017], và nhiều nỗ lực kết hợp định tuyến vào Transformer lớn sử dụng K = 2 [Lepikhin et al., 2020, Du et al., 2021]. Tuy nhiên gần đây điều này đã bị thách thức, và các cải tiến ổn định đã được đề xuất cho K = 1; cụ thể là Switch Transformer [Fedus et al., 2021]. Hầu hết SMOE, bao gồm Switch, đều dựa vào các tổn thất cân bằng phụ trợ khuyến khích đầu ra bộ định tuyến R(x) trở nên đồng đều hơn trên các minibatch đầu vào. Để cải thiện điều này, BASE [Lewis et al., 2021] hậu xử lý đầu ra bộ định tuyến với thuật toán Hungarian Matching tái gán các lựa chọn chuyên gia để đảm bảo tất cả chuyên gia được chọn đều đặn.

Triển khai BASE của chúng tôi thay thế Hungarian Matching bằng công thức Optimal Transport chính quy [Cuturi, 2013] sử dụng thuật toán Sinkhorn như bước matching xấp xỉ trong quá trình lựa chọn chuyên gia. Điều này cải thiện đáng kể hiệu quả định tuyến trên phần cứng tăng tốc (chi tiết trong §B.2.1). Chúng tôi gọi phương pháp kết quả là Sinkhorn-BASE (S-BASE), và sử dụng nó làm đại diện cho các phương pháp SMOE, vì các thử nghiệm sớm cho thấy lợi ích của cơ chế cân bằng của nó.

3.2 Định tuyến Hash Xác định dựa trên Đầu vào
Một phương pháp thay thế hoàn toàn tránh tham số bổ sung và biểu diễn R như một hàm cố định của đầu vào. Đây là khái niệm được khởi xướng bởi Hash Layers [Roller et al., 2021] mà tránh được nhu cầu học đồng thời φ và R. Triển khai của chúng tôi lấy ID token được gán cho đầu vào bởi tokenizer SentencePiece [Kudo and Richardson, 2018] và sử dụng phần dư của nó chia cho E làm lựa chọn chuyên gia. Xem §B.4 để biết chi tiết.

3.3 Định tuyến qua Reinforcement Learning
Cuối cùng, chúng tôi tái phân tích một kỹ thuật tối ưu hóa bộ định tuyến qua Reinforcement Learning (một lớp phương pháp chúng tôi gọi là RL-R), được đề xuất trong công trình sớm về tính toán có điều kiện nơ-ron [Bengio et al., 2013, 2016, Bengio, 2017, Denoyer and Gallinari, 2014]. Trong phương pháp này, mỗi bộ định tuyến được xem như một chính sách có hành động là lựa chọn chuyên gia trong mỗi lớp định tuyến và quan sát là các kích hoạt được truyền cho bộ định tuyến đó. Sau khi hoàn thành forward pass, xác suất mà Transformer Định tuyến gán cho token đầu ra đúng có thể được sử dụng như phần thưởng, việc tối đa hóa nó tương đương với tối thiểu hóa NLL. Để huấn luyện chung các chuyên gia và bộ định tuyến, chúng tôi tối thiểu hóa tổn thất tổng hợp được hình thành với tổn thất mô hình hóa ngôn ngữ và một thuật ngữ policy-gradient [Sutton et al., 2000] sử dụng tập chuyên gia được chọn làm hành động. Chúng tôi nhấn mạnh rằng lựa chọn chuyên gia tối ưu phụ thuộc không chỉ vào kích hoạt đầu vào mà còn vào tham số của phần còn lại của mạng. Điều này phá vỡ nền tảng lý thuyết, quan trọng đối với RL, rằng đây là Quá trình Quyết định Markov. Tuy nhiên, đã được quan sát thấy rằng vấn đề lý thuyết này không ảnh hưởng đến tính thực tế của phương pháp [Rosenbaum et al., 2019].

So với SMOE, RL-R được hưởng lợi từ việc tối ưu hóa trực tiếp các hành động để cải thiện tổn thất mô hình hóa ngôn ngữ. Tuy nhiên, việc không có bias này đi kèm với các phức tạp, đặc biệt là phương sai cao của gradient [Rosenbaum et al., 2019, Denoyer and Gallinari, 2014]. Chúng tôi sử dụng REINFORCE với baseline đã học [Williams, 1992, Sutton and Barto, 2018] để giải quyết vấn đề này, để cải thiện chính sách có nghĩa là tăng khả năng lựa chọn chuyên gia dẫn đến dự đoán token tiếp theo tốt hơn trung bình. Giống như SMOE, chúng tôi thấy hữu ích khi thêm thuật ngữ cân bằng. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên thử nghiệm định tuyến với Reinforcement Learning trên các mô hình ngôn ngữ dựa trên Transformer lớn — do đó chúng tôi khám phá các ablation chính trong Phụ lục B.3.

--- TRANG 5 ---
Luật Tỷ lệ Thống nhất cho Mô hình Ngôn ngữ Định tuyến

Bảng 1: Phù hợp RMSLE Leave-One-Out trong (N,E). Hàng cuối được tính riêng cho mỗi kích thước mô hình; điều này đưa ra giới hạn dưới của lỗi của bất kỳ luật tỷ lệ chung nào.

Bảng 2: Giá trị tỷ lệ dày đặc (xem thêm Phụ lục F).

4 Hành vi Tỷ lệ tại Hội tụ
Giả thuyết chính của chúng tôi là log-loss hội tụ của Mạng Định tuyến có tính song tuyến trong các thuật ngữ log N và log Ê, trong đó Ê là biến đổi bão hòa của E. Cụ thể, chúng tôi khớp luật tỷ lệ 6 tham số:

log L(N,E) ≈ a log N + b log Ê + c log N log Ê + d                (1)

trong đó 1/Ê ≜ 1/E - 1 + 1/E_start - 1/E_max - 1 + 1/E_max.

Chúng tôi có thể tổng quát hóa luật này trên một phạm vi rộng hơn các kiến trúc định tuyến bằng thay đổi biến, sử dụng chi phí suy luận mô hình F và tổng số tham số P, như:

log L(F,B) ≈ a log F + b log B̂ + c log F log B̂ + d;                (2)

trong đó B ≜ P/F và B → B̂ là cùng biến đổi bão hòa như E → Ê. Trước khi biện minh Phương trình (1), chúng tôi xác thực tính ứng cử của nó bằng cách khớp nó với dữ liệu thực nghiệm thu được trên một sweep lớn các mô hình. Sweep này bao gồm một Mạng Định tuyến được huấn luyện cho mỗi trong ba kỹ thuật được mô tả trong §3: trên sáu kích thước mô hình (được mô tả trong Bảng 4) trong khi thay đổi E trên [2,4,8,16,32,64,128,256,512]. Điều này tổng cộng 168 mô hình khác nhau, bao gồm cả baseline dày đặc.

Các tổn thất quan sát được cho mỗi mô hình được hiển thị trong Hình 2(a-c). Chúng tôi khớp Eq. (1) cho mỗi phương pháp định tuyến và vẽ dự đoán cho giá trị N cố định như các đường đứt nét. Độ phù hợp trên tất cả phương pháp là rõ ràng, cũng như hành vi rõ ràng rằng tăng E dẫn đến giảm tổn thất validation. Hình 2(d) vẽ dự đoán tương đối cho cả ba kỹ thuật, rõ ràng cho thấy S-BASE hoạt động tốt nhất trên tất cả kích thước mô hình, tiếp theo là RL-R, tiếp theo là HASH (xem §5.3). Phần còn lại của mục này biện minh các dạng hàm được chọn (1) và (2); đầu tiên giả sử các luật lũy thừa độc lập trong N và E (§4.1), sau đó giới thiệu tương tác nhân (§4.2) và bão hòa trong thuật ngữ thứ hai (§4.3), tiếp theo là thay đổi biến (§4.4). Lợi ích đạt được bởi sự tiến triển khớp này có thể được thấy trong Bảng 1. Các ký hiệu được nhắc lại trong Hình 3.

4.1 Luật Tỷ lệ Tách rời trong Kích thước Mô hình và Chuyên gia
Kaplan et al. [2020] lập luận rằng hiệu suất hội tụ của mô hình dày đặc với N tham số có thể mô hình hóa chính xác như luật lũy thừa hai tham số

log L(N) ≈ a log N + d; tức L(N) = N_c^N / N^a                    (3)

trong đó α_N ≜ a và N_c ≜ 10^{d/a}. Chúng tôi có thể tái ước lượng những hệ số này từ hiệu suất của các mô hình dày đặc của chúng tôi, dẫn đến ước lượng trong Bảng 2. Sự tương tự của α_N là kiểm tra độ tin cậy yên tâm (có sự khác biệt trong tập dữ liệu, từ vựng, tokenization và mô hình ảnh hưởng N_c).

Một giả thuyết ngay lập tức là đối với tất cả giá trị N, tỷ lệ trong E tuân theo luật lũy thừa tương tự:

log L_N(E) ≈ b log E + d'                                        (4)

Vì L_N(1) = L(N) (một thực tế chúng tôi sẽ gọi là tương đương dày đặc), (3) và (4) có thể kết hợp thành:

log L_N(E) ≈ a log N + b log E + d;                             (5)

tương ứng với luật lũy thừa tách rời nhân:

L_N(E) = 10^{d/a} / (N^a · E^b)                                 (6)

Nếu Eq. (4) khớp dữ liệu quan sát cho bất kỳ N nào, chúng ta có thể tiến hành với giả định rằng tỷ lệ trong E tuân theo luật lũy thừa cho N cố định. Quan sát b không đổi trên N sẽ cho phép khớp Eq. (5) với các mô hình trải dài trên N và E đồng thời.

Khớp. Giả thuyết đầu tiên dễ dàng được kiểm tra và xác nhận ở mức độ hợp lý. Chúng tôi khớp Eq. (4) cho mỗi kỹ thuật và giá trị N riêng biệt, được vẽ như các đường có màu trong Hình 4. Các giá trị b được hiển thị trong Hình 3.

Chúng tôi quan sát rằng b(N) đang tăng với N (các giá trị được liệt kê trong Bảng 6), tương ứng với giảm lợi ích từ định tuyến khi kích thước tăng, với độ dốc xấp xỉ tuyến tính trong log N (Hình 3). Eq. (5) yêu cầu b duy trì cố định trên N; do đó chúng tôi mong đợi nó dự đoán hiệu suất mô hình kém. Chúng tôi vẫn có thể thử khớp: được vẽ màu xám trong Hình 4. Về mặt định tính, điều này dự đoán sai một số tổn thất validation hơn 0.2, đặc biệt là đánh giá quá cao hiệu suất ở N và E lớn. Như báo cáo trong Bảng 1, khớp có giá trị RMSLE held-out lớn hơn 80e-4.

4.2 Tương tác Bậc hai trong N và E
Điều này thúc đẩy chúng tôi giới thiệu một phần mở rộng đơn giản: tương tác nhân giữa log N và log E. Điều này thuận tiện chính xác là hàm dẫn đến b tỷ lệ với log N và có dạng sau:

log L(N,E) ≈ a log N + b log E + c log N log E + d                (7)

Hàm này có tính chất rằng độ dốc log-log trong cả N và E là affine trong logarithm của biến khác. Nói cách khác, với E hoặc N cố định, hiệu suất L tỷ lệ với N hoặc E theo (3) và (4) với độ dốc được cho bởi:

a(E) ≜ ∂log L/∂log N = a + c log(E)                            (8)
b(N) ≜ ∂log L/∂log E = b + c log(N);

b(N) khớp hành vi báo cáo trong Bảng 6. Một bảng chuyển vị, khớp các tập mô hình với E cố định và N thay đổi, có thể được tìm thấy để khớp hành vi được dự đoán bởi a(E) (xem Bảng 8). Có hai biểu diễn không logarithmic đối xứng của (7), hữu ích để so sánh với (6):

L(N,E) = 10^{d/a} / (N^a · E^{b+c log(N)})                      (9a)
       = 10^{d/b} / (E^b · N^{a+c log(E)})                      (9b)

Khớp. Khớp song tuyến (7) thay vì (5) giảm đáng kể lỗi dự đoán cho N lớn (Bảng 1, Eq. (5) so với Eq. (7)), như được hiển thị trong Hình 4 (các đường đứt nét khớp với những đường đứt nét, trong khi khớp tách rời màu xám thì không). Chúng tôi xác minh tương đương dày đặc: α_N ≈ a, trong khi N_c ≈ exp(d/a), và do đó luật (7) đưa ra dự đoán tương tự với luật tham chiếu (3) cho các mô hình dày đặc. Dự đoán cho N cố định được hiển thị như các đường màu xám trong Hình 2.

Giải thích. Trong Eq. (7), khi c dương, độ dốc cải thiện chuyên gia b(N) giảm với kích thước mô hình N. Do đó, cả ba kỹ thuật định tuyến được xem xét đều dự đoán cải thiện giảm dần từ định tuyến khi tăng quy mô. Tuy nhiên, tỷ lệ của S-BASE được dự đoán (và thấy) tốt đáng kể hơn. Khi thiết kế kỹ thuật mới, chúng ta có thể khớp (7) và dự đoán hành vi tỷ lệ tốt hơn nếu c được khớp thấp hơn các kỹ thuật khác. Một mục tiêu rõ ràng cho công việc tương lai trong kỹ thuật định tuyến nên là tìm phương pháp với hệ số tỷ lệ c ≤ 0.

4.3 Tỷ lệ Bị chặn trong E
Phương trình (5) mô hình tỷ lệ trong E như luật lũy thừa. Đối với cả giá trị E nhỏ và lớn, có lý do để mong đợi một số độ lệch. Nếu kỹ thuật định tuyến suy giảm với E (ví dụ, phương sai của gradient trong RL-R sẽ tăng), hiệu suất cho E lớn có thể tệ hơn dự đoán. Mặt khác, overhead cố định (ví dụ, nhiễu từ tổn thất phụ trợ) có thể làm tệ tỷ lệ cho giá trị E thấp, ngược lại dẫn đến hiệu suất tốt hơn mong đợi. Cả hai hiện tượng đều xuất hiện rõ ràng trong Hình 2. Chúng tôi tìm cách mô hình hóa sự bão hòa này sao cho hành vi giới hạn trong E bị chặn ở cả hai bên. Chúng tôi chọn biến đổi sau, nhưng thảo luận trong §5.1 một số hàm ý độc lập với dạng bão hòa cụ thể được sử dụng:

1/Ê ≜ 1/E - 1/E_min + 1/E_start - 1/E_max - 1 + 1/E_max        (10)

Điều này được xây dựng sao cho chúng ta có Ê(E_min) = E_start, trong khi Ê → E_max khi E → ∞. Chúng tôi cố định E_min = 1, biểu thị giới hạn dưới của số lượng chuyên gia có ý nghĩa. Ê có thể được xem như phiên bản ngưỡng của E: tăng qua E_max sẽ cải thiện, nhưng không theo luật lũy thừa. Tương tự, khi E_start > 1, Ê > E cho giá trị E nhỏ. Thực tế, khớp giống nhau trên một phạm vi rộng các hàm ngưỡng khác nhau.

Khớp. Giải Phương trình (1), bằng Eq. (7) với E → Ê, phức tạp do tính không lồi của nó. Chúng tôi tìm các hệ số (a,b,c,d,E_start,E_max) như tốt nhất của các nghiệm lặp lại được cung cấp bởi thuật toán L-BFGS-B [Byrd et al., 1995]. Hình 2 cho thấy các đường cong khớp từ những phương trình này; hệ số được báo cáo trong Bảng 3.

Giải thích. So với việc sử dụng luật song tuyến đơn giản (7), khớp Eq. (1) cải thiện dự đoán cho giá trị E thấp nhất và cao nhất được xem xét. Quan trọng là, trong khi độ lệch từ luật lũy thừa (và do đó cải thiện trong RMSLE) tương đối nhỏ cho các giá trị E được xem xét, độ lệch vẫn rõ ràng (được thấy tốt nhất khi nhìn vào tổn thất thô trong Hình 21). Chúng tôi tin rằng việc mô hình hóa sự bão hòa này quan trọng vì (như được lập luận trong §5.2) hành vi giới hạn của hiệu suất mô hình khi N tăng khác biệt đáng kể khi bị chặn, với các tính chất quan trọng độc lập với E_max. Chúng tôi tiếp tục giả thuyết rằng công việc tương lai, có thể kiểm tra giá trị E lớn hơn nữa, sẽ thấy lợi ích định lượng hơn từ việc bao gồm những thuật ngữ này. Điều này đã có thể quan sát được trong Hình 20 khi lưu ý rằng luật (7) không đánh giá quá và dưới hiệu suất cho E = {2,4,256,512} như trong Hình 4. Các đường cong mức của Eq. (1) liệt kê {(N,E)} được dự đoán đạt hiệu suất cố định, như được hình dung trong Hình 1(b). Điều này chứng minh sức mạnh của định tuyến: một mô hình với N = 5M và E = 128 bằng hiệu suất của mô hình với N = 55M và E = 1, cái mà yêu cầu hơn mười lần tính toán trên mỗi suy luận.

4.4 Tổng quát hóa trên Các Biến thể Kiến trúc
Các mô hình được huấn luyện cho đến nay sử dụng lựa chọn cố định cho hai chi tiết chính của định tuyến: số lượng chuyên gia được thực thi trên mỗi điểm dữ liệu K và tần suất các lớp định tuyến trên độ sâu R (trước đây được đặt ở 1 và 0.5, tương ứng). Đối với bất kỳ giá trị được chọn nào của K và R, chúng ta có thể khớp Eq. (1) với hiệu suất quan sát, nhưng vì những biến này độc lập với N và E, chúng tôi không mong đợi cùng hệ số duy trì hiệu lực trên các giá trị của K và R. Để cho phép luật tỷ lệ thống nhất, chúng tôi sửa đổi Eq. (1) để sử dụng các thuật ngữ trong F, TeraFLOP yêu cầu trên mỗi forward pass, và trong tỷ lệ B ≜ P/F trong đó P là tổng số tham số. Cụ thể, F được thúc đẩy bởi xấp xỉ từ Kaplan et al. [2020] rằng F = 2N. B, tỷ lệ sử dụng tham số, là hàm affine của E, gần tuyến tính khi hầu hết tham số nằm trong các thành phần định tuyến của mô hình.

Sử dụng (F,B) thay vì (N,E) (và đặt E_min thành 1/2) dẫn đến Eq. (2). Để thể hiện lợi thế của thay đổi biến này, chúng tôi tiến hành hai thí nghiệm: thay đổi K trên {1,2,4} và R trên {0.25,0.5,1.0}. Trong cả hai trường hợp, chúng tôi thay đổi E ∈ {8,64,256} và N ∈ {15M,370M,870M}.

Khớp. Eq. (2) dự đoán hành vi tỷ lệ của các mô hình cũng như Eq. (1) cho kiến trúc định tuyến nhất định, như được chỉ ra trong Hình 24. Lợi ích của thay đổi biến được thấy rõ ràng nhất trong Hình 5, vẽ đường đồng mức của giá trị tổn thất cố định như hàm của (N,E) và của (F,B). Đối với (K,R) thay đổi, bề mặt tổn thất như hàm của N và E thay đổi: có nghĩa là khớp chung sẽ không chính xác. Được vẽ như hàm của (F,B), bề mặt tổn thất gần như giống nhau, gợi ý khớp chung giữa cả ba phương pháp (xem Hình 25 và Hình 26 cho khớp chung cho K và R tương ứng). Chúng tôi nhấn mạnh rằng R = 0.25 lệch hơi. Các giải thích hợp lý được thảo luận trong §D.4. Khả năng sử dụng khớp chung chỉ ra một điểm chính: các chi tiết kiến trúc K và R ít ảnh hưởng đến hành vi tỷ lệ của Mạng Định tuyến. Do đó, tổn thất của mạng có thể được dự đoán chỉ dựa trên flop suy luận F và tổng số tham số P.

5 Ứng dụng Luật Tỷ lệ
Tiếp theo, chúng tôi cung cấp hai ứng dụng của các luật tỷ lệ được trình bày. Chúng tôi nhấn mạnh lại rằng tất cả giá trị chỉ hợp lệ ở số lượng token cụ thể mà tất cả mô hình được huấn luyện: 130B. Phụ lục F cung cấp bằng chứng rằng phân tích của chúng tôi, nếu không phải giá trị số, vẫn mạnh mẽ với số lượng token.

5.1 Tương đương Tham số Hiệu quả
Chúng tôi tận dụng Eq. (1) để tính kích thước N của mô hình dày đặc cho cùng hiệu suất như Mạng Định tuyến. Cụ thể, chúng tôi giải L(N,1) = L(N,E), đưa ra

N* ≜ N·(Ê/E_start)^{b/α(E_start)} (11)

Ở đây α(E) ≜ a + c log E. Với mô hình có N và E, chúng tôi gọi N* là Số Tham số Hiệu quả (hoặc EPC) của mô hình đó. Eq. (1) dự đoán rằng hiệu suất của tất cả mô hình tăng như luật lũy thừa trong biến này

log L(N,E) = a log N*(N,E) + d. (12)

Kết quả vẽ tất cả mô hình như hàm của N* được hiển thị trong Hình 1(c): một khớp tốt trên bốn bậc độ lớn. Tỷ lệ theo N* dẫn đến luật lũy thừa thống nhất: hợp lệ cho cả mô hình ngôn ngữ dày đặc và định tuyến.

5.2 Hành vi Định tuyến cho N Lớn
EPC dẫn đến hiểu biết tốt hơn về hành vi định tuyến khi N tăng. Quan tâm ngay lập tức là N_cutoff: giá trị N mà N*(N,E) ≤ N. Đối với N lớn hơn, định tuyến sẽ không cải thiện hiệu suất. Điều này dễ dàng tìm thấy tuân theo log N_cutoff = -b/c. N_cutoff bằng 937B, 85B và 83B cho S-BASE, RL-R và HASH tương ứng. Những giá trị này phụ thuộc cao vào số lượng token được thấy, và N_cutoff được mong đợi tăng với số lượng token tăng.

Tiếp theo, chúng tôi xem xét N*_max(N) ≜ max_E N*(N,E), tức số tham số hiệu quả tối đa mà mạng định tuyến có thể đạt. Eq. (11) dự đoán rằng log N* là hàm affine của log N cho bất kỳ E cố định nào, và N*_max(N) = N cho N > N_cutoff. Do đó log N*_max là từng phần affine trong log N, như được hiển thị trong Hình 6:

∀N ≤ N_cutoff = 10^{-b/c}; N*_max(N) = N*(N,E_max);
∀N > N_cutoff; N*_max(N) = N. (13)

Lưu ý rằng N*_max liên tục gần N_cutoff, vì đối với tất cả E, N*(N_cutoff,E) = N_cutoff. Hơn nữa, độ dốc của N*_max(·) cho N ≤ N_cutoff dương bất cứ khi nào E_max ≤ E_start · 10^{a/c}, đúng với các hệ số của chúng tôi. Trong bối cảnh này, N*_max(·) là hàm không giảm của N. Do đó, đối với bất kỳ mạng định tuyến nào mà N < N_cutoff, N ≤ N*_max(N) ≤ N_cutoff, có nghĩa là định tuyến sẽ không bao giờ cho phép bạn huấn luyện mô hình mạnh hơn N_cutoff. Lưu ý rằng dù giá trị này không phụ thuộc vào E_max, sự tồn tại của nó phụ thuộc quan trọng vào biến đổi bão hòa: không có nó, N*_max không bị chặn.

5.3 Phân tích So sánh
Kaplan et al. [2020] sử dụng luật tỷ lệ để đóng gói và đối chiếu hành vi của toàn bộ lớp mô hình. Ở đây chúng tôi phản ánh phân tích này bằng cách sử dụng các luật tỷ lệ chúng tôi đã đề xuất để tóm tắt hành vi tương đối của ba kỹ thuật định tuyến được xem xét. Chúng tôi đưa ra bốn quan sát cụ thể:

• S-BASE luôn vượt trội hơn RL-R và HASH, mặc dù RL-R rất cạnh tranh ở N nhỏ hơn.
• Tất cả kỹ thuật định tuyến đều gặp phải hiệu quả giảm khi N tăng. Trong ba kỹ thuật, S-BASE tỷ lệ tốt nhất: tham số được khớp c thấp nhất.
• Đối với N nhỏ, RL-R và S-BASE tỷ lệ tương tự với số lượng chuyên gia và tốt hơn HASH (như được chỉ ra bởi việc tính độ dốc chuyên gia hiệu quả b(N) = b + c log N).
• HASH và RL-R duy trì hành vi luật lũy thừa lâu hơn S-BASE (E_max lớn hơn). Tuy nhiên, chúng gặp phải nhiều nhiễu hơn (c lớn hơn); dẫn đến hiệu suất tệ hơn cho hầu hết kích thước mô hình.
• HASH có overhead ban đầu lớn (E_start lớn hơn), rõ ràng thấy như độ cong rõ ràng hơn ở E nhỏ.

Đối với người thực hành quan tâm đến việc áp dụng kỹ thuật định tuyến, chúng tôi kết luận với một số khuyến nghị:
1. Sử dụng định tuyến khi huấn luyện bất kỳ mô hình nào với N ≤ 1.3B.
2. S-BASE là thuật toán định tuyến mặc định tốt. RL-R đôi khi sẽ khớp S-BASE về hiệu suất nhưng ít mạnh mẽ và có thể mở rộng hơn (§D.1).
3. Nhắm mục tiêu sử dụng E ∈ {64,128} chuyên gia. Giá trị lớn hơn sẽ tiếp tục cải thiện, nhưng với lợi ích giảm dần.
4. Sử dụng K = 1 chuyên gia. Định tuyến lớp với tần suất 0.5 ≤ R ≤ 1; tần suất thấp hơn giảm hiệu suất.
5. Nghiên cứu định tuyến tương lai nên tập trung vào các thuật ngữ c và E_max; chỉ thị giới hạn tỷ lệ tùy ý.
6. Kỹ thuật định tuyến mới phải được xác thực ở nhiều giá trị N và E khi so sánh với công việc trước. Kết quả trên kích thước đơn không thể ngoại suy.

6 Công trình Liên quan
Trong việc nghiên cứu các khía cạnh thực nghiệm của tỷ lệ, công trình này theo sau Kaplan et al. [2020]; kích thích nhiều nghiên cứu bao gồm Henighan et al. [2020], Hernandez et al. [2021] và Ghorbani et al. [2021]. Lý thuyết cơ bản ít được hiểu hơn, nhưng có một số khám phá không gian này bao gồm Hutter [2021] và Bahri et al. [2021].

Những nghiên cứu này, và của chúng tôi, phụ thuộc lẫn nhau vào một tập hợp lớn công trình cải thiện khả năng mở rộng của Transformer. Điều này bao gồm các mô hình như GPT-2 [Radford et al., 2019], GPT-3 [Brown et al., 2020], Jurassic-1 [Lieber et al., 2021] và Gopher [Rae et al., 2021], cũng như công trình cải thiện khả năng của những mô hình này được song song hóa hiệu quả trên nhiều thiết bị, bao gồm Shoeybi et al. [2019], Narayanan et al. [2019], Kim et al. [2021] và Xu et al. [2021].

Song song với tất cả điều này là nghiên cứu lâu dài về Mạng Định tuyến; một thuật ngữ được giới thiệu bởi Rosenbaum et al. [2018] nhưng được phát triển rộng rãi trong tài liệu như Conditional Computation [Bengio et al., 2013, 2016, Bengio, 2017, Denoyer and Gallinari, 2014] và Mixture of Experts [Jacobs et al., 1991, Collobert et al., 2003, Eigen et al., 2014]. Khung này đôi khi được tổng quát hóa thêm, được xem như tìm kiếm kiến trúc theo từng ví dụ trong Ramachandran and Le [2018] hoặc như bài toán đồ thị trong Denoyer and Gallinari [2014]. Định tuyến được phổ biến cho huấn luyện quy mô lớn bởi Shazeer et al. [2017], và được thúc đẩy bởi công trình bao gồm GShard [Lepikhin et al., 2020], Switch Transformer [Fedus et al., 2021] và GLaM [Du et al., 2021]. Theo hướng này, Artetxe et al. [2021] thực hiện phân tích so sánh các mạng dày đặc và SMOE với E = 512 phù hợp với kết quả của chúng tôi. Cuối cùng, kiến trúc định tuyến cốt lõi vẫn đang được cải thiện. Nie et al. [2021] điều chỉnh K qua huấn luyện trong khi Hazimeh et al. [2021] học nó qua tổn thất khả vi. Ramachandran and Le [2018] tăng K qua độ sâu và khuyến khích đa dạng kiến trúc trên các chuyên gia. Caccia et al. [2021] phát triển E trong suốt huấn luyện và Rajbhandari et al. [2022] đề xuất các mạng mà E thay đổi với độ sâu.

7 Kết luận
Sử dụng tính toán có điều kiện để mở rộng mạng nơ-ron từ lâu là mục tiêu nghiên cứu, và các phương pháp dựa trên Mạng Định tuyến đã tăng dần về mức độ phổ biến. Ở đây chúng tôi đã giới thiệu luật tỷ lệ (Eq. (1)) mô hình hóa hành vi của những mạng này. Luật tỷ lệ này dự đoán rằng, đối với tất cả mô hình được xem xét, việc giới thiệu định tuyến vào mô hình ngôn ngữ cải thiện hiệu suất. Cải thiện đó theo luật lũy thừa trong số lượng chuyên gia E giảm dần với kích thước mô hình N, và có thể được tổng quát hóa thêm trên các kiến trúc định tuyến với Eq. (2). Những luật tỷ lệ này định lượng sự khác biệt giữa ba kỹ thuật định tuyến khác nhau và dẫn đến một số vô hướng duy nhất (Eq. (11)) mô tả đồng thời hiệu suất của cả mô hình định tuyến và dày đặc.

Công trình này cung cấp khung thực nghiệm để phân tích các đổi mới tương lai trong định tuyến. Chúng tôi hy vọng bằng chứng áp đảo mà chúng tôi cung cấp về lợi ích của định tuyến khuyến khích nó được áp dụng nhanh chóng hơn như một công cụ mạnh mẽ để cải thiện mô hình, có đặc điểm tỷ lệ phù hợp với các phương pháp tỷ lệ truyền thống (về độ sâu và chiều rộng) và sẽ duy trì lợi ích lên đến các mô hình với kích thước mô hình cơ sở lớn hơn 900 tỷ tham số.
