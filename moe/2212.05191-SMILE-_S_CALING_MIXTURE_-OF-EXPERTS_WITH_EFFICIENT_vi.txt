# 2212.05191.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2212.05191.pdf
# Kích thước file: 1895954 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

Chaoyang He1, Shuai Zheng2, Aston Zhang2, George Karypis2, Trishul Chilimbi2
Mahdi Soltanolkotabi1, Salman Avestimehr1
1Đại học Southern California
2AWS AI
Email: chaoyang.he@usc.edu

TÓM TẮT
Song song hóa Mixture of Expert (MoE) là một tiến bộ gần đây giúp mở rộng kích thước mô hình với chi phí tính toán không đổi. MoE chọn các tập tham số khác nhau (tức là các chuyên gia) cho mỗi token đầu vào, tạo ra một mô hình được kích hoạt thưa thớt. Mặc dù có một số ứng dụng thành công của MoE, hiệu quả huấn luyện của nó giảm đáng kể khi số lượng chuyên gia tăng lên. Giai đoạn định tuyến trong MoE phụ thuộc vào hiệu quả của tập hợp truyền thông All2All, vốn gặp phải tình trạng tắc nghẽn mạng và có khả năng mở rộng kém. Để giảm thiểu những vấn đề này, chúng tôi giới thiệu SMILE, khai thác băng thông mạng không đồng nhất và chia định tuyến một bước thành định tuyến hai cấp. Kết quả thí nghiệm của chúng tôi cho thấy phương pháp được đề xuất đạt được tăng tốc 2.5 lần so với Switch Transformer về thông lượng tiền huấn luyện trên "Colossal Clean Crawled Corpus" mà không mất bất kỳ tốc độ hội tụ nào.

1 Giới thiệu
Các mô hình khổng lồ gần đây đã thu hút sự chú ý đáng kể do hiệu suất xuất sắc của chúng trong xử lý ngôn ngữ tự nhiên [1], thị giác máy tính [2], và học tập đa phương thức [3]. Tuy nhiên, việc huấn luyện các mô hình khổng lồ đòi hỏi tài nguyên tính toán và dữ liệu đáng kể. Khi kích thước mô hình được mở rộng, việc huấn luyện quy mô lớn như vậy trở nên tốn kém về mặt tính toán và không thân thiện với môi trường [4]. Theo quan điểm, tổng lượng phát thải net tCO2e cho GPT-3 là khoảng 552 tCO2e [4], trong khi một chuyến bay khứ hồi trực tiếp của một hành khách trên máy bay phản lực giữa San Francisco và New York phát thải khoảng 180 tCO2e [4]. Các nghiên cứu gần đây đã bắt đầu tìm kiếm các cách tiếp cận thay thế để cho phép hiệu quả tính toán lớn hơn.

Mixture of Experts (MoEs) [5] đã nổi lên như mạng thần kinh nền tảng để mở rộng khả năng mô hình bằng cách sử dụng số lượng lớn tham số trong khi duy trì chi phí tính toán không đổi bằng cách định tuyến đầu vào đến một tập con nhỏ các chuyên gia với một bộ định tuyến. Mặc dù MoEs có triển vọng về hiệu suất mô hình và hiệu quả suy luận, chúng đòi hỏi thiết kế và điều chỉnh cẩn thận của bộ định tuyến. Một bộ định tuyến thực tế nên cho phép cân bằng tải công việc cho các chuyên gia để tránh làm giảm hiệu suất mô hình hoặc giảm chi phí truyền thông để đảm bảo rằng việc huấn luyện kết thúc trong thời gian hợp lý [6,7,8]. Ví dụ, Switch Transformer [7] được giới thiệu để huấn luyện một mô hình với 1.6 nghìn tỷ tham số bằng cách đơn giản hóa MoE để định tuyến mỗi token chỉ đến một chuyên gia duy nhất và sử dụng một loss phụ trợ để cải thiện cân bằng tải công việc trong quá trình huấn luyện.

Mặc dù thành công của các tài liệu nêu trên trong việc tiền huấn luyện các mô hình khổng lồ, chúng có hai nhược điểm chính. Thứ nhất, chúng dựa vào một tập hợp truyền thông All2All cho cả trao đổi dữ liệu trong node và giữa các node, có băng thông không đồng nhất khác biệt một khoảng cách lớn (ví dụ, trong AWS EC2 P4d, băng thông đỉnh của EFA là 400Gbps, trong khi băng thông tổng hợp của NVSwitch bên trong một node là 600GB/s), do đó làm giảm hiệu quả truyền thông. Thứ hai, việc tính toán một bộ định tuyến cân bằng ngày càng đắt đỏ hơn khi sử dụng nhiều chuyên gia hơn. Trong Switch Transformer, một số lượng lớn chuyên gia gây ra tắc nghẽn, tạo ra các điểm nóng mạng và ảnh hưởng xấu đến hiệu suất.

Để giải quyết các nút thắt cổ chai trên, chúng tôi giới thiệu định tuyến hai cấp để mở rộng MoE với định tuyến hiệu quả hơn, được gọi là SMILE. Tổng quan hệ thống được minh họa trong Hình 1. Chúng tôi chia các chuyên gia thành hai cấp dựa trên cấu trúc lưới. Tất cả các chuyên gia trong một node được coi là một nhóm. Mỗi token đầu tiên được định tuyến đến một node và sau đó được

--- TRANG 2 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

Hình 1: Tổng quan về SMILE p(n×m)q. Định tuyến hai cấp chia việc phân phối token thành hai giai đoạn, trong đó ở giai đoạn đầu tiên mỗi token được định tuyến đến một node thông qua bộ định tuyến liên-node và sau đó được gán cho một GPU bởi bộ định tuyến nội-node. Điều này cho phép chúng tôi tận dụng tốt hơn các băng thông không đồng nhất để đạt được hiệu quả truyền thông lớn hơn.

gửi đến một GPU cụ thể trong node. Bằng cách này, tắc nghẽn mạng liên-node được giảm đáng kể. Hơn nữa, chi phí khởi chạy trên một node cho truyền thông All2All được giảm từ O(mn) xuống O(m+n), trong đó m và n biểu thị số lượng GPU trên mỗi node và tổng số node, tương ứng. Tương tự, độ phức tạp thời gian của định tuyến được giảm từ O(mnTd) xuống O(max(n,m)Td), trong đó T là tổng số token và d là kích thước ẩn của mô hình.

Trong các thí nghiệm, chúng tôi chứng minh rằng SMILE được đề xuất cải thiện thông lượng so với Switch Transformer lên 2.5× cho việc huấn luyện một mô hình 3.7B tham số trên 128 GPU trong khi có thể duy trì cùng tốc độ hội tụ. Các phân tích khả năng mở rộng cho thấy SMILE đạt được hiệu quả mở rộng yếu và mạnh tốt hơn đáng kể so với Switch Transformer, và duy trì lợi thế hiệu suất tốt khi chúng tôi tăng kích thước mô hình từ 3.7B lên 48B. Việc profiling một lớp MoE duy nhất xác nhận động lực của chúng tôi rằng truyền thông All2All là nút thắt cổ chai chính trong MoE và định tuyến hai cấp được đề xuất giảm thiểu đáng kể chi phí truyền thông.

2 Các công trình liên quan
Mixture of Experts (MoE), trong bối cảnh của các kiến trúc học sâu hiện đại, đã được chứng minh hiệu quả trong [5]. Trong công trình này, một lớp MoE được áp dụng giữa các lớp LSTM [9] xếp chồng, và các token được định tuyến riêng biệt đến các tập con khác nhau của các chuyên gia. Trong công trình của chúng tôi, chúng tôi xem xét một hybrid của song song hóa dữ liệu + chuyên gia, trong đó mỗi worker giữ một chuyên gia duy nhất cho mỗi lớp MoE, và số lượng chuyên gia mở rộng theo số lượng worker, tức là N = nm với m và n biểu thị số lượng GPU trên mỗi node và tổng số node. Đối với việc gán token, bộ định tuyến được trang bị một biến Wr ∈ R^(N×d) và tạo ra logits r(x) = Wr x, trong đó x ∈ R^d là vector ẩn của token. Các logits được chuẩn hóa thông qua một softmax để xây dựng xác suất chọn mỗi chuyên gia. Xác suất định tuyến cho chuyên gia e được cho bởi

pe(x) = re(x) / Σ(i=1 to N) ri(x)     (1)

Sau đó, top-k chuyên gia được chọn để xử lý token đã cho. Ký hiệu tập hợp các chuyên gia được chọn là I. Đầu ra của top-k chuyên gia được cho bởi

y(x) = Σ(e∈I) pe(x)Ee(x)     (2)

trong đó Ee(·) là mô hình con (ví dụ, perceptron đa lớp) cho chuyên gia e. Bằng cách này, số lượng tham số mô hình tăng tuyến tính theo số lượng chuyên gia chỉ với một lượng nhỏ chi phí tính toán bổ sung cho định tuyến. MoE cung cấp hiệu suất tối tân trong các benchmark mô hình hóa ngôn ngữ và dịch máy. Bước định tuyến có tổng độ phức tạp O(kmnTd), trong đó T là tổng số token. Lớp MoE được giới thiệu lại vào kiến trúc Transformer bởi thư viện Mesh Tensorflow [10] trong đó các lớp MoE được giới thiệu như một sự thay thế cho các lớp Feed-forward Network (FFN) trong Transformers [11]. Tuy nhiên, không có kết quả NLP đi kèm.

--- TRANG 3 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

Với những tiến bộ gần đây trong cơ sở hạ tầng học máy, GShard [6], mở rộng trình biên dịch XLA, sử dụng MoE để cải thiện đáng kể dịch máy qua 100 ngôn ngữ. Trong [12], một chiến lược MoE xác định khác được áp dụng để chia các tham số mô hình thành các nhóm ngôn ngữ không chồng lấp. Switch Transformer [7] đơn giản hóa quá trình định tuyến và chỉ chọn một chuyên gia duy nhất cho mỗi token. BASE [8] là một biến thể MoE khác xếp chồng nhiều lớp FFN như một chuyên gia duy nhất và chèn chúng vào một kiến trúc Transformer tiêu chuẩn. Điều này làm tăng đáng kể thời gian suy luận so với một Transformer vanilla.

Phương pháp được đề xuất của chúng tôi giới thiệu định tuyến hai cấp để tận dụng tốt hơn băng thông truyền thông không đồng nhất, và sử dụng hai loss cộng gộp để cân bằng tải, đạt được tăng tốc lớn. Một công trình đồng thời [13] độc lập xem xét all-to-all nội-node trước tiên sau đó là một all-to-all liên-node. Chúng tôi muốn nhấn mạnh rằng công trình này không thay đổi cơ chế định tuyến và chỉ khai thác all-to-all phân cấp cho suy luận.

3 Phương pháp
3.1 Bối cảnh: Nút thắt cổ chai của MoE trong việc mở rộng đến một số lượng lớn GPU

```cpp
for (int r = 0; r < numranks; r++) {
    if (count != 0) {
        ncclSend(sendbuff + r * rankdiff,
                count, type, r, comm, stream);
        ncclRecv(recvbuff + r * rankdiff,
                count, type, r, comm, stream);
    }
}
```

Hình 2: Việc triển khai truyền thông All2All trong NCCL (torch/csrc/cuda/nccl.cpp)

[THIS IS FIGURE/CHART: Biểu đồ thông lượng khi mở rộng Switch Transformer lên số lượng lớn GPU, với trục x là số node (1, 4, 8, 16) và trục y là thông lượng (samples/s) từ 0 đến 8000]

Hình 3: Kết quả thông lượng khi mở rộng Switch Transformer lên số lượng lớn GPU

MoE phụ thuộc mạnh vào hiệu suất của All2All. Tùy thuộc vào cấu trúc mạng, các thuật toán All2All khác nhau dẫn đến chi phí truyền thông, độ trễ và hành vi tắc nghẽn mạng khác nhau. Giả sử có tổng cộng N worker. Đối với cấu trúc vòng, All2All có chi phí truyền thông bậc hai và độ trễ tuyến tính trên N trong khi chi phí truyền thông và độ trễ được giảm xuống O(N^(3/2)) và O(N^(1/2)), tương ứng cho cấu trúc lưới như TPU [14]. Bất kể cấu trúc cơ bản của mạng, một cách tiếp cận tầm thường khác là gửi tất cả các tin nhắn bất đồng bộ vào mạng như được minh họa trong Hình 2. Thuật toán naïve này triển khai định tuyến một-một theo cặp và gặp phải tắc nghẽn mạng do độ rộng phân chia của mạng [15]. Như trong Hình 3, Switch Transformer có hiệu quả mở rộng rất kém khi số lượng node tăng từ 1 lên 16 (từ 8 lên 128 GPU). Thông lượng trên 8 node thậm chí còn tệ hơn so với 4 node.

3.2 Lớp MoE hiệu quả thông qua định tuyến hai cấp

Để giải quyết các nút thắt cổ chai trên, chúng tôi giới thiệu định tuyến hai cấp để mở rộng MoE với định tuyến hiệu quả hơn, được gọi là SMILE.

3.2.1 Kiến trúc mô hình: Điều phối All2All liên-node và All2All nội-node

Để tính đến tính chất không đồng nhất và phân cấp của mạng kết nối, chúng tôi chia các chuyên gia thành hoạt động All2All hai cấp. Tất cả các chuyên gia trong một node được nhóm lại với nhau. Như được hiển thị trong Hình 4, khi một token sẵn sàng để gửi đi, nó đầu tiên được định tuyến đến một node thông qua bộ định tuyến liên-node (màu xanh) và sau đó được gán cho một GPU thông qua bộ định tuyến nội-node (màu cam).

Định tuyến hai cấp được đề xuất giảm thời gian khởi chạy trên một node cho All2All từ O(mn) xuống O(m+n), trong đó m và n là số lượng GPU trên mỗi node và tổng số node, tương ứng. Về mặt hiệu quả

--- TRANG 4 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

Hình 4: Minh họa Lớp SMILE (m = 8, n = 8)

truyền thông, định tuyến hai cấp song song hóa nhiều tập hợp All2All, giảm thiểu sự can thiệp mạng giữa các luồng và giảm đáng kể tắc nghẽn mạng liên-node so với thuật toán naïve được triển khai trong NCCL (Tham khảo Hình 2).

Định tuyến hai cấp cũng đơn giản hóa tối ưu hóa router trong đó kích thước của bài toán định tuyến trong Switch Transformer (Phương trình 1) được giảm từ mn xuống max(n,m). Cụ thể, độ phức tạp định tuyến trong Switch Transformer được giảm từ O(mnTd) xuống O(max(n,m)Td).

Đường cong nét đứt trong Hình 4 cũng minh họa cách chúng tôi tính toán đầu ra của chuyên gia top-1:

h_out(i) = p_i(h_in)q_j(h_in)E_{i,j}(h_in)     (3)

trong đó p_i(h_in) và q_j(h_in) là xác suất định tuyến top-1 (được tạo bởi Phương trình 1) được phân bổ cho node i và chuyên gia cục bộ j, tương ứng, và E_{i,j} là chuyên gia thứ j trên node i. Cả bộ định tuyến liên-node và nội-node đều có tham số chia sẻ trên tất cả các worker, đảm bảo rằng kết quả không thay đổi khi một ví dụ đầu vào được xử lý bởi một worker khác. So với một bộ định tuyến duy nhất, bộ định tuyến hai cấp cũng giảm tổng số tham số router từ O(mn) xuống O(m+n).

3.2.2 Loss cân bằng tải cộng gộp

Khác với loss cân bằng tải một bước nhảy trong các công trình trước [5,10,6,7], chúng tôi sử dụng loss cân bằng tải (LB) cộng gộp cho định tuyến hai cấp. Cho N chuyên gia được đánh chỉ số từ i = 1 đến N, chúng tôi tách nó thành n × m, trong đó n là số lượng node GPU và m là số lượng GPU trên một node duy nhất (ví dụ, 8 GPU là cấu hình phổ biến cho một node GPU). Đối với một batch B với T token, loss cân bằng tải cộng gộp có hai thành phần, mỗi thành phần được tính như tích vô hướng có tỷ lệ giữa các vector phần phân phối và xác suất định tuyến:

loss_lb = α × Σ(i=1 to n) f_i P_i + β × Σ(j=1 to m) f'_j Q_j     (4)
         [inter-node LB loss]   [intra-node LB loss]

trong đó f_i và f'_j là phần các token được gửi đến node i và chuyên gia cục bộ j, tương ứng: f_i = (1/T) Σ_{x∈B} 1[argmax(p(x)) = i], f'_j = (1/T) Σ_{x∈B} 1[argmax(q(x)) = j]; P_i và Q_j là phần xác suất router được phân bổ cho node i và chuyên gia j tương ứng; P_i = (1/T) Σ_{x∈B} p_i(x), Q_j = (1/T) Σ_{x∈B} q_j(x); các siêu tham số α và β là các hệ số nhân. Giá trị tối thiểu đạt được dưới định tuyến liên-node và nội-node đồng đều, tức là min loss_lb = n × (1/n) × (1/n) + m × (1/m) × (1/m). Trong thực tế, chúng tôi chỉ đơn giản sử dụng α = β.

Để tính tổng loss mô hình trong quá trình huấn luyện, chúng tôi tính tổng loss_lb trong tất cả các lớp SMILE như loss phụ trợ:

loss_total = loss_train + Σ(l=1 to L) loss_lb^l     (5)

trong đó L biểu thị tổng số lớp SMILE và loss_lb^l là loss cân bằng tải trong lớp SMILE thứ l.

--- TRANG 5 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

3.2.3 Quản lý nhóm tiến trình hai cấp

Hình 5: Quản lý nhóm tiến trình hai cấp và mã giả của nó

Việc triển khai hệ thống của lớp SMILE đòi hỏi hai cấp quản lý tiến trình phân tán. Nhóm tiến trình cấp đầu tiên xử lý All2All cấp node, và cấp thứ hai quản lý All2All giữa các tiến trình GPU nội-node. Ngoài ra, hai nhóm tiến trình này nên được kết nối để hoàn thành định tuyến hai cấp mà không có sự can thiệp lẫn nhau.

Phía bên trái của Hình 5 cho thấy quá trình hợp tác giữa nhóm tiến trình liên-node cấp đầu tiên và nhóm tiến trình nội-node cấp thứ hai để hoàn thành định tuyến hai cấp. Dựa trên các yêu cầu như vậy, chúng tôi đề xuất một cơ chế quản lý tiến trình dựa trên API PyTorch dist.new_group. Như được hiển thị ở bên phải, đối với mỗi tiến trình GPU, chúng tôi tạo một nhóm tiến trình liên-node và một nhóm tiến trình nội-node, trong đó các xếp hạng tiến trình trong nhóm được hiển thị bằng văn bản màu xanh và văn bản màu cam, tương ứng. Dựa trên việc quản lý nhóm tiến trình này, khi thực hiện hoạt động All2All cho Lớp BiMoE, chúng tôi chỉ cần chỉ định phiên bản inter_node_process_group và phiên bản intra_node_process_group theo xếp hạng cục bộ. Phương pháp này đơn giản hóa rất nhiều việc quản lý nhóm tiến trình để bản thân lớp MoE không cần quan tâm đến các chi tiết triển khai hệ thống. Phía bên phải của hình cũng cho thấy quá trình của bốn hoạt động All2All tuần tự. Hai hoạt động All2All bổ sung được yêu cầu do định tuyến ngược cho lớp attention liên tiếp.

4 Thí nghiệm
4.1 Thiết lập thí nghiệm

Nhiệm vụ và Tập dữ liệu. Chúng tôi đánh giá SMILE trên các nhiệm vụ tiền huấn luyện NLP với các mô hình Transformer lớn. Chúng tôi sử dụng một nhiệm vụ mô hình hóa ngôn ngữ có mặt nạ [16,17,18] trong đó mô hình được huấn luyện để dự đoán các token bị thiếu. Chúng tôi đánh giá hiệu suất của SMILE bằng cách tiền huấn luyện trên "Colossal Clean Crawled Corpus" (C4), một tập hợp văn bản tiếng Anh có nguồn gốc từ việc quét web Common Crawl công cộng. Nó bao gồm các heuristic để trích xuất chỉ ngôn ngữ tự nhiên (trái ngược với boilerplate và các rác khác) ngoài việc loại bỏ trùng lặp rộng rãi [19]. Tập dữ liệu C4 được lấy từ phiên bản được tuyển chọn do Hugging Face Dataset lưu trữ. Nó có 129 tỷ token (từ) trong tập dữ liệu huấn luyện và 129 triệu token (từ) trong tập dữ liệu xác thực. Đối với việc huấn luyện song song trên số lượng lớn GPU, chúng tôi chia tập dữ liệu huấn luyện thành 32768 (1024 x 24) tệp, và tập dữ liệu xác thực thành 256 tệp. Chúng tôi sử dụng cùng từ vựng như mô hình T5 (11B) gốc (kích thước từ vựng là 32128).

Kiến trúc mô hình. Chúng tôi so sánh SMILE với Switch Transformer để chứng minh hiệu quả của định tuyến hai cấp. Phương pháp được đề xuất cũng có thể được sử dụng kết hợp với các mô hình MoE khác như GShard [6] và BASE [8]. Để so sánh công bằng, Switch Transformer và SMILE sử dụng cùng kiến trúc giống BERT (một ngăn xếp nhiều lớp Transformer tiêu chuẩn) nhưng thay thế mỗi lớp mạng feed forward (FFN) khác trong Transformer bằng một lớp MoE (mixture of experts). Trong mỗi lớp Transformer, lớp MoE theo sau một lớp multi-head attention, và chúng được

--- TRANG 6 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

tăng cường với một kết nối tắt theo sau bởi một hoạt động LayerNorm sau đó. Hàm kích hoạt trong lớp attention và FFN được đặt thành GELU với tỷ lệ dropout 0.1. Trong SMILE, kiến trúc định tuyến được định nghĩa theo thiết kế trong Mục 3.2.1. Đối với các kích thước mô hình khác nhau, chúng tôi chỉ thay đổi số lượng lớp ẩn, kích thước ẩn và kích thước trung gian (chi tiết được giới thiệu trong Mục A.2).

Phần cứng. Chúng tôi chạy thí nghiệm trên phần cứng tối tân trong AWS với hỗ trợ tiên tiến cho tính toán, truyền thông, lưu trữ: 1. Bộ gia tốc GPU: chúng tôi đánh giá tất cả baseline và SMILE trên các node AWS P4d. Mỗi node được trang bị 8 GPU NVIDIA A100. Chúng tôi mở rộng lên đến 16 node để đánh giá khả năng mở rộng; 2. Bộ truyền thông băng thông cao: Chúng tôi sử dụng AWS EFA (Elastic Fabric Adapter) cho mạng liên-node băng thông cao 400 Gbps. So với NVIDIA InfiniBand thường được sử dụng, giao diện phần cứng bypass hệ điều hành (OS) được xây dựng tùy chỉnh của EFA tăng cường hiệu suất truyền thông giữa các phiên bản. Thí nghiệm của chúng tôi cho thấy rằng ngay cả trong thiết lập băng thông cao như vậy, truyền thông All2All vẫn là nút thắt cổ chai trong các mô hình MoE (ví dụ, Switch Transformer); 3. Hệ thống tệp mạng: Để tăng cường hiệu suất truy cập các tệp dữ liệu theo cách phân tán, chúng tôi sử dụng AWS FSx với hỗ trợ SSD. Tổng chi phí lưu trữ cho tập dữ liệu C4 và tất cả mã nguồn là khoảng 800G.

Siêu tham số huấn luyện. Chúng tôi huấn luyện các mô hình MoE với bộ tối ưu hóa LAMB [20], trong đó tỷ lệ học được điều chỉnh trong khoảng {0.0001, 0.0003, 0.001, 0.003}, weight decay được cố định ở 0.01, và ε được đặt thành 1e-6. Chúng tôi cắt gradient nếu chuẩn l2 của chúng vượt quá 1.0. Như một thực hành phổ biến để giảm chi phí bộ nhớ GPU trong bộ tối ưu hóa LAMB, chúng tôi cũng kích hoạt độ chính xác một nửa (fp16). Chúng tôi sử dụng độ dài chuỗi 128. Trừ khi được chỉ định khác, chúng tôi cố định kích thước batch huấn luyện tổng thể là 16384 và kích thước micro batch là 128, trong đó kích thước micro batch đề cập đến kích thước batch trên mỗi GPU trên mỗi micro step và total_batch_size = micro_batch_size * num_micro_steps. Tích lũy gradient được áp dụng khi số lượng micro step lớn hơn 1. Chúng tôi sử dụng 128 vì đó là kích thước tối đa có thể được sử dụng dưới các ràng buộc bộ nhớ GPU với cấu hình phần cứng của chúng tôi. Đối với phân tích khả năng mở rộng, chúng tôi mở rộng số lượng node từ 1 node (8 GPU) lên 16 node (128 GPU).

Triển khai. Mã nguồn của chúng tôi được duy trì tốt như một gói Python pip. Chúng tôi triển khai thuật toán của mình với sự tích hợp của các framework PyTorch DDP và DeepSpeed. Quản lý nhóm tiến trình được giới thiệu trong được xử lý bởi các API nhóm PyTorch DDP [21]. Chúng tôi sử dụng bộ tối ưu hóa LAMB được triển khai bởi DeepSpeed [22]. Đối với việc huấn luyện hiệu quả bộ nhớ GPU của mô hình dense lớn, chúng tôi tái sử dụng một vài kỹ thuật được hỗ trợ bởi DeepSpeed, bao gồm tối ưu hóa ZERO [23], checkpoint kích hoạt và độ chính xác một nửa (fp16). Để phân tích sự phân tích thời gian chi tiết cho truyền thông và tính toán trong lớp MoE, chúng tôi sử dụng PyTorch Profiler. Bộ tải dữ liệu của chúng tôi cho tập dữ liệu C4 được tùy chỉnh với cơ chế pre-fetching để tải phân tán hiệu quả.

4.2 So sánh với BERT và Switch Transformer

Hình 6: Đường cong lặp-đến-perplexity
Hình 7: Loss cân bằng tải không được tỷ lệ hóa.

--- TRANG 7 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

Huấn luyện chính xác và nhanh hơn. Ngoài Switch Transformer, chúng tôi so sánh SMILE với các baseline BERT (110M) và BERT (3.7B) có cùng số lượng phép toán dấu phẩy động (FLOPs) và số lượng tham số như Switch Transformer và SMILE, tương ứng. Chúng tôi sử dụng α = 0.01 cho Switch Transformer và α = β = 0.005 (được giới thiệu trong Phương trình (4)) cho SMILE trong thí nghiệm của chúng tôi, và đặt hệ số dung lượng cho định tuyến là 2.0. Chúng tôi thay thế mỗi lớp feed-forward chia sẻ khác trong kiến trúc Transformer bằng một lớp MoE (Resp. SMILE).

Bảng 1: Thông lượng (mẫu/giây)
Mô hình | Thông lượng
BERT (110M) | 93282
BERT (3.7B) | 5114
Switch Transformer | 8112
SMILE | 20011

Từ Hình 6, 7 và Bảng 1, chúng tôi có bốn quan sát quan trọng. Thứ nhất, SMILE có cùng hành vi hội tụ như Switch Transformer, và nó hội tụ nhanh hơn BERT (110M). Thứ hai, cả SMILE và Switch Transformer đều hội tụ chậm hơn BERT (3.7B), điều này được mong đợi vì các mô hình MoE đánh đổi hội tụ để có hiệu quả tính toán lớn hơn. Thứ ba, cả Switch Transformer và SMILE đều chậm hơn BERT (110M), cho thấy rằng định tuyến là nút thắt cổ chai chính trong các mô hình MoE. Và, SMILE chạy nhanh hơn 2.5x và 3.9x so với Switch Transformer và BERT (3.7B), tương ứng. Điều này chứng minh rằng định tuyến hai cấp hiệu quả trong việc giảm chi phí của các lớp MoE tiêu chuẩn. Cuối cùng, SMILE đạt được loss cân bằng không được tỷ lệ hóa gấp đôi so với Switch Transformer, điều này được mong đợi vì SMILE có hai loss cộng gộp. Khi được tỷ lệ hóa với α, β, hai đường cong sẽ gần như trùng lặp với nhau.

Tiếp theo, chúng tôi cung cấp phân tích hiệu suất chi tiết và các nghiên cứu ablation để biện minh cho sự cần thiết của định tuyến hai cấp.

4.3 Khả năng mở rộng
4.3.1 Khả năng mở rộng trên truyền thông liên-node băng thông cao (400 Gbps)

Hình 8: Switch Transformer vs. SMILE. Số lượng node tăng từ 1 lên 16.
(a) Weak Scaling (b) Strong Scaling

Chúng tôi so sánh thông lượng (mẫu trên giây) giữa SMILE và Switch Transformers khi mở rộng số lượng node GPU (mỗi node có 8 GPU) từ 1 lên 16 trong băng thông cao. Cả weak scaling và strong scaling đều được đánh giá. Trong weak scaling, kích thước batch toàn cục được điều chỉnh theo số lượng GPU, trong khi trong strong scaling, cả kích thước batch toàn cục và kích thước micro batch trên mỗi GPU đều được cố định (số bước tích lũy gradient giảm khi số lượng node tăng lên). Từ kết quả trong Hình 8, chúng tôi có các quan sát sau.

1. Chi phí MoE trong Switch Transformer (Truyền thông All2All) không tầm thường ngay cả trong băng thông lớn được hỗ trợ bởi bộ điều hợp truyền thông tiên tiến (AWS EFA). Hiệu quả mở rộng của nó còn lâu mới đạt được mở rộng tuyến tính, có thể được giải thích bởi chi phí truyền thông liên-node bổ sung làm giảm hiệu suất; thậm chí tệ hơn nữa là thông lượng cuối cùng trên 16 node không tốt hơn đáng kể so với một node duy nhất và 8 node có thông lượng tệ hơn 4 node.

2. So với Switch Transformer, SMILE mở rộng tốt hơn nhiều từ 1 node lên 16 node. Thông lượng trên 16 node cao hơn 7.7x và 4x so với 1 node với weak scaling và strong scaling, tương ứng. Hơn nữa, khác với Switch Transformer, khi mở rộng từ 4 node lên 8 node, thông lượng vẫn tăng. Chúng tôi quan sát hiệu suất tệ hơn của SMILE trên 1 node với weak scaling, điều này là do chi phí bổ sung trong việc triển khai. Trên một node duy nhất, chúng tôi nên sử dụng trực tiếp Switch Transformer.

Do đó, chúng tôi kết luận rằng định tuyến hai cấp hiệu quả trong việc mở rộng MoE liên-node, và khả năng mở rộng được cải thiện đáng kể khi áp dụng SMILE.

4.3.2 Khả năng mở rộng w.r.t. Kích thước mô hình khác nhau

Bảng 2: So sánh thông lượng giữa Switch Transformer và SMILE (16 node P4d). Chúng tôi cố định tổng kích thước batch là 16384 và thay đổi kích thước micro batch tùy thuộc vào kích thước mô hình và bộ nhớ GPU.

Kích thước mô hình (128 Chuyên gia) | Cấu hình mô hình | Thông lượng (mẫu/giây)
Switch Transformer | SMILE

3.7B | micro_batch_size = 128, num_layers = 12, hidden_size = 768, intermediate_size = 3072 | 8112 | 20011 (↑2.47×)

13B | micro_batch_size = 64, num_layers = 24, hidden_size = 1024, intermediate_size = 4096 | 4001 | 6829 (↑1.71×)

48B | micro_batch_size = 64, num_layers = 36, hidden_size = 1600, intermediate_size = 6400 | 889 | 2223 (↑2.50×)

Để hiểu hiệu suất trong các kích thước mô hình khác nhau, chúng tôi đánh giá trên ba cấu hình mô hình như được hiển thị trong Bảng 2. Chúng tôi tiến hành thí nghiệm này trên 128 GPU, và số lượng chuyên gia được cố định là 128. Hai cấu hình đầu tiên sử dụng BERT_base và BERT_large như các mô hình dense cơ sở, trong khi cấu hình thứ ba được xây dựng bằng cách tăng kích thước ẩn và độ sâu mô hình. Kết quả này chứng minh rằng SMILE không bị giới hạn bởi một kiến trúc mô hình cụ thể, và vẫn đạt được tốc độ huấn luyện nhanh hơn 1.7-2.5 lần khi kích thước mô hình tăng đáng kể.

4.4 Hiểu SMILE: Phân tích thời gian và phân tích hiệu suất

Để làm rõ lợi ích khả năng mở rộng của SMILE, chúng tôi cũng đã thực hiện phân tích hiệu suất chi tiết. Việc phân tích trực tiếp hiệu suất của một lớp MoE duy nhất từ một pipeline huấn luyện end-to-end sẽ khó khăn và không chính xác, vì có các yếu tố khác liên quan bởi sự tương tác giữa song song hóa dữ liệu (AllReduce) và lớp MoE (All2All). Do đó, chúng tôi phát triển một mô hình nhỏ chỉ với một lớp MoE duy nhất và thực hiện huấn luyện với dữ liệu giả trên cùng cụm GPU với AWS EFA (16 node P4d). Bằng cách này, chúng tôi phân tích chi phí thời gian CUDA cho các giai đoạn khác nhau trong lớp MoE bằng PyTorch Profiler.

Kết quả được tóm tắt trong Hình 9 và Bảng 3. Trong Hình 9, chúng tôi chủ yếu chú thích chi phí thời gian cho các hoạt động All2All (do hai hop bổ sung trong định tuyến cho thứ tự ngược, SMILE có nhiều All2All hơn). Các quan sát sau cung cấp cho chúng tôi bằng chứng rõ ràng để hỗ trợ động lực thiết kế định tuyến hai cấp: (1) SMILE có thể cải thiện đáng kể chi phí của một lớp MoE duy nhất: thời gian chạy định tuyến hai cấp (bao gồm truyền thông EFA, truyền thông NVSwitch và tính toán GPU cho mạng chuyên gia) ít hơn 3.7 lần so với định tuyến một hop qua nhiều node (146 ms vs. 535 ms); (2) Chi phí thời gian All2All trong SMILE cũng nhỏ hơn 4.4 lần so với Switch Transformer (382 ms vs. 86 ms), phù hợp với phân tích chúng tôi đã giải thích trong Mục 3.1; (3) So với chi phí thời gian trên All2All liên-node (77 ms), chi phí thời gian trên All2All nội-node (9 ms) nhỏ hơn nhiều do băng thông cao hơn (600GB/s vs. 50GB/s); (4) Khi áp dụng SMILE, tỷ lệ (Thời gian All2All vs. Tổng thời gian) cũng được giảm từ 71% xuống 59%.

Để hiểu các chi tiết chi tiết hơn về hiệu suất, chúng tôi tham khảo toàn bộ ảnh chụp màn hình để visualize kết quả hiệu suất từ PyTorch Profiler trong Hình 10 và 11 trong Phụ lục.

--- TRANG 8 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

Hình 9: Phân tích chi phí thời gian trong các lớp MoE (16 node; chụp màn hình qua PyTorch Profiler)

Bảng 3: Phân tích chi phí thời gian trên mỗi lần lặp (micro-batch FP) trong các lớp MoE (16 node P4d)

| | Switch Transformer | SMILE |
|---|---|---|
| Tổng thời gian | 535 ms | 146 ms |
| Chi phí thời gian All2All | 382 ms | Liên node: 77 ms, Nội node: 9 ms |
| Chuyên gia FFN và các hoạt động khác (ví dụ, các hoạt động khác ngoài All2All) | 153 ms | 60 ms |
| Tỷ lệ (Thời gian All2All vs. Tổng thời gian) | 71% | 59% |

5 Kết luận
Chúng tôi đề xuất một thuật toán định tuyến và hệ thống mới cho lớp mixture-of-experts (MoE) được kích hoạt thưa thớt. Cụ thể, chúng tôi giới thiệu SMILE với định tuyến hai cấp tận dụng tốt hơn băng thông truyền thông không đồng nhất. Định tuyến hai cấp giảm đáng kể tranh chấp mạng, chi phí khởi chạy và độ phức tạp định tuyến. Thí nghiệm của chúng tôi chứng minh rằng SMILE được đề xuất cải thiện thông lượng huấn luyện 2.5× so với Switch Transformer trên 128 GPU mà không ảnh hưởng đến hội tụ.

Tài liệu tham khảo
[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.

[3] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748–8763. PMLR, 2021.

--- TRANG 9 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

[4] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David So, Maud Texier, và Jeff Dean. Carbon emissions and large neural network training. arXiv preprint arXiv:2104.10350, 2021.

[5] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, và Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

[6] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, và Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.

[7] William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.

[8] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, và Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, pages 6265–6274. PMLR, 2021.

[9] Sepp Hochreiter và Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.

[10] Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorflow: Deep learning for supercomputers. Advances in neural information processing systems, 31, 2018.

[11] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[12] Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond english-centric multilingual machine translation. Journal of Machine Learning Research, 22(107):1–48, 2021.

[13] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, và Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. arXiv preprint arXiv:2201.05596, 2022.

[14] Vipin Kumar, Ananth Grama, Anshul Gupta, và George Karypis. Introduction to parallel computing, volume 110. Benjamin/Cummings Redwood City, CA, 1994.

[15] Susanne E Hambrusch, Farooq Hameed, và Ashfaq A Khokhar. Communication operations on coarse-grained mesh architectures. Parallel Computing, 21(5):731–751, 1995.

[16] Wilson L Taylor. "cloze procedure": A new tool for measuring readability. Journalism quarterly, 30(4):415–433, 1953.

[17] William Fedus, Ian Goodfellow, và Andrew M Dai. Maskgan: Better text generation via filling in the_. arXiv preprint arXiv:1801.07736, 2018.

[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

[19] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.

[20] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, và Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.

[21] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704, 2020.

[22] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, và Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506, 2020.

[23] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–16. IEEE, 2020.

--- TRANG 10 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

A Phụ lục
A.1 Profiling hiệu suất trên các lớp MoE khác nhau

Hình 10: Profiling chi phí thời gian All2All lớp MoE Switch Transformer (16 node P4d)

Hình 11: Profiling chi phí thời gian All2All lớp SMILE (16 node P4d)

A.2 Góc nhìn khác để hiểu chi phí của truyền thông All2All

Từ kết quả, chúng tôi có thể quan sát rằng chi phí truyền thông liên-node gần như lớn hơn một vài lần so với tổng của truyền thông nội-node và lan truyền xuôi của chuyên gia. Được thúc đẩy bởi mối quan hệ như vậy trong chi phí thời gian, có thể chồng lấp chi phí truyền thông và chi phí tính toán.

Để xác minh ý tưởng này, chúng tôi sử dụng cơ chế pipeline để song song hóa việc thực thi truyền thông và tính toán trên các tài nguyên phần cứng khác nhau, tức là GPU và NIC. Chúng tôi đánh giá thông lượng trong một số lượng chunk khác nhau. Kết quả được hiển thị trong Hình 12. Thật không may, bất kể chúng tôi thao tác kích thước chunk như thế nào, hiệu suất vẫn không thể cải thiện. Chúng tôi cho rằng sự suy giảm hiệu suất là do sự gia tăng của nhiều hoạt động All2All hơn. Như chúng tôi biết từ Mục 4.4 rằng hoạt động All2All không tầm thường. Mặc dù truyền thông và truyền thông được chồng lấp ở một mức độ nào đó, số lượng hoạt động All2All được tăng lên đáng kể do số lượng hoạt động truyền thông All2All bên trong lớp MoE tăng tuyến tính theo số lượng chunk. Điều này cung cấp một góc nhìn mới để hiểu chi phí của truyền thông All2All trong lớp MoE.

--- TRANG 11 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ

Hình 12: Thông lượng thay đổi theo số lượng chunk trong chồng lấp pipeline

--- TRANG 12 ---
SMILE: MỞ RỘNG MIXTURE-OF-EXPERTS VỚI ĐỊNH TUYẾN HAI CẤP HIỆU QUẢ
