# 2206.04046.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2206.04046.pdf
# Kích thước tệp: 29226821 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
CÁC MIXTURE-OF-EXPERTS THƯA THỚT LÀ NHỮNG
NGƯỜI HỌC TỔNG QUÁT HÓA MIỀN
Bo Li1Yifei Shen2Jingkang Yang1Yezhen Wang3Jiawei Ren1
Tong Che3, 4Jun Zhang2Ziwei Liu1
1S-Lab, Đại học Công nghệ Nanyang
2Đại học Khoa học và Công nghệ Hồng Kông
3Viện AI Mila-Quebec4Nghiên cứu Nvidia
{libo0013,ziwei.liu}@ntu.edu.sg
TÓM TẮT
Nhận thức thị giác con người có thể dễ dàng tổng quát hóa đến dữ liệu thị giác ngoài phân phối,
điều này vượt xa khả năng của các mô hình học máy hiện đại. Tổng quát hóa miền (DG) nhằm thu hẹp
khoảng cách này, với các phương pháp DG hiện tại chủ yếu tập trung vào thiết kế hàm mất mát.
Trong bài báo này, chúng tôi đề xuất khám phá một hướng trực giao, tức là thiết kế kiến trúc cơ sở.
Điều này được thúc đẩy bởi một phát hiện thực nghiệm rằng các mô hình dựa trên transformer được
huấn luyện với tối thiểu hóa rủi ro thực nghiệm (ERM) vượt trội hơn các mô hình dựa trên CNN sử
dụng các thuật toán DG tiên tiến nhất (SOTA) trên nhiều bộ dữ liệu DG. Chúng tôi phát triển một
khung lý thuyết để đặc trưng cho độ bền vững của mạng đối với các dịch chuyển phân phối bằng cách
nghiên cứu sự căn chỉnh của kiến trúc mạng với các tương quan trong bộ dữ liệu. Phân tích này hướng
dẫn chúng tôi đề xuất một mô hình DG mới được xây dựng trên vision transformer, cụ thể là
Generalizable Mixture-of-Experts (GMoE). Các thử nghiệm mở rộng trên DomainBed chứng minh
rằng GMoE được huấn luyện với ERM vượt trội hơn các baseline DG SOTA với biên độ lớn. Hơn nữa,
GMoE bổ sung cho các phương pháp DG hiện tại và hiệu suất của nó được cải thiện đáng kể khi được
huấn luyện với các thuật toán DG.

1 GIỚI THIỆU
1.1 ĐỘNG LỰC
Tổng quát hóa đến dữ liệu ngoài phân phối (OOD) là một khả năng bẩm sinh của thị giác con người,
nhưng rất thách thức đối với các mô hình học máy (Recht et al., 2019; Geirhos et al., 2021; Ma et al.,
2022). Tổng quát hóa miền (DG) là một cách tiếp cận để giải quyết vấn đề này, khuyến khích các mô
hình có khả năng phục hồi dưới các dịch chuyển phân phối khác nhau như nền, ánh sáng, kết cấu,
hình dạng và các thuộc tính địa lý/nhân khẩu học.

Từ góc độ học biểu diễn, có một số paradigm hướng đến mục tiêu này, bao gồm căn chỉnh miền
(Ganin et al., 2016; Hoffman et al., 2018), dự đoán nhân quả bất biến (Arjovsky et al., 2019;
Krueger et al., 2021), meta-learning (Bui et al., 2021; Zhang et al., 2021c), ensemble learning
(Mancini et al., 2018; Cha et al., 2021b), và phân tách đặc trưng (Wang et al., 2021; Zhang et al.,
2021b). Cách tiếp cận phổ biến nhất để thực hiện những ý tưởng này là thiết kế một hàm mất mát
cụ thể. Ví dụ, DANN (Ganin et al., 2016) căn chỉnh các phân phối miền bằng mất mát đối nghịch.
Dự đoán nhân quả bất biến có thể được thực thi bằng hình phạt của chuẩn gradient (Arjovsky et al.,
2019) hoặc phương sai của rủi ro huấn luyện (Krueger et al., 2021). Meta-learning và các hàm mất
mát đặc trưng miền (Bui et al., 2021; Zhang et al., 2021c) cũng được sử dụng để tăng cường hiệu
suất. Các nghiên cứu gần đây đã chỉ ra rằng những cách tiếp cận này cải thiện ERM và đạt được
kết quả đầy hứa hẹn trên các bộ dữ liệu DG quy mô lớn (Wiles et al., 2021).

Trong khi đó, trong các tác vụ thị giác máy tính khác nhau, những đổi mới trong kiến trúc cơ sở đóng
vai trò then chốt trong việc tăng hiệu suất và đã thu hút nhiều sự chú ý (He et al., 2016; Hu et al.,
2018; Liu et al., 2021). Ngoài ra, đã được chứng minh thực nghiệm trong Sivaprasad et al. (2021)
rằng các kiến trúc CNN khác nhau có hiệu suất khác nhau trên các bộ dữ liệu DG. Được truyền cảm
hứng bởi những công trình tiên phong này, chúng tôi phỏng đoán rằng thiết kế kiến trúc cơ sở sẽ đầy
hứa hẹn cho DG. Để xác minh trực giác này, chúng tôi đánh giá một mô hình dựa trên transformer
và so sánh nó với các kiến trúc dựa trên CNN có chi phí tính toán tương đương, như thể hiện trong
Hình 1(a). Đáng ngạc nhiên, một ViT-S/16 vanilla (Dosovitskiy et al., 2021) được huấn luyện với
tối thiểu hóa rủi ro thực nghiệm (ERM) vượt trội hơn ResNet-50 được huấn luyện với các thuật toán
DG SOTA (Cha et al., 2021b; Rame et al., 2021; Shi et al., 2021) trên các bộ dữ liệu DomainNet,
OfficeHome và VLCS, mặc dù cả hai kiến trúc đều có số lượng tham số tương tự và có hiệu suất gần
nhau trên các miền trong phân phối. Chúng tôi xác thực lý thuyết hiệu ứng này dựa trên khung căn
chỉnh thuật toán (Xu et al., 2020a; Li et al., 2021). Trước tiên, chúng tôi chứng minh rằng một mạng
được huấn luyện với hàm mất mát ERM có khả năng chống chịu tốt hơn với các dịch chuyển phân
phối nếu kiến trúc của nó tương tự hơn với tương quan bất biến, trong đó sự tương tự được đo
chính thức bằng giá trị căn chỉnh được định nghĩa trong Xu et al. (2020a). Ngược lại, một mạng
kém bền vững hơn nếu kiến trúc của nó căn chỉnh với tương quan giả mạo. Sau đó, chúng tôi nghiên
cứu sự căn chỉnh giữa các kiến trúc cơ sở (tức là convolution và attention) và các tương quan trong
những bộ dữ liệu này, điều này giải thích hiệu suất vượt trội của các phương pháp dựa trên ViT.

Để cải thiện hiệu suất hơn nữa, phân tích của chúng tôi chỉ ra rằng chúng ta nên khai thác các thuộc
tính của tương quan bất biến trong các tác vụ thị giác và thiết kế kiến trúc mạng để căn chỉnh với
những thuộc tính này. Điều này đòi hỏi một nghiên cứu nằm ở giao điểm của tổng quát hóa miền
và thị giác máy tính cổ điển. Trong tổng quát hóa miền, người ta tin rằng dữ liệu được tạo thành
từ một số tập hợp các thuộc tính và các dịch chuyển phân phối của dữ liệu là các dịch chuyển phân
phối của những thuộc tính này (Wiles et al., 2021). Mô hình phân tách tiềm ẩn của những thuộc tính
này gần như giống hệt với mô hình sinh của các thuộc tính thị giác trong thị giác máy tính cổ điển
(Ferrari & Zisserman, 2007). Để nắm bắt những thuộc tính đa dạng này, chúng tôi đề xuất một
Generalizable Mixture-of-Experts (GMoE), được xây dựng dựa trên sparse mixture-of-experts
(sparse MoEs) (Shazeer et al., 2017) và vision transformer (Dosovitskiy et al., 2021). Sparse MoEs
ban đầu được đề xuất như những yếu tố quan trọng cho các mô hình cực kỳ lớn nhưng hiệu quả
(Fedus et al., 2022). Thông qua bằng chứng lý thuyết và thực nghiệm, chúng tôi chứng minh rằng
MoEs là các chuyên gia cho việc xử lý các thuộc tính thị giác, dẫn đến sự căn chỉnh tốt hơn với
các tương quan bất biến. Dựa trên phân tích của chúng tôi, chúng tôi sửa đổi kiến trúc của sparse
MoEs để nâng cao hiệu suất của chúng trong DG. Các thí nghiệm mở rộng chứng minh rằng GMoE
đạt hiệu suất tổng quát hóa miền vượt trội cả có và không có thuật toán DG.

1.2 ĐÓNG GÓP
Trong bài báo này, chúng tôi chính thức nghiên cứu tác động của kiến trúc cơ sở đến DG và đề xuất
phát triển các phương pháp DG hiệu quả bằng thiết kế kiến trúc cơ sở. Cụ thể, những đóng góp chính
của chúng tôi được tóm tắt như sau:

Góc nhìn mới về DG: Trái với các công trình trước đây, bài báo này khởi xướng một khám phá chính
thức về kiến trúc cơ sở trong DG. Dựa trên căn chỉnh thuật toán (Xu et al., 2020a), chúng tôi chứng
minh rằng một mạng bền vững hơn với các dịch chuyển phân phối nếu kiến trúc của nó căn chỉnh
với tương quan bất biến, trong khi kém bền vững hơn nếu kiến trúc của nó căn chỉnh với tương quan
giả mạo. Các định lý được xác minh trên các bộ dữ liệu tổng hợp và thực tế.

Một mô hình mới cho DG: Dựa trên phân tích lý thuyết của chúng tôi, chúng tôi đề xuất
Generalizable Mixture-of-Experts (GMoE) và chứng minh rằng nó có sự căn chỉnh tốt hơn so với
vision transformer. GMoE được xây dựng dựa trên sparse mixture-of-experts (Shazeer et al., 2017)
và vision transformer (Dosovitskiy et al., 2021), với việc nâng cao hiệu suất được hướng dẫn bởi
lý thuyết cho DG.

Hiệu suất xuất sắc: Chúng tôi xác thực hiệu suất của GMoE trên tất cả 8 bộ dữ liệu quy mô lớn
của DomainBed. Đáng chú ý, GMoE được huấn luyện với ERM đạt hiệu suất SOTA trên 7 bộ dữ
liệu trong cài đặt train-validation và trên 8 bộ dữ liệu trong cài đặt leave-one-domain-out. Hơn nữa,
GMoE được huấn luyện với các thuật toán DG đạt hiệu suất tốt hơn so với GMoE được huấn luyện
với ERM.

2 KIẾN THỨC CƠ BẢN
2.1 KÝ HIỆU
Trong toàn bộ bài báo này, a, a, A lần lượt biểu thị một số vô hướng, một vector cột, một ma trận.
O(·) và Ω(·) là các ký hiệu tiệm cận. Chúng tôi ký hiệu bộ dữ liệu huấn luyện, phân phối huấn luyện,
bộ dữ liệu kiểm tra, và phân phối kiểm tra lần lượt là Etr, Dtr, Ete, và Dte.

2.2 PHÂN TÁCH THUỘC TÍNH
Phân tách thuộc tính (Wiles et al., 2021) là một mô hình sinh thực tế dưới các dịch chuyển phân phối.
Xem xét một phân phối chung của đầu vào x và các thuộc tính tương ứng a1, ..., aK (ký hiệu là
a1:K) với ai ∈ Ai, trong đó Ai là một tập hợp hữu hạn. Nhãn có thể phụ thuộc vào một hoặc nhiều
thuộc tính. Ký hiệu yếu tố tiềm ẩn là z, quá trình sinh dữ liệu được cho bởi:

z ∼ p(z); ai ∼ p(ai|z); x ∼ p(x|z); p(a1:K, x) = p(a1:K) ∫ p(x|z)p(z|a1:K)dz. (1)

Dịch chuyển phân phối xảy ra nếu các phân phối biên khác nhau của các thuộc tính được cho nhưng
chúng chia sẻ cùng một quá trình sinh có điều kiện. Cụ thể, chúng ta có ptrain(a1:K) ≠ ptest(a1:K),
nhưng mô hình sinh trong phương trình 1 được chia sẻ qua các phân phối, tức là chúng ta có
ptest(a1:K, x) = ptest(a1:K) ∫ p(x|z)p(z|a1:K)dz và tương tự cho ptrain. Mô tả trên khá trừu tượng
và chúng tôi sẽ minh họa bằng một ví dụ.

Ví dụ 1. (DSPRITES (Matthey et al., 2017)) Xem xét A1 = {đỏ, xanh lam} và A2 = {elip, vuông}.
Nhiệm vụ mục tiêu là phân loại hình dạng, trong đó nhãn phụ thuộc vào thuộc tính a2. Trong bộ dữ
liệu huấn luyện, 90% elip có màu đỏ và 50% hình vuông có màu xanh lam, trong khi trong bộ dữ
liệu kiểm tra tất cả các thuộc tính được phân phối đều. Vì phần lớn elip có màu đỏ, bộ phân loại
sẽ sử dụng màu sắc như một đường tắt trong bộ dữ liệu huấn luyện, được gọi là geometric skews
(Nagarajan et al., 2020). Tuy nhiên, đường tắt này không tồn tại trong bộ dữ liệu kiểm tra và mạng
không thể tổng quát hóa.

Trong thị giác máy tính cổ điển, các thuộc tính được gọi là thuộc tính thị giác và chúng tuân theo
một quá trình sinh dữ liệu tương tự (Ferrari & Zisserman, 2007). Chúng ta sẽ thảo luận chi tiết về
chúng trong Phần 4.2.

2.3 CĂNCCHỈNH THUẬT TOÁN
Trước tiên, chúng tôi giới thiệu căn chỉnh thuật toán, đặc trưng cho tính dễ dàng của các tác vụ
lý luận IID bằng cách đo sự tương tự giữa kiến trúc cơ sở và hàm mục tiêu. Sự căn chỉnh được
định nghĩa chính thức như sau.

Định nghĩa 1. (Căn chỉnh; (Xu et al., 2020a)) Cho N là một mạng nơ-ron với n mô-đun {Ni}^n_i=1
và giả sử rằng một hàm mục tiêu để học y = g(x) có thể được phân tách thành n hàm f1, ..., fn.
Mạng N căn chỉnh với hàm mục tiêu nếu thay thế Ni bằng fi, nó xuất ra cùng giá trị với thuật toán
g. Giá trị căn chỉnh giữa N và f được định nghĩa là:

Alignment(N, f; ε, δ) := n max_i M(fi, Ni; ε, δ), (2)

trong đó M(fi, Ni; ε, δ) biểu thị thước đo độ phức tạp mẫu để Ni học fi với độ chính xác ε tại
xác suất thất bại δ dưới một thuật toán học khi phân phối huấn luyện giống với phân phối kiểm tra.

Trong Định nghĩa 1, nhiệm vụ ban đầu là học f, đây là một vấn đề thách thức. Trực quan, nếu chúng
ta có thể tìm được một kiến trúc cơ sở phù hợp cho nhiệm vụ này, nó giúp chia nhiệm vụ ban đầu
thành các tác vụ phụ đơn giản hơn, tức là học f1, ..., fn thay thế. Dưới các giả định của căn chỉnh
thuật toán (Xu et al., 2020a), f có thể được học một cách tối ưu nếu các tác vụ phụ f1, ..., fn có
thể được học một cách tối ưu. Do đó, một sự căn chỉnh tốt làm cho nhiệm vụ mục tiêu dễ học hơn,
và do đó cải thiện khả năng tổng quát hóa IID, được đưa ra trong Định lý 3 trong Phụ lục B.1.
Trong Phần 3, chúng tôi mở rộng khung này cho cài đặt DG.

3 VỀ TẦM QUAN TRỌNG CỦA KIẾN TRÚC MẠNG NƠ-RON ĐỐI VỚI
TỔNG QUÁT HÓA MIỀN

Trong phần này, chúng tôi nghiên cứu tác động của kiến trúc cơ sở đến DG, từ một ví dụ thúc đẩy
đến một khung chính thức.

3.1 MỘT VÍ DỤ THÚC ĐẨY: CNNS VERSUS VISION TRANSFORMERS
Chúng tôi áp dụng DomainBed (Gulrajani & Lopez-Paz, 2021) làm benchmark, triển khai các thuật
toán DG SOTA với ResNet50 làm cơ sở. Chúng tôi kiểm tra hiệu suất của ViT được huấn luyện với
ERM trên benchmark này, không áp dụng bất kỳ phương pháp DG nào. Kết quả được thể hiện trong
Hình 1(a). Đáng ngạc nhiên, ViT được huấn luyện với ERM đã vượt trội hơn CNNs với các thuật
toán DG SOTA trên một số bộ dữ liệu, điều này chỉ ra rằng việc lựa chọn kiến trúc cơ sở có thể
quan trọng hơn hàm mất mát trong DG. Trong phần còn lại của bài viết này, chúng tôi sẽ có được
sự hiểu biết lý thuyết về hiện tượng này và cải thiện ViT cho DG bằng cách sửa đổi kiến trúc của nó.

3.2 HIỂU BIẾT TỪ QUAN ĐIỂM LÝ THUYẾT
Thí nghiệm trên dẫn đến một câu hỏi hấp dẫn: kiến trúc cơ sở tác động như thế nào đến hiệu suất
của mạng trong DG? Trong phần này, chúng tôi nỗ lực trả lời câu hỏi này bằng cách mở rộng khung
căn chỉnh thuật toán (Xu et al., 2020a) cho cài đặt DG.

Để có một phân tích có thể xử lý được cho việc xấp xỉ hàm phi tuyến, trước tiên chúng tôi đưa ra
một giả định về dịch chuyển phân phối.

Giả định 1. Ký hiệu N1 là mô-đun đầu tiên của mạng (bao gồm một hoặc nhiều lớp) của mạng.
Gọi ptrain,N1(s) và ptest,N1(s) lần lượt là các hàm mật độ xác suất của các đặc trưng sau N1.
Giả sử rằng support của phân phối đặc trưng huấn luyện bao phủ support của phân phối đặc trưng
kiểm tra, tức là max_s ptest,N1(s)/ptrain,N1(s) ≤ C, trong đó C là một hằng số độc lập với số lượng
mẫu huấn luyện.

Nhận xét 1. (Giải thích Giả định 1) Điều kiện này thực tế trong DG, đặc biệt khi chúng ta có một
mô hình được huấn luyện trước cho việc phân tách (ví dụ, trên DomainBed (Gulrajani & Lopez-Paz,
2021)). Trong Ví dụ 1, phân phối huấn luyện và phân phối kiểm tra có cùng support. Trong
DomainNet, mặc dù những con voi trong quickdraw khác biệt về mặt thị giác so với những con voi
trong các miền khác, các thuộc tính/đặc trưng của hình quickdraw (ví dụ, tai to và mũi dài) được
bao phủ trong các miền huấn luyện. Từ góc độ kỹ thuật, không thể cho các mạng được huấn luyện
với gradient descent xấp xỉ một loạt các hàm phi tuyến trong chế độ ngoài-support (Xu et al., 2020b).
Do đó, điều kiện này là cần thiết nếu chúng ta không áp đặt các ràng buộc mạnh về các hàm mục tiêu.

Chúng tôi định nghĩa một số khái niệm chính trong DG. Hàm mục tiêu là một tương quan bất biến
qua các bộ dữ liệu huấn luyện và kiểm tra. Để đơn giản, chúng tôi giả sử rằng các nhãn không có
nhiễu.

Giả định 2. (Tương quan bất biến) Giả sử tồn tại một hàm gc sao cho đối với dữ liệu huấn luyện,
chúng ta có gc(N1(x)) = y; ∀x ∈ Etr, và đối với dữ liệu kiểm tra, chúng ta có
P_Dte[||gc(N1(x)) - y||] > 1 - ε.

Sau đó, chúng tôi giới thiệu tương quan giả mạo (Wiles et al., 2021), tức là một số thuộc tính
tương quan với nhãn trong bộ dữ liệu huấn luyện nhưng không trong bộ dữ liệu kiểm tra. Tương
quan giả mạo chỉ tồn tại nếu phân phối huấn luyện khác với phân phối kiểm tra và điều này phân
biệt DG với các cài đặt PAC-learning cổ điển (Xu et al., 2020a).

Giả định 3. (Tương quan giả mạo) Giả sử tồn tại một hàm gs sao cho đối với dữ liệu huấn luyện
gs(N1(x)) = y; ∀x ∈ Etr, và đối với dữ liệu kiểm tra, chúng ta có
P_Dte[||gs(N1(x)) - y|| > Ω(ε)] > 1 - δ.

Định lý tiếp theo mở rộng căn chỉnh thuật toán từ khả năng tổng quát hóa IID (Định lý 3) sang DG.

Định lý 1. (Tác động của Kiến trúc Cơ sở trong Tổng quát hóa Miền) Ký hiệu N' = {N2, ..., Nn}.
Giả sử chúng ta huấn luyện mạng nơ-ron với ERM, và Giả định 1, 2, 3 đều đúng, chúng ta có các
phát biểu sau:

1. Nếu Alignment(N', gc; ε, δ) ≤ |Etr|, chúng ta có P_Dte[||N(x) - y|| ≤ O(ε)] > 1 - O(δ);
2. Nếu Alignment(N', gs; ε, δ) ≤ |Etr|, chúng ta có P_Dte[||N(x) - y|| > Ω(ε)] > 1 - O(δ).

Nhận xét 2. (Giải thích Định lý 1) Bằng cách chọn một ε đủ nhỏ, chỉ có một trong
Alignment(N', gc; ε, δ) ≤ |Etr| và Alignment(N', gs; ε, δ) ≤ |Etr| là đúng. Do đó, Định lý 1 cho
thấy rằng các mạng căn chỉnh với tương quan bất biến bền vững hơn với các dịch chuyển phân phối.
Trong Phụ lục B.2, chúng tôi xây dựng một bộ dữ liệu tổng hợp thỏa mãn tất cả các giả định. Kết
quả thực nghiệm hoàn toàn khớp với Định lý 1. Trong các bộ dữ liệu thực tế, các nhãn có thể có
nhiễu màu, phụ thuộc vào tương quan giả mạo. Trong những trường hợp như vậy, mạng nên dựa
vào nhiều tương quan để fit nhãn tốt và tương quan căn chỉnh tốt nhất với mạng sẽ có tác động
chính đến hiệu suất của nó. Vui lòng tham khảo Phụ lục B.1 để xem chứng minh.

ViT với ERM versus CNNs với thuật toán DG Bây giờ chúng tôi sử dụng Định lý 1 để giải thích
các thí nghiệm trong phần trước. Điều kiện đầu tiên của Định lý 1 cho thấy rằng nếu kiến trúc mạng
nơ-ron căn chỉnh với tương quan bất biến, ERM là đủ để đạt hiệu suất tốt. Trong một số miền của
OfficeHome hoặc DomainNet, thuộc tính hình dạng có tương quan bất biến với nhãn, được minh
họa trong Hình 1(b). Ngược lại, một tương quan giả mạo tồn tại giữa thuộc tính texture và nhãn.
Theo phân tích trong Park & Kim (2022), multi-head attention (MHA) là các bộ lọc thông thấp với
thiên hướng hình dạng trong khi convolution là các bộ lọc thông cao với thiên hướng texture. Do
đó, một ViT đơn giản được huấn luyện với ERM có thể vượt trội hơn CNNs được huấn luyện với
các thuật toán DG SOTA.

Để cải thiện hiệu suất của ViT, Định lý 1 gợi ý rằng chúng ta nên khai thác các thuộc tính của
tương quan bất biến. Trong nhận dạng hình ảnh, các đối tượng được mô tả bằng các phần chức năng
(ví dụ, thuộc tính thị giác), với các từ liên kết với chúng (Zhou et al., 2014). Cấu hình của các đối
tượng có một mức độ tự do lớn, dẫn đến các hình dạng khác nhau trong một danh mục. Do đó, các
phần chức năng quan trọng hơn hình dạng trong nhận dạng hình ảnh và chúng tôi sẽ phát triển
các kiến trúc cơ sở để nắm bắt chúng trong phần tiếp theo.

4 GENERALIZABLE MIXTURE-OF-EXPERTS CHO TỔNG QUÁT HÓA MIỀN

Trong phần này, chúng tôi đề xuất Generalizable Mixture-of-Experts (GMoE) cho tổng quát hóa
miền, được hỗ trợ bởi thiết kế kiến trúc mạng nơ-ron hiệu quả và phân tích lý thuyết.

4.1 LỚP MIXTURE-OF-EXPERTS
Trong phần này, chúng tôi giới thiệu lớp mixture-of-experts (MoE), là thành phần thiết yếu của
GMoE. Một lớp ViT được tạo thành từ một MHA và một FFN. Trong lớp MoE, FFN được thay thế
bởi mixture-of-experts và mỗi expert được thực hiện bởi một FFN (Shazeer et al., 2017). Ký hiệu
đầu ra của MHA là x, đầu ra của lớp MoE với N experts được cho bởi:

fMoE(x) = Σ^N_{i=1} G(x)_i E_i(x) = Σ^N_{i=1} TOP_k(Softmax(Wx)) σ(W^2_{FFN_i}(σ(W^1_{FFN_i}x))), (3)

trong đó W là tham số có thể học được cho gate, W^1_{FFN_i} và W^2_{FFN_i} là các tham số có thể
học được cho expert thứ i, σ(·) là hàm kích hoạt phi tuyến, và phép toán TOP_k(·) là một embedding
một-nóng đặt tất cả các phần tử khác trong vector đầu ra bằng không ngoại trừ các phần tử có k giá
trị lớn nhất trong đó k là một siêu tham số. Cho x_in là đầu vào của lớp MoE, việc cập nhật được
cho bởi:

x = f_MHA(LN(x_in)) + x_in; x_out = f_MoE(LN(x)) + x;

trong đó f_MHA là lớp MHA, LN biểu thị layer normalization, và x_out là đầu ra của lớp MoE.

4.2 THUỘC TÍNH THỊ GIÁC, LỆNH ĐIỀU KIỆN, VÀ SPARSE MOES

Trong dữ liệu hình ảnh thế giới thực, nhãn phụ thuộc vào nhiều thuộc tính. Việc nắm bắt các thuộc
tính thị giác đa dạng đặc biệt quan trọng cho DG. Ví dụ, định nghĩa về một con voi trong từ điển
Oxford là "một loài động vật rất lớn với da xám dày, tai to, hai răng cong bên ngoài gọi là ngà, và
một chiếc mũi dài gọi là vòi". Định nghĩa này bao gồm ba thuộc tính hình dạng (tức là tai to, răng
cong bên ngoài, và mũi dài) và một thuộc tính texture (tức là da xám dày). Trong tác vụ IID ImageNet,
sử dụng thuộc tính phân biệt nhất, tức là da xám dày, là đủ để đạt độ chính xác cao (Geirhos et al.,
2018). Tuy nhiên, trong DomainNet, những con voi không còn có da xám trong khi mũi dài và tai
to được bảo tồn và mạng dựa vào da xám sẽ thất bại trong việc tổng quát hóa.

Lệnh điều kiện (tức là IF/ELSE trong lập trình), như thể hiện trong Thuật toán 1, là một công cụ
mạnh mẽ để hiệu quả nắm bắt các thuộc tính thị giác và kết hợp chúng cho DG. Giả sử chúng ta
huấn luyện mạng để nhận dạng những con voi trên DomainNet, như minh họa trong hàng đầu tiên
của Hình 1(b). Đối với những con voi trong các miền khác nhau, hình dạng và texture khác nhau
đáng kể trong khi các thuộc tính thị giác (tai to, răng cong, mũi dài) bất biến qua tất cả các miền.
Được trang bị lệnh điều kiện, việc nhận dạng những con voi có thể được biểu đạt như "nếu một con
vật có tai to, hai răng cong bên ngoài, và mũi dài, thì nó là một con voi". Sau đó, các tác vụ phụ
là nhận dạng những thuộc tính thị giác này, cũng đòi hỏi lệnh điều kiện. Ví dụ, phép toán cho
"răng cong bên ngoài" là "nếu patch thuộc về răng, thì chúng ta áp dụng một bộ lọc hình dạng
lên nó". Trong tài liệu, lớp MoE được coi là một cách tiếp cận hiệu quả để thực hiện tính toán có
điều kiện (Shazeer et al., 2017; Riquelme et al., 2021). Chúng tôi hình thức hóa trực giác này trong
định lý tiếp theo.

Định lý 2. Một mô-đun MoE trong phương trình 3 với N experts và k = 1 căn chỉnh với các lệnh
điều kiện trong Thuật toán 1 với:

Alignment = {
    (N+1)max(M_P, M(G, h_1; ε, δ)), nếu N < M;
    (N+1)max(max_{i∈{1,...,M}} M(f_{FFN_i}, h_{i+1}; ε, δ), M(G, h_1; ε, δ)), nếu N ≥ M;
} (4)

trong đó M(·, ·; ε, δ) được định nghĩa trong Định nghĩa 1, và M_P là giá trị mục tiêu tối ưu của
bài toán tối ưu hóa sau:

P: minimize_{I_1,...,I_N} max_{i∈{1,...,N}} M(f_{FFN_i}, ([1_{I_j}]_{j∈I_i} h_1)^T [h_j]_{j∈I_i}; ε, δ)
subject to ∪^N_{i=1} I_i = {2, 3, ..., M+1}, (5)

trong đó 1_{I_j} là hàm chỉ thị trên khoảng I_j.

Nhận xét 3. (Giải thích Định lý 2) Trong căn chỉnh thuật toán, mạng căn chỉnh tốt hơn với thuật
toán nếu giá trị căn chỉnh trong phương trình 2 thấp hơn. Giá trị căn chỉnh giữa MoE và lệnh điều
kiện phụ thuộc vào tích của N + 1 và một số hạng độ phức tạp mẫu. Khi chúng ta tăng số lượng
experts N, giá trị căn chỉnh trước tiên giảm khi nhiều experts phân tách các lệnh điều kiện ban đầu
thành nhiều tác vụ đơn giản hơn. Khi chúng ta tiếp tục tăng N, giá trị căn chỉnh tăng lên do yếu tố
N + 1 trong tích. Do đó, MoE căn chỉnh tốt hơn với lệnh điều kiện so với FFN ban đầu (tức là
N = 1). Ngoài ra, để tối thiểu hóa phương trình 5, các lệnh điều kiện tương tự nên được nhóm lại
với nhau. Thông qua các thí nghiệm trong Phần 5.4 và Phụ lục E.1, chúng tôi thấy rằng các lớp
sparse MoE thực sự là các chuyên gia cho các thuộc tính thị giác, và các thuộc tính thị giác tương
tự được xử lý bởi một expert. Vui lòng tham khảo Phụ lục B.3 để xem chứng minh.

4.3 ĐIỀU CHỈNH MOE CHO TỔNG QUÁT HÓA MIỀN
Trong tài liệu, có một số biến thể của kiến trúc MoE, ví dụ, Riquelme et al. (2021); Fedus et al.
(2022), và chúng ta nên xác định một biến thể cho DG. Bằng căn chỉnh thuật toán, để đạt được khả
năng tổng quát hóa tốt hơn, kiến trúc của sparse MoEs nên được thiết kế để xử lý hiệu quả các
thuộc tính thị giác. Trong phần sau, chúng tôi thảo luận về thiết kế kiến trúc của chúng tôi cho mục
đích này.

Sơ đồ định tuyến Router tuyến tính (tức là phương trình 3) thường được áp dụng trong MoEs cho
các tác vụ thị giác (Riquelme et al., 2021) trong khi các nghiên cứu gần đây trong NLP cho thấy
rằng cosine router đạt hiệu suất tốt hơn trong các tác vụ ngôn ngữ đa ngôn ngữ (Chi et al., 2022).
Đối với cosine router, cho đầu vào x ∈ R^d, embedding Wx ∈ R^{d_e} trước tiên được chiếu lên
một siêu cầu, sau đó nhân với một embedding đã học E ∈ R^{d_e×N}. Cụ thể, biểu thức cho gate
được cho bởi:

G(x) = TOP_k(Softmax(τ E^T Wx / (||Wx|| ||E||))),

trong đó τ là một siêu tham số. Từ góc độ xử lý hình ảnh, E có thể được hiểu là cookbook cho các
thuộc tính thị giác (Ferrari & Zisserman, 2007; Zhou et al., 2014) và tích vô hướng giữa E và Wx
với chuẩn hóa ℓ2 là một bộ lọc khớp. Chúng tôi cho rằng router tuyến tính sẽ gặp khó khăn trong
DG. Ví dụ, hình ảnh voi (và tất cả các patch của nó) trong miền Clipart có khả năng tương tự hơn
với các hình ảnh khác trong miền Clipart hơn là trong các miền khác. Vấn đề có thể được giảm nhẹ
với một codebook cho các thuộc tính thị giác và các bộ lọc khớp để phát hiện chúng. Vui lòng tham
khảo Phụ lục D.6 để xem nghiên cứu ablation.

Số lượng lớp MoE Every-two và last-two là hai phương pháp đặt vị trí thường được áp dụng trong
các nghiên cứu MoE hiện tại (Riquelme et al., 2021; Lepikhin et al., 2021). Cụ thể, every-two có
nghĩa là thay thế FFN của lớp chẵn bằng MoE, và last-two có nghĩa là đặt MoE ở hai lớp chẵn cuối
cùng. Đối với khả năng tổng quát hóa IID, every-two thường vượt trội hơn last-two (Riquelme et al.,
2021). Chúng tôi lập luận rằng last-two phù hợp hơn cho DG vì các câu điều kiện để xử lý các
thuộc tính thị giác là cấp cao. Từ thí nghiệm, chúng tôi phát hiện thực nghiệm rằng last-two đạt
hiệu suất tốt hơn so với every-two với ít tính toán hơn. Vui lòng tham khảo Phụ lục C.1 để biết
thêm thảo luận và Phụ lục D.6 để xem nghiên cứu ablation.

Kiến trúc cơ sở tổng thể của GMoE được thể hiện trong Hình 2. Để huấn luyện các experts đa dạng,
chúng tôi áp dụng perturbation trick và load balance loss như trong Riquelme et al. (2021). Do
giới hạn không gian, chúng tôi để chúng trong Phụ lục C.4.

5 KẾT QUẢ THỰC NGHIỆM

Trong phần này, chúng tôi đánh giá hiệu suất của GMoE trên các bộ dữ liệu DG quy mô lớn và trình
bày phân tích mô hình để hiểu GMoE.

5.1 KẾT QUẢ DOMAINBED
Trong phần này, chúng tôi đánh giá GMoE trên DomainBed (Gulrajani & Lopez-Paz, 2021) với 8
bộ dữ liệu benchmark: PACS, VLCS, OfficeHome, TerraIncognita, DomainNet, SVIRO, Wilds-
Camelyon và Wilds-FMOW. Thông tin chi tiết về các bộ dữ liệu và giao thức đánh giá được cung
cấp trong Phụ lục D.1. Các thí nghiệm được tính trung bình qua 3 lần chạy như được gợi ý trong
(Gulrajani & Lopez-Paz, 2021).

Chúng tôi trình bày kết quả trong Bảng 1 với tiêu chí lựa chọn train-validation, bao gồm các
phương pháp baseline và thuật toán DG gần đây cùng với GMoE được huấn luyện với ERM. Kết
quả chứng minh rằng GMoE không có thuật toán DG đã vượt trội hơn các đối thủ trên hầu hết tất
cả các bộ dữ liệu. Trong khi đó, GMoE có hiệu suất xuất sắc trong tiêu chí leave-one-domain-out,
và chúng tôi để kết quả trong Phụ lục D.3 do giới hạn không gian. Trong phần dưới của Bảng 1,
chúng tôi kiểm tra các phương pháp của chúng tôi trên ba bộ dữ liệu quy mô lớn: SVIRO, Wilds-
Camelyon, và Wilds-FMOW. Ba bộ dữ liệu này nắm bắt các dịch chuyển phân phối thế giới thực
qua một loạt các miền đa dạng. Chúng tôi áp dụng tiền xử lý dữ liệu và phân chia miền trong
DomainBed. Vì không có nghiên cứu trước đây tiến hành thí nghiệm trên những bộ dữ liệu này với
tiêu chí DomainBed, chúng tôi chỉ báo cáo kết quả của các phương pháp của chúng tôi, cho thấy
rằng GMoE vượt trội hơn hai baseline khác.

5.2 GMOE VỚI THUẬT TOÁN DG
Khả năng tổng quát hóa của GMoE đến từ kiến trúc cơ sở nội tại của nó, điều này trực giao với
các thuật toán DG hiện tại. Điều này ngụ ý rằng các thuật toán DG có thể được áp dụng để cải thiện
hiệu suất của GMoE. Để xác thực ý tưởng này, chúng tôi áp dụng hai thuật toán DG cho GMoE,
bao gồm một cách tiếp cận sửa đổi hàm mất mát (Fish) và một cách tiếp cận áp dụng model ensemble
(Swad). Kết quả trong Bảng 2 chứng minh rằng việc áp dụng GMoE thay vì ResNet-50 mang lại
sự cải thiện độ chính xác đáng kể cho các thuật toán DG này. Thí nghiệm về GMoE với nhiều thuật
toán DG hơn được trình bày trong Phụ lục D.4.

5.3 KẾT QUẢ TỔNG QUÁT HÓA MIỀN NGUỒN ĐƠN
Trong phần này, chúng tôi tạo ra một tác vụ thách thức, tổng quát hóa miền nguồn đơn, để tập trung
vào khả năng tổng quát hóa của kiến trúc cơ sở. Cụ thể, chúng tôi chỉ huấn luyện mô hình trên dữ
liệu từ một miền, sau đó kiểm tra mô hình trên nhiều miền để xác thực hiệu suất của nó qua tất cả
các miền. Đây là một tác vụ thách thức vì chúng ta không thể dựa vào nhiều miền để xác định các
tương quan bất biến, và các thuật toán DG phổ biến không thể được áp dụng. Chúng tôi so sánh
một số mô hình được đề cập trong phân tích trên (ví dụ, ResNet, ViT, GMoE) với quy mô tham số
khác nhau, bao gồm các phép toán dấu phẩy động trên giây (flops), độ chính xác IID và OOD của
chúng. Từ kết quả trong Bảng 3, chúng ta thấy rằng lợi ích tổng quát hóa OOD của GMoE so với
ResNet hoặc ViT lớn hơn nhiều so với trong cài đặt IID, điều này cho thấy rằng GMoE phù hợp
cho tổng quát hóa miền thách thức. Do giới hạn không gian, chúng tôi để các thí nghiệm với các
miền huấn luyện khác trong Phụ lục D.5.

5.4 PHÂN TÍCH MÔ HÌNH
Trong phần này, chúng tôi trình bày các bộ dữ liệu chẩn đoán để nghiên cứu các kết nối giữa lớp
MoE và các thuộc tính thị giác.

Bộ dữ liệu chẩn đoán: CUB-DG Chúng tôi tạo CUB-DG từ bộ dữ liệu Caltech-UCSD Birds (CUB)
ban đầu (Wah et al., 2011). Chúng tôi stylize các hình ảnh ban đầu thành ba miền khác, Candy,
Mosaic và Udnie. Các ví dụ về CUB-DG được trình bày trong Hình 3(a). Chúng tôi đánh giá GMoE
và các thuật toán DG khác giải quyết biểu diễn bất biến miền (ví dụ, DANN). Kết quả trong Phụ lục
E.1 chứng minh hiệu suất vượt trội của GMoE so với các thuật toán DG khác. Bộ dữ liệu CUB-DG
cung cấp các thuộc tính thị giác phong phú (ví dụ, hình dạng mỏ, màu bụng) cho mỗi hình ảnh.
Thông tin bổ sung đó cho phép chúng tôi đo lường mối tương quan giữa các thuộc tính thị giác và
việc lựa chọn expert của router.

Tương quan Thuộc tính Thị giác & Experts Chúng tôi chọn GMoE-S/16 với 6 experts trong mỗi
lớp MoE. Sau khi huấn luyện mô hình trên CUB-DG, chúng tôi thực hiện forward pass với các hình
ảnh huấn luyện và lưu lựa chọn top-1 của router. Vì mô hình MoE định tuyến các patch thay vì
hình ảnh và CUB-DG đã cung cấp vị trí của các thuộc tính thị giác, trước tiên chúng tôi ghép một
thuộc tính thị giác với 9 patch gần nhất của nó, sau đó tương quan thuộc tính thị giác và 9 patch
hiện tại của nó với các experts. Trong Hình 3(b), chúng tôi cho thấy histogram 2D về tương quan
giữa các experts được chọn và các thuộc tính. Không có bất kỳ tín hiệu giám sát nào cho các thuộc
tính thị giác, khả năng tương quan các thuộc tính tự động xuất hiện trong quá trình huấn luyện. Cụ
thể, 1) mỗi expert tập trung vào các thuộc tính thị giác riêng biệt; và 2) các thuộc tính tương tự
được chú ý bởi cùng một expert (ví dụ, cánh trái và cánh phải đều được chú ý bởi e4). Điều này
xác minh các dự đoán bởi Định lý 2.

Lựa chọn Expert Để hiểu rõ hơn về việc lựa chọn expert của toàn bộ hình ảnh, chúng tôi ghi lại
lựa chọn của router cho mỗi patch (trong GMoE-S/16, chúng tôi xử lý một hình ảnh thành 16×16
patches), và sau đó trực quan hóa việc lựa chọn top-1 cho mỗi patch trong Hình 3(c). Trong hình
ảnh này, chúng tôi đánh dấu expert được chọn cho mỗi patch bằng một số màu đen và vẽ các thuộc
tính thị giác khác nhau của con chim (ví dụ, loại mỏ và đuôi) bằng các vòng tròn lớn. Chúng ta
thấy mối quan hệ nhất quán giữa Hình 3(b) và Hình 3(c). Chi tiết, chúng ta thấy (1) experts 0 và
2 được định tuyến nhất quán với các patch trong vùng nền; (2) các vùng cánh trái, cánh phải, mỏ
và đuôi được chú ý bởi expert 3; (3) các vùng chân trái và chân phải được chú ý bởi expert 4. Nhiều
ví dụ hơn được đưa ra trong Phụ lục E.1.

6 KẾT LUẬN

Bài báo này là một bước đầu tiên trong việc khám phá tác động của kiến trúc cơ sở trong tổng quát
hóa miền. Chúng tôi đã chứng minh rằng một mạng bền vững hơn với các dịch chuyển phân phối
nếu kiến trúc của nó căn chỉnh tốt với tương quan bất biến, điều này được xác minh trên các bộ dữ
liệu tổng hợp và thực tế. Dựa trên phân tích lý thuyết của chúng tôi, chúng tôi đã đề xuất GMoE
và chứng minh hiệu suất vượt trội của nó trên DomainBed. Đối với các hướng nghiên cứu tương
lai, sẽ thú vị khi phát triển các kiến trúc cơ sở mới cho DG dựa trên căn chỉnh thuật toán và thị
giác máy tính cổ điển.
