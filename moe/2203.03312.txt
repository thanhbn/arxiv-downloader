# 2203.03312.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2203.03312.pdf
# File size: 1111229 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SkillNet-NLU: A Sparsely Activated Model for General-Purpose Natural
Language Understanding
Fan Zhang, Duyu Tang, Yong Dai, Cong Zhou, Shuangzhi Wu and Shuming Shi
Tencent AI Lab
Abstract
Prevailing deep models are single-purpose and
overspecialize at individual tasks. However,
when being extended to new tasks, they typi-
cally forget previously learned skills and learn
from scratch. We address this issue by in-
troducing SkillNet-NLU, a general-purpose
model that stitches together existing skills to
learn new tasks more effectively. The key fea-
ture of our approach is that it is sparsely ac-
tivated guided by predeﬁned skills. Different
from traditional dense models that always ac-
tivate all the model parameters, SkillNet-NLU
only activates parts of the model parameters
whose skills are relevant to the target task.
When learning for a new task, our approach
precisely activates required skills and also pro-
vides an option to add new skills. We evalu-
ate on natural language understandings tasks
and have the following ﬁndings. First, with
only one model checkpoint, SkillNet-NLU
performs better than task-speciﬁc ﬁne-tuning
and two multi-task learning baselines (i.e.,
dense model and Mixture-of-Experts model)
on six tasks. Second, sparsely activated pre-
training further improves the overall perfor-
mance. Third, SkillNet-NLU signiﬁcantly
outperforms baseline systems when being ex-
tended to new tasks.
1 Introduction
Recent years have witnessed the success of homo-
geneous models based on Transformer (Vaswani
et al., 2017) and pre-trained models (Devlin et al.,
2018) in artiﬁcial intelligence and natural language
processing. Many previous works use similar neu-
ral network models and repeat the same process:
learning from scratch1and ﬁne-tuning all the model
parameters for an isolated task. However, this dif-
fers from human learning in two aspects. First,
Correspondence to Duyu Tang (duyutang@tencent.com).
1In this work, the terminology “from scratch” refers to the
unawareness of task knowledge, even if the model is initialized
with pre-trained models like BERT (Devlin et al., 2018).we human beings don’t forget everything we have
learned and start learning new skills from nothing.
Instead, we combine existing skills to learn new
skills faster. Second, we have about 100 billion
neurons in our brain and different parts are special-
ized for different skills. When we solve a problem,
we don’t activate all the neurons but only call on
relevant parts.
In this work, we present an approach to ad-
dress the aforementioned issues. Our goal is to
advance from single-purpose models to general-
purpose models and from dense models to sparse
models. Speciﬁcally, we take natural language un-
derstanding (NLU) as a case study and present a
sparsely activated model that is capable of gen-
eralizing across many different NLU tasks. The
key feature of our approach is that it includes a set
of reusable parameterized “skill modules”, each
of which corresponds to a skill such as the skill
to understand the sentiment of texts ,the skill to
understand natural language questions ,the skill
to understand the meaning of texts in ﬁnance do-
main , etc. Different from traditional dense models
that always activate all the model parameters, our
approach sparsely activates parts of the model pa-
rameters, while deactivating the modules whose
skills are irrelevant to the task.
Let’s use three concrete examples to illustrate
how our model is sparsely activated when it is
adopted in downstream tasks. Let’s suppose we
have deﬁned seven skills, whose deﬁnitions are
given in Table 1. For the task of text classiﬁcation,
only the ability to get the semantic representation
of a sequence (i.e., s1) is required. Therefore, only
the parameters that relate to s1ands7are acti-
vated, as shown in Figure 1 (a)2. Compared to text
classiﬁcation, sentiment classiﬁcation requires an
additional skill to understand the sentiment of texts.
2We deﬁne a generic skill s7, which is always activated
as the default skill. This design aims to provide a backup for
handling new tasks that require totally unseen skills.
1arXiv:2203.03312v4  [cs.CL]  9 May 2022

--- PAGE 2 ---
  ading Comprehension
7activated)
L1L2
inputL12output
…(a) SkillNet-NLU for text classiﬁcation. s1 and s7 activated.
 ne Reading Comprehension
 and s7activated)
L1L2
inputL12output
… (b) SkillNet-NLU for sentiment classiﬁcation. s1, s4, s7
activated.
 ne Reading Comprehension
 and s7activated)
L1L2
inputL12output
…
(c) SkillNet-NLU for machine reading comprehension. s2,
s3, s5 and s7 activated.
 ne Reading Comprehension
 and s7activated)
L1L2
inputL12output
…(d) Mixture of experts.
Figure 1: Illustrative examples of our SkillNet-NLU for NLU tasks and the comparison to a fully activated MoE
model. In SkillNet-NLU (a, b and c), each pillar is a skill module. Pillars ﬁlled in color (e.g., yellow, green, purple,
blue, red and brown) are activated. Skills are deﬁned in Table 1.
Therefore, s1,s4ands7are activated, as given in
Figure 1 (b). For the task of machine reading com-
prehension, models need to understand the mean-
ing of the question ( s5), understand how question
and passage interact ( s3) and get the representa-
tion of each token ( s2). Therefore, s2,s3,s5
ands7are activated, as shown in Figure 1 (c).
We brieﬂy summarize how SkillNet-NLU dif-
fers from both multi-task learning methods and
Mixture-of-Experts (MoE) methods as follows.1.Multi-task learning methods (Liu et al., 2019)
typically have one shared feature representa-
tion layer (e.g., Transformer) plus multiple
task-speciﬁc prediction layers. It is unclear
what types of knowledge or skills are learned
in the feature representation layer. Unlike
multi-task learning methods, SkillNet-NLU
includes multiple skill modules with clear def-
initions. Skill modules are sparsely activated
depending on the necessity to the task. Intu-
itively, SkillNet-NLU does not overspecialize
2

--- PAGE 3 ---
Skill Description
s1 get the semantic meaning of a sequence
s2 get the semantic meaning of a token
s3 understand how two text segments interact
s4 understand the sentiment of texts
s5 understand natural language questions
s6 understand texts in ﬁnance domain
s7 generic skill
Table 1: Examples of skills and descriptions.
at the task level, but at an inherent skill level
through learning how each skill module works
and how multiple skill modules are combined
to tackle problems. We believe SkillNet-NLU
generalizes better to new tasks with unfore-
seen task deﬁnitions in the future.
2.MoE methods typically include multiple ho-
mogeneous neural modules (called experts)
in parallel, as given in Figure 1 (d), and fully
activate all the experts or partially activate a
part of experts guided by an additional param-
eterized gating module (Shazeer et al., 2017;
Lepikhin et al., 2020; Fedus et al., 2021; Du
et al., 2021). However, what type of knowl-
edge is learned in each expert is vague and
why some experts are activated is not inter-
pretable.3In SkillNet-NLU, the deﬁnition of
each skill module is clear and the reason for
a skill module being activated is that the skill
is necessary (judged by human developers or
users) to solve the task.
We use Transformer (Vaswani et al., 2017) and
BERT (Devlin et al., 2018) as the backbone to de-
velop our system. Transformer is a commonly used
model architecture with multiple layers and each
layer is composed of a multi-head attention net-
work followed by a feed-forward neural network
(FFN). There are many different ways to imple-
ment SkillNet-NLU, and our goal is to demonstrate
that a simple implementation works well in prac-
tice. Speciﬁcally, we implement skill modules as
homogeneous FFN networks. A skill module is
activated only if the skill is relevant to the task at
hand. Our model not only supports sparsely acti-
vated ﬁne-tuning, but also can be pre-trained in the
same sparse way through masked language model-
ing and next sentence prediction.
3An exception is a recent work on machine translation
where experts are selected based on the target language or
language pair (Kudugunta et al., 2021).We conduct experiments on Chinese natural lan-
guage understanding tasks. Experimental results
on six tasks (including sentiment classiﬁcation, nat-
ural language inference, semantic similarity, text
classiﬁcation, named entity recognition and ma-
chine reading comprehension) show that, with only
one model checkpoint, our approach performs bet-
ter than task-speciﬁc ﬁne-tuning and two multi-task
learning baselines: a dense model and a Mixture-
of-Experts model. Furthermore, after being pre-
trained with the same sparse manner, the overall
performance is further boosted. More importantly,
we show that when being extended to new tasks,
our approach signiﬁcantly outperforms baseline
systems.
2 Background
We give brief backgrounds on BERT and the stan-
dard BERT-based multi-task learning baseline.
BERT is a Transformer-based encoder (Vaswani
et al., 2017). It is usually used in a pre-training
and ﬁne-tuning framework. Model parameters are
ﬁrst pre-trained on a vast amount of unlabeled text
data with self-supervised objectives (e.g., masked
language modeling and next sentence prediction).
Then, for each downstream task, the pre-trained
model parameters are further ﬁne-tuned on each
task-speciﬁc data separately. If there are Ndown-
stream tasks, a standard solution would produce
NBERT models, each of which corresponds to a
particular task.
Since the smallest BERT model still has hun-
dreds of millions of parameters, an efﬁcient way
of avoiding deploying multiple copies of big mod-
els in practice is to train one multi-task model to
support multiple downstream tasks. A standard
multi-task method (Liu et al., 2019) appends dif-
ferent task-speciﬁc prediction layers on top of a
shared Transformer layer. In the training stage, all
tasks are optimized jointly. Intuitively, the Trans-
former layer learns the generic feature representa-
tions and each prediction layer learns to accomplish
a particular task. In practice, conducting the sec-
ond round of task-speciﬁc ﬁne-tuning, namely ﬁne-
tuning model parameters for each task separately
(i.e., producing Nmodels for Ntasks), might
produce higher accuracy. However, this contra-
dicts to our motivation of developing one general-
purpose model across multiple tasks. Therefore,
we don’t conduct the second round of task-speciﬁc
ﬁne-tuning in our experiments.
3

--- PAGE 4 ---
3 SkillNet-NLU
This section gives our SkillNet-NLU and its appli-
cation to natural language understanding tasks. We
ﬁrst describe the model architecture (§3.1). Then,
we present the tasks used for model training (§3.2),
how to do multi-task training with SkillNet-NLU
(§3.3) and how to extend the model to new tasks
(§3.4). Finally, we show how the model can be pre-
trained with model parameters sparsely activated
using traditional self-supervised learning objectives
(i.e., masked language modeling and next sentence
prediction) (§3.5).
3.1 Model Architecture
There are many different ways to implement our
SkillNet-NLU. The goal of this work is to demon-
strate that a simple and intuitive implementation of
the idea works well in practice, and we leave the
exploration of more advanced model architectures
in the future. Speciﬁcally, we build our SkillNet-
NLU using Transformer (Vaswani et al., 2017)
and BERT (Devlin et al., 2018) as the backbone.
Since both Transformer and BERT are ubiquitously
adopted in natural language processing tasks, we
don’t elaborate on the details and refer readers to
the original papers.
Transformer is a commonly used model archi-
tecture with multiple layers and each layer is
composed of a multi-head attention network fol-
lowed by a feed-forward neural network (FFN).
Our model architecture modiﬁes each of the Trans-
former layers and adds task-speciﬁc prediction lay-
ers on top of the representations of the last layer.
In Transformer, as given in Figure 4 (a), each
layer includes a multi-head attention network fol-
lowed by a feed-forward neural network (FFN).
In SkillNet-NLU, as shown in Figure 4 (b), we
have a set of FFN layers in parallel, each of which
stands for one particular skill (e.g., s1from Ta-
ble 2). When being applied to one task, only the
FFN layers corresponding to relevant skills are acti-
vated. For example, for the task of machine reading
comprehension, only s2,s3,s5ands7are rele-
vant, so the remaining FFN layers (i.e., s1,s4and
s6) are not activated. Considering that the number
of activated skills is variable, we accumulate the
output vectors of activated skill FFN layers with
average pooling. The remaining operations are the
same as the standard Transformer.
Speciﬁcally, given a sequence of input x=
fx1;:::;xng, our model ﬁrst performs multi-headself-attention for each token. Then, each skill mod-
ule FNNkfrom the set of activated skills Sobtains
skill-speciﬁc representations as follows,
hk=FNNk(Self-Attention (fx1;:::;xng));(1)
wherek2[1;jSj]indicates the k-th activated skill
module inS. For instance, for the task of machine
reading comprehension, as shown in Figure 4 (c),
jSj= 4 andS=fs2;s3;s5;s7g. Finally, we
adopt average-pooling over all the skill-speciﬁc
representations to compute the output embeddings
of words as follows,
v=AvgPool (h1;:::;hjSj): (2)
The aforementioned operations are performed for
multiple rounds. The embedding of each token
produced by the last layer is considered as the ﬁnal
feature representation.
3.2 Tasks
We use six NLU tasks as given in Table 2 to train
our multi-task model.
T1is sentiment classiﬁcation. Given a text se-
quence (e.g., a sentence) as the input, the output is
the polarity of the input. We consume the vector
of[CLS]to a softmax layer to conduct binary clas-
siﬁcation (i.e., positive v.s. negative). T4has the
similar conﬁguration. We activate s4additionally
forT2because it requires the skill of understanding
the sentiment in the texts.
T2is natural language inference. Given two text
sequences as the input, the output is the relation be-
tween two sequences as entailment, contradiction,
or neutral. We concatenate two input segments with
a[SEP]token and consume the vector of [CLS]to a
softmax layer. T3has the analogous conﬁguration.
s6is activated in T3because its data source comes
from the ﬁnance domain.
T5is named entity recognition. Given a se-
quence of words as the input, the task is detecting
whether each word is a named entity, and if yes,
predicting the entity type (e.g., person, organiza-
tion, location, etc.). We take the representations
of each word from the last layer and feed them to
Conditional Random Fields (CRF) (Lafferty et al.,
2001) to predict labels for words.
T6is machine reading comprehension. Given a
question and a passage as the input, the task is to
predict a span from the passage that answers the
question. The input of the model is the concate-
nation of the question and the passage, separated
4

--- PAGE 5 ---
Task Id TaskSkillsDataset
s1 s2 s3 s4 s5 s6 s7
T1 Sentiment Analysis X X X ChnSentiCorp (9.6k / 1.2k)
T2 Natural Language Inference X X X OCNLI (50k / 3k)
T3 Semantic Similarity X X X X AFQMC (34.3k / 4.3k)
T4 Text Classiﬁcation X X TNEWS (53.3k / 10k)
T5 Named Entity Recognition X X OntoNotes (15.7k / 4.3k)
T6 Machine Reading Comprehension X X X X CMRC 2018 (10k / 3.4k)
Table 2: Tasks and datasets used to train the multi-task model. Relevant skills (deﬁned in Table 1) for each dataset
is marked with a tick. The numbers of training and evaluation instances in each dataset are given in parentheses.
 ining with MLM
activated)
L1L2
inputL12output
…
(a) pre-training with masked language modeling. s2 and s7
activated.
 ining with NSP
 s7activated)
L1L2
inputL12output
…(b) pre-training with next sentence prediction. s1, s3, s7
activated.
Figure 2: An illustration of how our SkillNet-NLU is pre-trained with masked language modeling and next sentence
prediction. The model is sparsely activated during pre-training. Skills are deﬁned in Table 1.
with a [SEP]token. We take the representations of
words from the passage and predict whether each
of them is the starting index or the ending index of
the answer. Speciﬁcally, we introduce a start vector
vstart and an end vector vend. When predicting the
probability of a token being the start of the answer
span, we perform dot product between its vector
andvstart followed by softmax over all of the to-
kens in the paragraph. The analogous formula is
used for predicting the ending index.
3.3 Model Training
The overall training objective is to minimize the
sum of the losses of all tasks. Speciﬁcally, the
model is trained on the concatenation of training
samples from these tasks. In each iteration, a mini-
batch is selected from one task, and the model pa-
rameters are updated according to the task-speciﬁcobjective. We sample mini-batches from the N= 6
tasks according to a multinomial distribution with
probabilitiesfqigi=1:::N:
qi=p
iPN
j=1p
jwithpi=jTijPN
j=1jTjj; (3)
wherejTijindicates the number of training samples
in taskTi.
The sampling rate is a hyper-parameter to bal-
ance various tasks. If = 0:0,qi=1
N. Each task
is selected by the equal chance. Sampling with this
distribution increases the number of samples asso-
ciated with tasks with small size and alleviates the
bias towards high-resource tasks. If = 1:0, the
natural distribution of the tasks will be maintained
and low-resource tasks are not up-sampled. We set
the sampling rate = 1:0in experiments. Analysis
on the inﬂuence of is given in subsection 5.2.
5

--- PAGE 6 ---
3.4 Adaptation to New Tasks
We describe the adaptation of a well-trained multi-
task SkillNet-NLU to new tasks. We consider two
situations here, depending on whether new skills
are required to tackle the new task.
The ﬁrst situation is that existing skills consid-
ered in the multi-task training stage are sufﬁcient
to tackle the new task. Consider the new task of
open domain question answering that determines
whether a sentence from the given documents an-
swers the question. Despite exactly the same task
is unseen in the training stage, the relevant skills
(i.e., the skill of getting the semantic representation
of a sequence ( s1), the skill of understanding ques-
tion (s5) and the skill of understanding how two
segments interact ( s3)) are seen during multi-task
training. Therefore, we use the standard framework
that only activates relevant skills to tune model pa-
rameters for the new task.
The second situation is that the new task may
need new skills that are unseen in the multi-task
training stage. For example, the task of Chinese
medical question-answer matching may require an
additional skill of understanding texts in the medi-
cal domain, which is unseen in the multi-task train-
ing stage. Our model supports two ways to learn
for such new tasks. One way is to keep the num-
ber of skills unchanged and, intuitively, learn the
unseen skills (like medical text understanding) in
the general skill ( s7). Another way is to add a
new skill ( s8), that is activated together with other
activated skills to learn for the new task.
3.5 Sparse Pre-training
In this part, we show how the parameters of
SkillNet-NLU can be pre-trained with model pa-
rameters being sparsely activated. We adopt two
standard self-supervised learning objectives (De-
vlin et al., 2018) including masked language mod-
eling (MLM) and next sentence prediction (NSP).
To be speciﬁc, we activate two skills SMLM =
fs2;s7gfor the MLM task. For the NSP task,
three skillsSNSP =fs1;s3;s7gare activated.
We sampled the two tasks with the equal chance
and the overall learning objective is to minimize the
sum of the two losses. We refer readers to Devlin
et al. (2018) for the details of these two pre-training
tasks. After being pre-trained, the parameters of
pre-trained skills can be used to initialize the multi-
task model.4 Experiments
This section is organized as follows. We ﬁrst de-
scribe experiment settings (§4.1), and then report
results on multiple tasks (§4.2). Then, we present
results on two new tasks (§4.3).
4.1 Experimental Setup
Datasets We conduct multi-task training on six
Chinese natural language understanding datasets to
evaluate the performance of the models.
ChnSentiCorp (Tan, 2012) is a sentiment anal-
ysis dataset, where the text should be classiﬁed into
either a positive or negative label. OCNLI (Hu
et al., 2020) is a large-scale Chinese NLI dataset,
which requires to predict the relation of premise-
hypothesis pairs. The labels contain contradiction,
neutral and entailment. AFQMC (Xu et al., 2020)
is a binary classiﬁcation dataset from the ﬁnan-
cial domain, which aims to predict whether two
sentences are semantically similar. TNEWS (Xu
et al., 2020) is a short text classiﬁcation dataset con-
sisting of news titles, which requires to classify into
one of 15 classes. OntoNotes (Weischedel et al.,
2013) is designed for named entity recognition.
The entities contain several types including person,
organization and location, etc. CMRC 2018 (Cui
et al., 2019) is a span-extraction machine reading
comprehension dataset, which requires to extract a
passage span for the given question. Table 2 shows
the detailed statistics of these datasets.
Baselines We compare our SkillNet-NLU with
the following approaches:
Task-speciﬁc ﬁne-tuning : We ﬁne-tune all
the parameters of our BERT model4for each task
individually. Therefore, we have a total of six task-
speciﬁc models in our experiments.
Joint ﬁne-tuning (Dense) : We adopt our
BERT as a shared model to obtain feature repre-
sentation and then feed it to multiple task-speciﬁc
prediction layers. The parameters of the BERT
model and all the top layers are learned jointly on
the six tasks.
Joint ﬁne-tuning (MoE) : We set the number
of the FFNs in each layer as seven and activate
the top-2 FFNs for each token, determined by a
gating module. The parameters of these FFNs are
initialized with our BERT model and updated with
the task-speciﬁc prediction layers.
4We collect 800G pre-training data from web news and
blog articles, and train a Chinese BERT-base model with a
batch size of 10,240.
6

--- PAGE 7 ---
T1 T2 T3 T4 T5 T6 Avg
BERT Fine-tuning 94.7y74.6y74.2z56.1z78.284.5y77.1
Task-speciﬁc ﬁne-tuning 94.3 75.0 72.3 56.9 79.2 84.8 77.1
Joint ﬁne-tuning (Dense) 93.4 75.1 71.0 57.4 78.2 83.8 76.5
Joint ﬁne-tuning (MoE) 94.0 74.0 71.4 57.3 78.8 84.5 76.7
SkillNet-NLU w/o sparse pre-training 94.1 75.3 72.1 56.9 81.2 84.6 77.4
SkillNet-NLU w/ sparse pre-training 94.4 75.0 73.9 57.0 81.5 85.7 77.9
Table 3: Evaluation results on the six tasks during multi-task training. We report accuracy for T1T4and F1 for
T5T6.Avg is the average score of all tasks. Results withy,zandare based on google BERT from Cui et al.
(2021), Xu et al. (2020) and our experiments, respectively.
#Params Activated Dev Test
BERT Fine-tuningy102M 80.7 80.8
Task-speciﬁc ﬁne-tuning (BERT-base) 102M 80.3 80.9
Task-speciﬁc ﬁne-tuning (RoBERTa-large) 326M 82.7 83.2
Joint ﬁne-tuning (Dense) 102M 80.7 81.6
Joint ﬁne-tuning (MoE) 159M 81.0 82.4
SkillNet-NLU w/o sparse pre-training 272M 81.5 83.2
SkillNet-NLU w/ sparse pre-training 272M 83.9 84.4
Table 4: Evaluation results on the NLPCC-DBQA dataset. We report the F1 score on the dev and test set. Results
withyare based on google BERT from Sun et al. (2019).
We build our SkillNet-NLU using the implemen-
tation of BERT-base by HuggingFace’s Transform-
ers (Wolf et al., 2020)5, which has 12 Transformer
encoder layers, and 768 hidden state dimensions.
We have two conﬁgurations to do multi-task train-
ing. The ﬁrst setting ( w/o sparse pre-training )
is that all skill modules are initialized with FFN
layers from our Chinese BERT. The second set-
ting ( w/ sparse pre-training ) is that we use the
parameters after sparse pre-training to initialize the
skills. The details of sparse pre-training is shown
in Appendix B.
We conduct multi-task training for 50k steps
with a maximum length of 512, a batch size of
8. We use Adam (Kingma and Ba, 2014) as the
optimizer with 1= 0:9,2= 0:98,= 1e 6.
The learning rate is warmed up over the ﬁrst 5k
steps to a peak value of 2e 5, and then linearly
decayed. We show the learning curve of each task
in Appendix C.
5https://github.com/huggingface/
transformers4.2 Results
Table 3 shows the evaluation results of the baseline
systems as well as the proposed models on six tasks.
The two multi-task learning baselines (i.e., Joint
ﬁne-tuning (Dense) and Joint ﬁne-tuning (MoE))
perform slightly worse than task-speciﬁc ﬁne-
tuning. Our SkillNet-NLU without pre-training
outperforms the baseline systems and achieves an
average score of 77.4%, demonstrating the effec-
tiveness of the sparse activation. The performance
of the model with sparse pre-training is further
improved to 77.9%, which indicates that the skill
modules are learned better after pre-training with
the same sparse manner.
4.3 Results on New Tasks
In this section, we present the adaptation of a well-
trained multi-task SkillNet-NLU to new tasks. Re-
sults are reported in two settings, depending on
whether no new skills are required.
The ﬁrst new task is open domain question an-
swering. Given a question and a candidate sen-
tence, the task is determining whether the sentence
answers the question. We concatenate the question
7

--- PAGE 8 ---
Update Old Skills #Params Activated Dev Test
BERT Fine-tuningy110M 78.6 78.2
Task-speciﬁc ﬁne-tuning (BERT-base) 102M 78.4 78.1
Task-speciﬁc ﬁne-tuning (RoBERTa-large) 326M 78.9 78.7
Joint ﬁne-tuning (Dense) 102M 78.5 78.3
Joint ﬁne-tuning (MoE) 159M 78.7 78.4
No New Skills
SkillNet-NLU w/o sparse pre-training Y 272M 78.8 78.6
SkillNet-NLU w/ sparse pre-training Y 272M 79.0 78.9
Injecting New Skills
SkillNet-NLU w/o sparse pre-training N 57M 77.8 77.1
SkillNet-NLU w/ sparse pre-training N 57M 78.6 78.2
SkillNet-NLU w/o sparse pre-training Y 329M 79.2 79.0
SkillNet-NLU w/ sparse pre-training Y 329M 79.5 79.3
Table 5: Evaluation results on the cMed dataset. We report the top-1 accuracy on the dev and test set. Results with
yare based on google BERT from Cui and Han (2020).
and the candidate sentence with a [SEP] token
and consume the vector of [CLS] to a softmax
layer to conduct binary classiﬁcation. In this set-
ting, no new skills are not injected. So we activate
a set of four relevant skills SNLPCC DBQA =
fs1;s3;s5;s7gand ﬁne-tune all the parameters
of these skill modules for the new task.
We conduct experiments on the NLPCC-DBQA
dataset (Duan, 2016). Table 4 shows the number
of activated parameters and the F1 score of various
models. We can see that our ﬁnal system, SkillNet-
NLU with sparse pre-training, performs better than
the RoBERTa-large6baseline with smaller number
of activated parameters.
We consider the second new task of Chinese
medical question-answer matching. Given a ques-
tion and a candidate answer set, models are re-
quired to select the most relevant answer. The
input of the model is the concatenation of the
question and a candidate answer, separated with
a[SEP] token. We activate a set of four skills
ScMed =fs1;s3;s5;s7gand take the represen-
tation of the [CLS] to compute similarity between
the question and the candidate answer. We explore
whether to inject a new skill ( s8) of understanding
texts from the medical domain, which is unseen
in the multi-task training stage. If the new skill is
injected, we can initialize its parameters with the
general skill ( s7). Then, the parameters of four ac-
6We adopt RoBERTa-wwm-ext-large, which is pre-trained
on more pre-training data with the whole word mask strategy.tivated skills, as well as the new skill, are ﬁne-tuned
on the training data.
We conduct experiments on the cMedQA (Zhang
et al., 2017) dataset. Table 5 shows the number
of activated parameters and the top-1 accuracy of
various models. We show the model performance
by not injecting new skills in the second block. We
can see that our SkillNet-NLU without pre-training
outperforms the three baseline systems, achieving
a top-1 accuracy of 78.6%. The third block shows
the results by injecting a new skill. We can see that
the performance of SkillNet-NLU with or without
sparse pre-training is improved consistently. The
underlying reason is that the number of parameters
increased. Surprisingly, we ﬁnd that only updating
the new skill can achieve strong performance.
5 Ablation Study and Analysis
Evaluation results show that our SkillNet-NLU out-
performs task-speciﬁc ﬁne-tuning and two multi-
task learning baselines. In this section, we conduct
a detailed ablation study and experimental analyses
to better understand the proposed method. All the
results are based on SkillNet-NLU without sparse
pre-training, where all skill modules are initialized
with FFN layers from our Chinese BERT.
5.1 Ablation Study
We perform an ablation study to explore the ef-
fects of each skill. To be speciﬁc, we delete one
of the seven skills in turn, and then activate other
8

--- PAGE 9 ---
corresponding skills for each task. The ablation
results are presented in Table 6. From each row
of the table, we can see that the average score de-
crease when any skill is removed in SkillNet-NLU,
demonstrating that all the skills deﬁned are helpful
for the multi-task training. There is a signiﬁcant
drop when deleting the general skill s7, because
it is shared by all tasks. We can see that the task
performance drops sharply when some closely re-
lated skills are removed, especially for the skill that
is unique to the task (i.e., s4forT1,s5forT6,
s6forT3). We also ﬁnd that removing s2signiﬁ-
cantly affects the performance on T5T6while
doesn’t hurt the accuracy on T1T4. The reason
is that T1T4are sequence prediction tasks and
T5T6are token prediction tasks. Removing s2
makes the model overspecializing to sequence pre-
diction tasks, while is less versatile to other tasks
that require token prediction ability.
5.2 Inﬂuence of the Sampling Rate
00.20.40.60.8176.57777.5
Figure 3: Average score with different .
As described in Section 3.3, we sample training
examples from each task according to the sampling
rate. Figure 3 shows the average score with
different. We can see that the model performs
better when the sampling rate = 1:0, which
maintains the natural distribution of the task. The
underlying reason is that the size of these datasets
is relatively balanced. The results also indicate that
up-sampling datasets is consistently detrimental for
multi-task learning, which is consistent with Agha-
janyan et al. (2021). Therefore, we adopt = 1:0
throughout all of our experiments.
5.3 Inﬂuence of the Number of Top
SkillNet-NLU Layers
We also investigate how the number of top SkillNet-
NLU layers affects the model performance. We
conduct experiments based on SkillNet-NLU and
the number of the top SkillNet-NLU layers varies
from 3 to 12, increased by 3. We show the number
of total parameters and the average score of eachmodel in Table 7. We can see that the performance
consistently improves as the number grows, demon-
strating the effectiveness of our SkillNet-NLU. The
underlying reason is that when more SkillNet-NLU
layers are incorporated, the skills are better learned
as the number of parameters increases.
6 Conclusion
In this work, we present a general-purpose model
called SkillNet-NLU, and its application to natu-
ral language understanding tasks. SkillNet-NLU
includes a set of parameterized skill modules, and
sparsely activate some of the modules depending
on whether a skill is relevant to the target task. The
framework is generic and supports both multi-task
ﬁne-tuning and pre-training, both with sparse ac-
tivation. Results demonstrate that the approach
performs better than baseline systems on both old
and new tasks, and sparse pre-training brings fur-
ther improvements.
This work can be further improved from many
different angles, including deﬁning a broader range
of skills, exploring advanced model architectures,
expanding from one language to multi languages
or even from one modality to multiple modalities.
References
Armen Aghajanyan, Anchit Gupta, Akshat Shrivas-
tava, Xilun Chen, Luke Zettlemoyer, and Sonal
Gupta. 2021. Muppet: Massive multi-task rep-
resentations with pre-ﬁnetuning. arXiv preprint
arXiv:2101.11038 .
Xiongtao Cui and Jungang Han. 2020. Chinese med-
ical question answer matching based on interactive
sentence representation learning. arXiv preprint
arXiv:2011.13573 .
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, and
Ziqing Yang. 2021. Pre-training with whole word
masking for chinese bert. IEEE Transactions on Au-
dio, Speech and Language Processing .
Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao,
Zhipeng Chen, Wentao Ma, Shijin Wang, and Guop-
ing Hu. 2019. A span-extraction dataset for chinese
machine reading comprehension. Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805 .
9

--- PAGE 10 ---
T1 T2 T3 T4 T5 T6 Avg
SkillNet-NLU 94.08 75.25 72.13 56.94 81.19 84.64 77.37
– w/o s1 94.06 74.08 70.44 56.57 80.65 84.12 76.65
– w/o s2 94.24 75.22 71.34 57.11 78.82 83.55 76.71
– w/o s3 93.50 74.07 71.62 57.07 79.84 83.72 76.64
– w/o s4 93.42 74.87 72.06 56.99 78.70 84.08 76.69
– w/o s5 94.15 74.75 71.66 57.08 78.84 83.61 76.68
– w/o s6 93.43 73.63 71.28 56.87 80.86 84.23 76.72
– w/o s7 94.04 74.85 71.99 56.30 78.14 84.22 76.59
Table 6: Ablation results on the six tasks during multi-task training.
#Num #Params Total Avg
3 187M 76.5
6 272M 76.9
9 357M 77.2
12 422M 77.4
Table 7: The number of total parameters and average
score with the different number of top SkillNet-NLU
layers.
Nan Du, Yanping Huang, Andrew M Dai, Simon
Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim
Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,
et al. 2021. Glam: Efﬁcient scaling of language
models with mixture-of-experts. arXiv preprint
arXiv:2112.06905 .
Nan Duan. 2016. Overview of the nlpcc-iccpol 2016
shared task: Open domain chinese question answer-
ing. In NLPCC/ICCPOL .
William Fedus, Barret Zoph, and Noam Shazeer. 2021.
Switch transformers: Scaling to trillion parameter
models with simple and efﬁcient sparsity. arXiv
preprint arXiv:2101.03961 .
Hai Hu, Kyle Richardson, Liang Xu, Lu Li, Sandra
Kübler, and Lawrence Moss. 2020. OCNLI: Orig-
inal Chinese Natural Language Inference. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2020 .
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Sneha Kudugunta, Yanping Huang, Ankur Bapna,
Maxim Krikun, Dmitry Lepikhin, Minh-Thang Lu-
ong, and Orhan Firat. 2021. Beyond distillation:
Task-level mixture-of-experts for efﬁcient inference.
arXiv preprint arXiv:2110.03742 .
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random ﬁelds:Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning .
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,
Dehao Chen, Orhan Firat, Yanping Huang, Maxim
Krikun, Noam Shazeer, and Zhifeng Chen. 2020.
Gshard: Scaling giant models with conditional com-
putation and automatic sharding. arXiv preprint
arXiv:2006.16668 .
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-
feng Gao. 2019. Multi-task deep neural networks
for natural language understanding. arXiv preprint
arXiv:1901.11504 .
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff
Dean. 2017. Outrageously large neural networks:
The sparsely-gated mixture-of-experts layer. arXiv
preprint arXiv:1701.06538 .
Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi
Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao
Tian, and Hua Wu. 2019. Ernie: Enhanced rep-
resentation through knowledge integration. arXiv
preprint arXiv:1904.09223 .
Songbo Tan. 2012. Chnsenticorp.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems , pages 5998–6008.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, et al. 2013. Ontonotes release 5.0
ldc2013t19. Linguistic Data Consortium, Philadel-
phia, PA , 23.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Fun-
towicz, Joe Davison, Sam Shleifer, Patrick von
Platen, Clara Ma, Yacine Jernite, Julien Plu, Can-
wen Xu, Teven Le Scao, Sylvain Gugger, Mariama
10

--- PAGE 11 ---
Drame, Quentin Lhoest, and Alexander M. Rush.
2020. Huggingface’s transformers: State-of-the-
art natural language processing. arXiv preprint
arXiv:1910.03771 .
Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie
Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu,
Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu,
Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao
Wang, Weijian Xie, Yanting Li, Yina Patterson,
Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua
Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui
Zhang, Zhengliang Yang, Kyle Richardson, and
Zhenzhong Lan. 2020. Clue: A chinese language
understanding evaluation benchmark.
Sheng Zhang, Xin Zhang, Hui Wang, Jiajun Cheng, Pei
Li, and Zhaoyun Ding. 2017. Chinese medical ques-
tion answer matching using end-to-end character-
level multi-scale cnns. Applied Sciences .
A Model Architecture
B Sparse Pre-training Details
During pre-training, we initialize four skill mod-
ules (i.e., s1,s2,s3ands7) with FFN layers
from our Chinese BERT. We adopt the same pre-
training data and batch size that is used during the
pre-training of our Chinese BERT. SkillNet-NLU
is pre-trained with mixed-precision training on 32
Nvidia Tesla V100 32GB GPUs for 100k steps with
a maximum length of 512. We use Adam (Kingma
and Ba, 2014) as the optimizer with 1= 0:9,
2= 0:98,= 1e 6. The learning rate is warmed
up over the ﬁrst 10k steps to a peak value of 3e 5,
and then linearly decayed.
After being pre-trained, we can build a multi-
task model by initializing the corresponding four
skill modules. The parameters of other three skill
modules (i.e., s4,s5ands6) are initialized from
the general skill module s7.
C Learning Curves
We show the learning curves during multi-task
training in Figure 5.
11

--- PAGE 12 ---
Self-AttentionxyFFN s1FFN s2FFN s3FFN s4FFN s5FFN s6FFN s7yFFN LayerSelf-Attentionx(a) Each layer in Transformer
Self-AttentionxyFFN s1FFN s2FFN s3FFN s4FFN s5FFN s6FFN s7yFFN LayerSelf-Attentionx (b) Each layer in SkillNet-NLU
Figure 4: A simple implementation of SkillNet-NLU (b) with comparison to the standard Transformer (a). This
example illustrates the application of SkillNet-NLU to machine reading comprehension, where s2, s3, s5 and s7
are activated.
12

--- PAGE 13 ---
-0.0500.050.10.150.20.250.30.350.40.450.50.55
-5k 0 5k 10k15k20k25k30k35k40k45k50k55k
(a) Task T1
0.50.550.60.650.70.750.80.850.90.9511.051.1
-5k 0 5k 10k15k20k25k30k35k40k45k50k55k
(b) Task T2
0.510.520.530.540.550.560.570.580.590.60.610.620.63
-5k 0 5k 10k15k20k25k30k35k40k45k50k55k
(c) Task T3
0.911.11.21.31.41.51.61.71.81.922.1
-5k 0 5k 10k15k20k25k30k35k40k45k50k55k
(d) Task T4
-202468101214161820
-5k 0 5k 10k 15k 20k 25k 30k 35k 40k 45k 50k 55k
(e) Task T5
-0.500.511.522.533.544.5
-5k 0 5k 10k15k20k25k30k35k40k45k50k55k
(f) Task T6
Figure 5: The learning curve of each task during multi-task training.
12Figure 5: The learning curve of each task during multi-task training.
13
