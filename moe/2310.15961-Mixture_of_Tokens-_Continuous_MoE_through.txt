# 2310.15961.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2310.15961.pdf
# File size: 1085768 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Mixture of Tokens: Continuous MoE through
Cross-Example Aggregation
Szymon Antoniak∗
IDEAS NCBR
University of WarsawMichał Krutul∗
IDEAS NCBR
University of WarsawMaciej Pióro
IDEAS NCBR
Polish Academy of Sciences
Jakub Krajewski
IDEAS NCBR
University of WarsawJan Ludziejewski
IDEAS NCBR
University of WarsawKamil Ciebiera
IDEAS NCBR
University of WarsawKrystian Król
IDEAS NCBR
University of Warsaw
Tomasz Odrzygó´ zd´ z
IDEAS NCBRMarek Cygan
University of Warsaw
NomagicSebastian Jaszczur∗
IDEAS NCBR
University of Warsaw
Abstract
Mixture of Experts (MoE) models based on Transformer architecture are pushing
the boundaries of language and vision tasks. The allure of these models lies in
their ability to substantially increase the parameter count without a corresponding
increase in FLOPs. Most widely adopted MoE models are discontinuous with
respect to their parameters - often referred to as sparse . At the same time, existing
continuous MoE designs either lag behind their sparse counterparts or are incompat-
ible with autoregressive decoding. Motivated by the observation that the adaptation
of fully continuous methods has been an overarching trend in deep learning, we
develop Mixture of Tokens (MoT), a simple, continuous architecture that is capable
of scaling the number of parameters similarly to sparse MoE models. Unlike
conventional methods, MoT assigns mixtures of tokens from different examples to
each expert. This architecture is fully compatible with autoregressive training and
generation. Our best models not only achieve a 3×increase in training speed over
dense Transformer models in language pretraining but also match the performance
of state-of-the-art MoE architectures. Additionally, a close connection between
MoT and MoE is demonstrated through a novel technique we call transition tuning .
1 Introduction
Transformer-based Large Language Models (LLMs) constitute one of the most active fields in AI, ex-
hibiting human-level performance in a variety of tasks, including translation, language understanding,
reasoning, and code generation [OpenAI et al., 2023, Anil et al., 2023, Chowdhery et al., 2022]. The
exorbitant sizes of all state-of-the-art language models are integral to their success, with parameter
counts reaching tens or even hundreds of billions. This phenomenon is in line with Kaplan et al.
[2020] and Hoffmann et al. [2022], where the latter proposes that the optimal model size grows
proportionally to the available compute budget. Given that hardware efficiency has been steadily
increasing over the past decade [Sevilla et al., 2022, Rasch et al., 2023], these findings suggest that
scaling will remain a vital component of training increasingly capable models.
∗Core contributors. Detailed authors’ contributions are listed in Appendix C.
Preprint. Under review.arXiv:2310.15961v2  [cs.CL]  24 Sep 2024

--- PAGE 2 ---
Figure 1: Mixture of Tokens: Each expert receives a unique mixture of tokens in the group. Mixing
weights are determined by the controller, which is a fully connected layer (omitted for clarity). For a
given token, its update is a linear combination of expert outputs, with the coefficients equal to the
token’s original mixing weights for each expert.
However, model scaling invariably comes at a cost. Larger models execute more Floating Point
Operations (FLOPs) per token, which results in both training and inference becoming slower and
more expensive [Samsi et al., 2023, McDonald et al., 2022]. Mixture of Experts [Jacobs et al., 1991]
architectures offer an attractive alternative to standard Transformers by drastically increasing the
number of parameters. The core idea is to have multiple experts , each specializing in a different part
of the input space. Currently, state-of-the-art models based on MoE leverage sparsity by activating
only a fraction of the parameters for each token [Sanseviero et al., 2023]. This allows the networks
to increase the number of parameters by an order of magnitude while keeping the FLOPs per token
roughly constant. In this work, we use MoE to signify sparse Mixture of Experts architectures unless
explicitly stated otherwise.
The aforementioned sparsity is made possible with a router , a small network that selects the best
experts for each token. This makes the output of an MoE layer discontinuous with respect to its
parameters, as only a subset of the experts is chosen for each token (usually done with a discrete
top-k operation). The discontinuity and the resulting fluctuations of the router’s decisions were shown
to hurt training efficiency [Dai et al., 2022, Chi et al., 2022] and are hypothesized to be a source of
training instability in large MoE models [Mustafa et al., 2022, Puigcerver et al., 2023]. On the other
hand, existing continuous MoE architectures involve trade-offs, including inability to scale [Muqeeth
et al., 2023, Hazimeh et al., 2021], or incompatibility with autoregressive decoding [Puigcerver et al.,
2023].
This paper introduces Mixture of Tokens, a novel, continuous Transformer architecture closely related
to sparse Mixture of Experts. Similarly to MoE, it is capable of supporting large parameter counts
without incurring significant costs in FLOPs. The core idea behind our design is to for each expert to
process not individual tokens separately but their combined representation.
This technique results in a continuous model that avoids the top- koperation. It requires no additional
techniques commonly required in existing MoE designs (both sparse and continuous), such as load
balancing losses, calculating solutions to optimization problems, or non-homogeneous training
schedules [Hazimeh et al., 2021, Jaszczur et al., 2021, Dai et al., 2022]. It is capable of scaling the
parameter counts akin to sparse MoEs and is compatible with autoregressive language modeling and
generation.
2

--- PAGE 3 ---
Our contributions are the following:
•The Mixture of Tokens architecture, a scalable, continuous Mixture of Experts architecture
that jointly processes tokens from different examples.
•Benchmarking on autoregressive language pretraining against sparse Mixture of Experts
architectures, as well as the standard Transformer baseline, over which MoT demonstrates
a3×speedup.
• Analysis of scaling properties of Mixture of Experts models on multiple scales.
• Improved performance in low precision training compared to Mixture of Experts.
•A novel technique called transition tuning , which allows a pre-trained Mixture of Tokens
model to use sparse MoE inference once tuned, at a fraction of the pretraining cost.
2 Background and Related Work
In this section, we provide an overview of the approaches related to our work and discuss the
differences between various MoE designs. We will introduce the Mixture of Tokens architecture in
Section 3 and provide a detailed comparison between MoT and related methods in Section 3.3.
2.1 Large Language Models
Transformer scaling has been shown to be a critical factor in achieving state-of-the-art results in
language and vision tasks [Kaplan et al., 2020, Zhai et al., 2022], with the largest disclosed parameter
counts in dense models reaching hundreds of billions of parameters [Brown et al., 2020, Chowdhery
et al., 2022, Anil et al., 2023]. These large models exhibit impressive abilities that are not present in
their smaller counterparts [Wei et al., 2022]. Kaplan et al. [2020] and Hoffmann et al. [2022] have
shown that the final model performance is predictable and correlates directly with the model size and
the amount of training data. However, increasing model sizes raises the demand for computational
resources during both training and inference [Rae et al., 2022].
2.2 Mixture of Experts
Mixture of Experts (MoE) was first introduced by Jacobs et al. [1991] as an ensemble-like neural
network comprised of separate sub-networks called experts. The original design used a gating network
to select a soft assignment of experts for each input. In the context of Deep Learning, the notion of
an MoE layer was introduced in Eigen et al. [2014]. Shazeer et al. [2017] combined a sparse version
of MoE layers with an LSTM to train a model with over 100billion parameters, unprecedented at
the time. The design, similar to state-of-the-art MoE models today, used a small routing network to
decide the top- kbest experts for each input. By choosing only a subset of the experts, they were able
to increase the size of the network while keeping FLOPs per token roughly constant. The Transformer
was first combined with MoE layers in Lepikhin et al. [2020], where it replaced the feed-forward
layer. The design was further simplified in Fedus et al. [2022], which trained a model with 1.6trillion
parameters with top- 1routing. Since then, a number of studies investigated different sparse MoE
designs [Du et al., 2022, Jiang et al., 2024, Zhou et al., 2022, Roller et al., 2021, Lewis et al., 2021].
A comprehensive analysis of scaling properties of sparse MoE architectures can be found in Clark
et al. [2022].
2.3 Continuous Mixture of Experts
Continuous architectures serve an important role within the field due to their flexible and efficient
nature. Hazimeh et al. [2021] were pioneers in introducing them in MoE by presenting continuous
techniques for calculating encodings of the choice of an expert. In another approach, Muqeeth et al.
[2023] proposed a method where they merge experts based on the weights of the router network. In a
recent advancement, Puigcerver et al. [2023] proposed a continuous variant of MoE for the Vision
Transformer, where patches are mixed only within each image.
3

--- PAGE 4 ---
Figure 2: (Left) Diagram of a standard feed-forward layer featured in the Transformer architecture:
each token is processed with the same MLP, independent of other tokens. (Right) Diagram of a
Token Choice layer, where each token decides which expert to choose. This way, different experts
process a different number of tokens. If one expert is chosen by too many tokens, a portion of the
tokens are dropped — they receive no update.
2.4 From Hard to Soft Methods
From the very beginning of the Deep Learning field, we see movements from discrete functions toward
continuous ones. The first perceptron [McCulloch and Pitts, 1943] used "all-or-none" activation,
supposedly to align with propositional logic. This was later improved with soft activation functions,
enabling gradient descent and multi-layer neural networks. Similarly, soft attention, introduced in
Bahdanau et al. [2016], enabled RNNs to look at arbitrary input from the past while maintaining the
ability to learn the selection with standard gradient descent. This contrasts with hard attention, which
requires, for example, reinforcement learning techniques. While hard attention could perform on
par with soft attention [Xu et al., 2015, Zohourianshahzadi and Kalita, 2021], soft attention, with its
simplicity of training, presented better trade-offs and was later used as the basic building block of the
Transformer [Vaswani et al., 2017].
Mixture of Experts, introduced into Deep Learning by Shazeer et al. [2017], seems like a naturally
discrete function — after all, the expert either processes a given token or it doesn’t. However, much
like the shift from hard to soft attention, an expert in MoE can "attend" to a mix of tokens, taken as a
weighted average. This results in a smooth, continuous model and enables more stable training.
3 Mixture of Tokens
The goal of this work is to devise an efficient, continuous architecture that retains the scalability of
the Mixture of Experts while omitting the top-k operation that limits a token’s exposure to different
experts. An intuitive way to achieve this is to route all tokens to all experts, but this approach is
computationally infeasible for large-scale pretraining. To combat this constraint, the method explored
in this work considers what happens not to an individual token but to a whole group of tokens instead.
The main contribution of this work is the observation that allowing an expert to dynamically produce
a continuous representation of the entire group of tokens that is more lightweight to process than each
token individually yields positive results.
More specifically, in our design, an input batch is divided into groups of tokens, and each group is
processed independently. Given a group and a single expert, a scalar weight is produced for each
token. The weights are then normalized and used to compute a linear combination of the tokens,
which is used as the expert’s input. The experts’ outputs are used for token updates as follows: for
each input token, its update is a linear combination of expert outputs, with the token’s mixing weights
for each expert as coefficients2. A diagram of our method is presented in Figure 1.
To see why this method is scalable, it is helpful to examine the relationship between the number of
tokens in a group and the number of experts. Essentially, if the two are equal, the total computation
2The authors note that an MoT layer admits an efficient vectorized implementation, where all meaningful
computations are done with batched matrix multiplications.
4

--- PAGE 5 ---
Algorithm 1 Mixture of Tokens layer
1:foreachEin experts do:
2: weightsE= Softmax(Linear(tokens))
3: mix =P
itoken i∗weightsi,E
4: outputE=E(mix)
5:foreach i do
6: foreach E do
7: updatei=P
EoutputE∗weightsi,E
done by experts is the same as in the case of top-1 routing. This allows MoT to benefit from the same
parameter scaling as seen in MoE, which we confirm empirically in Section 4.2.
3.1 More mixtures per expert
Building on the design described above, we experiment with feeding more than one mixture into each
expert. If done without any further modifications, this would mean a linear increase in computation
costs for each extra mixture processed. In order to avoid this extra cost, MoT uses more experts, but
each expert has a proportionally reduced hidden dimension. This way, each mixture is processed by a
small expert, and the layer’s total number of parameters, as well as the number of FLOPs used by
all experts, stays the same. We find this design to bring consistent improvements as the number of
processed mixtures increases.
3.2 Token Groups in MoT
The question of how token groups are decided within a batch is crucial for compatibility with
autoregressive training and inference. The key insight is that tokens from the same sequence cannot
be placed in one group, as the mixing operation would result in an information leak. Due to this
restriction, MoT groups tokens from different examples based on their position in the sequence.
Consequently, all tokens within a group have the same position in their respective sequences. As
mentioned before, in order to keep the number of FLOPs per token constant, an increase in the
number of experts means an equal increase in group size. An illustration of how grouping is done
within a batch of tokens is shown in Figure 3.
musicismusicisaw ould
beallbealls t air w aybe
they outhey out oa
f oodneedf oodneedHe av enfine
IfA t t entionIfA t t entionbuyingChanc e
IfA t t entionTheChanc ebuyingChanc eaw oulds t aiw aybet oaHe av enfine
Figure 3: Each group consists of tokens with the same position in a sequence. In this example, the
group size is 2. Note that the maximal possible group size is equal to the batch size.
5

--- PAGE 6 ---
3.3 Comparison with other Mixture of Experts Architectures
Scaling. The technique featured in Hazimeh et al. [2021] is based on a continuously differentiable
sparse top-k router, which is a major advantage compared to the common top-k gating. However, this
approach requires that all experts are utilized in a portion of training, rendering it computationally
prohibitive for models with large numbers of experts. The architecture based on merging experts
proposed in Muqeeth et al. [2023] also presents an attractive, continuous alternative to top-k gating,
yet the cost of merging all experts again scales linearly with the number of experts. To combat this,
the technique is applied once per sequence, which limits the expressive power of the final model.
Training stability. Lepikhin et al. [2020] report instabilities during the training of large MoE models,
stemming from the inaccuracies when calculating router weights in low precision. To stabilize the
training, they resorted to using full precision. Fedus et al. [2022] made progress in using mixed
precision when training MoE by using selective high precision for gating. When comparing Lepikhin
et al. [2020], Fedus et al. [2022] to MoT, an advantage of our technique emerges - it is more robust
to training in lower precision than other methods. We conjecture, that this is due to the merging
mechanism being less susceptible to rounding errors than gating in sparse MoEs.
Token dropping. Token dropping is a phenomenon where tokens do not receive an update from
any expert. This can happen because the expert was chosen by too many tokens in a batch [Fedus
et al., 2022, Lepikhin et al., 2020, Zoph et al., 2022] or, in the case of routing experts to tokens,
when a token was not chosen by any expert [Zhou et al., 2022]. Existing techniques that combat this
phenomenon provide a partial solution, but the problem remains. In contrast, tokens in MoT are part
of every mixture produced within their group; hence, they always receive an update.
Auto-regressive decoding. Mixture of Tokens is based on the notion of merging tokens prior to
being processed by an expert. An encoder-only design of similar nature is featured in concurrent
work [Puigcerver et al., 2023]. The technique is based on merging patches within an image for vision
models. The crucial difference between this technique and MoT is that MoT is compatible with
autoregressive training and inference.
4 Experiments
The focus of this work is to investigate the efficiency of Tokens on autoregressive language modeling.
To measure model quality, we pretrain models for a fixed number of tokens and compare final
perplexity in accordance with existing MoE literature [Du et al., 2022, Fedus et al., 2022]. In all
experiments, the models are trained on the C4 dataset [Raffel et al., 2023] and use the GPT-2 tokenizer.
Unless specified otherwise, we use mixed precision, where all heavy computation is done in bfloat16,
whereas the optimizer state and the master weights are kept in full precision. To study the stability of
our model, we experiment with training fully in reduced precision.
Our main result is a substantial speed-up of MoT models compared to dense Transformers (Figure 7)
and results comparable to sparse MoEs (Figure 6). What follows is the analysis of scaling properties
of the MoT architecture with respect to the number of parameters (Figure 4) and the number of
mixtures sent to each expert (Figure 5). We investigate the model’s performance in low precision in
order to simulate training instability and find that MoT is less susceptible to instabilities that stem
from low-precision training. Lastly, we show the connection between MoT and MoE by spending
an additional fraction of pretraining compute to effectively turn a MoT model into a Token Choice
model (Section 4.4).
4.1 Model Architecture
The base of our experiments is a decoder-only Transformer based on GPT-2 [Radford et al., 2019].
We experiment on two model scales: a 77M Medium model and a 162M Base model (refer to
Appendix A for hyperparameters and training details). To then obtain a Mixture of Tokens model, we
replace the second half of feed-forward layers in the Transformer with MoT layers. Similar to MoE
models, the FLOPs and parameter counts in MoT are decoupled. We denote the model architecture
by its dense counterpart in terms of the number of FLOPs and, separately, the number of experts (or,
equivalently, group size). To this end, a MoT-Medium/ 32E model is one that uses the same number
of FLOPS as a Medium ( 77M) Transformer model but uses 32experts in MoT layers.
6

--- PAGE 7 ---
Regarding the design described in Section 3.1, Medium/ 32E/4denotes a model that uses MoT layers
with 32·4small experts that collectively have the same number of parameters as 32normal experts.
Besides using the Transformer as a baseline, we also compare against Token Choice [Fedus et al.,
2022] and Expert Choice [Zhou et al., 2022] as sparse MoE baselines. As Expert Choice is sensitive
to the batch size, to avoid discrepancies between training and inference, we group tokens prior to
routing in training Expert Choice models.
4.2 Scaling Results
Mixture of Tokens models demonstrate strong scaling properties with respect to the number of
parameters. As seen in Figure 4, increasing the number of experts in MoT layers while using the
same compute budget yields consistent improvements. All MoT models are a strict improvement over
the Transformer. The figure also features an ablation experiment, where the mixing weights are fixed
to1/n, with nas the group size. This corresponds to a uniform mixing strategy; the performance of
that model clearly suffers, confirming that MoT layers learn non-trivial mixing strategies.
Figure 4: Scaling with respect to the number of parameters. Also featured are the Transformer
baseline and an MoT model with a non-learnable, uniform routing strategy.
The increased number of token mixtures described in Section 3.1 is another axis of scaling for MoT
models, again exhibiting consistent improvements. We hypothesize that this phenomenon is due to
two mechanisms: first, the model is simply more expressive with a larger number of smaller experts,
and second, the model can allocate its focus (the mixing weights) more flexibly to more important
tokens while reducing the updates for trivial ones.
Figure 5: Scaling with respect to the number of token mixtures.
7

--- PAGE 8 ---
4.3 Comparison with the Transformer and Sparse MoEs
Crucially, the performance of Mixture of Tokens is comparable to that of the strong Mixture of
Experts baselines (Figure 6). An increased number of mixtures allows it to compete with both Expert
Choice and Token Choice architectures. As sparse routing is hypothesized to contribute to training
instabilities in large sparse models, Mixture of Tokens, being continuous, presents a promising
alternative. To investigate training instabilities at the scale we experiment on, we trained models fully
in bfloat16, as opposed to the mixed precision used in all other experiments. The results confirm
that MoT is more resistant to lower precision training: as the precision of training decreases, the
performance of Expert Choice drops below that of Mixture of Tokens, despite the former attaining
better perplexity using mixed precision. We find this to be evidence of the architecture’s potential for
stable training at higher model scales. See Table 1 for details.
Figure 6: Comparison of MoT and sMoE architectures. An increased number of smaller experts
allows MoT to match the performance of the best sMoE model. Due to computational constraints,
the models were trained for 100K steps.
Lastly, we combine our findings on MoT scaling properties to train our most efficient MoT model
and compare it to the Transformer baseline (Figure 7). The result is a model that attains the final
loss of the baseline in a third of the training steps. This represents a 3×improvement in terms of the
compute budget.
Figure 7: Our best MoT model reaches the final loss of the baseline in just 33 %of the compute
budget.
8

--- PAGE 9 ---
Figure 8: Transition tuning: The first 150K steps of the model are done using Mixture of Tokens
architecture. Then, a new Token Choice model is initialized with weights taken from the MoT model,
and the model trains for a further 15K steps to recover performance. The spike in loss is due to the
sudden change of architecture.
Table 1: Lower precision training result loss comparison. MoT performs better in the bfloat16-only
setting. Learning rates were separately tuned in lower precision for both Expert Choice and MoT.
Results averaged over 3 random seeds.
MoT-Medium/ 32E Expert Choice-Medium/ 32E
Mixed Precision 3.442 ( ±0.002) 3.420 ( ±0.002)
bf16 only 3.661 ( ±0.007) 3.728 ( ±0.044)
4.4 Transition Tuning
Mixture of Tokens suffers from a drawback common to MoEs, namely, it does not support unbatched
inference. This is a direct consequence of its design — in the forward pass, it groups several tokens
from different examples in the batch. With the growing adoption of Large Language Models to
consumer hardware [Touvron et al., 2023, Cerisara, 2023], the lack of support could invalidate the
architecture’s wider adoption. While Mixture of Tokens with a group size of one is technically
possible, in order to keep FLOPs constant, the layer would need to reduce trivially to a standard
Transformer MLP.
To address this, we demonstrate that the weights learned by Mixture of Tokens can be used to directly
initialize a Token Choice model with the same specifications (number of experts and expert size).
The layer responsible for producing mixing weights is used to initialize the sparse router. To mitigate
the performance difference caused by this change of architecture, we train the entire new model (no
weights are frozen) for 10% of the total pretraining steps of the original model in order to recover the
original model’s performance (measured in evaluation loss). We call this technique transition tuning .
This way, it is possible to train with Mixture of Tokens and enjoy unbatched generation at inference
time. We hypothesize that this pipeline would be especially attractive in setups where parts of the
model cannot be trained in higher precision, such as on specialized, low-precision hardware. The
results are presented in Figure 8.
5 Limitations and Future Work
With the strong performance of MoT on medium-sized models, an obvious next step is to train larger
models. This presents an opportunity to validate stability results on larger models, where training
instabilities are more common.
As with most Mixture of Experts models, the memory footprint of MoT layers is substantial. Scaled
models require large amounts of RAM on specialized hardware for training, making their adoption
9

--- PAGE 10 ---
expensive. To this end, an attractive future direction would be to investigate model distillation with
Mixture of Tokens models.
Lastly, both training and inference with MoT mix different examples within a single batch. This
mixing of tokens from different sequences and the necessity of performing batched inference may
be undesirable in some use cases. While performing unbatched inference is always inefficient with
LLMs, as the memory throughput to access model weights becomes the bottleneck, unbatched
inference still finds its uses. While transition tuning solves this problem, exploring different inference
strategies might bring new insights.
6 Conclusions
In this work, we presented the Mixture of Tokens, a novel continuous Mixture of Experts architecture
compatible with autoregressive decoding. This architecture scales to model sizes similar to sparse
Mixture of Experts, matches its performance, and is more resistant to training instabilities due to
lower precision training. Moreover, we introduced transition tuning, a technique for initializing an
MoE model with another pretrained MoE model of a different architecture, and showed that the new
model attains the performance of the original one using a fraction of the compute budget.
Acknowledgements
We would like to express sincere gratitude to Piotr Padlewski and Tomasz Trzci ´nski for general
feedback and Dagmara Rudzi ´nska for invaluable support with graphic design.
This work was funded by IDEAS NCBR, which also provided significant computational resources.
Marek Cygan was partially supported by an NCBiR grant POIR.01.01.01-00-0392/17-00. The
research was supported by PL-Grid infrastructure (grant PLG/2023/016148). We also benefited
from the Entropy cluster (hosted at the Faculty of Mathematics, Informatics and Mechanics of
the University of Warsaw) funded by NVIDIA, Intel, the Polish National Science Center grants
UMO-2017/26/E/ST6/00622 and 2022/45/N/ST6/02222, and ERC Starting Grant TOTAL.
References
OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red
Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavar-
ian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner,
Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim
Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully
Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won
Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah
Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien
Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman,
Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni,
Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene,
Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He,
Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele,
Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain,
Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn,
Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish
Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik
Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich,
Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy
Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie
Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini,
Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne,
Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David
Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
10

--- PAGE 11 ---
Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély,
Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo
Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano,
Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng,
Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto,
Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power,
Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis
Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted
Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel
Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon
Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky,
Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang,
Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea V oss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason
Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff,
Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu,
Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba,
Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang,
William Zhuk, and Barret Zoph. Gpt-4 technical report, 2023.
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark
Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury,
Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A.
Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa
Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,
Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,
Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,
Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang
Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,
Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John
Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov,
Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy,
Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So,
Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran V odrahalli, Xuezhi Wang,
Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting
Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny
Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M.
Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon
Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,
Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models,
2020.
11

--- PAGE 12 ---
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom
Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,
Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.
Training compute-optimal large language models, 2022.
Jaime Sevilla, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos.
Compute trends across three eras of machine learning. In 2022 International Joint Conference
on Neural Networks (IJCNN) . IEEE, July 2022. doi: 10.1109/ijcnn55064.2022.9891914. URL
http://dx.doi.org/10.1109/IJCNN55064.2022.9891914 .
Malte J. Rasch, Charles Mackin, Manuel Le Gallo, An Chen, Andrea Fasoli, Frederic Odermatt, Ning
Li, S. R. Nandakumar, Pritish Narayanan, Hsinyu Tsai, Geoffrey W. Burr, Abu Sebastian, and
Vijay Narayanan. Hardware-aware training for large-scale and diverse deep learning inference
workloads using in-memory computing-based accelerators, 2023.
Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam Michaleas, Michael Jones, William
Bergeron, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally. From words to watts: Benchmark-
ing the energy costs of large language model inference, 2023.
Joseph McDonald, Baolin Li, Nathan Frey, Devesh Tiwari, Vijay Gadepally, and Siddharth Samsi.
Great power, great responsibility: Recommendations for reducing energy for training language
models. In Findings of the Association for Computational Linguistics: NAACL 2022 . Association
for Computational Linguistics, 2022. doi: 10.18653/v1/2022.findings-naacl.151. URL http:
//dx.doi.org/10.18653/v1/2022.findings-naacl.151 .
Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures
of local experts. Neural Computation , 3(1):79–87, 1991. doi: 10.1162/neco.1991.3.1.79.
Omar Sanseviero, Lewis Tunstall, Philipp Schmid, Sourab Mangrulkar, Younes Belkada, and Pedro
Cuenca. Mixture of experts explained, 2023. URL https://huggingface.co/blog/moe .
Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, and Furu Wei. Stablemoe:
Stable routing strategy for mixture of experts, 2022.
Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,
Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On the representation
collapse of sparse mixture of experts, 2022.
Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal
contrastive learning with limoe: the language-image mixture of experts, 2022.
Joan Puigcerver, Carlos Riquelme, Basil Mustafa, and Neil Houlsby. From sparse to soft mixtures of
experts, 2023.
Mohammed Muqeeth, Haokun Liu, and Colin Raffel. Soft merging of experts with adaptive routing,
2023.
Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,
Rahul Mazumder, Lichan Hong, and Ed H. Chi. Dselect-k: Differentiable selection in the mixture
of experts with applications to multi-task learning, 2021.
Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Łukasz Kaiser, Wojciech Gajewski,
Henryk Michalewski, and Jonni Kanerva. Sparse is enough in scaling transformers, 2021.
Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers,
2022.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya
Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
12

--- PAGE 13 ---
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,
Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,
Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,
Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron
Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen
Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro,
Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,
Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,
Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume,
Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,
Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,
and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher,
2022.
David Eigen, Marc’Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep
mixture of experts, 2014.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and
Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding, 2020.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity, 2022.
Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim
Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,
Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson,
Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng
Chen, and Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2022.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,
Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-
Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le
Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.
Mixtral of experts, 2024.
Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng
Chen, Quoc Le, and James Laudon. Mixture-of-experts with expert choice routing, 2022.
Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. Hash layers for large sparse
models, 2021.
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:
Simplifying training of large, sparse models, 2021.
Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,
Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche,
Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones,
Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich
Elsen, Koray Kavukcuoglu, and Karen Simonyan. Unified scaling laws for routed language models,
2022.
13

--- PAGE 14 ---
Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity.
The bulletin of mathematical biophysics , 5:115–133, 1943.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate, 2016.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with
visual attention. CoRR , abs/1502.03044, 2015. URL http://arxiv.org/abs/1502.03044 .
Zanyar Zohourianshahzadi and Jugal K. Kalita. Neural attention for image captioning: review
of outstanding methods. Artificial Intelligence Review , 55(5):3833–3862, November 2021.
ISSN 1573-7462. doi: 10.1007/s10462-021-10092-2. URL http://dx.doi.org/10.1007/
s10462-021-10092-2 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR , abs/1706.03762, 2017. URL
http://arxiv.org/abs/1706.03762 .
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and
William Fedus. St-moe: Designing stable and transferable sparse expert models, 2022.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer, 2023.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
Christophe Cerisara. SlowLLM: large language models on consumer hardware . PhD thesis, CNRS,
2023.
14

--- PAGE 15 ---
A Training Hyperparameters
All models were trained using mixed precision unless explicitly stated. We trained all experiments
with a batch size of 256and a context length of 256for150Ktraining steps (unless explicitly
stated), resulting in a total of 10B training tokens. We used the AdamW optimizer with default
hyperparameters. When required, we employed a Fully Sharded Data Parallel approach from PyTorch
to parallelize the training across multiple machines. Learning rates were tuned separately depending
on model size and architecture. The optimal learning rate for Transformers was 1e-3 for Medium and
4e-4 for Base models, while for both MoT and MoE, they were 7e-4 for Medium and 2e-4 for Base.
Table 2: Training hyperparameters. The table provides example models featured in experiments. All
remaining models can be derived from this table.
Model Experts Expert Group Total Blocks dmodel dff #att.
size size params heads
Transformer-Medium - - - 77M 8 512 2048 8
MoT-Medium/32E 32 2048 32 336M 8 512 - 8
MoT-Medium/32E/8 256 256 32 337M 8 512 - 8
Transformer-Base - - - 162M 12 768 3072 12
MoT-Base/32E 32 3072 32 520M 12 768 - 12
MoT-Base/64E/16 1024 192 64 977M 12 768 - 12
B Reproducibility
The code and configuration files used to produce the results described in this work are available in
our public repository at https://github.com/llm-random/llm-random .
C Contributions
Szymon implemented a PoC and different variants of MoT, together with running experiments and
optimizations. Michał implemented and experimented with various MoT designs and contributed to
the infrastructure design and implementation. Sebastian provided the initial idea, research intuitions,
and direct project supervision. Maciej was responsible for parts of evaluation and significant
engineering. Jakub implemented MoE baselines, Jan stabilized Mixture of Experts training, while
both helped with MoE hyperparameter tuning. Tomasz consulted ideas and helped with cluster
infrastructure. Kamil and Krystian contributed to general engineering. Everybody above contributed
to the infrastructure of the project. Marek provided scientific advice and high-level supervision.
15
