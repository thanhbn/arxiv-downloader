# 2104.02640.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2104.02640.pdf
# File size: 4006128 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
To appear in the Electronic Journal of Statistics
A non-asymptotic approach for model
selection via penalization in
high-dimensional mixture of experts
modelsâˆ—
TrungTin Nguyenâ€ 
Normandie Univ, UNICAEN, CNRS, LMNO, 14000 Caen, France.
Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK,
Inria Grenoble Rhone-Alpes, 655 av. de l'Europe, 38335 Montbonnot, France.
e-mail: trung-tin.nguyen@inria.fr
url: http://trung-tinnguyen.github.io/
Hien Duy Nguyen
Department of Mathematical and Physical Sciences,
La Trobe University, Bundoora Melbourne 3066, Victoria Australia.
School of Mathematics and Physics,
University of Queensland, St. Lucia, Queensland, Australia.
e-mail: h.nguyen7@uq.edu.au
url: https://hiendn.github.io/
Faicel Chamroukhi
Normandie Univ, UNICAEN, CNRS, LMNO, 14000 Caen, France.
e-mail: faicel.chamroukhi@unicaen.fr
url: https://chamroukhi.com/
Florence Forbes
Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK,
Inria Grenoble Rhone-Alpes, 655 av. de l'Europe, 38335 Montbonnot, France.
e-mail: florence.forbes@inria.fr
url: http://mistis.inrialpes.fr/people/forbes/
Abstract: Mixture of experts (MoE) are a popular class of statistical
and machine learning models that have gained attention over the years
due to their exibility and eciency. In this work, we consider Gaussian-
gated localized MoE (GLoME) and block-diagonal covariance localized
MoE (BLoME) regression models to present nonlinear relationships in het-
erogeneous data with potential hidden graph-structured interactions be-
tween high-dimensional predictors. These models pose dicult statistical
estimation and model selection questions, both from a computational and
theoretical perspective. This paper is devoted to the study of the problem
âˆ—This work is partially supported by the French Ministry of Higher Education and Research
(MESRI), French National Research Agency (ANR) grant SMILES ANR-18-CE40-0014, Aus-
tralian Research Council grant number DP180101192, and the Inria LANDER project.
â€ Corresponding author.
1arXiv:2104.02640v3  [math.ST]  4 Sep 2022

--- PAGE 2 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 2
of model selection among a collection of GLoME or BLoME models charac-
terized by the number of mixture components, the complexity of Gaussian
mean experts, and the hidden block-diagonal structures of the covariance
matrices, in a penalized maximum likelihood estimation framework. In par-
ticular, we establish non-asymptotic risk bounds that take the form of weak
oracle inequalities, provided that lower bounds for the penalties hold. The
good empirical behavior of our models is then demonstrated on synthetic
and real datasets.
MSC2020 subject classications: Primary 62H30, 62E17; secondary
62H12.
Keywords and phrases: Mixture of experts, linear cluster-weighted mod-
els, mixture of regressions, Gaussian locally-linear mapping models, clus-
tering, oracle inequality, model selection, penalized maximum likelihood,
block-diagonal covariance matrix, graphical Lasso.
Contents
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.1 Mixture of experts models . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.3 Related literature . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.4 Main contributions . . . . . . . . . . . . . . . . . . . . . . . . . . 5
1.5 Main challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.6 Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2 Notation and framework . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.1 GLoME and BLoME models . . . . . . . . . . . . . . . . . . . . 8
2.1.1 Gaussian gating networks . . . . . . . . . . . . . . . . . . 11
2.1.2 Gaussian experts . . . . . . . . . . . . . . . . . . . . . . . 11
2.2 High-dimensional regression via GLLiM and BLLiM models . . . 13
2.3 Collection of GLoME and BLoME models . . . . . . . . . . . . . 15
2.4 Loss functions and penalized maximum likelihood estimator . . . 16
3 Main results of nite-sample oracle inequalities . . . . . . . . . . . . . 17
3.1 Deterministic collection of GLoME models . . . . . . . . . . . . . 17
3.2 Random subcollection of BLoME models . . . . . . . . . . . . . . 19
3.3 Practical application of nite-sample oracle inequalities . . . . . 20
4 Proofs of nite-sample oracle inequalities and main mathematical chal-
lenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.1 Proof for deterministic collections of GLoME models . . . . . . . 22
4.1.1 A general conditional density model selection theorem for
deterministic collection of models . . . . . . . . . . . . . 22
4.1.2 Sketch of the proof for LinBoSGaME models . . . . . . . 24
4.1.3 Our contributions on the proof for GLoME models . . . . 27
4.2 Proof of random subcollection of BLoME models . . . . . . . . . 29
4.2.1 A model selection theorem for MLE among a random sub-
collection . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4.2.2 Sketch of the proof for BLoME models . . . . . . . . . . . 30
4.2.3 Our contributions on BLoME models . . . . . . . . . . . 30

--- PAGE 3 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 3
5 Numerical experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
5.1 The procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
5.2 Simulated data sets . . . . . . . . . . . . . . . . . . . . . . . . . . 33
5.3 Ethanol data set . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
6 Conclusion and perspectives . . . . . . . . . . . . . . . . . . . . . . . . 49
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
A Our contributions on lemma proofs . . . . . . . . . . . . . . . . . . . . 57
A.1 Proof of Lemma 2.1: Inverse regression trick . . . . . . . . . . . 57
A.1.1 Elliptically symmetric distributions . . . . . . . . . . . . . 57
A.1.2 Relation between forward and inverse regression . . . . . 59
A.2 Proof of Lemma 4.5: Bracketing entropy of Gaussian gating net-
works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
A.2.1 Proof of Lemma A.5 . . . . . . . . . . . . . . . . . . . . . 62
A.2.2 Proof of Lemma A.6 . . . . . . . . . . . . . . . . . . . . . 63
A.3 Proof of Lemma 4.7: Bracketing entropy of Gaussian experts with
1-block covariance matrices . . . . . . . . . . . . . . . . . . . . . 66
A.4 Proof of Lemma 4.11 . . . . . . . . . . . . . . . . . . . . . . . . . 68
A.5 Proof of Lemma 4.12: Bracketing entropy of Gaussian experts
with multi-block-diagonal covariance matrices . . . . . . . . . . . 68
A.5.1 Proof of Lemma A.8 . . . . . . . . . . . . . . . . . . . . . 70
A.5.2 Proof of Lemma A.9 . . . . . . . . . . . . . . . . . . . . . 71
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
1. Introduction
1.1. Mixture of experts models
Mixture of experts (MoE) models, originally introduced as neural network archi-
tectures in [48, 50], are exible models that generalize the classical nite mixture
models as well as nite mixtures of regression models [66, Sec. 5.13]. Their ex-
ibility comes from allowing the mixture weights (or the gating functions) to
depend on the explanatory variables, along with the component densities (or
experts). In the context of regression, MoE models with Gaussian experts and
softmax or normalized Gaussian gating functions are the most popular choices
and are powerful tools for modeling more complex nonlinear relationships be-
tween outputs (responses) and inputs (predictors) that arise from dierent sub-
populations. The popularity of these conditional mixture density models arise
largely due to their universal approximation properties, which have been studied
in [49, 81, 74, 45, 73, 75], and which improve upon approximation capabilities
of unconditional nite mixture models, as studied in [41, 85, 80, 43, 44, 78, 77].
Detailed reviews on the practical and theoretical aspects of MoE models can be
found in [101, 59, 72, 76].
In this paper, we wish to investigate MoE models with Gaussian experts and
normalized Gaussian gating functions for clustering and regression, rst intro-
duced by [99], which extended the original MoE models of [48]. From hereon

--- PAGE 4 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 4
in, we refer to these models as Gaussian-gated localized MoE (GLoME) mod-
els and block-diagonal covariance for localized MoE (BLoME) models, and we
provide their precise denitions in Section 2.1. Furthermore, the original MoE
models with softmax gating functions will be referred to as linear-combination-
of-bounded-functions softmax-gated MoE (LinBoSGaME) regression models. In
particular, we simply refer to ane instances of LinBoSGaME models as softmax-
gated MoE (SGaME) regression models. It is worth pointing out that the BLoME
models generalize GLoME models by utilizing a parsimonious covariance struc-
ture, via block-diagonal structures for covariance matrices in the Gaussian ex-
perts. It is also interesting to point out that supervised Gaussian locally-linear
mapping (GLLiM) and block-diagonal covariance for Gaussian locally-linear
mapping (BLLiM) models in [26] and [34] are ane instances of GLoME and
BLoME models, respectively, where the latter pair consider general linear com-
bination of bounded functions instead of ane for mean functions of Gaussian
experts, in the prior pair.
One of the main disadvantages of LinBoSGaME models is the diculty of ap-
plying an expectation{maximization (EM) algorithm [27, 65], which requires an
internal iterative numerical optimization procedure ( e.g., iteratively-reweighted
least squares, Newton-Raphson algorithm) to update the softmax parameters.
To overcome this problem, we instead use the Gaussian gating network that
enables us to link BLoME with hierarchical Gaussian mixture model. Then,
the maximization with respect to the parameters of the gating network can be
solved analytically with the EM algorithm framework, which decreases the com-
putational complexity of the estimation routine. Furthermore, we can then also
make use of well-established theoretical results for nite mixture models.
We note here that both GLoME and BLoME models have been thoroughly
studied in the statistics and machine learning literatures in many dierent
guises, including localized MoE [86, 87, 69, 15], normalized Gaussian networks
[90], MoE modeling of priors in Bayesian nonparametric regression [83, 82],
cluster-weighted modeling [47], deep mixture of linear inverse regressions [55], hi-
erarchical Gaussian locally linear mapping structured mixture (HGLLiM) model
[95], multiple-output Gaussian gated mixture of linear experts [73], and approx-
imate Bayesian computation with surrogate posteriors using GLLiM [39].
1.2. Goals
Our main goal is to learn potentially nonlinear relationships between a multi-
variate output and a high-dimensional input issued from a heterogeneous pop-
ulation. This involves performing regression, clustering and model selection,
simultaneously. While estimation can be performed using standard EM algo-
rithms, it crucially depends and requires data-driven hyperparameter choices,
including the number of mixture components (or clusters), the degree of com-
plexity of each Gaussian expert's mean function, and the hidden block-diagonal
structures of the covariance matrices. Recall that hyperparameter choices of
data-driven learning algorithms belong to the class of model selection problems,

--- PAGE 5 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 5
which have attracted a lot of attention in statistics and machine learning over
the past 50 years [1, 58, 2, 60, 3]. Specically, given a set of models, how do
we select the one with the lowest possible risk from the data? Note that penal-
ization is one of the main strategies proposed for model selection. It suggests
choosing the estimator that minimizes the sum of its empirical risk and some
penalty terms corresponding to the t of the model to the data while avoiding
overtting.
1.3. Related literature
In general, model selection for MoE models is often performed using the Akaike
information criterion (AIC) [1], the Bayesian information criterion (BIC) [91]
or the BIC-like approximation of integrated classication likelihood (ICL-BIC)
[10]. An important limitation of these criteria, however, is that they are only
valid asymptotically. This implies that there is no nite-sample guarantee when
using AIC, BIC or ICL-BIC, to choose between dierent levels of complexity.
Their use in small samples is, therefore, ad hoc. To overcome such diculties,
[11] proposed a novel approach, called the slope heuristics, supported by a non-
asymptotic oracle inequality. This method leads to an optimal data-driven choice
of multiplicative constants for the penalties. Recent reviews and practical issues
concerning the slope heuristic can be found in [8, 3] and the references given
therein.
It should be emphasized that a general model selection result, originally es-
tablished by [60, Theorem 7.11], ensures a penalized criterion leads to a good
model selection and that the penalty is known only up to multiplicative con-
stants and is proportional to the dimensions of models. In particular, these
multiplicative constants can be calibrated by the slope heuristic approach in a
nite-sample setting. Then, in the spirit of the methods based on concentration
inequalities developed in [60, 61, 22, 23], a number of nite-sample oracle type
inequalities have been established for the least absolute shrinkage and selection
operator (LASSO) [94] and general penalized maximum likelihood estimators
(PMLE). These results include the works for high-dimensional Gaussian graph-
ical models [33], Gaussian mixture model selection [62, 63, 64], nite mixture
regression models [68, 28, 29, 31, 32], and LinBoSGaME models, outside the
high-dimensional setting [70].
1.4. Main contributions
To the best of our knowledge, no attempt has been made in the literature to
develop a nite-sample oracle inequality for the framework of MoE regression
models for high-dimensional data. In this paper, our rst original contribu-
tion is to provide nite-sample oracle inequalities, Theorems 3.1 and 3.2 for
high-dimensional GLoME and BLoME models, respectively. More specically,
we establish non-asymptotic risk bounds that take the form of weak oracle

--- PAGE 6 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 6
inequalities, provided that the lower bounds on the penalties are true, in high-
dimensional GLoME and BLoME models, based on an inverse regression strat-
egy and block-diagonal structures of the Gaussian covariance matrices experts.
Unlike traditional criteria such as AIC, BIC, or ICL-BIC, which are based on
asymptotic theory or a Bayesian approach, our contributed non-asymptotic risk
bounds allow the number nof observations to be xed while the dimensionality
and cardinality of the models, characterized by the number of covariates and
the size of the response, are allowed to grow with respect to nand can be much
larger than n.
In particular, our oracle inequalities show that the Jensen{Kullback{Leibler
loss performance of our PMLEs is comparable to that of oracle models if we
take suciently large constants in front of the penalties, whose shapes are only
known up to multiplicative constants and proportional to the dimensions of the
models. These theoretical justications for the shapes of the penalties motivate
us to make use of the slope heuristic criterion to select several hyperparameters,
including the number of mixture components, the degree of polynomial mean
functions, and the potential hidden block-diagonal structures of the covariance
matrices of the multivariate predictors.
Moreover, in Theorem 3.1, we extends a corollary of [70, Theorem 1], which
can be veried via Lemma 1 from [75], which makes explicit the relationship
between softmax and Gaussian gating classes.
Another signicant contribution is the numerical experiments for simulated
and real data sets in Section 5 that support our theoretical results, and the sta-
tistical study of non-asymptotic model selection in GLoME models. To the best
of our knowledge, instead of using classical asymptotic approaches for model
selection such as AIC, BIC and ICL-BIC, we are the rst to illustrate that
the slope heuristic works well for MoEs with Gaussian experts and normalized
Gaussian gating networks such as GLLiM, BLLiM, GLoME, and BLoME mod-
els. Note that our main objective here is to investigate how well the empirical
tensorized Kullback{Leibler divergence between the true model ( s
0) and the
selected model bs
bmfollows the nite-sample oracle inequality of Theorem 3.1, as
well as the convergence rate O(n 1) of the error upper bound. Therefore, we
focus on 1-dimensional data sets. Beyond the statistical estimation and model
selection purposes considered here, the dimensionality reduction capability of
GLLiM and BLLiM models in high-dimensional regression data, can be found
in [26, Section 6] and [34, Sections 3 and 4], respectively.
Specically, besides the important theoretical issues regarding the tightness of
upper bounds, how to integrate a priori information, and a minimax analysis of
our proposed PMLEs, our nite-sample oracle inequalities and the correspond-
ing illustrative numerical experiments help to partially answer the following two
important questions raised in the eld of MoE regression models: (1) What is
the number of mixture components Kto choose, given the sample size n; and
(2) Whether it is better to use a few complex experts or a combination of many
simple experts, given the total number of parameters. Note that such problems
are also considered in the work of [67, Proposition 1], where the authors provided
some qualitative guidance and only suggested a practical method for choosing

--- PAGE 7 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 7
Kandd, involving a complexity penalty or cross-validation. Furthermore, their
model is only non-regularized maximum-likelihood estimation, and thus is not
suitable for the high-dimensional setting.
Lastly, we emphasize that although the nite-sample oracle inequalities com-
pare performances of our estimators with the best model in the collection, they
also allow us to approximate a rich class of conditional densities if we take
enough clusters and/or a high enough degree dof polynomials in the mean of
the functions of Gaussian experts, in the context of mixture of Gaussian experts
[49, 67, 74, 45, 75]. This leads to the upper bounds of the risks being small.
1.5. Main challenges
For the Gaussian gating parameters, the technique for handling the logistic
weights in the SGaME models of [70] is not directly applicable to the GLoME or
BLoME framework, due to the quadratic form of the canonical link. Therefore,
we have to propose a reparameterization trick1to bound the metric entropy of
the Gaussian gating parameters space; see (4.4) and Lemma 4.5 for more details.
To work with conditional density estimation in the BLLiM and BLoME mod-
els, it is natural to make use of a general conditional density model selection
theorem from [22, 23], see also Theorem 4.1 for its adaption to our context.
However, it is worth mentioning that since the collection of models constructed
by the BLLiM model [34] or by some appropriate procedures for BLoME models
in practice is usually random, we have to utilize a model selection theorem for
MLE among a random subcollection (cf. [29, Theorem 5.1] and [33, Theorem
7.3]).
In particular, our collections of BLoME models must satisfy certain regularity
assumptions, see Section 4.2.3 for details, which are not easy to verify due to the
complexity of BLoME models and technical reasons. For BLoME models, the
main diculty in proving our oracle inequality lies in bounding the bracketing
entropy of the Gaussian gating functions and Gaussian experts with block-
diagonal covariance matrices. To solve the rst problem, we need to use our
previous trick of reparametrizing the Gaussian gating parameter space. For the
second one, based on some ideas of Gaussian mixture models from [41, 62],
we provide a novel extension for standard MoE models with Gaussian gating
networks. Note that our important contributions also extend the recent result on
block-diagonal covariance matrices in [33], which is only developed for Gaussian
graphical models.
In addition, there are two possible ways to decompose the bracketing entropy
of collection models based on dierent distances. As mentioning in Appendix
B.2.1 from [70], their decomposition boils down to assuming that the predictor
domain is bounded. And this limiting assumption can then be relaxed by using a
1Note that we only use this nomenclature to perform a change of variables of the Gaussian
gating parameters space of GLoME models via the logistic weights of SGaME models. This
reparameterization trick does not stand for the well-known one of variational autoencoders
(VAEs) in deep learning literature (see [52], for more details).

--- PAGE 8 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 8
smaller distance, namely a tensorized extension of the Hellinger distance d
nin
(2.22), but bounding the corresponding bracketing entropy becomes much more
dicult. Nevertheless, it is important to point out that we are able to weaken
this limiting assumption by using the smaller distance: d
n, for the bracketing
entropy of our collection of BLoME models, see Lemma 4.10 for more details.
This reinforces our original contributions concerning the control of bracketing
entropy of BLoME models.
1.6. Organization
The rest of this paper is organized as follows. In Section 2, we present the
notation and framework for GLoME, and BLoME models and their special
cases, GLLiM, and BLLiM models. In Section 3, we state the main results of
this paper: nite-sample oracle inequalities satised by the PMLEs. Section 4 is
devoted to the proofs and main mathematical challenges in establishing our new
nite-sample oracle inequalities. We experimentally evaluate our new results in
simulated and real datasets in Section 5. Some conclusions and perspectives
are provided in Section 6. The proofs of technical lemmas can be found in
Appendix A.
2. Notation and framework
From now on, for any L2N?, we write [ L] to denote the set f1;:::;Lg. We
are interested in estimating the law of the random variable Y= (Yj)j2[L],
conditionally on X= (Xj)j2[D]. Here and subsequently, given any n2N?, 
X[n];Y[n]
:= (Xi;Yi)i2[n]denotes a random sample, and xandystands for
the observed values of the random variables XandY, respectively. The following
assumptions will be needed throughout the paper. We assume that the covariates
Xare independent but not necessarily identically distributed. The assumptions
on the responses Yare stronger: conditional on X[n],Y[n]are independent,
and each Yfollows a law with true (but unknown) PDF s0(jX=x), which
is approximated via MoE models. For a matrix A, letm(A) andM(A) be,
respectively, the modulus of the smallest and largest eigenvalues of A.
2.1. GLoME and BLoME models
Motivated by an inverse regression framework, where the role of predictor and
response variables will be exchanged such that Ybecomes the input and Xplays
the role of a multivariate output, we consider GLoME and BLoME models with
inverse conditional probability density functions (PDFs) of the form (2.1). This
construction goes back to the work of [57, 99, 26, 84], and is very useful in a
high-dimensional regression context, where typically DL. In this way, we

--- PAGE 9 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 9
dene PDF of Xconditional on Y=yas follows:
s K;d(xjy) =KX
k=1gk(y;!)D(x;k;d(y);k); (2.1)
gk(y;!) =kL(y;ck; k)PK
j=1jL(y;cj; j) (2.2)
Here,gk(;!) andD(;k;d();k),k2[K],K2N?,d2N?, are called nor-
malized Gaussian gating functions (or Gaussian gating networks) and Gaussian
experts, respectively. Furthermore, we decompose the parameters of the model
as follows: K;d= (!;d;)2
KK;dVK=:	K;d,!= (;c; )2
(K 1CKV0
K) =: 
K,= (k)k2[K],c= (ck)k2[K], = ( k)k2[K],
d= (k;d)k2[K]2K;d, and = (k)k2[K]2VK. Note that K 1=n
(k)k2[K]2(R+)K;PK
k=1k= 1o
is aK 1 dimensional probability simplex,
CKis a set ofK-tuples of mean vectors of size L1,V0
Kis a set ofK-tuples
of elements inS++
L, whereS++
Ldenotes the collection of symmetric positive
denite matrices on RL,K;dis a set ofK-tuples of mean functions from RL
toRDdepending on a degree d(e.g., a degree of polynomials), and VKis a
set containing K-tuples fromS++
D.
Recall that GLLiM and BLLiM models are ane instances of GLoME and
BLoME models, and are especially useful for high-dimensional regression data,
since there exist link functions between the inverse and forward conditional den-
sity; see Figure 1 for comprehensive classication and nomenclature of standard
MoE regression models with Gaussian gating networks. Note that the principle
of inverse regression is only useful when the functions k;d(y) are linear, be-
cause there is no explicit way to express the law of YjXfrom that of XjYfor
higher degree of polynomials. However, to have more consistent notations with
the previous ane results of GLLiM, BLLiM models from [26, 34], we decide to
use the inverse regression frameworks instead of the forward one.
In the BLoME model, we wish to make use of block-diagonal covariance
structures by replacing kandVKwithk(Bk) and VK(B), dened in (2.3),
respectively (see, e.g., [34, 33]). This block-diagonal structures for covariance
matrices are not only used to trade-o between complexity and sparsity but
is also motivated by some real applications, where we want to perform predic-
tion on data sets with heterogeneous observations and hidden graph-structured
interactions between covariates; for instance, for gene expression data sets in
which conditionally on the phenotypic response, genes interact with few other
genes only, i.e., there are small modules of correlated genes (see [34, 33] for
more details). To be more precise, for k2[K], we decompose k(Bk) intoGk
blocks,Gk2N?, and we denote by d[g]
kthe set of variables into the gth group,
forg2[Gk], and by card
d[g]
k
the number of variables in the corresponding
set. Then, we dene Bk=
d[g]
k
g2[Gk]to be a block structure for the cluster
k, and B= (Bk)k2[K]to be the covariate indexes into each group for each clus-

--- PAGE 10 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 10
Gaussian g ating  
networks
Gaussian expert  
 means  
1-block c ovariance
matrices
Affine
Multi-block-diagonal
covariance matrices
GLLiM
 BLLiM
1-block c ovariance
matrices
Multi-block-diagonal
covariance matrices
GLoME
 BLoME
Linear combination
of  
bounded functions  
GLLiM : Gaussian Locally- Linear Mapping
BLLiM : Block-diagonal covariance Ga ussian Locally- Linear MappingGLoME : Gaussian-gated Localized  MoE
BLoME : Block-diagonal covariance Gaussian-gated Localized MoE
Figure 1: A comprehensive classication and nomenclature of standard MoE
regression models with Gaussian gating networks.
ter. In this way, to construct the block-diagonal covariance matrices, up to a
permutation, we make the following denition: VK(B) = (Vk(Bk))k2[K], for
everyk2[K],
Vk(Bk) =8
>>>>>>><
>>>>>>>:k(Bk)2S++
Dk(Bk) =Pk0
BBBB@[1]
k0::: 0
0 [2]
k::: 0
0 0...0
0 0:::[Gk]
k1
CCCCAP 1
k;
[g]
k2S++
card
d[g]
k;8g2[Gk]9
>>>>>>>=
>>>>>>>;:
(2.3)
Here, Pkcorresponds to the permutation leading to a block-diagonal matrix in
clusterk. It is worth pointing out that outside the blocks, all coecients of the
matrix are zeros and we also authorize reordering of the blocks: e.g.,f(1;3) ; (2;4)g
is identical tof(2;4) ; (1;3)g, and the permutation inside blocks: e.g., the par-
tition of 4 variables into blocks f(1;3) ; (2;4)gis the same as the partition
f(3;1) ; (4;2)g.
It is interesting to point out that the BLLiM framework aims to model a sam-

--- PAGE 11 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 11
ple of high-dimensional regression data issued from a heterogeneous population
with hidden graph-structured interaction between covariates. In particular, the
BLLiM model is considered as a good candidate for performing model-based
clustering and for predicting the response in situations aected by the \curse of
dimensionality" phenomenon, where the number of parameters could be larger
than the sample size. Indeed, to deal with high-dimensional regression prob-
lems, the BLLiM model is based on an inverse regression strategy, which in-
verts the role of the high-dimensional predictor and the multivariate response.
Therefore, the number of parameters to estimate is drastically reduced. More
precisely, BLLiM utilizes GLLiM, described in [25, 26], in conjunction with a
block-diagonal structure hypothesis on the residual covariance matrices, to make
a trade-o between complexity and sparsity. This prediction model is fully para-
metric and highly interpretable. For instance, it can be used for the analysis of
transcriptomic data in molecular biology to classify observations or predict phe-
notypic states; see e.g., [42, 71, 56]. Indeed, if the predictor variables are gene
expression data measured by microarrays or by the RNA-seq technologies, and
the response is a phenotypic variable, the BLLiM model not only provides clus-
ters of individuals based on the relation between gene expression data and the
phenotype, but also implies a gene regulatory network specic to each cluster
of individuals (see [34] for more details).
In order to establish our nite-sample oracle inequalities, Theorems 3.1 and 3.2,
we need to explicitly impose some classical boundedness conditions on the pa-
rameter space.
2.1.1. Gaussian gating networks
We shall restrict our study to bounded Gaussian gating parameter vectors.
Specically, we assume that there exist deterministic positive constants a;Ac;a ;A ,
such that!belongs toe
K, where
e
K=
!2
K:8k2[K];kckk1Ac;
a m( k)M( k)A ;ak
: (2.4)
2.1.2. Gaussian experts
Following the same structure for the Gaussian mean experts from [70], the set
K;dwill be chosen as a tensor product of compact sets of moderate dimen-
sion ( e.g., a set of polynomials of degree smaller than d, whose coecients are
smaller in absolute values than T). Then, K;dis dened as a linear combi-
nation of a nite set of bounded functions whose coecients belong to a com-
pact set. This general setting includes polynomial bases when the covariates are
bounded, Fourier bases on an interval, as well as suitably renormalized wavelet

--- PAGE 12 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 12
dictionaries. More specically, K;d=
k2[K]k;d=:K
k;d, where k;d=b;d,
8k2[K], and
b;d=8
<
:y7! dX
i=1(j)
i';i(y)!
j2[D]=: (d;j(y))j2[D]:kk1T9
=
;:
(2.5)
Here, the subscript bstands for "bounded functions", d2N?;T2R+, and
(';i)i2[d]is a collection of bounded functions on Y. In particular, we focus on
the boundedYcase and assume that Y= [0;1]L, without loss of generality. In
this case,';ican be chosen as monomials with maximum (non-negative) degree
d:yr=QL
l=1yrl
l. Recall that a multi-index r= (rl)l2[L];rl2N?[f0g;8l2[L],
is anL-tuple of nonnegative integers. We dene jrj=PL
l=1rland the number
jrjis called the order or degree of yr. Then, K;d=K
p;d, where
p;d=8
><
>:y7!0
@dX
jrj=0(j)
ryr1
A
j2[D]=: (d;j(y))j2[D]:kk1T9
>=
>;:(2.6)
Here, the subscript pstands for "polynomial".
For GLoME models, note that any covariance matrix kcan be decomposed
into the form BkPkAkP>
k, such that Bk=jkj1=Dis a positive scalar corre-
sponding to the volume, Pkis the matrix of eigenvectors of kandAkthe diag-
onal matrix of normalized eigenvalues of k;B 2R+;B+2R+,A( ;+) is
a set of diagonal matrices Ak, such thatjAkj= 1 and8i2[D]; (Ak)i;i
+, where ;+2R; andSO(D) is the special orthogonal group of dimension
D. In this way, we obtain what is known as the classical covariance matrix sets
described by [20] for Gaussian parsimonious clustering models, dened by
VK=n 
BkPkAkP>
k
k2[K]:8k2[K];B BkB+;
Pk2SO(D);Ak2A( ;+)o
: (2.7)
For the block-diagonal covariances of Gaussian experts from BLoME models,
we assume that there exist some positive constants mandMsuch that, for
everyk2[K],
eVK(B) =
(k(Bk))k2[K]2VK(B) :
0<mm(k(Bk))M(k(Bk))M
:(2.8)
Note that this is a typical assumption and is also used in the block-diagonal
covariance selection for Gaussian graphical models of [33].
Next, characterizations of GLLiM and BLLiM models are provided in Sec-
tion 2.2.

--- PAGE 13 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 13
2.2. High-dimensional regression via GLLiM and BLLiM models
The GLLiM and BLLiM models, as originally introduced in [26, 34], are used to
capture the nonlinear relationship between the response and the set of covariates
from a high-dimensional regression data, imposed by a potential hidden graph
structured interaction, typically in the case when DL, by theKlocally ane
mappings:
Y=KX
k=1I(Z=k) (A
kX+b
k+E
k): (2.9)
Here, Iis an indicator function and Zis a latent variable capturing a cluster
relationship, such that Z=kifYoriginates from cluster k2[K]. Cluster
specic ane transformations are dened by matrices A
k2RLDand vectors
b
k2RL. Furthermore, E
kare error terms capturing both the reconstruction
error due to the local ane approximations and the observation noise in RL.
Following the common assumption that E
kis a zero-mean Gaussian variable
with covariance matrix 
k2RLL, it holds that
p(Y=yjX=x;Z=k; 
K) = L(y;A
kx+b
k;
k); (2.10)
where we denote by  
Kthe vector of model parameters and  Lis the PDF of
a Gaussian distribution of dimension L. In order to allow the ane transfor-
mations to be local, Xis dened as a mixture of KGaussian components as
follows:
p(X=xjZ=k; 
K) = D(x;c
k; 
k);p(Z=k; 
k) =
k; (2.11)
where c
k2RD; 
k2RDD,= (
k)k2[K]2
K 1, and 
K 1is theK 1
dimensional probability simplex. Then, according to formulas for conditional
multivariate Gaussian variables and the following hierarchical decomposition
p(Y=y;X=x; 
K) =KX
k=1
kD(x;c
k; 
k) L(y;A
kx+b
k;
k);
we obtain the following forward conditional density [26]:
p(Y=yjX=x; 
K) =KX
k=1
kD(x;c
k; 
k)PK
j=1
jD 
x;c
j; 
jL(y;A
kx+b
k;
k);
(2.12)
where 
K= (;
K)2K 1
K=:	
K. Here,
K= (c
k; 
k;A
k;b
k;
k)k2[K]
and

K= 
RDS++
D(R)RLDRLS++
L(R)K:

--- PAGE 14 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 14
Without assuming anything on the structure of parameters, the dimension of
the model, denoted by dim ( ), is dened as the total number of parameters that
has to be estimated, as follows:
dim (	
K) =K
1 +D(L+ 1) +D(D+ 1)
2+L(L+ 1)
2+L
 1:
It is worth mentioning that dim ( 	K) is typically very large compared to
the sample size (see, e.g., [26, 34, 84] for more details in their real data sets)
wheneverDis large and DL. Furthermore, it is more realistic to make
assumption on the residual covariance matrices 
kof error vectors E
krather
than on  
k(cf. [26, Section 3]). This justies the use of the inverse regression
trick from [26], which leads a drastic reduction in the number of parameters to
be estimated.
More specically, in (2.12), the roles of input and response variables were
exchanged such that Ybecomes the covariates and Xplays the role of the
multivariate response. Therefore, its corresponding inverse conditional density
is dened as a Gaussian locally-linear mapping (GLLiM) model, based on the
previous hierarchical Gaussian mixture model, as follows:
p(Z=k; k) =k; (2.13)
p(Y=yjZ=k; K) = L(y;ck; k); (2.14)
p(X=xjY=y;Z=k; K) = D(x;Aky+bk;k); (2.15)
p(X=xjY=y; K) =KX
k=1kL(y;ck; k)PK
j=1jL(y;cj; j)D(x;Aky+bk;k);
(2.16)
where kis aDDcovariance structure (usually diagonal in GLLiM models,
chosen to reduce the number of parameters) automatically learned from data
and Kis the set of parameters, denoted by  K= (;K)2K 1K=:
	K. It is important to note that the BLLiM model imposes block-diagonal
structures on ( k)k2[K]to make a trade-o between complexity and sparsity.
The following interesting feature of both GLLiM and BLLiM models is proved
in Appendix A.1.
Lemma 2.1. The parameter  
Kin the forward conditional PDF, dened in
(2.12) , can then be deduced from  Kin(2.16) via the following one-to-one
correspondence:
K=0
BBBB@ck
 k
Ak
bk
k1
CCCCA
k2[K]7!0
BBBB@c
k
 
k
A
k
b
k

k1
CCCCA
k2[K]=0
BBBB@Akck+bk
k+Ak kA>
k

kA>
k 1
k

k(  1
kck A>
k 1
kbk) 
  1
k+A>
k 1
kAk 11
CCCCA
k2[K]2
K;
(2.17)
with.

--- PAGE 15 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 15
2.3. Collection of GLoME and BLoME models
For GLoME, we only need to choose the degree of polynomials dand the number
of components Kamong nite sets D= [dmax] andK= [Kmax], respectively,
wheredmax2N?andKmax2N?may depend on the sample size n. We wish
to estimate the unknown inverse conditional density s0by inverse conditional
densities belonging to the following collection of models ( Sm)m2M. Here, we
deneM=f(K;d) :K2K;d2Dg, and
Sm=n
(x;y)7!s K;d(xjy) =:sm(xjy) :
 K;d= (!;d;)2e
KK;dVKo
; (2.18)
wheres K;d,e
K,K;dandVKare dene previously in (2.1), (2.4), (2.6) (or
more general (2.5)) and (2.7), respectively.
While for the BLoME model, we also need to select Bfrom a list of can-
didate structures ( Bk)k2[K](B)k2[K], whereBdenotes the set of all pos-
sible partitions of the covariables indexed by [ D], for each cluster of individ-
uals.The collection of BLoME models is dened as follows: ( Sm)m2M,M=n
(K;d;B) :K2K;d2D;B2(B)k2[K]o
,
Sm=
(x;y)7!s K;d;B(xjy) :=sm(xjy) :
 K;d;B= (!;d;(B))2e
KK;deVK(B)
;(2.19)
wheres K;d;B,e
K,K;d, andeVK(B) are dened in (2.1), (2.4), (2.6) (or more
generally (2.5)), and (2.8), respectively. In theory, we would like to consider the
whole collection of models ( Sm)m2M. However, the cardinality of Bis large;
its size is a Bell number. Even for a moderate number of variables D, it is
not possible to explore the set B, exhaustively. We restrict our attention to a
random subcollection BRof moderate size. For example, we can consider the
BLLiM procedure from [34, Section 2.2].
Remark 2.2. It is worth noting that we can also dene the collection of the
forward GLoME and BLoME models in the same framework as in (2.18) and
(2.19), respectively. For example, in GloME models, the unknown forward con-
ditional density s
0is estimated via the following collection of forward models
(S
m)m2M, withM=KD , and
S
m=n
(x;y)7!s 
K(yjx) =:s
m(yjx) :
 
K= (!;
d;)2e

K
K;dV
K=:e	
K;do
;(2.20)
wheree

K,
K;dandV
Kare dened similar to (2.4), (2.6) (or more general
(2.5)) and (2.7), respectively.

--- PAGE 16 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 16
Motivated by the inverse conditional densities (2.16) and the one-to-one cor-
respondence in Lemma 2.1, for the sake of simplicity of notation, we only state
our main Theorems 3.1 and 3.2 for the collection of inverse models ( Sm)m2M
in (2.18) and (2.19), respectively. However, our nite-sample oracle inequalities,
Theorems 3.1 and 3.2 can be applied to the forward model ( S
m)m2M, estab-
lished in (2.20), if we consider yandxas realizations of predictors and response
variables, respectively.
2.4. Loss functions and penalized maximum likelihood estimator
In the maximum likelihood approach, the Kullback{Leibler divergence is the
most natural loss function, which is dened for two densities sandtby
KL(s;t) =(R
RDln
s(y)
t(y)
s(y)dy ifsdyis absolutely continuous w.r.t. tdy;
+1 otherwise:
However, to take into account the structure of conditional densities and the ran-
dom covariates 
Y[n]
, we consider the tensorized Kullback{Leibler divergence
KL
n, dened as:
KL
n(s;t) =EY[n]"
1
nnX
i=1KL (s(jYi);t(jYi))#
; (2.21)
ifsdyis absolutely continuous w.r.t. tdy, and +1otherwise. Note that if the
predictors are xed, this divergence is the classical xed design type divergence
in which there is no expectation. We refer to our result as a weak oracle inequal-
ity, because its statement is based on a smaller divergence, when compared to
KL
n, namely the tensorized Jensen-Kullback{Leibler divergence :
JKL
n
(s;t) =EY[n]"
1
nnX
i=11
KL (s(jYi);(1 )s(jYi) +t(jYi))#
;
with2(0;1). We note that JKL
n
was rst used in [22, 23]. However, a version
of this divergence appears explicitly with =1
2in [60], and it is also found
implicitly in [12]. This loss is always bounded by1
ln1
1 but behaves like KL
n,
whentis close tos. The main tools in the proof of such a weak oracle inequality
are deviation inequalities for sums of random variables and their suprema. These
tools require a boundedness assumption on the controlled functions which is not
satised by lnsm
s0, and thus also not satised by KL
n. Therefore, we consider
instead the use of JKL
n
. In particular, in general, it holds that Cd2
n
JKL
n
KL
n, whereC=1
min
1 
;1
ln
1 +
1 
 
(see [22, Prop.
1]) and d2
nis a tensorized extension of the squared Hellinger distance d2
n,
dened by
d2
n(s;t) =EY[n]"
1
nnX
i=1d2(s(jYi);t(jYi))#
: (2.22)

--- PAGE 17 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 17
Moreover, if we assume that, for any m2M and anysm2Sm;s0dsmd,
then (see [70, 22])
C
2 + lnks0=smk1KL
n(s0;sm)JKL
n
(s0;sm): (2.23)
In the context of MLE, given the collection of conditional densities Sm, we
aim to estimate s0by the conditional density bsmthat maximizes the likeli-
hood (conditionally to ( yi)i2[n]) or equivalently that minimizes the negative
log-likelihood (NLL), which we can write as:
bsm= argmin
sm2SmnX
i=1 ln [sm(xijyi)]:
We should work with almost minimizer of this quantity and dene a -log-
likelihood minimizer as any bsmthat satises:
nX
i=1 ln [bsm(xijyi)]inf
sm2SmnX
i=1 ln [sm(xijyi)] +; (2.24)
where the error term is necessary to avoid any existence issue, e.g., the in-
mum may not be reached. Roughly speaking, the Ekeland variational principle
asserts that, for any extended-valued lower semicontinuous function which is
bounded below, one can add a small perturbation to ensure the existence of the
minimum, see e.g., [14, Chapter 2]. This framework is also used in [22, 23, 70].
As always, using the NLL of the estimate in each model as a criterion is not
sucient. It is an underestimation of the risk of the estimate and this leads to
choosing models that are too complex. In the context of PMLE, by adding a
suitable penalty pen( m), one hopes to create a trade-o between good data t
and model complexity. For a given choice of pen( m), the selected model Sbmis
chosen as the one whose index is an 0-almost minimizer of the sum of the NLL
and this penalty:
nX
i=1 ln [bsbm(xijyi)] + pen (bm)inf
m2M(nX
i=1 ln [bsm(xijyi)] + pen( m))
+0:
(2.25)
Note thatbsbmis then called the 0-penalized likelihood estimate and depends on
both the error terms and0. From hereon in, the term selected model (estimate)
or best data-driven model (estimate) is used to indicate that it satises the
denition in (2.25).
3. Main results of nite-sample oracle inequalities
3.1. Deterministic collection of GLoME models
Theorem 3.1 provides a lower bound on the penalty function, pen( m), which
guarantees that the PMLE selects a best data-driven model that performs al-
most as well as the best model. We highlight our main contribution and briey

--- PAGE 18 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 18
establish the proof of Theorem 3.1 in Section 4.1.3. A more detail technical
proof can be found in Appendices A.2 and A.3.
Theorem 3.1 (Finite-sample oracle inequality for GLoME models) .Assume
that we observe 
x[n];y[n]
, arising from an unknown conditional density s0.
Given a collection of GLoME models, (Sm)m2M, there is a constant Csuch
that for any 2(0;1), for any m2M ,zm2R+, =P
m2Me zm<1and
anyC1>1, there is a constant depending only on andC1, such that if for
every index m2M ,
pen(m)>[(C+ lnn) dim (Sm) +zm]; (3.1)
then the0-penalized likelihood estimate bsbm, dened in (2.24) and(2.25) , satis-
es
EX[n];Y[n]
JKL
n
(s0;bsbm)
C1inf
m2M
inf
sm2SmKL
n(s0;sm) +pen(m)
n
+C1
n++0
n: (3.2)
It is worth noting that Theorem 3.1 extends a corollary of [70, Theorem 1],
which can be veried via Lemma 1 from [75], which makes explicit the relation-
ship between softmax and Gaussian gating classes.
Note that we only aim to construct upper bounds and do not focus on the
important question of the existence of the corresponding lower bounds or mini-
max adaptive estimation. However, as a special case of GLoME models, [62, 64]
consider a collection of univariate Gaussian mixture models having the same and
known component variance. They show that the PMLE bsbmis minimax adaptive
to the regularity , >0, of univariate density classes Hwhose logarithm of
the elements is locally -H older, with convergence rate n 2
2+1up to a power
of ln(n). Although this result is stated for the Hellinger risk, it remains valid
for the Kullback{Leibler divergence if we further assume that ln ( ks=tk1) is
uniformly bounded on [m2MSm, see e.g., Lemma 7.23 in [60]. Note that this
assumption guarantees that the Kullback{Leibler divergence and the Hellinger
distance are equivalent.
A special case of GLoME models, namely model-selection (nearly-D-sparse)
aggregation in mixture models, is considered by [17, 9, 89, 18, 24] and is re-
lated to our model selection results. More precisely, the authors of [89] did not
consider PMLEs with Kullback-Leibler loss but only with L2-loss or aggrega-
tion of a nite number of densities. Similarly, the results from [17, 9] dealt
with theL2-loss and investigate the Lasso and the Dantzig estimators, respec-
tively, suitably adapted to the problem of density estimation. With respect to
the Kullback{Leibler loss, [18] established model selection type oracle inequal-
ities with high probability rather than in expectation. In particular, a bound
in expectation with Kullback{Leibler loss in [24] is perhaps the most relevant
reference to our result. They established exact oracle inequalities with a rate-
optimal remainder term ((ln K)=n)1=2, up to a possible logarithm correction, in

--- PAGE 19 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 19
the problem of convex aggregation when the number Kof components is larger
thann1=2. It is worth noting that they did not consider PMLEs as we do or as
[62, 64], see e.g., (2.25). Instead, the constraint that the weight vector in the
mixture model belongs to the probabilistic simplex acts as a sparsity-inducing
penalty. However, in their collection of mixture models, the mixture components
are deterministic and chosen from a dictionary obtained on the basis of previ-
ous experiments or expert knowledge rather estimated from data. The adjective
exact refers to the fact that the \bias term" KL
n(s0;sm) is not multiplied by
factor strictly larger than one as in our Theorem 3.1.
To the best of our knowledge, providing a minimax analysis for the PMLEs
or the problem of model-selection (nearly-D-sparse) aggregation in the context
of standard MoE models is still an open question. In particular, it is not trivial
to extend such optimal risk bounds regarding Gaussian mixtures, see e.g., [64,
Theorem 2.8] or [24, Theorems 2.3 and 3.1], to standard MoE models. However,
we wish to provide such a minimax analysis in future research.
In contract to Theorem 3.1, in Theorem 3.2, we consider a random subcollec-
tion of models fM, included in the whole collection M. This is particularly useful
in a high-dimensional context where we cannot test all the models. Therefore,
we have to restrict ourselves to a smaller subcollection of models, which is then
potentially random.
3.2. Random subcollection of BLoME models
Note that the constructed collection of models with block-diagonal structures
for each cluster of individuals is designed, for example, by the BLLiM procedure
from [34], where each collection of partitions is sorted by sparsity level. Nev-
ertheless, our nite-sample oracle inequality, Theorem 3.2, still holds for any
random subcollection of M, which is constructed by some suitable tools in the
framework of BLoME regression models. We highlight our main contribution
and briey establish the proof of Theorem 3.2 in Section 4.2. A more detailed
technical proof can be found in Appendices A.4 and A.5 and Section 4.2.3.
Theorem 3.2 (Finite-sample oracle inequality for BLoME models) .Let(x[n];y[n])
be observations coming from an unknown conditional density s0. For each m=
(K;d;B)2(KD B)M , letSmbe dene by (2.19) . Assume that there
exists > 0andKL>0such that, for all m2M , one can nd sm2Sm,
such that
KL
n(s0;sm)inf
t2SmKL
n(s0;t) +KL
n;andsme s0: (3.3)
Next, we construct some random subcollections (Sm)m2fMof(Sm)m2Mby let-
tingfM 
KD BR
M , whereBRis a random subcollection B, of
moderate size. Consider the collection (bsm)m2fMof-log likelihood minimizers
satisfying (2.24) for all m2fM. Then, there is a constant Csuch that for any
2(0;1), and anyC1>1, there are two constants andC2depending only on

--- PAGE 20 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 20
andC1such that, for every index, m2M ,zm2R+, =P
m2Me zm<1
and
pen(m)[(C+ lnn) dim(Sm) + (1_)zm]; (3.4)
the0-penalized likelihood estimator bsbm, dened as in (2.25) on the subset fM
M, satises
EX[n];Y[n]
JKL
n
(s0;bsbm)
C1EX[n];Y[n]"
inf
m2fM
inf
t2SmKL
n(s0;t) + 2pen(m)
n#
+C2(1_)2
n+0+
n: (3.5)
It is worth mentioning that the comparison of the two inequalities in The-
orems 3.1 and 3.2 involves the following major dierences. First, to control
the random subcollection, in Theorem 3.2, we further assume condition (3.3).
However, this is not a strong assumption and is satised, for example, if s0is
bounded, with a compact support. Assumption (3.3) is needed because we con-
sider a random subcollection from the whole collection. Thanks to this assump-
tion, we can utilize Bernstein's inequality to control the additional randomness.
It is important to stress that the parameter depends on the true unknown den-
sitys0and cannot be explicitly determined for this reason. It appears not only
in the oracle type inequality (3.5), but also in the penalty term (3.4). However,
in some special cases of BLoME models, for instance nite mixture regression
models, we can construct a larger penalty independent of to obtain an oracle
type inequality but the price to pay for getting rid of in the risk bound is the
increased value of error upper bound, see Appendix C in [32] for more details.
Second, the constant  associated to the Kraft-type inequality for the collection
appears squared in the upper bound of the oracle inequality in Theorem 3.2.
This is because of the random subcollection fMofM, if the model collection is
xed, we get a linear bound as in Theorem 3.1.
3.3. Practical application of nite-sample oracle inequalities
It is important to mention that Theorems 3.1 and 3.2 provide some theoretical
justication regarding the penalty shapes when using the slope heuristic for our
collection of MoE models, including GLLiM, GLoME, BLLiM, BLoME models.
More precisely, our oracle inequalities show that the performance in JKL
n

loss of our PMLEs are roughly comparable to that of oracle models if we take
large enough constants in front of the penalties, whose forms are only known up
to multiplicative constants and proportional to the dimensions of models. This
motivate us to make use of the slope heuristic criterion to select our data-driven
hyperparameters, including the number of mixture components, the degree of
polynomial Gaussian expert's mean functions, and the potential hidden block-
diagonal structures of the covariance matrices of the multivariate predictor, see
more in Section 5.

--- PAGE 21 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 21
To be more precise, we consider the condition (3.6) for GLoME models. As
shown in the proof of Theorem 3.1, in fact we can replace the assumption on
pen(m) by a milder one. More precisely, given a constant C, which we specify
later, there is a constant depending only on andC1, such that for every
index m2M , pen( m) is bounded from below by
0
B@dim(Sm)0
B@2p
C+p2
+0
B@lnn
p
C+p2
dim (Sm)1
CA
+1
CA+zm1
CA:
(3.6)
In our numerical experiment, namely, Figures 3 and 4, we conrm the validity
of the linear penalty shape assumption, which supports the use of penalties
proportional to the dimension. This implies that the logarithm terms are not
detected in practice as shown in Figures 3 and 4 and thus only the preponderant
term in dim ( Sm) is retained in the penalty form.
In particular, based on Appendix B.4 from [22], we can make explicit the
dependence of the theoretical constant , with respect to andC1as follows. For
instance, in Theorem 3.1, given any 2(0;1) andC1>1, denepen= 1 1
C1.
Thenis determined by
=0
0(0
1+0
2)2 r
1 +72Cpen
0
0(0
1+0
2)2+ 1!
2Cpen+18
;
wheredis a given positive constant and
0
0=2 (2 +d)
1 +d;0
1=1p
(1 )
30
3p
2 + 12 + 16r1 

;
0
327;0
2=1p
(1 ) 
42 +3
4p
0
0!
:
For example, if =1
2,C1= 2,d= 1,0
3= 27, then pen= 1 1
C1=1
2,
0
0= 3,0
1= 56 + 162p
2,0
2= 84 +p
3
2, and
C=1
min1 
;1
ln
1 +
1 
 
= 2 ln 2 1;
2126069: (3.7)
According to the previous example, we can see that the theoretical penalty is
lower bounded by , which can be large in practice. This result is not surprising
since, according to [22, Appendix B.4, page 40, line 7], if we choose dsmall
enough then scales proportionally to
1
C(1 )pen=
(1 )2
ln
1 +
1 
 

--- PAGE 22 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 22
and thus explodes to + 1whengoes to 1 and C1goes to 1. Therefore, it is
important to study a natural question as to whether the constant appearing in
the penalty can be estimated from the data without losing theoretical guaranties
on the performance? No denite answer exists so far for MoE regression mod-
els, however at least our numerical experiment in Section 5 shows that the slope
heuristic proposed by [11] may lead to a good practical solution. In particular,
we seek to mathematically and fully justify the slope heuristic in MoE regression
models as in least-squares regression on a random (or xed) design with regres-
sogram (projection) estimators, respectively, see [11, 5, 4, 3] for greater details.
Furthermore, as is often the case in empirical processes theory, the constant 
appearing in the bound is pessimistic as shown in (3.7). Numerical experiments
in Section 5 show that there is a hope that this is only a technical issue. For
instance, Figures 5 and 6 show that the practical values of , selecting by slope
heuristic, are generally very small compared to the theoretical values.
4. Proofs of nite-sample oracle inequalities and main mathematical
challenges
4.1. Proof for deterministic collections of GLoME models
The deterministic collection of MoE models include GLLiM, GLoME, SGaME
and LinBoSGaME models, where nite-sample oracle inequalities have only been
well studied for the latter two models [22, Theorem 2], see also [23, Theorem
2.2.] and [70, Theorem 2]. We rst summarize this general model selection the-
orem and the techniques that [70] used to control the bracketing entropy of
LinBoSGaME models with softmax gating networks in Sections 4.1.1 and 4.1.2,
respectively. Then, we explain why such techniques can not be directly applied
to our collection of GLoME models via Section 4.1.3 and propose our approach
to highlight the main issues and our contributions.
4.1.1. A general conditional density model selection theorem for deterministic
collection of models
Before stating a general model selection theorem for conditional densities, we
have to present some regularity assumptions.
First, we need an information theory type assumption to control the com-
plexity of our collection. We assume the existence of a Kraft-type inequality for
the collection [60, 6].
Assumption 4.1 (K).There is a family (zm)m2Mof non-negative numbers
and a real number such that
 =X
m2Me zm<+1:
For technical reasons, a separability assumption, always satised in the set-
ting of this paper, is also required. It is a mild condition, which is classical

--- PAGE 23 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 23
in empirical process theory [97, 96]. Assumption 4.2 allows us to work with a
countable subset.
Assumption 4.2 (Sep).For every model Smin the collection (Sm)m2M, there
exists some countable subset S0
mofSmand a setX0
mwith(XnX0
m) = 0 ,
wheredenotes Lebesgue measure, such that for every t2Sm, there exist some
sequences (tk)k1of elements of S0
m, such that for every y2 Y and every
x2X0
m;ln (tk(xjy))k!+1     ! ln (t(xjy)).
Next, recall that the bracketing entropy of a set Swith respect to any distance
d, denoted byH[];d((;S)), is dened as the logarithm of the minimal number
N[];d(;S) of brackets [ t ;t+] coveringS, such that d(t ;t+). That is,
N[];d(;S) := min(
n2N?:9
t 
k;t+
k
k2[n]s.td(t 
k;t+
k);Sn[
k=1
t 
k;t+
k)
:
(4.1)
Here,s2
t 
k;t+
k
means that t 
k(x;y)s(x;y)t+
k(x;y),8(x;y)2XY .
We also need the following important Assumption 4.3 on Dudley-type integral
of these bracketing entropies, which is often utilized in empirical process theory
[97, 96, 53].
Assumption 4.3 (H).For every model Smin the collectionS, there is a non-
decreasing function msuch that7!1
m()is non-increasing on (0;1)and
for every2R+,
Z
0q
H[];d
n(;Sm(es;))dm();
whereSm(es;) =fsm2Sm: d
n(es;sm)g. The model complexity of Smis
then dened asDm=n2
m, wheremis the unique root of1
m() =pn.
Observe that the model complexity does not depend on the bracketing en-
tropies of the global models Sm, but rather on those of smaller localized sets
Sm(es;). We are now able to state an important weak oracle inequality, Theo-
rem 4.1, originally from [22, 23, Theorem 2].
Theorem 4.1. Assume that we observe 
x[n];y[n]
, arising from an unknown
conditional density s0. LetS= (Sm)m2Mbe an at most countable conditional
density model collection. Assume that Assumption 4.1 (K), Assumption 4.2
(Sep), and Assumption 4.3 (H) hold for every model Sm2S. Then, for any
2(0;1)and anyC1>1, there is a constant depending only on andC1,
such that for every index m2M ,
pen(m) 
n2
m+zm
withmis the unique root of1
m() =pn, such that the 0-penalized likelihood

--- PAGE 24 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 24
estimatorbsbmsatises
EX[n];Y[n]
JKL
n
(s0;bsbm)
C1inf
m2M
inf
sm2SmKL
n(s0;sm) +pen(m)
n
+C1
n++0
n: (4.2)
For the sake of generality, Theorem 4.1 is relatively abstract. Since the as-
sumptions of Theorem 4.1 are as general as possible. But from the practical
point of view, a natural question is the existence of interesting model collec-
tions that satisfy these assumptions. We will sketch the proof for collection of
LinBoSGaME models from [70, Theorem 1] and show that their result is not
directly applicable to the GLoME setting. The main reason is that the technique
for handling the linear combination of bounded functions for the weight func-
tions of logistic schemes of [70] is not valid for the Gaussian gating parameters
in GLoME models. Therefore, we propose a reparameterization trick to bound
the metric entropy of the Gaussian gating parameters space; see Section 4.1.3
for more details.
4.1.2. Sketch of the proof for LinBoSGaME models
To prove the main conditional density model selection theorem for LinBoSGaME
models, [70, Theorem 1], the authors have to make use of Theorem 4.1. Then,
they need to prove that their collection of LinBoSGaME models have to satisfy
Assumption 4.1 (K), Assumption 4.2 (Sep), and Assumption 4.3 (H). However,
they did not verify Assumption 4.1 (K) and Assumption 4.2 (Sep) and consid-
ered them as primative assumptions on their LinBoSGaME models because of
the complexity of LinBoSGaME models and technical reasons. Therefore, the
main diculty remains on verifying Assumption 4.3 (H) via bracketing entropy
controls of the linear combination of bounded functions for the weight functions
of logistic schemes.
Firstly, they dene the following distance over conditional densities:
sup
ydx(s;t) = sup
y2YZ
Xp
s(xjy) p
t(xjy)2
dx1=2
:
This leads straightforwardly to d2
n(s;t)supyd2
x(s;t). Then, they also dene
sup
ydk(g;g0) = sup
y2Y KX
k=1p
gk(y) q
g0
k(y)2!1=2
;
for any gating functions gandg0. To this end, given any densities sandtover
X, the following distance, depending on y, is constructed as follows:
sup
ymax
kdx(s;t) = sup
y2Ymax
k2[K]dx(sk(;y);tk(;y))
= sup
y2Ymax
k2[K]Z
Xp
sk(x;y) p
tk(x;y)2
dx1=2
:

--- PAGE 25 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 25
Then, they prove that denition of complexity of model Smin Assumption 4.3
(H) is related to an classical entropy dimension with respect to a Hellinger type
divergence d
n, due to Proposition 4.2.
Proposition 4.2 (Proposition 2 from [22]) .For any2(0;p
2], such that
H[];d
n(;Sm)dim(Sm) 
Cm+ ln 1

, the function
m() =p
dim (Sm) p
Cm+p+s
ln1
min (;1)!
satises Assumption 4.3 (H). Furthermore, the unique solution mof1
m() =pn, satises
n2
mdim(Sm)0
@2p
Cm+p2
+ 
lnn
 pCm+p2dim (Sm)!
+1
A:
Therefore, Proposition 4.2 implies that Assumption 4.3 (H) can be proved
via Lemma 4.3.
Lemma 4.3. For any2(0;p
2], the collection of LinBoSGaME models, S=
(Sm)m2M, satises
H[];d
n(;Sm)dim(Sm)
Cm+ ln1

:
Lemma 4.3 is then obtained by decomposing the entropy terms between
the softmax gating functions and the Gaussian experts. Note that both Lin-
BoSGaME and GLoME models share the same structures of Gaussian experts
mean, see Figure 1 again for more details. Therefore, in Section 4.1.3, we only
highlight our contributions regarding the control of bracketing entropy for the
parameter of gating network compared to [70].
More precisely, the authors of [70] rewrite the softmax gating parameters'
space as follows:
WK;dW=f0g
WK 1;W=(
y7!dWX
d=1!dW;d(y)2R: max
d2[dW]j!djTW)
;
PK=8
<
:y7! 
ewk(y)
PK
l=1ewl(y)!
k2[K]=: (gw;k(y))k2[K];w2WK;dW9
=
;:
Then, they also require the denition of metric entropy of the set WK:Hdksupk1(;WK),
which measures the logarithm of the minimal number of balls of radius at most
, according to a distance dksupk1, needed to cover WKwhere
dksupk1
(sk)k2[K];(tk)k2[K]
= max
k2[K]sup
y2Yksk(y) tk(y)k2; (4.3)

--- PAGE 26 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 26
for anyK-tuples of functions ( sk)k2[K], (tk)k2[K]andksk(y) tk(y)k2is the
Euclidean distance in RL. By using Lemma 5 and Proposition 2 from [70],
Lemma 4.3 holds true if we can prove Lemma 4.4. Note that the rst inequality
of Lemma 4.4 comes from [70, Lemma 4] and describes relationship between the
bracketing entropy of PKand the entropy of WK.
Lemma 4.4. For all2(0;p
2], there exists a constant CWKsuch that
H[];supydk
5;PK
H dksupk1 
3p
3
20p
K 1;WK!
dim (WK)
CWK+ ln20p
K 1
3p
3
:
By the linearity from the construction of linear combination of a nite set of
bounded functions whose coecients belong to a compact set, in the argument
from [70, Proof of Part 1 of Lemma 1, Page 1689], the second inequality of
Lemma 4.4 is then easily established as follows.
Proof of the second inequality of Lemma 4.4. Note that for all
w= (0;wk)k2[K 1]2WK;dW;v= (0;vk)k2[K 1]2WK;dW;
it holds that
dksupk1(w v) = max
k2[K 1]kWK vkk1
= max
k2[K 1]sup
y2YdWX
i=1!w
k;iW;i(y) dWX
i=1!v
k;iW;i(y)
max
k2[K 1]dWX
i=1!w
k;i !v
k;isup
y2YjW;i(y)j
|{z}
1
dW max
k2[K 1];i2[dW]!w
k;i !v
k;i:
Therefore, we obtain
Hdksupk1 
3p
3
20p
K 1;WK!
H dksupk1 
3p
3
20p
K 1dW;n
!2R(K 1)dW:k!k1TWo!
(K 1)dWln
1 +20p
K 1dWTW
3p
3
(K 1)dWlnp
2 +20p
K 1dWTW
3p
3+ ln1

:

--- PAGE 27 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 27
However, proving Lemma 4.5, our equivalent of Lemma 4.4, will be much
harder and could be used for controlling the bracketing entropy for many stan-
dard MoE regression models with Gaussian gating networks, see Section 4.1.3
for more details.
4.1.3. Our contributions on the proof for GLoME models
To prove Theorem 3.1, we also need to make use of Theorem 4.1. Then, our
model collection has to satisfy Assumption 4.1 (K), Assumption 4.2 (Sep), and
Assumption 4.3 (H).
Note that in the proof of LinBoSGaME models, the authors of [70] did not
prove Assumption 4.1 (K) and Assumption 4.2 (Sep) and considered them as
assumptions on their LinBoSGaME models because of the complexity of Lin-
BoSGaME models and technical reasons. However, in our proof for GLoME
models, we consider an explicit example where the model is dened by M=
KD = [Kmax][dmax],Kmax;dmax2N?, leads to the Assumption 4.1 (K) is
always satised. It is interesting to nd the optimal family ( zm)m2Msatisfying
Assumption 4.1 (K). To the best of our knowledge, this question is only partially
answered in some special cases of MoE regression models, e.g., Gaussian graph-
ical model [33], Gaussian nite mixture models [62], nite mixture of Gaussian
regression models [29, 32]. However, for the standard MoE regression models
such as LinBoSGaME and GLoME models, such question still remains open
due to the complexity of models. We hope to resolve this important and inter-
esting problem in our future work. Furthermore, note that the Assumption 4.2
(Sep) is true when we consider Gaussian densities [60].
Therefore, our model has only to satisfy the remaining Assumption 4.3 (H).
Following the same strategy as in the proof of LinBoSGaME models, the main
task for GLoME models is to establish Lemma 4.5, which is much more dicult
compared to Lemma 4.4 and is proved in Appendix A.2.
For the Gaussian gating parameters, to make use of the rst inequality from
Lemma 4.4 of [70], we propose the following reparameterization trick of the
Gaussian gating space, which is dened in (2.2), via the logistics scheme PK
and the nonlinear space WKas follows:
WK=n
y7!(ln (kL(y;ck; k)))k2[K]=: (wk(y;!))k2[K]=w(y;!) :!2e
Ko
;
PK=8
<
:y7! 
ewk(y)
PK
l=1ewl(y)!
k2[K]=: (gw;k(y))k2[K];w2WK9
=
;: (4.4)
We aim to provide the following important upper bound for metric entropy of
nonlinear space Lemma 4.5, which play a key step for controlling the bracketing
entropy not only for GLoME models but also for any standard MoE regression
models with Gaussian gating networks, e.g., BLLiM and BLoME models, see
again Figure 1 for comprehensive descriptions of this general class.

--- PAGE 28 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 28
Lemma 4.5. For all2(0;p
2], there exists a constant CWKsuch that
H[];supydk
5;PK
H dksupk1 
3p
3
20p
K 1;WK!
dim (WK)
CWK+ ln20p
K 1
3p
3
:
More precisely, Assumption 4.3 (H) is proved due to Lemma 4.3 and the
following Lemma 4.6 by using the fact thatPK
k=1gw;k(y) = 1;8y2Y;8w2
WKandH[];d
n(;Sm)H [];supydx(;Sm), which is obtained by denition of
bracketing entropy and d
n(s;t)supydx(s;t).
Lemma 4.6 (Lemma 5 from [70]) .Let
GK;d=n
XY3 (x;y)7!(D(x;vk;d(y);k))k2[K]:d2K;d;2VKo
:
For all2(0;p
2]andm2M ,
H[];supydx(;Sm)H [];supydk
5;PK
+H[];supymaxkdx
5;GK;d
:
By making use of Lemma 4.6, the remaining task is to control the bracketing
entropy of the Gaussian gating functions and experts separately via Lemmas 4.5
and 4.7, which are proved in Appendices A.2 and A.3, respectively.
Lemma 4.7. For all2(0;p
2], there exists a constant CGK;dsuch that
H[];supymaxkdx
5;GK;d
dim (GK;d)
CGK;d+ ln1

: (4.5)
To this end, Lemma 4.6 allows us to conclude that given C=CWK+
ln
5KmaxpKmax
aW
+CGK;d,
H[];supydx(;Sm)H [];supydk
5;PK
+H[];supymaxkdx
5;GK;d
dim(Sm)
C+ ln1

:
Then, Proposition 4.2 leads to
n2
mdim(Sm)0
B@2p
C+p2
+0
B@lnn
p
C+p2
dim (Sm)1
CA
+1
CA:
Finally, Theorem 4.1 implies that for any given collection of GLoME models
(Sm)m2M, the oracle inequality of Theorem 3.1 is satised if pen( m) is bounded

--- PAGE 29 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 29
from below by
0
B@dim(Sm)0
B@2p
C+p2
+0
B@lnn
p
C+p2
dim (Sm)1
CA
+1
CA+zm1
CA:
4.2. Proof of random subcollection of BLoME models
The random subcollection of MoE models include BLLiM and BLoME models
where nite-sample oracle inequalities were only well studied for nite mixture
of Gaussian regression models via a model selection theorem for MLE among a
random subcollection of models in regression framework of [29, Theorem 5.1],
see also [33, Theorem 7.3]. This is an extension of a whole collection of condi-
tional densities from [22, Theorem 2], and of [60, Theorem 7.11], working only
for density estimation. In Section 4.2.1, we rst summarize this theorem and the
techniques that [32] used to control the bracketing entropy of nite mixture of
Gaussian regression models with joint rank and variable selection for parsimo-
nious estimation in a high-dimensional framework. Then, we explain why such
techniques can not be applied to our collection of BLLiM and BLoME models
to highlight the main challenges and our contributions.
4.2.1. A model selection theorem for MLE among a random subcollection
We can now state the main result of [29, Theorem 5.1] for the model selection
theorem for MLE among a random subcollection.
Theorem 4.8 (Theorem 5.1 from [29]) .Let 
x[n];y[n]
be observations com-
ing from an unknown conditional density s0. Let the model collection (Sm)m2M
be an at most countable collection of conditional density sets. Assume that As-
sumption 4.1 (K), Assumption 4.2 (Sep), and Assumption 4.3 (H) hold for every
m2M . LetKL>0, and sm2Sm, such that
KL
n(s0;sm)inf
t2SmKL
n(s0;t) +KL
n;
and let >0, such that
sme s0: (4.6)
Introduce (Sm)m2fM, a random subcollection of (Sm)m2M. Consider the collec-
tion (bsm)m2fMof-log likelihood minimizer satisfying (2.24) for all m2fM.
Then, for any 2(0;1), and anyC1>1, there are two constants andC2
depending only on andC1, such that, for every index m2M ,
pen(m)(Dm+ (1_)zm);

--- PAGE 30 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 30
and where the model complexity Dmis dened in Assumption 4.3 (H), the 0-
penalized likelihood estimator bsbm, dened as in (2.25) on the subset fMM ,
satises
EX[n];Y[n]
JKL
n
(s0;bsbm)
C1EX[n];Y[n]"
inf
m2fM
inf
t2SmKL
n(s0;t) + 2pen(m)
n#
+C2(1_)2
n+0+
n:
4.2.2. Sketch of the proof for BLoME models
To work with conditional density estimation in the BLLiM and BLoME models,
it is natural to make use of Theorem 4.1. However, it is worth mentioning
that because the model collection constructed by the BLLiM [34] or by some
suitable procedures for BLoME models in practice is usually random, we have
to use a model selection theorem for MLE among a random subcollection (cf.
[29, Theorem 5.1] and [33, Theorem 7.3]).
More precisely, we explain how Theorem 4.8 implies the nite-sample or-
acle inequality, Theorem 3.2. To this end, our collections of BLoME models
has to satisfy some regularity assumptions, see Section 4.2.3 for more details.
For BLoME models, the main diculty in proving our oracle inequality lies in
bounding the bracketing entropy of the Gaussian gating functions and Gaussian
experts with block-diagonal covariance matrices. To overcome the former issue,
we follow a reparameterization trick of the Gaussian gating parameters space
in (4.4) and Lemma 4.5. For the second one, based on some ideas of Gaussian
mixture models from [41, 62], we contribute a novel extension for standard MoE
models with Gaussian gating networks. Note that our contributions extend the
recent novel result on block-diagonal covariance matrices in [33], which is only
developed for Gaussian graphical models.
4.2.3. Our contributions on BLoME models
It should be stressed that all we need is to verify that Assumption 4.3 (H),
Assumption 4.2 (Sep) and Assumption 4.1 (K) hold for every m2M . Ac-
cording to the result from [29, Section 5.3], Assumption 4.2 (Sep) holds when
we consider Gaussian densities and the assumption dened by (4.6) is true if
we assume further that the true conditional density s0is bounded and com-
pactly supported. Furthermore, since we restricted dandKtoD= [dmax]
andK= [Kmax], respectively, it is true that there exists a family ( zm)m2Mand
>0 such that, Assumption 4.1 (K) is satised. Therefore, the proof for the
remaining Assumption 4.3 (H) is our novel contribution. In particular, to the
best of our knowledge, there are no results that can be directly applied to As-
sumption 4.3 (H) for BLoME models due to their complexity with the Gaussian
gating functions and Gaussian experts with block-diagonal covariance matrices.

--- PAGE 31 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 31
This highlights our contributions to this challenging problem concerning the
work of [41, 62, 29, 32, 33, 70].
By using Lemma 4.3, Assumption 4.3 (H) holds true if we can prove that for
any2(0;p
2], the collection of LinBoSGaME models, S= (Sm)m2M, satises
H[];d
n(;Sm)dim(Sm)
Cm+ ln1

: (4.7)
Proof of 4.7. Note that (4.7) can be established by rst decomposing the en-
tropy term between the Gaussian gating functions and the Gaussian experts.
Motivated by our reparameterization trick of the Gaussian gating space in Sec-
tion 4.1.3 and (4.4), we dene Gaussian experts GK;d;Bas follows.
GK;d;B=n
(x;y)7!((x;k;d(y);k(Bk)))k2[K]:d2K;d;(B)2VK(B)o
:
There are two possible ways to decompose the bracketing entropy of Smbased
on dierent distances. For the rst approach, we can use Lemma 4.9 [70, Lemma
5]:
Lemma 4.9. For all2(0;p
2]andm2M ,
H[];supydx(;Sm)H [];supydk
5;PK
+H[];supymaxkdx
5;GK;d;B
:
As mentioning in Appendix B.2.1 from [70], Lemma 4.9 boils down to as-
suming that Yis bounded. Furthermore, they also claim that this boundedness
assumption can be relaxed when using smaller distance d
nbut bounding the
corresponding bracketing entropy becomes much more challenging. We success-
fully weaken such boundedness assumption via utilizing the smaller distance:
d
n, for the bracketing entropy of Smalthough bounding such bracketing en-
tropy forWKandGK;Bbecomes much more challenging. This reinforces our new
contributions concerning the control of bracketing entropy of BLoME models.
Consequently, this leads to the second approach via Lemma 4.10 [70, Lemma
6].
Lemma 4.10. For all2(0;p
2],
H[];d
n(;Sm)H [];dPK
2;PK
+H[];dGK;d;B
2;GK;d;B
;

--- PAGE 32 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 32
where
d2
PK 
g+;g 
=EY[n]"
1
nnX
i=1d2
k 
g+(Yi);g (Yi)#
=EY[n]"
1
nnX
i=1KX
k=1q
g+
k(Yi) q
g 
k(Yi)2#
;
d2
GK;d;B 
+; 
=EY[n]"
1
nnX
i=1KX
k=1d2
x 
+
k(;Yi); 
k(;Yi)#
=EY[n]"
1
nnX
i=1KX
k=1Z
Xq
+
k(x;Yi) q
+
k(x;Yi)2
dx#
:
Next, we make use of Lemma 4.11, which is proved in Appendix A.4, to
provide an upper bound on the bracketing entropy of SmandPKon the corre-
sponding distances d
nanddPK, respectively.
Lemma 4.11. It holds that
d
n(s;t)sup
ydx(s;t), andH[];d
n(;Sm)H [];supydx(;Sm); (4.8)
dPK 
g+;g 
sup
ydk(g+;g ), andH[];dPK
2;PK
H [];supydk
2;PK
:
(4.9)
Lemmas 4.10 and 4.11 imply that
H[];d
n(;Sm)H [];supydk
2;PK
+H[];dGK;d;B
2;GK;d;B
:
Based on this metric, one can rst relate the bracketing entropy of PKto
Hdksupk1(;WK), and then obtain the upper bound for its entropy via Lemma 4.5.
Then, we present our main contribution for BLoME models via Lemma 4.12.
This lemma allows us to construct the Gaussian brackets to handle the metric
entropy for Gaussian experts, which is established in Appendix A.5.
Lemma 4.12.
H[];dGK;d;B
2;GK;d;B
dim (GK;d;B)
CGK;d;B+ ln1

: (4.10)
Finally, (4.7) can easily proved via Lemmas 4.5 and 4.12.
5. Numerical experiments
Note that our numerical experiments in Section 5, and the codes written in the
Rprogramming language [93] are available in the following link:
https://github.com/Trung-TinNGUYEN/NamsGLoME-Simulation.

--- PAGE 33 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 33
5.1. The procedure
We illustrate our theoretical results in settings similar to those considered by
[21] and [70], including simulated as well as real data sets. We rst observe n
random samples ( xi;yi)i2[n]from an forward conditional density s
0, and look for
the best estimate among s
m2S
m;m2M , dened in (2.20). We considered the
simple case where the mean experts are linear functions, which leads to GLoME
and supervised GLLiM are identical models. Our aim is to estimate the best
number of components K, as well as the model parameters. As described in more
detail in [26], we use a GLLiM-EM algorithm to estimate the model parameters
for eachK, and select the optimal model using the penalized approach that was
described earlier. More precisely, in the following numerical experiments, the
GLoME model is learned using functions from the package xLLiM, available on
CRAN. It solves the inverse regression problem, dened in (2.16), and obtain
the inverse maximum likelihood estimates (MLE) ( bsm(xijyi))i2[N],m2M ,
then via (2.17), we obtain the forward MLE ( bs
m(yijxi))i2[N],m2M .
According to the general procedure for model selection, we rst compute
the forward MLE for each model m2M , whereM=K. Then, we select the
model that satises the denition (2.25) with pen( m) =dim(S
m), whereis a
positive hyperparameter. The point we want to stress here is that the theoretical
result stated in Theorem 3.1 gives the general form of penalty functions but it
does not provide explicit penalties since the lower bounds on penalty functions
in (3.1) are dened up to an unknown multiplicative constant and mixture
parameters are not bounded in practice. Nevertheless, Theorem 3.1 is required to
justify the shape of penalties using for the slope heuristic and allows the number
of variables Das well as responses Lto be large compared to the xed sample size
n. In particular, our Theorem 3.1 and Remark 2.2 guarantee that there exists a
large enough for which the estimate has the desired properties. Therefore, we
need a data-driven method to choose . According to the AIC or the BIC, we
can select= 1 or=lnn
2. An important limitation of these criteria, however,
is that they are only valid asymptotically. This implies that there are no nite-
sample guarantees when using AIC or BIC for choosing between dierent levels
of complexity. Their use in small sample settings is thus ad hoc. To overcome
such diculties, Birg e and Massart [11] proposed a novel approach, called slope
heuristics, supported by a non-asymptotic oracle inequality. This method leads
to an optimal data-driven choice of multiplicative constants for penalties, e.g.,
in our framework. Thus, we shall concentrate our attention on the slope heuristic
for choosing the number of mixture components in our numerical experiments.
5.2. Simulated data sets
Note that our main objective here is to investigate how well the empirical ten-
sorized Kullback{Leibler divergence between the true model ( s
0) and the se-
lected model bs
bmfollows the nite-sample oracle inequality of Theorem 3.1, as
well as the rate of convergence of the error term. Therefore, we focus on 1-
dimensional data sets, that is, with L=D= 1. Beyond the statistical estimation

--- PAGE 34 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 34
and model selection objectives considered here, the dimensionality reduction ca-
pability of GLLiM in high-dimensional regression data, typically DL, can
be found in [26, Section 6].
We construct simulated data sets following two scenarios: a well-specied
(WS) case in which the true forward conditional density belongs to the class of
proposed models:
s
0(yjx) =(x; 0:2;0:1)
(x; 0:2;0:1) + (x; 0:8;0:15)(y; 5x+ 2;0:09)
+(x; 0:8;0:15)
(x; 0:2;0:1) + (x; 0:8;0:15)(y; 0:1x;0:09);
and a misspecied (MS) case, whereupon such an assumption is not true:
s
0(yjx) =(x; 0:2;0:1)
(x; 0:2;0:1) + (x; 0:8;0:15) 
y;x2 6x+ 1;0:09
+(x; 0:8;0:15)
(x; 0:2;0:1) + (x; 0:8;0:15) 
y; 0:4x2;0:09
:
Figures 2a and 2e show some typical realizations of 2000 data points arising
from the WS and MS scenarios. Note that by using GLoME, our estimator
performs well in the WS setting (Figures 2b to 2d). In the MS case, we expect our
algorithm to automatically balance the model bias and its variance (Figures 2f
to 2h), which leads to the choice of a complex model, with 4 mixture components.
This observation will be elaborated upon in the subsequent experiments.
Firstly, by using the capushe (CAlibrating Penalities Using Slope HEuristics)
package in R[8], we can select the penalty coecient, along with the number
of mixture components K. This heuristic comprises two possible criteria: the
slope criterion and the jump criterion. The rst criterion consists of computing
theasymptotic slope of the log-likelihood (Figures 3 and 4), drawn according
to the model dimension, and then penalizing the log-likelihood by twice the
slope times the model dimension. Regarding the second criterion, one aims to
represent the dimension of the selected model according to (Figures 5 and 6),
and ndb, such that if <b, then the dimension of the selected model is large,
and of reasonable size, otherwise. The slope heuristic prescribes then the use of
= 2b. In our simulated data sets, Figures 7 to 10 show that the jump criterion
appears to work better. The slope criterion sometimes chooses highly complex
models in the WS case, with the problem exacerbated in the MS case.
Next, a close inspection shows that the bias-variance trade-o diers between
the two examples. We run our experiment over 100 trials with K2K = [20],
using both the jump and slope criteria. The rst remark is that the best choice
ofK= 2 appears to be selected with very high probability, even for large
samples (n= 10000) in the WS case. This can be observed in Figures 7a, 7b, 9a
and 9b. In the MS case, the best choice for Kshould balance between the model
approximation error term and the variance one, which is observed in Figures 8a,
8b, 10a and 10b. Here, the larger the number of samples n, the larger the value
ofKthat is selected as optimal.

--- PAGE 35 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 35
â—â—
â—
â—â— â—â—â—
â—â— â—â—â—
â—â— â—
â—â— â—
â—
â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—
â—â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—â—â—
â—â—
â—
â—â—â—â—â—
â—â—
â—â—â— â—
â—â— â—
â—â—
â—
â—â—â—
â—
â—â—
â—â—â—â—
â—
â—â—â—â—
â—
â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—
â—â—â—â—
â—â—â—â— â—
â—â—
â—
â—
â—â—
â—â—â—
â—
â—â—â— â—
â— â—â—â—â—â— â— â—
â—â—
â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—â—
â—
â—â—
â—â—
â—â—â—
â—
â—â—
â—
â—â—
â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—
â—â—
â—â—â—
â—â—
â—â—â—â—
â— â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—â— â—â—
â—â—â—
â—â—
â—â—â— â—â—
â—â—
â—
â—â—â—â—
â—â—
â—â—â—â—â—â— â—â—
â—
â—â—â—â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—
â—â—
â—
â—â—
â—â—
â—â— â—â—
â—â—â— â—
â—â—â—â—
â—â—â—
â—â—â—â—
â—
â—â—â—â—
â—â—â—
â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—
â—
â—â—
â—â—â—
â—â—â—â— â—
â—
â—â—â—
â—â—
â—â—
â—
â—â—
â— â—â—â—â—â—â—
â—
â—â— â—
â—â—â— â—
â—â—
â—â—
â—
â—
â—â—â—
â—
â—â—
â—â—
â—â—â—
â—
â—â—â—
â—
â—â—â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—â—â—
â—â—â— â—â—â—
â—â—
â—â— â—â—â—â—â—
â—
â—
â—â—
â—â—â—
â—â—â—
â— â—â—â— â—
â—â—â—
â—â—â— â— â—â—â—
â—â—â—â— â—â—â—
â—
â—â—â—â—
â—â—â— â—
â—
â—â—â—â—
â—â— â—â—
â—â—â—
â— â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—
â—
â—
â—â—â— â—
â—â—â—â—
â—â—â—
â—â—
â—â—
â—
â—â— â—â—â—â—
â— â—
â—â—â—
â—â— â—
â—â—
â—
â—â—â—
â—
â—â—
â—â— â—â—
â—â—
â—â—
â—â—
â—
â— â—â— â— â—â—â—
â—
â—â—
â—â—
â—
â—â—
â—â—
â—â—â—
â—â— â—â—
â—â—
â—â—â—
â—â—
â— â—â—â—â—
â—â—
â—â—â—
â—â—â—
â— â—â—â—
â—â—
â—â—
â— â—â—
â—â—â—
â—â—
â—â—
â—â— â—â—â—
â—â— â—
â—
â—â—
â—â—â—
â— â—â—
â—
â—â—
â—â—
â—â— â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—â— â—â—â—â—â—
â—â—
â—â—
â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â— â—â—
â— â—â—â—
â— â—â—
â—
â—â—â—
â— â—
â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—
â—â—â—â—â—â—
â—â—â—â—â—
â— â—â—â—
â—
â—â—
â—â—â—
â—â—
â—
â—â—â—â—â—
â— â— â—â—
â—
â—â—
â—
â—â—â—â—
â—â—â—â—â—â— â— â—â—â—
â—â—â—
â—â—â—
â—
â—
â—â—
â—â—â—â—
â—â—â—
â— â—
â—â—â—â—
â—â—â—â—
â—
â—â—â—â—â—
â—
â—â—â—
â— â—
â—â—
â—â—
â—â—
â—â—â— â—
â— â—â—
â—â—â—
â—â—
â—
â—â—
â—
â—â—
â—â—
â—â—â— â—
â—â—
â—â—
â—
â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â— â— â—â—
â—â— â— â—â—â—â— â—
â—â—
â—
â—â—â— â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—â— â—â—
â—â—
â—â—
â—â—â—
â—
â—â— â—â—
â—
â—â—â— â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—
â—â— â—â—â—â—
â—â—
â—
â—â—
â—
â—â—â— â—
â— â—â—
â—â—â—â—
â—â—
â—â—
â—â—
â—â—â— â—
â—â—â—â—â—â—
â—â—
â—â—â—
â—
â—â—â—
â—â—â—
â—
â—â— â—
â—â—
â—â—
â—â—
â—â—â—
â—â—
â—
â—â—â—â—
â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—â— â—â—
â—â— â—
â—â—â—
â—
â—â—â—
â—
â—â—â—â—
â—â—
â—â— â—â— â—
â—â—
â—
â—
â—â—
âˆ’2.50.02.55.0
0 1 2
XY
(a) Typical realization of example
WS
â—â—
â—â—
â—â— â—â—
â—â— â—â—â—â—
â—â— â— â—
â—
â—â—â—
â—â—â—â—â—â—
â—â—
â—â—â—
â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—â— â—â—
â—â—
â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â— â—
â—â—
â—â—â—
â—
â—â—
â—â—â—â—
â—â—â—â—
â—
â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—
â—â—â—
â—â—â—
â—â—
â—â—
â—
â—
â—
â—â—
â—â—â—
â—
â—â—â— â—
â—â—â—â—â— â—
â—â—
â—
â—â—
â—â—
â—â—
â—
â—â—â—â—
â—
â—
â—â—
â—â—â—
â—
â—â—
â—
â—â—
â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—
â—â—
â—â—â—
â—â—
â—â—â—â—
â— â—
â—â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—â— â—â—â—
â—
â—â—â—â—â—
â—â—â—â—â—â— â—â—
â—â—â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—
â—â—
â—
â—â—
â—â—â— â—â—
â—â—â— â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—
â—â—â—â—
â—â— â—â—
â—
â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—
â—
â—â—
â—â—â—
â—â—â—â— â—
â—
â—â—â—
â—â—
â—
â—
â—â—
â— â—â—â—â—â—â—
â—
â—â— â—
â—â—â— â—
â—â—
â—â—
â—
â—
â—â—â—
â—
â— â—â—
â—â—
â—â—â—
â—
â—â—â—
â—
â—â—â—â—
â—â—â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—â— â—â—â—
â—â—
â—â—â—â—â—â—â—â—
â— â—â—
â—â—â—
â—â—
â— â—â—â— â—
â—â—
â—â—â—â—â—
â—â—â—â— â—â—â—
â—
â—â—â—â—
â—â—â— â—
â—
â—â—â—
â—â— â—â—
â—â—â—
â— â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—
â—
â—
â—â— â—
â—â—â—â—
â—â—â—
â—â—
â—
â—â—
â— â— â—â—â—â—
â— â—
â—â—
â—
â—â—
â—â—
â—â—â—
â—
â—â—â— â—â—
â—â—
â—â—
â—â—
â—â— â—
â—â—â—â—
â—
â— â—â—
â—â—
â—
â—â—
â—â—
â—â— â—â—
â—â— â—â—
â—â—
â—â—â—â—
â—â—
â— â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â— â—â—â—
â—â—
â—â—
â— â—â—
â—â—â—
â—â—
â—â—
â—â—â—
â—â— â—
â—â—
â—â—
â—â—
â—
â—â—
â—â—
â—â— â—
â—â—
â—â—â—â—
â—â— â—â—
â—â—
â—â—
â—â—â—â—
â—â— â—
â—â—â—â—
â—â—â—
â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â— â—â—
â—â—â—
â— â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—â—â—
â—â— â—
â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â— â—
â—â—â—â—â—
â— â— â—â—
â—
â—â—â—
â—
â— â—â—
â—â—
â—â—â—â—â— â— â—â—â—
â—â—â—
â—â—â—
â—
â—
â—â—
â—â—â—â—
â—
â—â—
â—â—â—
â— â—
â—â—â—
â—â—â—â—
â—
â—â—â—â—â—
â—
â—â—â—
â— â— â—
â—â—
â—â—
â—
â—â—
â—â—â— â—
â— â—â—â—â—
â—â—
â—
â—â—
â—
â—â—
â—â—
â—â—â— â—
â—â—
â—â—
â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â— â—
â— â— â—â—
â— â— â—â—â—â—
â—â—
â—
â—
â— â—
â—â—â— â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—â— â—â—
â—â—
â—â—
â—â—â—
â—
â—â— â— â— â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â— â—â—â—â—
â—â—
â—â—
â—
â—â—â— â—
â— â—â—
â—â—â—â—
â—â—
â—â—â—
â—â— â—
â—â—â—â—â—â—
â—â—â—
â—
â—â—
â—â—
â—â— â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â—â—â—
â—
â—â—
â—â—â—â—
â—â—â—â—â—
â—â—â—
â—â— â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—â— â—â— â—â—
â—
â—
â—â—
âˆ’2.50.02.55.0
0 1 2
 XY(b) Clustering by GLoME
âˆ’0.50.00.51.01.5âˆ’4âˆ’20246
XY
0.00.20.40.60.81.01.2
(c) 2D view of the resulting condi-
tional density with the 2 regression
components
0.000.250.500.751.00
0 1 2
X Mixing probabilities(d) Gating network probabilities
â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—â— â—â—
â— â—â—
â—â—â—
â—â—
â—
â—â—â—
â—â—
â—â—â—
â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—
â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â— â—
â—â—â—
â—â—â—â—
â—â—â—â—â—
â—â—â—
â—
â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—â—â—â—â—
â—â— â—â—
â—â—
â—â—â—
â— â—â—â—
â—â—
â—â— â—â—
â—â—â—â—â—
â—â—â— â—
â—â—â—â—â—â—
â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—
â—
â—â— â—
â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—
â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—â—â—
â—â—â—â—â—â—
â—â—
â—â—â—
â—â—
â— â— â—â—
â—â—â—
â—â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—â— â—
â— â—â—â— â—
â—â—
â—â—â— â—â—
â—â—â—â—
â—â—
â—â—â—â—
â—
â—â—
â—â—
â—
â—â—
â—
â—â—
â—â—
â—â— â—â—
â—â—
â—â— â—â— â—
â—â—â—â—
â—â—â—â—â—
â—â—â—â—â—
â— â—
â—
â— â—â—â—â—
â—â—â—
â—â— â—â—
â—â—
â—â— â—â— â—
â—â—â—
â—â—
â—
â—â—â—â—â—
â—â—â—
â—â—
â—â—
â—
â—â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—
â—
â—â—â—
â—
â—â—â—
â—
â—â—
â—â—â—
â—
â—â—â—
â—â—â— â—
â—â—
â—â—â—
â— â— â—â—â—
â—
â—â—â—â— â—â—â— â—â—â—
â—
â—â—
â—â—
â—â—
â—â— â—â—
â—â—â—â—â—â—
â— â—â—â—
â—â—â—
â—
â—â—â—
â—â—
â— â—â—
â—â—â—
â—â—
â—
â—â—
â—â—â—
â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—
â—â—â—
â—
â—â—
â—
â—â—â—
â—â—â—â—
â—â—
â—
â—â—â—â—â—
â—
â—â—
â—â— â—
â—â—
â—â—
â—â—â—â—
â—
â—â— â—â—â—
â—â—â—â—â—â—
â—â—â— â— â—â—â—
â—â—
â—
â—â—
â—
â—â—â—
â—â—â—
â— â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—
â—â— â—â—
â—â—â—â—
â— â—
â—
â—â— â—
â—â—â—
â—â— â—â—â—
â—â—â—
â—â—â—â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—
â—
â—â—â—â—â—
â—â—
â—
â—â—â— â—â—â—â—â—
â—â—â—â—
â—
â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—â—â—â—
â—â—â—
â—â—
â—
â—â—â—â—
â—â— â—â—
â—â—â—â—â—
â—â—
â—â—
â— â—
â—
â—â—â—
â—â—â—
â— â—â—
â—â—
â—â—â—
â—â—
â—
â—â—â—
â—
â—â—
â—â—â—â—
â—â—
â—
â—â—â— â—
â—â—â—
â—â—â—â—
â—â—
â—
â—â—
â—
â—â—
â—
â—â—
â—â—â—
â—â—
â—â—
â—â—
â—
â— â—â—â—â—â—
â—â— â—â—
â—â—
â—
â—â—â—â—
â—â—
â—â—â— â— â—â—
â—â—
â—â—
â—
â— â—â—
â—
â— â—â—
â—â—â—â—â—
â—â—
â—
â—â—â—â—
â—â—
â— â—â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—â—â—
â—
â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—
â—
â—â—
â—â—â—
â—â—â—â— â—
â—â—â—â—â—
â— â—â—
â—â—â—â—â—
â—
â—
â—â—â—
â—â—â—â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—
â— â—â—â—â—
â— â—â—â—â—â—
â—â—â—â—
â—â—
â—
â—â—â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—
â—â—
â—
â—â—â—
â—â—â—
â—â—
â—
â—â—
âˆ’505
âˆ’1 0 1 2
XY
(e) Typical realization of example
MS
â—â—â—
â—â—â—
â—â—â—
â—
â—â—â—
â—â—
â—
â—â—â—â—
â—
â—
â—â—
â—â—
â—â—
â—â—â—
â—
â—â—
â—â—
â—
â—â—
â—â—
â—â—â—
â—â—â—
â—
â—
â—â—â—â—â—
â—â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—â—â—
â—â—â—
â—â—â—â—
â—â—
â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—
â—
â—â—â—
â—
â—â—
â—â—
â—â—
â—â—â—â—â—
â—
â—â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—â—
â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—
â—
â—â—
â—â—
â—â—
â—â—â—â—
â—
â—
â—â—â—â—
â—â—â—
â—â—â—
â—
â—
â—â—â—â—
â—
â—â—â—
â—â—
â—
â—â—
â—â—
â—
â—â—â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—
â—â—â—
â—
â—â—â—
â—
â—â—
â—
â—
â—â—
â—â—â—
â—â—â—
â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—
â—â—â—
â—â—
â—â—â—
â—
â—â—
â—â—â—
â—
â—â—â—
â—â—
â—â—
â—â—â—â—
â—
â—
â—
â—â—â—â—
â—â—â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—
â—â—
â—â—â—â—
â—â—
â—
â—â—
â—â—
â—â—â—
â—
â—â—
â—â—
â—
â—â—
â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—
â—â—
â—
â—â—
â—â—
â—
â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—
â—
â—
â—
â—â—
â—â—
â—
â—
â—
â—â—
â—
â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—â—â—â—
â—â—â—
â—â—â—â—
â—
â—
â—â—â—
â—â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—
â—â—â—
â—
â—â—
â—
â—â—â—â—â—
â—â—â—â—
â—â—
â—â—â—â—
â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—â—
â—â—
â—â—â—
â—
â—â—â—â—
â—â—â—
â—â—
â—â—
â—â—â—
â—
â—
âˆ’505
âˆ’1 0 1 2
 XY(f) Clustering by GLoME
âˆ’0.50.00.51.01.5âˆ’4âˆ’20246
XY
0.00.20.40.60.81.01.21.4
(g) 2D view of the resulting condi-
tional density with the 4 regression
components
0.000.250.500.75
âˆ’1 0 1 2
X Mixing probabilities(h) Gating network probabilities
Figure 2: Clustering deduced from the estimated conditional density of GLoME
by a MAP principle with 2000 data points of example WS and MS. The dash
and solid black curves present the true and estimated mean functions.

--- PAGE 36 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 36
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—âˆ’2500 âˆ’2000 âˆ’1500 âˆ’1000Contrast representation
penshape(m) (labels : Model names)âˆ’Î³n(s^m)
The regression line is computed with 11 points1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
0.20.61.0Successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^m)) for the regressionÎº
201918161513121097643
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—Selected models with respect to the successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^n)) for the regressionModel
201918161513121097643219
(a) Example WS.
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—âˆ’3000 âˆ’2500 âˆ’2000 âˆ’1500 âˆ’1000Contrast representation
penshape(m) (labels : Model names)âˆ’Î³n(s^m)
The regression line is computed with 3 points1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
0.51.01.52.0Successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^m)) for the regressionÎº
201918161513121097643
â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—
â—â—â—Selected models with respect to the successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^n)) for the regressionModel
20191816151312109764339
(b) Example MS.
Figure 3: Plot of the selected model dimension using the slope criterion with
2000 data points.

--- PAGE 37 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 37
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—âˆ’12000 âˆ’10000 âˆ’8000 âˆ’6000Contrast representation
penshape(m) (labels : Model names)âˆ’Î³n(s^m)
The regression line is computed with 11 points1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
0.40.60.81.0Successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^m)) for the regressionÎº
201918161513121097643
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—Selected models with respect to the successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^n)) for the regressionModel
2019181615131210976432
(a) Example WS.
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—âˆ’14000 âˆ’12000 âˆ’10000 âˆ’8000 âˆ’6000Contrast representation
penshape(m) (labels : Model names)âˆ’Î³n(s^m)
The regression line is computed with 5 points1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
0.51.01.52.02.5Successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^m)) for the regressionÎº
201918161513121097643
â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—Selected models with respect to the successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^n)) for the regressionModel
20191816151312109764341012
(b) Example MS.
Figure 4: Plot of the selected model dimension using the slope criterion with
10000 data points.

--- PAGE 38 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 38
Dimension Jump
Values of the penalty constant Îº11 ( 2 )17 ( 3 )59 ( 10 )113 ( 19 )119 ( 20 )Selected Complexity (Model)
Îº^djÎºopt 00.14 0.84 1.4 1.7Maximal Jump
(a) Example WS.
Dimension Jump
Values of the penalty constant Îº17 ( 3 )35 ( 6 )53 ( 9 )89 ( 15 )119 ( 20 )Selected Complexity (Model)
Îº^djÎºopt 00.21 1.3 2.1 2.5Maximal Jump
(b) Example MS.
Figure 5: Plot of the selected model dimension using the jump criterion with
2000 data points.

--- PAGE 39 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 39
Dimension Jump
Values of the penalty constant Îº11 ( 2 )113 ( 19 )Selected Complexity (Model)
Îº^djÎºopt 00.16 0.47 0.79 0.95Maximal Jump
(a) Example WS.
Dimension Jump
Values of the penalty constant Îº23 ( 4 )71 ( 12 )95 ( 16 )113 ( 19 )Selected Complexity (Model)
Îº^djÎºopt 00.43 2.6 4.3 5.1Maximal Jump
(b) Example MS.
Figure 6: Plot of the selected model dimension using the jump criterion with
10000 data points.

--- PAGE 40 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 40
Selected number of classesEmpirical Probability
5 10 15 200.0 0.2 0.4 0.6 0.8 1.0
(a) 2000 data points.
Selected number of classesEmpirical Probability
5 10 15 200.0 0.2 0.4 0.6 0.8
(b) 10000 data points.
Figure 7: Comparison histograms of selected Kin WS case using jump criterion
over 100 trials.

--- PAGE 41 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 41
Selected number of classesEmpirical Probability
5 10 15 200.00 0.05 0.10 0.15 0.20 0.25 0.30
(a) 2000 data points.
Selected number of classesEmpirical Probability
5 10 15 200.00 0.05 0.10 0.15
(b) 10000 data points.
Figure 8: Comparison histograms of selected Kin MS case using jump criterion
over 100 trials.

--- PAGE 42 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 42
Selected number of classesEmpirical Probability
5 10 15 200.0 0.2 0.4 0.6 0.8
(a) 2000 data points.
Selected number of classesEmpirical Probability
5 10 15 200.0 0.2 0.4 0.6 0.8
(b) 10000 data points.
Figure 9: Comparison histograms of selected Kin WS case using slope criterion
over 100 trials.

--- PAGE 43 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 43
Selected number of classesEmpirical Probability
5 10 15 200.00 0.05 0.10 0.15 0.20 0.25
(a) 2000 data points
Selected number of classesEmpirical Probability
5 10 15 200.00 0.05 0.10 0.15
(b) 10000 data points.
Figure 10: Comparison histograms of selected Kin MS case using slope criterion
over 100 trials.

--- PAGE 44 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 44
From hereon in, we only focus on the jump criterion, due to its stability re-
garding the choice of K. We wish to measure the performances of our chosen
GLoME models in term of the tensorized Kullback{Leibler divergence, KL
n,
which can not be calculated exactly in the case of Gaussian mixtures. There-
fore, we evaluate the divergence using a Monte Carlo simulation, since we know
the true density. We note that the variability of this randomized approxima-
tion has been demonstrated to be negligible in practice, which is also supported
in the numerical experiments by [70]. More precisely, we compute the Monte
Carlo approximation for the tensorized Kullback{Leibler divergence as follows.
First, note that the Monte Carlo approximation for tensorized Kullback{Leibler
divergence between the true model, s
0, and the selected model bs
bmcan be ap-
proximated as
KL
n(s
0;bs
bm) =EX"
1
nnX
i=1KL (s
0(jxi);bs
bm(jxi))#
1
nnX
i=1KL (s
0(jxi);bs
bm(jxi))
=1
nnX
i=11
nynyX
j=1lns
0(yi;jjxi)
bs
bm(yi;jjxi)
;
where the data xi;i2[n], and ( yi;j)j2[ny]are drawn from s
0(jxi). Then,
E
KL
n 
s
0;bs
bm
is approximated again by averaging over Nt= 100 Monte
Carlo trials. Therefore, the simulated data used for approximation can be writ-
ten as ( xi;yi;j)twithi2[n];j2[ny];t2[Nt].
Based on the approximation, Figures 11 and 12 show the box plots and the
mean of the tensorized Kullback{Leibler divergence over 100 trials, based on
the jump criterion. Our boxplots conrm that the mean tensorized Kullback{
Leibler divergence between bs
Kands
0, overK2f1;:::; 20gnumber of mixture
components, is always larger than the mean of tensorized Kullback{Leibler di-
vergence between the penalized estimator bs
bKands
0, which is consistent with
Theorem 3.1. In particular, if the true model belongs to our nested collection,
the mean tensorized Kullback{Leibler divergence seems to behave likedim(S
m)
2n
(shown by a dotted line), which can be explained by the AIC heuristic. More
precisely, we rstly assume that
S
m=n
XY3 (x;y)7!s
m:=s 
m(yjx) : 
m2	
mRdim(S
m)o
is identiable and make some strong regularity assumptions on  
m7!s
 m.
Further, we assume the existence of dim ( S
m)dim (S
m) matrices A( 
m) and

--- PAGE 45 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 45
B( 
m), which are dened as follows:
[A( 
m)]k;l=E"
1
nnX
i=1Z @2lns 
m
@ 
m;k@ 
m;l(yjxi)s
0(yjxi)dy#
;
[B( 
m)]k;l=E"
1
nnX
i=1Z@lns 
m
@ 
m;k(yjxi)@lns 
m
@ 
m;l(yjxi)s
0(yjxi)dy#
:
Then, the results from [98] and [22] imply that E[KL
n(s
0;bs
m)] is asymptoti-
cally equivalent to
KL
n 
s
0;s 
m
+1
2ntr
B( 
m)A( 
m) 1
;
where we dened  
m= argmins m2SmKL
n 
s
0;s 
m
.
In particular, E[KL
n(s
0;bs
m)] is asymptotically equivalent to1
2ndim (S
m),
whenevers
0belongs to the model collection S
m. Furthermore, even though
there is no theoretical guarantee, the slope of the mean error in the misspecied
case seems also to grow at the same rate asdim(S
m)
2n, for large enough number
of mixture components ( K6 in the WS case and K9 in the MS case).
Figure 13 shows that the error decays when the sample size ngrows, when
using the penalty based on the jump criterion. The rst remark is that we
observed the error decay is of order t=n, as predicted in by the theory, where t
is some constant, as expected in the well-specied case. The rate of convergence
for the misspecied case seems to be slower.
5.3. Ethanol data set
We now consider the use of GLoME models for performing clustering and re-
gression tasks on a real data set. Following the numerical experiments from [100]
and [70], we demonstrate our model on the ethanol data set of [16]. The data
comprises of 88 observations, which represent the relationship between the en-
gine's concentration of nitrogen oxide (NO) emissions and the equivalence ratio
(ER), a measure of the air-ethanol mix, used as a spark-ignition engine fuel
in a single-cylinder automobile test (Figures 18a and 18e). Our goal is then to
estimate the parameters of a GLoME model, as well as the number of mixture
components.
More precisely, we rst use the EM algorithm from the xLLiM package to
compute the forward PMLE of (2.1), for each K2[12], on the Ethanol data set.
Then, based on the slope heuristic (Figures 14 to 17), we select the best model.
Given the estimators of the model chosen, we obtain the estimated conditional
density and clustering by applying the maximum a posteriori probability (MAP)
rule (Figures 18 and 19).
Because we only have 88 data points and 6 parameters per class, the EM
algorithm is strongly dependent on the random initialization of the parameters.
One solution is that we can modify slightly that procedure in order to guarantee

--- PAGE 46 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 46
â—â—â—â—â—â—â—â—â—â—
â—â—
â—â—â—
â—â—â—â—
â—â—â—â—â—
â—
â—
â—â—
1234567891011121314151617181920SK0.00 0.01 0.02 0.03 0.04 0.05
asymptotic E[KL]
empirical E[KL]
E[KL] of the selected K
(a) 2000 data points.
â—
â—â—
â—â—â—â—â—â—â—â—â—
â—â—â—â—â—â—â—â—
â—â—â—â— â—â—â—â—
â—â—â—â—
â—â—â—â—
â—â—
â—â—â—â—â—
1234567891011121314151617181920SK0.000 0.002 0.004 0.006asymptotic E[KL]
empirical E[KL]
E[KL] of the selected K
(b) 10000 data points.
Figure 11: Box-plot of the tensorized Kullback{Leibler divergence according to
the number of mixture components in WS case using the jump criterion over 100
trials. The tensorized Kullback{Leibler divergence of the penalized estimator bsbK
is shown in the right-most box-plot of each graph.

--- PAGE 47 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 47
â—
â—
â—â—â—â—
â—
â—â—â—â—â—â—
â—
â—â—â—
â—â—
â—â—
â—â—
â—
â—
â—â—
â—â—
â—â—â—â—â—
â—
â—â—
â—â—â—â—
â—â—
â—â—
1234567891011121314151617181920SK0.02 0.04 0.06 0.08asymptotic E[KL]
empirical E[KL]
E[KL] of the selected K
(a) 2000 data points.
â—â—
â—â—
â—
â—â—â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—
â—â—
â—â—â—â—
â—
â—â—
â—
â—â—
â—â—â—
â—â—
â—
â—â—
â—â— â—â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—â—â—
â—â—
â—â—â—â—
â—
1234567891011121314151617181920SK0.010.020.030.040.050.060.07asymptotic E[KL]
empirical E[KL]
E[KL] of the selected K
(b) 10000 data points.
Figure 12: Box-plot of the tensorized Kullback{Leibler divergence according to
the number of mixture components in MS case using the jump criterion over 100
trials. The tensorized Kullback{Leibler divergence of the penalized estimator bsbK
is shown in the right-most box-plot of each graph.

--- PAGE 48 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 48
1eâˆ’043eâˆ’041eâˆ’033eâˆ’03
1000 3000 10000
Sample sizeMean of Kullback Leibler distanceError decay
E[KL]
linear regression of E[KL]
n âˆ’> t/n
(a) WS Example: The slope of the free regression line
is  1:287 and t= 3.
0.0030.0050.010
1000 3000 10000
Sample sizeMean of Kullback Leibler distanceError decay
E[KL]
linear regression of E[KL]
n âˆ’> t/n
(b) MS Example: The slope of the free regression line
is  0:6120 and t= 20.
Figure 13: Tensorized Kullback{Leibler divergence between the true and selected
densities based on the jump criterion, represented in a log-log scale, using 30
trials. A free least-square regression with standard error and a regression with
slope 1 were added to stress the two dierent behavior for each graph.

--- PAGE 49 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 49
that at least 10 points are assigned to each class so that the estimated parameters
are more stable (cf. [70]). In this work, we wish to investigate how well our
proposed PMLE performs for detecting the best number of mixture components
for the GLoME model. Thus, we run our experiment over 100 trials with dierent
initializations for the EM algorithm. Histograms of selected values of Kare
presented in Figures 14 and 15. Notice that it is quite unlikely that the true
conditional PDF of Ethanol data set belongs to our hypothesised collection of
GLoME models. In fact, this phenomenon has been observed in the MS case,
Figures 7 to 10, on the simulated data set. We believe that this is due to the
simplistic ane models used in our experiments. Furthermore, it seems that the
jump criterion outperformed the slope criterion in the stability of order selection
for GLoME models, as previously observed.
Based on the highest empirical probabilities in all situations, our procedure
selectsK= 4 components, which is consistent with the results from [70]. It
is worth noting that if we consider the regression of NO with respect to ER,
our proposed PMLE of GLoME performs very well for both the clustering and
regression tasks (Figure 19). Here, instead of considering the variable NO as the
covariate, we use it as the response variable. Then, the resulting clustering, the
estimated mean function (black curve) and mixing probabilities are more easily
interpretable. This is very similar to the results obtained in [70].
6. Conclusion and perspectives
We have studied the PMLEs for GLoME and BLoME regression models. Our
main contributions are to establish non-asymptotic risk bounds that take the
form of nite-sample weak oracle inequalities, provided that lower bounds on
the penalties hold true. We believe that our contributions help to popularize
GLoME and BLoME models as well as slope heuristics, by giving some non-
asymptotic theoretical foundations for model selection technique in this area and
demonstrating some interesting numerical schemes and experiments. In particu-
lar, we aim to provide extensions of the current nite-sample oracle inequalities,
Theorems 3.1 and 3.2, to more general frameworks where Gaussian experts are
replaced by the elliptical distributions in the future work.
Some important theoretical issues regarding the tightness of the bounds of the
PMLEs are still open. As per the SGaME models from [70], we also have to deal
with three potential issues: the dierences of divergence on the left (JKL
n
) and
the right (KL
n) hand side, C1>1, and the relationship betweenpen(m)
nand
the variance. The rst issue is important as in general we have JKL
n
(s0;sm)
KL
n(s0;sm). However, (2.23) ensures that the two divergences are equivalent
under regularity conditions. Namely, when
sup
m2Msup
sm2Smks0=smk1<1:
Such a strong assumption is satised as long as Xis compact, s0is com-
pactly supported, and the regression functions are uniformly bounded, and un-
der the condition that there is a uniform lower bound on the eigenvalues of

--- PAGE 50 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 50
Selected number of classesEmpirical Probability
2 4 6 8 10 120.0 0.1 0.2 0.3
(a) Jump criterion.
Selected number of classesEmpirical Probability
2 4 6 8 10 120.00 0.05 0.10 0.15 0.20 0.25
(b) Slope criterion.
Figure 14: Histogram of selected Kof GLoME on Ethanol data set based on
NO using slope heuristic.

--- PAGE 51 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 51
Selected number of classesEmpirical Probability
2 4 6 8 10 120.0 0.1 0.2 0.3 0.4
(a) Jump criterion.
Selected number of classesEmpirical Probability
2 4 6 8 10 120.00 0.05 0.10 0.15 0.20 0.25 0.30
(b) Slope criterion.
Figure 15: Histogram of selected Kof GLoME on Ethanol data set based on
ER using slope heuristic.

--- PAGE 52 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 52
Dimension Jump
Values of the penalty constant Îº3 ( 1 )23 ( 4 )47 ( 8 )59 ( 10 )Selected Complexity (Model)
Îº^djÎºopt 01.1 3.3 5.56.6Maximal Jump
(a) Jump criterion based on NO
Dimension Jump
Values of the penalty constant Îº23 ( 4 )29 ( 5 )71 ( 12 )Selected Complexity (Model)
Îº^djÎºopt 00.49 1.5 2.42.9Maximal Jump
(b) Jump criterion based on ER
Figure 16: The jump criterion corresponding to the models chosen with highest
empirical probabilities.

--- PAGE 53 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 53
â—â—â—â—â—â—
â—â—â—
â—
â—â—50 100 150 200Contrast representation
penshape(m) (labels : Model names)âˆ’Î³n(s^m)
The regression line is computed with 7 points1
2
3
4
5
6
7
8
9
10
11
12
2 4 6 810Successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^m)) for the regressionÎº
12121110987765432
â— â— â— â— â— â— â— â— â— â—
â—Selected models with respect to the successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^n)) for the regressionModel
1212111098776543214
(a) Slope criterion based on NO
â—â—â—â—â—
â—â— â—â—â—â—â—âˆ’100 âˆ’50 0 50Contrast representation
penshape(m) (labels : Model names)âˆ’Î³n(s^m)
The regression line is computed with 7 points1
2
3
4
5
6
7
8
9
10
11
12
1.01.21.41.61.82.0Successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^m)) for the regressionÎº
12121110987765432
â— â— â— â— â— â— â— â— â— â— â—Selected models with respect to the successive slope values
Number of points ( penshape(m),âˆ’Î³n(s^n)) for the regressionModel
121211109877654324
(b) Slope criterion based on ER
Figure 17: The slope criterion corresponding to the models chosen with highest
empirical probabilities.

--- PAGE 54 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 54
â—
â—â—
â—â—
â—â—
â—
â—â—
â—â—â—
â—â—â—â—â—
â—
â—â—
â—â—â—
â—
â—â—
â—â—
â—â—
â—â—
â—â—
â—
â—
â—â—
â—
â—â—
â—â—
â—â—â—â—
â—â—
â—
â—â—
â—
â—
â—â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—
â—
â—â—â—â—â—
â—â—
â—
â—â—â—
â—
â—
â—
â—
â—â—0.70.91.1
1 2 3 4
NOEquivalence Ratio
(a) Raw Ethanol data set based on NO.
â—
â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—â—
0.50.70.91.1
1 2 3 4
 NOEquivalence Ratio (b) Clustering by GLoME based on NO.
NO
Equivalence RatioConditional density
051015202530
(c) 3D view of the resulting conditional
density based on NO with the 4 clusters.
0.51.01.52.02.53.03.54.00.60.70.80.91.01.11.2
NOEquivalence Ratio(d) 2D view of the same conditional den-
sity on NO.
â—
â—
â—â—
â—â—
â—â—â—
â—â—
â—â—â—
â—â—â—â—â—
â—
â—â—
â—
â—â—
â—â—
â—â—â—
â—â—
â—â—
â—â—â—
â—
â—â—â—
â—â—
â—
â—â—
â—â—
â—â—
â—
â—â—â— â—
â—
â—â—
â—
â—â—â—
â—â—
â—â—
â—
â—
â—
â—â—
â—â—â—
â—
â—â—
â—â—â—â—
â— â—â—â—
â—â—â—
1234
0.7 0.9 1.1
Equivalence RatioNO
(e) Raw Ethanol data set based on ER.
â—â—â—
â—â—
â—â—â—
â—â— â—
â—â—â—â—
â—
â—â—â—
1234
0.7 0.9 1.1
Equivalence Ratio NO (f) Clustering by GLoME based on ER.
Equivalence Ratio
NOConditional density
0.00.51.01.52.02.53.0
(g) 3D view of the estimated conditional
density with the 4 clusters.
0.60.70.80.91.01.11.20.51.01.52.02.53.03.54.0
Equivalence RatioNO(h) 2D view of the same conditional den-
sity.
Figure 18: Estimated conditional density with 4 components based upon on the
covariate NO or ER from the Ethanol data set.

--- PAGE 55 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 55
â—
â—â—â—
â—â—
â—â—
â—â—â—â—â—
â—â—â—â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—â—
0.50.70.91.1
1 2 3 4
 NOEquivalence Ratio
(a) Clustering based on NO
â—â—â—
â—â—
â—â—â—
â—â— â—
â—â—â—â—
â—
â—â—â—
1234
0.7 0.9 1.1
Equivalence Ratio NO (b) Clustering based on ER
0.000.250.500.75
1 2 3 4
NO Mixing probabilities
(c) Gating network probabilities based on
NO
0.000.250.500.751.00
0.7 0.9 1.1
Equivalence Ratio Mixing probabilities(d) Gating network probabilities based on
ER
Figure 19: Clustering of Ethanol data set. The black curves present the estimated
mean functions. The size of the component mean functions corresponds to the
posterior mixture proportions.

--- PAGE 56 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 56
the covariance matrices. When s0=2(Sm)m2M, the error bound converges to
C1infsm2SmKL
n(s0;sm), which may be large. Nevertheless, as we consider
GLoME models, some recent results from [73, 75] imply that if we take a su-
ciently large number of mixture components, we can approximate a broad class
of densities, and thus the term on the right hand side is small for Ksuciently
large. This improves the error bound even when s0does not belong to Smfor any
m2M . For the last issue, it holds thatpen(m)
nis approximately proportional
to the asymptotic variance,dim(Sm)
n, in the parametric case, see its justication
in Section 3.3.
Finally, it is also interesting to point out that the weak inequalities in Theo-
rems 3.1 and 3.2 make use of dierent divergences and require some regularity
assumptions to be considered proper oracle inequalities. To illustrate the strict-
ness of the compactness assumption for s0, we only need to consider s0as a
univariate Gaussian PDF, which obviously does not satisfy such a hypothesis.
This motivates [79, Theorem 3.1] to investigate an l1-ball model selection pro-
cedure and an l1-oracle inequality of SGaME with the LASSO estimator. Such
anl1-oracle inequality can be considered as a complementary to Theorem 3.1.
In particular, the most intriguing property of the l1-oracle inequality is that
it requires only the boundedness assumption of the parameters of the model,
which is also required in Theorem 3.1, as well as in [92, 68, 30, 79]. Note that
such a mild assumption is quite common when working with MLE (cf. [7, 62]),
to tackle the problem of the unboundedness of the likelihood at the boundary of
the parameter space [66, 88], and to prevent it from diverging. Nevertheless, by
using the smaller divergence: JKL
n
(or more strict assumptions on s0andsm,
with the same divergence KL
nas ours), Theorem 3.1 obtain a faster conver-
gence rate of order 1 =ncompared to 1 =pnin thel1-oracle inequality. Therefore,
in the case where there is no guarantee of a compact support of s0or uniformly
bounded regression functions, the l1-oracle inequality gives a theoretical founda-
tion for the l1-ball model selection procedure, with the order of rate convergence
of 1=pn, with only the boundedness assumption on the parameter space.
Acknowledgments
TrungTin Nguyen is supported by a \Contrat doctoral" from the French Min-
istry of Higher Education and Research. Faicel Chamroukhi is granted by the
French National Research Agency (ANR) grant SMILES ANR-18-CE40-0014.
Hien Duy Nguyen is funded by Australian Research Council grant number
DP180101192. This research is funded directly by the Inria LANDER project.
TrungTin Nguyen also sincerely acknowledges Inria Grenoble-Rh^ one-Alpes Re-
search Centre for a valuable Visiting PhD Fellowship working with STATIFY
team so that this research can be completed, Erwan LE PENNEC and Lucie
Montuelle for providing the simulations for the SGaME models.

--- PAGE 57 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 57
Appendix A: Our contributions on lemma proofs
A.1. Proof of Lemma 2.1: Inverse regression trick
We rstly aim to provide the proof of one-to-one correspondence denes the
link between the inverse and forward conditional distributions not only for the
special case of Gaussian distribution in (2.17) but also for elliptical distributions
(cf. [19, 38]). It is worth mentioning that the multivariate normal distribution,
multivariate t-distribution and multivariate Laplace distribution are some in-
stances of elliptical distributions (cf. [40, Chapter 1], [46]). Note that a state-
ment similar to the following has been proved in the linear regression setting in
[35, Section 2.2]. We include a proof for mixture of regression models, which is
an extension to the aforementioned result.
A.1.1. Elliptically symmetric distributions
We will provide the proof of Lemma 2.1 by using some general results regarding
elliptical distributions.
Denition A.1. LetXbe aD-dimensional random vector. Then Xis said to
beelliptically distributed (or simply elliptical ) if and only if there exist a vector
2RD, a positive semidenite matrix 2RDDand a function :R+!R
such that the characteristic function t7!'X (t) ofX corresponds to
t7! 
t>t
;t2RD. We write XED(;;) to denote that Xis elliptical.
The function is referred to as the characteristic generator ofX. When
D= 1 the class of elliptical distributions coincides with the class of univariate
symmetric distributions. Thanks to Proposition 1 from [40], it holds that every
anely transformed elliptical random vector is elliptically distributed. More-
over, the following stochastic representation theorem, Theorem A.2, shows that
the converse is true if the transformation matrix has full rank.
Theorem A.2 (Theorem 1 from [19]) .LetXbe aD-dimensional random vec-
tor. Then XED(;;)with rank() =kif and only if
X=+RU(k);
whereU(k)is ak-dimensional random vector uniformly distributed on the unit
hypersphere with k 1dimensionsSk 1=
x2RD:kxk2= 1	
,Ris a non-
negative random variable with distribution function Frelated tobeing stochas-
tically independent of U(k),2RDand=>is a rank factorization of
where 2RDkwith rank() =k.
Note that via the transformation matrix , the spherical random vector U(k)
produces elliptically contoured density surfaces, whereas the generating random
variableRdetermines the distribution's shape, in particular the heaviness of the
distribution's tails. Further, determines the location of the random vector X.
The matrix is called the dispersion matrix orscatter matrix ofX. Therefore,

--- PAGE 58 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 58
it holds that every elliptical distribution belongs to a location-scale-family [51]
dened by an underlying spherical standard distribution.
Example A.3 (Multivariate normal distribution) .Let2RDand2
RDksuch that :=>2RDDis positive denite. The random vec-
torXD(;) is elliptically distributed since Xis representable as X=
+p
2
kU(k). The underlying spherical standard distribution is the standard
normal distribution. Further, since s7!exp( s=2) is the characteristic gener-
ator for the class of normal distributions, the characteristic function of X 
corresponds to t7!X (t) = exp 
 t>t
;t2Rd.
We next describe some important results on the conditional distributions of
elliptical random vectors (cf. [19, Corollary 5], [40, Chapter 1]).
Theorem A.4. LetXED(;;)with rank() =k. It holds that:
(a)E(X) =.
(b) var (X) =E(R2)
k= 20(0), ifis dierentiable at 0.
(c) The sum of independent elliptically distributed random vector with the same
dispersion matrix is elliptically too. Furthermore, the sum of two depen-
dent elliptical random vectors with the same dispersion matrix, which are
dependent only through their radial parts R, is also elliptical [46, Theorem
4.1]. More precisely, let RandeRbe nonnegative random variables and let
X:=+RZED(;;)andeX:=e+eReZED
e;;e
, where
R;eR
,Z, andeZare independent. Then X+eXED(+e;;).
(d) Ane transformation: every anely transformed and particularly every lin-
early combined elliptical random vector is elliptical, too. More formally, for
anyb2RL,A2RLD, and Y=b+AX where X=+RU(k)with
2RDk, it follows that Y=EL 
b+A;AA>;
since
Y=b+A
+RU(k)
= (A+b) +RAU(k): (A.1)
(e) Marginal distribution: let Pm2 f0;1gmD(mD)be a permutation
and deletion matrix, i.e.,Pmhas only binary entries of 0's and 1's and
PmP>
m= Im. So the transformation PmX=:Ypermutes and deletes
certain components of Xsuch that Yis ak-dimensional random vector
containing the remaining components of Xand having a (multivariate)
marginal distribution with respect to the joint distribution of X. Then by
(A.1) , we obtain YEm 
Pm;PmP>
m;
since
Y=Pm
+RU(k)
=Pm+RPmU(k): (A.2)
(f) Conditional distribution: let X= (X1;X2), where X1is ak-dimensional
sub-vector of X, and let =1112
2122
2RDD. Provided the condi-
tional random vector X2jX1=x1exists, it is also elliptically distributed:

--- PAGE 59 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 59
X2j(X1=x1)ED k(;;). Moreover, it can be presented stochas-
tically by
X2j(X1=x1) =+RU(D k) 
andU(D k)is uniformly distributed on S(D k 1), and
R=Rp
1 j
Rp
U(k)= 1
11(x1 1)
;
=2+21 1
11(x1 1);
=22 21 1
1112:
whereBeta (k=2;(D k)=2)andR;;U(k)andU(D k)are mutually
independent, and = ( )> .
A.1.2. Relation between forward and inverse regression
Proposition 1 from [26], a multivariate extension of [47], leads to a link between
GLLiM, dened in (2.16) models, and a Gaussian mixture model on the joint
variable [ X;Y]. This result motivates us to establish the general proof for the
relationship between forward and inverse mixture of elliptical regression models.
More precisely, we consider the following generative model, conditionally on the
cluster label:
[X;Y]j(Z=k)EL+D(mk;Vk;); (A.3)
whereEL+Ddenotes an elliptical distribution of dimension D+L, and are mk
andVkits location and scale parameters, respectively.
When applying the inverse regression strategy in the context of mixture of
elliptical locally-linear mapping, the key point is to account for (A.4):
X=KX
k=1I(Z=k) (AkY+bk+Ek); (A.4)
where Ak2RDLand vector bk2RD, and Ekis an error term capturing
both the reconstruction error due to the local ane approximation and the
observation noise in RD, into the parameterization of mkandVk. GivenZ=k,
it follows from (A.3) that Yis also elliptical distribution by using Theorem A.4
(e) and Ycan be assumed to have a location ck2RLand a scale matrix
 k2RLL. We assume further that the error term EkE(0;ek;k) is an
unobserved centered elliptical random noise with residual covariance matrix k
of typeek, and is independent of Y. Then, using (A.4) and Theorem A.4, we

--- PAGE 60 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 60
have
E(Xj(Z=k)) = E(AkY+bk+Ek) =Akck+bk;
var (Xj(Z=k)) = var ( AkY) + var ( Ek) =Ak kA>
k+k;
cov (X;Yj(Z=k)) = Ak k;cov (Y;X) = cov ( Y;AkY) = >
kA>
k;
mk=Akck+bk
ck
;
Vk=k+Ak kA>
kAk k
(Ak k)> k
:
(A.5)
Note that in the forward and inverse regression problems of elliptical locally-
linear mapping (containing the Gaussian case (2.10), (2.11), (2.15) and (2.14)),
by using Theorem A.4, the joint distribution dened in (A.3) allows us to con-
sider a mixture of linear regression problem, characterized by the following
marginal and conditional distributions:
Xj(Z=k)ED(c
k; 
k;); (A.6)
Yj(X;Z=k) =A
kX+b
k+E
k; (A.7)
Yj(Z=k)ED(ck; k;); (A.8)
Xj(Y;Z=k) =AkY+bk+Ek; (A.9)
where EkE(0;ek;k) and E
kE 
0;e
k;
k
.
Then, we claim that the joint distribution, dened in (A.3) and (A.5), leads to
the marginal and the conditional distributions of Equations (A.6) to (A.9) and
to a mapping between their mean and variance parameters. Indeed, by using
conditioning properties of elliptical distributions, see more in Theorem A.4,
implies the following marginal for Xand conditional distribution for Ygiven
Xas follows:
Xj(Z=k)ED 
Akck+bk;k+Ak kA>
k;
; (A.10)
Yj(X;Z=k)EL
myx
k;yx
k;e
; (A.11)
where the explicit expression of the characteristic function ecan be found in
[19, Corollary 5], and
myx
k=ck+ >
kA>
k 
k+Ak kA>
k 1(X Akck bk);
yx
k= k  >
kA>
k 
k+Ak kA>
k 1Ak k= 
  1
k+A>
k 1
kAk 1;
with the fact that the last equality is the Woodbury matrix identity. Note that
the locations and scale matrices of the conditional distribution do not depend
upon the third parameter of the joint distribution, and consequently, we do
not describe the explicit expression for e. We then utilize again the Woodbury
matrix identity and the symmetric property of  to identify (A.6) and (A.7)
with (A.10) and (A.11), respectively, which implies the following important

--- PAGE 61 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 61
connections:
c
k=Akck+bk; 
k=k+Ak kA>
k;
k= 
  1
k+A>
k 1
kAk 1;
A
k= >
kA>
k 
k+Ak kA>
k 1
= >
kA>
k 1
k  >
kA>
k 1
kAk 
  1
k+A>
k 1
kAk 1A>
k 1
k
=
 k 
  1
k+A>
k 1
kAk
  kA>
k 1
kAk 
  1
k+A>
k 1
kAk 1A>
k 1
k
= 
  1
k+A>
k 1
kAk 1A>
k 1
k=
kA>
k 1
k;
b
k=ck+ >
kA>
k 
k+Ak kA>
k 1( Akck bk)
=ck+ 
  1
k+A>
k 1
kAk 1A>
k 1
k( Akck bk)
= 
  1
k+A>
k 1
kAk 1 
  1
k+A>
k 1
kAk
ck+A>
k 1
k( Akck bk)
=
k(  1
kck A>
k 1
kbk):
Therefore, the desired results then are obtained by using the fact that the mul-
tivariate normal distribution not only has the property of having Gaussian
marginal and conditional distributions [13, Sections 2.3.1 and 2.3.2] but also
belongs to elliptical distributions (detailed in Example A.3). Furthermore, it
should be stressed that several versions of multivariate t-distributions ( e.g., Sec-
tion 5.5, page 94 of [54], [36]) have the previous property. This leads to the in-
verse regression model based on the multivariate t-distributions [84]. It will be
interesting to nd other sub-classes of elliptically contoured distributions that
have the closedness property on marginal and conditional distribution so that
the previous inverse regression trick can be applied.
A.2. Proof of Lemma 4.5: Bracketing entropy of Gaussian gating
networks
Note that the rst inequality of Lemma 4.5 comes from [70, Lemma 4] and
describes relationship between the bracketing entropy of PKand the entropy of
WK. Therefore, Lemma 4.5 is obtained by proving that there exists a constant
CWKsuch that82(0;2],
Hdksupk1(;WK)dim (WK)
CWK+ ln1

; (A.12)
where dim (WK) =K 1 +KL+KL(L+1)
2.
In order to establish the proof for (A.12), we have to construct rstly the -
covering K 1;!ofK 1via Lemma A.5, which is proved in Appendix A.2.1.
Lemma A.5 (Covering number of probability simplex with maximum norm) .
Given any>0, any2K 1, we can choose b2K 1;!, an-covering
ofK 1, so that maxk2[K]jk bkj. Furthermore, it holds that
N(;K 1;kk1)K(2e)K=2
K 1: (A.13)

--- PAGE 62 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 62
Then, by denition of the covering number, (A.12) is obtained immediately
via Lemma A.6, which controls the covering number of WKand is proved in
Appendix A.2.2.
Lemma A.6. Given a bounded set YinRLsuch thatY=
y2RL:kyk1CY	
,
it holds thatWKhas a covering number satised N 
;WK;dksupk1
C dim(WK),
for some constant C.
Indeed, Lemma A.6 implies the desired result by noting that
Hdksupk1(;WK) = lnN 
;WK;dksupk1
lnC
dim(WK)
= dim (WK)1
dim (WK)lnC+ ln1

= dim (WK)
CWK+ ln1

:
A.2.1. Proof of Lemma A.5
Note that [41, Lemma 2] provide a result for controlling a -Hellinger bracket-
ing of K 1. However, such result can not be applied for our Lemma A.5 since
they use-Hellinger bracketing entropy while we use -covering number for
the probability complex with maximum norm.
Given any= (k)k2[K]2K 1, let= (k)k2[K]wherek=pk;8k2
[K]. Then2K 1if and only if 2Q+\U, whereUis the surface of
the unit sphere and Q+is the positive quadrant of RK. Next, we divide the
unit cube in RKinto disjoint cubes with sides parallel to the axes and sides of
length=p
K. Let ( Cj)j2[N]is the subset of these cubes that have non-empty
intersection with Q+\U. For anyj2[N], letj= (j;k)k2[K]be the center of
the cube Cjand2
j=
2
j;k
k2[K].
Thenfjgj2[N]is a=
2p
K
-covering of Q+\U, since we have for any
= (k)k2[K]2Q+\U, there exists j02[N] such that2Cj0, and
k j0k1= max
k2[K]jk j0;kj
2p
K: (A.14)
Therefore, it follows that K 1;!:=
2
j	
j2[N]is a-covering of K 1, since
for any= (k)k2[K]2K 1, (A.14) leads to the existence of j02[N], such
that
 2
j0
1= max
k2[K]2
k 2
j0;k= max
k2[K]fjk j0;kjjk+j0;kjg

2p
Kmax
k2[K]jk+j0;kjp
K;

--- PAGE 63 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 63
where we used the fact that max k2[K]jk+j0;kj2. Now, it remains to count
the number of cubes N. LetTa=fz2Q+:kzk2agand letC=S
j2[N]Cj.
Note thatCT 1+ T1 T, and so
Volume(T)Volume(C) =Np
KK
:
Note that here we use the notation for the Archimedes' constant, which diers
from= (k)k2[K]for the mixing proportion of the GLoME model. Then, we
deneVK(a) =aKK=2as the volume of a sphere of radius a. Sincez!zze z
and (1 +)K (1 )K=KR1+
1 zK 1dz2K(1 +)K 1, it follows
that
N(;K 1;kk1)
NVolume (C)

=p
KK=1
2KVK(1 +) VK(1 )

=p
KK
=1
2Kh
(1 +)K (1 )Ki

=p
KKK=2
(K=2)!e
2K=2h
(1 +)K (1 )Ki
K
K(2e)K=2
K 1:
A.2.2. Proof of Lemma A.6
In order to nd an upper bound for a covering number of WK, we wish to
construct a nite -covering WK;!ofWK, with respect to the distance d ksupk1.
That is, given any  > 0;w(;!)2WK, we aim to prove that there exists
w(;b!)2WK;!such that
dksupk1(w(;!);w(;b!)) = max
k2[K]sup
y2YjWK(y;!) WK(y;b!)j:(A.15)

--- PAGE 64 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 64
In order to accomplish such task, given any positive constants c; ;, and
anyk2[K], let us dene
F=fY3 y7!ln (L(y;c; )) :kck1Ac;a m( )M( )A g;
Fck=n
ln (L(;ck; k)) : ln (L(;ck; k))2F;
ck;j2f CY+lc=L:l= 0;:::;d2CYL=ceg;j2[L]o
;(A.16)
Fck; k=(
ln (L(;ck; k)) : ln (L(;ck; k))2Fck; [vec (  k)]i;j=i;j 
L2;
i;j=j;i2Z\
 jL2A 
 k
;jL2A 
 k
;i2[L];j2[L])
;(A.17)
WK;!=
w(;!) :w(;!)2WK;
8k2[K];ln (L(;ck; k))2Fck; k;2K 1;!
: (A.18)
Here,deandbcare ceiling and oor functions, respectively, and vec ( ) is an
operator that stacks matrix columns into a column vector. In particular, we
denote K 1;!as a-covering of K 1, which is dened in Lemma A.5.
By the previous denition, it holds that 8k2[K],Fck; kF ckF, and
WK;!WK.
Next, we claim that WK;!is a nite-covering ofWKwith respect to the
distance dksupk1. To do this, for any w(;!) = (ln (kL(;ck; k)))k2[K]2
WK, ln (L(;ck; k))2F;2K 1, and for any k2[K], by (A.16), we rst
choose a function ln ( L(;bck; k))2Fckso that
kbck ckk1=LX
j=1jbck;j ck;jjLc
L=c:
Furthermore, by (A.17), we can obtain a result to construct the covariance
matrix lattice. That is, any ln ( L(;bck; k))2Fckcan be approximated by
ln
L
;bck;b k
2Fck; ksuch that
vec
b k
 vec ( k)
1vec
b k  k
1
=LX
i=1LX
j=1h
vec
b k  ki
i;jL2 
L2= :(A.19)
Note that since for any k2[K], (y;ck;vec ( k))7!ln (L(y;ck; k)) is
dierentiable, it is also continuous w.r.t. yand its parameters ckand k.
Thus, for every xed y2 Y, for everybck;ck2 X withbckck, and for
everyb k, k, where vec
b k
vec ( k), we can apply the mean value the-
orem (see [37, Lemma 2.5.1]) to ln ( L(y;; k)) and ln (L(y;bck;)) on the

--- PAGE 65 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 65
intervals [bck;ck] andh
vec
b k
;vec ( k)i
for somezck2(bck;ck) andz k2
vec
b k
;vec ( k)
, respectively, to get
ln (L(y;bck; k)) ln (L(y;ck; k)) = (bck ck)>rckln (L(y;zck; k));
ln
L
y;bck;b k
 ln (L(y;bck; k))
=
vec
b k
 vec ( k)>
rvec( k)ln (L(y;bck;z k)):
Moreover, ( y;ck;vec ( k))7! r ckln (L(y;zck; k)) and ( y;ck;vec ( k))7!
rvec( k)ln (L(y;bck;z k)) are continuous functions on the compact set U:=
YY [a ;A ]L2
leads to they attain minimum and maximum values (see [37,
Theorem 1.8.8]). That is, we can set
0<(Cc)>
1;:::;L:= max
k2[K]sup
(y;ck;vec( k))2UjrcklnjL(y;ck; k)jj<1;
0<(C )>
1;:::;L2:= max
k2[K]sup
(y;ck;vec( k))2Urvec( k)lnjL(y;ck; k) (x)j<1:
Therefore, by the Cauchy{Schwarz inequality, we have
sup
y2Yjln (L(y;bck; k)) ln (L(y;ck; k))jjbck ckj>(Cc)>
1;:::;L
=Cckbck ckk1Cc;
sup
y2Yln
L
y;bck;b k
 ln (L(y;bck; k))C vec
b k  k
1C  ;
and by using the triangle inequality, it follows that
max
k2[K]sup
y2Yln
L
y;bck;b k
 ln (L(y;ck; k))Ccc+C  :(A.20)
Moreover, for every 2K 1, Lemma A.5 implies that we can choose b2
K 1;!so that max k2[K]jk bkj. Notice that [ a;1)3t7!ln(t);a>
0 is a Lipschitz continuous function on [ a;1]. Indeed, by the mean value
theorem, it holds that there exists c2(t1;t2), such that
jln (t1) ln (t2)j= ln0(c)jt1 t2j1
ajt1 t2j, for allt1;t22[a;1):
(A.21)

--- PAGE 66 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 66
Therefore, (A.15) can be obtained by the following evaluation
dksupk1(w(;!);w(;b!))
= max
k2[K]sup
y2Yln (kL(y;ck; k)) ln
bkL
y;bck;b k
= max
k2[K]sup
y2Yln (k) ln (bk) + ln (L(y;ck; k)) ln
L
y;bck;b k
max
k2[K]jln (k) ln (bk)j+ max
k2[K]sup
y2Yln (L(y;ck; k)) ln
L
y;bck;b k
1
amax
k2[K]jk bkj+Ccc+C  (using (A.21) and (A.20))

a+Ccc+C  (using Lemma A.5) 
3+
3+
3=;
where we choose =a
3;c=
3C; =
3C . Finally, we get the covering
number
N 
;WK;dksupk1
card ( WK;!) =2CYL
cKL2A L2
 L(L+1)
2K
N(;K 1;kk1)
=C
KL+KL(L+1)
2+K 1=C
dim(WK);
where
C= (6CcCYL)KL 
6C A L2L(L+1)
2K3
aK 1
K(2e)K=2:
A.3. Proof of Lemma 4.7: Bracketing entropy of Gaussian experts
with 1-block covariance matrices
Note that the proof of Lemma 4.7 is adapted from [70] and is given here for the
sake of completeness.
Indeed, Lemmas 1 and 2 from [70] imply that there exists a constant CK;d=
lnp
2 +p
DdT 
(when K;d=K
b;d) orCK;d= lnp
2 +p
D d+L
L
T
(when K;d=K
p;d) such that,82 
0;p
2
,
Hdksupk1(;K;d)dim (K;d)
CK;d+ ln1

: (A.22)
Next, we rely on Proposition A.7 for constructing of Gaussian brackets to for
the Gaussian experts.

--- PAGE 67 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 67
Proposition A.7 (Proposition 2 from [70]) .Let17
29and=25( 1
2)
49(1+2
5).
For any 0< p
2, anyD1, and any 1
5q
2cosh(2
5)+1
2
D, let
(d;B;A;P)2K;d[B ;B+]A( ;+)SO(D)and
ed;eB;eA;eP
2
K;d[B ;B+]A ( ;+1)SO(D), and dene =BPAP>,e=
eBePeAeP>,
t (x;y) = (1 +) DD
x;ed(y);(1 +) 1e
;and
t+(x;y) = (1 +)DD
x;ed(y);(1 +)e
:
If
8
>>>><
>>>>:8y2Y;kd(y) ed(y)k2DL   
+2

(1 +2
25) 1eBBeB
8i2[D];A 1
i;i eA 1
i;i1
10
+
8x2RD;kPx ePxk1
10 
+kxk
then [t ;t+]is a
5Hellinger bracket such that t (x;y)D(x;d(y);)
t+(x;y).
Then, following the same argument as in [70, Appendix B.2.3], Proposi-
tion A.7 allows us to construct nets over the spaces of the means, the volumes,
the eigenvector matrices, the normalized eigenvalue matrices and then control
the bracketing entropy of GK;d. More precisely, three dierent contexts are con-
sidered for the mean, volume and matrix parameters. They can be all known
(?= 0), unknown but common to all classes ( ?=c), unknown and possibly
dierent for every class ( ?=K). For example, [ vK;B0;P0;A0] denotes a model
in which only mean vectors are assumed to be free. Then, we obtain
H[];supymaxkdx
5;GK;d
dim (GK;d)
CGK;d+ ln1

; (A.23)
where dim (GK;d) =Zv;?+ZB;?+D(D 1)
2ZP;?+ (D 1)ZA;?. Here,Zv;K=
dim (K;d);Zv;c= dim ( 1);Zv;0= 0,ZB;0=ZP;0=ZA;0= 0,ZB;c=
ZP;c=ZA;c= 1,ZB;K=ZP;K=ZA;K=K, and given a universal constant
cU,
CGK;d= ln 
5Ds
2cosh2
5
+1
2!
+CK;d+1
2ln+
DkB 2
 
+ ln0
@4 + 129 ln
B+
B 
101
A+D(D 1)
2ln (cU)
+ ln10+
 
+ ln4
5+52+
5 ln+
 
:

--- PAGE 68 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 68
A.4. Proof of Lemma 4.11
We rst aim to prove that d2
n(s;t)supyd2
x(s;t). Indeed, by denition, it
follows that
d2
n(s;t) =EY[n]"
1
nnX
i=1d2
x(s(jYi);t(jYi))#
=1
nnX
i=1EY[n]
d2
x(s(jYi);t(jYi))
=1
nnX
i=1Z
Yd2
x(s(jy);t(jy))sx;0(y)dy
sup
yd2
x(s;t)1
nnX
i=1Z
Ysx;0(y)dy= sup
yd2
x(s;t);
wheresx;0denotes that marginal PDF of s0, w.r.t. x. Consequently, it holds
that d
n(s;t) =p
d2
n(s;t)q
supyd2x(s;t) = supydx(s;t). To prove that
H[];d
n(;Sm)H [];supydx(;Sm);
it is sucient to check that
N[];d
n(;Sm)N [];supydx(;Sm):
By using the denition of bracketing entropy in (4.1) and d
n(s;t)supydx(s;t),
given
A=(
n2N?:9t 
1;t+
1;:::;t 
n;t+
ns.t sup
ydx(s;t) 
t 
k;t+
k
;Smn[
k=1
t 
k;t+
k)
;
B=(
n2N?:9t 
1;t+
1;:::;t 
n;t+
ns.t d
n 
t 
k;t+
k
;Smn[
k=1
t 
k;t+
k)
;
it leads to that ABand then (4.8) follows, since
N[];supydx(s;t)(;Sm) = minAminB=N[];d
n(;Sm):
With the similar argument as in the proof of (4.8), it holds that dPK(g+;g )
supydk(g+;g ) and (4.9) is proved.
A.5. Proof of Lemma 4.12: Bracketing entropy of Gaussian experts
with multi-block-diagonal covariance matrices
It is worth mentioning that without any structures on covariance matrices of
Gaussian experts from the collection M, Lemma 4.12 can be proved using [70,
Proposition 2 and Appendix B.2.3], for constructing of Gaussian brackets to
deal with the Gaussian experts. However, dealing with block-diagonal covari-
ance matrices with random subcollection is not a trivial extension. We have to
establish more constructive bracketing entropies in the spirits of [62, 29, 33].

--- PAGE 69 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 69
Given anyk2[K], by dening
Gd;Bk=
(x;y)7!(x;k;d(y);k(Bk)) =:k:
k;d2k;d;k(Bk)2Vk(Bk)	
;(A.24)
it follows thatGK;d;B=QK
k=1Gd;Bk, whereQstands for the cartesian product.
By using Lemma A.8, which is proved in Appendix A.5.1, it follows that
H[];dGK;d;B
2;GK;d;B
KX
k=1H[];dGd;Bk
2p
K;Gd;Bk
: (A.25)
Lemma A.8. GivenGK;d;B=QK
k=1Gd;Bk, whereGd;Bkis dened in (A.24) , it
holds that
N[];dGK;d;B
2;GK;d;B
KY
k=1N[];dGd;Bk
2p
K;Gd;Bk
;
where for any +; 2GK;d;Band any+
k; 
k2Gd;Bk;k2[K],
d2
GK;d;B 
+; 
=EY[n]"
1
nnX
i=1KX
k=1d2 
+
k(;Yi); 
k(;Yi)#
;
d2
Gd;Bk 
+
k; 
k
=EY[n]"
1
nnX
i=1d2 
+
k(;Yi); 
k(;Yi)#
:
Lemma 4.12 is proved via (A.25) and Lemma A.9, which is proved in Ap-
pendix A.5.2.
Lemma A.9. By deningGd;Bkas in (A.24) , for all2(0;p
2], it holds that
H[];dGd;Bk
2;Gd;Bk
dim (Gd;Bk)
CGd;Bk+ ln1

;where (A.26)
DBk=GkX
g=1card
d[g]
k
card
d[g]
k
 1
2;
CGd;Bk=DBkln
6p
6MD2(D 1)
mDBk
+ dim ( k;d) ln
6p
2Dexp(Ck;d)pm
dim (Gd;Bk):
Indeed, (A.25) and (A.26) lead to
H[];dGK;d;B
2;GK;d;B
KX
k=1H[];dGd;Bk
2p
K;Gd;Bk
KX
k=1dim (Gd;Bk)
CGd;Bk+ lnp
K
+ ln1

dim (GK;d;B)
CGK;d;B+ ln1

:

--- PAGE 70 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 70
Here,CGK;d;B=PK
k=1CGd;Bk+lnp
K
and note that dim ( GK;d;B) =PK
k=1dim (Gd;Bk),
dim (Gd;Bk) =DBk+ dim ( k;d), dim ( k;d) =Ddk;d;Ck;d=p
Ddk;dTk;d
(in cases where linear combination of bounded functions are used for means,
i.e.,k;d=b) or dim ( k;d) =D dk;d+L
L
,Ck;d=p
D dk;d+L
L
Tk;d(in
cases where we use polynomial means, i.e.,k;d=p).
A.5.1. Proof of Lemma A.8
By the denition of the bracketing entropy in (4.1), for each k2[K], letnh
l; 
k;l;+
kio
1lNGd;Bkbe a minimal covering of kbrackets for dGd;Bkof
Gd;Bk, with cardinality NGd;Bk. This leads to
8l2h
NGd;Bki
;dGd;Bk
l; 
k;l;+
k
k:
Therefore, we claim that the setnQK
k=1h
l; 
k;l;+
kio
1lNGd;Bkis a covering
of
2-bracket for dGK;d;BofGK;d;Bwith cardinalityQK
k=1N[];dGd;Bk(k;Gd;Bk).
Indeed, let any = (k)k2[K]2GK;d;B. Consequently, for each k2[K];k2
Gd;Bk, there exists l(k)2h
NGd;Bki
, such that
l(k); 
kkl(k);+
k;d2
Gd;Bk
l(k);+
k;l(k); 
k
(k)2:
Then, it follows that 2[ ;+]2nQK
k=1h
l; 
k;l;+
kio
1lNGd;Bk, with =

l(k); 
k
k2[K];+=
l(k);+
k
k2[K], which implies thatnQK
k=1h
l; 
k;l;+
kio
1lNGd;Bkis a bracket covering of GK;d;B.
Now, we want to verify that the size of this bracket is =2 by choosing k=

2p
K;8k2[K]. It follows that
d2
GK;d;B 
 ;+
=EY[n]"
1
nnX
i=1KX
k=1d2
l(k); 
k(;Yi);l(k);+
k(;Yi)#
=KX
k=1EY[n]"
1
nnX
i=1d2
l(k); 
k(;Yi);l(k);+
k(;Yi)#
=KX
k=1d2
Gd;Bk
l(k); 
k;l(k);+
k
K
2p
K2
=
22
:
To this end, by denition of a minimal
2-bracket covering number for GK;d;B,
Lemma A.8 is proved.

--- PAGE 71 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 71
A.5.2. Proof of Lemma A.9
To provide the upper bound of the bracketing entropy in (A.26), our technique
is adapted from the work of [41] for unidimensional Gaussian mixture families,
which is recently generalized to multidimensional case by [62] for Gaussian mix-
ture models. Furthermore, we make use of the results from [33] to deal with
block-diagonal covariance matrices, Vk(Bk);k2[K], and from [70] to handle
the means of Gaussian experts k;d;k2[K].
The main idea in our approach is to dene rstly a net over the parame-
ter spaces of Gaussian experts, k;dVk(Bk);k2[K], and to construct a
bracket covering of Gd;Bkaccording to the tensorized Hellinger distance. Note
that dim (Gd;Bk) = dim ( k;d) + dim ( Vk(Bk)).
Step 1: Construction of a net for the block-diagonal covariance ma-
trices. Firstly, for k2[K], we denote by Adj ( k(Bk)) the adjacency matrix
associated to the covariance matrix k(Bk). Note that this matrix of size D2
can be dened by a vector of concatenated upper triangular vectors. We are
going to make use of the result from [33] to handle the block-diagonal covari-
ance matrices k(Bk), via its corresponding adjacency matrix. To do this, we
need to construct a discrete space for f0;1gD(D 1)=2, which is a one-to-one
correspondence (bijection) with
ABk=fABk2SD(f0;1g) :9k(Bk)2Vk(Bk) s.t Adj ( k(Bk)) =ABkg;
whereSD(f0;1g) is the set of symmetric matrices of size Dtaking values on
f0;1g.
Then, we want to deduce a discretization of the set of covariance matrices.
Lethdenotes Hamming distance on f0;1gD(D 1)=2dened by
d(z;z0) =nX
i=1Ifz6=z0g;for allz;z02f0;1gD(D 1)=2:
Letf0;1gD(D 1)=2
Bkbe the subset off0;1gD(D 1)=2of vectors for which the cor-
responding graph has structure Bk=
d[g]
k
g2[Gk]. Corollary 1 and Proposition
2 from Supplementary Material A of [33] imply that there exists some subset
Roff0;1gD(D 1)=2, as well as its equivalent Adisc
Bkfor adjacency matrices such
that, given >0, and
~Sdisc
Bk() =
k(Bk)2S++
D(R) : Adj ( k(Bk))2Adisc
Bk;
[k(Bk)]i;j=i;j;i;j2 M
;M
\
Z
;

--- PAGE 72 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 72
it holds that for all
k(Bk);ek(Bk)
2
~Sdisc
Bk()2
s.t.k(Bk)6=ek(Bk),
k(Bk) ek(Bk)2
2DBk
2^2;
card
~Sdisc
Bk()
 $
2M
%
D(D 1)
2DBk!DBk
; (A.27)
DBk= dim ( Vk(Bk)) =GkX
g=1card
d[g]
k
card
d[g]
k
 1
2: (A.28)
By choosing 2DBk
2, given k(Bk)2Vk(Bk), then there exists ek(Bk)2
~Sdisc
Bk(), such that
k(Bk) ek(Bk)2
22: (A.29)
Step 2: Construction of a net for the mean functions. Based onek(Bk),
we can construct the following bracket covering of Gd;Bkby dening the nets
for the means of Gaussian experts. The proof of Lemma 1, page 1693, from [70]
implies that
N[];supykk2 
k;d;k;d
 
exp 
Ck;d
k;d!dim(k;d)
:
Here dim ( k;d) =Ddk;d, andCk;d=p
Ddk;dTk;din the general case or
dim (k;d) =D dk;d+L
L
, andCk;d=p
D dk;d+L
L
Tk;din the special case
of polynomial means. Then, by the denition of bracketing entropy in (4.1),
for any minimal k;d-bracketing covering of the means from Gaussian experts,
denoted by Gk;d 
k;d
, it is true that
card 
Gk;d 
k;d
 
exp 
Ck;d
k;d!dim(k;d)
: (A.30)
Therefore, given >0, which is specied later, we claim that the set
8
>><
>>:[l;u]l(x;y) = (1 + 2) D
x;ek;d(y);(1 +) 1ek(Bk)
;
u(x;y) = (1 + 2)D
x;ek;d(y);(1 +)ek(Bk)
;
ek;d2Gk;d 
k;d
;ek(Bk)2~Sdisc
Bk()9
>>=
>>;;
is ak;d-brackets set over Gd;Bk. Indeed, letXY 3 (x;y)7!f(x;y) =
(x;k;d(y);k(Bk)) be a function of Gd;Bk, wherek;d2k;dandk(Bk)2
Vk(Bk). According to (A.29), there exists ek(Bk)2~Sdisc
Bk(), such that
k(Bk) ek(Bk)2
22:

--- PAGE 73 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 73
By denition of Gk;d 
k;d
, there exists ek;d2Gk;d 
k;d
, such that
sup
y2Ykek;d(y) k;d(y)k2
22
k;d: (A.31)
Step 3: Upper bound of the number of the bracketing entropy. Next,
we wish to make use of Lemma A.10 to evaluate the ratio of two Gaussian
densities.
Lemma A.10 (Proposition C.1 from [62]) .Let(;1;1)and(;2;2)
be two Gaussian densities. If 2 1is a positive denite matrix then for all
x2RD,
(x;1;1)
(x;2;2)s
j2j
j1jexp1
2(1 2)>(2 1) 1(1 2)
:
The following Lemma A.11 allows us to fulll the assumptions of Lemma A.10.
Lemma A.11. Assume that 0<  < 2
m=9, and set= 3p=m. Then, for
everyk2[K],(1 +)ek(Bk) k(Bk)andk(Bk) (1 +) 1ek(Bk)are
both positive denite matrices. Moreover, for all x2RD,
x>h
(1 +)ek(Bk) k(Bk)i
xkxk2
2;
x>h
k(Bk) (1 +) 1ek(Bk)i
xkxk2
2:
Proof of Lemma A.11. For all x6=0, since
sup
2vp(k(Bk) ek(Bk))jj=k(Bk) ek(Bk)
2;
where vp denotes the spectrum of matrix,   m=3, and= 3=m, it
follow that
x>h
(1 +)ek(Bk) k(Bk)i
x
= (1 +)x>h
ek(Bk) k(Bk)i
x+x>k(Bk)x
 (1 +)ek(Bk) k(Bk)
2kxk2
2+mkxk2
2
(m (1 +))kxk2
2= (m  )kxk2
2
2
3m 
kxk2
2=kxk2
2>0;
and
x>h
k(Bk) (1 +) 1ek(Bk)i
x
= (1 +) 1x>h
k(Bk) ek(Bk)i
x+
1 (1 +) 1
x>k(Bk)x
m 
1 +
kxk2
2=2
1 +kxk2
2kxk2
2>0 ( since 0<< 1):

--- PAGE 74 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 74
By Lemma A.10 and the same argument as in the proof of Lemma B.9 from
[62], given 0 <<m=3, whereis chosen later, and = 3=m, we obtain
maxl(x;y)
f(x;y);f(x;y)
u(x;y)
(1 + 2) D
2exp 
kk;d(y) ek;d(y)k2
2
2!
:(A.32)
Because ln () is a non-decreasing function, ln (1 + 2 );82[0;1]. Com-
bined with (A.31) where 2
k;d=D, we conclude that
max
lnl(x;y)
f(x;y)
;lnf(x;y)
u(x;y)
 D
2ln (1 + 2) +2
k;d
2 D
2+2
k;d
2= 0:
This means that l(x;y)f(x;y)u(x;y);8(x;y)2XY . Hence, it remains
to bound the size of bracket [ l;u] w.r.t.dGd;Bk. To this end, we aim to verify
thatd2
Gd;Bk(l;u)
2. To do that, we make use of the following Lemma A.12.
Lemma A.12 (Proposition C.3 from [62]) .Let(;1;1)and(;2;2)
be two Gaussian densities with full rank covariance. It holds that
d2((;1;1);(;2;2))
= 2n
1 2D=2j12j 1=4 1
1+ 1
2 1=2exp 1
4(1 2)>(1+2) 1(1 2)o
:
Therefore, using the fact that cosh( t) =e t+et
2, Lemma A.12 leads to, for all
y2Y:
d2(l(;y);u(;y)) =Z
Xh
l(x;y) +u(x;y) 2p
l(x;y)u(x;y)i
dx
= (1 + 2) D+ (1 + 2)D 2
+ d2

;ek;d(y);(1 +) 1ek(Bk)
;
;ek;d(y);(1 +)ek(Bk)
= 2 cosh [Dln (1 + 2)] 2
+ 2
1 2D=2h
(1 +) 1+ (1 +)i D=2ek(Bk) 1=2ek(Bk)1=2
= 2 cosh [Dln (1 + 2)] 2 + 2 2 [cosh (ln (1 + ))] D=2
= 2g(Dln (1 + 2)) + 2h(ln (1 +));
whereg(t) = cosh(t) 1 =e t+et
2 1, andh(t) = 1 cosh(t) D=2. The upper
bounds of terms gandhseparately imply that, for all y2Y,
d2(l(;y);u(;y))2
2 cosh1p
6
2D2+1
42D2
62D2=2
4;
where we choose =3
m;=m
6p
6D,82(0;1];D2N?;m>0, which appears
in (A.32) and satises =
2p
6Dand 0<<m
3. Indeed, studying functions g

--- PAGE 75 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 75
andhyields
g0(t) = sinh(t);g00(t) = cosh(t)cosh(c);8t2[0;c];c2R+;
h0(t) =D
2cosh(t) D=2 1sinh(t);
h00(t) =D
2
 D
2 1
cosh(t) D=2 2sinh2(t) +D
2cosh(t) D=2
=D
2 
1 D
2+ 1sinh(t)
cosh(t)2!
cosh(t) D=2D
2;
where we used the fact that cosh( t)1. Then, since g(0) = 0;g0(0) = 0;h(0) =
0;h0(0) = 0, by applying Taylor's Theorem, it is true that
g(t) =g(t) g(0) g0(0)t=R0;1(t)cosh(c)t2
2;8t2[0;c];
h(t) =h(t) h(0) h0(0)t=R0;1(t)D
2t2
2D2
2t2
2;8t0:
We wish to nd an upper bound for t=Dln (1 + 2),D2N?,=
2p
6D,
2(0;1]. Since ln is an increasing function, then we have
t=Dln
1 +p
6D
Dln
1 +1p
6D
D1p
6D=1p
6;82(0;1];
since ln
1 +1p
6D
1p
6D,8D2N?. Then, since ln (1 + 2 )2;80,
g(Dln (1 + 2))cosh1p
6(Dln (1 + 2))2
2cosh1p
6D2
242;
h(ln (1 +))D2
2(ln (1 +))2
2D22
4:
Note that the set of =2-brackets [ l;u] overGd;Bkis totally dened by the
parameter spaces ~Sdisc
Bk() andGk;d 
k;d
. This leads to an upper bound of the
=2-bracketing entropy of Gd;Bkevaluated from an upper bound of the two set
cardinalities. Hence, given any  >0, by choosing =m
6p
6D,=3
m=
2p
6D,

--- PAGE 76 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 76
and2
k;d=D =D
2p
6Dm
6p
6D=2m
72D, it holds that
N[];dGd;Bk
2;Gd;Bk
card
~Sdisc
Bk()
card 
Gk;d 
k;d
 $
2M
%
D(D 1)
2DBk!DBk 
exp 
Ck;d
k;d!dim(k;d)
(using (A.28) and (A.30))
 
2M6p
6D
mD(D 1)
2DBk!DBk 
6p
2Dexp 
Ck;d
pm!dim(k;d)
= 
6p
6MD2(D 1)
mDBk!DBk 
6p
2Dexp 
Ck;d
pm!dim(k;d)1
DBk+dim( k;d)
:
Finally, by denition of bracketing entropy in (4.1), we obtain
H[];dGd;Bk
2;Gd;Bk
DBkln 
6p
6MD2(D 1)
mDBk!
+ dim ( k;d) ln 
6p
2Dexp 
Ck;d
pm!
+ (DBk+ dim ( k;d)) ln1

= dim (Gd;Bk)
CGd;Bk+ ln1

;
where dim (Gd;Bk) =DBk+ dim ( k;d) and
CGd;Bk=DBkln
6p
6MD2(D 1)
mDBk
+ dim ( k;d) ln
6p
2Dexp(Ck;d)pm
dim (Gd;Bk):
References
[1]Akaike, H. (1974). A new look at the statistical model identication.
IEEE Transactions on Automatic Control 19716{723.
[2]Anderson, D. andBurnham, K. (2002). Model Selection and Multi-
model Inference , 2 ed. A Pratical Information-Theoretic Approach .
Springer-Verlag, New York.
[3]Arlot, S. (2019). Minimal penalties and the slope heuristics: a survey.
Journal de la Soci et e Fran caise de Statistique 1601{106.
[4]Arlot, S. andBach, F. (2009). Data-driven calibration of linear estima-
tors with minimal penalties. Advances in Neural Information Processing
Systems 22.
[5]Arlot, S. andMassart, P. (2009). Data-driven Calibration of Penalties
for Least-Squares Regression. Journal of Machine learning research 10.
ISBN: 1532-4435.

--- PAGE 77 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 77
[6]Barron, A. R. ,Huang, C. ,Li, J. andLuo, X. (2008). The MDL
principle, penalized likelihoods, and statistical risk. Festschrift in Honor
of Jorma Rissanen on the Occasion of his 75th Birthday 33{63.
[7]Baudry, J.-P. (2009). S election de mod ele pour la classication non su-
pervis ee. Choix du nombre de classes., PhD thesis, Universit e Paris-Sud
XI.
[8]Baudry, J.-P. ,Maugis, C. andMichel, B. (2012). Slope heuristics:
overview and implementation. Statistics and Computing 22455{470.
[9]Bertin, K. ,Le Pennec, E. andRivoirard, V. (2011). Adaptive
Dantzig density estimation. Annales de l'I.H.P. Probabilit es et statistiques
4743{74. Publisher: Gauthier-Villars. MR2779396
[10]Biernacki, C. ,Celeux, G. andGovaert, G. (2000). Assessing a mix-
ture model for clustering with the integrated completed likelihood. IEEE
Transactions on Pattern Analysis and Machine Intelligence 22719{725.
[11]Birge, L. andMassart, P. (2007). Minimal penalties for Gaussian
model selection. Probability Theory and Related Fields 13833{73.
[12]Birge, L. ,Massart, P. et al. (1998). Minimum contrast estimators on
sieves: exponential bounds and rates of convergence. Bernoulli 4329{375.
[13]Bishop, C. M. (2006). Pattern Recognition and Machine Learning (In-
formation Science and Statistics) . Springer-Verlag, Berlin, Heidelberg.
[14]Borwein, J. M. andZhu, Q. J. (2004). Techniques of Variational Anal-
ysis. Springer.
[15]Bouchard, G. (2003). Localised Mixtures of Experts for Mixture
of Regressions. In Between Data Science and Applied Data Analysis
(M. Schader ,W. Gaul andM. Vichi , eds.) 155{164. Springer Berlin
Heidelberg, Berlin, Heidelberg.
[16]Brinkman, N. D. (1981). Ethanol Fuel|Single|Cylinder Engine Study
of Eciency and Exhaust Emissions. SAE transactions 1410{1424.
[17]Bunea, F. ,Tsybakov, A. B. ,Wegkamp, M. H. andBarbu, A.
(2010). SPADES and mixture models. The Annals of Statistics 382525 {
2558. Publisher: Institute of Mathematical Statistics.
[18]Butucea, C. ,Delmas, J.-F. ,Dutfoy, A. andFischer, R. (2017). Op-
timal exponential bounds for aggregation of estimators for the Kullback-
Leibler loss. Electronic Journal of Statistics 112258 { 2294. Publisher:
Institute of Mathematical Statistics and Bernoulli Society.
[19]Cambanis, S. ,Huang, S. andSimons, G. (1981). On the theory of
elliptically contoured distributions. Journal of Multivariate Analysis 11
368{385.
[20]Celeux, G. andGovaert, G. (1995). Gaussian parsimonious clustering
models. Pattern recognition 28781{793.
[21]Chamroukhi, F. ,Same, A. ,Govaert, G. andAknin, P. (2010). A
hidden process regression model for functional data description. Applica-
tion to curve discrimination. Neurocomputing 731210{1221.
[22]Cohen, S. andLe Pennec, E. (2011). Conditional density estimation
by penalized likelihood model selection and applications. Technical report,
INRIA .

--- PAGE 78 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 78
[23]Cohen, S. X. andLe Pennec, E. (2013). Partition-based conditional
density estimation. ESAIM: Probability and Statistics 17672{697.
[24]Dalalyan, A. S. andSebbar, M. (2018). Optimal Kullback{Leibler
aggregation in mixture density estimation by maximum likelihood. Math-
ematical Statistics and Learning 11{35. ISBN: 2520-2316.
[25]Deleforge, A. ,Forbes, F. ,Ba, S. andHoraud, R. (2015). Hyper-
Spectral Image Analysis With Partially Latent Regression and Spatial
Markov Dependencies. IEEE Journal of Selected Topics in Signal Pro-
cessing 91037{1048.
[26]Deleforge, A. ,Forbes, F. andHoraud, R. (2015). High-dimensional
regression with gaussian mixtures and partially-latent response variables.
Statistics and Computing 25893{911.
[27]Dempster, A. P. ,Laird, N. M. andRubin, D. B. (1977). Maximum
Likelihood from Incomplete Data via the EM Algorithm. Journal of the
Royal Statistical Society. Series B (Methodological) 391{38.
[28]Devijver, E. (2015). Anl1-oracle inequality for the Lasso in multivariate
nite mixture of multivariate Gaussian regression models. ESAIM: PS 19
649{670.
[29]Devijver, E. (2015). Finite mixture regression: a sparse variable selection
by model selection for clustering. Electronic journal of statistics 92642{
2674.
[30]Devijver, E. (2015). An l1-oracle inequality for the Lasso in nite mix-
ture of multivariate Gaussian regression. ESAIM: Probability and Statis-
tics19649-670.
[31]Devijver, E. (2017). Model-based regression clustering for high-
dimensional data: application to functional data. Advances in Data Anal-
ysis and Classication 11243{279.
[32]Devijver, E. (2017). Joint rank and variable selection for parsimonious
estimation in a high-dimensional nite mixture regression model. Journal
of Multivariate Analysis 1571{13.
[33]Devijver, E. andGallopin, M. (2018). Block-diagonal covariance se-
lection for high-dimensional Gaussian graphical models. Journal of the
American Statistical Association 113306{314.
[34]Devijver, E. ,Gallopin, M. andPerthame, E. (2017). Nonlinear
network-based quantitative trait prediction from transcriptomic data.
arXiv preprint arXiv:1701.07899 .
[35]Devijver, E. andPerthame, E. (2020). Prediction regions through
Inverse Regression. Journal of Machine Learning Research 211{24.
[36]Ding, P. (2016). On the Conditional Distribution of the Multivariate t
Distribution. The American Statistician 70293{295.
[37]Duistermaat, J. J. andKolk, J. A. (2004). Multidimensional real
analysis I: dierentiation 86. Cambridge University Press.
[38]Fang, K. T. ,Kotz, S. andNg, K. W. (1990). Symmetric Multivariate
And Related Distributions . Chapman and Hall.
[39]Forbes, F. ,Nguyen, H. D. ,Nguyen, T. T. andArbel, J. (2021).
Approximate Bayesian computation with surrogate posteriors. Preprint

--- PAGE 79 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 79
hal-03139256 .
[40]Frahm, G. (2004). Generalized Elliptical Distributions : Theory and Ap-
plications . Universit at zu K oln.
[41]Genovese, C. R. andWasserman, L. (2000). Rates of convergence for
the Gaussian mixture sieve. The Annals of Statistics 281105{1127.
[42]Golub, T. R. ,Slonim, D. K. ,Tamayo, P. ,Huard, C. ,Gaasen-
beek, M. ,Mesirov, J. P. ,Coller, H. ,Loh, M. L. ,Downing, J. R. ,
Caligiuri, M. A. ,Bloomfield, C. D. andLander, E. S. (1999).
Molecular classication of cancer: class discovery and class prediction by
gene expression monitoring. Science (New York, N.Y.) 286531{537.
[43]Ho, N. ,Nguyen, X. et al. (2016). Convergence rates of parameter esti-
mation for some weakly identiable nite mixtures. The Annals of Statis-
tics442726{2755.
[44]Ho, N. ,Nguyen, X. et al. (2016). On strong identiability and conver-
gence rates of parameter estimation in nite mixtures. Electronic Journal
of Statistics 10271{307.
[45]Ho, N. ,Yang, C.-Y. andJordan, M. I. (2019). Convergence Rates for
Gaussian Mixtures of Experts. arXiv preprint arXiv:1907.04377 .
[46]Hult, H. andLindskog, F. (2002). Multivariate extremes, aggregation
and dependence in elliptical distributions. Advances in Applied Probability
34587{608.
[47]Ingrassia, S. ,Minotti, S. C. andVittadini, G. (2012). Local Statis-
tical Modeling via a Cluster-Weighted Approach with Elliptical Distribu-
tions. Journal of Classication 29363{401.
[48]Jacobs, R. A. ,Jordan, M. I. ,Nowlan, S. J. andHinton, G. E.
(1991). Adaptive mixtures of local experts. Neural computation 379{87.
[49]Jiang, W. andTanner, M. A. (1999). Hierarchical mixtures-of-experts
for exponential family regression models: approximation and maximum
likelihood estimation. Annals of Statistics 987{1011.
[50]Jordan, M. I. andJacobs, R. A. (1994). Hierarchical mixtures of ex-
perts and the EM algorithm. Neural computation 6181{214.
[51]Kelker, D. (1970). Distribution Theory of Spherical Distributions and
a Location-Scale Parameter Generalization. Sankhy a: The Indian Journal
of Statistics, Series A (1961-2002) 32419{430.
[52]Kingma, D. P. andWelling, M. (2013). Auto-encoding variational
bayes. arXiv preprint arXiv:1312.6114. Proceedings of the 2nd Interna-
tional Conference on Learning Representations (ICLR) .
[53]Kosorok, M. R. (2007). Introduction to empirical processes and semi-
parametric inference . Springer Science & Business Media.
[54]Kotz, S. andNadarajah, S. (2004). Multivariate T-Distributions and
Their Applications . Cambridge University Press, Cambridge.
[55]Lathuili ere, S. ,Juge, R. ,Mesejo, P. ,Mu~noz-Salinas, R. andHo-
raud, R. (2017). Deep mixture of linear inverse regressions applied to
head-pose estimation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition 4817{4825.
[56]L^e Cao, K.-A. ,Rossouw, D. ,Robert-Grani e, C. andBesse, P.

--- PAGE 80 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 80
(2008). A sparse PLS for variable selection when integrating omics data.
Statistical applications in genetics and molecular biology 7Article 35.
[57]Li, K.-C. (1991). Sliced Inverse Regression for Dimension Reduction.
Journal of the American Statistical Association 86316{327.
[58]Mallows, C. L. (1973). Some Comments on CP. Technometrics 15661{
675.
[59]Masoudnia, S. andEbrahimpour, R. (2014). Mixture of experts: a
literature survey. Articial Intelligence Review 42275{293.
[60]Massart, P. (2007). Concentration Inequalities and Model Selection:
Ecole d'Et e de Probabilit es de Saint-Flour XXXIII-2003 . Springer.
[61]Massart, P. andMeynet, C. (2011). The Lasso as an l1-ball model
selection procedure. Electronic Journal of Statistics 5669-687.
[62]Maugis, C. andMichel, B. (2011). A non asymptotic penalized criterion
for Gaussian mixture model selection. ESAIM: Probability and Statistics
1541{68.
[63]Maugis, C. andMichel, B. (2011). Data-driven penalty calibration: A
case study for Gaussian mixture model selection. ESAIM: PS 15320{339.
[64]Maugis-Rabusseau, C. andMichel, B. (2013). Adaptive density es-
timation for clustering with Gaussian mixtures. ESAIM: Probability and
Statistics 17698{724.
[65]McLachlan, G. J. andKrishnan, T. (1997). The EM Algorithm and
Extensions . Wiley.
[66]McLachlan, G. J. andPeel, D. (2000). Finite Mixture Models . John
Wiley & Sons.
[67]Mendes, E. F. andJiang, W. (2012). On convergence rates of mixtures
of polynomial experts. Neural computation 243025{3051.
[68]Meynet, C. (2013). Anl1-oracle inequality for the Lasso in nite mixture
Gaussian regression models. ESAIM: Probability and Statistics 17650-
671.
[69]Moerland, P. (1999). Classication using localized mixture of experts.
InNinth International Conference on Articial Neural Networks 2838{
843.
[70]Montuelle, L. ,Le Pennec, E. et al. (2014). Mixture of Gaussian re-
gressions model with logistic weights, a penalized maximum likelihood
approach. Electronic Journal of Statistics 81661{1695.
[71]Nguyen, D. V. andRocke, D. M. (2002). Tumor classication by par-
tial least squares using microarray gene expression data. Bioinformatics
1839{50.
[72]Nguyen, H. D. andChamroukhi, F. (2018). Practical and theoretical
aspects of mixture-of-experts modeling: An overview. Wiley Interdisci-
plinary Reviews: Data Mining and Knowledge Discovery 8e1246.
[73]Nguyen, H. D. ,Chamroukhi, F. andForbes, F. (2019). Approxi-
mation results regarding the multiple-output Gaussian gated mixture of
linear experts model. Neurocomputing 366208{214.
[74]Nguyen, H. D. ,Lloyd-Jones, L. R. andMcLachlan, G. J. (2016).
A universal approximation theorem for mixture-of-experts models. Neural

--- PAGE 81 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 81
computation 282585{2593.
[75]Nguyen, H. D. ,Nguyen, T. ,Chamroukhi, F. andMcLach-
lan, G. J. (2021). Approximations of conditional probability density
functions in Lebesgue spaces via mixture of experts models. Journal of
Statistical Distributions and Applications 813.
[76]Nguyen, T. (2021). Model Selection and Approximation in High-
dimensional Mixtures of Experts Models: from Theory to Practice, PhD
Thesis, Normandie Universit e.
[77]Nguyen, T. ,Chamroukhi, F. ,Nguyen, H. andMcLachlan, G.
(2021). Approximation of probability density functions via location-scale
nite mixtures in Lebesgue spaces. Communications in Statistics - Theory
and Methods .
[78]Nguyen, T. ,Nguyen, H. D. ,Chamroukhi, F. andMcLach-
lan, G. J. (2020). Approximation by nite mixtures of continuous den-
sity functions that vanish at innity. Cogent Mathematics & Statistics 7
1750861.
[79]Nguyen, T. ,Nguyen, H. D. ,Chamroukhi, F. andMcLach-
lan, G. J. (2020). An l1-oracle inequality for the Lasso in mixture-of-
experts regression models. arXiv preprint arXiv:2009.10622 .
[80]Nguyen, X. (2013). Convergence of latent mixing measures in nite and
innite mixture models. The Annals of Statistics 41370{400.
[81]Norets, A. et al. (2010). Approximation of conditional densities by
smooth mixtures of regressions. The Annals of statistics 381733{1766.
[82]Norets, A. andPati, D. (2017). Adaptive Bayesian estimation of con-
ditional densities. Econometric Theory 33980{1012.
[83]Norets, A. andPelenis, J. (2014). Posterior consistency in conditional
density estimation by covariate dependent mixtures. Econometric Theory
30606{646.
[84]Perthame, E. ,Forbes, F. andDeleforge, A. (2018). Inverse regres-
sion approach to robust nonlinear high-to-low dimensional mapping. Jour-
nal of Multivariate Analysis 1631{14.
[85]Rakhlin, A. ,Panchenko, D. andMukherjee, S. (2005). Risk bounds
for mixture density estimation. ESAIM: PS 9220{229.
[86]Ramamurti, V. andGhosh, J. (1996). Structural adaptation in mixture
of experts. In Proceedings of 13th International Conference on Pattern
Recognition 4704{708 vol.4.
[87]Ramamurti, V. andGhosh, J. (1998). Use of localized gating in mixture
of experts networks. In Proc.SPIE 3390 .
[88]Redner, R. A. andWalker, H. F. (1984). Mixture densities, maximum
likelihood and the EM algorithm. SIAM review 26195{239.
[89]Rigollet, P. (2012). Kullback{Leibler aggregation and misspecied gen-
eralized linear models. The Annals of Statistics 40639 { 665. Publisher:
Institute of Mathematical Statistics.
[90]Sato, M. andIshii, S. (2000). On-line EM algorithm for the normalized
gaussian network. Neural computation 12407{432.
[91]Schwarz, G. et al. (1978). Estimating the dimension of a model. The

--- PAGE 82 ---
T. Nguyen et al./A non-asymptotic model selection in mixture of experts models 82
annals of statistics 6461{464.
[92]Stadler, N. ,Buhlmann, P. andvan de Geer, S. (2010).l1-
penalization for mixture regression models. TEST 19209{256.
[93]R Core Team (2020). R: A language and environment for statistical
computing. Vienna, Austria .
[94]Tibshirani, R. (1996). Regression shrinkage and selection via the lasso.
Journal of the Royal Statistical Society: Series B (Methodological) 58267{
288.
[95]Tu, C.-C. ,Forbes, F. ,Lemasson, B. andWang, N. (2019). Prediction
with high dimensional regression via hierarchically structured Gaussian
mixtures and latent variables. Journal of the Royal Statistical Society:
Series C (Applied Statistics) 681485{1507.
[96]van de Geer, S. (2000). Empirical Processes in M-estimation 6. Cam-
bridge university press.
[97]Van Der Vaart, A. andWellner, J. (1996). Weak Convergence and
Empirical Processes: With Applications to Statistics Springer Series in
Statistics. Springer 5859.
[98]White, H. (1982). Maximum Likelihood Estimation of Misspecied Mod-
els.Econometrica 501{25.
[99]Xu, L. ,Jordan, M. andHinton, G. E. (1995). An Alternative Model
for Mixtures of Experts. In Advances in Neural Information Processing
Systems (G. Tesauro ,D. Touretzky andT. Leen , eds.) 7. MIT Press.
[100] Young, D. S. (2014). Mixtures of regressions with changepoints. Statis-
tics and Computing 24265{281.
[101] Yuksel, S. E. ,Wilson, J. N. andGader, P. D. (2012). Twenty Years
of Mixture of Experts. IEEE Transactions on Neural Networks and Learn-
ing Systems 231177{1193.
