# 2308.14711.pdf
<<<<<<< Updated upstream
# Đã chuyển đổi từ PDF sang TXT
=======
# Chuyển đổi từ PDF sang TXT
>>>>>>> Stashed changes
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2308.14711.pdf
# Kích thước tệp: 905220 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Mạng Feedforward Nhanh
Peter Belcak và Roger Wattenhofer
ETH Zürich
{belcak,wattenhofer}@ethz.ch

Tóm tắt
<<<<<<< Updated upstream
Chúng tôi phá vỡ mối liên kết tuyến tính giữa kích thước lớp và chi phí suy luận của nó bằng cách giới thiệu kiến trúc fast feedforward (FFF), một giải pháp thay thế thời gian logarithmic cho các mạng feedforward. Chúng tôi chứng minh rằng FFF nhanh hơn tới 220 lần so với mạng feedforward, nhanh hơn tới 6 lần so với mạng mixture-of-experts, và thể hiện tính chất huấn luyện tốt hơn mixture-of-experts nhờ vào việc thực thi có điều kiện không nhiễu.

Đẩy FFF tới giới hạn, chúng tôi cho thấy chúng có thể sử dụng ít nhất 1% các neuron lớp cho suy luận trong vision transformer trong khi vẫn bảo toàn 94.2% hiệu suất dự đoán.

Giới thiệu
Lớp feedforward là khối xây dựng có nhiều tham số của các mô hình transformer (Vaswani et al. 2017). Phát triển đến hàng chục nghìn neuron ẩn trong những năm gần đây, chi phí suy luận của lớp feedforward hiện đã trở thành mục tiêu của những người tìm cách làm cho các mô hình lớn nhanh hơn.

Đã được công nhận rằng trong các mạng rất lớn, chỉ một phần nhỏ của các neuron ẩn feedforward đóng vai trò trong việc xác định đầu ra cho bất kỳ đầu vào đơn lẻ nào, và có thể thiết kế các mạng có tính mô-đun để tận dụng thực tế này (Bengio et al. 2015).

Công việc gần đây nhất về mô-đun hóa các lớp feedforward nhằm vào các thiết kế kiến trúc ngầm khuyến khích tính thưa thớt (Shazeer et al. 2017; Lepikhin et al. 2020; Fedus, Zoph, and Shazeer 2022). Chúng chia sẻ phương pháp chung là phân chia lớp feedforward thành các khối neuron riêng biệt – "experts" – và huấn luyện một lớp gating để xác định hỗn hợp của experts sử dụng trong forward pass. Tăng tốc suy luận sau đó được đạt được bằng cách chỉ sử dụng k khối có điểm cao nhất, hoặc một biến thể của nó. Phương pháp này thu nhỏ thời gian suy luận bằng một hằng số nhưng vẫn tuyến tính theo chiều rộng của lớp feedforward. Hơn nữa, nó dựa vào gating có nhiễu để cho phép cân bằng tải giữa các experts, làm phức tạp việc huấn luyện và khuyến khích sự trùng lặp.

Tóm tắt. Chúng tôi giới thiệu kiến trúc Fast Feedforward (FFF) – một đồng cấp của kiến trúc feedforward (FF) truy cập các khối neuron của nó trong thời gian logarithmic. FFF chia không gian đầu vào thành các vùng rời rạc bằng cách sử dụng một cây nhị phân khả vi và thực hiện học tập đồng thời các ranh giới vùng và các khối neuron được gán cho những vùng này. Điều này được đạt được bằng việc thực thi neuron có điều kiện cây: một tập hợp nhỏ các neuron nút được tách riêng để chọn những hỗn hợp nào của các khối neuron lá sẽ được tính toán để tạo ra đầu ra cuối cùng (Hình 1). Khi quá trình huấn luyện tiến triển, các ranh giới vùng trở nên cứng hơn, và các hỗn hợp có xu hướng chọn chỉ một lá có thể truy cập trong thời gian logarithmic.

--- TRANG 2 ---
Chính thức, đặt f: DI → DO là mục tiêu học tập. Phương pháp naïve là huấn luyện một lớp feedforward (FF) F có chiều rộng w để xấp xỉ f trên DI, tức là F ≈DI f.

Phương pháp mixture-of-experts (MoE) (Shazeer et al. 2017) là chọn chiều rộng expert e không cản trở hiệu suất, và sau đó huấn luyện các khối expert riêng biệt E1, ..., E⌈w/e⌉ của neuron được trộn bởi đầu ra được ngẫu nhiên hóa một phần của mạng gating có chiều rộng g = ⌈w/e⌉. Mục tiêu học tập f sau đó được xấp xỉ trên DI dưới hỗn hợp của k experts có điểm cao nhất, tức là m1Eb1 + ... + mkEbk ≈DI f. Đối với mạng FF tương ứng có chiều rộng w, một mạng MoE với g = ⌈w/e⌉ experts sử dụng ke neuron cho suy luận với overhead hỗn hợp là g.

Mạng fast feedforward (FFF) được thiết kế để tận dụng thực tế rằng các vùng khác nhau của không gian đầu vào kích hoạt các tập hợp neuron khác nhau trong các mạng rộng. FFF có độ sâu d học tập cùng lúc một phân hoạch cây R1, ..., R2d của không gian đầu vào được xác định bởi các nút của chúng, và 2d mạng feedforward lá nhỏ L1, ..., L2d có chiều rộng ℓ, được huấn luyện sao cho mỗi Li xấp xỉ hàm mục tiêu một phần f|Ri trên Ri, tức là Li ≈Ri f. Quan trọng là, một FFF có độ sâu d và chiều rộng lá ℓ có thể sử dụng tất cả 2dℓ neuron ẩn để học f, nhưng chỉ cần một lá có chiều rộng ℓ để tính toán đầu ra của nó cho bất kỳ ι ∈ DI, và thực hiện điều này với overhead lookup chỉ d neuron.

Để so sánh, các lá Li đối với FFF giống như các experts Ej đối với MoE, nhưng mạng cây FFF đang vùng hóa không gian đầu vào thay vì bỏ phiếu có nhiễu về khả năng expert. Đối với mạng feedforward tương ứng có chiều rộng w và lựa chọn kích thước lá ℓ, người ta có thể chọn d = log2⌈w/ℓ⌉ và truy cập lá trong O(d) = O(log w) thay vì O(g) = O(w) thời gian. Tình cờ, phương pháp FFF cũng tình cờ đại diện cho một sự nới lỏng khả vi của khái niệm cổ điển về cây k-d (Bentley 1975).

Quá trình cho phép phân mảnh một lớp feedforward lớn thành một số lớp lá nhỏ hơn trong khi bảo toàn hiệu suất dự đoán của nó là hardening. Trong huấn luyện, các nút của FFF thực hiện một cách đệ quy một lựa chọn mềm ⟨1−p, p⟩ trên các đầu ra của hai con của chúng (một "mixture of child experts" nếu chúng ta phải), và dựa trên loss cuối cùng phát sinh, optimizer cập nhật cả trọng số của các con và trọng số của cha mẹ đã tính toán lựa chọn. Trong suy luận, một quyết định cứng duy nhất (tức là "tiến hành đến con trái/phải") được thực hiện tùy thuộc vào kết quả làm tròn [p] tại mỗi nút. Hardening là quá trình các nút học ranh giới sao cho các lựa chọn mềm mà chúng thực hiện cho các đầu vào riêng lẻ chuyển từ hỗn hợp không quyết đoán (ví dụ ⟨49,51⟩) sang những cái quyết đoán hơn (ví dụ ⟨3,97⟩). Chúng tôi quan sát thấy rằng trong các cài đặt mà sức mạnh đại diện là dồi dào (tức là lá rộng và FFF sâu), quá trình thường diễn ra tự nhiên. Trong các cài đặt mà có thể cần nhiều sức mạnh đại diện hơn (ví dụ vision transformer rộng với nút cổ chai lá FFF), quá trình này hoặc diễn ra nhưng dừng lại sớm, hoặc diễn ra với tốc độ rất thấp, và chúng tôi chọn khuyến khích nó bằng cách thêm hardening loss.

Nhờ hardening, hiệu suất của cài đặt huấn luyện mềm được mang sang suy luận. Như một sản phẩm phụ, các vùng đã học cũng có thể được sử dụng như một phân hoạch của không gian đầu vào để giải thích được, chỉnh sửa mô hình phẫu thuật, giảm thiểu quên thảm khốc, giảm ngân sách dữ liệu replay, v.v.

Hướng dẫn sử dụng. Giả sử bạn có một kiến trúc hiện có có lớp FF có chiều rộng w và muốn thay thế nó bằng lớp FFF.

Trường hợp 1: "Tôi muốn suy luận nhanh hơn". Chọn ℓ << w sao cho ℓ phù hợp với ngân sách suy luận mục tiêu của bạn, và sau đó thử nghiệm với các độ sâu khác nhau d ≥ log2(w/ℓ) để đạt được hiệu suất mong muốn. Lưu ý rằng ℓ vẫn cần đủ lớn để có thể học hàm một phần trên vùng của nó, và chiều rộng huấn luyện cuối cùng 2dℓ có thể lớn hơn w.

Trường hợp 2: "Tôi muốn một phân hoạch của không gian đầu vào". Chọn d sao cho 2d đáp ứng kỳ vọng của bạn về số lượng vùng. Sau đó thử nghiệm với ℓ ≥ w2−d để có hiệu suất tốt nhất. Lưu ý rằng ℓ một lần nữa cần đủ lớn để có thể học hàm một phần trên vùng của nó, và đối với d lớn bạn có thể phải chống lại tích cực tác động của overfragmentation.

Đóng góp.
1. Chúng tôi giới thiệu kiến trúc fast feedforward (FFF), một đồng cấp của kiến trúc feedforward (FF) chỉ sử dụng một phần có thể truy cập trong thời gian logarithmic của các neuron của nó tại bất kỳ thời điểm nào.

2. Chúng tôi điều tra tác động của kích thước lá và độ sâu đối với hiệu suất dự đoán của FFF như các mô hình độc lập và cho thấy rằng trong các cài đặt đủ lớn, FFF cho hiệu suất tương đương với FF cùng chiều rộng huấn luyện trong khi thực hiện suy luận nhanh hơn đáng kể. Chúng tôi tiếp tục cho thấy rằng FFF mang lại hiệu suất ghi nhớ và tổng quát hóa tốt hơn FF cùng kích thước suy luận.

3. Chúng tôi so sánh kiến trúc FFF với phương pháp mixture-of-experts (MoE) về hiệu suất dự đoán và tốc độ suy luận khi số lượng khối tăng lên, và hỗ trợ các lợi thế được tuyên bố của thiết kế bằng thực nghiệm.

4. Chúng tôi chứng minh rằng FFF có thể được sử dụng khả thi thay cho FF như các phần của các kiến trúc lớn hơn, phức tạp hơn như transformer.

Văn bản kết thúc bằng một so sánh với công việc liên quan.

Thuật toán
Ký hiệu đầu vào mạng, kích thước đầu ra dimI, dimO.
Ký hiệu ⟨a, b, c⟩-feedforward network một mạng feedforward với a đầu vào, b neuron, và c đầu ra.

Lưu ý rằng chúng tôi ghi đè thuật ngữ được thiết kế cho các mạng đa lớp và chỉ nói về một tập hợp neuron có cả trọng số đầu vào và đầu ra. Ví dụ, chúng tôi sẽ gọi lớp feedforward BERT-base với kích thước đầu vào 768, 3072 neuron ẩn, và 768 neuron đầu ra là lớp feedforward với 3072 neuron, mỗi neuron có 768 đầu vào và 768 đầu ra. Điều này đơn giản hóa đáng kể việc trình bày của chúng tôi.

Định nghĩa. Một mạng fast feedforward có độ sâu d ≥ 0, kích thước nút n, và kích thước lá ℓ là một cặp ⟨N, L⟩.

N := {N0,0, ..., Nd−1,2d−1−1} là tập hợp các mạng feedforward nút ⟨dimI, n, 1⟩ với một activation sigmoidal bổ sung trên đầu ra. Các nút này

--- TRANG 3 ---
Thuật toán 1: FFF forward pass.
Đầu vào: Mẫu đầu vào ι ∈ DI, nút gốc N0,0
Đầu ra: Đầu ra ∈ DO của FFF cho ι

Hàm FORWARD_T(ι, Nm,n):
  nếu Nm,n ∈ L thì
    return Nm,n(ι)
  else
    cm,n ← Nm,n(ι)
    return cm,n FORWARD_T(ι, Nm+1,2n+1) + (1−cm,n) FORWARD_T(ι, Nm+1,2n)
  end

Hàm FORWARD_I(ι, Nm,n):
  nếu Nm,n ∈ L thì
    return Nm,n(ι)
  else
    cm,n ← Nm,n(ι)
    nếu cm,n ≥ 1/2 thì
      return FORWARD_I(ι, Nm+1,2n+1)
    else
      return FORWARD_I(ι, Nm+1,2n)
    end
  end

tạo thành một cây nhị phân khả vi cân bằng sao cho Nm+1,2n, Nm+1,2n+1 là các con của Nm,n.

L := {Nd,0, ..., Nd,2d−1} là tập hợp các mạng feedforward lá ⟨dimI, ℓ, dimO⟩. Tất cả trọng số đều có thể huấn luyện theo mặc định và forward pass được điều chỉnh bởi Thuật toán 1 hoàn toàn xác định.

Các nút của FFF được sắp xếp trong một cây dimI-d khả vi thực hiện một lựa chọn mềm trên các lá dưới dạng vector ngẫu nhiên c. Trong huấn luyện, FFF thực hiện một mixture of experts trên tất cả các lá trong L, với trọng số lựa chọn của hỗn hợp c được tính toán bằng cách đi lên qua cây từ nút gốc N0,0 (xem FORWARD_T). Trong suy luận, quyết định tại mỗi nút được lấy là gần nhất của {0,1}, và thuật toán forward pass tiến hành từ gốc, luôn chọn chỉ một nhánh tùy thuộc vào quyết định nút địa phương (xem FORWARD_I).

Vùng trách nhiệm và ranh giới của chúng. Thành phần cây của FFF tạo ra một phân hoạch của không gian đầu vào. Mỗi lá chịu trách nhiệm cho chính xác một vùng của phân hoạch này, mặc dù trong huấn luyện, dự đoán của nó trên vùng trách nhiệm riêng của nó được trộn với dự đoán của các lá khác. Các ranh giới giữa các vùng riêng lẻ được xác định bởi các mạng nút.

Trong trường hợp khi n = 1 và không có activation trên mạng nút ngoại trừ sigmoid đầu, ranh giới là mặt phẳng kích hoạt của neuron ẩn. Chuẩn của vector pháp tuyến của mặt phẳng (=trọng số của neuron) ảnh hưởng đến tốc độ sigmoid chuyển từ 0 đến 1 quanh ranh giới (xem Hình 1 dưới-trái). Điều này xác định mức độ rõ ràng của ranh giới.

Hardening. Để hiệu suất dự đoán của FORWARD_T mang sang FORWARD_I, người ta không được mất thông tin dự đoán khi làm tròn điểm số lựa chọn. Sự mất thông tin này là tối thiểu khi các quyết định ranh giới đã được hardening đúng cách (xem Giới thiệu). Như đã gợi ý ở trên, hardening tại một nút nói chung không phải liên quan đến điều chỉnh ranh giới như một đa tạp trong không gian – việc tái tỷ lệ đồng nhất tiệm tiến của các hệ số ranh giới (tức là nén sigmoid cuối cùng về hướng hàm step để làm cho ranh giới được xác định rõ ràng hơn) là đủ.

Diễn giải điểm số lựa chọn nút như xác suất Bernoulli, hardening có thể được theo dõi bằng cách giám sát trung bình batch của entropy của điểm số lựa chọn tại mỗi nút. Trong thực nghiệm của chúng tôi, chúng tôi thấy rằng làm tròn các cặp lựa chọn ⟨1−p, p⟩ với entropy dưới 0.10 có xu hướng dẫn đến chỉ những sai lệch rất khiêm tốn so với hiệu suất FORWARD_T. Đối với các tình huống mà hardening của quyết định nút không xảy ra ở mức độ đủ tự nhiên, hardening có thể được khuyến khích bằng cách thêm hardening loss.

Đặt Lpred là loss do đầu ra của FFF. Sau đó người ta có thể lấy tổng loss là Ltotal := Lpred + hLharden với 
Lharden := Σι∈B ΣN∈N H(N(ι)),
trong đó B ⊆ DI là một batch mẫu, H(p) là entropy của biến ngẫu nhiên Bernoulli, và h là siêu tham số huấn luyện kiểm soát tác động của hardening loss.

Overfragmentation. Nếu đẩy đến cực độ, cho phép mạng fast feedforward học quá nhiều ranh giới cứng dẫn đến overfragmentation – một hiện tượng trong đó mạng chia không gian đầu vào thành nhiều vùng rời rạc theo cấp số nhân và học xấp xỉ các phần của mục tiêu học tập theo cách quá cụ thể cho mỗi vùng. Overfragmentation có hai hậu quả trực tiếp: overfitting cục bộ và "shrinking batch problem".

Overfitting cục bộ biểu thị một quá trình tail xảy ra khi mô hình đã hardening đủ, trong đó các ranh giới vùng không còn linh hoạt và các lá nhất định học overfit dữ liệu huấn luyện trên vùng trách nhiệm của chúng. Điều này bởi vì chúng ngừng nhận cập nhật gradient có ý nghĩa lớn từ các vùng lân cận, nhưng có thể chịu trách nhiệm xử lý các mẫu test không được hiểu rõ bởi dữ liệu huấn luyện cho vùng của chúng. Overfitting cục bộ biểu hiện giống như overfitting cổ điển – hiệu suất validation ngừng cải thiện hoặc xấu đi trong khi học tập trên tập huấn luyện tiếp tục. Nó có thể được giảm thiểu bằng transposition con ngẫu nhiên – các quyết định mềm ⟨1−p, p⟩ tại mỗi nút có thể được transpose ngẫu nhiên với xác suất thấp nào đó thành ⟨p, 1−p⟩. Điều này là để expose các con của chúng với dữ liệu huấn luyện của các vùng lân cận để hỗ trợ hiệu suất tổng quát hóa và ở một mức độ nào đó đã xảy ra cho các ranh giới mềm, nhưng nó trở nên hiếm khi các ranh giới hardening.

Biến thể FFF của shrinking batch problem cũng là kết quả của hardening ranh giới vùng lá, và nó phát sinh trong các tình huống khi kích thước batch trở nên quá nhỏ cho phân hoạch của không gian đầu vào được học bởi cây FFF. Nếu phân hoạch của không gian đầu vào quá chi tiết và các ranh giới hardened, mỗi lá cuối cùng nhận cập nhật gradient có ý nghĩa chỉ từ một phần nhỏ của các mẫu huấn luyện, dẫn đến tiến trình gradient descent không chính xác. Shrinking batch dẫn đến hiệu suất học tập kém (ví dụ độ chính xác tập huấn luyện thấp, dừng sớm, phát triển hỗn loạn) nhưng có thể được giảm thiểu – một cách naïve với kích thước batch lớn hơn, tích lũy gradient, và tốc độ học nhỏ hơn; trong tổng quát đầy đủ với tối ưu hóa cục bộ.

Chúng tôi xem xét độ phức tạp của thuật toán trong các tham số d, n, ℓ. Lưu ý rằng chúng tôi thấy n = 1 đủ trong tất cả các thí nghiệm, nhưng chúng tôi giữ n để tổng quát.

Độ phức tạp huấn luyện. Forward pass huấn luyện FORWARD_T đi lên qua d cấp độ của cây, đi qua neuron nút để tính toán vector lựa chọn cuối cùng c trong thời gian O((2d−1)n). Sau đó, đầu ra lá được tính toán và trộn bởi c trong thời gian O(2dℓ). Điều này có nghĩa là thời gian O(2d(ℓ+n)−n) cho forward pass, và một backward pass (d+1)-bước quay lại quyết định trên gốc.

Từ góc độ triển khai, chúng tôi biểu thị việc đi lên qua cây như một vòng lặp đơn thực hiện d tính toán batch giống nhau, và sau đó thực hiện forward pass lá cuối cùng và mixture expert.

Độ phức tạp suy luận. FORWARD_I đi lên qua cây FFF trong d bước, luôn thực thi chính xác một mạng nút. Sau đó, nó thực hiện suy luận trên một lá, dẫn đến thời gian O(dn+ℓ).

Về mặt triển khai, việc đi lên từ gốc qua cây được thực thi như một tính toán batch của một tập hợp trọng số và bias được lập chỉ mục (multiply-and-accumulate), so sánh logit với 0, và tăng chỉ mục tùy thuộc vào kết quả của so sánh.

Trong kinh nghiệm sử dụng ahead-of-time compilation cho CUDA, việc lập chỉ mục chọn lọc trọng số cho quyết định nút biểu hiện trong mã native như một offset đơn giản trong data load cho batched matrix multiplication, chỉ có một implementation overhead hằng số nhỏ trên cấp độ phần cứng khi so sánh với các lớp feedforward.

Kích thước và chiều rộng. Mạng fast feedforward bao gồm neuron hai loại: neuron nút và lá. Để rõ ràng và thực hiện so sánh trực tiếp với các mạng feedforward tương ứng, chúng tôi phân biệt giữa các biến thể của kích thước và chiều rộng mạng.

Một FFF với d, n, ℓ như trong định nghĩa có training size (2d−1)n + 2dℓ – đây là tất cả neuron của mạng, và tất cả đều bị ảnh hưởng bởi tối ưu hóa. Nó tiếp tục có inference size dn + ℓ, vì đây là các neuron tham gia để tạo ra đầu ra suy luận bởi FORWARD_I.

Tuy nhiên, chỉ có neuron của các lá tạo ra đầu ra, với neuron nút chỉ liên quan đến việc tính toán hỗn hợp của đầu ra của các lá riêng lẻ. Do đó, chúng tôi nói rằng FFF có training width 2dℓ và inference width ℓ. Lưu ý rằng FFF với tất cả trọng số của mạng nút được đặt thành 0 tương đương với mạng feedforward vanilla có 2dℓ neuron (tới một tái tỷ lệ đồng nhất của trọng số đầu ra, có thể học được). Chúng tôi gọi sự khác biệt giữa training/inference size và width là overhead.

Thí nghiệm
Chúng tôi tiến hành một số thí nghiệm để (1) khám phá tác động của việc gán neuron với vùng ảnh hưởng có thể học, (2) so sánh hiệu suất dự đoán và tốc độ của FFF với MoE, và (3) đánh giá tính khả thi của FFF như các phần của kiến trúc sâu hơn. Nhiệm vụ của mỗi thí nghiệm là phân loại hình ảnh, và chúng tôi đánh giá độ chính xác phân loại của softmax của logit đầu ra theo cách thông thường. Đối với FFF, chúng tôi đo độ chính xác của việc đưa ra quyết định "cứng" tại mỗi nút (tức là chúng tôi sử dụng FORWARD_I).

Đối với mỗi dataset được xem xét, chúng tôi sử dụng các tập huấn luyện và test được chỉ định như được cung cấp. Chúng tôi tiếp tục chia tập huấn luyện đầy đủ 9:1 thành tập con huấn luyện và validation.

Để so sánh chất lượng của các mô hình riêng lẻ, chúng tôi đo bốn đại lượng.

Memorization accuracy (MA). Diễn giải mạng feedforward (nhanh) như bộ nhớ mô hình (Bau et al. 2020), chúng tôi đo khả năng học tập huấn luyện của chúng bằng cách tính toán độ chính xác của mạng overfitted trên tập huấn luyện. Nghĩa là, chúng tôi huấn luyện mạng cho đến khi độ chính xác của chúng trong huấn luyện ngừng cải thiện, và sau đó chạy một test trên dữ liệu huấn luyện. MA kết quả 100% có nghĩa là mạng đã ghi nhớ thành công tất cả các dự đoán cho dữ liệu huấn luyện.

Generalization accuracy (GA). Xem mạng feedforward (nhanh) như mô hình dự đoán độc lập, chúng tôi đo khả năng dự đoán chính xác các lớp của mẫu chưa thấy trước đây trong tập test. Đối với điều này, chúng tôi huấn luyện mạng cho đến khi độ chính xác validation của chúng ngừng cải thiện, và sử dụng mô hình tốt nhất về độ chính xác validation để đánh giá.

Thời gian suy luận và speedup. Triển khai riêng của chúng tôi về thuật toán FFF được cung cấp qua pip install fastfeedforward và trên GitHub. Chúng tôi biên dịch triển khai của FORWARD_I cho GPU NVIDIA A100 sử dụng biên dịch mô hình PyTorch 2.0.1 trong chế độ reduce-overhead. Sau đó chúng tôi chạy mỗi mô hình 10^4 lần với batch size 2048 trên một GPU NVIDIA A100 đơn. Khi có liên quan để so sánh, chúng tôi báo cáo thời gian suy luận trung bình t• mỗi forward pass đơn dưới các thử nghiệm lặp lại, cùng với độ lệch chuẩn của nó. Chúng tôi tiếp tục báo cáo speedup – phần tFF/tFFF, trong đó tFFF là thời gian suy luận trung bình cho mô hình FFF đã cho và tFF là thời gian suy luận trung bình cho mạng feedforward vanilla cùng training width. Đơn giản nói, speedup nói FFF nhanh hơn bao nhiêu so với FF có cùng số neuron có sẵn để đưa ra dự đoán trong huấn luyện, đo bằng lựa chọn phần mềm và phần cứng này. Trung bình và độ lệch cho speedup ở phụ lục.

Đánh giá khám phá
Để kiểm tra bản chất của mạng fast feedforward như một giải pháp thay thế cho mạng feedforward, chúng tôi đo tác động của các tham số của chúng đối với hiệu suất dự đoán và tốc độ trong bối cảnh.

Đánh giá với đối tác huấn luyện
Chủ đề. Chúng tôi điều tra mối quan hệ giữa cấu hình kích thước lá, độ sâu, và chiều rộng huấn luyện và hiệu suất ghi nhớ

--- TRANG 5 ---
[Bảng 1 tiếp tục với kết quả thí nghiệm chi tiết cho USPS, MNIST, và FashionMNIST]

và tổng quát hóa của mạng fast feedforward. Đối với mỗi FFF, chúng tôi cũng xem xét hiệu suất của một FF cùng training width. Chúng tôi so sánh với chúng có cùng training width thay vì training size vì chỉ có neuron lá trực tiếp liên quan đến việc tính toán dự đoán phân loại cho các đầu vào đã cho.

Phương pháp. Chúng tôi huấn luyện mạng fast feedforward cho training width w = 16, 32, 64, 128, kích thước lá ℓ = 1, 2, 4, 8, dataset USPS (Hull 1994), MNIST (LeCun, Cortes, and Burges 2010), và FashionMNIST (Xiao, Rasul, and Vollgraf 2017). Đối với mỗi cấu hình w, ℓ chúng tôi tính toán độ sâu là log2(w/ℓ). Tập hợp chiều rộng đã được chọn có mục đích: lưu ý rằng bất kỳ mạng fast feedforward nào với w, ℓ như trên đều có inference width nhỏ hơn 16 – cấu hình hẹp nhất của chúng tôi. Đối với mỗi chiều rộng, chúng tôi tiếp tục huấn luyện một mạng feedforward vanilla như baseline.

Chúng tôi đưa vào mạng các hình ảnh được làm phẳng. Để dễ so sánh, chúng tôi sử dụng batch size 256 và tối ưu hóa SGD thuần túy với tốc độ học 0.2 bất kể kích thước hay độ sâu của mạng, nhưng chúng tôi lưu ý rằng FFF sâu hơn đã được hưởng lợi từ batch size lớn hơn và tốc độ học nhỏ hơn. Chúng tôi tham gia hardening loss với tham số scale h = 3.0. Chúng tôi thực hiện 10 lần chạy cho mỗi cấu hình, và vì đây là đánh giá giới hạn kiến trúc, chúng tôi báo cáo hiệu suất của mô hình tốt nhất. Trung bình và độ lệch ở phụ lục.

Thảo luận. Kết quả của chúng tôi được liệt kê trong Bảng 1. Một hình ảnh hóa của quá trình hardening có thể được tìm thấy trong Hình 5 của phụ lục. Các quan sát chung là mỗi trong số: tăng chiều rộng, tăng kích thước lá, và tăng kích thước lá trong khi giữ độ sâu không đổi; phổ biến giúp hiệu suất ghi nhớ và tổng quát hóa. Chúng tôi đưa ra một số quan sát cụ thể liên quan đến đóng góp của chúng tôi.

FFF thực hiện tương đương với FF. Đối với chiều rộng và độ sâu đủ lớn trên USPS và MNIST, mạng fast feedforward chỉ kém hơn một chút (2-3%) so với mạng feedforward vanilla. Tình cờ, đây cũng là những cấu hình trong đó FFF mang lại cải thiện tốc độ suy luận tốt nhất so với các lớp feedforward cổ điển khi đo trên phần cứng của chúng tôi.

Lưu ý hiệu suất của FFF với w = 128, ℓ = 8 qua các dataset so với FF với w = 16. Hiệu suất gần đáng kể và thậm chí vượt qua FF, tất cả trong khi inference size của những FFF này (12) vẫn dưới FF (16).

Trên FashionMNIST chúng tôi quan sát cùng xu hướng nhưng lưu ý rằng một FFF ngoài phạm vi thử nghiệm của chúng tôi (w = 512, ℓ = 8) cuối cùng cần thiết để đưa FFF gần (MA=97.1, GA=88.1) với hiệu suất của FF.

Speedup tăng với chiều rộng. Các mạng rộng hơn và sâu hơn trong khi giữ kích thước lá không đổi, cải thiện tốc độ suy luận do kiến trúc fast feedforward mang lại càng đáng kể.

Chiều rộng không đổi dẫn đến trade-off tốc độ-hiệu suất. Nếu training width được giữ cố định, rõ ràng có trade-off giữa tăng độ sâu (và do đó tăng tốc độ) và hiệu suất (xem Bảng 1 từ trên xuống dưới).

Tăng độ sâu của mạng trong khi giữ training width không đổi dẫn đến giảm dần hiệu suất do FFF có lá nhỏ hơn, do đó ghi chú của chúng tôi trong "Hướng dẫn sử dụng" (xem Giới thiệu). Sau này chúng tôi sẽ thấy rằng trong kiến trúc sâu hơn, trade-off trong mạng chiều rộng không đổi giữa tốc độ suy luận và giảm hiệu suất do kích thước lá nhỏ dường như được giảm thiểu trong kiến trúc đa lớp.

Mạng độ sâu lớn lá nhỏ thể hiện overfragmentation. Chúng tôi quan sát (USPS, FashionMNIST) rằng giảm kích thước lá trong khi tăng độ sâu (giữ chiều rộng không đổi) dẫn đến hiệu suất ghi nhớ và tổng quát hóa xấu đi nhanh chóng. Sự khác biệt đặc biệt rõ ràng với độ sâu lớn hơn (ℓ=1,2, w=64, 128). Điều này được giải thích tốt bởi overfragmentation, vì đặc biệt đối với ℓ = 1, w = 128, mỗi lá nhận chỉ 2 mẫu mỗi batch trung bình. Kết quả trên USPS cho ℓ=1, w=16,32 là một ví dụ mô hình của điều này: chúng tôi thấy rằng FFF với ℓ=1, d=4 mang lại MA 83.4, nhưng người anh em sâu hơn ℓ=1, d=5 – có nhiều lá cùng kích thước hơn tại sự phân bố của nó – chỉ cho MA 77.3.

Tóm tắt. FFF cho hiệu suất dự đoán tương đương với FF cùng training width, nhanh hơn khi training width tăng, và nếu đẩy đến giới hạn thể hiện trade-off tốc độ-hiệu suất và overfragmentation.

Đánh giá với đối tác suy luận
Chủ đề. Tương tự thí nghiệm trên với xem xét chính cho training width, chúng tôi bây giờ đánh giá mạng fast feedforward với độ sâu và kích thước lá khác nhau đối với inference size của chúng. Chúng tôi làm inference size thành điểm so sánh với mạng feedforward vì, không giống inference width, nó trực tiếp tỷ lệ với chi phí tính toán suy luận.

Phương pháp. Chúng tôi huấn luyện mạng fast feedforward cho ℓ = 2, 4, 6, 8, 16, 32, d = 2, 6, dataset SVHN (Netzer et al. 2011), CIFAR10, CIFAR100 (Krizhevsky, Hinton et al. 2009), và cho mỗi cấu hình ℓ, d chúng tôi tính toán inference size là ℓ + d. Đối với mỗi inference size, chúng tôi tiếp tục huấn luyện một mạng feedforward vanilla như baseline.

Chúng tôi huấn luyện mạng với tất cả tham số như trên trừ h, nơi chúng tôi không tham gia hardening loss (h = 0) vì chúng tôi thấy rằng hardening có xu hướng xảy ra tự nhiên. Chúng tôi thực hiện 10 lần chạy cho mỗi cấu hình và báo cáo hiệu suất tốt nhất.

Thảo luận. Kết quả của chúng tôi thưa thớt vì tất cả các tổng khác nhau có thể có của kích thước lá và độ sâu và được hiển thị trong Hình 2. Phù hợp với trực giác, chúng tôi quan sát rằng độ sâu lớn hơn và kích thước lá lớn hơn dẫn đến hiệu suất ghi nhớ và tổng quát hóa tốt hơn.

Hơn nữa, FFF vượt trội FF cùng inference size. FFF với độ sâu và kích thước khác nhau nhất quán vượt trội FF với chiều rộng bằng FFF inference size, cả về MA và GA. Về MA, sự khác biệt rõ ràng và tăng với độ sâu và kích thước lá. Về GA, FFF ban đầu có lợi thế so với FF, nhưng sau đó hiệu suất của tất cả mô hình hợp nhất hướng về plateau tại giới hạn của phương pháp feedforward-only đơn lớp naïve.

Tóm tắt. FFF mang lại hiệu suất sẵn sàng hơn FF cùng inference width.

--- TRANG 7 ---
[Bảng 2 với kết quả so sánh feedforward, mixture-of-experts, và fast feedforward]

Đánh giá so sánh
Kiến trúc đối thủ trực tiếp của fast feedforward, đi kèm với tập hợp tham số thiết kế riêng, là lớp mixture-of-expert, mà chúng tôi lấy dưới dạng gốc (Shazeer et al. 2017).

So sánh hiệu suất dự đoán
Chủ đề. Chúng tôi so sánh FFF với MoE và FF với training width khác nhau về hiệu suất dự đoán. Chúng tôi giữ chiều rộng lá và expert không đổi và tập trung vào khả năng của kiến trúc mang lại tính chất ghi nhớ và tổng quát hóa tốt cũng như compute huấn luyện cần thiết để đạt được những tính chất đó.

Phương pháp. Chúng tôi thí nghiệm trên dataset CIFAR10 không được tăng cường. Chúng tôi huấn luyện mạng feedforward, mixture-of-experts, và fast feedforward với kích thước tăng sao cho chúng luôn đồng ý về training width. Để giữ inference width giống nhau, chúng tôi đặt leaf width thành 32 và expert width thành 16 với luôn tham gia k = 2 experts. Lưu ý rằng trong khi mạng single-expert có thể được sử dụng cho suy luận, chúng không thể truyền gradient đến mạng gating (xem Shazeer et al. (2017)). Chúng tôi lấy chiều rộng w = 2^6, 2^7, ..., 2^10, tương ứng với mạng MoE 16- đến 64-expert và FFF độ sâu 1 đến 5. Để khuyến khích equal importance và load balancing trong MoE, chúng tôi đặt w_importance = w_load = 0.1 phù hợp với công việc trước đó. Để khuyến khích FFF hardening, chúng tôi sử dụng h = 3.0. Chúng tôi huấn luyện tất cả mô hình với batch size 4096 nhiều nhất 7000 epoch, với early stopping sau 350 epoch nơi không có cải thiện trong độ chính xác validation tương ứng được thấy. Tất cả mô hình đã được huấn luyện với optimizer Adam, tốc độ học 0.001, với tốc độ học giảm một nửa trên plateau độ chính xác huấn luyện 250-epoch.

Thảo luận. Kết quả được liệt kê trong Bảng 2. Ngay từ đầu, chúng tôi quan sát rằng MA và GA được hưởng lợi từ training width lớn hơn qua tất cả mô hình trừ GA trên FFF, nơi chúng tôi thấy localized overfitting không được giảm thiểu ảnh hưởng tiêu cực đến hiệu suất.

FFF nhanh nhất mang lại MA và GA. Chúng tôi thấy rằng FFF nhanh nhất (về ETT) mang lại cả MA và GA, nhưng, nhất quán với đánh giá khám phá trước đó của chúng tôi, mang lại MA thấp hơn một chút so với FF cùng training width, và chịu đựng localized overfitting với độ sâu tăng.

FFF vượt trội MoE cùng training width. Chúng tôi quan sát rằng FFF nhất quán mang lại điểm MA và GA tốt hơn các mạng MoE cùng training width. Chúng tôi tiếp tục thấy rằng chúng làm điều đó tại ETT nhỏ hơn một bậc độ lớn. Chúng tôi quy sự khác biệt này chủ yếu cho noise được kiểm soát có thể học được đưa vào tính toán mixture expert để hỗ trợ load balancing và tổng quát hóa. Tuy nhiên, không có noise, mạng MoE sẽ overfit để học chỉ với một số expert. Chúng tôi cũng thí nghiệm với các giá trị khác nhau của w_importance và w_load, nhưng chúng tôi thấy những cái đó về cơ bản có hại cho nỗ lực load balancing. Giá trị cuối cùng của chúng tôi về batch importance và load phù hợp với những cái đạt được trong Shazeer et al. (2017).

Tóm tắt. FFF mang lại sức mạnh đại diện sẵn sàng hơn MoE cùng training width.

So sánh tốc độ suy luận
Chủ đề. Vì các hoạt động liên quan đến tính toán đầu ra mạng expert/lá giống nhau, sự khác biệt về tốc độ suy luận giữa mixture-of-experts và mạng fast feedforward đến hoàn toàn từ hoạt động của cơ chế gating/lookup. Do đó chúng tôi giữ chiều rộng expert/lá không đổi và đo thời gian cần thiết để thực hiện forward pass suy luận của mạng feedforward, mixture-of-experts, và fast feedforward qua các thử nghiệm lặp lại cho số lượng expert/lá tăng (tức là mạng rộng hơn và rộng hơn).

Để thêm tính thực tế, chúng tôi mô phỏng điều kiện của lớp feedforward BERT-base (Devlin et al. 2018), đặt chiều rộng đầu vào và đầu ra của tất cả neuron thành 768.

Phương pháp. Chúng tôi xem xét mô hình FF có chiều rộng 32×2^1 đến 32×2^5, nơi chúng tôi nổi bật 32 khối neuron để so sánh trực tiếp với các mô hình khác. Chúng tôi tiếp tục đánh giá mô hình MoE với expert width e = 32 và 2^1 đến 2^15 expert, và mô hình FFF với leaf width e = 32 và độ sâu 1 đến 15. Để loại bỏ tác động của tính toán mixture trên đo lường của chúng tôi, chúng tôi giữ e = ℓ và đặt k = 1, mặc dù cấu hình tham số MoE này không thể huấn luyện (xem trên). Khi đo, mỗi mô hình thực hiện suy luận trên đầu vào BERT-base với batch size 256 chính xác 20000 lần.

Thảo luận. Kết quả đo tốc độ suy luận được hình ảnh hóa trong Hình 3–4. Chúng tôi quan sát rằng cả MoE và FFF đều cung cấp tăng tốc đáng kể cho tốc độ suy luận khi so sánh với baseline FF. Tuy nhiên, Hình 4 cho thấy xu hướng rõ ràng của thời gian suy luận mô hình MoE tăng theo cấp số nhân với số mũ của expert count, tương phản rõ ràng với mối quan hệ tuyến tính giữa hai cái được thể hiện bởi mô hình FFF. Điều này hoàn toàn phù hợp với phân tích lý thuyết của chúng tôi về độ phức tạp thời gian suy luận của hai kiến trúc.

Tóm tắt. Chúng tôi đã xác nhận thực nghiệm sự khác biệt theo cấp số nhân giữa độ phức tạp thời gian suy luận của cơ chế nội bộ MoE và FFF.

Lớp fast feedforward như khối xây dựng
Chủ đề. Chúng tôi chứng minh rằng mạng fast feedforward có thể được sử dụng như lớp thay cho lớp feedforward tiêu chuẩn trong kiến trúc transformer, do đó mang lại cho nó một tăng hiệu suất đáng kể. Trước đó, chúng tôi lưu ý rằng kích thước lá quá nhỏ cho một vấn đề nhất định có thể dẫn đến sự xuất hiện của overfragmentation. Ở đây chúng tôi đẩy cài đặt thí nghiệm đến giới hạn về kích thước lá và điều tra tác động của overfragmentation trong transformer sâu.

Phương pháp. Chúng tôi thí nghiệm trên dataset CIFAR10 gồm hình ảnh 32x32-pixel 3-channel, với tăng cường lật ngang, dọc ngẫu nhiên, và tăng cường tuyến tính ngẫu nhiên (translate, rotate, scale). Như mô hình, chúng tôi sử dụng vision transformer 4-lớp với patch size 4, hidden dimension 128, input dropout 0.1, và không layer dropout.

Chúng tôi xem xét vision transformer với lớp feedforward của chúng được thay thế bằng lớp fast feedforward có training width 128, và baseline vision transformer với lớp feedforward có chiều rộng w = 128. Lớp fast feedforward có leaf size ℓ = 1, 2, 4, 8, 16, 32 và độ sâu log2(w/ℓ). Chúng tôi thử ba mức độ hardening: h = 5, 10, ∞, trong đó ∞ biểu thị rằng cây FFF đã được đóng băng hiệu quả từ đầu (tức là các ranh giới không thể huấn luyện). Chúng tôi sử dụng optimizer Adam với tốc độ học ban đầu 4e−4 và tốc độ học giảm một nửa trên plateau độ chính xác validation 50-epoch. Đối với mỗi cấu hình ℓ, d, chúng tôi báo cáo hiệu suất tổng quát hóa của mô hình tốt nhất và speedup đo được tại các lớp feedforward (không phải toàn bộ transformer).

Thảo luận. Kết quả thí nghiệm của chúng tôi có thể thấy trong Bảng 3. Một hình ảnh hóa của quá trình hardening qua các lớp của transformer có thể tìm thấy trong Hình 6 của phụ lục. Phù hợp với đánh giá của chúng tôi về độ phức tạp thuật toán, speedup đo được tại các lớp feedforward tăng với kích thước lá giảm. Hơn nữa:

FFF Single-neuron đủ. Chúng tôi quan sát rằng thậm chí lớp fast feedforward với inference width 1 đủ cho vision transformer mang lại hiệu suất hợp lý, với giảm tương đối trong hiệu suất chỉ 5.8%.

Tác động của overfragmentation bị dìm. Chúng tôi quan sát rằng hiệu suất tổng quát hóa GA chịu đựng tương đối nhẹ do tăng độ sâu và giảm kích thước lá, tương phản rõ ràng với kết quả của Bảng 1. Chúng tôi quy điều này cho độ sâu của transformer và coi nó như một dấu hiệu khuyến khích về tính khả thi của FFF như thay thế cho FF.

Tóm tắt. Lớp fast feedforward có thể mang lại tăng tốc suy luận trong vision transformer so với lớp feedforward cùng training width với chi phí giảm hiệu suất nhỏ.

Công việc liên quan
Công việc của chúng tôi giao với nỗ lực nghiên cứu trong hai lĩnh vực tăng tốc suy luận.

--- TRANG 9 ---
[Bảng 3 với kết quả thử nghiệm vision transformer]

Thực thi có điều kiện. Mặc dù ngày nay phần lớn không hoạt động do xu hướng rời khỏi kiến trúc tùy chỉnh, các thiết kế được sửa đổi cho mô hình MLP và CNN đã được đề xuất để cho phép thực thi một phần của chúng khi có thể.

Một số phương pháp đã được đề xuất (Davis and Arel 2013; Bengio, Léonard, and Courville 2013; Bengio et al. 2015; Almahairi et al. 2016) để học hoặc phân phối policy hoặc thành phần neuron kiểm soát bổ sung để quyết định khối neuron lớp nào thực thi trong forward pass.

So sánh, mạng fast feedforward hoàn toàn che giấu việc học vùng lá khỏi người dùng (trừ siêu tham số h nếu được sử dụng) và đi kèm dưới dạng sẵn sàng suy luận một khi được huấn luyện, không cần điều chỉnh khi được bao gồm như một phần của transformer.

Trong một tổng quát đáng chú ý của dòng công việc này với kiến trúc sâu, Ioannou et al. (2016) đề xuất một phương pháp ngang hàng với mạng neuron tích chập sâu học định tuyến đầu vào qua một chuỗi lớp trung gian được chọn.

Trong khi phương pháp của chúng tôi nhanh chóng định tuyến tín hiệu đến một mạng neuron feedforward lá đơn, nó không so sánh với mạng sâu.

Mô hình "mixture-of-experts" mô-đun. Các mô hình rất lớn thực tế đòi hỏi tính mô-đun. Cách đơn giản nhất để mô-đun hóa mô hình transformer lớn để giảm chi phí suy luận của chúng là phân chia lớp feedforward của chúng thành n khối neuron, và sau đó huấn luyện một classifier kiểm soát để chọn khối nào tham gia vào forward pass.

Điều này thường được thực hiện bằng cách huấn luyện một lớp tuyến tính kích hoạt softmax rộng để tạo ra vector ngẫu nhiên của điểm hỗn hợp được áp dụng cho đầu ra mỗi khối để tạo ra đầu ra cuối cùng. Một số biến thể của phương pháp này đã được đề xuất và thử nghiệm qua nhiều mô hình lớn (Shazeer et al. 2017; Lepikhin et al. 2020; Fedus, Zoph, and Shazeer 2022).

Chúng tôi so sánh kỹ lưỡng fast feedforward với mạng mixture-of-expert trong các phần trước. Để tóm tắt ngắn gọn, phương pháp mixture-of-experts giảm layer inference width bằng hệ số n/k, trong đó k là số khối có điểm cao nhất để tham gia vào suy luận, nhưng yêu cầu thời gian O(n) để chọn k khối, và thường dựa vào ngẫu nhiên hóa có kiểm soát để tránh hình thành sở thích mạnh chỉ cho một số expert. Điều này vẫn đúng ngay cả khi nhiều lớp của expert mixer được giới thiệu. So sánh trực tiếp, một mạng fast feedforward có độ sâu d = log2 n giảm suy luận bằng hệ số n và chỉ yêu cầu thời gian O(d) = O(log n) để quyết định lá nào sử dụng. Thừa nhận, để bù cho tác động của việc chỉ có một lá để đưa ra quyết định, các lá của lớp fast feedforward có thể phải rộng hơn một chút so với các khối của lớp mixture-of-experts tương ứng.

Regionalization. Một lợi thế bổ sung của FFF so với tất cả công việc liên quan là có tương ứng trực tiếp giữa các phần của mạng được sử dụng trong suy luận và các vùng có thể nhận dạng đại số của không gian đầu vào. Điều này có thể được tận dụng để giảm thiểu quên thảm khốc khi chỉnh sửa mô hình và giảm đáng kể ngân sách dữ liệu replay bằng cách áp dụng phân hoạch đã học của không gian đầu vào để phân hoạch dữ liệu huấn luyện.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ]

--- TRANG 11 ---
Kết quả mở rộng của Bảng 1
Bảng 4 liệt kê trung bình và độ lệch chuẩn của đánh giá khám phá của chúng tôi về mạng fast feedforward so với mạng feedforward.

Ngoài các quan sát được thực hiện trong văn bản chính, chúng tôi thấy rằng phương sai của MA và GA khi thực hiện các lần chạy lặp lại tăng với kích thước lá giảm và rõ ràng nhất được thấy cho training width nhỏ.

Hình ảnh hóa quá trình hardening
Để cung cấp một số insight và trực giác về hardening của ranh giới quyết định nút, chúng tôi theo dõi trung bình entropy batch qua các nút của lớp fast feedforward và báo cáo về sự tiến triển của chúng khi huấn luyện tiến triển bằng hình ảnh.

Hình 5 cho thấy sự tiến triển của trung bình entropy batch cho ba mô hình có độ sâu khác nhau có huấn luyện được chấm dứt bởi early stopping trên plateau độ chính xác validation FORWARD_I. Chúng tôi quan sát rằng bằng cách giữ dataset (và do đó độ phức tạp của không gian đầu vào) và kích thước lá không đổi, độ chính xác trung bình batch của lớp sâu hội tụ nhanh hơn cho mạng sâu hơn. Chúng tôi quy điều này đơn giản cho thực tế rằng mạng fast feedforward càng sâu trong thí nghiệm này, càng có nhiều lá có sẵn để chọn lọc phục vụ cho không gian con của không gian đầu vào, cho phép mạng tạo ra sự phân tách sạch hơn giữa các vùng đã học.

Hình 6 báo cáo sự tiến triển của trung bình entropy batch mỗi lớp cho mô hình visual transformer 4-lớp với mạng fast feedforward thay cho mạng feedforward vanilla. Chúng tôi lưu ý rằng sớm trong huấn luyện, entropy của các lớp gần đầu vào hơn (tức là các lớp sớm hơn/thấp hơn) nhanh hơn để hội tụ. Tuy nhiên, chúng tôi thấy rằng entropy của lớp thứ hai sau đó ngừng giảm có ý nghĩa, và entropy của lớp đầu tiên thực sự bắt đầu tăng. Điều này có thể được giải thích bởi ranh giới quyết định hardened cho thấy dấu hiệu hành vi nút cổ chai trong lớp cao hơn của transformer, do đó đòi hỏi các lớp thấp hơn đóng góp với biểu diễn đã học chất lượng cao hơn.

Diễn giải việc dừng hoặc tăng của entropy như một dấu hiệu của nhu cầu về sức mạnh đại diện nhiều hơn, chúng tôi lưu ý qua nhiều thí nghiệm rằng hình dạng lý tưởng cho vision transformer, thậm chí trên dataset tương đối đơn giản như CIFAR10, dường như nhiều hơn hướng về hình dạng kim tự tháp hơn là dòng lớp chiều rộng đồng nhất.

Mối quan hệ với mạng MoE multi-gating
Tài liệu gần đây hơn về mạng mixture-of-experts (Lepikhin et al. 2020; Fedus, Zoph, and Shazeer 2022) đề xuất sử dụng cấu trúc gating phân cấp hai lớp. Trừ tính ngẫu nhiên cố hữu của việc chọn lọc mạng gating, điều này có thể được xem như một bước nhỏ hướng đến FFF. Tuy nhiên, FFF chú ý đến các vùng được chỉ định rõ ràng của không gian thay vì được bỏ phiếu có nhiễu như expert phù hợp nhất cho đầu vào nhất định (mà chúng tôi đã chỉ ra mang lại cho chúng tính chất huấn luyện và suy luận tốt hơn nhiều), và hơn nữa, thậm chí dưới phương pháp như vậy, giảm thời gian suy luận của mạng mixture-of-expert vẫn không đổi trong chiều rộng của mạng mục tiêu.

[Hình 5 và 6 cho thấy sự tiến triển entropy]

Code
Chúng tôi cung cấp triển khai của chúng tôi như một gói Python được xây dựng trên framework PyTorch.
=======
Chúng tôi phá vỡ liên kết tuyến tính giữa kích thước lớp và chi phí suy luận bằng cách giới thiệu kiến trúc feedforward nhanh (FFF), một giải pháp thay thế thời gian logarit cho các mạng feedforward.
Chúng tôi chứng minh rằng FFF nhanh hơn tới 220 lần so với mạng feedforward, nhanh hơn tới 6 lần so với mạng hỗn hợp chuyên gia, và thể hiện tính chất huấn luyện tốt hơn so với hỗn hợp chuyên gia nhờ thực thi có điều kiện không nhiễu.
Đẩy FFF đến giới hạn, chúng tôi cho thấy chúng có thể sử dụng ít nhất 1% neuron lớp cho suy luận trong các transformer thị giác trong khi vẫn giữ được 94.2% hiệu suất dự đoán.

Giới thiệu
Lớp feedforward là một khối xây dựng nặng tham số của các mô hình transformer (Vaswani et al. 2017). Phát triển lên hàng chục nghìn neuron ẩn trong những năm gần đây, chi phí suy luận lớp feedforward hiện đã nằm trong tầm ngắm của những người muốn làm cho các mô hình lớn chạy nhanh hơn.

Người ta đã nhận ra rằng trong các mạng rất lớn, chỉ một phần nhỏ neuron ẩn feedforward đóng vai trò trong việc xác định đầu ra cho bất kỳ đầu vào đơn lẻ nào, và có thể thiết kế các mạng có tính modular để tận dụng thực tế này (Bengio et al. 2015).

Công trình gần đây nhất về việc modular hóa các lớp feedforward nhằm vào các thiết kế kiến trúc ngầm khuyến khích tính thưa thớt (Shazeer et al. 2017; Lepikhin et al. 2020; Fedus, Zoph, and Shazeer 2022). Chúng chia sẻ phương pháp chung là chia nhỏ lớp feedforward thành các khối neuron riêng biệt - "chuyên gia" - và huấn luyện một lớp gating để xác định hỗn hợp chuyên gia được sử dụng trong forward pass. Tăng tốc suy luận sau đó được đạt được bằng cách chỉ sử dụng k khối điểm cao nhất, hoặc một biến thể của nó. Phương pháp này giảm thời gian suy luận theo một hằng số nhưng vẫn tuyến tính theo chiều rộng của lớp feedforward. Hơn nữa, nó dựa vào gating nhiễu để cho phép cân bằng tải giữa các chuyên gia, làm phức tạp việc huấn luyện và khuyến khích sự trùng lặp.

Phác thảo. Chúng tôi giới thiệu kiến trúc Fast Feedforward (FFF) - một đồng cấp của kiến trúc feedforward (FF) truy cập các khối neuron của nó trong thời gian logarit. FFF chia không gian đầu vào thành các vùng rời rạc bằng cách sử dụng một cây nhị phân khả vi và thực hiện việc học đồng thời các ranh giới vùng và các khối neuron được gán cho các vùng này. Điều này được đạt được bằng thực thi có điều kiện theo cây của neuron: một tập hợp nhỏ neuron nút được tách ra để chọn những hỗn hợp nào của các khối neuron lá sẽ được tính toán để tạo ra đầu ra cuối cùng (Hình 1). Khi quá trình huấn luyện tiến triển, các ranh giới vùng cứng lại, và các hỗn hợp có xu hướng chỉ chọn một lá có thể truy cập trong thời gian logarit.

--- TRANG 2 ---
Chính thức, cho f: DI → DO là mục tiêu học.
Phương pháp ngây thơ là huấn luyện một lớp feedforward (FF) F với chiều rộng w để xấp xỉ f trên DI, tức là F ≈DI f.
Phương pháp hỗn hợp chuyên gia (MoE) (Shazeer et al. 2017) là chọn chiều rộng chuyên gia e không cản trở hiệu suất, sau đó huấn luyện các khối chuyên gia riêng biệt E1, . . . , E⌈w/e⌉ neuron được trộn bởi đầu ra được ngẫu nhiên hóa một phần của mạng gating có chiều rộng g = ⌈w/e⌉. Mục tiêu học f sau đó được xấp xỉ trên DI dưới hỗn hợp của k chuyên gia điểm cao nhất, tức là m1Eb1 + ... + mkEbk ≈DI f. Đối với mạng FF tương ứng có chiều rộng w, mạng MoE với g = ⌈w/e⌉ chuyên gia sử dụng ke neuron cho suy luận với overhead hỗn hợp là g.

Mạng feedforward nhanh (FFF) được thiết kế để tận dụng thực tế rằng các vùng khác nhau của không gian đầu vào kích hoạt các tập hợp neuron khác nhau trong các mạng rộng. FFF có độ sâu d học chung một phân vùng cây R1, . . . , R2d của không gian đầu vào được xác định bởi các nút của chúng, và 2d mạng feedforward lá nhỏ L1, . . . , L2d có chiều rộng ℓ, được huấn luyện sao cho mỗi Li xấp xỉ hàm mục tiêu một phần f|Ri trên Ri, tức là Li ≈Ri f. Quan trọng là, một FFF có độ sâu d và chiều rộng lá ℓ có thể sử dụng tất cả 2dℓ neuron ẩn để học f, nhưng chỉ cần một lá có chiều rộng ℓ để tính đầu ra của nó cho bất kỳ ι ∈ DI, và làm vậy với overhead lookup chỉ d neuron.

Để so sánh, các lá Li với FFF giống như các chuyên gia Ej với MoE, nhưng mạng cây FFF đang regionalize không gian đầu vào thay vì bỏ phiếu nhiễu về năng lực chuyên gia. Đối với mạng feedforward tương ứng có chiều rộng w và lựa chọn kích thước lá ℓ, người ta có thể chọn d = log2⌈w/ℓ⌉ và truy cập lá trong O(d) = O(log w) thay vì O(g) = O(w) thời gian. Ngẫu nhiên, phương pháp FFF cũng tình cờ đại diện cho một sự nới lỏng khả vi của khái niệm cổ điển về cây k-d (Bentley 1975).

Quá trình cho phép phân mảnh một lớp feedforward lớn thành một số lớp lá nhỏ hơn trong khi vẫn giữ được hiệu suất dự đoán của nó là hardening. Trong huấn luyện, các nút của FFF thực hiện đệ quy một lựa chọn mềm ⟨1−p, p⟩ trên đầu ra của hai con chúng (một "hỗn hợp chuyên gia con" nếu cần thiết), và dựa trên loss cuối cùng phát sinh, optimizer cập nhật cả trọng số của các con và trọng số của cha đã tính toán lựa chọn. Trong suy luận, một quyết định cứng duy nhất (tức là "tiến tới con trái/phải") được thực hiện tùy thuộc vào kết quả làm tròn [p] tại mỗi nút. Hardening là quá trình các nút học các ranh giới sao cho các lựa chọn mềm chúng thực hiện cho các đầu vào riêng lẻ chuyển từ các hỗn hợp không quyết định (ví dụ ⟨49,51⟩) sang các hỗn hợp quyết định hơn (ví dụ ⟨3,97⟩). Chúng tôi quan sát thấy rằng trong các cài đặt có sức mạnh đại diện dồi dào (tức là lá rộng và FFF sâu), quá trình này thường tự diễn ra. Trong các cài đặt có thể cần nhiều sức mạnh đại diện hơn (ví dụ transformer thị giác rộng với các bottleneck lá FFF), quá trình này hoặc diễn ra nhưng dừng lại sớm, hoặc diễn ra với tốc độ rất thấp, và chúng tôi chọn khuyến khích nó bằng cách thêm hardening loss.

Nhờ hardening, hiệu suất của cài đặt huấn luyện mềm được chuyển sang suy luận. Như một sản phẩm phụ, các vùng đã học cũng có thể được sử dụng như một phân vùng của không gian đầu vào để giải thích, chỉnh sửa mô hình phẫu thuật, giảm thiểu quên thảm khốc, giảm ngân sách dữ liệu replay, v.v.

Hướng dẫn sử dụng. Giả sử bạn có một kiến trúc hiện tại có lớp FF có chiều rộng w và muốn thay thế nó bằng lớp FFF.
Trường hợp 1: "Tôi muốn suy luận nhanh hơn". Chọn ℓ << w sao cho ℓ phù hợp với ngân sách suy luận mục tiêu của bạn, sau đó thử nghiệm với các độ sâu khác nhau d ≥ log2(w/ℓ) để đạt được hiệu suất mong muốn. Lưu ý rằng ℓ vẫn cần đủ lớn để có thể học hàm một phần trên vùng của nó, và chiều rộng huấn luyện cuối cùng 2dℓ có thể lớn hơn w.
Trường hợp 2: "Tôi muốn một phân vùng của không gian đầu vào". Chọn d sao cho 2d đáp ứng kỳ vọng của bạn về số lượng vùng. Sau đó thử nghiệm với ℓ ≥ w2−d để có hiệu suất tốt nhất. Lưu ý rằng ℓ một lần nữa cần đủ lớn để có thể học hàm một phần trên vùng của nó, và đối với d lớn bạn có thể phải chủ động chống lại tác động của phân mảnh quá mức.

Đóng góp.
1. Chúng tôi giới thiệu kiến trúc feedforward nhanh (FFF), một đồng cấp với kiến trúc feedforward (FF) chỉ sử dụng một phần có thể truy cập thời gian logarit của các neuron tại bất kỳ thời điểm nào.
2. Chúng tôi nghiên cứu ảnh hưởng của kích thước lá và độ sâu đến hiệu suất dự đoán của FFF như các mô hình riêng lẻ và cho thấy rằng trong các cài đặt đủ lớn, FFF cho hiệu suất tương đương với FF có cùng chiều rộng huấn luyện trong khi thực hiện suy luận nhanh hơn đáng kể. Chúng tôi tiếp tục cho thấy rằng FFF mang lại hiệu suất ghi nhớ và tổng quát hóa tốt hơn so với FF có cùng kích thước suy luận.
3. Chúng tôi so sánh kiến trúc FFF với phương pháp hỗn hợp chuyên gia (MoE) về hiệu suất dự đoán và tốc độ suy luận khi số lượng khối tăng, và hỗ trợ các lợi thế được tuyên bố của thiết kế bằng thực nghiệm.
4. Chúng tôi chứng minh rằng FFF có thể được sử dụng khả thi thay cho FF như các phần của kiến trúc lớn hơn, phức tạp hơn như transformer.

Văn bản đỉnh điểm vào một so sánh với các công trình liên quan.

Thuật toán
Ký hiệu đầu vào mạng, chiều đầu ra dimI, dimO.
Ký hiệu mạng feedforward ⟨a, b, c⟩ là mạng feedforward có a đầu vào, b neuron, và c đầu ra.

Lưu ý rằng chúng tôi ghi đè thuật ngữ được thiết kế cho mạng đa lớp và chỉ nói về một tập hợp neuron có cả trọng số đầu vào và đầu ra. Ví dụ, chúng tôi sẽ gọi lớp feedforward BERT-base với chiều đầu vào 768, 3072 neuron ẩn, và 768 neuron đầu ra là lớp feedforward với 3072 neuron, mỗi neuron có 768 đầu vào và 768 đầu ra. Điều này đơn giản hóa rất nhiều việc trình bày của chúng tôi.

Định nghĩa. Một mạng feedforward nhanh có độ sâu d ≥ 0, kích thước nút n, và kích thước lá ℓ là một cặp ⟨N, L⟩.
N := {N0,0, . . . , Nd−1,2d−1−1} là tập hợp các mạng feedforward nút ⟨dimI, n, 1⟩ với kích hoạt sigmoid bổ sung trên đầu ra. Các nút này

--- TRANG 3 ---
Thuật toán 1: Forward pass FFF.
Đầu vào: Mẫu đầu vào ι ∈ DI, nút gốc N0,0
Đầu ra: Đầu ra ∈ DO của FFF cho ι
Hàm FORWARDT(ι, Nm,n):
    nếu Nm,n ∈ L thì
        trả về Nm,n(ι)
    nếu không
        cm,n ← Nm,n(ι)
        trả về cm,n FORWARDT(ι, Nm+1,2n+1) + (1−cm,n)FORWARDT(ι, Nm+1,2n)
    kết thúc
Hàm FORWARDI(ι, Nm,n):
    nếu Nm,n ∈ L thì
        trả về Nm,n(ι)
    nếu không
        cm,n ← Nm,n(ι)
        nếu cm,n ≥ 1/2 thì
            trả về FORWARDI(ι, Nm+1,2n+1)
        nếu không
            trả về FORWARDI(ι, Nm+1,2n)
        kết thúc
    kết thúc

tạo thành một cây nhị phân khả vi cân bằng sao cho Nm+1,2n, Nm+1,2n+1 là các con của Nm,n.
L := {Nd,0, . . . , Nd,2d−1} là tập hợp các mạng feedforward lá ⟨dimI, ℓ, dimO⟩. Tất cả trọng số đều có thể huấn luyện theo mặc định và forward pass được điều chỉnh bởi Thuật toán 1 hoàn toàn xác định.

Các nút của FFF được sắp xếp trong một cây dimI-d khả vi thực hiện lựa chọn mềm trên các lá dưới dạng vector ngẫu nhiên c. Trong huấn luyện, FFF thực hiện một hỗn hợp chuyên gia trên tất cả các lá trong L, với trọng số lựa chọn của hỗn hợp c được tính bằng cách lên từ nút gốc N0,0 (cf. FORWARDT). Trong suy luận, quyết định tại mỗi nút được coi là gần nhất của {0,1}, và thuật toán forward pass tiến hành từ gốc, luôn chọn chỉ một nhánh tùy thuộc vào quyết định nút cục bộ (cf. FORWARDI).

Các vùng trách nhiệm và ranh giới của chúng. Thành phần cây của FFF tạo ra một phân vùng của không gian đầu vào. Mỗi lá chịu trách nhiệm cho chính xác một vùng của phân vùng này, mặc dù trong huấn luyện, dự đoán của nó trên vùng trách nhiệm riêng được trộn với dự đoán của các lá khác. Các ranh giới giữa các vùng riêng lẻ được xác định bởi các mạng nút.

Trong trường hợp khi n = 1 và không có kích hoạt trên mạng nút ngoại trừ sigmoid đầu, ranh giới là mặt phẳng kích hoạt của neuron ẩn. Chuẩn của vector pháp tuyến của mặt phẳng (= trọng số của neuron) ảnh hưởng đến tốc độ sigmoid chuyển từ 0 đến 1 xung quanh ranh giới (cf. Hình 1 dưới-trái). Điều này xác định mức độ rõ ràng của ranh giới được định nghĩa.

Hardening. Để hiệu suất dự đoán của FORWARDT chuyển sang FORWARDI, người ta không được mất thông tin dự đoán khi làm tròn điểm số lựa chọn. Mất thông tin này là tối thiểu khi các quyết định ranh giới đã được hardening đúng cách (cf. Giới thiệu). Như đã gợi ý ở trên, hardening tại một nút thường không phải liên quan đến điều chỉnh ranh giới như một đa tạp trong không gian - việc thay đổi tỷ lệ đồng đều tiến bộ của các hệ số ranh giới (tức là nén sigmoid cuối cùng về hàm bước để làm cho ranh giới được định nghĩa rõ ràng hơn) là đủ.

Diễn giải điểm số lựa chọn nút như xác suất Bernoulli, hardening có thể được theo dõi bằng cách giám sát trung bình batch của entropy của điểm số lựa chọn tại mỗi nút. Trong thực nghiệm của chúng tôi, chúng tôi thấy rằng làm tròn các cặp lựa chọn ⟨1−p, p⟩ với entropy dưới 0.10 có xu hướng dẫn đến chỉ những sai lệch rất khiêm tốn từ hiệu suất FORWARDT. Đối với các tình huống mà hardening của quyết định nút không xảy ra đến mức đủ tự nhiên, hardening có thể được khuyến khích bằng cách thêm hardening loss.

Cho Lpred là loss do đầu ra của FFF. Sau đó người ta có thể lấy tổng loss là Ltotal := Lpred + hLharden với
Lharden := ∑ι∈B ∑N∈N H(N(ι)),
trong đó B ⊆ DI là một batch mẫu, H(p) là entropy của biến ngẫu nhiên Bernoulli, và h là siêu tham số huấn luyện kiểm soát tác động của hardening loss.

Phân mảnh quá mức. Nếu đẩy đến cực đoan, cho phép mạng feedforward nhanh học quá nhiều ranh giới cứng dẫn đến phân mảnh quá mức - một hiện tượng mà mạng chia không gian đầu vào thành số lượng vùng rời rạc theo cấp số nhân và học xấp xỉ các phần của mục tiêu học theo cách quá cụ thể cho từng vùng. Phân mảnh quá mức có hai hậu quả trực tiếp: overfitting cục bộ và "vấn đề batch thu hẹp".

Overfitting cục bộ ký hiệu một quá trình đuôi xảy ra khi mô hình đã đủ hardening, trong đó các ranh giới vùng không còn linh hoạt và các lá nhất định học overfit dữ liệu huấn luyện trên các vùng trách nhiệm của chúng. Điều này là do chúng ngừng nhận các cập nhật gradient có ý nghĩa lớn từ các vùng lân cận, nhưng có thể chịu trách nhiệm xử lý các mẫu test không được hiểu rõ bởi dữ liệu huấn luyện cho vùng của chúng. Overfitting cục bộ thể hiện giống như overfitting cổ điển - hiệu suất validation ngừng cải thiện hoặc xấu đi trong khi học trên tập huấn luyện tiếp tục. Nó có thể được giảm thiểu bằng chuyển vị con ngẫu nhiên - các quyết định mềm ⟨1−p, p⟩ tại mỗi nút có thể được chuyển vị ngẫu nhiên với xác suất thấp nào đó thành ⟨p, 1−p⟩. Điều này nhằm phơi bày con của chúng với dữ liệu huấn luyện của các vùng lân cận để hỗ trợ hiệu suất tổng quát hóa và đã xảy ra một chút đối với các ranh giới mềm, nhưng trở nên hiếm khi các ranh giới hardening.

Biến thể FFF của vấn đề batch thu hẹp cũng là kết quả của hardening ranh giới vùng lá, và nó phát sinh trong các tình huống khi kích thước batch trở nên quá nhỏ cho phân vùng không gian đầu vào được học bởi cây FFF. Nếu phân vùng không gian đầu vào quá tinh vi và các ranh giới hardening, mỗi lá kết thúc nhận các cập nhật gradient có ý nghĩa chỉ từ một phần nhỏ của các mẫu huấn luyện, dẫn đến tiến trình gradient descent không chính xác. Batch shrinking dẫn đến hiệu suất học kém (ví dụ độ chính xác tập huấn luyện thấp, stall sớm, phát triển hỗn loạn) nhưng có thể được giảm thiểu - một cách ngây thơ với kích thước batch lớn hơn, tích lũy gradient, và tốc độ học nhỏ hơn; tổng quát đầy đủ với tối ưu hóa cục bộ.

Chúng tôi xem xét độ phức tạp của thuật toán của chúng tôi theo các tham số d, n, ℓ. Lưu ý rằng chúng tôi thấy n = 1 đủ trong tất cả các thử nghiệm, nhưng chúng tôi giữ n để tổng quát.

Độ phức tạp huấn luyện. Forward pass huấn luyện FORWARDT lên qua d mức của cây, đi qua neuron nút để tính vector lựa chọn cuối cùng c trong O((2d−1)n) thời gian. Sau đó, đầu ra lá được tính và trộn bởi c trong O(2dℓ) thời gian. Điều này có nghĩa là O(2d(ℓ+n)−n) thời gian cho forward pass, và một backward pass (d+1)-bước trở lại quyết định trên gốc.

Từ quan điểm triển khai, chúng tôi biểu thị việc lên qua cây như một vòng lặp đơn thực hiện d tính toán batch giống hệt nhau, sau đó thực hiện forward pass lá cuối cùng và hỗn hợp chuyên gia.

Độ phức tạp suy luận. FORWARDI lên qua cây FFF trong d bước, luôn thực thi chính xác một mạng nút. Sau đó, nó thực hiện suy luận trên một lá, dẫn đến O(dn+ℓ) thời gian.

Về mặt triển khai, việc lên từ gốc qua cây được thực thi như một tính toán batch của một tập hợp trọng số và bias được chỉ mục (nhân-và-tích lũy), so sánh logit với 0, và tiến chỉ mục tùy thuộc vào kết quả so sánh.

Trong kinh nghiệm sử dụng biên dịch ahead-of-time cho CUDA, việc lập chỉ mục chọn lọc trọng số cho quyết định nút thể hiện trong mã native như một offset đơn giản trong việc tải dữ liệu cho phép nhân ma trận batch, chỉ có overhead triển khai hằng số nhỏ ở mức phần cứng khi so sánh với lớp feedforward.

Kích thước và chiều rộng. Mạng feedforward nhanh bao gồm neuron hai loại: nút và lá. Để rõ ràng và thực hiện so sánh trực tiếp với mạng feedforward tương ứng, chúng tôi phân biệt giữa các biến thể của kích thước và chiều rộng mạng.

Một FFF với d, n, ℓ như trong định nghĩa có kích thước huấn luyện (2d−1)n+2dℓ - đây là tất cả neuron của mạng, và chúng đều bị ảnh hưởng bởi tối ưu hóa. Nó tiếp tục có kích thước suy luận dn+ℓ, vì đây là các neuron tham gia để tạo ra đầu ra suy luận bởi FORWARDI.

Tuy nhiên, chỉ có neuron của lá tạo ra đầu ra, với neuron nút chỉ tham gia vào tính toán hỗn hợp đầu ra của các lá riêng lẻ. Do đó, chúng tôi nói rằng FFF có chiều rộng huấn luyện 2dℓ và chiều rộng suy luận ℓ. Lưu ý rằng FFF với tất cả trọng số của mạng nút được đặt thành 0 tương đương với mạng feedforward vanilla có 2dℓ neuron (tối đa một tỷ lệ đồng đều của trọng số đầu ra, có thể học được). Chúng tôi gọi sự khác biệt giữa kích thước huấn luyện/suy luận và chiều rộng là overhead.

Thử nghiệm
Chúng tôi tiến hành một số thử nghiệm để (1) khám phá ảnh hưởng của việc gán neuron với các vùng ảnh hưởng có thể học, (2) so sánh hiệu suất dự đoán và tốc độ của FFF với MoE, và (3) đánh giá tính khả thi của FFF như các phần của kiến trúc sâu hơn. Nhiệm vụ của mỗi thử nghiệm là phân loại hình ảnh, và chúng tôi đánh giá độ chính xác phân loại của softmax của logit đầu ra theo cách thông thường. Đối với FFF, chúng tôi đo độ chính xác của việc đưa ra quyết định "cứng" tại mỗi nút (tức là chúng tôi sử dụng FORWARDI).

Đối với mỗi tập dữ liệu được xem xét, chúng tôi sử dụng tập huấn luyện và test được chỉ định như được cung cấp. Chúng tôi tiếp tục chia tập huấn luyện đầy đủ 9:1 thành tập con huấn luyện và validation.

Để so sánh chất lượng của các mô hình riêng lẻ, chúng tôi đo bốn đại lượng.

Độ chính xác ghi nhớ (MA). Diễn giải mạng feedforward (nhanh) như bộ nhớ mô hình (Bau et al. 2020), chúng tôi đo khả năng học tập huấn luyện của chúng bằng cách tính độ chính xác của mạng overfitted trên tập huấn luyện. Tức là, chúng tôi huấn luyện mạng cho đến khi độ chính xác của chúng trong huấn luyện ngừng cải thiện, sau đó chạy test trên dữ liệu huấn luyện. MA kết quả 100% có nghĩa là mạng đã ghi nhớ thành công tất cả dự đoán cho dữ liệu huấn luyện.

Độ chính xác tổng quát hóa (GA). Coi mạng feedforward (nhanh) như mô hình dự đoán riêng lẻ, chúng tôi đo khả năng dự đoán chính xác các lớp của mẫu chưa thấy trước đây trong tập test. Cho điều này, chúng tôi huấn luyện mạng cho đến khi độ chính xác validation của chúng ngừng cải thiện, và sử dụng mô hình tốt nhất về độ chính xác validation để đánh giá.

Thời gian suy luận và tăng tốc. Triển khai riêng của chúng tôi về các thuật toán FFF được cung cấp qua pip install fastfeedforward và trên GitHub. Chúng tôi biên dịch triển khai FORWARDI của chúng tôi cho GPU NVIDIA A100 sử dụng biên dịch mô hình PyTorch 2.0.1 trong chế độ reduce-overhead. Sau đó chúng tôi chạy mỗi mô hình 10^4 lần với kích thước batch 2048 trên một GPU NVIDIA A100 đơn. Khi liên quan để so sánh, chúng tôi báo cáo thời gian suy luận trung bình t• trên một forward pass đơn dưới các thử nghiệm lặp lại, cùng với độ lệch chuẩn của nó. Chúng tôi tiếp tục báo cáo tăng tốc - phân số tFF/tFFF, trong đó tFFF là thời gian suy luận trung bình cho mô hình FFF đã cho và tFF là thời gian suy luận trung bình cho mạng feedforward vanilla có cùng chiều rộng huấn luyện. Nói đơn giản, tăng tốc cho biết FFF nhanh hơn bao nhiêu so với FF có cùng số neuron có sẵn để đưa ra dự đoán trong huấn luyện, được đo bằng lựa chọn phần mềm và phần cứng này. Trung bình và độ lệch cho tăng tốc trong phụ lục.

Đánh giá khám phá
Để kiểm tra bản chất của mạng feedforward nhanh như một giải pháp thay thế cho mạng feedforward, chúng tôi đo ảnh hưởng của các tham số của chúng đến hiệu suất dự đoán và tốc độ trong bối cảnh.

Đánh giá với đối tác huấn luyện
Chủ đề. Chúng tôi nghiên cứu mối quan hệ giữa cấu hình kích thước lá, độ sâu, và chiều rộng huấn luyện và hiệu suất ghi nhớ

--- TRANG 5 ---
[Bảng 1 tiếp tục với các kết quả thử nghiệm trên USPS, MNIST, FashionMNIST với các mô hình khác nhau và độ chính xác MA, GA, tăng tốc tương ứng]

và tổng quát hóa của mạng feedforward nhanh. Đối với mỗi FFF, chúng tôi cũng xem xét hiệu suất của FF có cùng chiều rộng huấn luyện. Chúng tôi so sánh với chúng có cùng chiều rộng huấn luyện thay vì kích thước huấn luyện vì chỉ neuron lá trực tiếp tham gia vào tính toán dự đoán phân loại cho các đầu vào được cung cấp.

Phương pháp. Chúng tôi huấn luyện mạng feedforward nhanh cho chiều rộng huấn luyện w = 16, 32, 64, 128, kích thước lá ℓ = 1, 2, 4, 8, tập dữ liệu USPS (Hull 1994), MNIST (LeCun, Cortes, and Burges 2010), và FashionMNIST (Xiao, Rasul, and Vollgraf 2017). Đối với mỗi cấu hình w, ℓ chúng tôi tính độ sâu là log2(w/ℓ). Tập hợp các chiều rộng đã được chọn có mục đích: lưu ý rằng bất kỳ mạng feedforward nhanh nào với w, ℓ như trên đều có chiều rộng suy luận nhỏ hơn 16 - cấu hình hẹp nhất của chúng tôi. Đối với mỗi chiều rộng, chúng tôi tiếp tục huấn luyện mạng feedforward vanilla như baseline.

Chúng tôi đưa mạng hình ảnh phẳng. Để dễ so sánh, chúng tôi sử dụng kích thước batch 256 và tối ưu hóa SGD thuần túy với tốc độ học 0.2 bất kể kích thước hoặc độ sâu của mạng, nhưng chúng tôi lưu ý rằng FFF sâu hơn đã hưởng lợi từ kích thước batch lớn hơn và tốc độ học nhỏ hơn. Chúng tôi tham gia hardening loss với tham số tỷ lệ h = 3.0. Chúng tôi thực hiện 10 lần chạy cho mỗi cấu hình, và vì đây là đánh giá giới hạn kiến trúc, chúng tôi báo cáo hiệu suất của mô hình tốt nhất. Trung bình và độ lệch trong phụ lục.

Thảo luận. Kết quả của chúng tôi được liệt kê trong Bảng 1. Một hình ảnh hóa của quá trình hardening có thể được tìm thấy trong Hình 5 của phụ lục. Các quan sát chung là mỗi: tăng chiều rộng, tăng kích thước lá, và tăng kích thước lá trong khi giữ độ sâu không đổi; đều giúp hiệu suất ghi nhớ và tổng quát hóa. Chúng tôi đưa ra một số quan sát cụ thể liên quan đến đóng góp của chúng tôi.

FFF hoạt động tương đương với FF. Đối với chiều rộng và độ sâu đủ lớn trên USPS và MNIST, mạng feedforward nhanh chỉ hơi kém (2-3%) so với mạng feedforward vanilla. Tình cờ, đây cũng là các cấu hình trong đó FFF mang lại cải tiến tốc độ suy luận tốt nhất so với lớp feedforward cổ điển như được đo trên phần cứng của chúng tôi.

Lưu ý hiệu suất của FFF với w = 128, ℓ = 8 trên các tập dữ liệu so với FF với w = 16. Hiệu suất gần đáng kể và thậm chí vượt quá FF, tất cả trong khi kích thước suy luận của các FFF này (12) vẫn dưới FF (16).

Trên FashionMNIST chúng tôi quan sát cùng xu hướng nhưng lưu ý rằng FFF ngoài phạm vi kiểm tra của chúng tôi (w = 512, ℓ = 8) cuối cùng cần thiết để đưa FFF gần (MA = 97.1, GA = 88.1) với hiệu suất của FF.

Tăng tốc tăng theo chiều rộng. Các mạng rộng hơn và sâu hơn trở nên trong khi giữ kích thước lá không đổi, cải tiến tốc độ suy luận được mang lại bởi kiến trúc feedforward nhanh càng đáng kể.

Chiều rộng không đổi dẫn đến đánh đổi tốc độ-hiệu suất. Nếu chiều rộng huấn luyện được giữ cố định, rõ ràng có một sự đánh đổi giữa việc tăng độ sâu (và do đó tăng tốc độ) và hiệu suất (cf. Bảng 1 từ trên xuống dưới).

Tăng độ sâu của mạng trong khi giữ chiều rộng huấn luyện không đổi dẫn đến giảm hiệu suất dần dần do FFF có lá nhỏ hơn, do đó ghi chú của chúng tôi trong "Hướng dẫn sử dụng" (cf. Giới thiệu). Sau này chúng tôi sẽ thấy rằng trong kiến trúc sâu hơn, sự đánh đổi trong mạng chiều rộng không đổi giữa tốc độ suy luận và giảm hiệu suất do kích thước lá nhỏ dường như được giảm bớt trong kiến trúc đa lớp.

Mạng độ sâu lớn-lá nhỏ thể hiện phân mảnh quá mức. Chúng tôi quan sát (USPS, FashionMNIST) rằng giảm kích thước lá trong khi tăng độ sâu (giữ chiều rộng không đổi) dẫn đến hiệu suất ghi nhớ và tổng quát hóa xấu đi nhanh chóng. Sự khác biệt đặc biệt rõ ràng với độ sâu lớn hơn (ℓ = 1, 2, w = 64, 128). Điều này được giải thích tốt bởi phân mảnh quá mức, vì đặc biệt đối với ℓ = 1, w = 128, mỗi lá chỉ nhận 2 mẫu trên mỗi batch trung bình. Kết quả trên USPS cho ℓ = 1, w = 16, 32 là một ví dụ mẫu của điều này: chúng tôi thấy rằng FFF với ℓ = 1, d = 4 mang lại MA 83.4, nhưng người anh em sâu hơn ℓ = 1, d = 5 - có nhiều lá cùng kích thước hơn - chỉ cho MA 77.3.

TL;DR. FFF cho hiệu suất dự đoán tương đương với FF có cùng chiều rộng huấn luyện, nhanh hơn khi chiều rộng huấn luyện tăng, và nếu đẩy đến giới hạn thể hiện đánh đổi tốc độ-hiệu suất và phân mảnh quá mức.

Đánh giá với đối tác suy luận
Chủ đề. Tương tự như thử nghiệm trên với xem xét chính cho chiều rộng huấn luyện, chúng tôi bây giờ đánh giá mạng feedforward nhanh với các độ sâu và kích thước lá khác nhau về kích thước suy luận của chúng. Chúng tôi làm cho kích thước suy luận là điểm so sánh với mạng feedforward vì, không giống như chiều rộng suy luận, nó tỷ lệ thuận trực tiếp với chi phí tính toán của suy luận.

Phương pháp. Chúng tôi huấn luyện mạng feedforward nhanh cho ℓ = 2, 4, 6, 8, 16, 32, d = 2, 6, tập dữ liệu SVHN (Netzer et al. 2011), CIFAR10, CIFAR100 (Krizhevsky, Hinton et al. 2009), và cho mỗi cấu hình ℓ, d chúng tôi tính kích thước suy luận là ℓ + d. Đối với mỗi kích thước suy luận, chúng tôi tiếp tục huấn luyện mạng feedforward vanilla như baseline.

Chúng tôi huấn luyện mạng với tất cả tham số như trên ngoại trừ h, trong đó chúng tôi không tham gia hardening loss (h = 0) vì chúng tôi thấy rằng hardening có xu hướng tự xảy ra. Chúng tôi thực hiện 10 lần chạy cho mỗi cấu hình và báo cáo hiệu suất tốt nhất.

Thảo luận. Kết quả của chúng tôi thưa thớt vì tất cả các tổng khác nhau có thể có của kích thước lá và độ sâu và được hiển thị trong Hình 2. Phù hợp với trực giác, chúng tôi quan sát thấy rằng độ sâu lớn hơn và kích thước lá lớn hơn dẫn đến hiệu suất ghi nhớ và tổng quát hóa tốt hơn.

Hơn nữa, FFF vượt trội so với FF có cùng kích thước suy luận. FFF với các độ sâu và kích thước khác nhau liên tục vượt trội so với FF với chiều rộng bằng kích thước suy luận FFF, cả về MA và GA. Về MA, sự khác biệt rõ ràng và tăng với độ sâu và kích thước lá. Về GA, FFF ban đầu có lợi thế so với FF, nhưng sau này hiệu suất của tất cả mô hình hợp nhất hướng tới plateau tại giới hạn của phương pháp chỉ feedforward đơn lớp ngây thơ.

TL;DR. FFF mang lại hiệu suất dễ dàng hơn so với FF có cùng chiều rộng suy luận.

--- TRANG 7 ---
[Bảng 2 với kết quả so sánh giữa feedforward, mixture-of-experts, và fast feedforward networks]

Đánh giá so sánh
Kiến trúc đối thủ trực tiếp của feedforward nhanh, đi kèm với tập hợp tham số thiết kế riêng, là lớp hỗn hợp chuyên gia, mà chúng tôi lấy theo dạng gốc (Shazeer et al. 2017).

So sánh hiệu suất dự đoán
Chủ đề. Chúng tôi so sánh FFF với MoE và FF có chiều rộng huấn luyện khác nhau về hiệu suất dự đoán của chúng. Chúng tôi giữ chiều rộng lá và chuyên gia không đổi và tập trung vào khả năng của các kiến trúc mang lại tính chất ghi nhớ và tổng quát hóa tốt cũng như về compute huấn luyện cần thiết để đạt được những tính chất đó.

Phương pháp. Chúng tôi thử nghiệm trên tập dữ liệu CIFAR10 không tăng cường. Chúng tôi huấn luyện mạng feedforward, hỗn hợp chuyên gia, và feedforward nhanh có kích thước tăng dần sao cho chúng luôn thống nhất về chiều rộng huấn luyện. Để giữ chiều rộng suy luận giống nhau, chúng tôi đặt chiều rộng lá thành 32 và chiều rộng chuyên gia thành 16 với luôn tham gia k = 2 chuyên gia. Lưu ý rằng trong khi mạng chuyên gia đơn có thể được sử dụng cho suy luận, chúng không thể truyền gradient đến mạng gating (cf. Shazeer et al. (2017)). Chúng tôi lấy chiều rộng w = 2^6, 2^7, ..., 2^10, tương ứng với mạng MoE 16- đến 64-chuyên gia và FFF có độ sâu 1 đến 5. Để khuyến khích tầm quan trọng bình đẳng và cân bằng tải trong MoE, chúng tôi đặt wimportance = wload = 0.1 theo công trình trước. Để khuyến khích FFF hardening, chúng tôi sử dụng h = 3.0. Chúng tôi huấn luyện tất cả mô hình chiều rộng kích thước batch 4096 tối đa 7000 epoch, với dừng sớm sau 350 epoch không thấy cải thiện trong độ chính xác validation tương ứng. Tất cả mô hình đã được huấn luyện với optimizer Adam, tốc độ học 0.001, với tốc độ học giảm một nửa trên plateau độ chính xác huấn luyện 250-epoch.

Thảo luận. Kết quả được liệt kê trong Bảng 2. Ngay từ đầu, chúng tôi quan sát thấy rằng MA và GA hưởng lợi từ chiều rộng huấn luyện lớn hơn trên tất cả mô hình ngoại trừ GA trên FFF, nơi chúng tôi thấy overfitting cục bộ không được giảm thiểu ảnh hưởng tiêu cực đến hiệu suất.

FFF nhanh nhất để mang lại MA và GA. Chúng tôi thấy rằng FFF nhanh nhất (về ETT) để mang lại cả MA và GA, nhưng, phù hợp với đánh giá khám phá trước của chúng tôi, mang lại MA hơi thấp hơn so với FF có cùng chiều rộng huấn luyện, và chịu overfitting cục bộ với độ sâu tăng.

FFF vượt trội so với MoE có cùng chiều rộng huấn luyện. Chúng tôi quan sát thấy rằng FFF liên tục mang lại điểm MA và GA tốt hơn so với mạng MoE có cùng chiều rộng huấn luyện. Chúng tôi tiếp tục thấy rằng chúng làm vậy ở ETT nhỏ hơn một bậc độ lớn. Chúng tôi quy sự khác biệt này chủ yếu cho nhiễu được kiểm soát có thể học được được đưa vào tính toán hỗn hợp chuyên gia để hỗ trợ cân bằng tải và tổng quát hóa. Tuy nhiên, không có nhiễu, mạng MoE sẽ overfit để học chỉ với một số ít chuyên gia. Chúng tôi cũng thử nghiệm với các giá trị khác nhau của wimportance và wload, nhưng chúng tôi thấy những giá trị đó có xu hướng có hại cho nỗ lực cân bằng tải. Giá trị cuối cùng của chúng tôi về tầm quan trọng batch và tải phù hợp với những giá trị đạt được trong Shazeer et al. (2017).

TL;DR. FFF mang lại sức mạnh đại diện dễ dàng hơn so với MoE có chiều rộng huấn luyện bằng nhau.

So sánh tốc độ suy luận
Chủ đề. Vì các hoạt động liên quan đến tính toán đầu ra mạng chuyên gia/lá giống nhau, sự khác biệt về tốc độ suy luận giữa hỗn hợp chuyên gia và mạng feedforward nhanh chỉ đến từ chức năng của cơ chế gating/lookup. Do đó chúng tôi giữ chiều rộng chuyên gia/lá không đổi và đo thời gian cần thiết để thực hiện forward pass suy luận của mạng feedforward, hỗn hợp chuyên gia, và feedforward nhanh qua các thử nghiệm lặp lại cho số lượng chuyên gia/lá tăng dần (tức là mạng rộng hơn và rộng hơn).

Để thêm tính thực tế, chúng tôi mô phỏng điều kiện của lớp feedforward BERT-base (Devlin et al. 2018), đặt chiều rộng đầu vào và đầu ra của tất cả neuron thành 768 mỗi cái.

Phương pháp. Chúng tôi xem xét mô hình FF có chiều rộng 32×2^1 đến 32×2^5, trong đó chúng tôi nhấn mạnh các khối 32 neuron để so sánh trực tiếp với các mô hình khác. Chúng tôi tiếp tục đánh giá mô hình MoE với chiều rộng chuyên gia e = 32 và 2^1 đến 2^15 chuyên gia, và mô hình FFF với chiều rộng lá e = 32 và độ sâu 1 đến 15. Để loại bỏ tác động của tính toán hỗn hợp trên các phép đo của chúng tôi, chúng tôi giữ e = ℓ và đặt k = 1, mặc dù cấu hình tham số MoE này không thể huấn luyện được (cf. trên). Khi đo, mỗi mô hình thực hiện suy luận trên đầu vào BERT-base với kích thước batch 256 chính xác 20000 lần.

Thảo luận. Kết quả đo tốc độ suy luận được hình ảnh hóa trong Hình 3-4. Chúng tôi quan sát thấy rằng cả MoE và FFF đều cung cấp tăng tốc đáng kể cho tốc độ suy luận khi so sánh với baseline FF. Tuy nhiên, Hình 4 cho thấy xu hướng rõ ràng của thời gian suy luận mô hình MoE tăng theo cấp số nhân với lũy thừa của số lượng chuyên gia, trái ngược hoàn toàn với mối quan hệ tuyến tính giữa hai thể hiện bởi mô hình FFF. Điều này hoàn toàn phù hợp với phân tích lý thuyết của chúng tôi về độ phức tạp thời gian suy luận của hai kiến trúc.

TL;DR. Chúng tôi đã xác nhận thực nghiệm sự khác biệt lũy thừa giữa độ phức tạp thời gian suy luận của cơ chế nội bộ MoE và FFF.

Lớp feedforward nhanh như khối xây dựng
Chủ đề. Chúng tôi chứng minh rằng mạng feedforward nhanh có thể được sử dụng như lớp thay cho lớp feedforward tiêu chuẩn trong kiến trúc transformer, do đó mang lại cho nó một sự tăng tốc hiệu suất đáng kể. Trước đây, chúng tôi lưu ý rằng kích thước lá quá nhỏ cho một vấn đề nhất định có thể dẫn đến sự xuất hiện của phân mảnh quá mức. Ở đây chúng tôi đẩy thiết lập thực nghiệm của chúng tôi đến giới hạn về kích thước lá và nghiên cứu tác động của phân mảnh quá mức trong transformer sâu.

Phương pháp. Chúng tôi thử nghiệm trên tập dữ liệu CIFAR10 của hình ảnh 32x32-pixel 3-kênh, với tăng cường lật ngang, dọc ngẫu nhiên, và tăng cường tuyến tính ngẫu nhiên (dịch chuyển, xoay, tỷ lệ). Như mô hình, chúng tôi sử dụng vision transformer 4-lớp với kích thước patch 4, chiều ẩn 128, input dropout 0.1, và không layer dropout.

Chúng tôi xem xét vision transformer với lớp feedforward của chúng được thay thế bằng lớp feedforward nhanh có chiều rộng huấn luyện 128, và vision transformer baseline với lớp feedforward có chiều rộng w = 128. Lớp feedforward nhanh có kích thước lá ℓ = 1, 2, 4, 8, 16, 32 và độ sâu log2(w/ℓ). Chúng tôi thử ba mức hardening: h = 5, 10, ∞, trong đó ∞ ký hiệu rằng cây FFF đã được đóng băng hiệu quả từ đầu (tức là các ranh giới không thể huấn luyện). Chúng tôi sử dụng optimizer Adam với tốc độ học ban đầu 4e−4 và giảm một nửa tốc độ học trên plateau độ chính xác validation 50-epoch. Đối với mỗi cấu hình ℓ, d, chúng tôi báo cáo hiệu suất tổng quát hóa của mô hình tốt nhất và tăng tốc được đo tại lớp feedforward (không phải toàn bộ transformer).

Thảo luận. Kết quả thử nghiệm của chúng tôi có thể thấy trong Bảng 3. Một hình ảnh hóa của quá trình hardening qua các lớp của transformer có thể được tìm thấy trong Hình 6 của phụ lục. Phù hợp với đánh giá độ phức tạp thuật toán của chúng tôi, tăng tốc được đo tại lớp feedforward tăng với kích thước lá giảm. Hơn nữa:

FFF neuron đơn đủ. Chúng tôi quan sát thấy rằng ngay cả lớp feedforward nhanh với chiều rộng suy luận 1 cũng đủ cho vision transformer để mang lại hiệu suất hợp lý, với giảm hiệu suất tương đối chỉ 5.8%.

Tác động của phân mảnh quá mức bị triệt tiêu. Chúng tôi quan sát thấy rằng hiệu suất tổng quát hóa GA chỉ chịu tương đối nhẹ do tăng độ sâu và giảm kích thước lá, hoàn toàn trái ngược với kết quả của Bảng 1. Chúng tôi quy điều này cho độ sâu của transformer và coi đó là dấu hiệu khuyến khích về tính khả thi của FFF như thay thế cho FF.

TL;DR. Lớp feedforward nhanh có thể mang lại tăng tốc suy luận trong vision transformer so với lớp feedforward có cùng chiều rộng huấn luyện với chi phí giảm hiệu suất nhỏ.

Công trình liên quan
Công trình của chúng tôi chồng lấn với nỗ lực nghiên cứu trong hai lĩnh vực tăng tốc suy luận.

--- TRANG 9 ---
[Bảng 3 với kết quả kiểm tra vision transformer sử dụng lớp feedforward và fast feedforward]

Thực thi có điều kiện. Mặc dù ngày nay phần lớn không hoạt động do xu hướng chuyển khỏi kiến trúc tùy chỉnh, các thiết kế sửa đổi cho mô hình MLP và CNN đã được đề xuất để cho phép thực thi một phần của chúng khi có thể.

Một số phương pháp đã được đề xuất (Davis and Arel 2013; Bengio, Léonard, and Courville 2013; Bengio et al. 2015; Almahairi et al. 2016) để học hoặc phân phối chính sách hoặc các thành phần neuron kiểm soát bổ sung để quyết định khối neuron lớp nào sẽ thực thi trong forward pass.

So sánh, mạng feedforward nhanh hoàn toàn che giấu việc học các vùng lá khỏi người dùng (tiết kiệm từ siêu tham số h nếu sử dụng) và đi kèm dưới dạng sẵn sàng suy luận khi được huấn luyện, không yêu cầu điều chỉnh khi được bao gồm như một phần của transformer.

Trong một tổng quát hóa đáng chú ý của dòng công trình này với kiến trúc sâu, Ioannou et al. (2016) đã đề xuất một phương pháp ngang hàng với mạng neuron tích chập sâu học định tuyến đầu vào qua một chuỗi lớp trung gian được chọn. Trong khi phương pháp của chúng tôi nhanh chóng định tuyến tín hiệu đến một mạng neuron feedforward lá đơn, nó không so sánh với mạng sâu.

Mô hình modular "hỗn hợp chuyên gia". Mô hình rất lớn thực tế đòi hỏi tính modular. Cách đơn giản nhất để modular hóa mô hình transformer lớn để giảm chi phí suy luận của chúng là chia nhỏ lớp feedforward của chúng thành n khối neuron, sau đó huấn luyện một bộ phân loại kiểm soát để chọn khối nào tham gia vào forward pass. Điều này thường được thực hiện bằng cách huấn luyện một lớp tuyến tính được kích hoạt softmax rộng để tạo ra vector ngẫu nhiên điểm hỗn hợp được áp dụng cho đầu ra trên mỗi khối để tạo ra đầu ra cuối cùng. Một số biến thể của phương pháp này đã được đề xuất và kiểm tra trên nhiều mô hình lớn (Shazeer et al. 2017; Lepikhin et al. 2020; Fedus, Zoph, and Shazeer 2022).

Chúng tôi so sánh kỹ lưỡng feedforward nhanh với mạng hỗn hợp chuyên gia trong các phần trước. Tóm tắt ngắn gọn, phương pháp hỗn hợp chuyên gia giảm chiều rộng suy luận lớp theo hệ số n/k, trong đó k là số lượng khối điểm cao nhất để tham gia vào suy luận, nhưng yêu cầu O(n) thời gian để chọn k khối, và thường dựa vào ngẫu nhiên hóa có kiểm soát để tránh hình thành sở thích mạnh mẽ chỉ cho một số ít chuyên gia. Điều này vẫn đúng ngay cả khi nhiều lớp mixer chuyên gia được giới thiệu. Trong so sánh trực tiếp, mạng feedforward nhanh có độ sâu d = log2 n giảm suy luận theo hệ số n và chỉ yêu cầu O(d) = O(log n) thời gian để quyết định lá nào sử dụng. Phải thừa nhận, để bù đắp cho tác động của chỉ có một lá để đưa ra quyết định, lá của lớp feedforward nhanh có thể phải rộng hơn một chút so với khối của lớp hỗn hợp chuyên gia tương ứng.

Regionalization. Một lợi thế bổ sung của FFF so với tất cả công trình liên quan là có một sự tương ứng trực tiếp giữa các phần của mạng được sử dụng trong suy luận và các vùng có thể nhận dạng đại số của không gian đầu vào. Điều này có thể được tận dụng để giảm thiểu quên thảm khốc khi chỉnh sửa mô hình và giảm đáng kể ngân sách dữ liệu replay bằng cách áp dụng phân vùng đã học của không gian đầu vào để phân vùng dữ liệu huấn luyện.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo từ Almahairi đến Xiao, bao gồm các công trình về dynamic capacity networks, deep generative models, conditional computation, BERT, Switch transformers, handwritten text recognition, decision forests, MNIST, mixture-of-experts, attention mechanisms, và Fashion-MNIST]

--- TRANG 11 ---
Kết quả mở rộng của Bảng 1
Bảng 4 liệt kê trung bình và độ lệch chuẩn của đánh giá khám phá mạng feedforward nhanh so với mạng feedforward.

Ngoài các quan sát được đưa ra trong văn bản chính, chúng tôi thấy rằng phương sai của MA và GA khi thực hiện các lần chạy lặp lại tăng với kích thước lá giảm và thấy rõ nhất đối với chiều rộng huấn luyện nhỏ.

Hình ảnh hóa quá trình hardening
Để cung cấp một số cái nhìn sâu sắc và trực giác về hardening của ranh giới quyết định nút, chúng tôi theo dõi trung bình batch entropy qua các nút của lớp feedforward nhanh và báo cáo về sự phát triển của chúng khi huấn luyện tiến triển một cách trực quan.

Hình 5 cho thấy sự phát triển của trung bình batch entropy cho ba mô hình có độ sâu khác nhau mà việc huấn luyện được chấm dứt bởi dừng sớm trên plateau độ chính xác FORWARDI validation. Chúng tôi quan sát thấy rằng bằng cách giữ tập dữ liệu (và do đó độ phức tạp của không gian đầu vào) và kích thước lá không đổi, độ chính xác trung bình batch của lớp sâu hội tụ nhanh hơn đối với mạng sâu hơn. Chúng tôi quy điều này đơn giản cho thực tế rằng mạng feedforward nhanh càng sâu trong thử nghiệm này, càng có nhiều lá có sẵn để phục vụ có chọn lọc cho các không gian con của không gian đầu vào, cho phép mạng thực hiện các phân tách sạch hơn giữa các vùng đã học.

Hình 6 báo cáo sự phát triển của trung bình batch entropy trên mỗi lớp cho mô hình vision transformer 4-lớp với mạng feedforward nhanh thay cho mạng feedforward vanilla. Chúng tôi lưu ý rằng sớm trong huấn luyện, entropy của các lớp gần đầu vào hơn (tức là các lớp sớm hơn/thấp hơn) nhanh hơn để hội tụ. Tuy nhiên, chúng tôi thấy rằng entropy của lớp thứ hai sau đó ngừng giảm có ý nghĩa, và entropy của lớp đầu tiên thực sự bắt đầu tăng. Điều này có thể được giải thích bởi các ranh giới quyết định hardening cho thấy dấu hiệu hành vi nút cổ chai trong các lớp cao hơn của transformer, do đó cần thiết rằng các lớp thấp hơn đóng góp với các biểu diễn đã học có chất lượng cao hơn.

Diễn giải việc stall hoặc tăng entropy như một dấu hiệu của nhu cầu về nhiều sức mạnh đại diện hơn, chúng tôi lưu ý qua nhiều thử nghiệm rằng hình dạng lý tưởng cho một vision transformer, ngay cả trên tập dữ liệu tương đối đơn giản như CIFAR10, dường như hướng về hình dạng kim tự tháp hơn là đường thẳng các lớp chiều rộng đồng đều.

Mối quan hệ với mạng MoE đa gating
Văn học gần đây hơn về mạng hỗn hợp chuyên gia (Lepikhin et al. 2020; Fedus, Zoph, and Shazeer 2022) đề xuất sử dụng cấu trúc gating phân cấp, hai lớp. Tiết kiệm từ tính ngẫu nhiên vốn có của lựa chọn mạng gating, điều này có thể được xem như một bước nhỏ hướng tới FFF. Tuy nhiên, FFF chú ý đến các vùng được chỉ định rõ ràng của không gian thay vì được bỏ phiếu nhiễu như các chuyên gia phù hợp nhất cho một đầu vào nhất định (mà chúng tôi đã cho thấy mang lại cho chúng tính chất huấn luyện và suy luận tốt hơn nhiều), và hơn nữa, ngay cả dưới cách tiếp cận như vậy, việc giảm thời gian suy luận của mạng hỗn hợp chuyên gia vẫn không đổi trong chiều rộng của mạng mục tiêu.

[Hình 5: Sự phát triển của trung bình batch entropy quyết định trong lớp feedforward nhanh với ℓ = 8, d = 2, 3, 4, h = 3.0, được huấn luyện trên tập dữ liệu MNIST]

[Hình 6: Sự phát triển của trung bình batch entropy quyết định qua vision transformer 4-lớp có chiều 128 được trang bị lớp feedforward nhanh ℓ = 32, d = 2, h = 0.10, được huấn luyện trên tập dữ liệu CIFAR10]

Mã
Chúng tôi cung cấp các triển khai của chúng tôi như một gói Python được xây dựng trên framework PyTorch.
>>>>>>> Stashed changes

Chỉ cần sử dụng pip install fastfeedforward. Tài liệu được cung cấp cùng với mã.

--- TRANG 12 ---
<<<<<<< Updated upstream
[Bảng 4 với kết quả chi tiết bao gồm trung bình và độ lệch chuẩn cho tất cả các thí nghiệm]
=======
[Bảng 4: Kết quả chi tiết của thử nghiệm khám phá trên FFF với các giá trị trung bình và độ lệch chuẩn cho USPS, MNIST, FashionMNIST với các cấu hình khác nhau của MA, GA, và thời gian]
>>>>>>> Stashed changes
