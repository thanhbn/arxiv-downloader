# Mở rộng Quy mô Mô hình Ngôn ngữ Chuyên gia với Khám phá Miền Không giám sát

Suchin Gururangan* 1yMargaret Li* 1 2Mike Lewis2Weijia Shi1 2Tim Althoff1
Noah A. Smith1 3Luke Zettlemoyer1 2

## Tóm tắt
Các mô hình ngôn ngữ lớn thường được huấn luyện một cách dày đặc: tất cả các tham số đều được cập nhật dựa trên tất cả các đầu vào. Điều này đòi hỏi phải đồng bộ hóa hàng tỷ tham số trên hàng nghìn GPU. Chúng tôi giới thiệu một phương pháp đơn giản nhưng hiệu quả để huấn luyện các mô hình ngôn ngữ lớn và thưa thớt một cách bất đồng bộ trên các tập dữ liệu văn bản tùy ý. Phương pháp của chúng tôi phân cụm một tập dữ liệu thành các nhóm tài liệu liên quan, huấn luyện một mô hình ngôn ngữ chuyên gia riêng biệt trên mỗi cụm, và kết hợp chúng trong một tập hợp thưa thớt để suy luận. Cách tiếp cận này tổng quát hóa việc huấn luyện song song một cách đáng xấu hổ bằng cách tự động khám phá các miền cho mỗi chuyên gia, và loại bỏ gần như tất cả chi phí giao tiếp của các mô hình ngôn ngữ thưa thớt hiện có. Kỹ thuật của chúng tôi vượt trội hơn các baseline dày đặc trên nhiều tập dữ liệu và tác vụ few-shot, và phân tích của chúng tôi cho thấy việc chuyên biệt hóa các chuyên gia cho các cụm có ý nghĩa là chìa khóa cho những cải tiến này. Hiệu suất cũng cải thiện theo số lượng chuyên gia và kích thước dữ liệu huấn luyện, gợi ý rằng đây là một cách tiếp cận có hiệu quả cao và dễ tiếp cận để huấn luyện các mô hình ngôn ngữ lớn.

## 1. Giới thiệu
Các mô hình ngôn ngữ (LM) được huấn luyện trên tối đa hàng nghìn tỷ token văn bản (Hoffmann et al., 2022; Touvron et al., 2023). Điều này cải thiện hiệu suất trên nhiều tác vụ, nhưng cũng phát sinh chi phí cực kỳ cao: hàng nghìn GPU cần phải hoạt động đồng thời để cập nhật tất cả các tham số ở mỗi bước (Zhang et al., 2022; Chowdhery et al., 2022). Branch-Train-Merge (BTM; Li et al. 2022) giảm bớt chi phí này bằng cách chia tổng tài nguyên tính toán cho một tập hợp các mô hình ngôn ngữ chuyên gia (ELM) nhỏ hơn, mỗi mô hình được huấn luyện độc lập trên một tập con (hoặc miền) riêng biệt của tập dữ liệu huấn luyện và được kết hợp trong quá trình suy luận.

Trong công việc này, chúng tôi giới thiệu Cluster-Branch-Train-Merge (C-BTM; Hình 1), một thuật toán không cần metadata để mở rộng quy mô LM mà không cần đồng bộ hóa đa nút lớn. Chúng tôi sử dụng phân cụm không giám sát để khám phá các miền trong một tập dữ liệu, và huấn luyện một ELM trên mỗi cụm một cách độc lập (§2.1). Tại thời điểm suy luận, chúng tôi kích hoạt thưa thớt một tập con của các ELM đã huấn luyện (§2.2). Chúng tôi kết hợp các ELM bằng cách cân bằng đầu ra của chúng với khoảng cách giữa một embedding của ngữ cảnh hiện tại và trung tâm cụm của mỗi chuyên gia. Điều này cho phép tính toán thưa thớt đơn giản và hiệu quả (Fedus et al., 2022) bằng cách chỉ lấy ra các chuyên gia top-k khi dự đoán mỗi token mới.

C-BTM tổng quát hóa BTM bằng cách cho phép kiểm soát chi tiết về số lượng và kích thước của các cụm dữ liệu, vì chúng được học tự động mà không bị ràng buộc bởi metadata có sẵn. Chúng tôi sử dụng khả năng mới này để điều tra các tính chất mở rộng quy mô của C-BTM như một hàm số của số lượng chuyên gia được huấn luyện, kiểm soát nhiều yếu tố khác nhau (§3). Các thí nghiệm rộng rãi cho thấy rằng huấn luyện nhiều cụm hơn luôn dẫn đến perplexity xác thực tốt hơn so với các mô hình cụm đơn (tức là dày đặc), và số lượng cụm tối ưu tăng theo tổng tài nguyên tính toán (§4.1). Các kết quả này nhất quán cho cả chuyên gia 1.3B và 6.7B tham số.

Với nhiều cụm hơn, chúng tôi có thể song song hóa việc huấn luyện chuyên gia một cách tích cực: ví dụ, chúng tôi huấn luyện 128 ELM (tổng cộng 168B tham số) trên 168B token văn bản tổng hợp chỉ với 8 GPU tại một thời điểm. Điều này cho phép chúng tôi tránh nhiều khó khăn thực tế liên quan đến việc huấn luyện LM lớn trên nhiều nút đồng thời (§4.2). Hơn nữa, số lượng tham số tại thời điểm suy luận có thể được giữ cố định ngay cả khi số lượng chuyên gia tăng lên (§4.3): chỉ sử dụng top-2 hoặc top-4 chuyên gia có thể so sánh với việc sử dụng tất cả chuyên gia, trong khi chỉ sử dụng top-1 chuyên gia vẫn vượt trội hơn mô hình dày đặc. Huấn luyện với nhiều cụm hơn cũng hiệu quả hơn so với huấn luyện các mô hình dày đặc lớn hơn: trong §4.4, chúng tôi chứng minh rằng huấn luyện nhiều ELM 1.3B chuyên gia, và làm thưa thớt chúng thành một LM 5.2B tham số, đạt được perplexity tương tự như một mô hình dày đặc 6.7B, nhưng chỉ với 29% số FLOP huấn luyện. Những cải tiến này cũng được phản ánh trong các thí nghiệm phân loại văn bản few-shot (§5), cho thấy các mô hình C-BTM vượt trội hơn các baseline dày đặc ngay cả với suy luận thưa thớt nặng.

C-BTM cung cấp một cách tiếp cận mô hình hóa thưa thớt được đơn giản hóa đáng kể, loại bỏ gần như tất cả chi phí giao tiếp từ các sơ đồ LM thưa thớt hiện có. Các LM thưa thớt hiện có thường định tuyến các token khác nhau đến các tham số chuyên biệt (Lepikhin et al., 2021; Fedus et al., 2021; Clark et al., 2022). Tuy nhiên, chúng vẫn chưa được áp dụng rộng rãi, có thể một phần do chi phí giao tiếp của việc định tuyến mỗi token trong mỗi lớp thưa thớt (Artetxe et al., 2021), thách thức trong việc học chuyên biệt hóa các chuyên gia cho token (Zhou et al., 2022), và sự cần thiết của các cơ chế bổ sung để cân bằng việc sử dụng chuyên gia (Lewis et al., 2021). C-BTM cải thiện so với các LM thưa thớt bằng cách định tuyến chuỗi (thay vì token) sử dụng phân cụm cân bằng offline (thay vì cân bằng tải online) mà không có tham số chia sẻ giữa các chuyên gia. Chúng tôi so sánh trực tiếp với một mô hình mixture-of-experts với định tuyến top-2 (Lepikhin et al., 2021) trong §6.

Phân tích cuối cùng của chúng tôi (§7) cho thấy rằng phân cụm cân bằng là chìa khóa cho hiệu suất C-BTM; nó hoạt động tốt như việc gán chuyên gia với metadata vàng, và vượt trội đáng kể so với các baseline phân cụm ngẫu nhiên và không cân bằng. Nhìn chung, các phát hiện của chúng tôi gợi ý rằng C-BTM là một phương pháp hiệu quả và dễ tiếp cận để mở rộng quy mô các mô hình ngôn ngữ lớn thành các tập dữ liệu khổng lồ. Chúng tôi công bố mã nguồn và mô hình của mình công khai.

## 2. C-BTM
Chúng tôi giới thiệu C-BTM, một phương pháp huấn luyện song song đáng xấu hổ chuyên biệt hóa các mô hình ngôn ngữ chuyên gia cho các miền được khám phá thông qua phân cụm thay vì metadata. C-BTM cho phép mở rộng quy mô đến số lượng miền và ngân sách tính toán tùy ý trên bất kỳ tập dữ liệu nào. Trong phần này, chúng tôi trình bày việc huấn luyện C-BTM (Hình 2) và suy luận (Hình 3).

### 2.1. Huấn luyện

**Bước 0: Phân cụm** Để phân đoạn tập dữ liệu của chúng tôi, chúng tôi sử dụng phân cụm k-means, thực thi các cụm cân bằng. Các ELM được huấn luyện mà không có ràng buộc này có hiệu suất kém hơn (§7.2).

Xem xét quan điểm expectation-maximization lặp lại và cứng của phân cụm k-means. Trong bước expectation, mỗi embedding tài liệu được gán cho một trung tâm cụm dựa trên khoảng cách Euclidean của nó đến mỗi trung tâm. Trong bước maximization, mỗi trung tâm cụm được cập nhật để trở thành embedding trung bình của tập hợp tài liệu hiện tại được gán cho nó. Để cân bằng các cụm, chúng tôi hình thành bước expectation như một bài toán gán tuyến tính cân bằng (Malinen & Fränti, 2014). Cho D embedding tài liệu với biểu diễn {w1; : : : ; wD} và K trung tâm cụm với biểu diễn {h1; : : : ; hK}, chúng tôi gán mỗi tài liệu d cho một cụm với chỉ số gán ad ∈ {0; : : : ; K}:

max a1;:::;aD ∑(d=1 to D) dist(had; wd) s.t. ∀k, ∑(d=1 to D) 1[ad=k] = D/K

trong đó dist là khoảng cách Euclidean. Nhiều thuật toán tồn tại để giải quyết bài toán này; chúng tôi theo Lewis et al. (2021) và sử dụng thuật toán auction (Bertsekas, 1992). Chúng tôi chỉ sử dụng cân bằng khi ước tính các trung tâm cụm; chúng tôi sử dụng suy luận tham lam khi dự đoán các cụm, vì cân bằng tại thời điểm suy luận khó khăn đối với các tập dữ liệu khổng lồ.

Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng một hàm embedding tf-idf đơn giản, có hiệu quả cao ở quy mô lớn và dẫn đến các cụm có thể giải thích được. Chúng tôi chỉ sử dụng một shard của mỗi tập dữ liệu để huấn luyện mô hình phân cụm của chúng tôi. Bất kỳ tài liệu mới nào, một khi được embed, đều có thể được ánh xạ hiệu quả đến (các) cụm gần nhất mà không cần huấn luyện bổ sung. Bất kỳ hàm embedding nào đều có thể được sử dụng, mặc dù việc lựa chọn embedder có thể áp dụng các giả định khác nhau về cấu thành của một miền văn bản và đi kèm với các đánh đổi về hiệu quả.

**Bước 1: Phân nhánh (từ LM hạt giống)** Để bắt đầu huấn luyện các chuyên gia trên mỗi cụm k từ Bước 0, trước tiên chúng tôi phân nhánh từ (tức là tạo k bản sao của) một LM hạt giống. Các LM hạt giống rất quan trọng đối với chức năng tổng thể của các ELM, và các ELM hoạt động tốt nhất khi LM hạt giống đã được huấn luyện với một tập dữ liệu đa dạng (Li et al., 2022). Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng OPT LM (Zhang et al., 2022) làm hạt giống.

**Bước 2: Huấn luyện** Chúng tôi gán mỗi ELM cho một cụm duy nhất, và huấn luyện trên mỗi cụm với mục tiêu log likelihood.

**Bước 3: Hợp nhất** Sau khi huấn luyện trên miền được gán, chúng tôi thêm ELM mới vào một tập hợp lớn hơn để suy luận.

Trong công việc này, chúng tôi tập trung vào một lần lặp duy nhất của C-BTM để đơn giản. Công việc tương lai có thể khám phá việc phân nhánh từ các chuyên gia đã được huấn luyện trong nhiều lần lặp.

### 2.2. Suy luận

Tại thời điểm suy luận, chúng tôi sử dụng một tập hợp thưa thớt của các đầu ra của ELM cho các ngữ cảnh kiểm tra đến (Hình 3). Chính thức, xem xét rằng mô hình ngôn ngữ cung cấp, tại mỗi timestep, p(Xt|x<t). Chúng tôi giới thiệu một biến miền D, cùng với mỗi chuỗi. Sau đó, phân phối có điều kiện bước tiếp theo trên lịch sử x<t là:

p(Xt|x<t) = ∑(j=1 to k) p(Xt|x<t; D=j)p(D=j|x<t)

với trọng số tập hợp được đánh dấu.

Với embedder và mô hình phân cụm đã được huấn luyện trước từ Bước 0 (§2.1), chúng tôi embed ngữ cảnh hx<t và sử dụng k trung tâm cụm {hc0 : : : hck}. Chúng tôi đặt trọng số tập hợp như:

p(D=j|x<t) ∝ topk[exp(−dist(hx<t; hcj)²/T)]

trong đó dist là khoảng cách Euclidean, T là một tham số nhiệt độ làm sắc nét hoặc làm mịn phân phối xác suất trên các trung tâm cụm, và hàm top-k lọc k xác suất hàng đầu và chuẩn hóa lại phân phối để tổng bằng 1. Công thức này gợi nhớ đến các cơ chế truy xuất nearest-neighbor cho các mô hình ngôn ngữ (Khandelwal et al., 2019; Shi et al., 2022).

Các trọng số tập hợp này được cập nhật cho mỗi token đến, mặc dù trong các thí nghiệm riêng biệt chúng tôi quan sát thấy rằng các gán cụm (và thực tế là trọng số tập hợp) có thể được cố định cho nửa thứ hai của một tài liệu mà không làm giảm hiệu suất; điều này có thể tăng tốc suy luận hơn nữa.

Chúng tôi thấy rằng, trong thực tế, hiệu suất của các mô hình của chúng tôi mạnh mẽ ngay cả với top-2 hoặc top-4 chuyên gia, có nghĩa là chi phí suy luận của mô hình ngôn ngữ tương đương với một LM nhỏ hơn nhiều. Chúng tôi thực hiện một nghiên cứu thực nghiệm về các biến thể suy luận trong §4.3.

### 2.3. So sánh với Huấn luyện Dày đặc

Các LM dày đặc thường được huấn luyện sử dụng hàng trăm hoặc hàng nghìn GPU đồng thời, tất cả đều đồng bộ hóa gradient ở mỗi lần cập nhật. Ví dụ, OPT-175B (Zhang et al., 2022) được huấn luyện trên 992 GPU A100 80GB, và PaLM-540B (Chowdhery et al., 2022) được huấn luyện trên 6144 chip TPU v4. C-BTM cải thiện hiệu quả huấn luyện bằng cách giảm chi phí giao tiếp, vì chỉ các GPU huấn luyện cùng một ELM mới phải giao tiếp. Hơn nữa, khả năng lỗi GPU có thể tăng đáng kể theo số lượng GPU. C-BTM cải thiện khả năng phục hồi của việc huấn luyện phân tán, vì một lỗi GPU đơn lẻ chỉ làm trì hoãn việc huấn luyện cho một ELM duy nhất, trong khi ở việc huấn luyện dày đặc, một lỗi GPU đơn lẻ ảnh hưởng đến việc huấn luyện trên tất cả các GPU khác. C-BTM cũng làm cho việc huấn luyện LM lớn khả thi hơn trên các cụm GPU chia sẻ, vì nó hiệu quả phân tách việc huấn luyện thành các công việc nhỏ hơn có thể chạy bất đồng bộ. Điều này làm cho việc lập lịch công việc hiệu quả hơn bằng cách giảm số lượng GPU cần được phân bổ đồng thời.

### 2.4. So sánh với BTM (Li et al., 2022)

Phương pháp của chúng tôi giải quyết một số hạn chế của các kỹ thuật huấn luyện và suy luận được đề xuất bởi Li et al. (2022).

Đầu tiên, BTM bị giới hạn bởi dữ liệu huấn luyện có metadata có thể được sử dụng để xác định các miền của nó. Các tập dữ liệu LM điển hình, bao gồm C4 (Raffel et al., 2019) và the Pile (Gao et al., 2021), được lấy từ Internet mà không giữ lại nguồn gốc tài liệu tại thời điểm thu thập, và không khả thi để gán nhãn thủ công. Ngoài ra, số lượng chuyên gia tối ưu cho một kích thước tập dữ liệu cố định, kiến trúc mô hình và ngân sách vẫn chưa được biết, và khó khám phá với các miền dựa trên metadata, vì chúng không thể dễ dàng hợp nhất hoặc chia tách. C-BTM mở rộng khả năng áp dụng của BTM cho các tập dữ liệu tùy ý.

Hơn nữa, suy luận BTM tuân theo phương pháp cached prior được giới thiệu bởi Gururangan et al. (2022), trong đó các trọng số tập hợp được ước tính bằng cách sử dụng quy tắc Bayes trên dữ liệu giữ lại bổ sung, và prior P(D=j) được ước tính với một trung bình động theo cấp số nhân trên các chuỗi ước tính posterior yêu cầu các lần chuyển tiếp qua các chuyên gia. Ước tính này sau đó được cố định trong quá trình đánh giá dữ liệu kiểm tra.

Với C-BTM, chúng tôi định tuyến chỉ dựa trên ngữ cảnh hiện tại. Do đó, không cần dữ liệu bổ sung hoặc các lần chuyển tiếp qua các chuyên gia để ước tính trọng số tập hợp, cũng như chúng tôi không cần giả định rằng các tài liệu liền kề trong tập kiểm tra đến từ cùng một phân phối. Điều này cũng ngụ ý rằng kỹ thuật tính trung bình tham số của Li et al. (2022) không phù hợp với cài đặt của chúng tôi, vì nó yêu cầu cố định các trọng số được gán cho mỗi chuyên gia cho một tập hợp các tài liệu đánh giá. Công việc tương lai có thể khám phá việc hợp nhất các tham số chuyên gia cho mỗi ngữ cảnh trong quá trình suy luận.

### 2.5. So sánh với Mixture-of-Experts (MoE)

Giống như các mô hình MoE (ví dụ, Fedus et al., 2022), C-BTM cho phép mở rộng quy mô hiệu quả của các LM lớn trong khi vẫn giữ chi phí suy luận có thể quản lý được. Tuy nhiên, C-BTM định tuyến chuỗi (thay vì token) sử dụng phân cụm cân bằng offline (thay vì cân bằng tải online) mà không có tham số chia sẻ giữa các chuyên gia. Điều này loại bỏ hiệu quả tất cả các phức tạp liên quan đến việc cân bằng sử dụng chuyên gia (Lewis et al., 2021), tránh các hoạt động all-to-all đắt đỏ giữa các chuyên gia (Artetxe et al., 2021), và tự nhiên dẫn đến việc chuyên biệt hóa chuyên gia có thể giải thích được cho các miền của tập dữ liệu huấn luyện. Trong §6, chúng tôi so sánh trực tiếp với các baseline MoE được huấn luyện với sparse upcycling (Komatsuzaki et al., 2022), khởi tạo MoE với một checkpoint dày đặc, phản ánh cách C-BTM khởi tạo các ELM.

## 3. Thiết lập Thí nghiệm

Chúng tôi thiết kế một tập hợp các thí nghiệm để nghiên cứu C-BTM trên hai tập dữ liệu lớn (Hình 4) được chọn để khác biệt với tập dữ liệu được sử dụng để huấn luyện mô hình OPT hạt giống của chúng tôi, và báo cáo perplexity trên dữ liệu giữ lại từ mỗi tập dữ liệu.

### 3.1. Dữ liệu

**C4 (Raffel et al., 2019)** C4 là một phân phối có sẵn công khai của một snapshot Common Crawl trên bộ dữ liệu Huggingface. Chúng tôi sử dụng phiên bản no blocklist (en.noblocklist) để huấn luyện trên một tập dữ liệu nằm ngoài phân phối so với tập dữ liệu pretraining hạt giống (OPT) của chúng tôi. C4 bao gồm 393M tài liệu với tổng cộng 220B BPE token. Chúng tôi huấn luyện trên tối đa 168B token.

**S2ORC (Lo et al., 2019)** Semantic Scholar Research Open Corpus (S2ORC) là một tập dữ liệu có sẵn công khai các bài báo học thuật full-text từ Semantic Scholar. Tập dữ liệu bao gồm 20 lĩnh vực nghiên cứu (ví dụ, Sinh học, Khoa học Máy tính, Nghệ thuật), và chứa 16M tài liệu, tổng cộng 87B BPE token. Chúng tôi huấn luyện trên tối đa 168B token qua nhiều epoch.

**Dữ liệu đánh giá** Đối với tất cả các thí nghiệm, chúng tôi báo cáo perplexity mô hình hóa ngôn ngữ trên 200 tài liệu giữ lại được lấy mẫu ngẫu nhiên. Vì S2ORC không đi kèm với dữ liệu validation được định nghĩa trước, chúng tôi tạo một tập dữ liệu validation bằng cách lấy mẫu số lượng tài liệu bằng nhau từ mỗi lĩnh vực nghiên cứu.

### 3.2. Thiết lập Thí nghiệm

**Phân cụm dữ liệu** Chúng tôi phân đoạn mỗi tập dữ liệu sử dụng phân cụm k-means cân bằng cho k ∈ {2, 4, 8, 16, 32, 64, 128} (§2.1). Để huấn luyện các mô hình phân cụm, trước tiên chúng tôi embed tất cả dữ liệu với một vectorizer tf-idf sử dụng scikit-learn, với các giả định tối thiểu: chúng tôi chỉ loại bỏ stop-word từ một từ vựng cố định và thay thế số bằng một token dummy. Sau đó, chúng tôi giảm chiều của các embedding kết quả; chúng tôi thực hiện truncated SVD với 100 chiều, sau đó chuẩn hóa vector bằng cách loại bỏ mean và co tỷ lệ thành đơn vị phương sai, điều mà chúng tôi quan sát trong các thí nghiệm ban đầu đã cải thiện chất lượng phân cụm. Cuối cùng, các biểu diễn này được phân cụm sử dụng một triển khai Pytorch tùy chỉnh. Chúng tôi trình bày các cụm đã học và hình ảnh hóa trong Hình 4 và Hình A (trong phụ lục). Chúng tôi sử dụng một shard duy nhất của mỗi tập dữ liệu huấn luyện (384K tài liệu cho C4, 155K tài liệu cho S2ORC) để huấn luyện mô hình phân cụm và embedder của nó. Không có dữ liệu đánh giá nào được sử dụng trong quá trình này.

**LM Hạt giống** Vì các LM được huấn luyện trên tập dữ liệu đa dạng tạo ra hạt giống tốt hơn (Li et al., 2022), chúng tôi sử dụng các mô hình ngôn ngữ OPT đã được huấn luyện trước (Zhang et al., 2022) làm hạt giống cho tất cả các thí nghiệm.

**Siêu tham số mô hình** Chúng tôi sử dụng kiến trúc OPT được triển khai trong Metaseq (Zhang et al., 2022). Chúng tôi sử dụng OPT-1.3B cho tập hợp thí nghiệm ban đầu, và nhân bản các thí nghiệm của chúng tôi với OPT-6.7B. Theo Zhang et al. 2022, chúng tôi sử dụng từ vựng GPT-2 với 50,257 loại BPE (Radford et al., 2019), và huấn luyện với các chuỗi 2,048-token, qua các ranh giới tài liệu. Chúng tôi thêm một token beginning-of-document vào mỗi tài liệu. Chúng tôi đặt dropout thành 0.1 cho tất cả các tham số trừ những tham số của lớp embedding.

**Siêu tham số huấn luyện** Đối với tất cả các mô hình, chúng tôi cố định tỷ lệ học thành tỷ lệ được sử dụng trong quá trình pretraining OPT (2e-4 cho các mô hình 1.3B tham số; 1.2e-4 cho các mô hình 6.7B tham số; Zhang et al. 2022) sử dụng lịch trình tỷ lệ học giảm tuyến tính về zero (không có warmup), điều mà chúng tôi thấy hoạt động tốt cho hầu hết các cài đặt sau một tìm kiếm lưới các tỷ lệ học nhanh nhất tránh phân kỳ. Chúng tôi sử dụng kích thước batch 8 cho mỗi GPU, và huấn luyện với fp16 và fully-sharded data-parallel (Artetxe et al., 2021). Chúng tôi huấn luyện trên NVIDIA V100 32GB GPU. Tất cả các mô hình được huấn luyện với Metaseq (Zhang et al., 2022). Đối với một số lượng cụm k và tổng ngân sách GPU n, mỗi ELM được phân bổ n/k GPU, giữ tổng số FLOP hiệu quả cố định trên các mô hình được tiếp xúc với cùng số lượng token.

**Mở rộng quy mô** Chúng tôi huấn luyện tổng cộng 10K bước trong mỗi lần chạy; để tiếp xúc mô hình với nhiều token hơn, chúng tôi tăng tổng ngân sách GPU theo tỷ lệ, lên đến 64 GPU. Chúng tôi mô phỏng các ngân sách lớn hơn, lên đến 1024 GPU, bằng cách tăng các bước tích lũy gradient với 64 GPU. Phương pháp mở rộng quy mô này tăng kích thước batch hiệu quả của mô hình cho cùng số bước, và duy trì thời gian chạy gần như không đổi trong nhiều thí nghiệm của chúng tôi. Thiết lập thí nghiệm này cũng có nghĩa là khi số lượng cụm tăng, tổng tập hợp ELM được tiếp xúc với nhiều dữ liệu hơn với ít tính toán đồng thời hơn giữa các GPU.

Các cách khác để huấn luyện trên nhiều dữ liệu hơn (ví dụ, bằng cách giữ tổng kích thước batch cố định và tăng số bước) có thể mang lại kết quả khác nhau. Các kết hợp kích thước batch và tỷ lệ học tốt nhất để huấn luyện các mô hình ngôn ngữ có thể cụ thể cho nhiều yếu tố khác nhau, bao gồm kích thước mô hình, tập dữ liệu và tổng tài nguyên tính toán có sẵn (Shallue et al., 2018; McCandlish et al., 2018; Yang et al., 2021). Trong các thí nghiệm sơ bộ, chúng tôi thấy rằng các mô hình chuyên gia hưởng lợi từ tỷ lệ học nhanh hơn và kích thước batch lớn hơn. Với kích thước batch đủ lớn, các chuyên gia mạnh mẽ với nhiều tỷ lệ học khác nhau. Các thí nghiệm ngân sách lớn hơn của chúng tôi có thể hưởng lợi từ tỷ lệ học cao hơn, nhưng chúng tôi để việc tinh chỉnh thêm cho công việc tương lai.

**Suy luận** Một trong những siêu tham số chính cho suy luận là nhiệt độ T (Phương trình 3), điều này chi phối độ sắc nét của phân phối xác suất trên các chuyên gia cho một ngữ cảnh cho trước. Chúng tôi thấy rằng đặt T=0.1 hoạt động tốt cho hầu hết các cài đặt. Chúng tôi cũng tính toán các trung tâm cụm gần nhất cho mỗi ngữ cảnh đến, bất kể các gán cụm đã ổn định như thế nào cho một tài liệu. Tuy nhiên, chúng tôi thấy rằng các gán này có thể được cố định cho nửa thứ hai của một tài liệu mà không làm giảm perplexity; điều này có thể tăng tốc suy luận hơn nữa. Siêu tham số quan trọng khác là giá trị top-k, làm thưa thớt phân phối xác suất trên các chuyên gia. Đối với các thí nghiệm cốt lõi của chúng tôi trong §4.1, chúng tôi đặt top-k bằng tổng số chuyên gia mà chúng tôi đã huấn luyện cho mỗi mô hình. Chúng tôi khám phá ảnh hưởng của việc thực thi thưa thớt với các giá trị top-k thấp hơn trong §4.3.

**Baseline** Trong các thí nghiệm chính của chúng tôi (§4), chúng tôi so sánh với một baseline dày đặc mạnh (tức là mô hình 1-cụm của chúng tôi) theo pretraining OPT. Chúng tôi cũng tăng dần số lượng cụm mà chúng tôi huấn luyện cho một số lượng token cố định. Trong các thí nghiệm tiếp theo (§6), chúng tôi so sánh với các mô hình ngôn ngữ MoE được khởi tạo từ một checkpoint dày đặc.

### 3.3. Thực hiện So sánh Mô hình Công bằng

Chúng tôi tuân theo các khuyến nghị của Dehghani et al. (2021) và báo cáo kết quả với nhiều chỉ số chi phí, và trình bày chi tiết các lựa chọn của chúng tôi ở đây. Khi so sánh ngân sách huấn luyện mô hình, chúng tôi chủ yếu quan tâm đến chi phí tiền tệ thực sự của việc huấn luyện mô hình, thường được tính theo tỷ lệ trực tiếp với thời gian GPU. So sánh suy luận mô hình có hai cân nhắc chính: chi phí tiền tệ mà người triển khai mô hình phải gánh, một lần nữa được đo bằng thời gian GPU, và độ trễ cho người dùng cuối, hoặc thời gian wall-clock (tức là suy luận mô hình chậm như thế nào đối với người dùng cuối).

Chúng tôi rõ ràng không so sánh hoặc khớp số lượng tham số mô hình trong quá trình huấn luyện, điều này có ảnh hưởng tối thiểu đến chi phí huấn luyện mô hình tách biệt khỏi ảnh hưởng của nó đối với thời gian GPU. Số lượng tham số huấn luyện là một thước đo chi phí đặc biệt gây hiểu lầm không phù hợp cho các mô hình thưa thớt, vì chúng có thể duy trì FLOP và tốc độ suy luận của các mô hình dày đặc mặc dù huấn luyện nhiều tham số hơn (Dehghani et al., 2021).

**Thời gian GPU huấn luyện** Giả sử phần cứng cố định, thời gian GPU trong quá trình huấn luyện mô hình chủ yếu được xác định bởi FLOP và giao tiếp liên máy. Tuy nhiên, công việc trước thường chỉ khớp FLOP, bỏ qua giao tiếp liên GPU bổ sung được tạo ra bởi một số mô hình (ví dụ, MoE) làm tăng chi phí huấn luyện. Lý tưởng nhất, các so sánh của chúng tôi có thể trực tiếp cố định thời gian GPU. Điều này thách thức trong thực tế, vì ngay cả các tính toán giống hệt nhau trên cùng một nút GPU tại các thời điểm khác nhau có thể khác nhau đáng kể về tốc độ do các yếu tố như nhiệt độ, hoạt động khác trên nút, hoặc chất lượng kết nối GPU. Để duy trì tính nhất quán và công bằng mặc dù những yếu tố gây nhiễu này, kết quả của chúng tôi so sánh các mô hình khớp FLOP với cùng ngân sách dữ liệu huấn luyện trên cùng số lượng cập nhật (§4.1), nhưng cũng báo cáo tốc độ huấn luyện cho mỗi mô hình khớp FLOP (§4.2). Điều này cho phép chúng tôi tách biệt và phản ánh chính xác nhiều chỉ số chi phí của việc huấn luyện. Vì, trong các thí nghiệm của chúng tôi, các mô hình được tiếp xúc với cùng số lượng token phát sinh cùng số lượng FLOP, chúng tôi sử dụng kích thước dữ liệu huấn luyện như một thước đo có thể giải thích hơn của tổng ngân sách huấn luyện.

**Thời gian GPU suy luận** Thời gian GPU suy luận cũng chủ yếu là kết quả của FLOP và chi phí giao tiếp. Vì giao tiếp trong quá trình suy luận là tối thiểu, chúng tôi so sánh FLOP thông qua tham số suy luận (§4.3). Chúng tôi không tính đến FLOP của router C-BTM, có thể thay đổi dựa trên cách tiếp cận phân cụm, và tương đối không đáng kể.

**Độ trễ suy luận** FLOP không phải là một chỉ số lý tưởng cho độ trễ suy luận của các mô hình của chúng tôi, bởi vì C-BTM cho phép suy luận song song trên các ELM. Điều này có nghĩa là nếu các ELM chia sẻ cùng một kiến trúc (ví dụ, OPT-1.3B), độ trễ suy luận luôn tương đương với độ trễ của một ELM duy nhất, bất kể số lượng chuyên gia hoạt động. Tuy nhiên, độ trễ suy luận có thể khá khác nhau giữa các kiến trúc mô hình (ví dụ, OPT-1.3B và OPT-6.7B); chúng tôi thảo luận thêm về điều này trong §4.4. Như với thời gian GPU suy luận, chúng tôi không xem xét độ trễ của router C-BTM.

## 4. Kết quả Mô hình hóa Ngôn ngữ

Chúng tôi bắt đầu với một tập hợp các thí nghiệm trong đó chúng tôi huấn luyện LM với C-BTM trên các tập dữ liệu từ §3.1. Chúng tôi quan tâm đến việc đo lường cách hiệu suất thay đổi khi chúng tôi tăng tổng tài nguyên tính toán. Trước tiên chúng tôi so sánh các mô hình theo chi phí huấn luyện: tổng token huấn luyện (§4.1) và thời gian huấn luyện (§4.2). Sau đó, trong §4.3, chúng tôi so sánh hiệu suất mô hình dọc theo trục chi phí suy luận: tổng số lượng tham số tại thời điểm suy luận. Cuối cùng, trong §4.4 chúng tôi so sánh hiệu suất mô hình bằng cách cố định cả chi phí huấn luyện và suy luận. Trên tất cả các ngân sách tính toán, C-BTM cung cấp những lợi ích đáng kể so với việc huấn luyện dày đặc, và các cải tiến hiệu suất tăng lên khi tổng tài nguyên tính toán tăng.

### 4.1. Kiểm soát Tổng Token Huấn luyện

Đầu tiên, chúng tôi so sánh hiệu suất mô hình kiểm soát kích thước dữ liệu huấn luyện tổng thể (hoặc tương đương, FLOP huấn luyện; §3.3). Hình 5 cho thấy perplexity đánh giá trên C4 và S2ORC với tối đa 16 cụm. Huấn luyện trên nhiều hơn một cụm luôn vượt trội hơn huấn luyện với một cụm duy nhất (tức là một mô hình dày đặc). Khi lượng dữ liệu huấn luyện tăng, khoảng cách giữa các mô hình của chúng tôi và mô hình dày đặc mở rộng, chỉ ra rằng các chuyên gia sử dụng tốt hơn các tập dữ liệu huấn luyện lớn hơn, có thể do sự chuyên biệt hóa tăng của chúng. Những kết quả này gợi ý rằng khi chúng tôi tăng lượng dữ liệu có sẵn, C-BTM hưởng lợi từ nhiều cụm hơn.

Tuy nhiên, Hình 6 cho thấy rằng tồn tại một số lượng cụm tối ưu cho mỗi ngân sách token mà chúng tôi xem xét. Mỗi số lượng cụm có một phạm vi ngân sách mà chúng tối ưu, và mức tối ưu tiến triển một cách mịn màng từ số lượng cụm nhỏ hơn đến lớn hơn khi chúng tôi tăng kích thước dữ liệu huấn luyện. Nếu chúng tôi tăng số lượng cụm vượt quá mức tối ưu, mỗi chuyên gia có phần dữ liệu không đủ, dẫn đến hiệu suất kém hơn.

Tuy nhiên, chúng tôi quan sát thấy rằng sử dụng nhiều cụm hơn tối ưu cho các cài đặt ngân sách token cao nhất vẫn vượt trội hơn mô hình dày đặc. Vì việc huấn luyện với nhiều cụm hơn rẻ hơn cho một kích thước dữ liệu huấn luyện cố định do tính song song, có thể thích hợp trong một số cài đặt để huấn luyện với một số lượng lớn cụm mặc dù hiệu suất kém hơn tối ưu của chúng. Dựa trên các xu hướng chúng tôi quan sát ở quy mô này, chúng tôi kỳ vọng rằng số lượng cụm cao hơn sẽ trở nên tối ưu khi chúng tôi mở rộng quy mô kích thước dữ liệu huấn luyện hơn nữa.

Tính nhất quán của các kết quả của chúng tôi trên C4 và S2ORC gợi ý rằng những xu hướng chung này có thể được áp dụng rộng rãi cho nhiều tập dữ liệu. Tuy nhiên, số lượng cụm tối ưu cho một ngân sách tính toán cho trước có thể cụ thể cho tập dữ liệu. Công việc tương lai có thể khám phá mối quan hệ giữa các đặc điểm tập dữ liệu và số lượng cụm tối ưu.

Những xu hướng này nhất quán khi chúng tôi tăng kích thước của các chuyên gia lên 6.7B tham số (Hình 7), mặc dù các khoảng cách giữa các baseline của chúng tôi giảm, có thể do sự gia tăng đáng kể trong FLOP pretraining cho OPT-6.7B.

### 4.2. So sánh Thời gian Huấn luyện

Bây giờ, chúng tôi chuyển sang so sánh các mô hình của chúng tôi dựa trên thời gian huấn luyện. Chúng tôi đo tốc độ huấn luyện mỗi mô hình với giây tối đa mỗi lần cập nhật cho mỗi lần chạy huấn luyện. Đối với các mô hình C-BTM với nhiều hơn một cụm, chúng tôi sử dụng giây tối đa mỗi lần cập nhật trên tất cả các chuyên gia. Để làm cho các so sánh của chúng tôi công bằng, chúng tôi chỉ so sánh thời gian huấn luyện của các mô hình có cùng kích thước batch hiệu quả (§3.3). Kết quả của chúng tôi được hiển thị trong Hình 8. Khi chúng tôi tăng số lượng cụm và kích thước dữ liệu huấn luyện, tốc độ cập nhật cho C-BTM tăng lên, vì các mô hình với số lượng cụm cao hơn sử dụng ít GPU hơn mỗi chuyên gia dưới một ngân sách cố định, và không có giao tiếp giữa các chuyên gia. Điều này gợi ý rằng các mô hình C-BTM với nhiều cụm hơn có thể được tiếp xúc với nhiều dữ liệu hơn trong cùng lượng thời gian với các mô hình dày đặc.

Như đã thảo luận trong §2.3, C-BTM cũng cung cấp các tăng tốc thực tế quan trọng khi huấn luyện LM lớn ở quy mô. C-BTM chia các ngân sách tính toán lớn giữa nhiều mô hình, sao cho chúng tôi có thể huấn luyện trên 168B token chỉ với 8 GPU mỗi chuyên gia trong cài đặt 128-cụm. Trên các cụm đa nút chia sẻ, việc phân bổ nhiều công việc nhỏ hơn phát sinh thời gian chờ tích lũy ngắn hơn so với một công việc đồng bộ lớn duy nhất, vì chúng có thể sử dụng tài nguyên chia sẻ hiệu quả hơn, và chạy trên các nút nhàn rỗi ngắn hạn (Wortsman et al., 2022). Hơn nữa, việc huấn luyện LM lớn dễ bị lỗi nút, tăng vọt gradient và các hành vi bất ngờ khác (Zhang et al., 2022). Với các mô hình dày đặc, khi một nút bị lỗi, tất cả các nút phải khởi động lại do đồng bộ hóa. Với C-BTM, các chuyên gia được huấn luyện độc lập; nếu một nút bị lỗi, chỉ có chuyên gia tương ứng cần được khởi động lại, và tất cả các chuyên gia khác không bị ảnh hưởng.

### 4.3. Kiểm soát Chi phí Suy luận qua Số lượng Tham số

So sánh các mô hình chỉ với ngân sách huấn luyện bỏ qua thực tế là chi phí thời gian GPU suy luận C-BTM tăng lên khi chúng tôi tăng số lượng cụm, vì chúng tôi huấn luyện nhiều tham số hơn. Để giảm thiểu những chi phí này, chúng tôi có thể sử dụng hàm top-k (Phương trình 3) để động tăng sử dụng một tập con của các chuyên gia cho mỗi ngữ cảnh đến trong quá trình đánh giá (§2.2). Tiếp theo, chúng tôi nghiên cứu ảnh hưởng của số lượng tham số suy luận đối với hiệu suất mô hình. Chúng tôi tập trung vào ngân sách huấn luyện lớn nhất (tức là 168B token) cho các thí nghiệm này.

Kết quả (Hình 9) cho thấy rằng mặc dù huấn luyện nhiều tham số hơn, huấn luyện C-BTM với nhiều cụm và sau đó chỉ sử dụng top-1 chuyên gia vẫn vượt trội hơn mô hình dày đặc. Hơn nữa, sử dụng top-2 hoặc top-4 chuyên gia mang lại hiệu suất có thể so sánh với việc kích hoạt tất cả các chuyên gia. Đôi khi chúng tôi quan sát thấy rằng việc làm thưa thớt thậm chí có thể cải thiện hiệu suất một chút so với việc sử dụng tất cả các chuyên gia (ví dụ, xem mô hình 16 cụm cho C4 trong Hình 9). Chúng tôi suy đoán rằng việc có tất cả các chuyên gia hoạt động có thể tạo ra hiệu ứng nhiễu từ các chuyên gia chuyên biệt cho các cụm không liên quan đến ngữ cảnh thời gian kiểm tra.

Kết quả của chúng tôi trong Hình 10 gợi ý rằng việc làm thưa thớt ngay cả các mô hình chuyên gia lớn hơn (tức là những mô hình có nhiều cụm hơn mức tối ưu cho một ngân sách token cho trước) vẫn rất hiệu quả. Ở cài đặt cực đoan nhất, sử dụng top-1 chuyên gia cho mô hình 128 cụm (sử dụng 0.7% tổng tham số tại thời điểm suy luận cho mỗi ngữ cảnh) vẫn vượt trội hơn mô hình dày đặc, và mô hình top-4 chuyên gia (3.1% tổng tham số) có hiệu suất so sánh với việc sử dụng tất cả các chuyên gia.

Những kết quả này gợi ý rằng C-BTM dẫn đến một LM rất thưa thớt, và chi phí suy luận có thể được giữ cố định ngay cả khi số lượng chuyên gia tăng, mặc dù các chuyên gia bổ sung có thể được thêm vào để tăng hiệu suất hơn nữa.

### 4.4. So sánh với Mô hình Dày đặc Lớn hơn

Trong so sánh cuối cùng của phần này, chúng tôi xem xét cả chi phí huấn luyện và suy luận cùng nhau. Chúng tôi so sánh một mô hình 6.7B 1-cụm (dày đặc) và mô hình C-BTM với các chuyên gia 1.3B tham số, sử dụng 16 cụm (tối ưu trong các thí nghiệm của chúng tôi từ §4.1) và suy luận top-4, dẫn đến 5.2B tham số suy luận. Mô hình C-BTM này có chi phí suy luận thấp hơn so với mô hình dày đặc 6.7B tham số lớn hơn (§3.3). Mô hình trước sử dụng ít tham số suy luận hơn, phát sinh chi phí thời gian GPU suy luận nhỏ hơn, và có độ trễ thấp hơn, có thể so sánh với độ trễ của một ELM 1.3B tham số duy nhất.

Chúng tôi so sánh FLOP được sử dụng để huấn luyện mỗi mô hình. Theo Artetxe et al. (2021), chúng tôi xây dựng các đường cong hiệu quả liên tục bằng cách nội suy giữa các quan sát thực nghiệm của chúng tôi. Cụ thể, chúng tôi tính toán tăng tốc giữa các mô hình chuyên gia cụm của chúng tôi và mô hình dày đặc bằng cách nội suy giữa các quan sát rời rạc của giá trị perplexity cho một số FLOP thực nghiệm cho trước. Mục tiêu của chúng tôi là xác định số lượng FLOP cần thiết để đạt được một giá trị perplexity cụ thể. Nếu các ELM được huấn luyện với C-BTM đạt được perplexity tương tự như mô hình dày đặc với một nửa FLOP, chúng tôi kết luận rằng C-BTM đạt được tăng tốc 2×.

Kết quả của chúng tôi được trình bày trong Hình 11. Một mô hình C-BTM nhỏ hơn, được tiếp xúc với 168B token văn bản, có thể đạt được perplexity tương tự như mô hình dày đặc 6.7B lớn hơn với tăng tốc 3.5×. Các ước tính tăng tốc này phụ thuộc vào lượng pretraining được thực hiện trên mỗi mô hình. Công việc tương lai có thể thực hiện những thí nghiệm này với các mô hình lớn hơn và nhiều ELM hơn.

### 4.5. Tóm tắt

Kết quả của chúng tôi chứng minh rằng kiểm soát nhiều loại ngân sách tính toán khác nhau, C-BTM vượt trội hơn việc huấn luyện dày đặc trong mô hình hóa ngôn ngữ. Hơn nữa, chúng tôi chứng minh rằng C-BTM dẫn đến một mô hình ngôn ngữ thưa thớt hiệu quả, trong đó top-1, top-2 và top-4 chuyên gia từ các mô hình với ít nhất 8 cụm vượt trội đáng kể so với các mô hình 1-cụm, 2-cụm và 4-cụm. Những kết quả này gợi ý khả năng vượt trội hơn các mô hình dày đặc bằng cách tăng biên độ, trong khi giữ cả chi phí huấn luyện và suy luận cố định, khi tài nguyên tính toán và số lượng chuyên gia tăng.

## 5. Kết quả Tác vụ Downstream

Các xu hướng từ §4 có mở rộng sang các cài đặt downstream không? Để bắt đầu trả lời câu hỏi này, chúng tôi thực hiện đánh giá few-shot trên sáu tác vụ phân loại văn bản downstream. Chúng tôi thực sự thấy rằng các mô hình được huấn luyện với C-BTM vượt trội hơn các đối tác dày đặc của chúng trong những cài đặt này.

### 5.1. Thiết lập Thí nghiệm

**Tác vụ** Chúng tôi thử nghiệm với sáu tác vụ phân loại văn bản, bao gồm phân loại chủ đề, cảm xúc và phát hiện hate speech. Chi tiết của các tập dữ liệu có trong Phụ lục A.7.

**Suy luận few-shot** Chúng tôi thực hiện đánh giá 8-shot. Đối với mỗi tác vụ, chúng tôi lấy mẫu ngẫu nhiên 8 ví dụ với nhãn của chúng từ tập train, và thêm vào đầu chúng như các minh chứng cho mỗi ví dụ kiểm tra. Đối với các mô hình C-BTM, chúng tôi ước tính trọng số tập hợp cho mỗi ví dụ bằng cách chuyển cả ví dụ và các minh chứng qua phân cụm đã được huấn luyện trước của chúng tôi (§2.2). Chúng tôi tính xác suất của mỗi nhãn cho tác vụ dưới mô hình, và báo cáo độ chính xác bằng cách đếm tỷ lệ các ví dụ kiểm tra mà nhãn vàng có xác suất cao nhất. Chúng tôi báo cáo độ chính xác trung bình trên 5 seed ngẫu nhiên. Chúng tôi để lại phân tích cẩn thận về C-BTM với số lượng minh chứng thay đổi và các kỹ thuật suy luận few-shot cho công việc tương lai.

**Baseline** Chúng tôi so sánh hiệu suất của các mô hình C-BTM 1- và 16-cụm được huấn luyện trên 168B token của C4 (tức là ngân sách cao nhất của chúng tôi từ §4). Đối với mô hình 16-cụm, chúng tôi cũng thực hiện suy luận top-1 và top-4 (§4.3). Chúng tôi bổ sung so sánh với một baseline ngẫu nhiên, các mô hình OPT-1.3B và 6.7B gốc (không có bất kỳ huấn luyện bổ sung nào), và mô hình 1-cụm 6.7B tham số được huấn luyện trên 20B token của C4.

### 5.2. Kết quả

Kết quả của chúng tôi trong Bảng 1 cho thấy rằng mô hình C-BTM 16-cụm luôn vượt trội hơn baseline 1-cụm, 1.3B tham số, đôi khi một cách đáng kể. Điều này phù hợp với kết quả mô hình hóa ngôn ngữ của chúng tôi (§4.1). Mô hình 1-cụm đạt độ chính xác thấp hơn OPT-1.3B trên một số tác vụ mặc dù có huấn luyện bổ sung, gợi ý rằng các mô hình của chúng tôi có thể gặp phải catastrophic forgetting, vì tập dữ liệu C4 nằm ngoài miền so với OPT.

Tuy nhiên, mô hình 16-cụm vượt trội hơn OPT-1.3B trên tất cả các tác vụ ngoại trừ Twitter. Ngoài ra, suy luận top-1 và top-4 khớp hoặc vượt quá việc sử dụng tất cả các chuyên gia trong một số cài đặt, phù hợp với kết quả mô hình hóa ngôn ngữ của chúng tôi trong §4.3. Chúng tôi kiểm tra các cụm liên quan đến các chuyên gia có khả năng cao nhất cho mỗi tác vụ, và thấy rằng các thuật ngữ hàng đầu của chúng có liên quan đến miền của tác vụ (Bảng 10 trong phụ lục). Điều này hỗ trợ giả thuyết của chúng tôi rằng C-BTM có thể tận dụng bất kỳ phần nào của tập dữ liệu nằm trong miền với tác vụ kiểm tra, ngay cả khi tập dữ liệu huấn luyện nói chung có thể đủ ngoài miền để có tác động tiêu cực lên hiệu suất.

Sau đó chúng tôi phản ánh phân tích trong §4.4, bằng cách so sánh các mô hình 16-cụm của chúng tôi với các mô hình dày đặc 6.7B tham số. Đầu tiên, chúng tôi quan sát thấy rằng mô hình 1-cụm 6.7B của chúng tôi vượt trội hơn OPT-6.7B trên tất cả các tác vụ ngoại trừ Twitter, có thể vì mô hình này đã có ít tiếp xúc hơn với C4, và chịu ít catastrophic forgetting hơn. Mô hình 16-cụm của chúng tôi có hiệu suất so sánh với cả hai mô hình 6.7B, và trên nhiều tác vụ, mô hình 16-cụm của chúng tôi vượt trội hơn cả hai mô hình 6.7B, đã được huấn luyện với ít nhất 3.5× tài nguyên tính toán hơn (§4.4). Với suy luận top-4, các mô hình C-BTM kích hoạt thậm chí ít tham số hơn so với các mô hình 6.7B tham số, nhưng vẫn có hiệu suất so sánh.

Những kết quả này chứng thực các phát hiện của chúng tôi trong §4.4 rằng so với các mô hình dày đặc lớn hơn, các mô hình được huấn luyện với C-BTM có hiệu quả huấn luyện cao hơn và độ trễ suy luận thấp hơn, và dẫn đến hiệu suất so sánh hoặc tốt hơn.

Trong các thí nghiệm riêng biệt, chúng tôi quan sát thấy rằng việc định tuyến các ví dụ đến các chuyên gia dựa trên hiệu suất của chúng trên các ví dụ few shot, thay vì các cụm của chúng, dẫn đến hiệu suất tác vụ downstream thậm chí tốt hơn với các mô hình C-BTM. Điều này có thể là do hiệu suất few-shot phụ thuộc vào các yếu tố như thứ tự ví dụ, phân phối nhãn và chất lượng của các minh chứng, không nhất thiết gắn liền với miền của tác vụ (Min et al., 2022; Lu et al., 2022). Chúng tôi phân tích phát hiện này sâu hơn trong §A.7, và để việc phát triển cẩn thận hơn các giao thức định tuyến cho các tác vụ downstream cho công việc tương lai.

### 5.3. Tóm tắt

Chúng tôi chứng minh rằng, phù hợp với các kết quả mô hình hóa ngôn ngữ trong §4.1, C-BTM cải thiện hiệu suất downstream trên nhiều tác vụ phân loại văn bản few-shot. Các mô hình C-BTM nhất quán vượt trội hơn các baseline dày đặc 1-cụm, và thường vượt trội hơn các mô hình OPT gốc, mặc dù được huấn luyện trên một tập dữ liệu ngoài miền. Chúng tôi cũng thấy rằng kích hoạt top-k giảm chi phí suy luận với các tác động không đáng kể lên hiệu suất tác vụ downstream. Các mô hình C-BTM có hiệu suất so sánh với các mô hình baseline dày đặc 6.7B OPT và 1-cụm lớn hơn, mặc dù được huấn luyện với 3.5× ít tài nguyên tính toán hơn, và thậm chí khi kích hoạt ít tham số suy luận hơn.

## 6. So sánh với Mixture-of-Experts

Cuối cùng, chúng tôi so sánh C-BTM với một LM thưa thớt thay thế, một mixture-of-experts (MoE) học một định tuyến giữa các token và các chuyên gia feedforward trong transformer (Lepikhin et al., 2021; Fedus et al., 2021). Như đã thảo luận trong §2.5, C-BTM đơn giản hơn đáng kể so với MoE.

### 6.1. Sparse Upcycling

Để phản ánh việc khởi tạo hạt giống C-BTM, chúng tôi khởi tạo MoE của chúng tôi với một checkpoint dày đặc. Chúng tôi sử dụng kỹ thuật sparse upcycling từ Komatsuzaki et al. (2022). Upcycling một mô hình dày đặc thành một MoE với k chuyên gia bao gồm việc khởi tạo các tham số chia sẻ (ví dụ, các lớp attention và embedding) và k tham số chuyên gia (ví dụ, mọi lớp feedforward khác) từ một checkpoint dày đặc, và khởi tạo các tham số mới cho router. Sau đó mô hình đơn giản được huấn luyện như một MoE. Ở đây, chúng tôi sử dụng định tuyến top-2, token-level (Lepikhin et al., 2021).

### 6.2. Thiết lập Thí nghiệm

**Siêu tham số** Chúng tôi huấn luyện một MoE với sparse upcycling trên C4, bắt đầu từ OPT-1.3B và sử dụng thiết lập thí nghiệm chung được chi tiết trong §3.2. Chúng tôi tuân theo các cài đặt từ Komatsuzaki et al. (2022) càng gần càng tốt. Chúng tôi thực hiện các thí nghiệm với 8, 16, 32, 64, và 128 chuyên gia cho mỗi ngân sách tính toán. 8 và 16 chuyên gia tương tự, nhưng hơi kém hơn, 32 chuyên gia; 64 chuyên gia và 128 chuyên gia nhất quán có các mất mát bùng nổ, và một số ít huấn luyện thành công cũng tương tự nhưng hơi kém hơn 32 chuyên gia. Nói chung, chúng tôi thấy rằng cả số lượng chuyên gia lớn (và ngân sách tính toán cao hơn) dẫn đến tính không ổn định trong việc huấn luyện sparse upcycling.

Chúng tôi sử dụng 32 chuyên gia trong MoE của chúng tôi, một hệ số dung lượng là 2, và tiếp tục huấn luyện mà không đặt lại optimizer từ optimizer được sử dụng trong quá trình pretraining OPT. Chúng tôi đặt tất cả các siêu tham số giống như các mô hình C-BTM của chúng tôi (§3.2), ngoại trừ chúng tôi sử dụng tỷ lệ học đỉnh 2e-5, điều mà chúng tôi thấy là tỷ lệ học cao nhất không dẫn đến phân kỳ sau một lần quét. Chúng tôi phát hành mã nguồn của chúng tôi cho sparse upcycling, được triển khai trong Fairseq (Ott et al., 2019), công khai.

**Baseline** Chúng tôi so sánh MoE LM 32-chuyên gia với các mô hình C-BTM 1-cụm (tức là dày đặc) và 16-cụm.

### 6.3. Kết quả

MoE tiêu tốn nhiều FLOP hơn so với các mô hình khác do lớp feedforward bổ sung ở mọi block transformer khác để định tuyến top-2, cũng như phép chiếu định tuyến (Artetxe et al., 2021). Để rõ ràng và nhất quán, chúng tôi khớp cập nhật và báo cáo riêng chi phí thời gian GPU của các cập nhật, như trong §4.1.

Chúng tôi hiển thị kết quả trong Hình 12. MoE vượt trội đáng kể so với C-BTM với 16 cụm khi ngân sách tính toán tăng. Đáng ngạc nhiên, chúng tôi quan sát thấy rằng với đủ tài nguyên tính toán, MoE thậm chí còn kém hơn LM dày đặc, và khi ngân sách tính toán được tăng thêm, các mất mát nhất quán bùng nổ. Điều này gợi ý rằng sparse upcycling rất không ổn định, có thể do sự dịch chuyển phân phối từ pretraining.

Trong Hình 8, chúng tôi so sánh giây tối đa mỗi lần cập nhật của mô hình MoE với mô hình C-BTM. MoE trở nên chậm hơn đáng kể khi nhiều GPU được sử dụng trong quá trình huấn luyện. Điều này chủ yếu do giao tiếp all-to-all đắt đỏ xảy ra giữa các chuyên gia trong quá trình huấn luyện MoE, cần thiết để định tuyến token đến các chuyên gia (Artetxe et al., 2021). Mặt khác, phương pháp của chúng tôi không có bất kỳ tham số chia sẻ nào giữa các chuyên gia. Ngoài ra, MoE tiêu tốn nhiều FLOP hơn trong quá trình huấn luyện so với các mô hình C-BTM. Cuối cùng, MoE vẫn yêu cầu tính toán đồng bộ để huấn luyện các chuyên gia do các tham số chia sẻ, vì vậy chúng cũng bị ảnh hưởng bởi các khó khăn thực tế của việc huấn luyện các mô hình ngôn ngữ dày đặc ở quy mô §4.2.

### 6.4. Tóm tắt

Kết quả của chúng tôi gợi ý rằng các mô hình ngôn ngữ được huấn luyện với C-BTM vượt trội đáng kể so với các MoE được huấn luyện với cùng ngân sách. Các cải tiến hiệu suất của kỹ thuật chúng tôi có thể là kết quả của sự đơn giản của định tuyến xác định của chúng tôi (dựa trên các cụm được dẫn xuất thực nghiệm), tính không ổn định liên quan đến sparse upcycling, và các yếu tố khác.

## 7. Phân tích

Trong §4, §5, và §6, chúng tôi chứng minh rằng C-BTM vượt trội hơn các baseline được huấn luyện dày đặc và MoE được khớp tài nguyên tính toán. Bây giờ chúng tôi nghiên cứu cách tiếp cận phân cụm của chúng tôi chi tiết hơn và mô tả ảnh hưởng của nó lên hiệu suất tổng thể của C-BTM.

### 7.1. Phân cụm có quan trọng không?

Để đánh giá tầm quan trọng của thuật toán phân cụm, chúng tôi thực hiện C-BTM như trên, ngoại trừ chúng tôi gán mỗi tài liệu cho một cụm ngẫu nhiên, thay vì một cụm đã học. Điều này tương đương với baseline tập hợp ngẫu nhiên từ Li et al. (2022). Kết quả trong Hình 13 chứng minh rằng việc sử dụng các cụm ngẫu nhiên kém hơn đáng kể so với cả phương pháp của chúng tôi và baseline dày đặc. Do đó, chuyên biệt hóa cụm rất quan trọng đối với C-BTM. Điều này xác nhận kết quả từ Li et al. (2022), những người thấy rằng chuyên biệt hóa miền của các ELM rất quan trọng cho hiệu suất của tập hợp, cũng như những người từ Jang et al. (2023), những người chỉ ra rằng các ELM chuyên biệt hóa hướng dẫn chuyển giao sang các tác vụ khác với các hướng dẫn tương tự.

### 7.2. Việc cân bằng các cụm có quan trọng không?

Áp dụng một ràng buộc cân bằng trong k-means tránh kết quả thoái hóa của một đuôi dài trong kích thước cụm (Chang et al. 2014). Thực sự, với 10K tài liệu của dữ liệu validation giữ lại trong C4, chúng tôi quan sát thấy rằng phân cụm cân bằng tăng đáng kể kích thước cụm trung vị, và thu hẹp phạm vi của nó, tương đối với một baseline không cân bằng. Để đánh giá ảnh hưởng của việc cân bằng kích thước cụm lên hiệu suất của C-BTM, chúng tôi thực hiện C-BTM với một mô hình phân cụm k-means nhưng loại bỏ ràng buộc cân bằng. Đối với mô hình 8-cụm, chúng tôi quan sát thấy rằng cân bằng có ít ảnh hưởng. Tuy nhiên, đối với mô hình 32-cụm (Hình 14), phân cụm không cân bằng nhất quán dẫn đến hiệu suất kém hơn. Những kết quả này gợi ý rằng cân bằng trở nên quan trọng hơn khi tăng số lượng cụm. Điều này phù hợp với các thí nghiệm riêng biệt cho thấy đuôi dài trong kích thước cụm trở thành một vấn đề nhất quán hơn với số lượng cụm cao hơn.

### 7.3. Các cụm có được định nghĩa rõ ràng không? Các chuyên gia có chuyên biệt hóa không?

Vì chúng tôi sử dụng tf-idf như embedder tài liệu của chúng tôi trong C-BTM, chúng tôi có thể thực hiện một phép biến đổi ngược từ các trung tâm cụm vào không gian từ vựng để xác định các thuật ngữ có khả năng cao nhất sẽ được embed thành trung tâm cụm. Chúng tôi hiển thị năm thuật ngữ hàng đầu mỗi cụm trong §A.1. Chúng tôi quan sát thấy rằng khi số lượng cụm tăng, các thuật ngữ hàng đầu trên các cụm trở nên cụ thể và đa dạng hơn.

Tiếp theo, chúng tôi nghiên cứu liệu các ELM được huấn luyện trên các cụm này có chuyên biệt hóa không. Sử dụng mô hình 32-cụm được huấn luyện trên 84B token của C4, chúng tôi tính perplexity của tất cả các chuyên gia trên 200 tài liệu giữ lại trong mỗi cụm. Đối với mỗi cụm, chúng tôi sau đó đo tỷ lệ perplexity của mỗi chuyên gia với perplexity của chuyên gia được huấn luyện trên cụm đó. Chúng tôi hiển thị những tỷ lệ đó trong Hình 15. Chúng tôi thấy rằng tất cả các chuyên gia đều có hiệu suất tốt nhất trên cụm của riêng mình. Một số chuyên gia không chuyển giao tốt chút nào sang các cụm khác, trong khi những chuyên gia khác làm khá tốt. Tham chiếu chéo với các bảng thuật ngữ cụm trong §A.1, chúng tôi thấy rằng các chuyên gia cụm 3 và 5 có xu hướng khái quát hóa tốt và các thuật ngữ hàng đầu trong các cụm này tổng quát hơn (với các từ như "just,like,love"). Các chuyên gia chuyên biệt cho nội dung như "site, page, website" (cụm 0) và "app, phone, video" (cụm 29), có xu hướng làm kém trên tất cả các cụm khác. Những kết quả này gợi ý rằng các chuyên gia chuyên biệt hóa cho cụm của chúng. Chúng tôi suy luận rằng sự thành công của suy luận C-BTM thưa thớt là kết quả của chuyên biệt hóa chuyên gia, và các cải tiến hiệu suất C-BTM có thể một phần do hiệu quả mẫu của việc huấn luyện chuyên biệt hóa.

### 7.4. Các cụm và miền metadata so sánh như thế nào?

Động lực chính cho C-BTM là loại bỏ sự phụ thuộc vào metadata để mô tả các miền mà các ELM chuyên biệt hóa. Các cụm phản ánh tốt như thế nào việc phân đoạn tập dữ liệu được tạo ra bởi metadata? Chúng tôi sử dụng S2ORC để nghiên cứu câu hỏi này. Đầu tiên, chúng tôi căn chỉnh các cụm đã học từ một mô hình 32-cụm với metadata lĩnh vực-nghiên cứu có sẵn từ S2ORC (Lo et al., 2019). Sau đó chúng tôi hình ảnh hóa sự chồng chéo giữa metadata và các cụm (Hình 16). Chúng tôi quan sát chỉ một sự căn chỉnh một phần giữa metadata và các cụm trong S2ORC. Các tài liệu với một số nhãn metadata (ví dụ, Khoa học Môi trường, Khoa học Chính trị) chủ yếu được gán cho các cụm riêng của chúng, trong khi các tài liệu với các nhãn khác (ví dụ, Khoa học Máy tính, Vật lý) được phân bổ trên nhiều cụm.

Sự căn chỉnh một phần giữa metadata và các cụm gợi ý rằng các mô hình C-BTM có thể không có hiệu suất tương tự như những mô hình được huấn luyện với nhãn metadata để mô tả các miền. Để điều tra giả thuyết này sâu hơn, chúng tôi thực hiện các thí nghiệm sử dụng một tập con của the Pile (Gao et al., 2021) để so sánh hiệu suất của các chuyên gia được huấn luyện với metadata và các chuyên gia được huấn luyện với các cụm. Xem §A.5 để biết thêm chi tiết về tập dữ liệu này. Chúng tôi quan sát thấy rằng các chuyên gia được huấn luyện với các cụm đã học có hiệu suất hơi tốt hơn so với những chuyên gia với nhãn metadata trên dữ liệu validation giữ lại (Bảng 8 trong phụ lục). Cả hai kỹ thuật đều có hiệu suất tốt hơn so với việc huấn luyện chỉ với một cụm duy nhất trên the Pile, xác nhận kết quả của chúng tôi từ §4.1. Những kết quả này ngụ ý rằng metadata có thể không tương ứng với việc phân đoạn tối ưu nhất của tập dữ liệu. Tuy nhiên, việc sử dụng metadata có lợi thế về khả năng giải thích và đơn giản, và metadata có thể xác định các miền không chỉ dựa trên từ vựng (ví dụ, Lucy & Bamman, 2021; Gururangan et al., 2022). Công việc tương lai có thể khám phá việc kết hợp các ELM chuyên biệt hóa metadata- và cụm.

### 7.5. Tóm tắt

Phân tích của chúng tôi chứng minh rằng các cải tiến từ C-BTM không phải là kết quả của việc kết hợp một mình. Các thành phần khác nhau của phương pháp huấn luyện của chúng tôi, đặc biệt là bản chất của các cụm đã học, đóng vai trò quan trọng trong hiệu suất C-BTM. Cải thiện việc biểu diễn các miền trong một tập dữ liệu, có thể sử dụng các biểu diễn đã được huấn luyện trước khác (Aharoni & Goldberg, 2020) hoặc các thuật toán phân cụm phức tạp hơn (Ester et al., 1996; Chronopoulou et al., 2022), có thể cải thiện hiệu suất C-BTM.

## 8. Công việc Liên quan

**Các mô hình thưa thớt** C-BTM có liên quan chặt chẽ đến các mô hình thưa thớt chỉ kích hoạt một tập con của các tham số (Evci et al., 2020; Mostafa & Wang, 2019; Dettmers & Zettlemoyer, 2019). C-BTM được lấy cảm hứng từ MoE, nhưng đơn giản và hiệu quả hơn nhiều để huấn luyện. Hầu hết các phương pháp MoE dựa vào việc huấn luyện các cơ chế định tuyến dựa trên token (Lepikhin et al., 2021; Fedus et al., 2021; Lewis et al., 2021; Roller et al., 2021), nhưng những phương pháp khác dựa vào định tuyến tác vụ (Kudugunta et al., 2021) hoặc miền (Gururangan et al., 2022).

**Các mô hình ngôn ngữ chuyên gia** Như chúng tôi lưu ý trong suốt nghiên cứu, công việc này có liên quan trực tiếp nhất đến BTM (Li et al., 2022). BTM đến lượt nó một phần được lấy cảm hứng từ công việc trước về các biến thể của các mô hình MoE (Jacobs et al., 1991), nhưng đặc biệt là các lớp DEMix (Gururangan et al., 2022), thay thế các lớp feedforward transformer bằng các chuyên gia miền được định nghĩa bởi metadata. Jang et al. (2023) huấn luyện các mô hình ngôn ngữ chuyên gia trên các tác vụ dựa trên hướng dẫn, trong khi Pfeiffer et al. (2022) huấn luyện các mô hình ngôn ngữ chuyên gia trên các ngôn ngữ khác nhau.

**Định tuyến cụm** Chronopoulou et al. (2022) và Chronopoulou et al. (2023) sử dụng phân cụm phân cấp để xác định các miền để chuyên biệt hóa các chuyên gia adapter, và sử dụng các adapter trong một tập hợp hoặc trung bình tham số tại thời điểm suy luận. Duan et al. (2021) xây dựng các tập hợp của các mô hình cụ thể tác vụ bằng cách phân cụm dữ liệu huấn luyện của các tác vụ được giám sát. Gross et al. (2017) sử dụng một router dựa trên cụm tương tự như của chúng tôi trong một cài đặt phân loại hình ảnh sử dụng ResNet. Tuy nhiên, họ sử dụng định tuyến cứng (hoặc chỉ kích hoạt một chuyên gia duy nhất) trong cả huấn luyện và suy luận, trong khi chúng tôi sử dụng định tuyến cứng trong quá trình huấn luyện nhưng kết hợp các chuyên gia trong quá trình suy luận. Kỹ thuật suy luận của chúng tôi được lấy cảm hứng từ các cơ chế truy xuất nearest neighbor trong các mô hình ngôn ngữ (Khandelwal et al., 2019; Shi et al., 2022).

**Huấn luyện hiệu quả giao tiếp** Nghiên cứu của chúng tôi đóng góp vào một dòng nghiên cứu về các thuật toán hiệu quả giao tiếp để huấn luyện các mô hình lớn. Một số công việc trước đề xuất các cách để huấn luyện các mô hình dày đặc lớn một cách cộng tác trên các mạng phân tán của máy chủ (Borzunov et al., 2022; Yuan et al., 2022). Các công việc khác tập trung vào các hình thức mới của dữ liệu (Gan et al., 2021), mô hình (Ryabinin et al., 2023), và song song pipeline (Wang et al., 2022) để cho phép huấn luyện mô hình trên các thiết bị không đồng nhất có thể phục hồi từ lỗi nút. Wortsman et al. (2022) đề xuất một phương pháp hiệu quả giao tiếp của việc fine-tuning bằng cách huấn luyện một tập hợp các mô hình với các siêu tham số khác nhau trên các nút GPU riêng lẻ, và sau đó tính trung bình các tham số của chúng sau khi huấn luyện. Công việc của chúng tôi sử dụng chuyên biệt hóa chuyên gia cho việc huấn luyện hiệu quả giao tiếp, và C-BTM có thể được kết hợp với bất kỳ kỹ thuật nào trong số này để cải thiện hiệu quả huấn luyện.

## 9. Kết luận

Chúng tôi giới thiệu C-BTM, một kỹ thuật mới để huấn luyện hiệu quả các LM thưa thớt. C-BTM chia một tập dữ liệu thành k cụm, huấn luyện một LM chuyên gia trên mỗi cụm, và tạo ra một tập hợp thưa thớt trong quá trình suy luận. Chúng tôi quan sát thấy rằng số lượng cụm tối ưu cho C-BTM tăng theo lượng dữ liệu, và việc sử dụng nhiều cụm hơn cũng cho phép chúng tôi song song hóa việc huấn luyện một cách tích cực để mở rộng quy mô hiệu quả vào các tập dữ liệu khổng lồ. Công việc tương lai có thể điều tra C-BTM trong các cài đặt đa tác vụ hoặc đa ngôn ngữ, tính hữu ích của nhiều lần lặp của C-BTM trên một tập dữ liệu (có thể với phân cụm phân cấp), hoặc khả năng kết hợp định tuyến dựa trên metadata và cụm để mở rộng quy mô vào nhiều tập dữ liệu không đồng nhất song song.

## Lời cảm ơn

Bài báo này được hưởng lợi từ phản hồi chu đáo của một số người: Armen Aghajanyan, Tim Dettmers, Sneha Kudugunta, Stephen Roller, Swabha Swayamdipta, và Mitchell Wortsman.
