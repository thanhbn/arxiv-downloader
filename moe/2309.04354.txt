# 2309.04354.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2309.04354.pdf
# File size: 1394365 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Mobile V-MoEs: Scaling Down Vision Transformers
via Sparse Mixture-of-Experts
Erik Daxberger* Floris Weers Bowen Zhang Tom Gunter Ruoming Pang
Marcin Eichner Michael Emmersberger Yinfei Yang Alexander Toshev Xianzhi Du
Apple
Abstract
Sparse Mixture-of-Experts models (MoEs) have recently
gained popularity due to their ability to decouple model
size from inference efficiency by only activating a small sub-
set of the model parameters for any given input token. As
such, sparse MoEs have enabled unprecedented scalability,
resulting in tremendous successes across domains such as
natural language processing and computer vision. In this
work, we instead explore the use of sparse MoEs to scale-
down Vision Transformers (ViTs) to make them more attrac-
tive for resource-constrained vision applications. To this
end, we propose a simplified and mobile-friendly MoE de-
sign where entire images rather than individual patches are
routed to the experts. We also propose a stable MoE train-
ing procedure that uses super-class information to guide the
router. We empirically show that our sparse Mobile Vision
MoEs (V-MoEs) can achieve a better trade-off between per-
formance and efficiency than the corresponding dense ViTs.
For example, for the ViT-Tiny model, our Mobile V-MoE
outperforms its dense counterpart by 3.39% on ImageNet-
1k. For an even smaller ViT variant with only 54M FLOPs
inference cost, our MoE achieves an improvement of 4.66%.
1. Introduction
The trade-off between performance and efficiency of
neural networks (NNs) remains a challenge, especially in
settings where computational resources are limited. Re-
cently, sparsely-gated Mixture-of-Experts models (sparse
MoEs) have gained popularity as they provide a promis-
ing solution to this problem by enabling the decoupling of
model size from inference efficiency [3]. MoEs are NNs
that are partitioned into “experts”, which are trained jointly
with a router to specialize on subsets of the data. In MoEs,
each input is processed by only a small subset of model
parameters (aka conditional computation ). In contrast, tra-
ditional dense models activate all parameters for each input.
*Correspondence to: Erik Daxberger, edaxberger@apple.com .
50M 100M 500M 1B
FLOPs354045505560657075T op-1 Validation Accuracy on ImageNet12x384
6x384
12x192
(ViT-Tiny)
6x192
6x96
6x64Mobile V-MoE (ours)
Dense ViTFigure 1. Accuracy vs. FLOPs for ViTs of different sizes. La-
bels (e.g. 12 ×192, which is ViT-Tiny) refer to the number of ViT
layers (e.g. 12) and the hidden embedding dimension (e.g. 192).
The sparse MoEs outperform their corresponding dense baselines
across different model scales. Fig. 3a lists all numerical results.
Sparse MoEs were popularized in deep learning by [16],
which introduced sparse MoE-layers as drop-in replace-
ments for standard NN layers. Most recent MoEs are based
on the Transformer [19], which processes individual input
tokens; in accordance, recent MoEs also route individual in-
put tokens to experts, i.e., image patches in the case of Vi-
sion Transformers (ViTs) [2, 13] (see Fig. 2b). Conditional
computation as implemented by sparse MoEs has enabled
the training of Transformers of unprecedented size [4]. As
a result, MoEs have achieved impressive successes across
various domains including language [4, 10], vision [13],
speech [20] and multi-modal learning [12], and currently
hold state-of-the-art results on many benchmarks [21].
The ability to increase model capacity while keeping in-
ference cost low is also appealing for resource-constrained
vision problems. While Transformers are getting increas-
ingly established as the de-facto standard architecture for
large-scale visual modeling [2, 13], virtually all mobile-
1arXiv:2309.04354v1  [cs.CV]  8 Sep 2023

--- PAGE 2 ---
(a) Dense ViT (b) Regular V-MoE (c) Mobile V-MoE (d) Layer types
Figure 2. Model architectures. (a) The dense ViT baseline model uses dense ViT layers throughout. (b) Regular sparse V-MoE with
layer-wise per-patch routers. (c) Our proposed sparse Mobile V-MoE design with a single per-image router. In both (b) and (c), dense ViT
layers are followed by MoE-ViT layers (here, k= 1out of E= 3experts are activated per input). (d) In contrast to dense ViT layers [19],
MoE-ViT layers have a separate MLP per expert (preceded by a router) while all other parts of the layer are shared across all experts [13].
friendly models still leverage convolutions due to their effi-
ciency [1,5,6,11,15,18]. As such, conditional computation
could potentially enable attention-based models to reduce
the gap to convolutional models in the small-scale regime.
However, Transformer-based MoEs have not yet been ex-
plored for resource-constrained settings; this might be due
to two main weaknesses of recently-popularized MoEs [16].
Firstly, while per-token routing increases the flexibility
to learn an optimal computation path through the model, it
makes inference inefficient, as many (or even all) experts
need to be loaded for a single input image. Secondly, recent
MoEs train the routers jointly with the rest or the model in
an end-to-end fashion. To avoid collapse to just a few ex-
perts while ignoring all others, one needs to use load balanc-
ing mechanisms [3] such as dedicated auxiliary losses [16].
However, the resulting complex optimization objectives of-
ten lead to training instabilities / divergence [4, 10, 12, 21].
In this work, we investigate the potential of sparse MoEs
to scale-down ViTs for resource-constrained vision appli-
cations via an MoE design and training procedure that ad-
dresses the aforementioned issues. Our contributions are:
1. We propose a simplified, mobile-friendly sparse MoE
design in which a single router assigns entire images(rather than image patches) to the experts (see Fig. 2c).
2. We develop a simple yet robust training procedure in
which expert imbalance is avoided by leveraging se-
mantic super-classes to guide the router training.
3. We empirically show that our proposed sparse MoE
approach allows us to scale-down ViT models by im-
proving their performance vs. efficiency trade-off.
2. Scaling down ViTs via sparse MoEs
2.1. Conditional computation with sparse MoEs
An MoE implements conditional computation by activat-
ing different subsets of a NN (so-called experts) for differ-
ent inputs. We consider an MoE layer with Eexperts as
MoE (x) =EX
i=1g(x)iei(x), (1)
where x∈RDis the input to the layer, ei:RD→RD
is the function computed by expert i, and g:RD→RE
is the routing function which computes an input-dependent
weight for each expert [16]. In a ViT-based MoE, each ex-
perteiis parameterized by a separate multi-layer perceptron
2

--- PAGE 3 ---
(MLP) within the ViT layer, while the other parts are shared
across experts (see Fig. 2d). We use the routing function
g(x) =TOPk(softmax (Wx )), (2)
where the operation TOP k(x)sets all elements of xto zero
except those with the klargest values [13]. In a sparse MoE,
we have k≪E, s.t. we only need to load and compute the
kexperts with the largest routing weights. This allows us
to scale-up the overall model capacity (determined by E)
without increasing the inference cost (determined by k).
2.2. Efficient and robust MoEs for small-scale ViTs
Per-image routing. Recent large-scale sparse MoEs use
per-patch routing (i.e. the inputs xare individual image
patches). This generally requires a larger number of experts
to be activated for each image. For example, [13] show that
in their MoE with per-patch routing, “most images use –on
aggregate by pooling over all their patches– most of the ex-
perts” [13, Appendix E.3]. Thus, per-patch routing can in-
crease the computational and memory overhead of the rout-
ing mechanism and reduce the overall model efficiency. We
instead propose to use per-image routing (i.e., the inputs x
are entire images) to reduce the number of activated experts
per image, as also done in early works on MoEs [7, 9].
Super-class-based routing. Previous works on sparse
MoEs jointly train the router end-to-end together with the
experts and the dense ViT backbone, to allow the model to
learn the optimal assignment from inputs to experts based
on the data [13]. While learning the optimal routing mecha-
nism from scratch can result in improved performance, it of-
ten leads to training instabilities and expert collapse, where
most inputs are routed to only a small subset of the experts,
while all other experts get neglected during training [3].
Thus, an additional auxiliary loss is typically required to
ensure load-balancing between the different experts, which
can increase the complexity of the training process [3].
In contrast, we propose to group the classes of the dataset
into super-classes and explictly train the router to make each
expert specialize on one super-class. To this end, we add
an additional cross-entropy loss Lgbetween the router out-
putg(x)in Eq. (2) and the ground truth super-class labels
to the regular classification loss LCto obtain the overall
weighted loss L=LC+λLg(we use λ= 0.3in our exper-
iments, which we found to work well). Such a super-class
division is often readily provided with the dataset (e.g. for
CIFAR-10/100 or MS-COCO). If a dataset does not come
with a super-class division, we can easily obtain one as fol-
lows: 1) we first train a dense baseline model on the dataset;
2) we then compute the model’s confusion matrix over a
held-out validation set; 3) we finally construct a confusion
graph from the confusion matrix and apply a graph cluster-
ing algorithm to obtain the super-class division [8]. This ap-
proach encourages the super-classes to contain semanticallyID Classes Super-class
0 boxer, pug, Rottweiler dogs
1 orangutan, weasel, panda other mammals
2 toucan, flamingo, ostrich birds
3 eel, scorpion, hammerhead other animals
4 minivan, ambulance, taxi land vehicles
5 submarine, canoe, pirate sea vehicles
6 guacamole, hotdog, banana food
7 backpack, pyjama, kimono clothes
8 monitor, iPod, photocopier tech devices
9 xylophone, harp, trumpet instruments
Table 1. Super-class division for E= 10 . For each super-class,
we list three randomly chosen class names (which turn out to be
semantically related) together with a possible super-class name.
similar images that the model often confuses. Intuitively,
by allowing the different MoE experts to specialize on the
different semantic data clusters, performance on the highly-
confused classes should be improved. We use this approach
in our experiments on ImageNet-1k, computing the con-
fusion matrix via a dense ViT-S/16 model. The resulting
super-class division for E= 10 experts is shown in Tab. 1;
the super-classes contain semantically related classes.
3. Experiments
We now present empirical results on the standard
ImageNet-1k classification benchmark [14]. We train all
models from scratch on the ImageNet-1k training set of
1.28M images, and then evaluate their top-1 accuracy on
the held-out validation set of 50K images. In Sec. 3.1, we
first evaluate our proposed sparse Mobile V-MoE across a
range of model scales and show that they achieve better per-
formance vs. efficiency trade-offs than the respective dense
ViT baselines. In Sec. 3.2, we then conduct several ablation
studies to get a better understanding of the properties of our
proposed sparse MoE model design and training procedure.
3.1. Accuracy vs. efficiency across ViT scales
We consider ViT models (both MoEs and corresponding
dense baselines) of different sizes by scaling the total num-
ber of layers (we use 12, 9 or 6) and the hidden embedding
size (we use 384, 192, 96 or 64). The number of multi-head
self-attention heads is (6, 3, 3, 2) for the different hidden
embedding sizes. The embedding size of the MLP is 4×
the hidden embedding size, as is common practice. We use
E= 10 experts in total for the MoE, out of which k= 1
is activated per input image. Our MoEs comprise of L= 2
MoE-ViT layers preceded by (10, 7 or 4) dense ViT layers
(see Fig. 2c). We use a patch size of 32×32for all mod-
els. This is because the the patch size effectively controls
3

--- PAGE 4 ---
Model FLOPs Top-1 Accuracy
Dense MoE ∆
12×384 2297M 71.88 74.23 +2.35
9×384 1752M 69.94 72.47 +2.53
6×384 1207M 63.21 66.91 +3.70
12×1921618M 59.51 62.90 +3.39
9×192 478M 56.50 59.52 +3.02
6×192 338M 51.18 55.69 +4.51
12×96 176M 53.79 55.39 +1.60
9×96 140M 51.27 52.99 +1.72
6×96 103M 46.54 50.28 +3.74
12×64 88M 42.90 46.07 +3.17
9×64 71M 40.46 43.95 +3.49
6×64 54M 36.64 41.30 +4.66
(a)Accuracy vs. efficiency across ViT scales.E Router MoE ∆
5 86.43 72.33 +1.64
7 87.43 73.13 +2.44
10 87.12 73.52 +2.83
15 84.16 73.10 +2.41
20 84.08 73.36 +2.67
(b)Total number of experts E.
L Router MoE ∆
1 90.17 72.14 +1.45
2 87.12 73.52 +2.83
4 82.12 71.67 +0.98
6 77.70 70.07 -0.62
8 72.09 64.47 -6.22
(c)Number of MoE layers L.k FLOPs Dense MoE ∆
1 2534M 70.79 73.44 +2.65
2 2769M 71.70 74.42 +2.72
3 3005M 73.58 74.44 +0.86
5 3476M 74.87 74.32 -0.55
10 4653M 75.10 74.37 -0.73
(d)Number of experts kper image .
Routing Input Acc. ∆
Dense N/A 71.70
Super-class image 74.42 +2.72
Rand. class image 69.22 -2.48
End-to-end image 73.57 +1.87
End-to-end token 74.85 +3.15
(e)Routing strategies .
Figure 3. Empirical results. (a) Our Mobile V-MoEs outperform the respective dense ViTs across model scales. Model names
(e.g. 12 ×192) refer to the number of layers (12) and the embedding size (192). (b-e) Ablation studies using DeiT-Ti/16 [17], with k= 1,
E= 10 ,L= 2by default. Best performance vs. efficiency trade-off is achieved with (b) E= 10 experts total, (c) L= 2MoE layers (out
of 12 layers total), (d) k= 1ork= 2experts activated per image, (e) our semantic super-class routing; the settings used in (a) are bolded.
the trade-off between FLOPs and number of model param-
eters: as we aim to optimize for FLOPs, a larger patch size
(resulting in a fewer number of patches) is beneficial. We
also tried using a smaller patch size of 16×16, where the
result trends were basically the same (but where the num-
ber of FLOPs was higher relative to the model capacity and
thus accuracy). For the ViTs with hidden sizes 384 and 192,
we use the DeiT training recipe [17], while for hidden sizes
96 and 64, we use the standard ViT training recipe [2] to
avoid underfitting. Figs. 1 and 3a compare top-1 validation
accuracy vs. FLOPs. Our Mobile V-MoEs outperform the
corresponding dense ViT baselines across all model sizes.
3.2. Ablation studies
We train DeiT-Tiny [17] (12 layers total, 192 embedding
size, 16×16patch size) with k= 1out of E= 10 experts
per input, and with L= 2 MoE layers (unless noted other-
wise); the dense ViT baseline achieves 70.79% accuracy.
Total number of experts E.We consider different
widths of the MoE, i.e., different numbers of experts E(and
thus super-classes), ranging between E= 5 andE= 20 .
We report both the accuracy of the entire MoE model (i.e.,
on the 1,000-way classification task), as well as the accu-
racy of the router (i.e., on the E-way super-classification
task). Fig. 3b shows that overall performance improves until
E= 10 , from which point onwards it stagnates. The router
accuracy also drops beyond E= 10 due to the increased
difficulty of the E-way super-classification problem.
Number of MoE layers L.We consider different depths
of the MoE, i.e., different numbers of MoE layers L, rang-
1This corresponds to the ViT-Tiny model [17] with patch size 32×32.ing between L= 1 andL= 8 (out of 12 ViT layers in
total). We again report both the full MoE and router ac-
curacies. Fig. 3c shows that overall performance peaks at
L= 2, and rapidly decreases for larger L. This is due to
the router accuracy, which declines with increasing Las the
router gets less information (from the 12−LViT layers).
Number of experts kper image. We vary the number
of experts kactivated per image. We compare against dense
baselines that use an MLP with hidden dimension scaled
bykto match the MoE’s inference FLOPs. Fig. 3d shows
thatk= 1 andk= 2 perform best (relative to the dense
baseline), with decreasing performance delta for larger k.
Routing strategies. We compare our proposed semantic
super-class per-image routing vs. end-to-end-learned rout-
ing (both per-image and per-token) and a baseline with ran-
dom super-classes (for k=2). Fig. 3e shows that our method
(Fig. 2c) is better, except for learned per-token routing (as
in the regular V-MoE [13], Fig. 2b), which however needs
to activate many more experts and thus model parameters
for each input image (up to 11.05M, vs. 6.31M for ours).
4. Conclusions and future work
We showed that sparse MoEs can improve the perfor-
mance vs. efficiency trade-off compared to dense ViTs,
in an attempt to make ViTs more amenable to resource-
constrained applications. In the future, we aim to apply our
MoE design to models that are more mobile-friendly than
ViTs, e.g., light-weight CNNs such as MobileNets [5,6,15]
or ViT-CNN hybrids [1, 11, 18]. We also aim to consider
other vision tasks, e.g., object detection. Finally, we aim to
get actual on-device latency measurements for all models.
4

--- PAGE 5 ---
References
[1] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen
Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-
former: Bridging mobilenet and transformer. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 5270–5279, 2022. 2, 4
[2] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. arXiv preprint
arXiv:2010.11929 , 2020. 1, 4
[3] William Fedus, Jeff Dean, and Barret Zoph. A review
of sparse expert models in deep learning. arXiv preprint
arXiv:2209.01667 , 2022. 1, 2, 3
[4] William Fedus, Barret Zoph, and Noam Shazeer. Switch
transformers: Scaling to trillion parameter models with sim-
ple and efficient sparsity. J. Mach. Learn. Res , 23:1–40,
2021. 1, 2
[5] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh
Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,
Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-
bilenetv3. In Proceedings of the IEEE/CVF international
conference on computer vision , pages 1314–1324, 2019. 2,
4
[6] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-
tional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 , 2017. 2, 4
[7] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and
Geoffrey E Hinton. Adaptive mixtures of local experts. Neu-
ral computation , 3(1):79–87, 1991. 3
[8] Ruochun Jin, Yong Dou, Yueqing Wang, and Xin Niu. Con-
fusion graph: Detecting confusion communities in large
scale image classification. In IJCAI , pages 1980–1986, 2017.
3
[9] Michael I Jordan and Robert A Jacobs. Hierarchical mix-
tures of experts and the em algorithm. Neural computation ,
6(2):181–214, 1994. 3
[10] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
Shazeer, and Zhifeng Chen. Gshard: Scaling giant models
with conditional computation and automatic sharding. arXiv
preprint arXiv:2006.16668 , 2020. 1, 2
[11] Sachin Mehta and Mohammad Rastegari. Mobilevit: light-
weight, general-purpose, and mobile-friendly vision trans-
former. arXiv preprint arXiv:2110.02178 , 2021. 2, 4
[12] Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe
Jenatton, and Neil Houlsby. Multimodal contrastive learning
with limoe: the language-image mixture of experts. arXiv
preprint arXiv:2206.02770 , 2022. 1, 2
[13] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim
Neumann, Rodolphe Jenatton, Andr ´e Susano Pinto, Daniel
Keysers, and Neil Houlsby. Scaling vision with sparse mix-
ture of experts. Advances in Neural Information Processing
Systems , 34:8583–8595, 2021. 1, 2, 3, 4[14] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al. Imagenet large
scale visual recognition challenge. International journal of
computer vision , 115:211–252, 2015. 3
[15] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-
moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted
residuals and linear bottlenecks. In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 4510–4520, 2018. 2, 4
[16] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy
Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra-
geously large neural networks: The sparsely-gated mixture-
of-experts layer. arXiv preprint arXiv:1701.06538 , 2017. 1,
2
[17] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training
data-efficient image transformers & distillation through at-
tention. In International conference on machine learning ,
pages 10347–10357. PMLR, 2021. 4
[18] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, On-
cel Tuzel, and Anurag Ranjan. An improved one millisecond
mobile backbone. arXiv preprint arXiv:2206.04040 , 2022. 2,
4
[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems , 30, 2017. 1, 2
[20] Zhao You, Shulin Feng, Dan Su, and Dong Yu. Speechmoe:
Scaling to large acoustic models with dynamic routing mix-
ture of experts. arXiv preprint arXiv:2105.03036 , 2021. 1
[21] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yan-
ping Huang, Jeff Dean, Noam Shazeer, and William Fedus.
Designing effective sparse expert models. arXiv preprint
arXiv:2202.08906 , 2022. 1, 2
5
