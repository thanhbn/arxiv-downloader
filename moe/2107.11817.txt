# 2107.11817.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2107.11817.pdf
# File size: 2378435 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Go Wider Instead of Deeper
Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu, Yang You
Department of Computer Science, National University of Singapore, Singapore
ff.xue,ziji.shig@u.nus.edu, weifutao2019@gmail.com, yuxuanlou@u.nus.edu, fliuyong,youyg@comp.nus.edu.sg
Abstract
More transformer blocks with residual connections have
recently achieved impressive results on various tasks. To
achieve better performance with fewer trainable parameters,
recent methods are proposed to go shallower by parameter
sharing or model compressing along with the depth. How-
ever, weak modeling capacity limits their performance. Con-
trastively, going wider by inducing more trainable matrixes
and parameters would produce a huge model requiring ad-
vanced parallelism to train and inference.
In this paper, we propose a parameter-efﬁcient framework,
going wider instead of deeper. Specially, following exist-
ing works, we adapt parameter sharing to compress along
depth. But, such deployment would limit the performance.
To maximize modeling capacity, we scale along model width
by replacing feed-forward network (FFN) with mixture-of-
experts (MoE). Across transformer blocks, instead of shar-
ing normalization layers, we propose to use individual lay-
ernorms to transform various semantic representations in a
more parameter-efﬁcient way. To evaluate our plug-and-run
framework, we design WideNet and conduct comprehensive
experiments on popular computer vision and natural language
processing benchmarks. On ImageNet-1K, our best model
outperforms Vision Transformer (ViT) by 1:5%with 0:72
trainable parameters. Using 0:46and0:13parameters,
our WideNet can still surpass ViT and ViT-MoE by 0:8%
and2:1%, respectively. On four natural language processing
datasets, WideNet outperforms ALBERT by 1:8%on average
and surpass BERT using factorized embedding parameteriza-
tion by 0:8%with fewer parameters.1
Introduction
Transformer-based models have achieved promising results
on various tasks ( e.g., Q&A (Qu et al. 2019; Yang et al.
2020), relation extraction (Xue et al. 2020b,a; Zhou et al.
2020)). To further improve the effectiveness and efﬁciency
of the transformer, there are two trains of thought to de-
ploy trainable parameters. The ﬁrst thought is to scale
transformer along width to more trainable parameters ( e.g.,
Switch Transformer (Fedus, Zoph, and Shazeer 2021), ViT-
MoE (Riquelme et al. 2021)). These sparse models can scale
to extremely large models with comparable FLOPs by sparse
conditional computation. Another thought is to decrease the
1We will release our code upon acceptance.trainable parameters for a lite model. To this end, some
works propose to reuse the trainable parameters across trans-
former blocks ( e.g., Universal Transformer (Dehghani et al.
2018) and ALBERT (Lan et al. 2019)). Model compres-
sion (Xu et al. 2020; Sun et al. 2019) can also make trans-
former more parameter efﬁcient.
The two existing methods both have their own limitations.
For huge models, one typical and effective method to scale
trainable parameters is replacing part of the feed-forward
network (FFN) layer in transformer blocks with MoE lay-
ers. In each MoE layer, to reﬁne one single token represen-
tation, only a few experts are activated, so the MoE based
transformer holds comparable FLOPs with the vanilla trans-
former. However, during training and inference, we are re-
quired to use advanced parallelisms ( e.g., tensor (Shoeybi
et al. 2019), sequence (Li et al. 2021), pipeline (Huang et al.
2018) and expert parallelism (Lepikhin et al. 2020)) to hold
these models on TPU or GPU. Also, the performance cannot
improve linearly during scaling. Another limitation is that
the sparseness of MoE based models cannot scale well on
relatively small datasets. We will discuss the reason for this
phenomenon in the following sections. For small models, al-
though they can reduce trainable parameters signiﬁcantly by
going shallower, the performance of these shallower models
is still under the original transformers. These smaller mod-
els are constructed by compressing the original model along
with depth so all transformer blocks share the same knowl-
edge. Such structure induces the unavoidable loss of model
capacity.
In this paper, we present a parameter deployment frame-
work that deploys trainable parameters more effectively: go-
ing wider instead of deeper. We then implement it on the
transformer and named it as WideNet. Specially, we ﬁrst em-
ploys parameter sharing along with depth to go shallower.
Due to avoidable model capacity loss, we go wider by using
the same MoE layer in all transformer blocks. The multi-
head attention layer is also shared across the blocks. To help
the transformer blocks learn different semantics and max-
imize the modeling capacity from MoE layer, we do not
share the normalization layers. Different trainable param-
eters of the normalization layer enable transformer blocks
to be fed by diversiﬁed representations. Since the model-
ing capacity of each transformer block has been enhanced
by the MoE layer, it can model diversiﬁed semantics effec-arXiv:2107.11817v3  [cs.LG]  7 Sep 2021

--- PAGE 2 ---
LayerNormMulti -Head AttentionLayerNormMoEi   Transformer 
BlockRecurrence
FFN 0 FFN 1 FFN 2 FFN 3
Router
h ha  th
att
imoe
i
l
il+1ha  L
i ha  1
i iFigure 1: The overall architecture of the proposed WideNet. Compared with vanilla transformer, we replace FFN layer by MoE
layer and share the trainable parameters except the normalization layers.
tively with the same trainable parameters. Therefore, with
one attention layer and one single stronger MoE layer learn-
ing complex representations, and independent normalization
layers for diversiﬁed semantic representations, going wider
instead of deeper is a more parameter-efﬁcient and effective
framework.
Compared with simply scaling along the width, going
wider instead of deeper is a more parameter-efﬁcient frame-
work, which makes the models small enough to be adapted
to downstream tasks without advanced parallelisms. Second,
each expert in WideNet can be trained by more token repre-
sentations so that it has better generalization performance.
Compared with the models simply compressed along with
the depth, all transformer blocks in WideNet share one same
MoE layer instead of one FFN layer. Such structure maxi-
mizes the modeling ability of every transformer block. More
experts can model more complex token representations with
a stronger capacity. Another difference is the independent
normalization layers. These layers come with few additional
trainable parameters, but they can transform input represen-
tations to other semantic domains. In this case, with a strong
enough single MoE layer, WideNet can still model seman-
tics from different levels well. Moreover, in every trans-
former block, each expert only receives a part of token repre-
sentations that usually correspond to different input tokens.
Our contributions are summarized as three folds:
• To improve the parameter efﬁciency, we propose sharing
the MoE layer across transformer blocks. The shared ex-
perts can receive diversiﬁed token representations in dif-
ferent transformer blocks, which enables each expert to
be fully trained.
• We propose to keep individual normalization layer across
transformer blocks. The individual normalization layerscan transform input hidden vectors to semantic informa-
tion by adding few trainable parameters. Then, diversi-
ﬁed input can be fed into the same attention layer or
stronger MoE layer to model different semantics.
• By combing the two thoughts above, we propose go-
ing wider instead of deeper, a more parameter-efﬁcient
and effective framework. We then implement this frame-
work as WideNet and evaluate it on both computer vi-
sion and natural language processing tasks. Due to the
more efﬁcient parameter deployment, WideNet outper-
forms baselines with less trainable parameters. We ex-
pect our WideNet can serve as a next-generation trans-
former backbone.
Mixture-of-Experts
In this paper, we focus on a novel trainable parameter de-
ployment framework and implement this framework on the
transformer as WideNet. The overall structure is shown in
Fig. 1. We use Vision Transformer as the backbone in this
example, which means we normalize the representations
before the attention layer or FFN layer. We also extend
WideNet to other transformer models ( e.g., BERT (Devlin
et al. 2019)) in this paper. In WideNet, we replace the FFN
layer with the MoE layer. Parameter sharing across trans-
former blocks is employed for a more parameter-efﬁcient
deployment. Within each MoE layer, we have one router
to selectKexperts to learn more complex representations.
Please note the trainable parameters in layer normalization
are not shared for more diversiﬁed semantic representations.
Conditional Computation with MoE
Our core idea is to deploy more trainable parameters along
the width and fewer trainable parameters along with the

--- PAGE 3 ---
depth. To this end, we employ MoE to scale transformer
along with width. As a typical conditional computation
model (Bengio 2013), MoE only activates a few experts, i.e.,
subsets of a network. For each input, we feed only a part of
hidden representations required to be processed into the se-
lected experts.
Following Shazeer et al. (2017), given Etrainable experts
and input representation x2RD, the output of MoE model
can be formulated as:
MoE(x) =EX
i=1g(x)ie(x)i(1)
wheree()iis a non-linear transformation RD!RDof
ithexpert, andg()iisithelement of the output of trainable
routerg(), a non-linear mapping RD!RE. Usually, both
e()andg()are parameterized by neural networks.
According to the formulation above, when g()is a sparse
vector, only part of experts would be activated and updated
by back-propagation during training. In this paper, for both
vanilla MoE and our WideNet, each expert is an FFN layer.
Routing
To ensure a sparse routing g(), we use TopK() to select the
top ranked experts. Then, following Riquelme et al. (2021),
g()can be written as:
g(x) = TopK(softmax( f(x) +)) (2)
wheref()is routing linear transformation RD!RE, and
 N (0;1
E2)is a Gaussian noise for exploration of ex-
pert routing. We use softmax afterf()for better perfor-
mance and more sparse experts (Riquelme et al. 2021; Fe-
dus, Zoph, and Shazeer 2021). When KE, most ele-
ments of g(x)would be zero so that sparse conditional com-
putation is achieved.
Balanced Loading
In MoE based transformer, we dispatch each token to K
experts. During training, if the MoE model has no regular-
ization, most tokens may be dispatched to a small portion
of experts. Such an unbalanced assignment would decrease
the throughput of the MoE model. In addition, more im-
portantly, most additional trainable parameters would not be
fully trained so that the sparse conditional model cannot sur-
pass the corresponding dense model during scaling. There-
fore, for balanced loading, we have two things to avoid:
(1) too many tokens dispatched to one single expert, and
(2) too few tokens received by one single expert. To solve
the ﬁrst issue, buffer capacity Bis required. That is, for
each expert, we only preserve Btoken at most regardless of
how many tokens are dispatched to this expert. If more than
B=CKNL tokens are assigned, the left tokens would be
dropped.Cis the capacity ratio, a pre-deﬁned hyperparam-
eter to control the ratio of tokens preserved for each expert.
Usually,C2[1;2], and we set Cas 1.2 when no special
explanation is used. Kis the number of selected experts foreach token. Nis the batch size on each device2.Lis the
sequence length. For computer vision tasks, Ldenotes the
number of patch tokens in each image.
Buffer capacity Bhelps us drop redundant tokens for each
expert to maximize throughput but it cannot ensure all ex-
perts to receive enough token to train. In other words, un-
til now, the routing is still unbalanced. Therefore, we fol-
low Fedus, Zoph, and Shazeer (2021) to use a differen-
tiable load balance loss instead of separate load-balancing
and importance-weighting losses for a balanced loading in
the router. For each routing operation, given Eexperts and
Nbatches with NL tokens, the following auxiliary loss is
added to the total model loss during training:
lbalance =EEX
i=1miPi (3)
wheremis vector.ithelement is the fraction of tokens dis-
patched to expert i:
mi=1
LLX
j=1h(xj)i (4)
where h()is a index vector selected by TopK in Eq. 2.
h(xj)iisithelement of h(xj). It is noticeable that, different
fromg(x)iin Eq. 2,miandh(xj)iare non-differentiable.
However, a differentiable loss function is required to opti-
mize MoE in an end-to-end fashion. Therefore, we deﬁne Pi
in Eq. 3 as:
Pi= softmax( f(x) +)i (5)
We can observe Piisithelement of routing linear trans-
formation after softmax activation function, and Piis dif-
ferentiable.
The goal of load balancing loss is to achieve a balanced
assignment. When we minimize lbalance , we can see both m
andPwould close to a uniform distribution.
Go wider instead of deeper
Sharing MoE across transformer blocks
As shown in Fig. 1, WideNet adopts parameter sharing
across transformer blocks to improve parameter efﬁciency,
and MoE layer is used to improve model capacity. In addi-
tion, as we use the MoE layer to obtain a stronger modeling
ability, to overcome the overﬁtting from sparse conditional
computation, we are required to feed enough tokens to each
expert. To this end, WideNet uses the same router and ex-
perts in different transformer blocks. Formally, given hid-
den representations H1=fh1
1;h1
2;:::;h1
Lgas input of the
ﬁrst transformer block, we can deﬁne the parameter sharing
asHi+1= MoE(Hi), which is different from the existing
MoE based models Hi+1= MoEi(Hi). Please note that,
although we share trainable parameters in the MoE layer in-
cluding the router, token representations corresponding to
the same token are different in every transformer block.
2For easier using on downstream tasks, we implement our
method with only data parallelism.

--- PAGE 4 ---
That is,hj
iandhj+1
imay be dispatched to different experts.
Therefore, each expert would be trained by more varied to-
kens for better generalization performance.
Individual Layer Normalization
Although existing works (Lan et al. 2019) show that the ac-
tivations in different transformer blocks are similar, the co-
sine distance is still much larger than zero. Therefore, dif-
ferent from existing works (Dehghani et al. 2018; Lan et al.
2019) sharing all weights across transformer blocks, to en-
courage more diversiﬁed input representations of different
blocks, we only share multi-head attention layer and FFN
(or MoE) layer, which means trainable parameters of layer
normalization are different across blocks.
In summary, ithtransformer block in our framework can
be written as:
x0= LayerNormalatt
i(x)
x= MHA(x0) +x
x00= LayerNormalmoe
i(x)
x= MoE(x00) +x(6)
The normalization layer LayerNormal()is:
LayerNormal( x) =x E[x]p
Var[x] ++ (7)
where2RDand2RDare two trainable vectors.
Layer normalization only requires these two small vectors
so individual normalization would just add few trainable pa-
rameters into our framework. We can ﬁnd the difference be-
tween shared layer normalization and the individual ones is
the mean and magnitude of output. For shared layer normal-
ization, the input of MHA and MoE layer are more similar in
different transformer blocks. Since we have shared trainable
matrixes, we encourage more diversiﬁed input to represent
various semantics in different transformer blocks.
Optimization
Although we reuse the trainable parameters of the router in
every transformer block, the assignment would be different
due to different input representations. Therefore, given T
times routing operation with the same trainable parameters,
we have the following loss for optimization:
loss=lmain +TX
t=1lT
balance (8)
whereis a hyper-parameter to ensure a balanced assign-
ment, and we set it as a relatively large number, i.e.,0.01 in
this work. Similar to existing MoE based models, we found
the performance is non-sensitive to .lmain is the main tar-
get of our transformer. For example, on supervised image
classiﬁcation, lmain is cross-entropy loss.
Experiments
Computer Vision
Experimental Settings We use ILSVRC-2012 Ima-
geNet (Deng et al. 2009) and Cifar10 (Krizhevsky, HintonTable 1: Results on ImageNet-1K pretraining.
Model Parameters ImageNet-1K
ViT-B 87M 78.6
ViT-L 305M 77.5
ViT-MoE-B 128M 77.9
ViT-MoE-L 406M 77.4
WideNet-B 29M 77.5
WideNet-L 40M 79.5
WideNet-H 63M 80.1
et al. 2009) as platforms to evaluate our framework. Ima-
geNet we used in this work has 1k classes and 1.3M im-
ages. We denote it as ImageNet-1K in the following exper-
iments. We select ViT (Dosovitskiy et al. 2020) and ViT-
MoE (Riquelme et al. 2021) as baselines. We ﬁrst reimple-
ment ViT by Tensorﬂow 2.x and tune it to a reasonable per-
formance. For all models in this section, we use Inception-
style pre-processing, Mixup (Zhang et al. 2017), RandAug-
ment (Cubuk et al. 2020) and label smoothing (Szegedy et al.
2016; Yuan et al. 2020) as data augmentation. We also ob-
serve that AdamW optimizer (Loshchilov and Hutter 2017)
is sensitive to hyper-parameters and learning schedules.
LAMB optimizer (You et al. 2019b) can achieve comparable
performance but it is more robust to the hyper-parameters.
For fair comparison, following Zhai et al. (2021), we evalu-
ate WideNet on three scales ( i.e.,WideNet-Base, WideNet-
Large and WideNet-Huge). The attention and FFN dimen-
sions of different scales are the same as ViT-MoE except for
WideNet-B. For WideNet-B, we use a hidden dimension of
FFN as 4096 instead of 3072 for a more stable training.
Instead of achieving SoTA performance, the goal of this
paper is to show that our parameter deployment frame-
work can improve the transformer backbone with less train-
able parameters. Therefore, we employ LAMB instead of
AdamW for more general and typical experiments. For MoE
based models ( i.e., ViT-MoE and WideNet), we set the
weight of load balance loss as 0.01. Without special in-
structions, we use 4 experts in total and Top 2 experts se-
lected in each transformer block. The capacity ratio Cis
set as 1.2 for a trade-off between accuracy and speed. We
pretrain our models on 256 TPUv3 cores. According to re-
cent work (Zhai et al. 2021), different types of the prediction
head have no signiﬁcant difference on ImageNet’s few-shot
performance. We also verify this conclusion on training Im-
ageNet from scratch. In this work, for ViT, we use the typical
token head, which means we insert [CLS] token at the start
of patch tokens and use it to classify the image. For MoE
based models, to fully use the token representations after the
ﬁnal MoE layer, we employ a global average pooling head
instead of the token head.
During ﬁnetuning, we still follow (Dosovitskiy et al.
2020) and use SGD optimizer with momentum. Compared
with pretraining on ImageNet-1K, label smoothing and
warm-up are removed.

--- PAGE 5 ---
Table 2: Results of funetuning on GLUE benchmarks
Model #para SQuAD1.1 SQuAD2.0 MNLI SST-2 Avg
ALBERT 12M 89.3/82.3 80.0/77.1 81.5 90.3 84.0
BERT 89M 89.9/82.8 80.3/77.3 83.2 91.5 85.0
WideNet 4 experts 26M 89.6/82.7 80.6/77.4 82.6 91.1 84.7
WideNet 8 experts 45M 90.0/82.7 80.6/77.7 83.3 91.9 85.2
WideNet 16 experts 83M 90.9/83.8 81.0/77.9 84.1 92.2 85.8
Comparison with baselines We follow the hyper-
parameter setting of baselines in pretraining and ﬁnetuning
for a fair comparison. Please see Appendix for details. Such
implementation also shows that our model is robust to hyper-
parameters.
We report the Top-1 accuracy on ImageNet-1K in Ta-
ble 1 and Cifar10 in Appendix. Observe that WideNet-H
achieves the best performance and signiﬁcantly outperforms
ViT and ViT-MoE models on ImageNet-1K. Compared with
the strongest baseline, our WideNet-H outperforms ViT-B
by1:5%with less trainable parameters. Even if we use the
smallest model, WideNet-B, it still achieves comparable per-
formance with ViT-L and ViT-MoE-B with over 4less
trainable parameters. When we scale up to WideNet-L, it
has surpassed all baselines with half trainable parameters of
ViT-B and 0:13parameters of ViT-L.
Another observation is, unlike training MoE based mod-
els on huge datasets ( e.g., JFT-300M (Sun et al. 2017)
and C4 (Raffel et al. 2019)), MoE cannot beneﬁt ViT on
ImageNet-1K, which is 200 times smaller than original ViT-
MoE used in pretraining3.
Natural Language Processing
The main contribution of this work is to design a more
parameter-efﬁcient and plug-in framework for various AI
applications. Therefore, we further evaluate our work on nat-
ural language processing (NLP) after computer vision (CV).
The training of experiments on NLP can still be splitted into
2 stages, pretraining and ﬁnetuning.
Experimental Settings Following BERT (Devlin et al.
2019) and ALBERT (Lan et al. 2019), in this section, we
pretrain all models by English Wikipedia (Devlin et al.
2019) and BOOKCORPUS (Zhu et al. 2015). Since the goal
of this work is to design a parameter-efﬁcient framework, all
models including BERT use factorized embedding parame-
terization. That is, the WordPiece embedding size Eis 128.
The hyperparameter settings of experiments on NLP can be
found in Appendix, which is the same as ALBERT for a
fair comparison. Similar to the experiments on vision tasks,
we pretrain our models by LAMB on 256 TPUv3 cores.
The learning rate is 0.00176, which is the same as ALBERT
claimed (You et al. 2019a).
During ﬁnetuning, we evaluate our model on the Gen-
eral Language Understanding Evaluation (GLUE) bench-
mark (Wang et al. 2018), two versions of the Stanford Ques-
3This dataset is not publicly available.tion Answering (SQuAD) dataset (Rajpurkar et al. 2016; Ra-
jpurkar, Jia, and Liang 2018). For GLUE experiments, we
report median over 5 runs on development set because of
relatively large variance.
Downstream Evaluation Different from the experiments
on CV , we report the evaluation results on downstream tasks
directly in this section. As shown in Table 2, when we use
more experts, our WideNet outperforms ALBERT by a large
margin. For instance, WideNet with 4 experts surpasses AL-
BERT by 1:2%in average. When we increase the number of
expertsEto 16 to achieve slightly less trainiable parameters
than BERT with factorized embedding parameterization, our
WideNet also outperforms it on all four downstream tasks,
which shows the parameter-efﬁciency and effectiveness of
going wider instead of deeper.
MoE Analysis
To investigate the reason why MoE cannot scale well on
smaller datasets like ImageNet-1K, we conduct two sets of
experiments on ViT-MoE and WideNet, respectively. Given
following hyper-parameters: (1) Number of training images
NI; (2) Number of patch tokens per image Np; (3) Num-
ber of experts in each transformer block E; (4) Capacity ra-
tioC; (5) Number of experts selected in each transformer
blockK, as we usually use a large , we can assume few to-
kens would be dropped when we are using Cslightly larger
than1:0. Then, we can approximate TNINpK
E. Existing
works (Riquelme et al. 2021; Yang et al. 2021) have shown
that decreasing NI,Np,KandCcan induce a performance
drop. In the ﬁrst set of experiments of this section, we scale
the number of experts in every transformer block Eto con-
trol the tokens fed into each expert on ImageNet-1K.
Results are shown in Fig. 2. We observe that more experts
(trainable parameters) lead to overﬁtting although more ex-
perts mean stronger modeling capacity. Training accuracy is
lower than testing accuracy because of data augmentation
we introduced in the Experimental Settings Section.
To further verify that each expert requires varied tokens to
train, we conduct the second set of experiments on WideNet.
We deﬁne the transformer blocks using the same routing as-
signment that belongs to one group. To change the input di-
versity of each expert, each group includes more than one
transformer block. That is, the hidden representations cor-
responding to the same token would be fed into the same
expert within the same group. We set Ggroups in total and
each group includesD
Gtransformer blocks, where Dis the
number of transformer blocks.

--- PAGE 6 ---
4 6 8 10 12 14 16
Number of experts66.068.070.072.074.076.078.080.0Top-1 accuracy (%)
Train
TestFigure 2: Results of scaling the number of experts.
12 6 3 2
Number of groups40.045.050.055.060.065.070.075.080.0Top-1 accuracy (%)
Train
Test Figure 3: Results of scaling the number of groups.
As shown in Fig. 3, when we use fewer groups, which
means we have fewer routing operations, there is an obvious
performance drop. We can suggest less diversiﬁed tokens are
fed to each expert because fewer groups mean less routing
and assignments. Therefore, more diversiﬁed tokens are re-
quired to train MoE based models on smaller datasets. More
importantly, such results show the effectiveness and neces-
sity of our design, routing at every transformer block.
Layer Norm Analysis
We are to analyze the reason why individual layer normal-
ization can improve performance in this section. Compared
with the vanilla transformer structure, we share trainable
matrixes in MHA and FFN (or MoE) layer across trans-
former blocks. The modeling capacity is compressed due
to the same trainable parameters across blocks. Although
WideNet uses the MoE layer to replace the FFN layer to
improve capacity, different blocks are still using the same
trainable parameters. Therefore, in WideNet, we encourage
more diversiﬁed input to represent various semantics in dif-
ferent transformer blocks. Compared with vanilla ViT, we
expect a larger variance of trainable vectors andacross
blocks. In this section, we are interested in layer normaliza-
tion before MoE or FFN.
Therefore, for ithelement of trainable vector orinjth
block, we compute the distance between this element and all
other elements of all vectors from other blocks. Taken as
example, we can formulate the value ywe are interested in
as:
y=1
MN2NX
j=1MX
m=1NX
n=1I(j6=n)jij mnj (9)
whereNis the number of transformer blocks, Mis the di-
mension of vector or.
In Fig. 4 and Fig. 5, we can observe that both and
in WideNet have larger ythan those in ViT, which means
MoE receives more diversiﬁed input than ViT. Such result
proves our assumption that individual normalization layer
can help to model various semantics model with shared large
trainable matrixes like MoE.Table 3: Results of ablation study on ImageNet-1K to to in-
vestigate the contributions of our three key modiﬁcations
(i.e.,Independent Layer Normalization, scaling width with
MoE layer and compressing depth with parameter sharing).
Model Top-1 Parameters
WideNet-B 77.5 29M
w/ shared Layer Norm 76.3 29M
w/o MoE layer Nan 9M
w/o parameter sharing 77.9 128M
WideNet-L 79.5 40M
w/ shared Layer Norm 78.3 40M
w/o MoE layer 76.9 15M
w/o parameter sharing 77.4 406M
WideNet-H 80.1 63M
w/ shared Layer Norm 76.6 63M
w/o MoE layer 79.0 23M
w/o parameter sharing OOM
Ablation Study: Contributions of key modiﬁcations
We ﬁrst conduct the ablation study to investigate the con-
tributions of our three key modiﬁcations ( i.e.,Independent
Layer Normalization, scaling width with MoE layer, and
compressing depth with parameter sharing). The results are
reported in Table 3.
We ﬁrst replace the individual layer normalizations with
the shared ones. We can observe there is a performance drop
with almost the same trainable parameters. Such observa-
tion shows the effectiveness of our design. In addition, we
recover the MoE layer to the FFN layer. Without the MoE
layer, the training would be extremely difﬁcult with much
less trainable parameters. For example, WideNet-B without
MoE layer encounters gradient explosion, and there is a sig-
niﬁcant performance drop. Finally, without parameter shar-
ing across transformer blocks, we can also observe a slight
performance drop and signiﬁcant parameter increment. For
WideNet-H without parameter sharing, it encounters out-of-
memory when training on 256 TPUv3 cores.

--- PAGE 7 ---
0 2 4 6 8 10
i-th LayerNorm before MoE layer0.000200.000250.000300.000350.000400.000450.00050y of gammaViT-B
WideNet-BFigure 4: Divergence of with LayerNorm layers.
0 2 4 6 8 10
i-th LayerNorm before MoE layer0.000100.000150.000200.000250.000300.000350.000400.00045y of betaViT-B
WideNet-B Figure 5: Divergence of with LayerNorm layers.
Table 4: Results of ablation study on ImageNet-1K to evaluate our WideNet with comparable speed or computation cost.
#Blocks is the number of transformer blocks. FNN dim means the dimension of FFN layer. Para Sharing is whether we shared
parameters across transformer blocks. Time denotes to TPUv3 core days.
Model #Blocks FNN dim Para Sharing Top-1 #Para Time
ViT-L 24 4096  77.5 305M 0.08K
ViT-L 24 4096p76.9 15M 0.07K
WideNet-L 12 4096p78.2 40M 0.07K
ViT-L 24 8192p75.8 24M 0.09K
WideNet-L 24 4096p79.5 40M 0.14K
Ablation Study: Comparison with comparable speed or
computation cost As we set the number of selected ex-
pertsKas 2 and capacity ratio Cas 1.2 in WideNet, there is
extra computation cost than vanilla ViT. Therefore, we con-
duct a second set of ablation studies to evaluate our WideNet
with comparable speed or computation cost with the base-
lines.
As shown in Table 4, compared with ViT-L, WideNet-
L is more computation expensive. We can observe a train-
ing time increment. However, when WideNet-L uses fewer
transformer blocks ( i.e.,12 blocks) than ViT-L, WideNet-L
outperforms ViT-L by 0:7%with slightly less training time
and13:1%parameters, and, similarly, there is a larger per-
formance improvement than ViT-L with parameter sharing.
We also scale ViT-L using parameter sharing to a wider FFN
layer. Then, for each token, ViT-L would have comparable
computation with WideNet-L setting Kas 2. We can see
scaling to more trainable parameters and FLOPs cannot im-
prove the performance of ViT-L, which also shows the ef-
fectiveness and necessity of our framework. Although ViT-L
has a comparable computation cost with WideNet for each
token, WideNet still spends more training time per epoch.
According to our experiments, there are two reasons, i.e.,
routing operation and C > 1:0. We leave optimize this as
our future work.Conclusion
In this paper, we propose to go wider instead of deeper for
more efﬁcient and effective parameter deployment. We im-
plement this plug and play framework as WideNet. Espe-
cially, WideNet ﬁrst compresses trainable parameters along
with depth by parameter-sharing across transformer blocks.
To maximize the modeling ability of each transformer block,
we replace the FFN layer with the MoE layer. Then, individ-
ual layer normalization provides a more parameter-efﬁcient
way to transform semantic representations across blocks. We
show that WideNet achieves the best performance by less
trainable parameters on both computer vision and natural
language processing backbones. In particular, on ImageNet-
1K, our best model achieves 80:1Top-1 accuracy with only
63Mparameters, which outperforms ViT and ViT-MoE by a
large margin. On four natural language processing datasets,
WideNet outperforms ALBERT by a large margin and sur-
pass BERT with less trainable parameters. Also, the inves-
tigation shows the reason why MoE cannot scale well on
smaller datasets. That is, each expert requires enough tokens
to train. Moreover, we veriﬁed that individual normaliza-
tion can transform hidden representations to other domains
for more diversiﬁed semantics. In summary, we show that
there is a great potential of this framework to train more
parameter-efﬁcient models.

--- PAGE 8 ---
References
Bengio, Y . 2013. Deep learning of representations: Looking
forward. In International conference on statistical language
and speech processing , 1–37. Springer.
Cubuk, E. D.; Zoph, B.; Shlens, J.; and Le, Q. V . 2020. Ran-
daugment: Practical automated data augmentation with a re-
duced search space. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition Work-
shops , 702–703.
Dehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.; and
Kaiser, Ł. 2018. Universal transformers. arXiv preprint
arXiv:1807.03819 .
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-
Fei, L. 2009. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , 248–255. Ieee.
Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.
BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding. In Proceedings of the 2019 Con-
ference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers) , 4171–4186. Min-
neapolis, Minnesota: Association for Computational Lin-
guistics.
Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,
D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;
Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16
words: Transformers for image recognition at scale. arXiv
preprint arXiv:2010.11929 .
Fedus, W.; Zoph, B.; and Shazeer, N. 2021. Switch trans-
formers: Scaling to trillion parameter models with simple
and efﬁcient sparsity. arXiv preprint arXiv:2101.03961 .
Huang, Y .; Cheng, Y .; Bapna, A.; Firat, O.; Chen, M. X.;
Chen, D.; Lee, H.; Ngiam, J.; Le, Q. V .; Wu, Y .; et al.
2018. Gpipe: Efﬁcient training of giant neural networks us-
ing pipeline parallelism. arXiv preprint arXiv:1811.06965 .
Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple
layers of features from tiny images.
Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma,
P.; and Soricut, R. 2019. Albert: A lite bert for self-
supervised learning of language representations. arXiv
preprint arXiv:1909.11942 .
Lepikhin, D.; Lee, H.; Xu, Y .; Chen, D.; Firat, O.; Huang,
Y .; Krikun, M.; Shazeer, N.; and Chen, Z. 2020. Gshard:
Scaling giant models with conditional computation and au-
tomatic sharding. arXiv preprint arXiv:2006.16668 .
Li, S.; Xue, F.; Li, Y .; and You, Y . 2021. Sequence Par-
allelism: Making 4D Parallelism Possible. arXiv preprint
arXiv:2105.13120 .
Loshchilov, I.; and Hutter, F. 2017. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101 .
Qu, C.; Yang, L.; Qiu, M.; Croft, W. B.; Zhang, Y .; and Iyyer,
M. 2019. BERT with history answer embedding for con-
versational question answering. In Proceedings of the 42nd
International ACM SIGIR Conference on Research and De-
velopment in Information Retrieval , 1133–1136.Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;
Matena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2019. Explor-
ing the limits of transfer learning with a uniﬁed text-to-text
transformer. arXiv preprint arXiv:1910.10683 .
Rajpurkar, P.; Jia, R.; and Liang, P. 2018. Know What You
Don’t Know: Unanswerable Questions for SQuAD. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) , 784–
789. Melbourne, Australia: Association for Computational
Linguistics.
Rajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.
SQuAD: 100,000+ Questions for Machine Comprehension
of Text. In Proceedings of the 2016 Conference on Empir-
ical Methods in Natural Language Processing , 2383–2392.
Austin, Texas: Association for Computational Linguistics.
Riquelme, C.; Puigcerver, J.; Mustafa, B.; Neumann, M.;
Jenatton, R.; Pinto, A. S.; Keysers, D.; and Houlsby, N.
2021. Scaling Vision with Sparse Mixture of Experts. arXiv
preprint arXiv:2106.05974 .
Shazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.;
Hinton, G.; and Dean, J. 2017. Outrageously large neu-
ral networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538 .
Shoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P.; Casper,
J.; and Catanzaro, B. 2019. Megatron-lm: Training multi-
billion parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053 .
Sun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Re-
visiting unreasonable effectiveness of data in deep learning
era. In Proceedings of the IEEE international conference on
computer vision , 843–852.
Sun, S.; Cheng, Y .; Gan, Z.; and Liu, J. 2019. Patient
Knowledge Distillation for BERT Model Compression. In
Proceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , 4323–4332. Hong Kong, China: Asso-
ciation for Computational Linguistics.
Szegedy, C.; Vanhoucke, V .; Ioffe, S.; Shlens, J.; and Wojna,
Z. 2016. Rethinking the inception architecture for computer
vision. In Proceedings of the IEEE conference on computer
vision and pattern recognition , 2818–2826.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. 2018. GLUE: A Multi-Task Benchmark and
Analysis Platform for Natural Language Understanding. In
Proceedings of the 2018 EMNLP Workshop BlackboxNLP:
Analyzing and Interpreting Neural Networks for NLP , 353–
355. Brussels, Belgium: Association for Computational Lin-
guistics.
Xu, C.; Zhou, W.; Ge, T.; Wei, F.; and Zhou, M.
2020. BERT-of-Theseus: Compressing BERT by Progres-
sive Module Replacing. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language Process-
ing (EMNLP) , 7859–7869. Online: Association for Compu-
tational Linguistics.

--- PAGE 9 ---
Xue, F.; Sun, A.; Zhang, H.; and Chng, E. S. 2020a. An
Embarrassingly Simple Model for Dialogue Relation Ex-
traction. arXiv preprint arXiv:2012.13873 .
Xue, F.; Sun, A.; Zhang, H.; and Chng, E. S. 2020b. GDP-
Net: Reﬁning Latent Multi-View Graph for Relation Extrac-
tion. arXiv preprint arXiv:2012.06780 .
Yang, A.; Lin, J.; Men, R.; Zhou, C.; Jiang, L.; Jia, X.;
Wang, A.; Zhang, J.; Wang, J.; Li, Y .; et al. 2021. Ex-
ploring Sparse Expert Models and Beyond. arXiv preprint
arXiv:2105.15082 .
Yang, Z.; Garcia, N.; Chu, C.; Otani, M.; Nakashima, Y .; and
Takemura, H. 2020. Bert representations for video question
answering. In Proceedings of the IEEE/CVF Winter Confer-
ence on Applications of Computer Vision , 1556–1565.
You, Y .; Li, J.; Hseu, J.; Song, X.; Demmel, J.; and Hsieh,
C.-J. 2019a. Reducing BERT pre-training time from 3 days
to 76 minutes. arXiv preprint arXiv:1904.00962 .
You, Y .; Li, J.; Reddi, S.; Hseu, J.; Kumar, S.; Bhojana-
palli, S.; Song, X.; Demmel, J.; Keutzer, K.; and Hsieh, C.-J.
2019b. Large batch optimization for deep learning: Training
bert in 76 minutes. arXiv preprint arXiv:1904.00962 .
Yuan, L.; Tay, F. E.; Li, G.; Wang, T.; and Feng, J. 2020.
Revisiting knowledge distillation via label smoothing regu-
larization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 3903–3911.
Zhai, X.; Kolesnikov, A.; Houlsby, N.; and Beyer, L.
2021. Scaling vision transformers. arXiv preprint
arXiv:2106.04560 .
Zhang, H.; Cisse, M.; Dauphin, Y . N.; and Lopez-Paz, D.
2017. mixup: Beyond empirical risk minimization. arXiv
preprint arXiv:1710.09412 .
Zhou, W.; Huang, K.; Ma, T.; and Huang, J. 2020.
Document-Level Relation Extraction with Adaptive Thresh-
olding and Localized Context Pooling. arXiv preprint
arXiv:2010.11304 .
Zhu, Y .; Kiros, R.; Zemel, R.; Salakhutdinov, R.; Urtasun,
R.; Torralba, A.; and Fidler, S. 2015. Aligning books and
movies: Towards story-like visual explanations by watching
movies and reading books. In Proceedings of the IEEE in-
ternational conference on computer vision , 19–27.

--- PAGE 10 ---
Appendix
Finetuning results on computer vision
Table 5: Results on Cifar10 ﬁnetuning.
Model Parameters Cifar10
ViT-B 85M 98.3
ViT-L 305M 98.2
ViT-MoE-B 126M 98.5
ViT-MoE-L 404M 98.5
WideNet-B 27M 98.4
WideNet-L 38M 98.8
Results in Table 5 shows that our WideNet can transfer
better performance from pretraining to ﬁnetuning. WideNet-
L, which outperforms all baselines in pretraining, is still the
best model in ﬁnetuning.
Hyper-parameters of experiments on computer
vision
Table 6: Hyper-parameters on ImageNet-1K pretraining and
Cifar10 ﬁnetuning.
Parameter ImageNet-1K Cifar10
Epoch 300 100
Warmup Epochs 30 0
Batch Size 4096 512
Learning rate 0.01 0.03
Weight Decay 0.1 0
Dropout 0.1 0.1
Label smoothing 0.1 0
Mixup prob. 0.5 0.5
The value of hyper-parameters on computer vision exper-
iments is shown in Table 6. On ImageNet-1K cosine learn-
ing rate decay is used after 30 warmup epochs. Please note
all models are using the same hyper-parameters of Table 6,
which also shows the robustness of our model.
Hyper-parameters of experiments on natural
language processing
As shown in Table 7, we use larger capacity ratio Con
SQuAD1.1 as limited training data may induce unbalanced
token distribution. Therefore, we set as 0 andCas 2.0.Table 7: Hyper-parameters on downstream NLP tasks.
Parameter SQuAD1.1/2.0 MNLI SST2
Steps 3649/8144 10000 5234
Warmup 365/814 1000 314
Batch Size 48 128 128
LR 5e-5/3e-5 3e-5 4e-5
C 2.0/1.2 1.2 1.2
 0/0.01 0.01 0.01
Dropout 0.1/0 0 0
Max Length 384/512 512 512
