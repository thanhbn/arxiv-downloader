# 2105.15082.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2105.15082.pdf
# File size: 2322235 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
M6-T: Exploring Sparse Expert Models and Beyond
An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia
Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu
Jingren Zhou, Hongxia Yangy
Alibaba Group
{ya235025,junyang.ljy,menrui.mr,ericzhou.zc,jiangle.jl,
xianyan.xianyanjia,wangang.wa,wanglin.zj,jiamang.wang,jiufeng.ly,
di.zhangd,weilin.lw,jingren.zhou,yang.yhx}@alibaba-inc.com
xide.ql@taobao.com
Abstract
Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has become
a trend in model scaling. Still it is a mystery how MoE layers bring quality gains
by leveraging the parameters with sparse activation. In this work, we investigate
several key factors in sparse expert models. We observe that load imbalance may
not be a signiﬁcant problem affecting model quality, contrary to the perspectives
of recent studies [ 11;6;12], while the number of sparsely activated experts k
and expert capacity Cin top-krouting can signiﬁcantly make a difference in
this context. Furthermore, we take a step forward to propose a simple method
called expert prototyping that splits experts into different prototypes and applies
ktop-1routing. This strategy improves the model quality but maintains constant
computational costs, and our further exploration on extremely large-scale models
reﬂects that it is more effective in training larger models. We push the model
scale to over 1trillion parameters and implement it on solely 480NVIDIA V100-
32GB GPUs, in comparison with the recent SOTAs [ 11;6] on2048 TPU cores.
The proposed giant model achieves substantial speedup in convergence over the
same-size baseline.
1 Introduction
Large-scale pretraining has been demonstrating tremendous success across several ﬁelds, especially
natural language processing [ 4;21;32;23;2]. Recent studies have shown that scaling model size can
bring signiﬁcant quality gains in downstream task performance [ 32;23;2], and the model quality
scales as a power law with the data size, model scale, and amount of computation [ 10]. This can be
extended to the ﬁeld of multimodal representation learning [ 18;3], where models with outrageous
numbers of parameters [ 27;15] can achieve outstanding performance in cross-modal understanding
and generation. However, training dense models is computationally expensive and it is hard to train
them on super-large-scale data with limited computational resources.
Inspired by the success of Mixture-of-Experts (MoE) [30; 26], some recent studies have focused on
training large-scale sparse expert models with high training efﬁciency [ 11;6;15;8]. An MoE layer
consists of multiple experts and thus has a large model capacity. Each forward computation routes
a token tokexperts from N, wherekN. Such a routing mechanism allows the combination of
data parallelism and expert parallelism. Previous studies [ 11;6] show that it can achieve obvious
performance speedup with the same computational resources. However, training such large-scale MoE
models can be extremely difﬁcult owing to multiple factors, e.g. system challenges of communication
and load imbalance, and algorithmic challenges of training instabilities, etc.
Preprint. Under review.arXiv:2105.15082v5  [cs.LG]  9 Aug 2021

--- PAGE 2 ---
In this work, we conduct an analysis of the recent MoE models to ﬁgure out which factors inﬂuence the
model quality and training efﬁciency. We investigate several factors concerning sparse expert models,
including load balance, top- krouting strategies, MoE attention, etc. Our analysis demonstrates that
load imbalance is not a signiﬁcant problem affecting model quality, while the number of sparsely
activated experts kand expert capacity Cin top-krouting make a great difference in this context, and
larger values of koften contribute to better model performance. However, with the increase in the
value ofk, training models with conventional top- krouting implementation [ 11] becomes much less
efﬁcient.
For further exploration, we extend the experiments to large-scale models with over 10 and 100 billion
parameters respectively. Our ﬁndings and proposals are applicable to extremely large-scale models,
and results show that expert prototyping has more signiﬁcant advantages in training larger models. To
go even further, we push the model scale to over 1trillion parameters and successfully implement it on
solely 480NVIDIA V100-32GB GPUs, compared with the recent SOTAs on 2048 TPU cores [ 11;6].
We show that the 1-trillion-parameter model outperforms the baseline of the identical model scale,
and achieves around 5 times of speedup in training convergence.
To summarize, our contributions are as follows:
•We explore key factors inside MoE models, and ﬁnd that load imbalance may not be a
signiﬁcant problem affecting model quality in this context, while the number of sparsely
activated experts kand expert capacity Cin top-krouting signiﬁcantly impact the model
performance.
•We propose a simple method called expert prototyping that splits experts into kprototypes
and appliesktop-1activation with similar efﬁciency to top- 1routing. This strategy improves
the model quality but maintains constant computational costs, and it is even more effective
in training larger-scale models.
•We advance our models to 1trillion parameters and successfully implement it on solely 480
NVIDIA V100-32GB GPUs, in comparison with the recent SOTAs on 2048 TPU cores.
1-trillion-parameter expert prototyping model outperforms the same-scale baseline and
achieves substantial speedup in convergence.
2 Sparse Expert Models
Sparse expert models are regarded as a promising method for model scaling with high training
efﬁciency. Training dense models requires extremely high computational costs, while sparse expert
models can converge signiﬁcantly faster as it iterates on more data with far higher efﬁciency on a time
basis. Such mode of large-scale model training is also more environmentally friendly. In this setting,
a large amount of the model weights are distributed to different workers owing to the architecture
of MoE layers, and MoE allows the increase in parameters while keeping constant computational
costs [11; 6].
Mixture-of-Experts is essentially a routing algorithm that routes tokens to kspeciﬁed experts from N
(wherekN) for forward computation. It enables expert parallelism so that experts process input
tokens speciﬁed by gating functions simultaneously. Expert networks, each of which is a multi-layer
perceptron, are distributed across workers. We use a gating function which speciﬁes kfromNexperts
for an input token representation x. The chosen experts process the representation with forward
computation and reduce their results with weighted summation based on the gating values:
~x=kX
ipiEi(x); p =softmax (g); g =topk(softmax (Wgx)); (1)
whereEirefers to the i-th expert. The most typical MoE algorithms for large-scale sparse expert
models are Switch Transformer and GShard [ 6;11]. Their key difference lies in their choice of top- k
selection, which applies top- 1and top- 2selection respectively. Switch Transformer [ 6] noted that
routing a token to 1expert only is effective in preserving model quality and reducing computation
complexity, contrary to the idea of Shazeer et al. [30] thatkshould be larger than 1so that there are
non-trivial gradients to the routing functions.
What makes a difference in the performance and model quality is the actual implementation of
distributed experts. Model parallelism allows the partitioning of a large tensor across workers, and our
2

--- PAGE 3 ---
Figure 1: Curves of the developments of coefﬁcients of variation cvat different layers. Here we
demonstrate the development of cvof baseline and auxiliary loss at all layers. We also demonstrate
their curves of training log perplexity (see the black dotted curve). Auxiliary loss helps the model
gain highly balanced compute loads at every layer, but the higher balance has not been translated
to higher model quality. On the contrary, the behaviors of load balance in the vanilla MoE model
are peculiar. Though cvat all layers drop at the beginning, yet some of them even increase to a high
value afterward.
implementation even enables multiple experts on an identical worker, instead of one expert per worker.
Due to the dynamic nature of top- krouting, it may cause low efﬁciency if severe load imbalance
happens. A standard implementation to tackle the problem is the setting of expert capacity [ 11;6;31],
which is deﬁned as:
C=kT
N; (2)
whereTrefers to the number of tokens in a batch and refers to the capacity factor which is
commonly larger than 1:0. A larger capacity factor can provide more buffer for expert capacity.
Tokens are distributed to experts with all-to-all dispatching operations. The token representations
processed by selected experts are combined with all-to-all communication to their original workers.
On the condition of exceeded capacity, token representations skip forward computation by residual
connection. For experts still having space in their capacities after dispatching, we add padding tokens
to ﬁll in. Thus the computations and communications costs are positively related to the number of
expertsNand the expert capacity C. Low expert capacity can cause a signiﬁcant amount of dropped
tokens if there is load imbalance on experts, but increasing expert capacity will correspondingly
enhance the computation and communication costs.
3 Exploration of MoE Models
To comprehensively investigate the MoE models, we conduct a series of experiments on multimodal
pretraining following the practice of Lin et al. [15], and we evaluate different setups, including
the routing methods, capacity, MoE attention, etc. Speciﬁcally, we pretrain the MoE model on
M6-Corpus, and evaluate the model performance on zero-shot image captioning on the dataset
E-commerce IC [15]. We provide more experimental details in the appendix.
3.1 Development of Load Balance
Recent studies pointed out the signiﬁcance of balanced routing [ 6;11;12], and illustrated the
importance of balancing methods such as auxiliary expert balancing loss. We ﬁrst conduct experiments
on the MoE models with and without auxiliary differentiable load balancing loss respectively. We
pretrain both models for 500ksteps and compare their upstream and downstream performance.
Experimental results show that the one with auxiliary loss even performs worse in both upstream
evaluation of training log perplexity ( 2:694vs.2:645) and the perplexity evaluation of downstream
3

--- PAGE 4 ---
	
	

	
	


a. Top-k Routing b. k Top-1 Routing for Expert PrototypingFigure 2: A demonstration of top- 2routing and 2top-1routing for expert prototyping. In top- 2
routing, the router selects the top- 2from all the experts and sends the token representation to those
experts. In 2top-1routing for expert prototyping, experts are ﬁrst grouped into 2prototypes, and
there is a router for each prototype. Similarly, each router chooses the top- 1expert and sends the
token representation through it. The output from each prototype is summed up element-wisely for
the ﬁnal output.
task of image captioning ( 9:97vs.9:72). These observations intrigue us to further investigate the
relationship between load balance and model quality.
We evaluate the degree of compute load balance of experts at every layer. Following Shazeer et al.
[30], we also use the coefﬁcient of variation for evaluation. We deﬁne the coefﬁcient of variation
for effective compute loads as that of the number of real tokens computed by the experts cv=(T)
(T),
whereTrefers to tokens computed by experts. Note that in the implementation, experts that still
have buffer should ﬁll padding tokens in their capacities. We do not include this part of computation
in the evaluation of compute load balance. This metric reﬂects the degree of uniformity of token
assignment. Furthermore, we focus on the development of cvin the training process to evaluate the
change of load assignment.
We demonstrate the results in Figure 1. For all layers, signiﬁcant load imbalance exists at the initial
stage according to the high values of cv. Notably, the value is generally higher at the top layers. For
the model trained with auxiliary expert load balancing loss, cvat all layers drop drastically at the
initial stage to a low value around 0:3denoting highly balanced compute loads,1and they become
stable in the following. However, compute loads are quite different for the MoE model without
auxiliary loss. Though cvat all layers drop at the beginning, yet they fail to reach a low one denoting
highly balancing compute loads. Except for that, some even increase to a high value afterward. These
phenomena reﬂect the existence of load imbalance. Though auxiliary loss is advantageous in expert
load balancing, such an advantage has not been translated to those in upstream and downstream
performance, as mentioned above.
3.2 The Effects of Top- kSparse Activation
Previous work [ 6] pointed out that top- 1is sufﬁcient for high model quality in comparison with
top-kselection with k>1, while it has signiﬁcant advantages in computational efﬁciency, but the
1We manually observe the number of tokens that experts receive and ﬁnd that compute loads are highly
balanced
4

--- PAGE 5 ---
Log Perplexity
TrainingSteps1Top-12Top-1&Capacityk×4Top-1&Capacityk×2Top-1&Capacity1×4Top-1&Capacity1×
TrainingStepsTop-1Top-2&Capacityk×Top-4&Capacityk×Top-2&Capacity1×Top-4&Capacity1×Figure 3: Model performance with different top- krouting setups. The left one demonstrates the
performance of top- krouting with different kvalues. This demonstrates that larger kvalues even
with small capacity bring more beneﬁts, but the gap between top- 2and top- 4is much smaller than
that between top- 1and top- 2. The right one demonstrates the performance of ktop-1routing for
expert grouping. Similarly, ktop-1withk>1routing signiﬁcantly outperforms the baseline, even
with small capacity.
Table 1: FLOPs of models with different top- krouting strategies. We report the computation
efﬁciency (GFLOPs) of top- krouting and ktop-1expert grouping with different values of k. “Ca-
pacityk” refers to the standard setting of expert capacity based on the value k, whereC=kT
N,
while “Capacity 1” refers to the setting of expert capacity of top- 1routing for all the models, where
C=1T
N. Results show that in the case of limited expert capacity models with different strategies
have similar computation FLOPs.
Top-1 Top-2 Top-4 2 Top-1 4 Top-1
Capacityk 2733.53 3614.12 5619.01 3614.00 5618.54
Capacity 1 2733.53 2733.60 2733.67 2733.54 2733.57
experimental results in Lewis et al. [12] show that top- 2still outperforms the top- 1selection. Here
we conduct experiments to further investigate how top- kmethods inﬂuence the model quality.
The value of kdetermines the number of experts to pass through for each token. We evaluate the
model performance on the conditions of k2f1;2;4g. Note that as mentioned in Section 2, the
expert capacity C=kT
Nshould be different for different values of k, and thus their computation
complexities are actually different. To evaluate the performance of different routing methods on the
condition of identical expert capacity, we experiment on top- krouting methods where k>1with
a uniform capacity C=1T
N, denoted by “Capacity 1” in the ﬁgures and tables. In contrast,
we denote the original setting of expert capacity C=kT
Nas “Capacity k”. We report the
computation FLOPs of models with different top- krouting strategies on the conditions of standard
and limited expert capacities.2Table 1 demonstrates that larger values of klead to higher computation
complexity, unless when the expert capacity Cis limited.
The left plot in Figure 3 demonstrates the effects of choice of kon model convergence. We ﬁnd that
both top- 2and top- 4sparse activation can outperform the top- 1. Even when Cis limited, denoting
similar computational complexity, routing methods with k >1still outperform the MoE baseline
signiﬁcantly. We assume that tokens can be processed by different experts that function differently.
However, it is interesting to ﬁnd the diminishing return that the gap between top- 2and top- 4is much
smaller than that between top- 1and top- 2, especially on the condition of limited expert capacity.
Besides, due to the looping “argmax” operation in top- krouting, the efﬁciency decreases drastically
whenkincreases, as demonstrated in Table 2. It is possible to strike a perfect balance between
effectiveness and efﬁciency by selecting a proper value of k.
2We report the FLOPs of a single GPU, counted by Tensorﬂow proﬁler.
5

--- PAGE 6 ---
Table 2: Speed of models with different top- krouting strategies (ms/step). We report the training
speed of models with different routing strategies. “Base” refers to the model with 1:5billion
parameters, and “10B” refers to the large scale model with over 10billion parameters. The models
are implemented with the limited expert capacity C=1T
N.
Top-1Top-2Top-4 2 Top-1 4 Top-1
Base 214.3 218.2 305.3 220.1 225.3
10B 462.2 493.0 514.2 466.9 473.9
3.3kTop-1VS. Top-k
Previous analysis indicates that kexperts play different roles and outcompete a single expert. However,
the looping “argmax” operation in top- krouting incurs computation inefﬁciency. Take top- 2sparse
activation as an example. It performs the operation of “argmax” for two times to select the top- 2
experts. Larger kcan incur lower efﬁciency in the computation. We evaluate the computation
efﬁciency of top- 1,2and4, as shown in Table 2. For both “Base” model with 1:5billion parameters
and “10B” model with over 10billion parameters, top- krouting where k>1especially when k= 4
is saliently slower than top- 1.
To tackle the issue of efﬁciency, we propose a simple method called expert prototyping, which splits
experts intokprototypes. In each forward computation, each token is sent to the kprototypes, and in
each prototype, it is processed by the expert in each group selected by top- k0routing where in most
casesk0= 1for analysis. The outputs of prototypes are combined linearly:
y=kX
i=1mX
j=1pijEij(x); (3)
wheremrefers to the number of experts inside a group. This method avoids the looping argmax
operation, but instead, it generates koutputs in a parallel fashion. In comparison with top- k, we also
name itktop-1. We ﬁnd that the increase in kdoes not cause a signiﬁcant decrease in training speed,
as shown in Table 2.
We evaluate the model quality of ktop-1expert prototyping. The right plot of Figure 3 demonstrates
that expert grouping with k >1can achieve better performance compared with the top- 1sparse
activation. However, the improvement in the context of limited expert capacity is much smaller than
that in the context of standard expert capacity. We ﬁnd that this problem can be greatly alleviated in
training large-scale models, and we leave this issue to Section 4.
3.4 Expert Prototyping and MoE Attention
Most work [ 30;26;11;6] replaced FFN with MoE. Fedus et al. [6]pointed out an alternative design
of replacing linear transformations in attention with MoE. Note that there are 4linear transformations
forQ,K,Vand output, and they can be viewed as one-layer FFN without non-linear activation.
Thus it is possible to replace them with MoE, and we call it MoE attention in this paper. Preliminary
experiments in Fedus et al. [6]pointed out that such attention may bring beneﬁts to model quality, but
it suffers from training instabilities. We implement the MoE attention to our model, and we preserve
the MoE layer for FFN.
Surprisingly, according to the left plot in Figure 4, we reach a different ﬁnding that MoE attention
negatively affects the model performance, and its training becomes more difﬁcult and even causes
divergence. The incorporation of sparse-activated MoE with attention-based correlation computation
may highly increase the training difﬁculty. However, ktop-1expert prototyping can beneﬁt the
training of MoE attention. Expert prototyping with k >1helps the models converge, though the
speed of convergence is still slower than that of the MoE baseline. Furthermore, we evaluate the
effect of MoE attention in the setting of deeper models. Speciﬁcally, we increase the number of layers
by4times to 20, but we decrease the number of experts to 8, so that there is no signiﬁcant difference
in the number of parameters between shallow and deep models. We demonstrate the results in the
right plot of Figure 4. It shows that deeper models with MoE attention perform better and does not
diverge, but it still performs worse than the MoE baseline. Still in this case, ktop-1expert grouping
6

--- PAGE 7 ---
Log Perplexity
TrainingStepsMoEAttention1Top-1MoEAttention4Top-1MoEFFN1Top-1
Log Perplexity
TrainingStepsMoEAttention1Top-1MoEAttention4Top-1MoEFFN1Top-1Figure 4: Model performance with MoE attention. The left plot and right plot refer to the per-
formance of shallow and deep models respectively. The left one demonstrates the performance of
ktop-1 routing with different kvalues and model design for MoE models. Different from preliminary
experiments in Fedus et al. [6], MoE attention has negative effects on the model performance. The
right one demonstrates the performance of MoE attention in the setting of deeper models. Similarly,
MoE attention model still performs worse than the MoE baseline.
Table 3: Evaluation of PPL on E-commerce IC [ 15]. Following Radford et al. [21], we do not perform
any ﬁne-tuning for any of these results. “Capacity k” refers to the standard setting of expert capacity
based on the value k, whereC=kT
N, while “Capacity 1” refers to the setting of expert capacity
of top- 1routing for all the models, where C=1T
N.
Model Size Capacity Top- 1Top-2Top-4 2 Top-1 4 Top-1
Base Capacity k 9.72 8.77 8.28 8.73 8.39
Base Capacity 1 9.72 8.84 8.52 9.24 9.41
greatly beneﬁts the model quality, in consistency with the aforementioned observation. This shows
the effectiveness of ktop-1expert prototyping.
3.5 Validation on Downstream Tasks
We conduct experiments on image captioning and the details of experimental setups are demonstrated
in the appendix. We demonstrate the experimental results of MoE models with different setups for
routing and expert capacity in Table 3. It can be found that the upstream performance can generally
be translated to downstream tasks, which shows that upstream training log perplexity can be an
indicator for downstream performance. With “Capacity k” expert capacity, both top- krouting and
ktop-1expert prototyping where k>1signiﬁcantly outperform the baseline. Yet with “Capacity 1”
limited capacity, ktop-1expert prototyping has smaller advantages over the baseline compared with
top-krouting. This may attribute to the problem of network degeneration. However, this problem is
effectively alleviated in training large scale model, as illustrated in Section 4.
4 Rocketing to Trillion Parameters
In this section, we demonstrate that our ﬁndings and proposals are also applicable to large-scale
pretrained models, and we ﬁnally advance the model scale to 1 trillion parameters on solely 480
NVIDIA GPUs, in comparison with the recent SOTA on 2048 TPU cores [6].
We validate our ﬁndings on extremely large-scale models. We scale the model size up to 10and
100billion parameters respectively. For simplicity, we validate ktop-1expert prototyping on
the 10B and 100B-parameter models. We report the training log perplexity for their upstream
performance. Figure 5 demonstrates their performance, and it can be found that for both 10B and
100B models expert prototyping still have advantages over the MoE baseline, and similarly in both
contexts larger kfor expert prototyping can further beneﬁt the model quality. Besides, as to the
7

--- PAGE 8 ---
Log Perplexity
TrainingSteps1Top-12Top-14Top-1
Log Perplexity
TrainingSteps1Top-12Top-14Top-1Figure 5: Performance of 10B and 100B models with different routing strategies. In both cases,
expert prototyping performs better than the MoE baseline, and larger values of kfurther beneﬁt
model qualities.
Table 4: PPL evaluation of “10B” models on E-commerce IC [ 15]. Note that the “10B” models with
over10billion parameters are trained with the limited expert capacity where C=1T
N.
Model Size Top- 1Top-2 2 Top-1
10B 6.97 5.73 5.64
downstream performance, Table 3 shows that for the large scale model “10B”, 2Top-1can reach
similar performance to Top- 2routing with limited expert capacity. This shows the effectiveness of
expert prototyping in training models of a larger scale.
Based on the aforementioned ﬁndings and proposals, we move forward to build an extremely large-
scale model with over 1trillion parameters. Due to limited computational resources, we attempt to
ﬁgure out solutions to implement a 1-trillion-parameter model on solely 480NVIDIA V100-32GB
GPUs.
To be more speciﬁc, we implement our model on a cluster of single-GPU workers connected
by RDMA networks with a bandwidth of 100Gb. To save memory usage, we instead turn to
Adafactor [ 29] for optimization in concern of its sublinear memory costs. However, there are a series
of sporadic issues concerning training instabilities. Through trials and errors, we ﬁnd that such model
training is highly sensitive to learning rates, especially when being trained with Adafactor. We did
not use the default one 0:01due to divergence, but instead, we use 0:005to strike a balance between
training stability and convergence speed. Also, we ﬁnd that it is essential to lower the absolute values
of initialized weights, which is also illustrated in Fedus et al. [6]. We speciﬁcally reduce the BERT
initialization, a truncated normal distribution with = 0and= 0:02, by a factor of 10.
We ﬁrst evaluate the quality of models with different parameters but similar computation FLOPs by
observing training log perplexity. We compare the performance of MoE baseline models with 100
billion, 250billion parameters, and 1trillion parameters, and we observe that the results prove the
scaling law that models with larger capacity performs better, as demonstrated in Figure 6. Then we
implement both 1-trillion-parameter MoE baseline and our expert prototyping MoE model.3Still,
from Figure 6 we can ﬁgure out the proposal has a strong advantage over the compared model with 1
trillion parameters. We observe a substantial speedup in convergence, where our method is around
5times faster than the baseline. However, both models have similar computational FLOPs, which
demonstrates that our method strikes a far better balance between computational efﬁciency and model
quality.
3Due to limited computational resources and instabilities in systems and hardware, the trillion-parameter
expert prototyping model has been trained for only 30ksteps.
8

--- PAGE 9 ---
Log Perplexity
TrainingSteps100billion250billion1trillion1trillion&prototype
5xSpeedupFigure 6: Performance of baseline models with 100billion, 250billion, and 1trillion parame-
ters, as well as 1-trillion-parameter model with expert prototyping. The curves reﬂect the scaling
law, and also demonstrate the advantage of expert prototyping for giant models.
5 Related work
Pretraining has achieved great success in these years, and it has recently become a common practice
in natural language processing [ 20;4;22;36;16;5]. In the ﬁeld of cross-representation learning,
pretraining has also become signiﬁcant and pushed the limit of model performance in downstream
tasks [ 18;33;19;3;7;14;37;13;38]. Recent studies [ 10] demonstrate the power law of model scale
and performance. With the rapid development in distributed training and parallelism [ 32;24;28;25],
we have witnessed the burst of studies in extremely large scale pretraining in both natural language
processing [ 2;32] and multimodal pretraining [ 27;15] and also new state-of-the-art performance
in the recent two years. Though extremely large-scale dense models are highly effective especially
in the context of few-shot learning [ 2], some researchers have turned to sparse expert models for
efﬁcient large-scale pretraining. Inspired by the success of Mixture-of-Experts [ 30;26;31], recent
studies [ 11;6] expand the model size to over trillion parameters and fully utilize the advantages
of TPUs to build sparse expert models with Mesh-Tensorﬂow [ 31]. They demonstrate that sparse
expert models can perform much better than dense models with the same computational FLOPs but
their computational costs are similar. A series of the following work successfully implement sparse
expert models on NVIDIA GPU [ 15;12]. In this work, we follow the practice of Lin et al. [15] and
implement our models on the distributed learning framework Whale [35].
6 Conclusion
In this work, we explore the factors inside sparse expert models and investigate how they inﬂuence the
model quality and computational efﬁciency. We ﬁnd out that load imbalance may not be a signiﬁcant
issue affecting model quality, but the number of activated experts kand expert capacity Cplay a
signiﬁcant role in training MoE models. A simple method called expert prototyping that splits experts
into different prototypes and applies ktop-1routing can help the model achieve improved performance
while keeping constant computational costs. We extend our series of experiments to extremely large-
scale models with over 10and100billion parameters and demonstrate the effectiveness of expert
prototyping in training extremely large-scale models. Finally, we successfully implement the 1-
trillion parameter model on solely 480NVIDIA V100-32GB GPUs, compared with the recent SOTAs
implemented on 2048 TPU cores. We show that our simple method can improve the performance of
9

--- PAGE 10 ---
1-trillion-parameter sparse expert models effectively and help them achieve substantial speedup in
convergence.
References
[1]M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving,
M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. A.
Tucker, V . Vasudevan, P. Warden, M. Wicke, Y . Yu, and X. Zheng. Tensorﬂow: A system for
large-scale machine learning. In 12th USENIX Symposium onOperating Systems Design and
Implementation, OSDI 2016, pages 265–283, 2016.
[2]T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165, 2020.
[3]Y . Chen, L. Li, L. Yu, A. E. Kholy, F. Ahmed, Z. Gan, Y . Cheng, and J. Liu. UNITER: universal
image-text representation learning. In ECCV 2020, pages 104–120, 2020.
[4]J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional
transformers for language understanding. In NAACL-HLT 2019, pages 4171–4186, 2019.
[5]L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M. Zhou, and H. Hon. Uniﬁed
language model pre-training for natural language understanding and generation. In NeurIPS
2019, pages 13042–13054, 2019.
[6]W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models
with simple and efﬁcient sparsity. CoRR , abs/2101.03961, 2021. URL https://arxiv.org/
abs/2101.03961 .
[7]Z. Gan, Y .-C. Chen, L. Li, C. Zhu, Y . Cheng, and J. Liu. Large-scale adversarial training for
vision-and-language representation learning. In NeurIPS 2020, 2020.
[8]J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang. Fastmoe: A fast mixture-of-expert training
system. CoRR, abs/2103.13262, 2021.
[9]K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR
2016, pages 770–778, 2016.
[10] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford,
J. Wu, and D. Amodei. Scaling laws for neural language models. CoRR , abs/2001.08361, 2020.
[11] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen.
Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv
preprint arXiv:2006.16668, 2020.
[12] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer. BASE layers: Simplifying
training of large, sparse models. CoRR, abs/2103.16716, 2021.
[13] W. Li, C. Gao, G. Niu, X. Xiao, H. Liu, J. Liu, H. Wu, and H. Wang. UNIMO: towards
uniﬁed-modal understanding and generation via cross-modal contrastive learning. CoRR ,
abs/2012.15409, 2020.
[14] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong, F. Wei, Y . Choi,
and J. Gao. Oscar: Object-semantics aligned pre-training for vision-language tasks. CoRR ,
abs/2004.06165, 2020.
[15] J. Lin, R. Men, A. Yang, C. Zhou, M. Ding, Y . Zhang, P. Wang, A. Wang, L. Jiang, X. Jia,
J. Zhang, J. Zhang, X. Zou, Z. Li, X. Deng, J. Liu, J. Xue, H. Zhou, J. Ma, J. Yu, Y . Li, W. Lin,
J. Zhou, J. Tang, and H. Yang. M6: A chinese multimodal pretrainer. CoRR , abs/2103.00823,
2021.
[16] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,
and V . Stoyanov. Roberta: A robustly optimized BERT pretraining approach. CoRR ,
abs/1907.11692, 2019.
10

--- PAGE 11 ---
[17] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR 2019, 2019.
[18] J. Lu, D. Batra, D. Parikh, and S. Lee. Vilbert: Pretraining task-agnostic visiolinguistic
representations for vision-and-language tasks. In NeurIPS 2019, pages 13–23, 2019.
[19] J. Lu, V . Goswami, M. Rohrbach, D. Parikh, and S. Lee. 12-in-1: Multi-task vision and language
representation learning. CoRR, abs/1912.02315, 2019.
[20] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep
contextualized word representations. In NAACL-HLT 2018, pages 2227–2237, 2018.
[21] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are
unsupervised multitask learners.
[22] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Im-
proving language understanding by generative pre-training. URL
https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/
languageunsupervised/language understanding paper. pdf, 2018.
[23] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J.
Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. CoRR ,
abs/1910.10683, 2019.
[24] S. Rajbhandari, J. Rasley, O. Ruwase, and Y . He. Zero: Memory optimization towards training
a trillion parameter models. arXiv preprint arXiv:1910.02054, 2019.
[25] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y . He. Zero-inﬁnity: Breaking the GPU
memory wall for extreme scale deep learning. CoRR, abs/2104.07857, 2021.
[26] P. Ramachandran and Q. V . Le. Diversity and depth in per-example routing models. In 7th
International Conference onLearning Representations, ICLR 2019, 2019.
[27] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen, and I. Sutskever.
Zero-shot text-to-image generation, 2021.
[28] J. Ren, S. Rajbhandari, R. Y . Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y . He.
Zero-ofﬂoad: Democratizing billion-scale model training. arXiv preprint arXiv:2101.06840 ,
2021.
[29] N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost.
InProceedings ofthe35th International Conference onMachine Learning, ICML 2018 , pages
4603–4611, 2018.
[30] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outra-
geously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint
arXiv:1701.06538, 2017.
[31] N. Shazeer, Y . Cheng, N. Parmar, D. Tran, A. Vaswani, P. Koanantakool, P. Hawkins, H. Lee,
M. Hong, C. Young, R. Sepassi, and B. A. Hechtman. Mesh-tensorﬂow: Deep learning for su-
percomputers. In Advances inNeural Information Processing Systems 31:Annual Conference
onNeural Information Processing Systems 2018, NeurIPS 2018, pages 10435–10444, 2018.
[32] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm:
Training multi-billion parameter language models using model parallelism. arXiv preprint
arXiv:1909.08053, 2019.
[33] W. Su, X. Zhu, Y . Cao, B. Li, L. Lu, F. Wei, and J. Dai. Vl-bert: Pre-training of generic
visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019.
[34] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and
I. Polosukhin. Attention is all you need. In NeurIPS 2017, pages 5998–6008, 2017.
[35] A. Wang, X. Jia, L. Jiang, J. Zhang, Y . Li, and W. Lin. Whale: A uniﬁed distributed training
framework. arXiv preprint arXiv:2011.09208, 2020.
11

--- PAGE 12 ---
[36] Z. Yang, Z. Dai, Y . Yang, J. G. Carbonell, R. Salakhutdinov, and Q. V . Le. Xlnet: Generalized
autoregressive pretraining for language understanding. In NeurIPS 2019 , pages 5754–5764,
2019.
[37] F. Yu, J. Tang, W. Yin, Y . Sun, H. Tian, H. Wu, and H. Wang. Ernie-vil: Knowledge enhanced
vision-language representations through scene graph. arXiv preprint arXiv:2006.16934, 2020.
[38] P. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y . Choi, and J. Gao. Vinvl: Making visual
representations matter in vision-language models. CoRR, abs/2101.00529, 2021.
12

--- PAGE 13 ---
A Appendix
A.1 Multimodal Pretraining and Downstream Evaluation
In practice, we follow Lin et al. [15] that employs an extremely large-scale multimodal pretrained
model with MoE architecture in Chinese. Speciﬁcally, we pretrain a model on the image-text pairs
from the dataset M6-Corpus [ 15]. In multimodal pretraining, the pretrained model receives the inputs
of a pair of related image and text as the input and generates the high-level representations with
layers of Transformer [ 34]. In our experiments, we ﬁrst transform an input image to patch features
by splitting it into 44patches and extracting patch features with a trained ResNet [ 9]. We ﬂatten
the patch features of the input image to a sequence of representations and concatenate them with the
word embeddings of the text sequence shorter than 128words. Then we build a feature extractor
with multiple layers of transformer consisting of self attention and feed-forward neural networks
(FFN). Notably, in order to integrate MoE to the model architecture, we replace the FFN with MoE,
where FFN as experts are distributed across workers. We pretrain the model with the task of image
captioning, where the model learns to generate words autoregressively based on the previous context
including the patch features.
To comprehensively evaluate the performance of the methods, we conduct experiments on image
captioning in Chinese, and we follow Lin et al. [15] to use the dataset E-commerce IC. We focus on
the capability of language modeling of the pretrained model, and thus we use teacher forcing and
evaluate the performance by perplexity (PPL).
A.2 Experimental Setups
For the exploration, we investigate different setups for both models. Here we point out key conﬁg-
urations of our experimental setups and we demonstrate the details in Table 5. Following BERT-
Chinese [ 4], we use the same vocabulary with 21128 subwords. Following the practice of Raffel et al.
[23], we use the same hidden size 1024 for all the models. We generally scale the model size by
increasing the number of layers, the intermediate size, as well as the number of experts. For the setup
of attention, the number of attention head is 16and the attention head size is 64. For the initialization,
we use the BERT initialization with = 0and= 0:02for most cases, and we use an initialization
with a smaller standard deviation of 0:002for the 1Tmodel. As to the expert capacity, we generally
use a capacity factor of = 1:25for more buffer. The batch size per GPU is 8and the total batch
size is equal to the product of the batch size per GPU and the number of GPUs. We use AdamW
optimizer [ 17] for optimization except for the 1Tmodel where we use Adafactor [ 29] instead. The
learning rate for AdamW is 8e 5, and that for Adafactor is 5e 3. We use warmup schedule with a
warmup step of 500. The dropout rate for FFN and attention is 0:1. We use mixed precision training
for FP16 communication for all models except the 1Tone due to the issue of training instability.
Table 5: Hyperparameters for pretraining the MoE models.
Hparam base 10B 100B 1T
Hidden size 1024 1024 1024 1024
Intermediate size 4096 4096 4096 21248
Number of layers 5 10 24 24
Number of attention heads 16 16 16 16
Attention head size 64 64 64 64
Initializer range 0.02 0.02 0.02 0.002
Number of experts 32 128 512 960
Number of GPUs 8 16 128 480
Optimizer AdamW AdamW AdamW Adafactor
Learning rate 8e-5 8e-5 8e-5 5e-3
Mixed precision X X X 
FP16 communication X X X 
Params 1.4B 10.8B 103.2B 1002.7B
We implement our experiments on Tensorﬂow 1.15 [ 1]. Different from the original implementation
of Switch and GShard with Mesh-Tensorﬂow [ 31], we implement the multimodal pretrained model
with the framework Whale [ 35], which enables data, model, and expert parallelism on NVIDIA GPU.
13

--- PAGE 14 ---
A.3 Pseudo Code for Expert Prototyping
The pseudo codes for MoE layer and proposed expert prototyping in Whale are provided in Figure 7
and Figure 8 respectively. Table 6 illustrates the notations of speciﬁc tensor dimensions.
Table 6: Notation table for Pseudo code.
Variable Deﬁnition
D Number of workers
d Number of GPUs per worker (d=1 in this paper)
E Number of total experts
e Number of experts per worker (e*D=E)
C Capacity per expert
M Model size (same as hidden size, same as embedding size)
I Intermediate size
B Batch Size per GPU
L Sequence Length
T Number of tokens (T=B*L)
Z Number of prototypes
F Number of expert per prototype (Z*F=E)
Amount of All-to-All Communication There are two operations of all-to-all communication in
each MoE FFN layer in a forward propagation process (one for dispatch_inputs and the other for
outputs in the pseudo code). During the communication, each entry of the communicated tensor passes
to a worker once. Thus, the total amount of communication, which is O(EdCM ) +O(eDCM ) =
O(EdCM ) =O(ECM ), depends on the number of experts, capacity and model size.
Amount of Computation The total amount of computation in the MoE FFN layer is mainly
dominated by the two matrix multiplications, which transform the input tensor from the hidden size to
the intermediate size and then vice versa. The total computation of these two matrix multiplications
isO(DeCMI ) +O(DeCIM ) =O(ECMI ). In our proﬁling on the 1T-scale MoE model, these
two operations hold around 98% total forward FLOPs of the MoE FFN layer.
14

--- PAGE 15 ---
Figure 7: Pseudo code of the MoE Transformer layer in Whale.
15

--- PAGE 16 ---
Figure 8: Pseudo code of the Expert Prototyping.
16
