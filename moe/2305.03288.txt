# 2305.03288.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2305.03288.pdf
# File size: 756470 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Demystifying Softmax Gating Function in
Gaussian Mixture of Experts
Huy Nguyen
Department of Statistics and Data Sciences
The University of Texas at Austin
Austin, TX 78712
huynm@utexas.eduTrungTin Nguyen
Univ. Grenoble Alpes, Inria
CNRS, Grenoble INP, LJK,
38000 Grenoble, France
trung-tin.nguyen@inria.fr
Nhat Ho
Department of Statistics and Data Sciences
The University of Texas at Austin
Austin, TX 78712
minhnhat@utexas.edu
Abstract
Understanding the parameter estimation of softmax gating Gaussian mixture of
experts has remained a long-standing open problem in the literature. It is mainly
due to three fundamental theoretical challenges associated with the softmax gating
function: (i) the identifiability only up to the translation of parameters; (ii) the
intrinsic interaction via partial differential equations between the softmax gating
and the expert functions in the Gaussian density; (iii) the complex dependence
between the numerator and denominator of the conditional density of softmax
gating Gaussian mixture of experts. We resolve these challenges by proposing
novel V oronoi loss functions among parameters and establishing the convergence
rates of maximum likelihood estimator (MLE) for solving parameter estimation
in these models. When the true number of experts is unknown and over-specified,
our findings show a connection between the convergence rate of the MLE and a
solvability problem of a system of polynomial equations.
1 Introduction
Softmax gating Gaussian mixture of experts [ 32,37], a class of statistical machine learning models
that combine multiple simpler models, known as expert functions of the covariates, via softmax
gating networks to form more complex and accurate models, has found widespread use in various
applications, including speech recognition [ 51,64,65], natural language processing [ 14,20,17,54,
21], computer vision [ 52,3,15,44], and other applications [ 26,49,7,8,48,5,6]. Regarding the
applications of the softmax gating Gaussian mixture of experts in medicine [ 43] and physical sciences
[39], the parameters of each expert function play an important role in capturing the heterogeneity of
data. Thus, the main objective of these works is to conduct statistical inference for those parameters,
which leads to a need for convergence rates of parameter estimation in the softmax gating Gaussian
mixture of experts. However, a comprehensive theoretical understanding of parameter estimation in
that model has still remained a long-standing open problem in the literature.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2305.03288v2  [stat.ML]  30 Oct 2023

--- PAGE 2 ---
Parameter estimation has been studied quite extensively in standard mixture models. In his seminal
work, Chen et al. [ 9] established the convergence rate O(n−1/4)of parameter estimation in over-fitted
univariate mixture models, namely, the settings when the number of true components is unknown and
over-specified, and the family of distributions is strongly identifiable in the second order, e.g., location
Gaussian distributions. That slow and non-standard rate is due to the collapse of some parameters
into single pararameter or the convergence of weights to zero, which leads to the singularity of Fisher
information matrix around the true parameters. Then, Nguyen et al. [ 50] and Ho et al. [ 29] utilized
Wasserstein metrics to achieve this rate under the multivariate settings of second-order strongly
identifiable mixture models. Recently, Ho et al. [ 28] demonstrated that rates of the MLE can strictly
depend on the number of over-specified components when the mixture models are not strongly
identifiable, such as location-scale Gaussian mixtures. The minimax optimal behaviors of parameter
estimation were studied in [ 27,46]. From the computational side, the statistical guarantee of the
expectation-maximization (EM), e.g., [ 12], and moment methods had also been studied under both
exact-fitted [2, 1, 25] and over-fitted settings [19, 18, 61, 16, 62] of mixture models.
Compared to mixture models, there has been less research on parameter estimation of mixture of
experts. When the gating networks are independent of the covariates, Ho et al. [ 30] employed the
generalized Wasserstein loss function [ 59] to study the convergence rates of parameter estimation
in Gaussian mixture of experts. They proved that these rates are determined by the algebraic inde-
pendence of the expert functions and the partial differential equations with respect to the parameters.
Later, Do et al. [ 13] extended these results to general mixture of experts with covariate-free gating
network. Statistical guarantees of optimization methods for solving parameter estimation in Gaussian
mixture of experts with covariate-free gating functions were studied in [ 11,67,41,63]. When the
gating networks are softmax functions, parameter estimation becomes more challenging to understand
due to the complex structures of the softmax gating function in the Gaussian mixture of experts.
Before describing these phenomena in further details, we begin by formally introducing the softmax
gating Gaussian mixture of experts and related notions.
Problem setting: Assume that (X1, Y1), . . . , (Xn, Yn)∈Rd×Rare i.i.d. samples drawn from the
softmax gating Gaussian mixture of experts of order k∗whose conditional density function gG∗(Y|X)
is given by:
gG∗(Y|X) :=k∗X
i=1exp(( β∗
1i)⊤X+β∗
0i)
Pk∗
j=1exp(( β∗
1j)⊤X+β∗
0j)f(Y|(a∗
i)⊤X+b∗
i, σ∗
i), (1)
where f(.|µ, σ)is a Gaussian density function with mean µand variance σ. Here, we define
G∗:=Pk∗
i=1exp(β∗
0i)δ(β∗
1i,a∗
i,b∗
i,σ∗
i)as a true but unknown mixing measure , that is, a combination
of Dirac measures δassociated with true parameters θ∗
i:= (β∗
0i, β∗
1i, a∗
i, b∗
i, σ∗
i). Notably, G∗is not
necessarily a probability measure as the summation of its weights can be different from one. For the
purpose of the theory, we assume that θ∗
i∈Θ⊂R×Rd×Rd×R×R+where Θis a compact set,
andX∈ X ⊂ Rdwhere Xis a bounded set. Furthermore, we let (a∗
1, b∗
1, σ∗
1), . . . , (a∗
k∗, b∗
k∗, σ∗
k∗)
be pairwise distinct and at least one among β∗
11, . . . , β∗
1k∗be non-zero to guarantee the dependence
of softmax gating function on the covariate X. Finally, we assume that the covariate Xfollows
a continuous distribution to ensure that the softmax gating Gaussian mixture of experts is at least
identifiable up to translations (see Proposition 1).
Maximum likelihood estimation. Since the value of true order k∗is unknown in practice, to
estimate the unknown parameters in the softmax gating Gaussian mixture of experts (1), we consider
using maximum likelihood estimation (MLE) within a class of all mixing measures with at most k
components, which is defined as follows:
bGn∈arg max
G∈Ok(Θ)1
nnX
i=1log(gG(Yi|Xi)), (2)
where Ok(Θ) := {G=Pk′
i=1exp(β0i)δ(β1i,ai,bi,σi): 1≤k′≤kand(β0i, β1i, ai, bi, σi)∈Θ}.
To guarantee that the MLE bGnis a consistent estimator of G∗, we need k≥k∗. In this paper, we
study the convergence rate of the MLE bGnto the true mixing measure G∗under both the exact-fitted
settings, namely when k=k∗, and the over-fitted settings, namely when k > k ∗, of the softmax
gating Gaussian mixture of experts.
2

--- PAGE 3 ---
Fundamental challenges from the softmax gating function: There are three fundamental challenges
arising from the softmax gating function that create various obstacles in our convergence analysis:
(i)Firstly, parameters β∗
1i, β∗
0iof the softmax gating function are not identifiable as those of the
covariate-independent gating function in previous work. Instead, they are identifiable up to transla-
tions, that is, the softmax gating value does not change when we translate β∗
0itoβ∗
0i+t1andβ∗
1ito
β∗
1i+t2for any t1∈Randt2∈Rd. As a consequence, we need to introduce an infimum operator
in the V oronoi loss functions (see equations (4) and (6)) to deal with this issue.
(ii)Secondly, a key step in our proof techniques is to decompose the density discrepancy gbGn(Y|X)−
gG∗(Y|X)into a linear combination of linearly independent elements using Taylor expansions.
However, since the numerators and denominators of softmax gating functions are dependent, we
cannot apply the Taylor expansions directly to that density discrepancy as in previous work [ 30,13].
Moreover, there are two intrinsic interactions between parameters of the softmax gating’s numerators
and the Gaussian density function via the following partial differential equations (PDEs), which
induce a lot of linearly dependent derivative terms in the Taylor expansions:
∂2u
∂β1∂b=∂u
∂a;∂2u
∂b2= 2∂u
∂σ, (3)
where u(Y|X;β1, a, b, σ ) := exp( β⊤
1X)·f(Y|a⊤X+b, σ). Therefore, it takes us great effort to
group those linearly dependent terms together to obtain the desired linear combination of linearly
independent terms.
(iii)Lastly, given the above linear combination of linearly independent elements, when the density
estimation gbGn(Y|X)converges to the true density gG∗(Y|X), the associated coefficients in that
combination also go to zero. Then, via some transformations, those limits lead to a system of
polynomial equations introduced in equation (9). This system admits a much more complex structure
than what considered in previous work [30, 13].
These fundamental challenges from the softmax gating function suggest that the previous loss
functions, such as Wasserstein distance [ 50,28,30], being employed to study parameter estimation
in standard mixture models or mixture of experts with covariate-free gating functions are no longer
sufficient as they heavily rely on the assumptions that the weights of these models are independent of
the covariates.
Main contributions: To tackle these challenges of the softmax gating function, we propose two
novel V oronoi losses among parameters and establish the lower bounds of the Hellinger distance,
denoted as h(·,·), of the mixing densities of softmax gating Gaussian mixture of experts in terms of
these V oronoi losses to capture the behaviors of the MLE. Our results can be summarized as follows
(see also Table 1):
1. Exact-fitted settings : When k=k∗, we demonstrate that the Hellinger lower bound
EX[h(gG(·|X), gG∗(·|X))]≥C· D1(G, G∗)holds for any mixing measure G∈ Ok(Θ), where C
is some universal constant and the V oronoi metric D1(G, G∗)is defined as:
D1(G, G∗) := inf
t1,t2k∗X
j=1"X
i∈Ajexp(β0i)∥(∆t2β1ij,∆aij,∆bij,∆σij)∥
+X
i∈Ajexp(β0i)−exp(β∗
0j+t1)#
,(4)
where ∆t2β1ij:=β1i−β∗
1j−t2,∆aij:=ai−a∗
j,∆bij:=bi−b∗
j,∆σij:=σi−σ∗
j. The
infimum over t1∈Randt2∈Rdis to account for the identifiability up to the translation of
(β∗
0j, β∗
1j)k∗
j=1. Furthermore, Ajis a V oronoi cell of mixing measure Ggenerated by the true
component ω∗
j:= (β∗
1j, a∗
j, b∗
j, σ∗
j)for all 1≤j≤k∗[47], which is defined as follows:
Aj≡ A j(G) :={i∈ {1,2, . . . , k }:∥ωi−ω∗
j∥ ≤ ∥ ωi−ω∗
ℓ∥,∀ℓ̸=j}, (5)
where we denote ωi:= (β1i, ai, bi, σi). It is worth noting that the cardinality of each V oronoi cell
Ajindicates the number of components of Gapproximating the true component ω∗
jofG∗. As
EX[h(gbGn(·|X), gG∗(·|X))] = O(n−1/2), that lower bound of Hellinger distance indicates that
3

--- PAGE 4 ---
D1(bGn, G∗) =O(n−1/2). Therefore, the rates of estimating exp(β∗
0j), β∗
1j(up to translations) and
a∗
j, β∗
j, σ∗
jare of optimal order O(n−1/2).
2. Over-fitted settings : When k > k ∗, the lower bound of Hellinger distance in terms of the V oronoi
metric D1in the exact-fitted settings is no longer sufficient due to the collapse of softmax of vectors
in possibly kdimensions to softmax of vectors in k∗dimensions. Our approach is to define more
fine-grained V oronoi metric D2(G, G∗)to capture such collapse, which is given by:
D2(G, G∗) := inf
t1,t2X
j:|Aj|>1X
i∈Ajexp(β0i)
∥(∆t2β1ij,∆bij)∥¯r(|Aj|)+∥(∆aij,∆σij)∥¯r(|Aj|)/2
+X
j:|Aj|=1X
i∈Ajexp(β0i)∥(∆t2β1ij,∆aij,∆bij,∆σij)∥+k∗X
j=1X
i∈Ajexp(β0i)−exp(β∗
0j+t1),(6)
for any mixing measure G∈ O k(Θ). Here, the values of function ¯r(·)are determined by the
solvability of a system of polynomial equations defined in equation (9). We then show in Lemma 1
that¯r(2) = 4 ,¯r(3) = 6 , and we conjecture that ¯r(m) = 2 mfor any m≥2.
In high level, the aforementioned system of polynomial equations arises from the PDEs in equa-
tion (3)when we establish the lower bound EX[h(gG(·|X), gG∗(·|X))]≥C′D2(G, G∗)for any
G∈ O k(Θ)for some universal constant C′. Since EX[h(gbGn(·|X), gG∗(·|X))] =O(n−1/2), we
also have D2(bGn, G∗) =O(n−1/2)under the over-fitted settings of the softmax gating Gaussian
mixture of experts. As a consequence, the rates for estimating true parameters whose V oronoi cells
have only one component of the MLE are of order O(n−1/2). On the other hand, for true parameters
exp(β∗
0j), β∗
1j, a∗
j, b∗
j, σ∗
jwhose V oronoi cells have more than one component of the MLE, the esti-
mation rates are respectively O(n−1/2¯r(|Aj|))forβ∗
1j, b∗
j,O(n−1/¯r(|Aj|))fora∗
j, σ∗
j, andO(n−1/2)
forexp(β∗
0j). This rich spectrum of parameter estimation rates is due to the complex interaction
between the softmax gating and the expert functions.
Setting Loss Function gG∗(Y|X)exp(β∗
0j) β∗
1j, b∗
j a∗
j, σ∗
j
Exact-fitted D1 O(n−1/2)O(n−1/2) O(n−1/2) O(n−1/2)
Over-fitted D2 O(n−1/2)O(n−1/2)O(n−1/2¯r(|Aj|))O(n−1/¯r(|Aj|))
Table 1: Summary of density estimation and parameter estimation rates in the softmax gating Gaussian
mixture of experts under both the exact-fitted and over-fitted settings. Recall that the cardinality
of each V oronoi cell Ajgives the number of fitted components approximating true component
ω∗
j= (β∗
1j, a∗
j, b∗
j, σ∗
j)(see equation (5)). Furthermore, the notation ¯r(|Aj|)stands for the solvability
of the system of polynomial equations (9). For instance, if ω∗
jis fitted by two components, then we
have|Aj|= 2and¯r(|Aj|) = 4 . Please refer to Lemma 1 for more details of the values of function ¯r.
Practical implication: Although the slow rates of the MLE under the over-fitted settings of the
softmax gating Gaussian mixture of experts may seem discouraging, a practical implication of these
results is that we should not choose the number of experts kto be very large compared to the true
number of experts k∗. Furthermore, the slow rates can also be useful for post-processing procedures,
such as merge-truncate-merge procedure [ 24], with the MLE to reduce the number of experts so as to
consistently estimate k∗when the number of data is sufficiently large. In particular, an important
insight from the theoretical results is that we can merge the MLE parameters that are close and within
the range of their rates of convergence or truncate the parameters that lead to small weights of the
experts. As the sample size becomes sufficiently large, the reduced number of experts may converge
to the true number of experts. We leave an investigation of such model selection with the Gaussian
mixture of experts via the rates of MLE for future work.
Organization: The paper is organized as follows. In Section 2, we first provide background on the
identifiability and rate of conditional density estimation in the softmax gating Gaussian mixture of
experts. Next, we proceed to establish the convergence rate of the MLE under both the exact-fitted and
over-fitted settings of these models in Section 3. Then, we conclude the paper with a few discussions
in Section 4. Finally, full proofs of the results and a simulation study are provided in the Appendices.
4

--- PAGE 5 ---
Notation: Firstly, we denote [n] :={1,2, . . . , n }for any positive integer n. Next, for any vector
u∈Rdandz:= (z1, z2, . . . , z d)∈Nd, we denote uz=uz1
1uz2
2. . . uzd
d,|u|:=u1+u2+. . .+ud
andz! :=z1!z2!. . . z d!, while ∥u∥represents for its 2-norm value. Additionally, the notation |A|
indicates the cardinality of any set A. Given any two positive sequences {an}n≥1and{bn}n≥1, we
write an=O(bn)oran≲bnifan≤Cbnfor all n∈N, where C >0is some universal constant.
Lastly, for any two probability density functions p, qdominated by the Lebesgue measure µ, we denote
h2(p, q) =1
2R
(√p−√q)2dµas the their squared Hellinger distance and V(p, q) =1
2R
|p−q|dµ
as their Total Variation distance.
2 Background
In this section, we begin with the following result on the identifiability of the softmax gating Gaussian
mixture of experts, which was previously studied in [36].
Proposition 1 (Identifiability of the softmax gating Gaussian mixture of experts) .For any mixing
measures G=Pk
i=1exp(β0i)δ(β1i,ai,bi,σi)andG′=Pk′
i=1exp(β′
0i)δ(β′
1i,a′
i,b′
i,σ′
i), if we have
gG(Y|X) =gG′(Y|X)for almost surely (X, Y), then it follows that k=k′andG≡G′
t1,t2where
G′
t1,t2:=Pk′
i=1exp(β′
0i+t1)δ(β′
1i+t2,a′
i,b′
i,σ′
i)for some t1∈Randt2∈Rd.
Proof of Proposition 1 is in Appendix B.1. The identifiability of the softmax gating Gaussian mixture
of experts guarantees that the MLE bGn(2)converges to the true mixing measure G∗(up to the
translation of the parameters in the softmax gating).
Given the consistency of the MLE, it is natural to ask about its convergence rate to the true parameters.
Our next result establishes the convergence rate of conditional density estimation gbGn(Y|X)to the
true conditional density gG∗(Y|X), which lays an important foundation for the study of MLE’s
convergence rate.
Proposition 2 (Density estimation rate) .Given the MLE in equation (2), the conditional density
estimation gbGn(Y|X)has the following convergence rate:
P(EX[h(gbGn(·|X), gG∗(·|X))]> C(log(n)/n)1/2)≲exp(−clogn),
where candCare universal constants.
Proof of Proposition 2 is in Appendix B.2. The result of Proposition 2 indicates that under either
the exact-fitted or over-fitted settings of the softmax gating Gaussian mixture of experts, the rate of
the conditional density function gbGn(Y|X)to the true one gG∗(Y|X)under Hellinger distance is of
orderO(n−1/2)(up to some logarithmic factors), which is parametric on the sample size.
From density estimation to parameter estimation: The parametric rate of the conditional density
estimation in Proposition 2 suggests that as long as we can establish the Hellinger lower bound
EX[h(gG(·|X), gG∗(·|X))]≳D(G, G∗)for any mixing measure G∈ O k(Θ)for some metric D
among the parameters, then we obtain directly the parametric convergence rate of the MLE under the
metric D. Therefore, the main focus of the next section is to determine such metric Dand to establish
that lower bound under either exact-fitted or over-fitted settings of the Gaussian mixture of experts.
3 Convergence Rate of the Maximum Likelihood Estimation
In this section, we first study the convergence rate of the MLE under the exact-fitted settings of the
softmax gating Gaussian mixture of experts in Section 3.1. Then, we move to the over-fitted settings
in Section 3.2. Finally, we provide a proof sketch of the theories in Section 3.3.
3.1 Exact-fitted Settings
For the exact-fitted settings, namely, when the chosen number of experts kis equal to the true number
of experts k∗, as we mentioned in the introduction, the proper metric between the MLE and the true
5

--- PAGE 6 ---
mixing measure is the metric D1defined in equation (4), which is given by:
D1(G, G∗) := inf
t1,t2k∗X
j=1"X
i∈Ajexp(β0i)∥(∆t2β1ij,∆aij,∆bij,∆σij)∥
+X
i∈Ajexp(β0i)−exp(β∗
0j+t1)#
,
where ∆t2β1ij:=β1i−β∗
1j−t2,∆aij:=ai−a∗
j,∆bij:=bi−b∗
j,∆σij:=σi−σ∗
j. Here, Ajis
a V oronoi cell of Ggenerated by (β∗
1j, a∗
j, b∗
j, σ∗
j)for all 1≤j≤k∗. Furthermore, the infimum is
taken with respect to (t1, t2)∈R×Rdsuch that β∗
0j+t1andβ∗
1j+t2still lie inside the domain of
the parameter space Θ.
It is clear that D1(G, G∗) = 0 if and only if G≡G∗(up to translation). When D1(G, G∗)is
sufficiently small, there exist t1, t2such that all of ∆t2β1ij,∆aij,∆bij,∆σij, andP
i∈Ajexp(β0i)−
exp(β∗
0j+t1)are sufficiently small as well. Therefore, the loss function D1provides a useful metric
to measure the difference between the MLE and the true mixing measure. For any fixed t1, t2,
the computation of the summations in D1only has the complexity of the order O(k2
∗). To solve
the optimization with respect to t1, t2in the metric D1, we can utilize the projected subgradient
method with fixed step size [ 4], which has the complexity of the order O(ε−2)as the functions of t1
andt2are convex where εis a desired tolerance. Therefore, the total computational complexity of
approximating the value of the V oronoi loss function D1is at the order of O(k2
∗/ε2).
The following result establishes the lower bound of the Hellinger distance between the conditional
densities in terms of the loss function D1between corresponding mixing measures, which in turn
leads to the convergence rate of the MLE.
Theorem 1. Given the exact-fitted settings of the softmax gating Gaussian mixture of experts (1), i.e.,
k=k∗, we find that
EX[h(gG(·|X), gG∗(·|X))]≥C1· D1(G, G∗), (7)
for any G∈ Ek∗(Θ) := Ok∗(Θ)\ Ok∗−1(Θ)where C1is some universal constant depending only
onG∗andΘ. As a consequence, there exist universal constants C′
1andc1such that the convergence
rate of the MLE bGnunder the exact-fitted settings satisfies:
P(D1(bGn, G∗)> C′
1(log(n)/n)1/2)≲exp(−c1logn). (8)
Proof of Theorem 1 is in Appendix A.1. The parametric convergence rate of the MLE to G∗under the
metric D1suggests that the rates of estimating the true parameters exp(β∗
0j), β∗
1j(up to translation),
a∗
j,b∗
j, σ∗
jforj∈[k∗]are of order O(n−1/2), which are optimal up to logarithmic factors.
3.2 Over-fitted Settings
We now consider the over-fitted settings of the softmax gating Gaussian mixture of experts. Dif-
ferent from the exact-fitted settings, the softmax weights associated with the MLE collapse to
the softmax weights of the mixture of true experts as long as the MLE approaches the true mix-
ing measure G∗. More concretely, we can relabel the supports of the MLE bGnwith ˆkncom-
ponents ( ˆkn≤k) based on the V oronoi cells An
j:=Aj(bGn)such that we can rewrite it as
bGn=Pk∗
j=1P
i∈An
jexp(bβn
0i)δ(bβn
1i,ban
i,bbn
i,bσn
i)wherePk∗
j=1|An
j|=ˆkn,(ban
i,bbn
i,bσn
i)→(a∗
j, b∗
j, σ∗
j),
X
i∈An
jexp((bβn
1i)⊤X+bβn
0i)
Pk∗
j′=1P
i′∈An
j′exp((bβn
1i′)⊤X+bβn
0i′)→exp(( β∗
1j)⊤X+β∗
0j)
Pk∗
j′=1exp(( β∗
1j′)⊤X+β∗
0j′)
asnapproaches infinity for all 1≤i≤ An
jandj∈[k∗].
The collapse of the softmax weights along with the PDEs (3)between the softmax gating and the
expert functions in the Gaussian density create a complex interaction among the estimated parameters.
To disentangle such interaction, we rely on the solvability of a novel system of polynomial equations
6

--- PAGE 7 ---
defined in equation (9). In particular, for any m≥2, we define ¯r(m)as the smallest natural number
rsuch that the following system of polynomial equations:
mX
j=1X
(α1,α2,α3,α4)∈Iℓ1,ℓ2p2
5jpα1
1jpα2
2jpα3
3jpα4
4j
α1!α2!α3!α4!= 0, (9)
for any (ℓ1, ℓ2)∈Nd×Nsuch that 0≤ |ℓ1| ≤r,0≤ℓ2≤r−|ℓ1|and|ℓ1|+ℓ2≥1, does not have
any non-trivial solution for the unknown variables {p1j, p2j, p3j, p4j, p5j}m
j=1, namely, all of p5jare
non-zero and at least one among p3jis different from zero. The ranges of α1, α2, α3, α4in the above
sum satisfy Iℓ1,ℓ2={α= (α1, α2, α3, α4)∈Nd×Nd×N×N:α1+α2=ℓ1,|α2|+α3+2α4=
ℓ2}. When d= 1andr= 2, that system of equations becomes
mX
j=1p2
5jp1j= 0,mX
j=1p2
5jp2
1j= 0,mX
j=1p2
5j(p1jp3j+p2j) = 0 ,
mX
j=1p2
5jp3j= 0,mX
j=1p2
5j1
2p2
3j+p4j
= 0.
It is clear that we have non-trivial solutions p5j= 1,p1j= 0 for all j∈[m],|p21|=p31= 1,
|p22|=p32=−1,p41=p42=−1/2,p2j=p3j=p4j= 0for3≤j≤m.
When d= 1andr= 3, the system of equations can be written as follows:
mX
j=1p2
5jp1j= 0,mX
j=1p2
5jp3j= 0,mX
j=1p2
5j(p2j+p1jp3j) = 0 ,
mX
j=1p2
5jp2
1j= 0,mX
j=1p2
5j1
2p2
3j+p4j
= 0,mX
j=1p2
5j1
3!p3
3j+p3jp4j
= 0,
mX
j=1p2
5jp3
1j= 0,mX
j=1p2
5j1
2p2
1jp3j+p1jp2j
= 0,
mX
j=1p2
5j1
2p1j·p2
3j+p1jp4j+p2jp3j
= 0.
It can be seen that the following is a non-trivial solution of the above system: p5j= 1,p1j=p2j= 0
for all j∈[m],p31=√
3
3,p32=−√
3
3,p41=p42=−1
6,p3j=p4j= 0for3≤j≤m. Therefore,
we obtain that ¯r(m)≥4when m≥2andd= 1.
In general, when d= 1, the system of equations has (r2+ 3r)/2equations. Intuitively, when m
is sufficiently larger than (r2+ 3r)/2, the system may not have a non-trivial solution. For general
dimension dand parameter m≥2, finding the exact value of ¯r(m)is a non-trivial central problem in
algebraic geometry [55]. When mis small, the following lemma provides specific values for ¯r(m).
Lemma 1. For any d≥1, when m= 2,¯r(m) = 4 . When m= 3,¯r(m) = 6 .
Proof of Lemma 1 is in Appendix B.3. As mincreases, so does the value of ¯r(m). We conjecture
that¯r(m) = 2 mand leave the proof of that conjecture to future work.
By constructing the V oronoi loss function:
D2(G, G∗) := inf
t1,t2X
j:|Aj|>1X
i∈Ajexp(β0i)
∥(∆t2β1ij,∆bij)∥¯r(|Aj|)+∥(∆aij,∆σij)∥¯r(|Aj|)/2
+X
j:|Aj|=1X
i∈Ajexp(β0i)∥(∆t2β1ij,∆aij,∆bij,∆σij)∥+k∗X
j=1X
i∈Ajexp(β0i)−exp(β∗
0j+t1),
the following result demonstrates that the convergence rates of the MLE under the over-fitted settings
of the softmax gating Gaussian mixture of experts are determined by ¯r(·).
7

--- PAGE 8 ---
Theorem 2. Under the over-fitted settings of the softmax gating Gaussian mixture of experts (1),
namely, when k > k ∗, we obtain that
EX[h(gG(·|X), gG∗(·|X))]≥C2· D2(G, G∗), (10)
for any G∈ Ok(Θ)where C2is some universal constant depending only on G∗andΘ. Therefore,
that lower bound leads to the following convergence rate of the MLE:
P(D2(bGn, G∗)> C′
2(log(n)/n)1/2)≲exp(−c2logn), (11)
where C′
2andc2are some universal constants.
Proof of Theorem 2 is in Appendix A.2. A few comments with the result of Theorem 2 are in order.
(i) Rates of individual parameters: The convergence rate O(n−1/2)(up to some logarithmic term)
of the MLE under the loss function D2implies that for the true parameters exp(β∗
0j), β∗
1j, a∗
j, b∗
j, σ∗
j
whose V oronoi cells have only one component of the MLE, the rates for estimating them are O(n−1/2)
up to some logarithmic factor. On the other hand, for true parameters with greater than one component
in their V oronoi cells, the rates for estimating β∗
1j,b∗
jareO(n−1/2¯r(|An
j|))while those for a∗
j, σ∗
j
areO(n−1/¯r(|An
j|))(up to logarithmic factors). As the maximum value of |An
j|isˆkn−k∗+ 1,
it indicates that these rates (up to logarithmic factors) can be as worse as O(n−1/¯r(ˆkn−k∗+1))for
estimating a∗
j, σ∗
jandO(n−1/2¯r(ˆkn−k∗+1))for estimating β∗
1j, b∗
j.
(ii) Computation of Voronoi loss function D2:Similar to the V oronoi loss function D1in the
exact-fitted setting, the loss function D2is also computationally efficient. In particular, for any fixed
t1, t2, the computation of the summations in the formulation of D2is at the order O(k×k∗), which is
linear on kwhen k∗is fixed. Furthermore, we can solve the convex optimization problem with respect
tot1, t2with computational complexity at the order of O(ε−2)via the projected gradient descent
method with fixed step size where εis the error. Therefore, the total computational complexity of
approximating the V oronoi loss function D2is at the order of O(k×k∗/ε2).
(iii) Comparison with covariate-free gating network: We would like to remark that the results
being established for parameter estimation under the softmax gating network settings of over-fitted
Gaussian mixture of experts are in stark difference from those under the covariate-free gating network
settings of these models [ 30], namely, when the gating function is independent of the covariates X. In
particular, Theorem 2 in [ 30] shows that when the gating networks are independent of the covariates,
the convergence rates of estimating a∗
jare at the order of O(n−1/4)(up to some logarithmic factor),
which are independent of the number of over-fitted components. It is different from the rates of a∗
j
whose V oronoi cells have more than one component in the softmax gating settings, which depends on
the number of components that we over-fit the Gaussian mixture of experts (see discussion (i) after
Theorem 2). Furthermore, the rates of estimating b∗
j, σ∗
jwhen the gating networks are independent of
covariates are determined by a system of polynomial equations that is much simpler than the system
of equations (9)when the gating networks are softmax function. These differences are mainly due to
the intrinsic interaction characterized by partial differential equations with respect to the parameters
between the softmax gating networks and the expert functions in Gaussian distribution.
3.3 Proof Sketch
In this section, we provide a proof sketch for Theorems 1 and 2. To simplify the ensuing discussion,
the loss function Din the proof sketch is implicitly understood as either the loss function D1orD2
depending on the settings of the softmax gating Gaussian mixture of experts. To obtain the bound of
Hellinger distance between gGandgG∗in terms of D(G, G∗), it is sufficient to consider the lower
bound of the Total Variation distance EX[V(gG(·|X), gG∗(·|X))]in terms of D(G, G∗). To establish
this bound, we respectively prove its local and global versions by contradiction as follows:
Local version : In this part, we aim to show the following local inequality:
lim
ε→0inf
G∈Ok(Θ),D(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]/D(G, G∗)>0. (12)
Assume that this claim does not hold true, that is, there exists a sequence Gn=Pkn
i=1exp(βn
0i)δ(βn
1i,an
i,bn
i,σn
i)∈ O k(Θ) such that both EX[V(gGn(·|X), gG∗(·|X))]/D(Gn, G∗)
8

--- PAGE 9 ---
andD(Gn, G∗)approach zero as ntends to infinity. This implies that for any j∈[k∗], we haveP
i∈Ajexp(βn
0i)→exp(β∗
0j)and(βn
1i, an
i, bn
i, σn
i)→(β∗
1j, a∗
j, b∗
j, σ∗
j)and for all i∈ A j. For the
sake of presentation, we simplify the loss function Dby assuming that it is minimized when t1= 0
andt2=0d. Now, we decompose the quantity Qn= [Pk∗
j′=1exp(( β∗
1j′)⊤X+β∗
0j′)]·[gGn(Y|X)−
gG∗(Y|X)]as follows:
Qn=k∗X
j=1X
i∈Ajexp(βn
0i)h
u(Y|X;βn
1i, an
i, bn
i, σn
i)−u(Y|X;β∗
1j, a∗
j, b∗
j, σ∗
j)−v(Y|X;βn
1i)
+v(Y|X;β∗
1j)i
+k∗X
j=1X
i∈Ajexp(βn
0i)−exp(β∗
0j)h
u(Y|X;β∗
0j, a∗
j, b∗
j, σ∗
j)−v(Y|X;β∗
1j)i
,
where we define u(Y|X;β1, a, b, σ ) := exp( β⊤
1X)f(Y|a⊤X+b, σ)andv(Y|X;β1) :=
exp(β⊤
1X)gGn(Y|X). Next, for each j∈[k∗]andi∈ Aj, we denote h1(X, a∗
j, b∗
j) := ( a∗
j)⊤X+b∗
j
and then apply the Taylor expansions to the functions u(Y|X;βn
1i, an
i, bn
i, σn
i)andv(Y|X;βn
1i)up
to orders r1jandr2j(which we will choose later), respectively, as follows:
u(Y|X;βn
1i, an
i, bn
i, σn
i)−u(Y|X;β∗
1j, a∗
j, b∗
j, σ∗
j)
=2r1jX
|ℓ1|+ℓ2=1Tn
ℓ1,ℓ2(j)Xℓ1exp(( β∗
1j)⊤X)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) +R1ij(X, Y),
v(Y|X;βn
1i)−v(Y|X;β∗
1j) =r2jX
|γ|=1Sn
γ(j)Xγexp(( β∗
1j)⊤X)gGn(Y|X) +R2ij(X, Y),
where R1ij(X, Y)andR2ij(X, Y)are Taylor remainders such that Rρij(X, Y)/D(Gn, G∗)vanishes
asn→ ∞ forρ∈ {1,2}. As a result, the limit of Qn/D(Gn, G∗)when ngoes to infinity can be
seen as a linear combination of elements of the following set:
W: =(
Xℓ1exp(( β∗
1j)⊤X)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) :j∈[k∗],0≤2|ℓ1|+ℓ2≤2r1j)
∪
Xγexp(( β∗
1j)⊤X)gG∗(Y|X) :j∈[k∗],0≤ |γ| ≤r2j	
,
which is shown to be linearly independent. By the Fatou’s lemma, we demonstrate that
Qn/D(Gn, G∗)goes to zero as n→ ∞ , implying that all the coefficients in the representation
ofQn/D(Gn, G∗), denoted by Tn
ℓ1,ℓ2(j)/D(Gn, G∗)andSn
γ(j)/D(Gn, G∗), vanish when n→ ∞ .
Given that result, we aim to select the Taylor orders r1jandr2jsuch that at least one among the
limits of Tn
ℓ1,ℓ2(j)/D(Gn, G∗)andSn
γ(j)/D(Gn, G∗)is different from zero, which leads to a con-
tradiction. Hence, we obtain the local version of the desired inequality. Below are the details of
choosing appropriate Taylor orders in each setting.
Exact-fitted settings: Under this setting, since k∗is known, each of the V oronoi cells Ajfor
j∈[k∗]has only one element. Thus, for any i∈ A j, we have exp(βn
0i)→exp(β∗
0j)and
(βn
1i, an
i, bn
i, σn
i)→(β∗
1j, a∗
j, b∗
j, σ∗
j). Given that result, we will select r1j=r2j= 1 for all
j∈[k∗]as it suffices to show that at least one among the limits of Tn
ℓ1,ℓ2(j)/D(Gn, G∗)and
Sn
γ(j)/D(Gn, G∗)is different from zero. In particular, if all of them vanished, we would take the
sum of all the limits of Tn
ℓ1,ℓ2(j)/D(Gn, G∗)for(ℓ1, ℓ2)such that 0≤2|ℓ1|+ℓ2≤2, which leads
to a contradiction that 1 =D(Gn, G∗)/D(Gn, G∗)→0.
Over-fitted settings: Ask∗becomes unknown in this scenario, we need higher Taylor orders to
obtain the same result as in the exact-fitted setting. We will reuse the proof by contradiction method
to find out those orders. More specifically, assume that all the limits of Tn
ℓ1,ℓ2(j)/D(Gn, G∗)and
Sn
γ(j)/D(Gn, G∗)equal zero. After some steps of considering typical limits as in the previous
setting which requires r2j= 2for all j∈[k∗], we encounter the following system of polynomial
equations:
X
i∈AjX
(α1,α2,α3,α4)∈Iℓ1,ℓ2p2
5ipα1
1ipα2
2ipα3
3ipα4
4i
α1!α2!α3!α4!= 0,
9

--- PAGE 10 ---
for all (ℓ1, ℓ2)∈Nd×Nsuch that 0≤ |ℓ1| ≤r1j,0≤ℓ2≤r1j− |ℓ1|and|ℓ1|+ℓ2≥1
for some j∈[k∗]. Due to the construction of this system, it must have at least one non-trivial
solution. Therefore, if we choose r1j= ¯r(|Aj|)for all j∈[k∗], then the above system does
not admit any non-trivial solutions, which leads to a contradiction. Hence, we obtain the lo-
cal inequality in equation (12), which suggests that we can find a positive constant ε′such that
infG∈Ok(Θ),D(G,G∗)≤ε′EX[V(gG(·|X), gG∗(·|X))]/D(G, G∗)>0.
Global version: Therefore, it is sufficient to demonstrate the following global inequality:
inf
G∈Ok(Θ),D(G,G∗)>ε′EX[V(gG(·|X), gG∗(·|X))]/D(G, G∗)>0. (13)
Assume that this claim is not true, then we can find a mixing measure G′∈ O k(Θ) such that
gG′(Y|X) = gG∗(Y|X)for almost surely (X, Y). According to Proposition 1, we get that
D(G′, G∗) = 0 , which contradicts the hypothesis D(G′, G∗)> ε′. These arguments hold for
both exact-fitted and over-fitted settings up to some changes of notations.
Hence, the proof sketch is completed.
4 Discussion
In the paper, we study the convergence rates of parameter estimation under both the exact-fitted and
over-fitted settings of the softmax gating Gaussian mixture of experts. We introduce novel V oronoi
loss functions among parameters to resolve fundamental theoretical challenges posed by the softmax
gating function, including identifiability up to the translation of parameters, the interaction between
softmax weights and expert functions, and the dependence between the numerator and denominator
of the conditional density function. When the true number of experts is known, we demonstrate
that the rates for estimating true parameters are parametric on the sample size. On the other hand,
when the true number of experts is unknown and over-specified, these estimation rates turn out to be
determined by the solvability of a system of polynomial equations.
There are a few natural directions arising from the paper that we leave for furture work:
First, our work does not consider the top-K sparse softmax gating function, which has been widely
used to scale up massive deep learning architectures [ 68,54,21]. It is practically important to extend
the current theories to establish the convergence rates of parameter estimation in the Gaussian mixture
of experts with that gating function.
Second, the paper only takes into account the regression settings, namely when the distribution of
Yis assumed to be continuous. Given that mixture of experts has also been used in classification
settings [ 22,31,53,34,35,60], namely when Yis a discrete response variable, it is desirable to
establish a comprehensive theory for parameter estimation under these settings of mixtures of experts.
Third, the theories developed in the paper lay an important foundation for understanding parameter
estimation in more complex models, including hierarchical mixture of experts [ 33,51,37,66] and
multigate mixture of experts [44, 26, 45].
Finally, the convergence rates of the MLE in this work are established under the well-specified settings,
namely when the data are drawn from the softmax gating Gaussian mixture of experts. Nevertheless,
the convergence analysis of the MLE under the misspecified settings, namely when the data are not
necessarily generated from that model, has remained poorly understood. Under those settings, the
MLEbGnconverges to the mixing measures G∈arg minG∈Ok(Θ)KL(gG(Y|X), p(Y|X))where
p(Y|X)is the true conditional density function of Ygiven X, and it is not a softmax gating Gaussian
mixture of experts. Additionally, the notation KLstands for the Kullback-Leibler divergence. The
insights from our theories in the well-specified setting indicate that the V oronoi loss functions can be
used to obtain the precise rates of individual parameters of the MLE bGnto those of G.
Acknowledgements
NH acknowledges support from the NSF IFML 2019844 and the NSF AI Institute for Foundations of
Machine Learning.
10

--- PAGE 11 ---
References
[1]A. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and
hidden markov models. In COLT , 2012.
[2]S. Balakrishnan, M. J. Wainwright, and B. Yu. Statistical guarantees for the EM algorithm:
From population to sample-based analysis. Annals of Statistics , 45:77–120, 2017.
[3]H. Bao, W. Wang, L. Dong, Q. Liu, O.-K. Mohammed, K. Aggarwal, S. Som, S. Piao, and
F. Wei. VLMo: Unified vision-language pre-training with mixture-of-modality-experts. In
Advances in Neural Information Processing Systems , 2022.
[4] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, 2004.
[5]F. Chamroukhi and B. T. Huynh. Regularized Maximum-Likelihood Estimation of Mixture-
of-Experts for Regression and Clustering. In 2018 International Joint Conference on Neural
Networks (IJCNN) , pages 1–8, 2018.
[6]F. Chamroukhi and B.-T. Huynh. Regularized Maximum Likelihood Estimation and Feature
Selection in Mixtures-of-Experts Models. Journal de la Société Française de Statistique ,
160(1):57–85, 2019.
[7]F. Chamroukhi, A. Samé, G. Govaert, and P. Aknin. Time series modeling by a regression
approach based on a latent process. Neural Networks , 22(5-6):593–602, 2009. Publisher:
Elsevier.
[8]F. Chamroukhi, A. Samé, G. Govaert, and P. Aknin. A hidden process regression model for
functional data description. Application to curve discrimination. Neurocomputing , 73(7-9):1210–
1221, 2010.
[9]J. H. Chen. Optimal rate of convergence for finite mixture models. Annals of Statistics ,
23(1):221–233, 1995.
[10] K. Chen, L. Xu, and H. Chi. Improved learning algorithms for mixture of experts in multiclass
classification. Neural Networks , 12(9):1229–1252, 1999.
[11] Y . Chen, X. Yi, and C. Caramanis. A convex formulation for mixed regression with two
components: Minimax optimal rates. In COLT , 2014.
[12] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from Incomplete Data
Via the EM Algorithm. Journal of the Royal Statistical Society: Series B (Methodological) ,
39(1):1–22, Sept. 1977.
[13] D. Do, L. Do, and X. Nguyen. Strong identifiability and parameter learning in regression with
heterogeneous response. arXiv preprint arXiv:2212.04091 , 2022.
[14] T. G. Do, H. K. Le, T. Nguyen, Q. Pham, B. T. Nguyen, T.-N. Doan, C. Liu, S. Ramasamy,
X. Li, and S. HOI. HyperRouter: Towards Efficient Training and Inference of Sparse Mixture
of Experts. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language
Processing , Singapore, Dec. 2023. Association for Computational Linguistics.
[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16
words: Transformers for image recognition at scale. In International Conference on Learning
Representations , 2021.
[16] N. Doss, Y . Wu, P. Yang, and H. H. Zhou. Optimal estimation of high-dimensional Gaussian
location mixtures. The Annals of Statistics , 51(1):62 – 95, 2023. Publisher: Institute of
Mathematical Statistics.
[17] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun, Y . Zhou, A. Yu, O. Firat,
B. Zoph, L. Fedus, M. Bosma, Z. Zhou, T. Wang, E. Wang, K. Webster, M. Pellat, K. Robinson,
K. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. Le, Y . Wu, Z. Chen, and C. Cui. Glam:
Efficient scaling of language models with mixture-of-experts. In ICML , 2022.
[18] R. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan, and B. Yu. Sharp analysis of
expectation-maximization for weakly identifiable models. AISTATS , 2020.
[19] R. Dwivedi, N. Ho, K. Khamaru, M. J. Wainwright, M. I. Jordan, and B. Yu. Singularity,
misspecification, and the convergence rate of EM. Annals of Statistics , 44:2726–2755, 2020.
11

--- PAGE 12 ---
[20] D. Eigen, M. Ranzato, and I. Sutskever. Learning factored representations in a deep mixture of
experts. In ICLR Workshops , 2014.
[21] W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models
with simple and efficient sparsity. Journal of Machine Learning Research , 23:1–39, 2022.
[22] I. C. Gormley and T. B. Murphy. A mixture of experts model for rank data with applications in
election studies. The Annals of Applied Statistics , 2(4):1452 – 1477, 2008. Publisher: Institute
of Mathematical Statistics.
[23] P. J. Green. Iteratively Reweighted Least Squares for Maximum Likelihood Estimation, and
some Robust and Resistant Alternatives. Journal of the Royal Statistical Society. Series B
(Methodological) , 46(2):149–192, 1984. Publisher: [Royal Statistical Society, Wiley].
[24] A. Guha, N. Ho, and X. Nguyen. On posterior contraction of parameters and interpretability in
Bayesian mixture modeling. Bernoulli , 27(4):2159–2188, 2021.
[25] M. Hardt and E. Price. Tight bounds for learning a mixture of two gaussians. In STOC , 2015.
[26] H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoorthy, Y . Chen, R. Mazumder, L. Hong,
and E. Chi. DSelect-k: Differentiable Selection in the Mixture of Experts with Applications
to Multi-Task Learning. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P. S. Liang, and J. W.
Vaughan, editors, Advances in Neural Information Processing Systems , volume 34, pages
29335–29347. Curran Associates, Inc., 2021.
[27] P. Heinrich and J. Kahn. Strong identifiability and optimal minimax rates for finite mixture
estimation. The Annals of Statistics , 46(6):2844–2870, 2018.
[28] N. Ho and X. Nguyen. Convergence rates of parameter estimation for some weakly identifiable
finite mixtures. Annals of Statistics , 44:2726–2755, 2016.
[29] N. Ho and X. Nguyen. On strong identifiability and convergence rates of parameter estimation
in finite mixtures. Electronic Journal of Statistics , 10:271–307, 2016.
[30] N. Ho, C.-Y . Yang, and M. I. Jordan. Convergence rates for Gaussian mixtures of experts.
Journal of Machine Learning Research , 23(323):1–81, 2022.
[31] B. T. Huynh and F. Chamroukhi. Estimation and feature selection in mixtures of generalized
linear experts models. arXiv preprint arXiv:1907.06994 , 2019.
[32] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.
Neural Computation , 3, 1991.
[33] R. A. Jacobs, F. Peng, and M. A. Tanner. A Bayesian Approach to Model Selection in
Hierarchical Mixtures-of-Experts Architectures. Neural Networks , 10(2):231–241, 1997.
[34] W. Jiang and M. A. Tanner. Hierarchical Mixtures-of-Experts for Generalized Linear Models:
Some Results on Denseness and Consistency. In D. Heckerman and J. Whittaker, editors,
Proceedings of the Seventh International Workshop on Artificial Intelligence and Statistics ,
volume R2 of Proceedings of Machine Learning Research . PMLR, Jan. 1999.
[35] W. Jiang and M. A. Tanner. On the Approximation Rate of Hierarchical Mixtures-of-Experts
for Generalized Linear Models. Neural Computation , 11(5):1183–1198, July 1999.
[36] W. Jiang and M. A. Tanner. On the identifiability of mixtures-of-experts. Neural Networks ,
9:1253–1258, 1999.
[37] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural
Computation , 6:181–214, 1994.
[38] B. Krishnapuram, L. Carin, M. Figueiredo, and A. Hartemink. Sparse multinomial logistic
regression: fast algorithms and generalization bounds. IEEE Transactions on Pattern Analysis
and Machine Intelligence , 27(6):957–968, 2005.
[39] M. Kuusela, T. Vatanen, E. Malmi, T. Raiko, T. Aaltonen, and Y . Nagai. Semi-supervised
anomaly detection – towards model-independent searches of new physics. Journal of Physics:
Conference Series , 368:012032, 2011.
[40] J. Kwon and C. Caramanis. EM Converges for a Mixture of Many Linear Regressions. In
S. Chiappa and R. Calandra, editors, Proceedings of the Twenty Third International Conference
on Artificial Intelligence and Statistics , volume 108 of Proceedings of Machine Learning
Research , pages 1727–1736. PMLR, Aug. 2020.
12

--- PAGE 13 ---
[41] J. Kwon, N. Ho, and C. Caramanis. On the minimax optimality of the EM algorithm for learning
two-component mixed linear regression. In AISTATS , 2021.
[42] J. Kwon, W. Qian, C. Caramanis, Y . Chen, and D. Davis. Global Convergence of the EM
Algorithm for Mixtures of Two Component Linear Regression. In A. Beygelzimer and D. Hsu,
editors, Proceedings of the Thirty-Second Conference on Learning Theory , volume 99 of
Proceedings of Machine Learning Research , pages 2055–2110. PMLR, June 2019.
[43] Q. Li, R. Shi, , and F. Liang. Drug sensitivity prediction with high-dimensional mixture
regression. PLoS ONE , 2019.
[44] H. Liang, Z. Fan, R. Sarkar, Z. Jiang, T. Chen, K. Zou, Y . Cheng, C. Hao, and Z. Wang.
M3ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-
Accelerator Co-design. In NeurIPS , 2022.
[45] J. Ma, Z. Zhao, X. Yi, J. Chen, L. Hong, and E. H. Chi. Modeling Task Relationships in
Multi-Task Learning with Multi-Gate Mixture-of-Experts. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining , KDD ’18, pages
1930–1939, New York, NY , USA, 2018. Association for Computing Machinery. event-place:
London, United Kingdom.
[46] T. Manole and N. Ho. Uniform convergence rates for maximum likelihood estimation under
two-component gaussian mixture models. arXiv preprint arXiv:2006.00704 , 2020.
[47] T. Manole and N. Ho. Refined convergence rates for maximum likelihood estimation under finite
mixture models. In Proceedings of the 39th International Conference on Machine Learning ,
volume 162 of Proceedings of Machine Learning Research , pages 14979–15006. PMLR, 17–23
Jul 2022.
[48] L. Montuelle and E. Le Pennec. Mixture of Gaussian regressions model with logistic weights,
a penalized maximum likelihood approach. Electronic Journal of Statistics , 8(1):1661–1695,
2014.
[49] B. Mustafa, C. Ruiz, J. Puigcerver, R. Jenatton, and N. Houlsby. Multimodal contrastive learning
with limoe: the language-image mixture of experts. In NeurIPS , 2022.
[50] X. Nguyen. Convergence of latent mixing measures in finite and infinite mixture models. Annals
of Statistics , 4(1):370–400, 2013.
[51] F. Peng, R. A. Jacobs, and M. A. Tanner. Bayesian Inference in Mixtures-of-Experts and
Hierarchical Mixtures-of-Experts Models With an Application to Speech Recognition. Journal
of the American Statistical Association , 91(435):953–960, 1996.
[52] C. Ruiz, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Pinto, D. Keysers, and
N. Houlsby. Scaling vision with sparse mixture of experts. In NeurIPS , 2021.
[53] M. E. Ruiz and P. Srinivasan. Hierarchical Text Categorization Using Neural Networks. Infor-
mation Retrieval , 5(1):87–118, Jan. 2002.
[54] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously
large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference
on Learning Representations (ICLR) , 2017.
[55] B. Sturmfels. Solving Systems of Polynomial Equations . Providence, R.I, 2002.
[56] H. Teicher. On the mixture of distributions. Annals of Statistics , 31:55–73, 1960.
[57] H. Teicher. Identifiability of mixtures. Annals of Statistics , 32:244–248, 1961.
[58] S. van de Geer. Empirical processes in M-estimation . Cambridge University Press, 2000.
[59] C. Villani. Optimal transport: Old and New . Springer, 2008.
[60] S. Waterhouse and A. Robinson. Classification using hierarchical mixtures of experts. In
Proceedings of IEEE Workshop on Neural Networks for Signal Processing , pages 177–186,
1994.
[61] Y . Wu and P. Yang. Optimal estimation of Gaussian mixtures via denoised method of moments.
The Annals of Statistics , 48:1987–2007, 2020.
[62] Y . Wu and H. H. Zhou. Randomly initialized EM algorithm for two-component Gaussian
mixture achieves near optimality in o(√n)iterations. Mathematical Statistics and Learning ,
4:143–220, 2021.
13

--- PAGE 14 ---
[63] X. Yi, C. Caramanis, and S. Sanghavi. Alternating minimization for mixed linear regression. In
ICML , 2014.
[64] Z. You, S. Feng, D. Su, and D. Yu. Speechmoe: Scaling to large acoustic models with dynamic
routing mixture of experts. In Interspeech , 2021.
[65] Z. You, S. Feng, D. Su, and D. Yu. Speechmoe2: Mixture-of-experts model with improved
routing. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP) , pages 7217–7221, 2022.
[66] Y . Zhao, R. Schwartz, J. Sroka, and J. Makhoul. Hierarchical mixtures of experts methodology
applied to continuous speech recognition. In Advances in Neural Information Processing
Systems , volume 7, 1994.
[67] K. Zhong, P. Jain, and I. S. Dhillon. Mixed linear regression with multiple components. In
NeurIPS , 2016.
[68] Y . Zhou, T. Lei, H. Liu, N. Du, Y . Huang, V . Y . Zhao, A. M. Dai, Z. Chen, Q. V . Le, and J. Laudon.
Mixture-of-Experts with Expert Choice Routing. In A. H. Oh, A. Agarwal, D. Belgrave, and
K. Cho, editors, Advances in Neural Information Processing Systems , 2022.
14

--- PAGE 15 ---
In this supplementary material, we present the proofs of Theorems 1 and 2 in Appendix A, and then
provide proofs for the remaining results in Appendix B. Finally, we carry out a simulation study to
illustrate the various convergence rates that were derived in Theorems 1 and 2 in Appendix C.
A Proofs of Main Results
In this appendix, we provide proof for Theorem 1 in Appendix A.1, while leave that for Theorem 2 in
Appendix A.2. Prior to discussing in more detail, let us recall some notations for high-dimensional
settings that we will use in our proofs. Firstly, for any vector u:= (u1, u2, . . . , u d)∈Rdand
z:= (z1, z2, . . . , z d)∈Nd, we denote uz=uz1
1uz2
2. . . uzd
d,|u|:=u1+u2+. . .+udand
z! :=z1!z2!. . . z d!. Additionally, 0ddenotes the vector zero in Rd, whereas the notation 1stands for
the indicator function. Finally, we denote h1(X, a, b ) =a⊤X+bandh2(X, σ) =σas mean and
variance expert functions in this work for any (a, b, σ )∈Rd×R×R+andX∈ X.
A.1 Proof of Theorem 1
General Picture : In this proof, we focus mainly on establishing the following bound:
EX[V(gG(·|X), gG∗(·|X))]≥C1· D1(G, G∗). (14)
Then, the above bound together with the result of Proposition 2 lead to the conclusion of Theorem 1.
Local version : Firstly, we prove the local version of the inequality (14):
lim
ε→0inf
G∈Ek∗(Θ),
D1(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]/D1(G, G∗)>0. (15)
Suppose that the inequality in equation (15) does not hold true, then we can find a sequence Gn:=Pk∗
i=1exp(βn
0i)δ(βn
1i,an
i,bn
i,σn
i)∈ Ek∗(Θ)such that
EX[V(gGn(·|X), gG∗(·|X))]/D1(Gn, G∗)→0,
D1(Gn, G∗)→0,
asn→ ∞ . Next, we consider the V oronoi cells An
j:=Aj(Gn), forj∈[k∗], of the mixing measure
Gngenerated by the true components of G∗. Since the argument in this proof is asymptotic, we
assume without loss of generality (WLOG) that those V oronoi cells are independent of nfor all n∈N,
i.e.Aj=An
j. Additionally, since k∗is known under the exact-fitted settings and D1(Gn, G∗)→0,
the V oronoi cell Ajhas only one element for any j∈[k∗]. WLOG, we assume that Aj={j}for
allj∈[k∗], which follows that (an
j, bn
j, σn
j)→(a∗
j, b∗
j, σ∗
j)asn→ ∞ . Furthermore, there exist
t1∈Randt2∈Rdindependent of nsuch that exp(βn
0j)→exp(β∗
0j+t1)andβn
1j→β∗
1j+t2asn
approaches infinity for all j∈[k∗]. It indicates that we can upper bound the V oronoi loss function D1
asD1(Gn, G∗)≤ D′
1(Gn, G∗), where
D′
1(Gn, G∗) :=k∗X
j=1
exp(βn
0j)∥(∆t2βn
1j,∆an
j,∆bn
j,∆σn
j)∥+exp(βn
0j)−exp(β∗
0j+t1)
,
in which
∆t2βn
1j:=βn
1j−β∗
1j−t2,∆an
j:=an
j−a∗
j,
∆bn
j:=bn
j−b∗
j,∆σn
j:=σn
j−σ∗
j.
AsEX[V(gGn(·|X), gG∗(·|X))]/D1(Gn, G∗)→0when n→ ∞ , we also obtain that
EX[V(gGn(·|X), gG∗(·|X))]/D′
1(Gn, G∗)→0.
15

--- PAGE 16 ---
Step 1: Density Decomposition
Subsequently, we consider Qn:= [Pk∗
j=1exp(( β∗
1j+t2)⊤X+β∗
0j+t1)]·[gGn(Y|X)−gG∗(Y|X)],
which can decomposed as
Qn=k∗X
j=1exp(βn
0j)h
u(Y|X;βn
1j, an
j, bn
j, σn
j)−u(Y|X;β∗
1j+t2, a∗
j, b∗
j, σ∗
j)i
−k∗X
j=1exp(βn
0j)h
v(Y|X;βn
1j)−v(Y|X;β∗
1j+t2)i
+k∗X
j=1
exp(βn
0j)−exp(β∗
0j+t1)h
u(Y|X;β∗
1j+t2, a∗
j, b∗
j, σ∗
j)−v(Y|X;β∗
1j+t2)i
,
:=An+Bn+En, (16)
where we denote u(Y|X;β1, a, b, σ ) := exp( β⊤
1X)f(Y|a⊤X+b, σ)andv(Y|X;β1) :=
exp(β⊤
1X)gGn(Y|X). Next, by means of the first-order Taylor expansion, we rewrite Anas
An=k∗X
j=1X
|α|=1exp(βn
0j)
2α4α!(∆t2βn
1j)α1(∆an
ij)α2(∆bn
j)α3(∆σn
j)α4
×Xα1+α2exp(( β∗
1j+t2)⊤X)·∂|α2|+α3+2α4f
∂h|α2|+α3+2α4
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) +R1(X, Y)
=k∗X
j=12X
2|ℓ1|+ℓ2=1X
α∈Iℓ1,ℓ2exp(βn
0j)
2α4α!(∆t2βn
1j)α1(∆an
j)α2(∆bn
j)α3(∆σn
j)α4
×Xℓ1exp(( β∗
1j+t2)⊤X)·∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) +R1(X, Y), (17)
where R1(X, Y)is a Taylor remainder such that R1(X, Y)/D′
1(Gn, G∗)→0asn→ ∞ . Here, the
first equality is due to the following partial differential equation for the univariate Gaussian density:
∂α4f
∂hα4
2(Y|(a∗
j)⊤X+b∗
j, σ∗
j) =1
2α4·∂2α4f
∂h2α4
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j),
while the second equality is obtained by defining ℓ1=α1+α2,ℓ2=|α2|+α3+ 2α4and
Iℓ1,ℓ2:=
α= (αi)4
i=1∈Nd×Nd×N×N:α1+α2=ℓ1, α3+ 2α4=ℓ2− |α2|	
,(18)
for all (ℓ1, ℓ2)∈Nd×Nsuch that 1≤2|ℓ1|+ℓ2≤2. Analogously, Bncan be rewritten as
Bn=−k∗X
j=1X
|γ|=1exp(βn
0j)
γ!(∆t2βn
1j)γXγexp(( β∗
1j+t2)⊤X)gGn(Y|X) +R2(X, Y),(19)
where R2(X, Y)is a Taylor remainder such that R2(X, Y)/D′
1(Gn, G∗)→0asn→ ∞ . From the
formulations of An,BnandEn, we can represent Qnas the following linear combination
Qn=k∗X
j=12X
2|ℓ1|+ℓ2=0Tn
ℓ1,ℓ2(j)·Xℓ1exp(( β∗
1j+t2)⊤X)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j)
+k∗X
j=11X
|γ|=0Sn
γ(j)·Xγexp(( β∗
1j+t2)⊤X)gGn(Y|X),
with coefficients being denoted by Tn
ℓ1,ℓ2(j)andSn
γ(j)for all j∈[k∗],0≤2|ℓ1|+ℓ2≤2and
0≤ |γ| ≤1where
Tn
ℓ1,ℓ2(j) =

P
α∈Iℓ1,ℓ2exp(βn
0j)
2α4α!(∆t2βn
1j)α1(∆an
j)α2(∆bn
j)α3(∆σn
j)α4,(ℓ1, ℓ2)̸= (0d,0),
exp(βn
0j)−exp(β∗
0j+t1), (ℓ1, ℓ2) = (0d,0);
(20)
16

--- PAGE 17 ---
and
Sn
γ(j) =

−exp(βn
0j)
γ!(∆t2βn
1j)γ, |γ| ̸= 0,
−exp(βn
0j) + exp( β∗
0j+t1),|γ|= 0.(21)
Step 2: Non-vanishing coefficients
Now, we will demonstrate by contradiction that at least one among terms of the forms
Tn
ℓ1,ℓ2(j)/D′
1(Gn, G∗)andSn
γ(j)/D′
1(Gn, G∗)does not approach zero. Indeed, assume that all
of them vanish when n→ ∞ , then we get
1
D′
1(Gn, G∗)·k∗X
j=1exp(βn
0j)−exp(β∗
0j+t1)=k∗X
j=1|Tn
0d,0(j)|
D′
1(Gn, G∗)→0. (22)
Similarly, by considering the limits of Tn
ℓ1,ℓ2(j)/D′
1(Gn, G∗)for all j∈[k∗]and1≤2|ℓ1|+ℓ2≤2,
we obtain that
1
D′
1(Gn, G∗)·k∗X
j=1exp(βn
0j)∥(∆t2βn
1j,∆an
j,∆bn
j,∆σn
j)∥ →0. (23)
Combine the results in equations (22) and 23, we have 1 =D′
1(Gn, G∗)/D′
1(Gn, G∗)→0, which
is a contradiction. As a result, not all the limits of Tn
ℓ1,ℓ2(j)/D′
1(Gn, G∗)andSn
γ(j)/D′
1(Gn, G∗)
equal to zero.
Step 3: Fatou’s lemma involvement
Thus, let mnbe the maximum of the absolute values of those terms, we have that 1/mn̸→ ∞ . Then,
the Fatou’s lemma says that
lim
n→∞EX[V(gGn(·|X), gG∗(·|X))]
mn· D′
1(Gn, G∗)≥Z
lim inf
n→∞|gGn(Y|X)−gG∗(Y|X)|
2mn· D′
1(Gn, G∗)d(X, Y). (24)
By assumption, the left-hand side of the above equation equals to zero, therefore, the integrand in
the right-hand side also equals to zero for almost surely (X, Y), which leads to the following limit:
Qn/[mnD′
1(Gn, G∗)]→0asn→ ∞ for almost surely (X, Y). More specifically, we have
k∗X
j=12X
2|ℓ1|+ℓ2=0ηℓ1,ℓ2(j)·Xℓ1exp(( β∗
1j+t2)⊤X)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j)
+k∗X
j=11X
|γ|=0ωγ(j)·Xγexp(( β∗
1j+t2)⊤X)gG∗(Y|X) = 0 ,
for almost surely (X, Y), where ηℓ1,ℓ2(j)andωγ(j)are the limits of Tn
ℓ1,ℓ2(j)/[mnD′
1(Gn, G∗)]
andSn
γ(j)/[mnD′
1(Gn, G∗)], respectively, for all j∈[k∗],0≤2|ℓ1|+ℓ2≤2and0≤ |γ| ≤1.
Here, at least one among ηℓ1,ℓ2(j)andωγ(j)is different from zero. On the other hand, since the set
W1: =(
Xℓ1exp(( β∗
1j+t2)⊤X)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) :j∈[k∗],0≤2|ℓ1|+ℓ2≤2)
∪
Xγexp(( β∗
1j+t2)⊤X)gG∗(Y|X) :j∈[k∗],0≤ |γ| ≤1	
, (25)
is linearly independent (see Lemma 2 at the end of this proof), we obtain that ηℓ1,ℓ2(j) =ωγ(j) = 0
for all j∈[k∗],0≤2|ℓ1|+ℓ2≤2and0≤ |γ| ≤1, which is a contradiction.
Thus, we reach the local inequality in 15, that is, there exists ε′>0that satisfies
inf
G∈Ek∗(Θ),
D1(G,G∗)≤ε′EX[V(gG(·|X), gG∗(·|X))]/D1(G, G∗)>0.
17

--- PAGE 18 ---
Global version : Thus, it suffices to prove its following global inequality:
inf
G∈Ek∗(Θ),
D1(G,G∗)>ε′EX[V(gG(·|X), gG∗(·|X))]/D1(G, G∗)>0. (26)
Assume by contrary that there exists a sequence G′
n∈ Ek∗(Θ)that satisfies
limn→∞EX[V(gG′n(·|X), gG∗(·|X))]/D1(G′
n, G∗) = 0 ,
D1(G′
n, G∗)> ε′.
Therefore, we obtain that EX[V(gG′n(·|X), gG∗(·|X))]→0asn→ ∞ . Since the set Θis compact,
we are able to replace the sequence G′
nby its subsequence which converges to some mixing measure
G′∈ Ek∗(Θ)such that D(G′, G∗)> ε′. Then, by the Fatou’s lemma, we get
lim
n→∞EX[V(gG′n(·|X), gG∗(·|X))]≥1
2Z
lim inf
n→∞|gG′n(Y|X)−gG∗(Y|X)|d(X, Y),
which implies that
Z
|gG′(Y|X)−gG∗(Y|X)|d(X, Y) = 0
Thus, we obtain that gG′(Y|X) =gG∗(Y|X)for almost surely (X, Y). Now that the softmax
gating Gaussian mixture of experts is identifiable up to a translation (see Proposition 1), the mixing
measure G′admits the form G′=Pk∗
i=1exp(β∗
0τ(i)+t1)δ(β∗
1τ(i)+t2,a∗
τ(i),b∗
τ(i),σ∗
τ(i))for some t1∈R
andt2∈Rd, where τis some permutation of the set {1,2, . . . , k }. This leads to the fact that
D1(G′, G∗) = 0 , which contradicts the hypothesis D1(G′, G∗)> ε′>0. Hence, we obtain the
inequality in equation (14).
To complete the proof, we will show the previous claim regarding the independence of elements in
W1in the following lemma:
Lemma 2. The set W1defined in equation (25) is linearly independent w.r.t XandY.
Proof of Lemma 2. Assume that the following holds for almost surely (X, Y):
k∗X
j=12X
2|ℓ1|+ℓ2=0ηℓ1,ℓ2(j)·Xℓ1exp(( β∗
1j+t2)⊤X)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j)
+k∗X
j=11X
|γ|=0ωγ(j)·Xγexp(( β∗
1j+t2)⊤X)gG∗(Y|X) = 0 ,
where ηℓ1,ℓ2(j)∈Randωγ(j)∈R. Then, we need to show that ηℓ1,ℓ2(j) =ωγ(j) = 0 , for all
j∈[k∗],0≤2|ℓ1|+ℓ2≤2and0≤ |γ| ≤1. The above equation is equivalent to
k∗X
j=11X
ζ=0h2−2ζX
ℓ2=0ηζ,ℓ2(j)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) +ωζ(j)gG∗(Y|X)i
Xζexp(( β∗
1j+t2)⊤X) = 0 ,
for almost surely (X, Y). Since β∗
11, . . . , β∗
1k∗arek∗distinct values, we get that the set
exp(( β∗
1j+t2)⊤X) :j∈[k∗]	
is linearly independent, which implies that
1X
ζ=0h2−2ζX
ℓ2=0ηζ,ℓ2(j)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) +ωζ(j)gG∗(Y|X)i
Xζ= 0,
for all j∈[k∗]for almost surely (X, Y). Obviously, the above equation is a polynomial of X∈ X,
where Xis a compact subset of Rd. Then, we achieve that
2−2ζX
ℓ2=0ηζ,ℓ2(j)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) +ωζ(j)gG∗(Y|X) = 0 ,
18

--- PAGE 19 ---
for all j∈[k∗]andζ∈ {0,1}, for almost surely (X, Y). Again, as (a∗
j, b∗
j, σ∗
j)forj∈[k∗]arek∗
distinct tuples, we have that ((a∗
j)⊤X+b∗
j, σ∗
j)forj∈[k∗]are also k∗distinct tuples for almost
surely X. Therefore,
∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j), gG∗(Y|X)
is a linearly independent set. As a
result, ηℓ1,ℓ2(j) =ωγ(j) = 0 for all j∈[k∗],0≤2|ℓ1|+ℓ2≤2and0≤ |γ| ≤1.
Hence, the proof is completed.
A.2 Proof of Theorem 2
In this proof, we adapt the framework in Appendix A.1 to the setting of Theorem 2. However, since
the arguments utilized for the global version part remain the same (up to some changes of notations)
for the over-fitted settings, they will not be presented here again. Thus, we focus only on proving the
following local inequality:
lim
ε→0inf
G∈Ok(Θ),
D2(G,G∗)≤εEX[V(gG(·|X), gG∗(·|X))]/D2(G, G∗)>0. (27)
Assume that the above claim is not true, then there exists a sequence of mixing measures Gn:=Pkn
i=1exp(βn
0i)δ(βn
1i,an
i,bn
i,σn
i)∈ Ok(Θ)such that
EX[V(gGn(·|X), gG∗(·|X))]/D2(Gn, G∗)→0,
D2(Gn, G∗)→0,
when ntends to infinity. Since the proof argument is asymptotic, we also assume that kn=k′for all
n≥1. Following the proof argument of Theorem 1 in Appendix A.1, we also assume that the V oronoi
cellsAj=An
jdoes not change with nfor all j∈[k∗]. Additionally, since D2(Gn, G∗)→0, we
have(an
i, bn
i, σn
i)→(a∗
j, b∗
j, σ∗
j)for any i∈ Ajasnapproaches infinity. Furthermore, there exist
t1∈Randt2∈Rdsuch thatP
i∈Ajexp(βn
0i)→exp(β∗
0j+t1)andβn
1i→β∗
1j+t2for any i∈ Aj
andj∈[k∗]. Then, we can upper bound the V oronoi loss function D2asD2(Gn, G∗)≤ D′
2(Gn, G∗),
where
D′
2(Gn, G∗) :=X
j:|Aj|>1X
i∈Ajexp(βn
0i)
∥(∆t2βn
1ij,∆bn
ij)∥¯r(|Aj|)+∥(∆an
ij,∆σn
ij)∥¯r(|Aj|)/2
+X
j:|Aj|=1X
i∈Ajexp(βn
0i)∥(∆t2β1ij,∆an
ij,∆bn
ij,∆σn
ij)∥+k∗X
j=1X
i∈Ajexp(βn
0i)−exp(β∗
0j+t1),
in which
∆t2βn
1ij:=βn
1i−β∗
1j−t2,∆an
ij:=an
i−a∗
j,
∆bn
ij:=bn
i−b∗
j,∆σn
ij:=σn
i−σ∗
j.
Recall that EX[V(gGn(·|X), gG∗(·|X))]/D2(Gn, G∗)→0asn→ ∞ , which leads to
EX[V(gGn(·|X), gG∗(·|X))]/D′
2(Gn, G∗)→0.
Step 1: Density Decomposition
In this step, we decompose the quantity Qn= [Pk∗
j=1exp(( β∗
1j+t2)⊤X+β∗
0j+t1)]·[gGn(Y|X)−
gG∗(Y|X)]with abuse of notations in Appendix A.1 as follows:
Qn=k∗X
j=1X
i∈Ajexp(βn
0i)h
u(Y|X;βn
1i, an
i, bn
i, σn
i)−u(Y|X;β∗
1j+t2, a∗
j, b∗
j, σ∗
j)i
−k∗X
j=1X
i∈Ajexp(βn
0i)h
v(Y|X;βn
1i)−v(Y|X;β∗
1j+t2)i
+k∗X
j=1X
i∈Ajexp(βn
0i)−exp(β∗
0j+t1)h
u(Y|X;β∗
1j+t2, a∗
j, b∗
j, σ∗
j)−v(Y|X;β∗
1j+t2)i
,
:=An+Bn+En,
19

--- PAGE 20 ---
where we denote u(Y|X;β1, a, b, σ ) := exp( β⊤
1X)f(Y|a⊤X+b, σ)andv(Y|X;β1) :=
exp(β⊤
1X)gGn(Y|X). Since each V oronoi cell Ajpossibly has more than one element, we continue
to decompose AnandBnas follows:
An=X
j:|Aj|=1X
i∈Ajexp(βn
0i)h
u(Y|X;βn
1i, an
i, bn
i, σn
i)−u(Y|X;β∗
1j+t2, a∗
j, b∗
j, σ∗
j)i
+X
j:|Aj|>1X
i∈Ajexp(βn
0i)h
u(Y|X;βn
1i, an
i, bn
i, σn
i)−u(Y|X;β∗
1j+t2, a∗
j, b∗
j, σ∗
j)i
: =An,1+An,2,
and
Bn=−X
j:|Aj|=1X
i∈Ajexp(βn
0i)h
v(Y|X;βn
1i)−v(Y|X;β∗
1j+t2)i
−X
j:|Aj|>1X
i∈Ajexp(βn
0i)h
v(Y|X;βn
1i)−v(Y|X;β∗
1j+t2)i
: =Bn,1+Bn,2.
Now, we apply the first-order Taylor expansions to two terms An,1andBn,1as in equations (17) and
(19), while for An,2andBn,2, we use the Taylor expansions of orders ¯r(|Aj|)and2, respectively,
for each j:|Aj|>1as follows:
An,2=X
j:|Aj|>1X
i∈Aj2¯r(|Aj|)X
2|ℓ1|+ℓ2=1X
α∈Iℓ1,ℓ2exp(βn
0i)
2α4α!(∆t2βn
1ij)α1(∆an
ij)α2(∆bn
ij)α3(∆σn
ij)α4
×Xℓ1exp(( β∗
1j+t2)⊤X)·∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) +R3(X, Y),
Bn,2=−X
j:|Aj|>1X
i∈Aj2X
|γ|=1exp(βn
0i)
γ!(∆t2βn
1ij)γXγexp(( β∗
1j+t2)⊤X)gGn(Y|X) +R4(X, Y).
Here, Iℓ1,ℓ2:=
α= (αi)4
i=1∈Nd×Nd×N×N:α1+α2=ℓ1, α3+ 2α4=ℓ2− |α2|	
for
any(ℓ1, ℓ2)∈N2such that 1≤2|ℓ1|+ℓ2≤2¯r(|Aj|), while R3(X, Y)andR4(X, Y)are Taylor
remainders such that Rp(X, Y)/D′
2(Gn, G∗)→0when n→ ∞ forp∈ {3,4}.
As a result, Qncan be represented as
Qn=k∗X
j=12¯r(|Aj|)X
2|ℓ1|+ℓ2=0Tn
ℓ1,ℓ2(j)·Xℓ1exp(( β∗
1j+t2)⊤X)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j)
+k∗X
j=11+1{|Aj|>1}X
|γ|=0Sn
γ(j)·Xγexp(( β∗
1j+t2)⊤X)gGn(Y|X), (28)
with coefficients Tn
ℓ1,ℓ2(j)andSn
γ(j)being defined for any j∈[k∗],0≤2|ℓ1|+ℓ2≤2and
0≤ |γ| ≤1as
Tn
ℓ1,ℓ2(j) =

P
i∈AjP
α∈Iℓ1,ℓ2exp(βn
0i)
2α4α!(∆t2βn
1ij)α1(∆an
ij)α2(∆bn
ij)α3(∆σn
ij)α4,(ℓ1, ℓ2)̸= (0d,0),
P
i∈Ajexp(βn
0i)−exp(β∗
0j+t1), (ℓ1, ℓ2) = (0d,0);
and
Sn
γ(j) =

−P
i∈Ajexp(βn
0i)
γ!(∆t2βn
1ij)γ, |γ| ̸= 0,
−P
i∈Ajexp(βn
0i) + exp( β∗
0j+t1), |γ|= 0.
20

--- PAGE 21 ---
Step 2: Non-vanishing coefficients
Next, we will show that not all the quantities Tn
ℓ1,ℓ2(j)/D′
2(Gn, G∗)andSn
γ(j)/D′
2(Gn, G∗)go to 0
asn→ ∞ . Assume that all of them vanish when ntends to infinity. Then, by arguing similarly as in
equations (22) and (23), we obtain that
1
D′
2(Gn, G∗)·"k∗X
j=1X
i∈Ajexp(βn
0i)−exp(β∗
0j+t1)
+X
j:|Aj|=1X
i∈Ajexp(βn
0i)∥(∆t2βn
1ij,∆an
ij,∆bn
ij,∆σn
ij)∥#
→0.
Putting the above limit and the formulation of D2(Gn, G∗)together, we deduce that
1
D′
2(Gn, G∗)·X
j:|Aj|>1X
i∈Ajexp(β0i)
∥(∆t2βn
1ij,∆bn
ij)∥¯r(|Aj|)+∥(∆an
ij,∆σn
ij)∥¯r(|Aj|)/2
̸→0,
which indicates that there exists some index j∗∈[k∗]such that |Aj∗|>1and
1
D′
2(Gn, G∗)·X
i∈Aj∗exp(β0i)
∥(∆t2βn
1ij∗,∆bn
ij∗)∥¯r(|Aj|)+∥(∆an
ij∗,∆σn
ij∗)∥¯r(|Aj|)/2
̸→0,
for all t2∈Rd. Without loss of generality, we may assume that j∗= 1. Recall that for (ℓ1, ℓ2)∈
Nd×Nsuch that 1≤ |ℓ1|+ℓ2≤¯r(|A1|), we have Tn
ℓ1,ℓ2(1)/D′
2(Gn, G∗)→0asn→ ∞ . Thus,
by dividing this ratio and the left hand side of the above equation and let t2= 0, we obtain that
P
i∈A1P
α∈Iℓ1,ℓ2exp(βn
0i)
2α4α!(∆t2βn
1i1)α1(∆an
i1)α2(∆bn
i1)α3(∆σn
i1)α4
P
i∈A1exp(βn
0i)
∥(∆t2βn
1i1,∆bn
i1)∥¯r(|A1|)+∥(∆an
i1,∆σn
i1)∥¯r(|A1|)/2→0, (29)
for all (ℓ1, ℓ2)such that 1≤ |ℓ1|+ℓ2≤¯r(|A1|).
Let us define Mn:= max {∥∆t2βn
1i1∥,∥∆an
i1∥1/2,|∆bn
i1|,|∆σn
i1|1/2:i∈ A 1}andβn:=
max i∈A1exp(βn
0i). Note that the sequence exp(βn
0i)/βnis bounded, therefore, we can replace
it by its subsequence that has a positive limit p2
5i:= lim n→∞exp(βn
0i)/βn. Thus, at least one among
p2
5i, fori∈ A 1, equals 1.
In addition, we also define
(∆t2βn
1i1)/Mn→p1i,(∆an
i1)/Mn→p2i,
(∆bn
i1)/Mn→p3i,(∆σn
i1)/[2Mn]→p4i.
Here, at least one of p1i,p2i,p3iandp4ifori∈ A 1equals either 1 or −1. Next, we divide both
the numerator and the denominator of the ratio in equation (29) byβnMℓ1+ℓ2
n , and then achieve the
following system of polynomial equations:
X
i∈A1X
α∈Iℓ1,ℓ21
α!·p2
5ipα1
1ipα2
2ipα3
3ipα4
4i= 0,
for all (ℓ1, ℓ2)∈Nd×Nsuch that 1≤ |ℓ1|+ℓ2≤¯r(|A1|). However, based on the definition of
¯r(|A1|), the above system has no non-trivial solutions, which is a contradiction. Thus, not all the
quantities Tn
ℓ1,ℓ2(j)/D′
2(Gn, G∗)andSn
γ(j)/D′
2(Gn, G∗)go to 0 as n→ ∞ .
Step 3: Fatou’s lemma involvement
Subsequently, we denote by mnbe the maximum of the absolute values of those quantities. Based
on the result in Step 2, we know that 1/mn̸→ ∞ . Then, by applying the Fatou’s lemma as in
equation (24), we get that Qn/[mnD′
2(Gn, G∗)]→0asn→ ∞ for almost surely (X, Y). It follows
21

--- PAGE 22 ---
from the decomposition of Qnin equation (28) that
k∗X
j=12¯r(|Aj|)X
2|ℓ1|+ℓ2=0ηℓ1,ℓ2(j)·Xℓ1exp(( β∗
1j+t2)⊤X)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j)
+k∗X
j=11+1{|Aj|>1}X
|γ|=0ωγ(j)·Xγexp(( β∗
1j+t2)⊤X)gG∗(Y|X) = 0 ,
for almost surely (X, Y), where ηℓ1,ℓ2(j)andωγ(j)denote the limits of Tn
ℓ1,ℓ2(j)/[mnD′
2(Gn, G∗)]
andSn
γ(j)/[mnD′
2(Gn, G∗)]asn→ ∞ , respectively, for all j∈[k∗],0≤2|ℓ1|+ℓ2≤2¯r(|Aj|)
and0≤ |γ| ≤1 +1{|Aj|>1}. By definition, at least one among ηℓ1,ℓ2(j)andωγ(j)is different from
zero. Nevertheless, as the set
W2: =(
Xℓ1exp(( β∗
1j+t2)⊤X)∂ℓ2f
∂hℓ2
1(Y|(a∗
j)⊤X+b∗
j, σ∗
j) :j∈[k∗],0≤2|ℓ1|+ℓ2≤2¯r(|Aj|))
∪
Xγexp(( β∗
1j+t2)⊤X)gG∗(Y|X) :j∈[k∗],0≤ |γ| ≤1 +1{|Aj|>1}	
, (30)
is linearly independent w.r.t XandY(proof can be done similarly to Lemma 2), it follows that
ηℓ1,ℓ2(j) =ωγ(j) = 0 ,
for all j∈[k∗],0≤2|ℓ1|+ℓ2≤2¯r(|Aj|)and0≤ |γ| ≤1 +1{|Aj|>1}, which is a contradiction.
Hence, we achieve the inequality in equation (27), and complete the proof.
B Proofs of Auxiliary Results
In this appendix, we provide proofs for the results of Proposition 1 and Proposition 2 in Appendix B.1
and Appendix B.2, respectively, while we leave that for Lemma 1 in Appendix B.3.
B.1 Proof of Proposition 1
Given the notations in Proposition 1, assume that the equation gG(Y|X) =gG′(Y|X)holds true,
that is,
kX
i=1exp(( β1i)⊤X+β0i)
Pk
j=1exp(( β1j)⊤X+β0j)f(Y|(ai)⊤X+bi, σi)
=k′X
i=1exp(( β′
1i)⊤X+β′
0i)
Pk′
j=1exp(( β′
1j)⊤X+β′
0j)f(Y|(a′
i)⊤X+b′
i, σ′
i),(31)
for almost surely (X, Y). Then, it follows from the identifiability of the location-scale Gaussian
mixtures [ 56,57] that the number of components and the weight set of the mixing measure Gequal
to those of its counterpart G′, i.e.k=k′and
(
exp(( β1i)⊤X+β0i)
Pk
j=1exp(( β1j)⊤X+β0j):i∈[k])
≡(
exp(( β′
1i)⊤X+β′
0i)
Pk
j=1exp(( β′
1j)⊤X+β′
0j):i∈[k])
,
for almost surely X. For simplicity, we may assume that
exp(( β1i)⊤X+β0i)
Pk
j=1exp(( β1j)⊤X+β0j)=exp(( β′
1i)⊤X+β′
0i)
Pk
j=1exp(( β′
1j)⊤X+β′
0j),
for all i∈[k]. Since the softmax function is invariant to translation, we get that β0i=β′
0i+t1and
β1i=β′
1i+t2for some t1∈Randt2∈Rd. Therefore, equation (31) reduces to
kX
i=1exp(β0i)u(Y|X;β1i, ai, bi, σi) =kX
i=1exp(β0i)u(Y|X;β1i, a′
i, b′
i, σ′
i)), (32)
22

--- PAGE 23 ---
for almost surely (X, Y), where u(Y|X;β1, a, b, σ ) := exp( β⊤
1X)f(Y|a⊤X+b, σ)for all i∈[k].
Next, we will partition the index set [k]intoqsubsets U1, U2, . . . , U qsuch that for each ℓ∈[q], we
haveexp(β0i) = exp( β0i′)for any i, i′∈Uℓ. As a result, equation (32) can be rewritten as
qX
ℓ=1X
i∈Uℓexp(β0i)u(Y|X;β1i, ai, bi, σi) =qX
ℓ=1X
i∈Uℓexp(β0i)u(Y|X;β1i, a′
i, b′
i, σ′
i),
for almost surely (X, Y). Given the above equation, for each ℓ∈[q], we obtain that

((ai)⊤X+bi, σi) :i∈Uℓ	
≡
((a′
i)⊤X+b′
i, σ′
i) :i∈Uℓ	
,
for almost surely X, which directly leads to
{(ai, bi, σi) :i∈Uℓ} ≡ { (a′
i, b′
i, σ′
i) :i∈Uℓ}.
WLOG, we assume that (ai, bi, σi) = (a′
i, b′
i, σ′
i)for all i∈Uℓ. Consequently,
qX
ℓ=1X
i∈Uℓexp(β0i)δ{β1i,ai,bi,σi}=qX
ℓ=1X
i∈Uℓexp(β′
0i+t1)δ{β′
1i+t2,a′
i,b′
i,σ′
i},
or equivalently, G≡G′
t1,t2. Hence, the proof is completed.
B.2 Proof of Proposition 2
Our proof will be based on the convergence rates of density estimation from MLE in Theorem 7.4 in
[58]. Before stating this result here, let us introduce some necessary notations. Firstly, let Pk(Θ)be
the set of conditional densities of all mixing measures in Ok(Θ), i.e.,Pk(Θ) := {gG(Y|X) :G∈
Ok(Θ)}. Additionally, we define
ePk(Θ) := {g(G+G∗)/2(Y|X) :G∈ Ok(Θ)},
eP1/2
k(Θ) := {g1/2
(G+G∗)/2(Y|X) :G∈ Ok(Θ)}.
Next, for each δ >0, the Hellinger ball centered around the conditional density gG∗(Y|X)and
intersected with the set eP1/2
k(Θ)is denoted by
eP1/2
k(Θ, δ) :=n
g1/2∈eP1/2
k(Θ) : h(g, gG∗)≤δo
.
Finally, in order to measure the size of the above set, [58] proposes using the following quantity:
JB(δ,eP1/2
k(Θ, δ)) :=Zδ
δ2/213H1/2
B(t,eP1/2
k(Θ, t),∥ · ∥) dt∨δ, (33)
where HB(t,eP1/2
k(Θ, t),∥ · ∥)denotes the bracketing entropy [ 58] ofeP1/2
k(Θ, u)under the 2-norm,
andt∨δ:= max {t, δ}. Now, we are ready to recall the statement of Theorem 7.4 in [58]:
Theorem 3 (Theorem 7.4, [ 58]).TakeΨ(δ)≥ J B(δ,eP1/2
k(Θ, δ))that satisfies Ψ(δ)/δ2is a non-
increasing function of δ. Then, for some universal constant cand for some sequence (δn)such that√nδ2
n≥cΨ(δn), we achieve that
P
EX[h(gbGn(·|X), gG∗(·|X))]> δ
≤cexp
−nδ2
c2
,
for all δ≥δn.
The proof of this theorem can be seen in [58].
Proof of Proposition 2. Back to our main proof, since
HB(t,eP1/2
k(Θ, t),∥ · ∥)≤HB(t,Pk(Θ, t), h)
23

--- PAGE 24 ---
for any t >0, it follows from equation (33) that
JB(δ,eP1/2
k(Θ, δ))≤Zδ
δ2/213H1/2
B(t,Pk(Θ, t), h) dt∨δ≲Zδ
δ2/213log(1/t)dt∨δ,
where we apply the upper bound of a bracketing entropy in Lemma 3 (cf. the end of this proof)
in the second inequality. Let Ψ(δ) = δ·[log(1 /δ)]1/2, we have Ψ(δ)/δ2is a non-increasing
function of θ. Moreover, the above equation deduces that Ψ(δ)≥ JB(δ,eP1/2
k(Θ, δ)). Additionally,
letδn=p
log(n)/n, we have that√nδ2
n≥cΨ(δn)for some universal constant c. As all the
assumptions are met, Theorem 3 gives us
P(EX[h(gbGn(·|X), gG∗(·|X))]> C(log(n)/n)1/2)≲exp(−clog(n)),
for some universal constant Cthat depends only on Θ.
For completion, we will provide the result regarding the upper bound of a bracketing entropy in the
following lemma:
Lemma 3. Assume that Θis a bounded set, then the following inequality holds true for any
0≤ε≤1/2:
HB(ε,Pk(Θ), h)≲log(1/ε).
Proof of Lemma 3. Firstly, we will establish an upper bound for the univariate Gaussian density
f(Y|a⊤X+b, σ). Since both XandΘare bounded sets, there exist positive constants κ, u, ℓ such
that−κ≤a⊤X+b≤κandℓ≤σ≤u. As a result,
f(Y|a⊤X+b, σ) =1√2πh2exp
−(Y−h1)2
2h2
≤1√
2πℓ.
For any |Y| ≥2κ, we have that(Y−h1)2
2h2≥Y2
8u, which leads to
f(Y|a⊤X+b, σ)≤1√
2πℓexp
−Y2
8u
.
Putting the above results together, we obtain that f(Y|a⊤X+b, σ)≤K(Y|X), where we define
K(Y|X) :=1√
2πℓexp
−Y2
8u
if|Y| ≥2κ, and K(Y|X) :=1√
2πℓotherwise.
Subsequently, let η≤ε, we assume that the set Pk(Θ)has an η-cover (under ℓ1-norm) denoted by
{π1, . . . , π N}, where N:=N(η,Pk(Θ),∥·∥1)is known as the η-covering number of Pk(Θ). Then,
we will build up the brackets of the form [νi(Y|X), µi(Y|X)]for all i∈[N]as follows:
νi(Y|X) := max {πi(Y|X)−η,0},
µi(Y|X) := max {πi(Y|X) +η, K(Y|X)}.
Consequently, it can be checked that Pk(Θ)⊂SN
i=1[νi(Y|X), µi(Y|X)]with a note that µi(Y|X)−
νi(Y|X)≤min{2η, K(Y|X)}. Next, for each i∈[N], we attempt to give an upper bound for
∥µi−νi∥1=Z
|Y|<2κ(µi(Y|X)−νi(Y|X)) d(X, Y) +Z
|Y|≥2κ(µi(Y|X)−νi(Y|X)) d(X, Y)
≤Rη+ exp
−R2
2u
≤R′η,
where R:= max {2κ,√
8u}log(1/η)andR′is some positive constant. By definition of the bracket-
ing entropy, since HB(R′η,Pk(Θ),∥ · ∥1)is the logarithm of the smallest number of brackets of size
R′ηnecessary to cover Pk(Θ), we achieve that
HB(R′η,Pk(Θ),∥ · ∥ 1)≤logN= log N(η,Pk(Θ),∥ · ∥ 1).
Assume that the following upper bound for the covering number logN(η,Pk(Θ),∥ · ∥ 1)≲log(1/η)
holds true (proof is provided below), then the above result leads to
HB(R′η,Pk(Θ),∥ · ∥ 1)≲log(1/η).
24

--- PAGE 25 ---
By selecting η=ε/R′, we receive that HB(ε,Pk(Θ),∥ · ∥ 1)≲log(1/ε). Furthermore, since the
Hellinger distance is upper bounded by the L1-norm, we reach the desired conclusion:
HB(ε,Pk(Θ), h)≲log(1/ε).
Upper bound of the covering number. For completion, we will establish the following upper bound
for the covering number, i.e.,
logN(η,Pk(Θ),∥ · ∥ 1)≲log(1/η).
Let us denote ∆ :={(β0, β1)∈R×Rd: (β0, β1, a, b, σ )∈Θ}andΩ :={(a, b, σ )∈Rd×R×R+:
(β0, β1, a, b, σ )∈Θ}. Since Θis a compact set, ∆andΩare also compact. Thus, we can find
η-covers ∆ηandΩηfor∆andΩ, respectively. It can be verified that |∆η| ≤ O (η−(d+1)k)and
|Ωη|≲O(η−(d+2)k).
For each mixing measure G=Pk
i=1exp(β0i)δ(β1i,ai,bi,σi)∈ O k(Θ), we consider another one
denoted by eG:=Pk
i=1exp(β0i)δ(β1i,ai,bi,σi), where (ai,bi,σi)∈Ωηsuch that (ai,bi,σi)are
the closest to (ai, bi, σi)in that set for all i∈[k]. In addition, we also take into account the
following mixing measure G:=Pk
i=1exp(β0i)δ(β1i,ai,bi,σi), where (β0i,β1i)∈∆ηare the closest
to(β0i, β1i)in that set. We can verify that the conditional density gGbelongs to the following set:
R:={gG∈ Pk(Θ) : ( β0i, β1i)∈∆η,(ai, bi, σi)∈Ωη,∀i∈[k]}.
Let us denote Softmax( β⊤
1iX+β0i) :=exp(β⊤
1iX+β0i)
Pk
j=1exp(β⊤
1jX+β0j). From the formulation of eG, we
get the following bounds:
∥gG−geG∥1≤kX
i=1Z
XSoftmax( β⊤
1iX+β0i)f(Y|(ai)⊤X+bi, σi)−f(Y|(ai)⊤X+bi,σi)dX
≤kX
i=1Z
Xf(Y|(ai)⊤X+bi, σi)−f(Y|(ai)⊤X+bi,σi)dX
≲kX
i=1(∥ai−ai∥+|bi−bi|+|σi−σi|)
≲η, (34)
where the second inequality follows from the facts that Xis a bounded set. Note that Softmax is a
Lipschitz function with Lipschitz constant L≥0. Additionally, since Xis a bounded set, there exists
a constant B >0such that ∥X∥ ≤Bfor any X∈ X. As a result, we get
∥geG−gG∥1≤kX
i=1Z
XSoftmax( β⊤
1iX+β0i)−Softmax( β⊤
1iX+β0i)f(Y|(ai)⊤X+bi,σi)dX
≲LkX
i=1Z
X
∥β1i−β1i∥ · ∥X∥+|β0i−β0i|
dX
≤Lkη(B+ 1), (35)
where the second inequality follows from the fact that Softmax is a Lipschitz function and the
Gaussian density f(Y|(ai)⊤X+bi,σi)is bounded. Putting the bounds in equations (34) and(35)
together with the triangle inequality, we receive that
∥gG−gG∥1≤ ∥gG−geG∥1+∥geG−gG∥1≲η,
which means that Ris anη-cover (not necessarily smallest) of the metric space (Pk(Θ),∥ · ∥ 1). By
definition of the covering number, we know that
N(η,Pk(Θ),∥ · ∥ 1)≤ |R| =|∆η| × |Ωη| ≤ O (η−(d+1)k)· O(η(−d+2)k)≤ O(η−(2d+3)k),
which implies that,
logN(η,Pk(Θ),∥ · ∥∞)≲log(1/η).
Hence, the proof is completed.
25

--- PAGE 26 ---
B.3 Proof of Lemma 1
First of all, let us recall the system of polynomial equations of interest here:
mX
j=1X
α∈Iℓ1,ℓ2p2
5jpα1
1jpα2
2jpα3
3jpα4
4j
α1!α2!α3!α4!= 0, (36)
whereIℓ1,ℓ2={α= (α1, α2, α3, α4)∈Nd×Nd×N×N:α1+α2=ℓ1, α3+ 2α4=ℓ2−|α2|}
for any (ℓ1, ℓ2)∈Nd×Nsuch that 0≤ |ℓ1| ≤r,0≤ℓ2≤r− |ℓ1|and|ℓ1|+ℓ2≥1.
In this proof, we denote p1j= (p1j1, p1j2, . . . , p 1jd)andp2j= (p2j1, p2j2, . . . , p 2jd).
When m= 2:By observing a portion of the above system when ℓ1=0d, which is given by
mX
j=1X
α3+2α4=ℓ2p2
5jpα3
3jpα4
4j
α3!α4!= 0, ℓ 2= 1,2, . . . , r. (37)
It follows from Proposition 2.1 in [ 28] that the smallest natural number rsuch that the system (37)
does not have any non-trivial solutions when m= 2isr= 4. It is worth noting that a solution of the
system 37 is considered non-trivial in [ 28] if all the values of p5jare different from zero, whereas
at least one among p3jis non-zero, which aligns with our definition of non-trivial solutions for the
system (36). Thus, we get ¯r(m)≤4, and it suffices to demonstrate that ¯r(m)>3. Indeed, when
r= 3, the system in equation (36) can be written as follows:
mX
j=1p2
5jp1jl= 0∀l∈[d],mX
j=1p2
5jp3j= 0,mX
j=1p2
5j(p2ju+p1jvp3j) = 0∀u, v∈[d],
mX
j=1p2
5jp1jup1jv= 0∀u, v∈[d],mX
j=1p2
5j1
2p2
3j+p4j
= 0,mX
j=1p2
5j1
3!p3
3j+p3jp4j
= 0,
mX
j=1p2
5jp1jup1jvp1jl= 0∀u, v, l∈[d],mX
j=1p2
5j1
2p1jup1jvp3j+p1jlp2jτ
= 0∀u, v, l, τ ∈[d],
mX
j=1p2
5j1
2p1ju·p2
3j+p1jvp4j+p2jlp3j
= 0∀u, v, l, τ ∈[d].
(38)
It can be seen that the following is a non-trivial solution of the above system: p5j= 1,p1j=p2j=0d
for all j∈[m],p31= 1,p32=−1,p41=p42=−1
2. Therefore, we obtain that ¯r(m)>3, which
leads to ¯r(m) = 4 .
When m= 3:Note that ¯r(m)is a monotonically increasing function of m. Therefore, it follows
from the previous result that ¯r(m)>¯r(2) = 4 , or equivalently, ¯r(m)≥5. Additionally, according
to Proposition 2.1 in [ 28], we deduce that ¯r(m)≤6based on the reduced system in equation (37).
Thus, we only need to show that ¯r(m)>5. Indeed, the system (36) when r= 5is a combination of
26

--- PAGE 27 ---
the system in equation (38) and the following system:
mX
j=1p2
5jp1jup1jvp1jlp1jτ= 0∀u, v, l, τ ∈[d],mX
j=1p2
5j1
4!p4
3j+1
2!p2
3jp4j+1
2!p2
4j
= 0,
mX
j=1p2
5j1
3!p1jup3
3j+p1jvp3jp4j+1
2!p2jlp2
3j+p2jτp4j
= 0∀u, v, l, τ ∈[d],
mX
j=1p2
5j1
3!p1ju1p1ju2p1ju3p3j+1
2!p1jv1p1jv2p2jv3
= 0∀(ui)3
i=1,(vi)3
i=1∈[d]3,
mX
j=1p2
5j1
2!2!p1ju1p1ju2p2
3j+1
2!p1ju3p1ju4p4j+p1ju5p1ju6p3j
= 0∀(ui)6
i=1∈[d]6,
mX
j=1p2
5j5Y
i=1p1jui= 0∀(ui)5
i=1∈[d]5,
mX
j=1p2
5j1
5!p5
3j+1
3!p3
3jp4j+1
2!p3jp2
4j
= 0,
mX
j=1p2
5j1
4!p1ju1p4
3j+1
2!p1ju2p2
3jp4j+1
2!p1ju3p2
4j+1
3!p2ju4p3
3j+p2ju5p3jp4j
= 0
∀(ui)5
i=1∈[d]5,
mX
j=1p2
5j1
4!4Y
i=1p1juip3j+1
3!7Y
i=5p1juip2ju8
= 0∀(ui)8
i=1∈[d]8,
mX
j=1p2
5j1
2!3!2Y
i=1p1juip3
3j+1
2!4Y
i=3p1juip3jp4j+p1ju5p2ju6(1
2p2
3j+p4j) +1
2!8Y
i=7p2juip3j
= 0
∀(ui)8
i=1∈[d]8,
mX
j=1p2
5j1
3!2!3Y
i=1p1juip2
3j+1
3!6Y
i=4p1juip4j+1
2!p1ju7p2ju8p3j+1
2!p1ju911Y
i=10p2jui
= 0
∀(ui)11
i=1∈[d]11.
We can verify that the following is a non-trivial solution of this system:
p5j= 1, p 1j=p2j=0d,∀j∈[m],
p31=√
3
3, p 32=−√
3
3, p 33= 0,
p41=p42=−1
6, p 43= 0.
Hence, we conclude that ¯r(m) = 6 .
C Experiments
In this appendix, we conduct a simulation study to empirically validate our theoretical results on the
convergence rates of maximum likelihood estimation (MLE) in the softmax gating Gaussian mixture
of experts established in Theorem 1 and Theorem 2.
C.1 Numerical Schemes
We illustrate the heterogeneity convergence rates of the MLE under the softmax gating Gaussian
mixture of experts via exact-fitted and over-fitted models, which correspond to the exact-fitted
27

--- PAGE 28 ---
and over-fitted settings described in Sections 3.1 and 3.2, respectively. For each case, we let X
be uniformly distributed over [0,1]and we generate observations Yfrom the conditional density
gG∗(Y|X)of softmax gating Gaussian mixture of experts model in equation (1). Here, the true
mixing measure G∗=Pk∗
i=1exp(β∗
0i)δ(β∗
1i,a∗
i,b∗
i,σ∗
i), where k∗= 2, is specified as follows:
β∗
01=−8, β∗
11= 25, a∗
1=−20, b∗
1= 15, σ∗
1= 0.3,
β∗
02= 0, β∗
12= 0, a∗
2= 20, b∗
2=−5, σ∗
2= 0.4.
Then, we compute the MLE bGnw.r.t. a number of components kfor each sample. For both of these
settings, we choose k∈ {k∗, k∗+ 1, k∗+ 2}. In order to perform the MLE, we use a numerical
scheme based on the EM algorithm [ 12] similar to the one used by Chamroukhi et al. [ 7,8]. Note
that the main difference with a classical EM is in the maximization step, as there are no closed
formulas for updating the softmax gating parameters (β0i, β1i),∀i∈[k]. For this purpose, following
the results of Chamroukhi et al. [ 7,8], see also [ 38,10,23], we use a multi-class iterative reweighted
least-squares algorithm. All code for our simulation study below was written in Python 3.9.13 on a
standard Unix machine.
We choose the convergence criteria ϵ= 10−6and2000 maximum EM iterations. Our goal is to
illustrate the theoretical properties of the estimator bGn. Therefore, we have initialized the EM
algorithm in a favourable way. More specifically, we first randomly partitioned the set {1, . . . , k }
intok∗index sets J1, . . . , J k∗, each containing at least one point, for any given kandk∗and for each
replication. Finally, we sampled β∗
1j(resp. a∗
j, b∗
j, σ∗
j) from a unique Gaussian distribution centered
onβ∗
1t(resp. a∗
t, b∗
t, σ∗
t), with vanishing covariance so that j∈Jt.
C.2 Empirical Convergence Rates
The empirical mean of discrepancies D1andD2between bGnandG∗, and the choice of kfor exact-
fitted and over-fitted models are reported in Figures 1 and 2, respectively. It can be observed that
the average discrepancies from bGntoG0vanish at a rate of O(n−1/2)up to a logarithmic factor, as
envisaged by Theorems 1 and 2. Although these empirical rates of convergence are similar for the
two models, they imply that the convergence behaviour of the individual fitted parameters is very
different in an over-fitted setting, which was already discussed in more detail in the Section 3.2.
C.2.1 Exact-fitted Model
We generate 40samples of size nfor each setting, given 200different choices of sample size n
between 102and105. The empirical parametric convergence rate of the MLE to G∗under the metric
D1in Figure 1 is consistent with the theoretical rates of estimating the true parameters exp(β∗
0i), β∗
1i
(up to translation), a∗
i,b∗
i, σ∗
ifori∈[k∗], which are of order O(n−1/2)up to logarithmic factors.
C.2.2 Over-fitted Model
In the over-fitted setting, we generate 40samples of size nfor each setting, given 200different
choices of sample size nbetween nmin≈14∗103fork=k∗+ 1,nmin≈27∗103fork=k∗+ 2
andnmax= 105. To the best of our knowledge, there is still a lack of theoretical understanding of EM
performance, in particular an established algorithm that enjoys global convergence for the parameter
estimation of the over-fitted softmax gating Gaussian mixture of experts. The most related theoretical
results are only for the mixture of expert with covariate-free gating networks in [ 41,40,42]. This
explains why in Figure 2 for over-fitted setting, we have not plotted the error bar due to the instability
of the EM algorithm for finding the global solution. Moreover, the sample size must be large enough
so that the empirical behaviour of the MLE from the EM algorithm matches the theoretical rate of
orderO(n−1/2)up to a logarithmic term.
28

--- PAGE 29 ---
6 8 10
log(sample size)−4−3−2−101log(loss)1.9n−0.50049
D1(/hatwideGn,G∗)Figure 1: Log-log scaled plots of the simulation results for exact-fitted setting. We compute the
estimator bGnon40independent samples of size nbetween 102and105. We plot its mean discrepancy
from the true mixing measure in red, with error bars representing two empirical standard deviations.
We also plot the least-squares fitted linear regression line of these points in a black dash-dotted line.
9.5 10.0 10.5 11.0 11.5
log(sample size)−10123log(loss)4.9n−0.4926
D2(/hatwideGn,G∗)
(a) Over-fitted, k= 3,nmin= 14070
10.25 10.50 10.75 11.00 11.25 11.50
log(sample size)0123log(loss)5.7n−0.49297
D2(/hatwideGn,G∗) (b) Over-fitted, k= 4,nmin= 26733
Figure 2: Log-log scaled plots of the empirical mean of the discrepancy D2between bGnandG∗
(red curves) and least-squares fitted linear regression (black dash-dotted lines) are shown using 40
independent sample sizes nbetween nminand105.
29
