# 2212.05055.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2212.05055.pdf
# Kích thước tệp: 914561 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023
TÁI CHẾ THƯA: HUẤN LUYỆN
HỖN HỢP CÁC CHUYÊN GIA TỪ CÁC CHECKPOINT DÀY ĐẶC
Aran Komatsuzaki†Joan Puigcerver†James Lee-Thorp†
Viện Công nghệ Georgia Google Research Google Research
Carlos Riquelme Basil Mustafa Joshua Ainslie
Google Research Google Research Google Research
Yi Tay Mostafa Dehghani Neil Houlsby
Google Research Google Research Google Research

TÓM TẮT
Huấn luyện các mạng nơ-ron lớn, sâu để hội tụ có thể tốn kém một cách cấm đoán.
Kết quả là, thường chỉ một lựa chọn nhỏ các mô hình dày đặc phổ biến được tái sử dụng qua
các bối cảnh và nhiệm vụ khác nhau. Ngày càng nhiều, các mô hình được kích hoạt thưa, tìm cách
tách rời kích thước mô hình khỏi chi phí tính toán, đang trở thành một lựa chọn thay thế hấp dẫn
cho các mô hình dày đặc. Mặc dù hiệu quả hơn về mặt chất lượng và chi phí tính toán,
các mô hình thưa vẫn đòi hỏi nhiều dữ liệu và tốn kém để huấn luyện từ đầu ở quy mô lớn.
Trong công trình này, chúng tôi đề xuất tái chế thưa – một cách đơn giản để tái sử dụng
chi phí huấn luyện đã đầu tư bằng cách khởi tạo một mô hình Hỗn hợp các Chuyên gia được kích hoạt thưa
từ một checkpoint dày đặc. Chúng tôi cho thấy rằng các mô hình ngôn ngữ T5 Base, Large, và
XL được tái chế thưa và các mô hình Vision Transformer Base và Large, tương ứng,
vượt trội đáng kể so với các đối tác dày đặc của chúng trên SuperGLUE và ImageNet,
chỉ sử dụng 50% chi phí tiền huấn luyện dày đặc ban đầu đã đầu tư. Các mô hình được tái chế
cũng vượt trội hơn các mô hình thưa được huấn luyện từ đầu với 100% ngân sách tính toán
tiền huấn luyện dày đặc ban đầu.¹

1 GIỚI THIỆU
Tăng quy mô là một trong những động lực chính cho hiệu suất tốt hơn trong học sâu. Từ BERT (Devlin
et al., 2019) đến GPT-3 (Brown et al., 2020) đến PaLM (Chowdhery et al., 2022) trong xử lý ngôn ngữ tự nhiên,
hay từ AlexNet (Krizhevsky et al., 2017) đến ViT-G (Zhai et al., 2022) trong thị giác, những bước đột phá
về hiệu suất đã được đạt được từ phần cứng, tập dữ liệu và kiến trúc lớn hơn. Xu hướng này
cũng đúng trong nhiều lĩnh vực khác, bao gồm tiếng nói (Baevski et al., 2020), học tăng cường
(Schrittwieser et al., 2020), học đa phương thức (Yu et al., 2022), và các ứng dụng khoa học
của học sâu (Jumper et al., 2021).

Tuy nhiên, hầu hết các mạng nơ-ron tiên tiến được huấn luyện từ đầu; nghĩa là, bắt đầu từ
các trọng số được khởi tạo ngẫu nhiên. Chi phí huấn luyện các mạng như vậy đang tăng nhanh chóng. Ví dụ, trong
ngôn ngữ, BERT-Large (345M tham số, được đề xuất vào năm 2018) yêu cầu ước tính 0,5 ZFLOPS
để huấn luyện, trong khi GPT-3 (175B tham số, từ năm 2020) yêu cầu 314 ZFLOPS (Brown et al., 2020),
và PaLM (540B tham số, từ năm 2022) yêu cầu 2527 ZFLOPS (Chowdhery et al., 2022). Kết quả
của những chi phí tính toán này, nghiên cứu về các mô hình ngôn ngữ lớn mới thường bị giới hạn cho
một số lượng nhỏ các nhóm có quyền truy cập vào nhiều tài nguyên. Để cho phép tiến bộ đáng kể hơn nữa,
chúng ta phải phát triển các cách rẻ hơn để huấn luyện các mô hình khổng lồ.

Trong bài báo này, chúng tôi khám phá tái chế mô hình: nâng cấp một mô hình hiện có với một
ngân sách tính toán bổ sung tương đối nhỏ. Cụ thể, chúng tôi tập trung vào tái chế các mô hình dày đặc
thành các Hỗn hợp các Chuyên gia (MoE) được kích hoạt thưa lớn hơn. Chúng tôi không sử dụng bất kỳ
nguồn dữ liệu độc đáo mới nào (Wei et al., 2021; Ouyang et al., 2022). Chúng tôi giả định sự tồn tại
của một checkpoint Transformer dày đặc đã được tiền huấn luyện (ví dụ (Wolf et al., 2020)), mà chúng tôi
sau đó sử dụng để khởi động nóng việc huấn luyện một MoE. Bằng cách tận dụng khả năng bổ sung
từ các lớp MoE, chúng tôi thu được một mô hình MoE hiệu suất cao hơn mô hình ban đầu, với chi phí
nhỏ hơn so với chi phí đã sử dụng để huấn luyện mô hình ban đầu. Trên tất cả các kích thước mô hình
mà chúng tôi nghiên cứu cho cả ngôn ngữ và thị giác, với ít hơn 40% ngân sách bổ sung, tái chế
cải thiện hiệu suất của mạng vượt quá những gì sẽ đạt được bằng cách tiếp tục huấn luyện
mô hình Transformer ban đầu.

Tái chế thưa có thể đặc biệt có giá trị trong hai kịch bản: (i) Người ta có quyền truy cập vào một
Transformer đã được tiền huấn luyện (có nhiều cái có sẵn công khai) và muốn cải thiện nó với một
ngân sách tính toán khiêm tốn hoặc bị hạn chế. (ii) Người ta đang lập kế hoạch huấn luyện một mô hình lớn,
và không biết liệu một mô hình dày đặc hay MoE sẽ hiệu quả hơn (cái sau thường có hiệu suất cao hơn,
nhưng thách thức hơn về mặt kỹ thuật để huấn luyện): người ta có thể có cả hai bằng cách
đầu tiên huấn luyện mô hình dày đặc, sau đó tái chế nó thành một mô hình MoE khi mô hình dày đặc
bão hòa.

Một thách thức trung tâm trong tái chế mô hình là vượt qua sự giảm hiệu suất ban đầu do
thay đổi cấu trúc của một mạng đã được huấn luyện. Chúng tôi trình bày một công thức phẫu thuật mô hình
hiệu quả trong cả thị giác và ngôn ngữ, và nhiều thí nghiệm loại bỏ cho các thành phần chính
làm cho nó hoạt động tốt. Trong các thí nghiệm trên Vision Transformers (Dosovitskiy et al., 2021)
và các mô hình ngôn ngữ T5 (Raffel et al., 2020), chúng tôi cho thấy rằng tái chế rất hiệu quả
khi ngân sách tính toán nằm giữa +10% và +60% chi phí để huấn luyện mạng ban đầu (dày đặc).
Ví dụ, tăng hiệu suất của ViT-B/16 ít nhất 1% trên ImageNet 10-shot yêu cầu thêm 58%
thời gian huấn luyện bổ sung (so với checkpoint ban đầu) nếu chúng ta tiếp tục huấn luyện mô hình dày đặc;
tuy nhiên, nó chỉ mất 13% thời gian huấn luyện bổ sung với phiên bản được tái chế. Tương tự,
các mô hình T5-Large và T5-Base được tái chế vượt trội hơn các đối tác dày đặc của chúng
1,5-2 điểm tuyệt đối trên SuperGLUE sử dụng 46% và 55% huấn luyện bổ sung, tương ứng.

2 KIẾN THỨC NỀN TẢNG
Trong phần này, chúng tôi tóm tắt các thành phần chính được sử dụng trong tái chế thưa: các mô hình
ngôn ngữ và thị giác dựa trên Transformer, và Hỗn hợp các Chuyên gia (MoE) được kích hoạt thưa.

2.1 HỖN HỢP CÁC CHUYÊN GIA ĐƯỢC KÍCH HOẠT THƯA (MOE)
Các mô hình dày đặc áp dụng tất cả các tham số cho mọi đầu vào. Theo đó, việc tăng khả năng mô hình
dẫn đến tăng chi phí tính toán. Các mô hình thưa cố gắng giảm bớt vấn đề cơ bản này bằng cách chỉ
kích hoạt một tập con các tham số cho mỗi đầu vào. Các mô hình Hỗn hợp các Chuyên gia (MoE) được kích hoạt thưa
là một họ các mô hình thưa thân thiện với bộ gia tốc cho phép huấn luyện các mô hình
với tới hàng nghìn tỷ tham số (Shazeer et al., 2017; Fedus et al., 2022).

Các mô hình MoE thường xen kẽ các khối Transformer dày đặc tiêu chuẩn với các khối MoE. Cụ thể,
chúng ta thường thay thế các MLP trong một khối Transformer bằng một số "chuyên gia" (thường là chính
các MLP) với các tham số có thể học khác nhau và một bộ định tuyến—một mạng nơ-ron nhỏ—quyết định
chuyên gia nào được áp dụng cho mỗi token riêng lẻ. Một số thuật toán định tuyến đã được phát triển,
ví dụ Top-K (Shazeer et al., 2017), các lớp BASE và Sinkhorn-BASE (Lewis et al., 2021; Clark
et al., 2022), các lớp Hash (Roller et al., 2021), và định tuyến Expert Choice (Zhou et al., 2022).

Chúng tôi tập trung chủ yếu vào định tuyến Expert Choice, hoạt động như sau. Gọi E là tổng số
chuyên gia trong một lớp MoE, và n là tổng số token. Bộ định tuyến xuất ra một ma trận
R ∈ R^(n×E) với các xác suất định tuyến, trong đó hàng r_i ∈ R^E tương ứng với token thứ i và là một
phân phối trên E chuyên gia (r_ij ≥ 0 và Σ_j r_ij = 1). Sau đó, mỗi chuyên gia e độc lập chọn
T token với xác suất cao nhất cho e (tức là, chúng ta thực hiện top-T trên mỗi cột) và xử lý chúng.
Chúng tôi tham số hóa T là T = C(n/E), trong đó C là một hệ số khả năng mà chúng tôi kiểm soát
để chọn nhiều hay ít token hơn trên mỗi chuyên gia. Khi C = 1, mỗi chuyên gia xử lý chính xác n/E token;
lưu ý rằng một số token có thể được xử lý bởi nhiều chuyên gia, trong khi những token khác thì không.
Điều này cho phép tăng số lượng tham số mô hình với overhead FLOPs tối thiểu.² Đặt C > 1 thường
dẫn đến hiệu suất cao hơn với chi phí tính toán cao hơn.

²Overhead FLOPs đến từ việc tính toán R của bộ định tuyến (tương đối khiêm tốn).

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

[Hình 1: Quy trình khởi tạo tái chế. Tất cả các tham số, và tùy chọn trạng thái tối ưu hóa của chúng,
được sao chép từ checkpoint ban đầu, ngoại trừ những tham số tương ứng với bộ định tuyến MoE,
không tồn tại trong kiến trúc ban đầu. Cụ thể, các chuyên gia trong lớp MoE mới là những bản sao
giống hệt nhau của lớp MLP ban đầu được thay thế.]

2.2 CÁC KIẾN TRÚC
Chúng tôi áp dụng cùng một công thức tái chế thưa cho cả nhiệm vụ ngôn ngữ và thị giác, tập trung vào
các kiến trúc T5 (encoder-decoder) (Raffel et al., 2020; Narang et al., 2021) và Vision Transformer (encoder)
(Dosovitskiy et al., 2021), tương ứng. Chúng tôi thường áp dụng cùng một hàm cổng và các siêu tham số MoE
trong các encoder của cả hai mô hình. Xem Phần 3.1 cho các lựa chọn thiết kế cụ thể
và Phụ lục A cho sự khác biệt giữa cài đặt tái chế thị giác và ngôn ngữ.

Thị giác. Vision Transformers (ViT) là các kiến trúc Transformer chỉ có encoder (Liu et al., 2021;
Radford et al., 2021; Touvron et al., 2021; He et al., 2022) tokenize và nhúng hình ảnh. Chúng tôi
tái chế các mô hình dựa trên các biến thể B/32, B/16, L/32 và L/16. Các MoE kết quả rộng rãi
tuân theo Vision MoE Transformers ("V-MoE") (Riquelme et al., 2021), với hai điểm khác biệt;
chúng tôi thực hiện global average pooling (Zhai et al., 2022) và sử dụng định tuyến Expert Choice.

Ngôn ngữ. Chúng tôi thí nghiệm với T5 (Raffel et al., 2020) encoder-decoder như mô hình ngôn ngữ
nguyên mẫu của chúng tôi. Chúng tôi tái chế các biến thể Base, Large, và XL của mô hình. Chúng tôi
thưa hóa cả encoder và decoder. Như trong cài đặt thị giác của chúng tôi, encoder mô hình áp dụng
định tuyến Expert Choice. Chúng tôi sử dụng định tuyến Top-K trong decoder với K = 2; xem thêm Phần 3.1.

3 THUẬT TOÁN TÁI CHẾ
Thuật toán được minh họa trong Hình 1. Để tái chế một mô hình, chúng ta cần các tham số của mô hình dày đặc
(tức là một checkpoint). Số lượng và hình dạng của các khối Transformer trong mô hình mới
giống hệt với trong mô hình dày đặc ban đầu. Một tập con các lớp MLP được mở rộng thành các lớp MoE.
Các lớp MLP còn lại, cùng với tất cả các lớp layer-norm và attention, và các lớp embedding và output
được sao chép từ mô hình ban đầu sang mô hình mới. Mỗi lớp MoE chứa một số lượng
chuyên gia cố định. Mỗi chuyên gia được khởi tạo như một bản sao của MLP ban đầu. Ngoài ra, chúng tôi
thêm một bộ định tuyến có trọng số được khởi tạo ngẫu nhiên. Trong Phần 4.2.2, chúng tôi thí nghiệm
với các biến thể khác nhau của công thức cơ bản này. Sau khi mô hình mới được tải và khởi tạo,
chúng tôi tiếp tục huấn luyện nó trong một số bước bổ sung tùy thuộc vào ngân sách và tài nguyên có sẵn.
Chúng tôi sử dụng các siêu tham số ban đầu: cùng kích thước batch, lịch trình tốc độ học,
và weight decay dẫn đến checkpoint ban đầu; xem thêm Phụ lục A cho chi tiết huấn luyện đầy đủ.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

3.1 CÁC QUYẾT ĐỊNH THIẾT KẾ
Hiệu suất của một mô hình được tái chế bị ảnh hưởng mạnh bởi cấu hình của các lớp MoE.
Tăng khả năng mô hình bằng cách tăng số lượng lớp được tái chế, số lượng chuyên gia hoặc
khả năng chuyên gia thường sẽ dẫn đến một mô hình chất lượng cao hơn, nhưng cũng sẽ tăng
chi phí tính toán và/hoặc dẫn đến sự giảm chất lượng ban đầu lớn hơn, do việc tái cấu hình
các lớp mạnh mẽ hơn.

Loại bộ định tuyến. Đối với các mô hình thị giác được tái chế và cho encoder của các mô hình ngôn ngữ được tái chế,
chúng tôi sử dụng định tuyến Expert Choice với hệ số khả năng C = 2. Để tránh sự khác biệt
thời gian huấn luyện (teacher forcing toàn batch) và thời gian suy luận (giải mã tự hồi quy token đơn),
chúng tôi sử dụng định tuyến Top-K (K = 2) trong decoder ngôn ngữ. Trong Phần 4.2.2, chúng tôi cho thấy
rằng định tuyến Expert Choice vượt trội hơn định tuyến Top-K tiêu chuẩn cho tái chế,
trong khi cả hai đều tốt hơn tiếp tục dày đặc.

Số lượng lớp để tái chế. Thêm nhiều lớp MoE hơn tăng khả năng mô hình một cách đáng kể,
với cái giá là tăng chi phí của mô hình, và cũng làm cho chất lượng của mô hình được tái chế
ban đầu giảm nhiều hơn so với mô hình dày đặc ban đầu. Dựa trên thí nghiệm loại bỏ của chúng tôi
trong Phần 4.2.2 và các quy ước thịnh hành trong tài liệu MoE (Lepikhin et al., 2021),
trừ khi được chỉ định khác, chúng tôi thay thế một nửa số lớp MLP trong các mô hình được tái chế
của chúng tôi bằng các lớp MoE.

Số lượng chuyên gia để thêm vào các lớp được tái chế. Mỗi chuyên gia mới cung cấp các tham số
có thể học mới mở rộng khả năng mô hình. Khả năng chuyên gia—số lượng token mà chuyên gia xử lý—
tỷ lệ nghịch với số lượng chuyên gia, do đó việc thêm nhiều chuyên gia hơn không ảnh hưởng đáng kể
đến FLOPS hoặc thời gian chạy của mô hình. Tuy nhiên, với một số lượng chuyên gia rất lớn,
mô hình được tái chế gặp phải sự giảm chất lượng ban đầu lớn hơn so với mô hình dày đặc cơ sở.
Với đủ tính toán tái chế, sự giảm ban đầu này có thể được khắc phục. Trong các nghiên cứu của chúng tôi,
chúng tôi tái chế với +20% đến +100% chi phí tính toán của mô hình dày đặc cơ sở ban đầu,
và trong chế độ này, chúng tôi thấy rằng 32 chuyên gia cung cấp một sự thỏa hiệp tốt.
Chúng tôi khám phá việc thay đổi số lượng chuyên gia trong Phần 4.2.2.

Khả năng chuyên gia. Bằng cách điều chỉnh khả năng chuyên gia, C, chúng tôi kiểm soát số lượng
chuyên gia xử lý mỗi token trung bình.³ Khả năng chuyên gia lớn hơn thường mang lại
chất lượng lớn hơn nhưng cũng tăng FLOPS và thời gian chạy. Mặc dù việc tăng khả năng chuyên gia
mang lại lợi ích chất lượng trên cơ sở mỗi bước, chúng tôi thấy rằng C = 2 thường cung cấp
chất lượng tốt trên cơ sở thời gian tính toán. Chúng tôi thí nghiệm loại bỏ thông qua các hệ số
khả năng khác nhau trong Phần 4.2.2.

Tiếp tục trạng thái tối ưu hóa (chỉ thị giác). Khi tái chế một mô hình, chúng ta có thể tiếp tục
trạng thái tối ưu hóa từ checkpoint dày đặc ban đầu cùng với các tham số mô hình. Trong Phụ lục B.6,
chúng tôi thấy rằng việc tái sử dụng trạng thái tối ưu hóa cung cấp một sự thúc đẩy hiệu suất
cho các mô hình thị giác. Tuy nhiên, chúng tôi không thấy bất kỳ cải thiện nào từ việc tái sử dụng
trạng thái tối ưu hóa mô hình dày đặc trong các thí nghiệm ngôn ngữ của chúng tôi, vì vậy chúng tôi
chỉ tái sử dụng trạng thái tối ưu hóa cho các mô hình thị giác.

Chuẩn hóa trọng số sau định tuyến (chỉ thị giác). Trong nỗ lực giảm sự giảm hiệu suất
khi áp dụng phẫu thuật mô hình tái chế, chúng tôi đã thử chuẩn hóa trọng số kết hợp của bộ định tuyến
của mỗi token về 1. Điều này tuân theo trực giác rằng mỗi token trước đây chỉ được xử lý bởi
một "chuyên gia" MLP duy nhất trong mô hình dày đặc. Phụ lục B.7 cho thấy rằng việc chuẩn hóa
trọng số bộ định tuyến giúp các mô hình thị giác được tái chế, nhưng làm tổn hại hiệu suất của
các mô hình ngôn ngữ được tái chế. Một giả thuyết cho hành vi khác biệt này là các mô hình thị giác
sử dụng định tuyến Expert Choice ở khắp nơi, nhưng các mô hình ngôn ngữ sử dụng Expert Choice
trong encoder và định tuyến Top-K trong decoder.

4 CÁC THÍ NGHIỆM
Trong phần này, chúng tôi trình bày các kết quả thí nghiệm chính của bài báo. Chúng tôi cũng chia sẻ
những điều rút ra từ một số thí nghiệm loại bỏ nhằm xác định các khía cạnh chính của thuật toán
của chúng tôi; kết quả đầy đủ được bao gồm trong Phụ lục B. Hầu hết các kết quả được trình bày
dưới dạng biểu đồ chất lượng vs. chi phí, trong đó chúng tôi sử dụng hiệu suất upstream hoặc downstream
để đo lường chất lượng, và thời gian huấn luyện theo TPU-core-days (như các chỉ số chi phí
nổi bật (Dehghani et al., 2021)) hoặc các bước huấn luyện (khi chi phí trên mỗi bước là như nhau
cho tất cả các mô hình được so sánh) để đo lường chi phí tính toán.

³Đối với định tuyến Expert Choice, khả năng nhiều hơn có nghĩa là mỗi chuyên gia có thể chọn nhiều token hơn.
Đối với định tuyến Top-K tiêu chuẩn, khả năng nhiều hơn có nghĩa là mỗi token có khả năng cao hơn
để phù hợp với bộ đệm của chuyên gia mong muốn.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

[Hình 2: Hiệu suất tiền huấn luyện đạt được bởi các phương pháp tiếp tục dày đặc và tái chế,
cho các biến thể Transformer khác nhau. Biểu đồ bên trái cho thấy hiệu suất trên nhiệm vụ thị giác
và biểu đồ bên phải trên nhiệm vụ văn bản. Trục x hiển thị thời gian tiền huấn luyện bổ sung (TPU-core-days),
so với tổng thời gian cần thiết để huấn luyện các checkpoint dày đặc ban đầu, cho mỗi kích thước.
Các đường ngang biểu thị chất lượng (trục y) của các checkpoint dày đặc ban đầu.]

4.1 CÀI ĐẶT THÍ NGHIỆM
Tất cả các thí nghiệm được tái chế bắt đầu từ một checkpoint mô hình dày đặc đã được tiền huấn luyện.
Vì tất cả các checkpoint dày đặc khởi đầu của chúng tôi được huấn luyện với lịch trình tốc độ học
căn bậc hai nghịch đảo, việc huấn luyện có thể được tiếp tục mà không có sự gián đoạn trong
lịch trình tốc độ học. Chúng tôi tái chế các mô hình và tiếp tục huấn luyện, hiển thị hiệu suất
cho các lượng huấn luyện tiếp tục khác nhau. Như một cơ sở, chúng tôi cũng tiếp tục huấn luyện
của mô hình dày đặc ban đầu ("tiếp tục dày đặc").

Các thí nghiệm thị giác. Các mô hình MoE Vision Transformers ("V-MoE") được huấn luyện
rộng rãi theo giao thức của Riquelme et al. (2021). Tiền huấn luyện upstream được thực hiện
trên JFT300M (Sun et al., 2017), với các chỉ số xác thực được tính toán trên một tập hợp
894,574 ví dụ. Việc chuyển giao few-shot tuân theo Dosovitskiy et al. (2021), theo đó một
bộ hồi quy bình phương tối thiểu dự đoán các lớp one-hot cho các biểu diễn hình ảnh đã được
đông lạnh. Chúng tôi xác thực thêm kết quả của mình trên ImageNet sử dụng 10-shot –
tức là 10 ví dụ huấn luyện trên mỗi lớp. Chúng tôi thực hiện điều này cho 5 tập huấn luyện
khác nhau, và báo cáo độ chính xác trung bình qua chúng. Đối với tinh chỉnh đầy đủ, chúng tôi
thay thế đầu tiền huấn luyện bằng một đầu được khởi tạo ngẫu nhiên, và tinh chỉnh toàn bộ mạng.
Xem Phụ lục A.2.2 để biết thêm chi tiết.

Các thí nghiệm ngôn ngữ. Các thí nghiệm ngôn ngữ của chúng tôi tuân theo cài đặt của Raffel et al. (2020):
chúng tôi tiền huấn luyện sử dụng nhiệm vụ span corruption trên tập dữ liệu English C4
(Raffel et al., 2020) và tinh chỉnh trên một hỗn hợp tỷ lệ của tất cả các nhiệm vụ SuperGLUE
(Wang et al., 2019) đồng thời. Chúng tôi bao gồm các chi tiết huấn luyện cụ thể trong Phụ lục A.2,
nhưng làm nổi bật một khía cạnh quan trọng ở đây: Đối với các kích thước mô hình Base, mà chúng tôi
thực hiện phần lớn các thí nghiệm loại bỏ của mình, chúng tôi tự tiền huấn luyện checkpoint khởi đầu
dày đặc cơ sở. Để làm nổi bật tính linh hoạt của thuật toán tái chế của chúng tôi, đối với các mô hình
Large và XL, chúng tôi thay vào đó bắt đầu tất cả các thí nghiệm từ các checkpoint T5 1.1 chính thức
(Narang et al., 2021; Roberts et al., 2022).

4.2 CÁC KẾT QUẢ THÍ NGHIỆM

4.2.1 KẾT QUẢ CỐT LÕI
Hình 2 hiển thị một so sánh chi tiết về các chỉ số upstream của các mô hình được tái chế
và các mô hình tiếp tục dày đặc ở các kích thước mô hình khác nhau cho cả thị giác (panel trái)
và ngôn ngữ (panel phải). Đối với bất kỳ kích thước mô hình và nhiệm vụ nào, chúng tôi quan sát
rằng các mô hình dày đặc và được tái chế thực hiện gần nhau khi chúng tôi áp dụng một ngân sách
huấn luyện bổ sung rất hạn chế – thực sự, gần với đường ngang gián đoạn đại diện cho hiệu suất
của checkpoint ban đầu. Khi chúng tôi áp dụng một lượng tính toán bổ sung không tầm thường,
một mô hình rõ ràng xuất hiện cho thấy những lợi ích mạnh mẽ được mang lại bởi kiến trúc được tái chế.

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

[Hình 3: Hiệu suất tinh chỉnh đầy đủ đạt được bởi các phương pháp tiếp tục dày đặc và tái chế.
Biểu đồ bên trái cho thấy hiệu suất trên ImageNet và biểu đồ bên phải trên các nhiệm vụ SuperGLUE.
Trục x hiển thị thời gian tiền huấn luyện bổ sung (TPU-core-days), so với tổng thời gian cần thiết
để huấn luyện các checkpoint dày đặc ban đầu, cho mỗi kích thước. Các đường ngang biểu thị
chất lượng (trục y) của các checkpoint dày đặc ban đầu.]

Hình 3 hiển thị hiệu suất sau khi tinh chỉnh các mô hình được huấn luyện trong Hình 2. Đối với
thị giác (panel trái), các lợi ích hiệu suất upstream thường chuyển giao khá sạch sẽ đến downstream.
Đối với ngôn ngữ (panel phải), có nhiều biến thiên hơn trong hiệu suất sau tinh chỉnh.
Tuy nhiên, xu hướng rõ ràng ủng hộ các mô hình ngôn ngữ được tái chế.

[Hình 4: Hiệu suất tiền huấn luyện đạt được bởi phương pháp tái chế và một mô hình MoE được huấn luyện
từ đầu, cho B/16 (biểu đồ trái, nhiệm vụ thị giác) và Base (biểu đồ phải, nhiệm vụ văn bản).
Trục x hiển thị thời gian tiền huấn luyện bổ sung (TPU-core-days), so với tổng thời gian cần thiết
để huấn luyện các checkpoint dày đặc ban đầu. Mô hình MoE được huấn luyện từ đầu chỉ bắt kịp
với mô hình ngôn ngữ được tái chế sau khoảng 120% ngân sách tính toán checkpoint dày đặc ban đầu
(hai chấm cam và xanh cuối cùng từ bên phải).]

Hình 4 so sánh tái chế thưa với các mô hình thưa được huấn luyện từ đầu. Vì huấn luyện từ đầu
không tái sử dụng chi phí tính toán đã đầu tư vào checkpoint dày đặc, nó mất nhiều thời gian hơn,
trên cơ sở thời gian huấn luyện bổ sung, để bắt kịp với các mô hình được tái chế. Mô hình MoE
ngôn ngữ được huấn luyện từ đầu yêu cầu khoảng 120% ngân sách tính toán của checkpoint dày đặc
ban đầu để bắt kịp với mô hình được tái chế. Các lợi ích chất lượng tương đối nhanh hơn,
trên cơ sở mỗi bước, của các mô hình MoE được huấn luyện từ đầu có thể được quy cho tốc độ học
tương đối lớn hơn và rằng các chuyên gia có thể phát triển và đa dạng hóa độc lập từ đầu.
Hình 4 gợi ý rằng, với một ngân sách tính toán rất lớn (>100% ngân sách tính toán của mô hình
dày đặc ban đầu), mô hình MoE-từ-đầu cuối cùng sẽ bắt kịp với mô hình được tái chế. Đối với
các chế độ tính toán lớn như vậy, có thể tốt hơn là huấn luyện các mô hình MoE từ đầu.
Đối với các ngân sách tính toán bị hạn chế hoặc giới hạn (<100% ngân sách tính toán ban đầu),
tái chế thưa là một sử dụng tài nguyên hiệu quả hơn.

[Hình 5: Hiệu suất tiền huấn luyện đạt được bởi tái chế thưa và khởi động nóng dày đặc
từ một checkpoint T5 Base (nhiệm vụ văn bản). Các trục x hiển thị thời gian tiền huấn luyện bổ sung
(TPU-core-days) và tiền huấn luyện bổ sung (Peta)FLOPs, so với các checkpoint dày đặc ban đầu.
Theo các khuyến nghị trong (Rae et al., 2021), chúng tôi chỉ tăng số lượng lớp khi khởi động nóng
("depth tiling") để khớp gần đúng với thời gian chạy của mô hình thưa.]

Cuối cùng, Hình 5 so sánh tái chế thưa với khởi động nóng ("tái chế dày đặc"). Chúng tôi khởi động nóng
các mô hình lớn hơn từ checkpoint Base dày đặc bằng cách nhân đôi các lớp mới ("depth tiling")
trong các mô hình tiling giống như trong (Rae et al., 2021). Các mô hình được tái chế dày đặc
nhanh chóng thấy lợi ích so với checkpoint dày đặc ban đầu, nhưng kém hiệu suất hơn mô hình thưa.
Chúng tôi không thử tăng các chiều ẩn của mô hình ("width tiling"), mà (Rae et al., 2021) thấy
là kém hiệu quả hơn.

4.2.2 CÁC THÍ NGHIỆM LOẠI BỎ
Trong phần này, chúng tôi tóm tắt các thí nghiệm loại bỏ kiến trúc và huấn luyện quan trọng
so với mô hình cơ sở được mô tả trong Phần 3. Kết quả đầy đủ được cung cấp trong Phụ lục B.
Trừ khi được nêu khác, các thí nghiệm loại bỏ thị giác sử dụng một mô hình thưa B/16 với
32 chuyên gia, C = 1 và 6 lớp MoE được đặt trong các khối cuối cùng của mô hình.
Checkpoint dày đặc được huấn luyện trong 14 epoch, và chúng tôi huấn luyện thêm 7 epoch
(lên đến tổng cộng 21 epoch). Đối với các thí nghiệm loại bỏ ngôn ngữ của chúng tôi,
cấu hình mặc định của chúng tôi không thay đổi: chúng tôi sử dụng một mô hình Base với
32 chuyên gia, C = 2 và 6 lớp MoE xen kẽ trong toàn bộ mô hình. Chúng tôi huấn luyện
trong khoảng 0,5 triệu đến 1 triệu bước bổ sung.

Lượng tiền huấn luyện dày đặc. Hiệu quả tái chế có thể, về nguyên tắc, phụ thuộc vào mức độ
hội tụ của mô hình dày đặc ban đầu. Để khám phá điều này, trong Hình 6, chúng tôi tái chế
một mô hình thị giác B/16 bắt đầu từ các checkpoint dày đặc khác nhau với lượng tiền huấn luyện
khác nhau. Từ một checkpoint dày đặc đã cho, chúng tôi so sánh tái chế và tiếp tục dày đặc
trong 200k bước. Độc lập với khi nào chúng tôi bắt đầu tái chế, cải thiện hiệu suất từ việc
thực hiện như vậy khá nhất quán.

Loại bộ định tuyến. Trong khi công thức tái chế mặc định của chúng tôi sử dụng định tuyến Expert Choice
(trong encoder), cùng một công thức có thể được áp dụng cho các cơ chế định tuyến khác.
Đối với thị giác, Phụ lục B.1 cho thấy rằng mặc dù định tuyến Top-K, với Batch Prioritized Routing (BPR)
(Riquelme et al., 2021), khớp với hiệu suất định tuyến Expert Choice trên cơ sở mỗi bước,
vì nó hơi chậm hơn, định tuyến Top-K kém hiệu suất hơn định tuyến Expert Choice trên cơ sở
thời gian huấn luyện. Lưu ý cả hai cách tiếp cận đều tốt hơn dày đặc.

Hệ số khả năng chuyên gia. Càng nhiều token được xử lý trên mỗi chuyên gia, lượng tính toán
trên mỗi ví dụ đầu vào càng lớn và (thường) chất lượng mô hình càng cao. Các mô hình thưa
của chúng tôi kiểm soát mượt mà điều này thông qua hệ số khả năng chuyên gia C. Phụ lục B.2
khám phá cách sự đánh đổi hiệu suất-tốc độ thay đổi như một hàm của C. Mặc dù việc tăng
khả năng chuyên gia mang lại lợi ích chất lượng trên cơ sở mỗi bước, chúng tôi thấy rằng
C = 2 thường cung cấp chất lượng tốt nhất trên cơ sở thời gian tính toán, cho cả mô hình
ngôn ngữ và thị giác.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2023

[Hình 6: Hiệu suất tái chế như một hàm của lượng bước tiền huấn luyện cho checkpoint dày đặc
ban đầu. Trục y hiển thị hiệu suất sau 200k bước huấn luyện thêm trên đỉnh checkpoint dày đặc
ban đầu, cho cả mô hình tiếp tục dày đặc và được tái chế. Trục x hiển thị thời gian huấn luyện
của checkpoint dày đặc ban đầu. Các lợi ích từ tái chế khá nhất quán độc lập với lượng
tiền huấn luyện ban đầu. Lưu ý: đối với thí nghiệm loại bỏ cụ thể này, chúng tôi sử dụng
một hệ số khả năng C = 1, để đảm bảo rằng FLOPS và thời gian chạy của mô hình dày đặc
và mô hình được tái chế thưa gần như có thể so sánh được trên cơ sở mỗi bước.]

Số lượng lớp MoE. Một quyết định chính khi tái chế một mô hình là bao nhiêu lớp thưa để thêm.
Khả năng mô hình và số lượng tham số tăng với số lượng lớp MoE, nhưng với cái giá là làm chậm
thời gian chạy mô hình. Phụ lục B.4 cung cấp một thí nghiệm loại bỏ về số lượng lớp MoE
trong một mô hình thị giác B/16 được tái chế. Trong trường hợp này, khoảng 6 lớp MoE
(trong tổng số 12 lớp) cung cấp chất lượng độ chính xác tốt nhất, mặc dù ít lớp thưa hơn
làm giảm chi phí tính toán.

Khởi tạo các chuyên gia. Công thức tái chế tiêu chuẩn sao chép và nhân đôi MLP dày đặc
cho mỗi chuyên gia. Khi bộ định tuyến chỉ đạo các token khác nhau đến mỗi chuyên gia,
các chuyên gia sẽ bắt đầu phân kỳ với nhau, và từ trọng số MLP ban đầu của chúng.
Phụ lục B.5 so sánh công thức tiêu chuẩn với việc khởi tạo ngẫu nhiên các chuyên gia
(tức là huấn luyện chúng từ đầu). Đối với các ngân sách tính toán hạn chế, việc khởi tạo
ngẫu nhiên các chuyên gia kém hiệu suất hơn công thức tái chế tiêu chuẩn; đối với các ngân sách
tính toán lớn hơn, nó cuối cùng khớp với hiệu suất công thức tái chế tiêu chuẩn.

Chúng tôi cũng đã thử sao chép trọng số MLP ban đầu và thêm nhiễu độc lập (Gaussian)
cho mỗi chuyên gia, trong nỗ lực thúc đẩy đa dạng hóa chuyên gia nhiều hơn. Việc thêm
quá nhiều nhiễu khi sao chép trọng số MLP vào các chuyên gia làm tổn hại hiệu suất,
trong khi việc thêm lượng nhỏ nhiễu có ít hoặc không có tác dụng; xem thêm Phụ lục B.9.

Số lượng chuyên gia. Việc thêm nhiều chuyên gia hơn tăng số lượng tham số mô hình và,
đến một điểm nào đó, chất lượng của mô hình. Cho rằng số lượng token mà mỗi chuyên gia
xử lý tỷ lệ nghịch với số lượng chuyên gia (xem Phần 2.1), việc thêm nhiều chuyên gia hơn
không ảnh hưởng đáng kể đến FLOPS của mô hình cũng như thời gian chạy của nó. Tuy nhiên,
đối với một số lượng chuyên gia rất lớn, mô hình được tái chế có thể gặp phải sự giảm chất lượng
ban đầu lớn hơn so với mô hình dày đặc cơ sở. Phụ lục B.3 khám phá sự đánh đổi này
và cho thấy rằng, ít nhất đối với các mô hình kích thước Base, nhiều chuyên gia hơn thường
mang lại hiệu suất tốt hơn.

5 CÁC CÔNG TRÌNH LIÊN QUAN

Tái sử dụng các tham số đã huấn luyện. Các công trình trước đây đã tập trung vào tăng tốc
huấn luyện thông qua khởi động nóng bằng cách tái sử dụng các tham số của một mô hình hiện có.
Berner et al. (2019) khám phá ý tưởng tái sử dụng phiên bản trước của một mô hình đã huấn luyện
trong một quá trình huấn luyện dài sử dụng một môi trường đang phát triển. Cho một mô hình
đã huấn luyện, Net2Net (Chen et al., 2015) đề xuất một khởi tạo bảo toàn hàm để khởi động nóng
việc huấn luyện một mô hình sâu hơn hoặc rộng hơn. Gần đây, Gopher (Rae et al., 2021) cũng
khám phá việc khởi động nóng các mô hình lớn hơn từ các mô hình nhỏ hơn trong một chế độ
huấn luyện tính toán lớn và cho thấy rằng mô hình lớn hơn, được khởi động nóng có thể hội tụ
đến một chất lượng có thể so sánh với mô hình tương đương được huấn luyện từ đầu. Trong Phần 4.2.1,
chúng tôi cho thấy rằng khởi động nóng kém hiệu suất đáng kể so với tái chế thưa. Yang et al. (2021);
Lin et al. (2021) cho thấy rằng họ có thể giảm số lượng lần lặp huấn luyện với các mô hình
ban đầu chia sẻ tham số qua các lớp (Lan et al., 2019; Dehghani et al., 2018) nhưng dần dần
hủy chia sẻ (hoặc "delink") các tham số trong khi huấn luyện.

Trong nỗ lực giảm tổng chi phí huấn luyện, một số công trình khám phá việc tăng trưởng
các mô hình một cách tiệm tiến trong quá trình huấn luyện (Gong et al., 2019; Dong et al., 2020;
Li et al., 2020; Shen et al., 2022). Ý tưởng cốt lõi là phân tách quá trình huấn luyện thành
các giai đoạn, mỗi giai đoạn áp dụng các toán tử tăng trưởng để tăng kích thước mô hình
từ giai đoạn trước bằng cách sao chép trọng số hoặc xếp chồng các lớp mới lên trên.
Trong một số trường hợp, mỗi giai đoạn huấn luyện sẽ chỉ cập nhật các tham số của các lớp mới,
điều này tiết kiệm chi phí của một tính toán ngược đầy đủ (Yang et al., 2020). Gu et al. (2020)
cho thấy rằng việc chia tỷ lệ hợp chất (chia tỷ lệ độ sâu, chiều rộng và độ dài đầu vào cùng nhau)
là thuận lợi và đề xuất một chiến lược với các toán tử tăng trưởng khác nhau trên mỗi chiều.

Tái chế thưa, mà chúng tôi giới thiệu trong bài báo này, tuân theo một động lực tương tự.
Tuy nhiên, không giống như các công trình trên, chúng tôi tập trung vào các chế độ tính toán
là một phần nhỏ của huấn luyện mô hình ban đầu. Chúng tôi cũng trình bày một công thức
để tăng trưởng một mô hình dày đặc đã huấn luyện thành một mô hình thưa, thay vì một mô hình
dày đặc lớn hơn. Điều này cho phép chúng tôi tận hưởng khả năng bổ sung do tăng tham số,
trong khi duy trì chi phí suy luận do tính thưa của tính toán.

Tỉa. Tỉa thường được sử dụng như một tìm kiếm kiến trúc sau huấn luyện để xây dựng
các mô hình nhỏ hơn và nhanh hơn từ các mô hình lớn hơn (LeCun et al., 1989; Gale et al., 2019;
Blalock et al., 2020). Tuy nhiên, "tỉa động" (Evci et al., 2020) cũng đã được sử dụng
trong quá trình huấn luyện để tìm các kiến trúc thưa hơn từ các mô hình dày đặc. Tương tự như tỉa,
tái chế thưa cũng giới thiệu tính thưa vào một mô hình dày đặc, tuy nhiên, không giống như tỉa,
chúng tôi tăng trưởng các mô hình dày đặc hiện có thành một mô hình thưa lớn hơn.

Hỗn hợp các Chuyên gia được kích hoạt thưa (MoE). Trong công trình này, chúng tôi thưa hóa
các mô hình dày đặc hiện có thành các mô hình MoE. Các mô hình MoE (Shazeer et al., 2017)
cung cấp lời hứa tăng quy mô mô hình (số lượng tham số) với sự tăng chi phí tính toán dưới tuyến tính
(FLOPS). Gần đây, đã có một số lượng công trình MoE ngày càng tăng đạt được chất lượng
tiên tiến và lợi ích hiệu quả đáng kể trên cả nhiệm vụ ngôn ngữ và thị giác (Lepikhin et al., 2021;
Fedus et al., 2022; Riquelme et al., 2021; Artetxe et al., 2021; Du et al., 2021; Zoph et al., 2022;
Mustafa et al., 2022). Tất cả các mô hình này đều lớn và được huấn luyện từ đầu với trọng số
được khởi tạo ngẫu nhiên và kiến trúc cố định.

Một số công trình MoE cũng đã cố gắng cải thiện các thuật toán huấn luyện thông thường
bằng cách thích ứng hoặc "phát triển" kiến trúc mô hình trong quá trình huấn luyện. Nie et al. (2021)
thưa hóa các lớp MoE một cách tiệm tiến trong quá trình huấn luyện bằng cách từ từ điều chỉnh
hàm cổng từ một "cài đặt dày đặc", trong đó tất cả các token được định tuyến đến tất cả
các chuyên gia, đến một "cài đặt thưa" hoàn toàn, trong đó các token chỉ được định tuyến
đến một tập con các chuyên gia. Zhang et al. (2022) quan sát rằng, đối với hầu hết các đầu vào,
chỉ một phần nhỏ các kích hoạt MLP của Transformer là khác không. Dựa trên điều này,
họ đề xuất quy trình thưa hóa chia các tham số của các khối MLP thành nhiều chuyên gia
và thêm một cơ chế định tuyến. Tương tự, Zuo et al. (2022) chia các MLP trong một mô hình
dày đặc đã được tiền huấn luyện thành nhiều chuyên gia để tạo thành một mô hình thưa
để tinh chỉnh. Gần gũi với công trình của chúng tôi, Wu et al. (2022) trình bày một thuật toán
mới để thưa hóa các mô hình dày đặc trong bối cảnh tinh chỉnh trên các nhiệm vụ phát hiện
và phân đoạn. Tương tự như Nie et al. (2021), sự giảm hiệu suất ban đầu khi huấn luyện
trên tập dữ liệu ban đầu được tránh bằng cách áp dụng một hỗn hợp dày đặc của các chuyên gia
trong lượt truyền tiến. Tuy nhiên, ở quy mô lớn mục tiêu của chúng tôi, việc kích hoạt đồng thời
tất cả các chuyên gia cho mỗi token là không khả thi. Cuối cùng, Gururangan et al. (2022)
thích ứng các mô hình ngôn ngữ chuyên gia lĩnh vực thưa với các lĩnh vực mới bằng cách
khởi tạo một chuyên gia lĩnh vực mới từ chuyên gia hiện có có khả năng cao nhất
dưới phân phối hậu lĩnh vực.

6 KẾT LUẬN
Huấn luyện các mạng nơ-ron lớn trên các tập dữ liệu khổng lồ đã chứng minh là một xu hướng
thành công đáng kể trong nghiên cứu học sâu, đặc biệt là trong những năm gần đây. Nó cũng
đã chứng minh là rất tốn kém về mặt tính toán. Các mô hình đã được tiền huấn luyện hiện
có sẵn rộng rãi, do đó làm cho nhiều học viên có thể tinh chỉnh và thích ứng thêm
các kiến trúc mô hình cố định trên dữ liệu quan tâm của họ. Tuy nhiên, tiến bộ đáng kể
yêu cầu cung cấp tính linh hoạt hơn trong việc thích ứng và cải thiện bản thân kiến trúc mô hình.

Chúng tôi đề xuất một công thức đơn giản để tái sử dụng các checkpoint dày đặc đã được
tiền huấn luyện để khởi tạo các mô hình thưa mạnh mẽ hơn. Thuật toán của chúng tôi tận dụng
tính toán và trọng số của mô hình đã được tiền huấn luyện, và cung cấp một chuyển đổi mượt mà
đến các mô hình Hỗn hợp các Chuyên gia được kích hoạt thưa cung cấp nhiều khả năng
và tính linh hoạt hơn khi suy luận. Chúng tôi trình bày các kết quả thí nghiệm cho cả
mô hình thị giác và ngôn ngữ ở các quy mô khác nhau; những kết quả này chứng minh
các lợi ích hiệu suất lớn so với việc tiếp tục mô hình dày đặc. Các thí nghiệm loại bỏ
của chúng tôi làm nổi bật tầm quan trọng của các lựa chọn thuật toán cẩn thận, và gợi ý
các khía cạnh chính cần xem xét khi cố gắng tìm sự đánh đổi hiệu suất-chi phí tốt
cho các ngân sách tính toán cụ thể.

Học chuyển giao và điều chỉnh prompt đang trở nên ngày càng phổ biến, và có lý do chính đáng.
Nó cho phép tái sử dụng và điều chỉnh các mô hình bởi một cơ sở rộng lớn hơn của các nhà nghiên cứu
và học viên có thể chỉ có quyền truy cập vào tài nguyên tính toán và dữ liệu hạn chế.
Theo đó, chúng tôi tin rằng các kỹ thuật nhằm tăng trưởng các mô hình hiện có, phân chia
hoặc đông lạnh các mô-đun con, nhân đôi và sau đó tách ghép các thành phần mô hình,
và cuối cùng tiếp tục huấn luyện một cách mượt mà sau phẫu thuật mô hình, sẽ chứng minh
là thiết yếu cho một hệ sinh thái động của các mô hình. Chúng tôi tóm tắt quá trình như vậy
là tái chế, và cung cấp một phiên bản đầu tiên trong bối cảnh các mô hình thưa. Chúng tôi
mong chờ các phần mở rộng và cải tiến mới trên ý tưởng đơn giản này.

TÀI LIỆU THAM KHẢO
[Danh sách tài liệu tham khảo được dịch giữ nguyên định dạng và nội dung tiếng Anh]

--- TRANG 10 ---
[Tiếp tục dịch các trang còn lại với cùng cách thức...]
