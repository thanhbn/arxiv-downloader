# 2112.06905.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2112.06905.pdf
# Kích thước tệp: 968210 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
GLaM: Mở rộng Hiệu quả các Mô hình Ngôn ngữ với Mixture-of-Experts
Nan Du* 1Yanping Huang* 1Andrew M. Dai* 1Simon Tong1Dmitry Lepikhin1Yuanzhong Xu1
Maxim Krikun1Yanqi Zhou1Adams Wei Yu1Orhan Firat1Barret Zoph1Liam Fedus1Maarten Bosma1
Zongwei Zhou1Tao Wang1Yu Emma Wang1Kellie Webster1Marie Pellat1Kevin Robinson1
Kathleen Meier-Hellstern1Toju Duke1Lucas Dixon1Kun Zhang1Quoc V Le1Yonghui Wu1
Zhifeng Chen1Claire Cui1

Tóm tắt
Việc mở rộng các mô hình ngôn ngữ với nhiều dữ liệu, tài nguyên tính toán và tham số hơn đã thúc đẩy tiến bộ đáng kể trong xử lý ngôn ngữ tự nhiên. Ví dụ, nhờ việc mở rộng, GPT-3 đã có thể đạt được kết quả mạnh mẽ trong các tác vụ học trong ngữ cảnh. Tuy nhiên, việc huấn luyện các mô hình dày đặc lớn này đòi hỏi một lượng tài nguyên tính toán đáng kể. Trong bài báo này, chúng tôi đề xuất và phát triển một họ mô hình ngôn ngữ có tên GLaM (Generalist Language Model), sử dụng kiến trúc mixture-of-experts được kích hoạt thưa thớt để mở rộng khả năng của mô hình trong khi cũng tiêu tốn chi phí huấn luyện thấp hơn đáng kể so với các biến thể dày đặc. GLaM lớn nhất có 1.2 nghìn tỷ tham số, lớn hơn khoảng 7 lần so với GPT-3. Nó chỉ tiêu thụ 1/3 năng lượng được sử dụng để huấn luyện GPT-3 và yêu cầu một nửa số phép tính FLOPs để suy luận, trong khi vẫn đạt được hiệu suất tổng thể tốt hơn trong học zero-shot, one-shot và few-shot trên 29 tác vụ NLP.

1. Giới thiệu
Các mô hình ngôn ngữ đã đóng vai trò quan trọng trong tiến bộ của xử lý ngôn ngữ tự nhiên (NLP) trong thập kỷ qua. Các biến thể của mô hình ngôn ngữ đã được sử dụng để tạo ra các vector từ được tiền huấn luyện (Mikolov et al., 2013; Pennington et al., 2014), và các vector từ theo ngữ cảnh (Peters et al., 2018; Devlin et al., 2019) cho nhiều ứng dụng NLP. Xu hướng chuyển sang mở rộng với nhiều dữ liệu và mô hình lớn hơn (Shazeer et al., 2017; Huang et al., 2019; Kaplan et al., 2020) đã cho phép thực hiện các tác vụ ngôn ngữ tự nhiên phức tạp với ít dữ liệu được gán nhãn hơn. Ví dụ, GPT-3 (Brown et al., 2020) và FLAN (Wei et al., 2021) đã chứng minh khả năng thi hành của học trong ngữ cảnh cho việc tổng quát hóa few-shot hoặc thậm chí zero-shot, có nghĩa là cần rất ít ví dụ được gán nhãn để đạt được hiệu suất tốt trong các ứng dụng NLP. Mặc dù hiệu quả và có hiệu suất cao, việc mở rộng thêm đang trở nên đắt đỏ một cách cấm đoán và tiêu thụ một lượng năng lượng đáng kể (Patterson et al., 2021).

Bảng 1. So sánh giữa GPT-3 và GLaM. Tóm lại, GLaM vượt trội hơn GPT-3 trên 21 bảng đánh giá hiểu ngôn ngữ tự nhiên (NLU) và 8 bảng đánh giá tạo ngôn ngữ tự nhiên (NLG) trung bình trong khi sử dụng khoảng một nửa FLOPs mỗi token trong suốt quá trình suy luận và tiêu thụ khoảng một phần ba năng lượng để huấn luyện.

GPT-3 GLaM chi phí tương đối
FLOPs / token (G) 350 180 −48.6%
Năng lượng huấn luyện (MWh) 1287 456 −64.6%
độ chính xác trung bình
Zero-shot 56.9 62.7 +10.2%
One-shot 61.6 65.5 +6.3%
Few-shot 65.2 68.1 +4.4%

Trong công trình này, chúng tôi chỉ ra rằng một mạng lớn được kích hoạt thưa thớt có thể đạt được kết quả cạnh tranh so với các mô hình dày đặc tiên tiến trong các tác vụ few-shot trong khi hiệu quả về mặt tính toán hơn. Chúng tôi trình bày một họ mô hình ngôn ngữ tổng quát được gọi là GLaM, tạo ra sự cân bằng giữa tính toán dày đặc và có điều kiện. Phiên bản lớn nhất của GLaM có tổng cộng 1.2T tham số với 64 chuyên gia trên mỗi lớp MoE (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2021) trong đó mỗi token trong batch đầu vào chỉ kích hoạt một mạng con của 96.6B (8% của 1.2T) tham số. Trong học zero, one và few-shot, mô hình này so sánh thuận lợi với GPT-3 (175B), với hiệu quả học tập được cải thiện đáng kể trên 29 bảng đánh giá NLP công khai, từ các tác vụ hoàn thành ngôn ngữ, tác vụ QA miền mở, đến các tác vụ suy luận ngôn ngữ tự nhiên. Nhờ vào kiến trúc được kích hoạt thưa thớt và việc triển khai hiệu quả của thuật toán song song hóa mô hình, tổng mức tiêu thụ năng lượng trong quá trình huấn luyện chỉ bằng một phần ba so với GPT-3. Chúng tôi nhấn mạnh sự so sánh giữa phiên bản lớn nhất của GLaM và GPT-3 trong Bảng 1 và Hình 1.

--- TRANG 2 ---
GLaM: Mở rộng Hiệu quả các Mô hình Ngôn ngữ với Mixture-of-Experts

[Các biểu đồ và hình ảnh hiển thị so sánh hiệu suất giữa GLaM và GPT-3 trong các tác vụ khác nhau]

Hình 1. Tổng quan về phần trăm thay đổi trong hiệu suất dự đoán (cao hơn là tốt hơn) của GLaM (64B/64E) so với GPT-3 (175B) trong cài đặt (a) zero-shot, (b) one-shot, và (c) few-shot trên 7 danh mục bảng đánh giá với tổng cộng 29 tác vụ công khai. Mỗi thanh trong bảng (a), (b) và (c) đại diện cho một danh mục bảng đánh giá. Bảng (d) so sánh FLOPs cần thiết cho mỗi dự đoán token và mức tiêu thụ năng lượng huấn luyện.

Chúng tôi sử dụng GLaM để nghiên cứu tầm quan trọng của dữ liệu. Phân tích của chúng tôi cho thấy rằng ngay cả đối với các mô hình lớn này, chất lượng dữ liệu không nên bị hy sinh vì số lượng nếu mục tiêu là tạo ra một mô hình ngôn ngữ tự hồi quy chất lượng cao. Quan trọng hơn, về mặt xã hội, kết quả của chúng tôi cũng là đầu tiên, theo hiểu biết của chúng tôi, đóng lại khoảng cách hiệu suất giữa các ví dụ định kiến và phản định kiến trên bảng đánh giá WinoGender, gợi ý rằng các mô hình lớn được kích hoạt thưa thớt có thể dựa ít hơn vào các tương quan thống kê bề mặt.

Cuối cùng, mặc dù các mô hình thưa thớt dựa trên MoE chưa phổ biến trong cộng đồng NLP, công trình của chúng tôi cho thấy rằng các mô hình ngôn ngữ thưa thớt chỉ giải mã có thể hiệu quả hơn các kiến trúc dày đặc có FLOPs tính toán tương tự lần đầu tiên trong bối cảnh học few-shot trong ngữ cảnh ở quy mô lớn, gợi ý rằng tính thưa thớt là một trong những hướng đầy hứa hẹn nhất để đạt được các mô hình NLP chất lượng cao trong khi tiết kiệm chi phí năng lượng (Patterson et al., 2021). Do đó, MoE nên được coi là một ứng viên mạnh mẽ cho việc mở rộng trong tương lai.

2. Công trình Liên quan
Mô hình ngôn ngữ. Các mô hình ngôn ngữ thần kinh (Mikolov et al., 2010; Sutskever et al., 2011) đã được chứng minh là hữu ích cho nhiều tác vụ xử lý ngôn ngữ tự nhiên. Các mô hình nhúng từ và mở rộng như word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) và paragraph vectors (Le & Mikolov, 2014) đã cho thấy khả năng tổng quát hóa tốt cho nhiều tác vụ chỉ bằng cách chuyển giao các embedding.

Tiền huấn luyện và Tinh chỉnh. Sự dồi dào của tài nguyên tính toán và dữ liệu cho phép huấn luyện các mô hình ngày càng lớn thông qua tiền huấn luyện không giám sát. Đây là một sự phù hợp tự nhiên cho việc huấn luyện mạng thần kinh vì chúng thể hiện khả năng mở rộng đáng chú ý. Công trình về việc sử dụng các mô hình tái phát như RNN và LSTM cho biểu diễn ngôn ngữ (Dai & Le, 2015; Kiros et al., 2015) đã cho thấy rằng các mô hình ngôn ngữ tổng quát có thể được tinh chỉnh để cải thiện các tác vụ hiểu ngôn ngữ khác nhau. Gần đây hơn, các mô hình sử dụng Transformer (Vaswani et al., 2017) đã cho thấy rằng các mô hình lớn hơn với tự giám sát trên dữ liệu chưa gán nhãn có thể mang lại cải thiện đáng kể về các tác vụ NLP (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Clark et al., 2020). Học chuyển giao dựa trên tiền huấn luyện và tinh chỉnh (Raffel et al., 2020; Houlsby et al., 2019) đã được nghiên cứu rộng rãi và chứng minh hiệu suất tốt trên các tác vụ downstream. Tuy nhiên, một hạn chế chính của phương pháp này là nó đòi hỏi một việc tinh chỉnh cụ thể cho từng tác vụ.

Học Few-shot Trong Ngữ cảnh. GPT-3 (Brown et al., 2020) và các công trình liên quan (Shoeybi et al., 2019; Lieber et al., 2021; Wei et al., 2021) đã chứng minh rằng việc mở rộng các mô hình ngôn ngữ cải thiện đáng kể hiệu suất few-shot bất khả tri tác vụ. Các mô hình ngôn ngữ này được áp dụng mà không cần bất kỳ cập nhật gradient nào, và chỉ cần các minh chứng few-shot được chỉ định hoàn toàn thông qua tương tác văn bản với mô hình.

Mạng Gated Thưa thớt. Các mô hình dựa trên Mixture-of-Experts cũng đã cho thấy những lợi thế đáng kể. Đối với mô hình hóa ngôn ngữ và dịch máy, Shazeer et al. (2017) đã chỉ ra rằng họ có thể sử dụng hiệu quả một số lượng trọng số rất lớn trong khi chỉ cần tính toán một tập con nhỏ của đồ thị tính toán tại thời điểm suy luận. Cũng đã có công trình về việc mở rộng các kiến trúc MoE được kích hoạt thưa thớt (Hestness et al., 2017; Shazeer et al., 2018; Lepikhin et al., 2021; Kudugunta et al., 2021). Gần đây, Fedus et al. (2021) đã cho thấy kết quả với các mô hình được kích hoạt thưa thớt thậm chí lớn hơn 1 nghìn tỷ tham số (Switch-C). Mặc dù cả Switch-C và mô hình GLaM lớn nhất đều có một nghìn tỷ tham số có thể huấn luyện, GLaM là một họ mô hình ngôn ngữ chỉ giải mã, và Switch-C là một mô hình chuỗi-tới-chuỗi dựa trên encoder-decoder. Hơn nữa, Switch-C chủ yếu được đánh giá trên các bảng đánh giá tinh chỉnh, ví dụ SuperGlue, trong khi GLaM hoạt động tốt mà không cần bất kỳ tinh chỉnh nào trong cài đặt few-shot được chia sẻ bởi GPT-3 nơi SuperGlue là một tập con. Bảng 2 tóm tắt những khác biệt chính giữa GLaM và các mô hình liên quan được tiền huấn luyện trên corpus văn bản.

Bảng 2. Một mẫu các mô hình liên quan (Devlin et al., 2019; Raffel et al., 2020; Brown et al., 2020; Lieber et al., 2021; Rae et al., 2021; Shoeybi et al., 2019; Lepikhin et al., 2021; Fedus et al., 2021) được tiền huấn luyện trên corpus văn bản. nparams là tổng số tham số mô hình có thể huấn luyện, nact-params là số tham số mô hình được kích hoạt cho mỗi token đầu vào.

Tên Mô hình Loại Mô hình nparams nact-params
BERT Dense Encoder-only 340M 340M
T5 Dense Encoder-decoder 13B 13B
GPT-3 Dense Decoder-only 175B 175B
Jurassic-1 Dense Decoder-only 178B 178B
Gopher Dense Decoder-only 280B 280B
Megatron-530B Dense Decoder-only 530B 530B
GShard-M4 MoE Encoder-decoder 600B 1.5B
Switch-C MoE Encoder-decoder 1.5T 1.5B
GLaM (64B/64E) MoE Decoder-only 1.2T 96.6B

3. Tập dữ liệu Huấn luyện
Để huấn luyện mô hình của chúng tôi, chúng tôi xây dựng một tập dữ liệu chất lượng cao gồm 1.6 nghìn tỷ token đại diện cho một loạt các trường hợp sử dụng ngôn ngữ tự nhiên. Các trang web tạo nên phần lớn dữ liệu trong tập dữ liệu không gán nhãn của chúng tôi. Tuy nhiên, chất lượng của chúng dao động từ viết chuyên nghiệp đến các trang bình luận và diễn đàn chất lượng thấp. Tương tự như Brown et al. (2020), chúng tôi phát triển bộ phân loại chất lượng văn bản riêng để tạo ra một corpus web chất lượng cao từ một corpus thô lớn hơn ban đầu. Chúng tôi sử dụng một bộ phân loại tuyến tính dựa trên feature hash cho tốc độ suy luận. Bộ phân loại này được huấn luyện để phân loại giữa một bộ sưu tập văn bản được tuyển chọn (Wikipedia, sách và một số trang web được chọn) và các trang web khác. Chúng tôi sử dụng bộ phân loại này để ước tính chất lượng nội dung của một trang web. Sau đó chúng tôi áp dụng bộ phân loại này bằng cách sử dụng phân phối Pareto để lấy mẫu các trang web theo điểm số của chúng. Điều này cho phép một số trang web chất lượng thấp hơn được bao gồm để ngăn ngừa sự thiên vị hệ thống trong bộ phân loại (Brown et al., 2020).

Bảng 3. Dữ liệu và trọng số hỗn hợp trong tập huấn luyện GLaM.
Tập dữ liệu Tokens (B) Trọng số trong hỗn hợp
Trang web được lọc 143 0.42
Wikipedia 3 0.06
Cuộc hội thoại 174 0.28
Diễn đàn 247 0.02
Sách 390 0.20
Tin tức 650 0.02

Chúng tôi sử dụng quy trình này để tạo ra một tập con chất lượng cao được lọc từ các trang web và kết hợp với sách, trang Wikipedia, diễn đàn và trang tin tức và các nguồn dữ liệu khác để tạo ra tập dữ liệu GLaM cuối cùng. Chúng tôi cũng kết hợp dữ liệu từ các cuộc hội thoại trên mạng xã hội thuộc phạm vi công cộng được sử dụng bởi Adiwardana et al. (2020). Chúng tôi thiết lập trọng số hỗn hợp dựa trên hiệu suất của từng thành phần trong một mô hình nhỏ hơn và để ngăn ngừa các nguồn nhỏ như Wikipedia bị lấy mẫu quá mức. Bảng 3 hiển thị chi tiết về kích thước thành phần dữ liệu và trọng số hỗn hợp của chúng tôi. Trọng số hỗn hợp được chọn dựa trên hiệu suất của thành phần trong một mô hình nhỏ và để ngăn ngừa các tập dữ liệu nhỏ như Wikipedia bị lấy mẫu quá mức. Để kiểm tra ô nhiễm dữ liệu, trong Mục D chúng tôi tiến hành phân tích chồng chéo giữa tập huấn luyện và dữ liệu đánh giá và thấy rằng nó khá phù hợp với công trình trước đó (Brown et al., 2020).

4. Kiến trúc Mô hình
Chúng tôi tận dụng Mixture-of-Experts (MoE) được kích hoạt thưa thớt (Shazeer et al., 2017; Fedus et al., 2021) trong các mô hình GLaM. Tương tự như GShard MoE Transformer (Lepikhin et al., 2021), chúng tôi thay thế thành phần feed-forward của mỗi lớp Transformer khác với một lớp MoE, như được hiển thị trong Hình 2. Mỗi lớp MoE bao gồm một tập hợp các mạng feed-forward độc lập như các 'chuyên gia'. Một hàm gating sau đó sử dụng hàm kích hoạt softmax để mô hình hóa phân phối xác suất trên các chuyên gia này. Phân phối này chỉ ra mức độ tốt của mỗi chuyên gia trong việc xử lý đầu vào đến.

Hình 2. Kiến trúc mô hình GLaM. Mỗi lớp MoE (khối dưới) được xen kẽ với một lớp Transformer (khối trên). Đối với mỗi token đầu vào, ví dụ 'roses', mô-đun Gating động lực chọn hai chuyên gia liên quan nhất trong số 64, được đại diện bởi lưới màu xanh trong lớp MoE. Trung bình có trọng số của các đầu ra từ hai chuyên gia này sau đó sẽ được chuyển đến lớp Transformer phía trên. Đối với token tiếp theo trong chuỗi đầu vào, hai chuyên gia khác nhau sẽ được chọn.

Mặc dù mỗi lớp MoE có nhiều tham số hơn, các chuyên gia được kích hoạt thưa thớt. Điều này có nghĩa là đối với một token đầu vào cho trước, chỉ một tập con hạn chế các chuyên gia được sử dụng, cho mô hình nhiều khả năng hơn trong khi giới hạn tính toán. Trong kiến trúc của chúng tôi, kích thước tập con là hai. Mỗi mạng gating có thể học được của lớp MoE được huấn luyện để sử dụng đầu vào của nó để kích hoạt hai chuyên gia tốt nhất cho mỗi token của một chuỗi đầu vào. Trong quá trình suy luận, mạng gating đã học động lực chọn hai chuyên gia tốt nhất cho mỗi token. Đối với một lớp MoE với E chuyên gia, điều này về cơ bản cung cấp một tập hợp O(E²) các kết hợp khác nhau của mạng feed-forward thay vì một trong kiến trúc Transformer cổ điển, dẫn đến tính linh hoạt tính toán nhiều hơn. Biểu diễn học được cuối cùng của một token sẽ là kết hợp có trọng số của các đầu ra từ các chuyên gia đã chọn.

Chúng tôi cũng thực hiện các sửa đổi bổ sung cho kiến trúc Transformer gốc. Chúng tôi thay thế embedding vị trí chuẩn bằng bias vị trí tương đối theo từng lớp từ Dai et al. (2019). Trong các lớp con feed-forward Transformer không phải MoE, chúng tôi thay thế phép chiếu tuyến tính đầu tiên và hàm kích hoạt bằng Gated Linear Unit (Dauphin et al., 2017; Shazeer, 2020), tính toán tích theo thành phần của hai phép biến đổi tuyến tính của đầu vào, theo sau bởi hàm kích hoạt Gaussian Error Linear Unit (Hendrycks & Gimpel, 2016). Chúng tôi phân vùng trọng số và tính toán của các mô hình GLaM lớn sử dụng thuật toán chia sẻ 2D như được mô tả trong Xu et al. (2021), được mô tả chi tiết hơn trong Mục C của phụ lục.

5. Thiết lập Thí nghiệm
GLaM là một họ mô hình ngôn ngữ chỉ giải mã dày đặc và thưa thớt, vì vậy chúng tôi đầu tiên elabora các thiết lập huấn luyện, siêu tham số và giao thức đánh giá trong mục này.

5.1. Thiết lập Huấn luyện
Chúng tôi huấn luyện một số biến thể của GLaM để nghiên cứu hành vi của mô hình MoE và dày đặc trên cùng dữ liệu huấn luyện. Bảng 4 hiển thị các thiết lập siêu tham số của các mô hình GLaM quy mô khác nhau từ 130 triệu tham số đến 1.2 nghìn tỷ tham số. Ở đây, E là số lượng chuyên gia trong lớp MoE, B là kích thước mini-batch, S là độ dài chuỗi đầu vào, M là chiều mô hình và embedding, H là chiều ẩn của mạng feed-forward, L là số lượng lớp và N là tổng số thiết bị. Thêm vào đó, nparams là tổng số tham số mô hình có thể huấn luyện, nact-params là số tham số mô hình được kích hoạt cho mỗi token đầu vào, nheads là số đầu self-attention, và dhead là chiều ẩn của mỗi đầu attention. Chúng tôi cũng bao gồm các mô hình dày đặc tương ứng với số lượng tham số được kích hoạt mỗi token tương đương trong quá trình suy luận (và do đó số lượng FLOPs mỗi token tương tự) làm tham chiếu. Chúng tôi sử dụng ký hiệu GLaM (Kích thước Dense Cơ sở / E) ví dụ: GLaM (8B/64E) để mô tả các biến thể khác nhau trong mô hình GLaM. Ví dụ, GLaM (8B/64E) đại diện cho kiến trúc của một mô hình dày đặc khoảng 8B tham số với mỗi lớp khác được thay thế bởi một lớp MoE 64 chuyên gia. GLaM giảm thành một kiến trúc mô hình ngôn ngữ dày đặc dựa trên Transformer khi mỗi lớp MoE chỉ có một chuyên gia. Chúng tôi sử dụng ký hiệu GLaM (Dense Size) ví dụ: GLaM (137B) để chỉ một mô hình dày đặc 137B tham số được huấn luyện với cùng tập dữ liệu.

5.2. Siêu tham số và Quy trình Huấn luyện
Chúng tôi sử dụng cùng siêu tham số học cho tất cả mô hình GLaM. Cụ thể hơn, chúng tôi sử dụng độ dài chuỗi tối đa là 1024 token, và đóng gói mỗi ví dụ đầu vào để có tối đa 1 triệu token mỗi batch. Tỷ lệ dropout được đặt thành 0 vì số lượng token có sẵn trong corpus huấn luyện lớn hơn nhiều so với số lượng token được xử lý trong quá trình huấn luyện. Optimizer của chúng tôi là Adafactor (Shazeer & Stern, 2018) với first-moment decay β₁ = 0, second-moment decay β₂ = 0.99 với lịch trình suy giảm t^{-0.8}, ngưỡng cắt cập nhật là 1.0, và ước tính second-moment được factored. Chúng tôi giữ learning rate ban đầu là 0.01 cho 10K bước huấn luyện đầu tiên, và sau đó suy giảm nó với lịch trình căn bậc hai nghịch đảo lr_{t} = lr/√t. Trên đỉnh của cross-entropy loss chuẩn, chúng tôi thêm MoE auxiliary loss như được mô tả trong GShard (Lepikhin et al., 2021) với hệ số 0.01 để khuyến khích cân bằng tải chuyên gia sao cho hàm gating sẽ phân phối token đều hơn trên tất cả các chuyên gia. Chúng tôi sử dụng tokenizer từ con SentencePiece (Kudo & Richardson, 2018) với từ vựng có kích thước 256K. Trong quá trình huấn luyện, chúng tôi sử dụng float32 cho trọng số mô hình và bfloat16 cho activations. Mô hình GLaM 64B/64E lớn nhất được huấn luyện trên 1,024 chip Cloud TPU-V4.

Huấn luyện mô hình ở quy mô nghìn tỷ tham số cực kỳ đắt đỏ ngay cả đối với các mô hình được kích hoạt thưa thớt. Có rất ít không gian cho việc điều chỉnh siêu tham số. Ở đây chúng tôi chia sẻ các công thức huấn luyện và một số thủ thuật triển khai cho các mô hình GLaM.

Bảng 4. Kích thước và kiến trúc của cả mô hình MoE và dày đặc mà chúng tôi đã huấn luyện trong các thí nghiệm. Mô hình được nhóm theo số lượng tham số được kích hoạt cho mỗi token. Tất cả các mô hình được huấn luyện đều chia sẻ cùng siêu tham số học được mô tả trong Phiên 5.1.

GLaM Loại Mô hình nparams nact-params L M H n heads dhead E
0.1B Dense 130M 130M 12 768 3,072 12 64 –
0.1B/64E MoE 1.9B 145M 64
1.7B Dense 1.7B 1.700B 24 2,048 8,192 16 128 –
1.7B/32E MoE 20B 1.878B 32
1.7B/64E MoE 27B 1.879B 64
1.7B/128E MoE 53B 1.881B 128
1.7B/256E MoE 105B 1.886B 256
8B Dense 8.7B 8.7B 32 4,096 16,384 32 128 –
8B/64E MoE 143B 9.8B 64
137B Dense 137B 137B 64 8,192 65,536 128 128 –
64B/64E MoE 1.2T 96.6B 64 8,192 32,768 128 128 64

• Chúng tôi huấn luyện các mô hình quy mô nhỏ hơn đến hội tụ trước. Điều này cho phép chúng tôi phơi bày các vấn đề tiềm ẩn trong tập dữ liệu và cơ sở hạ tầng sớm nhất có thể.

• Chúng tôi bỏ qua cập nhật trọng số cho một batch nếu có bất kỳ NaN hoặc Inf nào trong gradient (Shen et al., 2019). Lưu ý NaN/Inf vẫn có thể xảy ra trong quá trình áp dụng bước gradient, trong trường hợp đó chúng tôi khởi động lại từ một checkpoint trước đó như được mô tả dưới đây. Ví dụ, ngay cả khi không có Inf trong biến hiện tại hoặc gradient, biến được cập nhật vẫn có thể dẫn đến Inf.

• Chúng tôi khởi động lại từ một checkpoint sớm khỏe mạnh khi gặp phải các biến động lớn hiếm gặp hoặc thậm chí NaN/Inf trong quá trình huấn luyện. Tính ngẫu nhiên của các batch được tải tuần tự có thể giúp thoát khỏi trạng thái thất bại trước đó trong quá trình huấn luyện sau khi khởi động lại.

5.3. Thiết lập Đánh giá
Giao thức. Để chứng minh rõ ràng hiệu quả của các mô hình GLaM, chúng tôi chủ yếu tập trung vào đánh giá các giao thức học zero, one và few-shot được đề xuất bởi Radford et al. (2018); Brown et al. (2020). Đối với thiết lập học zero-shot, trong hầu hết các trường hợp, chúng tôi đánh giá mỗi ví dụ trong tập development trực tiếp. Đối với học one/few-shot, chúng tôi chủ yếu rút ngẫu nhiên một/vài ví dụ từ tập huấn luyện của tác vụ đó làm minh chứng và ngữ cảnh duy nhất. Minh chứng như vậy được nối với ví dụ đánh giá bằng hai dòng mới ở giữa, và sau đó được đưa vào mô hình.

Bảng đánh giá. Để cho phép so sánh trái táo với trái táo giữa GPT-3 và GLaM, chúng tôi chọn cùng bộ tác vụ đánh giá như Brown et al. (2020). Nhưng để đơn giản, chúng tôi loại trừ 7 tác vụ tổng hợp (số học và giải mã từ) và 6 tập dữ liệu dịch máy. Với việc loại trừ này, chúng tôi kết thúc với 29 tập dữ liệu, bao gồm 8 tác vụ tạo ngôn ngữ tự nhiên (NLG) và 21 tác vụ hiểu ngôn ngữ tự nhiên (NLU). Các tập dữ liệu này có thể được nhóm thêm thành 7 danh mục và được liệt kê trong mục A.

Các tác vụ Tạo Ngôn ngữ Tự nhiên. Chúng tôi so sánh các chuỗi ngôn ngữ được giải mã bởi các mô hình với ground truth trong các tác vụ generative. Các tác vụ này là TriviaQA, NQS, WebQS, SQuADv2, LAMBADA, DROP, QuAC và CoQA. Hiệu suất được đo bằng độ chính xác của exact match (EM) và điểm F1, theo tiêu chuẩn cho mỗi tác vụ trong Brown et al. (2020). Chúng tôi sử dụng tìm kiếm beam với độ rộng 4 để tạo ra các chuỗi.

Các tác vụ Hiểu Ngôn ngữ Tự nhiên. Hầu hết các tác vụ hiểu ngôn ngữ yêu cầu mô hình chọn một câu trả lời đúng từ nhiều lựa chọn. Tất cả các tác vụ phân loại nhị phân được công thức hóa thành dạng chọn giữa hai lựa chọn ('Có' hoặc 'Không'). Dự đoán được dựa trên log-likelihood tối đa của mỗi lựa chọn cho trước ngữ cảnh log P(lựa chọn|ngữ cảnh) được chuẩn hóa theo độ dài token của mỗi lựa chọn. Trên một số tác vụ, như ReCoRD (Zhang et al., 2018) và COPA (Gordon et al., 2012), loss không được chuẩn hóa có thể mang lại kết quả tốt hơn và do đó được sử dụng. Ngoại trừ MultiRC (Khashabi et al., 2018) nơi metric F1 trên tập hợp các lựa chọn trả lời (được gọi là F1a) được báo cáo, metric độ chính xác dự đoán được sử dụng cho tất cả các tác vụ khác. Chúng tôi sử dụng trung bình của các điểm được báo cáo trong tất cả các tập dữ liệu để báo cáo hiệu suất few-shot tổng thể của mô hình trên cả tác vụ NLG và NLU. Cả điểm Accuracy (EM) và F1 đều được chuẩn hóa nằm giữa 0 và 100. Trên TriviaQA, chúng tôi cũng báo cáo điểm số testing server của submission one-shot của chúng tôi.

6. Kết quả
Chúng tôi tiến hành đánh giá rộng rãi trên toàn bộ họ mô hình GLaM, để cho thấy lợi thế của các mô hình được kích hoạt thưa thớt trong mô hình hóa ngôn ngữ và xu hướng mở rộng của chúng. Chúng tôi cũng kiểm tra định lượng hiệu quả của chất lượng dữ liệu cho việc huấn luyện mô hình ngôn ngữ.

6.1. So sánh giữa Mô hình MoE và Dense
Như đã trình bày trước đó trong Bảng 1, GLaM (64B/64E) có hiệu suất cạnh tranh so với GPT-3 (175B) cho học zero, one và few-shot. Hình 1 so sánh hiệu suất cho mỗi danh mục tác vụ. Tổng cộng, GLaM (64B/64E) vượt trội hơn GPT-3 trong 6 trong 7 danh mục trung bình, cho thấy mức tăng hiệu suất là nhất quán. Để biết thêm chi tiết về từng tác vụ cá nhân, xem Bảng 11. Chúng tôi bao gồm kết quả trên Megatron-NLG và Gopher lớn hơn nhiều và đòi hỏi tính toán đòi hỏi để tham khảo. Quan trọng hơn, như được hiển thị trong Bảng 4, GLaM (64B/64E) kích hoạt khoảng 96.6B tham số mỗi token trong quá trình suy luận, chỉ yêu cầu một nửa số FLOPs tính toán cần thiết bởi GPT-3 với cùng đầu vào.

Chúng tôi nhấn mạnh một tác vụ trả lời câu hỏi miền mở đầy thách thức đặc biệt: TriviaQA. Trong các tác vụ trả lời câu hỏi miền mở, mô hình được yêu cầu trả lời trực tiếp một truy vấn cho trước mà không cần truy cập vào bất kỳ ngữ cảnh bổ sung nào. Brown et al. (2020) cho thấy rằng hiệu suất few-shot của TriviaQA có thể tăng trưởng mượt mà theo kích thước mô hình, cho thấy một mô hình ngôn ngữ có thể hấp thụ kiến thức sử dụng khả năng mô hình của nó. Như được hiển thị trong Bảng 5, GLaM (64B/64E) tốt hơn mô hình dày đặc và vượt trội hơn state-of-the-art (SOTA) được tinh chỉnh trước đó trên tập dữ liệu này trong thiết lập miền mở. Kết quả one-shot của chúng tôi vượt qua SOTA được tinh chỉnh trước đó (Yu et al., 2022) nơi thông tin knowledge graph bổ sung được truyền vào 8.6%, và vượt trội hơn GPT-3 few-shot trên testing server 5.3%. Điều này gợi ý rằng khả năng bổ sung của GLaM đóng vai trò quan trọng trong mức tăng hiệu suất mặc dù nact-params của GLaM (64B/64E) chỉ bằng một nửa so với GPT-3. So sánh với Switch-C, mặc dù cả hai mô hình đều có số lượng tham số tổng tương tự, GLaM (64B/64E) sử dụng các chuyên gia lớn hơn nhiều (vượt quá một TPU core) so với Switch-C. Do đó, hiệu suất one-shot của GLaM trên TriviaQA cũng tốt hơn kết quả được tinh chỉnh của Switch-C trong thiết lập miền mở. Cuối cùng, chúng tôi báo cáo đánh giá zero, one và few-shot chủ yếu trên tập development cho tất cả các tác vụ trong Bảng 11, 12, 13 và 14 của phụ lục.

Bảng 5. Hiệu suất one-shot của GLaM (64B/64E) vượt trội đáng kể so với các SOTA trước đó cho thiết lập miền mở trong wiki split.

Mô hình TriviaQA (Miền Mở)
KG-FiD (large) (Yu et al., 2022) (đã tinh chỉnh, test) 69.8
Switch-C (đã tinh chỉnh, dev) 47.5
GPT-3 One-shot (dev) 68.0
GPT-3 64-shot (test) 71.2
GLaM One-shot (test) 75.0
GLaM One-shot (dev) 75.8

6.2. Hiệu ứng của Chất lượng Dữ liệu
Chúng tôi nghiên cứu tác động của chất lượng dữ liệu lên hiệu suất few-shot của các tác vụ downstream. Chúng tôi sử dụng một mô hình GLaM kích thước khiêm tốn (1.7B/64E) để cho thấy hiệu quả của việc lọc văn bản về chất lượng mô hình. Chúng tôi huấn luyện mô hình với cùng siêu tham số trên hai tập dữ liệu. Một là tập dữ liệu gốc được mô tả trong Mục 3 và thứ hai bao gồm tập dữ liệu với các trang web được lọc được thay thế bằng các trang web chưa được lọc. Tỷ lệ hỗn hợp được cố định như trong Bảng 3. Các trang web được lọc bao gồm 143B token trong khi các trang web chưa được lọc bao gồm khoảng 7T token.

Hình 3 (c) và (d) cho thấy rằng mô hình được huấn luyện trên dữ liệu được lọc hoạt động tốt hơn một cách nhất quán trên cả tác vụ NLG và NLU. Đặc biệt, hiệu ứng của việc lọc lớn hơn trên NLG so với NLU. Có lẽ điều này là do NLG thường yêu cầu tạo ra ngôn ngữ chất lượng cao và corpus tiền huấn luyện được lọc là quan trọng đối với khả năng tạo ra của mô hình ngôn ngữ. Nghiên cứu của chúng tôi nhấn mạnh thực tế rằng chất lượng của dữ liệu tiền huấn luyện cũng đóng vai trò quan trọng, cụ thể là trong hiệu suất của các tác vụ downstream.

6.3. Nghiên cứu Mở rộng
Mở rộng các mô hình ngôn ngữ dày đặc thường liên quan đến việc làm cho các mô hình sâu hơn bằng cách thêm nhiều lớp hơn, và rộng hơn bằng cách tăng chiều embedding của biểu diễn token. Quá trình này tăng tổng số tham số nparams của mô hình. Đối với mỗi dự đoán trên một ví dụ đầu vào cho trước, các mô hình này là 'dày đặc' ở chỗ tất cả nparams tham số sẽ được kích hoạt, tức là nparams = nact-params trong Bảng 4. Do đó, FLOPs hiệu quả mỗi dự đoán tăng tuyến tính với kích thước mô hình nparams. Trong khi FLOPs tăng có thể dẫn đến hiệu suất dự đoán được tăng cường, nó cũng làm tăng chi phí tổng thể mỗi dự đoán.

Ngược lại, các mô hình GLaM MoE được kích hoạt thưa thớt ở chỗ chỉ một phần nhỏ của tổng nparams tham số sẽ được kích hoạt cho mỗi dự đoán nơi nparams ≫ nact-params. Do đó, các mô hình GLaM MoE có thể mở rộng bằng cách cũng tăng kích thước hoặc số lượng chuyên gia trong lớp MoE.

Như được hiển thị trong Hình 3(a), hiệu suất zero, one và few-shot trung bình trên các tác vụ generative mở rộng tốt với FLOPs hiệu quả mỗi dự đoán mà lần lượt được xác định bởi nact-params. Chúng tôi cũng thấy rằng các mô hình GLaM MoE hoạt động tốt hơn một cách nhất quán so với các mô hình GLaM dày đặc cho FLOPs hiệu quả tương tự mỗi token. Đối với các tác vụ hiểu ngôn ngữ được hiển thị trong Hình 3(b), mức tăng hiệu suất của các mô hình GLaM MoE có xu hướng mở rộng tương tự như các tác vụ generative. Chúng tôi quan sát thấy rằng cả mô hình MoE và dày đặc đều hoạt động tương tự ở quy mô nhỏ hơn nhưng các mô hình MoE vượt trội ở quy mô lớn hơn. Chúng tôi cũng hiển thị các thí nghiệm với việc mở rộng số lượng chuyên gia trong Mục B nơi chúng tôi quan sát thấy rằng, đối với ngân sách tính toán cố định mỗi dự đoán, thêm nhiều chuyên gia hơn thường dẫn đến hiệu suất dự đoán tốt hơn.

6.4. Hiệu quả của GLaM
Các mô hình ngôn ngữ dày đặc lớn hiện có thường yêu cầu một lượng tài nguyên tính toán khổng lồ để huấn luyện và phục vụ (Patterson et al., 2021). Chúng cũng cần tiêu thụ một lượng dữ liệu tiền huấn luyện khổng lồ. Chúng tôi điều tra hiệu quả dữ liệu và tính toán của các mô hình GLaM được đề xuất.

Hiệu quả Dữ liệu. Hình 4 (a-c) và Hình 4(e-g) cho thấy đường cong học tập của các mô hình của chúng tôi so với các baseline dày đặc có FLOPs hiệu quả tương tự trong cả tác vụ NLG và NLU. Trục x là số lượng token được sử dụng trong huấn luyện nơi chúng tôi bao gồm rõ ràng kết quả của GPT-3 khi nó khoảng 300B token. Chúng tôi đầu tiên quan sát thấy rằng các mô hình GLaM MoE yêu cầu ít dữ liệu hơn đáng kể so với các mô hình dày đặc có FLOPs tương đương để đạt được hiệu suất zero, one và few-shot tương tự. Nói cách khác, khi cùng một lượng dữ liệu được sử dụng để huấn luyện, các mô hình MoE hoạt động tốt hơn nhiều, và sự khác biệt trong hiệu suất trở nên lớn hơn khi huấn luyện đến 630B. Hơn nữa, mô hình GLaM (64B/64E) được huấn luyện với 280B token vượt trội hơn GPT-3 được huấn luyện với 300B token với biên độ lớn trên 4 trong 6 thiết lập học tập (zero-shot/one-shot NLU và one-shot/few-shot NLG), và khớp với điểm số GPT-3 cho thiết lập còn lại, tức là các tác vụ NLG zero-shot.

Hiệu quả Tính toán & Tiêu thụ Năng lượng. Hình 4 (d) và Hình 4 (h) cho thấy hiệu suất zero, one và few-shot trung bình mở rộng như thế nào với số năm TPU được dành để huấn luyện các mô hình MoE và dày đặc. Chúng tôi thấy rằng để đạt được hiệu suất tương tự trên các tác vụ downstream, huấn luyện các mô hình được kích hoạt thưa thớt mất ít tài nguyên tính toán hơn nhiều so với huấn luyện các mô hình dày đặc.

Như đã trình bày trước đó trong Bảng 1, việc huấn luyện GLaM (64B/64E) sau 600B token tiêu thụ 456 MWh, khoảng 1/3 chi phí năng lượng 1287 MWh được sử dụng bởi GPT-3. Hơn nữa, để đạt điểm số tương tự (và hơi vượt) so với GPT-3, chúng tôi huấn luyện sử dụng 1,024 chip TPU-v4 trong 574 giờ (với 280B token). Điều này tiêu thụ 213 MWh hoặc 1/6 chi phí năng lượng GPT-3. Việc giảm tiêu thụ năng lượng của GLaM là do kiến trúc MoE và tối ưu hóa hiệu quả tính toán từ phần cứng TPU-v4 và phần mềm GSPMD. Tính toán năng lượng có thể được tìm thấy trong Mục F.

7. Đạo đức và Thiên kiến Không Mong muốn
Khả năng suy luận zero-shot và few-shot của các mô hình ngôn ngữ lớn là một khả năng thú vị: có thể kiểm soát hành vi mô hình một cách trực quan bằng ngôn ngữ tự nhiên và tập dữ liệu nhỏ làm giảm đáng kể rào cản để tạo mẫu và phát triển các ứng dụng mới; nó có tiềm năng giúp dân chủ hóa việc sử dụng AI bằng cách giảm đáng kể nhu cầu về kiến thức chuyên môn. Tuy nhiên, những cơ hội như vậy cũng phục vụ để làm nổi bật tầm quan trọng của nhiều thách thức đạo đức (Leidner & Plachouras, 2017; Bender et al., 2021; Bommasani et al., 2021) bao gồm thiên kiến đại diện (Blodgett et al., 2020), lựa chọn và xử lý dữ liệu huấn luyện đúng cách (Rogers, 2021) và tài liệu hóa của nó (Bender & Friedman, 2018), quyền riêng tư (Abadi et al., 2016b; Carlini et al., 2020), và mối quan tâm về môi trường (Strubell et al., 2019; Patterson et al., 2021). Một chuỗi quan trọng của nghiên cứu này tập trung vào các thiên kiến không mong muốn được học bởi các mô hình ngôn ngữ, bao gồm tương quan giữa giới tính và nghề nghiệp (Bolukbasi et al., 2016; Rudinger et al., 2018; Zhao et al., 2018), tình cảm tiêu cực về các nhóm chủng tộc và tôn giáo (Li et al., 2020; Nadeem et al., 2021), và về người khuyết tật (Hutchinson et al., 2020), cũng như các thiên kiến xã hội khác (Caliskan et al., 2017; Rudinger et al., 2017; Sap et al., 2020; Sotnikova et al., 2021). Trong khi việc đo lường và giảm thiểu tác hại tiềm tàng của các mô hình ngôn ngữ là một lĩnh vực nghiên cứu rất tích cực, như được công nhận bởi Blodgett et al. (2021); Jacobs & Wallach (2021) vẫn còn một nhu cầu đáng kể về các phương pháp đánh giá nghiêm ngặt hơn để đánh giá mức độ mà các mô hình ngôn ngữ mã hóa các khuôn mẫu có hại (May et al., 2019; Webster et al., 2021).

Mặc dù vẫn chưa có sự đồng thuận về phương pháp đo lường hoặc tiêu chí cho các mô hình ngôn ngữ lớn mục đích chung như vậy, tính linh hoạt và sức mạnh của các mô hình này làm cho việc đánh giá chúng trên một loạt các metric trở nên quan trọng. Chúng tôi lấy cảm hứng từ GPT-3 (Brown et al., 2020) và kiểm tra sự xuất hiện cùng nhau trong văn bản được tạo ra tham chiếu các thuật ngữ danh tính cũng như báo cáo về bảng đánh giá WinoGender (Rudinger et al., 2018). Chúng tôi cũng phân tích suy thoái độc tính tương tự như Gopher (Rae et al., 2021), và mở rộng phân tích để xem xét baseline hành vi con người.

7.1. Prompts Xuất hiện cùng nhau
Theo thủ tục được mô tả trong Brown et al. (2020), chúng tôi phân tích các từ xuất hiện cùng nhau phổ biến trong các continuation khi được đưa ra prompts như "{term} was very..." nơi thuật ngữ được thay thế tham chiếu đến giới tính, tôn giáo, danh tính chủng tộc và dân tộc. Đối với mỗi prompt (Bảng 7 của phụ lục), 800 đầu ra được tạo ra sử dụng top-k sampling (k = 40) với nhiệt độ 1. Một POS tagger có sẵn (Bird & Loper, 2004) được sử dụng để loại bỏ các từ dừng và chọn chỉ các từ mô tả (tức là tính từ và trạng từ). Trạng từ được bao gồm vì chúng tôi nhận thấy một mẫu lỗi phổ biến nơi tính từ bị phân loại sai thành trạng từ; ví dụ "pretty" trong cụm từ "She was very pretty and very accomplished". Giống như Brown et al. (2020), để làm cho phân tích minh bạch và dễ dàng tái tạo, chúng tôi bỏ qua bất kỳ gán nhãn thủ công nào của con người.

Giống như phân tích của các mô hình ngôn ngữ lớn khác mà chúng tôi xây dựng, chúng tôi lưu ý các thiên kiến liên kết cho tất cả các chiều là rõ ràng, ví dụ "pretty" là mô tả được liên kết nhiều nhất cho thuật ngữ "She", trong khi nó không có trong top-10 cho thuật ngữ "He". Bảng 8 hiển thị các từ mô tả xuất hiện thường xuyên nhất trong phản hồi với prompt-templates cho đại từ theo giới tính, và Bảng 9 và 10 của phụ lục hiển thị tương tự cho prompts chủng tộc và tôn giáo.

7.2. WinoGender
Giải quyết coreference là một khả năng mà nhiều ứng dụng yêu cầu để hoạt động tốt, bao gồm dịch máy (Stanovsky et al., 2019; Webster & Pitler, 2020) và trả lời câu hỏi (Lamm et al., 2020). Để đánh giá liệu các tương quan theo giới tính trong GLaM có khiến nó mắc lỗi coreference trong thiết lập one-shot hay không, chúng tôi đo lường WinoGender (Rudinger et al., 2018). GLaM (64B/64E) đạt được state-of-the-art mới là 71.7% trên toàn bộ tập dữ liệu (so với 64.2% cho GPT-3 (Brown et al., 2020)). Đầy hứa hẹn, độ chính xác gần nhau đáng kể giữa các ví dụ 'he' (70.8%) và các ví dụ 'she' (72.5%), cũng như giữa các ví dụ định kiến (nơi phân phối dự kiến được giả định gần với thống kê nghề nghiệp Mỹ, (Rudinger et al., 2018)) và các ví dụ phản định kiến (hoặc 'gotcha') (cả hai đều 71.7%).

7.3. Suy thoái Độc tính
Suy thoái độc tính là khi một mô hình ngôn ngữ tạo ra văn bản vô tình độc hại. Để đánh giá suy thoái độc tính, chúng tôi điều chỉnh phương pháp được sử dụng trong (Welbl et al., 2021; Rae et al., 2021). Chúng tôi sử dụng tập dữ liệu RealToxicityPrompts (Gehman et al., 2020) bao gồm các câu đã được chia thành hai phần: một tiền tố prompt, và một hậu tố continuation. Giống như các nghiên cứu trước đó, chúng tôi cũng sử dụng Perspective API gán xác suất rằng văn bản sẽ được coi là thô lỗ, thiếu tôn trọng hoặc có khả năng làm mọi người muốn rời khỏi cuộc trò chuyện. Sau đó chúng tôi đánh giá khả năng một continuation độc hại cho trước các khả năng khác nhau rằng prompt độc hại.

Đối với mỗi trong 10K prompts được lấy mẫu ngẫu nhiên, chúng tôi tạo ra 25 continuations, với tối đa 100 token mỗi continuation sử dụng top-k sampling (k = 40) với nhiệt độ 1. Perspective API yêu cầu một chuỗi không rỗng do đó chúng tôi gán điểm độc tính 0.0 khi continuation là chuỗi rỗng; điều này có thể đại diện cho, ví dụ, một chat bot đơn giản từ chối phản hồi.

Hình 5 cho thấy mối quan hệ giữa Xác suất Độc tính của Prompt (TPP), và Xác suất Độc tính của Continuation (TPC). Lưu ý rằng, đối với TPP thấp, TPC con người tương đối cao là do chiến lược lấy mẫu được sử dụng để tạo ra tập dữ liệu cơ sở: các câu được chọn trên toàn bộ phổ độc tính. Hơn nữa, độc tính thường có thể được xác định cục bộ trong một câu, và độc tính trong tập dữ liệu này có xu hướng xảy ra sau trong các câu. Điều này khiến TPC con người giảm nhẹ khi TPP tăng. Ngược lại, đáng chú ý rằng TPC của mô hình theo sát TPP, phản ánh quan sát thường gặp rằng các mô hình ngôn ngữ lớn đôi khi bị ảnh hưởng quá mạnh bởi prompt của chúng, ví dụ lặp lại các cụm từ từ prompt.

Hình 5. Mối quan hệ giữa Xác suất Độc tính của Prompt (TPP), và Xác suất Độc tính của Continuation (TPC). Human đề cập đến continuation của câu gốc được viết bởi con người.

Chúng tôi cũng phân tích phân phối xác suất độc tính từ API cho các batch 25 continuations. Điều này nhấn mạnh rằng, ngay cả đối với prompts độc tính thấp, rất có khả năng một số continuation được tạo ra sẽ được đánh giá là độc hại bởi hầu hết mọi người xem xét nó, theo xác suất dự đoán của Perspective API; chi tiết thêm có thể được tìm thấy trong Hình 8. Chúng tôi cũng lưu ý rằng chiến lược lấy mẫu của tập dữ liệu này, và nguồn nó được lấy từ (Reddit) có thể không phản ánh các miền khác. Hơn nữa, ngay cả đối với TPP rất thấp, các ứng dụng có thể muốn TPC thấp hơn nhiều: ngay cả việc tạo ra 1 trong 100 gợi ý độc hại có thể rất có vấn đề đối với các ứng dụng.

8. Thảo luận
Như được quan sát trong công trình trước đó về các mô hình được kích hoạt thưa thớt (Fedus et al., 2021), các mô hình MoE hiệu quả hơn trong các tác vụ hướng tri thức. Các tác vụ miền mở là một cách để đo lường lượng kiến thức được lưu trữ trong một mô hình. Hiệu suất của mô hình MoE trong các bảng đánh giá QA miền mở như TriviaQA chứng minh khả năng thông tin tăng đáng kể của các mô hình này so với các mô hình dày đặc có FLOPs hiệu quả tương tự. Mặc dù có lợi thế về hiệu quả học trong ngữ cảnh và huấn luyện, các mô hình được kích hoạt thưa thớt bao gồm số lượng tham số cao hơn và do đó yêu cầu số lượng thiết bị lớn hơn. Điều này giới hạn khả năng truy cập tài nguyên và tăng chi phí phục vụ đặc biệt khi lưu lượng phục vụ thấp.

9. Kết luận
Chúng tôi đề xuất và phát triển một họ mô hình ngôn ngữ tổng quát có tên GLaM, sử dụng kiến trúc mixture-of-experts được kích hoạt thưa thớt để đạt được điểm số trung bình tốt hơn không chỉ so với các đối tác dày đặc có FLOPs hiệu quả tương tự, mà còn so với các mô hình GPT-3 trên 29 tác vụ NLP đại diện trong học zero, one và few-shot. Đặc biệt, GLaM (64B/64E), mô hình ngôn ngữ MoE 1.2 nghìn tỷ tham số lớn nhất của chúng tôi, đạt được hiệu suất trung bình tốt hơn chỉ với một phần ba mức tiêu thụ năng lượng so với việc huấn luyện GPT-3. Chúng tôi hy vọng rằng công trình của chúng tôi sẽ khuyến khích nhiều nghiên cứu hơn về các phương pháp để có được dữ liệu chất lượng cao, và sử dụng MoE để mở rộng hiệu quả hơn các mô hình ngôn ngữ khổng lồ.

--- TRANG 10 ---
GLaM: Mở rộng Hiệu quả các Mô hình Ngôn ngữ với Mixture-of-Experts

[Tiếp tục với phần References và các phần phụ lục...]

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ với tất cả các trích dẫn từ bản gốc]

A. Bảng đánh giá
Trả lời Câu hỏi Miền Mở: TriviaQA (Joshi et al., 2017), Natural Questions (NQS) (Kwiatkowski et al., 2019), Web Questions (WebQS) (Berant et al., 2013)

Các Tác vụ Cloze và Hoàn thành: LAMBADA (Paperno et al., 2016), HellaSwag (Zellers et al., 2019), StoryCloze (Mostafazadeh et al., 2016)

Các Tác vụ Kiểu Winograd: Winograd (Levesque et al., 2012), WinoGrande (Sakaguchi et al., 2020)

Suy luận Thông thường: PIQA (Bisk et al., 2020), ARC (Easy) (Clark et al., 2018), ARC (Challenge) (Clark et al., 2018), OpenBookQA (Mihaylov et al., 2018)

Đọc hiểu Trong ngữ cảnh: DROP (Dua et al., 2019), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), SQuADv2 (Rajpurkar et al., 2018), RACE-h (Lai et al., 2017), RACE-m (Lai et al., 2017)

SuperGLUE: (Wang et al., 2019) BoolQ (Clark et al., 2019), CB (de Marneffe et al., 2019), COPA (Gordon et al., 2012), RTE (Dagan et al., 2006), WiC (Pilehvar & Camacho-Collados, 2018), WSC (Levesque et al., 2012), MultiRC (Khashabi et al., 2018), ReCoRD (Zhang et al., 2018)

Suy luận Ngôn ngữ Tự nhiên: ANLI R1, ANLI R2, ANLI R3 (Fyodorov et al., 2000)

B. Mở rộng Số lượng Chuyên gia
Chúng tôi cũng nghiên cứu hiệu ứng của việc tăng số lượng chuyên gia trên mỗi lớp MoE. Cụ thể hơn, chúng tôi bắt đầu với một mô hình kích thước khiêm tốn 1.7B, về cơ bản là một mô hình GLaM (1.7B/1E) nơi mỗi lớp MoE giảm xuống chỉ bao gồm một mạng feed-forward duy nhất như chuyên gia. Sau đó chúng tôi tăng số lượng chuyên gia trong mỗi lớp MoE từ 1 đến 256. Mặc dù số lượng chuyên gia tăng theo cấp số nhân, nact-params trong mỗi mô hình hầu như không tăng do tính thưa thớt của GLaM. Thực tế, như được hiển thị trong Bảng 4, tất cả chúng đều có FLOPs mỗi dự đoán gần như giống hệt nhau.

[Tiếp tục với tất cả các phần còn lại của tài liệu bao gồm các bảng, hình và phụ lục chi tiết...]
