# 2204.09179.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2204.09179.pdf
# Kích thước tệp: 922529 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Về Sụp Đổ Biểu Diễn Của
Hỗn Hợp Thưa Thớt Các Chuyên Gia
Zewen Chiy, Li Dongz, Shaohan Huangz, Damai Daik, Shuming Maz, Barun Patraz
Saksham Singhalz, Payal Bajajz, Xia Songz, Xian-Ling Maoy, Heyan Huangy, Furu Weiz
yViện Công Nghệ Bắc Kinh
zMicrosoft Corporation
kĐại Học Bắc Kinh
https://aka.ms/msragi
Tóm Tắt
Hỗn hợp thưa thớt các chuyên gia cung cấp dung lượng mô hình lớn hơn trong khi chỉ yêu cầu chi phí tính toán không đổi. Nó sử dụng cơ chế định tuyến để phân phối các token đầu vào tới các chuyên gia phù hợp nhất theo biểu diễn ẩn của chúng. Tuy nhiên, việc học cơ chế định tuyến như vậy khuyến khích việc gom cụm token xung quanh các tâm chuyên gia, hàm ý xu hướng sụp đổ biểu diễn. Trong công trình này, chúng tôi đề xuất ước tính điểm định tuyến giữa các token và chuyên gia trên một siêu cầu chiều thấp. Chúng tôi thực hiện các thí nghiệm rộng rãi về tiền huấn luyện mô hình ngôn ngữ đa ngôn ngữ và tinh chỉnh trên các tác vụ hạ lưu. Kết quả thí nghiệm trên bảy tiêu chuẩn đa ngôn ngữ cho thấy phương pháp của chúng tôi đạt được cải thiện nhất quán. Chúng tôi cũng trình bày phân tích toàn diện về hành vi biểu diễn và định tuyến của các mô hình. Phương pháp của chúng tôi giảm thiểu vấn đề sụp đổ biểu diễn và đạt định tuyến nhất quán hơn so với các phương pháp hỗn hợp chuyên gia cơ sở.

1 Giới Thiệu
Mở rộng dung lượng mô hình đã được chứng minh là cách tiếp cận đầy hứa hẹn để đạt hiệu suất tốt hơn trên nhiều vấn đề như tiền huấn luyện mô hình ngôn ngữ (Radford et al., 2019; Raffel et al., 2020), và học biểu diễn thị giác (Dosovitskiy et al., 2021; Bao et al., 2022). Mặc dù có hiệu quả, việc tăng số lượng tham số dẫn đến chi phí tính toán lớn hơn, điều này thúc đẩy các nghiên cứu gần đây khám phá các mô hình Hỗn Hợp Thưa Thớt Các Chuyên Gia (SMoE) (Shazeer et al., 2017; Fedus et al., 2021; Lepikhin et al., 2021). SMoE tăng dung lượng mô hình bằng cách xây dựng một số mạng nơ-ron được kích hoạt thưa thớt. Với chi phí tính toán gần như không đổi, các mô hình SMoE đạt hiệu suất tốt hơn so với các mô hình dày đặc trên nhiều tác vụ khác nhau, bao gồm dịch máy (Lepikhin et al., 2021), phân loại hình ảnh (Riquelme et al., 2021), và nhận dạng giọng nói (Kumatani et al., 2021).

Cơ chế định tuyến đóng vai trò quan trọng trong các mô hình SMoE. Cho một token đầu vào, bộ định tuyến đo độ tương tự giữa mỗi token và các chuyên gia. Sau đó chúng ta phân phối token tới các chuyên gia phù hợp nhất theo điểm định tuyến. Các nghiên cứu gần đây đã khám phá các thuật toán gán token khác nhau để cải thiện việc huấn luyện SMoE. Ví dụ, Lewis et al. (2021) công thức hóa định tuyến SMoE như một bài toán gán tuyến tính tối đa hóa độ tương tự token-chuyên gia toàn cục. Zhou et al. (2022) để các chuyên gia chọn top token thay vì gán token tới top chuyên gia. Roller et al. (2021) và Dai et al. (2022) đề xuất giữ lựa chọn định tuyến nhất quán. Nhiều nghiên cứu trong những năm gần đây tập trung vào cách thiết kế thuật toán gán token-chuyên gia. Trong bài báo này, chúng tôi trình bày rằng các cơ chế định tuyến hiện tại có xu hướng đẩy các biểu diễn ẩn gom cụm xung quanh các tâm chuyên gia, hàm ý xu hướng sụp đổ biểu diễn, điều này lại làm hại hiệu suất mô hình.

Zewen Chi và Damai Dai đóng góp trong thời gian thực tập tại Microsoft Research.
Hội nghị lần thứ 36 về Hệ thống Xử lý Thông tin Nơ-ron (NeurIPS 2022).arXiv:2204.09179v3  [cs.CL]  12 Oct 2022

--- TRANG 2 ---
Để giảm thiểu vấn đề sụp đổ biểu diễn, chúng tôi giới thiệu một thuật toán định tuyến đơn giản nhưng hiệu quả cho các mô hình hỗn hợp thưa thớt các chuyên gia. Cụ thể hơn, thay vì sử dụng trực tiếp các vector ẩn cho định tuyến, chúng tôi chiếu các vector ẩn vào một không gian chiều thấp hơn. Sau đó, chúng tôi áp dụng chuẩn hóa L2 cho cả biểu diễn token và nhúng chuyên gia, tức là đo điểm định tuyến trên một siêu cầu chiều thấp. Bên cạnh đó, chúng tôi đề xuất một cổng chuyên gia mềm với nhiệt độ có thể học, để học điều khiển việc kích hoạt các chuyên gia.

Chúng tôi đánh giá phương pháp đề xuất trên tiền huấn luyện mô hình ngôn ngữ đa ngôn ngữ và tinh chỉnh trên các tác vụ hạ lưu. Kết quả thí nghiệm cho thấy mô hình của chúng tôi liên tục vượt trội so với các mô hình SMoE cơ sở về cả hiệu suất mô hình ngôn ngữ và tinh chỉnh. Hơn nữa, phân tích chỉ ra rằng phương pháp của chúng tôi giảm thiểu vấn đề sụp đổ biểu diễn so với SMoE cơ sở. Phương pháp của chúng tôi cũng đạt được hành vi định tuyến nhất quán hơn trong cả tiền huấn luyện và tinh chỉnh, điều này xác nhận hiệu quả của thuật toán định tuyến đề xuất.

Đóng góp của chúng tôi được tóm tắt như sau:
• Chúng tôi chỉ ra vấn đề sụp đổ biểu diễn trong các mô hình hỗn hợp thưa thớt các chuyên gia, vấn đề này chưa được khám phá kỹ trong các công trình trước.
• Chúng tôi đề xuất ước tính điểm định tuyến giữa các token và chuyên gia trên một siêu cầu chiều thấp để giảm thiểu sụp đổ biểu diễn.
• Chúng tôi thực hiện các thí nghiệm rộng rãi về tiền huấn luyện mô hình ngôn ngữ đa ngôn ngữ và tinh chỉnh trên các tác vụ hạ lưu.
• Chúng tôi trình bày phân tích chi tiết về hành vi định tuyến và tính chất biểu diễn, cho thấy phương pháp của chúng tôi cải thiện hiệu suất và đạt định tuyến nhất quán hơn.

2 Kiến Thức Nền Tảng
2.1 Hỗn Hợp Thưa Thớt Các Chuyên Gia
Các mô hình Hỗn Hợp Thưa Thớt Các Chuyên Gia (SMoE) tận dụng tính toán có điều kiện, và đã được chứng minh là cách tiếp cận đầy hứa hẹn để mở rộng số lượng tham số. Trong công trình này, chúng tôi xem xét SMoE cho Transformers, trong đó các lớp SMoE được chèn vào giữa các khối Transformer liền kề. Mỗi lớp SMoE bao gồm một bộ định tuyến và một số mạng chuyên gia. Theo hầu hết các công trình trước (Fedus et al., 2021), chúng tôi sử dụng các mạng feed-forward làm chuyên gia, thay vì các mô-đun tự chú ý.

Đối với token đầu vào x với biểu diễn ẩn h∈Rd, bộ định tuyến tính toán điểm định tuyến giữa h và chuyên gia thứ i bằng độ đo tương tự tích vô hướng si = h⊤ei, trong đó ei ∈ Rd là một nhúng chuyên gia có thể học, và d là kích thước ẩn của mô hình. Sau đó, bộ định tuyến sử dụng hàm cổng thưa thớt g(r) để làm cho mạng chuyên gia được kích hoạt có điều kiện.

Trong bài báo này, chúng tôi chủ yếu tập trung vào định tuyến top-1, tức là chỉ chuyên gia có điểm định tuyến lớn nhất được kích hoạt. Chính thức, xem xét một lớp SMoE với N chuyên gia, hàm tiến của SMoE có thể được viết như:
k = arg max_i si = arg max_i h⊤ei (1)
fSMoE(h) = h + g(sk)fFFN_k(h) (2)
trong đó fFFN_k(·) biểu thị mạng chuyên gia thứ k được triển khai như các mạng feed-forward xếp chồng. Hơn nữa, chúng tôi khám phá cả cổng softmax (Lepikhin et al., 2021; Fedus et al., 2021) và cổng sigmoid (Lewis et al., 2021; Dai et al., 2022) cho hàm g(sk):
g(sk) = {
    exp(sk)/∑_{j=1}^N exp(sj); cổng softmax
    σ(sk); cổng sigmoid
} (3)
trong đó σ(·) là hàm sigmoid.

2.2 Sụp Đổ Biểu Diễn Của Hỗn Hợp Thưa Thớt Các Chuyên Gia
Chúng tôi trình bày cách sụp đổ biểu diễn xảy ra trong các mô hình hỗn hợp thưa thớt các chuyên gia. Để thuận tiện, chúng tôi sử dụng h' = fSMoE(h) để biểu thị đầu ra của lớp SMoE như trong Phương trình (2), Sk = g(sk) để biểu thị đầu ra thứ k của hàm softmax, và hFFN = fFFN_k(h) để biểu thị đầu ra của mạng chuyên gia thứ k. Ma trận Jacobian đối với h được cho bởi:
J = J1 + J2 = (I + SkJFFN) + ∑_{j=1}^N Sk(δkj - Sj)hFFN e_j^⊤ (4)
trong đó δkj là delta Kronecker. Phương trình có nghĩa là ma trận Jacobian có thể được phân tách thành hai số hạng. Số hạng đầu tiên J1 biểu thị việc tạo ra biểu diễn token tốt hơn với kích hoạt hiện tại Sk. Số hạng thứ hai J2 có nghĩa là học hàm cổng tốt hơn để có điểm kích hoạt thích hợp Sk. Sau lan truyền ngược, gradient được nhận từ hai đường dẫn trên, được viết là ∇_h L = J1^⊤ ∇_{h'} L + J2^⊤ ∇_{h'} L. Số hạng thứ hai có thể được mở rộng như:
J2^⊤ ∇_{h'} L = ∑_{j=1}^N Sk(δkj - Sj)(hFFN^⊤ ∇_{h'} L)ej = ∑_{j=1}^N cj ej (5)
trong đó cj = Sk(δkj - Sj)(hFFN^⊤ ∇_{h'} L). Phương trình trên chỉ ra rằng biểu diễn token h có xu hướng được cập nhật hướng tới một tổ hợp tuyến tính của các nhúng chuyên gia.

Phát hiện này cũng đúng cho định tuyến top-K (Lepikhin et al., 2021) trong đó K chuyên gia hàng đầu (K≤N) được kích hoạt cho mỗi token. Hàm tiến của định tuyến top-K là k1, k2, ..., kK = topK(sk) và h' = fSMoE(h) = h + ∑_{i=1...K} g(s_{ki})fFFN_{ki}(h). Hàm cổng được định nghĩa là g(s_{ki}) = exp(s_{ki})/∑_{j=1...K} exp(s_{kj}). Tương tự như Phương trình (5), chúng ta có:
J2^⊤ ∇_{h'} L = ∑_{i=1}^K ∑_{j=1}^K S_{ki}(δ_{kikj} - S_{kj})(h^⊤_{FFN_{ki}} ∇_{h'} L)e_{kj} = ∑_{j=1}^K cj e_{kj} (6)
Do đó, phát hiện trên đúng cho định tuyến top-K.

Chúng tôi cho rằng hành vi như vậy có khả năng làm hại khả năng biểu diễn của Transformers. Thứ nhất, xem xét rằng N vector chuyên gia có thể trải rộng một không gian N-chiều tối đa thông qua các tổ hợp tuyến tính. Vì N nhỏ hơn nhiều so với kích thước ẩn d trong thực tế, không gian con trải rộng không tận dụng đầy đủ toàn bộ dung lượng có sẵn. Do đó, cơ chế làm cho vector ẩn Transformer h sụp đổ thành một không gian con N-chiều, hàm ý xu hướng sụp đổ biểu diễn từ Rd sang RN trong đó N<<d trong thực tế. Thứ hai, Phương trình (5) chỉ ra rằng vector ẩn h có xu hướng tương tự như nhúng chuyên gia mà nó được định tuyến tới. Nếu các trạng thái ẩn được định tuyến tới cùng một chuyên gia, chúng sẽ được đẩy gần nhau hơn. Tuy nhiên, chúng ta muốn khuyến khích các biểu diễn đa dạng hơn, để chúng có thể biểu cảm và phân biệt hơn. Hiện tượng này có thể hạn chế khả năng biểu đạt của các trạng thái ẩn, đặc biệt khi một chuyên gia có xu hướng thống trị định tuyến.

3 Phương Pháp
Chúng tôi giới thiệu thuật toán định tuyến cho hỗn hợp thưa thớt các chuyên gia, đo điểm định tuyến giữa các token và chuyên gia trên một siêu cầu chiều thấp. Như được trình bày trong Hình 1b, chúng tôi giải quyết vấn đề sụp đổ biểu diễn của SMoE bằng cách áp dụng giảm chiều và chuẩn hóa L2 cho các biểu diễn token và nhúng chuyên gia. Sau đó, chúng tôi mô tả cách kết hợp thuật toán định tuyến vào một mô hình SMoE dưới mô hình tiền huấn luyện-sau đó-tinh chỉnh.

3.1 Thuật Toán Định Tuyến
Giảm Chiều Để giảm thiểu vấn đề sụp đổ biểu diễn được đề cập trong Phần 2.2, chúng tôi biểu diễn nhúng chuyên gia ei và vector token h trong một không gian chiều thấp thay vì không gian ẩn chiều cao ban đầu. Cụ thể, chúng tôi đầu tiên tham số hóa các chuyên gia với các nhúng chiều thấp hơn ei ∈ Rde sao cho de nhỏ hơn nhiều so với kích thước ẩn Transformer d. Tiếp theo, chúng tôi thực hiện phép chiếu trên các trạng thái ẩn fproj(h), chiếu h vào không gian nhúng chuyên gia. Chúng tôi sử dụng phép chiếu tuyến tính fproj(h) = Wh sao cho W ∈ Rde×d. Do đó, hàm tính điểm định tuyến giữa các token và chuyên gia có thể được viết là si = (Wh)⊤ei. Thông thường chúng tôi đặt de = N/2 (tức là một nửa số lượng chuyên gia) trong triển khai của chúng tôi.

Được truyền cảm hứng bởi Jing et al. (2022), giảm chiều giảm thiểu các vấn đề được mô tả trong Phần 2.2 từ hai khía cạnh. Đầu tiên, phép chiếu tuyến tính Wh cách ly tương tác trực tiếp giữa vector ẩn h và nhúng chuyên gia ei, có xu hướng làm giảm sụp đổ nối tiếp cho các biểu diễn. Thứ hai, việc áp dụng một bộ chiếu hạng thấp cho các vector ẩn là tự nhiên, vì số lượng chuyên gia thường nhỏ hơn nhiều so với kích thước ẩn của Transformers. Do đó, chiều giảm phù hợp hơn với bản chất hạng thấp của định tuyến.

Chuẩn Hóa L2 Sau giảm chiều, chúng tôi áp dụng chuẩn hóa L2 cho cả biểu diễn token và nhúng chuyên gia. Điểm định tuyến của chúng tôi được định nghĩa là:
si = (Wh)⊤ei / (||Wh|| ||ei||) (7)
trong đó ||·|| là chuẩn hóa L2. Do đó, các biểu diễn kết quả được chuyển đổi thành một tỷ lệ nhất định với điểm định tuyến ổn định.

Như được mô tả trong Phần 2.2, nếu một chuyên gia thống trị một tập hợp các trạng thái ẩn, các biểu diễn được đẩy về phía nhúng chuyên gia. Để tận dụng đầy đủ không gian, chúng tôi ưu tiên tính đồng nhất lớn hơn của các biểu diễn trong khi tránh các chuyên gia thống trị. Cho một vector ẩn h, điểm định tuyến tích vô hướng si = (Wh)⊤ei bị ảnh hưởng bởi cả ||ei|| và cos(Wh, ei). Vì vậy một số chuyên gia được phân bổ với nhiều token hơn vì giá trị lớn hơn của ||ei||. Ngược lại, chuẩn hóa L2 chiếu các vector trên siêu cầu đơn vị, điều này ngăn chặn hiệu ứng không mong muốn của ||ei||. Trực quan hóa trong Hình 2b cũng xác nhận rằng phương pháp của chúng tôi cải thiện tính đồng nhất của các biểu diễn đã học.

Mẹo Triển Khai. Khi mở rộng mô hình với nhiều chuyên gia hơn, một cách thực nghiệm, chúng tôi quan sát rằng việc gán token kết quả có thể dao động nếu chuẩn nhúng chuyên gia ||ei|| nhỏ. Do đó, chúng tôi khởi tạo các nhúng chuyên gia với chuẩn L2 là 0.1 và giữ chuẩn không đổi trong quá trình huấn luyện. Vì các nhúng chuyên gia được tham số hóa trong không gian Rde, tốc độ thay đổi góc của ei tỷ lệ nghịch với ||ei||. Kết quả là, nếu chuẩn nhỏ, góc của ei được cập nhật nhanh, cuối cùng dẫn đến dao động của việc gán token, đặc biệt khi mở rộng mô hình với nhiều chuyên gia hơn.

Cổng với Nhiệt Độ Có Thể Học Ngoài ra, chúng tôi thêm một vô hướng nhiệt độ có thể học trong hàm cổng SMoE g(sk). Vì chuẩn hóa L2 thay đổi tỷ lệ điểm định tuyến sk về phạm vi [-1, 1], việc sử dụng trực tiếp các điểm cho cổng SMoE có xu hướng làm cho việc kích hoạt chuyên gia quá bảo thủ. Nhiệt độ được giới thiệu cho phép bộ định tuyến điều chỉnh cổng g(sk) tương ứng. Cụ thể hơn, hàm cổng của chúng tôi là:
g(sk) = {
    exp(sk/τ)/∑_{j=1}^N exp(sj/τ); cổng softmax
    σ(sk/τ); cổng sigmoid
} (8)
trong đó σ(·) là hàm sigmoid, và vô hướng nhiệt độ τ có thể học.

3.2 Mục Tiêu Huấn Luyện
Mục tiêu huấn luyện là cùng tối thiểu hóa mất mát của tác vụ mục tiêu và một mất mát cân bằng tải phụ trợ (Fedus et al., 2021). Mất mát cân bằng tải được tính toán riêng biệt cho mỗi bộ định tuyến. Đối với mỗi bộ định tuyến, cho tần suất ti của việc có bao nhiêu token được định tuyến tới chuyên gia thứ i và điểm định tuyến si, mất mát cân bằng tải được tính toán thông qua:
Lbalance = N/|B| ∑_{i=1}^N ∑_{token∈B} ti exp(si/τ0) / ∑_{j=1}^N exp(sj/τ0) (9)
trong đó N là số lượng chuyên gia, B là một lô ví dụ huấn luyện, |B| là số lượng token, và τ0 là một nhiệt độ hằng số. Khác với τ có thể học trong Phương trình (8), τ0 được giữ cố định trong quá trình huấn luyện. Mục tiêu huấn luyện tổng thể là tối thiểu hóa:
L = Ltask + λLbalance (10)
trong đó λ là hệ số cho cân bằng tải. Số hạng Ltask được xác định bởi tác vụ cụ thể mà Transformer học. Ví dụ, chúng tôi sử dụng mất mát mô hình ngôn ngữ có mặt nạ (Devlin et al., 2019) cho tiền huấn luyện, và mục tiêu học chuỗi-tới-chuỗi cho dịch máy nơ-ron.

3.3 Định Tuyến Đóng Băng Trong Tinh Chỉnh
Chúng tôi đánh giá SMoE dưới mô hình tiền huấn luyện-sau đó-tinh chỉnh trong công trình của chúng tôi. Trong quá trình tinh chỉnh, chúng tôi đóng băng tất cả các tham số của chuyên gia, bao gồm cả bộ định tuyến và các mạng chuyên gia. Bởi vì các tập dữ liệu tinh chỉnh thường nhỏ so với kho dữ liệu tiền huấn luyện. Chúng tôi thấy rằng các mô hình SMoE có xu hướng quá khớp với các tác vụ hạ lưu, điều này thường dẫn đến định tuyến không nhất quán. Đóng băng các tham số SMoE giúp giảm thiểu các vấn đề trên. Chú ý rằng chúng tôi vẫn sử dụng mất mát cân bằng tải mặc dù các bộ định tuyến được giữ cố định, điều này cải thiện hiệu suất tinh chỉnh trong thực nghiệm của chúng tôi.

4 Thí Nghiệm
Chúng tôi thực hiện thí nghiệm trên tiền huấn luyện mô hình ngôn ngữ đa ngôn ngữ (Devlin et al., 2019). Chúng tôi đánh giá hiệu suất bằng cách tinh chỉnh các mô hình đã tiền huấn luyện trên các tiêu chuẩn hạ lưu khác nhau. Chúng tôi cũng so sánh mất mát xác thực của tác vụ mô hình ngôn ngữ có mặt nạ. Phương pháp của chúng tôi được đặt tên là X-MOE trong các phần sau.

4.1 Thiết Lập Thí Nghiệm
Dữ Liệu Tiền Huấn Luyện Theo (Chi et al., 2021), chúng tôi sử dụng sự kết hợp của CCNet (Wenzek et al., 2019) và bản sao Wikipedia làm kho dữ liệu tiền huấn luyện. Chúng tôi lấy mẫu các câu trong 94 ngôn ngữ từ kho dữ liệu, và sử dụng phân phối cân bằng lại được giới thiệu bởi Conneau và Lample (2019), điều này tăng xác suất của các ngôn ngữ ít tài nguyên.

Kiến Trúc Mô Hình và Siêu Tham Số Chúng tôi xây dựng các mô hình X-MOE sử dụng bộ mã hóa Transformer (L = 12, H = 768, A = 12) với từ vựng được cung cấp bởi Conneau et al. (2020) làm kiến trúc xương sống. Theo Lewis et al. (2021), chúng tôi xây dựng một lớp thưa thớt 32 chuyên gia với 3 lớp con FFN, và chèn nó sau lớp Transformer thứ 6. Chiều định tuyến de được đặt là 16. Nhiệt độ cổng τ0 được đặt là 0.3 và 0.07 cho cổng softmax và sigmoid tương ứng. Các siêu tham số chi tiết của các mô hình X-MOE có thể được tìm thấy trong Phụ lục A. Các mô hình X-MOE được tiền huấn luyện với bộ tối ưu hóa Adam (β1 = 0.9, β2 = 0.98) sử dụng kích thước lô 2,048 trong 125K bước. Quy trình tiền huấn luyện mất 2 ngày trên 2 Nvidia DGX-2 Stations. Phụ lục B và Phụ lục C cung cấp các siêu tham số chi tiết cho tiền huấn luyện và tinh chỉnh X-MOE.

Baseline Chúng tôi xem xét hai baseline trong thí nghiệm của chúng tôi. (1) Dense là một bộ mã hóa Transformer dày đặc không có các mô-đun kích hoạt thưa thớt. (2) SMoE là triển khai của chúng tôi cho Switch Transformers (Fedus et al., 2021). Baseline SMoE được xây dựng với cùng thiết lập với X-MOE. Ngoài triển khai cổng softmax ban đầu của nó, chúng tôi cũng triển khai một biến thể cổng sigmoid (Lewis et al., 2021; Dai et al., 2022) của Switch Transformers làm phương pháp baseline. Chú ý rằng các mô hình baseline được tiền huấn luyện với cùng dữ liệu huấn luyện như X-MOE để so sánh công bằng.

4.2 Đánh Giá Hạ Lưu
Chúng tôi thực hiện đánh giá hạ lưu trên bảy tiêu chuẩn hiểu biết đa ngôn ngữ được sử dụng rộng rãi từ XTREME (Hu et al., 2020). Cụ thể, chúng tôi thực hiện thí nghiệm trên gán nhãn từ loại Universal Dependencies v2.5 (Zeman et al., 2019), nhận dạng thực thể có tên WikiAnn (Pan et al., 2017; Rahimi et al., 2019), suy luận ngôn ngữ tự nhiên (XNLI; Conneau et al. 2018), đối thủ diễn giải từ xáo trộn từ (PAWS-X; Yang et al. 2019), và hỏi đáp trên MLQA (Lewis et al., 2020), XQuAD (Artetxe et al., 2020), và TyDiQA-GoldP (Clark et al., 2020). Trong số các tiêu chuẩn, chúng tôi áp dụng thiết lập chuyển giao đa ngôn ngữ, trong đó các mô hình được tinh chỉnh với dữ liệu huấn luyện bằng tiếng Anh và đánh giá trong tất cả các ngôn ngữ mục tiêu.

Bảng 1 trình bày kết quả đánh giá trên bảy tác vụ hạ lưu từ tiêu chuẩn XTREME. Đối với mỗi tác vụ, kết quả đầu tiên được tính trung bình giữa các ngôn ngữ kiểm tra và sau đó được tính trung bình trên năm hạt giống ngẫu nhiên. Tổng thể, mô hình X-MOE cổng softmax đạt hiệu suất tốt nhất, đạt điểm trung bình 65.3. So sánh các mô hình SMoE với mô hình dày đặc, các mô hình SMoE cho thấy cải thiện đáng chú ý, chỉ ra rằng các mô hình SMoE hưởng lợi từ dung lượng mô hình lớn. So sánh X-MOE với hai baseline SMoE, nó cho thấy rằng các mô hình X-MOE cung cấp cải thiện nhất quán trên các tác vụ hạ lưu, chứng minh hiệu quả của thuật toán định tuyến đề xuất của chúng tôi. Chúng tôi cũng xác thực X-MOE dưới thiết lập định tuyến top-2. Bảng 7 trình bày kết quả đánh giá trên XNLI, cho thấy cải thiện nhất quán so với baseline cho cả thiết lập định tuyến top-1 và top-2.

4.3 Đánh Giá Thượng Lưu
Chúng tôi so sánh các mô hình đã tiền huấn luyện về hiệu suất thượng lưu bằng độ bối rối xác thực trên mô hình ngôn ngữ có mặt nạ (MLM). Chúng tôi lấy mẫu các câu đa ngôn ngữ từ mC4 (Xue et al., 2020), và xây dựng một tập dữ liệu xác thực MLM chứa 65,536 chuỗi với độ dài khoảng 512. Kết quả được hiển thị trong Bảng 2. Tương tự như kết quả hạ lưu, chúng tôi quan sát rằng các mô hình SMoE hoạt động tốt hơn so với mô hình dày đặc. Về các mô hình SMoE, các mô hình X-MOE với cả cổng softmax và sigmoid đều đạt độ bối rối mô hình ngôn ngữ có mặt nạ thấp hơn so với các đối tác của chúng. Trong số tất cả các mô hình đã tiền huấn luyện, X-MOE cổng softmax đạt độ bối rối xác thực thấp nhất. Kết quả cho thấy rằng phương pháp của chúng tôi không chỉ hoạt động tốt cho việc học các biểu diễn văn bản có thể chuyển giao cho các tác vụ hạ lưu, mà còn mang lại cải thiện cho tác vụ mô hình ngôn ngữ có mặt nạ thượng lưu. So sánh kết quả thượng lưu với kết quả hạ lưu, nó cho thấy rằng đạt được độ bối rối thượng lưu thấp hơn không hứa hẹn hiệu suất hạ lưu tốt hơn. Ví dụ, mô hình X-MOE cổng sigmoid có độ bối rối lớn hơn so với baseline SMoE cổng softmax, nhưng vượt trội hơn hiệu suất tinh chỉnh của baseline trên các tác vụ hạ lưu.

Chúng tôi cũng thực hiện thí nghiệm trên tác vụ dịch máy đa ngôn ngữ. Như được hiển thị trong Bảng 4, chúng tôi trình bày điểm BLEU trên tập dữ liệu WMT-10 (Wang et al., 2020) trong đó các mô hình được đánh giá theo hướng 'x→en'. X-MOE liên tục vượt trội so với cả mô hình dày đặc và baseline SMoE trong tám hướng dịch.

4.4 Nghiên Cứu Loại Bỏ
Thuật Toán Định Tuyến Để hiểu rõ hơn thuật toán định tuyến của chúng tôi, chúng tôi tiền huấn luyện một số biến thể của các mô hình X-MOE cổng sigmoid với các kết hợp khác nhau của giảm chiều (Dim. Red.), chuẩn hóa L2 (L2Norm), và đóng băng định tuyến (Frozen). Để so sánh công bằng, tất cả các mô hình được tiền huấn luyện và tinh chỉnh dưới cùng một thiết lập, tức là dữ liệu huấn luyện, các bước, và các hạt giống ngẫu nhiên. Chúng tôi đánh giá các mô hình trên XNLI và MLQA, và báo cáo kết quả trong Bảng 3. Sử dụng đồng thời ba phương pháp định tuyến đạt hiệu suất tốt nhất. Khi loại bỏ một trong ba phương pháp định tuyến, mô hình hoạt động kém hơn, chứng minh rằng X-MOE hưởng lợi từ cả ba thành phần.

Chiều Của Nhúng Chuyên Gia Chúng tôi thực hiện thí nghiệm bằng cách điều chỉnh chiều định tuyến cho giảm chiều. Cụ thể, chúng tôi so sánh các mô hình X-MOE cổng sigmoid với các chiều định tuyến N/4, N/2, N, 2N, và 4N, trong đó N là số lượng chuyên gia. Bảng 5 hiển thị hiệu suất hạ lưu. Nó cho thấy rằng sử dụng chiều định tuyến N/2 cung cấp hiệu suất tốt nhất cho XNLI và N/4 là tốt nhất cho MLQA. Kết quả cũng xác nhận rằng giảm chiều phù hợp hơn với bản chất hạng thấp của định tuyến SMoE.

Cân Bằng Tải Trong Tinh Chỉnh Chúng tôi khám phá liệu cân bằng tải có lợi cho việc tinh chỉnh các mô hình SMoE hay không. Để đạt được điều này, chúng tôi thêm mất mát cân bằng tải vào tổng mất mát với các trọng số khác nhau khi tinh chỉnh các mô hình X-MOE trên XNLI và MLQA. Bảng 6 hiển thị điểm xác thực trung bình trong đó chúng tôi tìm kiếm hệ số cân bằng tải từ 0 đến 10^-1. Chúng tôi quan sát rằng sử dụng mất mát cân bằng trong quá trình tinh chỉnh có lợi một chút cho X-MOE. Khi loại bỏ mất mát cân bằng, X-MOE vẫn duy trì kết quả tương đương trên cả XNLI và MLQA.

4.5 Phân Tích
Sụp Đổ Biểu Diễn Chúng tôi phân tích định tính vấn đề sụp đổ biểu diễn bằng cách trực quan hóa các chuyên gia. Hình 2a và 2b minh họa cấu trúc không gian của các chuyên gia của baseline SMoE và X-MOE trong không gian hyperbolic, được tạo ra bởi Uniform Manifold Approximation and Projection (UMAP; McInnes et al. 2018) với n-neighbor là 100 và min-dist là 1. Mỗi điểm dữ liệu đại diện cho một token được định tuyến, trong đó chúng tôi sử dụng các trạng thái ẩn cho baseline SMoE và các biểu diễn token được chiếu cho X-MOE. Mỗi màu đại diện cho một chuyên gia mà các token được gán tới.

Hình 2a cho thấy rằng hầu hết các điểm được trộn lẫn với nhau với một lượng lớn không gian có sẵn không được sử dụng, cho thấy sụp đổ biểu diễn trong không gian nhúng chuyên gia. Ngược lại, X-MOE trong Hình 2b cho thấy một không gian đặc trưng được tổ chức tốt với sự phân biệt rõ ràng giữa các cụm. Điều này chỉ ra rằng các phương pháp định tuyến của chúng tôi thành công trong việc chiếu các token vào không gian nhúng chuyên gia với các đặc trưng định tuyến được bảo toàn.

Ngoài ra, chúng tôi thực hiện phân tích định lượng về mức độ sụp đổ biểu diễn cho các trạng thái ẩn Transformer đã học được đưa vào định tuyến SMoE. Chúng tôi sử dụng độ đo sụp đổ biểu diễn được đề xuất trong (Zhu et al., 2021). Cho các biểu diễn cần đo, chúng tôi sử dụng W và B để biểu thị các ma trận hiệp phương sai trong lớp và giữa các lớp tương ứng. Độ đo sụp đổ biểu diễn (RC) được tính toán thông qua:
RC = Tr(WB†) (11)
trong đó B† là nghịch đảo giả của B. Giá trị RC nhỏ hơn chỉ ra sụp đổ biểu diễn ở mức độ lớn hơn. Hình 2c minh họa các độ đo trong quá trình tiền huấn luyện, trong đó dữ liệu được lấy mẫu từ tập xác thực được đề cập trong Phần 4.3. Baseline SMoE không giống như các mô hình đặc trưng không bị ràng buộc có thể sụp đổ thực nghiệm về gần như RC bằng không, nhưng vẫn cho thấy xu hướng giảm nhất quán qua tiền huấn luyện, hàm ý xu hướng sụp đổ biểu diễn. Khác biệt, X-MOE đạt được điểm RC lớn hơn so với baseline SMoE với xu hướng tăng qua tiền huấn luyện.

Tính Nhất Quán Định Tuyến Qua Tiền Huấn Luyện Chúng tôi kiểm tra liệu thuật toán định tuyến đề xuất của chúng tôi có đạt được định tuyến nhất quán hơn qua quá trình huấn luyện hay không. Chúng tôi đo tính nhất quán định tuyến thông qua độ đo tỷ lệ dao động định tuyến (RF). Dao động định tuyến được định nghĩa là sự thay đổi của chuyên gia mục tiêu của một token đầu vào. Tương ứng, tỷ lệ RF đo tỷ lệ RF giữa checkpoint hiện tại và checkpoint cuối cùng cho cùng một đầu vào. Tỷ lệ RF thấp hơn chỉ ra tính nhất quán định tuyến tốt hơn. Như được hiển thị trong Hình 3a, chúng tôi trình bày tỷ lệ RF trên tập xác thực MLM được đề cập trong Phần 4.3. Sau bước 15K, X-MOE cho thấy tỷ lệ RF thấp hơn nhiều so với baseline SMoE, chỉ ra rằng mô hình của chúng tôi tạo ra hành vi định tuyến nhất quán hơn.

Tính Nhất Quán Giữa Các Lần Chạy Qua Tinh Chỉnh Trong các thí nghiệm đánh giá hạ lưu, chúng tôi thấy rằng hành vi định tuyến của các mô hình baseline SMoE có thể nhạy cảm với các hạt giống ngẫu nhiên. Vì các việc gán token đã học khác nhau cho các thứ tự dữ liệu huấn luyện khác nhau, hiệu suất hạ lưu cuối cùng có thể đa dạng giữa các lần chạy. Do đó, chúng tôi nghiên cứu hành vi định tuyến của các mô hình baseline SMoE và X-MOE qua tinh chỉnh. Để đạt được điều này, chúng tôi phát triển một độ đo, được gọi là tính nhất quán giữa các lần chạy, đo mức độ gần gũi của việc gán token hội tụ giữa các lần chạy với các hạt giống khác nhau. Xem xét một mô hình với N chuyên gia, cho l = [n1, ..., nN] biểu thị tổng tải của các chuyên gia, trong đó ni đại diện cho số lượng token được gán cho chuyên gia thứ i. Cho hai tải l1 và l2 từ hai lần chạy với các hạt giống khác nhau, độ tương tự giữa l1 và l2 được định nghĩa là hệ số tương quan Pearson (PCC) giữa chúng, được ký hiệu là ρ(l1, l2). Ở đây PCC chỉ phục vụ như một độ đo tương tự chứ không phải đo tương quan tuyến tính giữa các biến. Bằng cách mở rộng nó cho m lần chạy với các hạt giống khác nhau cho mỗi lần chạy, chúng tôi định nghĩa tính nhất quán giữa các lần chạy là trung bình của ma trận tương quan IC = Σ_{i,j∈{1...m}} ρ(li, lj)/m².

Chúng tôi tinh chỉnh các mô hình X-MOE và baseline SMoE trên XNLI cho 12 lần chạy riêng biệt. Sau đó chúng tôi tính toán tính nhất quán giữa các lần chạy cho mỗi 100 mini-batch, tức là tải chuyên gia được tích lũy trong 100 bước. Hình 3b minh họa tính nhất quán giữa các lần chạy. Baseline SMoE hội tụ về các giải pháp định tuyến khác nhau qua nhiều lần chạy tinh chỉnh, mặc dù sự khác biệt duy nhất giữa các lần chạy là hạt giống ngẫu nhiên. So sánh, X-MOE đạt được tính nhất quán giữa các lần chạy tốt hơn đáng kể so với baseline SMoE. Đường cong của X-MOE chỉ ra rằng các mô hình có các hành vi định tuyến khác nhau ở đầu quá trình tinh chỉnh, nhưng cuối cùng hội tụ về gần như cùng hành vi định tuyến.

5 Công Trình Liên Quan
SMoE cho Các Mô Hình Quy Mô Lớn Các mô hình Hỗn Hợp Thưa Thớt Các Chuyên Gia (SMoE) được giới thiệu bởi Shazeer et al. (2017), mở rộng hỗn hợp các chuyên gia (Jacobs et al., 1991; Jordan và Jacobs, 1994) với các kỹ thuật tính toán có điều kiện (Bengio et al., 2013; 2015). Tận dụng tính toán có điều kiện, SMoE cho phép tăng dung lượng mô hình một cách lớn trong khi duy trì hiệu quả tính toán. Để khám phá tiềm năng của SMoE, các nghiên cứu gần đây áp dụng SMoE trong một loạt rộng các vấn đề học máy như dịch máy (Lepikhin et al., 2021), phân loại hình ảnh (Riquelme et al., 2021), nhận dạng giọng nói (Kumatani et al., 2021). Ngoài tình huống học có giám sát, đã có công trình khám phá SMoE dưới mô hình tiền huấn luyện-tinh chỉnh, và quan sát sự khác biệt giữa chất lượng tiền huấn luyện mạnh và hiệu suất tinh chỉnh kém (Fedus et al., 2021; Artetxe et al., 2021; Zoph et al., 2022). Bên cạnh đó, hành vi mở rộng của SMoE cũng được nghiên cứu (Clark et al., 2022; Du et al., 2021).

Thuật Toán Định Tuyến SMoE Nhiều nghiên cứu gần đây khám phá các thuật toán gán token cho định tuyến SMoE. Các lớp BASE (Lewis et al., 2021) công thức hóa vấn đề định tuyến token như một bài toán gán tuyến tính. Các lớp Hash (Roller et al., 2021) sử dụng thuật toán gán không có tham số định tuyến token bằng hashing. Zhou et al. (2022) để mỗi chuyên gia chọn top-k token thay vì phân phối token tới chuyên gia. Dai et al. (2022) đề xuất đóng băng hàm định tuyến để giảm dao động định tuyến. Các phương pháp này tập trung vào thuật toán gán trong định tuyến, nhưng thuật toán định tuyến của chúng tôi tập trung vào cải thiện độ đo định tuyến cơ bản, vẫn còn ít được khám phá.

Sụp Đổ Biểu Diễn Sụp đổ biểu diễn, còn được gọi là sụp đổ nơ-ron, là sự thoái hóa của các biểu diễn trong quá trình huấn luyện các mạng nơ-ron. Một số nghiên cứu quan sát rằng sự biến đổi trong lớp của các biểu diễn trong các mạng phân loại trở nên không đáng kể ở giai đoạn cuối của quá trình huấn luyện (Papyan et al., 2020; Zhu et al., 2021; Tirer và Bruna, 2022). Bên cạnh đó, hiện tượng này cũng được quan sát trong tinh chỉnh mô hình ngôn ngữ (Aghajanyan et al., 2021), và học biểu diễn thị giác (Chen và He, 2021; Ermolov et al., 2021; Jing et al., 2022). Các nghiên cứu này tập trung vào các mạng nơ-ron được kích hoạt dày đặc. Trong công trình này, chúng tôi chỉ ra vấn đề sụp đổ biểu diễn trong các mô hình SMoE.

6 Kết Luận
Trong công trình này, chúng tôi chỉ ra vấn đề sụp đổ biểu diễn trong các mô hình hỗn hợp thưa thớt các chuyên gia (SMoE), và đề xuất một thuật toán định tuyến ước tính điểm định tuyến trên một siêu cầu chiều thấp. Chúng tôi thực hiện các thí nghiệm rộng rãi về tiền huấn luyện mô hình ngôn ngữ đa ngôn ngữ. Kết quả thí nghiệm trên các tiêu chuẩn khác nhau chứng minh rằng phương pháp của chúng tôi mang lại cải thiện nhất quán so với các baseline SMoE về cả hiệu suất mô hình ngôn ngữ và tinh chỉnh. Bên cạnh đó, phương pháp của chúng tôi giảm thiểu xu hướng sụp đổ biểu diễn và đạt được định tuyến nhất quán hơn. Chúng tôi sẽ cải thiện công trình từ các khía cạnh sau. Đầu tiên, hầu hết các thí nghiệm X-MOE hiện tại được thực hiện trên các tác vụ ngôn ngữ, như tiền huấn luyện mô hình ngôn ngữ đa ngôn ngữ, và dịch máy. Chúng tôi cũng sẽ đánh giá phương pháp đề xuất trên tiền huấn luyện thị giác (Bao et al., 2022; Peng et al., 2022) và tiền huấn luyện đa phương thức (Wang et al., 2022). Thứ hai, chúng tôi muốn báo cáo kết quả mở rộng kích thước mô hình. Cải thiện hiệu suất có xu hướng lớn hơn với số lượng chuyên gia lớn hơn.

Cân Nhắc Đạo Đức Một trong những tác động tiêu cực xã hội của việc huấn luyện các mô hình quy mô lớn là chi phí tính toán và môi trường cao. Bài báo của chúng tôi tập trung vào cải thiện SMoE, thường hiệu quả hơn so với huấn luyện mô hình dày đặc với cùng số lượng tham số. Vì vậy các thuật toán SMoE tốt hơn có khả năng tiết kiệm tính toán cần thiết và giảm phát thải CO2 từ tính toán. Hơn nữa, X-MOE cải thiện tiền huấn luyện và tinh chỉnh đa ngôn ngữ, để chúng ta có thể chuyển giao tốt hơn kiến thức đa ngôn ngữ từ các ngôn ngữ có nhiều tài nguyên sang ít tài nguyên. Phước lành của kích thước mô hình lớn hơn mang lại bởi SMoE giảm xung đột tham số của tính đa ngôn ngữ, trong khi giữ chi phí tính toán có thể quản lý.

Lời Cảm Ơn
Chúng tôi muốn cảm ơn Bo Zheng và Zhiliang Peng cho các cuộc thảo luận hữu ích.

Tài Liệu Tham Khảo
[Danh sách tài liệu tham khảo dài được dịch trong phiên bản đầy đủ...]

A Siêu Tham Số Mô Hình
Bảng 8 trình bày các siêu tham số mô hình của X-MOE. Nhiệt độ cổng τ0 được khởi tạo là 0.3 và 0.07 cho cổng softmax và sigmoid tương ứng. Chúng tôi sử dụng cùng từ vựng như XLM-R (Conneau et al., 2020) với 250K từ phụ được token hóa bởi SentencePiece (Kudo và Richardson, 2018).

B Siêu Tham Số cho Tiền Huấn Luyện
Bảng 9 trình bày các siêu tham số cho tiền huấn luyện.

C Siêu Tham Số cho Tinh Chỉnh
Bảng 10 trình bày các siêu tham số cho tinh chỉnh trên các tác vụ hạ lưu XTREME.
