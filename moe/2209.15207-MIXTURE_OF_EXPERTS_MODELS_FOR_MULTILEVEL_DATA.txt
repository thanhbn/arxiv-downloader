# 2209.15207.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2209.15207.pdf
# File size: 1693499 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
MIXTURE OF EXPERTS MODELS FOR MULTILEVEL DATA :
MODELLING FRAMEWORK AND APPROXIMATION THEORY
Tsz Chai FungSpark C. Tseungy
ABSTRACT
Multilevel data are prevalent in many real-world applications. However, it remains an open
research problem to identify and justify a class of models that ﬂexibly capture a wide range
of multilevel data. Motivated by the versatility of the mixture of experts (MoE) models in
ﬁtting regression data, in this article we extend upon the MoE and study a class of mixed
MoE (MMoE) models for multilevel data. Under some regularity conditions, we prove that
the MMoE is dense in the space of any continuous mixed effects models in the sense of
weak convergence. As a result, the MMoE has a potential to accurately resemble almost all
characteristics inherited in multilevel data, including the marginal distributions, dependence
structures, regression links, random intercepts and random slopes. In a particular case where
the multilevel data is hierarchical, we further show that a nested version of the MMoE
universally approximates a broad range of dependence structures of the random effects
among different factor levels.
Keywords Artiﬁcial neural network Crossed and nested random effects DensenessMixed effects
modelsUniversal approximation theorem
1 Introduction
Mixture of experts (MoE) model, which is ﬁrst introduced by Jacobs et al. [1991] (see also, e.g., Jordan
and Jacobs [1994] and McLachlan and Peel [2000] for details), is a probabilistic version of neural network
architecture useful for ﬂexible regression, classiﬁcation and distribution modelling, with applications to
various areas including healthcare, business, social and environmental science. Readers may refer to Yuksel
et al. [2012], Masoudnia and Ebrahimpour [2014] and Nguyen and Chamroukhi [2018] for the literature
reviews on both the theories and applications of the MoE.
The model structure of the MoE is as follows. Suppose that we have Nobservations (y;x) =
f(yi;xi)gi=1;:::;N, whereyi= (yi1;:::;yiK)is aK-dimensional response variable with output space
Department of Risk Management and Insurance, Georgia State University. 35 Broad Street NW, Atlanta, GA 30303, United
States. Email address: tfung@gsu.edu .
yDepartment of Statistical Sciences, University of Toronto. Ontario Power Building, 700 University Avenue, 9th Floor, Toronto,
ON M5G 1Z5, Canada. Email addresses: spark.tseung@mail.utoronto.ca .arXiv:2209.15207v1  [math.ST]  30 Sep 2022

--- PAGE 2 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
Y RKandxi= (xi1:::;xiP)are thePcovariates or features with input space X RP. Under
the MoE framework, the conditional distribution function of yigivenxiis
F(yi;; ;gjxi) =gX
j=1j(xi;)F0(yi; jjxi); (1)
wheregis the number of latent classes. Here, j(xi;)>0is called the gating function withPg
j=1j(xi;) = 1 and parameters . While the most common choice of gating function is the logit-linear
or softmax gating (Jacobs et al. [1991]) given by j(xi;) = expfj;0+T
jxig=Pg
j0=1expfj0;0+T
j0xig
with=fj0;j:j= 1;:::;gg, alternative gating functions such as Gaussian gating (Xu et al. [1995]),
student-t gating (Ingrassia et al. [2012]) and probit gating (Geweke and Keane [2007]) are also explored
in literature. Also, F0(yi; jjxi)is a probability distribution called the expert function with parameters
 :=f j:j= 1;:::;gg. While a common choice for the expert function is a Gaussian distribution (Jor-
dan and Jacobs [1992]), there has been substantial developments on alternative choices of expert functions
to cater for various distributional characteristics such as heavy-tailedness (Laplace by Nguyen and McLach-
lan [2016], t-distribution by Chamroukhi [2016], skewed t by Chamroukhi [2017] and transformed gamma
by Fung et al. [2021]) and discrete distributions (Poisson by Grun and Leisch [2008] and Erlang Count by
Fung et al. [2019b]).
Model ﬂexibility is a crucial desirable property for the class of MoE, and there are extensive research work
on the approximation theory for the MoE. Zeevi et al. [1998] shows that the mean function of a univariate
(K= 1) logit-gated MoE can approximate any Sobolev class functions. This result is extended by Jiang and
Tanner [1999a], who considers the transformed Sobolev class. Without considering the convergence rate,
Nguyen et al. [2016] proves that the MoE mean function is dense in the class of any continuous functions
without the restriction of the Sobolev class, and Nguyen et al. [2019] shows similar denseness results using
a multivariate ( K > 1) Gaussian-gated MoE.
Apart from studying the mean functions, some research works focus on conditional density approximation
with respect to the Hellinger distance, Kullback–Leibler (KL) divergence, or Lebesgue space. Jiang and
Tanner [1999b] and Mendes and Jiang [2012] generalize the results of Jiang and Tanner [1999a] by demon-
strating the approximation capability of the MoE to any exponential family non-linear regression models.
Norets et al. [2010] shows that the logit-gated MoE with Gaussian expert function can approximate any
conditional densities. Similar results are proved by Norets and Pelenis [2014] and Nguyen et al. [2019],
who consider the Gaussian gating functions. Recently, Nguyen et al. [2021] proves that the class of MoE is
dense in the Lebesgue space.
Another stream of distribution approximation theorems studies the denseness in the sense of Prohorov metric
of weak convergence. Extending upon Tijms [1994] and Breuer and Baum [2005] who explore denseness of
ﬁnite mixture and phase-type distributions in the space of any probability distributions, Fung et al. [2019a]
formulates the concept of “denseness” in regression settings and shows that the class of MoE is dense in
the space of any regression distributions, subject to some regularity conditions such as Lipschitz continuity
and distribution tightness. In contrast to other existing works on distribution approximations, the results of
Fung et al. [2019a] are very general as: (i) they hold under a wide range of choices of expert functions (not
restricted to Gaussian or other symmetric expert functions); (ii) the target distribution is not restricted to a
special class (e.g., exponential family regression models).
2

--- PAGE 3 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
Despite of its model ﬂexibility, the aforementioned modelling framework implicitly assumes that the input-
output pairs are independent among observations. This is not true for multilevel data (Goldstein [2011]).
Apart from the inputs xi, there are also Llevels of factors i= (i1;:::;iL)jointly affecting the output
yi. While these factors are unobserved, they are clustered into various units for each level l= 1;:::;L
and we know how they are clustered. As shown in Figure 1, for each level l= 1;:::;L , each observation
iis assigned into one of the Slunits, where cl() :f1;:::;Ng 7! f 1;:::;Slgis denoted as a known
function mapping an observation index to one of the Slunits, withc(i) = (c1(i):::;cL(i)). We have
il=i0l:=(s)
lifcl(i) =cl(i0) =s. As the factor (s)
laffects multiple observations at the same
time, there are interdependencies among observations. Ignoring such a dependency would lead to spurious,
misleading or biased clustering and prediction outcomes (Goldstein [2011]).
Multilevel data are prevalent across many applications. The most classic one is the school problem (Aitkin
and Longford [1986], Goldstein [1986] and Frees and Kim [2006]), where “school” and “classroom within a
school” act as two levels of factors affecting the performance of a student (as an observation). Multilevel data
structure can also be caused by repeated measurements collected in longitudinal studies. This is common
across various areas including health (e.g., Molenberghs et al. [2010]) and business (e.g., Boucher and
Denuit [2006]). For instance, the medical outcome of a patient or the amount of insurance claims by a
policyholder are measured or collected repeatedly over time. A remarkable special case of multilevel data
is the hierarchical (or nested) data, where the Lfactor levels are ranked from high to low, and every lower
level factor belongs to a particular upper level factor. The school problem is a clear example of hierarchical
data.
A popular model to account for the interdependencies among observations is the generalized linear mixed
effects model (GLMM) (Goldstein [1986] and McGilchrist [1994]), which assumes that the output yide-
pends on the sum of ﬁxed effects (i.e., the impact of the inputs xi)and random effects (i.e., the impact of the
factorsi). To improve model ﬂexibility or achieve speciﬁc clustering purposes, the GLMM framework is
extended to a non-linear setting (Davidian and Gallant [1993] and Gregoire and Schabenberger [1996]), for-
mulated in a neural network structure (Bakker and Heskes [2003]) or integrated to a ﬁnite-mixture modelling
framework (Ng et al. [2004] and Ng et al. [2006]). Despite of the desirable properties of the MoE models
leading to extensive applications, the research works of mixed effects models in the context of MoE frame-
work are relatively scarce. Yau et al. [2003] ﬁrst proposes a two-component logit-gated Gaussian-expert
MoE with random effects incorporated in both gating and expert functions. Ng and McLachlan [2007] then
formulates a general g-component mixed effect MoE with the use of logistic expert functions for binary
classiﬁcations. Ng and McLachlan [2014] considers a similar framework with random effects only incorpo-
rated to the expert functions. Nonetheless, all the aforementioned mixed effect MoE models only deal with
a single level of random effect (i.e., L= 1).
Motivated by the increasing popularity of the MoE models, the prevalence of multilevel data and the de-
sirability of formally justifying model ﬂexibility through approximation theories, the contributions of this
paper are fourfold: Firstly, we propose the mixed MoE (MMoE) for multilevel regression data that allows for
multiple levels of random effects. Compared to the existing literature (Yau et al. [2003], Ng and McLachlan
[2007] and Ng and McLachlan [2014]), our proposed model is reduced in two ways: (i) random effects are
incorporated only into the gating functions and assumed to be independent, and the model includes the ran-
dom effects differently; (ii) regression links are removed from the expert functions. Secondly, we formulate
the deﬁnition of denseness for the class of mixed effects models in the sense of weak convergence, which
3

--- PAGE 4 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
Figure 1: Multilevel data structure and modelling framework.
directly extends upon Fung et al. [2019a] who formulate denseness for regression distributions. Thirdly,
we prove that the class of MMoE is dense in the space of any continuous mixed effects models subject to
some mild regularity conditions. This not only demonstrates the ﬂexibility of the proposed model in captur-
ing various aspects of multilevel data characteristics such as joint distributions, regression patterns, random
intercepts, and random slopes, but also suggests that our proposed model is parsimonious with a reduced
structure. Compared to Fung et al. [2019a], several assumptions for the denseness theorem are also relaxed
in this paper. For example, Lipschitz continuity and distribution tightness are no longer explicitly required.
Hence, the proof techniques of this paper are quite different from those in Fung et al. [2019a]. Fourthly, in
a particular case of hierarchical data, we further prove that a nested version of the MMoE can accurately
approximate a wide range of dependence structures between the upper and lower level factors, even if the
MMoE considered is a simpliﬁed model class consisting of only independent random effects across levels.
This result is critical for many applications, e.g., the classroom impact on a student’s performance is likely
dependent on the school the student attends.
The focus of this paper is to formulate the MMoE model for multilevel data and theoretically justify its
versatility. In a subsequent paper (Tseung et al. [2022]), we will address the estimation and application
problems under the proposed MMoE. A stochastic variational ECM algorithm is proposed to efﬁciently es-
timate the model parameters. Also, the MMoE is applied to an automobile insurance dataset, demonstrating
its ability to reasonably predict policyholders’ future claims based on their past claim histories.
This paper is structured as follows. Section 2 deﬁnes a generalized class of mixed effect models for mul-
tilevel data, which includes nearly all mixed effect models in the literature. In Section 3, we introduce the
MMoE as a candidate class of mixed effect models to ﬂexibly capture multilevel data. Interpretation and
visualization of the proposed model are also provided. Section 4 deﬁnes “denseness” in the context of mixed
effect models and proves that the MMoE is a universal approximator of most mixed effect models subject to
some mild conditions. In Section 5, we discuss the model formulation and denseness property in a special
case where the dataset is hierarchical with nested random effects. The ﬁndings are summarized in Section
4

--- PAGE 5 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
7, accompanying some limitations of the denseness theory in justifying the approximation capability of the
proposed MMoE.
2 Mixed effect models for multilevel data
Datasets with multilevel structure are often modelled by a mixed effects model. Under this modelling
framework, the effects of the known inputs xion the outputyiare perceived as the “ﬁxed effect” or “hard
sharing of parameters”, while the Llevels of unobserved factors iare treated as random (speciﬁed by a
distribution) and their impacts on yiare regarded as “random effect” or “soft sharing of parameters”. In
this section, we will discuss some technical details regarding a generalized framework of the mixed effects
models.
Let(
;F;P)be the probability space and suppose that (s)
lis aF-measurable map from (
;F)to(l;Ql)
for everyl= 1;:::;L ands= 1;:::;Sl, where lis the space of (s)
loril.(s)
lis a random variable if
(l;Ql) = (R;R)whereRis a Borel set, but we do not want to impose such a restriction. It is because
the factors(s)
lare unobserved and we are unsure if these factors can be quantiﬁed as a real number. Using
clinical studies as an example, it is impossible to summarize the unobserved characteristics of hospitals or
doctors into just a single number as their impacts to a patient’s medical outcome are complicated, possibly
related to many unknown hidden factors including hospital’s medical equipment and ﬁnancial support, and
doctor’s education and expertise. The space lalso varies among different mixed effects models in liter-
ature. For example, l=Rfor most GLMMs (McGilchrist [1994]), l=R2g 1for the GLMM MoE
model by Ng and McLachlan [2007], and l=RgKfor the mixture of random effects models by Ng and
McLachlan [2014].
Under the generalized mixed effects model, we assume that yiis inﬂuenced directly only by xiandi.
Conditioned on xiandi, it is further assumed that fyigi=1;:::;N are mutually independent. In particular,
we have
yijxi;iindH(jxi;i); i = 1;:::;N; (2)
whereHcan be any probability distributions. Figure 1 shows a visualization of the modelling framework.
With these assumptions, the joint distribution of ygivenxis given by
~H(yjx) =Z

"NY
i=1H(yijxi;i)#
dP (3)
Suppose that each measurable space (l;Ql)is also equipped by a probability measure Gl, corre-
sponding to the “distribution” of (s)
l. Denote further (~;~Q;G)as the product of probability spaces
f(l;Ql;Gl)gl=1;:::;L;s=1;:::;S l. Then, the joint distribution can be re-written as
~H(yjx) =Z
~"NY
i=1H(yijxi;i)#
dG(~); (4)
where ~=f(s)
lgl=1;:::;L;s=1;:::;S l. Note that the above model framework includes a very wide range of
models for multilevel or hierarchical data, including generalized linear mixed models (GLMM) and non-
linear mixed effects models (see, e.g., Goldstein [1986] and Davidian and Gallant [1993]). As there are
no restrictions on the functional forms of HandG, the above model structure is indeed very general,
5

--- PAGE 6 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
automatically containing any possible joint distributions of yijxi;i, regression links between xiandyi
(including non-linear effect and interactions among covariates), effects of unobserved factors ialone onyi
(called random intercept), and interactions between iandxi(called random slope).
Since we have deﬁned (~;~Q;G)as a product probability space, the following assumption has been implic-
itly made on(s)
l:
Assumption 1.f(s)
lgl=1;:::;L;s=1;:::;S lare mutually independent.
Therefore,G(~)can be written as
G(~) =LY
l=1SlY
s=1Gl((s)
l) ordG(~) =LY
l=1SlY
s=1Gl(d(s)
l): (5)
Note that the prior independence assumption across factors within a level (i.e., s= 1;:::;Sl) is very natural
for most data structures especially for those involving repeated measurements (see, e.g., Goldstein [1986],
Yau et al. [2003], Ng et al. [2004], Boucher and Denuit [2006], and Ng and McLachlan [2007]). The prior
independence across levels (i.e., l= 1;:::;L ) is also often assumed for datasets with multilevel structure
(see, e.g., Goldstein [1986] and McGilchrist [1994]).
3 Mixture of experts model with random effect
Despite of the generality of the above mixed effect model (Equation (3)), it is essential to appropriately spec-
ify the functional forms of HandGto model a multilevel dataset. However, this is challenging, especially
when the space ~of the latent factors ~is not observed from the dataset. Recall that a multilevel dataset
only provides information on how each observation is classiﬁed into one of the factors for each level l, but
not on what the factors are or how to quantify these factors.
In this section, we introduce the mixture of experts (MoE) model with random effects, called the mixed
MoE (MMoE), as a candidate regression model to cater for multilevel data structure. The justiﬁcation of the
proposed model, which analyzes the ability of the MMoE to accurately approximate the generalized form
of the mixed effect model (Equation (3)), will be presented in the next section.
Under the MMoE, we assume that each observation iis equipped by Llevels of random effects, denoted
by anL-vectorwi= (wi1;:::;wiL). Similar to the mapping of the unobserved factors iintroduced in
Section 1, we also have wil=wi0l:=w(s)
lifcl(i) =cl(i0) =s. The only difference between iandwiis
that we restrict wil2Rinto a Euclidean space instead of a general space ~, which is unknown and hard to
specify. Similar to ~, we also deﬁne w=fw(s)
lgl=1;:::;L;s=1;:::;S las the random effects across all levels and
factors.
The distribution function of yiconditional on xiandwiis given by
F(yi;;; ;gjxi;wi) =gX
j=1j(xi;wi;;)F0(yi; j); (6)
wheregis the number of latent classes, j(xi;wi;;)is the mixing weight for the jthclass (called the
gating function), =fj0;j:j= 1;:::;gg2A are the regression parameters of the gating function,
=fj:j= 1;:::;gg2B are the coefﬁcients of the random effects and  =f j:j= 1;:::;gg2	
6

--- PAGE 7 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
Figure 2: Model structure of the MMoE.
are the parameters of a pre-speciﬁed multivariate distribution F0(called the expert function). We also specify
j(xi;wi;;)as a logit linear gating function, given by
j(xi;wi;;) =expfj0+T
jxi+T
jwigPg
j0=1expfj00+T
j0xi+T
j0wig; j = 1;2;:::;g: (7)
Apart from that, the random effects fw(s)
lgl=1;:::;L;s=1;:::;S lare assumed to be independent across lands,
andw(s)
lfollows a ﬁxed pre-speciﬁed distribution lwith no extra parameters in it. Based on the above
model speciﬁcation, the joint distribution of ygivenxis
~F(y;x) := ~F(y;;;	;gjx) =ZNY
i=1F(yi;;;	jxi;wi)d(w); (8)
where is the joint distribution of w, given by
(w) =LY
l=1SlY
s=1l(w(s)
l) ord(w) =LY
l=1SlY
s=1l(dw(s)
l): (9)
The model can be interpreted as follows with an illustration displayed in Figure 2. Each observation is
assigned into one of the ghomogeneous subgroups with classiﬁcation probabilities equal to the gating
functionsj(xi;wi;;). The classiﬁcation probabilities vary among observations as they depend on both
the inputsxiand the unobserved factors wi. Conditioned on the subgroup observation ibelongs to, the
outputsyiare governed by a homogeneous probability distribution F0(yi; j)independent of xiandwi.
7

--- PAGE 8 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
One of the noticeable difference between the above model and the standard structure of MoE (i.e., Equation
(1)) is that here we remove the regression relationship on the expert functions (i.e., the expert functions do
not depend on the inputs xi), which is considered as a reduced MoE (RMoE) by Fung et al. [2019a]. Also,
note that our model assumes that the level- lrandom effect wilis the same across all ggating functions (i.e.,
wildoes not depend on j). This assumption is different from Ng and McLachlan [2007], who considers
multiple different independent level- lrandom effects across the gating functions.
Remark 1. The proposed reduced modelling framework may be advantageous in two ways. Firstly, we may
select among a wide range of probability distributions as the expert function F0, including more complex
non-exponential classes of distributions (e.g., phase-type distributions) where regression modelling on these
distributions may be either infeasible or computationally challenging. Secondly, simpliﬁed model structure
is favourable for interpretation, as our proposed model enables clustering of observations into homogeneous
subgroups, and explains the variability of each level- lfactor by a single source (i.e., wil2R) instead of
multiple sources by Ng and McLachlan [2007].
The remaining issue is: how does such a reduced structure affect its model ﬂexibility? To justify the pro-
posed modelling structure, we will demonstrate the denseness property of our proposed model in the follow-
ing section, which means that the MMoE model structure of Equation (8) can approximate any generalized
form of mixed effect models expressed by Equation (3). This will provide evidences suggesting that our
proposed model is parsimonious. In other words, the MMoE has the simplest structure without harming its
representation capability.
4 Denseness theory
This section studies the approximation capability of the class of MMoE models. Our goal is to show that
the proposed MMoE is versatile enough to approximate any mixed effects models under mild regularity
conditions, even if the MMoE is constructed in a reduced form: (i) the gating function is restricted to be
a logit linear gating; (ii) regression link is removed in the expert functions; (iii) the random effects are
restricted to follow some ﬁxed pre-determined distributions. Before that, we need to technically formulate
a class of mixed effects models and deﬁne “denseness” for mixed effects models. These deﬁnitions are the
extensions of Fung et al. [2019a], who deﬁnes “regression distributions” and “denseness” in the regression
settings without considering random effects.
DenoteT:=T1:::TLas a collection of some spaces of :=1:::L,Has a collection
of some distribution functions Hon(yijxi;i), andGlas a collection of probability measures Glon(s)
l
withG:=G1:::GL. Also, letCbe a set containing all possible mappings c(), and deﬁne a vector
S= (S1;:::;SL)withS=NL. “A class of mixed effects models” and “mixed effects distributions” are
ﬁrst deﬁned as follows:
Deﬁnition 1. A class of mixed effects models ML(X;T;H;G) :=f~H(;X;;H;G ) :l2
Tl;H2 H;Gl2 Gl;l= 1;:::;Lgis a collection of mixed effects distributions ~H(;X;;H;G ),
where each mixed effects distribution ~H(;X;;H;G ) :=f~H(yjx) := ~H(yjx;;H;G ) =R
~hQN
i=1H(yijxi;i)i
dG(~) :xi2X;i2f1;:::;Ng;N2N;S2S;c2Cg is itself a collection of
joint probability distributions.
8

--- PAGE 9 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
In the spirit of Fung et al. [2019a], denseness is deﬁned in the sense of weak convergence of probability
distributions. Therefore, before deﬁning denseness, we need to deﬁne weak convergence of mixed effects
distributions as follows:
Deﬁnition 2. Consider a sequence of mixed effects distributions ~H(n):=~H(;X;(n);H(n);G(n))and a
target mixed effects distribution ~H:=~H(;X;;H;G ). We say thatf~H(n)gn=1;2;:::weakly converge to
~Hif and only if for every given xi2X (for alli2f1;:::;Ng),N2N,S2S andc2C, we have
~H(jx;(n);H(n);G(n))D  !~H(jx;;H;G )asn!1 , whereD  !represents a weak convergence or
convergence in distribution. If the distributional convergence is uniform across any compact input space
X X , i.e. ~H(yjx;(n);H(n);G(n))!~H(yjx;;H;G )uniformly on (y;x)withxi2X(for all
i2f1;:::;Ng), then we say that f~H(n)gn=1;2;:::weakly converge to ~Hcompactly.
Now, we are able to extend the formalism of Fung et al. [2019a] and deﬁne denseness in the settings of
mixed effects models:
Deﬁnition 3. Consider two classes of mixed effects models M1L:=ML(X;T1;H1;G1)andM2L:=
ML(X;T2;H2;G2).M1Lis dense inM2Lif and only if for every (2;H2;G2)2 T 2H 2G 2,
there exists a sequence of f((n)
1;H(n)
1;G(n)
1)gn=1;2;:::with ((n)
1;H(n)
1;G(n)
1)2 T 1H 1G 1such
that the mixed effects distributions f~H(n)
1:=~H(;X;(n)
1;H(n)
1;G(n)
1)gn=1;2;:::weakly converge to
~H2:=~H(;X;2;H2;G2). Iff~H(n)
1gn=1;2;:::weakly converge to ~H2compactly, thenM1Lis said to
be compactly dense in M2L.
The above deﬁnition of denseness means that any mixed effects models in the class M2Lcan be represented
or approximated arbitrarily well by the models within another class M1L, in the sense of weak convergence
of joint distributions ~H(yjx)across allNobservations. Less technically speaking, M1Lcan be interpreted
as a model class that is at least as rich or ﬂexible as the class M2L.
With all relevant deﬁnitions constructed, we now consider a class of generalized mixed effects models
Mgen
L(X) :=ML(X;Tgen;Hgen;Ggen)expressed in the form of Equation (3), where Tgen(also denoted as
Tgen:=Tgen
1:::Tgen
L),HgenandGgenall correspond to collections of any spaces or functions satisfying
the following two mild technical assumptions:
Assumption 2. Each space l2Tgen
lis equipped by a complete separable metric dl.
Assumption 3. For every probability distribution functions H2Hgen,H(yijxi;i)is continuous with
respect to (yi;xi;i).
Now, we turn to the class of MMoE with a pre-determined choice of expert function F0and
joint distribution of random effects . We express the class of MMoE as MMMoE
L(X;F0;) :=
ML(X;TMMoE;HMMoE;GMMoE), where the two sets TMMoE
l=fRgandGMMoE=fgonly contain a
single element each. Also, the set of mixed effects distributions is given by HMMoE=fF(;;; ;gj;) :
2A;2B; 2	;g2Ng, whereFis given by the form of Equation (6). We also impose the
following two restrictions on the choices of F0and, which are essential for the denseness property of the
class of MMoE models:
Assumption 4. F0satisﬁes the denseness condition outlined by Proposition 3.1 of Fung et al. [2019a],
meaning that for every q2RK, there exists a sequence of parameters f (n)(q)gn=1;2;:::such that
F0(; (n)(q))D  !qasn  !1 .
Assumption 5. lis a continuous distribution function for every l= 1;:::;L .
9

--- PAGE 10 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
The above two assumptions are not necessarily mild. As discussed in Fung et al. [2019a], some common
distributions, such as Pareto and exponential distributions, do not satisfy the denseness condition under
Assumption 4. Moreover, Assumption 5 does not hold whenever we choose any discrete distributions for
l. Nonetheless, note that the expert function F0and random effect distribution lare both pre-determined,
so we have the control to choose suitable functions that fulﬁll Assumptions 4 and 5 before modelling a
multilevel dataset via the MMoE. For example, one may choose Gamma, Weibull, log-normal, or inverse-
Burr distributions as the expert function F0(Fung et al. [2019a]), and select a normal distribution as the
random effect distribution l.
We now present the main result which justiﬁes the representation capability of the proposed class of MMoE
models. The proof appears in the Appendix.
Theorem 1. Suppose that Assumptions 1 to 5 are satisﬁed. Then, MMMoE
L(X;F0;)is compactly dense in
Mgen
L(X).
Remark 2. The theorem above requires that the target distribution His a continuous distribution with
respect toyi(see Assumption 3). However, it is also important to investigate into the approximation results
for discrete distributions (see e.g. Jiang and Tanner [1999b] and Fung et al. [2019a]). As discussed by
Norets et al. [2010], any discrete distribution can be represented by a continuous latent distribution. Then,
it is obvious that Theorem 1 still holds for discrete distributions if the denseness condition in Assumption 4
is changed from q2RKtoq2NK.
Clearly, the denseness property provides a theoretical justiﬁcation for the ﬂexibility of the MMoE to simul-
taneously capture various model characteristics, including the joint distributions (e.g., multimodal distribu-
tions and dependence among outputs), the regression patterns (e.g., non-linear links and interactions among
covariates), the random intercept (e.g., peculiar effects of the unobserved factors to the outputs) and the ran-
dom slopes (e.g., interactions between covariates and random effects). As a result, the denseness property
theoretically demonstrates the ability of the MMoE to potentially ﬁt and resemble well any multilevel data
which can be generated among a rich class of mixed effects models. Also, since our proposed model has a
reduced structure (explained in the previous section), the denseness property also suggests that the proposed
model is parsimonious.
5 Nested mixed effects models for hierarchical data
Nested mixed effect model is a special case of the model described in Section 2 with multiple random effects.
Speciﬁcally, the Lrandom effects are levelled in a way that the ﬁrst and L-th random effects correspond
to the highest and the lowest levels respectively. Any observations sharing the same lower level random
effect unit must also have the same upper level unit. If the random effects are not nested, the corresponding
mixed effect model is called a “crossed” mixed effect model. The school problem (see, e.g., Aitkin and
Longford [1986]) is a classical example where the random effects are nested. In this example, there are
two factors affecting a student’s performance: School and classroom. Since classmates must come from the
same school, we have L= 2 with “school” and “classroom” respectively being the ﬁrst and second levels
of random effects.
Before deﬁning a nested model, it is more convenient to adopt an alternative set of notations to identify the
observations. Denote i= (i1;i2;:::;iL+1)as an identiﬁer of an observation, and il= (i1;i2;:::;il)as
an identiﬁer up to level l. Here,i1is a label of the level- 1factor unit. For l= 2;3;:::;L ,ilis a label
10

--- PAGE 11 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
of the level-lfactor unit given that the labels of the ﬁrst (l 1)factors levels are il 1.iL+1represents the
observation label given that the Lfactors are labelled by iL. For example, in the school problem with L= 2,
i= (2;3;5)corresponds to the ﬁfth student of the third classroom of the second school.
Furthermore, denote N0as the number of level- 1factor units, so that the support of i1is given byI1:=
f1;:::;N 0g. Forl= 2;3;:::;L , deﬁneNil 1as the number of level- lfactor units where the ﬁrst (l 1)
factor levels are labelled as il 1, such that the support of ilisIl:=fil:il 12Il 1;il= 1;:::;Nil 1g.
Similarly,NiLis the number of observations having iLas the label of the Lfactors. Then, the support of
iisI:=fi:iL2IL;iL+1= 1;:::;NiLg. Also, the total number of observations is given by N=
PN0
i1=1PNi1
i2=1PNiL 1
iL=1NiL. A tree diagram in Figure 3 visualizes the structure of nested hierarchical
data.
As a result, the nested multilevel dataset is given by (y;x) :=f(yi;xi)gi2I, wherexi2X  RPand
yi2Y RKare respectively the input and output of an observation labelled as i2I.
In this section, we will deﬁne the generalized class of nested mixed effects models, formulate the proposed
MMoE model with nested random effects, and construct the denseness theory for the class of nested MMoE.
5.1 Generalized nested mixed effect model
Similar to the MMoE deﬁned in Section 2, under the nested MMoE, the response yidepends on its covariates
xiandLlevels of latent random factors i1;:::;iLillustrated in Figure 3. The dependence assumption
among the latent factors is stated as follows:
Assumption 6. Conditioned on i1;:::;iL, theNobservations are independent. The set of parents of il
is given by pa (il) = (i1;:::;i(l 1))forl= 1;:::;L .
In other words, the lower level factor only depends directly on its corresponding upper level factors. Under
a nested hierarchical data structure, we are able to relax the independence assumption in Assumption 1
by allowing the dependence of lower level random effects on their parents (upper level effects). For l=
1;:::;L , we also denote Glas the distribution of the level- lfactor conditioned on pa (il).
The joint distribution of ygivenxis given by
~H(yjx) =Z
~"Y
i2IH(yi;xiji)#
dG(~) (10)
with
G(~) =N0Y
i1=1G1(i1)Ni1Y
i2=1G2(i2ji1)NiL 1Y
iL=1GL(iLji1;:::;iL 1); (11)
where ~=fil:il2Il;l= 1;:::;Lg.
5.2 Nested mixed mixture of experts model
Analogous to the MMoE introduced in Section 3, we construct a nested MMoE for hierarchical data.
Denotewi= (wi1;:::;wiL)2RLas theLlevels of random effects of observation i. Also, deﬁne
w=fwilgil2Il;l=1;:::;L as all the random effects aggregated across all observations. With a slight change
11

--- PAGE 12 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
Figure 3: Hierarchical data structure and modelling framework.
of notations from Equations (6) and (7),the distribution function of yiconditional on xiandwiis then
F(yi;;; ;gjxi;wi) =gX
j=1j(xi;wi;;)F0(yi; j); (12)
where the gating function given by
j(xi;wi;;) =expfj0+T
jxi+T
jwigPg
j0=1expfj00+T
j0xi+T
j0wig; j = 1;2;:::;g: (13)
Similar to Section 3, the random effects fwilgl=1;:::;L;il=1;:::;N il 1are constructed to be independent across
landilwithwill. This construction is simpliﬁed from Assumption 6 of the generalized nested mixed
models where the random effects may depend on their parents. Adapting from Equations (8) and (9), the
joint distribution of ygivenxis
~F(y;x) := ~F(y;;;	;gjx) =ZY
i2IF(yi;;;	jxi;wi)d(w); (14)
where is the joint distribution of wgiven by
(w) =LY
l=1Y
il2Ill(wil) ord(w) =LY
l=1Y
il2Ill(dwil): (15)
From Equation (15) above, the random effects are still assumed to be independent under the nested MMoE.
However, we will show in the following subsection that such a speciﬁcation sufﬁces to approximate the
dependence of lower level random effects on their parents under a hierarchical data structure.
5.3 Denseness theory for nested MMoE
Analogous to Section 4, it is desirable to develop an approximation theory for the nested
MMoE in the space of the generalized nested mixed effect models. Denote N =
12

--- PAGE 13 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
(N0;fNi1gi12I1;fNi2gi22I2;:::;fNiLgiL2IL)as the number of factors belonging to each parent
factors for each level with N02NandNil2Nforl= 1;:::;L , andNcontains all combinations of
possibleN. Other notations, unless speciﬁed otherwise, are consistent to those deﬁned by Section 4. The
equivalent deﬁnitions analogous to Section 4 for hierarchical data structure are listed as follows:
Deﬁnition 4. A class of nested mixed effects models ML(X;T;H;G) :=f~H(;X;;H;G ) :l2
Tl;H2H;Gl2Gl;l= 1;:::;Lgis a collection of nested mixed effects distributions ~H(;X;;H;G ),
where each nested mixed effects distribution ~H(;X;;H;G ) :=f~H(yjx) := ~H(yjx;;H;G ) =R
~Q
i2IH(yijxi;i)
dG(~) :xi2X;i2I;N2Ng is itself a collection of joint probability dis-
tributions.
Deﬁnition 5. Consider a sequence of nested mixed effects distributions ~H(n):=~H(;X;(n);H(n);G(n))
and a target nested mixed effects distribution ~H:=~H(;X;;H;G ). We say thatf~H(n)gn=1;2;:::
weakly converge to ~Hif and only if for every given xi2 X (for alli2 I ),N2 N , we have
~H(jx;(n);H(n);G(n))D  !~H(jx;;H;G )asn!1 , whereD  !represents a weak convergence or
convergence in distribution. If the distributional convergence is uniform across any compact input space
X X , i.e. ~H(yjx;(n);H(n);G(n))!~H(yjx;;H;G )uniformly on (y;x)withxi2X(for all
i2f1;:::;Ng), then we say that f~H(n)gn=1;2;:::weakly converge to ~Hcompactly.
The denseness deﬁnition for hierarchical data structure (nested mixed effects models) is exactly the same
as Deﬁnition 3. Deﬁne Mgen
L(X)as the class of generalized nested mixed effects models expressed in
Equation (10), subject to Assumptions 2 and 3. Also denote MMMoE
L(X;F0;)as the class of nested
MMoE given by Equation (14). We have the following denseness theorem:
Theorem 2. Suppose that Assumptions 2 to 6 hold. Then, MMMoE
L(X;F0;)is compactly dense in
Mgen
L(X).
The proof is leveraged to Appendix B. Theorem 2 suggests that the nested MMoE has a potential to ap-
proximate any generalized nested mixed effect models arbitrarily accurately, even if the random effects are
restricted to be independent under the nested MMoE while the random effects under the generalized nested
mixed effect models can be dependent (Assumption 6). In contrast, under Theorem 1, the MMoE can only
approximate mixed effect models with independent random effects (Assumption 1). Therefore, given that
the data structure is hierarchical, Theorem 2 is a stronger theoretical result than Theorem 1.
6 Numerical Illustration
In this section, we present a numerical example to demonstrate how the proposed MMoE model can approxi-
mate any mixed effects model. For illustration purposes, all variables in this section will be one-dimensional,
i.e.yifor the response, 1for the general random effect, and w1for the random effect used in MMoE. We
assume there are no covariates xifor reasons to be explained later. In the following, we ﬁrst demonstrate
how to construct an MMoE such that the conditional distribution of yigiven1can be approximated arbi-
trarily well by that of yigivenw1for a single observation yi. Then, we discuss how this is related to the
denseness property of MMoE in Theorem 1.
We ﬁrst construct the true model as follows. Consider ~=1= [ 2;2]as the space for 1. The general
random effect 1is assumed to follow Uniform ( 2;2)with distribution function G1(1) = (2 +1)=4and
densityg1(1) = 1=4. We assume yij1Normal (1;1)with density h(yij1) =1p
2e 1
2(yi 1)2.
13

--- PAGE 14 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
We then aim to use MMoE to approximate the true model mentioned above. For MMoE, we choose W1=R
as the space for w1. The random effect w1in MMoE is assumed to be standard normal with distribution
function 1()and density 1(). Denotef(yijw1)as the conditional density of yigivenw1. We aim to
constructf(yijw1)such that the conditional distribution h(yij1),8121, can be approximated arbitrarily
well byf(yijw1),9w12W 1.
We start by discretizing both 1andW1intoD1pairs of subspaces f1;d1gd1=1;:::;D 1and
fW1;d1gd1=1;:::;D 1such thatG1(1;d1) =  1(W1;d1)ford1= 1;2;:::;D 1. For each 1;d1, we pick

1;d121;d1such thath(y1j
1;d1)is a reasonable approximation of h(y1j121;d1). Then, each
h(y1j
1;d1)is approximated by the following ﬁnite mixture
f(yijw1) =D1X
d1=1(u)
d1(w1)h(y1j
1;d1) (16)
where the mixing weights are given by
(u)
d1(w1) =expfu(~d1;0+~d1;1w1)gPD1
d0
1=1expfu(~d0
1;0+~d0
1;1w1)g; d 1= 1;2;:::;D 1: (17)
Note Equation (16) is not yet the formulation of MMoE, since each h(yij
1;d1)is still dependent on 
1;d1.
Following the same idea in Fung et al. [2019a], Appendix A shows that h(yij
1;d1)can be further approxi-
mated by a ﬁnite mixture of expert functions with ﬁxed parameters independent of the random effects, which
ultimately leads to an MMoE similar to Equation (6). We will omit such approximation procedures and work
with Equation (16) to avoid further complications which are not relevant to random effects. Meanwhile, the
approximation procedures in the presence of covariates xiare also similar to those presented in Fung et al.
[2019a] and are therefore deferred to Appendix A, which is why we have presented an example without
covariates for conciseness.
Apart fromh(yij
1;d1), the mixing weights (u)
d1(w1)in Equation (17) contains an additional control param-
eterucompared with MMoE. When the coefﬁcients f(~d1;0;~d1;1)gd1=1;2;:::;D 1are carefully constructed,
(u)
d1(w1)!1fw12W 1;d1gasu!1 ford1= 1;2;:::;D 1. Consequently, the ﬁnite mixture in Equation
(16) degenerates to h(yij
1;d1)wheneverw12W 1;d1andu!1 .
To summarize, given 1;d1, we ﬁrst approximate h(yij121;d1)byh(yij
1;d1)for some
1;d12
1;d1. Then,h(yij
1;d1)is further approximated by f(yijw12 W 1;d1), which is done for all pairs of
(1;d1;W1;d1). In effect, the subspace W1;d1of random effects w1in MMoE becomes specialised in ap-
proximating the subspace 1;d1of some general random effects 1. As the partition of (1;d1;W1;d1)
becomes ﬁner and ﬁner, we can ﬁnd a mapping 1!W 1such that the conditional distribution h(yij1)
for any121can be approximated by f(yijw1)for somew12W 1.
As a concrete example, Table 1 illustrates the above procedure with D1= 5 partitions for 1andW1,
where we choose the midpoint 
1;d1as the representative value for the subspace 1;d1. The values of
f(~d1;0;~d1;1)gd1=1;2;:::;D 1are chosen according to Lemma 3.1 in Fung et al. [2019a] to ensure the con-
vergence of mixing weights (u)
d1(w1)to the degenerate indicators 1fw12W 1;d1g. This example is il-
lustrated by the second row in Figure 4, where the each vertical slice of the plot shows the conditional
distribution of f(yijw1)(blue) foru= 1;10;100 and1000 from left to right, and h(yij121;d1)
14

--- PAGE 15 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
(green) ford1= 1;2;:::; 5. Indeed, we observe each f(yijw12W 1;d1)approximates the correspond-
ingh(yij121;d1)better asuincreases. The third and fourth row of Figure 4 show the same sets of plots,
but forD1= 10 andD1= 100 partitions for both 1andW1. However, if the number of partition is too
small, the MMoE will underestimate the variance of yigivenw1compared with the variance of yigiven1
under the true model, even if u!1 , as shown in the ﬁrst row of Figure 4 with D1= 2. In the limiting
case where the partition becomes inﬁnitely ﬁne, we can ﬁnd a unique f(yijw1)to approximate h(yij1)for
all121. For example, the conditional distribution h(yij1= 1)in the original mixed effects model
will be eventually be approximated by f(yijw1= 0:67)in MMoE.
Finally, once the conditional distribution h(yij1)can be approximated by f(yijw1), the denseness prop-
erty in Theorem 1 naturally follows. More speciﬁcally, given the conditional independence assumption in
Equation (2), the conditional joint distribution ofQN
i=1h(yiji)can also be approximated arbitrarily well
byQN
i=1f(yijwi), whereiandwifor each observation yiare speciﬁed by 1,w1and the mapping func-
tionc1(i). In the case of ﬁnite partition of subspaces f1;d1gd1=1;:::;D 1andfW1;d1gd1=1;:::;D 1such that
G1(1;d1) =  1(W1;d1)ford1= 1;2;:::;D 1, the marginal joint distribution of all yi’s are also approxi-
mated arbitrarily well, which is evident from
f(Y) =D1X
d1=1"NY
i=1h(yiji)1fi21;d1g#
G1(1;d1) (18)
and
g(Y) =D1X
d1=1"NY
i=1f(yijwi)1fwi2W 1;d1g#
1(W1;d1): (19)
In the limiting scenario where the partition of 1andW1becomes inﬁnitely ﬁne, the above equations
recover the integral forms in Equation (4) and (8), thus demonstrating the denseness property of MMoE in
Theorem 1.
7 Discussions
In this paper, we introduce a class of the mixed mixture of experts models (MMoE) for multilevel regression
data. We prove that the MMoE is dense in the space of generalized mixed effects models, which is a rich
class containing almost all models in the literature having independent random effects, in the sense of weak
convergence. We further study a special case where the data is hierarchical in structure. In this case, the
proposed nested MMoE is shown to be dense in the space of generalized nested mixed effects models where
the random effects can possibly be dependent. The two denseness theorems justify the versatility of the
MMoE in catering for a broad range of multilevel data characteristics, including the marginal distribution,
dependence, regression link, random intercept and random slope.
This paper aims to prove the most general results imposing minimal assumptions. The only practical restric-
tion is that the expert function F0(yi; j)in Equation (6) needs to approximate any degenerate distributions
(Assumption 4). This assumption is much weaker than those applied to the existing approximation theorems
(see, e.g, Norets and Pelenis [2014] and Nguyen et al. [2019]), which require that the expert density function
is a scalable symmetric function (Equation (3.1) of Norets and Pelenis [2014]), and that the target density
function does not change abruptly w.r.t. yiandxi(Equation (3.2) of Norets and Pelenis [2014]). On the
other hand, there are several limitations of the denseness theorems formulated in this paper. Firstly, weak
15

--- PAGE 16 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
Table 1: Example of approximation with ﬁve partitions of 1andW1.
d1 1 2 3 4 5
1;d1 [ 2:00; 1:20] ( 1:20; 0:40] ( 0:40;0:40] (0:40;1:20] (1:20;2:00]
W1;d1 ( 1; 0:84] ( 0:84; 0:25] ( 0:25;0:25] (0:25;0:84] (0:84;+1)

1;d1 1:60 0:80 0 :00 0 :80 1 :60
~d1;0 0:00 0 :23 0 :29 0 :23 0 :00
~d1;1 0:00 0 :25 0 :50 0 :75 1 :00
Figure 4: Illustration of using f(yijw1)(blue) to approximate h(yij121;d1)(green) for different num-
bers of partitions of 1andW1. Each vertical slice corresponds to the conditional density of f(yijw1)or
h(yij121;d1). From left to right, the control parameter uranges from 1, 10, 100 to 1000 in the blue
plots, and eventually f(yijw1) =h(yij121;d1)for allw12W 1;d1asu!1 . From top to bottom, the
number of partition D1ranges from 2, 5, 10 to 100. When the partition becomes inﬁnitely ﬁne and u!1 ,
eachh(yij1)is approximated arbitrarily well by some f(yijw1), e.g.h(yij1= 1) =f(yijw1= 0:67).
16

--- PAGE 17 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
convergence does not guarantee the approximation capability in terms of moments (e.g., the mean function
studied by Nguyen et al. [2016]) or some distance metrics (e.g., the KL divergence studied by Jiang and
Tanner [1999a], Norets et al. [2010] and Nguyen et al. [2019]). To establish the denseness theorems w.r.t.
moments and other distance metrics, one needs to further assume that the moments of the MMoE expert
functions and the target distributions are ﬁnite, and that the conditions indicated by Equations (3.1) and
(3.2) of Norets and Pelenis [2014] are fulﬁlled. Secondly, as described in Section 3.4 of Fung et al. [2019a],
the denseness theorems do not provide a convergence rate, so there is no control on the mixture components
gto approximate any generalized mixed effects distributions at a desired level of accuracy. To establish
the rate results, one needs to impose further assumptions on the target distribution ~H(yjx)in Equation (4)
and the MMoE distribution ~F(y;x)in Equation (8). We leave these technical establishments as a future
research direction.
8 Acknowledgements
This research did not receive any speciﬁc grant from funding agencies in the public, commercial, or not-for-
proﬁt sectors.
References
M. Aitkin and N. Longford. Statistical modelling issues in school effectiveness studies. Journal of the Royal
Statistical Society: Series A (General) , 149(1):1–26, 1986.
B. Bakker and T. Heskes. Task clustering and gating for bayesian multitask learning. Journal of Machine
Learning Research , 4(May):83–99, 2003.
J.-P. Boucher and M. Denuit. Fixed versus random effects in Poisson regression models for claim counts: A
case study with motor insurance. ASTIN Bulletin: The Journal of the IAA , 36(1):285–301, 2006.
L. Breuer and D. Baum. An introduction to queueing theory - and matrix-analytic methods . Springer Science
& Business Media, 2005.
F. Chamroukhi. Robust mixture of experts modeling using the t distribution. Neural Networks , 79:20–36,
2016.
F. Chamroukhi. Skew t mixture of experts. Neurocomputing , 266:390–408, 2017.
M. Davidian and A. R. Gallant. The nonlinear mixed effects model with a smooth random effects density.
Biometrika , 80(3):475–488, 1993.
E. W. Frees and J.-S. Kim. Multilevel model prediction. Psychometrika , 71(1):79–104, 2006.
T. C. Fung, A. L. Badescu, and X. S. Lin. A class of mixture of experts models for general insurance:
Theoretical developments. Insurance: Mathematics and Economics , 89:111–127, 2019a.
T. C. Fung, A. L. Badescu, and X. S. Lin. A class of mixture of experts models for general insurance:
Application to correlated claim frequencies. ASTIN Bulletin: The Journal of the IAA , 49(3):647–688,
2019b.
T. C. Fung, A. L. Badescu, and X. S. Lin. A new class of severity regression models with an application to
IBNR prediction. North American Actuarial Journal , 25(2):206–231, 2021.
J. Geweke and M. Keane. Smoothly mixing regressions. Journal of Econometrics , 138(1):252–290, 2007.
17

--- PAGE 18 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
H. Goldstein. Multilevel mixed linear model analysis using iterative generalized least squares. Biometrika ,
73(1):43–56, 1986.
H. Goldstein. Multilevel statistical models , volume 922. John Wiley & Sons, 2011.
T. G. Gregoire and O. Schabenberger. A non-linear mixed-effects model to predict cumulative bole volume
of standing trees. Journal of Applied Statistics , 23(2-3):257–272, 1996.
B. Grun and F. Leisch. Flexmix version 2: Finite mixtures with concomitant variables and varying and
constant parameters. Journal of Statistical Software , 28(4):1–35, 2008.
S. Ingrassia, S. C. Minotti, and G. Vittadini. Local statistical modeling via a cluster-weighted approach with
elliptical distributions. Journal of classiﬁcation , 29(3):363–401, 2012.
R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural
computation , 3(1):79–87, 1991.
W. Jiang and M. A. Tanner. On the approximation rate of hierarchical mixtures-of-experts for generalized
linear models. Neural computation , 11(5):1183–1198, 1999a.
W. Jiang and M. A. Tanner. Hierarchical mixtures-of-experts for exponential family regression models:
approximation and maximum likelihood estimation. Annals of Statistics , pages 987–1011, 1999b.
M. I. Jordan and R. A. Jacobs. Hierarchies of adaptive experts. In Advances in neural information processing
systems , pages 985–992, 1992.
M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the EM algorithm. Neural computation ,
6(2):181–214, 1994.
S. Masoudnia and R. Ebrahimpour. Mixture of experts: a literature survey. Artiﬁcial Intelligence Review ,
42(2):275–293, 2014.
C. McGilchrist. Estimation in generalized mixed models. Journal of the Royal Statistical Society: Series B
(Methodological) , 56(1):61–69, 1994.
G. J. McLachlan and D. Peel. Finite mixture models . John Wiley & Sons, 2000.
E. F. Mendes and W. Jiang. On convergence rates of mixtures of polynomial experts. Neural computation ,
24(11):3025–3051, 2012.
G. Molenberghs, G. Verbeke, C. G. Dem ´etrio, and A. M. Vieira. A family of generalized linear models for
repeated measures with normal and conjugate random effects. Statistical science , 25(3):325–347, 2010.
S.-K. Ng and G. J. McLachlan. Extension of mixture-of-experts networks for binary classiﬁcation of hier-
archical data. Artiﬁcial Intelligence in Medicine , 41(1):57–67, 2007.
S.-K. Ng and G. J. McLachlan. Mixture models for clustering multilevel growth trajectories. Computational
Statistics & Data Analysis , 71:43–51, 2014.
S.-K. Ng, G. McLachlan, K. K. Yau, and A. H. Lee. Modelling the distribution of ischaemic stroke-speciﬁc
survival time using an em-based mixture approach with random effects adjustment. Statistics in Medicine ,
23(17):2729–2744, 2004.
S.-K. Ng, G. J. McLachlan, K. Wang, L. Ben-Tovim Jones, and S.-W. Ng. A mixture model with random-
effects components for clustering correlated gene-expression proﬁles. Bioinformatics , 22(14):1745–1752,
2006.
18

--- PAGE 19 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
H. D. Nguyen and F. Chamroukhi. Practical and theoretical aspects of mixture-of-experts modeling: An
overview. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery , 8(4):e1246, 2018.
H. D. Nguyen and G. J. McLachlan. Laplace mixture of linear experts. Computational Statistics & Data
Analysis , 93:177–191, 2016.
H. D. Nguyen, L. R. Lloyd-Jones, and G. J. McLachlan. A universal approximation theorem for mixture-
of-experts models. Neural computation , 28(12):2585–2593, 2016.
H. D. Nguyen, F. Chamroukhi, and F. Forbes. Approximation results regarding the multiple-output gaussian
gated mixture of linear experts model. Neurocomputing , 366:208–214, 2019.
H. D. Nguyen, T. Nguyen, F. Chamroukhi, and G. J. McLachlan. Approximations of conditional probability
density functions in lebesgue spaces via mixture of experts models. Journal of Statistical Distributions
and Applications , 8(1):1–15, 2021.
A. Norets and J. Pelenis. Posterior consistency in conditional density estimation by covariate dependent
mixtures. Econometric Theory , 30(3):606–646, 2014.
A. Norets et al. Approximation of conditional densities by smooth mixtures of regressions. The Annals of
statistics , 38(3):1733–1766, 2010.
H. Tijms. Stochastic Models: An Algorithm Approach . John Wiley, 1994.
S. C. Tseung, T. C. Fung, I. W. Chan, A. L. Badescu, and X. S. Lin. A posteriori risk classiﬁcation and
ratemaking with random effects in the mixture-of-experts model. Working paper, 2022.
L. Xu, M. I. Jordan, and G. E. Hinton. An alternative model for mixtures of experts. Advances in neural
information processing systems , pages 633–640, 1995.
K. K. Yau, A. H. Lee, and A. S. Ng. Finite mixture regression model with random effects: application to
neonatal hospital length of stay. Computational statistics & data analysis , 41(3-4):359–366, 2003.
S. E. Yuksel, J. N. Wilson, and P. D. Gader. Twenty years of mixture of experts. IEEE transactions on
neural networks and learning systems , 23(8):1177–1193, 2012.
A. J. Zeevi, R. Meir, and V . Maiorov. Error bounds for functional approximation and estimation using
mixtures of experts. IEEE Transactions on Information Theory , 44(3):1010–1025, 1998.
Appendix A Proof of Theorem 1
We ﬁrst introduce the following functions, where the detailed notations would be explained later in the
proof.
~H(yjx) =Z
~"NY
i=1H(yijxi;i)#
dG(~); (20)
~H(yjx) =X
~d2~DNY
i=1H(yijxi;
d(c(i)))G(~~d); (21)
~F(u)(yjx) =ZNY
i=1F(u)(yijxi;wi)d(w) (22)
19

--- PAGE 20 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
withF(u)(yijxi;wi) =P
d2D+(u)
d(wi)H(yijxi;
d),
~F(M;Q;t;u )(yjx) =ZNY
i=1F(M;Q;t;u )(yijxi;wi)d(w) (23)
withF(M;Q;t;u )(yijxi;wi) =P
m2MP
q2QP
d2D+(t;u)
j(xi;wi;~Q
q;~d)1fyibycM
mg,
~F(M;Q;t;u;v )(yjx) =ZNY
i=1F(M;Q;t;u;v )(yijxi;wi)d(w) (24)
withF(M;Q;t;u;v )(yijxi;wi) =P
m2MP
q2QP
d2D+(t;u)
j(xi;wi;~Q
q;~d)F0(yi; M(v)
m). Note that
Equation (20) is in the form of the generalized mixed effects models under Equation (4), while Equation
(24) is in the MMoE framework under Equation (8). The main proof idea is to bound the approximation
errors between any two consecutive functions from Equations (20) to (24):
A.1 Step 1: Approximating Equation (20) by Equation (21)
As the metric space (l;dl)is complete separable (Assumption 2), the probability measure Glon(s)
l
is tight, i.e.81>0,9llcompact such that Gl(l) :=P((s)
l2l)1 1. Since l
is compact, for any 1>0we can ﬁnd subspaces fl;dlgdl=1;:::;D l(dlis called the subspace index) and
pointsf
l;dlgdl=1;:::;D lsuch that[dl=1;:::;D ll;dl=l,
l;dl2l;dlfor everydl= 1;:::;Dlandl;dlis
covered by the ball with radius 1=Lcentered at
l;dl. Due to the uniform continuity of Hw.r.t.ionl
(Assumption 3 implies uniform convergence in compact space), for any 2>0we can choose sufﬁciently
small1with the aforementioned subspace partitioning such that jH(yijxi;i) H(yijxi;
d(c(i)))j2
ifil2l;d(c(i))
lfor everyl= 1;:::;L , where
dc(i)=f
1;d(c(i))
lgl=1;:::;L andd(c(i))=fd(c(i))
lgl=1;:::;L
withd(c(i))
l2 f1;:::;Dlg. Note here the superscript (c(i))ofd(c(i))
lrepresents the subspace index dl
corresponding to the ithobservation.
Deﬁne ~H(yjx)in the form of Equation (21), where ~D=QL
l=1QSl
s=1D(s)
lwithD(s)
l=f1;:::;Dlg,
~d=fd(s)
lgl=1;:::;L;s=1;:::;S land~~d=QL
l=1QSl
s=1l;d(s)
l. We ﬁrst introduce the following technical lemma
which can be easily proved by induction:
Lemma 1. Given 0ai;bi1andjai bijfor alli= 1;:::;N , thenjQN
i=1ai QN
i=1bijN.
SinceP(~=2[ ~d2~D~~d)P
l=1;:::;L;s=1;:::;S lP((s)
l=2l)NL 1, we can now rewrite ~H(yjx)as
~H(yjx) =X
~d2~DZ
~~dNY
i=1H(yijxi;i)dG(~) +NLO1(1); (25)
and~H(yjx)as
~H(yjx) =X
~d2~DZ
~~dNY
i=1H(yijxi;
d(c(i)))dG(~); (26)
20

--- PAGE 21 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
where 0 O 1(1)1, and we have the following approximation error bound between ~H(yjx)and
~H(yjx):
j~H(yjx) ~H(yjx)jX
~d2~DZ
~~dNY
i=1H(yijxi;
d(c(i))) NY
i=1H(yijxi;i)dG(~) +NL 1
N2+NL 1; (27)
where the second inequality is resulted from Lemma 1.
A.2 Step 2: Approximating Equation (21) by Equation (22)
Partition the space of wil(i.e.R) intoDl+2adjacent half open half closed intervals Wl;dl= (w
l;dl 1;w
l;dl]
(fordl= 1;:::;Dl),Wl;0= ( 1;w
l;0]andWl;Dl+1= (w
l;Dl;1)such that l(Wl;dl) =Gl(l;dl)for
everydl= 1;:::;Dl. Note that such a partitioning always exists as lis a continuous distribution. Also
denote the domain of (the L-vector)d= (d1;:::;dL)asD=QL
l=1f1;:::;Dlgand the corresponding
extended domainD+=QL
l=1f0;1;:::;Dl+ 1g.
Denote ~F(u)(yjx)as the form of Equation (22) with (u)
d(wi)given by
(u)
d(wi) = expfu(~d;0+~T
dwi)g=X
d02D+expfu(~d0;0+~T
d0wi)g: (28)
We have the following technical lemma to help us choose suitable parameters f(~d;0;~d)gd2D+of(u)
d(wi):
Lemma 2. There exists parameters f(~d;0;~d)gd2D+of(u)
d(wi)such that (u)
d(wi)u!1   !QL
l=11
wil(Wl;dl)for everyd2 D+, where 1
w(W)is an indicator which equals to 1 if wfalls inside
the interior ofW, equals to a number between 0 and 1 if Wis on the boundary of W, and equals to 0
otherwise.
Proof. With a slight (notational) variant of Lemma 3.1 of Fung et al. [2019a], it can be easily shown that
there exists parameters f(~d;0;~d)gd2D+such that ~d;0+~T
dwi>maxd06=d;d02D+~d0;0+~T
d0wiif and
only ifwil2W
l;dlfor everyd2D+, whereW
l;dlis the interior ofWl;dl. Under a slight notational variant
of Lemma 3.2 of Fung et al. [2019a], we have (u)
d(wi)u!1   ! 1f~d;0+~T
dwi>maxd06=d;d02D+~d0;0+
~T
d0wig+O1(1)1f~d;0+~T
dwi= maxd06=d;d02D+~d0;0+~T
d0wig=QL
l=11
wil(Wl;dl), where 0
O1(1)1.
Choosing the parameters indicated by the above lemma, we have the following approximation result between
~H(yjx)and~F(u)(yjx):
~F(u)(yjx)u!1   !ZNY
i=1X
d2D+ LY
l=11
wil(Wl;dl)!
H(yijxi;
d)d(w)
=Z
WNY
i=1X
d2D+ LY
l=11
wil(Wl;dl)!
H(yijxi;
d)d(w) +O1(1)
21

--- PAGE 22 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
=X
~d2~DZ
W~dNY
i=1X
d2D LY
l=11
wil(Wl;dl)!
H(yijxi;
d)d(w) +O1(1)
=X
~d2~DZ
W~dNY
i=1H(yijxi;
d(c(i)))d(w) +O1(1)
=X
~d2~DNY
i=1H(yijxi;
d(c(i)))G(~~d) +O1(1)
=~H(yjx) +O1(1); (29)
where W(s)
l=[Dl
dl=1Wl;dl,W=QL
l=1QSl
s=1W(s)
landW~d=QL
l=1QSl
s=1Wl;d(s)
l. The convergence
is resulted from the Dominated Convergence Theorem (DCT), which is obviously uniform on (y;x)as
(u)
d(wi)(the only term in ~F(u)(yjx)which depends on u) does not depend on (y;x)andH(yijxi;
d)is
bounded above at 1. The third last equality holds as the events of the indicator functions only (jointly) hold
ifd=d(c(i))whenw2W ~d. The second last equality holds as the integrand is constant on w2W ~dand
(W~d) =QL
l=1QSl
s=1l(Wl;d(s)
l) =QL
l=1QSl
s=1Gl(l;d(s)
l) =G(~~d).
A.3 Step 3: Approximating Equation (22) by Equation (23)
Partition the space of yik2Rinto adjacent half open half closed intervals fYM
k;m= ((m 1=2)hy
M;(m+
1=2)hy
M]gm= M;:::;M 1;M. DeﬁneYM:=QK
k=1[M
m= MYM
k;m:=QK
k=1YM
k= ( (M+ 1=2)hy
M;(M+
1=2)hy
M]K,YM
k; (M+1)= ( 1; (M+ 1=2)hy
M]andYM
k;M+1= ((M+ 1=2)hy
M;1). Choosehy
Msuch
thathy
M#0andMhy
M"1 asM!1 . Also denoteYM
m=YM
1;m1YM
K;m Kas a hypercube with
m:= (m1;:::;mK)2M :=f (M+ 1);:::; (M+ 1)gK.
Similarly, partition the space of xip2Rinto adjacent half open half closed intervals fXQ
p;q= ((q 
1=2)hx
Q;(q+ 1=2)hx
Q]gq= Q;:::;Q 1;Q. DeﬁneXQ:=QP
p=1[Q
q= QXQ
p;q:=QP
p=1XQ
p= ( (Q+
1=2)hx
Q;(Q+ 1=2)hx
Q]P,XQ
p; (Q+1)= ( 1; (Q+ 1=2)hx
Q]andXQ
p;Q+1= ((Q+ 1=2)hx
Q;1). Choose
hx
Qsuch thathx
Q#0andQhx
Q"1 asQ!1 . Also denoteXQ
q=YQ
1;q1XQ
P;qPas a hypercube
withq:= (q1;:::;qP)2Q:=f (Q+ 1);:::; (Q+ 1)gP.
Denote ~F(M;Q;t;u )(yjx)as the form of Equation (23), where bycM
m:= (bycM
m1;:::;bycM
mK)(inside the ex-
pression ofF(M;Q;t;u )(yijxi;wi)) represents the leftmost vertex of the hypercube YM
m. Also, the function
(t;u)
j(xi;wi;~Q
q;~d)is a logit linear gating function given by
(t;u)
j(xi;wi;~Q
q;~d)
=expflogH(YM
mjxQ
q;
d) +t(~Q
q;0+~QT
qxi) +u(~d;0+~T
dwi)g
P
m02MP
q02QP
d02D+expflogH(YM
m0jxQ
q0;
d0) +t(~Q
q0;0+~QT
q0xi) +u(~d0;0+~T
d0wi)g(30)
=expft(~Q
q;0+~QT
qxi)g
P
q02Qexpft(~Q
q0;0+~QT
q0xi)gexpfu(~d;0+~T
dwi)gP
d02D+expfu(~d0;0+~T
d0wi)gH(YM
mjxQ
q;
d)
:=(t)
q(xi)(u)
d(wi)H(YM
mjxQ
q;
d); (31)
22

--- PAGE 23 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
wherexQ
q= (xQ
1;q1;:::;xQ
P;qP)is the mid-point of the hypercube XQ
q. Here, “mid-points” for Xp; (Q+1)
andXp;(Q+1)are respectively deﬁned as  (Q+ 1)hx
Qand(Q+ 1)hx
Q.
Further denote RQ(xi) =fq:8pwe havejpjQ(xip) xQ
p;qpj hx
QgandR0Q(xi) =fq:
9psuch thatjpjQ(xip) xQ
p;qpj> hx
Qg, wherepjQ(xip)is the projection of xipon the intervalXQ
p(i.e.
pjQ(xip)is the point inXQ
pwherexipis closest to). To derive the approximation results, we ﬁrst introduce
the following two technical lemmas.
Lemma 3. Given any ﬁxed Qandhx
Q, there exists parameters f~Q
q;0;~QT
qgq2Q, such that for any 3>0,
we haveP
q2R0Q(xi)(t)
q(xi)3for allxi2X with sufﬁciently large t.
Proof. This follows directly from the proof of Theorem 3.2 of Fung et al. [2019a].
Lemma 4. The probability distributions fH(yijxi;i)gxi2X;i2are tight for any compact spaces Xand
.
Proof. Divide the compact space Z:=XintoDsubspacesfZdgd=1;:::;D, where each subspace Zdis
small enough to be covered by a ball with radius . Deﬁnez
d:= (x
d;
d)as an arbitrary interior point of Zd
ford= 1;:::;D . For eachd= 1;:::;D , we choose a response space ~Yd2Y such thatH(~Ydjx
d;
d)
1 =2. Select a compact space ~Ywhich covers allf~Ydgd=1;:::;D, and we have H(~Yjx
d;
d)1 =2
true for alld= 1;:::;D . Uniform continuity of Hon any compact space implies that jH(~Yjx
d;
d) 
H(~Yjxi;i)j=2for sufﬁcient small if(xi;i)2Zd. Overall, we have H(~Yjxi;i)1 , and
hence the result follows.
As a result, for any compact covariates space of Xand any4>0, we can use Lemma 4 to select a
rectangular output space Ysuch thatH(Yjxi;i)1 4for anyxi2Xandi2, where =QL
l=1l
and note that lis deﬁned in Section A.1. Then we have for all xi;xQ
q2Xand
d2:
jH(dyeM
mjxQ
q;
d) H(pjM(dyeM
m)jxQ
q;
d)j4; (32)
and
jH(yijxi;
d) H(pjM(yi)jxi;
d)j4; (33)
wherepjM(yi)is the projection of yionY, anddyeM
m:= (dyeM
m1;:::;dyeM
mK)is the rightmost vertex of
hypercubeYM
m. Further, because of the uniform continuity of H(yijxi;
d)on(xi;
d)within a compact
support, for any 5>0, we can choose sufﬁcient large Q(to makehx
Qsmall enough while XQcovers X)
andM(to makehy
Msmall enough while YMcovers Y) such that for any yi2YM
mandq2RQ(xi), we
have:
jH(pjM(yi)jxi;
d) H(pjM(dyeM
m)jxQ
q;
d)j5: (34)
Summarizing the above three equations, we have:
jH(yijxi;
d) H(dyeM
mjxQ
q;
d)j24+5: (35)
Note thatF(M;Q;t;u )(yijxi;wi)can be re-written as
F(M;Q;t;u )(yijxi;wi) =X
m2MX
q2QX
d2D+(t)
q(xi)(u)
d(wi)H(dyeM
mjxQ
q;
d)1fyi2YM
mg
23

--- PAGE 24 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
=X
m2MX
q2RQ(xi)X
d2D+(t)
q(xi)(u)
d(wi)H(dyeM
mjxQ
q;
d)1fyi2YM
mg+O1(3);
(36)
where 0O 1(3)3. Note that the last equality is resulted from Lemma 3. Then, the approximation
result is given by:
j~F(M;Q;t;u )(yjx) ~F(u)(yjx)j
ZNY
i=1F(M;Q;t;u )(yijxi;wi) NY
i=1F(u)(yijxi;wi)d(w)
NZF(M;Q;t;u )(yijxi;wi) F(u)(yijxi;wi)d(w)
NZX
m2MX
q2RQ(xi)X
d2D+(t)
q(xi)(u)
d(wi)H(yijxi;
d) H(dyeM
mjxQ
q;
d)1fyi2YM
mgd(w) + 23
N(24+5+ 23); (37)
where the second inequality is resulted from Lemma 1, the third and last inequalities are respectively resulted
from Equations (36) and (35).
A.4 Step 4: Approximating Equation (23) by Equation (24)
In the ﬁnal step, we denote ~F(M;Q;t;u;v )(yjx)as the form of Equation (24), where F0(yi; M(v)
m)in
F(M;Q;t;u;v )(yijxi;wi)is chosen in the way that F0(yi; M(v)
m)D  !1fyi bycM
mgasv! 1 . Due
to the distributional convergence as well as Mis a ﬁnite set, for any 6>0we can ﬁnd a sufﬁcient large v
such that for every m2M , we have:
jF0(yi; M(v)
m) 1fyibycM
mgj6+KX
k=11fyik2L
k(bycM
mk)g; (38)
whereis chosen to be 0<<hy
M=2, andL
k(bycM
mk)represents non-overlapping intervals for mk=
 (M+ 1);:::; (M+ 1) with
L
k(bycM
mk) =(
[bycM
mk ;bycM
mk+];ifmk> (M+ 1)
( 1;bycM
mk 2]; m k= (M+ 1)(39)
Note that the rightmost term in Equation (38) is to control for the fact that the weak convergence of F0to
the indicator is not uniform when yikis close tobycM
mk. Consider the bound
jF(M;Q;t;u;v )(yijxi;wi) F(M;Q;t;u )(yijxi;wi)j
X
m2MX
q2QX
d2D+(t)
q(xi)(u)
d(wi)H(YM
mjxQ
q;
d)jF0(yi; M(v)
m) 1fyibycM
mgj
(
max
q2Q;d2D+X
m2MKX
k=1H(YM
mjxQ
q;
d)1fyik2L
k(bycM
mk)g)
+6
24

--- PAGE 25 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
=KX
k=18
<
:max
q2Q;d2D+X
mk= (M+1);:::;(M+1)Hk(Yk;mkjxQ
q;
d)1fyik2L
k(bycM
mk)g9
=
;+6: (40)
SinceL
k(bycM
mk)is non-overlapping for mk= (M+ 1);:::; (M+ 1) , only one term in the sum-
mation of Equation (40) is non-zero. Since Hkis a continuous distribution, for any 7>0, we have
Hk(Yk;mkjxQ
q;
d)7for anyq2Q ,d2D+andmk= (M+ 1);:::; (M+ 1) given thatMis
sufﬁciently large. Finally, using the same proof idea as Equation (37), we have
j~F(M;Q;t;u;v )(yjx) ~F(M;Q;t;u )(yjx)jN(K7+6): (41)
In summary, based on Equations (27), (29) (37) and (41), Theorem 1 holds because for sufﬁciently large M,
Q,tandv, the following inequality holds uniformly for each xifalling into a compact covariates space:
j~F(M;Q;t;u!1;v)(yjx) ~H(yjx)j(N2+1) +O1(1) +N(24+5+ 23) +N(K7+6);(42)
where1to7can be chosen to be arbitrarily small, and any parameters chosen in Steps 1 to 4 are indepen-
dent ofN,S= (S1;:::;SL)andc().
Appendix B Proof of Theorem 2
Other than the notational differences, the proof ideas of Theorems 1 and 2 are substantially similar. Precisely,
the 4-step framework used to prove Theorem 1 also applies to prove Theorem 2. As a result, we only present
a sketch proof of Theorem 2, with an emphasis on the key differences of proof techniques between the two
theorems. Unless speciﬁed otherwise, the notations adopted in this proof section is the same as those deﬁned
in Appendix A.
Analogous to Equations (20) and (21), in Step 1 we examine an approximation bound between the following
two equations:
~H(yjx) =Z
~"Y
i2IH(yijxi;i)#
dG(~); (43)
and
~H(yjx) =X
~d2~DY
i2IH(yijxi;
d(iL))G(~~d): (44)
Similar to Appendix A.1, we choose compact spaces fl2gl=1;:::;L such that P(\L
l=1\il2Ilfil2
lg)1 NL 1, whereIl=fil:i1= 1;:::;N 0;:::;il= 1;:::;Nil 1g. Also partition very granular
subspacesfl;dlgdl=1;:::;D lofland deﬁne the interior points f
l;dlgdl=1;:::;D lin the exact same way as
Appendix A.1. Deﬁne here ~D=QL
l=1Q
il2IlD(il)
lwithD(il)
l=f1;:::;Dlg,~d=fd(il)
lgl=1;:::;L;il2Il
withd(il)
l2D(il)
l,d(iL)=fd(il)
lgl=1;:::;L, and ~~d=QL
l=1Q
il2Ill;d(il)
l. Then, using the exact logic as
Equations (25) to (27), an arbitrarily small approximation error bound between Equations (43) and (44) can
be obtained.
In Step 2, we deﬁne the following function analogous to Equation (22) and derive its error bound for ap-
proximating Equation (44):
25

--- PAGE 26 ---
Mixture of experts models for multilevel data: modelling framework and approximation theory
~F(u)(yjx) =ZY
i2IF(u)(yijxi;wi)d(w) (45)
withF(u)(yijxi;wi) =P
d2D+(u)
d(wi)H(yijxi;
d). Here, we choose (u)
d(wi)in a different way as
Equation (28), which is crucial to cater for the dependencies of random effects across levels, as follows:
(u)
d(wi) = expfLX
l=1u1=l(~dl;0+~dl;1wil)g=X
d02D+expfLX
l=1u1=l(~d0
l;0+~d0
l;1wil)g; (46)
whereD=QL
l=1f1;:::;DlgandD+=QL
l=1f0;1;:::;Dl+ 1gare exactly the same as those deﬁned in
Appendix A.2, and dl= (d1;:::;dl)andd0
l= (d0
1;:::;d0
l)withd=dL(andd0=d0
L).
In contrast to Appendix A.2, we construct intervals Wl;dlsuch that[Dl+1
dl=0Wl;dl=Rfor anydl 12Ql 1
l0=1f1;:::;Dl0gandl(Wl;dl) =Gl(l;dlj1;d1;:::;l 1;dl 1), which represents the probability of
level-lrandom effect belongs to l;dlconditioned on the corresponding upper level random effects fall into
(1;d1;:::;l 1;dl 1). The following lemma is analogous to Lemma 2:
Lemma 5. There exists parameters f(~dl;0;~dl;1)gd2D+;l=1;:::;L of(u)
d(wi)such that(u)
d(wi)u!1   !QL
l=11
wil(Wl;dl)for everyd2D+.
Proof. Similar to the proof of Lemma 2, we choose suitable parameters such that ~dl;0+~dl;1wil>
maxd0
l6=dl~d
l;0+~d
l;1wilif and only if wil2W
l;dlfor everyd2D+andl= 1;:::;L , whereW
l;dlis the
interior ofWl;dlandd
l= (d1;:::;dl 1;d0
l). Observe the expressionPL
l=1u1=l(~dl;0+~dl;1wil)in Equa-
tion (46) where the term corresponding to a higher level factor (small l) dominates when uis large. There-
fore, for sufﬁcient large u, we havePL
l=1u1=l(~dl;0+~dl;1wil)>maxd6=d0PL
l=1u1=l(~d0
l;0+~d0
l;1wil)
forwisatisfyingwil2W
l;dlfor everyl= 1;:::;L . Referring to the same logic as the proof of Lemma 2
and the result follows.
Now, the approximation bound between Equations (44) and (45) can be obtained using the same logic
(with notational variations) as that outlined by Equation (29), where we further note that (W~d) =QL
l=1Q
il2Ill(Wl;d(il)
l) =QL
l=1Q
il2IlGl(l;d(il)
lj1;d(i1)
1;:::;
l 1;d(il 1)
l 1) =G(~~d)withW~d=
QL
l=1Q
il2IlWl;d(il)
l.
Step 3 and 4 involve evaluations of the following expressions in analogous to Equations (23) and (24):
~F(M;Q;t;u )(yjx) =ZY
i2IF(M;Q;t;u )(yijxi;wi)d(w) (47)
withF(M;Q;t;u )(yijxi;wi) =P
m2MP
q2QP
d2D+(t;u)
j(xi;wi;~Q
q;~d)1fyibycM
mg,
~F(M;Q;t;u;v )(yjx) =ZY
i2IF(M;Q;t;u;v )(yijxi;wi)d(w) (48)
withF(M;Q;t;u;v )(yijxi;wi) =P
m2MP
q2QP
d2D+(t;u)
j(xi;wi;~Q
q;~d)F0(yi; M(v)
m). The deriva-
tion techniques here are exactly the same as those presented in Appendices A.3 and A.4, so this part of the
proof is omitted.
26
