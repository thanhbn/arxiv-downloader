# 2303.14177.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2303.14177.pdf
# File size: 6401066 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Suchin Gururangan* 1yMargaret Li* 1 2Mike Lewis2Weijia Shi1 2Tim Althoff1
Noah A. Smith1 3Luke Zettlemoyer1 2
Abstract
Large language models are typically trained
densely: all parameters are updated with respect
to all inputs. This requires synchronization of
billions of parameters across thousands of GPUs.
We introduce a simple but effective method to
asynchronously train large, sparse language mod-
els on arbitrary text corpora. Our method clusters
a corpus into sets of related documents, trains
a separate expert language model on each clus-
ter, and combines them in a sparse ensemble for
inference. This approach generalizes embarrass-
ingly parallel training by automatically discover-
ing the domains for each expert, and eliminates
nearly all the communication overhead of existing
sparse language models. Our technique outper-
forms dense baselines on multiple corpora and
few-shot tasks, and our analysis shows that spe-
cializing experts to meaningful clusters is key to
these gains. Performance also improves with the
number of experts and size of training data, sug-
gesting this is a highly efﬁcient and accessible
approach to training large language models.
1. Introduction
Language models (LMs) are trained on up to trillions of
tokens of text (Hoffmann et al., 2022; Touvron et al., 2023).
This improves performance on many tasks, but also incurs
an extreme cost: thousands of GPUs need to be active si-
multaneously to update all parameters at each step (Zhang
et al., 2022; Chowdhery et al., 2022). Branch-Train-Merge
(BTM; Li et al. 2022) alleviates this cost by dividing the
total compute among a collection of smaller expert language
models (ELMs), each independently trained on a distinct
subset (or domain ) of the training corpus and ensembled
*Equal contributionyWork done while at Meta AI.
1University of Washington2Meta AI3Allen Institute
for Artiﬁcial Intelligence. Correspondence to: Suchin
Gururangan <sg01@cs.washington.edu>, Margaret Li
<margsli@cs.washington.edu>.
1B 2B 5B 10B 20B 40B 80B 168B
Token Count121314151617Perplexity
Dense
c-BTM (k=4)
c-BTM (k=16)C4Figure 1. We present C-BTM, a new technique to asyn-
chronously scale expert LMs (§2). C-BTM splits a corpus into
kclusters, trains an expert LM on each cluster, and creates a sparse
ensemble during inference. Above, LMs trained with C-BTM
(with 4 or 16 clusters) achieve lower validation perplexity than
compute-matched dense LMs. These LMs begin with OPT-1.3B
(Zhang et al., 2022), and are further trained on C4 (Raffel et al.,
2019). The optimal cluster count for C-BTM , and its performance
gains, increase with the size of training data (shown in log-scale).
during inference. However, BTM relies on document meta-
data to identify domains, and such supervision is not always
available (e.g., in large Internet crawls; Raffel et al., 2019;
Rae et al., 2021; Gao et al., 2021). Moreover, the optimal
number of metadata-based domains for a ﬁxed budget is un-
known, since metadata cannot be easily merged or divided.
In this work, we introduce Cluster-Branch-Train-Merge ( C-
BTM ; Figure 1), a metadata-free algorithm to scale LMs
without massive multi-node synchronization. We use un-
supervised clustering to discover domains in a corpus, and
train an ELM on each cluster independently (§2.1). At infer-
ence time, we sparsely activate a subset of the trained ELM s
(§2.2). We ensemble ELM s by weighting their outputs with
the distances between an embedding of the current context
and each expert’s cluster center. This enables simple and ef-
ﬁcient sparse computation (Fedus et al., 2022) by retrieving
only the top- kexperts when predicting each new token.arXiv:2303.14177v1  [cs.CL]  24 Mar 2023

--- PAGE 2 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Figure 2. C-BTM training process (§2.1). C-BTM begins with unsupervised domain discovery using k-means clustering. We then
initialize expert language models (ELMs) with a seed language model (e.g., OPT; Zhang et al. 2022) and train an ELM on each cluster.
The resulting experts are added to a larger collection for sparse inference.
C-BTM generalizes BTM by allowing for ﬁne-grained con-
trol over the number and size of data clusters, since they are
automatically learned without being constrained by avail-
able metadata. We use this new capability to investigate the
scaling properties of C-BTM as a function of the number of
experts trained, controlling for a variety of factors (§3). Ex-
tensive experiments show that training more clusters always
results in better validation perplexity than single cluster (i.e.,
dense) models, and the optimal cluster count increases with
the overall compute (§4.1). These results are consistent for
both 1.3B and 6.7B parameter experts.
With more clusters, we can aggressively parallelize expert
training: for example, we train 128 ELM s (168B parameters
in total) on 168B tokens of text in aggregate with only 8
GPUs at a time. This enables us to avoid many practical
difﬁculties associated with training large LMs across many
nodes simultaneously (§4.2). Moreover, the number of
parameters at inference time can be kept constant even as the
number of experts grows (§4.3): using just the top-2 or top-4
experts is comparable to using all experts, while using just
the top-1 expert still outperforms the dense model. Training
with more clusters is also more effective than training larger
dense models: in §4.4, we demonstrate that training many
1.3B expert LMs, and sparsifying them to a 5.2B parameter
LM, achieves the same perplexity as a 6.7B dense model,
but with only 29% as many training FLOPs. These gains
are also reﬂected in few-shot text classiﬁcation experiments
(§5), which show that C-BTM models outperform dense
baselines even with heavily sparsiﬁed inference.
C-BTM provides a radically simpliﬁed sparse modeling
approach that eliminates nearly all communication over-
head from existing sparse LM schemes. Existing sparse
LMs typically route different tokens to specialist parameters
(Lepikhin et al., 2021; Fedus et al., 2021; Clark et al., 2022).
However, they have yet to be widely adopted, perhaps due
in part to the communication costs of routing each token
in each sparse layer (Artetxe et al., 2021), challenges in
learning to specialize experts to tokens (Zhou et al., 2022),and the necessity of additional mechanisms to balance ex-
pert utilization (Lewis et al., 2021). C-BTM improves over
sparse LMs by routing sequences (instead of tokens) using
ofﬂine balanced clustering (instead of online load balancing)
with no shared parameters between experts. We compare
directly to a mixture-of-experts model with top-2 routing
(Lepikhin et al., 2021) in §6.
Our ﬁnal analysis (§7) shows that balanced clustering is
key to C-BTM performance; it works as well as expert
assignment with gold metadata, and strongly outperforms
random and unbalanced clustering baselines. Overall, our
ﬁndings suggest that C-BTM is an efﬁcient and accessible
method to scale large language models into massive datasets.
We release our code and models publicly.1
2.C-BTM
We introduce C-BTM , a method for embarrassingly parallel
training that specializes expert language models to domains
discovered through clustering instead of metadata. C-BTM
enables scaling to arbitrary numbers of domains and com-
pute budgets on any corpus. In this section, we outline
C-BTM training (Figure 2) and inference (Figure 3).
2.1. Training
Step 0: Cluster To segment our corpus, we employ
k-means clustering, enforcing balanced clusters. ELMs
trained without this constraint perform worse (§7.2).2
Consider the iterative, hard expectation-maximization view
ofk-means clustering. In the expectation step, each docu-
ment embedding is assigned to a cluster center based on its
Euclidean distance to each center. In the maximization step,
each cluster center is updated to be the mean embedding of
the current set of documents assigned to it. To balance the
1https://github.com/kernelmachine/cbtm
2Other techniques to improve clusters, e.g. k-means++ (Arthur
& Vassilvitskii, 2007), can be used to improve performance.

--- PAGE 3 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
clusters, we formulate the expectation step as a balanced lin-
ear assignment problem (Malinen & Fränti, 2014). Given D
document embeddings with representations fw1; : : : ; w Dg
andKcluster centers with representations fh1; : : : ; h Kg,
we assign each document dto a cluster with the assignment
index ad2f0; : : : ; Kg:
max
a1;:::;aDDX
d=1 dist(had; wd)s.t.8k,DX
d=11ad=k=D
K
(1)
where dist is the Euclidean distance. Many algorithms
exist to solve this problem; we follow Lewis et al. (2021)
and use the auction algorithm (Bertsekas, 1992). We only
use balancing when estimating the cluster centers; we use
greedy inference when predicting clusters, as balancing at
inference time is cumbersome for massive corpora.
In our experiments, we use a simple tf-idf embedding func-
tion, which is highly efﬁcient at scale and leads to inter-
pretable clusters.3We only use a single shard of each cor-
pus to train our clustering model. Any new document, once
embedded, can be efﬁciently mapped to its nearest cluster(s)
without additional training. Any embedding function can be
used, though the choice of embedder may apply different
assumptions of what constitutes a textual domain and come
with efﬁciency trade-offs.4Comparing to other embedding
or clustering methods is an interesting area for future work,
and could likely improve performance.
Step 1: Branch (from seed LM) To begin training ex-
perts on each of the kclusters from Step 0, we ﬁrst branch
from (i.e., make kcopies of) a seed LM. Seed LMs are
critical for the overall functionality of ELMs, and ELMs
perform best when the seed LM has been trained with a
diverse corpus (Li et al., 2022). In our experiments, we use
an OPT LM (Zhang et al., 2022) as our seed.5
Step 2: Train We assign each ELM to a single cluster,
and train on each cluster with the log likelihood objective.
Step 3: Merge After training on the assigned domain, we
add the new ELM into a larger collection for inference.
In this work, we focus on a single iteration of C-BTM for
simplicity. Future work may explore branching from already
trained experts in multiple iterations.
3In initial experiments, tf-idf outperformed other scalable text
embeddings, like hash embeddings (Svenstrup et al., 2017).
4tf-idf assumes that domains are lexically-driven, which may
not correspond with other notions of domain.
5Li et al. 2022 ﬁnd that dedicating more compute to branching
(rather than seed training) leads to better in-domain performance,
and the choice of seed LM has a strong effect on the modularity
of the resulting ELMs. Future work may explore the effect of
different seed LMs on C-BTM performance.
Figure 3. C-BTM inference process (§2.2). At inference time,
we embed each incoming context and estimate a probability dis-
tribution over clusters, by calculating the distance between the
embedded context and each cluster center. We use this probability
distribution, optionally sparsiﬁed to use only the top-k experts, to
weight an output ensemble of the ELMs.
2.2. Inference
At inference time, we use a sparse ensemble of the outputs
of ELMs for incoming test contexts (Figure 3). Formally,
consider that the language model provides, at each timestep,
p(Xtjx<t). We introduce a domain variable D, alongside
each sequence. Then the next-step conditional distribution
on the history x<tis:
p(Xtjx<t)=kX
j=1p(Xtjx<t; D=j)p(D=jjx<t)|{z }
ensemble weights
(2)
With the pretrained embedder and clustering model from
Step 0 (§2.1), we embed the context hx<tand use the k
cluster centersfhc0: : : h ckg. We set ensemble weights as:
p(D=jjx<t)/topk[exp( dist(hx<t; hcj)2=T)](3)
Where dist is the Euclidean distance, Tis a temperature pa-
rameter which sharpens or smoothes the probability distribu-
tion over cluster centers, and the top-k function ﬁlters for the
top-kprobabilities and renormalizes the distribution to sum
to 1. This formulation is reminiscent of nearest-neighbor re-
trieval mechanisms for language models (Khandelwal et al.,
2019; Shi et al., 2022).
These ensemble weights are updated for every incoming
token, although in separate experiments we observe that
we ﬁnd that cluster assignments (and in effect, ensemble
weights) can be ﬁxed for the second half of a document with
no drop in performance; this can further speedup inference.
We ﬁnd that, in practice, the performance of our models
is robust to even top-2 or top-4 experts, meaning that the
inference costs of the language model are equivalent to
a much smaller LM. We perform an empirical study of
inference variations in §4.3.

--- PAGE 4 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
2.3. Comparing to Dense Training
Dense LMs are typically trained using hundreds or thou-
sands of concurrent GPUs, all of which synchronize gradi-
ents each update. For example, OPT-175B (Zhang et al.,
2022) was trained on 992 80GB A100 GPUs, and PaLM-
540B (Chowdhery et al., 2022) was trained on 6144 TPU
v4 chips. C-BTM improves training efﬁciency by reduc-
ing communication overhead, as only GPUs training the
same ELM must communicate. Furthermore, the chance
of a GPU failure can grow considerably with the number
of GPUs. C-BTM improves the resiliency of distributed
training, since a single GPU failure only delays training for
a single ELM, whereas in dense training, a single GPU fail-
ure afﬂicts training on all other GPUs. C-BTM also makes
training large LMs more feasible on shared GPU clusters,
since it effectively decomposes training into smaller jobs
which can run asynchronously. This makes job scheduling
more efﬁcient by reducing the number of GPUs that need to
be allocated simultaneously.
2.4. Comparing to BTM (Li et al., 2022)
Our method addresses several limitations of the training and
inference techniques proposed by Li et al. (2022).
First, BTM is limited to training data with metadata which
can be used to determine its domains. Typical LM corpora,
including C4 (Raffel et al., 2019) and the Pile (Gao et al.,
2021), are sourced from the Internet without retaining doc-
ument provenance at collection time, and are infeasible to
label manually. Also, the optimal number of experts for a
ﬁxed corpus size, model architecture, and budget remains
unknown, and is difﬁcult to explore with metadata-based
domains, since they cannot be easily merged or divided.
C-BTM broadens the applicability of BTM to arbitrary
datasets.
Moreover, BTM inference follows the cached prior method
introduced by Gururangan et al. (2022), where the ensem-
ble weights are estimated using Bayes’ rule on additional
held out data, and the prior P(D=j)is estimated with an
exponential moving average over sequences of posterior es-
timates that require forward passes on experts. This estimate
is then ﬁxed during test data evaluation.
With C-BTM , we route based only on the current context.
Thus, no additional data or forward passes through the ex-
perts are needed to estimate ensemble weights, nor do we
need to assume that adjacent documents in the test set come
from the same distribution. This also implies that the param-
eter averaging technique of Li et al. (2022) is not well suited
to our setting, as it requires ﬁxing the weights assigned to
each expert for a set of evaluation documents. Future work
may explore merging expert parameters for each context
during inference.2.5. Comparing to Mixture-of-Experts (MoE)
Like MoE models (e.g., Fedus et al., 2022), C-BTM allows
for efﬁcient scaling of large LMs while keeping inference
costs manageable. However, C-BTM routes sequences (in-
stead of tokens) using ofﬂine balanced clustering (instead
of online load balancing) with no shared parameters be-
tween experts. This eliminates effectively all complexities
associated with balancing expert utilization (Lewis et al.,
2021), avoids expensive all-to-all operations between ex-
perts (Artetxe et al., 2021), and naturally leads to inter-
pretable expert specialization to domains of the training cor-
pus. In §6, we compare directly to MoE baselines trained
with sparse upcycling (Komatsuzaki et al., 2022), which
initializes the MoE with a dense checkpoint, mirroring how
C-BTM initializes ELMs.
3. Experimental Setup
We design a set of experiments to study C-BTM on two large
corpora (Figure 4) selected to be distinct from the corpus
used to train our seed OPT model, and report perplexity on
held out data from each corpus.
3.1. Data
C4 (Raffel et al., 2019) C4 is a publicly available
distribution of a Common Crawl snapshot on Hug-
gingface datasets.6We use the no blocklist version
(en.noblocklist ) to train on a dataset that is out of
distribution to our seed (OPT) pretraining corpus. C4 con-
sists of 393M documents totaling 220B BPE tokens. We
train on up to 168B tokens.
S2ORC (Lo et al., 2019) The Semantic Scholar Research
Open Corpus (S2ORC) is a publicly available corpus of
full-text academic papers from the Semantic Scholar.7The
corpus spans 20 ﬁelds of study (e.g., Biology, Computer
Science, Art), and contains 16M documents, totaling 87B
BPE tokens. We train on up to 168B tokens over multiple
epochs.8
Evaluation data For all experiments, we report language
modeling perplexity on 200 randomly-sampled held out doc-
uments. Because S2ORC does not come with pre-deﬁned
validation data, we create a validation corpus by sampling
an equal number of documents from each ﬁeld of study.
6https://huggingface.co/datasets/c4
7https://allenai.org/data/s2orc
8While it is not common to train large LMs for multiple epochs,
we do not observe overﬁtting in any of our experiments, consistent
with other studies that train LMs on academic literature for multiple
epochs (Taylor et al., 2022).

--- PAGE 5 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
study, participant, disease
theorem, proof, let
quark, mass, Higgsprotein, DNA, genestraining, data, learningsocial, political, people
cell, expression, micelaser, beam, opticallanguage, words, speech
flow, velocity, fieldSemantic Scholar Open Research Corpus (S2ORC)
food, recipes, minutes
games, team, seasonreligion, life, peoplepolitics, president, saidproducts, online, quality
film, movie, storymusic, album, band
market, company, year
patient, cancer, bodyColossal Cleaned  Crawled Corpus (C4)
file, window, click
Figure 4. We train and evaluate on two large text corpora (§3.1). C4 (left; Raffel et al. 2019) and S2ORC (right; Lo et al. 2019) are
diverse and contain many different clusters of text, indicated by these UMAP visualizations of 400K random documents in each corpus,
colored with 32 automatically discovered and annotated clusters. See §7.3 for the description of our clustering and annotation procedure,
and Figure A in the appendix for annotations of all clusters in these plots.
3.2. Experimental Setup
Clustering the data We segment each corpus using bal-
anced k-means clustering for k2{2, 4, 8, 16, 32, 64, 128}
(§2.1). To train the clustering models, we ﬁrst embed all data
with a tf-idf vectorizer using scikit-learn,9with minimal as-
sumptions: we only remove stop-words from a ﬁxed lexicon
and replace numbers with a dummy token. We then reduce
the dimensionality of the resulting embeddings; we perform
truncated SVD with 100 dimensions, then normalize the vec-
tor by removing its mean and scaling to unit variance, which
we observed in initial experiments improved the clustering
quality. Finally, these representations are clustered using
a custom Pytorch implementation.10We present learned
clusters and visualizations in Figure 4 and Figure A (in the
appendix). We use a single shard of each training corpus
(384K documents for C4, 155K documents for S2ORC) to
train the clustering model and its embedder. No evaluation
data is used in this process.
Seed LM As LMs trained on diverse corpora make for bet-
ter seeds (Li et al., 2022), we use pretrained OPT language
models (Zhang et al., 2022) as our seed for all experiments.
Model hyperparameters We use the OPT architecture
implemented in Metaseq (Zhang et al., 2022). We use OPT-
9https://scikit-learn.org/
10https://github.com/kernelmachine/
balanced-kmeans1.3B for the initial set of experiments, and replicate our
experiments with OPT-6.7B. Following Zhang et al. 2022,
we use the GPT-2 vocabulary of 50,257 BPE types (Rad-
ford et al., 2019), and train with 2,048-token sequences,
across document boundaries. We prepend a beginning-of-
document token to each document. We set dropout to 0.1
for all parameters except those of the embedding layer.
Training hyperparameters For all models, we ﬁx the
learning rate to that used during OPT pretraining (2e-4 for
1.3B parameter models; 1.2e-4 for 6.7B parameter models;
Zhang et al. 2022) using a linear decay learning rate sched-
ule to zero (with no warmup), which we found to work well
for most settings after a grid search of fastest learning rates
that avoided divergence. We use a batch size of 8 for each
GPU, and train with fp16 and fully-sharded data-parallel
(Artetxe et al., 2021). We train on NVIDIA V100 32GB
GPUs. All models are trained with Metaseq (Zhang et al.,
2022). For a given number of clusters kand total GPU bud-
getn, each ELM is allocated n=k GPUs, keeping the total
effective number of FLOPs ﬁxed across models exposed to
the same number of tokens. See §A.2 for more details.
Scaling We train for a total of 10K steps in each run; to
expose the model to more tokens, we increase the total GPU
budget proportionally, up to 64 GPUs. We simulate larger
budgets, up to 1024 GPUs, by increasing gradient accumu-
lation steps with 64 GPUs. This method of scaling increases
the model’s effective batch size for the same number of

--- PAGE 6 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
steps, and maintains near constant run-times across our
many experiments. This experimental setup also means that
as the number of clusters increases, the overall set of ELMs
is exposed to more data with less simultaneous computation
among GPUs.
Other ways of training on more data (e.g., by keeping to-
tal batch size ﬁxed and increasing step count) may yield
different results. The best batch size and learning rate com-
binations for training language models are likely speciﬁc to
a variety factors, including the model size, dataset, and total
compute available (Shallue et al., 2018; McCandlish et al.,
2018; Yang et al., 2021). In preliminary experiments, we
found that expert models beneﬁt from faster learning rates
and larger batch sizes. Given a sufﬁciently large batch size,
experts are robust to a variety of learning rates. Our larger
budget experiments might beneﬁt from higher learning rates,
but we leave further tuning for future work.
Inference One of the key hyperparameters for inference
is the temperature T(Equation 3), which governs the sharp-
ness of the probability distribution over experts for a given
context. We ﬁnd that setting T=0.1 works well for most
settings (see §A.6 for more details). We also compute the
nearest cluster centers for every incoming context, regard-
less of how stable the cluster assignments already are for
a document. However, we ﬁnd that these assignments can
be ﬁxed for the second half of a document with no drop in
perplexity; this can further speedup inference. The other
important hyperparameter is the top-k value, which sparsi-
ﬁes the probability distribution over experts. For our core
experiments in §4.1, we set top-k to the total number of
experts we have trained for each model. We explore the
effect of enforcing sparsity with lower top-k values in §4.3.
Baselines In our primary experiments (§4), we compare
with a strong dense baseline (i.e., our 1-cluster model) fol-
lowing OPT pretraining. We also progressively increase
the number of clusters we train with for a ﬁxed number
of tokens. In subsequent experiments (§6), we compare to
MoE language models initialized from a dense checkpoint.
3.3. Making Fair Model Comparisons
We follow the recommendations of Dehghani et al. (2021)
and report results with multiple cost metrics, and detail
our choices here. When comparing model training budgets,
we are primarily concerned with the true monetary cost of
model training, which is typically billed in direct propor-
tion to GPU-time. Model inference comparisons have two
main considerations: monetary cost incurred by the model
deployer, again measured in GPU-time, and latency for end-
users, or wall-clock time (i.e., how slow a model inference
is for an end-user).
1B20B 40B 80B 168B
Token Count1314151617Perplexity
C4
1 cluster
2 clusters
4 clusters
8 clusters
16 clusters
1B20B 40B 80B 168B
Token Count10.511.011.512.012.513.013.514.0Perplexity
S2ORC
1 cluster
2 clusters
4 clusters
8 clusters
16 clustersFigure 5. Increasing cluster count in C-BTM improves lan-
guage modeling performance for a ﬁxed compute budget
(§4.1). Performance of ELM s trained with C-BTM as a function
of the total number of tokens trained on, which, in our experiments,
equalizes FLOP count. Training with more than one cluster always
outperforms the compute-matched, single cluster dense model, and
we observe improving performance (and in §4.2, faster updates) as
we increase the number of clusters.
We explicitly do notcompare or match the number of model
parameters during training, which has minimal bearing on
the cost of model training separately from its inﬂuence on
GPU-time. The number of training parameters is a particu-
larly misleading cost measure that is unsuitable for sparse
models, since they can maintain the FLOPs and inference
speed of dense models despite training many more parame-
ters (Dehghani et al., 2021).

--- PAGE 7 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Training GPU-time Assuming ﬁxed hardware, GPU-
time during model training is determined mostly by FLOPs
and inter-machine communications. However, prior work
typically only FLOP-matches, ignoring the additional inter-
GPU communications incurred by some models (e.g., MoE)
that increase training costs. Ideally, our comparisons could
directly ﬁx GPU-time. This is challenging in practice, as
even identical computations on the same GPU node at dif-
ferent times can vary wildly in speed due factors like tem-
perature, other activity on the node, or the quality of GPU
interconnect. To maintain consistency and fairness despite
these confounds, our results compare FLOP-matched mod-
els with the same training data budget over the same number
of updates (§4.1), but also report the speed of training for
each FLOP-matched model (§4.2). This allows us to disen-
tangle and accurately reﬂect multiple cost metrics of train-
ing. Since, in our experiments, models exposed to the same
number of tokens incur the same number of FLOPs, we use
training data size as a more interpretable measurement of
the overall training budget (see §A.2 for more details).
Inference GPU-time Inference GPU-time is also primar-
ily the result of FLOPs and communication costs. Since
communication during inference is minimal, we compare
FLOPs via inference parameters (§4.3). We do not account
for the FLOPs of the C-BTM router, which varies based on
the clustering approach, and is relatively negligible.
Inference latency FLOPs is not an ideal metric for infer-
ence latency of our models, because C-BTM allows for par-
allel inference across ELMs. This means that if ELMs share
the same architecture (e.g., OPT-1.3B), inference latency
is always equivalent to that of a single ELM, regardless of
the number of experts active. However, inference latency
may be quite different between model architectures (e.g.,
OPT-1.3B and OPT-6.7B); we discuss this further in §4.4.
As with inference GPU-time, we do not consider the latency
of the C-BTM router.
4. Language Modeling Results
We begin with a set of experiments in which we train LMs
with C-BTM on datasets from §3.1. We are interested in
measuring how performance changes as we increase overall
compute. We ﬁrst compare models against training costs:
total training tokens (§4.1) and training time (§4.2). Then,
in §4.3, we compare model performance along an axis of
inference costs: the total parameter count at inference time .
Finally, in §4.4 we compare model performance by ﬁxing
both training and inference costs. Across all computational
budgets, C-BTM provides substantial beneﬁts over dense
training, and performance improvements increase as the
total compute grows.
1 2 4 8 16 32 64 128
Number of Clusters1314151617Perplexity
C4
Tokens
1.3B
2.6B
5.2B
10B
21B
42B
84B
168B
1 2 4 8 16 32
Number of Clusters1112131415Perplexity
S2ORC
Tokens
1.3B
2.6B
5.2B
10B
21B
42B
84B
168BFigure 6. There exists an optimal cluster count for each com-
pute budget (§4.1). The optimal cluster count increases as one
increases the compute budget, but using too many clusters without
sufﬁciently increasing compute can degrade performance. For
both C4 and S2ORC, 16 clusters gets the best performance at the
highest budget (168B tokens), although higher cluster counts still
outperform the 1-cluster (dense) model ( x= 1 in this graph).
4.1. Controlling for Total Training Tokens
First, we compare model performance controlling for over-
all training data size (or equivalently, training FLOPs; §3.3).

--- PAGE 8 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Figure 5 shows evaluation perplexity on C4 and S2ORC
with up to 16 clusters. Training on more than one cluster
always outperforms training with a single cluster (i.e., a
dense model). As the amount of training data grows, the
gap between our models and the dense one widens, indicat-
ing that experts make better use of larger training datasets,
possibly due to their increased specialization. These results
suggest that as we increase the amount of data available,
C-BTM beneﬁts from more clusters.
However, Figure 6 shows that there exists an optimal cluster
count for each token budget that we consider. Each number
of clusters has a budget range in which they are optimal,
and the optimum smoothly progresses from smaller to larger
cluster counts as we increase the training data size. If we
increase the cluster count past the optimum, each expert
has an insufﬁcient share of the data, resulting in worse
performance.
Nevertheless, we observe that using more clusters than opti-
mal for the highest token budget settings still outperforms
the dense model. Since it is cheaper to train with more
clusters for a ﬁxed training data size due to parallelism, it
may be preferable in some settings to train with a large num-
ber of clusters despite their less-than-optimal performance.
Based on the trends we observe at this scale, we expect that
higher cluster counts would become optimal as we scale the
training data size even further.
The consistency of our results on C4 and S2ORC suggests
that these general trends may be widely applicable to many
datasets. However, the optimal number of clusters for a
given computational budget is likely dataset speciﬁc. Future
work may explore relationships between dataset features
and the optimal cluster count.
These trends are consistent as we increase the size of our
experts to 6.7B parameters (Figure 7), although the gaps
between our baselines reduce, likely due to the substantial
increase in pretraining FLOPs for OPT-6.7B.11
4.2. Comparing Training Time
Now, we turn to comparing our models based on training
times. We measure the speed of training each model with
the maximum seconds-per-update for each training run.12
For C-BTM models with more than one cluster, we use the
maximum seconds-per-update across all experts. To make
our comparisons fair, we only compare the training times of
models that have the same effective batch size (§3.3). Our
results are displayed in Figure 8. As we increase the number
11OPT-6.7B was pretrained for 1.83 ZFLOPs, while the OPT-
1.3B was trained for 0.34 ZFLOPS.
12Other measures of seconds-per-update (e.g., average, median)
tend to be noisy, due to factors such as dataloading and bursty
GPU activity.
1B 5B 10B 20B
Token Count11.512.012.513.013.5Perplexity
C4
1 cluster
2 clusters
4 clusters
8 clusters
1 2 4 8 16 32
Number of Clusters11.512.012.513.013.5Perplexity
C4
Tokens
1.3B
2.6B
5.2B
10B
21BFigure 7. Our results are consistent even as we increase expert
size to 6.7B parameters (§4.1). 8 clusters is optimal at 21B to-
kens, as it is for the 1.3B parameter ELMs. However, the gaps
between these models are smaller, due to the substantial increase
in pretraining FLOPs for the OPT-6.7B checkpoint.
of clusters and training data size, the update speed for C-
BTM increases , since models with higher cluster counts
use fewer GPUs per expert under a ﬁxed budget, and there
is no communication between experts. This suggests that
C-BTM models with more clusters can be exposed to more
data for the same amount time as dense models.
As discussed in §2.3, C-BTM also provides important prac-
tical speedups when training large LMs at scale. C-BTM
divides large compute budgets among many models, such
that we can train on 168B tokens with only 8 GPUs per
expert in the 128-cluster setting. On shared multi-node clus-

--- PAGE 9 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
8 16 32 64
Total GPUs78910Max seconds/update
MoE
1 cluster
2 clusters
4 clusters
8 clusters
Figure 8. Models trained with more clusters have faster up-
dates as we increase the total compute (§4.2). We display the
maximum seconds-per-update for C-BTM and MoE models with
varying GPU counts (across all experts). Under ﬁxed compute,
training with more clusters uses fewer GPUs per expert, and C-
BTM avoids communication between experts, resulting in faster
updates. On the other hand, MoE models are much slower to train,
due to extensive communication between experts (§6.3), as well as
additional FLOPs from top-2 routing (Artetxe et al., 2021).
ters, allocating many smaller jobs incurs shorter cumulative
wait times than a single, large synchronous job, since they
can make more efﬁcient use of shared resources, and run on
short-lived, idle nodes (Wortsman et al., 2022). Furthermore,
large LM training is prone to node failures, gradient spikes,
and other unexpected behaviors (Zhang et al., 2022). With
dense models, when one node fails, all nodes must restart
due to synchronization. With C-BTM , experts are trained
independently; if a node fails, only the corresponding expert
needs to be restarted, and all other experts are unaffected.
4.3. Controlling for Inference Costs via Parameter
Count
Comparing models with just training budgets ignores the
fact that C-BTM inference GPU-time costs grow as we
increase the number of clusters, since we train more param-
eters. To mitigate these costs, we can use the top-k function
(Equation 3) to dynamically use a subset of experts for each
incoming context during evaluation (§2.2). Next, we study
the effect of inference parameter count on model perfor-
mance. We focus on the largest training budget (i.e., 168B
tokens) for these experiments.
Results (Figure 9) show that despite training many more
parameters, training C-BTM with many clusters and then us-
ing only the top-1 expert still outperforms the dense model.
Further, using the top-2 or top-4 experts yields comparable
1.3B
(top-1)2.6B
(top-2)5.2B
(top-4)10B
(top-8)20B
(top-16)
# of Inference Parameters12.012.513.013.514.0Perplexity
C4
1 cluster
2 clusters
4 clusters
8 clusters
16 clusters
1.3B
(top-1)2.6B
(top-2)5.2B
(top-4)10B
(top-8)20B
(top-16)
# of Inference Parameters10.410.610.811.011.211.411.6Perplexity
S2ORC
1 cluster
2 clusters
4 clusters
8 clusters
16 clustersFigure 9. Sparse top- kinference performance at 168B token
budget (§4.3). ELMs perform well even with heavily sparsiﬁed
inference. Top-1 inference substantially outperforms the densely
trained baseline at no additional inference cost, and top-2 and top-4
inference performs comparably to (and is sometimes slightly better
than) activating all experts. In our setup, inference parameters are
proportional to inference FLOP count (§3.3).
performance to activating all experts. Sometimes we ob-
serve that sparsifying can even slightly improve performance
over using all experts (for example, see the 16 cluster model
for C4 in Figure 9). We speculate that having all experts
active may introduce interference effects from experts that
are specialized to clusters unrelated to test-time contexts.
Our results in Figure 10 suggest that sparsifying even larger
expert models (i.e., those with more clusters than the optimal

--- PAGE 10 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
1.3B 2.6B 5.2B 10B 20B 42B 84B 168B
# of Inference Parameters12.0012.2512.5012.7513.0013.2513.5013.7514.00Perplexity
1 cluster
2 clusters
4 clusters
top-1
top-2top-4 top-16top-32top-64top-128C4
16 clusters
32 clusters
64 clusters
128 clusters
Figure 10. Train large, then sparsify (§4.3). Despite using more
than the optimal cluster count for the 168B token budget, sparsify-
ing the 32, 64, and 128-cluster models with top-1, top-2, or top-4
experts is usually better than training 1-, 2-, or 4-cluster models.
for a given token budget) is still highly effective. At the
most extreme setting, using the top-1 expert for the 128
cluster model (using 0.7% of total parameters at inference
time for each context) still outperforms the dense model, and
the top-4 expert model (3.1% of total parameters) performs
comparably to using all experts.
These results suggest that C-BTM results in a highly sparse
LM, and that inference costs can be kept constant even as
the number of experts grows, though additional experts can
be added to further boost performance.
4.4. Comparing to a Larger Dense Model
In our ﬁnal comparison of this section, we consider both
training and inference costs together. We compare a 6.7B
1-cluster (dense) model and C-BTM model with 1.3B pa-
rameter experts, which uses 16 clusters (optimal in our ex-
periments from §4.1) and top-4 inference, resulting in 5.2B
inference parameters. This C-BTM model has lower infer-
ence cost than the larger 6.7B parameter dense model (§3.3).
The former uses fewer inference parameters, incurring a
smaller inference GPU-time cost, and has lower latency,
comparable to that of a single 1.3B-parameter ELM.
We compare the FLOPs used to train each model. Follow-
ing Artetxe et al. (2021), we build continuous efﬁciency
curves by interpolating between our empirical observations.
Speciﬁcally, we calculate the speedup between our cluster
expert models and dense model by interpolating between the
discrete observations of perplexity values for a given empir-
ical number of FLOPs.13Our goal is to identify the FLOP
13See §A.3 for details on this interpolation.
0.5 1.0 1.5 2.0 2.5
Total ZFLOPs1214161820Perplexity
3.5x speedup  0 tokens 
 (zero-shot)
8B tokens
168B tokens45B tokens
 20B tokens 4B tokens  0 tokens 
 (zero-shot)C4
16 clusters (5.2B inference params)
1 cluster (6.7B inference params)Figure 11. Training with C-BTM is substantially more efﬁ-
cient than training a larger dense LM (§4.4). We train a
16-cluster C-BTM model and use top-4 inference, resulting in
a 5.2B parameter LM, and compare its performance with a 6.7B
parameter dense LM. The C-BTM model at 168B tokens achieves
the same perplexity on C4 as a 6.7B dense model with 3.5x fewer
ZFLOPs. The total ZFLOPs includes the cost of pretraining the
seed OPT checkpoints.
count necessary to achieve a particular perplexity value. If
ELMs trained with C-BTM achieve the same perplexity
as the dense model with half the FLOPs, we conclude that
C-BTM achieves a 2 speedup.
Our results are presented in Figure 11. A smaller C-BTM
model, exposed to 168B tokens of text, can achieve the
same perplexity as the larger 6.7B dense model with 3.5 
speedup. These speedup estimates are dependent on the
amount of pretraining performed on each model. Future
work may perform these experiments with larger models
and many more ELMs.
4.5. Summary
Our results demonstrate that controlling for a variety of
different types of computational budget, C-BTM outper-
forms dense training in language modeling. Furthermore,
we demonstrate that C-BTM results in an effective sparse
language model, where the top-1, top-2 and top-4 experts
from models with at least 8 clusters signiﬁcantly outper-
form 1-cluster, 2-cluster and 4-cluster models. These results
suggest the possibility of outperforming dense models by in-
creasing margins, while keeping both training and inference
costs ﬁxed, as compute and the number of experts grow.

--- PAGE 11 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Few-shot Text Classiﬁcation Accuracy (%)
AGNews DBPedia SST-2 Amazon Phrasebank Twitter
#Model (inference parameters) Topic Topic Sentiment Sentiment Sentiment Hatespeech Average
Random chance 25.0 7.10 50.0 50.0 33.3 50.0 35.9
OPT (1.3B) 42.9 57.2 72.8 81.3 72.5 65.1 65.3
OPT (6.7B) 51.9 58.9 77.0 83.8 76.4 39.6 64.6C41-cluster (1.3B) 47.4 61.1 80.2 80.7 66.6 60.9 66.2
1-cluster (6.7B) 68.1 62.4 80.7 84.9 80.6 37.4 69.0
16-cluster; top-1 (1.3B) 47.1 62.9 74.3 79.1 72.9 56.4 65.4
16-cluster; top-4 (5.2B) 49.3 62.3 80.0 81.3 78.7 61.3 68.8
16-cluster; top-16 (20.8B) 50.6 62.0 84.0 83.2 78.6 61.7 69.9
Table 1. C-BTM models outperform dense counterparts on downstream text classiﬁcation tasks (§5.2). We display accuracy of
models from §4 on six text classiﬁcation tasks, using eight demonstrations for each example and no additional ﬁne-tuning. We report
accuracy averaged over ﬁve random seeds. The 1- and 16-cluster models are trained on 168B tokens of C4 (i.e., our highest budget). The
16-cluster model, with top-4 or top-16 inference, always outperforms the 1-cluster model, and top-1 inference usually outperforms the
1-cluster model at no additional inference cost. We include average performance of models across tasks for readability.
5. Downstream Task Results
Do the trends from §4 extend to downstream settings? To be-
gin to answer this question, we perform few-shot evaluation
on six downstream text classiﬁcation tasks. We indeed ﬁnd
that models trained with C-BTM outperform their dense
counterparts in these settings.
5.1. Experimental Setup
Tasks We experiment with six text classiﬁcation tasks,
spanning topic, sentiment, and hatespeech classiﬁcation.
Details of the datasets are in Appendix A.7.
Few-shot inference We perform 8-shot evaluations. For
each task, we randomly sample 8 examples with their la-
bels from the train set, and prepend them as demonstrations
for each test example. For C-BTM models, we estimate
ensemble weights for each example by passing both the
example and the demonstrations through our pretrained clus-
terer (§2.2). We calculate the probability of each label for
the task under the model, and report accuracy by counting
the proportion of test examples where the gold label has
the highest probability. We report average accuracy over 5
random seeds. We leave careful analysis of C-BTM with
varying numbers of demonstrations and few-shot inference
techniques to future work.
Baselines We compare the performance of 1- and 16-
cluster C-BTM models trained on 168B tokens of C4 (i.e.,
our highest budget from §4). For the 16-cluster model, we
also perform top-1 and top-4 inference (§4.3). We addi-
tionally compare against a random baseline, the original
OPT-1.3B and 6.7B models (without any additional train-
ing), and the 6.7B parameter 1-cluster model trained on 20B
tokens of C4.5.2. Results
Our results in Table 1 show that the 16-cluster C-BTM
model always outperforms the 1-cluster, 1.3B parameter
baseline, sometimes dramatically. This aligns with our lan-
guage modeling results (§4.1). The 1-cluster model achieves
lower accuracy than OPT-1.3B on some tasks despite addi-
tional training, suggesting that our models may suffer from
catastrophic forgetting, since the C4 corpus is out-of-domain
to OPT.
Nevertheless, the 16-cluster model outperforms OPT-1.3B
on all tasks other than Twitter. Also, top-1 and top-4 infer-
ence matches or exceeds using all experts in some settings,
consistent with our language modeling results in §4.3. We
examine the clusters associated with the most likely experts
for each task, and ﬁnd that their top-terms are relevant to the
task’s domain (Table 10 in the appendix). This supports our
hypothesis that C-BTM is able to leverage any part of the
corpus which is in-domain to the test task, even if the train-
ing corpus as a whole might be sufﬁciently out-of-domain
as to have a negative effect on performance.
We then mirror the analysis in §4.4, by comparing our 16-
cluster models to 6.7B parameter dense models. First, we ob-
serve that our 1-cluster 6.7B model outperforms OPT-6.7B
on all tasks except Twitter, possibly because this model has
had less exposure to C4, and suffers from less catastrophic
forgetting. Our 16-cluster model performs comparably to
both 6.7B models, and on multiple tasks, our 16-cluster
model outperforms both 6.7B models, which have been
trained with at least 3.5 more compute (§4.4). With top-4
inference, C-BTM models activate even fewer parameters
than the 6.7B parameter models, yet perform comparably.
These results corroborate our ﬁndings in §4.4 that compared
to larger dense models, models trained with C-BTM have
more training efﬁciency and lower inference latency, and
result in comparable or better performance.

--- PAGE 12 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
In separate experiments, we observe that routing examples
to experts based on their performance on few shot examples,
rather their clusters, results in even better downstream task
performance with C-BTM models. This is likely because
few-shot performance depends on factors such as example
order, label distributions, and the quality of the demonstra-
tions, which not necessarily tied to the domain of the task
(Min et al., 2022; Lu et al., 2022). We analyze this ﬁnd-
ing further in §A.7, and leave more careful development of
routing protocols for downstream tasks to future work.
5.3. Summary
We demonstrate that, consistent with the language modeling
results in §4.1, C-BTM improves downstream performance
on a variety of few-shot text classiﬁcation tasks. C-BTM
models consistently outperform dense 1-cluster baselines,
and usually outperform the original OPT models, despite
being trained on an out-of-domain corpus. We also ﬁnd
that top- kactivation reduces inference costs with negligible
effects on downstream task performance. C-BTM models
perform comparably to larger, 6.7B OPT and 1-cluster dense
baseline models, despite being trained with 3.5x less com-
pute, and even when activating fewer inference parameters.
6. Comparing to Mixture-of-Experts
Finally, we compare C-BTM against an alternative sparse
LM, a mixture-of-experts (MoE) which learns a routing
between tokens and feedforward experts in the transformer
(Lepikhin et al., 2021; Fedus et al., 2021). As discussed in
§2.5, C-BTM is substantially simpler than MoE.
6.1. Sparse Upcycling
To mirror C-BTM seed initialization, we initialize our MoE
with a dense checkpoint. We use the sparse upcycling tech-
nique from Komatsuzaki et al. (2022). Upcycling a dense
model into an MoE with kexperts entails initializing shared
parameters (e.g., attention and embedding layers) and kex-
pert parameters (e.g., every other feedforward layer) from
a dense checkpoint, and initializing new parameters for the
router. Then the model is simply trained as an MoE. Here,
we use top-2, token-level routing (Lepikhin et al., 2021).
6.2. Experimental Setup
Hyperparameters We train an MoE with sparse upcy-
cling on C4, starting from OPT-1.3B and using the same
general experimental setup detailed in §3.2. We follow the
settings from Komatsuzaki et al. (2022) as closely as pos-
sible. We conducted experiments with 8, 16, 32, 64, and
128 experts for each compute budget. 8 and 16 experts are
similar to, but slightly worse than, 32 experts; 64 experts
and 128 experts consistently have exploding losses, and the
1B 10B 20B
Number of tokens14.014.515.015.516.016.517.0Perplexity
C4
cBTM (1 Cluster)
MoE (32 experts)
cBTM (16 clusters)Figure 12. MoE underperforms C-BTM (§6.3). We compare a
32-expert MoE with top-2 routing (Lepikhin et al., 2021) trained
with sparse-upcycling (Komatsuzaki et al., 2022). While the MoE
outperforms C-BTM models with 16 experts at small budgets, it
fails at larger budgets, even under-performing the dense model. We
speculate this could be due to distribution shifts after pretraining,
which might increase the instability of upcycling.
few which successfully train are also similar to but slightly
worse than 32 experts. In general, we ﬁnd that both large
expert count (and higher compute budgets) result in sparse
upcycling training instability.
We use 32 experts in our MoE, a capacity factor of 2, and
continue training without resetting the optimizer from that
used during OPT pretraining. We set all hyperparameters to
be the same as our C-BTM models (§3.2), except that we
use a peak learning rate of 2e-5, which we found to be the
highest learning rate that that did not result in divergence
after a sweep. We release our code for sparse upcycling,
implemented in Fairseq (Ott et al., 2019), publicly.14
Baselines We compare the 32-expert MoE LM to 1-cluster
(i.e., dense) and 16-cluster C-BTM models.
6.3. Results
The MoE expends more FLOPs than the other models due to
the additional feedforward layer at every other transformer
block for top-2 routing, as well as the routing projection
(Artetxe et al., 2021). For clarity and consistency, we update-
match and separately report GPU-time cost of updates, as
in §4.1.
We display results in Figure 12. MoE substantially under-
performs C-BTM with 16 clusters as the compute budget
14https://github.com/kernelmachine/
moe-fairseq

--- PAGE 13 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
grows. Surprisingly, we observe that with enough compute,
MoE underperforms even the dense LM, and that when
compute budgets are further increased, losses consistently
explode. This suggests that sparse upcycling is highly un-
stable, possibly due to distribution shifts from pretraining.
In Figure 8, we compare the maximum seconds-per-update
of the MoE model with that of the C-BTM models. MoE
becomes substantially slower as more GPUs are used during
training. This is largely due to expensive all-to-all commu-
nication that occurs between experts during MoE training,
which is necessary to route tokens to experts (Artetxe et al.,
2021). On the other hand, our method does not have any
shared parameters between experts. Also, MoE expends
more FLOPs during training than the C-BTM models. Fi-
nally, MoE still requires synchronous compute to train ex-
perts due to shared parameters, so they are also afﬂicted by
the practical difﬁculties of training dense language models
at scale §4.2.
6.4. Summary
Our results suggest that language models trained with C-
BTM substantially outperform MoEs trained to the same
budget. The performance gains of our technique likely are a
result of the simplicity of our deterministic routing (based
on empirically derived clusters), instabilities associated with
sparse upcycling, and other factors.
7. Analysis
In §4, §5, and §6, we demonstrate that C-BTM outperforms
compute-matched densely trained and MoE baselines. We
now study our clustering approach in more detail and de-
scribe its effect on overall performance of C-BTM.
7.1. Is clustering important?
To assess the importance of the clustering algorithm, we
perform C-BTM as above, except that we assign each doc-
ument to a random cluster, rather than a learned one. This
is equivalent to the random ensemble baseline from Li et al.
(2022). Results in Figure 13 demonstrate that using random
clusters dramatically underperforms both our method and
the dense baseline. Therefore, cluster specialization is vital
for C-BTM . This conﬁrms results from Li et al. (2022),
who found that domain specialization of ELMs is critical
for performance the ensemble, as well as those from Jang
et al. (2023), who show that instruction-specialized ELMs
transfer to other tasks with similar instructions.
7.2. Is it important to balance the clusters?
Applying a balancing constraint in k-means avoids the de-
generate outcome of a long tail in cluster sizes (Chang et al.
1B 10B 20B 40B 80B
Number of tokens14.014.515.015.516.016.5Perplexity
C4
1 cluster
8 random clusters
32 random clustersFigure 13. Random clusters underperform (§7.1). Training ex-
perts on random clusters underperforms even the dense, single
cluster model, showing the importance of cluster specialization.
Random clusters become more harmful as the cluster count grows.
20B 40B 80B
Number of tokens13.013.514.014.5Perplexity
C4
1 cluster
32 unbalanced clusters
32 balanced clusters
Figure 14. Cluster balancing improves performance (§7.2).
When training on C4 with 32 clusters, balancing consistently im-
proves over the unbalanced version, suggesting that cluster size
important aspect to C-BTM.
2014). Indeed, with 10K documents of held out validation
data in C4, we observe that balanced clustering signiﬁcantly
increases the median cluster size, and narrows its range, rel-
ative to an unbalanced baseline (§A.4). To assess the effect
of balancing cluster size on the performance of C-BTM ,
we perform C-BTM with a k-means clustering model but
remove the balancing constraint. For the 8-cluster model,
we observe that balancing has little effect. However, for the
32-cluster model (Figure 14), unbalanced clustering consis-
tently leads to worse performance. These results suggest

--- PAGE 14 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
that balancing becomes more important as one scales the
number of clusters. This is consistent with separate experi-
ments that show that the long tail in cluster sizes becomes a
more consistent problem with higher cluster counts.
7.3. Are clusters well deﬁned? Do experts specialize?
Since we use tf-idf as our document embedder in C-BTM ,
we can perform an inverse transform from the cluster centers
into the vocabulary space to identify terms that most likely
would have been embedded as the cluster center. We display
the top ﬁve terms per cluster in §A.1. We observe that as the
number of clusters increases, the top terms across clusters
become more speciﬁc and varied.
Next, we study whether ELMs trained on these clusters spe-
cialize. Using the 32-cluster model trained on 84B tokens
of C4, we compute perplexity of all experts across 200 held
out documents in each cluster. For each cluster, we then
measure the ratio of the perplexity of each expert to the per-
plexity of the expert trained on that cluster. We display those
ratios in Figure 15. We see that all experts perform best on
their own cluster. Some experts do not transfer well at all
to other clusters, while others do reasonably well. Cross
referencing with the cluster term tables in §A.1, we see that
cluster experts 3 and 5 tend to generalize well and the top
terms in these clusters are more generic (with words such
as "just,like,love"). The experts specialized to content such
as"site, page, website" (cluster 0) and "app, phone, video"
(cluster 29), tend to do poorly on all other clusters.15These
results suggest that experts specialize to their cluster. We
infer that the success of sparse C-BTM inference is a result
of expert specialization, and that C-BTM performance gains
may be partially due to the sample efﬁciency of specialized
training.
7.4. How do clusters and metadata domains compare?
The key motivation for C-BTM is to remove the reliance on
metadata to delineate the domains to which ELMs specialize.
How well do clusters reﬂect the dataset segmentation pro-
duced by metadata? We use S2ORC to study this question.
First, we align the learned clusters from a 32-cluster model
with the ﬁelds-of-study metadata available from S2ORC
(Lo et al., 2019). Then we visualize the overlap between
the metadata and clusters (Figure 16). We observe only a
partial alignment between metadata and clusters in S2ORC.
Documents with some metadata labels (e.g., Enviromental
Science, Political Science) are mostly assigned to their own
clusters, while documents with other labels (e.g., Computer
Science, Physics) are distributed across multiple clusters.
15This result imply that cluster experts can be removed to ﬁlter
out unwanted generations after training, without signiﬁcantly im-
pacting performance on other content. We leave such exploration
to future work.
024681012141618202224262830
Cluster0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30Expert
1.01.52.02.53.03.54.0
perplexity increase
Figure 15. Experts specialize to their cluster (§7.3). Here, we
use the 32-cluster model trained on 84B tokens of C4. Each cell
is a ratio between one expert’s test perplexity on a cluster to that
of the expert trained on that cluster. The diagonal indicates that
experts perform best on the cluster they were trained on. Many
experts transfer well across clusters, but some do not.
The partial alignment between metadata and clusters sug-
gests that C-BTM models may not have the same perfor-
mance as those trained with metadata labels to delineate
domains. To investigate this hypothesis further, we perform
experiments using a subset of the Pile (Gao et al., 2021) to
compare the performance of experts trained with metadata
and experts trained with clusters. See §A.5 for more details
on this corpus. We observe that experts trained with learned
clusters perform slightly better than those with metdata la-
bels on a held out validation data (Table 8 in the appendix).
Both techniques perform better than training with just a
single cluster on the Pile, conﬁrming our results from §4.1.
These results imply that metadata may not correspond with
the most optimal segmentation of the corpus. However,
using metadata has the advantage of interpretability and
simplicity, and metadata can identify domains that are not
just lexically driven (e.g., Lucy & Bamman, 2021; Guru-
rangan et al., 2022). Future work may explore combining
metadata- and cluster-specialized ELMs.
7.5. Summary
Our analysis demonstrates that the improvements from C-
BTM are not the result of ensembling alone. Various com-
ponents of our training method, particularly the nature of
the learned clusters, play a critical role in C-BTM perfor-
mance. Improving the representation of domains in a corpus,
perhaps using other pretrained representations (Aharoni &
Goldberg, 2020) or more sophisticated clustering algorithms
(Ester et al., 1996; Chronopoulou et al., 2022), are likely to
improve C-BTM performance.

--- PAGE 15 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Env. Science
Poli. Science
Business
Geology
Mathematics
Sociology
Philosophy
History
Geography
Art
Chemistry
Biology
materials
Economics
Psychology
Medicine
Engineering
Physics
Comp. Science
Metadata0
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30Cluster
020406080100
% overlap
Figure 16. Clusters and metadata do not perfectly align (§7.4).
Each cell in the heatmap is the % overlap between a cluster and
a metadata label identifying the ﬁeld-of-study of a document in
S2ORC; high overlap indicates that most documents with the cor-
responding label get assigned to the corresponding cluster. While
documents with certain labels (e.g., Environmental Science, Politi-
cal Science, Business) get primarily assigned to a single cluster,
documents with other labels (e.g., Engineering, Physics, Computer
Science) are distributed across multiple clusters.
8. Related Work
Sparse Models C-BTM is closely related to sparse mod-
els which activate only a subset of parameters (Evci et al.,
2020; Mostafa & Wang, 2019; Dettmers & Zettlemoyer,
2019). C-BTM is inspired by MoE, but is much simpler
and more efﬁcient to train. Most MoE methods rely on
training token-based routing mechanisms (Lepikhin et al.,
2021; Fedus et al., 2021; Lewis et al., 2021; Roller et al.,
2021), but others rely on task (Kudugunta et al., 2021) or
domain (Gururangan et al., 2022) routing.
Expert Language Models As we note throughout the
study, this work is most directly related to BTM (Li et al.,
2022). BTM is in turn partially inspired by prior work on
variations of MoE models (Jacobs et al., 1991), but espe-
cially DEMix layers (Gururangan et al., 2022), which re-
place transformer feedforward layers with metadata-deﬁned
domain experts. Jang et al. (2023) train expert language
models on instruction-based tasks, while Pfeiffer et al.
(2022) train expert language models on different languages.
Cluster Routing Chronopoulou et al. (2022) and
Chronopoulou et al. (2023) use hierarchical clustering to
identify domains to specialize adapter experts to, and use the
adapters in an ensemble or parameter average at inferencetime. Duan et al. (2021) build ensembles of task-speciﬁc
models by clustering the training data of supervised tasks.
Gross et al. (2017) employ a cluster-based router similar to
ours in an image classiﬁcation setting using ResNets. How-
ever, they use a hard routing (or only activate a single expert)
in both training and inference, while we use hard routing
during training but ensemble experts during inference. Our
inference technique is inspired by nearest neighbor retrieval
mechanisms in language models (Khandelwal et al., 2019;
Shi et al., 2022).
Communication-efﬁcient training Our study con-
tributes to a line of research into communication-efﬁcient
algorithms for training large models. Some previous work
proposes ways to train large dense models collaboratively
over distributed networks of servers (Borzunov et al., 2022;
Yuan et al., 2022). Other works focus on new forms of
data (Gan et al., 2021), model (Ryabinin et al., 2023),
and pipeline (Wang et al., 2022) parallelism to allow for
model training on heterogeneous devices that can recover
from node failures. Wortsman et al. (2022) propose a
communication-efﬁcient method of ﬁne-tuning by training
a collection of models with different hyperparameters on
individual GPU nodes, and then averaging their parameters
after training. Our work uses expert specialization for
communication efﬁcient training, and C-BTM can be
combined with any of these other techniques to improve
training efﬁciency.
9. Conclusion
We introduce C-BTM , a new technique to efﬁciently train
sparse LMs. C-BTM splits a corpus into kclusters, trains
an expert LM on each cluster, and creates a sparse ensemble
during inference. We observe that the optimal number of
clusters for C-BTM increases with the amount of data, and
using more clusters also allows us to aggressively parallelize
training to efﬁciently scale into massive datasets. Future
work could investigate C-BTM in multitask or multilingual
settings, the usefulness of multiple iterations of C-BTM on
a corpus (perhaps with hierarchical clustering), or the possi-
bility of combining metadata- and cluster-based routing to
scale into many heterogeneous datasets in parallel.
Acknowledgements
This paper beneﬁted from thoughtful feedback from a num-
ber of people: Armen Aghajanyan, Tim Dettmers, Sneha
Kudugunta, Stephen Roller, Swabha Swayamdipta, and
Mitchell Wortsman.

--- PAGE 16 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
References
Aharoni, R. and Goldberg, Y . Unsupervised domain
clusters in pretrained language models. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics , pp. 7747–7763, Online,
July 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.acl-main.692. URL https:
//aclanthology.org/2020.acl-main.692 .
Artetxe, M., Bhosale, S., Goyal, N., Mihaylov, T., Ott, M.,
Shleifer, S., Lin, X. V ., Du, J., Iyer, S., Pasunuru, R.,
Anantharaman, G., Li, X., Chen, S., Akin, H., Baines, M.,
Martin, L., Zhou, X., Koura, P. S., O’Horo, B., Wang, J.,
Zettlemoyer, L., Diab, M., Kozareva, Z., and Stoyanov,
V . Efﬁcient large scale language modeling with mixtures
of experts, 2021. URL https://arxiv.org/abs/
2112.10684 .
Arthur, D. and Vassilvitskii, S. K-means++: The advantages
of careful seeding. In Proceedings of the Eighteenth
Annual ACM-SIAM Symposium on Discrete Algorithms ,
SODA ’07, pp. 1027–1035, USA, 2007. Society for Indus-
trial and Applied Mathematics. ISBN 9780898716245.
Barbieri, F., Camacho-Collados, J., Espinosa Anke,
L., and Neves, L. TweetEval: Uniﬁed bench-
mark and comparative evaluation for tweet classiﬁca-
tion. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020 , pp. 1644–1650, On-
line, November 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.
148. URL https://aclanthology.org/2020.
findings-emnlp.148 .
Bertsekas, D. P. Auction algorithms for network ﬂow prob-
lems: A tutorial introduction. Computational Optimiza-
tion and Applications , 1:7–66, 1992.
Borzunov, A., Baranchuk, D., Dettmers, T., Ryabinin, M.,
Belkada, Y ., Chumachenko, A., Samygin, P., and Raf-
fel, C. Petals: Collaborative inference and ﬁne-tuning
of large models, 2022. URL https://arxiv.org/
abs/2209.01188 .
Chang, X., Nie, F., Ma, Z., and Yang, Y . Balanced k-means
and min-cut clustering, 2014. URL https://arxiv.
org/abs/1411.6235 .
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton,
C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
S., Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer,
N., Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B.,
Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,
G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,
S., Michalewski, H., Garcia, X., Misra, V ., Robinson,K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,
H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,
Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S.,
Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polo-
zov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz,
M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K.,
Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm:
Scaling language modeling with pathways, 2022. URL
https://arxiv.org/abs/2204.02311 .
Chronopoulou, A., Peters, M., and Dodge, J. Efﬁcient hierar-
chical domain adaptation for pretrained language models.
InProceedings of the 2022 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics: Human Language Technologies , pp. 1336–1351,
Seattle, United States, July 2022. Association for Compu-
tational Linguistics. doi: 10.18653/v1/2022.naacl-main.
96. URL https://aclanthology.org/2022.
naacl-main.96 .
Chronopoulou, A., Peters, M. E., Fraser, A. M., and
Dodge, J. Adaptersoup: Weight averaging to improve
generalization of pretrained language models. ArXiv ,
abs/2302.07027, 2023.
Clark, A., Casas, D. d. l., Guy, A., Mensch, A., Paganini,
M., Hoffmann, J., Damoc, B., Hechtman, B., Cai, T.,
Borgeaud, S., Driessche, G. v. d., Rutherford, E., Henni-
gan, T., Johnson, M., Millican, K., Cassirer, A., Jones,
C., Buchatskaya, E., Budden, D., Sifre, L., Osindero,
S., Vinyals, O., Rae, J., Elsen, E., Kavukcuoglu, K.,
and Simonyan, K. Uniﬁed scaling laws for routed lan-
guage models, 2022. URL https://arxiv.org/
abs/2202.01169 .
Dehghani, M., Arnab, A., Beyer, L., Vaswani, A., and Tay,
Y . The efﬁciency misnomer, 2021. URL https://
arxiv.org/abs/2110.12894 .
Dettmers, T. and Zettlemoyer, L. Sparse networks from
scratch: Faster training without losing performance.
CoRR , abs/1907.04840, 2019. URL http://arxiv.
org/abs/1907.04840 .
Duan, Z., Zhang, H., Wang, C., Wang, Z., Chen, B., and
Zhou, M. Enslm: Ensemble language model for data
diversity by semantic clustering. In Annual Meeting of
the Association for Computational Linguistics , 2021.
Ester, M., Kriegel, H.-P., Sander, J., and Xu, X. A density-
based algorithm for discovering clusters in large spatial
databases with noise. In Proceedings of the Second Inter-
national Conference on Knowledge Discovery and Data
Mining , KDD’96, pp. 226–231. AAAI Press, 1996.
Evci, U., Gale, T., Menick, J., Castro, P. S., and
Elsen, E. Rigging the lottery: Making all tickets

--- PAGE 17 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
winners. In III, H. D. and Singh, A. (eds.), Pro-
ceedings of the 37th International Conference on Ma-
chine Learning , volume 119 of Proceedings of Machine
Learning Research , pp. 2943–2952. PMLR, 13–18 Jul
2020. URL https://proceedings.mlr.press/
v119/evci20a.html .
Fedus, W., Zoph, B., and Shazeer, N. Switch transform-
ers: Scaling to trillion parameter models with simple and
efﬁcient sparsity. J. Mach. Learn. Res , 23:1–40, 2021.
Fedus, W., Dean, J., and Zoph, B. A review of sparse
expert models in deep learning, 2022. URL https:
//arxiv.org/abs/2209.01667 .
Gan, S., Lian, X., Wang, R., Chang, J., Liu, C., Shi, H.,
Zhang, S., Li, X., Sun, T., Jiang, J., Yuan, B., Yang, S.,
Liu, J., and Zhang, C. Bagua: Scaling up distributed
learning with system relaxations, 2021. URL https:
//arxiv.org/abs/2107.01499 .
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,
Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,
Presser, S., and Leahy, C. The pile: An 800gb dataset of
diverse text for language modeling, 2021. URL https:
//arxiv.org/abs/2101.00027 .
Gross, S., Ranzato, M., and Szlam, A. Hard mixtures of
experts for large scale weakly supervised vision. In Pro-
ceedings of the IEEE Conference on Computer Vision
and Pattern Recognition , pp. 6865–6873, 2017.
Gururangan, S., Lewis, M., Holtzman, A., Smith, N. A.,
and Zettlemoyer, L. DEMix layers: Disentangling do-
mains for modular language modeling. In Proceedings
of the 2022 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pp. 5557–5576, Seat-
tle, United States, July 2022. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2022.naacl-main.
407. URL https://aclanthology.org/2022.
naacl-main.407 .
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,
Cai, T., Rutherford, E., de Las Casas, D., Hendricks,
L. A., Welbl, J., Clark, A., et al. Training compute-
optimal large language models. 2022.
Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E.
Adaptive mixtures of local experts. Neural computation ,
3(1):79–87, 1991.
Jang, J., Kim, S., Ye, S., Kim, D., Logeswaran, L., Lee, M.,
Lee, K., and Seo, M. Exploring the beneﬁts of training
expert language models over instruction tuning, 2023.
URL https://arxiv.org/abs/2302.03202 .Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L.,
and Lewis, M. Generalization through memorization:
Nearest neighbor language models, 2019. URL https:
//arxiv.org/abs/1911.00172 .
Komatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz, C. R.,
Mustafa, B., Ainslie, J., Tay, Y ., Dehghani, M., and
Houlsby, N. Sparse upcycling: Training mixture-of-
experts from dense checkpoints, 2022. URL https:
//arxiv.org/abs/2212.05055 .
Kudugunta, S., Huang, Y ., Bapna, A., Krikun, M., Lepikhin,
D., Luong, M.-T., and Firat, O. Beyond distillation: Task-
level mixture-of-experts for efﬁcient inference, 2021.
URL https://arxiv.org/abs/2110.03742 .
Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas,
D., Mendes, P., Hellmann, S., Morsey, M., Van Kleef, P.,
Auer, S., and Bizer, C. Dbpedia - a large-scale, multilin-
gual knowledge base extracted from wikipedia. Semantic
Web Journal , 6, 01 2014. doi: 10.3233/SW-140134.
Lepikhin, D., Lee, H., Xu, Y ., Chen, D., Firat, O., Huang, Y .,
Krikun, M., Shazeer, N., and Chen, Z. {GS}hard: Scal-
ing giant models with conditional computation and auto-
matic sharding. In International Conference on Learning
Representations , 2021. URL https://openreview.
net/forum?id=qrwe7XHTmYb .
Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettle-
moyer, L. Base layers: Simplifying training of large,
sparse models, 2021. URL https://arxiv.org/
abs/2103.16716 .
Li, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T.,
Smith, N. A., and Zettlemoyer, L. Branch-train-merge:
Embarrassingly parallel training of expert language mod-
els, 2022. URL https://arxiv.org/abs/2208.
03306 .
Lo, K., Wang, L. L., Neumann, M., Kinney, R., and Weld,
D. S. S2orc: The semantic scholar open research cor-
pus, 2019. URL https://arxiv.org/abs/1911.
02782 .
Lu, Y ., Bartolo, M., Moore, A., Riedel, S., and Stene-
torp, P. Fantastically ordered prompts and where to
ﬁnd them: Overcoming few-shot prompt order sensi-
tivity. In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pp. 8086–8098, Dublin, Ireland,
May 2022. Association for Computational Linguistics.
doi: 10.18653/v1/2022.acl-long.556. URL https:
//aclanthology.org/2022.acl-long.556 .
Lucy, L. and Bamman, D. Characterizing English vari-
ation across social media communities with BERT.

--- PAGE 18 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Transactions of the Association for Computational
Linguistics , 9:538–556, 2021. doi: 10.1162/tacl_
a_00383. URL https://aclanthology.org/
2021.tacl-1.33 .
Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng,
A. Y ., and Potts, C. Learning word vectors for sentiment
analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies , pp. 142–150, Portland, Ore-
gon, USA, June 2011. Association for Computational
Linguistics. URL https://aclanthology.org/
P11-1015 .
Malinen, M. I. and Fränti, P. Balanced k-means for cluster-
ing. In International Workshop on Structural and Syntac-
tic Pattern Recognition , 2014.
Malo, P., Sinha, A., Korhonen, P., Wallenius, J., and Takala,
P. Good debt or bad debt: Detecting semantic orienta-
tions in economic texts. Journal of the Association for
Information Science and Technology , 65, 2014.
McCandlish, S., Kaplan, J., Amodei, D., and Team, O. D.
An empirical model of large-batch training, 2018. URL
https://arxiv.org/abs/1812.06162 .
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,
Hajishirzi, H., and Zettlemoyer, L. Rethinking the
role of demonstrations: What makes in-context learn-
ing work? In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing , pp.
11048–11064, Abu Dhabi, United Arab Emirates, De-
cember 2022. Association for Computational Linguis-
tics. URL https://aclanthology.org/2022.
emnlp-main.759 .
Mostafa, H. and Wang, X. Parameter efﬁcient training of
deep convolutional neural networks by dynamic sparse
reparameterization. In Chaudhuri, K. and Salakhutdinov,
R. (eds.), Proceedings of the 36th International Confer-
ence on Machine Learning , volume 97 of Proceedings of
Machine Learning Research , pp. 4646–4655. PMLR, 09–
15 Jun 2019. URL https://proceedings.mlr.
press/v97/mostafa19a.html .
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng,
N., Grangier, D., and Auli, M. fairseq: A fast, extensible
toolkit for sequence modeling, 2019. URL https://
arxiv.org/abs/1904.01038 .
Pfeiffer, J., Goyal, N., Lin, X., Li, X., Cross, J., Riedel, S.,
and Artetxe, M. Lifting the curse of multilinguality by pre-
training modular transformers. In Proceedings of the 2022
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Language
Technologies , pp. 3479–3495, Seattle, United States, July2022. Association for Computational Linguistics. doi:
10.18653/v1/2022.naacl-main.255. URL https://
aclanthology.org/2022.naacl-main.255 .
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and
Sutskever, I. Language models are unsupervised multitask
learners. 2019.
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoff-
mann, J., Song, F., Aslanides, J., Henderson, S., Ring,
R., Young, S., Rutherford, E., Hennigan, T., Menick,
J., Cassirer, A., Powell, R., Driessche, G. v. d., Hen-
dricks, L. A., Rauh, M., Huang, P.-S., Glaese, A., Welbl,
J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Hig-
gins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E.,
Jayakumar, S., Buchatskaya, E., Budden, D., Sutherland,
E., Simonyan, K., Paganini, M., Sifre, L., Martens, L.,
Li, X. L., Kuncoro, A., Nematzadeh, A., Gribovskaya,
E., Donato, D., Lazaridou, A., Mensch, A., Lespiau,
J.-B., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sotti-
aux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama,
D., d’Autume, C. d. M., Li, Y ., Terzi, T., Mikulik, V .,
Babuschkin, I., Clark, A., Casas, D. d. L., Guy, A.,
Jones, C., Bradbury, J., Johnson, M., Hechtman, B., Wei-
dinger, L., Gabriel, I., Isaac, W., Lockhart, E., Osin-
dero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K.,
Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu,
K., and Irving, G. Scaling language models: Methods,
analysis & insights from training gopher, 2021. URL
https://arxiv.org/abs/2112.11446 .
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Explor-
ing the limits of transfer learning with a uniﬁed text-to-
text transformer, 2019. URL https://arxiv.org/
abs/1910.10683 .
Roller, S., Sukhbaatar, S., Szlam, A., and Weston, J. E.
Hash layers for large sparse models. In Beygelzimer,
A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.),
Advances in Neural Information Processing Systems ,
2021. URL https://openreview.net/forum?
id=lMgDDWb1ULW .
Ryabinin, M., Dettmers, T., Diskin, M., and Borzunov, A.
Swarm parallelism: Training large models can be sur-
prisingly communication-efﬁcient, 2023. URL https:
//arxiv.org/abs/2301.11913 .
Shallue, C. J., Lee, J., Antognini, J., Sohl-Dickstein, J.,
Frostig, R., and Dahl, G. E. Measuring the effects of
data parallelism on neural network training. 2018. doi:
10.48550/ARXIV .1811.03600. URL https://arxiv.
org/abs/1811.03600 .

--- PAGE 19 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Shi, W., Michael, J., Gururangan, S., and Zettlemoyer, L.
knn-prompt: Nearest neighbor zero-shot inference, 2022.
URL https://arxiv.org/abs/2205.13792 .
Svenstrup, D., Hansen, J. M., and Winther, O. Hash em-
beddings for efﬁcient word representations, 2017. URL
https://arxiv.org/abs/1709.03933 .
Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn,
A., Saravia, E., Poulton, A., Kerkez, V ., and Stojnic, R.
Galactica: A large language model for science, 2022.
URL https://arxiv.org/abs/2211.09085 .
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro,
E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and
Lample, G. Llama: Open and efﬁcient foundation lan-
guage models, 2023. URL https://arxiv.org/
abs/2302.13971 .
Wang, J., Yuan, B., Rimanic, L., He, Y ., Dao, T., Chen,
B., Re, C., and Zhang, C. Fine-tuning language models
over slow networks using activation compression with
guarantees, 2022. URL https://arxiv.org/abs/
2206.01299 .
Wortsman, M., Gururangan, S., Li, S., Farhadi, A., Schmidt,
L., Rabbat, M., and Morcos, A. S. lo-ﬁ: distributed
ﬁne-tuning without communication, 2022. URL https:
//arxiv.org/abs/2210.11948 .
Yang, G., Hu, E., Babuschkin, I., Sidor, S., Liu, X.,
Farhi, D., Ryder, N., Pachocki, J., Chen, W., and
Gao, J. Tuning large neural networks via zero-shot
hyperparameter transfer. In Ranzato, M., Beygelzimer,
A., Dauphin, Y ., Liang, P., and Vaughan, J. W. (eds.),
Advances in Neural Information Processing Systems ,
volume 34, pp. 17084–17097. Curran Associates,
Inc., 2021. URL https://proceedings.
neurips.cc/paper/2021/file/
8df7c2e3c3c3be098ef7b382bd2c37ba-Paper.
pdf.
Yuan, B., He, Y ., Davis, J. Q., Zhang, T., Dao, T., Chen, B.,
Liang, P., Re, C., and Zhang, C. Decentralized training of
foundation models in heterogeneous environments, 2022.
URL https://arxiv.org/abs/2206.01288 .
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V ., Mi-
haylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D.,
Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer,
L. Opt: Open pre-trained transformer language mod-
els, 2022. URL https://arxiv.org/abs/2205.
01068 .
Zhang, X., Zhao, J., and LeCun, Y . Character-level convo-
lutional networks for text classiﬁcation, 2016.Zhou, Y ., Lei, T., Liu, H., Du, N., Huang, Y ., Zhao, V .,
Dai, A., Chen, Z., Le, Q., and Laudon, J. Mixture-of-
experts with expert choice routing, 2022. URL https:
//arxiv.org/abs/2202.09368 .

--- PAGE 20 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
A. Appendix
room, home, kitchenpark, hotel, beachart, design, imagemusic, album, bandsex, women, datingcar, vehicle, roadmarket, company, yearuniversity, research, developmentpolitics, president, saidschool, student, educationday, event, yearbusiness, marketing, management
books, read, writingreligion, life, peoplegames, team, seasonfood, recipe, minutespatients, treatment, bodyhealth, care, childrenlove, great, likejust, like, knowblack, white, colorfilm, movie, storyproducts, online, qualityFile, window, clickdata, information, softwareemail, contact, comColossal Cleaned Crawled Corpus (C4)
language, words, speechstudents, learning, teacherssocial, political, peoplebook, research, socialuser, data, informationnetwork, node, algorithmsalgorithm, graph, verticesimage, object, segmentationtraining, learning, datatheorem, let, proofwater, soil, dataprotein, gene, DNAquark, mass, Higgsstars, galaxies, universeequation, field, theoryflow, velocity, fieldstress, strain, surfacequantum, spin, statesurface, nm, filmlaser, beam, opticalparticipant, task, visualpower, noise, channelchildren, women, healthpatient, treatment, diseasestudy, risk, age
cell, expression, miceSemantic Scholar Research Corpus (S2ORC)
Figure 17. Fully annotated UMAP visualization of 32 clusters in C4 and S2ORC. We annotate the clusters using an inverse transformation
from our cluster centers back into the tf-idf vocabulary space, identifying the most likely words to generate the cluster center. We display
the top 3 terms associated with each cluster center here.

--- PAGE 21 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
A.1. Clusters
Cluster Term 0 Term 1 Term 2 Term 3 Term 4
0 ﬁg energy al et ﬁeld
1 data patients social time al
Table 2. Top terms associated with each cluster in S2ORC (2 clusters)
Cluster Term 0 Term 1 Term 2 Term 3 Term 4
0 eq energy quantum equation ﬁeld
1 algorithm image model data noise
2 cells cell ﬁg al protein
3 students social people research education
4 patients patient study participants children
5 ﬁg temperature energy surface beam
6 user data node network nodes
7 et al stars galaxies velocity
Table 3. Top terms associated with each cluster in S2ORC (8 clusters)
Cluster Term 0 Term 1 Term 2 Term 3 Term 4
0 students learning teachers teaching education
1 language word words speech english
2 network node nodes networks algorithm
3 study risk age data group
4 power noise channel signal frequency
5 quantum spin state magnetic states
6 participants task visual stimulus et
7 theorem let proof lemma set
8 training learning data network model
9 laser beam optical ﬁg pulse
10 protein genes gene dna proteins
11 water soil data et al
12 algorithm graph vertices problem vertex
13 ﬂow velocity ﬁeld magnetic wave
14 user data users information service
15 stars galaxies et al star
16 quark mass gev higgs energy
17 et al brain neurons ﬁg
18 stress strain surface shear ﬁg
19 image images algorithm object segmentation
20 energy electron ﬁg eq state
21 equation ﬁeld eq equations theory
22 patients patient treatment study disease
23 model time robot state control
24 children child women health social
25 surface nm ﬁlm layer graphene
26 patient patients pain surgery treatment
27 social political people policy public
28 social participants health self people
29 book research social information data
30 temperature water heat phase thermal
31 cells cell expression mice ﬁg
Table 4. Top terms associated with each cluster in S2ORC (32 Clusters).

--- PAGE 22 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Cluster Term 0 Term 1 Term 2 Term 3 Term 4
0 like just time new great
1 new information use business services
Table 5. Top terms associated with each cluster in C4 (2 clusters)
Cluster Term 0 Term 1 Term 2 Term 3 Term 4
0 students health research school university
1 music new love ﬁlm art
2 said year state team new
3 business company services service market
4 like just time don really
5 use information data page click
6 design black white color high
7 home water food park area
Table 6. Top terms associated with each cluster in C4 (8 clusters)
Cluster Term 0 Term 1 Term 2 Term 3 Term 4
0 site page website web search
1 art design image images gallery
2 health care children child medical
3 just like don know ve
4 data information software use management
5 love great like just really
6 game team games season play
7 information email contact com address
8 power high use light steel
9 students school student education learning
10 market company year ﬁnancial tax
11 patients treatment body cancer pain
12 room home kitchen bedroom living
13 music album band song songs
14 car vehicle cars new road
15 park hotel beach area travel
16 day event year time wedding
17 service services quality customer best
18 book books read writing story
19 ﬁlm movie story new like
20 black white color dress look
21 business company marketing management customers
22 university research development education science
23 city community county said police
24 sex women porn dating girls
25 products product online quality order
26 religion life people jesus time
27 water skin use oil like
28 said politics president government state
29 app phone video mobile casino
30 ﬁle windows use click software
31 food add recipe minutes wine
Table 7. Top terms associated with each cluster in C4 (32 Clusters).
A.2. Comparing FLOP counts via training data size
To make fair comparisons across models with different numbers of ELMs, for a given number of clusters kand total GPU
budget n, each ELM is allocated n=k GPUs. This keeps the total effective number of FLOPs ﬁxed across models exposed to

--- PAGE 23 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
the same number of tokens. We can show this analytically; following Artetxe et al. 2021, we calculate the number of FLOPs
to train a single ELM in our experiments:
FELM (T;k)=96lh2T
k
1 +s
6h+V
16lh
where Tis the total training tokens (i.e., sequence length batch size per GPU number of GPUs), kis the number of
clusters, lis the number of layers, his the hidden dimension, sis the sequence length, and Vis the vocabulary.
Therefore, the total cost in FLOPs to train kELMs with a particular model architecture (e.g., OPT-1.3B) on Ttokens in
aggregate is equivalent to that of a single dense LM of the same architecture trained on Ttokens:
kFELM (T;k)=FELM(T, 1)
This means that even though C-BTM trains ktimes more parameters than an equivalent dense model, it does so at the
same overall cost in FLOPs . So, our comparisons of models with various numbers of ELMs are fair, as long as they have
been exposed to the same number of training tokens and have the same underlying architecture for each ELM. Therefore,
throughout the paper, we use training data size as a more interpretable metric of the overall compute budget.
A.3. Interpolating between empirical observations when comparing training costs and performance
We interpolate between our empirical observations using the following function, proposed in Artetxe et al. 2021:
c(t) = exp(log clo(t) +r(logchi(t) logclo(t)))
where r=t tlo
thi tlo,thiandtloare the closest empirical performances to tandclo(t)andchi(t)are their corresponding
training cost in FLOPs. We use this interpolation to compute the speedup factor cdense (t)=ccbtm(t).
A.4. Effect of cluster balancing on cluster sizes
Using a held out set of 10K documents from C4, we ablate our balancing procedure from §2.1, and display a boxplot
showing cluster sizes in Figure 18.
Balanced Unbalanced
Clustering type02004006008001000Cluster size
Figure 18. Cluster balancing narrows the range, and increases the median size, of clusters (§7.2). Here, we ablate our balancing
procedure from §2.1 on a 10K held-out documents in C4.
A.5. The Pile experiments
For the experiment in §7.4, we additionally train on The Pile, which is a publicly available corpus of diverse language
modeling datasets. We use the ﬁltered version from the OPT pretraining data (Zhang et al., 2022), and subselect 8 datasets
of the 13 used for OPT pretraining, which enabled easier experimentation §7.4. These 8 datasets include: Common

--- PAGE 24 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Crawl, HackerNews comments, Reddit comments16, the Gutenberg corpus, STORIES corpus, OpenWebText2, Deepmind-
Mathematics, and English Wikipedia. In aggregate, these datasets consist of 420M documents, totaling 116B BPE tokens.
For evaluation, we sample an equal number of documents from each constituent dataset. We also train a k=8 clustering
model on this dataset with one shard, or 1.5M documents.
5.2B Tokens
8 Metadata 8 Clusters 1 Cluster
8.4 8.3 8.5
Table 8. Experts trained with clusters perform slightly better than experts trained with metadata (§7.4). Here, we train each model
for 5.2B tokens on 8 corpora of the Pile, and evaluate perplexity on a held out random sample of the constituent datasets. See §A.5 for
more details on the dataset.
A.6. Sparsity
Dense 2-cluster 8-cluster 32-cluster
top-1 top-2 top-1 top-2 top-4 top-8 top-1 top-2 top-4 top-8 top-16 top-32TEMPERATURE0.01 13.82 13.64 13.60 13.64 13.50 13.49 13.49 13.55 13.45 13.45 13.45 13.45 13.45
0.05 - - 13.51 - 13.32 13.25 13.25 - 13.25 13.19 13.17 13.17 13.17
0.1 - - 13.50 - 13.27 13.22 13.24 - 13.16 13.05 13.00 13.01 13.01
0.2 - - 13.54 - 13.29 13.34 13.50 - 13.14 13.06 13.07 13.07 13.28
0.3 - - 13.59 - 13.33 13.45 13.72 - 13.17 13.15 13.27 13.54 13.78
0.5 - - 13.64 - 13.38 13.59 13.97 - 13.22 13.30 13.57 14.02 14.46
1 - - 13.69 - 13.43 13.73 14.19 - 13.30 13.49 13.89 14.46 15.04
Table 9. Results with the dense (1-cluster), 2-cluster, 8-cluster, and 32-cluster C4 models trained on 84B tokens when varying the
temperature ( T) andtopk hyperparameters. Optimal performance for almost every model and topk value is achieved at T= 0:1.
A.7. Downstream tasks
Tasks We experiment with six text classiﬁcation tasks which span topic classiﬁcation (AGNews; Zhang et al. 2016 and
DBPedia; Lehmann et al. 2014); sentiment analysis (SST-2; Maas et al. 2011, Amazon; Zhang et al. 2016, and Phrasebank
Malo et al. 2014); and hatespeech detection (Twitter; Barbieri et al. 2020).
Performance Routing We introduce additional routing procedures for few-shot text classiﬁcation, as the clustering
method described in §2.2 ignores the signiﬁcance of labels and does not take into account the context’s word order, which
may be crucial when the context contains demonstrations for a downstream task. Further, the speciﬁc ordering of in-context
demonstrations is known to affect model performance (Lu et al., 2022). There is not sufﬁcient evidence to suggest that the
relative rank of different models on a task stays constant through these performance variances; thus it may be important
to route to experts differently depending on demonstration example order. To take into account the order of tokens in the
context, we introduce 3 variations on routing, based on expert performance on the end task, which take inspiration from the
mixture probability estimation methods of Gururangan et al. (2022); Li et al. (2022).
To perform Fixed Performance Routing with demonstrations and validation set examples , we select 8 examples randomly
from the validation set, such that there is no overlap with the 8 demonstration examples used in the context at test time. We
concatenate the 8 demonstrations with one validation set context and evaluate the accuracy of each ELM on the sequence,
repeating for each of the 8 examples from the validation set. The ﬁnal routing probability distribution over experts for this
task is determined by a softmax over the average accuracy of the model over the 8 examples. We use this ﬁxed probability
distribution for all test examples.
To perform Fixed Performance Routing with only demonstrations , we adapt the procedure above such that no validation
examples are necessary. Instead, we take a random permutation of the 8 demonstration examples. We remove the label of
the last, such that we effectively use the ﬁrst 7 as demonstrations, and rely on the last example to estimate performance.
16Reddit comments, like the rest of these datasets, are not collected by us but were part of the Pile (Gao et al., 2021), a publicly available
third-party dataset.

--- PAGE 25 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Task 1st 2nd 3rd
AGNews game, team, season said, government, president business, company, market
DBPedia students, school, university said, government, president city, park, hotel
SST-2 book, ﬁlm, life love, family, great music, art, band
Amazon book, ﬁlm, life just, like, know game, team, season
Phrasebank business, company, market service, customer, products said, government, president
Twitter data, software, download click, website, page just, like, know
Table 10. Top-terms of clusters associated with top-3 experts for each classiﬁcation task (§5.2) By inspection, the highest probability
experts are usually quite relevant to task’s domain.
We repeat this 8 times, generating a new random permutation each time, and once again take the softmax over the average
accuracy of the model for each permutation, ﬁxing this distribution for all test examples.
Finally, we have Updating Performance Routing , in which no estimations are done before test-time. At test time, we begin
with a uniform probability over all experts, which we update with an exponential moving average with each test example:
after each example (prepended with the 8 demonstrations), we update the expert probabilities with the softmax over the
accuracy of each expert on that example. Once again, this distribution is ﬁxed for all test examples.
Results Full results, in Table 11, show that Fixed Performance Routing with demonstrations and validation set examples
achieves the best performance overall, with optimal performance occurring at top-4 expert activation, which we also found in
the language modeling results of §4.1. Both Fixed Performance Routing methods perform best with top-4 expert activation,
and only suffer small performance degradations when reduced to top-1 expert activation. This aligns well with the patterns
observed in §4.1, which further supports the incorporation of end task performance in routing when adapting to new tasks,
even when we base this evaluation only on the demonstration examples – that is, without any additional data. We leave for
future work further tuning of the optimal settings for Performance Routing.

--- PAGE 26 ---
Scaling Expert Language Models with Unsupervised Domain Discovery
Few-shot Text Classiﬁcation Accuracy (%)
AGNews DBPedia SST-2 Amazon Phrasebank Twitter
#Model (inference parameters) Topic Topic Sentiment Sentiment Sentiment Hatespeech Average
Random chance 25.0 7.10 50.0 50.0 33.3 50.0 35.9
OPT (1.3B) 42.9 57.2 72.8 81.3 72.5 65.1 65.3
OPT (6.7B) 51.9 58.9 77.0 83.8 76.4 39.6 64.6
1-cluster (1.3B) 47.4 61.1 80.2 80.7 66.6 60.9 66.2
1-cluster (6.7B) 68.1 62.4 80.7 84.9 80.6 37.4 69.0
Cluster Routing
16-cluster; top-1 (1.3B) 47.1 62.9 74.3 79.1 72.9 56.4 65.4
16-cluster; top-4 (5.2B) 49.3 62.3 80.0 81.3 78.7 61.3 68.8
16-cluster; top-16 (20.8B) 50.6 62.0 84.0 83.2 78.6 61.7 69.9
Updating Performance Routing
16-cluster; top-1 (1.3B) 54.5 63.4 83.4 83.6 74.7 64.8 63.3
16-cluster; top-4 (5.2B) 51.6 61.5 88.6 83.7 80.7 65.3 68.8
16-cluster; top-16 (20.8B) 51.1 60.4 86.0 83.5 79.8 62.9 69.1
Fixed Performance Routing (8 demonstrations)
16-cluster; top-1 (1.3B) 45.3 61.9 81.2 83.6 76.4 60.1 68.1
16-cluster; top-4 (5.2B) 51.2 60.9 81.4 83.0 80.5 60.9 69.6
16-cluster; top-16 (20.8B) 50.6 60.2 84.1 83.5 79.1 60.5 69.6
Fixed Performance Routing (8 demonstrations + 8 validation examples)
16-cluster; top-1 (1.3B) 54.5 63.4 83.4 83.6 74.7 64.8 70.7
16-cluster; top-4 (5.2B) 51.6 61.5 88.6 83.7 80.7 65.3 71.9
16-cluster; top-16 (20.8B) 51.1 60.4 86.0 83.5 79.8 62.9 70.6
Table 11. C-BTM models with performance routing achieve even better performance on downstream tasks (§A.7). We display
performance of models on six text classiﬁcation tasks, using eight demonstrations for each example and no additional ﬁne-tuning. We
compare our cluster routing method (described in §5) to variants of performance routing (described in §A.7). Fixed performance routing
with 8 demonstrations and 8 validation examples usually gets the best performance on downstream tasks, consistently outperforming even
the 6.7B parameter baselines. Fixed performance routing with top-4 inference always improves performance over using all experts, and
top-1 inference does substantially better than the dense baselines at no additional inference costs. We include average performance across
tasks for readability.
