# SkillNet-NLU: Một Mô hình Kích hoạt Thưa thớt cho Hiểu Ngôn ngữ Tự nhiên Đa mục đích

Fan Zhang, Duyu Tang, Yong Dai, Cong Zhou, Shuangzhi Wu và Shuming Shi
Tencent AI Lab

## Tóm tắt

Các mô hình sâu phổ biến hiện tại có đơn mục đích và quá chuyên biệt hóa ở các tác vụ riêng lẻ. Tuy nhiên, khi được mở rộng sang các tác vụ mới, chúng thường quên đi các kỹ năng đã học trước đó và học lại từ đầu. Chúng tôi giải quyết vấn đề này bằng cách giới thiệu SkillNet-NLU, một mô hình đa mục đích kết hợp các kỹ năng hiện có để học các tác vụ mới một cách hiệu quả hơn. Đặc điểm chính của phương pháp của chúng tôi là nó được kích hoạt thưa thớt được hướng dẫn bởi các kỹ năng được định nghĩa trước. Khác với các mô hình dày đặc truyền thống luôn kích hoạt toàn bộ các tham số mô hình, SkillNet-NLU chỉ kích hoạt các phần tham số mô hình có kỹ năng liên quan đến tác vụ mục tiêu. Khi học cho một tác vụ mới, phương pháp của chúng tôi kích hoạt chính xác các kỹ năng cần thiết và cũng cung cấp tùy chọn thêm kỹ năng mới. Chúng tôi đánh giá trên các tác vụ hiểu ngôn ngữ tự nhiên và có những phát hiện sau. Thứ nhất, chỉ với một checkpoint mô hình, SkillNet-NLU hoạt động tốt hơn so với tinh chỉnh đặc thù tác vụ và hai baseline học đa tác vụ (tức là mô hình dày đặc và mô hình Mixture-of-Experts) trên sáu tác vụ. Thứ hai, tiền huấn luyện kích hoạt thưa thớt cải thiện thêm hiệu suất tổng thể. Thứ ba, SkillNet-NLU vượt trội đáng kể so với các hệ thống baseline khi được mở rộng sang các tác vụ mới.

## 1 Giới thiệu

Những năm gần đây đã chứng kiến sự thành công của các mô hình đồng nhất dựa trên Transformer (Vaswani et al., 2017) và các mô hình được tiền huấn luyện (Devlin et al., 2018) trong trí tuệ nhân tạo và xử lý ngôn ngữ tự nhiên. Nhiều nghiên cứu trước đây sử dụng các mô hình mạng neuron tương tự và lặp lại cùng một quy trình: học từ đầu và tinh chỉnh tất cả các tham số mô hình cho một tác vụ cô lập. Tuy nhiên, điều này khác với việc học của con người ở hai khía cạnh. Thứ nhất, con người chúng ta không quên tất cả những gì đã học và bắt đầu học kỹ năng mới từ con số không. Thay vào đó, chúng ta kết hợp các kỹ năng hiện có để học các kỹ năng mới nhanh hơn. Thứ hai, chúng ta có khoảng 100 tỷ neuron trong não và các phần khác nhau được chuyên biệt hóa cho các kỹ năng khác nhau. Khi giải quyết một vấn đề, chúng ta không kích hoạt tất cả các neuron mà chỉ gọi các phần liên quan.

Trong nghiên cứu này, chúng tôi trình bày một phương pháp để giải quyết các vấn đề nêu trên. Mục tiêu của chúng tôi là tiến từ các mô hình đơn mục đích sang các mô hình đa mục đích và từ các mô hình dày đặc sang các mô hình thưa thớt. Cụ thể, chúng tôi lấy hiểu ngôn ngữ tự nhiên (NLU) làm nghiên cứu trường hợp và trình bày một mô hình kích hoạt thưa thớt có khả năng tổng quát hóa trên nhiều tác vụ NLU khác nhau. Đặc điểm chính của phương pháp chúng tôi là nó bao gồm một tập hợp các "mô-đun kỹ năng" có tham số có thể tái sử dụng, mỗi mô-đun tương ứng với một kỹ năng như kỹ năng hiểu cảm xúc của văn bản, kỹ năng hiểu câu hỏi ngôn ngữ tự nhiên, kỹ năng hiểu ý nghĩa của văn bản trong lĩnh vực tài chính, v.v. Khác với các mô hình dày đặc truyền thống luôn kích hoạt tất cả các tham số mô hình, phương pháp của chúng tôi kích hoạt thưa thớt các phần tham số mô hình, đồng thời vô hiệu hóa các mô-đun có kỹ năng không liên quan đến tác vụ.

Hãy sử dụng ba ví dụ cụ thể để minh họa cách mô hình của chúng tôi được kích hoạt thưa thớt khi được áp dụng trong các tác vụ downstream. Giả sử chúng ta đã định nghĩa bảy kỹ năng, với các định nghĩa được đưa ra trong Bảng 1. Đối với tác vụ phân loại văn bản, chỉ cần khả năng lấy biểu diễn ngữ nghĩa của một chuỗi (tức là s1). Do đó, chỉ các tham số liên quan đến s1 và s7 được kích hoạt, như được hiển thị trong Hình 1(a). So với phân loại văn bản, phân loại cảm xúc cần thêm kỹ năng hiểu cảm xúc của văn bản.

Chúng tôi định nghĩa một kỹ năng tổng quát s7, luôn được kích hoạt như kỹ năng mặc định. Thiết kế này nhằm cung cấp phương án dự phòng để xử lý các tác vụ mới yêu cầu kỹ năng hoàn toàn chưa thấy.

Do đó, s1, s4 và s7 được kích hoạt, như được đưa ra trong Hình 1(b). Đối với tác vụ đọc hiểu máy, các mô hình cần hiểu ý nghĩa của câu hỏi (s5), hiểu cách câu hỏi và đoạn văn tương tác (s3) và lấy biểu diễn của mỗi token (s2). Do đó, s2, s3, s5 và s7 được kích hoạt, như được hiển thị trong Hình 1(c).

Chúng tôi tóm tắt ngắn gọn cách SkillNet-NLU khác biệt với cả phương pháp học đa tác vụ và phương pháp Mixture-of-Experts (MoE) như sau:

1. Các phương pháp học đa tác vụ (Liu et al., 2019) thường có một lớp biểu diễn đặc trưng chia sẻ (ví dụ: Transformer) cộng với nhiều lớp dự đoán đặc thù tác vụ. Không rõ loại kiến thức hoặc kỹ năng nào được học trong lớp biểu diễn đặc trưng. Khác với các phương pháp học đa tác vụ, SkillNet-NLU bao gồm nhiều mô-đun kỹ năng với định nghĩa rõ ràng. Các mô-đun kỹ năng được kích hoạt thưa thớt tùy thuộc vào sự cần thiết đối với tác vụ. Theo trực giác, SkillNet-NLU không quá chuyên biệt hóa ở cấp độ tác vụ, mà ở cấp độ kỹ năng vốn có thông qua việc học cách mỗi mô-đun kỹ năng hoạt động và cách nhiều mô-đun kỹ năng được kết hợp để giải quyết vấn đề. Chúng tôi tin rằng SkillNet-NLU tổng quát hóa tốt hơn cho các tác vụ mới với định nghĩa tác vụ không lường trước được trong tương lai.

2. Các phương pháp MoE thường bao gồm nhiều mô-đun neuron đồng nhất (được gọi là experts) song song, như được đưa ra trong Hình 1(d), và kích hoạt đầy đủ tất cả các experts hoặc kích hoạt từng phần một số experts được hướng dẫn bởi một mô-đun cổng có tham số bổ sung (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021; Du et al., 2021). Tuy nhiên, loại kiến thức được học trong mỗi expert là mơ hồ và lý do tại sao một số experts được kích hoạt là không thể diễn giải. Trong SkillNet-NLU, định nghĩa của mỗi mô-đun kỹ năng rõ ràng và lý do để một mô-đun kỹ năng được kích hoạt là kỹ năng đó cần thiết (được đánh giá bởi các nhà phát triển hoặc người dùng con người) để giải quyết tác vụ.

Chúng tôi sử dụng Transformer (Vaswani et al., 2017) và BERT (Devlin et al., 2018) làm xương sống để phát triển hệ thống của mình. Transformer là một kiến trúc mô hình được sử dụng phổ biến với nhiều lớp và mỗi lớp bao gồm một mạng attention đa đầu theo sau bởi một mạng neuron feedforward (FFN). Có nhiều cách khác nhau để triển khai SkillNet-NLU, và mục tiêu của chúng tôi là chứng minh rằng một triển khai đơn giản hoạt động tốt trong thực tế. Cụ thể, chúng tôi triển khai các mô-đun kỹ năng như các mạng FFN đồng nhất. Một mô-đun kỹ năng chỉ được kích hoạt nếu kỹ năng đó liên quan đến tác vụ hiện tại. Mô hình của chúng tôi không chỉ hỗ trợ tinh chỉnh kích hoạt thưa thớt mà còn có thể được tiền huấn luyện theo cách thưa thớt tương tự thông qua masked language modeling và next sentence prediction.

Ngoại lệ là một nghiên cứu gần đây về dịch máy nơi các experts được chọn dựa trên ngôn ngữ đích hoặc cặp ngôn ngữ (Kudugunta et al., 2021).

Chúng tôi tiến hành thí nghiệm trên các tác vụ hiểu ngôn ngữ tự nhiên tiếng Trung. Kết quả thí nghiệm trên sáu tác vụ (bao gồm phân loại cảm xúc, suy luận ngôn ngữ tự nhiên, tương tự ngữ nghĩa, phân loại văn bản, nhận dạng thực thể có tên và đọc hiểu máy) cho thấy rằng, chỉ với một checkpoint mô hình, phương pháp của chúng tôi hoạt động tốt hơn so với tinh chỉnh đặc thù tác vụ và hai baseline học đa tác vụ: mô hình dày đặc và mô hình Mixture-of-Experts. Hơn nữa, sau khi được tiền huấn luyện theo cách thưa thớt tương tự, hiệu suất tổng thể được cải thiện thêm. Quan trọng hơn, chúng tôi cho thấy rằng khi được mở rộng sang các tác vụ mới, phương pháp của chúng tôi vượt trội đáng kể so với các hệ thống baseline.

## 2 Kiến thức nền tảng

Chúng tôi đưa ra kiến thức nền tảng ngắn gọn về BERT và baseline học đa tác vụ dựa trên BERT tiêu chuẩn.

BERT là một bộ mã hóa dựa trên Transformer (Vaswani et al., 2017). Nó thường được sử dụng trong framework tiền huấn luyện và tinh chỉnh. Các tham số mô hình trước tiên được tiền huấn luyện trên lượng lớn dữ liệu văn bản không có nhãn với các mục tiêu tự giám sát (ví dụ: masked language modeling và next sentence prediction). Sau đó, đối với mỗi tác vụ downstream, các tham số mô hình được tiền huấn luyện được tinh chỉnh thêm trên mỗi dữ liệu đặc thù tác vụ riêng biệt. Nếu có N tác vụ downstream, một giải pháp tiêu chuẩn sẽ tạo ra N mô hình BERT, mỗi mô hình tương ứng với một tác vụ cụ thể.

Vì mô hình BERT nhỏ nhất vẫn có hàng trăm triệu tham số, một cách hiệu quả để tránh triển khai nhiều bản sao của các mô hình lớn trong thực tế là huấn luyện một mô hình đa tác vụ để hỗ trợ nhiều tác vụ downstream. Một phương pháp đa tác vụ tiêu chuẩn (Liu et al., 2019) thêm các lớp dự đoán đặc thù tác vụ khác nhau trên đầu một lớp Transformer chia sẻ. Trong giai đoạn huấn luyện, tất cả các tác vụ được tối ưu hóa cùng nhau. Theo trực giác, lớp Transformer học các biểu diễn đặc trưng tổng quát và mỗi lớp dự đoán học để hoàn thành một tác vụ cụ thể. Trong thực tế, tiến hành vòng thứ hai tinh chỉnh đặc thù tác vụ, tức là tinh chỉnh các tham số mô hình cho mỗi tác vụ riêng biệt (tức là tạo ra N mô hình cho N tác vụ), có thể tạo ra độ chính xác cao hơn. Tuy nhiên, điều này mâu thuẫn với động lực phát triển một mô hình đa mục đích trên nhiều tác vụ. Do đó, chúng tôi không tiến hành vòng thứ hai tinh chỉnh đặc thù tác vụ trong các thí nghiệm của mình.

## 3 SkillNet-NLU

Phần này trình bày SkillNet-NLU của chúng tôi và ứng dụng của nó cho các tác vụ hiểu ngôn ngữ tự nhiên. Đầu tiên, chúng tôi mô tả kiến trúc mô hình (§3.1). Sau đó, chúng tôi trình bày các tác vụ được sử dụng để huấn luyện mô hình (§3.2), cách thực hiện huấn luyện đa tác vụ với SkillNet-NLU (§3.3) và cách mở rộng mô hình sang các tác vụ mới (§3.4). Cuối cùng, chúng tôi cho thấy cách mô hình có thể được tiền huấn luyện với các tham số mô hình được kích hoạt thưa thớt bằng cách sử dụng các mục tiêu học tự giám sát truyền thống (tức là masked language modeling và next sentence prediction) (§3.5).

### 3.1 Kiến trúc mô hình

Có nhiều cách khác nhau để triển khai SkillNet-NLU của chúng tôi. Mục tiêu của nghiên cứu này là chứng minh rằng một triển khai đơn giản và trực quan của ý tưởng hoạt động tốt trong thực tế, và chúng tôi để việc khám phá các kiến trúc mô hình tiên tiến hơn cho tương lai. Cụ thể, chúng tôi xây dựng SkillNet-NLU sử dụng Transformer (Vaswani et al., 2017) và BERT (Devlin et al., 2018) làm xương sống. Vì cả Transformer và BERT đều được áp dụng rộng rãi trong các tác vụ xử lý ngôn ngữ tự nhiên, chúng tôi không trình bày chi tiết và hướng độc giả đến các bài báo gốc.

Transformer là một kiến trúc mô hình được sử dụng phổ biến với nhiều lớp và mỗi lớp bao gồm một mạng attention đa đầu theo sau bởi một mạng neuron feedforward (FFN). Kiến trúc mô hình của chúng tôi sửa đổi mỗi lớp Transformer và thêm các lớp dự đoán đặc thù tác vụ trên đầu các biểu diễn của lớp cuối cùng.

Trong Transformer, như được đưa ra trong Hình 4(a), mỗi lớp bao gồm một mạng attention đa đầu theo sau bởi một mạng neuron feedforward (FFN). Trong SkillNet-NLU, như được hiển thị trong Hình 4(b), chúng tôi có một tập hợp các lớp FFN song song, mỗi lớp đại diện cho một kỹ năng cụ thể (ví dụ: s1 từ Bảng 2). Khi được áp dụng cho một tác vụ, chỉ các lớp FFN tương ứng với các kỹ năng liên quan được kích hoạt. Ví dụ, đối với tác vụ đọc hiểu máy, chỉ s2, s3, s5 và s7 là liên quan, vì vậy các lớp FFN còn lại (tức là s1, s4 và s6) không được kích hoạt. Xét rằng số lượng kỹ năng được kích hoạt thay đổi, chúng tôi tích lũy các vector đầu ra của các lớp FFN kỹ năng được kích hoạt bằng average pooling. Các thao tác còn lại giống như Transformer tiêu chuẩn.

Cụ thể, cho một chuỗi đầu vào x = {x1, ..., xn}, mô hình của chúng tôi trước tiên thực hiện multi-head self-attention cho mỗi token. Sau đó, mỗi mô-đun kỹ năng FNNk từ tập hợp các kỹ năng được kích hoạt S thu được các biểu diễn đặc thù kỹ năng như sau:

hk = FNNk(Self-Attention({x1, ..., xn})) (1)

trong đó k ∈ [1, |S|] chỉ ra mô-đun kỹ năng được kích hoạt thứ k trong S. Ví dụ, đối với tác vụ đọc hiểu máy, như được hiển thị trong Hình 4(c), |S| = 4 và S = {s2, s3, s5, s7}. Cuối cùng, chúng tôi áp dụng average-pooling trên tất cả các biểu diễn đặc thù kỹ năng để tính toán các embedding đầu ra của từ như sau:

v = AvgPool(h1, ..., h|S|) (2)

Các thao tác nêu trên được thực hiện trong nhiều vòng. Embedding của mỗi token được tạo ra bởi lớp cuối cùng được coi là biểu diễn đặc trưng cuối cùng.

### 3.2 Các tác vụ

Chúng tôi sử dụng sáu tác vụ NLU như được đưa ra trong Bảng 2 để huấn luyện mô hình đa tác vụ của chúng tôi.

T1 là phân loại cảm xúc. Cho một chuỗi văn bản (ví dụ: một câu) làm đầu vào, đầu ra là độ phân cực của đầu vào. Chúng tôi sử dụng vector của [CLS] vào một lớp softmax để thực hiện phân loại nhị phân (tức là tích cực vs tiêu cực). T4 có cấu hình tương tự. Chúng tôi kích hoạt s4 bổ sung cho T2 vì nó cần kỹ năng hiểu cảm xúc trong văn bản.

T2 là suy luận ngôn ngữ tự nhiên. Cho hai chuỗi văn bản làm đầu vào, đầu ra là mối quan hệ giữa hai chuỗi như entailment, contradiction, hoặc neutral. Chúng tôi nối hai segment đầu vào với một token [SEP] và sử dụng vector của [CLS] vào một lớp softmax. T3 có cấu hình tương tự. s6 được kích hoạt trong T3 vì nguồn dữ liệu của nó đến từ lĩnh vực tài chính.

T5 là nhận dạng thực thể có tên. Cho một chuỗi từ làm đầu vào, tác vụ là phát hiện xem mỗi từ có phải là thực thể có tên hay không, và nếu có, dự đoán loại thực thể (ví dụ: người, tổ chức, địa điểm, v.v.). Chúng tôi lấy các biểu diễn của mỗi từ từ lớp cuối cùng và đưa chúng vào Conditional Random Fields (CRF) (Lafferty et al., 2001) để dự đoán nhãn cho các từ.

T6 là đọc hiểu máy. Cho một câu hỏi và một đoạn văn làm đầu vào, tác vụ là dự đoán một span từ đoạn văn trả lời câu hỏi. Đầu vào của mô hình là sự nối của câu hỏi và đoạn văn, được phân cách bởi một token [SEP]. Chúng tôi lấy các biểu diễn của các từ từ đoạn văn và dự đoán xem mỗi từ có phải là chỉ số bắt đầu hay chỉ số kết thúc của câu trả lời. Cụ thể, chúng tôi giới thiệu một vector bắt đầu vstart và một vector kết thúc vend. Khi dự đoán xác suất của một token là điểm bắt đầu của span câu trả lời, chúng tôi thực hiện tích vô hướng giữa vector của nó và vstart theo sau bởi softmax trên tất cả các token trong đoạn văn. Công thức tương tự được sử dụng để dự đoán chỉ số kết thúc.

### 3.3 Huấn luyện mô hình

Mục tiêu huấn luyện tổng thể là tối thiểu hóa tổng các loss của tất cả các tác vụ. Cụ thể, mô hình được huấn luyện trên sự nối của các mẫu huấn luyện từ các tác vụ này. Trong mỗi iteration, một mini-batch được chọn từ một tác vụ, và các tham số mô hình được cập nhật theo mục tiêu đặc thù tác vụ. Chúng tôi lấy mẫu các mini-batch từ N = 6 tác vụ theo phân phối đa thức với xác suất {qi}i=1,...,N:

qi = (√|Ti|) / (∑j=1^N √|Tj|) với pi = |Ti| / (∑j=1^N |Tj|) (3)

trong đó |Ti| chỉ ra số lượng mẫu huấn luyện trong tác vụ Ti.

Tỷ lệ lấy mẫu α là một siêu tham số để cân bằng các tác vụ khác nhau. Nếu α = 0.0, qi = 1/N. Mỗi tác vụ được chọn với cơ hội bằng nhau. Lấy mẫu với phân phối này tăng số lượng mẫu liên quan đến các tác vụ có kích thước nhỏ và giảm thiểu bias đối với các tác vụ có nhiều tài nguyên. Nếu α = 1.0, phân phối tự nhiên của các tác vụ sẽ được duy trì và các tác vụ có ít tài nguyên không được up-sample. Chúng tôi đặt tỷ lệ lấy mẫu α = 1.0 trong các thí nghiệm. Phân tích về ảnh hưởng của α được đưa ra trong mục 5.2.

### 3.4 Thích ứng với các tác vụ mới

Chúng tôi mô tả việc thích ứng của một SkillNet-NLU đa tác vụ được huấn luyện tốt với các tác vụ mới. Chúng tôi xem xét hai tình huống ở đây, tùy thuộc vào việc có cần kỹ năng mới để giải quyết tác vụ mới hay không.

Tình huống thứ nhất là các kỹ năng hiện có được xem xét trong giai đoạn huấn luyện đa tác vụ đủ để giải quyết tác vụ mới. Xét tác vụ mới của trả lời câu hỏi miền mở để xác định xem một câu từ các tài liệu đã cho có trả lời câu hỏi hay không. Mặc dù chính xác tác vụ tương tự không được thấy trong giai đoạn huấn luyện, các kỹ năng liên quan (tức là kỹ năng lấy biểu diễn ngữ nghĩa của một chuỗi (s1), kỹ năng hiểu câu hỏi (s5) và kỹ năng hiểu cách hai segment tương tác (s3)) đã được thấy trong quá trình huấn luyện đa tác vụ. Do đó, chúng tôi sử dụng framework tiêu chuẩn chỉ kích hoạt các kỹ năng liên quan để tinh chỉnh các tham số mô hình cho tác vụ mới.

Tình huống thứ hai là tác vụ mới có thể cần kỹ năng mới chưa được thấy trong giai đoạn huấn luyện đa tác vụ. Ví dụ, tác vụ khớp câu hỏi-trả lời y tế tiếng Trung có thể cần kỹ năng bổ sung để hiểu văn bản trong lĩnh vực y tế, chưa được thấy trong giai đoạn huấn luyện đa tác vụ. Mô hình của chúng tôi hỗ trợ hai cách để học cho các tác vụ mới như vậy. Một cách là giữ nguyên số lượng kỹ năng và, theo trực giác, học các kỹ năng chưa thấy (như hiểu văn bản y tế) trong kỹ năng tổng quát (s7). Cách khác là thêm một kỹ năng mới (s8), được kích hoạt cùng với các kỹ năng được kích hoạt khác để học cho tác vụ mới.

### 3.5 Tiền huấn luyện thưa thớt

Trong phần này, chúng tôi cho thấy cách các tham số của SkillNet-NLU có thể được tiền huấn luyện với các tham số mô hình được kích hoạt thưa thớt. Chúng tôi áp dụng hai mục tiêu học tự giám sát tiêu chuẩn (Devlin et al., 2018) bao gồm masked language modeling (MLM) và next sentence prediction (NSP). Cụ thể, chúng tôi kích hoạt hai kỹ năng SMLM = {s2, s7} cho tác vụ MLM. Đối với tác vụ NSP, ba kỹ năng SNSP = {s1, s3, s7} được kích hoạt. Chúng tôi lấy mẫu hai tác vụ với cơ hội bằng nhau và mục tiêu học tổng thể là tối thiểu hóa tổng của hai loss. Chúng tôi hướng độc giả đến Devlin et al. (2018) để biết chi tiết của hai tác vụ tiền huấn luyện này. Sau khi được tiền huấn luyện, các tham số của các kỹ năng được tiền huấn luyện có thể được sử dụng để khởi tạo mô hình đa tác vụ.

## 4 Thí nghiệm

Phần này được tổ chức như sau. Chúng tôi trước tiên mô tả cài đặt thí nghiệm (§4.1), sau đó báo cáo kết quả trên nhiều tác vụ (§4.2). Tiếp theo, chúng tôi trình bày kết quả trên hai tác vụ mới (§4.3).

### 4.1 Cài đặt thí nghiệm

**Tập dữ liệu** Chúng tôi tiến hành huấn luyện đa tác vụ trên sáu tập dữ liệu hiểu ngôn ngữ tự nhiên tiếng Trung để đánh giá hiệu suất của các mô hình.

ChnSentiCorp (Tan, 2012) là một tập dữ liệu phân tích cảm xúc, trong đó văn bản phải được phân loại thành nhãn tích cực hoặc tiêu cực. OCNLI (Hu et al., 2020) là một tập dữ liệu NLI tiếng Trung quy mô lớn, yêu cầu dự đoán mối quan hệ của các cặp tiền đề-giả thiết. Các nhãn bao gồm contradiction, neutral và entailment. AFQMC (Xu et al., 2020) là một tập dữ liệu phân loại nhị phân từ lĩnh vực tài chính, nhằm dự đoán xem hai câu có tương tự ngữ nghĩa hay không. TNEWS (Xu et al., 2020) là một tập dữ liệu phân loại văn bản ngắn bao gồm các tiêu đề tin tức, yêu cầu phân loại thành một trong 15 lớp. OntoNotes (Weischedel et al., 2013) được thiết kế cho nhận dạng thực thể có tên. Các thực thể bao gồm một số loại như người, tổ chức và địa điểm, v.v. CMRC 2018 (Cui et al., 2019) là một tập dữ liệu đọc hiểu máy trích xuất span, yêu cầu trích xuất một span đoạn văn cho câu hỏi đã cho. Bảng 2 hiển thị thống kê chi tiết của các tập dữ liệu này.

**Baselines** Chúng tôi so sánh SkillNet-NLU của chúng tôi với các phương pháp sau:

**Tinh chỉnh đặc thù tác vụ**: Chúng tôi tinh chỉnh tất cả các tham số của mô hình BERT cho mỗi tác vụ riêng lẻ. Do đó, chúng tôi có tổng cộng sáu mô hình đặc thù tác vụ trong các thí nghiệm của mình.

**Tinh chỉnh chung (Dense)**: Chúng tôi áp dụng BERT của chúng tôi như một mô hình chia sẻ để thu được biểu diễn đặc trưng và sau đó đưa nó vào nhiều lớp dự đoán đặc thù tác vụ. Các tham số của mô hình BERT và tất cả các lớp trên cùng được học chung trên sáu tác vụ.

**Tinh chỉnh chung (MoE)**: Chúng tôi đặt số lượng FFN trong mỗi lớp là bảy và kích hoạt top-2 FFN cho mỗi token, được xác định bởi một mô-đun gating. Các tham số của các FFN này được khởi tạo với mô hình BERT của chúng tôi và được cập nhật với các lớp dự đoán đặc thù tác vụ.

Chúng tôi thu thập 800G dữ liệu tiền huấn luyện từ tin tức web và bài viết blog, và huấn luyện một mô hình Chinese BERT-base với batch size 10,240.

Chúng tôi xây dựng SkillNet-NLU sử dụng triển khai BERT-base của HuggingFace's Transformers (Wolf et al., 2020), có 12 lớp encoder Transformer và 768 chiều trạng thái ẩn. Chúng tôi có hai cấu hình để thực hiện huấn luyện đa tác vụ. Cài đặt thứ nhất (w/o sparse pre-training) là tất cả các mô-đun kỹ năng được khởi tạo với các lớp FFN từ Chinese BERT của chúng tôi. Cài đặt thứ hai (w/ sparse pre-training) là chúng tôi sử dụng các tham số sau tiền huấn luyện thưa thớt để khởi tạo các kỹ năng. Chi tiết về tiền huấn luyện thưa thớt được hiển thị trong Phụ lục B.

Chúng tôi tiến hành huấn luyện đa tác vụ trong 50k bước với độ dài tối đa 512, batch size 8. Chúng tôi sử dụng Adam (Kingma và Ba, 2014) làm optimizer với β1 = 0.9, β2 = 0.98, ε = 1e-6. Tỷ lệ học được làm ấm trong 5k bước đầu tiên đến giá trị đỉnh 2e-5, sau đó giảm tuyến tính. Chúng tôi hiển thị đường cong học của mỗi tác vụ trong Phụ lục C.

### 4.2 Kết quả

Bảng 3 hiển thị kết quả đánh giá của các hệ thống baseline cũng như các mô hình đề xuất trên sáu tác vụ. Hai baseline học đa tác vụ (tức là Tinh chỉnh chung (Dense) và Tinh chỉnh chung (MoE)) hoạt động hơi kém hơn so với tinh chỉnh đặc thù tác vụ. SkillNet-NLU của chúng tôi không có tiền huấn luyện vượt trội hơn các hệ thống baseline và đạt điểm trung bình 77.4%, chứng minh tính hiệu quả của kích hoạt thưa thớt. Hiệu suất của mô hình với tiền huấn luyện thưa thớt được cải thiện thêm lên 77.9%, cho thấy rằng các mô-đun kỹ năng được học tốt hơn sau khi tiền huấn luyện theo cách thưa thớt tương tự.

### 4.3 Kết quả trên các tác vụ mới

Trong phần này, chúng tôi trình bày việc thích ứng của một SkillNet-NLU đa tác vụ được huấn luyện tốt với các tác vụ mới. Kết quả được báo cáo trong hai cài đặt, tùy thuộc vào việc có cần kỹ năng mới hay không.

Tác vụ mới thứ nhất là trả lời câu hỏi miền mở. Cho một câu hỏi và một câu ứng cử, tác vụ là xác định xem câu đó có trả lời câu hỏi hay không. Chúng tôi nối câu hỏi và câu ứng cử với một token [SEP] và sử dụng vector của [CLS] vào một lớp softmax để thực hiện phân loại nhị phân. Trong cài đặt này, không có kỹ năng mới nào được thêm vào. Vì vậy chúng tôi kích hoạt một tập hợp bốn kỹ năng liên quan SNLPCCDBQA = {s1, s3, s5, s7} và tinh chỉnh tất cả các tham số của các mô-đun kỹ năng này cho tác vụ mới.

Chúng tôi tiến hành thí nghiệm trên tập dữ liệu NLPCC-DBQA (Duan, 2016). Bảng 4 hiển thị số lượng tham số được kích hoạt và điểm F1 của các mô hình khác nhau. Chúng ta có thể thấy rằng hệ thống cuối cùng của chúng tôi, SkillNet-NLU với tiền huấn luyện thưa thớt, hoạt động tốt hơn so với baseline RoBERTa-large với số lượng tham số được kích hoạt nhỏ hơn.

Chúng tôi xem xét tác vụ mới thứ hai về khớp câu hỏi-trả lời y tế tiếng Trung. Cho một câu hỏi và một tập hợp câu trả lời ứng cử, các mô hình được yêu cầu chọn câu trả lời liên quan nhất. Đầu vào của mô hình là sự nối của câu hỏi và một câu trả lời ứng cử, được phân cách bởi một token [SEP]. Chúng tôi kích hoạt một tập hợp bốn kỹ năng ScMed = {s1, s3, s5, s7} và lấy biểu diễn của [CLS] để tính toán độ tương tự giữa câu hỏi và câu trả lời ứng cử. Chúng tôi khám phá xem có nên thêm một kỹ năng mới (s8) về hiểu văn bản từ lĩnh vực y tế, chưa được thấy trong giai đoạn huấn luyện đa tác vụ. Nếu kỹ năng mới được thêm vào, chúng tôi có thể khởi tạo các tham số của nó với kỹ năng tổng quát (s7). Sau đó, các tham số của bốn kỹ năng được kích hoạt, cũng như kỹ năng mới, được tinh chỉnh trên dữ liệu huấn luyện.

Chúng tôi tiến hành thí nghiệm trên tập dữ liệu cMedQA (Zhang et al., 2017). Bảng 5 hiển thị số lượng tham số được kích hoạt và độ chính xác top-1 của các mô hình khác nhau. Chúng tôi hiển thị hiệu suất mô hình bằng cách không thêm kỹ năng mới trong khối thứ hai. Chúng ta có thể thấy rằng SkillNet-NLU của chúng tôi không có tiền huấn luyện vượt trội hơn ba hệ thống baseline, đạt độ chính xác top-1 78.6%. Khối thứ ba hiển thị kết quả bằng cách thêm một kỹ năng mới. Chúng ta có thể thấy rằng hiệu suất của SkillNet-NLU có hoặc không có tiền huấn luyện thưa thớt được cải thiện liên tục. Lý do cơ bản là số lượng tham số tăng lên. Đáng ngạc nhiên, chúng tôi thấy rằng chỉ cập nhật kỹ năng mới có thể đạt được hiệu suất mạnh.

## 5 Nghiên cứu loại bỏ và phân tích

Kết quả đánh giá cho thấy rằng SkillNet-NLU của chúng tôi vượt trội hơn tinh chỉnh đặc thù tác vụ và hai baseline học đa tác vụ. Trong phần này, chúng tôi tiến hành nghiên cứu loại bỏ chi tiết và phân tích thí nghiệm để hiểu rõ hơn về phương pháp đề xuất. Tất cả các kết quả đều dựa trên SkillNet-NLU không có tiền huấn luyện thưa thớt, trong đó tất cả các mô-đun kỹ năng được khởi tạo với các lớp FFN từ Chinese BERT của chúng tôi.

### 5.1 Nghiên cứu loại bỏ

Chúng tôi thực hiện nghiên cứu loại bỏ để khám phá tác động của mỗi kỹ năng. Cụ thể, chúng tôi xóa lần lượt một trong bảy kỹ năng, sau đó kích hoạt các kỹ năng tương ứng khác cho mỗi tác vụ. Kết quả loại bỏ được trình bày trong Bảng 6. Từ mỗi hàng của bảng, chúng ta có thể thấy rằng điểm trung bình giảm khi bất kỳ kỹ năng nào bị loại bỏ trong SkillNet-NLU, chứng minh rằng tất cả các kỹ năng được định nghĩa đều hữu ích cho huấn luyện đa tác vụ. Có sự giảm đáng kể khi xóa kỹ năng tổng quát s7, vì nó được chia sẻ bởi tất cả các tác vụ. Chúng ta có thể thấy rằng hiệu suất tác vụ giảm mạnh khi một số kỹ năng liên quan chặt chẽ bị loại bỏ, đặc biệt là kỹ năng duy nhất cho tác vụ (tức là s4 cho T1, s5 cho T6, s6 cho T3). Chúng tôi cũng thấy rằng loại bỏ s2 ảnh hưởng đáng kể đến hiệu suất trên T5-T6 trong khi không làm tổn hại độ chính xác trên T1-T4. Lý do là T1-T4 là các tác vụ dự đoán chuỗi và T5-T6 là các tác vụ dự đoán token. Loại bỏ s2 làm cho mô hình quá chuyên biệt hóa cho các tác vụ dự đoán chuỗi, trong khi ít linh hoạt hơn cho các tác vụ khác yêu cầu khả năng dự đoán token.

### 5.2 Ảnh hưởng của tỷ lệ lấy mẫu

Như được mô tả trong Phần 3.3, chúng tôi lấy mẫu các ví dụ huấn luyện từ mỗi tác vụ theo tỷ lệ lấy mẫu α. Hình 3 hiển thị điểm trung bình với α khác nhau. Chúng ta có thể thấy rằng mô hình hoạt động tốt hơn khi tỷ lệ lấy mẫu α = 1.0, duy trì phân phối tự nhiên của tác vụ. Lý do cơ bản là kích thước của các tập dữ liệu này tương đối cân bằng. Kết quả cũng cho thấy rằng up-sampling các tập dữ liệu liên tục có hại cho học đa tác vụ, điều này phù hợp với Aghajanyan et al. (2021). Do đó, chúng tôi áp dụng α = 1.0 trong toàn bộ các thí nghiệm của mình.

### 5.3 Ảnh hưởng của số lượng lớp SkillNet-NLU trên cùng

Chúng tôi cũng điều tra cách số lượng lớp SkillNet-NLU trên cùng ảnh hưởng đến hiệu suất mô hình. Chúng tôi tiến hành thí nghiệm dựa trên SkillNet-NLU và số lượng lớp SkillNet-NLU trên cùng thay đổi từ 3 đến 12, tăng 3. Chúng tôi hiển thị số lượng tham số tổng cộng và điểm trung bình của mỗi mô hình trong Bảng 7. Chúng ta có thể thấy rằng hiệu suất liên tục cải thiện khi số lượng tăng lên, chứng minh tính hiệu quả của SkillNet-NLU của chúng tôi. Lý do cơ bản là khi nhiều lớp SkillNet-NLU được kết hợp, các kỹ năng được học tốt hơn khi số lượng tham số tăng lên.

## 6 Kết luận

Trong nghiên cứu này, chúng tôi trình bày một mô hình đa mục đích gọi là SkillNet-NLU và ứng dụng của nó cho các tác vụ hiểu ngôn ngữ tự nhiên. SkillNet-NLU bao gồm một tập hợp các mô-đun kỹ năng có tham số, và kích hoạt thưa thớt một số mô-đun tùy thuộc vào việc một kỹ năng có liên quan đến tác vụ mục tiêu hay không. Framework này mang tính tổng quát và hỗ trợ cả tinh chỉnh đa tác vụ và tiền huấn luyện, cả hai đều với kích hoạt thưa thớt. Kết quả chứng minh rằng phương pháp này hoạt động tốt hơn so với các hệ thống baseline trên cả tác vụ cũ và mới, và tiền huấn luyện thưa thớt mang lại cải thiện thêm.

Nghiên cứu này có thể được cải thiện thêm từ nhiều góc độ khác nhau, bao gồm định nghĩa một phạm vi kỹ năng rộng hơn, khám phá các kiến trúc mô hình tiên tiến, mở rộng từ một ngôn ngữ sang đa ngôn ngữ hoặc thậm chí từ một phương thức sang nhiều phương thức.

## Tài liệu tham khảo

[Các tài liệu tham khảo được liệt kê với định dạng tiêu chuẩn...]

## Phụ lục

### A Kiến trúc mô hình

### B Chi tiết tiền huấn luyện thưa thớt

Trong quá trình tiền huấn luyện, chúng tôi khởi tạo bốn mô-đun kỹ năng (tức là s1, s2, s3 và s7) với các lớp FFN từ Chinese BERT của chúng tôi. Chúng tôi áp dụng cùng dữ liệu tiền huấn luyện và batch size được sử dụng trong quá trình tiền huấn luyện Chinese BERT của chúng tôi. SkillNet-NLU được tiền huấn luyện với huấn luyện độ chính xác hỗn hợp trên 32 GPU Nvidia Tesla V100 32GB trong 100k bước với độ dài tối đa 512. Chúng tôi sử dụng Adam (Kingma và Ba, 2014) làm optimizer với β1 = 0.9, β2 = 0.98, ε = 1e-6. Tỷ lệ học được làm ấm trong 10k bước đầu tiên đến giá trị đỉnh 3e-5, sau đó giảm tuyến tính.

Sau khi được tiền huấn luyện, chúng tôi có thể xây dựng một mô hình đa tác vụ bằng cách khởi tạo bốn mô-đun kỹ năng tương ứng. Các tham số của ba mô-đun kỹ năng khác (tức là s4, s5 và s6) được khởi tạo từ mô-đun kỹ năng tổng quát s7.

### C Đường cong học

Chúng tôi hiển thị các đường cong học trong quá trình huấn luyện đa tác vụ trong Hình 5.
