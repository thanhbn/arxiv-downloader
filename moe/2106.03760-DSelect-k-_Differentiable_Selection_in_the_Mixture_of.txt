# 2106.03760.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2106.03760.pdf
# File size: 3290911 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
DSelect-k: Differentiable Selection in the Mixture of
Experts with Applications to Multi-Task Learning
Hussein Hazimeh1, Zhe Zhao1, Aakanksha Chowdhery1, Maheswaran Sathiamoorthy1
Yihua Chen1,Rahul Mazumder2,Lichan Hong1,Ed H. Chi1
1Google, {hazimeh,zhezhao,chowdhery,nlogn,yhchen,lichan,edchi}@google.com
2Massachusetts Institute of Technology, rahulmaz@mit.edu
Abstract
The Mixture-of-Experts (MoE) architecture is showing promising results in improv-
ing parameter sharing in multi-task learning (MTL) and in scaling high-capacity
neural networks. State-of-the-art MoE models use a trainable “sparse gate” to
select a subset of the experts for each input example. While conceptually appealing,
existing sparse gates, such as Top-k, are not smooth. The lack of smoothness can
lead to convergence and statistical performance issues when training with gradient-
based methods. In this paper, we develop DSelect-k: a continuously differentiable
and sparse gate for MoE, based on a novel binary encoding formulation. The gate
can be trained using ﬁrst-order methods, such as stochastic gradient descent, and
offers explicit control over the number of experts to select. We demonstrate the
effectiveness of DSelect-k on both synthetic and real MTL datasets with up to
128tasks. Our experiments indicate that DSelect-k can achieve statistically sig-
niﬁcant improvements in prediction and expert selection over popular MoE gates.
Notably, on a real-world, large-scale recommender system, DSelect-k achieves
over22% improvement in predictive performance compared to Top-k. We provide
an open-source implementation of DSelect-k1.
1 Introduction
The Mixture of Experts (MoE) [ 14] is the basis of many state-of-the-art deep learning models. For
example, MoE-based layers are being used to perform efﬁcient computation in high-capacity neural
networks and to improve parameter sharing in multi-task learning (MTL) [ 33,22,21]. In its simplest
form, a MoE consists of a set of experts (neural networks) and a trainable gate. The gate assigns
weights to the experts on a per-example basis, and the MoE outputs a weighted combination of the
experts. This per-example weighting mechanism allows experts to specialize in different partitions of
the input space, which has the potential to improve predictive performance and interpretability. In
Figure 1 (left), we show an example of a simple MoE architecture that can be used as a standalone
learner or as a layer in a neural network.
The literature on the MoE has traditionally focused on softmax-based gates, in which all experts
are assigned nonzero weights [ 17]. To enhance the computational efﬁciency and interpretability of
MoE models, recent works use sparse gates that assign nonzero weights to only a small subset of
the experts [ 1,33,29,21]. Existing sparse gates are not differentiable, and reinforcement learning
algorithms are commonly used for training [ 1,29]. In an exciting work, [ 33] introduced a new sparse
gate (Top-k gate) and proposed training it using stochastic gradient descent (SGD). The ability to
train the gate using SGD is appealing because it enables end-to-end training. However, the Top-k gate
is not continuous, which can lead to convergence issues in SGD that affect statistical performance (as
we demonstrate in our experiments).
1https://github.com/google-research/google-research/tree/master/dselect_k_moe
35th Conference on Neural Information Processing Systems (NeurIPS 2021).arXiv:2106.03760v3  [cs.LG]  31 Dec 2021

--- PAGE 2 ---
Figure 1: (Left) : An example of a MoE that can be used as a standalone learner or layer in a neural
network. Here “Ei” denotes the i-th expert. (Right): A multi-gate MoE for learning two tasks
simultaneously. “Task i NN” is a neural network that generates the output of Task i.
In this paper, we introduce DSelect-k: a continuously differentiable and sparse gate for MoE. Given
a user-speciﬁed parameter k, the gate selects at most kout of thenexperts. This explicit control
over sparsity leads to a cardinality-constrained optimization problem, which is computationally
challenging. To circumvent this challenge, we propose a novel, unconstrained reformulation that
is equivalent to the original problem. The reformulated problem uses a binary encoding scheme to
implicitly enforce the cardinality constraint. We demonstrate that by carefully smoothing the binary
encoding variables, the reformulated problem can be effectively optimized using ﬁrst-order methods
such as SGD. DSelect-k has a unique advantage over existing methods in terms of compactness and
computational efﬁciency. The number of parameters used by DSelect-k is logarithmic in the number
of experts, as opposed to linear in existing gates such as Top-k. Moreover, DSelect-k’s output can be
computed efﬁciently via a simple, closed-form expression. In contrast, state-of-the-art differentiable
methods for stochastic k-subset selection and Top-k relaxations, such as [ 26,40]2, require solving an
optimization subproblem (for each input example) to compute the gate’s output.
DSelect-k supports two gating mechanisms: per-example andstatic . Per-example gating is the
classical gating technique used in MoE models, in which the weights assigned to the experts are
a function of the input example [ 14,33]. In static gating, a subset of experts is selected and the
corresponding weights do not depend on the input [ 29]. Based on our experiments, each gating
mechanism can outperform the other in certain settings. Thus, we study both mechanisms and
advocate for experimenting with each.
MTL is an important area where MoE models in general, and our gate in particular, can be useful.
The goal of MTL is to learn multiple tasks simultaneously by using a shared model. Compared to the
usual single task learning, MTL can achieve better generalization performance through exploiting
task relationships [ 4]. One key problem in MTL is how to share model parameters between tasks [ 31].
For instance, sharing parameters between unrelated tasks can potentially degrade performance. The
multi-gate MoE [ 22] is a ﬂexible architecture that allows for learning what to share between tasks.
Figure 1 (right) shows an example of a multi-gate MoE, in the simple case of two tasks. Here, each
task has its own gate that adaptively controls the extent of parameter sharing. In our experiments, we
study the effectiveness of DSelect-k in the context of the multi-gate MoE.
Contributions: On a high-level, our main contribution is DSelect-k: a new continuously differ-
entiable and sparse gate for MoE, which can be directly trained using ﬁrst-order methods. Our
technical contributions can be summarized as follows. (i)The gate selects (at most) kout of then
experts, where kis a user-speciﬁed parameter. This leads to a challenging, cardinality-constrained
optimization problem. To deal with this challenge, we develop a novel, unconstrained reformulation,
and we prove that it is equivalent to the original problem. The reformulation uses a binary encoding
scheme that implicitly imposes the cardinality constraint using learnable binary codes. (ii)To make
the unconstrained reformulation smooth, we relax and smooth the binary variables. We demonstrate
that, with careful initialization and regularization, the resulting problem can be optimized with
ﬁrst-order methods such as SGD. (iii)We carry out a series of experiments on synthetic and real MTL
datasets, which show that our gate is competitive with state-of-the-art gates in terms of parameter
sharing and predictive performance. (iv)We provide an open-source implementation of DSelect-k.
2These methods were not designed speciﬁcally for the MoE.
2

--- PAGE 3 ---
1.1 Related Work
MoE and Conditional Computation: Since MoE was introduced by [ 14], an exciting body of work
has extended and studied this model, e.g., see [ 17,13,16]. Recently, MoE-based models are showing
success in deep learning. For example, [ 33] introduced the sparse Top-k gate for MoE and showed
signiﬁcant computational improvements on machine translation tasks; we discuss exact connections
to this gate in Section 2. The Top-k gate has also been utilized in several state-of-the-art deep learning
models that considered MTL tasks, e.g., [ 21,28,9]. Our work is also related to the conditional
computation models that activate parts of the neural network based on the input [ 2,1,33,12,36].
Unlike DSelect-k, these works are based on non-differentiable models, or heuristics where the training
and inference models are different.
Stochastic k-Subset Selection and Top-k Relaxation : A related line of work focuses on stochastic
k-subset selection in neural networks, e.g., see [ 26,5,39] and the references therein. Speciﬁcally,
these works propose differentiable methods for sampling k-subsets from a categorical distribution,
based on extensions or generalizations of the Gumbel-softmax trick [ 23,15]. However, in the MoE
we consider deterministic subset selection—determinism is a common assumption in MoE models
that can improve interpretability and allows for efﬁcient implementations [ 14,17,33]. In contrast,
the stochastic approaches described above are suitable in applications where there is an underlying
sampling distribution, such as in variational inference [ 19]. Another related work is the differentiable
relaxation of the Top-k operator proposed by [ 40]. All the aforementioned works perform dense
training (i.e., the gradients of all experts, even if not selected, will have to be computed during
backpropagation), whereas DSelect-k can (to an extent) exploit sparsity to speed up training, as we
will discuss in Sections 2 and 3. Moreover, the stochastic k-subset selection framework in [ 26] (which
encompasses several previous works) and the Top-k relaxation in [ 40] require solving an optimization
subproblem to compute the gate output–each example will require solving a separate subproblem in
the per-example gating setting, which can be computationally prohibitive. In contrast, DSelect-k’s
output is computed efﬁciently via a closed-form expression.
Sparse Transformations to the Simplex: These are sparse variants of the softmax function that
can output sparse probability vectors, e.g., see [ 24,27,6,3]. While similar to our work in that they
output sparse probability vectors, these transformations cannot control the sparsity level precisely
as DSelect-k does (through a cardinality constraint). Thus, these transformations may assign some
examples or tasks sparse combinations and others dense combinations.
MTL: In Appendix A, we review related literature on MTL.
2 Gating in the Mixture of Experts
In this section, we ﬁrst review the MoE architecture and popular gates, and then discuss how these
gates compare to our proposal. We will assume that the inputs to the MoE belong to a space
XRp. In its simplest form, the MoE consists of a set of nexperts (neural networks) fi:X!Ru,
i2f1;2;:::;ng, and a gate g:X!Rnthat assigns weights to the experts. The gate’s output is
assumed to be a probability vector, i.e., g(x)0andPn
i=1g(x)i= 1, for anyx2X. Given an
examplex2X, the corresponding output of the MoE is a weighted combination of the experts:
nX
i=1fi(x)g(x)i: (1)
Next, we discuss two popular choices for the gate g(:)that can be directly optimized using SGD.
Softmax Gate: A classical model for g(x)is the softmax gate: (Ax+b), where(:)is the softmax
function,A2Rnpis a trainable weight matrix, and b2Rnis a bias vector [ 17]. This gate is dense,
in the sense that all experts are assigned nonzero probabilities. Note that static gating (i.e., gating
which does not depend on the input example) can be obtained by setting A= 0.
Top-k Gate: This is a sparse variant of the softmax gate that returns a probability vector with only k
nonzero entries [ 33]. The Top-k gate is deﬁned by (KeepTopK (Ax+b)), where for any vector v,
KeepTopK (v)i:=viifviis in the top k elements of v, and KeepTopK (v)i:= 1 otherwise3. This
gate is conceptually appealing since it allows for direct control over the number of experts to select
3To balance the load across experts, [33] add noise and additional regularizers to the model.
3

--- PAGE 4 ---
Figure 2: Expert weights output by Top-k (left) and DSelect-k (right) during training on synthetic data
generated from a MoE, under static gating. Each color represents a separate expert. Here DSelect-k
recovers the true experts used by the data-generating model, whereas Top-k does not recover and
exhibits oscillatory behavior. See Appendix C.2 for details on the data and setup.
and is trained using SGD. Moreover, the Top-k gate supports conditional training : in backpropagation,
for each input example, only the gradients of the loss w.r.t. top k elements need to be computed.
With a careful implementation, conditional training can lead to computational savings. However, the
Top-k gate is not continuous, which implies that the gradient does not exist at certain inputs. This
can be problematic when training is done using gradient-based methods. To gain more insight, in
Figure 2 (left), we plot the expert weights chosen by the Top-k gate during training with SGD. The
results indicate an oscillatory behavior in the output of the Top-k gate, which can be attributed to its
discontinuous nature: a small change in the input can lead to “jumps” in the output.
Comparison with DSelect-k: We develop DSelect-k in Section 3. Here we present a high-level
comparison between DSelect-k and Top-k. Similar to Top-k, DSelect-k can select kout of then
experts and can be trained using gradient-based optimization methods. A major advantage of DSelect-
k over Top-k is that it is continuously differentiable, which leads to more stable selection of experts
during training—see Figure 2 (right). During inference, DSelect-k only needs to evaluate a subset
of the experts, which can lead to computational savings. However, DSelect-k supports conditional
training only partially. At the start of training, it uses all the available experts, so conditional training
is not possible. As we discuss in Section 3, after a certain point during training, DSelect-k converges
to a small subset of the experts, and then conditional training becomes possible. Our experiments
indicate that DSelect-k can have a signiﬁcant edge over Top-k in terms of prediction and expert
selection performance, so the full support for conditional training in Top-k seems to come at the
expense of statistical performance.
3 Differentiable and Sparse Gating
In this section, we develop DSelect-k, for both the static and per-example gating settings. First, we
introduce the problem setup and notation. To simplify the presentation, we will develop the gate for
a single supervised learning task, and we note that the same gate can be used in MTL models. We
assume that the task has an input space XRp, an output spaceY, and an associated loss function
`:YR!R. We denote the set of Ntraining examples by D=f(xi;yi)2XYgN
i=1. We
consider a learning model deﬁned by the MoE in Equation (1). For simplicity, we assume that the
experts are scalar-valued and belong to a class of continuous functions H. We assume that the number
of expertsn= 2mfor some integer m—in Appendix B.2, we discuss how the gate can be extended
to arbitraryn. For convenience, given a non-negative integer i, we denote the set f1;2;:::;igby[i].
In Section 3.1, we develop DSelect-k for static gating setting. Then, in Section 3.2, we generalize it
to the per-example setting.
3.1 DSelect-k for Static Gating
Our goal here is to develop a static gate that selects a convex combination of at most kout of then
experts. The output of the gate can be thought of as a probability vector wwith at most knonzero
entries, where wiis the weight assigned to the expert fi. A natural way to minimize the empirical
risk of the MoE model is by solving the following problem:
4

--- PAGE 5 ---
min
f1;:::f n;w1
NX
(x;y)2D`
y;nX
i=1fi(x)wi
(2a)
s:t:kwk0k (2b)
nX
i=1wi= 1; w0: (2c)
In the above, the L0norm ofw,kwk0, is equal to the number of nonzero entries in w. Thus,
the cardinality constraint (2b) ensures that the gate selects at most kexperts. Problem (2)is a
combinatorial optimization problem that is not amenable to SGD due to the cardinality constraint (2b)
and the simplex constraints in (2c). In what follows of this section, we ﬁrst transform Problem (2)into
an equivalent unconstrained optimization problem, based on a binary encoding scheme. However, the
unconstrained problem cannot be directly handled using SGD due to the presence of binary variables.
Thus, in a second transformation, we smooth the binary variables, which leads to an optimization
problem that is amenable to SGD.
Road map: In Section 3.1.1, we introduce the single expert selector : a construct for choosing 1 out
ofnexperts by using binary encoding. In Section 3.1.2, we leverage the single expert selector to
transform Problem (2)into an unconstrained one. Then, in Section 3.1.3, we smooth the unconstrained
problem and discuss how SGD can be applied.
3.1.1 Single Expert Selection using Binary Encoding
The single expert selector (selector, for short) is a fundamental construct that we will later use to
convert Problem (2)to an unconstrained optimization problem. At a high-level, the single expert
selector chooses the index of 1 out of the nexperts and returns a one-hot encoding of the choice.
For example, in the case of 4experts, the selector can choose the ﬁrst expert by returning the binary
vector [1 0 0 0]T. Generally, the selector can choose any of the experts, and its choice is determined
by a set of binary encoding variables, as we will describe next.
The selector is parameterized by m(recall thatm= log2n) binary variables, z1;z2;:::;zm, where
we view these variables collectively as a binary number: zmzm 1:::z 1. The integer represented
by the latter binary number determines which expert to select. More formally, let lbe the integer
represented by the binary number zmzm 1:::z 1. The selector is a function r:Rm!f0;1gnwhich
mapsz:= [z1;z2;:::;zm]Tto a one-hot encoding of the integer (l+ 1) . For example, if all the zi’s
are0, then the selector returns a one-hot encoding of the integer 1. Next, we deﬁne the selector r(z).
For easier exposition, we start with the special case of 4experts and then generalize to nexperts.
Special case of 4experts: In this case, the selector uses two binary variables z1andz2. Letlbe the
integer represented by the binary number z2z1. Then, the selector should return a one-hot encoding
of the integer (l+ 1) . To achieve this, we deﬁne the selector r(z)as follows:
r(z) = [ z1z2; z1z2;z1z2; z1z2]T(3)
where zi:= 1 zi. By construction, exactly one entry in r(z)is 1 (speciﬁcally, r(z)l+1= 1) and the
rest of the entries are zero. For example, if z1=z2= 0, thenr(z)1= 1andr(z)i= 0,i2f2;3;4g.
General case of nexperts: Here we generalize the selector r(z)to the case of nexperts. To aid in
the presentation, we make the following deﬁnition. For any non-negative integer l, we deﬁneB(l)
as the set of indices of the nonzero entries in the binary representation of l(where we assume that
the least signiﬁcant bit is indexed by 1). For example,B(0) =;,B(1) =f1g,B(2) =f2g, and
B(3) =f1;2g. For everyi2[n], we deﬁne the i-th entry ofr(z)as follows:
r(z)i=Y
j2B(i 1)(zj)Y
j2[m]nB(i 1)(1 zj) (4)
In the above, r(z)iis a product of mbinary variables, which is equal to 1iff the integer (i 1)is
represented by the binary number zmzm 1:::z 1. Therefore, r(z)returns a one-hot encoding of the
index of the selected expert. Note that when n= 4, deﬁnitions (3) and (4) are equivalent.
3.1.2 Multiple Expert Selection via Unconstrained Minimization
In this section, we develop a combinatorial gate that allows for transforming Problem (2)into an
unconstrained optimization problem. We design this gate by creating kinstances of the single expert
5

--- PAGE 6 ---
selectorr(:), and then taking a convex combination of these kinstances. More formally, for every
i2[k], letz(i)2f0;1gmbe a (learnable) binary vector, so that the output of the i-th instance of the
selector isr(z(i)). LetZbe akmmatrix whose i-th row isz(i). Moreover, let 2Rkbe a vector
of learnable parameters. We deﬁne the combinatorial gate qas follows:
q(;Z) =kX
i=1()ir(z(i));
where we recall that (:)is the softmax function. Since for every i2[k],r(z(i))is a one-hot vector,
we havekq(;Z)k0k. Moreover, since the weights of the selectors are obtained using a softmax,
we haveq(;Z)0andPn
i=1q(;Z)i= 1. Thus,q(;Z)has the same interpretation of win
Problem (2), without requiring any constraints. Therefore, we propose replacing win the objective of
Problem (2)withq(;Z)and removing all the constraints. This replacement leads to an equivalent
unconstrained optimization problem, as we state in the next proposition.
Proposition 1. Problem (2)is equivalent4to:
min
f1;:::f n;;Z1
NX
(x;y)2D`
y;nX
i=1fi(x)q(;Z)i
z(i)2f0;1gm; i2[k](5)
The proof of Proposition 1 is in Appendix B.1. Unlike Problem (2), Problem (5)does not involve any
constraints, aside from requiring binary variables. However, these binary variables cannot be directly
handled using ﬁrst-order methods. Next, we discuss how to smooth the binary variables in order to
obtain a continuous relaxation of Problem (5).
3.1.3 Smooth Gating
In this section, we present a procedure to smooth the binary variables in Problem (5)and discuss
how the resulting problem can be optimized using ﬁrst-order methods. The procedure relies on the
smooth-step function, which we deﬁne next.
Smooth-step Function: This is a continuously differentiable and S-shaped function, similar in
shape to the logistic function. However, unlike the logistic function, the smooth-step function can
output 0 and 1 exactly for sufﬁciently large magnitudes of the input. The smooth-step and logistic
functions are depicted in Appendix B.3. More formally, given a non-negative scaling parameter ,
the smooth-step function, S:R!R, is a cubic piecewise polynomial deﬁned as follows:
S(t) =8
><
>:0 ift =2
 2
3t3+3
2t+1
2if =2t=2
1 ift=2
The parameter controls the width of the fractional region (i.e., the region where the function is
strictly between 0 and 1). Note that S(t)is continuously differentiable at all points—this follows
since at the boundary points =2, we have:S0( =2) =S0(=2) = 0 . This function has been
recently used for conditional computation in soft trees [ 11] and is popular in the computer graphics
literature [8, 30].
Smoothing: We obtain DSelect-k from the combinatorial gate q(;Z)by (i) relaxing every binary
variable inZto be continuous in the range ( 1;+1), i.e.,Z2Rkm, and (ii) applying the
smooth-step function to Zelement-wise. Formally, DSelect-k is a function ~qdeﬁned as follows:
~q(;Z) :=q(;S(Z)) =kX
i=1()ir 
S(z(i))
; (6)
where the matrix S(Z)is obtained by applying S()toZelement-wise. Note that ~q(;Z)is
continuously differentiable so it is amenable to ﬁrst-order methods. If S(Z)is binary, then ~q(;Z)
4Equivalent means that the two problems have the same optimal objective, and given an optimal solution for
one problem, we can construct an optimal solution for the other.
6

--- PAGE 7 ---
selects at most kexperts (this holds since ~q(;Z) =q(;S(Z)), and from Section 3.1.2, qselects at
mostkexperts when its encoding matrix is binary). However, when S(Z)has any non-binary entries,
then more than kexperts can be potentially selected, meaning that the cardinality constraint will not
be respected. In what follows, we discuss how the gate can be optimized using ﬁrst-order methods,
while ensuring that S(Z)converges to a binary matrix so that the cardinality constraint is enforced.
We propose using ~q(;Z)in MoE, which leads to the following optimization problem:
min
f1;:::f n;;Z1
NX
(x;y)2D`
y;nX
i=1fi(x)~q(;Z)i
: (7)
Problem (7)can be viewed as a continuous relaxation of Problem (5). If the experts are differentiable,
then the objective of Problem (7)is differentiable. Thus, we propose optimizing MoE end-to-end
using ﬁrst-order methods. We note that ~q(;Z)uses(k+klogn)learnable parameters. In contrast,
the Top-k and softmax gates (discussed in Section 2) use nparameters. Thus, for relatively small k,
our proposal uses a smaller number of parameters. Next, we discuss how DSelect-k’s parameters
should be initialized in order to ensure that it is trainable.
Initialization: By the deﬁnition of the smooth-step function, if S(Zij)is binary then S0(Zij) = 0 ,
and consequently@`
@Zij= 0. This implies that, during optimization, if S(Zij)becomes binary, the
variableZijwill not be updated in any subsequent iteration. Thus, we have to be careful about the
initialization of Z. For example, if Zis initialized so that S(Z)is a binary matrix then the gate will
not be trained. To ensure that the gate is trainable, we initialize each Zijso that 0<S(Zij)<1.
This way, the Zij’s can have nonzero gradients at the start of optimization.
Accelerating Convergence to Binary Solutions: Recall that we need S(Z)to converge to a binary
matrix, in order for the gate ~qto respect the cardinality constraint (i.e., to select at most kexperts).
Empirically, we observe that if the optimizer runs for a sufﬁciently large number of iterations,
thenS(Z)typically converges to a binary matrix. However, early stopping of the optimizer can
be desired in practice for computational and statistical considerations, and this can prevent S(Z)
from converging. To encourage faster convergence towards a binary S(Z), we will add an entropy
regularizer to Problem (7). The following proposition is needed before we introduce the regularizer.
Proposition 2. For anyz2Rm,2Rk, andZ2Rkm,r(S(z))and~q(;Z)belong to the
probability simplex.
The proof of the proposition is in Appendix B.1. Proposition 2 implies that, during training, the
output of each single expert selector used by ~q(;Z), i.e.,r(S(z(i)))fori2[k], belongs to the
probability simplex. Note that the entropy of each r(S(z(i)))is minimized by any one-hot encoded
vector. Thus, for each r(S(z(i))), we add an entropy regularization term that encourages convergence
towards one-hot encoded vectors; equivalently, this encourages convergence towards a binary S(Z).
Speciﬁcally, we solve the following regularized variant of Problem (7):
min
f1;:::f n;;ZX
(x;y)2D1
N`
y;nX
i=1fi(x)~q(;Z)i
+
(Z)
where 
(Z) :=Pk
i=1h 
r(S(z(i)))
andh(:)is the entropy function. The hyperparameter is
non-negative and controls how fast each selector converges to a one-hot encoding. In our experiments,
we tune over a range of values. When selecting the best hyperparameters from tuning, we disregard
anywhose corresponding solution does not have a binary S(Z). In Appendix C.3, we report the
number of training steps required for S(Z)to converge to a binary matrix, on several real datasets.
Other alternatives to ensure that S(Z)converges to a binary matrix are also possible. One alternative
is to regularize the entropy of each entry in S(Z)separately. Another alternative is to anneal the
parameterof the smooth-step function towards zero.
Softmax-based Alternative to Binary Encoding: Recall that our proposed selectors in (6), i.e.,
r 
S(z(i))
,i2[k], learn one-hot vectors exactly (by using binary encoding). One practical alternative
for learning a one-hot vector is by using a softmax function with temperature annealing. Theoretically,
this alternative cannot return a one-hot vector, but after training, the softmax output can be transformed
to a one-hot vector using a heuristic (e.g., by taking an argmax). In Appendix C.1, we perform an
ablation study in which we replace the selectors in DSelect-k with softmax functions (along with
temperature annealing or entropy regularization).
7

--- PAGE 8 ---
3.2 DSelect-k for Per-example Gating
In this section, we generalize the static version of DSelect-k, ~q(;Z), to the per-example gating
setting. The key idea is to make the gate’s parameters andZfunctions of the input, so that the
gate can make decisions on a per-example basis. Note that many functional forms are possible for
these parameters. For simplicity and based on our experiments, we choose to make andZlinear
functions of the input example. More formally, let G2Rkp,W(i)2Rmp,i2[k], be a set
of learnable parameters. Given an input example x2Rp, we set=Gxandz(i)=W(i)xin
~q(;Z)(to simplify the presentation, we do not include bias terms). Thus, the per-example version
of DSelect-k is a function vdeﬁned as follows:
v(G;W;x ) =kX
i=1(Gx)ir 
S(W(i)x)
:
In the above, the term r 
S(W(i)x)
represents the i-th single expert selector, whose output depends
on the example x; thus different examples are free to select different experts. The term (Gx)ideter-
mines the input-dependent weight assigned to the i-th selector. The gate v(G;W;x )is continuously
differentiable in the parameters GandW, so we propose optimizing it using ﬁrst-order methods.
Similar to the case of static gating, if S(W(i)x)is binary for all i2[k], then eachr 
S(W(i)x)
will
select exactly one expert, and the example xwill be assigned to at most kexperts.
To encourage S(W(i)x),i2[k]to become binary, we introduce an entropy regularizer, similar in
essence to that in static gating. However, unlike static gating, the regularizer here should be on a
per-example basis, so that each example respects the cardinality constraint. By Proposition 2, for any
i2[k],r 
S(W(i)x)
belongs to the probability simplex. Thus, for each example xin the training
data, we introduce a regularization term of the form: 
(W;x) :=P
i2[k]h
r 
S(W(i)x)
, and
minimize the following objective function:
X
(x;y)2D 
1
N`
y;nX
i=1fi(x)v(G;W;x )i
+
(W;x)!
;
whereis a non-negative hyperparameter. Similar to the case of static gating, we tune over a range
ofvalues, and we only consider the choices of that force the average number of selected experts
per example to be less than or equal to k. If the application requires that the cardinality constraint
be satisﬁed strictly for every example (not only on average), then annealing in the smooth-step
function towards zero enforces this.
4 Experiments
We study the performance of DSelect-k in the context of MTL and compare with state-of-the-art gates
and baselines. In the rest of this section, we present experiments on the following real MTL datasets:
MovieLens, Multi-MNIST, Multi-Fashion MNIST, and on a real-world, large-scale recommender
system. Moreover, in Appendix C, we present an additional experiment on synthetic data (with up to
128tasks), in which we study statistical performance and perform ablation studies.
Competing Methods: We focus on a multi-gate MoE, and study the DSelect-k and Top-k gates
in both the static and per-example gating settings. For static gating, we also consider a Gumbel-
softmax based gate [ 34]–unlike DSelect-k this gate cannot control the sparsity level explicitly (see the
supplementary for details). In addition, we consider two MTL baselines. The ﬁrst baseline is a MoE
with a softmax gate (which uses all the available experts). The second is a shared bottom model [ 4],
where all tasks share the same bottom layers, which are in turn connected to task-speciﬁc neural nets.
Experimental Setup: All competing models were implemented in TensorFlow 2. We used Adam
[18] and Adagrad [ 7] for optimization, and we tuned the key hyperparameters using random grid
search (with an average of 5trials per grid point). Full details on the setup are in Appendix D.
4.1 MovieLens
Dataset: MovieLens [ 10] is a movie recommendation dataset containing records for 4;000movies
and6;000users. Following [ 37], for every user-movie pair, we construct two tasks. Task 1 is a
binary classiﬁcation problem for predicting whether the user will watch a particular movie. Task 2
8

--- PAGE 9 ---
is a regression problem to predict the user’s rating (in f1;2;:::; 5g) for a given movie. We use 1:6
million examples for training and 200;000for each of the validation and testing sets.
Experimental Details: We use the cross-entropy and squared error losses for tasks 1 and 2, re-
spectively. We optimize a weighted average of the two losses, i.e., the ﬁnal loss function is
(Loss of Task 1 ) + (1 )(Loss of Task 2 ), and we report the results for 2f0:1;0:5;0:9g.
The same loss function is also used for tuning and testing. The architecture consists of a multi-gate
MoE with 8experts, where each of the experts and the task-speciﬁc networks is composed of ReLU-
activated dense layers. For each , we tune over the optimization and gate-speciﬁc hyperparameters,
including the number of experts to select (i.e., k in DSelect-k and Top-k). After tuning, we train
each model for 100repetitions (using random initialization) and report the averaged results. For full
details, see Appendix D.1.
Results: In Table 1, we report the test loss and the average number of selected experts. The results
indicate that for all values of , either one of our DSelect-k gates (static or per-example) outperforms
the competing methods, in terms of both the test loss and the number of selected experts. In the static
gating setting, there does not seem to be a clear winner among the three competing methods (Top-k,
DSelect-k, and Gumbel Softmax), but we note that DSelect-k outperforms both Top-k and Gumbel
Softmax for two out of the three choices of . Notably, the softmax MoE is uniformly outperformed
by the DSelect-k and Top-k gates, so sparsity in gating seems to be beneﬁcial on this dataset. Our
hypothesis is that softmax MoE is overﬁtting and the sparse gating methods are mitigating this issue.
In Table C.5 in the appendix, we additionally report the individual task metrics (loss and accuracy).
= 0:1 = 0:5 = 0:9
Loss Experts Loss Experts Loss Experts
StaticDSelect-k 40155 2.7 38043 1:5 36902 1:3
Top-k 40124 2:0 38182 2.0 36936 2.0
Gumbel Softmax 41713 2.7 38982 2.6 36884 3.6
Per-exampleDSelect-k 40066 1:5 38233 1:2 36792 1:1
Top-k 40278 2.0 38414 2.0 37413 2.0
BaselinesSoftmax MoE 40901 8.0 39603 8.0 384710 8.0
Shared Bottom 40372 - 38682 - 36871 -
Table 1: Test loss (with standard error) and average number of selected experts on MovieLens. The parameter
is the weight of Task 1’s loss (see text for details). The test loss is multiplied by 104.
4.2 Multi-MNIST and Multi-Fashion MNIST
Datasets: We consider two image classiﬁcation datasets: Multi-MNIST and Multi-Fashion [ 32],
which are multi-task variants of the MNIST [ 20] and Fashion MNIST [ 38] datasets. We construct the
Multi-MNIST dataset similar to [ 32]: uniformly sample two images from MNIST and overlay them
on top of each other, and (ii) shift one digit towards the top-left corner and the other digit towards
the bottom-right corner (by 4pixels in each direction). This procedure leads to 3636images with
some overlap between the digits. The Multi-Fashion is constructed in a similar way by overlaying
images from the Fashion MNIST dataset. For each dataset, we consider two classiﬁcation tasks:
Task 1 is to classify the top-left item and Task 2 is to classify the bottom-right item. We use 100;000
examples for training, and 20;000examples for each of the validation and testing sets.
Experimental Details: We use cross-entropy loss for each task and optimize the sum of the losses5.
The model is a multi-gate MoE with 8experts, where each expert is a convolutional neural network
and each task-speciﬁc network is composed of a number of dense layers. We tune the optimization
and gate-speciﬁc hyperparameters, including the number of experts to select, and use the average of
the task accuracies as the tuning metric. After tuning, we train each model for 100repetitions (using
random initialization) and report the averaged results. For full details, see Appendix D.2.
Results: In Table 2, we report the test accuracy and the number of selected experts for the Multi-
MNIST and Multi-Fashion datasets. On Multi-MNIST, DSelect-k (static) outperforms Top-k and
Gumbel Softmax, in terms of both task accuracies and number of selected experts. For example, it
achieves over 1%improvement in Task 2’s accuracy compared to Top-k (static). DSelect-k (static)
comes close to the performance of the Softmax MoE, but uses less experts ( 1:7vs.8experts). Here
DSelect-k (per-example) does not offer improvement over the static variant (unlike the MovieLens
dataset). On Multi-Fashion, we again see that DSelect-k (static) performs best in terms of accuracy.
5Due to the symmetry in the problem, assigning the two tasks equal weights is a reasonable choice.
9

--- PAGE 10 ---
Multi-MNIST Multi-Fashion MNIST
Accuracy 1 Accuracy 2 Experts Accuracy 1 Accuracy 2 Experts
StaticDSelect-k 92:560:03 90:980:04 1:7 83:780:05 83:340:05 1:8
Top-k 91:930:06 90:030:08 4 83:440:07 82:660:08 4
Gumbel Softmax 92:30:05 90:630:05 1.8 83:660:07 83:280:05 1:5
Per-exampleDSelect-k 92:420:03 90:70:03 1:5 83:690:04 83:130:04 1:5
Top-k 92:270:03 90:450:03 4 83:660:04 83:150:04 4
BaselinesSoftmax MoE 92:610:03 91:00:03 8 83:480:04 82:810:04 8
Shared Bottom 91:30:04 89:470:04 - 82:050:05 81:370:06 -
Table 2: Test accuracy (with standard error) and number of selected experts on Multi-MNIST/Fashion.
Tasks / Methods DSelect-k Top-k
E. Task 1 (AUC) 0.81030.0002 0.74810.0198
E. Task 2 (AUC) 0.81610.0002 0.76240.0169
E. Task 3 (RMSE) 0.28740.0002 0.34060.0180
E. Task 4 (RMSE) 0.87810.0014 1.12130.0842
E. Task 5 (AUC) 0.75240.0003 0.59660.0529
S. Task 1 (AUC) 0.61330.0066 0.50800.0047
S. Task 2 (AUC) 0.84680.0289 0.59810.0616
S. Task 3 (AUC) 0.92590.0008 0.66650.0091
Table 3: Average performance (AUC and RMSE) and standard
error on a real-world recommender system with 8 tasks: “E.”
and “S.” denote engagement and satisfaction tasks, respectively.
Figure 3: Expert weights of the DSelect-k gates
on the recommender system.
4.3 A Large-scale Recommender System
We study the performance of DSelect-k and Top-k in a real-word, large-scale content recommendation
system. The system encompasses hundreds of millions of unique items and billions of users.
Architecture and Dataset: The system consists of a candidate generator followed by a multi-task
ranking model, and it adopts a framework similar to [ 41,35]. The ranking model makes predictions
for 6 classiﬁcation and 2 regression tasks. These can be classiﬁed into two categories: (i) engagement
tasks (e.g., predicting user clicks, bad clicks, engagement time), and (ii) satisfaction tasks (e.g.,
predicting user satisfaction behaviors such as likes and dislikes). We construct the dataset from the
system’s user logs (which contain historical information about the user and labels for the 8tasks).
The dataset consists of billions of examples (we do not report the exact number for conﬁdentiality).
We use a random 90/10 split for the training and evaluation sets.
Experimental Details: We use the cross-entropy and squared error losses for the classiﬁcation
and regression tasks, respectively. The ranking model is based on a multi-gate MoE, in which each
task uses a separate static gate. The MoE uses 8experts, each composed of dense layers. For
both the DSelect-k and Top-k based models, we tune the learning rate and the experts’ architecture.
Then, using the best hyperparameters, we train the ﬁnal models for 5repetitions (using random
initialization). For additional details, see Appendix D.3.
Results: In Table 3, we report the out-of-sample performance metrics for the 8tasks. The results
indicate that DSelect-k outperforms Top-k on all tasks, with the improvements being most pronounced
on the satisfaction tasks. In Figure 3, we show a heatmap of the expert weights chosen by the DSelect-
k gates. Notably, for DSelect-k, all engagement tasks share at least one expert, and two of the
satisfaction tasks share the same expert.
5 Conclusion
We introduced DSelect-k: a continuously differentiable and sparse gate for MoE, which can be
trained using ﬁrst-order methods. Given a user-speciﬁed parameter k, the gate selects at most kof
thenexperts. Such direct control over the sparsity level is typically handled in the literature by
adding a cardinality constraint to the optimization problem. One of the key ideas we introduced is a
binary encoding scheme that allows for selecting kexperts, without requiring any constraints in the
optimization problem. We studied the performance of DSelect-k in MTL settings, on both synthetic
and real datasets. Our experiments indicate that DSelect-k can achieve signiﬁcant improvements in
prediction and expert selection, compared to state-of-the-art MoE gates and MTL baselines.
Societal Impact: MoE models are used in various applications (as discussed in the introduction).
DSelect-k can improve the interpretability and efﬁciency of MoE models, thus beneﬁting the underly-
ing applications. We do not see direct, negative societal impacts from our proposal.
10

--- PAGE 11 ---
Acknowledgements: The research was conducted while Hussein Hazimeh was at Google, and part
of the writing was done during his time at MIT. At MIT, Hussein Hazimeh and Rahul Mazumder
acknowledge research funding from the Ofﬁce of Naval Research [Grant ONR-N000141812298].
References
[1]Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional com-
putation in neural networks for faster models. CoRR , abs/1511.06297, 2015. URL http:
//arxiv.org/abs/1511.06297 .
[2]Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432 , 2013.
[3]Mathieu Blondel, Andre Martins, and Vlad Niculae. Learning classiﬁers with fenchel-young
losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference
on Artiﬁcial Intelligence and Statistics , pages 606–615. PMLR, 2019.
[4] Rich Caruana. Multitask learning. Machine learning , 28(1):41–75, 1997.
[5]Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An
information-theoretic perspective on model interpretation. In International Conference on
Machine Learning , pages 883–892. PMLR, 2018.
[6]Gonçalo M Correia, Vlad Niculae, and André FT Martins. Adaptively sparse transformers. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,
pages 2174–2184, 2019.
[7]John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of machine learning research , 12(7), 2011.
[8]David S Ebert, F Kenton Musgrave, Darwyn Peachey, Ken Perlin, and Steven Worley. Texturing
& modeling: a procedural approach . Morgan Kaufmann, 2003.
[9]William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961 , 2021.
[10] F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context. Acm
transactions on interactive intelligent systems (tiis) , 5(4):1–19, 2015.
[11] Hussein Hazimeh, Natalia Ponomareva, Petros Mol, Zhenyu Tan, and Rahul Mazumder. The
tree ensemble layer: Differentiability meets conditional computation. In Hal Daumé III and
Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning ,
volume 119 of Proceedings of Machine Learning Research , pages 4138–4148, Virtual, 13–18
Jul 2020. PMLR.
[12] Yani Ioannou, Duncan Robertson, Darko Zikic, Peter Kontschieder, Jamie Shotton, Matthew
Brown, and Antonio Criminisi. Decision forests, convolutional networks and the models
in-between. arXiv preprint arXiv:1603.01250 , 2016.
[13] Robert A Jacobs. Bias/variance analyses of mixtures-of-experts architectures. Neural computa-
tion, 9(2):369–383, 1997.
[14] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures
of local experts. Neural computation , 3(1):79–87, 1991.
[15] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.
arXiv preprint arXiv:1611.01144 , 2016.
[16] Wenxin Jiang and Martin A Tanner. On the identiﬁability of mixtures-of-experts. Neural
Networks , 12(9):1253–1258, 1999.
[17] Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.
Neural computation , 6(2):181–214, 1994.
11

--- PAGE 12 ---
[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings , 2015.
[19] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 , 2013.
[20] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs
[Online]. Available: http://yann.lecun.com/exdb/mnist , 2, 2010.
[21] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-
tional computation and automatic sharding. arXiv preprint arXiv:2006.16668 , 2020.
[22] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. Modeling task
relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages
1930–1939, 2018.
[23] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. arXiv preprint arXiv:1611.00712 , 2016.
[24] Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention
and multi-label classiﬁcation. In International Conference on Machine Learning , pages 1614–
1623. PMLR, 2016.
[25] Krzysztof Maziarz, Eﬁ Kokiopoulou, Andrea Gesmundo, Luciano Sbaiz, Gabor Bartok,
and Jesse Berent. Gumbel-matrix routing for ﬂexible multi-task learning. arXiv preprint
arXiv:1910.04915 , 2019.
[26] Max Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, and Chris J Maddison. Gradient
estimation with stochastic softmax tricks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33,
pages 5691–5704. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/3df80af53dce8435cf9ad6c3e7a403fd-Paper.pdf .
[27] Ben Peters, Vlad Niculae, and André FT Martins. Sparse sequence-to-sequence models. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages
1504–1519, 2019.
[28] Prajit Ramachandran and Quoc V Le. Diversity and depth in per-example routing models. In
International Conference on Learning Representations , 2018.
[29] Clemens Rosenbaum, Tim Klinger, and Matthew Riemer. Routing networks: Adaptive selection
of non-linear functions for multi-task learning. In International Conference on Learning
Representations , 2018.
[30] Randi J Rost, Bill Licea-Kane, Dan Ginsburg, John Kessenich, Barthold Lichtenbelt, Hugh
Malan, and Mike Weiblen. OpenGL shading language . Pearson Education, 2009.
[31] Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint
arXiv:1706.05098 , 2017.
[32] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In
Advances in neural information processing systems , pages 3856–3866, 2017.
[33] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V . Le, Geoffrey E.
Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-
experts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings , 2017.
12

--- PAGE 13 ---
[34] Ximeng Sun, Rameswar Panda, Rogerio Feris, and Kate Saenko. Adashare: Learning what to
share for efﬁcient deep multi-task learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems , volume 33,
pages 8728–8740. Curran Associates, Inc., 2020. URL https://proceedings.neurips.
cc/paper/2020/file/634841a6831464b64c072c8510c7f35c-Paper.pdf .
[35] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. Progressive layered extraction
(ple): A novel multi-task learning (mtl) model for personalized recommendations. In Fourteenth
ACM Conference on Recommender Systems , pages 269–278, 2020.
[36] Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph E Gonzalez. Skipnet: Learning
dynamic routing in convolutional networks. In Proceedings of the European Conference on
Computer Vision (ECCV) , pages 409–424, 2018.
[37] Yuyan Wang, Zhe Zhao, Bo Dai, Christopher Fifty, Dong Lin, Lichan Hong, and Ed H Chi.
Small towers make big differences. arXiv preprint arXiv:2008.05808 , 2020.
[38] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747 , 2017.
[39] Sang Michael Xie and Stefano Ermon. Reparameterizable subset sampling via continuous
relaxations. In International Joint Conference on Artiﬁcial Intelligence , 2019.
[40] Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas
Pﬁster. Differentiable top-k with optimal transport. Advances in Neural Information Processing
Systems , 33, 2020.
[41] Zhe Zhao, Lichan Hong, Li Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee Kumthekar,
Maheswaran Sathiamoorthy, Xinyang Yi, and Ed Chi. Recommending what video to watch
next: a multitask ranking system. In Proceedings of the 13th ACM Conference on Recommender
Systems , pages 43–51, 2019.
13

--- PAGE 14 ---
A Additional Related Work
MTL: In MTL, deep learning-based architectures that perform soft-parameter sharing, i.e., share
model parameters partially, are proving to be effective at exploiting both the commonalities and
differences among tasks [ 31]. One ﬂexible architecture for soft-parameter sharing is the multi-gate
MoE [ 22]. We use the multi-gate MoE in our experiments and compare both sparse and dense
gates—[ 22] considered only dense gates. In addition, several works have recently considered gate-
like structures for ﬂexible parameter sharing in MTL. For instance, [ 34,25] give each task the
ﬂexibility to use or ignore components inside the neural network. The decisions are modeled using
binary random variables, and the corresponding probability distributions are learned using SGD and
the Gumbel-softmax trick [ 15]. This approach is similar to static gating, but it does not support
per-example gating. Moreover, the number of nonzeros cannot be directly controlled (in contrast
to our gate). Our work is also related to [ 29] who introduced “routers” (similar to gates) that can
choose which layers or components of layers to activate per-task. The routers in the latter work are
not differentiable and require reinforcement learning.
B Methodology Details
B.1 Proofs
B.1.1 Proof Proposition 1
Letf=ffigi2[n]. To prove equivalence, we need to establish the following two directions: (I) an
optimal solution (f;;Z)to Problem (5) can be used to construct a feasible solution (f;w)to
Problem (2) and both solutions have the same objective, and (II) an optimal solution (f;w)to
Problem (2) can be used to construct a feasible solution (f;;Z )to Problem (5) and both solutions
have the same objective. Direction (I) is trivial: the solution deﬁned by f=fandw=q(;Z)
is feasible for Problem (2) and has the same objective as (f;;Z).
Next, we show Direction (II). Let s=kwk0and denote by tjthe index of the j-th largest element
inw, i.e., the nonzero entries in warew
t1>w
t2>>w
ts. For everyi2[s], setz(i)to the
binary representation of ti 1. Ifs<k, then we set the remaining (unset) z(i)’s as follows: for
i2fs+ 1;s+ 2;:::;kgsetz(i)to the binary representation of ts 1. By this construction, the
nonzero indices selected by r(z(i)),i2[k]are exactly the nonzero indices of w.
To construct , there are two cases to consider: (i) s=kand (ii)s< k. Ifs=k, then set
i= log(w
ti)fori2[k]. Therefore, ()i=w
tifori2[k], and consequently q(;Z) =w.
Otherwise, if s< k, then seti= log(w
ti)fori2[s 1]andi= log(w
ts=(k s))for
i2[s;s+ 1;:::;k ]. Thus, fori2[s 1], we have()i=w
ti, i.e., the weights of the nonzero
indicestj,j2[s 1]inq(;Z)are equal to those in w. The weight assigned to the nonzero index
tsinq(;Z)is:P
i2[s;s+1;:::;k]()i=P
i2[s;s+1;:::;k]w
ts=(k s) =w
ts. Therefore,
q(;Z) =w. In both (i) and (ii), we have q(;Z) =w, so the solution (f;;Z )is feasible and
has the same objective as (f;w).
B.1.2 Proof of Proposition 2
First, we will use induction to show that r(S(z))belongs to the probability simplex. Speciﬁcally, we
will prove that for any integer t1andz2Rt,r(S(z))belongs to the probability simplex.
Our base case is for t= 1. In this case, there is a single binary encoding variable z12Rand
2experts. The single expert selector r(S(z1))is deﬁned as follows: r(S(z1))1= 1 S(z1)and
r(S(z1))2=S(z1). The latter two terms are non-negative and sum up to 1. Thus, r(S(z1))belongs
to the probability simplex.
Our induction hypothesis is that for some t1and anyz2Rt,r(S(z))belongs to the probability
simplex. For the inductive step, we need to show that for any v2Rt+1,r(S(v))belongs to the
probability simplex. From the deﬁnition of r(:), the following holds:
r(S(v))i=r(S([v1;v2;:::;vt]T))i(1 S(vt+1))i2[2t]
r(S([v1;v2;:::;vt]T))i 2tS(vt+1)i2[2t+1]n[2t](B.8)
14

--- PAGE 15 ---
By the induction hypothesis, we have r(S([v1;v2;:::;vt]T))i0for anyi. Moreover, S(:)is a
non-negative function. Therefore, r(S(v))i0for anyi. It remains to show that the sum of the
entries inr(S(v))is1, which we establish next:
2t+1X
i=1r(S(v))i=2tX
i=1r(S(v))i+2t+1X
i=2t+1r(S(v))i
(B.8)=2tX
i=1r(S([v1;:::;vt]T))i(1 S(vt+1)) +2t+1X
i=2t+1r(S([v1;:::;vt]T))i 2tS(vt+1)
(B.9)
Using a change of variable, the second summation in the above can be rewritten as follows:P2t+1
i=2t+1r(S([v1;v2;:::;vt]T))i 2tS(vt+1) =P2t
i=1r(S([v1;v2;:::;vt]T))iS(vt+1). Plugging
the latter equality into (B.9) and simplifying, we getP2t+1
i=1r(S(v))i= 1. Therefore, r(S(v))
belongs to the probability simplex, which establishes the inductive step. Finally, we note that ~q(;Z)
belongs to the probability simplex since it is a convex combination of probability vectors.
B.2 Extending DSelect-k to arbitrary n
Suppose that the number of experts nis not a power of 2. For the DSelect-k gate ~q(;Z)to work
in this setting, we need each single expert selector rused by the gate to be able to handle nexperts.
Next, we discuss how rcan handlenwhen it is not a power of 2. Letmbe the smallest integer
so thatn <2m. Then, we treat the problem as if there are 2mexperts and use a binary encoding
vectorz2Rm. Fori2[n], we let entry r(z)ibe the weight of expert iin the MoE. Note that the
entriesr(z)i,i2fn+ 1;n+ 2;:::; 2mg, are not associated with any experts. To avoid a situation
wherer(z)assigns nonzero probability to the latter entries, we add the following penalty to the
objective function:P
i2[n]r(z)iwhereis a non-negative parameter used to control the strength of
the penalty. This penalty encourages to r(z)i;i2[n](i.e., the entries associated with the nexperts)
to get more probability. In our experiments, we observe thatP
i2[n]r(z)iconverges to 1, when
is sufﬁciently large. The penalty described above is part of our TensorFlow implementation of
DSelect-k. We also note that there are other potential alternatives to deal with the entries r(z)i,
i2fn+ 1;n+ 2;:::; 2mg, without adding a penalty to the objective function. For example, one
alternative is to randomly assign each of r(z)i,i2fn+ 1;n+ 2;:::; 2mgto one of the nexperts.
B.3 Smooth-step Function
In Figure B.4, we plot the smooth-step function [ 11] and the logistic function L(x) = (1 +e 6x) 1.
Note that the logistic function is re-scaled to be on the same scale as the smooth-step function.
Figure B.4: The Smooth-step ( = 1) and Logistic functions.
15

--- PAGE 16 ---
C Additional Experimental Results
C.1 Prediction and Expert Selection Performance on Synthetic Data
In this experiment, we aim to (i) understand how the number of experts and tasks affects the prediction
and expert selection performance for the different gates, and (ii) quantify the beneﬁt from binary
encoding in our gate through an ablation study. We focus on a static gating setting, where we consider
the DSelect-k and Top-k gates, in addition to two variants of the DSelect-k gate used for ablation. To
better quantify the expert selection performance and avoid model mis-speciﬁcation, we use synthetic
data generated from a multi-gate MoE. First, we describe our data generation process.
Synthetic Data Generation: We consider 128regression tasks, separated into four mutually
exclusive groups:fGigi2[4], whereGiis the set of indices of the tasks in group i. As we will discuss
next, the tasks are constructed in a way so that tasks within each group are highly related, while tasks
across groups are only marginally related. Such a construction mimics real-world applications in
which tasks can be clustered in terms of relatedness.
Each group consists of 16tasks which are generated from a group-speciﬁc MoE. The group-speciﬁc
MoE consists of 4experts:ffigi2[4]. Each expert is the sum of 4ReLU-activated units. The output
of each task in the group is a convex combination of the 4experts. Speciﬁcally, for each task t2[16]
in the group, let (t)2R4be a task-speciﬁc weight vector. Then, given an input vector x, the output
of tasktis deﬁned as follows:
y(t)(x) :=4X
i=1((t))ifi(x)
For each group, we create an instance of the group-speciﬁc MoE described above, where we initialize
all the weights randomly and independently from the other groups. In particular, we sample the
weights of each expert independently from a standard normal distribution. To encourage relatedness
among tasks in each group, we sample the task weights [(1);(2):::(16)]from a zero-mean
multivariate normal distribution where we set the correlation between any two task weights to 0:8.
To generate the data, we sample a data matrix X, with 140;000observations and 10features, from a
standard normal distribution. The data matrix is shared by all 128tasks and the regression outputs
are obtained by using Xas an input to each group-speciﬁc MoE. We use 100;000observations for
the training set and 20;000observations for each of the validation and testing sets.
Experiment Design: We consider a multi-gate MoE and compare the following static gates:
DSelect-k gate, Top-k gate, and an “ablation” gate (which will be discussed later in this section). Our
goal is to study how, for each gate, the number of tasks affects the prediction and expert selection
performance. To this end, we consider 4regression problems, each for a different subset of the 128
tasks; speciﬁcally, we consider predicting the tasks in (i) G1(16tasks), (ii)G1[G2(32tasks), (iii)
G1[G2[G3(64tasks), and (iv) G1[G2[G3[G4(128tasks). In each of the four problems, we
use a multi-gate MoE to predict the outputs of the corresponding tasks simultaneously. The MoE
has the same number of experts used to generate the data, i.e., if Tis the total number of tasks in
the problem, the MoE consists of T=4experts, where the experts are similar to those used in data
generation (but are trainable). Each task is associated with a task-speciﬁc gate, which chooses a
convex combination of 4out of theT=4experts. Note that unlike the architecture used to generate
the data, each task gate here is connected to all experts, even those belonging to the unrelated groups.
The architecture used to generate the data can be recovered if the task gates across groups do not
share experts, and the task gates within each group share the same 4experts. We use squared error
loss for training and tuning.
Ablation: In addition to comparing the DSelect-k and Top-k gates, we perform an ablation study
to gain insight on the role of binary encoding in the DSelect-k gate. Recall that in the DSelect-k
gate, we introduced the single expert selector which learns a one-hot encoded vector (using a binary
encoding scheme). In the literature, a popular way to learn such one-hot encoded vectors is by using
a softmax (with additional heuristics such as temperature annealing to ensure that the probability
concentrates on one entry). Thus, in our ablation study, we consider the DSelect-k gate ~q(;Z),
and we replace each single expert selector r(:)with a softmax-based selector. More precisely, let
2Rkand(i)2Rn,i2[k], be learnable parameter vectors. Then, we consider the following
“ablation” gate: h(;) :=Pk
i=1()i((i)):Here,()idetermines the weight assigned to
16

--- PAGE 17 ---
Figure C.5: Predictive and expert selection performance on synthetic data generated from a MoE.
selectori, and((i))acts as a surrogate to the single expert selector r(S(z(i))). To ensure that
((i))selects a single expert (i.e., leads to a one-hot encoding), we consider two alternatives: (i)
annealing the temperature of ((i))during training6, and (ii) augmenting the objective with an
entropy regularization term (similar to that of the DSelect-k gate) to minimize the entropy of each
((i)). In our results, we refer to (i) by “Ablation (Annealing)” and to (ii) by “Ablation (Entropy)”.
Note that these two ablation alternatives can converge to a one-hot encoding asymptotically (due to
the nature of the softmax), whereas our proposed gate can converge in a ﬁnite number of steps.
Measuring Expert Selection Performance: To quantify the similarity between the experts selected
by the different tasks, we use the Jaccard index. Given two tasks, let AandBbe the sets of experts
selected by the ﬁrst and second tasks, respectively. The Jaccard index of these two sets is deﬁned
by:jA\Bj=jA[Bj. In our experiments, we compute: (i) the average Jaccard index for the related
tasks, and (ii) the average Jaccard index for unrelated tasks. Speciﬁcally, we obtain (i) by computing
the Jaccard index for each pair of related tasks, and then averaging. We obtain (ii) by computing the
Jaccard index over all pairs of tasks that belong to different groups (i.e., pairs in the same group are
ignored), and then averaging.
Results: After tuning, we train each competing model, with the best hyperparameters, for 100
randomly-initialized repetitions. In Figure C.5, we plot the performance measures (averaged over
the repetitions) versus the number of tasks. In the left plot, we report the MSE on the test set.
In the middle and right plots we report the (averaged) Jaccard index for the related and unrelated
tasks, respectively. In the latter two plots, we also consider a random gate which chooses 4experts
uniformly at random, and plot the expected value of its Jaccard index. In Figure C.5 (middle), a larger
index is better since the related tasks will be sharing more experts. In contrast, in Figure C.5 (right),
a lower index is preferred since the unrelated tasks will be sharing less experts. For all methods,
the Jaccard index in Figures C.5 (middle) and (right) decreases with the number of tasks. This is
intuitive, since as the number of tasks increases, we use more experts, giving any two given gates
more ﬂexibility in choosing mutually exclusive subsets of experts.
Overall, the results indicate that DSelect-k gate signiﬁcantly outperforms Top-k in all the considered
performance measures, and the differences become more pronounced as the number of tasks increases.
For example, at 128tasks, DSelect-k achieves over 40% improvement in MSE and 76% improvement
in Jaccard index for related tasks, compared to Top-k. The DSelect-k gate also outperforms the
6There are pathological cases where annealing the temperature in softmax will converge to more than one
nonzero entry. This can happen when multiple entries in the input to the softmax have exactly the same value.
17

--- PAGE 18 ---
two ablation gates in which we replace the binary encoding by a Softmax-based selector. The latter
improvement suggests that the proposed binary encoding scheme is relatively effective at selecting
the right experts. We also investigated the poor performance of the Ablation (Entropy) gate, and it
turns out that the Softmax-based single expert selectors, i.e., the ((i))’s, tend to select the same
expert. Speciﬁcally, we set k= 4in the ablation gate, but it ends up selecting 2experts in many of
the training repetitions. In contrast, the DSelect-k and Top-k gates select 4experts.
C.2 Gate Visualizations
C.2.1 MovieLens
In Figure C.6, we plot the expert weights during training on the MovieLens dataset, for the Top-k and
DSelect-k gates (after tuning both models). The plots show that Top-k exhibits frequent “jumps”,
where in a single training step an expert’s weight can abruptly change from a nonzero value to
zero. These jumps keep occurring till the end of training (at around 105training steps). In contrast,
DSelect-k has smooth transitions during training. Additional details on the MovieLens dataset and
the MoE architecture used can be found in Section 4.1 of the paper.
Figure C.6: Expert weights during training on the MovieLens dataset. Each color corresponds to a
separate expert. The plots are for the best models obtained after tuning.
C.2.2 Synthetic Data
Here we consider a binary classiﬁcation dataset generated from a static MoE that consists of 4experts.
We train another MoE model which employs 16experts: 4of these experts are copies (i.e., have
exactly same weights) of the 4experts used in data generation, and the rest of the experts are randomly
initialized. We freeze all the experts and train only over the gate parameters. In this simple setting, we
expect the gate to be able to recover the 4experts that were used to generate the data. We trained two
MoE models: one based on Top-k and another based on DSelect-k. After tuning both models, Top-k
recovered only 1right expert (and made 3mistakes), whereas our model recovered all 4experts. In
Figures C.7 and C.8, we plot the expert weights during training, for the Top-k and DSelect-k gates,
respectively. The Top-k exhibits a sharp oscillatory behavior during training, whereas DSelect-k has
smooth transitions.
Additional details on data generation and model: We consider a randomly initialized “data-
generating” MoE with 4experts (each is a ReLU-activated dense layer with 4units). The output of
the MoE is obtained by taking the average of the 4experts and feeding that into a single logistic
unit. We generate a multivariate normal data matrix Xwith 20,000 observations and 10 features
(10,000 observations are allocated for each of the training and validation sets) . To generate binary
classiﬁcation labels, we use Xas an input to the data-generating MoE and apply a sign function to
the corresponding output. For training and tuning, we consider a MoE architecture with 16experts:
4of these experts are copies of the experts used in data generation, and the rest of the experts are
initialized randomly. All experts are frozen (not trainable). A trainable gate chooses 4out of the 16
experts, and the ﬁnal result is fed into a single logistic unit. We optimized the cross-entropy loss
using Adam with a batch size of 256, and tuned over the learning rate in f10 1;10 2;:::; 10 5g.
18

--- PAGE 19 ---
Figure C.7: Expert weights during training on synthetic data generated from a MoE. Each color
corresponds to a separate expert. The left plot is a magniﬁed version of the right plot. The plots are
for the best model obtained after tuning.
Figure C.8: Expert weights during training on synthetic data generated from a MoE. Each color
corresponds to a separate expert. The left plot is a magniﬁed version of the right plot. The plots are
for the best model obtained after tuning.
C.3 Gate Convergence and FLOPS
In Table C.4, we report the percentage of training steps required for S(Z)to converge to a binary
matrix in the DSelect-k gate, on several real datasets. These results are based on the tuned models
discussed in Section 4 of the paper. We also report the number of ﬂoating point operations (FLOPS)
required by the MoE based on DSelect-k relative to the MoE based on Top-k, during training. The
results indicate that the number of training steps till convergence to a binary matrix depends on the
speciﬁc dataset: ranging from only 0:04% on the MovieLens dataset to 80% on the Multi-Fashion
MNIST. Moreover, on certain datasets (MovieLens with = 0:9and Multi-MNIST), DSelect-k
requires less FLOPS during training than Top-k, i.e., DSelect-k is effective at conditional training (on
these particular datasets).
MovieLensMulti-MNIST Multi-Fashion= 0:1= 0:5= 0:9
% Training Steps until Binary S(Z) 11.37 9.39 0.04 42.33 80.02
FLOPS(DSelect-k)/FLOPS(Top-k) 1.5 1.2 0.6 0.8 1.2
Table C.4: We report two statistics: (i) Percentage of training steps required for the DSelect-k gate to converge
to a binary matrix, and (ii) the number of FLOPS needed by the DSelect-k based MoE during training relative
to that of Top-k. The parameter controls the weight assigned to task 1’s loss in the MovieLens dataset—see
Section 4.1 of the paper for more details.
19

--- PAGE 20 ---
C.4 MovieLens
In Table C.5, we report the accuracy for task 1 (classiﬁcation) and the loss for task 2 ( regression) for
the competing methods on the MovieLens dataset.
= 0:1 = 0:5 = 0:9
T2 Loss T1 Accuracy T2 Loss T1 Accuracy T2 Loss T1 Accuracy
StaticDSelect-k 40385 83:030:07 39266 83:860:02 39435 84:040:01
Top-k 40564 84:090:05 40024 84:210:02 38844 84:170:02
Gumbel Softmax 41722 80:760:04 40853 83:710:01 38784 84:170:02
Per-exampleDSelect-k 40307 83:030:12 39817 83:920:03 39321 84:070:02
Top-k 40579 83:280:139958 83:910:03 39144 84:050:01
BaselinesSoftmax MoE 40471 78:730:01 40283 83:560:01 39703 83:860:02
Shared Bottom 39932 79:060:02 38752 82:650:02 39912 84:010:01
Table C.5: Mean test loss for task 2 (T2) and accuracy for task 1 (T1) on MovieLens (standard error is shown
next to each mean). The parameter determines the weight of Task 1’s loss (see text for details). The test loss is
multiplied by 104.
D Experimental Details
Computing Setup: We ran the experiments on a cluster that automatically allocates the computing
resources. We do not report the exact speciﬁcations of the cluster for conﬁdentiality.
Gumbel-softmax Gate: [34,25] present an approach for learning which layers in a neural network
to activate on a per-task basis. The decision to select each layer is modeled using a binary random
variable whose distribution is learned using the Gumbel-softmax trick. Note that the latter approach
does not consider a MoE model. Here we adapt the latter approach to the MoE; speciﬁcally we
consider a Gumbel-softmax gate that uses binary variables to determine which experts to select. Given
nexpertsffign
i=1, this gate uses nbinary random variables fUign
i=1, whereUidetermines whether
expertfiis selected. Moreover, the gate uses an additional learnable vector 2Rnthat determines
the weights of the experts. Speciﬁcally, the gate is a function d(;U)whosei-th component (for any
i2[n])is given by:
d(;U)i=()iUi
To learn the distribution of the Ui’s, we use the Gumbel-softmax trick as described in [ 34]. More-
over, following [ 34], we add the following sparsity-inducing penalty to the objective function:
P
i2[n]log i, where iis the Bernoulli distribution parameter of Ui, andis a non-negative
parameter used to control the number of nonzeros selected by the gate. Note that the latter penalty
cannot directly control the number of nonzeros as in DSelect-k or Top-k.
D.1 MovieLens
Architecture: For MoE-based models, we consider a multi-gate MoE architecture (see Figure 1),
where each task is associated with a separate gate. The MoE uses 8experts, each of which is a
ReLU-activated dense layer with 256units, followed by a dropout layer (with a dropout rate of
0:5). For each of the two tasks, the corresponding convex combination of the experts is fed into
a task-speciﬁc subnetwork. The subnetwork is composed of a dense layer (ReLU-activated with
256units) followed by a single unit that generates the ﬁnal output of the task. The shared bottom
model uses a dense layer (whose number of units is a hyperparameter) that is shared by the two tasks,
followed by a dropout layer (with a rate of 0:5). For each task, the output of the shared layer is fed
into a task-speciﬁc subnetwork (same as that of the MoE-based models).
Hyperparameters and Tuning: We tuned each model using random grid search, with an average of
5trials per grid point. We used Adagrad with a batch size of 128and considered the following hyper-
parameters and ranges: Learning Rate: f0:001;0:01;0:1;0:2;0:3g, Epochs:f5;10;20;30;40;50g,
k for Top-k and DSelect-k: f2;4g,for DSelect-k:f0:1;1;10g,for smooth-step:f1;10g, Units
in Shared bottom: f32;256;2048;4096;8192g,in Gumbel-softmax: f10 6;10 5;:::; 10g. For
Gumbel-softmax, we pick the best solution whose expected number of nonzeros is less than or equal
to4.
20

--- PAGE 21 ---
D.2 Multi-MNIST and Multi-Fashion MNIST
Architecture: MoE-based models use a multi-gate MoE (as in Figure 1). Each of the 8experts is a
CNN that is composed (in order) of: (i) convolutional layer 1 (kernel size = 5, number of ﬁlters =
10, ReLU-activated) followed by max pooling, (ii) convolutional layer 2 (kernel size = 5, number of
ﬁlters = 20, ReLU-activated) followed by max pooling, and (iii) a stack of ReLU-activated dense
layers with 50units each (the number of layers is a hyperparameter). The subnetwork speciﬁc to
each task is composed of a stack of 3 dense layers: the ﬁrst two have 50 ReLU-activated units and the
third has 10 units followed by a softmax. The shared bottom model uses a shared CNN (with the
same architecture as the CNN in the MoE). For each task, the output of the shared CNN is fed into a
task-speciﬁc subnetwork (same as that of the MoE-based models).
Hyperparameters and Tuning: We tuned each model using random grid search, with an average
of5trials per grid point. We used Adam with a batch size of 256and considered the following hyper-
parameters and ranges: Learning Rate: f0:01;0:001;0:0001;0:00001g, Epochs:f25;50;75;100g, k
for Top-k and DSelect-k: f2;4g,for smooth-step:f0:1;1;10g, Number of dense layers in CNN:
f1;3;5g,in Gumbel-softmax: f0;10 3;10 2;10 1;1;10;1000g. For Gumbel-softmax, we pick
the best solution whose expected number of nonzeros is less than or equal to 4.
D.3 Recommender System
Each of the 8experts in the MoE consists of a stack of ReLU-activated dense layers with 256units
each. We ﬁx k to 2 in both DSelect-k and Top-k. We tune over the learning rate and architecture. For
both models, training is terminated when there is no signiﬁcant improvement in the validation loss.
D.4 Synthetic Data
We tuned each model using random grid search, with an average of 5trials per grid point. We used
Adam with a batch size of 256and considered the following hyperparameters and ranges: Learning
rate:f0:001;0:01;0:1g, Epochsf25;50;75;100g,for smooth-step:f5;10;15g,for DSelect-k:
f0:001;0:005;0:01;0:1g,for Ablation (Entropy): f10 6;10 5;10 4;10 3;10 2;10 1;1;100g.
Moreover, for Ablation (Annealing), we anneal the temperature of softmax starting from a hyperpa-
rametersdown to 10 16(the temperatures are evenly spaced on a logarithmic scale). We tune the
starting temperature soverf10 6;10 5;510 5;10 4;2:510 4;510 4;7:510 4;10 3;5
10 3;10 2g(note that such a ﬁne grid was necessary to get annealing to work for the ablation gate).
21
