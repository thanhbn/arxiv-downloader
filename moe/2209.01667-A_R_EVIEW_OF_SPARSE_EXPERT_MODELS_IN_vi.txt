# 2209.01667.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2209.01667.pdf
# Kích thước file: 3918653 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
MỘT KHẢO SÁT VỀ CÁC MÔ HÌNH CHUYÊN GIA THƯA TRONG
HỌC SÂU
William Fedus
Google BrainJeff Dean
Google ResearchBarret Zoph
Google Brain

TÓM TẮT
Các mô hình chuyên gia thưa là một khái niệm ba mười năm tuổi đang nổi lên như một kiến trúc phổ biến trong học sâu. Lớp kiến trúc này bao gồm Mixture-of-Experts, Switch Transformers, Routing Networks, BASE layers, và những kiến trúc khác, tất cả đều có ý tưởng thống nhất là mỗi ví dụ được xử lý bởi một tập con của các tham số. Bằng cách làm như vậy, mức độ thưa thớt tách rời số lượng tham số khỏi tính toán trên mỗi ví dụ, cho phép có các mô hình cực lớn nhưng hiệu quả. Các mô hình kết quả đã chứng minh được những cải thiện đáng kể trên các lĩnh vực đa dạng như xử lý ngôn ngữ tự nhiên, thị giác máy tính, và nhận dạng giọng nói. Chúng tôi khảo sát khái niệm về các mô hình chuyên gia thưa, cung cấp mô tả cơ bản về các thuật toán phổ biến, bối cảnh hóa những tiến bộ trong kỷ nguyên học sâu, và kết thúc bằng việc làm nổi bật các lĩnh vực cho công việc tương lai.

1 GIỚI THIỆU
Những tiến bộ đáng chú ý trong học máy – đặc biệt là trong ngôn ngữ tự nhiên – đã được đạt được bằng cách tăng ngân sách tính toán, dữ liệu huấn luyện, và kích thước mô hình. Các mô hình ngôn ngữ mốc quan trọng bao gồm GPT-2 (Radford et al., 2018), BERT (Devlin et al., 2018), T5 (Raffel et al., 2019), GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), Chinchilla (Hoffmann et al., 2022), và PaLM (Chowdhery et al., 2022). Tuy nhiên, các mô hình hiện đại hiện tại yêu cầu hàng nghìn bộ tăng tốc chuyên biệt, được kết nối với nhau trong vài tuần hoặc vài tháng. Do đó, những mô hình này tốn kém để sản xuất và phát sinh chi phí năng lượng cao (Patterson et al., 2021). Vì thế, khi quy mô của các hệ thống học máy tăng lên, lĩnh vực này đã tìm kiếm những mô hình huấn luyện và phục vụ hiệu quả hơn.
Các mô hình chuyên gia thưa đã nổi lên như một giải pháp đầy hứa hẹn.

F F N  1 F F N  2 F F N  4 F F N  3Add + Normaliz e
F F N  1 F F N  2 F F N  4 F F N  3
Self -A tt entionAdd + Normaliz e
x 1 x 2y 1 y 2 
Add + Normaliz e
Self -A tt entionAdd + Normaliz eSparse FFN La y ery
x
"The" "Dog"Add + Normaliz e
Self -A tt entionAdd + Normaliz eFFN La y ery
xDe n s e  M o d e l S pa r s e  M o d e l
F F NAdd + Normaliz e
F F N
Self -A tt entionAdd + Normaliz e
x 1 x 2y 1
"The" "Dog"y 2

Hình 1: So sánh một Transformer dày đặc và chuyên gia thưa. Một mô hình dày đặc (trái) gửi cả hai token đầu vào đến cùng các tham số mạng truyền thẳng (FFN). Một mô hình chuyên gia thưa (phải) định tuyến mỗi token đầu vào độc lập giữa bốn chuyên gia của nó (FFN1 FFN4). Trong sơ đồ này, mỗi mô hình sử dụng một lượng tính toán tương tự, nhưng mô hình thưa có nhiều tham số duy nhất hơn.
Lưu ý trong khi hình này trình bày một cách tiếp cận cụ thể và phổ biến của các lớp mạng truyền thẳng thưa trong một Transformer (Vaswani et al., 2017), kỹ thuật này tổng quát hơn.

Các mô hình chuyên gia thưa, trong đó, Mixture-of-Experts (MoE) là biến thể phổ biến nhất, là những mạng nơ-ron mà một tập hợp các tham số được phân vùng thành "chuyên gia", mỗi chuyên gia có trọng số duy nhất.
Đóng góp bằng nhau. Thư từ liên hệ với fliam.fedus,barretzoph g@gmail.com .
1arXiv:2209.01667v1  [cs.LG]  4 Sep 2022

--- TRANG 2 ---
Trong quá trình huấn luyện và suy luận, các mô hình định tuyến các ví dụ đầu vào đến các trọng số chuyên gia cụ thể. Kết quả là, mỗi ví dụ chỉ tương tác với một tập con của các tham số mạng, tương phản với cách tiếp cận thông thường mà toàn bộ mạng được sử dụng cho mỗi đầu vào. Vì chỉ một phần nhỏ của các chuyên gia được sử dụng cho mỗi ví dụ, lượng tính toán có thể vẫn nhỏ so với tổng kích thước mô hình.
Nhiều mô hình chuyên gia thưa hiện đại lấy cảm hứng từ Shazeer et al. (2017), đã huấn luyện mô hình lớn nhất vào thời điểm đó và đạt được kết quả mô hình hóa ngôn ngữ và dịch thuật tiên tiến.
Các mô hình chuyên gia thưa đã càng tăng phổ biến khi được kết hợp với các mô hình ngôn ngữ Transformer (Lepikhin et al., 2020; Fedus et al., 2021). Và trong khi hầu hết công việc tập trung vào xử lý ngôn ngữ tự nhiên, chúng cũng được sử dụng thành công trong nhiều lĩnh vực khác bao gồm thị giác máy tính (Puigcerver et al., 2020), nhận dạng giọng nói (You et al., 2021) và học đa phương thức (Mustafa et al., 2022). Nghiên cứu gần đây của Clark et al. (2022) nghiên cứu chặt chẽ các thuộc tính mở rộng của các mô hình chuyên gia thưa trên các kích thước mô hình khác nhau và số lượng chuyên gia. Hơn nữa, các kết quả tiên tiến trên nhiều tiêu chuẩn hiện đang được nắm giữ bởi các mô hình chuyên gia thưa như ST-MoE (Zoph et al., 2022).
Lĩnh vực này đang phát triển nhanh chóng với các tiến bộ nghiên cứu và kỹ thuật tăng cường hiểu biết của chúng ta và cải thiện kết quả thực nghiệm.

Chúng tôi thu hẹp khảo sát của mình vào các mô hình chuyên gia thưa trong kỷ nguyên học sâu (theo kinh nghiệm từ 2012-
trở đi), thuật lại những tiến bộ gần đây và thảo luận về những hướng đi tương lai đầy hứa hẹn. Để có một đánh giá toàn diện về lịch sử của Mixture-of-Experts, có trước những tiến bộ học sâu gần đây, chúng tôi giới thiệu độc giả đến khảo sát, "Hai mươi năm Mixture-of-Experts" (Yuksel et al., 2012). Hơn nữa, các mô hình chuyên gia thưa có thể được coi là một lớp đặc biệt của các mô hình tính toán thích ứng được khảo sát trong Xu and McAuley (2022). Cuối cùng, Tay et al. (2020) khảo sát một tập hợp rộng lớn hơn các phương pháp nhằm tăng hiệu quả tính toán của Transformers, trong đó, các mô hình chuyên gia thưa là một cách tiếp cận đầy hứa hẹn.

2 CÁC MÔ HÌNH CHUYÊN GIA THƯA
Khái niệm MoE trong học máy có từ ít nhất ba thập kỷ trước với công trình của Jacobs et al. (1991); Jordan and Jacobs (1994). Trong các khái niệm ban đầu, các chuyên gia định nghĩa toàn bộ một mạng nơ-ron và MoE tương tự như các phương pháp ensemble.

2.1 TRONG HỌC SÂU
Eigen et al. (2013) đề xuất các kiến trúc sử dụng các lớp Mixture-of-Experts chồng lên nhau trên MNIST bị rung lắc (LeCun et al., 1998). Công việc này sử dụng một hỗn hợp liên tục của các đầu ra của chuyên gia (lựa chọn mềm) thay vì hạn chế vào tập con hàng đầu của các chuyên gia tại mỗi lớp (lựa chọn cứng) – hạn chế tính thực tiễn của nó1. Tuy nhiên, công việc này đã tạo tiền đề cho các triển khai hiệu quả sau này dựa trên ý tưởng MoE như một thành phần của mạng nơ-ron. Thành công quy mô lớn đầu tiên của cách tiếp cận này trong học sâu đến từ Shazeer et al. (2017). Công việc này chèn một lớp MoE giữa hai lớp LSTM (Hochreiter and Schmidhuber, 1997) nơi đầu ra từ LSTM lớp dưới được gửi để tính toán trong MoE. Mô hình thưa kết quả đã đạt tiên tiến trong dịch máy, mặc dù biến thể lớn nhất với 131,072 chuyên gia và 137B tham số tổng quát hóa kém hơn các biến thể nhỏ hơn. Tuy nhiên, mặc dù thành công này, nghiên cứu tiếp theo tương đối im ắng với trọng tâm lớn hơn vào việc nghiên cứu trực tiếp Transformer (Vaswani et al., 2017). Điều này thay đổi với việc phát hành GShard (Lepikhin et al., 2020) và Switch Transformers (Fedus et al., 2021) – cả hai đều thay thế các lớp truyền thẳng trong Transformers bằng các lớp chuyên gia. Tuy nhiên, trong khi cách tiếp cận chuyên gia-như-một-lớp đã trở thành mô hình chủ đạo, các công trình gần đây hơn xem xét lại khái niệm chuyên gia như các mô hình hoàn toàn độc lập (Gururangan et al., 2021; Li et al., 2022). Điều này mang lại lợi ích về tính mô-đun và khả năng tổng hợp; Li et al. (2022) cho thấy các mạng tùy chỉnh có thể được xây dựng bằng cách tổng hợp chúng từ các mô hình ngôn ngữ chuyên gia được huấn luyện trên các miền cụ thể.

Hình 2 minh họa cơ chế định tuyến top-k ban đầu được đề xuất trong Shazeer et al. (2017) là nền tảng cho nhiều công trình tiếp theo. Các tiến bộ mới cho thuật toán định tuyến được mô tả trong Mục 4. Việc chọn chuyên gia dựa trên đầu vào thường đòi hỏi một lựa chọn rời rạc (tức là chuyên gia nào để sử dụng), điều này làm phức tạp các thuật toán lan truyền ngược dựa trên tính khả vi. Như một giải pháp, Shazeer et al. (2017) đề xuất một hàm định tuyến top-k nhận đầu vào là một token
1Chi phí tính toán đầy đủ được phát sinh với lựa chọn mềm ngay cả khi chuyên gia không cần thiết (tức là một trọng số định tuyến cực nhỏ).
2

--- TRANG 3 ---
 0 . 3  
0 . 5  
1 . 2 1 . 6  
 0 . 6  
1 . 30 . 1  
 1 . 1  
0 . 70 . 8  
 0 . 2  
1 . 5 0 . 1  
 0 . 4  
 1 . 10 . 2  
1 . 3  
 0 . 72 . 3  
 1 . 1  
0 . 11 . 7  
0 . 9  
0 . 4E x pe r t  1 E x pe r t  2 E x pe r t  5 E x pe r t  4 E x pe r t  3
R o u t e r  W e i gh t sT o k e n   
R e pr e s e n t a t i o n s
 0 . 3  
0 . 5  
1 . 20 . 2  
1 . 3  
 0 . 7Do t  P r o d u c t3 . 1 3
0 . 5 1
2 . 2 5 1 . 3 2
 2 . 8 10 . 1 4
 0 . 2 5
2 . 6 11 . 9 7
 0 . 6 80 . 7 4
1 . 5 8
0 . 020 . 1
 0 . 4 1R o u t e r  S c o r e s
0 . 6 7
0 . 05
0 . 2 70 . 01
0 . 000 . 05
0 . 03
0 . 5 90 . 3 1
0 . 020 . 2 2
0 . 5
0 . 10 . 1 1
0 . 0 7N o r m a l i z e d
R o u t e r  S c o r e sE x pe r t  W e i gh t s
T 1 T 2 T 3E 1 E 2 E 3 E 4 E 5T 1 T 2 T 3E 1 E 2 E 3 E 4 E 5

Hình 2: Sơ đồ định tuyến top-k. Chúng tôi trực quan hóa một ví dụ về sơ đồ định tuyến token top-k
trên năm chuyên gia và ba token đầu vào. Mỗi chuyên gia và token được mã hóa màu và trọng số router
(Wr) có một biểu diễn cho mỗi chuyên gia (màu phù hợp). Để xác định định tuyến, trọng số router
thực hiện một tích vô hướng với mỗi embedding token (x) để tạo ra điểm số router (h(x)).
Những điểm số này sau đó được chuẩn hóa để tổng bằng một (p(x)).

biểu diễn x và sau đó định tuyến nó đến top-k chuyên gia trong tập hợp {Ei}N i=1 của N chuyên gia. Router có một biến có thể huấn luyện Wr tính toán logits h(x) = Wr x được chuẩn hóa
qua phân phối softmax trên N chuyên gia. Giá trị cổng cho chuyên gia i được cho bởi,
pi(x) = eh(x)i / ∑N j eh(x)j. (1)

Chúng tôi ký hiệu tập hợp các chỉ số chuyên gia top-k được chọn là T. Tính toán đầu ra của lớp là
tổ hợp trọng số tuyến tính của tính toán của mỗi chuyên gia trên token theo giá trị cổng,
y = ∑ i∈T pi(x)Ei(x). (2)

Chúng tôi lưu ý rằng trái ngược với Eigen et al. (2013), lựa chọn này chỉ trên top-k chuyên gia và do đó hiệu quả hơn về mặt tính toán.

2.2 TRÊN PHẦN CỨNG HIỆN ĐẠI
Các mô hình chuyên gia thưa hiện đại đã được đồng thiết kế với các hệ thống phân tán được sử dụng để huấn luyện các mạng nơ-ron lớn nhất. Đây là trường hợp đặc biệt của mạng nơ-ron thưa (Gale et al., 2019; Dettmers and Zettlemoyer, 2019; Evci et al., 2020) tương tự ở chỗ chúng chỉ sử dụng một tập con tham số, nhưng khác biệt vì chúng có các mẫu thưa có khả năng không đều. Và trong khi mạng nơ-ron thưa generic (với các mẫu thưa không đều) làm giảm tổng FLOPs lý thuyết, chúng thường không được hỗ trợ hiệu quả trên phần cứng hiện tại chuyên về các phép toán đại số tuyến tính trên các khối bộ nhớ liền kề (đều). Mặt khác, các mô hình chuyên gia thưa kích hoạt toàn bộ khối tham số (tức là toàn bộ ma trận), và do đó dễ dàng chuyển đổi tiết kiệm FLOPs lý thuyết thành tiết kiệm thời gian thực tế trên phần cứng hiện đại (Fedus et al., 2021; Rajbhandari et al., 2022).

Các mạng nơ-ron lớn nhất (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022) hiện vượt xa khả năng bộ nhớ của một bộ tăng tốc đơn lẻ và do đó các tensor (ví dụ: trọng số, kích hoạt, biến tối ưu hóa) được chia nhỏ sử dụng các chiến lược song song khác nhau. Ba cách tiếp cận phổ biến
3

--- TRANG 4 ---
bao gồm song song dữ liệu (trọng số mô hình được sao chép, nhưng dữ liệu được chia nhỏ), song song tensor-model (Shazeer et al., 2018) (tensor dữ liệu và trọng số được chia trên các thiết bị), và song song đường ống (Harlap et al., 2018; Huang et al., 2019) (toàn bộ các lớp hoặc nhóm các lớp được chia trên các thiết bị).
Mixture-of-Experts phù hợp tự nhiên với những sơ đồ song song này. Các chuyên gia cư trú trên các bộ tăng tốc khác nhau và dữ liệu đầu vào được gửi và lấy từ chúng một cách động. Các kiến trúc sớm thường sử dụng nhiều chuyên gia nhỏ sẽ vừa với một bộ tăng tốc riêng lẻ (Lepikhin et al., 2020), nhưng các công trình sau đó thiết kế các chuyên gia lớn hơn phải được chia trên các bộ tăng tốc (Fedus et al., 2021; Du et al., 2021) và yêu cầu tối ưu hóa bổ sung cho hiệu quả giao tiếp (Shazeer et al., 2018; Roberts et al., 2022; Rajbhandari et al., 2022).

Định tuyến động trên các hệ thống phân tán phát sinh chi phí giao tiếp bổ sung ngoài các mô hình Transformer tiêu chuẩn. Gửi đầu vào đến các chuyên gia thường được triển khai như một nguyên thủy giao tiếp all2all, nơi mỗi bộ tăng tốc giao tiếp dữ liệu đến tất cả các bộ tăng tốc khác.2 Yếu tố dung lượng tác động trực tiếp đến chi phí giao tiếp bằng cách điều chỉnh kích thước batch chuyên gia (Lepikhin et al., 2020) để trở thành CF(B/E), trong đó CF là yếu tố dung lượng, B là tổng token trên mỗi batch và E là số lượng chuyên gia. Giá trị yếu tố dung lượng lớn hơn có thể cải thiện chất lượng, nhưng với chi phí tăng giao tiếp, bộ nhớ và tính toán. Các triển khai hiệu quả của nguyên thủy all2all, cùng với các thay đổi trong thuật toán định tuyến (ví dụ: giảm yếu tố dung lượng), làm giảm chi phí giao tiếp được thêm vào từ các thuật toán chuyên gia thưa.

Khi huấn luyện các Transformer phân tán bình thường, người ta biết trước batch dữ liệu nào mà mỗi bộ tăng tốc sẽ xử lý. Tuy nhiên, các thuật toán định tuyến động phá vỡ thuộc tính này vì đầu vào được định tuyến động đến các chuyên gia, điều này thường có thể dẫn đến số lượng đầu vào khác nhau được gửi đến mỗi chuyên gia. Do đó, các thuật toán định tuyến thường khuyến khích cân bằng tải trên các bộ tăng tốc để khuyến khích sử dụng tốt. Cân bằng tải đã được thực hiện bởi các loss phụ trợ (Shazeer et al., 2017) cũng như thông qua việc xử lý điều này như một bài toán gán tuyến tính (Lewis et al., 2021; Clark et al., 2022). Thêm chi tiết về các tiến bộ trong cân bằng tải được cung cấp trong Mục 4.

Cuối cùng, các tiến bộ hệ thống gần đây đã cải thiện hơn nữa cả việc huấn luyện và triển khai các mô hình MoE. Jaszczur et al. (2021) làm thưa tất cả các lớp (ví dụ: dày đặc và tự-chú ý) của một mô hình Transformer để đạt được tăng tốc suy luận 37× cho trường hợp đặc biệt của suy luận một ví dụ đơn (không batch). Kossmann et al. (2022) nới lỏng các ràng buộc của kích thước batch chuyên gia tĩnh với thư viện RECOMPILE. Hệ thống này biên dịch lại và tối ưu hóa động các tài nguyên tính toán của các mô hình Mixture-of-Experts để kích thước tensor được khớp với nhu cầu tính toán của chuyên gia, không phải mảng được thiết lập tĩnh. Tiếp theo, ngoài song song dữ liệu-, mô hình-, và chuyên gia-, thư viện DeepSpeed-MoE (Rajbhandari et al., 2022) hỗ trợ phân vùng ZeRO (Rajbhandari et al., 2019) (phân vùng đầy đủ các tensor và tập hợp lại khi cần) và ZeRO-Offload (chuyển tải lên CPU để giảm sử dụng bộ nhớ GPU). Hệ thống này mang lại cải thiện suy luận 10× (Rajbhandari et al., 2022) và dịch thuật tiên tiến (Kim et al., 2021) – tăng tính thực tiễn của những mô hình này cho các dịch vụ sản xuất.

3 THUỘC TÍNH MỞ RỘNG CỦA CÁC MÔ HÌNH CHUYÊN GIA THƯA
Cross-entropy loss của các mô hình ngôn ngữ nơ-ron dày đặc được chỉ ra là mở rộng như một định luật lũy thừa (tức là l(x) = (c/x)α cho một biến x) đối với số lượng tham số mô hình, lượng dữ liệu, và ngân sách tính toán khi không bị hạn chế bởi hai yếu tố còn lại (Kaplan et al., 2020). Các hệ số định luật lũy thừa sau đó được điều chỉnh trong Hoffmann et al. (2022), chỉ ra rằng các mô hình tối ưu-tính toán yêu cầu sự cân bằng chặt chẽ hơn trong việc mở rộng dữ liệu và tham số. Ngược lại, nghiên cứu sớm trong các mô hình chuyên gia thưa mở rộng theo kinh nghiệm – đạt được kết quả thực nghiệm mạnh mẽ – nhưng không có đặc tính hóa cẩn thận các định luật mở rộng. Hơn nữa, một số công trình làm nổi bật những khác biệt giữa hành vi upstream (ví dụ: pre-training) và downstream (ví dụ: fine-tuning) (Fedus et al., 2021; Artetxe et al., 2021), làm phức tạp hơn nữa việc hiểu và giải thích các mô hình chuyên gia thưa.

2Nhiều thuật toán định tuyến (nhưng không phải tất cả) phát sinh hai chi phí giao tiếp all2all trong bước truyền thẳng và hai chi phí khác trong bước truyền ngược. Một ví dụ về thuật toán định tuyến sử dụng nhiều hơn là lớp BASE (Lewis et al., 2021), yêu cầu bốn all2all trong bước truyền thẳng và bốn khác trong bước truyền ngược.
4

--- TRANG 5 ---
3.1 MỞ RỘNG UPSTREAM
Các mô hình chuyên gia thưa đã xuất sắc khi được huấn luyện trên các tập dữ liệu lớn. Một mô hình phổ biến trong xử lý ngôn ngữ tự nhiên là thực hiện huấn luyện upstream (ví dụ: pre-training) sau đó tiếp theo là huấn luyện downstream (ví dụ: fine-tuning) trên phân phối dữ liệu có quan tâm cụ thể. Các mô hình chuyên gia thưa đã liên tục mang lại lợi ích cao so với các đối tác dày đặc trong giai đoạn upstream. Shazeer et al. (2017) trình bày các đường cong mở rộng đối với tham số mô hình và ngân sách tính toán trên 1-Billion-Word Language-Modeling Benchmark (Chelba et al., 2013), đạt được lợi ích đáng kể so với các phiên bản dày đặc. Lepikhin et al. (2020) trình bày cải thiện dịch thuật như một hàm của quy mô mô hình, và thu được 13.5 điểm BLEU trên mô hình thưa 600B tham số lớn nhất của họ. Switch Transformers (Fedus et al., 2021) đo được tăng tốc 4-7× trong thời gian thực sử dụng cùng tài nguyên tính toán so với các mô hình T5. Công trình cũng nghiên cứu mở rộng cross entropy loss như một hàm của số lượng tham số, nhưng quan sát thấy lợi ích giảm dần với 256+ chuyên gia.

Để tiếp tục hiểu biết của chúng ta, Artetxe et al. (2021) phân biệt hành vi mở rộng upstream của các mô hình MoE trên dữ liệu in-domain và out-of-domain và tìm thấy mở rộng tốt hơn đáng kể cho mô hình hóa ngôn ngữ in-domain so với các mô hình dày đặc, xác nhận những khó khăn trong việc chuyển giao từ Fedus et al. (2021).

1 2 4 8 16 32 64 128 256
Expert Count1.6×1001.65×1001.7×1001.75×1001.8×100Validation Loss
Switch
1 2 4 8 16 32 64 128 256 512
Expert Count2×1002.05×1002.1×1002.15×1002.2×1002.25×100Validation Loss
Hash
RL-R
S-Base

Hình 3: Biểu đồ mở rộng thưa với số lượng chuyên gia. Biểu đồ mở rộng cross-entropy như một hàm của số lượng chuyên gia được hiển thị từ Fedus et al. (2021) (trái) và ba biến thể thưa từ Clark et al. (2022), S-Base, RL-R, Hash (phải). Điểm trên cùng bên trái trong cả hai biểu đồ là một mô hình dày đặc khớp tính toán gần đúng. Khi số lượng chuyên gia tăng, các mô hình trở nên ngày càng thưa và mang lại loss validation thấp hơn.

Sau những thành công thực nghiệm sớm này, Clark et al. (2022) thực hiện nỗ lực quy mô lớn đầu tiên để đặc tính hóa toán học các thuộc tính mở rộng của các mô hình chuyên gia thưa. Công trình này xem xét ba lớp mô hình thưa và rút ra một khái niệm về số lượng tham số hiệu quả (EPC). EPC ước tính tương đương tham số-dày đặc cho một mô hình chuyên gia thưa, dựa trên FLOPs và số lượng chuyên gia. Nó được rút ra bằng cách phỏng đoán rằng các mô hình chuyên gia thưa tuân theo một loss song tuyến tính và được chỉ ra thực nghiệm rằng cross entropy loss mở rộng như một định luật lũy thừa trong biến này. Hình 3 trình bày mở rộng cross entropy của Switch Transformers ở bên trái và ba biến thể thưa của Clark et al. (2022) ở bên phải.

Một thuộc tính chính của các đường cong mở rộng là lợi ích của các mô hình chuyên gia thưa giảm theo quy mô, mà khi ngoại suy, ngụ ý rằng sẽ không có lợi ích nào nữa từ sự thưa thớt vượt quá 900B tham số FLOPs. Kết quả này, tuy nhiên, phụ thuộc vào số lượng token được sử dụng để huấn luyện và tất cả các mô hình chỉ sử dụng 130B token. Nhưng dưới ánh sáng của các kết quả mở rộng gần đây từ Hoffmann et al. (2022) khuyến nghị nhiều token hơn để huấn luyện các mô hình tối ưu-tính toán (Chinchilla là mô hình 70B tham số được huấn luyện trên 1.4T token), công việc tương lai có thể xem xét lại phân tích này.

3.2 MỞ RỘNG DOWNSTREAM
Tuy nhiên, mở rộng upstream đáng tin cậy không ngay lập tức mang lại lợi ích nhất quán trên các tác vụ downstream. Trong một công trình làm nổi bật thách thức của việc chuyển giao, Fedus et al. (2021) quan sát được cải thiện 4× pre-training với một Transformer encoder-decoder tính toán thấp, tham số cao (1.6T tham số với 2048 chuyên gia trên mỗi lớp thưa), nhưng nó fine-tune kém trên các tác vụ nặng về lý luận như SuperGLUE (Wang et al., 2019) so với các mô hình dày đặc. Phát hiện này gợi ý về nghiên cứu cần thiết tiếp theo cũng như sự cân bằng tiềm năng cần thiết giữa tính toán và tham số. Tuy nhiên, kết quả thực nghiệm mạnh mẽ sớm theo sau trong suy luận few-shot, fine-tuning, và các phương thức khác.

Du et al. (2021) trình bày mở rộng của các mô hình GLaM thưa dao động từ 1B-64B FLOPs sử dụng 64 chuyên gia trên mỗi lớp thưa. GLaM đạt được kết quả tiên tiến, vượt trội so với mô hình GPT-3 175B tham số (Brown et al., 2020) trong hiệu suất zero và one-shot, trong khi sử dụng ít FLOPs hơn 49% trên mỗi token khi suy luận và tiêu thụ điện năng thấp hơn 65% (biểu đồ trái trong Hình 4). Trong một ví dụ khác về các mô hình thưa hoạt động tốt trên suy luận few-shot, hợp tác BIG-Bench (Srivastava et al., 2022) đo được cải thiện 2× của mô hình thưa so với mô hình dày đặc trên 161 tác vụ JSON được đóng góp (biểu đồ phải trong Hình 4).

100101102
GFlops Per Token Prediction10203040506070Accuracy (EM)
0.1B1.7B8B137B
0.1B/64E1.7B/64E8B/64E64B/64E
Dense
Sparse
1071081091010
Effective Parameter Count2
0246810Normalized Pref. Metrics
BIG-G (0-shot)
BIG-G (1-shot)
BIG-G (2-shot)
BIG-G Sparse (0)
BIG-G Sparse (1)
BIG-G Sparse (2)

Hình 4: Mở rộng thưa cho suy luận few-shot. Trái: Du et al. (2021) đo hiệu suất suy luận few-shot trên TriviaQA, chứng minh lợi ích nhất quán của các mô hình MoE thưa so với các mô hình dày đặc lên đến 137B tham số. Mỗi nhãn, như 8B/64E, cho biết có bao nhiêu tham số trên mỗi đầu vào được sử dụng (8B) và có bao nhiêu chuyên gia (64E). Phải: BigBench (Srivastava et al., 2022) nghiên cứu các thuộc tính mở rộng few-shot trên một tập lớn hơn gồm 161 tác vụ JSON được đóng góp để xác nhận cải thiện của các mô hình chuyên gia thưa so với các đối tác dày đặc khớp FLOP.

Cuối cùng, Srivastava et al. (2022) nghiên cứu việc hiệu chuẩn của các mô hình thưa trên các tác vụ lựa chọn nhiều BIG-Bench. Hiệu chuẩn đo mức độ mà xác suất của một dự đoán khớp với xác suất đúng. Công trình này đo hiệu chuẩn bằng Expected Calibration Error (Naeini et al., 2015) là độ lệch tuyệt đối giữa xác suất dự đoán và độ chính xác trung bình, sau khi phân bin các ví dụ theo xác suất dự đoán của chúng. Trong khi hiệu chuẩn cải thiện cho cả mô hình dày đặc và thưa lớn hơn (Hình 5), các mô hình thưa được tìm thấy là khớp với hiệu chuẩn của một mô hình dày đặc sử dụng nhiều FLOPs hơn 10×.

3.3 MỞ RỘNG SỐ LƯỢNG, KÍCH THƯỚC VÀ TẦN SUẤT CỦA CÁC LỚP CHUYÊN GIA
Một số siêu tham số quan trọng, ngoài những siêu tham số trong một Transformer dày đặc, chi phối quy mô của các mô hình chuyên gia thưa bao gồm, 1) số lượng chuyên gia, 2) kích thước của mỗi chuyên gia, và 3) tần suất của các lớp chuyên gia. Các quyết định có thể có tác động đáng kể đến mở rộng upstream và downstream.

Nhiều công trình trước đây mở rộng đến hàng nghìn chuyên gia tương đối nhỏ trên mỗi lớp, đã tạo ra chất lượng pre-training và dịch thuật xuất sắc (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021). Tuy nhiên, chất lượng của các mô hình thưa bị giảm không tương xứng dưới sự dịch chuyển miền (Artetxe et al., 2021) hoặc khi fine-tuning trên các phân phối tác vụ khác nhau (Fedus et al., 2021). Các mô hình thưa tiên tiến cho suy luận few-shot (GLaM (Du et al., 2021)) và cho fine-tuning (ST-MoE (Zoph et al., 2022)) chỉ sử dụng tới 64 chuyên gia lớn hơn – một sự cân bằng tốt hơn giữa tính toán và tham số. Kết quả của việc tăng kích thước chuyên gia, những mô hình này yêu cầu các chiến lược chia nhỏ cấp hệ thống cụ thể trên các bộ tăng tốc để chạy hiệu quả (Du et al., 2021; Rajbhandari et al., 2022).

Tiếp theo, chúng tôi tóm tắt các quy ước hiện tại xung quanh tần suất của các lớp chuyên gia. Thông thường, các mô hình thưa được xây dựng bằng cách bắt đầu với một mô hình dày đặc và chèn hoặc thay thế các lớp chuyên gia thưa
6

--- TRANG 6 ---
10710810910101011
Effective Parameter Count0.250.300.350.400.45Expected Calibration Error
Calibration on Multiple-Choice Tasks (ECE)
BIG-G (0-shot)
BIG-G (1-shot)
BIG-G (2-shot)
BIG-G (3-shot)
BIG-G Sparse (0-shot)
BIG-G Sparse (1-shot)
BIG-G Sparse (2-shot)
BIG-G Sparse (3-shot)

Hình 5: Hiệu chuẩn mô hình thưa. Expected Calibration Error cải thiện theo quy mô cho cả mô hình dày đặc và thưa. Tuy nhiên, các mô hình thưa thể hiện hiệu chuẩn tốt hơn đáng kể và gần đúng khớp với hiệu chuẩn của một mô hình dày đặc lớn hơn 10×. Hình được tái tạo từ Srivastava et al. (2022).

theo khoảng thời gian cố định hoặc theo kinh nghiệm. Ví dụ, Rajbhandari et al. (2022) đặt nhiều lớp thưa hơn gần các lớp cuối của mạng. Trong Transformer, cách tiếp cận phổ biến nhất là thay thế mỗi lớp Feed-Forward Network (FFN) khác (Lepikhin et al., 2020; Du et al., 2021; Artetxe et al., 2021; Rajbhandari et al., 2022), tức là thay thế với tần suất 0.5. Tuy nhiên, các tần suất khác đã được sử dụng, bao gồm mỗi lớp thứ tư (0.25) trong Zoph et al. (2022) và mỗi lớp (ví dụ: 1.0) trong Fedus et al. (2021). Cuối cùng, tần suất 0.5-1.0 được khuyến nghị bởi Clark et al. (2022).

Cuối cùng, câu trả lời cho câu hỏi về các siêu tham số tối ưu phụ thuộc vào ứng dụng và đặc tả hệ thống phần cứng. Công trình trước đây chứng minh kết quả pre-training và dịch thuật mạnh mẽ với số lượng chuyên gia cao (Shazeer et al., 2017; Lepikhin et al., 2020), trong khi các mô hình hoạt động tốt nhất dưới sự chuyển giao đã sử dụng ít hơn, chuyên gia lớn hơn (Du et al., 2021; Zoph et al., 2022; Mustafa et al., 2022). Hơn nữa, những quyết định này phụ thuộc vào phần cứng cao. Do các chi phí giao tiếp all2all được thêm vào để triển khai định tuyến, các mạng có tốc độ kết nối chậm hơn có thể thấy ít lớp chuyên gia hơn là tối ưu trên cơ sở thời gian đến một chất lượng nhất định. Một mô phỏng về các thuộc tính tính toán, bộ nhớ, và giao tiếp của một hệ thống phân tán sẽ hỗ trợ đáng kể các nhà thực hành để xác định nhanh hơn các thiết lập tối ưu, mà không cần các lần khởi chạy thử-và-sai tốn kém.

Chúng tôi lưu ý rằng phân tích và đánh đổi này là cho cách tiếp cận chuyên gia-như-một-lớp (Eigen et al., 2013). Ngược lại, cách tiếp cận Branch-Train-Merge (BTM) đối với chuyên gia (Li et al., 2022) là "dễ xử lý song song" ở chỗ mỗi chuyên gia là một mô hình ngôn ngữ hoàn chỉnh, có thể huấn luyện độc lập và không đồng bộ, mà không có chi phí giao tiếp đắt đỏ. Do đó, cách tiếp cận này và những cách tiếp cận khác theo sau có đặc tính mở rộng hoàn toàn khác nhau với số lượng chuyên gia.

4 THUẬT TOÁN ĐỊNH TUYẾN
Thuật toán định tuyến, một tính năng chính của tất cả các kiến trúc chuyên gia thưa, xác định nơi gửi các ví dụ. Lĩnh vực này đã được nghiên cứu rộng rãi, bao gồm các phương pháp phản trực quan sử dụng các mẫu định tuyến cố định, không học (Roller et al., 2021). Thông thường quyết định định tuyến ngây thơ là không khả vi vì nó đưa ra quyết định rời rạc về chuyên gia nào để chọn. Bài toán lựa chọn chuyên gia có thể được đúc lại như một bài toán Bandit và một số công trình đã sử dụng học tăng cường để học lựa chọn (Bengio et al., 2016; Rosenbaum et al., 2017; 2019; Clark et al., 2022).
Shazeer et al. (2017) đề xuất một heuristic khả vi sidestep những thách thức học tăng cường. Thay vì định tuyến ví dụ đến chuyên gia được chọn và tiến hành, đầu ra của tính toán chuyên gia được trọng số bởi xác suất chọn nó (Phương trình 2). Điều này tạo ra gradient đến router vì xác suất chọn chuyên gia là khả vi. Trái ngược với cách tiếp cận Bandit, nơi chỉ một chuyên gia có thể được chọn, Shazeer et al. (2017) phỏng đoán rằng cần thiết phải định tuyến đến top-k chuyên gia với k > 1. Trực giác là hai hoặc nhiều chuyên gia trên cùng một ví dụ cho phép mạng so sánh và tối ưu hóa hiệu suất tương đối. Lepikhin et al. (2020) sau đó điều chỉnh cùng thuật toán định tuyến cho kiến trúc Transformer, mang lại kết quả dịch máy tiên tiến. Tuy nhiên, Fedus et al. (2021) chứng minh rằng định tuyến top-1 có thể đạt được kết quả cạnh tranh, được xác nhận bởi công trình sau này (Clark et al., 2022).

4.1 PHÂN LOẠI ĐỊNH TUYẾN
Một cách để hiểu nhiều thuật toán định tuyến là phân tích ma trận điểm số định tuyến (tức là điểm số router từ Hình 2). Như một ví dụ minh họa, chúng tôi sử dụng một mô hình chuyên gia thưa ngôn ngữ tự nhiên. Hình 6 cho thấy các điểm số router chưa chuẩn hóa được tính toán cho ba token (cột) được định tuyến trên năm chuyên gia (hàng). Mỗi giá trị được tạo ra bởi tích vô hướng của embedding token và embedding chuyên gia (từ trọng số router). Một khi các điểm số được tính toán, có nhiều cách khác nhau để
7

--- TRANG 7 ---
3.13
0.51
2.25-1.32
-2.810.14
-0.25
2.611.97
-0.680.74
1.58
0.020.1
-0.41Experts
E1 E2 E3 E4 E5T1 T2 T3
3.13
0.51
2.25-1.32
-2.810.14
-0.25
2.611.97
-0.680.74
1.58
0.020.1
-0.41ExpertsT okensE1 E2 E3 E4 E5T1 T2 T3
3.13
0.51
2.25-1.32
-2.810.14
-0.25
2.611.97
-0.680.74
1.58
0.020.1
-0.41ExpertsT okensE1 E2 E3 E4 E5T1 T2 T3T okensChoose T op-KChoose T op-K
Globally
Decide Expert
Assignment

Hình 6: Ba lớp thuật toán định tuyến phổ biến. Chúng tôi minh họa ba phương pháp với ma trận kích hoạt ExpertsTokens thu được thông qua quá trình được giải thích trong Hình 2. Trái: "Choose Top-k" dọc theo trục Experts bao gồm thuật toán định tuyến top-k tiêu chuẩn (Shazeer et al., 2017; Lepikhin et al., 2020). Giữa: "Choose Top-k" dọc theo trục Tokens là các thuật toán định tuyến như Zhou et al. (2022). Phải: "Globally Decide Expert Assignment" là các thuật toán định tuyến như lớp BASE (Lewis et al., 2021; Clark et al., 2022).

xác định chuyên gia nào nên nhận token nào. Chúng tôi làm nổi bật ba danh mục phổ biến:
1) mỗi token chọn top-k chuyên gia, 2) mỗi chuyên gia chọn top-k token, và 3) xác định toàn cục token nào nên đi đến mỗi chuyên gia (và không sử dụng cách tiếp cận tham lam). Phân loại này cũng gợi ý hơn nữa các thuật toán định tuyến chưa được khám phá. Một ví dụ là thuật toán có lợi bằng cách nhìn cả theo chiều ngang và chiều dọc tại các điểm số router, nhưng không phát sinh chi phí của việc nhìn toàn cục. Một token trước tiên có thể chọn chuyên gia nào nó muốn đi đến, sau đó dựa trên thông tin này, mỗi chuyên gia có thể chọn token nào nó muốn.

Mỗi token chọn top-k chuyên gia. Lớp thuật toán định tuyến này có mỗi token chọn top-k chuyên gia để được gửi đến. Đây là công thức định tuyến top-2 ban đầu được đề xuất trong Shazeer et al. (2017) và được sử dụng trong Lepikhin et al. (2020) đạt được kết quả dịch máy tiên tiến. Fedus et al. (2021); Rajbhandari et al. (2022) sử dụng định tuyến top-1 với thành công. Clark et al. (2022) đề xuất một thuật toán định tuyến học tăng cường sử dụng định tuyến top-1. Tuy nhiên, thay vì chia tỷ lệ đầu ra của tính toán chuyên gia bằng xác suất router, họ sử dụng REINFORCE (Williams, 1992) với phần thưởng là cross entropy âm của token được dự đoán. Hình 7 mô tả các thuật toán định tuyến top-1, top-2 và học tăng cường.

Yang et al. (2021) giới thiệu một phần mở rộng của định tuyến top-1 bằng cách sử dụng nguyên mẫu chuyên gia để chia chuyên gia thành các nhóm khác nhau và sau đó áp dụng k quy trình định tuyến top-1. Nie et al. (2021) bắt đầu định tuyến như một hàm cổng mềm nơi tất cả chuyên gia được huấn luyện (ví dụ: một mô hình dày đặc) và ủ xuống thuật toán định tuyến top-1 tiêu chuẩn. Cách tiếp cận này (DTS-Gate) cải thiện so với Switch Transformer (Fedus et al., 2021) trên pre-training OpenWebText. Dua et al. (2021) đề xuất một cách tiếp cận tương tự là đầu tiên huấn luyện một mô hình dày đặc nơi mỗi đầu vào đi đến mọi chuyên gia và sau đó thích ứng nó để trở thành thưa. Hazimeh et al. (2021) đề xuất DSelect-k, là một phiên bản mượt mà của thuật toán định tuyến top-k cải thiện so với định tuyến top-k tiêu chuẩn. Rajbhandari et al. (2022) thiết kế PR-
8

--- TRANG 8 ---
R o u t e rF F N  1 F F N  2 F F N  4 F F N  3A d d  +  N o r m a l i z e
F F N  1 F F N  2 F F N  4 F F N  3
R o u t e r
x 1 x2y 1 y 2  
p =  0 . 6 5p =  0 . 8
"The" "Dog"R o u t e rF F N  1 F F N  2 F F N  4 F F N  3A d d  +  N o r m a l i z e
F F N  1 F F N  2 F F N  4 F F N  3
R o u t e r
x 1 x2y 1 y 2  
p =  0 . 6 5p =  0 . 8
"The" "Dog"p =  0 . 1 5
p =  0 . 3T o p - 1  R o u t i n g T o p - 2  R o u t i n g
H a s h
F u n c t i o nF F N  1 F F N  2 F F N  4 F F N  3A d d  +  N o r m a l i z e
F F N  1 F F N  2 F F N  4 F F N  3
x 1 x2y 1 y 2  
"The" "Dog"H a s h  R o u t i n g
H a s h
F u n c t i o nF F N  1 F F N  2 F F N  4 F F N  3A d d  +  N o r m a l i z e
x 1 x2y 1 y 2  
"The" "Dog"E x p e r t  C h o o s e s  T o k e n s
R o u t e r R o u t e r R o u t e r R o u t e r
F F N  1 F F N  2 F F N  4 F F N  3A d d  +  N o r m a l i z e
x 1 x2y 1 y 2  
"The" "Dog"B AS E  R o u t i n g
S o l v e  Li n e a r
A s s i gn m e n tA d d  +  N o r m a l i z e
x 1 x2y 1 y 2  
"The" "Dog"R e i n f o r c e m e n t  L e a r n i n g
R o u t e rF F N  1 F F N  2 F F N  4 F F N  3 F F N  1 F F N  2 F F N  4 F F N  3
R o u t e rL o s s  + =  - l o g (      )  *  RL o s s  + =  - l o g (      )  *  R

Hình 7: Trực quan hóa sáu thuật toán định tuyến khác nhau. Mỗi sơ đồ là của một mô hình chuyên gia thưa Transformer với bốn chuyên gia (mạng truyền thẳng) định tuyến hai token: "The" và "Dog".

MoE sử dụng định tuyến top-2, nhưng mỗi token được gửi đến một lớp dày đặc chung và một chuyên gia duy nhất mà nó chọn (thay vì hai chuyên gia).

Riquelme et al. (2021) giới thiệu một cải tiến cho định tuyến top-k có tên Batch Prioritized Routing (BPR) cho các mô hình phân loại ảnh ViT. Các mô hình MoE sử dụng kích thước batch cố định trên mỗi chuyên gia, có thể gây ra token tràn nếu không có đủ dung lượng. Nếu một token tràn thì không có tính toán nào sẽ được áp dụng cho token đó tại chuyên gia đã cho đó. Trong định tuyến top-k, ưu tiên của token nào để không loại bỏ tại một chuyên gia tràn được đưa cho các token được gửi sớm hơn trong một câu/batch. BPR thay vào đó ưu tiên các đầu vào có điểm số định tuyến cao hơn. Điều này có liên quan đến các mô hình ViT vì không có tính chất tự hồi quy cho các đầu vào, vì vậy tất cả đầu vào có thể thấy nhau. Trong ngôn ngữ thường có thứ tự trái-sang-phải của các đầu vào, về lý thuyết có thể cho phép mô hình gian lận trong quá trình huấn luyện.
Zoph et al. (2022) thấy định tuyến BPR hữu ích cho các mô hình ngôn ngữ MoE. Kim et al. (2021) đề xuất ngẫu nhiên hóa việc ưu tiên các token trong các chuỗi để đảm bảo định tuyến không thiên vị đối với các token đầu trong câu.

Định tuyến tĩnh. Hầu hết các thuật toán định tuyến học động các quyết định định tuyến trong khi huấn luyện, nhưng điều này có thể được xác định tĩnh trước khi huấn luyện bắt đầu. Các thuật toán định tuyến động thường hoạt động trên các biểu diễn đầu vào nội bộ trong mạng, vì vậy các quyết định định tuyến tính đến token hiện tại và đầu vào trước đó cho mô hình (thường thông qua lớp tự-chú ý). Hầu hết các thuật toán định tuyến là động, nhưng một ví dụ đáng chú ý của thuật toán định tuyến tĩnh là Hash Layers từ Roller et al. (2021). Công trình này cho thấy định tuyến cố định ngẫu nhiên bằng cách hash token đầu vào dẫn đến hiệu suất cạnh tranh với định tuyến học. Cân bằng tải được đạt được bằng cách chọn các hàm hash trước khi huấn luyện cân bằng các batch token. Một mô tả về Hash Layers có thể được tìm thấy trong Hình 7.

Mỗi chuyên gia chọn top-k token. Thay vì mỗi token chọn chuyên gia nào để được gửi đến, Zhou et al. (2022) lật ngược điều này và có mỗi chuyên gia chọn token nào nó muốn được định tuyến đến. Điều này làm giảm bớt nhu cầu cho các loss cân bằng tải phụ trợ được thêm vào trong quá trình huấn luyện hoặc cho các thuật toán gán tuyến tính. Bây giờ mỗi chuyên gia sẽ luôn có cùng số lượng token, mặc dù một số token có thể không được gửi đến chuyên gia nào hoặc một số token có thể được gửi đến tất cả chúng. Thực nghiệm thuật toán này hoạt động tốt và có một giải thích tính toán thích ứng nơi mô hình có thể ngầm định áp dụng nhiều tính toán hơn cho các token nhất định.

Xác định toàn cục token nào nên đi đến mỗi chuyên gia. Lớp BASE (Lewis et al., 2021) xử lý định tuyến token như một bài toán gán tuyến tính. Nó nhằm mục đích định tuyến một số cố định token đến mỗi chuyên gia và tối đa hóa các điểm số từ ma trận định tuyến. Vì các token trên mỗi bộ xử lý có mối tương quan cao vì chúng đến từ cùng các câu, token được xáo trộn ngẫu nhiên xung quanh trước khi giải quyết cục bộ bài toán gán tuyến tính trên mỗi thiết bị. Việc xáo trộn này giới thiệu hai nguyên thủy giao tiếp bổ sung (all2all) trong cả bước truyền thẳng và truyền ngược. Clark et al. (2022) đề xuất biến thể riêng của lớp BASE (S-BASE) sử dụng công thức vận tải tối ưu.

Các thuật toán định tuyến khác. Một số thuật toán định tuyến không phù hợp gọn gàng với ba danh mục trên. Zuo et al. (2021) giới thiệu THOR, một thuật toán ngẫu nhiên chọn hai chuyên gia cho mỗi đầu vào trong quá trình huấn luyện và suy luận và tìm thấy cải thiện 2 điểm BLEU so với các mô hình MoE tiêu chuẩn. Gururangan et al. (2021) đề xuất DEMix, rõ ràng có các chuyên gia khác nhau cho các miền pre-training khác nhau (ví dụ: luật, y tế, v.v.). Các chuyên gia sau đó có thể được chọn bằng cách thực hiện khớp miền trên các đầu vào. Fan et al. (2021) sử dụng các sublayer cụ thể ngôn ngữ rõ ràng nơi token đầu vào có thể được định tuyến một cách xác định dựa trên ngôn ngữ của chúng. Điều này tránh cần các thuật toán định tuyến động. Ma et al. (2018) giới thiệu một thuật toán định tuyến đa cổng nơi mỗi tác vụ có hàm cổng duy nhất riêng của nó.

4.2 CÂN BẰNG TỞI
Hầu hết các thuật toán định tuyến xử lý cân bằng tải bằng cách thêm một loss phụ trợ trong quá trình huấn luyện để khuyến khích lượng token bằng nhau được gửi đến các chuyên gia khác nhau (Shazeer et al., 2017). Một số thuật toán định tuyến xử lý cân bằng tải thông qua thiết kế của chúng: BASE Layers (Lewis et al., 2021) giải quyết một bài toán gán tuyến tính thực thi số lượng token bằng nhau đi đến mỗi chuyên gia như một phần của câu lệnh bài toán. S-BASE từ Clark et al. (2022) theo một giao thức tương tự, nhưng giải quyết bài toán gán sử dụng vận tải tối ưu. Nie et al. (2021) bắt đầu bằng cách huấn luyện một mô hình Mixture-of-Expert nơi tất cả token được gửi đến mỗi chuyên gia, nhưng theo thời gian thích ứng mạng để thực hiện định tuyến top-1. Thuật toán này không cần cân bằng tải vì mạng tự nhiên học để chuyên môn hóa các biểu diễn chuyên gia qua quá trình huấn luyện.

5 CÁC MÔ HÌNH CHUYÊN GIA THƯA TRÊN CÁC MIỀN
Các mô hình chuyên gia thưa và MoE được giới thiệu và phổ biến trong xử lý ngôn ngữ tự nhiên (NLP). Miền này là một sự phù hợp tự nhiên cho các mô hình lớn được hưởng lợi từ tính sẵn có dễ dàng của hàng nghìn tỷ token và các thuật toán tự giám sát mạnh mẽ của dự đoán từ tiếp theo và mô hình hóa ngôn ngữ che. Tuy nhiên, tác động của những mô hình này đang nhanh chóng lan rộng đến các miền khác bao gồm thị giác máy tính, nhận dạng giọng nói và ứng dụng đa phương thức. Sự lan rộng của các kỹ thuật từ NLP đến các miền khác đã được tăng tốc vì Transformer đã được nhanh chóng áp dụng trong các miền và phương thức khác. Một số ví dụ là phân loại ảnh (Dosovitskiy et al., 2020), phát hiện đối tượng (Carion et al., 2020), hệ thống khuyến nghị (Chen et al., 2019), nhận dạng giọng nói (Dong et al., 2018; Nakatani, 2019; Gulati et al., 2020).

10

--- TRANG 9 ---
Trên các miền khác nhau, các kiến trúc và thuật toán thưa vẫn gần như giống nhau, nhưng những gì được định tuyến đến các chuyên gia là khác nhau. Bảng 1 cho thấy các đầu vào lớp thưa khác nhau cho nhiều miền khác nhau.

Miền Biểu diễn Đầu vào
NLP Từ, từ con, hoặc câu
Thị giác Miếng ảnh
Giọng nói Spectrogram
Đa phương thức Từ hoặc miếng ảnh

Bảng 1: Đầu vào cho các mô hình thưa trong các miền khác nhau. Đầu vào được sử dụng để xác định chuyên gia nào để định tuyến đến và là những gì lớp MoE sẽ áp dụng tính toán.

5.1 XỬ LÝ NGÔN NGỮ TỰ NHIÊN
Ban đầu Shazeer et al. (2017) giới thiệu lớp Mixture-of-Expert cho mô hình hóa ngôn ngữ LSTM và dịch máy. Các lớp được chèn giữa các lớp tiêu chuẩn trong mô hình LSTM. Các công trình tiếp theo hiện dựa trên Transformers, và các lớp chuyên gia thường thay thế các lớp dày đặc.

Lepikhin et al. (2020) đầu tiên giới thiệu lớp MoE vào Transformers và nghiên cứu nó trong bối cảnh dịch máy. Họ đạt được kết quả dịch thuật tiên tiến trên 100 ngôn ngữ khác nhau khi mở rộng lên đến 2048 chuyên gia trên mỗi lớp chuyên gia. Fedus et al. (2021) sau đó tạo ra một mô hình ngôn ngữ thưa 1.6T tham số đạt được chất lượng pre-training tiên tiến. Họ cũng nghiên cứu việc sử dụng các lớp thưa để tạo ra các kích hoạt q/k/v trong các lớp Self-Attention, nhưng thấy kỹ thuật này không ổn định hơn. Lee-Thorp and Ainslie (2022) giới thiệu Fast Sparse Mixer, là một mô hình chỉ encoder đạt được tăng tốc huấn luyện 89% và tăng tốc suy luận 98% so với BERT (Devlin et al., 2018).

Gần đây, đã có một cơn bão nghiên cứu MoE về nhiều chủ đề khác nhau trong miền NLP. Ví dụ, các kiến trúc MoE trước đây trong miền NLP hoạt động ở cấp từ hoặc byte-pair. Kudugunta et al. (2021) thay vào đó có một kiến trúc MoE định tuyến ở cấp tác vụ hoặc câu, cho phép suy luận và phục vụ hiệu quả hơn. Điều này được nghiên cứu trong bối cảnh dịch máy nơi các câu sẽ được định tuyến dựa trên ngôn ngữ mà chúng đang dịch sang.

Các kết quả mới đã có thể đẩy tiên tiến trên các tiêu chuẩn suy luận few-shot và fine-tuning. Du et al. (2021) huấn luyện một mô hình ngôn ngữ decoder-only MoE và đạt được kết quả few-shot tiên tiến, trong khi chỉ yêu cầu 1/3 tính toán cần thiết để huấn luyện GPT-3. Zoph et al. (2022) giới thiệu ST-MoE, một mô hình encoder-decoder thưa đạt được tiên tiến trên một tập lớn các tác vụ lý luận và tạo sinh bao gồm SuperGLUE, ARC Easy/Challenge, XSum, CNN-GM, Web-QA, ANLI, và Winogrande. ST-MoE vượt trội so với PaLM-540B (Chowdhery et al., 2022) khi fine-tuning trên SuperGLUE, trong khi sử dụng khoảng 20× ít FLOPs pre-training hơn và 40× ít FLOPs suy luận hơn.

5.2 THỊ GIÁC MÁY TÍNH
Do tính phổ quát của Transformers (ví dụ: ViT (Dosovitskiy et al., 2020)), việc áp dụng cải tiến cho kiến trúc MoE trên các miền đã có kết quả. Riquelme et al. (2021) tạo ra một mô hình MoE thị giác bằng cách thêm các lớp MoE vào kiến trúc ViT. Mô hình của họ, V-MoE, được áp dụng cho phân loại ảnh và có thể chỉ sử dụng một nửa lượng tính toán suy luận trong khi khớp hiệu suất của các kiến trúc tiên tiến trước đây. Lou et al. (2021) giới thiệu một mô hình MLP MoE thưa cho phân loại ảnh dựa trên kiến trúc MLP-Mixer (Tolstikhin et al., 2021). Biến thể MoE của họ đạt được hiệu suất phân loại ảnh tốt hơn trên ImageNet và CIFAR so với đối tác dày đặc.

Wu et al. (2022) cải thiện hiệu quả huấn luyện của các mô hình MoE thông qua lớp Residual Mixture-of-Expert được đề xuất. Kiến trúc này đạt được giảm 30% chi phí huấn luyện và chất lượng tương đương với các mô hình MoE tiêu chuẩn trên cả phân đoạn và phát hiện đối tượng. Hwang et al. (2022) triển khai một khung hiệu quả và chiến lược song song thích ứng cho các lớp MoE. Để đánh giá hệ thống của họ, họ thêm các lớp MoE vào kiến trúc Swin Transformer V2 (Liu et al., 2022) cho phân loại ảnh và phát hiện đối tượng. Biến thể MoE của họ đạt được tăng tốc 1.5x-2x trong cả huấn luyện và suy luận so với triển khai MoE trước đây. Aljundi et al. (2017) sử dụng các mô hình chuyên gia trong thiết lập học liên tục nơi họ thêm các chuyên gia mới theo thời gian và chứng minh cải thiện trên phân loại ảnh và dự đoán video. Caccia et al. (2021) tăng động số lượng chuyên gia trong quá trình huấn luyện cho các mô hình phân loại ảnh trên CIFAR và MNIST.

11

--- TRANG 10 ---
Ramachandran and Le (2018) nghiên cứu cách độ sâu và đa dạng kiến trúc tác động đến hiệu suất mô hình chuyên gia thưa và đạt được lợi ích trên nhận dạng ảnh. Kirsch et al. (2018) phát triển một thuật toán end-to-end để thực hiện tính toán có điều kiện dựa trên dữ liệu đầu vào đạt được hiệu suất phân loại ảnh mạnh mẽ.

5.3 NHẬN DẠNG GIỌNG NÓI
SpeechMoE (You et al., 2021) sử dụng các mô hình Transformer MoE cho nhận dạng giọng nói và đạt được cải thiện tỷ lệ lỗi ký tự mạnh mẽ trên bốn tập dữ liệu. Họ sử dụng các loss phụ trợ mới để thúc đẩy sự thưa thớt và giới thiệu một kiến trúc định tuyến mới. SpeechMoE2 (You et al., 2022) tiếp tục cải thiện kết quả của SpeechMoE bằng cách tạo ra một thuật toán định tuyến mới thêm vào thông tin phụ trợ mới để đưa ra quyết định định tuyến. Kumatani et al. (2021) mang lại cải thiện cho nhận dạng giọng nói đa ngôn ngữ bằng cách sử dụng các lớp MoE trong hai loại kiến trúc Transformer giọng nói khác nhau: sequence-to-sequence và transducers.

5.4 ĐA PHƯƠNG THỨC VÀ ĐA TÁC VỤ
Mustafa et al. (2022) thực hiện học đa phương thức bằng cách huấn luyện một mô hình MoE (LIMoE) nhận đầu vào cả ảnh và văn bản và học sử dụng một loss tương phản tương tự như CLIP (Radford et al., 2021). Lớp MoE có thể định tuyến cả miếng ảnh và token từ đến các chuyên gia có sẵn. Mô hình vượt trội so với CLIP khi sử dụng một chiến lược huấn luyện tương đương, và khi được mở rộng hơn nữa khớp với các phương pháp tiên tiến.

6 KHI NÀO SỬ DỤNG MÔ HÌNH THƯA SO VỚI MÔ HÌNH DÀY ĐẶC
Một câu hỏi phổ biến là nếu bạn được cho một ngân sách tính toán hoặc FLOP cố định (ví dụ: 100 GPU trong 20 giờ), loại mô hình nào bạn nên huấn luyện để đạt được hiệu suất tốt nhất? Nhiều công trình trước đây cho thấy rằng sự thưa thớt tốt hơn một mô hình dày đặc cho loại thiết lập này (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021; Du et al., 2021; Artetxe et al., 2021; Lewis et al., 2021). Với tất cả các kết quả tiên tiến mạnh mẽ sử dụng các mô hình thưa, tại sao bạn không bao giờ không sử dụng một mô hình thưa so với một mô hình dày đặc?

Các mô hình thưa là một tổng quát hóa của một mô hình dày đặc; một mô hình thưa với một chuyên gia duy nhất là gần như một mô hình dày đặc. Về cơ bản, các mô hình thưa cho phép tăng rộng số lượng tham số trong một mô hình bằng cách tăng số lượng chuyên gia, trong khi giữ FLOPs trên mỗi ví dụ gần như không đổi. Điều này có thể tốt hoặc xấu tùy thuộc vào thiết lập và cách mô hình sẽ được sử dụng sau này.

Ở mức độ cao, sự thưa thớt là tốt khi bạn có nhiều bộ tăng tốc (ví dụ: GPU/TPU) để chứa tất cả các tham số bổ sung đi kèm khi sử dụng sự thưa thớt. Thường các mô hình được huấn luyện sử dụng song song dữ liệu nơi các máy khác nhau sẽ nhận các lát cắt khác nhau của dữ liệu huấn luyện/suy luận. Các máy được sử dụng để hoạt động trên các lát cắt khác nhau của dữ liệu bây giờ có thể được sử dụng để chứa nhiều tham số mô hình hơn. Do đó, các mô hình thưa tốt khi huấn luyện với song song dữ liệu và/hoặc có thông lượng cao khi phục vụ: huấn luyện/phục vụ trên nhiều máy có thể chứa tất cả các tham số.

Sử dụng sự thưa thớt cũng yêu cầu xem xét cẩn thận về cách mô hình sẽ được sử dụng trong việc sử dụng downstream. Nếu có nhiều máy để pre-train một mô hình, nhưng ít hơn nhiều để fine-tuning hoặc phục vụ, thì lượng sự thưa thớt (ví dụ: số lượng chuyên gia) nên được điều chỉnh để phù hợp với lượng bộ nhớ có sẵn trong các trường hợp sử dụng downstream. Đây thường là một xem xét thiết kế thực tế được sử dụng trong tài liệu.

Trên cơ sở mỗi tham số, các mô hình thưa sẽ luôn trông tệ hơn so với các mô hình dày đặc. Giả sử rằng tất cả tham số được giữ trong bộ nhớ bộ tăng tốc, đây là một yêu cầu tương tự như tìm kiếm mô hình tốt nhất có thể vừa với một kích thước phần cứng nhất định (ví dụ: 4 GPU), nơi một lần nữa một mô hình thưa sẽ là một lựa chọn tồi hơn một mô hình dày đặc. Như đã đề cập ở trên, các mô hình thưa là một sự phù hợp tuyệt vời khi bạn có khả năng đang huấn luyện hoặc phục vụ trên nhiều máy song song để chứa các tham số mô hình bổ sung từ các chuyên gia.

Tuy nhiên, tất cả hy vọng không bị mất cho các mô hình thưa trong các thiết lập hạn chế bộ nhớ. Fedus et al. (2021) cho thấy rằng các mô hình thưa hoạt động tốt với chỉ hai chuyên gia, yêu cầu bộ nhớ bổ sung hạn chế. Nghiên cứu mới cũng cho phép vượt qua thiếu hụt bộ nhớ GPU/TPU bằng cách trao đổi động bộ nhớ mô hình giữa CPU và GPU (xem Mục 2.2 để biết thêm chi tiết). Các cách tiếp cận khác để giảm dấu chân bộ nhớ của các mô hình thưa được thảo luận trong Mục 7.3.

7 CẢI THIỆN HUẤN LUYỆN MÔ HÌNH THƯA
Các mô hình thưa thường có động học khác nhau so với các mô hình dày đặc và được hưởng lợi từ các phương pháp huấn luyện và fine-tuning khác nhau.

7.1 KHÔNG ỔN ĐỊNH
Các mô hình thưa thường được báo cáo là không ổn định hơn, có nghĩa là loss phân kỳ và tăng (Lepikhin et al., 2020; Fedus et al., 2021; Zoph et al., 2022; Mustafa et al., 2022). Sự không ổn định cũng xuất hiện thường xuyên hơn ở quy mô mô hình lớn hơn. Lepikhin et al. (2020) gặp phải sự không ổn định huấn luyện sử dụng kích hoạt bfloat16 với một mô hình 1 nghìn tỷ tham số. Fedus et al. (2021) gặp phải sự không ổn định trong mô hình Switch-XXL tính toán cao nhất của họ. Zoph et al. (2022) gặp phải sự không ổn định trong các mô hình lớn nhất của họ, đặc biệt là trong thiết lập đa ngôn ngữ. Mustafa et al. (2022) quan sát tăng sự không ổn định khi thực hiện huấn luyện đa phương thức trên cả ảnh và văn bản.

Nhiều nghiên cứu đã được thực hiện để cải thiện động học huấn luyện của các mô hình thưa. Lepikhin et al. (2020) lưu ý rằng sự không ổn định mô hình lớn nhất có thể được sửa bằng cách huấn luyện các mô hình sử dụng độ chính xác cao hơn (float32), nhưng đi kèm với chi phí sử dụng bộ nhớ nhiều hơn và huấn luyện chậm hơn. Fedus et al. (2021) khuyến nghị sử dụng tỷ lệ khởi tạo trọng số thấp hơn và chuyển đổi chỉ một tập con cụ thể của mạng định tuyến sang độ chính xác cao hơn để có sự ổn định/huấn luyện mô hình tốt hơn. Du et al. (2021) bỏ qua các batch dữ liệu có bất kỳ NaN hoặc Inf nào trong gradient và cũng khởi động lại mô hình từ một checkpoint trước đó khi có bất kỳ sự phân kỳ huấn luyện nào xảy ra. Artetxe et al. (2021) đề xuất một khởi tạo thông minh hơn của lớp chuyên gia để tính đến kích thước batch giảm của trọng số chuyên gia. Vì mỗi chuyên gia sẽ có kích thước batch B/E3 các tác giả đề xuất chia tỷ lệ gradient của lớp chuyên gia bằng 1/√E. Zoph et al. (2022) giới thiệu router z-loss để cải thiện cả sự không ổn định mô hình và chất lượng. Loss phụ trợ này nhằm mục đích giảm lỗi làm tròn floating point bằng cách khuyến khích các logit đi vào hàm router vẫn nhỏ trong quá trình huấn luyện. Mustafa et al. (2022) nghiên cứu rộng rãi nhiều kỹ thuật để sửa sự không ổn định mô hình, và sử dụng kết hợp các cách tiếp cận khác nhau bao gồm router z-loss và hai loss entropy mới.

7.2 CHUYỂN GIAO ĐẾN PHÂN PHỐI MỚI
Một số bài báo nghiên cứu, đặc biệt là ở quy mô lớn hơn, lưu ý rằng các mô hình MoE được chuyển giao đến các miền mới (như trong fine-tuning) tụt hậu so với các đối tác dày đặc. Fedus et al. (2021); Narang et al. (2021) so sánh perplexity pre-training với hiệu suất fine-tuning cho các mô hình dày đặc và thưa. Họ nhận thấy đối với một perplexity pre-training nhất định, các mô hình thưa fine-tuning tệ hơn trên các tác vụ lý luận, nhưng tốt hơn trên các tác vụ nặng về kiến thức. Ngoài hiệu suất mô hình hóa ngôn ngữ out-of-domain tệ hơn, Artetxe et al. (2021) quan sát fine-tuning tệ hơn so với các mô hình dày đặc trên nhiều tác vụ bao gồm HellaSwag, PIQA và Winogrande.

Một số cách khác nhau đã được đề xuất để giúp giải quyết các vấn đề fine-tuning. Một là mở rộng các mô hình bằng cách có nhiều FLOP hơn so với nhiều sự thưa thớt hơn (ví dụ: ít chuyên gia hơn, nhưng làm chúng lớn hơn). Fedus et al. (2021) huấn luyện một mô hình 1.6T tham số với 2048 chuyên gia, nhưng nó chỉ có nhiều FLOP như một mô hình dày đặc 2B. Ngược lại, mô hình với hiệu suất fine-tuning tốt nhất chỉ có 128 chuyên gia, nhưng lượng FLOP như một mô hình dày đặc 11B. Đánh đổi ít sự thưa thớt hơn cho nhiều FLOP hơn khi mở rộng một mô hình là một cách đơn giản để đảm bảo hiệu suất fine-tuning tốt hơn. Zoph et al. (2022) nhận thấy rằng các siêu tham số fine-tuning tối ưu (ví dụ: tỷ lệ học và kích thước batch) có thể khác nhau đáng kể cho các mô hình dày đặc và thưa. Sử dụng các siêu tham số tốt nhất cho các mô hình dày đặc trên các mô hình thưa có thể che giấu bất kỳ cải thiện pre-training sự thưa thớt nào – do đó, một nghiên cứu siêu tham số độc lập là có lợi.

7.3 SUY LUẬN
Theo thiết kế, các mô hình chuyên gia thưa có nhiều tham số hơn các đối tác dày đặc. Trong khi tính toán được thực hiện vẫn tương đối thấp, dấu chân bộ nhớ có thể là một gánh nặng. Do đó, một số nghiên cứu đã tập trung vào việc giảm số lượng tham số cần thiết tại thời điểm suy luận để giảm bớt các yêu cầu phục vụ. Kudugunta et al. (2021) định tuyến ở cấp tác vụ thay vì cấp từ hoặc token cho dịch máy. Điều này cho phép suy luận hiệu quả hơn vì tập con trọng số chỉ cho các tác vụ cần thiết là bắt buộc. Kim et al. (2021) cắt tỉa các chuyên gia tại suy luận để giảm dấu chân bộ nhớ của mô hình. Hai phương pháp khác nhau được sử dụng để cắt tỉa: ngẫu nhiên chọn một tập con của các chuyên gia và chọn các chuyên gia với sử dụng cao nhất tại thời điểm suy luận. Fedus et al. (2021) chưng cất các mô hình thưa lớn thành các mô hình dày đặc nhỏ hơn cho mô hình hóa ngôn ngữ và fine-tuning. Rajbhandari et al. (2022) nghiên cứu chưng cất của các mô hình thưa cho mô hình hóa ngôn ngữ bằng cách giảm độ sâu trong các lớp chuyên gia của mạng. Rajbhandari et al. (2022) triển khai một phiên bản tối ưu hóa của MoE vào khung DeepSpeed dẫn đến độ trễ suy luận nhanh hơn 7.3× so với các khung hiện có.

8 KHẢ NĂNG GIẢI THÍCH
Các mô hình chuyên gia thưa tự nhiên cho phép nghiên cứu khả năng giải thích vì mỗi đầu vào được xử lý bởi một tập con có thể nhận dạng, rời rạc của các trọng số mô hình (tức là các chuyên gia được chọn). Do đó, thay vì nhiệm vụ khó khăn của việc giải thích có thể hàng nghìn tỷ số floating point, người ta có thể thay vào đó đọc ra một tập nhỏ rời rạc các số nguyên tương ứng với chuyên gia nào mà đầu vào được gửi đến.

Shazeer et al. (2017) thực hiện các nghiên cứu sơ bộ về chuyên môn hóa chuyên gia cho encoder của lớp MoE 2048 của họ trên tác vụ dịch máy WMT '14 EnFr. Họ xác định ba chuyên gia, một với chuyên môn hóa từ xung quanh đổi mới, thứ hai xử lý mạo từ "a", và thứ ba được định tuyến các từ đồng nghĩa của tốc độ.

Sau đó, các phân tích rộng rãi hơn được thực hiện bởi Lewis et al. (2021) trên các kiến trúc dựa trên Transformer. Lewis et al. (2021) thực hiện một nghiên cứu nơi họ theo dõi token đầu vào trước thường xuyên nhất khi chuyên gia được chọn. Điều này tiết lộ chuyên môn hóa trong số lượng, con số, sở hữu, các đoạn từ con, và các cụm động từ, danh từ và tính từ liên quan, với các kết quả được chọn trình bày trong Bảng 2.

Chuyên gia Top-5 token đứng trước
5 year, years, billion, millions, tonnes
9 electronic, local, public, national, outdoor
34 to, will, should it, may
42 two, 50, 1, 80, 000
62 work, started, involved, working, launched
72 is, was, be, been, were
74 going, go, come, back, return
101 B, T, W, H, k

Bảng 2: Chuyên môn hóa chuyên gia dựa trên bối cảnh đứng trước trong BASE Layers. Chúng tôi tái tạo một phần bảng của Lewis et al. (2021), trình bày top-năm token đứng trước thường xuyên nhất cho các chuyên gia được chọn. Ví dụ này cho thấy các chuyên gia chuyên môn hóa trong dấu câu, liên từ & mạo từ, động từ, mô tả thị giác, tên riêng, đếm & con số.

Zoph et al. (2022) huấn luyện một Transformer encoder-decoder và tìm thấy các mẫu tương tự trong encoder, bao gồm các chuyên gia chuyên môn hóa theo cách nông, chẳng hạn như trên mạo từ (ví dụ: "a", "the"). Bảng 3 tái tạo một phần các chuyên môn hóa quan sát được của Zoph et al. (2022). Những nghiên cứu đó tiếp tục
14

--- TRANG 11 ---
tìm thấy chuyên môn hóa chuyên gia trong dấu câu, con số, tên riêng, động từ, màu sắc và token mặt nạ đặc biệt được sử dụng cho mục tiêu pre-training.

Chuyên môn hóa chuyên gia Vị trí chuyên gia Token được định tuyến
Sentinel tokens Lớp 1 been<extra id4><extra id7>floral to
<extra id10><extra id12><extra id15>
<extra id17><extra id18><extra id19>...
Dấu câu Lớp 2 , , , , , , , , , - , , , , , ). )
Lớp 6 , , , , , , : . : , & , & & ? & - , , ? , , , . <extra id27>
Liên từ và mạo từ Lớp 3 The the the the the the the the the The the the
Lớp 6 a and and and and and and and or and a and .
Động từ Lớp 1 died falling identified fell closed left posted lost felt
left said read miss place struggling falling signed died
Mô tả thị giác Lớp 0 her over her know dark upper dark outer
color, spatial position center upper blue inner yellow raw mama
bright bright over open your dark blue
Tên riêng Lớp 1 A Mart Gr Mart Kent Med Cor Tri Ca Mart
R Mart Lorraine Colin Ken Sam Ken Gr Angel A
Đếm và con số Lớp 1 after 37 19. 6. 27 I I Seven 25 4, 54 I two dead we
written and numerical forms Some 2012 who we few lower each

Bảng 3: Chuyên môn hóa chuyên gia encoder trong ST-MoE. Chúng tôi tái tạo một bảng của Zoph et al. (2022) chứng minh chuyên môn hóa chuyên gia trong dấu câu, liên từ & mạo từ, động từ, mô tả thị giác, tên riêng, đếm & con số.

Nhưng một phân tích sâu hơn của kiến trúc ST-MoE encoder-decoder đầy đủ tìm thấy bằng chứng rõ ràng hơn về chuyên môn hóa trong encoder, thay vì decoder. Điều này đáng để nghiên cứu thêm về giá trị và vị trí của các lớp chuyên gia. Thiếu chuyên môn hóa rõ ràng có thể báo hiệu một mẫu khó nhận biết hoặc không có mẫu hữu ích.

Khả năng giải thích của các mô hình chuyên gia thưa không chỉ giới hạn ở văn bản. Một ví dụ là LIMoE (Mustafa et al., 2022), một mô hình đa phương thức được quan sát học các chuyên gia chuyên môn hóa trong dữ liệu văn bản và thị giác, bao gồm các miếng kết cấu, thực vật, mắt, và từ (Hình 8). Như trong các mô hình dựa trên văn bản, độ phức tạp của chuyên môn hóa thay đổi đáng kể. Ví dụ, các chuyên gia dựa trên văn bản được tìm thấy kéo dài các mục tiêu đơn giản như xử lý mạo từ "a" lên đến các khái niệm phức tạp hơn như động từ quá khứ. Tương tự, trong các mô hình đa phương thức, sự tinh vi của chuyên môn hóa chuyên gia thay đổi từ các khái niệm đơn giản như kết cấu cơ bản lên đến các đối tượng cấp cao như bánh xe hoặc tay nắm cửa.

Cuối cùng, chúng tôi làm nổi bật một hạn chế đáng kể của những cách tiếp cận khả năng giải thích này. Chúng xem xét các token hoặc miếng đến từng chuyên gia theo cách hẹp. Cụ thể, embedding ban đầu của một từ hoặc của một miếng kết hợp thông tin bối cảnh từ dữ liệu xung quanh (Transformers làm điều này thông qua tự-chú ý hoặc chú ý encoder-decoder). Do đó, chuyên môn hóa tinh tế hơn có thể bị bỏ lỡ bởi những kỹ thuật heuristic này. Công việc khả năng giải thích cẩn thận và kỹ lưỡng hơn sẽ cần thiết để hiểu rõ hơn các mô hình chuyên gia thưa. Việc phát hành các checkpoint mô hình chuyên gia thưa bởi Artetxe et al. (2021) và Fedus et al. (2021) cho phép các nhóm rộng lớn hơn phân tích và giải thích những động học này.

9 HƯỚNG TƯƠNG LAI VÀ KẾT LUẬN
Mặc dù các mô hình chuyên gia thưa và Mixture-of-Experts có từ ít nhất đầu những năm chín mười – nhiều câu hỏi vẫn còn. Chúng tôi kết thúc khảo sát của mình với một phỏng đoán về các lĩnh vực công việc tương lai đầy hứa hẹn, cụ thể là làm nổi bật giao điểm với hai phát triển gần đây (tính toán thích ứng và phương pháp truy xuất) và suy nghĩ chia tay của chúng tôi.

15

--- TRANG 12 ---
Hình 8: Chuyên môn hóa chuyên gia thị giác trong LIMoE. Chúng tôi tái tạo một hình từ Mustafa et al. (2022) tìm thấy chuyên gia chuyên môn hóa trong các miếng kết cấu (đặc và sọc), đối tượng tự nhiên (thực vật, tay, mắt), và đối tượng nhân tạo (bánh xe, tay nắm cửa, từ).

Tính toán Thích ứng. Tính toán thích ứng là ý tưởng rằng các đầu vào khác nhau cho một hệ thống học máy có thể sử dụng lượng tính toán khác nhau (tức là lượng hoặc loại tính toán được thích ứng một cách linh hoạt). Các mô hình thưa xây dựng trên ý tưởng phản chiếu: mỗi đầu vào sử dụng cùng lượng tính toán, nhưng có khả năng với các tham số khác nhau. Tuy nhiên, những kỹ thuật này không loại trừ lẫn nhau; một số thuật toán định tuyến (Mục 4) cho phép tính toán thích ứng bằng cách gửi một token đến một số biến thiên của chuyên gia (Riquelme et al., 2021; Zhou et al., 2022). Các mô hình tương lai vẫn có thể được hưởng lợi bằng cách kết hợp các kỹ thuật tính toán thích ứng khác – ví dụ, ngoài việc chọn chuyên gia nào, một mạng có thể chọn số lượng lớp để sử dụng (Schuster et al., 2022).

Các lớp chuyên gia không đồng nhất cũng là một sự phù hợp tự nhiên cho tính toán thích ứng. Hầu hết các mô hình thưa sử dụng các chuyên gia cùng loại và kích thước để đơn giản và hiệu quả trên phần cứng hiện đại. Nhưng bằng cách cho phép các chuyên gia khác nhau về kích thước (ví dụ: về độ sâu hoặc chiều rộng), quyết định định tuyến sau đó sẽ dẫn đến lượng tính toán khác nhau. Các hệ thống phần mềm mới, như Pathways (Dean, 2021), sẽ giúp tạo điều kiện cho việc triển khai hiệu quả của những kiến trúc và thuật toán không đồng nhất này trên phần cứng hiện đại.

Phương pháp Truy xuất. Cơ chế truy xuất hiệu quả mở rộng khả năng của các mô hình bằng cách cho phép chúng truy cập động thông tin ngoài bối cảnh hiện tại hoặc những gì được lưu trữ trong các tham số (Khandelwal et al., 2019; Guu et al., 2020; Borgeaud et al., 2022). Các mô hình chuyên gia thưa và mô hình truy xuất có một mục tiêu chồng chéo: tăng khả năng của mô hình để lưu trữ, truy xuất, và áp dụng kiến thức tốt hơn. Các mô hình chuyên gia thưa làm điều này theo cách tham số (tức là các chuyên gia chứa nhiều tham số có thể học hơn), trong khi các hệ thống dựa trên truy xuất nhúng thông tin có thể được truy xuất động một cách không tham số (tức là tìm kiếm láng giềng gần nhất trên một corpus). Nghiên cứu các đánh đổi và kết hợp cả hai cách tiếp cận có khả năng chứng minh là một hướng tương lai hữu ích.

Kết luận. Sự thưa thớt giảm chi phí huấn luyện và suy luận, dẫn đến các mô hình khổng lồ với độ chính xác tốt hơn các đối tác dày đặc. Nhưng nhiều câu hỏi mở vẫn còn. Ví dụ, chúng ta vẫn hiểu kém về cách số lượng tối ưu và kích thước của các chuyên gia phụ thuộc vào tác vụ (ví dụ: nên sử dụng một vài chuyên gia lớn hay nhiều chuyên gia nhỏ cho dịch thuật?). Như nhiều công trình đã chỉ ra, đạt được tổng quát hóa out-of-domain mạnh mẽ ít đơn giản hơn và cần có lời giải thích tốt hơn. Hơn nữa, hầu hết các mô hình chuyên gia thưa có đa dạng kiến trúc tương đối thấp nơi các lớp thưa được xen kẽ theo các khoảng thời gian đều. Các mô hình tương lai có thể được hưởng lợi từ cấu trúc ít tiêu chuẩn hóa hơn và các kiến trúc chuyên gia không đồng nhất. Ngoài ra, độ chi tiết thích hợp của sự thưa thớt vẫn phải được xác định: hầu hết các công trình đã tập trung vào các chuyên gia thay thế các thành phần, như các lớp mạng truyền thẳng, nhưng lợi ích của các chuyên gia mô-đun hơn, độc lập được phát hiện (Gururangan et al., 2021; Li et al., 2022). Lĩnh vực này vẫn đang khám phá các thuộc tính của các mô hình chuyên gia thưa, bao gồm hiệu chuẩn được cải thiện nhiều (Srivastava et al., 2022); những thuộc tính khác vẫn chưa được biết bao gồm động học của chúng dưới huấn luyện không đồng bộ (Recht et al., 2011) hoặc khả năng ghi nhớ của chúng (Carlini et al., 2020). Tóm lại, những mô hình này đặt ra vô số các vấn đề toán học, kỹ thuật, và nghiên cứu thách thức, nhưng các giải pháp cho đến nay đã mang lại lợi ích đáng kể và chúng tôi tin rằng nhiều cải tiến hơn nữa đang chờ đợi phía trước.

LỜI CẢM ÓN
Chúng tôi muốn cảm ơn các tác giả cốt lõi BIG-Bench, Mike Lewis, Aidan Clark, Diego de Las Casas, Nan Du, và Carlos Riquelme vì đã cho phép tái tạo các hình và bảng tại đây. Chúng tôi cũng muốn cảm ơn Daniel S. Park, Nan Du, Jason Wei, James Lee-Thorp, và Yanqi Zhou vì phản hồi và nhận xét về các bản nháp của chúng tôi.

17

--- TRANG 13 ---
TÀI LIỆU THAM KHẢO
Rahaf Aljundi, Punarjay Chakravarty, và Tinne Tuytelaars. Expert gate: Lifelong learning with
a network of experts. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 3366–3375, 2017.

Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria
Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giri Anantharaman, Xian Li, Shuohui
Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo,
Jeff Wang, Luke Zettlemoyer, Mona Diab, Zornitsa Kozareva, và Ves Stoyanov. Efficient large
scale language modeling with mixtures of experts, 2021.

Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, và Doina Precup. Conditional computation
in neural networks for faster models, 2016.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.
Improving language models by retrieving from trillions of tokens. In International Conference on
Machine Learning, pages 2206–2240. PMLR, 2022.

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Lucas Caccia, Jing Xu, Myle Ott, Marc'Aurelio Ranzato, và Ludovic Denoyer. On anytime learning at macroscale. arXiv preprint arXiv:2106.09563, 2021.

Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, và
Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on
computer vision, pages 213–229. Springer, 2020.

Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine
Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, và Colin Raffel.
Extracting training data from large language models, 2020.

Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, và Tony
Robinson. One billion word benchmark for measuring progress in statistical language modeling.
arXiv preprint arXiv:1312.3005, 2013.

Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, và Wenwu Ou. Behavior sequence transformer for
e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop on
Deep Learning Practice for High-Dimensional Sparse Data, pages 1–4, 2019.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann,
Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling laws for
routed language models. arXiv preprint arXiv:2202.01169, 2022.

Jeff Dean. Introducing pathways: A next-generation ai architecture. Google AI Blog, 2021.

Tim Dettmers và Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing
performance. arXiv preprint arXiv:1907.04840, 2019.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Linhao Dong, Shuang Xu, và Bo Xu. Speech-transformer: a no-recurrence sequence-to-sequence
model for speech recognition. In 2018 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 5884–5888. IEEE, 2018.

18

--- TRANG 14 ---
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An
image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020.

Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim
Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten Bosma,
Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathy
Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen,
và Claire Cui. Glam: Efficient scaling of language models with mixture-of-experts, 2021.

Dheeru Dua, Shruti Bhosale, Vedanuj Goswami, James Cross, Mike Lewis, và Angela Fan. Tricks
for training sparse translation models. arXiv preprint arXiv:2110.08246, 2021.

David Eigen, Marc'Aurelio Ranzato, và Ilya Sutskever. Learning factored representations in a deep
mixture of experts. arXiv preprint arXiv:1312.4314, 2013.

Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, và Erich Elsen. Rigging the lottery:
Making all tickets winners. In International Conference on Machine Learning, pages 2943–2952.
PMLR, 2020.

Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond english-centric
multilingual machine translation. Journal of Machine Learning Research, 22(107):1–48, 2021.

William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.

Trevor Gale, Erich Elsen, và Sara Hooker. The state of sparsity in deep neural networks. arXiv
preprint arXiv:1902.09574, 2019.

Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo
Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer
for speech recognition. arXiv preprint arXiv:2005.08100, 2020.

Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A. Smith, và Luke Zettlemoyer. Demix
layers: Disentangling domains for modular language modeling, 2021.

Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, và Mingwei Chang. Retrieval augmented
language model pre-training. In International Conference on Machine Learning, pages 3929–
3938. PMLR, 2020.

Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Greg
Ganger, và Phil Gibbons. Pipedream: Fast and efficient pipeline parallel dnn training. arXiv
preprint arXiv:1806.03377, 2018.

Hussein Hazimeh, Zhe Zhao, Aakanksha Chowdhery, Maheswaran Sathiamoorthy, Yihua Chen,
Rahul Mazumder, Lichan Hong, và Ed H. Chi. Dselect-k: Differentiable selection in the mixture
of experts with applications to multi-task learning, 2021.

Sepp Hochreiter và Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997.

Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.

Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong
Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural
networks using pipeline parallelism. In Advances in neural information processing systems, pages
103–112, 2019.

19

--- TRANG 15 ---
Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas,
Jithin Jose, Prabhat Ram, et al. Tutel: Adaptive mixture-of-experts at scale. arXiv preprint
arXiv:2206.03382, 2022.

Robert A Jacobs, Michael I Jordan, Steven J Nowlan, và Geoffrey E Hinton. Adaptive mixtures of
local experts. Neural computation, 3(1):79–87, 1991.

Sebastian Jaszczur, Aakanksha Chowdhery, Afroz Mohiuddin, Lukasz Kaiser, Wojciech Gajewski,
Henryk Michalewski, và Jonni Kanerva. Sparse is enough in scaling transformers. Advances in
Neural Information Processing Systems, 34:9895–9907, 2021.

Michael I Jordan và Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.
Neural computation, 6(2):181–214, 1994.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020.

Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, và Mike Lewis. Generalization
through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172,
2019.

Young Jin Kim, Ammar Ahmad Awan, Alexandre Muzio, Andres Felipe Cruz Salinas, Liyang Lu,
Amr Hendy, Samyam Rajbhandari, Yuxiong He, và Hany Hassan Awadalla. Scalable and efficient moe training for multitask multilingual models, 2021.

Louis Kirsch, Julius Kunze, và David Barber. Modular networks: Learning to decompose neural
computation. Advances in neural information processing systems, 31, 2018.

Ferdinand Kossmann, Zhihao Jia, và Alex Aiken. Optimizing mixture of experts using dynamic
recompilations. arXiv preprint arXiv:2205.01848, 2022.

Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang
Luong, và Orhan Firat. Beyond distillation: Task-level mixture-of-experts for efficient inference.
arXiv preprint arXiv:2110.03742, 2021.

Kenichi Kumatani, Robert Gmyr, Felipe Cruz Salinas, Linquan Liu, Wei Zuo, Devang Patel, Eric
Sun, và Yu Shi. Building a great multi-lingual teacher with sparsely-gated mixture of experts for
speech recognition. arXiv preprint arXiv:2112.05820, 2021.

Yann LeCun, Léon Bottou, Yoshua Bengio, và Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

James Lee-Thorp và Joshua Ainslie. Sparse mixers: Combining moe and mixing to build a more
efficient bert. arXiv preprint arXiv:2205.12399, 2022.

Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, và Zhifeng Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.

Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, và Luke Zettlemoyer. Base layers:
Simplifying training of large, sparse models. arXiv preprint arXiv:2103.16716, 2021.

Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith, và Luke
Zettlemoyer. Branch-train-merge: Embarrassingly parallel training of expert language models,
2022. URL https://arxiv.org/abs/2208.03306.

Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng
Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12009–12019,
2022.

Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, và Yang You. Sparse-mlp: A fully-mlp architecture
with conditional computation. arXiv preprint arXiv:2109.02008, 2021.

20

--- TRANG 16 ---
Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, và Ed H Chi. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 1930–1939,
2018.

Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, và Neil Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. arXiv preprint
arXiv:2206.02770, 2022.

Mahdi Pakdaman Naeini, Gregory Cooper, và Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence,
2015.

Tomohiro Nakatani. Improving transformer-based end-to-end speech recognition with connectionist
temporal classification and language model integration. In Proc. Interspeech, 2019.

Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021.

Xiaonan Nie, Shijie Cao, Xupeng Miao, Lingxiao Ma, Jilong Xue, Youshan Miao, Zichao
Yang, Zhi Yang, và Bin Cui. Dense-to-sparse gate for mixture-of-experts. arXiv preprint
arXiv:2112.14397, 2021.

David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,
David So, Maud Texier, và Jeff Dean. Carbon emissions and large neural network training. arXiv
preprint arXiv:2104.10350, 2021.

Joan Puigcerver, Carlos Riquelme, Basil Mustafa, Cedric Renggli, André Susano Pinto, Sylvain
Gelly, Daniel Keysers, và Neil Houlsby. Scalable transfer learning with expert models. arXiv
preprint arXiv:2009.13239, 2020.

Alec Radford, Karthik Narasimhan, Tim Salimans, và Ilya Sutskever. Improving language understanding by generative pre-training, 2018.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International Conference on Machine Learning,
pages 8748–8763. PMLR, 2021.

Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,
Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,
Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron
Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen
Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,
Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,
Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol
Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,
và Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2021.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, và Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, và Yuxiong He. Zero: Memory optimization
towards training a trillion parameter models. arXiv preprint arXiv:1910.02054, 2019.

21

--- TRANG 17 ---
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, và Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts
inference and training to power next-generation ai scale. arXiv preprint arXiv:2201.05596, 2022.

Prajit Ramachandran và Quoc V Le. Diversity and depth in per-example routing models. In
International Conference on Learning Representations, 2018.

Benjamin Recht, Christopher Re, Stephen Wright, và Feng Niu. Hogwild!: A lock-free approach
to parallelizing stochastic gradient descent. Advances in neural information processing systems,
24, 2011.

Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, và Neil Houlsby. Scaling vision with sparse mixture of experts.
arXiv preprint arXiv:2106.05974, 2021.

Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel
Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor
Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini
Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan
Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten
Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan
Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, và Andrea Gesmundo. Scaling
up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022. URL
https://arxiv.org/abs/2203.17189.

Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, và Jason Weston. Hash layers for large sparse
models. arXiv preprint arXiv:2106.04426, 2021.

Clemens Rosenbaum, Tim Klinger, và Matthew Riemer. Routing networks: Adaptive selection of
non-linear functions for multi-task learning. arXiv preprint arXiv:1711.01239, 2017.

Clemens Rosenbaum, Ignacio Cases, Matthew Riemer, và Tim Klinger. Routing networks and the
challenges of modular and compositional computation. arXiv preprint arXiv:1904.12774, 2019.

Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Q Tran, Yi Tay, và
Donald Metzler. Confident adaptive language modeling. arXiv preprint arXiv:2207.07061, 2022.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
và Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
arXiv preprint arXiv:1701.06538, 2017.

Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,
Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorflow: Deep
learning for supercomputers. In Advances in Neural Information Processing Systems, pages
10414–10423, 2018.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the
imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint
arXiv:2206.04615, 2022.

Yi Tay, Mostafa Dehghani, Dara Bahri, và Donald Metzler. Efficient transformers: A survey. ACM
Computing Surveys (CSUR), 2020.

Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An
all-mlp architecture for vision. Advances in Neural Information Processing Systems, 34:24261–
24272, 2021.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.

22

--- TRANG 18 ---
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, và Samuel Bowman. Superglue: A stickier benchmark for general-purpose language
understanding systems. In Advances in Neural Information Processing Systems, pages 3266–
3280, 2019.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3):229–256, 1992.

Lemeng Wu, Mengchen Liu, Yinpeng Chen, Dongdong Chen, Xiyang Dai, và Lu Yuan. Residual
mixture of experts. arXiv preprint arXiv:2204.09636, 2022.

Canwen Xu và Julian McAuley. A survey on dynamic neural networks for natural language processing. arXiv preprint arXiv:2202.07101, 2022.

An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, và Hongxia Yang. M6-t:
Exploring sparse expert models and beyond, 2021.

Zhao You, Shulin Feng, Dan Su, và Dong Yu. Speechmoe: Scaling to large acoustic models with
dynamic routing mixture of experts. arXiv preprint arXiv:2105.03036, 2021.

Zhao You, Shulin Feng, Dan Su, và Dong Yu. Speechmoe2: Mixture-of-experts model with improved routing. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), pages 7217–7221. IEEE, 2022.

Seniha Esen Yuksel, Joseph N Wilson, và Paul D Gader. Twenty years of mixture of experts. IEEE
transactions on neural networks and learning systems, 23(8):1177–1193, 2012.

Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng
Chen, Quoc Le, và James Laudon. Mixture-of-experts with expert choice routing. arXiv preprint
arXiv:2202.09368, 2022.

Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, và
William Fedus. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906,
2022.

Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, và
Jianfeng Gao. Taming sparsely activated transformer with stochastic experts, 2021.

23
