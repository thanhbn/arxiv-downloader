# 2310.00811.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2310.00811.pdf
# File size: 550563 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
SPARSE BACKPROPAGATION FOR MOE T RAINING
Liyuan Liu§Jianfeng Gao§Weizhu Chen‡
§Microsoft Research ‡Microsoft Azure AI
{lucliu, jfgao, wzchen }@microsoft.com
ABSTRACT
One defining characteristic of Mixture-of-Expert (MoE) models is their capac-
ity for conducting sparse computation via expert routing, leading to remarkable
scalability. However, backpropagation, the cornerstone of deep learning, requires
dense computation, thereby posting challenges in MoE gradient computations.
Here, we introduce SparseMixer, a scalable gradient estimator that bridges the gap
between backpropagation and sparse expert routing. Unlike typical MoE training
which strategically neglects certain gradient terms for the sake of sparse com-
putation and scalability, SparseMixer provides scalable gradient approximations
for these terms, enabling reliable gradient estimation in MoE training. Grounded
in a numerical ODE framework, SparseMixer harnesses the mid-point method, a
second-order ODE solver, to deliver precise gradient approximations with negli-
gible computational overhead. Applying SparseMixer to Switch Transformer on
both pre-training and machine translation tasks, SparseMixer showcases consid-
erable performance gain, accelerating training convergence up to 2 times1.
1 I NTRODUCTION
The significant success of large-scale pre-training across various applications has underscored the
imperative need for scalable models that are economically feasible (Chowdhery et al., 2022; Ope-
nAI, 2023; Touvron et al., 2023). Recent advances in sparsely activated networks, prominently
known as Mixture-of-Experts (MoE), have attracted widespread interest (Shazeer et al., 2017; Lep-
ikhin et al., 2020; Fedus et al., 2021; Riquelme et al., 2021; Mustafa et al., 2022). Unlike traditional
networks that densely activate all modules for all input, MoE selectively activates parts of modules to
specific inputs through a process called expert routing, leading to notable efficiency enhancements.
However, such efficiency gain comes at a cost: gradient estimation in MoE becomes challenging
due to expert routing. Specifically, the routing function, being discrete in nature, produces non-
differentiable outputs. Meanwhile, backpropagation, the cornerstone of deep learning, relies on the
Chain rule, making it exclusively compatible with differentiable functions (Rosenblatt, 1957; Bengio
et al., 2013), and cannot be directly applied for gradient computation of expert routing.
Numerous methods have emerged to bridge discrete and back-propagation, and most of them are
based on Straight-Through (ST) (Rosenblatt, 1957; Bengio et al., 2013; Jang et al., 2017; Liu et al.,
2023). Unfortunately, all existing ST estimators are incompatible with MoE, since they require
activating all experts for gradient computing, thereby eliminating all the efficiency improvements
of MoE. Consequently, typical MoE training strategically neglects the gradient computation for
routing, trading certain training signals for sparse computation. Despite the scalability brought by
sparse computation, this trade-off may result in slow convergence and improperly trained models.
Our solution to this quandary is SparseMixer—a novel approach designed to reconcile the divide
between sparse MoE routing and backpropagation. Drawing inspiration from numerical methods
for ordinary differential equations (ODE), SparseMixer provides reliable gradient approximation
for expert routing, even when only a subset of experts are activated. Moreover, to furnish accurate
gradient approximations with negligible computation overheads, we integrate the mid-point method,
a second-order numerical ODE solver, which matches the Taylor expansion of the gradient to the
second order without requiring the Hessian matrix or other second-order derivatives.
1Implementations are available at https://github.com/microsoft/SparseMixer/ .
1arXiv:2310.00811v1  [cs.LG]  1 Oct 2023

--- PAGE 2 ---
We apply SparseMixer to Switch Transformer on both pretraining and neural machine translation.
SparseMixer not only accelerates training convergence by up to two times but also facilitates MoE
with properly trained expert routing. Remarkably, while Switch Transformer underperforms the
dense model in all three pretraining settings, incorporating SparseMixer as the gradient estimator
allows the resulting MoE models to consistently outperform the dense model.
2 R ELATED WORK AND PRELIMINARY
Mixture-of-Expert for Transformer. The idea of Mixture-of-Expert models originates from
Jacobs et al. (1991) and Jordan & Jacobs (1994), which integrates many separate networks and uses
each to handle a separate subset of training cases. Recently, many attempts have been made to
leverage this idea for scaling large language models (Shazeer et al., 2017; Lepikhin et al., 2020;
Lewis et al., 2021; Fedus et al., 2021).
To keep things straightforward, we will first focus on a simplified setting of the switch Transformer
layer (Fedus et al., 2021). We will then discuss its difference with the Switch Transformer and
necessary adaptations in Section 3.5, while the resulting algorithm can be easily extended to other
MoE designs. Considering a set of N experts, {fi(x)}N
i=1, the gate value of expert iis computed with
the softmax function as πi=softmax (θ)i=exp(θi)Pn
j=1exp(θj), where θ=Wr·x. For i∈[1,···, N],
we mark its one-hot representation as Ii∈ RN×1, whose element equals 1if it is the i-th element
or equals 0otherwise. Let Dbe a discrete random variable and D∈ {I1,···,IN}. Note that Dis
sampled as D∼πduring training, and is computed as D←arg maxIiπIiduring inference.
Then, the final output of this MoE layer is y=πDfD(x). Marking other parts of the neural
network as a differentiable function g:Rn→ R , we aim to minimize:
min
WrL(Wr),where L(Wr) =ED∼softmax (Wrx)[g(πDfD(x))] =X
DπD·g(πDfD(x)).(1)
Gradient Computation for Expert Routing. For simplicity, we mark∂L(Wr)
∂Wras∇0+∇1:
∂L
∂Wr:=∇0+∇1,where ∇0=X
Iig(πIifIi(x))∂πIi
∂ Wrand∇1=X
IiπIi∂g(πIifIi(x))
∂ Wr.(2)
It is easy to notice that ∇1can be computed reliably via backpropagation. ∇0, however, is hard to
reliably estimate in typical MoE training practice. In this study, we focus our discussions on ∇0.
5.35.45.55.65.75.8Training PPLMoE with 2 experts | WMT'14 En-DE
4.95.05.15.25.35.45.55.65.7MoE with 4 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.74.84.95.05.15.25.35.45.5MoE with 6 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.44.54.64.74.84.95.05.15.25.3Training PPLMoE with 8 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch3.84.04.24.44.6MoE with 16 experts | WMT'14 En-DE
Switch
Switch + SparseMixer
Switch + SparseMixer-ablation-1
Switch + SparseMixer-ablation-2
Figure 1: Training curves of Switch Transformer on WMT’14 En-De.
2

--- PAGE 3 ---
REINFORCE (Williams, 1992) is unbiased (i.e., E[∇REINFORCE ] =∇0) and only requires the dis-
tribution of the discrete variable to be differentiable (i.e., no backpropagation through g):
∇REINFORCE :=g(πDfD(x))∂logπD
∂ Wr. (3)
Despite the ∇REINFORCE estimator being unbiased, it tends to have prohibitively high variance, espe-
cially for networks that have other sources of randomness (i.e., dropout or other independent random
variables). Recently, attempts have been made to reduce the variance of REINFORCE (Gu et al.,
2016; Tucker et al., 2017; Grathwohl et al., 2018; Shi et al., 2022). Still, it has been found that the
REINFORCE-style estimators fail to work well in MoE training (Kool et al., 2021).
Straight-Through. Despite ∇REINFORCE being unbiased, it treats the remaining network ( g)
as a black-box and only leverages the zero-order information of g. In practice, a popular family
of estimators, Straight-Through (ST), leveraging the first-order information of g(note that gis a
scalar and g′is a vector), has been shown to achieve a better performance in more complicated
settings (Liu et al., 2023). ST computes the backpropagation “through” a surrogate that treats the
non-differentiable function (e.g., the sampling of D) as an identity function (Rosenblatt, 1957; Ben-
gio et al., 2013; Jang et al., 2017; Liu et al., 2023). In our MoE setting, ST treats the sampling of D
as an identity function and estimates the gradient as:
b∇ST:=∂g(πDfD(x))
∂πDfD(x)∂P
iDiπIifIi(x)
∂D∂πD
∂Wr. (4)
An alternative strategy is to conduct the concrete random variable relaxation (Maddison et al., 2014;
Jang et al., 2017). It is observed that the sampling of Dcan be reparameterized using Gumbel
random variables at the zero-temperature limit of the tempered softmax (Gumbel, 1954):
D= lim
τ→0Sτ,where Sτ=softmax τ(θ+G),Giare i.i.d., and Gi∼Gumbel (0,1).
Straight-Through Gumbel-Softmax (STGS) treats the zero-temperature limit as identity function
during the backpropagation:
b∇STGS:=∂g(πDfD(x))
∂πDfD(x)∂P
iSτ,iπIifIi(x)
∂Sτ∂Sτ
∂Wr. (5)
Although E[b∇ST]has been formally established as a first-order approximation of ∇0(Liu et al.,
2023), applying ST estimators necessitates the need for computing fi(x)for all i∈ {I1,···,IN},
i.e., the outputs from all experts. For example, in Equation 4, we have∂P
iDiπIifIi(x)
∂D=
diag(P
iDiπIifIi(x)), which involves the computation of {fI1(x),···, fIN(x)}. Essentially,
computing all fIiturns MoE into a densely activated network. Thus, using ST-style estimators
undermines the sparse computation, fundamentally obstructing the scaling of MoE models.
MoE Training Practice. Due to all these challenges, the current MoE training practice trades
certain training signals for scalability. Specifically, ∇0is strategically neglected in gradient compu-
tation (the value of ∇0is set to 0), and only ∇1is used for model training (Fedus et al., 2021).
Despite the success of such practice, it remains unclear on the impact of neglecting ∇0, how to
conduct training with only part of the gradient, and whether gradient descent is still effective after
neglecting ∇0. In this study, we aim to bridge backpropagation and expert routing by providing a
scalable and reliable approximation of ∇0and scale MoE models without ∇0being neglected .
3 F ROM DISCRETE TO SPARSE : SPARSE MIXER
Although ST estimators bridged discrete variables and backpropagation, they require the network to
be densely activated. Here, we first discuss the intrinsic limitation of ST estimators. Then, we go
beyond discrete and bridge sparse expert routing and backpropagation.
3.1 W HYEXISTING ST E STIMATORS ARE NOT SCALABLE ?
Targeting to approximate gradients for discrete variables in the general multinomial case, we for-
mally establishes that E[b∇ST]is a first-order approximation of ∇0in Liu et al. (2023). To discuss ST
3

--- PAGE 4 ---
in the general setting, we reparameterize the expert network y←πDfDas a function of discrete
variables, marked as y←h(D) =P
iDiπIifIi. Then, we have2:
∇0=X
Ii(h(Ii)−E[h])∂πIi
∂ Wr=X
IiX
IjπIj(h(Ii)−h(Ij))∂πIi
∂ Wr. (6)
Specifically, approximating h(Ii)−h(Ij)ash′(Ij)·(Ii−Ij), the resulting gradient approximation
will have the same form as E[b∇ST](Liu et al., 2023). In numerical analyses, this approximation
is known as the forward Euler method (briefly introduced in Appendix A), which has first-order
accuracy. In Liu et al. (2023), we also explored higher-order ODE solvers to better approximate
h(Ii)−h(Ij). However, all these attempts require the network to be densely activated, since∂h
∂D=P
IiDiπIifIinecessitates the computation of {fI1,···, fIN}. In order words, although those
ST estimators bridge discrete and backpropagation, their computations are dense instead of sparse,
blocking their application on MoE training.
3.2 E XPERT ROUTING GRADIENT APPROXIMATION : BACKPROPAGATION MADE SPARSE
To bridge the gap between sparse MoE routing and back-propagation, we need to approximate ∇0
without requiring outputs from all experts. In our study, we present a novel framework to move
beyond ST and bridge backpropagation and sparse expert routing.
Gradient Approximation for Expert Routing. Here, we start by introducing the most simple
gradient estimator, i.e., b∇SparseMixer -1st , where
b∇SparseMixer -1st :=∂g(πDfD(x))
∂ Wr.
Similar to E[b∇ST],E[b∇SparseMixer -1st ]is a first-order approximation of ∇0. To demonstrate this, we
take an alternative approach to rewrite ∇0:
∇0=X
Ii(g(πIifIi)−g(0))∂πIi
∂ Wr. (7)
Adopting the Euler method, we estimate g(πIifIi)−g(0)asg′(πIifIi)·πIifIi. Then, we have:
∇0forward Euler≈X
Iig′(πIifIi)·πIifIi·∂πIi
∂ Wr=ED∼π[∂g(πDfD(x))
∂ Wr] =E[b∇SparseMixer -1st ].
Gradient Approximation for General Discrete Variables. To compare with existing ST esti-
mators, we apply b∇SparseMixer -1st to the general case. Similar to Equation 7, we have
∇0=X
Ii(h(Ii)−h(0))∂πIi
∂ Wrforward Euler≈X
Iih′(Ii)·Ii·∂πIi
∂ Wr. (8)
Notably, the first-order approximation of Equation 8 only requires the output of one expert, i.e.,
h′(Ii)·Ii=X
IjDIj·πIj·fIj·Ii=πIjfIj. (9)
In other words, Equation 8, taking h(0)as the baseline, leverages the one-hot representation Ii
to reduce the computation requirement of unactivated experts, thus achieving sparse computations.
Meanwhile, the first-order approximation of Equation 6, i.e., h′(Ij)·(Ii−Ij), has the term h′(Ij)·Ii
and requires a dense computation.
As a summary, both b∇STandb∇SparseMixer -1st adopt the forward Euler method and achieve first-order
accuracy. At the same time, b∇SparseMixer -1st only requires the output of one expert thus not sacrificing
scalability, while b∇STrequires the output of all experts.
2Commonly referred to as baseline subtraction. NoteP
iE[g]∂πIi
∂ Wr=E[g]∂P
IiπIi
∂ Wr=E[g]∂1
∂ Wr= 0.
4

--- PAGE 5 ---
3.3 A CHIEVING SECOND -ORDER ACCURACY WITH THE MID-POINT METHOD
The literature on numerical methods for differential equations shows that it is possible to achieve
higher-order accuracy without computing higher-order derivatives . To furnish accurate gradient
approximations, we employ a second-order ODE method, the mid-point method (briefly introduced
in Appendix A). Specifically, b∇SparseMixer-2rd is a second-order approximation of ∇, where
b∇SparseMixer-2rd := 2·∂g(πDfD(x)
2)
∂ Wr.
To demonstrate the connection between b∇SparseMixer-2rd and the mid-point method, we employ the
mid-point method to approximate g(πIifIi)−g(0)asg′(πIifIi
2)·πIifIi, which also requires only
the output of one expert. Similarly, it is easy to note:
∇0mid−point≈X
Iig′(πIifIi
2)·πIifIi·∂πIi
∂ Wr=ED∼π[2·∂g(πDfD(x)
2)
∂ Wr] =E[b∇SparseMixer-2rd ].
Notably, it is feasible to employ more advanced ODE solvers like RKF4 and approximate ∇0with
even higher-order accuracy (Fehlberg, 1969). In our experiments, we observe that the mid-point
method is accurate enough and decide to stick to the mid-point method for simplicity.
3.4 B ALANCING ROUTER TRAINING AND EXPERT TRAINING
Comparing to b∇SparseMixer -1st ,b∇SparseMixer-2rd provides better gradient estimation for router train-
ing. However, b∇SparseMixer-2rd causes additional difficulties for expert training. Specifically,
b∇SparseMixer-2rd requires to change the MoE output from y←πDfD(x)toy←πDfD(x)
2, lead-
ing to a gap between the training ( y←πDfD(x)
2) and the inference ( y←πDfD(x)). As discussed
in Section 4.3, such gap creates significant obstacles for MoE training.
Meanwhile, Dis assigned as D←arg maxIiπIiduring the inference, instead of being sampled
fromπ. Thus, it would be sufficient to close the gap by only applying b∇SparseMixer-2rd whenD̸=
arg maxIiπIi. Accordingly, we propose SparseMixer to balance router training and expert training:
b∇SparseMixer := (1−δD)b∇SparseMixer-2rd +δDb∇SparseMixer-1st ,where δD=(1,ifD= arg max
IiπIi
0,otherwise.
Computational Efficiency of SparseMixer . b∇SparseMixer does not require Hessian or other
second-order derivatives, thus having negligible computation overheads (empirical verifications are
discussed in Section 4.4). At the same time, similar to b∇ST, our proposed algorithm can be easily in-
tegrated with popular library like PyTorch, making it easy to be integrated with existing algorithms.
3.5 F ROM SIMPLIFIED MOETOSWITCH TRANSFORMER
As mentioned in Section 2, our modeling of MoE is a simplified Switch Transformer. Here, we
first discuss the difference between our simplified setting and Switch Transformer, and then move to
necessary modifications to apply SparseMixer to Switch Transformer.
Discussion on Setting Difference. The difference between our simplified setting and switch
Transformer is the sampling of D. Specifically, in our simplified setting, we assume Dis sampled
fromπ; in Switch Transformer, Dis sampled as:
D= arg max
Ii(θIi·uIi),where uIiiid∼Uniform (1−r,1 +r). (10)
As discussed in Fedus et al. (2021), directly sampling Dfromπleads to notable performance
degradation (also discussed in Section 4.2). Meanwhile, in the Switch Transformer, the distribution
ofDdoes have no analytical form and thus no analytical gradients, making SparseMixer not directly
5

--- PAGE 6 ---
0 5 10 15 20 25 30
Epoch0.220.240.260.280.300.32Replace AccuracyMoE with 2 experts | ELECTRA-base
0 5 10 15 20 25 30
Epoch0.240.260.280.300.320.34MoE with 4 experts | ELECTRA-base
0 5 10 15 20 25 30
Epoch0.260.280.300.320.340.36MoE with 8 experts | ELECTRA-base
Switch + SparseMixer
SwitchFigure 2: Training curves of Switch Transformer on ELECTRA-base training.
applicable. In our experiments, we deploy a sampling process that is differentiable as sampling from
π, while sharing some important properties with Switch Transformer
Sampling Property of Switch Transformer. Here, we mark θ∗:= max IiθIi. Then, in Switch
Transformer, Iiwill never be sampled if θ∗−θIi> r·(|θ∗|+|θIi|). In other words, the distribution
ofDin switch Transformer is masked: small probabilities would directly drop to zero once the
corresponding logits hit a threshold. In our experiments, we observe that such sparse distribution
plays a crucial role in the success of MoE (as elaborated in Section 4.2).
Applying SparseMixer to Switch Transformer. Correspondingly, we changed the computation
ofπfromπi=softmax (θ)i=exp(θi)Pn
j=1exp(θj), toπi=exp(θi)·∆i Pn
j=1exp(θj)·∆j, where ∆j=δ(θ∗−θIi≤
r·(|θ∗|+|θIi|)). In other words, we apply a mask to the softmax function, in order to sample only
from experts that are not masked by the Switch Transformer.
Additionally, since the value of πwill be different after applying the mask (which impacts the
gradient magnitude of other components), we further changed the output of the MoE layer from
πD·fD(x)toω·πD·fD(x), where ωis trainable and is initialized as the 1vector. Intuitively,
ωcan be viewed as an adaptation on the learning rate for training expert networks. Note that, ωcan
be re-parameterized into the feedforward layer after training.
4 E XPERIMENTS
Here, we conduct experiments on both pretraining and neural machine translation tasks. We closely
follow the experiment setting of the existing study. Due to the constraint of computation resources,
we left MoE related hyper-parameters untuned in all settings, i.e., the jitter (i.e., rin Equation 10)
is set to 0.1 and the ratio for the load balancing loss is set to 0.01 (Fedus et al., 2021). Detailed
experiment configurations are elaborated in Appendix B.
4.1 A PPLYING SPARSE MIXER ON SWITCH TRANSFORMER
NMT on WMT’14 En-De. We visualized the training curve in Figure 1 and sum-
marized the BLEU score in Table 1. Regarding both convergence speed and the final per-
formance, Switch+SparseMixer consistently outperforms Switch in all five settings. Notably,
Switch+SparseMixer matches the training performance of Switch with about 50% less training up-
dates when N∈ {4,6,8}and about 40% less training updates when N∈ {2,16}.
Table 1: BLEU score on WMT’14 En-De ( Nrefers to the number of experts).
DenseMixture-of-Expert
N= 2 N= 4 N= 6 N= 8 N= 16
Transformer-base 28.33 / / / / /
Switch / 28.17 28.05 27.96 27.99 27.81
Switch+SparseMixer / 28.72 28.61 28.32 28.12 28.08
6

--- PAGE 7 ---
Table 2: Results on the GLUE development set. S refers to Switch and S+S refers to
Switch+SparseMixer. A VG is the average score across eight tasks.
N Model A VGMNLI-(m/mm) QQP QNLI SST-2 CoLA RTE MRPC STS-B
(Acc.) (Acc.) (Acc.) (Acc.) (Mat. Corr.) (Acc.) (Acc.) (Spear. Corr.)
1 Dense 87.37 88.72/88.40 91.90 93.36 93.35 68.71 82.31 89.95 90.83
2S 87.62 88.55/88.34 91.86 93.52 94.27 67.90 83.76 90.69 90.52
S+S 88.31 89.06/88.78 91.98 93.54 94.38 69.96 85.20 91.67 90.81
4S 87.02 88.12/88.40 91.73 93.21 93.92 70.89 77.26 90.44 90.49
S+S 87.63 88.97/88.41 91.92 93.54 94.04 71.00 80.87 90.69 90.72
8S 87.27 88.43/88.22 91.78 93.23 94.84 68.06 80.87 90.44 90.62
S+S 87.71 88.69/88.47 92.03 93.41 94.15 69.00 83.76 89.95 90.81
0 50 100 150 200 250 300 350
Epoch5.45.65.86.06.2Training PPLMoE with 2 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch5.05.25.45.65.86.0MoE with 4 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.85.05.25.45.65.86.0MoE with 6 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.44.64.85.05.25.45.65.86.0MoE with 8 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.004.254.504.755.005.255.505.756.00MoE with 16 experts | WMT'14 En-DE
Switch
Switch w.o. Scaling
Figure 3: Comparison between Switch Transformer andSwitch Transformer without Scaling .
We can observe that, with more experts, MoE models achieve lower training loss with a worse BLEU
score. Specifically, although Switch Transformer achieves better training performance, its final
performance (BLEU score) never outperforms the Dense model, regardless of how many experts
it has. We believe it requires more data to fully unleash the potential of MoE and suggest this
phenomenon indicates that MoE models are prone to overfitting (Zuo et al., 2022).
Meanwhile, without changing hyper-parameters or model architectures, the downstream perfor-
mance of Switch + SparseMixer outperforms both Dense and Switch, when N∈ {2,4}. Specif-
ically, SparseMixer improves the performance of Switch from 28.17 to 28.72 (when N= 2) and
from 28.05 to 28.61 (when N= 4). This phenomenon implies that, with the help of SparseMixer, a
sound gradient estimator, MoE learns an expert routing that generalizes better.
Pretraining. Following previous work (Dong et al., 2023), we visualized the training curve
in Figure 2 and summarized the fine-tuning results in Table 2. Regarding both convergence speed
and downstream performance, Switch+SparseMixer consistently outperforms Switch in all settings.
Also, similar to the experiments on machine translation, we observe that MoE models are easier to
overfit and both settings achieve the best downstream performance with two experts.
Also, it is worth mentioning that, while Switch Transformer only outperforms the dense model when
the number of experts is set to 2, Switch + SparseMixer consistently outperforms the Dense model
in all four settings. This phenomenon further verifies our intuition that SparseMixer facilitates MoE
models with better expert router training, thus having the resulting model to generalize better.
4.2 D ISCUSSIONS
Here, we conduct experiments to discuss our modeling of the MoE layer as in Section 2.
Importance of Scaling Expert Outputs with Gating Networks. One important design detail of
MoE is to scale the output of the expert network with the gating network. Specifically, the output of
the MoE layer is computed as y←πDfD(x), instead of y←fD(x). This scaling design greatly
facilitates the derivation of SparseMixer in Section 3, and inspires the introduction of ω(further
discussed in Section 4.3). Here, we empirically demonstrate that this scaling design also plays an
important role in Switch Transformer.
Specifically, we conduct experiments with a variant of Switch Transformer, i.e., Switch w.o. Scal-
ing, which sets the output of the MoE layer as y←fD(x). We apply this Switch variant on
WMT’14 En-De and visualize the training curve in Figure 3. Switch ( y←πDfD(x)) significantly
outperforms this variant ( y←fD(x)). Also, we can observe that, when the number of experts is
7

--- PAGE 8 ---
0 50 100 150 200 250 300 350
Epoch5.45.65.86.06.2Training PPLMoE with 2 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch5.05.25.45.65.86.0MoE with 4 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.85.05.25.45.65.86.0MoE with 6 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.44.64.85.05.25.45.65.86.0MoE with 8 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.004.254.504.755.005.255.505.756.00MoE with 16 experts | WMT'14 En-DE
Switch + SparseMixer
Switch + SparseMixer w.o. MaskFigure 4: Comparison between SparseMixer andSparseMixer without applying mask to sampling .
0 50 100 150 200 250 300 350
Epoch5.35.45.55.65.75.8Training PPLMoE with 2 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.95.05.15.25.35.45.55.65.7MoE with 4 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.74.84.95.05.15.25.35.45.5MoE with 6 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch4.44.54.64.74.84.95.05.15.25.3MoE with 8 experts | WMT'14 En-DE
0 50 100 150 200 250 300 350
Epoch3.84.04.24.44.6MoE with 16 experts | WMT'14 En-DE
Switch + SparseMixer
Switch + SparseMixer-2rd
Figure 5: Comparison between SparseMixer andSparseMixer-2rd .
set to 6, using this variant would lead to additional training instability, which further demonstrates
the importance of the scaling design.
Importance of Applying Mask to Softmax. In Section 3.5, we identify that the sampling in
Switch Transformer plays an important role in the success of Switch Transformer. As discussed in
Fedus et al. (2021), directly using softmax sampling would lead to an inferior performance.
Here, we demonstrate that this masked softmax sampling also plays an important role in Switch +
SparseMixer. Specifically, we conduct experiments with a variant of SparseMixer, i.e., SparseMixer
w.o. Mask, which computes πi←softmax (θ)i. We apply SparseMixer w.o. Mask on WMT’14 En-
De and visualize the training curve in Figure 4. SparseMixer ( πi←exp(θi)·∆i Pn
j=1exp(θj)·∆j) significantly
outperforms this variant ( y←softmax (θ)i). Also, we can observe that, when the number of experts
is set to 6, using this variant would lead to additional training instability, which further demonstrates
the importance of applying mask to softmax.
4.3 A BLATION
Here, we conduct experiments to discuss the design details of SparseMixer.
Importance of Balancing Expert Learning and Routing Learning. While SparseMixer-2rd
provides better gradient approximation for expert routing, it creates a gap between training and
inference. To demonstrate the importance of balancing router training and expert training, we con-
duct experiments on applying SparseMixer-2rd on WMT’14 En-De. As visualized in Figure 5,
SparseMixer consistently outperforms SparseMixer-2rd in all cases. Also, SparseMixer-2rd exhibits
training instability when setting the number of experts to 2.
Mid-point Method and ωScaling. To better understand the benefit introducing ω(as in Sec-
tion 3.5) and make comparisons with SparseMixer-1st, we conduct additional ablation studies on
WMT’14 En-De. Specifically, we consider two SparseMixer variants:
• ablation-1 removes ωfrom SparseMixer (i.e., changes the output of Switch + SparseMixer from
ω·πD·fD(x)toπD·fD(x)).
• ablation-2 further replaces the mid-point method with the forward Euler method in SparseMixer-
ablation-1, i.e., b∇SparseMixer-1st is employed as the gradient estimator and ωis removed.
We apply these two variants to WMT’14 En-De and visualize their training curve in Figure 1. The
results further verified our intuition that ωfacilitates MoE training by alleviating the impact of
applying masks. Also, it shows that integrating the mid-point method helps to better approximate
expert routing gradient.
8

--- PAGE 9 ---
Table 3: Average Training Time Cost (s/update). Nrefers to the number of experts.
WMT’14 En-De Pretraining
N= 2 N= 4 N= 6 N= 8 N= 16 N= 2 N= 4 N= 8
Switch 0.32 0.33 0.34 0.36 0.40 1.87 1.90 1.98
Switch + SparseMixer 0.32 0.33 0.34 0.36 0.40 1.87 1.90 1.98
4.4 E FFICIENCY
We summarized the average time cost per update in Table 3. Switch+SparseMixer achieves an iden-
tical average time cost with Switch in all eight settings. This shows that the computation overheads
of SparseMixer are negligible.
5 C ONCLUSION
In this study, we present SparseMixer to move beyond discrete and bridge the gap between sparse
MoE routing and backpropagation. Rooted in a numerical ODE framework, SparseMixer harnesses
the mid-point method, a second-order ODE solver, to deliver precise gradient approximations with
negligible computational overhead. In our experiments on both neural machine translation and pre-
training tasks, SparseMixer not only accelerates training convergence by up to two times but also
facilitates MoE with properly trained expert routing. Remarkably, while Switch Transformer under-
performs the dense model in all three pretraining settings, incorporating SparseMixer as the gradient
estimator allows the resulting MoE models to consistently outperform the dense model.
There are multiple interesting directions to be explored in the future. While our method is based on
first-order and second-order ODE solvers, it would be interesting to apply higher-order ODE solvers
and even adaptive ODE solvers like RKF4 (Fehlberg, 1969). Also, since our study paves the way
towards designing gradient approximation for scalable MOE training, we plan to further improve
the architecture design of MoE models. In the end, as we observed that MoE models are easier to
overfit, we plan to study the scaling law of sparse models and facilitate large-scale pre-training.
REFERENCES
Uri M. Ascher and Linda R. Petzold. Computer methods for ordinary differential equations and
differential-algebraic equations . 1998.
Payal Bajaj, Chenyan Xiong, Guolin Ke, Xiaodong Liu, Di He, Saurabh Tiwary, Tie-Yan Liu, Paul
Bennett, Xia Song, and Jianfeng Gao. Metro: Efficient denoising pretraining of large scale au-
toencoding language models with model generated signals. ArXiv , abs/2204.06644, 2022.
Yoshua Bengio, Nicholas L ´eonard, and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. ArXiv , abs/1308.3432, 2013.
Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Lev-
eling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014
workshop on statistical machine translation. In Workshop on Statistical Machine Translation ,
2014.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,
Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam
Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James
Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-
skaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin
Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret
Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
9

--- PAGE 10 ---
Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways.
ArXiv , abs/2204.02311, 2022.
Kevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. Electra: Pre-training
text encoders as discriminators rather than generators. In ICLR , 2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL , 2019.
Chengyu Dong, Liyuan Liu, Hao Cheng, Jingbo Shang, Jianfeng Gao, and Xiaodong Liu. Under-
stand and modularize generator optimization in electra-style pretraining. In ICML , 2023.
William Fedus, Barret Zoph, and Noam M. Shazeer. Switch transformers: Scaling to trillion param-
eter models with simple and efficient sparsity. ArXiv , abs/2101.03961, 2021.
Erwin Fehlberg. Classical fifth-and seventh-order runge-kutta formulas with stepsize control. Com-
puting , 1969.
Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Kristjanson Duvenaud. Back-
propagation through the void: Optimizing control variates for black-box gradient estimation. In
ICLR , 2018.
Shixiang Shane Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backprop-
agation for stochastic neural networks. In ICLR , 2016.
Emil Julius Gumbel. Statistical Theory of Extreme Values and Some Practical Applications : A
Series of Lectures . 1954.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert
with disentangled attention. ArXiv , abs/2006.03654, 2020.
Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures
of local experts. Neural Computation , 3:79–87, 1991.
Eric Jang, Shixiang Shane Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax.
InICLR , 2017.
Michael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the em algorithm.
Neural Computation , 6:181–214, 1994.
Wouter Kool, Chris J. Maddison, and Andriy Mnih. Unbiased gradient estimation with balanced
assignments for mixtures of experts. In I (Still) Can’t Believe It’s Not Better Workshop at NeurIPS
2021 , 2021.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam M. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding. ArXiv , abs/2006.16668, 2020.
Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers:
Simplifying training of large, sparse models. In ICML , 2021.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. In ICLR , 2020a.
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the diffi-
culty of training transformers. In EMNLP , 2020b.
Liyuan Liu, Chengyu Dong, Xiaodong Liu, Bin Yu, and Jianfeng Gao. Bridging discrete and back-
propagation: Straight-through and beyond. ArXiv , abs/2304.08612, 2023.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks
for natural language understanding. In ACL, 2019.
10

--- PAGE 11 ---
Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu.
Understanding and improving transformer from a multi-particle dynamic system point of view.
InICLR Workshop DeepDiffEq , 2020.
Chris J. Maddison, Daniel Tarlow, and Thomas P. Minka. A* sampling. In NIPS , 2014.
Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Mul-
timodal contrastive learning with limoe: the language-image mixture of experts. ArXiv ,
abs/2206.02770, 2022.
OpenAI. Gpt-4 technical report. ArXiv , abs/2303.08774, 2023.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In NAACL-HLT
Demonstrations , 2019.
Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. ArXiv , abs/1910.10683, 2019.
Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr ´e Su-
sano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. In
NeurIPS , 2021.
Frank Rosenblatt. The perceptron, a perceiving and recognizing automaton Project Para . Cornell
Aeronautical Laboratory, 1957.
Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V . Le, Geoffrey E.
Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-
experts layer. In ICLR , 2017.
Jiaxin Shi, Yuhao Zhou, Jessica Hwang, Michalis Titsias, and Lester Mackey. Gradient estimation
with discrete stein operators. In NeurIPS , 2022.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In CVPR , 2016.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
ArXiv , abs/2307.09288, 2023.
G. Tucker, Andriy Mnih, Chris J. Maddison, John Lawson, and Jascha Narain Sohl-Dickstein. Re-
bar: Low-variance, unbiased gradient estimates for discrete latent variable models. In NIPS ,
2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
Glue: A multi-task benchmark and analysis platform for natural language understanding. In
BlackboxNLP@EMNLP , 2018.
Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning , 8:229–256, 1992.
11

--- PAGE 12 ---
Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-
ralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by
watching movies and reading books. In ICCV , 2015.
Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and
Jianfeng Gao. Taming sparsely activated transformer with stochastic experts. In ICLR , 2022.
A F ORWARD EULER METHOD AND MIDPOINT METHOD
For simplicity, we consider a simple function g(x) :R → R that is three times differentiable on
[t0, t1]. Now, we proceed to a simple introduction to approximateRt1
t0g′(x)dxwith the Forward
Euler Method and the Midpoint Method. For a detailed introduction to numerical ODE methods,
please refer to Ascher & Petzold (1998).
Forward Euler Method. Here, we approximate g(t1)with the first-order Taylor expansion, i.e.,
g(t1) =g(t0) +g′(t0)·(t1−t0) +O((t1−t0)2), then we haveRt1
t0g′(x)dx≈g′(t0)(t1−t0).
Since we used the first-order Taylor expansion, this approximation has first-order accuracy.
Midpoint Method. First, we approximate g(t1)with the second-order Taylor expansion:
g(t1) =g(t0) +g′(t0)·(t1−t0) +g′′(t0)
2·(t1−t0)2+O((t1−t0)3). (11)
Then, we show that we can match this approximation by using g′(t1+t0
2). Taylor expanding
g′(t1+t0
2)to the first-order, we have:
g′(t1+t0
2) =g′(t0) +g′′(t0)·t1−t0
2+O((t1−t0)2)
Therefore, we have:
g(t0) +g′(t1+t0
2)(t1−t0) =g(t0) +g′(t0)·(t1−t0) +g′′(t0)
2·(t1−t0)2+O((t1−t0)3).
It is easy to notice that the right-hand side of the above equation matches the second-order Taylor
expansion of g(t1)as in Equation 11. Therefore, the above approximation (i.e., approximating
g(t1)−g(t0)asg′(t1+t0
2)(t1−t0)) has second-order accuracy.
Connection to f(Ii)−f(0).By setting g(x) =f(x·Ii), we have g(1)−g(0) = f(Ii)−f(0).
Then, it is easy to notice that the forward Euler Method approximates f(Ii)−f(0)as∂f(Ii)
∂IiIiand
has first-order accuracy. Also, the mid-point method approximates f(Ii)−f(0)as∂f(Ii/2)
∂Ii/2Iiand
has second-order accuracy.
B E XPERIMENT SETTING
Here, we conduct experiments on both pretraining and neural machine translation tasks. We closely
follow the experiment setting of the existing study. Due to the constraint of computation resources,
we left MoE related hyper-parameters untuned in all settings, i.e., jitter ( r) is set to 0.1 and load
balance loss ratio is set to 0.01 (Fedus et al., 2021).
B.1 N EURAL MACHINE TRANSLATION
Problem Setting. Our experiments are based on the fairseq package (Ott et al., 2019). As to pre-
processing, we follow the public released script from previous work (Lu et al., 2020), and conduct
evaluations on the provided ‘newstest14‘ file. More details can be found in Bojar et al. (2014).
Model Architecture. As to model specifics, we directly adopt the Transformer-base model on the
WMT’14 En-De datasets. Specifically, we use encoder-decoder Transformer models with 6 encoder
layers, 6 decoder layers, 512-dimension word embedding, 8-head attentions, and 2048-dimension
feed-forward layers. Following Fedus et al. (2021), we apply MoE layers at every other feed-forward
12

--- PAGE 13 ---
layers, set jitter to 0.1, and configure load balance ratio as 1·10−2. As the number of experts, we
consider 5 different settings, i.e., N∈ {2,4,6,8,16}. Label smoothed cross-entropy is used as the
objective function with the uncertainty set as 0.1(Szegedy et al., 2016).
Training Settings. We mostly followed (Liu et al., 2020a) for training settings. Specifically, we
use Adam as the optimizer set (β1, β2)as(0.9,0.98), use inverse sqrt learning rate scheduler with a
warmup phrase (8000 steps). All dropout ratios (including activation dropout and attention dropout)
are set to 0.1. The maximum learning rate is set to 7·10−4and the maximum token number per
batch is set to 217. We conduct training for 4·105updates and report the performance of the last
checkpoint and the checkpoint with the lowest development loss.
B.2 P RE-TRAINING
Pre-training Setup. We follow the standard settings for training Base models (Clark et al., 2020;
Bajaj et al., 2022; Dong et al., 2023), Specifically, we employ Wikipedia and BookCorpus (Zhu
et al., 2015) for pre-training and set the sequence length to 512, which leads to 16GB of texts and
256M samples. We use a cased sentence piece BPE vocabulary of 128K tokens following He et al.
(2020), and conduct pre-training for 125K updates with a batch size of 2048 sentences.
Model Architecture. Our main model (discriminator) setting follows the BERT basearchitec-
ture (Devlin et al., 2019). Specifically, the model has 12 layers, 768-dimension embedding, and
12-head attention. As to the feed-forward networks, we set the number of hidden state dimensions
to 3076. Following Bajaj et al. (2022) and Dong et al. (2023), we further enhanced the model with
the T5 relative position encoding (Raffel et al., 2019) and use 32bins. We set dropout as 0.1and
employ Admin (Liu et al., 2020b) for model initialization to stabilize the training. Following Fedus
et al. (2021), we apply MoE layers at every other feed-forward layers, set jitter to 0.1, and config-
ure load balance ratio as 1·10−2. As the number of experts, we consider 3 different settings, i.e.,
N∈ {2,4,8}. As to the auxiliary model, we follow previous works (Clark et al., 2020; Bajaj et al.,
2022) to set the size of the auxiliary model (generator) to be 4layers.
Optimization. We configure the optimizer as Adam, (β1, β2)as(0.9,0.98), weight decay as
0.01, the loss weight as 50, the peak learning rate as 5e−4, and the warmup steps as 10K.
Downstream evaluation setup. We conduct evaluation on downstream tasks following the setup
in previous works (Bajaj et al., 2022). Specifically, we conduct single-task, single-model fine-tuning
on the GLUE (Wang et al., 2018) benchmark. As summarized in the Appendix (Table 4), GLUE
includes 9 subtasks. Following Liu et al. (2019), we conduct a grid-search on hyper-parameters and
report the best performance for both Switch and Swith + SparseMixer. The complete search space
is included in Appendix (Table 5).
13

--- PAGE 14 ---
Table 4: GLUE task descriptions and statistics. The second and fourth column denotes the number
of training examples and the number of classes. Note that STS-B is a regression task.
Corpus |Train| |Label|Task Metric(s) Domain
Single-Sentence Classification
CoLA 8.5k 2 acceptibility Matthews corr. misc.
SST-2 67k 2 sentiment accuracy movie reviews
Sentence Similarity/Paraphrase
MRPC 3.7k 2 paraphrase accuracy news
STS-B 5.7k - similarity Spearman corr. misc.
QQP 364k 2 similarity accuracy social QA questions
Natural Language Inference (NLI)
MNLI 393k 3 NLI (mis)matched acc. misc.
QNLI 108k 2 QA/NLI accuracy Wikipedia
RTE 2.5k 2 NLI accuracy misc.
WNLI 634 2 coreference/NLI accuracy fiction books
Table 5: Hyperparameter search space in fine-tuning.
Hyperparameters Base
Sequence Length 256
Optimizer Adam
Peak Learning Rate {5e-5,1e-4, 3e-4 }
Max Epochs {2,3,5,10 }
Batch size {16, 32}
Learning rate decay Linear
Weight Decay {0, 0.01 }
Warm-up Proportion {6 %, 10 % }
Adam ϵ 1e-6
Adam (β1, β2) (0 .9,0.98)
Gradient Clipping 1.0
Dropout 0.1
14
