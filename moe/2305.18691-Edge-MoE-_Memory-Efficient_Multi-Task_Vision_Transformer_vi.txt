# 2305.18691.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2305.18691.pdf
# Kích thước tệp: 1223707 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Edge-MoE: Kiến trúc Vision Transformer Đa tác vụ Tiết kiệm Bộ nhớ với Tính thưa thớt Cấp độ tác vụ thông qua Mixture-of-Experts
Rishov Sarkar1, Hanxue Liang2, Zhiwen Fan2, Zhangyang Wang2, Cong Hao1
1Trường Kỹ thuật Điện và Máy tính, Viện Công nghệ Georgia
2Trường Kỹ thuật Điện và Máy tính, Đại học Texas tại Austin
rishov.sarkar@gatech.edu, lhx92505991@gmail.com, {zhiwenfan, atlaswang}@utexas.edu, callie.hao@ece.gatech.edu
Tóm tắt —Cộng đồng thị giác máy tính đang chấp nhận hai mô hình học tập đầy hứa hẹn: Vision Transformer (ViT) và Multi-task Learning (MTL). Các mô hình ViT thể hiện hiệu suất vượt trội so với các mạng tích chập truyền thống nhưng thường được công nhận là tốn nhiều tính toán, đặc biệt là cơ chế tự chú ý với độ phức tạp bậc hai. MTL sử dụng một mô hình để suy luận nhiều tác vụ với hiệu suất tốt hơn bằng cách thực thi biểu diễn chung giữa các tác vụ, nhưng một nhược điểm lớn là, hầu hết các chế độ MTL đều yêu cầu kích hoạt toàn bộ mô hình ngay cả khi chỉ cần một hoặc một vài tác vụ, gây ra lãng phí tính toán đáng kể. M3ViT là mô hình ViT đa tác vụ mới nhất giới thiệu mixture-of-experts (MoE), trong đó chỉ một phần nhỏ các mạng con ("experts") được kích hoạt một cách thưa thớt và động dựa trên tác vụ hiện tại. M3ViT đạt được độ chính xác tốt hơn và giảm hơn 80% tính toán đồng thời mở đường cho MTL thời gian thực hiệu quả sử dụng ViT.

Mặc dù có những ưu điểm thuật toán của MTL, ViT và thậm chí M3ViT, vẫn còn nhiều thách thức cho việc triển khai hiệu quả trên FPGA. Ví dụ, trong các mô hình Transformer/ViT nói chung, cơ chế tự chú ý được biết đến là tốn nhiều tính toán và yêu cầu băng thông cao. Ngoài ra, các phép toán softmax và hàm kích hoạt GELU được sử dụng rộng rãi, không may có thể tiêu thụ hơn một nửa toàn bộ tài nguyên FPGA (LUT). Trong mô hình M3ViT, cơ chế MoE đầy hứa hẹn cho đa tác vụ phơi bày những thách thức mới về chi phí truy cập bộ nhớ và cũng tăng việc sử dụng tài nguyên do có nhiều loại lớp hơn.

Để giải quyết những thách thức này trong cả các mô hình Transformer/ViT nói chung và M3ViT đa tác vụ tiên tiến với MoE, chúng tôi đề xuất Edge-MoE, bộ tăng tốc FPGA từ đầu đến cuối đầu tiên cho ViT đa tác vụ với một bộ sưu tập phong phú các đổi mới kiến trúc. Đầu tiên, cho các mô hình Transformer/ViT nói chung, chúng tôi đề xuất (1) một cơ chế sắp xếp lại mới cho tự chú ý, giảm yêu cầu băng thông từ tỷ lệ thuận xuống hằng số bất kể song song mục tiêu; (2) một xấp xỉ softmax một lần qua nhanh; (3) một xấp xỉ GELU chính xác và chi phí thấp, có thể giảm đáng kể độ trễ tính toán và sử dụng tài nguyên; và (4) một đơn vị tính toán thống nhất và linh hoạt có thể được chia sẻ bởi hầu hết tất cả các lớp tính toán để giảm tối đa việc sử dụng tài nguyên. Thứ hai, cho M3ViT đa tác vụ tiên tiến với MoE, chúng tôi đề xuất một phương pháp sắp xếp lại patch mới để hoàn toàn loại bỏ bất kỳ chi phí truy cập bộ nhớ nào. Thứ ba, chúng tôi thực hiện triển khai và đo lường trên bo mạch trên FPGA Xilinx ZCU102, với chức năng được xác minh và thiết kế phần cứng mã nguồn mở, đạt được hiệu suất năng lượng tốt hơn 2.24× và 4.90× so với GPU (A6000) và CPU (Xeon 6226R) tương ứng. Một cuộc biểu diễn video thời gian thực của ViT đa tác vụ được tăng tốc của chúng tôi trên tập dữ liệu lái xe tự động có sẵn trên GitHub,1 cùng với thiết kế FPGA của chúng tôi sử dụng High-Level Synthesis, mã host, bitstream FPGA và kết quả hiệu suất trên bo mạch.

I. GIỚI THIỆU
Vision Transformer (ViT) [2], [7], [18], [28] đã được ưa chuộng rộng rãi trong lĩnh vực thị giác máy tính nhờ hiệu suất ấn tượng. So với các mạng nơ-ron tích chập (CNN) sử dụng mảng pixel, ViT chia hình ảnh thành các patch có kích thước cố định và coi chúng như "token" trong NLP. Các embedding của token sẽ được học và đưa vào bộ mã hóa transformer cùng với embedding vị trí. ViT đã thể hiện hiệu suất vượt trội trong các tác vụ phát hiện đối tượng và phân đoạn ngữ nghĩa [18], [28].
1https://github.com/sharc-lab/Edge-MoE

Chuyển đổi tác vụ giữa ước lượng độ sâu và phân đoạn
Hình ảnh thô
Đầu ra
Hình ảnh
Hình 1. Demo triển khai trên bo mạch cho bộ tăng tốc MTL ViT của chúng tôi với chức năng và hiệu suất được đảm bảo. Toàn bộ M3ViT chạy trên bo mạch FPGA ZCU102; đầu ra được truyền tới laptop chỉ để hiển thị.

Trong khi đó, Multi-task Learning (MTL) là một kịch bản đầy hứa hẹn trong đó một thuật toán nhỏ gọn duy nhất có thể đồng thời học nhiều tác vụ khác nhau với kích thước mô hình nhỏ hơn nhiều so với học tác vụ đơn (STL) [31]. Ngoài ra, MTL có thể học một biểu diễn đặc trưng được cải thiện bằng cách chia sẻ biểu diễn và sử dụng regularization giữa các tác vụ liên quan [20], [34]. MTL quan trọng nhưng thách thức đối với các ứng dụng thực tế, đặc biệt khi mô hình sẽ được triển khai trong môi trường có khả năng tính toán hạn chế nhưng có yêu cầu thời gian thực. Ví dụ, lái xe tự động [13] yêu cầu nhiều tác vụ được thực hiện trên cùng một nền tảng, chẳng hạn như phát hiện làn đường, phát hiện người đi bộ, phân đoạn, v.v., trong đó MTL được kỳ vọng cung cấp hiệu suất thời gian thực cho mỗi tác vụ cũng như chuyển đổi tác vụ nhanh chóng. Do đó, MTL thời gian thực với chuyển đổi tác vụ nhanh chóng có nhu cầu rất lớn cho các hệ thống AI tương lai.

Mặc dù có rất nhiều nghiên cứu khám phá ViT và MTL riêng biệt, việc áp dụng MTL cho ViT cũng đã xuất hiện với kết quả hấp dẫn. Một loại kiến trúc MTL phổ biến [8], [17], [21], [25] sử dụng backbone chung với head độc lập cho từng tác vụ cụ thể, trong khi loại kiến trúc MTL khác [30], [32], [36], [37] đưa ra dự đoán tác vụ với bộ giải mã thống nhất và sau đó thực hiện những cải tiến thêm trên các dự đoán ban đầu. Tuy nhiên, những mô hình này thường có số lượng FLOP lớn [31] và không phù hợp cho các ứng dụng thời gian thực trên các hệ thống hạn chế tài nguyên hoặc nhạy cảm với độ trễ. Chen et. al [3] đề xuất một ViT đa tác vụ được huấn luyện trước để xử lý hình ảnh, bao gồm nhiều cặp head và tail cùng cấu trúc transformer được chia sẻ giữa các tác vụ, và Park et. al [22] đề xuất một vision transformer đa tác vụ cho chẩn đoán COVID-19 và định lượng mức độ nghiêm trọng. Một nghiên cứu gần đây nhất M3ViT [16] giới thiệu mixture-of-expert (MoE) vào MTL ViT. Trong khi các mô hình MTL hiện tại cần kích hoạt toàn bộ backbone vô điều kiện ngay cả khi chỉ cần một hoặc một vài tác vụ [8], [17], [21], [31], M3ViT có thể kích hoạt động và thưa thớt chỉ một phần nhỏ các expert, được chọn bởi một mạng gating, cho một tác vụ cụ thể. M3ViT không chỉ đạt được độ chính xác tối ưu trên các tập dữ liệu đa tác vụ mà còn giảm đáng kể kích thước mô hình hơn 80% so với các mô hình khác có độ chính xác tương tự, làm cho nó hấp dẫn cho việc triển khai phần cứng. Hình 2 minh họa sự khác biệt giữa học tác vụ đơn (STL), học đa tác vụ (MTL) và MTL+MoE [16].

Mặc dù có những tiến bộ trong MTL với kích thước mô hình nhỏ hơn và các expert được kích hoạt thưa thớt trong M3ViT, vẫn còn nhiều thách thức để đạt được suy luận thời gian thực cho các mô hình ViT đa tác vụ trên FPGA. Đầu tiên, các transformer, bao gồm cả mô hình ViT, được biết đến với độ phức tạp bậc hai của tính toán lớp tự chú ý [4], [7], [35]: với N token tổng cộng, mỗi token cần tính toán các yếu tố chú ý với tất cả N token bao gồm cả chính nó. Tính toán tự chú ý đã trở thành một nút thắt cổ chai chính ngay cả đối với các mô hình có kích thước nhỏ hơn như DeiT [29] và T2T-ViT [33]. Thứ hai, mặc dù M3ViT đề xuất MoE được kích hoạt thưa thớt với tính toán được giảm đáng kể, nhưng nó đưa ra những thách thức mới về truy cập bộ nhớ: vì các expert được xác định động trong quá trình thực hiện, người ta phải hoặc lưu trữ tất cả trọng số cho tất cả expert trên chip với chi phí bộ nhớ lớn, hoặc chỉ tải expert cần thiết với việc di chuyển dữ liệu off-chip nặng. Thứ ba, trong các mô hình Transformer và ViT, các phép toán softmax được sử dụng rộng rãi sau mỗi lớp attention (ngược lại với mạng tích chập trong đó softmax chỉ được sử dụng ở đầu ra cuối cùng), điều này đòi hỏi một lượng lớn tính toán phi tuyến không thân thiện với FPGA tiêu thụ một lượng lớn tài nguyên và thời gian thực hiện (chi tiết thêm trong Mục III-A2). Thứ tư, trong nhiều mô hình Transformer và ViT tiên tiến, một loại hàm kích hoạt mới, GELU (Gaussian Error Linear Unit) [11], đã được sử dụng rộng rãi để cải thiện hiệu quả và độ chính xác huấn luyện [1], [7], [9], [27], [29]; tuy nhiên, tính phi tuyến của nó đưa ra một chi phí tài nguyên lớn và sụt giảm độ chính xác rõ ràng trên FPGA (chi tiết thêm trong Mục III-A3).

Để giải quyết những thách thức này, chúng tôi đề xuất Edge-MoE, một bộ tăng tốc phần cứng với các kỹ thuật kiến trúc sáng tạo, để cung cấp hiệu suất thời gian thực cho mô hình ViT đa tác vụ M3ViT, mặc dù các kỹ thuật được đề xuất có thể áp dụng chung cho các mô hình Transformer/ViT tiêu chuẩn. Bảng I tóm tắt các kỹ thuật được đề xuất, các mô hình áp dụng và lợi ích. Chúng tôi tóm tắt những đóng góp như sau:

•Theo hiểu biết tốt nhất của chúng tôi, Edge-MoE là bộ tăng tốc từ đầu đến cuối đầu tiên cho Vision Transformer đa tác vụ, với triển khai và đo lường trên FPGA

BẢNG I
Danh sách các kỹ thuật đề xuất của chúng tôi và các mô hình áp dụng cùng lợi ích.
(Transf.: Transformer; ViT: Vision Transformer; M3ViT: ViT đa tác vụ với mixture-of-expert; Lat.: Latency; Res: Resource)

Các Kỹ thuật Đề xuất | Transf. | ViT | M3ViT | Lợi ích
➊Sắp xếp lại attention | ✔ | ✔ | ✔ | Lat.
➋Xấp xỉ softmax | ✔ | ✔ | ✔ | Res., Lat.
➌Xấp xỉ GELU | ✔ | ✔ | ✔ | Res.
➍Đơn vị tính toán thống nhất | ✔ | ✔ | ✔ | Res., Lat.
➎Sắp xếp lại patch (MTL) | – | – | ✔ | Res., Lat.

, chức năng được xác minh và thiết kế phần cứng mã nguồn mở sử dụng High-Level Synthesis (HLS). Hình 1 mô tả triển khai trên bo mạch của chúng tôi với hiệu suất thời gian thực trên tập dữ liệu lái xe tự động; một video clip đầy đủ có sẵn trên GitHub.2

•Đối với các mô hình Transformer/ViT nói chung với những thách thức phổ biến (ví dụ: tự chú ý nặng, softmax và GELU), chúng tôi đề xuất một bộ sưu tập các kỹ thuật sáng tạo, bao gồm: ➊một cơ chế sắp xếp lại attention mới để giảm băng thông yêu cầu từ tỷ lệ thuận xuống hằng số; ➋một xấp xỉ softmax một lần qua để đạt được cả độ chính xác cao và tốc độ tính toán nhanh; ➌một xấp xỉ GELU chính xác và chi phí thấp với tài nguyên phần cứng cực thấp; và ➍một đơn vị tính toán thống nhất và linh hoạt có thể được chia sẻ bởi hầu hết tất cả các lớp tuyến tính, giảm drastically việc sử dụng tài nguyên và do đó dẫn đến tăng tốc đáng kể. Các kỹ thuật được đề xuất có thể được áp dụng trực tiếp cho bất kỳ mô hình Transformer/ViT nào để giảm tài nguyên và độ trễ.

•Đối với mô hình ViT đa tác vụ tiên tiến với mixture-of-expert, M3ViT [16], chúng tôi đề xuất ➎một cơ chế sắp xếp lại patch mới để loại bỏ chi phí bộ nhớ từ di chuyển dữ liệu off-chip, đạt được chi phí bằng không cho việc chuyển đổi expert và chuyển đổi tác vụ. Bằng cách triển khai M3ViT trên một FPGA duy nhất, chúng tôi chứng minh rằng rất có thể các ViT đa tác vụ tiết kiệm năng lượng và thời gian thực chạy trên các thiết bị edge.

•Vì M3ViT là mô hình ViT tiên tiến nhất có tất cả các thành phần trong các mô hình Transformer và ViT nói chung và cũng được trang bị MTL và MoE tiên tiến, chúng tôi sử dụng nó làm nghiên cứu trường hợp của chúng tôi mà không mất tính tổng quát. Bộ tăng tốc của chúng tôi đạt được gần 30 khung hình mỗi giây, hiệu suất năng lượng tốt hơn 2.24× so với GPU (RTX A6000),

2https://github.com/sharc-lab/Edge-MoE/raw/main/demo.mp4

--- TRANG 2 ---
Đầu vào
Học Tác vụ Đơn (STL)
Đầu vào
Backbone 1
Backbone 2
Backbone 3
Backbone 4
Tác vụ 1
Tác vụ 2
Tác vụ 3
Tác vụ 4
Học Đa Tác vụ (MTL)
Backbone Chung
Tác vụ 1
Tác vụ 2
Tác vụ 3
Tác vụ 4
Học Đa Tác vụ + Mixture-of-Expert (MoE)
Đầu vào
Expert 1.1
Tác vụ 1
Tác vụ 2
Tác vụ 3
Tác vụ 4
Expert 1.2
Expert 1.3
Expert 1.4
Expert 2.1
Expert 2.2
Expert 2.3
Expert 2.4
Expert 3.1
Expert 3.2
Expert 3.3
Expert 3.4
Chạy một tác vụ đơn chỉ có thể kích hoạt một backbone
Độ chính xác thường thấp hơn so với MTL
Độ chính xác cao hơn so với STL
Chạy chỉ một tác vụ cần kích hoạt toàn bộ backbone chung
Độ chính xác cao hơn so với STL
Kích hoạt backbone thưa thớt nếu không cần tất cả tác vụ
Trọng số cần thiết cho expert thay đổi động; có thể gây ra chi phí tải lớn → được giải quyết trong nghiên cứu này
(a) (b) (c)

Hình 2. Ưu và nhược điểm của học tác vụ đơn (STL), học đa tác vụ (MTL) và học đa tác vụ với mixture-of-expert (MoE). Ưu điểm đáng kể nhất của MTL-MoE là backbone được kích hoạt thưa thớt, tiết kiệm cả tính toán và footprint bộ nhớ. Thách thức của MTL-MoE là việc kích hoạt "expert" là động, phụ thuộc vào khung hình ảnh hiện tại. Do đó, mặc dù nó tiết kiệm đáng kể tính toán và bộ nhớ, nó có thể gây ra chi phí tải trọng số lớn và triệt tiêu lợi ích. Trong nghiên cứu này, chúng tôi đề xuất một kiến trúc phần cứng mới cho MTL-MoE hiệu quả với chi phí bằng không.

và tốt hơn 4.90× so với CPU (Xeon 6226R). Kết quả được đo trên bo mạch đánh giá FPGA Xilinx ZCU102 dưới tần số 300 MHz.

•Các kỹ thuật đề xuất của chúng tôi không chỉ dành riêng cho FPGA mà có thể áp dụng cho thiết kế ASIC. Chúng tôi chỉ sử dụng FPGA để đánh giá cụ thể.

II. KIẾN THỨC NỀN TẢNG VÀ NGHIÊN CỨU LIÊN QUAN
A. Vision Transformer và M3ViT
Vision Transformer (ViT) được đề xuất đầu tiên bởi Dosovitskiy et. al [7] bằng cách điều chỉnh Transformer trong Xử lý Ngôn ngữ Tự nhiên (NLP) để xử lý hình ảnh cho các tác vụ thị giác máy tính. Tương tự như "token" trong Transformer, mỗi hình ảnh đầu tiên được chia thành "patch", trong đó mỗi patch có kích thước P×P và có P² pixel. Sau đó các patch sẽ được làm phẳng thành vector, chiếu thành embedding tuyến tính và đưa vào bộ mã hóa transformer tiêu chuẩn theo thứ tự tuần tự, thường với embedding vị trí. Mỗi khối của bộ mã hóa thường bao gồm một lớp tự chú ý, các lớp chuẩn hóa, một lớp multi-layer perceptron (MLP) và các lớp kích hoạt. Trong lớp tự chú ý (khối màu xanh trong Hình 3 trái), chúng tôi ký hiệu embedding patch đầu vào bằng X∈RL×d trong đó d là chiều embedding patch và L là số lượng patch. Ba ma trận Q, K và V có thể được tính như sau:
Q=WQX, K=WKX, V=WVX, trong đó WQ, WK và WV là trọng số. Cuối cùng, đầu ra Y=softmax(QKT)V∈RL×d. Tự chú ý này có độ phức tạp bậc hai trong softmax, trong đó mỗi patch cần tính toán điểm chú ý của nó với tất cả n patch.

Trên cơ sở ViT, M3ViT [16] là mô hình ViT đa tác vụ tiên tiến nhất, đổi mới giới thiệu cơ chế mixture-of-expert (MoE) để kích hoạt thưa thớt chỉ một phần nhỏ của mô hình cho một tác vụ nhất định, nhằm giảm độ phức tạp tính toán. Hình 3 trái minh họa cấu trúc mô hình của M3ViT. Theo kiến trúc ViT chung, M3ViT bao gồm một module patch embedding tiếp theo là mười hai khối lặp lại. Bên trong mỗi khối sau tự chú ý, tuy nhiên, có hai lựa chọn: các khối chẵn sử dụng khối ViT truyền thống (khối màu vàng trong Hình 3 trái), trong khi các khối lẻ sử dụng khối MoE (khối màu hồng). Một khối ViT truyền thống bao gồm hai lớp kết nối đầy đủ với hàm kích hoạt GELU [11]. Một khối MoE là một tập hợp của m "expert", trong đó mỗi expert là một MLP nhỏ hơn. Đầu ra kết quả của một lớp MoE là tổng của k expert hàng đầu được chọn từ m ứng viên expert sử dụng mạng gating đặc trưng cho tác vụ. Ví dụ, trong ví dụ được hiển thị trong hình, có hai mạng gating cho hai tác vụ. Khi tác vụ A đang được thực hiện, nó kích hoạt expert 1 và m−1, trong khi các expert khác không được tính toán; khi tác vụ B đang được thực hiện, nó kích hoạt expert 1 và m và những expert khác bị vô hiệu hóa. Ưu điểm của phương pháp MoE này là kích hoạt thưa thớt các expert để giảm đáng kể tính toán. Cụ thể, trong suy luận mô hình đa tác vụ thực tế, không phải tất cả các tác vụ đều luôn cần thiết. Với MoE, chỉ một phần nhỏ các expert liên quan đến tác vụ được chọn để tính toán. Tuy nhiên, không có MoE, toàn bộ mô hình phải luôn được tính toán.

B. Bộ tăng tốc ViT hiện có trên FPGA
Một số nghiên cứu trước đây nhắm đến việc tăng tốc các mô hình dựa trên Transformer trên FPGA, như VAQF [26], SPViT [12], FTRANS [14], Qi et al. [24], Peng et al. [23] và Auto-ViT-Acc [15]. Những nghiên cứu này lưu ý rằng các mô hình Transformer tốn nhiều tính toán và bộ nhớ và quá lớn để vừa với một FPGA. Một số giải pháp áp dụng các kỹ thuật nén mô hình có tổn thất khác nhau, chẳng hạn như lượng tử hóa kích hoạt, cắt tỉa token, ma trận khối tuần hoàn (BCM) cho trọng số, cắt tỉa trọng số cân bằng khối và cắt tỉa trọng số khối cân bằng cột. Tất cả những điều này đều yêu cầu sử dụng huấn luyện nhận thức nén để tránh sụt giảm độ chính xác mô hình. Ví dụ, VAQF là thuật toán lượng tử hóa bit thấp cho ViT và sử dụng 1-bit cho kích hoạt và 6-bit cho trọng số, trong khi Auto-ViT-Acc là khung lượng tử hóa lai có thể tự động tìm kiếm sơ đồ lượng tử hóa tốt nhất. Giải quyết việc tăng tốc ViT từ các góc độ khác nhau, những đổi mới phần cứng của chúng tôi không dựa vào nén mô hình hoặc huấn luyện lại và thực tế là trực giao với nhiều kỹ thuật nén được đề xuất trong các nghiên cứu trước, có thể được áp dụng cùng với các kỹ thuật đề xuất của chúng tôi.

Ngoài ra, thay vì tập trung vào toàn bộ mô hình Transformer/ViT, Zhang et al. [35] đề xuất một bộ tăng tốc tự chú ý dựa trên FPGA bằng cách cắt tỉa trọng số, trong khi Lu et. al [19] đề xuất một thiết kế dựa trên mảng systolic cho triển khai lớp attention. Ngược lại, chúng tôi đề xuất một triển khai mô hình ViT hoàn chỉnh từ đầu đến cuối, điều này phơi bày những thách thức bổ sung bị che giấu bởi việc chỉ nhìn vào một phần của mô hình.

Đối với học đa tác vụ, M3ViT tiên tiến [16] xuất sắc trong cả độ chính xác thuật toán cao và độ phức tạp tính toán thấp, nhờ vào cơ chế MoE. Nó gợi ý một phương pháp thân thiện với phần cứng cho tính toán MoE, nhưng không may, nó chỉ cung cấp một ước lượng lạc quan sử dụng FPGA mà không có bất kỳ triển khai phần cứng thực tế nào. Do đó, nghiên cứu của chúng tôi là, theo hiểu biết tốt nhất của chúng tôi, bộ tăng tốc FPGA đầu tiên cho mô hình ViT đa tác vụ với MoE.

III. THÁCH THỨC
Mặc dù có những ưu điểm thuật toán của các mô hình ViT và tính thưa thớt tính toán của M3ViT cho học đa tác vụ, vẫn còn những thách thức nghiêm trọng để thực sự triển khai các mô hình ViT và M3ViT trên FPGA để đạt được hiệu suất thời gian thực, trong đó nhiều thách thức đang bị bỏ qua bởi các nghiên cứu hiện tại. Chúng tôi nêu bật một số từ hai góc độ: những thách thức chung cho hầu hết các mô hình Transformer/ViT, và những thách thức cụ thể cho M3ViT đa tác vụ.

A. Thách thức Chung cho Transformer/ViT
1) Yêu cầu Băng thông Cao cho Tự chú ý: Đã được biết rằng tự chú ý tốn nhiều tính toán [19], [35] và bị hạn chế băng thông. Không lặp lại các phát biểu trước đây, chúng tôi nhấn mạnh rằng, việc đạt được độ trễ thấp đòi hỏi băng thông cực cao hoặc bộ nhớ on-chip lớn để đệm càng nhiều trọng số càng tốt. Cả hai đều không thực tế đối với các thiết bị edge với tài nguyên hạn chế.

2) Các Phép toán Softmax Lớn với Tràn số Không Mong muốn: Các kiến trúc Transformer, bao gồm M3ViT, sử dụng rộng rãi các toán tử softmax như một phần của cơ chế attention để tính toán điểm chú ý cho mỗi cặp token. Ngoài ra, trên cơ sở Transformer truyền thống, M3ViT cũng yêu cầu softmax để tính toán trọng số expert tương đối từ mạng gating cho mỗi token trong khối Mixture-of-Experts. Hàm mũ phi tuyến bên trong softmax cực kỳ không thân thiện với phần cứng (và không may được sử dụng rất nhiều trong Transformer/ViT): tính toán độ chính xác thấp có thể gây ra lỗi lớn, trong khi tính toán độ chính xác cao tiêu thụ một lượng đáng kể tài nguyên và độ trễ.

Hơn nữa, tràn số trong hàm mũ dẫn đến lỗi thảm khốc trong kết quả softmax. Vì điểm softmax cho mỗi phần tử phụ thuộc vào tổng của exp(·) cho mỗi phần tử khác, việc tràn số đơn lẻ thường dẫn đến lỗi lan rộng lớn, được truyền tiếp qua các lần lặp tiếp theo của cơ chế attention của Transformer. Kết quả là, độ chính xác tác vụ hạ lưu giảm xuống gần như bằng không.

Vì các thuật ngữ mũ exp(·) phải được tính toán nhiều lần cho softmax exp(xi)/∑j=1^N exp(xj), việc sử dụng datatype điểm cố định trên FPGA là tốt hơn. Tuy nhiên, vì giá trị của nó tăng theo cấp số nhân, nó có thể dễ dàng tràn phạm vi biểu diễn của nó ngay cả khi đối số nhỏ và gây ra lỗi tính toán lớn. Ví dụ,

--- TRANG 3 ---
ViT Đa tác vụ sử dụng Mixture-of-Expert (MoE) | Kiến trúc Bộ tăng tốc Được đề xuất
Khối M³ViT (Lặp lại 12×) | Đầu vào | Patch Embedding

Hỗ trợ các cấu hình khác nhau tại runtime để tính toán các phần khác nhau của mô hình

Tính toán Patch Embeddings | Layer Norm | Khối ViT | Layer Norm | Tự chú ý | Khối MoE | Khối chẵn | Khối lẻ | Đầu ra

Tự chú ý | Tính toán Q, K, V | M = Q x K | Softmax | M' x V | Chiếu Tuyến tính

Lớp Kết nối Đầy đủ 1 | GELU | Lớp Kết nối Đầy đủ 2

GELU | Khối MoE | Khối ViT

Module Tuyến tính Thống nhất và Linh hoạt
Chiều Đầu vào | Chiều Đầu ra | Có hoặc không có GELU? | Expert thưa thớt hoặc tuyến tính dày đặc? | Tùy chọn cấu hình

Mạng Gating MoE
Thực hiện trên | Thực hiện trên | Thực hiện trên

Mạng Gating Tác vụ A | Mạng Gating Tác vụ B | Expert m-1 | Expert m | Layer Norm | Lớp MoE 1 | Lớp MoE 2 | Expert 1 | Expert 2 | Layer Norm | Cộng Ma trận

Nhân Ma trận + Xấp xỉ Softmax | Sắp xếp lại & Nhóm Patch | Bộ đệm Trọng số Expert | Patch [1, 3, 8]: Expert 1 | Patch [2, 7]: Expert 4 | Patch [6, 9, 10, 11]: Expert 7 | ... | Lấy Trọng số Expert | Trọng số expert 1 | Nhân Vector Matrix | Xấp xỉ GELU | Sắp xếp lại Attention 1 | 2 | 3 | 5 | 4

Hình 3. Trái: kiến trúc M3ViT đa tác vụ với mixture-of-expert (MoE) [16]. Nó bao gồm một module patch embedding tiếp theo là 12 khối, mỗi khối chứa một module tự chú ý tiếp theo là một khối ViT (trên các khối số chẵn) hoặc một khối MoE (trên các khối số lẻ). Phải: kiến trúc FPGA được đề xuất của chúng tôi với các kỹ thuật mới, được đánh dấu bằng ➊∼➎. ➊∼➍ áp dụng được cho các mô hình Transformer/ViT nói chung; ➎ chuyên dụng cho MoE đa tác vụ bên trong M3ViT.

Động cơ tuyến tính | Các đơn vị phụ trợ | Động cơ attention | Logic điều khiển cấp cao

Đơn vị patch embedding | BRAM trọng số Embedding | Loader | Đơn vị LayerNorm | BRAM trọng số LayerNorm | Loader | Đơn vị gating MoE | BRAM trọng số gating MoE | Loader | Đơn vị cộng | DRAM | Đơn vị M × V | Đơn vị lớp tuyến tính thống nhất | BRAM trọng số lớp tuyến tính | Loader | Đơn vị Q × K | Đơn vị Softmax

Hình 4. Kiến trúc hệ thống của Edge-MoE.

exp(7) ≈ 1096.63, đã quá lớn để biểu diễn trong bất kỳ datatype điểm cố định có dấu nào với ít hơn 12 bit nguyên.

3) Chi phí Phần cứng Đắt đỏ của Kích hoạt GELU: Các mô hình Transformer và ViT hiện đại, bao gồm M3ViT, sử dụng hàm kích hoạt GELU (Gaussian Error Linear Unit) [11] cho các lớp kết nối đầy đủ và MLP thay vì ReLU. GELU có thể được coi là ReLU mượt mà hơn nhưng có tính biểu đạt cao hơn vì tính phi tuyến tốt hơn và dẫn đến hội tụ nhanh hơn và tốt hơn của mạng nơ-ron [6], [7], [16], [18]. GELU(x) được tính như sau:

GELU(x) = xΦ(x) = x·0.5(1 + erf(x/√2)) (1)

trong đó Φ(x) là hàm phân phối tích lũy Gaussian tiêu chuẩn và erf(·) là hàm lỗi Gauss.

Việc triển khai kích hoạt GELU sử dụng Phương trình (1) cực kỳ đắt trên FPGA: nó yêu cầu 161.8k LUT (59% số LUT có sẵn trên ZCU102) chỉ cho một instance duy nhất sử dụng datatype điểm cố định 32-bit. Do đó, người ta phải sử dụng xấp xỉ cho GELU.

Với sự tương đồng của erf(·) và tanh(·), được gợi ý rằng GELU có thể được xấp xỉ sử dụng tanh(·) như sau [11]:

GELU(x) ≈ 0.5x(1 + tanh(√(2/π)(x + 0.044715x³))) (2)

Tuy nhiên, điều này vẫn tiêu thụ tài nguyên phần cứng đáng kể: chỉ một instance tiêu thụ 18.7k LUT (6% LUT có sẵn trên ZCU102), trong khi có hàng ngàn instance dự kiến được tính toán song song. Cùng một nghiên cứu cũng gợi ý một xấp xỉ sử dụng hàm sigmoid, tiêu thụ ít tài nguyên hơn nhiều, 4.7k LUT (1%) cho một instance, nhưng ít chính xác hơn đáng kể.

Do đó, thật thách thức để duy trì cả độ chính xác xấp xỉ cao và độ phức tạp tính toán thấp cùng tài nguyên phần cứng trong tính toán hàm kích hoạt GELU.

4) Trùng lặp Logic Lớp Tuyến tính: Các mô hình Transformer và ViT sử dụng rộng rãi các lớp tuyến tính kết nối đầy đủ, bao gồm trong MLP, patch embedding cho tự chú ý và chiếu tuyến tính ở cuối tự chú ý. Tạo một module phần cứng riêng cho mỗi phép toán tuyến tính tiêu thụ quá nhiều tài nguyên và giới hạn tính song song, và do đó cần chia sẻ tài nguyên tích cực giữa các lớp tuyến tính trong các module khối khác nhau. Tuy nhiên, mặc dù bản chất tính toán của các lớp tuyến tính giống nhau, cấu trúc của chúng khác nhau khi xuất hiện trong các khối khác nhau. Các biến thể bao gồm: chiều đầu vào và đầu ra, địa chỉ lấy và lưu trữ trọng số, cơ chế song song dữ liệu ưa thích (phân vùng), và có sử dụng hàm kích hoạt hay không.

Mô hình M3ViT làm trầm trọng thêm thách thức bằng cách giới thiệu một loại khối khác: khối MoE ngoài khối ViT truyền thống (xem Hình 3 trái). Các khối ViT sử dụng MLP với chiều ẩn lớn hơn so với các MLP được sử dụng trong các khối MoE. Hơn nữa, do bản chất của tính toán mixture-of-experts, các MLP expert không xử lý tất cả token; thay vào đó, chúng phải chấp nhận đầu vào thưa thớt chỉ của một tập con các token đầu vào.

Do đó, thiết kế một đơn vị tính toán thống nhất để phù hợp với tất cả các loại lớp tuyến tính để khuyến khích chia sẻ tài nguyên và do đó cho phép tính song song lớn hơn là một thách thức quan trọng.

B. Thách thức Cụ thể cho Mô hình MTL M3ViT
1) Truy cập Bộ nhớ cho Mixture-of-Experts: Một đặc điểm xác định của thuật toán M3ViT là việc sử dụng mixture-of-experts (MoE) để làm thưa thớt mô hình ViT lớn bằng cách chọn, cho mỗi token đầu vào, một tập hợp "expert" khác nhau để sử dụng để tính toán biểu diễn đầu ra của nó. Một mạng gating được sử dụng để chấm điểm mỗi m expert đối với mỗi token, và các expert có k điểm cao nhất được sử dụng cho đầu ra của token đó. Hình 9(a–b) mô tả cách hình ảnh đầu vào được chia thành các patch và cách mỗi patch chọn một tập con expert cho tính toán của nó.

Sau khi tính toán mạng gating, một phương pháp tự nhiên để thực hiện tính toán MoE tiếp theo là xem nó tương tự như bất kỳ MLP nào khác: đầu tiên, tải tất cả trọng số cho tất cả expert vào BRAM on-chip. Sau đó, tính toán đầu ra của mỗi token rất đơn giản bằng cách cung cấp trọng số của mỗi k expert được chọn từ BRAM làm đầu vào trọng số cho MLP. Tuy nhiên, phương pháp này yêu cầu tải tất cả m expert trên chip. Trong M3ViT, m = 16, và toàn bộ trọng số expert không thể vừa với các tài nguyên BRAM có sẵn.

Một phương pháp thay thế là tính toán các patch từng cái một và lấy mỗi expert chỉ khi cần thiết, như được hiển thị trong Hình 9(c). Ví dụ, khi tính toán patch 1, vì các expert được chọn của nó là 1 và 4, trọng số expert 1 được tải đầu tiên, tiếp theo là expert 4; tiếp theo, khi tính toán patch 2, trọng số expert 1 phải được tải lại vì chúng đã bị hoán đổi ra. Điều này tránh được vấn đề BRAM hạn chế, nhưng nó gây ra độ trễ bộ nhớ nghiêm trọng, vì trọng số expert phải được tải lại liên tục.

IV. CÁC PHƯƠNG PHÁP ĐỀ XUẤT
Trong phần này, chúng tôi mô tả các giải pháp mới của chúng tôi để giảm bớt những thách thức đã đề cập trước đó. Kiến trúc tổng thể được đề xuất được trình bày trong Hình 3 với các kỹ thuật mới được đề xuất của chúng tôi, được đánh dấu từ ➊ đến ➎, trong đó ➊∼➍ có thể áp dụng chung cho các mô hình Transformer và ViT tiêu chuẩn, và ➎ cụ thể cho M3ViT đa tác vụ với MoE. Hình 4 cho thấy một góc nhìn khác, bao gồm hệ thống phân cấp bộ nhớ.

Thiết kế được đề xuất là một bộ tăng tốc từ đầu đến cuối hoàn chỉnh bao gồm các thành phần chính và kỹ thuật chính sau:
• Tính toán patch embedding ban đầu;
• Một mạng gating đặc trưng cho tác vụ để tạo ra sự lựa chọn expert;
• Sắp xếp lại và nhóm patch để hợp nhất việc tải trọng số expert và loại bỏ chi phí truy cập bộ nhớ (➎ trong Mục IV-D);
• Một module tuyến tính thống nhất và linh hoạt hợp nhất một phần lớn các lớp tuyến tính trên toàn bộ mô hình (➍ trong Mục IV-E);
• Một module sắp xếp lại attention hiệu quả cho tính toán tự chú ý với truy cập bộ nhớ được giảm đáng kể và tính song song tăng, vượt qua nút thắt cổ chai băng thông (➊ trong Mục IV-A);
• Một module xấp xỉ softmax một lần qua, chính xác và chi phí thấp được sử dụng cho tự chú ý (➋ trong Mục IV-B);
• Một module xấp xỉ GELU chính xác và chi phí thấp được tích hợp vào module tuyến tính thống nhất (➌ trong Mục IV-C);

Lưu ý rằng mặc dù bộ tăng tốc được mô tả là cho M3ViT, nó có thể dễ dàng áp dụng cho các mô hình Transformer và ViT tiêu chuẩn chỉ bằng cách loại bỏ lớp MoE và module sắp xếp lại patch. Trong các thí nghiệm, chúng tôi cũng sẽ đánh giá hiệu suất của nó trên các mô hình ViT tiêu chuẩn. Trong các phần sau, chúng tôi thảo luận về các kỹ thuật chính được đề xuất của chúng tôi.

A. Sắp xếp lại Attention để Giảm Băng thông
Không sắp xếp lại. Cơ chế attention trong các mô hình Transformer cũng như M3ViT tốn nhiều tính toán và bị hạn chế băng thông. Hình 5 (trên) minh họa sự hạn chế băng thông bằng cách hiển thị luồng tính toán của Qi × Kj cho tất cả các token, trong đó 1 ≤ i ≤ N, 1 ≤ j ≤ N, và N là tổng số token. Để hoàn thành tính toán như vậy cho tất cả các token, cần N² cặp nhân vector

Được sắp xếp lại để giảm thiểu băng thông DRAM
✓ Truyền (N²/4 + N + 3) khối dữ liệu từ DRAM
✓ Không yêu cầu băng thông bổ sung cho bộ nhớ K

Nhân ma trận naiv cho song song 4×
✗ Truyền (N² + N) khối dữ liệu từ DRAM
✗ Yêu cầu băng thông 4× cho bộ nhớ K

Q1 Load
1
2 Iter.
Q2
3
4
5
⋮
Q3
Q4
N
Q5 N+1
Q6 N+2
Q7 N+3
Q8 N+4
⋮
N²/4
N²/4+1
N²/4+2
N²/4+3
N+5

Q1 Load
1
2 Iter.
⋮
N/4
N/4+1
N/4+2
⋮
N²/4

Q2
Load
K1
K2
K3
K4
K5
K N
K1
K2
K3
K4
⋮
K N
K1
K2
K3
⋮
K5 Load ×4
K1…4
K5…8
⋮
K N−3… N
K1…4
K5…8
⋮
K N−3… N

Compute
Q1 × K1
Q1 × K2
Q1 × K3
Q1 × K4
Q1 × K5
Q1 × K N
Q5 × K1
⋮
Q5 × K2
Q5 × K3
Q5 × K4
Q N−3 × K N
⋮
Q5 × K5

Compute
Q1 × K1
Q1 × K5
⋮
Q1 × K N−3
Q2 × K1
Q2 × K5
⋮
Q N × K N−3

Compute
Q2 × K2
Q2 × K3
Q2 × K4
Q2 × K5
Q2 × K N
Q2 × K1
⋮
Q6 × K2
Q6 × K3
Q6 × K4
Q N−2 × K N
Q N−2 × K1
⋮
Q6 × K5

Q1 × K2
Compute
Q1 × K6
⋮
Q1 × K N−2
Q2 × K2
Q2 × K6
⋮
Q N × K N−2

Compute
Q3 × K3
Q3 × K4
Q3 × K5
Q3 × K N
Q3 × K1
⋮
Q3 × K2
Q7 × K3
Q7 × K4
Q N−1 × K N
Q N−1 × K1
⋮
Q N−1 × K2
Q7 × K5

Compute
Q1 × K3
Q1 × K7
⋮
Q1 × K N−1
Q2 × K3
Q2 × K7
⋮
Q N × K N−1

Compute
Q4 × K4
Q4 × K5
Q4 × K N
Q4 × K1
⋮
Q4 × K2
Q4 × K3
Q8 × K4
Q N × K N
Q N × K1
⋮
Q N × K2
Q N × K3
Q8 × K5

Compute
Q1 × K4
Q1 × K8
⋮
Q1 × K N
Q2 × K4
Q2 × K8
⋮
Q N × K N

Hình 5. Sự sắp xếp lại tính toán được đề xuất cho tính song song trong cơ chế tự chú ý.

Sắp xếp lại được đề xuất của chúng tôi. Để giải quyết thách thức này, chúng tôi giới thiệu một chiến lược sắp xếp lại mới, được hiển thị trong Hình 5 (dưới), đạt được

--- TRANG 4 ---
BẢNG II
Lượng tải dữ liệu, độ trễ, yêu cầu bộ nhớ và yêu cầu băng thông có và không có sắp xếp lại attention được đề xuất của chúng tôi dưới tính song song p.

Phương pháp | Tải Dữ liệu | Độ trễ | Băng thông | Bộ nhớ
Không sắp xếp lại | N²+N | N²/p | ∼p | p+1
Có sắp xếp lại | N²/p+N+p−1 | N²/p+p−1 | ∼1 | p+1

tính song song tùy ý với băng thông hằng số cho các ma trận đầu vào. Cụ thể, cho một hệ số song song mong muốn p (4 trong hình), chúng tôi đầu tiên cache một lô p token của Qi trong bộ đệm cục bộ (ví dụ: Q1 đến Q4 trong lô đầu tiên); sau đó, trong mỗi lần lặp, chúng tôi tải một token Kj mới và nhân nó với p token của Q trong bộ đệm cục bộ. Bộ nhớ cần thiết là p bộ đệm cho token Q và 1 bộ đệm cho token K, cũng tổng cộng p+1 bộ đệm.

Lưu ý rằng một số token Q nhất định không được căn chỉnh với đầu ma trận K, chẳng hạn như Q2 trong hình. Do đó, một số đầu ra ban đầu bị "thiếu", như Q2×K1, vì Q2 không có trong bộ đệm trong lần lặp mà K1 được lấy từ DRAM. Những đầu ra "thiếu" này được xem xét lại sau khi phần còn lại của ma trận K được xử lý, trong khi lô Q tiếp theo đang được đọc. Thuận tiện, đầu ra cuối cùng cho mỗi token được tính toán ngay trước khi một token mới được đọc vào bộ đệm, do đó không gây ra thời gian nhàn rỗi giữa các lô.

Ở cuối quá trình, khi tất cả Q đã được đọc, tối đa p−1 lần lặp được thêm vào để xử lý bất kỳ đầu ra "thiếu" còn lại trong lô cuối cùng.

Vì Q được xử lý theo từng lô p token tại một thời điểm, mỗi token của K được tái sử dụng trên p token, có nghĩa là toàn bộ ma trận K chỉ được đọc N/p lần. Điều này dẫn đến việc truyền N²/p khối của ma trận K, cộng với tối đa p−1 khối được truyền thêm ở cuối để tính cho các đầu ra thiếu. Ma trận Q vẫn chỉ được đọc một lần, dẫn đến tổng cộng N²/p+N+p−1 khối dữ liệu được truyền. Với tổng độ trễ N²/p+p−1, băng thông yêu cầu là khoảng 1 bất kể tính song song p. Việc tái sử dụng ma trận K này cho p khối Q cũng chính xác là lý do tại sao chiến lược cơ bản cần p lần băng thông cho K để đạt được cùng một mức độ song song.

Bảng II tóm tắt hiệu suất không có và có sắp xếp lại được đề xuất của chúng tôi với tính song song p, bao gồm tổng số tải dữ liệu, độ trễ, băng thông yêu cầu và bộ nhớ on-chip. Rõ ràng, không có sắp xếp lại, yêu cầu băng thông tỷ lệ thuận với tính song song, trong khi cơ chế sắp xếp lại được đề xuất của chúng tôi giảm băng thông xuống một hằng số.

Mặc dù chúng tôi chỉ mô tả tối ưu hóa trên Q×K, nó có thể được áp dụng tương tự cho M′×V theo chiều ngược lại: thay vì tạo ra tối đa bốn phần tử của Q×K trong mỗi lần lặp, như được hiển thị trong Hình 5 (dưới), bốn phần tử tại các chỉ số đó được tải mỗi lần lặp. Sau đó chúng được xử lý bằng softmax, nhân với ma trận V (được tải giống như K trong Hình 5 dưới), và tích lũy vào một cache của bốn token. Sau mỗi N lần lặp, thay vì tải bốn token của Q, nội dung của cache bốn token được ghi lại vào DRAM như là kết quả cuối cùng của bốn token trong tính toán M′×V.

B. Tích lũy Softmax Một lần qua
1) Bias động cho bù trừ độ chính xác: Như đã thảo luận trong Mục III-A2, thách thức lớn nhất của tính toán softmax trên phần cứng là tràn của thuật ngữ mũ exp(x), làm ảnh hưởng lớn đến độ chính xác mô hình vì softmax được sử dụng rộng rãi trong các mô hình Transformer và ViT.

Để giải quyết vấn đề tràn để bù trừ mất độ chính xác, chúng tôi đề xuất một bias bù trừ, ký hiệu là b, để điều chỉnh lại phạm vi đầu ra cho exp(xi) và exp(xj) khi tính toán exp(xi)/∑j=1^N exp(xj).

Cụ thể, chúng tôi trừ b khỏi đối số của cả hai hàm mũ exp(xi) và exp(xj) để giảm độ lớn của các thuật ngữ mũ; nhờ vào tính chất cộng của hàm mũ, việc bù trừ không thay đổi kết quả softmax:

exp(xi−b)/∑j=1^N exp(xj−b) = exp(−b)exp(xi)/(exp(−b)∑j=1^N exp(xj)) = exp(xi)/∑j=1^N exp(xj) (3)

Thách thức là, tuy nhiên, việc chọn giá trị bias b lớn hơn có thể làm cho thuật ngữ mũ exp(x−b) ít có khả năng tràn hơn, nhưng nó cũng gây ra mất độ chính xác. Nếu x−b giảm quá nhiều, giá trị exp(x−b) có thể quá nhỏ để được biểu diễn chính xác bởi các số điểm cố định. Hình 6 minh họa sự đánh đổi này: khi đầu vào x nhỏ, b lớn hơn sẽ làm cho giá trị exp(x−b) làm tròn về không; khi x lớn, b nhỏ hơn sẽ dẫn đến tràn. Do đó, bias tối ưu b phụ thuộc rất nhiều vào giá trị đầu vào x, và do đó phương pháp "một kích cỡ phù hợp tất cả" sử dụng bias b được xác định trước không khả thi cho datatype điểm cố định cho tính toán softmax.

−4 −2 0 2 4
−20
−10
0
10
Tràn
Làm tròn về không
Phạm vi sử dụng được bị hạn chế
bởi độ chính xác và phạm vi của
datatype, bất kể bias b
Bias b
Đầu vào x

Hình 6. Phạm vi sử dụng được (được đánh dấu màu xanh) của hàm exp(x−b) cho datatype điểm cố định có dấu 32-bit với 10 bit nguyên cho các giá trị khác nhau của bias b. b quá lớn sẽ dẫn đến làm tròn về không, trong khi b quá nhỏ sẽ dẫn đến tràn; giá trị b tối ưu phụ thuộc vào đầu vào x.

Do đó, chúng tôi đề xuất động quyết định giá trị bias b cho mỗi token (với embedding xi), trong đó b = maxj∈{1,...,N}(xj) đảm bảo rằng exp(xj−b) ≤ 1 cho tất cả j ∈ {1,...,N}, ngăn chặn tràn đồng thời cũng bảo toàn độ chính xác. Với bias động này, ngay cả khi một thuật ngữ cụ thể exp(xj−b) gặp mất độ chính xác hoặc làm tròn về không, độ chính xác vẫn được duy trì. Lý do là, ngay cả khi exp(xj−b) làm tròn về không, điều đó ngụ ý rằng xj nhỏ hơn nhiều so với max(xj), và do đó đóng góp của nó vào tính toán softmax tổng thể là không đáng kể và có thể bị bỏ qua mà không mất độ chính xác.

2) Từ softmax ba lần qua đến softmax một lần qua: Tuy nhiên, sử dụng bias động có nghĩa là một phương pháp ba lần qua đắt đỏ để tính toán Phương trình (3): Lần qua 1 trên tất cả token j ∈ {1,...,N} được yêu cầu để quét tất cả giá trị đầu vào tìm bias tối ưu b; Lần qua 2 được yêu cầu để tính toán tổng các thuật ngữ mũ trong mẫu số s = ∑j=1^N exp(xj−b); Lần qua 3 được yêu cầu để tính toán exp(xi−b)/s cho mỗi token để hoàn thành tính toán softmax.

Để giảm tính toán từ ba lần qua xuống một lần qua để giảm độ trễ tính toán, chúng tôi đề xuất một thuật toán trực tuyến để tính toán bias b và tổng mẫu số softmax s đồng thời, hiệu quả kết hợp Lần qua 1 và Lần qua 2. Quá trình được đề xuất được hiển thị trong Thuật toán 1. Chúng tôi bắt đầu bằng cách khởi tạo tổng thành 0 và bias thành giá trị âm nhất có thể biểu diễn bởi datatype điểm cố định (được ký hiệu là −∞) (dòng 1). Cho mỗi phần tử xj trong dãy, chúng tôi đầu tiên xác định xem phần tử này có phải là giá trị tối đa mới và do đó nên là bias mới hay không. Nếu có, chúng tôi tỷ lệ tổng hiện tại bằng exp(b−xj), do đó hiệu quả cập nhật bias từ b thành xj cho tất cả các phần tử trước đó trong phép tính tổng mà không làm rõ ràng như vậy, và thêm 1, đơn giản là exp(xj−b) với b = xj (dòng 4–5). Ngược lại, chúng tôi thêm exp(xj−b) vào tổng, trong đó b là bias hiện tại (dòng 7). Hình 7 cho thấy một ví dụ về thuật toán này trên ba phần tử, {0.2, 0.1, 0.3}. Nó cho thấy rằng, trong khi chúng tôi đang xử lý các phần tử từng cái một, cả tổng và bias b đều được cập nhật đồng thời trong một lần qua.

Thuật toán 1 Thuật toán trực tuyến để tính toán bias softmax b và tổng mẫu số s đồng thời
Yêu cầu: x, một danh sách N phần tử
Đảm bảo: b = max(x), s = ∑j=1^N exp(xj−b)
1: b ← −∞, s ← 0
2: for j ∈ {1,...,N} do
3:    if xj > b then
4:        s ← s · exp(b−xj) + 1
5:        b ← xj
6:    else
7:        s ← s + exp(xj−b)
8:    end if
9: end for

sum ← 0
bias ← −∞
sum ← 1
bias ← 0.2
sum ← 1 + exp(0.1 − 0.2)
bias ← 0.2
sum ← sum · exp(0.2 − 0.3) + 1
bias ← 0.3

Tính toán mẫu số softmax cho dãy:
x1 = 0.2
x2 = 0.1
x3 = 0.3

Khởi tạo:
0.2 > bias:
0.3 > bias:
0.1 < bias:

Hình 7. Một ví dụ với ba phần tử minh họa cách thuật toán softmax trực tuyến của chúng tôi hoạt động bất kể thứ tự các phần tử được sắp xếp.

Cuối cùng, để tránh Lần qua 3 riêng biệt để tính toán kết quả softmax cuối cùng exp(xi−b)/s cho tất cả token, chúng tôi giữ điểm số ban đầu xi được lưu trữ cùng với bias b và mẫu số s. Quá trình tiếp theo cần kết quả softmax, chẳng hạn như quá trình con tự chú ý đọc điểm chú ý và nhân chúng với ma trận V, bao gồm phần cứng mũ và chia để tính toán kết quả softmax exp(xi−b)/s khi nó đọc mỗi phần tử xi đến. Với pipelining, độ trễ thêm vào cho mũ và chia là không đáng kể.

C. Xấp xỉ GELU Chi phí thấp Chính xác
Như đã thảo luận trong Mục III-A3, tính toán GELU trực tiếp cực kỳ đắt, trong khi các phương pháp xấp xỉ hiện tại sử dụng tanh(·) trong Phương trình (2) hoặc sigmoid hoặc là tốn tài nguyên hoặc là rất không chính xác. Để giải quyết thách thức này, chúng tôi đề xuất một xấp xỉ chính xác và thân thiện với phần cứng cho GELU với chi phí phần cứng cực thấp. Phương pháp xấp xỉ được đề xuất bao gồm bốn bước tối ưu hóa, được mô tả trong Hình 8.

Đầu tiên, chúng tôi nhận thấy rằng giá trị của GELU tương tự như ReLU, được tính toán dễ dàng như hàm ReLU(x) = max(x, 0). Do đó, chúng tôi sử dụng hàm ReLU làm xấp xỉ cơ bản với một giá trị hiệu chuẩn nhỏ δ(x) dựa trên đầu vào x:

GELU(x) ≈ ReLU(x) − δ(x) (4)

trong đó δ(x) có thể được lấy mẫu đều thành các biểu diễn điểm cố định, được tính toán trước và lưu trữ trong bảng tra cứu trong ROM. Hình 8 hiển thị ba đường cong: đường cong ReLU màu cam, đường cong GELU màu xanh và sự khác biệt δ của chúng màu xanh lá.

Thứ hai, chúng tôi nhận thấy rằng erf(·) trong GELU là một hàm lẻ giữ erf(−z) = −erf(z). Với x > 0, chúng tôi có:

δ(x) = ReLU(x) − GELU(x) = x − x · (1/2)(1 + erf(x/√2)) (5)

δ(−x) = 0 − (−x) · (1/2)(1 + erf(−x/√2))
      = x · (1/2)(1 − erf(x/√2)) = δ(x) (6)

Do đó, sự khác biệt giữa GELU và ReLU đối xứng về không, tức là δ(x) là một hàm chẵn. Điều này cho phép chúng tôi chỉ lưu trữ các giá trị của ReLU(x) − GELU(x) trong đó x ≥ 0. Hình 8 cho thấy chỉ một nửa đường cong δ(x) được lấy mẫu và lưu trữ on-chip.

Thứ ba, để lưu trữ bảng tra cứu cho các giá trị rời rạc hóa cho δ(x), vì 0 ≤ ReLU(x) − GELU(x) < 1 cho tất cả x ∈ R, chúng tôi có thể chỉ lưu trữ 22 bit phân số không dấu của datatype điểm cố định độ chính xác cao 32-bit được sử dụng cho xấp xỉ GELU, duy trì độ chính xác đầy đủ mà không yêu cầu lưu trữ bất kỳ bit nguyên nào.

Thứ tư, chúng tôi cắt bỏ bảng tra cứu tại điểm mà GELU(x) làm tròn thành ReLU(x) dựa trên datatype điểm cố định 32-bit; đối với bất kỳ giá trị truy vấn x nào ngoài phạm vi này, chúng tôi chỉ đơn giản sử dụng ReLU(x) như một xấp xỉ đủ chính xác của GELU(x). Ngoài ra, kích thước bước bảng tra cứu được chọn là một lũy thừa âm của hai. Điều này làm cho phép chia cần thiết để tính toán chỉ số bảng tra cứu tương đương với một phép dịch bit, làm cho nó rất hiệu quả và rẻ để triển khai trong phần cứng.

D. Sắp xếp lại Tính toán Expert-by-Expert
Như đã mô tả trong Mục III-B1, các mô hình MoE như M3ViT có thách thức bổ sung ngoài các mô hình dựa trên Transformer tiêu chuẩn, đó là xử lý nhiều expert trong một mẫu truy cập không thể dự đoán mà không liên tục tải lại trọng số expert. Phương pháp được đề xuất của chúng tôi là sắp xếp lại các tính toán từng token thành từng expert thay vào đó, như được hiển thị trong Hình 9(d). Trong quá trình xử lý mạng gating, khi các MLP expert top-k được chọn cho mỗi token, các chỉ số token được thêm vào hàng đợi theo expert để xử lý sau.

Chúng tôi xây dựng một metaqueue của tất cả các expert có hàng đợi có độ dài khác không. Điều này cho phép chúng tôi bỏ qua bước tải của bất kỳ expert nào không được sử dụng trong khối MoE hiện tại, chẳng hạn như expert 3 trong hình. Sau đó, cho mỗi expert trong metaqueue, chúng tôi tải trọng số và bias expert vào module lớp tuyến tính thống nhất của chúng tôi và tạo ra tất cả đầu ra cho expert đó. Mạng gating gán điểm cho mỗi cặp token/expert, và đầu ra của mỗi expert cho một token đã cho được trọng số bằng điểm trước khi được tích lũy lên đầu ra tồn tại

1.5
1.0
0.5
0.0
0.5
1.0
1.5
x
0.0
0.5
1.0
1.5
GELU(x)
ReLU(x)
ReLU(x)
GELU(x)
Được lấy mẫu cho bảng tra cứu

Hình 8. Xấp xỉ GELU sử dụng ReLU và một hàm hiệu chuẩn δ(x). δ(x) là một hàm chẵn, vì vậy chúng tôi lấy mẫu đều δ(x) > 0 và lưu trữ các giá trị rời rạc hóa trong bảng tra cứu.

--- TRANG 5 ---
P1 P2 P3
P4 P5 P6
P7 P8 P9
(a) Hình ảnh đầu vào được chia thành các patch

P1 Expert 1
Expert 2
Expert 3
Expert 4
P2
P3
(b) Mạng gating chọn expert cho mỗi patch/token

(c) Giải pháp naiv: thứ tự tính toán patch-by-patch

P1 Tính toán E1 × P1 Load E1
Tính toán E4 × P1 Load E3
P2 Tính toán E1 Load lại E1
Tính toán E2 Load E3
Load E1
Load E4
Tính toán E1 × P2 Load lại E1
Tính toán E4 × P2 Load lại E4
P3 Tính toán E1 Load lại E1
Tính toán E2 Load E3
Tính toán E2 × P3 Load E2
Tính toán E4 × P3 Load lại E4

(d) Thứ tự tính toán expert-by-expert được đề xuất: tái sử dụng tối đa expert đã tải

Tính toán E1 × P1 Load E1
Tính toán E1 × P2
Tính toán E2 × P3 Load E2
Tính toán E4 × P1 Load E4
Tính toán E4 × P2
Tính toán E4 × P3

Các patch có thể được lưu trữ tất cả on-chip
Trọng số expert lớn hơn nhiều; không thể lưu trữ tất cả on-chip

Hình 9. Chiến lược sắp xếp lại tính toán expert-by-expert của chúng tôi (d). So với phương pháp naiv patch-by-patch, phương pháp được đề xuất tái sử dụng tối đa mỗi expert đã tải và do đó mỗi expert chỉ cần được tải một lần. Phương pháp này loại bỏ tất cả chi phí bộ nhớ và di chuyển dữ liệu tiềm năng được giới thiệu bởi MoE.

một phần cho token đó. Các token không có trong hàng đợi được tạo trước đó của expert được bỏ qua hoàn toàn.

Bằng cách tổng hợp trước một hàng đợi của tất cả các token mà một MLP expert cần tính toán, chúng tôi hợp nhất việc tải trọng số và bias của mỗi expert và tối đa hóa việc tái sử dụng chúng. Hơn nữa, với ping-pong buffering, chúng tôi có thể hoàn toàn ẩn độ trễ tải của hầu hết các MLP expert, ngoại trừ trường hợp expert đầu tiên trong metaqueue hoặc bất kỳ trường hợp nào của sự mất cân bằng workload (trong đó expert trước đó chỉ có một vài token để tính toán và hoàn thành tính toán của nó sớm trong khi expert tiếp theo vẫn đang tải).

E. Module Lớp Tuyến tính Thống nhất
Trên toàn bộ mô hình M3ViT, các lớp tuyến tính với chiều đầu vào/đầu ra khác nhau có mặt trong các khối sau: (1) trên đầu vào dày đặc, từ chiều đầu vào đến chiều ẩn khối ViT; (2) trên đầu vào dày đặc, từ chiều ẩn khối ViT đến chiều đầu ra; (3) trên đầu vào thưa thớt, từ chiều đầu vào đến chiều ẩn khối MoE; (4) trên đầu vào thưa thớt, từ chiều ẩn khối MoE đến chiều đầu ra; (5) trên đầu vào dày đặc, từ chiều đầu vào đến chiều đầu ra. Để mỗi lớp tuyến tính tiêu thụ phần cứng tính toán riêng của nó sẽ dẫn đến lãng phí tài nguyên lớn, đặc biệt là DSP, điều này sẽ hạn chế đáng kể tính song song tối đa và làm tổn hại độ trễ. Chúng tôi hợp nhất tất cả các lớp tuyến tính trong toàn bộ mô hình thành một module tính toán lớp tuyến tính duy nhất, cho phép tất cả các lớp tuyến tính tận dụng tính song song cao. Như được hiển thị trong Hình 3, module tuyến tính thống nhất (khối màu xanh lá bên phải) có thể xử lý nhiều loại lớp tuyến tính với cấu hình linh hoạt thời gian chạy, bao gồm chiều đầu vào/đầu ra biến đổi, đầu vào dày đặc hoặc thưa thớt, và có sử dụng GELU hay không.

Xử lý chiều đầu vào/đầu ra biến đổi. Lưu ý rằng việc chia sẻ tài nguyên cho các lớp tuyến tính với chiều đầu vào và đầu ra khác nhau là không đơn giản. Trong HLS, một lớp tuyến tính thường được triển khai như một vòng lặp lồng nhau, trong đó vòng lặp ngoài lặp qua chiều đầu ra (out_dim) và vòng lặp trong lặp qua chiều đầu vào (in_dim). HLS chỉ có thể tạo một pipeline từ cả hai vòng lặp khi vòng lặp trong có một ràng buộc hằng số (tức là, in_dim là hằng số). Nhưng vì các lớp tuyến tính trong Edge-MoE có chiều đầu vào khác nhau, in_dim không thể là hằng số. Do đó vòng lặp không thể được làm phẳng thành một pipeline, dẫn đến độ trễ nghiêm trọng, vì toàn bộ chiều đầu ra phải được xử lý trước khi chiều tiếp theo có thể bắt đầu.

Để giải quyết hạn chế này, chúng tôi đề xuất sử dụng một vòng lặp pipelined được làm phẳng thủ công duy nhất theo dõi các chỉ số vòng lặp ảo trong các thanh ghi riêng biệt, được tăng thủ công bên trong vòng lặp. Hình 10 hiển thị pseudocode của một vòng lặp được làm phẳng thủ công.

1 int next_i = 0, next_j = 0, iters = out_dim * in_dim;
2 for (int iter = 0; iter < iters; iter++) {
3   int i = next_i, j = next_j;
4   next_i = (j == out_dim-1) ? next_i+1 : next_i;
5   next_j = (j == in_dim-1) ? 0 : next_j+1;
6   // ...loop body...
7 }

Hình 10. Pseudocode của một vòng lặp được làm phẳng thủ công. Kỹ thuật này biến một vòng lặp lồng nhau (ví dụ: for-loop i = 0 to out_dim chứa for-loop j = 0 to in_dim) với ràng buộc biến đổi thành một vòng lặp có thể pipeline duy nhất.

Lớp tuyến tính thống nhất chấp nhận trọng số và bias ở định dạng khối, trong đó một khối duy nhất đại diện cho tất cả trọng số và bias cần thiết cho một chu kỳ tính toán duy nhất. Một module tải trọng số thống nhất được sử dụng để đọc trọng số từ dạng tuần tự của chúng từ DRAM off-chip vào BRAM ở định dạng khối yêu cầu. Module thống nhất bao gồm một dataflow streaming hỗ trợ pipelining inter-token để giảm thiểu độ trễ.

Xử lý các sơ đồ lượng tử hóa điểm cố định lai. Trong khi thống nhất tất cả các lớp tuyến tính, các module tải bias cũng được thống nhất. Tuy nhiên, các lớp khác nhau cần sử dụng các datatype điểm cố định khác nhau cho bias để duy trì độ chính xác. Ví dụ, các lớp tuyến tính được sử dụng trong tự chú ý yêu cầu bias 16-bit với 7 bit nguyên, nhưng các MLP trong các khối ViT và MoE yêu cầu độ chính xác cao hơn nhưng phạm vi thấp hơn, sử dụng bias 16-bit với chỉ 5 bit nguyên. Để sử dụng module tuyến tính thống nhất, chúng tôi riêng biệt chuyển đổi các bias này thành một loại bias rộng hơn duy nhất, có đủ bit nguyên để bao phủ phạm vi và đủ bit phân số để bao phủ độ chính xác của cả hai datatype, như được hiển thị trong Hình 11.

Datatype 16-bit cho bias tự chú ý
Datatype 16-bit cho bias MLP
Datatype bias rộng kết quả

Hình 11. Datatype điểm cố định bias rộng được sử dụng trong module lớp tuyến tính thống nhất.

Xử lý đầu vào thưa thớt và dày đặc. Để có thể xử lý cả đầu vào thưa thớt và dày đặc đồng thời cho các lớp MoE và không MoE, lớp tuyến tính thống nhất trực tiếp điều khiển các quá trình đọc và ghi từ DRAM. Các quá trình đọc và ghi mỗi cái đều chứa hai submodule cho truy cập DRAM trực tiếp (dày đặc) và gián tiếp (thưa thớt, được lập chỉ mục bởi hàng đợi theo expert), và module cấp cao nhất chuyển một cờ để chỉ ra cái nào nên được sử dụng tại bất kỳ thời điểm nào. Submodule writer gián tiếp cũng hỗ trợ tích lũy có trọng số của đầu ra lớp tuyến tính trên bộ đệm đầu ra hiện tại thay vì ghi đè lên nó, do đó cho phép tích lũy đầu ra theo expert trực tiếp mà không cần bước tổng hợp bổ sung.

Các quá trình đọc và ghi DRAM hỗ trợ tổng hợp và phân tách các khối được xử lý bởi submodule tính toán cốt lõi, cho phép sử dụng kích thước khối lớn hơn so với những gì được hỗ trợ bởi giao diện AXI với DRAM.

Cuối cùng, một cờ điều khiển xem quá trình writer có nên áp dụng hàm GELU trước khi ghi đầu ra hay không, điều này cho phép các MLP trong các khối ViT và MoE kết hợp hàm kích hoạt mà không cần bước bổ sung. Nhờ việc sử dụng pipelining, độ trễ thêm vào bởi hàm kích hoạt này là không đáng kể.

F. Mạng Gating cho Đa Tác vụ
Việc sử dụng Mixture-of-Experts đặc biệt quan trọng trong M3ViT, vì nó là chìa khóa để cho phép Học Đa Tác vụ hiệu quả. Hình 3 (trái) minh họa cách M3ViT sử dụng MoE trong kịch bản đa tác vụ: các mạng gating riêng biệt được sử dụng cho mỗi tác vụ để chọn các expert tốt nhất cho một kết hợp token và tác vụ hạ lưu đã cho. Triển khai của chúng tôi tải trọng số mạng gating từ DRAM chỉ khi cần thiết để tính toán, điều này cho phép chuyển đổi giữa các tác vụ dễ dàng với chi phí bằng không chỉ bằng cách cập nhật con trỏ đến mạng gating cụ thể cho tác vụ.

V. THỰC NGHIỆM
Chúng tôi tiến hành một số thí nghiệm trên bo mạch được xác minh để chứng minh tính hiệu quả của các phương pháp được đề xuất của chúng tôi. Tất cả mã trên bo mạch được triển khai thông qua High-Level Synthesis thông qua Xilinx Vitis HLS 2021.1. Chúng tôi triển khai bitstream lên FPGA Xilinx ZCU102 và sử dụng thư viện PYNQ cho mã host. Tần số clock của thiết kế chúng tôi là 300 MHz. Tất cả thí nghiệm sử dụng tập dữ liệu Cityscapes [5], với hình ảnh kích thước 128×256 được chia thành các patch kích thước 16×16.

So sánh chính của chúng tôi là với baseline CPU và GPU và thiết kế FPGA không có các kỹ thuật được đề xuất của chúng tôi. Chúng tôi thấy khó khăn để thực hiện so sánh công bằng với các nghiên cứu trước đó vì hai lý do chính. Đầu tiên, chúng tôi nhắm đến tăng tốc ViT từ một góc độ khác với các nghiên cứu trước đó và

2%
4%
11%
25%
25%
9%
5%
19%
Phân tích Độ trễ
Patch Embedding
Layer Norm
Attention Linear
Attention Q×K
Attention M×V
Matrix Addition
ViT Blocks
MoE Blocks

5%
10%
21%
15%
12%
1%
20%
16%
Sử dụng Tài nguyên LUT
Patch Embedding
Layer Norm
Unified Linear
Attention Q×K
Attention M×V
Matrix Addition
MoE Gating
Other

Hình 12. Phân tích độ trễ và sử dụng LUT trong triển khai FPGA của chúng tôi cho mô hình M3ViT.

không dựa vào nén mô hình hoặc huấn luyện lại; những kỹ thuật này trực giao với chúng tôi và có thể được áp dụng cùng nhau. Thứ hai, các nghiên cứu trước đó về tăng tốc ViT thiếu triển khai từ đầu đến cuối trên bo mạch, điều này phơi bày khó khăn để so sánh.

A. So sánh với CPU và GPU trên M3ViT
Chúng tôi đầu tiên đánh giá bộ tăng tốc của chúng tôi cho M3ViT vì nó là mô hình ViT đa tác vụ tiên tiến nhất, sử dụng ViT tiêu chuẩn làm backbone nhưng cũng được trang bị các tính năng MoE, làm cho nó thách thức hơn các mô hình ViT tiêu chuẩn. Do đó, chúng tôi chọn M3ViT làm baseline chính của chúng tôi, được đánh giá trên các tác vụ phân đoạn ngữ nghĩa và ước lượng độ sâu. Chúng tôi triển khai bộ tăng tốc của chúng tôi trên FPGA ZCU102 và so sánh với baseline CPU (Intel Xeon 6226R) và GPU (NVIDIA RTX A6000) được triển khai trong PyTorch sử dụng FastMoE [10], một thư viện sử dụng kỹ thuật scatter/gather để tối ưu hóa tính toán MoE. Công suất được đo bằng Intel RAPL và NVIDIA SMI tương ứng. Chúng tôi sử dụng trọng số điểm cố định 16-bit và kích hoạt điểm cố định 32-bit. Kết quả trong Bảng IV.

Đầu tiên, về độ chính xác, cả baseline phần mềm M3ViT và Edge-MoE đều vượt trội hơn baseline học tác vụ đơn (STL) (∆m Acc. trong Bảng IV): M3ViT phần mềm vượt trội hơn STL +0.76%, và Edge-MoE trên FPGA hoạt động tốt hơn +0.67% – chỉ giảm 0.09%. Thứ hai, về hiệu suất năng lượng, Edge-MoE trên FPGA đạt được tiết kiệm đáng kể: 4.90× so với CPU và 2.24× so với GPU.

B. Các Mô hình ViT Tiêu chuẩn
Chúng tôi cũng đánh giá mức độ hiệu quả của các kỹ thuật tối ưu hóa của chúng tôi trong việc giảm độ trễ của một số mô hình ViT khác nhau khi được triển khai trên FPGA.

Kết quả trong Bảng III cho thấy cải thiện nhất quán từ các phương pháp được đề xuất của chúng tôi dao động từ 9.76× đến 9.84× trên các mô hình không phải M3ViT, cũng như hơn 10× tăng tốc trên M3ViT với Mixture-of-Experts. Sự tăng tốc đáng tin cậy thu được từ các tối ưu hóa của chúng tôi cho thấy rằng, bên cạnh các kỹ thuật cụ thể cho mô hình của chúng tôi cho M3ViT, các phương pháp của chúng tôi có thể áp dụng chung và hiệu quả khi áp dụng cho bất kỳ mô hình ViT nào.

C. Phân tích Độ trễ và Tài nguyên
Để hiểu phần nào của mô hình của chúng tôi đắt nhất và phần nào mất nhiều thời gian nhất để tính toán, chúng tôi đo độ trễ và sử dụng tài nguyên trên bo mạch của các thành phần khác nhau trong triển khai M3ViT của chúng tôi. Hình 12 hiển thị kết quả của chúng tôi.

Ngay cả ở mức song song 4×, các phép nhân attention Q×K và M′×V mất một nửa tổng thời gian tính toán, chứng minh sự cần thiết của việc tăng tốc tính toán này.

Ngoài ra, hiệu quả của lớp tuyến tính thống nhất của chúng tôi nổi bật: nó mất phần lớn nhất của tài nguyên LUT, nhưng kết quả là, nó tăng tốc đáng kể các lớp tuyến tính attention, khối ViT và khối MoE, chỉ mất 35% tổng độ trễ kết hợp.

--- TRANG 6 ---
BẢNG III
Các mô hình ViT phổ biến và M3ViT đa tác vụ với mixture-of-expert (MoE) được đánh giá trong nghiên cứu này, và việc giảm độ trễ của chúng bằng cách áp dụng các kỹ thuật được đề xuất.

| Mô hình | Lớp | Chiều | Ẩn | MLP | Head | Params | Độ trễ (ms) | | |
|---------|-----|--------|-----|-----|------|--------|-------------|-----|-----|
| | | | | | | | Không opt. | Có opt. | Tăng tốc |
| ViT-Base [7] | 12 | 768 | 3072 | 12 | 86M | 4061.1 | 414.32 | 9.80× |
| ViT-Large | 24 | 1024 | 4096 | 16 | 307M | 14264 | 1450.6 | 9.83× |
| ViT-Huge | 32 | 1280 | 5120 | 16 | 632M | 29502 | 2997.9 | 9.84× |
| DeiT-Small [28] | 12 | 384 | 1536 | 6 | 22M | 1063.6 | 109.00 | 9.76× |
| DeiT-Base | 12 | 768 | 3072 | 12 | 86M | 4061.1 | 414.32 | 9.80× |
| M3ViT+MoE [16] | 12 | 192 | 768 | 3 | 7M | 353.44 | 34.64 | 10.20× |

BẢNG IV
So sánh CPU và GPU. Kích thước batch là 1 hình ảnh.

| Thiết bị | Độ trễ | Công suất | Năng lượng | Tần số | ∆mAcc. |
|----------|--------|-----------|------------|---------|---------|
| CPU | 169.72 ms | 14.53 W | 2.466 J (4.90×) | 2500 MHz | +0.76% |
| GPU | 13.73 ms | 82.24 W | 1.129 J (2.24×) | 1800 MHz | +0.76% |
| Edge-MoE | 34.64 ms | 14.54 W | 0.504 J (1.00×) | 300 MHz | +0.67% |

D. Nghiên cứu Ablation
Trong Bảng V, chúng tôi tiến hành nghiên cứu ablation để đánh giá hiệu quả của các tối ưu hóa của chúng tôi đối với độ trễ, sử dụng tài nguyên và độ chính xác. Chúng tôi hiển thị sự tiến triển gia tăng của các yếu tố này khi mỗi kỹ thuật chính của chúng tôi được áp dụng, bao gồm hiệu ứng gián tiếp của việc tài nguyên phần cứng chi phí thấp hơn cho phép tăng song song lớn hơn.

Chúng tôi thấy rằng mỗi tính năng của chúng tôi dẫn đến tăng tốc đáng chú ý. Ngoài ra, hầu hết các tài nguyên phần cứng theo xu hướng giảm dần theo thời gian, chứng minh thực nghiệm rằng các kỹ thuật của chúng tôi thân thiện với phần cứng. Tuy nhiên, số lượng DSP được sử dụng vẫn giống nhau trong suốt phần lớn nghiên cứu ablation, vì việc tăng song song liên tục đã phủ nhận grảm ảnh hưởng đến số lượng DSP.

Chúng tôi chú ý đến ba hàng của Bảng V đặc biệt:
1) Softmax một lần qua với bias động: Do những thách thức gây ra bởi tràn của exp(·) được mô tả trong Mục III-A2, tất cả các kiến trúc đều sử dụng bias động, như trong Mục IV-B1. Tuy nhiên, hàng thứ ba giới thiệu tối ưu hóa softmax một lần qua của chúng tôi từ Mục IV-B2, thay vì ba lần qua.

2) GELU chính xác, chi phí thấp: Hầu hết các tối ưu hóa của chúng tôi đều không ảnh hưởng đến độ chính xác tác vụ hạ lưu và tương đương toán học với các phiên bản chưa tối ưu hóa. Tuy nhiên, triển khai GELU chính xác, chi phí thấp của chúng tôi thay thế xấp xỉ dựa trên sigmoid kém chính xác hơn từ các kiến trúc trước đó, được mô tả trong Mục III-A3. Không chỉ phương pháp dựa trên hiệu chuẩn của chúng tôi tăng độ chính xác xấp xỉ, nó còn giảm thêm việc sử dụng tài nguyên, cho phép song song cao hơn dẫn đến tăng tốc 1.66× so với kiến trúc trước đó.

3) Module lớp tuyến tính thống nhất: Module lớp tuyến tính thống nhất được đề xuất của chúng tôi dẫn đến thay đổi tương đối lớn nhất về độ trễ trong bảng, nhanh hơn 2× so với kiến trúc trước đó, đồng thời cũng giảm việc sử dụng tất cả bốn tài nguyên phần cứng. Như được mô tả trong Mục IV-E, lớp tuyến tính thống nhất thay thế năm module phần cứng riêng biệt cho các lớp tuyến tính riêng biệt, do đó giảm đáng kể số lượng tài nguyên tiêu thụ. Điều này, đến lượt nó, cho phép song song cao hơn đáng kể trong lớp tuyến tính thống nhất, có lợi cho tất cả các phần của M3ViT trước đây đang sử dụng module riêng biệt, trong khi vẫn để lại việc sử dụng tài nguyên giảm.

VI. KẾT LUẬN
Trong bài báo này, chúng tôi đã đề xuất một bộ tăng tốc FPGA mới với một bộ sưu tập các kỹ thuật sáng tạo cho các mô hình Vision Transformer (ViT) và một ViT đa tác vụ tiên tiến sử dụng Mixture-of-Expert (MoE). Theo hiểu biết tốt nhất của chúng tôi, đây là triển khai FPGA từ đầu đến cuối đầu tiên cho ViT đa tác vụ, với chức năng được xác minh trên bo mạch. Chúng tôi đề xuất năm kỹ thuật chính: (1) một cơ chế sắp xếp lại tính toán attention giảm yêu cầu băng thông từ tỷ lệ thuận xuống hằng số, bất kể tính song song; (2) một xấp xỉ softmax một lần qua đạt được cả độ chính xác cao và tốc độ; (3) một xấp xỉ kích hoạt GELU chính xác với chi phí phần cứng cực thấp; (4) một lớp tuyến tính thống nhất và linh hoạt để chia sẻ tài nguyên tích cực; (5) một cơ chế tính toán MoE expert-by-expert mới yêu cầu chi phí bộ nhớ và truyền dữ liệu bằng không. Các kỹ thuật được đề xuất có thể giảm độ trễ hơn 18× khi áp dụng cho M3ViT và hơn 9× khi áp dụng cho các mô hình ViT tiêu chuẩn. So với CPU và GPU, thiết kế của chúng tôi đạt được hiệu suất năng lượng tốt hơn 4.90× và 2.24× tương ứng. Trên tập dữ liệu xe tự lái đa tác vụ, chúng tôi đạt được gần 30 khung hình mỗi giây, cho thấy hiệu suất hứa hẹn cho MTL ViT trong các ứng dụng thực tế.

LỜI CẢM ƠN
Nghiên cứu này và các tác giả được hỗ trợ một phần bởi chương trình Học bổng Đổi mới Qualcomm 2022, Quỹ Khoa học Quốc gia dưới Số Grant 2202329, và Trung tâm Nghiên cứu về Hệ thống Phân cấp Tính toán Mới (CRNCH) tại Georgia Tech.

TÀI LIỆU THAM KHẢO
[1] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lučić, và C. Schmid, "Vivit: A video vision transformer," trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, tr. 6836–6846.

[2] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, và S. Zagoruyko, "End-to-end object detection with transformers," trong European conference on computer vision. Springer, 2020, tr. 213–229.

[3] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, và W. Gao, "Pre-trained image processing transformer," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, tr. 12 299–12 310.

[4] Q. Chen, C. Sun, Z. Lu, và C. Gao, "Enabling energy-efficient inference for self-attention mechanisms in neural networks," trong 2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS). IEEE, 2022, tr. 25–28.

[5] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, và B. Schiele, "The cityscapes dataset for semantic urban scene understanding," trong Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

[6] J. Devlin, M.-W. Chang, K. Lee, và K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.

[7] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929, 2020.

[8] Y. Gao, J. Ma, M. Zhao, W. Liu, và A. L. Yuille, "Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, tr. 3205–3214.

[9] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, và Y. Wang, "Transformer in transformer," Advances in Neural Information Processing Systems, tập 34, tr. 15 908–15 919, 2021.

--- TRANG 7 ---
BẢNG V
Nghiên cứu ablation của các kỹ thuật đề xuất của chúng tôi. Tất cả các giá trị độ trễ, tài nguyên và độ chính xác đều được đo trên bo mạch. Baseline là thiết kế bộ tăng tốc M3ViT hoàn chỉnh không có các kỹ thuật đề xuất của chúng tôi. Áp dụng tất cả sáu kỹ thuật có thể dẫn đến tăng tốc hơn 18× và không có sụt giảm độ chính xác.

| Kiến trúc | Độ trễ (Tăng tốc) | Tài nguyên phần cứng | | | | Phân đoạn ngữ nghĩa (mIoU ↑) | Ước lượng độ sâu (RMSE ↓) | Lợi ích độ chính xác MTL ∆m(↑) |
|-----------|-------------------|---------|------|-----|-----|---------------------------|------------------------|------------------------------|
| | | BRAM | DSP | LUT | FF | | | |
| Baseline không có kỹ thuật đề xuất | 650.3 ms (1.00×) | 84.6% | 65.9% | 60.8% | 46.9% | 63.8066 | 0.0373 | +0.58% |
| + Sắp xếp lại expert-by-expert (§IV-D) | 433.4 ms (1.50×) | 84.3% | 65.9% | 59.0% | 45.4% | 63.8066 | 0.0373 | +0.58% |
| + Softmax bias động một lần qua (§IV-B) | 353.4 ms (1.84×) | 84.0% | 67.9% | 59.0% | 45.7% | 63.8066 | 0.0373 | +0.58% |
| + GELU chính xác, chi phí thấp (§IV-C) | 212.9 ms (3.05×) | 64.1% | 63.0% | 45.5% | 34.9% | 63.8491 | 0.0372 | +0.67% |
| + Module lớp tuyến tính thống nhất (§IV-E) | 104.3 ms (6.23×) | 47.5% | 57.3% | 44.1% | 27.0% | 63.8491 | 0.0372 | +0.67% |
| + Sắp xếp lại attention trên Q×K (§IV-A) | 59.2 ms (10.98×) | 48.1% | 61.6% | 46.5% | 27.5% | 63.8491 | 0.0372 | +0.67% |
| + Sắp xếp lại attention trên M′×V (§IV-A) | 34.6 ms (18.77×) | 50.1% | 76.3% | 46.8% | 29.4% | 63.8491 | 0.0372 | +0.67% |
| Độ chính xác baseline phần mềm (PyTorch) | - | - | - | - | - | 63.9450 | 0.0372 | +0.76% |

[10] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, và J. Tang, "FastMoE: A fast mixture-of-expert training system," Tháng 3 2021.

[11] D. Hendrycks và K. Gimpel, "Gaussian error linear units (gelus)," arXiv preprint arXiv:1606.08415, 2016.

[12] Z. Kong, P. Dong, X. Ma, X. Meng, W. Niu, M. Sun, B. Ren, M. Qin, H. Tang, và Y. Wang, "SPViT: Enabling faster vision transformers via soft token pruning," Tháng 12 2021.

[13] D.-G. Lee, "Fast drivable areas estimation with multi-task learning for real-time autonomous driving assistant," Applied Sciences, tập 11, số 22, tr. 10713, 2021.

[14] B. Li, S. Pandey, H. Fang, Y. Lyv, J. Li, J. Chen, M. Xie, L. Wan, H. Liu, và C. Ding, "FTRANS: Energy-efficient acceleration of transformers using FPGA," trong Proceedings of the ACM/IEEE International Symposium on Low Power Electronics and Design, ser. ISLPED '20. New York, NY, USA: Association for Computing Machinery, Tháng 8 2020, tr. 175–180.

[15] Z. Li, M. Sun, A. Lu, H. Ma, G. Yuan, Y. Xie, H. Tang, Y. Li, M. Leeser, Z. Wang et al., "Auto-vit-acc: An fpga-aware automatic acceleration framework for vision transformer with mixed-scheme quantization," arXiv preprint arXiv:2208.05163, 2022.

[16] H. Liang, Z. Fan, R. Sarkar, Z. Jiang, T. Chen, Y. Cheng, C. Hao, và Z. Wang, "M³vit: Mixture-of-experts vision transformer for efficient multi-task learning with model-accelerator co-design," trong 36th Annual Conference on Neural Information Processing System (NeurIPS 2022), Tháng 12 2022.

[17] S. Liu, E. Johns, và A. J. Davison, "End-to-end multi-task learning with attention," trong Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, tr. 1871–1880.

[18] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, và B. Guo, "Swin transformer: Hierarchical vision transformer using shifted windows," trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, tr. 10 012–10 022.

[19] S. Lu, M. Wang, S. Liang, J. Lin, và Z. Wang, "Hardware accelerator for multi-head attention and position-wise feed-forward in the transformer," trong 2020 IEEE 33rd International System-on-Chip Conference (SOCC). IEEE, 2020, tr. 84–89.

[20] J. Ma, Z. Zhao, X. Yi, J. Chen, L. Hong, và E. H. Chi, "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts," trong Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018, tr. 1930–1939.

[21] I. Misra, A. Shrivastava, A. Gupta, và M. Hebert, "Cross-stitch networks for multi-task learning," trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, tr. 3994–4003.

[22] S. Park, G. Kim, Y. Oh, J. B. Seo, S. M. Lee, J. H. Kim, S. Moon, J.-K. Lim, và J. C. Ye, "Multi-task vision transformer using low-level chest x-ray feature corpus for covid-19 diagnosis and severity quantification," Medical Image Analysis, tập 75, tr. 102299, 2022.

[23] H. Peng, S. Huang, T. Geng, A. Li, W. Jiang, H. Liu, S. Wang, và C. Ding, "Accelerating transformer-based deep learning models on FPGAs using column balanced block pruning," trong 2021 22nd International Symposium on Quality Electronic Design (ISQED), Tháng 4 2021, tr. 142–148.

[24] P. Qi, Y. Song, H. Peng, S. Huang, Q. Zhuge, và E. H.-M. Sha, "Accommodating transformer onto FPGA: Coupling the balanced model compression and FPGA-implementation optimization," trong Proceedings of the 2021 on Great Lakes Symposium on VLSI. New York, NY, USA: Association for Computing Machinery, Tháng 6 2021, tr. 163–168.

[25] S. Ruder, J. Bingel, I. Augenstein, và A. Søgaard, "Latent multi-task architecture learning," trong Proceedings of the AAAI Conference on Artificial Intelligence, tập 33, số 01, 2019, tr. 4822–4829.

[26] M. Sun, H. Ma, G. Kang, Y. Jiang, T. Chen, X. Ma, Z. Wang, và Y. Wang, "VAQF: Fully automatic software-hardware co-design framework for low-bit vision transformer," Tháng 2 2022.

[27] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit et al., "Mlp-mixer: An all-mlp architecture for vision," Advances in Neural Information Processing Systems, tập 34, tr. 24 261–24 272, 2021.

[28] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, và H. Jegou, "Training data-efficient image transformers & distillation through attention," trong International Conference on Machine Learning, tập 139, Tháng 7 2021, tr. 10 347–10 357.

[29] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, và H. Jégou, "Training data-efficient image transformers & distillation through attention," trong International Conference on Machine Learning. PMLR, 2021, tr. 10 347–10 357.

[30] S. Vandenhende, S. Georgoulis, và L. V. Gool, "Mti-net: Multi-scale task interaction networks for multi-task learning," trong European Conference on Computer Vision. Springer, 2020, tr. 527–543.

[31] S. Vandenhende, S. Georgoulis, W. Van Gansbeke, M. Proesmans, D. Dai, và L. Van Gool, "Multi-task learning for dense prediction tasks: A survey," IEEE transactions on pattern analysis and machine intelligence, 2021.

[32] D. Xu, W. Ouyang, X. Wang, và N. Sebe, "Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing," trong Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, tr. 675–684.

[33] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, Z.-H. Jiang, F. E. Tay, J. Feng, và S. Yan, "Tokens-to-token vit: Training vision transformers from scratch on imagenet," trong Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, tr. 558–567.

[34] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, và S. Savarese, "Taskonomy: Disentangling task transfer learning," trong Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, tr. 3712–3722.

[35] X. Zhang, Y. Wu, P. Zhou, X. Tang, và J. Hu, "Algorithm-hardware co-design of attention mechanism on fpga devices," ACM Transactions on Embedded Computing Systems (TECS), tập 20, số 5s, tr. 1–24, 2021.

[36] Z. Zhang, Z. Cui, C. Xu, Z. Jie, X. Li, và J. Yang, "Joint task-recursive learning for semantic segmentation and depth estimation," trong Proceedings of the European Conference on Computer Vision (ECCV), 2018, tr. 235–251.

[37] Z. Zhang, Z. Cui, C. Xu, Y. Yan, N. Sebe, và J. Yang, "Pattern-affinitive propagation across depth, surface normal and semantic segmentation," trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, tr. 4106–4115.
