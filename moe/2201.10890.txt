# 2201.10890.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2201.10890.pdf
# File size: 1703049 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
One Student Knows All Experts Know: From Sparse to Dense
Fuzhao Xue1Xiaoxin He1Xiaozhe Ren2Yuxuan Lou1Yang You1
1Department of Computer Science, National University of Singapore
2Huawei Noah‚Äôs Ark Lab
Abstract
Human education system trains one student by multiple
experts. Mixture-of-experts (MoE) is a powerful sparse ar-
chitecture including multiple experts. However, sparse MoE
model is easy to overÔ¨Åt, hard to deploy, and not hardware-
friendly for practitioners. In this work, inspired by the
human education model, we propose a novel task, knowl-
edge integration, to obtain a dense student model (OneS)
as knowledgeable as one sparse MoE. We investigate this
task by proposing a general training framework including
knowledge gathering and knowledge distillation. Specif-
ically, to gather key knowledge from different pre-trained
experts, we Ô¨Årst investigate four different possible knowl-
edge gathering methods, i.e. summation, averaging, Top-K
Knowledge Gathering (Top-KG), and Singular Value De-
composition Knowledge Gathering (SVD-KG) proposed in
this paper. We then reÔ¨Åne the dense student model by knowl-
edge distillation to offset the noise from gathering. On Im-
ageNet, our OneS preserves 61:7%beneÔ¨Åts from MoE and
achieves 78:4%top-1 accuracy ImageNet with only 15M
parameters. On four natural language processing datasets,
OneS obtains 88:2%MoE beneÔ¨Åts and outperforms the best
baseline by 51:7%using the same architecture and train-
ing data. In addition, compared with the MoE counterpart,
OneS can achieve 3:7inference speedup due to less com-
putation and hardware-friendly architecture.
1. Introduction
Revisiting how we become a researcher, most peo-
ple learn from multiple teachers ( i.e. experts). Existing
work [2] in education also shows that experts from dif-
ferent subjects can help students reach deep understanding
and train more talents. The students who integrate knowl-
edge from experts can become as knowledgeable as the set
of these experts fast. Inspired by such human education
model, this work focuses on training a powerful deep learn-
ing model by collecting knowledge from a set of experts.
Recent study in deep learning proposed mixture-of-
experts (MoE), a deep neural network with multiple experts.
Expert 1 Expert 2 Expert 3 Expert 4Mixture -of-Experts
StudentDense ModelFigure 1. Human education model matches MoE and dense model.
Each expert is a sub-neural network in the whole model.
The key idea of MoE is to divide and conquer the task. MoE
encourages each expert to learn from a task-speciÔ¨Åc subset
of the input. For each subset of the input, there would be
only a sub-network activated. Such sparse computation of
MoE enables us to scale model to trillions of parameters
with comparable computation cost [8].
The MoE model is powerful and achieved promising re-
sults due to its large but sparse-activated model capacity.
However, MoE is easy to overÔ¨Åt. We usually pre-train an
MoE on a large dataset and then Ô¨Åne-tune it on various
downstream tasks. In most cases, these downstream tasks
are the target problem we want to solve. Compared with
dense models, more trainable parameters and sparse con-
ditional computation introduce overÔ¨Åtting [14, 27] during
Ô¨Åne-tuning, especially when the scale of dataset is not large
enough. In addition, even if we trained an MoE model suc-
cessfully, it is hard to deploy. For MoE with trillions of
parameters, we need to deploy different experts on different
devices to reduce the memory consumption on device ( e.g.
GPU, TPU). Third, MoE model is not hardware-friendly.
Expert parallelism is communication expensive. For GPU
clusters, the all-to-all operation is too slow to scale the MoE
model up. Besides, the gating function includes numerous
operations to create token masks, select top-k experts, and
perform cumulative-sum to Ô¨Ånd the token-id going to each
1arXiv:2201.10890v4  [cs.LG]  25 Oct 2022

--- PAGE 2 ---
expert and sparse matrix-multiply [16]. All these operations
are wasteful due to the sparse tensor representation. More
importantly, they are extremely slow due to many kernel call
invocations. In summary, the sparse MoE model is power-
ful, but it is relatively hard to use in practice. The dense
model is widely used but weaker than the sparse model with
comparable computation cost. Then, is it possible to com-
bine the strength of sparse and dense model to train a model
that is both effective and easy to use?
In this work, inspired by human education model, we
propose a new task, i.e. knowledge integration. As a gen-
eral training framework, knowledge integration includes
two steps, i.e. knowledge gathering and knowledge distilla-
tion. In knowledge gathering, we treat each expert in MoE
as a specialist in human education. The student is a dense
model, and we are to collect knowledge from all experts
and assign the knowledge to the student. To gather knowl-
edge from experts, as the Ô¨Årst work focusing on this task,
we investigate four different possible solutions, i.e. sum-
mation, averaging, Top-K Knowledge Gathering (Top-KG),
and Singular Value Decomposition Knowledge Gathering
(SVD-KG) proposed in this work. For the Top-KG and
SVD-KG, we use Top-K selection or SVD to extract key
knowledge from different experts of a pre-trained MoE, and
then, we initialize the feed-forward network (FFN) layers
for a dense model to approximate the MoE. To further re-
Ô¨Åne the model from noise, we use knowledge distillation [9]
to Ô¨Åne-tune the student. Please note in knowledge distilla-
tion stage, we use the whole MoE model to teach the student
dense model. The Ô¨Ånal student model has the same archi-
tecture as a standard dense model, but, it would cover the
knowledge of MoE with many experts and much more train-
able parameters. The framework described above matches
well with the human education model, one student inte-
grates knowledge from multiple experts so that the student
can learn fast.
Our contributions are summarized as follows:
‚Ä¢ We propose a new task, knowledge integration. The
goal is to combine the effectiveness of the sparse MoE
model and the usability of the dense model. To our best
knowledge, this is the Ô¨Årst work focusing on learning
a dense model from a pre-trained MoE model.
‚Ä¢ We propose to solve knowledge integration in two
steps, knowledge gathering and knowledge distillation.
To gather, we Ô¨Årst investigate four different possible
knowledge gathering methods, i.e. summation, aver-
aging, Top-KG and SVD-KG proposed in this paper.
Top-KG and SVD-KG are novel methods to extract
and merge key knowledge from experts of a pre-trained
MoE to initialize a dense model.
‚Ä¢ We evaluate our general training framework in differ-
ent areas, i.e. computer vision and natural languageprocessing. On ImageNet, compared with baselines,
our OneS preserve 23:1%more beneÔ¨Åts from MoE. On
natural language processing benchmarks, we achieve
88:2%MoE beneÔ¨Åts with only 46% parameters, and
we outperform baselines ( e.g. Distill, Switch) using al-
most the same architecture and training data. Also, due
to the hardware-friendly model architecture, OneS can
achieve 3:7inference speedup over the MoE coun-
terpart.
2. Preliminary
2.1. Mixture-of-Experts
Mixture-of-experts is a typical conditional computation
model. In this work, we use a pre-trained MoE model as a
teacher, and a dense model as a student to imitate the human
education model. Therefore, we brieÔ¨Çy review MoE Ô¨Årst.
Given one MoE model with Etrainable experts and input
representation x2RD, the output of MoE model can be
formulated as [21],:
MoE(x) =EX
i=1G(x)iei(x) (1)
whereei()is a non-linear transformation RD!RDof
theithexpert, and G() :RD!REis the gating net-
work,G(x)iis the routing weights of xto thei-thexpert.
Usually, both e()andG()are parameterized by neural net-
works. Please note the output of G()should be activated
by softmax function:
G(x) = topK(!(h(x) +)) (2)
where!is the softmax function, h()is a linear layer map-
pingRD!RE, andN (0;1
E2)is a Gaussian noise
for exploration of expert routing. The top-K selection is a
key module to activate sub-network sparsely. We usually
setKas 1 or 2 for comparable computation cost with the
corresponding dense model.
When training MoE model, if we have no regularization,
most tokens may be dispatched to a small portion of experts,
and other experts receive few tokens. Such an imbalanced
assignment would lead to lower efÔ¨Åciency and inferior ac-
curacy [8, 12]. Therefore, to achieve balanced workload for
different experts, we usually combines router g()with load
balance loss [12] Lbalance :
Lbalance =EEX
i=1miPi (3)
wheremis a vector and the ithelement ofmrepresents the
fraction of tokens dispatched to expert i:
mi=1
NNX
j=1k(xj)i (4)
2

--- PAGE 3 ---
whereNis the number of tokens to route, k(xj)is an index
vector from top-K function. Since the index vector genera-
tion here is non-differentiable, we deÔ¨Åne Pias:
Pi=!(h(x) +)i (5)
wherePisg(x)without top-K routing. When we minimize
Lbalance , we can see both mandPwould close to a uniform
distribution.
The trainable router here can also be replaced by non-
trainable modules, e.g. BASE layer [13]. This work focuses
on integrating knowledge from a pre-trained MoE instead
of MoE variants.
2.2. Problem Formulation
We have two stages in the knowledge integration frame-
work proposed in this work: (1) knowledge gathering from
MoE; (2) knowledge distillation to further reÔ¨Åne the new
dense model ( i.e. student). For the Ô¨Årst stage, given E
expertsfe1();e2();:::;e E()g, we are to maximize the
knowledge covered in the dense model s(). We use
transformer-based MoE to introduce our framework due to
its popularity. Given input representation x, within one
transformer block, each expert is an FFN, which can be for-
mulated as:
ei(x) =fi
2((fi
1(x))) (6)
wherefi
1()andfi
2()and linear transformations of ithex-
pert,()is the activation functions. For the dense student,
we have the same architecture but different trainable param-
eters:
s(x) =g2((g1(x))) (7)
where()would be the same activation function as ex-
perts. The only difference is the trainable parameters in
linear transformations. Then, our target is to approxi-
mate the trainable parameters of g1andg2according to
ff1
1;:::;fE
1gandff1
2;:::;fE
2g, respectively. We deÔ¨Åne
this target as knowledge gathering from MoE.
The second stage is Ô¨Åne-tuning the dense student to mini-
mize the difference between teacher output and student out-
put. We can easily Ô¨Ånd this task closer to knowledge dis-
tillation [9], so in this paper, we follow the typical KD ap-
proaches as our solution.
Our goal is to preserve MoE‚Äôs beneÔ¨Åts by a dense student
as much as possible. So, we deÔ¨Åne a metric, MoE beneÔ¨Åts,
to measure the ability of a dense student to integrate knowl-
edge from the MoE counterpart. The MoE beneÔ¨Åts can be
written as:
MoE benets =score student score dense
score MoE score dense(8)
where score can be any metric to evaluate the model. For
instance, score is accuracy for image classiÔ¨Åcation. Thescore dense here denotes the dense model‚Äôs performance
without knowledge integration proposed.
3. Approach
In general, the Ô¨Ånal target of this work is to obtain a
dense student model that is easy to use and as effective as
the sparse MoE. To this end, we propose a general train-
ing framework, knowledge integration, to integrate knowl-
edge from sparse MoE teacher to dense student. The pro-
posed knowledge integration includes two stages: knowl-
edge integration from MoE and knowledge distillation to
reÔ¨Åne the student. An overview of the proposed general
training framework is shown in Figure 2. The Ô¨Årst step is to
initialize the dense student. For most trainable layers ( e.g.
embedding layer, attention layer, normalization layer), the
teacher and the student have the same structure (We name
such layers as perfectly matched layers in this work.), so we
can copy the weights from teachers following Switch Trans-
former [8] directly. The challenging part is the MoE layer.
MoE layer has much more trainable parameters than the
dense counterpart with a single FFN layer, and each expert
is actually an FFN layer with unique weights and biases.
The core issue is to incorporate knowledge from different
FFN experts and assign the knowledge to one single FFN in
the student model. To this end, we investigate four differ-
ent possible knowledge gathering methods, i.e., summation,
averaging, Top-KG and SVD-KG. Then, knowledge distil-
lation is to Ô¨Åne-tune the initialized model to further improve
performance.
3.1. Knowledge Gathering from MoE
We Ô¨Årst formulate our KG task. Given an MoE layer
withEexperts, the target here is to gather knowledge from
all experts for one dense student. According to Eq. 6 and
Eq. 7, each expert comprises two linear layers, and the stu-
dent shares the same model structure with one single expert.
For brevity, we treat each expert as one linear transforma-
tion to show our idea, which can be expanded to multiple
linear layers easily. For E linear layers ff1;f2;:::;fEg,
each linear layer fi() :Rd1!Rd2with weights Wi
f2
Rd1d2and biasbi
f2Rd2,
KG(f1;f2;:::;fE)
=KG(W1
f;W2
f;:::;WE
f;b1
f;b2
f;:::;bE
f)
(Wg;bg) =g(9)
whereg() :Rd1!Rd2is a linear layer with Wg2Rd1d2
and biasbg2Rd2.
Before merging the weights, we Ô¨Årst initialize bgfrom
different experts. Since it has much fewer trainable param-
eters, we simply average the bias vector from different ex-
3

--- PAGE 4 ---
Expert 2
Router MoEExpert 1 Expert 3 Expert 4
Self-AttentionDense FFN
Self-AttentionKnowledge GatheringExpert 2
Router MoEExpert 1 Expert 3 Expert 4
Self-AttentionDense FFN
Self-AttentionOutput OutputKnowledge Distillation
Knowledge GatheringKnowledge DistillationKnowledge 
Integration
Merge
CopyFigure 2. An overview of our general training framework proposed. The overall training framework is knowledge integration, and it
includes two stages, knowledge gathering and knowledge distillation. In knowledge gathering, we investigate four different methods to
merge the knowledge from MoE, including summation, averaging and Top-KG and SVD-KG.
perts:
bg=1
EEX
i=1bi
f (10)
We employ such a simple policy because knowledge stored
in bias is much less than in weights, due to fewer trainable
parameters. We justify this assumption by experiments in
Appendix E.
After copying the weights and bias in the perfectly
matched layers and averaging bias in the MoE layers, we
initialize the dense student model weights by sparse MoE.
As the Ô¨Årst work focusing on this task, we investigate four
methods to gather the knowledge, i.e. summation, aver-
aging, Top-KG and SVD-KG. The Ô¨Årst two are the most
straightforward methods. We also propose two novel ap-
proaches, Top-KG and SVD-KG to extract key knowledge
from different experts of a pre-trained MoE.
3.1.1 Summation and Averaging
For weights in MoE, we Ô¨Årst consider two simple methods.
The Ô¨Årst one is the summation:
Wg=EX
i=1Wi
f (11)
and the second one is averaging:
Wg=1
EEX
i=1Wi
f (12)
Although these two gathering methods are simple, as the
Ô¨Årst work focusing on this task, we investigate them to pave
the way for gathering knowledge from MoE models.3.2. Top-K Knowledge Gathering
We also propose two novel methods to gather knowl-
edge. For weights, in MoE, a wide over-parameterized
model with much more trainable parameters, it is challeng-
ing to cover all knowledge in a narrow dense model. There-
fore, we have to extract the key knowledge from each expert
and then merge them into a single small dense model. Then,
the question is, how can we extract the key knowledge of
each trainable matrix ( i.e. weights)? We Ô¨Årst propose Top-
K knowledge gathering to extract the sub-matrix of each
expert. Forithexpert weight matrix Wi2Rd1d2, we cal-
culate the l2 norm of each column as li2Rd1. We then use
Top-K selection to pick Kcolumns ofWiaccording to li,
whereK=d2
E. The extracted matrix Wi
g2Rd1K. Then
we concatenate the extracted matrices from all experts as
Ô¨Ånal student initialization Wg2Rd1d2.
In practice, since each expert has two linear layers
Wi12Rd1d2andWi22Rd2d1, there would be a
column-mismatch for two extracted matrices from the same
expert if we select the sub-matrices of these two matrices
independently. To alleviate this issue, we calculate the l2
norm of each column in Wi1and the l2 norm of each row
inWi2. The sum of these two l2 norm vectors, i.e.li2Rd1
is fed into Top-K selection and then extract the sub-matrix.
3.2.1 SVD Knowledge Gathering
We investigate another novel way to extract key knowledge
from experts. Low-rank compression [3] has shown promis-
ing results in capturing key knowledge, which was used to
convert a not low-rank matrix to a rank- kdecomposition
of the weight matrix. Such a low-rank matrix can approxi-
mate the knowledge of the whole matrix. On this basis, we
4

--- PAGE 5 ---
can merge the low-rank matrix easier by reconstructing a
high-rank matrix from multiple low-rank matrices. Please
note, in this work, obtaining rank- kdecomposition is not
our target. Instead, the rank- kdecomposition is just an in-
termediate step of our decomposing and merging. In this
work, we propose to use SVD to extract key knowledge and
merge them to initialize another dense matrix:
Wi
f=Ui
fSi
fVi
fTUi
fKiSi
fKiVi
fT
Ki (13)
whereUi
f2Rd1d1andVi
f2Rd2d2are unitary matrices,
Si
f2Rd1d2is a diagonal matrix. We usually select the
top-K elements in Si
fand then construct Ui
fKi2Rd1Ki,
Si
fKi2RKiKiandVi
fKi2Rd2Kito approximate Wi
f.
Whenkis Ô¨Åxed, every matrix has the rank- kdecom-
position to approximate the original matrix. However, we
cannot guarantee the key knowledge in every expert can be
covered by a Ô¨Åxed rank- kdecomposition. Thus, we deÔ¨Åne
an adaptive SVD ratio 2(0;1]to ensure:
(Si
fKi)(Si
f) (14)
where(Si
f)denotes the sum of diagonal elements of Si
f.
If= 1, all ranks would be preserved for a full-rank ma-
trix. We then collect the decomposition of each expert and
concatenate them as:

Ugt =
U1
fK1::: UE
fKE
;
[Sg] =2
64S1
fK1
...
SE
fKE3
75;
[Vg] =2
64V1
fK1
...
VE
fKE3
75(15)
We can then obtain Wgas:
Wg=UgSgVgT(16)
Wgis a rank-Kgmatrix, where Kg= E
i=1Ki, covering
the key knowledge of every expert.
After SVD-KG, knowledge has been integrated from
pre-trained MoE. However, during knowledge gathering, it
is unavoidable to induce noise when we remove conditional
computation. Detailed analysis of the induced noise during
gathering can be found in Appendx A.
3.3. Knowledge Distillation
To mine the knowledge from noise, we adopt soft knowl-
edge distillation [9] to Ô¨Åne-tune the dense student. Soft
distillation minimizes the Kullback-Leibler divergence be-
tween the output of the teacher and the student. The corre-
sponding distillation loss can be written as:
Lsoft
distill=T2LKL(!(zs=T);!(zt=T)) (17)where!is the softmax function, LKLis Kullback-Leibler
divergence loss, zsandztare the logits of student and
teacher, respectively, and Tis the softmax temperature. We
also considered hard-label distillation [24] and compared its
performance with soft distillation. Please see Appendix C
for details.
3.4. Optimization
Our Ô¨Ånal loss function is simple:
Ltotal=Lmain+ (1 )Ldistill (18)
whereis used to balance the main loss and the distillation
loss. The main loss depends on the task. For instance, to
classify images, it is cross-entropy. For BERT pre-training,
it should be the masked language modeling loss and next
sentence prediction loss. The distillation loss here can be ei-
ther soft distillation loss or hard-label distillation loss. Since
our pre-trained MoE is Ô¨Åxed during knowledge distillation,
we do not need the load balance loss of MoE-based trans-
former.
4. Experiments
4.1. Computer Vision
Experimental settings To evaluate our general train-
ing framework, we conduct experiments on two different
areas, computer vision and natural language processing.
Datasets For vision, we select two widely-used image clas-
siÔ¨Åcation benchmarks, ILSVRC-2012 ImageNet [5] and Ci-
far10 [10], as platforms to evaluate our framework on com-
puter vision. ILSVRC-2012 ImageNet dataset we used in
this work has 1k classes and 1.3M images. We denote
it as ImageNet in the following experiments for brevity.
Baselines As we are the Ô¨Årst work, to our best knowl-
edge, focusing on integrating knowledge from a pre-trained
MoE, the only two existing strong baselines are the knowl-
edge distillation framework proposed in Meta AI MoE [1]
and Switch Transformer [8]. The Ô¨Årst one simply initial-
izes the student dense model randomly. The second work
initializes the dense model with the non-expert weights.
That is, they simply copy the layer which can be perfectly
matched into the dense model. For the weights that cannot
be matched ( i.e. experts), they skip the initialization from
MoE and train these layers from scratch instead. In our
work, for brevity, we denote these two approaches as Dis-
till and Switch, respectively. We also report the result of
Vision Transformer (ViT) on the same setting to compare
the parameter efÔ¨Åciency. Teacher In our training frame-
work, we need an MoE model to initialize our dense stu-
dent model ( i.e. knowledge gathering) and perform knowl-
edge distillation. In this work, we apply the pre-trained
5

--- PAGE 6 ---
Model MoE or Dense Para Sharing #Para ImageNet BeneÔ¨Åts(%)
ViTViT-B Dense 87M 78.6 -
ViT-L Dense 305M 77.5 -
ViT-B Dense X 10M 72.8 -
ViT-L Dense X 15M 76.9 -
TeacherWideNet-B MoE X 29M 77.5 -
WideNet-L MoE X 40M 79.5 -
BaselineDistill-B Dense X 10M 73.8 21.3
Distill-L Dense X 15M 77.3 15.3
Switch-B Dense X 10M 74.8 42.6
Switch-L Dense X 15M 77.8 34.6
OursOneS-B Sum Dense X 10M 75.2 51.1
OneS-L Sum Dense X 15M 78.2 48.1
OneS-B Avg Dense X 10M 75.3 53.2
OneS-L Avg Dense X 15M 78.0 40.7
OneS-B Top-K Dense X 10M 75.3 53.2
OneS-L Top-K Dense X 15M 78.4 57.7
OneS-B SVD Dense X 10M 75.7 61.7
OneS-L SVD Dense X 15M 78.4 57.7
Table 1. Top-1 Accuracy and MoE BeneÔ¨Åts(%) on ImageNet pre-training. As we deÔ¨Åned in Eq. 8, MoE BeneÔ¨Åts denotes the percentage
of performance improvement from MoE that can be preserved in the dense student model. The Para Sharing denotes whether the trainable
parameters are shared across transformer blocks. We use such model ( i.e. WideNet) as the MoE layer dominates the trainable parameters,
which can verify the effectiveness of knowledge integration methods directly. For the ViT without parameter sharing, we can usually
observe the overÔ¨Åtting issue when training on ImageNet.
Model #Para Cifar10
ViTViT-B 85M 98.3
ViT-L 305M 98.2
TeacherWideNet-B 27M 98.4
WideNet-L 38M 98.8
BaselineSwitch-B 9M 97.9
Switch-L 13M 98.3
OursOneS-B 9M 98.1
OneS-L 13M 98.5
Table 2. Top-1 Accuracy on Cifar10 Ô¨Åne-tuning. We use our de-
fault knowledge gathering choice, SVD-KG, to gather the knowl-
edge during pre-training. That is, for OneS, we Ô¨Ånetune a dense
model without knowledge distillation.
WideNet [27]1as the platform. WideNet is an MoE-based
transformer with only one trainable transformer block. This
transformer block uses MoE instead of FFN layer to learn
the local representation. The main focus of this paper is to
verify the knowledge in the pre-trained MoE can be pre-
served in the dense student, so we use WideNet as our
teacher model to verify the effectiveness of our approach
in a more straightforward manner. Hyper-parameters For
1We try two different scales of WideNet ( i.e. WideNet-Base, WideNet-
Large) as our teacher, respectively.a fair comparison, we follow the data augmentation used in
teacher model: Inception-style pre-processing, Mixup [30],
RandAugment [4] and label smoothing [23, 29]. We use
LAMB [28] optimizer. Batch size and learning rate are set
as 4096 and 0.004, respectively. For the teacher model, all
settings of WideNet [27] are the same as reported in their
paper. Please note we freeze all trainable weights of the
teacher model ( i.e. WideNet) in the knowledge distillation
stage of OneS. For distillation hyper-parameters, we set 
as 0.25 and temperature Tas 1.0. Linear learning rate decay
is applied.
We also Ô¨Åne-tune our pre-trained student model on Cifar-
10. The setting is the same as ViT and WideNet. We use
SGD optimizer with momentum. Following existing works,
label smoothing and warm-up are removed. Please see Ap-
pendix for other training details.
4.1.1 Results on ImageNet
We report the top-1 accuracy and MoE beneÔ¨Åts on Ima-
geNet in Table 1. In this table, as we deÔ¨Åned in Eq. 8,
the MoE beneÔ¨Åts means how much improvement the dense
model preserved, after knowledge integration. First, after
investigating four different KG methods, the SVD-based in-
tegration method performs best. Therefore, we set the SVD-
based method as the default choice in the following experi-
ments. Top-K-based integration method performs compa-
6

--- PAGE 7 ---
Model #para FLOPs Speedup SQuAD1.1 SQuAD2.0 MNLI SST-2 Avg BeneÔ¨Åts(%)
Teacher WideNet 26M 2:4 1:0 89.6/82.7 80.6/77.4 82.6 91.1 84.71 -
BaselineALBERT 12M 1:0 3:7 89.3/82.3 80.0/77.1 81.5 90.3 84.03 0.0
Distill 12M 1:0 3:7 89.4/82.7 79.8/76.6 81.9 90.7 84.21 26.5
Switch 12M 1:0 3:7 89.5/82.6 79.9/77.0 82.0 90.3 84.20 25.0
Ours OneS 12M 1:0 3:7 89.7/83.0 80.2/77.1 82.3 91.2 84.63 88.2
Table 3. Results of Ô¨Åne-tuning on MNLI, SST-2, and two versions of SQuAD datasets. The two numbers of F1 and EM for each SQuAD
dataset are Ô¨Årst averaged. The FLOPs here means the Ô¨Çoating-point operations in FFN layer or MoE layer. We only report the FLOPs in
FFN or MoE layer because FLOPs at other layers are the same. We also compare the inference speed on TPU v3-8 to show the usability of
dense model. The beneÔ¨Åts here is the MoE beneÔ¨Åts we proposed in Eq. 8.
rably with SVD-based method at large scale but slightly
worse at base level. We suggest the reason is large model
has larger capacity and is more robust to sparse column
drop. Also, we observe that OneS-L-SVD achieves 78:4%
top-1 accuracy on ImageNet with only 15M parameters.
Compared with the strongest Switch-L, our model has 0:6
points improvement. Compared with the teacher model,
OneS-L-SVD outperforms WideNet-B by 0:9%with half
of the parameters. As a Ô¨Ånal result, OneS-L-SVD achieves
comparable performance with ViT-B with only 17% train-
able parameters. More importantly, in [27], without MoE,
WideNet-L can obtain only achieve 76:9%top-1 accu-
racy. Our OneS has the totally same architecture as that,
but we can achieve 78:4%accuracy. That is, our OneS-
L-SVD preserves 61:7%improvement ( i.e. MoE beneÔ¨Åts)
from WideNet. In addition, our OneS-B-SVD achieves 57:7
MoE beneÔ¨Åts, which outperforms the strongest baseline ( i.e.
Switch) by 23:1points. Such results show the effectiveness
of knowledge integration.
4.1.2 Results on Cifar10
We further Ô¨Åne-tune our dense student model, OneS on Ci-
far10 in this part. As shown in Table 2, our OneS-L still out-
performs our baselines, Switch-B and Switch-L, by 0:3%
and0:6%respectively. The OneS-L can even achieve com-
parable performance with WideNet-B with 0:33trainable
parameters. OneS-B also achieves better performance than
Switch-B due to knowledge gathering. In summary, the re-
sults on Cifar10 show the improvement of pre-training on
ImageNet can propagate to the downstream task.
4.2. Natural Language Processing
Experimental settings Similar to experiments on com-
puter vision tasks, we still have two stages of training in
natural language processing. The difference is, following
existing works [6, 11, 27], we focus on the performance
of downstream tasks instead of pre-training. Datasets We
use English Wikipedia [6] and BOOKCORPUS [33] as our
pre-training corpus. For Ô¨Åne-tuning, we evaluate our workon General Language Understanding Evaluation (GLUE)
benchmark [26], two different versions of the Stanford
Question Answering (SQuAD) dataset [17, 18]. For GLUE
experiments, we report median over 5 runs following exist-
ing works [11,27]. Baselines Similar to the experiments on
computer vision, we still select Distill and Switch as our di-
rect baselines, although our work is the Ô¨Årst one focusing on
this task. The student model here also has the same archi-
tecture as ALBERT except for the individual layer normal-
ization [27]. Therefore, another baseline is ALBERT. We
expect our OneS can outperform ALBERT with the almost
same architecture, a comparable number of parameters, and
the same pre-training dataset. Hyper-parameters After ini-
tialization, we further train OneS by a linear combination of
masked language modeling loss, sentence order prediction
loss, and soft knowledge distillation loss. Following [20],
we only feed the logits of masked language modeling loss to
Ldistill . We still freeze all trainable weights of the teacher
MoE model (WideNet) in the training stage of OneS. is
set as 0:75, andis0:25in this part. The ablation study of
these settings can be found in Appendix D. Other detailed
hyper-parameters can be found in Appendix B.2.
4.2.1 Results on NLU benchmarks
After pre-training, we Ô¨Åne-tune our OneS without distilla-
tion loss. Such a setting is different from existing work on
distilling language models. The reason is, one of our goals
is to obtain an easy-to-use model without expert routing. If
we still have an MoE teacher, the downstream Ô¨Åne-tuning
still requires complicated hardware and software co-design
for MoE. The results on downstream natural language un-
derstanding tasks are shown in Table 3. In general, we
can observe OneS outperforms ALBERT and baselines ( i.e.
Distill and Switch) on all tasks by achieving 88:2%MoE
beneÔ¨Åts. For instance, on four tasks, OneS surpass Switch
by0:42on average. Also, we achieve 53:2%and51:7%
MoE beneÔ¨Åts over Switch and Distill, respectively. On a
few tasks, e.g. SQuAD1.1 and SST-2, OneS can even out-
perform the teacher MoE model, WideNet. We suggest that
7

--- PAGE 8 ---
Model ImageNet
OneS-B 75.7
w/o KG 73.8
w/o KD 75.0
w/o KG & KD 72.8
OneS-L 78.4
w/o KG 77.3
w/o KD 77.6
w/o KG & KD 76.9
Table 4. Top-1 Accuracy of ablation study on ImageNet to inves-
tigate the contributions of knowledge gathering (KG) and knowl-
edge distillation (KD). The KG here is using SVD-KG, and the
KD here is using soft-distillation, as we found they perform better
by investigation.
MoE model tends to overÔ¨Åt on small datasets. OneS has
MoE‚Äôs knowledge but a dense structure, so that the beneÔ¨Åts
from pre-training can propagate to downstream tasks easier.
Compared with MoE model, another strength of our
OneS is the inference speed. The reason why MoE is so
slow is, MoE model has gating function and sparse einsum
operators due to conditional computation, which would re-
duce the computational efÔ¨Åciency. However, our model can
achieve 3:7inference speedup. Please note that WideNet
only uses 2:4FLOPs at MoE layers. For other layers,
WideNet has the same computation cost as OneS or AL-
BERT, so global FLOPs is less than 2:4times of OneS.
Therefore, although one reason why OneS can achieve such
high efÔ¨Åciency is less computation, another important rea-
son is, the dense model is more hardware-friendly than
sparse MoE model.
4.3. Ablation study
We conduct four sets of ablation studies in this work.
The Ô¨Årst set is to investigate the contributions of knowl-
edge gathering and knowledge distillation. As shown in
Table 4, there is a signiÔ¨Åcant performance drop without
knowledge gathering, which shows the knowledge included
in pre-trained sparse model is critical to improve the student
model‚Äôs performance. For the model without KD, in this
experiment, we adopt the Lmain in Eq. 18 as the only loss
function. We can see the knowledge distillation is helpful,
as the prediction of teacher can instruct the student to mine
knowledge in noisy weights gathered. In addition, when the
dense model does not gather knowledge from MoE, the KD
enables the training process of the lite model ( i.e. OneS-B)
more stable. For the large model, removing both knowledge
gathering and knowledge distillation will also harm the per-
formance.
Since we conduct two stages of training in our frame-
work, the total training steps of OneS are more than the
dense model trained from scratch without distillation. The
second set of ablation study is to verify whether the im-
450 525 600
Global Training Epochs77.277.477.677.878.078.278.4Top-1 Accuracy on ImageNet
WideNet-L w/o MoE
OneStu-LFigure 3. Top-1 Accuracy of ablation on ImageNet to investigate
the contribution of more global training epochs.
provement of our model is from more training iterations.
To this end, we train the OneS without KG and KD from
scratch for comparable global training epochs. We use
OneS-L as a platform for this set of experiments because
we observe the unstable training of OneS-B without both
KG and KD. As shown in Figure 3, when training with com-
parable global epochs, our OneS outperforms baselines by
a large margin consistently. Also, when scaling to more
epochs, WideNet without MoE stops to improve, but our
OneS can still obtain beneÔ¨Åts from more training. We also
investigate two types of knowledge distillation approaches,
soft distillation [9] and hard-label distillation [24]. The last
set is to ablate the SVD ratio . Please see Appendix C and
Appendix D for details.
5. Conclusion and Future Work
In this paper, inspired by the human education model, we
propose knowledge integration, a new task to combine the
effectiveness of the MoE model and the usability of dense
model. As the Ô¨Årst work focusing on this task, our solu-
tion is integrating knowledge in two steps ( i.e. knowledge
gathering and knowledge distillation). Knowledge gather-
ing focuses on gathering knowledge from pre-trained MoE
to initialize dense student models. Knowledge distillation is
to further reÔ¨Åne the dense one. Experiments show that our
OneS achieves outstanding effectiveness and efÔ¨Åciency on
computer vision and natural language processing tasks. It
is noteworthy our OneS can even preserve 88:2%beneÔ¨Åts
from MoE with 0:42FLOPs per MoE or FFN layer, 3:7
inference speedup, and 46% trainable parameters.
In the future, we plan to explore more advanced knowl-
edge gathering and distillation approaches to better inte-
grate knowledge of MoE into a dense student. In addition,
although most recent MoE-based transformer are using the
same architecture for different experts, it is valuable to in-
vestigate the approach to gather knowledge from experts
with different architectures. Last, we expect to adapt our
approach to the extremely huge MoE model like GLaM [7].
8

--- PAGE 9 ---
References
[1] Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du,
Srinivasan Iyer, Ramakanth Pasunuru, et al. EfÔ¨Åcient large
scale language modeling with mixtures of experts. arXiv
preprint arXiv:2112.10684 , 2021. 5
[2] John D Bransford, Ann L Brown, and Rodney R Cocking.
How people learn: Brain, mind, experience, and school. Na-
tional Academy Press, 1999. 1
[3] Patrick CHen, Hsiang-Fu Yu, Inderjit S Dhillon, and Cho-Jui
Hsieh. DRONE: Data-aware low-rank compression for large
NLP models. In A. Beygelzimer, Y . Dauphin, P. Liang, and
J. Wortman Vaughan, editors, Advances in Neural Informa-
tion Processing Systems , 2021. 4
[4] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le. Randaugment: Practical automated data augmenta-
tion with a reduced search space. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops , pages 702‚Äì703, 2020. 6
[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition , pages 248‚Äì255. Ieee, 2009. 5
[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova. BERT: Pre-training of deep bidirectional trans-
formers for language understanding. In Proceedings of the
2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) , pages
4171‚Äì4186, Minneapolis, Minnesota, June 2019. Associa-
tion for Computational Linguistics. 7, 12
[7] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi
Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: EfÔ¨Åcient
scaling of language models with mixture-of-experts. arXiv
preprint arXiv:2112.06905 , 2021. 8
[8] William Fedus, Barret Zoph, and Noam Shazeer. Switch
transformers: Scaling to trillion parameter models with sim-
ple and efÔ¨Åcient sparsity. arXiv preprint arXiv:2101.03961 ,
2021. 1, 2, 3, 5, 12, 13
[9] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015. 2, 3, 5, 8, 12
[10] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5
[11] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin
Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite
bert for self-supervised learning of language representations.
arXiv preprint arXiv:1909.11942 , 2019. 7, 12
[12] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
Shazeer, and Zhifeng Chen. Gshard: Scaling giant models
with conditional computation and automatic sharding. arXiv
preprint arXiv:2006.16668 , 2020. 2, 12
[13] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal,
and Luke Zettlemoyer. Base layers: Simplifying training of
large, sparse models. In ICML , 2021. 3[14] Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You.
Sparse-mlp: A fully-mlp architecture with conditional com-
putation. arXiv preprint arXiv:2109.02008 , 2021. 1, 12
[15] Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han,
Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng Li,
Maosong Sun, et al. Knowledge inheritance for pre-trained
language models. arXiv preprint arXiv:2105.13880 , 2021.
13
[16] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia
Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan,
Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advanc-
ing mixture-of-experts inference and training to power next-
generation ai scale. arXiv preprint arXiv:2201.05596 , 2022.
2
[17] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what
you don‚Äôt know: Unanswerable questions for SQuAD. In
Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers) ,
pages 784‚Äì789, Melbourne, Australia, July 2018. Associa-
tion for Computational Linguistics. 7
[18] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. SQuAD: 100,000+ questions for machine com-
prehension of text. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing , pages
2383‚Äì2392, Austin, Texas, Nov. 2016. Association for Com-
putational Linguistics. 7
[19] Carlos Riquelme Ruiz, Joan Puigcerver, Basil Mustafa,
Maxim Neumann, Rodolphe Jenatton, Andr ¬¥e Susano Pinto,
Daniel Keysers, and Neil Houlsby. Scaling vision with
sparse mixture of experts. In A. Beygelzimer, Y . Dauphin, P.
Liang, and J. Wortman Vaughan, editors, Advances in Neural
Information Processing Systems , 2021. 12
[20] Victor Sanh, Lysandre Debut, Julien Chaumond, and
Thomas Wolf. Distilbert, a distilled version of bert: smaller,
faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 ,
2019. 7
[21] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy
Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outra-
geously large neural networks: The sparsely-gated mixture-
of-experts layer. arXiv preprint arXiv:1701.06538 , 2017. 2
[22] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi
Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and
Hua Wu. Ernie: Enhanced representation through knowledge
integration. arXiv preprint arXiv:1904.09223 , 2019. 13
[23] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna. Rethinking the inception archi-
tecture for computer vision. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition , pages
2818‚Äì2826, 2016. 6
[24] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herve Jegou. Training
data-efÔ¨Åcient image transformers & distillation through at-
tention. In International Conference on Machine Learning ,
volume 139, pages 10347‚Äì10357, July 2021. 5, 8, 12
[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural
information processing systems , pages 5998‚Äì6008, 2017. 12
9

--- PAGE 10 ---
[26] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel Bowman. GLUE: A multi-task
benchmark and analysis platform for natural language un-
derstanding. In Proceedings of the 2018 EMNLP Workshop
BlackboxNLP: Analyzing and Interpreting Neural Networks
for NLP , pages 353‚Äì355, Brussels, Belgium, Nov. 2018. As-
sociation for Computational Linguistics. 7
[27] Fuzhao Xue, Ziji Shi, Futao Wei, Yuxuan Lou, Yong Liu,
and Yang You. Go wider instead of deeper. ArXiv ,
abs/2107.11817, 2021. 1, 6, 7, 12
[28] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv
Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Dem-
mel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimiza-
tion for deep learning: Training bert in 76 minutes. arXiv
preprint arXiv:1904.00962 , 2019. 6
[29] Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi
Feng. Revisiting knowledge distillation via label smoothing
regularization. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3903‚Äì
3911, 2020. 6
[30] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimiza-
tion. arXiv preprint arXiv:1710.09412 , 2017. 6
[31] Zhengyan Zhang, Yuxian Gu, Xu Han, Shengqi Chen, Chao-
jun Xiao, Zhenbo Sun Yuan Yao, Fanchao Qi, Jian Guan, Pei
Ke, Yanzheng Cai, et al. Cpm-2: Large-scale cost-effective
pre-trained language models. AI Open , 2022. 13
[32] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,
Maosong Sun, and Jie Zhou. MoeÔ¨Åcation: Conditional com-
putation of transformer models for efÔ¨Åcient inference. arXiv
preprint arXiv:2110.01786 , 2021. 13
[33] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov,
Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Align-
ing books and movies: Towards story-like visual explana-
tions by watching movies and reading books. In Proceed-
ings of the IEEE international conference on computer vi-
sion, pages 19‚Äì27, 2015. 7
10

--- PAGE 11 ---
Appendix
A. Knowledge Gathering Noise Analysis
We are to discuss and analyze the induced noise during
SVD knowledge gathering in this section.
Given one MoE layer MoE(), the target of SVD-KG is
to integrate its knowledge to a dense layer g()in the stu-
dent model. For brevity, we set every expert and the dense
student layer as the single linear layer. There are Eexperts
in MoE layer:ff1;:::;fEgwith weightsfW1
f;:::;WE
fg
and biasfb1
f;:::;bE
fg. The dense student layer is gwith
weightsWgand biasbg. According to Eq. 1, the MoE layer
can be written as:
MoE(x) =EX
i=1G(x)iei(x)
=EX
i=1pihi(Wi
fx+bi
f)(19)
wherepis the routing score of router, his an index vector.
For the selected experts, hi= 1, andhi= 0 for other un-
selected experts. Due to the load balance loss during MoE
training, we can assume pi1:0whenhi= 1. Then, we
can approximate MoE layer by SVD:
MoE(x) =EX
i=1pihi(Ui
fSi
fVi
fTx+bi
f)
EX
i=1hi(Ui
fKiSi
fKiVi
fT
Kix+bi
f)
EX
i=1hiKiX
j=1uij
fKisij
fKivij
fT
Kix+EX
i=1hibi
f
(20)
whereKiis the selected rank of i-th expert.
According to Eq. 16, g()can be formulated as:
g(x) =EX
i=1KiX
j=1uij
fKisij
fKivij
fT
Kix+1
EEX
i=1bi
f (21)
For brevity, to analyze, we assume MoE layer here is to
select the 1-st expert, and then the MoE layer can be written
as:
MoE(x)K1X
j=1u1j
fK1s1j
fK1v1j
fT
K1x+b1
f (22)
and the student dense layer:g(x) =K1X
j=1u1j
fK1s1j
fK1v1j
fT
K1x+b1
f
+EX
i=2KiX
j=1uij
fK1sij
fK1vij
fT
K1x
+1
EEX
i=2bi
f E 1
Eb1
f(23)
Since the non-selected experts do not interact with the
current input token x, we assume, for the non-selected ex-
perts, we let 1=fi(x)and1N(1;2
1)and2=bi
fx
and2N(2;2
2)According to Eq. 14, g(x)can be writ-
ten as:
g(x) =K1X
j=1u1j
fK1s1j
fK1v1j
fT
K1x+[(E 1)1 E 1
E2]
(24)
The low-rank approximation ensuresPK1
j=1u1j
fK1s1j
fK1v1j
fT
K1+b1
fcover most informative
knowledge in the selected expert, and noise reduced
linearly along . When we are integrating knowledge from
experts, a smaller is required to reduce noise.
B. Hyper-parameters
B.1. Computer Vision
Table 5. Hyper-parameters on ImageNet pre-training and Cifar10
Ô¨Ånetuning.andare from Eq. 18 and Eq. 14
Parameter ImageNet Cifar10
Epoch 300 100
Warmup Epochs 30 0
Batch Size 4096 512
Learning rate 0.004 0.03
Weight Decay 0.1 0
Dropout 0.1 0.1
Label smoothing 0.1 0
Mixup prob. 0.5 0.5
 0.25 -
 0.75 -
Most hyper-parameters are set following existing works
(e.g. ViT, WideNet). The main difference is the learning
rate. Since we are training from a dense model initialized by
a MoE model. We observe that a large learning rate harms
accuracy. We, therefore, set a smaller learning rate as 0.004
(0.01 in WideNet).
11

--- PAGE 12 ---
B.2. Natural Language Processing
Table 6. Hyper-parameters on NLP downstream tasks Ô¨Åne-tuning.
Parameter SQuAD1.1/2.0 MNLI SST2
Steps 3649/8144 10000 5234
Warmup 365/814 1000 314
Batch Size 48 128 128
LR 5e-5/3e-5 3e-5 4e-5
Dropout 0/0 0 0
Max Length 384/512 512 512
We follow the hyper-parameters in [6, 11, 27] and the Ô¨Å-
nal hyper-parameters are reported in Table 6.
C. Hard-label Distillation
C.1. Method
The hard-label distillation takes the hard decision of the
teacher as a true label. In other words, it treats the knowl-
edge distillation task as a typical classiÔ¨Åcation task, super-
vised by both the prediction from the teacher and ground
truth.
Lhard
distill = L CE(!(zs);argmax(zt)) (25)
whereLCEis the cross-entropy loss, argmax is used to
obtain the hard label of teacher‚Äôs prediction.
C.2. Evaluation
Table 7. Top-1 Accuracy of different knowledge distillation ap-
proaches On ImageNet.
Approach ImageNet
Soft distillation 75.7
Hard-label distillation 75.4
We investigate two types of knowledge distillation
approaches, soft distillation [9] and hard-label distilla-
tion [24], as we introduced in Section 3.3. The results is
reported in Table 7. We observe that hard-label distilla-
tion can achieve comparable performance with soft distil-
lation. Since soft distillation is popular in more tasks and
has slightly better performance, we suggest using soft dis-
tillation as the default choice.
D. Ablation Study on SVD Ratio
We also conduct ablation study on SVD ratio , which
denotes the ratio of selected k. As shown in Figure 4, when
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
SVD Ratio75.275.375.475.575.675.7Top-1 Accuracy on ImageNet
Figure 4. Top-1 Accuracy of ablation on ImageNet to ablate the
SVD ratio.
= 0:75, OneS-B achieves sweet point.
E. Experimental JustiÔ¨Åcation for Less Knowl-
edge in Bias
Table 8. Top-1 Accuracy of MoE model without bias.
Approach ImageNet
WideNet-B 77.5
WideNet-B w/o bias 77.3
We re-trained the teacher MoE model ( i.e. WideNet-B)
without bias in MoE layer. As shown in Table 8, we found
that there is no obvious performance drop. That is, the bias
in MoE layer has little impact on results, which means there
is less knowledge than weights.
F. Related Work
F.1. Mixture-of-Experts
MoE has shown promising results on various tasks. Re-
cent works scaled a dense model to a sparse one by MoE.
Faster convergence speed of MoE can save the global com-
putation cost. One typical way to use MoE is, by replacing
the FFN layer in transformer [25] by an MoE layer. Lep-
ikhin et al. [12] Ô¨Årst scale machine translation transformer
model to 600 million parameters using automatic sharding.
After that, Fedus et al . [8] further scales the transformer
to trillion parameter models with simple and efÔ¨Åcient spar-
sity and shows promising results on natural language under-
standing. In computer vision, ViT-MoE [19] matches SoTA
performance on ImageNet using 14:7billion of parameters,
while requiring as little as half of the computation at infer-
ence time. Recent work [14] investigated the MoE usage on
12

--- PAGE 13 ---
MLP-Mixer, which also achieved better effectiveness and
efÔ¨Åciency than the dense model. Instead of scaling up, this
work uses and Ô¨Åxes the pre-trained MoE model. The core
target is to combine the effectiveness of MoE and the us-
ability of dense model.
F.2. Knowledge Integration
Knowledge inheritance [15] is related to our knowledge
integration. Knowledge inheritance usually inherits knowl-
edge from small pre-trained model and then speed-up the
training of large models. Contrastively, our work is inte-
grating knowledge from a large MoE model. Sun et al. [22]
proposed to integrate knowledge by using knowledge mask-
ing strategies. Please note our knowledge integration is dif-
ferent from theirs. Instead of a self-supervised learning ap-
proach to integrate knowledge from data, our work is to in-
tegrate knowledge from pre-trained MoE. There are also a
few works focusing on inheriting knowledge from a dense
model to initialize a MoE model, which can be seen as an in-
verse process of ours. For instance, Zhang et al. [31] dupli-
cated dense model multiple times to initialize MoE models.
Zhang et al. [32] proposed MoEÔ¨Åcation. The proposed ap-
proach is to inherit knowledge from a dense model and ob-
tain an MoE model with comparable parameters to reduce
the computation cost. In general, MoEÔ¨Åcation is a sparsi-
Ô¨Åcation approach. In Switch Transformer [8], authors tried
to initialize trainable parameters except for MoE layers to
speed-up MoE training, although their main purpose is to
scale transformer to trillions of parameters.
13
