# Học các Biểu diễn Phân tích trong Mô hình Hỗn hợp Chuyên gia Sâu

David Eigen1;2Marc'Aurelio Ranzato1Ilya Sutskever1
1Google, Inc.
2Khoa Khoa học Máy tính, Viện Courant, NYU
deigen@cs.nyu.edu ranzato@fb.com ilyasu@google.com

Tóm tắt
Các mô hình Hỗn hợp Chuyên gia kết hợp đầu ra của nhiều mạng "chuyên gia", mỗi mạng chuyên môn hóa cho một phần khác nhau của không gian đầu vào. Điều này được thực hiện bằng cách huấn luyện một mạng "cổng" ánh xạ mỗi đầu vào thành một phân phối trên các chuyên gia. Các mô hình như vậy cho thấy triển vọng để xây dựng các mạng lớn hơn vẫn rẻ về mặt tính toán tại thời điểm kiểm tra, và có thể song song hóa hơn tại thời điểm huấn luyện. Trong công trình này, chúng tôi mở rộng mô hình Hỗn hợp Chuyên gia thành một mô hình chồng tầng, Hỗn hợp Chuyên gia Sâu, với nhiều bộ cổng và chuyên gia. Điều này làm tăng cấp số nhân số lượng chuyên gia hiệu quả bằng cách liên kết mỗi đầu vào với một tổ hợp chuyên gia ở mỗi tầng, nhưng vẫn duy trì kích thước mô hình khiêm tốn. Trên một phiên bản được dịch chuyển ngẫu nhiên của bộ dữ liệu MNIST, chúng tôi thấy rằng Hỗn hợp Chuyên gia Sâu tự động học cách phát triển các chuyên gia phụ thuộc vào vị trí ("ở đâu") tại tầng đầu tiên, và các chuyên gia dành riêng cho lớp ("là gì") tại tầng thứ hai. Ngoài ra, chúng tôi thấy rằng các tổ hợp khác nhau được sử dụng khi áp dụng mô hình vào một bộ dữ liệu các âm đơn lẻ của giọng nói. Những điều này chứng minh việc sử dụng hiệu quả tất cả các tổ hợp chuyên gia.

1 Giới thiệu
Các mạng sâu đã đạt được hiệu suất rất tốt trong nhiều nhiệm vụ khác nhau, ví dụ [10, 5, 3]. Tuy nhiên, một hạn chế cơ bản của các kiến trúc này là toàn bộ mạng phải được thực thi cho tất cả các đầu vào. Gánh nặng tính toán này đặt ra giới hạn kích thước mạng. Một cách để mở rộng quy mô các mạng này trong khi giữ chi phí tính toán thấp là tăng tổng số lượng tham số và đơn vị ẩn, nhưng chỉ sử dụng một phần nhỏ của mạng cho mỗi đầu vào cụ thể. Sau đó, học một hàm ánh xạ rẻ về mặt tính toán từ đầu vào đến các phần thích hợp của mạng.

Mô hình Hỗn hợp Chuyên gia [7] là một phiên bản liên tục của điều này: Một mạng cổng được học kết hợp đầu ra của N mạng "chuyên gia" để tạo ra một đầu ra cuối cùng. Mặc dù mô hình này không tự mình đạt được những lợi ích tính toán được nêu ở trên, nó cho thấy triển vọng như một bước đệm hướng tới các mạng có thể thực hiện mục tiêu này.

Trong công trình này, chúng tôi mở rộng Hỗn hợp Chuyên gia để sử dụng một mạng cổng khác nhau ở mỗi tầng trong một mạng đa tầng, tạo thành một Hỗn hợp Chuyên gia Sâu (DMoE). Điều này làm tăng số lượng chuyên gia hiệu quả bằng cách giới thiệu một số lượng đường dẫn cấp số nhân thông qua các tổ hợp khác nhau của chuyên gia ở mỗi tầng. Bằng cách liên kết mỗi đầu vào với một tổ hợp như vậy, mô hình của chúng tôi sử dụng các tập con khác nhau của các đơn vị cho các đầu vào khác nhau. Do đó, nó có thể vừa lớn vừa hiệu quả cùng một lúc.

Chúng tôi chứng minh hiệu quả của phương pháp này bằng cách đánh giá nó trên hai bộ dữ liệu. Sử dụng một bộ dữ liệu MNIST bị lắc, chúng tôi chỉ ra rằng DMoE học cách phân tích các khía cạnh khác nhau của biểu diễn dữ liệu ở mỗi tầng (cụ thể là vị trí và lớp), sử dụng hiệu quả tất cả các đường dẫn. Chúng tôi cũng thấy rằng tất cả các tổ hợp được sử dụng khi áp dụng mô hình của chúng tôi vào một bộ dữ liệu các âm đơn lẻ của giọng nói.

Marc'Aurelio Ranzato hiện làm việc tại Facebook AI Group.

2 Công trình Liên quan
Một Hỗn hợp Chuyên gia (MoE) tiêu chuẩn [7] học một tập hợp các mạng chuyên gia fi cùng với một mạng cổng g. Mỗi fi ánh xạ đầu vào x thành C đầu ra (một cho mỗi lớp c = 1, ..., C), trong khi g(x) là một phân phối trên các chuyên gia i = 1, ..., N có tổng bằng 1. Đầu ra cuối cùng sau đó được cho bởi Phương trình 1

FMoE(x) = ∑(i=1 to N) gi(x)softmax(fi(x))                 (1)
         = ∑(i=1 to N) p(ei|x)p(c|ei, x) = p(c|x)        (2)

Điều này cũng có thể được xem như một mô hình xác suất, trong đó xác suất cuối cùng trên các lớp được cận biên hóa trên việc lựa chọn chuyên gia: thiết lập p(ei|x) = gi(x) và p(c|ei, x) = softmax(fi(x)), chúng ta có Phương trình 2.

Một tích của chuyên gia (PoE) [6] tương tự, nhưng thay vào đó kết hợp xác suất logarit để tạo thành một tích:

FPoE(x) ∝ ∏(i=1 to N) softmax(fi(x)) = ∏(i=1 to N) pi(c|x)    (3)

Cũng liên quan chặt chẽ đến công trình của chúng tôi là Hỗn hợp Chuyên gia Phân cấp [9], học một hệ thống phân cấp các mạng cổng trong cấu trúc cây. Đầu ra của mỗi mạng chuyên gia tương ứng với một lá trong cây; các đầu ra sau đó được trộn theo trọng số cổng tại mỗi nút.

Mô hình của chúng tôi khác với mỗi mô hình trong ba mô hình này vì nó động lắp ráp một tổ hợp chuyên gia phù hợp cho mỗi đầu vào. Đây là một thể hiện của khái niệm tính toán có điều kiện do Bengio [1] đưa ra và được xem xét trong một thiết lập ngẫu nhiên một tầng bởi Bengio, Leonard và Courville [2]. Bằng cách điều kiện hóa các mạng cổng và chuyên gia của chúng tôi trên đầu ra của tầng trước, mô hình của chúng tôi có thể biểu diễn một số lượng chuyên gia hiệu quả lớn cấp số nhân.

3 Phương pháp
Để mở rộng MoE thành DMoE, chúng tôi giới thiệu hai bộ chuyên gia với các mạng cổng (g1, f1i) và (g2, f2j), cùng với một tầng tuyến tính cuối cùng f3 (xem Hình 1). Đầu ra cuối cùng được tạo ra bằng cách kết hợp các hỗn hợp ở mỗi tầng:

z1 = ∑(i=1 to N) g1i(x)f1i(x)
z2 = ∑(j=1 to M) g2j(z1)f2j(z1)
F(x) = z3 = softmax(f3(z2))

Chúng tôi thiết lập mỗi fli thành một ánh xạ tuyến tính duy nhất với chỉnh lưu, và mỗi gli thành hai tầng ánh xạ tuyến tính với chỉnh lưu (nhưng với ít đơn vị ẩn); f3 là một tầng tuyến tính duy nhất. Xem Phần 4 để biết chi tiết.

Chúng tôi huấn luyện mạng bằng cách sử dụng hạ gradient ngẫu nhiên (SGD) với một ràng buộc bổ sung trên các gán cổng (được mô tả bên dưới). Chỉ SGD dẫn đến một cực tiểu cục bộ thoái hóa: Các chuyên gia ở mỗi tầng hoạt động tốt nhất cho vài ví dụ đầu tiên cuối cùng lấn át các chuyên gia còn lại. Điều này xảy ra vì các ví dụ đầu tiên làm tăng trọng số cổng của các chuyên gia này, điều này lần lượt khiến chúng được chọn với trọng số cổng cao thường xuyên hơn. Điều này khiến chúng được huấn luyện nhiều hơn, và trọng số cổng của chúng tăng lên một lần nữa, cứ thế ad infinitum.

Để chống lại điều này, chúng tôi đặt một ràng buộc trên các gán cổng tương đối cho mỗi chuyên gia trong quá trình huấn luyện. Cho Gli(t) = ∑(t'=1 to t) gli(xt') là tổng gán chạy cho chuyên gia i của tầng l tại bước t, và cho Gl(t) = 1/N ∑(i=1 to N) Gli(t) là trung bình của chúng (ở đây, xt' là ví dụ huấn luyện tại bước t'). Sau đó cho mỗi chuyên gia i, chúng tôi thiết lập gli(xt) = 0 nếu Gli(t) - Gl(t) > m cho một ngưỡng biên m, và chuẩn hóa lại

Hình 1: (a) Hỗn hợp Chuyên gia; (b) Hỗn hợp Chuyên gia Sâu với hai tầng.

phân phối gl(xt) để tổng bằng 1 trên các chuyên gia i. Điều này ngăn chặn các chuyên gia bị sử dụng quá mức ban đầu, dẫn đến các gán cân bằng. Sau khi huấn luyện với ràng buộc tại chỗ, chúng tôi gỡ bỏ nó và tiếp tục huấn luyện trong giai đoạn tinh chỉnh thứ hai.

4 Thí nghiệm
4.1 MNIST bị lắc
Chúng tôi đã huấn luyện và kiểm tra mô hình của chúng tôi trên MNIST với các phép dịch chuyển ngẫu nhiên đồng nhất 4 pixel, dẫn đến các hình ảnh thang độ xám có kích thước 36×36. Như đã giải thích ở trên, mô hình được huấn luyện để phân loại chữ số thành mười lớp.

Đối với nhiệm vụ này, chúng tôi thiết lập tất cả f1i và f2j thành các mô hình tuyến tính một tầng với chỉnh lưu, f1i(x) = max(0, W1ix + b1i), và tương tự cho f2j. Chúng tôi thiết lập f3 thành một tầng tuyến tính, f3(z2) = W3z2 + b3.

Chúng tôi thay đổi số lượng đơn vị ẩn đầu ra của f1i và f2j giữa 20 và 100. Đầu ra cuối cùng từ f3 có 10 đơn vị (một cho mỗi lớp).

Các mạng cổng g1 và g2 mỗi cái được cấu thành từ hai tầng tuyến tính + chỉnh lưu với 50 hoặc 20 đơn vị ẩn, và 4 đơn vị đầu ra (một cho mỗi chuyên gia), tức là g1(x) = softmax(B max(0, Ax + a) + b), và tương tự cho g2.

Chúng tôi đánh giá hiệu quả của việc sử dụng một hỗn hợp ở tầng thứ hai bằng cách so sánh với việc chỉ sử dụng một chuyên gia cố định duy nhất ở tầng thứ hai, hoặc nối các đầu ra của tất cả các chuyên gia. Lưu ý rằng đối với một hỗn hợp với h đơn vị ẩn, mô hình nối tương ứng có Nh đơn vị ẩn. Do đó, chúng tôi mong đợi mô hình nối sẽ hoạt động tốt hơn hỗn hợp, và hỗn hợp sẽ hoạt động tốt hơn mạng đơn lẻ. Tốt nhất là hỗn hợp càng gần với ranh giới chuyên gia nối càng tốt. Trong mỗi trường hợp, chúng tôi giữ nguyên kiến trúc tầng đầu tiên (một hỗn hợp).

Chúng tôi cũng so sánh mô hình hai tầng với một mô hình một tầng trong đó tầng ẩn z1 được ánh xạ đến đầu ra cuối cùng thông qua tầng tuyến tính và softmax. Cuối cùng, chúng tôi so sánh với một mạng sâu kết nối đầy đủ có cùng tổng số tham số. Điều này được xây dựng bằng cách sử dụng cùng số lượng đơn vị tầng thứ hai z2, nhưng mở rộng số lượng đơn vị tầng đầu tiên z1 sao cho tổng số tham số bằng với DMoE (bao gồm các tham số mạng cổng của nó).

4.2 Giọng nói Âm đơn lẻ
Ngoài ra, chúng tôi chạy mô hình của chúng tôi trên một bộ dữ liệu các mẫu giọng nói âm đơn lẻ. Bộ dữ liệu này là một tập con ngẫu nhiên khoảng một triệu mẫu từ một cơ sở dữ liệu độc quyền lớn hơn của vài trăm giờ dữ liệu tiếng Anh Mỹ được thu thập bằng cách sử dụng Voice Search, Voice Typing và dữ liệu đọc [8]. Đối với các thí nghiệm của chúng tôi, mỗi mẫu được giới hạn trong 11 khung cách nhau 10ms, và có 40 thùng tần số. Mỗi đầu vào được đưa vào mạng như một vector 440 chiều. Có 40 lớp âm vị đầu ra có thể.

Chúng tôi huấn luyện một mô hình với 4 chuyên gia ở tầng đầu tiên và 16 ở tầng thứ hai. Cả hai tầng đều có 128 đơn vị ẩn. Các mạng cổng mỗi cái có hai tầng, với 64 đơn vị trong tầng ẩn. Như trước đây, chúng tôi đánh giá hiệu quả của việc sử dụng một hỗn hợp ở tầng thứ hai bằng cách so sánh với việc chỉ sử dụng một chuyên gia duy nhất ở tầng thứ hai, hoặc nối đầu ra của tất cả các chuyên gia.

5 Kết quả
5.1 MNIST bị lắc
Bảng 1 hiển thị lỗi trên các tập huấn luyện và kiểm tra cho mỗi kích thước mô hình (tập kiểm tra là tập kiểm tra MNIST với một phép dịch chuyển ngẫu nhiên duy nhất trên mỗi hình ảnh). Trong hầu hết các trường hợp, các chuyên gia chồng sâu hoạt động giữa các đường cơ sở chuyên gia đơn lẻ và nối trên tập huấn luyện, như mong đợi. Tuy nhiên, các mô hình sâu thường gặp phải tình trạng quá khớp: lỗi của hỗn hợp trên tập kiểm tra tệ hơn so với chuyên gia đơn lẻ cho hai trong số bốn kích thước mô hình. Đáng khuyến khích, DMoE hoạt động gần như tốt bằng một mạng kết nối đầy đủ (DNN) với cùng số lượng tham số, mặc dù mạng này áp đặt ít ràng buộc hơn trên cấu trúc của nó.

Trong Hình 2, chúng tôi hiển thị gán trung bình cho mỗi chuyên gia (tức là đầu ra cổng trung bình), cả theo phép dịch chuyển đầu vào và theo lớp. Tầng đầu tiên gán chuyên gia theo phép dịch chuyển, trong khi gán đồng nhất theo lớp. Ngược lại, tầng thứ hai gán chuyên gia theo lớp, nhưng đồng nhất theo phép dịch chuyển. Điều này cho thấy rằng hai tầng chuyên gia thực sự được sử dụng theo cách bổ sung, sao cho tất cả các tổ hợp chuyên gia đều hiệu quả. Các chuyên gia tầng đầu tiên trở nên chọn lọc với nơi chữ số xuất hiện, bất kể lớp thành viên của nó, trong khi các chuyên gia tầng thứ hai chọn lọc với lớp chữ số là gì, bất kể vị trí của chữ số.

Cuối cùng, Hình 3 hiển thị chín ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi tổ hợp chuyên gia. Các gán tầng đầu tiên chạy qua các hàng, trong khi tầng thứ hai chạy qua các cột. Lưu ý phép dịch chuyển của mỗi chữ số thay đổi theo hàng nhưng không đổi qua các cột, trong khi điều ngược lại đúng cho lớp của chữ số. Hơn nữa, các lớp dễ nhầm lẫn có xu hướng được nhóm lại với nhau, ví dụ 3 và 5.

Lỗi Tập Kiểm tra: MNIST bị lắc
Mô hình | Cổng Ẩn | Chuyên gia Đơn | DMoE | Nối Tầng2 | DNN
4×100×4×100 | 50×50 | 1.33 | 1.42 | 1.30 | 1.30
4×100×4×20 | 50×50 | 1.58 | 1.50 | 1.30 | 1.41
4×100×4×20 | 50×20 | 1.41 | 1.39 | 1.30 | 1.40
4×50×4×20 | 20×20 | 1.63 | 1.77 | 1.50 | 1.67
4×100 (một tầng) | 50 | 2.86 | 1.72 | 1.69 | –

Lỗi Tập Huấn luyện: MNIST bị lắc
Mô hình | Cổng Ẩn | Chuyên gia Đơn | DMoE | Nối Tầng2 | DNN
4×100×4×100 | 50×50 | 0.85 | 0.91 | 0.77 | 0.60
4×100×4×20 | 50×50 | 1.05 | 0.96 | 0.85 | 0.90
4×100×4×20 | 50×20 | 1.04 | 0.98 | 0.87 | 0.87
4×50×4×20 | 20×20 | 1.60 | 1.41 | 1.33 | 1.32
4×100 (một tầng) | 50 | 2.99 | 1.78 | 1.59 | –

Bảng 1: So sánh DMoE cho MNIST với các phép dịch chuyển ngẫu nhiên, với các đường cơ sở (i) chỉ sử dụng một chuyên gia tầng thứ hai, (ii) nối tất cả các chuyên gia tầng thứ hai, và (iii) một DNN có cùng tổng số tham số. Đối với cả (i) và (ii), các chuyên gia trong tầng đầu tiên được trộn để tạo thành z1. Các mô hình được chú thích với "# chuyên gia # đơn vị ẩn" cho mỗi tầng.

MNIST bị lắc: Mô hình Sâu Hai Tầng
theo Phép dịch chuyển | theo Lớp

Tầng 1
[Hiển thị gán tầng 1 với màu sắc khác nhau]

Tầng 2
[Hiển thị gán tầng 2 với màu sắc khác nhau]

MoE Một Tầng
không có
lắc—
[Hiển thị gán MoE một tầng]

Hình 2: Đầu ra cổng trung bình cho tầng đầu tiên và thứ hai, cả theo phép dịch chuyển và theo lớp. Màu sắc biểu thị trọng số cổng. Các phân phối theo phép dịch chuyển cho thấy gán cổng trung bình cho mỗi trong số bốn chuyên gia cho mỗi trong số 99 phép dịch chuyển có thể. Các phân phối theo lớp cho thấy gán cổng trung bình cho mỗi trong số bốn chuyên gia (hàng) cho mỗi trong số mười lớp (cột). Lưu ý tầng đầu tiên tạo ra các gán độc quyền theo phép dịch chuyển, trong khi tầng thứ hai gán chuyên gia theo lớp. Để so sánh, chúng tôi hiển thị các gán theo lớp của một MoE tiêu chuẩn được huấn luyện trên MNIST không có lắc, sử dụng 5 chuyên gia 20 đơn vị ẩn.

5.2 Giọng nói Âm đơn lẻ
Bảng 2 hiển thị lỗi trên các tập huấn luyện và kiểm tra. Như trường hợp với MNIST, lỗi của hỗn hợp trên tập huấn luyện nằm giữa hai đường cơ sở. Tuy nhiên, trong trường hợp này, hiệu suất tập kiểm tra gần như giống nhau cho cả hai đường cơ sở cũng như hỗn hợp.

Hình 4 hiển thị 16 ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi tổ hợp chuyên gia (chúng tôi chỉ hiển thị 4 chuyên gia ở tầng thứ hai do hạn chế về không gian). Như trước đây, các gán tầng đầu tiên chạy qua các hàng, trong khi tầng thứ hai chạy qua các cột. Mặc dù không dễ diễn giải như đối với MNIST, mỗi tổ hợp chuyên gia dường như xử lý một phần riêng biệt của đầu vào. Điều này được củng cố thêm bởi Hình 5, nơi chúng tôi vẽ số lượng gán trung bình cho mỗi tổ hợp chuyên gia. Ở đây, việc lựa chọn chuyên gia tầng thứ hai phụ thuộc ít vào việc lựa chọn chuyên gia tầng đầu tiên.

Lỗi Âm vị Tập Kiểm tra: Giọng nói Âm đơn lẻ
Mô hình | Cổng Ẩn | Chuyên gia Đơn | Chuyên gia Trộn | Nối Tầng2
4×128×16×128 | 64×64 | 0.55 | 0.55 | 0.56
4×128 (một tầng) | 64 | 0.58 | 0.55 | 0.55

Lỗi Âm vị Tập Huấn luyện: Giọng nói Âm đơn lẻ
Mô hình | Cổng Ẩn | Chuyên gia Đơn | Chuyên gia Trộn | Nối Tầng2
4×128×16×128 | 64×64 | 0.47 | 0.42 | 0.40
4×128 (một tầng) | 64 | 0.56 | 0.50 | 0.50

Bảng 2: So sánh DMoE cho dữ liệu giọng nói âm đơn lẻ. Ở đây cũng vậy, chúng tôi so sánh với các đường cơ sở chỉ sử dụng một chuyên gia tầng thứ hai, hoặc nối tất cả các chuyên gia tầng thứ hai.

Hình 3: Chín ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi tổ hợp chuyên gia, cho bộ dữ liệu mnist bị lắc. Các chuyên gia tầng đầu tiên ở các hàng, trong khi tầng thứ hai ở các cột.

6 Kết luận
Mô hình Hỗn hợp Chuyên gia Sâu mà chúng tôi xem xét là một bước hứa hẹn hướng tới việc phát triển các mô hình lớn, thưa thớt chỉ tính toán một tập con của chính chúng cho bất kỳ đầu vào nào. Chúng tôi thấy chính xác các gán cổng cần thiết để sử dụng hiệu quả tất cả các tổ hợp chuyên gia: đối với MNIST bị lắc, một phân tích thành phép dịch chuyển và lớp, và việc sử dụng khác biệt của mỗi tổ hợp cho dữ liệu giọng nói âm đơn lẻ. Tuy nhiên, chúng tôi vẫn sử dụng một hỗn hợp liên tục của đầu ra của các chuyên gia thay vì hạn chế vào vài chuyên gia hàng đầu — một phần mở rộng như vậy là cần thiết để hoàn thành mục tiêu chỉ sử dụng một phần nhỏ của mô hình cho mỗi đầu vào. Một phương pháp thực hiện điều này cho một tầng đơn lẻ đã được mô tả bởi Collobert et al. [4], có thể được điều chỉnh cho trường hợp đa tầng của chúng tôi; chúng tôi hy vọng sẽ giải quyết điều này trong công việc tương lai.

Lời cảm ơn
Các tác giả muốn cảm ơn Matthiew Zeiler vì những đóng góp của anh ấy về việc thực thi các ràng buộc cân bằng trong quá trình huấn luyện.

Ví dụ Gán Kết hợp
Tầng 1 | Tầng 2 chuyên gia cả hai tầng

Hình 4: 16 ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi tổ hợp chuyên gia cho dữ liệu giọng nói âm đơn lẻ. Các chuyên gia tầng đầu tiên ở các hàng, trong khi tầng thứ hai ở các cột. Mỗi mẫu được biểu diễn bởi 40 giá trị tần số của nó (trục dọc) và 11 khung liên tiếp (trục ngang). Đối với hình này, chúng tôi sử dụng bốn chuyên gia trong mỗi tầng.

Giọng nói Âm đơn lẻ: Gán Có điều kiện

Gán Chuyên gia Tầng 1 Trên Mỗi Điểm Dữ liệu | Tầng 2 Theo Nhãn

Kết hợp trộn tốt

thang màu [0] | thang màu [0]

Hình 5: Số lượng gán kết hợp cho bộ dữ liệu giọng nói âm đơn lẻ. Ở đây chúng tôi vẽ tích trung bình của trọng số cổng tầng đầu tiên và thứ hai cho mỗi tổ hợp chuyên gia. Chúng tôi chuẩn hóa mỗi hàng, để tạo ra một phân phối có điều kiện: Điều này cho thấy các gán cổng trung bình trong tầng thứ hai cho một gán tầng đầu tiên. Lưu ý các gán kết hợp được trộn tốt: Lựa chọn chuyên gia tầng thứ hai không phụ thuộc nhiều vào lựa chọn chuyên gia tầng đầu tiên. Màu sắc dao động từ xanh đậm (0) đến đỏ đậm (0.125).

Tài liệu tham khảo
[1] Y. Bengio. Deep learning of representations: Looking forward. CoRR, abs/1305.0445, 2013.
[2] Y. Bengio, N. Léonard, and A. C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013.
[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high performance convolutional neural networks for image classification. In IJCAI, 2011.
[4] R. Collobert, Y. Bengio, and S. Bengio. Scaling large learning problems with hard parallel mixtures. International Journal on Pattern Recognition and Artificial Intelligence (IJPRAI), 17(3):349–365, 2003.
[5] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP, 2013.
[6] G. E. Hinton. Products of experts. ICANN, 1:1–6, 1999.
[7] R. A. Jacobs, M. I. Jordan, S. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts. Neural Computation, 3:1–12, 1991.
[8] N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke. Application of pretrained deep neural networks to large vocabulary speech recognition. Interspeech, 2012.
[9] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural Computation, 6:181–214, 1994.
[10] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS, 2012.
