# 1312.4314.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/1312.4314.pdf
# Kích thước tệp: 1928905 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Học Biểu Diễn Nhân Tử trong một
Hỗn Hợp Sâu của các Chuyên Gia
David Eigen1;2Marc'Aurelio Ranzato1Ilya Sutskever1
1Google, Inc.
2Khoa Khoa học Máy tính, Viện Courant, NYU
deigen@cs.nyu.edu ranzato@fb.com ilyasu@google.com
Tóm tắt
Hỗn Hợp các Chuyên Gia kết hợp đầu ra của nhiều mạng "chuyên gia", mỗi mạng
chuyên về một phần khác nhau của không gian đầu vào. Điều này đạt được bằng cách
huấn luyện một mạng "cổng" ánh xạ mỗi đầu vào thành một phân phối trên các chuyên gia. Các
mô hình như vậy cho thấy triển vọng cho việc xây dựng các mạng lớn hơn mà vẫn rẻ để tính toán
tại thời điểm kiểm tra, và có thể song song hóa hơn tại thời gian huấn luyện. Trong công trình này, chúng ta mở
rộng Hỗn Hợp các Chuyên Gia thành một mô hình xếp chồng, Hỗn Hợp Sâu của các Chuyên Gia, với
nhiều tập hợp cổng và chuyên gia. Điều này tăng theo cấp số nhân số lượng
chuyên gia hiệu quả bằng cách liên kết mỗi đầu vào với một kết hợp chuyên gia tại mỗi
lớp, nhưng vẫn duy trì kích thước mô hình khiêm tốn. Trên một phiên bản dịch chuyển ngẫu nhiên của
bộ dữ liệu MNIST, chúng ta thấy rằng Hỗn Hợp Sâu của các Chuyên Gia tự động học để
phát triển các chuyên gia phụ thuộc vị trí ("ở đâu") tại lớp đầu tiên, và các chuyên gia đặc trưng cho lớp
("là gì") tại lớp thứ hai. Ngoài ra, chúng ta thấy rằng các kết hợp khác nhau
được sử dụng khi mô hình được áp dụng cho một bộ dữ liệu của các âm tố đơn lẻ trong tiếng nói.
Những điều này chứng minh việc sử dụng hiệu quả tất cả các kết hợp chuyên gia.
1 Giới thiệu
Các mạng sâu đã đạt được hiệu suất rất tốt trong nhiều nhiệm vụ khác nhau, ví dụ [10, 5, 3]. Tuy nhiên,
một hạn chế cơ bản của các kiến trúc này là toàn bộ mạng phải được thực thi cho tất cả
đầu vào. Gánh nặng tính toán này áp đặt giới hạn kích thước mạng. Một cách để mở rộng các mạng này
trong khi giữ chi phí tính toán thấp là tăng tổng số tham số và đơn vị ẩn, nhưng chỉ sử dụng một phần nhỏ của mạng cho mỗi đầu vào nhất định. Sau đó, học một
hàm ánh xạ rẻ về mặt tính toán từ đầu vào đến các phần thích hợp của mạng.
Mô hình Hỗn Hợp các Chuyên Gia [7] là một phiên bản liên tục của điều này: Một mạng cổng học được trộn
đầu ra của N mạng "chuyên gia" để tạo ra một đầu ra cuối cùng. Mặc dù mô hình này bản thân nó
không đạt được những lợi ích tính toán được nêu ở trên, nó cho thấy triển vọng như một bước đệm hướng tới
các mạng có thể hiện thực hóa mục tiêu này.
Trong công trình này, chúng ta mở rộng Hỗn Hợp các Chuyên Gia để sử dụng một mạng cổng khác nhau tại mỗi lớp trong
một mạng đa lớp, tạo thành một Hỗn Hợp Sâu của các Chuyên Gia (DMoE). Điều này tăng số lượng
chuyên gia hiệu quả bằng cách giới thiệu một số lượng đường dẫn theo cấp số nhân qua các kết hợp khác nhau của
chuyên gia tại mỗi lớp. Bằng cách liên kết mỗi đầu vào với một kết hợp như vậy, mô hình của chúng ta sử dụng các
tập con khác nhau của các đơn vị cho các đầu vào khác nhau. Do đó nó có thể vừa lớn vừa hiệu quả cùng một lúc.
Chúng ta chứng minh tính hiệu quả của phương pháp này bằng cách đánh giá nó trên hai bộ dữ liệu. Sử dụng một
bộ dữ liệu MNIST bị nhiễu, chúng ta cho thấy rằng DMoE học để nhân tử các khía cạnh khác nhau của biểu diễn dữ liệu
tại mỗi lớp (cụ thể, vị trí và lớp), tận dụng hiệu quả tất cả các đường dẫn. Chúng ta cũng thấy rằng tất cả
các kết hợp được sử dụng khi áp dụng mô hình của chúng ta cho một bộ dữ liệu của các âm tố đơn lẻ trong tiếng nói.
Marc'Aurelio Ranzato hiện tại làm việc tại Nhóm AI Facebook.
1arXiv:1312.4314v3  [cs.LG]  9 Mar 2014

--- TRANG 2 ---
2 Công trình Liên quan
Một Hỗn Hợp Chuyên Gia chuẩn (MoE) [7] học một tập hợp các mạng chuyên gia fi cùng với một
mạng cổng g. Mỗi fi ánh xạ đầu vào x đến C đầu ra (một cho mỗi lớp c = 1, : : : , C), trong khi g(x) là một
phân phối trên các chuyên gia i = 1, : : : , N có tổng bằng 1. Đầu ra cuối cùng sau đó được cho bởi Công thức 1
FMoE(x) =NX
i=1gi(x)softmax( fi(x)) (1)
=NX
i=1p(eijx)p(cjei; x) =p(cjx) (2)
Điều này cũng có thể được xem như một mô hình xác suất, trong đó xác suất cuối cùng trên các lớp được tích phân biên
trên việc lựa chọn chuyên gia: đặt p(eijx) =gi(x) và p(cjei; x) = softmax( fi(x)), chúng ta có
Công thức 2.
Một tích các chuyên gia (PoE) [6] tương tự, nhưng thay vào đó kết hợp các xác suất log để tạo thành một tích:
FPoE(x)/NY
i=1softmax( fi(x)) =NY
i=1pi(cjx) (3)
Cũng liên quan chặt chẽ đến công trình của chúng ta là Hỗn Hợp Phân Cấp của các Chuyên Gia [9], học một hệ thống phân cấp
của các mạng cổng trong một cấu trúc cây. Đầu ra của mỗi mạng chuyên gia tương ứng với một lá trong cây;
các đầu ra sau đó được trộn theo trọng số cổng tại mỗi nút.
Mô hình của chúng ta khác với mỗi ba mô hình này vì nó lắp ráp động một
kết hợp chuyên gia phù hợp cho mỗi đầu vào. Đây là một thể hiện của khái niệm tính toán có điều kiện
được đưa ra bởi Bengio [1] và được xem xét trong một bối cảnh ngẫu nhiên một lớp bởi Bengio, Leonard và
Courville [2]. Bằng cách điều kiện các mạng cổng và chuyên gia của chúng ta trên đầu ra của lớp trước,
mô hình của chúng ta có thể biểu đạt một số lượng chuyên gia hiệu quả lớn theo cấp số nhân.
3 Phương pháp
Để mở rộng MoE thành DMoE, chúng ta giới thiệu hai tập hợp chuyên gia với các mạng cổng (g1; f1
i) và
(g2; f2
j), cùng với một lớp tuyến tính cuối cùng f3 (xem Hình 1). Đầu ra cuối cùng được tạo ra bằng cách kết hợp
các hỗn hợp tại mỗi lớp:
z1=NX
i=1g1
i(x)f1
i(x)
z2=MX
j=1g2
j(z1)f2
j(z1)
F(x) = z3= softmax( f3(z2))
Chúng ta đặt mỗi fl
i thành một ánh xạ tuyến tính đơn với rectification, và mỗi gl
i thành hai lớp ánh xạ tuyến tính với
rectification (nhưng với ít đơn vị ẩn); f3 là một lớp tuyến tính đơn. Xem Phần 4 để biết chi tiết.
Chúng ta huấn luyện mạng sử dụng gradient descent ngẫu nhiên (SGD) với một ràng buộc bổ sung trên
các phân công cổng (được mô tả dưới đây). Chỉ SGD dẫn đến một cực tiểu địa phương thoái hóa: Các chuyên gia tại
mỗi lớp hoạt động tốt nhất cho một vài ví dụ đầu tiên cuối cùng áp đảo các chuyên gia còn lại.
Điều này xảy ra vì các ví dụ đầu tiên tăng trọng số cổng của các chuyên gia này, điều này lần lượt
khiến chúng được chọn với trọng số cổng cao thường xuyên hơn. Điều này khiến chúng huấn luyện nhiều hơn,
và trọng số cổng của chúng tăng lại, cứ thế mãi mãi.
Để chống lại điều này, chúng ta đặt một ràng buộc trên các phân công cổng tương đối cho mỗi chuyên gia trong quá trình huấn
luyện. Đặt Gl
i(t) =Pt
t0=1gl
i(xt0) là tổng phân công chạy cho chuyên gia i của lớp l tại bước t, và
đặt Gl(t) =1
NPN
i=1Gl
i(t) là giá trị trung bình của chúng (ở đây, xt0 là ví dụ huấn luyện tại bước t0). Sau đó cho mỗi
chuyên gia i, chúng ta đặt gl
i(xt) = 0 nếu Gl
i(t) − Gl(t) > m cho một ngưỡng biên m, và chuẩn hóa lại
2

--- TRANG 3 ---
x	

f11(x)	

f21(x)	

fN1(x)	

g1(x)	

z1	

. . .	

x	

f11(x)	

f21(x)	

fN1(x)	

g1(x)	

z1	

. . .	

f12(x)	

f22(x)	

fM2(x)	

g2(x)	

z2	

. . .	

z3	

(a) (b)
Hình 1: (a) Hỗn Hợp các Chuyên Gia; (b) Hỗn Hợp Sâu của các Chuyên Gia với hai lớp.
phân phối gl(xt) để có tổng bằng 1 trên các chuyên gia i. Điều này ngăn chặn các chuyên gia khỏi việc được sử dụng quá mức ban đầu,
dẫn đến các phân công cân bằng. Sau khi huấn luyện với ràng buộc tại chỗ, chúng ta bỏ nó và tiếp tục
huấn luyện trong giai đoạn tinh chỉnh thứ hai.
4 Thí nghiệm
4.1 MNIST Nhiễu
Chúng ta đã huấn luyện và kiểm tra mô hình của chúng ta trên MNIST với các phép dịch chuyển đều ngẫu nhiên 4 pixel, tạo ra
các hình ảnh grayscale kích thước 36×36. Như được giải thích ở trên, mô hình được huấn luyện để phân loại chữ số
thành mười lớp.
Cho nhiệm vụ này, chúng ta đặt tất cả f1
i và f2
j thành các mô hình tuyến tính một lớp với rectification, f1
i(x) =
max(0 ; W1
ix+b1
i), và tương tự cho f2
j. Chúng ta đặt f3 thành một lớp tuyến tính, f3(z2) = W3z2+b3.
Chúng ta thay đổi số lượng đơn vị ẩn đầu ra của f1
i và f2
j giữa 20 và 100. Đầu ra cuối cùng
từ f3 có 10 đơn vị (một cho mỗi lớp).
Các mạng cổng g1 và g2 mỗi cái gồm hai lớp tuyến tính+rectification với 50
hoặc 20 đơn vị ẩn, và 4 đơn vị đầu ra (một cho mỗi chuyên gia), tức là g1(x) = softmax( Bmax(0 ; Ax+
a) +b), và tương tự cho g2.
Chúng ta đánh giá tác động của việc sử dụng một hỗn hợp tại lớp thứ hai bằng cách so sánh với việc chỉ sử dụng một
chuyên gia cố định duy nhất tại lớp thứ hai, hoặc nối đầu ra của tất cả các chuyên gia. Lưu ý rằng cho một hỗn hợp
với h đơn vị ẩn, mô hình nối tương ứng có Nh đơn vị ẩn. Do đó chúng ta mong đợi
mô hình nối hoạt động tốt hơn hỗn hợp, và hỗn hợp hoạt động tốt hơn
mạng đơn. Tốt nhất là hỗn hợp gần nhất có thể với ranh giới chuyên gia nối. Trong mỗi trường hợp, chúng ta giữ
kiến trúc lớp đầu tiên giống nhau (một hỗn hợp).
Chúng ta cũng so sánh mô hình hai lớp với một mô hình một lớp trong đó lớp ẩn z1 được
ánh xạ đến đầu ra cuối cùng thông qua lớp tuyến tính và softmax. Cuối cùng, chúng ta so sánh với một mạng sâu kết nối đầy đủ với cùng tổng số tham số. Điều này được xây dựng sử dụng
cùng số lượng đơn vị lớp thứ hai z2, nhưng mở rộng số lượng đơn vị lớp đầu tiên z1 sao cho
tổng số tham số giống như DMoE (bao gồm các tham số mạng cổng của nó).
3

--- TRANG 4 ---
4.2 Âm tố Đơn lẻ trong Tiếng nói
Ngoài ra, chúng ta đã chạy mô hình của chúng ta trên một bộ dữ liệu của các mẫu âm tố đơn lẻ trong tiếng nói. Bộ dữ liệu này là một
tập con ngẫu nhiên của khoảng một triệu mẫu từ một cơ sở dữ liệu độc quyền lớn hơn của vài trăm
giờ dữ liệu tiếng Anh Mỹ được thu thập sử dụng Voice Search, Voice Typing và dữ liệu đọc [8]. Cho các thí nghiệm của chúng ta, mỗi mẫu được giới hạn ở 11 khung cách nhau 10ms, và có 40 bin tần số.
Mỗi đầu vào được đưa vào mạng như một vector 440 chiều. Có 40 lớp âm tố đầu ra có thể.
Chúng ta huấn luyện một mô hình với 4 chuyên gia tại lớp đầu tiên và 16 tại lớp thứ hai. Cả hai lớp đều có
128 đơn vị ẩn. Các mạng cổng mỗi cái là hai lớp, với 64 đơn vị trong lớp ẩn. Như
trước đây, chúng ta đánh giá tác động của việc sử dụng một hỗn hợp tại lớp thứ hai bằng cách so sánh với việc chỉ sử dụng
một chuyên gia duy nhất tại lớp thứ hai, hoặc nối đầu ra của tất cả các chuyên gia.
5 Kết quả
5.1 MNIST Nhiễu
Bảng 1 cho thấy lỗi trên các tập huấn luyện và kiểm tra cho mỗi kích thước mô hình (tập kiểm tra là tập kiểm tra MNIST với một phép dịch chuyển ngẫu nhiên duy nhất trên mỗi hình ảnh). Trong hầu hết các trường hợp, các chuyên gia xếp chồng sâu hoạt động
giữa các baseline chuyên gia đơn và nối trên tập huấn luyện, như mong đợi. Tuy nhiên, các
mô hình sâu thường gặp phải overfitting: lỗi của hỗn hợp trên tập kiểm tra tệ hơn so với
chuyên gia đơn cho hai trong số bốn kích thước mô hình. Đáng khích lệ, DMoE hoạt động gần như tốt bằng
một mạng kết nối đầy đủ (DNN) với cùng số lượng tham số, mặc dù mạng này
áp đặt ít ràng buộc hơn trên cấu trúc của nó.
Trong Hình 2, chúng ta cho thấy phân công trung bình cho mỗi chuyên gia (tức là đầu ra cổng trung bình), cả theo
dịch chuyển đầu vào và theo lớp. Lớp đầu tiên phân công chuyên gia theo dịch chuyển, trong khi phân công
đều theo lớp. Ngược lại, lớp thứ hai phân công chuyên gia theo lớp, nhưng đều theo
dịch chuyển. Điều này cho thấy rằng hai lớp chuyên gia thực sự đang được sử dụng theo cách bổ sung,
để tất cả các kết hợp chuyên gia đều hiệu quả. Các chuyên gia lớp đầu tiên trở nên chọn lọc với nơi
chữ số xuất hiện, bất kể lớp thành viên của nó, trong khi các chuyên gia lớp thứ hai chọn lọc với
lớp chữ số là gì, bất kể vị trí của chữ số.
Cuối cùng, Hình 3 cho thấy chín ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi kết hợp chuyên gia.
Các phân công lớp đầu tiên chạy qua các hàng, trong khi lớp thứ hai chạy qua các cột. Lưu ý
dịch chuyển của mỗi chữ số thay đổi theo hàng nhưng không đổi qua các cột, trong khi điều ngược lại đúng cho
lớp của chữ số. Hơn nữa, các lớp dễ nhầm lẫn có xu hướng được nhóm lại với nhau, ví dụ 3 và 5.
Lỗi Tập Kiểm tra: MNIST Nhiễu
Mô hình Gate Hids Chuyên Gia Đơn DMoE Concat Lớp2 DNN
4×100×4×100 50×50 1.33 1.42 1.30 1.30
4×100×4×20 50×50 1.58 1.50 1.30 1.41
4×100×4×20 50×20 1.41 1.39 1.30 1.40
4×50×4×20 20×20 1.63 1.77 1.50 1.67
4×100(một lớp) 50 2.86 1.72 1.69 –
Lỗi Tập Huấn luyện: MNIST Nhiễu
Mô hình Gate Hids Chuyên Gia Đơn DMoE Concat Lớp2 DNN
4×100×4×100 50×50 0.85 0.91 0.77 0.60
4×100×4×20 50×50 1.05 0.96 0.85 0.90
4×100×4×20 50×20 1.04 0.98 0.87 0.87
4×50×4×20 20×20 1.60 1.41 1.33 1.32
4×100(một lớp) 50 2.99 1.78 1.59 –
Bảng 1: So sánh DMoE cho MNIST với dịch chuyển ngẫu nhiên, với các baseline (i) chỉ sử dụng
một chuyên gia lớp thứ hai, (ii) nối tất cả các chuyên gia lớp thứ hai, và (iii) một DNN với cùng
tổng số tham số. Cho cả (i) và (ii), các chuyên gia trong lớp đầu tiên được trộn để tạo thành z1.
Các mô hình được chú thích với "# chuyên gia × # đơn vị ẩn" cho mỗi lớp.
4

--- TRANG 5 ---
MNIST Nhiễu: Mô hình Sâu Hai Lớp
theo Dịch chuyển theo Lớp
Lớp 1
Lớp 2
Hình 2: Đầu ra cổng trung bình cho lớp đầu tiên và thứ hai, cả theo dịch chuyển và theo lớp. Màu sắc
chỉ ra trọng số cổng. Các phân phối theo dịch chuyển cho thấy phân công cổng trung bình cho mỗi
bốn chuyên gia cho mỗi trong số 9×9 phép dịch chuyển có thể. Các phân phối theo lớp cho thấy phân công cổng trung bình cho mỗi bốn chuyên gia (hàng) cho mỗi trong số mười lớp (cột). Lưu ý
lớp đầu tiên tạo ra các phân công độc quyền theo dịch chuyển, trong khi lớp thứ hai phân công chuyên gia theo
lớp. Để so sánh, chúng ta cho thấy các phân công theo lớp của một MoE chuẩn được huấn luyện trên MNIST mà không có
nhiễu, sử dụng 5 chuyên gia 20 đơn vị ẩn.
5.2 Âm tố Đơn lẻ trong Tiếng nói
Bảng 2 cho thấy lỗi trên các tập huấn luyện và kiểm tra. Như trường hợp của MNIST, lỗi của hỗn hợp
trên tập huấn luyện nằm giữa hai baseline. Tuy nhiên trong trường hợp này, hiệu suất tập kiểm tra
gần như giống nhau cho cả hai baseline cũng như hỗn hợp.
Hình 4 cho thấy 16 ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi kết hợp chuyên gia (chúng ta chỉ cho thấy 4 chuyên gia tại lớp thứ hai do hạn chế không gian). Như trước đây, các phân công lớp đầu tiên
chạy qua các hàng, trong khi lớp thứ hai chạy qua các cột. Mặc dù không dễ hiểu như đối với
MNIST, mỗi kết hợp chuyên gia có vẻ xử lý một phần riêng biệt của đầu vào. Điều này được
củng cố thêm bởi Hình 5, nơi chúng ta vẽ số lượng phân công trung bình cho mỗi kết hợp chuyên gia.
Ở đây, lựa chọn chuyên gia lớp thứ hai phụ thuộc ít vào lựa chọn chuyên gia lớp đầu tiên.
Lỗi Âm tố Tập Kiểm tra: Âm tố Đơn lẻ trong Tiếng nói
Mô hình Gate Hids Chuyên Gia Đơn Chuyên Gia Trộn Concat Lớp2
4×128×16×128 64×64 0.55 0.55 0.56
4×128(một lớp) 64 0.58 0.55 0.55
Lỗi Âm tố Tập Huấn luyện: Âm tố Đơn lẻ trong Tiếng nói
Mô hình Gate Hids Chuyên Gia Đơn Chuyên Gia Trộn Concat Lớp2
4×128×16×128 64×64 0.47 0.42 0.40
4×128(một lớp) 64 0.56 0.50 0.50
Bảng 2: So sánh DMoE cho dữ liệu âm tố đơn lẻ trong tiếng nói. Ở đây cũng vậy, chúng ta so sánh với
các baseline chỉ sử dụng một chuyên gia lớp thứ hai, hoặc nối tất cả các chuyên gia lớp thứ hai.
5

--- TRANG 6 ---
Hình 3: Chín ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi kết hợp chuyên gia, cho
bộ dữ liệu mnist nhiễu. Các chuyên gia lớp đầu tiên ở các hàng, trong khi lớp thứ hai ở các cột.
6 Kết luận
Mô hình Hỗn Hợp Sâu của các Chuyên Gia mà chúng ta xem xét là một bước đầy hứa hẹn hướng tới việc phát triển các
mô hình lớn, thưa thớt chỉ tính toán một tập con của chính chúng cho bất kỳ đầu vào nhất định nào. Chúng ta thấy chính xác các
phân công cổng cần thiết để sử dụng hiệu quả tất cả các kết hợp chuyên gia: cho MNIST nhiễu,
một phân tích thành dịch chuyển và lớp, và việc sử dụng khác biệt của mỗi kết hợp cho dữ liệu âm tố đơn lẻ
trong tiếng nói. Tuy nhiên, chúng ta vẫn sử dụng một hỗn hợp liên tục của các đầu ra của chuyên gia thay vì hạn chế
đến một vài đầu ra hàng đầu — một sự mở rộng như vậy là cần thiết để thực hiện mục tiêu của chúng ta là chỉ sử dụng một phần nhỏ của
mô hình cho mỗi đầu vào. Một phương pháp thực hiện điều này cho một lớp đơn đã được mô tả bởi
Collobert et al. [4], có thể được điều chỉnh cho trường hợp đa lớp của chúng ta; chúng ta hy vọng sẽ giải quyết điều này
trong công việc tương lai.
Lời cảm ơn
Các tác giả muốn cảm ơn Matthiew Zeiler vì những đóng góp của anh ấy về việc thực thi các ràng buộc cân bằng
trong quá trình huấn luyện.
6

--- TRANG 7 ---
Ví dụ Phân công Kết hợp
Lớp 1/Lớp 2/chuyên gia/cả hai lớp
Hình 4: 16 ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi kết hợp chuyên gia cho dữ liệu âm tố đơn lẻ
trong tiếng nói. Các chuyên gia lớp đầu tiên ở các hàng, trong khi lớp thứ hai ở các cột. Mỗi mẫu được biểu diễn bởi
40 giá trị tần số (trục dọc) và 11 khung liên tiếp (trục ngang). Cho hình này, chúng ta sử dụng bốn
chuyên gia trong mỗi lớp.
Âm tố Đơn lẻ trong Tiếng nói: Phân công Có Điều kiện
Số Phân công Chuyên Gia Lớp 1 Trên Mỗi Điểm Dữ liệu/Lớp 2 Theo Nhãn
Kết hợp trộn tốt
thang màu [0, 0.125]/thang màu [0, 0.125]
Hình 5: Số lượng phân công kết hợp cho bộ dữ liệu âm tố đơn lẻ trong tiếng nói. Ở đây chúng ta vẽ tích trung bình
của trọng số cổng lớp đầu tiên và thứ hai cho mỗi kết hợp chuyên gia. Chúng ta chuẩn hóa mỗi
hàng, để tạo ra một phân phối có điều kiện: Điều này cho thấy các phân công cổng trung bình trong lớp thứ hai
cho trước một phân công lớp đầu tiên. Lưu ý các phân công kết hợp được trộn tốt: Lựa chọn chuyên gia lớp thứ hai
không phụ thuộc nhiều vào lựa chọn chuyên gia lớp đầu tiên. Màu sắc từ xanh đậm
(0) đến đỏ đậm (0.125).
7

--- TRANG 8 ---
Tài liệu tham khảo
[1] Y. Bengio. Deep learning of representations: Looking forward. CoRR, abs/1305.0445, 2013.
2
[2] Y. Bengio, N. Léonard, và A. C. Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. 2
[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, và J. Schmidhuber. Flexible, high
performance convolutional neural networks for image classification. Trong IJCAI, 2011. 1
[4] R. Collobert, Y. Bengio, và S. Bengio. Scaling large learning problems with hard parallel
mixtures. International Journal on Pattern Recognition and Artificial Intelligence (IJPRAI),
17(3):349–365, 2003. 6
[5] A. Graves, A. Mohamed, và G. Hinton. Speech recognition with deep recurrent neural net-
works. Trong ICASSP, 2013. 1
[6] G. E. Hinton. Products of experts. ICANN, 1:1–6, 1999. 2
[7] R. A. Jacobs, M. I. Jordan, S. Nowlan, và G. E. Hinton. Adaptive mixtures of local experts.
Neural Computation, 3:1–12, 1991. 1, 2
[8] N. Jaitly, P. Nguyen, A. Senior, và V. Vanhoucke. Application of pretrained deep neural
networks to large vocabulary speech recognition. Interspeech, 2012. 4
[9] M. I. Jordan và R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural
Computation, 6:181–214, 1994. 2
[10] A. Krizhevsky, I. Sutskever, và G.E. Hinton. Imagenet classification with deep convolutional
neural networks. Trong NIPS, 2012. 1
8
