# 2106.04426.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2106.04426.pdf
# Kích thước tệp: 371521 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Các Lớp Băm cho Mô hình Thưa Thớt Lớn
Stephen Roller Sainbayar Sukhbaatar Arthur Szlam Jason Weston
Facebook AI Research
Tóm tắt
Chúng tôi nghiên cứu việc huấn luyện các lớp thưa thớt sử dụng các tham số khác nhau cho các đầu vào khác nhau dựa trên băm trong các mô hình Transformer lớn. Cụ thể, chúng tôi sửa đổi lớp feedforward để băm đến các tập trọng số khác nhau tùy thuộc vào token hiện tại, trên tất cả các token trong chuỗi. Chúng tôi chỉ ra rằng quy trình này hoặc vượt trội hoặc cạnh tranh với các phương pháp mixture-of-expert học định tuyến như Switch Transformers và BASE Layers, trong khi không yêu cầu tham số định tuyến hoặc các thuật ngữ bổ sung trong hàm mục tiêu như mất mát cân bằng tải, và không có thuật toán gán phức tạp. Chúng tôi nghiên cứu hiệu suất của các kỹ thuật băm khác nhau, kích thước băm và đặc trưng đầu vào, và chỉ ra rằng các băm cân bằng và ngẫu nhiên tập trung vào các đặc trưng cục bộ nhất hoạt động tốt nhất, so với việc học các cụm hoặc sử dụng ngữ cảnh tầm xa hơn. Chúng tôi chỉ ra cách tiếp cận của chúng tôi hoạt động tốt cả trên các tác vụ mô hình hóa ngôn ngữ lớn và đối thoại, và trên các tác vụ tinh chỉnh downstream.

1 Giới thiệu
Các nghiên cứu gần đây về mô hình Transformer đã cho thấy xu hướng rõ ràng hướng tới cải thiện với quy mô trong dữ liệu và kích thước mô hình [1], phản ánh cùng xu hướng trong Machine Learning nói chung. Tuy nhiên, khi được kiến trúc một cách ngây thơ, các mô hình lớn hơn (về số lượng tham số) chậm hơn trong huấn luyện và đánh giá; và ở quy mô cực đại, với các hệ thống máy tính hiện tại, đòi hỏi kỹ thuật phức tạp để tạo thuận lợi cho giao tiếp giữa các worker. Để giải quyết những thách thức này, các nhà nghiên cứu đã nghiên cứu các mô hình Mixtures-of-Experts (MoE) [2,3,4,5,6,7,8], nơi một "gater" định tuyến tính toán thông qua một tập con thưa thớt của các trọng số của mô hình ("các module chuyên gia"). Cụ thể trong bối cảnh Transformers cho Xử lý Ngôn ngữ Tự nhiên (NLP), các cách tiếp cận gần đây đã dẫn đến hiệu suất tiên tiến trong mô hình hóa ngôn ngữ [8]. Các mô hình MoE cho phép tăng số lượng tham số trong mô hình trong khi giữ ổn định số lượng tính toán ảnh hưởng đến một mẫu nhất định.

Một thành phần chính cho mô hình MoE là chiến lược định tuyến (gating). Trong khi các mô hình MoE có thể có lợi thế về mặt tính toán trên mỗi tham số so với mô hình dày đặc, chúng có thể kém mạnh mẽ về chức năng trên mỗi tham số. Một chiến lược định tuyến kém có thể dẫn đến các module chuyên gia không được chuyên môn hóa đúng cách (về cơ bản tạo ra một mô hình ensemble ngẫu nhiên); hoặc chuyên môn hóa quá mức, sử dụng hàm gán dữ liệu để overfit. Trong khi đó, chính chiến lược định tuyến phải hiệu quả.

Một cách tiếp cận tiêu chuẩn là huấn luyện một lớp trọng số thực hiện quyết định định tuyến dựa trên đầu vào của lớp sẽ được định tuyến. Theo kiểu cổ điển, điều này có thể đã được thực hiện với softmax trên lựa chọn các module chuyên gia, và được khớp thông qua backpropagation. Tuy nhiên, một softmax dày đặc yêu cầu tất cả các module chuyên gia chạy trên tất cả các điểm dữ liệu tại thời điểm huấn luyện, điều này phủ nhận việc tiết kiệm tính toán. Một số công trình đã chỉ ra rằng độ thưa thớt có thể được duy trì trong quá trình huấn luyện, ví dụ [9,7,8,10]. Đặc biệt, Switch Transformers [8] chọn chuyên gia hàng đầu cho mỗi token sử dụng softmax trên trạng thái ẩn của token, nhưng yêu cầu một thuật ngữ cân bằng tải trong hàm mục tiêu hoặc chúng có thể trở nên mất cân bằng hoặc thoái hóa, cho kết quả kém. BASE Layers [10] sử dụng thuật toán gán tuyến tính để cố gắng giải quyết cùng vấn đề.

Preprint. Đang được xem xét.arXiv:2106.04426v3 [cs.LG] 20 Jul 2021

--- TRANG 2 ---
. . . . . . . . . . . .
"We" "eat" "every" "taco"1MoE
FFN1. . .
2MoE
FFN2. . .
3MoE
FFN3. . .
4MoE
FFN4. . .
self-attentionLớp lLớp l+ 1
FFN 1 FFN 2 FFN 3
Hash3
hlhl
MoE FFN

Hình 1: Tổng quan về Hash Layer. Các token được định tuyến đến các module chuyên gia cố định dựa trên băm của chúng.

Trong công trình này, chúng tôi mô tả một chiến lược định tuyến đơn giản, thưa thớt, hiệu quả dựa trên băm các token đầu vào hiệu quả trong bối cảnh Transformers-for-NLP. Chúng tôi chỉ ra cách tiếp cận này hiệu quả trên một số bộ dữ liệu, so sánh thuận lợi với cả Switch Transformers và BASE Layers. Vì chiến lược định tuyến không yêu cầu tham số bổ sung, không thay đổi hàm mục tiêu hoặc thuật toán gán, tính đơn giản của nó có nghĩa là nó mạnh mẽ, nhanh và dễ thực hiện. Chúng tôi cung cấp phân tích chi tiết để giải thích tại sao phương pháp của chúng tôi hoạt động, và trong những điều kiện nào. Cho rằng khi huấn luyện các mô hình rất lớn, người ta có thể thường chỉ có một cơ hội duy nhất với ngân sách tính toán yêu cầu, và những người thực验 sẽ không thể thử nhiều lựa chọn tham số, do đó chúng tôi ủng hộ cách tiếp cận của chúng tôi như một ứng viên mạnh mẽ cho bối cảnh như vậy.

2 Nền tảng
Trước tiên, chúng ta hãy giới thiệu bối cảnh Mixture-of-Experts nơi chúng tôi áp dụng chiến lược định tuyến dựa trên băm của chúng tôi.
Chúng tôi sử dụng cùng bối cảnh như [11,8,10] nơi một mạng feedforward (FFN) trong Transformer được thay thế bằng phiên bản MoE của nó. Cho một chuỗi đầu vào được token hóa {x1; x2; : : : ; xT} của T token, một biểu diễn cho mỗi token được tính toán song song bởi một Transformer tiêu chuẩn [12]

hL1;hL2; : : : ; hLT=TRANSFORMER (x1; x2; : : : ; xT): (1)

Transformer bao gồm L lớp tính toán trạng thái ẩn cuối cùng cho mỗi token, và mỗi lớp được cấu thành từ các lớp con self-attention và FFN, nơi FFN là các mạng kết nối đầy đủ hai lớp

hlt=SelfAttn (hl−1t) hlt=FFN(hlt): (2)

Ở đây chúng tôi bỏ qua skip-connections và normalization cho ngắn gọn. Sau đó chúng ta có thể thay thế một hoặc nhiều lớp con FFN bằng các module chuyên gia. Thay thế FNN tại lớp l bằng K FFN chuyên gia, đầu ra của chúng sau đó được trộn với một số hàm gating g():

hlt=FFN(hlt)! hlt=KXi=1gi(hlt)FFNi(hlt); t= 1; : : : ; T; (3)

nơi quan trọng là mỗi token được định tuyến đến một hỗn hợp khác nhau của các chuyên gia, vì hàm gating phụ thuộc vào trạng thái ẩn cụ thể hlt của token.

Các phương pháp MoE thưa thớt giả định các giá trị gating gi thường bằng không, vì vậy chỉ một vài chuyên gia cần được tính toán để có hiệu quả tốt hơn. Vì các FFN chuyên gia không chia sẻ tham số, số lượng tham số tăng với K trong khi lượng tính toán cho mỗi token đầu vào giữ nguyên nếu MoE FFN chỉ định tuyến đến một chuyên gia duy nhất, và tính toán gi rẻ. Trong khi điều này cho phép huấn luyện các mô hình dung lượng lớn với ngân sách tính toán nhỏ, việc tối ưu hóa gi trong bối cảnh thưa thớt có thể phức tạp.

3 Phương pháp
Trong bài báo này, chúng tôi đề xuất một cơ chế gating đơn giản đặc biệt hiệu quả vì chỉ một chuyên gia hoạt động, và nó không có tham số mạng định tuyến nào để học. Công trình gần đây [11,8,10] phải học các tham số xác định định tuyến đến các module chuyên gia dựa trên trạng thái ẩn, phải được tối ưu hóa song song với chính các trọng số chuyên gia. Điều này có thể gây khó khăn vì trong quá trình huấn luyện, thành viên cho mỗi chuyên gia đang thay đổi trong khi nó đang cố gắng học ánh xạ

--- TRANG 3 ---
cho những thành viên đó. Thay vào đó, chúng tôi ủng hộ ánh xạ cố định đến các chuyên gia. Cụ thể, bằng cách băm các token vào một số bucket cố định, mỗi bucket tương ứng với một chuyên gia:

hlt=FFNhash (xt)(hlt); t= 1; : : : ; T: (4)

Trong khi FFN vẫn nhận trạng thái ẩn hlt làm đầu vào, hàm định tuyến của chúng tôi sử dụng token đầu vào gốc xt thay vì trạng thái ẩn, xem Hình 1 để mô tả đồ họa. Chúng tôi tự do lựa chọn từ các hàm băm có thể khác nhau, mà chúng tôi sẽ xem xét dưới đây. Tuy nhiên, cho mục đích huấn luyện, hàm băm được cố định trước, và theo cách này, cơ chế định tuyến của chúng tôi không yêu cầu huấn luyện và không có tham số có thể điều chỉnh.

3.1 Hàm Băm
Các hàm băm đã được sử dụng lâu dài trong khắp Khoa học Máy tính [13], và có thể có nhiều dạng khác nhau. Trong công trình của chúng tôi, chúng tôi thường sử dụng các hàm băm được tính toán trước, sử dụng bảng tra cứu trong quá trình học - được tính toán trước - để ánh xạ các token đến các module chuyên gia.

Chúng tôi xem xét một số loại hàm băm như các lựa chọn có thể cho việc định tuyến token đến các module chuyên gia.
Đơn giản nhất là Random Hash, trong đó chúng tôi gán mỗi token cho một chuyên gia cố định, ngẫu nhiên tại thời điểm khởi tạo. Do phân phối Zipfian của tần suất token, điều này tự nhiên tạo ra mất cân bằng trên các module chuyên gia khác nhau. Vì cân bằng đã được chỉ ra trước đây là quan trọng để huấn luyện các mô hình MoE [8,10], chúng tôi cũng xem xét Balanced assignment. Trong phương pháp này, chúng tôi xây dựng bảng tra cứu trước khi huấn luyện mô hình sử dụng phân phối dữ liệu huấn luyện bằng cách gán tham lam các token thường xuyên nhất cho các bucket trống nhất. Cấu trúc gán kết quả cân bằng hơn đáng kể so với Random Hashing, nhưng không hoàn hảo, vì tần suất của một số token vượt quá phân phối lý tưởng.

Random và Balanced hashing khai thác inductive bias của các mô hình auto-regressive và băm trên token đầu vào, nhưng chúng tôi cũng xem xét các khả năng khác: Bigram Hash sử dụng token hiện tại và trước đó (xt−1,xt) thay vì chỉ token hiện tại, trong khi Previous Token Hash sử dụng token trước đó xt−1, bỏ qua đầu vào hiện tại. Chúng tôi cũng xem xét một kiểm tra tỉnh táo băm dựa trên Position trong chuỗi, mà chúng tôi mong đợi có ít tác động, vì các vị trí tuyệt đối mang ít thông tin trong ngôn ngữ tự nhiên. Mỗi hàm băm này được sử dụng để đánh giá giá trị của thông tin được định tuyến trong phân tích thực nghiệm tiếp theo của chúng tôi.

Như một baseline trên, chúng tôi cũng đánh giá sử dụng Oracle Future Hash, băm dựa trên token đầu ra xt+1, thay vì token đầu vào. Oracle Hash này kiểm tra mức độ mạnh mẽ của các quyết định định tuyến trong việc giải quyết một tác vụ. Tương tự, chúng tôi cũng xem xét Predicted Future Token Hash, sử dụng một Transformer baseline để đưa ra dự đoán về token đầu ra, và sau đó băm trên dự đoán này.

Clustered Hashes Dựa trên trực giác rằng các token tương tự có thể muốn được định tuyến đến cùng một chuyên gia, chúng tôi cũng thử nghiệm với Clustered Hashes. Chúng tôi thu được các cụm bằng cách thực hiện k-means clustering với một số cụm cố định sử dụng token embeddings từ một mô hình Transformer baseline. Mỗi chuyên gia được gán một centroid, và các token được gán cho cụm gần nhất của chúng.

Dispersed Hashes Chúng tôi cũng xem xét giả thuyết ngược lại: rằng các token tương tự nên được đặt trong các bucket khác nhau, nơi giả định là các token rất tương tự cần sự phân biệt tinh tế đòi hỏi nhiều dung lượng mô hình hơn (do đó gán cho các chuyên gia khác nhau). Để làm điều này, chúng tôi sử dụng cùng các cụm k-means như trước, nhưng phân phối tất cả các token trong mỗi cụm đều trên tất cả các bucket.

3.2 MultiHash Layers
Trong cách tiếp cận FFN MoE tiêu chuẩn, tất cả K module chuyên gia có tham số độc lập, nhưng ở đây chúng tôi xem xét một tùy chọn khác. Được biết trong tài liệu băm rằng nhiều băm có thể cung cấp phân bổ tốt hơn trong nhiều ngữ cảnh [14]. Chúng tôi xem xét các sơ đồ như vậy trong ngữ cảnh định tuyến thưa thớt. Giả sử chúng ta được cho N hàm băm khác nhau, và cho một token đầu vào x chúng ta tính toán các băm này, ký hiệu là km=hashm(x); m= 1; : : : ; N. Giả sử FFN chuyên gia thông thường là một hàm B(relu(A(h))) nơi A:Rd!RD và B:RD!Rd, chúng tôi chia các lớp tuyến tính thành N đoạn, Am:Rd!RD/N và Bm:RD!Rd/N. Sau đó chúng tôi tính toán:

v=relu([Ak1(h); : : : ; AkN(h)]) FFNMH(h) = [Bk1(v); : : : ; BkN(v)]:

--- TRANG 4 ---
Tức là, sử dụng băm để chọn các tham số chúng ta sẽ sử dụng cho mỗi đoạn, và sau đó nối chúng lại với nhau. Ưu điểm là bây giờ chúng ta không còn phụ thuộc vào chất lượng của một hàm băm duy nhất, mà có nhiều cơ hội để tạo ra các phân vùng chất lượng tốt. Điều này có lẽ cũng có thể được xem là tương tự với quy trình multi-head attention đã được sử dụng trong Transformers.

4 Công trình Liên quan
Các mô hình MoE thưa thớt, nơi chỉ một vài module chuyên gia hoạt động cho bất kỳ đầu vào nào, đặc biệt trong ngữ cảnh NLP, đã được nghiên cứu gần đây trong [6,11]. Trong các công trình này, gating được học thông qua backpropagation, có lẽ với một regularizer để khuyến khích cân bằng tải trên các chuyên gia. [8] chỉ ra rằng các mô hình trong [11] có thể được huấn luyện thành công với mỗi đầu vào được gán cho chính xác một chuyên gia. Một cách tiếp cận khác như vậy cho Transformers, nơi định tuyến được học thông qua việc giải quyết một vấn đề gán tuyến tính, được nghiên cứu trong [10]. [?] sử dụng một cách tiếp cận khác, nơi các product key cho phép tìm kiếm láng giềng gần nhất để chọn tham số. Nói chung hơn, việc sử dụng MoE để đánh đổi thời gian tính toán (với chi phí phân mảnh dữ liệu có thể) có lịch sử lâu dài, xem ví dụ [3, 7].

Cách tiếp cận trong công trình này khác với tất cả những cái này ở chỗ các phân công không sử dụng học tập gì cả, và thay vào đó tận dụng các inductive bias có thể trong bối cảnh ngôn ngữ tự nhiên. Đặc biệt, chúng tôi sử dụng thực tế rằng n-gram bản thân chúng là những mô hình ngôn ngữ tạm ổn [15]. Do đó công trình này liên quan đến công trình trước đây cố gắng kết hợp các mô hình ngôn ngữ neural và n-gram [16, 17, 18, 19, 20, 21].

Công trình của chúng tôi cũng liên quan đến feature hashing trong các mô hình tuyến tính và phương pháp kernel [22,23], nơi các đặc trưng từ hoặc n-gram được băm để cung cấp một không gian đặc trưng chiều thấp mới. [22] chỉ ra rằng khi thực hiện feature hashing như vậy, sự tương tác giữa các không gian con ngẫu nhiên là không đáng kể với xác suất cao. [?] sử dụng băm để nén các mạng neural, thay vì tăng các tham số của chúng như chúng tôi làm ở đây. Công trình về Transformers ngữ cảnh dài gần đây đã sử dụng các kỹ thuật băm để tăng tốc truy cập đến lịch sử token tầm xa thông qua các mẫu self-attention thưa thớt, đặc biệt trong Routing Transformers [24] và Reformer [25]. Ngược lại, công trình của chúng tôi sử dụng băm để truy cập một tập lớn các tham số thông qua định tuyến thưa thớt, thay vì truy cập thưa thớt đến các đặc trưng đầu vào.

5 Thử nghiệm
5.1 Tác vụ
Pushshift.io Reddit Chúng tôi sử dụng một biến thể của các cuộc thảo luận Reddit, cũng đã được sử dụng trong một số nghiên cứu hiện có, xem ví dụ [26,27,28,29]. Theo [30], chúng tôi sử dụng một bộ dữ liệu Reddit đã tồn tại trước đó được trích xuất và thu thập bởi bên thứ ba và được cung cấp trên pushshift.io [31], huấn luyện để tạo một bình luận có điều kiện trên toàn bộ thread dẫn đến bình luận, bao gồm 1.5B ví dụ huấn luyện. Chúng tôi sử dụng cùng từ điển BPE như [32], bao gồm 8008 token.

RoBERTa+cc100en Data Chúng tôi sử dụng cùng dữ liệu được sử dụng để huấn luyện BASE [10], bao gồm khoảng 100B token, kết hợp các corpus được sử dụng trong RoBERTa [33] với tập con tiếng Anh của corpus CC100 [34]. Từ điển GPT2, có kích thước 51200, được sử dụng để token hóa. Cho các thử nghiệm seq2seq của chúng tôi, chúng tôi sắp xếp dữ liệu này chia theo câu để dự đoán lượt tiếp theo. Chúng tôi xem xét nó như tác vụ mô hình hóa ngôn ngữ dự định ban đầu trong các thử nghiệm của chúng tôi so sánh với BASE [10].

Wikitext-103 Wikitext-103 là một benchmark mô hình hóa ngôn ngữ nhỏ hơn [35] bao gồm một bộ sưu tập các bài viết Wikipedia hơn 100 triệu token, và một kích thước từ vựng cố định 270K token được cung cấp. Chúng tôi xem điều này như một tác vụ seq2seq trong các thử nghiệm của chúng tôi, một lần nữa chia theo câu.

Downstream BST tasks Cuối cùng, chúng tôi sử dụng các tác vụ đối thoại Blended Skill Talk (BST) được sử dụng trong [32] sau khi pre-training pushshift.io Reddit để đánh giá hiệu suất tinh chỉnh của các mô hình dày đặc so với thưa thớt.

5.2 Cài đặt Thử nghiệm
Seq2Seq Setup Phần lớn các thử nghiệm của chúng tôi được thực hiện trong nền tảng ParlAI1 sử dụng khung Transformer encoder-decoder. Đầu tiên chúng tôi huấn luyện một số Transformer tiêu chuẩn (dày đặc), với
1http://parl.ai

--- TRANG 5 ---
Bảng 1: So sánh các Mô hình trên pushshift.io Reddit. Chúng tôi chỉ ra ba kích thước của Transformer dày đặc so với Switch Transformers và sử dụng Hash Layers với số lượng module và lớp thưa thớt khác nhau, ví dụ 5x16 có nghĩa là 5 lớp thưa thớt với 16 module mỗi lớp. Tất cả các module Switch và Hash Layer được xây dựng với cùng độ phức tạp tính toán như Transformer baseline 11 lớp, nhưng có nhiều tham số hơn; các mô hình dày đặc lớn hơn có tổng số tham số tương tự, nhưng sử dụng nhiều tính toán hơn.

Model Configuration Params Valid PPL Test PPL
Baseline Transformer layers=11, d=1024, D=4096 222M 24.90 24.96
Wider Transformer (more compute) layers=11, d=2048, D=6144 755M 23.32 23.38
Deeper Transformer (more compute) layers=22, d=1536, D=4096 755M 22.72 22.78
Switch Transformer layers=11,modules=1x64, load_bal=0.1 751M 23.65 23.73
Hash Layer layers=11,modules=1x64 751M 23.16 23.23
Switch Transformer layers=11,modules=1x128, load_bal=0.1 1.28B 23.52 23.58
Hash Layer layers=11,modules=1x128 1.28B 22.89 22.95
Switch Transformer layers=11,modules=5x16, load_bal=0.01 852M 23.19 23.25
Switch Transformer layers=11,modules=5x16, load_bal=0.1 852M 23.00 22.93
Hash Layer layers=11,modules=5x16 852M 23.21 23.27

Bảng 2: So sánh các Mô hình trên RoBERTa+cc100en Data. Chúng tôi so sánh một transformer dày đặc với cùng tham số như các mô hình thưa thớt của chúng tôi, ngoại trừ với 1 lớp thưa thớt với 64 module (1x64).

Model Configuration Params Valid PPL
Baseline Transformer layers=11, d=1024, D=4096 266M 28.85
Switch Transformer layers=11, modules=1x64, load_bal=0.1 795M 27.41
Hash Layer layers=11, modules=1x64 794M 26.99

2 lớp encoder và 11 hoặc 22 lớp decoder, theo cấu trúc trong [32] để huấn luyện trên pushshift.io Reddit. Chúng tôi gọi cái có 11 lớp và kích thước embedding d= 1024 và kích thước lớp ẩn FFN D= 4096 là Baseline Transformer của chúng tôi. Chúng tôi cũng huấn luyện mô hình "Wider" với D= 6144, và mô hình "Deeper" với 22 lớp decoder, và D= 4096. Mô hình Baseline có 222M tham số, và "Wider" và "Deeper" được chọn để cả hai đều có 755M tham số mỗi cái. Những mô hình này được so sánh với các phương pháp Hash Layer được chi tiết trong phần 3 và với Switch Transformers cùng kích thước và cài đặt. Cân bằng tải cho Switch được tối ưu hóa trên tập validation. Cho cả Hash và Switch, chúng tôi sử dụng kích thước Transformer "Baseline" được chi tiết ở trên như kiến trúc mà chúng tôi thêm các lớp định tuyến thưa thớt bằng cách thay thế một hoặc nhiều lớp dày đặc gốc.
Tất cả các thử nghiệm được chạy trong 100k cập nhật; một bảng siêu tham số được cung cấp trong tiểu mục B.1.

BASE Comparison Trong khi phần lớn phân tích của chúng tôi diễn ra trong cài đặt được mô tả ở trên với các mô hình lên đến 1.28B tham số, để kiểm tra các phương pháp của chúng tôi ở quy mô trên các mô hình thưa thớt lớn hơn, chúng tôi áp dụng cài đặt BASE Layer [10] và code base2 thay vào đó nơi chúng tôi so sánh các mô hình Hash và BASE Layer 4.5B tham số. Cài đặt này sử dụng các mô hình ngôn ngữ thuần túy thay vì cài đặt Seq2Seq ở trên. Chúng tôi sử dụng kiến trúc, dữ liệu (RoBERTa+cc100en), và siêu tham số trực tiếp từ [10], sử dụng hoặc một lớp định tuyến thưa thớt duy nhất bao gồm 3 FFN xếp chồng (D= 8192) trên lớp giữa của mạng 25 lớp, hoặc 3 lớp định tuyến được phân bố đều trong mạng. Để so sánh trực tiếp với BASE, chúng tôi giữ tất cả siêu tham số cố định và chỉ thay đổi phương pháp định tuyến; chúng tôi sử dụng Hash Layer gán cân bằng trong trường hợp này. Chúng tôi huấn luyện cho đến khi đạt 40k bước. Một bảng siêu tham số được cung cấp trong tiểu mục B.2.

5.3 Kết quả và Phân tích
5.3.1 So sánh giữa Hash, Switch và các mô hình Dense

Hash vs. Switch routing trên một lớp duy nhất Đầu tiên chúng tôi so sánh Hash layer (với balanced hash) với Switch layer, trên một Transformer dày đặc khác, nơi định tuyến thưa thớt được thực hiện trên lớp 7 của decoder. Cả hai phương pháp sử dụng 64 FFN chuyên gia với tổng 751M tham số. Kết quả trên pushshift.io Reddit được đưa ra trong Bảng 1 (hàng 4 và 5) và trên dữ liệu RoBERTa+cc100en trong Bảng 2 (hàng 2 và 3). Chúng tôi thấy Hash Layers vượt trội Switch trên cả hai bộ dữ liệu khoảng 0.4-0.5 perplexity.

2Được cung cấp trong Fairseq [36].

--- TRANG 6 ---
10000 30000 50000 70000 90000
Number of Updates2224262830PerplexityPushshift.io Reddit
Transformer
Switch
Hash Layer
0 10000 20000 30000 40000
Number of Updates10111213141516PerplexityRoBERTa + cc100en
BASE
Hash Layer
3x Hash Layer

Hình 2: So sánh Hash Layers với các mô hình khác. (trái) Validation perplexity của baseline Transformer, Switch Transformer, và Hash Layer trên bộ dữ liệu pushshift.io Reddit với 128 module. (phải) Validation perplexity của BASE, Hash Layer, và mô hình Hash Layer sâu hơn trên bộ dữ liệu RoBERTa+cc100en. Tất cả các mô hình thưa thớt có cùng số lượng tham số.

16 32 64 128
Number of Experts23.0023.2523.5023.7524.00Perplexity
Performance by Number of Experts
Switch
Hash Layer
1 3 5 7 9
Layer23.0023.2523.5023.7524.00Perplexity
Position of Hash Layer
Hash Layer

Hình 3: So sánh Số lượng Module Chuyên gia và Vị trí Lớp Khác nhau. Chúng tôi so sánh (trái) validation perplexity theo số lượng module chuyên gia trên tác vụ pushshift.io Reddit cho Hash hoặc Switch Layer trên lớp 7 của decoder 11 lớp trong Transformer. Baseline Transformer đạt perplexity 24.9. Chúng tôi so sánh hiệu suất khi điều chỉnh vị trí lớp của Hash Layer 64 module trên cùng tác vụ (phải). Đặt trên các lớp sau hoạt động tốt nhất.

Dense vs. Sparse Models Cả mô hình thưa thớt Hash và Switch đều vượt trội Baseline dày đặc (222M tham số) mà chúng dựa trên, cũng như Wider Transformer (755M tham số). Tuy nhiên, Deeper Transformer (755M tham số) vượt trội các mô hình thưa thớt có số lượng tham số tương tự. Tuy nhiên, chúng tôi lưu ý rằng do tính toán có điều kiện dày đặc thay vì có điều kiện, nó chậm hơn trong tốc độ suy luận. Chúng tôi xem điều này như một xu hướng chung: các mô hình dày đặc tốt có thể tận dụng nhiều sức mạnh hơn từ cùng số lượng tham số so với các mô hình thưa thớt. Tuy nhiên, các mô hình thưa thớt, mặc dù lãng phí hơn về bộ nhớ, cho perplexity tốt hơn cho cùng tốc độ (tức là, chúng ta nên so sánh với Baseline Transformer trong trường hợp này, có khoảng cùng lượng tính toán).

Hash layer module size Chúng tôi tiến hành cùng các thử nghiệm pushshift.io Reddit như trên, nhưng thay đổi số lượng module chuyên gia trong cả Hash và Switch. Tăng từ 64 lên 128 module (tổng 1.28B tham số) thấy cải thiện thậm chí lớn hơn của Hash so với Switch (khoảng 0.6 perplexity), xem Bảng 1 (hàng 6 và 7), và Hình 2 (trái). Thử các số module nhỏ hơn, 16 và 32, và vẽ đồ thị tất cả kết quả trong Hình 3 (trái) chúng tôi thấy rằng đối với số lượng module nhỏ, Hash và Switch hoạt động tương tự, nhưng khoảng cách tăng lớn hơn khi số lượng module tăng. Đối với số lượng module nhỏ, chúng tôi giả thuyết rằng học định tuyến, như Switch làm, sẽ quan trọng hơn để có hiệu suất với những lựa chọn đó, nhưng với số lượng module lớn hơn nhiều lựa chọn định tuyến có thể hoạt động. Do đó, Hash layers có thể hoạt động tốt trong bối cảnh đó, và học định tuyến trở nên ít quan trọng hơn.

Hash layer position Chúng tôi cũng thử nghiệm để tìm vị trí tốt nhất theo lớp cho định tuyến thưa thớt diễn ra. Trong Hình 3 (phải) chúng tôi vẽ đồ thị perplexity cho Hash Layer 64 module, đặt trên các lớp khác nhau của decoder. Chúng tôi thấy rằng các lớp sau hoạt động tốt hơn, nhưng thậm chí lựa chọn hoạt động tệ nhất (lớp 1) vẫn hoạt động tốt so với các baseline khác: tốt như Switch Transformers sử dụng các lớp sau thực tế. Chúng tôi lưu ý rằng phân tích BASE Layers [10] cho thấy xu hướng tương tự rằng các lớp sau hoạt động tốt. Giả thuyết rằng tính toán có điều kiện cho khả năng tạo ra các chuyên môn hóa tinh tế, suy ra rằng việc tạo ra những phân biệt đó sau khi các đặc trưng rõ ràng hơn đã được trích xuất trước là đáng giá. Chúng tôi sẽ quay lại lập luận này trong các thử nghiệm sau.

--- TRANG 7 ---
Bảng 3: Các Phương pháp Hash Layering Khác nhau trên pushshift.io Reddit.

Model Hashing Type Valid PPL Test PPL
Baseline Transformer - 24.90 24.96
Hash Layer 1x64 Balanced assignment 23.16 23.23
Hash Layer 1x64 Fixed random assignment 23.22 23.27
Hash Layer 1x64 Token clustering (using Baseline Transformer) 23.90 23.99
Hash Layer 1x64 Dispersed Hash (within token clusters) 23.17 23.22
Hash Layer 1x64 Hash on position 25.07 25.14
Hash Layer 1x64 Bigrams 24.19 24.28
Hash Layer 1x64 Previous token 24.16 24.22
Hash Layer 1x64 Future token predictions (using Transformer Baseline) 25.02 25.09
Hash Layer 1x64 Future token (Oracle) 1.97 1.97
Hash Layer 5x16 Same hash per layer (balance assignment) 23.74 23.81
Hash Layer 5x16 Different Hash per layer 23.21 23.27

Hash Bucket (sorted)Bucket FrequencyIdeal balanceRandom Hash Bucket Frequency
Hash Bucket (sorted)Bucket FrequencyIdeal balanceBalanced Hash Bucket Frequency

Hình 4: Tần suất tương đối cho 64 module chuyên gia với Random Hash (trái) và Balanced Hash (phải). Phân phối Zipfian làm cho cân bằng hoàn hảo không thể, nhưng Balanced Hash gần hơn.

Multi-layer routing Chúng tôi đánh giá việc đặt định tuyến thưa thớt mỗi lớp khác, mỗi cái 16 module khác nhau trong Bảng 1 (hàng 8-10). Switch và Hash hoạt động tương tự trong bối cảnh này, với Switch vượt trội với lựa chọn tối ưu 0.1 cân bằng tải (23.00 vs. 23.21), và cùng hiệu suất (23.19) cho tham số cân bằng 0.01. Cho kết quả của Hình 3 (trái), số lượng module nhỏ trong trường hợp này có thể làm cho hiệu suất gần nhau.

Downstream fine-tuning Chúng tôi so sánh một số mô hình pushshift.io Reddit cho mục tiêu tinh chỉnh trên các tác vụ downstream. Chúng tôi thử nghiệm với việc tinh chỉnh toàn bộ mô hình, hoặc đóng băng một số phần của mô hình trong quá trình tinh chỉnh, cũng như thay đổi cân bằng tải cho Switch tại thời điểm fine-tune. Kết quả được đưa ra trong Phụ lục A. Chúng tôi thấy rằng kết quả fine-tune thường đồng ý với hiệu suất ban đầu trên tác vụ pre-training pushshift.io Reddit, và thứ tự các phương pháp được giữ nguyên. Hash vượt trội Switch một chút, cả hai đều vượt trội mô hình Baseline, và các mô hình dày đặc lớn hơn hoạt động tốt hơn, như mong đợi. Đóng băng các phần của mô hình thường làm tổn hại fine-tuning, trừ khi phần được đóng băng là phần thưa thớt của mô hình. Có vẻ như trong trường hợp đó chỉ tinh chỉnh các phần dày đặc của mô hình là đủ cho hiệu suất tốt. Chỉ điều chỉnh phần thưa thớt của mô hình, mặt khác, làm tổn hại hiệu suất, có lẽ vì phần lớn dung lượng của mô hình nằm ở đó.

5.3.2 Phân tích Hàm Băm
Chúng tôi đánh giá các lựa chọn khác nhau của hàm băm được chi tiết trong tiểu mục 3.1. Kết quả tổng thể được đưa ra trong Bảng 3 trên bộ dữ liệu pushshift.io Reddit sử dụng Hash Layer 64 module.

Random và Balanced Hash Functions Chúng tôi thấy rằng gán ngẫu nhiên cố định (hàng 3) và gán cân bằng (hàng 2) hoạt động tương tự tốt về perplexity (23.22 vs. 23.16 valid perplexity). Tuy nhiên, gán cân bằng, như tên gọi, cân bằng hơn, xem Hình 4, có thể làm cho nó hiệu quả hơn về mặt các sơ đồ huấn luyện phân tán.

Clustering Hash Functions Thú vị là, sử dụng các băm dựa trên cụm ("Token clustering", hàng 4) hoạt động rõ ràng tệ hơn các băm ngẫu nhiên (23.90 vs. 23.22). Chúng tôi giả thuyết rằng nếu mục tiêu của tính toán có điều kiện là tạo ra các phân biệt tinh tế, thì những phân biệt đó có nhiều khả năng xuất hiện giữa các token trong cùng một cụm, do đó chúng nên ở các băm khác nhau (các phần của đồ thị tính toán), không phải cùng một cái. Chúng tôi cung cấp bằng chứng một phần cho điều này bằng cách băm trong các cụm token thay vào đó ("Dispersed Hash", hàng 5), điều này khôi phục hiệu suất tương tự như băm ngẫu nhiên

--- TRANG 8 ---
Bảng 4: So sánh các Mô hình trên Wikitext-103. Chúng tôi so sánh baseline dense Transformer với các mô hình thưa thớt của chúng tôi, có 1 lớp thưa thớt với 16 module (1x16). Chúng tôi hiển thị kết quả với hai từ điển khác nhau, từ điển BB [32] BPE (8008 token) và từ điển tiêu chuẩn cho tác vụ (267,739 token). Vì đây là các từ điển khác nhau, perplexity không so sánh được giữa các cột.

Std. Dict BB Dict
Model Configuration Valid PPL Valid PPL
Baseline Transformer layers=8, d=512,D=512 33.09 12.58
Switch Transformer layers=8, modules=1x16, load_bal=0.1 31.76 11.67
Hash Layer layers=8, modules=1x16 32.32 11.58

hash (23.17 vs. 23.22). Chúng tôi lưu ý rằng các phương pháp learn-to-route như Switch Transformers và BASE sử dụng các hàm đơn giản của trạng thái ẩn để thực hiện định tuyến, thường cung cấp các module chuyên gia được nhóm [10], do đó có thể là một bất lợi cho những phương pháp đó.

Position-based Hash Function Chúng tôi tiến hành thử nghiệm băm dựa trên vị trí chuỗi mà thôi. Chúng tôi xem xét thử nghiệm này như một kiểm tra tỉnh táo, chúng tôi không mong đợi việc chọn tính toán có điều kiện dựa trên vị trí trong chuỗi đầu ra sẽ giúp ích. Thực sự, hóa ra điều này không tốt hơn baseline Transformer dày đặc. Do đó có vẻ như định tuyến dựa trên nội dung đầu vào quan trọng hơn nhiều.

Bigram Hash Function Băm dựa trên hai token cuối (bigram) hoạt động tệ hơn việc chỉ sử dụng token cuối (24.19 vs. 23.16). Chúng tôi giả thuyết có hai lý do cho điều này: (1) đầu tiên, token cuối rõ ràng là quan trọng nhất, và bigram thêm một đặc trưng ít liên quan hơn; (2) điều này tạo ra quá nhiều băm, hoạt động kém hơn. Các thử nghiệm tiếp theo sẽ giúp kiểm tra những tuyên bố này.

Previous Token Hashing Băm dựa trên token trước rõ ràng tệ hơn việc sử dụng token hiện tại (24.16 vs. 23.16), và cho hiệu suất tương tự việc sử dụng bigram, giúp xác nhận phần đầu của giả thuyết bigram ở trên.

Dictionary size Chúng tôi thực hiện thử nghiệm trên Wikitext-103 trong hai bối cảnh: sử dụng từ điển cho sẵn 267k token, hoặc sử dụng từ điển 8k chúng tôi sử dụng trong các thử nghiệm pushshift.io Reddit, theo [32]. Kết quả, so sánh với Switch và baseline Transformer, được đưa ra trong Bảng 4. Chúng tôi thấy rằng Hash hoạt động tốt cho từ điển nhỏ, vượt trội Switch một chút. Tuy nhiên, trên từ điển lớn hơn, nó hoạt động tệ hơn Switch. Vì đây là cùng dữ liệu nhưng chỉ token hóa đã thay đổi, chúng tôi kết luận băm được tạo ra từ từ điển nhỏ hơn dễ học hơn, giúp xác nhận phần thứ hai của giả thuyết bigram ở trên.

Oracle Future Token Hashing Chúng tôi đánh giá băm sử dụng oracle token tiếp theo sẽ được dự đoán. Điều này cho perplexity 1.9. Sử dụng thông tin oracle chỉ để chọn giữa các module đủ để về cơ bản giải quyết một tác vụ.

Predicted Future Token Hashing Kết quả cuối đặt ra câu hỏi: nếu chúng ta có thể dự đoán token tiếp theo, và băm dựa trên dự đoán đó thay vào đó – liệu nó có tốt hơn băm trên token hiện tại không? Do đó chúng tôi đã thử băm sử dụng Baseline Transformer để dự đoán nhãn, cho perplexity 25.02 – thực sự không đánh bại chính Baseline. Có vẻ như bias của các dự đoán token giới hạn khả năng của định tuyến thưa thớt để cải thiện.

Multi-hashing Chúng tôi đánh giá kỹ thuật multi-hashing được mô tả trong tiểu mục 3.2. Kết quả được đưa ra trong Phụ lục A, so sánh với Switch và băm tiêu chuẩn. Mặc dù cùng số lượng tham số được sử dụng trong tất cả trường hợp, chúng tôi thấy cải thiện khi chia băm thành 2, 4 hoặc 8 băm khác nhau so với một băm duy nhất, với kết quả cải thiện đều đặn cho cả 16 hoặc 32 module.

5.3.3 Phân tích Switch Transformer
Switch load balancing Chúng tôi hiển thị hiệu suất của Switch cho các giá trị khác nhau của tham số cân bằng tải trên pushshift.io Reddit trong Phụ lục A. Rõ ràng việc lựa chọn tham số quan trọng, với kết quả biến thiên trong phạm vi 1 điểm perplexity.

Switch with Token-based Routing Cho phân tích oracle và predicted token hashing trong tiểu mục 5.3.2, chúng tôi giả thuyết rằng các biểu diễn ẩn trong các lớp của Transformer, bị thiên về các dự đoán của mô hình, có thể không tối ưu cho định tuyến. Do đó chúng tôi thử nghiệm với một hybrid giữa Switch và Hash Layers: trên lớp thưa thớt, thay vì sử dụng trạng thái ẩn như

--- TRANG 9 ---
Bảng 5: Thử nghiệm multi-hashing trên pushshift.io Reddit. Khi multi-hashing, cùng số lượng tham số được sử dụng, nhưng trọng số FFN được chia và được lập chỉ mục thành nhiều băm và sau đó được nối lại với nhau cho bước forward.

Model Configuration Params Valid PPL Test PPL
Switch Transformer layers=11,modules=1x32, load_bal=0.1 483M 23.79 23.84
Hash Layer layers=11,modules=1x32 483M 23.58 23.65
MultiHash Layer layers=11,modules=1x32,hashes=2 483M 23.48 23.53
MultiHash Layer layers=11,modules=1x32,hashes=4 483M 23.38 23.45
MultiHash Layer layers=11,modules=1x32,hashes=8 483M 23.28 23.34

Bảng 6: Switch Transformers với Token-Based Routing trên pushshift.io Reddit. Chúng tôi so sánh Switch tiêu chuẩn định tuyến dựa trên trạng thái ẩn với token feature-routing ('Token Switch').

Model Configuration Params Valid PPL Test PPL
Switch Transformer layers=11,modules=1x64, load_bal=0.1 751M 23.65 23.73
Token Switch layers=11,modules=1x64, load_bal=0.1 751M 23.43 23.43
Switch Transformer layers=11,modules=1x128, load_bal=0.1 1.28B 23.52 23.58
Token Switch layers=11,modules=1x128, load_bal=0.1 1.28B 23.26 23.32

đầu vào router Switch, chúng tôi sử dụng token hiện tại thay vào đó. Để chuyển đổi token thành vector, chúng tôi sử dụng bảng tra cứu bổ sung, tức là, một tập tham số có thể học bổ sung có kích thước bằng từ điển. Những tham số này độc lập với trạng thái ẩn và chỉ được sử dụng bởi router để học tuyến đường tốt nhất.
Kết quả được đưa ra trong Bảng 6. Chúng tôi thấy điều này mang lại một số cải thiện nhỏ cho Switch với 64 và 128 module trên một lớp duy nhất, khẳng định tính hữu ích của định tuyến dựa trên token.

5.3.4 So sánh với BASE Layers
Tiếp theo chúng tôi so sánh với BASE Layers. Sử dụng code base BASE Layer, chúng tôi thực hiện Hash Layers trong chính xác cùng cài đặt, chỉ thay đổi phương pháp định tuyến, và để mọi thứ khác cố định. Hình 2 (phải) hiển thị kết quả so sánh Hash với BASE cho các mô hình 4.5B tham số. Trong suốt toàn bộ quá trình chạy, chúng tôi thấy rằng Hash vượt trội BASE tại mỗi bước huấn luyện. Trong các phần đầu của huấn luyện, Hash có lẽ sẽ có lợi thế trong việc có thể chuyên môn hóa các module chuyên gia sớm hơn, trong khi BASE phải học thành viên cho mỗi module chuyên gia. Sau đó trong huấn luyện, BASE trở nên hơi không ổn định có lẽ vì các phân công chuyên gia thay đổi, trong khi hiệu suất Hash tiếp tục cải thiện một cách mượt mà.
Ngoài ra, để chứng minh Hash Layers vẫn hiệu quả khi được xếp chồng, chúng tôi huấn luyện một mô hình với 3 Hash Layers (sử dụng băm ngẫu nhiên), nhưng ít tham số hơn cho mỗi module chuyên gia để tổng tham số vẫn không đổi ở 4.5B (xem tiểu mục B.2). Chúng tôi thấy rằng sử dụng nhiều Hash Layers cho một cải thiện nhỏ nhưng nhất quán, gợi ý Hash Layers sẽ hiệu quả thậm chí ở độ sâu nhiều hơn.

Ngoài các cải thiện hiệu suất so với BASE, chúng tôi cũng thấy rằng Hash Layers hiệu quả hơn về tổng tính toán. Đặc biệt, BASE yêu cầu hai giao tiếp all-to-all: cái đầu tiên de-correlates các batch để làm cho việc cân bằng phân công ngẫu nhiên hơn, và cái thứ hai định tuyến các trạng thái đến chuyên gia được gán của chúng. Vì Hash Layers sử dụng các phân công cố định, được tính toán trước, chúng tránh bước de-correlation. Trong thực tế, chúng tôi thấy điều này cho cải thiện khoảng 11% trong updates-per-second.
Khi số lượng lớp chuyên gia tăng, sự khác biệt này sẽ trở nên cường điệu hơn.

6 Kết luận
Chúng tôi đã giới thiệu một cách tiếp cận đơn giản và hiệu quả cho các mô hình thưa thớt trong bối cảnh Transformers-for-NLP dựa trên hash layers. Chúng tôi đã chỉ ra trên nhiều bộ dữ liệu và với phân tích trong các bối cảnh khác nhau rằng cách tiếp cận này có tính cạnh tranh cao với các phương pháp hiện có như Switch Transformers và BASE Layers, trong khi mạnh mẽ và đơn giản hơn nhiều – không yêu cầu tham số học bổ sung, thuật toán gán hoặc thay đổi hàm mục tiêu. Cho rằng các nhà nghiên cứu thường chỉ có một cơ hội để huấn luyện các mô hình rất lớn, điều này làm cho cách tiếp cận của chúng tôi trở thành ứng viên mạnh mẽ cho những chạy như vậy.
Trong khi các thử nghiệm của chúng tôi mở rộng lên đến 4.5B tham số, chúng tôi không đạt được quy mô của các công trình công nghiệp lớn như [8], và chúng tôi hy vọng thấy công trình tương lai tiến hành những thử nghiệm như vậy. Cuối cùng, cho rằng cách tiếp cận định tuyến của chúng tôi không cần học, kết quả của chúng tôi có lẽ gợi ý rằng không có cách tiếp cận hiện tại nào đang định tuyến đặc biệt tốt. Do đó chúng tôi tin rằng learn-to-route nên tiếp tục là đối tượng nghiên cứu của công trình tương lai, và xem công trình của chúng tôi như một baseline mạnh mẽ cho nghiên cứu như vậy.

--- TRANG 10 ---
10

--- TRANG 11 ---
Tài liệu Tham khảo
[1]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020.
[2]Robert A Jacobs, Michael I Jordan, Steven J Nowlan, và Geoffrey E Hinton.. Adaptive mixtures
of local experts. Neural computation, 3(1):79–87, 1991.
[3]Yoshua Bengio, Réjean Ducharme, Pascal Vincent, và Christian Janvin. A neural probabilistic
language model. The journal of machine learning research, 3:1137–1155, 2003.
[4]Seniha Esen Yuksel, Joseph N Wilson, và Paul D Gader. Twenty years of mixture of experts.
IEEE transactions on neural networks and learning systems, 23(8):1177–1193, 2012.
[5]David Eigen, Marc'Aurelio Ranzato, và Ilya Sutskever. Learning factored representations in a
deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013.
[6]Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
và Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. arXiv preprint arXiv:1701.06538, 2017.
[7]Sam Gross, Marc'Aurelio Ranzato, và Arthur Szlam. Hard mixtures of experts for large scale
weakly supervised vision. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6865–6873, 2017.
[8]William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion
parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.
[9]Ronan Collobert, Yoshua Bengio, và Samy Bengio. Scaling large learning problems with hard
parallel mixtures. International Journal of pattern recognition and artificial intelligence, 17
(03):349–365, 2003.
[10] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, và Luke Zettlemoyer. Base layers:
Simplifying training of large, sparse models. arXiv preprint arXiv:2103.16716, 2021.
[11] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, và Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.
[12] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pages 5998–6008, 2017.
[13] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, và Clifford Stein. Introduction to
algorithms. MIT press, 2009.
[14] Andrei Z Broder và Anna R Karlin. Multilevel adaptive hashing. In Proceedings of the first
annual ACM-SIAM symposium on Discrete algorithms, pages 43–53, 1990.
[15] Reinhard Kneser và Hermann Ney. Improved backing-off for m-gram language modeling. In
1995 international conference on acoustics, speech, and signal processing, volume 1, pages
181–184. IEEE, 1995.
[16] Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černocký, và Sanjeev Khudanpur. Recurrent neural network based language model. In Eleventh annual conference of the international
speech communication association, 2010.
[17] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, và
Tony Robinson. One billion word benchmark for measuring progress in statistical language
modeling. arXiv preprint arXiv:1312.3005, 2013.
[18] Graham Neubig và Chris Dyer. Generalizing and hybridizing count-based and neural language
models. arXiv preprint arXiv:1606.00499, 2016.

--- TRANG 12 ---
[19] Sébastien Jean, Kyunghyun Cho, Roland Memisevic, và Yoshua Bengio. On using very large
target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007, 2014.
[20] Armand Joulin, Moustapha Cissé, David Grangier, Hervé Jégou, et al. Efficient softmax
approximation for gpus. In International Conference on Machine Learning, pages 1302–1310.
PMLR, 2017.
[21] Anton Bakhtin, Arthur Szlam, Marc'Aurelio Ranzato, và Edouard Grave. Lightweight adaptive
mixture of neural and n-gram language models. arXiv preprint arXiv:1804.07705, 2018.
[22] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, và Josh Attenberg. Feature
hashing for large scale multitask learning. In Proceedings of the 26th annual international
conference on machine learning, pages 1113–1120, 2009.
[23] Bing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi,
Olivier Chapelle, và Kilian Weinberger. Learning to rank with (a lot of) word features.
Information retrieval, 13(3):291–314, 2010.
[24] Aurko Roy, Mohammad Saffar, Ashish Vaswani, và David Grangier. Efficient content-based
sparse attention with routing transformers. Transactions of the Association for Computational
Linguistics, 9:53–68, 2021.
[25] Nikita Kitaev, Łukasz Kaiser, và Anselm Levskaya. Reformer: The efficient transformer.
arXiv preprint arXiv:2001.04451, 2020.
[26] Yinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong, Noah Constant, Petr Pilar, Heming Ge,
Yun-Hsuan Sung, Brian Strope, và Ray Kurzweil. Learning semantic textual similarity from
conversations. In Proceedings of The Third Workshop on Representation Learning for NLP,
pages 164–174, Melbourne, Australia, July 2018. Association for Computational Linguistics.
[27] Pierre-Emmanuel Mazaré, Samuel Humeau, Martin Raison, và Antoine Bordes. Training
millions of personalized dialogue agents. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages 2775–2779, Brussels, Belgium, October-
November 2018. Association for Computational Linguistics.
[28] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, và Richard Socher.
CTRL: A conditional transformer language model for controllable generation. arXiv preprint
arXiv:1909.05858, 2019.
[29] Kurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Y-Lan Boureau, và Jason Weston. The
dialogue dodecathlon: Open-domain knowledge and image grounded conversational agents,
2019.
[30] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, và Jason Weston. Poly-encoders:
Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In
Proceedings of the International Conference on Learning Representations, 2019.
[31] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, và Jeremy Blackburn.
The pushshift reddit dataset. arXiv preprint arXiv:2001.08435, 2020.
[32] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu,
Myle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot.
arXiv preprint arXiv:2004.13637, 2020.
[33] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
[34] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, và Veselin Stoyanov.
Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116,
2019.

--- TRANG 13 ---
[35] Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.
[36] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
và Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint
arXiv:1904.01038, 2019.

--- TRANG 14 ---
A Kết quả Bổ sung
Bảng 7: Thử nghiệm multi-hashing trên pushshift.io Reddit. Khi multi-hashing, cùng số lượng tham số được sử dụng, nhưng trọng số FFN được chia và được lập chỉ mục thành nhiều băm và sau đó được nối lại với nhau cho bước forward.

Model Configuration Params Valid Test
Switch Transformer layers=11,modules=1x16, load_bal=0.1 348M 24.00 24.13
Hash Layer layers=11,modules=1x16 348M 24.01 24.06
MultiHash Layer layers=11,modules=1x16,hashes=2 348M 23.88 23.93
MultiHash Layer layers=11,modules=1x16,hashes=4 348M 23.73 23.80
MultiHash Layer layers=11,modules=1x16,hashes=8 348M 23.83 23.88
Switch Transformer layers=11,modules=1x32, load_bal=0.1 483M 23.79 23.84
Hash Layer layers=11,modules=1x32 483M 23.58 23.65
MultiHash Layer layers=11,modules=1x32,hashes=2 483M 23.48 23.53
MultiHash Layer layers=11,modules=1x32,hashes=4 483M 23.38 23.45
MultiHash Layer layers=11,modules=1x32,hashes=8 483M 23.28 23.34

Bảng 8: Fine-tuning các Mô hình Dense và Sparse trong các cấu hình khác nhau trên BST Tasks.

Model Configuration Params BST Valid
Baseline Transformer layers=11, d=1024, D=4096 222M 14.21
Wider Transformer layers=11, d=2048, D=6144 755M 12.48
Deeper Transformer layers=22, d=1536, D=4096 755M 12.83
Switch 1x64 No weights frozen, load_bal=0.0 751M 13.67
Switch 1x64 No weights frozen, load_bal=0.1 751M 13.67
Switch 1x64 Switch weights frozen 751M 13.65
Switch 1x64 Router weights frozen 751M 13.61
Switch 1x64 All layers but last frozen 751M 14.42
Switch 1x64 All layers but Switch frozen 751M 14.37
Hash 1x64 No weights frozen 751M 13.45
Hash 1x64 Hash weights frozen 751M 13.56
Hash 1x64 All layers but last frozen 751M 14.29
Hash 1x64 All layers but Hash frozen 751M 14.12

Bảng 9: Switch Transformer Load Balancing. Chúng tôi hiển thị perplexity với 64 module trên tác vụ pushshift.io Reddit cho các tham số cân bằng tải khác nhau. Việc lựa chọn tham số quan trọng; không có cân bằng, mô hình hoạt động tệ hơn.

Model Load balance Valid Test
Baseline Transformer - 24.90 24.96
Switch 0 24.80 24.86
Switch 0.01 23.95 24.01
Switch 0.05 23.68 23.74
Switch 0.1 23.65 23.73
Switch 0.5 23.68 23.74

--- TRANG 15 ---
B Siêu tham số
B.1 So sánh với Switch
Chúng tôi đưa ra ở đây các tham số được sử dụng trong cài đặt pushshift.io Reddit và RoBERTa+cc100en tiêu chuẩn của chúng tôi.
Các thử nghiệm khác với thay đổi tham số khác với những cái này được chỉ ra trong văn bản chính.

Hyperparameter Switch Hash Layer
Total parameters 751,224,896 751,159,296
Expert Modules per MoE layer 64 64
Number of MoE layers 1 1
FFNs per Expert Module 1 1
Embedding Size 1024 1024
FFN Size 4096 4096
Attention Heads 16 16
Number of encoder layers 2 2
Number of decoder layers 11 11
Context Length 128 128
Label Length 128 128
Batchsize 40 40
Gradient Accumulation 1 1
Maximum LR 0.002 0.002
Warmup 10,000 steps 10,000 steps
LR Scheduler InvSqrt InvSqrt
Maximum steps 100,000 100,000
Optimizer ADAM ADAM
Gradient Clip 1.0 1.0

B.2 So sánh với Base

Hyperparameter BASE Hash Layer 3x Hash Layer
Shared parameters 1,313,460,224 1,313,460,224 1,313,460,224
Parameters per Expert 100,706,304 100,706,304 33,568,768
Total parameters 4,536,061,952 4,536,061,952 4,536,061,952
Expert Modules per MoE layer 32 32 32
Number of MoE layers 1 1 3
FFNs per Expert Module 3 3 1
Embedding Size 2048 2048 2048
FFN Size 8192 8192 8192
Attention Heads 16 16 16
Number of shared layers 24 24 24
Context Length 1024 1024 1024
Batchsize 2 2 2
Gradient Accumulation 4 4 4
Total tokens per update 512k 512k 512k
Maximum LR 7.5e-4 7.5e-4 7.5e-4
Warmup 2000 steps 2000 steps 2000 steps
LR Scheduler Poly Decay Poly Decay Poly Decay
Maximum steps 62,500 62,500 62,500
Optimizer ADAM ADAM ADAM
Gradient Clip 0.1 0.1 0.1

Lưu ý rằng trong các so sánh với BASE, chúng tôi sử dụng phương pháp gradient clipping của BASE, tính toán gradient norm chỉ dựa trên các tham số chia sẻ để tránh giao tiếp bổ sung giữa các thiết bị.

--- TRANG 16 ---
C Tài nguyên Tính toán
Tất cả các thử nghiệm được chạy trên một cluster nội bộ. Trừ khi được đánh dấu khác, tất cả các thử nghiệm sử dụng 8 GPU V100 32GB trong khoảng 20 giờ.

Ngoại lệ:
• Các baseline Transformer dày đặc lớn hơn và thử nghiệm 128 module sử dụng 16 V100.
• Các so sánh với BASE sử dụng 32 V100 trong khoảng 2 ngày.

D Tác động Xã hội
Các cải thiện trong mô hình hóa ngôn ngữ có thể có tác động trên một số lượng lớn bề mặt khắp nhân loại. Hash Layer cũng có thể được sử dụng để huấn luyện các mô hình lớn hơn nhiều, có thể có tác động gia tăng đến môi trường, mặc dù với chi phí phân số so với các mô hình dày đặc tương đương tham số. Hash Layer cũng cung cấp một giảm thiểu không tầm thường trong tài nguyên tính toán so với công trình trước đó của BASE.

Các bộ dữ liệu được sử dụng trong công trình này chứa nội dung văn bản đa dạng và có thể gây phản cảm, vì chúng ban đầu được thu thập từ Internet bởi các bên thứ ba. Giảm thiểu các tác động tiêu cực của những nỗ lực này là một lĩnh vực nghiên cứu quan trọng, nhưng ngoài phạm vi của bài báo này. Chúng tôi mong đợi (nhưng không chứng minh) rằng những nỗ lực giảm thiểu như vậy có khả năng trực giao và bổ sung cho công trình của chúng tôi về cải thiện kiến trúc.
