# 2308.12066.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2308.12066.pdf
# Kích thước tệp: 1342561 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Pre-gated MoE: Thiết kế Đồng thời Thuật toán-Hệ thống cho
Suy luận Mixture-of-Expert Nhanh và Có thể Mở rộng
Ranggi Hwang∗†
KAIST
ranggi.hwang@kaist.ac.krJianyu Wei∗†
USTC / Microsoft Research
noob@mail.ustc.edu.cnShijie Cao
Microsoft Research
shijiecao@microsoft.com
Changho Hwang
Microsoft Research
changhohwang@microsoft.comXiaohu Tang†
Microsoft Research
v-xiaohutang@microsoft.comTing Cao
Microsoft Research
ting.cao@microsoft.comMao Yang
Microsoft Research
maoyang@microsoft.com
Tóm tắt—Các mô hình ngôn ngữ lớn (LLM) dựa trên transformer đã có những bước tiến đáng kể trong những năm gần đây, thành công này được thúc đẩy bởi việc mở rộng kích thước mô hình của chúng. Mặc dù có hiệu suất thuật toán cao, các yêu cầu tính toán và bộ nhớ của LLM đặt ra những thách thức chưa từng có. Để giải quyết yêu cầu tính toán cao của LLM, kiến trúc Mixture-of-Experts (MoE) đã được giới thiệu, có thể mở rộng kích thước mô hình mà không cần tỷ lệ thuận với yêu cầu tính toán. Thật không may, nhu cầu bộ nhớ cao và việc kích hoạt động các chuyên gia thưa thớt của MoE hạn chế khả năng ứng dụng vào các vấn đề thực tế. Các giải pháp trước đây offload các tham số chuyên gia đói bộ nhớ của MoE vào bộ nhớ CPU không đạt hiệu quả vì độ trễ để di chuyển các chuyên gia được kích hoạt từ CPU sang GPU gây ra overhead hiệu suất cao. Hệ thống Pre-gated MoE được đề xuất của chúng tôi giải quyết hiệu quả các thách thức tính toán và bộ nhớ của kiến trúc MoE thông thường bằng cách sử dụng thiết kế đồng thời thuật toán-hệ thống. Pre-gated MoE sử dụng hàm pre-gating mới của chúng tôi, giúp giảm bớt tính chất động của việc kích hoạt chuyên gia thưa thớt, cho phép hệ thống được đề xuất giải quyết dung lượng bộ nhớ lớn của MoE trong khi vẫn đạt hiệu suất cao. Chúng tôi chứng minh rằng Pre-gated MoE có thể cải thiện hiệu suất, giảm mức tiêu thụ bộ nhớ GPU, đồng thời duy trì cùng mức độ chất lượng mô hình. Những tính năng này cho phép hệ thống Pre-gated MoE triển khai các LLM quy mô lớn một cách hiệu quả về chi phí chỉ bằng một GPU duy nhất với hiệu suất cao.
Thuật ngữ chỉ mục—Mixture-of-expert, hệ thống suy luận, machine learning, mô hình ngôn ngữ lớn, offloading bộ nhớ

I. GIỚI THIỆU
Các ứng dụng machine learning (ML) dựa trên mô hình ngôn ngữ lớn (LLM) đã gây bão trên toàn thế giới, được triển khai rộng rãi trong các sản phẩm hướng tới người tiêu dùng [24], [26], [33]. Thành công của LLM được thúc đẩy bởi việc mở rộng dung lượng mô hình (tức là kích thước mô hình) và tập dữ liệu huấn luyện, kích thước mô hình được huấn luyện lớn nhất tăng khoảng 1.000× trong 5 năm qua, từ vài trăm triệu tham số đến gần quy mô một nghìn tỷ tham số [3], [5], [39]. Với kích thước mô hình lớn hơn mang lại độ chính xác cao hơn, có khả năng các mô hình tương lai cũng sẽ tăng dung lượng mô hình.
Tuy nhiên, một thách thức quan trọng trong việc tăng trưởng bền vững kích thước mô hình là yêu cầu tính toán ngày càng khắt khe.

Để giải quyết yêu cầu tính toán cao của LLM, mô hình Mixture-of-Experts (MoE) [37] đã được đề xuất như một giải pháp thay thế cho các LLM dense trước đây [3], [5], [29], [39]. Sức mạnh của MoE đến từ khả năng mở rộng dung lượng mô hình bằng cách tăng số lượng tham số chuyên gia trong một khối MoE. Mặc dù kích thước tham số mô hình tăng lên, MoE sử dụng hàm gate để chỉ kích hoạt một phần các chuyên gia theo cách thưa thớt, cho phép chúng đạt được chi phí tính toán dưới tuyến tính so với dung lượng mô hình. Ngược lại, các LLM dense trước đây kích hoạt toàn bộ tham số mô hình để suy luận và gây ra chi phí tính toán tăng bình phương theo kích thước mô hình, dẫn đến overhead tính toán đáng kể.

Mặc dù có những ưu điểm, một thách thức quan trọng của MoE là yêu cầu bộ nhớ lớn và các chuyên gia thưa thớt được kích hoạt động gây ra chi phí triển khai cao, làm hạn chế khả năng ứng dụng của MoE trong các vấn đề thực tế.

1)Yêu cầu bộ nhớ lớn của các chuyên gia. Trong khi việc kích hoạt thưa thớt các tham số mô hình (tức là các chuyên gia) giúp giảm chi phí tính toán để đạt được chất lượng mô hình tương đương với các đối tác LLM dense, MoE yêu cầu bộ nhớ lớn hơn đáng kể để chứa số lượng lớn các chuyên gia. Ví dụ, một Google Switch-Transformer [8] dựa trên MoE có thể có tới 75× tham số nhiều hơn so với mô hình T5 dense tương đương về FLOPs [29]. Nói cách khác, MoE có hiệu quả bộ nhớ thấp hơn nhiều so với các LLM dense, mang lại những thách thức quan trọng ở cấp hệ thống trong việc triển khai các LLM dựa trên MoE. Để chứa kích thước mô hình lớn của MoE dưới kích thước bộ nhớ hạn chế của GPU, có thể sử dụng nhiều GPU để triển khai MoE, trong đó expert parallelism được sử dụng để phân phối các tham số chuyên gia trên các GPU để chỉ lưu trữ một phần của các chuyên gia để suy luận [18], [19], [30].

2)Kích hoạt động và thưa thớt của các chuyên gia. Mặc dù các giải pháp đa GPU có thể phân phối tham số chuyên gia trên bộ nhớ GPU, MoE chỉ kích hoạt một phần các chuyên gia theo cách thưa thớt [37]. Điều này làm cho số lượng chuyên gia thực sự được sử dụng trên mỗi GPU trở nên rất nhỏ hoặc không tồn tại (tức là không có chuyên gia nào trong GPU được kích hoạt, để GPU rảnh rỗi) [21]. Hơn nữa, việc kích hoạt chuyên gia thưa thớt được quyết định động vào thời gian chạy, khiến việc dự đoán số lượng chuyên gia sẽ được kích hoạt trong mỗi GPU trở nên khó khăn. Do đó, tính toán hiệu quả được thực hiện trên mỗi GPU trở nên thấp, thể hiện mức sử dụng GPU tính toán thấp và làm trầm trọng thêm tổng chi phí sở hữu (TCO) để triển khai.

Nghiên cứu trước đây về triển khai MoE tìm cách giải quyết những thách thức kép này bằng cách offload các tham số chuyên gia đói bộ nhớ của MoE vào bộ nhớ CPU hoặc SSD [1], [14], [18], [38] (được gọi là MoE-offload dưới đây). Lợi ích của MoE-offload là giảm số lượng GPU cần thiết để triển khai MoE, giúp tăng hiệu quả tính toán của GPU để suy luận. Tuy nhiên, việc offload các tham số MoE không phải là giải pháp hoàn hảo vì nó đi kèm với sự gia tăng đáng kể về độ trễ suy luận, làm giảm chất lượng dịch vụ (QoS) cho người dùng cuối. Điều này là do CPU offloading chỉ có thể giải quyết yêu cầu bộ nhớ lớn của MoE mà không giải quyết vấn đề phụ thuộc dữ liệu phát sinh với việc kích hoạt thưa thớt động, một đặc điểm độc đáo của MoE. Trong một khối MoE, tồn tại một phụ thuộc tuần tự giữa (1) "lựa chọn" những chuyên gia nào nên được kích hoạt (sử dụng hàm gate của MoE), và (2) "thực thi" các chuyên gia được kích hoạt để suy luận. Vì phụ thuộc dữ liệu như vậy được giải quyết động bởi dữ liệu đầu vào, quá trình hai giai đoạn của (1) lựa chọn chuyên gia và (2) thực thi chuyên gia phải được tuần tự liên tiếp. Do đó, độ trễ để di chuyển các tham số chuyên gia được kích hoạt từ CPU sang GPU không thể được ẩn dưới MoE-offload và gây ra overhead hiệu suất nghiêm trọng, không thể giải quyết các thách thức nêu trên của việc triển khai MoE (Mục III-B).

Trong nghiên cứu này, chúng tôi đề xuất Pre-gated MoE, một thiết kế đồng thời thuật toán-hệ thống cho phép suy luận MoE có mức tiêu thụ bộ nhớ GPU thấp trong khi vẫn đạt hiệu suất cao, giảm đáng kể TCO. Chúng tôi tóm tắt ngắn gọn những đóng góp chính và tính mới của Pre-gated MoE dưới đây.

•(Thuật toán) Trong kiến trúc MoE thông thường, hàm gate trong khối MoE thứ N lựa chọn các chuyên gia để kích hoạt sau đó sẽ được thực thi trong cùng khối MoE thứ N. Trong thiết kế được đề xuất của chúng tôi, chúng tôi sửa đổi vai trò của hàm gate để lựa chọn trước các chuyên gia sẽ được kích hoạt cho khối MoE tiếp theo (do đó có tên mới là hàm pre-gate). Cụ thể hơn, hàm pre-gate trong khối MoE thứ N lựa chọn các chuyên gia để kích hoạt cho khối MoE thứ (N+1). Tính mới của hàm pre-gate nằm ở khả năng loại bỏ hoàn toàn phụ thuộc tuần tự giữa giai đoạn lựa chọn chuyên gia và giai đoạn thực thi chuyên gia trong bất kỳ khối MoE nào (tức là phụ thuộc dữ liệu hiện tồn tại giữa việc lựa chọn chuyên gia của khối MoE thứ N và thực thi chuyên gia của khối thứ (N+1)), mà hệ thống được đề xuất sử dụng hiệu quả cho tối ưu hóa hiệu suất như chi tiết dưới đây.

•(Hệ thống) Tương tự như các hệ thống MoE-offload trước đây, Pre-gated MoE của chúng tôi lưu trữ các tham số chuyên gia hạn chế về dung lượng bộ nhớ trong bộ nhớ CPU và giảm số lượng GPU cần thiết để suy luận. Khác với MoE-offload, Pre-gated MoE sử dụng hàm pre-gate để chồng chéo độ trễ di chuyển chuyên gia CPU→GPU với giai đoạn thực thi chuyên gia, giảm thiểu tác động của việc di chuyển chuyên gia đến hiệu suất. Cụ thể, Pre-gated MoE sử dụng hàm pre-gate thứ N để xác định tập hợp các chuyên gia sẽ kích hoạt cho khối MoE thứ (N+1), trước, hiệu quả prefetch chỉ các chuyên gia được kích hoạt lên GPU để chuẩn bị cho việc thực thi khối thứ (N+1) trong khi đồng thời thực hiện thực thi chuyên gia cho khối MoE thứ N.

Chúng tôi đánh giá hệ thống Pre-gated MoE bằng các mô hình MoE tiên tiến, đạt được độ chính xác mô hình tương đương hoặc thậm chí cao hơn so với mô hình MoE gốc trên một loạt các tác vụ xử lý ngôn ngữ tự nhiên (NLP) (ví dụ: tóm tắt và trả lời câu hỏi). Đồng thời, việc tách biệt giai đoạn lựa chọn chuyên gia và giai đoạn thực thi chuyên gia cho phép Pre-gated MoE giảm đáng kể độ trễ suy luận đầu cuối, chỉ thêm 23% overhead hiệu suất so với giải pháp tối ưu hiệu suất, chỉ sử dụng GPU có thể lưu trữ toàn bộ tham số MoE trong bộ nhớ GPU. Pre-gated MoE cũng giảm mức tiêu thụ bộ nhớ GPU đỉnh 4,2× so với chỉ sử dụng GPU, cho phép triển khai các LLM lớn hơn trong một GPU duy nhất. Nhìn chung, hệ thống Pre-gated MoE đưa ra một giải pháp nhanh, có thể mở rộng và hiệu quả về chi phí để phục vụ các LLM dựa trên MoE.

II. BỐI CẢNH

A. LLM Dense sử dụng Transformers
Kiến trúc mô hình Transformer. Các mô hình Transformer [42] đã trở thành cách tiếp cận chủ đạo trong thiết kế các ứng dụng ML cho xử lý ngôn ngữ tự nhiên (NLP), do khả năng nắm bắt các phụ thuộc tầm xa và các mẫu phức tạp trong dữ liệu [6], [39]. Có hai cách chính để cấu trúc mô hình transformer: kiến trúc encoder-decoder [29] và kiến trúc chỉ decoder [3]. Kiến trúc encoder-decoder bao gồm một module encoder xử lý dữ liệu đầu vào (chuỗi các token đầu vào) và một module decoder tạo ra đầu ra (một token đầu ra trên mỗi decoder). Mặt khác, kiến trúc chỉ decoder ngầm kết hợp quá trình mã hóa trong các khối transformer, loại bỏ nhu cầu về một module encoder riêng biệt. Bất kể kiến trúc nào được sử dụng, cả kiến trúc encoder-decoder và chỉ decoder đều sử dụng các thành phần chính sau của transformer: lớp self-attention, lớp feed-forward networks (FFN) position-wise, normalization và residual connection, như thể hiện trong Hình 1(a). Self-attention giúp xác định mối quan hệ giữa các từ và phụ thuộc của chúng trong một chuỗi, trong khi lớp FFN áp dụng các biến đổi phi tuyến để nắm bắt các mẫu phức tạp trong dữ liệu đầu vào. Cả self-attention và FFN đều chiếm một phần đáng kể trong tính toán cũng như yêu cầu bộ nhớ của các mô hình transformer.

--- TRANG 3 ---
Lớp FFN
ChuẩnhóaĐầu raĐầu vàoLớp AttentionChuẩnhóa(a)Khối transformer dense.

Chuyên gia 1Chuyên gia 2Chuyên gia 3Chuyên gia 0
ChuẩnhóaHàm GateĐầu raĐầu vàoLớp AttentionChuẩnhóa
(b)Khối MoE thưa thớt.

Hình 1: (a) Một khối transformer dense bao gồm lớp self-attention, lớp feed-forward networks (FFN), normalization và residual connection. (b) Một khối MoE thay thế lớp FFN của khối transformer thông thường để tạo ra tính thưa thớt. Ví dụ giả định khối MoE có bốn lớp chuyên gia. Mỗi chuyên gia có cùng kích thước với lớp FFN của khối transformer dense tương ứng.

Thách thức trong mở rộng LLM dense. Thành công của các LLM dense dựa trên transformer chủ yếu được thúc đẩy bởi việc mở rộng dung lượng mô hình (tức là kích thước mô hình) bằng cách xếp chồng một loạt các khối transformer [17], [28], mang lại độ chính xác mô hình cao hơn. Tuy nhiên, một thách thức chính trong việc tăng trưởng bền vững dung lượng mô hình là chi phí tính toán và bộ nhớ ngày càng khắt khe, cho cả huấn luyện và suy luận. Đặc biệt, khi kích thước của LLM tăng lên, nhu cầu về tính toán và bộ nhớ tăng theo cấp số nhân, khiến việc đưa mô hình vào trong các ràng buộc bộ nhớ của GPU hiện đại trở nên khó khăn trong khi vẫn duy trì hiệu quả tính toán cao [40]. Hơn nữa, chi phí năng lượng liên quan đến huấn luyện các LLM này đang tăng đáng kể, gây ra mối quan ngại nghiêm trọng về tác động môi trường của việc huấn luyện và phục vụ LLM [27], [44].

B. LLM Sparse sử dụng Mixture-of-Experts (MoE)
Kiến trúc mô hình MoE. Để giải quyết yêu cầu tính toán cao của LLM dense, mô hình Mixture-of-Experts (MoE) [7], [8], [11], [37], [41] đã được giới thiệu, khai thác tính thưa thớt trong kiến trúc mô hình để giảm chi phí tính toán cao của LLM. MoE được thiết kế để bắt chước hành vi của bộ não con người, bao gồm các vùng chuyên biệt được điều chỉnh cho các tác vụ cụ thể. Bằng cách kích hoạt thưa thớt chỉ một tập con các tham số, MoE có thể mở rộng kích thước mô hình mà không có sự gia tăng tương ứng về chi phí tính toán (FLOPs).

Hình 1(b) minh họa kiến trúc mô hình của một khối MoE, được chuyển đổi từ khối transformer dense trong Hình 1(a) bằng cách thay thế lớp FFN gốc trong khối transformer bằng một khối MoE. Khối MoE bao gồm hai thành phần chính: hàm gate và lớp chuyên gia. Hàm gate chịu trách nhiệm xác định mức độ liên quan của mỗi chuyên gia đối với một token đầu vào nhất định, từ đó gán xác suất cho mỗi chuyên gia dựa trên tầm quan trọng của họ đối với

0100200300400500
1 8 16 32 64 128 256 1 128
Dense MoE Dense MoE
Switch-Base Switch-LargeGFLOPs /seqHình 2: Số lượng FLOPs cần thiết cho mỗi chuỗi khi triển khai Switch-Transformer (MoE) và T5 (dense). Trong hình này, chúng tôi thể hiện cả phiên bản mô hình "Base" và "Large" của SwitchTransformer và T5 tương đương FLOPs (Mục V chi tiết cấu hình mô hình được nghiên cứu trong công trình này). Các con số đại diện cho số lượng chuyên gia có sẵn trong khối MoE (tức là T5 dense tương đương với việc chỉ có một chuyên gia duy nhất).

đầu vào cụ thể. Mặt khác, lớp chuyên gia là một lớp FFN dense tập trung vào việc xử lý các mẫu riêng biệt trong dữ liệu đầu vào. Trong quá trình suy luận mô hình, hàm gate lựa chọn những chuyên gia nào nên được kích hoạt cho mỗi token đầu vào dựa trên xác suất được gán của chúng. Sau đó, các chuyên gia được kích hoạt xử lý các token đầu vào bằng cách thực thi các token đầu vào được gán và tạo ra các token đầu ra. Do đó, việc đánh giá một khối MoE bao gồm một quy trình hai giai đoạn, (1) lựa chọn chuyên gia và (2) thực thi chuyên gia, một quy trình phụ thuộc dữ liệu đầu vào phải được thực hiện tuần tự.

Trong các mô hình MoE tiên tiến, số lượng chuyên gia được kích hoạt thường rất nhỏ (ví dụ: Google's Switch-Transformer [8] và Meta's NLLB-MoE [41] chỉ kích hoạt top-1 và top-2 chuyên gia tương ứng), làm cho suy luận của MoE thể hiện tính thưa thớt cao.

Chi phí tính toán của MoE sparse so với LLM dense. Hiệu quả tính toán của MoE đạt được bằng cách kích hoạt có chọn lọc một tập con nhỏ các chuyên gia cho mỗi token đầu vào, thay vì kết nối dense tất cả các lớp và neuron trong mô hình. Hình 2 so sánh số lượng FLOPs cần thiết cho mỗi chuỗi giữa một mô hình MoE sparse đại diện (Google's SwitchTransformer [8]) và đối tác mô hình dense với số lượng FLOPs iso (Google's T5 [29]). Như thể hiện, chi phí tính toán của MoE vẫn không đổi, bất kể số lượng chuyên gia (tức là kích thước mô hình), nhấn mạnh thực tế là MoE có thể mở rộng dung lượng mô hình với overhead tính toán tối thiểu.

III. ĐỘNG LỰC

A. Các thách thức chính của suy luận MoE
Trong khi MoE mang lại lợi thế trong việc mở rộng kích thước mô hình LLM mà không tăng đáng kể chi phí tính toán, nó đặt ra một số thách thức chính như tóm tắt dưới đây.

1)Dung lượng bộ nhớ lớn. Lợi thế lớn nhất của MoE là hiệu quả tính toán cao, đến từ khả năng mở rộng dung lượng mô hình một cách hiệu quả về chi phí bằng cách sử dụng một số lượng lớn chuyên gia. Tuy nhiên, điều này đi kèm với chi phí tiêu thụ bộ nhớ cao, khiến dung lượng bộ nhớ tổng thể của MoE trở nên

--- TRANG 4 ---
020406080100120
181632641282561128DenseMoEDenseMoESwitch-BaseSwitch-LargeDung lượng mô hình (GB)Tham số MoETham số Non-MoEHình 3: Yêu cầu dung lượng bộ nhớ để triển khai SwitchTransformer (MoE) và T5 (dense). Tham số MoE bao gồm cả lớp chuyên gia và hàm gate, trong khi phần còn lại của các lớp được đánh dấu là tham số Non-MoE. Như mô tả, tham số chuyên gia MoE chiếm phần lớn mức tiêu thụ bộ nhớ của mô hình.

lớn hơn một bậc so với đối tác dense, ví dụ: SwitchTransformer có thể tiêu thụ tới 75× mức tiêu thụ bộ nhớ cao hơn so với T5 dense (Hình 3). Việc sử dụng bộ nhớ lớn như vậy đặt ra một số trở ngại cho MoE, một thách thức quan trọng là không thể đưa mô hình vào trong bộ nhớ cục bộ của một GPU duy nhất, chỉ có kích thước vài chục GB.

2)Kích hoạt chuyên gia động và thưa thớt. Vì việc lưu trữ toàn bộ tham số mô hình MoE trong một GPU duy nhất là thách thức, các giải pháp đa GPU chia tham số chuyên gia trên bộ nhớ của nhiều GPU có thể là giải pháp khả thi cho suy luận MoE hiệu suất cao. Thật không may, vì MoE chỉ kích hoạt một phần các chuyên gia theo cách thưa thớt, số lượng chuyên gia thực sự được thực thi bởi mỗi GPU để suy luận trở nên rất thấp. Trên thực tế, các giải pháp đa GPU để triển khai MoE gặp phải mức sử dụng tính toán GPU thấp và làm giảm TCO. Hơn nữa, vì các chuyên gia MoE được kích hoạt thưa thớt, một phần đáng kể tham số chuyên gia được phân bổ bên trong bộ nhớ GPU đắt tiền rất có thể sẽ không được sử dụng khi phục vụ bất kỳ yêu cầu suy luận nào (ví dụ: SwitchTransformer kích hoạt top-1 trong số 128 chuyên gia chỉ thực thi 0,8% chuyên gia của nó mỗi suy luận). Cuối cùng, vì các chuyên gia được kích hoạt thưa thớt được xác định động theo cách phụ thuộc dữ liệu đầu vào, việc dự đoán chuyên gia nào sẽ được kích hoạt vào thời gian chạy trở nên khó khăn, ngăn chặn bất kỳ giải pháp cân bằng tải nào để phân phối tốt hơn số lượng chuyên gia được kích hoạt trên các GPU một cách đều đặn. Vì dung lượng bộ nhớ hạn chế của GPU chính là lý do tại sao một hệ thống đa GPU là cần thiết để triển khai MoE, việc sử dụng bộ nhớ GPU dưới mức tối ưu như vậy là một sự lãng phí đáng kể.

B. Giải pháp trước đây: CPU Offloading các tham số chuyên gia
Thách thức quản lý hiệu quả kích thước mô hình lớn của LLM trong các ràng buộc bộ nhớ GPU hạn chế đã dẫn đến một số nghiên cứu trước đây ủng hộ việc offload các tham số LLM đói bộ nhớ vào CPU DRAM hoặc thậm chí SSD [31], [35]. Các phương pháp dựa trên CPU offload này cũng đã được khám phá trong bối cảnh MoE để giải quyết các thách thức nêu trên, tức là dung lượng bộ nhớ lớn và chi phí triển khai cao [1], [14], [15], [38]. Mặc dù các giải pháp dựa trên CPU offload có thể giúp giảm số lượng GPU cần thiết để phục vụ MoE, độ trễ để chuyển các tham số mô hình được offload CPU sang bộ nhớ GPU có thể làm giảm hiệu suất đầu cuối. Điều này là do không có nghiên cứu trước đây nào giải quyết cơ bản thách thức kích hoạt chuyên gia động và thưa thớt và vấn đề phụ thuộc tuần tự của nó. Dưới đây chúng tôi phân loại các giải pháp offload CPU trước đây thành hai loại, (1) fetch-on-demand và (2) prefetch-all, thảo luận về lợi ích cũng như hạn chế của chúng.

Fetch-on-demand. Điểm thiết kế này [15] sử dụng CPU offloading dựa trên fetch-on-demand cho việc phục vụ MoE. Dưới thiết kế hệ thống này, tất cả tham số chuyên gia được offload vào bộ nhớ CPU tối ưu hóa dung lượng. Vào thời gian chạy, một khi giai đoạn lựa chọn chuyên gia xác định những chuyên gia nào được kích hoạt, những chuyên gia được kích hoạt đó được di chuyển vào bộ nhớ GPU theo yêu cầu. Vì bộ nhớ GPU chỉ được sử dụng để lưu trữ các chuyên gia được kích hoạt (và không phải toàn bộ chuyên gia như được thực hiện trong hệ thống đa GPU cơ bản), nó giúp cải thiện đáng kể việc sử dụng bộ nhớ GPU. Tuy nhiên, quá trình di chuyển các chuyên gia được kích hoạt theo yêu cầu làm tuần tự hóa giai đoạn lựa chọn chuyên gia với giai đoạn thực thi chuyên gia, gây ra overhead hiệu suất đáng chú ý. Trong phần còn lại của bài báo này, chúng tôi gọi điểm thiết kế này là MoE-OnDemand.

Prefetch-all. Để ẩn tốt hơn độ trễ chuyển chuyên gia CPU→GPU, nghiên cứu trước đây về SE-MoE [38] đề xuất một CPU offloading dựa trên prefetching cho MoE, trong đó các tham số chuyên gia được di chuyển chủ động vào bộ nhớ GPU trước khi sử dụng thực tế (từ nay được gọi là MoE-Prefetch). Tương tự như MoE-OnDemand, MoE-Prefetch offload tất cả tham số chuyên gia trong bộ nhớ CPU. MoE-Prefetch sau đó di chuyển toàn bộ tham số chuyên gia được sử dụng bởi khối MoE tiếp theo trong khi việc thực thi chuyên gia của khối MoE hiện tại đang diễn ra. Trong khi MoE-Prefetch có thể giúp chồng chéo tính toán (thực thi chuyên gia khối MoE hiện tại) với giao tiếp (chuyển tất cả chuyên gia cần thiết cho khối MoE tiếp theo), nó gặp phải một số hạn chế. Thứ nhất, MoE-Prefetch không thể mở rộng vì thời gian chuyển chuyên gia CPU→GPU có thể bị cấm khi tồn tại một số lượng lớn chuyên gia (ví dụ: Google's SwitchTransformer [8] chứa tới 256 chuyên gia, trong khi Meta's NLLB-MoE [41] sử dụng 128 chuyên gia). Thứ hai, tại bất kỳ thời điểm thực thi khối MoE nào, bộ nhớ GPU phải đủ lớn để lưu trữ cả tham số lớp chuyên gia hiện tại cũng như khối MoE tiếp theo, có thể làm quá tải bộ nhớ GPU khan hiếm. Để giảm bớt nhu cầu bộ nhớ GPU đáng kể cần thiết để chuyển toàn bộ tham số chuyên gia, người ta có thể cân nhắc việc prefetch chỉ một tập con các chuyên gia được dự đoán sẽ hoạt động trong khối tiếp theo. Tuy nhiên, việc dự đoán các chuyên gia hoạt động không phải lúc nào cũng đảm bảo độ chính xác, và các dự đoán sai có thể dẫn đến các hình phạt như lãng phí bộ nhớ GPU và sự tuần tự hóa không thể tránh khỏi của độ trễ chuyển chuyên gia, do đó làm tăng độ trễ suy luận đầu cuối.

Ngoài những nghiên cứu trước đây này tập trung vào các quyết định CPU offloading cho suy luận MoE, [14] đặc trưng hóa MoE

--- TRANG 5 ---
HBM GPU
DDR CPU
SSDPCIe
Các lớp MoEActivationCác lớp Non-MoE
80GB >
1TB >
>1TBPCIeHình 4: Hệ thống Pre-gated MoE để triển khai các LLM dựa trên MoE. Các tham số MoE đói bộ nhớ, thưa thớt được offload hoàn toàn vào bộ nhớ CPU tối ưu hóa dung lượng và chỉ được chuyển sang bộ nhớ GPU khi cần thiết để suy luận. Phần còn lại của các tham số dense, non-MoE được lưu trữ cục bộ trong bộ nhớ GPU.

độ trễ suy luận triển khai và việc sử dụng bộ nhớ trên các thành phần khác nhau của kiến trúc mô hình MoE, đề xuất một số chiến lược tối ưu hóa như dynamic gating và expert buffering. Dynamic gating giúp giảm phân bổ bộ nhớ lãng phí cho suy luận MoE đa GPU và expert buffering là một kỹ thuật có thể giúp giảm độ trễ suy luận bằng cách cache các chuyên gia hoạt động nóng trong bộ nhớ GPU. Trong Mục VI-D, chúng tôi thảo luận thêm về khả năng áp dụng expert caching trên các điểm thiết kế MoE-offload.

Nhìn chung, chúng tôi kết luận rằng phụ thuộc tuần tự cố hữu giữa giai đoạn lựa chọn chuyên gia và giai đoạn thực thi chuyên gia đặt ra một số thách thức trong việc thiết kế một hệ thống MoE offload CPU hiệu quả về hiệu suất. Mục tiêu chính của bài báo này là khai thác tính chất động và thưa thớt của các mô hình MoE để thiết kế một giải pháp hệ thống toàn diện cân bằng hiệu quả giữa hiệu quả bộ nhớ và hiệu suất cao.

IV. PRE-GATED MOE: THIẾT KẾ ĐỒNG THỜI THUẬT TOÁN VÀ HỆ THỐNG CHO SUY LUẬN MOE NHANH & CÓ THỂ MỞ RỘNG

A. Tổng quan cấp cao
Chúng tôi đề xuất Pre-gated MoE, một thiết kế đồng thời thuật toán-hệ thống cho suy luận MoE có thể mở rộng và hiệu suất cao. Pre-gated MoE được thiết kế để giải quyết thách thức dung lượng bộ nhớ lớn của MoE đồng thời giảm thiểu tính chất động của việc kích hoạt chuyên gia thưa thớt để cải thiện hiệu suất. Những tính năng này cho phép Pre-gated MoE triển khai LLM quy mô lớn chỉ bằng một GPU duy nhất. Vì các chuyên gia được kích hoạt được xác định động theo cách phụ thuộc đầu vào, tất cả tham số chuyên gia phải được bảo tồn mọi lúc, bất kể việc sử dụng thực tế của nó. Để quản lý hiệu quả việc lưu trữ dung lượng mô hình đáng kể của MoE, Pre-gated MoE cân nhắc cẩn thận tiện ích thực tế của tham số mô hình để quyết định vị trí lưu trữ của chúng. Như thể hiện trong Hình 4, các tham số non-MoE dense được lưu trữ cục bộ trong bộ nhớ GPU vì chúng luôn được sử dụng, bất kể giá trị đầu vào. Trong khi đó, các tham số MoE thưa thớt được offload hoàn toàn vào DRAM của CPU vì (1) chúng chiếm phần lớn dung lượng mô hình của LLM nên CPU offloading có thể giúp

tiết kiệm đáng kể bộ nhớ GPU, và (2) chỉ một phần nhỏ các chuyên gia MoE được kích hoạt thực sự được sử dụng để suy luận. Như chúng tôi chi tiết trong mục này, việc lưu trữ phân cấp các tham số MoE như vậy và việc triển khai của nó tỏ ra hiệu quả trong việc giảm thiểu việc sử dụng bộ nhớ GPU trong khi vẫn cung cấp hiệu suất cao.

Như đã thảo luận trong Mục III-B, nghiên cứu trước đây đã theo hai phương pháp chính, MoE-OnDemand và MoE-Prefetch. Vì các khối MoE truyền thống phải thực thi tuần tự lựa chọn chuyên gia theo sau là thực thi chuyên gia theo cách phụ thuộc dữ liệu đầu vào, cả MoE-OnDemand và MoE-Prefetch đều gặp phải hiệu suất dưới mức tối ưu. Đặc biệt, MoE-OnDemand trực tiếp tiếp xúc với độ trễ giao tiếp CPU→GPU để di chuyển các chuyên gia được kích hoạt như một phần của thời gian suy luận đầu cuối. Điều này là do việc thực thi chuyên gia phải luôn được đi trước bởi giai đoạn lựa chọn chuyên gia. MoE-Prefetch có thể ẩn độ trễ giao tiếp để chuyển tham số chuyên gia ở một mức độ nào đó, nhưng nó vẫn gặp phải mất mát hiệu suất vì tất cả tham số chuyên gia phải được chuyển sang GPU, mặc dù chỉ một phần nhỏ trong số chúng thực sự được sử dụng để suy luận.

Mục tiêu chính của Pre-gated MoE là giảm thiểu tác động của việc kích hoạt chuyên gia thưa thớt được xác định động của khối MoE và sử dụng thuộc tính đó để cải thiện hiệu suất. Cụ thể, Pre-gated MoE giới thiệu một hàm gate mới tách biệt giai đoạn lựa chọn chuyên gia khỏi giai đoạn thực thi chuyên gia. Lợi ích của việc tách biệt lựa chọn chuyên gia với thực thi chuyên gia là hai mặt. Thứ nhất, nó cho phép hệ thống của chúng tôi giảm đáng kể độ trễ để di chuyển chuyên gia từ CPU sang GPU vì chỉ các chuyên gia được kích hoạt sẽ được di chuyển dưới thiết kế được đề xuất của chúng tôi. Thứ hai, overhead hiệu suất của việc di chuyển các chuyên gia được kích hoạt có thể được ẩn hiệu quả bằng cách chồng chéo nó với tính toán của khối MoE. Trong phần còn lại của mục này, chúng tôi chi tiết hai khía cạnh chính của thiết kế đồng thời thuật toán-hệ thống.

B. (Thuật toán) Kiến trúc Pre-gated MoE
Hàm Pre-gate. Trong kiến trúc mô hình MoE truyền thống, mỗi khối MoE chứa một hàm gate lựa chọn các chuyên gia để kích hoạt trong cùng khối MoE đó. Vì chỉ những chuyên gia được kích hoạt mới chịu giai đoạn thực thi chuyên gia tiếp theo, việc chồng chéo giai đoạn lựa chọn chuyên gia với giai đoạn thực thi chuyên gia là không thể. Trong kiến trúc

--- TRANG 6 ---
Chuyên gia 1
Chuyên gia 2
Chuyên gia 3Chuyên gia 0Pre-gate
Lớp Non-MoE
Chuyên gia 1
Chuyên gia 2
Chuyên gia 3Chuyên gia 0Pre-gate
Lớp Non-MoE
Hình 5: Hai khối MoE liên tiếp sử dụng hàm pre-gate được đề xuất của chúng tôi. Để ngắn gọn, chúng tôi chỉ cung cấp minh họa chi tiết về các khối MoE (phần còn lại của các lớp non-MoE được hợp nhất thành một khối duy nhất trong hình này). Các đường dẫn residual trong một khối transformer không được hiển thị. Như mô tả, một hàm pre-gate được huấn luyện để lựa chọn chuyên gia nào sẽ kích hoạt cho khối MoE tiếp theo.

mô hình MoE được đề xuất của chúng tôi, chúng tôi giới thiệu hàm pre-gate được huấn luyện để lựa chọn trước các chuyên gia sẽ kích hoạt cho khối MoE tiếp theo thay vì khối MoE hiện tại. Cụ thể hơn, hàm pre-gate cho khối MoE thứ N được huấn luyện để tạo ra các mask kích hoạt sử dụng trong khối MoE thứ (N+1) để lựa chọn chuyên gia nào sẽ kích hoạt (Hình 5). Nghiên cứu trước đây đã khám phá các cách thay thế để huấn luyện hàm gate, được fine-tune cho các hàm mục tiêu cụ thể như tăng cường độ chính xác mô hình và giảm bớt vấn đề mất cân bằng tải của token đầu vào khi phân phối trên nhiều GPU [8], [20], [36], [46], [47]. Cách tiếp cận được thực hiện với Pre-gated MoE của chúng tôi phù hợp với những nghiên cứu trước đây này nhưng với một sự khác biệt quan trọng – hàm pre-gate của chúng tôi được thiết kế để lựa chọn xác định và tính toán trước chuyên gia nào sẽ kích hoạt cho khối MoE tiếp theo. Như chúng tôi chứng minh trong Mục VI-C, hàm pre-gate của chúng tôi có tác động tối thiểu đến độ chính xác mô hình của LLM và được chứng minh là có tính bền vững cao.

Vì hàm pre-gate của chúng tôi được huấn luyện để lựa chọn mask hoạt động cho khối MoE tiếp theo, hai câu hỏi quan trọng vẫn còn: (1) Pre-gated MoE architecture của chúng tôi lựa chọn các chuyên gia để kích hoạt cho khối MoE đầu tiên như thế nào (tức là khối MoE đầu tiên không có khối MoE trước đó sẽ lựa chọn các chuyên gia để kích hoạt thay mặt cho khối đầu tiên)? (2) Vai trò của hàm pre-gate cho khối MoE cuối cùng là gì (tức là khối MoE cuối cùng không có khối MoE tiếp theo)? Chúng tôi trả lời những câu hỏi này bằng Hình 6. Trong các LLM dựa trên MoE thông thường, một lần lặp decoder duy nhất tạo ra một token đầu ra duy nhất (từ) và nhiều lần lặp giải mã được thực hiện trong một lần chạy suy luận duy nhất để tạo ra kết quả đầu ra cuối cùng (là một loạt token). Như thể hiện trong Hình 6, một lần lặp decoder duy nhất bao gồm việc thực thi một số ngăn xếp khối MoE. Trong thiết kế MoE được đề xuất của chúng tôi, khối MoE đầu tiên sử dụng hai hàm gate, gate đầu tiên lựa chọn các chuyên gia được kích hoạt cho khối MoE đầu tiên

Chuyên gia 1Chuyên gia 2Chuyên gia 3Chuyên gia 0Lớp Non-MoELần lặp decoder 0Gate đầu tiên
Pre-gate
Chuyên gia 1Chuyên gia 2Chuyên gia 3Chuyên gia 0Lớp Non-MoE
Pre-gate
Chuyên gia 1Chuyên gia 2Chuyên gia 3Chuyên gia 0Lớp Non-MoE
Gate cuối cùng
Chuyên gia 1Chuyên gia 2Chuyên gia 3Chuyên gia 0Lớp Non-MoELần lặp decoder 1Gate đầu tiên
Pre-gate
Chuyên gia 1Chuyên gia 2Chuyên gia 3Chuyên gia 0Lớp Non-MoE
Pre-gate
Chuyên gia 1Chuyên gia 2Chuyên gia 3Chuyên gia 0Lớp Non-MoE
Gate cuối cùngHình 6: Chuỗi thực thi của khối pre-gated MoE trong suốt hai lần lặp decoder liên tiếp. Chúng tôi giả định LLM bao gồm ba khối pre-gated MoE, yêu cầu ba lần thực thi khối MoE cho một lần lặp giải mã duy nhất. Khối MoE đầu tiên sử dụng hai hàm gate (một cho khối MoE hiện tại và một cho khối MoE tiếp theo) trong khi khối MoE cuối cùng không sử dụng bất kỳ hàm gate nào.

(giống hệt với kiến trúc MoE thông thường) và gate thứ hai (hàm pre-gate của chúng tôi) lựa chọn các chuyên gia để kích hoạt cho khối MoE thứ hai. Ngược lại, vì khối MoE cuối cùng không có khối MoE tiếp theo để thực thi trong cùng lần lặp decoder, chúng tôi không sử dụng hàm pre-gate cho khối MoE cuối cùng. Trên thực tế, hàm pre-gate không lựa chọn các chuyên gia được kích hoạt trên các lần lặp decoder khác nhau.

Huấn luyện hàm pre-gate. Các LLM ngày nay trước tiên được pretrain trên lượng lớn dữ liệu văn bản bao gồm nhiều ngôn ngữ và miền ứng dụng. Giai đoạn pretraining yêu cầu một lượng lớn sức mạnh tính toán (vài chục nghìn GPU) và thường mất vài tháng để hoàn thành (ví dụ: GPT-3 được pretrain trên hàng trăm tỷ token trong hơn một tháng bằng hàng nghìn GPU [3]). Một khi LLM được pretrain, nó trải qua giai đoạn fine-tuning với các tập dữ liệu cụ thể cho các trường hợp sử dụng cụ thể (ví dụ: tóm tắt, trả lời câu hỏi). Việc huấn luyện Pre-gated MoE của chúng tôi không thay đổi cách thức giai đoạn pretraining tốn kém tài nguyên được thực hiện, vì các hàm pre-gate của chúng tôi được huấn luyện gia tăng trong giai đoạn fine-tuning. Cụ thể, chúng tôi sử dụng các tham số mô hình MoE được pretrain hiện có nguyên trạng nhưng thay đổi kiến trúc mô hình MoE để chứa đúng các chức năng của hàm pre-gate cũng như các tăng cường cần thiết trong khối MoE đầu tiên/cuối cùng (xem Hình 6). Sau đó chúng tôi trải qua giai đoạn fine-tuning như yêu cầu bởi tác vụ downstream, giống hệt với cách các mô hình MoE thông thường sẽ được fine-tune theo cách cụ thể cho tác vụ. Khi Pre-gated MoE của chúng tôi được fine-tune trong cùng số lần lặp huấn luyện fine-tuning so với các mô hình MoE thông thường, chúng tôi quan sát thấy không có sự suy giảm đáng chú ý nào về độ chính xác mô hình LLM, điều mà chúng tôi sẽ trình bày chi tiết hơn trong Mục VI-C.

--- TRANG 7 ---
(b) (a)Chuyên gia 0
Chuyên gia 1
Chuyên gia 3Pre-gate
Pre-gate
Pre-gate
Khối 0 Khối 1Chuyên gia 2 Chuyên gia 2Chuyên gia 01
Chuyên gia 1
Chuyên gia 32
HBMGPU
DDRCPUChuyên gia 0
Chuyên gia 21
2
Chuyên gia 1
Chuyên gia 3Hình 8: (a) Minh họa cách ví dụ trong Hình 7 được xử lý trên hệ thống Pre-gated MoE, trong đó việc thực thi chuyên gia trong khối MoE 0 (sử dụng các chuyên gia được kích hoạt 0 và 2) diễn ra đồng thời trong khi các chuyên gia được kích hoạt 1 và 3 của khối MoE 1 tiếp theo được di chuyển sang bộ nhớ GPU. (b) Việc di chuyển theo yêu cầu chỉ các chuyên gia được kích hoạt (màu xanh lá) cho phép toàn bộ chuyên gia được lưu trữ trong bộ nhớ CPU (màu xanh), tiết kiệm đáng kể dung lượng bộ nhớ GPU.

C. (Hệ thống) Di chuyển chuyên gia có trước
Hàm pre-gate của chúng tôi cung cấp cho các mô hình MoE khả năng xác định chuyên gia nào sẽ được kích hoạt trong khối MoE tiếp theo trong khi khối MoE hiện tại đang được thực thi, mang lại cơ hội mới cho tối ưu hóa hiệu suất cấp hệ thống. Đặc biệt, ngoại trừ khối MoE đầu tiên, giai đoạn thực thi chuyên gia của tất cả khối MoE hiện được tách biệt hoàn toàn khỏi giai đoạn lựa chọn chuyên gia mà không có bất kỳ phụ thuộc dữ liệu nào, cho phép cả hai giai đoạn được thực thi đồng thời (Hình 7)¹. Tính năng như vậy mở ra một số cơ hội như chi tiết dưới đây.

CPU offloading với overhead di chuyển chuyên gia tối thiểu. Một hạn chế chính của các giải pháp CPU offloading trước đây là độ trễ để di chuyển các tham số chuyên gia được offload CPU hoặc được tiếp xúc trực tiếp như một phần của thời gian suy luận đầu cuối (MoE-OnDemand) hoặc kích thước của các chuyên gia được di chuyển đơn giản là quá lớn mà, mặc dù có cơ hội chồng chéo việc di chuyển chuyên gia với thực thi chuyên gia, việc sao chép các chuyên gia làm quá tải hiệu suất đầu cuối (MoE-Prefetch). Mặt khác, Pre-gated MoE của chúng tôi có thể đánh giá chuyên gia nào sẽ được kích hoạt trước, chỉ di chuyển các chuyên gia được kích hoạt cho khối MoE tiếp theo trong khi các chuyên gia của khối MoE hiện tại đang được thực thi. Điều này giải quyết hiệu quả những thách thức kép của các kỹ thuật CPU offloading trước đây, cụ thể là (a) tuần tự hóa lựa chọn chuyên gia và thực thi chuyên gia của MoE-OnDemand (được giải quyết bởi việc di chuyển chuyên gia đồng thời và thực thi chuyên gia của Pre-gated MoE) và (b) độ trễ di chuyển chuyên gia lớn của MoE-Prefetch (được giải quyết bởi khả năng của Pre-gated MoE chỉ di chuyển các chuyên gia được kích hoạt). Các mô hình MoE tiên tiến sử dụng một số lượng lớn chuyên gia trong một khối MoE trong khi chỉ kích hoạt một tập con rất nhỏ của chúng (ví dụ: Google's SwitchTransformer [8] chứa tới 256 chuyên gia nhưng chỉ kích hoạt chuyên gia top-1, trong khi Meta's

¹Khối MoE đầu tiên là ngoại lệ duy nhất đối với thuộc tính này dưới thiết kế Pre-gated MoE của chúng tôi – do thiếu hàm pre-gate trong khối MoE đầu tiên, chúng tôi phải thực thi tuần tự các giai đoạn lựa chọn chuyên gia và thực thi chuyên gia của nó, giống hệt với các mô hình MoE thông thường. Vì các LLM tiên tiến thường chứa hàng chục khối MoE, hầu hết các khối MoE có thể chồng chéo lựa chọn chuyên gia với thực thi chuyên gia.

Pre-gatePrefetchGiai đoạn lựa chọn
Chuyên gia 0
Chuyên gia 2
Giai đoạn thực thi Khối 1Các lớp Non-MoE
Đầu vào
Pre-gatePrefetchGiai đoạn lựa chọn
Chuyên gia 3Chuyên gia 1
Giai đoạn thực thi Khối 0Các lớp Non-MoE
Đầu raChuyên gia 0
Chuyên gia 2
Chuyên gia 3Chuyên gia 1Hình 7: Hàm pre-gate của chúng tôi giúp loại bỏ phụ thuộc tuần tự giữa giai đoạn lựa chọn chuyên gia và giai đoạn thực thi chuyên gia của một khối MoE. Hàm gate được triển khai như một lớp MLP nhỏ gọn có yêu cầu tính toán thấp, vì vậy việc di chuyển có trước các chuyên gia được kích hoạt ngay sau hàm gate (màu xanh) thể hiện hành vi bị ràng buộc bởi giao tiếp PCIe. Nhìn chung, Pre-gated MoE của chúng tôi cho phép giai đoạn thực thi chuyên gia bị ràng buộc tính toán (màu xanh lá) thực thi đồng thời với giai đoạn lựa chọn chuyên gia bị ràng buộc giao tiếp (màu xanh) cho tất cả các khối MoE (ngoại trừ khối MoE đầu tiên). Ví dụ giả định rằng chuyên gia 0 và 2 được kích hoạt cho khối MoE đầu tiên trong khi chuyên gia 1 và 3 được kích hoạt cho khối MoE thứ hai.

NLLB-MoE [41] sử dụng 128 chuyên gia và kích hoạt chuyên gia top-2). Như mô tả trong Hình 8, chúng ta có thể thấy rõ lợi ích của cách thiết kế đồng thời thuật toán-hệ thống có thể giải quyết hiệu quả các hạn chế của các giải pháp CPU-offloading hiện có, tối đa hóa cơ hội chồng chéo độ trễ di chuyển chuyên gia với thời gian thực thi chuyên gia đồng thời đảm bảo rằng kích thước truyền dữ liệu CPU→GPU được giảm thiểu. Hình 9 chỉ ra các hạn chế của MoE-OnDemand và MoE-Prefetch và cách Pre-gated MoE của chúng tôi giải quyết thành công các thiếu sót của nó, có khả năng đạt hiệu suất của một điểm thiết kế lý tưởng, chỉ sử dụng GPU khi độ trễ để di chuyển các chuyên gia được kích hoạt có thể được ẩn hoàn toàn bên trong giai đoạn thực thi chuyên gia MoE.

∀N,0≤N<Số lượng khối MoE
Bộ nhớ GPU đỉnh =max 
Non MoE M+N+1
∑
L=NAct Exp L!
(1)

Sử dụng bộ nhớ GPU thấp cho triển khai LLM lớn. Phần lớn dung lượng mô hình của LLM dựa trên MoE được tập trung xung quanh các tham số MoE. Vì hệ thống Pre-gated MoE của chúng tôi offload toàn bộ tham số MoE vào bộ nhớ CPU và chỉ di chuyển các chuyên gia được kích hoạt sang GPU, chúng tôi có thể giảm đáng kể việc sử dụng đỉnh bộ nhớ GPU. Trong hệ thống Pre-gated MoE của chúng tôi, việc sử dụng bộ nhớ GPU đỉnh chủ yếu được chi phối bởi dung lượng bộ nhớ cần thiết để lưu trữ (1) tất cả các tham số non-MoE (được lưu trữ tĩnh trong bộ nhớ GPU) và (2) các chuyên gia hoạt động cho cả khối MoE hiện tại và tiếp theo (được xác định động vào thời gian chạy và sao chép sang bộ nhớ GPU). Phương trình 1 tóm tắt việc sử dụng bộ nhớ GPU đỉnh để lưu trữ các tham số mô hình của LLM dựa trên MoE dưới Pre-gated MoE. Trong phương trình này, Non MoE M đại diện cho tổng kích thước của các tham số non-MoE trong khi ∑N+1
L=NAct Exp L đại diện cho

--- TRANG 8 ---
BẢNG I: Cấu hình mô hình của Google's SwitchTransformer.
Mô hình Chuyên gia Lớp Tham số (B) Dung lượng (GB)
Switch-Base8 12 0.7 2.8
64 12 3.8 15.2
128 12 7.5 30.0
Switch-Large 128 24 26.4 105.6

kích thước tổng hợp của các tham số chuyên gia hoạt động trên hai khối MoE liên tiếp (thứ N và thứ (N+1)). Vì các tham số chuyên gia chiếm phần lớn kích thước mô hình của LLM dựa trên MoE (xem Hình 3) và chỉ một phần nhỏ các chuyên gia được kích hoạt trong quá trình suy luận, việc sử dụng bộ nhớ GPU đỉnh trong Phương trình 1 trở nên thấp hơn nhiều so với chỉ sử dụng GPU và cũng có thể đạt mức tiêu thụ bộ nhớ của thiết kế MoE-OnDemand tối ưu bộ nhớ. Một lợi thế chính của việc giảm việc sử dụng bộ nhớ GPU đỉnh là nó tạo điều kiện cho việc triển khai các LLM lớn hơn đáng kể trên các hệ thống có tài nguyên bộ nhớ GPU hạn chế (ví dụ: thiết bị desktop và edge). Trong Mục VI-B, chúng tôi chứng minh khả năng mở rộng và áp dụng của Pre-gated MoE để triển khai các LLM quy mô lớn.

V. PHƯƠNG PHÁP LUẬN

Cấu hình hệ thống. Chúng tôi thực hiện đánh giá của mình bằng hai điểm thiết kế hệ thống, chỉ sử dụng GPU và CPU-GPU, sử dụng CPU AMD EPYC 7V12 64-Core với bộ nhớ DDR4 1.8TB và một GPU NVIDIA A100 duy nhất với 80GB HBM. CPU và GPU giao tiếp qua kênh PCIe (gen4) với băng thông truyền dữ liệu 32 GB/giây.

Thiết kế chỉ sử dụng GPU lý tưởng giả định toàn bộ tham số mô hình được lưu trữ trong bộ nhớ GPU, vì vậy tất cả các tính toán để suy luận được thực hiện trên GPU. Lưu ý rằng các giải pháp đa GPU tận dụng expert parallelism có thể gặp phải mất mát hiệu suất do giao tiếp liên GPU và các vấn đề mất cân bằng tải. Để đánh giá thận trọng, chúng tôi thử nghiệm với hệ thống chỉ sử dụng GPU dưới hệ thống một GPU duy nhất có thể đạt hiệu suất cao nhất. Do đó, chỉ sử dụng GPU đại diện cho một hệ thống suy luận MoE tối ưu hiệu suất, cận trên mà chúng tôi so sánh Pre-gated MoE với.

Mặt khác, thiết kế CPU-GPU sử dụng cả bộ nhớ GPU và CPU để lưu trữ các tham số mô hình, trong đó chỉ các tham số non-MoE (dense) được lưu trữ liên tục trong bộ nhớ GPU trong khi các tham số MoE (sparse) được offload hoàn toàn vào bộ nhớ CPU (Hình 4). Hệ thống Pre-gated MoE của chúng tôi cũng như hai hệ thống MoE CPU offloading cơ bản (MoE-OnDemand và MoE-Prefetch) sử dụng cấu hình hệ thống CPU-GPU như vậy.

Mô hình và tập dữ liệu. Chúng tôi sử dụng Google's SwitchTransformer [8] làm MoE cơ bản cho các đánh giá của mình, một mô hình MoE quy mô lớn tiên tiến. Các trọng số được pretrain mã nguồn mở có sẵn tại HuggingFace [43] được sử dụng để fine-tune cả Pre-gated MoE cũng như mô hình MoE cơ bản cho các tác vụ downstream (Bảng I). Về dữ liệu huấn luyện, chúng tôi nghiên cứu ba tập dữ liệu bao gồm hai tác vụ downstream riêng biệt: một từ tác vụ tóm tắt (Xsum [23]) và hai từ tác vụ trả lời câu hỏi sách đóng (CB Web QA [2], SQuAD [32]). Các chỉ số đánh giá bao gồm điểm Rouge-1 và

Khối 0Thời gian
Chỉ sử dụng GPU Tính toán Non-MoE Chuyên gia Non-MoE Chuyên gia
CPU-GPU
(MoE-OnDemand)FetchTính toán
Giao tiếp.Non-MoE Chuyên gia Non-MoE Chuyên gia
Fetch
CPU-GPU
(MoE-Prefetch)Prefetch-All Prefetch-AllTính toán
Giao tiếp.Non-MoE Chuyên gia Non-MoE Chuyên gia
CPU-GPU
(Pre-gated MoE)Tính toán
Giao tiếp. Prefetch-ActNon-MoE Chuyên gia Non-MoE Chuyên gia
Khối 1Tốc độ dự kiến
cải thiệnHình 9: Dòng thời gian thực thi giữa hệ thống Pre-gated MoE của chúng tôi và ba thiết kế cơ bản (chỉ sử dụng GPU, MoE-OnDemand, và MoE-Prefetch). Thanh màu đen đại diện cho độ trễ để thực thi các hàm gate. Chỉ sử dụng GPU là một điểm thiết kế lý tưởng, có dung lượng bộ nhớ GPU vô hạn, cho phép toàn bộ tham số mô hình được lưu trữ trong bộ nhớ GPU (tức là không có độ trễ giao tiếp để di chuyển chuyên gia từ CPU sang GPU). Trong Pre-gated MoE của chúng tôi, độ trễ để di chuyển chuyên gia có thể được ẩn bởi cả thời gian thực thi chuyên gia và lớp non-MoE (ví dụ: lớp self-attention).

Rouge-2 [22] cho tóm tắt, và điểm ExactMatch và F1 cho trả lời câu hỏi.

Huấn luyện mô hình (fine-tuning). Chúng tôi áp dụng các cấu hình fine-tuning giống hệt nhau trên tất cả kiến trúc mô hình bao gồm Pre-gated MoE và MoE thông thường. Như đã thảo luận trong Mục IV-B, giai đoạn fine-tuning sử dụng các trọng số được pretrain từ mô hình MoE thông thường. Chúng tôi sử dụng mini-batch chứa 256 chuỗi, mỗi chuỗi có độ dài 256 token, để fine-tune mô hình trong 2.048 bước (tức là 2^27 token tổng cộng). Tỷ lệ học không đổi 0.0001 được sử dụng.

Triển khai phần mềm. Tất cả các hệ thống chỉ sử dụng GPU và CPU-GPU của chúng tôi được triển khai bằng NVIDIA's FasterTransformer [25], một thư viện CUDA hiệu suất cao tiên tiến được sử dụng rộng rãi trong các máy chủ suy luận sản xuất trong ngành. Vì hiệu suất suy luận đầu cuối ít nhạy cảm hơn với tác vụ downstream mà mô hình MoE được huấn luyện, chúng tôi báo cáo số liệu hiệu suất bằng mô hình MoE được fine-tune cho tác vụ trả lời câu hỏi sách đóng với tập dữ liệu SQuAD. Khi báo cáo độ chính xác mô hình, chúng tôi sử dụng hai tác vụ downstream như đã thảo luận ở trên.

VI. ĐÁNH GIÁ

Trong mục này, trước tiên chúng tôi chứng minh hiệu quả của Pre-gated MoE trong việc cải thiện hiệu suất (Mục VI-A) và thảo luận về khả năng mở rộng của nó đối với các mô hình MoE quy mô lớn (Mục VI-B). Sau đó chúng tôi đánh giá định lượng tác động của Pre-gated MoE đối với độ chính xác mô hình (Mục VI-C) và cuối cùng trình bày các nghiên cứu độ nhạy như một điểm thảo luận (Mục VI-D), chứng minh tính bền vững của Pre-gated MoE.

A. Hiệu suất
Trong mục này, chúng tôi chủ yếu tập trung vào các tình huống suy luận batch đơn vì các hệ thống phục vụ ML sản xuất thực tế được tối ưu hóa cho kích thước batch là 1 [9], [10], [34]. Như đã thảo luận trong Mục IV-B, hiệu suất đầu cuối của các giải pháp CPU offloading chủ yếu được xác định bởi mức độ ẩn thời gian giao tiếp CPU→GPU (để di chuyển chuyên gia) bên trong thời gian thực thi khối MoE. Hơn nữa, thời gian suy luận MoE đầu cuối chủ yếu được chi phối bởi một loạt thời gian thực thi khối MoE (có kích thước giống hệt nhau). Do đó, trước tiên chúng tôi tập trung vào việc so sánh thời gian thực thi của một khối MoE duy nhất giữa Pre-gated MoE và các hệ thống cơ bản. Sau đó chúng tôi

--- TRANG 9 ---
0306090120150
8-chuyên gia 64-chuyên gia 128-chuyên gia 128-chuyên gia
Switch-Base Switch-LargeThroughput suy luận đầu cuối
(Token/s)Chỉ sử dụng GPU Pre-gated MoE MoE-OnDemand MoE-Prefetch
OOMHình 11: Throughput suy luận đầu cuối. Chỉ sử dụng GPU gặp lỗi hết bộ nhớ (OOM) trong Switch-Large.

thảo luận về các cải thiện trong throughput suy luận đầu cuối, được đo bằng số token được xử lý mỗi giây.

Độ trễ khối MoE. Hình 10 tóm tắt độ trễ trung bình trong việc thực thi một khối MoE duy nhất. Trên tất cả cấu hình, Pre-gated MoE giảm đáng kể độ trễ trung bình 1,7× (tối đa 1,9×) và 42× (tối đa 125×) so với MoE-OnDemand và MoE-Prefetch tương ứng. Pre-gated MoE cũng thể hiện độ trễ tương đương với chỉ sử dụng GPU tối ưu hiệu suất, chỉ phát sinh 19% overhead độ trễ trên tất cả cấu hình mô hình Switch-Base. Vì MoE-Prefetch phải di chuyển tất cả chuyên gia, nó gặp phải độ trễ cao nhất khi số lượng chuyên gia lớn hơn trực tiếp dẫn đến overhead hiệu suất cao hơn. MoE-OnDemand làm tốt hơn MoE-Prefetch, nhờ khả năng chỉ di chuyển các chuyên gia được kích hoạt. Tuy nhiên, MoE-OnDemand vẫn gặp phải độ trễ dài hơn Pre-gated MoE do sự tuần tự hóa của các giai đoạn lựa chọn chuyên gia và thực thi chuyên gia.

Đáng chú ý là chỉ sử dụng GPU tối ưu hiệu suất không thể chạy mô hình MoE lớn nhất, tức là Switch-Large với 128 chuyên gia (105.6 GB), do hạn chế về dung lượng bộ nhớ GPU, dẫn đến lỗi hết bộ nhớ (OOM). Pre-gated MoE vẫn cho thấy độ trễ ngắn nhất trong số ba thiết kế dựa trên CPU-GPU với Switch-Large, đạt giảm độ trễ 1,9× và 125× so với MoE-OnDemand và MoE-Prefetch tương ứng.

Throughput suy luận đầu cuối. Hình 11 hiển thị throughput suy luận đầu cuối trên tất cả cấu hình mô hình. Pre-gated MoE đạt throughput trung bình 111 token/giây trên tất cả cấu hình mô hình Switch-Base, cải thiện trung bình 1,5× (tối đa 1,6×) và 27× (tối đa 55×) so với MoE-OnDemand và MoE-Prefetch tương ứng. Hơn nữa, Pre-gated MoE có thể đạt 81% throughput của giải pháp chỉ sử dụng GPU lý tưởng, chứng minh tính hiệu quả về chi phí vượt trội. Đối với mô hình Switch-Large với 128 chuyên gia, Pre-gated MoE đạt throughput 42 token/giây, cao hơn 1,6× và 52× so với MoE-OnDemand và MoE-Prefetch tương ứng.

B. Khả năng mở rộng
Như đã thảo luận trong Mục II-B, phần lớn kích thước mô hình của MoE được chi phối bởi các tham số chuyên gia nhưng chỉ một phần nhỏ các chuyên gia thực sự được kích hoạt để thực thi. Do đó, việc phân bổ bộ nhớ GPU một cách thận trọng để sử dụng hiệu quả trở nên quan trọng trong việc giảm thiểu việc sử dụng bộ nhớ GPU đỉnh, giúp triển khai các LLM quy mô lớn. Hình 12 so sánh

1 1 1 1.2 1.2 1.2 1 2.0 1.9 1.9 1.9 7 54 107 125 
01101001,000
8-chuyên gia 64-chuyên gia 128-chuyên gia 128-chuyên gia
Switch-Base Switch-LargeĐộ trễ khối MoE
(được chuẩn hóa)Chỉ sử dụng GPU Pre-gated MoE MoE-OnDemand MoE-Prefetch
OOMHình 10: Độ trễ trung bình phát sinh khi thực thi một khối MoE duy nhất (được chuẩn hóa với chỉ sử dụng GPU). Vì chỉ sử dụng GPU gặp lỗi hết bộ nhớ (OOM) trong Switch-Large, chúng tôi chuẩn hóa độ trễ của MoE-OnDemand và MoE-Prefetch với Pre-gated MoE. Lưu ý rằng trục y trong biểu đồ này được vẽ theo thang logarit.

0.00.20.40.60.81.01.2
8-chuyên gia 64-chuyên gia 128-chuyên gia 256-chuyên gia 128-chuyên gia
Switch-Base LargeSử dụng bộ nhớ GPU đỉnh
(được chuẩn hóa)Chỉ sử dụng GPU Pre-gated MoE MoE-OnDemand MoE-Prefetch
OOMHình 12: Mức tiêu thụ bộ nhớ GPU đỉnh (được chuẩn hóa với chỉ sử dụng GPU). Chúng tôi đánh giá thêm Switch-Base với 256 chuyên gia để chứng minh thêm khả năng mở rộng của Pre-gated MoE trong việc triển khai các LLM dựa trên MoE lớn hơn. Đối với Switch-Large với 128 chuyên gia, chỉ sử dụng GPU gặp lỗi OOM, vì vậy chúng tôi chuẩn hóa việc sử dụng bộ nhớ của Pre-gated MoE và MoE-OnDemand với MoE-Prefetch.

việc sử dụng bộ nhớ GPU đỉnh của Pre-gated MoE so với các hệ thống cơ bản để chứng minh khả năng mở rộng của Pre-gated MoE.

Trong số bốn thiết kế, chỉ sử dụng GPU cho thấy việc sử dụng bộ nhớ đỉnh cao nhất vì nó chỉ dựa vào bộ nhớ GPU để phân bổ tất cả tham số mô hình và activation đầu vào/đầu ra. Cả ba hệ thống CPU-GPU đều có thể giảm đáng kể việc sử dụng bộ nhớ GPU đỉnh, vì các tham số chuyên gia đói bộ nhớ được offload vào bộ nhớ CPU. Ngoài ra, hãy chú ý cách khoảng cách sử dụng bộ nhớ GPU giữa chỉ sử dụng GPU và ba thiết kế CPU-GPU dựa trên CPU offloading dần dần tăng khi số lượng chuyên gia tăng. Điều này là do số lượng chuyên gia lớn hơn có sẵn trong một khối MoE, CPU offloading sẽ cung cấp nhiều tiết kiệm bộ nhớ GPU hơn. Tuy nhiên, MoE-Prefetch vẫn tiêu thụ trung bình 51% việc sử dụng bộ nhớ GPU đỉnh của chỉ sử dụng GPU vì nó luôn di chuyển toàn bộ tham số chuyên gia vào bộ nhớ GPU. MoE-OnDemand tối ưu bộ nhớ làm tốt hơn nhiều so với MoE-Prefetch vì nó chỉ di chuyển các chuyên gia được kích hoạt theo yêu cầu, cho thấy việc sử dụng bộ nhớ GPU đỉnh thấp nhất. Hệ thống Pre-gated MoE được đề xuất của chúng tôi có thể tiêu thụ chỉ 23% việc sử dụng bộ nhớ GPU đỉnh của chỉ sử dụng GPU trong khi chỉ phát sinh thêm 0,2% mức tiêu thụ bộ nhớ GPU so với MoE-OnDemand tối ưu bộ nhớ.

Nhìn chung, những kết quả này chứng minh rằng Pre-gated MoE có khả năng đạt hiệu suất được cung cấp với chỉ sử dụng GPU tối ưu hiệu suất (Hình 11) đồng thời cũng đạt được hiệu quả tài nguyên của MoE-OnDemand tối ưu bộ nhớ, đạt khả năng mở rộng cao để triển khai các LLM lớn.

C. Độ chính xác mô hình
Trong mục con này, chúng tôi định lượng tác động của hàm pre-gate đối với độ chính xác mô hình của MoE. Bảng II so sánh độ chính xác mô hình của SwitchTransformer có và không có hàm pre-gate của chúng tôi được sử dụng cho các tác vụ downstream khác nhau. Trong Switch-Base với 8 chuyên gia, là kích thước nhỏ nhất trong các cấu hình mô hình được nghiên cứu, Pre-gated MoE nhất quán thể hiện độ chính xác mô hình cao hơn một chút trên tất cả các tác vụ downstream. Khi kích thước mô hình tăng lên với số lượng chuyên gia lớn hơn, Pre-gated MoE phát sinh sự suy giảm độ chính xác nhỏ cho một số tác vụ downstream, nhưng nhìn chung nó tiếp tục mang lại kết quả độ chính xác mô hình cạnh tranh. Tuy nhiên, mức độ của

--- TRANG 10 ---
BẢNG II: Ảnh hưởng của hàm pre-gate đối với độ chính xác mô hình của Google's SwitchTransformer. R1 và R2 tương ứng đại diện cho điểm Rouge-1 và Rouge-2. Đối với tất cả chỉ số điểm, cao hơn là tốt hơn.
Xsum CB Web QA SQuAD
R1 R2 ExactMatch F1 ExactMatch F1
Base-8 34.6 13.0 26.0 30.9 77.4 85.8
Pre-gated 34.7 13.0 28.2 32.6 78.2 86.0
Base-128 38.1 16.6 27.4 33.1 81.7 89.2
Pre-gated 38.0 16.5 25.8 32.2 82.2 89.4
Large-128 40.2 18.8 31.0 36.5 82.4 90.1
Pre-gated 40.1 18.6 30.5 36.2 81.9 90.2

76777879
ExactMatchĐiểm đánh giá84858687
F1MoE thông thường
Pre-gated MoE (N=1)
Pre-gated MoE (N=2)
Pre-gated MoE (N=3)
Hình 13: Ảnh hưởng đến độ chính xác mô hình khi thay đổi mức kích hoạt của hàm pre-gate, từ một khối MoE phía trước (N=1, cấu hình mặc định của chúng tôi) đến khối MoE thứ 2/3 phía trước (N=2/3). Đánh giá được thực hiện trên Switch-Base với 8 chuyên gia cho tác vụ trả lời câu hỏi sách đóng được huấn luyện với tập dữ liệu SQuAD (điểm ExactMatch (trái) và F1 (phải) là các chỉ số đánh giá cho tác vụ đã cho).

các biến thể được quan sát về độ chính xác không biểu thị sự cải thiện hay suy giảm đáng kể về khả năng cơ bản của mô hình. Một phân tích chi tiết về lý do tại sao hàm pre-gate của chúng tôi cải thiện độ chính xác mô hình của một số tác vụ downstream nằm ngoài phạm vi của công trình này. Nói chung, độ chính xác mô hình bền vững của Pre-gated MoE được quan sát trên các kích thước mô hình khác nhau và các tác vụ downstream khác nhau nhấn mạnh tính bền vững thuật toán của đề xuất của chúng tôi.

Điều quan trọng cần nhấn mạnh là việc fine-tuning cho cả Pre-gated MoE và MoE thông thường được thực hiện bằng cách sử dụng cùng các tham số mô hình được pretrain với cùng số lần lặp huấn luyện. Thực tế là Pre-gated MoE tạo ra độ chính xác mô hình tương đương trong những điều kiện này chứng minh tính bền vững của đề xuất của chúng tôi. Hơn nữa, nó cũng cho thấy rằng đề xuất của chúng tôi có thể sử dụng hiệu quả các tài nguyên và công thức huấn luyện/fine-tuning có sẵn để triển khai (ví dụ: các tham số mô hình được pretrain từ các mô hình MoE thông thường), tăng cường khả năng ứng dụng của nó.

D. Thảo luận
Pre-gating để kích hoạt các chuyên gia ở các khối khác nhau. Cho đến nay chúng tôi đã giả định rằng hàm pre-gate của chúng tôi được huấn luyện để lựa chọn trước các chuyên gia sẽ kích hoạt cho khối MoE tiếp theo. Nói cách khác, mức kích hoạt của hàm pre-gate (N) là một (N=1) khối MoE phía trước của khối MoE hiện tại. Để khám phá các tối ưu hóa tiềm năng trong kiến trúc MoE bằng hàm pre-gate của chúng tôi, chúng tôi đánh giá độ chính xác mô hình của MoE khi hàm pre-gate được huấn luyện để lựa chọn các chuyên gia sẽ kích hoạt cho khối MoE tiếp theo thứ 2/3 phía trước (N=2/3), kết quả được hiển thị trong Hình 13.

Như mô tả, cấu hình Pre-gated MoE mặc định của chúng tôi (pre-gating với mức kích hoạt-1, tức là N=1) trong mô hình Switch-Base với 8 chuyên gia nhất quán cho thấy độ chính xác mô hình cao nhất so với phần còn lại của các điểm thiết kế bao gồm cấu trúc MoE thông thường (tức là lựa chọn chuyên gia để kích hoạt cho khối MoE hiện tại, N=0) cũng như các hàm pre-gate được huấn luyện để lựa chọn khối MoE tiếp theo thứ 2/3 phía trước (N=2/3). Lưu ý rằng độ chính xác mô hình dần giảm khi mức kích hoạt của hàm pre-gate tăng (từ N=1 đến 3). Chúng tôi đoán rằng khối MoE được lựa chọn trước càng xa khỏi hàm pre-gate hiện tại, thì hàm pre-gate hiện tại càng ít có khả năng activation đầu vào sẽ chứa thông tin hữu ích để lựa chọn chính xác chuyên gia nào phù hợp nhất để kích hoạt. Một đánh giá chi tiết về điều này nằm ngoài phạm vi của công trình này và chúng tôi để lại như công việc tương lai.

Số lượng chuyên gia được kích hoạt. Sức mạnh của MoE đến từ việc kích hoạt thưa thớt các chuyên gia (được thiết kế để bắt chước hành vi của bộ não con người, tức là các vùng chuyên biệt của não được điều chỉnh cho các tác vụ cụ thể), cho phép kiến trúc mô hình mở rộng dung lượng mô hình mà không tăng tỷ lệ thuận nhu cầu tính toán. Ví dụ, cấu hình mô hình mặc định của Google's SwitchTransformer chỉ kích hoạt một chuyên gia duy nhất (kích hoạt top-1) trong một suy luận batch đơn, vì vậy mô hình SwitchBase với 64 chuyên gia sẽ chỉ kích hoạt 1,56% chuyên gia của nó. Để hoàn thiện nghiên cứu của chúng tôi, chúng tôi hiển thị trong Hình 14 hiệu suất của Pre-gated MoE khi chúng tôi tăng thủ công số lượng chuyên gia được kích hoạt trong Switch-Base với 64 chuyên gia từ 1 chuyên gia (1,56% kích hoạt chuyên gia) đến 64 chuyên gia (100% kích hoạt chuyên gia).

Có hai quan sát chính có thể được rút ra từ thí nghiệm này. Thứ nhất, tất cả các giải pháp dựa trên CPU offloading (Pre-gated MoE, MoE-OnDemand, và MoE-Prefetch) gặp phải sự suy giảm hiệu suất cao hơn so với chỉ sử dụng GPU khi số lượng chuyên gia được kích hoạt tăng. Điều này được mong đợi vì hành vi của MoE trở nên tương tự như mô hình LLM dense khi số lượng chuyên gia lớn hơn được kích hoạt (tức là tất cả tham số mô hình được sử dụng với kích hoạt 100%), làm cho các giải pháp CPU offloading kém hiệu quả hơn. Thứ hai, khoảng cách hiệu suất giữa MoE-Prefetch và Pre-gated MoE dần giảm khi số lượng chuyên gia được kích hoạt tăng. Vì MoE-Prefetch di chuyển toàn bộ tham số chuyên gia cho mỗi khối MoE, số lượng chuyên gia được kích hoạt lớn hơn làm giảm các tham số chuyên gia được overfetch không cần thiết, thu hẹp

015304560
1-chuyên gia 4-chuyên gia 16-chuyên gia 32-chuyên gia 64-chuyên gia
(1.56%) (6.25%) (25%) (50%) (100%)
Số lượng chuyên gia hoạt động / Tổng số chuyên giaĐộ trễ khối MoE
(được chuẩn hóa)Chỉ sử dụng GPU Pre-gated MoE MoE-OnDemand MoE-PrefetchHình 14: Ảnh hưởng của số lượng chuyên gia được kích hoạt đối với độ trễ khối MoE (được chuẩn hóa với chỉ sử dụng GPU). Đánh giá được thực hiện bằng Switch-Base với 64 chuyên gia.

khoảng cách hiệu suất của nó so với Pre-gated MoE. Tuy nhiên, đối với các mô hình MoE với kích hoạt chuyên gia thưa thớt (cách phổ biến nhất để phát triển kiến trúc mô hình MoE), Pre-gated MoE chứng minh tính bền vững của nó và nhất quán cung cấp hiệu suất vượt trội so với các hệ thống CPU-GPU khác.

Caching chuyên gia trên Pre-gated MoE. Nghiên cứu trước đây của Huang et al. [14] đặc trưng hóa các mô hình MoE cho dịch máy và mô hình hóa ngôn ngữ, phát hiện ra sự tồn tại của một số chuyên gia hoạt động nóng trong quá trình suy luận. Dựa trên quan sát như vậy, [14] khám phá expert buffering cho suy luận MoE, cache các chuyên gia hoạt động nóng trong bộ nhớ GPU bằng chính sách thay thế cache vào cuối ra đầu (LIFO), trong khi đệm phần còn lại trong bộ nhớ CPU. Để đánh giá hiệu quả của expert caching [14], [38] trên Pre-gated MoE của chúng tôi cũng như các thiết kế MoE dựa trên CPU offloading khác, chúng tôi triển khai hệ thống caching trên cả Pre-gated MoE và MoE-OnDemand và đánh giá hiệu suất của nó.

Như thể hiện trong Hình 15, caching chuyên gia thường cung cấp lợi ích hiệu suất cho cả Pre-gated MoE và MoE-OnDemand, bất kể các loại chính sách thay thế cache được sử dụng. Tuy nhiên, hiệu quả của caching rõ ràng hơn với MoE-OnDemand vì overhead hiệu suất phát sinh với việc di chuyển chuyên gia nghiêm trọng hơn dưới điểm thiết kế này, khác với Pre-gated MoE đã có khả năng ẩn hầu hết độ trễ di chuyển chuyên gia bằng cách chồng chéo nó với thực thi chuyên gia.

Pre-gated MoE với SSD offloading. Nghiên cứu trước đây [38] đánh giá hiệu quả của việc offload các tham số MoE vào SSD như phương tiện để triển khai các LLM thậm chí lớn hơn. Để đánh giá hiệu quả của Pre-gated MoE trên điểm thiết kế như vậy, chúng tôi triển khai Pre-gated MoE và tất cả các hệ thống cơ bản trên hệ thống phục vụ MoE dựa trên SSD offloading, kết quả được tóm tắt trong Hình 16. Như mô tả, lợi ích hiệu suất của Pre-gated MoE so với các hệ thống cơ bản khác giảm so với hệ thống MoE offload "DRAM" CPU

00.511.52không cache1%10%20%không cache1%10%20%không cache1%10%20%không cache1%10%20%không cache1%10%20%không cache1%10%20%
có cache có cache có cache có cache có cache có cache
LIFO LFU LRU LIFO LFU LRU
Pre-gated MoE MoE-OnDemandThroughput đầu cuối
(được chuẩn hóa)Hình 15: Throughput đầu cuối của Pre-gated MoE và MoE-OnDemand khi được đánh giá với mô hình Switch-Large (128 chuyên gia) (được chuẩn hóa với Pre-gated MoE không có caching). Khi caching được bật, chúng tôi thay đổi phần trăm chuyên gia được cache bên trong bộ nhớ GPU (từ 1% đến 20%) và so sánh hiệu quả của nó. Để hoàn thiện nghiên cứu của chúng tôi, chúng tôi đánh giá không chỉ chính sách LIFO được đề xuất bởi [14] mà còn chính sách thay thế ít được sử dụng nhất (LFU) [38] và chính sách ít được sử dụng gần đây nhất (LRU).

11
0.01 0.01 00.20.40.60.81
Switch-LargeSwitch-XXLThroughput đầu cuối(được chuẩn hóa)Pre-gated MoEMoE-OnDemandMoE-PrefetchHình 16: Throughput suy luận đầu cuối của SSD offloading. Trong thí nghiệm này, chúng tôi đánh giá thêm một mô hình MoE lớn hơn có tên Switch-XXL, một kiến trúc mô hình dựa trên SwitchTransformer có cấu hình giống hệt Switch-Large nhưng tăng cả kích thước vector đặc trưng và số lượng head lên 4×, lên tới 395 tỷ tham số (16× nhiều hơn Switch-Large) và 217 GB kích thước mô hình sau khi áp dụng quantization. Chỉ sử dụng GPU gặp lỗi OOM, vì vậy hiệu suất được chuẩn hóa với Pre-gated MoE.

hệ thống MoE. Điều này là vì, khi các tham số MoE được offload vào SSD, độ trễ di chuyển chuyên gia giữa SSD→GPU trở nên dài hơn nhiều so với di chuyển từ CPU DRAM (do băng thông truyền dữ liệu thấp hơn nhiều giữa SSD so với CPU DRAM). Do đó, độ trễ di chuyển chuyên gia trở thành một thắt cổ chai hiệu suất đầu cuối lớn đến mức nó hoàn toàn áp đảo toàn bộ hệ thống, làm cho hiệu quả của bất kỳ phương pháp dựa trên CPU offloading nào trở nên nhỏ hơn. Tuy nhiên, Pre-gated MoE nhất quán mang lại hiệu suất cao hơn so với tất cả các hệ thống cơ bản khác, chứng minh tính bền vững của nó.

VII. CÁC CÔNG TRÌNH LIÊN QUAN
Tồn tại một số lượng lớn các công trình trước đây khám phá các hệ thống suy luận ML cho các LLM dựa trên MoE [4], [12], [13], [16], [21], [30], [38], [45]. Trong mục này, chúng tôi tóm tắt các công trình trước đây bằng cách phân loại chúng thành ba loại khác nhau: (1) hệ thống cho huấn luyện MoE, 2) hệ thống cho suy luận MoE, và 3) kiến trúc mô hình MoE cho triển khai MoE hiệu quả.

Hệ thống cho huấn luyện MoE. Nghiên cứu trước đây về FastMoE [12] và FasterMoE [13] đề xuất các tối ưu hóa cấp hệ thống cho các giải pháp đa GPU, cụ thể giải quyết vấn đề mất cân bằng tải trong huấn luyện MoE. Tutel [16] trình bày tối ưu hóa song song đa GPU động và pipelining cho các hệ thống huấn luyện MoE phân tán. SmartMoE [45] khám phá các chiến lược tìm kiếm hiệu quả để song song hóa huấn luyện MoE. TA-MoE [4] và Li et al. [21] đề xuất tối ưu hóa cho giao tiếp all-to-all và định tuyến chuyên gia của huấn luyện MoE. Khác với Pre-gated MoE tập trung vào suy luận, tất cả các công trình trước đây này tập trung vào huấn luyện MoE trên các hệ thống đa GPU, giả định tất cả tham số mô hình được phân vùng trên các GPU cho phép mỗi phân vùng mô hình được lưu trữ trong bộ nhớ GPU.

Hệ thống cho suy luận MoE. DeepSpeed-MoE [30] và Li et al. [21] đề xuất các tối ưu hóa giao tiếp hiệu quả cũng như tối ưu hóa kernel tính toán cho các hệ thống suy luận MoE dựa trên đa GPU. DeepSpeed-inference [1] đề xuất offload các tensor đói bộ nhớ (ví dụ: activation, tham số) vào bộ nhớ CPU và NVMe SSD theo ZeRO-offload [35] và ZeRO-infinity [31]. Tuy nhiên, DeepSpeed-inference

--- TRANG 12 ---
không đánh giá tính năng offload tham số của họ đối với các kiến trúc MoE sparse nhắm đến các tham số chuyên gia hạn chế dung lượng bộ nhớ. HuggingFace Accelerate [15] và SE-MoE [38] tương ứng triển khai các hệ thống MoE-OnDemand và MoE-Prefetch mà chúng tôi đánh giá trong bài báo này, một hệ thống suy luận MoE dựa trên CPU offloading.

Kiến trúc mô hình MoE hiệu quả. DeepSpeed-inference [1] đề xuất các kiến trúc PR-MoE và Mixture-of-Student (MoS), giúp nén đáng kể kích thước mô hình của MoE. Tuy nhiên, các mô hình này yêu cầu các sửa đổi đáng kể đối với kiến trúc mô hình dựa trên knowledge distillation và thường dẫn đến suy giảm độ chính xác mô hình. Hơn nữa, các mô hình này được thiết kế cho cấu hình chỉ sử dụng GPU, khác với Pre-gated MoE dựa trên CPU offloading. SE-MoE [38] cũng đề xuất một kiến trúc mô hình MoE nhỏ gọn dựa trên distillation, nén và pruning, nhưng nó gặp phải sự suy giảm không đáng kể về độ chính xác mô hình. Mặt khác, Pre-gated MoE của chúng tôi chỉ yêu cầu các thay đổi khiêm tốn đối với kiến trúc mô hình MoE mà không ảnh hưởng đến độ chính xác mô hình.

VIII. KẾT LUẬN
Bài báo này trình bày Pre-gated MoE, thiết kế đồng thời thuật toán-hệ thống của chúng tôi cho suy luận MoE có thể mở rộng và hiệu suất cao. Pre-gated MoE giải quyết hiệu quả hai thách thức chính của MoE (dung lượng bộ nhớ lớn và tính chất động của việc kích hoạt chuyên gia thưa thớt) thông qua hàm pre-gate mới của chúng tôi, giúp giảm bớt tính chất động của việc kích hoạt chuyên gia thưa thớt, cho phép hệ thống được đề xuất giải quyết dung lượng bộ nhớ lớn của MoE đồng thời đạt hiệu suất cao. So với các hệ thống suy luận MoE tiên tiến, Pre-gated MoE cải thiện throughput suy luận đồng thời giảm đáng kể mức tiêu thụ bộ nhớ GPU. Quan trọng là, Pre-gated MoE mang lại độ chính xác mô hình tương đương trên các tác vụ xử lý ngôn ngữ tự nhiên khác nhau, tạo điều kiện cho việc áp dụng trong một loạt các ứng dụng thực tế.

LỜI CẢM ỌN
Nghiên cứu này được hỗ trợ bởi MSIT (Bộ Khoa học, ICT), Hàn Quốc, dưới Chương trình Đào tạo Toàn cầu Cá nhân Tiềm năng Cao (RS-2022-00155958) được giám sát bởi IITP (Viện Lập kế hoạch & Đánh giá Công nghệ Thông tin & Truyền thông) và dự án này được hỗ trợ (một phần) bởi Microsoft Research Asia.

TÀI LIỆU THAM KHẢO
[1] R. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li, E. Zheng, O. Ruwase, S. Smith, M. Zhang, J. Rasley, và Y. He, "DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale," in Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis (SC), 2022.

[2] J. Berant, A. Chou, R. Frostig, và P. Liang, "Semantic Parsing on Freebase from Question-Answer Pairs," in Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013.

[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, và D. Amodei, "Language Models are Few-Shot Learners," in Proceedings of the International Conference on Neural Information Processing Systems (NIPS), 2020.

[4] C. Chen, M. Li, Z. Wu, D. Yu, và C. Yang, "TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training," in Proceedings of the International Conference on Neural Information Processing Systems (NIPS), 2022.

[5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, và N. Fiedel, "PaLM: Scaling Language Modeling with Pathways," in arxiv.org, 2022.

[6] J. Devlin, M. Chang, K. Lee, và K. Toutanova, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding," in arxiv.org, 2018.

[7] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, và C. Cui, "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts," in Proceedings of the International Conference on Machine Learning (ICML), 2022.

[8] W. Fedus, B. Zoph, và N. Shazeer, "Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity," The Journal of Machine Learning Research, vol. 23, p. 39, 2022.

[9] J. Fowers, K. Ovtcharov, M. Papamichael, T. Massengill, M. Liu, D. Lo, S. Alkalay, M. Haselman, L. Adams, M. Ghandi, S. Heil, P. Patel, A. Sapek, G. Weisz, L. Woods, S. Lanka, S. K. Reinhardt, A. M. Caulfield, E. S. Chung, và D. Burger, "A Configurable Cloud-Scale DNN Processor for Real-Time AI," in Proceedings of the International Symposium on Computer Architecture (ISCA), 2018.

[10] E. Frantar, S. Ashkboos, T. Hoefler, và D. Alistarh, "OPTQ: Accurate Quantization for Generative Pre-trained Transformers," in Proceedings of the International Conference on Learning Representations (ICLR), 2023.

[11] Google, "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context," https://storage.googleapis.com/deepmind-media/gemini/gemini_v15_report.pdf, 2024.

[12] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, và J. Tang, "FastMoE: A Fast Mixture-of-Expert Training System," in arxiv.org, 2021.

[13] J. He, J. Zhai, T. Antunes, H. Wang, F. Luo, S. Shi, và Q. Li, "FasterMoE: Modeling and Optimizing Training of Large-Scale Dynamic Pre-Trained Models," in Proceedings of the Symposium on Principles and Practice of Parallel Programming (PPOPP), 2022.

[14] H. Huang, N. Ardalani, A. Sun, L. Ke, H.-H. S. Lee, A. Sridhar, S. Bhosale, C.-J. Wu, và B. Lee, "Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference," in arxiv.org, 2023.

[15] HuggingFace, "HuggingFace Accelerate," https://huggingface.co/docs/accelerate/index, 2022.

[16] C. Hwang, W. Cui, Y. Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram, J. Chau, P. Cheng, F. Yang, M. Yang, và Y. Xiong, "Tutel: Adaptive Mixture-of-Experts at Scale," in arxiv.org, 2023.

[17] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, và D. Amodei, "Scaling Laws for Neural Language Models," in arxiv.org, 2020.

[18] Y. J. Kim, A. A. Awan, A. Muzio, A. F. C. Salinas, L. Lu, A. Hendy, S. Rajbhandari, Y. He, và H. H. Awadalla, "Scalable and Efficient MoE Training for Multitask Multilingual Models," in arxiv.org, 2021.

[19] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, và Z. Chen, "GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding," in Proceedings of the International Conference on Learning Representations (ICLR), 2021.

--- TRANG 13 ---
[20] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, và L. Zettlemoyer, "BASE Layers: Simplifying Training of Large, Sparse Models," in Proceedings of the International Conference on Machine Learning (ICML), 2021.

[21] J. Li, Y. Jiang, Y. Zhu, C. Wang, và H. Xu, "Accelerating Distributed MoE Training and Inference with Lina," in Proceedings of the USENIX Annual Technical Conference (ATC), 2023.

[22] C.-Y. Lin, "ROUGE: A Package for Automatic Evaluation of Summaries," in Text Summarization Branches Out, 2004.

[23] S. Narayan, S. B. Cohen, và M. Lapata, "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization," in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018.

[24] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, và C. Xiong, "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis," in Proceedings of the International Conference on Learning Representations (ICLR), 2023.

[25] NVIDIA, https://github.com/NVIDIA/FasterTransformer, 2019.

[26] OpenAI, "Introducing ChatGPT," https://openai.com/blog/chatgpt, 2022.

[27] D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, và J. Dean, "Carbon Emissions and Large Neural Network Training," in arxiv.org, 2021.

[28] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri, S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen, S. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato, A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. de Masson d'Autume, Y. Li, T. Terzi, V. Mikulik, I. Babuschkin, A. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart, S. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis, K. Kavukcuoglu, và G. Irving, "Scaling Language Models: Methods, Analysis & Insights from Training Gopher," in arxiv.org, 2022.

[29] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, và P. J. Liu, "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer," The Journal of Machine Learning Research, vol. 21, p. 67, 2020.

[30] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, và Y. He, "DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale," in Proceedings of the International Conference on Machine Learning (ICML), 2022.

[31] S. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, và Y. He, "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning," in Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis (SC), 2021.

[32] P. Rajpurkar, J. Zhang, K. Lopyrev, và P. Liang, "SQuAD: 100,000+ Questions for Machine Comprehension of Text," in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, 2016.

[33] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, và I. Sutskever, "Zero-Shot Text-to-Image Generation," in Proceedings of the International Conference on Machine Learning (ICML), 2021.

[34] V. J. Reddi, C. Cheng, D. Kanter, P. Mattson, G. Schmuelling, C.-J. Wu, B. Anderson, M. Breughe, M. Charlebois, W. Chou, R. Chukka, C. Coleman, S. Davis, P. Deng, G. Diamos, J. Duke, D. Fick, J. S. Gardner, I. Hubara, S. Idgunji, T. B. Jablin, J. Jiao, T. S. John, P. Kanwar, D. Lee, J. Liao, A. Lokhmotov, F. Massa, P. Meng, P. Micikevicius, C. Osborne, G. Pekhimenko, A. T. R. Rajan, D. Sequeira, A. Sirasao, F. Sun, H. Tang, M. Thomson, F. Wei, E. Wu, L. Xu, K. Yamada, B. Yu, G. Yuan, A. Zhong, P. Zhang, và Y. Zhou, "MLPerf Inference Benchmark," in arxiv.org, 2019.

[35] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, và Y. He, "ZeRO-Offload: Democratizing Billion-Scale Model Training," in Proceedings of the USENIX Annual Technical Conference (ATC), 2021.

[36] S. Roller, S. Sukhbaatar, A. Szlam, và J. E. Weston, "Hash Layers For Large Sparse Models," in Proceedings of the International Conference on Neural Information Processing Systems (NIPS), 2021.

[37] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, và J. Dean, "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer," in Proceedings of the International Conference on Learning Representations (ICLR), 2017.

[38] L. Shen, Z. Wu, W. Gong, H. Hao, Y. Bai, H. Wu, X. Wu, J. Bian, H. Xiong, D. Yu, và Y. Ma, "SE-MoE: A Scalable and Efficient Mixture-of-Experts Distributed Training and Inference System," in arxiv.org, 2023.

[39] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, và B. Catanzaro, "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism," in arxiv.org, 2020.

[40] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti, E. Zhang, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi, Y. He, M. Houston, S. Tiwary, và B. Catanzaro, "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model," in arxiv.org, 2022.

[41] N. Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, và J. Wang, "No Language Left Behind: Scaling Human-Centered Machine Translation," in arxiv.org, 2022.

[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, và I. Polosukhin, "Attention Is All You Need," in Proceedings of the International Conference on Neural Information Processing Systems (NIPS), 2017.

[43] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, và A. M. Rush, "HuggingFace's Transformers: State-of-the-art Natural Language Processing," in arxiv.org, 2020.

[44] C.-J. Wu, R. Raghavendra, U. Gupta, B. Acun, N. Ardalani, K. Maeng, G. Chang, F. A. Behram, J. Huang, C. Bai, M. Gschwind, A. Gupta, M. Ott, A. Melnikov, S. Candido, D. Brooks, G. Chauhan, B. Lee, H.-H. S. Lee, B. Akyildiz, M. Balandat, J. Spisak, R. Jain, M. Rabbat, và K. Hazelwood, "Sustainable AI: Environmental Implications, Challenges and Opportunities," in arxiv.org, 2022.

[45] M. Zhai, J. He, Z. Ma, Z. Zong, R. Zhang, và J. Zhai, "SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization," in Proceedings of the USENIX Annual Technical Conference (ATC), 2023.

[46] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Y. Zhao, A. M. Dai, Z. Chen, Q. V. Le, và J. Laudon, "Mixture-of-Experts with Expert Choice Routing," in Proceedings of the International Conference on Neural Information Processing Systems (NIPS), 2022.

[47] S. Zuo, X. Liu, J. Jiao, Y. J. Kim, H. Hassan, R. Zhang, T. Zhao, và J. Gao, "Taming Sparsely Activated Transformer with Stochastic Experts," in Proceedings of the International Conference on Learning Representations (ICLR), 2022.

--- TRANG 14 ---
PHỤ LỤC

A. Tóm tắt
Repository đánh giá artifact này chứa triển khai cơ bản của hệ thống Pre-gated MoE. Artifact được thiết kế để chứng minh hiệu suất của Pre-gated MoE của chúng tôi so với ba baseline được đề cập trong bài báo này. Như chi tiết trong Mục V, chúng tôi đã xây dựng Pre-gated MoE trên Google's SwitchTransformer [8], sử dụng NVIDIA's FasterTransformer [25].

B. Danh sách kiểm tra artifact (meta-information)
• Thuật toán: Thuật toán Pre-gated MoE
• Chương trình: C++, Python
• Mô hình: Google's SwitchTransformer
• Phần cứng: Ít nhất một GPU với 40GB bộ nhớ và CPU với 128GB bộ nhớ.
• Đầu ra: Kết quả chính của bài báo bao gồm độ trễ khối MoE trung bình, throughput suy luận, và mức tiêu thụ bộ nhớ GPU đỉnh.
• Cần bao nhiêu dung lượng đĩa (khoảng)?: Trên 100GB để lưu trữ các tham số mô hình.
• Cần bao nhiêu thời gian để chuẩn bị workflow (khoảng)?: 6 giờ
• Cần bao nhiêu thời gian để hoàn thành thí nghiệm (khoảng)?: 30 phút
• Có sẵn công khai?: Có
• Đã lưu trữ: https://doi.org/10.5281/zenodo.10976343

C. Mô tả
1) Cách truy cập: Artifact có sẵn trong repositories lưu trữ trên Zenodo và GitHub.
• Zenodo: https://doi.org/10.5281/zenodo.10976343
• GitHub: https://github.com/ranggihwang/Pregated_MoE

2) Phụ thuộc phần cứng: Để tái tạo các kết quả được trình bày trong bài báo, cần phần cứng sau:
• CPU với ít nhất 128GB bộ nhớ.
• GPU với ít nhất 40GB bộ nhớ. (Chúng tôi khuyến nghị sử dụng GPU gần đây, như NVIDIA A100 với 80GB HBM, để có hiệu suất tối ưu.)
• Ngoài ra, đảm bảo có hơn 100GB dung lượng lưu trữ đĩa cho các tham số mô hình.

3) Phụ thuộc phần mềm: Chúng tôi khuyên sử dụng Docker image theo mô tả trong repository để tránh hầu hết các vấn đề liên quan đến phần mềm. Repository bao gồm script tự động hóa việc cài đặt tất cả các phụ thuộc phần mềm cần thiết để biên dịch và chạy artifact. Chi tiết thêm được cung cấp trong tài liệu repository.

4) Tập dữ liệu: Để sử dụng artifact, cần tải xuống các trọng số mô hình cho SwitchTransformer từ HuggingFace. Repository của chúng tôi cung cấp hướng dẫn chi tiết và script để tải xuống các trọng số mô hình này.

D. Cài đặt
1) Tạo thư mục để chuẩn bị mô hình.
2) Khởi chạy container Docker bằng lệnh sau, thay thế ${DATA_PATH} bằng đường dẫn đến thư mục chuẩn bị mô hình của bạn:

docker run -ti --gpus all --shm-size 5g --name pregated -v ${DATA_PATH}:/data nvcr.io/nvidia/pytorch:22.09-py3 bash

3) Clone repository và bắt đầu quá trình build. Tham số -DSM nên khớp với compute capability của GPU. Tham khảo tài liệu để chọn giá trị phù hợp cho thiết lập của bạn.

# build trên A100
mkdir -p FasterTransformer/build
cd FasterTransformer/build
cmake -DSM=80 -DCMAKE_BUILD_TYPE=Release -DBUILD_PYT=ON -DBUILD_MULTI_GPU=ON ..
make -j

4) Cài đặt các phụ thuộc Python cần thiết:

pip install -r ../examples/pytorch/t5/requirement.txt

E. Workflow thí nghiệm
1) Chuẩn bị các mô hình.

mkdir /data/ft
cd /workspace/FasterTransformer/
./scripts/convert.sh

2) Bắt đầu đánh giá bằng script được cung cấp:

cd /workspace/FasterTransformer/
# logs sẽ được xuất ra đây
mkdir logs/
python scripts/eval_all.py

F. Đánh giá và kết quả mong đợi
Script sẽ tạo ra các tệp đầu ra ở định dạng CSV bao gồm block_lats.csv, throughputs.csv và peak_mems.csv, chứa dữ liệu về độ trễ khối MoE, throughput suy luận, và việc sử dụng bộ nhớ đỉnh tương ứng. Kết quả được hiển thị trong Hình 10, Hình 11, và Hình 12.

G. Tùy chỉnh thí nghiệm
Để tùy chỉnh thí nghiệm, bạn có thể sửa đổi cấu hình đánh giá trong scripts/eval_all.py

H. Ghi chú

I. Phương pháp luận
Phương pháp luận nộp bài, đánh giá và gắn nhãn:
• https://www.acm.org/publications/policies/artifact-review-and-badging-current
• http://cTuning.org/ae/submission-20201122.html
• http://cTuning.org/ae/reviewing-20201122.html
