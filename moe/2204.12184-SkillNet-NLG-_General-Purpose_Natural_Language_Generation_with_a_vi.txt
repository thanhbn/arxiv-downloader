# 2204.12184.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2204.12184.pdf
# Kích thước tệp: 2389861 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
SkillNet-NLG: Sinh ngôn ngữ tự nhiên đa mục đích với
phương pháp kích hoạt thưa thớt
Junwei Liao, Duyu Tang, Fan Zhang và Shuming Shi
Tencent AI Lab
Tóm tắt
Chúng tôi trình bày SkillNet-NLG, một phương pháp kích hoạt thưa thớt xử lý nhiều tác vụ sinh ngôn ngữ tự nhiên với một mô hình duy nhất. Khác với các mô hình dày đặc truyền thống luôn kích hoạt toàn bộ các tham số, SkillNet-NLG chọn lọc kích hoạt các phần tham số liên quan để hoàn thành một tác vụ, trong đó mức độ liên quan được điều khiển bởi một tập hợp các kỹ năng được định nghĩa trước. Ưu điểm của thiết kế mô hình này là nó cung cấp cơ hội để điều chỉnh chính xác các kỹ năng liên quan để học các tác vụ mới một cách hiệu quả. Chúng tôi đánh giá trên các tác vụ sinh ngôn ngữ tự nhiên tiếng Trung. Kết quả cho thấy rằng, chỉ với một tệp mô hình duy nhất, SkillNet-NLG vượt trội hơn các phương pháp hiệu suất tốt nhất trước đây trên bốn trong số năm tác vụ. SkillNet-NLG hoạt động tốt hơn hai đường chuẩn học đa tác vụ (một mô hình dày đặc và một mô hình Mixture-of-Expert) và đạt hiệu suất tương đương với các mô hình chuyên biệt cho từng tác vụ. Cuối cùng, SkillNet-NLG vượt trội hơn các hệ thống đường chuẩn khi được điều chỉnh cho các tác vụ mới.

1 Giới thiệu
Tính linh hoạt của Transformer (Vaswani et al., 2017) tạo điều kiện cho việc phát triển các mô hình đa tác vụ sử dụng một mô hình để xử lý nhiều tác vụ (Liu et al., 2019; Raffel et al., 2019; Lewis et al., 2020). Những mô hình này thường là "dày đặc" — tất cả các tham số mô hình được kích hoạt cho tất cả các tác vụ. Tuy nhiên, chưa rõ những kỹ năng nào được học trong những phần tham số nào. Mặc dù việc giải quyết các tác vụ khác nhau đòi hỏi các kỹ năng khác nhau (Dean, 2021; Tang et al., 2022), các mô hình dày đặc không cho phép chúng ta thực hiện các thao tác tinh tế để chọn các kỹ năng khác nhau cho các tác vụ khác nhau. Hơn nữa, khi điều chỉnh một mô hình dày đặc đã được huấn luyện tốt để học các tác vụ mới, tất cả các kỹ năng "mơ hồ" được mã hóa được chuyển giao một cách mù quáng, bất kể mức độ liên quan của chúng đến các tác vụ.

Trong công trình này, chúng tôi đề xuất một mô hình sinh ngôn ngữ tự nhiên đa mục đích gọi là SkillNet-
Liên hệ: Duyu Tang (duyutang@tencent.com).
L1L2
đầu vàoL12đầu ra
……Hình 1: Minh họa mô hình kích hoạt thưa thớt
SkillNet-NLG cho việc sinh đối thoại. Mỗi trụ đại diện cho một mô-đun kỹ năng và những trụ được tô màu (ví dụ: màu vàng, xanh lá, đỏ và tím) được kích hoạt.

NLG. Ý tưởng cơ bản là mô hình bao gồm nhiều mô-đun kỹ năng, mỗi mô-đun đại diện cho một kỹ năng cụ thể được định nghĩa trong Bảng 1. Thay vì kích hoạt tất cả các tham số trong các mô hình dày đặc truyền thống, chúng tôi chỉ kích hoạt các kỹ năng liên quan cho một tác vụ hạ lưu. Như được minh họa trong Hình 1, đối với tác vụ sinh đối thoại, SkillNet-NLG cần khả năng tạo ra ngôn ngữ mở (Sopenend), hiểu bối cảnh đối thoại (Sconversation) và hiểu các câu hỏi ngôn ngữ tự nhiên (Squestion). Do đó, các mô-đun kỹ năng liên quan đến Sopenend, Sconversation, Squestion và Sgeneral1 được kích hoạt. Các mô-đun còn lại (Snonopenend và Sdatatotext) không được kích hoạt. Chúng tôi phát triển SkillNet-NLG dựa trên Trans-
1Chúng tôi định nghĩa một kỹ năng chung Sgeneral, hoạt động như một kỹ năng mặc định và luôn được kích hoạt.
1arXiv:2204.12184v1 [cs.CL] 26 Apr 2022

--- TRANG 2 ---
Kỹ năng Định nghĩa
Sopenend sinh văn bản mở
Snonopenend sinh văn bản không mở
Sconversation hiểu bối cảnh đối thoại
Sdatatotext sinh văn bản từ dữ liệu có cấu trúc
Squestion hiểu câu hỏi ngôn ngữ tự nhiên
Sgeneral kỹ năng chung
Bảng 1: Các kỹ năng và định nghĩa của SkillNet-NLG.

former (Vaswani et al., 2017) với cấu trúc encoder-decoder. Chúng tôi chỉnh sửa mọi lớp khác trong cả Transformer encoder và decoder thông qua việc thay thế một lớp FFN bằng nhiều lớp FFN, mỗi lớp tương ứng với một kỹ năng.

Chúng tôi tiến hành các thí nghiệm mở rộng trên các tác vụ sinh ngôn ngữ tự nhiên tiếng Trung2. Chúng tôi xem xét năm tác vụ (tóm tắt văn bản, sinh quảng cáo, hỏi đáp, sinh đối thoại và sửa lỗi ngữ pháp) để huấn luyện đa tác vụ. Với một mô hình duy nhất, SkillNet-NLG vượt trội hơn các phương pháp hiệu suất tốt nhất trước đây trên bốn trong số năm tác vụ và hoạt động tốt hơn cả hai đường chuẩn đa tác vụ dày đặc và MoE. Chúng tôi tiếp tục điều chỉnh các mô hình đã được huấn luyện tốt cho ba tác vụ mới (sinh luận từ chủ đề, sinh paraphrase và sinh câu chuyện), và thấy rằng SkillNet-NLG hoạt động tốt hơn tất cả các hệ thống đường chuẩn nêu trên.

2 Phương pháp
Trong phần này, chúng tôi trước tiên giới thiệu ngắn gọn Transformer (§2.1), mà chúng tôi sử dụng làm xương sống của SkillNet-NLG. Sau đó, chúng tôi mô tả kiến trúc mô hình được đề xuất (§2.2). Cuối cùng, chúng tôi trình bày cách thực hiện huấn luyện đa tác vụ (§2.3) với SkillNet-NLG.

2.1 Transformer
Mô hình Transformer (Vaswani et al., 2017) có cấu trúc encoder-decoder với nhiều lớp. Trong encoder, mỗi lớp bao gồm một mạng tự chú ý đa đầu (Attention) và một lớp mạng feed forward (FFN). Cụ thể, cho đầu vào lớp hin, đầu ra lớp được tính như sau:
hout = FNN(Attention(hin)): (1)
Mỗi lớp của decoder tương tự như encoder ngoại trừ việc nó chèn thêm một lớp Attention bổ sung, thực hiện chú ý đa đầu trên đầu ra của ngăn xếp encoder. Vì Transformer là
2Phương pháp của chúng tôi không phụ thuộc vào ngôn ngữ. Chúng tôi để dành việc mở rộng SkillNet-NLG sang nhiều ngôn ngữ hơn trong tương lai.

một mô hình thường được sử dụng trong xử lý ngôn ngữ tự nhiên, chúng tôi loại bỏ mô tả chi tiết và giới thiệu độc giả đến bài báo gốc.

2.2 SkillNet-NLG
Chúng tôi phát triển SkillNet-NLG sử dụng Transformer (Vaswani et al., 2017) làm xương sống. Như được thể hiện trong Hình 1, ý tưởng cấp cao là SkillNet-NLG có nhiều mô-đun kỹ năng và chỉ kích hoạt các kỹ năng liên quan khi được áp dụng cho một tác vụ hạ lưu. Cụ thể, chúng tôi chỉnh sửa một lớp Transformer (cho cả encoder và decoder) thông qua việc thay thế một lớp FFN bằng nhiều lớp FFN, mỗi lớp tương ứng với một kỹ năng. Khi mô hình xử lý một tác vụ, chỉ các lớp FFN tương ứng với các kỹ năng liên quan được kích hoạt.

Ví dụ, đối với tác vụ sinh đối thoại, chúng tôi chỉ kích hoạt Sopenend, Sconversation, Squestion và Sgeneral. Các mô-đun còn lại (Snonopenend và Sdatatotext) không được kích hoạt. Đối với một lớp FFN cụ thể FFNk, nó hoạt động giống như lớp FFN gốc và tạo ra các biểu diễn đặc trưng cho kỹ năng như sau:
hk = FFNk(Attention(hin)): (2)
Vì kích thước của tập hợp các mô-đun được kích hoạt là biến thiên, chúng tôi tính toán các biểu diễn đầu ra sử dụng trung bình pooling như sau:
hout = 1/|S| Σ(k=1 to |S|) hk; (3)
trong đó S là tập hợp các kỹ năng được kích hoạt.

Đối với tác vụ sinh đối thoại, như được hiển thị trong Hình 1, S = {Sopenend, Sconversation, Squestion, Sgeneral}. Các hoạt động còn lại trong SkillNet-NLG giống như Transformer gốc. Theo Lepikhin et al. (2020), chúng tôi chỉ thực hiện những thay đổi trên trong mọi lớp Transformer khác để tránh thêm quá nhiều tham số.

2.3 Huấn luyện mô hình
Mô hình được huấn luyện trên hỗn hợp các mẫu huấn luyện từ tất cả các tác vụ. Trong mỗi lần lặp, một mini-batch được chọn từ một tác vụ. Một tiền tố đặc trưng cho tác vụ được thêm vào đầu vào. Mô hình tính toán mất mát cross-entropy giữa văn bản được sinh ra và văn bản tham chiếu để cập nhật các tham số mô hình. Vì dữ liệu huấn luyện của các tác vụ khác nhau không cân bằng, chúng tôi theo Tang et al. (2022) và áp dụng một

--- TRANG 3 ---
TácvụKỹ năng
SopenendSnonopenendSconversationSdatatotextSquestionSgeneral
Tác vụ để huấn luyện các mô hình đa tác vụ
Tóm tắt văn bản X X
Sinh quảng cáo X X X
Hỏi đáp X X X
Sinh đối thoại X X X X
Sửa lỗi ngữ pháp X X
Tác vụ mới để tinh chỉnh các mô hình đa tác vụ đã được huấn luyện tốt
Sinh luận từ chủ đề X X X
Sinh paraphrase X X
Sinh câu chuyện X X
Bảng 2: Mối quan hệ giữa các tác vụ và kỹ năng. Các kỹ năng liên quan cho mỗi tác vụ được đánh dấu bằng dấu tick.

chiến lược trộn có nhiệt độ điều chỉnh để lấy mẫu dữ liệu. Cụ thể, chúng tôi lấy mẫu các mini-batch từ N tác vụ theo xác suất {p1, ..., pN}:
pi = Di^(1/T) / Σ(j=1 to N) Dj^(1/T) với Di = min(ni, K); (4)
trong đó ni là số lượng ví dụ huấn luyện cho tác vụ thứ i. K là một siêu tham số. T là nhiệt độ lấy mẫu. Phân phối này tương đương với phân phối dữ liệu gốc khi T = 1 và gần với phân phối đều khi giá trị lớn hơn (ví dụ, T = 1024). Chúng tôi phân tích ảnh hưởng của T trong §3.4.

3 Thí nghiệm
Trong phần này, chúng tôi mô tả các thiết lập thí nghiệm và báo cáo kết quả.

3.1 Thiết lập thí nghiệm
Chúng tôi xem xét năm tác vụ để huấn luyện đa tác vụ. Chúng tôi so sánh với các đường chuẩn sau.
Tinh chỉnh chuyên biệt cho tác vụ: Chúng tôi tinh chỉnh tất cả các tham số của mô hình BART3 cho từng tác vụ riêng lẻ. Kết quả là, chúng tôi có tổng cộng năm mô hình chuyên biệt cho tác vụ cho năm tác vụ.
Tinh chỉnh chung (Dày đặc): Chúng tôi tinh chỉnh mô hình BART chung trên năm tác vụ.
Tinh chỉnh chung (MoE): Chúng tôi huấn luyện một đường chuẩn Mixture-of-Experts (MoE) (Lepikhin et al., 2020) với cùng số lượng sáu chuyên gia. Đối với mỗi token, chúng tôi sử dụng một hàm gating để kích hoạt chọn lọc top-2 chuyên gia. Các tham số của mô hình
3Chúng tôi tiền huấn luyện một mô hình BART tiếng Trung mạnh trên một bộ sưu tập 800G dữ liệu tin tức web.

được khởi tạo với mô hình BART của chúng tôi và được học chung trên năm tác vụ.
Bảng 2 trình bày các tác vụ này và các kỹ năng được kích hoạt cho mỗi tác vụ. Theo các công trình hiện có, chúng tôi báo cáo ROUGE-L cho các tập dữ liệu LCSTS và MATINF-QA, BLEU-4 cho các tập dữ liệu AdGen và KdConv, F0.5 cho tập dữ liệu NLPCC, tương ứng. Chúng tôi lấy trung bình các điểm số này làm tham chiếu cho hiệu suất tổng thể. Thống kê tập dữ liệu và chi tiết huấn luyện được trình bày trong Phụ lục B và C, tương ứng.

3.2 Kết quả tổng thể
Bảng 3 cho thấy kết quả của các đường chuẩn cũng như SkillNet-NLG trên năm tác vụ. Nhìn chung, SkillNet-NLG hoạt động tốt hơn tinh chỉnh chuyên biệt cho tác vụ và hai đường chuẩn học đa tác vụ (tức là Tinh chỉnh chung (Dày đặc) và Tinh chỉnh chung (MoE)) về điểm số trung bình. Chỉ với một mô hình duy nhất, SkillNet-NLG vượt trội hơn các phương pháp tốt nhất trước đây trên bốn trong số năm tác vụ, chứng minh hiệu quả của phương pháp kích hoạt thưa thớt.

3.3 Thích ứng với các tác vụ mới
Trong phần này, chúng tôi điều chỉnh các mô hình đã được huấn luyện tốt trên năm tác vụ cho các tác vụ mới riêng biệt.
Bảng 4 cho thấy kết quả của các mô hình khác nhau trên ba tác vụ mới. Theo các nghiên cứu hiện có, chúng tôi báo cáo BLEU-2 cho các tập dữ liệu ZhiHu và OutGen và báo cáo BLEU-4 cho tập dữ liệu PKUPB. Chúng ta có thể thấy rằng SkillNet-NLG vượt trội hơn tinh chỉnh chuyên biệt cho tác vụ và hai đường chuẩn đa tác vụ. SkillNet-NLG đạt hiệu suất tương đương với Qiao et al. (2020) trên ZhiHu, sử dụng cơ sở tri thức bên ngoài. SkillNet-NLG đạt được cải thiện 1,22 so với LongLM large, có số lượng tham số lớn hơn

--- TRANG 4 ---
LCSTS AdGen MATINF-QA KdConv NLPCC Trung bình
Hệ thống tốt nhất trước đây 41.87y 10.63y 20.51y 18.50z 36.97⬤ 25.70
Tinh chỉnh chuyên biệt cho tác vụ 42.05 10.38 21.06 21.11 36.42 26.20
Tinh chỉnh chung (Dày đặc) 41.77 10.25 20.32 21.16 36.19 25.94
Tinh chỉnh chung (MoE) 41.80 10.25 20.56 20.71 35.96 25.86
SkillNet-NLG 42.40 10.80 20.73 20.76 36.68 26.27
Bảng 3: Kết quả kiểm tra trên năm tập dữ liệu tác vụ trong quá trình huấn luyện đa tác vụ. Trung bình là điểm số trung bình của tất cả các tác vụ.
y chỉ ra điểm số từ CPT-Large (Shao et al., 2021). z chỉ ra điểm số từ mBART-Large (Liu et al., 2020). ⬤ chỉ ra điểm số từ Mask GEC (Zhao and Wang, 2020).

ZhiHu PKUPB OutGen
Hệ thống tốt nhất trước đây 11.02y – 24.77z
Tinh chỉnh chuyên biệt cho tác vụ 10.56 31.88 25.23
Tinh chỉnh chung (Dày đặc) 10.53 31.93 24.47
Tinh chỉnh chung (MoE) 10.83 31.51 24.23
SkillNet-NLG 10.98 32.02 25.99
Bảng 4: Kết quả kiểm tra trên ba tập dữ liệu tác vụ mới. Kết quả với y từ SCTKG(Gold-Senti) (Qiao et al., 2020). Kết quả với z từ LongLM large (Guan et al., 2021).

(tức là một tỷ) tham số và được tiền huấn luyện trên dữ liệu trong miền quy mô lớn.

3.4 Ảnh hưởng của các chiến lược lấy mẫu dữ liệu
Như được mô tả trong Phần 2.3, chúng tôi lấy mẫu các ví dụ huấn luyện từ mỗi tác vụ bằng cách thay đổi nhiệt độ lấy mẫu T. Hình 2 cho thấy các điểm số với các giá trị T khác nhau trên các tập phát triển của năm tác vụ. Khi T = 1, các ví dụ huấn luyện được lấy mẫu theo tỷ lệ với kích thước dữ liệu huấn luyện của mỗi tác vụ. Vì các tập dữ liệu này rất không cân bằng (như được đưa ra trong Bảng 5), tác vụ tài nguyên cao LCSTS đạt điểm số cao nhất trong khi tác vụ tài nguyên thấp KdConv đạt điểm số thấp nhất. Khi T tăng, sự mất cân bằng dữ liệu giữa các tác vụ tài nguyên cao và tài nguyên thấp giảm dần. Khi T = 4, mô hình đạt được sự cân bằng giữa hai cực trị và đạt điểm số trung bình tốt nhất trên các tập phát triển. Do đó, chúng tôi áp dụng T = 4 trong suốt tất cả các thí nghiệm.

4 Kết luận
Trong công trình này, chúng tôi trình bày một mô hình đa mục đích gọi là SkillNet-NLG. Nó xử lý nhiều tác vụ sinh ngôn ngữ tự nhiên với một mô hình duy nhất. Đặc điểm chính của phương pháp chúng tôi là nó được kích hoạt thưa thớt được hướng dẫn bởi một tập hợp các kỹ năng được định nghĩa trước. Chỉ các tham số của các kỹ năng liên quan được kích hoạt. Ưu điểm của thiết kế mô hình như vậy là nó cho phép chúng tôi chỉ chuyển giao các kỹ năng liên quan để học các tác vụ mới. Kết quả thí nghiệm trên các tác vụ NLG tiếng Trung xác minh hiệu quả của phương pháp chúng tôi. Trong tương lai, chúng tôi dự định điều chỉnh mô hình cho nhiều ngôn ngữ hơn và thậm chí nhiều phương thức hơn.

Tài liệu tham khảo
Jeff Dean. 2021. Introducing pathways: A next-generation ai architecture. In Google Blog.
Xiaocheng Feng, Ming Liu, Jiahao Liu, Bing Qin, Yibo

--- TRANG 5 ---
Sun, and Ting Liu. 2018. Topic-to-essay generation with neural networks. In IJCAI, pages 4078–4084.
Jian Guan, Zhuoer Feng, Yamei Chen, Ruilin He, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2021. Lot: A benchmark for evaluating chinese long text understanding and generation.
Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LCSTS: A large scale Chinese short text summarization dataset. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1967–1972, Lisbon, Portugal. Association for Computational Linguistics.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871–7880.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. 2019. Multi-task deep neural networks for natural language understanding. arXiv preprint arXiv:1901.11504.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.
Lin Qiao, Jianhao Yan, Fandong Meng, Zhendong Yang, and Jie Zhou. 2020. A sentiment-controllable topic-to-essay generator with topic knowledge graph. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3336–3344.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.
Yunfan Shao, Zhichao Geng, Yitao Liu, Junqi Dai, Fei Yang, Li Zhe, Hujun Bao, and Xipeng Qiu. 2021. Cpt: A pre-trained unbalanced transformer for both chinese language understanding and generation. arXiv preprint arXiv:2109.05729.
Yu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding, Chao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen, Yanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu, Weibao Gong, Jianzhong Liang, Zhizhou Shang, Peng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao Tian, Hua Wu, and Haifeng Wang. 2021. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint arXiv:2107.02137.
Duyu Tang, Fan Zhang, Yong Dai, Cong Zhou, Shuangzhi Wu, and Shuming Shi. 2022. Skillnet: A sparsely activated model for general-purpose natural language understanding. arXiv preprint arXiv:2203.03312.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.
Canwen Xu, Jiaxin Pei, Hongtao Wu, Yiyu Liu, and Chenliang Li. 2020. MATINF: A jointly labeled large-scale dataset for classification, question answering and summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3586–3596, Online. Association for Computational Linguistics.
Pengcheng Yang, Lei Li, Fuli Luo, Tianyu Liu, and Xu Sun. 2019. Enhancing topic-to-essay generation with external commonsense knowledge. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2002–2012.
Yuanyuan Zhao, Nan Jiang, Weiwei Sun, and Xiaojun Wan. 2018. Overview of the nlpcc 2018 shared task: Grammatical error correction. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 439–445. Springer.
Zewei Zhao and Houfeng Wang. 2020. Maskgec: Improving neural grammatical error correction via dynamic masking. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 1226–1233.
Hao Zhou, Chujie Zheng, Kaili Huang, Minlie Huang, and Xiaoyan Zhu. 2020. KdConv: A Chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7098–7108, Online. Association for Computational Linguistics.

--- TRANG 6 ---
A Chi phí tính toán
Bảng 6 cho thấy số lượng tham số của các mô hình. Số lượng tham số được kích hoạt của SkillNet-NLG phụ thuộc vào số lượng mô-đun kỹ năng được kích hoạt khi thực hiện một tác vụ cụ thể (Bảng 6).

B Các tập dữ liệu
Bảng 5 cho thấy thống kê của tất cả các tập dữ liệu tiếng Trung được sử dụng trong thí nghiệm. Chúng tôi đầu tiên sử dụng năm tập dữ liệu tác vụ để huấn luyện các mô hình đa tác vụ và đánh giá hiệu suất. Sau đó chúng tôi sử dụng ba tập dữ liệu tác vụ khác để tinh chỉnh các mô hình tương ứng.

Tóm tắt văn bản được thiết kế để tạo điều kiện nắm bắt nhanh chóng bản chất của một tài liệu đầu vào bằng cách tạo ra một bản tóm tắt cô đọng nội dung của nó. LCSTS là một tập dữ liệu tóm tắt văn bản ngắn tiếng Trung quy mô lớn (Hu et al., 2015) được thu thập từ Sina Weibo. Chúng tôi sử dụng cùng phân chia dữ liệu và thước đo đánh giá như Shao et al. (2021).

Sinh quảng cáo nhằm mục đích tạo ra một quảng cáo dài cho một tập hợp các cặp giá trị thuộc tính của một mặt hàng. AdGen bao gồm 119K cặp bảng thông số quần áo và văn bản quảng cáo của chúng từ một nền tảng thương mại điện tử Trung Quốc. Theo Shao et al. (2021), chúng tôi sử dụng cùng tiền xử lý dữ liệu và định dạng dữ liệu đầu vào như một danh sách các cặp giá trị thuộc tính.

Hỏi đáp là tạo ra một câu trả lời bằng ngôn ngữ tự nhiên cho một câu hỏi được đưa ra. MATINF-QA là một tập dữ liệu QA miền mở tiếng Trung quy mô lớn được thu thập bởi Xu et al. (2020), chứa 1,07 triệu cặp câu hỏi-câu trả lời từ lĩnh vực sức khỏe. thai sản và chăm sóc trẻ em

Sinh đối thoại là tạo ra một phản hồi dựa trên các phát ngôn lịch sử trong một cuộc đối thoại. KdConv là một tập dữ liệu đối thoại được điều khiển bởi kiến thức đa miền chứa 4,5K cuộc trò chuyện từ ba miền (Zhou et al., 2020). Chúng tôi theo Sun et al. (2021) để phân chia dữ liệu và tiền xử lý, và loại trừ các triplet tri thức khỏi đầu vào.

Sửa lỗi ngữ pháp (GEC) là tác vụ sửa các loại lỗi khác nhau trong văn bản như lỗi chính tả, dấu câu, ngữ pháp và lựa chọn từ. NLPCC được cung cấp bởi NLPCC 2018 Shared Task4 (Zhao et al., 2018) chứa các văn bản tiếng Trung quy mô lớn được viết bởi những người không phải người bản ngữ trong đó các lỗi ngữ pháp đã được chú thích và sửa chữa bởi người bản ngữ. Chúng tôi sử dụng bộ tính điểm MaxMatch (M2) chính thức để đánh giá
4http://tcci.ccf.org.cn/conference/2018/taskdata.php

các mô hình5.

Sinh luận từ chủ đề lấy một tập hợp các từ chủ đề làm đầu vào và xuất ra một bài luận (một đoạn văn) dưới chủ đề của các chủ đề. ZhiHu là một tập dữ liệu sinh luận từ chủ đề (Feng et al., 2018) được thu thập từ ZhiHu, một trang web hỏi đáp tiếng Trung. Nó bao gồm 100 từ chủ đề tần suất cao và các bài luận tiếng Trung có độ dài từ 50 đến 100. Chúng tôi sử dụng cùng phân chia dữ liệu và thước đo đánh giá như Yang et al. (2019)6.

Sinh paraphrase là tác vụ tạo ra một câu đầu ra bảo toàn ý nghĩa của câu đầu vào nhưng chứa các biến thể trong lựa chọn từ và ngữ pháp. PKU Paraphrase Bank (PKUPB) là một kho ngữ liệu paraphrase cấp câu quy mô lớn cho tiếng Trung chứa 509.832 cặp câu7. Chúng tôi lấy mẫu ngẫu nhiên 10.000 cặp làm tập xác thực và kiểm tra tương ứng và sử dụng phần còn lại làm tập huấn luyện.

Sinh câu chuyện nhằm mục đích tạo ra một câu chuyện hợp lý từ một bối cảnh dẫn đầu. Câu chuyện phải duy trì sự nhất quán chủ đề trong suốt toàn bộ tài liệu cũng như giữ tính sáng tạo. OutGen là một tập dữ liệu sinh câu chuyện có điều kiện dàn ý được giới thiệu bởi Guan et al. (2021), yêu cầu tạo ra một câu chuyện dài mạch lạc có điều kiện trên một dàn ý của các nhân vật và sự kiện. Dàn ý là một tập hợp các cụm từ không theo thứ tự. Chúng tôi sử dụng cùng phân chia dữ liệu và thước đo đánh giá được cung cấp bởi Guan et al. (2021)8.

C Huấn luyện mô hình
C.1 Huấn luyện đa tác vụ
Chúng tôi xây dựng mô hình SkillNet bằng cách sử dụng triển khai BART-large của HuggingFace's Transformers9 (Wolf et al., 2020), có 12 lớp encoder, 12 lớp decoder, 1024 chiều trạng thái ẩn và 4096 chiều FFN. Tất cả các mô-đun kỹ năng được khởi tạo với các lớp FFN từ mô hình BART tiếng Trung tiền huấn luyện của chúng tôi. Chúng tôi tiến hành huấn luyện đa tác vụ trong 100k bước với độ dài nguồn tối đa là 512, độ dài đích tối đa là 200 và kích thước batch là 512. Chúng tôi sử dụng Adam (Kingma and Ba, 2014) làm bộ tối ưu hóa với β1 = 0,9; β2 = 0,999; ε = 1e-8. Tốc độ học được làm ấm
5http://www.comp.nus.edu.sg/nlp/software.html
6Tập dữ liệu có thể tải xuống bằng https://pan.baidu.com/s/17pcfWUuQTbcbniT0tBdwFQ
7https://github.com/pkucoli/PKU-Paraphrase-Bank
8Dữ liệu và script đánh giá có sẵn tại https://github.com/thu-coai/LOT-LongLM.
9https://github.com/huggingface/transformers

--- TRANG 7 ---
Tác vụ Tập dữ liệu #Huấn luyện #Phát triển #Kiểm tra Thước đo
Tác vụ để huấn luyện các mô hình đa tác vụ
Tóm tắt văn bản LCSTS 2160k 30k 725 ROUGE-L
Sinh quảng cáo AdGen 114k 1k 3k BLEU-4
Hỏi đáp MATINF-QA 740k 100k 210k ROUGE-L
Sinh đối thoại KdConv 63k 9k 9k BLEU-4
Sửa lỗi ngữ pháp NLPCC 1200k 5k 2k F0.5
Tác vụ mới để tinh chỉnh các mô hình đa tác vụ đã được huấn luyện tốt
Sinh luận từ chủ đề ZhiHu 27k 300 2,3k BLEU-2
Sinh paraphrase PKUPB 490k 10k 10k BLEU-4
Sinh câu chuyện OutGen 1456 242 729 BLEU-2
Bảng 5: Thống kê các tập dữ liệu.

#Tổng tham số Tác vụ #Kỹ năng #Tham số được kích hoạt
Tinh chỉnh chuyên biệt cho tác vụ 376,46M — — 376,46M
Tinh chỉnh chung (Dày đặc) 376,46M — — 376,46M
Tinh chỉnh chung (MoE) 880,28M — — 477,28M
SkillNet-NLG 880,20M Tóm tắt văn bản 2 477,20M
Sinh quảng cáo 3 577,95M
Hỏi đáp 3 577,95M
Sinh đối thoại 4 678,70M
Sửa lỗi ngữ pháp 2 477,20M
Sinh luận từ chủ đề 3 577,95M
Sinh paraphrase 2 477,20M
Sinh câu chuyện 2 477,20M
Bảng 6: Số lượng tham số của các mô hình.

trong 10k bước đầu tiên đến giá trị đỉnh là 3e-5, và sau đó giảm dần tuyến tính. Chúng tôi hiển thị đường cong học tập của mỗi tác vụ trong Phụ lục D. Chúng tôi đặt giới hạn kích thước K = 221 và nhiệt độ lấy mẫu T = 4 sau khi tìm kiếm trong {1, 2, 4, 8, 16, 1024}. Trong giai đoạn suy luận, chúng tôi sử dụng giải mã tìm kiếm chùm tia và đặt kích thước chùm tia là 4 cho tất cả các tác vụ.

C.2 Huấn luyện tác vụ mới
Bảng 7 cho thấy các siêu tham số cụ thể được sử dụng để huấn luyện ba tác vụ mới. Các tham số huấn luyện khác giống như đối với huấn luyện đa tác vụ.

D Đường cong học tập
Chúng tôi hiển thị các đường cong học tập trong quá trình huấn luyện đa tác vụ trong Hình 3.

ZhiHu PKUPB OutGen
Epochs 16 6 16
Kích thước batch 128 64 64
Tốc độ học 3e-5 3e-5 5e-5
Độ dài nguồn tối đa 30 140 100
Độ dài đích tối đa 170 140 310
Thước đo cho mô hình tốt nhất BLEU-2 BLEU-4 BLEU-2
Bảng 7: Tham số huấn luyện để tinh chỉnh SkillNet-NLG đã được huấn luyện tốt trên các tác vụ mới.

--- TRANG 8 ---
(a) Mất mát huấn luyện trên tất cả các tập dữ liệu tác vụ.
(b) Mất mát huấn luyện trên LCSTS.
(c) Mất mát huấn luyện trên AdGen.
(d) Mất mát huấn luyện trên MATINF-QA.
(e) Mất mát huấn luyện trên KdConv.
(f) Mất mát huấn luyện trên NLPCC.
Hình 3: Các đường cong học tập của các tác vụ trong quá trình huấn luyện đa tác vụ.
