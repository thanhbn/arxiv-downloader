Tài liệu tham khảo

[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, và Illia Polosukhin. Attention is all you need. Trong Proc. Advances in Neural Information Processing Systems (NIPS), trang 5998–6008, Long Beach, CA, USA, tháng 12 năm 2017.

[2] Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to recurrent nets. Neural Computation, 4(1):131–139, 1992.

[3] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language models are unsupervised multitask learners. 2019.

[4] Tom B Brown et al. Language models are few-shot learners. Trong Proc. Advances in Neural Information Processing Systems (NeurIPS), chỉ ảo, tháng 12 năm 2020.

[5] OpenAI. ChatGPT. https://openai.com/blog/chatgpt, 2022.

[6] OpenAI. GPT-4 technical report. Preprint arXiv:2303.08774, 2023.

[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. LLaMA: Open and efficient foundation language models. Preprint arXiv:2302.13971, 2023.

[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, và Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Trong Proc. Advances in Neural Information Processing Systems (NeurIPS), chỉ ảo, tháng 5 năm 2021.

[9] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, và Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Trong Proc. Advances in Neural Information Processing Systems (NeurIPS), trang 15084–15097, chỉ ảo, tháng 12 năm 2021.

[10] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, và Lukasz Kaiser. Universal Transformers. Trong Int. Conf. on Learning Representations (ICLR), New Orleans, LA, USA, tháng 5 năm 2019.

[11] Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, 1990.

[12] Michael I Jordan. Attractor dynamics and parallelism in a connectionist sequential machine. Trong Proc. Conf. of the Cognitive Science Society, trang 531–546. Amherst, MA, USA, tháng 8 năm 1986.

[13] Sepp Hochreiter và Jürgen Schmidhuber. Long short-term memory. Neural computation, trang 1735–1780, 1997.

[14] Santiago Ontañón, Joshua Ainslie, Zachary Fisher, và Vaclav Cvicek. Making transformers solve compositional tasks. Trong Proc. Association for Computational Linguistics (ACL), trang 3591–3607, Dublin, Ireland, tháng 5 năm 2022.

[15] Róbert Csordás, Kazuki Irie, và Jürgen Schmidhuber. The devil is in the detail: Simple tricks improve systematic generalization of Transformers. Trong Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP), Punta Cana, Dominican Republic, tháng 11 năm 2021.

[16] Róbert Csordás, Kazuki Irie, và Jürgen Schmidhuber. The neural data router: Adaptive control flow in transformers improves systematic generalization. Trong Int. Conf. on Learning Representations (ICLR), chỉ ảo, tháng 4 năm 2022.

[17] Jürgen Schmidhuber. Self-delimiting neural networks. Preprint arXiv:1210.0118, 2012.

[18] Alex Graves. Adaptive computation time for recurrent neural networks. Trong Int. Conf. on Learning Representations (ICLR) Workshop Track, Vancouver, Canada, tháng 4 năm 2016.

[19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. Scaling laws for neural language models. Preprint arXiv:2001.08361, 2020.

[20] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, và Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? Trong Findings of the Association for Computational Linguistics: EMNLP, Singapore, tháng 12 năm 2023.

[21] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, và Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Compututaion, 3(1):79–87, 1991.

[22] John B. Hampshire II và Alexander H. Waibel. The meta-pi network: connectionist rapid adaptation for high-performance multi-speaker phoneme recognition. Trong Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), trang 165–168, Albuquerque, New Mexico, USA, tháng 4 năm 1990.

[23] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, và Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. Trong Int. Conf. on Learning Representations (ICLR), Toulon, France, tháng 4 năm 2017.

[24] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, và Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. Trong Int. Conf. on Learning Representations (ICLR), chỉ ảo, tháng 5 năm 2021.

[25] William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Preprint arXiv:2101.03961, 2021.

[26] Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack W. Rae, Erich Elsen, Koray Kavukcuoglu, và Karen Simonyan. Unified scaling laws for routed language models. Preprint arXiv:2202.01169, 2022.

[27] Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, và Zhang Xiong. Mixture of attention heads: Selecting attention heads per token. Trong Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP), trang 4150–4162, Abu Dhabi, United Arab Emirates, tháng 12 năm 2022.

[28] Róbert Csordás, Kazuki Irie, và Jürgen Schmidhuber. Approximating two-layer feedforward networks for efficient transformers. Trong Findings of the Association for Computational Linguistics: EMNLP 2023, tháng 11 năm 2023.

[29] Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pióro, Michal Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Król, Tomasz Odrzygózdz, Piotr Sankowski, Marek Cygan, và Sebastian Jaszczur. Scaling laws for fine-grained mixture of experts. Preprint arXiv:2402.07871, 2024.

[30] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, và Wenfeng Liang. DeepSeekMoE: Towards ultimate expert specialization in mixture-of-experts language models. Preprint arXiv:2401.06066, 2024.

[31] Róbert Csordás, Piotr Piękos, Kazuki Irie, và Jürgen Schmidhuber. SwitchHead: Accelerating transformers with mixture-of-experts attention. Trong Preprint arXiv:2312.07987, tháng 12 năm 2023.

[32] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, và Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

[33] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, và Tie-Yan Liu. On layer normalization in the transformer architecture. Trong Proc. Int. Conf. on Machine Learning (ICML), tập 119, trang 10524–10533, chỉ ảo, tháng 7 năm 2020.

[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Identity mappings in deep residual networks. Trong Proc. European Conf. on Computer Vision (ECCV), trang 630–645, Amsterdam, Netherlands, tháng 10 năm 2016.

[35] Jimmy Lei Ba, Jamie Ryan Kiros, và Geoffrey E Hinton. Layer normalization. Preprint arXiv:1607.06450, 2016.

[36] Simon J. Thorpe. Local vs. distributed coding. Intellectica, 8:3–40, 1989.

[37] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, và Christopher Olah. Toy models of superposition. Transformer Circuits Thread, 2022.

[38] Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron C. Courville, và Chuang Gan. Sparse universal transformer. Trong Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP), trang 169–179, Singapore, tháng 12 năm 2023.

[39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research (JMLR), 21:140:1–140:67, 2020.

[40] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, và Nolan Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama, tháng 6 năm 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B.

[41] Luca Soldaini và Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Báo cáo kỹ thuật, Allen Institute for AI, 2023. https://github.com/allenai/pes2o.

[42] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, và Harm de Vries. The Stack: 3 TB of permissively licensed source code. Preprint arXiv:2211.15533, 2022.

[43] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, và Yunfeng Liu. RoFormer: Enhanced transformer with rotary position embedding. Preprint arXiv:2104.09864, 2021.

[44] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, và Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse context. Trong Proc. Association for Computational Linguistics (ACL), Berlin, Germany, tháng 8 năm 2016.

[45] Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, và Samuel R. Bowman. BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Association for Computational Linguistics (TACL), 8:377–392, 2020.

[46] Felix Hill, Antoine Bordes, Sumit Chopra, và Jason Weston. The Goldilocks principle: Reading children's books with explicit memory representations. Trong Int. Conf. on Learning Representations (ICLR), San Juan, Puerto Rico, tháng 5 năm 2016.

[47] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. Hellaswag: Can a machine really finish your sentence? Trong Proc. Association for Computational Linguistics (ACL), trang 4791–4800, Florence, Italy, tháng 8 năm 2019.

[48] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, và Yejin Choi. PIQA: reasoning about physical commonsense in natural language. Trong Proc. AAAI Conf. on Artificial Intelligence, trang 7432–7439, New York, NY, USA, tháng 2 năm 2020. AAAI Press.

[49] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, và Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge. Preprint arXiv:1803.05457, 2018.

[50] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. ALBERT: A lite BERT for self-supervised learning of language representations. Trong Int. Conf. on Learning Representations (ICLR), chỉ ảo, tháng 4 năm 2020.

[51] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. BERT: pre-training of deep bidirectional Transformers for language understanding. Trong Proc. North American Chapter of the Association for Computational Linguistics on Human Language Technologies (NAACL-HLT), trang 4171–4186, Minneapolis, MN, USA, tháng 6 năm 2019.

[52] Leon Bergen, Timothy J. O'Donnell, và Dzmitry Bahdanau. Systematic generalization with edge transformers. Trong Proc. Advances in Neural Information Processing Systems (NeurIPS), trang 1390–1402, chỉ ảo, tháng 12 năm 2021.

[53] Shufang Xie, Huishuai Zhang, Junliang Guo, Xu Tan, Jiang Bian, Hany Hassan Awadalla, Arul Menezes, Tao Qin, và Rui Yan. Residual: Transformer with dual residual connections. Preprint arXiv:2304.14802, 2023.

[54] Sho Takase và Shun Kiyono. Lessons on parameter sharing across layers in transformers. Trong Nafise Sadat Moosavi, Iryna Gurevych, Yufang Hou, Gyuwan Kim, Young Jin Kim, Tal Schuster, và Ameeta Agrawal, biên tập, SustaiNLP Workshop, trang 78–90, Toronto, Canada, tháng 7 năm 2023. Association for Computational Linguistics.

[55] Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee, và Sunghun Kim. SOLAR 10.7B: Scaling large language models with simple yet effective depth up-scaling. Preprint arXiv:2312.15166, 2023.

[56] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. Trong Proc. Advances in Neural Information Processing Systems (NeurIPS), New Orleans, Louisiana, USA, tháng 12 năm 2022.

[57] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, và Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. Trong Proc. Advances in Neural Information Processing Systems (NeurIPS), trang 8024–8035, Vancouver, Canada, tháng 12 năm 2019.

[58] Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization. Trong Int. Conf. on Learning Representations (ICLR), New Orleans, LA, USA, tháng 5 năm 2019.

[59] Taku Kudo và John Richardson. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Trong Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP), trang 66–71, Brussels, Belgium, tháng 10 năm 2018.

A Phụ lục

A.1 Tuyên bố Tác động Rộng rãi

Chúng tôi coi đây là một bài báo nghiên cứu nền tảng không có tác động xã hội trực tiếp. Tuy nhiên, nghiên cứu mới xây dựng trên công trình này có thể phá vỡ rào cản tổng quát hóa của các mô hình hiện tại, cho phép suy luận tốt hơn. Điều này có thể là một bước nhảy hướng tới Trí tuệ Nhân tạo Tổng quát, có thể có những hậu quả không thể đoán trước, cả tích cực và tiêu cực. Ngoài ra, việc triển khai tốt hơn các CUDA kernels của chúng tôi có thể dẫn đến các mô hình nền tảng hiệu quả hơn so với những mô hình hiện tại, có thể làm chúng dễ tiếp cận hơn. Điều này có thể có lợi vì giảm việc sử dụng năng lượng, nhưng nó cũng có thể cho phép tạo ra nội dung có hại dễ dàng hơn như tin tức giả hoặc deepfakes.

A.2 Norm Residual Tăng trưởng trong Standard Transformers

Trong Phần 2.4, chúng tôi đã thảo luận về vấn đề norm residual tăng trưởng trong Transformers tiêu chuẩn. Ở đây, chúng tôi đo norm L2 của sự khác biệt của residual trước và sau khi áp dụng một lớp Transformer tiêu chuẩn (cả khối attention và MLP) trong các lớp khác nhau của Transformer 44M tham số được huấn luyện trên C4. Kết quả được hình dung trong Hình 12. Có thể thấy rằng norm của các cập nhật thực sự tăng trong các lớp sau.

A.3 Hiệu suất Zero-Shot Downstream của các biến thể SUT

Ngoài việc đánh giá perplexity của các biến thể SUT khác nhau trong Hình 7, chúng tôi cũng hiển thị hiệu suất zero-shot downstream của chúng trong Bảng 2. Có thể thấy rằng MoEUT liên tục vượt trội hơn cả SUT và SUT không có ACT.

A.4 Hyperparameters

Tất cả các mô hình của chúng tôi được huấn luyện trong PyTorch [57] với batch size 64, độ dài ngữ cảnh 1024, trong 100k iterations, learning rate 0.00025, optimizer AdamW [58] với hyperparameters mặc định, weight decay 0.01. Chúng được huấn luyện trên một node duy nhất theo cách data-parallel. Learning rate được giảm xuống 10% giá trị ban đầu bằng cosine decay. Chúng tôi sử dụng gradient clipping κ và Nwarmup linear learning rate warmup steps (xem Bảng 3). Không có mô hình nào sử dụng dropout. Đối với entropy regularization của MLP expert selection, chúng tôi sử dụng γ = 0.01 và cho SwitchHead attention δ = 0.001. Expert dropout không được sử dụng. Tất cả các mô hình của chúng tôi sử dụng tokenizer SentencePiece [59] với 8000 tokens, được huấn luyện trên tập con của tập huấn luyện cho tập dữ liệu nhất định. Tất cả các mô hình được huấn luyện với mixed precision. Hyperparameters của các mô hình SUT có thể tìm thấy trong Bảng 4. Lưu ý rằng ý nghĩa của các tham số không tương tự trực tiếp với của chúng tôi. Vui lòng tham khảo Tan et al. [38] để biết thêm chi tiết.

A.5 Ảnh hưởng của dexpert và K

Ở đây, chúng tôi phân tích ảnh hưởng của kích thước expert cho một lượng compute cố định (dexpert·K được giữ không đổi). Kết quả được hiển thị trong Hình 13. Có thể thấy rằng sử dụng fine-grained mixture of experts (experts nhỏ) thực sự quan trọng cho hiệu suất tốt. Trong các mô hình của chúng tôi, chúng tôi sử dụng dexpert = 128. Sử dụng experts nhỏ hơn giảm đáng kể hiệu quả compute của Triton kernel. Xin lưu ý rằng các thí nghiệm này giữ số lượng active channels trong khối MLP không đổi. Do đó, ảnh hưởng dựa hoàn toàn trên động lực của cơ chế lựa chọn.

Chúng tôi cũng phân tích hiệu suất của mô hình MoEUT với số lượng experts hoạt động khác nhau trong lớp MLP. Điều này thay đổi lượng compute chi tiêu trong lớp, và số lượng active channels. Chúng tôi hiển thị kết quả trong Hình 14. Việc tăng số lượng experts hoạt động luôn cải thiện hiệu suất, nhưng lợi ích giảm dần. Chúng tôi chọn K = 16 cho các thí nghiệm của chúng tôi vì lý do hiệu quả.

A.6 Phân tích Bổ sung

Chuyên biệt hóa expert cho tokens/layers. Bây giờ ngược lại với phân tích "sự đa dạng lựa chọn expert per-token" được trình bày trong văn bản chính, chúng tôi phân tích liệu các experts được kích hoạt bởi một token có layer-specific cho token cụ thể đó hay không. Để làm điều này, chúng tôi đếm số lượng experts duy nhất được sử dụng bởi mỗi token và tính toán tỷ lệ tương ứng cho mỗi lớp. Kết quả được hiển thị trong Hình 15. Ở đây, chúng tôi sắp xếp các tokens (trục x) theo tần suất của chúng trong tập validation (cùng thứ tự được sử dụng cho tất cả các lớp). 6000 tokens đầu tiên được hiển thị.

Chúng tôi quan sát rằng đối với các tokens thường xuyên nhất (hướng về phần trái của biểu đồ), điểm số cao gần 1.0 được thu được trong nhiều lớp cho một token nhất định; điều này có nghĩa là (gần như) tất cả experts được sử dụng bởi token cụ thể đó được sử dụng trong nhiều lớp. Ngược lại, chúng tôi quan sát rằng experts có xu hướng layer-specific hơn cho các tokens ít phổ biến (hướng về phần phải của biểu đồ). Ngoài ra, tập hợp experts được chọn trong các lớp sớm thường ít đa dạng hơn so với phần còn lại của các lớp: chỉ một phần nhỏ của các experts được sử dụng có mặt ở đó. Điều này phù hợp với những phát hiện cho Layer 1 trong Hình 10.

A.7 So sánh Thời gian Wall Clock và Bộ nhớ

Để hiển thị việc sử dụng tài nguyên thực tế của các mô hình trực tiếp, chúng tôi chạy một thí nghiệm có kiểm soát trên phần cứng giống hệt với mô hình 244M tham số và các baselines tương ứng.

Chúng tôi đo thời gian iteration huấn luyện và sử dụng bộ nhớ trên 8 GPU V100 32GB. Ở đây một "iteration" tương ứng với batch size hiệu quả 64x1024 tokens cho tất cả các mô hình. Thời gian iteration huấn luyện được đo bằng cách sử dụng batch size cho mỗi mô hình phù hợp với GPUs; các mô hình yêu cầu 1 hoặc 2 gradient accumulation steps để đạt được batch size hiệu quả tùy thuộc vào yêu cầu bộ nhớ của chúng. Chúng tôi đo thời gian huấn luyện ngay sau khi khởi tạo và một giai đoạn warmup. Việc sử dụng bộ nhớ được đo bằng cách sử dụng 2 grad accumulation steps cho tất cả các mô hình để so sánh công bằng. Lưu ý rằng khoảng 3GB bộ nhớ được sử dụng bởi các tham số mô hình và trạng thái optimizer trên mỗi GPU.

Mặc dù MoEUT của chúng tôi với triển khai kernel hiện tại chậm hơn so với transformer lớp dense không chia sẻ tương ứng, nó nhanh hơn đáng kể và sử dụng ít bộ nhớ hơn nhiều so với các biến thể UT thay thế.

A.8 Yêu cầu Compute

Chúng tôi báo cáo phần cứng được sử dụng và thời gian wall clock yêu cầu cho các thí nghiệm chính của chúng tôi trong bài báo trong Bảng 6. Tất cả các thí nghiệm được thực hiện trong các cụm riêng. Thời gian được báo cáo trong định dạng "hh:mm". Chúng tôi báo cáo số lượng GPUs (NGPU) được sử dụng cho thí nghiệm cụ thể đó (và không phải tổng số GPUs trong hệ thống). Đối với số lượng CPUs (NCPU) và RAM, chúng tôi báo cáo tổng số trong node, vì những tài nguyên này được chia sẻ giữa các lần chạy đồng thời.

Lưu ý rằng báo cáo được tạo từ các log của Weights and Biases. Vì điều này, thời gian có thể bao gồm ảnh hưởng của việc khởi động lại do SLURM preemption và ảnh hưởng cho việc chậm lại thỉnh thoảng của network drive. Ngoài ra, trong một vài trường hợp, buffer log của Weights and Biases bị tràn, làm cho không thể xác định số lượng GPUs được sử dụng cho thí nghiệm. Trong trường hợp này, chúng tôi báo cáo "??" cho thí nghiệm tương ứng. Lưu ý rằng chúng tôi chỉ báo cáo việc sử dụng tài nguyên của các thí nghiệm cuối cùng ở đây. Chúng tôi ước tính rằng tổng chi phí của các thí nghiệm thất bại và các lần chạy sơ bộ ít nhất cao hơn một bậc so với điều này.
