# 2204.09598.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2204.09598.pdf
# Kích thước tệp: 351038 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
XÂY DỰNG HỆ THỐNG HỎI ĐÁP MẠNH MẼ VỚI
MIXTURE OF EXPERTS DỰA TRÊN TRANSFORMER
Yu Qing Zhou
Đại học Stanford
ivanz@stanford.eduXixuan Julie Liu
Đại học Stanford
xl99@stanford.eduYuanzhe Dong
Đại học Stanford
yzd@stanford.edu

TÓM TẮT
Trong bài báo này, chúng tôi hướng tới việc xây dựng một hệ thống hỏi đáp mạnh mẽ có thể thích ứng với các tập dữ liệu ngoài miền. Một mạng đơn lẻ có thể khớp quá mức với mối tương quan bề ngoài trong phân phối huấn luyện, nhưng với một số lượng có ý nghĩa các mạng con chuyên gia, một mạng cổng chọn lựa một tổ hợp thưa thớt các chuyên gia cho mỗi đầu vào, và sự cân bằng cẩn thận về tầm quan trọng của các mạng con chuyên gia, mô hình Mixture-of-Experts (MoE) cho phép chúng tôi huấn luyện một người học đa nhiệm có thể được tổng quát hóa cho các tập dữ liệu ngoài miền. Chúng tôi cũng khám phá khả năng đưa các lớp MoE lên giữa DistilBERT [1] và thay thế mạng feed forward dày đặc bằng các lớp switch FFN được kích hoạt thưa thớt, tương tự như kiến trúc Switch Transformer [2], điều này đơn giản hóa thuật toán định tuyến MoE với chi phí truyền thông và tính toán giảm. Ngoài kiến trúc mô hình, chúng tôi khám phá các kỹ thuật tăng cường dữ liệu bao gồm Easy Data Augmentation (EDA) và dịch ngược, để tạo ra sự khác biệt có ý nghĩa hơn trong số dữ liệu huấn luyện ngoài miền nhỏ, do đó tăng cường hiệu suất và tính mạnh mẽ của các mô hình. Trong bài báo này, chúng tôi cho thấy rằng sự kết hợp giữa kiến trúc tốt nhất và các kỹ thuật tăng cường dữ liệu đạt được điểm F1 53.477 trong đánh giá ngoài miền, đây là mức tăng hiệu suất 9.52% so với mô hình cơ sở. Trên tập kiểm tra cuối cùng, chúng tôi báo cáo F1 59.506 và EM 41.651 cao hơn. Chúng tôi đã thành công chứng minh tính hiệu quả của kiến trúc Mixture-of-Expert trong nhiệm vụ QA Mạnh mẽ.

Từ khóa: Trí tuệ nhân tạo, Xử lý ngôn ngữ tự nhiên, Học máy

1 Giới thiệu
Trong nhiệm vụ hỏi đáp (QA), một mô hình sẽ được đưa một câu hỏi làm đầu vào, cùng với một đoạn văn dài làm ngữ cảnh. Nó được kỳ vọng sẽ xuất ra một câu trả lời cho câu hỏi. Có rất nhiều loại câu hỏi khác nhau, bao gồm tại sao, cái gì, như thế nào, dựa trên sự thật, dựa trên ngữ nghĩa, v.v. Cụ thể cho nhiệm vụ của chúng tôi, mô hình cần chọn một đoạn văn bản (các chỉ mục bắt đầu và kết thúc) từ đoạn văn ngữ cảnh làm câu trả lời cho câu hỏi, nếu câu hỏi có thể trả lời được, và xuất ra N/A nếu không.

Tính mạnh mẽ đối với dữ liệu ngoài phân phối là rất quan trọng để xây dựng các hệ thống NLP có thể tổng quát hóa vì dữ liệu huấn luyện và kiểm tra thường đến từ các tương tác người dùng hoặc nguồn khác nhau. Trong bài báo này, chúng tôi được cung cấp ba tập dữ liệu đọc hiểu chính trong miền (Natural Questions [3], NewsQA [4] và SQuAD [5]) và ba tập dữ liệu đọc hiểu nhỏ ngoài miền (RelationExtraction [6], DuoRC [7], RACE [8]) để huấn luyện một hệ thống QA sẽ được đánh giá trên các ví dụ kiểm tra từ các tập dữ liệu ngoài miền.

Với sự biến đổi của các nhiệm vụ QA được yêu cầu, chúng tôi hướng tới việc xây dựng một người học ngôn ngữ đa nhiệm cho bài báo của mình. Và kỹ thuật Mixture-Of-Experts (MoE), nhằm chia một nhiệm vụ phức tạp thành các nhiệm vụ con thích hợp, mỗi nhiệm vụ có thể được giải quyết bởi một mạng chuyên gia, dường như là một cách tiếp cận trực quan. Cùng với trọng số hỗn hợp cho các mô hình chuyên gia này được tạo ra bởi một hàm cổng, chúng ta có thể có một mô hình QA có thể ngoại suy tốt hơn tại mỗi ví dụ trong quá trình suy luận.

Trong bài báo này, chúng tôi khám phá hai kiến trúc trên xương sống DistilBERT, MoE và Switch Transformer, và huấn luyện các mô hình trên các tập dữ liệu trong miền và các tập dữ liệu ngoài miền được tăng cường để cải thiện hiệu suất QA thích ứng miền. Chúng tôi thực hiện và phân tích các thí nghiệm mở rộng để hiểu tính hiệu quả của các phương pháp và đạt được sự kết hợp tốt nhất của các mô hình và kỹ thuật thông qua nghiên cứu loại bỏ.

--- TRANG 2 ---
Trí tuệ nhân tạo Stanford

2 Công trình liên quan
Trong khi một mạng đơn lẻ có thể khớp quá mức với phân phối bề ngoài trong dữ liệu huấn luyện trong miền, với một số lượng có ý nghĩa các mạng con chuyên gia, một mạng cổng chọn lựa một tổ hợp thưa thớt các chuyên gia cho mỗi ví dụ đầu vào, và sự cân bằng cẩn thận về tầm quan trọng của các mạng con chuyên gia, một mô hình Mixture-of-Experts (MoE) [9] có thể huấn luyện một người học mạnh mẽ có thể được tổng quát hóa cho các tập dữ liệu ngoài miền. Tuy nhiên, bài báo [9] không đề cập đến việc MoE áp dụng tốt như thế nào cho nhiệm vụ QA.

Được truyền cảm hứng bởi thành công của Transformer quy mô lớn [10], trong khi tìm kiếm hiệu quả tính toán lớn hơn, Switch Transformer [2] được đề xuất như một mô hình chuyên gia được kích hoạt thưa thớt. Nó kích hoạt một tập con của các trọng số mạng nơ-ron cho mỗi ví dụ đầu vào. Switch Transformer đơn giản hóa thuật toán định tuyến MoE với chi phí truyền thông và tính toán giảm.

Ngoài các kiến trúc mới, tăng cường dữ liệu cũng có thể tăng cường hiệu suất và tính mạnh mẽ của huấn luyện. Các kỹ thuật tăng cường dữ liệu dễ dàng (EDA) [11], bao gồm thay thế từ đồng nghĩa, xóa ngẫu nhiên, hoán đổi ngẫu nhiên, và chèn ngẫu nhiên, đã cho thấy tính hiệu quả trên các tập dữ liệu nhỏ, mặc dù tính đơn giản của chúng. Dịch ngược là một kỹ thuật khác cũng đã được chứng minh cải thiện hiệu suất đọc hiểu [12], do đó ngày càng phổ biến.

3 Phương pháp
Như hình 1 cho thấy, sau lớp đầu ra của DistilBERT [1], chúng tôi thêm một lớp được kết nối đầy đủ song song như chuyên gia và một lớp tuyến tính khác phục vụ như hàm cổng, trước khi tạo ra đầu ra cuối cùng. Với đầu vào x, đầu ra y của mô hình là y=Σⁿᵢ₌₁G(x)ᵢEᵢ(x), trong đó G(x)ᵢ là đầu ra của hàm cổng và E(x) là đầu ra của mạng chuyên gia thứ i.

Hình 1: Kiến trúc Sparsely-Gated Mixture-of-Experts [9]

Đối với Switch Transformer, chúng tôi đưa các lớp MoE lên giữa mô hình DistilBERT [1] và thay thế mạng feed forward dày đặc bằng các lớp switch FFN được kích hoạt thưa thớt, như hình 2 cho thấy. Thông qua các thử nghiệm, chúng tôi thấy rằng 8 lớp switch FFN hoạt động tốt nhất. Chúng tôi chọn 8 lớp switch FFN trong các thí nghiệm sau.

Đối với EDA, sau khi tăng cường dữ liệu của mỗi đoạn văn ngữ cảnh, chúng tôi khớp lại các câu trả lời trong ngữ cảnh được tăng cường. Để giảm thất bại của việc khớp lại, chúng tôi tránh các thao tác trên các từ trong ngữ cảnh cũng xuất hiện trong các câu trả lời.

Tương tự, đối với dịch ngược, chúng tôi chỉ dịch ngữ cảnh trước và sau câu trả lời. Chúng tôi sử dụng Google Translation API cho tốc độ và độ chính xác tốt hơn. Chúng tôi sử dụng tiếng Tây Ban Nha, tiếng Pháp và tiếng Đức làm ngôn ngữ trung gian (Phụ lục A.1).

4 Thí nghiệm
4.1 Dữ liệu
Có tổng cộng 6 tập dữ liệu. Ba tập dữ liệu trong miền, SQuAD [5], NewsQA [4], và Natural Questions [3], chủ yếu được sử dụng để huấn luyện hệ thống QA; ba tập dữ liệu ngoài miền, DuoRC [7], RACE [8], và RelationExtraction [6], được sử dụng cho đánh giá. Trong quá trình huấn luyện, mô hình sẽ thấy 50,000 ví dụ huấn luyện từ mỗi tập dữ liệu trong miền và chỉ 127 ví dụ từ mỗi tập dữ liệu ngoài miền để tinh chỉnh. Cuối cùng, chúng tôi sẽ báo cáo hiệu suất trên các tập kiểm tra từ ba tập dữ liệu ngoài miền.

--- TRANG 3 ---
Trí tuệ nhân tạo Stanford

Hình 2: Kiến trúc Switch transformer [2]

4.2 Phương pháp đánh giá
Để đánh giá, chúng tôi sẽ báo cáo hiệu suất trên cả Exact Match (EM) và điểm F1, được tính trung bình trên toàn bộ tập dữ liệu đánh giá.

• Exact Match: đây là một phép đo nhị phân xem liệu dự đoán của mô hình có khớp chính xác với câu trả lời đích hay không.
• Điểm F1: được tính dựa trên độ chính xác và độ nhớ bằng cách so sánh dự đoán của mô hình với câu trả lời đích từng từ.

4.3 Chi tiết thí nghiệm
Từ các thí nghiệm của chúng tôi, chúng tôi thấy rằng tất cả các mô hình sẽ hội tụ trong vòng 5 epoch. Do đó, chúng tôi huấn luyện các mô hình trong 5 epoch, với tốc độ học 3e-5. Chúng tôi sử dụng kích thước batch là 16.

• Đối với mô hình Sparse-gated Mixture-of-Expert (MoE), chúng tôi sử dụng chiều ẩn 3,072 và đánh giá số lượng chuyên gia trong phạm vi từ 1 đến 16.
• Đối với Switch Transformer, chúng tôi khám phá số lượng lớp transformer trong phạm vi từ 1 đến 16.

Chúng tôi báo cáo hiệu suất ngay tức thì của mô hình cơ sở được huấn luyện chỉ với tập dữ liệu trong miền. Đối với các thí nghiệm khác, chúng tôi sử dụng sự kết hợp của các ví dụ trong miền và ngoài miền để huấn luyện.

Việc tăng cường dữ liệu chỉ được áp dụng cho các ví dụ huấn luyện ngoài miền, xem xét sự thống trị không cân xứng của các ví dụ trong miền trong tập huấn luyện.

4.4 Kết quả
Chúng tôi chia sẻ tóm tắt kết quả thí nghiệm của chúng tôi về các kiến trúc mô hình MoE khác nhau và các kỹ thuật tăng cường dữ liệu trong Bảng 1. Đầu tiên, với mô hình DistilBERT cơ sở, chúng tôi cải thiện F1 bằng cách đơn giản bao gồm các ví dụ Ngoài miền trong tập huấn luyện.

Trong so sánh giữa kiến trúc MoE, chúng tôi thấy cả Sparsely-gated MoE và Switch Transformer đều đạt được hiệu suất tốt hơn so với DistilBERT cơ sở. Switch Transformer đẩy hiệu suất lên 3.222 đến 52.052!

Đáng chú ý, chúng tôi đã thử huấn luyện một mô hình DistilBERT riêng biệt cho mỗi tập dữ liệu, và một MLP nhỏ làm hàm cổng. Cách tiếp cận này thất bại trong so sánh với mô hình cơ sở. Để hiểu điều này, chúng tôi xem xét hiệu suất của mỗi mô hình DistilBERT riêng biệt. Như được hiển thị trong Bảng 2, tất cả các mô hình được huấn luyện trên tập dữ liệu đơn lẻ đều khớp quá mức đáng kể với tập dữ liệu trong miền và do đó hoạt động kém trong các ví dụ ngoài miền trên tập xác thực. F1 ngoài miền tốt nhất chỉ là 43.469, từ mô hình được huấn luyện trên NewsQA. Sau khi kết hợp ba mô hình như chuyên gia và tinh chỉnh hàm cổng với các ví dụ huấn luyện ngoài miền, mô hình mới có F1 tốt hơn trên tập xác thực ngoài miền, 47.096, nhưng vẫn hoạt động kém hơn Baseline vì không ai trong số họ hoạt động tốt trên các ví dụ ngoài miền. Đây là một hỗn hợp của "không-chuyên-gia". Ngược lại, đối với cả Sparsely-gated MoE và Switch Transformers, cả hai đều được tiếp xúc với tất cả 6 tập dữ liệu khác nhau trong huấn luyện, vì vậy các chuyên gia bên trong được ủy thác học các phân phối cơ bản khác nhau giữa các tập dữ liệu và hàm cổng được huấn luyện để chọn các chuyên gia phù hợp cho mỗi ví dụ đầu vào. Cơ chế huấn luyện "tự giám sát" này cho phép chúng được tổng quát hóa tại các nhiệm vụ QA đa dạng, do đó mạnh mẽ hơn đối với sự chuyển đổi miền.

Bảng 1: Một nghiên cứu loại bỏ về kiến trúc mô hình và tăng cường dữ liệu. Hiệu suất được báo cáo là F1 đạt được trên tập dữ liệu xác thực ngoài miền. Cột 'Cải thiện' chỉ ra sự cải thiện so với cơ sở.

Xử lý | Thí nghiệm | F1 | Cải thiện
Cơ sở | DistilBERT Baseline | 48.83 | -
| DistilBERT +OOD | 51.330 | 2.5
Khám phá Kiến trúc MoE | Một Chuyên gia mỗi Tập dữ liệu | 47.096 | -1.734
| Sparsely-gated MoE | 51.901 | 3.071
| Switch Transformer | 52.052 | 3.222
Tăng cường Dữ liệu (với Switch Transformer) | EDA | 52.396 | 3.566
| Dịch ngược | 52.905 | 4.075
| EDA + dịch ngược | 53.477 | 4.647

Bảng 2: Hiệu suất của mô hình DistilBERT được huấn luyện riêng biệt trên mỗi tập dữ liệu trong miền, được báo cáo trên cả tập xác thực trong miền và ngoài miền.

Tập dữ liệu Huấn luyện | F1 Trong miền | F1 Ngoài miền
NewsQA | 55.66 | 43.469
SQuAD | 54.046 | 42.126
Natural Questions | 57.058 | 39.813

Từ so sánh các kiến trúc MoE khác nhau, chúng tôi thấy rằng Switch Transformer cho F1 ngoài miền tốt nhất. Sau đó, chúng tôi đánh giá tính hiệu quả của các kỹ thuật tăng cường dữ liệu với Switch Transformers. Chúng tôi quan sát thấy sự cải thiện 0.344 và 0.853 tương ứng với Easy Data Augmentation (EDA) và dịch ngược. Khi chúng được áp dụng cùng nhau với Switch Transformer, chúng tôi thấy điểm F1 cao hơn nữa là 53.477. Điều này có nghĩa là các kỹ thuật tăng cường dữ liệu khác nhau có thể bổ sung cho nhau. Trong công việc tương lai, chúng tôi khuyến nghị khám phá các kỹ thuật tăng cường dữ liệu khác nhau khác để xem liệu hiệu suất có thể được nâng lên mức cao hơn.

Sự kết hợp giữa kiến trúc MoE tốt nhất và tăng cường dữ liệu đạt được điểm F1 53.477 trong tập xác thực ngoài miền, đây là mức tăng hiệu suất 9.52% so với cơ sở. Trên tập kiểm tra cuối cùng, chúng tôi báo cáo F1 59.322 và EM 41.995 cao hơn. Điều này hiệu quả cho thấy tính mạnh mẽ của hệ thống QA của chúng tôi.

4.4.1 Phân tích Định lượng về Số lượng Chuyên gia
Ở đây chúng tôi đang hiển thị phân tích định lượng chi tiết hơn xung quanh số lượng chuyên gia trong các kiến trúc MoE khác nhau.

Đầu tiên, chúng tôi xem xét kiến trúc Sparse-gated Mixture-of-Expert đơn giản. Trong Hình 4.4.1, chúng tôi xem xét ảnh hưởng đến hiệu suất bởi số lượng chuyên gia trong mạng. So với tối thiểu 1 chuyên gia, mô hình đạt được F1 tốt hơn với 2 chuyên gia và EM tốt hơn với 4 chuyên gia. Đây là một dấu hiệu tốt của việc thêm các chuyên gia bổ sung giúp cải thiện tính mạnh mẽ. Tuy nhiên, hiệu suất bắt đầu giảm với nhiều chuyên gia được thêm vào mô hình. Chúng tôi tin rằng đó là do thực tế là mô hình thiết lập sự phụ thuộc quá mức vào cùng một vài chuyên gia và các chuyên gia khác chỉ thêm tiếng ồn thay vì ý kiến có giá trị cho các đầu ra. Trong giai đoạn đầu của huấn luyện, một vài chuyên gia may mắn tạo ra kết quả tốt, vì vậy họ được hàm cổng ưu tiên hơn và các tham số của họ được cập nhật thường xuyên hơn, do đó củng cố sự mất cân bằng này. Một giải pháp khả thi cho vấn đề này là áp dụng nhiều ràng buộc hơn để cân bằng tầm quan trọng và khối lượng công việc giữa các chuyên gia trong hàm mất mát, điều mà chúng tôi sẽ đi sâu vào phần 4.4.3.

Chúng tôi cũng xem xét ảnh hưởng của số lượng chuyên gia trong Switch Transformer, như được hiển thị trong Hình 4. Trong so sánh, Switch Transformer gặp phải vấn đề tương tự là các chuyên gia bổ sung trên 4 không đóng góp vào hiệu suất, nhưng so với MoE có cổng thưa thớt, Switch Transformer mạnh mẽ hơn đối với việc thêm chuyên gia. Trong Switch Transformer [2], đối với mỗi lớp Switch, một mất mát phụ mới được tính toán để cân bằng tải và tầm quan trọng của các chuyên gia. Dựa trên đánh giá của chúng tôi, có vẻ như mất mát phụ trong Switch Transformer có hiệu quả trong việc cân bằng giữa các chuyên gia.

--- TRANG 4 ---
Trí tuệ nhân tạo Stanford

Hình 3: Hiệu suất đạt được với số lượng chuyên gia khác nhau trong mô hình Sparse-gated Mixture-of-Expert, được đánh giá trong các chỉ số F1 và EM ngoài miền.

Hình 4: Hiệu suất của MoE so với Transformer, với số lượng chuyên gia khác nhau

4.4.2 Phân tích Định lượng về Tăng cường Dữ liệu
Trong phần này, chúng tôi đi sâu vào tính hiệu quả của tăng cường dữ liệu trong DistilBERT cơ sở, Sparsely-gated MoE, và Switch Transformer tương ứng.

Đối với Easy Data Augmentation (EDA), theo mặc định chúng tôi tạo ra 4 ngữ cảnh được tăng cường mỗi ngữ cảnh đầu vào. sr là tỷ lệ phần trăm thay thế từ đồng nghĩa ngẫu nhiên trong một câu sử dụng từ điển từ đồng nghĩa WordNet, rs là tỷ lệ phần trăm các từ được hoán đổi vị trí ngẫu nhiên trong một câu, ri là tỷ lệ phần trăm chèn một từ đồng nghĩa của một từ ngẫu nhiên vào một vị trí ngẫu nhiên trong một câu, và rd là tỷ lệ phần trăm các từ được xóa ngẫu nhiên trong một câu.

Đầu tiên, chúng tôi huấn luyện mô hình DistilBERT chỉ trên các ví dụ ngoài miền, được tăng cường với EDA, dịch ngược, và sự kết hợp của chúng. Như được hiển thị trong bảng 3, tất cả việc tăng cường dữ liệu này đều mang lại sự bổ sung có ý nghĩa của tập dữ liệu huấn luyện và cải thiện hiệu suất. Sự kết hợp của cả hai tăng cường dữ liệu mang lại mức tăng hiệu suất nhiều nhất. Đây là một tín hiệu rất tích cực rằng lợi ích của các kỹ thuật tăng cường dữ liệu có thể bổ sung cho nhau.

Trong thí nghiệm với Sparsely-gated MoE, chúng tôi đánh giá tính hiệu quả của EDA.

Cuối cùng, trên Switch Transformer, chúng tôi áp dụng dịch ngược thông qua tiếng Tây Ban Nha và báo cáo hiệu suất trong Bảng 5. Đối với hầu hết tất cả số lượng chuyên gia, chỉ có một ngoại lệ, dịch ngược cải thiện hiệu suất so với đối tác không có dịch ngược. Mức tăng lớn nhất được quan sát tại Switch Transformer với 16 chuyên gia, có khả năng vì mô hình đó có khả năng lớn nhất trong tất cả. Dựa trên những kết quả này, chúng tôi thêm dịch ngược vào cấu hình mô hình cuối cùng.

Thú vị, trên Distilbert (bảng 3), dịch ngược chỉ với tiếng Tây Ban Nha hoạt động tốt hơn với nhiều ngôn ngữ trung gian (Tây Ban Nha+Pháp, hoặc Tây Ban Nha+Pháp+Đức). Điều này có thể là do việc khớp quá mức từ nhiều phiên bản dữ liệu huấn luyện được dịch ngược. Tuy nhiên, trong cấu hình mô hình cuối cùng Switch Transformer, sử dụng ba ngôn ngữ trung gian (Tây Ban Nha, Pháp, Đức) dẫn đến hiệu suất tốt hơn chỉ với tiếng Tây Ban Nha (bảng 1). Điều này cho thấy rằng một mô hình với khả năng lớn hơn có thể hưởng lợi tốt hơn từ tăng cường dữ liệu quy mô lớn hơn.

Bảng 3: Tăng cường Dữ liệu - DistilBERT được huấn luyện chỉ trên dữ liệu OOD

EDA | Dịch ngược | F1 | Cải thiện
Không có | Không có | 25.971 | -
sr = 0.3 | Không có | 26.416 | 0.445
sr = rs = ri = rd = 0.1 | Không có | 28.445 | 2.474
Không có | Tây Ban Nha | 30.17 | 4.199
Không có | Tây Ban Nha, Pháp | 29.741 | 3.77
Không có | Tây Ban Nha, Pháp, Đức | 29.231 | 3.26
sr = rs = ri = rd = 0.1 | Tây Ban Nha | 30.638 | 4.667 (17.97%)

Bảng 4: Tăng cường Dữ liệu - EDA

Mô hình | #chuyên gia | EDA | F1 | Cải thiện
MoE | 1 | sr = 0.3 | 52.599 | 1.387
MoE | 2 | sr = 0.3 | 51.617 | 0.101

--- TRANG 5 ---
Trí tuệ nhân tạo Stanford

Bảng 5: Tăng cường Dữ liệu - Dịch ngược

Mô hình | #chuyên gia | Dịch ngược | F1 | Cải thiện
các lớp transformer = 8
Transformer | 1 | Tây Ban Nha | 52.599 | 0.547
Transformer | 2 | Tây Ban Nha | 51.617 | 0.226
Transformer | 4 | Tây Ban Nha | 51.719 | -0.145
Transformer | 8 | Tây Ban Nha | 51.706 | 1.118
Transformer | 10 | Tây Ban Nha | 50.816 | 0.073
Transformer | 12 | Tây Ban Nha | 50.73 | 0.447
Transformer | 16 | Tây Ban Nha | 51.888 | 1.161

4.4.3 Phân tích Định lượng về Mất mát Cân bằng Tải của Switch Transformer
Vì trong phần 4.4.1 chúng tôi nghi ngờ rằng sự mất cân bằng về tầm quan trọng của các chuyên gia trong quá trình huấn luyện có thể là lý do tại sao các mô hình với 1 hoặc 2 chuyên gia hoạt động tốt nhất, chúng tôi quyết định điều tra thêm về mất mát cân bằng tải trong switch transformer, đây là một mất mát phụ được giới thiệu để khuyến khích tải cân bằng giữa các chuyên gia 2. Đối với mỗi lớp Switch, với N chuyên gia được lập chỉ mục bởi i = 1 đến N và một batch B với T token, mất mát cân bằng tải được tính như tích vô hướng có tỷ lệ giữa các vector f và P:

lossload = N∑ᵢ₌₁ᴺ fᵢPᵢ (1)

trong đó fᵢ là phần của token được gửi đến chuyên gia i và Pᵢ là phần của xác suất router được phân bổ cho chuyên gia i. Phương trình 1 khuyến khích định tuyến đều của batch token qua N chuyên gia. Siêu tham số α là hệ số nhân cho mất mát phụ.

Đối với các thí nghiệm ở trên, chúng tôi sử dụng α = 0.01 mặc định. Bây giờ chúng tôi muốn xem liệu hiệu suất của các mô hình với 4 và 16 chuyên gia có cải thiện với hệ số mất mát cân bằng tải lớn hơn α ∈ {0.1, 0.05, 1, 2}.

Nếu chúng tôi so sánh theo hàng, bằng cách tăng hệ số α, chúng tôi quan sát thấy hiệu suất của mô hình với 4 chuyên gia không cải thiện, nhưng mô hình với 16 chuyên gia có cải thiện. Chúng tôi tin rằng đó là bởi vì với nhiều chuyên gia, mô hình có thể phân tán tải giữa các chuyên gia khác nhau và tận dụng nhiều chuyên gia dễ dàng hơn.

--- TRANG 6 ---
Trí tuệ nhân tạo Stanford

Bảng 6: Hiệu suất của các mô hình transformer (điểm đánh giá F1 ngoài miền), với hệ số mất mát cân bằng tải khác nhau

Hệ số mất mát cân bằng tải α | 0.05 | 0.1 | 1 | 2
Transformer 4 | 52.828 | 52.697 | 52.351 | 49.741
Transformer 16 | 50.441 | 50.909 | 51.033 | 51.325

Tuy nhiên, hiệu suất tốt nhất trong Bảng 6 được đạt với 4 chuyên gia và hệ số nhỏ hơn α = 0.05. Nó vẫn nhỏ hơn hiệu suất tốt nhất trong Bảng 1, đạt được với 1 chuyên gia. Hệ số α cung cấp sự đánh đổi giữa đảm bảo cân bằng tải và mục tiêu entropy chéo chính. Có vẻ như mất mát cân bằng tải này có hiệu quả trong việc phân tán tải giữa các chuyên gia và liên quan nhiều chuyên gia hơn vào nhiệm vụ, nhưng nó không cải thiện nhiệm vụ cuối cùng. Một kỹ thuật cân bằng tải tốt hơn sẽ cần thiết cho nhiệm vụ Q&A mạnh mẽ này.

5 Phân tích
Chúng tôi đã thực hiện đánh giá định tính bằng cách xem xét dự đoán của mô hình đối với các ví dụ ngoài miền và so sánh chúng với các câu trả lời được gắn nhãn tương ứng. Nhìn chung, chúng tôi thấy hệ thống của chúng tôi cung cấp câu trả lời đáng tin cậy và hợp lý cho hầu hết các cặp ngữ cảnh-câu hỏi. Nó đặc biệt tốt trong việc trả lời các câu hỏi dựa trên sự thật, như được hiển thị trong danh sách ví dụ dưới đây.

Đối với những câu hỏi này, có một câu trả lời duy nhất, đặc biệt có thể được tìm thấy trong đoạn văn ngữ cảnh. Mô hình của chúng tôi có thể xác định câu trả lời từ ngữ cảnh, do đó đưa ra các dự đoán khớp chính xác với các câu trả lời mong đợi.

• Bạn có thể tìm thấy Bcl-2 trên nhiễm sắc thể nào?
–Dự đoán: cấy ghép thận
–Câu trả lời: cấy ghép thận
• Người dùng mới của Facebook cần gì để tạo tài khoản?
–Dự đoán: địa chỉ email
–Câu trả lời: địa chỉ email
• Vị trí của dự án là gì?
–Dự đoán: sa mạc Arizona
–Câu trả lời: sa mạc Arizona
• Tên đội của Boris Diaw là gì?
–Dự đoán: San Antonio Spurs
–Câu trả lời: San Antonio Spurs
• Hasumi nhận bằng MBA ở đâu?
–Dự đoán: Đại học Harvard
–Câu trả lời: Đại học Harvard

Cũng có một số ví dụ mà chúng tôi thấy trong đó hoặc nhiều câu trả lời nên được cho phép, nhưng vì chỉ một câu trả lời duy nhất được cung cấp trong nhãn, mô hình của chúng tôi bị phạt một cách sai lầm. Ví dụ, đối với câu hỏi đầu tiên dưới đây, chúng tôi tin rằng giai đoạn thứ ba và giai đoạn rút lui là tương đương, dựa trên ngữ cảnh, nhưng vì chỉ có "giai đoạn rút lui" được cung cấp trong nhãn, dự đoán được coi là 0 EM và 0.5 F1. Tương tự, trong ví dụ thứ hai, cả "sương mù xám" và "khói bụi" đều là cùng một thứ, nhưng dự đoán của mô hình được coi là 0 EM và 0 F1. Đây là giới hạn của tập dữ liệu đánh giá và các chỉ số của chúng tôi.

• Ở giai đoạn nào mọi người sẽ cảm thấy khó chịu nhất?
–Dự đoán: giai đoạn thứ ba
–Câu trả lời: giai đoạn rút lui
• Theo báo cáo tin tức, Bắc Kinh có điểm chung gì với Los Angeles?
–Dự đoán: sương mù xám
–Câu trả lời: khói bụi

Chúng tôi cũng thấy rằng mô hình của chúng tôi không tốt trong việc tóm tắt một đoạn văn hoặc trả lời các câu hỏi phức tạp với nhiều điều kiện. Trong ví dụ đầu tiên dưới đây, câu hỏi bắt đầu bằng "chúng ta biết gì về X" – đây là về việc tóm tắt ý chính từ đoạn văn ngữ cảnh. Mô hình của chúng tôi đưa ra nửa câu có thể là mở rộng "sư tử biển California" và không có ý nghĩa gì. Tương tự, câu hỏi thứ hai là về "A làm gì với B", và dự đoán của mô hình rất xa so với câu trả lời mong đợi. Điều này cho thấy rằng mô hình của chúng tôi không được huấn luyện đủ để trả lời loại câu hỏi này. Chúng tôi xem qua các ví dụ huấn luyện trong miền và thấy rất ít câu hỏi trông như thế này. Điều này giải thích tại sao mô hình của chúng tôi kém trong loại câu hỏi này. Một loại câu hỏi khác mà mô hình thất bại có nhiều điều kiện, như ví dụ thứ ba dưới đây. Câu hỏi yêu cầu địa điểm nơi hai điều kiện cần được đáp ứng. Mô hình của chúng tôi thất bại trong việc cung cấp câu trả lời đúng. Loại câu hỏi này yêu cầu lý luận dựa trên một đoạn văn dài. Đây là một nhiệm vụ khó khăn và mô hình của chúng tôi chưa học tốt để trả lời loại câu hỏi này.

• Chúng ta biết gì về sư tử biển California?
–Dự đoán: là nhanh nhất trong tất cả
–Câu trả lời: Con đực lớn hơn con cái nhiều
• Dong-Jin làm gì với Ryu?
–Dự đoán: phục kích và giết các thương gia nội tạng.
–Câu trả lời: thiết lập một cạm bẫy điện trên núm cửa của anh ta, khiến Ryu bất tỉnh
• Bạn có thể tận hưởng cả giao thông thuận tiện và bãi biển đẹp ở đâu?
–Dự đoán: Thái Lan có rất nhiều thứ để cung cấp, từ Bangkok trung tâm tiệc tùng
–Câu trả lời: Melbourne, Úc
• Nội các Santer-Poos II bắt đầu vào năm nào?
–Dự đoán: 1989 và 13 tháng 7 năm 1994
–Câu trả lời: 1989
• Tại sao Ryu và Dong Jin chờ đợi tại nhà riêng của nhau?
–Dự đoán: nỗ lực giết
–Câu trả lời: Ryu đến nhà riêng của Dong-jin trong nỗ lực giết anh ta

Cuối cùng, chúng tôi thấy ví dụ sau rất thú vị. Câu hỏi chứa lỗi: nó nên là tên anh trai thay vì tên các anh trai. Mô hình của chúng tôi có thể bối rối về lỗi này, vì vậy nó trả về hai tên tương tự, Constantine II và Constantius II, trong khi câu trả lời mong đợi là "Constantius II". Nó cho thấy rằng mô hình của chúng tôi chú ý rất nhiều đến các từ trong câu hỏi và có thể bối rối nếu câu hỏi chứa lỗi chính tả hoặc lỗi ngữ pháp.

• Tên anh trai của Constans là gì?
–Dự đoán: Constantine II và Constantius II
–Câu trả lời: Constantius II

6 Kết luận
Để kết luận, sự kết hợp giữa kiến trúc MoE tốt nhất và tăng cường dữ liệu của chúng tôi đạt được điểm F1 53.477, đây là mức tăng hiệu suất 9.52%. Trên tập kiểm tra cuối cùng, chúng tôi báo cáo F1 59.506 và EM 41.651 cao hơn. Chúng tôi đã thành công chứng minh tính hiệu quả của kiến trúc Mixture-of-Expert trong nhiệm vụ QA Mạnh mẽ. Dựa trên phân tích định tính, chúng tôi thấy mô hình của chúng tôi rất đáng tin cậy và chính xác trong việc trả lời các câu hỏi dựa trên sự thật mà câu trả lời có thể được tìm thấy từ đoạn văn ngữ cảnh; nó thất bại trong các câu hỏi yêu cầu lý luận hoặc tóm tắt các đoạn văn dài.

Một giới hạn của công việc chúng tôi là chúng tôi không có thời gian để điều tra sự mất cân bằng về tầm quan trọng của các chuyên gia trong các mô hình. Các hướng cho công việc tương lai có thể bao gồm phân tích chuyên môn được thu thập của mỗi chuyên gia, điều chỉnh thêm các hàm mất mát để huấn luyện các chuyên gia tốt hơn, và các cơ chế định tuyến khác nhau của hàm cổng (ví dụ, phân loại tập dữ liệu có thể là một cách đơn giản để hướng dữ liệu đầu vào đến các chuyên gia chuyên biệt trên mỗi tập dữ liệu).

Tài liệu tham khảo
[1] Victor Sanh, Lysandre Debut, Julien Chaumond, và Thomas Wolf. Distilbert, một phiên bản cất cánh của bert: nhỏ hơn, nhanh hơn, rẻ hơn và nhẹ hơn, 2020.
[2] William Fedus, Barret Zoph, và Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.

--- TRANG 7 ---
Trí tuệ nhân tạo Stanford

[3] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, và Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019.

[4] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, và Kaheer Suleman. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 191–200, Vancouver, Canada, August 2017. Association for Computational Linguistics.

[5] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. Squad: 100,000+ questions for machine comprehension of text, 2016.

[6] Omer Levy, Minjoon Seo, Eunsol Choi, và Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333–342, Vancouver, Canada, August 2017. Association for Computational Linguistics.

[7] Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, và Karthik Sankaranarayanan. Duorc: Towards complex language understanding with paraphrased reading comprehension, 2018.

[8] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, và Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations, 2017.

[9] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, và Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.

[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998–6008, 2017.

[11] Jason Wei và Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text classification tasks, 2019.

[12] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, và Quoc V. Le. Qanet: Combining local convolution with global self-attention for reading comprehension, 2018.

A Phụ lục
A.1 Khám phá dịch ngược
Sử dụng Google API, chúng tôi nhận ra rằng các ngôn ngữ gần với tiếng Anh hơn làm ngôn ngữ trung gian dẫn đến sự ổn định hơn trong dịch thuật.

Ví dụ, chuỗi từ dữ liệu huấn luyện "EETdE BTdB $28,530,608 EETdE BTdB Memphis Grizzlies EETdE EETrE BTrB BTdB James Harden EETdE BTdB $28,299,399 EETdE BTdB Houston Rockets EETdE EETrE BTrB BTdB DeMar DeRozan EETdE BTdB $27,739,975 EETdE BTdB Toronto Raptors EETdE EETrE EETableE" thất bại trong việc được dịch qua tiếng Trung do các từ không thông thường không thể được khớp trong tiếng Trung.

Khi được chia thành các cụm từ nhỏ hơn như "EETdE BTdB $28,530,608 EETdE BTdB " nó có thể được dịch qua tiếng Trung.

Tuy nhiên, chuỗi gốc có thể được dịch qua các ngôn ngữ gần với tiếng Anh hơn, như tiếng Tây Ban Nha hoặc tiếng Pháp.
