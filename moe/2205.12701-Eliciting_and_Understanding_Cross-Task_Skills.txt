# 2205.12701.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2205.12701.pdf
# File size: 885978 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Eliciting and Understanding Cross-Task Skills
with Task-Level Mixture-of-Experts
Qinyuan Ye Juan Zha Xiang Ren
University of Southern California
{qinyuany, juanzha, xiangren}@usc.edu
Abstract
Recent works suggest that transformer mod-
els are capable of multi-tasking on diverse
NLP tasks and adapting to new tasks efﬁ-
ciently. However, the potential of these multi-
task models may be limited as they use the
same set of parameters for alltasks. In con-
trast, humans tackle tasks in a more ﬂexible
way, by making proper presumptions on what
skills and knowledge are relevant and execut-
ing only the necessary computations. Inspired
by this, we propose to use task-level mixture-
of-expert models, which has a collection of
transformer layers ( i.e., experts) and a router
component that chooses from these experts dy-
namically and ﬂexibly. We ﬁnd that these mod-
els help improve the average performance gain
(ARG) metric by 2.6% when adapting to un-
seen tasks in the few-shot setting and by 5.6%
in the zero-shot generalization setting. Fur-
ther, we show that the learned routing deci-
sions partly rediscover human categorization
of NLP tasks – certain experts are strongly as-
sociated with extractive tasks, some with clas-
siﬁcation tasks, and some with tasks requiring
world knowledge.1
1 Introduction
Pre-trained transformer models (Devlin et al., 2019;
Liu et al., 2019b) have demonstrated remarkable
capabilities in natural language processing (NLP)
in recent years. Moreover, generative transform-
ers can be viewed as a universal model that can
be optimized for any language task primed into
text-to-text format (Raffel et al., 2020). Recently,
researchers found that training these transformer
models to multi-task on a diverse collection of NLP
tasks is beneﬁcial – not only are they better at han-
dling seen tasks (Aghajanyan et al., 2021; Aribandi
et al., 2022), but also at generalizing and adapting
to unseen tasks (Wei et al., 2021; Sanh et al., 2022).
1Our code will be released at https://github.com/
INK-USC/CrossTaskMoE .
Expert 
1.1
Task 
A
IMDB
Task 
B
QQP
Does 
this 
help 
multi-task 
learning? 
(§5)
Does 
this 
improve 
generalization 
to 
new 
tasks? 
(§6)
What 
is 
learned 
by 
each 
expert? 
(§7)
Expert 
1.2
Expert 
1.3
Task 
C
CommonsenseQA
Expert 
2.1
Expert 
2.2
Expert 
2.3
Expert 
3.1
Expert 
3.2
Expert 
3.3
Expert 
4.1
Expert 
4.2
Expert 
4.3
Task-Level 
Mixture-of-ExpertsFigure 1: Illustration of Task-level Mixture-of-
Expert Models. In this work, we train such models
to multi-task on diverse NLP tasks, aiming at modeling
skill sharing explicitly and understanding the learned
patterns.
However, little is known about how multi-
tasking capabilities and cross-task generalization
is achieved, especially that the exact same set of
weights is applied, and the same computation is
executed, for very different tasks. Humans, on
the other hand, do not exhaust their brain capac-
ity for every task at hand. Humans develop skill
sets and accumulate knowledge during learning,
and can reuse and recompose them when facing a
task. Inspired by this, we hypothesize that a model
that explicitly emulate skill and knowledge shar-
ing may help improve multi-task performance and
generalization to new tasks. A natural ﬁt for this
goal would be task-level mixture-of-expert models
(Jacobs et al., 1991; Kudugunta et al., 2021), where
the model computation is conditioned on the task at
hand. More speciﬁcally, the model contains a col-
lection of experts and a router that chooses from the
experts and composes the ﬁnal model (Fig. 1-2).
In this paper, we ﬁrst empirically investigate sev-
eral key design choices for effectively training task-
level mixture-of-experts models (§5). We further
test the model’s task-level generalization capabili-arXiv:2205.12701v2  [cs.CL]  22 Nov 2022

--- PAGE 2 ---
ties by testing it on unseen tasks (§6). Compared
to a multi-task BART-Base (Lewis et al., 2020)
baseline, our ﬁnal method leads to an 2.6% im-
provement in the average performance gain (ARG)
metric when adapting to 18 unseen tasks (Ye et al.,
2021) in the few-shot learning setting. Further, a
gain of 5.6% in ARG is obtained in the zero-shot
setting with P3 dataset (Sanh et al., 2022). Lastly,
we conduct a detailed analysis quantifying the cor-
relations between the learned routes and the charac-
teristics of tasks (§7). We ﬁnd that the routing de-
cisions, though learned purely from multi-tasking
without prior knowledge, strongly correlate with
human understanding of task characteristics, such
as the task being a classiﬁcation task, the task being
extractive, or the task requiring world knowledge.
2 Related Work
Massive Multi-task Learning. Multi-task learn-
ing (Caruana, 1997) has been continuously ex-
plored in NLP and is shown to be beneﬁcial (Mc-
Cann et al., 2018; Liu et al., 2019a). Recently,
multi-task learning in NLP is brought to a new scale
by using a signiﬁcantly larger collection of tasks
and examples (Aghajanyan et al., 2021; Aribandi
et al., 2022; Khashabi et al., 2020; Hendrycks et al.,
2021). These work demonstrate that multi-task
learning improves the learning of text representa-
tion and thus boost the performance of seen tasks.
Moreover, these models also exhibit strong adapt-
ability to unseen tasks, in both few-shot (Ye et al.,
2021) and zero-shot settings (Wei et al., 2021; Sanh
et al., 2022; Mishra et al., 2021). Despite their ef-
fectiveness in terms of performance, how a model
learns and spontaneously develops language skills
during multi-task learning is a relatively under-
explored topic. In our work, we try to investigate
this question by training task-level MoE models
and interpreting them. We additionally discuss con-
temporary works (Ponti et al., 2022; Gupta et al.,
2022; Asai et al., 2022) in Appendix D.
Mixture-of-Experts in NLP. Mixture-of-
experts models (Jacobs et al., 1991) divide the
problem space into several sub-spaces and allow
experts to be specialized in each subspace. Re-
cently this concept is successfully applied to NLP
(Shazeer et al., 2017), enabling models of billion
or even trillion parameter scale (Fedus et al., 2021;
Du et al., 2021; Artetxe et al., 2021; Zoph et al.,
2022). However these applications mainly focus on
thescaling aspects. Besides, most of them selectexperts on a per-example or per-token basis. In this
work we are interested in multi-task learning with
per-task gating decisions (Rosenbaum et al., 2018;
Kudugunta et al., 2021), and mainly focus on
understanding and interpreting task transferability.
Task Transferability in NLP. Phang et al.
(2018) explored supplementary training on inter-
mediate tasks (STILT), i.e., training on a data-rich
intermediate task before ﬁne-tuning on the target
task. STILT improves performance on the target
task and stabilizes the ﬁne-tuning process. Pruk-
sachatkun et al. (2020) and Vu et al. (2020) further
investigated when and why intermediate task trans-
fer works. These studies mainly focus on transfer-
ability between speciﬁc source-target pairs , while
we consider a more general setting of transferring
within and beyond a group of NLP tasks.
3 Problem Setting
Our goal is to better understand multi-task learning
with mixture-of-experts models with an explicit
routing mechanism. We also hypothesize that such
models help improve the model’s capability to gen-
eralize/adapt to new tasks. Our problem setting
closely resembles CrossFit (Ye et al., 2021). In the
following, we introduce data usage (§3.1), training
procedure (§3.2), and evaluation protocol (§3.3).
3.1 Data Usage
Assume that we have a collection of diverse NLP
tasksT, partitioned into two non-overlapping
sets (Ttrain;Ttest). These sets are also referred
to as (Meta-Train, Meta-Test). Ttrain is mainly
used for multi-task learning; Ttestis used to
quantify the model’s adaptability to new tasks.
Each taskT2 T has three subsets, i.e., T=
(Dtrain;Ddev;Dtest). Additionally, we assume
that all tasks are cast to a uniﬁed text-to-text for-
mat, i.e.,D=f(x;y)g, wherexis the input text
sequence, and yis the output text sequence.
3.2 Training Procedure
The training procedure has two stages: (1) an up-
stream learning stage for multi-task learning on
Ttrain, to develop the skills that are needed to solve
different tasks; and (2) a downstream ﬁne-tuning
stage onTtest, for evaluating the model’s ability to
adapt to new tasks. During the upstream learning
stage, the model is expected to be trained for multi-
task learning with the Dtrain from tasks inTtrain.

--- PAGE 3 ---
Ddevfor tasks inTtrain will be used for hyperpa-
rameter tuning and model selection. During the
downstream ﬁne-tuning stage, the model will be
ﬁne-tuned on each task in Ttestrespectively. Dtrain
will be used for ﬁne-tuning, Ddevfor validation,
andDtestfor reporting the ﬁnal performance.
3.3 Evaluation Protocol
Each task inThas a pre-deﬁned evaluation metric.
For example, F1 score for classiﬁcation tasks, and
accuracy for multi-choice QA tasks. During the up-
stream learning stage, for simplicity, the model is
validated on the averageDdevperformance on all
tasks inTtrain, and we report averageDdevperfor-
mance andDtestperformance. During the down-
stream ﬁne-tuning stage, we compare the model’s
performance to the baseline of ﬁne-tuning a vanilla
transformer (without upstream learning), and com-
pute the average relative performance gain (ARG)
as our evaluation metric. More details about the
baselines and ARG are deferred to §6.
4 Task-level MoE Transformers
Recall that our goal is to better elicit transferable
skills during multi-task learning, and understand
how those skills contribute the model performance.
For this purpose we develop a mixture-of-experts
variant of text-to-text transformer models, condi-
tioning on task representations. The model con-
tains two major components: (1) a router that se-
lects and decides which experts to use for each
task in each layer, based on its task representation;
(2)a collection of experts that are dynamically
composed into a ﬁnal model based on the router
selection. See Fig. 2 for a detailed illustration.
In the following, we introduce the router and the
experts in more details. Note that we provide a gen-
eral description in this section, and leave speciﬁc
design choices in §5.3 for empirical comparison.
Collection of Experts. In an original implemen-
tation of text-to-text models (Raffel et al., 2020;
Lewis et al., 2020), there are ntransformer layers
stacked and executed sequentially. The ﬁrst n=2
layers are encoder layers and the last n=2layers are
decoder layers. In our variant of transformer mod-
els, we copied each layer for mtimes, resulting in
mnexperts in total. We refer to the j-th expert
in thei-th layer as E(i;j). Note that we assume
that each transformer block is an expert, which is
different from Kudugunta et al. (2021). This is to
make whole model dynamic and conpositional.
Transformer 
Layer
(Expert 
1)
Transformer 
Layer
(Expert 
2)
Transformer 
Layer
(Expert 
3)
Selection 
Function
Router
Task 
Representation
Input 
Hidden 
States
Output 
1
Output 
2
Output 
3
0.3
0.3
0.5
0.2
Output 
Hidden 
States
??
0.5
??
0.2
??
??
Mixture-of-Experts 
Transformer 
Layer
RouterFigure 2: Task-level Mixture-of-experts Trans-
former models used in this study. Right: A router
takes in a task representation and make decisions on
expert selection. Left: the weighted sum of the outputs
from each expert are considered the ﬁnal output for this
layer.
Router. For a given task Tk2T, withkas its
task index, the router ﬁrst takes the task representa-
tion (Tk) from a look-up embedding table ( T). The
router network outputs a matrix L2Rmn, where
Li;jrepresents the logits of using expert E(i;j)in
layeri.Lgoes through a selection function fto
normalize the routing decisions in each layer, re-
sulting in a ﬁnal decision matrix D2Rmn.
Task-level MoE Transformers. We use the deci-
sion matrix Dfrom the router to control the compu-
tation conducted by the experts. More speciﬁcally,
in layeri, given input hidden states h(i)
in, the output
h(i)
outwould be the weighted sum of all experts in
the layer, and the weights are speciﬁed in Di;,i.e.,
h(i)
out=mX
j=1Di;jE(i;j)(h(i)
in) (1)
5 Applying Task-level MoE Models to
Multi-task Learning
In our pilot studies, we found it is non-trivial to
train these mixture-of-experts models properly and
effectively. In this section, we present a detailed
empirical study on baselines and design choices.
We ﬁrst introduce experiment details in §5.1. We
then start with investigating simple baselines such
as random or average routing (§5.2), which will
help navigate our experiments on learning task-
level MoE models. In §5.3 we introduce different
variants we experiment with for learning task-level
MoEs, and we summarize our ﬁndings in §5.4.

--- PAGE 4 ---
5.1 Experiment Details
Data. We previously discussed that a collection
of diverse NLP tasks is required for the purpose of
our study (§3.1). In our experiments, we use the
task collection in CrossFit (Ye et al., 2021), which
contains NLP tasks covering a wide range of task
formats, goals and domains. We use its random
task partition, with 120 tasks in Ttrain and 18 tasks
inTtest. All tasks are converted to a uniﬁed text-
to-text format and sub-sampled to be few-shot2.
Details about the tasks are listed in Appendix E-F.
Model and Its Initialization. We previously in-
troduced the model architecture of task-level MoEs
in §4. In our experiments, the model is instanti-
ated with the pre-trained BART-Base model (Lewis
et al., 2020), a 12-layer encoder-decoder trans-
former model ( n= 12 ). Allmexperts in layer
iare initialized from the i-th layer of the BART-
Base model. Additionally we add a Gaussian noise
with variance of 1e-8 to the weights of each expert
to avoid symmetry. We manually set the number of
experts per layer m= 3to allow sufﬁcient ﬂexibil-
ity while maintain a tractable model size.
Training Details. Deferred in Appendix B.1.
5.2 Investigation on Baselines
Before we experiment with learning routers, we
ﬁrst launch a series of baseline experiments related
to the task-level MoE architecture. The goal is to
get insights to help us better design our ﬁnal model.
We experiment with (1) Vanilla transformer ,
where mixture-of-experts are not involved;
(2) Instance-level random routing, where the
routes are randomly sampled for each instance
during the forward pass; (3) Task-level random
routing, where routes are sampled for each task
once before training; (4) Average routing, where
each experts were assigned the same weight in
Eq. (1), i.e.,Di;j= 1=3. For (2) and (3), we try
random selecting either one or two out of the three
experts in each layer (denoted as “1/3” and “2/3”).
In the case of “2/3”, the output is the average of
the outputs produced by the activated experts.
Findings. Performance of these baseline models
are in top 2 sections in Table 1. We also plot the
dev loss and performance curves during vanilla
baseline training in Fig. 6 in Appendix C.1. We
have the following ﬁndings.
2For classiﬁcation tasks, there are 16 examples per task in
Dtrain ; for non-classiﬁcation tasks, Dtrain has 32 examples.(1)In Fig. 6, we found that dev losses dip in the
early phase of training, then gradually rise. Mean-
while, the dev performance continue to increase.
This is an important lesson learned for compar-
ing different design choices: the simple and faster
heuristic of model selection based on dev loss may
be sub-optimal. We hypothesize this is because the
text generation loss may not align well with the
ﬁnal evaluation metric3.
(2)All random routing methods (except for
“Random Task 2/3”) leads to worsened perfor-
mance compared to vanilla transformer baselines.
This suggests that introducing sparsity and routing
mechanism into transformer models naively can
in fact hurt performance. This may be due to un-
derﬁtting (the number of examples routed to each
expert is reduced) or asynchronism in optimization
(a different collection of experts is activated and
updated at each optimization step).
(3)The observation that Random Task Routing
(2/3) is better than Vanilla and Average Routing
suggests that task interference exists in multi-task
models with fully shared parameters, and allowing
task-speciﬁc computations (as in Random Task 2/3)
can be helpful. The observation that Random Task
2/3 is better than 1/3 suggests that performance is
highly sensitive to the portion of shared vs. task-
speciﬁc parameters. There is a ﬁne line between
MoE mechanism being helpful or being intrusive,
adding difﬁculty to training MoE models.
5.3 Investigation on Design Choices
In the following we describe the key design choices
we compared in training task-level MoEs.
Expert Selection. The selection function f
is responsible for normalizing and discretizing
(if necessary) the logit output of router net-
work into ﬁnal decisions. We consider three
variants: (a) Softmax , the default design in
most MoE models. (b) Gumbel-Softmax (Jang
et al., 2016), which add gumbel-distributed noise
to the logits and promote discrete decisions.
(c) Gumbel-Softmax ST , where ST stands for
straight-through estimator. For (b) and (c), we
apply the temperature annealing mechanism to en-
courage exploration in the beginning of training.
Router Architecture. Router is a key compo-
nent for our MoE model which computes the logits
3This ﬁnding is relevant to Csordás et al. (2021) which
advocates proper validation protocol.

--- PAGE 5 ---
of selecting experts based on input task represen-
tations (see §4). We consider three router architec-
ture with different complexities: (d) MLP , which
contains two dense layers separated by GELU ac-
tivation. (e) Bi-LSTM , which takes the sum of
the task representation and a positional embedding
as input at each time step ( i.e., layer). One linear
layer is used to project the LSTM states to routing
decisions. (f) Transformer (Vaswani et al., 2017),
which takes the same input as Bi-LSTM and ap-
plies one single transformer encoder layer.
Task Representations. Vu et al. (2020) sug-
gest that pre-computed task representations contain
rich information for predicting task transferability.
Here we consider incorporating these task repre-
sentations as the initialization for the look-up em-
bedding table Tin our model (§4). In particular,
we consider: (g) Random , which initialized ev-
ery task representation with a randomly initialized
768d vector. (h) TextEmb , which is produced by
encoding the input text with a pre-trained BART-
Base model and taking the representations of the
last encoder layer. We tried both the average rep-
resentation of all tokens in the sequence (A VG)
and BOS token representation. (i) FT-TextEmb ,
which is mostly identical to (h), despite that the
BART-Base model is ﬁrst ﬁne-tuned on the Dtrain
of the current task. (j) Fisher-TaskEmb (Vu et al.,
2020), which is the diagonal of ﬁsher information
of the trainable parameters in a model. We use
adapter (Houlsby et al., 2019) ﬁne-tuning on Dtrain
and compute the ﬁsher information on these adapter
parameters to avoid expensive computations.
Freezing Task Representations. Since adapt-
ability to unseen task will be considered in later
parts of this study, we further consider between
(k) not freezing and (l) freezing the task repre-
sentations during multi-task learning. We conjec-
ture that the structure of seen task representations
may be changed after multi-task learning, while
the unseen task representations may not reﬂect the
change; hence the freezing variant.
Two-stage Training. In §5.2, we ﬁnd that in-
troducing routing mechanism naively may lead to
worsened performance. Also, average routing is sta-
ble and achieves competitive performance. Based
on these observations, we design a two-stage train-
ing strategy to combine the beneﬁts of both meth-
ods. In the ﬁrst stage, the model jointly learns the
router and the experts. In the second stage, theModel Compute Dev (%) Test (%)
Vanilla Transformers
(1) BART-Base 1x 54.47 0.05 48.930.23
(1) BART-Large - 58.10 0.20 54.060.22
Baselines
(2) Random Inst. Routing (1/3) 1x 47.50 0.20 41.870.76
(2) Random Inst. Routing (2/3) 2x 44.81 1.76 38.481.00
(3) Random Task Routing (1/3) 1x 52.89 0.57 47.270.35
(3) Random Task Routing (2/3) 2x 55.350.23 50.440.29
(4) Average Routing (3/3) 3x 54.61 0.11 50.020.19
Task-level Mixture-of-Experts
(c) + (d) + (g) + (k) + (n) 1x 55.280.12 50.520.38
(c) + (d) + (j) + (k) + (m) 1x 53.07 0.45 48.160.34
(c) + (d) + (j) + (l) + (m) 1x 53.06 0.19 47.640.79
(c) + (d) + (j) + (l) + (n) 1x 55.400.08 50.390.68
Table 1: Performance on baselines and selected mod-
els.Average performance on Ddev/Dtestover tasks in
Ttrain are reported. Average and standard dev. are com-
puted based on runs with three different random seeds.
Model Dev (%) Model Dev (%)
Expert Selection Task Repr. (cont.)
(a) Softmax 40.93 (i) FT-TextEmb-BOS 52.93
(b) Gumbel-Softmax 52.02 (i) FT-TextEmb-A VG 53.29
(c) Gumbel-Softmax ST 53.14 (j) Fisher-TaskEmb 53.51
Router Architecture Freeze Task Repr.
(d) MLP 53.14 (k) Not Freezing 53.51
(e) LSTM 53.55 (l) Freezing 53.37
(f) Transformer 53.13 - -
Task Repr. Two-stage Training
(g) Random 53.14 (m) Use one stage 53.51
(h) TextEmb-BOS 52.51 (n) Use two stages 55.36
(h) TextEmb-A VG 53.30 - -
Table 2: Investigation on Design Choices. By default
the model uses (c) + (d) + (g) + (k) + (m) when com-
paring different choices in each colored section.
experts are re-initialized from BART’s pre-trained
weights, and the routes gradually transforms from
average routing to the learned routes by controlling
the temperature used in the softmax function. As a
result, in the beginning of the training, the temper-
ature is set to be high, so the router is functioning
like average routing; during the training process,
the temperature decreases gradually, and the router
will give more discrete routing decisions.
5.4 Results and Findings
We ﬁrst present the performance of variants men-
tioned above in Table 2. For the best-performing
model variants, we run three times with differ-
ent random seeds to reduce variance in perfor-
mance (Table 1, Bottom). We have the follow-
ing observations. (1) What helps? We found
that the choice of selection function and the
two-stage learning procedure are important for
training task-level MoEs. Gumbel-Softmax with
straight-through estimator achieves the best per-

--- PAGE 6 ---
formance among the three choices.4Two-stage
training helps improve performance by 1.8%.5(2)
What doesn’t help? We did not observe signiﬁ-
cant difference with choices in router architecture
ortask representation initialization . Supposedly,
LSTMs and transformers are able to capture re-
lations more complicated than MLPs, and pre-
computed task representations carry richer informa-
tion about the task than random initialization. This
unexpected observation suggests that the router
struggle to leverage task-level information with
the current training methods and supervision sig-
nals. (3) Comparing with the baselines. Our best
task-level MoE using random initialized task rep-
resentations ( (c)+(d)+(g)+(k)+(n)) can rival
the best baselines in §5.2 (Random Task Routing
2/3), while using half of its computation in a for-
ward pass. With careful design, task-level MoEs
are beneﬁcial for multi-task learning.
6 Generalizing to Unseen Tasks
We hypothesize that task-level MoE models can
recombine the learned skills effectively when they
encounter new tasks. In §6.1 we evaluate the mod-
els obtained in §5 on adapting to new tasks in a
few-shot learning setting. In §6.2 we further ex-
tend our method to a zero-shot learning setting and
test it on the P3 dataset (Sanh et al., 2022).
6.1 Few-shot Adaptation
Compared Methods. We use the following mod-
els as initialization for few-shot ﬁne-tuning on un-
seen tasks ( Ttest).(1) Direct Fine-tuning. For
each unseen task, we ﬁne-tune the off-the-shelf
BART-Base model with its Dtrain.(2) Multi-task
BART. We take the multi-task BART-Base from §5
as initialization and ﬁne-tune the model on Dtrain.
(3) Baseline Routing BART. We re-use the mod-
els using random task routing (1/3, 2/3) and av-
erage routing in §5. (4) Learned Routing BART.
We take the (c)+(d)+(j)+(l)+(n)model from
§5. This models uses ﬁsher information as the task
representation (j)and the representations for seen
tasks are frozen (l)during multi-task learning. For
the unseen task, we ﬁrst compute its ﬁsher infor-
mation based on Dtrain and feed it to the learned
router to select experts. We then ﬁne-tune the se-
lected experts on Dtrain.
4See Appendix C.3 for further investigation.
5We also use heterogeneous batching (Aghajanyan et al.,
2021) and two-speed learning rate (Ponti et al., 2022) in our
model as recommended by these works.Data and Evaluation. We use the 18 unseen
tasks speciﬁed in CrossFit random partition in Ye
et al. (2021)6. We ﬁrst obtain the performance of
ﬁne-tuning the pre-trained BART-Base model as
the baseline. Then we compute and report the av-
erage relative gain (ARG) over pre-trained BART
for the multi-task BART and routing BART meth-
ods. For example, if ﬁne-tuning pre-trained BART
achieves 50% accuracy on task A and 80% F1 on
task B, and ﬁne-tuning multi-task BART achieves
80% accuracy on task A and 60% F1 on task B, the
ARG would be the average of (80% 50%)=50%
and(60% 80%)=80%, which equals to 17:5%.
Results. We present the performance gains on
individual tasks and their average in Fig. 3. Multi-
task BART remains a strong baseline, achieving an
ARG of 9.74%. Random task routing (2/3) and av-
erage routing baselines achieves 10.21% and 8.06%
respectively. Our task-level MoE model (c)+(d)
+(j)+(l)+(n)achieves the best average perfor-
mance gain (12.30%), which is 2.6% higher than
the multi-task BART. We observe that negative
transfers are alleviated and few-shot performance
are improved compared to the baselines for many
tasks. This suggest that our task-level MoE model
is learning reusable experts and meaningful routes.
6.2 Zero-shot Generalization
In this section, we modify our proposed method to
zero-shot learning settings where each unseen task
has no labeled data. We use Public Pool of Prompts
(P3) dataset as our testbed (Sanh et al., 2022).
Data. Following Sanh et al. (2022); Bach et al.
(2022), we use the prompt templates to change
texts from various NLP tasks into a uniﬁed text-
to-text formats. Speciﬁcally, we have 36 upstream
tasks forTtrain, and 10 tasks for Ttest. We use
accuracy as the evaluation metric. We report both
the average performance on Ttest(A VG), and the
average performance gain (ARG) described in §6.1.
Compared Methods. For all the models, we
train on the Dtrain for all tasks inTtrain, and di-
rectly test the model on Dtestfor each task inTtest.
We mainly compare four methods: (1)Multi-task
BART-Base. (2)Random Task Routing (2/3). (3)
We train a new (c)+(d)+(h)+(l)+(m) model
on P3 data. (4)Similar to (3), we train a model with
the conﬁguration of (c)+(d)+(h)+(l)+(n).
6We exclude Free-base QA and Yelp Polarity from the
evaluation as performance is unusually unstable on these tasks.

--- PAGE 7 ---
crawl_domainag_newsai2_arc
amazon_polarity tweet_eval/uni00ADironyglue/uni00ADcola wiki_split
blimp/uni00AD...licensor_presentethos/uni00ADsexual_orient.circa
ethos/uni00ADdisabilityglue/uni00ADqnli
superglue/uni00ADrterace/uni00ADhighquorefhatexplain
blimp/uni00AD..._npi_scopebreak/uni00ADQDMR/uni00AD50%/uni00AD25%0%25%50%75%100%Relative/uni00A0Performance/uni00A0Gain/uni00A0(%)direct/uni00A0fine/uni00ADtuning
multi/uni00ADtask
task/uni00A0random/uni00A0routing/uni00A0(1/3)task/uni00A0random/uni00A0routing/uni00A0(2/3)
avg/uni00A0routing/uni00A0(3/3)
task/uni00A0moe/uni00A0(c)+(d)+(j)+(l)+(n)
average/uni00A0relative /uni00A0gain/uni00A0(ARG)/uni00AD10%/uni00AD5%0%5%10%15%20%Figure 3: Few-shot Performance on Unseen Tasks. Bar heights represent relative performance gain over directly
ﬁne-tuning a pre-trained BART-Base model. The right-most bars are the average performance gain.
Main models anli_r3 HellaSwag cb wic wsc winogrande arc-chan. obqa piqa SQuADv2 A VG ARG
Multi-task BART-Base 27.6 22.0 44.6 43.1 57.5 52.7 23.1 26.2 26.4 14.6 33.7 -
Random Task Routing (2/3) 23.7 14.5 19.3 37.8 45.2 49.0 14.5 18.5 3.5 9.1 23.5 -33.6
(c) + (d) + (h) + (l) + (m) 33.7 20.7 43.6 40.4 50.2 46.8 11.8 18.1 22.4 18.6 32.2 -8.3
(c) + (d) + (h) + (l) + (n) 32.0 23.7 44.3 43.4 56.5 52.2 21.1 28.5 30.2 17.6 34.9 5.6
Table 3: Zero-shot Performance on Unseen Tasks . Accuracy (%) on the test set of 10 unseen tasks. We compare
the A VG and calculate the ARG of routing model (c) + (d) + (h) + (l) + (m) and (c) + (d) + (h) + (l) + (n) over
multi-task BART-Base. The former routing model uses one-stage training while the latter uses two-stage straining.
Note that in the zero-shot setting, we cannot use
pre-computed task representations for unseen tasks
based on labeled examples (as described in §5.3).
Therefore for (h) TextEmb used in (3)and(4),
we encode prompt templates as the auxiliary task
information. More details are in Appendix B.3.
Results. We present the results in Table 3. Our
ﬁndings are: (1)Compared to the multi-task BART-
base baseline with an A VG of 33.7%, our rout-
ing model (4) achieves a higher A VG (34.9%) and
a positive ARG (5.6%). This demonstrates the
model’s improved generalization ability to novel
tasks in the zero-shot setting. (2)The gap between
model (3) and model (4) shows that the two-stage
training strategy is essential in the zero-shot set-
ting as well. (3)Different from the ﬁndings in the
few-shot setting, Random Task Routing (2/3) has
a negative ARG (-33.6%). Without labeled data
in unseen tasks, random routing cannot actively
select relevant experts or update model parameters,
resulting in worsened performance. In contrast,
task-level MoE has the ﬂexibility to select relevant
experts and achieves better performance.
7 Interpreting the Routes and Experts
7.1 Learning Dynamics of the Routes
We visualized the learned routing decisions of
the(c)+(d)+(g)+(k)+(m) model trained on
CrossFit data in Fig. 4. Note that (g)representsthat the task representations are randomly initial-
ized and learned spontaneously during multi-task
learning. We observe that distinct patterns for clas-
siﬁcation and generation tasks emerge in the early
stage of the training (step 3000). These patterns
transition from coarse-grained to ﬁne-grained grad-
ually in the training process. These observations
align with our expectation that task-level MoEs are
learning to share parameters for similar tasks and
avoid interference among dissimilar tasks.
7.2 Correlation with Task Features
To better understand the learned routing decisions,
we investigate the relation between the routing de-
cisions and manually-deﬁned task features. In the
following, we ﬁrst describe the methodology of
computing correlation, then describe the features
we investigate, and ﬁnally describe our ﬁndings.
Method. For each task inTtrain, we ﬁrst com-
pute the routing decisions D2Rmnusing the
learned model. For each expert E(i;j), we consider
the routing decision Di;jof all tasks as a feature.
Altogether, we have mnfeatures of dimension
jTtrainj(the number of tasks). Additionally, we
havetmanually-deﬁned features on all tasks, giv-
ingtfeatures of dimension jTtrainj. We compute
Pearson correlation coefﬁcient between each pair
of learned routing decisions and manual feature,
resulting in a Rmntmatrix quantifying the corre-
lation between mnexperts andtmanual features.

--- PAGE 8 ---
Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11(a) Step 3000Classification Conditional Generation Other QA
Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11(b) Step 6000
Layer0
Layer1
Layer2
Layer3
Layer4
Layer5
Layer6
Layer7
Layer8
Layer9
Layer10
Layer11(c) Step 60000
0.00.20.40.60.81.0
0.00.20.40.60.81.0
0.00.20.40.60.81.0Figure 4: Routing Decisions Learned During Multi-
task Learning ((c) + (d) + (g) + (k) + (m) ). The
router is able to distinguish classiﬁcation tasks from
other types of tasks after 3000 steps of the training. It
then gradually learns more ﬁne-grained patterns.
Feature Name Example Description
Task Format
Extractive SQuAD, Race Output is always a substring of the input
Sentence Completion HellaSwag,
LAMA-ProbesRequires the model to ﬁll in a blank in the in-
put or continue to generate based on the input
Required Skills and Knowledge
Linguistic Blimp, CoLA Tasks focusing on grammatical correctness, se-
mantic equivalence and linguistic phenomenon
Commonsense CommonsenseQA Tasks testing commonsense knowledge and
reasoning capabilities
Co-reference Wino_grande Tasks requiring co-reference resolution
Multi-hop Reasoning DROP Tasks requiring multi-hop/multi-step reason-
ing
Implicit Knowledge TriviaQA Tasks requiring world knowledge (acquired
during pre-training)
Synthesize Break, XSum Combining ideas and allowing an evolving
understanding of text
Table 4: Additional Features on Format, High-level
Skills and Knowledge.
Manual Features. We consider the following
features in our correlation study7. The ﬁnal fea-
ture table (tjTtrainj) is in Table 9.
•Task Format. We use the task categories pro-
vided in Ye et al. (2021). The top-level labels in-
clude Clas sification ,Ques tion Answer ing,
Conditional Generation , and Others. Tasks
in each category are divided into sub-categories.
For example, QA tasks are further categorized
into machine read ingcomprehension (MRC) ,
multiple-choice QA,closed -book QA, etc.
•Input/Output Length. We classify tasks with
into three features based on their average in-
put length: hasShort Input(shortest 25%), has
7We admit that several categorization criteria are subjective
and they are by no means exhaustive for fully describing a
task. We use these features mainly to quantify the relation
between human understanding of tasks and the learned routes.
ClassificationQA
Con. Gen.Other
Long-form QAClosed-book QADialog Gen.
SummarizationhasShortOutputhasMediumOutputhasLongOutputExtractive
World KnowledgeSent. CompletionSynthesizeL0E1
L0E2
L0E3
L1E1
L1E2
L1E3
L2E1
L2E2
L2E3
L3E1
L3E2
L3E3
L4E1
L4E2
L4E3
L5E1
L5E2
L5E3
ClassificationQA
Con. Gen.Other
Long-form QAClosed-book QADialog Gen.
SummarizationhasShortOutputhasMediumOutputhasLongOutputExtractive
World KnowledgeSent. CompletionSynthesizeL6E1
L6E2
L6E3
L7E1
L7E2
L7E3
L8E1
L8E2
L8E3
L9E1
L9E2
L9E3
L10E1
L10E2
L10E3
L11E1
L11E2
L11E3
1.0
0.5
0.00.51.0Figure 5: Pearson Correlation Between Learned
Routes and Selected Manual Features. Correlation
withp<0:01are visualized. “L0E1” stands for expert
1 in layer 0. The correlation is computed based on
a(c) + (d) + (g) + (k) model, where (g) means the
task embedding table Tis randomly initialized. This
suggests that without prior knowledge of the tasks, the
router can partially rediscover human categorization of
tasks during multi-task learning.
Long Input(longest 25%), hasMedi umInput(re-
mainder). We also classify tasks into three fea-
tures based on their average output length: has
Short Output(<3 tokens), hasLongOut put(>
10 tokens), and hasMedi umOut put(remainder).
•Text Domain. We categorize tasks with into do-
mains such as Science &Tech nology,Social
Network ,News ,Web,Bio-Medical,Review ,Di
alog, and Books .
•Granularity. We categorize tasks into Span -
level (e.g., acronym identiﬁcation); Sentence -
level (e.g., tweet classiﬁcation); Para graph -
level (e.g., news summarization) based on their
main focus. This is different from input length.
•Additional Features: Format, High-level
Skills and Knowledge8.We additionally de-
scribe several common task characteristics in
Table 4. These include whether a task is Ex
trac tive , requires Sentence Completion , or
requires high-level skills such as Co-reference .
Findings. Results on selected features are visual-
ized in Fig. 5. Visualization of the complete pairs
of expert and feature are in Fig. 7-8. We have the
following observations: (1)There exists strong cor-
relation between several pairs of routing decisions
and manual features. For example, L1E2, L3E1,
L6E1 are positively correlated with the feature of
Classiﬁcation, suggesting that these experts are
8These features are mostly inspired by dataset papers such
as SQuAD (Rajpurkar et al., 2016), BLiMP (Warstadt et al.,
2020), MNLI (Williams et al., 2018), HotpotQA (Yang et al.,
2018), CommonsenseQA (Talmor et al., 2019).

--- PAGE 9 ---
Manual Feature Top3 Exp Task All Top1 Top3
ClassiﬁcationL1E2
L6E1
L3E1imdb 92.49 91.87 88.70
sms spam 63.54 63.54 62.88
emo 82.06 65.46 16.22
Conditional
GenerationL9E2
L5E3
L7E2gigaword 30.00 26.51 17.91
aeslc 14.52 15.31 14.76
kilt_wow 6.39 6.01 4.73
Closed-book
QAL3E2
L4E2
L6E3kilt_trex 31.85 25.63 28.13
kilt_zsre 13.13 11.25 9.38
numer_sense 34.38 33.75 20.00
Table 5: Performance when top correlated experts
are disabled. “Top1” means the most positively corre-
lated expert is disabled. Performance gradually drops
as more experts are disabled.
Task All Top1 Top3 Rand1 Rand3 Least1 Least3
imdb 92.49 91.87 88.70 92.49 91.66 92.49 92.49
sms spam 63.54 63.54 62.88 63.54 63.53 63.54 63.54
emo 82.06 65.46 16.22 82.06 64.13 82.06 82.06
Table 6: Disabling top/least correlated experts and
random experts. The experts that positively corre-
late (Top1/Top3) with the “classiﬁcation” feature con-
tribute more to the performance than randomly selected
or least correlated experts (Least1/Least3).
likely to be selected for classiﬁcation tasks. (2)The
correlations are strongest with the top-level task
category features ( i.e.,Clas sification ,QA,Con
ditional Generation ), suggesting that the router
may understand and categorize tasks in a way simi-
lar to us. (3)However, correlation does not imply
causal relationships. The correlation patterns of
Clas sification andhasShort Outputare simi-
lar, the same applies to Conditional Generation
andhasLongOut put. We cannot conclude whether
the router is making router decisions depending on
output length, task format, or other hidden aspects.
7.3 Expert Disabling Experiments
We further examine the learned task-level MoE
models by disabling experts during evaluation. By
“disabling”, we simply set the pre-softmax logit to
be 1, so that the second-best expert in that layer
will be selected instead. We hypothesize that if
an expert corresponds to a critical skill required
by a certain type of tasks, then disabling it should
bring signiﬁcant performance drop. (1)We select
three manual features: Clas sification ,Condi
tional Generation ,Closed -book QA, and select
three tasks that belong to these categories. We
select the top 3 experts that positively correlate
with these features, and disable them during eval-
uation. Results are listed in Table 5. As expected,
these correlated experts are indispensable for theTask All}Top1}Top3~Top1~Top3
}imdb 92.49 91.87 88.70 92.49 92.49
}emo 82.06 65.46 16.22 82.06 82.06
~kilt_zsre 13.13 13.13 12.50 11.25 9.38
~numer_sense 34.38 34.38 34.38 33.75 20.00
Table 7: Disabling experts associated with different
task categories. }=Classiﬁcation,~=Closed-book
QA. Performance does not drop signiﬁcantly when ex-
perts relevant to other features are disabled (red area).
task performance. Performance gradually drops as
more experts are disabled (All !Top1!Top3).
(2)For the three classiﬁcation tasks we select, we
further compare the performance when disabling
most/least correlated experts and random experts.
Results are presented in in Table 6. Results suggest
experts that are positively correlated with the clas-
siﬁcation feature are more important to the ﬁnal
performance. (3)We further take two classiﬁcation
tasks (}) and two closed-book QA tasks ( ~), and
consider disabling experts correlated with classiﬁ-
cation and closed-book feature. Results are shown
in Table 7. Performance are not inﬂuenced signiﬁ-
cantly when experts relevant to other features are
disabled. To conclude , this set of experiments sug-
gests that experts that positively correlate with a
speciﬁc type of tasks are irreplaceable; they greatly
contribute to the performance of that type of tasks.
8 Conclusions
Inspired by how humans accumulate skills from
past experience and re-use them to solve new
tasks, in this paper, we develop and conduct exten-
sive experiments with transformer-based task-level
mixture-of-expert (MoE) models, in hope to pro-
vide new insights on multi-task learning and cross-
task generalization in NLP. Firstly, we empirically
investigate importance design choices and quantify
their inﬂuence on ﬁnal model. Secondly, in both
few-shot and zero-shot settings, we demonstrate
that task-level mixture-of-expert models are better
at generalizing to new tasks. Finally, by conducting
a detailed analysis on the routing decisions, we ﬁnd
they have strong correlations with human-deﬁned
task characteristics, even when the decisions are
learned spontaneously without no prior knowledge
such as pre-computed task representations. We
hope our work provide useful advice on training
and interpreting multi-task models in NLP and we
hope it will inspire future work in improving multi-
task learning and cross-task generalization in NLP.

--- PAGE 10 ---
Limitations
Although we have done much analysis on the corre-
lation between learned routes and task characteris-
tics, it is yet challenging to (1) ground each expert
to human-understandable language skills; (2) un-
derstand their causal relationships. Much more
needs to be discussed on how to systematically de-
ﬁne the atomic/basic skills that are used in solving
NLP tasks. In terms of model optimization, we
ﬁnd that we cannot achieve the best performance
using the one-stage training strategy, and our best
method takes more training time and needs more
delicate hyper-parameters selection compared to
the vanilla multi-task model. We hypothesize that
there are optimization challenges in training task-
level mixture-of-expert models. We hope future
work can investigate and address this problem.
Acknowledgments
We thank authors and crowd-workers of all datasets
used in our study. We thank huggingface datasets
team (Lhoest et al., 2021) for making NLP datasets
more accessible. We thank anonymous reviewers,
members of USC INK Lab and USC NLP commu-
nity for their valuable feedback. This work is sup-
ported in part by the Ofﬁce of the Director of Na-
tional Intelligence (ODNI), Intelligence Advanced
Research Projects Activity (IARPA), via Contract
No. 2019-19051600007; the DARPA MCS pro-
gram under Contract No. N660011924033; the
Defense Advanced Research Projects Agency with
award W911NF-19-20271; NSF IIS 2048211.
References
Alessandro Achille, Michael Lam, Rahul Tewari,
Avinash Ravichandran, Subhransu Maji, Charless C.
Fowlkes, Stefano Soatto, and Pietro Perona. 2019.
Task2vec: Task embedding for meta-learning. In
2019 IEEE/CVF International Conference on Com-
puter Vision, ICCV 2019, Seoul, Korea (South), Oc-
tober 27 - November 2, 2019 , pages 6429–6438.
IEEE.
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava,
Xilun Chen, Luke Zettlemoyer, and Sonal Gupta.
2021. Muppet: Massive multi-task representations
with pre-ﬁnetuning. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 5799–5811, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Tiago A. Almeida, José María G. Hidalgo, and Akebo
Yamakami. 2011. Contributions to the study of smsspam ﬁltering: New collection and results. In Pro-
ceedings of the 11th ACM Symposium on Document
Engineering , DocEng ’11, page 259–262, New York,
NY , USA. Association for Computing Machinery.
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019. MathQA: Towards interpretable
math word problem solving with operation-based
formalisms. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 2357–2367, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,
Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Hon-
glei Zhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni,
Jai Gupta, Kai Hui, Sebastian Ruder, and Donald
Metzler. 2022. Ext5: Towards extreme multi-task
scaling for transfer learning. In International Con-
ference on Learning Representations .
Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor
Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,
Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,
Giri Anantharaman, Xian Li, Shuohui Chen, Halil
Akin, Mandeep Baines, Louis Martin, Xing Zhou,
Punit Singh Koura, Brian O’Horo, Jeff Wang, Luke
Zettlemoyer, Mona T. Diab, Zornitsa Kozareva, and
Ves Stoyanov. 2021. Efﬁcient large scale lan-
guage modeling with mixtures of experts. CoRR ,
abs/2112.10684.
Akari Asai, Mohammadreza Salehi, Matthew E Peters,
and Hannaneh Hajishirzi. 2022. Attentional mix-
tures of soft prompt tuning for parameter-efﬁcient
multi-task knowledge sharing. arXiv preprint
arXiv:2205.11961 .
Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert
Webson, Colin Raffel, Nihal V . Nayak, Abheesht
Sharma, Taewoon Kim, M Saiful Bari, Thibault
Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli,
Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gun-
jan Chhablani, Han Wang, Jason Fries, Maged Al-
shaibani, Shanya Sharma, Urmish Thakker, Khalid
Almubarak, Xiangru Tang, Dragomir Radev, Mike
Tian-jian Jiang, and Alexander Rush. 2022. Prompt-
Source: An integrated development environment
and repository for natural language prompts.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising tex-
tual entailment challenge. In Proceedings of the sec-
ond PASCAL challenges workshop on recognising
textual entailment , volume 6, pages 6–4. Venice.
Francesco Barbieri, Jose Camacho-Collados, Luis Es-
pinosa Anke, and Leonardo Neves. 2020. TweetE-
val: Uniﬁed benchmark and comparative evaluation
for tweet classiﬁcation. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2020 ,

--- PAGE 11 ---
pages 1644–1650, Online. Association for Computa-
tional Linguistics.
Max Bartolo, Alastair Roberts, Johannes Welbl, Sebas-
tian Riedel, and Pontus Stenetorp. 2020. Beat the
AI: Investigating adversarial human annotation for
reading comprehension. Transactions of the Associ-
ation for Computational Linguistics , 8:662–678.
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo
Giampiccolo. 2009. The ﬁfth pascal recognizing tex-
tual entailment challenge. In TAC.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1533–1544, Seattle, Wash-
ington, USA. Association for Computational Lin-
guistics.
Chandra Bhagavatula, Ronan Le Bras, Chaitanya
Malaviya, Keisuke Sakaguchi, Ari Holtzman, Han-
nah Rashkin, Doug Downey, Wen tau Yih, and Yejin
Choi. 2020. Abductive commonsense reasoning. In
International Conference on Learning Representa-
tions .
Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng
Gao, and Yejin Choi. 2020. PIQA: reasoning about
physical commonsense in natural language. In The
Thirty-Fourth AAAI Conference on Artiﬁcial Intelli-
gence, AAAI 2020, The Thirty-Second Innovative Ap-
plications of Artiﬁcial Intelligence Conference, IAAI
2020, The Tenth AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020 , pages 7432–
7439. AAAI Press.
Michael Boratko, Xiang Li, Tim O’Gorman, Rajarshi
Das, Dan Le, and Andrew McCallum. 2020. Pro-
toQA: A question answering dataset for prototypi-
cal common-sense reasoning. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 1122–1136,
Online. Association for Computational Linguistics.
Jan A. Botha, Manaal Faruqui, John Alex, Jason
Baldridge, and Dipanjan Das. 2018. Learning to
split and rephrase from Wikipedia edit history. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing , pages
732–737, Brussels, Belgium. Association for Com-
putational Linguistics.
Rich Caruana. 1997. Multitask learning. Machine
learning , 28(1):41–75.
Ankush Chatterjee, Kedhar Nath Narahari, Meghana
Joshi, and Puneet Agrawal. 2019. SemEval-2019
task 3: EmoContext contextual emotion detection in
text. In Proceedings of the 13th International Work-
shop on Semantic Evaluation , pages 39–48, Min-
neapolis, Minnesota, USA. Association for Compu-
tational Linguistics.Anthony Chen, Gabriel Stanovsky, Sameer Singh, and
Matt Gardner. 2020a. MOCHA: A dataset for train-
ing and evaluating generative reading comprehen-
sion metrics. In Proceedings of the 2020 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 6521–6532, Online. Associa-
tion for Computational Linguistics.
Michael Chen, Mike D’Arcy, Alisa Liu, Jared Fer-
nandez, and Doug Downey. 2019. CODAH: An
adversarially-authored question answering dataset
for common sense. In Proceedings of the 3rd Work-
shop on Evaluating Vector Space Representations
for NLP , pages 63–69, Minneapolis, USA. Associ-
ation for Computational Linguistics.
Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai
Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and
William Yang Wang. 2020b. Tabfact: A large-scale
dataset for table-based fact veriﬁcation. In 8th Inter-
national Conference on Learning Representations,
ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020 . OpenReview.net.
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. BoolQ: Exploring the surprising
difﬁculty of natural yes/no questions. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers) , pages 2924–2936, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. 2018. Think you have solved question an-
swering? try arc, the ai2 reasoning challenge. ArXiv
preprint , abs/1803.05457.
Arman Cohan, Waleed Ammar, Madeleine van Zuylen,
and Field Cady. 2019. Structural scaffolds for ci-
tation intent classiﬁcation in scientiﬁc publications.
InProceedings of the 2019 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technolo-
gies, Volume 1 (Long and Short Papers) , pages
3586–3596, Minneapolis, Minnesota. Association
for Computational Linguistics.
Róbert Csordás, Kazuki Irie, and Juergen Schmidhu-
ber. 2021. The devil is in the detail: Simple tricks
improve systematic generalization of transformers.
InProceedings of the 2021 Conference on Empiri-
cal Methods in Natural Language Processing , pages
619–634, Online and Punta Cana, Dominican Re-
public. Association for Computational Linguistics.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In Machine Learning Challenges Work-
shop , pages 177–190. Springer.
Pradeep Dasigi, Nelson F. Liu, Ana Marasovi ´c,
Noah A. Smith, and Matt Gardner. 2019. Quoref:

--- PAGE 12 ---
A reading comprehension dataset with questions re-
quiring coreferential reasoning. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 5925–5932, Hong Kong,
China. Association for Computational Linguistics.
Thomas Davidson, Dana Warmsley, Michael Macy,
and Ingmar Weber. 2017. Automated hate speech
detection and the problem of offensive language. In
Proceedings of the 11th International AAAI Confer-
ence on Web and Social Media , ICWSM ’17, pages
512–515.
Ona de Gibert, Naiara Perez, Aitor García-Pablos, and
Montse Cuadros. 2018. Hate speech dataset from
a white supremacy forum. In Proceedings of the
2nd Workshop on Abusive Language Online (ALW2) ,
pages 11–20, Brussels, Belgium. Association for
Computational Linguistics.
Marie-Catherine de Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
Proceedings of Sinn und Bedeutung , 23(2):107–124.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
T. Diggelmann, Jordan L. Boyd-Graber, Jannis Bu-
lian, Massimiliano Ciaramita, and Markus Leip-
pold. 2020. Climate-fever: A dataset for veriﬁca-
tion of real-world climate claims. ArXiv preprint ,
abs/2012.00614.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2019. Wizard
of wikipedia: Knowledge-powered conversational
agents. In 7th International Conference on Learn-
ing Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019 . OpenReview.net.
William B. Dolan and Chris Brockett. 2005. Automati-
cally constructing a corpus of sentential paraphrases.
InProceedings of the Third International Workshop
on Paraphrasing (IWP2005) .
Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong,
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, Bar-
ret Zoph, Liam Fedus, Maarten Bosma, Zongwei
Zhou, Tao Wang, Yu Emma Wang, Kellie Webster,
Marie Pellat, Kevin Robinson, Kathleen S. Meier-
Hellstern, Toju Duke, Lucas Dixon, Kun Zhang,
Quoc V . Le, Yonghui Wu, Z. Chen, and Claire Cui.
2021. Glam: Efﬁcient scaling of language models
with mixture-of-experts. ArXiv , abs/2112.06905.Matthew Dunn, Levent Sagun, Mike Higgins, V . U.
Güney, V olkan Cirik, and Kyunghyun Cho. 2017.
Searchqa: A new q&a dataset augmented with
context from a search engine. ArXiv preprint ,
abs/1704.05179.
Ondˇrej Dušek, David M. Howcroft, and Verena Rieser.
2019. Semantic noise matters for neural natural lan-
guage generation. In Proceedings of the 12th Inter-
national Conference on Natural Language Genera-
tion, pages 421–426, Tokyo, Japan. Association for
Computational Linguistics.
Ondˇrej Dušek, Jekaterina Novikova, and Verena Rieser.
2020. Evaluating the State-of-the-Art of End-to-End
Natural Language Generation: The E2E NLG Chal-
lenge. Computer Speech & Language , 59:123–156.
Hady Elsahar, Pavlos V ougiouklis, Arslen Remaci,
Christophe Gravier, Jonathon Hare, Frederique
Laforest, and Elena Simperl. 2018. T-REx: A large
scale alignment of natural language with knowledge
base triples. In Proceedings of the Eleventh Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2018) , Miyazaki, Japan. European
Language Resources Association (ELRA).
Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and
Dragomir Radev. 2019. Multi-news: A large-scale
multi-document summarization dataset and abstrac-
tive hierarchical model. In Proceedings of the 57th
Annual Meeting of the Association for Computa-
tional Linguistics , pages 1074–1084, Florence, Italy.
Association for Computational Linguistics.
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
Long form question answering. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics , pages 3558–3567, Florence,
Italy. Association for Computational Linguistics.
Manaal Faruqui and Dipanjan Das. 2018. Identifying
well-formed natural language questions. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing , pages 798–803,
Brussels, Belgium. Association for Computational
Linguistics.
William Fedus, Barret Zoph, and Noam Shazeer. 2021.
Switch transformers: Scaling to trillion parameter
models with simple and efﬁcient sparsity. arXiv
preprint arXiv:2101.03961 .
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing , pages 1–9, Prague. Association
for Computational Linguistics.
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and
Aleksander Wawer. 2019. SAMSum corpus: A
human-annotated dialogue dataset for abstractive
summarization. In Proceedings of the 2nd Workshop

--- PAGE 13 ---
on New Frontiers in Summarization , pages 70–79,
Hong Kong, China. Association for Computational
Linguistics.
Andrew Gordon, Zornitsa Kozareva, and Melissa
Roemmele. 2012. SemEval-2012 task 7: Choice
of plausible alternatives: An evaluation of common-
sense causal reasoning. In *SEM 2012: The First
Joint Conference on Lexical and Computational Se-
mantics – Volume 1: Proceedings of the main con-
ference and the shared task, and Volume 2: Pro-
ceedings of the Sixth International Workshop on Se-
mantic Evaluation (SemEval 2012) , pages 394–398,
Montréal, Canada. Association for Computational
Linguistics.
Shashank Gupta, Subhabrata Mukherjee, Krishan Sub-
udhi, Eduardo Gonzalez, Damien Jose, Ahmed H
Awadallah, and Jianfeng Gao. 2022. Sparsely acti-
vated mixture-of-experts are robust multi-task learn-
ers.arXiv preprint arXiv:2204.07689 .
Harsha Gurulingappa, Abdul Mateen Rajput, Angus
Roberts, Juliane Fluck, Martin Hofmann-Apitius,
and Luca Toldo. 2012. Development of a benchmark
corpus to support the automatic extraction of drug-
related adverse effects from medical case reports.
Journal of Biomedical Informatics , 45(5):885–892.
Text Mining and Natural Language Processing in
Pharmacogenomics.
Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.
Question-answer driven semantic role labeling: Us-
ing natural language to annotate natural language.
InProceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing , pages
643–653, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. 2021. Measuring massive multitask language
understanding. Proceedings of the International
Conference on Learning Representations (ICLR) .
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bor-
dino, Hagen Fürstenau, Manfred Pinkal, Marc Span-
iol, Bilyana Taneva, Stefan Thater, and Gerhard
Weikum. 2011. Robust disambiguation of named en-
tities in text. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Process-
ing, pages 782–792, Edinburgh, Scotland, UK. Asso-
ciation for Computational Linguistics.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin de Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly.
2019. Parameter-efﬁcient transfer learning for nlp.
InICML .
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-
Yew Lin, and Deepak Ravichandran. 2001. Toward
semantics-based answer pinpointing. In Proceed-
ings of the First International Conference on Human
Language Technology Research .Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and
Yejin Choi. 2019. Cosmos QA: Machine reading
comprehension with contextual commonsense rea-
soning. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
2391–2401, Hong Kong, China. Association for
Computational Linguistics.
R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hin-
ton. 1991. Adaptive mixtures of local experts. Neu-
ral Computation , 3:79–87.
Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categor-
ical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144 .
Chao Jiang, Mounica Maddela, Wuwei Lan, Yang
Zhong, and Wei Xu. 2020. Neural CRF model for
sentence alignment in text simpliﬁcation. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 7943–
7960, Online. Association for Computational Lin-
guistics.
Kelvin Jiang, Dekun Wu, and Hui Jiang. 2019. Free-
baseQA: A new factoid QA data set matching trivia-
style question-answer pairs with Freebase. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers) , pages 318–323,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. 2018. Looking be-
yond the surface: A challenge set for reading com-
prehension over multiple sentences. In Proceedings
of the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Pa-
pers) , pages 252–262, New Orleans, Louisiana. As-
sociation for Computational Linguistics.
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish
Sabharwal, Oyvind Tafjord, Peter Clark, and Han-
naneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-
mat boundaries with a single QA system. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2020 , pages 1896–1907, Online. As-
sociation for Computational Linguistics.
Tushar Khot, Peter Clark, Michal Guerquin, Peter
Jansen, and Ashish Sabharwal. 2020. Qasc: A
dataset for question answering via sentence compo-
sition. Proceedings of the AAAI Conference on Arti-
ﬁcial Intelligence , 34(05):8082–8090.
Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018.
Scitail: A textual entailment dataset from science
question answering. In Proceedings of the Thirty-
Second AAAI Conference on Artiﬁcial Intelligence,

--- PAGE 14 ---
(AAAI-18), the 30th innovative Applications of Arti-
ﬁcial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artiﬁcial Intel-
ligence (EAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018 , pages 5189–5197. AAAI Press.
Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim.
2019. Abstractive summarization of Reddit posts
with multi-level memory networks. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers) , pages 2519–2531, Min-
neapolis, Minnesota. Association for Computational
Linguistics.
Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .
Neema Kotonya and Francesca Toni. 2020. Ex-
plainable automated fact-checking for public health
claims. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 7740–7754, Online. Associa-
tion for Computational Linguistics.
Sneha Kudugunta, Yanping Huang, Ankur Bapna,
Maxim Krikun, Dmitry Lepikhin, Minh-Thang Lu-
ong, and Orhan Firat. 2021. Beyond distillation:
Task-level mixture-of-experts for efﬁcient inference.
InFindings of the Association for Computational
Linguistics: EMNLP 2021, Virtual Event / Punta
Cana, Dominican Republic, 16-20 November, 2021 ,
pages 3577–3599. Association for Computational
Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: A benchmark for question an-
swering research. Transactions of the Association
for Computational Linguistics , 7:452–466.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. 2017. RACE: Large-scale ReAd-
ing comprehension dataset from examinations. In
Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing , pages
785–794, Copenhagen, Denmark. Association for
Computational Linguistics.
Rémi Lebret, David Grangier, and Michael Auli. 2016.
Neural text generation from structured data with
application to the biography domain. In Proceed-
ings of the 2016 Conference on Empirical Methods
in Natural Language Processing , pages 1203–1213,
Austin, Texas. Association for Computational Lin-
guistics.
Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,
D. Kontokostas, Pablo N. Mendes, Sebastian Hell-
mann, M. Morsey, Patrick van Kleef, S. Auer, andC. Bizer. 2015. Dbpedia - a large-scale, multilingual
knowledge base extracted from wikipedia. Semantic
Web, 6:167–195.
Hector J. Levesque, Ernest Davis, and Leora Morgen-
stern. 2012. The winograd schema challenge. In
Proceedings of the Thirteenth International Confer-
ence on Principles of Knowledge Representation
and Reasoning , KR’12, page 552–561. AAAI Press.
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke
Zettlemoyer. 2017. Zero-shot relation extraction via
reading comprehension. In Proceedings of the 21st
Conference on Computational Natural Language
Learning (CoNLL 2017) , pages 333–342, Vancou-
ver, Canada. Association for Computational Linguis-
tics.
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 7871–7880, Online. Association
for Computational Linguistics.
Quentin Lhoest, Albert Villanova del Moral, Yacine
Jernite, Abhishek Thakur, Patrick von Platen, Suraj
Patil, Julien Chaumond, Mariama Drame, Julien Plu,
Lewis Tunstall, Joe Davison, Mario Šaško, Gun-
jan Chhablani, Bhavitvya Malik, Simon Brandeis,
Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas
Patry, Angelina McMillan-Major, Philipp Schmid,
Sylvain Gugger, Clément Delangue, Théo Matus-
sière, Lysandre Debut, Stas Bekman, Pierric Cis-
tac, Thibault Goehringer, Victor Mustar, François
Lagunas, Alexander Rush, and Thomas Wolf. 2021.
Datasets: A community library for natural language
processing. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing: System Demonstrations , pages 175–184, On-
line and Punta Cana, Dominican Republic. Associ-
ation for Computational Linguistics.
Xin Li and Dan Roth. 2002. Learning question clas-
siﬁers. In COLING 2002: The 19th International
Conference on Computational Linguistics .
Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and
Xiang Ren. 2020a. Birds have four legs?!
NumerSense: Probing Numerical Commonsense
Knowledge of Pre-Trained Language Models. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 6862–6868, Online. Association for Computa-
tional Linguistics.
Bill Yuchen Lin, Kangmin Tan, Chris Miller, Bei-
wen Tian, and Xiang Ren. 2022. Unsupervised
cross-task generalization via retrieval augmentation.
arXiv preprint arXiv:2204.07937 .
Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei
Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang

--- PAGE 15 ---
Ren. 2020b. CommonGen: A constrained text gen-
eration challenge for generative commonsense rea-
soning. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020 , pages 1823–1840,
Online. Association for Computational Linguistics.
Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-
ner. 2019. Reasoning over paragraph effects in situ-
ations. In Proceedings of the 2nd Workshop on Ma-
chine Reading for Question Answering , pages 58–
62, Hong Kong, China. Association for Computa-
tional Linguistics.
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 158–167, Vancou-
ver, Canada. Association for Computational Linguis-
tics.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-
feng Gao. 2019a. Multi-task deep neural networks
for natural language understanding. In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 4487–4496, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
Roberta: A robustly optimized bert pretraining ap-
proach. arXiv preprint arXiv:1907.11692 .
Annie Louis, Dan Roth, and Filip Radlinski. 2020. “I’d
rather just go to bed”: Understanding indirect an-
swers. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 7411–7425, Online. Associa-
tion for Computational Linguistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y . Ng, and Christopher Potts.
2011. Learning word vectors for sentiment analy-
sis. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies , pages 142–150, Port-
land, Oregon, USA. Association for Computational
Linguistics.
Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wal-
lenius, and Pyry Takala. 2014. Good debt or bad
debt: Detecting semantic orientations in economic
texts. J. Assoc. Inf. Sci. Technol. , 65(4):782–796.
Irene Manotas, Ngoc Phuoc An V o, and Vadim Sheinin.
2020. LiMiT: The literal motion in text dataset. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2020 , pages 991–1000, Online.
Association for Computational Linguistics.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zampar-elli. 2014. A SICK cure for the evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the Ninth International Conference on Lan-
guage Resources and Evaluation (LREC’14) , pages
216–223, Reykjavik, Iceland. European Language
Resources Association (ELRA).
Binny Mathew, Punyajoy Saha, Seid Muhie Yi-
mam, Chris Biemann, Pawan Goyal, and Ani-
mesh Mukherjee. 2020. Hatexplain: A benchmark
dataset for explainable hate speech detection. ArXiv
preprint , abs/2012.10289.
Julian J. McAuley and Jure Leskovec. 2013. Hidden
factors and hidden topics: understanding rating di-
mensions with review text. In Seventh ACM Confer-
ence on Recommender Systems, RecSys ’13, Hong
Kong, China, October 12-16, 2013 , pages 165–172.
ACM.
Bryan McCann, Nitish Shirish Keskar, Caiming Xiong,
and Richard Socher. 2018. The natural language de-
cathlon: Multitask learning as question answering.
arXiv preprint arXiv:1806.08730 .
Clara H. McCreery, Namit Katariya, Anitha Kannan,
Manish Chablani, and Xavier Amatriain. 2020. Ef-
fective transfer learning for identifying similar ques-
tions: Matching user questions to COVID-19 faqs.
InKDD ’20: The 26th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining, Virtual
Event, CA, USA, August 23-27, 2020 , pages 3458–
3465. ACM.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
Sabharwal. 2018. Can a suit of armor conduct elec-
tricity? a new dataset for open book question an-
swering. In Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing ,
pages 2381–2391, Brussels, Belgium. Association
for Computational Linguistics.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2021. Cross-task generaliza-
tion via natural language crowdsourcing instructions.
arXiv preprint arXiv:2104.08773 .
Ioannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,
and Grigorios Tsoumakas. 2020. Ethos: an on-
line hate speech detection dataset. ArXiv preprint ,
abs/2006.08328.
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
Ça˘glar Gu lçehre, and Bing Xiang. 2016. Abstrac-
tive text summarization using sequence-to-sequence
RNNs and beyond. In Proceedings of The 20th
SIGNLL Conference on Computational Natural Lan-
guage Learning , pages 280–290, Berlin, Germany.
Association for Computational Linguistics.
Nikita Nangia, Clara Vania, Rasika Bhalerao, and
Samuel R. Bowman. 2020. CrowS-pairs: A chal-
lenge dataset for measuring social biases in masked
language models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language

--- PAGE 16 ---
Processing (EMNLP) , pages 1953–1967, Online. As-
sociation for Computational Linguistics.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated Gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction (AKBC-WEKEX) , pages 95–100, Mon-
tréal, Canada. Association for Computational Lin-
guistics.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don’t give me the details, just the summary!
topic-aware convolutional neural networks for ex-
treme summarization. In Proceedings of the 2018
Conference on Empirical Methods in Natural Lan-
guage Processing , pages 1797–1807, Brussels, Bel-
gium. Association for Computational Linguistics.
Yixin Nie, Adina Williams, Emily Dinan, Mohit
Bansal, Jason Weston, and Douwe Kiela. 2020. Ad-
versarial NLI: A new benchmark for natural lan-
guage understanding. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 4885–4901, Online. Association
for Computational Linguistics.
A. Othman and M. Jemni. 2012. English-asl gloss
parallel corpus 2012: Aslg-pc12. In IEnglish-ASL
Gloss Parallel Corpus 2012 .
Bo Pang and Lillian Lee. 2005. Seeing stars: Ex-
ploiting class relationships for sentiment categoriza-
tion with respect to rating scales. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL’05) , pages 115–
124, Ann Arbor, Michigan. Association for Compu-
tational Linguistics.
Dimitris Pappas, Petros Stavropoulos, Ion Androut-
sopoulos, and Ryan McDonald. 2020. BioMRC: A
dataset for biomedical machine reading comprehen-
sion. In Proceedings of the 19th SIGBioMed Work-
shop on Biomedical Language Processing , pages
140–149, Online. Association for Computational
Linguistics.
Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim
Rocktäschel, Yuxiang Wu, Alexander H. Miller, and
Sebastian Riedel. 2020. How context affects lan-
guage models’ factual predictions. In Automated
Knowledge Base Construction .
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP) , pages 2463–2473, Hong Kong, China. As-
sociation for Computational Linguistics.
Jason Phang, Thibault Févry, and Samuel R Bowman.
2018. Sentence encoders on stilts: Supplementarytraining on intermediate labeled-data tasks. arXiv
preprint arXiv:1811.01088 .
Mohammad Taher Pilehvar and Jose Camacho-
Collados. 2019. WiC: the word-in-context dataset
for evaluating context-sensitive meaning represen-
tations. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 1267–1273, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Edoardo M Ponti, Alessandro Sordoni, and Siva Reddy.
2022. Combining modular skills in multitask learn-
ing. arXiv preprint arXiv:2202.13914 .
Amir Pouran Ben Veyseh, Franck Dernoncourt,
Quan Hung Tran, and Thien Huu Nguyen. 2020.
What does this acronym mean? introducing a new
dataset for acronym identiﬁcation and disambigua-
tion. In Proceedings of the 28th International Con-
ference on Computational Linguistics , pages 3285–
3301, Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.
Yada Pruksachatkun, Jason Phang, Haokun Liu,
Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe
Pang, Clara Vania, Katharina Kann, and Samuel R.
Bowman. 2020. Intermediate-task transfer learning
with pretrained language models: When and why
does it work? In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics , pages 5231–5247, Online. Association for
Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search , 21(140):1–67.
Altaf Rahman and Vincent Ng. 2012. Resolving com-
plex cases of deﬁnite pronouns: The Winograd
schema challenge. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning , pages 777–789, Jeju Island, Korea.
Association for Computational Linguistics.
Nazneen Fatema Rajani, Bryan McCann, Caiming
Xiong, and Richard Socher. 2019. Explain yourself!
leveraging language models for commonsense rea-
soning. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 4932–4942, Florence, Italy. Association
for Computational Linguistics.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and
Percy Liang. 2016. SQuAD: 100,000+ questions for
machine comprehension of text. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2383–2392, Austin,
Texas. Association for Computational Linguistics.

--- PAGE 17 ---
Hannah Rashkin, Eric Michael Smith, Margaret Li, and
Y-Lan Boureau. 2019. Towards empathetic open-
domain conversation models: A new benchmark and
dataset. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 5370–5381, Florence, Italy. Association
for Computational Linguistics.
Anna Rogers, Olga Kovaleva, Matthew Downey, and
Anna Rumshisky. 2020. Getting closer to ai com-
plete question answering: A set of prerequisite real
tasks. Proceedings of the AAAI Conference on Arti-
ﬁcial Intelligence , 34(05):8722–8731.
Clemens Rosenbaum, Tim Klinger, and Matthew
Riemer. 2018. Routing networks: Adaptive selec-
tion of non-linear functions for multi-task learning.
InInternational Conference on Learning Represen-
tations .
Amrita Saha, Rahul Aralikatte, Mitesh M. Khapra, and
Karthik Sankaranarayanan. 2018. DuoRC: Towards
complex language understanding with paraphrased
reading comprehension. In Proceedings of the 56th
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers) , pages
1683–1693, Melbourne, Australia. Association for
Computational Linguistics.
Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhaga-
vatula, and Yejin Choi. 2020. Winogrande: An ad-
versarial winograd schema challenge at scale. Pro-
ceedings of the AAAI Conference on Artiﬁcial Intel-
ligence , 34(05):8732–8740.
Victor Sanh, Albert Webson, Colin Raffel, Stephen
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Tae-
woon Kim, Gunjan Chhablani, Nihal Nayak, De-
bajyoti Datta, Jonathan Chang, Mike Tian-Jian
Jiang, Han Wang, Matteo Manica, Sheng Shen,
Zheng Xin Yong, Harshit Pandey, Rachel Bawden,
Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-
heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-
son Alan Fries, Ryan Teehan, Teven Le Scao, Stella
Biderman, Leo Gao, Thomas Wolf, and Alexan-
der M Rush. 2022. Multitask prompted training en-
ables zero-shot task generalization. In International
Conference on Learning Representations .
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan
Le Bras, and Yejin Choi. 2019. Social IQa: Com-
monsense reasoning about social interactions. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 4463–
4473, Hong Kong, China. Association for Computa-
tional Linguistics.
Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,
Junlin Wu, and Yi-Shin Chen. 2018. CARER: Con-
textualized affect representations for emotion recog-
nition. In Proceedings of the 2018 Conference onEmpirical Methods in Natural Language Processing ,
pages 3687–3697, Brussels, Belgium. Association
for Computational Linguistics.
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,
Andy Davis, Quoc V . Le, Geoffrey E. Hinton, and
Jeff Dean. 2017. Outrageously large neural net-
works: The sparsely-gated mixture-of-experts layer.
InICLR (Poster) . OpenReview.net.
Emily Sheng and David Uthus. 2020. Investigating
societal biases in a poetry composition system. In
Proceedings of the Second Workshop on Gender
Bias in Natural Language Processing , pages 93–106,
Barcelona, Spain (Online). Association for Compu-
tational Linguistics.
Damien Sileo, Tim Van De Cruys, Camille Pradel,
and Philippe Muller. 2019. Mining discourse mark-
ers for unsupervised sentence representation learn-
ing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
3477–3486, Minneapolis, Minnesota. Association
for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing ,
pages 1631–1642, Seattle, Washington, USA. Asso-
ciation for Computational Linguistics.
Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi,
and Claire Cardie. 2019. DREAM: A challenge data
set and models for dialogue-based reading compre-
hension. Transactions of the Association for Com-
putational Linguistics , 7:217–231.
Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau
Yih, and Ashish Sabharwal. 2019a. Quarel: A
dataset and models for answering questions about
qualitative relationships. Proceedings of the AAAI
Conference on Artiﬁcial Intelligence , 33(01):7063–
7071.
Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter
Clark. 2019b. QuaRTz: An open-domain dataset of
qualitative relationship questions. In Proceedings of
the 2019 Conference on Empirical Methods in Nat-
ural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 5941–5946, Hong Kong,
China. Association for Computational Linguistics.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. 2019. CommonsenseQA: A ques-
tion answering challenge targeting commonsense
knowledge. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,

--- PAGE 18 ---
pages 4149–4158, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-
ter Clark, and Antoine Bosselut. 2019. WIQA: A
dataset for “what if...” reasoning over procedural
text. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
6076–6085, Hong Kong, China. Association for
Computational Linguistics.
James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction
and VERiﬁcation. In Proceedings of the 2018
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers) , pages 809–819, New Orleans, Louisiana.
Association for Computational Linguistics.
Sowmya Vajjala and Ivana Lu ˇci´c. 2018. On-
eStopEnglish corpus: A new corpus for automatic
readability assessment and text simpliﬁcation. In
Proceedings of the Thirteenth Workshop on Innova-
tive Use of NLP for Building Educational Applica-
tions , pages 297–304, New Orleans, Louisiana. As-
sociation for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information process-
ing systems , 30.
Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-
dro Sordoni, Adam Trischler, Andrew Mattarella-
Micke, Subhransu Maji, and Mohit Iyyer. 2020. Ex-
ploring and predicting transferability across NLP
tasks. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP) , pages 7882–7926, Online. Associa-
tion for Computational Linguistics.
Jixuan Wang, Kuan-Chieh Wang, Frank Rudzicz, and
Michael Brudno. 2021. Grad2task: Improved few-
shot text classiﬁcation using gradients for task rep-
resentation. In Advances in Neural Information Pro-
cessing Systems .
William Yang Wang. 2017. “liar, liar pants on ﬁre”: A
new benchmark dataset for fake news detection. In
Proceedings of the 55th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers) , pages 422–426, Vancouver, Canada.
Association for Computational Linguistics.
Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-
hananey, Wei Peng, Sheng-Fu Wang, and Samuel R.
Bowman. 2020. BLiMP: The benchmark of linguis-
tic minimal pairs for English. Transactions of the As-
sociation for Computational Linguistics , 8:377–392.Alex Warstadt, Amanpreet Singh, and Samuel R. Bow-
man. 2019. Neural network acceptability judgments.
Transactions of the Association for Computational
Linguistics , 7:625–641.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.
Crowdsourcing multiple choice science questions.
InProceedings of the 3rd Workshop on Noisy User-
generated Text , pages 94–106, Copenhagen, Den-
mark. Association for Computational Linguistics.
Johannes Welbl, Pontus Stenetorp, and Sebastian
Riedel. 2018. Constructing datasets for multi-hop
reading comprehension across documents. Transac-
tions of the Association for Computational Linguis-
tics, 6:287–302.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume
1 (Long Papers) , pages 1112–1122, New Orleans,
Louisiana. Association for Computational Linguis-
tics.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
ner, Yoav Goldberg, Daniel Deutch, and Jonathan
Berant. 2020. Break it down: A question under-
standing benchmark. Transactions of the Associa-
tion for Computational Linguistics , 8:183–198.
Wenhan Xiong, Jiawei Wu, Hong Wang, Vivek Kulka-
rni, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and
William Yang Wang. 2019. TWEETQA: A social
media focused question answering dataset. In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 5020–
5031, Florence, Italy. Association for Computa-
tional Linguistics.
Yi Yang, Wen-tau Yih, and Christopher Meek. 2015.
WikiQA: A challenge dataset for open-domain ques-
tion answering. In Proceedings of the 2015 Con-
ference on Empirical Methods in Natural Language
Processing , pages 2013–2018, Lisbon, Portugal. As-
sociation for Computational Linguistics.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D. Manning. 2018. HotpotQA: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 2369–2380, Brussels, Belgium. Association
for Computational Linguistics.

--- PAGE 19 ---
Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021.
CrossFit: A few-shot learning challenge for cross-
task generalization in NLP. In Proceedings of the
2021 Conference on Empirical Methods in Natural
Language Processing , pages 7163–7189, Online and
Punta Cana, Dominican Republic. Association for
Computational Linguistics.
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
Dongxu Wang, Zifan Li, James Ma, Irene Li,
Qingning Yao, Shanelle Roman, Zilin Zhang,
and Dragomir Radev. 2018. Spider: A large-
scale human-labeled dataset for complex and cross-
domain semantic parsing and text-to-SQL task. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing , pages
3911–3921, Brussels, Belgium. Association for
Computational Linguistics.
Rowan Zellers, Yonatan Bisk, Roy Schwartz, and
Yejin Choi. 2018. SWAG: A large-scale adversar-
ial dataset for grounded commonsense inference. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing , pages 93–
104, Brussels, Belgium. Association for Computa-
tional Linguistics.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali
Farhadi, and Yejin Choi. 2019. HellaSwag: Can
a machine really ﬁnish your sentence? In Pro-
ceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 4791–
4800, Florence, Italy. Association for Computational
Linguistics.
Hao Zhang, Jae Ro, and Richard Sproat. 2020. Semi-
supervised URL segmentation with recurrent neu-
ral networks pre-trained on knowledge graph enti-
ties. In Proceedings of the 28th International Con-
ference on Computational Linguistics , pages 4667–
4675, Barcelona, Spain (Online). International Com-
mittee on Computational Linguistics.
Rui Zhang and Joel Tetreault. 2019. This email could
save your life: Introducing the task of email subject
line generation. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics , pages 446–456, Florence, Italy. Association
for Computational Linguistics.
Sheng Zhang, X. Liu, J. Liu, Jianfeng Gao, Kevin
Duh, and Benjamin Van Durme. 2018. Record:
Bridging the gap between human and machine com-
monsense reading comprehension. ArXiv preprint ,
abs/1810.12885.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.
Character-level convolutional networks for text clas-
siﬁcation. In Advances in Neural Information Pro-
cessing Systems 28: Annual Conference on Neural
Information Processing Systems 2015, December 7-
12, 2015, Montreal, Quebec, Canada , pages 649–
657.Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
PAWS: Paraphrase adversaries from word scram-
bling. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
1298–1308, Minneapolis, Minnesota. Association
for Computational Linguistics.
Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2sql: Generating structured queries
from natural language usin. ArXiv preprint ,
abs/1709.00103.
Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan
Roth. 2019. “going on a vacation” takes longer
than “going for a walk”: A study of temporal com-
monsense understanding. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP) , pages 3363–3369, Hong Kong,
China. Association for Computational Linguistics.
Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du,
Yanping Huang, Jeff Dean, Noam Shazeer, and
William Fedus. 2022. Designing effective sparse ex-
pert models. arXiv preprint arXiv:2202.08906 .
A Computing Task Representations
In the following, we describe the method to con-
struct the task representations used in §5.3.
TaskEmb. Task2Vec (Achille et al., 2019) is a
method to generate tasks embedding for visual
classiﬁcation tasks based ﬁsher information ma-
trix (FIM). It was then extended to NLP domain
(Vu et al., 2020; Wang et al., 2021) and was found
to be useful. We compute the empirical ﬁsher and
use them as task representations, following Vu et al.
(2020). Speciﬁcally, given a model Pparameter-
ized by(e.g., a BART-Base model) and a set of
labeled examplesf(x;y)g, we ﬁrst ﬁne-tune the
model on the examples, then compute the ﬁsher
information matrix:
F=1
nnX
i=1
rlogP
yijxi
rlogP
yijxiT
(2)
To reduce the computational complexity, (1) we
only use the diagonal entries of F, following
Achille et al. (2019) and Vu et al. (2020); (2) we
use a parameter-efﬁcient ﬁne-tuning method named
adapter ﬁne-tuning (Houlsby et al., 2019) and only
compute the FIM with respect to adapter param-
eters. (3) we use PCA to reduce the dimension
(d= 768 , which is the same as TextEmb), as we
will use these representations as input to our router
in the task-level MoE model.

--- PAGE 20 ---
TextEmb and FT-TextEmb. For TextEmb, we
ﬁrst concatenate the input sequence xand the out-
put sequence yinto a longer sequence, and feed
it to the encoder of BART to get token-level rep-
resentations. For TextEmb-A VG, we compute the
average over tokens for each example, and then
average over all examples, to get a ﬁnal vector as
task representation. For TextEmb-BOS, we aver-
age the BOS representation of all examples9. For
fair comparison with TaskEmb, which ﬁne-tunes
the model on labeled examples and thus may obtain
extra information through this process, we also in-
clude FT-TextEmb-A VG and FT-TextEmb-BOS in
our comparison. In these two variants, the BART
model is ﬁrst ﬁne-tune on the labeled examples
f(x;y)g.
B Additional Experiment Details
B.1 Multi-task Learning Experiments
We concatenate the Dtrain of the 120 tasks in
Ttrain into a large dataset and use it for multi-task
learning. We adopt heterogeneous batching (Agha-
janyan et al., 2021), i.e., each batch contains exam-
ples from different tasks. For the vanilla multi-task
baseline, we train the model for 30,000 steps, with
the batch size equals to 32 and the learning rate
equals to 3e-5. For BART-Large we use the same
setting, except that the learning rate is set to 1e-5.
We use validation every 3,000 steps and select the
best model based on validation performance.
For the task-level MoE models, they are trained
with a basic learning rate of 1e-5, while we set the
router with bigger learning rate of 1e-3 based on
our pilot experiments following Ponti et al. (2022).
For the task representations, we use 1e-2 as learn-
ing rate when they are randomly initialized, and
1e-3 when initialized from pre-computed represen-
tations. We train the model for 60,000 steps be-
cause it takes more exploration time for the routes
and experts to be stable. All models are trained
with Adam optimizer (Kingma and Ba, 2014).
B.2 Few-shot Adaptation Experiments
For few-shot ﬁne-tuning we mainly follow the ex-
periment setting in Ye et al. (2021). Each task has
ﬁve different few-shot samples of (Dtrain;Ddev).
We train on Dtrain for 1000 steps, and validate
onDdevevery 100 steps. We run a grid search
9We later found out that this is less meaningful since BART
pre-training does not train these BOS tokens with any special
objective.for learning rate {1e-5, 2e-5, 5e-5} and batch size
{2,4,8} for each few-shot sample. Finally, the
model with best Ddevperformance is evaluated
onDtest, the we report the performance on Dtest.
B.3 Zero-shot Experiments
Data. Following Sanh et al. (2022) and Lin et al.
(2022), we use the prompt templates in the Pub-
lic Pool of Prompts (P3) (Bach et al., 2022) to
change texts from various NLP tasks into a uni-
ﬁed text-to-text format. To save compute, we use
a sub-sampled version of P3 dataset. We use up
to 5k examples for Dtrain, 1k examples for both
DdevandDtestfollowing Lin et al. (2022) for all
tasks. We use 36 upstream tasks (which is the same
as T0 upstream learning) for Ttrain and use 10 un-
seen tasks as ourTtest.Dtrain for tasks inTtrain
are used for upstream learning; Dtestfor tasks in
Ttestare used for reporting the performance. For
simplicity, we only keep the prompt that can be
evaluated with accuracy, and we report the mean
acurracy for all tasks in Ttest.
Training. (1) For Multi-task BART-Base and
Random Task Routing (2/3), we use 1e-5 as the
learning rate, 16 as training batch size, and the
total training steps is set to be 200k. (2) For the
(c)+(d)+(h)+(l)+(m) model, we use 1e-5 as
the base learning rate for experts and 1e-3 for the
router. We train the model for 200k steps. (3) For
the(c)+(d)+(h)+(l)+(n)model, we use 1e-5
as the base learning rate for experts and 1e-3 for
router. For the ﬁrst learning stage we train for 60k
steps, and 200k steps for the second stage. For both
MoE models we use a batch size as 4. In this zero-
shot setting, the task representation is computed by
applying TextEmb-A VG (h)to the prompt tem-
plates.
C Extended Results and Analysis
C.1 Loss and Performance Discrepancy
In Fig. 6, we plot the Ddevloss and performance
during multitask learning. We conclude that Ddev
loss does not align well with the ﬁnal metrics, and
thus validation should be done with the ﬁnal met-
rics.
C.2 Full Manual Feature Correlation Results
We show the full results of Pearson Correlation
between learned routes and manual features in Fig-
ure 7 and Figure 8. Figure 7 is based on routes

--- PAGE 21 ---
5000 10000 15000 20000 25000 30000
Training Steps2.42.52.62.72.82.93.03.1Loss
0.250.300.350.400.450.500.550.60
Performance
Multi-task Vanilla Baseline Training
Loss (Left Axis)
BART-Base
BART-LargePerformance (Right Axis)
BART-Base
BART-LargeFigure 6: Dev loss and dev performance discrepancy
when training multi-task transformer baselines. We
found that smaller dev loss does not guarantee better
dev performance. Dev losses tend to plunge then rise,
while dev performance continue to increase. BART-
Large outperforms BART-Base despite larger dev loss.
in the (c)+(d)+(g)+(k)model, and Figure 8 is
based on the (c) + (d) + (j) + (k) model.
C.3 Further Investigation on Selection
Functions
In our initial experiments, the implementation
of softmax does not have temperature annealing.
When we include this trick, the performance is
comparable to gumbel-softmax ST.
D Discussion on Contemporary Works
Training dynamical models that condition the com-
putation on task information is a growing and active
research ﬁeld. Several contemporary works (Ponti
et al., 2022; Gupta et al., 2022; Asai et al., 2022)
are studying this problem. We share similar moti-
vations with these works; meanwhile, these works
and ours differ in methodology and research focus.
We would like to highlight that (1) we conduct ex-
tensive analysis on interpreting the learned routes
and experts in §7; (2) we use 120 seen tasks and 18
unseen tasks, which is more diverse, and creates a
challenging learning setting. We hope our ﬁndings
are useful to the EMNLP community.

--- PAGE 22 ---
ClassificationQA
Con. Gen.Other
Sentiment AnalysisNLI
Fact CheckingEmotion
Hate SpeechTopicMRC
Multiple ChoiceBinary QA
Long-form QAClosed-book QADialog
SummarizationSlot FillingRegressionhasShortInputhasMediumInputhasLongInputhasShortOutputhasMediumOutputhasLongOutput
Science & TechnologySocial NetworkNews Web
Bio-MedicalReviewDialogBooksPhraseSentenceParagraphExtractiveLinguistic
CommonsenseCo-reference
World KnowledgeMulti-hop
Sent. CompletionSynthesizeL0E1
L0E2
L0E3
L1E1
L1E2
L1E3
L2E1
L2E2
L2E3
L3E1
L3E2
L3E3
L4E1
L4E2
L4E3
L5E1
L5E2
L5E3
L6E1
L6E2
L6E3
L7E1
L7E2
L7E3
L8E1
L8E2
L8E3
L9E1
L9E2
L9E3
L10E1
L10E2
L10E3
L11E1
L11E2
L11E3
1.0
 0.5
 0.0 0.5 1.0Figure 7: Pearson Correlation Between Learned Routes and Manual Features. Correlation with p<0:01are
visualized. The correlation is based on a (c) + (d) + (g) + (k) model.
ClassificationQA
Con. Gen.Other
Sentiment AnalysisNLI
Fact CheckingEmotion
Hate SpeechTopicMRC
Multiple ChoiceBinary QA
Long-form QAClosed-book QADialog
SummarizationSlot FillingRegressionhasShortInputhasMediumInputhasLongInputhasShortOutputhasMediumOutputhasLongOutput
Science & TechnologySocial NetworkNews Web
Bio-MedicalReviewDialogBooksPhraseSentenceParagraphExtractiveLinguistic
CommonsenseCo-reference
World KnowledgeMulti-hop
Sent. CompletionSynthesizeL0E1
L0E2
L0E3
L1E1
L1E2
L1E3
L2E1
L2E2
L2E3
L3E1
L3E2
L3E3
L4E1
L4E2
L4E3
L5E1
L5E2
L5E3
L6E1
L6E2
L6E3
L7E1
L7E2
L7E3
L8E1
L8E2
L8E3
L9E1
L9E2
L9E3
L10E1
L10E2
L10E3
L11E1
L11E2
L11E3
1.0
 0.5
 0.0 0.5 1.0
Figure 8: Pearson Correlation Between Learned Routes and Manual Features. Correlation with p<0:01are
visualized. The correlation is based on a (c) + (d) + (j) + (k) model.

--- PAGE 23 ---
E Tasks Used and References
We list all the tasks used in this paper in Table 8 and its corresponding manual feature labels in Table 9.
Table 8: Tasks used in this work.
Task Name Ontology Reference
acronym_identiﬁcation other Pouran Ben Veyseh et al. 2020
ade_corpus_v2-classiﬁcation cls/other Gurulingappa et al. 2012
ade_corpus_v2-dosage other/slot ﬁlling Gurulingappa et al. 2012
ade_corpus_v2-effect other/slot ﬁlling Gurulingappa et al. 2012
adversarialqa qa/machine reading comprehension Bartolo et al. 2020
aeslc cg/summarization Zhang and Tetreault 2019
ag_news cls/topic Gulli (link)
ai2_arc qa/multiple-choice qa Clark et al. 2018
amazon_polarity cls/sentiment analysis McAuley and Leskovec 2013
anli cls/nli Nie et al. 2020
app_reviews other/regression Missing
aqua_rat qa/multiple-choice qa Ling et al. 2017
art (abductive nli) other Bhagavatula et al. 2020
aslg_pc12 other Othman and Jemni 2012
biomrc qa/machine reading comprehension Pappas et al. 2020
blimp-anaphor_gender_agreement other/linguistic phenomenon Warstadt et al. 2020
blimp-anaphor_number_agreement other/linguistic phenomenon Warstadt et al. 2020
blimp-determiner_noun_agreement_with_adj_irregular_1 other/linguistic phenomenon Warstadt et al. 2020
blimp-ellipsis_n_bar_1 other/linguistic phenomenon Warstadt et al. 2020
blimp-ellipsis_n_bar_2 other/linguistic phenomenon Warstadt et al. 2020
blimp-existential_there_quantiﬁers_1 other/linguistic phenomenon Warstadt et al. 2020
blimp-irregular_past_participle_adjectives other/linguistic phenomenon Warstadt et al. 2020
blimp-sentential_negation_npi_licensor_present other/linguistic phenomenon Warstadt et al. 2020
blimp-sentential_negation_npi_scope other/linguistic phenomenon Warstadt et al. 2020
blimp-wh_questions_object_gap other/linguistic phenomenon Warstadt et al. 2020
boolq qa/binary Clark et al. 2019
break-QDMR other Wolfson et al. 2020
break-QDMR-high-level other Wolfson et al. 2020
circa cls/other Louis et al. 2020
climate_fever cls/fact checking Diggelmann et al. 2020
codah qa/multiple-choice qa Chen et al. 2019
common_gen other Lin et al. 2020b
commonsense_qa qa/multiple-choice qa Talmor et al. 2019
cos_e other/generate explanation Rajani et al. 2019
cosmos_qa qa/multiple-choice qa Huang et al. 2019
crawl_domain other Zhang et al. 2020
crows_pairs other Nangia et al. 2020
dbpedia_14 cls/topic Lehmann et al. 2015
deﬁnite_pronoun_resolution other Rahman and Ng 2012
discovery cls/other Sileo et al. 2019
dream qa/multiple-choice qa Sun et al. 2019
duorc qa/machine reading comprehension Saha et al. 2018
e2e_nlg_cleaned other Dušek et al. 2020, 2019
eli5-askh qa/long-form qa Fan et al. 2019
eli5-asks qa/long-form qa Fan et al. 2019
eli5-eli5 qa/long-form qa Fan et al. 2019
emo cls/emotion Chatterjee et al. 2019
emotion cls/emotion Saravia et al. 2018
empathetic_dialogues cg/dialogue Rashkin et al. 2019
ethos-directed_vs_generalized cls/hate speech detection Mollas et al. 2020
ethos-disability cls/hate speech detection Mollas et al. 2020
ethos-gender cls/hate speech detection Mollas et al. 2020
ethos-national_origin cls/hate speech detection Mollas et al. 2020
ethos-race cls/hate speech detection Mollas et al. 2020
ethos-religion cls/hate speech detection Mollas et al. 2020
ethos-sexual_orientation cls/hate speech detection Mollas et al. 2020
ﬁnancial_phrasebank cls/sentiment analysis Malo et al. 2014
freebase_qa qa/closed-book qa Jiang et al. 2019
gigaword cg/summarization Napoles et al. 2012
glue-cola cls/other Warstadt et al. 2019
glue-mnli cls/nli Williams et al. 2018
glue-mrpc cls/paraphrase Dolan and Brockett 2005
glue-qnli cls/nli Rajpurkar et al. 2016
glue-qqp cls/paraphrase (link)
glue-rte cls/nliDagan et al. 2005; Bar-Haim et al. 2006
Giampiccolo et al. 2007; Bentivogli et al. 2009
glue-sst2 cls/sentiment analysis Socher et al. 2013
glue-wnli cls/nli Levesque et al. 2012
google_wellformed_query cls/other Faruqui and Das 2018
hate_speech18 cls/hate speech detection de Gibert et al. 2018
hate_speech_offensive cls/hate speech detection Davidson et al. 2017
hatexplain cls/hate speech detection Mathew et al. 2020
health_fact cls/fact checking Kotonya and Toni 2020
hellaswag qa/multiple-choice qa Zellers et al. 2019
hotpot_qa qa/machine reading comprehension Yang et al. 2018
imdb cls/sentiment analysis Maas et al. 2011
jeopardy qa/closed-book qa (link)
Continued on next page

--- PAGE 24 ---
Task Name Ontology Reference
kilt_ay2 other/entity linking Hoffart et al. 2011
kilt_fever cls/fact checking Thorne et al. 2018
kilt_hotpotqa qa/closed-book qa Yang et al. 2018
kilt_nq qa/closed-book qa Kwiatkowski et al. 2019
kilt_trex qa/closed-book qa Elsahar et al. 2018
kilt_wow cg/dialogue Dinan et al. 2019
kilt_zsre qa/closed-book qa Levy et al. 2017
lama-conceptnet qa/closed-book qa Petroni et al. 2019, 2020
lama-google_re qa/closed-book qa Petroni et al. 2019, 2020
lama-squad qa/closed-book qa Petroni et al. 2019, 2020
lama-trex qa/closed-book qa Petroni et al. 2019, 2020
liar cls/fact checking Wang 2017
limit other Manotas et al. 2020
math_qa qa/multiple-choice qa Amini et al. 2019
mc_taco qa/binary Zhou et al. 2019
medical_questions_pairs cls/paraphrase McCreery et al. 2020
mocha other/regression Chen et al. 2020a
multi_news cg/summarization Fabbri et al. 2019
numer_sense qa/closed-book qa Lin et al. 2020a
onestop_english cls/other Vajjala and Lu ˇci´c 2018
openbookqa qa/multiple-choice qa Mihaylov et al. 2018
paws cls/paraphrase Zhang et al. 2019
piqa other Bisk et al. 2020
poem_sentiment cls/sentiment analysis Sheng and Uthus 2020
proto_qa other Boratko et al. 2020
qa_srl other He et al. 2015
qasc qa/multiple-choice qa Khot et al. 2020
quail qa/multiple-choice qa Rogers et al. 2020
quarel qa/multiple-choice qa Tafjord et al. 2019a
quartz-no_knowledge qa/multiple-choice qa Tafjord et al. 2019b
quartz-with_knowledge qa/multiple-choice qa Tafjord et al. 2019b
quoref qa/machine reading comprehension Dasigi et al. 2019
race-high qa/multiple-choice qa Lai et al. 2017
race-middle qa/multiple-choice qa Lai et al. 2017
reddit_tifu-title cg/summarization Kim et al. 2019
reddit_tifu-tldr cg/summarization Kim et al. 2019
ropes qa/machine reading comprehension Lin et al. 2019
rotten_tomatoes cls/sentiment analysis Pang and Lee 2005
samsum cg/summarization Gliwa et al. 2019
scicite cls/other Cohan et al. 2019
sciq qa/multiple-choice qa Welbl et al. 2017
scitail cls/nli Khot et al. 2018
search_qa qa/closed-book qa Dunn et al. 2017
sick cls/nli Marelli et al. 2014
sms_spam cls/other Almeida et al. 2011
social_i_qa qa/multiple-choice qa Sap et al. 2019
spider cg/other Yu et al. 2018
squad-no_context qa/closed-book qa Rajpurkar et al. 2016
squad-with_context qa/machine reading comprehension Rajpurkar et al. 2016
superglue-cb cls/nli de Marneffe et al. 2019
superglue-copa qa/multiple-choice qa Gordon et al. 2012
superglue-multirc qa/multiple-choice qa Khashabi et al. 2018
superglue-record qa/machine reading comprehension Zhang et al. 2018
superglue-rte cls/nliDagan et al. 2005; Bar-Haim et al. 2006
Giampiccolo et al. 2007; Bentivogli et al. 2009
superglue-wic cls/other Pilehvar and Camacho-Collados 2019
superglue-wsc cls/other Levesque et al. 2012
swag qa/multiple-choice qa Zellers et al. 2018
tab_fact cls/fact checking Chen et al. 2020b
trec cls/other Li and Roth 2002; Hovy et al. 2001
trec-ﬁnegrained cls/other Li and Roth 2002; Hovy et al. 2001
tweet_eval-emoji cls/emotion Barbieri et al. 2020
tweet_eval-emotion cls/emotion Barbieri et al. 2020
tweet_eval-hate cls/emotion Barbieri et al. 2020
tweet_eval-irony cls/emotion Barbieri et al. 2020
tweet_eval-offensive cls/emotion Barbieri et al. 2020
tweet_eval-sentiment cls/emotion Barbieri et al. 2020
tweet_eval-stance_abortion cls/emotion Barbieri et al. 2020
tweet_eval-stance_atheism cls/emotion Barbieri et al. 2020
tweet_eval-stance_climate cls/emotion Barbieri et al. 2020
tweet_eval-stance_feminist cls/emotion Barbieri et al. 2020
tweet_eval-stance_hillary cls/emotion Barbieri et al. 2020
tweet_qa qa/machine reading comprehension Xiong et al. 2019
web_questions qa/closed-book qa Berant et al. 2013
wiki_auto cls/other Jiang et al. 2020
wiki_bio cg/other Lebret et al. 2016
wiki_qa cls/other Yang et al. 2015
wiki_split cg/other Botha et al. 2018
wikisql cg/other Zhong et al. 2017
wino_grande qa/multiple-choice qa Sakaguchi et al. 2020
wiqa qa/multiple-choice qa Tandon et al. 2019
xsum cg/summarization Narayan et al. 2018
yahoo_answers_topics cls/topic (link)
yelp_polarity cls/sentiment analysis Zhang et al. 2015; (link)
Continued on next page

--- PAGE 25 ---
Task Name Ontology Reference
yelp_review_full other/regression Zhang et al. 2015; (link)
cnn_dailymail cg/summarization Nallapati et al. 2016
wiki_hop qa/multiple-choice qa Welbl et al. 2018
F Random Task Partition
Different from the original random task partition used in Ye et al. (2021), we remove yelp_polarity and
freebase_qa fromTtestbecause we observe unusual instability when doing few-shot ﬁne-tuning on these
tasks.
1{
2 " train ": [ /quotesingle.Varglue -mrpc /quotesingle.Var,/quotesingle.Varmath_qa /quotesingle.Var,/quotesingle.Varquarel /quotesingle.Var,/quotesingle.Vare2 e_nlg_cleaned /quotesingle.Var,/quotesingle.Vartweet_eval - stance_atheism /quotesingle.Var,/quotesingle.Varlama -squad /quotesingle.Var
,/quotesingle.Vartab_fact /quotesingle.Var,/quotesingle.Varaqua_rat /quotesingle.Var,/quotesingle.Vartweet_eval -emoji /quotesingle.Var,/quotesingle.Varglue -wnli /quotesingle.Var,/quotesingle.Varcodah /quotesingle.Var,/quotesingle.Vartweet_eval - offensive /quotesingle.Var,/quotesingle.Var
wiki_qa /quotesingle.Var,/quotesingle.Varblimp - ellipsis_n_bar_ 1 /quotesingle.Var,/quotesingle.Varopenbookqa /quotesingle.Var,/quotesingle.Varsms_spam /quotesingle.Var,/quotesingle.Varacronym_identification /quotesingle.Var,/quotesingle.Varblimp -
determiner_noun_agreement_with_adj_irregular_ 1 /quotesingle.Var,/quotesingle.Varethos - national_origin /quotesingle.Var,/quotesingle.Varspider /quotesingle.Var,/quotesingle.Var
definite_pronoun_resolution /quotesingle.Var,/quotesingle.Varhellaswag /quotesingle.Var,/quotesingle.Varsuperglue -wsc /quotesingle.Var,/quotesingle.Varnumer_sense /quotesingle.Var,/quotesingle.Varade_corpus_v 2-dosage /quotesingle.Var,
/quotesingle.Varblimp - ellipsis_n_bar_ 2 /quotesingle.Var,/quotesingle.Varkilt_ay 2 /quotesingle.Var,/quotesingle.Varsquad - no_context /quotesingle.Var,/quotesingle.Vargoogle_wellformed_query /quotesingle.Var,/quotesingle.Varxsum /quotesingle.Var,/quotesingle.Varwiqa /quotesingle.Var
,/quotesingle.Vartweet_eval - stance_abortion /quotesingle.Var,/quotesingle.Varreddit_tifu -tldr /quotesingle.Var,/quotesingle.Varade_corpus_v 2-effect /quotesingle.Var,/quotesingle.Varqa_srl /quotesingle.Var,/quotesingle.Varethos -
religion /quotesingle.Var,/quotesingle.Varcommonsense_qa /quotesingle.Var,/quotesingle.Varjeopardy /quotesingle.Var,/quotesingle.Varbiomrc /quotesingle.Var,/quotesingle.Varsuperglue - multirc /quotesingle.Var,/quotesingle.Varethos -race /quotesingle.Var,/quotesingle.Vareli 5-askh /quotesingle.Var,
/quotesingle.Varglue -qqp /quotesingle.Var,/quotesingle.Varpaws /quotesingle.Var,/quotesingle.Varethos - directed_vs_generalized /quotesingle.Var,/quotesingle.Varglue - sst 2 /quotesingle.Var,/quotesingle.Varmocha /quotesingle.Var,/quotesingle.Vartweet_eval -hate /quotesingle.Var,/quotesingle.Varglue
-rte /quotesingle.Var,/quotesingle.Varblimp - anaphor_number_agreement /quotesingle.Var,/quotesingle.Varlama - conceptnet /quotesingle.Var,/quotesingle.Varhate_speech_offensive /quotesingle.Var,/quotesingle.Varsuperglue -wic
/quotesingle.Var,/quotesingle.Varboolq /quotesingle.Var,/quotesingle.Varkilt_hotpotqa /quotesingle.Var,/quotesingle.Varquartz - no_knowledge /quotesingle.Var,/quotesingle.Varaslg_pc 12 /quotesingle.Var,/quotesingle.Varsick /quotesingle.Var,/quotesingle.Vartweet_eval - stance_climate
/quotesingle.Var,/quotesingle.Vartweet_eval - sentiment /quotesingle.Var,/quotesingle.Varcrows_pairs /quotesingle.Var,/quotesingle.Varglue -mnli /quotesingle.Var,/quotesingle.Varmedical_questions_pairs /quotesingle.Var,/quotesingle.Varbreak -QDMR -high -
level /quotesingle.Var,/quotesingle.Varqasc /quotesingle.Var,/quotesingle.Varimdb /quotesingle.Var,/quotesingle.Varethos - gender /quotesingle.Var,/quotesingle.Vartrec - finegrained /quotesingle.Var,/quotesingle.Varadversarialqa /quotesingle.Var,/quotesingle.Varonestop_english /quotesingle.Var,/quotesingle.Var
web_questions /quotesingle.Var,/quotesingle.Varduorc /quotesingle.Var,/quotesingle.Varyelp_review_full /quotesingle.Var,/quotesingle.Varswag /quotesingle.Var,/quotesingle.Varproto_qa /quotesingle.Var,/quotesingle.Varscitail /quotesingle.Var,/quotesingle.Vartweet_eval -
stance_feminist /quotesingle.Var,/quotesingle.Varlimit /quotesingle.Var,/quotesingle.Varcommon_gen /quotesingle.Var,/quotesingle.Varscicite /quotesingle.Var,/quotesingle.Varblimp - irregular_past_participle_adjectives /quotesingle.Var,/quotesingle.Var
social_i_qa /quotesingle.Var,/quotesingle.Varanli /quotesingle.Var,/quotesingle.Varkilt_zsre /quotesingle.Var,/quotesingle.Varcosmos_qa /quotesingle.Var,/quotesingle.Varsuperglue - record /quotesingle.Var,/quotesingle.Varsquad - with_context /quotesingle.Var,/quotesingle.Varemotion /quotesingle.Var
,/quotesingle.Varblimp - existential_there_quantifiers_ 1 /quotesingle.Var,/quotesingle.Varrace - middle /quotesingle.Var,/quotesingle.Varkilt_wow /quotesingle.Var,/quotesingle.Varsciq /quotesingle.Var,/quotesingle.Varwino_grande /quotesingle.Var,/quotesingle.Var
rotten_tomatoes /quotesingle.Var,/quotesingle.Varsuperglue -cb /quotesingle.Var,/quotesingle.Varpoem_sentiment /quotesingle.Var,/quotesingle.Varropes /quotesingle.Var,/quotesingle.Varreddit_tifu -title /quotesingle.Var,/quotesingle.Varpiqa /quotesingle.Var,/quotesingle.Var
climate_fever /quotesingle.Var,/quotesingle.Varlama - google_re /quotesingle.Var,/quotesingle.Varsearch_qa /quotesingle.Var,/quotesingle.Varwiki_auto /quotesingle.Var,/quotesingle.Varmc_taco /quotesingle.Var,/quotesingle.Varblimp -
wh_questions_object_gap /quotesingle.Var,/quotesingle.Varhotpot_qa /quotesingle.Var,/quotesingle.Varemo/quotesingle.Var,/quotesingle.Varkilt_nq /quotesingle.Var,/quotesingle.Varkilt_trex /quotesingle.Var,/quotesingle.Varquartz - with_knowledge /quotesingle.Var,/quotesingle.Var
dbpedia_ 14 /quotesingle.Var,/quotesingle.Varyahoo_answers_topics /quotesingle.Var,/quotesingle.Varapp_reviews /quotesingle.Var,/quotesingle.Varsuperglue -copa /quotesingle.Var,/quotesingle.Varblimp -
anaphor_gender_agreement /quotesingle.Var,/quotesingle.Varhate_speech 18 /quotesingle.Var,/quotesingle.Vargigaword /quotesingle.Var,/quotesingle.Varmulti_news /quotesingle.Var,/quotesingle.Varaeslc /quotesingle.Var,/quotesingle.Varquail /quotesingle.Var],
3 " dev ": [ /quotesingle.Varcos_e /quotesingle.Var,/quotesingle.Varkilt_fever /quotesingle.Var,/quotesingle.Vareli 5-asks /quotesingle.Var,/quotesingle.Vartrec /quotesingle.Var,/quotesingle.Vareli 5-eli 5 /quotesingle.Var,/quotesingle.Varart/quotesingle.Var,/quotesingle.Varempathetic_dialogues /quotesingle.Var,/quotesingle.Var
tweet_qa /quotesingle.Var,/quotesingle.Varwikisql /quotesingle.Var,/quotesingle.Varlama -trex /quotesingle.Var,/quotesingle.Vartweet_eval - stance_hillary /quotesingle.Var,/quotesingle.Vardiscovery /quotesingle.Var,/quotesingle.Vartweet_eval - emotion /quotesingle.Var,
/quotesingle.Varliar /quotesingle.Var,/quotesingle.Varwiki_bio /quotesingle.Var,/quotesingle.Vardream /quotesingle.Var,/quotesingle.Varade_corpus_v 2-classification /quotesingle.Var,/quotesingle.Varhealth_fact /quotesingle.Var,/quotesingle.Varsamsum /quotesingle.Var,/quotesingle.Var
financial_phrasebank /quotesingle.Var],
4 " test ": [ /quotesingle.Varquoref /quotesingle.Var,/quotesingle.Varwiki_split /quotesingle.Var,/quotesingle.Varethos - disability /quotesingle.Var,/quotesingle.Varsuperglue -rte /quotesingle.Var,/quotesingle.Varglue -cola /quotesingle.Var,/quotesingle.Varethos -
sexual_orientation /quotesingle.Var,/quotesingle.Varblimp - sentential_negation_npi_scope /quotesingle.Var,/quotesingle.Varai2_arc /quotesingle.Var,/quotesingle.Varamazon_polarity /quotesingle.Var,/quotesingle.Varrace -
high /quotesingle.Var,/quotesingle.Varblimp - sentential_negation_npi_licensor_present /quotesingle.Var,/quotesingle.Vartweet_eval -irony /quotesingle.Var,/quotesingle.Varbreak -QDMR /quotesingle.Var,/quotesingle.Var
crawl_domain /quotesingle.Var,/quotesingle.Varglue -qnli /quotesingle.Var,/quotesingle.Varhatexplain /quotesingle.Var,/quotesingle.Varag_news /quotesingle.Var,/quotesingle.Varcirca /quotesingle.Var],
5}
G Manually-Deﬁned Features

--- PAGE 26 ---
Task Name Science Technology Social Network News Web Bio-Medical Review Dialog Books Financial Phrase Sentence Paragraph Extractive Linguistic Commonsense Co-reference World Knowledge Multi-hop Sentence Completion Synthesize
acronym_identiﬁcation 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 0 0
ade_corpus_v2-classiﬁcation 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
ade_corpus_v2-dosage 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0
ade_corpus_v2-effect 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0
adversarialqa 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0
aeslc 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1
ag_news 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
ai2_arc 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0
amazon_polarity 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0
anli 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
app_reviews 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0
aqua_rat 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0
art 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0
aslg_pc12 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1
biomrc 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0
blimp-anaphor_gender_agreement 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0
blimp-anaphor_number_agreement 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0
blimp-determiner_noun_agreement_with_adj_irregular_1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0
blimp-ellipsis_n_bar_1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0
blimp-ellipsis_n_bar_2 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0
blimp-existential_there_quantiﬁers_1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0
blimp-irregular_past_participle_adjectives 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0
blimp-sentential_negation_npi_licensor_present 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0
blimp-sentential_negation_npi_scope 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0
blimp-wh_questions_object_gap 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0
boolq 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0
break-QDMR 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1
break-QDMR-high-level 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1
circa 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0
climate_fever 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0
codah 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0
common_gen 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1
commonsense_qa 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0
cos_e 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0
cosmos_qa 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0
crawl_domain 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1
crows_pairs 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0
dbpedia_14 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
deﬁnite_pronoun_resolution 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0
discovery 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0
dream 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 0
duorc 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0
e2e_nlg_cleaned 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
eli5-askh 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0
eli5-asks 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0
eli5-eli5 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0
emo 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0
emotion 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
empathetic_dialogues 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0
ethos-directed_vs_generalized 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
ethos-disability 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
ethos-gender 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
ethos-national_origin 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
ethos-race 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
ethos-religion 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
ethos-sexual_orientation 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
ﬁnancial_phrasebank 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0
freebase_qa 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0
gigaword 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
glue-cola 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0
glue-mnli 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0
glue-mrpc 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0
glue-qnli 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
glue-qqp 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0
glue-rte 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
glue-sst2 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0
glue-wnli 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0
google_wellformed_query 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0
hate_speech18 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
hate_speech_offensive 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
hatexplain 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
health_fact 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0
hellaswag 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0
hotpot_qa 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0
imdb 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0
jeopardy 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0
kilt_ay2 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0
kilt_fever 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0
kilt_hotpotqa 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0
kilt_nq 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0
kilt_trex 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0
kilt_wow 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0
kilt_zsre 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0
lama-conceptnet 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0
lama-google_re 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0
lama-squad 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0
lama-trex 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0
liar 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0
limit 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0
math_qa 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0
mc_taco 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0
medical_questions_pairs 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0
mocha 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0
multi_news 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1
numer_sense 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0
onestop_english 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
openbookqa 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0
paws 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0
piqa 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0
poem_sentiment 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0
proto_qa 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0
qa_srl 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0
qasc 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0
quail 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0
quarel 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0
quartz-no_knowledge 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0
quartz-with_knowledge 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0
quoref 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0
race-high 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0
race-middle 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0
reddit_tifu-title 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1
reddit_tifu-tldr 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1
ropes 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0
rotten_tomatoes 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0
samsum 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1
scicite 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
sciq 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0
scitail 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
search_qa 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0
sick 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
sms_spam 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
social_i_qa 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0
spider 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1
squad-no_context 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0
squad-with_context 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0
superglue-cb 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0
superglue-copa 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0
superglue-multirc 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0
superglue-record 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0
superglue-rte 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
superglue-wic 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0
superglue-wsc 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0
swag 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0
tab_fact 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0
trec 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
trec-ﬁnegrained 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-emoji 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1
tweet_eval-emotion 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-hate 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-irony 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-offensive 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-sentiment 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-stance_abortion 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-stance_atheism 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-stance_climate 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-stance_feminist 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_eval-stance_hillary 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
tweet_qa 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
web_questions 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0
wiki_auto 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0
wiki_bio 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1
wiki_qa 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
wiki_split 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 1
wikisql 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1
wino_grande 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 1 0
wiqa 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0
xsum 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1
yahoo_answers_topics 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
yelp_polarity 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0
yelp_review_full 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0
Table 9: Full feature table used for analysis in §7.
