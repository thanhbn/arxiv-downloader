EvoMoE: Một Khung Đào Tạo Mixture-of-Experts Tiến Hóa
thông qua Cổng Dày-Thưa-Thớt

Xiaonan Nie, Xupeng Miao, Shijie Cao, Lingxiao Ma, Qibin Liu, Jilong Xue,
Youshan Miao, Yi Liu, Zhi Yang, Bin Cui

Trường Khoa học Máy tính & Phòng thí nghiệm Trọng điểm Công nghệ Phần mềm Tin cậy cao (MOE), Đại học Bắc Kinh
Đại học Carnegie Mellon, Microsoft Research, Tencent Inc
Viện Khoa học Xã hội Tính toán, Đại học Bắc Kinh (Thanh Đảo), Trung Quốc

Tóm tắt - Mixture-of-experts (MoE) đang trở nên phổ biến do sự thành công trong việc cải thiện chất lượng mô hình, đặc biệt là trong các Transformer. Bằng cách định tuyến token với cổng thưa thớt đến một vài chuyên gia (tức là, một phần nhỏ của mô hình đầy đủ), MoE có thể dễ dàng tăng các tham số mô hình lên quy mô rất lớn trong khi vẫn giữ chi phí tính toán ở mức không đổi. Hầu hết các công trình hiện tại chỉ khởi tạo một số chuyên gia ngẫu nhiên, thiết lập một chiến lược cổng cố định (ví dụ, Top-k), và đào tạo mô hình từ đầu theo cách tùy ý. Chúng tôi nhận định rằng những mô hình MoE này đang gặp khó khăn từ các chuyên gia chưa trưởng thành và cổng thưa thớt không ổn định, có hại cho hiệu suất hội tụ.

Trong bài báo này, chúng tôi đề xuất một khung đào tạo MoE end-to-end hiệu quả có tên EvoMoE. EvoMoE bắt đầu từ việc đào tạo một chuyên gia duy nhất và dần dần tiến hóa thành một cấu trúc MoE lớn và thưa thớt. EvoMoE chủ yếu bao gồm hai giai đoạn: giai đoạn đa dạng hóa chuyên gia để đào tạo chuyên gia cơ sở trong một thời gian và sinh ra nhiều chuyên gia đa dạng từ nó, và giai đoạn thưa thớt hóa cổng để học một cổng thưa thớt thích ứng và kích hoạt một số lượng chuyên gia động. EvoMoE tự nhiên tách rời việc học chung của cả chuyên gia và cổng thưa thớt và tập trung vào việc học kiến thức cơ bản với một chuyên gia duy nhất ở giai đoạn đầu đào tạo. Sau đó nó đa dạng hóa các chuyên gia và tiếp tục đào tạo MoE với một cổng Dày-thành-Thưa thớt mới (DTS-Gate). Cụ thể, thay vì sử dụng một cổng thưa thớt vĩnh viễn, DTS-Gate bắt đầu như một cổng dày đặc định tuyến token đến tất cả chuyên gia, sau đó dần dần và thích ứng trở nên thưa thớt hơn trong khi định tuyến đến ít chuyên gia hơn. Đánh giá được tiến hành trên ba mô hình và tác vụ phổ biến, bao gồm RoBERTa cho tác vụ mô hình hóa ngôn ngữ có mặt nạ, GPT cho tác vụ mô hình hóa ngôn ngữ và Transformer cho tác vụ dịch máy. Kết quả cho thấy EvoMoE vượt trội hơn các baseline hiện tại, bao gồm Switch, BASE Layer, Hash Layer và StableMoE. Cụ thể, EvoMoE vượt trội hơn các phương pháp MoE khác trên benchmark GLUE lên đến 0.562 và 0.403 trung bình.

I. GIỚI THIỆU

Kiến trúc mô hình Transformer đang trở nên ngày càng quan trọng trong khai thác dữ liệu và đã đạt được những kết quả ấn tượng trong nhiều ứng dụng, như xử lý ngôn ngữ tự nhiên, thị giác máy tính, học đồ thị, và hệ thống gợi ý. Gần đây, có xu hướng cải thiện khả năng của các mô hình Transformer thông qua việc mở rộng quy mô dữ liệu và mô hình. Cụ thể, nghiên cứu đã khám phá quy luật mở rộng của các mô hình transformer cho thấy hiệu suất mô hình tỷ lệ như một luật lũy thừa với kích thước dữ liệu, kích thước mô hình và tính toán. Tuy nhiên, với sự gia tăng nhanh chóng của kích thước mô hình, việc mở rộng mô hình lên quy mô cực lớn trở nên khó khăn do sức mạnh tính toán hạn chế của các thiết bị phần cứng có sẵn. Để giải quyết những thách thức này, Mixture-of-Experts (MoE) có cổng thưa thớt, một dạng phổ biến của tính toán có điều kiện, đã được đề xuất để tăng kích thước mô hình mà không tăng chi phí tính toán (ví dụ, FLOPs) tỷ lệ thuận. Cụ thể, các token đầu vào được định tuyến bởi một cổng thưa thớt đến một vài chuyên gia, dẫn đến chi phí tính toán thấp hơn so với một mô hình dày đặc có cùng kích thước mô hình.

Sự thành công của mô hình MoE phụ thuộc vào cả dung lượng mô hình lớn được giới thiệu bởi nhiều chuyên gia và các kết nối định tuyến thưa thớt phức tạp được học bởi mạng cổng. Nhiều công trình hiện tại đang khám phá các mạng cổng mới để cải thiện chất lượng mô hình hoặc hiệu quả đào tạo. Họ thường áp dụng một kiến trúc cổng thưa thớt được định trước (ví dụ, Top-1 hoặc Top-2 với một số lượng chuyên gia được kích hoạt cố định), và sau đó đào tạo các tham số mô hình của cả cổng và chuyên gia cùng nhau từ đầu. Tuy nhiên, việc đào tạo chung như vậy trên kiến trúc thưa thớt được định trước có thể hạn chế nghiêm trọng chất lượng mô hình, và thậm chí cả hiệu quả đào tạo. Đặc biệt, khi bắt đầu đào tạo một mô hình MoE, cả cổng và các chuyên gia đều được khởi tạo ngẫu nhiên. Cổng không có bằng chứng để quyết định chuyên gia nào sẽ xử lý một token đầu vào, và các chuyên gia cũng không có kinh nghiệm để xử lý một token đầu vào được gán ngẫu nhiên. Đào tạo tất cả chuyên gia từ trạng thái ngẫu nhiên với các mẫu được định tuyến ngẫu nhiên đòi hỏi một quá trình khởi động dài và trùng lặp. Hơn nữa, những cổng được định trước này hạn chế MoE chỉ khám phá 1 hoặc 2 chuyên gia tại một thời điểm. Nhưng trong giai đoạn đầu với một cổng chưa trưởng thành, những cơ hội nhỏ như vậy có thể dễ dàng bị ảnh hưởng bởi các nhiễu định tuyến ngẫu nhiên, và việc định tuyến không phù hợp thậm chí có thể được củng cố trong thời gian dài. Quan sát của chúng tôi cho thấy rằng việc định tuyến ngẫu nhiên như vậy trong giai đoạn ban đầu và việc cập nhật mô hình dựa trên củng cố tầm xa trong các phương pháp hiện tại có thể ảnh hưởng đến cả thời gian đào tạo và chất lượng mô hình cuối cùng.

Trong bài báo này, để vượt qua những hạn chế trong các phương pháp hiện tại, chúng tôi xem xét lại quá trình học của các mô hình MoE và ủng hộ một mô hình đào tạo end-to-end đơn giản nhưng hiệu quả, có tên EvoMoE. Thay vì trực tiếp đào tạo từ một kiến trúc thưa thớt được định trước, EvoMoE dần dần tiến hóa thành một kiến trúc MoE đa dạng và thưa thớt từ một mô hình ban đầu trong hai giai đoạn: đa dạng hóa chuyên gia và thưa thớt hóa cổng. Cụ thể, chúng tôi nhận thấy rằng cả cổng và các chuyên gia đều hoạt động kém trong đào tạo MoE do đó dẫn đến hiệu suất định tuyến không ổn định. Được thúc đẩy bởi thành công của các mô hình chia sẻ trọng số, trong giai đoạn đầu, chúng tôi giới thiệu cơ chế đa dạng hóa chuyên gia để học kiến thức được chia sẻ chung giữa các chuyên gia khác nhau. Cơ chế đề xuất của chúng tôi chỉ đào tạo một chuyên gia chung với tất cả token đầu vào lúc đầu, có thể được xem như việc chia sẻ các tham số mô hình qua tất cả các chuyên gia. Để bao gồm tính đa dạng của những chuyên gia này, chúng tôi sau đó ngẫu nhiên nhiễu loạn mỗi chuyên gia với các mặt nạ khác nhau như trạng thái mô hình ban đầu của các bước đào tạo tiếp theo. Trong giai đoạn thưa thớt hóa cổng, ràng buộc chia sẻ trọng số được giải phóng và việc đào tạo MoE chuyển sang cách thức được kích hoạt thưa thớt trên những chuyên gia đa dạng này. Không giống như cổng thưa thớt được định trước trong các công trình trước đây, chúng tôi giới thiệu cổng DTS (Dày-thành-Thưa thớt) để quyết định cổng thưa thớt dần dần cho các mô hình MoE. Chúng tôi đề xuất cổng DTS để thích ứng học một mạng cổng tốt hơn từ một cổng dày đặc và dần dần định tuyến token đến ít chuyên gia hơn, làm cho cấu trúc đào tạo thưa thớt hơn và liên tục giảm chi phí tính toán, trong khi vẫn giữ chất lượng mô hình cải thiện như thường lệ. Cụ thể, để thực hiện cổng DTS, ý tưởng của chúng tôi là cẩn thận kiểm soát nhiệt độ của một hàm định tuyến dựa trên softmax, để điều chỉnh phân bố trọng số giữa các chuyên gia và kiểm soát tính thưa thớt của lớp MoE trong quá trình đào tạo.

Tóm lại, EvoMoE tiến bộ trong hai khía cạnh. Thứ nhất, so với việc đào tạo chung của cổng và chuyên gia từ đầu, EvoMoE tách quá trình đào tạo chung và cung cấp cơ hội để đào tạo chuyên gia trong một khởi đầu ấm áp. Cơ chế đào tạo cổng sau chuyên gia như vậy có thể giảm rất nhiều thử nghiệm lỗi ngẫu nhiên ở đầu. Thứ hai, so với việc cập nhật mô hình dựa trên củng cố, bắt đầu với một cổng dày đặc cho phép chúng tôi nhận được phản hồi đào tạo từ tất cả các chuyên gia đa dạng và điều chỉnh trọng số định tuyến trực tiếp theo hướng đúng, không chỉ tăng tốc độ hội tụ của cổng, mà còn có lợi cho việc chuyên môn hóa chuyên gia.

Chúng tôi đánh giá EvoMoE trên ba mô hình và tác vụ phổ biến, bao gồm RoBERTa (Chỉ-Encoder) cho tác vụ mô hình hóa ngôn ngữ có mặt nạ (MLM), GPT (Chỉ-Decoder) cho tác vụ mô hình hóa ngôn ngữ (LM) và Transformer (Encoder-Decoder) cho tác vụ dịch máy (MT). Kết quả cho thấy EvoMoE vượt trội hơn các baseline hiện tại, bao gồm Switch, BASE Layer, Hash Layer và StableMoE. Cụ thể, trên tác vụ MLM, EvoMoE vượt trội hơn các phương pháp MoE khác lên đến 0.562 điểm GLUE và 0.403 trung bình cho benchmark GLUE; trên tác vụ LM, EvoMoE vượt trội hơn các phương pháp MoE khác lên đến 0.88 ppl và 0.545 ppl trung bình; trên tác vụ dịch thuật, EvoMoE có thể đạt được cải thiện 1.0 điểm BLEU trung bình cũng như tăng tốc 1.33x so với Switch Transformer. Các thí nghiệm cũng xác minh khả năng của EvoMoE trong việc mở rộng mô hình với nhiều chuyên gia hơn hoặc nhiều lớp MoE hơn.

Phần còn lại của bài báo được tổ chức như sau. Chúng tôi trước tiên giới thiệu nền tảng về Transformers và MoEs trong Mục II. Và chúng tôi xác định hai khuyết điểm chính trong quy trình đào tạo MoE hiện tại bao gồm sự phù hợp và tính không ổn định trong Mục II-C. Được thúc đẩy bởi những tính chất này, chúng tôi trình bày thiết kế EvoMoE của chúng tôi trong Mục III và giới thiệu giai đoạn đa dạng hóa chuyên gia và giai đoạn thưa thớt hóa cổng tương ứng. Mục IV mô tả một số chi tiết thực hiện. Chúng tôi cung cấp phương pháp đánh giá và tiến hành các thí nghiệm đáng kể dưới các cài đặt khác nhau trong mục V để hỗ trợ các tuyên bố của chúng tôi. Các phương pháp liên quan khác được giới thiệu trong Mục VI. Cuối cùng, chúng tôi cung cấp một số nhận xét kết luận trong mục VII.

II. CHUẨN BỊ

A. Transformer

Kiến trúc mô hình Transformer đã chứng minh hiệu suất vượt trội trong nhiều tác vụ chuỗi-tới-chuỗi trong xử lý ngôn ngữ tự nhiên (NLP), chứa nhiều lớp encoder và decoder. Mỗi lớp encoder được xếp chồng bởi một mạng self-attention đa đầu và một mạng feed-forward theo vị trí (FFN), được minh họa trong Hình 2(a). Nó sử dụng một kết nối dư trên mỗi trong hai lớp phụ này, theo sau bởi một lớp chuẩn hóa. Chính thức, mỗi lớp phụ, ví dụ, attention và FFN, tạo ra đầu ra của nó như LayerNorm(x+Sublayer(x)). Decoder được xây dựng tương tự, ngoại trừ cơ chế cross-attention bổ sung giữa attention và FFN để giới thiệu đầu ra của encoder. Đối với một chuỗi token đầu vào (x1;:::;xs) ∈ RD, chúng tôi công thức hóa hàm của mỗi lớp phụ như sau:

Attention: Module attention có thể nắm bắt các phụ thuộc giữa các token trong chuỗi, và hiệu quả trong mô hình hóa chuỗi. Nó thực hiện như một hàm ba ngôi, ánh xạ các truy vấn đầu vào (Q), khóa (K) và giá trị (V) đến đầu ra (O). Phương trình 1 biểu diễn Scaled Dot-Product Attention, thực hiện tích vô hướng của mỗi truy vấn với tất cả các khóa, chia mỗi cái cho √dk và sau đó áp dụng hàm softmax để có được trọng số của mỗi giá trị. Ngoài ra, dk là chiều của các truy vấn và khóa.

Attention(Q,K,V) = softmax(QKT/√dk)V (1)

Multi-Head Attention: Vaswani et al. đề xuất cơ chế multi-head attention để cùng nhau học từ các không gian biểu diễn khác nhau tại các vị trí khác nhau và do đó cải thiện hiệu suất mô hình. Multi-head attention chiếu tuyến tính các truy vấn, khóa và giá trị h lần với các chiếu tuyến tính đã học đến dk, dk và dv chiều, tương ứng.

MultiHead(Q,K,V) = Concat(head1,...,headh)WO (2)

trong đó headi = Attention(QWiQ,KWiK,VWiV)

Các chiếu là các ma trận tham số có thể huấn luyện, trong đó WiQ ∈ Rdmodel×dk, WiK ∈ Rdmodel×dk, WiV ∈ Rdmodel×dv. Trong khi đó, h là số lượng đầu, và dk = dv = dmodel/h. Vì chiều của mỗi đầu được giảm từ dmodel xuống dmodel/h, chi phí thời gian của multi-head attention tương tự như attention gốc. Ngoài ra, decoder sử dụng một self-attention có mặt nạ, chỉ nhìn thấy các token ở phía trái của chuỗi.

Position-wise Feed-Forward Networks: Mỗi lớp transformer cũng bao gồm một mạng feed-forward được kết nối đầy đủ (Phương trình 3), bao gồm hai mạng được kết nối đầy đủ và một hàm kích hoạt ReLU.

FFN(xs) = W2ReLU(W1xs + b1) + b2 (3)

B. Mixture of Experts

Vì các mô hình được huấn luyện trước lớn hơn luôn đạt được chất lượng mô hình tốt hơn, kích thước của các mô hình NLP hiện đại đã tăng 10× mỗi năm, ví dụ, BERT, GPT, T5, GPT-2, GPT-3, đòi hỏi ngân sách tính toán ngày càng tăng. Để cải thiện khả năng mô hình mà không tăng ngân sách tính toán, các nhà nghiên cứu gần đây đã mở rộng thưa thớt các transformer như Hình 2(b) bằng cách thay thế mạng feed-forward bằng kiến trúc mixture of experts (MoE) và chỉ kích hoạt một tập con của những chuyên gia này cho mỗi mẫu đầu vào. Các thành phần chính của kiến trúc MoE bao gồm một mạng chuyên gia E để mở rộng khả năng mô hình và một mạng cổng thưa thớt G để giới thiệu tính thưa thớt mô hình.

Expert Network: Mạng chuyên gia E bao gồm một chuỗi các chuyên gia {e1;:::;eN} để tăng khả năng mô hình, trong đó mỗi chuyên gia ei đại diện cho một mạng nơ-ron đơn, ví dụ, FFN, và chứa các tham số riêng của nó. Trong Hình 3, lớp MoE bao gồm ba mạng FFN. Đối với mỗi chuyên gia ei (ei: RD → RD), nó lấy token xs làm đầu vào để tạo ra đầu ra riêng ei(xs). Đầu ra cuối cùng của mạng chuyên gia ys là tổ hợp có trọng số tuyến tính của đầu ra mỗi chuyên gia trên token bằng đầu ra của cổng, được công thức hóa như Phương trình 4.

ys = ΣN(i=1) g(xs)i ei(xs) (4)

Trong Hình 3, mạng chuyên gia lấy token đầu vào xs: [0.2; 0.4; 1.5] và tạo ra đầu ra của mỗi chuyên gia riêng lẻ trên xs, ví dụ, e0(xs), e1(xs) và e2(xs). Bằng cách kết hợp đầu ra của cổng, tức là, [0.35; 0.65; 0], đầu ra của lớp MoE này là ys = 0.35 × e0(xs) + 0.65 × e1(xs).

Sparse Gate Network: Mạng cổng thưa thớt G là thành phần chính để giới thiệu tính thưa thớt mô hình, lấy một batch token {x1;:::;xs} làm đầu vào và tạo ra xác suất của chúng đối với tất cả các chuyên gia {e1;:::;eN}. Shazeer et al. đề xuất cổng Top-K như Phương trình 5, chỉ giữ lại k giá trị hàng đầu trước hàm softmax. Ngoài ra, Wg là một biến có thể huấn luyện (Wg ∈ RD×N) và xác định các chuyên gia được nhắm mục tiêu cho mỗi token.

g(xs) = softmax(TopK(xs Wg)) (5)

Chúng tôi minh họa quy trình làm việc của một lớp MoE trong Hình 3, trong đó k = 2 và Wg là một ma trận 3×3 (tức là, số chiều tính năng × số chuyên gia) để biểu diễn tham số của mạng cổng. Chúng tôi trước tiên thực hiện một tích vô hướng trên xs và Wg để tính toán độ tương tự giữa token đầu vào và các chuyên gia. Kết quả, [2.01; 2.64; 1.8], chỉ ra rằng đầu vào ưa thích e1 > e0 > e2 và chúng tôi chỉ kích hoạt e0 và e1 vì k = 2. Cuối cùng, chúng tôi tiến hành một hàm softmax để có được điểm trọng số của mỗi chuyên gia và thực hiện một tổng có trọng số để có được đầu ra cuối cùng ys.

Công việc trước đây chủ yếu tập trung vào cách cải thiện chất lượng và hiệu quả của việc đào tạo mạng cổng thưa thớt như vậy. Shazeer et al. đề xuất cổng noisy Top-K trên các lớp Long Short-Term Memory (LSTM) và Lepikhin et al. giới thiệu MoE với cổng Top-2 vào Transformer. Lewis et al. áp dụng giải pháp số cho định tuyến cân bằng token-tới-chuyên gia và Roller et al. sử dụng chiến lược định tuyến dựa trên hash.

Distributed Training of MoE Models: Đào tạo song song chuyên gia là một phương pháp song song cụ thể cho các mô hình MoE, được đề xuất lần đầu bởi GShard. Các chuyên gia được đặt trên các worker khác nhau và mỗi worker lấy một batch mẫu đào tạo khác nhau. Đối với các lớp không phải MoE, song song chuyên gia hoạt động giống như song song dữ liệu. Trong các lớp MoE, các token trong chuỗi được gửi đến các worker nơi các chuyên gia mong muốn của chúng cư trú. Tương tự như song song mô hình, đầu ra của mỗi lớp MoE được trao đổi lại để được tổ chức lại thành các chuỗi ban đầu cho tính toán của lớp tiếp theo. Vì các mô hình MoE thường có nhiều chuyên gia, song song chuyên gia có thể mở rộng với kích thước mô hình tốt hơn so với song song mô hình.

C. Quan sát và Động lực

Trong phần này, chúng tôi xem xét lại quá trình học của các mô hình MoE và giới thiệu hai phát hiện chính của chúng tôi như sau, điều này thúc đẩy chúng tôi thiết kế khung EvoMoE.

Conformity trong Mixture of Experts: Một phát hiện thú vị là conformity. Trong giai đoạn đào tạo đầu, các phương pháp đào tạo chung hiện tại của MoE thưa thớt khiến quyết định định tuyến tuân theo hầu hết các token. Ở đây chúng tôi đào tạo một mô hình GPT bao gồm 24 lớp transformer, với mỗi lớp FFN được thay thế bởi lớp MoE 16-chuyên gia sử dụng cổng Top-1.

Hình 4(a) cho thấy rằng hầu hết các token vẫn tập trung vào chuyên gia thứ 8 lúc đầu, vì nó đã được củng cố một cách tham lam. Sau khoảng hàng trăm bước đào tạo (tức là, 1 duration bằng 40 bước đào tạo), các chuyên gia khác dần dần bắt kịp và khối lượng công việc trở nên cân bằng. Hiện tượng như vậy thúc đẩy chúng tôi tập trung vào việc đào tạo một chuyên gia chung và sử dụng các tài nguyên tính toán để tăng tốc giai đoạn đầu.

Instability trong Mixture of Experts: Một phát hiện quan trọng khác là tính không ổn định. Chúng tôi lấy một token đơn "the" làm ví dụ và Hình 4(b) cho thấy kết quả lựa chọn chuyên gia của nó trong một quá trình đào tạo dài hơn. Như chúng ta có thể thấy, việc lựa chọn rất không ổn định sau này vì cả mạng cổng và các chuyên gia đều không đủ hiểu biết để có được một mô hình định tuyến ổn định, đặc biệt là ở giai đoạn đầu của quá trình đào tạo. Điều này chỉ ra rằng các cổng được định trước (ví dụ, Top-K) trong các công trình hiện tại, giả định một số lượng chuyên gia được kích hoạt cố định, có thể hạn chế việc khám phá các chuyên gia có giá trị tiềm năng. Việc tăng một cách tích cực số lượng chuyên gia được kích hoạt có thể cải thiện khả năng mô hình nhưng về bản chất vi phạm ý định thiết kế ban đầu của MoE thưa thớt. Một tình huống khó xử như vậy thúc đẩy chúng tôi thiết kế một giải pháp thích ứng để cân bằng sự đánh đổi giữa hiệu suất hội tụ và chi phí tính toán.

III. PHƯƠNG PHÁP

Các quan sát trong Mục II-C thúc đẩy EvoMoE, một khung hai giai đoạn đào tạo dần dần và thích ứng các mô hình dựa trên MoE, khác với các phương pháp hiện tại đào tạo chung mạng cổng và mạng chuyên gia trên một cổng thưa thớt được định trước (ví dụ, Top-1 hoặc Top-2) và một chuỗi các chuyên gia được khởi tạo ngẫu nhiên. Như được hiển thị trong Hình 1, EvoMoE bao gồm hai giai đoạn: giai đoạn đa dạng hóa chuyên gia và giai đoạn thưa thớt hóa cổng. Trong giai đoạn đa dạng hóa chuyên gia, EvoMoE chia sẻ các trọng số giữa các chuyên gia trong một lớp MoE cho một số bước đào tạo và sau đó làm cho các chuyên gia đa dạng bằng cách che ngẫu nhiên. Trong giai đoạn thưa thớt hóa cổng, EvoMoE giới thiệu cổng dày-thành-thưa thớt (tức là, DTS), bắt đầu định tuyến như một cổng dày đặc định tuyến token đến tất cả các chuyên gia và sau đó thích ứng học các trọng số định tuyến đến mỗi chuyên gia và dần dần làm mềm thành cổng Top-1 tiêu chuẩn.

A. Công thức hóa Vấn đề

Cho một token đầu vào xs, một chuỗi các chuyên gia {e1;:::;eN} và một cổng có thể học với tham số Wg, Func được áp dụng bởi mạng cổng để xác định các chuyên gia được nhắm mục tiêu cho nó, tức là, phép gán token-tới-chuyên gia, được công thức hóa trong Phương trình 6. g(xs) là một vector 1×N, biểu diễn điểm số của xs đối với các chuyên gia. Trong khi đó, mỗi chuyên gia sẽ xử lý token đầu vào riêng biệt như ei(xs) và kết hợp đầu ra của chúng như Phương trình 7.

g(xs) = Func(xs Wg) (6)

ys = ΣN(i=1) g(xs)i ei(xs) (7)

Công việc hiện tại áp dụng một Top-K được định trước làm Func, như Top-2 cho GShard và Top-1 cho Switch-Transformer. Tuy nhiên, do tính không thể vi phân của Top-K, chỉ có các chuyên gia được chọn mới sẽ lan truyền ngược gradient của họ đến mạng cổng và cập nhật các cột tương ứng trong Wg. Ví dụ, chỉ có 1 chuyên gia được chọn và 1 cột của cổng sẽ được cập nhật trong Switch-Transformer. Vì vậy, việc tối ưu hóa vấn đề lựa chọn chuyên gia này khó khăn đối với cổng Top-K. Hơn nữa, như quan sát trong Hình 4(a), tải của các chuyên gia cực kỳ mất cân bằng ở giai đoạn đầu đào tạo và do đó hầu hết các GPU gặp phải tỷ lệ sử dụng thấp do stragglers trong song song chuyên gia.

B. Giai đoạn 1: Đa dạng hóa Chuyên gia

Vì mạng cổng và mạng chuyên gia đều được khởi tạo ngẫu nhiên, nó đòi hỏi một lượng lớn ngân sách tính toán cho thử nghiệm và lỗi, không hiệu quả cho việc đào tạo mô hình. Dựa trên quan sát trong Mục II-C rằng hầu hết các token được xử lý bởi cùng một chuyên gia và các chuyên gia khác lãng phí ngân sách tính toán của họ, chúng tôi đào tạo một chuyên gia được chia sẻ thay vì N chuyên gia riêng lẻ trong giai đoạn đầu (được minh họa như phía trái của Hình 1). Vì tất cả các chuyên gia trong cùng một lớp MoE chia sẻ trọng số, mô hình bằng với mô hình không phải MoE tương ứng như một mô hình dày đặc nhỏ.

Thuật toán 1 minh họa quy trình đào tạo MoE trong khung EvoMoE của chúng tôi. Đầu tiên, các token đầu vào được xử lý bởi chuyên gia được chia sẻ e0 (dòng 1-2). Sau đó EvoMoE chuyển việc đào tạo thành đào tạo mô hình MoE tiêu chuẩn, bằng cách thêm một mạng cổng tại mỗi lớp MoE và đa dạng hóa tất cả các chuyên gia từ chuyên gia được chia sẻ (dòng 4-5). Sau giai đoạn đa dạng hóa chuyên gia này, EvoMoE bước vào giai đoạn thưa thớt hóa cổng, nơi nó lên lịch các hệ số nhiệt độ cổng và sau đó thu được mối quan hệ định tuyến token-tới-chuyên gia từ DTS-gate (dòng 7-9). Các token sẽ được gửi đến các chuyên gia tương ứng và tổng hợp lại với nhau bằng phép toán tổng có trọng số (dòng 10-12).

Nhiều kỹ thuật đa dạng hóa có thể được áp dụng để sinh ra nhiều chuyên gia đa dạng từ một chuyên gia, như nhiễu, NAS, mặt nạ ngẫu nhiên. EvoMoE áp dụng mặt nạ ngẫu nhiên, che một phần trọng số của chuyên gia được chia sẻ thành 0 (được hiển thị như Hình 5). Ví dụ, chuyên gia-1 được khởi tạo bằng cách che giá trị trung tâm. Giai đoạn đa dạng hóa chuyên gia được đề xuất tránh đào tạo chung từ đầu và các chuyên gia đa dạng được đào tạo tốt có thể được xem như một khởi tạo tốt hơn để có lợi cho sự hội tụ mô hình tiếp theo.

C. Giai đoạn 2: Thưa thớt hóa Cổng

Mặc dù cổng thưa thớt đã chứng minh hiệu quả mô hình vượt trội trong cả đào tạo và suy luận, công việc trước đây có xu hướng hội tụ đến một mô hình dưới tối ưu dưới ngân sách tính toán cố định hoặc kích thước tập dữ liệu do việc đào tạo chung của mạng cổng và mạng chuyên gia được khởi tạo ngẫu nhiên. Trong bài báo này, chúng tôi đề xuất một cơ chế mới để đào tạo mạng cổng, có tên Dense-to-Sparse gate (DTS-Gate, như được minh họa trong Thuật toán 2), bắt đầu như một cổng dày đặc định tuyến token đến hầu hết các chuyên gia và sau đó dần dần trở nên thưa thớt hơn. DTS-Gate có lợi từ việc đào tạo đầy đủ các chuyên gia trong giai đoạn đầu và sau đó làm cho việc lựa chọn chuyên gia trở nên thưa thớt hơn dựa trên các chuyên gia chuyên môn. Quá trình dày-thành-thưa thớt này chỉ chiếm một phần nhỏ so với tổng thời gian đào tạo, thường mất từ ngày đến tuần.

Gate with Temperature: Để kiểm soát tính thưa thớt trong quá trình đào tạo, chúng tôi áp dụng nhiệt độ softmax để điều chỉnh phân bố trọng số giữa các chuyên gia. Được công thức hóa như Phương trình 8, Wg là tham số của cổng, ϵ là nhiễu bổ sung và được lấy mẫu từ phân bố Gumbel(0,1), và τ là nhiệt độ softmax kiểm soát phân bố. Khi τ tăng, phân bố của g'(xs) trở nên đồng đều hơn, phát triển nhiều chuyên gia hơn vào tính toán của mỗi token. Khi τ tiếp cận 0, phân bố trở thành one-hot, tự tin hơn cho mạng cổng.

g'(xs) = e^((xs Wg + ϵ)/τ) / Σ^N_(s'=1) e^((xs' Wg + ϵ)/τ) (8)

Content-based Sparsity: Khác với cổng Top-K tĩnh hiện tại, EvoMoE áp dụng phương pháp thưa thớt dựa trên nội dung để xác định số lượng chuyên gia được kích hoạt, giữ lại giá trị vượt quá một ngưỡng c. Như được công thức hóa bởi Phương trình 9, chúng tôi loại bỏ các chuyên gia có trọng số dưới ngưỡng c và không có thông tin liên lạc hoặc tính toán bổ sung nào sẽ bị lãng phí. Điều đáng chú ý là tổng điểm số của các chuyên gia được chọn không thể bằng 1 vì chúng tôi không chuẩn hóa chúng sau khi loại bỏ. Việc giữ lại điểm số ban đầu là hữu ích, đặc biệt khi chỉ có một chuyên gia được chọn, điều này đã được xác minh trong Switch transformer.

g(xs)i = {
  g'(xs)i, if g'(xs)i > c
  0, else
} (9)

Sparsity Scheduler: Với nhiệt độ tăng, phân bố có xu hướng đồng đều và nhiều chuyên gia hơn sẽ được chọn. Vì vậy, tính thưa thớt giảm và chi phí đào tạo của mạng nơ-ron sẽ tăng. Ngược lại, ít chuyên gia hơn được tham gia vào tính toán và do đó tính thưa thớt tăng. Bằng cách lên lịch nhiệt độ của Phương trình 8, chúng ta có thể kiểm soát tính thưa thớt của lớp MoE qua các giai đoạn đào tạo khác nhau. Có một sự đánh đổi giữa chất lượng mô hình và chi phí đào tạo cho việc lựa chọn nhiệt độ. Ví dụ, khi phân bố của các chuyên gia gần như one-hot, nó sẽ dẫn đến phương sai lớn của gradient giữa các chuyên gia và do đó làm cho việc học MoE trở nên khó khăn. Để tối ưu hóa vấn đề này, DTS-Gate của chúng tôi bắt đầu ở nhiệt độ lớn định tuyến token đến hầu hết các chuyên gia và sau đó làm mềm đến nhiệt độ nhỏ dần dần làm thưa thớt lớp MoE.

Balance Loss: Tương tự như Switch transformer, chúng tôi sử dụng balance loss L_balance để tránh các phép gán mất cân bằng cho các chuyên gia khác nhau có thể gây ra vấn đề straggler và do đó dẫn đến hiệu quả đào tạo thấp.

L_balance = α N Σ^N_(i=1) (Σ_(xs∈B) I{g(xs)i > 0} / |B|) × (Σ_(xs∈B) g'(xs)i) (10)

Như được công thức hóa trong Phương trình 10, α là một siêu tham số và B đại diện cho batch token hiện tại. Σ_(xs∈B) I{g(xs)i > 0} đại diện cho số lượng token được gửi đến chuyên gia i và Σ_(xs∈B) g'(xs)i đại diện cho xác suất cổng được phân bổ cho chuyên gia i. Trực giác, balance loss sẽ giảm lượng dữ liệu cho các chuyên gia quá tải và tiến tới tải cân bằng ở mức dữ liệu batch.

Training Objective: Trong giai đoạn đầu, các chuyên gia của mỗi lớp MoE chia sẻ cùng trọng số và do đó tải có thể được chia cho chúng một cách đồng đều. Mục tiêu đào tạo là tối ưu hóa chất lượng mô hình (tức là, L_task). Trong giai đoạn thứ hai, cả chất lượng mô hình và hiệu quả đào tạo (tức là, khối lượng công việc cân bằng giữa các chuyên gia) cần được xem xét.

L = {
  L_task, if stage = 1
  L_task + L_balance, else
} (11)

IV. THỰC HIỆN

EvoMoE được thực hiện bằng cách thêm hỗ trợ cho các mô hình MoE trên FairSeq. Trong khi đó, EvoMoE đề xuất một số tối ưu hóa hệ thống, bao gồm:

Topology-Aware Hierarchical All-To-All Communication: Trong phép toán AllToAll, mỗi GPU gửi dữ liệu của nó đến tất cả các GPU (one-for-all) và nhận dữ liệu được gửi bởi tất cả các GPU (all-for-one), trong đó mỗi dữ liệu sẽ được chia đều thành n phần. Các phép toán AllToAll hiện tại được thực hiện trong NCCL và MPI có thể gặp phải việc sử dụng băng thông mạng thấp vì kích thước thông điệp nhỏ. Chúng tôi đề xuất Topology-Aware Hierarchical AllToAll, kết hợp các mạng phân cấp (intra-node và inter-node) và tổng hợp thông điệp, để tối ưu hóa giao tiếp giữa các node đa được trang bị một NIC. Nó trước tiên thu thập dữ liệu của tám GPU bên trong cùng một node vào một GPU, và sau đó thực hiện một phép biến đổi bố cục dữ liệu để tổ chức vị trí của các token. Sau đó, nó khởi chạy giao tiếp All-To-All giữa các node, và sau đó thực hiện phép biến đổi bố cục dữ liệu và phân tán các token đến chuyên gia tương ứng của nó. Theo cách này, kích thước dữ liệu được chuyển giữa các node lớn hơn #GPU² lần so với trước đây. Trong khi đó, AllToAll tách rời hai cấp này cũng sử dụng đầy đủ băng thông intra-node (NvLink hoặc PCIe) và inter-node (Infiniband hoặc Ethernet).

MoE-Aware Recomputation: Recomputation là một kỹ thuật chính thống để giảm dấu chân bộ nhớ của việc đào tạo mô hình, loại bỏ các tensor feature map trong lượt forward để tiết kiệm bộ nhớ và sau đó tái tạo chúng để tính toán gradient bằng cách thực hiện tính toán tương ứng. Các hệ thống hiện tại, ví dụ, DeepSpeed và FairSeq, áp dụng recomputation như cấu hình được khuyến nghị để đào tạo các mô hình lớn, chỉ lưu tensor đầu vào của mỗi lớp Transformer và thực hiện lại toàn bộ Transformer trong backward. Vì lớp MoE liên quan đến hai giao tiếp All-To-All trong forward, thực hiện lại chúng có thể dẫn đến chi phí thời gian lớn. Để giữ hiệu quả bộ nhớ trong khi cải thiện hiệu quả đào tạo, chúng tôi đề xuất recomputation nhận biết MoE, lưu ba tensor của mỗi lớp Transformer, bao gồm tensor đầu vào của lớp Transformer, và tensor đầu vào của hai phép toán All-To-All.

V. THÍ NGHIỆM

A. Thiết lập Thí nghiệm

1) Môi trường Máy: Chúng tôi tiến hành thí nghiệm trên DGX-A100, nơi mỗi máy chủ được trang bị 2 CPU AMD và 8 GPU NVIDIA Ampere A100 (40GB), với Ubuntu 20.04, CUDA 11.3, CuDNN 8.2.0 và NCCL 2.12.7. Các GPU bên trong một máy chủ được kết nối thông qua NVLink 3.0 và các máy chủ được kết nối với 8 NIC InfiniBand thông qua tổng cộng 8×200 Gbps băng thông. RDMA được sử dụng mặc định và phiên bản PyTorch là 1.11.

2) Baselines: Để xác minh hiệu quả của phương pháp của chúng tôi, chúng tôi so sánh nó với một số baseline đại diện, bao gồm Switch-Transformer, BASELayer, HashLayer, DSelectK và StableMoE. Switch-Transformer đề xuất áp dụng cổng Top-1 để đào tạo các mô hình quy mô lớn. BASELayer công thức hóa việc định tuyến token-chuyên gia như một bài toán gán tuyến tính và đảm bảo tải tính toán cân bằng bằng cách sử dụng các thuật toán số. HashLayer thay thế mạng cổng bằng một chiến lược định tuyến dựa trên hash (ví dụ, random hash, clustered hash). DSelectK bao gồm các cổng thưa thớt (Top-K) trong kiến trúc MoE đa cổng (tức là, MMoE) để chia sẻ tham số tốt hơn giữa các tác vụ khác nhau và đào tạo các cổng từ dày đặc thành thưa thớt để mượt mà. StableMoE cũng đề xuất hai giai đoạn đào tạo, học cổng cũng như chưng cất nó thành một cổng nhẹ trong giai đoạn đầu, và đóng băng tham số của cổng để định tuyến ổn định trong giai đoạn thứ hai. EvoMoE của chúng tôi chủ yếu chứa hai giai đoạn: giai đoạn đa dạng hóa chuyên gia để sinh ra nhiều chuyên gia đa dạng từ một chuyên gia cơ sở được đào tạo tốt, và giai đoạn thưa thớt hóa cổng dần dần và thích ứng học một cổng ngày càng thưa thớt từ một cổng dày đặc.

3) Benchmark và Tập dữ liệu: Chúng tôi đánh giá EvoMoE trên ba tác vụ phổ biến, bao gồm tác vụ dịch máy (MT) cho các mô hình dành riêng cho miền, tác vụ Masked Language Modeling (MLM) và tác vụ mô hình hóa ngôn ngữ (LM) cho các mô hình được huấn luyện trước.

Chúng tôi áp dụng kiến trúc Transformer tiêu chuẩn (Encoder-Decoder) cho tác vụ MT và đào tạo mô hình trên bốn tập dữ liệu dịch thuật phổ biến, WMT17 (Tiếng Anh sang Tiếng Đức/Tiếng Đức sang Tiếng Anh), và WMT14 (Tiếng Anh sang Tiếng Pháp/Tiếng Pháp sang Tiếng Anh). Điểm BLEU của các tập test được báo cáo để so sánh.

Chúng tôi áp dụng kiến trúc RoBERTa (Chỉ-Encoder) cho tác vụ MLM và đào tạo mô hình trên tổ hợp các tập dữ liệu, bao gồm Wikipedia, BooksCorpus, OpenWebText và CC-100. Hơn nữa, những tập dữ liệu này được token hóa bằng mã hóa byte-pair với kích thước từ vựng 50257. Các mô hình được xác thực trên benchmark General Language Understanding Evaluation (GLUE) nổi tiếng để so sánh.

Chúng tôi áp dụng kiến trúc GPT (Chỉ-Decoder) cho tác vụ LM và đào tạo mô hình trên OpenWebText như Radford et al. Chúng tôi báo cáo perplexity (PPL) train/valid/test để so sánh.

Chúng tôi cũng báo cáo FLOPs suy luận của mỗi mô hình, biểu thị tốc độ triển khai mô hình này trong công nghiệp. Tất cả dữ liệu đào tạo được tải xuống và tiền xử lý bằng cách làm theo các script ví dụ từ Fairseq.

4) Chi tiết Siêu tham số: Chúng tôi mở rộng thưa thớt các mô hình này bằng cách thay thế mỗi lớp feed-forward (FFN) khác bằng Lớp MoE-FFN, chứa một chuỗi các chuyên gia FFN. Tất cả các mô hình sử dụng hàm kích hoạt GeLU, bộ lập lịch tỷ lệ học đa thức và trình tối ưu hóa Adam, trong đó β₁ = 0.9 và β₂ = 0.98. Chúng tôi đặt clip norm là 0.0, weight decay là 0.1 và tỷ lệ dropout là 0.1. Chúng tôi sử dụng CrossEntropy làm tiêu chí và sử dụng kỹ thuật label smoothed với hệ số 0.1 cho tác vụ MT. Hệ số của balance loss được đặt là 0.1 trong Switch-Transformer, StableMoE và EvoMoE của chúng tôi. Chúng tôi đặt ngưỡng c của cổng dày-thành-thưa thớt của chúng tôi là 0.001 qua các bước đào tạo, xác định trọng số của chuyên gia quan trọng như thế nào và là một sự đánh đổi giữa chi phí đào tạo và chất lượng mô hình từ quan điểm của chúng tôi.

B. Kết quả GLUE

Kiến trúc Mô hình: Chúng tôi huấn luyện trước mô hình RoBERTa đại diện cho tác vụ mô hình hóa ngôn ngữ có mặt nạ, nơi chúng tôi đặt Transformer tiêu chuẩn (TRM) với 24 lớp encoder, chiều ẩn là 1024 và số lượng đầu attention là 16. Chúng tôi thay thế mỗi lớp FFN khác trong Transformer tiêu chuẩn bằng lớp MoE (16 chuyên gia mỗi lớp) để xây dựng các mô hình MoE. Transformer tiêu chuẩn là một mô hình dày đặc và chứa tổng cộng 355M tham số, có FLOPs suy luận là 207B. Trong khi đó, mô hình MoE thưa thớt chứa tổng cộng 1759M tham số, bao gồm 259M tham số cho backbone được chia sẻ và 1536M tham số cho mạng chuyên gia. Trong cài đặt của chúng tôi rằng chỉ 1 chuyên gia hoạt động tại một thời điểm, mỗi token đầu vào sẽ kích hoạt 335M tham số của các mô hình MoE thưa thớt, giống như mô hình Transformer tiêu chuẩn ngoại trừ mạng cổng. Để khớp chính xác tốc độ suy luận (FLOPs) của các mô hình MoE, chúng tôi tăng nhẹ kích thước ẩn FFN của TRM tiêu chuẩn để xây dựng TRM lớn hơn.

Hiệu suất Mô hình: Chúng tôi huấn luyện trước mỗi mô hình trong tổng cộng 100k bước, 5k trong số đó là giai đoạn khởi động. Đối với EvoMoE của chúng tôi, chúng tôi lên lịch 5k bước đầu tiên làm giai đoạn đa dạng hóa chuyên gia và 5k bước tiếp theo để làm mềm nhiệt độ từ 2.0 đến 0.3. Sau giai đoạn huấn luyện trước, chúng tôi tinh chỉnh các mô hình được huấn luyện trước trên mỗi tác vụ GLUE và tóm tắt kết quả trong Bảng II. Đối với RTE, chúng tôi tinh chỉnh nó bắt đầu từ mô hình MNLI thay vì mô hình được huấn luyện trước như Liu et al.

So với các baseline khác, EvoMoE đạt được kết quả hiện đại trên 7 trong 8 tác vụ và điểm trung bình tốt nhất. Mô hình MoE được xây dựng bằng cách thêm mạng cổng và thay thế lớp FFN ban đầu của Transformer tiêu chuẩn, tăng kích thước mô hình của nó và do đó mở rộng khả năng của nó. Do đó, tất cả các mô hình MoE đều vượt trội hơn mô hình backbone của chúng (TRM tiêu chuẩn), ví dụ, 89.750 cho TRM tiêu chuẩn và 90.313 (+ 0.563) cho Switch TRM đối với điểm trung bình. TRM lớn hơn vượt trội hơn TRM tiêu chuẩn một chút vì kích thước mô hình lớn của nó. Như được xác minh bởi Kaplan et al., các mô hình lớn hơn có xu hướng hiệu quả hơn về mẫu, biểu thị chất lượng mô hình tốt hơn với dữ liệu/bước đào tạo cố định.

So với các phương pháp MoE khác, EvoMoE có lợi từ việc đào tạo đầy đủ các chuyên gia trong giai đoạn đầu và sau đó làm cho việc lựa chọn chuyên gia trở nên thưa thớt hơn dựa trên các chuyên gia chuyên môn. Cụ thể, EvoMoE vượt trội hơn các phương pháp MoE khác trên benchmark GLUE lên đến 0.562 và 0.403 trung bình. Switch TRM đào tạo chung các chuyên gia và cổng được khởi tạo ngẫu nhiên, nhằm học tham số tốt hơn cũng như định tuyến cân bằng. Việc tối ưu hóa chúng đồng thời khó khăn và do đó hoạt động kém trong các mô hình MoE. Để giảm thiểu vấn đề này, StableMoE đóng băng tham số của mạng cổng sau giai đoạn đào tạo đầu và cải thiện so với Switch-TRM. Hash Layer sử dụng chiến lược hash cố định để định tuyến token, dựa trên embedding đầu vào. Vì cả chiến lược hash và embedding đầu vào đều cố định, các lớp Hash chỉ cần học tham số của các chuyên gia. Tuy nhiên, nó có thể dẫn đến dưới tối ưu vì chiến lược hash được chọn dựa trên kiến thức con người và có thể không phù hợp. BASE Layer thực thi một phép gán token-tới-chuyên gia cân bằng thông qua một bài toán gán tuyến tính, đơn giản hóa việc đào tạo theo cách khác. Tất cả những công trình này tìm ra vấn đề của việc đào tạo chung và nhắm mục tiêu giảm thiểu nó.

C. Kết quả Mô hình hóa Ngôn ngữ

Kiến trúc Mô hình: Chúng tôi huấn luyện trước mô hình GPT đại diện cho tác vụ mô hình hóa ngôn ngữ, nơi chúng tôi đặt Transformer tiêu chuẩn (TRM) với 24 lớp decoder, chiều ẩn là 1024 và số lượng đầu attention là 16. Mỗi lớp FFN khác được thay thế bằng trong Transformer tiêu chuẩn bằng lớp MoE (16 chuyên gia mỗi lớp) để xây dựng các mô hình MoE. Có tổng cộng 12 lớp MoE và do đó 192 chuyên gia (tức là, 12×16). Trong khi đó, TRM lớn hơn được mở rộng bằng cách tăng kích thước ẩn FFN của nó.

Hiệu suất Mô hình: Chúng tôi huấn luyện trước mỗi mô hình trên tập dữ liệu OpenWebText trong tổng cộng 200k bước, 10k trong số đó là giai đoạn khởi động. Đối với EvoMoE của chúng tôi, chúng tôi lên lịch 10k bước đầu tiên làm giai đoạn đa dạng hóa chuyên gia và 5k bước tiếp theo để làm mềm nhiệt độ từ 2.0 đến 0.3. Chúng tôi báo cáo perplexity trên tập test. Kết quả được tóm tắt trong Bảng III.

So với các baseline khác, EvoMoE của chúng tôi đạt được kết quả tốt nhất trong tất cả các baseline. Cụ thể, perplexity của EvoMoE là 12.24, đạt được cải thiện 2.90 so với 15.14 của TRM tiêu chuẩn. Trong khi đó, tất cả các mô hình MoE đều vượt trội hơn mô hình backbone của chúng (TRM tiêu chuẩn) vì khả năng mô hình lớn của chúng. TRM lớn hơn vượt trội hơn TRM tiêu chuẩn một chút vì kích thước mô hình lớn của nó, chứng minh tính hiệu quả mẫu của các mô hình lớn.

So với các phương pháp MoE khác, EvoMoE có lợi từ việc đào tạo đầy đủ các chuyên gia trong giai đoạn đầu và sau đó làm cho việc lựa chọn chuyên gia trở nên thưa thớt hơn dựa trên các chuyên gia chuyên môn. Cụ thể, EvoMoE vượt trội hơn các phương pháp MoE khác lên đến 0.88 ppl và 0.545 ppl trung bình. Phân tích giữa các phương pháp khác nhau giống như trong kết quả GLUE.

D. Kết quả Dịch máy

Kiến trúc Mô hình: Chúng tôi huấn luyện trước mô hình T5 đại diện cho tác vụ dịch máy, nơi chúng tôi đặt Transformer tiêu chuẩn (TRM) với 12 lớp encoder-decoder, chiều ẩn là 768 và số lượng đầu attention là 12. Mỗi lớp FFN khác được thay thế bằng trong Transformer tiêu chuẩn bằng lớp MoE (4 chuyên gia mỗi lớp) để xây dựng các mô hình MoE.

Hiệu suất Mô hình: Chúng tôi so sánh EvoMoE với Transformer và Switch-Transformer trên bốn tập dữ liệu cặp ngôn ngữ, bao gồm Tiếng Anh sang Tiếng Đức, Tiếng Đức sang Tiếng Anh, Tiếng Anh sang Tiếng Pháp và Tiếng Pháp sang Tiếng Anh. Kết quả được hiển thị trong Bảng IV, V, VI. Chúng tôi nhận xét rằng những mô hình này đều có cùng tốc độ suy luận ngay cả khi các mô hình MoE mở rộng kích thước tham số. Chúng tôi hiển thị điểm BLEU trên tập test của mỗi tập dữ liệu trong Bảng IV. EvoMoE vượt trội hơn các phương pháp khác 1 điểm BLEU trung bình. Mặc dù Switch-Transformer mở rộng kích thước mô hình, nó vẫn đạt được hiệu suất tương tự với Transformer-Base, có hiệu quả tham số. Bảng V cho thấy hiệu suất BLEU của số lượng chuyên gia khác nhau trên tập dữ liệu Tiếng Anh-Tiếng Đức. EvoMoE vẫn có thể vượt trội hơn Switch-Transformer khoảng 1.3 BLEU với số lượng chuyên gia tăng. Do chất lượng của tập dữ liệu, hiệu ứng của việc tăng số lượng chuyên gia bị hạn chế.

Hiệu quả Mô hình: Bảng VI cho thấy hiệu quả mô hình của EvoMoE và Switch Gate trên tập dữ liệu Tiếng Anh-Tiếng Đức. EvoMoE hiệu quả về tốc độ hội tụ. Ví dụ, EvoMoE chỉ cần 75% ngân sách tính toán của Switch-Transformer để đạt được cùng PPL. Điều đáng chú ý là tăng tốc so với Switch-Transformer cải thiện khi số lượng chuyên gia tăng.

Nghiên cứu Ablation: Chúng tôi trình bày một nghiên cứu ablation về EvoMoE để cho thấy ảnh hưởng của hai giai đoạn bằng cách loại bỏ giai đoạn đa dạng hóa chuyên gia và giai đoạn thưa thớt hóa cổng tương ứng. Kết quả được tóm tắt trong Bảng IV V VI. Về metric hiệu suất mô hình, nó sẽ dẫn đến suy giảm hiệu suất khi EvoMoE loại bỏ giai đoạn thưa thớt hóa cổng, như 38.3/39.2 trong Fr-En của Bảng IV. Trong khi đó, điều đáng chú ý là ảnh hưởng ít như đối với w/ và w/o giai đoạn đa dạng hóa chuyên gia, khuyến khích chúng tôi bao gồm giai đoạn này để tiết kiệm ngân sách tính toán. Về metric hiệu quả FLOPs, giai đoạn thưa thớt hóa cổng có thể cải thiện hiệu quả FLOPs 17%. Bằng cách giới thiệu giai đoạn đa dạng hóa chuyên gia, EvoMoE có thể có được thêm 4% cải thiện.

Tóm lại, giai đoạn thưa thớt hóa cổng có thể cải thiện đáng kể cả hiệu suất mô hình và hiệu quả FLOPs và giai đoạn đa dạng hóa chuyên gia có thể giới thiệu hiệu quả FLOPs bổ sung mà không suy giảm hiệu suất. Trong các phần tiếp theo, chúng tôi sẽ phân tích chi tiết giai đoạn thưa thớt hóa cổng và đánh giá nó ở quy mô lớn.

E. Phân tích Giai đoạn Thưa thớt hóa Cổng

Kiến trúc Mô hình: Chúng tôi huấn luyện trước mô hình GPT đại diện cho tác vụ mô hình hóa ngôn ngữ, nơi chúng tôi đặt GPT-ori với 24 lớp decoder, chiều ẩn là 1024 và số lượng đầu attention là 16. Mỗi lớp FFN khác được thay thế bằng trong Transformer tiêu chuẩn bằng lớp MoE (16 chuyên gia mỗi lớp) để xây dựng mô hình GPT-MoE. GPT-Switch đại diện cho việc đào tạo mô hình MoE với cổng Switch, giữ lựa chọn Top-1. GPT-DTS đại diện cho việc đào tạo mô hình MoE với cổng dày-thành-thưa thớt, bắt đầu như một cổng dày đặc định tuyến token đến hầu hết các chuyên gia và sau đó dần dần trở nên thưa thớt hơn.

Chúng tôi so sánh FLOPs cần thiết để đào tạo mô hình để cho thấy hiệu quả FLOPs của các phương pháp khác nhau. Hiệu quả FLOPs được định nghĩa là hiệu suất mô hình tốt nhất (PPL) có thể đạt được với số lượng phép toán dấu phẩy động cố định (ngân sách tính toán). Vì thời gian đào tạo thực tế có thể bị ảnh hưởng bởi nỗ lực kỹ thuật hệ thống về chi tiết thực hiện, không phải là trọng tâm của chúng tôi trong phương pháp này. Thay vào đó, trong các thí nghiệm của chúng tôi, chúng tôi thích chọn độ phức tạp tính toán để so sánh công bằng.

Hiệu suất Mô hình: Chúng tôi huấn luyện trước mỗi mô hình trên tập dữ liệu OpenWebText trong tổng cộng 300k bước, 10k trong số đó là giai đoạn khởi động. Đối với EvoMoE của chúng tôi, chúng tôi lên lịch 10k bước đầu tiên để làm mềm nhiệt độ từ 2.0 đến 0.3. Chúng tôi báo cáo perplexity trên tập validation. Kết quả được hiển thị trong Hình 6. Để cải thiện hiệu quả tính toán, chỉ một phần tham số được sử dụng cho mỗi token trong các mô hình thưa thớt với chi phí hiệu suất mô hình. DTS-Gate nhằm chuyển việc đào tạo mô hình từ dày đặc sang thưa thớt, và giữ chi phí suy luận giống như các mô hình thưa thớt. Các thí nghiệm cho thấy rằng so với Switch-Transformer hiện đại trong mô hình GPT-MoE với tập dữ liệu OpenWebText, GPT-DTS có thể có được tăng tốc 2.0x để đạt được cùng validation perplexity (Hình 6(a)), cũng như hiệu quả FLOPs cao hơn với tăng tốc 1.42x (Hình 6(b)).

Các thí nghiệm cũng xác minh khả năng của cổng dày-thành-thưa thớt để mở rộng mô hình với nhiều chuyên gia hơn hoặc nhiều lớp MoE hơn.

Comparison with Sparse Models. MoE-Switch định trước mạng cổng Top-1 tĩnh và đào tạo chung mạng cổng và chuyên gia. Khác với GPT-Switch, GPT-DTS sử dụng nhiệt độ để điều chỉnh phân bố của token-tới-chuyên gia (one-hot hoặc uniform) và ngưỡng để loại bỏ tính toán của các chuyên gia có trọng số thấp. EvoMoE hoạt động tốt hơn GPT-Switch trong hiệu quả mẫu vì nhiều chuyên gia hơn tham gia vào đào tạo và cập nhật ở đầu, được hiển thị như Hình 6(a). Về hiệu quả FLOPs, DTS-Gate trước tiên bao gồm nhiều chuyên gia hơn vào đào tạo khởi động, có hiệu quả FLOPs kém. Nhưng với việc đào tạo tiếp tục, GPT-DTS có thể có được hơn 25% cải thiện trong hiệu quả FLOPs so với Switch-Transformer hiện đại trong mô hình GPT-MoE với tập dữ liệu OpenWebText.

F. Khả năng Mở rộng

Trong phần này, chúng tôi điều tra các cài đặt thí nghiệm khác nhau để xác thực khả năng mở rộng của DTS-Gate của chúng tôi.

Kiến trúc Mô hình: Chúng tôi chọn GPT-small làm mô hình backbone cho tác vụ mô hình hóa ngôn ngữ, nơi chúng tôi đặt mô hình với 12 lớp decoder, chiều ẩn là 768 và số lượng đầu attention là 12.

Tăng Số lượng Chuyên gia: Dựa trên mô hình GPT-Small với 117M tham số, chúng tôi thay thế lớp FFN thứ 7 bằng một lớp MoE và thay đổi số lượng chuyên gia của nó trong {1;4;8;16}. Như được hiển thị bởi Hình 7(a), với số lượng chuyên gia tăng, EvoMoE giữ cải thiện nhất quán (tức là, PPL thấp hơn) trong quá trình đào tạo.

Tăng Số lượng Lớp MoE: Tương tự, chúng tôi cũng thay đổi số lượng lớp MoE để xác thực hiệu suất của cổng DTS. Chúng tôi chèn k lớp MoE vào GPT-Small, trong đó k ∈ {0;1;4;6} và mỗi lớp MoE chứa 8 chuyên gia. Hình 7(b) cho thấy bằng cách tăng lớp MoE, EvoMoE có thể đạt được hiệu suất mô hình tốt hơn với cùng FLOPs.

G. Hiệu ứng của Sparsity Scheduler

Điều đáng chú ý là một số siêu tham số được giới thiệu trong cổng Dense-To-Sparse, như nhiệt độ max/min và các iteration decay. Trong phần này, chúng tôi phân tích hiệu ứng của cài đặt siêu tham số khác nhau bằng cách tiến hành thí nghiệm của các cài đặt khác nhau. Mô hình đào tạo là GPT-MoE, decoder 24 lớp với 12 lớp MoE (16 chuyên gia mỗi lớp) và tập dữ liệu là OpenWebText.

Chúng tôi decay nhiệt độ từ giá trị max đến giá trị min trong 15000 iteration đầu tiên và chuyển sang Top1 sau đó. Các thí nghiệm với các giá trị max khác nhau đến giá trị min khác nhau được đánh giá, và kết quả được hiển thị trong Hình 8.

Max/Min Temperature Dưới nhiệt độ nhỏ, phân bố trọng số của các chuyên gia gần với one-hot, dẫn đến phân bố một-token-một-chuyên gia và chi phí đào tạo thấp, nhưng phương sai của gradient lớn. Ngược lại, nhiệt độ lớn dẫn đến phân bố trọng số cổng gần như đồng đều, phát triển nhiều chuyên gia hơn vào đào tạo nhưng phương sai của gradient nhỏ. Như được hiển thị trong Hình 8, chúng tôi thấy rằng hai siêu tham số này có ảnh hưởng thấp đến chất lượng mô hình dưới cùng ngân sách đào tạo, ngoại trừ giá trị cực đoan, ví dụ, 1.0 cho giá trị max và 0.1 cho giá trị min.

H. Visualization of Expert Specialization

Chúng tôi trực quan hóa chiến lược định tuyến của mô hình GPT được huấn luyện trước bởi EvoMoE trong Bảng VII thông qua embedding đầu vào tương ứng, nơi mỗi lớp MoE chứa 16 chuyên gia. Đối với mỗi chuyên gia, chúng tôi trình bày Top5 token tiếp theo được gán và đưa ra mô tả để giải thích từ quan điểm của chúng tôi. Ví dụ, Chuyên gia 1 nắm bắt các động từ phụ và Chuyên gia 6 nắm bắt các trường hợp sở hữu. Những chuyên gia này có thể nắm bắt ngữ cảnh địa phương của mỗi embedding tốt. Đối với các chuyên gia khác, việc trực quan hóa khó khăn vì ảnh hưởng của ngữ cảnh dài.

VI. CÔNG VIỆC LIÊN QUAN

a) Static Sparse Neural Networks: Khai thác tính thưa thớt trong mạng nơ-ron sâu có thể giảm cả yêu cầu lưu trữ và tính toán cho đào tạo và suy luận mô hình. Một trong những phương pháp sparsification được sử dụng rộng rãi nhất là weight pruning. Các nghiên cứu trước đã đề xuất cắt bỏ các trọng số dư thừa hoặc ít hữu ích dựa trên các tiêu chí pruning khác nhau (ví dụ, tầm quan trọng của từng trọng số hoặc nhóm trọng số) và sau đó tinh chỉnh các trọng số còn lại để lấy lại độ chính xác đã mất. Sau khi pruning và tinh chỉnh, một phần trọng số bị loại bỏ vĩnh viễn, tạo ra một mô hình thưa thớt tĩnh trong DNN. Mô hình/cấu trúc thưa thớt là một sự đánh đổi giữa hiệu quả mô hình và hiệu quả phần cứng. Công việc đầu cố gắng tăng tỷ lệ thưa thớt hoặc độ chính xác mô hình bằng cách sử dụng các phương pháp sparsification không có cấu trúc, trong khi công việc gần đây tập trung nhiều hơn vào tính thưa thớt có cấu trúc để tăng tốc thực tế trên phần cứng. Thú vị thay, chỉ ra rằng đào tạo một mạng thưa thớt từ đầu vượt trội hoặc có thể so sánh với các phương pháp dựa trên pruning. EvoMoE của chúng tôi áp dụng một cổng dày-thành-thưa thớt, tương tự như các phương pháp dựa trên pruning đào tạo tất cả các chuyên gia trước và sau đó học định tuyến cổng thưa thớt.

b) Conditional Computation with MoE: Khác với các mạng nơ-ron thưa thớt tĩnh trước đây loại bỏ vĩnh viễn một phần trọng số, conditional computation chỉ kích hoạt tham số liên quan của mô hình trên cơ sở mỗi mẫu, có thể được coi là một cấu trúc thưa thớt động vẫn giữ tất cả trọng số mô hình nhưng mang lại tính thưa thớt vào tính toán. Kiến trúc mixture-of-expert (MoE), như một dạng cụ thể của conditional computation, chứa một chuỗi các chuyên gia và một mạng cổng có thể huấn luyện định tuyến mỗi mẫu đầu vào đến các chuyên gia tương ứng. Conditional computation có khả năng giảm chi phí suy luận (mà không giảm khả năng mô hình) hoặc tăng khả năng mô hình (mà không tăng chi phí suy luận) từ góc độ tăng tốc hoặc mở rộng mô hình. Mặt khác, các phần được kích hoạt thưa thớt (tức là, MoE trong các mô hình) có thể được coi là các khối thưa thớt có cấu trúc, không giới thiệu chi phí tính toán bổ sung. Tuy nhiên, các mô hình conditional computation thường khó đào tạo, vì chúng yêu cầu học các quyết định định tuyến rời rạc từ các ví dụ riêng lẻ đến các chuyên gia và mạng cổng có xu hướng hội tụ đến một trạng thái chỉ chọn cùng một vài chuyên gia. LSTM-MoE, GShard và Switch-Transformer sử dụng các loss cân bằng tải phụ trợ để giảm thiểu hiện tượng tự củng cố này và do đó cải thiện hiệu quả đào tạo. Trong các mô hình MoE như vậy, mạng cổng và các chuyên gia, như hai thành phần quan trọng, được đào tạo chung có thể can thiệp lẫn nhau. Trong EvoMoE, chúng tôi xem xét tách rời việc đào tạo các chuyên gia và mạng cổng bằng cách bao gồm tất cả các chuyên gia bắt đầu với nhiệt độ cao trong Gumbel-Softmax và sau đó đào tạo mạng cổng để trở nên thưa thớt hơn và chọn chuyên gia tốt nhất thông qua việc decay nhiệt độ này. BASELayer công thức hóa việc định tuyến token-chuyên gia như một bài toán gán tuyến tính và đảm bảo tải tính toán cân bằng bằng cách sử dụng các thuật toán số. HashLayer thay thế mạng cổng bằng một chiến lược định tuyến dựa trên hash (ví dụ, random hash, clustered hash dispersed hash). MoEfication sử dụng kích hoạt thưa thớt trong mạng feed-forward (FFN) của mô hình Transformer và chia mỗi FFN dày đặc lớn thành nhiều chuyên gia để tăng tốc suy luận mô hình, trong khi EvoMoE phát triển một mô hình dày đặc nhỏ thành một mô hình MoE lớn và thưa thớt.

c) Multi-Task Learning with MoE: Multi-task learning (MTL) áp dụng một kiến trúc được chia sẻ để học nhiều tác vụ, khai thác mối quan hệ giữa các tác vụ và đạt được hiệu suất tổng quát hóa tốt hơn. Tuy nhiên, chia sẻ tham số giữa các tác vụ không liên quan có thể làm giảm hiệu suất. Kiến trúc multi-gate MoE (tức là, MMoE) được giới thiệu như một cách hiệu quả để khai thác cả điểm chung và khác biệt giữa các tác vụ, trong đó mỗi tác vụ có cổng riêng của nó thích ứng kiểm soát mức độ chia sẻ tham số. DSelect-K bao gồm các cổng thưa thớt (Top-K) để chia sẻ tham số tốt hơn và đào tạo các cổng từ dày đặc sang thưa thớt để mượt mà, Về DSelect-K và cổng DTS trong EvoMoE của chúng tôi đều đề xuất cơ chế dày-thành-thưa thớt, chúng đang cố gắng giải quyết hai vấn đề hoàn toàn khác nhau, mặc dù cả hai công trình đều cho thấy SparseMoE tốt hơn DenseMoE một cách trùng hợp. Đối với công việc DTS của chúng tôi, vì DenseMoE hoạt động tốt nhưng chi phí cho việc huấn luyện trước mô hình lớn, chúng tôi cố gắng tìm một giải pháp hiệu quả hơn (SparseMoE). Trong khi đối với công việc đa tác vụ, vì DenseMoE hoạt động kém đối với học đa tác vụ, họ cố gắng tìm một giải pháp tốt hơn để đối phó với các tác vụ khác nhau, tức là, DSelectK. Do đó, hai công trình này có động lực rõ ràng khác nhau.

VII. KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Các mô hình MoE gặp phải thách thức hiệu quả đào tạo do khó khăn trong việc đào tạo nhiều chuyên gia và mạng cổng cùng nhau. Trong công việc này, chúng tôi trình bày một khung đào tạo MoE EvoMoE tách rời việc đào tạo các chuyên gia và mạng cổng bằng cách trước tiên sinh ra nhiều chuyên gia đa dạng từ một chuyên gia cơ sở được đào tạo tốt và sau đó học một cổng ngày càng thưa thớt từ một cổng dày đặc. Các đánh giá của chúng tôi cho thấy EvoMoE không chỉ có thể đạt được chất lượng mô hình tốt hơn trong Transformers với ngân sách tính toán đã cho mà còn đạt được hiệu quả FLOPs tốt hơn khi so sánh với các công trình trước đây trong đào tạo MoE. Mặt khác, EvoMoE mở ra những thách thức cho việc thực thi hệ thống do tính toán trong giai đoạn đầu và khả năng thích ứng của các chuyên gia. Trong tương lai, chúng tôi muốn thiết kế và thực hiện các tối ưu hóa cấp hệ thống để đạt được đào tạo hiệu quả trong cả chất lượng mô hình và thực thi hệ thống.
