# 2110.04260.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2110.04260.pdf
# Kích thước tệp: 2422893 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2022
THUẦN HÓA TRANSFORMER KÍCH HOẠT THƯA THỚT VỚI
CÁC CHUYÊN GIA NGẪU NHIÊN
Simiao Zuoy, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang,
Tuo ZhaoyandJianfeng Gao
yViện Công nghệ GeorgiaMicrosoft
fsimiaozuo,tourzhao g@gatech.edu ,
fxiaodl,jian.jiao,youki,hanyh,bzhang,jfgao g@microsoft.com
TÓM TẮT
Các mô hình kích hoạt thưa thớt (SAMs), chẳng hạn như Mixture-of-Experts (MoE), có thể dễ dàng mở rộng quy mô để có số lượng tham số lớn một cách đáng kinh ngạc mà không làm tăng đáng kể chi phí tính toán. Tuy nhiên, SAMs được báo cáo là không hiệu quả về mặt tham số sao cho các mô hình lớn hơn không phải lúc nào cũng dẫn đến hiệu suất tốt hơn. Trong khi hầu hết các nghiên cứu đang diễn ra tập trung vào việc cải thiện các mô hình SAMs bằng cách khám phá các phương pháp định tuyến đầu vào đến các chuyên gia, phân tích của chúng tôi tiết lộ rằng nghiên cứu như vậy có thể không dẫn đến giải pháp mà chúng tôi mong đợi, tức là các phương pháp định tuyến thường được sử dụng dựa trên cơ chế cổng không hoạt động tốt hơn so với việc định tuyến ngẫu nhiên đầu vào đến các chuyên gia. Trong bài báo này, chúng tôi đề xuất một mô hình dựa trên chuyên gia mới, THOR (Transformer witH StOchastic ExpeRts). Không giống như các mô hình dựa trên chuyên gia cổ điển, chẳng hạn như Switch Transformer (Fedus et al., 2021), các chuyên gia trong THOR được kích hoạt ngẫu nhiên cho mỗi đầu vào trong quá trình huấn luyện và suy luận. Các mô hình THOR được huấn luyện bằng cách sử dụng một hàm mất mát được điều chỉnh tính nhất quán, trong đó các chuyên gia không chỉ học từ dữ liệu huấn luyện mà còn từ các chuyên gia khác như giáo viên, sao cho tất cả các chuyên gia đưa ra các dự đoán nhất quán. Chúng tôi xác thực hiệu quả của THOR trên các tác vụ dịch máy. Kết quả cho thấy các mô hình THOR hiệu quả hơn về mặt tham số ở chỗ chúng vượt trội đáng kể so với các mô hình Transformer và MoE qua các thiết lập khác nhau. Ví dụ, trong dịch thuật đa ngôn ngữ, THOR vượt trội hơn Switch Transformer 2 điểm BLEU và đạt được cùng điểm BLEU như một mô hình MoE hiện đại (Kim et al., 2021) lớn hơn 18 lần. Mã của chúng tôi được công khai tại: https://github.com/microsoft/Stochastic-Mixture-of-Experts.

1 GIỚI THIỆU
Các mô hình mạng neural lớn đã cho thấy hiệu quả trong nhiều tác vụ xử lý ngôn ngữ tự nhiên như dịch máy (Lewis et al., 2020; Conneau & Lample, 2019), hiểu ngôn ngữ tự nhiên (Devlin et al., 2019; Liu et al., 2019; He et al., 2020), và tạo sinh ngôn ngữ tự nhiên (Radford et al., 2019; Brown et al., 2020). Các mô hình này thường được kích hoạt dày đặc. Tức là, một mô hình sử dụng tất cả các tham số của nó để xử lý tất cả các đầu vào. Một nhược điểm của các mô hình này là chi phí huấn luyện cấm đoán. Hơn nữa, kích thước cực lớn làm giảm đáng kể tốc độ suy luận, hạn chế thêm tính thực tế của các mô hình.

Để giải quyết những vấn đề này, các mô hình kích hoạt thưa thớt (SAMs, Shazeer et al. 2017) đã được đề xuất. Một SAM tự thích nghi chọn một tập con các tham số của nó cho các đầu vào khác nhau trong quá trình huấn luyện và suy luận mô hình. Điều này làm cho việc huấn luyện SAMs lớn hơn một bậc độ lớn so với các mô hình kích hoạt dày đặc mà không làm tăng đáng kể chi phí tính toán trở nên khả thi. Ví dụ, GShard kích hoạt thưa thớt (Lepikhin et al., 2020) bao gồm hơn 600 tỷ tham số và Switch Transformer (Fedus et al., 2021) 1.5 nghìn tỷ tham số, trong khi GPT-3 (Brown et al., 2020), được cho là mô hình kích hoạt dày đặc lớn nhất, chỉ bao gồm 175 tỷ tham số.

Khối xây dựng của SAMs là lớp chuyên gia, chứa một cơ chế attention và nhiều mạng neural feed-forward (FFNs) song song. Mỗi FFN được gọi là một chuyên gia. Trong quá trình huấn luyện, một đầu vào được định tuyến đến một số lượng cố định các chuyên gia, sao cho số lượng các phép toán điểm nổi (FLOPs) của một lần truyền thuận vẫn không đổi, bất kể tổng số chuyên gia. Do đó, huấn luyện SAMs hiệu quả về mặt chi phí hơn nhiều so với huấn luyện các mô hình kích hoạt dày đặc. Ví dụ, huấn luyện Switch-large (Fedus et al., 2021) và T5-large (Raffel et al., 2019) yêu cầu cùng số FLOPs truyền thuận, mặc dù mô hình trước lớn hơn 35 lần (26.3 so với 0.74 tỷ tham số).

Tuy nhiên, SAMs được báo cáo là không hiệu quả về mặt tham số. Ví dụ, mặc dù mô hình Switch-large lớn hơn T5-large 35 lần, hiệu suất của nó trên benchmark GLUE (Wang et al., 2019a) chỉ tốt hơn một chút (88.5 so với 87.8). Cũng có những trường hợp hiệu suất của SAMs thậm chí còn tệ hơn các mô hình kích hoạt dày đặc nhỏ hơn. Ví dụ, hiệu suất của Switch-large tệ hơn T5-large trên ARC Reasoning Challenge (66.0 so với 68.8) (Clark et al., 2018). Trong một ví dụ khác, mặc dù GShard (Lepikhin et al., 2020) cho thấy lợi ích đáng kể so với các mô hình kích hoạt dày đặc, một sự suy giảm lợi ích với số lượng tham số lớn hơn đã được quan sát.

Hầu hết các nghiên cứu đang diễn ra đã tập trung vào việc cải thiện SAMs bằng cách phát triển các phương pháp định tuyến hiệu quả. Vì chỉ một tập con các tham số mô hình (tức là các chuyên gia) được cập nhật cho mỗi đầu vào trong quá trình huấn luyện, chúng ta cần quyết định chuyên gia nào sẽ được kích hoạt với một đầu vào cho trước. Các công trình hiện tại (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021; Yang et al., 2021) sử dụng một mạng cổng cho việc định tuyến đầu vào. Tuy nhiên, cơ chế cổng gặp phải vấn đề mất cân bằng tải khét tiếng: trọng số của cổng có thể sụp đổ sao cho gần như tất cả các đầu vào đều được định tuyến đến cùng một chuyên gia. Do đó, nhiều phương pháp được đề xuất để giảm thiểu vấn đề này, chẳng hạn như cổng nhiễu (Shazeer et al., 2017), công suất chuyên gia (Lepikhin et al., 2020), hàm mất mát cân bằng tải (Lepikhin et al., 2020; Fedus et al., 2021), và cổng kTop-1 (Yang et al., 2021). Tuy nhiên, các phương pháp định tuyến này chưa được chứng minh hiệu quả để làm cho SAMs hiệu quả hơn về mặt tham số. Để hiểu tại sao SAMs không hiệu quả về mặt tham số, chúng tôi phân tích hiệu suất của một số mô hình MoE cổ điển. Phân tích của chúng tôi tiết lộ rằng một SAM không phải lúc nào cũng vượt trội hơn một mô hình kích hoạt dày đặc có kích thước tương tự, xác nhận các kết quả được báo cáo trong Yang et al. (2021). Hơn nữa, chúng tôi cũng quan sát thấy rằng phương pháp định tuyến được sử dụng rộng rãi dựa trên cơ chế cổng không hoạt động tốt hơn so với việc định tuyến ngẫu nhiên đầu vào đến các chuyên gia.

Được truyền cảm hứng từ các phát hiện của chúng tôi, chúng tôi đề xuất một SAM mới, THOR (Transformer witH StOchastic ExpeRts). Không giống như các SAMs cổ điển, chẳng hạn như Switch Transformer, các chuyên gia trong THOR được kích hoạt ngẫu nhiên (không cần bất kỳ cơ chế cổng nào) cho mỗi đầu vào trong quá trình huấn luyện và suy luận. Các mô hình THOR được huấn luyện bằng cách tối thiểu hóa cả hàm mất mát entropy chéo và một thuật ngữ điều chỉnh tính nhất quán, sao cho các chuyên gia có thể học không chỉ từ dữ liệu huấn luyện mà còn từ các chuyên gia khác như giáo viên để tất cả các chuyên gia đưa ra các dự đoán nhất quán.

Để xác thực hiệu quả của THOR, chúng tôi đã tiến hành các thí nghiệm mở rộng về dịch máy sử dụng ba thiết lập: tài nguyên thấp, tài nguyên phong phú, và đa ngôn ngữ. Kết quả cho thấy các mô hình THOR vượt trội hơn các mô hình MoE hiện đại trung bình 2 điểm BLEU trên mười hai tác vụ dịch tài nguyên thấp. Trong thiết lập tài nguyên phong phú, THOR đạt được kết quả hiện đại mới trên hai benchmark dịch được sử dụng rộng rãi, WMT'16 En-De và WMT'14 En-Fr. Trên các tác vụ dịch đa ngôn ngữ, mô hình THOR với 300 triệu tham số đạt được cải thiện 2 điểm BLEU so với một mô hình MoE hiện đại cùng kích thước. Hơn nữa, mô hình của chúng tôi đạt được kết quả hiện đại trên các tác vụ này — cùng điểm BLEU được đạt bởi mô hình Z-code MoE (Kim et al., 2021) với 5.5 tỷ tham số (lớn hơn 18 lần).

2 KIẾN THỨC NỀN
Transformer. Mô hình Transformer (Vaswani et al., 2017) đã chứng minh hiệu suất vượt trội trong nhiều tác vụ xử lý ngôn ngữ tự nhiên chuỗi-sang-chuỗi, chẳng hạn như dịch máy neural. Mô hình bao gồm một bộ mã hóa và một bộ giải mã. Bộ mã hóa bao gồm nhiều lớp mã hóa, mỗi lớp có cấu trúc giống nhau. Một lớp mã hóa sử dụng một cơ chế self-attention và một mạng neural feed-forward (FFN). Bộ giải mã được xây dựng tương tự, ngoại trừ một cơ chế cross-attention bổ sung trong mỗi lớp giải mã.

Các Mô hình Kích hoạt Thưa thớt. Khối xây dựng của SAMs là lớp chuyên gia, tương tự như lớp Transformer. Mỗi lớp chuyên gia này chứa một cơ chế attention và nhiều FFNs song song, trong đó mỗi FFN được gọi là một chuyên gia. Gọi {Ei}^N_{i=1} là các chuyên gia, và N là tổng số chuyên gia. Một cơ chế cổng quyết định chuyên gia nào đầu vào sẽ được định tuyến đến. Tại mỗi lớp chuyên gia, cho một vector đầu vào x ∈ R^d, trong đó d là chiều nhúng, giá trị cổng của việc định tuyến x đến chuyên gia Ei là

pi(x) = [Softmax(Wgx)]i; (1)

trong đó Wg ∈ R^{N×d} là ma trận trọng số có thể huấn luyện của cơ chế cổng. Cho các giá trị cổng {pi(x)}^N_{i=1}, chúng ta chọn top-K chuyên gia để tạo thành một tập hợp các chuyên gia được kích hoạt T ⊆ {1...N}, trong đó |T| = K. Sau đó đầu ra x_{out} của lớp chuyên gia là

x_{out} = Σ_{i∈T} pi(x)Ei(x). (2)

Chú ý rằng trong Eq. 2, đầu vào x chỉ kích hoạt K thay vì N chuyên gia, trong đó K ≪ N, ví dụ, K = 2 và N = 2048 trong GShard (Lepikhin et al., 2020). Điều này ngụ ý rằng số lượng FLOPs cần thiết cho một lần truyền thuận không tăng theo số lượng chuyên gia N. Do đó, SAMs có thể mở rộng quy mô đến kích thước khổng lồ mà không có bất kỳ sự gia tăng đáng kể nào về thời gian huấn luyện và thời gian suy luận.

Ma trận trọng số cổng Wg (Eq. 1) được huấn luyện cùng với phần còn lại của các tham số mô hình. Vì không có ràng buộc nào về các trọng số đã học, có thể Wg sụp đổ sao cho một hàng chiếm ưu thế, tức là tất cả các đầu vào đều được định tuyến đến một chuyên gia. Vấn đề này được gọi là mất cân bằng tải. Các công trình hiện tại áp dụng các phương pháp heuristic đặc biệt khác nhau để giảm thiểu vấn đề này, ví dụ, thêm nhiễu Gaussian vào Eq. 1 (cổng nhiễu, Shazeer et al. 2017), giới hạn số lượng tối đa các đầu vào có thể được định tuyến đến một chuyên gia (công suất chuyên gia, Lepikhin et al. 2020), áp đặt một hàm mất mát cân bằng tải (Lepikhin et al., 2020; Fedus et al., 2021), và sử dụng gán tuyến tính (Lewis et al., 2021). Có các công trình khác loại bỏ cơ chế cổng sao cho mất cân bằng tải không còn là vấn đề, ví dụ, bằng cách kết hợp các hàm hash (Roller et al., 2021). Bên cạnh vấn đề mất cân bằng tải, cũng có các cuộc thảo luận sôi nổi về cách xây dựng T trong Eq. 2. Ví dụ, Shazeer et al. (2017); Lepikhin et al. (2020); Yang et al. (2021) phỏng đoán rằng việc định tuyến đầu vào đến K > 1 chuyên gia là cần thiết, trong khi Fedus et al. (2021) lập luận rằng sử dụng K = 1 là đủ và hiệu quả hơn về mặt tính toán.

3 PHÂN TÍCH CÁC MÔ HÌNH KÍCH HOẠT THƯA THỚT
Chúng tôi điều tra hành vi của cơ chế cổng của một số mô hình MoE cổ điển. Chúng tôi tiến hành thí nghiệm trên một tác vụ dịch đa ngôn ngữ, {De, Vi} → En. Thêm chi tiết được trình bày trong Phụ lục A.

Chúng tôi xem xét hai mô hình MoE được đề xuất trong Shen et al. (2019), được gọi là MoE(dec) và MoE(tok) tương ứng, và ba biến thể của Switch Transformer được đề xuất trong Fedus et al. (2021). Số lượng chuyên gia được đặt là hai cho tất cả các mô hình MoE. Chúng tôi so sánh chúng với mô hình Transformer (Vaswani et al., 2017) có cùng kích thước mô hình.

Hình 1 cho thấy các hàm mất mát validation và điểm BLEU của ba mô hình: Transformer, MoE(dec), và MoE(tok). Chúng ta thấy rằng hai mô hình MoE thực hiện rất tương tự nhau, và không mô hình nào vượt trội hơn Transformer một cách đáng kể.

Để diễn giải kết quả của Hình 1, chúng tôi kiểm tra tải của mỗi chuyên gia và điểm tin cậy của việc định tuyến đầu vào đến các chuyên gia khác nhau. Tải của một chuyên gia được định nghĩa là tỷ lệ đầu vào được gán cho nó. Đối với một đầu vào được định tuyến đến một chuyên gia, điểm tin cậy định tuyến của nó (đầu ra của cơ chế cổng) xác định mức độ ưu tiên, ví dụ, nếu điểm tin cậy định tuyến là 0.5, thì cổng không có sở thích nào cho chuyên gia nào. Đối với mỗi chuyên gia, chúng tôi tính điểm tin cậy định tuyến trung bình trên tất cả các đầu vào được gán cho nó.

Hình 2 cho thấy rằng sau giai đoạn đầu của huấn luyện (tức là 200 vòng lặp đầu tiên), trọng số cổng sụp đổ và gần như tất cả các đầu vào đều được định tuyến đến chuyên gia 2. Ngoài ra, điểm tin cậy định tuyến trung bình của chuyên gia 2 gần bằng 1.0, có nghĩa là cổng ưu tiên mạnh mẽ chuyên gia 2 hơn chuyên gia 1. Trong trường hợp này, chỉ có một trong các chuyên gia được huấn luyện đầy đủ. Hình 3 mô tả một kịch bản khác, trong đó các đầu vào được phân phối ngẫu nhiên đến các chuyên gia. Chú ý rằng sau khoảng 4000 vòng lặp, hai chuyên gia được tải đều nhau, và xác suất gán bất kỳ đầu vào nào cho chuyên gia 1 và chuyên gia 2 gần như giống nhau, cho thấy rằng cơ chế cổng không có sở thích nào cho chuyên gia nào.

Chúng tôi đã xác định hai hành vi của cơ chế cổng: mất cân bằng tải và định tuyến ngẫu nhiên. Hành vi trước cũng được báo cáo trong các bài báo gần đây (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021). Chúng tôi tiếp tục điều tra Switch Transformer (Fedus et al., 2021), một biến thể MoE hiện đại kết hợp các phương pháp khác nhau để giải quyết vấn đề mất cân bằng tải. Ngoài ra, vì hành vi của cơ chế cổng trong Switch Transformer bắt chước định tuyến ngẫu nhiên (xem Phụ lục A), chúng tôi kiểm tra hiệu ứng của việc loại bỏ cổng và gán ngẫu nhiên đầu vào cho các chuyên gia.

Hình 4 minh họa các hàm mất mát validation và điểm BLEU của Transformer và ba biến thể của Switch Transformer, trong đó các đầu vào được định tuyến theo tokens (được gọi là Switch(t)), câu (Switch(s)), hoặc được định tuyến ngẫu nhiên (Switch(r)). Tương tự như kết quả trong Hình 1, chúng ta thấy rằng bốn mô hình thực hiện tương tự nhau. Điều này cho thấy rằng ngay cả sau khi chúng ta giảm thiểu mất cân bằng tải, hiệu suất mô hình không được cải thiện (tức là các Switch Transformers không vượt trội hơn Transformer vanilla), và hiệu suất của Switch Transformer không thay đổi nhiều giữa các phương pháp định tuyến khác nhau, bao gồm cả định tuyến ngẫu nhiên.

Chúng tôi nhận xét rằng trong bài báo này, chúng tôi tập trung vào các tác vụ xử lý ngôn ngữ tự nhiên, đặc biệt là dịch máy neural. Có các công trình khác trong các lĩnh vực nghiên cứu khác nhau (ví dụ, thị giác máy tính) đưa ra các kết luận khác với chúng tôi (Riquelme et al., 2021). Chúng tôi quy kết điều này cho sự khác biệt nội tại giữa phân loại hình ảnh và tạo sinh ngôn ngữ, ví dụ, mỗi đầu vào trong trường hợp trước thuộc về một danh mục được định nghĩa rõ ràng, trong khi không có kiến thức như vậy tồn tại trong trường hợp sau.

Tóm lại, các thí nghiệm tiết lộ:
• Một mô hình kích hoạt thưa thớt không phải lúc nào cũng vượt trội hơn một mô hình kích hoạt dày đặc có cùng kích thước mô hình.
• Phương pháp định tuyến được sử dụng rộng rãi dựa trên cơ chế cổng không hoạt động tốt hơn so với việc định tuyến ngẫu nhiên đầu vào đến các chuyên gia.

4 THOR: TRANSFORMER VỚI CÁC CHUYÊN GIA NGẪU NHIÊN
Sự không hiệu quả của cơ chế cổng, như được hiển thị trong các thí nghiệm của chúng tôi, thúc đẩy chúng tôi đề xuất một mô hình dựa trên chuyên gia mới, THOR (Transformer witH StOchastic ExpeRts). Trong THOR, một cặp chuyên gia được chọn ngẫu nhiên và kích hoạt trong mỗi lớp trong một vòng lặp huấn luyện, và sau đó tất cả các đầu vào trong một batch được xử lý bằng cùng một cặp chuyên gia. Phương pháp của chúng tôi đơn giản hóa đáng kể thiết kế mô hình, và có hai lợi thế bổ sung. Thứ nhất, nó loại bỏ vấn đề mất cân bằng tải vì việc chọn ngẫu nhiên một cặp chuyên gia trong mỗi vòng lặp cho phép mỗi chuyên gia có cơ hội công bằng được huấn luyện đầy đủ. Các phương pháp heuristic đặc biệt, chẳng hạn như hàm mất mát cân bằng tải, như đã thảo luận trong Phần 2, không còn cần thiết nữa. Thứ hai, không giống như cơ chế cổng, THOR không giới thiệu bất kỳ tham số mô hình bổ sung nào.

Một vấn đề của THOR là không có cơ chế cổng, các chuyên gia cần được chọn ngẫu nhiên trong quá trình suy luận, và chúng ta có thể có kết quả suy luận không nhất quán do các hạt giống ngẫu nhiên khác nhau. Ví dụ, trên một tập dữ liệu dịch từ tiếng Séc sang tiếng Anh, các thí nghiệm của chúng tôi cho thấy rằng tính ngẫu nhiên có thể dẫn đến sự khác biệt 0.5 điểm BLEU.

Để giải quyết vấn đề này, chúng tôi giới thiệu một bộ điều chỉnh tính nhất quán trong mục tiêu huấn luyện của THOR. Cụ thể, gọi N là số lượng chuyên gia, L là số lượng lớp, và E^l_i là một chuyên gia được kích hoạt (là một FFN) trong lớp l, trong đó 1 ≤ i ≤ N và 1 ≤ l ≤ L. Chúng tôi sử dụng p = f(x; {E^l_i}^L_{l=1}) để biểu thị xác suất dự đoán của đầu vào x sử dụng mô hình f trong đó các chuyên gia {E^l_i}^L_{l=1} được kích hoạt.

Hình 5 minh họa một vòng lặp huấn luyện với các chuyên gia ngẫu nhiên. Để ngắn gọn, chúng tôi hiển thị một mô hình chỉ với một lớp Transformer. Chú ý rằng thay vì kích hoạt một chuyên gia cho mỗi lớp trong một vòng lặp, chúng tôi chọn kích hoạt một cặp chuyên gia trong THOR. Kết quả là, chúng tôi có hai xác suất dự đoán được tạo ra bởi hai lựa chọn tương ứng: p1 = f(x; {E^l_i}^L_{l=1}) và p2 = f(x; {E^l_j}^L_{l=1}). Sau đó, mục tiêu huấn luyện của THOR đối với các mẫu huấn luyện (x, y) trong tập dữ liệu D là

min Σ_{(x,y)∈D} ℓ(x,y) = CE(p1,y) + CE(p2,y) + λCR(p1,p2);

trong đó CR(p1,p2) = 1/2(KL(p1||p2) + KL(p2||p1)). (3)

Ở đây, CE là hàm mất mát entropy chéo, bộ điều chỉnh tính nhất quán CR được định nghĩa là trung bình của hai thuật ngữ phân kỳ Kullback-Leibler (KL), và λ là một siêu tham số kiểm soát sức mạnh của bộ điều chỉnh. Trong huấn luyện SGD mini-batch, chúng tôi lấy mẫu ngẫu nhiên một cặp chuyên gia để kích hoạt tại mỗi lớp cho mỗi batch. Trong quá trình suy luận, chúng tôi cũng có thể chọn ngẫu nhiên một chuyên gia để kích hoạt tại mỗi lớp cho mỗi đầu vào, tương tự như trong huấn luyện. Chúng tôi cũng có thể sử dụng các phương pháp lựa chọn chuyên gia khác nhau, chẳng hạn như tập hợp chuyên gia, như sẽ được thảo luận trong Phần 5 (Bảng 5).

Mục tiêu huấn luyện THOR của Eq. 3 buộc tất cả các chuyên gia tối thiểu hóa lỗi huấn luyện trong khi đưa ra các dự đoán giống nhau càng nhiều càng tốt. Do đó, trong mỗi bước huấn luyện, mỗi chuyên gia tối ưu hóa các tham số của mình bằng cách học từ cả dữ liệu huấn luyện (thông qua việc tối thiểu hóa hàm mất mát entropy chéo) và chuyên gia được ghép cặp của nó như một giáo viên (thông qua việc tối thiểu hóa phân kỳ KL). Mặc dù các chuyên gia này được học để đưa ra các dự đoán nhất quán, chúng hội tụ đến các tối ưu (cục bộ) khác nhau do tính ngẫu nhiên được giới thiệu trong huấn luyện, ví dụ, khởi tạo, SGD mini-batch, định tuyến ngẫu nhiên, v.v. Do đó, mỗi chuyên gia học từ một tập hợp các giáo viên đa dạng trong suốt quá trình huấn luyện, điều này giúp cải thiện hiệu suất của mô hình. Ngoài ra, bằng cách phạt các chuyên gia mang lại các dự đoán không nhất quán với những chuyên gia khác, bộ điều chỉnh tính nhất quán cũng giúp giảm phương sai của dự đoán mô hình.

THOR về mặt khái niệm tương tự như dropout (Srivastava et al., 2014) vì cả hai phương pháp đều định tuyến một đầu vào đến một số thành phần mạng con được chọn ngẫu nhiên (tức là các chuyên gia trong THOR và các neuron trong dropout). Tuy nhiên, THOR khác với dropout ở một số khía cạnh quan trọng, làm cho nó trở thành lựa chọn tốt hơn để huấn luyện và phục vụ hiệu quả các mô hình neural quy mô lớn. Thứ nhất, THOR có thể được áp dụng cho cả huấn luyện và suy luận, trong khi dropout chỉ được sử dụng cho huấn luyện. Thứ hai, THOR được chứng minh là mạnh mẽ hơn trong huấn luyện mô hình quy mô lớn so với dropout. Ví dụ, các mô hình của chúng tôi ít có khả năng quá khớp hơn với sự gia tăng số lượng chuyên gia (xem Hình 9). Thứ ba, THOR dẫn đến một mô hình thưa thớt có cấu trúc hơn so với dropout, sao cho một mô hình THOR quy mô lớn có thể được huấn luyện dễ dàng hơn nhiều bằng cách sử dụng các cụm GPU, ví dụ, bằng cách đặt các chuyên gia khác nhau trên các GPU khác nhau song song.

5 THÍ NGHIỆM
Chúng tôi đánh giá THOR trên dịch máy neural. Chúng tôi áp dụng ba thiết lập: dịch tài nguyên thấp, dịch tài nguyên phong phú, và dịch đa ngôn ngữ. Đối với dịch tài nguyên thấp và tài nguyên phong phú, chúng tôi huấn luyện tất cả các mô hình bằng Fairseq (Ott et al., 2019). Đối với dịch đa ngôn ngữ, chúng tôi sử dụng DeepSpeed MoE (Kim et al., 2021) để thực hiện các mô hình MoE. Tất cả các thí nghiệm được tiến hành trên GPU NVIDIA V100. Các thí nghiệm bổ sung, bao gồm mở rộng quy mô mô hình và so sánh tốc độ suy luận, được trì hoãn đến Phụ lục D.

5.1 BASELINE
Chúng tôi sử dụng hai baselines trong các thí nghiệm.

• Transformer (Vaswani et al., 2017) đạt hiệu suất vượt trội trong nhiều tác vụ học chuỗi-sang-chuỗi, chẳng hạn như dịch máy neural.
• Switch Transformer (Fedus et al., 2021) là một mô hình MoE hiện đại, sử dụng cơ chế cổng để định tuyến đầu vào và sử dụng hàm mất mát cân bằng tải để giảm mất cân bằng tải.

Để xác minh hiệu quả của bộ điều chỉnh tính nhất quán được áp đặt trong Eq. 3, chúng tôi cũng so sánh THOR với các mô hình Transformer được huấn luyện bằng hai phương pháp điều chỉnh phổ biến. Chúng tôi nhận xét rằng hai phương pháp này chia sẻ chi phí tính toán tương tự với THOR, tức là chúng cũng yêu cầu hai lần truyền thuận trong mỗi vòng lặp huấn luyện.

• SMART (Jiang et al., 2020) sử dụng một bộ điều chỉnh đối thủ tạo ra sự mượt mà để phạt sự khác biệt trường hợp xấu nhất giữa các dự đoán của một đầu vào sạch và một đầu vào bị nhiễu.
• R3F (Aghajanyan et al., 2020) sử dụng một bộ điều chỉnh để giảm sự sụp đổ biểu diễn. Phương pháp này đã được chứng minh hiệu quả trong các tác vụ xử lý ngôn ngữ tự nhiên khác nhau.

Tất cả các phương pháp được huấn luyện cho cùng số lượng FLOPs trong các thí nghiệm để so sánh công bằng.

5.2 DỊCH TÀI NGUYÊN THẤP
Chúng tôi sử dụng sáu cặp ngôn ngữ: tiếng Anh sang tiếng Việt, tiếng Anh sang tiếng Đức, và tiếng Anh sang tiếng Pháp từ IWSLT; tiếng Anh sang tiếng Romania, tiếng Anh sang tiếng Latvia, và tiếng Anh sang tiếng Séc từ Europarl. Thống kê tập dữ liệu được tóm tắt trong Bảng 6 (Phụ lục B).

Bảng 1: Kết quả thí nghiệm trên các tập dữ liệu tài nguyên thấp. Kết quả tốt nhất trên mỗi tập dữ liệu được in đậm.

En-Vi Vi-En En-De De-En En-Fr Fr-En
Transformer (Vaswani et al., 2017) 31.3 29.4 28.1 34.8 39.2 38.1
SMART (Jiang et al., 2020) 32.5 30.5 29.3 35.8 40.0 38.8
R3F (Aghajanyan et al., 2020) 32.2 30.7 29.2 35.7 39.7 38.9
Switch (Fedus et al., 2021) 31.7 29.5 28.4 34.6 39.1 38.2
THOR 34.0 33.0 31.1 37.8 40.7 40.0

En-Ro Ro-En En-Lv Lv-En En-Cs Cs-En
Transformer (Vaswani et al., 2017) 23.5 25.0 13.6 15.8 16.1 20.4
SMART (Jiang et al., 2020) 24.6 25.7 14.2 16.3 16.7 21.4
R3F (Aghajanyan et al., 2020) 23.8 25.8 14.4 16.3 16.8 21.6
Switch (Fedus et al., 2021) 23.8 24.4 13.8 16.1 16.1 20.6
THOR 25.2 27.1 15.2 17.4 17.6 22.4

Để đánh giá THOR với các kích thước mô hình khác nhau, chúng tôi sử dụng kiến trúc Transformer-base (Vaswani et al., 2017) trên các tập dữ liệu Europarl, và một mô hình nhỏ hơn trên các tập dữ liệu IWSLT. So với Transformer-base, mô hình nhỏ hơn giảm chiều ẩn từ 2048 xuống 1024, và giảm số lượng đầu từ 8 xuống 4 với chiều của mỗi đầu tăng gấp đôi. Chúng tôi sử dụng hai chuyên gia cho các mô hình dựa trên chuyên gia. Chúng tôi nhận xét rằng mặc dù THOR tăng số lượng tham số, tốc độ suy luận của nó (về FLOPs) giống như Transformer-base vì chỉ có một chuyên gia được kích hoạt cho mỗi đầu vào. Độc giả quan tâm tham khảo Phụ lục C để biết thêm chi tiết.

Kết quả thí nghiệm trong Bảng 1 cho thấy hiệu suất của Switch Transformer ngang bằng với Transformer vanilla, ví dụ, điểm BLEU trung bình của nó trên 12 tập dữ liệu là 26.3, giống như Transformer. Kết quả xác nhận rằng SAMs không vượt trội hơn các mô hình kích hoạt dày đặc có kích thước mô hình tương tự. Ngược lại, THOR đạt được cải thiện hơn 1.0 điểm BLEU so với Switch Transformer trong tất cả 12 tác vụ. THOR cũng vượt trội đáng kể so với các mô hình được huấn luyện bằng hai phương pháp điều chỉnh cạnh tranh, SMART và R3F.

5.3 DỊCH TÀI NGUYÊN PHONG PHÚ
Chúng tôi sử dụng hai benchmark dịch tài nguyên phong phú được áp dụng rộng rãi: dịch từ tiếng Anh sang tiếng Đức từ WMT'16 và dịch từ tiếng Anh sang tiếng Pháp từ WMT'14. Tập dữ liệu trước bao gồm 4.5 triệu cặp câu huấn luyện, và tập dữ liệu sau 36 triệu cặp. Chúng tôi làm theo các bước tiền xử lý trong Ott et al. (2018).

Để đánh giá THOR, chúng tôi sử dụng kiến trúc Transformer-big (Vaswani et al., 2017) và chúng tôi đặt số lượng chuyên gia cho cả THOR và Switch Transformer là 4. Độc giả quan tâm tham khảo Phụ lục C để biết thêm chi tiết.

Bảng 2 báo cáo điểm BLEU và điểm sacreBLEU (Post, 2018) của các mô hình khác nhau. Chúng ta thấy rằng THOR đạt được kết quả hiện đại mới trong thiết lập mà không sử dụng tăng cường dữ liệu hay mô hình ngôn ngữ tiền huấn luyện. Cụ thể, THOR nâng cao kết quả hiện đại trước đó (Liu et al., 2020b;c) 0.3 điểm BLEU trên tác vụ dịch En-De và 0.1 điểm BLEU trên tác vụ dịch En-Fr. THOR cũng vượt trội đáng kể so với các mô hình được huấn luyện bằng hai phương pháp điều chỉnh khác, SMART (Jiang et al., 2020) và R3F (Aghajanyan et al., 2020). Tương tự như những gì được quan sát trong dịch tài nguyên thấp, Switch Transformer (Fedus et al., 2021) không vượt trội hơn Transformer vanilla (Ott et al., 2018).

Bảng 2: Điểm BLEU và sacreBLEU trên WMT'14 En-Fr và WMT'16 En-De. Kết quả của Jiang et al. (2020), Aghajanyan et al. (2020), và Fedus et al. (2021) là từ việc thực hiện của chúng tôi.

BLEU En-De En-Fr
Vaswani et al. (2017) 28.4 41.8
Ott et al. (2018) 29.3 43.2
Wang et al. (2019b) 29.6 —
Wu et al. (2019a) 29.7 43.2
So et al. (2019) 29.8 41.3
Jiang et al. (2020) 29.8 43.4
Wu et al. (2019b) 29.9 43.3
Aghajanyan et al. (2020) 29.4 43.3
Liu et al. (2020c) 30.1 43.8
Fedus et al. (2021) 29.3 43.0
THOR 30.4 43.8

sacreBLEU En-De En-Fr
Ott et al. (2018) 28.6 41.4
Jiang et al. (2020) 29.1 41.5
So et al. (2019) 29.2 —
Aghajanyan et al. (2020) 29.0 41.5
Liu et al. (2020c) 29.5 41.8
Fedus et al. (2021) 28.6 41.1
THOR 29.6 41.9

5.4 DỊCH ĐA NGÔN NGỮ
Chúng tôi đã thu thập 10 cặp ngôn ngữ từ các tập dữ liệu WMT, và xây dựng một từ điển 64k mục từ cho tất cả các ngôn ngữ. Thống kê chi tiết được tóm tắt trong Bảng 7 (Phụ lục B). Vui lòng tham khảo Kim et al. (2021) để biết thêm chi tiết. Chúng tôi không sử dụng học đa tác vụ hoặc dữ liệu đơn ngôn ngữ bổ sung trong các thí nghiệm.

Chúng tôi sử dụng kiến trúc mô hình sau: chiều nhúng được đặt là 768 và chiều ẩn cho FFN được đặt là 3072; chúng tôi sử dụng 12 lớp encoder và 6 lớp decoder, trong đó mỗi lớp có 12 đầu attention, và chiều của mỗi đầu là 64. Chúng tôi đặt số lượng chuyên gia là 4 cho cả THOR và Switch Transformer.

Bảng 3 báo cáo điểm BLEU trung bình của việc dịch từ tiếng Anh sang các ngôn ngữ khác, dịch từ các ngôn ngữ khác sang tiếng Anh, và điểm tổng thể của 20 tác vụ. Chúng ta thấy rằng so với Switch Transformer cùng kích thước (tức là 300 triệu tham số), mô hình của chúng tôi đạt được cải thiện 2 điểm trong điểm BLEU tổng thể. Ngoài ra, mô hình của chúng tôi hiệu quả hơn nhiều về mặt tham số so với Switch Transformer. Mô hình THOR với 300 triệu tham số đạt được cùng điểm BLEU (24.4) mà Switch Transformer với 5.5 tỷ tham số đạt được, lớn hơn hơn 18 lần.

Bảng 3: Kết quả dịch đa ngôn ngữ. Ở đây "E" có nghĩa là số lượng chuyên gia.

En→Others Others→En Average
Switch (32E, 5.5B) — — 24.4
Switch (4E, 300M) 20.3 24.6 22.4
THOR (4E, 300M) 21.4 27.4 24.4

Hình 6 cho thấy điểm BLEU trong tất cả 20 tác vụ dịch. Chú ý rằng THOR vượt trội hơn baseline trên 17 trong số 20 tác vụ. Sự cải thiện nói chung đáng kể hơn trên các tác vụ với tập dữ liệu nhỏ hơn. Ví dụ, mô hình của chúng tôi đạt được cải thiện điểm BLEU 4.7 và 6.7 trên Gu-En (85k) và Hi-En (264k) tương ứng. Trên các tác vụ với tập dữ liệu lớn hơn, sự cải thiện đạt được bởi mô hình của chúng tôi ít đáng kể hơn, nhưng vẫn quan trọng, ví dụ, +0.9 điểm BLEU trên Cs-En (10M) và +1.1 Fi-En (4.8M). Đối với ba tác vụ duy nhất mà mô hình của chúng tôi kém hơn baseline, khoảng cách nhỏ, ví dụ, -0.4, -0.2, và -0.4 điểm BLEU trên En-Cs, En-De, và En-Fr tương ứng.

Hình 6: Chi tiết kết quả dịch đa ngôn ngữ.

5.5 CÁC THÍ NGHIỆM ABLATION
Mục tiêu Huấn luyện. Chúng tôi kiểm tra những đóng góp tương đối của ba thuật ngữ mất mát được sử dụng trong mục tiêu huấn luyện THOR của Eq. 3: CE1, CE2 và CR. Kết quả trong Bảng 4 cho thấy rằng bộ điều chỉnh tính nhất quán CR rất quan trọng đối với hiệu suất mô hình, và việc bỏ đi một trong hai thuật ngữ CE chỉ dẫn đến mất mát điểm BLEU rất nhỏ vì hai thuật ngữ entropy chéo đóng vai trò giống nhau trong huấn luyện.

Phương pháp Suy luận. Chúng tôi so sánh ba phương pháp suy luận: (1) Dispatch(s) sử dụng định tuyến ngẫu nhiên cấp độ câu, trong đó tất cả các token trong một câu được định tuyến đến cùng một chuyên gia; (2) Dispatch(t) sử dụng định tuyến ngẫu nhiên cấp độ token, trong đó các token trong một câu được định tuyến đến các chuyên gia khác nhau; (3) Ensemble, trong đó mỗi câu được định tuyến đến tất cả N chuyên gia, và N biểu diễn ẩn trong mỗi lớp được lấy trung bình. Lưu ý rằng số lượng FLOPs lớn hơn cho Ensemble vì chúng ta cần chạy truyền thuận cho mỗi đầu vào qua N chuyên gia. Bảng 5 cho thấy rằng Dispatch(s) và Dispatch(t) thực hiện tương tự nhau, và Ensemble mang lại điểm BLEU tốt nhất với chi phí thời gian suy luận dài hơn.

Bảng 4: Ảnh hưởng của ba thuật ngữ mất mát trong mục tiêu huấn luyện của Eq. 3, được kiểm tra trên dịch Cs-En.

Loss terms BLEU
CE1 + CE2 + CR 22.4
CE1 + CR 22.2
CE1 + CE2 20.8
CE1 20.6

Bảng 5: Hiệu suất và chi phí của ba phương pháp suy luận, được kiểm tra trên dịch Cs-En.

BLEU time
Dispatch(s) 22.4 1
Dispatch(t) 22.4 1
Ensemble 22.6 N

Sức mạnh điều chỉnh. Để điều tra hiệu ứng của sức mạnh điều chỉnh λ, chúng tôi chạy thí nghiệm trên tập dữ liệu dịch Cs-En trong thiết lập tài nguyên thấp. Hình 7 cho thấy hiệu suất mô hình không rất nhạy cảm với λ miễn là giá trị đủ lớn, chẳng hạn >2.0.

Tính nhất quán của Dự đoán Mô hình. Chúng tôi nghiên cứu phương sai của dự đoán mô hình do việc sử dụng các chuyên gia được kích hoạt ngẫu nhiên trong quá trình suy luận. Chúng tôi so sánh THOR và Switch Transformer, trong đó chúng tôi loại bỏ cổng được huấn luyện trong quá trình suy luận. Đối với mỗi mô hình, chúng tôi tính phương sai của dự đoán mô hình dựa trên 20 lần chạy. Như được hiển thị trong Hình 8, THOR đưa ra các dự đoán nhất quán hơn Switch Transformer do việc sử dụng bộ điều chỉnh tính nhất quán cho huấn luyện mô hình. Phương sai của THOR dưới 0.002, trong khi phương sai của Switch Transformer là 0.008, lớn hơn bốn lần. Chúng tôi nhận xét rằng bằng cách loại bỏ bộ định tuyến được huấn luyện từ Switch Transformer, hiệu suất mô hình chỉ giảm một cách cận biên (từ 20.6 xuống 20.4). Điều này chỉ ra thêm rằng một bộ định tuyến được huấn luyện có thể không tốt hơn một bộ định tuyến ngẫu nhiên.

Overfit. Chúng tôi so sánh mô hình THOR và mô hình Transformer về khả năng overfit dữ liệu huấn luyện khi kích thước mô hình tăng. Chúng tôi chạy thí nghiệm trên dữ liệu De-En trong thiết lập tài nguyên thấp, trong đó tỷ lệ dropout của các FFN trong Transformer được chọn sao cho số lượng tham số được huấn luyện trong một vòng lặp giống như mô hình THOR. Như được hiển thị trong Hình 9, THOR không cho thấy bất kỳ dấu hiệu overfit nào — chúng tôi quan sát được sự cải thiện nhất quán trong điểm BLEU khi chúng tôi tăng số lượng chuyên gia từ 2 lên 8. Ngược lại, hiệu suất của mô hình Transformer xấu đi khi chúng tôi tăng chiều ẩn của FFN từ 2k lên 8k. Chúng tôi nhận xét rằng chúng tôi cũng quan sát hiện tượng overfit trên các tập dữ liệu lớn hơn, ví dụ, Transformer overfit trên tập dữ liệu Cs-En khi chúng tôi đặt chiều ẩn của FFN là 16k.

Hình 7: Ảnh hưởng của sức mạnh điều chỉnh tính nhất quán λ trên dịch Cs-En.

Hình 8: Biểu đồ violin của tính nhất quán hiệu suất trên dịch Cs-En.

Hình 9: BLEU so với kích thước mô hình trên dịch De-En.

6 KẾT LUẬN
Chúng tôi trình bày một mô hình kích hoạt thưa thớt dựa trên chuyên gia mới, THOR. Không giống như các SAMs hiện tại, chẳng hạn như Switch Transformer, các chuyên gia trong THOR được kích hoạt ngẫu nhiên cho mỗi đầu vào trong quá trình huấn luyện và suy luận. Các mô hình THOR được huấn luyện bằng cách sử dụng một hàm mất mát được điều chỉnh tính nhất quán, trong đó mỗi chuyên gia không chỉ học từ dữ liệu huấn luyện mà còn từ các chuyên gia khác như giáo viên để tất cả các chuyên gia đưa ra các dự đoán nhất quán. Kết quả là, không chỉ các mô hình THOR quy mô lớn có thể được huấn luyện và phục vụ một cách hiệu quả như các mô hình MoE cổ điển, các mô hình THOR cũng chứng minh khả năng tổng quát hóa tốt hơn ở chỗ chúng hiệu quả hơn về mặt tham số, ít có khả năng overfit hơn, đưa ra các dự đoán nhất quán hơn, và đạt được kết quả tốt hơn một cách nhất quán qua các thiết lập khác nhau. Chúng tôi xác thực hiệu quả của THOR thông qua một nghiên cứu thực nghiệm toàn diện về dịch máy. Trong tất cả ba thiết lập (tức là dịch tài nguyên thấp, tài nguyên phong phú, và đa ngôn ngữ), các mô hình THOR vượt trội đáng kể so với Transformer vanilla, và Switch Transformer, một mô hình MoE hiện đại.

LỜI CẢM ƠN
Chúng tôi cảm ơn Rukmini Lyer, Kevin Duh, Hao Cheng, Chunyuan Li, Johannes Gehrke, các đồng nghiệp từ đội Microsoft Bing Ads và Microsoft Research cho những thảo luận và bình luận quý báu của họ.

TÀI LIỆU THAM KHẢO
[Danh sách đầy đủ các tài liệu tham khảo từ trang 10-12]

--- TRANG 14 ---
A PHÂN TÍCH CÁC MÔ HÌNH KÍCH HOẠT THƯA THỚT

A.1 CHI TIẾT HUẤN LUYỆN
Chúng tôi xem xét hai mô hình Mixture-of-Experts (MoE) được đề xuất trong Shen et al. (2019), được ký hiệu là "MoE(dec)" và "MoE(tok)". Trong biến thể đầu tiên, mỗi chuyên gia là một bộ giải mã Transformer riêng biệt. Trong biến thể thứ hai, mỗi chuyên gia là một token khác nhau, tức là nếu chúng ta định tuyến đầu vào đến chuyên gia một, thì chúng ta thay thế token <bos> (begin-of-sentence) trong câu đầu vào bằng token <expert1>. Lưu ý rằng các nhúng của các token chuyên gia này được huấn luyện cùng với phần còn lại của các tham số mô hình. Các mô hình này được trang bị khung tối ưu hóa expectation-maximization. Khung như vậy tạo điều kiện tính toán xác suất gán một đầu vào cho một chuyên gia cụ thể theo cơ chế cổng. Vui lòng tham khảo Shen et al. (2019) để biết chi tiết về các mô hình này.

Chúng tôi sử dụng thiết lập dịch đa ngôn ngữ, trong đó chúng tôi áp dụng hai tập dữ liệu: De-En từ IWSLT'14 và Vi-En từ IWSLT'15. Đối với mỗi tập dữ liệu, chúng tôi sử dụng mã hóa cặp byte (BPE, Sennrich et al. 2016) với 10,000 thao tác hợp nhất để tiền xử lý. Sau đó chúng tôi nối hai tập dữ liệu đã được tiền xử lý. Chúng tôi học một từ điển riêng biệt cho En và {De+Vi}, dẫn đến khoảng 9k và 12k từ vựng tương ứng.

Để huấn luyện, chúng tôi sử dụng Adam (Kingma & Ba, 2015) làm bộ tối ưu hóa và chúng tôi đặt tốc độ học là 0.001. Chúng tôi đặt kích thước batch tương đương với 64k token, ví dụ, chúng tôi sử dụng 8k token trên mỗi GPU với 8 GPU. Các chi tiết huấn luyện khác theo việc thực hiện Fairseq. Để suy luận, chúng tôi sử dụng kích thước beam là 5 và penalty độ dài là 1.0.

A.2 KẾT QUẢ BỔ SUNG
Chúng tôi cũng vẽ biểu đồ điểm tin cậy định tuyến trung bình và tải của các chuyên gia cho Switch(s) và Switch(t), tương tự như Hình 2 và Hình 3. Trước tiên chúng tôi điều tra Switch Transformer mà không có hàm mất mát cân bằng tải.

[Tiếp tục với phần còn lại của tài liệu...]
