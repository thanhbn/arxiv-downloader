# 1312.4314.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/1312.4314.pdf
# Kích thước tệp: 1928905 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Học Biểu Diễn Nhân Tử trong
Hỗn Hợp Chuyên Gia Sâu
David Eigen1;2Marc'Aurelio Ranzato1Ilya Sutskever1
1Google, Inc.
2Dept. of Computer Science, Courant Institute, NYU
deigen@cs.nyu.edu ranzato@fb.com ilyasu@google.com
Tóm tắt
Hỗn hợp Chuyên gia kết hợp đầu ra của nhiều mạng "chuyên gia", mỗi mạng
chuyên môn hóa trong một phần khác nhau của không gian đầu vào. Điều này đạt được bằng cách huấn
luyện một mạng "gating" ánh xạ mỗi đầu vào thành một phân phối trên các chuyên gia. Những
mô hình như vậy cho thấy tiềm năng để xây dựng các mạng lớn hơn mà vẫn rẻ để tính toán
tại thời điểm kiểm tra, và có thể song song hóa nhiều hơn tại thời điểm huấn luyện. Trong công trình này, chúng tôi mở
rộng Hỗn hợp Chuyên gia thành một mô hình xếp chồng, Hỗn hợp Chuyên gia Sâu, với
nhiều tập gating và chuyên gia. Điều này tăng theo cấp số nhân số lượng
chuyên gia hiệu quả bằng cách liên kết mỗi đầu vào với một tổ hợp của các chuyên gia tại mỗi
tầng, nhưng vẫn duy trì kích thước mô hình khiêm tốn. Trên một phiên bản dịch chuyển ngẫu nhiên của
bộ dữ liệu MNIST, chúng tôi thấy rằng Hỗn hợp Chuyên gia Sâu tự động học để
phát triển các chuyên gia phụ thuộc vị trí ("where") tại tầng đầu tiên, và các chuyên gia đặc trưng lớp
("what") tại tầng thứ hai. Ngoài ra, chúng tôi thấy rằng các tổ hợp khác nhau
được sử dụng khi mô hình được áp dụng cho bộ dữ liệu các monophone giọng nói.
Những điều này chứng minh việc sử dụng hiệu quả tất cả các tổ hợp chuyên gia.
1 Giới thiệu
Mạng sâu đã đạt được hiệu suất rất tốt trong nhiều tác vụ khác nhau, ví dụ [10, 5, 3]. Tuy nhiên,
một hạn chế cơ bản của các kiến trúc này là toàn bộ mạng phải được thực thi cho tất cả
đầu vào. Gánh nặng tính toán này áp đặt giới hạn kích thước mạng. Một cách để mở rộng các mạng này
trong khi giữ cho chi phí tính toán thấp là tăng tổng số lượng tham số và đơn vị
ẩn, nhưng chỉ sử dụng một phần nhỏ của mạng cho mỗi đầu vào đã cho. Sau đó, học một hàm ánh xạ
tính toán rẻ từ đầu vào đến các phần thích hợp của mạng.
Mô hình Hỗn hợp Chuyên gia [7] là một phiên bản liên tục của điều này: Một mạng gating được học trộn
đầu ra của N mạng "chuyên gia" để tạo ra một đầu ra cuối cùng. Mặc dù mô hình này bản thân nó
không đạt được các lợi ích tính toán được nêu ở trên, nó cho thấy tiềm năng như một bước đệm hướng tới
các mạng có thể thực hiện mục tiêu này.
Trong công trình này, chúng tôi mở rộng Hỗn hợp Chuyên gia để sử dụng một mạng gating khác nhau tại mỗi tầng trong
một mạng đa tầng, tạo thành một Hỗn hợp Chuyên gia Sâu (DMoE). Điều này tăng số lượng
chuyên gia hiệu quả bằng cách giới thiệu một số mũ các đường dẫn qua các tổ hợp khác nhau của
chuyên gia tại mỗi tầng. Bằng cách liên kết mỗi đầu vào với một tổ hợp như vậy, mô hình của chúng tôi sử dụng các
tập con khác nhau của các đơn vị của nó cho các đầu vào khác nhau. Do đó nó có thể vừa lớn vừa hiệu quả cùng một lúc.
Chúng tôi chứng minh hiệu quả của phương pháp này bằng cách đánh giá nó trên hai bộ dữ liệu. Sử dụng một
bộ dữ liệu MNIST bị nhiễu, chúng tôi cho thấy DMoE học cách nhân tử các khía cạnh khác nhau của biểu diễn dữ liệu
tại mỗi tầng (cụ thể, vị trí và lớp), tạo ra việc sử dụng hiệu quả tất cả các đường dẫn. Chúng tôi cũng thấy rằng tất cả
các tổ hợp được sử dụng khi áp dụng mô hình của chúng tôi cho bộ dữ liệu các monophone giọng nói.
Marc'Aurelio Ranzato hiện đang làm việc tại Facebook AI Group.
1arXiv:1312.4314v3  [cs.LG]  9 Mar 2014

--- TRANG 2 ---
2 Công trình liên quan
Một Hỗn hợp Chuyên gia (MoE) tiêu chuẩn [7] học một tập các mạng chuyên gia fi cùng với một mạng gating g. Mỗi fi ánh xạ đầu vào x thành C đầu ra (một cho mỗi lớp c = 1; : : : ; C), trong khi g(x) là một phân phối trên các chuyên gia i = 1; : : : ; N mà tổng bằng 1. Đầu ra cuối cùng sau đó được cho bởi Phương trình 1
FMoE(x) =NX
i=1gi(x)softmax( fi(x)) (1)
=NX
i=1p(eijx)p(cjei; x) =p(cjx) (2)
Điều này cũng có thể được xem như một mô hình xác suất, nơi xác suất cuối cùng trên các lớp được biên hóa
trên việc lựa chọn chuyên gia: đặt p(eijx) =gi(x) và p(cjei; x) = softmax( fi(x)), chúng ta có
Phương trình 2.
Một tích chuyên gia (PoE) [6] tương tự, nhưng thay vào đó kết hợp xác suất log để tạo thành một tích:
FPoE(x)/NY
i=1softmax( fi(x)) =NY
i=1pi(cjx) (3)
Cũng liên quan chặt chẽ đến công trình của chúng tôi là Hỗn hợp Chuyên gia Phân cấp [9], học một hệ thống phân cấp
của các mạng gating trong cấu trúc cây. Đầu ra của mỗi mạng chuyên gia tương ứng với một lá trong cây;
các đầu ra sau đó được trộn theo trọng số gating tại mỗi nút.
Mô hình của chúng tôi khác với mỗi trong ba mô hình này vì nó lắp ráp động một tổ hợp chuyên gia phù hợp
cho mỗi đầu vào. Đây là một thể hiện của khái niệm tính toán có điều kiện
được đưa ra bởi Bengio [1] và được kiểm tra trong một môi trường ngẫu nhiên một tầng bởi Bengio, Leonard và
Courville [2]. Bằng cách điều kiện các mạng gating và chuyên gia của chúng tôi trên đầu ra của tầng trước,
mô hình của chúng tôi có thể biểu diễn một số lượng mũ lớn các chuyên gia hiệu quả.
3 Phương pháp
Để mở rộng MoE thành DMoE, chúng tôi giới thiệu hai tập chuyên gia với mạng gating (g1; f1
i) và
(g2; f2
j), cùng với một tầng tuyến tính cuối cùng f3 (xem Hình 1). Đầu ra cuối cùng được tạo ra bằng cách kết hợp
các hỗn hợp tại mỗi tầng:
z1=NX
i=1g1
i(x)f1
i(x)
z2=MX
j=1g2
j(z1)f2
j(z1)
F(x) = z3= softmax( f3(z2))
Chúng tôi đặt mỗi fl
i thành một ánh xạ tuyến tính đơn với chỉnh lưu, và mỗi gl
i thành hai tầng ánh xạ tuyến tính với
chỉnh lưu (nhưng với ít đơn vị ẩn); f3 là một tầng tuyến tính đơn. Xem Mục 4 để biết chi tiết.
Chúng tôi huấn luyện mạng sử dụng hạ gradient ngẫu nhiên (SGD) với một ràng buộc bổ sung trên
phân công gating (được mô tả dưới đây). SGD tự nó dẫn đến một cực tiểu địa phương thoái hóa: Các chuyên gia tại
mỗi tầng hoạt động tốt nhất cho vài ví dụ đầu tiên cuối cùng áp đảo các chuyên gia còn lại.
Điều này xảy ra vì các ví dụ đầu tiên tăng trọng số gating của các chuyên gia này, điều này lần lượt
khiến chúng được chọn với trọng số gating cao thường xuyên hơn. Điều này khiến chúng huấn luyện nhiều hơn,
và trọng số gating của chúng tăng lại, ad infinitum.
Để chống lại điều này, chúng tôi đặt một ràng buộc trên các phân công gating tương đối cho mỗi chuyên gia trong quá trình huấn
luyện. Gọi Gl
i(t) =Pt
t0=1gl
i(xt0) là tổng phân công chạy cho chuyên gia i của tầng l tại bước t, và
gọi Gl(t) =1
NPN
i=1Gl
i(t) là trung bình của chúng (ở đây, xt0 là ví dụ huấn luyện tại bước t0). Sau đó cho mỗi
chuyên gia i, chúng tôi đặt gl
i(xt) = 0 nếu Gl
i(t) − Gl(t) > m cho một ngưỡng biên m, và chuẩn hóa lại
2

--- TRANG 3 ---
x	

f11(x)	

f21(x)	

fN1(x)	

g1(x)	

z1	

. . .	

x	

f11(x)	

f21(x)	

fN1(x)	

g1(x)	

z1	

. . .	

f12(x)	

f22(x)	

fM2(x)	

g2(x)	

z2	

. . .	

z3	

(a) (b)
Hình 1: (a) Hỗn hợp Chuyên gia; (b) Hỗn hợp Chuyên gia Sâu với hai tầng.
phân phối gl(xt) để tổng bằng 1 trên các chuyên gia i. Điều này ngăn các chuyên gia bị sử dụng quá mức ban đầu,
dẫn đến phân công cân bằng. Sau khi huấn luyện với ràng buộc tại chỗ, chúng tôi bỏ nó và tiếp tục
huấn luyện trong giai đoạn tinh chỉnh thứ hai.
4 Thí nghiệm
4.1 MNIST Nhiễu
Chúng tôi đã huấn luyện và kiểm tra mô hình của mình trên MNIST với dịch chuyển ngẫu nhiên đều 4 pixel, dẫn đến
hình ảnh xám có kích thước 36×36. Như đã giải thích ở trên, mô hình được huấn luyện để phân loại chữ số
thành mười lớp.
Cho tác vụ này, chúng tôi đặt tất cả f1
i và f2
j thành các mô hình tuyến tính một tầng với chỉnh lưu, f1
i(x) =
max(0 ; W1
ix+b1
i), và tương tự cho f2
j. Chúng tôi đặt f3 thành một tầng tuyến tính, f3(z2) = W3z2+b3.
Chúng tôi thay đổi số lượng đơn vị ẩn đầu ra của f1
i và f2
j giữa 20 và 100. Đầu ra cuối cùng
từ f3 có 10 đơn vị (một cho mỗi lớp).
Các mạng gating g1 và g2 mỗi cái được tạo thành từ hai tầng tuyến tính+chỉnh lưu với hoặc 50
hoặc 20 đơn vị ẩn, và 4 đơn vị đầu ra (một cho mỗi chuyên gia), tức là g1(x) = softmax( Bmax(0 ; Ax+
a) +b), và tương tự cho g2.
Chúng tôi đánh giá tác động của việc sử dụng hỗn hợp tại tầng thứ hai bằng cách so sánh với việc chỉ sử dụng một
chuyên gia cố định duy nhất tại tầng thứ hai, hoặc nối đầu ra của tất cả chuyên gia. Lưu ý rằng cho một hỗn hợp
với h đơn vị ẩn, mô hình nối tương ứng có Nh đơn vị ẩn. Do đó chúng tôi mong đợi
mô hình nối sẽ hoạt động tốt hơn hỗn hợp, và hỗn hợp hoạt động tốt hơn
mạng đơn. Tốt nhất là hỗn hợp càng gần càng tốt với giới hạn chuyên gia nối. Trong mỗi trường hợp, chúng tôi giữ
kiến trúc tầng đầu tiên giống nhau (một hỗn hợp).
Chúng tôi cũng so sánh mô hình hai tầng với mô hình một tầng trong đó tầng ẩn z1 được
ánh xạ đến đầu ra cuối cùng thông qua tầng tuyến tính và softmax. Cuối cùng, chúng tôi so sánh với một mạng sâu
kết nối đầy đủ với cùng tổng số tham số. Điều này được xây dựng sử dụng
cùng số lượng đơn vị tầng thứ hai z2, nhưng mở rộng số đơn vị tầng đầu tiên z1 sao cho
tổng số tham số giống như DMoE (bao gồm các tham số mạng gating của nó).
3

--- TRANG 4 ---
4.2 Giọng nói Monophone
Ngoài ra, chúng tôi chạy mô hình của mình trên bộ dữ liệu các mẫu giọng nói monophone. Bộ dữ liệu này là một
tập con ngẫu nhiên của khoảng một triệu mẫu từ cơ sở dữ liệu độc quyền lớn hơn gồm vài trăm
giờ dữ liệu tiếng Anh Mỹ được thu thập sử dụng Voice Search, Voice Typing và dữ liệu đọc [8]. Cho các
thí nghiệm của chúng tôi, mỗi mẫu được giới hạn ở 11 khung cách nhau 10ms, và có 40 bin tần số.
Mỗi đầu vào được đưa vào mạng như một vector 440 chiều. Có 40 lớp phoneme đầu ra có thể.
Chúng tôi huấn luyện một mô hình với 4 chuyên gia ở tầng đầu tiên và 16 ở tầng thứ hai. Cả hai tầng có
128 đơn vị ẩn. Các mạng gating mỗi cái có hai tầng, với 64 đơn vị trong tầng ẩn. Như
trước đây, chúng tôi đánh giá tác động của việc sử dụng hỗn hợp tại tầng thứ hai bằng cách so sánh với việc chỉ sử dụng
một chuyên gia duy nhất tại tầng thứ hai, hoặc nối đầu ra của tất cả chuyên gia.
5 Kết quả
5.1 MNIST Nhiễu
Bảng 1 cho thấy lỗi trên tập huấn luyện và kiểm tra cho mỗi kích thước mô hình (tập kiểm tra là tập kiểm tra MNIST
với một dịch chuyển ngẫu nhiên duy nhất trên mỗi hình ảnh). Trong hầu hết các trường hợp, các chuyên gia xếp chồng sâu hoạt động
giữa đường cơ sở chuyên gia đơn và nối trên tập huấn luyện, như mong đợi. Tuy nhiên, các
mô hình sâu thường gặp phải quá khớp: lỗi của hỗn hợp trên tập kiểm tra tệ hơn của
chuyên gia đơn cho hai trong số bốn kích thước mô hình. Đáng khích lệ, DMoE hoạt động gần như tốt bằng
một mạng kết nối đầy đủ (DNN) với cùng số lượng tham số, mặc dù mạng này
áp đặt ít ràng buộc hơn trên cấu trúc của nó.
Trong Hình 2, chúng tôi cho thấy phân công trung bình cho mỗi chuyên gia (tức là đầu ra gating trung bình), cả theo dịch chuyển đầu vào
và theo lớp. Tầng đầu tiên phân công chuyên gia theo dịch chuyển, trong khi phân công là
đều theo lớp. Ngược lại, tầng thứ hai phân công chuyên gia theo lớp, nhưng đều theo
dịch chuyển. Điều này cho thấy hai tầng chuyên gia thực sự được sử dụng theo cách bổ sung,
để tất cả các tổ hợp chuyên gia đều hiệu quả. Các chuyên gia tầng đầu tiên trở nên chọn lọc với nơi
chữ số xuất hiện, bất kể lớp thành viên của nó, trong khi các chuyên gia tầng thứ hai chọn lọc với
lớp chữ số là gì, bất kể vị trí của chữ số.
Cuối cùng, Hình 3 cho thấy chín ví dụ kiểm tra với giá trị gating cao nhất cho mỗi tổ hợp chuyên gia.
Phân công tầng đầu tiên chạy trên các hàng, trong khi tầng thứ hai chạy trên các cột. Lưu ý
dịch chuyển của mỗi chữ số thay đổi theo hàng nhưng không đổi trên các cột, trong khi điều ngược lại đúng cho
lớp của chữ số. Hơn nữa, các lớp dễ nhầm lẫn có xu hướng được nhóm lại với nhau, ví dụ 3 và 5.
Lỗi Tập Kiểm tra: MNIST Nhiễu
Mô hình Gate Hids Chuyên gia Đơn DMoE Concat Tầng2 DNN
4×100→4×100 50→50 1.33 1.42 1.30 1.30
4×100→4×20 50→50 1.58 1.50 1.30 1.41
4×100→4×20 50→20 1.41 1.39 1.30 1.40
4×50→4×20 20→20 1.63 1.77 1.50 1.67
4×100(một tầng) 50 2.86 1.72 1.69 –
Lỗi Tập Huấn luyện: MNIST Nhiễu
Mô hình Gate Hids Chuyên gia Đơn DMoE Concat Tầng2 DNN
4×100→4×100 50→50 0.85 0.91 0.77 0.60
4×100→4×20 50→50 1.05 0.96 0.85 0.90
4×100→4×20 50→20 1.04 0.98 0.87 0.87
4×50→4×20 20→20 1.60 1.41 1.33 1.32
4×100(một tầng) 50 2.99 1.78 1.59 –
Bảng 1: So sánh DMoE cho MNIST với dịch chuyển ngẫu nhiên, với các đường cơ sở (i) chỉ sử dụng
một chuyên gia tầng thứ hai, (ii) nối tất cả chuyên gia tầng thứ hai, và (iii) một DNN với cùng
tổng số tham số. Cho cả (i) và (ii), các chuyên gia trong tầng đầu tiên được trộn để tạo thành z1.
Các mô hình được chú thích với "# chuyên gia × # đơn vị ẩn" cho mỗi tầng.
4

--- TRANG 5 ---
MNIST Nhiễu: Mô hình Sâu Hai Tầng
theo Dịch chuyển theo Lớp
Tầng 1
Phân công tầng
Phân công tầng
Phân công tầng
Phân công tầng
Tầng 2
[nhiều dòng văn bản kỹ thuật với từ khóa như "joint init random gate trained baseline block layer baseline target", "balanced", "finetune" được giữ nguyên]
MoE Một tầng
không có
nhiễu—
[văn bản kỹ thuật về "moe with relu in each expert train test with softmax", "random assignments essentially doing model averaging between layer relu networks" được giữ nguyên]
Hình 2: Đầu ra gating trung bình cho tầng đầu tiên và thứ hai, cả theo dịch chuyển và theo lớp. Màu
chỉ ra trọng số gating. Các phân phối theo dịch chuyển cho thấy phân công gating trung bình cho mỗi trong
bốn chuyên gia cho mỗi trong 99 dịch chuyển có thể. Các phân phối theo lớp cho thấy phân công gating trung bình
cho mỗi trong bốn chuyên gia (hàng) cho mỗi trong mười lớp (cột). Lưu ý
tầng đầu tiên tạo ra phân công độc quyền theo dịch chuyển, trong khi tầng thứ hai phân công chuyên gia theo
lớp. Để so sánh, chúng tôi cho thấy phân công theo lớp của một MoE tiêu chuẩn được huấn luyện trên MNIST không có
nhiễu, sử dụng 5 chuyên gia 20 đơn vị ẩn.
5.2 Giọng nói Monophone
Bảng 2 cho thấy lỗi trên tập huấn luyện và kiểm tra. Như trường hợp với MNIST, lỗi của hỗn hợp trên
tập huấn luyện nằm giữa hai đường cơ sở. Trong trường hợp này, tuy nhiên, hiệu suất tập kiểm tra
xấp xỉ giống nhau cho cả hai đường cơ sở cũng như hỗn hợp.
Hình 4 cho thấy 16 ví dụ kiểm tra với giá trị gating cao nhất cho mỗi tổ hợp chuyên gia (chúng tôi chỉ cho thấy
4 chuyên gia ở tầng thứ hai do hạn chế không gian). Như trước đây, phân công tầng đầu tiên
chạy trên các hàng, trong khi tầng thứ hai chạy trên các cột. Mặc dù không thể giải thích được như với
MNIST, mỗi tổ hợp chuyên gia dường như xử lý một phần riêng biệt của đầu vào. Điều này được
củng cố thêm bởi Hình 5, nơi chúng tôi vẽ số lượng phân công trung bình cho mỗi tổ hợp chuyên gia.
Ở đây, sự lựa chọn chuyên gia tầng thứ hai phụ thuộc ít vào sự lựa chọn chuyên gia tầng đầu tiên.
Lỗi Phone Tập Kiểm tra: Giọng nói Monophone
Mô hình Gate Hids Chuyên gia Đơn Chuyên gia Trộn Concat Tầng2
4×128→16×128 64→64 0.55 0.55 0.56
4×128(một tầng) 64 0.58 0.55 0.55
Lỗi Phone Tập Huấn luyện: Giọng nói Monophone
Mô hình Gate Hids Chuyên gia Đơn Chuyên gia Trộn Concat Tầng2
4×128→16×128 64→64 0.47 0.42 0.40
4×128(một tầng) 64 0.56 0.50 0.50
Bảng 2: So sánh DMoE cho dữ liệu giọng nói monophone. Ở đây cũng vậy, chúng tôi so sánh với
các đường cơ sở chỉ sử dụng một chuyên gia tầng thứ hai, hoặc nối tất cả chuyên gia tầng thứ hai.
5

--- TRANG 6 ---
[Văn bản kỹ thuật "ffnets single block single layer" được giữ nguyên]
Hình 3: Chín ví dụ kiểm tra với giá trị gating cao nhất cho mỗi tổ hợp chuyên gia, cho bộ dữ liệu
mnist nhiễu. Các chuyên gia tầng đầu tiên ở các hàng, trong khi tầng thứ hai ở các cột.
6 Kết luận
Mô hình Hỗn hợp Chuyên gia Sâu mà chúng tôi kiểm tra là một bước hứa hẹn hướng tới việc phát triển các
mô hình lớn, thưa thớt chỉ tính toán một tập con của chính chúng cho bất kỳ đầu vào nào được cho. Chúng tôi thấy chính xác các
phân công gating cần thiết để sử dụng hiệu quả tất cả các tổ hợp chuyên gia: cho MNIST nhiễu,
một nhân tử hóa thành dịch chuyển và lớp, và việc sử dụng riêng biệt của mỗi tổ hợp cho dữ liệu giọng nói
monophone. Tuy nhiên, chúng tôi vẫn sử dụng một hỗn hợp liên tục của đầu ra các chuyên gia thay vì hạn chế
đến vài chuyên gia hàng đầu — một phần mở rộng như vậy là cần thiết để hoàn thành mục tiêu của chúng tôi là chỉ sử dụng một phần nhỏ của
mô hình cho mỗi đầu vào. Một phương pháp thực hiện điều này cho một tầng đơn đã được mô tả bởi
Collobert et al. [4], có thể được điều chỉnh cho trường hợp đa tầng của chúng tôi; chúng tôi hy vọng sẽ giải quyết điều này
trong công trình tương lai.
Lời cảm ơn
Các tác giả muốn cảm ơn Matthiew Zeiler vì những đóng góp của anh ấy về việc thực thi các ràng buộc
cân bằng trong quá trình huấn luyện.
6

--- TRANG 7 ---
Ví dụ Phân công Chung
Tầng 1→Tầng 2→chuyên gia→cả hai tầng
Hình 4: 16 ví dụ kiểm tra với giá trị gating cao nhất cho mỗi tổ hợp chuyên gia cho dữ liệu giọng nói
monophone. Các chuyên gia tầng đầu tiên ở các hàng, trong khi tầng thứ hai ở các cột. Mỗi mẫu được biểu diễn bởi
40 giá trị tần số của nó (trục dọc) và 11 khung liên tiếp (trục ngang). Cho hình này, chúng tôi sử dụng bốn
chuyên gia trong mỗi tầng.
Giọng nói Monophone: Phân công Có điều kiện
Phân công Chuyên gia Tầng 1 Mỗi Điểm dữ liệu Tầng 2 Theo Nhãn
Chung trộn tốt
thang màu [0→] thang màu [0→]
Hình 5: Số lượng phân công chung cho bộ dữ liệu giọng nói monophone. Ở đây chúng tôi vẽ tích trung bình
của trọng số gating tầng đầu tiên và thứ hai cho mỗi tổ hợp chuyên gia. Chúng tôi chuẩn hóa mỗi
hàng, để tạo ra một phân phối có điều kiện: Điều này cho thấy các phân công gating trung bình trong tầng thứ hai
cho một phân công tầng đầu tiên. Lưu ý các phân công chung được trộn tốt: Sự lựa chọn chuyên gia tầng thứ hai
không phụ thuộc nhiều vào sự lựa chọn chuyên gia tầng đầu tiên. Màu sắc từ xanh đậm
(0) đến đỏ đậm (0.125).
7

--- TRANG 8 ---
Tài liệu tham khảo
[1] Y. Bengio. Deep learning of representations: Looking forward. CoRR, abs/1305.0445, 2013.
2
[2] Y. Bengio, N. Léonard, and A. C. Courville. Estimating or propagating gradients through
stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. 2
[3] D. C. Ciresan, U. Meier, J. Masci, L. M. Gambardella, and J. Schmidhuber. Flexible, high
performance convolutional neural networks for image classification. In IJCAI, 2011. 1
[4] R. Collobert, Y. Bengio, and S. Bengio. Scaling large learning problems with hard parallel
mixtures. International Journal on Pattern Recognition and Artificial Intelligence (IJPRAI),
17(3):349–365, 2003. 6
[5] A. Graves, A. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural net-
works. In ICASSP, 2013. 1
[6] G. E. Hinton. Products of experts. ICANN, 1:1–6, 1999. 2
[7] R. A. Jacobs, M. I. Jordan, S. Nowlan, and G. E. Hinton. Adaptive mixtures of local experts.
Neural Computation, 3:1–12, 1991. 1, 2
[8] N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke. Application of pretrained deep neural
networks to large vocabulary speech recognition. Interspeech, 2012. 4
[9] M. I. Jordan and R. A. Jacobs. Hierarchical mixtures of experts and the em algorithm. Neural
Computation, 6:181–214, 1994. 2
[10] A. Krizhevsky, I. Sutskever, and G.E. Hinton. Imagenet classification with deep convolutional
neural networks. In NIPS, 2012. 1
8
