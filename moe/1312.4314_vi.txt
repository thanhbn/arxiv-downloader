# Học biểu diễn phân tích trong mô hình hỗn hợp chuyên gia sâu

David Eigen1;2Marc'Aurelio Ranzato1Ilya Sutskever1
1Google, Inc.  
2Dept. of Computer Science, Courant Institute, NYU
deigen@cs.nyu.edu ranzato@fb.com ilyasu@google.com

Tóm tắt
Hỗn hợp chuyên gia kết hợp đầu ra của nhiều mạng "chuyên gia", mỗi mạng chuyên về một phần khác nhau của không gian đầu vào. Điều này được thực hiện bằng cách huấn luyện một mạng "cổng" ánh xạ mỗi đầu vào thành một phân phối trên các chuyên gia. Những mô hình như vậy cho thấy tiềm năng để xây dựng các mạng lớn hơn vẫn rẻ để tính toán tại thời điểm kiểm tra, và có thể song song hóa hơn tại thời điểm huấn luyện. Trong công trình này, chúng tôi mở rộng hỗn hợp chuyên gia thành mô hình xếp chồng, hỗn hợp chuyên gia sâu, với nhiều bộ cổng và chuyên gia. Điều này tăng số lượng chuyên gia hiệu quả theo cấp số nhân bằng cách liên kết mỗi đầu vào với một tổ hợp chuyên gia tại mỗi lớp, nhưng vẫn duy trì kích thước mô hình khiêm tốn. Trên một phiên bản dịch chuyển ngẫu nhiên của tập dữ liệu MNIST, chúng tôi thấy rằng hỗn hợp chuyên gia sâu tự động học để phát triển các chuyên gia phụ thuộc vị trí ("ở đâu") tại lớp đầu tiên, và các chuyên gia đặc thù lớp ("là gì") tại lớp thứ hai. Ngoài ra, chúng tôi thấy rằng các tổ hợp khác nhau được sử dụng khi mô hình được áp dụng vào tập dữ liệu các đơn âm tiếng nói. Những điều này chứng minh việc sử dụng hiệu quả tất cả các tổ hợp chuyên gia.

1 Giới thiệu
Mạng sâu đã đạt được hiệu suất rất tốt trong nhiều tác vụ khác nhau, ví dụ [10, 5, 3]. Tuy nhiên, một hạn chế cơ bản của những kiến trúc này là toàn bộ mạng phải được thực thi cho tất cả đầu vào. Gánh nặng tính toán này áp đặt giới hạn kích thước mạng. Một cách để mở rộng những mạng này lên trong khi giữ chi phí tính toán thấp là tăng tổng số tham số và đơn vị ẩn, nhưng chỉ sử dụng một phần nhỏ của mạng cho mỗi đầu vào cụ thể. Sau đó, học một hàm ánh xạ rẻ về mặt tính toán từ đầu vào đến các phần thích hợp của mạng.

Mô hình hỗn hợp chuyên gia [7] là một phiên bản liên tục của điều này: Một mạng cổng được học trộn các đầu ra của N mạng "chuyên gia" để tạo ra một đầu ra cuối cùng. Mặc dù mô hình này bản thân nó không đạt được các lợi ích tính toán được nêu ở trên, nó cho thấy tiềm năng như một bước đệm hướng tới các mạng có thể thực hiện mục tiêu này.

Trong công trình này, chúng tôi mở rộng hỗn hợp chuyên gia để sử dụng một mạng cổng khác nhau tại mỗi lớp trong mạng đa lớp, tạo thành hỗn hợp chuyên gia sâu (DMoE). Điều này tăng số lượng chuyên gia hiệu quả bằng cách giới thiệu số lượng đường dẫn theo cấp số nhân qua các tổ hợp khác nhau của chuyên gia tại mỗi lớp. Bằng cách liên kết mỗi đầu vào với một tổ hợp như vậy, mô hình của chúng tôi sử dụng các tập con khác nhau của các đơn vị cho các đầu vào khác nhau. Do đó nó có thể vừa lớn vừa hiệu quả cùng một lúc.

Chúng tôi chứng minh tính hiệu quả của cách tiếp cận này bằng cách đánh giá nó trên hai tập dữ liệu. Sử dụng tập dữ liệu MNIST bị nhiễu, chúng tôi cho thấy rằng DMoE học để phân tích các khía cạnh khác nhau của biểu diễn dữ liệu tại mỗi lớp (cụ thể là vị trí và lớp), sử dụng hiệu quả tất cả các đường dẫn. Chúng tôi cũng thấy rằng tất cả các tổ hợp được sử dụng khi áp dụng mô hình của chúng tôi vào tập dữ liệu các đơn âm tiếng nói.

Marc'Aurelio Ranzato hiện đang làm việc tại nhóm AI Facebook.

2 Nghiên cứu liên quan
Một hỗn hợp chuyên gia tiêu chuẩn (MoE) [7] học một tập các mạng chuyên gia fi cùng với một mạng cổng g. Mỗi fi ánh xạ đầu vào x thành C đầu ra (một cho mỗi lớp c = 1, ..., C), trong khi g(x) là một phân phối trên các chuyên gia i = 1, ..., N có tổng bằng 1. Đầu ra cuối cùng sau đó được cho bởi phương trình 1

FMoE(x) = ∑(i=1 to N) gi(x)softmax(fi(x))  (1)
= ∑(i=1 to N) p(ei|x)p(c|ei,x) = p(c|x)  (2)

Điều này cũng có thể được xem như một mô hình xác suất, trong đó xác suất cuối cùng trên các lớp được tích phân biên trên việc lựa chọn chuyên gia: đặt p(ei|x) = gi(x) và p(c|ei,x) = softmax(fi(x)), chúng ta có phương trình 2.

Một tích chuyên gia (PoE) [6] tương tự, nhưng thay vào đó kết hợp log xác suất để tạo thành một tích:

FPoE(x) ∝ ∏(i=1 to N) softmax(fi(x)) = ∏(i=1 to N) pi(c|x)  (3)

Cũng liên quan chặt chẽ đến công trình của chúng tôi là hỗn hợp chuyên gia phân cấp [9], học một cấu trúc phân cấp của các mạng cổng trong cấu trúc cây. Đầu ra của mỗi mạng chuyên gia tương ứng với một lá trong cây; các đầu ra sau đó được trộn theo trọng số cổng tại mỗi nút.

Mô hình của chúng tôi khác với mỗi mô hình trong ba mô hình này vì nó lắp ráp động một tổ hợp chuyên gia phù hợp cho mỗi đầu vào. Đây là một thể hiện của khái niệm tính toán có điều kiện được đưa ra bởi Bengio [1] và được kiểm tra trong môi trường ngẫu nhiên một lớp bởi Bengio, Leonard và Courville [2]. Bằng cách điều kiện hóa các mạng cổng và chuyên gia của chúng tôi trên đầu ra của lớp trước, mô hình của chúng tôi có thể biểu diễn một số lượng chuyên gia hiệu quả lớn theo cấp số nhân.

3 Phương pháp
Để mở rộng MoE thành DMoE, chúng tôi giới thiệu hai tập chuyên gia với các mạng cổng (g1, f1i) và (g2, f2j), cùng với một lớp tuyến tính cuối cùng f3 (xem hình 1). Đầu ra cuối cùng được tạo ra bằng cách kết hợp các hỗn hợp tại mỗi lớp:

z1 = ∑(i=1 to N) g1i(x)f1i(x)
z2 = ∑(j=1 to M) g2j(z1)f2j(z1)
F(x) = z3 = softmax(f3(z2))

Chúng tôi đặt mỗi fli thành một ánh xạ tuyến tính đơn với chỉnh lưu, và mỗi gli thành hai lớp ánh xạ tuyến tính với chỉnh lưu (nhưng với ít đơn vị ẩn); f3 là một lớp tuyến tính đơn. Xem mục 4 để biết chi tiết.

Chúng tôi huấn luyện mạng bằng cách sử dụng gradient descent ngẫu nhiên (SGD) với ràng buộc bổ sung về phân công cổng (được mô tả dưới đây). Chỉ SGD dẫn đến cực tiểu địa phương thoái hóa: Các chuyên gia tại mỗi lớp hoạt động tốt nhất cho một vài ví dụ đầu tiên cuối cùng áp đảo các chuyên gia còn lại. Điều này xảy ra vì các ví dụ đầu tiên tăng trọng số cổng của những chuyên gia này, điều này lần lượt làm cho chúng được chọn với trọng số cổng cao thường xuyên hơn. Điều này làm cho chúng huấn luyện nhiều hơn, và trọng số cổng của chúng tăng lên lần nữa, cứ thế tiếp tục.

Để chống lại điều này, chúng tôi đặt một ràng buộc về phân công cổng tương đối cho mỗi chuyên gia trong quá trình huấn luyện. Cho Gli(t) = ∑(t'=1 to t) gli(xt') là tổng phân công tích lũy cho chuyên gia i của lớp l tại bước t, và cho Gl(t) = (1/N)∑(i=1 to N) Gli(t) là trung bình của chúng (ở đây, xt' là ví dụ huấn luyện tại bước t'). Sau đó cho mỗi chuyên gia i, chúng tôi đặt gli(xt) = 0 nếu Gli(t) - Gl(t) > m với ngưỡng biên m, và chuẩn hóa lại phân phối gl(xt) để có tổng bằng 1 trên các chuyên gia i. Điều này ngăn chặn việc chuyên gia bị sử dụng quá mức ban đầu, dẫn đến phân công cân bằng. Sau khi huấn luyện với ràng buộc, chúng tôi bỏ nó và tiếp tục huấn luyện trong giai đoạn tinh chỉnh thứ hai.

4 Thí nghiệm

4.1 MNIST nhiễu
Chúng tôi đã huấn luyện và kiểm tra mô hình của chúng tôi trên MNIST với dịch chuyển ngẫu nhiên đồng nhất 4 pixel, tạo ra hình ảnh grayscale kích thước 36×36. Như đã giải thích ở trên, mô hình được huấn luyện để phân loại các chữ số thành mười lớp.

Cho tác vụ này, chúng tôi đặt tất cả f1i và f2j thành các mô hình tuyến tính một lớp với chỉnh lưu, f1i(x) = max(0, W1ix + b1i), và tương tự cho f2j. Chúng tôi đặt f3 thành một lớp tuyến tính, f3(z2) = W3z2 + b3.

Chúng tôi thay đổi số lượng đơn vị ẩn đầu ra của f1i và f2j giữa 20 và 100. Đầu ra cuối cùng từ f3 có 10 đơn vị (một cho mỗi lớp).

Các mạng cổng g1 và g2 mỗi cái bao gồm hai lớp tuyến tính + chỉnh lưu với 50 hoặc 20 đơn vị ẩn, và 4 đơn vị đầu ra (một cho mỗi chuyên gia), tức là g1(x) = softmax(B max(0, Ax + a) + b), và tương tự cho g2.

Chúng tôi đánh giá hiệu quả của việc sử dụng hỗn hợp tại lớp thứ hai bằng cách so sánh với việc chỉ sử dụng một chuyên gia cố định duy nhất tại lớp thứ hai, hoặc nối đầu ra của tất cả chuyên gia. Lưu ý rằng cho một hỗn hợp với h đơn vị ẩn, mô hình nối tương ứng có Nh đơn vị ẩn. Do đó chúng tôi mong đợi mô hình nối hoạt động tốt hơn hỗn hợp, và hỗn hợp hoạt động tốt hơn mạng đơn. Tốt nhất là hỗn hợp gần với giới hạn chuyên gia nối nhất có thể. Trong mỗi trường hợp, chúng tôi giữ kiến trúc lớp đầu tiên như nhau (một hỗn hợp).

Chúng tôi cũng so sánh mô hình hai lớp với mô hình một lớp trong đó lớp ẩn z1 được ánh xạ đến đầu ra cuối cùng thông qua lớp tuyến tính và softmax. Cuối cùng, chúng tôi so sánh với mạng sâu kết nối đầy đủ với cùng tổng số tham số. Điều này được xây dựng bằng cách sử dụng cùng số lượng đơn vị lớp thứ hai z2, nhưng mở rộng số lượng đơn vị lớp đầu tiên z1 sao cho tổng số tham số giống như DMoE (bao gồm tham số mạng cổng của nó).

4.2 Đơn âm tiếng nói
Ngoài ra, chúng tôi chạy mô hình trên tập dữ liệu mẫu đơn âm tiếng nói. Tập dữ liệu này là một tập con ngẫu nhiên của khoảng một triệu mẫu từ cơ sở dữ liệu độc quyền lớn hơn của vài trăm giờ dữ liệu tiếng Anh Mỹ được thu thập bằng tìm kiếm giọng nói, nhập giọng nói và dữ liệu đọc [8]. Cho các thí nghiệm của chúng tôi, mỗi mẫu được giới hạn ở 11 khung cách nhau 10ms, và có 40 bin tần số. Mỗi đầu vào được đưa vào mạng như một vector 440 chiều. Có 40 lớp âm vị đầu ra có thể.

Chúng tôi huấn luyện mô hình với 4 chuyên gia tại lớp đầu tiên và 16 tại lớp thứ hai. Cả hai lớp đều có 128 đơn vị ẩn. Các mạng cổng mỗi cái có hai lớp, với 64 đơn vị trong lớp ẩn. Như trước đây, chúng tôi đánh giá hiệu quả của việc sử dụng hỗn hợp tại lớp thứ hai bằng cách so sánh với việc chỉ sử dụng một chuyên gia duy nhất tại lớp thứ hai, hoặc nối đầu ra của tất cả chuyên gia.

5 Kết quả

5.1 MNIST nhiễu
Bảng 1 cho thấy lỗi trên tập huấn luyện và kiểm tra cho mỗi kích thước mô hình (tập kiểm tra là tập kiểm tra MNIST với một dịch chuyển ngẫu nhiên duy nhất cho mỗi hình ảnh). Trong hầu hết các trường hợp, chuyên gia xếp chồng sâu hoạt động giữa các baseline chuyên gia đơn và nối trên tập huấn luyện, như mong đợi. Tuy nhiên, các mô hình sâu thường chịu overfitting: lỗi của hỗn hợp trên tập kiểm tra tệ hơn chuyên gia đơn cho hai trong số bốn kích thước mô hình. Đáng khích lệ, DMoE hoạt động gần như tốt bằng mạng kết nối đầy đủ (DNN) với cùng số tham số, mặc dù mạng này áp đặt ít ràng buộc hơn về cấu trúc.

Trong hình 2, chúng tôi cho thấy phân công trung bình cho mỗi chuyên gia (tức là đầu ra cổng trung bình), cả theo dịch chuyển đầu vào và theo lớp. Lớp đầu tiên phân công chuyên gia theo dịch chuyển, trong khi phân công đồng nhất theo lớp. Ngược lại, lớp thứ hai phân công chuyên gia theo lớp, nhưng đồng nhất theo dịch chuyển. Điều này cho thấy hai lớp chuyên gia thực sự được sử dụng theo cách bổ sung, để tất cả tổ hợp chuyên gia đều hiệu quả. Các chuyên gia lớp đầu tiên trở nên chọn lọc với nơi chữ số xuất hiện, bất kể lớp thành viên của nó, trong khi các chuyên gia lớp thứ hai chọn lọc với lớp chữ số là gì, bất kể vị trí của chữ số.

Cuối cùng, hình 3 cho thấy chín ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi tổ hợp chuyên gia. Phân công lớp đầu tiên chạy trên các hàng, trong khi lớp thứ hai chạy trên các cột. Lưu ý dịch chuyển của mỗi chữ số thay đổi theo hàng nhưng không đổi trên các cột, trong khi điều ngược lại đúng cho lớp của chữ số. Hơn nữa, các lớp dễ nhầm lẫn có xu hướng được nhóm lại với nhau, ví dụ 3 và 5.

5.2 Đơn âm tiếng nói
Bảng 2 cho thấy lỗi trên tập huấn luyện và kiểm tra. Như trường hợp MNIST, lỗi của hỗn hợp trên tập huấn luyện nằm giữa hai baseline. Tuy nhiên, trong trường hợp này, hiệu suất tập kiểm tra gần như giống nhau cho cả hai baseline cũng như hỗn hợp.

Hình 4 cho thấy 16 ví dụ kiểm tra với giá trị cổng cao nhất cho mỗi tổ hợp chuyên gia (chúng tôi chỉ hiển thị 4 chuyên gia tại lớp thứ hai do hạn chế không gian). Như trước đây, phân công lớp đầu tiên chạy trên các hàng, trong khi lớp thứ hai chạy trên các cột. Mặc dù không dễ giải thích như MNIST, mỗi tổ hợp chuyên gia dường như xử lý một phần riêng biệt của đầu vào. Điều này được củng cố thêm bởi hình 5, nơi chúng tôi vẽ số lượng phân công trung bình cho mỗi tổ hợp chuyên gia. Ở đây, lựa chọn chuyên gia lớp thứ hai phụ thuộc ít vào lựa chọn chuyên gia lớp đầu tiên.

6 Kết luận
Mô hình hỗn hợp chuyên gia sâu mà chúng tôi xem xét là một bước hứa hẹn hướng tới phát triển các mô hình lớn, thưa thớt chỉ tính toán một tập con của chính chúng cho bất kỳ đầu vào nào. Chúng tôi thấy chính xác các phân công cổng cần thiết để sử dụng hiệu quả tất cả tổ hợp chuyên gia: cho MNIST nhiễu, một phân tích thành dịch chuyển và lớp, và sử dụng đặc biệt của mỗi tổ hợp cho dữ liệu đơn âm tiếng nói. Tuy nhiên, chúng tôi vẫn sử dụng hỗn hợp liên tục của đầu ra chuyên gia thay vì hạn chế đến vài chuyên gia hàng đầu — một mở rộng như vậy là cần thiết để hoàn thành mục tiêu chỉ sử dụng một phần nhỏ của mô hình cho mỗi đầu vào. Một phương pháp thực hiện điều này cho một lớp đã được mô tả bởi Collobert et al. [4], có thể được điều chỉnh cho trường hợp đa lớp của chúng tôi; chúng tôi hy vọng giải quyết điều này trong công việc tương lai.

Lời cảm ơn
Các tác giả xin cảm ơn Matthiew Zeiler vì đóng góp của anh ấy trong việc thực thi các ràng buộc cân bằng trong quá trình huấn luyện.
