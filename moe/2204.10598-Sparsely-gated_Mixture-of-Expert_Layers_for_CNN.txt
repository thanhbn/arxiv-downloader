# 2204.10598.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/moe/2204.10598.pdf
# File size: 12805572 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Sparsely-gated Mixture-of-Expert Layers for CNN
Interpretability
Svetlana Pavlitska1;2, Christian Hubschneider1;2, Lukas Struppek2, J. Marius Z ¬®ollner1;2
1FZI Research Center for Information Technology
2Karlsruhe Institute of Technology (KIT)
Karlsruhe, Germany
pavlitska@fzi.de
Abstract ‚ÄîSparsely-gated Mixture of Expert (MoE) layers have
been recently successfully applied for scaling large transformers,
especially for language modeling tasks. An intriguing side effect
of sparse MoE layers is that they convey inherent interpretability
to a model via natural expert specialization. In this work, we
apply sparse MoE layers to CNNs for computer vision tasks
and analyze the resulting effect on model interpretability. To
stabilize MoE training, we present both soft and hard constraint-
based approaches. With hard constraints, the weights of certain
experts are allowed to become zero, while soft constraints balance
the contribution of experts with an additional auxiliary loss.
As a result, soft constraints handle expert utilization better
and support the expert specialization process, while hard con-
straints maintain more generalized experts and increase overall
model performance. Our Ô¨Åndings demonstrate that experts can
implicitly focus on individual sub-domains of the input space.
For example, experts trained for CIFAR-100 image classiÔ¨Åcation
specialize in recognizing different domains such as Ô¨Çowers or
animals without previous data clustering. Experiments with
RetinaNet and the COCO dataset further indicate that object
detection experts can also specialize in detecting objects of
distinct sizes.
Index Terms ‚Äîmixtures of experts, interpretability
I. I NTRODUCTION
Sparse Mixture of Expert (MoE) layers have recently gained
popularity thanks to their ability to scale up models to
billions and lately even trillions of parameters [1]‚Äì[3]. The
focus, however, was almost exclusively on transformer models
for language modeling tasks. In this work, we insert MoE
layers into convolutional neural networks (CNNs) and apply
the approach to the basic computer vision tasks of image
classiÔ¨Åcation and object detection. To tackle a well-known
problem of unstable expert training, we present soft and
hard constraints, encouraging balanced expert utilization. The
models further provide an additional hyperparameter to adjust
the number of active experts in each forward pass and, thereby,
the computational complexity.
Inherent model interpretability is one side effect of embed-
ding MoE layers into model architectures. For the language
modeling tasks, the experts were shown to mostly specialize
on shallow concepts [2], [4], [5]. All previous works rely on
transformers. To the best of our knowledge, we are the Ô¨Årst to
study the impact of sparse MoE layers, embedded in CNNs,
on model interpretability in computer vision tasks. Due to the
Gate...
InputOutput
Fig. 1: The integration of sparse MoE layers into a ResNet-like
architecture: each expert is a residual block. The gate selects
topk(here:k= 2) active experts and predicts the weighting
of their outputs. When trained, experts specialize on semantic
label groups, e.g., Ô¨Çowers or animals.
larger receptive Ô¨Åeld, experts can focus on more high-level
semantic concepts.
Our contributions can be summarized as follows:
We apply the concept of sparse MoE layers, primarily
used in transformers so far, to CNNs.
We analyze the semantics of the learned experts and
evaluate the impact of different constraints for balancing
load specialization on the interpretability strength of the
model.
We evaluate the concept on two separate tasks: image
classiÔ¨Åcation and object detection.
II. R ELATED WORK
A. Sparse MoEs
Classic mixture-of-expert models, introduced by Jacobs et
al. [6], consist of a variable number of expert models and
a single gate to combine the expert outputs. Eigen et al. [7]
developed the idea of using MoEs as subcomponents of models
with individually learned gates. This allows for sharing the
remaining parts of architectures and enables multiple MoE
1arXiv:2204.10598v3  [cs.CV]  27 Apr 2023

--- PAGE 2 ---
layers within a single architecture. The transition to conditional
computing through sparse expert activations was Ô¨Årst explored
by Shazeer et al. [8] using LSTM networks. By activating
a Ô¨Åxed number of experts, it was possible to decouple the
number of parameters from FLOPs required for inference
through MoEs.
Fedus et al. [1] took a further step towards sparse activations
and showed that it is possible to train transformer-based MoE
models while activating only a single expert, resulting in MoE
layers that add only little compute overhead. Several works [9],
[10] have since explored the single-expert regime, and Zopf
et al. [2] derived guidelines to design sparse MoEs effectively
using transformers.
Sparse MoEs have thus gained popularity for massive lan-
guage models [8], [11], [12]. Performance boosts, achieved via
sparse MoEs, make training these models possible, although
only on large clusters of powerful GPUs.
In computer vision, the application of sparse MoE has cen-
tered around transformer models [9], [13], [14]. In contrast, in
our work, we consider CNNs, which are still the most widely
spread architecture in the computer vision area.
Although conventional MoEs at the model level have been
successfully evaluated on CNNs [15]‚Äì[19], embedding MoE
layers into CNNs has received signiÔ¨Åcantly less attention.
One of the few works is that by Yang et al. [20], where
the kernels of convolutional layers are activated on a per-
example basis. Wang et al. [21] explored a deep dynamic
routing approach for CNNs. In their DeepMoE model, each
convolutional layer is replaced with an MoE layer, and a multi-
headed gating network selects and re-weights the channels
in each convolutional layer. DeepMoE is trained end-to-end,
it demonstrated a 1000x increase in model capacity while
maintaining computational efÔ¨Åciency with only minor losses.
Differently from the work by Wang et al. [21], we propose
experts at the residual block level and analyze to which extent
the latent representations learned by a model correspond to
human-understandable concepts.
B. Balancing Expert Utilization
A known MoE problem is the focus of gates on a small
subset of all available experts. The weights assigned to other
experts are permanently zero or negligibly small. Because
some experts perform better in the Ô¨Årst iterations, the gate
increases their probability to be activated. Consequently, these
few experts improve above average, and the gate assigns even
higher weights to them. This self-reinforcing process contin-
ues, s.t. the optimizer ends up in a poor local minimum [7].
Eigen et al. [7] proposed a hard constraint on the relative
gating assignments to each expert applied during training.
For this, the weights assigned to each expert are summed
over all training samples. If this value surpasses the average
performance in the Ô¨Årst iterations, the gate increases their
probability of being zero. The remaining positive weights are
recomputed using softmax to maintain a convex combination
of the experts. Further hard constraints were proposed based
on expert capacities [1], [4], [9] to encourage the usage ofall experts but also to ensure efÔ¨Åcient usage of the available
hardware.
Shazeer et al. [8] presented a soft constraint approach
introducing an auxiliary importance loss (see below), which
encourages equal importance for all experts during training.
The number of training samples per expert may still vary since
importance is used instead of the mean number of samples per
expert.
Another auxiliary loss for load balancing was proposed by
Lepikhin et al. [11] ‚Äì it limits the number of tokens processed
by one expert with a threshold. The auxiliary loss in the switch
transformer model [1] aimed at the uniform distribution of a
batch of tokens.
C. Interpretability via Sparse MoEs
We refer to interpretability as a model ability to explain or to
provide meaning to a human in an understandable way [22].
Semantics or visual explainability of a model is crucial to
ensure that humans can trust its predictions [23]. Out of
a plethora of interpretability approaches [24], MoEs favor
those which open up the black box and analyze intermediate
representations learned by a model.
In the early experiments by Shazeer et al. [8] on the tasks of
language modeling, the experts specialized on syntax and/or
semantics. The authors provide examples of the specialization
for three selected experts: one expert was used for the word
innovation , another one for the article a, and another one on
the concept of fast, rapid action.
Lewis et al. [4] showed that experts specialize in very local
syntactic information: experts learned clusters of numbers,
abbreviations, possessive pronouns, etc. No specialization at
the semantic level was observed.
Similar behavior was described in the later work by Zoph
et al. [2]. The experts were found to specialize in punc-
tuation, articles, conjunctions, proper nouns, and numbers.
Interestingly, even in the case of multilingual sparse models,
no specialization in languages was observed. Instead, the
experts continued to focus on the same shallow concepts
like punctuation, articles, or numbers. Mustafa et al. [5] ex-
plored multimodal sparsely-activated models. The text experts
specialized in nouns and adjectives, whereas image experts
specialized on semantic concepts like body parts, textures,
fauna, food, and doors.
The work on visual sparse transformers by Riquelme et
al. [13] is closest to ours because it deals with image data.
Here, the experts specialize in discriminating between small
sets of classes. Expert-class correlation is strong only for the
last few layers, whereas no expert specialization was observed
for the early layers. Further work with visual transformers by
Wu et al. [25] has demonstrated expert specialization across
ImageNet classes.
In the case of visual transformers, image patches are tokens,
routed to experts. In our work, the routing is performed at the
level of whole images, which leads to semantic specialization
at the image level.
2

--- PAGE 3 ---
III. A PPROACH
Our approach comprises three components: (1) embedding
sparse MoE layers in CNNs, (2) balancing expert utilization
via constraints, and (3) expert specialization analysis revealing
concepts learned in MoE layers.
A. Sparsely-gated MoEs for CNNs
We propose a method to embed sparse MoE layers into
CNNs with the goal to achieve computational complexity
similar to the baseline in terms of parameters. Without loss
of generality, we consider a CNN consisting of residual
blocks [26] (see Figure 1). The proposed ResBlock-MoE
architecture uses a complete residual block as its expert. The
MoE layer encapsulates multiple replicas of the block that are
activated and mixed using a gate. We experiment with inserting
MoE layers at different positions in the model architecture.
We consider two types of gates (see Figure 2): the GAP-
FC gate consists of a GAP layer followed by a single
fully-connected layer, whereas the Conv-GAP-FC additionally
contains a convolutional layer, which can use detailed, local
information encoded in the input features.
B. Constraints to Balance Expert Utilization
Formally, an MoE consists of a set of NexpertsE1;:::;E N.
For a given input x, each expert Eiproduces an output ei(x).
The gate computes a weight vector G(x) = [g1(x);:::;g N(x)].
The Ô¨Ånal MoE output is a weighted sum of the expert outputs:
FMoE(x) =PN
i=1gi(x)ei(x). To measure the utilization of
experts, we deÔ¨Åne an importance vectorI(X) =P
x2XG(x)
for each batch of training samples X, and the importance of
a single expert EiasIi(X) =P
x2Xgi(x)[8].
We refer to the problem of unbalanced MoE expert uti-
lization as dying experts , analogous to the dying ReLU [27].
We consider an expert as dead if it receives less than 1%
average importance on the test set. To mitigate the problem,
we propose one soft and two hard constraints.
Hard Constraints : motivated by the work of Eigen et
al. [7], we propose two hard constraints on importance. Both
hard constraints are only active during training and deactivate
experts for an entire batch.
We denote the mean importance of batch XasI(X)and
deÔ¨Åne the relative importance of expertEiforXas follows:
Irel
i(X) =Ii(X) I(X)
I(X)(1)
In the relative importance constraint , the expert weight is
zeroed for a batch, if the running relative importance of this
expert exceeds the predeÔ¨Åned threshold mrel:
gi(Xt) = 0()t 1X
t0=1Irel
i(Xt0)>m rel (2)
For the mean importance constraint , we deÔ¨Åne the mean
importance assigned to expert Eiup to time step t:
Ii(Xt) =1
ttX
t0=1Ii(Xt0)
jXt0j(3)
Softmax
Fully connected  
( N)
Global average
pooling
xG(x)(a)GAP-FC gate
Softmax
Fully connected  
(N)
Global average
pooling
3x3 Conv , 256
xG(x) (b)Conv-GAP-FC gate
Fig. 2: Gate architectures.
In this constraint, the expert weight is zeroed when the mean
importance for this expert exceeds the mean importance of the
batch by some predeÔ¨Åned threshold mmean :
gi(Xt) = 0()Ii(Xt) I(Xt)>m mean (4)
The relative importance constraint takes a stronger focus on
the recent past, whereas the mean importance approach takes a
holistic view, with all past importance values having the same
impact on the constraint.
Soft Constraints : the Ô¨Årst soft-constrained approach, origi-
nally proposed by Shazeer et al. [8], is an auxiliary importance
lossLimp, following Equation 5. It uses the squared coefÔ¨Åcient
of variation of importance vector I(X)for batchXand a
weighting factor wimp:
Limp=wimpCV(I(X))2=wimpI(X)
I(X)2
(5)
We propose another soft constraint that takes a probabilistic
view on expert importance. For this, we interpret an MoE as a
probability model in which class probabilities are marginalized
over expert selection. Each weight gi(x)is thus the probability
p(Eijx)to select a speciÔ¨Åc expert for a given input, and
outputs of each expert ei(x)quantify the probability p(cjEi;x)
of each class c2C. The MoE output is then deÔ¨Åned as
follows:
FMoE(x) =NX
i=1p(Eijx)p(cjEi;x) =p(cjx) (6)
We interpret the gate output as a discrete probability distribu-
tionPwith probability P(EijX)for expertEito be selected
for inputX(Xis a random variable for input x).
In expectation, the gate should assign each expert Eithe
same average weighting, equal to EX[P(EijX)] =1
N. The
expected weight assignment thus corresponds to a discrete uni-
form distribution Qwith probability Q(EijX) =Q(Ei) =1
N.
We deÔ¨Åne an auxiliary KL-divergence loss LKLas the
KL-divergence DKL(PjjQ)betweenPandQ, weighted by
3

--- PAGE 4 ---
TABLE I: Expert utilization: coefÔ¨Åcients of variation for the number of samples ( CVact) and importance ( CVimp) assigned to
each expert. A higher CV value means a higher variance in expert utilization. #stands for the number of experts alive. We
highlight cases when all experts are alive.
MoE Soft constraints Hard constraints
position Importance loss KL-divergence loss Relative importance Mean importance
CVact CVimp #CVact CVimp #CVact CVimp # CVact CVimp #
1 6.72 3.82 4 9.23 6.36 4 56.00 56.34 4 97.37 103.23 2.33
ResBlock-MoE , 2 10.24 2.44 4 10.55 2.99 4 52.20 51.61 4 78.32 97.60 3.00
4 experts 3 5.87 2.17 4 9.35 2.21 4 43.19 25.21 4 90.98 98.32 2.33
4 7.97 2.86 4 8.41 2.79 4 13.39 2.91 4 62.99 95.53 3.00
1 10.69 10.43 10 14.93 13.47 10 160.60 168.11 5.33 199.77 200.02 2.00
ResBlock-MoE , 2 7.25 8.63 10 8.62 8.56 10 165.66 186.78 5.67 170.56 182.24 3.00
10 experts 3 6.43 5.83 10 10.51 5.10 10 125.13 144.25 6.67 166.65 158.01 3.33
4 4.80 4.48 10 8.95 6.67 10 10.61 9.37 10 147.22 158.82 3.67
TABLE II: Mean accuracy and sample standard deviation of image classiÔ¨Åcation models with sparse MoE layers and GAP-FC
gate. We highlight cases that beat the baseline accuracy.
MoE Soft constraints Hard constraints
position Importance loss KL-divergence loss Relative importance Mean importance
Baseline 72.620.29
1 72.240.49 72.720.36 72.210.42 73.000.40
ResBlock-MoE , 2 72.180.29 72.250.17 72.180.48 72.950.35
4 experts 3 71.650.43 71.540.22 72.050.15 72.610.37
4 71.950.39 71.800.23 73.100.25 72.570.31
1 71.510.23 71.320.60 71.600.09 72.280.15
ResBlock-MoE, 2 72.760.50 71.870.30 71.510.44 72.080.32
10 experts 3 71.470.29 71.430.12 70.820.16 72.050.61
4 71.610.16 71.990.23 72.840.30 73.090.35
hyperparameter wKL. The probability P(EijX=X) =Ii(X)
jXj
is computed as the average importance per sample in batch X.
LKLis then deÔ¨Åned as follows:
LKL=wKLDKL(PjjQ) =
wKLNX
i=1P(EijX)lnP(EijX)
Q(Ei)
=wKLNX
i=1Ii(X)
jXjlnIi(X)N
jXj(7)
Limppenalizes inequality in importance distribution harder
thanLKL, thus achieving an equal expert utilization. On
the other hand,LKLleads to higher variance in the expert
utilization but still avoids dying experts.
IV. E XPERIMENTS WITH SPARSE MOES FOR IMAGE
CLASSIFICATION
A. Experimental Setup
We use the ResNet-18 [26] architecture1consisting of four
ResNet blocks, while the Ô¨Årst convolutional and pooling layers
were replaced by a single 3x3 convolution to adjust for the
lower input resolution. We run experiments on the CIFAR-100
dataset [28]. The MoE experts follow the architecture of the
residual blocks, but we adjust the number of Ô¨Ålters of each
expert in a bottleneck manner, thus reducing the number of
parameters to maintain comparable computational complexity.
1Available at https://github.com/pytorch/vision/blob/master/torchvision/
models/resnet.pyWe also add an additional projection shortcut that connects
the MoE layer input with its output. We evaluate embedding
sparsely-gated MoE in each possible position in ResNet-18.
Since representations learned by a CNN evolve from earlier
to later layers, we expect the MoE layer to learn human-
understandable concepts to a greater extent when inserted in
later layers.
We train models with 4 experts and set the number of active
experts tok= 2 . We set weights for auxiliary losses to
wimp=wKL= 0:5, and thresholds for hard constraints to
mrel= 0:5andmmean = 0:3.
All experiments are performed using NVIDIA GeForce
RTX 2080 TI GPU. The models are trained with batch size 128
for 150 epochs with the Adam optimizer [29]. All experiments
are repeated three times, and the averaged values are reported.
B. Performance and Expert Utilization
Dying experts : soft constraints successfully mitigated the
experts death (see Table I). Both auxiliary losses minimize the
variation in the importance per expert, whereas a higher varia-
tion in the number of samples per expert was observed for the
KL-divergence loss. Hard constraints, however, demonstrated
worse results. Mean importance models could not keep all
experts alive in a single model, whereas relative importance
models had no dying experts for models with 4 experts. Also,
increasing the number of experts to 10 led to more variance
in expert utilization.
Accuracy and computational cost : in our setting, embed-
ding MoE layers into the model aims at boosting the inter-
4

--- PAGE 5 ---
(a) Importance loss, pos. 1
 (b) Importance loss, pos. 4
 (c) Relative importance, pos. 1
 (d) Relative importance, pos. 4
Fig. 3: Visualization of gate logits using t-SNE for ResBlock-MoE with 4 experts and a GAP-FC gate.
Expert 1 Expert 2 Expert 3 Expert 4Gating Network
(a)GAP-FC gate, importance loss, MoE layer
at position 1
Expert 1 Expert 2 Expert 3 Expert 4Gating Network(b)GAP-FC gate, importance loss, MoE layer
at position 4
Expert 1 Expert 2 Expert 3 Expert 4Gating Network(c)Conv-GAP-FC gate, importance loss,
MoE layer at position 4
Expert 1 Expert 2 Expert 3 Expert 4Gating Network
(d)GAP-FC gate, relative importance con-
straint, MoE layer at position. 1
Expert 1 Expert 2 Expert 3 Expert 4Gating Network(e)GAP-FC gate, relative importance con-
straint, MoE layer at position 4
Fig. 4: Assignment of input samples to speciÔ¨Åc experts in models with ResBlock-MoE with 4 experts, visualized with t-SNE.
pretability strength, not at beating the baseline performance.
Out of the evaluated models, the best results were achieved
with the mean importance constraint (see Table II). The choice
of a constraint thus provides an evident trade-off between
overall performance and the death of experts.
In all models, we keep the number of active experts
k= 2 to maintain a computational budget comparable to the
baseline. We measure the computational budget in the multi-
ply‚Äìaccumulate (MAC) operations, and the baseline reaches
0.56 GMac. Although increasing the number of active experts
kled to better performance, each additionally activated expert
adds about 0.06 to 0.08 GMac depending on the layer position.
Fork= 3, the overall model reaches on average 0.63 GMac,
fork= 4 already 0.7 GMac. kthus controls a trade-off
between accuracy and computational complexity.
Impact of the gate architecture : we have evaluated replac-
ing the GAP-FC gate with Conv-GAP-FC gate, exemplary forResBlock-MoE with 4 experts at position 4 and importance
loss. This model reached the accuracy of 72.42 0.27, beating
the corresponding GAP-FC model (71.950.39), but not the
baseline (72.620.29). Hard-constrained models using Conv-
GAP-FC gate suffer massively from dying experts, even with
a decreased learning rate.
C. Interpretability via Sparsely-Gated MoEs
Dataset partitioning by gate : visual assessment of the gate
logits, plotted with t-SNE (see Figure 3) demonstrates, that
MoE at position 1 leads to assignment based on the dominant
colors of input images, whereas for position 4, the distinctions
are much more faded. Furthermore, visible structures are less
signiÔ¨Åcant for the soft-constrained models, compared to the
hard-constrained case.
The resulting sample assignment to different experts reveals
more striking differences across constraints (see Figure 4).
For earlier layers, the gate divides the data into 2 major
5

--- PAGE 6 ---
TABLE III: Class-wise weight allocation for the ResNet-18 models. Results for the top 5 classes for which the experts receive
the largest weights.
Constraint Gate MoE Expert 1 Expert 2 Expert 3 Expert 4
position Class Weight Class Weight Class Weight Class WeightSoft
Importance loss
GAP-FCorange 0.71 caterpillar 0.4 chair 0.59 dolphin 0.78
rose 0.64 forest 0.38 cockroach 0.55 shark 0.68
1 apple 0.59 mushroom 0.34 tank 0.46 mountain 0.66
sweet pepper 0.58 butterÔ¨Çy 0.31 plate 0.45 skyscraper 0.65
sunÔ¨Çower 0.56 aquarium Ô¨Åsh 0.30 lawn mower 0.43 sea 0.63
mountain 0.86 leopard 0.78 poppy 0.80 chair 0.85
dolphin 0.86 forest 0.75 orange 0.79 cockroach 0.85
4 shark 0.80 porcupine 0.72 rose 0.76 telephone 0.65
sea 0.79 tiger 0.64 tulip 0.71 lamp 0.64
whale 0.79 mushroom 0.63 sunÔ¨Çower 0.71 bottle 0.63Conv-GAP-
FCplain 0.88 chair 0.90 leopard 0.77 poppy 0.74
mountain 0.87 telephone 0.80 porcupine 0.75 orange 0.70
4 sea 0.84 cockroach 0.78 crocodile 0.65 tulip 0.69
dolphin 0.82 clock 0.63 forest 0.64 rose 0.66
cloud 0.78 cup 0.60 kangaroo 0.63 hamster 0.65Hard
Relative
importance
GAP-FCchair 0.96 plain 0.87 poppy 0.86 wolf 0.73
television 0.79 willow tree 0.82 sunÔ¨Çower 0.83 racoon 0.69
4 bottle 0.79 oak tree 0.79 rose 0.80 possum 0.60
telephone 0.75 forest 0.68 tulip 0.77 motorcycle 0.58
cup 0.74 maple tree 0.66 orange 0.74 skunk 0.56
TABLE IV: Correlation between weight assignment and expert performance for the models with the ResBlock-MoE layer at
position 4.
Importance loss Relative importance constraint
Pearson Spearman Pearson Spearman
Correlation between expert accuracy and sparse weights 0.8720 0.8870 0.5827 0.5765
Correlation between expert accuracy and non-sparse weights 0.8705 0.8903 0.5919 0.5981
Correlation between expert accuracy and activations per expert 0.8501 0.8301 0.5981 0.6172
subdomains, while for position 4, the gate varies more between
different expert combinations. Weight assignment in deeper
MoE layers is thus based more on high-level features and
leads to stronger differentiation. For hard-constrained models,
visible structures are less signiÔ¨Åcant.
A more complex Conv-GAP-FC gate subdivides the as-
signments clearer. The gate thus produces less unambiguous
weight vectors and selects experts deÔ¨Ånitely.
Experts specialization : to analyze the implicit specializa-
tion of experts on distinct subdomains of the input space,
we only activate one speciÔ¨Åc expert during evaluation and
assign all weights to it. We then analyzed classes that received
the largest weights in each expert during evaluation (Table
III). We could observe distinct repeating clusters of classes
for different models, e.g., Ô¨Çowers, marine animals, trees, and
furniture. MoE layers thus learn the natural clustering of the
concepts represented in input images. Repeating clusters are
formed regardless of the gate architecture and constraint, but
inserting MoE layers at the deeper positions in a network
led to larger weights assigned to the experts, indicating better
specialization.
Furthermore, we determined, that a gate chooses the best-
performing expert for the images of the corresponding cluster.
For this, we extracted results for classes in which each expert is
assigned the highest and lowest weights. Full MoE performs in
73 out of 100 classes at least as well as its best experts in this
domain. The gate can thus identify experts for distinct domainsand suitably support experts with inferior performance. It is
consequently able to reasonably combine the output feature
maps to improve the overall predictions.
Expert utilization vs. accuracy : next, we evaluated the
correlation between the average weights assigned to an ex-
pert (sparse and non-sparse, i.e. with all experts activated)
and every single expert‚Äôs test accuracy per class. We also
evaluated the correlation between accuracy and the number
of activations per expert. The results (see Table IV) indicate
a strong relationship for the soft-constrained model. For the
hard-constrained case, the experts are more generalized and do
not show as large performance variations on different classes,
the gate does not rely on the same experts for a certain class.
V. E XPERIMENTS WITH SPARSE MOES FOR OBJECT
DETECTION
A. Experimental Setup
We use a pre-trained2RetinaNet [30] with the ResNet-50
backbone as a baseline, and the COCO dataset [31]. We train
all models using = 2 and= 0:25for the focal loss.
We embed the sparse MoE layers in two manners: (1)
2Block-MoE : by replacing the regression and classiÔ¨Åcation
subnets with two separate MoE blocks, and (2) SingleMoE :
with a single gate shared between regressor and classiÔ¨Åer. We
keep the backbone weights frozen during training. We also
2Available at https://github.com/yhenon/pytorch-retinanet
6

--- PAGE 7 ---
TABLE V: Object detection mAP (%) of MoE models on
COCO test-dev2017. We highlight cases that beat the baseline
accuracy.
Model Constraint mAP mAP@.5
@[.5, .95] @.5
Baseline 35.0 52.5
SingleMoE KL-divergence loss 33.4 50.9
KL-divergence loss 33.5 50.9
2Block-MoE Importance loss 33.5 50.9
Rel. imp. constraint 33.7 51.0
2Block-MoE KL-divergence loss 33.6 50.9
with Conv4 Rel. imp. constraint 33.6 50.9
2Block-MoE with KL-divergence loss 35.1 52.7
pre-trained weights Rel imp. constraint 34.4 51.7
2Block-MoE with Conv4
and pre-trained weights KL-divergence loss 35.1 52.6
trained models with unfrozen weights but did not observe
performance improvements. Mean importance constraints are
not included in the evaluation, since the models suffer from
massive dying expert problems.
The gate is Conv-GAP-FC (see Figure 2b). All experts are
initialized using the Kaiming approach [32] to learn more
diverse features. Weighting factors are reduced to wimp=
wKL= 0:25to guarantee better expert utilization for deeper
MoE layers. Additionally, we set mrel= 0:3to avoid dying
experts. We train models with 4 experts and set k= 2.
B. Evaluation
Performance : the hard-constrained models performed
slightly better than the soft-constrained ones, although none
of the models with MoE layers outperformed the baseline (see
Table V). We further observe only a slight performance drop
for the Single model compared to two separate gates.
Specialization of the 2Block-MoE in regressor : to gain
insights into the weighting decisions of BBox regressor, we
Ô¨Årst analyze the behavior visually. For this, we keep kcls=
2experts active in the classiÔ¨Åer MoE and only analyze the
regressor MoE.
Visual comparison of the bounding boxes (see Figure 5)
for a selected image reveals, that all experts have problems
estimating the precise object boundaries for an atypical pose,
whereas predictions on an object with a clear front view vary
less. The gate is not always able to pick the single best expert.
Still, the inactive experts would both overestimate the bottom
of the bounding box, while the selected experts both predict
the bottom tightly. Predictions made by distinct experts in the
hard-constrained model vary less than for models trained with
soft constraints.
To analyze the behavior of distinct experts in the regressor,
we compute the averagely assigned weights to each regression
expert during the evaluation and break down the weight
assignments into the different feature map levels P3toP7. On
each feature map level, the gating network computes separate
weights and consequently selects experts independently of
other levels. We compute the average weight assignments on
(a) KL-divergence loss (b) KL-divergence loss
Weights: ( 0.44, 0.15, 0.19, 0.22) Weights: ( 0.44, 0.15, 0.19, 0.22)
Sparse weights: (0.67, 0.33) Sparse weights: (0.67, 0.33)
(c) Importance loss (d) Importancee loss
Weights: ( 0.29,0.33, 0.21, 0.16) Weights: (0.18, 0.15, 0.23,0.44)
Sparse weights: (0.47, 0.53) Sparse weights: (0.35, 0.65)
(e) Relative importance constraint (f) Relative importance constraint
Weights: ( 0.46, 0.18, 0.18, 0.18) Weights: ( 0.26, 0.24, 0.26, 0.23)
Sparse weights: (0.71, 0.29) Sparse weights: (0.50, 0.50)
Fig. 5: Comparison of bounding box predictions by distinct
experts and the distribution of weights of 2Block-MoE with
different load balancing constraints for two views on the same
object. Bounding box predictions of two active experts are
green, and predictions of inactive experts are red. Raw image
source: own recording of an author.
7

--- PAGE 8 ---
TABLE VI: Averagely assigned weights to each distinct
2Block-MoE regression expert during evaluation on the vali-
dation set 2017. Results are computed on the sparse vectors.
Levels correspond to the feature map levels of RetinaNet. We
highlight the top weights on each feature map level.
Constraint Level Expert 1 Expert 2 Expert 3 Expert 4
P3 65.54% 0.00% 0.16% 34.30%
KL-divergence P4 56.44% 0.04% 0.52% 42.99%
loss P5 2.32% 2.25% 47.43% 48.00%
P6 0.01% 55.83% 43.94% 0.22%
P7 0.04% 66.49% 33.37% 0.10%
P3 0.03% 0.00% 35.24% 64.73%
Importance P4 0.51% 0.01% 41.19% 58.28%
loss P5 50.10% 1.60% 47.07% 1.23%
P6 47.34% 52.59% 0.07% 0.00%
P7 27.36% 71.28% 1.27% 0.09%
P3 67.75% 2.62% 13.85% 15.79%
Relative P4 33.00% 16.23% 47.13% 3.64%
importance P5 0.11% 50.90% 46.77% 2.21%
constraint P6 9.69% 29.81% 23.81% 36.69%
P7 0.29% 54.30% 44.03% 1.38%
TABLE VII: Averagely assigned weights to each distinct
2Block-MoE classiÔ¨Åcation expert during evaluation on the val-
idation set 2017. Results are computed on the sparse vectors.
Levels correspond to the feature map levels of RetinaNet. We
highlight the top weights on each feature map level.
Constraint Level Expert 1 Expert 2 Expert 3 Expert 4
P3 0.00% 0.05% 59.36% 40.59%
KL-divergence P4 0.00% 4.44% 57.12% 38.44%
loss P5 1.42% 58.42% 8.15% 32.01%
P6 26.55% 58.19% 0.00% 15.26%
P7 90.02% 7.17% 2.34% 0.47%
P3 43.27% 56.70% 0.03% 0.00%
Importance P4 39.70% 58.04% 2.26% 0.01%
loss P5 31.15% 9.40% 58.17% 1.28%
P6 10.42% 0.00% 58.58% 31.00%
P7 0.51% 0.27% 9.03% 90.19%
P3 0.06% 69.68% 11.67% 18.59%
Relative P4 15.54% 21.43% 18.57% 44.46%
importance P5 54.54% 0.04% 23.78% 21.64%
constraint P6 15.74% 0.10% 32.83% 51.33%
P7 52.52% 0.27% 26.60% 20.62%
the sparse weight vectors after selecting the k= 2 active
experts (see Table VI).
We also illustrate the results in Figure 6 by plotting the
predicted bounding boxes for several images3with objects of
varying scales. The results show that the gating network selects
experts depending on the feature map levels.
The hard-constrained gating network using the relative
importance constraint assigns weights in a more diversiÔ¨Åed
way (see Table VI). Still, experts tend to be mainly utilized for
different feature map levels. We conclude that distinct experts
in the hard-constrained case perform better because they are
activated on a larger number of different feature map levels
and are trained to compute bounding boxes on different scales.
Expert 1 is the only expert that is primarily used for detecting
small objects.
3Images taken from Wikimedia Commons, licensed under CC BY-SA 4.0
(b-d) and CC BY 4.0 (a,e,f).
(a) Medium objects ( P5-P6) (b) Small to medium objects ( P3-P5)
(c) Large object ( P7) (d) Small to medium objects ( P4-P6)
(e) Small to medium objects ( P4-P6) (f) Small to medium objects ( P4-P6)
Fig. 6: Bounding box predictions by 2Block-MoE with KL-
divergence loss. Each bounding box caption includes the index
of the two highest-weighted experts, the weight assigned to the
top expert, and the feature map level on which the detection
is made. Output images are partially cropped.
Altogether, we conclude that the gating network selects
experts mainly depending on the input of different feature map
levels. Even though we did not apply the constraints level-
wise, the experts performed similarly accurate when detecting
objects of different scales at different feature map levels.
Improvements could be made by training speciÔ¨Åc experts on
datasets, particularly for small or large objects.
Specialization of the 2Block-MoE in classiÔ¨Åer : we analyze
the behavior of the classiÔ¨Åer experts analogously by keeping
kreg= 2 and assigning all weights in the classiÔ¨Åer MoE to
a speciÔ¨Åc expert (see Table VII). We observed that classiÔ¨Åer
experts vary to a greater extent than regressor experts. For
the model with KL-divergence loss, Expert 1 stands out and
performs signiÔ¨Åcantly worse than the other experts. The expert
is mostly utilized on feature map level P7with about 90%
8

--- PAGE 9 ---
Fig. 7: Comparison of different classiÔ¨Åer expert predictions
and the full MoE model. All detections are made by 2Block-
MoE with KL-divergence loss. For each bounding box, we
state the predicted class and the conÔ¨Ådence score. We further
state two active experts for each MoE prediction in brackets.
of weight assignments. Consequently, the expert specializes
in detecting large objects on this speciÔ¨Åc feature map size.
Differences between the other experts in this model are still
small but slightly larger than for the regressor experts. Experts
in the hard-constrained model perform slightly better on their
own and are closer to the MoE model utilizing kcls= 2
experts.
Weight assignments in the classiÔ¨Åer MoE are similar to
the regressor MoE, differences arise for levels P6andP7.
While the gating network in the regressor MoE focuses on
two experts per level, the classiÔ¨Åer‚Äôs gating network utilizes
three experts for P6and mostly a single expert for P7. The
hard-constrained model also assigns weights more diversiÔ¨Åed
to its experts in the classiÔ¨Åer MoE.
Figure 7 shows predictions of distinct classiÔ¨Åer experts for
an exemplary image. We set kreg= 2 and plot predictions
using distinct classiÔ¨Åcation experts and compare the experts‚Äô
predictions to the full MoE model on the same input image.
Note that all experts can detect relevant objects correctly but
vary in their conÔ¨Ådence. Experts 2 and 4 also falsely predict
additional objects.
Overall, MoE layers embedded in the classiÔ¨Åer and regres-
sor subnets, the decision units in the network, allow us to
gain insights into the decision processes of a model. We canTABLE VIII: Averagely assigned weights to each distinct
regression andclassiÔ¨Åcation expert of the 2Block-MoE with
pre-trained weights using KL-divergence loss during eval-
uation on the validation set 2017. Levels correspond to the
feature map levels of RetinaNet. We highlight the top weights
on each feature map level.
Level Expert 1 Expert 2 Expert 3 Expert 4
P3 37.83% 48.78% 0.64% 12.76%
P4 36.75% 46.07% 1.95% 15.23%
Regressor P5 21.00% 38.26% 15.33% 25.40%
P6 8.46% 24.40% 38.56% 28.58%
P7 3.20% 30.58% 50.67% 15.55%
P3 0.01% 48.13% 50.76% 1.09%
P4 0.75% 41.80% 46.31% 11.14%
ClassiÔ¨Åer P5 29.08% 29.53% 22.19% 19.20%
P6 54.14% 22.64% 8.07% 15.15%
P7 55.03% 18.59% 9.42% 16.95%
separately analyze the predictions of each expert and com-
pare them to the resulting MoE prediction. Our experiments
have demonstrated, that distinct experts specialize in detecting
objects of speciÔ¨Åc sizes.
Impact of pre-training : lastly, we investigated the behavior
of MoE models using pre-trained expert weights. For this, we
re-used baseline weights and added Gaussian noise to enforce
expert specializations. We also trained Conv4 models by only
replacing the 4thconvolutional layers in the regressor and
classiÔ¨Åer subnets with an MoE layer. For comparison, we also
trained Conv4 models without pre-training.
Both models with pre-trained weights outperformed the
baseline slightly and also outperformed models trained from
scratch, whereas soft-constrained models showed larger im-
provements (see Table V). The analysis of weight assignments
(see Table VIII) has shown again, that expert utilization is
mainly dependent on the feature map levels. However, expert
utilization is more scattered, and level boundaries are less
rigid. We assume that for longer training durations, the gating
network would refer stronger to one or two experts on each
level, comparable to other MoE models. For the classiÔ¨Åer
MoE, assigned weightings are also distributed better between
different feature map levels
Overall, using pre-trained weights helps to increase perfor-
mance compared to training from scratch, but obstructs expert
specializations. Consequently, the decision-making process of
a model becomes less transparent and less interpretable. This
again stresses the existence of a trade-off between inter-
pretability and expert specialization and model performance.
VI. C ONCLUSION
In this work, we applied the sparsely-gated MoE layers to
CNNs for computer vision tasks with the goal of increasing
the model interpretability. We presented constraints to mitigate
the dying expert problem, which tackle the issue from different
angles and lead to different MoE behavior. Our analysis
has revealed several interconnections between the proposed
constraints on the one hand, and model performance as well
as interpretability, on the other hand. Hard constraints result in
9

--- PAGE 10 ---
better overall performance and generalized experts, although
the mean importance constraint is particularly prone to the
dying expert problem. Soft constraints, on the other side, lead
to better expert specialization. The usage of constraints thus
helps to control the interplay between model performance,
training stability, and expert specialization.
Our experiments have revealed inherent interpretability for
two evaluated computer vision tasks. For the image classiÔ¨Åca-
tion task, experts focused on distinct repeating class groups,
whereas, for object detection, they specialized in objects of
distinct sizes. We hope that our insights pave the way for
further research on the interpretability of deep neural networks.
ACKNOWLEDGEMENT
The research leading to these results is funded by the Ger-
man Federal Ministry for Economic Affairs and Climate Ac-
tion within the project ‚ÄúKI Absicherung‚Äú (grant 19A19005W)
and by KASTEL Security Research Labs. The authors would
like to thank the consortium for the successful cooperation.
REFERENCES
[1] W. Fedus, B. Zoph, and N. Shazeer, ‚ÄúSwitch transformers: Scaling to
trillion parameter models with simple and efÔ¨Åcient sparsity,‚Äù The Journal
of Machine Learning Research , 2022.
[2] B. Zoph, ‚ÄúDesigning effective sparse expert models,‚Äù in IEEE Inter-
national Parallel and Distributed Processing Symposium - Workshops .
IEEE, 2022.
[3] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,
Y . Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou,
T. Wang, Y . E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-
Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V . Le, Y . Wu, and Z. Chen,
‚ÄúGlam: EfÔ¨Åcient scaling of language models with mixture-of-experts,‚Äù
inInternational Conference on Machine Learning (ICML) . PMLR,
2022.
[4] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer,
‚ÄúBASE layers: Simplifying training of large, sparse models,‚Äù in Inter-
national Conference on Machine Learning (ICML) . PMLR, 2021.
[5] B. Mustafa, C. Riquelme, J. Puigcerver, R. Jenatton, and N. Houlsby,
‚ÄúMultimodal contrastive learning with limoe: the language-image mix-
ture of experts,‚Äù CoRR , vol. abs/2206.02770, 2022.
[6] R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, ‚ÄúAdaptive
mixtures of local experts,‚Äù Neural computation , vol. 3, no. 1, 1991.
[7] D. Eigen, M. Ranzato, and I. Sutskever, ‚ÄúLearning factored represen-
tations in a deep mixture of experts,‚Äù in International Conference on
Learning Representations (ICLR) , 2014.
[8] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. V . Le, G. E.
Hinton, and J. Dean, ‚ÄúOutrageously large neural networks: The sparsely-
gated mixture-of-experts layer,‚Äù in International Conference on Learning
Representations (ICLR) , 2017.
[9] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y . Aminabadi, A. A.
Awan, J. Rasley, and Y . He, ‚ÄúDeepspeed-moe: Advancing mixture-of-
experts inference and training to power next-generation AI scale,‚Äù in
International Conference on Machine Learning (ICML) . PMLR, 2022.
[10] S. Roller, S. Sukhbaatar, A. Szlam, and J. Weston, ‚ÄúHash layers for
large sparse models,‚Äù in Conference on Neural Information Processing
Systems (NIPS/NeurIPS) , 2021.
[11] H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and
Z. Chen, ‚ÄúGshard: Scaling giant models with conditional computation
and automatic sharding,‚Äù in International Conference on Learning
Representations (ICLR) , 2021.
[12] A. Clark, D. de Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoff-
mann, B. Damoc, B. A. Hechtman, T. Cai, S. Borgeaud, G. van den
Driessche, E. Rutherford, T. Hennigan, M. J. Johnson, A. Cassirer,
C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals,
M. Ranzato, J. W. Rae, E. Elsen, K. Kavukcuoglu, and K. Simonyan,
‚ÄúUniÔ¨Åed scaling laws for routed language models,‚Äù in International
Conference on Machine Learning (ICML) . PMLR, 2022.[13] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. S.
Pinto, D. Keysers, and N. Houlsby, ‚ÄúScaling vision with sparse mixture
of experts,‚Äù in Conference on Neural Information Processing Systems
(NIPS/NeurIPS) , 2021.
[14] F. Xue, Z. Shi, F. Wei, Y . Lou, Y . Liu, and Y . You, ‚ÄúGo wider instead
of deeper,‚Äù in AAAI Conference on ArtiÔ¨Åcial Intelligence . AAAI Press,
2022.
[15] K. Ahmed, M. H. Baig, and L. Torresani, ‚ÄúNetwork of experts for
large-scale image categorization,‚Äù in European Conference on Computer
Vision (ECCV) . Springer, 2016.
[16] S. Gross, M. Ranzato, and A. Szlam, ‚ÄúHard mixtures of experts for large
scale weakly supervised vision,‚Äù in IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) . IEEE Computer Society, 2017.
[17] V . Pahuja, J. Fu, and C. J. Pal, ‚ÄúLearning sparse mixture of experts for
visual question answering,‚Äù CoRR , vol. abs/1909.09192, 2019.
[18] S. Pavlitskaya, C. Hubschneider, M. Weber, R. Moritz, F. H ¬®uger,
P. Schlicht, and J. M. Z ¬®ollner, ‚ÄúUsing mixture of expert models to gain
insights into semantic segmentation,‚Äù in IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition (CVPR) Workshops . Computer
Vision Foundation / IEEE, 2020.
[19] S. Pavlitskaya, C. Hubschneider, and M. Weber, ‚ÄúEvaluating mixture-of-
experts architectures for network aggregation,‚Äù in Deep Neural Networks
and Data for Automated Driving: Robustness, Uncertainty QuantiÔ¨Åca-
tion, and Insights Towards Safety . Springer, 2022.
[20] B. Yang, G. Bender, Q. V . Le, and J. Ngiam, ‚ÄúCondconv: Conditionally
parameterized convolutions for efÔ¨Åcient inference,‚Äù in Conference on
Neural Information Processing Systems (NIPS/NeurIPS) , 2019.
[21] X. Wang, F. Yu, L. Dunlap, Y . Ma, R. Wang, A. Mirhoseini, T. Darrell,
and J. E. Gonzalez, ‚ÄúDeep mixture of experts via shallow embedding,‚Äù
inConference on Uncertainty in ArtiÔ¨Åcial Intelligence, UAI . AUAI
Press, 2019.
[22] A. B. Arrieta, N. D. Rodr ¬¥ƒ±guez, J. D. Ser, A. Bennetot, S. Tabik, A. Bar-
bado, S. Garc ¬¥ƒ±a, S. Gil-Lopez, D. Molina, R. Benjamins, R. Chatila,
and F. Herrera, ‚ÄúExplainable artiÔ¨Åcial intelligence (XAI): concepts,
taxonomies, opportunities and challenges toward responsible AI,‚Äù Inf.
Fusion , 2020.
[23] Q. Zhang, Y . N. Wu, and S. Zhu, ‚ÄúInterpretable convolutional neural
networks,‚Äù in IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) . Computer Vision Foundation / IEEE, 2018.
[24] S. Houben, S. Abrecht, M. Akila, A. B ¬®ar, F. Brockherde, P. Feifel, T. Fin-
gscheidt, S. S. Gannamaneni, S. E. Ghobadi, A. Hammam, A. Haselhoff,
F. Hauser, C. Heinzemann, M. Hoffmann, N. Kapoor, F. Kappel,
M. Klingner, J. Kronenberger, F. K ¬®uppers, J. L ¬®ohdeÔ¨Ånk, M. Mlynarski,
M. Mock, F. Mualla, S. Pavlitskaya, M. Poretschkin, A. Pohl, V . R.
Kumar, J. Rosenzweig, M. Rottmann, S. R ¬®uping, T. S ¬®amann, J. D.
Schneider, E. Schulz, G. Schwalbe, J. Sicking, T. Srivastava, S. Varghese,
M. Weber, S. Wirkert, T. Wirtz, and M. Woehrle, ‚ÄúInspect, understand,
overcome: A survey of practical methods for AI safety,‚Äù in Deep Neural
Networks and Data for Automated Driving: Robustness, Uncertainty
QuantiÔ¨Åcation, and Insights Towards Safety . Springer, 2022.
[25] L. Wu, M. Liu, Y . Chen, D. Chen, X. Dai, and L. Yuan, ‚ÄúResidual
mixture of experts,‚Äù CoRR , 2022.
[26] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image
recognition,‚Äù in IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR) . IEEE, 2016.
[27] L. Lu, Y . Shin, Y . Su, and G. E. Karniadakis, ‚ÄúDying relu and initial-
ization: Theory and numerical examples,‚Äù CoRR , vol. abs/1903.06733,
2019.
[28] A. Krizhevsky, G. Hinton et al. , ‚ÄúLearning multiple layers of features
from tiny images,‚Äù 2009.
[29] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic optimization,‚Äù
inInternational Conference on Learning Representations (ICLR) , 2015.
[30] T. Lin, P. Goyal, R. B. Girshick, K. He, and P. Doll ¬¥ar, ‚ÄúFocal loss for
dense object detection,‚Äù in IEEE International Conference on Computer
Vision (ICCV) . IEEE Computer Society, 2017.
[31] T. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll ¬¥ar, and C. L. Zitnick, ‚ÄúMicrosoft COCO: common objects
in context,‚Äù in European Conference on Computer Vision (ECCV) .
Springer, 2014.
[32] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDelving deep into rectiÔ¨Åers:
Surpassing human-level performance on imagenet classiÔ¨Åcation,‚Äù in
IEEE International Conference on Computer Vision (ICCV) . IEEE,
2015.
10
