# 2310.09832.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2310.09832.pdf
# Kích thước tệp: 498874 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Hợp nhất các Chuyên gia thành Một:
Cải thiện Hiệu quả Tính toán của Hỗn hợp Chuyên gia
Shwai He1Run-Ze Fan3Liang Ding2∗Li Shen4Tianyi Zhou1∗Dacheng Tao2
1University of Maryland, College Park2The University of Sydney
3University of Chinese Academy of Sciences4JD Explore Academy
shwaihe@umd.edu ,liangding.liam@gmail.com ,tianyi@umd.edu
Tóm tắt
Mở rộng quy mô của các mô hình ngôn ngữ thường dẫn đến những tiến bộ đáng kể trong các nhiệm vụ NLP. Nhưng nó thường đi kèm với chi phí tính toán ngày càng tăng. Mặc dù Hỗn hợp Chuyên gia thưa thớt (MoE) có thể giảm chi phí bằng cách kích hoạt một tập con nhỏ các tham số (ví dụ: một chuyên gia) cho mỗi đầu vào, việc tính toán của nó tăng đáng kể nếu tăng số lượng chuyên gia được kích hoạt, hạn chế tính hữu dụng thực tế của nó. Liệu chúng ta có thể giữ lại những lợi thế của việc thêm nhiều chuyên gia mà không tăng đáng kể chi phí tính toán? Trong bài báo này, chúng tôi đầu tiên chứng minh tính ưu việt của việc lựa chọn nhiều chuyên gia và sau đó đề xuất một phương pháp hiệu quả tính toán gọi là Hợp nhất Chuyên gia thành Một (MEO), giúp giảm chi phí tính toán xuống mức của một chuyên gia duy nhất. Các thí nghiệm mở rộng cho thấy MEO cải thiện đáng kể hiệu quả tính toán, ví dụ: FLOPS giảm từ 72.0G của MoE vani xuống 28.9G (MEO). Hơn nữa, chúng tôi đề xuất một khối chú ý cấp độ token giúp tăng cường hiệu quả và hiệu suất của MEO cấp độ token, ví dụ: 83.3% (MEO) so với 82.6% (MoE vani) điểm trung bình trên bộ đánh giá GLUE. Mã nguồn của chúng tôi sẽ được phát hành khi được chấp nhận. Mã nguồn sẽ được phát hành tại:https://github.com/Shwai-He/MEO .

1 Giới thiệu
Mở rộng quy mô mô hình ngôn ngữ đã đạt được tiến bộ đáng hứa hẹn trong lĩnh vực NLP (Brown et al., 2020; OpenAI, 2023). Để tăng thêm kích thước mô hình trong một ngân sách tính toán, các mạng kích hoạt thưa thớt (Du et al., 2022; Artetxe et al., 2022) chỉ sử dụng một vài tham số cho mỗi đầu vào. Một phương pháp được nghiên cứu rộng rãi là Hỗn hợp Chuyên gia (MoE, Shazeer et al., 2017), huấn luyện nhiều mạng chuyên gia nhưng chỉ chọn một tập con của chúng cho một đầu vào cụ thể (Jacobs et al., 1991; Jordan và Jacobs, 1994). So với các mạng dày đặc có cùng kích thước mô hình, MoE hiệu quả giảm chi phí tính toán.

∗Tác giả liên hệ

Hình 1: Hiệu suất so với FLOPs của MoE và MEO ở cấp độ token khi số lượng chuyên gia khác nhau (tức là 1, 2, 4, 8, 16, 32) được chọn. Chúng tôi lấy ba kích thước khác nhau của BERT làm mô hình chuyên gia.

Mặc dù tăng số lượng chuyên gia được chọn cho mỗi đầu vào có thể cải thiện tính đa dạng biểu diễn (Yang et al., 2019) và hiệu suất nhiệm vụ hạ nguồn (Shazeer et al., 2017; Yang et al., 2019), nó thường đi kèm với chi phí tính toán tăng đáng kể. Nghiên cứu thực nghiệm của chúng tôi (Hình 1 và Bảng 1) xác minh những Ưu và Nhược điểm (hiệu suất vượt trội so với chi phí tính toán cao) của việc lựa chọn nhiều chuyên gia trong suy luận MoE. Do đó, để giữ lại lợi thế của MoE về hiệu quả tính toán, các công việc hiện tại chủ yếu chỉ chọn một chuyên gia duy nhất cho mỗi đầu vào trong các ứng dụng hoặc thí nghiệm (Fedus et al., 2021), điều này không thể tránh khỏi việc làm giảm hiệu suất.

Công việc của chúng tôi nhằm cải thiện hiệu quả tính toán của suy luận MoE với nhiều chuyên gia được chọn, để tái tạo lại đáng kể hiệu suất bị giảm. Tính toán liên quan đến MoE chủ yếu bao gồm suy luận trên mỗi chuyên gia được chọn và tổng hợp các đầu ra của chúng, với phần trước chiếm ưu thế về chi phí. Do đó, chi phí tăng tuyến tính theo số lượng chuyên gia được chọn. Để vượt qua nút thắt cổ chai tính toán, thay vào đó chúng tôi đề xuất Hợp nhất Chuyên gia thành Một (MEO),

--- TRANG 2 ---
!!!"Đầu vào "Mạng Gating##,%##",%"#!,%!#$,%$Đầu ra &××+(a) MoE
×##,%##",%"#!,%!Mạng GatingĐầu vào "'#,(%+Đầu ra &#$,%$×
 (b) MEO
Hình 2: Sơ đồ của (a) MoE và (b) MEO được đề xuất, với trường hợp m= 2 chuyên gia được chọn. MoE kết hợp tuyến tính các đầu ra từ chuyên gia, trong khi MEO trước tiên hợp nhất các chuyên gia thành một và sau đó tính toán đầu vào.

thay đổi thứ tự tính toán của hai phép toán, tức là đầu tiên hợp nhất các tham số của các chuyên gia được chọn thành một chuyên gia rồi sau đó suy luận trên chuyên gia đã được hợp nhất. Vì việc hợp nhất tham số chỉ yêu cầu phép tổng, một lớp MEO (gần như) chỉ tiêu thụ tính toán của suy luận một chuyên gia duy nhất, bất kể có bao nhiêu chuyên gia được chọn. Điều này dẫn đến chi phí suy luận gần như không đổi khi mở rộng quy mô khả năng mô hình (tức là số lượng chuyên gia được chọn) để cải thiện hiệu suất.

MEO có thể được áp dụng như một sự thay thế trực tiếp cho MoE, đã được triển khai ở các cấp độ khác nhau, ví dụ: lựa chọn chuyên gia cho mỗi token (Shazeer et al., 2017), mỗi chuỗi (Ye et al., 2022), mỗi nhiệm vụ (Kudugunta et al., 2021), v.v. Ở cấp độ chuỗi/nhiệm vụ, các nghiên cứu thực nghiệm của chúng tôi chứng minh rằng thay thế MoE bằng MEO cải thiện đáng kể hiệu quả tính toán, ví dụ: giảm FLOPS từ 72.0G xuống 28.9G, mà không làm tổn hại đến hiệu suất. Ngoài ra, chúng tôi đề xuất một cơ chế chú ý cấp độ token giúp tăng cường hiệu quả và hiệu suất hơn nữa, ví dụ: từ 82.6% (MoE) lên 83.3% (MEO) trên BERT-Base (Hình 1).

2 Phương pháp
Tóm tắt về Hỗn hợp Chuyên gia. Cho một token xi trong chuỗi đầu vào x∈Rs×d, MoE chọn m chuyên gia từ n (m≤n) chuyên gia (E1, . . . , En) dựa trên một mạng gating. Chúng tôi ký hiệu G là điểm số gating và T là chỉ số của các chuyên gia được chọn. MoE kết hợp tuyến tính các đầu ra của các chuyên gia được chọn:

yi=X
k∈TGk(xi)·Ek(xi). (1)

MoE thực hiện ở các cấp độ khác nhau, ví dụ: token, chuỗi và nhiệm vụ, trong đó MoE chọn chuyên gia dựa trên một token đơn lẻ, chuỗi đầu vào hoặc embedding nhiệm vụ (hoặc id nhiệm vụ):

G(xi) =

GATE (xi), Cấp độ token
GATE (1
sPs
i=1xi), Cấp độ chuỗi
GATE (task _ids), Cấp độ nhiệm vụ,(2)

trong đó "GATE" biểu thị hàm gating.

Bảng 1: Ảnh hưởng của số lượng chuyên gia được chọn đến hiệu suất. Kết quả tốt nhất được in đậm.

m#FLOPs. SST-2 STSB MNLI QNLI Trung bình
1 7.5G 87.1 86.1 77.8 85.8 84.2
2 9.6G 87.9 86.8 78.2 86.2 84.8
4 13.9G 88.2 87.1 78.3 86.4 85.0
8 22.5G 88.3 87.7 79.1 86.8 85.5
16 39.7G 88.4 87.5 78.8 86.6 85.3
32 74.1G 88.2 87.6 78.6 86.3 85.2

Động lực. Trong khi nhiều mô hình MoE chủ đạo có xu hướng chọn chuyên gia top-1 (Fedus et al., 2021), việc lựa chọn nhiều chuyên gia có tiềm năng tăng cường sức mạnh biểu diễn (Chen et al., 2020; Yang et al., 2019). Về mặt thực nghiệm, chúng tôi tiến hành các thí nghiệm sơ bộ trên BERT-Small (Bhargava et al., 2021) để xác minh điều này.

Trong Bảng 1, rõ ràng việc lựa chọn nhiều chuyên gia đóng góp vào hiệu suất tốt hơn. Mặc dù việc lựa chọn quá nhiều chuyên gia không tối ưu vì nó gây ra sự can thiệp giữa các chuyên gia cản trở hiệu suất (Mustafa et al., 2022; Zhu et al., 2022), các thí nghiệm sơ bộ của chúng tôi chỉ ra sự cần thiết của việc lựa chọn nhiều chuyên gia.

Tuy nhiên, việc lựa chọn nhiều chuyên gia dẫn đến sự gia tăng đáng kể FLOPs (ví dụ: 74.1G so với 7.5G khi tăng m từ 1 lên 32). Hiện tượng này thúc đẩy chúng tôi suy ngẫm xem liệu có tồn tại một phương pháp hiệu quả để đạt được cả hiệu suất cao và hiệu quả tính toán. Mục tiêu của chúng tôi là đảm bảo chi phí tính toán nhất quán, bất kể số lượng chuyên gia được chọn.

--- TRANG 3 ---
Hợp nhất Chuyên gia thành Một. Chi phí tính toán của MoE chủ yếu bao gồm tính toán của từng chuyên gia (tức là P
k∈TO(Ek)) và hỗn hợp các đầu ra từ chuyên gia (tức là O(G) và O(P
k∈TGk·Ek)). Đáng chú ý, tính toán của từng chuyên gia đóng vai trò chủ đạo, với chi phí của một chuyên gia duy nhất vượt trội đáng kể so với chi phí hỗn hợp:

O(Ek)≫O(G) +O(X
k∈TGk·Ek), (3)

trong đó O(·) đo lường chi phí tính toán.

Mặt khác, khi số lượng chuyên gia được chọn m tăng, thành phần P
k∈TO(Ek) tăng đáng kể, trong khi sự gia tăng của O(P
k∈TGk·Ek) là rất nhỏ. Do đó, điều cần thiết là giải quyết xu hướng tăng của P
k∈TO(Ek) để tăng cường hiệu quả tính toán.

Như được minh họa trong Hình 2, chúng tôi đề xuất phương pháp gọi là Hợp nhất Chuyên gia thành Một (MEO), trong đó ý tưởng chính là tận dụng điểm số gating để tổng hợp các tham số của các chuyên gia được chọn (điều này tương tự như cơ chế hợp nhất mô hình có trọng số đơn giản (Li et al., 2023)):

ˆWi=X
k∈TGk(xi)·Wk,ˆbi=X
k∈TGk(xi)·bk,(4)

trong đó Wk, bk biểu thị trọng số và bias của chuyên gia thứ k, trong khi ˆWi,ˆbi là trọng số và bias tổng hợp cho xi. Đầu ra của MEO được cho bởi:

yi=σ(ˆWixi+ˆbi), (5)

trong đó σ biểu thị hàm kích hoạt.

Chi phí tính toán của MEO chủ yếu bao gồm O(σ(ˆWixi+ˆbi)), O(P
k∈TGk·Wk), O(P
k∈TGk·bk), và O(G). Trong số đó, O(σ(ˆWixi+ˆbi)) là yếu tố chủ đạo. Điều đáng chú ý là O(σ(ˆWixi+ˆbi)) tương đương với chi phí tính toán của một mạng kết nối hoàn toàn và không phụ thuộc vào số lượng chuyên gia được chọn. Do đó, MEO nén chi phí tính toán một cách đáng kể.

MEO ở Các Cấp độ Khác nhau. Trong trường hợp MEO cấp độ chuỗi và nhiệm vụ, tất cả các token trong một chuỗi chia sẻ cùng điểm số gating, cũng như các tham số tổng hợp ˆW và ˆb1. Tính chất này cho phép dễ dàng áp dụng MEO ở các cấp độ này.

1chúng tôi bỏ qua chỉ số dưới của ˆW và ˆb ở cấp độ chuỗi và nhiệm vụ vì mỗi token chia sẻ cùng tham số tổng hợp.

Tuy nhiên, khi áp dụng trực tiếp MEO ở cấp độ token, tình huống khác biệt. Vì điểm số gating của mỗi token trong một chuỗi là duy nhất, việc sử dụng MEO một cách đơn giản sẽ yêu cầu tổng hợp nhiều bộ trọng số và bias, dẫn đến chi phí triển khai tăng. Do đó, chúng tôi tinh chỉnh và tăng cường khung của MEO cấp độ token một cách cụ thể.

MEO Cấp độ Token. MEO cấp độ token được đề xuất của chúng tôi nhằm kết hợp thông tin cấp độ token với chi phí tính toán bổ sung tối thiểu. Cụ thể, việc lựa chọn chuyên gia được thực hiện ở cấp độ chuỗi, do đó bảo tồn thông tin ngữ cảnh và loại bỏ sự cần thiết của việc tổng hợp nhiều trọng số và bias cho từng token riêng lẻ. Để nắm bắt sự nhận dạng của mỗi token, chúng tôi tận dụng cơ chế chú ý token được lấy cảm hứng từ Houlsby et al. (2019); Li et al. (2021).

Cụ thể, cho chuỗi đầu vào x∈Rs×d, chúng tôi sử dụng một khối thắt cổ chai chuyên biệt, được lấy cảm hứng từ các cấu trúc giống adapter (Houlsby et al., 2019; Pfeiffer et al., 2021). Lớp thắt cổ chai kết hợp trọng số down-projection Wdown∈Rd×d
r, một hàm kích hoạt f và trọng số up-projection Wup∈Rd
r×d, với hệ số giảm r= 64 đảm bảo chi phí tính toán bổ sung thấp. Bằng cách hoạt động trên từng token riêng lẻ, thắt cổ chai áp dụng chú ý cấp độ token cho chuỗi đầu vào x:

x←x+f(xWdown)Wup. (6)

Với việc bao gồm nhận dạng token trong đầu vào cập nhật, MEO thực hiện tổng hợp ˆW và ˆb thông qua lựa chọn chuyên gia cấp độ chuỗi. Sau đó, các ˆW và ˆb tổng hợp này được sử dụng để tính toán đầu ra kết hợp với đầu vào.

3 Đánh giá Thực nghiệm

Thiết lập Thí nghiệm. Các thí nghiệm được tiến hành trên Bốn bộ đánh giá được sử dụng rộng rãi, bao gồm các nhiệm vụ hiểu và sinh: (1) GLUE (Wang et al., 2019), chứa các nhiệm vụ hiểu như suy luận ngôn ngữ tự nhiên, phân tích tình cảm và đánh giá độ tương tự câu; (2) XSum (Narayan et al., 2018), một tập dữ liệu tóm tắt trong đó các mô hình được yêu cầu tạo ra một tóm tắt ngắn cho một bài báo cho trước; (3) WikiText-2 (Merity et al., 2016), một tập hợp hơn 100 triệu token được trích xuất từ tập hợp các bài báo Tốt và Nổi bật đã được xác minh trên Wikipedia trong đó các mô hình được sử dụng để sinh ra các token tiếp theo; (4)

--- TRANG 4 ---
Bảng 2: Kết quả thực nghiệm cho MEO và MoE ở cấp độ nhiệm vụ (task) và cấp độ chuỗi (seq). Chúng tôi cũng báo cáo hiệu suất của các lớp feedforward vani ("Vanilla") để tham khảo. Kết quả được hiển thị là điểm trung bình của 5 lần chạy. Kết quả tốt nhất được in đậm.✶ biểu thị phương pháp có FLOPs ít hơn ("Vanilla" không được bao gồm).

Phương pháp #FLOPs. CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE Trung bình
Vanilla 28.5G 54.6 91.1 84.6 85.8 90.2 80.6 90.4 66.4 80.5
MoE task 72.0G 58.5 91.3 85.8 89.2 90.5 82.7 90.5 69.3 82.2
MEO task ✶28.9G 59.1 91.2 85.5 89.3 90.4 83.0 90.9 68.9 82.3
MoE seq 72.0G 59.8 91.5 86.5 89.5 90.6 83.4 90.7 70.4 82.8
MEO seq ✶28.9G 60.1 91.9 86.3 89.4 90.7 83.7 91.2 70.3 83.0

SQuAD v1.1 (Rajpurkar et al., 2016), một tập dữ liệu cặp đôi cho câu hỏi và đoạn văn Wikipedia trong đó các mô hình chọn đoạn trả lời cho câu hỏi từ đoạn văn.

Chúng tôi tuân theo Zhong et al. (2022a,b); He et al. (2023a) để tiến hành thí nghiệm trên bộ đánh giá GLUE được sử dụng rộng rãi, chứa các nhiệm vụ hiểu như suy luận ngôn ngữ tự nhiên, phân tích tình cảm, đánh giá độ tương tự câu, v.v. Chúng tôi sử dụng Adam (Kingma và Ba, 2015) làm bộ tối ưu hóa với β1,β2= 0.9, 0.98. Để điều chỉnh, chúng tôi đặt weight decay là 0.1 và tìm kiếm lưới learning rate từ {1e-5, 5e-5, 1e-4, 5e-4}, trong đó chúng tôi khởi động learning rate trong 10% bước đầu tiên (của tổng số bước huấn luyện). Đối với các quy mô dữ liệu khác nhau, chúng tôi tìm kiếm lưới epoch huấn luyện và batch size từ {5, 10, 15, 20}, và {8, 16, 32, 64}, tương ứng. Độ dài tối đa là 128 cho GLUE, 1024 cho WikiText và 384 cho SQuAD. Đối với XSum, chúng tôi đặt độ dài tối đa của các bài báo nguồn là 512 và độ dài tối đa của tóm tắt đích là 128. Chúng tôi tuân theo các công trình trước đó (Phang et al., 2018; Lee et al., 2020; Dodge et al., 2020; Wang et al., 2022; He et al., 2023b) để tinh chỉnh các mô hình ngôn ngữ được đào tạo trước, ví dụ: BERT (Devlin et al., 2019), trên tập huấn luyện hạ nguồn và báo cáo kết quả bằng checkpoint cuối cùng.

Kết quả Chính. Theo Shazeer et al. (2017); Gao et al. (2022), chúng tôi tiến hành thí nghiệm trên BERT-Base (Devlin et al., 2019) và thay thế các lớp feedforward ("Vanilla") bằng MoE hoặc MEO, với thiết lập m= 4 và n= 16. Trong Bảng 2, chúng tôi so sánh cẩn thận MEO được đề xuất với MoE ở cấp độ nhiệm vụ và chuỗi, về mặt hiệu quả tính toán và hiệu suất. So với MoE, MEO giảm đáng kể chi phí tính toán trong khi đạt được hiệu suất tương đương. Cụ thể, so với các lớp feedforward vani, Phép toán Điểm Nổi (FLOPs) của MEO chỉ tăng rất ít (tức là khoảng 1%), trong khi MoE nhân FLOPs khoảng 2.53 lần.

/uni00000014 /uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000036/uni00000048/uni0000004f/uni00000048/uni00000046/uni00000057/uni00000048/uni00000047/uni00000003/uni00000028/uni0000005b/uni00000053/uni00000048/uni00000055/uni00000057/uni00000056/uni00000014/uni00000018/uni00000015/uni00000013/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000017/uni00000018/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000037/uni0000004c/uni00000050/uni00000048/uni00000003/uni0000000b/uni00000056/uni0000000c/uni00000030/uni00000052/uni00000028 /uni00000030/uni00000028/uni00000032

Hình 3: So sánh thời gian suy luận giữa MoE và MEO dưới một loạt số lượng chuyên gia được chọn khác nhau (tức là 1, 2, 4, 8, 16).

Phân tích Giảm Tính toán. So với một lớp kết nối hoàn toàn, MEO chỉ giới thiệu tính toán trong mạng gating O(G(x)) và hợp nhất chuyên gia (tức là O(P
k∈TGk·Wk) và O(P
k∈TGk·bk)). Tính toán bổ sung là tối thiểu so với của từng chuyên gia.

Trong thực tế, chúng tôi sử dụng tám GPU NVIDIA V100 Tensor Core để đo thời gian suy luận của MEO và MoE trên BERT-Base khi chọn số lượng chuyên gia khác nhau (tức là n= 1,2,4,8,16). Thời gian suy luận được tính bằng tổng thời gian chạy trên tập xác thực MNLI với batch size 16. Theo Hình 3, khi số lượng chuyên gia được chọn tăng, thời gian suy luận của MEO tương đối nhất quán, trong khi MoE thể hiện thời gian suy luận tăng đáng kể. Điều này làm nổi bật lợi thế của MEO trong hiệu quả tính toán, điều này trở nên rõ ràng hơn khi số lượng chuyên gia được chọn tăng.

Bảng 3: So sánh giữa MEO và MoE với việc sử dụng hàm kích hoạt khác nhau (tức là hàm kích hoạt bên trong (in) và bên ngoài (out) chuyên gia).

Phương pháp FLOPs SST-2 QQP MNLI QNLI Trung bình
Vanilla 7.5G 86.9 89.1 77.2 85.2 84.6
MoE in 22.6G 87.9 89.4 77.8 85.7 85.2
MoE out 22.5G 87.6 89.2 78.0 85.6 85.1
MEO ✶7.7G 88.1 89.7 78.2 86.2 85.6

--- TRANG 5 ---
/uni00000030/uni00000035/uni00000033/uni00000026 /uni00000036/uni00000037/uni00000036/uni00000025 /uni00000030/uni00000031/uni0000002f/uni0000002c /uni00000034/uni00000031/uni0000002f/uni0000002c/uni0000001a/uni00000019/uni00000011/uni00000013/uni0000001a/uni0000001c/uni00000011/uni00000013/uni0000001b/uni00000015/uni00000011/uni00000013/uni0000001b/uni00000018/uni00000011/uni00000013/uni0000001b/uni0000001b/uni00000011/uni00000013/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000030/uni00000052/uni00000028 /uni00000030/uni00000028/uni00000032

Hình 4: So sánh hiệu suất giữa MoE và MEO cấp độ token, trong đó chúng tôi lấy BERT-Small làm backbone với thiết lập m= 8 và n= 32.

Phân tích Hàm Kích hoạt. Trong nhiều trường hợp mà một chuyên gia Ei biểu thị một lớp tuyến tính không có hàm kích hoạt phi tuyến, đầu ra của MEO (yi=σ(ˆWixi+ˆbi)) tương đương với của MoE (yi=σ(P
k∈TGk·(Wkxi+bk))). Tuy nhiên, nếu chuyên gia Ei liên quan đến một hàm kích hoạt, đầu ra của MoE là yi=P
k∈T(Gk·σ(Wkxi+bk)), dẫn đến sự khác biệt trong đầu ra và có thể trong hiệu suất. Như được mô tả trong Hình 3, chúng tôi so sánh MEO với MoE với việc sử dụng kích hoạt khác nhau, trong đó chúng tôi xem xét hai kịch bản: hàm kích hoạt bên trong hoặc bên ngoài chuyên gia. Kết quả chứng minh rằng khoảng cách hiệu suất giữa hai kịch bản là tối thiểu và cho thấy tính hiệu quả của MEO trong việc xử lý các mạng chuyên gia kết hợp hàm kích hoạt.

Bảng 4: Hiệu suất của MEO Cấp độ Token, trong đó chúng tôi lấy BERT-Large làm backbone với thiết lập m= 2 và n= 8.

Phương pháp FLOPs SST-2 MRPC STSB QNLI Trung bình
Vanilla 87.2G 93.2 86.8 89.1 91.8 90.2
MoE 139.0G 93.7 87.2 89.7 92.2 90.7
MEO ✶91.2G 94.1 87.5 89.8 92.4 91.0

Hiệu quả của MEO Cấp độ Token. Đối với MEO ở cấp độ token, chúng tôi đã kết hợp các khối chú ý cấp độ token. Để đánh giá chi phí triển khai của các khối mới thêm, chúng tôi đầu tiên tính toán tham số và FLOPs bổ sung, với BERT-Small làm backbone. Chi phí bổ sung của các khối đã thêm là tối thiểu (tức là 0.6M tham số và 0.15 GFLOPs).

Hơn nữa, trong Hình 4, chúng tôi trình bày so sánh hiệu suất giữa MEO và MoE cấp độ token trong bốn nhiệm vụ hiểu ngôn ngữ tự nhiên, trong đó MEO vượt trội hơn MoE một cách nhất quán qua các nhiệm vụ này, ví dụ: 78.9% so với 78.1% trên MNLI. Đối với điểm trung bình trên bộ đánh giá GLUE, MEO tăng hiệu suất một cách đáng kể, tức là 83.3% so với 82.6% trên BERT-Base và 77.8% so với 77.3% trên BERT-Small.

Chúng tôi cũng triển khai MEO cấp độ token trên BERT-Large, sử dụng 8 chuyên gia và chọn 2 chuyên gia, tạo ra một mô hình với khoảng 1.75 tỷ tham số. Như được chứng minh trong Bảng 4, MEO liên tục tăng cường hiệu suất qua các nhiệm vụ khác nhau, ví dụ: cải thiện 0.4% trong SST-2 khi so sánh với MoE. Đáng chú ý, chi phí tính toán bổ sung là tối thiểu, với chỉ 4.0 GFLOPs tăng so với mô hình Vanilla. Do đó, MEO cấp độ token chứng minh là một lựa chọn thay thế hiệu quả và có hiệu suất cho MoE cấp độ token.

Chuyển đổi sang các kiến trúc và nhiệm vụ khác nhau. Sử dụng MEO trong các kiến trúc BERT tăng cường hiệu quả tính toán và hiệu suất, và chúng tôi tiếp tục xác thực tính hiệu quả của MEO trên một loạt các kiến trúc cho các nhiệm vụ khác nhau. Trong Bảng 5, chúng tôi sử dụng BART-Large (Lewis et al., 2020) cho XSum (Narayan et al., 2018), GPT-2-Small (Radford et al., 2019) cho WikiText (Merity et al., 2016), và T5-Base (Raffel et al., 2020) cho SQuAD (Rajpurkar et al., 2016). MEO và MoE được triển khai ở cấp độ token. Xem xét tài nguyên tính toán hạn chế, chúng tôi đặt m= 2 và n= 8 cho BART và GPT-2, trong khi m= 4 và n= 16 được đặt cho T5.

Rõ ràng, MEO vượt trội hơn MoE tiêu chuẩn trong ba nhiệm vụ, cho thấy tính phổ quát của nó trong cả hiểu và sinh ngôn ngữ tự nhiên.

Bảng 5: Hiệu quả trên các kiến trúc và nhiệm vụ khác nhau. XSum, WikiText và SQuAD được đánh giá với ROUGE-2 (R2.), Perplexity (PPL) và Exact Match (EM), tương ứng.

Phương pháp XSum WikiText SQuAD
FLOPs R2. FLOPs PPL FLOPs EM
Vanilla 369.4G 21.9 295.4G 21.9 90.2G 81.6
MoE 576.6G 22.2 412.2G 21.1 221.3G 82.0
MEO ✶383.6G 22.4✶303.2G 20.9✶93.5G 82.1

4 Kết luận

Trong công trình này, chúng tôi nghiên cứu có hệ thống chi phí tính toán của Hỗn hợp Chuyên gia. Dựa trên các phát hiện của chúng tôi, chúng tôi đề xuất một sự thay thế trực tiếp gọi là Hợp nhất Chuyên gia thành Một (MEO) để tăng cường hiệu quả tính toán. Ngoài ra, chúng tôi đề xuất một cơ chế chú ý Cấp độ Token giúp tăng cường hiệu suất hơn nữa. Nghiên cứu của chúng tôi chỉ ra thực nghiệm tiềm năng để biến MEO thành một kiến trúc hiệu quả tiêu chuẩn vàng trong cộng đồng NLP.

--- TRANG 6 ---
5 Hạn chế

Bất chấp tiến bộ mà chúng tôi đã đạt được, vẫn còn những hạn chế trong công việc của chúng tôi. Trong khi kiến trúc của chúng tôi cho hỗn hợp chuyên gia chứng minh hiệu quả cải thiện, vẫn cần khám phá thêm về mặt triển khai của nó. Cụ thể, việc xác định số lượng chuyên gia tối ưu trong các lớp cụ thể và lựa chọn các cấp độ MoE khác nhau đòi hỏi nghiên cứu thêm. Chúng tôi tin rằng với việc triển khai các chiến lược triển khai hiệu quả, phương pháp của chúng tôi có tiềm năng trở nên cạnh tranh hơn nữa.

Lời cảm ơn

Chúng tôi biết ơn những người đánh giá ẩn danh EMNLP và chủ tịch khu vực vì những bình luận và đề xuất sâu sắc của họ.

Tuyên bố Đạo đức

Chúng tôi xem xét nghiêm túc các cân nhắc đạo đức và tuân thủ nghiêm ngặt Chính sách Đạo đức EMNLP. Bài báo này tập trung vào hiệu quả cao hơn của các mạng động, ví dụ: hỗn hợp chuyên gia. Cả tập dữ liệu và mô hình được sử dụng trong bài báo này đều có sẵn công khai và đã được các nhà nghiên cứu áp dụng rộng rãi. Chúng tôi đảm bảo rằng các phát hiện và kết luận của bài báo này được báo cáo chính xác và khách quan.

Tài liệu tham khảo

Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, Giridharan Anantharaman, Xian Li, Shuohui Chen, Halil Akin, Mandeep Baines, Louis Martin, Xing Zhou, Punit Singh Koura, Brian O'Horo, Jeffrey Wang, Luke Zettlemoyer, Mona T. Diab, Zornitsa Kozareva, và Veselin Stoyanov. 2022. Mô hình ngôn ngữ quy mô lớn hiệu quả với hỗn hợp chuyên gia. Trong EMNLP.

Prajjwal Bhargava, Aleksandr Drozd, và Anna Rogers. 2021. Tổng quát hóa trong NLI: Những cách (không) để vượt qua heuristic đơn giản. Trong Workshop thứ hai về Insights from Negative Results in NLP.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Các mô hình ngôn ngữ là những người học few-shot. Trong NeurIPS.

Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, và Zicheng Liu. 2020. Convolution động: Chú ý trên các kernel convolution. Trong CVPR.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. 2019. Bert: Pre-training của transformers hai chiều sâu để hiểu ngôn ngữ. Trong NAACL.

Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, và Noah Smith. 2020. Tinh chỉnh các mô hình ngôn ngữ được đào tạo trước: Khởi tạo trọng số, thứ tự dữ liệu và dừng sớm. arXiv preprint.

Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P. Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen S. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Zhifeng Chen, và Claire Cui. 2022. Glam: Mở rộng quy mô hiệu quả của các mô hình ngôn ngữ với hỗn hợp chuyên gia. Trong ICML.

William Fedus, Barret Zoph, và Noam Shazeer. 2021. Switch transformers: Mở rộng quy mô đến các mô hình có nghìn tỷ tham số với tính thưa thớt đơn giản và hiệu quả. J. Mach. Learn. Res.

Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, và Ji-Rong Wen. 2022. Kiến trúc hỗn hợp chuyên gia parameter-efficient cho các mô hình ngôn ngữ được đào tạo trước. Trong COLING.

Shwai He, Liang Ding, Daize Dong, Boan Liu, Fuqiang Yu, và Dacheng Tao. 2023a. PAD-net: Một framework hiệu quả cho các mạng động. Trong ACL.

Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, và Dacheng Tao. 2023b. Mera: Hợp nhất các adapter được đào tạo trước cho học few-shot. arXiv preprint.

Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, và Sylvain Gelly. 2019. Học chuyển giao parameter-efficient cho nlp. Trong ICML.

Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, và Geoffrey E. Hinton. 1991. Hỗn hợp thích ứng của các chuyên gia cục bộ. Neural Comput.

Michael I. Jordan và Robert A. Jacobs. 1994. Hỗn hợp phân cấp của chuyên gia và thuật toán EM. Neural Comput.

Diederik P Kingma và Jimmy Ba. 2015. Adam: Một phương pháp cho tối ưu hóa ngẫu nhiên. Trong ICLR.

--- TRANG 7 ---
Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin, Minh-Thang Luong, và Orhan Firat. 2021. Vượt qua chưng cất: Hỗn hợp chuyên gia cấp độ nhiệm vụ cho suy luận hiệu quả. Trong EMNLP.

Cheolhyoung Lee, Kyunghyun Cho, và Wanmo Kang. 2020. Mixout: Điều chỉnh hiệu quả để tinh chỉnh các mô hình ngôn ngữ được đào tạo trước quy mô lớn. Trong ICLR.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, và Luke Zettlemoyer. 2020. BART: Đào tạo trước chuỗi-đến-chuỗi khử nhiễu cho sinh ngôn ngữ tự nhiên, dịch và hiểu. Trong ACL.

Chao Li, Aojun Zhou, và Anbang Yao. 2021. Convolution động toàn diện. Trong ICLR.

Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, và Li Shen. 2023. Hợp nhất mô hình sâu: Một khảo sát. arXiv preprint.

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. 2016. Các mô hình hỗn hợp sentinel con trỏ.

Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, và Neil Houlsby. 2022. Học tương phản đa phương thức với limoe: hỗn hợp chuyên gia ngôn ngữ-hình ảnh. Trong NeurIPS.

Shashi Narayan, Shay B. Cohen, và Mirella Lapata. 2018. Đừng cho tôi chi tiết, chỉ tóm tắt thôi! mạng neural tích chập nhận thức chủ đề cho tóm tắt cực độ. Trong EMNLP.

OpenAI. 2023. Báo cáo kỹ thuật GPT-4. arXiv preprint.

Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, và Iryna Gurevych. 2021. Adapterfusion: Tổng hợp nhiệm vụ không phá hoại cho học chuyển giao. Trong EACL.

Jason Phang, Thibault Févry, và Samuel R Bowman. 2018. Bộ mã hóa câu trên cà kheo: Đào tạo bổ sung trên các nhiệm vụ dữ liệu được gán nhãn trung gian. arXiv preprint.

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. 2019. Các mô hình ngôn ngữ là những người học đa nhiệm không giám sát.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Khám phá giới hạn của học chuyển giao với một transformer văn bản-đến-văn bản thống nhất. J. Mach. Learn. Res.

Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, và Percy Liang. 2016. SQuAD: 100,000+ câu hỏi cho hiểu máy của văn bản. Trong EMNLP.

Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, và Jeff Dean. 2017. Các mạng neural cực kỳ lớn: Lớp hỗn hợp chuyên gia có cổng thưa thớt. Trong ICLR.

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, và Samuel R Bowman. 2019. GLUE: Một nền tảng đánh giá và phân tích đa nhiệm vụ cho hiểu ngôn ngữ tự nhiên. Trong ICLR.

Yequan Wang, Hengran Zhang, Aixin Sun, và Xuying Meng. 2022. Cort: Một baseline mới cho phân loại ý kiến so sánh bằng dual prompts. Trong EMNLP.

Brandon Yang, Gabriel Bender, Quoc V. Le, và Jiquan Ngiam. 2019. Condconv: Convolution có điều kiện tham số hóa cho suy luận hiệu quả. Trong NeurIPS.

Qinyuan Ye, Juan Zha, và Xiang Ren. 2022. Gợi ra và hiểu các kỹ năng cross-task với hỗn hợp chuyên gia cấp độ nhiệm vụ. Trong EMNLP.

Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du, và Dacheng Tao. 2022a. E2s2: Đào tạo trước chuỗi-đến-chuỗi tăng cường mã hóa cho hiểu và sinh ngôn ngữ. arXiv preprint.

Qihuang Zhong, Liang Ding, Li Shen, Peng Mi, Juhua Liu, Bo Du, và Dacheng Tao. 2022b. Cải thiện tối ưu hóa nhận thức độ sắc nét với mặt nạ fisher để tổng quát hóa tốt hơn trên các mô hình ngôn ngữ. Trong EMNLP.

Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, và Jifeng Dai. 2022. Uni-perceiver-moe: Học các mô hình chuyên gia tổng quát thưa thớt với moe có điều kiện. Trong NeurIPS.
