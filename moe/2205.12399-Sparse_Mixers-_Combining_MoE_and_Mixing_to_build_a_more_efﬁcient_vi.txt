# 2205.12399.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2205.12399.pdf
# Kích thước file: 614869 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Sparse Mixers: Kết hợp MoE và Mixing để xây dựng một
BERT hiệu quả hơn
James Lee-Thorp và Joshua Ainslie
Google Research
{jamesleethorp, jainslie}@google.com
Tóm tắt
Chúng tôi kết hợp khả năng của Mixture-of-Experts (MoE) gọi thưa với tốc độ
và sự ổn định của các phép biến đổi trộn tuyến tính để thiết kế mô hình
encoder Sparse Mixer. Sparse Mixer có hiệu suất hơi tốt hơn BERT trên
GLUE và SuperGLUE, nhưng quan trọng hơn là huấn luyện nhanh hơn 65%
và chạy suy luận nhanh hơn 61%. Chúng tôi cũng trình bày một biến thể
nhanh hơn, được đặt tên đơn giản là Fast Sparse Mixer, có hiệu suất hơi
thấp hơn BERT trên SuperGLUE, nhưng huấn luyện và chạy gần như nhanh
gấp đôi. Chúng tôi biện minh cho thiết kế của hai mô hình này bằng cách
thực hiện ablation cẩn thận qua các cơ chế trộn khác nhau, cấu hình MoE
và siêu tham số. Sparse Mixer khắc phục nhiều vấn đề về độ trễ và sự ổn định
của các mô hình MoE và mở ra triển vọng phục vụ các mô hình student thưa,
mà không cần phải distill chúng thành các biến thể dày đặc.¹

1 Giới thiệu
Các mô hình Mixture-of-Experts (MoE) gọi thưa đã thu hút sự quan tâm mạnh mẽ
trong những năm gần đây (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus
et al., 2022; Riquelme et al., 2021; Du et al., 2021;
Artetxe et al., 2021; Clark et al., 2022; Mustafa
et al., 2022). Các mô hình MoE hứa hẹn chi phí tính toán
dưới tuyến tính so với số lượng tham số mô hình. Bằng cách
huấn luyện các "experts" có thể xử lý độc lập các phần
dữ liệu đầu vào khác nhau, các lớp MoE tăng khả năng
mô hình với việc tăng FLOPS có hạn.

Có lẽ vì sự đánh đổi thuận lợi giữa khả năng và tính toán,
hầu hết các nghiên cứu MoE gần đây, bao gồm các nghiên cứu
nói trên, đã tập trung vào việc sử dụng MoE để mở rộng quy mô
các mô hình lớn. Việc sử dụng các lớp MoE để mở rộng quy mô
thành các mô hình lớn hơn mang lại lợi ích về chất lượng và
hiệu quả huấn luyện tổng thể so với các mô hình dày đặc,
nhưng không có lợi ích về độ trễ bước huấn luyện hoặc suy luận.
Thật vậy, nhiệm vụ phục vụ những mô hình này trong thực tế
hoặc bị bỏ qua hoặc được giao cho việc distill mô hình teacher
thưa thành mô hình student dày đặc (Hinton et al., 2015),
thường với mất mát chất lượng đáng kể so với mô hình teacher
thưa. Ví dụ, Fedus et al. (2022) chỉ có thể distill khoảng 30%
lợi ích chất lượng của Switch Transformer thành một mô hình
dày đặc.

Trực giao với MoE, các mô hình trộn hiệu quả
(Tolstikhin et al., 2021; Liu et al., 2021; Lee-Thorp
et al., 2021) thay thế attention trong các mô hình
giống Transformer bằng các phép biến đổi tuyến tính
đơn giản hơn hoặc các khối MLP "trộn" các biểu diễn
đầu vào. Các phép biến đổi tuyến tính đặc biệt hấp dẫn
vì chúng nhanh hơn so với các phép chiếu kết hợp và
tích vô hướng trong một lớp attention.

Trong công trình này, chúng tôi khai thác cả hai hướng
MoE và mixing để xây dựng các mô hình encoder thưa
có độ trễ thấp mà chúng tôi hy vọng có thể được sử dụng
trong các cài đặt production. Chúng tôi tập trung vào
các mô hình encoder, và các mô hình giống BERT nói riêng,
vì chúng được sử dụng rộng rãi trong thực tế – ví dụ,
trong dual encoders cho retrieval (Bromley et al., 1993;
Karpukhin et al., 2020).

So với mô hình Transformer gốc (Vaswani et al., 2017),
chúng tôi tăng tốc mô hình theo hai cách. (1) Chúng tôi
sử dụng khả năng tăng từ các sublayers MoE để bù đắp
việc giảm tham số trong các phần khác của mô hình.
(2) Chúng tôi sử dụng các phép biến đổi trộn để thay thế
một phần lớn các sublayers self-attention bằng các phép
biến đổi tuyến tính nhanh hơn. Mô hình kết quả, mà chúng
tôi đặt tên là Sparse Mixer, có hiệu suất hơi tốt hơn
(<1%) BERT trên GLUE (Wang et al., 2018) và SuperGLUE
(Wang et al., 2019), nhưng quan trọng nhất là huấn luyện
nhanh hơn 65% và chạy suy luận nhanh hơn 61%. Chúng tôi
cũng giới thiệu một biến thể đơn giản của Sparse Mixer,
được đặt tên đơn giản là Fast Sparse Mixer, có hiệu suất
hơi thấp hơn (<0.2%) BERT trên SuperGLUE, nhưng chạy
gần như nhanh gấp đôi: huấn luyện nhanh hơn 89% và
chạy suy luận nhanh hơn 98%.

Một phát hiện thú vị trong công trình của chúng tôi là
sự tương tác ổn định huấn luyện giữa các thành phần mô hình
thưa và trộn. Như một điểm so sánh, chúng tôi thấy rằng
việc đơn giản thay thế các sublayers feed-forward dày đặc
trong BERT bằng các biến thể MoE tạo ra các mô hình cực kỳ
không ổn định; xem Phần 5. Tuy nhiên, những sự không ổn định
này tan biến khi chúng tôi thay thế các sublayers self-attention
bằng các sublayers trộn. Chúng tôi giả thuyết rằng cơ sở
self-attention có trọng số liên quan (phụ thuộc token) là
nguồn gốc của sự không ổn định, và do đó việc thay thế
phần lớn các sublayers self-attention bằng các sublayers
trộn khiến các mô hình sparse mixer trở nên rất ổn định.

Tóm lại, chúng tôi giới thiệu hai mô hình:
• Sparse Mixer, khớp với BERT trên GLUE và SuperGLUE
nhưng chạy nhanh hơn 61-65%.
• Fast Sparse Mixer, có hiệu suất hơi thấp hơn BERT
(<0.2%) nhưng nhanh gần gấp 2 lần.

Chúng tôi biện minh cho thiết kế của các mô hình này bằng
cách thực hiện ablation qua các cấu hình trộn mô hình, MoE
và siêu tham số. Với Sparse Mixers, chúng tôi chứng minh
rằng sự suy giảm tốc độ và ổn định của các mô hình MoE
có thể được khắc phục bằng cách sử dụng các cơ chế trộn.
Điều này mở ra triển vọng phục vụ trực tiếp các mô hình
thưa, thay vì phải distill chúng thành các biến thể dày đặc.

2 Công trình liên quan
Mixture-of-Experts gọi thưa. Các mô hình Mixture-of-Experts
(MoE) được giới thiệu bởi Jacobs et al. (1991); Jordan và
Jacobs (1994) và gần đây được phổ biến bởi Shazeer et al.
(2017). Công trình gần đây, như (Zoph et al., 2022), đã
thực hiện lời hứa của các mô hình MoE bằng cách đạt được
kết quả tốt nhất trên một số benchmark NLP. Như với các
nghiên cứu MoE đương đại (Du et al., 2021; Lepikhin et al.,
2021), các mô hình này lớn và chủ yếu tập trung vào chất
lượng mô hình. Khi hiệu quả được nghiên cứu, nó thường
ở mức độ của một metric hiệu quả tổng thời gian huấn luyện.
Ví dụ, mặc dù tốc độ mỗi bước huấn luyện của Switch
Transformer (Fedus et al., 2022) chậm hơn Transformer
gốc, nhưng vì Switch Transformer vượt qua độ chính xác
cao nhất của mô hình gốc trong một phần nhỏ các bước,
Switch Transformer có thể được mô tả chính xác là một
mô hình hiệu quả hơn. Tuy nhiên, tốc độ bước chậm hơn
là điểm yếu chí mạng cho việc phục vụ những mô hình như
vậy; người ta thường không thể yêu cầu người dùng đợi
lâu hơn cho một phản hồi mô hình chính xác hơn.

Một ngoại lệ đáng chú ý là Jaszczur et al. (2021),
những người thưa hóa nhiều thành phần của Transformer,
chủ yếu bằng cách thay thế softmaxes bằng argmaxes,
để đạt được tăng tốc hơn 2 lần trong tốc độ suy luận
không batch trên CPUs cho kích thước mô hình Base/Large.
Trái ngược với công trình của chúng tôi, việc tăng tốc
của họ không chuyển sang phần cứng accelerator hoặc
sang huấn luyện.

Các cơ chế memory là một kỹ thuật thưa phổ biến khác
để thêm khả năng vào các mô hình với việc tăng tính toán
có hạn; xem, ví dụ, (Weston et al., 2015; Sukhbaatar et al.,
2015; Lample et al., 2019). Trong khi hấp dẫn về mặt trực
quan và hứa hẹn về mặt thực nghiệm, các triển khai không
tối ưu (đặc biệt là look-ups) cho phần cứng accelerator
thường tạo ra các mô hình memory có tính chất tính toán
lý thuyết thuận lợi, nhưng chậm trong thực tế.

Trộn. Một số công trình gần đây đã khám phá các cơ chế
trộn, như nhân ma trận (Tay et al., 2020; Lee-Thorp et al.,
2021), các khối MLP (Tolstikhin et al., 2021; Liu et al.,
2021), và các phép biến đổi phổ (Lee-Thorp et al., 2021),
như một sự thay thế hiệu quả cho attention trong các mô hình
giống Transformer. You et al. (2020); Raganato et al. (2020);
Lee-Thorp et al. (2021) thấy rằng các mô hình attention-mixing
lai, trong đó một phần hoặc một số lượng hạn chế các sublayers
attention được giữ lại, nhanh hơn Transformers chỉ với việc
suy giảm độ chính xác rất hạn chế. Dựa trên những công trình
này, chúng tôi sử dụng các sublayers MLP thưa để bù đắp
cho khoảng cách độ chính xác còn lại.

Cấu hình tham số mô hình. Việc mở rộng quy mô các mô hình
đã được chứng minh là một chương trình thành công để tăng
chất lượng mô hình (Kaplan et al., 2020; Raffel et al., 2020;
Brown et al., 2020). Mối quan hệ giữa số lượng tham số mô hình
và chất lượng mô hình có thể được mô hình hóa tương đối
thông qua một luật lũy thừa (Kaplan et al., 2020; Clark et al.,
2022; Hoffmann et al., 2022). Tuy nhiên, cấu hình của những
tham số này trong mô hình cũng đóng vai trò quan trọng
trong chất lượng và hiệu quả mô hình. Phù hợp với Tay et al.
(2021b), chúng tôi thấy rằng việc làm cho mô hình mỏng hơn
(kích thước mô hình nhỏ hơn) nhưng sâu hơn (nhiều lớp hơn)
thường là một cách hiệu quả để phân phối tham số trong
toàn bộ mô hình.

Distillation. Knowledge distillation (Hinton et al., 2015)
là một kỹ thuật mạnh mẽ đã được triển khai thành công
để huấn luyện các mô hình BERT "student" hiệu quả từ
các mô hình "teacher" lớn hơn (Sanh et al., 2019; Jiao et al.,
2020; Sun et al., 2019; Xu et al., 2020). Mặc dù chúng tôi
nghi ngờ rằng Sparse Mixer sẽ cung cấp một kiến trúc
distillation hứa hẹn, chúng tôi xem các kỹ thuật distillation
bản thân là trực giao với mục tiêu tối ưu hóa kiến trúc
của chúng tôi.

--- TRANG 2 ---
Thật vậy, chúng tôi thấy rằng trong Figure 2, việc tăng
tốc và lợi ích chất lượng từ Sparse Mixer được chuyển
sang cả kích thước lớn hơn (teacher) và nhỏ hơn (student).

3 Mô hình
3.1 Kiến trúc
Không gian thiết kế của chúng tôi cho Sparse Mixer
được xây dựng từ các khối encoder xếp chồng của BERT
(Devlin et al., 2019), mà chúng tôi sử dụng làm Transformer
encoder chính tắc (Vaswani et al., 2017). Mỗi khối encoder
chứa một sublayer trộn hoặc self-attention và một sublayer
MLP (dày đặc hoặc MoE), được kết nối với các kết nối
dư và layer norms. Chúng tôi giữ các lớp embedding đầu
vào và chiếu đầu ra tiêu chuẩn BERT (Devlin et al., 2019);
xem cũng Phụ lục A.1. Chúng tôi đi đến ngăn xếp khối
encoder Sparse Mixer, được hiển thị trong Figure 1, bằng
cách thực hiện ablation cẩn thận qua các cơ chế trộn,
cấu hình MoE và siêu tham số mô hình trong Phần 4.

3.2 MoE
Trong một lớp MoE, chúng tôi khởi tạo nhiều instance
khác nhau ("experts") của lớp và thực hiện tính toán
song song với mỗi instance trên các shard dữ liệu riêng biệt.
Do đó, các lớp MoE được kích hoạt thưa có khả năng
lớn hơn các lớp dày đặc. Khi chúng tôi tăng số lượng
experts, chúng tôi thường giảm expert capacity – số lượng
tokens được xử lý bởi một expert riêng lẻ. Cụ thể hơn,
với E biểu thị số lượng experts và n số lượng tokens,
chúng tôi đặt
expert capacity = cf × n/E;
trong đó cf là hệ số capacity vô hướng. Đối với cf ≤ 1,
điều này cho phép chúng tôi tăng số lượng tham số mô hình
với việc tăng FLOPS tối thiểu.²

Routing. Chúng tôi sử dụng một router hoặc gating function
để hướng dẫn cẩn thận các shard dữ liệu giữa các experts.
Điều này theo trực quan rằng expert A có thể trở nên
chuyên biệt trong việc xử lý các inputs trong một phần
của không gian embedding, trong khi các experts B, C, ...
chuyên biệt cho các phần khác của không gian embedding.
Chính router đảm bảo sự thưa thớt bằng cách chỉ gán
một tập con tokens cho mỗi expert, do đó đảm bảo rằng
chỉ một tập con tham số được kích hoạt cho mỗi token.

Thiết kế router là một lĩnh vực nghiên cứu tích cực
(Lewis et al., 2021; Roller et al., 2021; Zhou et al., 2022;
Clark et al., 2022). Chúng tôi giới hạn bản thân trong
hai loại router: "Tokens Choose" truyền thống và "Experts
Choose". Chúng tôi theo thực hành tiêu chuẩn của routing
ở mức token – router gán mỗi token cho một tập con experts.
Cả hai thuật toán gán đều tạo ra router logits bằng cách
chiếu các biểu diễn token từ kích thước embedding, dm,
sang kích thước expert, E. Chúng tôi áp dụng softmax
để chuẩn hóa các logits thành một phân phối xác suất.
Cuối cùng, tokens được gán cho experts sử dụng một
trong các thuật toán gán.

Tokens Choose routing. Đối với routing Tokens Choose
(Shazeer et al., 2017), mỗi token được gán cho top-k
experts của nó. Chúng tôi tập trung vào routing top-1
("Switch") (Fedus et al., 2022). Vì expert capacities
bị hạn chế, không có đảm bảo rằng một token nhất định
có thể được route đến expert hàng đầu của nó, mặc dù
bất kỳ token nào không đến được expert vẫn sẽ lan truyền
vào khối encoder tiếp theo thông qua kết nối dư. Cũng
không có đảm bảo rằng một expert nhất định nhận được
ít nhất một token. Vì vậy, để đảm bảo rằng tính toán
được phân phối hiệu quả giữa các experts, chúng tôi
bao gồm một loss cân bằng tải như trong (Shazeer et al.,
2017; Fedus et al., 2022).

Chúng tôi có thể tăng expert capacity bằng cách tăng
capacity factor, cf. Điều này sẽ tăng xác suất rằng
một token nhất định được route đến các experts mong muốn.
Giảm cf sẽ làm thưa hơn nữa mô hình và tăng tốc
sublayer MoE.³ Chúng tôi sử dụng Batch Prioritized
Routing (Riquelme et al., 2021) để ưu tiên routing
các tokens có xác suất router cao nhất, thay vì chỉ
routing các tokens theo thứ tự từ trái sang phải trong batch.

Experts Choose routing. Đối với thuật toán gán Experts
Choose (Zhou et al., 2022), các experts chọn top tokens
của chúng, thay vì tokens chọn experts. Điều này thực
sự tương đương với việc chuyển vị các xác suất router
trước phép toán top-k. Mỗi expert thực hiện phép toán
top-k với k = expert capacity. Một token riêng lẻ có thể
được xử lý bởi nhiều experts hoặc không có expert nào
cả. Vì các experts có sự lựa chọn tokens và luôn lấp đầy
buffer của chúng, việc tăng capacity factor, cf, sẽ tăng
cả số lượng tokens mà một expert xử lý và cũng là số
lượng experts mà một token nhất định được route tới.
Vì mỗi expert luôn lấp đầy capacity của nó, không cần
auxiliary loading balancing loss.

Kích thước nhóm token. Tokens được chia thành các nhóm
và việc gán expert được thực hiện trên cơ sở từng nhóm.
Kích thước nhóm lớn hơn sẽ dẫn đến các tính toán top-k
và sắp xếp chậm hơn nhưng chính xác hơn, trong khi
kích thước nhóm nhỏ hơn sẽ dẫn đến các lựa chọn routing
nhanh hơn nhưng xấp xỉ hơn. Trong thực tế, chúng tôi
thấy rằng các lựa chọn routing không hoàn hảo có thể
chấp nhận được và mặc định sử dụng kích thước nhóm
4096 tokens.

Chiến lược song song hóa. Trong công trình này, chúng tôi
tập trung vào các kiến trúc nhanh hơn, có thể phục vụ
được bằng cách sử dụng parallelism expert và dữ liệu.
Chúng tôi sử dụng data parallelism để shard dữ liệu
trên các thiết bị, và expert parallelism để phân vùng
experts trên các thiết bị; ví dụ, đặt experts 1 và 2
trên thiết bị 1, experts 3 và 4 trên thiết bị 2, v.v.
Model parallelism là trục thứ ba để shard các trọng
số mô hình (ma trận) trên các thiết bị; ví dụ, expert 1
được chia trên các thiết bị 1 và 2, expert 2 được chia
trên các thiết bị 3 và 4, v.v. Model parallelism thường
có lợi nhất cho việc mở rộng quy mô đến các kích thước
mô hình lớn hơn.

3.3 Trộn
Chúng tôi sử dụng các phép biến đổi trộn tuyến tính
đơn giản như các thay thế trực tiếp cho một tập con
các sublayers self-attention. Các phép biến đổi trộn
cung cấp tốc độ để giảm khả năng và tính linh hoạt.
Thật vậy, cơ chế attention chứa bốn phép chiếu được
tham số hóa và hai phép toán tích vô hướng ("QK" và "V"),
cho phép các sublayers self-attention xây dựng các biểu
diễn trong một cơ sở phụ thuộc token có tính biểu đạt
cao. Mặt khác, các phép biến đổi trộn mà chúng tôi
nghiên cứu được triển khai thông qua hai phép chiếu
độc lập với token, một dọc theo mỗi kích thước sequence
và model. Việc cố định cơ sở trộn, so với các inputs
dữ liệu khác nhau, hóa ra lại ổn định hóa mô hình.

Các phép biến đổi phổ. Chúng tôi thử nghiệm với các
phép biến đổi Fourier và Hartley (Lee-Thorp et al., 2021).
Chúng tôi tích hợp các phép biến đổi này thông qua một
sublayer Fourier. Sublayer Fourier áp dụng một phép
biến đổi Fourier rời rạc 1D (DFT) dọc theo kích thước
sequence, Fseq, và một DFT 1D dọc theo kích thước ẩn, Fh:
y = ℜ{Fseq(Fh(x))};                    (1)
trong đó ℜ biểu thị phần thực.

Sublayer Hartley sử dụng Phương trình (1) với DFT
được thay thế bằng Discrete Hartley Transform, H.⁴
Chúng tôi tính toán các phép biến đổi Fourier và Hartley
sử dụng Fast Fourier Transform (FFT) (Cooley và Tukey,
1965; Frigo và Johnson, 2005).

Trong Phương trình (1), chúng tôi biến đổi dọc theo
cả kích thước sequence và ẩn. Mặc dù mục đích chính
của một sublayer trộn là kết hợp các inputs dọc theo
kích thước sequence, Lee-Thorp et al. (2021) thấy rằng
việc trộn dọc theo kích thước ẩn cũng cải thiện chất
lượng mô hình.

Các phép chiếu ma trận có cấu trúc. Chúng tôi khám phá
các ma trận có cấu trúc dựa trên giả thuyết rằng việc
thêm cấu trúc vào cơ sở trộn có thể cải thiện phân phối
của các biểu diễn đầu ra. Chúng tôi xem xét hai ma trận
có cấu trúc được tham số hóa: Toeplitz và circulant.
Ma trận Toeplitz là ma trận trong đó mỗi đường chéo
là hằng số. Ma trận circulant là một loại ma trận
Toeplitz đặc biệt, trong đó tất cả các hàng được cấu
thành từ các phần tử giống nhau nhưng được xoay một
phần tử sang phải so với hàng trước đó. Đối với cả
hai ma trận, các trọng số được học. Sublayer trộn
tương ứng trộn dọc theo kích thước sequence và ẩn.
Ví dụ, cho trường hợp Toeplitz, chúng tôi thực hiện:
y = Tseq × Th × x;                      (2)
trong đó Tseq và Th biểu thị các ma trận Toeplitz.⁵

⁴ Hartley Transform, biến đổi input thực thành output thực, có thể được mô tả bằng Fourier Transform: H = ℜ{F} + ℑ{F}. Trong trường hợp Hartley Transform, chúng tôi có thể bỏ qua ℜ từ Phương trình (1).

⁵ Các phép nhân ma trận liên quan đến ma trận circulant có thể

--- TRANG 3 ---
Linear Mixing
Linear Mixing 4 x
Dense MLP Sparse MLP Self-Attention Dense MLP 
4 x
5 x
Linear Mixing Dense MLP 
1 x

Hình 1: Các khối encoder Sparse Mixer cho cấu hình Base. Layer norms, kết nối dư, các lớp embedding và lớp đầu ra không được hiển thị. Các khối K=4 hàng đầu chứa self-attention và dense MLPs; các khối M=4 giữa chứa mixing và sparse MLPs; và các khối L=1 và P=5 còn lại chứa mixing và dense MLPs.

được tính toán sử dụng FFT (Davis, 1970). Điều này yêu cầu ba phép toán: một FFT để chéo hóa tính toán, một để áp dụng phép nhân ma trận được chéo hóa và một iFFT để biến đổi trở lại không gian thực. Ma trận Toeplitz có thể được nhúng trong ma trận circulant để tận dụng cùng một tính toán FFT. Trong thực tế, chúng tôi thấy rằng đối với độ dài sequence tiêu chuẩn (512), việc sử dụng FFT chậm hơn so với phép nhân ma trận trực tiếp trên cả GPU và TPU.

Các phép chiếu ma trận vanilla. Chúng tôi cũng xem xét các phép chiếu ma trận được tham số hóa "không có cấu trúc", dày đặc hoàn toàn. Theo (Lee-Thorp et al., 2021), chúng tôi gọi sublayer trộn phát sinh từ trường hợp này là sublayer "Linear". Sublayer Linear thực hiện cùng FLOPS như các sublayers ma trận có cấu trúc (miễn là FFT không được sử dụng), nhưng linh hoạt hơn do số lượng trọng số ma trận tăng.

3.4 Triển khai
Chúng tôi huấn luyện và tối ưu hóa mô hình trên 8 GPU V100. Chúng tôi tin rằng kết quả của chúng tôi khá mạnh mẽ đối với các accelerators khác nhau (ví dụ TPU) vì hầu hết các sửa đổi của chúng tôi đều quy về các phép nhân ma trận thân thiện với accelerator. Trong Phần 5, chúng tôi mở rộng quy mô kích thước mô hình lên và xuống trên TPUs và thấy rằng các đánh đổi hiệu quả thuận lợi tương tự vẫn tồn tại. Chúng tôi sử dụng JAX (Bradbury et al., 2018) trong framework Flax (Heek et al., 2020).⁶

4 Coordinate Descent
Chúng tôi huấn luyện trong một cài đặt transfer learning điển hình (Devlin et al., 2019): Pre-training Masked Language Modelling (MLM) và Next Sentence Prediction (NSP), theo sau là fine-tuning trên GLUE (Wang et al., 2018) và SuperGLUE (Wang et al., 2019). Khi so sánh các mô hình, chúng tôi luôn sử dụng cùng một cài đặt chính xác cho tất cả các mô hình và baselines. Cụ thể, chúng tôi theo cài đặt pre-training trong (Devlin et al., 2019) với một vài cập nhật: (1) chúng tôi pre-train trên bộ dữ liệu C4 lớn hơn nhiều (Raffel et al., 2020); (2) chúng tôi sử dụng mô hình từ vựng SentencePiece 32000 (Kudo và Richardson, 2018) được huấn luyện trên tập con 100 triệu câu của C4; và (3) chúng tôi sử dụng batch size nhỏ hơn là 64 (Devlin et al. (2019) sử dụng 256). Chúng tôi sử dụng độ dài sequence 512 trong suốt quá trình pre-training. Các thí nghiệm được chạy trên 8 chip GPU V100, ngoại trừ các thí nghiệm scaling (Phần 5) được chạy trên 32 chip TPU v3.

Trong phần này, chúng tôi thực hiện một "coordinate descent" qua các cấu hình mô hình của chúng tôi cho đến khi chúng tôi đạt được thiết kế Sparse Mixer cuối cùng. Với số lượng lớn các siêu tham số mô hình cần khám phá, chúng tôi thực hiện nhiều tìm kiếm tham số song song. Ví dụ, hình dạng mô hình và cấu hình MoE được khám phá độc lập và sau đó các cấu hình hứa hẹn nhất từ mỗi chương trình được kết hợp.

Đối với nghiên cứu coordinate descent của chúng tôi, chúng tôi chỉ pre-train trong 500k bước, mà chúng tôi thấy khá chỉ thị về hiệu suất mô hình. Các mô hình được fine-tuned với cùng batch size (64) trên split Validation của mỗi nhiệm vụ GLUE tương ứng trong 5 epochs và kết quả tốt nhất cho mỗi nhiệm vụ được chọn từ ba learning rates cơ bản mặc định, được điều chỉnh từ Devlin et al. (2019): {10⁻⁵, 5×10⁻⁵, 10⁻⁴}. Mô hình cuối cùng của chúng tôi được pre-train lâu hơn và được đánh giá trên cả GLUE và SuperGLUE cho một tập hợp rộng hơn các cấu hình huấn luyện trong Phần 5.

Chúng tôi ưu tiên hiệu quả – tốc độ và độ chính xác. Chúng tôi sử dụng tốc độ bước pre-training như một proxy cho độ trễ mô hình. Chúng tôi dựa vào điểm GLUE trung bình downstream như metric độ chính xác chính, nhưng dự phòng vào độ chính xác MLM và NSP upstream khi điểm GLUE giữa các biến thể mô hình tương tự. Các thí nghiệm coordinate descent bổ sung được tóm tắt trong Phụ lục A.2, và kết quả GLUE đầy đủ cho tất cả các thí nghiệm coordinate descent được cung cấp trong Phụ lục A.3.

4.1 Trộn
Cơ chế trộn. Chúng tôi so sánh các cơ chế trộn được thảo luận trong Phần 2. Đối với mỗi mô hình trộn, chúng tôi đầu tiên thay thế tất cả các sublayers self-attention bằng sublayer trộn tương ứng. Kết quả được hiển thị trong Bảng 1. Các mô hình phổ (Fourier và Hartley) có hiệu suất tốt nhất trên GLUE. Mô hình Linear có hiệu suất hơi thấp hơn các mô hình phổ, trong khi các mô hình trộn có cấu trúc (Circulant và Toeplitz) có hiệu suất tệ nhất trên GLUE. Các phương pháp phổ, được triển khai hiệu quả sử dụng FFTs, là nhanh nhất.⁷

Trộn lai attention. Chúng tôi chọn hai ứng viên đại diện mạnh từ Bảng 1, cụ thể là các mô hình Hartley và Linear, và thay thế một tập con các sublayers trộn hàng đầu nhất bằng self-attention. Kết quả được tóm tắt trong Bảng 2. Khi chúng tôi bao gồm self-attention, chúng tôi thấy rằng mô hình Linear lai cung cấp lợi ích chất lượng lớn hơn so với mô hình Hartley lai. Mặc dù các mô hình Hartley lai nhanh hơn, một so sánh iso-speed vẫn cho thấy rằng các mô hình Linear lai hiệu quả hơn. Ví dụ, Hartley-6 và Linear-4 có tốc độ gần như nhau, nhưng mô hình Linear-4 chính xác hơn. Do đó, chúng tôi chọn sử dụng mô hình Linear-4. Trong Phụ lục A.2 (Bảng 11), chúng tôi chỉ ra rằng chúng tôi có độ chính xác tốt nhất khi các sublayers self-attention được đặt ở các lớp hàng đầu.

4.2 Hình dạng mô hình
Tất cả các thí nghiệm hình dạng mô hình được chạy song song và bắt đầu từ cấu hình Linear-4.

Kích thước mô hình. Trong việc tìm kiếm một mô hình hiệu quả hơn, chúng tôi cố gắng thu gọn mô hình bằng cách giảm kích thước mô hình (Bảng 3) và kích thước kích hoạt MLP trung gian (Bảng 12 trong Phụ lục A.2). Đối với mỗi tọa độ, chúng tôi thấy rằng có các ngưỡng cắt (dff = 2048 và dm = 512) dưới đó chất lượng mô hình giảm mạnh. Chúng tôi chọn các ngưỡng cắt này như các giá trị hình dạng mô hình tối ưu của chúng tôi. Chính trong việc giảm hai siêu tham số này mà chúng tôi có được tăng tốc lớn nhất trong mô hình của chúng tôi. Tuy nhiên, có sự suy giảm chất lượng đáng kể phải được bù đắp bởi khả năng tăng từ các sublayers MoE trong Phần 4.3.

Số lượng lớp. Chúng tôi thay đổi số lượng lớp trong Bảng 13 trong Phụ lục A.2. Chúng tôi chọn 14 lớp, vượt qua đó chúng tôi không thấy lợi ích chất lượng.

4.3 MoE
Cấu hình khởi đầu của chúng tôi cho các ablations MoE là cấu hình Linear-4 với mỗi sublayer MLP dày đặc khác được thay thế bởi một sublayer MoE (6 sublayers MoE) và 16 experts trong mỗi sublayer MoE. Chúng tôi thực hiện các thí nghiệm MoE song song với các tối ưu hóa hình dạng mô hình, vì vậy tất cả các ablations MoE được thực hiện trên mô hình kích thước Base mặc định với 12 lớp, dff = 3072 và dm = 768.

Như trong (Zoph et al., 2022), chúng tôi thấy rằng chúng tôi phải điều chỉnh giao thức học fine-tuning để chuyển tốt hơn bất kỳ lợi ích MLM pre-training MoE nào xuống downstream. Cụ thể, các mô hình encoder MoE của chúng tôi có lợi từ các learning rates cơ bản lớn hơn ({10⁻⁴, 5×10⁻⁴, 10⁻³}) và các tỷ lệ dropout lớn hơn (0.2) cho experts; xem Phụ lục A.4 để so sánh learning rates và tỷ lệ dropout expert. Đối với so sánh mô hình cuối cùng của chúng tôi với BERT trong Phần 5, chúng tôi xem xét một phạm vi rộng các learning rates cơ bản cho tất cả các mô hình.⁸

Bảng 1: Các metric độ chính xác trung bình và tốc độ bước pre-training trung vị cho các mô hình trộn. Mô hình "Fourier" giống hệt với FNet (Lee-Thorp et al., 2021). Tăng tốc so với BERT (xem Bảng 8) được hiển thị trong dấu ngoặc. Các metric tốt nhất được tô đậm, trong khi các metric tốt thứ hai được gạch chân. Dấu sao chỉ ra các cấu hình được chọn.

Độ chính xác (%) Tốc độ
Mô hình GLUE MLM NSP (ms/batch)
Fourier 78.4 55.7 75.4 173 (1.75x)
Hartley ⭐ 78.0 58.5 74.9 172 (1.77x)
Circulant 75.1 58.3 75.6 200 (1.52x)
Toeplitz 76.5 57.7 76.5 200 (1.52x)
Linear ⭐ 77.7 57.6 77.4 200 (1.52x)

⁶ Mã nguồn Sparse Mixers có sẵn tại https://github.com/google-research/google-research/tree/master/sparse_mixers.

⁷ Có một chút nhiễu trong đo lường tốc độ, vì vậy chúng tôi không đọc quá nhiều vào những khác biệt tốc độ nhỏ giữa các mô hình Fourier và Hartley.

⁸ Phù hợp với (Zoph et al., 2022), chúng tôi thấy rằng các learning rates cơ bản lớn hơn không có lợi cho các mô hình dày đặc.

--- TRANG 4 ---
Bảng 2: Metrics cho các mô hình attention-mixing lai. Hartley-k biểu thị mô hình với k sublayers self-attention và 12-k sublayers Hartley.

Độ chính xác (%) Tốc độ
Mô hình GLUE MLM NSP (ms/batch)
Hartley-0 78.0 58.5 74.9 172 (1.76x)
Hartley-1 78.0 51.9 75.3 183 (1.66x)
Hartley-2 81.1 61.3 79.8 193 (1.57x)
Hartley-3 77.9 50.3 76 204 (1.49x)
Hartley-4 82.7 62.6 81 216 (1.41x)
Hartley-6 82.9 63.5 81.2 234 (1.30x)
Linear-0 77.7 57.6 77.4 200 (1.51x)
Linear-1 78.1 62.5 78.3 208 (1.46x)
Linear-2 82.8 62.8 81 218 (1.40x)
Linear-3 82.8 63.3 81.6 226 (1.35x)
Linear-4 ⭐ 83.4 63.6 81.7 235 (1.29x)
Linear-6 83.6 64 81.7 251 (1.21x)

Bảng 3: Thay đổi kích thước mô hình, dm. Như trong Transformer, chúng tôi đặt kích thước mô hình và embedding bằng nhau. Đối với các sublayers self-attention, chúng tôi cố định số lượng heads self-attention thành dm/64.

Độ chính xác (%) Tốc độ
dm GLUE MLM NSP (ms/batch)
768 83.4 63.6 81.7 235 (1.29x)
512 ⭐ 83.0 62.5 80.9 161 (1.89x)
256 80.7 58.9 78.4 91 (3.34x)
128 71.6 54 73.8 58 (5.29x)

Bảng 4: Metrics độ chính xác và tốc độ cho routing Top-1 Tokens Choose (TC) và Experts Choose (EC).

Độ chính xác (%) Tốc độ
Router GLUE MLM NSP (ms/batch)
TC 83.4 64 80.8 280 (1.09x)
EC ⭐ 83.5 64.6 81.2 283 (1.08x)

Routers. Các cơ chế routing được so sánh trong Bảng 4. Chúng tôi chọn routing Experts Choose vì nó đạt được kết quả độ chính xác hơi cao hơn và không yêu cầu cấu hình load balancing loss.

Lớp MoE. Trong Bảng 5, chúng tôi thay đổi số lượng sublayers MoE và bố cục của những lớp đó trong mô hình. Khi chúng tôi tăng số lượng lớp MoE, độ chính xác MLM cải thiện, nhưng những lợi ích pre-training này không luôn dẫn đến hiệu suất GLUE tốt hơn. Đây là một xu hướng chung mà chúng tôi quan sát thấy; xem thêm Phụ lục A.2 và (Zoph et al., 2022). Chúng tôi chọn 4 lớp MoE, có hiệu suất tốt trên GLUE và tốt hơn mô hình 2 sublayers MoE trên nhiệm vụ MLM.

Kết quả của các thí nghiệm bố cục MoE rõ ràng hơn – chúng tôi chọn sử dụng bố cục MIDDLE, đặt tất cả sublayers MoE ở các lớp giữa của mô hình. Tuy nhiên, thú vị là lưu ý rằng bố cục TOP tăng lớn độ chính xác MLM, nhưng không cải thiện độ chính xác GLUE downstream.

Số lượng experts. Chúng tôi có thể tăng số lượng experts để tăng khả năng của mô hình. Đối với số lượng lớn experts, chi phí tính toán của việc gán routing trở nên đáng kể hơn, trong khi tín hiệu huấn luyện cho một expert riêng lẻ trở nên quá yếu để tạo điều kiện cho việc huấn luyện hiệu quả vì mỗi expert xử lý một phần quá nhỏ dữ liệu. Tìm kiếm sự thỏa hiệp giữa chất lượng và tốc độ, chúng tôi cuối cùng chọn sử dụng 16 experts. Kết quả được tóm tắt trong Bảng 14 trong Phụ lục A.2.

Bảng 5: Thay đổi số lượng và bố cục của các sublayers MoE. Định nghĩa bố cục: 6-BOTTOM (6 lớp đầu tiên), 6-MIDDLE (6 lớp giữa) hoặc 6-MIXED (mỗi lớp lẻ), 6-MIXED-odd (mỗi lớp chẵn), và 6-TOP (6 lớp cuối). Số lượng experts và expert capacity – số lượng tokens được xử lý bởi mỗi expert – được cố định. Mỗi lớp MoE thêm một số chi phí tính toán và giao tiếp thiết bị, làm chậm mô hình.

Độ chính xác (%) Tốc độ
Cấu hình GLUE MLM NSP (ms/batch)
2-MIXED 83.6 63.6 81.3 246 (1.23x)
4-MIXED ⭐ 83.6 63.9 81.3 264 (1.15x)
6-MIXED 83.5 64.6 81.2 283 (1.08x)
12-MIXED 83.1 64.9 81.4 352 (0.86x)
6-BOTTOM 83.2 62.7 81.4 289 (1.05x)
6-MIDDLE ⭐ 83.9 64 81.7 284 (1.08x)
6-MIXED 83.5 64.6 81.2 283 (1.08x)
6-MIXED-odd 83.2 64.8 81.6 292 (1.04x)
6-TOP 83.4 65.4 81.2 287 (1.06x)

Kích thước expert. Chúng tôi có thể kiểm soát số lượng tham số trong mỗi expert bằng cách thay đổi dff của nó. Trong Bảng 15 (Phụ lục A.2), chúng tôi thấy rằng: (1) việc sử dụng experts nhỏ hơn mang lại sự giảm độ chính xác nhỏ, nhưng lợi ích tốc độ hạn chế; và (2) việc tăng kích thước expert tăng độ chính xác MLM, nhưng không tăng GLUE. Vì vậy, để đơn giản, chúng tôi chọn giữ dff expert cùng kích thước với dff dày đặc.

4.4 Sparse Mixer
Kết hợp các kết quả trước đó, chúng tôi đi đến mô hình Sparse Mixer trong Hình 1:

Hình dạng: 14 lớp, 512 dm, 2056 dff;
Thưa: 4 MIDDLE MoE, 16 experts, 2056 dff, routing EC, 1.0 cf;
Mixer: Linear, 4 TOP Attention layers.

5 Đánh giá Sparse Mixer
So sánh huấn luyện đầy đủ với BERT. Khi so sánh Sparse Mixer và BERT, cả hai mô hình đều được pre-train trên C4 trong 1M bước đầy đủ, với batch size 64, và sau đó được đánh giá trên cả GLUE và SuperGLUE cho một phạm vi lớn hơn các batch sizes fine-tuning (16, 32, và 64) và learning rates cơ bản ({10⁻⁵, 5×10⁻⁵, 10⁻⁴, 5×10⁻⁴, 10⁻³}). Kết quả tốt nhất trên tất cả learning rates (cho mỗi nhiệm vụ) và batch sizes (cho tất cả nhiệm vụ) được hiển thị trong Bảng 6 và 7; xem Phụ lục A.3 cho kết quả của tất cả batch sizes.⁹

Điểm GLUE của BERT và Sparse Mixer rất giống nhau, mặc dù chúng phân kỳ một chút nhiều hơn trên SuperGLUE, nơi Sparse Mixer có hiệu suất đặc biệt tốt trên nhiệm vụ CB, nhưng hiệu suất thấp hơn BERT trên các nhiệm vụ multi-label MultiRC và ReCoRD.

Mở rộng quy mô Sparse Mixer. Bảng 6-8 chỉ ra rằng Sparse Mixer hiệu quả hơn BERT trong cấu hình Base. Trong Hình 2, chúng tôi so sánh BERT và Sparse Mixer trên một lựa chọn các kích thước mô hình. Chúng tôi sử dụng độ chính xác MLM như một proxy cho độ chính xác mô hình và tốc độ bước pre-training như một proxy cho tốc độ mô hình tổng thể. Tốc độ bước pre-training là một proxy tốt cho tốc độ suy luận (xem Bảng 8). Độ chính xác MLM chỉ mang tính chỉ thị về độ chính xác downstream. Chúng tôi xây dựng một hình tốc độ-độ chính xác tương tự cho độ chính xác NSP trong Hình 4 trong Phụ lục A.5. Những lưu ý này sang một bên, Hình 2 cho thấy rằng tốc độ và độ chính xác thuận lợi của Sparse Mixer mở rộng đến các kích thước mô hình khác, vì nó định nghĩa frontier hiệu quả trên tất cả các kích thước mô hình được xem xét.

⁹ Theo Devlin et al. (2019), chúng tôi bỏ qua nhiệm vụ WNLI.

--- TRANG 5 ---
Bảng 6: Kết quả GLUE trên split Validation. Chúng tôi báo cáo điểm F1/accuracy cho QQP và MRPC, tương quan Spearman cho STS-B và điểm accuracy cho tất cả các nhiệm vụ khác. Các metric accuracy MNLI được báo cáo theo splits match/mismatch.

Mô hình MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Trung bình
BERT 81.3 / 81.8 86.7 / 90.3 88.9 91.1 77.6 87.3 90.5 / 86.8 69.7 84.7
Sparse Mixer 80.7 / 81 87.1 / 90.5 89.1 90.9 79 88.1 90.4 / 86.3 72.2 85.0

Bảng 7: Kết quả SuperGLUE Validation. Chúng tôi báo cáo điểm macro-F1 cho CB, điểm micro-F1/exact match cho MultiRC, điểm F1/exact match cho ReCoRD, và điểm accuracy cho tất cả các nhiệm vụ khác.

Mô hình BoolQ CB COPA MultiRC ReCoRD RTE WiC Trung bình
BERT 74.6 86.4 / 85.7 58 74.1 / 26.2 68.6 / 52.2 65 65.7 65.7
Sparse Mixer 74.4 93.3 / 92.9 62 72.4 / 22.5 65.9 / 49.2 64.6 66.5 66.4

Bảng 8: Đặc tính tính toán mô hình cho BERT, Sparse Mixer (SM) và Fast Sparse Mixer (FSM), được giới thiệu trong Phần 5. "Size" là số lượng tham số mô hình. Tốc độ chạy được đo bằng tốc độ suy luận trên mỗi ví dụ và tốc độ bước pre-training trên mỗi ví dụ.

GFLOPS Kích thước Suy luận Huấn luyện
Mô hình (/ex) (M) (ms/ex) (ms/ex)
BERT 102 112 1.34 4.75
SM 73 180 0.84 (1.61x) 2.87 (1.65x)
FSM 60 180 0.68 (1.98x) 2.51 (1.89x)

Bảng 9: Fast Sparse Mixer (FSM). Sparse Mixer mặc định (SM) sử dụng capacity factor (cf) là 1 và kích thước nhóm routing (g) là 4096. Một số cấu hình ít thuận lợi hơn bị bỏ qua.

Độ chính xác (%) Tốc độ
Mô hình GLUE SuperGLUE (ms/batch)
BERT 84.7 65.7 304
SM 85.0 66.4 184 (1.65x)
FSM (cf=0.5) 84.7 65.6 161 (1.89x)
g=2048 84.5 65.1 173 (1.75x)
cf=0.75,g=2048 84.3 65.2 165 (1.84x)

Bảng 10: Sự ổn định của BERT, sparse BERTs và Sparse Mixer (SM). BERT-k biểu thị mô hình BERT với k lớp MoE. Các lần chạy "không ổn định" gặp phải gradient blow-up và không thể hội tụ đến loss tối ưu (hoặc hội tụ được). Chúng tôi sử dụng batch sizes 64 và 256. Metrics accuracy và tốc độ được báo cáo cho các lần chạy batch 64.

Ổn định Độ chính xác (%) Tốc độ
Mô hình 64 256 GLUE S.GLUE (ms/batch)
BERT 3/4 4/4 84.7 65.7 304
SM 4/4 4/4 85.0 66.4 184 (1.65x)
BERT-4 0/4 0/4 - - -
BERT-12 1/4 0/4 84.1 60.9 426 (0.71x)

Đánh đổi độ chính xác để có thêm tốc độ. Chúng tôi thiết kế một mô hình thưa hơn nữa bằng cách giảm capacity factor expert. Điều này giảm số lượng tokens mà mỗi expert xử lý và mang lại tăng tốc đáng kể với sự suy giảm chất lượng hạn chế: với sự giảm độ chính xác nhỏ (0.2%) trên SuperGLUE so với BERT, một Sparse Mixer với capacity factor 0.5 huấn luyện nhanh hơn 89% và chạy suy luận nhanh hơn 98%; xem Bảng 8. Chúng tôi đặt tên biến thể này của mô hình là Fast Sparse Mixer.¹⁰ Chúng tôi cũng thử nghiệm với việc giảm kích thước nhóm routing token, nhưng điều này dẫn đến sự giảm chất lượng lớn hơn.

Sự ổn định. Bảng 10 so sánh sự ổn định của Sparse Mixer, BERT và "sparse BERTs" – các biến thể MoE của BERT. Sparse Mixer rất ổn định, thậm chí so với BERT (dày đặc). Các sparse BERTs rất không ổn định, với chỉ một lần chạy ổn định cuối cùng tạo ra một mô hình chậm có hiệu suất đáng kể thấp hơn BERT.¹¹ Chúng tôi giả thuyết rằng sự ổn định cải thiện của Sparse Mixer là do việc thay thế hầu hết các sublayers self-attention bằng trộn, điều này hạn chế mô hình vào một cơ sở trộn ít biến thiên hơn.

6 Kết luận
Các phép biến đổi trộn và MoE kết hợp tốt với nhau. Sử dụng MoE cho khả năng và trộn cho tốc độ và sự ổn định, chúng tôi đã giới thiệu Sparse Mixer – một mô hình có hiệu suất hơi tốt hơn BERT trên GLUE và SuperGLUE, nhưng quan trọng hơn là huấn luyện nhanh hơn 65% và chạy suy luận nhanh hơn 61%. Chúng tôi cũng đã trình bày một biến thể nhanh hơn, Fast Sparse Mixer, có hiệu suất hơi thấp hơn BERT trên SuperGLUE, nhưng chạy gần như nhanh gấp đôi: huấn luyện nhanh hơn 89% và chạy suy luận nhanh hơn 98%. Sparse Mixer khắc phục nhiều vấn đề về tốc độ và sự ổn định của các mô hình MoE và mở ra triển vọng phục vụ các mô hình student thưa.

Hạn chế
Chỉ mô hình encoder. Chúng tôi đã tập trung công trình của mình vào các mô hình giống BERT vì chúng được sử dụng cực kỳ rộng rãi.¹² Tuy nhiên, điều này giới hạn sự tập trung của chúng tôi vào các encoders, không phù hợp cho các nhiệm vụ sinh tạo. Các mô hình encoder-decoder và decoder-only của sparse mixer, về nguyên tắc, là những mở rộng đơn giản: Các decoders Linear có thể được thiết kế bằng cách che "có tính nhân quả" ma trận Linear và encoder-decoder mixing cũng có thể được thiết kế với việc che cẩn thận. Tuy nhiên, chúng tôi nghi ngờ rằng các phần của chương trình coordinate descent sẽ cần được lặp lại. Ví dụ, bằng chứng cho thấy rằng cross-attention có thể quan trọng đối với hiệu suất của các mô hình encoder-decoder (You et al., 2020). Tuy nhiên, chúng tôi hy vọng rằng công thức Sparse Mixer hiện tại đóng vai trò là điểm khởi đầu và lộ trình để tổng quát hóa sang các kiến trúc khác.

Các nhiệm vụ và frameworks học tập đa dạng hơn. Chúng tôi chỉ đánh giá Sparse Mixer trên GLUE và SuperGLUE. Sẽ tốt khi nhìn vào một tập hợp rộng hơn các nhiệm vụ, bao gồm Q&A. Chúng tôi cũng bám sát cài đặt huấn luyện BERT gốc (Devlin et al., 2019), nhưng có những cải tiến regime huấn luyện tiềm năng có thể được giới thiệu, chẳng hạn như huấn luyện lâu hơn nhiều như RoBERTa (Liu et al., 2019b) hoặc sử dụng cài đặt huấn luyện generator-discriminator ELECTRA (Clark et al., 2020).

"Manual ML". Trong việc thiết kế kiến trúc Sparse Mixer, chúng tôi đã tối ưu hóa cấu hình mô hình một tọa độ siêu tham số tại một thời điểm. Vì vậy, trong khi gradient descent thủ công của chúng tôi cung cấp khả năng diễn giải và hiểu biết sư phạm, nó có thể không tối ưu. Sẽ thú vị khi thấy công trình tương lai mở rộng cả không gian tọa độ và tối ưu hóa cùng lúc nhiều tọa độ sử dụng Automated Machine Learning (AutoML) (Thornton et al., 2013; Liu et al., 2019a; Peng et al., 2020).

Chuỗi đầu vào dài. Do sự hiện diện của các lớp attention, Sparse Mixer sẽ không mở rộng quy mô tốt như các Transformers hiệu quả đối với các inputs chuỗi dài. Điều này có thể được bù đắp bằng cách thay thế bằng các xấp xỉ hiệu quả của cơ chế attention (Tay et al., 2021a).

Lời cảm ơn
Chúng tôi muốn gửi lời cảm ơn lớn tới Parker Schuh vì sự giúp đỡ quan trọng với việc triển khai Mixture-of-Experts. Chúng tôi cũng cảm ơn Santiago Ontañón vì nhiều phiên brainstorming hữu ích xung quanh việc thiết kế và đánh giá mô hình Sparse Mixer.

¹⁰ Sự giảm độ chính xác tối thiểu từ việc giảm capacity factor, cf, có thể được giảm thiểu bằng cách huấn luyện với cf=1 mặc định, và sau đó sử dụng cf nhỏ hơn trong quá trình suy luận, mặc dù sự không khớp có thể tạo ra kết quả không mong đợi.

¹¹ Có thể cải thiện sự ổn định của các sparse BERTs bằng cách điều chỉnh các cấu hình MoE, chẳng hạn như số lượng experts hoặc router z-loss. Điều đó nói rằng, như chi tiết trong Phần 4.3, Sparse Mixer đã mạnh mẽ dưới tất cả những thay đổi như vậy.

¹² Xem, ví dụ, https://huggingface.co/models.

--- TRANG 6 ---
Tài liệu tham khảo
[Danh sách tài liệu tham khảo dài với các trích dẫn khoa học từ Mikel Artetxe đến Barret Zoph, được format theo chuẩn học thuật]

--- TRANG 7 ---
[Tiếp tục danh sách tài liệu tham khảo từ Sainbayar Sukhbaatar đến Barret Zoph]

--- TRANG 8 ---
[Tiếp tục và kết thúc danh sách tài liệu tham khảo]

--- TRANG 9 ---
Linear Mixing
MLP Add & Normalize
Mixing Add & Normalize
Word Position Type Dense Output Projection
Input Output
N x

Hình 3: Kiến trúc encoder dựa trên khối. Mô hình có N khối encoder, mỗi khối chứa các sublayers mixing và MLP. Mỗi sublayer MLP có thể là thưa hoặc dày đặc. Mỗi sublayer mixing có thể sử dụng self-attention hoặc phép biến đổi mixing.

A Phụ lục
A.1 Kiến trúc cơ bản
Không gian thiết kế của chúng tôi cho Sparse Mixer được xây dựng từ các khối encoder xếp chồng của BERT (Devlin et al., 2019) trong Hình 3. Mỗi khối encoder chứa một sublayer mixing và một sublayer MLP, được kết nối với các kết nối dư và layer norms. Chúng tôi giữ các lớp embedding đầu vào và chiếu đầu ra tiêu chuẩn BERT (Devlin et al., 2019).

A.2 Khám phá thêm các tọa độ
Bố cục sublayer attention. Các sublayer attention 4 nên được đặt ở đâu trong mô hình? Chúng tôi kiểm tra xem tốt nhất là đặt 4 sublayers self-attention ở TOP (4 lớp cuối), BOTTOM (4 lớp đầu), MIDDLE (4 lớp giữa) hay MIXED (mỗi lớp thứ ba). Bảng 11 cho thấy bố cục TOP là tốt nhất.

Ngõ cụt mixing. Chúng tôi đã thử nghiệm hai sửa đổi mixing khác không mang lại lợi ích chất lượng (hoặc độ trễ): (1) thêm bias term vào các phép biến đổi mixing, và (2) thêm dropout trong quá trình fine-tuning vào các sublayers mixing.

Kích thước kích hoạt trung gian. Dưới dff = 2048, chất lượng mô hình giảm đáng kể. Chúng tôi chọn ngưỡng cắt này như kích thước MLP mô hình tối ưu của chúng tôi.

Số lượng lớp. Chúng tôi không thấy lợi ích chất lượng vượt quá 14 lớp. Vì chúng tôi dự định làm mỏng mô hình (giảm dff và dm), chúng tôi chọn tăng nhẹ số lượng lớp lên 14.

[Bảng 11, 12, 13, 14, 15 và các mô tả chi tiết về các thí nghiệm]

--- TRANG 10 ---
[Hình 4: Đồ thị tốc độ-độ chính xác NSP pre-training cho Sparse Mixer và BERT]

[Bảng 16: Kết quả GLUE đầy đủ cho tất cả các thí nghiệm coordinate descent]

--- TRANG 11 ---
[Bảng 17: Kết quả GLUE Validation cho so sánh cuối cùng]

[Bảng 18: Kết quả SuperGLUE Validation cho so sánh cuối cùng]

--- TRANG 12 ---
[Bảng 19: Tối ưu hóa giao thức fine-tuning cho các mô hình MoE]

[Bảng 20: Cấu hình mô hình cho các đồ thị tốc độ-độ chính xác]
