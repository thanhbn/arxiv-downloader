# 2310.07096.pdf
<<<<<<< Updated upstream
# Chuyển đổi từ PDF sang TXT
=======
# Được chuyển đổi từ PDF sang TXT
>>>>>>> Stashed changes
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/moe/2310.07096.pdf
# Kích thước file: 1167488 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Sparse Universal Transformer
Shawn Tan1 *
<<<<<<< Updated upstream
tanjings@mila.quebecYikang Shen2 *
yikang.shen@ibm.com
Zhenfang Chen2
zfchen@ibm.comAaron Courville1
courvila@iro.umontreal.caChuang Gan2
chuangg@ibm.com
1Mila, University of Montreal2MIT-IBM Watson AI Lab

Tóm tắt
Universal Transformer (UT) là một biến thể của Transformer chia sẻ tham số qua các lớp của nó. Bằng chứng thực nghiệm cho thấy UT có khả năng tổng quát hóa cấu trúc tốt hơn Vanilla Transformers (VT) trong các tác vụ ngôn ngữ hình thức. Việc chia sẻ tham số cũng mang lại hiệu quả tham số tốt hơn so với VT. Mặc dù có nhiều ưu điểm, việc mở rộng tham số UT tốn nhiều tính toán và bộ nhớ hơn nhiều so với việc mở rộng VT. Bài báo này đề xuất Sparse Universal Transformer (SUT), sử dụng Sparse Mixture of Experts (SMoE) và một cơ chế dừng động dựa trên stick-breaking mới để giảm độ phức tạp tính toán của UT trong khi vẫn duy trì hiệu quả tham số và khả năng tổng quát hóa. Các thí nghiệm cho thấy SUT đạt được hiệu suất tương đương với các mô hình baseline mạnh trong khi chỉ sử dụng một nửa tính toán và tham số trên WMT'14 và kết quả tổng quát hóa mạnh mẽ trên các tác vụ ngôn ngữ hình thức (Suy luận logic và CFQ). Cơ chế dừng mới cũng cho phép giảm khoảng 50% tính toán trong quá trình suy luận với rất ít giảm hiệu suất trên các tác vụ ngôn ngữ hình thức.

1 Giới thiệu
Các nghiên cứu lý thuyết gần đây đã chỉ ra rằng Transformers có độ sâu hữu hạn gặp vấn đề về khả năng biểu đạt sẽ dẫn đến thất bại trong việc tổng quát hóa (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022; Liu et al., 2022). Delétang et al. (2022) đã chạy nhiều kiến trúc neural trên một bộ các ngôn ngữ tổng hợp khác nhau được tạo ra từ các mức độ khác nhau của hệ thống phân cấp Chomsky và xác nhận thực nghiệm những kết quả này, cho thấy VT gặp khó khăn trong việc tổng quát hóa đối với ngôn ngữ Regular. Universal Transformers (UT; Dehghani et al. 2018) là các Transformers chia sẻ tham số ở mỗi lớp của kiến trúc. Csordás et al. (2021) đã thực hiện một số thí nghiệm tổng quát hóa cấu trúc trên VT và UT cùng với các embedding vị trí tuyệt đối và tương đối, và cho thấy UT với embedding vị trí tương đối hoạt động tốt hơn trong các tác vụ này.

Tuy nhiên, nhiệm vụ mở rộng UT là thách thức do độ phức tạp tính toán của nó (Kaplan et al., 2020; Tay et al., 2022; Takase và Kiyono, 2021). Xét một VT với P tham số cho mỗi lớp và L lớp. Đánh giá VT như vậy có độ phức tạp tính toán liên quan đến kích thước mô hình LP. Một UT tương đương về kích thước sẽ có một khối UT với LP tham số và độ phức tạp tính toán xấp xỉ LP để chạy khối một lần. Để chạy UT như vậy cho L lớp tương đương sẽ phải chịu độ phức tạp L²P. Độ phức tạp tính toán tăng này trực tiếp dẫn đến thời gian huấn luyện và suy luận tăng lên. Theo Takase và Kiyono (2021), UT yêu cầu gấp đôi thời gian huấn luyện và nhiều bộ nhớ GPU hơn so với VT trong tác vụ dịch thuật WMT English-German.

Các mạng neural được kích hoạt thưa thớt được giới thiệu để giảm độ phức tạp tính toán của các mô hình lớn. Những mạng này kích hoạt các phần của mạng có điều kiện dựa trên đầu vào, chỉ tính toán các phần của mô hình, do đó tách biệt số lượng tham số khỏi độ phức tạp tính toán. Phương pháp này cho phép tăng đáng kể số lượng tham số mà không tăng tỷ lệ thuận độ phức tạp tính toán. Shazeer et al. (2017) đã giới thiệu Sparse Mixture of Experts (SMoE), sử dụng toán tử top-k để cho phép tính toán thưa thớt của các experts. Điều này cho phép thay thế lớp FeedForward (FFD) trong Transformer bằng một tập hợp gồm E FFD, nhưng chỉ có k FFD (trong đó k < E) cần được đánh giá, có điều kiện dựa trên đầu vào. Zhang et al. (2022) sau đó đã giới thiệu Mixture of Attention heads (MoA), cho phép Transformers thay thế lớp Multihead Attention (MHA) của nó bằng một tập hợp gồm E attention heads và chỉ kích hoạt k heads có điều kiện dựa trên đầu vào, làm mô hình thưa thớt hơn nữa.

Bài báo này giới thiệu Sparse Universal Transformer (SUT), áp dụng các kỹ thuật tính toán thưa thớt nói trên cho UT. Ngoài ra, chúng tôi thay thế cơ chế dừng theo vị trí trong UT bằng một công thức stick-breaking mới có giải thích xác suất, cho phép chúng tôi giới thiệu một penalty Adaptive Computation Time (ACT; Graves 2016) để tối thiểu hóa việc sử dụng lớp. Nó cũng cung cấp một cách dễ dàng để điều chỉnh sự cân bằng giữa lượng tính toán và hiệu suất mô hình trong quá trình suy luận, cải thiện hơn nữa hiệu quả của SUT tại thời điểm suy luận.

Để chứng minh việc mở rộng hiệu quả, chúng tôi thực hiện các thí nghiệm trên dịch thuật WMT'14 English sang German, cho thấy SUT có thể đạt được hiệu suất tốt hơn với cùng số lượng tham số, trong khi phát sinh chi phí tính toán ít hơn so với UT dense tương đương. Vì thiết lập UT là trường hợp đặc biệt của SUT, chúng tôi cho thấy trên các tác vụ Compositional Freebase Questions (CFQ; Keysers et al. 2019) rằng UT có tính chất tổng quát hóa cấu trúc tốt hơn, cải thiện kết quả CFQ từ Csordás et al. (2021). Sử dụng tác vụ Logical Inference (Bowman et al., 2015), chúng tôi phân tích hành vi của UT trên việc tổng quát hóa độ dài và cấu trúc. Cuối cùng, chúng tôi cho thấy cơ chế dừng có thể được sử dụng để đạt được hiệu quả hơn nữa trong thời gian suy luận, và nghiên cứu sự cân bằng giữa hiệu quả và hiệu suất.

Hình 1: Một VT có các khối Transformer riêng biệt cho mỗi lớp, với các tham số khác nhau. Đối với UT có cùng số lượng tham số, khối UT sẽ có kích thước ~3 lần so với mỗi khối VT. Chạy khối này cho 3 lớp sau đó sẽ phát sinh khoảng 9 lần bộ nhớ thời gian chạy. Sử dụng SMoE có thể khôi phục xấp xỉ cùng chi phí tính toán như VT.

--- TRANG 2 ---

Hình 2: Ví dụ về các phân chia tổng quát hóa cấu trúc từ (Shen et al., 2019). Sự kết hợp của not và and không bao giờ được nhìn thấy trong kết hợp liên tiếp trong quá trình huấn luyện, và VT có thể học một lối tắt ngăn cản việc tổng quát hóa trong quá trình kiểm tra.

2 Bối cảnh & Công trình liên quan

Vượt qua các hạn chế của VT với UT Dziri et al. (2023) và Liu et al. (2022) phát hiện rằng Vanilla Transformers học các lối tắt cho các tác vụ đòi hỏi các thao tác cấu trúc nhiều bước, và thất bại trong việc tổng quát hóa trên các trường hợp lớn hơn của vấn đề đòi hỏi nhiều bước hơn. Các kết quả lý thuyết cũng cho thấy Vanilla Transformers có những hạn chế trong những gì chúng có thể tính toán hỗ trợ những phát hiện này (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022). Universal Transformers (Dehghani et al., 2018) là Transformers với trọng số được ràng buộc qua tất cả các lớp, và một cơ chế dừng bổ sung để quyết định khi nào dừng. Trong một kịch bản lý tưởng với vô hạn lớp (giờ đây tất cả các lớp có cùng tham số), UT, giống như Neural GPU (Kaiser và Sutskever, 2015), là Turing-complete, vượt qua nhiều vấn đề đã đề cập ở trên.

Trong thực tế, ngay cả với độ sâu hạn chế, UT đã thể hiện các tính chất mang lại cho chúng hiệu suất tốt hơn trong các tác vụ tổng quát hóa cấu trúc (Csordás et al., 2021). UT cho phép các thao tác được học trong Transformer trong quá trình huấn luyện là bất biến theo thứ tự độ sâu. Nếu một số thao tác trong quá trình huấn luyện được học để thực hiện theo một thứ tự nhất định, trong thời gian kiểm tra, UT có thể tổng quát hóa thành một thứ tự thao tác chưa từng thấy.

Thách thức với việc Mở rộng UT Mặc dù có những khả năng cấu trúc này, hiệu suất có xu hướng giảm trên các tác vụ thực tế khi sử dụng UT. ALBERT (Lan et al., 2019) đã cải thiện hiệu quả tham số bằng cách chia sẻ tham số qua các lớp. Điều này được thúc đẩy bởi một quan sát rằng Transformers có xu hướng học để thực hiện các thao tác tương tự trong các lớp, và việc chia sẻ các tham số này sẽ giảm sự dư thừa này. Tuy nhiên, các tác giả quan sát thấy sự giảm hiệu suất khi chia sẻ tham số, trái ngược với Dehghani et al. (2018).

--- TRANG 3 ---

Vấn đề có thể là về dung lượng mô hình? Các thí nghiệm với ALBERT cho thấy rằng việc mở rộng ALBERT có thể vượt trội hơn baseline BERT, ngay cả trên các tác vụ thực tế (Lan et al., 2019). Kaplan et al. (2020) cũng cho thấy rằng Transformer chia sẻ tham số có tính chất mở rộng tốt hơn về mặt tham số-hiệu suất, nhưng có tính chất kém hơn về mặt tính toán-hiệu suất, vì số lượng tham số khiến tính toán tăng lên. Tay et al. (2022) mở rộng các mô hình chuỗi khác nhau, và nhận xét về khó khăn trong việc mở rộng UT, hạn chế các thí nghiệm họ có thể thực hiện trên UT. Takase và Kiyono (2021) phác thảo một số chiến lược mở rộng Transformers chia sẻ tham số để đối phó với những vấn đề này bằng cách sử dụng các sơ đồ chia sẻ tham số khác nhau.

Các thí nghiệm của chúng tôi cho thấy rằng các kỹ thuật SMoE có thể được áp dụng thành công cho UT để mở rộng nó, đạt được hiệu quả tham số của UT mà không phải chịu cùng chi phí tính toán. Chúng tôi cũng thực hiện các thí nghiệm hỗ trợ các tuyên bố tổng quát hóa cấu trúc của công trình trước đây, và cung cấp các baseline tốt hơn cho những tác vụ đó.

3 Phương pháp

Giống như UT, chúng tôi tái sử dụng cùng một khối SUT cho mỗi lớp của Transformer. Trong mỗi khối SUT, chúng tôi sử dụng SMoE để đạt được sự thưa thớt cho mạng feed-forward (FFD) và các attention heads riêng biệt. Chúng tôi sử dụng loss Mutual Information Maximization được đề xuất trong Chen et al. (2022) và được sửa đổi cho các tác vụ không giám sát trong Shen et al. (2023). Cuối cùng, chúng tôi đề xuất một công thức quá trình stick-breaking cho việc dừng động, ảnh hưởng đến cách cơ chế attention hoạt động trong SUT, và loss phụ Adaptive Computation Time (ACT) chúng tôi sử dụng để tối thiểu hóa số lượng lớp được sử dụng.

3.1 Sparse Mixture of Experts

Một module Mixture of Experts bao gồm E sub-modules f₁, ..., fₑ. Cũng có một mạng gating, chúng tôi sẽ ký hiệu bởi g(e|h) - cho bất kỳ đầu vào h nào vào module MoE, mạng gating sẽ dự đoán một phân phối trên E experts. Khi k < E, chúng tôi gọi đây là Sparse Mixture of Experts (SMoE), và g(e|h) > 0 chỉ cho k experts, trong khi duy trì rằng ∑ᵢ₌₁ᴱ g(e|h) = 1.

Đầu ra cuối cùng của SMoE sau đó được đưa ra bởi y = ∑ᵢ₌₁ᴱ g(e|h)·fₑ(h), trong đó g(e|h) = 0, fₑ(h) sẽ không cần được đánh giá, giảm chi phí tính toán trong quá trình huấn luyện và suy luận.

Chúng tôi thay thế lớp Feed-forward (FFD) trong khối Transformer bằng một hỗn hợp các FFD. Mỗi Mixture of FFD có thể được mô tả bằng một 3-tuple, (E, k, D): E experts, k cho số lượng experts được sử dụng trong thao tác top-k, và D cho kích thước của lớp ẩn cho mỗi expert FFD. Đối với lớp attention, chúng tôi sử dụng Mixture of Multihead Attention (MoMHA) được đề xuất bởi Zhang et al. (2022) và Shen et al. (2023). Mỗi module MoMHA có thể được mô tả bằng một 5-tuple, (E, k, H, D, W), với E đại diện cho số lượng experts, k đại diện cho tham số k trong thao tác top-k, H đại diện cho số lượng attention heads trên mỗi expert, và D cho kích thước trên mỗi head. Giống như trong MoA, MoMHA chỉ duy trì một tập hợp duy nhất gồm H phép chiếu key-value được chia sẻ giữa các experts, trong khi có E phép chiếu query của H heads mỗi cái. W đại diện cho kích thước cửa sổ embedding vị trí tương đối, tham số hóa 2W + 1 embeddings cho W vị trí trước và sau vị trí hiện tại. Hình 3 (Trái) cho thấy sơ đồ của một khối SUT.

Kỹ thuật này đã được sử dụng để giảm chi phí tính toán cả trong thời gian huấn luyện và suy luận cho các mô hình lớn.

Tối đa hóa Thông tin Tương hỗ Giống như các mô hình khác dựa trên kích hoạt có điều kiện, các loss phụ cần thiết để hỗ trợ việc học một module quyết định experts nào được kích hoạt, và để đảm bảo rằng tất cả experts được sử dụng, cân bằng tải cho việc xử lý. Đối với điều này, chúng tôi sử dụng Mutual Information Maximization được giới thiệu trong Chen et al. (2022) cho loss phụ (để được tối đa hóa):

L_MIM = ∑ᵢ₌₁ᴱ g(e) log g(e) - 1/|X| ∑ₕ∈X ∑ᵢ₌₁ᴱ g(e|h) log g(e|h), (1)

trong đó,
g(e) = 1/|X| ∑ₕ∈X g(e|h)

--- TRANG 4 ---

MLP 1 Router MLP 2 MLP 3 MLP 4
Layer Norm Layer Norm Att 1 Router Att 2 Att 3 Att 4 Halting

Hình 3: Trái: Sơ đồ của một khối SUT. Phải: Trong khi đầu vào của mỗi khối SUT là đầu ra của lớp trước, cơ chế attention chú ý đến trạng thái đã dừng của timestep. Khi xác suất dừng vượt quá α_thresh, trạng thái ẩn được sao chép đơn giản. Cuối cùng, trạng thái đã dừng được sử dụng làm đầu ra của SUT.

Thuật toán 1: Cơ chế dừng tại một timestep t cho trước
for l = 1 to L do
    if ∑ˡ⁻¹ₗ'=1 α⁽ᵗ⁾ₗ' < α_thresh then
        α̂⁽ᵗ⁾ₗ₋₁ = halt(h⁽ᵗ⁾ₗ₋₁)
        α⁽ᵗ⁾ₗ₋₁ = α̂⁽ᵗ⁾ₗ₋₁ ∏ˡ⁻²ₗ'=1(1-α̂⁽ᵗ⁾ₗ')
        a⁽ᵗ⁾ₗ = Attention(h⁽ᵗ⁾ₗ₋₁, Sₗ₋₁, Sₗ₋₁)
        h⁽ᵗ⁾ₗ = FeedForward(h⁽ᵗ⁾ₗ₋₁, a⁽ᵗ⁾ₗ)
        s⁽ᵗ⁾ₗ = (1-∑ˡ⁻¹ₗ'=1 α⁽ᵗ⁾ₗ') · h⁽ᵗ⁾ₗ + ∑ˡ⁻¹ₗ'=1 α⁽ᵗ⁾ₗ' · h⁽ᵗ⁾ₗ'
    else
        h⁽ᵗ⁾ₗ = h⁽ᵗ⁾ₗ₋₁
        s⁽ᵗ⁾ₗ = s⁽ᵗ⁾ₗ₋₁
    end if
end for

Cụ thể, chúng tôi sử dụng phiên bản không giám sát được đề xuất bởi Shen et al. (2023) giả định một phân phối đồng nhất trên tất cả các token và lớp, dẫn đến mục tiêu phụ sau. Trong thiết lập SUT, mạng gating được sử dụng |X| = L·T lần, trong đó L là số lượng lớp, và T là số lượng timesteps.

Theo trực giác, hạng mục entropy làm tăng entropy của xác suất biên của các dự đoán mạng gating, mà tại mức tối đa có nghĩa là trọng số cho mỗi mạng gating trên toàn bộ minibatch là đồng nhất. Hạng mục entropy có điều kiện làm giảm entropy có điều kiện, điều này khiến dự đoán của mạng gating trở nên sắc nét, và cũng phạt giải pháp phân phối đồng nhất cho mạng gating.

3.2 Stick-breaking Dynamic Halting

Đã có một số phương pháp để trang bị cho các mô hình khả năng đưa ra dự đoán mà không cần phải sử dụng tất cả các lớp của mô hình (Graves, 2016; Tan và Sim, 2016; Dehghani et al., 2018; Elbayad et al., 2019; Schuster et al., 2022). Động lực cho điều này bao gồm: (1) các đầu vào khác nhau yêu cầu lượng lặp lại khác nhau để đưa ra dự đoán, (2) giảm chi phí tính toán.

UT thực hiện một cơ chế tương tự, nhưng phiên bản dừng của UT khó diễn giải. Ở đây chúng tôi chọn một phiên bản nguyên tắc của cơ chế dừng động dựa trên quá trình stick-breaking, xem nó như một phân phối xác suất. Đầu tiên, α̂⁽ᵗ⁾ₗ là các xác suất dừng được dự đoán bởi halt(h⁽ᵗ⁾ₗ), một hàm được thực hiện bởi một MLP nhận embedding của lớp trước. Sau đó, xác suất của bất kỳ lớp nào dừng được tính bởi α⁽ᵗ⁾ₗ = α̂⁽ᵗ⁾ₗ ∏ˡ⁻¹ₗ'=1(1-α̂⁽ᵗ⁾ₗ'). (2)

Một công thức tương tự được mô tả trong Graves (2016) và Tan và Sim (2016). Thuật toán 1 cho thấy cách cơ chế được thực hiện tại bất kỳ timestep nào. hₗ₋₁ là đầu ra của lớp trước cho timestep hiện tại.

Có điều kiện trên thực tế là chúng ta đang tính toán h⁽ᵗ⁾ₗ, timestep t không được dừng trước hoặc tại l-1. Vì vậy chúng ta có thể sử dụng h⁽ᵗ⁾ₗ, trạng thái chưa dừng, làm đầu vào cho việc tính toán query attention của khối. Tuy nhiên, vì timestep t có thể chú ý đến tất cả các timestep khác, và các bước khác này có thể đã dừng, chúng ta sử dụng các trạng thái đã dừng Sₗ₋₁ cho các lớp trước.

Tuy nhiên, vì việc dừng là một quyết định 'mềm', chúng ta có thể nới lỏng yêu cầu đánh giá tất cả các trạng thái dừng có thể và sử dụng trạng thái dừng mong đợi làm thay thế. Các cơ chế dừng trước đây sử dụng một cơ chế 'gating' của tổng lồi giữa các đầu ra đã được gated trước đó và đầu ra của bước hiện tại hₗ = αₗ·ĥₗ + (1-αₗ)·hₗ₋₁ (Dehghani et al., 2018). Điều này có thể dẫn đến gradient nhỏ một cách biến mất lên các lớp khi (1-αₗ) nhân lên. Thay vào đó, chúng ta có thể tính toán embedding dừng mong đợi tại bất kỳ l nào,

s⁽ᵗ⁾ₗ = (1-∑ˡ⁻¹ₗ'=1 α⁽ᵗ⁾ₗ') · h⁽ᵗ⁾ₗ + ∑ˡ⁻¹ₗ'=1 α⁽ᵗ⁾ₗ' h⁽ᵗ⁾ₗ' (3)

Nếu α⁽ᵗ⁾ₗ = 1 cho một số l, s⁽ᵗ⁾ₗ = h⁽ᵗ⁾ₗ, khôi phục hành vi của quyết định dừng rời rạc. Chúng ta sử dụng s⁽ᵗ⁾ₗ làm đầu vào cho các biến đổi key và value của attention.

Giải thích xác suất này cũng cho phép chúng ta áp đặt một loss trên số lượng lớp mong đợi được sử dụng tại mỗi bước, thiên vị mô hình về phía ít lặp lại hơn, do đó tiết kiệm chi phí tính toán.

L_ACT = 1/T ∑ᵀₜ₌₁ ∑ᴸₗ₌₁ α⁽ᵗ⁾ₗ · l. (4)

Chúng tôi sử dụng một ngưỡng α_thresh = 0.999, sao cho tổng tích lũy của các xác suất dừng đã vượt quá điều này, không có tính toán nào sẽ được thực hiện cho timestep đó, và các embedding của lớp trước sẽ được sao chép. Do thao tác routing được yêu cầu trong việc thực hiện SMoE, chúng ta có thể đơn giản route các trạng thái đã dừng đến một expert "No Op", dẫn đến tiết kiệm thực sự trong chi phí tính toán khi việc dừng đạt ngưỡng sớm. Chúng tôi thấy rằng việc điều chỉnh ngưỡng này sau khi huấn luyện có thể duy trì hiệu suất trong khi tiết kiệm các bước tính toán.

4 Thí nghiệm

Đầu tiên, chúng tôi cho thấy rằng chúng ta có thể mở rộng UT với SUT trên tác vụ dịch thuật WMT'14 English-German (Bojar et al., 2014). Sau đó chúng tôi đã chạy các thí nghiệm trên Compositional Freebase Questions (CFQ; Keysers et al. 2019) để kiểm tra các tính chất tổng quát hóa cấu trúc. Để phân tích thêm hành vi của mô hình trong các thiết lập tổng quát hóa cấu trúc, chúng tôi kiểm tra mô hình của chúng tôi trên tác vụ suy luận Logic từ (Bowman et al., 2015). Tất cả các thí nghiệm đều được thực hiện trong khuôn khổ Fairseq (Ott et al., 2019).

4.1 Dịch thuật English sang German

Chúng tôi thực hiện các thí nghiệm trên bộ dữ liệu dịch thuật WMT'14 English-German (Bojar et al., 2014). Chúng tôi sử dụng tiền xử lý từ Liu et al. (2020). Chúng tôi sử dụng từ điển được kết hợp và chia sẻ tất cả các word embeddings của encoder và decoder. Để đánh giá, chúng tôi lấy trung bình của 5 mô hình tốt nhất cuối cùng theo điểm negative log-likelihood của chúng. Chúng tôi báo cáo điểm BLEU (Papineni et al., 2002), và cũng báo cáo MACs (Multiply-Accumulate Operations) để đánh giá chi phí tính toán runtime của các mô hình khác nhau. MACs của các mô hình trước được tính toán trong Zhang et al. (2022).

Kết quả được báo cáo trong Bảng 1. Chúng tôi so sánh với các baseline mạnh trong khi tính đến số lượng tham số trong những mô hình này. Ngoài ra, chúng tôi huấn luyện hai UT bằng cách đặt E = 1, k = 1, và tham số hóa các lớp FFD và Attention với tham số để phù hợp với thiết lập ~65M và ~110M của chúng tôi cho SUT. Cả SUT và UT đều thể hiện hiệu quả tham số tốt khi so sánh với các mô hình trước đây. Trong lớp tham số ~110M, SUT và UT hoạt động ở khoảng 29.4 và 29.6 BLEU tương ứng, trong khi các mô hình trước đây yêu cầu ~200M tham số. Trong khi SUT không hoạt động tốt bằng UT, nhưng các tính toán cần thiết trong thời gian chạy có thể thấp đến một phần năm của UT. Ngoài ra, vì chúng tôi giữ k không đổi cho SUT, MACs vẫn không đổi khi SUT mở rộng.

Chúng tôi đã chạy các thí nghiệm loại bỏ các khía cạnh khác nhau của mô hình và quá trình huấn luyện của nó, bao gồm: loss phụ MIM, Mixture of MHA, loss ACT, và cơ chế dừng. Kết quả trong Bảng 2. Việc giới thiệu nhiều heads cho MoA là quan trọng trong việc nhìn thấy tăng hiệu suất trong tác vụ này, cũng như có loss MIM làm mục tiêu phụ cân bằng tải. Thú vị là, việc dừng đóng góp nhiều tăng hiệu suất như nó làm trong CFQ.

Bảng 1: Điểm BLEU trên bộ dữ liệu dịch thuật WMT14 En-De. MACs (Multiply–Accumulate Operations) đo độ phức tạp tính toán của mỗi mô hình.

[Bảng với các kết quả BLEU, tham số và MACs cho các mô hình khác nhau]

Bảng 2: Nghiên cứu Ablation. "– MIM loss" có nghĩa là thay thế loss MIM bằng loss cân bằng tải được sử dụng trong (Fedus et al., 2021). "– MoMHA" có nghĩa là thay thế MoMHA bằng MoA được giới thiệu trong (Zhang et al., 2022).

[Bảng với kết quả ablation study]

--- TRANG 5 ---

Bảng 3: Sự đồng xuất hiện Expert-Word của FFD.

[Bảng cho thấy sự liên kết giữa các experts và từ]

Ngoài ra, chúng tôi tính toán top 5 tokens xuất hiện cùng với mỗi expert, bất kể lớp nào, và thấy rằng có những liên kết nhất định tồn tại. Chúng tôi chọn một số experts trong Bảng 3 cho thấy dấu hiệu rõ ràng của việc đồng xuất hiện với các tokens dường như cho thấy một mẫu. Điều này gợi ý rằng trong khi có thể có sự dư thừa giữa các experts, các nhóm experts có thể chuyên môn hóa trong các tác vụ nhất định, dẫn đến một số tính mô-đun. Công việc tương lai có thể điều tra xem liệu tính mô-đun như vậy có thể dẫn đến tổng quát hóa mạnh mẽ hơn.

4.2 Compositional Freebase Questions

Chúng tôi chạy các thí nghiệm trên bộ dữ liệu Compositional Freebase Questions (CFQ; Keysers et al. 2019) để xác định khả năng tổng quát hóa cấu trúc của SUT. Đây là một tác vụ dịch thuật từ ngôn ngữ tự nhiên sang truy vấn SPARQL. Như một ví dụ, chuỗi "Who wrote M1 and wrote a film" sẽ được dịch thành chuỗi đích "SELECT DISTINCT ?x0 WHERE { ?x0 a people.person . ?x0 film.writer.film ?x1 M1 . ?x1 a film.film }". CFQ kiểm tra tổng quát hóa cấu trúc sử dụng khái niệm compound divergence, đo lường sự khác biệt giữa tập huấn luyện và tập kiểm tra về mặt kết hợp của tokens, mà họ gọi là compounds. Theo hiểu biết của chúng tôi, các mô hình có hiệu suất tốt nhất hiện tại hoặc fine-tune một mô hình ngôn ngữ được huấn luyện trước hoặc, sử dụng kiến thức về tác vụ để thiết kế một prompt phù hợp cho một mô hình ngôn ngữ lớn (Drozdov et al., 2022). Trong khi cách tiếp cận prompting cực kỳ hiệu quả tại tác vụ CFQ, chúng tôi xem tác vụ này như một benchmark cho tổng quát hóa cấu trúc nói chung và nên được xem cùng với các thí nghiệm khác, đặc biệt là dữ liệu thực tế (như dịch thuật). Khi sử dụng kiến thức miền của tác vụ trong prompt, kết quả có thể chỉ ra hiệu suất tốt hơn với một cách tiếp cận cụ thể cho CFQ (và có lẽ các tác vụ dịch SQL khác) nhưng có thể khó ngoại suy sang các thiết lập khác.

Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng các script tiền xử lý từ Zheng và Lapata (2021). Các script thực hiện tiền xử lý cho chuỗi đích đơn giản hóa chuỗi đích theo cách được thực hiện trong Furrer et al. (2020). Tương ứng, chúng tôi huấn luyện một baseline Transformer trên đích đã biến đổi. Chúng tôi đã thực hiện tìm kiếm trên các siêu tham số SUT, sử dụng tập validation MCD1, và tập tham số hoạt động tốt nhất là Attention (E = 1, k = 1, H = 8, D = 64, W = 1) và FFD (E = 1, k = 1, D = 1024), tương ứng với thiết lập UT. Tham khảo Phụ lục A để biết thêm chi tiết. Vì CFQ là một tác vụ tương đối nhỏ, quy mô lớn hơn không phải là một yếu tố và có thể gợi ý rằng chuyên môn hóa expert có thể không hữu ích bằng. Kết quả được hiển thị trong Bảng 4. Trong các trường hợp có và không có việc dừng, mô hình đã vượt trội hơn các benchmark trước đây, bao gồm baseline UT từ Bergen et al. (2021). Để so sánh công bằng hơn, chúng tôi sử dụng cùng siêu tham số như triển khai UT của chúng tôi, chúng tôi sửa đổi triển khai UT của chúng tôi để tương tự hơn với UT dựa trên T5 trong Bergen et al. (2021). Những thay đổi này bao gồm: bias vị trí tương đối bucketed được sử dụng bởi T5, và chuyển từ layer-norm sau sang layer-norm trước. Trong khi điều này dẫn đến kết quả cải thiện nhiều so với bài báo gốc, triển khai UT của chúng tôi vẫn vượt trội hơn nó.

Mô hình Dangle (Zheng và Lapata, 2021), vượt trội hơn mô hình của chúng tôi, cũng yêu cầu chạy lại encoder cho mỗi token được giải mã. Đây là một quá trình đắt đỏ, nhưng với việc cả phương pháp của chúng tôi và Dangle đều hoạt động tốt trong tác vụ này, là bằng chứng bổ sung rằng các quá trình lặp lại có lợi cho tổng quát hóa cấu trúc.

--- TRANG 6 ---

Bảng 4: Kết quả CFQ. Kết quả trên UT là trung bình của 5 lần chạy trên các seeds khác nhau.

[Bảng với kết quả CFQ cho các mô hình khác nhau]

4.3 Suy luận Logic

Chúng tôi sử dụng tác vụ suy luận logic từ (Bowman et al., 2015) như một bàn kiểm tra cho UT. Mặc dù sự đơn giản rõ ràng của ngôn ngữ, tác vụ này vốn đòi hỏi mô hình phải học cấu trúc phân cấp của vấn đề. Mỗi trường hợp của tác vụ bao gồm hai câu logic, và mục tiêu là dự đoán liệu các câu có tương đương, mâu thuẫn, rời rạc, hay kéo theo theo một trong hai hướng. Ví dụ, cho s₁ = a và s₂ = a (hoặc b), thì s₁ ⊏ s₂. Trọng tâm của tác vụ là huấn luyện mô hình trên các chuỗi có 0-6 toán tử logic và đánh giá nó trên các chuỗi có 7-12 toán tử. Với thiết lập sequence-to-sequence của chúng tôi, chúng tôi chuyển đổi tác vụ thành một tác vụ dịch thuật. Mô hình nhận sentence1 #SEP# sentence2 làm câu nguồn, với câu đích là nhãn token đơn cho cặp đó.

Chúng tôi huấn luyện một mô hình 12 lớp với Attention (E = 12, k = 4, H = 2, D = 32, W = 1) và FFD (E = 12, K = 4, D = 128) và việc dừng. Tham khảo Phụ lục A để biết thêm chi tiết. Huấn luyện một Vanilla Transformer 12 lớp đạt được kết quả xấp xỉ như trong Shen et al. (2019), vì vậy chúng tôi báo cáo kết quả của họ. Kết quả của chúng tôi trong Bảng 5 xác nhận những phát hiện của Tran et al. (2018), cho thấy rằng với sự lặp lại trong SUT, chúng ta có thể tổng quát hóa đến các chuỗi dài hơn của tác vụ. Trong khi có các mô hình khác gây ra cấu trúc cây hoạt động cực kỳ tốt trên tác vụ, chúng tôi muốn đánh giá mô hình của chúng tôi so với các kiến trúc phổ biến khác. LSTM là một baseline mạnh, và chúng tôi thấy rằng UT vượt trội hơn nó trong tổng quát hóa. Chúng tôi cũng đánh giá UT trên các phân chia tổng quát hóa cấu trúc như được đề xuất trong (Shen et al., 2019), trong đó các phân chia A, B, và C có độ khó tăng dần. Kết quả cho thấy UT có thể tổng quát hóa tốt hơn cho các phân chia A và B, vượt trội hơn LSTM và VT. Phân chia C vẫn là thách thức đối với các biến thể Transformer.

Bảng 5: Độ chính xác kiểm tra của các mô hình, được huấn luyện trên độ dài thao tác ≤6, với kết quả ngoài phân phối của chúng được hiển thị ở đây (độ dài 7-12). Baseline LSTM từ Bowman et al. (2015), và baseline Transformer từ Shen et al. (2019)

[Bảng với kết quả độ chính xác cho các độ dài thao tác khác nhau]

Ngoài ra, chúng tôi tính toán độ sâu dừng trung bình cho dữ liệu kiểm tra được phân đoạn theo số lượng toán tử. Vì nhiều toán tử hơn yêu cầu nhiều lồng ghép biểu thức hơn trong các chuỗi, cần nhiều đệ quy hơn để phân tích chuỗi đúng cách. Như mong đợi, trong Hình 4, độ sâu dừng trung bình tăng khi nhiều toán tử được sử dụng. Số lượng toán tử cho các mệnh đề này tương quan với độ dài, cho thấy rằng SUT có thể phù hợp để tổng quát hóa cho độ dài. Chúng tôi bao gồm các thí nghiệm thêm về tổng quát hóa độ dài trong Phụ lục Bảng 8.

Hình 4: Độ sâu dừng động trung bình của mô hình UT khi số lượng toán tử tăng trong tập kiểm tra. Mô hình học cách suy nghĩ nhiều hơn khi vấn đề khó hơn.

4.4 Giảm Tính toán Sau Huấn luyện

Việc giảm α_thresh sau khi huấn luyện có khiến mô hình dừng sớm hơn, tiết kiệm tính toán không? Điều đó sẽ tốn cho chúng ta bao nhiêu về mặt độ chính xác?

Chúng tôi ước tính các tính toán khối SUT được bỏ qua với các giá trị khác nhau của α_thresh ∈ {0.1, 0.2, ..., 0.9} bằng cách xem xét các mẫu dừng của decoder với các cặp nguồn-đích ground truth. Chúng tôi chuyển cặp nguồn-đích vào mô hình và phân tích các mẫu dừng của mô hình, cho chúng tôi ước tính thô về lượng tính toán sẽ được tiết kiệm dưới dạng phần trăm của việc tính toán tất cả các lớp của SUT.

Hình 5: Trên: Biểu đồ của 1-∑ˡ⁻¹ₗ'=1 α⁽ᵗ⁾ₗ', cho một ví dụ đầu vào Logical Inference — trục x: timesteps, trục y: lớp. Điều này hiển thị mẫu dừng của mô hình: xanh đậm đại diện cho đã dừng, trong khi vàng đại diện cho đang hoạt động. Dưới: Đường cong cân bằng Hiệu quả vs Hiệu suất khi α_thresh được điều chỉnh.

Suy luận Logic Chúng tôi quan sát hiệu suất kết quả trên phân chia khó nhất của tập kiểm tra với 12 thao tác. Do mẫu dừng đã bão hòa, xác suất dừng αₗ tăng đột ngột từ gần 0 đến các giá trị cao hơn, dẫn đến giảm gần như không đổi ~50% thời gian tính toán bất kể ngưỡng.

CFQ Sử dụng phân chia kiểm tra MCD1 của bộ dữ liệu, và mô hình hoạt động tốt nhất của chúng tôi trên MCD1, chúng tôi thực hiện điều chỉnh α_thresh. Các mẫu dừng phản ánh cấu trúc lặp lại của SQL, sử dụng ít bước hơn cho '.' và 'WHERE', trong khi phần chính của vùng trong {...} yêu cầu nhiều bước SUT hơn trước khi dừng. Đáng ngạc nhiên, khi 0.8 ≤ α_thresh ≤ 0.999, độ chính xác vẫn khá không đổi. Ước tính 33% các bước tính toán đã được bỏ qua tại α_thresh = 0.8. Tại α_thresh = 0.1, có sự gia tăng nhỏ trong số bước tính toán, điều này có thể xảy ra vì việc dừng sớm hơn sẽ dẫn đến các embedding khác nhau, và dẫn đến các quyết định dừng khác nhau trong các timestep khác. Nhìn chung, kết quả gợi ý rằng chúng ta có thể tiết kiệm khoảng 20% các bước tính toán SUT mà không có bất kỳ sự giảm độ chính xác nào, và khoảng ~50% cho sự giảm 0.2%.

Hình 6: Biểu đồ dừng và đường cong cân bằng cho CFQ. (Xem Hình 5 để mô tả)

Dịch thuật English-German Đối với bộ dữ liệu lớn hơn này, chúng tôi thấy rằng những mô hình dịch thuật này dừng muộn hơn nhiều, gợi ý rằng tác vụ dịch thuật yêu cầu nhiều bước tính toán hơn so với SUT 6 lớp chúng tôi đã sử dụng. Tuy nhiên, việc tăng thêm số lượng lớp lên 12 lớp không mang lại nhiều lợi ích, như được chứng minh bởi việc dừng trong Hình 4, đó là một ví dụ về cơ chế dừng sử dụng gần như tất cả các lớp. Để so sánh, mô hình Admin 60L-12L, yêu cầu encoder 60 lớp để đạt được hiệu suất của nó. Ngay cả khi α_thresh = 1, các bước tính toán bỏ qua vẫn ở khoảng 33%, so với 80% trong tác vụ CFQ. Chúng tôi thấy rằng chúng ta có thể giảm tính toán 9% trong khi vẫn duy trì điểm BLEU là 29.1.

Hình 7: Biểu đồ dừng và đường cong cân bằng cho Dịch thuật English-German. (Xem Hình 5 để mô tả)

5 Kết luận

Chúng tôi cho thấy rằng có thể mở rộng UT thông qua SUT, và SUT vượt trội hơn các mô hình cùng dung lượng trong tác vụ dịch thuật WMT'14 English-to-German. Bản chất đệ quy của cả UT và SUT cho phép các inductive bias tốt hơn, điều mà chúng tôi đã chứng minh trong các tác vụ tổng hợp như CFQ và suy luận logic. VT đã được chứng minh là kém trong những tác vụ tổng quát hóa cấu trúc này mà không có kiến thức miền bổ sung. Cơ chế dừng động stick-breaking cũng cho phép điều chỉnh chi phí tính toán sau huấn luyện, là một lợi ích cho việc triển khai ở quy mô lớn.

Hạn chế Trong khi các thí nghiệm trong bài báo này cho thấy các tính chất tổng quát hóa mong muốn của UT, có một số khía cạnh của tổng quát hóa cấu trúc mà SUT không giải quyết. Quan trọng, trong khi chúng tôi chứng minh việc mở rộng UT thông qua SMoE, cần có thêm các thí nghiệm trên các thiết lập lớn hơn để xác định tính khả thi trong các hệ thống quy mô lớn. Các vấn đề khác cũng có thể xuất hiện trong việc mở rộng thêm SUT, nhưng chúng tôi tin rằng có đủ tài liệu để dựa vào để tìm giải pháp cho những vấn đề này.

--- TRANG 7 ---

[Các trang còn lại chứa danh sách tài liệu tham khảo và phụ lục với các chi tiết thí nghiệm bổ sung]

Tài liệu tham khảo

[Danh sách đầy đủ các tài liệu tham khảo với tác giả, tiêu đề, và thông tin xuất bản]

A Chi tiết Thí nghiệm

Tất cả các mô hình của chúng tôi được huấn luyện trên GPU V100. Chúng tôi sử dụng Adam Optimizer với β₁ = 0.9, β₂ = 0.98 và ε = 1e⁻⁹. Chúng tôi sử dụng bộ lập lịch tốc độ học căn bậc hai nghịch đảo. Trong quá trình huấn luyện, chúng tôi sử dụng làm mịn nhãn (Szegedy et al., 2016) với giá trị 0.1. Thêm các siêu tham số huấn luyện có thể tìm thấy trong Bảng 7.

B Kết quả Thí nghiệm Bổ sung

B.1 Suy luận Logic

Chuyên môn hóa Expert chúng ta có thể kiểm tra tần suất routing đến mỗi expert liên quan đến token đầu vào. Chúng tôi tính toán thông tin tương hỗ điểm (PMI; Church và Hanks 1990) của mỗi từ đến mỗi expert ở tất cả các lớp. Nhìn vào các expert MoMHA, chúng ta thấy một số mẫu gợi ý các expert đã chuyên môn hóa trong các nhiệm vụ của chúng. Ví dụ, toán tử or đang thống trị expert 9. Thú vị, toán tử not phụ thuộc vào experts 4 và 5, không giống như các toán tử or và and, là nhị phân và phụ thuộc vào experts 1 và 2. Các biến a đến f dường như cũng có các mẫu sử dụng expert tương tự. Điều này gợi ý rằng các experts chuyên môn hóa trong thao tác của chúng.

Hình 8: Ái lực Expert-token cho tác vụ suy luận logic. Chúng tôi vẽ điểm PMI giữa các tokens (trục y) và các experts (trục x). Nhìn vào các expert MoMHA, chúng ta thấy một số mẫu gợi ý các expert đã chuyên môn hóa trong các nhiệm vụ của chúng.

B.2 Long Range Arena ListOps

Bảng 8: Kết quả trên benchmark Long-range Arena ListOps. Chúng tôi bao gồm Transformer và top 2 kết quả từ Tay et al. (2020) để ngắn gọn, và kết quả bên ngoài hoạt động tốt nhất trên bảng xếp hạng tại https://github.com/google-research/long-range-arena

[Bảng với kết quả độ chính xác]

B.3 Tổng quát hóa SCAN & PCFG

Trong tất cả các trường hợp, chúng tôi chạy một mô hình Vanilla Transformer (VT) baseline sử dụng mô hình IWSLT Transformer từ fairseq, và so sánh với một baseline VT liên quan từ tài liệu. Chúng tôi không thể tái tạo kết quả add_turn_left trong Furrer et al. (2020) trên SCAN. Chúng tôi cũng thấy rằng kết quả VT của chúng tôi trên các phân chia PCFG cao hơn một chút.

Những kết quả này cho thấy rằng có những sắc thái đối với tổng quát hóa cấu trúc. Trong khi SUT có thể mang lại lợi ích nhất quán trong trường hợp PCFG, các khía cạnh tổng quát hóa khó hơn của add_jump và độ dài trong SCAN có thể yêu cầu một loại tổng quát hóa khác. Theo hiểu biết của chúng tôi, có công việc đối phó rõ ràng với add_jump (Russin et al., 2019; Li et al., 2019) và những công việc khác đối phó với phân chia độ dài bằng cách giải mã trong cấu trúc cây (Tan et al., 2020; Kim, 2021). SUT giải quyết một số khía cạnh này của tổng quát hóa cấu trúc, nhưng không phải tất cả, và điều này minh họa thêm việc tổng quát hóa mà SUT có thể hỗ trợ.

Bảng 9: Kết quả thí nghiệm cho Vanilla Transformers (VT) và Universal Transformer (SUT với 1 expert). Kết quả là độ chính xác khớp chuỗi đầy đủ.

[Bảng với kết quả SCAN và PCFG]

[Các bảng và hình ảnh bổ sung với thông tin siêu tham số và kết quả thí nghiệm chi tiết]
=======
tanjings@mila.quebec
Yikang Shen2 *
yikang.shen@ibm.com
Zhenfang Chen2
zfchen@ibm.com
Aaron Courville1
courvila@iro.umontreal.ca
Chuang Gan2
chuangg@ibm.com
1Mila, University of Montreal
2MIT-IBM Watson AI Lab

Tóm tắt
Universal Transformer (UT) là một biến thể của Transformer chia sẻ tham số qua các lớp của nó. Bằng chứng thực nghiệm cho thấy UT có khả năng khái quát hóa tổng hợp tốt hơn so với Vanilla Transformers (VTs) trong các tác vụ ngôn ngữ hình thức. Việc chia sẻ tham số cũng mang lại hiệu quả tham số tốt hơn so với VTs. Mặc dù có nhiều ưu điểm, việc mở rộng tham số UT tốn nhiều tài nguyên tính toán và bộ nhớ hơn nhiều so với mở rộng VT. Bài báo này đề xuất Sparse Universal Transformer (SUT), tận dụng Sparse Mixture of Experts (SMoE) và một cơ chế dừng động mới dựa trên stick-breaking để giảm độ phức tạp tính toán của UT trong khi vẫn giữ được hiệu quả tham số và khả năng khái quát hóa. Các thí nghiệm cho thấy SUT đạt được hiệu suất tương đương với các mô hình cơ sở mạnh mẽ trong khi chỉ sử dụng một nửa tính toán và tham số trên WMT'14 và kết quả khái quát hóa mạnh mẽ trên các tác vụ ngôn ngữ hình thức (Suy luận logic và CFQ). Cơ chế dừng mới cũng cho phép giảm khoảng 50% tính toán trong quá trình suy luận với sự giảm hiệu suất rất ít trên các tác vụ ngôn ngữ hình thức.

1 Giới thiệu
Các công trình lý thuyết gần đây đã chỉ ra rằng các Transformer có độ sâu hữu hạn có vấn đề về khả năng biểu đạt sẽ dẫn đến thất bại trong việc khái quát hóa (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022; Liu et al., 2022). Delétang et al. (2022) đã chạy một số kiến trúc mạng nơ-ron trên một bộ các ngôn ngữ tổng hợp khác nhau được tạo ra từ các mức độ khác nhau của thứ bậc Chomsky và xác nhận thực nghiệm những kết quả này, cho thấy VTs gặp khó khăn trong việc khái quát hóa sang các ngôn ngữ Chính quy. Universal Transformers (UTs; Dehghani et al. 2018) là các Transformer chia sẻ tham số ở mọi lớp của kiến trúc. Csordás et al. (2021) đã thực hiện một số thí nghiệm khái quát hóa tổng hợp trên VTs và UTs cùng với các embedding vị trí tuyệt đối và tương đối, và cho thấy UTs với embedding vị trí tương đối hoạt động tốt hơn trong các tác vụ này.

Tuy nhiên, việc mở rộng UTs là một thách thức do độ phức tạp tính toán của nó (Kaplan et al., 2020; Tay et al., 2022; Takase và Kiyono, 2021). Xem xét một VT với P tham số cho mỗi lớp và L lớp. Đánh giá một VT như vậy có độ phức tạp tính toán liên quan đến kích thước mô hình LP. Một UT tương đương về kích thước sẽ có một khối UT với LP tham số và độ phức tạp tính toán khoảng LP để chạy khối một lần. Để chạy một UT như vậy cho L lớp tương đương sẽ tốn độ phức tạp L²P. Độ phức tạp tính toán tăng này trực tiếp dẫn đến thời gian huấn luyện và suy luận tăng lên. Theo Takase và Kiyono (2021), UT yêu cầu gấp đôi thời gian huấn luyện và nhiều bộ nhớ GPU hơn so với VT trong tác vụ dịch thuật Anh-Đức WMT.

Các mạng nơ-ron kích hoạt thưa thớt được giới thiệu để giảm độ phức tạp tính toán của các mô hình lớn. Những mạng này kích hoạt các phần của mạng có điều kiện trên đầu vào, chỉ tính toán các phần của mô hình, do đó tách rời số lượng tham số khỏi độ phức tạp tính toán. Phương pháp này cho phép tăng mạnh số lượng tham số mà không tăng tỷ lệ thuận độ phức tạp tính toán. Shazeer et al. (2017) đã giới thiệu Sparse Mixture of Experts (SMoE), sử dụng toán tử top-k để cho phép tính toán thưa thớt của các chuyên gia. Điều này cho phép thay thế lớp FeedForward (FFD) trong Transformer bằng một tập hợp của E FFDs, nhưng chỉ k FFDs (trong đó k < E) sẽ phải được đánh giá, có điều kiện trên đầu vào. Zhang et al. (2022) sau đó giới thiệu Mixture of Attention heads (MoA), cho phép Transformers thay thế lớp Multihead Attention (MHA) bằng một tập hợp của E đầu attention và chỉ kích hoạt k đầu có điều kiện trên đầu vào, làm thưa thớt hơn nữa mô hình.

Bài báo này giới thiệu Sparse Universal Transformer (SUT), áp dụng các kỹ thuật tính toán thưa thớt nói trên cho UT. Ngoài ra, chúng tôi thay thế cơ chế dừng theo vị trí trong UT bằng một công thức stick-breaking mới có diễn giải xác suất, cho phép chúng tôi giới thiệu một phạt Adaptive Computation Time (ACT; Graves 2016) để tối thiểu hóa việc sử dụng lớp. Nó cũng cung cấp một cách dễ dàng để điều chỉnh sự cân bằng giữa lượng tính toán và hiệu suất mô hình trong quá trình suy luận, cải thiện thêm hiệu quả của SUT tại thời điểm suy luận.

Để chứng minh việc mở rộng hiệu quả, chúng tôi thực hiện các thí nghiệm trên dịch thuật WMT'14 Anh sang Đức, cho thấy SUT có thể đạt được hiệu suất tốt hơn với cùng số lượng tham số, trong khi tốn ít chi phí tính toán hơn so với UT dày đặc tương đương. Vì thiết lập UT là một trường hợp cụ thể của SUT, chúng tôi cho thấy trên các tác vụ Compositional Freebase Questions (CFQ; Keysers et al. 2019) rằng UTs có tính chất khái quát hóa tổng hợp tốt hơn, cải thiện kết quả CFQ từ Csordás et al. (2021). Sử dụng tác vụ Logical Inference (Bowman et al., 2015), chúng tôi phân tích hành vi của UT trong việc khái quát hóa độ dài và tổng hợp. Cuối cùng, chúng tôi cho thấy cơ chế dừng có thể được sử dụng để đạt được hiệu quả hơn nữa trong thời gian suy luận, và nghiên cứu sự cân bằng giữa hiệu quả và hiệu suất.

Hình 1: Một VT có các khối Transformer riêng biệt cho mỗi lớp, với các tham số khác nhau. Đối với một UT với cùng số lượng tham số, khối UT sẽ có kích thước khoảng 3 lần so với mỗi khối VT. Chạy khối này trong 3 lớp sau đó sẽ tốn khoảng 9 lần bộ nhớ runtime. Sử dụng SMoEs có thể khôi phục khoảng cùng chi phí tính toán như VT.

Hình 2: Ví dụ về các phân chia khái quát hóa tổng hợp từ (Shen et al., 2019). Sự kết hợp của "not" và "and" không bao giờ được thấy trong kết hợp liên tiếp trong quá trình huấn luyện, và một VT có thể học một lối tắt ngăn cản việc khái quát hóa trong thời gian kiểm tra.

2 Bối cảnh & Công trình liên quan
Vượt qua các hạn chế của VT bằng UT Dziri et al. (2023) và Liu et al. (2022) phát hiện rằng Vanilla Transformers học các lối tắt cho các tác vụ đòi hỏi các phép toán tổng hợp nhiều bước, và thất bại trong việc khái quát hóa trên các trường hợp lớn hơn của vấn đề đòi hỏi nhiều bước hơn. Các kết quả lý thuyết cũng cho thấy rằng Vanilla Transformers có các hạn chế trong những gì chúng có thể tính toán hỗ trợ những phát hiện này (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022). Universal Transformers (Dehghani et al., 2018) là các Transformer với trọng số được ràng buộc qua tất cả các lớp, và một cơ chế dừng bổ sung để quyết định khi nào dừng lại. Trong một kịch bản lý tưởng của các lớp vô hạn (bây giờ tất cả các lớp có cùng tham số) UT, giống như Neural GPU (Kaiser và Sutskever, 2015), là Turing-complete, điều này vượt qua nhiều vấn đề đã nêu ở trên.

Trong thực tế, ngay cả với độ sâu hạn chế, UTs đã thể hiện các tính chất mang lại cho chúng hiệu suất tốt hơn trong các tác vụ khái quát hóa tổng hợp (Csordás et al., 2021). UTs cho phép các phép toán được học trong Transformer trong quá trình huấn luyện là bất biến với thứ tự độ sâu. Nếu một số phép toán trong quá trình huấn luyện được học để thực hiện theo một thứ tự nhất định, trong thời gian kiểm tra, UT có thể khái quát hóa sang một thứ tự phép toán chưa được thấy.

Thách thức với việc Mở rộng UT Mặc dù có những khả năng tổng hợp này, hiệu suất có xu hướng giảm trên các tác vụ thực tế khi sử dụng UTs. ALBERT (Lan et al., 2019) cải thiện hiệu quả tham số bằng cách chia sẻ tham số qua các lớp. Điều này được thúc đẩy bởi quan sát rằng các Transformer có xu hướng học để thực hiện các phép toán tương tự trong các lớp, và việc chia sẻ những tham số này sẽ giảm sự dư thừa này. Tuy nhiên, các tác giả quan sát thấy sự giảm hiệu suất khi chia sẻ tham số, trái ngược với Dehghani et al. (2018).

Liệu vấn đề có phải là về khả năng mô hình? Các thí nghiệm với ALBERT cho thấy rằng mở rộng ALBERT có thể vượt trội hơn so với baseline BERT, ngay cả trên các tác vụ thực tế (Lan et al., 2019). Kaplan et al. (2020) cũng cho thấy rằng một Transformer chia sẻ tham số có tính chất mở rộng tốt hơn về mặt tham số-hiệu suất, nhưng tính chất kém hơn về mặt tính toán-hiệu suất, vì số lượng tham số làm tăng tính toán.

Tay et al. (2022) mở rộng các mô hình chuỗi khác nhau, và nhận xét về khó khăn với việc mở rộng UTs, hạn chế các thí nghiệm họ có thể thực hiện trên UT. Takase và Kiyono (2021) phác thảo một số chiến lược mở rộng các Transformer chia sẻ tham số để đối phó với những vấn đề này bằng cách sử dụng các lược đồ chia sẻ tham số khác nhau.

Các thí nghiệm của chúng tôi cho thấy rằng các kỹ thuật SMoE có thể được áp dụng thành công cho UT để mở rộng nó, đạt được hiệu quả tham số của UT mà không tốn cùng chi phí tính toán. Chúng tôi cũng thực hiện các thí nghiệm hỗ trợ các tuyên bố về khái quát hóa tổng hợp của công trình trước đây, và cung cấp các baseline tốt hơn cho những tác vụ đó.

3 Phương pháp
Giống như UT, chúng tôi tái sử dụng cùng một khối SUT cho mỗi lớp của Transformer. Trong mỗi khối SUT, chúng tôi sử dụng SMoEs để đạt được sự thưa thớt cho mạng feed-forward (FFD) và các đầu attention riêng biệt. Chúng tôi sử dụng loss tối đa hóa Mutual Information được đề xuất trong Chen et al. (2022) và được sửa đổi cho các tác vụ không giám sát trong Shen et al. (2023). Cuối cùng, chúng tôi đề xuất một công thức quy trình stick-breaking cho dừng động, ảnh hưởng đến cách cơ chế attention hoạt động trong SUT, và loss phụ trợ Adaptive Computation Time (ACT) chúng tôi sử dụng để tối thiểu hóa số lượng lớp được sử dụng.

3.1 Sparse Mixture of Experts
Một mô-đun Mixture of Experts bao gồm E mô-đun con f1, . . . , fE. Cũng có một mạng gating, mà chúng tôi sẽ ký hiệu bằng g(e|h) – cho bất kỳ đầu vào h nào vào mô-đun MoE, mạng gating sẽ dự đoán một phân phối trên E chuyên gia. Khi k < E, chúng tôi gọi đây là Sparse Mixture of Experts (SMoE), và g(e|h) > 0 chỉ cho k chuyên gia, trong khi duy trì ∑ᴱₑ₌₁ g(e|h) = 1.

Đầu ra cuối cùng của SMoE sau đó được cho bởi y = ∑ᴱₑ₌₁ g(e|h)·fₑ(h), trong đó g(e|h) = 0, fₑ(h) sẽ không cần được đánh giá, giảm chi phí tính toán trong quá trình huấn luyện và suy luận.

Chúng tôi thay thế lớp Feed-forward (FFD) trong khối Transformer bằng một hỗn hợp các FFDs. Mỗi Mixture of FFD có thể được mô tả bằng một bộ ba (E, k, D): E chuyên gia, k cho số lượng chuyên gia được sử dụng trong phép toán top-k, và D cho chiều của lớp ẩn cho mỗi chuyên gia FFD. Đối với lớp attention, chúng tôi sử dụng Mixture of Multihead Attention (MoMHA) được đề xuất bởi Zhang et al. (2022) và Shen et al. (2023). Mỗi mô-đun MoMHA có thể được mô tả bằng một bộ năm (E, k, H, D, W), với E đại diện cho số lượng chuyên gia, k đại diện cho tham số k trong phép toán top-k, H đại diện cho số lượng đầu attention mỗi chuyên gia, và D cho các chiều mỗi đầu. Giống như trong MoA, MoMHA chỉ duy trì một tập hợp duy nhất của H phép chiếu key-value được chia sẻ giữa các chuyên gia, trong khi có E phép chiếu query của H đầu mỗi cái. W đại diện cho kích thước cửa sổ embedding vị trí tương đối, tham số hóa 2W + 1 embeddings cho W vị trí trước và sau vị trí hiện tại. Hình 3 (Trái) cho thấy sơ đồ của một khối SUT.

Kỹ thuật này đã được sử dụng để giảm chi phí tính toán trong cả thời gian huấn luyện và suy luận cho các mô hình lớn.

Tối đa hóa Mutual Information Giống như các mô hình khác dựa trên kích hoạt có điều kiện, các loss phụ trợ được cần thiết để hỗ trợ việc học một mô-đun quyết định chuyên gia nào được kích hoạt, và để đảm bảo rằng tất cả các chuyên gia được sử dụng, cân bằng tải cho việc xử lý. Cho điều này, chúng tôi sử dụng tối đa hóa Mutual Information được giới thiệu trong Chen et al. (2022) cho loss phụ trợ (được tối đa hóa):

LMIM = ∑ᴱₑ₌₁ g(e) log g(e) - (1/|X|) ∑ₕ∈X ∑ᴱₑ₌₁ g(e|h) log g(e|h)

trong đó,
g(e) = (1/|X|) ∑ₕ∈X g(e|h)

Cụ thể, chúng tôi sử dụng phiên bản không giám sát được đề xuất bởi Shen et al. (2023) giả định một phân phối đồng đều trên tất cả các token và lớp, dẫn đến mục tiêu phụ trợ sau. Trong thiết lập SUT, mạng gating được sử dụng |X| = L·T lần, trong đó L là số lượng lớp, và T là số lượng timesteps.

Về trực giác, thuật ngữ entropy tăng entropy của xác suất biên của các dự đoán mạng gating, mà ở mức tối đa có nghĩa là trọng số cho mỗi mạng gating trên toàn bộ minibatch là đồng đều. Thuật ngữ entropy có điều kiện giảm entropy có điều kiện, gây ra dự đoán của mạng gating trở nên sắc nét, và cũng phạt giải pháp phân phối đồng đều cho mạng gating.

3.2 Stick-breaking Dynamic Halting
Đã có một số phương pháp để cung cấp cho các mô hình khả năng đưa ra dự đoán mà không cần phải sử dụng tất cả các lớp của mô hình (Graves, 2016; Tan và Sim, 2016; Dehghani et al., 2018; Elbayad et al., 2019; Schuster et al., 2022). Động lực cho điều này bao gồm: (1) các đầu vào khác nhau yêu cầu số lượng lặp khác nhau để đưa ra dự đoán, (2) giảm chi phí tính toán.

UT thực hiện một cơ chế tương tự, nhưng phiên bản dừng của UT khó diễn giải. Ở đây chúng tôi chọn một phiên bản nguyên tắc của cơ chế dừng động dựa trên quy trình stick-breaking, xem nó như một phân phối xác suất. Đầu tiên, α̂ₗ⁽ᵗ⁾ là các xác suất dừng được dự đoán bởi halt(hₗ⁽ᵗ⁾), một hàm được thực hiện bởi một MLP nhận embedding của lớp trước đó. Sau đó, xác suất của bất kỳ lớp nào dừng được tính bởi:

αₗ⁽ᵗ⁾ = α̂ₗ⁽ᵗ⁾ ∏ₗ'₌₁ˡ⁻¹ (1 - α̂ₗ'⁽ᵗ⁾)

Một công thức tương tự được mô tả trong Graves (2016) và Tan và Sim (2016). Thuật toán 1 cho thấy cách cơ chế được thực hiện tại bất kỳ timestep nào. hₗ₋₁ là đầu ra của lớp trước đó cho timestep hiện tại.

Có điều kiện trên việc chúng ta đang tính toán hₗ⁽ᵗ⁾, timestep t không được dừng trước hoặc tại l-1. Vì vậy chúng ta có thể sử dụng hₗ⁽ᵗ⁾, trạng thái chưa dừng, làm đầu vào cho việc tính toán query attention của khối. Tuy nhiên, vì timestep t có thể chú ý đến tất cả các timestep khác, và các bước khác này có thể đã dừng, chúng ta sử dụng các trạng thái đã dừng Sₗ₋₁ cho các lớp trước đó.

Tuy nhiên, bởi vì việc dừng là một quyết định 'mềm', chúng ta có thể nới lỏng yêu cầu đánh giá tất cả các trạng thái dừng có thể và sử dụng trạng thái dừng kỳ vọng như một thay thế. Các cơ chế dừng trước đây sử dụng một cơ chế 'gating' của tổng lồi giữa các đầu ra đã được gated trước đó và đầu ra của bước hiện tại hₗ = αₗ·ĥₗ + (1-αₗ)·hₗ₋₁ (Dehghani et al., 2018). Điều này có thể dẫn đến các gradient nhỏ dần về phía các lớp trên khi (1-αₗ) nhân lên. Thay vào đó, chúng ta có thể tính toán embedding dừng kỳ vọng tại bất kỳ l nào,

sₗ⁽ᵗ⁾ = (1 - ∑ₗ'₌₁ˡ⁻¹ αₗ'⁽ᵗ⁾)·hₗ⁽ᵗ⁾ + ∑ₗ'₌₁ˡ⁻¹ αₗ'⁽ᵗ⁾ hₗ'⁽ᵗ⁾

Nếu αₗ⁽ᵗ⁾ = 1 cho một số l, sₗ⁽ᵗ⁾ = hₗ⁽ᵗ⁾, khôi phục hành vi của quyết định dừng rời rạc. Chúng ta sử dụng sₗ⁽ᵗ⁾ làm đầu vào cho các phép biến đổi key và value attention.

Diễn giải xác suất này cũng cho phép chúng ta áp đặt một loss về số lượng lớp kỳ vọng được sử dụng ở mỗi bước, thiên về phía ít lặp hơn, do đó tiết kiệm chi phí tính toán.

LACT = (1/T) ∑ₜ₌₁ᵀ ∑ₗ₌₁ᴸ αₗ⁽ᵗ⁾·l

Chúng tôi sử dụng ngưỡng αthresh = 0.999, sao cho tổng tích lũy của các xác suất dừng đã vượt quá điều này, không có tính toán nào sẽ được thực hiện cho timestep đó, và các embeddings của lớp trước đó sẽ được sao chép. Do phép toán routing yêu cầu trong việc thực hiện SMoEs, chúng ta có thể đơn giản định tuyến các trạng thái đã dừng đến một chuyên gia "No Op", dẫn đến tiết kiệm thực sự trong chi phí tính toán khi việc dừng đạt ngưỡng sớm. Chúng tôi thấy rằng điều chỉnh ngưỡng này sau khi huấn luyện có thể duy trì hiệu suất trong khi tiết kiệm các bước tính toán.

4 Thí nghiệm
Đầu tiên, chúng tôi cho thấy rằng chúng ta có thể mở rộng UT bằng SUT trên tác vụ dịch thuật WMT'14 Anh-Đức (Bojar et al., 2014). Sau đó chúng tôi chạy các thí nghiệm trên Compositional Freebase Questions (CFQ; Keysers et al. 2019) để kiểm tra các tính chất khái quát hóa tổng hợp. Để phân tích thêm hành vi của mô hình dưới các thiết lập khái quát hóa tổng hợp, chúng tôi kiểm tra mô hình của chúng tôi trên tác vụ Logical inference từ (Bowman et al., 2015). Tất cả các thí nghiệm đều được thực hiện trong framework Fairseq (Ott et al., 2019).

4.1 Dịch thuật Anh sang Đức
Chúng tôi thực hiện các thí nghiệm trên bộ dữ liệu dịch thuật WMT'14 Anh-Đức (Bojar et al., 2014). Chúng tôi sử dụng tiền xử lý từ Liu et al. (2020). Chúng tôi sử dụng từ điển được nối và chia sẻ tất cả các embedding từ của encoder và decoder. Để đánh giá, chúng tôi lấy trung bình 5 mô hình tốt nhất cuối cùng theo điểm negative log-likelihood của chúng. Chúng tôi báo cáo điểm BLEU (Papineni et al., 2002), và cũng báo cáo MACs (Multiply-Accumulate Operations) để đánh giá chi phí tính toán runtime của các mô hình khác nhau. MACs của các mô hình trước đây được tính toán trong Zhang et al. (2022).

Kết quả được báo cáo trong Bảng 1. Chúng tôi so sánh với các baseline mạnh mẽ trong khi tính đến số lượng tham số trong các mô hình này. Ngoài ra, chúng tôi huấn luyện hai UTs bằng cách thiết lập E = 1, k = 1, và tham số hóa các lớp FFD và Attention với tham số để phù hợp với thiết lập ~65M và ~110M của chúng tôi cho SUT. Cả SUTs và UTs đều chứng minh hiệu quả tham số tốt khi so sánh với các mô hình trước đây. Trong lớp tham số ~110M, SUT và UT hoạt động ở khoảng 29.4 và 29.6 BLEU tương ứng, trong khi các mô hình trước đây yêu cầu ~200M tham số. Trong khi SUT không hoạt động tốt bằng UT, nhưng các tính toán yêu cầu trong runtime có thể thấp đến một phần năm của UT. Ngoài ra, bởi vì chúng tôi giữ k không đổi cho SUT, MACs vẫn không đổi khi SUT mở rộng lên.

Chúng tôi chạy các thí nghiệm loại bỏ các khía cạnh khác nhau của mô hình và quy trình huấn luyện, bao gồm: loss phụ trợ MIM, Mixture of MHA, loss ACT, và cơ chế dừng. Kết quả trong Bảng 2. Việc giới thiệu nhiều đầu vào MoA là quan trọng trong việc thấy được lợi ích hiệu suất trong tác vụ này, cũng như có loss MIM như một mục tiêu phụ trợ cân bằng tải. Thú vị là, việc dừng đóng góp nhiều lợi ích hiệu suất như nó làm trong CFQ.

Ngoài ra, chúng tôi tính toán 5 token hàng đầu xuất hiện kết hợp với mỗi chuyên gia, bất kể lớp nào, và thấy rằng một số liên kết tồn tại. Chúng tôi chọn một số chuyên gia trong Bảng 3 cho thấy dấu hiệu rõ ràng của việc xuất hiện đồng thời với các token dường như thể hiện một mô hình. Điều này gợi ý rằng trong khi có thể có sự dư thừa giữa các chuyên gia, các nhóm chuyên gia có thể chuyên môn hóa về các nhiệm vụ nhất định, dẫn đến một số tính mô-đun. Nghiên cứu trong tương lai có thể điều tra xem tính mô-đun như vậy có thể dẫn đến khái quát hóa mạnh mẽ hơn hay không.

4.2 Compositional Freebase Questions
Chúng tôi chạy các thí nghiệm trên bộ dữ liệu Compositional Freebase Questions (CFQ; Keysers et al. 2019) để xác định khả năng khái quát hóa tổng hợp của SUT. Đây là một tác vụ dịch thuật từ ngôn ngữ tự nhiên sang truy vấn SPARQL. Ví dụ, chuỗi "Who wrote M1 and wrote a film" sẽ được dịch sang chuỗi đích "SELECT DISTINCT ?x0 WHERE { ?x0 a people.person . ?x0 film.writer.film ?x1 M1 . ?x1 a film.film }". CFQ kiểm tra khái quát hóa tổng hợp bằng cách sử dụng khái niệm compound divergence, đo lường sự khác biệt giữa tập huấn luyện và tập kiểm tra về mặt kết hợp các token, mà họ gọi là compounds. Theo hiểu biết của chúng tôi, các mô hình hoạt động tốt nhất hiện tại hoặc fine-tune một mô hình ngôn ngữ được huấn luyện trước hoặc sử dụng kiến thức về tác vụ để thiết kế một prompt phù hợp cho một mô hình ngôn ngữ lớn (Drozdov et al., 2022). Trong khi phương pháp prompting cực kỳ hiệu quả trong tác vụ CFQ, chúng tôi xem tác vụ như một benchmark cho khái quát hóa tổng hợp nói chung và nên được xem kết hợp với các thí nghiệm khác, đặc biệt là dữ liệu thực tế (như dịch thuật). Khi sử dụng kiến thức miền của tác vụ trong prompt, kết quả có thể chỉ ra hiệu suất tốt hơn với một phương pháp cụ thể cho CFQ (và có thể các tác vụ dịch SQL khác) nhưng có thể khó ngoại suy sang các thiết lập khác.

Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng các script tiền xử lý từ Zheng và Lapata (2021). Các script thực hiện tiền xử lý cho chuỗi đích để đơn giản hóa chuỗi đích theo cùng cách được thực hiện trong Furrer et al. (2020). Tương ứng, chúng tôi huấn luyện một Transformer baseline trên đích đã được biến đổi. Chúng tôi thực hiện tìm kiếm trên các siêu tham số SUT, sử dụng tập validation MCD1, và tập tham số hoạt động tốt nhất là Attention (E = 1, k = 1, H = 8, D = 64, W = 1) và FFD (E = 1, k = 1, D = 1024), tương ứng với thiết lập UT. Tham khảo Phụ lục A để biết thêm chi tiết. Vì CFQ là một tác vụ tương đối nhỏ, quy mô lớn hơn không phải là một yếu tố và có thể gợi ý rằng chuyên môn hóa chuyên gia có thể không hữu ích. Kết quả được hiển thị trong Bảng 4. Trong các trường hợp có và không có dừng, mô hình đã vượt trội so với các benchmark trước đây, bao gồm baseline UT từ Bergen et al. (2021). Để so sánh công bằng hơn, chúng tôi sử dụng cùng siêu tham số với việc thực hiện UT của chúng tôi, chúng tôi sửa đổi việc thực hiện UT của chúng tôi để giống hơn với UT dựa trên T5 trong Bergen et al. (2021). Những thay đổi này bao gồm: bias vị trí tương đối theo bucket được sử dụng bởi T5, và chuyển từ post layer-norm sang pre layer-norm. Trong khi điều này dẫn đến kết quả cải thiện nhiều so với bài báo gốc, việc thực hiện UT của chúng tôi vẫn vượt trội hơn nó.

Mô hình Dangle (Zheng và Lapata, 2021), vượt trội hơn mô hình của chúng tôi, cũng yêu cầu chạy lại encoder cho mỗi token được decode. Đây là một quá trình tốn kém, nhưng cho rằng cả phương pháp của chúng tôi và Dangle đều hoạt động tốt trong tác vụ này, là bằng chứng bổ sung rằng các quá trình lặp có lợi cho khái quát hóa tổng hợp.

4.3 Logical Inference
Chúng tôi sử dụng tác vụ logical inference từ (Bowman et al., 2015) như một bàn kiểm tra cho UT. Mặc dù sự đơn giản rõ ràng của ngôn ngữ, tác vụ vốn đòi hỏi mô hình phải học cấu trúc phân cấp của vấn đề. Mỗi trường hợp của tác vụ bao gồm hai câu logic, và mục tiêu là dự đoán xem các câu có tương đương, mâu thuẫn, rời rạc, hay entail theo một trong hai hướng. Ví dụ, cho s1 = a và s2 = a (or b), thì s1 ⊏ s2. Điểm mấu chốt của tác vụ là huấn luyện mô hình trên các chuỗi có 0-6 toán tử logic và đánh giá nó trên các chuỗi có 7-12 toán tử. Cho thiết lập sequence-to-sequence của chúng tôi, chúng tôi chuyển đổi tác vụ thành một tác vụ dịch thuật. Mô hình nhận sentence1 #SEP# sentence2 làm câu nguồn, với câu đích là nhãn token đơn cho cặp đó.

Chúng tôi huấn luyện một mô hình 12 lớp với Attention (E = 12, k = 4, H = 2, D = 32, W = 1) và FFD (E = 12, K = 4, D = 128) và dừng. Tham khảo Phụ lục A để biết thêm chi tiết. Huấn luyện một Vanilla Transformer 12 lớp đạt được kết quả tương đương như trong Shen et al. (2019), vì vậy chúng tôi báo cáo kết quả của họ. Kết quả của chúng tôi trong Bảng 5 xác nhận các phát hiện của Tran et al. (2018), cho thấy rằng với sự lặp lại trong SUTs, chúng ta có thể khái quát hóa sang các chuỗi dài hơn của tác vụ. Trong khi có các mô hình khác tạo ra cấu trúc cây hoạt động cực kỳ tốt trong tác vụ, chúng tôi muốn đánh giá mô hình của chúng tôi so với các kiến trúc phổ biến khác. LSTM là một baseline mạnh mẽ, và chúng tôi thấy rằng UT vượt trội hơn nó trong khái quát hóa. Chúng tôi cũng đánh giá UTs trên các phân chia khái quát hóa tổng hợp được đề xuất trong (Shen et al., 2019), trong đó các phân chia A, B, và C có độ khó tăng dần. Kết quả cho thấy rằng UTs có thể khái quát hóa tốt hơn cho các phân chia A và B, vượt trội hơn LSTM và VT. Phân chia C vẫn là thách thức cho các biến thể Transformer.

Ngoài ra, chúng tôi tính toán độ sâu dừng trung bình cho dữ liệu kiểm tra được phân đoạn theo số lượng toán tử. Bởi vì nhiều toán tử yêu cầu nhiều lồng ghép biểu thức hơn trong các chuỗi, cần nhiều hồi quy hơn để phân tích đúng chuỗi. Như mong đợi, trong Hình 4, độ sâu dừng trung bình tăng khi sử dụng nhiều toán tử hơn. Số lượng toán tử cho các mệnh đề này có tương quan với độ dài, điều này gợi ý rằng SUTs có thể phù hợp để khái quát hóa cho độ dài. Chúng tôi bao gồm các thí nghiệm thêm về khái quát hóa độ dài trong Phụ lục Bảng 8.

4.4 Giảm Tính toán Hậu huấn luyện
Việc giảm αthresh sau khi huấn luyện có làm cho mô hình dừng sớm hơn, tiết kiệm tính toán không? Điều đó sẽ tốn bao nhiêu về mặt độ chính xác?

Chúng tôi ước tính các tính toán khối SUT bị bỏ qua với các giá trị khác nhau của αthresh ∈ {0.1, 0.2, ..., 0.9} bằng cách nhìn vào các mô hình dừng của decoder với các cặp nguồn-đích ground truth. Chúng tôi truyền cặp nguồn-đích vào mô hình và phân tích các mô hình dừng của mô hình, cho chúng tôi một ước tính sơ bộ về lượng tính toán sẽ được tiết kiệm như một phần trăm của việc tính toán tất cả các lớp của SUT.

Logical Inference Chúng tôi quan sát hiệu suất kết quả trên phân chia khó nhất của tập kiểm tra với 12 phép toán. Do mô hình dừng đã bão hòa, xác suất dừng αl tăng vọt nhanh chóng từ gần 0 đến các giá trị cao hơn, dẫn đến việc giảm khoảng 50% thời gian tính toán gần như không đổi bất kể ngưỡng.

CFQ Sử dụng phân chia kiểm tra MCD1 của bộ dữ liệu, và mô hình hoạt động tốt nhất của chúng tôi trên MCD1, chúng tôi thực hiện điều chỉnh αthresh. Các mô hình dừng phản ánh cấu trúc lặp lại của SQL, sử dụng ít bước hơn cho '.' và 'WHERE', trong khi phần chính của vùng trong {...} yêu cầu nhiều bước SUT hơn trước khi dừng. Đáng ngạc nhiên, khi 0.8 ≤ αthresh ≤ 0.999, độ chính xác vẫn khá không đổi. Ước tính 33% các bước tính toán đã bị bỏ qua tại αthresh = 0.8. Tại αthresh = 0.1, có một sự tăng nhẹ về số lượng bước được tính toán, điều này có thể xảy ra vì việc dừng sớm hơn sẽ dẫn đến các embedding khác nhau, và dẫn đến các quyết định dừng khác nhau trong các timestep khác. Nhìn chung, kết quả gợi ý rằng chúng ta có thể tiết kiệm khoảng 20% các bước tính toán SUT mà không có bất kỳ sự giảm nào về độ chính xác, và khoảng ~50% cho việc giảm 0.2%.

Dịch thuật Anh-Đức Đối với bộ dữ liệu lớn hơn này, chúng tôi thấy rằng các mô hình dịch thuật này dừng muộn hơn nhiều, gợi ý rằng tác vụ dịch thuật yêu cầu nhiều bước tính toán hơn so với SUT 6 lớp mà chúng tôi sử dụng. Tuy nhiên, việc tăng thêm số lượng lớp lên 12 lớp không mang lại nhiều lợi ích, như được chứng minh bởi việc dừng trong Hình 4, đây là một ví dụ về cơ chế dừng sử dụng gần như tất cả các lớp. Để so sánh, mô hình Admin 60L-12L, yêu cầu encoder 60 lớp để đạt được hiệu suất của nó. Ngay cả khi αthresh = 1, các bước tính toán bị bỏ qua vẫn ở khoảng 33%, so với 80% trong tác vụ CFQ. Chúng tôi thấy rằng chúng ta có thể giảm tính toán 9% trong khi vẫn giữ được điểm BLEU là 29.1.

5 Kết luận
Chúng tôi cho thấy rằng có thể mở rộng UT thông qua SUTs, và SUTs vượt trội hơn các mô hình cùng khả năng trong tác vụ dịch thuật WMT'14 Anh-Đức. Tính chất đệ quy của cả UTs và SUTs cho phép các bias quy nạp tốt hơn, mà chúng tôi đã chứng minh trong các tác vụ tổng hợp như CFQ và logical inference. VTs đã được chứng minh là kém trong các tác vụ khái quát hóa tổng hợp này mà không có kiến thức miền bổ sung. Cơ chế dừng động stick-breaking cũng cho phép điều chỉnh chi phí tính toán hậu huấn luyện, đây là một lợi ích cho triển khai quy mô lớn.

Hạn chế Trong khi các thí nghiệm trong bài báo này cho thấy các tính chất khái quát hóa mong muốn của UTs, có một số khía cạnh của khái quát hóa tổng hợp mà SUTs không giải quyết được. Quan trọng là, trong khi chúng tôi chứng minh việc mở rộng UTs thông qua SMoEs, cần có thêm các thí nghiệm trên các thiết lập lớn hơn để xác định tính khả thi trong các hệ thống quy mô lớn. Các vấn đề khác cũng có thể xuất hiện trong việc mở rộng SUTs hơn nữa, nhưng chúng tôi tin rằng có đủ tài liệu để dựa vào để tìm giải pháp cho những vấn đề này.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được bao gồm với tất cả các trích dẫn từ bài báo gốc]

A Chi tiết Thí nghiệm
Tất cả các mô hình của chúng tôi được huấn luyện trên GPU V100. Chúng tôi sử dụng bộ tối ưu hóa Adam với β₁ = 0.9, β₂ = 0.98 và ε = 1e-9. Chúng tôi sử dụng bộ lập lịch tốc độ học nghịch đảo căn bậc hai. Trong quá trình huấn luyện, chúng tôi sử dụng làm mượt nhãn (Szegedy et al., 2016) với giá trị 0.1. Thêm siêu tham số huấn luyện có thể được tìm thấy trong Bảng 7.

B Kết quả Thí nghiệm Bổ sung
[Phần này bao gồm tất cả các kết quả thí nghiệm bổ sung, bảng, và phân tích được trình bày trong phụ lục của bài báo gốc]
>>>>>>> Stashed changes
