[CONTEXT from previous chunk - for reference only]
Specifically, we empower the agent with embodied
action ability and tool-handling skills, enabling it to adeptly adapt to environmental changes, provide
feedback, and even influence and mold the environment. The framework can be tailored for different application scenarios, i.e. not every specific component
will be used in all studies. In general, agents operate in the following workflow: First, the perception
10

--- PAGE 11 ---
module, corresponding to human sensory systems such as the eyes and ears, perceives changes in the
external environment and then converts multimodal information into an understandable representation
for the agent.
[NEW CONTENT to translate]
Subsequently, the brain module, serving as the control center, engages in information
processing activities such as thinking, decision-making, and operations with storage including
memory and knowledge. Finally, the action module, corresponding to human limbs, carries out the
execution with the assistance of tools and leaves an impact on the surroundings. By repeating the
above process, an agent can continuously get feedback and interact with the environment.
3.1 Brain
BrainNatural Language
Interaction §3.1.1High-quality
generationBang et al. [ 132], Fang et al. [ 133],
Lin et al. [ 127], Lu et al. [ 134], etc.
Deep understandingBuehler et al. [ 135], Lin et al.
[128], Shapira et al. [ 136], etc.
Knowledge §3.1.2Knowledge in
LLM-based agentPretrain modelHill et al. [ 137], Collobert et al.
[138], Kaplan et al. [ 139], Roberts
et al. [ 140], Tandon et al. [ 141], etc.
Linguistic knowledgeVulic et al. [ 142], Hewitt et al.
[143], Rau et al. [ 144], Yang et al.
[145], Beloucif et al. [ 146], Zhang
et al. [ 147], Bang et al. [ 132], etc.
Commensense
knowledgeSafavi et al. [ 148], Jiang et
al. [149], Madaan [ 150], etc.
Actionable
knowledgeXu et al. [ 151], Cobbe et al. [ 152],
Thirunavukarasu et al. [ 153], Lai et
al. [154], Madaan et al. [ 150], etc.
Potential issues
of knowledgeEdit wrong and
outdated knowledgeAlKhamissi et al. [ 155], Kemker et
al. [156], Cao et al. [ 157], Yao et
al. [158], Mitchell et al. [ 159], etc.
Mitigate hallucinationManakul et al. [ 160], Qin et al. [ 94],
Li et al. [ 161], Gou et al. [ 162], etc.
Memory §3.1.3Memory capabilityRaising the length
limit of TransformersBART [ 163], Park et al. [ 164], LongT5
[165], CoLT5 [ 166], Ruoss et al. [ 167], etc.
Summarizing
memoryGenerative Agents [ 22], SCM
[168], Reflexion [ 169], Memory-
bank [ 170], ChatEval [ 171], etc.
Compressing mem-
ories with vectors
or data structuresChatDev [ 109], GITM [ 172], RET-LLM
[173], AgentSims [ 174], ChatDB [ 175], etc.
Memory retrievalAutomated retrievalGenerative Agents [ 22], Memory-
bank [ 170], AgentSims [ 174], etc.
Interactive retrieval Memory Sandbox[ 176], ChatDB [ 175], etc.
Reasoning &
Planning §3.1.4ReasoningCoT [ 95], Zero-shot-CoT [ 96],
Self-Consistency [ 97], Self-
Polish [ 99], Selection-Inference
[177], Self-Refine [ 178], etc.
PlaningPlan formulationLeast-to-Most [ 98], SayCan [ 179], Hug-
gingGPT [ 180], ToT [ 181], PET [ 182],
DEPS [ 183], RAP [ 184], SwiftSage
[185], LLM+P [ 125], MRKL [ 186], etc.
Plan reflectionLLM-Planner [ 101], Inner Monologue
[187], ReAct [ 91], ChatCoT [ 188], AI
Chains [ 189], V oyager [ 190], Zhao
et al. [ 191], SelfCheck [ 192], etc.
Transferability &
Generalization §3.1.5Unseen task
generalizationT0 [106], FLAN [ 105], Instruct-
GPT [ 24], Chung et al. [ 107], etc.
In-context learningGPT-3 [ 41], Wang et al. [ 193], Wang
et al. [ 194], Dong et al. [ 195], etc.
Continual learningKe et al. [ 196], Wang et al. [ 197], Raz-
daibiedina et al. [ 198], V oyager [ 190], etc.
Figure 3: Typology of the brain module.
11

--- PAGE 12 ---
The human brain is a sophisticated structure comprised of a vast number of interconnected neu-
rons, capable of processing various information, generating diverse thoughts, controlling different
behaviors, and even creating art and culture [ 199]. Much like humans, the brain serves as the central
nucleus of an AI agent, primarily composed of a large language model. Operating mechanism. To ensure effective communication, the ability to engage in natural lan-
guage interaction (§3.1.1) is paramount. After receiving the information processed by the perception
module, the brain module first turns to storage, retrieving in knowledge (§3.1.2) and recalling from
memory (§3.1.3). These outcomes aid the agent in devising plans, reasoning, and making informed
decisions (§3.1.4). Additionally, the brain module may memorize the agent’s past observations,
thoughts, and actions in the form of summaries, vectors, or other data structures. Meanwhile, it
can also update the knowledge such as common sense and domain knowledge for future use. The
LLM-based agent may also adapt to unfamiliar scenarios with its inherent generalization and transfer
ability (§3.1.5). In the subsequent sections, we delve into a detailed exploration of these extraordinary
facets of the brain module as depicted in Figure 3.
3.1.1 Natural Language Interaction
As a medium for communication, language contains a wealth of information. In addition to the
intuitively expressed content, there may also be the speaker’s beliefs, desires, and intentions hidden
behind it [ 200]. Thanks to the powerful natural language understanding and generation capabilities
inherent in LLMs [ 25;201;202;203], agents can proficiently engage in not only basic interactive
conversations [ 204;205;206] in multiple languages [ 132;202] but also exhibit in-depth comprehen-
sion abilities, which allow humans to easily understand and interact with agents [ 207;208]. Besides,
LLM-based agents that communicate in natural language can earn more trust and cooperate more
effectively with humans [130]. Multi-turn interactive conversation. The capability of multi-turn conversation is the foundation of
effective and consistent communication. As the core of the brain module, LLMs, such as GPT series
[40;41;201], LLaMA series [ 201;209] and T5 series [ 107;210], can understand natural language
and generate coherent and contextually relevant responses, which helps agents to comprehend better
and handle various problems [ 211]. However, even humans find it hard to communicate without
confusion in one sitting, so multiple rounds of dialogue are necessary. Compared with traditional
text-only reading comprehension tasks like SQuAD [ 212], multi-turn conversations (1) are interactive,
involving multiple speakers, and lack continuity; (2) may involve multiple topics, and the information
of the dialogue may also be redundant, making the text structure more complex [ 147]. In general, the
multi-turn conversation is mainly divided into three steps: (1) Understanding the history of natural
language dialogue, (2) Deciding what action to take, and (3) Generating natural language responses. LLM-based agents are capable of continuously refining outputs using existing information to conduct
multi-turn conversations and effectively achieve the ultimate goal [132; 147]. High-quality natural language generation. Recent LLMs show exceptional natural language
generation capabilities, consistently producing high-quality text in multiple languages [ 132;213]. The coherency [ 214] and grammatical accuracy [ 133] of LLM-generated content have shown steady
enhancement, evolving progressively from GPT-3 [ 41] to InstructGPT [ 24], and culminating in GPT-4
[25]. See et al. [ 214] empirically affirm that these language models can “adapt to the style and
content of the conditioning text” [ 215]. And the results of Fang et al. [ 133] suggest that ChatGPT
excels in grammar error detection, underscoring its powerful language capabilities. In conversational
contexts, LLMs also perform well in key metrics of dialogue quality, including content, relevance,
and appropriateness [ 127]. Importantly, they do not merely copy training data but display a certain
degree of creativity, generating diverse texts that are equally novel or even more novel than the
benchmarks crafted by humans [ 216]. Meanwhile, human oversight remains effective through the
use of controllable prompts, ensuring precise control over the content generated by these language
models [134]. Intention and implication understanding. Although models trained on the large-scale corpus are
already intelligent enough to understand instructions, most are still incapable of emulating human
dialogues or fully leveraging the information conveyed in language [ 217]. Understanding the implied
meanings is essential for effective communication and cooperation with other intelligent agents [ 135],
12

--- PAGE 13 ---
and enables one to interpret others’ feedback. The emergence of LLMs highlights the potential
of foundation models to understand human intentions, but when it comes to vague instructions or
other implications, it poses a significant challenge for agents [ 94;136]. For humans, grasping the
implied meanings from a conversation comes naturally, whereas for agents, they should formalize
implied meanings into a reward function that allows them to choose the option in line with the
speaker’s preferences in unseen contexts [ 128]. One of the main ways for reward modeling is
inferring rewards based on feedback, which is primarily presented in the form of comparisons [ 218]
(possibly supplemented with reasons [ 219]) and unconstrained natural language [ 220]. Another way
involves recovering rewards from descriptions, using the action space as a bridge [ 128]. Jeon et al.
[221] suggests that human behavior can be mapped to a choice from an implicit set of options, which
helps to interpret all the information in a single unifying formalism. By utilizing their understanding
of context, agents can take highly personalized and accurate action, tailored to specific requirements.
3.1.2 Knowledge
Due to the diversity of the real world, many NLP researchers attempt to utilize data that has a larger
scale. This data usually is unstructured and unlabeled [ 137;138], yet it contains enormous knowledge
that language models could learn. In theory, language models can learn more knowledge as they have
more parameters [ 139], and it is possible for language models to learn and comprehend everything in
natural language. Research [ 140] shows that language models trained on a large-scale dataset can
encode a wide range of knowledge into their parameters and respond correctly to various types of
queries. Furthermore, the knowledge can assist LLM-based agents in making informed decisions
[222]. All of this knowledge can be roughly categorized into the following types:
•Linguistic knowledge. Linguistic knowledge [ 142;143;144] is represented as a system of
constraints, a grammar, which defines all and only the possible sentences of the language. It
includes morphology, syntax, semantics [ 145;146], and pragmatics. Only the agents that acquire
linguistic knowledge can comprehend sentences and engage in multi-turn conversations [ 147]. Moreover, these agents can acquire multilingual knowledge [ 132] by training on datasets that
contain multiple languages, eliminating the need for extra translation models.
•Commonsense knowledge. Commonsense knowledge [ 148;149;150] refers to general world
facts that are typically taught to most individuals at an early age. For example, people commonly
know that medicine is used for curing diseases, and umbrellas are used to protect against rain. Such
information is usually not explicitly mentioned in the context. Therefore, the models lacking the
corresponding commonsense knowledge may fail to grasp or misinterpret the intended meaning
[141]. Similarly, agents without commonsense knowledge may make incorrect decisions, such as
not bringing an umbrella when it rains heavily.
•Professional domain knowledge. Professional domain knowledge refers to the knowledge associ-
ated with a specific domain like programming [ 151;154;150], mathematics [ 152], medicine [ 153],
etc. It is essential for models to effectively solve problems within a particular domain [ 223]. For
example, models designed to perform programming tasks need to possess programming knowledge,
such as code format. Similarly, models intended for diagnostic purposes should possess medical
knowledge like the names of specific diseases and prescription drugs. Although LLMs demonstrate excellent performance in acquiring, storing, and utilizing knowledge
[155], there remain potential issues and unresolved problems. For example, the knowledge acquired
by models during training could become outdated or even be incorrect from the start. A simple way to
address this is retraining. However, it requires advanced data, extensive time, and computing resources. Even worse, it can lead to catastrophic forgetting [ 156]. Therefore, some researchers[ 157;158;159]
try editing LLMs to locate and modify specific knowledge stored within the models. This involved
unloading incorrect knowledge while simultaneously acquiring new knowledge. Their experiments
show that this method can partially edit factual knowledge, but its underlying mechanism still
requires further research. Besides, LLMs may generate content that conflicts with the source or
factual information [ 224], a phenomenon often referred to as hallucinations [ 225]. It is one of
the critical reasons why LLMs can not be widely used in factually rigorous tasks. To tackle this
issue, some researchers [ 160] proposed a metric to measure the level of hallucinations and provide
developers with an effective reference to evaluate the trustworthiness of LLM outputs. Moreover,
some researchers[ 161;162] enable LLMs to utilize external tools[ 94;226;227] to avoid incorrect
13

--- PAGE 14 ---
knowledge. Both of these methods can alleviate the impact of hallucinations, but further exploration
of more effective approaches is still needed.
3.1.3 Memory
In our framework, “memory” stores sequences of the agent’s past observations, thoughts and actions,
which is akin to the definition presented by Nuxoll et al. [ 228]. Just as the human brain relies on
memory systems to retrospectively harness prior experiences for strategy formulation and decision-
making, agents necessitate specific memory mechanisms to ensure their proficient handling of
a sequence of consecutive tasks [ 229;230;231]. When faced with complex problems, memory
mechanisms help the agent to revisit and apply antecedent strategies effectively. Furthermore, these
memory mechanisms enable individuals to adjust to unfamiliar environments by drawing on past
experiences. With the expansion of interaction cycles in LLM-based agents, two primary challenges arise. The
first pertains to the sheer length of historical records. LLM-based agents process prior interactions
in natural language format, appending historical records to each subsequent input. As these records
expand, they might surpass the constraints of the Transformer architecture that most LLM-based
agents rely on. When this occurs, the system might truncate some content. The second challenge is
the difficulty in extracting relevant memories. As agents amass a vast array of historical observations
and action sequences, they grapple with an escalating memory burden. This makes establishing
connections between related topics increasingly challenging, potentially causing the agent to misalign
its responses with the ongoing context. Methods for better memory capability. Here we introduce several methods to enhance the memory
of LLM-based agents.
•Raising the length limit of Transformers. The first method tries to address or mitigate the inherent
sequence length constraints. The Transformer architecture struggles with long sequences due to
these intrinsic limits. As sequence length expands, computational demand grows exponentially
due to the pairwise token calculations in the self-attention mechanism. Strategies to mitigate
these length restrictions encompass text truncation [ 163;164;232], segmenting inputs [ 233;234],
and emphasizing key portions of text [ 235;236;237]. Some other works modify the attention
mechanism to reduce complexity, thereby accommodating longer sequences [ 238;165;166;167].
•Summarizing memory. The second strategy for amplifying memory efficiency hinges on the
concept of memory summarization. This ensures agents effortlessly extract pivotal details from
historical interactions. Various techniques have been proposed for summarizing memory. Using
prompts, some methods succinctly integrate memories [ 168], while others emphasize reflective
processes to create condensed memory representations [ 22;239]. Hierarchical methods streamline
dialogues into both daily snapshots and overarching summaries [ 170]. Notably, specific strategies
translate environmental feedback into textual encapsulations, bolstering agents’ contextual grasp
for future engagements [ 169]. Moreover, in multi-agent environments, vital elements of agent
communication are captured and retained [171].
•Compressing memories with vectors or data structures. By employing suitable data structures,
intelligent agents boost memory retrieval efficiency, facilitating prompt responses to interactions. Notably, several methodologies lean on embedding vectors for memory sections, plans, or dialogue
histories [ 109;170;172;174]. Another approach translates sentences into triplet configurations
[173], while some perceive memory as a unique data object, fostering varied interactions [ 176]. Furthermore, ChatDB [ 175] and DB-GPT [ 240] integrate the LLMrollers with SQL databases,
enabling data manipulation through SQL commands. Methods for memory retrieval. When an agent interacts with its environment or users, it is
imperative to retrieve the most appropriate content from its memory. This ensures that the agent
accesses relevant and accurate information to execute specific actions. An important question arises:
How can an agent select the most suitable memory. Typically, agents retrieve memories in an
automated manner [ 170;174]. A significant approach in automated retrieval considers three metrics:
Recency, Relevance, and Importance. The memory score is determined as a weighted combination of
these metrics, with memories having the highest scores being prioritized in the model’s context [ 22].
14

--- PAGE 15 ---
Some research introduces the concept of interactive memory objects, which are representations of
dialogue history that can be moved, edited, deleted, or combined through summarization. Users can
view and manipulate these objects, influencing how the agent perceives the dialogue [ 176]. Similarly,
other studies allow for memory operations like deletion based on specific commands provided by
users [175]. Such methods ensure that the memory content aligns closely with user expectations.
3.1.4 Reasoning and Planning
Reasoning. Reasoning, underpinned by evidence and logic, is fundamental to human intellectual
endeavors, serving as the cornerstone for problem-solving, decision-making, and critical analysis
[241;242;243]. Deductive, inductive, and abductive are the primary forms of reasoning commonly
recognized in intellectual endeavor [ 244]. For LLM-based agents, like humans, reasoning capacity is
crucial for solving complex tasks [25]. Differing academic views exist regarding the reasoning capabilities of large language models. Some
argue language models possess reasoning during pre-training or fine-tuning [ 244], while others
believe it emerges after reaching a certain scale in size [ 26;245]. Specifically, the representative
Chain-of-Thought (CoT) method [ 95;96] has been demonstrated to elicit the reasoning capacities of
large language models by guiding LLMs to generate rationales before outputting the answer. Some
other strategies have also been presented to enhance the performance of LLMs like self-consistency
[97], self-polish [ 99], self-refine [ 178] and selection-inference [ 177], among others. Some studies
suggest that the effectiveness of step-by-step reasoning can be attributed to the local statistical
structure of training data, with locally structured dependencies between variables yielding higher data
efficiency than training on all variables [246]. Planning is a key strategy humans employ when facing complex challenges. For humans,
planning helps organize thoughts, set objectives, and determine the steps to achieve those objectives
[247;248;249]. Just as with humans, the ability to plan is crucial for agents, and central to this
planning module is the capacity for reasoning [ 250;251;252]. This offers a structured thought
process for agents based on LLMs. Through reasoning, agents deconstruct complex tasks into more
manageable sub-tasks, devising appropriate plans for each [ 253;254]. Moreover, as tasks progress,
agents can employ introspection to modify their plans, ensuring they align better with real-world
circumstances, leading to adaptive and successful task execution. Typically, planning comprises two stages: plan formulation and plan reflection.
•Plan formulation. During the process of plan formulation, agents generally decompose an
overarching task into numerous sub-tasks, and various approaches have been proposed in this phase. Notably, some works advocate for LLM-based agents to decompose problems comprehensively in
one go, formulating a complete plan at once and then executing it sequentially [ 98;179;255;256]. In contrast, other studies like the CoT-series employ an adaptive strategy, where they plan and
address sub-tasks one at a time, allowing for more fluidity in handling intricate tasks in their entirety
[95;96;257]. Additionally, some methods emphasize hierarchical planning [ 182;185], while
others underscore a strategy in which final plans are derived from reasoning steps structured in
a tree-like format. The latter approach argues that agents should assess all possible paths before
finalizing a plan [ 97;181;184;258;184]. While LLM-based agents demonstrate a broad scope of
general knowledge, they can occasionally face challenges when tasked with situations that require
expertise knowledge. Enhancing these agents by integrating them with planners of specific domains
has shown to yield better performance [125; 130; 186; 259].
•Plan reflection. Upon formulating a plan, it’s imperative to reflect upon and evaluate its merits. LLM-based agents leverage internal feedback mechanisms, often drawing insights from pre-existing
models, to hone and enhance their strategies and planning approaches [ 169;178;188;192]. To
better align with human values and preferences, agents actively engage with humans, allowing
them to rectify some misunderstandings and assimilate this tailored feedback into their planning
methodology [ 108;189;190]. Furthermore, they could draw feedback from tangible or virtual
surroundings, such as cues from task accomplishments or post-action observations, aiding them in
revising and refining their plans [91; 101; 187; 191; 260].
15

--- PAGE 16 ---
3.1.5 Transferability and Generalization
Intelligence shouldn’t be limited to a specific domain or task, but rather encompass a broad range
of cognitive skills and abilities [ 31]. The remarkable nature of the human brain is largely attributed
to its high degree of plasticity and adaptability. It can continuously adjust its structure and function
in response to external stimuli and internal needs, thereby adapting to different environments and
tasks. These years, plenty of research indicates that pre-trained models on large-scale corpora can
learn universal language representations [ 36;261;262]. Leveraging the power of pre-trained models,
with only a small amount of data for fine-tuning, LLMs can demonstrate excellent performance in
downstream tasks [ 263]. There is no. need to train new models from scratch, which saves a lot of
computation resources. However, through this task-specific fine-tuning, the models lack versatility
and struggle to be generalized to other tasks. Instead of merely functioning as a static knowledge
repository, LLM-based agents exhibit dynamic learning ability which enables them to adapt to novel
tasks swiftly and robustly [24; 105; 106]. Unseen task generalization. Studies show that instruction-tuned LLMs exhibit zero-shot gener-
alization without the need for task-specific fine-tuning [ 24;25;105;106;107]. With the expansion
of model size and corpus size, LLMs gradually exhibit remarkable emergent abilities in unfamiliar
tasks [ 132]. Specifically, LLMs can complete new tasks they do not encounter in the training stage by
following the instructions based on their own understanding. One of the implementations is multi-task
learning, for example, FLAN [ 105] finetunes language models on a collection of tasks described via
instructions, and T0 [ 106] introduces a unified framework that converts every language problem into
a text-to-text format. Despite being purely a language model, GPT-4 [ 25] demonstrates remarkable
capabilities in a variety of domains and tasks, including abstraction, comprehension, vision, coding,
mathematics, medicine, law, understanding of human motives and emotions, and others [ 31]. It is
noticed that the choices in prompting are critical for appropriate predictions, and training directly on
the prompts can improve the models’ robustness in generalizing to unseen tasks [ 264]. Promisingly,
such generalization capability can further be enhanced by scaling up both the model size and the
quantity or diversity of training instructions [94; 265]. In-context learning. Numerous studies indicate that LLMs can perform a variety of complex
tasks through in-context learning (ICL), which refers to the models’ ability to learn from a few
examples in the context [ 195]. Few-shot in-context learning enhances the predictive performance
of language models by concatenating the original input with several complete examples as prompts
to enrich the context [ 41]. The key idea of ICL is learning from analogy, which is similar to the
learning process of humans [ 266]. Furthermore, since the prompts are written in natural language,
the interaction is interpretable and changeable, making it easier to incorporate human knowledge into
LLMs [ 95;267]. Unlike the supervised learning process, ICL doesn’t involve fine-tuning or parameter
updates, which could greatly reduce the computation costs for adapting the models to new tasks. Beyond text, researchers also explore the potential ICL capabilities in different multimodal tasks
[193;194;268;269;270;271], making it possible for agents to be applied to large-scale real-world
tasks. Continual learning. Recent studies [ 190;272] have highlighted the potential of LLMs’ planning
capabilities in facilitating continuous learning [ 196;197] for agents, which involves continuous
acquisition and update of skills. A core challenge in continual learning is catastrophic forgetting
[273]: as a model learns new tasks, it tends to lose knowledge from previous tasks. Numerous efforts
have been devoted to addressing the above challenge, which can be broadly separated into three
groups, introducing regularly used terms in reference to the previous model [ 274;275;276;277],
approximating prior data distributions [ 278;279;280], and designing architectures with task-adaptive
parameters [ 281;198]. LLM-based agents have emerged as a novel paradigm, leveraging the planning
capabilities of LLMs to combine existing skills and address more intricate challenges. V oyager [ 190]
attempts to solve progressively harder tasks proposed by the automatic curriculum devised by GPT-4
[25]. By synthesizing complex skills from simpler programs, the agent not only rapidly enhances its
capabilities but also effectively counters catastrophic forgetting.
16

--- PAGE 17 ---
PerceptionTextual Input §3.2.1
Visual Input §3.2.2Visual encoderViT [ 282], VQV AE [ 283], Mobile-
ViT [ 284], MLP-Mixer [ 285], etc.
Learnable
architectureQuery basedKosmos [ 286], BLIP-2 [ 287], In-
structBLIP [ 288], MultiModal-
GPT [ 289], Flamingo [ 290], etc.
Projection basedPandaGPT [ 291], LLaV A
[292], Minigpt-4 [ 118], etc.
Auditory Input §3.2.3Cascading manner AudioGPT [ 293], HuggingGPT [ 180], etc.
Transfer
visual methodAST [ 294], HuBERT [ 295] , X-LLM
[296], Video-LLaMA [ 297], etc.
Other Input §3.2.4 InternGPT [ 298], etc.
Figure 4: Typology of the perception module.
3.2 Perception
Both humans and animals rely on sensory organs like eyes and ears to gather information from their
surroundings. These perceptual inputs are converted into neural signals and sent to the brain for
processing [ 299;300], allowing us to perceive and interact with the world. Similarly, it’s crucial
for LLM-based agents to receive information from various sources and modalities. This expanded
perceptual space helps agents better understand their environment, make informed decisions, and
excel in a broader range of tasks, making it an essential development direction. Agent handles this
information to the Brain module for processing through the perception module. In this section, we introduce how to enable LLM-based agents to acquire multimodal perception
capabilities, encompassing textual (§ 3.2.1), visual (§ 3.2.2), and auditory inputs (§ 3.2.3). We also
consider other potential input forms (§ 3.2.4) such as tactile feedback, gestures, and 3D maps to
enrich the agent’s perception domain and enhance its versatility.3). The typology diagram for the
LLM-based agent perception is depicted in Figure 4.
3.2.1 Textual Input
Text is a way to carry data, information, and knowledge, making text communication one of the most
important ways humans interact with the world. An LLM-based agent already has the fundamental
ability to communicate with humans through textual input and output [ 114]. In a user’s textual
input, aside from the explicit content, there are also beliefs, desires, and intentions hidden behind
it. Understanding implied meanings is crucial for the agent to grasp the potential and underlying
intentions of human users, thereby enhancing its communication efficiency and quality with users. However, as discussed in § 3.1.1, understanding implied meanings within textual input remains
challenging for the current LLM-based agent. For example, some works [ 128;218;219;220] employ
reinforcement learning to perceive implied meanings and models feedback to derive rewards. This
helps deduce the speaker’s preferences, leading to more personalized and accurate responses from
the agent. Additionally, as the agent is designed for use in complex real-world situations, it will
inevitably encounter many entirely new tasks. Understanding text instructions for unknown tasks
places higher demands on the agent’s text perception abilities. As described in § 3.1.5, an LLM that
has undergone instruction tuning [ 105] can exhibit remarkable zero-shot instruction understanding
and generalization abilities, eliminating the need for task-specific fine-tuning.
3.2.2 Visual Input
Although LLMs exhibit outstanding performance in language comprehension [ 25;301] and multi-turn
conversations [ 302], they inherently lack visual perception and can only understand discrete textual
content. Visual input usually contains a wealth of information about the world, including properties
of objects, spatial relationships, scene layouts, and more in the agent’s surroundings. Therefore,
integrating visual information with data from other modalities can offer the agent a broader context
and a more precise understanding [120], deepening the agent’s perception of the environment. To help the agent understand the information contained within images, a straightforward approach
is to generate corresponding text descriptions for image inputs, known as image captioning [ 303;
304;305;306;307]. Captions can be directly linked with standard text instructions and fed into
the agent. This approach is highly interpretable and doesn’t require additional training for caption
generation, which can save a significant number of computational resources. However, caption
17

--- PAGE 18 ---
generation is a low-bandwidth method [ 120;308], and it may lose a lot of potential information
during the conversion process. Furthermore, the agent’s focus on images may introduce biases. Inspired by the excellent performance of transformers [ 309] in natural language processing, re-
searchers have extended their use to the field of computer vision. Representative works like
ViT/VQV AE [ 282;283;284;285;310] have successfully encoded visual information using trans-
formers. Researchers first divide an image into fixed-size patches and then treat these patches, after
linear projection, as input tokens for Transformers [ 292]. In the end, by calculating self-attention
between tokens, they are able to integrate information across the entire image, resulting in a highly
effective way to perceive visual content. Therefore, some works [ 311] try to combine the image
encoder and LLM directly to train the entire model in an end-to-end way. While the agent can achieve
remarkable visual perception abilities, it comes at the cost of substantial computational resources. Extensively pre-trained visual encoders and LLMs can greatly enhance the agent’s visual perception
and language expression abilities [ 286;312]. Freezing one or both of them during training is a
widely adopted paradigm that achieves a balance between training resources and model performance
[287]. However, LLMs cannot directly understand the output of a visual encoder, so it’s necessary
to convert the image encoding into embeddings that LLMs can comprehend. In other words, it
involves aligning the visual encoder with the LLM. This usually requires adding an extra learnable
interface layer between them. For example, BLIP-2 [ 287] and InstructBLIP [ 288] use the Querying
Transformer(Q-Former) module as an intermediate layer between the visual encoder and the LLM
[288]. Q-Former is a transformer that employs learnable query vectors [ 289], giving it the capability
to extract language-informative visual representations. It can provide the most valuable information
to the LLM, reducing the agent’s burden of learning visual-language alignment and thereby mitigating
the issue of catastrophic forgetting. At the same time, some researchers adopt a computationally
efficient method by using a single projection layer to achieve visual-text alignment, reducing the need
for training additional parameters [ 118;291;312]. Moreover, the projection layer can effectively
integrate with the learnable interface to adapt the dimensions of its outputs, making them compatible
with LLMs [296; 297; 313; 314]. Video input consists of a series of continuous image frames. As a result, the methods used by
agents to perceive images [ 287] may be applicable to the realm of videos, allowing the agent to have
good perception of video inputs as well. Compared to image information, video information adds
a temporal dimension. Therefore, the agent’s understanding of the relationships between different
frames in time is crucial for perceiving video information. Some works like Flamingo [ 290;315]
ensure temporal order when understanding videos using a mask mechanism. The mask mechanism
restricts the agent’s view to only access visual information from frames that occurred earlier in time
when it perceives a specific frame in the video.
3.2.3 Auditory Input
Undoubtedly, auditory information is a crucial component of world information. When an agent
possesses auditory capabilities, it can improve its awareness of interactive content, the surrounding
environment, and even potential dangers. Indeed, there are numerous well-established models and
approaches [ 293;316;317] for processing audio as a standalone modality. However, these models
often excel at specific tasks. Given the excellent tool-using capabilities of LLMs (which will be
discussed in detail in §3.3), a very intuitive idea is that the agent can use LLMs as control hubs,
invoking existing toolsets or model repositories in a cascading manner to perceive audio information. For instance, AudioGPT [ 293], makes full use of the capabilities of models like FastSpeech [ 317],
GenerSpeech [ 316], Whisper [ 316], and others [ 318;319;320;321;322] which have achieved
excellent results in tasks such as Text-to-Speech, Style Transfer, and Speech Recognition. An audio spectrogram provides an intuitive representation of the frequency spectrum of an audio signal
as it changes over time [ 323]. For a segment of audio data over a period of time, it can be abstracted
into a finite-length audio spectrogram. An audio spectrogram has a 2D representation, which can
be visualized as a flat image. Hence, some research [ 294;295] efforts aim to migrate perceptual
methods from the visual domain to audio. AST (Audio Spectrogram Transformer) [ 294] employs a
Transformer architecture similar to ViT to process audio spectrogram images. By segmenting the
audio spectrogram into patches, it achieves effective encoding of audio information. Moreover, some
researchers [ 296;297] have drawn inspiration from the idea of freezing encoders to reduce training
18

--- PAGE 19 ---
time and computational costs. They align audio encoding with data encoding from other modalities
by adding the same learnable interface layer.
3.2.4 Other Input
As mentioned earlier, many studies have looked into perception units for text, visual, and audio. However, LLM-based agents might be equipped with richer perception modules. In the future, they
could perceive and understand diverse modalities in the real world, much like humans. For example,
agents could have unique touch and smell organs, allowing them to gather more detailed information
when interacting with objects. At the same time, agents can also have a clear sense of the temperature,
humidity, and brightness in their surroundings, enabling them to take environment-aware actions. Moreover, by efficiently integrating basic perceptual abilities like vision, text, and light sensitivity,
agents can develop various user-friendly perception modules for humans. InternGPT [ 298] introduces
pointing instructions. Users can interact with specific, hard-to-describe portions of an image by using
gestures or moving the cursor to select, drag, or draw. The addition of pointing instructions helps
provide more precise specifications for individual text instructions. Building upon this, agents have
the potential to perceive more complex user inputs. For example, technologies such as eye-tracking
in AR/VR devices, body motion capture, and even brainwave signals in brain-computer interaction. Finally, a human-like LLM-based agent should possess awareness of a broader overall environment. At present, numerous mature and widely adopted hardware devices can assist agents in accomplishing
this. Lidar [ 324] can create 3D point cloud maps to help agents detect and identify objects in their
surroundings. GPS [ 325] can provide accurate location coordinates and can be integrated with map
data. Inertial Measurement Units (IMUs) can measure and record the three-dimensional motion
of objects, offering details about an object’s speed and direction. However, these sensory data are
complex and cannot be directly understood by LLM-based agents. Exploring how agents can perceive
more comprehensive input is a promising direction for the future.
3.3 Action
ActionTextual Output §3.3.1
Tools §3.3.2Learning toolsToolformer [ 92], TALM [ 326], Instruct-
GPT [ 24], Clarebout et al. [ 327], etc.
Using toolsWebGPT [ 90], OpenAGI [ 211], Visual
ChatGPT [ 328], SayCan [ 179], etc.
Making toolsLATM [ 329], CREATOR [ 330],
SELF-DEBUGGING [ 331], etc.
Embodied
Action §3.3.3LLM-based
Embodied actionsSayCan [ 179], EmbodiedGPT [ 121],
InstructRL [ 332], Lynch et al. [ 333],
V oyager [ 190], AlphaBlock [ 334], DEPS
[183], LM-Nav [ 335], NavGPT [ 336], etc.
Prospective to the
embodied actionMineDojo [ 337], Kanitscheider et al. [ 338],
DECKARD [ 339], Sumers et al. [ 340], etc.
Figure 5: Typology of the action module. After humans perceive their environment, their brains integrate, analyze, and reason with the perceived
information and make decisions. Subsequently, they employ their nervous systems to control their
bodies, enabling adaptive or creative actions in response to the environment, such as engaging in
conversation, evading obstacles, or starting a fire.