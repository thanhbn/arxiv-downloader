[CONTEXT from previous chunk - for reference only]
Threat to the well-being of the human race. Apart from the potential unemployment crisis, as
AI agents continue to evolve, humans (including developers) might struggle to comprehend, predict,
or reliably control them [ 654]. If these agents advance to a level of intelligence surpassing human
capabilities and develop ambitions, they could potentially attempt to seize control of the world,
resulting in irreversible consequences for humanity, akin to Skynet from the Terminator movies.
[NEW CONTENT to translate]
As stated by Isaac Asimov’s Three Laws of Robotics [ 655], we aspire for LLM-based agents to
refrain from harming humans and to obey human commands. Hence, guarding against such risks
to humanity, researchers must comprehensively comprehend the operational mechanisms of these
potent LLM-based agents before their development [ 656]. They should also anticipate the potential
direct or indirect impacts of these agents and devise approaches to regulate their behavior.
6.4 Scaling Up the Number of Agents
As mentioned in § 4 and § 5, multi-agent systems based on LLMs have demonstrated superior
performance in task-oriented applications and have been able to exhibit a range of social phenomena
in simulation. However, current research predominantly involves a limited number of agents, and
very few efforts have been made to scale up the number of agents to create more complex systems
or simulate larger societies [ 207;657]. In fact, scaling up the number of agents can introduce
greater specialization to accomplish more complex and larger-scale tasks, significantly improving
task efficiency, such as in software development tasks or government policy formulation [ 109]. Addi-
tionally, increasing the number of agents in social simulations enhances the credibility and realism of
such simulations [ 22]. This enables humans to gain insights into the functioning, breakdowns, and
potential risks of societies; it also allows for interventions in societal operations through customized
approaches to observe how specific conditions, such as the occurrence of black swan events, affect
the state of society. Through this, humans can draw better experiences and insights to improve the
harmony of real-world societies.
45

--- PAGE 46 ---
Pre-determined scaling. One very intuitive and simple way to scale up the number of agents is for
the designer to pre-determine it [ 108;412]. Specifically, by pre-determining the number of agents,
their respective roles and attributes, the operating environment, and the objectives, designers can allow
agents to autonomously interact, collaborate, or engage in other activities to achieve the predefined
common goals. Some research has explored increasing the number of agents in the system in this
pre-determined manner, resulting in efficiency advantages, such as faster and higher-quality task
completion, and the emergence of more social phenomena in social simulation scenarios [ 22;410]. However, this static approach becomes limiting when tasks or objectives evolve. As tasks grow more
intricate or the diversity of social participants increases, expanding the number of agents may be
needed to meet goals, while reducing agents could be essential for managing computational resources
and minimizing waste. In such instances, the system must be manually redesigned and restarted by
the designer. Dynamic scaling. Another viable approach to scaling the number of agents is through dynamic
adjustments [ 409;410]. In this scenario, the agent count can be altered without halting system
operations. For instance, in a software development task, if the original design only included
requirements engineering, coding, and testing, one can increase the number of agents to handle
steps like architectural design and detailed design, thereby improving task quality. Conversely, if
there are excessive agents during a specific step, like coding, causing elevated communication costs
without delivering substantial performance improvements compared to a smaller agent count, it may
be essential to dynamically remove some agents to prevent resource waste. Furthermore, agents can autonomously increase the number of agents [ 409] themselves to distribute
their workload, ease their own burden, and achieve common goals more efficiently. Of course, when
the workload becomes lighter, they can also reduce the number of agents delegated to their tasks
to save system costs. In this approach, the designer merely defines the initial framework, granting
agents greater autonomy and self-organization, making the entire system more autonomous and
self-organized. Agents can better manage their workload under evolving conditions and demands,
offering greater flexibility and scalability. Potential challenges. While scaling up the number of agents can lead to improved task efficiency
and enhance the realism and credibility of social simulations [ 22;109;520], there are several
challenges ahead of us. For example, the computational burden will increase with the large number
of deployed AI agents, calling for better architectural design and computational optimization to
ensure the smooth running of the entire system. For example, as the number of agents increases, the
challenges of communication and message propagation become quite formidable. This is because the
communication network of the entire system becomes highly complex. As previously mentioned in §
5.3.3, in multi-agent systems or societies, there can be biases in information dissemination caused
by hallucinations, misunderstandings, and the like, leading to distorted information propagation. A
system with more agents could amplify this risk, making communication and information exchange
less reliable [ 405]. Furthermore, the difficulty of coordinating agents also magnifies with the increase
in their numbers, potentially making cooperation among agents more challenging and less efficient,
which can impact the progress towards achieving common goals. Therefore, the prospect of constructing a massive, stable, continuous agent system that faithfully
replicates human work and life scenarios has become a promising research avenue. An agent with the
ability to operate stably and perform tasks in a society comprising hundreds or even thousands of
agents is more likely to find applications in real-world interactions with humans in the future.
6.5 Open Problems
In this section, we discuss several open problems related to the topic of LLM-based agents. The debate over whether LLM-based agents represent a potential path to AGI.6Artificial
General Intelligence (AGI), also known as Strong AI, has long been the ultimate pursuit of humanity
in the field of artificial intelligence, often referenced or depicted in many science fiction novels and
films. There are various definitions of AGI, but here we refer to AGI as a type of artificial intelligence
6Note that the relevant debates are still ongoing, and the references here may include the latest viewpoints,
technical blogs, and literature.
46

--- PAGE 47 ---
that demonstrates the ability to understand, learn, and apply knowledge across a wide range of tasks
and domains, much like a human being [ 31;658]. In contrast, Narrow AI is typically designed for
specific tasks such as Go and Chess and lacks the broad cognitive abilities associated with human
intelligence. Currently, whether large language models are a potential path to achieving AGI remains
a highly debated and contentious topic [659; 660; 661; 662]. Given the breadth and depth of GPT-4’s capabilities, some researchers (referred to as proponents)
believe that large language models represented by GPT-4 can serve as early versions of AGI systems
[31]. Following this line of thought, constructing agents based on LLMs has the potential to bring
about more advanced versions of AGI systems. The main support for this argument lies in the idea
that as long as they can be trained on a sufficiently large and diverse set of data that are projections of
the real world, encompassing a rich array of tasks, LLM-based agents can develop AGI capabilities. Another interesting argument is that the act of autoregressive language modeling itself brings about
compression and generalization abilities: just as humans have emerged with various peculiar and
complex phenomena during their survival, language models, in the process of simply predicting the
next token, also achieve an understanding of the world and the reasoning ability [579; 660; 663]. However, another group of individuals (referred to as opponents) believes that constructing agents
based on LLMs cannot develop true Strong AI [ 664]. Their primary argument centers around
the notion that LLMs, relying on autoregressive next-token prediction, cannot generate genuine
intelligence because they do not simulate the true human thought process and merely provide
reactive responses [ 660]. Moreover, LLMs also do not learn how the world operates by observing
or experiencing it, leading to many foolish mistakes. They contend that a more advanced modeling
approach, such as a world model [665], is necessary to develop AGI. We cannot definitively determine which viewpoint is correct until true AGI is achieved, but we believe
that such discussions and debates are beneficial for the overall development of the community. From virtual simulated environment to physical environment. As mentioned earlier, there
is a significant gap between virtual simulation environments and the real physical world: Virtual
environments are scenes-constrained, task-specific, and interacted with in a simulated manner [ 391;
666], while real-world environments are boundless, accommodate a wide range of tasks, and interacted
with in a physical manner. Therefore, to bridge this gap, agents must address various challenges
stemming from external factors and their own capabilities, allowing them to effectively navigate and
operate in the complex physical world. First and foremost, a critical issue is the need for suitable hardware support when deploying the
agent in a physical environment. This places high demands on the adaptability of the hardware. In a
simulated environment, both the perception and action spaces of an agent are virtual. This means
that in most cases, the results of the agent’s operations, whether in perceiving inputs or generating
outputs, can be guaranteed [ 395]. However, when an agent transitions to a real physical environment,
its instructions may not be well executed by hardware devices such as sensors or robotic arms,
significantly affecting the agent’s task efficiency. Designing a dedicated interface or conversion
mechanism between the agent and the hardware device is feasible. However, it can pose challenges
to the system’s reusability and simplicity. In order to make this leap, the agent needs to have enhanced environmental generalization capabilities. To integrate seamlessly into the real physical world, they not only need to understand and reason
about ambiguous instructions with implied meanings [ 128] but also possess the ability to learn and
apply new skills flexibly [ 190;592]. Furthermore, when dealing with an infinite and open world, the
agent’s limited context also poses significant challenges [ 236;667]. This determines whether the
agent can effectively handle a vast amount of information from the world and operate smoothly. Finally, in a simulated environment, the inputs and outputs of the agent are virtual, allowing for
countless trial and error attempts [ 432]. In such a scenario, the tolerance level for errors is high and
does not lead to actual harm. However, in a physical environment, the agent’s improper behavior or
errors may cause real and sometimes irreversible harm to the environment. As a result, appropriate
regulations and standards are highly necessary. We need to pay attention to the safety of agents when
it comes to making decisions and generating actions, ensuring they do not pose threats or harm to the
real world.
47

--- PAGE 48 ---
Collective intelligence in AI agents. What magical trick drives our intelligence. The reality is,
there’s no. magic to it. As Marvin Minsky eloquently expressed in “The Society of Mind” [ 442],
the power of intelligence originates from our immense diversity, not from any singular, flawless
principle. Often, decisions made by an individual may lack the precision seen in decisions formed by
the majority. Collective intelligence is a kind of shared or group intelligence, a process where the
opinions of many are consolidated into decisions. It arises from the collaboration and competition
amongst various entities. This intelligence manifests in bacteria, animals, humans, and computer
networks, appearing in various consensus-based decision-making patterns. Creating a society of agents does not necessarily guarantee the emergence of collective intelligence
with an increasing number of agents. Coordinating individual agents effectively is crucial to mitigate
“groupthink” and individual cognitive biases, enabling cooperation and enhancing intellectual perfor-
mance within the collective. By harnessing communication and evolution within an agent society,
it becomes possible to simulate the evolution observed in biological societies, conduct sociological
experiments, and gain insights that can potentially advance human society. Agent as a Service / LLM-based Agent as a Service. With the development of cloud computing,
the concept of XaaS (everything as a Service) has garnered widespread attention [ 668]. This business
model has brought convenience and cost savings to small and medium-sized enterprises or individuals
due to its availability and scalability, lowering the barriers to using computing resources. For example,
they can rent infrastructure on a cloud service platform without the need to buy computational
machines and build their own data centers, saving a significant amount of manpower and money. This
approach is known as Infrastructure as a Service (IaaS) [ 669;670]. Similarly, cloud service platforms
also provide basic platforms (Platform as a Service, PaaS) [ 671;672], and specific business software
(Software as a Service, SaaS) [673; 674], and more. As language models have scaled up in size, they often appear as black boxes to users. Therefore,
users construct prompts to query models through APIs, a method referred to as Language Model
as a Service (LMaaS) [ 675]. Similarly, because LLM-based agents are more complex than LLMs
and are more challenging for small and medium-sized enterprises or individuals to build locally,
organizations that possess these agents may consider offering them as a service, known as Agent as a
Service (AaaS) or LLM-based Agent as a Service (LLMAaaS). Like other cloud services, AaaS can
provide users with flexibility and on-demand service. However, it also faces many challenges, such
as data security and privacy issues, visibility and controllability issues, and cloud migration issues,
among others. Additionally, due to the uniqueness and potential capabilities of LLM-based agents, as
mentioned in § 6.3, their robustness, trustworthiness, and concerns related to malicious use need to
be considered before offering them as a service to customers.
7 Conclusion
This paper provides a comprehensive and systematic overview of LLM-based agents, discussing
the potential challenges and opportunities in this flourishing field. We begin with a philosophical
perspective, elucidating the origin and definition of agent, it evolution in the field of AI, and why
LLMs are suited to serve as the main part of the brain of agents. Motivated by these background
information, we present a general conceptual framework for LLM-based agents, comprising three
main components: the brain, perception, and action. Next, we introduce the wide-ranging applications
of LLM-based agents, including single-agent applications, multi-agent systems, and human-agent
collaboration. Furthermore, we move beyond the notion of agents merely as assistants, exploring their
social behavior and psychological activities, and situating them within simulated social environments
to observe emerging social phenomena and insights for humanity. Finally, we engage in discussions
and offer a glimpse into the future, touching upon the mutual inspiration between LLM research and
agent research, the evaluation of LLM-based agents, the risks associated with them, the opportunities
in scaling the number of agents, and some open problems like Agent as a Service and whether
LLM-based agents represent a potential path to AGI. We hope our efforts can provide inspirations to
the community and facilitate research in related fields.
48

--- PAGE 49 ---
Acknowledgements
Thanks to Professor Guoyu Wang for carefully reviewing the ethics of the article. Thanks to Jinzhu
Xiong for her excellent drawing skills to present an amazing performance of Figure 1. References
[1] Russell, S. Artificial intelligence a modern approach. Pearson Education, Inc., 2010.
[2] Diderot, D. Diderot’s early philosophical works . 4. Open Court, 1911.
[3] Turing, A. Computing machinery and intelligence. Springer, 2009.
[4]Wooldridge, M. Intelligent agents: theory and practice. Rev. ,
10(2):115–152, 1995.
[5]Schlosser, M. Zalta, ed., The Stanford Encyclopedia of Philosophy. Meta-
physics Research Lab, Stanford University, Winter 2019 edn., 2019.
[6]Agha, G. Actors: a Model of Concurrent Computation in Distributed Systems (Parallel
Processing, Semantics, Open, Programming Languages, Artificial Intelligence). Ph.D. thesis,
University of Michigan, USA, 1985.
[7]Green, S., L. Nangle, et al. Software agents: A review. Department of Computer
Science, Trinity College Dublin, Tech. TCS-CS-1997-06 , 1997.
[8] Genesereth, M. Software agents. ACM , 37(7):48–53, 1994.
[9] Goodwin, R. Formalizing properties of agents. Comput. , 5(6):763–781, 1995.
[10] Padgham, L., M. Developing intelligent agent systems: A practical guide. John
Wiley & Sons, 2005.
[11] Shoham, Y. Agent oriented programming. Pólos, eds., Knowledge Repre-
sentation and Reasoning Under Uncertainty, Logic at Work [International Conference Logic
at Work, Amsterdam, The Netherlands, December 17-19, 1992] , vol. 808 of Lecture Notes in
Computer Science , pages 123–129. Springer, 1992.
[12] Hutter, M. Universal artificial intelligence: Sequential decisions based on algorithmic
probability. Springer Science & Business Media, 2004.
[13] Fikes, R., N. STRIPS: A new approach to the application of theorem proving to
problem solving. Cooper, ed., Proceedings of the 2nd International Joint Confer-
ence on Artificial Intelligence. London, UK, September 1-3, 1971 , pages 608–620. William
Kaufmann, 1971.
[14] Sacerdoti, E. Planning in a hierarchy of abstraction spaces. Nilsson, ed., Proceedings
of the 3rd International Joint Conference on Artificial Intelligence. Standford, CA, USA, August
20-23, 1973 , pages 412–422. William Kaufmann, 1973.
[15] Brooks, R. Intelligence without representation. Artificial intelligence , 47(1-3):139–159,
1991.
[16] Maes, P. Designing autonomous agents: Theory and practice from biology to engineering and
back. MIT press, 1990.
[17] Ribeiro, C. Reinforcement learning agents. Artificial intelligence review , 17:223–250, 2002.
[18] Kaelbling, L. Littman, A. Reinforcement learning: A survey. Journal of
artificial intelligence research , 4:237–285, 1996.
[19] Guha, R. Enabling agents to work together. Communications of the ACM ,
37(7):126–142, 1994.
49

--- PAGE 50 ---
[20] Kaelbling, L. P., et al. An architecture for intelligent reactive systems. Reasoning about
actions and plans , pages 395–410, 1987.
[21] Sutton, R. Reinforcement learning: An introduction. MIT press, 2018.
[22] Park, J. O’Brien, C. Cai, et al. Generative agents: Interactive simulacra of human
behavior. CoRR , abs/2304.03442, 2023.
[23] Wang, Z., G. Yang, et al. Interactive natural language processing. CoRR ,
abs/2305.13246, 2023.
[24] Ouyang, L., J. Jiang, et al. Training language models to follow instructions with human
feedback. In NeurIPS . 2022.
[25] OpenAI. GPT-4 technical report. CoRR , abs/2303.08774, 2023.
[26] Wei, J., Y. Bommasani, et al. Emergent abilities of large language models. Res. , 2022, 2022.
[27] Liu, R., R. Jia, et al. Training socially aligned language models in simulated human
society. CoRR , abs/2305.16960, 2023.
[28] Sumers, T. Narasimhan, et al. Cognitive architectures for language agents. CoRR , abs/2309.02427, 2023.
[29] Weng, L. Llm-powered autonomous agents. lilianweng.github.io , 2023.
[30] Bisk, Y ., A. Holtzman, J. Thomason, et al. Experience grounds language. Liu, eds., Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 , pages 8718–
8735. Association for Computational Linguistics, 2020.
[31] Bubeck, S., V. Chandrasekaran, R. Eldan, et al. Sparks of artificial general intelligence: Early
experiments with GPT-4. CoRR , abs/2303.12712, 2023.
[32] Anscombe, G. Harvard University Press, 2000.
[33] Davidson, D. Actions, reasons, and causes. The Journal of Philosophy , 60(23):685–700, 1963.
[34] —. Bronaugh, R. Binkley, eds., Agent, Action, and Reason ,
pages 1–37. University of Toronto Press, 1971.
[35] Dennett, D. Précis of the intentional stance. Behavioral and brain sciences , 11(3):495–505,
1988.
[36] Barandiaran, X. Di Paolo, M. Defining agency: Individuality, normativity,
asymmetry, and spatio-temporality in action. Adaptive Behavior , 17(5):367–386, 2009.
[37] McCarthy, J. Ascribing mental qualities to machines. Stanford University. Computer Science
Department, 1979.
[38] Rosenschein, S. The synthesis of digital machines with provable epistemic
properties. In Theoretical aspects of reasoning about knowledge , pages 83–98. Elsevier, 1986.
[39] Radford, A., K. Narasimhan, T. Salimans, et al. Improving language understanding by
generative pre-training. OpenAI , 2018.
[40] Radford, A., J. Child, et al. Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[41] Brown, T. Ryder, et al. Language models are few-shot learners. Larochelle, M. Ranzato, R. Hadsell, M. Lin, eds., Advances in Neural In-
formation Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual . 2020.
50

--- PAGE 51 ---
[42] Lin, C., A. Li, et al. Limitations of autoregressive models and their alternatives. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tür, I. Beltagy, S. Bethard,
R. Cotterell, T. Chakraborty, Y. Zhou, eds., Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 , pages 5147–5173. Association for
Computational Linguistics, 2021.
[43] Tomasello, M. Constructing a language: A usage-based theory of language acquisition. Harvard university press, 2005.
[44] Bloom, P. How children learn the meanings of words. MIT press, 2002.
[45] Zwaan, R. Embodied sentence comprehension. Grounding cognition: The
role of perception and action in memory, language, and thinking , 22, 2005.
[46] Andreas, J. Language models as agent models. Goldberg, Z. Kozareva, Y. Zhang, eds.,
Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United
Arab Emirates, December 7-11, 2022 , pages 5769–5779. Association for Computational
Linguistics, 2022.
[47] Wong, L., G. Lew, et al. From word models to world models: Translating from
natural language to the probabilistic language of thought. CoRR , abs/2306.12672, 2023.
[48] Radford, A., R. Józefowicz, I. Learning to generate reviews and discovering
sentiment. CoRR , abs/1704.01444, 2017.
[49] Li, B. Implicit representations of meaning in neural language models. Navigli, eds., Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event,
August 1-6, 2021 , pages 1813–1827. Association for Computational Linguistics, 2021.
[50] Mukhopadhyay, U., L. Stephens, M. Huhns, et al. An intelligent system for document
retrieval in distributed office environments. Sci. , 37(3):123–135, 1986.
[51] Maes, P. Situated agents can have goals. Robotics Auton. Syst. , 6(1-2):49–70, 1990.
[52] Nilsson, N. Toward agent programs with circuit semantics. Tech. rep., 1992.
[53] Müller, J. Modelling interacting agents in dynamic environments. In Proceed-
ings of the 11th European Conference on Artificial Intelligence , pages 709–713. 1994.
[54] Brooks, R. A robust layered control system for a mobile robot. IEEE journal on robotics and
automation , 2(1):14–23, 1986.
[55] Brooks, R. Intelligence without reason. In The artificial life route to artificial intelligence ,
pages 25–81. Routledge, 2018.
[56] Newell, A., H. Computer science as empirical inquiry: Symbols and search. ACM , 19(3):113–126, 1976.
[57] Ginsberg, M. Essentials of Artificial Intelligence. Morgan Kaufmann, 1993.
[58] Wilkins, D. Practical planning - extending the classical AI planning paradigm. Morgan
Kaufmann series in representation and reasoning. Morgan Kaufmann, 1988.
[59] Shardlow, N. Action and agency in cognitive science. Ph.D. thesis, Master’s thesis, Department
of Psychlogy, University of Manchester, Oxford . . . , 1990.
[60] Sacerdoti, E. The nonlinear nature of plans. In Advance Papers of the Fourth International
Joint Conference on Artificial Intelligence, Tbilisi, Georgia, USSR, September 3-8, 1975 , pages
206–214. 1975.
[61] Russell, S. Do the right thing: studies in limited rationality. MIT press, 1991.
51

--- PAGE 52 ---
[62] Schoppers, M. Universal plans for reactive robots in unpredictable environments. Mc-
Dermott, ed., Proceedings of the 10th International Joint Conference on Artificial Intelligence. Milan, Italy, August 23-28, 1987 , pages 1039–1046. Morgan Kaufmann, 1987.
[63] Brooks, R. A robust layered control system for a mobile robot. Robotics Autom. ,
2(1):14–23, 1986.
[64] Minsky, M. Steps toward artificial intelligence. Proceedings of the IRE , 49(1):8–30, 1961.
[65] Isbell, C., C. Shelton, M. Kearns, et al. A social reinforcement learning agent. In
Proceedings of the fifth international conference on Autonomous agents , pages 377–384. 2001.
[66] Watkins, C. Learning from delayed rewards, 1989.
[67] Rummery, G. On-line Q-learning using connectionist systems , vol. 37. University of Cambridge, Department of Engineering Cambridge, UK, 1994.
[68] Tesauro, G., et al. Temporal difference learning and td-gammon. Communications of the ACM ,
38(3):58–68, 1995.
[69] Li, Y. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274 , 2017.
[70] Silver, D., A. Maddison, et al. Mastering the game of go with deep neural
networks and tree search. nature , 529(7587):484–489, 2016.
[71] Mnih, V ., K. Kavukcuoglu, D. Silver, et al. Playing atari with deep reinforcement learning.
arXiv preprint arXiv:1312.5602 , 2013.
[72] Farebrother, J., M. Machado, M. Generalization and regularization in DQN. CoRR , abs/1810.00123, 2018.
[73] Zhang, C., O. Vinyals, R. Munos, et al. A study on overfitting in deep reinforcement learning. CoRR , abs/1804.06893, 2018.
[74] Justesen, N., R. Torrado, P. Bontrager, et al. Illuminating generalization in deep rein-
forcement learning through procedural level generation. arXiv preprint arXiv:1806.10729 ,
2018.
[75] Dulac-Arnold, G., N. Mankowitz, et al. Challenges of real-world reinforcement
learning: definitions, benchmarks and analysis. Learn. , 110(9):2419–2468, 2021.
[76] Ghosh, D., J. Kumar, et al. Why generalization in RL is difficult: Epistemic
pomdps and implicit partial observability. Ranzato, A. Beygelzimer, Y. Dauphin,
P. Vaughan, eds., Advances in Neural Information Processing Systems 34: Annual
Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,
2021, virtual , pages 25502–25515. 2021.
[77] Brys, T., A. Harutyunyan, M. Taylor, et al. Policy transfer using reward shaping. Bordini, E. Elkind, eds., Proceedings of the 2015 International Conference on
Autonomous Agents and Multiagent Systems, AAMAS 2015, Istanbul, Turkey, May 4-8, 2015 ,
pages 181–188. ACM, 2015.
[78] Parisotto, E., J. Salakhutdinov. Actor-mimic: Deep multitask and transfer reinforce-
ment learning. arXiv preprint arXiv:1511.06342 , 2015.
[79] Zhu, Z., K. Transfer learning in deep reinforcement learning: A survey. CoRR ,
abs/2009.07888, 2020.
[80] Duan, Y ., J. Schulman, X. Chen, et al. Rl$ˆ2$: Fast reinforcement learning via slow reinforce-
ment learning. CoRR , abs/1611.02779, 2016.
[81] Finn, C., P. Model-agnostic meta-learning for fast adaptation of deep
networks. Teh, eds., Proceedings of the 34th International Conference
on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017 , vol. 70 of
Proceedings of Machine Learning Research , pages 1126–1135. PMLR, 2017.
52

--- PAGE 53 ---
[82] Gupta, A., R. Mendonca, Y. Liu, et al. Meta-reinforcement learning of structured exploration
strategies. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, R. Gar-
nett, eds., Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal,
Canada , pages 5307–5316. 2018.
[83] Rakelly, K., A. Finn, et al. Efficient off-policy meta-reinforcement learning via
probabilistic context variables. Chaudhuri, R. Salakhutdinov, eds., Proceedings of the
36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA , vol. 97 of Proceedings of Machine Learning Research , pages 5331–5340. PMLR, 2019.
[84] Fakoor, R., P. Chaudhari, S. Soatto, et al. Meta-q-learning. arXiv preprint arXiv:1910.00125 ,
2019.
[85] Vanschoren, J. Meta-learning: A survey. arXiv preprint arXiv:1810.03548 , 2018.
[86] Taylor, M. Transfer learning for reinforcement learning domains: A survey. Res. , 10:1633–1685, 2009.
[87] Tirinzoni, A., A. Pirotta, et al. Importance weighted transfer of samples in reinforce-
ment learning. Krause, eds., Proceedings of the 35th International Conference
on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018 ,
vol. 80 of Proceedings of Machine Learning Research , pages 4943–4952. PMLR, 2018.
[88] Beck, J., R. Liu, et al. A survey of meta-reinforcement learning. CoRR ,
abs/2301.08028, 2023.
[89] Wang, L., C. Feng, et al. A survey on large language model based autonomous agents. CoRR , abs/2308.11432, 2023.
[90] Nakano, R., J. Balaji, et al. Webgpt: Browser-assisted question-answering with
human feedback. CoRR , abs/2112.09332, 2021.
[91] Yao, S., J. Yu, et al. React: Synergizing reasoning and acting in language models. InThe Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023. OpenReview.net, 2023.
[92] Schick, T., J. Dwivedi-Yu, R. Dessì, et al. Toolformer: Language models can teach themselves
to use tools. CoRR , abs/2302.04761, 2023.
[93] Lu, P., B. Cheng, et al. Chameleon: Plug-and-play compositional reasoning with
large language models. CoRR , abs/2304.09842, 2023.
[94] Qin, Y ., S. Lin, et al. Tool learning with foundation models. CoRR , abs/2304.08354,
2023.
[95] Wei, J., X. Schuurmans, et al. Chain-of-thought prompting elicits reasoning in large
language models. In NeurIPS . 2022.
[96] Kojima, T., S. Reid, et al. Large language models are zero-shot reasoners. In
NeurIPS . 2022.
[97] Wang, X., J. Schuurmans, et al. Self-consistency improves chain of thought reasoning
in language models. In The Eleventh International Conference on Learning Representations,
ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.
[98] Zhou, D., N. Schärli, L. Hou, et al. Least-to-most prompting enables complex reasoning in
large language models. In The Eleventh International Conference on Learning Representations,
ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.
[99] Xi, Z., S. Zhou, et al. Self-polish: Enhance reasoning in large language models via
problem refinement. CoRR , abs/2305.14497, 2023.
53

--- PAGE 54 ---
[100] Shinn, N., F. Cassano, B. Labash, et al. Reflexion: Language agents with verbal reinforcement
learning. arXiv preprint arXiv:2303.11366 , 2023.
[101] Song, C. Washington, et al. Llm-planner: Few-shot grounded planning for
embodied agents with large language models. CoRR , abs/2212.04088, 2022.
[102] Akyürek, A. Akyürek, A. Kalyan, et al. RL4F: generating natural language feedback
with reinforcement learning for repairing model outputs. Boyd-Graber,
N. Okazaki, eds., Proceedings of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages
7716–7733. Association for Computational Linguistics, 2023.
[103] Peng, B., M. He, et al. Check your facts and try again: Improving large language
models with external knowledge and automated feedback. CoRR , abs/2302.12813, 2023.
[104] Liu, H., C. Sferrazza, P. Languages are rewards: Hindsight finetuning using human
feedback. arXiv preprint arXiv:2302.02676 , 2023.
[105] Wei, J., M. Zhao, et al. Finetuned language models are zero-shot learners. In
The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,
April 25-29, 2022. OpenReview.net, 2022.
[106] Sanh, V ., A. Raffel, et al. Multitask prompted training enables zero-shot task
generalization. In The Tenth International Conference on Learning Representations, ICLR
2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.
[107] Chung, H. Longpre, et al. Scaling instruction-finetuned language models. CoRR , abs/2210.11416, 2022.
[108] Li, G., H. Hammoud, H. Itani, et al. CAMEL: communicative agents for "mind"
exploration of large scale language model society. CoRR , abs/2303.17760, 2023.
[109] Qian, C., X. Yang, et al. Communicative agents for software development. CoRR ,
abs/2307.07924, 2023.
[110] Boiko, D. MacKnight, G. Emergent autonomous scientific research capabilities
of large language models. CoRR , abs/2304.05332, 2023.
[111] Du, Y ., S. Torralba, et al. Improving factuality and reasoning in language models
through multiagent debate. CoRR , abs/2305.14325, 2023.
[112] Liang, T., Z. Jiao, et al. Encouraging divergent thinking in large language models
through multi-agent debate. CoRR , abs/2305.19118, 2023.
[113] Castelfranchi, C. Guarantees for autonomy in cognitive agent architecture. Wooldridge,
N. Jennings, eds., Intelligent Agents, ECAI-94 Workshop on Agent Theories, Architectures,
and Languages, Amsterdam, The Netherlands, August 8-9, 1994, Proceedings , vol. 890 of
Lecture Notes in Computer Science , pages 56–70. Springer, 1994.
[114] Gravitas, S. Auto-GPT: An Autonomous GPT-4 experiment, 2023. URL https://github.
com/Significant-Gravitas/Auto-GPT , 2023.
[115] Nakajima, Y. Python. https://github. com/yoheinakajima/babyagi , 2023.
[116] Yuan, A., A. Reif, et al. Wordcraft: Story writing with large language models. Jacucci, S. Ruotsalo, K. Gajos, eds., IUI 2022: 27th
International Conference on Intelligent User Interfaces, Helsinki, Finland, March 22 - 25,
2022 , pages 841–852. ACM, 2022.
[117] Franceschelli, G., M. On the creativity of large language models. CoRR ,
abs/2304.00008, 2023.
[118] Zhu, D., J. Shen, et al. Minigpt-4: Enhancing vision-language understanding with
advanced large language models. arXiv preprint arXiv:2304.10592 , 2023.
54

--- PAGE 55 ---
[119] Yin, S., C. Zhao, et al. A survey on multimodal large language models. CoRR ,
abs/2306.13549, 2023.
[120] Driess, D., F. Sajjadi, et al. Palm-e: An embodied multimodal language model. Brunskill, K. Engelhardt, S. Scarlett, eds., International
Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , vol.
202 of Proceedings of Machine Learning Research , pages 8469–8488. PMLR, 2023.
[121] Mu, Y ., Q. Hu, et al. Embodiedgpt: Vision-language pre-training via embodied
chain of thought. CoRR , abs/2305.15021, 2023.
[122] Brown, J. Beyond conflict monitoring: Cognitive control and the neural basis of thinking
before you act. Current Directions in Psychological Science , 22(3):179–185, 2013.
[123] Kang, J., R. Laroche, X. Yuan, et al. Think before you act: Decision transformers with internal
working memory. CoRR , abs/2305.16338, 2023.
[124] Valmeekam, K., S. Sreedharan, M. Marquez, et al. On the planning abilities of large language
models (A critical investigation with a proposed benchmark). CoRR , abs/2302.06706, 2023.
[125] Liu, B., Y. Zhang, et al. LLM+P: empowering large language models with optimal
planning proficiency. CoRR , abs/2304.11477, 2023.
[126] Liu, H., C. Sferrazza, P. Chain of hindsight aligns language models with feedback. CoRR , abs/2302.02676, 2023.
[127] Lin, Y ., Y. Llm-eval: Unified multi-dimensional automatic evaluation for open-domain
conversations with large language models. CoRR , abs/2305.13711, 2023.
[128] Lin, J., D. Klein, et al. Inferring rewards from language in context. Muresan,
P. Villavicencio, eds., Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May
22-27, 2022 , pages 8546–8560. Association for Computational Linguistics, 2022.
[129] Fu, Y ., H. Khot, et al. Improving language model negotiation with self-play and
in-context learning from AI feedback. CoRR , abs/2305.10142, 2023.
[130] Zhang, H., W. Shan, et al. Building cooperative embodied agents modularly with large
language models. CoRR , abs/2307.02485, 2023.
[131] Darwin’s, C. On the origin of species. published on , 24:1, 1859.
[132] Bang, Y ., S. Cahyawijaya, N. Lee, et al. A multitask, multilingual, multimodal evaluation of
chatgpt on reasoning, hallucination, and interactivity. CoRR , abs/2302.04023, 2023.
[133] Fang, T., S. Lan, et al. Is chatgpt a highly fluent grammatical error correction system. A comprehensive evaluation. CoRR , abs/2304.01746, 2023.
[134] Lu, A., H. Zhang, et al. Bounding the capabilities of large language models in open
text generation with prompt constraints. Vlachos, I. Augenstein, eds., Findings of the
Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023 ,
pages 1937–1963. Association for Computational Linguistics, 2023.
[135] Buehler, M. Weisswange. Theory of mind based assistive communication
in complex human robot cooperation. CoRR , abs/2109.01355, 2021.
[136] Shapira, N., M. Alavi, et al. Clever hans or neural theory of mind? stress testing
social reasoning in large language models. CoRR , abs/2305.14763, 2023.
[137] Hill, F., K. Learning distributed representations of sentences from un-
labelled data. Nenkova, O. Rambow, eds., NAACL HLT 2016, The 2016
Conference of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, San Diego California, USA, June 12-17, 2016 , pages
1367–1377. The Association for Computational Linguistics, 2016.
55

--- PAGE 56 ---
[138] Collobert, R., J. Bottou, et al. Natural language processing (almost) from scratch. Res. , 12:2493–2537, 2011.
[139] Kaplan, J., S. McCandlish, T. Henighan, et al. Scaling laws for neural language models. CoRR ,
abs/2001.08361, 2020.
[140] Roberts, A., C. How much knowledge can you pack into the parameters
of a language model. Liu, eds., Proceedings of the 2020
Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pages 5418–5426. Association for Computational Linguistics, 2020.
[141] Tandon, N., A. Varde, G. de Melo. Commonsense knowledge in machine intelligence. SIGMOD Rec. , 46(4):49–52, 2017.
[142] Vulic, I., E. Litschko, et al. Probing pretrained language models for lexical
semantics. Liu, eds., Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20,
2020 , pages 7222–7240. Association for Computational Linguistics, 2020.
[143] Hewitt, J., C. A structural probe for finding syntax in word representations. Burstein, C. Solorio, eds., Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and
Short Papers) , pages 4129–4138.