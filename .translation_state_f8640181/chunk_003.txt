[CONTEXT from previous chunk - for reference only]
Inertial Measurement Units (IMUs) can measure and record the three-dimensional motion
of objects, offering details about an object’s speed and direction. However, these sensory data are
complex and cannot be directly understood by LLM-based agents. Exploring how agents can perceive
more comprehensive input is a promising direction for the future.
3.3 Action
ActionTextual Output §3.3.1
Tools §3.3.2Learning toolsToolformer [ 92], TALM [ 326], Instruct-
GPT [ 24], Clarebout et al. [ 327], etc.
Using toolsWebGPT [ 90], OpenAGI [ 211], Visual
ChatGPT [ 328], SayCan [ 179], etc.
Making toolsLATM [ 329], CREATOR [ 330],
SELF-DEBUGGING [ 331], etc.
Embodied
Action §3.3.3LLM-based
Embodied actionsSayCan [ 179], EmbodiedGPT [ 121],
InstructRL [ 332], Lynch et al. [ 333],
V oyager [ 190], AlphaBlock [ 334], DEPS
[183], LM-Nav [ 335], NavGPT [ 336], etc.
Prospective to the
embodied actionMineDojo [ 337], Kanitscheider et al. [ 338],
DECKARD [ 339], Sumers et al. [ 340], etc.
Figure 5: Typology of the action module.
[NEW CONTENT to translate]
After humans perceive their environment, their brains integrate, analyze, and reason with the perceived
information and make decisions. Subsequently, they employ their nervous systems to control their
bodies, enabling adaptive or creative actions in response to the environment, such as engaging in
conversation, evading obstacles, or starting a fire. When an agent possesses a brain-like structure with
capabilities of knowledge, memory, reasoning, planning, and generalization, as well as multimodal
perception, it is also expected to possess a diverse range of actions akin to humans to respond to
its surrounding environment. In the construction of the agent, the action module receives action
sequences sent by the brain module and carries out actions to interact with the environment. As
Figure 5 shows, this section begins with textual output (§ 3.3.1), which is the inherent capability
of LLM-based agents. Next we talk about the tool-using capability of LLM-based agents (§ 3.3.2),
which has proved effective in enhancing their versatility and expertise. Finally, we discuss equipping
the LLM-based agent with embodied action to facilitate its grounding in the physical world (§ 3.3.3).
19

--- PAGE 20 ---
3.3.1 Textual Output
As discussed in § 3.1.1, the rise and development of Transformer-based generative large language
models have endowed LLM-based agents with inherent language generation capabilities [ 132;
213]. The text quality they generate excels in various aspects such as fluency, relevance, diversity,
controllability [ 127;214;134;216]. Consequently, LLM-based agents can be exceptionally strong
language generators.
3.3.2 Tool Using
Tools are extensions of the capabilities of tool users. When faced with complex tasks, humans employ
tools to simplify task-solving and enhance efficiency, freeing time and resources. Similarly, agents
have the potential to accomplish complex tasks more efficiently and with higher quality if they also
learn to use and utilize tools [94]. LLM-based agents have limitations in some aspects, and the use of tools can strengthen the agents’
capabilities. First, although LLM-based agents have a strong knowledge base and expertise, they
don’t have the ability to memorize every piece of training data [ 341;342]. They may also fail to steer
to correct knowledge due to the influence of contextual prompts [ 226], or even generate hallucinate
knowledge [ 208]. Coupled with the lack of corpus, training data, and tuning for specific fields and
scenarios, agents’ expertise is also limited when specializing in specific domains [ 343]. Specialized
tools enable LLMs to enhance their expertise, adapt domain knowledge, and be more suitable for
domain-specific needs in a pluggable form. Furthermore, the decision-making process of LLM-based
agents lacks transparency, making them less trustworthy in high-risk domains such as healthcare and
finance [ 344]. Additionally, LLMs are susceptible to adversarial attacks [ 345], and their robustness
against slight input modifications is inadequate. In contrast, agents that accomplish tasks with the
assistance of tools exhibit stronger interpretability and robustness. The execution process of tools
can reflect the agents’ approach to addressing complex requirements and enhance the credibility of
their decisions. Moreover, for the reason that tools are specifically designed for their respective usage
scenarios, agents utilizing such tools are better equipped to handle slight input modifications and are
more resilient against adversarial attacks [94]. LLM-based agents not only require the use of tools, but are also well-suited for tool integration. Lever-
aging the rich world knowledge accumulated through the pre-training process and CoT prompting,
LLMs have demonstrated remarkable reasoning and decision-making abilities in complex interactive
environments [ 97], which help agents break down and address tasks specified by users in an appropri-
ate way. What’s more, LLMs show significant potential in intent understanding and other aspects
[25;201;202;203]. When agents are combined with tools, the threshold for tool utilization can be
lowered, thereby fully unleashing the creative potential of human users [94]. Understanding tools. A prerequisite for an agent to use tools effectively is a comprehensive
understanding of the tools’ application scenarios and invocation methods. Without this understanding,
the process of the agent using tools will become untrustworthy and fail to genuinely enhance the
agent’s capabilities. Leveraging the powerful zero-shot and few-shot learning abilities of LLMs
[40; 41], agents can acquire knowledge about tools by utilizing zero-shot prompts that describe tool
functionalities and parameters, or few-shot prompts that provide demonstrations of specific tool usage
scenarios and corresponding methods [ 92;326]. These learning approaches parallel human methods
of learning by consulting tool manuals or observing others using tools [ 94]. A single tool is often
insufficient when facing complex tasks. Therefore, the agents should first decompose the complex
task into subtasks in an appropriate manner, and their understanding of tools play a significant role in
task decomposition. Learning to use tools. The methods for agents to learn to utilize tools primarily consist of learning
from demonstrations andlearning from feedback. This involves mimicking the behavior of human
experts [ 346;347;348], as well as understanding the consequences of their actions and making
adjustments based on feedback received from both the environment and humans [ 24;349;350]. Environmental feedback encompasses result feedback on whether actions have successfully completed
the task and intermediate feedback that captures changes in the environmental state caused by actions;
human feedback comprises explicit evaluations and implicit behaviors, such as clicking on links [ 94].
20

--- PAGE 21 ---
If an agent rigidly applies tools without adaptability , it cannot achieve acceptable performance
in all scenarios. Agents need to generalize their tool usage skills learned in specific contexts to
more general situations, such as transferring a model trained on Yahoo search to Google search. To
accomplish this, it’s necessary for agents to grasp the common principles or patterns in tool usage
strategies, which can potentially be achieved through meta-tool learning [ 327]. Enhancing the agent’s
understanding of relationships between simple and complex tools, such as how complex tools are
built on simpler ones, can contribute to the agents’ capacity to generalize tool usage. This allows
agents to effectively discern nuances across various application scenarios and transfer previously
learned knowledge to new tools [ 94]. Curriculum learning [ 351], which allows an agent to start
from simple tools and progressively learn complex ones, aligns with the requirements. Moreover,
benefiting from the understanding of user intent reasoning and planning abilities, agents can better
design methods of tool utilization and collaboration and then provide higher-quality outcomes. Making tools for self-sufficiency. Existing tools are often designed for human convenience, which
might not be optimal for agents. To make agents use tools better, there’s a need for tools specifically
designed for agents. These tools should be more modular and have input-output formats that are
more suitable for agents. If instructions and demonstrations are provided, LLM-based agents also
possess the ability to create tools by generating executable programs, or integrating existing tools into
more powerful ones [ 94;330;352]. and they can learn to perform self-debugging [ 331]. Moreover, if
the agent that serves as a tool maker successfully creates a tool, it can produce packages containing
the tool’s code and demonstrations for other agents in a multi-agent system, in addition to using the
tool itself [ 329]. Speculatively, in the future, agents might become self-sufficient and exhibit a high
degree of autonomy in terms of tools. Tools can expand the action space of LLM-based agents. With the help of tools, agents can utilize
various external resources such as web applications and other LMs during the reasoning and planning
phase [ 92]. This process can provide information with high expertise, reliability, diversity, and quality
for LLM-based agents, facilitating their decision-making and action. For example, search-based tools
can improve the scope and quality of the knowledge accessible to the agents with the aid of external
databases, knowledge graphs, and web pages, while domain-specific tools can enhance an agent’s
expertise in the corresponding field [ 211;353]. Some researchers have already developed LLM-based
controllers that generate SQL statements to query databases, or to convert user queries into search
requests and use search engines to obtain the desired results [ 90;175]. What’s more, LLM-based
agents can use scientific tools to execute tasks like organic synthesis in chemistry, or interface
with Python interpreters to enhance their performance on intricate mathematical computation tasks
[354;355]. For multi-agent systems, communication tools (e.g., emails) may serve as a means for
agents to interact with each other under strict security constraints, facilitating their collaboration , and
showing autonomy and flexibility [94]. Although the tools mentioned before enhance the capabilities of agents, the medium of interaction
with the environment remains text-based. However, tools are designed to expand the functionality of
language models, and their outputs are not limited to text. Tools for non-textual output can diversify
themodalities of agent actions, thereby expanding the application scenarios of LLM-based agents. For example, image processing and generation can be accomplished by an agent that draws on a
visual model [ 328]. In aerospace engineering, agents are being explored for modeling physics and
solving complex differential equations [ 356]; in the field of robotics, agents are required to plan
physical operations and control the robot execution [ 179]; and so on. Agents that are capable of
dynamically interacting with the environment or the world through tools, or in a multimodal manner,
can be referred to as digitally embodied [ 94]. The embodiment of agents has been a central focus of
embodied learning research. We will make a deep discussion on agents’ embodied action in §3.3.3.
3.3.3 Embodied Action
In the pursuit of Artificial General Intelligence (AGI), the embodied agent is considered a pivotal
paradigm while it strives to integrate model intelligence with the physical world. The Embodiment
hypothesis [357] draws inspiration from the human intelligence development process, posing that an
agent’s intelligence arises from continuous interaction and feedback with the environment rather than
relying solely on well-curated textbooks. Similarly, unlike traditional deep learning models that learn
explicit capabilities from the internet datasets to solve domain problems, people anticipate that LLM-
based agents’ behaviors will no. longer be limited to pure text output or calling exact tools to perform
21

--- PAGE 22 ---
particular domain tasks [ 358]. Instead, they should be capable of actively perceiving, comprehending,
and interacting with physical environments, making decisions, and generating specific behaviors to
modify the environment based on LLM’s extensive internal knowledge. We collectively term these
asembodied actions , which enable agents’ ability to interact with and comprehend the world in a
manner closely resembling human behavior. The potential of LLM-based agents for embodied actions. Before the widespread rise of LLMs,
researchers tended to use methods like reinforcement learning to explore the embodied actions of
agents. Despite the extensive success of RL-based embodiment [ 359;360;361], it does have certain
limitations in some aspects. In brief, RL algorithms face limitations in terms of data efficiency,
generalization, and complex problem reasoning due to challenges in modeling the dynamic and
often ambiguous real environment, or their heavy reliance on precise reward signal representations
[362]. Recent studies have indicated that leveraging the rich internal knowledge acquired during the
pre-training of LLMs can effectively alleviate these issues [120; 187; 258; 363].
•Cost efficiency. Some on-policy algorithms struggle with sample efficiency as they require fresh
data for policy updates while gathering enough embodied data for high-performance training is
costly and noisy. The constraint is also found in some end-to-end models [ 364;365;366]. By
leveraging the intrinsic knowledge from LLMs, agents like PaLM-E [ 120] jointly train robotic data
with general visual-language data to achieve significant transfer ability in embodied tasks while
also showcasing that geometric input representations can improve training data efficiency.
•Embodied action generalization. As discussed in section §3.1.5, an agent’s competence should
extend beyond specific tasks. When faced with intricate, uncharted real-world environments, it’s
imperative that the agent exhibits dynamic learning and generalization capabilities. However,
the majority of RL algorithms are designed to train and evaluate relevant skills for specific tasks
[101;367;368;369]. In contrast, fine-tuned by diverse forms and rich task types, LLMs have
showcased remarkable cross-task generalization capabilities [ 370;371]. For instance, PaLM-
E exhibits surprising zero-shot or one-shot generalization capabilities to new objects or novel
combinations of existing objects [ 120]. Further, language proficiency represents a distinctive
advantage of LLM-based agents, serving both as a means to interact with the environment and as a
medium for transferring foundational skills to new tasks [ 372]. SayCan [ 179] decomposes task
instructions presented in prompts using LLMs into corresponding skill commands, but in partially
observable environments, limited prior skills often do not achieve satisfactory performance [ 101]. To address this, V oyager [ 190] introduces the skill library component to continuously collect novel
self-verified skills, which allows for the agent’s lifelong learning capabilities.
•Embodied action planning. Planning constitutes a pivotal strategy employed by humans in
response to complex problems as well as LLM-based agents. Before LLMs exhibited remarkable
reasoning abilities, researchers introduced Hierarchical Reinforcement Learning (HRL) methods
while the high-level policy constraints sub-goals for the low-level policy and the low-level policy
produces appropriate action signals [ 373;374;375]. Similar to the role of high-level policies, LLMs
with emerging reasoning abilities [ 26] can be seamlessly applied to complex tasks in a zero-shot or
few-shot manner [ 95;97;98;99]. In addition, external feedback from the environment can further
enhance LLM-based agents’ planning performance. Based on the current environmental feedback,
some work [ 101;91;100;376] dynamically generate, maintain, and adjust high-level action plans
in order to minimize dependency on prior knowledge in partially observable environments, thereby
grounding the plan. Feedback can also come from models or humans, which can usually be referred
to as the critics, assessing task completion based on the current state and task prompts [25; 190]. Embodied actions for LLM-based agents. Depending on the agents’ level of autonomy in a task
or the complexity of actions, there are several fundamental LLM-based embodied actions, primarily
including observation, manipulation, and navigation.
•Observation. Observation constitutes the primary ways by which the agent acquires environmental
information and updates states, playing a crucial role in enhancing the efficiency of subsequent
embodied actions. As mentioned in §3.2, observation by embodied agents primarily occurs in
environments with various inputs, which are ultimately converged into a multimodal signal. A
common approach entails a pre-trained Vision Transformer (ViT) used as the alignment module for
text and visual information and special tokens are marked to denote the positions of multimodal
data [ 120;332;121]. Soundspaces [ 377] proposes the identification of physical spatial geometric
22

--- PAGE 23 ---
elements guided by reverberant audio input, enhancing the agent’s observations with a more
comprehensive perspective [ 375]. In recent times, even more research takes audio as a modality
for embedded observation. Apart from the widely employed cascading paradigm [293; 378; 316],
audio information encoding similar to ViT further enhances the seamless integration of audio
with other modalities of inputs [ 294]. The agent’s observation of the environment can also be
derived from real-time linguistic instructions from humans, while human feedback helps the agent
in acquiring detail information that may not be readily obtained or parsed [333; 190].
•Manipulation. In general, manipulation tasks for embodied agents include object rearrangements,
tabletop manipulation, and mobile manipulation [ 23;120]. The typical case entails the agent
executing a sequence of tasks in the kitchen, which includes retrieving items from drawers and
handing them to the user, as well as cleaning the tabletop [ 179]. Besides precise observation,
this involves combining a series of subgoals by leveraging LLM. Consequently, maintaining
synchronization between the agent’s state and the subgoals is of significance. DEPS [ 183] utilizes
an LLM-based interactive planning approach to maintain this consistency and help error correction
from agent’s feedback throughout the multi-step, long-haul reasoning process. In contrast to these,
AlphaBlock [ 334] focuses on more challenging manipulation tasks (e.g. making a smiley face
using building blocks), which requires the agent to have a more grounded understanding of the
instructions. Unlike the existing open-loop paradigm, AlphaBlock constructs a dataset comprising
35 complex high-level tasks, along with corresponding multi-step planning and observation pairs,
and then fine-tunes a multimodal model to enhance its comprehension of high-level cognitive
instructions.
•Navigation. Navigation permits agents to dynamically alter their positions within the environ-
ment, which often involves multi-angle and multi-object observations, as well as long-horizon
manipulations based on current exploration [ 23]. Before navigation, it is essential for embodied
agents to establish prior internal maps about the external environment, which are typically in the
form of a topological map, semantic map or occupancy map [ 358]. For example, LM-Nav [ 335]
utilizes the VNM [ 379] to create an internal topological map. It further leverages the LLM and
VLM for decomposing input commands and analyzing the environment to find the optimal path. Furthermore, some [ 380;381] highlight the importance of spatial representation to achieve the
precise localization of spatial targets rather than conventional point or object-centric navigation
actions by leveraging the pre-trained VLM model to combine visual features from images with 3D
reconstructions of the physical world [ 358]. Navigation is usually a long-horizon task, where the
upcoming states of the agent are influenced by its past actions. A memory buffer and summary
mechanism are needed to serve as a reference for historical information [ 336], which is also
employed in Smallville and V oyager [ 22;190;382;383]. Additionally, as mentioned in §3.2, some
works have proposed the audio input is also of great significance, but integrating audio information
presents challenges in associating it with the visual environment. A basic framework includes a
dynamic path planner that uses visual and auditory observations along with spatial memories to
plan a series of actions for navigation [375; 384]. By integrating these, the agent can accomplish more complex tasks, such as embodied question
answering, whose primary objective is autonomous exploration of the environment, and responding
to pre-defined multimodal questions, such as Is the watermelon in the kitchen larger than the pot. Which one is harder. To address these questions, the agent needs to navigate to the kitchen, observe
the sizes of both objects and then answer the questions through comparison [358]. In terms of control strategies, as previously mentioned, LLM-based agents trained on particular
embodied datasets typically generate high-level policy commands to control low-level policies for
achieving specific sub-goals. The low-level policy can be a robotic transformer [ 120;385;386],
which takes images and instructions as inputs and produces control commands for the end effector as
well as robotic arms in particular embodied tasks [ 179]. Recently, in virtual embodied environments,
the high-level strategies are utilized to control agents in gaming [ 172;183;190;337] or simulated
worlds [ 22;108;109]. For instance, V oyager [ 190] calls the Mineflayer [ 387] API interface to
continuously acquire various skills and explore the world. Prospective future of the embodied action. LLM-based embodied actions are seen as the bridge
between virtual intelligence and the physical world, enabling agents to perceive and modify the
environment much like humans. However, there remain several constraints such as high costs of
physical-world robotic operators and the scarcity of embodied datasets, which foster a growing
23

--- PAGE 24 ---
interest in investigating agents’ embodied actions within simulated environments like Minecraft
[183;338;337;190;339]. By utilizing the Mineflayer [ 387] API, these investigations enable cost-
effective examination of a wide range of embodied agents’ operations including exploration, planning,
self-improvement, and even lifelong learning [ 190]. Despite notable progress, achieving optimal
embodied actions remains a challenge due to the significant disparity between simulated platforms and
the physical world. To enable the effective deployment of embodied agents in real-world scenarios,
an increasing demand exists for embodied task paradigms and evaluation criteria that closely mirror
real-world conditions [ 358]. On the other hand, learning to ground language for agents is also an
obstacle. For example, expressions like “jump down like a cat” primarily convey a sense of lightness
and tranquility, but this linguistic metaphor requires adequate world knowledge [ 30]. [340] endeavors
to amalgamate text distillation with Hindsight Experience Replay (HER) to construct a dataset as
the supervised signal for the training process. Nevertheless, additional investigation on grounding
embodied datasets still remains necessary while embodied action plays an increasingly pivotal role
across various domains in human life.
4 Agents in Practice: Harnessing AI for Good
Agents in Practice:
Harnessing AI for GoodSingle Agent
Deployment §4.1Task-oriented
Deploytment §4.1.1Web scenariosWebAgent [ 388], Mind2Web [ 389],
WebGum [ 390], WebArena [ 391],
Webshop [ 392], WebGPT [ 90], Kim
et al. [ 393], Zheng et al. [ 394], etc.
Life scenariosInterAct [ 395], PET [ 182], Huang
et al. [ 258], Gramopadhye et al.
[396], Raman et al. [ 256], etc.
Innovation-oriented
Deploytment §4.1.2Li et al. [ 397], Feldt et al. [ 398], ChatMOF
[399], ChemCrow [ 354], Boiko et al.
[110], SCIENCEWORLD et al. [ 400], etc.
Lifecycle-oriented
Deploytment §4.1.3V oyager [ 190], GITM [ 172],
DEPS [ 183], Plan4MC [ 401],
Nottingham et al. [ 339], etc.
Multi-Agents
Interaction §4.2Cooperative
Interaction §4.2.1Disordered
cooperationChatLLM [ 402], RoCo [ 403],
Blind Judgement [ 404], etc.
Ordered cooperationMetaGPT [ 405], ChatDev [ 109], CAMEL
[108], AutoGen [ 406], SwiftSage [ 185],
ProAgent [ 407], DERA [ 408], Talebi-
rad et al. [ 409], AgentVerse [ 410],
CGMI [ 411], Liu et al. [ 27], etc.
Adversarial
Interaction §4.2.2ChatEval [ 171], Xiong et al.
[412], Du et al. [ 111], Fu et al.
[129], Liang et al. [ 112], etc.
Human-Agent
Interaction §4.3Instructor-Executor
Paradigm §4.3.1Education Dona [ 413], Math Agents [ 414], etc.
HealthHsu et al. [ 415], HuatuoGPT [ 416],
Zhongjing [ 417], LISSA [ 418], etc.
Other ApplicationsGao et al. [ 419], PEER [ 420], DIAL-
GEN [ 421], AssistGPT [ 422], etc.
Equal Partnership
Paradigm §4.3.2Empathetic
CommunicatorSAPIEN [ 423], Hsu et al.
[415], Liu et al. [ 424], etc.
Human-Level
ParticipantBakhtin et al. [ 425], FAIR et al. [ 426],
Lin et al. [ 427], Li et al. [ 428], etc.
Figure 6: Typology of applications of LLM-based agents. The LLM-based agent, as an emerging direction, has gained increasing attention from researchers. Many applications in specific domains and tasks have already been developed, showcasing the
powerful and versatile capabilities of agents. We can state with great confidence that, the possibility
of having a personal agent capable of assisting users with typical daily tasks is larger than ever
before [ 398]. As an LLM-based agent, its design objective should always be beneficial to humans,
i.e., humans can harness AI for good. Specifically, we expect the agent to achieve the following
objectives:
24

--- PAGE 25 ---
Single AgentAgent-AgentAgent-Human
Figure 7: Scenarios of LLM-based agent applications. We mainly introduce three scenarios: single-
agent deployment, multi-agent interaction, and human-agent interaction. A single agent possesses
diverse capabilities and can demonstrate outstanding task-solving performance in various application
orientations. When multiple agents interact, they can achieve advancement through cooperative or
adversarial interactions. Furthermore, in human-agent interactions, human feedback can enable
agents to perform tasks more efficiently and safely, while agents can also provide better service to
humans.
1.Assist users in breaking free from daily tasks and repetitive labor, thereby Alleviating human work
pressure and enhancing task-solving efficiency.
2.No longer necessitates users to provide explicit low-level instructions. Instead, the agent can
independently analyze, plan, and solve problems.
3.After freeing users’ hands, the agent also liberates their minds to engage in exploratory and
innovative work, realizing their full potential in cutting-edge scientific fields. In this section, we provide an in-depth overview of current applications of LLM-based agents, aiming
to offer a broad perspective for the practical deployment scenarios (see Figure 7). First, we elucidate
the diverse application scenarios of Single Agent, including task-oriented, innovation-oriented, and
lifecycle-oriented scenarios (§ 4.1). Then, we present the significant coordinating potential of Multiple
Agents. Whether through cooperative interaction for complementarity or adversarial interaction
for advancement, both approaches can lead to higher task efficiency and response quality (§ 4.2). Finally, we categorize the interactive collaboration between humans and agents into two paradigms
and introduce the main forms and specific applications respectively (§ 4.3). The topological diagram
for LLM-based agent applications is depicted in Figure 6.
4.1 General Ability of Single Agent
Currently, there is a vibrant development of application instances of LLM-based agents [ 429;430;
431]. AutoGPT [ 114] is one of the ongoing popular open-source projects aiming to achieve a fully
autonomous system. Apart from the basic functions of large language models like GPT-4, the
AutoGPT framework also incorporates various practical external tools and long/short-term memory
management. After users input their customized objectives, they can free their hands and wait
for AutoGPT to automatically generate thoughts and perform specific tasks, all without requiring
additional user prompts. As shown in Figure 8, we introduce the astonishingly diverse capabilities that the agent exhibits in
scenarios where only one single agent is present.
4.1.1 Task-oriented Deployment
The LLM-based agents, which can understand human natural language commands and perform
everyday tasks [ 391], are currently among the most favored and practically valuable agents by users. This is because they have the potential to enhance task efficiency, alleviate user workload, and
promote access for a broader user base. In task-oriented deployment , the agent follows high-level
instructions from users, undertaking tasks such as goal decomposition [ 182;258;388;394], sequence
planning of sub-goals [ 182;395], interactive exploration of the environment [ 256;391;390;392],
until the final objective is achieved. To explore whether agents can perform basic tasks, they are first deployed in text-based game
scenarios. In this type of game, agents interact with the world purely using natural language [ 432]. By reading textual descriptions of their surroundings and utilizing skills like memory, planning,
25

--- PAGE 26 ---
Task: Create a New Medicine
Task: Maintain a Lifelong Survival
Task:Find the UmbrellaInnovation-OrientedLifecycle-Oriented
增加光影
Task-Oriented
Innovation-Oriented
Lifecycle-Oriented
Figure 8: Practical applications of the single LLM-based agent in different scenarios. In task-
oriented deployment , agents assist human users in solving daily tasks. They need to possess basic
instruction comprehension and task decomposition abilities. In innovation-oriented deployment ,
agents demonstrate the potential for autonomous exploration in scientific domains. In lifecycle-
oriented deployment , agents have the ability to continuously explore, learn, and utilize new skills to
ensure long-term survival in an open world.
and trial-and-error [ 182], they predict the next action. However, due to the limitation of foundation
language models, agents often rely on reinforcement learning during actual execution [ 432;433;434]. With the gradual evolution of LLMs [ 301], agents equipped with stronger text understanding and
generation abilities have demonstrated great potential to perform tasks through natural language. Due
to their oversimplified nature, naive text-based scenarios have been inadequate as testing grounds
for LLM-based agents [ 391]. More realistic and complex simulated test environments have been
constructed to meet the demand. Based on task types, we divide these simulated environments into
web scenarios andlife scenarios , and introduce the specific roles that agents play in them. In web scenarios. Performing specific tasks on behalf of users in a web scenario is known as the
web navigation problem [ 390]. Agents interpret user instructions, break them down into multiple
basic operations, and interact with computers. This often includes web tasks such as filling out
forms, online shopping, and sending emails. Agents need to possess the ability to understand
instructions within complex web scenarios, adapt to changes (such as noisy text and dynamic HTML
web pages), and generalize successful operations [ 391]. In this way, agents can achieve accessibility
and automation when dealing with unseen tasks in the future [ 435], ultimately freeing humans from
repeated interactions with computer UIs. Agents trained through reinforcement learning can effectively mimic human behavior using predefined
actions like typing, searching, navigating to the next page, etc. They perform well in basic tasks
such as online shopping [ 392] and search engine retrieval [ 90], which have been widely explored. However, agents without LLM capabilities may struggle to adapt to the more realistic and complex
scenarios in the real-world Internet. In dynamic, content-rich web pages such as online forums or
online business management [391], agents often face challenges in performance. In order to enable successful interactions between agents and more realistic web pages, some
researchers [ 393;394] have started to leverage the powerful HTML reading and understanding
abilities of LLMs. By designing prompts, they attempt to make agents understand the entire HTML
source code and predict more reasonable next action steps. Mind2Web [ 389] combines multiple
LLMs fine-tuned for HTML, allowing them to summarize verbose HTML code [ 388] in real-world
scenarios and extract valuable information. Furthermore, WebGum [ 390] empowers agents with
visual perception abilities by employing a multimodal corpus containing HTML screenshots. It
simultaneously fine-tunes the LLM and a visual encoder, deepening the agent’s comprehensive
understanding of web pages. In life scenarios. In many daily household tasks in life scenarios, it’s essential for agents to
understand implicit instructions and apply common-sense knowledge [ 433]. For an LLM-based agent
trained solely on massive amounts of text, tasks that humans take for granted might require multiple
26

--- PAGE 27 ---
trial-and-error attempts [ 432]. More realistic scenarios often lead to more obscure and subtle tasks. For example, the agent should proactively turn it on if it’s dark and there’s a light in the room. To
successfully chop some vegetables in the kitchen, the agent needs to anticipate the possible location
of a knife [182]. Can an agent apply the world knowledge embedded in its training data to real interaction scenarios. Huang et al. [ 258] lead the way in exploring this question. They demonstrate that sufficiently large
LLMs, with appropriate prompts, can effectively break down high-level tasks into suitable sub-tasks
without additional training. However, this static reasoning and planning ability has its potential
drawbacks. Actions generated by agents often lack awareness of the dynamic environment around
them. For instance, when a user gives the task “clean the room”, the agent might convert it into
unfeasible sub-tasks like “call a cleaning service” [396]. To provide agents with access to comprehensive scenario information during interactions, some
approaches directly incorporate spatial data and item-location relationships as additional inputs to
the model. This allows agents to gain a precise description of their surroundings [ 395;396]. Wu
et al. [ 182] introduce the PET framework, which mitigates irrelevant objects and containers in
environmental information through an early error correction method [ 256]. PET encourages agents
to explore the scenario and plan actions more efficiently, focusing on the current sub-task.
4.1.2 Innovation-oriented Deployment
The LLM-based agent has demonstrated strong capabilities in performing tasks and enhancing the
efficiency of repetitive work. However, in a more intellectually demanding field, like cutting-edge
science, the potential of agents has not been fully realized yet. This limitation mainly arises from
two challenges [ 399]: On one hand, the inherent complexity of science poses a significant barrier. Many domain-specific terms and multi-dimensional structures are difficult to represent using a single
text. As a result, their complete attributes cannot be fully encapsulated. This greatly weakens the
agent’s cognitive level. On the other hand, there is a severe lack of suitable training data in scientific
domains, making it difficult for agents to comprehend the entire domain knowledge [ 400;436]. If the
ability for autonomous exploration could be discovered within the agent, it would undoubtedly bring
about beneficial innovation in human technology. Currently, numerous efforts in various specialized domains aim to overcome this challenge [ 437;438;
439]. Experts from the computer field make full use of the agent’s powerful code comprehension
and debugging abilities [ 398;397]. In the fields of chemistry and materials, researchers equip agents
with a large number of general or task-specific tools to better understand domain knowledge. Agents
evolve into comprehensive scientific assistants, proficient in online research and document analysis to
fill data gaps. They also employ robotic APIs for real-world interactions, enabling tasks like material
synthesis and mechanism discovery [110; 354; 399]. The potential of LLM-based agents in scientific innovation is evident, yet we do not expect their
exploratory abilities to be utilized in applications that could threaten or harm humans. Boiko et
al. [110] study the hidden dangers of agents in synthesizing illegal drugs and chemical weapons,
indicating that agents could be misled by malicious users in adversarial prompts. This serves as a
warning for our future work.
4.1.3 Lifecycle-oriented Deployment
Building a universally capable agent that can continuously explore, develop new skills, and maintain
a long-term life cycle in an open, unknown world is a colossal challenge. This accomplishment is
regarded as a pivotal milestone in the field of AGI [ 183]. Minecraft, as a typical and widely explored
simulated survival environment, has become a unique playground for developing and testing the
comprehensive ability of an agent. Players typically start by learning the basics, such as mining
wood and making crafting tables, before moving on to more complex tasks like fighting against
monsters and crafting diamond tools [ 190]. Minecraft fundamentally reflects the real world, making
it conducive for researchers to investigate an agent’s potential to survive in the authentic world. The survival algorithms of agents in Minecraft can generally be categorized into two types [ 190]:
low-level control andhigh-level planning. Early efforts mainly focused on reinforcement learning
[190;440] and imitation learning [ 441], enabling agents to craft some low-level items. With the
emergence of LLMs, which demonstrated surprising reasoning and analytical capabilities, agents
27

--- PAGE 28 ---
begin to utilize LLM as a high-level planner to guide simulated survival tasks [ 183;339]. Some
researchers use LLM to decompose high-level task instructions into a series of sub-goals [ 401], basic
skill sequences [ 339], or fundamental keyboard/mouse operations [ 401], gradually assisting agents in
exploring the open world. V oyager[ 190], drawing inspiration from concepts similar to AutoGPT[ 114], became the first LLM-
based embodied lifelong learning agent in Minecraft, based on the long-term goal of “discovering
as many diverse things as possible”. It introduces a skill library for storing and retrieving complex
action-executable code, along with an iterative prompt mechanism that incorporates environmental
feedback and error correction. This enables the agent to autonomously explore and adapt to unknown
environments without human intervention. An AI agent capable of autonomously learning and
mastering the entire real-world techniques may not be as distant as once thought [401].
4.2 Coordinating Potential of Multiple Agents
Motivation and Background. Although LLM-based agents possess commendable text under-
standing and generation capabilities, they operate as isolated entities in nature [ 409]. They lack the
ability to collaborate with other agents and acquire knowledge from social interactions. This inherent
limitation restricts their potential to learn from multi-turn feedback from others to enhance their
performance [ 27]. Moreover, they cannot be effectively deployed in complex scenarios requiring
collaboration and information sharing among multiple agents. As early as 1986, Marvin Minsky made a forward-looking prediction. In his book The Society of
Mind [442], he introduced a novel theory of intelligence, suggesting that intelligence emerges from
the interactions of many smaller agents with specific functions. For instance, certain agents might be
responsible for pattern recognition, while others might handle decision-making or generate solutions. This idea has been put into concrete practice with the rise of distributed artificial intelligence [ 443].