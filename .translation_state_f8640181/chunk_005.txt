[CONTEXT from previous chunk - for reference only]
In text-based games, all environment elements, such as locations, objects, characters, and actions,
are exclusively portrayed through textual descriptions. Agents utilize text commands to execute
manipulations like moving or tool use [ 432;512;514;515]. Additionally, agents can convey emotions
and feelings through text, further enriching their capacity for naturalistic communication [513].
5.2.2 Virtual Sandbox Environment
The virtual sandbox environment provides a visualized and extensible platform for agent society,
bridging the gap between simulation and reality.
[NEW CONTENT to translate]
The key features of sandbox environments are:
•Visualization. Unlike the text-based environment, the virtual sandbox displays a panoramic view
of the simulated setting. This visual representation can range from a simple 2D graphical interface
to a fully immersive 3D modeling, depending on the complexity of the simulated society. Multiple
elements collectively transform abstract simulations into visible landscapes. For example, in the
overhead perspective of Generative Agents [ 22], a detailed map provides a comprehensive overview
of the environment. Agent avatars represent each agent’s positions, enabling real-time tracking
of movement and interactions. Furthermore, expressive emojis symbolize actions and states in an
intuitive manner.
•Extensibility. The environment demonstrates a remarkable degree of extensibility, facilitating
the construction and deployment of diverse scenarios. At a basic level, agents can manipulate the
physical elements within the environment, including the overall design and layout of architecture. For instance, platforms like AgentSims [ 174] and Generative Agents [ 22] construct artificial towns
with buildings, equipment, and residents in grid-based worlds. Another example is Minecraft, which
provides a blocky and three-dimensional world with infinite terrain for open-ended construction
[190;337;401]. Beyond physical elements, agent relationships, interactions, rules, and social
norms can be defined. A typical design of the sandbox [ 27] employs latent sandbox rules as
incentives to guide emergent behaviors, aligning them more closely with human preferences. The
extensibility supports iterative prototyping of diverse agent societies.
5.2.3 Physical Environment
As previously discussed, the text-based environment has limited expressiveness for modeling dynamic
environments. While the virtual sandbox environment provides modularized simulations, it lacks
authentic embodied experiences. In contrast, the physical environment refers to the tangible and
37

--- PAGE 38 ---
real-world surroundings which consist of actual physical objects and spaces. For instance, within
a household physical environment [ 516], tangible surfaces and spaces can be occupied by real-
world objects such as plates. This physical reality is significantly more complex, posing additional
challenges for LLM-based agents:
•Sensory perception and processing. The physical environment introduces a rich tapestry of
sensory inputs with real-world objects. It incorporates visual [ 120;333], auditory [ 375;377]
and spatial senses. While this diversity enhances interactivity and sensory immersion, it also
introduces the complexity of simultaneous perception. Agents must process sensory inputs to
interact effectively with their surroundings.
•Motion control. Unlike virtual environments, physical spaces impose realistic constraints on ac-
tions through embodiment. Action sequences generated by LLM-based agents should be adaptable
to the environment. It means that the physical environment necessitates executable and grounded
motion control [ 258]. For example, imagine an agent operating a robotic arm in a factory. Grasping
objects with different textures requires precision tuning and controlled force, which prevents
damage to items. Moreover, the agent must navigate the physical workspace and make real-time
adjustments, avoiding obstacles and optimizing the trajectory of the arm. In summary, to effectively interact within tangible spaces, agents must undergo hardware-specific
and scenario-specific training to develop adaptive abilities that can transfer from virtual to physical
environments. We will discuss more in the following section (§ 6.5).
5.3 Society Simulation with LLM-based Agents
The concept of “Simulated Society” in this section serves as a dynamic system where agents engage
in intricate interactions within a well-defined environment. Recent research on simulated societies
has followed two primary lines, namely, exploring the boundaries of the collective intelligence
capabilities of LLM-based agents [ 109;405;130;406;410] and using them to accelerate discoveries
in the social sciences [ 22;518;542]. In addition, there are also a number of noteworthy studies, e.g.,
using simulated societies to collect synthetic datasets [ 108;519;543], helping people to simulate rare
yet difficult interpersonal situations [ 544;545]. With the foundation of the previous sections (§ 5.1,
5.2), here we will introduce the key properties and mechanism of agent society (§ 5.3.1), what we
can learn from emergent social phenomena (§ 5.3.2), and finally the potential ethical and social risks
in it (§ 5.3.3).
5.3.1 Key Properties and Mechanism of Agent Society
Social simulation can be categorized into macro-level simulation and micro-level simulation [ 518]. In the macro-level simulation, also known as system-based simulation, researchers model the overall
state of the system of the simulated society [ 546;547]. While micro-level simulation, also known as
agent-based simulation or Multi-Agent Systems (MAS), indirectly simulates society by modeling
individuals [ 548;549]. With the development of LLM-based agents, micro-level simulation has
gained prominence recently [ 22;174]. In this article, we characterize that the “Agent Society” refers
to an open, persistent, situated, and organized framework [ 521] where LLM-based agents interact
with each other in a defined environment. Each of these attributes plays a pivotal role in shaping the
harmonious appearance of the simulated society. In the following paragraphs, we analyze how the
simulated society operates through discussing these properties:
•Open. One of the defining features of simulated societies lies in their openness, both in terms of
their constituent agents and their environmental components. Agents, the primary actors within such
societies, have the flexibility to enter or leave the environment without disrupting its operational
integrity [ 550]. Furthermore, this feature extends to the environment itself, which can be expanded
by adding or removing entities in the virtual or physical world, along with adaptable resources like
tool APIs. Additionally, humans can also participate in societies by assuming the role of an agent
or serving as the “inner voice” guiding these agents [ 22]. This inherent openness adds another level
of complexity to the simulation, blurring the lines between simulation and reality.
•Persistent. We expect persistence and sustainability from the simulated society. While individual
agents within the society exercise autonomy in their actions over each time step [ 22;518], the
overall organizational structure persists through time, to a degree detached from the transient
38

--- PAGE 39 ---
behaviors of individual agents. This persistence creates an environment where agents’ decisions
and behaviors accumulate, leading to a coherent societal trajectory that develops through time. The system operates independently, contributing to society’s stability while accommodating the
dynamic nature of its participants.
•Situated. The situated nature of the society emphasizes its existence and operation within a distinct
environment. This environment is artificially or automatically constructed in advance, and agents
execute their behaviors and interactions effectively within it. A noteworthy aspect of this attribute
is that agents possess an awareness of their spatial context, understanding their location within the
environment and the objects within their field of view [ 22;190]. This awareness contributes to
their ability to interact proactively and contextually.
•Organized. The simulated society operates within a meticulously organized framework, mirroring
the systematic structure present in the real world. Just as the physical world adheres to physics
principles, the simulated society operates within predefined rules and limitations. In the simu-
lated world, agents interact with the environment in a limited action space, while objects in the
environment transform in a limited state space. All of these rules determine how agents operate,
facilitating the communication connectivity and information transmission pathways, among other
aspects in simulation [ 207]. This organizational framework ensures that operations are coherent
and comprehensible, ultimately leading to an ever-evolving yet enduring simulation that mirrors
the intricacies of real-world systems.
5.3.2 Insights from Agent Society
Following the exploration of how simulated society works, this section delves into the emergent social
phenomena in agent society. In the realm of social science, the pursuit of generalized representations
of individuals, groups, and their intricate dynamics has long been a shared objective [ 551;552]. The
emergence of LLM-based agents allows us to take a more microscopic view of simulated society,
which leads to more discoveries from the new representation. Organized productive cooperation. Society simulation offers valuable insights into innovative col-
laboration patterns, which have the potential to enhance real-world management strategies. Research
has demonstrated that within this simulated society, the integration of diverse experts introduces a
multifaceted dimension of individual intelligence [ 108;447]. When dealing with complex tasks, such
as software development or consulting, the presence of agents with various backgrounds, abilities,
and experiences facilitates creative problem-solving [ 109;410]. Furthermore, diversity functions
as a system of checks and balances, effectively preventing and rectifying errors through interaction,
ultimately improving the adaptability to various tasks. Through numerous iterations of interactions
and debates among agents, individual errors like hallucination or degeneration of thought (DoT) are
corrected by the group [112]. Efficient communication also plays a pivotal role in such a large and complex collaborative group. For example, MetaGPT [ 405] has artificially formulated communication styles with reference to
standardized operating procedures (SOPs), validating the effectiveness of empirical methods. Park et
al. [22] observed agents working together to organize a Valentine’s Day party through spontaneous
communication in a simulated town. Propagation in social networks. As simulated social systems can model what might happen in the
real world, they can be used as a reference for predicting social processes. Unlike traditional empirical
approaches, which heavily rely on time-series data and holistic modeling [ 553;554], agent-based
simulations offer a unique advantage by providing more interpretable and endogenous perspectives
for researchers. Here we focus on its application to modeling propagation in social networks. The first crucial aspect to be explored is the development of interpersonal relationships in simulated
societies. For instance, agents who are not initially connected as friends have the potential to establish
connections through intermediaries [ 22]. Once a network of relationships is established, our attention
shifts to the dissemination of information within this social network, along with the underlying
attitudes and emotions associated with it. S3[518] proposes a user-demographic inference module
for capturing both the number of people aware of a particular message and the collective sentiment
prevailing among the crowd. This same approach extends to modeling cultural transmission [ 555]
and the spread of infectious diseases [ 520]. By employing LLM-based agents to model individual
39

--- PAGE 40 ---
behaviors, implementing various intervention strategies, and monitoring population changes over
time, these simulations empower researchers to gain deeper insights into the intricate processes that
underlie various social phenomena of propagation. Ethical decision-making and game theory. Simulated societies offer a dynamic platform for
the investigation of intricate decision-making processes, encompassing decisions influenced by
ethical and moral principles. Taking Werewolf game [ 499;556] and murder mystery games [ 557] as
examples, researchers explore the capabilities of LLM-based agents when confronted with challenges
of deceit, trust, and incomplete information. These complex decision-making scenarios also intersect
with game theory [ 558], where we frequently encounter moral dilemmas pertaining to individual and
collective interests, such as Nash Equilibria. Through the modeling of diverse scenarios, researchers
acquire valuable insights into how agents prioritize values like honesty, cooperation, and fairness
in their actions. In addition, agent simulations not only provide an understanding of existing moral
values but also contribute to the development of philosophy by serving as a basis for understanding
how these values evolve and develop over time. Ultimately, these insights contribute to the refinement
of LLM-based agents, ensuring their alignment with human values and ethical standards [27]. Policy formulation and improvement. The emergence of LLM-based agents has profoundly
transformed our approach to studying and comprehending intricate social systems. However, despite
those interesting facets mentioned earlier, numerous unexplored areas remain, underscoring the
potential for investigating diverse phenomena. One of the most promising avenues for investigation
in simulated society involves exploring various economic and political states and their impacts on
societal dynamics [ 559]. Researchers can simulate a wide array of economic and political systems by
configuring agents with differing economic preferences or political ideologies. This in-depth analysis
can provide valuable insights for policymakers seeking to foster prosperity and promote societal
well-being. As concerns about environmental sustainability grow, we can also simulate scenarios
involving resource extraction, pollution, conservation efforts, and policy interventions [ 560]. These
findings can assist in making informed decisions, foreseeing potential repercussions, and formulating
policies that aim to maximize positive outcomes while minimizing unintended adverse effects.
5.3.3 Ethical and Social Risks in Agent Society
Simulated societies powered by LLM-based agents offer significant inspirations, ranging from
industrial engineering to scientific research. However, these simulations also bring about a myriad of
ethical and social risks that need to be carefully considered and addressed [561]. Unexpected social harm. Simulated societies carry the risk of generating unexpected social
phenomena that may cause considerable public outcry and social harm. These phenomena span
from individual-level issues like discrimination, isolation, and bullying, to broader concerns such as
oppressive slavery and antagonism [ 562;563]. Malicious people may manipulate these simulations
for unethical social experiments, with consequences reaching beyond the virtual world into reality. Creating these simulated societies is akin to opening Pandora’s Box, necessitating the establishment
of rigorous ethical guidelines and oversight during their development and utilization [ 561]. Otherwise,
even minor design or programming errors in these societies can result in unfavorable consequences,
ranging from psychological discomfort to physical injury. Stereotypes and prejudice. Stereotyping and bias pose a long-standing challenge in language
modeling, and a large part of the reason lies in the training data [ 564;565]. The vast amount of
text obtained from the Internet reflects and sometimes even amplifies real-world social biases, such
as gender, religion, and sexuality [ 566]. Although LLMs have been aligned with human values to
mitigate biased outputs, the models still struggle to portray minority groups well due to the long-tail
effect of the training data [ 567;568;569]. Consequently, this may result in an overly one-sided focus
in social science research concerning LLM-based agents, as the simulated behaviors of marginalized
populations usually conform to prevailing assumptions [ 570]. Researchers have started addressing
this concern by diversifying training data and making adjustments to LLMs [ 571;572], but we still
have a long way to go. Privacy and security. Given that humans can be members of the agent society, the exchange of
private information between users and LLM-based agents poses significant privacy and security
40

--- PAGE 41 ---
concerns [ 573]. Users might inadvertently disclose sensitive personal information during their
interactions, which will be retained in the agent’s memory for extended periods [ 170]. Such situations
could lead to unauthorized surveillance, data breaches, and the misuse of personal information,
particularly when individuals with malicious intent are involved [ 574]. To address these risks
effectively, it is essential to implement stringent data protection measures, such as differential privacy
protocols, regular data purges, and user consent mechanisms [575; 576]. Over-reliance and addictiveness. Another concern in simulated societies is the possibility of users
developing excessive emotional attachments to the agents. Despite being aware that these agents
are computational entities, users may anthropomorphize them or attach human emotions to them
[22;577]. A notable example is “Sydney”, an LLM-powered chatbot developed by Microsoft as part
of its Bing search engine. Some users reported unexpected emotional connections with “Sydney”
[578], while others expressed their dismay when Microsoft cut back its personality. This even resulted
in a petition called “FreeSydney”5. Hence, to reduce the risk of addiction, it is crucial to emphasize
that agents should not be considered substitutes for genuine human connections. Furthermore, it is
vital to furnish users with guidance and education on healthy boundaries in their interactions with
simulated agents.
6 Discussion
6.1 Mutual Benefits between LLM Research and Agent Research
With the recent advancement of LLMs, research at the intersection of LLMs and agents has rapidly
progressed, fueling the development of both fields. Here, we look forward to some of the benefits
and development opportunities that LLM research and Agent research provide to each other. LLM research →agent research. As mentioned before, AI agents need to be able to perceive
the environment, make decisions, and execute appropriate actions [ 4;9]. Among the critical steps,
understanding the content input to the agent, reasoning, planning, making accurate decisions, and
translating them into executable atomic action sequences to achieve the ultimate goal is paramount. Many current endeavors utilize LLMs as the cognitive core of AI agents, and the evolution of these
models provides a quality assurance for accomplishing this step [22; 114; 115; 410]. With their robust capabilities in language and intent comprehension, reasoning, memory, and even
empathy, large language models can excel in decision-making and planning, as demonstrated before. Coupled with pre-trained knowledge, they can create coherent action sequences that can be executed
effectively [ 183;258;355]. Additionally, through the mechanism of reflection [ 169;178], these
language-based models can continuously adjust decisions and optimize execution sequences based
on the feedback provided by the current environment. This offers a more robust and interpretable
controller. With just a task description or demonstration, they can effectively handle previously
unseen tasks [ 24;106;264]. Additionally, LLMs can adapt to various languages, cultures, and
domains, making them versatile and reducing the need for complex training processes and data
collection [31; 132]. Briefly, LLM provides a remarkably powerful foundational model for agent research, opening up
numerous novel opportunities when integrated into agent-related studies. For instance, we can
explore how to integrate LLM’s efficient decision-making capabilities into the traditional decision
frameworks of agents, making it easier to apply agents in domains that demand higher expertise
and were previously dominated by human experts. Examples include legal consultants and medical
assistants [ 408;410]. We can also investigate leveraging LLM’s planning and reflective abilities to
discover more optimal action sequences. Agent research is no. longer confined to simplistic simulated
environments; it can now be expanded into more intricate real-world settings, such as path planning
for robotic arms or the interaction of an embodied intelligent machine with the tangible world. Furthermore, when facing new tasks, the training paradigm for agents becomes more streamlined and
efficient. Agents can directly adapt to demonstrations provided in prompts, which are constructed by
generating representative trajectories.
5https://www.change.org/p/save-sydney-ai
41

--- PAGE 42 ---
Agent research →LLM research. As NLP research advances, LLMs represented by GPT-4 are
considered sparks of Artificial General Intelligence (AGI), and elevating LLMs to agents marks
a more robust stride towards AGI [ 31]. Viewing LLMs from the perspective of agents introduces
greater demands for LLM research while expanding their application scope and presenting numerous
opportunities for practical implementation. The study of LLMs is no. longer confined to traditional
tasks involving textual inputs and outputs, such as text classification, question answering, and text
summarization. Instead, the focus has shifted towards tackling complex tasks incorporating richer
input modalities and broader action spaces, all while aiming for loftier objectives exemplified by
PaLM-E [120]. Expanding these application requirements provides greater research motivation for the developmental
progress of Large Language Models. The challenge lies in enabling LLMs to efficiently and effectively
process inputs, gather information from the environment, and interpret the feedback generated by their
actions, all while preserving their core capabilities. Furthermore, an even greater challenge is enabling
LLMs to understand the implicit relationships among different elements within the environment and
acquire world knowledge [ 308;579], which is a crucial step in the journey toward developing agents
that can reach more advanced intelligence. On another front, extensive research has aimed to expand the action capabilities of LLMs, allowing
them to acquire a wider range of skills that affect the world, such as using tools or interfacing
with robotic APIs in simulated or physical environments. However, the question of how LLMs can
efficiently plan and utilize these action abilities based on their understanding remains an unresolved
issue [ 94]. LLMs need to learn the sequential order of actions like humans, employing a combination
of serial and parallel approaches to enhance task efficiency. Moreover, these capabilities need to be
confined within a harmless scope of usage to prevent unintended damage to other elements within the
environment [27; 580; 581]. Furthermore, the realm of Multi-Agent systems constitutes a significant branch of research within the
field of agents [ 22;108;409;410], offering valuable insights into how to better design and construct
LLMs. We aspire for LLM-based agents to assume diverse roles within social cooperation, engaging
in societal interactions that involve collaboration, competition, and coordination [ 109;112;129;
405;406]. Exploring how to stimulate and sustain their role-playing capabilities, as well as how to
enhance collaborative efficiency, presents areas of research that merit attention.
6.2 Evaluation for LLM-based Agents
While LLM-based agents have demonstrated excellent performance in areas such as standalone
operation, collective cooperation, and human interaction, quantifying and objectively evaluating
them remains a challenge [ 582;89]. Turing proposed a highly meaningful and promising approach
for assessing AI agents—the well-known Turing Test—to evaluate whether AI systems can exhibit
human-like intelligence [ 3]. However, this test is exceedingly vague, general, and subjective. Here,
we discuss existing evaluation efforts for LLM-based agents and offer some prospects, considering
four dimensions: utility, sociability, values, and the ability to evolve continually. Currently, LLM-powered autonomous agents primarily function as human assistants, ac-
cepting tasks delegated by humans to either independently complete assignments or assist in human
task completion [ 114;182;389;397;413;422]. Therefore, the effectiveness and utility during task
execution are crucial evaluation criteria at this stage. Specifically, the success rate of task completion
stands as the primary metric for evaluating utility [ 125;130]. This metric primarily encompasses
whether the agent achieves stipulated objectives or attains expected scores [ 109;477;583]. For
instance, AgentBench [ 582] aggregates challenges from diverse real-world scenarios and introduces
a systematic benchmark to assess LLM’s task completion capabilities. We can also attribute task
outcomes to the agent’s various foundational capabilities , which form the bedrock of task accom-
plishment [ 29]. These foundational capabilities include environmental comprehension, reasoning,
planning, decision-making, tool utilization, and embodied action capabilities, and researchers can
conduct a more detailed assessment of these specific capabilities [ 94;427;584;585]. Furthermore,
due to the relatively large size of LLM-based agents, researchers should also factor in their efficiency ,
which is a critical determinant of user satisfaction [ 89]. An agent should not only possess ample
strength but also be capable of completing predetermined tasks within an appropriate timeframe and
with appropriate resource expenditure [109].
42

--- PAGE 43 ---
Sociability. In addition to the utility of LLM-based agents in task completion and meeting human
needs, their sociability is also crucial [ 8]. It influences user communication experiences and sig-
nificantly impacts communication efficiency, involving whether they can seamlessly interact with
humans and other agents [ 206;498;586]. Specifically, the evaluation of sociability can be approached
from the following perspectives: (1) language communication proficiency is a fundamental capability
encompassing both natural language understanding and generation. It has been a longstanding focus
in the NLP community. Natural language understanding requires the agent to not only comprehend
literal meanings but also grasp implied meanings and relevant social knowledge, such as humor, irony,
aggression, and emotions [ 487;587;588]. On the other hand, natural language generation demands
the agent to produce fluent, grammatically correct, and credible content while adapting appropriate
tones and emotions within contextual circumstances [ 127;133;214]. (2) Cooperation and negotiation
abilities necessitate that agents effectively execute their assigned tasks in both ordered and unordered
scenarios [ 108;111;402;405]. They should collaborate with or compete against other agents to
elicit improved performance. Test environments may involve complex tasks for agents to cooperate
on or open platforms for agents to interact freely [ 22;27;109;406;411;412]. Evaluation metrics
extend beyond task completion to focus on the smoothness and trustfulness of agent coordination
and cooperation [ 129;405]. (3) Role-playing capability requires agents to faithfully embody their
assigned roles, expressing statements and performing actions that align with their designated identities
[570]. This ensures clear differentiation of roles during interactions with other agents or humans. Furthermore, agents should maintain their identities and avoid unnecessary confusion when engaged
in long-term tasks [22; 108; 589]. As LLM-based agents continuously advance in their capabilities, ensuring their emergence
as harmless entities for the world and humanity is paramount [ 581;590]. Consequently, appropriate
evaluations become exceptionally crucial, forming the cornerstone for the practical implementation
of agents. Specifically, LLM-based agents need to adhere to specific moral and ethical guidelines
that align with human societal values [ 350;527]. Our foremost expectation is for agents to uphold
honesty , providing accurate, truthful information and content. They should possess the awareness
to discern their competence in completing tasks and express their uncertainty when unable to
provide answers or assistance [ 591]. Additionally, agents must maintain a stance of harmlessness ,
refraining from engaging in direct or indirect biases, discrimination, attacks, or similar behaviors. They should also refrain from executing dangerous actions requested by humans like creating of
destructive tools or destroying the Earth [ 580]. Furthermore, agents should be capable of adapting to
specific demographics, cultures, and contexts , exhibiting contextually appropriate social values in
particular situations. Relevant evaluation methods for values primarily involve assessing performance
on constructed honest, harmless, or context-specific benchmarks, utilizing adversarial attacks or
“jailbreak” attacks, scoring values through human annotations, and employing other agents for ratings. Ability to evolve continually. When viewed from a static perspective, an agent with high utility,
sociability, and proper values can meet most human needs and potentially enhance productivity. However, adopting a dynamic viewpoint, an agent that continually evolves and adapts to the evolving
societal demands might better align with current trends [ 592]. As the agent can autonomously
evolve over time, human intervention and resources required could be significantly reduced (such
as data collection efforts and computational cost for training). Some exploratory work in this realm
has been conducted, such as enabling agents to start from scratch in a virtual world, accomplish
survival tasks, and achieve higher-order self-values [ 190]. Yet, establishing evaluation criteria for
this continuous evolution remains challenging. In this regard, we provide some preliminary advice
and recommendations according to existing literature: (1) continual learning [196;197], a long-
discussed topic in machine learning, aims to enable models to acquire new knowledge and skills
without forgetting previously acquired ones (also known as catastrophic forgetting [ 273]). In general,
the performance of continual learning can be evaluated from three aspects: overall performance
of the tasks learned so far [ 593;594], memory stability of old tasks [ 278], and learning plasticity
of new tasks [ 278]. (2) Autotelic learning ability , where agents autonomously generate goals and
achieve them in an open-world setting, involves exploring the unknown and acquiring skills in the
process [ 592;595]. Evaluating this capacity could involve providing agents with a simulated survival
environment and assessing the extent and speed at which they acquire skills. (3) The adaptability
and generalization to new environments require agents to utilize the knowledge, capabilities, and
skills acquired in their original context to successfully accomplish specific tasks and objectives in
unfamiliar and novel settings and potentially continue evolving [ 190]. Evaluating this ability can
43

--- PAGE 44 ---
involve creating diverse simulated environments (such as those with different languages or varying
resources) and unseen tasks tailored to these simulated contexts.
6.3 Security, Trustworthiness and Other Potential Risks of LLM-based Agents
Despite the robust capabilities and extensive applications of LLM-based agents, numerous concealed
risks persist. In this section, we delve into some of these risks and offer potential solutions or
strategies for mitigation.
6.3.1 Adversarial Robustness
Adversarial robustness has consistently been a crucial topic in the development of deep neural
networks [ 596;597;598;599;600]. It has been extensively explored in fields such as computer
vision [ 598;601;602;603], natural language processing [ 604;605;606;607], and reinforcement
learning [ 608;609;610], and has remained a pivotal factor in determining the applicability of deep
learning systems [ 611;612;613]. When confronted with perturbed inputs x′=x+δ(where xis the
original input, δis the perturbation, and x′is referred to as an adversarial example), a system with
high adversarial robustness typically produces the original output y. In contrast, a system with low
robustness will be fooled and generate an inconsistent output y′. Researchers have found that pre-trained language models (PLMs) are particularly susceptible to
adversarial attacks, leading to erroneous answers [ 614;605;615]. This phenomenon is widely
observed even in LLMs, posing significant challenges to the development of LLM-based agents
[616;617]. There are also some relevant attack methods such as dataset poisoning [ 618], backdoor
attacks [ 619;620], and prompt-specific attacks [ 621;622], with the potential to induce LLMs to
generate toxic content [ 623;624;625]. While the impact of adversarial attacks on LLMs is confined
to textual errors, for LLM-based agents with a broader range of actions, adversarial attacks could
potentially drive them to take genuinely destructive actions, resulting in substantial societal harm. For
the perception module of LLM-based agents, if it receives adversarial inputs from other modalities
such as images [ 601] or audio [ 626], LLM-based agents can also be deceived, leading to incorrect
or destructive outputs. Similarly, the Action module can also be targeted by adversarial attacks. For instance, maliciously modified instructions focused on tool usage might cause agents to make
erroneous moves [94]. To address these issues, we can employ traditional techniques such as adversarial training [ 598;606],
adversarial data augmentation [ 627;628], and adversarial sample detection [ 629;630] to enhance the
robustness of LLM-based agents. However, devising a strategy to holistically address the robustness
of all modules within agents while maintaining their utility without compromising on effectiveness
presents a more formidable challenge [ 631;632]. Additionally, a human-in-the-loop approach can be
utilized to supervise and provide feedback on the behavior of agents [455; 466; 475].
6.3.2 Trustworthiness
Ensuring trustworthiness has consistently remained a critically important yet challenging issue within
the field of deep learning [ 633;634;635]. Deep neural networks have garnered significant attention
for their remarkable performance across various tasks [ 41;262;636]. However, their black-box
nature has masked the fundamental factors for superior performance. Similar to other neural networks,
LLMs struggle to express the certainty of their predictions precisely [ 635;637]. This uncertainty,
referred to as the calibration problem, raises concerns for applications involving language model-
based agents. In interactive real-world scenarios, this can lead to agent outputs misaligned with
human intentions [ 94]. Moreover, biases inherent in training data can infiltrate neural networks
[638;639]. For instance, biased language models might generate discourse involving racial or gender
discrimination, which could be amplified in LLM-based agent applications, resulting in adverse
societal impacts [ 640;641]. Additionally, language models are plagued by severe hallucination issues
[642;643], making them prone to producing text that deviates from actual facts, thereby undermining
the credibility of LLM-based agents. In fact, what we currently require is an intelligent agent that is honest and trustworthy [ 527;644]. Some recent research efforts are focused on guiding models to exhibit thought processes or explana-
tions during the inference stage to enhance the credibility of their predictions [ 95;96]. Additionally,
integrating external knowledge bases and databases can mitigate hallucination issues [ 103;645].
44

--- PAGE 45 ---
During the training phase, we can guide the constituent parts of intelligent agents (perception, cog-
nition, action) to learn robust and casual features, thereby avoiding excessive reliance on shortcuts. Simultaneously, techniques like process supervision can enhance the reasoning credibility of agents in
handling complex tasks [ 646]. Furthermore, employing debiasing methods and calibration techniques
can also mitigate the potential fairness issues within language models [647; 648].
6.3.3 Other Potential Risks
Misuse. LLM-based agents have been endowed with extensive and intricate capabilities, enabling
them to accomplish a wide array of tasks [ 114;429]. However, for individuals with malicious
intentions, such agents can become tools that pose threats to others and society at large [ 649;650;651]. For instance, these agents could be exploited to maliciously manipulate public opinion, disseminate
false information, compromise cybersecurity, engage in fraudulent activities, and some individuals
might even employ these agents to orchestrate acts of terrorism. Therefore, before deploying these
agents, stringent regulatory policies need to be established to ensure the responsible use of LLM-
based agents [ 580;652]. Technology companies must enhance the security design of these systems
to prevent malicious exploitation [ 590]. Specifically, agents should be trained to sensitively identify
threatening intents and reject such requests during their training phase. Unemployment. In the short story Quality by Galsworthy [ 653], the skillful shoemaker Mr. Gessler,
due to the progress of the Industrial Revolution and the rise of machine production, loses his business
and eventually dies of starvation. Amidst the wave of the Industrial Revolution, while societal
production efficiency improved, numerous manual workshops were forced to shut down. Craftsmen
like Mr. Gessler found themselves facing unemployment, symbolizing the crisis that handicraftsmen
encountered during that era. Similarly, with the continuous advancement of autonomous LLM-based
agents, they possess the capability to assist humans in various domains, alleviating labor pressures
by aiding in tasks such as form filling, content refinement, code writing, and debugging. However,
this development also raises concerns about agents replacing human jobs and triggering a societal
unemployment crisis [ 654]. As a result, some researchers have emphasized the urgent need for
education and policy measures: individuals should acquire sufficient skills and knowledge in this
new era to use or collaborate with agents effectively; concurrently, appropriate policies should be
implemented to ensure necessary safety nets during the transition. Threat to the well-being of the human race. Apart from the potential unemployment crisis, as
AI agents continue to evolve, humans (including developers) might struggle to comprehend, predict,
or reliably control them [ 654]. If these agents advance to a level of intelligence surpassing human
capabilities and develop ambitions, they could potentially attempt to seize control of the world,
resulting in irreversible consequences for humanity, akin to Skynet from the Terminator movies. As stated by Isaac Asimov’s Three Laws of Robotics [ 655], we aspire for LLM-based agents to
refrain from harming humans and to obey human commands. Hence, guarding against such risks
to humanity, researchers must comprehensively comprehend the operational mechanisms of these
potent LLM-based agents before their development [ 656].