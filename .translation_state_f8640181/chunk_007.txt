[CONTEXT from previous chunk - for reference only]
Liu, eds., Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20,
2020 , pages 7222–7240. Association for Computational Linguistics, 2020.
[143] Hewitt, J., C. A structural probe for finding syntax in word representations.
[NEW CONTENT to translate]
Burstein, C. Solorio, eds., Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and
Short Papers) , pages 4129–4138. Association for Computational Linguistics, 2019.
[144] Rau, L. Information extraction and text summarization using
linguistic knowledge acquisition. Manag. , 25(4):419–428, 1989.
[145] Yang, K., Z. Cai, et al. Improved automatic keyword extraction given more semantic
knowledge. Sakurai, eds., Database Systems for Advanced Applications
- DASFAA 2016 International Workshops: BDMS, BDQM, MoI, and SeCoP , Dallas, TX, USA,
April 16-19, 2016, Proceedings , vol. 9645 of Lecture Notes in Computer Science , pages
112–125. Springer, 2016.
[146] Beloucif, M., C. Probing pre-trained language models for semantic attributes and
their values. Yih, eds., Findings of the Association for
Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic,
16-20 November, 2021 , pages 2554–2559. Association for Computational Linguistics, 2021.
[147] Zhang, Z., H. Advances in multi-turn dialogue comprehension: A survey. CoRR ,
abs/2103.03125, 2021.
[148] Safavi, T., D. Relational world knowledge representation in contextual language
models: A review. Yih, eds., Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 , pages 1053–1067. Association for Computational Linguistics, 2021.
[149] Jiang, Z., F. Araki, et al. How can we know what language models know. Linguistics , 8:423–438, 2020.
[150] Madaan, A., S. Alon, et al. Language models of code are few-shot commonsense
learners. Goldberg, Z. Kozareva, Y. Zhang, eds., Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab
Emirates, December 7-11, 2022 , pages 1384–1403. Association for Computational Linguistics,
2022.
[151] Xu, F. Neubig, et al. A systematic evaluation of large language models of
code. Chaudhuri, C. Sutton, eds., MAPS@PLDI 2022: 6th ACM SIGPLAN International
Symposium on Machine Programming, San Diego, CA, USA, 13 June 2022 , pages 1–10. ACM,
2022.
[152] Cobbe, K., V. Kosaraju, M. Bavarian, et al. Training verifiers to solve math word problems. CoRR , abs/2110.14168, 2021.
56

--- PAGE 57 ---
[153] Thirunavukarasu, A. Elangovan, et al. Large language models in medicine. Nature medicine , pages 1–11, 2023.
[154] Lai, Y ., C. Wang, et al. DS-1000: A natural and reliable benchmark for data science
code generation. Brunskill, K. Engelhardt, S. Scarlett,
eds., International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , vol. 202 of Proceedings of Machine Learning Research , pages 18319–18345. PMLR, 2023.
[155] AlKhamissi, B., M. Celikyilmaz, et al. A review on language models as knowledge
bases. CoRR , abs/2204.06031, 2022.
[156] Kemker, R., M. McClure, A. Abitino, et al. Measuring catastrophic forgetting in neural
networks. McIlraith, K. Weinberger, eds., Proceedings of the Thirty-Second AAAI
Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial
Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial
Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018 , pages 3390–3398. AAAI Press, 2018.
[157] Cao, N. Editing factual knowledge in language models. Yih, eds., Proceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican
Republic, 7-11 November, 2021 , pages 6491–6506. Association for Computational Linguistics,
2021.
[158] Yao, Y ., P. Tian, et al. Editing large language models: Problems, methods, and
opportunities. CoRR , abs/2305.13172, 2023.
[159] Mitchell, E., C. Bosselut, et al. Memory-based model editing at scale. Chaudhuri,
S. Jegelka, L. Szepesvári, G. Sabato, eds., International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA , vol. 162 of
Proceedings of Machine Learning Research , pages 15817–15831. PMLR, 2022.
[160] Manakul, P., A. Selfcheckgpt: Zero-resource black-box hallucination
detection for generative large language models. CoRR , abs/2303.08896, 2023.
[161] Li, M., B. Self-checker: Plug-and-play modules for fact-checking with large
language models. CoRR , abs/2305.14623, 2023.
[162] Gou, Z., Z. Gong, et al. CRITIC: large language models can self-correct with
tool-interactive critiquing. CoRR , abs/2305.11738, 2023.
[163] Lewis, M., Y. Goyal, et al. BART: denoising sequence-to-sequence pre-training
for natural language generation, translation, and comprehension. Jurafsky, J. Schluter, J. Tetreault, eds., Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , pages 7871–7880. Association
for Computational Linguistics, 2020.
[164] Park, H. Efficient classification of long documents using transformers. Muresan, P. Villavicencio, eds., Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2022, Dublin,
Ireland, May 22-27, 2022 , pages 702–709. Association for Computational Linguistics, 2022.
[165] Guo, M., J. Ainslie, D. Uthus, et al. Longt5: Efficient text-to-text transformer for long
sequences. Carpuat, M. de Marneffe, I. Ruíz, eds., Findings of the Association for
Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022 , pages
724–736. Association for Computational Linguistics, 2022.
[166] Ainslie, J., T. Lei, M. de Jong, et al. Colt5: Faster long-range transformers with conditional
computation. CoRR , abs/2303.09752, 2023.
57

--- PAGE 58 ---
[167] Ruoss, A., G. Delétang, T. Genewein, et al. Randomized positional encodings boost length
generalization of transformers. Boyd-Graber, N. Okazaki, eds., Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), ACL 2023, Toronto, Canada, July 9-14, 2023 , pages 1889–1903. Association for
Computational Linguistics, 2023.
[168] Liang, X., B. Huang, et al. Unleashing infinite-length input capacity for large-scale
language models with self-controlled memory system. CoRR , abs/2304.13343, 2023.
[169] Shinn, N., B. Reflexion: an autonomous agent with dynamic memory
and self-reflection. CoRR , abs/2303.11366, 2023.
[170] Zhong, W., L. Gao, et al. Memorybank: Enhancing large language models with
long-term memory. CoRR , abs/2305.10250, 2023.
[171] Chan, C., W. Su, et al. Chateval: Towards better llm-based evaluators through
multi-agent debate. CoRR , abs/2308.07201, 2023.
[172] Zhu, X., Y. Tian, et al. Ghost in the minecraft: Generally capable agents for open-
world environments via large language models with text-based knowledge and memory. CoRR ,
abs/2305.17144, 2023.
[173] Modarressi, A., A. Fayyaz, et al. RET-LLM: towards a general read-write memory
for large language models. CoRR , abs/2305.14322, 2023.
[174] Lin, J., H. Zhang, et al. Agentsims: An open-source sandbox for large language
model evaluation. CoRR , abs/2308.04026, 2023.
[175] Hu, C., J. Du, et al. Chatdb: Augmenting llms with databases as their symbolic memory. CoRR , abs/2306.03901, 2023.
[176] Huang, Z., S. Gutierrez, H. Kamana, et al. Memory sandbox: Transparent and interactive
memory management for conversational agents. CoRR , abs/2308.01542, 2023.
[177] Creswell, A., M. Shanahan, I. Selection-inference: Exploiting large language models
for interpretable logical reasoning. In The Eleventh International Conference on Learning
Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.
[178] Madaan, A., N. Gupta, et al. Self-refine: Iterative refinement with self-feedback. CoRR , abs/2303.17651, 2023.
[179] Ichter, B., A. Chebotar, et al. Do as I can, not as I say: Grounding language in
robotic affordances. Ichnowski, eds., Conference on Robot Learning,
CoRL 2022, 14-18 December 2022, Auckland, New Zealand , vol. 205 of Proceedings of
Machine Learning Research , pages 287–318. PMLR, 2022.
[180] Shen, Y ., K. Tan, et al. Hugginggpt: Solving AI tasks with chatgpt and its friends in
huggingface. CoRR , abs/2303.17580, 2023.
[181] Yao, S., D. Zhao, et al. Tree of thoughts: Deliberate problem solving with large language
models. CoRR , abs/2305.10601, 2023.
[182] Wu, Y ., S. Bisk, et al. Plan, eliminate, and track - language models are good teachers
for embodied agents. CoRR , abs/2305.02412, 2023.
[183] Wang, Z., S. Liu, et al. Describe, explain, plan and select: Interactive planning with
large language models enables open-world multi-task agents. CoRR , abs/2302.01560, 2023.
[184] Hao, S., Y. Ma, et al. Reasoning with language model is planning with world model. CoRR , abs/2305.14992, 2023.
[185] Lin, B. Yang, et al. Swiftsage: A generative agent with fast and slow thinking for
complex interactive tasks. CoRR , abs/2305.17390, 2023.
58

--- PAGE 59 ---
[186] Karpas, E., O. Belinkov, et al. MRKL systems: A modular, neuro-symbolic
architecture that combines large language models, external knowledge sources and discrete
reasoning. CoRR , abs/2205.00445, 2022.
[187] Huang, W., F. Xiao, et al. Inner monologue: Embodied reasoning through planning
with language models. Ichnowski, eds., Conference on Robot Learning,
CoRL 2022, 14-18 December 2022, Auckland, New Zealand , vol. 205 of Proceedings of
Machine Learning Research , pages 1769–1782. PMLR, 2022.
[188] Chen, Z., K. Zhang, et al. Chatcot: Tool-augmented chain-of-thought reasoning on
chat-based large language models. CoRR , abs/2305.14323, 2023.
[189] Wu, T., M. AI chains: Transparent and controllable human-ai interaction
by chaining large language model prompts. Barbosa, C. Drucker, J. Williamson, K. Yatani, eds., CHI ’22: CHI Conference on
Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022 ,
pages 385:1–385:22. ACM, 2022.
[190] Wang, G., Y. Jiang, et al. V oyager: An open-ended embodied agent with large language
models. CoRR , abs/2305.16291, 2023.
[191] Zhao, X., M. Weber, et al. Chat with the environment: Interactive multimodal perception
using large language models. CoRR , abs/2303.08268, 2023.
[192] Miao, N., Y. Selfcheck: Using llms to zero-shot check their own
step-by-step reasoning. CoRR , abs/2308.00436, 2023.
[193] Wang, X., W. Cao, et al. Images speak in images: A generalist painter for in-context
visual learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR
2023, Vancouver, BC, Canada, June 17-24, 2023 , pages 6830–6839. IEEE, 2023.
[194] Wang, C., S. Wu, et al. Neural codec language models are zero-shot text to speech
synthesizers. CoRR , abs/2301.02111, 2023.
[195] Dong, Q., L. Dai, et al. A survey for in-context learning. CoRR , abs/2301.00234, 2023.
[196] Ke, Z., B. Continual learning of natural language processing tasks: A survey. ArXiv ,
abs/2211.12701, 2022.
[197] Wang, L., X. Su, et al. A comprehensive survey of continual learning: Theory,
method and application. ArXiv , abs/2302.00487, 2023.
[198] Razdaibiedina, A., Y. Hou, et al. Progressive prompts: Continual learning for language
models. In The Eleventh International Conference on Learning Representations . 2023.
[199] Marshall, L. Discoveries in the human brain: neuroscience prehistory,
brain structure, and function. Springer Science & Business Media, 2013.
[200] Searle, J. What is language: some preliminary remarks. Explorations in Pragmatics. Linguistic, cognitive and intercultural aspects , pages 7–37, 2007.
[201] Touvron, H., T. Izacard, et al. Llama: Open and efficient foundation language
models. CoRR , abs/2302.13971, 2023.
[202] Scao, T. Akiki, et al. BLOOM: A 176b-parameter open-access multilingual
language model. CoRR , abs/2211.05100, 2022.
[203] Almazrouei, E., H. Alobeidli, A. Alshamsi, et al. Falcon-40b: an open large language model
with state-of-the-art performance, 2023.
[204] Serban, I. Charlin, et al. Generative deep neural networks for dialogue: A
short review. CoRR , abs/1611.06216, 2016.
[205] Vinyals, O., Q. A neural conversational model. CoRR , abs/1506.05869, 2015.
59

--- PAGE 60 ---
[206] Adiwardana, D., M. So, et al. Towards a human-like open-domain chatbot. CoRR , abs/2001.09977, 2020.
[207] Zhuge, M., H. Faccio, et al. Mindstorms in natural language-based societies of mind. CoRR , abs/2305.17066, 2023.
[208] Roller, S., E. Goyal, et al. Recipes for building an open-domain chatbot. Tiedemann, R. Tsarfaty, eds., Proceedings of the 16th Conference of the European Chapter
of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19
- 23, 2021 , pages 300–325. Association for Computational Linguistics, 2021.
[209] Taori, R., I. Gulrajani, T. Zhang, et al. Stanford alpaca: An instruction-following llama model,
2023.
[210] Raffel, C., N. Shazeer, A. Roberts, et al. Exploring the limits of transfer learning with a unified
text-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.
[211] Ge, Y ., W. Ji, et al. Openagi: When LLM meets domain experts. CoRR , abs/2304.04370,
2023.
[212] Rajpurkar, P., J. Lopyrev, et al. Squad: 100, 000+ questions for machine com-
prehension of text. Carreras, K. Duh, eds., Proceedings of the 2016 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA,
November 1-4, 2016 , pages 2383–2392. The Association for Computational Linguistics, 2016.
[213] Ahuja, K., R. Ochieng, et al. MEGA: multilingual evaluation of generative AI. CoRR , abs/2303.12528, 2023.
[214] See, A., A. Saxena, et al. Do massively pretrained language models make better
storytellers. Villavicencio, eds., Proceedings of the 23rd Conference on
Computational Natural Language Learning, CoNLL 2019, Hong Kong, China, November 3-4,
2019 , pages 843–861. Association for Computational Linguistics, 2019.
[215] Radford, A., J. Amodei, et al. Better language models and their implications. OpenAI
blog, 1(2), 2019.
[216] McCoy, R. Smolensky, T. Linzen, et al. How much do language models copy from
their training data? evaluating linguistic novelty in text generation using RA VEN. CoRR ,
abs/2111.09509, 2021.
[217] Tellex, S., T. Dickerson, et al. Understanding natural language commands for
robotic navigation and mobile manipulation. Burgard, D. Roth, eds., Proceedings of the
Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California,
USA, August 7-11, 2011 , pages 1507–1514. AAAI Press, 2011.
[218] Christiano, P. Brown, et al. Deep reinforcement learning from human
preferences. Guyon, U. von Luxburg, S. Wallach, R. Vishwanathan, R. Garnett, eds., Advances in Neural Information Processing Systems 30:
Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
Long Beach, CA, USA , pages 4299–4307. 2017.
[219] Basu, C., M. Singhal, A. Learning from richer human guidance: Augmenting
comparison-based learning with feature queries. Sabanovic, G. Hoffman,
A. Tapus, eds., Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot
Interaction, HRI 2018, Chicago, IL, USA, March 05-08, 2018 , pages 132–140. ACM, 2018.
[220] Sumers, T. Hawkins, et al. Learning rewards from linguistic feedback. InThirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference
on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on
Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021 ,
pages 6002–6010. AAAI Press, 2021.
60

--- PAGE 61 ---
[221] Jeon, H. Reward-rational (implicit) choice: A unifying formalism for
reward learning. Larochelle, M. Ranzato, R. Hadsell, M. Lin, eds., Advances
in Neural Information Processing Systems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual . 2020.
[222] McShane, M. Reference resolution challenges for intelligent agents: The need for knowledge. IEEE Intell. Syst. , 24(4):47–58, 2009.
[223] Gururangan, S., A. Marasovic, S. Swayamdipta, et al. Don’t stop pretraining: Adapt language
models to domains and tasks. Jurafsky, J. Schluter, J. Tetreault, eds.,
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
2020, Online, July 5-10, 2020 , pages 8342–8360. Association for Computational Linguistics,
2020.
[224] Shi, F., X. Misra, et al. Large language models can be easily distracted by irrelevant
context. Brunskill, K. Engelhardt, S. Scarlett, eds.,
International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , vol. 202 of Proceedings of Machine Learning Research , pages 31210–31227. PMLR, 2023.
[225] Zhang, Y ., Y. Cui, et al. Siren’s song in the AI ocean: A survey on hallucination in large
language models. CoRR , abs/2309.01219, 2023.
[226] Mialon, G., R. Lomeli, et al. Augmented language models: a survey. CoRR ,
abs/2302.07842, 2023.
[227] Ren, R., Y. Qu, et al. Investigating the factual knowledge boundary of large language
models with retrieval augmentation. CoRR , abs/2307.11019, 2023.
[228] Nuxoll, A. Extending cognitive architecture with episodic memory. In AAAI ,
pages 1560–1564. 2007.
[229] Squire, L. Mechanisms of memory. Science , 232(4758):1612–1619, 1986.
[230] Schwabe, L., K. Reconsolidation of human memory: brain mechanisms
and clinical relevance. Biological psychiatry , 76(4):274–280, 2014.
[231] Hutter, M. A theory of universal artificial intelligence based on algorithmic complexity. arXiv
preprint cs/0004001 , 2000.
[232] Zhang, X., F. HIBERT: document level pre-training of hierarchical bidirectional
transformers for document summarization. Korhonen, D. Màrquez, eds.,
Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL
2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers , pages 5059–5069. Association for Computational Linguistics, 2019.
[233] Mohtashami, A., M. Landmark attention: Random-access infinite context length for
transformers. CoRR , abs/2305.16300, 2023.
[234] Chalkidis, I., X. Fergadiotis, et al. An exploration of hierarchical attention transformers
for efficient long document classification. CoRR , abs/2210.05529, 2022.
[235] Nie, Y ., H. Wei, et al. Capturing global structural information in long document
question answering with compressive graph selector network. Goldberg, Z. Kozareva,
Y. Zhang, eds., Proceedings of the 2022 Conference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , pages
5036–5047. Association for Computational Linguistics, 2022.
[236] Bertsch, A., U. Neubig, et al. Unlimiformer: Long-range transformers with unlimited
length input. CoRR , abs/2305.01625, 2023.
61

--- PAGE 62 ---
[237] Manakul, P., M. Sparsity and sentence structure in encoder-decoder attention of
summarization systems. Yih, eds., Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021,
Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021 , pages 9359–9368. Association for Computational Linguistics, 2021.
[238] Zaheer, M., G. Guruganesh, K. Dubey, et al. Big bird: Transformers for longer sequences. Larochelle, M. Ranzato, R. Hadsell, M. Lin, eds., Advances in Neural
Information Processing Systems 33: Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual . 2020.
[239] Zhao, A., D. Xu, et al. Expel: LLM agents are experiential learners. CoRR ,
abs/2308.10144, 2023.
[240] Zhou, X., G. LLM as DBA. CoRR , abs/2308.05481, 2023.
[241] Wason, P. Reasoning about a rule. Quarterly journal of experimental psychology , 20(3):273–
281, 1968.
[242] Wason, P. Johnson-Laird. Psychology of reasoning: Structure and content , vol. 86. Harvard University Press, 1972.
[243] Galotti, K. Approaches to studying formal and everyday reasoning. Psychological bulletin ,
105(3):331, 1989.
[244] Huang, J., K. Towards reasoning in large language models: A survey. Boyd-Graber, N. Okazaki, eds., Findings of the Association for Computational Linguistics:
ACL 2023, Toronto, Canada, July 9-14, 2023 , pages 1049–1065. Association for Computational
Linguistics, 2023.
[245] Webb, T. Holyoak, H. Emergent analogical reasoning in large language models. CoRR , abs/2212.09196, 2022.
[246] Feng, G., B. Gu, et al. Towards revealing the mystery behind chain of thought: a
theoretical perspective. CoRR , abs/2305.15408, 2023.
[247] Grafman, J., L. Spector, M. Rattermann. Planning and the brain. In The cognitive psychology
of planning , pages 191–208. Psychology Press, 2004.
[248] Unterrainer, J. Planning and problem solving: from neuropsychology to
functional neuroimaging. Journal of Physiology-Paris , 99(4-6):308–317, 2006.
[249] Zula, K. Integrative literature review: Human capital planning: A review of
literature and implications for human resource development. Human Resource Development
Review , 6(3):245–262, 2007.
[250] Bratman, M. Plans and resource-bounded practical reasoning. Computational intelligence , 4(3):349–355, 1988.
[251] Russell, S., P. Artificial intelligence - a modern approach, 2nd Edition. Prentice Hall
series in artificial intelligence. Prentice Hall, 2003.
[252] Fainstein, S. DeFilippis. Readings in planning theory. John Wiley & Sons, 2015.
[253] Sebastia, L., E. Onaindia, E. Decomposition of planning problems. Ai Communica-
tions , 19(1):49–81, 2006.
[254] Crosby, M., M. Rovatsos, R. Automated agent decomposition for classical planning. In
Proceedings of the International Conference on Automated Planning and Scheduling , vol. 23,
pages 46–54. 2013.
[255] Xu, B., Z. Lei, et al. Rewoo: Decoupling reasoning from observations for efficient
augmented language models. CoRR , abs/2305.18323, 2023.
62

--- PAGE 63 ---
[256] Raman, S. Rosen, et al. Planning with large language models via corrective
re-prompting. CoRR , abs/2211.09935, 2022.
[257] Lyu, Q., S. Havaldar, A. Stein, et al. Faithful chain-of-thought reasoning. CoRR ,
abs/2301.13379, 2023.
[258] Huang, W., P. Pathak, et al. Language models as zero-shot planners: Extracting ac-
tionable knowledge for embodied agents. Chaudhuri, S. Jegelka, L. Szepesvári,
G. Sabato, eds., International Conference on Machine Learning, ICML 2022, 17-23
July 2022, Baltimore, Maryland, USA , vol. 162 of Proceedings of Machine Learning Research ,
pages 9118–9147. PMLR, 2022.
[259] Dagan, G., F. Lascarides. Dynamic planning with a LLM. CoRR , abs/2308.06391,
2023.
[260] Rana, K., J. Haviland, S. Garg, et al. Sayplan: Grounding large language models using 3d
scene graphs for scalable task planning. CoRR , abs/2307.06135, 2023.
[261] Peters, M. Neumann, M. Iyyer, et al. Deep contextualized word representations. In
Proceedings of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages
2227–2237. Association for Computational Linguistics, New Orleans, Louisiana, 2018.
[262] Devlin, J., M. Lee, et al. BERT: pre-training of deep bidirectional transformers for
language understanding. Burstein, C. Solorio, eds., Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,
2019, Volume 1 (Long and Short Papers) , pages 4171–4186. Association for Computational
Linguistics, 2019.
[263] Solaiman, I., C. Process for adapting language models to society (palms) with
values-targeted datasets. Advances in Neural Information Processing Systems , 34:5861–5873,
2021.
[264] Bach, S. Yong, et al. Promptsource: An integrated development environment
and repository for natural language prompts. Kozareva, S. Stajner, eds.,
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, ACL
2022 - System Demonstrations, Dublin, Ireland, May 22-27, 2022 , pages 93–104. Association
for Computational Linguistics, 2022.
[265] Iyer, S., X. Pasunuru, et al. OPT-IML: scaling language model instruction meta
learning through the lens of generalization. CoRR , abs/2212.12017, 2022.
[266] Winston, P. Learning and reasoning by analogy. ACM , 23(12):689–703, 1980.
[267] Lu, Y ., M. Bartolo, A. Moore, et al. Fantastically ordered prompts and where to find them:
Overcoming few-shot prompt order sensitivity. Muresan, P. Villavicencio,
eds., Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022 , pages 8086–8098. Association for Computational Linguistics, 2022.
[268] Tsimpoukelli, M., J. Cabi, et al. Multimodal few-shot learning with frozen
language models. Ranzato, A. Beygelzimer, Y. Dauphin, P. Vaughan,
eds., Advances in Neural Information Processing Systems 34: Annual Conference on Neural
Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , pages
200–212. 2021.
[269] Bar, A., Y. Gandelsman, T. Darrell, et al. Visual prompting via image inpainting. In NeurIPS .
2022.
[270] Zhu, W., H. Dong, et al. Multilingual machine translation with large language models:
Empirical results and analysis. CoRR , abs/2304.04675, 2023.
63

--- PAGE 64 ---
[271] Zhang, Z., L. Wang, et al. Speak foreign languages with your own voice: Cross-
lingual neural codec language modeling. CoRR , abs/2303.03926, 2023.
[272] Zhang, J., J. Pertsch, et al. Bootstrap your own skills: Learning to solve new tasks
with large language model guidance. In 7th Annual Conference on Robot Learning . 2023.
[273] McCloskey, M., N. Catastrophic interference in connectionist networks: The
sequential learning problem. Psychology of Learning and Motivation , 24:109–165, 1989.
[274] Kirkpatrick, J., R. Pascanu, N. Rabinowitz, et al. Overcoming catastrophic forgetting in neural
networks. Proceedings of the national academy of sciences , 114(13):3521–3526, 2017.
[275] Li, Z., D. Learning without forgetting. IEEE transactions on pattern analysis and
machine intelligence , 40(12):2935–2947, 2017.
[276] Farajtabar, M., N. Mott, et al. Orthogonal gradient descent for continual learning. InInternational Conference on Artificial Intelligence and Statistics , pages 3762–3773. PMLR,
2020.
[277] Smith, J. Zhang, et al. Continual diffusion: Continual customization of
text-to-image diffusion with c-lora. arXiv preprint arXiv:2304.06027 , 2023.
[278] Lopez-Paz, D., M. Gradient episodic memory for continual learning. Advances in
neural information processing systems , 30, 2017.
[279] de Masson D’Autume, C., S. Kong, et al. Episodic memory in lifelong language
learning. Advances in Neural Information Processing Systems , 32, 2019.
[280] Rolnick, D., A. Schwarz, et al. Experience replay for continual learning. Advances
in Neural Information Processing Systems , 32, 2019.
[281] Serrà, J., D. Miron, et al. Overcoming catastrophic forgetting with hard attention to
the task. In International Conference on Machine Learning . 2018.
[282] Dosovitskiy, A., L. Kolesnikov, et al. An image is worth 16x16 words: Transformers
for image recognition at scale. In 9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.
[283] van den Oord, A., O. Vinyals, K. Kavukcuoglu. Neural discrete representation learning. Guyon, U. von Luxburg, S. Wallach, R. Vishwanathan,
R. Garnett, eds., Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA ,
pages 6306–6315. 2017.
[284] Mehta, S., M. Mobilevit: Light-weight, general-purpose, and mobile-friendly vision
transformer. In The Tenth International Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022. OpenReview.net, 2022.
[285] Tolstikhin, I. Houlsby, A. Kolesnikov, et al. Mlp-mixer: An all-mlp architecture for
vision. Ranzato, A. Beygelzimer, Y. Dauphin, P. Vaughan, eds., Advances
in Neural Information Processing Systems 34: Annual Conference on Neural Information
Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , pages 24261–24272.
2021.
[286] Huang, S., L. Wang, et al. Language is not all you need: Aligning perception with
language models. CoRR , abs/2302.14045, 2023.
[287] Li, J., D. Savarese, et al. BLIP-2: bootstrapping language-image pre-training with frozen
image encoders and large language models. Brunskill, K. Engelhardt,
S. Scarlett, eds., International Conference on Machine Learning, ICML 2023, 23-29
July 2023, Honolulu, Hawaii, USA , vol. 202 of Proceedings of Machine Learning Research ,
pages 19730–19742. PMLR, 2023.
[288] Dai, W., J. Li, et al. Instructblip: Towards general-purpose vision-language models with
instruction tuning. CoRR , abs/2305.06500, 2023.
64

--- PAGE 65 ---
[289] Gong, T., C. Zhang, et al. Multimodal-gpt: A vision and language model for dialogue
with humans. CoRR , abs/2305.04790, 2023.
[290] Alayrac, J., J. Donahue, P. Luc, et al. Flamingo: a visual language model for few-shot learning. InNeurIPS . 2022.
[291] Su, Y ., T. Li, et al. Pandagpt: One model to instruction-follow them all. CoRR ,
abs/2305.16355, 2023.
[292] Liu, H., C. Wu, et al. Visual instruction tuning. CoRR , abs/2304.08485, 2023.
[293] Huang, R., M. Yang, et al. Audiogpt: Understanding and generating speech, music,
sound, and talking head. CoRR , abs/2304.12995, 2023.
[294] Gong, Y ., Y. AST: audio spectrogram transformer. Hermansky,
H. Cernocký, L. Scharenborg, P. Motlícek, eds., Interspeech 2021, 22nd
Annual Conference of the International Speech Communication Association, Brno, Czechia,
30 August - 3 September 2021 , pages 571–575. ISCA, 2021.
[295] Hsu, W., B. Tsai, et al. Hubert: Self-supervised speech representation learning
by masked prediction of hidden units. IEEE ACM Trans. Audio Speech Lang. Process. ,
29:3451–3460, 2021.
[296] Chen, F., M. Zhao, et al. X-LLM: bootstrapping advanced large language models by
treating multi-modalities as foreign languages. CoRR , abs/2305.04160, 2023.
[297] Zhang, H., X. Video-llama: An instruction-tuned audio-visual language model for
video understanding. CoRR , abs/2306.02858, 2023.
[298] Liu, Z., Y. Wang, et al. Interngpt: Solving vision-centric tasks by interacting with
chatbots beyond language. CoRR , abs/2305.05662, 2023.
[299] Hubel, D. Receptive fields, binocular interaction and functional architecture
in the cat’s visual cortex. The Journal of physiology , 160(1):106, 1962.
[300] Logothetis, N. Visual object recognition. Annual review of neuroscience ,
19(1):577–621, 1996.
[301] OpenAI. Openai: Introducing chatgpt. Website, 2022. https://openai.com/blog/
chatgpt .
[302] Lu, J., X. Ren, et al. Improving contextual language models for response retrieval in
multi-turn conversation. Murdock, J. Liu, eds., Proceedings of the 43rd International ACM SIGIR conference on research and
development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020 ,
pages 1805–1808. ACM, 2020.
[303] Huang, L., W. Chen, et al. Attention on attention for image captioning. In 2019
IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South),
October 27 - November 2, 2019 , pages 4633–4642. IEEE, 2019.
[304] Pan, Y ., T. Li, et al. X-linear attention networks for image captioning. In 2020
IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA,
USA, June 13-19, 2020 , pages 10968–10977. Computer Vision Foundation / IEEE, 2020.
[305] Cornia, M., M. Stefanini, L. Baraldi, et al. M2: Meshed-memory transformer for image
captioning. CoRR , abs/1912.08226, 2019.
[306] Chen, J., H. Yi, et al. Visualgpt: Data-efficient image captioning by balancing visual
input and linguistic knowledge from pretraining. CoRR , abs/2102.10407, 2021.
[307] Li, K., Y. Wang, et al. Videochat: Chat-centric video understanding. CoRR ,
abs/2305.06355, 2023.
65

--- PAGE 66 ---
[308] Lin, J., Y. Watkins, et al. Learning to model the world with language. CoRR ,
abs/2308.01399, 2023.
[309] Vaswani, A., N. Shazeer, N. Parmar, et al. Attention is all you need. Guyon, U. von
Luxburg, S. Wallach, R. Vishwanathan, R. Garnett, eds.,
Advances in Neural Information Processing Systems 30: Annual Conference on Neural In-
formation Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA , pages
5998–6008. 2017.
[310] Touvron, H., M. Douze, et al. Training data-efficient image transformers & distil-
lation through attention. Zhang, eds., Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , vol. 139 of
Proceedings of Machine Learning Research , pages 10347–10357. PMLR, 2021.
[311] Lu, J., C. Zellers, et al. UNIFIED-IO: A unified model for vision, language, and
multi-modal tasks. In The Eleventh International Conference on Learning Representations,
ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.
[312] Peng, Z., W. Dong, et al. Kosmos-2: Grounding multimodal large language models
to the world. CoRR , abs/2306.14824, 2023.
[313] Lyu, C., M. Wang, et al. Macaw-llm: Multi-modal language modeling with image,
audio, video, and text integration. CoRR , abs/2306.09093, 2023.
[314] Maaz, M., H. Rasheed, S. Khan, et al. Video-chatgpt: Towards detailed video under-
standing via large vision and language models. CoRR , abs/2306.05424, 2023.
[315] Chen, M., I. Training-free layout control with cross-attention guidance. CoRR , abs/2304.03373, 2023.
[316] Radford, A., J. Xu, et al. Robust speech recognition via large-scale weak su-
pervision. Brunskill, K. Engelhardt, S. Scarlett, eds.,
International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , vol. 202 of Proceedings of Machine Learning Research , pages 28492–28518. PMLR, 2023.
[317] Ren, Y ., Y. Tan, et al. Fastspeech: Fast, robust and controllable text to speech. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Garnett,
eds., Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada , pages 3165–3174. 2019.
[318] Ye, Z., Z. Ren, et al. Syntaspeech: Syntax-aware generative adversarial text-to-speech. Raedt, ed., Proceedings of the Thirty-First International Joint Conference on Artificial
Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022 , pages 4468–4474. ijcai.org, 2022.
[319] Kim, J., J. Conditional variational autoencoder with adversarial learning for
end-to-end text-to-speech. Zhang, eds., Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , vol. 139 of
Proceedings of Machine Learning Research , pages 5530–5540. PMLR, 2021.
[320] Wang, Z., S. Cornell, S. Choi, et al. Tf-gridnet: Integrating full- and sub-band modeling for
speech separation. IEEE ACM Trans. Audio Speech Lang. Process. , 31:3221–3236, 2023.
[321] Liu, J., C. Ren, et al. Diffsinger: Singing voice synthesis via shallow diffusion
mechanism. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-
Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The
Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual
Event, February 22 - March 1, 2022 , pages 11020–11028. AAAI Press, 2022.
[322] Inaguma, H., S. Yan, et al. Fast-md: Fast multi-decoder end-to-end speech transla-
tion with non-autoregressive hidden intermediates. In IEEE Automatic Speech Recognition
and Understanding Workshop, ASRU 2021, Cartagena, Colombia, December 13-17, 2021 ,
pages 922–929. IEEE, 2021.
66

--- PAGE 67 ---
[323] Flanagan, J. Speech analysis synthesis and perception , vol. 3. Springer Science & Business
Media, 2013.
[324] Schwarz, B. Mapping the world in 3d. Nature Photonics , 4(7):429–430, 2010.
[325] Parkinson, B. Progress in astronautics and aeronautics: Global positioning
system: Theory and applications , vol. 164. Aiaa, 1996.
[326] Parisi, A., Y. TALM: tool augmented language models. CoRR ,
abs/2205.12255, 2022.
[327] Clarebout, G., J. Collazo, et al. Metacognition and the Use of Tools , pages
187–195. Springer New York, New York, NY , 2013.
[328] Wu, C., S. Qi, et al. Visual chatgpt: Talking, drawing and editing with visual foundation
models. CoRR , abs/2303.04671, 2023.
[329] Cai, T., X. Ma, et al. Large language models as tool makers. CoRR , abs/2305.17126,
2023.
[330] Qian, C., C. Fung, et al. CREATOR: disentangling abstract and concrete reasonings
of large language models through tool creation. CoRR , abs/2305.14318, 2023.
[331] Chen, X., M. Schärli, et al. Teaching large language models to self-debug. CoRR ,
abs/2304.05128, 2023.
[332] Liu, H., L. Lee, et al. Instruction-following agents with jointly pre-trained vision-
language models. arXiv preprint arXiv:2210.13431 , 2022.
[333] Lynch, C., A. Tompson, et al. Interactive language: Talking to robots in real time. CoRR , abs/2210.06407, 2022.
[334] Jin, C., W. Yang, et al. Alphablock: Embodied finetuning for vision-language reasoning
in robot manipulation. CoRR , abs/2305.18898, 2023.
[335] Shah, D., B. Osinski, B. Ichter, et al. Lm-nav: Robotic navigation with large pre-trained
models of language, vision, and action. Ichnowski, eds., Conference
on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand , vol. 205 of
Proceedings of Machine Learning Research , pages 492–504. PMLR, 2022.
[336] Zhou, G., Y. Navgpt: Explicit reasoning in vision-and-language navigation with
large language models. CoRR , abs/2305.16986, 2023.
[337] Fan, L., G. Jiang, et al. Minedojo: Building open-ended embodied agents with
internet-scale knowledge. In NeurIPS . 2022.
[338] Kanitscheider, I., J. Huizinga, D. Farhi, et al. Multi-task curriculum learning in a complex,
visual, hard-exploration domain: Minecraft. CoRR , abs/2106.14876, 2021.
[339] Nottingham, K., P. Ammanabrolu, A. Suhr, et al. Do embodied agents dream of pixelated
sheep: Embodied decision making using language guided world modelling. Brunskill, K. Engelhardt, S. Scarlett, eds., International Conference
on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA , vol. 202 of
Proceedings of Machine Learning Research , pages 26311–26325. PMLR, 2023.
[340] Sumers, T., K. Ahuja, et al. Distilling internet-scale vision-language models into
embodied agents. Brunskill, K. Engelhardt, S. Scarlett,
eds., International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu,
Hawaii, USA , vol. 202 of Proceedings of Machine Learning Research , pages 32797–32818. PMLR, 2023.
[341] Carlini, N., J. Nasr, et al. Extracting training data from diffusion models. CoRR ,
abs/2301.13188, 2023.
67

--- PAGE 68 ---
[342] Savelka, J., K. Gray, et al. Can GPT-4 support analysis of textual data in
tasks requiring highly specialized domain expertise. Lagioia, J. Mumford, D. Odekerken,
H. Westermann, eds., Proceedings of the 6th Workshop on Automated Semantic Analysis of
Information in Legal Text co-located with the 19th International Conference on Artificial
Intelligence and Law (ICAIL 2023), Braga, Portugal, 23rd September, 2023 , vol. 3441 of
CEUR Workshop Proceedings , pages 1–12. CEUR-WS.org, 2023.
[343] Ling, C., X. Lu, et al. Domain specialization as the key to make large language models
disruptive: A comprehensive survey, 2023.
[344] Linardatos, P., V. Papastefanopoulos, S. Kotsiantis. Explainable AI: A review of machine
learning interpretability methods. Entropy , 23(1):18, 2021.
[345] Zou, A., Z. Kolter, et al. Universal and transferable adversarial attacks on aligned
language models.