# 2309.07864.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/agent/2309.07864.pdf
# File size: 6832174 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
The Rise and Potential of Large Language Model
Based Agents: A Survey
Zhiheng Xi∗†, Wenxiang Chen∗, Xin Guo∗, Wei He∗, Yiwen Ding∗, Boyang Hong∗,
Ming Zhang∗, Junzhe Wang∗, Senjie Jin∗, Enyu Zhou∗,
Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang,
Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin,
Shihan Dou, Rongxiang Weng, Wensen Cheng,
Qi Zhang†, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang and Tao Gui†
Fudan NLP Group
Abstract
For a long time, humanity has pursued artificial intelligence (AI) equivalent to or
surpassing the human level, with AI agents considered a promising vehicle for
this pursuit. AI agents are artificial entities that sense their environment, make
decisions, and take actions. Many efforts have been made to develop intelligent
agents, but they mainly focus on advancement in algorithms or training strategies
to enhance specific capabilities or performance on particular tasks. Actually, what
the community lacks is a general and powerful model to serve as a starting point
for designing AI agents that can adapt to diverse scenarios. Due to the versatile
capabilities they demonstrate, large language models (LLMs) are regarded as
potential sparks for Artificial General Intelligence (AGI), offering hope for building
general AI agents. Many researchers have leveraged LLMs as the foundation to
build AI agents and have achieved significant progress. In this paper, we perform
a comprehensive survey on LLM-based agents. We start by tracing the concept
of agents from its philosophical origins to its development in AI, and explain
why LLMs are suitable foundations for agents. Building upon this, we present
a general framework for LLM-based agents, comprising three main components:
brain, perception, and action, and the framework can be tailored for different
applications. Subsequently, we explore the extensive applications of LLM-based
agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-
agent cooperation. Following this, we delve into agent societies, exploring the
behavior and personality of LLM-based agents, the social phenomena that emerge
from an agent society, and the insights they offer for human society. Finally, we
discuss several key topics and open problems within the field. A repository for the
related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.
†Correspondence to: zhxi22@m.fudan.edu.cn, {qz, tgui}@fudan.edu.cn
∗Equal Contribution.arXiv:2309.07864v3  [cs.AI]  19 Sep 2023

--- PAGE 2 ---
Contents
1 Introduction 4
2 Background 6
2.1 Origin of AI Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 Technological Trends in Agent Research . . . . . . . . . . . . . . . . . . . . . . . 7
2.3 Why is LLM suitable as the primary component of an Agent’s brain? . . . . . . . . 9
3 The Birth of An Agent: Construction of LLM-based Agents 10
3.1 Brain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3.1.1 Natural Language Interaction . . . . . . . . . . . . . . . . . . . . . . . . 12
3.1.2 Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.1.3 Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.1.4 Reasoning and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
3.1.5 Transferability and Generalization . . . . . . . . . . . . . . . . . . . . . . 16
3.2 Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.2.1 Textual Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.2.2 Visual Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
3.2.3 Auditory Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
3.2.4 Other Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.3 Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
3.3.1 Textual Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.3.2 Tool Using . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
3.3.3 Embodied Action . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4 Agents in Practice: Harnessing AI for Good 24
4.1 General Ability of Single Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.1.1 Task-oriented Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.1.2 Innovation-oriented Deployment . . . . . . . . . . . . . . . . . . . . . . . 27
4.1.3 Lifecycle-oriented Deployment . . . . . . . . . . . . . . . . . . . . . . . 27
4.2 Coordinating Potential of Multiple Agents . . . . . . . . . . . . . . . . . . . . . . 28
4.2.1 Cooperative Interaction for Complementarity . . . . . . . . . . . . . . . . 28
4.2.2 Adversarial Interaction for Advancement . . . . . . . . . . . . . . . . . . 30
4.3 Interactive Engagement between Human and Agent . . . . . . . . . . . . . . . . . 30
4.3.1 Instructor-Executor Paradigm . . . . . . . . . . . . . . . . . . . . . . . . 31
4.3.2 Equal Partnership Paradigm . . . . . . . . . . . . . . . . . . . . . . . . . 32
5 Agent Society: From Individuality to Sociality 33
5.1 Behavior and Personality of LLM-based Agents . . . . . . . . . . . . . . . . . . . 34
5.1.1 Social Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
2

--- PAGE 3 ---
5.1.2 Personality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
5.2 Environment for Agent Society . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
5.2.1 Text-based Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
5.2.2 Virtual Sandbox Environment . . . . . . . . . . . . . . . . . . . . . . . . 37
5.2.3 Physical Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
5.3 Society Simulation with LLM-based Agents . . . . . . . . . . . . . . . . . . . . . 38
5.3.1 Key Properties and Mechanism of Agent Society . . . . . . . . . . . . . . 38
5.3.2 Insights from Agent Society . . . . . . . . . . . . . . . . . . . . . . . . . 39
5.3.3 Ethical and Social Risks in Agent Society . . . . . . . . . . . . . . . . . . 40
6 Discussion 41
6.1 Mutual Benefits between LLM Research and Agent Research . . . . . . . . . . . . 41
6.2 Evaluation for LLM-based Agents . . . . . . . . . . . . . . . . . . . . . . . . . . 42
6.3 Security, Trustworthiness and Other Potential Risks of LLM-based Agents . . . . . 44
6.3.1 Adversarial Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
6.3.2 Trustworthiness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
6.3.3 Other Potential Risks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
6.4 Scaling Up the Number of Agents . . . . . . . . . . . . . . . . . . . . . . . . . . 45
6.5 Open Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
7 Conclusion 48
3

--- PAGE 4 ---
1 Introduction
“If they find a parrot who could answer to everything, I would claim it to be an
intelligent being without hesitation. ”
—Denis Diderot, 1875
Artificial Intelligence (AI) is a field dedicated to designing and developing systems that can replicate
human-like intelligence and abilities [ 1]. As early as the 18th century, philosopher Denis Diderot
introduced the idea that if a parrot could respond to every question, it could be considered intelligent
[2]. While Diderot was referring to living beings, like the parrot, his notion highlights the profound
concept that a highly intelligent organism could resemble human intelligence. In the 1950s, Alan
Turing expanded this notion to artificial entities and proposed the renowned Turing Test [ 3]. This
test is a cornerstone in AI and aims to explore whether machines can display intelligent behavior
comparable to humans. These AI entities are often termed “agents”, forming the essential building
blocks of AI systems. Typically in AI, an agent refers to an artificial entity capable of perceiving its
surroundings using sensors, making decisions, and then taking actions in response using actuators
[1; 4]. The concept of agents originated in Philosophy, with roots tracing back to thinkers like Aristotle
and Hume [ 5]. It describes entities possessing desires, beliefs, intentions, and the ability to take
actions [ 5]. This idea transitioned into computer science, intending to enable computers to understand
users’ interests and autonomously perform actions on their behalf [ 6;7;8]. As AI advanced, the term
“agent” found its place in AI research to depict entities showcasing intelligent behavior and possessing
qualities like autonomy, reactivity, pro-activeness, and social ability [ 4;9]. Since then, the exploration
and technical advancement of agents have become focal points within the AI community [ 1;10]. AI
agents are now acknowledged as a pivotal stride towards achieving Artificial General Intelligence
(AGI)1, as they encompass the potential for a wide range of intelligent activities [4; 11; 12]. From the mid-20th century, significant strides were made in developing smart AI agents as research
delved deep into their design and advancement [ 13;14;15;16;17;18]. However, these efforts have
predominantly focused on enhancing specific capabilities, such as symbolic reasoning, or mastering
particular tasks like Go or Chess [ 19;20;21]. Achieving a broad adaptability across varied scenarios
remained elusive. Moreover, previous studies have placed more emphasis on the design of algorithms
and training strategies, overlooking the development of the model’s inherent general abilities like
knowledge memorization, long-term planning, effective generalization, and efficient interaction
[22;23]. Actually, enhancing the inherent capabilities of the model is the pivotal factor for advancing
the agent further, and the domain is in need of a powerful foundational model endowed with a variety
of key attributes mentioned above to serve as a starting point for agent systems. The development of large language models (LLMs) has brought a glimmer of hope for the further
development of agents [ 24;25;26], and significant progress has been made by the community
[22;27;28;29]. According to the notion of World Scope (WS) [ 30] which encompasses five
levels that depict the research progress from NLP to general AI (i.e., Corpus, Internet, Perception,
Embodiment, and Social), the pure LLMs are built on the second level with internet-scale textual
inputs and outputs. Despite this, LLMs have demonstrated powerful capabilities in knowledge
acquisition, instruction comprehension, generalization, planning, and reasoning, while displaying
effective natural language interactions with humans. These advantages have earned LLMs the
designation of sparks for AGI [ 31], making them highly desirable for building intelligent agents to
foster a world where humans and agents coexist harmoniously [ 22]. Starting from this, if we elevate
LLMs to the status of agents and equip them with an expanded perception space and action space,
they have the potential to reach the third and fourth levels of WS. Furthermore, these LLMs-based
agents can tackle more complex tasks through cooperation or competition, and emergent social
phenomena can be observed when placing them together, potentially achieving the fifth WS level. As
shown in Figure 1, we envision a harmonious society composed of AI agents where human can also
participate. In this paper, we present a comprehensive and systematic survey focusing on LLM-based agents,
attempting to investigate the existing studies and prospective avenues in this burgeoning field. To this
end, we begin by delving into crucial background information (§ 2). In particular, we commence by
tracing the origin of AI agents from philosophy to the AI domain, along with a brief overview of the
1Also known as Strong AI.
4

--- PAGE 5 ---
Let me experience thefestival inthis world...User
Multi-AgentOrdering dishes and cooking Taskplanning and solving
Band performingDiscussing decoration
Kitchen
ConcertCooperationOutdoorsActingwithtools
An Envisioned Agent SocietyFigure 1: Scenario of an envisioned society composed of AI agents, in which humans can also
participate. The above image depicts some specific scenes within society. In the kitchen, one agent
orders dishes, while another agent is responsible for planning and solving the cooking task. At the
concert, three agents are collaborating to perform in a band. Outdoors, two agents are discussing
lantern-making, planning the required materials, and finances by selecting and using tools. Users can
participate in any of these stages of this social activity.
debate surrounding the existence of artificial agents (§ 2.1). Next, we take the lens of technological
trends to provide a concise historical review of the development of AI agents (§ 2.2). Finally, we
delve into an in-depth introduction of the essential characteristics of agents and elucidate why large
language models are well-suited to serve as the main component of brains or controllers for AI agents
(§ 2.3). Inspired by the definition of the agent, we present a general conceptual framework for the LLM-
based agents with three key parts: brain, perception, and action (§ 3), and the framework can be
tailored to suit different applications. We first introduce the brain, which is primarily composed of
a large language model (§ 3.1). Similar to humans, the brain is the core of an AI agent because it
not only stores crucial memories, information, and knowledge but also undertakes essential tasks
of information processing, decision-making, reasoning, and planning. It is the key determinant of
whether the agent can exhibit intelligent behaviors. Next, we introduce the perception module (§
3.2). For an agent, this module serves a role similar to that of sensory organs for humans. Its primary
function is to expand the agent’s perceptual space from text-only to a multimodal space that includes
diverse sensory modalities like text, sound, visuals, touch, smell, and more. This expansion enables
the agent to better perceive information from the external environment. Finally, we present the action
module for expanding the action space of an agent (§ 3.3). Specifically, we expect the agent to be
able to possess textual output, take embodied actions, and use tools so that it can better respond to
environmental changes and provide feedback, and even alter and shape the environment. After that, we provide a detailed and thorough introduction to the practical applications of LLM-
based agents and elucidate the foundational design pursuit—“Harnessing AI for good” (§ 4). To start,
we delve into the current applications of a single agent and discuss their performance in text-based
tasks and simulated exploration environments, with a highlight on their capabilities in handling
specific tasks, driving innovation, and exhibiting human-like survival skills and adaptability (§ 4.1). Following that, we take a retrospective look at the development history of multi-agents. We introduce
the interactions between agents in LLM-based multi-agent system applications, where they engage in
5

--- PAGE 6 ---
collaboration, negotiation, or competition. Regardless of the mode of interaction, agents collectively
strive toward a shared objective (§ 4.2). Lastly, considering the potential limitations of LLM-based
agents in aspects such as privacy security, ethical constraints, and data deficiencies, we discuss
the human-agent collaboration. We summarize the paradigms of collaboration between agents and
humans: the instructor-executor paradigm and the equal partnership paradigm, along with specific
applications in practice (§ 4.3). Building upon the exploration of practical applications of LLM-based agents, we now shift our
focus to the concept of the “ Agent Society ”, examining the intricate interactions between agents and
their surrounding environments (§ 5). This section begins with an investigation into whether these
agents exhibit human-like behavior and possess corresponding personality (§5.1). Furthermore, we
introduce the social environments within which the agents operate, including text-based environment,
virtual sandbox, and the physical world (§5.2). Unlike the previous section (§ 3.2), here we will
focus on diverse types of the environment rather than how the agents perceive it. Having established
the foundation of agents and their environments, we proceed to unveil the simulated societies that
they form (§5.3). We will discuss the construction of a simulated society, and go on to examine the
social phenomena that emerge from it. Specifically, we will emphasize the lessons and potential risks
inherent in simulated societies. Finally, we discuss a range of key topics (§ 6) and open problems within the field of LLM-based
agents: (1) the mutual benefits and inspirations of the LLM research and the agent research, where
we demonstrate that the development of LLM-based agents has provided many opportunities for
both agent and LLM communities (§ 6.1); (2) existing evaluation efforts and some prospects for
LLM-based agents from four dimensions, including utility, sociability, values and the ability to
continually evolve (§ 6.2); (3) potential risks of LLM-based agents, where we discuss adversarial
robustness and trustworthiness of LLM-based agents. We also include the discussion of some other
risks like misuse, unemployment and the threat to the well-being of the human race (§ 6.3); (4)
scaling up the number of agents, where we discuss the potential advantages and challenges of scaling
up agent counts, along with the approaches of pre-determined and dynamic scaling (§ 6.4); (5) several
open problems, such as the debate over whether LLM-based agents represent a potential path to AGI,
challenges from virtual simulated environment to physical environment, collective Intelligence in AI
agents, and Agent as a Service (§ 6.5). After all, we hope this paper could provide inspiration to the
researchers and practitioners from relevant fields.
2 Background
In this section, we provide crucial background information to lay the groundwork for the subsequent
content (§ 2.1). We first discuss the origin of AI agents, from philosophy to the realm of AI, coupled
with a discussion of the discourse regarding the existence of artificial agents (§ 2.2). Subsequently,
we summarize the development of AI agents through the lens of technological trends. Finally, we
introduce the key characteristics of agents and demonstrate why LLMs are suitable to serve as the
main part of the brains of AI agents (§ 2.3).
2.1 Origin of AI Agent
“Agent” is a concept with a long history that has been explored and interpreted in many fields. Here,
we first explore its origins in philosophy, discuss whether artificial products can possess agency in a
philosophical sense, and examine how related concepts have been introduced into the field of AI. Agent in philosophy. The core idea of an agent has a historical background in philosophical
discussions, with its roots traceable to influential thinkers such as Aristotle and Hume, among others
[5]. In a general sense, an “agent” is an entity with the capacity to act, and the term “agency” denotes
the exercise or manifestation of this capacity [ 5]. While in a narrow sense, “agency” is usually used to
refer to the performance of intentional actions; and correspondingly, the term “agent” denotes entities
that possess desires, beliefs, intentions, and the ability to act [ 32;33;34;35]. Note that agents can
encompass not only individual human beings but also other entities in both the physical and virtual
world. Importantly, the concept of an agent involves individual autonomy, granting them the ability
to exercise volition, make choices, and take actions, rather than passively reacting to external stimuli.
6

--- PAGE 7 ---
From the perspective of philosophy, is artificial entities capable of agency. In a general sense,
if we define agents as entities with the capacity to act, AI systems do exhibit a form of agency [ 5]. However, the term agent is more usually used to refer to entities or subjects that possess consciousness,
intentionality, and the ability to act [ 32;33;34]. Within this framework, it’s not immediately clear
whether artificial systems can possess agency, as it remains uncertain whether they possess internal
states that form the basis for attributing desires, beliefs, and intentions. Some people argue that
attributing psychological states like intention to artificial agents is a form of anthropomorphism and
lacks scientific rigor [ 5;36]. As Barandiaran et al. [ 36] stated, “Being specific about the requirements
for agency has told us a lot about how much is still needed for the development of artificial forms of
agency.” In contrast, there are also researchers who believe that, in certain circumstances, employing
the intentional stance (that is, interpreting agent behavior in terms of intentions) can provide a better
description, explanation and abstraction of the actions of artificial agents, much like it is done for
humans [11; 37; 38]. With the advancement of language models, the potential emergence of artificial intentional agents
appears more promising [ 24;25;39;40;41]. In a rigorous sense, language models merely function
as conditional probability models, using input to predict the next token [ 42]. Different from this,
humans incorporate social and perceptual context, and speak according to their mental states [ 43;
44]. Consequently, some researchers argue that the current paradigm of language modeling is not
compatible with the intentional actions of an agent [ 30;45]. However, there are also researchers
who propose that language models can, in a narrow sense, serve as models of agents [ 46;47]. They
argue that during the process of context-based next-word prediction, current language models can
sometimes infer approximate, partial representations of the beliefs, desires, and intentions held by
the agent who generated the context. With these representations, the language models can then
generate utterances like humans. To support their viewpoint, they conduct experiments to provide
some empirical evidence [46; 48; 49]. Introduction of agents into AI. It might come as a surprise that researchers within the mainstream
AI community devoted relatively minimal attention to concepts related to agents until the mid to late
1980s. Nevertheless, there has been a significant surge of interest in this topic within the realms of
computer science and artificial intelligence communities since then [ 50;51;52;53]. As Wooldridge
et al. [ 4] stated, we can define AI by saying that it is a subfield of computer science that aims to
design and build computer-based agents that exhibit aspects of intelligent behavior. So we can treat
“agent” as a central concept in AI. When the concept of agent is introduced into the field of AI, its
meaning undergoes some changes. In the realm of Philosophy, an agent can be a human, an animal,
or even a concept or entity with autonomy [ 5]. However, in the field of artificial intelligence, an
agent is a computational entity [ 4;7]. Due to the seemingly metaphysical nature of concepts like
consciousness and desires for computational entities [ 11], and given that we can only observe the
behavior of the machine, many AI researchers, including Alan Turing, suggest temporarily setting
aside the question of whether an agent is “actually” thinking or literally possesses a “mind” [ 3]. Instead, researchers employ other attributes to help describe an agent, such as properties of autonomy,
reactivity, pro-activeness and social ability [ 4;9]. There are also researchers who held that intelligence
is “in the eye of the beholder”; it is not an innate, isolated property [ 15;16;54;55]. In essence, an
AI agent is not equivalent to a philosophical agent; rather, it is a concretization of the philosophical
concept of an agent in the context of AI. In this paper, we treat AI agents as artificial entities that are
capable of perceiving their surroundings using sensors, making decisions, and then taking actions in
response using actuators [1; 4].
2.2 Technological Trends in Agent Research
The evolution of AI agents has undergone several stages, and here we take the lens of technological
trends to review its development briefly. Symbolic Agents. In the early stages of artificial intelligence research, the predominant approach
utilized was symbolic AI, characterized by its reliance on symbolic logic [ 56;57]. This approach
employed logical rules and symbolic representations to encapsulate knowledge and facilitate reasoning
processes. Early AI agents were built based on this approach [ 58], and they primarily focused on two
problems: the transduction problem and the representation/reasoning problem [ 59]. These agents
are aimed to emulate human thinking patterns. They possess explicit and interpretable reasoning
7

--- PAGE 8 ---
frameworks, and due to their symbolic nature, they exhibit a high degree of expressive capability
[13;14;60]. A classic example of this approach is knowledge-based expert systems. However,
symbolic agents faced limitations in handling uncertainty and large-scale real-world problems
[19;20]. Additionally, due to the intricacies of symbolic reasoning algorithms, it was challenging to
find an efficient algorithm capable of producing meaningful results within a finite timeframe [ 20;61]. Reactive agents. Different from symbolic agents, reactive agents do not use complex symbolic
reasoning. Instead, they primarily focus on the interaction between the agent and its environment,
emphasizing quick and real-time responses [ 15;16;20;62;63]. These agents are mainly based on
a sense-act loop, efficiently perceiving and reacting to the environment. The design of such agents
prioritizes direct input-output mappings rather than intricate reasoning and symbolic operations
[52]. However, Reactive agents also have limitations. They typically require fewer computational
resources, enabling quicker responses, but they might lack complex higher-level decision-making
and planning capabilities. Reinforcement learning-based agents. With the improvement of computational capabilities and
data availability, along with a growing interest in simulating interactions between intelligent agents
and their environments, researchers have begun to utilize reinforcement learning methods to train
agents for tackling more challenging and complex tasks [ 17;18;64;65]. The primary concern in this
field is how to enable agents to learn through interactions with their environments, enabling them to
achieve maximum cumulative rewards in specific tasks [ 21]. Initially, reinforcement learning (RL)
agents were primarily based on fundamental techniques such as policy search and value function
optimization, exemplified by Q-learning [ 66] and SARSA [ 67]. With the rise of deep learning, the
integration of deep neural networks and reinforcement learning, known as Deep Reinforcement
Learning (DRL), has emerged [ 68;69]. This allows agents to learn intricate policies from high-
dimensional inputs, leading to numerous significant accomplishments like AlphaGo [ 70] and DQN
[71]. The advantage of this approach lies in its capacity to enable agents to autonomously learn in
unknown environments, without explicit human intervention. This allows for its wide application
in an array of domains, from gaming to robot control and beyond. Nonetheless, reinforcement
learning faces challenges including long training times, low sample efficiency, and stability concerns,
particularly when applied in complex real-world environments [21]. Agents with transfer learning and meta learning. Traditionally, training a reinforcement learning
agent requires huge sample sizes and long training time, and lacks generalization capability [ 72;
73;74;75;76]. Consequently, researchers have introduced transfer learning to expedite an agent’s
learning on new tasks [ 77;78;79]. Transfer learning reduces the burden of training on new tasks
and facilitates the sharing and migration of knowledge across different tasks, thereby enhancing
learning efficiency, performance, and generalization capabilities. Furthermore, meta-learning has also
been introduced to AI agents [ 80;81;82;83;84]. Meta-learning focuses on learning how to learn,
enabling an agent to swiftly infer optimal policies for new tasks from a small number of samples
[85]. Such an agent, when confronted with a new task, can rapidly adjust its learning approach by
leveraging acquired general knowledge and policies, consequently reducing the reliance on a large
volume of samples. However, when there exist significant disparities between source and target tasks,
the effectiveness of transfer learning might fall short of expectations and there may exist negative
transfer [ 86;87]. Additionally, the substantial amount of pre-training and large sample sizes required
by meta learning make it hard to establish a universal learning policy [81; 88]. Large language model-based agents. As large language models have demonstrated impressive
emergent capabilities and have gained immense popularity [ 24;25;26;41], researchers have started to
leverage these models to construct AI agents [ 22;27;28;89]. Specifically, they employ LLMs as the
primary component of brain or controller of these agents and expand their perceptual and action space
through strategies such as multimodal perception and tool utilization [ 90;91;92;93;94]. These LLM-
based agents can exhibit reasoning and planning abilities comparable to symbolic agents through
techniques like Chain-of-Thought (CoT) and problem decomposition [ 95;96;97;98;99;100;101]. They can also acquire interactive capabilities with the environment, akin to reactive agents, by
learning from feedback and performing new actions [ 102;103;104]. Similarly, large language
models undergo pre-training on large-scale corpora and demonstrate the capacity for few-shot and
zero-shot generalization, allowing for seamless transfer between tasks without the need to update
parameters [ 41;105;106;107]. LLM-based agents have been applied to various real-world scenarios,
8

--- PAGE 9 ---
such as software development [ 108;109] and scientific research [ 110]. Due to their natural language
comprehension and generation capabilities, they can interact with each other seamlessly, giving rise
to collaboration and competition among multiple agents [ 108;109;111;112]. Furthermore, research
suggests that allowing multiple agents to coexist can lead to the emergence of social phenomena [ 22].
2.3 Why is LLM suitable as the primary component of an Agent’s brain. As mentioned before, researchers have introduced several properties to help describe and define
agents in the field of AI. Here, we will delve into some key properties, elucidate their relevance to
LLMs, and thereby expound on why LLMs are highly suited to serve as the main part of brains of AI
agents. Autonomy means that an agent operates without direct intervention from humans
or others and possesses a degree of control over its actions and internal states [ 4;113]. This
implies that an agent should not only possess the capability to follow explicit human instructions for
task completion but also exhibit the capacity to initiate and execute actions independently. LLMs
can demonstrate a form of autonomy through their ability to generate human-like text, engage
in conversations, and perform various tasks without detailed step-by-step instructions [ 114;115]. Moreover, they can dynamically adjust their outputs based on environmental input, reflecting a
degree of adaptive autonomy [ 23;27;104]. Furthermore, they can showcase autonomy through
exhibiting creativity like coming up with novel ideas, stories, or solutions that haven’t been explicitly
programmed into them [ 116;117]. This implies a certain level of self-directed exploration and
decision-making. Applications like Auto-GPT [114] exemplify the significant potential of LLMs in
constructing autonomous agents. Simply by providing them with a task and a set of available tools,
they can autonomously formulate plans and execute them to achieve the ultimate goal. Reactivity. Reactivity in an agent refers to its ability to respond rapidly to immediate changes and
stimuli in its environment [ 9]. This implies that the agent can perceive alterations in its surroundings
and promptly take appropriate actions. Traditionally, the perceptual space of language models
has been confined to textual inputs, while the action space has been limited to textual outputs. However, researchers have demonstrated the potential to expand the perceptual space of LLMs using
multimodal fusion techniques, enabling them to rapidly process visual and auditory information from
the environment [ 25;118;119]. Similarly, it’s also feasible to expand the action space of LLMs
through embodiment techniques [ 120;121] and tool usage [ 92;94]. These advancements enable
LLMs to effectively interact with the real-world physical environment and carry out tasks within it. One major challenge is that LLM-based agents, when performing non-textual actions, require an
intermediate step of generating thoughts or formulating tool usage in textual form before eventually
translating them into concrete actions. This intermediary process consumes time and reduces the
response speed. However, this aligns closely with human behavioral patterns, where the principle of
“think before you act” is observed [122; 123]. Pro-activeness. Pro-activeness denotes that agents don’t merely react to their environments; they
possess the capacity to display goal-oriented actions by proactively taking the initiative [ 9]. This
property emphasizes that agents can reason, make plans, and take proactive measures in their actions
to achieve specific goals or adapt to environmental changes. Although intuitively the paradigm
of next token prediction in LLMs may not possess intention or desire, research has shown that
they can implicitly generate representations of these states and guide the model’s inference process
[46;48;49]. LLMs have demonstrated a strong capacity for generalized reasoning and planning. By
prompting large language models with instructions like “let’s think step by step”, we can elicit their
reasoning abilities, such as logical and mathematical reasoning [ 95;96;97]. Similarly, large language
models have shown the emergent ability of planning in forms of goal reformulation [ 99;124], task
decomposition [98; 125], and adjusting plans in response to environmental changes [100; 126]. Social ability. Social ability refers to an agent’s capacity to interact with other agents, including
humans, through some kind of agent-communication language [ 8]. Large language models exhibit
strong natural language interaction abilities like understanding and generation [ 23;127;128]. Com-
pared to structured languages or other communication protocals, such capability enables them to
interact with other models or humans in an interpretable manner. This forms the cornerstone of
social ability for LLM-based agents [ 22;108]. Many researchers have demonstrated that LLM-based
9

--- PAGE 10 ---
agents can enhance task performance through social behaviors such as collaboration and competition
[108;111;129;130]. By inputting specific prompts, LLMs can also play different roles, thereby
simulating the social division of labor in the real world [ 109]. Furthermore, when we place multiple
agents with distinct identities into a society, emergent social phenomena can be observed [22].
3 The Birth of An Agent: Construction of LLM-based Agents
Look at the sky, do you think it will rain tomorrow. Ifso, give the umbrella to me.Environment
Perception
Tools
CallingAPI …
Embodiment
TextReasoningfromthe current weather conditionsand the weather reports on the internet, it is likely to rain tomorrow.Here is your umbrella. Brain
Knowledge
MemoryStorageDecisionMaking
Planning/ ReasoningRecallSummaryRetrieveLearnGeneralize/Transfer
Inputs
AgentAction
Figure 2: Conceptual framework of LLM-based agent with three components: brain, perception, and
action. Serving as the controller, the brain module undertakes basic tasks like memorizing, thinking,
and decision-making. The perception module perceives and processes multimodal information
from the external environment, and the action module carries out the execution using tools and
influences the surroundings. Here we give an example to illustrate the workflow: When a human
asks whether it will rain, the perception module converts the instruction into an understandable
representation for LLMs. Then the brain module begins to reason according to the current weather
and the weather reports on the internet. Finally, the action module responds and hands the umbrella
to the human. By repeating the above process, an agent can continuously get feedback and interact
with the environment.
“Survival of the Fittest” [ 131] shows that if an individual wants to survive in the external environment,
he must adapt to the surroundings efficiently. This requires him to be cognitive, able to perceive
and respond to changes in the outside world, which is consistent with the definition of “agent”
mentioned in §2.1. Inspired by this, we present a general conceptual framework of an LLM-based
agent composed of three key parts: brain, perception, and action (see Figure 2). We first describe
the structure and working mechanism of the brain, which is primarily composed of a large language
model (§ 3.1). The brain is the core of an AI agent because it not only stores knowledge and memories
but also undertakes indispensable functions like information processing and decision-making. It
can present the process of reasoning and planning, and cope well with unseen tasks, exhibiting
the intelligence of an agent. Next, we introduce the perception module (§ 3.2). Its core purpose
is to broaden the agent’s perception space from a text-only domain to a multimodal sphere that
includes textual, auditory, and visual modalities. This extension equips the agent to grasp and utilize
information from its surroundings more effectively. Finally, we present the action module designed
to expand the action space of an agent (§ 3.3). Specifically, we empower the agent with embodied
action ability and tool-handling skills, enabling it to adeptly adapt to environmental changes, provide
feedback, and even influence and mold the environment. The framework can be tailored for different application scenarios, i.e. not every specific component
will be used in all studies. In general, agents operate in the following workflow: First, the perception
10

--- PAGE 11 ---
module, corresponding to human sensory systems such as the eyes and ears, perceives changes in the
external environment and then converts multimodal information into an understandable representation
for the agent. Subsequently, the brain module, serving as the control center, engages in information
processing activities such as thinking, decision-making, and operations with storage including
memory and knowledge. Finally, the action module, corresponding to human limbs, carries out the
execution with the assistance of tools and leaves an impact on the surroundings.