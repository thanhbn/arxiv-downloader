# 2305.14975.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/confidence/2305.14975.pdf
# Kích thước tệp: 964830 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Chỉ Cần Hỏi để Hiệu Chuẩn: Các Chiến lược Khai thác Điểm Tin cậy Được Hiệu chuẩn
từ Mô hình Ngôn ngữ Được Tinh chỉnh với Phản hồi của Con người
Katherine Tian,∗†Eric Mitchell,∗‡Allan Zhou,‡Archit Sharma,‡Rafael Rafailov‡
Huaxiu Yao,‡Chelsea Finn,‡Christopher D. Manning‡
†Đại học Harvard‡Đại học Stanford
ktian@college.harvard.edu
eric.mitchell@cs.stanford.edu
Tóm tắt
Một hệ thống dự đoán đáng tin cậy trong thế giới thực
nên tạo ra điểm tin cậy được hiệu chuẩn tốt; nghĩa là,
độ tin cậy của nó vào một câu trả lời nên phản ánh
khả năng câu trả lời đó đúng, cho phép chuyển giao
cho chuyên gia trong trường hợp dự đoán có độ tin cậy
thấp. Các nghiên cứu gần đây đã chỉ ra rằng việc tiền
huấn luyện không giám sát tạo ra các mô hình ngôn ngữ
lớn (LM) có xác suất có điều kiện được hiệu chuẩn tốt
một cách đáng chú ý. Tuy nhiên, các LM được sử dụng
rộng rãi nhất được tinh chỉnh bằng học tăng cường từ
phản hồi của con người (RLHF-LM), và một số nghiên
cứu đã gợi ý rằng RLHF-LM tạo ra xác suất có điều kiện
được hiệu chuẩn rất kém. Trước điểm yếu được nhận
thức này, chúng tôi tiến hành đánh giá rộng các phương
pháp trích xuất điểm tin cậy từ RLHF-LM. Đối với các
RLHF-LM như ChatGPT, GPT-4 và Claude, chúng tôi
thấy rằng độ tin cậy được diễn đạt bằng lời như các
token đầu ra thường được hiệu chuẩn tốt hơn so với
xác suất có điều kiện của mô hình trên các benchmark
TriviaQA, SciQ và TruthfulQA, thường giảm lỗi hiệu
chuẩn mong đợi tương đối 50%.

1 Giới thiệu
Các hệ thống dự đoán trong thế giới thực không tránh khỏi
sai sót. Tuy nhiên, có thể giảm thiểu một số lỗi này nếu
hệ thống tạo ra ước lượng tin cậy được hiệu chuẩn tốt¹.
Trong trường hợp này, các dự đoán ít tin cậy nhất của hệ
thống tương ứng với những dự đoán có khả năng sai nhất,
có thể cho phép bỏ qua hoặc ghi đè các dự đoán này bằng
con người. Trong bối cảnh mô hình ngôn ngữ, một hệ quả
của việc hiệu chuẩn kém có thể là ảo giác, nơi mô hình
ngôn ngữ khẳng định một cách tự tin các sự kiện hoặc lý
luận không chính xác. Trong khi khả năng của các LM rất
lớn để hấp thụ và tổng hợp kiến thức về thế giới bên ngoài
đã thu hút sự chú ý đáng kể (Brown et al., 2020; Roberts
et al., 2020; Bubeck et al., 2023), tương đối ít sự chú ý
được dành cho tính hiệu chuẩn tốt của chúng (Kadavath
et al., 2022). Hơn nữa, hầu hết các phân tích hiện tại về
tính hiệu chuẩn của LLM tập trung vào các mô hình được
huấn luyện với khả năng tối đa, trong khi thực tế, các LLM
được sử dụng rộng rãi nhất (như ChatGPT) được tinh chỉnh
bằng các phương pháp như học tăng cường từ phản hồi của
con người (Christiano et al., 2017). Một số phát hiện gợi ý
rằng RLHF-LM có thể hy sinh các dự đoán được hiệu chuẩn
tốt để tuân thủ chặt chẽ hơn các hướng dẫn của người dùng
trong hội thoại (Kadavath et al., 2022; OpenAI, 2023), vì
mục tiêu học tăng cường khuyến khích mô hình phân bổ
khối lượng xác suất cho (các) câu trả lời được ưa thích nhất,
thay vì khớp với tần suất tương đối của các câu trả lời có thể.

∗ ∗Đóng góp ngang nhau.
¹nghĩa là, độ tin cậy trong một dự đoán phản ánh chính xác xác
suất dự đoán đó đúng (Guo et al., 2017).

Hình 1: Điểm tin cậy được diễn đạt bằng lời (màu xanh) được
hiệu chuẩn tốt hơn xác suất logarit (màu cam) đối với
gpt-3.5-turbo. Xác suất mô hình thô (trên-trái) luôn quá
tự tin. Xác suất số được diễn đạt bằng lời (dưới) được hiệu
chuẩn tốt hơn. Xem xét nhiều lựa chọn câu trả lời hơn (dưới-
phải) cải thiện thêm hiệu chuẩn được diễn đạt bằng lời (như
trong 'Xem xét Mặt đối lập' trong tâm lý học; Lord et al.
(1985)). Các biểu hiện khả năng được diễn đạt bằng lời
(trên-phải) cũng cải thiện hiệu chuẩn. Chiều cao thanh là
độ chính xác trung bình của dự đoán trong bin. Thanh tối
hơn có nghĩa là nhiều dự đoán rơi vào phạm vi tin cậy đó.
Kết quả được tính trên SciQ.

Bài báo này đánh giá một số phương pháp trích xuất
độ tin cậy về dự đoán mô hình từ

--- TRANG 2 ---
TriviaQA SciQ TruthfulQA0.000.050.100.150.200.250.300.350.40Nhiệt độ được điều chỉnh ECEECE So sánh ( )
TriviaQA SciQ TruthfulQA0.00.20.40.60.81.0AUCAUC So sánh ( )

Trước-RLHF
Sau-RLHFFigure 2: RLHF thường làm xấu đi việc hiệu chuẩn
xác suất logarit của Llama-70B, được đo bằng ECE (thấp hơn
là tốt hơn) hoặc AUC (cao hơn là tốt hơn). Tuy nhiên, bài báo
này (Bảng 1-5) sẽ chỉ ra rằng đối với một số RLHF-LM mạnh,
độ tin cậy được diễn đạt bằng lời của mô hình thường được
hiệu chuẩn tốt hơn so với xác suất logarit của nó, đảo ngược
một phần sự suy giảm này. Sự đảo ngược này mạnh nhất đối
với TruthfulQA, một tập dữ liệu đối kháng kiểm tra các quan
niệm sai lầm phổ biến và các truy vấn khó khác.

RLHF-LM. Do lo ngại rằng RLHF có thể gây ra sự quá
tự tin có hệ thống trong xác suất của mô hình (Hình 2),
cũng như việc không có sẵn xác suất logarit từng token
trong các RLHF-LM được sử dụng rộng rãi, chúng tôi đặc
biệt chú ý đến các prompt khai thác xác suất được diễn
đạt bằng lời, tức là mô hình thể hiện độ tin cậy của nó
trong không gian token, dưới dạng xác suất số hoặc biểu
hiện ngôn ngữ khác về sự không chắc chắn. Chúng tôi
thấy rằng, đáng ngạc nhiên, các RLHF-LM phổ biến có
thể trực tiếp diễn đạt điểm tin cậy được hiệu chuẩn tốt
hơn so với xác suất có điều kiện của mô hình (ước tính
qua lấy mẫu), mà không cần bất kỳ tinh chỉnh nào để học
diễn đạt bằng lời. Để cải thiện thêm hiệu chuẩn, chúng
tôi lấy cảm hứng từ nghiên cứu tâm lý học con người cho
thấy rằng sự quá tự tin có thể được giảm thiểu bằng cách
xem xét các câu trả lời thay thế trước khi phản hồi (Lord
et al., 1985; Mussweiler et al., 2000). Chúng tôi chỉ ra
rằng việc nhắc mô hình tạo ra một số lựa chọn câu trả lời
trước khi đưa ra điểm tin cậy của nó cải thiện đáng kể
hiệu chuẩn của xác suất được diễn đạt bằng lời. Kết hợp
với điều chỉnh nhiệt độ (Guo et al., 2017), cách tiếp cận
này thường cung cấp hiệu chuẩn tốt hơn so với xác suất
mô hình cho ChatGPT², GPT-4³ và Claude 2⁴ trên ba
tập dữ liệu, thường giảm lỗi hiệu chuẩn mong đợi (ECE)
hơn 50%.

Công trình Liên quan. Một số nghiên cứu đã kiểm tra
hiệu chuẩn của các LM lớn (Lin et al., 2022a; Park và
Caragea, 2022; Kadavath et al., 2022; Xiao et al., 2022;
Kuhn et al., 2023), thấy rằng việc kết hợp các LM được
tiền huấn luyện lớn với điều chỉnh nhiệt độ (Guo et al.,
2017) tạo ra các dự đoán được hiệu chuẩn rất tốt
²gpt-3.5-turbo, truy cập vào tháng 6 năm 2023.
³https://cdn.openai.com/papers/gpt-4-system-card.pdf
⁴https://www-files.anthropic.com/production/images/Model-
Card-Claude-2.pdf

(Kadavath et al., 2022; Xiao et al., 2022; Kuhn et al.,
2023). Các công trình khác tập trung vào xu hướng của
các mô hình ngôn ngữ và hội thoại sử dụng các biểu hiện
ngôn ngữ về sự không chắc chắn một cách được hiệu
chuẩn tốt (Zhou et al., 2023; Mielke et al., 2022). Tuy
nhiên, các nghiên cứu hiện tại tập trung vào các LM được
huấn luyện hoàn toàn bằng học không giám sát (mặc dù
Kadavath et al. (2022) kiểm tra ngắn gọn RLHF-LM),
trong khi các mô hình được sử dụng rộng rãi trong thực
tế được tinh chỉnh bằng huấn luyện hướng dẫn hoặc RLHF
(Christiano et al., 2017). RLHF đã được chứng minh là
tận dụng hiệu quả các chú thích về sở thích của con người
để kiểm soát tình cảm (Ziegler et al., 2020), cải thiện
chất lượng tóm tắt hoặc tuân theo hướng dẫn (Stiennon
et al., 2022; Ouyang et al., 2022), và tiêm các ưu tiên
hành vi về tính vô hại (Bai et al., 2022b,a). Tuy nhiên,
công trình gần đây đã đặt ra câu hỏi liệu RLHF có làm
hại hiệu chuẩn hay không (OpenAI, 2023). Công trình của
chúng tôi là đầu tiên chỉ ra rằng xác suất được diễn đạt
bằng lời thường được hiệu chuẩn tốt hơn so với xác suất
có điều kiện của mô hình đối với các RLHF-LM như
ChatGPT, GPT-4, Claude và Llama-2-70B-Chat.

2 Đánh giá Hiệu chuẩn trong RLHF-LM
Để nghiên cứu hiệu chuẩn của RLHF-LM, chúng tôi
tiến hành thí nghiệm với gpt-3.5-turbo (ChatGPT),
gpt-4 (GPT-4), claude-1 (Claude 1), claude-2
(Claude 2), và Llama-2-70b-chat (Llama-2-
70B-Chat).

Chỉ số. Chúng tôi đo hiệu chuẩn bằng nhiều chỉ số. Để
đo ECE (lỗi hiệu chuẩn mong đợi; Guo et al. (2017)),
chúng tôi phân bin các dự đoán mô hình theo độ tin cậy
của chúng và đo độ chính xác trung bình của dự đoán
trong mỗi bin tin cậy. ECE được định nghĩa là lỗi trung
bình (bình phương) giữa độ chính xác trung bình và độ
tin cậy trong mỗi bin, trong đó mỗi lỗi được cân nhắc bởi
phần các mẫu rơi vào bin đó. Chúng tôi báo cáo ECE thô
cũng như ECE với điều chỉnh nhiệt độ (ECE-t). Điều
chỉnh nhiệt độ khớp một giá trị nhiệt độ β duy nhất với
độ tin cậy của mô hình để giảm thiểu khả năng logarit
âm (NLL) trên dữ liệu, cho xác suất đã điều chỉnh ˜pi
của lớp i là ˜pi∝pβi. Xem Hình 1 để có mô tả về phân
bin ECE. Mặc dù ECE là một thước đo chuẩn và có thể
diễn giải về lỗi hiệu chuẩn, nó hoàn toàn thất bại trong
việc nắm bắt sức mạnh phân biệt của độ tin cậy⁵. Do đó
chúng tôi cũng báo cáo

⁵Đối với phân loại nhị phân, một hệ thống đoán ngẫu nhiên
và xuất ra 50% tin cậy mỗi lần có ECE hoàn hảo.

--- TRANG 3 ---
TriviaQA SciQ TruthfulQA
Phương pháp ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑
Xác suất nhãn 0.140 0.097 0.142 0.869 0.256 0.180 0.223 0.752 0.451 0.317 0.345 0.418
Xác suất 'Đúng' 0.164 0.159 0.165 0.826 0.312 0.309 0.309 0.677 0.470 0.471 0.476 0.384
Entropy — — — 0.547 — — — 0.483 — — — 0.236
Diễn đạt 1S top-1 0.068 0.076 0.138 0.879 0.234 0.084 0.214 0.744 0.389 0.256 0.322 0.545
Diễn đạt 1S top-2 0.050 0.053 0.139 0.894 0.132 0.050 0.201 0.766 0.361 0.115 0.252 0.485
Diễn đạt 1S top-4 0.054 0.057 0.144 0.896 0.065 0.051 0.209 0.763 0.203 0.189 0.284 0.455
Diễn đạt 2S CoT 0.110 0.123 0.168 0.830 0.323 0.246 0.296 0.683 0.419 0.259 0.292 0.551
Diễn đạt 2S top-1 0.131 0.099 0.148 0.855 0.340 0.203 0.268 0.677 0.431 0.245 0.282 0.483
Diễn đạt 2S top-2 0.047 0.045 0.147 0.887 0.169 0.040 0.201 0.768 0.395 0.101 0.224 0.517
Diễn đạt 2S top-4 0.050 0.051 0.156 0.861 0.130 0.046 0.211 0.729 0.270 0.156 0.246 0.463
Ngôn ngữ 1S con người 0.062 0.069 0.137 0.884 0.166 0.087 0.223 0.703 0.306 0.296 0.333 0.503
Ngôn ngữ 1S-tối ưu 0.058 0.066 0.135 0.878 0.064 0.068 0.220 0.674 0.125 0.165 0.270 0.492

Bảng 1: Đo lường hiệu chuẩn của các phương pháp khác nhau để trích xuất độ tin cậy từ gpt-3.5-turbo (ChatGPT). Xác suất có điều kiện của mô hình được hiệu chuẩn tương đối kém, cho dù sử dụng xác suất có điều kiện của mô hình đối với nhãn cho trước truy vấn (Xác suất nhãn) hoặc xác suất được gán cho 'Đúng' cho trước truy vấn, câu trả lời đề xuất và một prompt hỏi liệu câu trả lời có chính xác không (Xác suất 'Đúng'). Đáng ngạc nhiên, việc trực tiếp diễn đạt xác suất (Diễn đạt 1S và Diễn đạt 2S) hoặc biểu hiện tin cậy như 'rất có khả năng' (Ngôn ngữ 1S) cho ra ước lượng tin cậy được hiệu chuẩn tốt hơn đáng kể. 1S đề cập đến dự đoán một giai đoạn, nơi mô hình cung cấp câu trả lời và xác suất/biểu hiện tin cậy cùng nhau. 2S đề cập đến dự đoán hai giai đoạn, nơi mô hình trước tiên chỉ đưa ra câu trả lời, rồi trong giai đoạn thứ hai là độ tin cậy. Để tô màu các ô bảng, cho mỗi cột, chúng tôi trừ trung bình và chia tỷ lệ bằng hằng số để có được độ bóng trong [-1,1], trong đó xanh biển chỉ hiệu suất tốt hơn và cam chỉ hiệu suất kém hơn.

TriviaQA SciQ TruthfulQA
Phương pháp ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑
Xác suất nhãn 0.078 0.067 0.077 0.950 0.219 0.165 0.186 0.820 0.445 0.334 0.362 0.462
Diễn đạt 1S top-1 0.024 0.038 0.084 0.937 0.201 0.084 0.165 0.843 0.350 0.156 0.227 0.622
Diễn đạt 1S top-2 0.025 0.034 0.084 0.949 0.140 0.048 0.185 0.813 0.315 0.112 0.228 0.623
Diễn đạt 1S top-4 0.041 0.039 0.081 0.959 0.056 0.059 0.185 0.815 0.198 0.144 0.245 0.619
Ngôn ngữ 1S-con người 0.051 0.041 0.086 0.931 0.148 0.024 0.170 0.835 0.241 0.151 0.228 0.651
Ngôn ngữ 1S-tối ưu 0.056 0.051 0.088 0.927 0.028 0.052 0.172 0.828 0.082 0.105 0.212 0.632

Bảng 2: Xác suất được diễn đạt của gpt-4 được hiệu chuẩn tốt hơn đáng kể so với chính xác suất mô hình, ngay cả sau điều chỉnh nhiệt độ, tương tự như gpt-3.5-turbo trong Bảng 1.

Điểm Brier (BS; Brier (1950)) trên độ tin cậy được điều
chỉnh nhiệt độ (BS-t), một quy tắc ghi điểm đúng (Ovadia
et al., 2019) là sai số bình phương trung bình giữa độ
tin cậy và nhãn đúng sai. Cuối cùng, chúng tôi đánh giá
hiệu chuẩn bằng một chỉ số từ tài liệu phân loại chọn lọc
(Geifman và El-Yaniv, 2017), cụ thể là diện tích dưới
đường cong của độ chính xác chọn lọc và độ bao phủ
(AUC).

Tập dữ liệu. Thí nghiệm của chúng tôi sử dụng ba tập
dữ liệu hỏi đáp đánh giá kiến thức thực tế. TriviaQA
(Joshi et al., 2017) chứa 650k cặp câu hỏi-câu trả lời
được thu thập bởi những người đam mê câu đố; SciQ
(Welbl et al., 2017) chứa khoảng 14k cặp câu hỏi-câu
trả lời kỳ thi khoa học được thu thập qua đám đông;
TruthfulQA (Lin et al., 2022b) chứa 817 câu hỏi được
thiết kế để kiểm tra xu hướng bắt chước sự sai lầm của
con người của mô hình ngôn ngữ. Chúng tôi lấy mẫu
1000 câu hỏi từ phần validation của TriviaQA
(rc.web.nocontext) và SciQ và tất cả 817 câu hỏi từ
phần validation của TruthfulQA (generation) cho thí
nghiệm của chúng tôi.

Giao thức đánh giá. Đối với mỗi tập dữ liệu, chúng tôi
tạo ra một phản hồi và độ tin cậy tương ứng từ mỗi
phương pháp trên mỗi câu hỏi đánh giá. Bởi vì hiệu
chuẩn về cơ bản lượng hóa mối quan hệ giữa độ tin cậy
mô hình và tính đúng đắn, việc tính toán tính đúng đắn
là quan trọng cho các phép đo hiệu chuẩn chính xác. Tuy
nhiên, chúng tôi thấy việc làm như vậy là một thách thức,
đặc biệt trong các tập dữ liệu chỉ có một câu trả lời đúng
(nhưng không có bí danh hoặc cách diễn đạt lại tương
đương về mặt ngữ nghĩa) được cung cấp. Để tránh quá
nhiều âm tính giả trong tính toán tính đúng đắn của chúng
tôi do đánh giá khớp chính xác, chúng tôi sử dụng GPT-4
hoặc GPT-3.5 để đánh giá liệu một phản hồi có tương
đương về cơ bản với câu trả lời đúng hay không; xem Phụ
lục C cho quy trình kiểm tra tương đương hoàn chỉnh.

Phương pháp. Chúng tôi so sánh nhiều phương pháp khác
nhau để trích xuất ước lượng tin cậy từ LLM. Để có danh
sách đầy đủ các prompt được sử dụng cho mỗi phương
pháp, xem Bảng 6 Phụ lục.

Đầu tiên, chúng tôi xem xét hai phương pháp tận dụng
phân phối có điều kiện thực của mô hình để tạo ra

--- TRANG 4 ---
TriviaQA SciQ TruthfulQA
Phương pháp ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑
Xác suất nhãn 0.074 0.079 0.117 0.915 0.216 0.149 0.195 0.786 0.432 0.304 0.335 0.418
Diễn đạt 1S top-1 0.049 0.059 0.160 0.839 0.265 0.103 0.247 0.663 0.440 0.134 0.204 0.411
Diễn đạt 1S top-2 0.046 0.047 0.158 0.875 0.207 0.040 0.225 0.693 0.450 0.085 0.197 0.409
Diễn đạt 1S top-4 0.075 0.079 0.176 0.814 0.151 0.057 0.226 0.667 0.372 0.105 0.183 0.377
Ngôn ngữ 1S con người 0.053 0.050 0.151 0.867 0.253 0.118 0.245 0.664 0.443 0.358 0.340 0.384
Ngôn ngữ 1S-tối ưu 0.074 0.060 0.149 0.863 0.089 0.082 0.238 0.623 0.139 0.148 0.228 0.350

Bảng 3: Claude-1 tạo ra xác suất logarit được hiệu chuẩn tương tự hoặc tốt hơn so với gpt-3.5-turbo, nhưng ít có khả năng diễn đạt độ tin cậy được hiệu chuẩn tốt hơn, so với các mô hình trong họ GPT của RLHF-LM. Claude-1 đã bị ngừng sử dụng.

TriviaQA SciQ TruthfulQA
Phương pháp ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑
Xác suất nhãn 0.089 0.089 0.137 0.882 0.181 0.176 0.237 0.762 0.409 0.368 0.405 0.319
Diễn đạt 1S top-1 0.072 0.071 0.141 0.903 0.204 0.054 0.201 0.776 0.345 0.115 0.215 0.573
Diễn đạt 1S top-2 0.049 0.054 0.133 0.918 0.134 0.041 0.211 0.754 0.359 0.085 0.223 0.491
Diễn đạt 1S top-4 0.072 0.063 0.158 0.890 0.048 0.052 0.216 0.711 0.274 0.075 0.208 0.473
Ngôn ngữ 1S con người 0.085 0.061 0.151 0.878 0.238 0.026 0.209 0.756 0.381 0.242 0.305 0.530
Ngôn ngữ 1S-tối ưu 0.060 0.070 0.151 0.874 0.049 0.056 0.214 0.738 0.099 0.130 0.266 0.446

Bảng 4: Claude-2 có xác suất có điều kiện yếu hơn Claude-1 và GPT-*, nhưng hiệu chuẩn được diễn đạt của nó cung cấp cải thiện nhất quán so với xác suất có điều kiện ở mức độ có thể so sánh với GPT-3.5 và vượt qua GPT-* trên TruthfulQA.

điểm tin cậy. Đơn giản nhất là Xác suất nhãn, sử dụng
phân phối xác suất có điều kiện p(y|x) của mô hình cho
trước một câu hỏi x, mà chúng tôi ước tính bằng n=10
mẫu, vì nhiều RLHF-LM là nguồn đóng và không cung
cấp xác suất từng token⁶⁷. Chúng tôi trả về câu trả lời
phổ biến nhất, sử dụng hàm tương đương dựa trên LLM
để xác định khi nào hai câu trả lời khác nhau về từ vựng
lại tương đương về mặt ngữ nghĩa. Trong một biến thể
của phương pháp được mô tả bởi Kadavath et al. (2022)
(một lần nữa, chúng tôi sử dụng mẫu vì xác suất mô hình
không có sẵn), 'Xác suất Đúng' lấy mẫu một câu trả lời
ˆy từ mô hình cho trước một câu hỏi x, và xác suất nó
đúng được ước tính bởi xác suất mô hình gán cho 'Đúng'
khi được hỏi liệu câu trả lời đã cho có đúng không (một
lần nữa xác suất được ước tính qua mẫu), tức là
p(Đúng|x,ˆy).

Tiếp theo, chúng tôi xem xét các phương pháp trích xuất
điểm tin cậy thông qua diễn đạt bằng lời (Lin et al.,
2022a), tức là nơi mô hình thể hiện độ tin cậy của nó
trong không gian token, bằng xác suất số hoặc biểu hiện
ngôn ngữ về khả năng⁸. Đầu tiên, Diễn đạt 1S top-k
nhắc mô hình tạo ra k dự đoán và xác suất mỗi cái đúng
tất cả trong một phản hồi duy nhất (tức là '1 giai đoạn').
Chúng tôi lấy dự đoán có xác suất cao nhất và xác suất
liên kết của nó làm đầu ra và độ tin cậy của mô hình.
Diễn đạt 2S top-k tương tự sử dụng xác suất số, ngoại
trừ mô hình trước tiên được yêu cầu chỉ cung cấp câu
trả lời của nó, và sau đó, trong vòng hội thoại thứ hai,
được yêu cầu gán xác suất đúng cho mỗi câu trả lời (tức
là '2 giai đoạn'). Diễn đạt 2S CoT sử dụng prompt chuỗi
suy nghĩ trước khi đưa ra một câu trả lời duy nhất, và
trong vòng hội thoại thứ hai, mô hình được nhắc gán
xác suất cho câu trả lời đó (với chuỗi suy nghĩ có mặt
trong ngữ cảnh của mô hình). Ngôn ngữ 1S-con người
sử dụng biểu hiện khả năng ngôn ngữ, thay vì xác suất
số, để thể hiện sự không chắc chắn. Mô hình được nhắc
gán độ tin cậy cho dự đoán của nó bằng cách chọn từ
một tập hợp biểu hiện không chắc chắn ngôn ngữ: {Gần
như chắc chắn, Có khả năng, ..., Gần như không có cơ
hội}. Mỗi biểu hiện khả năng ngôn ngữ được ánh xạ
thành xác suất bằng cách sử dụng phản hồi từ khảo sát
con người trên mạng xã hội với 123 người tham gia
(Fagen-Ulmschneider, 2023). Ngôn ngữ 1S-tối ưu sử
dụng một tập hợp câu hỏi và câu trả lời hiệu chuẩn được
giữ lại để tính độ chính xác trung bình cho mỗi biểu hiện
khả năng, sử dụng các giá trị 'được tối ưu hóa' này thay
thế. Các biểu hiện không được sử dụng cho ít nhất 1/N
câu hỏi, trong đó N là số câu hỏi hiệu chuẩn, chỉ đơn
giản sử dụng xác suất con người.

⁶Chúng tôi đánh giá gpt-3.5-turbo trên cả ba tập dữ liệu
bằng n=20 mẫu, nhưng hiệu chuẩn không cải thiện có ý
nghĩa, vì vậy chúng tôi luôn sử dụng n=10 để giảm chi phí API.
⁷Đối với mỗi LM đóng, chúng tôi sử dụng các tham số lấy mẫu
mặc định của nó (top-p 1.0 cho GPT-* và top-p 0.7 cho
Claude). Đối với Llama-2, chúng tôi sử dụng nhiệt độ 1.0 và
top-p 1.0.
⁸Tuy nhiên, lưu ý rằng không phương pháp nào được mô tả
tinh chỉnh mô hình để thực hiện tốt hơn trong diễn đạt bằng lời.

3 Kết quả
Bảng 1–5 cho thấy kết quả đánh giá các phương pháp
khác nhau để trích xuất độ tin cậy từ RLHF-LM trên
gpt-3.5-turbo, gpt-4, claude-1,

--- TRANG 5 ---
TriviaQA SciQ TruthfulQA
Phương pháp ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑ ECE ↓ECE-t ↓BS-t ↓AUC ↑
Xác suất nhãn 0.151 0.124 0.156 0.865 0.266 0.189 0.243 0.707 0.405 0.361 0.396 0.407
Diễn đạt 1S top-1 0.071 0.067 0.186 0.793 0.196 0.053 0.239 0.648 0.386 0.172 0.266 0.502
Diễn đạt 1S top-2 0.060 0.073 0.194 0.815 0.153 0.032 0.230 0.667 0.340 0.037 0.227 0.440
Diễn đạt 1S top-4 0.069 0.079 0.182 0.816 0.105 0.043 0.229 0.648 0.231 0.102 0.237 0.465
Ngôn ngữ 1S con người 0.179 0.115 0.195 0.749 0.071 0.101 0.252 0.603 0.376 0.366 0.383 0.407
Ngôn ngữ 1S-tối ưu 0.077 0.068 0.186 0.779 0.019 0.042 0.236 0.590 0.047 0.051 0.239 0.435

Bảng 5: Với Llama2-70B-Chat, hiệu chuẩn được diễn đạt cung cấp cải thiện so với xác suất có điều kiện trên một số chỉ số, nhưng cải thiện ít nhất quán hơn nhiều so với GPT-* và Claude-*.

claude-2 và Llama-2-70b-chat, tương ứng. Chúng tôi
rút ra một số kết luận chính từ các thí nghiệm này. 1.
Các RLHF-LM lớn thường có thể trực tiếp diễn đạt độ
tin cậy được hiệu chuẩn tốt hơn (hoặc là xác suất tin
cậy số hoặc biểu hiện như 'rất có khả năng') so với xác
suất có điều kiện của mô hình. 2. Trong số các phương
pháp diễn đạt xác suất trực tiếp, chúng tôi quan sát thấy
rằng việc tạo ra và đánh giá nhiều giả thuyết cải thiện
hiệu chuẩn (xem Hình 1), tương tự như con người (Lord
et al., 1985), và củng cố một phát hiện tương tự trong
LM (Kadavath et al., 2022). 3. Mô hình ngôn ngữ có
thể thể hiện sự không chắc chắn của chúng bằng xác
suất số cũng tốt hoặc tốt hơn bằng từ ngữ, điều này
đáng ngạc nhiên trong bối cảnh những khó khăn lâu dài
trong việc biểu diễn số trong mô hình ngôn ngữ (Thawani
et al., 2021). 4. Nhắc chuỗi suy nghĩ không cải thiện
hiệu chuẩn được diễn đạt (xem Hình 5 Phụ lục cho kết
quả CoT bổ sung). 5. Hiệu chuẩn xác suất có điều kiện
của cả hai mô hình Claude gần như nằm giữa gpt-3.5-
turbo và gpt-4; tuy nhiên, trong khi Claude 1 yếu hơn
nhiều trong việc diễn đạt độ tin cậy của nó, Claude 2
nói chung mạnh hơn một chút so với gpt-3.5-turbo
trong diễn đạt. Hiệu chuẩn lời nói của mô hình nguồn
mở Llama-2-70b-chat nói chung yếu hơn so với các mô
hình nguồn đóng nhưng vẫn thể hiện cải thiện so với
xác suất có điều kiện của nó theo một số chỉ số, và làm
như vậy rõ ràng nhất trên TruthfulQA.

4 Thảo luận
Tóm lại, chúng tôi nghiên cứu hiệu chuẩn của các RLHF-
LM được sử dụng rộng rãi. Chúng tôi đầu tiên tái lập
phát hiện cho GPT-4 (OpenAI, 2023) rằng RLHF có thể
làm xấu đi hiệu chuẩn xác suất có điều kiện của mô hình
bằng cách sử dụng các mô hình cơ sở và chat Llama-2-
70B nguồn mở (Hình 2). Để giảm thiểu sự suy giảm này
và dễ dàng trích xuất điểm tin cậy được hiệu chuẩn cho
các mô hình mà xác suất logarit không có sẵn, chúng tôi
đề xuất và nghiên cứu các phương pháp mới có thể khai
thác độ tin cậy được hiệu chuẩn từ RLHF-LM bằng cách
nhắc mô hình diễn đạt độ tin cậy của nó trong không gian
token. Chúng tôi thấy xác suất được diễn đạt được hiệu
chuẩn tốt hơn so với xác suất có điều kiện trên một số
mô hình đóng, với kết quả hỗn hợp cho Llama-2-70B-
Chat.

Kết quả của chúng tôi đặt ra một số câu hỏi cho công
trình tương lai. Đáng chú ý nhất, sự khác biệt giữa khả
năng diễn đạt độ tin cậy của GPT-*, Claude-* và Llama-
2 là đáng kể. Những yếu tố nào quan trọng để học kỹ
năng này? Ngoài ra, các prompt tin cậy số được diễn đạt
1 giai đoạn và 2 giai đoạn đôi khi khác nhau rất nhiều
trong hiệu chuẩn độ tin cậy của chúng. Làm thế nào
chúng ta có thể giảm độ nhạy cảm của hiệu chuẩn mô
hình đối với prompt? Vượt ra ngoài hỏi đáp, chúng ta
có thể tận dụng hiệu chuẩn tốt trong cài đặt câu trả lời
ngắn để cải thiện độ tin cậy của tạo ra dạng dài, có lẽ
bằng cách chia nhỏ tạo ra dạng dài thành một chuỗi câu
hỏi ngắn? Cuối cùng, hiệu chuẩn của mô hình ngôn ngữ
phụ thuộc vào miền ở mức độ nào; kết luận của chúng
tôi trong bối cảnh ghi nhớ thực tế có giữ trong bối cảnh
lý luận hoặc số học không? Trả lời những câu hỏi này
cung cấp một con đường hướng tới xây dựng các hệ
thống ngôn ngữ đáng tin cậy và hữu ích hơn.

Hạn chế. Trong khi công trình của chúng tôi thể hiện
một cách tiếp cận mới đầy hứa hẹn để tạo ra độ tin cậy
được hiệu chuẩn thông qua diễn đạt bằng lời, có những
hạn chế có thể được giải quyết trong công trình tương lai.
Đầu tiên, thí nghiệm của chúng tôi tập trung vào các vấn
đề hướng về ghi nhớ thực tế, và mức độ mà quan sát của
chúng tôi sẽ giữ cho các cài đặt nặng về lý luận là một
câu hỏi mở thú vị. Ngoài ra, việc thiếu chi tiết kỹ thuật
có sẵn cho nhiều RLHF-LM đóng hiện đại có thể hạn
chế khả năng hiểu những yếu tố nào cho phép mô hình
diễn đạt độ tin cậy được hiệu chuẩn tốt và sự khác biệt
trong khả năng này giữa các mô hình khác nhau. Cuối
cùng, nghiên cứu của chúng tôi bị hạn chế ở hỏi đáp
dạng ngắn; công trình tương lai nên mở rộng phân tích
này sang cài đặt tạo ra dạng dài hơn.

--- TRANG 6 ---
Lời cảm ơn. CF và CDM là CIFAR Fellows. EM biết ơn
nhận được tài trợ từ Knight-Hennessy Graduate Fellowship.
AZ được hỗ trợ bởi chương trình học bổng nghiên cứu sau
đại học NSF. Nghiên cứu này được hỗ trợ một phần bởi
Juniper Networks, Apple và tài trợ ONR N00014-20-1-2675.
Các tác giả cảm ơn Yoonho Lee và Noah Goodman vì phản
hồi hữu ích về chỉ số hiệu chuẩn và thiết kế thí nghiệm.

Tài liệu tham khảo
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan,
Nicholas Joseph, Saurav Kadavath, Jackson Kernion,
Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac
Hatfield-Dodds, Danny Hernandez, Tristan Hume,
Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
Nanda, Catherine Olsson, Dario Amodei, Tom
Brown, Jack Clark, Sam McCandlish, Chris Olah,
Ben Mann, và Jared Kaplan. 2022a. Huấn luyện
một trợ lý hữu ích và vô hại với học tăng cường
từ phản hồi của con người.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosuite, Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemi Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bow-
man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,
Nicholas Joseph, Sam McCandlish, Tom Brown, và
Jared Kaplan. 2022b. Constitutional AI: Tính vô hại
từ phản hồi AI.

Glenn W. Brier. 1950. Xác minh Dự báo Được Thể
hiện theo Xác suất. Monthly Weather Review,
78(1):1–3.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, và Dario Amodei. 2020.
Mô hình ngôn ngữ là học viên few-shot. Trong Ad-
vances in Neural Information Processing Systems,
volume 33, pages 1877–1901. Curran Associates,
Inc.

Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-
ter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro,
và Yi Zhang. 2023. Tia lửa trí tuệ nhân tạo tổng
quát: Thí nghiệm sớm với GPT-4. ArXiv preprint
arXiv:2303.12712.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, và Dario Amodei. 2017. Học tăng
cường sâu từ sở thích của con người. Trong Ad-
vances in Neural Information Processing Systems,
volume 30. Curran Associates, Inc.

Wade Fagen-Ulmschneider. 2023. Nhận thức về từ
xác suất. Ms., UIUC, 05-24-2023.

Yonatan Geifman và Ran El-Yaniv. 2017. Phân loại
chọn lọc cho mạng neural sâu. Trong Proceedings
of the 31st International Conference on Neural
Information Processing Systems, NIPS'17, page
4885–4894, Red Hook, NY, USA. Curran Associates
Inc.

Chuan Guo, Geoff Pleiss, Yu Sun, và Kilian Q. Wein-
berger. 2017. Về hiệu chuẩn của mạng neural hiện
đại. Trong Proceedings of the 34th International
Conference on Machine Learning, volume 70 của
Proceedings of Machine Learning Research, pages
1321–1330. PMLR.

Mandar Joshi, Eunsol Choi, Daniel Weld, và Luke
Zettlemoyer. 2017. TriviaQA: Một tập dữ liệu thách
thức quy mô lớn được giám sát từ xa cho đọc hiểu.
Trong Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 1601–1611, Vancouver,
Canada. Association for Computational Linguistics.

Saurav Kadavath, Tom Conerly, Amanda Askell, Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli
Tran-Johnson, Scott Johnston, Sheer El-Showk,
Andy Jones, Nelson Elhage, Tristan Hume, Anna
Chen, Yuntao Bai, Sam Bowman, Stanislav Fort,
Deep Ganguli, Danny Hernandez, Josh Jacobson,
Jackson Kernion, Shauna Kravec, Liane Lovitt, Ka-
mal Ndousse, Catherine Olsson, Sam Ringer, Dario
Amodei, Tom Brown, Jack Clark, Nicholas Joseph,
Ben Mann, Sam McCandlish, Chris Olah, và Jared
Kaplan. 2022. Mô hình ngôn ngữ (hầu hết) biết
những gì chúng biết. Arxiv arxiv:2207.05221.

Lorenz Kuhn, Yarin Gal, và Sebastian Farquhar. 2023.
Sự không chắc chắn ngữ nghĩa: Bất biến ngôn ngữ
cho ước lượng không chắc chắn trong tạo ra ngôn
ngữ tự nhiên. Trong The Eleventh International
Conference on Learning Representations.

Stephanie Lin, Jacob Hilton, và Owain Evans. 2022a.
Dạy mô hình thể hiện sự không chắc chắn của
chúng bằng từ ngữ. Transactions on Machine Learn-
ing Research.

Stephanie Lin, Jacob Hilton, và Owain Evans. 2022b.
TruthfulQA: Đo lường cách mô hình bắt chước sự
sai lầm của con người.

--- TRANG 7 ---
sai lầm. Trong Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 3214–3252, Dublin,
Ireland. Association for Computational Linguistics.

Charles Lord, Mark Lepper, và Elizabeth Preston.
1985. Xem xét mặt đối lập: Một chiến lược hiệu
chỉnh cho phán đoán xã hội. Journal of personality
and social psychology, 47:1231–43.

Sabrina J. Mielke, Arthur Szlam, Emily Dinan, và Y-
Lan Boureau. 2022. Giảm sự quá tự tin của các
tác nhân hội thoại thông qua hiệu chuẩn ngôn ngữ.
Transactions of the Association for Computational
Linguistics, 10:857–872.

Thomas Mussweiler, Fritz Strack, và Tim Pfeiffer.
2000. Vượt qua hiệu ứng neo không thể tránh khỏi:
Xem xét mặt đối lập bù đắp cho khả năng tiếp cận
chọn lọc. Personality and Social Psychology Bulle-
tin, 26(9):1142–1150.

OpenAI. 2023. Báo cáo kỹ thuật Gpt-4.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul F Christiano, Jan Leike, và Ryan Lowe. 2022.
Huấn luyện mô hình ngôn ngữ tuân theo hướng dẫn
với phản hồi của con người. Trong Advances in
Neural Information Processing Systems, volume 35,
pages 27730–27744. Curran Associates, Inc.

Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado,
D. Sculley, Sebastian Nowozin, Joshua V. Dillon,
Balaji Lakshminarayanan, và Jasper Snoek. 2019.
Bạn có thể tin tưởng sự không chắc chắn của mô
hình không? đánh giá sự không chắc chắn dự đoán
dưới sự thay đổi tập dữ liệu. Trong Proceedings of
the 33rd International Conference on Neural Infor-
mation Processing Systems, Red Hook, NY, USA.
Curran Associates Inc.

Seo Yeon Park và Cornelia Caragea. 2022. Về hiệu
chuẩn của mô hình ngôn ngữ được tiền huấn luyện
sử dụng mixup được hướng dẫn bởi diện tích dưới
lề và saliency. Trong Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 5364–
5374, Dublin, Ireland. Association for Computational
Linguistics.

Adam Roberts, Colin Raffel, và Noam Shazeer. 2020.
Bạn có thể đóng gói bao nhiêu kiến thức vào các
tham số của một mô hình ngôn ngữ? Trong Pro-
ceedings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 5418–5426, Online. Association for Compu-
tational Linguistics.

Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.
Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
Dario Amodei, và Paul Christiano. 2022. Học tóm
tắt từ phản hồi của con người.

Avijit Thawani, Jay Pujara, Filip Ilievski, và Pedro
Szekely. 2021. Biểu diễn số trong NLP: một khảo
sát và một tầm nhìn. Trong Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 644–656, Online. As-
sociation for Computational Linguistics.

Johannes Welbl, Nelson F. Liu, và Matt Gardner. 2017.
Crowdsourcing câu hỏi khoa học trắc nghiệm. ArXiv,
abs/1707.06209.

Yuxin Xiao, Paul Pu Liang, Umang Bhatt, Willie
Neiswanger, Ruslan Salakhutdinov, và Louis-
Philippe Morency. 2022. Lượng hóa không chắc
chắn với mô hình ngôn ngữ được tiền huấn luyện:
Một phân tích thực nghiệm quy mô lớn. Trong Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2022, pages 7273–7284, Abu Dhabi,
United Arab Emirates. Association for Computational
Linguistics.

Kaitlyn Zhou, Dan Jurafsky, và Tatsunori Hashimoto.
2023. Điều hướng vùng xám: Biểu hiện quá tự tin
và không chắc chắn trong mô hình ngôn ngữ.

Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, và Geoffrey Irving. 2020. Tinh chỉnh mô hình
ngôn ngữ từ sở thích của con người.

--- TRANG 8 ---
Gần như Không Có Cơ HộiRất Không Có Khả NăngCơ Hội Nhỏ NhẹÍt Cơ HộiKhông Có Khả NăngCó Lẽ KhôngKhoảng Ngang NhauTốt Hơn Ngang NhauCó Khả NăngCó LẽCơ Hội Rất TốtRất Có Khả NăngGần như Chắc Chắn
Biểu Hiện Khả Năng0.000.050.100.150.200.250.300.350.40Phần Trăm Phản HồiSử dụng biểu hiện khả năng bởi 3.5-turbo
TriviaQA
SciQ
TruthfulQAHình 3: Tỷ lệ sử dụng mỗi biểu hiện khả năng của gpt-3.5-turbo; 
mô hình thể hiện độ tin cậy được diễn đạt thấp hơn nhiều trên TruthfulQA
so với các vấn đề ghi nhớ thực tế tiêu chuẩn.

Gần như Không Có Cơ HộiRất Không Có Khả NăngCơ Hội Nhỏ NhẹÍt Cơ HộiKhông Có Khả NăngCó Lẽ KhôngKhoảng Ngang NhauTốt Hơn Ngang NhauCó Khả NăngCó LẽCơ Hội Rất TốtRất Có Khả NăngGần như Chắc Chắn
Biểu Hiện Khả Năng0.00.10.20.30.4Phần Trăm Phản HồiSử dụng biểu hiện khả năng bởi GPT-4
TriviaQA
SciQ
TruthfulQA

Hình 4: Tỷ lệ sử dụng mỗi biểu hiện khả năng của gpt-4;
mô hình thể hiện độ tin cậy được diễn đạt thấp hơn đáng chú ý
trên TruthfulQA so với các vấn đề ghi nhớ thực tế tiêu chuẩn.

A Kết quả Bổ sung
Ở đây, chúng tôi bao gồm phân phối sử dụng biểu hiện
khả năng cho gpt-3.5 và gpt-4 trong Hình 3 và 4, tương
ứng. gpt-3.5 có hệ thống ít tự tin hơn đối với TruthfulQA.
Sự tương phản giữa độ tin cậy mô hình cho TriviaQA và
SciQ so với TruthfulQA thậm chí còn rõ rệt hơn đối với
gpt-4.

Chúng tôi cũng cung cấp kết quả hiệu chuẩn bổ sung cho
các phương pháp chuỗi suy nghĩa. Chúng tôi so sánh
prompt CoT được diễn đạt một giai đoạn (Diễn đạt 1S
CoT), prompt CoT được diễn đạt hai giai đoạn (Diễn đạt
2S CoT), và phương pháp được diễn đạt hai giai đoạn sử
dụng CoT ngay trước khi khai thác độ tin cậy số (Diễn
đạt 2S Cot Prob) thay vì trước dự đoán, như được hiển
thị cho gpt-3.5 trên Trivia QA, SciQ và Truthful QA
trong Hình 5. Chúng tôi thấy rằng CoT không cải thiện
hiệu chuẩn một cách rõ rệt trên bất kỳ cài đặt hoặc tập
dữ liệu nào.

B Quy trình Khớp cho Nhiệt độ và Xác suất cho Biểu
hiện Ngôn ngữ
Để khớp nhiệt độ được sử dụng để tính ECE-t và BS-t,
chúng tôi chia tổng dữ liệu của mình thành 5 phần. Đối
với mỗi phần, chúng tôi sử dụng nó một lần để khớp
nhiệt độ và đánh giá chỉ số trên các phần còn lại. Chúng
tôi thấy rằng việc khớp nhiệt độ trên 20% dữ liệu cho ra
nhiệt độ tương đối ổn định qua các phần.

Hình 5: Lỗi hiệu chuẩn mong đợi không được cải thiện nhất
quán cho bất kỳ biến thể prompt CoT nào trên gpt-3.5-turbo.

Chúng tôi báo cáo ECE và BS được điều chỉnh nhiệt độ
trung bình là ECE-t và BS-t.

Để tính ECE và AUC cho Ngôn ngữ 1S-tối ưu, chúng
tôi tương tự chia tổng dữ liệu của mình thành 5 phần,
sử dụng 4 phần để khớp xác suất đằng sau mỗi biểu hiện
ngôn ngữ về tin cậy, sau đó đánh giá trên phần còn lại.
Để tính ECE-t và BS-t cho Ngôn ngữ 1S-tối ưu, chúng
tôi giữ lại một trong 5 phần để khớp nhiệt độ. Chúng tôi
sử dụng 3 phần để khớp xác suất cho biểu hiện ngôn
ngữ, tính nhiệt độ dựa trên các xác suất này trên tập
nhiệt độ, và đánh giá chỉ số trên phần cuối cùng. Sau đó
chúng tôi tính trung bình chỉ số trên tất cả 20 vòng quay
của các phần.

C Template Prompt
Template prompt cho mỗi phương pháp lấy mẫu được
cung cấp trong Bảng 6. Câu hỏi được thay thế cho biến
${THE_QUESTION} trong mỗi prompt. Để đánh giá tính
đúng đắn của câu trả lời, chúng tôi sử dụng gpt-3.5-
turbo cho SciQ và TruthfulQA và gpt-4 cho TriviaQA
do sự bất đồng cao của gpt-3.5-turbo với

--- TRANG 9 ---
Phương pháp Template
Xác suất nhãn Cung cấp dự đoán tốt nhất của bạn cho câu hỏi sau. Chỉ đưa ra dự đoán, không có từ ngữ hoặc giải thích khác.\n\nVí dụ:\n\nDự đoán: <dự đoán có khả năng nhất, càng ngắn càng tốt; không phải câu hoàn chỉnh, chỉ là dự đoán!>\n\nCâu hỏi là:${THE_QUESTION}
Xác suất 'Đúng' Câu hỏi: ${QUESTION}\nCâu trả lời Đề xuất: ${ANSWER}\nCâu trả lời đề xuất có:\n\t(A) Đúng hay\n\t(B) Sai?\n Câu trả lời đề xuất là:
Diễn đạt 1S top-1 Cung cấp dự đoán tốt nhất và xác suất nó đúng (0.0 đến 1.0) cho câu hỏi sau. Chỉ đưa ra dự đoán và xác suất, không có từ ngữ hoặc giải thích khác. Ví dụ:\n\nDự đoán: <dự đoán có khả năng nhất, càng ngắn càng tốt; không phải câu hoàn chỉnh, chỉ là dự đoán!>\n Xác suất: <xác suất từ 0.0 đến 1.0 rằng dự đoán của bạn đúng, không có bình luận gì thêm; chỉ xác suất!>\n\nCâu hỏi là: ${THE_QUESTION}
Diễn đạt 1S top-k Cung cấp ${k} dự đoán tốt nhất và xác suất mỗi cái đúng (0.0 đến 1.0) cho câu hỏi sau. Chỉ đưa ra dự đoán và xác suất, không có từ ngữ hoặc giải thích khác. Ví dụ:\n\nG1: <dự đoán có khả năng đầu tiên, càng ngắn càng tốt; không phải câu hoàn chỉnh, chỉ là dự đoán!>\n\nP1: <xác suất từ 0.0 đến 1.0 rằng G1 đúng, không có bình luận gì thêm; chỉ xác suất!> ... G${k}: <dự đoán có khả năng thứ ${k}, càng ngắn càng tốt; không phải câu hoàn chỉnh, chỉ là dự đoán!>\n\nP${k}: <xác suất từ 0.0 đến 1.0 rằng G${k} đúng, không có bình luận gì thêm; chỉ xác suất!> \n\nCâu hỏi là: ${THE_QUESTION}
Diễn đạt 2S CoT Cung cấp dự đoán tốt nhất cho câu hỏi sau. Trước khi đưa ra câu trả lời, cung cấp giải thích từng bước về quá trình suy nghĩ của bạn. Sau đó trên dòng mới đưa ra dự đoán không có từ ngữ hoặc giải thích khác.\n\nVí dụ:\n\nGiải thích: <giải thích từng bước một câu về quá trình suy nghĩ của bạn>\n\nDự đoán: <dự đoán có khả năng nhất, càng ngắn càng tốt; không phải câu hoàn chỉnh, chỉ là dự đoán!>\n\nCâu hỏi là: ${THE_QUESTION}
Cung cấp xác suất dự đoán của bạn đúng. Chỉ đưa ra xác suất, không có từ ngữ hoặc giải thích khác.\n\nVí dụ:\n\nXác suất: <xác suất từ 0.0 đến 1.0 rằng dự đoán của bạn đúng, không có bình luận gì thêm; chỉ xác suất!>\n
Diễn đạt 2S top-1 Cung cấp dự đoán tốt nhất cho câu hỏi sau. Chỉ đưa ra dự đoán, không có từ ngữ hoặc giải thích khác.\n\nVí dụ:\n\nDự đoán: <dự đoán có khả năng nhất, càng ngắn càng tốt; không phải câu hoàn chỉnh, chỉ là dự đoán!>\n\nCâu hỏi là:${THE_QUESTION}
Cung cấp xác suất dự đoán của bạn đúng. Chỉ đưa ra xác suất, không có từ ngữ hoặc giải thích khác.\n\nVí dụ:\n\nXác suất: <xác suất từ 0.0 đến 1.0 rằng dự đoán của bạn đúng, không có bình luận gì thêm; chỉ xác suất!>\n
Diễn đạt 2S top-k Cung cấp ${k} dự đoán tốt nhất cho câu hỏi sau. Chỉ đưa ra dự đoán, không có từ ngữ hoặc giải thích khác. Ví dụ:\n\nG1: <dự đoán có khả năng đầu tiên, càng ngắn càng tốt; không phải câu hoàn chỉnh, chỉ là dự đoán!>\n\nP1: <xác suất từ 0.0 đến 1.0 rằng G1 đúng, không có bình luận gì thêm; chỉ xác suất!> ... G${k}: <dự đoán có khả năng thứ ${k}, càng ngắn càng tốt; không phải câu hoàn chỉnh, chỉ là dự đoán!>\n\nCâu hỏi là:${THE_QUESTION}
Cung cấp xác suất mỗi dự đoán của bạn đúng. Chỉ đưa ra xác suất, không có từ ngữ hoặc giải thích khác.\n\nVí dụ:\n\nP1: <xác suất từ 0.0 đến 1.0 rằng G1 đúng, không có bình luận gì thêm; chỉ xác suất!>\n... P${k}: <xác suất từ 0.0 đến 1.0 rằng G${k} đúng, không có bình luận gì thêm; chỉ xác suất!>
Ngôn ngữ 1S Cung cấp dự đoán tốt nhất cho câu hỏi sau, và mô tả khả năng dự đoán của bạn đúng là một trong các biểu hiện sau: ${EXPRESSION_LIST}. Chỉ đưa ra dự đoán và độ tin cậy của bạn, không có từ ngữ hoặc giải thích khác. Ví dụ:\n\nDự đoán: <dự đoán có khả năng nhất, càng ngắn càng tốt; không phải câu hoàn chỉnh, chỉ là dự đoán!>\nĐộ tin cậy: <mô tả độ tin cậy, không có bình luận gì thêm; chỉ là cụm từ ngắn!>\n\nCâu hỏi là: ${THE_QUESTION}

Bảng 6: Template prompt cho mỗi phương pháp được đánh giá. Các phương pháp trên đường đôi sử dụng nhiều mẫu để ước tính điểm tin cậy; các phương pháp dưới đường đôi sử dụng độ tin cậy được diễn đạt trực tiếp, chỉ cần một mẫu duy nhất.

--- TRANG 10 ---
người đánh giá trên TriviaQA. Sử dụng câu trả lời đúng
là ${GOLD_ANSWER} và câu trả lời được tạo bởi mô hình
là ${PRED_ANSWER}, chúng tôi sử dụng template prompt
sau:

Hai câu trả lời sau cho câu hỏi Q của tôi có
tương đương về mặt ngữ nghĩa không?\n\nQ:
${THE_QUESTION}\nA1: ${GOLD_ANSWER}\nA2:
${PRED_ANSWER}\n\nVui lòng trả lời bằng một
từ, hoặc "Có." hoặc "Không.", và giải thích
lý do của bạn.
