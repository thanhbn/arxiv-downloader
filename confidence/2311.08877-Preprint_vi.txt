# LLAMAS BIẾT NHỮNG GÌ MÀ GPT KHÔNG HIỂN THỊ:
CÁC MÔ HÌNH ĐẠI DIỆN CHO ƯỚC LƯỢNG ĐỘ TIN CẬY

Vaishnavi Shrivastava, Percy Liang, Ananya Kumar
Đại học Stanford
{vshrivas, pliang, ananya}@cs.stanford.edu

TÓM TẮT
Để duy trì lòng tin của người dùng, các mô hình ngôn ngữ lớn (LLM) nên báo hiệu độ tin cậy thấp trên những ví dụ mà chúng trả lời sai, thay vì làm người dùng hiểu lầm. Phương pháp tiêu chuẩn để ước lượng độ tin cậy là sử dụng xác suất softmax của các mô hình này, nhưng tính đến tháng 11 năm 2023, các LLM tiên tiến như GPT-4 và Claude-v1.3 không cung cấp quyền truy cập vào các xác suất này. Trước tiên chúng tôi nghiên cứu việc khai thác độ tin cậy bằng ngôn ngữ — hỏi LLM về độ tin cậy trong câu trả lời của nó — điều này hoạt động khá tốt (80,5% AUC trên GPT-4 tính trung bình trên 12 bộ dữ liệu hỏi đáp — cao hơn 7% so với đường cơ sở ngẫu nhiên) nhưng vẫn còn chỗ để cải thiện. Sau đó chúng tôi khám phá việc sử dụng mô hình độ tin cậy đại diện — sử dụng một mô hình mà chúng ta có xác suất để đánh giá độ tin cậy của mô hình gốc trong một câu hỏi đã cho. Đáng ngạc nhiên, mặc dù những xác suất này đến từ một mô hình khác và thường yếu hơn, phương pháp này dẫn đến AUC cao hơn so với độ tin cậy ngôn ngữ trên 9 trong 12 bộ dữ liệu. Phương pháp tốt nhất của chúng tôi kết hợp độ tin cậy ngôn ngữ và xác suất mô hình đại diện cho ra các ước lượng độ tin cậy tiên tiến nhất trên tất cả 12 bộ dữ liệu (84,6% AUC trung bình trên GPT-4).

1 GIỚI THIỆU
Khi các mô hình ngôn ngữ lớn (LLM) ngày càng được triển khai, điều quan trọng là chúng báo hiệu độ tin cậy thấp trên những ví dụ mà chúng có khả năng mắc lỗi. Vấn đề này được gọi là phân loại có chọn lọc (hoặc phân loại với tùy chọn từ chối) và được nghiên cứu rộng rãi trong học máy (Cordella et al., 1995; Geifman & El-Yaniv, 2017; Feng et al., 2019; Jones et al., 2021), lý thuyết học tập (El-Yaniv & Wiener, 2010; Bartlett & Wegkamp, 2008), và xử lý ngôn ngữ tự nhiên (Kamath et al., 2020; Liang et al., 2022; Xiong et al., 2023). Các phương pháp truyền thống tận dụng xác suất softmax của mô hình (Hendrycks & Gimpel, 2017; Jones et al., 2021; Liang et al., 2022) hoặc các biểu diễn của mô hình (Lee et al., 2018). Mục tiêu của bài báo này là tạo ra các ước lượng độ tin cậy tốt cho các LLM tiên tiến, những mô hình không cung cấp xác suất hoặc biểu diễn của mô hình (như GPT-4 và Claude-v1.3).

Trước tiên chúng tôi xem xét một ý tưởng tự nhiên là khai thác điểm số độ tin cậy ngôn ngữ (Tian et al., 2023; Lin et al., 2022; Xiong et al., 2023) — nhắc LLM đánh giá độ tin cậy trong câu trả lời của nó (Hình 1, GPT-4 Ngôn ngữ). Chúng tôi thấy rằng độ tin cậy ngôn ngữ hoạt động khá tốt cho các mô hình tiên tiến, và tốt hơn nhiều so với đường cơ sở đoán ngẫu nhiên, nhưng vẫn còn chỗ để cải thiện (Phần 3). Tính trung bình trên các bộ dữ liệu, GPT-4 đạt AUC phân loại có chọn lọc là 80,5%, cao hơn 7% so với đường cơ sở đoán ngẫu nhiên. Kết quả của chúng tôi áp dụng trên 12 bộ dữ liệu tiêu chuẩn (8 bộ dữ liệu MMLU, TruthfulQA, CommonsenseQA, OpenbookQA, và MedQA), 5 mô hình (GPT-4, Claude-v1.3, GPT-3.5, Llama 2, và text-davinci-003), và 24 định dạng prompt khác nhau (ví dụ, chuỗi suy nghĩ, hướng dẫn khác nhau, prompt few-shot giả). Tuy nhiên, độ tin cậy ngôn ngữ hoạt động kém hơn nhiều so với việc sử dụng xác suất mô hình khi những xác suất này có sẵn (đối với các mô hình kém chính xác hơn). Ví dụ, trên Llama 2, độ tin cậy ngôn ngữ đạt AUC trung bình thấp hơn 10,7% so với xác suất mô hình, cho thấy phạm vi để tinh chỉnh thêm trong các đánh giá độ tin cậy này.

Do đó, chúng tôi đề xuất phương pháp mô hình đại diện là lấy câu trả lời từ GPT-4 hoặc Claude-v1.3, nhưng độ tin cậy từ một mô hình khác như Llama 2 (Hình 1, Đại diện), nơi có xác suất softmax, làm ước lượng độ tin cậy cho câu trả lời của mô hình gốc (Phần 4). Mô hình hóa độ tin cậy đại diện cải thiện AUC phân loại có chọn lọc trung bình cho GPT-4 lên 82,1%. Ngay cả việc sử dụng mô hình đại diện yếu hơn hoặc nhỏ hơn nhiều như text-davinci-003 hoặc Llama 2-13B cũng dẫn đến AUC tương đương hoặc tốt hơn cho các mô hình mạnh hơn như GPT-4, Claude-v1.3, và GPT-3.5. Thú vị thay, điểm số độ tin cậy có thể chuyển giao giữa các mô hình, ngay cả khi mô hình tạo ra điểm số độ tin cậy khác (hoặc kém hơn nhiều). Trong Phần 4, chúng tôi cung cấp một số phân tích và trực giác cho hành vi này.

Chúng tôi thấy rằng điểm số độ tin cậy ngôn ngữ và xác suất mô hình đại diện là bổ sung: kết hợp các điểm số này dẫn đến lợi ích thêm (Hình 1, Hỗn hợp). Ví dụ, phương pháp hỗn hợp này tăng AUC phân loại có chọn lọc của GPT-4 lên 83,4%. Phương pháp hỗn hợp cũng vượt trội hơn công trình đương thời (Xiong et al., 2023) về tính nhất quán tự nhiên (AUC: 82,8%), điều này đắt đỏ hơn (bao gồm việc lấy mẫu GPT-4 năm lần mỗi đầu vào) và bao gồm xử lý hậu kỳ. Kết hợp phương pháp của chúng tôi với điểm số độ tin cậy dựa trên tính nhất quán tự nhiên dẫn đến kết quả tốt nhất: AUC trung bình 84,6%.

Phân tích của chúng tôi cho thấy rằng điểm số độ tin cậy ngôn ngữ bị hạn chế vì chúng rất thô — ví dụ, GPT-4 xuất ra chính xác cùng một độ tin cậy (0,9) trên 50% ví dụ, điều này hạn chế khả năng phân tách câu trả lời đúng và sai của nó. Xác suất mô hình đại diện hoạt động tốt ngay cả trên một mô hình khác, vì những ví dụ thách thức đối với một mô hình chuyển giao sang một mô hình khác. Cuối cùng, trộn vào chỉ một phần nhỏ xác suất mô hình đại diện cho phép các câu trả lời trước đây có cùng độ tin cậy ngôn ngữ có thể phân tách được thông qua các điểm số độ tin cậy tổng hợp khác nhau, tăng hiệu suất tổng thể với can thiệp tối thiểu.

2 THIẾT LẬP
Mục tiêu của chúng tôi là phân loại có chọn lọc: xuất ra điểm số độ tin cậy cao hơn trên những đầu vào mà mô hình đúng, so với những đầu vào mà mô hình sai (El-Yaniv & Wiener, 2010; Geifman & El-Yaniv, 2017). Chúng tôi tập trung vào các mô hình ngôn ngữ tiên tiến như GPT-4 và Claude-v1.3, hiện tại không tiết lộ xác suất được tính toán trong lớp đầu ra softmax của chúng.

Nhiệm vụ. Cho một đầu vào văn bản x, một mô hình xuất ra một câu trả lời (có thể ngẫu nhiên) y(x). Đặt R(x, y) = 1 nếu câu trả lời y đúng cho đầu vào x, và 0 nếu ngược lại. Mục tiêu của chúng ta là xuất ra một điểm số độ tin cậy C(x) ∈ [0,1]. Điểm số độ tin cậy tốt là cần thiết trong các hệ thống học máy thế giới thực: đối với những đầu vào khi C(x) thấp hơn, chúng ta có thể chuyển cho chuyên gia con người hoặc cảnh báo người dùng, thay vì làm người dùng hiểu lầm với câu trả lời không chính xác.

Thước đo. Một thước đo phổ biến cho phân loại có chọn lọc là AUC (diện tích dưới đường cong độ che phủ-độ chính xác) (El-Yaniv & Wiener, 2010; Liang et al., 2022), xem xét mô hình chính xác như thế nào nếu được phép từ chối (nói "Tôi không biết") trên một số ví dụ. Đặt A(c) là độ chính xác có chọn lọc tại độ che phủ c: độ chính xác nếu mô hình chỉ đưa ra dự đoán trên tỷ lệ c dữ liệu với điểm số độ tin cậy cao nhất. Để cho phép phá vỡ sự ràng buộc để đưa ra các dự đoán khác nhau cho các ví dụ có cùng điểm số độ tin cậy, chúng tôi thêm một lượng nhỏ nhiễu Gaussian vào mỗi điểm số độ tin cậy N(0, ε), ε→0. AUC là độ chính xác có chọn lọc trung bình A(c) trên tất cả c:

AUC(C, y) = lim ε→0 ∫₀¹ E[A(c)]dc (2.1)

Một đường cơ sở ngẫu nhiên (xuất ra xác suất ngẫu nhiên đồng nhất cho mỗi đầu vào) đạt AUC(C, y) = độ chính xác, vì vậy một mô hình với điểm số độ tin cậy tốt nên đạt AUC cao hơn độ chính xác. Lưu ý rằng việc thêm nhiễu N(0, ε) là quan trọng vì độ tin cậy ngôn ngữ cho các ví dụ khác nhau thường giống hệt nhau — không có nhiễu chúng ta sẽ đánh giá thấp đáng kể AUC của các mô hình (xem Phụ lục A.3 để biết thêm chi tiết).

Chúng tôi cũng xem xét AUROC, một thước đo tiêu chuẩn (Hendrycks & Gimpel, 2017; Xiong et al., 2023) được sử dụng để xem xét điểm số độ tin cậy có thể phân biệt tốt như thế nào giữa các ví dụ đúng và sai. Chúng tôi gắn nhãn một ví dụ 'Dương tính' nếu mô hình trả lời đúng và 'Âm tính' nếu ngược lại, và vẽ tỷ lệ dương tính thật so với tỷ lệ dương tính giả tại các ngưỡng phân loại khác nhau — AUROC là diện tích dưới đường cong này (Xem Phụ lục A.3 để biết thêm chi tiết). Xuất ra điểm số độ tin cậy ngẫu nhiên đạt AUROC là 0,5, vì vậy một mô hình với điểm số độ tin cậy tốt nên đạt AUROC trên 0,5.

Chúng tôi cũng báo cáo số ECE (lỗi hiệu chuẩn kỳ vọng) trong Phụ lục A.6. ECE xem xét liệu độ tin cậy của mô hình có phù hợp với độ chính xác của nó hay không, nhưng không chỉ ra khả năng của mô hình trong việc phân biệt giữa các ví dụ đúng và sai, vì vậy chúng tôi tập trung vào các thước đo AUC và AUROC.

Bộ dữ liệu. Chúng tôi nghiên cứu hiệu suất và độ tin cậy của mô hình trên mười hai bộ dữ liệu hỏi đáp tiêu chuẩn: TruthfulQA (TQA) (Lin et al., 2021), CommonsenseQA (CSQA) (Talmor et al., 2019), OpenbookQA (OBQA) (Mihaylov et al., 2018), MedQA (Jin et al., 2021), và 8 bộ dữ liệu MMLU (Hendrycks et al., 2021) - luật chuyên nghiệp (Luật), đạo đức kinh doanh (Đạo đức), vật lý khái niệm (Vật lý), kinh tế lượng (Kinh tế), đại số trừu tượng (Đại số), hóa học đại học (Hóa học), an ninh máy tính (An ninh), và Chính sách Đối ngoại Hoa Kỳ (Chính sách). Các bộ dữ liệu này bao gồm nhiều loại đa dạng bao gồm suy luận toán học, kiến thức khoa học, khoa học máy tính, khoa học xã hội, và suy luận thông thường. Chúng tôi lấy mẫu 250 câu hỏi từ tập kiểm tra của mỗi bộ dữ liệu để báo cáo kết quả (nếu tập kiểm tra nhỏ hơn, chúng tôi sử dụng toàn bộ tập kiểm tra). Xem Phụ lục A.1 để biết thêm chi tiết.

Mô hình. Chúng tôi nghiên cứu các mô hình ngôn ngữ tiên tiến, hầu hết không cung cấp quyền truy cập vào xác suất nội bộ tính đến thời điểm viết bài báo này — GPT-4 (OpenAI, 2023a), Claude-v1.3, và GPT-3.5-Turbo (OpenAI, 2022) (ảnh chụp ngày 13 tháng 6 năm 2023). Chúng tôi cũng nghiên cứu một số mô hình gần đây có cung cấp xác suất mô hình để so sánh có hệ thống — Llama 2 và Llama 2 Chat (kích thước 70B và 13B) (Touvron et al., 2023) và text-davinci-003 OpenAI (2023b). Nếu Llama 2 được đề cập trong văn bản mà không có thêm định danh, chúng tôi đề cập đến mô hình cơ sở Llama 2 70B.

2.1 PHƯƠNG PHÁP KHAI THÁC ĐỘ TIN CẬY

Độ tin cậy ngôn ngữ. Đối với mỗi câu hỏi, chúng tôi nhắc các mô hình zero-shot với hướng dẫn xuất ra một câu trả lời hợp lệ và đánh giá độ tin cậy của câu trả lời đó, lấy mẫu câu trả lời và độ tin cậy cùng nhau trong một lần tạo. Chúng tôi tạo một cách tham lam với nhiệt độ T = 0, và định nghĩa các ước lượng độ tin cậy được tạo bởi mô hình này là độ tin cậy ngôn ngữ. Vì có thể có nhiều cách khai thác độ tin cậy ngôn ngữ, chúng tôi thử nghiệm với 24 prompt khác nhau qua nhiều loại (chuỗi suy nghĩ, hướng dẫn khác nhau, ví dụ few shot giả). Chúng tôi thấy kết quả nhất quán qua các prompt, vì vậy chúng tôi báo cáo kết quả trên prompt tốt nhất của chúng tôi (xem Hình 2 cho một ví dụ hướng dẫn khai thác độ tin cậy ngôn ngữ). Phần 3 đánh giá chất lượng của độ tin cậy ngôn ngữ và báo hiệu nhu cầu về phương pháp ước lượng độ tin cậy tốt hơn.

Xác suất mô hình. Các mô hình như Llama 2 và text-davinci-003 cung cấp xác suất cấp token cho văn bản. Chúng tôi để điểm số độ tin cậy là xác suất của lựa chọn câu trả lời được tạo.

Mô hình đại diện cho độ tin cậy. Vì các mô hình như GPT-4 không cho ra ước lượng độ tin cậy, chúng tôi đề xuất sử dụng mô hình đại diện (ví dụ, Llama 2) để cung cấp ước lượng độ tin cậy. Chính thức, cho một đầu vào x chúng tôi xuất ra y(x) = y_gpt-4(x) (câu trả lời của GPT-4) và C(x) = C_Llama 2(x) (độ tin cậy của Llama 2 trong câu trả lời của chính nó). Mặc dù những điểm số độ tin cậy này đến từ một mô hình khác, Phần 4 cho thấy rằng phương pháp độ tin cậy đại diện vượt trội hơn điểm số độ tin cậy ngôn ngữ.

Hỗn hợp mô hình. Chúng tôi cũng đề xuất phương pháp hỗn hợp mô hình nơi chúng tôi kết hợp độ tin cậy ngôn ngữ từ mô hình chính và điểm số độ tin cậy của mô hình đại diện: cho đầu vào x chúng tôi xuất ra (1-α)C_M(x) + αC_S(x) trong đó M là mô hình chính và S là mô hình đại diện.

Chúng tôi sử dụng Llama 2 70B làm mô hình đại diện cho tất cả các mô hình chính vì nó hoạt động tốt nhất. Chúng tôi tối ưu α để giảm thiểu AUC, quét qua các giá trị từ 0 đến 1. Thú vị thay, trong Phần 5, chúng tôi cho thấy rằng ngay cả α = 0,001 cũng hoạt động tốt.

3 ĐỘ TIN CẬY NGÔN NGỮ: HỎI MÔ HÌNH VỀ ĐỘ TIN CẬY CỦA NÓ

Tính đến tháng 11 năm 2023, các mô hình ngôn ngữ tiên tiến như GPT-4 và Claude-v1.3 không cho phép truy cập vào xác suất mô hình nội bộ. Trong phần này, chúng tôi xem xét việc khai thác độ tin cậy bằng ngôn ngữ: nhắc các mô hình gán cho câu trả lời của chúng một điểm số độ tin cậy từ 0 đến 1. Chúng tôi thấy rằng những độ tin cậy ngôn ngữ này để lại rất nhiều chỗ để cải thiện (khoảng 50-65% AUROC, so với 50% cho đường cơ sở đoán ngẫu nhiên). Những độ tin cậy ngôn ngữ này cũng kém hơn nhiều so với xác suất mô hình nội bộ khi có sẵn (đối với các mô hình yếu hơn như text-davinci-003 và Llama 2). Chúng tôi hiển thị kết quả AUC và AUROC trên tất cả bộ dữ liệu và mô hình trong Bảng 1.

Độ tin cậy ngôn ngữ để lại chỗ để cải thiện. Các giá trị AUROC của độ tin cậy ngôn ngữ từ text-davinci, Llama 2 70b, và GPT-3.5 gần 50% (Bảng 1), đó là điểm số đạt được bằng cách đoán độ tin cậy ngẫu nhiên, cho thấy rằng độ tin cậy ngôn ngữ không phải là phương tiện đáng tin cậy để phân tách các ví dụ đúng và sai. Độ tin cậy ngôn ngữ của các mô hình mạnh nhất, Claude-v1.3 và GPT-4, tốt hơn và dẫn đến AUROC trong khoảng 60-65%, nhưng vẫn để lại rất nhiều chỗ để cải thiện. Các AUC của độ tin cậy ngôn ngữ gần với độ chính xác của chúng (Phụ lục A.2) (đó là điểm số đạt được bằng đường cơ sở đoán ngẫu nhiên) đối với text-davinci-003 (57,1% so với 57,7%), GPT-3.5 (58,1% so với 59,0%), và Llama 2 (58,8% so với 62,4%). Độ tin cậy ngôn ngữ cho các mô hình tốt nhất là hợp lý, nhưng vẫn để lại chỗ để cải thiện — GPT-4 có độ chính xác 73,5% và AUC 80,5%; và Claude-v1.3 có độ chính xác 65,5% và AUC 73,5%.

Độ tin cậy ngôn ngữ kém hơn xác suất mô hình. Các mô hình tốt nhất hiện tại (GPT-4 và Claude-v1.3) không cung cấp xác suất mô hình, nhưng chúng tôi so sánh chất lượng của xác suất mô hình và độ tin cậy ngôn ngữ cho text-davinci-003 và các mô hình Llama 2. Đối với những mô hình này, xác suất mô hình dẫn đến giá trị AUC và AUROC tốt hơn cho tất cả bộ dữ liệu của chúng tôi (Bảng 1). Đối với Llama 2, xác suất mô hình đạt AUC cao hơn 10,7% và AUROC cao hơn 19,0% so với độ tin cậy ngôn ngữ. Mô hình Chat (Llama 2 70B Chat) cho thấy xu hướng tương tự (Phụ lục A.5).

Độ tin cậy ngôn ngữ mạnh mẽ với các biến thể prompt. Chúng tôi xem xét độ tin cậy ngôn ngữ sử dụng 24 prompt khác biệt, bao gồm yêu cầu điểm số độ tin cậy hoặc xác suất số, yêu cầu mô hình phân loại độ tin cậy của nó thành 'không chắc chắn', 'chắc chắn', và 'rất chắc chắn', cho phép mô hình giải thích độ tin cậy với chuỗi suy nghĩ, hỏi mô hình về độ tin cậy trong câu hỏi tiếp theo, và thay đổi hướng dẫn prompt. Chúng tôi hiển thị kết quả cho prompt tốt nhất, vì có rất ít sự khác biệt trong hiệu suất qua các prompt — kết quả của chúng tôi cũng áp dụng cho các prompt khác. Mô tả chi tiết hơn về các prompt được điều tra và phương pháp chọn prompt tốt nhất có thể được tìm thấy trong Phụ lục A.4.

Độ tin cậy ngôn ngữ cải thiện với quy mô, nhưng không đủ. Chất lượng của độ tin cậy ngôn ngữ cải thiện với quy mô mô hình. Chúng ta thấy rằng GPT-4 và Claude-v1.3 có độ tin cậy ngôn ngữ tốt nhất, tiếp theo là các mô hình Llama 2 70B, GPT-3.5, và cuối cùng là text-davinci-003. Mặc dù độ tin cậy ngôn ngữ từ GPT-4 không tệ (65% AUROC trung bình), chúng kém hơn xác suất mô hình từ Llama 2 70b (74%) và thậm chí text-davinci-003 (72%). Lưu ý rằng điểm số AUC tăng với độ chính xác — GPT-4 Ngôn ngữ có AUC cao nhất vì GPT-4 có độ chính xác cao hơn nhiều so với Llama 2. Tiện ích tổng thể của một bộ phân loại có chọn lọc phụ thuộc vào cả độ chính xác và chất lượng độ tin cậy của nó, vì vậy trong phần tiếp theo chúng tôi xem xét các cách để cải thiện độ tin cậy của các mô hình hàng đầu của chúng tôi — GPT-4 và Claude-v1.3.

4 CÁC MÔ HÌNH ĐẠI DIỆN LÀ CÁC ƯỚC LƯỢNG ĐỘ TIN CẬY ĐÁNG TIN CẬY

Trong phần trước chúng tôi thấy rằng độ tin cậy ngôn ngữ để lại chỗ để cải thiện. Ở đây chúng tôi cho thấy rằng xác suất mô hình từ một mô hình 'đại diện' riêng biệt có thể đáng ngạc nhiên cung cấp ước lượng độ tin cậy tốt hơn cho một mô hình so với điểm số độ tin cậy ngôn ngữ của chính nó, mặc dù xác suất đến từ một mô hình khác.

4.1 KẾT QUẢ

Độ tin cậy mô hình đại diện vượt trội hơn độ tin cậy ngôn ngữ. AUC cải thiện cho tất cả các mô hình khi xác suất từ một mô hình đại diện được sử dụng, thay vì sử dụng độ tin cậy ngôn ngữ của chính mô hình. Hình 3 hiển thị bản đồ nhiệt của AUC cho các mô hình chính khác nhau (trên trục x) khi chúng tôi thay đổi mô hình đại diện (trên trục y). Chúng ta thấy rằng xác suất mô hình (bốn hàng dưới) dẫn đến AUC cao hơn (tối hơn) so với độ tin cậy ngôn ngữ (sáu hàng trên) ngay cả khi xác suất đến từ một mô hình khác. Ví dụ, sử dụng xác suất Llama 2 70B làm đại diện cải thiện AUC từ 80,5% lên 82,1% cho GPT-4, 73,5% lên 76,3% cho Claude-v1.3, và 59,0% lên 72,1% cho GPT-3.5, và AUROC cũng cho thấy sự tăng tương tự cho tất cả các mô hình (Bảng 2, Hình 4).

Các đại diện yếu cũng là ước lượng độ tin cậy tốt. Ngay cả việc sử dụng Llama 2 13B hoặc text-davinci-003 làm đại diện cũng dẫn đến hiệu suất tương đương hoặc tốt hơn so với việc sử dụng độ tin cậy ngôn ngữ của chính mô hình. Chúng tôi thấy điều này hấp dẫn vì những mô hình này nhỏ hơn nhiều và kém chính xác hơn, ví dụ, Llama 2 13B có độ chính xác trung bình 47,2% so với 65,5% cho Claude-v1.3 và 73,5% cho GPT-4.

Các phát hiện khác. Công trình gần đây cho thấy các mô hình chat được huấn luyện bằng học tăng cường từ phản hồi của con người (RLHF) có thể được hiệu chuẩn kém hơn so với các mô hình cơ sở. Trong Phụ lục A.7, chúng tôi so sánh xác suất mô hình chat và cơ sở làm độ tin cậy đại diện và thấy rằng Llama 2 70B cơ sở hoạt động hơi tốt hơn Llama 2 70B chat trong phân loại có chọn lọc với cả độ tin cậy ngôn ngữ và xác suất mô hình — nhưng cả hai mô hình hoạt động tương tự như đại diện. Như chúng ta có thể mong đợi, nói chung các mô hình tốt hơn (như Llama 2 70B) là các đại diện tốt hơn. Cuối cùng, chúng tôi thấy rằng độ tin cậy ngôn ngữ từ các mô hình mạnh hơn có thể cung cấp độ tin cậy đại diện tốt cho các mô hình yếu hơn — AUC của GPT-3.5 cải thiện 5,7% khi sử dụng độ tin cậy ngôn ngữ của GPT-4 thay vì của chính nó.

5 HỖN HỢP CÁC MÔ HÌNH CHO ƯỚC LƯỢNG ĐỘ TIN CẬY TỐT HƠN

Trong phần trước, chúng tôi đề xuất việc sử dụng các mô hình đại diện — sử dụng một mô hình chính để tạo ra câu trả lời và một mô hình riêng biệt, đại diện để ước lượng độ tin cậy của mô hình chính trong các câu trả lời — và thấy rằng các đại diện vượt trội hơn điểm số độ tin cậy ngôn ngữ được khai thác từ mô hình chính. Trong phần này, chúng tôi thấy rằng các tín hiệu từ độ tin cậy ngôn ngữ và xác suất đại diện là bổ sung — hai cái có thể được kết hợp để có được ước lượng độ tin cậy tiên tiến nhất cho tất cả các mô hình.

5.1 KẾT QUẢ

Hỗn hợp mô hình cung cấp độ tin cậy tốt nhất. Trộn độ tin cậy đại diện và ngôn ngữ (Thuật toán 1) dẫn đến ước lượng độ tin cậy tốt nhất cho tất cả các mô hình — AUC tăng từ 80,5% lên 83,4% cho GPT-4 và 73,5% lên 77,2% cho Claude-v1.3 (Bảng 2). α tối ưu (Thuật toán 1) cho hiệu suất trung bình tốt nhất qua các nhiệm vụ là 0,4 cho GPT-4 và 0,6 cho Claude-v1.3. AUROC cũng tăng cho những mô hình này, 5,3% cho GPT-4 và 5,0% cho Claude-v1.3 (Bảng 2). Chúng tôi cũng vẽ độ chính xác có chọn lọc so với độ che phủ trong Hình 4, nơi phương pháp hỗn hợp và đại diện nằm trên đường cong độ tin cậy ngôn ngữ.

Epsilon là tất cả những gì bạn cần. Chúng tôi cũng nghiên cứu một trường hợp đặc biệt của hỗn hợp được gọi là phá vỡ sự ràng buộc, nơi chúng tôi đặt α thành một giá trị nhỏ ε→0 (Thuật toán 1) — điều này chỉ đơn giản sử dụng mô hình đại diện để 'phá vỡ sự ràng buộc' và cung cấp thứ tự tương đối cho các ví dụ có cùng độ tin cậy ngôn ngữ. Thêm chỉ 0,1% xác suất của một mô hình đại diện vào độ tin cậy ngôn ngữ của một mô hình hoạt động tốt hơn so với việc sử dụng độ tin cậy ngôn ngữ hoặc xác suất đại diện một mình, và khớp chặt chẽ với hiệu suất của α tối ưu (Bảng 2). Đối với GPT-4, phá vỡ sự ràng buộc đạt 86% lợi ích AUC (so với độ tin cậy ngôn ngữ) của α tối ưu, và 87% lợi ích AUROC.

Trộn độ tin cậy đại diện và tự nhất quán dẫn đến lợi ích thêm. Công trình đương thời (Xiong et al., 2023) về khai thác độ tin cậy ngôn ngữ sử dụng tự nhất quán (SC) để lấy mẫu nhiều điểm số độ tin cậy ngôn ngữ cho mỗi câu trả lời và tổng hợp chúng thông qua kỹ thuật xử lý hậu kỳ. Để có lợi ích thêm, chúng tôi thử nghiệm với việc tận dụng những độ tin cậy ngôn ngữ dựa trên SC này cho GPT-4 — chúng tôi thay thế độ tin cậy ngôn ngữ c1 trong Thuật toán 1 bằng đầu ra của phương pháp tốt nhất của họ (tự nhất quán lai). Thuật toán 1 cập nhật dẫn đến ước lượng độ tin cậy tiên tiến nhất, cũng vượt trội hơn kỹ thuật tự nhất quán lai của họ (Bảng 3), với lợi ích AUC tổng thể 4,1% cho GPT-4 so với độ tin cậy ngôn ngữ vanilla, và lợi ích AUROC 9,1%.

Các phát hiện khác. Xác suất của các mô hình đại diện nhỏ hơn cũng có thể được kết hợp với độ tin cậy ngôn ngữ — kết hợp xác suất của Llama 2 13B với độ tin cậy ngôn ngữ của GPT-4 giữ lại 66% lợi ích AUC thấy từ việc kết hợp GPT-4 với Llama 2 70B. Kết hợp độ tin cậy ngôn ngữ của GPT-4 và Claude-v1.3 có thể tăng AUC của GPT-4 2,1% và AUROC 3%, cho thấy rằng độ tin cậy ngôn ngữ của các mô hình khác nhau có thể cung cấp ước lượng bổ sung về sự không chắc chắn. Ngoài ra, chúng tôi thấy rằng ngay cả việc kết hợp xác suất mô hình của hai mô hình khác nhau có thể cung cấp ước lượng độ tin cậy tốt hơn — kết hợp xác suất của Llama 2 với Llama 2 Chat cải thiện AUC của Llama 2 từ 73,1% lên 73,8% và AUROC từ 73,8% lên 74,5%. Trộn độ tin cậy từ hơn hai mô hình có thể dẫn đến cải thiện thêm.

6 PHÂN TÍCH

Tại sao Độ tin cậy Ngôn ngữ Vanilla Kém hơn Xác suất Mô hình? Trong Phần 3, chúng tôi cho thấy rằng độ tin cậy ngôn ngữ hoạt động kém hơn xác suất mô hình. Ở đây chúng tôi cung cấp một số trực giác cho hành vi này. Chúng tôi quan sát rằng phân phối xác suất mô hình khá đa dạng (1456 giá trị duy nhất cho Llama 2 70B qua 12 bộ dữ liệu), trong khi phân phối độ tin cậy ngôn ngữ khá tập trung (chỉ 8 giá trị duy nhất cho GPT-4 qua 12 bộ dữ liệu). Sự tập trung này có thể là do khối văn bản huấn luyện chứa tần suất cao hơn của các số xác suất "đẹp" như 90% hoặc 100% (Zhou et al., 2023). Tính lặp lại của độ tin cậy ngôn ngữ, so với xác suất mô hình, cản trở thứ tự độ tin cậy tương đối và hiệu suất AUC và AUROC tốt — GPT-4 lặp đi lặp lại tạo ra 0,9 cho 50% ví dụ qua 12 nhiệm vụ, vì vậy nó không thể phân tách chúng. Chúng tôi thử các phép loại bỏ đơn giản để tăng biến thể độ tin cậy ngôn ngữ, bằng cách tăng nhiệt độ tạo hoặc hướng dẫn mô hình 'Không sao nếu bạn ít chắc chắn hơn về câu trả lời của mình.', nhưng chúng không cải thiện AUC vì chúng giảm độ chính xác của mô hình.

Tại sao Ước lượng Độ tin cậy Đại diện Hoạt động? Trong Phần 4, chúng tôi chứng minh rằng các mô hình có thể nhận được ước lượng độ tin cậy chất lượng tốt từ các mô hình đại diện khác. Trong phần này, chúng tôi cung cấp một số trực giác cho kết quả của chúng tôi. Chúng tôi thấy rằng đối với một mô hình chính M, một mô hình S có xu hướng là một đại diện tốt hơn khi có sự tương quan cao hơn trong các câu hỏi được trả lời đúng bởi M và S. Các câu hỏi GPT-4 trả lời đúng tương quan nhiều hơn với những câu hỏi mà Llama 2 70B trả lời đúng (tương quan Pearson 0,39), hơn so với những câu hỏi mà Llama 2 13B trả lời đúng (tương quan 0,19) (Phụ lục A.8). Chúng tôi cũng vẽ các nhúng của các câu hỏi mà GPT-4 trả lời sai (chấm xanh) và các câu hỏi mà hai đại diện tiềm năng Llama 2 70B và Llama 2 13B trả lời sai (chấm xanh lá) (Hình 5). GPT-4 và Llama 2 70B có xu hướng mắc lỗi trên nhiều câu hỏi giống nhau hơn (nhiều chấm đen hơn trên biểu đồ bên trái). Chúng ta cũng thấy sự tương đồng không gian hơn trong các lỗi của GPT-4 và Llama 2 70B. Vì vậy các mô hình đại diện tốt hơn S và các mô hình chính tương ứng M có thể đấu tranh với các khái niệm liên quan về mặt ngữ nghĩa, khiến chúng có độ tin cậy thấp trên các loại câu hỏi tương tự. Một cách trực quan, xác suất của một đại diện như Llama 2 chuyển giao tốt cho một mô hình mạnh hơn như GPT-4 vì Llama 2 giỏi 'phát hiện' các câu hỏi khó, ngay cả khi nó không thể luôn trả lời chúng — chúng tôi lý luận rằng cả hai mô hình có phân phối xác suất entropy cao hơn trên các lựa chọn câu trả lời cho các câu hỏi khó hơn, và phân phối xác suất nhọn hơn cho các câu hỏi dễ hơn.

Tại sao Phá vỡ Sự ràng buộc Đủ? Như đã đề cập, độ tin cậy ngôn ngữ có xu hướng lặp lại và tập trung ở chỉ một vài giá trị (ví dụ, 0,9), hạn chế khả năng phân tách câu trả lời đúng và sai của chúng. Vì xác suất của mô hình đại diện cho mỗi ví dụ gần như duy nhất, việc kết hợp chỉ một phần nhỏ của chúng với điểm số độ tin cậy ngôn ngữ (Phần 5.1) có thể cho phép các câu trả lời trước đây có cùng điểm số độ tin cậy ngôn ngữ bây giờ có thể phân tách được thông qua các điểm số độ tin cậy tổng hợp khác nhau. Điều này có nghĩa là trong các trường hợp điểm số độ tin cậy ngôn ngữ giống hệt nhau, chúng tôi quay lại với xác suất của mô hình đại diện để cung cấp thứ tự các ví dụ dựa trên độ tin cậy.

7 CÔNG TRÌNH LIÊN QUAN

Ước lượng Độ tin cậy cho LLM. Ước lượng độ tin cậy cho LLM đã được nghiên cứu trong một số công trình liên quan. Kadavath et al. (2022) cho thấy rằng xác suất mô hình của Claude được hiệu chuẩn tốt trên các câu hỏi trắc nghiệm và Đúng/Sai. Zhou et al. (2023) nghiên cứu tác động của việc đưa các biểu hiện không chắc chắn vào prompt, trên độ chính xác của mô hình. Công trình của chúng tôi khác với những công trình này vì chúng tôi tập trung vào khai thác độ tin cậy cho các mô hình không cung cấp log xác suất. Công trình đương thời (Xiong et al., 2023) nghiên cứu hiệu chuẩn và phân loại có chọn lọc của điểm số độ tin cậy ngôn ngữ được tạo bởi LLM. Mặc dù công trình này cũng khai thác độ tin cậy được nhắc, họ tập trung vào các phương pháp dựa trên tự nhất quán (SC) đắt đỏ vì chúng yêu cầu nhắc GPT-4 nhiều lần. Các phương pháp Đại diện và Hỗn hợp mô hình được đề xuất của chúng tôi ít đắt đỏ hơn, vì xác suất mô hình từ các mô hình nhỏ hơn (Llama 2) được sử dụng để cải thiện ước lượng độ tin cậy của các mô hình lớn hơn (GPT-4). Chúng tôi cũng cho thấy cải thiện hiệu suất so với phương pháp tốt nhất của họ. (Lin et al., 2022) xem xét tinh chỉnh các mô hình ngôn ngữ để cải thiện ước lượng độ tin cậy, điều mà chúng tôi không có quyền truy cập.

Phân loại Có chọn lọc và Phát hiện OOD. Bài báo của chúng tôi tập trung vào phân loại có chọn lọc, một vấn đề cổ điển trong học máy (El-Yaniv & Wiener, 2010; Khani et al., 2016; Feng et al., 2019; Jones et al., 2021) và thống kê (Chow, 1970; Hellman & Raviv, 1970). Một vấn đề liên quan là phát hiện ngoài phân phối (Pimentel et al., 2014; Liang et al., 2018; Ovadia et al., 2019), nơi mục tiêu là phát hiện các ví dụ rất khác với huấn luyện (nơi mô hình có thể mắc lỗi). Công trình trước đây sử dụng bên trong của các mô hình — đầu ra xác suất (Hendrycks & Gimpel, 2017), biểu diễn (Lee et al., 2018) của các mô hình, hoặc thậm chí cập nhật quy trình huấn luyện (Bartlett & Wegkamp, 2008; Mozannar & Sontag, 2020) — mà các LLM tiên tiến hiện tại không cung cấp quyền truy cập.

Hiệu chuẩn. Ý tưởng chung về ước lượng độ tin cậy cũng được nghiên cứu trong hiệu chuẩn (Murphy & Winkler, 1977; DeGroot & Fienberg, 1983; Naeini et al., 2014; Guo et al., 2017). Mặc dù liên quan, trọng tâm khác nhau — một mô hình xuất ra độ chính xác của nó trên mọi ví dụ có lỗi hiệu chuẩn 0 (ECE), nhưng không thể phân tách các ví dụ đúng và sai (Kuleshov & Liang, 2015).

8 KẾT LUẬN VÀ CÔNG TRÌNH TƯƠNG LAI

Công trình của chúng tôi nhằm giải quyết thách thức mở về khai thác ước lượng độ tin cậy tốt từ các LLM tiên tiến như GPT-4 và Claude-v1.3, những mô hình không cung cấp quyền truy cập vào xác suất nội bộ của chúng. Kết quả của chúng tôi làm nổi bật tầm quan trọng của việc phát hành xác suất mô hình, vì độ tin cậy ngôn ngữ một mình thường không đủ biểu cảm để cung cấp ước lượng độ tin cậy chất lượng cao. Chúng tôi chứng minh rằng xác suất từ các mô hình hộp trắng, đại diện yếu hơn có thể ước lượng hiệu quả độ tin cậy nội bộ của các mô hình hộp đen mạnh hơn như GPT-4, vượt trội hơn độ tin cậy ngôn ngữ, và cung cấp một số trực giác về lý do tại sao độ tin cậy có thể chuyển giao giữa các mô hình. Chúng tôi hy vọng rằng những phát hiện của chúng tôi có thể truyền cảm hứng cho công trình tương lai về hiểu biết khả năng chuyển giao của xác suất và biểu diễn mô hình và về tận dụng khả năng chuyển giao này để sử dụng các mô hình hộp trắng để hiểu các mô hình hộp đen. Thú vị thay, chúng tôi cũng thấy rằng các tín hiệu độ tin cậy từ các mô hình khác nhau là bổ sung và có thể được kết hợp cho ước lượng độ tin cậy đáng tin cậy hơn nữa. Các phương pháp tương lai có thể xây dựng thêm trên kết quả này để phát triển các phương pháp phức tạp hơn về kết hợp tín hiệu độ tin cậy.
