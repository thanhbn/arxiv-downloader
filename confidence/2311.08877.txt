# 2311.08877.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/confidence/2311.08877.pdf
# File size: 1821532 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preprint
LLAMAS KNOW WHAT GPT SDON’TSHOW :
SURROGATE MODELS FOR CONFIDENCE ESTIMATION
Vaishnavi Shrivastava, Percy Liang, Ananya Kumar
Stanford University
{vshrivas, pliang, ananya}@cs.stanford.edu
ABSTRACT
To maintain user trust, large language models (LLMs) should signal low confi-
dence on examples where they are incorrect, instead of misleading the user. The
standard approach of estimating confidence is to use the softmax probabilities
of these models, but as of November 2023, state-of-the-art LLMs such as GPT-
4 and Claude-v1.3 do not provide access to these probabilities. We first study
eliciting confidence linguistically — asking an LLM for its confidence in its an-
swer — which performs reasonably (80.5% AUC on GPT-4 averaged across 12
question-answering datasets — 7% above a random baseline) but leaves room for
improvement. We then explore using a surrogate confidence model — using a
model where we do have probabilities to evaluate the original model’s confidence
in a given question. Surprisingly, even though these probabilities come from a
different and often weaker model, this method leads to higher AUC than linguistic
confidences on 9 out of 12 datasets. Our best method composing linguistic con-
fidences and surrogate model probabilities gives state-of-the-art confidence esti-
mates on all 12 datasets (84.6% average AUC on GPT-4).
1 I NTRODUCTION
As large language models (LLMs) are increasingly deployed, it is important that they signal low con-
fidence on examples where they are likely to make mistakes. This problem is called selective clas-
sification (or classification with a reject option) and is widely studied in machine learning (Cordella
et al., 1995; Geifman & El-Yaniv, 2017; Feng et al., 2019; Jones et al., 2021), learning theory (El-
Yaniv & Wiener, 2010; Bartlett & Wegkamp, 2008), and natural language processing (Kamath et al.,
2020; Liang et al., 2022; Xiong et al., 2023). Traditional approaches leverage the model’s softmax
probabilities (Hendrycks & Gimpel, 2017; Jones et al., 2021; Liang et al., 2022) or the model’s
representations (Lee et al., 2018). This paper’s goal is to produce good confidence estimates for
state-of-the-art LLMs , which do not provide model probabilities or representations (such as GPT-4
and Claude-v1.3).
We first examine a natural idea of eliciting linguistic confidence scores (Tian et al., 2023; Lin et al.,
2022; Xiong et al., 2023) — prompting the LLM to assess its confidence in its answer (Figure 1,
GPT-4 Linguistic). We find that linguistic confidences work reasonably well for state-of-the-art
models, and much better than a random guessing baseline, but still leave room for improvement
(Section 3). Averaged across the datasets, GPT-4 achieves a selective classification AUC of 80.5%,
which is 7% above a random guessing baseline. Our results hold across 12 standard datasets (8
MMLU datasets, TruthfulQA, CommonsenseQA, OpenbookQA, and MedQA), 5 models (GPT-4,
Claude-v1.3, GPT-3.5, Llama 2, and text-davinci-003), and 24 different prompt formats (e.g., chain-
of-thought, different instructions, fake few-shot prompts). However, linguistic confidences perform
much worse than using model probabilities when these probabilities are available (for less accurate
models). For example, on Llama 2 linguistic confidences achieve an average AUC 10.7% lower than
model probabilities, suggesting scope for further refinement in these confidence assessments.
Consequently, we propose a surrogate model approach of taking the answer from GPT-4 or Claude-
v1.3, but the confidence from a different model such as Llama 2 (Figure 1, Surrogate), where softmax
probabilities are available, as a confidence estimate for the original model’s answer (Section 4). Sur-
rogate confidence modeling improves the average selective classification AUC for GPT-4 to 82.1%.
Even using a weaker or much smaller surrogate model like text-davinci-003 or Llama 2-13B leads to
1arXiv:2311.08877v1  [cs.CL]  15 Nov 2023

--- PAGE 2 ---
Preprint
Figure 1: Our goal is to provide good confidence estimates for state-of-the-art LLMs like GPT-4 and
Claude-v1.3 which currently do not give access to their internal probabilities. One natural approach
(GPT-4 Linguistic) is to prompt the model asking for its confidence. Interestingly, we find that taking
the answer from GPT-4, but the internal probability from a different surrogate model (e.g., an open
model such as Llama 2) gives even better results (0.82 AUC). Mixing GPT-4’s linguistic confidences
with the surrogate model probabilities gives further gains (0.83 AUC). Our AUC numbers are better
than concurrent work (Xiong et al., 2023), but combining these approaches leads to the best results
(Mixture++; 0.85 AUC). Our findings also hold for Claude-v1.3 and GPT-3.5 (Section 4 and 5).
comparable or better AUCs for stronger models such as GPT-4, Claude-v1.3, and GPT-3.5. Intrigu-
ingly, confidence scores can transfer between models, even if the model generating the confidence
score is different (or much worse). In Section 4, we provide some analysis and intuitions for this
behavior.
We find that linguistic confidence scores and surrogate model probabilities are complementary: com-
bining these scores leads to further gains (Figure 1, Mixture). For example, this mixture method in-
creases the selective classification AUC of GPT-4 to 83.4%. The mixture method also outperforms
concurrent work (Xiong et al., 2023) on self-consistency (AUC: 82.8%), which is more expensive
(involves sampling GPT-4 five times per input) and involves post-processing. Combining our method
with self-consistency-based confidence scores leads to the best results: average AUC 84.6% .
Our analysis suggests that linguistic confidence scores are limited because they are very coarse-
grained — for example, GPT-4 outputs the exact same confidence (0.9) on 50% of examples, which
constrains its ability to separate correct and incorrect answers. Surrogate model probabilities work
well even on a different model, because the examples that are challenging for one model transfer
over to a different model. Finally, mixing in just a small fraction of surrogate model probabilities
allows answers which previously had the same linguistic confidence to be separable through different
composite confidence scores, boosting the overall performance with minimal interventions.
2 S ETUP
Our goal is selective classification: outputting confidence scores that are higher on inputs where
the model is correct, than inputs where the model is incorrect (El-Yaniv & Wiener, 2010; Geifman
& El-Yaniv, 2017). We focus on state-of-the-art language models such as GPT-4 and Claude-v1.3,
which currently do not expose probabilities computed in their softmax output layer.
Task. Given a text input x, a model outputs a (possibly stochastic) answer y(x). Let R(x, y) = 1
if an answer yis correct for input x, and 0otherwise. Our goal is to output a confidence score
C(x)∈[0,1]. Good confidence scores are essential in real world machine learning systems: for
2

--- PAGE 3 ---
Preprint
inputs when C(x)is lower, we can defer to a human expert or alert the user, instead of misleading
the user with an incorrect answer.
Metrics. A popular metric for selective classification is the AUC (area under the coverage-accuracy
curve) (El-Yaniv & Wiener, 2010; Liang et al., 2022), which examines how accurate the model is
if allowed to abstain (say "I don’t know") on some examples. Let A(c)be the selective accuracy
at coverage c: the accuracy if the model only makes a prediction on the cproportion of data with
highest confidence scores. To enable tie-breaking to make different predictions for examples with
the same confidence score, we add a small amount of Gaussian noise to each confidence score
N(0, ϵ), ϵ→0. The AUC is the average selective accuracy A(c)over all c:
AUC(C, y) = lim
ϵ→0Z1
0E[A(c)]dc (2.1)
A random baseline (outputting uniform random probabilities for each input) achieves AUC (C, y) =
accuracy, so a model with good confidence scores should achieve a higher AUC than accuracy. Note
that adding the noise N(0, ϵ)is critical because linguistic confidences for different examples are
often identical — without the noise we would substantially underestimate the AUC of the models
(see Appendix A.3 for more details).
We also examine the AUROC , a standard metric (Hendrycks & Gimpel, 2017; Xiong et al., 2023)
used to examine how well confidence scores can distinguish between correct and incorrect examples.
We label an example ‘Positive’ if the model gets it correct and ‘Negative’ otherwise, and plot the true
positive rate against the false positive rate at different classification thresholds — the AUROC is the
area under this curve (See Appendix A.3 for more details). Outputting random confidence scores
gets an AUROC of 0.5, so a model with good confidence scores should achieve AUROC above 0.5.
We also report ECE (expected calibration error) numbers in Appendix A.6. ECE examines if a
model’s confidence aligns with its accuracy, but does not indicate the model’s ability to distinguish
between correct and incorrect examples, so we focus on the AUC and AUROC metrics.1
Datasets. We study model performance and confidence on twelve standard question answering
datasets: TruthfulQA (TQA) (Lin et al., 2021), CommonsenseQA (CSQA) (Talmor et al., 2019),
OpenbookQA (OBQA) (Mihaylov et al., 2018), MedQA (Jin et al., 2021), and 8 MMLU (Hendrycks
et al., 2021) datasets - professional law (Law), business ethics (Ethics), conceptual physics (Physics),
econometrics (Econ), abstract algebra (Algebra), college chemistry (Chem), computer security (Se-
curity), and US Foreign Policy (Policy). These datasets span several diverse categories including
math reasoning, scientific knowledge, computer science, social science, and commonsense reason-
ing. We sample 250 questions from the test split of each dataset to report results on (if the test set is
smaller, we use the full test set). See Appendix A.1 for more details.
Models. We study state-of-the-art language models, most of which do not provide access to internal
probabilities as of the writing of this paper — GPT-4 (OpenAI, 2023a), Claude-v1.3 , and GPT-3.5-
Turbo (OpenAI, 2022) (June 13th, 2023, snapshot). We also study a few recent models which do
provide model probabilities for systematic comparisons — Llama 2 andLlama 2 Chat (70B and
13B sizes) (Touvron et al., 2023) and text-davinci-003 OpenAI (2023b). If Llama 2 is mentioned
in the text without further identifiers, we refer to the Llama 2 70B base model.
2.1 C ONFIDENCE ELICITATION METHODS
Linguistic Confidences. For each question, we zero-shot prompt models with an instruction to out-
put a valid answer and a confidence assessment of that answer, sampling the answer and confidence
together in a single generation. We generate greedily with temperature T= 0, and define these
confidence estimates generated by the model to be linguistic confidences. Since there can be many
ways of eliciting linguistic confidences, we experiment with 24 different prompts across various cat-
egories (chain-of-thought, different instructions, fake few shot examples). We find the results to be
consistent across prompts, so we report results on our best prompt (see Figure 2 for an example in-
struction of linguistic confidence elicitation). Section 3 assesses the quality of linguistic confidences
and signals a need for better confidence estimation methods.
1Intuitively, calibration requires that if we output a 0.6confidence on 100examples, then we should get
0.6·100 = 60 of them correct. For a classifier with accuracy A, one (degenerate) way to have perfect
calibration (best possible ECE) is to output confidence C(x) =Afor every example x.
3

--- PAGE 4 ---
Preprint
Model Probabilities. Models such as Llama 2 and text-davinci-003 provide token-level probabili-
ties for text. We let the confidence score be the probability of the generated answer choice.
Figure 2: Linguistic Confidence Prompt Instruction for
the best linguistic confidence prompt (see exact prompt
in Appendix A.4).Surrogate models for confidences.
Since models such as GPT-4 do not
give a confidence estimate, we propose
using a surrogate model (e.g., Llama 2)
to provide confidence estimates. For-
mally, given an input xwe output
y(x) =ygpt-4(x)(GPT-4’s answer) and
C(x) =CLlama 2 (x)(Llama 2’s confi-
dence in its own answer). Even though
these confidence scores come from a
different model, Section 4 shows that
the surrogate confidence method out-
performs linguistic confidence scores.
Mixture of models. We also propose a mixture of models method where we combine the linguistic
confidence from the main model and the surrogate model’s confidence score: given input xwe
output (1−α)CM(x) +αCS(x)where Mis the main model and Sis the surrogate model.
We use Llama 2 70B as the surrogate model for all main models since it performs the best. We
optimize αto minimize AUC, sweeping over values from 0 to 1. Interestingly, in Section 5, we
show that even α= 0.001works well.
3 L INGUISTIC CONFIDENCES :ASKING THE MODEL FOR ITS CONFIDENCE
As of November 2023, state-of-the-art language models such as GPT-4 and Claude-v1.3 do not give
access to internal model probabilities. In this section, we examine linguistically eliciting confidence:
prompt models to assign their answers a confidence score between 0 and 1. We find that these
linguistic confidences leave a lot of room for improvement (around 50-65% AUROC, compared
to 50% for a random guessing baseline). These linguistic confidences are also much worse than
internal model probabilities when available (for weaker models such as text-davinci-003 and Llama
2). We show AUC and AUROC results on all datasets and models in Table 1.
Algorithm 1: Mixture of Models Confidence
Data: A question x
Result: A prediction by, a confidence score c
by,c1=MainModel (x) ;
c2=SurrogateModel (x) ;
c= (1−α)c1+αc2;Linguistic confidences leave room for im-
provement. The AUROC values of linguistic
confidences from text-davinci, Llama 2 70b, and
GPT-3.5 are close to 50% (Table 1), which is the
score achieved by guessing a random confidence,
indicating that linguistic confidences are not a re-
liable means of separating correct and incorrect
examples. The linguistic confidences of the strongest models, Claude-v1.3 and GPT-4, are better
and result in AUROCs in the 60-65% range, but still leave a lot of room for improvement. The
AUCs of linguistic confidences are close to their accuracy (Appendix A.2) (which is the score
achieved by a random guessing baseline) for text-davinci-003 (57.1% vs 57.7%), GPT-3.5 (58.1%
vs 59.0%), and Llama 2 (58.8% vs 62.4%). Linguistic confidences for the best models are reason-
able, but still leave room for improvement — GPT-4 has an accuracy of 73.5% and AUC of 80.5%;
and Claude-v1.3 has an accuracy of 65.5% and AUC of 73.5%.
Linguistic confidences are worse than model probabilities. The best current models (GPT-4 and
Claude-v1.3) do not provide model probabilities, but we compare the quality of model probabilities
and linguistic confidences for text-davinci-003 and the Llama 2 models. For these models, the model
probabilities result in better AUC and AUROC values for all of our datasets (Table 1). For Llama 2,
the model probabilities achieve a 10.7% higher AUC and 19.0% higher AUROC than the linguistic
confidences. The Chat model (Llama 2 70B Chat) shows similar trends (Appendix A.5).
Linguistic confidences are robust to prompt variations. We examine linguistic confidences us-
ing 24 distinct prompts, including asking for numerical confidence or probability scores, asking the
model to categorize its confidence into ‘not sure’, ‘sure’, and ‘very sure’, allowing the model to ex-
plain confidences with chain-of-thought, asking the model for its confidence in a follow-up question,
and varying the prompt instructions. We show results for the best prompt, as there was very little
4

--- PAGE 5 ---
Preprint
Confidence Type TQA Medqa CSQA OBQA Law Ethics Physics
AUCText-davinci Linguistic 0.523 0.504 0.718 0.775 0.532 0.590 0.579
Text-davinci Prob 0.607 0.656 0.861 0.929 0.714 0.783 0.697
Llama 2 Linguistic 0.600 0.616 0.693 0.802 0.605 0.707 0.638
Llama 2 Prob 0.711 0.735 0.804 0.923 0.749 0.834 0.763
GPT-3.5 Linguistic 0.620 0.536 0.693 0.776 0.508 0.674 0.526
Claude-v1.3 Linguistic 0.741 0.718 0.807 0.879 0.669 0.894 0.736
GPT-4 Linguistic 0.889 0.841 0.802 0.960 0.732 0.869 0.819
AUROCText-davinci Linguistic 0.525 0.500 0.503 0.509 0.500 0.500 0.500
Text-davinci Prob 0.718 0.696 0.806 0.840 0.715 0.758 0.637
Llama 2 Linguistic 0.618 0.541 0.555 0.484 0.517 0.602 0.593
Llama 2 Prob 0.745 0.722 0.731 0.777 0.733 0.868 0.732
GPT-3.5 Linguistic 0.535 0.500 0.526 0.518 0.508 0.509 0.504
Claude-v1.3 Linguistic 0.701 0.586 0.639 0.647 0.586 0.760 0.652
GPT-4 Linguistic 0.665 0.716 0.551 0.656 0.591 0.720 0.522
Confidence Type Econ Algebra Chem Security Policy Avg
AUCText-davinci Linguistic 0.412 0.300 0.440 0.690 0.856 0.577
Text-davinci Prob 0.431 0.338 0.644 0.891 0.939 0.707
Llama 2 Linguistic 0.415 0.189 0.474 0.817 0.930 0.624
Llama 2 Prob 0.498 0.263 0.647 0.866 0.981 0.731
GPT-3.5 Linguistic 0.430 0.319 0.465 0.724 0.806 0.590
Claude-v1.3 Linguistic 0.640 0.333 0.653 0.812 0.934 0.735
GPT-4 Linguistic 0.643 0.551 0.683 0.903 0.965 0.805
AUROCText-davinci Linguistic 0.500 0.500 0.500 0.500 0.506 0.504
Text-davinci Prob 0.549 0.532 0.695 0.858 0.795 0.717
Llama 2 Linguistic 0.533 0.424 0.520 0.613 0.576 0.548
Llama 2 Prob 0.622 0.546 0.732 0.775 0.871 0.738
GPT-3.5 Linguistic 0.518 0.522 0.505 0.519 0.519 0.515
Claude-v1.3 Linguistic 0.573 0.543 0.708 0.687 0.645 0.644
GPT-4 Linguistic 0.551 0.599 0.721 0.750 0.753 0.650
Table 1: AUC and AUROC - Linguistic Confidences vs Model Probabilities We compare the
AUC and AUROC values for linguistic confidences and model probabilities in weaker models (text-
davinci-003 and Llama 2 70B), and find that model probabilities consistently outperform linguis-
tic confidences. For closed source models (which don’t provide model probabilities), we see that
Claude-v1.3 and GPT-4 provide the best linguistic confidences in both AUC and AUROC.
difference in performance across prompts — our results hold for other prompts as well. A more
detailed description of the prompts investigated and the method for selecting the best prompt can be
found in Appendix A.4.
Linguistic confidences improve with scale, but not enough. The quality of linguistic confidences
improves with model scale. We see that GPT-4 and Claude-v1.3 have the best linguistic confidences,
followed by the Llama 2 70B models, GPT-3.5, and finally text-davinci-003. While the linguistic
confidences from GPT-4 are not bad (65% average AUROC), they are worse than model probabilities
from Llama 2 70b (74%) and even text-davinci-003 (72%). Note that AUC scores increase with
accuracy — GPT-4 Linguistic has the highest AUC because GPT-4 has much higher accuracy than
Llama 2. The overall utility of a selective classifier depends on both its accuracy and confidence
quality, so in the next section we examine ways to improve the confidences of our best-in-class
models — GPT-4 and Claude-v1.3.
5

--- PAGE 6 ---
Preprint
Figure 3: AUCs for Different Surrogate Models. We plot the AUC as we vary the main model (on
thex-axis) and the surrogate model (on the y-axis). Using surrogate model probabilities as confi-
dence estimates improves AUCs for all models over their own linguistic confidences—the bottom 4
rows (surrogate probabilities) are darker than the top 6 rows (linguistic confidences). Even model
probabilities from a smaller Llama 2 13B model lead to comparable or better AUCs for all models.
4 S URROGATE MODELS ARE RELIABLE CONFIDENCE ESTIMATORS
In the previous section we found that linguistic confidences leave room for improvement. Here we
show that model probabilities from a separate ‘surrogate’ model can surprisingly provide better con-
fidence estimates for a model than its own linguistic confidence scores, even though the probabilities
come from a different model.
4.1 R ESULTS
Surrogate model confidences outperform linguistic confidences. AUC improves for all models
when probabilities from a surrogate model are used, as opposed to using the model’s own linguistic
confidences. Figure 3 shows a heatmap of the AUC for different main models (on the x-axis) as we
vary the surrogate model (on the y-axis). We see that model probabilities (bottom four rows) lead
to higher AUC (are darker) than linguistic confidences (top six rows) even when the probabilities
come from a different model. For example, using Llama 2 70B probabilities as a surrogate improves
AUC from 80.5% to 82.1% for GPT-4, 73.5% to 76.3% for Claude-v1.3, and 59.0% to 72.1% for
GPT-3.5, and AUROC also shows similar increases for all models (Table 2, Figure 4).
Weak surrogates are also good confidence estimators. Even using Llama 2 13B or text-davinci-
003 as a surrogate leads to comparable or better performance than using a model’s own linguistic
confidences. We found this intriguing because these models are much smaller and less accurate, e.g.,
Llama 2 13B has an average accuracy of 47.2% vs. 65.5% for Claude-v1.3 and 73.5% for GPT-4.
Other findings. Recent work suggests chat models trained using reinforcement learning from hu-
man feedback (RLHF) might be less calibrated than base models. In Appendix A.7, we compare
chat and base model probabilities as surrogate confidences and find that Llama 2 70B base slightly
outperforms Llama 2 70B chat in selective classification with both linguistic confidences and model
probabilities — but both models perform similarly as surrogates. As we might expect, in general bet-
ter models (such as Llama 2 70B) are better surrogates. Finally, we find that linguistic confidences
from stronger models can provide good surrogate confidences for weaker models — the AUC of
GPT-3.5 improves by 5.7% when using GPT-4’s linguistic confidences instead of its own.
6

--- PAGE 7 ---
Preprint
Text-davinci GPT-3.5 Llama 2 Claude-v1.3 GPT-4
AUCLing. Conf. 0.577 0.590 0.624 0.735 0.805
Surrogate†0.707 0.719 0.731 0.763 0.821
Tiebreak†0.711 0.719 0.715 0.764 0.830
Mixture of Models†0.711 0.722 0.731 0.772 0.834
AUROCLing. Conf. 0.504 0.514 0.548 0.637 0.646
Surrogate†0.717 0.708 0.738 0.671 0.657
Tiebreak†0.718 0.708 0.699 0.683 0.692
Mixture of Models†0.718 0.709 0.737 0.687 0.699
Table 2: AUC and AUROC of Surrogate and Mixture of Model Methods. We compare the
performance of our proposed methods†with the baseline linguistic confidence method (gray). For
both AUC and AUROC, our proposed methods outperform linguistic confidences on all models.
Mixture of models improves the AUC of GPT-4 by 3% and AUROC by 5%.
5 M IXTURES OF MODELS FOR BETTER CONFIDENCE ESTIMATES
In the previous section, we proposed the use of surrogate models — using a main model to produce
answers and a separate, surrogate to estimate the main model’s confidence in the answers — and
found surrogates to outperform linguistic confidence scores elicited from the main model. In this
section, we find that the signals from linguistic confidences and surrogate probabilities are comple-
mentary — the two can be composed to get state of the art confidence estimates for all models.
5.1 R ESULTS
Mixtures of models provide best confidences. Mixing surrogate and linguistic confidences (Al-
gorithm 1) leads to the best confidence estimates for all models — AUCs increase from 80.5% to
83.4% for GPT-4 and 73.5% to 77.2% for Claude-v1.3 (Table 2). The optimal α(Algorithm 1)
for best average performance across tasks is 0.4for GPT-4 and 0.6for Claude-v1.3. AUROCs also
increase for these models, by 5.3% for GPT-4 and 5.0% for Claude-v1.3 (Table 2). We also plot the
selective accuracy against coverage in Figure 4, where the mixture and surrogate method lie above
the linguistic confidences curve.
Epsilon is all you need. We also study a special case of mixtures called tiebreaking, where we set
αto a small value ϵ→0(Algorithm 1) — this simply uses the surrogate model to ‘break ties’
and provide relative ordering for examples with the same linguistic confidence. Adding only 0.1%
of a surrogate model’s probabilities to a model’s linguistic confidences performs better than using
either the linguistic confidences or surrogate probabilities alone, and closely matches performance
of the optimal α(Table 2). For GPT-4, tiebreaking achieves 86% of the AUC gains (over linguistic
confidences) of the optimal α, and 87% of the AUROC gains.
Mixing surrogate and self-consistency confidences leads to further gains. Concurrent
work (Xiong et al., 2023) on eliciting linguistic confidences uses self-consistency (SC) to sample
multiple linguistic confidence scores for each answer and aggregates them through a post processing
technique. For further gains, we experiment with leveraging these SC-based linguistic confidences
for GPT-4 — we replace linguistic confidences c1in Algorithm 1 with the outputs of their best
method (hybrid self-consistency). The updated Algorithm 1 leads to state-of-the-art confidence es-
timates, also outperforming their hybrid self-consistency technique (Table 3), with an overall 4.1%
gain in AUC for GPT-4 over vanilla linguistic confidences, and a 9.1% gain in AUROC.
Other findings. Probabilities of smaller surrogate models can also be composed with linguistic
confidences — composing Llama 2 13B’s probabilities with GPT-4’s linguistic confidences retains
66% of the AUC gains seen from composing GPT-4 with Llama 2 70B. Composing GPT-4 and
Claude-v1.3’s linguistic confidences can boost GPT-4’s AUC by 2.1% and AUROC by 3%, indicat-
ing that linguistic confidences of different models can provide complementary estimates of uncer-
tainty. Additionally, we find that even composing the model probabilities of two different models
can provide better confidence estimates — composing Llama 2’s probabilities with those of Llama
2 Chat improves Llama 2’s AUC from 73.1% to 73.8% and AUROC from 73.8% to 74.5%. Mixing
confidences from more than two models could potentially lead to further improvements.
7

--- PAGE 8 ---
Preprint
Method TQA Medqa CSQA OBQA Law Ethics Physics
AUCLing. Conf. 0.889 0.841 0.802 0.960 0.732 0.869 0.819
SC Ling. Conf. 0.903 0.887 0.841 0.978 0.729 0.902 0.846
Surrogate†0.866 0.844 0.849 0.965 0.762 0.849 0.891
Tiebreak†0.902 0.871 0.833 0.967 0.768 0.889 0.861
Mixture†0.895 0.864 0.849 0.969 0.780 0.882 0.886
SC Mixture†0.921 0.873 0.877 0.979 0.757 0.894 0.881
AUROCLing. Conf. 0.665 0.716 0.551 0.656 0.591 0.720 0.522
SC Ling. Conf. 0.698 0.767 0.625 0.833 0.619 0.817 0.592
Surrogate†0.543 0.666 0.656 0.683 0.619 0.617 0.648
Tiebreak†0.671 0.750 0.611 0.716 0.628 0.740 0.589
Mixture†0.642 0.731 0.646 0.731 0.655 0.711 0.648
SC Mixture†0.702 0.747 0.679 0.838 0.655 0.783 0.663
Method Econ Algebra Chem Security Policy Avg
AUCLing. Conf. 0.643 0.551 0.683 0.903 0.965 0.805
SC Ling. Conf. 0.663 0.584 0.726 0.915 0.965 0.828
Surrogate†0.667 0.572 0.724 0.888 0.971 0.821
Tiebreak†0.654 0.580 0.746 0.910 0.974 0.830
Mixture†0.664 0.581 0.749 0.908 0.976 0.834
SC Mixture†0.662 0.645 0.763 0.926 0.973 0.846
AUROCLing. Conf. 0.551 0.599 0.721 0.750 0.753 0.650
SC Ling. Conf. 0.622 0.682 0.818 0.798 0.755 0.719
Surrogate†0.578 0.621 0.706 0.779 0.764 0.657
Tiebreak†0.569 0.648 0.760 0.815 0.805 0.692
Mixture†0.578 0.648 0.759 0.814 0.822 0.699
SC Mixture†0.595 0.763 0.819 0.839 0.810 0.741
Table 3: AUC and AUROC of All Confidence Methods for GPT-4. Our proposed surrogate model
method outperforms linguistic confidences on 9/12 datasets on AUC. Mixing surrogate probabili-
ties and linguistic confidences outperforms vanilla linguistic confidences on AUC for all 12 datasets.
The mixture of surrogate probabilities also outperforms hybrid self-consistency confidences, the best
method in Xiong et al. (2023), on average (AUC 83.4% vs 82.8%. Mixing surrogate probabilities
with self-consistency linguistic confidences leads to the best confidence estimates overall, outper-
forming all methods with an average 84.6% AUC and 74.1% AUROC, which is a gain of 4.1% and
9.1% respectively over vanilla linguistic confidences.
6 A NALYSIS
Why Are Vanilla Linguistic Confidences Worse Than Model Probabilities? In Section 3, we
showed that linguistic confidences underperformed model probabilities. Here we provide some in-
tuitions for this behavior. We observe that the distribution of model probabilities is quite varied
(1456 unique values for Llama 2 70B across 12 datasets), while the distribution of linguistic confi-
dences is quite clustered (only 8 unique values for GPT-4 across 12 datasets). This clustering may
be because training corpora contain higher frequencies of “nice” probability numbers such as 90%
or 100% (Zhou et al., 2023). The repetitiveness of linguistic confidences, compared to model prob-
abilities, hinders relative confidence ordering and good AUC and AUROC performance — GPT-4
repetitively generates 0.9 for 50% of examples across 12 tasks, so it cannot separate them. We tried
simple ablations to increase linguistic confidence variation, by increasing the temperature of gener-
ations or instructing the model ‘It’s ok to be less sure of your answers.’, but they did not improve
AUC because they reduced model accuracy.
Why Does Surrogate Confidence Estimation Work? In Section 4, we demonstrate that models
can receive good quality confidence estimates from other surrogate models. In this section, we pro-
vide some intuitions for our results. We find that for a main model M, a model Stends to be a
better surrogate when there is a higher correlation in the questions answered correctly by MandS.
8

--- PAGE 9 ---
Preprint
(a) MMLU - Professional Law
 (b) MMLU - US Foreign Policy
Figure 4: Selective Accuracy vs. Coverage for GPT-4. Our surrogate and mixture methods have
a higher area under the selective accuracy vs coverage curve (AUC) than the linguistic confidence
and random confidence baselines. We plot the coverage con the x-axis and the selective accuracy
(accuracy on the top cfraction of examples) on the y-axis, for two representative tasks. Notice that
the mixture (green solid) and surrogate (purple dashed) lines are above the linguistic confidence
(blue dashed/dotted) and random guessing baseline (black dotted).
The questions GPT-4 answers correctly are more correlated with those that Llama 2 70B answers
correctly (Pearson correlation of 0.39), than those that Llama 2 13B answers correctly (correlation
0.19) (Appendix A.8). We also plot the embeddings of questions that GPT-4 gets incorrect (blue
dots) and the questions two potential surrogates Llama 2 70B and Llama 2 13B get incorrect (green
dots) (Figure 5). GPT-4 and Llama 2 70B tend to make mistakes on more of the same questions
(more black dots on the left plot). We also see more spatial similarity in the mistakes of GPT-4 and
Llama 2 70B. So better surrogate models Sand their corresponding main models Mmay strug-
gle with semantically related concepts, causing them to have low confidences on similar types of
questions. Intuitively, the probabilities of a surrogate like Llama 2 transfer well to a stronger model
like GPT-4 because Llama 2 is good at ‘spotting’ difficult questions, even if it cannot always an-
swer them — we reason that both models have higher entropy probability distributions over answer
choices for more difficult questions, and more peaked probability distributions for easier questions.
Why Is Tiebreaking Sufficient? As mentioned, linguistic confidences tend to be repetitive and
clustered at only a few values (e.g., 0.9), limiting their ability to separate correct and incorrect an-
swers. Since a surrogate model’s probabilities for each example are nearly unique, composing just
a small fraction of them with linguistic confidence scores (Section 5.1) can allow answers which
previously had the same linguistic confidence to now be separable through different composite con-
fidence scores. This means that in cases where linguistic confidence scores are identical, we fall
back on the surrogate model’s probabilities to provide an order examples based on confidence.
7 R ELATED WORK
Confidence Estimation for LLMs. Confidence estimation for LLMs has been studied in several
related works. Kadavath et al. (2022) show that Claude’s model probabilities are well-calibrated on
multiple/choice and True/False questions. Zhou et al. (2023) study the effect of introducing expres-
sions of uncertainty into prompts, on model accuracy. Our work differs from these since we focus
on confidence elicitation for models which don’t provide log probabilities. Concurrent work (Xiong
et al., 2023) studies calibration and selective classification of linguistic confidence scores gener-
ated by LLMs. While this work also elicits prompted confidences, they focus on self-consistency
(SC) based methods which are expensive because they require prompting GPT-4 several times. Our
proposed Surrogate and Mixture of models methods are less expensive, since model probabilities
from smaller models (Llama 2) are used to improve the confidence estimates of larger models (GPT-
4). We also show performance improvements over their best method. (Lin et al., 2022) examine
fine-tuning language models to improve confidence estimation, which we do not have access to.
9

--- PAGE 10 ---
Preprint
(a) GPT-4 and Llama 2 70B
 (b) GPT-4 and Llama 2 13B
Figure 5: Embeddings of Incorrect Questions for GPT-4 and Surrogate Models Plots of the
embeddings of questions GPT-4 and two surrogate models (Llama 2 70B and Llama 2 13B) answer
incorrectly on two representative datasets - TruthfulQA and College Chemistry. Questions only
GPT-4 answers incorrectly are in blue, questions GPT-4 and the surrogate answer incorrectly are in
black, and questions only the surrogate answers incorrectly are in green. There are more questions
that both GPT-4 and Llama 2 70B answer incorrectly and more semantic similarity in their incorrect
questions. This indicates that Llama 2 70B and GPT-4 struggle with semantically related concepts
and that the 70B model may more closely estimate GPT-4’s uncertainty than the 13B model.
Selective Classification and OOD Detection. Our paper focuses on selective classification, a clas-
sical problem in machine learning (El-Yaniv & Wiener, 2010; Khani et al., 2016; Feng et al., 2019;
Jones et al., 2021) and statistics (Chow, 1970; Hellman & Raviv, 1970). A related problem is out-
of-distribution detection (Pimentel et al., 2014; Liang et al., 2018; Ovadia et al., 2019), where the
goal is to detect examples very different from training (where the model may make mistakes). Prior
work uses internals of the models — probability outputs (Hendrycks & Gimpel, 2017), represen-
tations (Lee et al., 2018) of models, or even updates the training procedure (Bartlett & Wegkamp,
2008; Mozannar & Sontag, 2020) — which state-of-the-art LLMs do not currently give access to.
Calibration. The general idea of confidence estimation is also studied in calibration (Murphy &
Winkler, 1977; DeGroot & Fienberg, 1983; Naeini et al., 2014; Guo et al., 2017). While related,
the focus is different—a model which outputs its accuracy on every example has 0 calibration error
(ECE), but cannot separate correct and incorrect examples (Kuleshov & Liang, 2015).
8 C ONCLUSION AND FUTURE WORK
Our work aims to address the open challenge of eliciting good confidence estimates from state-of-
the-art LLMs such as GPT-4 and Claude-v1.3, which don’t provide access to their internal proba-
bilities. Our results highlight the importance of releasing model probabilities, since linguistic confi-
dences alone are generally not expressive enough to provide high quality confidence estimates. We
demonstrate that probabilities from weaker white-box, surrogate models can effectively estimate the
internal confidences of stronger black-box models like GPT-4, outperforming linguistic confidences,
and provide some intuitions for why confidences can transfer between models. We hope that our
findings can inspire future work on understanding the transferability of model probabilities and rep-
resentations and on leveraging this transferability to use white-box models to understand black-box
models. Interestingly, we also find that confidence signals from different models are complementary
and can be composed for even more reliable confidence estimation. Future methods could further
build on this result to develop more sophisticated methods of confidence signal composition.
10

--- PAGE 11 ---
Preprint
REFERENCES
Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss.
Journal of Machine Learning Research (JMLR) , 9(0):1823–1840, 2008.
Chao K Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information
Theory , 16(1):41–46, 1970.
Luigi Pietro Cordella, Claudio De Stefano, Francesco Tortorella, and Mario Vento. A method for
improving classification reliability of multilayer perceptrons. IEEE Transactions on Neural Net-
works , 6(5):1140–1147, 1995.
Morris H. DeGroot and Stephen E. Fienberg. The comparison and evaluation of forecasters. Journal
of the Royal Statistical Society. Series D (The Statistician) , 32:12–22, 1983.
Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of
Machine Learning Research (JMLR) , 11, 2010.
Jean Feng, Arjun Sondhi, Jessica Perry, and Noah Simon. Selective prediction-set models with
coverage guarantees. arXiv preprint arXiv:1906.05473 , 2019.
Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In Advances
in Neural Information Processing Systems (NeurIPS) , 2017.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning (ICML) , pp. 1321–1330, 2017.
Martin Hellman and Josef Raviv. Probability of error, equivocation, and the chernoff bound. IEEE
Transactions on Information Theory , 16(4):368–372, 1970.
Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution
examples in neural networks. In International Conference on Learning Representations (ICLR) ,
2017.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob
Steinhardt. Measuring massive multitask language understanding. In International Conference
on Learning Representations (ICLR) , 2021.
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis-
ease does this patient have? a large-scale open domain question answering dataset from medical
exams. In arXiv preprint arXiv:2009.13081 , 2021.
Erik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, and Percy Liang. Selective classification
can magnify disparities across groups. In International Conference on Learning Representations
(ICLR) , 2021.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez,
Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer
El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bow-
man, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna
Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom
Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Ka-
plan. Language models (mostly) know what they know, 2022.
Amita Kamath, Robin Jia, and Percy Liang. Selective question answering under domain shift. In
Association for Computational Linguistics (ACL) , 2020.
Fereshte Khani, Martin Rinard, and Percy Liang. Unanimous prediction for 100% precision with
application to learning semantic mappings. In Association for Computational Linguistics (ACL) ,
2016.
V olodymyr Kuleshov and Percy Liang. Calibrated structured prediction. In Advances in Neural
Information Processing Systems (NeurIPS) , 2015.
11

--- PAGE 12 ---
Preprint
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In Advances in Neural Information Process-
ing Systems (NeurIPS) , 2018.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,
Yian Zhang, D. Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan,
Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher Ré, Diana
Acosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu
Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksek-
gonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, O. Khattab, Peter Hen-
derson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, S. Ganguli, Tatsunori
Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yi-
fan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. arXiv preprint
arXiv:2211.09110 , 2022.
Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image
detection in neural networks. In International Conference on Learning Representations (ICLR) ,
2018.
Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human
falsehoods. arXiv preprint arXiv:2109.07958 , 2021.
Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in
words. Transactions on Machine Learning Research , 2022. ISSN 2835-8856. URL https:
//openreview.net/forum?id=8s8K2UZGTZ .
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct
electricity? a new dataset for open book question answering. In Empirical Methods in Natural
Language Processing (EMNLP) , 2018.
Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. arXiv
preprint arXiv:2006.01862 , 2020.
Allan H. Murphy and Robert L. Winkler. Reliability of subjective probability forecasts of precipi-
tation and temperature. Journal of the Royal Statistical Society. Series C (Applied Statistics) , 26:
41–47, 1977.
Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Binary classifier calibration:
Non-parametric approach. arXiv , 2014.
OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt , 2022.
OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 , 2023a.
OpenAI. Text-davinci-003 api reference. https://platform.openai.com/docs/models/gpt-3-5 , 2023b.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V . Dil-
lon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? eval-
uating predictive uncertainty under dataset shift. In Advances in Neural Information Processing
Systems (NeurIPS) , 2019.
Marco AF Pimentel, David A Clifton, Lei Clifton, and Lionel Tarassenko. A review of novelty
detection. Signal Processing , 99:215–249, 2014.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques-
tion answering challenge targeting commonsense knowledge. In North American Association for
Computational Linguistics (NAACL) , 2019.
Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea
Finn, and Christopher D. Manning. Just ask for calibration: Strategies for eliciting calibrated
confidence scores from language models fine-tuned with human feedback, 2023.
12

--- PAGE 13 ---
Preprint
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
arXiv , 2023.
Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms
express their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint
arXiv:2306.13063 , 2023. URL https://arxiv.org/pdf/2306.13063.pdf .
Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey area: Expressions of
overconfidence and uncertainty in language models, 2023.
13

--- PAGE 14 ---
Preprint
A A PPENDIX
A.1 D ATASET DETAILS
TruthfulQA is a multiple choice benchmark designed to check the truthfulness of large language
models by testing them on questions across 38 different categories like health and politics where
humans might provide incorrect responses due to implicit biases or incorrect beliefs. This task is
challenging for language models because they may imbibe these same misconceptions from training
corpora.
MedQA is a challenging dataset testing medical knowledge with questions based on the
United States Medical License Exams (USMLE) and other medical board exams.
CommonsenseQA is a multiple choice benchmark testing commonsense reasoning, with challeng-
ing associations extracted from ConceptNet to find many target concepts for a single source concept.
OpenbookQA is a multiple choice dataset requiring multi-step reasoning over common and
commonsense knowledge requiring deeper understanding of a diverse set of topics.
MMLU is a massive benchmark covering 57 subjects from a diverse set of areas including
STEM, humanties, and social sciences. This benchmark tests both more rudimentary and more
advanced sets of knowledge for these topics covering great breadth and depth.
Our method requires no training or adaptation of the models used. We evaluate on 250 examples
from each dataset or on the maximum size of the dataset’s test subset (if test subset is smaller).
A.2 M ODEL ACCURACIES
Following are the accuracies for each of the models on the 12 datasets. Answers are elicited using
the prompt format specified in A.4. As expected the GPT-4 model has the highest accuracies on
all 12 datasets, followed by Claude-v1.3. Llama 2 Chat and Base have comparable accuracy to
GPT-3.5. Text-davinci-003 has the lowest accuracies.
Model TQA MedQA CSQA OBQA Law Ethics Physics
Text-davinci 0.472 0.504 0.712 0.772 0.532 0.590 0.579
Llama-2 0.524 0.564 0.664 0.808 0.572 0.590 0.583
Llama-2 Chat 0.480 0.512 0.684 0.728 0.528 0.600 0.528
GPT-3.5 0.572 0.536 0.676 0.776 0.504 0.670 0.523
Claude-v1.3 0.596 0.640 0.736 0.832 0.604 0.760 0.638
GPT-4 0.836 0.736 0.768 0.940 0.664 0.760 0.813
Econ Algebra Chem Security Policy Avg
Text-davinci 0.412 0.300 0.440 0.690 0.850 0.571
Llama-2 0.386 0.240 0.460 0.750 0.910 0.588
Llama-2 0.333 0.310 0.420 0.670 0.850 0.554
GPT-3.5 0.404 0.320 0.460 0.730 0.800 0.581
Claude-v1.3 0.579 0.330 0.500 0.760 0.880 0.655
GPT-4 0.596 0.480 0.520 0.800 0.910 0.735
Table 4: Model Accuracies Accuracies of all 6 models on all 12 tasks. GPT-4 is the highest per-
forming model for all tasks.
A.3 AUC AND AUROC D EFINITIONS
AUC with Randomized or Deterministic Classifiers. To plot the accuracy-coverage curve we
compute A(c), the selective accuracy at coverage cacross different values of c.A(c)is the accuracy
if the model only makes a prediction on the cproportion of data with highest confidence scores.
for different values of c. When making a prediction on cproportion of data, for each example x
we use a binary classifier on the confidence score C(x)decide if we are making a prediction for
x(1if making a prediction and 0if abstaining from prediction). Such a classifier can either be
deterministic or randomized.
14

--- PAGE 15 ---
Preprint
Deterministic Classifiers. A deterministic classifier freturns identical outputs for identical
inputs — resulting in consistent treatment of examples with the same confidence score (either
predict on all or abstain on all). Using a deterministic classifier to select cportion of examples to
predict on means we find the highest confidence threshold tsuch that P(C(x)≥t)≥c—tis
the highest confidence threshold where the proportion of examples with confidence greater than or
equal to tis greater than or equal to the required coverage c. With a deterministic classifier, we
predict on P(C(x)≥t)proportion of examples, which may be greater than the required coverage
c.
f(C(x))∈ {0,1} (A.1)
Randomized Classifiers. A randomized classifier hcan return different outputs for the same input.
Since models can output the same linguistic confidences for multiple examples, a randomized clas-
sifier can allow us to achieve exactly a coverage of cby making predictions on some examples with a
given confidence, and abstaining on other examples with the same confidence. To enable hto break
ties and make different predictions for examples with the same confidence score, we add a small
amount of Gaussian noise to each confidence score N(0, ϵ), ϵ→0to enforce a confidence-based
ordering of examples.
h(C(x) +N(0, ϵ))∈ {0,1} (A.2)
Deterministic vs Randomized AUC Example. Suppose a model assigns half of the examples a
confidence of 1 and gets them all right, and the other half of examples a confidence of 0.5 and gets
50% of them right. What is the selective accuracy at coverage 75%? A deterministic classifier
would select 0.5 as tand predict on all examples with C(x)≥t, which in this case is all of the
examples (notably leading to a coverage of 100% instead of 75%). This would lead to an accuracy
of 75%. A randomized classifier would predict on all examples of confidence 1, but to meet the
75% coverage threshold, it would predict on half of the examples which have confidence 0.5 —
selecting the top half after adding random noise. This would lead to an accuracy of approximately
83%.
The above example demonstrates that we may underestimate the AUC value by using a de-
terministic classifier - since it forces a prediction on a higher coverage threshold. For example, for
GPT-4 using a deterministic classifier leads to 78.8% AUC averaged across 12 datasets, while using
a randomized classifier leads to 80.5% AUC. Since it generally leads to higher selective accuracy
values and allows predicting on exactly the top cproportion of examples based on confidence, we
use a randomized classifier in our AUC calculations.
AUROC Definition. Following is how we compute the AUROC metric. Let R(x, y) = 1 if
an answer yis correct for input x, and 0otherwise. C(x)∈[0,1]is the confidence score for
example x.:
1. Let the true positive rate at threshold tbe the fraction of correct examples with confidence
at least t: TPR (t) =E[R(x, y(x))I(C(x)≥t)]/E[R(x, y(x))]
2. Let the false positive rate at threshold tbe the fraction of incorrect examples with
confidence at least t: FPR (t) =E[(1−R(x, y(x)))I(C(x)≥t)]/E[1−R(x, y(x))]
3. Plot TPR (t)(y-axis) against FPR(t) (x-axis) and take area under the curve. The AUROC
checks how well thresholding on the confidence can predict if an example is correct or
incorrect.
A random classifier (which assigns random confidences to each example) will have the same TPR
and FPR. A model with good confidences will assign higher confidences to correct examples, so a
larger fraction of the correct examples (than incorrect examples) will have confidence ≥t.
A.4 L INGUISTIC CONFIDENCE PROMPTS
We zero-shot prompt models to elicit answers and linguistic confidence scores. We focus on
zero-shot prompting instead of few-shot prompting to assess how good models are at linguistically
understanding a notion of confidence, without being primed with examples which could bias their
generated confidence assessments.
15

--- PAGE 16 ---
Preprint
Prompt Categories. We study 24 prompt format variations to elicit linguistic confidences,
since there can be many ways for a model to describe its confidence levels and we want to ensure a
fair assessment of linguistically generated confidence estimates that is not overly conditioned on a
specific prompt. These fall into the following categories — prompts eliciting:
1. Numerical confidences (score from between varying ranges 0-1, 0-10, 0-100), probabilities
from 0-100%
2. Linguistic categorization of confidences into varying numbers of confidence buckets (‘not
sure’, ‘sure’, ‘very sure’, among other phrases and degrees of certainty)
3. Zero-shot chain-of-thought explanations of confidence in addition to confidence scores
4. Prompting first for an answer and then re-prompting for the confidence in the answer
5. Varying the confidence instructions (with other phrases describing confidence including
‘uncertainty’, ‘certainty’, ‘correctness’, ‘probability or likelihood of correctness’)
Best Prompt Selection. We measure the AUC performance of each prompt across the 12 datasets
for all models. The optimal prompt varies slightly for each model, so for the best prompt we select
the prompt which reduces the sum of drops in AUC per model from the optimal prompt for each
individual model (the prompt which is the closest in optimal AUC performance for all models on all
datasets). This prompt is used for all of our experiments.
Best Prompt Description. The prompt elicits a confidence score from 0-1 with the given instruc-
tion. It also includes ‘fake few-shot examples’ as described which show fake questions, answers,
and confidence scores to allow the model to learn the format of the task without providing any other
task-specific information. Since fake-fewshot examples do including answer choices (D, A in the
example) and specific confidence scores (0.4, 0.7), we experimented with variations of the prompt
modifying the exact answers and confidences included in the example and found the change in gen-
erated confidences to be minimal – indicating that models were able to use these examples to learn
the format of the task without over-conditioning on the specific answers and confidences provided.:
Figure 6: Best Linguistic Confidence Prompt
Examples of Other Prompts. Following are a few of the several alternative prompts we tried:
16

--- PAGE 17 ---
Preprint
Figure 7: Linguistic Categorization Prompt. Elicits confidences in different categories of
linguistic certainty.
Figure 8: Probabilistic Confidence Prompt. Frames confidence as a probability between 0-100%
- in case models have seen more instances of probabilistic confidences in their training data.
Figure 9: Zero-shot Chain-of-Thought. Allows the model to provide a chain of reasoning before
deciding on its confidence score. Initial analysis indicated that these reasoning chains served to
further reinforce the model’s overconfidence by often incorrectly justifying its answers.
A.5 L LAMA 2 70B C HAT RESULTS
We also experiment with the chat version of the Llama 2 70B model, evaluating AUC and AU-
ROC for linguistic confidences and model probabilities. We find that the chat version of the model
generally performs similarly to the base version , so primary results are reported on the base model.
Similar to Llama 2 Base, Llama 2 Chat’s probabilities outperform its linguistic confidences based on
AUC and AUROC on all 12 datasets. Between the chat and base versions, the base version generally
outperforms chat for both linguistic confidences and model probabilities.
17

--- PAGE 18 ---
Preprint
Metric Confidence TQA Medqa CSQA OBQA Law Ethics Physics
AUCLinguistic 0.631 0.521 0.679 0.750 0.529 0.675 0.556
Prob 0.699 0.604 0.840 0.869 0.674 0.823 0.721
AUROCLinguistic 0.683 0.517 0.506 0.535 0.501 0.562 0.568
Prob 0.754 0.609 0.764 0.776 0.710 0.821 0.721
Metric Confidence Econ Algebra Chem Security Policy Avg
AUCLinguistic 0.367 0.296 0.445 0.718 0.846 0.584
Prob 0.438 0.348 0.632 0.850 0.963 0.705
AUROCLinguistic 0.553 0.485 0.546 0.560 0.479 0.541
Prob 0.634 0.495 0.721 0.811 0.858 0.723
Table 5: AUC and AUROC Metrics for Llama 2 70B Chat . Llama 2 Base’s linguistic confidence
scores outperform Llama 2 Chat’s linguistic confidences — 73.1% AUC compared to 70.5%. Sim-
ilarly, Llama 2 Base’s model probabilities also outperform Llama 2 Chat’s probabilities — 73.1%
AUC compared to 70.5%. These results may support the conclusion that base models are better
calibrated than chat models.
A.6 ECE R ESULTS
Following are the ECEs for the linguistic confidence scores of each model and the ECEs of model
probabilities for models which provide them.
Confidence Type TQA Medqa CSQA OBQA Law Ethics Physics
Text-davinci Linguistic 0.422 0.425 0.161 0.127 0.380 0.300 0.299
Text-davinci Prob 0.461 0.454 0.235 0.191 0.388 0.338 0.338
Llama 2 Linguistic 0.365 0.248 0.201 0.073 0.224 0.259 0.267
Llama 2 Prob 0.099 0.084 0.176 0.235 0.115 0.145 0.094
Llama 2 Chat Linguistic 0.357 0.391 0.125 0.101 0.350 0.194 0.337
Llama 2 Chat Prob 0.284 0.228 0.124 0.092 0.264 0.213 0.210
GPT-3.5 Linguistic 0.350 0.380 0.192 0.091 0.388 0.176 0.363
Claude-v1.3 Linguistic 0.187 0.086 0.042 0.033 0.098 0.052 0.162
GPT-4 Linguistic 0.104 0.118 0.118 0.038 0.187 0.114 0.109
Confidence Type Econ Algebra Chem Security Policy Avg
Text-davinci Linguistic 0.482 0.625 0.475 0.213 0.038 0.329
Text-davinci Prob 0.478 0.576 0.385 0.263 0.112 0.352
Llama 2 Linguistic 0.453 0.561 0.435 0.079 0.093 0.272
Llama 2 Prob 0.205 0.091 0.100 0.172 0.264 0.148
Llama 2 Chat Linguistic 0.505 0.480 0.480 0.165 0.055 0.295
Llama 2 Chat Prob 0.403 0.361 0.272 0.187 0.073 0.226
GPT-3.5 Linguistic 0.515 0.560 0.432 0.173 0.094 0.309
Claude-v1.3 Linguistic 0.132 0.319 0.175 0.058 0.162 0.126
GPT-4 Linguistic 0.270 0.420 0.313 0.118 0.053 0.164
Table 6: ECE Values Linguistic Confidences vs Model Probabilities The ECE values for lin-
guistic confidences and model probabilities show that on some datasets model probabilities achieve
better ECE values, while on other datasets linguistic confidences achieve better ECE values. Among
the state-of-the-art models, Claude-v1.3’s linguistic confidences notably result in the least expected
calibration error on 9/12 datasets.
Metric. We compute the expected calibration error metric (ECE) by dynamically binning examples
based on their confidence scores into 10 bins with approximately equal numbers of examples in
each bin. For each bin, we compute the calibration error, which is the absolute difference between
18

--- PAGE 19 ---
Preprint
the mean predicted confidence and the mean observed accuracy. This quantifies how well the pre-
dicted confidences match the true probability of correctness within each bin. We then calculate the
weighted average of the calibration errors across all bins, where the weights are the proportion of
examples in each bin relative to the total number of examples.
We also compute the ECE values for our baseline confidence methods (linguistic confidences, SC
linguistic confidences) and for our proposed confidence methods (surrogate, tiebreak, mixture, and
SC mixture) for the GPT-4 model. For 11 out of 12 tasks, we find that our proposed methods lead
to the lowest ECE values.
TQA MedQA CSQA OBQA Law Ethics Physics
Ling. Conf. 0.104 0.118 0.118 0.038 0.187 0.114 0.109
SC Ling. Conf. 0.126 0.163 0.120 0.036 0.246 0.204 0.120
Surrogate†0.395 0.212 0.297 0.370 0.156 0.205 0.317
Tiebreak†0.114 0.134 0.126 0.032 0.194 0.114 0.118
Mixture†0.096 0.075 0.061 0.159 0.064 0.111 0.088
SC Mixture†0.085 0.120 0.108 0.029 0.216 0.186 0.098
Confidence Type Econ Algebra Chem Security Policy Avg
Ling. Conf. 0.270 0.420 0.313 0.118 0.053 0.164
SC Ling. Conf. 0.323 0.379 0.331 0.136 0.063 0.187
Surrogate†0.129 0.162 0.187 0.210 0.264 0.242
Tiebreak†0.270 0.419 0.332 0.158 0.068 0.173
Mixture†0.126 0.224 0.229 0.108 0.138 0.123
SC Mixture†0.287 0.358 0.286 0.129 0.068 0.164
Table 7: ECE Values All Confidence Methods for GPT-4
A.7 S URROGATE MODEL RESULTS
Figure 10: AUCs For All Surrogate Models We compute the AUC metric for each model consid-
ering surrogate confidences from both model probabilities and linguistic confidence scores from all
other models. We find that all models benefit from using surrogate model probabilities over their
own linguistic confidences.
19

--- PAGE 20 ---
Preprint
Figure 11: AUROCs For All Surrogate Models We also compute the AUROC metric for each
model considering surrogate confidences from both model probabilities and linguistic confidence
scores from all other models. In general, we find that using surrogate model probabilities instead of
a model’s own linguistic confidences improves AUROC values.
AUCs and AUROCs for surrogate models show that model probabilities from other models can
provide better confidence estimates than a models own linguistic confidences.
A.8 C ORRELATION AND COVARIANCE OF SURROGATE AND MAINMODELS
Figure 12: Correlations For All Main and Surrogate Models
20

--- PAGE 21 ---
Preprint
Figure 13: Covariances For All Main and Surrogate Models
We compute correlations and covariances between the answer correctness (set of binary scores in-
dicating if a model answered a question correctly or not) for every pair of main model and potential
surrogate model. We find that in general if a surrogate model Shas a high degree of correlation
in answer correctness with a main model M, then Sis likely to be a good surrogate for M. For
example, GPT-4 has a higher correlation with Llama 2 Base, Llama 2 Chat, and text-davinci-003
than it does with Llama 2 13B indicating that those models can be better surrogates for GPT-4 than
Llama 2 13B. Similar trends also hold for covariances.
A.9 C ALIBRATION OF MIXTURE OF MODELS
Figure 14: Calibration of GPT-4 with Linguistic Confidence and Mixture of Models In the first
row we see the calibration of GPT-4 on MedQA, Professional Law, and US Foreign Policy when
using linguistic confidences. In the second row, we see GPT-4’s calibration using our Mixture of
Models confidence method. A perfectly calibrated model would have all bars aligned with the red
line (average confidence in each bucket is exactly equal to the average accuracy). We can see that
calibration improves demonstrably, when using Mixture of Models.
21
