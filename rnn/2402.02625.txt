# 2402.02625.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rnn/2402.02625.pdf
# File size: 992209 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ICML 2024 Next Generation of Sequence Modeling Architectures Workshop
Enhancing Transformer RNNs with Multiple Temporal Perspectives
Razvan-Gabriel Dumitru RDUMITRU @ARIZONA .EDU
University of Arizona, Tucson, AZ, USA
Darius Peteleaza DARIUS .PETELEAZA @MULTIVERSX .COM
MultiversX
Lucian Blaga University of Sibiu, Sibiu, Romania
Mihai Surdeanu MSURDEANU @ARIZONA .EDU
University of Arizona, Tucson, AZ, USA
Abstract
We introduce the concept of multiple temporal perspectives, a novel approach applicable to Recurrent
Neural Network (RNN) architectures for enhancing their understanding of sequential data. This
method involves maintaining diverse temporal views of previously encountered text, significantly
enriching the language models’ capacity to interpret context. To show the efficacy of this approach,
we incorporate it into the Receptance Weighted Key Value (RWKV) architecture, addressing its
inherent challenge of retaining all historical information within a single hidden state. Notably, this
improvement is achieved with a minimal increase in the number of parameters –even as little as
0.04% of the original number of parameters. Further, the additional parameters necessary for the
multiple temporal perspectives are fine-tuned with minimal computational overhead, avoiding the
need for a full pre-training. The resulting model maintains linear computational complexity during
prompt inference, ensuring consistent efficiency across various sequence lengths. The empirical
results and ablation studies included in our research validate the effectiveness of our approach,
showcasing improved performance across multiple benchmarks. The code, model weights and
datasets are open-sourced at https://github.com/RazvanDu/TemporalRNNs.
1. Introduction
The RWKV (Receptance Weighted Key Value) architecture [ 18] bridges the gap between RNNs and
Transformers due to its ability to be trained like a Transformer, while offering the inference efficiency
of an RNN. The model further stands out for its ability to execute time-parallel processing during
training, significantly reducing the computational load. During prompt inference, RWKV ensures
linear computational complexity in relation to the sequence length in its sequential decoding mode,
akin to conventional RNNs, thus providing consistent efficiency across various sequence lengths.
Here we address the above limitation with a novel idea in the realm of RNN architectures:
multiple temporal perspectives . At a high level, our method maintains multiple temporal views of
the previously seen text, thereby enriching the language model’s understanding of sequential data.
Figure 2 shows an actual example that highlights how the different perspectives are used during
decoding.
The main contributions of our paper are:
•A novel approach that maintains multiple temporal perspectives within the RWKV architecture,
enhancing its capacity to process and interpret complex language data.
© R.-G. Dumitru, D. Peteleaza & M. Surdeanu.arXiv:2402.02625v2  [cs.LG]  11 Jul 2024

--- PAGE 2 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
•Empirical demonstration of the model’s capacity to learn different perspectives from a limited
amount of data with a minimal (even as little as 0.04%) increase in the number parameters.
Despite these constrained settings, we show that that our approach outperforms the original
RWKV architecture on several benchmarks.
•An ablation analysis which indicates the importance of maintaining multiple perspectives as
well as the significance of their careful integration.
2. Related Work
Addressing the computational demands of Transformer-based architectures has been a fundamental
area of research in recent years. Efforts have centered around optimizing the attention mechanism, a
cornerstone of Transformer efficiency but also a source of its computational complexity. Innovations
in this domain have led to various Transformer adaptations [ 1,14], which seek to streamline the
attention mechanism to reduce computational load. These adaptations include introducing sparse
attention patterns [ 16,21] and formulating methods to compute the attention matrix in a more
resource-efficient manner [7, 8, 15].
State space models (SSMs) have been gaining traction in NLP for their ability to model sequential
data efficiently. The S4 model [ 13] is a notable example, demonstrating the effectiveness of SSMs
in handling long-range dependencies within text. Variants of S4, such as those explored by [ 9,19]
and [ 12], further refine this approach, offering improvements in scalability and performance. SSMs
represent a shift from traditional recurrent architectures, providing a framework that is well-suited
for modeling complex temporal dynamics in language data.
3. Proposed Approach
Figure 1: Proposed architecture with multiple temporal perspectives. The green blocks represent the newly added
perspective-specific elements. Each input copy ( C1-Cn) corresponds to one perspective, processed by its dedicated
temporal component ( µ1-µn), used for training. LN denotes Layer Normalization components. The TIME MIXING,
CHANNEL MIXING and HEAD components are described in detail in Figure 3, Figure 4 and in Section 3.1.
3.1. Multiple Temporal Perspectives
Unlike RWKV’s [ 18] single state per channel mixing block, our method employs nstates in each
block, enabling the model to process data from multiple temporal contexts in parallel. This develop-
ment extends to the time mixing blocks as well. RWKV has one state in each time mixing block,
complemented by four additional blocks for computing attention. We expand this by having nsuch
2

--- PAGE 3 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
blocks operating in parallel. This parallel structure allows each block to independently process
different segments of temporal data, leading to a more comprehensive understanding of the input
sequence, as shown in Figure 2. Mathematically, the time-mixing blocks are described by equations
1, 2, and 3. Similarly the channel-mixing blocks are described by Equations 1 and 2:
r(i)
t=Wr·(µ(i)
r·x(i)
t+ (1−µ(i)
r)·x(i)
t−1) (1)
k(i)
t=Wk·(µ(i)
k·x(i)
t+ (1−µ(i)
k)·x(i)
t−1) (2)
v(i)
t=Wv·(µ(i)
v·x(i)
t+ (1−µ(i)
v)·x(i)
t−1) (3)
where iiterates over all available perspectives.
Each perspective iuses replicated temporal components ( µ(i)
r,µ(i)
k,µ(i)
v) that enable the model to
combine the current time step ( x(i)
t) with the previous one ( x(i)
t−1). The temporal components µare
essential as they are the ones being customized during our fine-tuning process. The positional weight
decay vectors ( Wr,Wk,Wv) are shared across perspectives, which is critical in managing memory
usage efficiently due to their substantial memory footprint. The duplication of ( x(i)
t) inputs across
each perspective reflects the model’s adaptive response to the introduction of multiple temporal
viewpoints, directly affecting the inputs processed through each perspective.
3.2. Perspective Aggregation Strategies
We explored three distinct methods for combining the outputs from the multiple temporal perspectives
incorporated into our model: average, transformer-like, and a novel method developed by us that is
a weighted average with weights learned from data. These aggregation techniques implement the
HEAD component in Figure 1. Due to space limitations, they are described in detail in the Appendix
(Section 5).
4. Experiments
4.1. Experimental Setup
In our experimental framework, we used the English subset of the Wikipedia dataset (dump of 1st
of March 2022) [ 23], for training our multiple temporal perspectives. We start from pre-trained
RWKV-v4 models [ 5], which were originally trained on The Pile dataset [ 10]. It should be noted that
the Wikipedia dataset is included as part of The Pile, the dataset that RWKV-v4 was pre-trained on,
so no additional data was used for training beyond this pre-existing corpus.
4.2. Model Comparison
In this subsection of our experiments, we present a detailed analysis comparing the performance of
our enhanced RWKV-v4 model with several established models, including the default RWKV-v4,
Pythia [ 2], and GPT-Neo [ 4]. Our evaluation uses a suite of popular and challenging benchmarks,
namely LAMBADA [17], ARC-Easy [6], WinoGrande [20], HellaSwag [24], and PIQA [3].
Table 1 summarizes our results. The table shows that our method outperforms the RWKV-v4
model by a statistically significant margin on 10 out of 18 configurations and is comparable on 6 out
of the remaining 8 settings. Notably, the improvements in model performance are achieved with a
minimal increase in the number of parameters (Table 2), underscoring the efficiency of our approach.
Further, we ensure that our enhancements improve model capabilities without incurring a substantial
3

--- PAGE 4 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
Table 1: Performance of our method on five datasets compared against three other comparable language models. Green
cells (and italic font) indicate results where our model exceeded the performance of the default RWKV-v4 architecture;
purple cells denote results that are within the margin of error compared to RWKV-v4; and red cells highlight instances
where our model underperformed relative to RWKV-v4. For clarity and emphasis, the best results per dataset among all
compared models are highlighted in bold. This color scheme is designed to be accessible to individuals with color vision
deficiencies.
Model Param LAMBADA LAMBADA ARC-E WinoGrande HellaSwag PIQA
B↓ ppl↓ acc↑ acc↑ acc↑ accnorm ↑acc↑
Ours 0.17 29.25±0.24 34.11±0.15 47.41±0.27 51.56±0.36 32.37±0.07 64.56±0.08
RWKV-v4 0.17 30.55 32.91 46.88 50.82 32.36 64.79
Pythia 0.16 24.38 38.97 45.12 52.01 31.63 62.68
GPT-Neo 0.16 30.27 37.36 43.73 50.43 30.42 63.06
Ours 0.43 12.56±0.08 46.53±0.1 53.04±0.28 52.03±0.12 40.72±0.08 67.57±0.39
RWKV-v4 0.43 13.04 46.05 52.65 52.09 40.72 67.84
Pythia 0.40 11.58 50.44 50.38 53.35 39.10 66.70
GPT-Neo 0.40 13.88 47.29 48.91 51.14 37.64 65.07
Ours 1.5 6.85±0.01 57.28±0.16 60.94±0.27 55.38±0.16 52.72±0.14 72.01±0.06
RWKV-v4 1.5 6.91 57.36 60.52 54.53 52.79 72.14
Pythia 1.4 6.58 60.43 57.74 56.51 50.82 71.11
GPT-Neo 1.4 7.5 57.25 56.19 54.93 48.94 71.16
computational cost, reflecting a significant advancement in model optimization techniques. For
example, using a single NVIDIA RTX 4090 GPU, training one mini-epoch on the smallest model
took only approximately 30 minutes.
All in all, this comparative analysis highlights the competitive advantage of our method, which
reinforces the potential of our multiple temporal perspectives approach to enhance RNN-based
models in a variety of complex NLP tasks.
4.3. Ablation Studies
To further explore the the effects of various components and design choices within our approach we
also performed ablation studies, which can be found in the Appendix (Section 5).
5. Conclusion
Our research introduced an advancement to RNN architectures, namely multiple temporal perspec-
tives. Our contribution enhances sequential data interpretation with minimal impact on model
complexity. Our design features parallelizable temporal perspectives, reminiscent of the multi-head
attention mechanism in transformers [ 22], and by having these perspectives share most of the pa-
rameters, we achieve a minimal parameter increase ( <0.1%), ensuring our architecture is both
efficient and streamlined, maintaining a balance between complexity and functionality. The efficient
parameter sharing is what truly differentiates our paper from a Mixture of Experts (MoEs) approach
where each smaller model has its own complete set of parameters.
We apply our idea to the RWKV architecture [ 18], a recent hybrid approach that trains like a trans-
former, but performs inference in linear time similar to a RNN. By applying our multiple temporal
perspectives to RWKV , we effectively address the challenge of historical information retention. Our
4

--- PAGE 5 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
empirical results demonstrate enhanced performance across five challenging benchmarks, affirming
the potential of this approach to augment RNN capabilities while maintaining linear computational
complexity during inference.
Appendix
Example of Perspective Weights
Figure 2: Runtime example of the proposed RNN decoder with four different temporal perspectives. The X axis shows
a sentence that was decoded left-to-right. Column ishows the distribution of importance weights assigned to the four
temporal perspectives when decoding the token at position i. For example, when decoding the word “dog,” the model
prioritizes Perspective 2, followed by 1, 3, and 4. Perspectives 1 and 2 have higher weights in most cases, reflecting their
practicality for capturing commonly occurring syntactic patterns, while Perspectives 3 and 4 are applied in contextually
specific instances where unique temporal considerations are required.
Architectural Components
Figure 3: Time mixing
 Figure 4: Channel mixing
5

--- PAGE 6 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
Perspective Aggregation Strategies
AVERAGE AGGREGATION
Oavg=head (1
nnX
i=1pi) (4)
The first method we tested was a simple average aggregation approach (Figure 5). In this method,
the outputs of all temporal perspectives are treated equally, regardless of the specific token being
processed. This lack of data dependency means that each perspective contributes uniformly to
the final output. While conceptually straightforward, our empirical experiments showed that this
approach posed a significant challenge in terms of learning. The neural network had to work harder
to discern meaningful patterns and information from equally weighted perspectives, which in some
cases could lead to less efficient learning. The equation 4 describes the mechanism in detail. In all
figures, the green blocks ( p1-pn) represent the temporal perspectives.
Figure 5: Simple average aggregation mechanism
TRANSFORMER -LIKE AGGREGATION
Otrans=head (W·Concat (p1, p2, . . . , p n) +b), where
W∈Rembd×(n·embd )and b∈Rembd(5)
The second method we explored was inspired by the head aggregation strategy in the transformer
architecture (Figure 6). This method introduces data dependency, addressing a key limitation of the
average method. It works by taking a concatenation of all embeddings produced by the different
perspectives and then employing an MLP-based approach to reduce their size to a single embedding,
which is then used in downstream computations. However, this method comes with a significant
drawback: it adds a considerable number of extra parameters to the model. Our experiments revealed
that this approach was time-consuming in terms of learning. Even with sufficient training time, the
model struggled to learn effectively, making this method less feasible for practical applications. The
equation 5 describes the mechanism in detail.
6

--- PAGE 7 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
Figure 6: Transformer-like aggregation mechanism
OUR ORIGINAL AGGREGATION
Osoftmax =Softmax 
W·(1
nnX
i=1pi) +b!
, where
W∈Rn×embdandb∈Rn(6)
Oours=nX
i=1Osoftmax ,i·head (pi) (7)
Our original aggregation method (Figure 7) also incorporates data dependency, thus avoiding
the shortcomings of the simple average approach introduced above. Our approach begins with an
averaging component; this is followed by a context-dependent weighting that contains a linear layer,
which evaluates and assigns relevance to each perspective for a given context. The final output is then
constructed by combining the outputs from all perspectives, with the contribution of each perspective
being influenced by its assigned weight.
Further, in our method, each perspective’s initial state is identical to that of the original model,
ensuring that initially, all perspectives behave similarly. We then introduce noise into these perspec-
tives and fine-tune them aiming to surpass the original model’s performance (see next subsection).
This strategy is effective because: (a) starting from the same initial state as the original model and
then introducing variability through noise encourages the perspectives to adapt and improve beyond
their starting point, and (b) it starts from a pre-trained model and, thus, can skip this step. Our
experiments demonstrated that this method not only overcomes the limitations of the previous two
methods but also leads to perspectives that are better than their original state, enhancing the overall
model’s performance. Equation 6 describes the mechanism in detail and equation 7 sums it up as the
final output.
Figure 7: Our approach, weighted average with learned weights aggregation mechanism
7

--- PAGE 8 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
Training Method and Noise Addition
FOCUSED TRAINING ON TEMPORAL COMPONENTS
In our approach, we specifically targeted the training of the temporal components of the model. This
decision was made to minimize the risk of overfitting the model to the data. By concentrating on the
temporal aspects, we ensure that the core functionality of the model remains robust while optimizing
its ability to process and interpret temporal information. To this end, we fine-tune the architecture by
freezing the original parameters of the model and train the temporal components. We further detail
this process in the next section.
STRATEGIC NOISE ADDITION TO THE PERSPECTIVE SELECTOR
A notable aspect of our training process involves the addition of noise to help differentiate between
the multiple perspectives. Empirical observations from our experiments indicated that adding noise
to the perspective aggregator (the “Linear” layer component in Figure 7) results in more favorable
outcomes compared to introducing noise directly to the temporal values of the perspectives. This
observation led us to hypothesize that adding noise to the temporal values might lead to the loss of
valuable information. Therefore, by introducing noise to the linear layer responsible for selecting
perspectives, we can enhance the model’s performance without compromising the integrity of the
temporal data. The initial noise applied to this linear layer is calibrated with a standard deviation of
0.01 and a mean of 0, ensuring a subtle yet effective alteration in the model’s learning process.
Table 2: Number of parameters compared with the original RWKV-v4 model.
RWKV-v4 Size Our Approach Increase %
1.6934×1081.6948×1080.08%
4.3039×1084.3077×1080.09%
1.5151×1091.5158×1090.04%
Experimental Setup
As mentioned in the previous section, the temporal perspectives were initialized with the weights of
the original RWKV-v4 model. We employed a random search strategy to select hyperparameters over
several training iterations, optimizing them within the confines of our available hardware resources.
Due to hardware limitations, we constrained our batch size to 2 and concentrated our training on the
novel aspect of our architecture –the temporal perspectives– without retraining the entire model.
We conducted our experiments across three RWKV-v4 model sizes: 169M, 430M, and 1.5B
parameters. The smaller models underwent training for 8 mini-epochs, with each comprising 16,000
contexts, amounting to roughly 131 million tokens. The largest model, with 1.5 billion parameters,
was trained for 8 mini-epochs, each with 48,000 contexts chosen at random, totaling approximately
394 million tokens. Note that in the context of leading models trained on tens of trillions of tokens,
the number of tokens we use for training is low. Initially, our architecture’s development and testing
phases were conducted on a server equipped with a single RTX 4090 GPU. Then, we transitioned
to using a single NVIDIA A100 GPU for our final training rounds. Training one mini-epoch on
the smallest model took approximately 30 minutes, while the medium-sized model required around
8

--- PAGE 9 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
1 hour per mini-epoch. Given the larger model’s increased complexity and the three times larger
dataset, completing one mini-epoch required close to 6 hours of training time. This leads to a total of
2 days for a full training of the 1.5B model, which we ran 3 time for each ablation study.
For model optimization, we adopted an exponential learning rate decay strategy, similar to the
one in the original RWKV-v4 model. Our learning rate started at a peak of 3e-5 and decreased
towards a minimum value of 1e-5.
We evaluated our model’s performance using the EleutherAI Evaluation Harness [11].
Each experiment was replicated three times using unique random seeds to assess the stability of
the results.
Ablation Studies
In this section, we examine the effects of various components and design choices within our approach.
All ablation studies were performed on the 169M-parameter model and included four temporal
perspectives (unless specified otherwise).
NUMBER OF PERSPECTIVES
Figure 8: Number of perspectives impact on performance. Each line represents the mean perplexity (lower is better)
across three runs, while the shaded regions indicate the standard deviation.
Our first ablation study focused on the number of temporal perspectives and their influence on
model performance, using the LAMBADA benchmark as a testing ground. The overall results are
shown in Figure 8. This experiment revealed a positive correlation between the number of perspectives
and the model’s predictive capabilities, with models that featured an increased number of perspectives
demonstrating superior performance. As Figure 8 shows, two to three perspectives offer the optimal
balance between complexity and effectiveness, maximizing the model’s understanding of the input
sequence.
9

--- PAGE 10 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
Furthermore, our analysis of RWKV combined with LoRA, as well as RWKV with our training
approach, indicates that the enhanced performance is attributed to the novel perspectives, rather
than the additional fine-tuning on Wikipedia (already part of the pre-training set), or the additional
parameters added to the model. We hypothesize that LoRA’s comparatively lower performance could
be due to the model being overly specialized towards Wikipedia content.
PERSPECTIVE AGGREGATION STRATEGIES
Table 3: Performance of the various perspective aggregation strategies on two datasets.
Data Set Simple Average Linear Our Approach
ARC-Easy acc 47.32±0.05% 45 .20±0.38% 47.42±0.27%
LAMBADA ppl 30.44±0.13 37 .39±1.12 29.25±0.24
LAMBADA acc 33.17±0.17% 32 .72±1.05% 34.11±0.15%
Table 4: Impact of noise placement on two datasets.
Data Set Noise on Temporal Information Noise on the Linear Layer
ARC-Easy acc 47.40±0.25% 47.42±0.27%
LAMBADA ppl 29.34±0.09 29.25±0.24
LAMBADA acc 33.88±0.05% 34.11±0.15%
The second study compared the three perspective aggregation techniques introduced in Sec-
tion 3.2. The experimental results summarized in Table 3 favored our original aggregation method,
which consistently outperformed others in terms of contributing to the overall performance gains.
NOISE PLACEMENT
We explored the addition of noise both to the temporal information and to the linear layer in our
perspective aggregation component. Our experiments (Table 4) consistently showed that adding
noise into the linear layer led to more substantial improvements in performance, suggesting that this
strategy enables the model to escape local minima more effectively and generalize better.
PERSPECTIVES WEIGHTS APPLIED ON TOKENS
In a qualitative study on the weight distribution across temporal perspectives, our findings revealed a
distinctive pattern of model reliance on two primary perspectives (see Figure 2). These perspectives
demonstrated an adaptive behavior, selectively prioritizing parts of the input sentence based on
contextual relevance.
Limitations
In identifying the limitations of our study, several key aspects emerged. The integration of multiple
temporal perspectives in the RWKV architecture increases inference time complexity proportionally
with the number of perspectives while maintaining the same memory utilization. However, since
10

--- PAGE 11 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
the perspectives are independent of each other, an efficient implementation could parallelize them
retaining the original RWKV inference efficiency. Additionally, the back-propagation process
requires more memory and time proportional to the added perspectives.
Our methodology, constrained by computational resources, utilized a smaller batch size of 2,
possibly limiting the full potential of our approach. We anticipate that a larger batch size or training a
model from scratch with our method could lead to improved outcomes, but these hypotheses remain
untested due to our resource limitations.
Accessibility
Our approach to integrating multiple temporal perspectives within RNN architectures has the potential
to democratize access to recent advancements in large language models (LLMs). By enhancing
the efficiency of these models, we may enable populations with limited computational resources to
harness the power of advanced NLP tools.
Furthermore, we have selected a color scheme that prioritizes accessibility, ensuring the visuals
are clear and discernible to individuals with color vision deficiencies. This inclusive approach reflects
our commitment to making our research accessible to a wider audience, including those with varying
visual abilities.
Impact Statement
In our paper on enhancing Transformer RNNs with multiple temporal perspectives, we recognize
the potential broader impacts of our work in the field of Machine Learning. While our primary
objective is technical advancement, we are mindful of the ethical aspects and societal consequences
of our research. Improved machine learning models, like the one we propose, could significantly
impact areas such as data privacy, algorithmic bias, and automation in various industries. We believe
it’s important to consider these impacts as part of responsible AI development. Although our work
does not directly address these ethical issues, we encourage ongoing dialogue and research in these
areas to ensure the benefits of AI advancements are balanced with societal well-being and ethical
considerations.
References
[1]Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer,
2020.
[2]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien,
Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward
Raff, et al. Pythia: A suite for analyzing large language models across training and scaling. In
International Conference on Machine Learning , pages 2397–2430. PMLR, 2023.
[3]Yonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning
about physical commonsense in natural language. Proceedings of the AAAI Conference on
Artificial Intelligence , 34(05):7432–7439, Apr. 2020. doi: 10.1609/aaai.v34i05.6239. URL
https://ojs.aaai.org/index.php/AAAI/article/view/6239 .
11

--- PAGE 12 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
[4]Sidney Black, Stella Biderman, Eric Hallahan, Quentin Gregory Anthony, Leo Gao, Laurence
Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Martin Pieler,
USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and
Samuel Weinbach. GPT-neox-20b: An open-source autoregressive language model. In
Challenges & Perspectives in Creating Large Language Models , 2022. URL https://
openreview.net/forum?id=HL7IhzS8W5 .
[5]BlinkDL. Rwkv-v4 weights. https://huggingface.co/BlinkDL/ , 2023. Model
weights for RWKV .
[6]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning
challenge. ArXiv , abs/1803.05457, 2018. URL https://api.semanticscholar.org/
CorpusID:3922816 .
[7]Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.
[8]Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and
memory-efficient exact attention with IO-awareness. In Alice H. Oh, Alekh Agarwal, Danielle
Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems ,
2022. URL https://openreview.net/forum?id=H4DqfPSibmx .
[9]Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re.
Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh
International Conference on Learning Representations , 2023. URL https://openreview.
net/forum?id=COZDy0WYGg .
[10] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:
An 800gb dataset of diverse text for language modeling, 2020.
[11] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles
Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas
Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron,
Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A
framework for few-shot language model evaluation, 12 2023. URL https://zenodo.
org/records/10256836 .
[12] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces,
2023.
[13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured
state spaces. In International Conference on Learning Representations , 2022. URL https:
//openreview.net/forum?id=uYLFoz1vlAC .
[14] Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and
Yinfei Yang. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the
Association for Computational Linguistics: NAACL 2022 , pages 724–736, Seattle, United States,
12

--- PAGE 13 ---
ENHANCING TRANSFORMER RNN S WITH MULTIPLE TEMPORAL PERSPECTIVES
July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.55.
URLhttps://aclanthology.org/2022.findings-naacl.55 .
[15] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan
May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. In The Eleventh
International Conference on Learning Representations , 2023. URL https://openreview.
net/forum?id=qNLe3iq2El .
[16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy
Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper un-
derstanding of commonsense stories. In Proceedings of the 2016 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies ,
pages 839–849, San Diego, California, June 2016. Association for Computational Linguistics.
doi: 10.18653/v1/N16-1098. URL https://aclanthology.org/N16-1098 .
[17] Denis Paperno, Germ ´an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,
Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern ´andez. The lambada dataset:
Word prediction requiring a broad discourse context, 2016.
[18] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao,
Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV , Xuzheng He, Haowen Hou,
Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna
Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind,
Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu,
and Rui-Jie Zhu. Rwkv: Reinventing rnns for the trans former era, 2023.
[19] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y . Fu, Tri Dao, Stephen Baccus, Yoshua
Bengio, Stefano Ermon, and Christopher R ´e. Hyena hierarchy: Towards larger convolutional
language models, 2023.
[20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An
adversarial winograd schema challenge at scale. Commun. ACM , 64(9):99–106, aug 2021.
ISSN 0001-0782. doi: 10.1145/3474381. URL https://doi.org/10.1145/3474381 .
[21] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.
ACM Comput. Surv. , 55(6), dec 2022. ISSN 0360-0300. doi: 10.1145/3530811. URL
https://doi.org/10.1145/3530811 .
[22] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
[23] Wikimedia. Wikimedia downloads, 2023. URL https://dumps.wikimedia.org/ .
[24] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag:
Can a machine really finish your sentence? In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics , pages 4791–4800, Florence, Italy,
July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL
https://aclanthology.org/P19-1472 .
13
