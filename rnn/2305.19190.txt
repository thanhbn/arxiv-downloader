# 2305.19190.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rnn/2305.19190.pdf
# File size: 1859713 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2024
INVERSE APPROXIMATION THEORY FOR NONLINEAR
RECURRENT NEURAL NETWORKS
Shida Wang
Department of Mathematics
National University of Singapore
e0622338@u.nus.eduZhong Li
Microsoft Research Asia
lzhong@microsoft.com
Qianxiao Li∗
Department of Mathematics
Institute for Functional Intelligent Materials
National University of Singapore
qianxiao@nus.edu.sg
ABSTRACT
We prove an inverse approximation theorem for the approximation of nonlinear
sequence-to-sequence relationships using recurrent neural networks (RNNs). This
is a so-called Bernstein-type result in approximation theory, which deduces prop-
erties of a target function under the assumption that it can be effectively approx-
imated by a hypothesis space. In particular, we show that nonlinear sequence
relationships that can be stably approximated by nonlinear RNNs must have an
exponential decaying memory structure - a notion that can be made precise. This
extends the previously identified curse of memory in linear RNNs into the gen-
eral nonlinear setting, and quantifies the essential limitations of the RNN archi-
tecture for learning sequential relationships with long-term memory. Based on
the analysis, we propose a principled reparameterization method to overcome the
limitations. Our theoretical results are confirmed by numerical experiments.
1 I NTRODUCTION
Recurrent neural networks (RNNs) (Rumelhart et al., 1986) are one of the most basic machine
learning models to learn the relationship between sequential or temporal data. They have wide
applications from time series prediction (Connor et al., 1994), text generation (Sutskever et al.,
2011), speech recognition (Graves & Jaitly, 2014) to sentiment classification (Tang et al., 2015).
However, when there are long-term dependencies in the data, empirical results (Bengio et al., 1994)
show that RNN may encounter difficulties in learning. In this paper, we investigate this problem
from the view of approximation theory.
From approximation perspective, there are various types of theorems characterizing the connections
between target relationships and model architectures for learning them. Universal approximation
(Achieser, 2013, p. 32) and Jackson-type theorem (Achieser, 2013, p. 187) provide basic guarantees
of approximation and error estimates of sufficiently regular target functions by a particular hypoth-
esis space. A number of such results are available for sequence modeling, including the RNN (Li
et al., 2020; 2022). On the other hand, a relatively under-investigated domain in the machine learn-
ing literature are Bernstein-type theorems (Bernstein, 1914; Li et al., 2022), which are also known as
inverse approximation theorems. These results aims to characterize the regularity of target relation-
ships, assuming that they can be approximated efficiently with a hypothesis space. These regularity
notions intimately depends on, thus gives important insights into, the structure of the hypothesis
space under study.
∗Corresponding author
1arXiv:2305.19190v4  [cs.LG]  6 Feb 2024

--- PAGE 2 ---
Published as a conference paper at ICLR 2024
This paper establishes an inverse approximation result for the approximation of nonlinear function-
als via RNNs. Previous works (Li et al., 2020; 2022) indicate that linear functionals that can be
universally approximated by linear RNNs must have exponential decaying memory. This phenom-
ena was coined the curse of memory for linear RNNs. A natural question is whether the nonlinear
recurrent activation used in practical RNNs changes the situation. This is important since a bigger
hypothesis space may lift restrictions on the target functions. Moreover, it is known that nonlinear
activation is crucial for feed-forward networks to achieve universality (Cybenko, 1989). Thus, it is
worthwhile to investigate whether the linear Bernstein result generalizes to the case of approximat-
ing nonlinear sequence relationships with nonlinear RNNs. In this paper, we prove that nonlinear
RNNs still suffer from a curse of memory in approximation - nonlinear functionals that can be sta-
bly approximated by RNNs with nonlinear activations must have an exponential decaying memory
function. The notions of stable approximation and memory function can be concretely defined. Our
results make precise the empirical observation that the RNN architecture has inherent limitations
when modeling long-time dependencies.
In summary, our main contributions are:
1. We extends the concept of memory function in the linear settings (Li et al., 2020; 2022)
to the nonlinear setting. This memory function can be numerically quantified in sequence
modeling applications.
2. We introduce a notion of stable approximation, which ensures that an approximant has the
possibility to be found by a gradient-based optimization algorithm.
3. We prove, to the best of our knowledge, the first Bernstein-type approximation theorem
for nonlinear functional sequences through nonlinear RNNs. Our results characterize the
essential limit of nonlinear RNNs in learning long-term relationship. Our analysis also
suggests that appropriate parameterization can alleviate the ‘curse of memory’ phenomenon
in learning targets with long memory. The theoretical result is corroborated with numerical
experiments.
Notation. For a sequence of d-dimensional vectors indexed by R,x={xt∈Rd:t∈R},
we denote the supremum norm by ∥x∥∞:= supt∈R|xt|∞. Here |x|∞:= max i|xi|,|x|2:=pP
ix2
i,|x|1:=P
i|xi|are the usual max ( L∞) norm, L2norm and L1norm. The bold face
represents sequence while the normal letters are scalars, vectors or functions. Throughout this paper
we use ∥ · ∥ to denote norms over sequences of vectors, or function(al)s, while | · |(with subscripts)
represents the norm of number, vector or weights tuple. The hat notation in this paper refers to the
hypothesis space (functional) while the original symbol is referring to the target space (functional).
2 P ROBLEM FORMULATION AND PRIOR RESULTS ON LINEAR RNN S
In this section, we introduce the problem formulation of sequence modeling as a functional sequence
approximation problem. We pay particular attention to distinguish two types of results: forward
(Jackson-type) and inverse (Bernstein-type) approximation theorems. For approximation theory in
machine learning, most existing results focus on forward theorems. However, inverse approximation
theorems are of significant importance in revealing the fundamental limitations of an approximation
approach. The present paper focuses on establishing such results in the general, non-linear setting.
We conclude this section with a review of known Bernstein-type estimates, which is currently re-
stricted to the linear case. In so doing, we highlight the definition of memory in the linear case,
which motivates our general definition of memory for nonlinear functional sequences in Section 3.1.
The relationship between memory and approximation is central to our results.
2.1 T HE APPROXIMATION PROBLEM FOR SEQUENCE MODELING
The goal of sequential modeling is to learn a relationship between an input sequence x={xt}
and a corresponding output sequence y={yt}. For ease of analysis, we adopt the continuous-
time setting in (Li et al., 2020), where t∈R. This is also a natural setting for irregularly sampled
time series (Lechner & Hasani, 2020). The input sequence space is X=C0(R,Rd), the space of
continuous functions vanishing at infinity. We assume the input and output sequences are related
by a sequence of functionals H={Ht:X 7→R;t∈R}viayt=Ht(x), t∈R. The sequential
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2024
approximation problem can be formulated as the approximation of the target functional sequence H
by a functional sequence bHfrom a model hypothesis space such as RNNs.
Forward and inverse approximation theorems. Given a hypothesis space bH(m)of complexity
m≥1(e.g. width- mRNNs), forward approximation theorems, also called Jackson-type theorems,
bound the optimal approximation error inf
bH∈bH(m)∥H−bH∥ ≤C(H, m).
Inverse approximation (Bernstein-type) results are “converse” statements to Jackson-type results.
From the starting assumption that an efficient approximation exists for a specific target H, Bernstein-
type results deduce the approximation spaces that Hought to belong to, i.e. it identifies a com-
plexity or regularity measure C(·)and show that C(H)is necessarily finite. Take the trigono-
metric polynomial approximation as an example, Bernstein (Achieser, 2013, p. 187) proved that if
inf
bH∈bH(m)∥H−bH∥ ≤c
mα+δ, for all m≥1and some δ >0,c >0, then C(H) =|H(α)|<∞, i.e.
Hmust be α-times differentiable with δ-H¨older continuous derivatives.
Bernstein-type inverse approximation results are important in characterizing the approximation ca-
pabilities for hypothesis spaces. For trigonometric polynomial example, it says that only smooth
functions can be efficiently approximated, thereby placing a concrete limitation on the approxima-
tion capabilities of these models. Our goal in this paper is to deduce analogues of this result, but
for the approximation of general nonlinear functional sequences by RNNs. Unlike the classical case
where the notion of regularity enters in the form of smoothness, here we shall investigate the concept
of memory as a quantifier of regularity - a notion that we will make precise subsequently.
2.2 T HERNN ARCHITECTURE AND PRIOR RESULTS
The continuous-time RNN architecture parameterizes functional sequences by the introduction of a
hidden dynamical system
dht
dt=σ(Wht+Uxt+b)h−∞= 0,
ˆyt=c⊤ht, t ∈R.(1)
Here, ˆyt∈Ris the predicted output sequence value, and ht∈Rmdenotes the hidden state1.
The well-definedness of the continuous-time RNN will be discussed in Appendix A.1. The hyper-
parameter mis also known as the hidden dimension, or width, of recurrent neural networks. For
different hidden dimensions m, the RNN is parameterized by trainable weights (W, U, b, c ), where
W∈Rm×mis the recurrent kernel, U∈Rm×dis the input kernel, b∈Rmis the bias and c∈Rmis
the readout. The complexity of the RNN hypothesis space is characterized by the hidden dimension
m. The nonlinearity arises from the activation function σ(·), which is a scalar function performed
element-wise, such as tanh,hardtanh ,sigmoid orReLU . The hypothesis space of RNNs is thus the
following functional sequence space
bH(m)
RNN={x7→ˆ yvia Equation (1) :W∈Rm×m, U∈Rm×d, b∈Rm, c∈Rm}. (2)
Before presenting our main results, we review known Jackson and Bernstein-type results established
for linear RNNs, corresponding to setting σ(z) =zandb= 0in equation 1. We shall pay attention
to the definition of memory for a target functional sequence, and how it relates to approximation
properties under the RNN hypothesis space.
We begin with some definitions on (sequences of) functionals as introduced in (Li et al., 2020).
Definition 2.1. LetH={Ht:X 7→ R;t∈R}be a sequence of functionals. Without
loss of generality, we assume Ht(0) = 0 (Otherwise we can consider the adjusted relationship
Hadjusted
t (x) :=Ht(x)−Ht(0)).
1. (Linear )Htis linear if for any λ, λ′∈Randx,x′∈ X ,Ht(λx+λ′x′) =λHt(x) +
λ′Ht(x′).
1The boundary condition h−∞ = 0 is consistent with practical implementations such as TensorFlow and
PyTorch, where the initial value of hidden state is set to be zero by default.
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2024
2. (Continuous )Htis continuous if for any x,′x∈ X,limx′→x|Ht(x′)−Ht(x)|= 0.
3. (Bounded )Htis bounded if sup{x∈X,x̸=0}|Ht(x)|
∥x∥∞<∞.
4. (Time-homogeneous )H={Ht:t∈R}is time-homogeneous (or shift-equivariant) if
the input-output relationship commutes with time shift: let [Sτ(x)]t=xt−τbe a shift
operator, then H(Sτx) =SτH(x)
5. (Causal )Htis causal if it does not depend on future values of the input. That is, if x,x′
satisfy xt=x′
tfor any t≤t0, then Ht(x) =Ht(x′)for any t≤t0.
6. (Regular )Htis regular if for any sequence {x(n):n∈N}such that x(n)
s→0for almost
every s∈R, then limn→∞Ht(x(n)) = 0 .
The works in Li et al. (2020; 2022) study the approximation of functional sequences satisfying
Definition 2.1 by linear RNNs. A key idea is showing that any such functional sequence Hadmits a
Riesz representation (see Appendix A.2 and Appendix A.3)
Ht(x) =Z∞
0ρ(s)⊤xt−sds, t ∈R. (3)
In this sense, ρcompletely determines H, and its approximation using linear RNNs can be reduced to
the study of the approximation of ρ∈L1([0,∞),Rd)by exponential sums of the form (c⊤eWsU)⊤.
An important observation here is that ρcaptures the memory pattern of the target linear functional
sequence: if ρdecays rapidly, then the target has short memory, and vice versa.
By assuming that a target functional sequence Hcan be approximated uniformly by stable RNNs,
then the memory of the target functional sequence must satisfy eβ0t|ρ(t)|1=o(1)ast→ ∞ for
some β0>0. This was coined the “curse of memory” (Li et al., 2020; 2022) and reveals fundamental
limitations of the RNN architecture to capture long-term memory structures.
The focus of this paper is to investigate whether the addition of nonlinear activation changes this
result. In other words, would the curse of memory hold for nonlinear RNNs in the approximation of
suitably general nonlinear functionals? This is a meaningful question, since Bernstein-type results
essentially constrain approximation spaces, and so a larger hypothesis space may relax such con-
straints. We expand on this in Appendix A.4. A significant challenge in the nonlinear setting is the
lack of a Riesz representation result, and thus one needs to carefully define a notion of memory that
is consistent with ρin the linear case, but can still be used in the nonlinear setting to prove inverse
approximation theorems. Moreover, we will need to introduce a general notion of approximation
stability, which together with the generalized memory definition allows us to derive a Bernstein-type
result that holds beyond the linear case.
3 M AIN RESULTS
In this section, we establish a Bernstein-type approximation result for nonlinear functional se-
quences using nonlinear RNNs. We first give a definition of memory function for nonlinear function-
als. It is compatible with the memory definition in the linear functionals and it can be queried and
verified in applications. Next, we propose the framework of stable approximation. It is a mild re-
quirement from the perspective of approximation, but a desirable one from the view of optimization.
Moreover, we show that any linear functional with an exponential decaying memory can be sta-
bly approximated. Based on the memory function definition and stable approximation framework,
we prove a Bernstein-type theorem. The theorem shows that any nonlinear functionals that can
be stably approximated by general nonlinear RNNs must have an exponentially decaying memory,
which confirms that the curse-of-memory phenomenon is not limited to the linear case. Numerical
verifications are included to demonstrate the result.
3.1 M EMORY FUNCTION FOR NONLINEAR FUNCTIONALS
Recall that the memory for a linear functional sequence is defined by its Riesz representation in
Equation (3). While there are no known general analogues of Riesz representation for nonlinear
functionals, we may consider other means to extract an effective memory function from H.
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2024
Letx∈Rdand consider the following Heaviside input sequence ux
t=x·1[0,∞)(t) =x t≥0,
0t <0.
In the linear case, notice that according to Equation (3)
sup
x̸=0d
dtHt(ux)
∥ux∥∞= sup
x̸=0|x⊤ρ(t)|
|x|∞=|ρ(t)|1. (4)
Hence, conditions on |ρ(t)|1may be replaced by conditions on the left hand side, which is well-
defined also for nonlinear functionals. This motivates the following definition of memory function
for nonlinear functional sequences.
Definition 3.1 (Memory function of nonlinear functional sequences) .For continuous, causal, regu-
lar and time-homogeneous functional sequences H={Ht(x) :t∈R}onX, define the following
function as the memory function ofHover bounded Heaviside input ux:
M(H)(t) := sup
x̸=01
|x|∞d
dtHt(ux). (5)
In particular, in this paper we consider nonlinear functionals whose memory function is finite for all
t. Unlike traditional methods that evaluate memory through heuristic tasks, our approach offers a
precise, task-independent characterization of model memories. If the oracle of the target functional
is available, the memory function can be evaluated and the result is named queried memory. In
Appendix F and Appendix G, we discuss the memory function evaluated over different test inputs
and show the numerical equivalence in Appendix H. Without target functional oracle, we may ap-
proximate the target with the learned model and still evaluate the memory function. If the queried
memory are decaying for all Heaviside inputs, then we say the corresponding nonlinear functional
sequence has a decaying memory. We demonstrate in Appendix B that the memory querying shows
the memory pattern of LSTM and bidirectional LSTM sequence-to-sequence models in sentiment
analysis on IMDB movie reviews.
Definition 3.2 (Decaying memory) .For continuous, causal, regular and time-homogeneous func-
tional sequences H={Ht(x) :t∈R}onX, we say it has a decaying memory if:
lim
t→∞M(H)(t) = 0 . (6)
We say that Hhas an exponential decaying memory if for some β >0,
lim
t→∞eβtM(H)(t) = 0 . (7)
Furthermore, the family {Hm}has a uniformly decaying memory if
lim
t→∞sup
mM(Hm)(t) = 0 . (8)
Remark 3.3.The requirement of decaying memory on time-homogeneous functionals is mild since
it is satisfied ifdHt
dtis continuous at Heaviside input, under the topology of point-wise convergence
(see Appendix A.5). We show thatdHt
dtare point-wise continuous over Heaviside inputs for all
RNNs, thus RNNs have decaying memory (see Appendix A.6). Another related notion of fading
memory is discussed in the Appendix A.7.
3.2 S TABLE APPROXIMATION
We now introduce the stable approximation framework. Let us write the hypothesis space bH(m)
as a parametric space bH(m)={bH(·;θm) :θm∈Θm}where for each m,Θmis a subset of a
Euclidean space with dimension depending on m, representing the parameter space defining the
hypothesis and bHis a parametric model. For example, in the case of RNNs, the parameter θmis
(Wm, Um, bm, cm)∈Θm:={Rm×m×Rm×d×Rm×Rm}andmis the hidden dimension of the
RNN.
Let us consider a collection of functional sequences {bHm=bH(·;θm) :m≥1}that approximates
a target functional sequence H. Stable approximation requires that, if one were to perturb each
parameter θmby a small amount, the resulting approximant sequence should still have a continuous
perturbation error. For the gradient-based optimization, this condition is necessary for one to find
such an approximant sequence, as small perturbations of parameters should keep perturbation error
continuous for gradients to be computed. We now define this notion of stability precisely.
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2024
103
102
101
100101
Perturbation 
104
103
102
101
100101102Perturbation error Em()
0
0
m=2
m=4
m=6
m=8
m=14
(a) Target with exponential decay memory
103
102
101
100101
Perturbation 
104
103
102
101
100101102Perturbation error Em()
0
m=2
m=4
m=6
m=8
m=14 (b) Target with polynomial decay memory
Figure 1: Perturbation errors for linear functionals with different decaying memory. The antici-
pated limiting curve E(β)is marked with a black dashed line. (a) For linear functional sequences
with exponential decaying memory, there exists a perturbation radius β0such that the perturbation
error E(β)for0≤β < β 0is continuous. (b) Approximation of linear functional sequences with
polynomial decaying memory. As hidden dimension mincreases, the perturbation radius where the
error remains small decreases, suggesting that there may not exist a β0achieving the stable approx-
imation condition. The intersections of lines are shifting left as the hidden dimension mincreases.
The anticipated limiting curve E(β)is not continous for the polynomial decaying memory target.
Definition 3.4. For target Hand parameterized model bH(·, θm), we define the perturbation error
for hidden dimension mto be:
Em(β) := sup
˜θm∈{θ:|θ−θm|2≤β}∥H−bH(·;˜θm)∥ (9)
Moreover, E(β) := lim supm→∞Em(β)is the (asymptotic) perturbation error. Here |θ|2:=
max(|W|2,|U|2,|b|2,|c|2).
Definition 3.5 (Stable approximation via parameterized models) .Letβ0>0. We say a target
functional sequence Hadmits a β0-stable approximation under {bH(m)}, if there exists a sequence
of parameterized approximants bHm=bH(·, θm),θm∈Θmsuch that
lim
m→∞∥H−bHm∥ →0, (10)
and for all 0≤β≤β0, the perturbation error satisfies that E(β)is continuous in βfor0≤β≤β0.
Remark 3.6.It can be seen that approximation only requires E(0) = 0 . Therefore the stable approx-
imation condition generalizes the approximation by requiring the continuity of Earound β= 0. If
an approximation is unstable ( E(0) = 0 ,limβ→0E(β)>0), it is difficult to be found by gradient-
based optimizations. Since our notion of stability depends on the size of weight perturbations, one
may wonder whether rescaling the norm of weights separately for each machieves stability. There
are two issues with this approach. First, the rescaled version is no-longer the usual RNN hypoth-
esis space. Second, to achieve stability the rescaling rule may depend on information of the target
functional sequence, which we have no access to in practice. We discuss this in detail in Appendix C.
Next, we demonstrate that the stable approximation condition is not too stringent in the sense that,
for linear functional sequence with exponential decaying memory (Equation (7)) admits a stable
approximation. We show the numerical verification of this result in Figure 1. The approximation
of linear functional with exponential decay can be seen in the left panel at β= 0 since increasing
the hidden dimension mwill make the estimated error decrease to 0overβ∈[0, β0]. Stable
approximation can be verified that for positive perturbation β, adding the hidden dimension does
not increase the perturbation error E(β). In contrast, for linear functional with polynomial decaying
memory, the perturbation error E(β)is not continuous at β= 0.
3.3 B ERNSTEIN -TYPE APPROXIMATION RESULT FOR NONLINEAR RNN S
We now present the main result of this paper, which is a Bernstein-type approximation result for
nonlinear functional sequences using nonlinear RNNs. The key question is whether the addition
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2024
of nonlinearity alleviates the curse of memory limitation and allows an efficient approximation of
functionals with slow memory decay. In the following, we show that the answer is negative, and a
similar Bernstein-type approximation result holds for nonlinear functionals and RNNs with a class
of recurrent activations, including the most often used hardtanh/tanh activations.
Definition 3.7. We consider the Sobolev-type norm:
H−bH
W1= sup
t 
∥Ht−bHt∥∞+dHt
dt−dbHt
dt
∞!
. (11)
The nonlinear functional norm is given by ∥Ht∥∞:= supx̸=0|Ht(x)|
∥x∥∞+|Ht(0)|= supx̸=0|Ht(x)|
∥x∥∞.
Definition 3.8. We consider the following family of bounded monotone Lipschitz continuous
activations which are locally-linear/locally-tanh around 0: For some Z0>0,
A0:={σ(·)|σ(z) =cσz, cσ>0,|z|< Z 0}, (12)
A1:={σ(·)|σ(0) = 0; σdifferentiable , σ′(z) =a−bσ(z)2, a, b≥0,|z|< Z 0}. (13)
Notice A0∪ A 1includes the commonly used activations such as hardtanh and tanh. In particular,
tanh corresponds to the case a=b= 1forA1withZ0=∞.
Theorem 3.9. Assume His a sequence of bounded continuous, causal, regular and time-
homogeneous functionals on Xwith decaying memory. Let the activation be in A0∪ A 1. Sup-
poseHisβ0-stably approximated by a sequence of RNNs {bH(·, θm)}∞
m=1in the norm defined in
Equation (11). If the perturbed models’ memory functions are uniformly decaying (as defined in
Definition 3.2) and the weight norms are uniformly bounded in m:
sup
m|θm|2<∞. (14)
Then the memory function M(H)(t)of the target decays exponentially:
lim
t→∞eβtM(H)(t) = 0 , β < β 0. (15)
The proofs are included in the Appendix A.8. Given that approximations are required to be stable,
the decaying memory property ensures that the derivative of the hidden states for the perturbed
model approaches 0 as time t→ ∞ . Using the Hartman-Grobman theorem, we can obtain bounds
on the eigenvalues of the matrices Wm. In Appendix J, we demonstrate that our methods can be
generalized to analyze the dynamics of GRU and LSTM. The framework is similar while the final
hidden dynamics of GRU and LSTM require more techniques to analyze.
Interpretation of Theorem 3.9. Our main result (Theorem 3.9) extends the previous linear result
from Li et al. (2022). Instead of smoothness (measured by the Sobolev norm) as a regularity mea-
sure, the RNN Bernstein-type result identifies exponential decaying memory ( eβtM(H)(t)→0) as
the right regularity measure. If we can approximate some target functionals stably using nonlinear
RNN, then that target must have exponential decaying memory. Previously this was only known
for linear case, but for nonlinear case, even addition of nonlinearity substantially increases model
complexity, it does not fix the essential memory limitation of RNNs.
From the numerical perspective, the theorem implies the following two statements, and we provide
numerical verification for each of them. First, if the memory function of a target functional sequence
decays slower than exponential (e.g. M(H)(t) =C
(t+1)1.5), the optimization is difficult and the
approximation in Figure 2 is achieved at 1000 epochs while typically exponential decaying memory
achieves the approximation at 10 epochs. When the approximation is achieved, it can be seen in
Figure 2 that, for larger perturbation scale β, there is no perturbation stability. Second, if a target
functional sequence can be well-approximated and the approximation’s stability radius β0can be
shown to be positive, then the target functional sequence should have exponential decaying memory.
See Figure 3 for the approximation filtered with perturbation stability requirement. (See Figure 5 in
Appendix B for the validation of memory over general sentiment classification task.)
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2024
105
104
103
102
101
100
perturbation 
102
101
100101102Perturbation error E()
m=4
m=8
m=16
m=64
Figure 2: Target with polynomial decaying memory + approximation (achieved at 1000 epochs) →
no stability. Similar to the linear functional case, when approximating nonlinear functionals with
polynomial decaying memory by tanh RNN, the intersections of curves are shifting left as the hidden
dimension mincreases.
1010
108
106
104
102
100
Perturbation 
102
101
100101Perturbed error Em()
stability of the approximation
m=4
m=8
m=16
m=32
m=64
0 20 40 60 80 100
Time t105
104
103
102
Memory function (H)(t)
memory function of the target functional
Figure 3: Stable approximation via RNNs implies exponential decaying memory. We construct sev-
eral randomly-initialized RNN models as teacher models with large hidden dimension ( m= 256 ).
When approximating the teacher model with a series of student RNN models, we can numerically
verify the approximation’s stability (left panel). We can apply a filtering: we only select those
teacher models which both can be approximated, and the approximations are stable (with perturba-
tion error Em(β)having a positive stability radius). We found that the only teachers that remain are
those with exponential decaying memory functions. An example is shown in the right panel.
3.4 S UITABLE PARAMETRIZATION ENABLES STABLE APPROXIMATION
The key insight of Theorem 3.9 can be summarized as follow: in order to approximate targets with
non-exponential decaying memory, the recurrent weights of RNNs must have eigenvalues’ real part
approaching 0 on the negative side. However, if the largest eigenvalue real parts are approaching
zero, then its stability under perturbation will decrease. This is why the approximation and stability
cannot be achieved at the same time if the target’s memory does not decay exponentially. The
stability problem can be resolved via reparameterization as the stability radius is not decreasing
even when the eigenvalues are approaching 0. If we reparameterize the recurrent weights so that
it approach zero and remain stable (i.e., eigenvalue real part being negative) under perturbations,
then this architecture will maintain stability while having the possibility of approximation. We can
accomplish this by substituting the recurrent weight with a continuous matrix function, which we
will refer to as “stable reparameterization”
g:Rm×m→Rm×m,−, g(M) =W. (16)
This reparameterized RNN is stable as the eigenvalues’ real part are always negative. We show there
are several methods to achieve this reparameterization: The exponential function g(M) =−eM
and the softplus function g(M) =−log(1 + eM)maps the eigenvalues of Mto negative range
(see Figure 4 and Figure 8 for the stable approximation of linear functional with polynomial decay
memory). LRU (Orvieto et al., 2023) proposed to parameterizes the real part of eigenvalues by
exp(−exp(λ)), which corresponds to the discrete case for g(M) =−eM.
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2024
103
102
101
100101
Perturbation 
104
103
102
101
100101102Perturbation error Em()
0
m=2
m=4
m=6
m=8
m=14
(a) Exp reparameterization + Linear RNN
103
102
101
100101
Perturbation 
104
103
102
101
100101102Perturbation error Em()
0
m=2
m=4
m=6
m=8
m=14 (b) Softplus reparameterization + Linear RNN
Figure 4: Stable approximation of linear functionals with polynomial decay memory by linear RNN
with exp and softplus reparameterization. The limiting dashed curve E(β)shall be continuous.
4 R ELATED WORK
Various results have been established in RNNs approximation theory, see Sontag (1998); Hanson
et al. (2021) and references therein. For unbounded input index sets, Lpapproximation is estab-
lished by Gonon & Ortega (2020). In Gonon & Ortega (2021), the universal approximation theorem
is constructed for functionals with fading memory in the discrete time setting. In Li et al. (2020), the
universal approximation theorem and Jackson-type approximation theorem characterize the density
and speed of linear RNNs applied to linear functionals. Most existing results are forward (Jackson-
type) approximation theorems, which upper bound the optimal approximation error. Of most rele-
vance is the Bernstein-type result proved in Li et al. (2022), where it has been proved that the linear
functional sequences that can be efficiently approximated by linear RNNs must have an exponential
decaying memory. However, the main limitation of the above result is the linear setting for both
models and targets.
The notion of approximation stability is one of the central concepts we exploit in this paper. We note
that in classical approximation theory, stable approximation has numerous definitions depending on
the setting (DeV ore et al., 2021). For example, in nonlinear approximation (DeV ore, 1998), a stably
approximating sequence {Hm}ofHis one that satisfies |Hm| ≤C|H|for some absolute constant
C > 0and all m. This approach is taken to show the non-existence of stable procedure to approx-
imating functions from equally-spaced samples with exponential convergence on analytic functions
(Platte et al., 2011). This notion of stability is on the conditioning of the approximation problem.
In contrast, our notion of stability introduced in Section 3.2 is more similar to a uniform continuity
requirement. Pertaining to sequence modeling, a related but different notion of dynamic stability
(Hanson & Raginsky, 2020) was used to prove a Jackson-type results for universal simulation of
dynamical systems. There, the stability is akin to requiring the uniform (in inputs) continuity of the
flow-map of the RNN hidden dynamics. In practice, some specific forms of the stable reparameter-
ization we defined in Equation (16) has been adopted in state-space models optimization (Gu et al.,
2020; 2021; Smith et al., 2023; Wang & Xue, 2023).
5 C ONCLUSION
In summary, we derive an inverse approximation result in the setting of sequence modeling using
nonlinear RNNs. We show that, assuming that a given target sequence relationship (mathemati-
cally understood as a nonlinear functional sequence) can be stably approximated by RNNs with
nonlinear activations, then the target functional sequence’s memory structure must be exponentially
decreasing. This places a priori limitations on the ability of RNNs to learn long-term memory in
sequence modeling, and makes precise the empirical observation that RNNs do not perform well
for such problems. From the approximation viewpoint, our results show that this failure is not only
due to learning algorithms (e.g. explosion of gradients), but also due to fundamental limitations of
the RNN hypothesis space. At the same time, our analysis points to reparameterization as a princi-
pled methodology to remedy the limitations of RNN when it comes to long-term memory and we
demonstrate its effectiveness in by learning linear functionals with polynomial memory.
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2024
ACKNOWLEDGMENTS
This research is supported by the National Research Foundation, Singapore, under the NRF fellow-
ship (project No. NRF-NRFF13-2021-0005). Shida Wang is supported by NUS-RMI Scholarship.
REFERENCES
N. I. Achieser. Theory of Approximation . Courier Corporation, June 2013. ISBN 978-0-486-15313-
1.
Y . Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is
difficult. IEEE Transactions on Neural Networks , 5(2):157–166, March 1994. ISSN 1941-0093.
doi: 10.1109/72.279181.
Serge Bernstein. Sur la meilleure approximation de —x— par des polynomes de degr ´es donn ´es.
Acta Mathematica , 37(1):1–57, December 1914. ISSN 1871-2509. doi: 10.1007/BF02401828.
Stephen Boyd, L. O. Chua, and C. A. Desoer. Analytical Foundations of V olterra Series. IMA
Journal of Mathematical Control and Information , 1(3):243–282, January 1984. ISSN 0265-
0754. doi: 10.1093/imamci/1.3.243.
Jerome T. Connor, R. Douglas Martin, and Les E. Atlas. Recurrent neural networks and robust time
series prediction. IEEE transactions on neural networks , 5(2):240–254, 1994.
G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Con-
trol, Signals and Systems , 2(4):303–314, December 1989. ISSN 1435-568X. doi: 10.1007/
BF02551274.
Ronald DeV ore, Boris Hanin, and Guergana Petrova. Neural network approximation. Acta Numer-
ica, 30:327–444, 2021.
Ronald A DeV ore. Nonlinear approximation. Acta numerica , 7:51–150, 1998. ISSN 1474-0508.
Lukas Gonon and Juan-Pablo Ortega. Reservoir Computing Universality With Stochastic Inputs.
IEEE Transactions on Neural Networks and Learning Systems , 31(1):100–112, January 2020.
ISSN 2162-2388. doi: 10.1109/TNNLS.2019.2899649.
Lukas Gonon and Juan-Pablo Ortega. Fading memory echo state networks are universal. Neural
Networks , 138:10–13, 2021.
Alex Graves and Navdeep Jaitly. Towards End-To-End Speech Recognition with Recurrent Neural
Networks. In Proceedings of the 31st International Conference on Machine Learning , pp. 1764–
1772. PMLR, June 2014.
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R ´e. HiPPO: Recurrent Memory
with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems ,
volume 33, pp. 1474–1487. Curran Associates, Inc., 2020.
Albert Gu, Karan Goel, and Christopher Re. Efficiently Modeling Long Sequences with Structured
State Spaces. In International Conference on Learning Representations , October 2021.
Joshua Hanson and Maxim Raginsky. Universal Simulation of Stable Dynamical Systems by Recur-
rent Neural Nets. In Proceedings of the 2nd Conference on Learning for Dynamics and Control ,
pp. 384–392. PMLR, July 2020.
Joshua Hanson, Maxim Raginsky, and Eduardo Sontag. Learning Recurrent Neural Net Models of
Nonlinear Systems. In Proceedings of the 3rd Conference on Learning for Dynamics and Control ,
pp. 425–435. PMLR, May 2021.
Hassan K. Khalil. Nonlinear systems third edition (2002), 2002.
Mathias Lechner and Ramin Hasani. Learning long-term dependencies in irregularly-sampled time
series. arXiv preprint arXiv:2006.04418 , 2020.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2024
Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. On the Curse of Memory in Recurrent Neural
Networks: Approximation and Optimization Analysis. In International Conference on Learning
Representations , October 2020.
Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and Optimization Theory for
Linear Continuous-Time Recurrent Neural Networks. Journal of Machine Learning Research , 23
(42):1–85, 2022. ISSN 1533-7928.
Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pas-
canu, and Soham De. Resurrecting Recurrent Neural Networks for Long Sequences, March 2023.
Rodrigo B. Platte, Lloyd N. Trefethen, and Arno B. J. Kuijlaars. Impossibility of Fast Stable Ap-
proximation of Analytic Functions from Equispaced Samples. SIAM Review , 53(2):308–318,
January 2011. ISSN 0036-1445. doi: 10.1137/090774707.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning representations by
back-propagating errors. Nature , 323(6088):533–536, October 1986. ISSN 1476-4687. doi:
10.1038/323533a0.
Jimmy T. H. Smith, Andrew Warrington, and Scott Linderman. Simplified State Space Layers for
Sequence Modeling. In International Conference on Learning Representations , February 2023.
Eduardo D. Sontag. A learning result for continuous-time recurrent neural networks. Systems &
Control Letters , 34(3):151–158, June 1998. ISSN 01676911. doi: 10.1016/S0167-6911(98)
00006-1.
Ilya Sutskever, James Martens, and Geoffrey Hinton. Generating Text with Recurrent Neural Net-
works. In International Conference on Machine Learning , pp. 1017–1024, January 2011.
Duyu Tang, Bing Qin, and Ting Liu. Document Modeling with Gated Recurrent Neural Network
for Sentiment Classification. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing , pp. 1422–1432, Lisbon, Portugal, September 2015. Association
for Computational Linguistics. doi: 10.18653/v1/D15-1167.
Shida Wang and Beichen Xue. State-space models with layer-wise nonlinearity are universal approx-
imators with exponential decaying memory. In Thirty-Seventh Conference on Neural Information
Processing Systems , November 2023.
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2024
A T HEORETICAL RESULTS AND PROOFS
In this section, we first attach the famous Riesz representation theorem and the previous results for
linear RNNs in Section A.2 and Section A.3.
A.1 W ELL-DEFINEDNESS OF CONTINUOUS -TIME RNN
Here we discuss the well-definedness of continuous-time nonlinear RNNs. The −∞ boundary con-
dition is chosen to make the time-homogeneity well-defined, representing a more general scenario
than the boundary at 0. Regarding issues on the input continuity, Heaviside inputs are point-wise
limit of smooth inputs ux= lim k→∞ux
k,cont . Therefore the outputs of continuous targets function-
als are well-defined: Ht(ux) =Ht(lim k→∞ux
k,cont ) = lim k→∞Ht(ux
k,cont ).
A.2 R IESZ REPRESENTATION THEOREM
Theorem A.1 (Riesz-Markov-Kakutani representation theorem) .Assume H:C0(Rd)7→Ris
a linear and continuous functional. Then there exists a unique, vector-valued, regular, countably
additive signed measure µonRsuch that
H(x) =Z
Rx⊤
sdµ(s) =dX
i=1Z
Rxs,idµi(s). (17)
In addition, we have ∥H∥:= sup∥x∥X≤1|H(x)|=∥µ∥1(R) :=P
i|µi|(R).
Based on the representation theorem, one can further obtain that the functionals {Ht:t∈R}
satisfying properties in Definition 2.1 can be represented by the following convolutional form
Ht(x) =Z∞
0x⊤
t−sρ(s)ds, t ∈R, (18)
and the representation function ρ: [0,∞)→Rdis a measurable and integrable function with
∥ρ∥L1([0,∞))= supt∈R∥Ht∥. The details can be found in Appendix A.3 or Li et al. (2020), where
the causality and time-homogeneity play a key role.
A.3 T HREE TYPES OF APPROXIMATION FOR LINEAR FUNCTIONALS AND LINEAR RNN S
We include the statements of the universal approximation theorem and approximation rate for linear
functionals by linear RNNs from Li et al. (2020).
Theorem A.2 (Universal approximation for linear functionals by linear RNNs (Li et al., 2020)) .
Let{Ht:t∈R}be a sequence of linear, continuous, causal, regular and time-homogeneous
functionals defined on X. Then, for any ϵ >0, there exists {¯Ht:t∈R} ∈¯Hlinearsuch that
sup
t∈R∥Ht−¯Ht∥ ≡sup
t∈Rsup
∥x∥X≤1|Ht(x)−¯Ht(x)| ≤ϵ. (19)
Theorem A.3 (Jackson-type approximation rate for linear functionals by linear RNNs (Li et al.,
2020)) .Assume the same conditions as Theorem A.2. Consider the output of constant signals
yi(t) =Ht(ei), i= 1, . . . , d,
where eiis a constant signal with ei,t=ei1{t≥0}, and{ei}d
i=1denote the standard basis vectors in
Rd. Suppose there exist constants α∈N+, β, γ > 0such that for i= 1, . . . , d ,k= 1, . . . , α + 1,
yi(t)∈C(α+1)(R)and
eβty(k)
i(t) =o(1), as t →+∞,
sup
t≥0|eβty(k)
i(t)|
βk≤γ.
Then, there exists a universal constant C(α)only depending on α, such that for any m∈N+, there
exists a sequence of width- mRNN functionals {¯Ht:t∈R} ∈¯Hlinear
m such that
R(m) = sup
t∈R∥Ht−¯Ht∥ ≡sup
t∈Rsup
∥x∥X≤1|Ht(x)−¯Ht(x)| ≤C(α)γd
βmα. (20)
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2024
Theorem A.4 (Bernstein-type approximation for linear functionals by linear RNNs (Li et al., 2022)) .
Let{Ht:t∈R}be a sequence of linear, continuous, causal, regular and time-homogeneous
functionals defined on X. Consider the output of constant signals
yi(t) =Ht(ei)∈C(α+1)(R), i∈1, . . . , d, α ∈N+. (21)
Suppose that for each m∈N+, there exists a sequence of width- mRNN functionals {¯Ht:t∈R} ∈
¯Hlinear
m approximating Htin the following sense:
lim
m→∞sup
t≥0|¯y(k)
i,m(t)−y(k)
i(t)|= 0, i= 1, . . . , d, k = 1, . . . , α + 1, (22)
where
¯yi,m(t) =¯Ht(ei), i= 1, . . . , d. (23)
Define wm= max j∈[m]Re(λj), where {λj}m
j=1are the eigenvalues of ¯Win{¯Ht:t∈R}.
Assume the parameters are uniformly bounded and there exists a constant β > 0such that
lim supm→∞wm<−β, then we have
eβty(k)
i(t) =o(1)ast→+∞, i= 1, . . . , d, k = 1, . . . , α + 1. (24)
A.4 W HY SOLVE CURSE OF MEMORY BY ADDING NONLINEARITY ?
In feed-forward neural networks, the inclusion of nonlinear activation is known to fundamentally
change the approximation capacities of multi-layer networks. Without this nonlinear activation,
the model simply performs a linear transformation. In sequence modeling, linear RNNs serve as
universal approximators for linear functionals. However, they struggle to learn linear functionals
with polynomial decaying memory. Given this context, one might naturally wonder if incorporating
nonlinear activation could expand the hypothesis space in such a way as to overcome this ‘curse of
memory’ problem.
Section 3.3 indicates that the presence of nonlinear activation does not fundamentally alter the mem-
ory pattern of models. Consequently, for stable approximation, targets must demonstrate an expo-
nentially decaying memory. The proofs also show that hardtanh/tanh RNNs require the eigenvalues
to be bounded away from 0.
The findings in Section 3.3 lead to the proposal of stable parameterisation as a potential solution
for long-memory learning. Stable parameterisation ensures that the real parts of eigenvalues for the
recurrent weight matrix remain negative, even when the weights undergo perturbations. In Figure 4,
we demonstrate that linear RNNs can stably approximate linear functionals with polynomial decay
memory when the recurrent weight matrix is parameterised using exponential/softplus parameterisa-
tion. Moreover, by selecting linear functionals with either exponential or polynomial decay memory,
we can clearly change the targets’ memory structures.
A.5 P OINT -WISE CONTINUITY LEADS TO DECAYING MEMORY
Here we give the proof of decaying memory based on the point-wise continuity ofdHt
dtand bound-
edness and time-homogeneity of H:
Proof.
lim
t→∞dHt
dt(ux)= lim
t→∞dH0
dt(x·1{s≥−t})=dH0
dt(x)= 0.
The first equation comes from time-homogeneity. The second equation is derived from the point-
wise continuity where input xmeans constant xfor all time x=x·1{s≥−∞} . Again by time-
homogeneity, the output over constant input is the same: Ht(x) =Hs(x)for all s, t. Therefore
|dH0
dt(x)|= 0.
A.6 P OINT -WISE CONTINUITY OF NONLINEAR RNN S
We prove that nonlinear RNNs with Lipschitz continuous activations are point-wise continuous over
bounded, piece-wise constant input sequences, including the Heaviside inputs.
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2024
Proof. Without loss of generality, assume t >0. The following | · |refers to p=∞norm.
Assume the continuous inputs sequence converges to some bounded piece-wise constant input se-
quence xin point-wise sense: limk→∞xk=x. For any bounded interval [−B, B], we can see the
outputs are well-defined and the point-wise (in t) convergence holds:
Lethk,tandhtbe the hidden states over xkandx. By definition and triangular inequality
d|hk,t−ht|
dt=|σ(Whk,t+Uxk,t)−σ(Wht+Uxt)| ≤L(|W||hk,t−ht|+|U||xk,t−xt|).
Here Lis the Lipschitz constant of activation σ.
Apply Gr ¨onwall inequality to the above inequality, we have:
|hk,t−ht| ≤Zt
0eL|W|(t−s)L|U||xk,s−xs|ds
As the inputs are bounded, by dominated convergence theorem we have RHS converges to 0 there-
fore
lim
k→∞|hk,t−ht|= 0.
Therefore the point-wise convergence ofdHt
dtcan be achieved:
lim
k→∞|dyk,t
dt−dyt
dt| ≤lim
k→∞|c|L(|W||hk,t−ht|+|U||xk,t−xt|) = 0 .
So we have: limk→∞dyk,t
dt=dyt
dt. ThereforedHt
dtare point-wise continuous functionals over
“bounded piece-wise constant input sequence”.
A.7 C OMPARISON OF DECAYING MEMORY AND FADING MEMORY
The targets with decaying memory is a common assumption in the sense that it can also be de-
rived from the uniformly asymptotically incrementally stable assumption from Hanson & Raginsky
(2020), which is demonstrated by ‘imperfect system models may still be capable of generating out-
puts that uniformly approximate the outputs of the original system over infinite time intervals‘.
There is a concept of fading memory introduced in the nonlinear functional approximation with
V olterra series (Boyd et al., 1984). A functional is said to have fading memory if there exists a
monotonically decreasing function w:R+→(0,1],limt→∞w(t) = 0 , such that for any ϵ >0,
there exists a δ >0such that for any x,x′∈ X,
|Ht(x)−Ht(x′)|< ϵwhenever sup
s∈(−∞,t]|xs−x′
s|w(t−s)< δ. (25)
Remark A.5.In terms of the relations between fading memory and decaying memory, they are
independent. On one hand, there exists a so-called peak-hold operator (Boyd et al., 1984) with a
decaying memory (but not fading memory). On the other hand, it is possible for a linear functional
to only have the fading memory (but no decaying memory), where it needs only the requirementR∞
0|ρ(s)|2
|w(s)|ds <∞, which does not imply lims→∞|ρ(s)|2= 0.
A.8 I NVERSE APPROXIMATION THEOREM FOR LINEAR -LIKE ACTIVATIONS
We hereafter call eHm=bH(·,˜θm)to be the perturbed models.
We will briefly summarize the idea of the proof. Given that approximations are defined to be stable,
the decaying memory property ensures that the derivative of the hidden states for the perturbed
model approaches 0 as time t→ ∞ . The decay rate of the memory function is characterised by the
rate at whichd˜ht
dtconverges to 0. Using the Hartman-Grobman theorem, we can obtain bounds on
the eigenvalues of the matrices Wm. These bounds, in turn, determine the decay rate ofd˜ht
dt, leading
to an exponential decay in the model’s memory function. Finally, since the models with uniformly
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2024
exponential decaying memory can only approximate targets with exponential decay, the memory
function of the nonlinear target functionals must also be decaying exponentially.
For simplicity of the notation, we strengthen the assumption from lim supm→∞tolimm→∞. To get
the result for lim supm→∞, we only need to consider the subsequence of mkwith a limit.
Proof. Define the derivative of hidden states (for unperturbed model bHt(·, θm)) to be vm,t=dhm,t
dt,
similarly, ˜vm,tis the derivative of hidden states for perturbed models ( eHt=bHt(·,˜θm)),˜vm,t=
d˜hm,t
dt.
Since each perturbed model has a decaying memory, by Lemma A.6, we have
lim
t→∞˜vm,t= 0,∀m. (26)
In particular, for each mand perturbation within the stability radius, there exists t0which depends
onmandZ0such that |˜vm,t|∞< Z 0, t≥t0.
If the inputs are limited to Heaviside input, the derivative of perturbed models’ hidden states ˜vm,t
satisfies the following dynamics:
d˜vm,t
dt=σ′(˜vm,t)◦(fWm˜vm,t) =cσfWm˜vm,t, t≥t0 (27)
˜vm,t0=σ(fWht0+eUmxt0+˜bm),|˜vm,t0|∞< Z 0. (28)
Select a sequence of perturbed recurrent matrices {fWm,k}∞
k=1satisfying the following two proper-
ties:
1.cσfWm,kis Hyperbolic, which means the real part of the eigenvalues of the matrix are
nonzero.
2.limk→∞(fWm,k−Wm) =β0Im. (Notice that |fWm,k−Wm|2≤β0Imfor all k)
Moreover, by Lemma A.7, we know the each hyperbolic matrix cσfWm,kis Hurwitz as the system
for˜vm,tis asymptotically stable.
max
i∈[m](Re(λi(fWm)))<0, (29)
max
i∈[m](Re(λi(Wm)))<−β0. (30)
Therefore the original unperturbed recurrent weight matrix Wmsatisfies the following eigenvalue
inequality uniformly inm.
sup
mmax
i∈[m](Re(λi(Wm)))<−β0. (31)
As the memory functions of the perturbed models are uniformly decaying, we consider a subclass
of perturbed models where only the weight of linear readout map cmare perturbed. In other words,
we are considering perturbed models with weights (Wm, Um, bm,˜cm). Since the supremum of
perturbed model memory functions are decaying:
lim
t→∞sup
md
dt˜ym,t= 0. (32)
For any ϵ >0, there exists a Tϵ>0such that for all t > T ϵ,
sup
|˜cm−cm|2≤β0d
dt˜ym,t= sup
|˜cm−cm|2≤β0|˜c⊤
mvm,t|< ϵ. (33)
We have
|cmvm,t|+β0|vm,t|2< ϵ, (34)
|vm,t|2<ϵ
β0, t > T ϵ. (35)
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2024
Select ϵ=1
2β0Z0, then we have the unperturbed derivative |vm,t|2< Z 0fort > T ϵ. Therefore
the dynamics of vm,tis exactly the linear system in t > T ϵ. It can be seen that for t≥Tϵ
|vm,t|2=|eWm(t−Tϵ)vm,Tϵ|2 (36)
≤ |eWm(t−Tϵ)|2|vm,Tϵ|2 (37)
≤e−β0(t−Tϵ)|vm,Tϵ|2 (38)
The model memory decays exponentially: for t≥Tϵ,
|c⊤
mvm,t| ≤ |cm|2|vm,Tϵ|2e−β0(t−Tϵ)(39)
≤ |cm|2Z0e−β0(t−Tϵ)(40)
=|cm|2Z0eβ0Tϵe−β0t(41)
Fort∈[0, Tϵ], as the memory function is continuous, there exists a constant C′:=
sup
t∈[0,Tϵ]eβ0t|c⊤
mvm,t| ≤eβ0TϵZ0sup
m|cm|2such that
|c⊤
mvm,t| ≤C′e−β0t, t∈[0, Tϵ] (42)
Therefore, there exists constants C0:= max(supm|cm|2·Z0eβ0Tϵ, C′)such that for any m,
M(bHm)(t) = sup
x̸=01
|x|∞+ 1d
dtym,t=|c⊤
mvm,t| ≤C0e−β0t, t≥0. (43)
Last, by Lemma A.10, the target Hhas an exponentially decaying memory as it is approximated by
a sequence of models {bHm}∞
m=1with uniformly exponentially decaying memory.
A.9 I NVERSE APPROXIMATION THEOREM FOR TANH -LIKE ACTIVATIONS
Next we give the proof for RNNs using tanh-like activations.
Proof. Similar to the previous proof, we still have
lim
t→∞˜vm,t= 0,∀m. (44)
In particular, for each mand perturbation within the stability radius, there exists t0such that
|˜vm,t|∞< Z 0, t≥t0.
Now, the perturbed hidden states satisfies the following dynamics:
d˜vm,t
dt=σ′
tanh(˜vm,t)◦fWm˜vm,t= (I−Diag(˜vm,t)2)fWm˜vm,t, (45)
˜vm,0=σtanh(eUmx0+˜bm). (46)
Select a sequence of perturbed recurrent matrices {fWm,k}∞
k=1satisfying the following two proper-
ties:
1.fWm,kis Hyperbolic, which means the real part of the eigenvalues of the matrix are nonzero.
2.limk→∞(fWm,k−Wm) =β0Im.
By Lemma A.7, we know each hyperbolic matrix fWm,kis Hurwitz as the target functional sequence
has a stable approximation. Similarly, we have the following uniform bound on eigenvalues of
{Wm}:
sup
mmax
i∈[m](Re(λi(Wm)))≤ −β0. (47)
Since every Wmis Hurwitz matrix, consider the corresponding (continuous) Lyapunov equation
(Khalil, 2002)
WT
mPm+PmWm=−Qm. (48)
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2024
For simplicity, we select the matrix Qm=Im. For any positive definite matrix Qm, it is known that
Pmhas an explicit integral form:
Pm=Z∞
0eW⊤
mtQmeWmtdt=Z∞
0eW⊤
mtImeWmtdt. (49)
By Lemma A.8, we know Pmhas a uniformly bounded L2norm
sup
m∥Pm∥2≤1
2β0. (50)
We construct a Lyapunov function V(v) =vTPmv≤1
2β0|v|2
2, which satisfies the following differ-
ential equation:
dV(vm,t)
dt=v⊤
m,t(W⊤
m(I−Diag(vm,t)2)Pm+Pm(I−Diag(vm,t)2)Wm)vm,t
=v⊤
m,t(W⊤
mPm+PmWm)vm,t
−v⊤
m,t(W⊤
mDiag(vm,t)2Pm+PmDiag(vm,t)2Wm)vm,t
=− |vm,t|2
2−v⊤
m,t(W⊤
mDiag(vm,t)2Pm+PmDiag(vm,t)2Wm)vm,t.(51)
By Lemma A.9, for any positive L∈(0,1), there is an ΥL=q
β0L
M0>0such that the following
inequality holds for any m,
|v⊤
m,t(W⊤
mDiag(vm,t)2Pm+PmDiag(vm,t)2Wm)vm,t| ≤L|vm,t|2
2,∀|vm,t|2≤ΥL. (52)
As the memory functions of the perturbed models are uniformly decaying, we consider a subclass of
perturbed models where only the linear readout map cmare perturbed. Since the limit of perturbed
model memory functions are decaying:
lim
t→∞sup
md
dt˜ym,t= 0. (53)
For any ϵ >0, there exists a Tϵ>0such that for all t > T ϵ
sup
|˜cm−cm|2≤β0d
dt˜ym,t= sup
|˜cm−cm|2≤β0|˜c⊤
mvm,t|< ϵ. (54)
We have
|cmvm,t|+β0|vm,t|2< ϵ, (55)
|vm,t|2<ϵ
β0. (56)
Select ϵ < β 0ΥL, we have
|vm,t|2<ϵ
β0<ΥL. (57)
The Lyapunov function satisfies the following inequality for t≥Tϵ
dV(vm,t)
dt=− |vm,t|2
2−v⊤
m,t(W⊤
mDiag(vm,t)2Pm+PmDiag(vm,t)2Wm)vm,t (58)
≤ − | vm,t|2
2+L|vm,t|2
2 (59)
=−(1−L)|vm,t|2
2 (60)
≤ −2(1−L)β0V(vm,t). (61)
The Lyapunov function V(vm,t)is decaying exponentially for ϵ < β 0ΥL, t≥Tϵ,
V(vm,t)≤e−2(1−L)β0(t−Tϵ)V(vm,Tϵ). (62)
Notice that |vm,t|2<ϵ
β0<ΥL. Therefore
V(vm,t)≤e−2(1−L)β0(t−Tϵ)·1
2β0Υ2
L. (63)
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2024
Since the model weights are uniformly bounded, there is a constant Msuch that
sup
m|Wm|2=M <∞. (64)
Since|Im|2= 1≤2|Wm|2|Pm|2, this implies
min
m|Pm|2≥1
2M>0. (65)
AsV(vm,t)≥1
2M|vm,t|2
2, there exists a uniform constant C0:=q
M
β0ΥL=q
ML
M0such that
|vm,t|2≤C0e−(1−L)β0(t−Tϵ),∀t > T ϵ. (66)
Since the model weights cmare uniformly (in m) bounded C1:= supm|cm|2<∞. Therefore the
model memories are uniformly (in m) exponentially decaying
sup
mM(bHm)(t) = sup
msup
x̸=0d
dtbHm,t(ux)
|x|∞+ 1≤C0C1e−(1−L)β0(t−Tϵ). (67)
Notice that Tϵis independent of m, it only depends on ϵ.
TakeL→0, for any β < β 0there exists a constant C∗which depends on βsuch that
sup
mM(bHm)(t)≤C∗e−βt. (68)
Last, by Lemma A.10, the target has exponentially decaying memory as it is approximated by a
sequence of models with uniformly exponentially decaying memory.
A.10 P ROOFS OF LEMMAS
In the following section we include the lemmas used in the proof of main theorems. Lemma A.10 is
used in the proof for Theorem 3.9. Lemmas A.6 to A.10 are applied in Appendix A.9.
Lemma A.6. Assume the target functional sequence has a β0-stable approximation and the per-
turbed model has a decaying memory, we show that ˜vm,t→0for all m.
Proof. For any m, fixfWmandeUm. Since the perturbed model has a decaying memory,
lim
t→∞d
dteHm(ux)= lim
t→∞˜c⊤
m˜vm,t= 0. (69)
According to the definition of stable approximation about perturbation, as t→ ∞ ,˜vm,tvanishes
over the projection to ˜cmfor any |˜cm−cm|∞≤β. By linear algebra, there exist {∆ci}m
i=1,
|∆ci|∞< β such that cm+ ∆c1, . . . , cm+ ∆cmform a basis of Rm. We can then decompose any
vector uinto
u=k1(cm+ ∆c1) +···+km(cm+ ∆cm). (70)
Take the inner product of uand˜vm,t, we have
lim
t→∞u⊤˜vt=mX
i=1kilim
t→∞(cm+ ∆ci)⊤˜vt= 0 (71)
As the above result holds for any vector u, we get
lim
t→∞˜vm,t= 0. (72)
Lemma A.7. Consider a dynamical system with the following dynamics:
dvt
dt=Diag(σ′(vt))Wvt,
v0=σ(Ux0+b).(73)
IfW∈Rm×mis hyperbolic and the system in Equation (73) is asymptotically stable over any
bounded Heaviside input |x0|1< B, then the matrix Wis Hurwitz.
18

--- PAGE 19 ---
Published as a conference paper at ICLR 2024
Proof. When σis the hardtanh activation, σ′(z) = 1 for|z| ≤1, Diag (σ′(vm,t)) =Im. Also, σ−1
is continuous at 0, therefore σis an open mapping around 0. For sufficiently large B, there exists
δm>0such that a small ball centered at 0is contained in the initializations that are stable for the
system in Equation (73).
v0∈B(0, δm)⊆ {σ(Ux0+b) :|x0|1< B}. (74)
Since limt→∞vt= 0 for any v0∈B(0, δm), it implies the local asymptotic stability at the origin.
IfWhas an eigenvalue with a positive real part, according to Hartman-Grobman theorem, ˜vthas an
unstable manifold locally at the origin. However, this contradicts the asymptotic stability around the
origin with the initialization v0inB(0, δm)⊂Rm.
Lemma A.8. Given a sequence of Lyapunov equation with Hurwitz matrices {Wm}∞
m=1
W⊤
mPm+PmWm=−Im. (75)
Assume the eigenvalues of Wmare uniformly bounded away from 0:
sup
i∈[m]Re(λi(Wm))≤ −β0. (76)
Show that the L2norm of Pmare uniformly bounded
sup
m∥Pm∥2≤1
2β0. (77)
Proof. By the theory of Lyapunov equation, it can be verified that the symmetric positive-definite
matrix
Pm=Z∞
0eW⊤
mtImeWmtdt (78)
is the solution to the above Lyapunov equation.
Assume Wm’s eigenvalue-eigenvector couples are (λi, vi), i∈[m]. Without loss of generality, we
assume viare all unit eigenvectors.
∥vi∥2= 1,1≤i≤m. (79)
For simplicity, we first assume the eigenvalues are distinct. Therefore, any unit vector ucan be
decomposed into linear combination of eigenvectors
u=mX
i=1civi,mX
i=1c2
i= 1. (80)
We have
u⊤Pmu=Z∞
0∥eWmtu∥2
2dt (81)
=Z∞
0mX
i=1c2
ie2λit∥vi∥2
2dt (82)
=Z∞
0mX
i=1c2
ie2λitdt (83)
≤Z∞
0mX
i=1c2
ie−2β0tdt (84)
=Z∞
0e−2β0tdt=1
2β0. (85)
Therefore, ∥Pm∥2≤1
2β0. Notice the bound on L2norm is uniform as the eigenvalue bound on Wm
is uniform.
19

--- PAGE 20 ---
Published as a conference paper at ICLR 2024
Next, if Wmhas repeated eigenvalues, we can consider Wmwith slight perturbations such that the
perturbed fWmare diagonalizable. In this case the eigenvalues might not be distinct but it’s feasible
to construct the eigenvectors to be orthogonal to each other. We can bound the perturbed matrix’s
L2norm with the previous argument. The corresponding ePmconverges to Pmby the continuity of
explicit form in Equation (78).
Lemma A.9. Assume {Wm∈Rm×m}∞
m=1is a sequence of Hurwitz matrices with eigenvalues
bounded away from 0:
sup
mmax
i∈[m](Re(λi(Wm)))≤ −β0. (86)
Assume the max norms for matrices {Wm}∞
m=1are uniformly bounded:
sup
msup
1≤i,j≤m|Wm,ij| ≤M0. (87)
Define the solution to the following Lyapunov equation to be Pm:
W⊤
mPm+PmWm=−Im. (88)
We show that for any positive constant L, there exists an Υ>0such that the following inequality
holds for any mand|vm|2≤Υ:
|v⊤
m(W⊤
mDiag(vm)2Pm+PmDiag(vm)2Wm)vm| ≤L|vm|2
2. (89)
Proof. First, by the property of matrix and vector norm:
|v⊤
mPmDiag(vm)2Wmvm| ≤ |vm|2|Pm|2|Diag(vm)2Wmvm| (90)
≤1
2β0|vm|2|Diag(vm)2Wmvm|. (91)
Notice that the second inequality holds as a direct result for Lemma A.8.
Then,
|Diag(vm)2Wmvm|2=vuutmX
i=1(mX
j=1v2
m,jWm,jivm,i)2 (92)
≤vuutmX
i=1(mX
j=1v2
m,j|Wm,ji||vm,i|)2 (93)
≤vuutmX
i=1(mX
j=1v2
m,jM0|vm,i|)2 (94)
=M0vuutmX
i=1(mX
j=1v2
m,j|vm,i|)2 (95)
=M0vuut(mX
j=1v2
m,j)2(mX
i=1|vm,i|2) (96)
=M0(mX
j=1v2
m,j)vuut(mX
i=1|vm,i|2) (97)
≤M0|vm|3
2 (98)
≤M0Υ2|vm|2. (99)
20

--- PAGE 21 ---
Published as a conference paper at ICLR 2024
For any 0<Υ≤q
β0L
M0, we have
|v⊤
m(W⊤
mDiag(vm)2Pm+PmDiag(vm)2Wm)vm| ≤2∗1
2β0|vm|2|Diag(vm)2Wmvm|(100)
≤1
β0M0Υ2|vm|2
2 (101)
≤L|vm|2
2. (102)
Lemma A.10 . Consider a continuous function f: [0,∞)→R, assume it can be approximated by a
sequence of continuous functions {fm}∞
m=1universally:
lim
m→∞sup
t|f(t)−fm(t)|= 0. (103)
Assume the approximators fmare uniformly exponentially decaying with the same β0>0:
lim
t→∞sup
m∈N+eβ0t|fm(t)| →0. (104)
Then the function fis also decaying exponentially:
lim
t→∞eβt|f(t)| →0,∀0< β < β 0. (105)
Proof. Given a function f∈C([0,∞)), we consider the transformation Tf: [0,1]→Rdefined
as:
(Tf)(s) =(
0, s = 0
f(−logs
β0)
s, s ∈(0,1].(106)
Under the change of variables s=e−β0t, we have:
f(t) =e−β0t(Tf)(e−β0t), t≥0. (107)
According to uniformly exponentially decaying assumptions on fm:
lim
s→0+(Tfm)(s) = lim
t→∞fm(t)
e−β0t= lim
t→∞eβ0tfm(t) = 0 , (108)
which implies Tfm∈C([0,1]).
For any β < β 0, letδ=β0−β >0. Next we have the following estimate
sup
s∈[0,1]|(Tfm1)(s)−(Tfm2)(s)| (109)
= sup
t≥0fm1(t)
e−βt−fm2(t)
e−βt(110)
≤max
sup
0≤t≤T0fm1(t)
e−βt−fm2(t)
e−βt, C0e−δT0
(111)
≤max
eβT0sup
0≤t≤T0|fm1(t)−fm2(t)|, C0e−δT0
(112)
where C0is a constant uniform in m.
For any ϵ >0, take T0=−ln(ϵ
C0)
δ,we have C0e−δT0≤ϵ. For sufficiently large Mwhich depends
onϵandT0, by universal approximation (Equation (103)), we have ∀m1, m2≥M,
sup
0≤t≤T0|fm1(t)−fm2(t)| ≤e−βT0ϵ, (113)
eβT0sup
0≤t≤T0|fm1(t)−fm2(t)| ≤ϵ. (114)
Therefore, {fm}is a Cauchy sequence in C([0,∞)).
21

--- PAGE 22 ---
Published as a conference paper at ICLR 2024
Since{fm}is a Cauchy sequence in C([0,∞))equipped with the sup-norm, using the above esti-
mate we can have {Tfm}is a Cauchy sequence in C([0,1])equipped with the sup-norm. By the
completeness of C([0,1]), there exists f∗∈C([0,1])withf∗(0) = 0 such that
lim
m→∞sup
s∈[0,1]|(Tfm)(s)−f∗(s)|= 0. (115)
Given any s >0, we have
f∗(s) = lim
m→∞(Tfm)(s) = (Tf)(s), (116)
hence
lim
t→∞eβtf(t) = lim
s→0+(Tf)(s) =f∗(0) = 0 . (117)
B M EMORY QUERY IN SENTIMENT ANALYSIS BASED ON IMDB MOVIE
REVIEWS
In the following example, we show the memory function of sentiment score’s for single repeated
word using LSTM and fine-tuned BERT model. In Figure 5, it can be seen that the memory function
of simple words such as “good” and “bad” is decaying exponentially. However, the memory of
sentiment in “ha” can be complicated as it’s not decaying fast. This phenomenon also holds for
stacked Bidirectional LSTM models.
As a comparison, we can see the memory pattern of BERT over repeated characters or words can be
decaying relatively slower. At the same time, the fluctuation of memory of “ha”, “bad” and “good”
can fluctuate a lot (compared with LSTM models). These phenomena indicate that the transformer-
type architectures might not have exponential decaying memory issues. (See Figure 6)
0 25 50 75 100 125 150 175 200
Repeat times106
105
104
103
102
101
100Memory of sentiment scoresha 
good 
bad 
(a) Bidirectional LSTM
0 25 50 75 100 125 150 175 200
Repeat times106
105
104
103
102
101
100Memory of sentiment scoresha 
good 
bad (b) Stacked Bidirectional LSTM
Figure 5: Memory function of sentiment scores for different words based on IMDB movie reviews
using Bidirectional LSTM and stacked Bidirectional LSTM
C R ESCALING IS NOT AN A PRIORI SOLUTION TO STABLE APPROXIMATION
Now we consider the special case: Assume Wis diagonal W=diag([w1, w2, . . . , w m]). Element
of matrix Uis 1. And the vector c= [c1, c2, . . . , c m].
In this section, we compare the vanilla linear RNN with a memory function ˆρ(t) =Pm
k=1ckewkt
and a rescaled linear RNN with a memory function ˆρ(t) =Pm
k=1cke1
kwkt. They are represented
in the simplest form with ckandwkas the trainable weights. It can be seen that the target linear
functional with memory function ρ(t) =P∞
k=11
k2e−1
ktcannot be stably approximated by linear
RNN but can be stably approximated by rescaled linear RNN. (For rescaled linear RNN, one just
need to pick wk= 1.)
22

--- PAGE 23 ---
Published as a conference paper at ICLR 2024
0 20 40 60 80 100104
103
102
101
repeat of 'a '
0 20 40 60 80 100104
103
102
101
repeat of 'b '
0 20 40 60 80 100104
103
102
101
repeat of 'c '
0 20 40 60 80 100105
104
103
102
repeat of 'd '
0 10 20 30 40104
103
102
101
repeat of 'ha '
0 5 10 15 20 25 30104
103
repeat of 'bad '
0 5 10 15 20 25 30
repeat times104
103
102
Memory of sentiment scorerepeat of 'yes '
0 5 10 15 20104
103
102
101
repeat of 'good '
Figure 6: Memory for sentiment scores based on IMDB movie reviews
Thex-axis is the inputs repeated times, which can be viewed as the input length. The y-axis is the
derivative of sentiment score over repeated inputs, which is the memory of this sentiment analysis
model. This tiny example implies that the memory function required in application can be in various
patterns. The decaying pattern can be fluctuating in terms of the scale.
However, this rescaling does not work for all memory functions. Take the memory function ρ(t) =P∞
k=11
k2e−1
k2tas an example, it may not be possible to stably approximate target with this memory
function with linear RNN or the rescaled version.
Based on the above argument, there does not exists a rescaled RNN that can work for linear func-
tional with arbitrary memory function. In particular, it’s always feasible to construct a slow decay
target that a fixed rescaled linear RNN cannot stably approximate. This result indicates that rescaling
is not an a priori solution to achieve stable approximation. In practice, it’s hard to get the memory
function decay speed therefore it’s impractical to use the rescaling method.
D N UMERICAL EXPERIMENTS DETAILS
Here we give the numerical experiments details for both the linear and nonlinear RNN experiments.
Linear functionals approximated by linear RNNs The linear functional approximation is equiv-
alent to the approximation of function ρ(t)by exponential sum c⊤eWtU. For simplicity, we con-
duct the approximation in the interpolation approach and consider the one-dimensional input and
one-dimensional output case. Furthermore we reduce the approximation problem (numerically) into
the least square fitting over the discrete time grid. We fit the coefficients of polynomial and evaluate
the approximation error over [1,2, . . . , 100].
23

--- PAGE 24 ---
Published as a conference paper at ICLR 2024
The exponential decaying memory function is manually selected to show the significance across
different hidden dimensions:
ρ(t) = 0 .9t(118)
while the polynomial decaying memory function is ρ(t) =1
(t+1)1.1. The additional 1is added as we
require the memory function to be integrable.
The perturbation list β∈[0,5.0∗10−4,5.0∗10−4∗21, . . . , 5.0∗10−4∗220]. Each evaluation of
the perturbed error is sampled with 10 different weight perturbations to reduce the variance.
The approximation of linear functional by linear RNN has simple structures. So we did not use
gradient-based optimisation. The approximation of linear functional via linear RNN is equivalent to
the minimisation problem minc,W,U||Ht−ˆHt||,which can be simplified into the following function
approximation problem minc,W,U supR∞
0|ρ(s)−c⊤esWU|ds.After eigenvalue decompositions of
Wand change of variables, the above problem can be reduced to the polynomial approximation of
function ρ(−lnt)over interval (0,1]. Therefore we solve this approximation problem via linear
regression instead of the gradient-based optimisation. The code are attached in the supplementary
materials.
Nonlinear functionals approximated by nonlinear RNNs Still, we consider the one-
dimensional input and one-dimensional output case. We train the model over timestamp
[0.1,0.2, . . . , 3.2]and evaluate the approximation error over the same horizon [0.1,0.2, . . . , 10.0].
In the experiments to approximate the nonlinear functionals by nonlinear RNNs, we train each model
for 1000 epochs, the stopping criterion is the validation loss achieving 10−8. The optimizer used is
Adam with initial learning rate 0.005. The loss function is mean squared error. The batch size is 128
while the train set size and test set size are 12800.
The polynomial decaying memory function is ρ(t) =1
(t+1)1.5equals to 1 at t= 0. The only
difference is the decaying speed.
The perturbation list β∈[0,10−11∗20,10−11∗21, . . . , 10−11∗235]. Each evaluation of the
perturbed error is sampled with 3 different weight perturbations (and take the maximum) to reduce
the variance of the perturbation error.
E A DDITIONAL NUMERICAL RESULTS
Curse of memory over ReLU activation Despite the activation relu is not smooth at σ(0) = 0 ,
we numerically verify that the curse of memory phenomenon also holds. In Figure 7, the left RNN
using ReLU cannot stably approximate target with polynomial memory while the RNN with Softplus
reparameterization can stably approximate the target.
Stable re-parameterization as a potential solution In Figure 4, we show that stable reparame-
terization such as exponential reparameterization and softplus reparameterization enables the stable
approximation of linear functional with polynomial memory by linear RNNs. As shown in Figure 8,
we further verify that stable re-parameterization enables the stable approximation of targets with
polynomial decaying memory by hardtanh/tanh RNNs.
24

--- PAGE 25 ---
Published as a conference paper at ICLR 2024
104
103
102
101
Perturbation10010510101015102010251030Perturbation errorPerturbation Error for Different Hidden Dimensions
Hidden Dimension 8
Hidden Dimension 16
Hidden Dimension 32
Hidden Dimension 64
(a) Without reparameterization + ReLU RNN
104
103
102
101
100
Perturbation102
101
100101102103104Perturbation errorPerturbation Error for Different Hidden Dimensions
Hidden Dimension 8
Hidden Dimension 16
Hidden Dimension 32
Hidden Dimension 64
(b) Softplus reparameterization + ReLU RNN
Figure 7: When the activation used is ReLU, vanilla nonlinear RNN does not have stable approxi-
mation while the softplusRNN can be stably approximated.
25

--- PAGE 26 ---
Published as a conference paper at ICLR 2024
(a) Softplus reparameterization + Hardtanh RNN
(b) Softplus reparameterization + Tanh RNN
Figure 8: Linear functionals with polynomial memory can be stably approximated by hardtanh/tanh
RNNs using Softplus parameterisation for W. These empirical findings underscore that stable pa-
rameterisation is pivotal for attaining a stable approximation for targets with long-term memory.
The experiments build upon and extend the results presented in Figure 4.
26

--- PAGE 27 ---
Published as a conference paper at ICLR 2024
0 500 1000 1500 2000
Steps0.751.001.251.501.752.002.25Train loss (step)parameterization=Direct
parameterization=Exp
parameterization=Inverse
parameterization=Softplus
(a) train loss
0 500 1000 1500 2000
Steps0.10.20.30.40.50.60.70.8Train acc (step)
parameterization=Direct
parameterization=Exp
parameterization=Inverse
parameterization=Softplus (b) train accuracy
250 500 750 1000 1250 1500 1750 2000
Steps0.81.01.21.41.61.82.02.22.4Val loss (epoch)parameterization=Direct
parameterization=Exp
parameterization=Inverse
parameterization=Softplus
(c) validation loss
250 500 750 1000 1250 1500 1750 2000
Steps0.20.30.40.50.60.7Val acc (epoch)
parameterization=Direct
parameterization=Exp
parameterization=Inverse
parameterization=Softplus (d) validation accuracy
Figure 9: Image classification on MNIST. The performance of stable reparameterization is better
than the direct parameterization of recurrent weight g(M) =M. Under the same weight initializa-
tion, the stable reparameterization can speed up the optimization. The shadow corresponds to the
standard error evaluated over 3 repeats.
Test loss(std err) Test accuracy (std err)
softplus (stable) 0.8903 (0.0004) 71.36(0.03)
exp (stable) 0.9028 (7.2E-05) 70.55(0.01)
inverse (stable) 0.9082 (0.0005) 70.77(0.05)
direct (unstable) 0.9177 (0.0105) 68.47(0.06)
Table 1: Test loss and accuracy of different parameterization methods over MNIST
Stable re-parameterization facilitate the optimization In this subsection, we demonstrate the
optimization advantages provided by stable re-parameterization in enhancing long-term memory
learning. We train the nonlinear RNNs on the MNIST dataset for 10 epochs. The models are
initialized with identical weight configurations, underscoring the impact of recurrent weight param-
eterization. As illustrated in Figure 9, the implementation of stable reparameterization significantly
expedites the optimization process. It’s important to note that this parameterization technique does
not alter the inherent capacity of the model, resulting in comparable final performance outcomes
(Table 1).
F M EMORY FUNCTION DEFINED VIA IMPULSE INPUTS
Besides the Heaviside inputs, it’s also feasible to define the memory function via the impulse inputs.
Letx∈Rdand consider the following impulse inputs input sequence ix
t=x t = 0,
0t̸= 0.Hereafter
27

--- PAGE 28 ---
Published as a conference paper at ICLR 2024
we call the following memory function to be impulse-inputs-induced memory function.
M(H)(t) := sup
x̸=01
|x|∞|Ht(ix)|. (119)
In the linear functional case, notice that according to Equation (3)
sup
x̸=0|Ht(ix)|
∥ix∥∞= sup
x̸=0|x⊤ρ(t)|
|x|∞=|ρ(t)|1. (120)
Based on the impulse-inputs-induced memory function, if we assume this memory function and its
derivative are decaying, then we have a similar result as Theorem 3.9.
Sketch proof for Theorem 3.9 based on impulse-inputs-induced memory function Assume β0
is the stability radius, apply the proof of Theorem 3.9, we have for any β < β 0
lim
t→∞eβtd
dtyt→0. (121)
FixC > 0, there exists t0such that for any t≥t0,|d
dtyt| ≤Ce−βt. Since the target has decaying
memory y∞:= lim t→∞yt= 0. Then we have
lim
t→∞eβt|yt−y∞|= lim
t→∞|Z∞
td
dsysds| (122)
≤lim
t→∞|Z∞
tCe−β(s−t)ds| (123)
≤C <∞. (124)
Therefore the memory function based on the impulse inputs is also decaying exponentially.
lim
t→∞eβt|yt| ≤C. (125)
G M EMORY FUNCTION DEFINED VIA REVERSED INPUTS
Letx∈Rdand consider the following reversed inputs input sequence rx
t=x t < 0,
0t≥0.Hereafter
we call the following memory function to be impulse-inputs-induced memory function.
M(H)(t) := sup
x̸=01
|x|∞d
dtHt(rx). (126)
In the linear case, notice that according to Equation (3)
sup
x̸=0d
dtHt(rx)
∥rx∥∞= sup
x̸=0|x⊤ρ(t)|
|x|∞=|ρ(t)|1. (127)
Based on the reversed-inputs-induced memory function, if we assume this memory function and its
derivative are decaying, then we have Theorem 3.9.
Sketch proof for Theorem 3.9 based on reversed-inputs-induced memory function Assume
β0is the stability radius, apply the proof of Theorem 3.9, we have for any β < β 0
lim
t→∞eβtd
dtyt→0. (128)
Therefore the memory function based on the reversed inputs is also decaying exponentially.
The memory function defined over reversed inputs can be harder to evaluate as the outputs of
different input sequences (x,0, . . . , ),(x, x,0, . . . , 0),(x, x, x, 0, . . . , 0), ...need to be evaluated.
28

--- PAGE 29 ---
Published as a conference paper at ICLR 2024
0 20 40 60 80 100 120
time t1016
1013
1010
107
104
101
memory 
layers=1
layers=2
layers=3
layers=4
(a) RNN - Heaviside memory
0 20 40 60 80 100 120
time t1030
1026
1022
1018
1014
1010
106
102
memory 
layers=1
layers=2
layers=3
layers=4 (b) RNN - Impulse memory
0 20 40 60 80 100 120
time t1015
1012
109
106
103
100memory 
layers=1
layers=2
layers=3
layers=4
(c) GRU - Heaviside memory
0 20 40 60 80 100 120
time t1015
1012
109
106
103
100memory 
layers=1
layers=2
layers=3
layers=4 (d) GRU - Impulse memory
0 20 40 60 80 100 120
time t1015
1012
109
106
103
100memory 
layers=1
layers=2
layers=3
layers=4
(e) LSTM - Heaviside memory
0 20 40 60 80 100 120
time t1015
1012
109
106
103
100memory 
layers=1
layers=2
layers=3
layers=4 (f) LSTM - Impulse memory
0 500 1000 1500 2000
time t1012
1010
108
106
104
102
memory 
layers=1
layers=2
layers=3
(g) S4 - Heaviside memory
0 500 1000 1500 2000
time t1012
1010
108
106
104
102
memory 
layers=1
layers=2
layers=3 (h) S4 - Impulse memory
Figure 10: Memory functions of randomly-initialized recurrent networks evaluated over Heaviside
inputs and impulse inputs. The memory functions evaluated over different inputs are numerically
equivalent . The shadow indicates the error bar for 20 repeats.
H Q UALITATIVE EQUIVALENCE OF DIFFERENT MEMORY FUNCTION
DEFINITIONS
Apart from the previous sections that study the equivalence of memory functions over linear func-
tionals. In Figure 10, we show that the memory functions evaluated over different test inputs are 29

--- PAGE 30 ---
Published as a conference paper at ICLR 2024
qualitatively equivalent. Since the reversed inputs are harder to evaluate numerically, we only com-
pare the memory functions evaluated over Heaviside inputs and impulse inputs.
Besides the qualitative comparison that different memory functions are “equivalent”, here we show
they all characterize the decay speed of the model’s memory of a impulse change at time t= 0.
Relation between memory decaying speed and target dependence on early inputs An intuitive
explanation is that current memory functions characterize how fast the memory about the sudden
input change at t= 0 decays as tgrows. Inputs such as Heaviside inputs, reversed inputs, and
impulse inputs all have this feature. Therefore the memory function evaluated over these inputs can
be interpreted as a measure for “dependence of the target function on early inputs”.
ρT:= sup
x̸=0|yT(ix)|
|x|= sup
x̸=0HT((x,0, ...))
|x|.Impulse-inputs-based memory
We show the “equivalence” of different memory functions in linear functional sense in Appendix F
and G. Since memory functions over Heaviside inputs are easier to evaluate numerically (as shown
in Appendix H), we adopt the corresponding derivative form (as shown in Equation 5) to define the
memory function.
Here we explain the reason that the memory function is evaluated over certain subclass of inputs
rather than all continuous inputs:
Why special inputs A natural question is why we need a particular set of inputs rather than use
any continuous input. The identity functional Ht(x) =xttakes the input sequence xt=1
t, t >0to
a polynomial decaying output yt=1
t. However, this does not mean the functional has a polynomial-
decaying memory. In fact, this identity functional has no memory of past inputs in the sense that
changing x0does not influence the output of ytwitht > 0. The above example indicates the
selection of inputs to extract the memory pattern needs to be chosen properly. This is also one of the
reasons we select the Heaviside inputs as the test inputs to construct the derivative form.
I A DDITIONAL NUMERICAL RESULTS FOR THE FILTERING EXPERIMENT
In this section, we first show the existence of nonlinewar RNNs with non-decaying memory. There-
fore in Figure 3 we need to filter the targets. Then the example of learning transformer with nonlinear
RNN is included. This example shows that transformer in general does not have a decaying memory.
Therefore the learning of transformer can highly difficult that we cannot achieve the approximation
with 1000 epochs training.
I.1 E XISTENCE OF NONLINEAR RNN S WITHOUT DECAYING MEMORY
In Figure 3, we show that the filtering of nonlinear RNNs with stable approximation presents the
target with only exponentially decaying memory. Here we present in Figure 11 that nonlinear RNNs
with randomly initialized weights have diverse memory patterns. Moreover, consider the following
examples.
30

--- PAGE 31 ---
Published as a conference paper at ICLR 2024
100101102103
Time t107
106
105
104
103
102
101
100memory function evaluated
(a) Examples of nonlinear RNNs have non-decaying memory
(b) Example of 2D RNN with periodic memory W=
1 1
−4−3
.
Figure 11: In the left panel, we show the existence of tanh RNNs with non-decaying memory. In
the right panel, we use a specific 2D tanh RNN to show the existence of a periodic trajectory. In
the figure, [0,0],[1,1],[−1,−1]are the attractor for the trajectories of (v1, v2) = (dht,1
dt,dht,2
dt).
Therefore the boundary of the different converging set will be a periodic trajectory over the 2D
plane. The periodic trajectory of hidden state derivative indicates a periodic memory function .
I.2 F ILTERING TOWARDS TRANSFORMER
Apart from the nonlinear RNNs targets, we also construct targets with polynomial decaying memory.
It can be seen in Figure 12 that the transformers with non-decaying memory function cannot be
stably approximated by tanh RNNs.
31

--- PAGE 32 ---
Published as a conference paper at ICLR 2024
(a) Memory function of the target randomly initialized transformer
104
103
102
101
100
Perturbation101
100101102103104105Perturbation errorHidden Dimension 8
Hidden Dimension 16
Hidden Dimension 32
Hidden Dimension 64
(b) Inexistence of stable approximation
Figure 12: If the target functional does not have an exponentially decaying memory, then the targets
cannot be stably approximated. The largest RNN with hidden dimension m= 64 failed to achieve
an approximation error smaller than the case for m= 32 andm= 16 .
J D ISCUSSION ON EXTENDING THE THEOREM 3.9TOGRU/LSTM
We will analyze GRU as LSTM comes with more complicated structure. The classical GRU is of
the following structure:
zt=σ(Wzxt+Uzht−1+bz), (129)
rt=σ(Wrxt+Urht−1+br), (130)
ˆht=ϕ(Whxt+Uh(rt⊙ht−1) +bh), (131)
ht= (1−zt)⊙ht−1+zt⊙ˆht. (132)
32

--- PAGE 33 ---
Published as a conference paper at ICLR 2024
Here σis sigmoid activation and ϕis the tanh activation. The hidden state equations can be refor-
mulated into
ht=ht−1+zt⊙(ˆht−ht−1). (133)
The corresponding continuous-time version can be given by
dht
dt=zt⊙(ˆht−ht) (134)
=σ(Wzxt+Uzht+bz)⊙(ˆht−ht) (135)
=σ(Wzxt+Uzht+bz)⊙(ϕ(Whxt+Uh(rt⊙ht) +bh)−ht) (136)
=σ(Wzxt+Uzht+bz) (137)
⊙(ϕ(Whxt+Uh(σ(Wrxt+Urht+br)⊙ht) +bh)−ht). (138)
Letx∈Rdand we consider the Heaviside inputs ux
t=x·1[0,∞)(t) =x t≥0,
0t <0.After change
of variable over the bias term bz:=Wzx+bz, br:=Wrx+br, bh:=Wzx+bh, the corresponding
hidden dynamics can be simplified into
dht
dt=σ(Uzht+bz) (139)
⊙(ϕ(Uh(σ(Urht+br)⊙ht) +bh)−ht), (140)
Notice that h∗= 0is an equilibrium point if bh= 0:
σ(bz)⊙(ϕ(Uh(σ(br)⊙0) + 0) −0) = 0 . (141)
Similar linearization can be utilized to analyse the corresponding first-order system:
dht
dt=Diag(σ(bz))(UhDiag(σ(br))−I)ht. (142)
Our stable approximation argument can be used to show Diag (σ(bz))(UhDiag(σ(br))−I)is Hur-
witz. However, h∗= 0 might not be the only equilibrium point. Furthermore, when bh̸= 0, it’s
harder to find all equilibrium points therefore further analysis are needed.
33
