# 2305.19190.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rnn/2305.19190.pdf
# Kích thước tệp: 1859713 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
LÝ THUYẾT XẤP XỈ NGHỊCH ĐẢO CHO MẠNG NEURAL HỒI QUY
PHI TUYẾN
Shida Wang
Khoa Toán học
Đại học Quốc gia Singapore
e0622338@u.nus.eduZhong Li
Microsoft Research Asia
lzhong@microsoft.com
Qianxiao Li∗
Khoa Toán học
Viện Vật liệu Thông minh Chức năng
Đại học Quốc gia Singapore
qianxiao@nus.edu.sg
TÓM TẮT
Chúng tôi chứng minh một định lý xấp xỉ nghịch đảo cho việc xấp xỉ các mối quan hệ chuỗi-sang-chuỗi phi tuyến sử dụng mạng neural hồi quy (RNN). Đây là kết quả kiểu Bernstein trong lý thuyết xấp xỉ, suy ra các tính chất của hàm mục tiêu dưới giả thiết rằng nó có thể được xấp xỉ hiệu quả bởi một không gian giả thuyết. Cụ thể, chúng tôi chỉ ra rằng các mối quan hệ chuỗi phi tuyến có thể được xấp xỉ ổn định bởi RNN phi tuyến phải có cấu trúc bộ nhớ suy giảm theo hàm mũ - một khái niệm có thể được làm rõ. Điều này mở rộng lời nguyền bộ nhớ đã được xác định trước đó trong RNN tuyến tính vào bối cảnh phi tuyến tổng quát, và lượng hóa các hạn chế thiết yếu của kiến trúc RNN để học các mối quan hệ tuần tự với bộ nhớ dài hạn. Dựa trên phân tích này, chúng tôi đề xuất một phương pháp tái tham số hóa có nguyên tắc để vượt qua các hạn chế. Kết quả lý thuyết của chúng tôi được xác nhận bởi các thí nghiệm số.

1 GIỚI THIỆU
Mạng neural hồi quy (RNN) (Rumelhart et al., 1986) là một trong những mô hình học máy cơ bản nhất để học mối quan hệ giữa dữ liệu tuần tự hoặc thời gian. Chúng có ứng dụng rộng rãi từ dự đoán chuỗi thời gian (Connor et al., 1994), sinh văn bản (Sutskever et al., 2011), nhận dạng giọng nói (Graves & Jaitly, 2014) đến phân loại cảm xúc (Tang et al., 2015). Tuy nhiên, khi có sự phụ thuộc dài hạn trong dữ liệu, kết quả thực nghiệm (Bengio et al., 1994) cho thấy RNN có thể gặp khó khăn trong việc học. Trong bài báo này, chúng tôi điều tra vấn đề này từ góc độ lý thuyết xấp xỉ.

Từ góc độ xấp xỉ, có nhiều loại định lý khác nhau đặc trưng cho mối liên hệ giữa các mối quan hệ mục tiêu và kiến trúc mô hình để học chúng. Xấp xỉ phổ quát (Achieser, 2013, tr. 32) và định lý kiểu Jackson (Achieser, 2013, tr. 187) cung cấp các đảm bảo cơ bản về xấp xỉ và ước lượng lỗi của các hàm mục tiêu đủ chính quy bởi một không gian giả thuyết cụ thể. Một số kết quả như vậy có sẵn cho mô hình hóa chuỗi, bao gồm RNN (Li et al., 2020; 2022). Mặt khác, một lĩnh vực tương đối ít được điều tra trong văn liệu học máy là các định lý kiểu Bernstein (Bernstein, 1914; Li et al., 2022), còn được gọi là các định lý xấp xỉ nghịch đảo. Những kết quả này nhằm đặc trưng tính chính quy của các mối quan hệ mục tiêu, giả định rằng chúng có thể được xấp xỉ hiệu quả với một không gian giả thuyết. Những khái niệm chính quy này phụ thuộc mật thiết vào, do đó đưa ra những hiểu biết quan trọng về, cấu trúc của không gian giả thuyết đang được nghiên cứu.

∗Tác giả liên hệ
1arXiv:2305.19190v4  [cs.LG]  6 Feb 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bài báo này thiết lập một kết quả xấp xỉ nghịch đảo cho việc xấp xỉ các hàm phi tuyến thông qua RNN. Các nghiên cứu trước đó (Li et al., 2020; 2022) chỉ ra rằng các hàm tuyến tính có thể được xấp xỉ phổ quát bởi RNN tuyến tính phải có bộ nhớ suy giảm theo hàm mũ. Hiện tượng này được gọi là lời nguyền bộ nhớ cho RNN tuyến tính. Một câu hỏi tự nhiên là liệu kích hoạt hồi quy phi tuyến được sử dụng trong RNN thực tế có thay đổi tình hình hay không. Điều này quan trọng vì một không gian giả thuyết lớn hơn có thể nâng cao các hạn chế đối với các hàm mục tiêu. Hơn nữa, người ta biết rằng kích hoạt phi tuyến là quan trọng để mạng truyền thẳng đạt được tính phổ quát (Cybenko, 1989). Do đó, việc điều tra xem kết quả Bernstein tuyến tính có tổng quát hóa cho trường hợp xấp xỉ các mối quan hệ chuỗi phi tuyến với RNN phi tuyến hay không là có giá trị. Trong bài báo này, chúng tôi chứng minh rằng RNN phi tuyến vẫn gặp phải lời nguyền bộ nhớ trong xấp xỉ - các hàm phi tuyến có thể được xấp xỉ ổn định bởi RNN với kích hoạt phi tuyến phải có hàm bộ nhớ suy giảm theo hàm mũ. Các khái niệm xấp xỉ ổn định và hàm bộ nhớ có thể được định nghĩa cụ thể. Kết quả của chúng tôi làm rõ quan sát thực nghiệm rằng kiến trúc RNN có những hạn chế vốn có khi mô hình hóa các phụ thuộc thời gian dài.

Tóm lại, những đóng góp chính của chúng tôi là:
1. Chúng tôi mở rộng khái niệm hàm bộ nhớ trong bối cảnh tuyến tính (Li et al., 2020; 2022) sang bối cảnh phi tuyến. Hàm bộ nhớ này có thể được lượng hóa số trong các ứng dụng mô hình hóa chuỗi.
2. Chúng tôi giới thiệu khái niệm xấp xỉ ổn định, đảm bảo rằng một xấp xỉ có khả năng được tìm thấy bởi thuật toán tối ưu hóa dựa trên gradient.
3. Chúng tôi chứng minh, theo hiểu biết tốt nhất của chúng tôi, định lý xấp xỉ kiểu Bernstein đầu tiên cho các chuỗi hàm phi tuyến thông qua RNN phi tuyến. Kết quả của chúng tôi đặc trưng giới hạn thiết yếu của RNN phi tuyến trong việc học mối quan hệ dài hạn. Phân tích của chúng tôi cũng gợi ý rằng tham số hóa thích hợp có thể làm giảm hiện tượng 'lời nguyền bộ nhớ' trong việc học các mục tiêu có bộ nhớ dài. Kết quả lý thuyết được xác nhận bằng các thí nghiệm số.

Ký hiệu. Cho một chuỗi các vector d-chiều được chỉ mục bởi R, x={xt∈Rd:t∈R}, chúng tôi ký hiệu chuẩn supremum bằng ∥x∥∞:= supt∈R|xt|∞. Ở đây |x|∞:= max i|xi|, |x|2:=√∑ix²i, |x|1:=∑i|xi| là các chuẩn max (L∞), L2 và L1 thông thường. Ký hiệu in đậm đại diện cho chuỗi trong khi các chữ cái thường là số vô hướng, vector hoặc hàm. Trong toàn bộ bài báo này, chúng tôi sử dụng ∥ · ∥ để ký hiệu chuẩn trên các chuỗi vector hoặc hàm(số), trong khi | · | (với chỉ số dưới) đại diện cho chuẩn của số, vector hoặc tuple trọng số. Ký hiệu mũ trong bài báo này đề cập đến không gian giả thuyết (hàm) trong khi ký hiệu gốc đề cập đến không gian mục tiêu (hàm).

2 CÔNG THỨC BÀI TOÁN VÀ KẾT QUẢ TRƯỚC ĐÂY VỀ RNN TUYẾN TÍNH
Trong phần này, chúng tôi giới thiệu công thức bài toán của mô hình hóa chuỗi như một bài toán xấp xỉ chuỗi hàm. Chúng tôi đặc biệt chú ý đến việc phân biệt hai loại kết quả: các định lý xấp xỉ tiến (kiểu Jackson) và nghịch đảo (kiểu Bernstein). Đối với lý thuyết xấp xỉ trong học máy, hầu hết các kết quả hiện có tập trung vào các định lý tiến. Tuy nhiên, các định lý xấp xỉ nghịch đảo có tầm quan trọng đáng kể trong việc tiết lộ những hạn chế cơ bản của một phương pháp xấp xỉ. Bài báo hiện tại tập trung vào việc thiết lập những kết quả như vậy trong bối cảnh phi tuyến tổng quát. Chúng tôi kết thúc phần này bằng một đánh giá về các ước lượng kiểu Bernstein đã biết, hiện tại chỉ giới hạn trong trường hợp tuyến tính. Khi làm như vậy, chúng tôi nêu bật định nghĩa bộ nhớ trong trường hợp tuyến tính, điều này thúc đẩy định nghĩa chung của chúng tôi về bộ nhớ cho các chuỗi hàm phi tuyến trong Phần 3.1. Mối quan hệ giữa bộ nhớ và xấp xỉ là trung tâm cho kết quả của chúng tôi.

2.1 BÀI TOÁN XẤP XỈ CHO MÔ HÌNH HÓA CHUỖI
Mục tiêu của mô hình hóa tuần tự là học một mối quan hệ giữa một chuỗi đầu vào x={xt} và một chuỗi đầu ra tương ứng y={yt}. Để dễ phân tích, chúng tôi áp dụng thiết lập thời gian liên tục trong (Li et al., 2020), trong đó t∈R. Đây cũng là một thiết lập tự nhiên cho chuỗi thời gian lấy mẫu không đều (Lechner & Hasani, 2020). Không gian chuỗi đầu vào là X=C0(R,Rd), không gian các hàm liên tục biến mất ở vô cực. Chúng tôi giả định các chuỗi đầu vào và đầu ra liên quan bởi một chuỗi các hàm H={Ht:X 7→R;t∈R} thông qua yt=Ht(x), t∈R. Bài toán xấp xỉ tuần tự có thể được công thức hóa như việc xấp xỉ chuỗi hàm mục tiêu H bởi một chuỗi hàm Ĥ từ một không gian giả thuyết mô hình như RNN.

2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Các định lý xấp xỉ tiến và nghịch đảo. Cho một không gian giả thuyết Ĥ(m) có độ phức tạp m≥1 (ví dụ: RNN có độ rộng m), các định lý xấp xỉ tiến, còn được gọi là các định lý kiểu Jackson, giới hạn lỗi xấp xỉ tối ưu inf Ĥ∈Ĥ(m)∥H−Ĥ∥ ≤C(H, m).

Các kết quả xấp xỉ nghịch đảo (kiểu Bernstein) là các khẳng định "ngược lại" với các kết quả kiểu Jackson. Từ giả thiết khởi đầu rằng một xấp xỉ hiệu quả tồn tại cho một mục tiêu cụ thể H, các kết quả kiểu Bernstein suy ra các không gian xấp xỉ mà H nên thuộc về, tức là nó xác định một biện pháp độ phức tạp hoặc tính chính quy C(·) và chỉ ra rằng C(H) nhất thiết phải hữu hạn. Lấy việc xấp xỉ đa thức lượng giác làm ví dụ, Bernstein (Achieser, 2013, tr. 187) đã chứng minh rằng nếu inf Ĥ∈Ĥ(m)∥H−Ĥ∥ ≤c/mα+δ, cho tất cả m≥1 và một số δ>0, c>0, thì C(H)=|H(α)|<∞, tức là H phải có thể vi phân α lần với các đạo hàm δ-Hölder liên tục.

Các kết quả xấp xỉ nghịch đảo kiểu Bernstein quan trọng trong việc đặc trưng khả năng xấp xỉ cho các không gian giả thuyết. Đối với ví dụ đa thức lượng giác, nó nói rằng chỉ các hàm trơn mới có thể được xấp xỉ hiệu quả, do đó đặt ra một hạn chế cụ thể về khả năng xấp xỉ của các mô hình này. Mục tiêu của chúng tôi trong bài báo này là suy ra các tương tự của kết quả này, nhưng cho việc xấp xỉ các chuỗi hàm phi tuyến tổng quát bằng RNN. Không giống như trường hợp cổ điển nơi khái niệm tính chính quy xuất hiện dưới dạng độ trơn, ở đây chúng tôi sẽ điều tra khái niệm bộ nhớ như một thước đo tính chính quy - một khái niệm mà chúng tôi sẽ làm rõ sau đó.

2.2 KIẾN TRÚC RNN VÀ KẾT QUẢ TRƯỚC ĐÂY
Kiến trúc RNN thời gian liên tục tham số hóa các chuỗi hàm bằng cách đưa vào một hệ thống động ẩn

dht/dt=σ(Wht+Uxt+b)h−∞= 0,
ŷt=c⊤ht, t ∈R.(1)

Ở đây, ŷt∈R là giá trị chuỗi đầu ra dự đoán, và ht∈Rm ký hiệu trạng thái ẩn¹. Tính được định nghĩa tốt của RNN thời gian liên tục sẽ được thảo luận trong Phụ lục A.1. Siêu tham số m cũng được biết đến như chiều ẩn, hoặc độ rộng, của mạng neural hồi quy. Đối với các chiều ẩn khác nhau m, RNN được tham số hóa bởi các trọng số có thể huấn luyện (W, U, b, c), trong đó W∈Rm×m là kernel hồi quy, U∈Rm×d là kernel đầu vào, b∈Rm là bias và c∈Rm là readout. Độ phức tạp của không gian giả thuyết RNN được đặc trưng bởi chiều ẩn m. Tính phi tuyến phát sinh từ hàm kích hoạt σ(·), là một hàm vô hướng được thực hiện theo từng phần tử, như tanh, hardtanh, sigmoid hoặc ReLU. Không gian giả thuyết của RNN do đó là không gian chuỗi hàm sau đây

Ĥ(m)RNN={x7→ŷ thông qua Phương trình (1) :W∈Rm×m, U∈Rm×d, b∈Rm, c∈Rm}. (2)

Trước khi trình bày kết quả chính của chúng tôi, chúng tôi xem xét các kết quả kiểu Jackson và Bernstein đã biết được thiết lập cho RNN tuyến tính, tương ứng với việc đặt σ(z)=z và b=0 trong phương trình 1. Chúng tôi sẽ chú ý đến định nghĩa bộ nhớ cho một chuỗi hàm mục tiêu, và cách nó liên quan đến các tính chất xấp xỉ dưới không gian giả thuyết RNN.

Chúng tôi bắt đầu với một số định nghĩa về (các chuỗi) hàm như được giới thiệu trong (Li et al., 2020).

Định nghĩa 2.1. Cho H={Ht:X 7→ R;t∈R} là một chuỗi các hàm. Không mất tổng quát, chúng tôi giả định Ht(0)=0 (Nếu không, chúng tôi có thể xem xét mối quan hệ được điều chỉnh Hadjustedt(x):=Ht(x)−Ht(0)).

1. (Tuyến tính) Ht là tuyến tính nếu với bất kỳ λ, λ′∈R và x,x′∈X, Ht(λx+λ′x′)=λHt(x)+λ′Ht(x′).

¹Điều kiện biên h−∞=0 nhất quán với các triển khai thực tế như TensorFlow và PyTorch, nơi giá trị ban đầu của trạng thái ẩn được đặt thành zero theo mặc định.

3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
2. (Liên tục) Ht là liên tục nếu với bất kỳ x,x′∈X, limx′→x|Ht(x′)−Ht(x)|=0.
3. (Bị chặn) Ht bị chặn nếu sup{x∈X,x̸=0}|Ht(x)|/∥x∥∞<∞.
4. (Đồng nhất thời gian) H={Ht:t∈R} là đồng nhất thời gian (hoặc bất biến dịch chuyển) nếu mối quan hệ đầu vào-đầu ra giao hoán với dịch chuyển thời gian: cho [Sτ(x)]t=xt−τ là toán tử dịch chuyển, thì H(Sτx)=SτH(x)
5. (Nhân quả) Ht là nhân quả nếu nó không phụ thuộc vào các giá trị tương lai của đầu vào. Tức là, nếu x,x′ thỏa mãn xt=x′t cho bất kỳ t≤t0, thì Ht(x)=Ht(x′) cho bất kỳ t≤t0.
6. (Chính quy) Ht là chính quy nếu với bất kỳ chuỗi {x(n):n∈N} sao cho x(n)s→0 cho hầu hết mọi s∈R, thì limn→∞Ht(x(n))=0.

Các nghiên cứu trong Li et al. (2020; 2022) nghiên cứu việc xấp xỉ các chuỗi hàm thỏa mãn Định nghĩa 2.1 bằng RNN tuyến tính. Một ý tưởng chính là chỉ ra rằng bất kỳ chuỗi hàm như vậy H đều có biểu diễn Riesz (xem Phụ lục A.2 và Phụ lục A.3)

Ht(x)=∫∞0ρ(s)⊤xt−sds, t∈R. (3)

Theo nghĩa này, ρ hoàn toàn xác định H, và việc xấp xỉ nó bằng RNN tuyến tính có thể được rút gọn thành việc nghiên cứu xấp xỉ ρ∈L1([0,∞),Rd) bằng các tổng mũ có dạng (c⊤eWsU)⊤.

Một quan sát quan trọng ở đây là ρ nắm bắt mô hình bộ nhớ của chuỗi hàm tuyến tính mục tiêu: nếu ρ suy giảm nhanh, thì mục tiêu có bộ nhớ ngắn, và ngược lại.

Bằng cách giả định rằng một chuỗi hàm mục tiêu H có thể được xấp xỉ đồng đều bởi các RNN ổn định, thì bộ nhớ của chuỗi hàm mục tiêu phải thỏa mãn eβ0t|ρ(t)|1=o(1) khi t→∞ cho một số β0>0. Điều này được gọi là "lời nguyền bộ nhớ" (Li et al., 2020; 2022) và tiết lộ những hạn chế cơ bản của kiến trúc RNN để nắm bắt các cấu trúc bộ nhớ dài hạn.

Trọng tâm của bài báo này là điều tra xem việc thêm kích hoạt phi tuyến có thay đổi kết quả này hay không. Nói cách khác, liệu lời nguyền bộ nhớ có giữ cho RNN phi tuyến trong việc xấp xỉ các hàm phi tuyến thích hợp tổng quát hay không? Đây là một câu hỏi có ý nghĩa, vì các kết quả kiểu Bernstein về cơ bản ràng buộc các không gian xấp xỉ, và do đó một không gian giả thuyết lớn hơn có thể nới lỏng những ràng buộc như vậy. Chúng tôi mở rộng điều này trong Phụ lục A.4. Một thách thức đáng kể trong bối cảnh phi tuyến là thiếu kết quả biểu diễn Riesz, và do đó người ta cần định nghĩa cẩn thận một khái niệm bộ nhớ nhất quán với ρ trong trường hợp tuyến tính, nhưng vẫn có thể được sử dụng trong bối cảnh phi tuyến để chứng minh các định lý xấp xỉ nghịch đảo. Hơn nữa, chúng ta sẽ cần giới thiệu một khái niệm chung về tính ổn định xấp xỉ, cùng với định nghĩa bộ nhớ tổng quát cho phép chúng ta suy ra một kết quả kiểu Bernstein giữ vượt ra ngoài trường hợp tuyến tính.

3 KẾT QUẢ CHÍNH
Trong phần này, chúng tôi thiết lập một kết quả xấp xỉ kiểu Bernstein cho các chuỗi hàm phi tuyến sử dụng RNN phi tuyến. Trước tiên chúng tôi đưa ra định nghĩa hàm bộ nhớ cho các hàm phi tuyến. Nó tương thích với định nghĩa bộ nhớ trong các hàm tuyến tính và nó có thể được truy vấn và xác minh trong các ứng dụng. Tiếp theo, chúng tôi đề xuất khung xấp xỉ ổn định. Đó là một yêu cầu nhẹ nhàng từ góc độ xấp xỉ, nhưng là một yêu cầu mong muốn từ góc độ tối ưu hóa. Hơn nữa, chúng tôi chỉ ra rằng bất kỳ hàm tuyến tính nào có bộ nhớ suy giảm theo hàm mũ đều có thể được xấp xỉ ổn định. Dựa trên định nghĩa hàm bộ nhớ và khung xấp xỉ ổn định, chúng tôi chứng minh một định lý kiểu Bernstein. Định lý chỉ ra rằng bất kỳ hàm phi tuyến nào có thể được xấp xỉ ổn định bởi RNN phi tuyến tổng quát phải có bộ nhớ suy giảm theo hàm mũ, điều này xác nhận rằng hiện tượng lời nguyền bộ nhớ không giới hạn trong trường hợp tuyến tính. Các xác minh số được bao gồm để chứng minh kết quả.

3.1 HÀM BỘ NHỚ CHO CÁC HÀM PHI TUYẾN
Nhớ lại rằng bộ nhớ cho một chuỗi hàm tuyến tính được định nghĩa bởi biểu diễn Riesz của nó trong Phương trình (3). Mặc dù không có các tương tự tổng quát đã biết của biểu diễn Riesz cho các hàm phi tuyến, chúng ta có thể xem xét các phương tiện khác để trích xuất một hàm bộ nhớ hiệu quả từ H.

4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Cho x∈Rd và xem xét chuỗi đầu vào Heaviside sau đây uxt=x·1[0,∞)(t)=x nếu t≥0, 0 nếu t<0.

Trong trường hợp tuyến tính, lưu ý rằng theo Phương trình (3)

supx̸=0 (d/dt)Ht(ux)/∥ux∥∞= supx̸=0|x⊤ρ(t)|/|x|∞=|ρ(t)|1. (4)

Do đó, các điều kiện về |ρ(t)|1 có thể được thay thế bằng các điều kiện ở vế trái, điều này được định nghĩa tốt cũng cho các hàm phi tuyến. Điều này thúc đẩy định nghĩa sau về hàm bộ nhớ cho các chuỗi hàm phi tuyến.

Định nghĩa 3.1 (Hàm bộ nhớ của các chuỗi hàm phi tuyến). Cho các chuỗi hàm liên tục, nhân quả, chính quy và đồng nhất thời gian H={Ht(x):t∈R} trên X, định nghĩa hàm sau đây như hàm bộ nhớ của H trên đầu vào Heaviside bị chặn ux:

M(H)(t):= supx̸=0 (1/|x|∞)(d/dt)Ht(ux). (5)

Đặc biệt, trong bài báo này chúng tôi xem xét các hàm phi tuyến có hàm bộ nhớ hữu hạn cho mọi t. Không giống như các phương pháp truyền thống đánh giá bộ nhớ thông qua các tác vụ heuristic, phương pháp của chúng tôi cung cấp một đặc trưng chính xác, độc lập với tác vụ về bộ nhớ mô hình. Nếu oracle của hàm mục tiêu có sẵn, hàm bộ nhớ có thể được đánh giá và kết quả được gọi là bộ nhớ được truy vấn. Trong Phụ lục F và Phụ lục G, chúng tôi thảo luận về hàm bộ nhớ được đánh giá trên các đầu vào kiểm tra khác nhau và chỉ ra tính tương đương số trong Phụ lục H. Không có oracle hàm mục tiêu, chúng ta có thể xấp xỉ mục tiêu với mô hình đã học và vẫn đánh giá hàm bộ nhớ. Nếu bộ nhớ được truy vấn đang suy giảm cho tất cả các đầu vào Heaviside, thì chúng ta nói rằng chuỗi hàm phi tuyến tương ứng có bộ nhớ suy giảm. Chúng tôi chứng minh trong Phụ lục B rằng việc truy vấn bộ nhớ cho thấy mô hình bộ nhớ của LSTM và mô hình chuỗi-sang-chuỗi LSTM hai chiều trong phân tích cảm xúc trên đánh giá phim IMDB.

Định nghĩa 3.2 (Bộ nhớ suy giảm). Cho các chuỗi hàm liên tục, nhân quả, chính quy và đồng nhất thời gian H={Ht(x):t∈R} trên X, chúng ta nói nó có bộ nhớ suy giảm nếu:

limt→∞M(H)(t)=0. (6)

Chúng ta nói rằng H có bộ nhớ suy giảm theo hàm mũ nếu với một số β>0,

limt→∞eβtM(H)(t)=0. (7)

Hơn nữa, họ {Hm} có bộ nhớ suy giảm đồng đều nếu

limt→∞supmM(Hm)(t)=0. (8)

Nhận xét 3.3. Yêu cầu bộ nhớ suy giảm trên các hàm đồng nhất thời gian là nhẹ nhàng vì nó được thỏa mãn nếu dHt/dt liên tục tại đầu vào Heaviside, dưới tô pô hội tụ theo điểm (xem Phụ lục A.5). Chúng tôi chỉ ra rằng dHt/dt liên tục theo điểm trên các đầu vào Heaviside cho tất cả RNN, do đó RNN có bộ nhớ suy giảm (xem Phụ lục A.6). Một khái niệm liên quan khác về bộ nhớ mờ dần được thảo luận trong Phụ lục A.7.

3.2 XẤP XỈ ỔN ĐỊNH
Bây giờ chúng tôi giới thiệu khung xấp xỉ ổn định. Hãy viết không gian giả thuyết Ĥ(m) như một không gian tham số Ĥ(m)={Ĥ(·;θm):θm∈Θm} trong đó với mỗi m, Θm là một tập con của không gian Euclidean có chiều phụ thuộc vào m, đại diện cho không gian tham số định nghĩa giả thuyết và Ĥ là một mô hình tham số. Ví dụ, trong trường hợp RNN, tham số θm là (Wm, Um, bm, cm)∈Θm:={Rm×m×Rm×d×Rm×Rm} và m là chiều ẩn của RNN.

Hãy xem xét một tập hợp các chuỗi hàm {Ĥm=Ĥ(·;θm):m≥1} xấp xỉ một chuỗi hàm mục tiêu H. Xấp xỉ ổn định yêu cầu rằng, nếu người ta nhiễu loạn mỗi tham số θm bằng một lượng nhỏ, thì chuỗi xấp xỉ kết quả vẫn nên có lỗi nhiễu loạn liên tục. Đối với tối ưu hóa dựa trên gradient, điều kiện này cần thiết để người ta có thể tìm thấy chuỗi xấp xỉ như vậy, vì các nhiễu loạn nhỏ của tham số nên giữ lỗi nhiễu loạn liên tục để gradient có thể được tính toán. Bây giờ chúng tôi định nghĩa khái niệm ổn định này một cách chính xác.

5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
[Hình 1: Lỗi nhiễu loạn cho các hàm tuyến tính có bộ nhớ suy giảm khác nhau. Đường cong giới hạn dự kiến E(β) được đánh dấu bằng một đường nét đứt màu đen. (a) Đối với các chuỗi hàm tuyến tính có bộ nhớ suy giảm theo hàm mũ, tồn tại một bán kính nhiễu loạn β0 sao cho lỗi nhiễu loạn E(β) cho 0≤β<β0 là liên tục. (b) Xấp xỉ các chuỗi hàm tuyến tính có bộ nhớ suy giảm theo đa thức. Khi chiều ẩn m tăng, bán kính nhiễu loạn nơi lỗi vẫn nhỏ giảm, gợi ý rằng có thể không tồn tại β0 đạt điều kiện xấp xỉ ổn định. Các giao điểm của các đường đang dịch chuyển sang trái khi chiều ẩn m tăng. Đường cong giới hạn dự kiến E(β) không liên tục cho mục tiêu bộ nhớ suy giảm theo đa thức.]

Định nghĩa 3.4. Cho mục tiêu H và mô hình tham số Ĥ(·, θm), chúng tôi định nghĩa lỗi nhiễu loạn cho chiều ẩn m là:

Em(β):= supθ̃m∈{θ:|θ−θm|2≤β}∥H−Ĥ(·;θ̃m)∥ (9)

Hơn nữa, E(β):= lim supm→∞Em(β) là lỗi nhiễu loạn (tiệm cận). Ở đây |θ|2:= max(|W|2,|U|2,|b|2,|c|2).

Định nghĩa 3.5 (Xấp xỉ ổn định thông qua các mô hình tham số). Cho β0>0. Chúng ta nói một chuỗi hàm mục tiêu H thừa nhận xấp xỉ β0-ổn định dưới {Ĥ(m)}, nếu tồn tại một chuỗi các xấp xỉ tham số Ĥm=Ĥ(·, θm), θm∈Θm sao cho

limm→∞∥H−Ĥm∥→0, (10)

và cho tất cả 0≤β≤β0, lỗi nhiễu loạn thỏa mãn E(β) liên tục trong β cho 0≤β≤β0.

Nhận xét 3.6. Có thể thấy rằng xấp xỉ chỉ yêu cầu E(0)=0. Do đó điều kiện xấp xỉ ổn định tổng quát hóa xấp xỉ bằng cách yêu cầu tính liên tục của E xung quanh β=0. Nếu một xấp xỉ không ổn định (E(0)=0, limβ→0E(β)>0), thì khó có thể được tìm thấy bởi các tối ưu hóa dựa trên gradient. Vì khái niệm ổn định của chúng tôi phụ thuộc vào kích thước của các nhiễu loạn trọng số, người ta có thể tự hỏi liệu việc tái chia tỷ lệ chuẩn của trọng số riêng biệt cho mỗi m có đạt được tính ổn định hay không. Có hai vấn đề với phương pháp này. Thứ nhất, phiên bản tái chia tỷ lệ không còn là không gian giả thuyết RNN thông thường. Thứ hai, để đạt được tính ổn định, quy tắc tái chia tỷ lệ có thể phụ thuộc vào thông tin của chuỗi hàm mục tiêu, mà chúng ta không có quyền truy cập trong thực tế. Chúng tôi thảo luận điều này chi tiết trong Phụ lục C.

Tiếp theo, chúng tôi chứng minh rằng điều kiện xấp xỉ ổn định không quá nghiêm ngặt theo nghĩa rằng, cho chuỗi hàm tuyến tính có bộ nhớ suy giảm theo hàm mũ (Phương trình (7)) thừa nhận xấp xỉ ổn định. Chúng tôi cho thấy xác minh số của kết quả này trong Hình 1. Việc xấp xỉ hàm tuyến tính có suy giảm theo hàm mũ có thể được thấy trong bảng trái tại β=0 vì việc tăng chiều ẩn m sẽ làm cho lỗi ước lượng giảm về 0 trên β∈[0, β0]. Xấp xỉ ổn định có thể được xác minh rằng cho nhiễu loạn dương β, việc thêm chiều ẩn không làm tăng lỗi nhiễu loạn E(β). Ngược lại, cho hàm tuyến tính có bộ nhớ suy giảm theo đa thức, lỗi nhiễu loạn E(β) không liên tục tại β=0.

3.3 KẾT QUẢ XẤP XỈ KIỂU BERNSTEIN CHO RNN PHI TUYẾN
Bây giờ chúng tôi trình bày kết quả chính của bài báo này, đó là kết quả xấp xỉ kiểu Bernstein cho các chuỗi hàm phi tuyến sử dụng RNN phi tuyến. Câu hỏi chính là liệu việc thêm tính phi tuyến có làm giảm bớt hạn chế lời nguyền bộ nhớ và cho phép xấp xỉ hiệu quả các hàm có suy giảm bộ nhớ chậm hay không. Trong phần sau, chúng tôi chỉ ra rằng câu trả lời là phủ định, và một kết quả xấp xỉ kiểu Bernstein tương tự giữ cho các hàm phi tuyến và RNN với một lớp các kích hoạt hồi quy, bao gồm các kích hoạt hardtanh/tanh được sử dụng thường xuyên nhất.

6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Định nghĩa 3.7. Chúng tôi xem xét chuẩn kiểu Sobolev:

∥H−Ĥ∥W1= supt(∥Ht−Ĥt∥∞+∥dHt/dt−dĤt/dt∥∞). (11)

Chuẩn hàm phi tuyến được cho bởi ∥Ht∥∞:= supx̸=0|Ht(x)|/∥x∥∞+|Ht(0)|= supx̸=0|Ht(x)|/∥x∥∞.

Định nghĩa 3.8. Chúng tôi xem xét họ các kích hoạt Lipschitz đơn điệu bị chặn sau đây là tuyến tính cục bộ/tanh cục bộ xung quanh 0: Cho một số Z0>0,

A0:={σ(·)|σ(z)=cσz, cσ>0,|z|<Z0}, (12)
A1:={σ(·)|σ(0)=0; σ có thể vi phân, σ′(z)=a−bσ(z)2, a, b≥0,|z|<Z0}. (13)

Lưu ý A0∪A1 bao gồm các kích hoạt thường được sử dụng như hardtanh và tanh. Đặc biệt, tanh tương ứng với trường hợp a=b=1 cho A1 với Z0=∞.

Định lý 3.9. Giả sử H là một chuỗi các hàm bị chặn liên tục, nhân quả, chính quy và đồng nhất thời gian trên X với bộ nhớ suy giảm. Cho kích hoạt thuộc A0∪A1. Giả sử H được xấp xỉ β0-ổn định bởi một chuỗi RNN {Ĥ(·, θm)}∞m=1 trong chuẩn được định nghĩa trong Phương trình (11). Nếu các hàm bộ nhớ của các mô hình nhiễu loạn suy giảm đồng đều (như được định nghĩa trong Định nghĩa 3.2) và các chuẩn trọng số bị chặn đồng đều trong m:

supm|θm|2<∞. (14)

Thì hàm bộ nhớ M(H)(t) của mục tiêu suy giảm theo hàm mũ:

limt→∞eβtM(H)(t)=0, β<β0. (15)

Các chứng minh được bao gồm trong Phụ lục A.8. Cho rằng các xấp xỉ được yêu cầu phải ổn định, tính chất bộ nhớ suy giảm đảm bảo rằng đạo hàm của các trạng thái ẩn cho mô hình nhiễu loạn tiếp cận 0 khi thời gian t→∞. Sử dụng định lý Hartman-Grobman, chúng ta có thể thu được các giới hạn về các giá trị riêng của các ma trận Wm. Trong Phụ lục J, chúng tôi chứng minh rằng các phương pháp của chúng tôi có thể được tổng quát hóa để phân tích động học của GRU và LSTM. Khung tương tự trong khi động học ẩn cuối cùng của GRU và LSTM yêu cầu nhiều kỹ thuật hơn để phân tích.

Giải thích Định lý 3.9. Kết quả chính của chúng tôi (Định lý 3.9) mở rộng kết quả tuyến tính trước đó từ Li et al. (2022). Thay vì độ trơn (được đo bằng chuẩn Sobolev) như một biện pháp tính chính quy, kết quả kiểu Bernstein của RNN xác định bộ nhớ suy giảm theo hàm mũ (eβtM(H)(t)→0) như biện pháp tính chính quy đúng. Nếu chúng ta có thể xấp xỉ một số hàm mục tiêu ổn định bằng RNN phi tuyến, thì mục tiêu đó phải có bộ nhớ suy giảm theo hàm mũ. Trước đây điều này chỉ được biết cho trường hợp tuyến tính, nhưng cho trường hợp phi tuyến, ngay cả việc thêm tính phi tuyến tăng đáng kể độ phức tạp của mô hình, nó cũng không khắc phục hạn chế bộ nhớ thiết yếu của RNN.

Từ góc độ số, định lý ngụ ý hai khẳng định sau, và chúng tôi cung cấp xác minh số cho mỗi khẳng định. Thứ nhất, nếu hàm bộ nhớ của một chuỗi hàm mục tiêu suy giảm chậm hơn hàm mũ (ví dụ: M(H)(t)=C/(t+1)1.5), việc tối ưu hóa là khó khăn và xấp xỉ trong Hình 2 được đạt được ở 1000 epoch trong khi thông thường bộ nhớ suy giảm theo hàm mũ đạt được xấp xỉ ở 10 epoch. Khi xấp xỉ được đạt, có thể thấy trong Hình 2 rằng, đối với tỷ lệ nhiễu loạn β lớn hơn, không có tính ổn định nhiễu loạn. Thứ hai, nếu một chuỗi hàm mục tiêu có thể được xấp xỉ tốt và bán kính ổn định β0 của xấp xỉ có thể được chỉ ra là dương, thì chuỗi hàm mục tiêu nên có bộ nhớ suy giảm theo hàm mũ. Xem Hình 3 cho xấp xỉ được lọc với yêu cầu tính ổn định nhiễu loạn. (Xem Hình 5 trong Phụ lục B cho việc xác nhận bộ nhớ trên tác vụ phân loại cảm xúc tổng quát.)

7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
[Hình 2: Mục tiêu có bộ nhớ suy giảm theo đa thức + xấp xỉ (đạt được ở 1000 epoch) → không có tính ổn định. Tương tự như trường hợp hàm tuyến tính, khi xấp xỉ các hàm phi tuyến có bộ nhớ suy giảm theo đa thức bằng tanh RNN, các giao điểm của các đường cong đang dịch chuyển sang trái khi chiều ẩn m tăng.]

[Hình 3: Xấp xỉ ổn định thông qua RNN ngụ ý bộ nhớ suy giảm theo hàm mũ. Chúng tôi xây dựng một số mô hình RNN được khởi tạo ngẫu nhiên làm mô hình giáo viên với chiều ẩn lớn (m=256). Khi xấp xỉ mô hình giáo viên với một loạt mô hình RNN học sinh, chúng ta có thể xác minh số tính ổn định của xấp xỉ (bảng trái). Chúng ta có thể áp dụng một bộ lọc: chúng ta chỉ chọn những mô hình giáo viên có thể được xấp xỉ và các xấp xỉ ổn định (với lỗi nhiễu loạn Em(β) có bán kính ổn định dương). Chúng tôi phát hiện rằng những giáo viên duy nhất còn lại là những người có hàm bộ nhớ suy giảm theo hàm mũ. Một ví dụ được hiển thị trong bảng phải.]

3.4 THAM SỐ HÓA THÍCH HỢP CHO PHÉP XẤP XỈ ỔN ĐỊNH
Hiểu biết chính của Định lý 3.9 có thể được tóm tắt như sau: để xấp xỉ các mục tiêu có bộ nhớ suy giảm không theo hàm mũ, các trọng số hồi quy của RNN phải có phần thực của giá trị riêng tiếp cận 0 ở phía âm. Tuy nhiên, nếu phần thực của giá trị riêng lớn nhất đang tiếp cận không, thì tính ổn định của nó dưới nhiễu loạn sẽ giảm. Đây là lý do tại sao xấp xỉ và tính ổn định không thể đạt được cùng một lúc nếu bộ nhớ của mục tiêu không suy giảm theo hàm mũ. Vấn đề ổn định có thể được giải quyết thông qua tái tham số hóa vì bán kính ổn định không giảm ngay cả khi các giá trị riêng đang tiếp cận 0. Nếu chúng ta tái tham số hóa các trọng số hồi quy sao cho nó tiếp cận không và vẫn ổn định (tức là, phần thực của giá trị riêng là âm) dưới nhiễu loạn, thì kiến trúc này sẽ duy trì tính ổn định trong khi có khả năng xấp xỉ. Chúng ta có thể thực hiện điều này bằng cách thay thế trọng số hồi quy bằng một hàm ma trận liên tục, mà chúng ta sẽ gọi là "tái tham số hóa ổn định"

g:Rm×m→Rm×m,−, g(M)=W. (16)

RNN được tái tham số hóa này ổn định vì phần thực của giá trị riêng luôn âm. Chúng tôi chỉ ra có một số phương pháp để đạt được tái tham số hóa này: Hàm mũ g(M)=−eM và hàm softplus g(M)=−log(1+eM) ánh xạ các giá trị riêng của M vào phạm vi âm (xem Hình 4 và Hình 8 cho xấp xỉ ổn định của hàm tuyến tính có bộ nhớ suy giảm theo đa thức). LRU (Orvieto et al., 2023) đề xuất tham số hóa phần thực của giá trị riêng bằng exp(−exp(λ)), tương ứng với trường hợp rời rạc cho g(M)=−eM.

8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
[Hình 4: Xấp xỉ ổn định của các hàm tuyến tính có bộ nhớ suy giảm theo đa thức bằng RNN tuyến tính với tái tham số hóa exp và softplus. Đường cong giới hạn nét đứt E(β) phải liên tục.]

4 NGHIÊN CỨU LIÊN QUAN
Nhiều kết quả khác nhau đã được thiết lập trong lý thuyết xấp xỉ RNN, xem Sontag (1998); Hanson et al. (2021) và các tài liệu tham khảo trong đó. Đối với các tập chỉ số đầu vào không bị chặn, xấp xỉ Lp được thiết lập bởi Gonon & Ortega (2020). Trong Gonon & Ortega (2021), định lý xấp xỉ phổ quát được xây dựng cho các hàm có bộ nhớ mờ dần trong thiết lập thời gian rời rạc. Trong Li et al. (2020), định lý xấp xỉ phổ quát và định lý xấp xỉ kiểu Jackson đặc trưng mật độ và tốc độ của RNN tuyến tính áp dụng cho các hàm tuyến tính. Hầu hết các kết quả hiện có là các định lý xấp xỉ tiến (kiểu Jackson), giới hạn trên lỗi xấp xỉ tối ưu. Có liên quan nhất là kết quả kiểu Bernstein được chứng minh trong Li et al. (2022), nơi đã được chứng minh rằng các chuỗi hàm tuyến tính có thể được xấp xỉ hiệu quả bởi RNN tuyến tính phải có bộ nhớ suy giảm theo hàm mũ. Tuy nhiên, hạn chế chính của kết quả trên là thiết lập tuyến tính cho cả mô hình và mục tiêu.

Khái niệm tính ổn định xấp xỉ là một trong những khái niệm trung tâm mà chúng tôi khai thác trong bài báo này. Chúng tôi lưu ý rằng trong lý thuyết xấp xỉ cổ điển, xấp xỉ ổn định có nhiều định nghĩa khác nhau tùy thuộc vào thiết lập (DeVore et al., 2021). Ví dụ, trong xấp xỉ phi tuyến (DeVore, 1998), một chuỗi xấp xỉ ổn định {Hm} của H là một chuỗi thỏa mãn |Hm|≤C|H| cho một số hằng số tuyệt đối C>0 và tất cả m. Phương pháp này được thực hiện để chỉ ra sự không tồn tại của quy trình ổn định để xấp xỉ các hàm từ các mẫu cách đều với hội tụ mũ trên các hàm giải tích (Platte et al., 2011). Khái niệm ổn định này về điều kiện của bài toán xấp xỉ. Ngược lại, khái niệm ổn định của chúng tôi được giới thiệu trong Phần 3.2 tương tự hơn với yêu cầu tính liên tục đồng đều. Liên quan đến mô hình hóa chuỗi, một khái niệm liên quan nhưng khác về tính ổn định động (Hanson & Raginsky, 2020) được sử dụng để chứng minh một kết quả kiểu Jackson cho mô phỏng phổ quát của các hệ thống động. Ở đó, tính ổn định giống như yêu cầu tính liên tục đồng đều (trong đầu vào) của bản đồ dòng chảy của động học ẩn RNN. Trong thực tế, một số dạng cụ thể của tái tham số hóa ổn định mà chúng tôi định nghĩa trong Phương trình (16) đã được áp dụng trong tối ưu hóa mô hình không gian trạng thái (Gu et al., 2020; 2021; Smith et al., 2023; Wang & Xue, 2023).

5 KẾT LUẬN
Tóm lại, chúng tôi suy ra một kết quả xấp xỉ nghịch đảo trong bối cảnh mô hình hóa chuỗi sử dụng RNN phi tuyến. Chúng tôi chỉ ra rằng, giả định rằng một mối quan hệ chuỗi mục tiêu nhất định (được hiểu về mặt toán học như một chuỗi hàm phi tuyến) có thể được xấp xỉ ổn định bởi RNN với kích hoạt phi tuyến, thì cấu trúc bộ nhớ của chuỗi hàm mục tiêu phải giảm theo hàm mũ. Điều này đặt ra những hạn chế tiên nghiệm về khả năng của RNN để học bộ nhớ dài hạn trong mô hình hóa chuỗi, và làm rõ quan sát thực nghiệm rằng RNN không hoạt động tốt cho những vấn đề như vậy. Từ quan điểm xấp xỉ, kết quả của chúng tôi cho thấy rằng thất bại này không chỉ do các thuật toán học (ví dụ: bùng nổ gradient), mà còn do những hạn chế cơ bản của không gian giả thuyết RNN. Đồng thời, phân tích của chúng tôi chỉ ra tái tham số hóa như một phương pháp có nguyên tắc để khắc phục những hạn chế của RNN khi nói đến bộ nhớ dài hạn và chúng tôi chứng minh hiệu quả của nó bằng cách học các hàm tuyến tính có bộ nhớ đa thức.

9

--- TRANG 10 ---
[Phần acknowledgments và references tiếp tục với danh sách các tài liệu tham khảo...]
