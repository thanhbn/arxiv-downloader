# 2404.07904.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rnn/2404.07904.pdf
# Kích thước tệp: 1041275 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
HGRN2: RNN Tuyến tính Có Cổng với Mở rộng Trạng thái
1Zhen Qin†,2Songlin Yang†,3Weixuan Sun,3Xuyang Shen,3Dong Li,3Weigao Sun,
3Yiran Zhong∗
1TapTap2MIT CSAIL3OpenNLPLab, Shanghai AI Lab
/gtbhttps://github.com/OpenNLPLab/HGRN2
Tóm tắt
RNN tuyến tính có cổng phân cấp (HGRN, Qin et al. 2023c) đã chứng minh
tốc độ huấn luyện cạnh tranh và hiệu suất trong mô hình hóa ngôn ngữ đồng thời
cung cấp suy luận hiệu quả. Tuy nhiên, kích thước trạng thái hồi quy của HGRN
vẫn tương đối nhỏ, hạn chế khả năng biểu đạt của nó. Để giải quyết vấn đề này,
chúng tôi giới thiệu một cơ chế mở rộng trạng thái dựa trên tích ngoài đơn giản,
làm tăng đáng kể kích thước trạng thái hồi quy mà không cần thêm bất kỳ tham số
nào. Cải tiến này cũng cung cấp một cách giải thích tương tự attention tuyến tính
cho HGRN2, cho phép huấn luyện hiệu quả phần cứng. Các thí nghiệm mở rộng
của chúng tôi xác minh lợi thế của HGRN2 so với HGRN một cách nhất quán
qua các thiết lập khác nhau và cạnh tranh với các mô hình hồi quy khác.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã đạt được thành công thực nghiệm đáng kể trong những năm gần đây.
Tuy nhiên, việc phục vụ các LLM dựa trên Transformer rất tốn kém do quản lý bộ nhớ cache KV đắt đỏ.
Mặt khác, mạng nơ-ron hồi quy (RNN) cung cấp độ phức tạp suy luận tuyến tính với kích thước trạng thái không đổi, khiến chúng lý tưởng cho việc phục vụ. Do đó, có sự quan tâm đáng kể trong việc nghiên cứu các mô hình hồi quy tuyến tính có thể song song hóa, chẳng hạn như RNN tuyến tính (Peng et al., 2023; Orvieto et al., 2023; Qin et al., 2023c; De et al., 2024), attention tuyến tính (Sun et al., 2023; Qin et al., 2023b; Yang et al., 2023; 2024; Arora et al., 2024), và mô hình không gian trạng thái (Gu et al., 2022a; Smith et al., 2023; Gu & Dao, 2023; Dao & Gu, 2024).

RNN có kích thước trạng thái hồi quy cố định để mã hóa tất cả thông tin lịch sử. Do đó, điều quan trọng đối với RNN là (i) sử dụng hiệu quả các trạng thái có kích thước cố định và (ii) tăng kích thước trạng thái hồi quy để nâng cao khả năng bộ nhớ. Các cải tiến gần đây trong RNN tuyến tính theo cách tiếp cận này, kết hợp các kỹ thuật như độ suy giảm phụ thuộc dữ liệu và mở rộng trạng thái.

Độ suy giảm phụ thuộc dữ liệu (còn được gọi là cổng quên) rất quan trọng đối với RNN (van der Westhuizen & Lasenby, 2018), cho phép chúng giữ lại có chọn lọc thông tin hữu ích trong khi xóa thông tin không liên quan. Điều này cho phép trạng thái hồi quy có kích thước cố định lưu trữ chỉ thông tin quan trọng một cách hiệu quả hơn. HGRN (Qin et al., 2023c) đầu tiên nhấn mạnh tầm quan trọng của độ suy giảm phụ thuộc dữ liệu đối với RNN tuyến tính. Nhiều mô hình hồi quy tuyến tính gần đây, như Mamba (Gu & Dao, 2023), Gated Linear Attention (GLA, Yang et al. 2023), Griffin (De et al., 2024), và RWKV-6 (Peng et al., 2024), cũng sử dụng độ suy giảm phụ thuộc dữ liệu.

Tuy nhiên, HGRN không tăng kích thước trạng thái hồi quy, bị hạn chế nghiêm trọng bởi khả năng bộ nhớ hạn chế. Hạn chế này ngăn cản nó đạt được hiệu suất mô hình hóa ngôn ngữ giống như LLaMa (Touvron et al., 2023a;b), như đã lưu ý trong Qin et al. (2024). Các mô hình hồi quy tuyến tính tiên tiến gần đây, như Mamba, GLA, và RWKV-6, đã giải quyết vấn đề này bằng cách sử dụng kỹ thuật mở rộng trạng thái. Những kỹ thuật này tăng đáng kể kích thước trạng thái hồi quy và do đó nâng cao khả năng bộ nhớ, điều đã được chứng minh là
∗Tác giả liên hệ. Email: zhongyiran@gmail.com. †Đóng góp ngang nhau.
1arXiv:2404.07904v2 [cs.CL] 19 Aug 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
quan trọng đối với hiệu suất mô hình hóa ngôn ngữ và có mối tương quan trực tiếp với khả năng truy xuất
(Arora et al., 2024).

Trong công trình này, chúng tôi đề xuất HGRN2, nhằm tăng kích thước trạng thái hồi quy cho HGRN
trong khi vẫn giữ cả hiệu quả tham số và huấn luyện. Chúng tôi đầu tiên khám phá các ma trận có cấu trúc
để mở rộng kích thước trạng thái trực tiếp theo cách tiết kiệm tham số. Theo thực nghiệm, chúng tôi phát hiện
rằng cách tiếp cận này cải thiện hiệu suất mô hình hóa ngôn ngữ nhưng vẫn gặp phải những thiếu hiệu quả trong huấn luyện, làm hạn chế việc mở rộng kích thước trạng thái hồi quy. Lấy cảm hứng từ attention tuyến tính,
chúng tôi sau đó khám phá việc sử dụng cơ chế mở rộng trạng thái dựa trên tích ngoài không tham số.
Cách tiếp cận này cho phép mở rộng hiệu quả kích thước trạng thái hồi quy trong quá trình huấn luyện mà không
cần thêm tham số. Do dạng nhân ma trận của attention tuyến tính,
chúng tôi có thể tận dụng thuật toán huấn luyện attention tuyến tính hiệu quả phần cứng được mô tả trong
Yang et al. (2023); Qin et al. (2024) cho các thí nghiệm quy mô lớn. Kết quả là, HGRN2 có thể
được coi là một tham số hóa cải tiến của GLA.

Chúng tôi đánh giá HGRN2 một cách mở rộng qua nhiều nhiệm vụ khác nhau, chứng minh rằng nó luôn luôn
vượt trội so với HGRN1 trong nhiều lĩnh vực. Trong mô hình hóa ngôn ngữ, chúng tôi cho thấy HGRN2
rất cạnh tranh so với các mô hình hiệu quả dưới bậc hai khác.

2 Kiến thức cơ bản

2.1 RNN tuyến tính có cổng

Cho đầu vào x∈RN×d, trong đó độ dài chuỗi là N và chiều mô hình là d, một
lớp hồi quy tuyến tính có cổng tối giản (Martin & Cundy, 2018) biến đổi đầu vào x thành
trạng thái ẩn h∈RN×d và đầu ra y∈RN×d, được định nghĩa như sau:

gt=σ(Uxt+bu),
it=τ(Vxt+bv),
ot=σ(Wxt+bw),
ht=gt⊙ht−1+(1−gt)⊙it,
yt=ht⊙ot,(1)

trong đó ⊙ biểu thị tích theo từng phần tử; σ là hàm sigmoid, và τ là một hàm
kích hoạt phi tuyến (chúng tôi chọn sử dụng SiLU); it là vector đầu vào; gt và ot tương ứng là
cổng quên và cổng đầu ra. Cổng đầu vào được liên kết với cổng quên như 1−gt, một
cách tiếp cận phổ biến được sử dụng trong nhiều RNN có cổng như GRU (Chung et al., 2014).

2.2 HGRN (Qin et al., 2023c)

So với Phương trình 1, HGRN thực hiện hai điều chỉnh: (i) hồi quy có giá trị phức và (ii)
cổng quên với các giá trị cận dưới tăng đơn điệu từ lớp dưới lên lớp trên.

Đối với (i), tương tự như các phát hiện trong Gu & Dao (2023) và De et al. (2024), chúng tôi phát hiện theo thực nghiệm rằng hồi quy có giá trị phức không cần thiết, như được hiển thị trong Bảng 1. Lý do tại sao
HGRN thấy nó hữu ích là do mở rộng trạng thái: trạng thái hồi quy có giá trị phức có kích thước gấp đôi so với trạng thái hồi quy có giá trị thực. Nếu chúng ta trực tiếp mở rộng kích thước trạng thái hồi quy có giá trị thực từ d đến 2d, hiệu suất mô hình hóa ngôn ngữ trên corpus Wikitext-103 thậm chí còn tốt hơn. Do đó, chúng tôi chỉ xem xét hồi quy có giá trị thực sau đó.

Đối với (ii), giả sử tổng số lớp là L. HGRN giới thiệu một ma trận có thể học được độc lập với dữ liệu Γ∈RL×d, trong đó Γi
biểu thị các giá trị thấp nhất của cổng quên cho lớp thứ i tại tất cả các bước thời gian. HGRN lập luận rằng cận dưới này nên tăng đơn điệu từ dưới lên trên, khuyến khích các lớp dưới mô hình hóa phụ thuộc cục bộ ngắn hạn và các lớp trên mô hình hóa phụ thuộc dài hạn. Để thực thi tính đơn điệu này, HGRN sử dụng toán tử cumax (Shen et al., 2018):

β:=cumax(Γ) =cumsum(softmax(Γ, dim=0), dim=0)∈RL×d,βi=[β]i∈Rd.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Bảng 1: So sánh HGRN thực và HGRN phức. Chúng tôi phát hiện rằng HGRN thực với
kích thước trạng thái gấp đôi hoạt động tốt hơn HGRN phức trong mô hình hóa ngôn ngữ Wiki103.

Phương pháp Kích thước trạng thái PPL(val) PPL(test) Tham số (M)
HGRN1 phức 2d 24.14 24.82 46.25
HGRN1 thực d 25.34 26.12 46.24
HGRN1 thực 2d 24.04 24.64 45.46

Để ngăn cận dưới đạt giá trị một ở lớp cao nhất, HGRN trừ tất cả giá trị β
bằng β0, làm cho cận dưới cho lớp đầu tiên bằng không. Sau khi có được các giá trị cận dưới, cổng quên gt học phần dư thay vào đó, dẫn đến cổng quên mới ft:

fi_t=βi+(1−βi)⊙gi_t,
hi_t=fi_t⊙hi_{t−1}+(1−fi_t)⊙ii_t,(2)

trong đó chỉ số trên biểu thị chỉ số lớp. Cách tiếp cận cận dưới cộng này đã được chứng minh là giảm thiểu vấn đề cổng bão hòa (Gu et al., 2020).

3 Phương pháp

3.1 Khám phá các phương pháp mở rộng trạng thái

Mục tiêu của công trình này là mở rộng kích thước trạng thái hồi quy HGRN từ d đến nd, trong đó n
là tỷ lệ mở rộng trạng thái. Tuy nhiên, nếu chúng ta sử dụng tham số hóa gốc trong Phương trình 1, các ma trận U,V,W sẽ có chiều d×nd, trở nên rất không hiệu quả về tham số
khi n lớn. Lý tưởng nhất, số lượng tham số nên khoảng d2, như trong trường hợp gốc cho mỗi phép chiếu. Để đạt được điều này, chúng tôi đầu tiên xem xét việc sử dụng ma trận có cấu trúc (ví dụ, ma trận thứ hạng thấp) để thay thế ma trận chiếu dày đặc Rd→Rnd, như được mô tả trong Bảng 2.

Bảng 2: Phương pháp Mở rộng Trạng thái Tiết kiệm Tham số (PESE) sử dụng ký hiệu Einstein Summation. Màu xanh biểu thị đầu vào, Màu đen biểu thị trọng số độc lập với dữ liệu, và Màu đỏ biểu thị đầu ra. Chúng tôi liệt kê Einstein Summation cho thứ hạng thấp (LR), biến đổi tuyến tính nhóm (GLT), biến đổi tuyến tính nhóm với tương tác (GLTI), tích Khatri-Rao (KRP), và tích Kronecker (KP).

Phương pháp Phương trình Số tham số
Ngây thơ d,d nd→nd nd2
LR d,d r,r nd→nd dr(n+1)≈d2
GLT d=(ne)→ne ne,ne d→nd d2
GLTI d=(ne)→ne ne,ne d,nn→nd d2+n2
KRP d,n d→nd nd
KP d,d d,n→nd d2+n

Sau khi có được g,i,o được mở rộng, chúng tôi đưa chúng vào các lớp hồi quy tuyến tính có cổng theo từng phần tử như trong Phương trình 1 và Phương trình 2, tạo ra vector đầu ra yt∈Rn×d. Để chiếu chiều mở rộng trở về chiều gốc, chúng tôi đơn giản tính tổng theo chiều tương ứng với n.

Kết quả được hiển thị trong Bảng 3. Chúng tôi phát hiện rằng mở rộng trạng thái nói chung cải thiện hiệu suất, với ma trận thứ hạng thấp hoạt động tốt nhất trong số các ứng viên này.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Bảng 3: Nghiên cứu loại bỏ PESE. Nghiên cứu loại bỏ về các phương pháp tiết kiệm tham số khác nhau, như được mô tả trong Bảng 2. Mỗi mô hình được huấn luyện trên 10 tỷ token từ bộ dữ liệu Pile.

Phương pháp n PPL Tham số (M)
Xfmr - 5.16 380
Xfmr++ - 4.62 386
HGRN1 1 5.10 379
LR 4 4.76 385
8 4.77 386
GLT 4 5.06 386
GLTI 4 4.83 386
KRP 4 5.08 386
KP 4 5.06 386
HGRN2 4 4.79 385
8 4.73 385
128 4.62 385

Tuy nhiên, những phương pháp này gặp phải vấn đề thiếu hiệu quả huấn luyện, vì chúng yêu cầu thực hiện hồi quy tuyến tính theo từng phần tử trong chiều cao (tức là nd). Vì những phép toán theo từng phần tử này không thể tận dụng tensor core (một đơn vị nhân ma trận nhanh trên GPU), chi phí FLOP và I/O tăng mạnh làm chậm đáng kể quá trình huấn luyện khi n lớn. Chúng tôi nhận thấy rằng điều này tương tự như trường hợp trong Mamba1, yêu cầu tỷ lệ mở rộng tương đối nhỏ (tức là n=16) và triển khai CUDA I/O hiệu quả tùy chỉnh để đạt được tốc độ chạy hợp lý.

Trong phần tiếp theo, chúng tôi khám phá một chiến lược thay thế không thay thế các ma trận chiếu dày đặc bằng các ma trận có cấu trúc mà thay vào đó thay đổi các phép toán cổng theo từng phần tử trong Phương trình 1 thành các phép toán ma trận/vector khác tương tự như những phép toán được sử dụng trong attention tuyến tính. Cách tiếp cận này cho phép huấn luyện hiệu quả hơn.

3.2 HGRN2

[Biểu đồ cấu trúc mạng HGRN2 với các thành phần Input, Linear, Output, HGRU2, GLU và các phép toán Recurrent Computing]

Hình 1: Cấu trúc Mạng của HGRN2. Mỗi lớp HGRN2 bao gồm một lớp token mixer, HGRU2, và một channel mixer, GLU. HGRU2 sử dụng tính toán hồi quy như được mô tả trong Phương trình 3, trong đó it là trạng thái đầu vào, gt là cổng quên, ot là cổng đầu ra, và βi là cận dưới cho lớp i.

Sự thay đổi từ HGRN1 sang HGRN2 đơn giản nhưng hiệu quả. Đối với cổng đầu vào, HGRN2 thay thế tích theo từng phần tử bằng tích ngoài để mở rộng trạng thái. Do đó, ht∈Rd×d, và HGRN2 đầu tiên chéo hóa vector cổng quên và sử dụng tích ma trận để cập nhật trạng thái ẩn. Đối với cổng đầu ra, HGRN2 thay thế tích theo từng phần tử bằng phép nhân ma trận-vector để chiếu trạng thái mở rộng trở về chiều gốc. Phương trình hồi quy của HGRN2 như sau:

ht=ht−1·Diag{ft}+it⊗(1−ft)∈Rd×d,
yt=ht·ot∈Rd,(3)

trong đó Diag biểu thị việc chéo hóa vector, · biểu thị tích ma trận, và ⊗ biểu thị tích ngoài.

1Mặc dù Mamba có cơ chế attention (Ali et al., 2024) tương tự như trong attention tuyến tính, việc tính toán attention không thể được biểu diễn dưới dạng phép nhân ma trận như attention tuyến tính, và do đó không tạo điều kiện cho gia tốc GPU dựa trên tensor core, như đã được thừa nhận rõ ràng trong Mamba2 (Dao & Gu, 2024).

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

[Biểu đồ thể hiện mối quan hệ giữa Expand Ratio và PPL cho hai tập dữ liệu]

Hình 2: Nghiên cứu loại bỏ Tỷ lệ Mở rộng (Chiều Head). Chúng tôi kiểm tra mối quan hệ giữa PPL (Perplexity) và tỷ lệ mở rộng trên tập dữ liệu Wikitext-103 (Merity et al., 2017) (trái) và một tập con của tập dữ liệu Pile (Gao et al., 2020) (phải).

Biến thể Đa đầu. Độ phức tạp của hồi quy tăng mạnh từ O(BNd) lên O(BNd2) do mở rộng trạng thái. Để giải quyết điều này, chúng tôi giới thiệu một biến thể đa đầu của HGRN (tương tự như trong attention tuyến tính) sao cho độ phức tạp được giảm xuống O(BNd2/H) cho số đầu H, hiệu quả làm cho kích thước trạng thái d2/H, tức là tỷ lệ mở rộng n=dh=d/H.2 Chúng tôi thực hiện nghiên cứu loại bỏ về tỷ lệ mở rộng (hoặc chiều đầu) n=d/H, như được hiển thị trong Hình 2. Kết quả cho thấy mở rộng trạng thái cải thiện đáng kể hiệu suất mô hình hóa ngôn ngữ. Tuy nhiên, khi chiều đầu (tức là tỷ lệ mở rộng trạng thái) vượt quá 128, lợi ích hiệu suất giảm dần. Để cân bằng chi phí tính toán và hiệu suất, chúng tôi chọn dh=128 cho các thí nghiệm chính.

So sánh với GLA. Điều quan trọng cần lưu ý là dạng hồi quy trong HGRN2 giống hệt với GLA (Yang et al., 2023), ngoại trừ tham số hóa cụ thể. Chúng tôi liệt kê các tương ứng giữa hai tham số hóa trong Bảng 4. Như được hiển thị, cổng đầu ra trong HGRN2 tương ứng với query trong GLA, trong khi cổng đầu ra trong GLA bị bỏ qua trong HGRN2. Vector key trong GLA tương ứng với cổng đầu vào trong HGRN2, được liên kết với cổng quên, do đó tiết kiệm tham số.

Bảng 4: Sự tương ứng giữa HGRN2 và GLA như sau.

HGRN2 GLA
o(cổng đầu ra) q(vector query)
1−f(cổng đầu vào) k(vector key)
i(vector đầu vào) v(vector value)
f(cổng quên) α(cổng quên)
− o(cổng đầu ra)

Huấn luyện Hiệu quả Phần cứng. Do cấu trúc tính toán tương tự với GLA, chúng tôi có thể trực tiếp tận dụng thuật toán chunkwise và kernel được tối ưu hóa cao của họ để huấn luyện quy mô lớn hiệu quả phần cứng. Để biết thêm chi tiết, chúng tôi giới thiệu độc giả đến bài báo của họ.

Nhận xét Kết luận. Mặc dù HGRN2 có nhiều điểm tương đồng với GLA, chúng tôi tin rằng HGRN2 cung cấp một góc nhìn độc đáo khác biệt với attention tuyến tính, xuất phát từ cách tiếp cận của RNN tuyến tính có cổng. Ví dụ, có thể không rõ ràng ngay lập tức từ góc nhìn attention tuyến tính tại sao vector key nên được ràng buộc trong khoảng (0, 1) hoặc tại sao vector key và giá trị cổng quên nên có tổng bằng một. Tuy nhiên, những khái niệm này trở nên khá trực quan khi bắt đầu từ khung RNN tuyến tính có cổng và khám phá mở rộng trạng thái.

2Xem Bolya et al. (2022) để phân tích độ phức tạp chi tiết hơn.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

4 Thí nghiệm

4.1 MQAR

Thiết lập. Multi-Query Associative Recall (MQAR) (Arora et al., 2023) là phiên bản nâng cao của tập dữ liệu induction head tổng hợp (Fu et al., 2023), được thiết kế để kiểm tra khả năng gợi nhớ liên kết trong ngữ cảnh của các mô hình dưới bậc hai. Arora et al. (2023) phát hiện mối tương quan mạnh giữa độ chính xác MQAR và hiệu suất mô hình hóa ngôn ngữ. Thiết lập thí nghiệm của chúng tôi tuân thủ nghiêm ngặt bài báo gốc3. Quét siêu tham số của chúng tôi bao gồm các khoảng sau: tỷ lệ mở rộng ∈ {64, 128} và tốc độ học ∈ {1e−5, 5e−5, 1e−4, 5e−4, 1e−3, 5e−3, 1e−2}.

Kết quả. Như được hiển thị trong Hình 3, HGRN2 vượt trội đáng kể so với HGRN1 qua các chiều mô hình khác nhau, chứng minh lợi ích của mở rộng trạng thái trong việc cải thiện khả năng bộ nhớ và do đó khả năng gợi nhớ trong ngữ cảnh.

[Biểu đồ hiển thị kết quả MQAR với độ chính xác theo chiều mô hình cho các độ dài chuỗi khác nhau]

Hình 3: Kết quả trên MQAR, trong đó trục x biểu thị chiều mô hình và trục y biểu thị độ chính xác. Nhiệm vụ trở nên khó khăn hơn khi độ dài chuỗi tăng. HGRN2 vượt trội so với HGRN1 trong tất cả các tình huống.

4.2 Mô hình hóa ngôn ngữ

4.2.1 Wikitext-103

Bảng 5: Kết quả trên Wikitext-103.

Mô hình PPL (val) PPL (test) Tham số (M)
Transformer 24.40 24.78 44.65
FLASH 25.92 26.70 42.17
1+elu 27.44 28.05 44.65
Performer 62.50 63.16 44.65
cosFormer 26.53 27.06 44.65
Syn(D) 31.31 32.43 46.75
Syn(R) 33.68 34.78 44.65
gMLP 28.08 29.13 47.83
S4 38.34 39.66 45.69
DSS 39.39 41.07 45.73
GSS 29.61 30.74 43.84
RWKV-4 24.31 25.07 46.23
LRU 29.86 31.12 46.24
TNN 23.98 24.67 48.68
Mamba 22.58 23.19 44.39
HGRN1 24.14 24.82 46.25
HGRN2 23.10 23.73 44.66

Thiết lập. Cho thí nghiệm Wikitext-103, chúng tôi tuân theo cấu hình của HGRN1 để xác thực hiệu suất của các mô hình 44M so với một loạt các mô hình dưới bậc hai: FLASH (Hua et al., 2022), 1+elu (Katharopoulos et al., 2020), Performer (Choromanski et al., 2021), cosFormer (Qin et al., 2022b), Syn(D), Syn(R) (Tay et al., 2021a), gMLP (Liu et al., 2021), S4 (Gu et al., 2022a), DSS (Gupta & Berant, 2022), RWKV-v4 (Peng et al., 2023), LRU (Orvieto et al., 2023), HGRN1 (Qin et al., 2023c), TNN (Qin et al., 2023a), và Mamba (Gu & Dao, 2023). Tất cả kết quả được báo cáo đều từ các lần chạy của chúng tôi dưới cùng thiết lập.

Kết quả. Bảng 5 hiển thị kết quả. HGRN2 rõ ràng vượt trội so với HGRN1 nhưng hơi kém hiệu suất so với Mamba.

4.2.2 Slimpajama

Chúng tôi thực hiện thí nghiệm mô hình hóa ngôn ngữ với 1.3B và 2.7B tham số trên tập dữ liệu Slimpajama (Soboleva et al., 2023), sử dụng codebase FLASH LINEAR ATTENTION (Yang & Zhang, 2024) để huấn luyện.4 Kết quả, được hiển thị trong Bảng 6, chứng minh rằng

3https://github.com/HazyResearch/zoology
4Các checkpoint mô hình có sẵn tại https://huggingface.co/fla-hub.

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

HGRN2 luôn luôn vượt trội so với các mô hình hồi quy tuyến tính cạnh tranh khác qua ba quy mô mô hình. Điều này cho thấy HGRN2 cung cấp tham số hóa vượt trội so với GLA, vì cả hai mô hình đều chia sẻ cấu trúc hồi quy giống hệt nhau.

Bảng 6: Kết quả mô hình hóa ngôn ngữ Slimpajama.

Lamb. Wiki. ARC eARC cHella. Lamb. PIQA Wino. Avg
ppl↓ ppl↓ acc acc n accn acc acc acc

1.3B tham số với 100B token huấn luyện
Transformer++ 15.3 17.1 54.1 27.1 49.3 47.0 70.3 54.9 50.5
Mamba 16.5 18.2 57.3 26.6 48.1 43.4 69.5 53.7 49.8
RetNet 15.4 17.3 57.4 27.9 50.3 44.6 71.7 51.8 50.6
GLA 15.4 17.6 55.4 27.7 49.0 46.4 69.9 54.0 50.4
HGRN2 11.8 16.9 58.1 28.1 51.8 49.4 71.4 52.3 51.9

2.7B tham số với 100B token huấn luyện
Transformer++ 10.7 15.2 59.8 27.5 54.2 52.3 72.7 56.2 53.8
Mamba 13.6 15.9 60.7 29.8 53.9 46.4 72.8 53.9 52.9
RetNet 11.9 15.8 59.6 28.1 54.0 49.6 72.3 53.8 52.9
GLA 12.4 15.5 59.2 29.9 54.0 50.4 71.7 55.7 53.5
HGRN2 8.8 14.6 60.8 30.3 58.7 55.4 73.0 54.2 55.4

4.2.3 The Pile

Chúng tôi cũng thực hiện thí nghiệm trên tập dữ liệu Pile. Đầu tiên, chúng tôi huấn luyện các mô hình HGRN1 và HGRN2 150M, 350M, và 1B cho 100B token, và kết quả được hiển thị trong Bảng 7. Chúng tôi quan sát thấy HGRN2 luôn luôn vượt trội so với HGRN1.

Bảng 7: So sánh giữa HGRN1 và HGRN2 trên Các Nhiệm vụ Lý luận Thông thường.

Mô hình Bn Tham số Bn Token PIQA Hella. Wino. ARC-e ARC-c OBQA A VG
HGRN1 0.15 100 65.02 33.33 50.20 46.68 23.81 28.60 41.27
HGRN2 0.15 100 66.43 35.44 51.70 46.63 24.32 28.40 42.15
HGRN1 0.35 100 66.70 38.12 51.70 49.20 25.26 30.60 43.60
HGRN2 0.39 100 69.97 46.16 52.72 53.58 23.98 32.40 46.47
HGRN1 1 100 70.89 48.02 51.62 55.64 27.90 31.60 47.61
HGRN2 1 100 74.16 54.85 56.12 58.71 27.22 34.00 50.84

Tiếp theo, chúng tôi mở rộng phạm vi token lên 300B và huấn luyện các mô hình baseline mạnh, Mamba và LLaMA, dưới cùng thiết lập để so sánh. Chúng tôi cũng so sánh chúng với một số mô hình ngôn ngữ mã nguồn mở, như OPT (Zhang et al., 2022), Pythia (Biderman et al., 2023), BLOOM (Scao et al., 2022), và RWKV-4 (Peng et al., 2023). Chúng tôi phát hiện rằng HGRN2 hoạt động cạnh tranh với Mamba, LLaMA, và các LLM mã nguồn mở khác.

Để đánh giá khả năng ngữ cảnh dài, chúng tôi thực hiện kiểm tra trên SCROLLs (Shaham et al., 2022) và phát hiện rằng HGRN2 thể hiện hành vi mở rộng tốt hơn so với Mamba, cho thấy khả năng ngữ cảnh dài mạnh hơn, có thể do kích thước trạng thái hồi quy lớn hơn. Tuy nhiên, chúng tôi cũng quan sát thấy rằng mô hình HGRN2 7B vẫn không mạnh bằng mô hình LLaMA, cho thấy hành vi mở rộng của các mô hình tuyến tính cho mô hình hóa ngữ cảnh dài vẫn là một lĩnh vực cần nghiên cứu thêm.

Để kiểm tra khả năng truy xuất của các mô hình 3B được huấn luyện, chúng tôi chạy chế độ dễ của Bài kiểm tra Needle in a Haystack.5 LLaMA gần như đạt được hiệu suất truy xuất hoàn hảo cho đánh giá

5Trong chế độ này (Shen, 2024; Shen et al., 2024), cả câu hỏi và câu trả lời (cặp QA) đều được nhúng trong một văn bản dài, thách thức mô hình định vị và phản hồi câu truy vấn. Chế độ này đặc biệt phù hợp cho các mô hình cơ sở không có instruction tuning. Ngược lại, chế độ tiêu chuẩn chỉ đặt câu trả lời trong ngữ cảnh dài, yêu cầu mô hình hiểu câu hỏi và tìm câu trả lời liên quan.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Bảng 8: So sánh giữa HGRN2 và các mô hình ngôn ngữ mã nguồn mở khác, cùng với các mô hình baseline mạnh (LLaMA và Mamba được huấn luyện lại dưới cùng thiết lập), trên Các Nhiệm vụ Lý luận Thông thường. †biểu thị mô hình được huấn luyện bởi chúng tôi.

Mô hình Bn Tham số Bn Token PIQA Hella. Wino. ARC-e ARC-c OBQA A VG
OPT 0.35 300 64.58 36.69 52.49 44.02 23.89 28.20 41.65
Pythia 0.40 300 67.08 40.52 53.59 51.81 24.15 29.40 44.43
BLOOM 0.56 350 64.09 46.97 52.80 47.35 29.38 28.20 42.23
RWKV-4 0.43 - 67.52 39.00 51.14 52.86 25.17 32.40 45.00
Llama†0.4 350 67.19 38.75 52.19 49.24 23.72 30.00 43.51
Mamba†0.4 300 67.90 40.74 52.72 53.07 24.74 31.20 45.06
HGRN2†0.4 300 67.74 40.32 51.78 54.21 24.83 31.20 45.01

GPT-Neo 1.3 300 71.11 48.93 54.93 56.19 25.85 33.60 48.44
OPT 1.3 300 71.71 53.70 59.35 57.24 29.69 33.20 50.82
Pythia 1.4 300 70.67 47.18 53.51 56.99 26.88 31.40 47.77
BLOOM 1.3 350 71.42 49.83 51.47 55.63 29.40 44.50 47.27
RWKV-4 1.5 - 72.36 52.48 54.62 60.48 29.44 34.00 50.56
Llama†1.0 300 69.97 47.04 52.72 57.07 26.18 32.60 47.93
Mamba†1.0 300 71.27 50.15 56.35 58.71 29.27 31.20 49.45
HGRN2†1.0 300 71.65 49.52 54.38 60.27 28.07 33.40 49.55

OPT 2.7 300 73.83 60.60 61.01 60.77 31.31 35.20 53.79
Pythia 2.8 300 74.10 59.31 59.91 64.14 33.02 35.60 54.35
BLOOM 3.0 350 70.57 54.53 58.49 59.43 30.38 32.20 50.77
RWKV-4 3.0 - 72.42 58.75 57.30 62.92 35.15 36.20 53.79
Llama†3.0 350 73.18 57.88 59.59 63.93 33.51 35.40 53.93
Mamba†3.0 300 74.92 61.68 59.19 65.33 31.45 35.60 55.31
HGRN2†3.0 300 74.10 61.48 58.64 65.61 34.47 35.60 54.98

Llama†7.0 300 75.19 64.39 61.88 67.55 35.41 35.00 56.57
HGRN2†7.0 300 76.50 66.96 61.40 69.02 36.86 38.00 58.12

Bảng 9: So sánh Hiệu suất trên SCROLLS. R-1/2/L đại diện cho kích thước tham số, token, và rouge-1/rouge-2/rouge-l, tương ứng.

Mô hình Tham số Token GovRep SumScr QMSum Qspr Nrtv QALT CNLI Avg ↑
Bn Bn R-1/2/L R-1/2/L R-1/2/L F1 F1 EM EM

Llama 0.4 300 8.2/3.5/6.2 11.3/1.6/8.7 10.7/2.1/9.4 17.8 15.4 28.0 13.9 10.5
Mamba 0.4 300 8.2/2.4/6.2 11.2/1.8/8.9 9.3/1.6/8.4 14.9 11.6 25.8 19.4 10.0
HGRN2 0.4 300 15.3/3.5/10.9 7.4/0.8/6.2 8.3/1.2/7.4 12.4 10.9 26.4 31.5 10.9

Llama 1.0 300 12.9/3.1/9.4 9.5/0.8/7.7 10.9/2.2/9.4 22.8 16.0 28.4 9.9 11.0
Mamba 1.0 300 15.2/4.2/10.6 12.3/1.6/9.4 13.9/3.1/11.7 18.3 14.7 26.7 9.1 11.6
HGRN2 1.0 300 14.9/4.2/10.5 11.4/1.4/9.2 10.9/2.3/9.7 16.2 15.1 27.8 10.6 11.1

Llama 3.0 300 11.2/4.9/8.1 11.9/1.9/9.3 16.1/4.3/12.9 28.6 20.8 30.4 20.2 13.9
Mamba 3.0 300 21.5/6.6/13.9 13.2/2.0/10.1 15.0/3.2/12.3 22.1 17.9 28.8 24.0 14.7
HGRN2 3.0 300 21.7/6.6/14.1 14.6/2.1/10.8 12.5/2.7/10.6 25.4 18.8 28.9 31.9 15.4

Llama 7.0 300 17.4/7.3/11.4 12.9/1.8/10.0 14.6/3.7/11.8 32.4 22.3 33.8 10.0 14.6
HGRN2 7.0 300 14.9/5.2/10.2 15.4/2.4/11.1 14.3/3.0/11.8 27.1 19.6 30.1 10.0 13.5

độ dài không lớn hơn độ dài huấn luyện. Như được hiển thị trong Hình 4, HGRN2 và Mamba vẫn gặp khó khăn trong các nhiệm vụ truy xuất; tuy nhiên, HGRN2 vượt trội so với Mamba do kích thước trạng thái lớn hơn, được hỗ trợ bởi mở rộng trạng thái kiểu attention tuyến tính.

[Hình ảnh hiển thị kết quả Needle in a Haystack Test cho Mamba và HGRN2]

Hình 4: Bài kiểm tra Needle in a Haystack chế độ dễ trên các mô hình 3B: Mamba (trái) và HGRN2 (phải). Độ dài ngữ cảnh đánh giá là 16K, và các mô hình được huấn luyện trên độ dài chuỗi 8K.

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

4.3 Long Range Arena

Bảng 10: Kết quả trên LRA. †biểu thị kết quả được báo cáo bởi Alonso et al. (2024).

Mô hình ListOps Text Retrieval Image Pathfinder Path-X A VG
Transformer 38.37 61.95 80.69 40.57 65.26 - 47.81
cosFormer 36.50 67.70 83.15 51.23 71.96 - 51.76
FLASH 38.70 64.10 86.10 47.40 70.25 - 51.09
S4 59.60 86.82 90.90 88.65 94.20 96.35 86.09
TNN 61.04 87.90 90.97 88.24 93.00 96.10 86.21
S5 62.15 89.31 91.40 88.00 95.33 98.56 87.46
Mega 63.14 90.43 91.25 90.44 96.01 97.98 88.21
SGConv 61.45 89.20 91.11 87.97 95.46 97.83 87.17
LRU 60.20 89.40 89.90 89.00 95.10 94.20 86.30
Mamba†38.02 82.98 72.14 69.82 69.26 67.32 66.59
Griffin†32.34 71.75 66.58 61.15 73.38 69.53 62.45
HGRN1 59.95 88.14 94.23 88.69 92.92 97.50 86.91
HGRN2 60.52 88.97 95.07 89.33 93.95 98.12 87.66

Thiết lập. Long Range Arena (Tay et al., 2021b) là một benchmark được thiết kế để đánh giá khả năng của mô hình trong việc xử lý các phụ thuộc tầm xa. Chúng tôi sử dụng cấu hình của HGRN1 và so sánh với các phương pháp hiện có, như được hiển thị dưới đây.

Kết quả. Bảng 10 hiển thị kết quả. HGRN2 vượt trội so với HGRN1, trong khi Mamba và Griffin thất bại trong việc đạt độ chính xác cao trên benchmark này.

4.4 Mô hình hóa Hình ảnh

Thiết lập. Cho nhiệm vụ phân loại hình ảnh, chúng tôi tuân theo cấu hình của HGRN1 và huấn luyện nó trên ImageNet-1k, so sánh với TNN và transformer vani.

Kết quả. Bảng 11 hiển thị kết quả. HGRN2 vượt trội so với HGRN1 với kích thước tham số tương tự, đồng thời cũng chứng minh lợi thế so với các mô hình TNN (Qin et al., 2023a) và DeiT trước đó (Touvron et al., 2021).

Bảng 11: So sánh hiệu suất phân loại hình ảnh trên ImageNet-1k. HGRN2 hoạt động thuận lợi so với các phương pháp cạnh tranh với kích thước tham số tương tự.

DeiT-Tiny DeiT-Small
Mô hình Top-1 Acc Tham số (M) Top-1 Acc Tham số (M)
DeiT 72.20 5.7 79.90 22.0
TNN 72.29 6.4 79.20 23.4
HGRN1 74.40 6.1 80.09 23.7
HGRN2 75.39 6.1 80.12 23.8

5 Công trình liên quan

Các mô hình hồi quy tuyến tính. Các mô hình hồi quy tuyến tính chủ yếu bao gồm RNN tuyến tính, mô hình không gian trạng thái, và attention tuyến tính. Các mô hình không gian trạng thái (SSM) đang nhận được sự chú ý lớn kể từ công trình tiên phong S4 (Gu et al., 2022a) và phiên bản chéo hóa hiệu quả hơn (Gu et al., 2022b). Mặc dù có hiệu suất xuất sắc trong benchmark LRA, nó đã được chứng minh có hiệu suất kém hơn trong mô hình hóa ngôn ngữ. Cơ chế cổng đã được chứng minh là quan trọng trong việc cải thiện hiệu suất mô hình hóa ngôn ngữ của SSM (Mehta et al., 2023; Wang et al.,

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

2022; Gu & Dao, 2023). Gupta et al. (2022) xây dựng mối liên hệ giữa SSM và RNN tuyến tính. Orvieto et al. (2023) đề xuất một lớp RNN tuyến tính (tức là LRU) lấy cảm hứng từ SSM. Peng et al. (2023) thành công trong việc mở rộng các mô hình RNN tuyến tính lên hàng tỷ tham số lần đầu tiên.

Đối với các mô hình attention tuyến tính, hiệu suất mô hình hóa ngôn ngữ của chúng đã kém hiệu suất softmax attention trong thời gian dài. Một số cải tiến đã được đề xuất để thu hẹp khoảng cách hiệu suất: (i) kết hợp cơ chế quên (Peng et al., 2021; Schlag et al., 2021; Sun et al., 2023; Qin et al., 2023b; Yang et al., 2023; Peng et al., 2024), (ii) sử dụng attention cục bộ (Qin et al., 2022a; Zhang et al., 2023; Arora et al., 2024; Ren et al., 2024), (iii) sử dụng feature map đa thức bậc cao hơn (Arora et al., 2024; Kacham et al., 2023) để làm cho phân phối attention kết quả sắc nét hơn (Zhang et al., 2024), (iv) sử dụng quy tắc cập nhật hồi quy biểu đạt hơn nhưng hiệu quả (Schlag et al., 2021; Yang et al., 2024; Liu et al., 2024; Sun et al., 2024a).

Hồi quy tuyến tính có cổng. Martin & Cundy (2018) đầu tiên đề xuất một lớp hồi quy tuyến tính có cổng tối giản và cho thấy cách sử dụng thuật toán parallel scan để huấn luyện RNN tuyến tính song song theo độ dài chuỗi. Qin et al. (2023c) phần lớn dựa trên công trình này với một số điều chỉnh và nhấn mạnh tầm quan trọng của độ suy giảm phụ thuộc dữ liệu. De et al. (2024) xây dựng mô hình của họ trên LRU (Orvieto et al., 2023) và thay thế độ suy giảm độc lập với dữ liệu bằng độ suy giảm phụ thuộc dữ liệu. Họ tiếp tục sử dụng sliding-window attention để tăng cường hiệu suất. Những mô hình này bị hạn chế về kích thước trạng thái hồi quy.

Các mô hình hồi quy có cổng với trạng thái hồi quy có giá trị ma trận đã được nghiên cứu trong tài liệu về Neural Turing Machine (NTM Graves et al. 2014) và linear Transformer (Katharopoulos et al., 2020). Trong NTM, số lượng memory slot có thể được coi là tỷ lệ mở rộng trạng thái được thảo luận trong công trình này. NTM cũng bao gồm độ suy giảm phụ thuộc dữ liệu dưới dạng erase vector. Tuy nhiên, NTM khó song song hóa và do đó chậm để huấn luyện trong thực tế. Linear transformer được biết đến với dạng hồi quy (Katharopoulos et al., 2020) và được biết đến có liên quan chặt chẽ với fast weight programming (FWP Schlag et al. 2021). Gated FWP đã được nghiên cứu kể từ Schlag & Schmidhuber (2017); Zhang & Zhou (2017), và gần đây đã được xem xét lại trong Peng et al. (2021); Mao (2022); Yang et al. (2023); Katsch (2023); Pramanik et al. (2023). Đặc biệt, Yang et al. (2023) đã đề xuất một thuật toán huấn luyện hiệu quả phần cứng cho những loại mô hình này.

Gần đây hơn, Mamba2 (Dao & Gu, 2024), xLSTM (Beck et al., 2024), và Gated Retention (Sun et al., 2024b) đã cho thấy rằng việc chia sẻ độ suy giảm phụ thuộc dữ liệu qua các chiều khác nhau trong cùng một head là hiệu quả. Cách tiếp cận này cải thiện hiệu quả so với GLA vì các tính toán intra-chunk phù hợp hơn với gia tốc nhân ma trận dựa trên tensor core, với chi phí hy sinh tính chi tiết của độ suy giảm. Trong GLA/HGRN2, mỗi chiều head có tỷ lệ suy giảm riêng, trong khi ở Mamba2/xLSTM/Gated Retention, tất cả các chiều chia sẻ sự suy giảm dưới một head duy nhất. Đây là một câu hỏi thú vị để nghiên cứu độ suy giảm chi tiết sẽ mang lại bao nhiều cải thiện.

6 Kết luận

Trong công trình này, chúng tôi đề xuất HGRN2, một cải tiến của HGRN (Qin et al., 2023c) sử dụng cơ chế mở rộng trạng thái dựa trên tích ngoài lấy cảm hứng từ attention tuyến tính, cho phép huấn luyện hiệu quả. Các thí nghiệm qua nhiều nhiệm vụ xác thực lợi thế của HGRN2 so với HGRN1.

Lời cảm ơn

Chúng tôi cảm ơn Yu Zhang đã thực hiện một số thí nghiệm mô hình hóa ngôn ngữ và những thảo luận có giá trị.

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Tài liệu tham khảo

Ameen Ali, Itamar Zimerman, và Lior Wolf. The hidden attention of mamba models. 2024.
URL https://api.semanticscholar.org/CorpusID:268248520.

Carmen Amo Alonso, Jerome Sieber, và Melanie Nicole Zeilinger. State space models as foundation models: A control theoretic overview. 2024. URL https://api.
semanticscholar.org/CorpusID:268681121.

Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou,
Atri Rudra, và Christopher Ré. Zoology: Measuring and improving recall in efficient
language models. arXiv:2312.04927, 2023.

Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley,
James Zou, Atri Rudra, và Christopher Ré. Simple linear attention language models
balance the recall-throughput tradeoff. CoRR, abs/2402.18668, 2024. doi: 10.48550/ARXIV.
2402.18668. URL https://doi.org/10.48550/arXiv.2402.18668.

Maximilian Beck, Korbinian Poppel, Markus Spanring, Andreas Auer, Oleksandra Prud-
nikova, Michael K Kopp, Günter Klambauer, Johannes Brandstetter, và Sepp Hochre-
iter. xlstm: Extended long short-term memory. ArXiv, abs/2405.04517, 2024. URL
https://api.semanticscholar.org/CorpusID:269614336.

Stella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O'Brien, Eric
Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward
Raff, Aviya Skowron, Lintang Sutawika, và Oskar van der Wal. Pythia: A suite for
analyzing large language models across training and scaling. ArXiv, abs/2304.01373, 2023.
URL https://api.semanticscholar.org/CorpusID:257921893.

Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, và Judy Hoffman. Hydra
attention: Efficient attention with many heads. Trong ECCV Workshops, 2022. URL https:
//api.semanticscholar.org/CorpusID:252284084.

Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, An-
dreea Gane, Tamás Sarlós, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz
Kaiser, David Benjamin Belanger, Lucy J. Colwell, và Adrian Weller. Rethinking attention with performers. Trong 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.

Junyoung Chung, Çağlar Gülçehre, KyungHyun Cho, và Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555,
2014. URL http://arxiv.org/abs/1412.3555.

Tri Dao và Albert Gu. Transformers are ssms: Generalized models and efficient algorithms
through structured state space duality. ArXiv, abs/2405.21060, 2024. URL https://api.
semanticscholar.org/CorpusID:270199762.

Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru,
Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume
Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando
de Freitas, và Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local
attention for efficient language models. ArXiv, abs/2402.19427, 2024. URL https://api.
semanticscholar.org/CorpusID:268091246.

Daniel Y. Fu, Tri Dao, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, và Christopher
Ré. Hungry hungry hippos: Towards language modeling with state space models. Trong
The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
May 1-5, 2023. OpenReview.net, 2023.

Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,
Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, và Connor
Leahy. The Pile: An 800gb dataset of diverse text for language modeling. arXiv preprint
arXiv:2101.00027, 2020.

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Alex Graves, Greg Wayne, và Ivo Danihelka. Neural turing machines. ArXiv,
abs/1410.5401, 2014. URL https://api.semanticscholar.org/CorpusID:15299054.

Albert Gu và Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.
2023.

Albert Gu, Çağlar Gülçehre, Thomas Paine, Matt Hoffman, và Razvan Pascanu. Improving
the gating mechanism of recurrent neural networks. Trong Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119
của Proceedings of Machine Learning Research, pp. 3800–3809. PMLR, 2020. URL http:
//proceedings.mlr.press/v119/gu20a.html.

Albert Gu, Karan Goel, và Christopher Ré. Efficiently modeling long sequences with
structured state spaces. Trong The Tenth International Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a.

Albert Gu, Ankit Gupta, Karan Goel, và Christopher Ré. On the parameterization and
initialization of diagonal state space models. ArXiv, abs/2206.11893, 2022b. URL https:
//api.semanticscholar.org/CorpusID:249953875.

Ankit Gupta và Jonathan Berant. Diagonal state spaces are as effective as structured
state spaces. ArXiv, abs/2203.14343, 2022. URL https://api.semanticscholar.org/
CorpusID:247762199.

Ankit Gupta, Harsh Mehta, và Jonathan Berant. Simplifying and understanding state
space models with diagonal linear rnns. ArXiv, abs/2212.00768, 2022. URL https:
//api.semanticscholar.org/CorpusID:254125297.

Weizhe Hua, Zihang Dai, Hanxiao Liu, và Quoc V. Le. Transformer quality in linear time.
Trong Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, và
Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July
2022, Baltimore, Maryland, USA, volume 162 của Proceedings of Machine Learning Research, pp.
9099–9117. PMLR, 2022.

Praneeth Kacham, Vahab Mirrokni, và Peilin Zhong. Polysketchformer: Fast transformers
via sketching polynomial kernels, 2023.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, và François Fleuret. Transformers
are rnns: Fast autoregressive transformers with linear attention. Trong International conference
on machine learning, pp. 5156–5165. PMLR, 2020.

Tobias Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling.
ArXiv, abs/2311.01927, 2023.

Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, và Qian Liu. Longhorn: State
space models are amortized online learners. 2024. URL https://api.semanticscholar.
org/CorpusID:271310065.

Hanxiao Liu, Zihang Dai, David R. So, và Quoc V. Le. Pay attention to mlps, 2021.

Huanru Henry Mao. Fine-tuning pre-trained transformers into decaying fast weights. Trong
Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,
pp. 10236–10242, Abu Dhabi, United Arab Emirates, December 2022. Association for
Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.697.

Eric Martin và Chris Cundy. Parallelizing linear recurrent neural nets over sequence length.
Trong 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada,
April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.

Harsh Mehta, Ankit Gupta, Ashok Cutkosky, và Behnam Neyshabur. Long range language
modeling via gated state spaces. Trong The Eleventh International Conference on Learning
Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Stephen Merity, Caiming Xiong, James Bradbury, và Richard Socher. Pointer sentinel
mixture models. 5th International Conference on Learning Representations, ICLR, Toulon,
France, 2017.

Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Çağlar Gülçehre, Razvan
Pascanu, và Soham De. Resurrecting recurrent neural networks for long sequences. Trong
Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
và Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023,
23-29 July 2023, Honolulu, Hawaii, USA, volume 202 của Proceedings of Machine Learning
Research, pp. 26670–26698. PMLR, 2023. URL https://proceedings.mlr.press/v202/
orvieto23a.html.

Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao,
Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran G. V., Xuzheng He, Haowen
Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau,
Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang,
Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao,
Peng Zhou, Jian Zhu, và Rui-Jie Zhu. RWKV: reinventing rnns for the transformer era.
CoRR, abs/2305.13048, 2023. doi: 10.48550/ARXIV.2305.13048.

Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman,
Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemysław Kazienko, G Kranthiki-
ran, Jan Kocoń, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas
Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Woźniak,
Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, và Ruijie Zhu.
Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. ArXiv,
abs/2404.05892, 2024. URL https://api.semanticscholar.org/CorpusID:269010053.

Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, và Lingpeng
Kong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021.

Subhojeet Pramanik, Esraa Elelimy, Marlos C. Machado, và Adam White. Recurrent linear
transformers. CoRR, abs/2310.15719, 2023.

Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, và
Yiran Zhong. The devil in linear transformer. arXiv preprint arXiv:2210.10340, 2022a.

Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan,
Lingpeng Kong, và Yiran Zhong. cosformer: Rethinking softmax in attention. Trong The
Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April
25-29, 2022. OpenReview.net, 2022b.

Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai,
Lingpeng Kong, và Yiran Zhong. Toeplitz neural network for sequence modeling.
Trong The Eleventh International Conference on Learning Representations (ICLR), 2023a. URL
https://openreview.net/forum?id=IxmWsm4xrua.

Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei,
Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.
arXiv preprint arXiv:2307.14995, 2023b.

Zhen Qin, Songlin Yang, và Yiran Zhong. Hierarchically gated recurrent neural network for
sequence modeling. Trong Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz
Hardt, và Sergey Levine (eds.), Advances in Neural Information Processing Systems 36:
Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans,
LA, USA, December 10 - 16, 2023, 2023c. URL http://papers.nips.cc/paper_files/
paper/2023/hash/694be3548697e9cc8999d45e8d16fe1e-Abstract-Conference.html.

Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, và Yiran Zhong. Lightning
attention-2: A free lunch for handling unlimited sequence lengths in large language
models. 2024.

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, và Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. ArXiv,
abs/2406.07522, 2024. URL https://api.semanticscholar.org/CorpusID:270380294.

Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow,
Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan
Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammana-
manchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral,
Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy,
Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lau-
rençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan,
Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreis-
berg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin
Leong, Daniel Alexander van Strien, David Ifeoluwa Adelani, Dragomir R. Radev, Ed-
uardo González Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Natan, Francesco De
Toni, Gérard Dupont, Germán Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyam-
ina, Hieu Trung Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios,
Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg,
Josephine Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Lean-
dro von Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey,
Manuel Romero Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang,
Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A.
Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier
Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo,
Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto López, Rui
Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Has-
san Muhammad, Shanya Sharma, S. Longpre, Somaieh Nikpoor, S. Silberberg, Suhas
Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai,
Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J.
Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud
Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey,
Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari,
Maged S. Al-Shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie,
Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Févry,
Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng-Xin Yong, Zhiqing
Sun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung
Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared
Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi,
Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von
Platen, Pierre Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari,
Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers, Ahmed
Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramo-
nian, Aurélie Névéol, Charles Lovering, Daniel H Garrette, Deepak R. Tunuguntla,
Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra
Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde,
Xiangru Tang, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clin-
ciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal,
Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, S. Osher Pais, Ta-
tiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly
Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bam-
berger, Zdeněk Kasner, Zdeněk Kasner, Amanda Pestana, Amir Feizpour, Ammar Khan,
Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona Unldreaj, Arash
Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi,
Benjamin Ayoade Ajibade, Bharat Kumar Saxena, Carlos Muñoz Ferrandis, Danish Con-
tractor, David M. Lansky, Davis David, Douwe Kiela, Duong Anh Nguyen, Edward Tan,
Emi Baylor, Ezinwanne Ozoani, Fatim Tahirah Mirza, Frankline Ononiwu, Habib Rezane-
jad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jan
Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, Lívia Dutra, Mairon Samagaio,

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna,
Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour
Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran An, R. P. Kromann, Ryan Hao, Samira
Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-Cong
Le, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh
Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada,
Ayush Kumar Singh, Benjamin Beilharz, Bo Wang, Caio Matheus Fonseca de Brito, Chenxi
Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier, Daniel León Periñán, Daniel Molano,
Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin
Bayrak, Gully Burns, Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang, John
Giorgi, Jonas Golde, Jose David Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu,
Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc Pàmies, María An-
drea Castillo, Marianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan,
Michael Weinberg, M Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun
Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner,
Pascale Fung, Patricia Haller, Patrick Haller, Renata Eisenberg, Robert Martin, Rodrigo
Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh,
Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan
Schweter, Sushil Pratap Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech
Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yu Xu, Zhee Xao
Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, và Thomas Wolf. Bloom: A
176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100, 2022.
URL https://api.semanticscholar.org/CorpusID:253420279.

Imanol Schlag và Jürgen Schmidhuber. Gated fast weights for on-the-fly neural program
generation. 2017. URL https://api.semanticscholar.org/CorpusID:216094255.

Imanol Schlag, Kazuki Irie, và Jürgen Schmidhuber. Linear transformers are secretly fast
weight programmers. Trong Marina Meila và Tong Zhang (eds.), Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,
volume 139 của Proceedings of Machine Learning Research, pp. 9355–9366. PMLR, 2021.

Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan
Xiong, Mor Geva, Jonathan Berant, và Omer Levy. SCROLLS: Standardized CompaRison
over long language sequences. Trong Yoav Goldberg, Zornitsa Kozareva, và Yue Zhang
(eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,
pp. 12007–12021, Abu Dhabi, United Arab Emirates, December 2022. Association for
Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.823. URL https://
aclanthology.org/2022.emnlp-main.823.

Xuyang Shen. Llmtest needleinahaystack hfmodel: Support huggingface model to do simple
retrieval from llm models at various context lengths to measure accuracy, 2024. URL
https://github.com/XuyangShen/LLMTest_NeedleInAHaystack_HFModel.

Xuyang Shen, Dong Li, Ruitao Leng, Zhen Qin, Weigao Sun, và Yiran Zhong. Scaling laws
for linear complexity language models, 2024. URL https://arxiv.org/abs/2406.16690.

Yikang Shen, Shawn Tan, Alessandro Sordoni, và Aaron C. Courville. Ordered neurons:
Integrating tree structures into recurrent neural networks. ArXiv, abs/1810.09536, 2018.
URL https://api.semanticscholar.org/CorpusID:53034786.

Jimmy T. H. Smith, Andrew Warrington, và Scott W. Linderman. Simplified state space
layers for sequence modeling. Trong The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.

Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, và Nolan
Dey. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama, 2023.

Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois,
Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, và Carlos Guestrin.
Learning to (learn at test time): Rnns with expressive hidden states. 2024a. URL https:
//api.semanticscholar.org/CorpusID:271039606.

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang,
và Furu Wei. Retentive network: A successor to transformer for large language models.
arXiv preprint arXiv:2307.08621, 2023.

Yutao Sun, Li Dong, Yi Zhu, Shaohan Huang, Wenhui Wang, Shuming Ma, Quanlu Zhang,
Jianyong Wang, và Furu Wei. You only cache once: Decoder-decoder architectures for
language models. ArXiv, abs/2405.05254, 2024b. URL https://api.semanticscholar.
org/CorpusID:269626143.

Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, và Che Zheng. Synthesizer:
Rethinking self-attention in transformer models, 2021a.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng
Rao, Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: A benchmark for
efficient transformers. Trong 9th International Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview.
net/forum?id=qVyeW-grC2k.

Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles,
và Herve Jegou. Training data-efficient image transformers &amp; distillation through
attention. Trong International Conference on Machine Learning, volume 139, pp. 10347–10357,
July 2021.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,
2023a.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning
Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,
Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,
Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, và Thomas Scialom. Llama 2: Open foundation and fine-tuned chat
models, 2023b.

Jos van der Westhuizen và Joan Lasenby. The unreasonable effectiveness of the forget gate.
CoRR, abs/1804.04849, 2018.

Junxiong Wang, Jing Nathan Yan, Albert Gu, và Alexander M. Rush. Pretraining without
attention. CoRR, abs/2212.10544, 2022.

Songlin Yang và Yu Zhang. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism, January 2024. URL https://github.com/
sustcsonglin/flash-linear-attention.

Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, và Yoon Kim. Gated linear
attention transformers with hardware-efficient training. CoRR, abs/2312.06635, 2023. doi:
10.48550/ARXIV.2312.06635. URL https://doi.org/10.48550/arXiv.2312.06635.

Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, và Yoon Kim. Parallelizing linear
transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484,
2024.

Jun Zhang, Shuyang Jiang, Jiangtao Feng, Lin Zheng, và Lingpeng Kong. Linear attention
via orthogonal memory, 2023.

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Michael Zhang, Kush Bhatia, Hermann Kumbong, và Christopher Ré. The hedgehog &
the porcupine: Expressive linear attentions with softmax mimicry, 2024.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona T. Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott,
Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang,
và Luke Zettlemoyer. Opt: Open pre-trained transformer language models. ArXiv,
abs/2205.01068, 2022. URL https://api.semanticscholar.org/CorpusID:248496292.

Wei Zhang và Bowen Zhou. Learning to update auto-associative memory in recurrent
neural networks for improving sequence memorization. ArXiv, abs/1709.06493, 2017.
URL https://api.semanticscholar.org/CorpusID:22458497.

A Phụ lục

A.1 Cấu hình Thí nghiệm

Trong Bảng 12, các cấu hình thí nghiệm cung cấp thiết lập chi tiết cho cả thí nghiệm Mô hình Ngôn ngữ Tự hồi quy (ALM) và ImageNet (IM), tập trung vào các tập dữ liệu WikiText-103 và ImageNet-1k, tương ứng. Các thí nghiệm ALM sử dụng Byte Pair Encoding (BPE) với kích thước từ vựng 50,265 và độ dài chuỗi 512, có tổng kích thước batch 128 và 50,000 cập nhật. Các thí nghiệm ImageNet phân biệt giữa các mô hình 6 triệu và 23 triệu tham số, với tổng kích thước batch 1024 và 2048, cả hai đều chạy trong 300 epoch nhưng với các giai đoạn khởi động khác nhau. Các chiến lược tối ưu hóa khác nhau giữa Adam cho ALM và AdamW cho IM, với các bộ lập lịch tốc độ học và siêu tham số cụ thể được điều chỉnh cho quy mô của từng mô hình. Các cấu hình bổ sung phác thảo các biến thể về độ phức tạp mô hình, từ 0.15 đến 2.9 triệu tham số, điều chỉnh các lớp, chiều ẩn và GPU được sử dụng, nhằm mục đích khám phá toàn diện hiệu suất mô hình qua các quy mô và thiết lập.

Bảng 12: Cấu hình Toàn diện của Mô hình và Quy trình Huấn luyện cho Các Thí nghiệm HGRN2 "Tổng kích thước batch" có nghĩa là batch pergpu×update freq×num gpus; "ALM" là viết tắt của Autoregressive Language Model; "IM" là viết tắt của Image Modeling.

ALM IM(6M) IM(23M)
Tập dữ liệu WikiText-103 ImageNet-1k ImageNet-1k
Phương pháp tokenizer BPE - -
Kích thước từ vựng nguồn 50265 - -
Độ dài chuỗi 512 - -
Tổng kích thước batch 128 1024 2048
Số cập nhật/epoch 50k cập nhật 300 epoch 300 epoch
Bước/epoch khởi động 4k bước 20 epoch 10 epoch
Tốc độ học đỉnh 5e-4 7.5e-4 7.5e-4
Bộ lập lịch tốc độ học Inverse sqrt Cosine Cosine
Trình tối ưu hóa Adam Adamw Adamw
Adam ϵ 1e-8 1e-8 1e-8
Adam (β1,β2) (0.9, 0.98) (0.9, 0.98) (0.9, 0.98)
Phân rã trọng số 0.1 0.05 0.1
Cắt gradient - 5.0 5.0

Bảng 13: Cấu hình mô hình

Tham số Lớp Chiều Ẩn Tỷ lệ Exp. T.Đ.H. Kích thước Batch ĐộDàiChuỗi GPUS
0.15 15 768 128 3.00E-04 26 2048 8
0.385 26 1024 128 3.00E-04 15 2048 8
1 18 2048 128 3.00E-04 10 2048 16
2.9 36 2560 128 3.00E-04 36 2048 64

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

A.2 Đường cong Loss của HGRN2

Các đường cong loss huấn luyện cho các mô hình HGRN2 có kích thước khác nhau—150M, 385M, và 1B, như được hiển thị trong Hình 5, cho thấy khi số lượng tham số tăng, hiệu suất của mô hình cải thiện, với mô hình 1B luôn luôn vượt trội so với các mô hình khác.

[Biểu đồ đường cong loss theo token huấn luyện cho các mô hình 150M, 385M, 1B]

Hình 5: Loss huấn luyện theo token huấn luyện cho các mô hình 150m, 385m, 1B. Tất cả các mô hình tiêu thụ khoảng 100 tỷ token, và có thể quan sát thấy rằng khi kích thước mô hình tăng, loss giảm đáng kể.
