# Mạng Nơ-ron Hồi quy Có Cổng Phân cấp cho Mô hình hóa Chuỗi

1Zhen Qin⋆, 2Songlin Yang⋆, 1Yiran Zhong
1OpenNLPLab, Phòng thí nghiệm Trí tuệ nhân tạo Thượng Hải, 2MIT CSAIL
https://github.com/OpenNLPLab/HGRN

Tóm tắt
Transformers đã vượt qua RNNs về độ phổ biến do khả năng vượt trội trong huấn luyện song song và mô hình hóa phụ thuộc dài hạn. Gần đây, đã có sự quan tâm trở lại việc sử dụng RNNs tuyến tính để mô hình hóa chuỗi hiệu quả. Những RNNs tuyến tính này thường sử dụng cơ chế cổng ở đầu ra của lớp hồi quy tuyến tính trong khi bỏ qua tầm quan trọng của việc sử dụng cổng quên trong hồi quy. Trong bài báo này, chúng tôi đề xuất mô hình RNN tuyến tính có cổng được gọi là Mạng Nơ-ron Hồi quy Có Cổng Phân cấp (HGRN), bao gồm các cổng quên được giới hạn dưới bởi một giá trị có thể học. Giới hạn dưới tăng đơn điệu khi di chuyển lên các lớp. Điều này cho phép các lớp trên mô hình hóa phụ thuộc dài hạn và các lớp dưới mô hình hóa các phụ thuộc cục bộ, ngắn hạn hơn. Các thí nghiệm về mô hình hóa ngôn ngữ, phân loại hình ảnh và benchmark long-range arena thể hiện hiệu quả và tính hiệu quả của mô hình đề xuất. Mã nguồn có sẵn tại https://github.com/OpenNLPLab/HGRN.

1 Giới thiệu
Mô hình hóa chuỗi là một vấn đề cơ bản trong nhiều lĩnh vực khác nhau như xử lý ngôn ngữ tự nhiên [12,43,44,61,64], phân tích chuỗi thời gian [84], thị giác máy tính [3,13,45,74], và xử lý âm thanh [1,18,73]. Trước khi phát minh ra Transformers [81], RNN và các biến thể của nó là lựa chọn chính của kiến trúc cho mô hình hóa chuỗi, và đã được sử dụng rộng rãi trong dịch máy [6], dự đoán giá cổ phiếu [68], dự báo thời tiết [65], nhận dạng giọng nói [51], v.v. RNNs có hai nhược điểm chính: huấn luyện tuần tự chậm và khả năng hạn chế trong mô hình hóa phụ thuộc dài hạn. Với sự phát triển nhanh chóng của deep learning và việc sử dụng phổ biến của GPUs, những nhược điểm này ngăn cản nó phát triển mạnh trong các tác vụ mô hình hóa chuỗi dài hiện đại. Trong khi đó, Transformers [81] đã nhanh chóng trở nên phổ biến và hiện thống trị nhiều lĩnh vực nghiên cứu khác nhau trong mô hình hóa chuỗi do khả năng tốt hơn trong huấn luyện song song và mô hình hóa phụ thuộc dài hạn. Tuy nhiên, độ phức tạp thời gian bậc hai của Transformer làm cho mô hình hóa chuỗi dài trở nên đắt đỏ. Mặt khác, RNN cung cấp độ phức tạp tuyến tính và phục vụ như một lựa chọn lý tưởng cho mô hình hóa chuỗi dài. Công trình này nhằm giải quyết những nhược điểm của RNN, làm sống lại khả năng ứng dụng của chúng trong các tác vụ mô hình hóa chuỗi dài.

Để giải quyết vấn đề hiệu quả huấn luyện, chúng tôi chuyển sang các biến thể RNN hiệu quả hơn sử dụng quan hệ hồi quy tuyến tính theo phần tử (ELR) [48]. ELR cung cấp hai lợi thế chính: (i) Loại bỏ tính phi tuyến trong hồi quy cho phép huấn luyện song song. (ii) Bằng cách giả định tính độc lập giữa các trạng thái ẩn riêng biệt, nó cho phép cập nhật trạng thái ẩn hiệu quả (thông qua phép nhân theo phần tử thay vì phép nhân ma trận) [20,40]. Đáng chú ý, ELR đã được sử dụng trong nhiều mô hình RNN tuyến tính hiện đại, bao gồm các phiên bản đường chéo của các mô hình không gian trạng thái có cấu trúc [21] (S4) [20,26,71] và RWKV [55]. Trong những tiến bộ gần đây, nhiều nghiên cứu đã kết hợp cơ chế cổng vào đầu ra của các lớp hồi quy tuyến tính [11,46,49,55,82], tương tự như các cổng đầu ra trong LSTMs và dẫn đến những cải thiện hiệu suất đáng kể. Tuy nhiên, hầu hết các nghiên cứu hiện tại bỏ qua tầm quan trọng của cổng quên, cái thường được coi là cổng quan trọng nhất trong LSTMs [19,80]. Trong công trình này, chúng tôi nhấn mạnh tầm quan trọng của việc sử dụng cổng quên trong RNNs tuyến tính và áp dụng RNNs tuyến tính có cổng cho cả hiệu quả và hiệu suất cao.

Để nắm bắt hiệu quả các phụ thuộc dài hạn trong RNNs có cổng, việc duy trì giá trị cổng quên cao gần một là rất quan trọng [23]. Tuy nhiên, các cổng trong vùng bão hòa (tức là gần không hoặc một) gặp phải vấn đề gradient biến mất [23]. Hơn nữa, nếu tất cả giá trị cổng quên đều gần một, RNNs sẽ không thể quên hiệu quả thông tin không liên quan, làm tổn hại khả năng mô hình hóa phụ thuộc ngắn hạn. Để giải quyết những thách thức này, chúng tôi giới thiệu Đơn vị Hồi quy Có Cổng Phân cấp (HGRU). Trong HGRU, chúng tôi thêm một giá trị có thể học cộng thêm, được gọi là giới hạn dưới, vào giá trị cổng quên ban đầu, hiệu quả giảm thiểu vấn đề cổng bão hòa [23] bằng cách đẩy kích hoạt cổng ra khỏi vùng bão hòa. Hơn nữa, chúng tôi thiết kế các giới hạn dưới tăng đơn điệu khi chúng ta di chuyển lên các lớp của RNN. Điều này đảm bảo rằng giá trị cổng quên trong các lớp dưới vẫn tương đối nhỏ, cho phép việc quên cần thiết thông tin quá khứ để mô hình hóa phụ thuộc ngắn hạn. Ngược lại, trong lớp trên cùng, giá trị cổng quên tiếp cận một, tạo điều kiện cho việc mô hình hóa hiệu quả các phụ thuộc dài hạn. Mô hình đề xuất của chúng tôi đã được chứng minh là rất hiệu quả và hiệu suất, như được thể hiện bởi hiệu suất xuất sắc trong mô hình hóa ngôn ngữ, phân loại hình ảnh và các benchmark long-range arena.

2 Công trình liên quan
Trộn token hiệu quả cho mô hình hóa chuỗi. [83] trừu tượng hóa self-attention (SA) như trộn token, do đó biến đổi kiến trúc Transformer thành MetaFormer. MetaFormer bao gồm các thành phần thiết yếu như token mixer, channel mixer, kết nối dư và LayerNorm. Sự trừu tượng này nhấn mạnh rằng thành công của Transformers không chỉ dựa vào SA mà là sự tích hợp toàn diện của những thành phần này. Đáng chú ý, token mixers có thể được thay thế bằng các lựa chọn thay thế đơn giản hơn như các lớp pooling mà không làm giảm hiệu suất của mô hình trong bối cảnh vision transformer. Đối với các tác vụ mô hình hóa chuỗi, [29] cung cấp phân tích và thảo luận toàn diện về các chiến lược trộn token khác nhau. Hai ứng cử viên nổi bật, long convolution và linear recurrence, cho thấy triển vọng như sự thay thế cho các module SA trong mô hình hóa chuỗi dài do độ phức tạp thời gian tiệm cận vượt trội và hiệu suất cạnh tranh. Trong các mô hình long convolution [14,41,57,59], kích thước kernel khớp với độ dài chuỗi đầu vào, cho phép bối cảnh rộng hơn so với convolutions truyền thống. Huấn luyện được thực hiện sử dụng thuật toán biến đổi Fourier nhanh (FFT) hiệu quả O(nlogn). Tuy nhiên, long convolutions gặp phải những thách thức như nhu cầu suy luận convolution nhân quả, cái yêu cầu cache tất cả các tính toán lịch sử tương tự như cache key-value (KV) trong SA. Điều này có thể dẫn đến giới hạn bộ nhớ khi xử lý chuỗi dài. Hơn nữa, độ phức tạp suy luận của long convolutions vẫn cao hơn so với RNNs. Những yếu tố này làm cho RNNs tuyến tính trở thành lựa chọn thay thế phù hợp hơn để thay thế SA trong mô hình hóa chuỗi dài. TransNormerLLM [61] mở rộng token mixing hiệu quả trong các mô hình ngôn ngữ lớn để đạt được hiệu suất cạnh tranh và hiệu quả huấn luyện và suy luận vượt trội so với các mô hình dựa trên transformer.

Hồi quy tuyến tính theo phần tử. Tốc độ huấn luyện chậm hơn của RNNs truyền thống có thể được quy cho hai yếu tố chính: (i) Việc cập nhật trạng thái ẩn liên quan đến phép nhân ma trận đầy đủ. (ii) Sự hiện diện của tính phi tuyến trong hồi quy ngăn cản tính toán song song. Để giải quyết vấn đề đầu tiên, [40] giới thiệu tương tác đơn giản hóa giữa các trạng thái ẩn. Điều này cho phép cập nhật trạng thái ẩn được thực hiện sử dụng phép nhân theo phần tử thay vì phép nhân ma trận đầy đủ. Họ chứng minh rằng cách tiếp cận này đặc biệt nhanh khi hồi quy (phi tuyến) cho mỗi chiều được hợp nhất trong một CUDA kernel duy nhất. Tương tự, cho trường hợp tuyến tính, các phiên bản đường chéo của S4 [20,26] cũng đã thể hiện cải thiện tốc độ so với S4 bằng cách tận dụng hồi quy theo phần tử. Liên quan đến thách thức thứ hai, khả năng nắm bắt phụ thuộc phi tuyến trên dữ liệu quá khứ có thể đạt được bằng cách xếp chồng nhiều lớp hồi quy tuyến tính xen kẽ với các khối MLP phi tuyến. Điều này cho thấy tiềm năng loại bỏ tính phi tuyến, như được đề xuất bởi [4,25,48]. Hỗ trợ thực nghiệm cho hiệu quả của chiến lược này đến sau đó, như được chứng minh bởi [11,20,24,53,55,71]. [52] tiếp tục nhấn mạnh rằng kiến trúc như vậy vẫn sở hữu các thuộc tính Universal Approximator, do đó biện minh cho việc sử dụng hồi quy tuyến tính. Bằng cách loại trừ tính phi tuyến, [48,71] đã chỉ ra rằng thuật toán parallel scan có thể được sử dụng cho huấn luyện song song.

Hồi quy tuyến tính có thể được phân loại rộng rãi thành trung bình động hàm mũ (EMA) và các lược đồ cổng, như được ghi nhận bởi [48]. Sự khác biệt chính là liệu tỷ lệ suy giảm có phụ thuộc vào dữ liệu hay không. Các mô hình như S4 [21], S4D [20], MEGA [46], RWKV [55], và LRU [53] sử dụng cách tiếp cận EMA, trong đó tỷ lệ suy giảm là tĩnh cho tất cả các bước thời gian (tức là độc lập với dữ liệu), trong khi mô hình của chúng tôi sử dụng tỷ lệ suy giảm động phụ thuộc vào dữ liệu thông qua việc sử dụng cổng quên. Chúng tôi nhận xét về tầm quan trọng của việc kết hợp tỷ lệ suy giảm phụ thuộc vào dữ liệu, cái bị bỏ qua phần lớn bởi các công trình hiện tại trong RNNs tuyến tính. Mặc dù liquid S4 [28] sử dụng ma trận chuyển đổi động (cái tương đương với tỷ lệ suy giảm phụ thuộc vào dữ liệu), nó sử dụng một dạng hạn chế cho huấn luyện dựa trên FFT. Mô hình của chúng tôi không có quan điểm convolutional và do đó không thể sử dụng FFT cho huấn luyện nhưng cho phép sử dụng parallel scans.

Lĩnh vực Transformers tuyến tính và RNNs tuyến tính thể hiện mối quan hệ gần gũi. [34] chỉ ra rằng Transformers tuyến tính có thể được tái cấu trúc thành RNNs trong quá trình giải mã tự hồi quy, tiết lộ sự tương tự với các quy tắc cập nhật được quan sát trong tích ngoài trọng số nhanh cộng [66,67]. Những cập nhật này có thể được xem như một trường hợp đặc biệt của hồi quy tuyến tính theo phần tử, trong đó giá trị cổng quên được đặt nhất quán là một qua thời gian và các trạng thái ẩn là hai chiều. Tuy nhiên, công thức này trong Transformers tuyến tính thiếu khả năng quên thông tin không liên quan, dẫn đến vấn đề pha loãng attention [60]. Để giải quyết hạn chế này, [66] giới thiệu quy tắc delta để quên các giá trị liên quan với write key hiện tại bằng cách loại bỏ giá trị tương ứng trước khi thêm giá trị mới. Thay vào đó, [47,56] đề xuất các cơ chế cổng tương tự như những cái trong RNNs có cổng để tạo điều kiện cho việc quên thông tin không liên quan.

Phụ thuộc dài hạn trong RNNs. RNNs thiếu trong mô hình hóa phụ thuộc dài hạn, cái thường được quy cho vấn đề gradient biến mất. Ba phương pháp thường được áp dụng để giảm thiểu vấn đề này. (i) Cơ chế cổng [9,17,23,30,70], cái được tin là rất quan trọng cho thành công của LSTMs, sử dụng quy tắc cập nhật trạng thái ẩn cộng (thay vì nhân) để cải thiện dòng gradient. (ii) Regularizing hoặc khởi tạo các eigenvalue của ma trận trọng số hồi quy (gần) một thông qua ma trận đơn vị [38] hoặc ma trận unitary [2]. Trong trường hợp RNN tuyến tính đường chéo, các eigenvalue trùng với tỷ lệ suy giảm theo phần tử, và LRU [53] sử dụng các kỹ thuật đại số tuyến tính ngẫu nhiên để khởi tạo eigenvalue gần một. [53] cũng nhận xét thú vị rằng nhiều mô hình không gian trạng thái hiện đại sử dụng giá trị bước thời gian rất nhỏ khi khởi tạo để discretization, dẫn đến eigenvalue hoặc tỷ lệ suy giảm gần một. (iii) Thêm kết nối skip giữa các bước thời gian xa để cho phép lối tắt cho dòng gradient [5,8,37]. Cách tiếp cận của chúng tôi kết hợp (i) và (ii), cái cải thiện cơ chế cổng với tỷ lệ suy giảm động được regularized tiếp cận một trong lớp trên.

3 Phương pháp
3.1 Tổng quan kiến trúc
Mạng Hồi quy Có Cổng Phân cấp (HGRN) đề xuất của chúng tôi được mô tả trong Hình 1. Nó có nhiều lớp xếp chồng, mỗi lớp bao gồm một module trộn token HGRU và một module trộn kênh GLU (Gated Linear Unit [69]).

3.2 Khám phá HGRU

Chúng tôi bắt đầu với một lớp hồi quy tuyến tính có cổng đơn giản, được định nghĩa như sau:
ft = Sigmoid(xtWf + bf) ∈ R¹×d,
it = Sigmoid(xtWi + bi) ∈ R¹×d,
ct = SiLU(xtWt + bz) ∈ R¹×d,
ht = ft ⊙ ht-1 + it ⊙ ct ∈ R¹×d,
h0 = 0 ∈ R¹×d,                                   (1)

trong đó ⊙ biểu thị phép nhân theo phần tử. Theo thuật ngữ được sử dụng trong tài liệu RNN, chúng tôi gọi ft và it lần lượt là cổng quên và cổng đầu vào. Đáng chú ý rằng ft và it chỉ phụ thuộc vào xt chứ không phải ht-1. Đặc tính này cho phép sử dụng thuật toán parallel scan [48,71], nếu không thì không khả thi.

Sau đó chúng tôi thực hiện những thay đổi sau đây hướng tới HGRU cuối cùng từng bước một.

Hồi quy phức. Đối với RNNs tuyến tính với tỷ lệ suy giảm tĩnh, việc thực hiện eigendecomposition trên ma trận trọng số hồi quy để đạt được hồi quy tuyến tính theo phần tử là phổ biến. Tuy nhiên, nếu chỉ cho phép eigenvalue thực, nó hạn chế phạm vi của ma trận trọng số hồi quy là đối xứng, giới hạn tính biểu đạt của mô hình. Để vượt qua hạn chế này, RNNs tuyến tính thường sử dụng eigenvalue phức để tăng cường sức mạnh biểu đạt của mô hình [20,26,27,32,53]. Được thúc đẩy bởi điều này, chúng tôi mở rộng mô hình để xem xét ht, it, ct ∈ C¹×d như các giá trị phức.

Đối với đầu vào ct, chúng tôi parameterize các phần thực và ảo riêng biệt như sau:
Re(ct) = SiLU(xtWcr + bcr) ∈ R¹×d,
Im(ct) = SiLU(xtWci + bci) ∈ R¹×d.

Liên quan đến giá trị cổng quên, chúng tôi thấy thuận tiện khi sử dụng biểu diễn hàm mũ của số phức và parameterize ft như sau: ft = λt ⊙ exp(iθt). Ở đây, i² = -1, λt, θt ∈ Rd và exp(iθt) = cos θt + sin θt i. Đối số độ lớn λt xác định cường độ nhớ thông tin lịch sử, trong khi đối số pha θt xác định tần số dao động. Chúng tôi thấy rằng parameterizing θt theo cách độc lập với dữ liệu là tốt hơn, vì nó cho phép diễn giải rõ ràng về việc mã hóa thông tin vị trí tương đối (xem phần con tiếp theo để thảo luận thêm), cái gợi nhớ đến Rotary Positional Embedding (RoPE) [72]. Chúng tôi chia sẻ θ qua các bước thời gian, tức là ft = λt ⊙ exp(iθ), khởi tạo θ như RoPE làm, nhưng làm nó có thể học như LRPE [63].

Giới hạn dưới cho giá trị cổng quên. Vì cường độ nhớ thông tin chỉ liên quan đến đối số độ lớn λt, chúng tôi tập trung vào cách thêm giới hạn dưới cho λt. Như đã đề cập trước đó, chúng tôi muốn đặt giới hạn dưới tăng đơn điệu cho giá trị cổng quên (độ lớn). Được truyền cảm hứng từ ON-LSTM [70], chúng tôi sử dụng hàm kích hoạt cummax để đạt được điều này. Cụ thể, chúng tôi phân bổ Γ ∈ RH×d để parameterize giới hạn dưới độc lập cho tất cả trạng thái ẩn, trong đó H là số lớp. Giả sử chỉ số lớp là k, chúng tôi có các tính toán sau:

P = Softmax(Γ, dim = 0) ∈ RH×d,
γk = [Cumsum(P, dim = 0)]k ∈ R¹×d.

Ở đây chúng tôi định nghĩa [Cumsum(x)]k = (Σᵢ₌₁ᵏ xi) - x₁ để ngăn giới hạn dưới của lớp cao nhất là một vì chúng tôi vẫn muốn khả năng quên thông tin không liên quan.

Chúng tôi nhận xét rằng có sự khác biệt trong việc sử dụng cummax giữa mô hình của chúng tôi và ON-LSTM. Trong ON-LSTM, cummax được áp dụng cho chiều trạng thái ẩn trong một lớp duy nhất, trong khi trong trường hợp của chúng tôi, chúng tôi áp dụng cummax trên chiều lớp qua các lớp khác nhau để cho phép các lớp trên mô hình hóa phụ thuộc tầm xa.

Cuối cùng, λt trong lớp thứ k được parameterize như sau:
μt = Sigmoid(xtWμ + bμ) ∈ R¹×d,
λt = γk + (1 - γk) ⊙ μt ∈ R¹×d.

So với trước đó (tức là không có giới hạn dưới), để đạt được cùng giá trị tỷ lệ quên γ̄ gần một, μt sẽ được đẩy ra khỏi vùng bão hòa của hàm kích hoạt Sigmoid (tức là gần một),
μt = (γ̄ - γk)/(1 - γk) < γ̄,
do đó giảm thiểu vấn đề gradient biến mất [23] và làm cho tối ưu hóa dựa trên gradient dễ dàng hơn.

Ràng buộc cổng đầu vào và cổng quên. Để giảm số lượng tham số, việc sử dụng đơn vị rò rỉ là phổ biến, tức là ràng buộc cổng đầu vào với cổng quên sử dụng it = 1 - ft, cái có mối quan hệ gần gũi với discretization của hệ thống thời gian liên tục [75] và trung bình động hàm mũ [33], và đã được chứng minh hiệu quả thực nghiệm [9,19]. Để cho phép diễn giải rõ ràng về việc mã hóa thông tin vị trí tương đối, chúng tôi chỉ áp dụng chiến lược này trên đối số độ lớn:

ht = λt ⊙ exp(iθ) ⊙ ht-1 + (1 - λt) ⊙ ct ∈ C¹×d.   (2)

Cổng đầu ra và projection. Việc thêm cổng vào đầu ra của lớp hồi quy đã được chỉ ra là hiệu quả trong các mô hình không gian trạng thái [11,46,49,82]. Được thúc đẩy bởi điều này, chúng tôi kết hợp cổng đầu ra trước khi thực hiện projection đầu ra như sau và có HGRU:

gt = Sigmoid(Wgxt + bg) ∈ R¹×2d,
o't = LayerNorm(gt ⊙ [Re(ht), Im(ht)]) ∈ R¹×2d,
ot = o'tWo + bo ∈ R¹×d.                           (3)

3.3 Quan điểm trộn token của HGRU
Chúng tôi cung cấp quan điểm trộn token của HGRU tương tự như [32]. Mở rộng Phương trình 2, chúng tôi có:

ht = Σₛ₌₁ᵗ (1 - λs)[∏ₖ₌ₛ₊₁ᵗ λk exp(iθ)]cs = Σₛ₌₁ᵗ (1 - λs)[∏ₖ₌ₛ₊₁ᵗ λk]exp(i(t-s)θ)cs   (4)

Được viết dưới dạng ma trận, chúng tôi có:

H = [h₁; ...; ...; hₙ], A = [[1-λ₁, 0, ..., 0]; [(1-λ₁)λ₂exp(iθ), 1-λ₂, ...; [..., ..., 0]; [(1-λ₁)[∏ₖ₌₂ⁿ λk]exp(i(n-1)θ), ..., ..., 1-λₙ]], C = [c₁; ...; ...; cₙ]   (5)

Vậy module trộn token có thể được hình thành như sau:
H = AC.                                           (6)

Lưu ý rằng ma trận trộn token A có thể được phân tách thành hai phần A = Λ ⊙ Θ:

Λ = [[1-λ₁, 0, ..., 0]; [(1-λ₁)λ₂, 1-λ₂, ...; [..., ..., 0]; [(1-λ₁)[∏ₖ₌₂ⁿ λk], ..., ..., 1-λₙ]], Θ = [[1, 0, ..., 0]; [exp(iθ), 1, ...; [..., ..., 0]; [exp(i(n-1)θ), ..., ..., 1]]   (7)

Sự phân tách này có nghĩa là ma trận trộn token Λ có thể được tách thành hai module độc lập, trong đó Λ mô hình hóa phụ thuộc tầm xa và Θ, một ma trận Toeplitz, mô hình hóa mối quan hệ vị trí tương đối và tăng cường tính biểu đạt. Lưu ý rằng nếu Θ phụ thuộc vào đầu vào, thì ma trận Λ sẽ không còn là ma trận Toeplitz, do đó không thể nắm bắt thông tin vị trí tương đối. Nó cũng có thể được xem như cơ chế attention được tăng cường bởi RoPE: Λ tương ứng với ma trận attention nhưng điểm attention ở đây là tích tích lũy của tỷ lệ suy giảm phụ thuộc vào dữ liệu; Θ trực tiếp tương ứng với RoPE.

4 Thí nghiệm
Chúng tôi thực hiện phân tích so sánh giữa HGRN đề xuất và bốn cấu trúc mô hình hóa chuỗi được áp dụng rộng rãi, tức là dựa trên attention, dựa trên MLP, dựa trên FFT, và dựa trên không gian trạng thái. Chúng tôi đánh giá HGRN trên bộ dữ liệu WikiText-103 [50] và bộ dữ liệu Pile [15] cho mô hình hóa ngôn ngữ tự hồi quy, cũng như khả năng ngoại suy độ dài. Để đánh giá độ chính xác và hiệu quả của mô hình trong việc xử lý phụ thuộc dài hạn, chúng tôi sử dụng benchmark LRA [78]. Ngoài ra, chúng tôi thể hiện tính bền vững của HGRN trong tác vụ thị giác máy tính trên bộ dữ liệu ImageNet-1k.

4.1 Thiết lập
Chúng tôi triển khai các mô hình trong Pytorch [54] và huấn luyện chúng trên 8 GPU Nvidia A100. Đối với HGRN, chúng tôi thấy rằng việc hợp nhất hồi quy theo phần tử vào một CUDA kernel duy nhất dẫn đến tốc độ chạy nhanh trong thực tế. [48] cũng thấy rằng trừ khi độ dài chuỗi đủ lớn, việc triển khai parallel scan không nhất thiết nhanh hơn sequential scan. Do đó, chúng tôi sử dụng sequential scan dựa trên CUDA để triển khai; tuy nhiên, mô hình của chúng tôi có tiềm năng mô hình hóa chuỗi rất dài thông qua việc sử dụng parallel scan.

Chúng tôi áp dụng cùng cấu hình huấn luyện cho tất cả đối thủ cạnh tranh, bao gồm kích thước batch, learning rate, epochs hoặc iterations huấn luyện, v.v. Chúng tôi liệt kê các siêu tham số chi tiết trong Phụ lục. Đối với mô hình hóa ngôn ngữ tự hồi quy, chúng tôi thực hiện ba bộ thí nghiệm. Đầu tiên, chúng tôi xác nhận hiệu suất của hai mô hình quy mô khác nhau trên bộ dữ liệu Wikitext-103. Chúng tôi sử dụng cấu hình TNN để xác minh hiệu suất của mô hình khoảng 44m, và cấu hình Hyena để xác minh hiệu suất của mô hình khoảng 125m. Để đánh giá hiệu suất của các mô hình quy mô lớn hơn, chúng tôi huấn luyện Transformer và HGRN 1b trên bộ dữ liệu Pile sử dụng 10b token. Để đánh giá hiệu suất trong các tác vụ downstream, chúng tôi huấn luyện các mô hình HGRN 150m, 350m, và 1b trên bộ dữ liệu Pile sử dụng 100b token và thực hiện đánh giá zero-shot trên các tác vụ downstream.

Đối với benchmark LRA, chúng tôi báo cáo kết quả trên tất cả 6 tác vụ. Đối với phân loại hình ảnh trên bộ dữ liệu ImageNet-1k, chúng tôi tích hợp HGRN vào cấu trúc DeiT [79], chúng tôi thay thế các lớp transformer bằng các module HGRN của chúng tôi. Nó được so sánh với hiệu suất của DeiT vanilla trên bộ dữ liệu ImageNet-1K cho phân loại hình ảnh.

4.2 Kết quả

Mô hình hóa ngôn ngữ tự hồi quy. Mô hình hóa ngôn ngữ tự hồi quy đứng như một tác vụ nổi bật trong lĩnh vực xử lý ngôn ngữ tự nhiên, vì nó phục vụ như một thước đo khả năng suy luận nhân quả của mô hình ngôn ngữ. Tác vụ này yêu cầu mô hình ước tính phân phối xác suất của token tiếp theo dựa trên các token đã thấy trước đó.

Chúng tôi cho thấy hiệu suất của mô hình hóa ngôn ngữ tự hồi quy trong bảng 1 và bảng 2. So với các phương pháp dựa trên transformer, HGRN hoạt động tốt hơn hầu hết các biến thể hiệu quả của vanilla transformer như FLASH [31], 1+elu [35], Performer [7] và cosFormer [62]. Ngoài ra, HGRN đạt kết quả tốt hơn các phương pháp dựa trên MLP với biên độ đáng chú ý. Tuy nhiên, HGRN hoạt động tương tự như transformer gốc [81]. Cuối cùng, HGRN chia sẻ khái niệm tương tự với các phương pháp dựa trên RNN như S4 [22], DSS [26], GSS [49], RWKV [55], và LRU [53], HGRN của chúng tôi cũng đạt hiệu suất vượt trội so với tất cả các phương pháp dựa trên RNN. Điều này cung cấp bằng chứng rằng HGRN có thể là một phương pháp hiệu quả trong LM. Chúng tôi cũng báo cáo khả năng ngoại suy của HGRN so với các phương pháp trước đó trong Bảng 14.

Chúng tôi cũng huấn luyện mô hình 1b trên bộ dữ liệu Pile và so sánh nó với LRU và Transformer. Cụ thể, các tham số huấn luyện của chúng tôi bao gồm độ dài chuỗi 1024, kích thước batch 96, 100k cập nhật, và learning rate 5e-4. Có thể thấy rằng HGRN vẫn hoạt động tốt hơn ở quy mô 1b. Ngoài ra, chúng tôi huấn luyện 100b token của HGRN trên bộ dữ liệu Pile ở kích thước 150m, 350m, và 1b, và đánh giá chúng so với các mô hình dựa trên Transformer mã nguồn mở trong các tác vụ downstream. Chúng tôi chọn So sánh về Lý luận Thông thường và các tác vụ Super GLUE, và tất cả đánh giá được thực hiện sử dụng lm-evaluation-harness [16]. HGRN đạt hiệu suất tương đương với các mô hình dựa trên Transformer khi chỉ tiêu thụ 1/3 số token.

Long Range Arena. LRA [77] được đề xuất như một đánh giá toàn diện để đánh giá hiệu suất của các mô hình trong việc xử lý phụ thuộc dài hạn trong nhiều tác vụ mô hình hóa tuần tự khác nhau. Chúng tôi cho thấy so sánh hiệu suất giữa HGRN và các phương pháp hiện có trong Bảng 6. HGRN đạt kết quả tương đương với các phương pháp SOTA khác.

Phân loại hình ảnh. Kết quả phân loại hình ảnh trên bộ dữ liệu ImageNet-1K được trình bày trong Bảng 7. Đáng chú ý, với kích thước tham số tương đương, mô hình HGRN đề xuất của chúng tôi thể hiện hiệu suất vượt trội so với các phương pháp trước đó như TNN và vanilla transformer. Nó chứng minh khả năng của HGRN trong mô hình hóa các phương thức thị giác.

4.3 Nghiên cứu loại bỏ
Chúng tôi thực hiện các nghiên cứu loại bỏ trong thiết lập quy mô nhỏ nhất (tức là thiết lập TNN [59] trên bộ dữ liệu WikiText103) để xác minh kỹ lưỡng tính hiệu quả của từng thành phần đề xuất trong HGRN. Các thí nghiệm được thực hiện trên bộ dữ liệu Pile sử dụng mô hình 1b với 10b token cho thí nghiệm cổng quên.

Ảnh hưởng của cổng quên. Trong bảng 8, chúng tôi chứng minh vai trò của cổng quên. Từ bảng 8, chúng tôi quan sát thấy việc loại bỏ cổng quên làm giảm đáng kể hiệu suất của HGRN, trong khi việc thêm cổng quên vào LRU cải thiện hiệu suất. Mặt khác, việc sử dụng cổng quên độc lập với dữ liệu (chỉ giới hạn dưới) dẫn đến hiệu suất thấp hơn so với cổng quên phụ thuộc vào dữ liệu.

Ảnh hưởng của cổng đầu vào và cổng đầu ra. Bảng 9 xác nhận tính hiệu quả của việc sử dụng cổng đầu ra và ràng buộc cổng đầu vào và cổng quên. w/o input gate có nghĩa là loại bỏ term 1-λt. w/o output gate có nghĩa là loại bỏ nhánh trái của HGRU trong hình 1. Thiết kế của chúng tôi đạt hiệu suất tốt nhất.

Ảnh hưởng của giới hạn dưới trong giá trị cổng quên. Chúng tôi chứng minh tính hiệu quả của việc giới thiệu giới hạn dưới trong Bảng 10 và Bảng 13. Từ Bảng 10, chúng tôi quan sát thấy cổng (tức là không có giới hạn dưới) quan trọng hơn giới hạn dưới (tức là chỉ giới hạn dưới). Việc kết hợp cổng và giới hạn dưới luôn mang lại lợi ích, nhưng cải thiện đáng kể nhất phát sinh từ giới hạn dưới tăng đơn điệu. Điều này phù hợp với trực giác rằng các lớp thấp hơn nên tập trung chủ yếu vào các token gần, trong khi các lớp trên nên chú ý rộng hơn để nắm bắt phụ thuộc dài hạn [58].

Bảng 13 nhấn mạnh vai trò thiết yếu của giới hạn dưới trong các tác vụ xử lý chuỗi dài. Trong những tác vụ này, hiệu suất của mô hình đặc biệt kém và đôi khi không hội tụ nếu không có giới hạn dưới. Đáng chú ý rằng các tác vụ mô hình hóa ngôn ngữ không yêu cầu phụ thuộc dài hạn rộng rãi, điều này giải thích tại sao mô hình hoạt động tốt ngay cả khi không có giới hạn dưới. Tuy nhiên, trong tác vụ LRA, khả năng nắm bắt phụ thuộc dài hạn là rất quan trọng để đạt hiệu suất thỏa đáng.

Ảnh hưởng của hồi quy phức. Bảng 11 xác nhận tiện ích của việc kết hợp giá trị phức trong hồi quy tuyến tính theo phần tử. Ngoài ra, các thí nghiệm cho thấy rằng đối số pha θ không nên phụ thuộc vào dữ liệu.

4.4 Phân tích giá trị cổng quên
Chúng tôi trình bày phân phối giá trị cổng quên qua các lớp cho các phương pháp khác nhau trong Bảng 12 và trực quan hóa histogram của mỗi lớp trong Hình 2, được huấn luyện trên tác vụ mô hình hóa ngôn ngữ tự hồi quy. Kết quả chứng minh rằng việc thêm giới hạn dưới hiệu quả làm tăng giá trị cổng quên trung bình trong các lớp cao hơn (5-6). Đáng chú ý, giá trị cổng quên trung bình trong lớp cao nhất đạt 0.98, cho phép mô hình hóa phụ thuộc dài hạn.

Thú vị là giá trị cổng quên trung bình của mô hình LRU luôn vượt quá những giá trị của biến thể mô hình của chúng tôi không có giới hạn dưới, theo eigenvalue của chúng. Tuy nhiên, bất chấp điều này, hiệu suất mô hình hóa ngôn ngữ của LRU thấp hơn so với biến thể của chúng tôi. Cụ thể, LRU ghi điểm 24.71, trong khi biến thể của chúng tôi ghi điểm 31.12. Điều này cho thấy rằng việc sử dụng cổng phụ thuộc vào dữ liệu để giữ lại thông tin liên quan một cách có chọn lọc là có lợi, thay vì dựa vào giá trị cổng quên độc lập với dữ liệu qua tất cả các bước thời gian.

5 Kết luận
Trong công trình này, chúng tôi đã chỉ ra rằng RNNs tuyến tính có cổng có thể đạt được hiệu suất ấn tượng qua các tác vụ và phương thức khác nhau mà không làm giảm hiệu quả. Chúng tôi nhấn mạnh tầm quan trọng của cổng quên cho RNNs tuyến tính trong mô hình hóa ngôn ngữ và nhấn mạnh tầm quan trọng của giới hạn dưới cộng trên giá trị cổng quên để mô hình hóa phụ thuộc dài hạn.

Lời cảm ơn
Công trình này được hỗ trợ một phần bởi Chương trình R&D Quốc gia Trọng điểm của Trung Quốc (SỐ.2022ZD0160100).

Hạn chế và tác động rộng hơn
Đánh giá thực nghiệm của chúng tôi về HGRN vẫn ở quy mô nhỏ hơn so với các mô hình quy mô lớn khác. Hậu quả xã hội tiêu cực tiềm tàng bao gồm việc lạm dụng các mô hình não bộ cho các mục đích hoặc ứng dụng không phù hợp, cái phải được cấm bởi các quy tắc thích hợp. Trong kỷ nguyên của các mô hình ngôn ngữ lớn, chi phí suy luận là hạn chế chính của các mô hình dựa trên transformer. RNNs cung cấp giải pháp với chi phí suy luận thấp hơn. Điều này có thể dẫn đến sự tiến hóa đáng kể trong lĩnh vực này.
