# 2402.19427.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/rnn/2402.19427.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 951680 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================

--- TRANG 1 ---
2024-3-1
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i
Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£
SohamDe*1,SamuelL.Smith*1,AnushanFernando*1,AleksandarBotev*1,GeorgeCristian-Muraru*1,
AlbertGu2,RubaHaroun1,LeonardBerrada1,YutianChen1,SrivatsanSrinivasan1,GuillaumeDesjardins1,
ArnaudDoucet1,DavidBudden1,YeeWhyeTeh1,RazvanPascanu1,NandoDeFreitas1vÃ CaglarGulcehre1
*ÄÃ³ng gÃ³p ngang nhau,1GoogleDeepMind,2CÃ´ng viá»‡c Ä‘Æ°á»£c thá»±c hiá»‡n khi á»Ÿ GoogleDeepMind
Máº¡ng nÆ¡-ron há»“i quy (RNN) cÃ³ tá»‘c Ä‘á»™ suy luáº­n nhanh vÃ  má»Ÿ rá»™ng hiá»‡u quáº£ trÃªn chuá»—i dÃ i, nhÆ°ng chÃºng khÃ³
huáº¥n luyá»‡n vÃ  khÃ³ má»Ÿ rá»™ng. ChÃºng tÃ´i Ä‘á» xuáº¥t Hawk, má»™t RNN vá»›i há»“i quy tuyáº¿n tÃ­nh cÃ³ cá»•ng, vÃ  Griffin,
má»™t mÃ´ hÃ¬nh lai káº¿t há»£p há»“i quy tuyáº¿n tÃ­nh cÃ³ cá»•ng vá»›i attention cá»¥c bá»™. Hawk vÆ°á»£t qua hiá»‡u suáº¥t Ä‘Æ°á»£c bÃ¡o cÃ¡o
cá»§a Mamba trÃªn cÃ¡c tÃ¡c vá»¥ downstream, trong khi Griffin Ä‘áº¡t hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng Llama-2 máº·c dÃ¹
Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn Ã­t hÆ¡n 6 láº§n sá»‘ token. ChÃºng tÃ´i cÅ©ng chá»‰ ra ráº±ng Griffin cÃ³ thá»ƒ ngoáº¡i suy trÃªn cÃ¡c chuá»—i dÃ i
hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i nhá»¯ng gÃ¬ Ä‘Æ°á»£c tháº¥y trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. CÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i Ä‘áº¡t hiá»‡u quáº£ pháº§n cá»©ng
tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i Transformer trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, vÃ  trong suy luáº­n chÃºng cÃ³ Ä‘á»™ trá»… tháº¥p hÆ¡n vÃ  thÃ´ng lÆ°á»£ng
cao hÆ¡n Ä‘Ã¡ng ká»ƒ. ChÃºng tÃ´i má»Ÿ rá»™ng Griffin lÃªn Ä‘áº¿n 14B tham sá»‘, vÃ  giáº£i thÃ­ch cÃ¡ch phÃ¢n chia mÃ´ hÃ¬nh
Ä‘á»ƒ huáº¥n luyá»‡n phÃ¢n tÃ¡n hiá»‡u quáº£.

1. Giá»›i thiá»‡u
Máº¡ng nÆ¡-ron há»“i quy (RNN) Ä‘Ã³ng vai trÃ² trung tÃ¢m trong nhá»¯ng ngÃ y Ä‘áº§u cá»§a há»c sÃ¢u vÃ  nghiÃªn cá»©u NLP
(Elman, 1990; Siegelmann vÃ  Sontag, 1991; Hochreiter vÃ  Schmidhuber, 1997; Mikolov et al.,
2010; Bahdanau et al., 2014; Sutskever et al., 2014), vÃ  Ä‘áº¡t Ä‘Æ°á»£c thÃ nh cÃ´ng thá»±c táº¿ trong nhiá»u á»©ng dá»¥ng,
bao gá»“m há»‡ thá»‘ng dá»‹ch mÃ¡y end-to-end Ä‘áº§u tiÃªn cá»§a Google (Wu et al., 2016). Tuy nhiÃªn trong nhá»¯ng
nÄƒm gáº§n Ä‘Ã¢y, cáº£ há»c sÃ¢u vÃ  NLP Ä‘á»u bá»‹ thá»‘ng trá»‹ bá»Ÿi kiáº¿n trÃºc Transformer (Vaswani
et al., 2017), káº¿t há»£p cÃ¡c perceptron Ä‘a lá»›p (MLP) vÃ  multi-head attention (MHA).
Transformer Ä‘áº¡t hiá»‡u suáº¥t tá»‘t hÆ¡n RNN trong thá»±c táº¿ vÃ  cÅ©ng ráº¥t hiá»‡u quáº£ trong viá»‡c sá»­ dá»¥ng
pháº§n cá»©ng hiá»‡n Ä‘áº¡i (Kaplan et al., 2020). CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n dá»±a trÃªn Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn
bá»™ dá»¯ liá»‡u khá»•ng lá»“ thu tháº­p tá»« web Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c thÃ nh cÃ´ng Ä‘Ã¡ng chÃº Ã½ (Brown et al., 2020; Rae et al., 2021;
Hoffmann et al., 2022; Touvron et al., 2023; Achiam et al., 2023; Gemini Team Google, 2023).

Máº·c dÃ¹ thÃ nh cÃ´ng, Transformer khÃ³ má»Ÿ rá»™ng hiá»‡u quáº£ cho chuá»—i dÃ i do
Ä‘á»™ phá»©c táº¡p báº­c hai cá»§a attention toÃ n cá»¥c. NgoÃ i ra, sá»± tÄƒng trÆ°á»Ÿng tuyáº¿n tÃ­nh cá»§a bá»™ nhá»› cache Key-Value (KV)
vá»›i chiá»u dÃ i chuá»—i lÃ m cho Transformer cháº­m trong suy luáº­n. Máº·c dÃ¹ Multi-Query Attention
(MQA) (Shazeer, 2019) giáº£m thiá»ƒu má»™t pháº§n váº¥n Ä‘á» nÃ y báº±ng cÃ¡ch giáº£m kÃ­ch thÆ°á»›c cache theo má»™t há»‡ sá»‘ khÃ´ng Ä‘á»•i,
cache váº«n tÄƒng tuyáº¿n tÃ­nh theo chiá»u dÃ i chuá»—i. MÃ´ hÃ¬nh ngÃ´n ngá»¯ há»“i quy trÃ¬nh bÃ y má»™t
lá»±a chá»n háº¥p dáº«n vÃ¬ chÃºng nÃ©n toÃ n bá»™ chuá»—i thÃ nh má»™t tráº¡ng thÃ¡i áº©n cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh Ä‘Æ°á»£c cáº­p nháº­t
láº·p Ä‘i láº·p láº¡i. Tuy nhiÃªn Ä‘á»ƒ thay tháº¿ Transformer, cÃ¡c mÃ´ hÃ¬nh RNN má»›i pháº£i chá»©ng minh khÃ´ng chá»‰ hiá»‡u suáº¥t
tÆ°Æ¡ng Ä‘Æ°Æ¡ng á»Ÿ quy mÃ´ mÃ  cÃ²n Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u quáº£ pháº§n cá»©ng tÆ°Æ¡ng tá»± (Gu et al., 2021a; Mehta et al., 2022;
Smith et al., 2022; Orvieto et al., 2023b; Dao et al., 2022b; Poli et al., 2023; Gu vÃ  Dao, 2023).

Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t lá»›p RG-LRU, má»™t lá»›p há»“i quy tuyáº¿n tÃ­nh cÃ³ cá»•ng má»›i, xung quanh Ä‘Ã³ chÃºng tÃ´i thiáº¿t káº¿
má»™t khá»‘i há»“i quy má»›i Ä‘á»ƒ thay tháº¿ MQA. ChÃºng tÃ´i xÃ¢y dá»±ng hai mÃ´ hÃ¬nh má»›i sá»­ dá»¥ng khá»‘i há»“i quy nÃ y: Hawk,
má»™t mÃ´ hÃ¬nh xen káº½ MLP vá»›i cÃ¡c khá»‘i há»“i quy, vÃ  Griffin, má»™t mÃ´ hÃ¬nh lai xen káº½
MLP vá»›i há»—n há»£p cÃ¡c khá»‘i há»“i quy vÃ  attention cá»¥c bá»™ (Beltagy et al., 2020). ChÃºng tÃ´i chá»‰ ra ráº±ng:

1. Hawk vÃ  Griffin thá»ƒ hiá»‡n quy luáº­t mÃ´ rá»™ng power law giá»¯a loss held-out vÃ  training FLOP, lÃªn Ä‘áº¿n vÃ  vÆ°á»£t
qua 7B tham sá»‘ (HÃ¬nh 1(a)), nhÆ° Ä‘Ã£ quan sÃ¡t trÆ°á»›c Ä‘Ã¢y cho Transformer (Kaplan et al., 2020).
2. Griffin Ä‘áº¡t loss held-out tháº¥p hÆ¡n má»™t chÃºt so vá»›i cÃ¡c baseline Transformer máº¡nh á»Ÿ táº¥t cáº£ cÃ¡c quy mÃ´ mÃ´ hÃ¬nh.

TÃ¡c giáº£ liÃªn há»‡: {sohamde, slsmith}@google.comarXiv:2402.19427v1 [cs.LG] 29 Feb 2024

--- TRANG 2 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

10181019102010211022
Training Flops2.02.53.0Validation Loss
MQA
Hawk
Griffin
(a) ÄÆ°á»ng cong má»Ÿ rá»™ng trong huáº¥n luyá»‡n

512 1024 2048 4096
Num tokens decoded05000100001500020000Max tokens/s decoded1.0x
1.0x
1.0x
1.0x1.1x 1.8x 3.3x 6.6x2.5x 4.1x 7.3x 14.8x
MQA
Griffin
Hawk (b) ThÃ´ng lÆ°á»£ng tá»‘i Ä‘a á»Ÿ quy mÃ´ 1B tham sá»‘.

HÃ¬nh 1|a) Hawk, Griffin vÃ  baseline MQA Transformer cá»§a chÃºng tÃ´i Ä‘á»u cho tháº¥y má»Ÿ rá»™ng power law giá»¯a
loss held-out vÃ  training FLOP, vá»›i Griffin Ä‘áº¡t loss held-out tháº¥p nháº¥t á»Ÿ táº¥t cáº£ ngÃ¢n sÃ¡ch FLOP.
MÃ´ hÃ¬nh Griffin lá»›n nháº¥t Ä‘Æ°á»£c hiá»ƒn thá»‹ cÃ³ 14B tham sá»‘. b) Hawk vÃ  Griffin Ä‘áº¡t thÃ´ng lÆ°á»£ng cao hÆ¡n Ä‘Ã¡ng ká»ƒ
so vá»›i MQA Transformer, Ä‘áº·c biá»‡t khi chiá»u dÃ i cá»§a máº«u tÄƒng.

3. ChÃºng tÃ´i overtrain Hawk vÃ  Griffin trÃªn 300B token á»Ÿ má»™t pháº¡m vi quy mÃ´ mÃ´ hÃ¬nh. Hawk-3B vÆ°á»£t qua
hiá»‡u suáº¥t Ä‘Æ°á»£c bÃ¡o cÃ¡o cá»§a Mamba-3B (Gu vÃ  Dao, 2023) trÃªn cÃ¡c tÃ¡c vá»¥ downstream, máº·c dÃ¹
Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t ná»­a sá»‘ token. Griffin-7B vÃ  Griffin-14B Ä‘áº¡t hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng Llama-2
(Touvron et al., 2023) máº·c dÃ¹ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn khoáº£ng 7 láº§n Ã­t token hÆ¡n (Pháº§n 3.2).

4. Cáº£ Hawk vÃ  Griffin Ä‘á»u Ä‘áº¡t hiá»‡u quáº£ huáº¥n luyá»‡n tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i Transformer trÃªn TPU-v3. VÃ¬
cÃ¡c lá»›p RNN Ä‘Æ°á»ng chÃ©o bá»‹ giá»›i háº¡n bá»Ÿi bá»™ nhá»›, chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y vá»›i má»™t kernel cho lá»›p RG-LRU,
Ä‘Æ°á»£c triá»ƒn khai trong Pallas (Bradbury et al., 2018), giáº£m thiá»ƒu viá»‡c truyá»n táº£i bá»™ nhá»› (Pháº§n 4).

5. Trong suy luáº­n, cáº£ Hawk vÃ  Griffin Ä‘áº¡t thÃ´ng lÆ°á»£ng cao hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i MQA Trans-
former (HÃ¬nh 1(b)), vÃ  chÃºng Ä‘áº¡t Ä‘á»™ trá»… tháº¥p hÆ¡n khi láº¥y máº«u chuá»—i dÃ i (Pháº§n 5).

6. Griffin hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n Transformer khi Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c chuá»—i dÃ i hÆ¡n so vá»›i nhá»¯ng gÃ¬ tháº¥y
trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, vÃ  cÅ©ng cÃ³ thá»ƒ há»c hiá»‡u quáº£ cÃ¡c tÃ¡c vá»¥ sao chÃ©p vÃ  truy xuáº¥t tá»« dá»¯ liá»‡u huáº¥n luyá»‡n
(Pháº§n 6). Tuy nhiÃªn, Hawk vÃ  Griffin hoáº¡t Ä‘á»™ng kÃ©m hÆ¡n Transformer khi chÃºng tÃ´i Ä‘Ã¡nh giÃ¡
cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c pre-train trÃªn cÃ¡c tÃ¡c vá»¥ sao chÃ©p vÃ  truy xuáº¥t chÃ­nh xÃ¡c mÃ  khÃ´ng fine-tuning.

2. Kiáº¿n trÃºc MÃ´ hÃ¬nh
Táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i chá»©a cÃ¡c thÃ nh pháº§n sau: (i) má»™t khá»‘i residual, (ii) má»™t khá»‘i MLP, vÃ  (iii) má»™t
khá»‘i temporal-mixing. Trong khi (i) vÃ  (ii) giá»‘ng nhau trÃªn táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh, chÃºng tÃ´i xem xÃ©t ba khá»‘i temporal
mixing: Multi-Query Attention (MQA) toÃ n cá»¥c, MQA cá»¥c bá»™ (sliding-window) vÃ  khá»‘i há»“i quy Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i. NhÆ° má»™t pháº§n cá»§a khá»‘i há»“i quy, chÃºng tÃ´i sá»­ dá»¥ng Real-Gated Linear Recurrent Unit (RG-LRU)
â€“ má»™t lá»›p há»“i quy má»›i Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« Linear Recurrent Unit (Orvieto et al., 2023b).

Khá»‘i residual, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 2(a), Ä‘á»‹nh nghÄ©a cáº¥u trÃºc toÃ n cá»¥c cá»§a cÃ¡c mÃ´ hÃ¬nh vÃ  Ä‘Æ°á»£c láº¥y cáº£m há»©ng
tá»« pre-norm Transformer (Xiong et al., 2020). Sau khi nhÃºng chuá»—i Ä‘áº§u vÃ o, chÃºng tÃ´i truyá»n nÃ³ qua
ğ‘ khá»‘i nhÆ° váº­y (ğ‘ biá»ƒu thá»‹ Ä‘á»™ sÃ¢u mÃ´ hÃ¬nh), vÃ  sau Ä‘Ã³ chÃºng tÃ´i Ã¡p dá»¥ng RMSNorm (Zhang vÃ  Sennrich,
2019) Ä‘á»ƒ táº¡o ra cÃ¡c activation cuá»‘i cÃ¹ng. Äá»ƒ tÃ­nh xÃ¡c suáº¥t token, chÃºng tÃ´i Ã¡p dá»¥ng má»™t lá»›p tuyáº¿n tÃ­nh cuá»‘i cÃ¹ng
theo sau bá»Ÿi softmax. Trá»ng sá»‘ cá»§a lá»›p nÃ y Ä‘Æ°á»£c chia sáº» vá»›i lá»›p nhÃºng Ä‘áº§u vÃ o.

2.1. Khá»‘i residual
Khá»‘i residual chá»©a hai thÃ nh pháº§n, Ä‘Æ°á»£c Ã¡p dá»¥ng theo thá»© tá»±. ThÃ nh pháº§n Ä‘áº§u tiÃªn láº¥y tráº¡ng thÃ¡i áº©n
ğ‘¥ vÃ  Ã¡p dá»¥ng RMSNorm (Zhang vÃ  Sennrich, 2019), theo sau bá»Ÿi khá»‘i temporal-mixing.
2

--- TRANG 3 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

Láº·p láº¡i 
 N láº§n 
(a) Khá»‘i residual RMSNorm 
RMSNorm Temporal 
mixing block MLP block 
Linear GeLULinear 
Linear 
Linear GeLULinear 
Linear Temporal 
Conv1D RG-LRU (b) Khá»‘i MLP cÃ³ cá»•ng
(c) Khá»‘i há»“i quy(vÃ­ dá»¥ MQA hoáº·c khá»‘i há»“i quy) 

HÃ¬nh 2|a) XÆ°Æ¡ng sá»‘ng chÃ­nh cá»§a kiáº¿n trÃºc mÃ´ hÃ¬nh lÃ  khá»‘i residual, Ä‘Æ°á»£c xáº¿p chá»“ng ğ‘
láº§n. b) Khá»‘i MLP cÃ³ cá»•ng mÃ  chÃºng tÃ´i sá»­ dá»¥ng. c) Khá»‘i há»“i quy mÃ  chÃºng tÃ´i Ä‘á» xuáº¥t nhÆ° má»™t thay tháº¿
cho Multi Query Attention (MQA). NÃ³ sá»­ dá»¥ng lá»›p RG-LRU Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i, Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong Pháº§n 2.4.

Sau Ä‘Ã³ chÃºng tÃ´i há»£p nháº¥t Ä‘áº§u ra vá»›i má»™t káº¿t ná»‘i bá» qua tá»« ğ‘¥ thÃ´ng qua phÃ©p cá»™ng. TÆ°Æ¡ng tá»±, thÃ nh pháº§n thá»© hai
Ã¡p dá»¥ng RMSNorm, theo sau bá»Ÿi khá»‘i MLP vÃ  sau Ä‘Ã³ há»£p nháº¥t Ä‘áº§u ra cá»§a nÃ³ vá»›i má»™t káº¿t ná»‘i bá» qua
tá»« Ä‘áº§u vÃ o cá»§a RMSNorm. Khá»‘i nÃ y Ä‘Æ°á»£c minh há»a trong HÃ¬nh 2(a).

2.2. Khá»‘i MLP
ChÃºng tÃ´i sá»­ dá»¥ng má»™t khá»‘i MLP cÃ³ cá»•ng (Dauphin et al., 2017) (Ä‘Æ°á»£c minh há»a trong HÃ¬nh 2(b)), táº¡o ra hai
nhÃ¡nh tá»« Ä‘áº§u vÃ o cÃ³ chiá»u ğ·. ChÃºng tÃ´i Ã¡p dá»¥ng má»™t lá»›p tuyáº¿n tÃ­nh vá»›i chiá»u Ä‘áº§u ra ğ‘€ğ· trÃªn má»—i
nhÃ¡nh, trong Ä‘Ã³ ğ‘€ biá»ƒu thá»‹ há»‡ sá»‘ má»Ÿ rá»™ng. Äá»ƒ Ä‘Æ¡n giáº£n, chÃºng tÃ´i sá»­ dá»¥ng ğ‘€=3 trong toÃ n bá»™ cÃ´ng trÃ¬nh nÃ y. ChÃºng tÃ´i
Ã¡p dá»¥ng phi tuyáº¿n GeLU (Hendrycks vÃ  Gimpel, 2016) trÃªn má»™t trong cÃ¡c nhÃ¡nh trÆ°á»›c khi há»£p nháº¥t chÃºng
báº±ng phÃ©p nhÃ¢n element-wise, tÆ°Æ¡ng tá»± nhÆ° GeGeLU (Shazeer, 2020). Tuy nhiÃªn, trong khá»‘i MLP cá»§a chÃºng tÃ´i,
chÃºng tÃ´i Ã¡p dá»¥ng má»™t lá»›p tuyáº¿n tÃ­nh cuá»‘i cÃ¹ng vá»›i chiá»u Ä‘áº§u ra ğ· trÃªn Ä‘áº§u ra cá»§a lá»›p GeGeLU.

2.3. Khá»‘i temporal-mixing
Khá»‘i temporal-mixing lÃ  thÃ nh pháº§n cá»§a mÃ´ hÃ¬nh tá»•ng há»£p cÃ¡c activation lá»›p áº©n
táº¡i cÃ¡c vá»‹ trÃ­ temporal khÃ¡c nhau trong chuá»—i. ChÃºng tÃ´i xem xÃ©t ba khá»‘i temporal-mixing: MQA toÃ n cá»¥c
(Shazeer, 2019), MQA cá»¥c bá»™ (Beltagy et al., 2020) vÃ  khá»‘i há»“i quy Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i.

Multi-query attention toÃ n cá»¥c Trá»« khi Ä‘Æ°á»£c nÃªu khÃ¡c, chÃºng tÃ´i sá»­ dá»¥ng MQA thay vÃ¬ MHA Ä‘á»ƒ cáº£i thiá»‡n
tá»‘c Ä‘á»™ suy luáº­n cá»§a cÃ¡c baseline Transformer (Shazeer, 2019). ChÃºng tÃ´i sá»­ dá»¥ng chiá»u head cá»‘ Ä‘á»‹nh
ğ·â„ğ‘’ğ‘ğ‘‘=128, vÃ  chÃºng tÃ´i cá»‘ Ä‘á»‹nh sá»‘ lÆ°á»£ng attention head ğ» sao cho ğ»ğ·â„ğ‘’ğ‘ğ‘‘=ğ·. Äiá»u nÃ y yÃªu cáº§u chiá»u mÃ´ hÃ¬nh
ğ· pháº£i lÃ  bá»™i sá»‘ cá»§a 128. ChÃºng tÃ´i khÃ´ng sá»­ dá»¥ng báº¥t ká»³ positional embedding tuyá»‡t Ä‘á»‘i nÃ o, nhÆ°ng chÃºng tÃ´i sá»­ dá»¥ng
Rotary Position Embedding (RoPE) (Su et al., 2021) nhÆ° má»™t positional embedding tÆ°Æ¡ng Ä‘á»‘i.

Sliding window attention cá»¥c bá»™ Má»™t trong nhá»¯ng nhÆ°á»£c Ä‘iá»ƒm chÃ­nh cá»§a viá»‡c sá»­ dá»¥ng attention toÃ n cá»¥c lÃ 
Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n tÄƒng báº­c hai theo chiá»u dÃ i chuá»—i. Äá»ƒ giáº£i quyáº¿t Ä‘iá»u nÃ y, má»™t sá»‘ cÃ´ng trÃ¬nh
Ä‘Ã£ báº¯t Ä‘áº§u Ã¡p dá»¥ng attention cá»¥c bá»™ (Beltagy et al., 2020), cÃ²n Ä‘Æ°á»£c gá»i lÃ  sliding window attention.
NÃ³ cho phÃ©p má»—i vá»‹ trÃ­ chá»‰ attention Ä‘áº¿n má»™t sá»‘ lÆ°á»£ng cá»‘ Ä‘á»‹nh cÃ¡c token trong quÃ¡ khá»©. Äiá»u nÃ y khÃ´ng chá»‰ giáº£m
FLOP tÃ­nh toÃ¡n mÃ  cÃ²n giá»›i háº¡n kÃ­ch thÆ°á»›c cache KV theo kÃ­ch thÆ°á»›c cá»­a sá»•, khiáº¿n nÃ³ khÃ´ng cÃ²n
báº­c hai theo chiá»u dÃ i chuá»—i. Táº¥t cáº£ cÃ¡c chi tiáº¿t khÃ¡c giá»‘ng nhÆ° MQA toÃ n cá»¥c.

3

--- TRANG 4 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

Khá»‘i há»“i quy Khá»‘i há»“i quy cá»§a chÃºng tÃ´i (HÃ¬nh 2(c)) tÆ°Æ¡ng tá»± nhÆ° khá»‘i GSS (Mehta et al., 2022)
vÃ  khá»‘i Ä‘Æ°á»£c sá»­ dá»¥ng bá»Ÿi Mamba (Gu vÃ  Dao, 2023). ChÃºng tÃ´i láº¥y Ä‘áº§u vÃ o cÃ³ chiá»u ğ· vÃ  Ã¡p dá»¥ng hai
lá»›p tuyáº¿n tÃ­nh vá»›i chiá»u Ä‘áº§u ra ğ·ğ‘…ğ‘ğ‘ song song, táº¡o ra hai nhÃ¡nh. TrÃªn nhÃ¡nh Ä‘áº§u tiÃªn,
chÃºng tÃ´i Ã¡p dá»¥ng má»™t lá»›p Conv1D separable nhá», Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« Shift-SSM trong H3 (Dao et al., 2022b), vá»›i
chiá»u bá»™ lá»c temporal lÃ  4. LÆ°u Ã½ ráº±ng lá»›p Conv1D nÃ y ráº¥t nhá», chá»‰ vá»›i 4ğ·ğ‘…ğ‘ğ‘ tham sá»‘.
ChÃºng tÃ´i theo sau lá»›p Conv1D vá»›i lá»›p RG-LRU Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i (Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bÃªn dÆ°á»›i.) TrÃªn nhÃ¡nh thá»© hai,
chÃºng tÃ´i Ã¡p dá»¥ng phi tuyáº¿n GeLU vÃ  sau Ä‘Ã³ há»£p nháº¥t cÃ¡c nhÃ¡nh báº±ng phÃ©p nhÃ¢n element-wise. Sau Ä‘Ã³
chÃºng tÃ´i Ã¡p dá»¥ng má»™t lá»›p tuyáº¿n tÃ­nh cuá»‘i cÃ¹ng vá»›i chiá»u Ä‘áº§u ra ğ·.

2.4. Real-Gated Linear Recurrent Unit (RG-LRU)
Lá»›p RG-LRU Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i cÃ³ má»™t há»“i quy Ä‘Æ¡n giáº£n Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« Linear Recurrent Unit (LRU)
(Orvieto et al., 2023b), nhÆ°ng káº¿t há»£p má»™t cÆ¡ cháº¿ gating Ä‘Æ°á»£c thÃºc Ä‘áº©y bá»Ÿi vÄƒn liá»‡u vá»
RNN phi tuyáº¿n, Ä‘áº·c biá»‡t lÃ  LSTM (Hochreiter vÃ  Schmidhuber, 1997) vÃ  GRU (Chung et al., 2014). CÃ¡c
phÆ°Æ¡ng trÃ¬nh mÃ´ táº£ lá»›p nhÆ° sau:

ğ‘Ÿğ‘¡=ğœ(ğ‘Šğ‘ğ‘¥ğ‘¡+ğ‘ğ‘),recurrence gate (1)
ğ‘–ğ‘¡=ğœ(ğ‘Šğ‘¥ğ‘¥ğ‘¡+ğ‘ğ‘¥),input gate (2)
ğ‘ğ‘¡=ğ‘ğ‘ğ‘Ÿğ‘¡, (3)
â„ğ‘¡=ğ‘ğ‘¡âŠ™â„ğ‘¡âˆ’1+âˆšï¸ƒ
1âˆ’ğ‘2
ğ‘¡âŠ™(ğ‘–ğ‘¡âŠ™ğ‘¥ğ‘¡). (4)

Äáº§u ra cá»§a lá»›p lÃ  ğ‘¦ğ‘¡=â„ğ‘¡, vÃ  phi tuyáº¿n ğœ trong cÃ¡c phÆ°Æ¡ng trÃ¬nh lÃ  hÃ m sigmoid. Trá»ng sá»‘ há»“i quy
ğ‘ trong PhÆ°Æ¡ng trÃ¬nh (4) lÃ  Ä‘Æ°á»ng chÃ©o. Do Ä‘Ã³ táº¥t cáº£ cÃ¡c phÃ©p toÃ¡n Ä‘á»u theo element-wise. ChÃºng tÃ´i tham sá»‘ hÃ³a
ğ‘ trong PhÆ°Æ¡ng trÃ¬nh (3) lÃ  ğ‘=ğœ(Î›), trong Ä‘Ã³ Î› lÃ  má»™t tham sá»‘ cÃ³ thá»ƒ há»c. Äiá»u nÃ y Ä‘áº£m báº£o ráº±ng 0â‰¤ğ‘â‰¤1, Ä‘áº£m báº£o
ráº±ng há»“i quy á»•n Ä‘á»‹nh. Biáº¿n ğ‘ lÃ  má»™t háº±ng sá»‘ cÃ³ giÃ¡ trá»‹ scalar Ä‘Æ°á»£c Ä‘áº·t thÃ nh 8. Äá»ƒ á»•n Ä‘á»‹nh sá»‘, trong thá»±c táº¿ chÃºng tÃ´i tÃ­nh ğ‘ğ‘ğ‘Ÿğ‘¡ trong log-space (xem Phá»¥ lá»¥c A). Lá»›p cÃ³ gate trÃªn cáº£ Ä‘áº§u vÃ o ğ‘¥ vÃ 
trá»ng sá»‘ há»“i quy ğ‘. Tuy nhiÃªn, khÃ´ng gate nÃ o phá»¥ thuá»™c vÃ o tráº¡ng thÃ¡i há»“i quy â„ğ‘¡âˆ’1, Ä‘iá»u nÃ y Ä‘áº£m báº£o ráº±ng
tÃ­nh toÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c thá»±c hiá»‡n hiá»‡u quáº£ trÃªn thiáº¿t bá»‹. ChÃºng tÃ´i khá»Ÿi táº¡o cáº£ ğ‘Šğ‘ vÃ  ğ‘Šğ‘¥ báº±ng LeCun init
(LeCun et al., 2002). ChÃºng tÃ´i khá»Ÿi táº¡o Î› sao cho ğ‘ğ‘ Ä‘Æ°á»£c phÃ¢n bá»‘ Ä‘á»u giá»¯a 0.9 vÃ  0.999 táº¡i
Ä‘áº§u huáº¥n luyá»‡n, tÆ°Æ¡ng tá»± nhÆ° Orvieto et al. (2023b). KhÃ´ng giá»‘ng nhÆ° nhiá»u cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y trong vÄƒn liá»‡u SSM,
RG-LRU khÃ´ng sá»­ dá»¥ng khá»Ÿi táº¡o Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« lÃ½ thuyáº¿t Ä‘a thá»©c trá»±c giao (Gu et al., 2020),
vÃ  nÃ³ cÅ©ng khÃ´ng Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a nhÆ° sá»± rá»i ráº¡c hÃ³a cá»§a má»™t há»‡ thá»‘ng liÃªn tá»¥c cÆ¡ báº£n (Gu et al., 2021a).

KhÃ´ng giá»‘ng nhÆ° lá»›p LRU gá»‘c, chÃºng tÃ´i khÃ´ng sá»­ dá»¥ng Ä‘áº¡i sá»‘ phá»©c trong há»“i quy. Trong khi sá»­ dá»¥ng
há»“i quy phá»©c sáº½ dáº«n Ä‘áº¿n má»™t lá»›p biá»ƒu cáº£m hÆ¡n (Orvieto et al., 2023a), chÃºng tÃ´i tháº¥y ráº±ng
há»“i quy phá»©c khÃ´ng cÃ³ lá»£i cho mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ trong thá»±c táº¿, nhÆ° cÅ©ng Ä‘Æ°á»£c quan sÃ¡t bá»Ÿi Gu vÃ  Dao
(2023).1

HÃ nh vi gate Input gate ğ‘–ğ‘¡ tÆ°Æ¡ng tá»± nhÆ° trong LSTM, cÃ³ thá»ƒ lá»c (hoáº·c giáº£m xuá»‘ng) Ä‘áº§u vÃ o
ğ‘¥ğ‘¡. Tuy nhiÃªn, theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i, recurrence gate ğ‘Ÿğ‘¡ khÃ¡c vá»›i cÃ¡c cÆ¡ cháº¿ gating khÃ¡c
trong vÄƒn liá»‡u. VÃ­ dá»¥, cÆ¡ cháº¿ selection Ä‘Æ°á»£c Ä‘á» xuáº¥t trong Mamba (Gu vÃ  Dao, 2023) cÃ³ thá»ƒ
so sÃ¡nh vá»›i update gate cá»§a GRU ná»™i suy giá»¯a tráº¡ng thÃ¡i trÆ°á»›c Ä‘Ã³ vÃ  quan sÃ¡t hiá»‡n táº¡i
ğ‘¥ğ‘¡. TÃ¡c Ä‘á»™ng cá»§a nÃ³ trÃªn tráº¡ng thÃ¡i áº©n cho phÃ©p nÃ³ reset tráº¡ng thÃ¡i vÃ  quÃªn báº¥t ká»³ thÃ´ng tin nÃ o nÃ³ giá»¯
tá»« quÃ¡ khá»©, tÆ°Æ¡ng tá»± nhÆ° forget gate trong LSTM. NgÆ°á»£c láº¡i, recurrence gate cá»§a chÃºng tÃ´i cÃ³ thá»ƒ xáº¥p xá»‰
ná»™i suy giá»¯a cáº­p nháº­t LRU tiÃªu chuáº©n tá»« Orvieto et al. (2023a) vÃ  tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³,
cho phÃ©p nÃ³ hiá»‡u quáº£ loáº¡i bá» Ä‘áº§u vÃ o vÃ  báº£o tá»“n táº¥t cáº£ thÃ´ng tin tá»« lá»‹ch sá»­ trÆ°á»›c Ä‘Ã³
(xem Phá»¥ lá»¥c A Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t). ChÃºng tÃ´i tin ráº±ng vai trÃ² chÃ­nh cá»§a gate nÃ y lÃ  cho phÃ©p mÃ´ hÃ¬nh Ä‘áº¡t Ä‘Æ°á»£c
bá»™ nhá»› siÃªu exponential báº±ng cÃ¡ch giáº£m áº£nh hÆ°á»Ÿng cá»§a cÃ¡c Ä‘áº§u vÃ o khÃ´ng cÃ³ thÃ´ng tin.

1ChÃºng tÃ´i Ä‘á» xuáº¥t ablating viá»‡c sá»­ dá»¥ng sá»‘ phá»©c cho cÃ¡c modality khÃ¡c vÃ  cung cáº¥p thÃªm thÃ´ng tin vá»
phiÃªn báº£n giÃ¡ trá»‹ phá»©c cá»§a lá»›p RG-LRU trong Phá»¥ lá»¥c B.
4

--- TRANG 5 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

3. MÃ´ hÃ¬nh Há»“i quy Má»Ÿ rá»™ng Hiá»‡u quáº£ nhÆ° Transformer
NghiÃªn cá»©u má»Ÿ rá»™ng cung cáº¥p nhá»¯ng hiá»ƒu biáº¿t quan trá»ng vá» cÃ¡ch Ä‘iá»u chá»‰nh cÃ¡c siÃªu tham sá»‘ cá»§a mÃ´ hÃ¬nh vÃ 
hÃ nh vi cá»§a nÃ³ á»Ÿ quy mÃ´. á» Ä‘Ã¢y, chÃºng tÃ´i Ä‘á»‹nh nghÄ©a cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trong nghiÃªn cá»©u cá»§a chÃºng tÃ´i,
vÃ  cung cáº¥p cÃ¡c Ä‘Æ°á»ng cong má»Ÿ rá»™ng lÃªn Ä‘áº¿n vÃ  vÆ°á»£t qua 7B tham sá»‘. Cuá»‘i cÃ¹ng, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh trÃªn cÃ¡c
tÃ¡c vá»¥ downstream. ChÃºng tÃ´i xem xÃ©t 3 há» mÃ´ hÃ¬nh trong cÃ´ng trÃ¬nh nÃ y; (1) baseline MQA-Transformer, (2) Hawk; mÃ´ hÃ¬nh
RNN thuáº§n tÃºy cá»§a chÃºng tÃ´i, vÃ  (3) Griffin; mÃ´ hÃ¬nh lai káº¿t há»£p cÃ¡c khá»‘i há»“i quy vá»›i attention cá»¥c bá»™. ChÃºng tÃ´i
Ä‘á»‹nh nghÄ©a cÃ¡c siÃªu tham sá»‘ mÃ´ hÃ¬nh chÃ­nh cho cÃ¡c mÃ´ hÃ¬nh trÃªn má»™t pháº¡m vi quy mÃ´ trong Phá»¥ lá»¥c C.

Baseline MQA Transformer Baseline Transformer cá»§a chÃºng tÃ´i sá»­ dá»¥ng máº«u residual vÃ  khá»‘i MLP cÃ³ cá»•ng
Ä‘Æ°á»£c mÃ´ táº£ trong Pháº§n 2, káº¿t há»£p vá»›i MQA (Shazeer, 2019) vÃ  RoPE (Su et al., 2021).

Hawk Kiáº¿n trÃºc Hawk sá»­ dá»¥ng cÃ¹ng máº«u residual vÃ  khá»‘i MLP nhÆ° baseline Transformer
cá»§a chÃºng tÃ´i, nhÆ°ng chÃºng tÃ´i sá»­ dá»¥ng khá»‘i há»“i quy Ä‘Æ°á»£c giá»›i thiá»‡u trong Pháº§n 2.3 vá»›i lá»›p RG-LRU (xem Pháº§n 2.4)
nhÆ° khá»‘i temporal mixing cá»§a chÃºng tÃ´i, thay vÃ¬ MQA. ChÃºng tÃ´i má»Ÿ rá»™ng chiá»u rá»™ng cá»§a khá»‘i há»“i quy theo há»‡ sá»‘
khoáº£ng 43 (tá»©c lÃ  ğ·ğ‘…ğ‘ğ‘â‰ˆ4ğ·/3) Ä‘á»ƒ Ä‘áº¡i khÃ¡i khá»›p vá»›i sá»‘ lÆ°á»£ng tham sá»‘ cá»§a khá»‘i MHA
khi cáº£ hai sá»­ dá»¥ng cÃ¹ng chiá»u mÃ´ hÃ¬nh ğ·.2 Xem Phá»¥ lá»¥c C cho cÃ¡c siÃªu tham sá»‘ chÃ­nh xÃ¡c.

Griffin Lá»£i Ã­ch chÃ­nh cá»§a cÃ¡c khá»‘i há»“i quy so vá»›i attention toÃ n cá»¥c lÃ  chÃºng sá»­ dá»¥ng kÃ­ch thÆ°á»›c tráº¡ng thÃ¡i cá»‘ Ä‘á»‹nh
Ä‘á»ƒ tÃ³m táº¯t chuá»—i, trong khi kÃ­ch thÆ°á»›c cache KV cá»§a MQA tÄƒng tá»‰ lá»‡ vá»›i chiá»u dÃ i chuá»—i.
VÃ¬ attention cá»¥c bá»™ (Pháº§n 2.3) cÃ³ cÃ¹ng tÃ­nh cháº¥t nÃ y, viá»‡c káº¿t há»£p cÃ¡c khá»‘i há»“i quy vá»›i attention cá»¥c bá»™ báº£o tá»“n lá»£i Ã­ch nÃ y. ChÃºng tÃ´i tháº¥y sá»± káº¿t há»£p nÃ y cá»±c ká»³ hiá»‡u quáº£, vÃ¬ attention cá»¥c bá»™ mÃ´ hÃ¬nh chÃ­nh xÃ¡c
quÃ¡ khá»© gáº§n Ä‘Ã¢y, trong khi cÃ¡c lá»›p há»“i quy cÃ³ thá»ƒ truyá»n thÃ´ng tin qua cÃ¡c chuá»—i dÃ i.

Griffin sá»­ dá»¥ng cÃ¹ng máº«u residual vÃ  khá»‘i MLP nhÆ° baseline Transformer cá»§a chÃºng tÃ´i. Tuy nhiÃªn khÃ´ng giá»‘ng
cáº£ baseline MQA Transformer vÃ  mÃ´ hÃ¬nh Hawk, Griffin sá»­ dá»¥ng há»—n há»£p cÃ¡c khá»‘i há»“i quy
vÃ  khá»‘i MQA. Cá»¥ thá»ƒ, chÃºng tÃ´i sá»­ dá»¥ng cáº¥u trÃºc lá»›p báº±ng cÃ¡ch xen káº½ hai khá»‘i residual vá»›i
má»™t khá»‘i há»“i quy theo sau bá»Ÿi má»™t khá»‘i residual sá»­ dá»¥ng khá»‘i attention cá»¥c bá»™ (MQA) Ä‘Æ°á»£c mÃ´ táº£
trong Pháº§n 2.3. Trá»« khi Ä‘Æ°á»£c nÃªu khÃ¡c, kÃ­ch thÆ°á»›c cá»­a sá»• attention cá»¥c bá»™ Ä‘Æ°á»£c cá»‘ Ä‘á»‹nh á»Ÿ 1024 token.

3.1. ÄÆ°á»ng cong má»Ÿ rá»™ng
ChÃºng tÃ´i trÃ¬nh bÃ y káº¿t quáº£ má»Ÿ rá»™ng chÃ­nh trong HÃ¬nh 1(a). Cáº£ ba há» mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n á»Ÿ má»™t pháº¡m vi
quy mÃ´ mÃ´ hÃ¬nh tá»« 100M Ä‘áº¿n 7B tham sá»‘, vá»›i má»™t mÃ´ hÃ¬nh Griffin bá»• sung cÃ³ 14 tá»· tham sá»‘.
ChÃºng tÃ´i tÄƒng sá»‘ lÆ°á»£ng token huáº¥n luyá»‡n Ä‘á»ƒ tá»‰ lá»‡ thuáº­n vá»›i sá»‘ lÆ°á»£ng tham sá»‘
cá»§a mÃ´ hÃ¬nh, nhÆ° Ä‘Æ°á»£c quy Ä‘á»‹nh bá»Ÿi luáº­t má»Ÿ rá»™ng Chinchilla (Hoffmann et al., 2022). CÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n
trÃªn bá»™ dá»¯ liá»‡u Massive Text (Hoffmann et al., 2022), trÆ°á»›c Ä‘Ã¢y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n Gopher (Rae et al., 2021)
vÃ  Chinchilla (Hoffmann et al., 2022), máº·c dÃ¹ chÃºng tÃ´i sá»­ dá»¥ng phÃ¢n phá»‘i táº­p con dá»¯ liá»‡u hÆ¡i khÃ¡c.
Chiá»u dÃ i chuá»—i 2048 token Ä‘Æ°á»£c sá»­ dá»¥ng (xem Pháº§n 6 cho káº¿t quáº£ vá»›i chuá»—i dÃ i hÆ¡n.) Táº¥t cáº£
thÃ­ nghiá»‡m sá»­ dá»¥ng optimizer AdamW (Loshchilov vÃ  Hutter, 2017). ChÃºng tÃ´i Ä‘iá»u chá»‰nh learning rate,
weight decay vÃ  tham sá»‘ ğ›½2 cho cÃ¡c mÃ´ hÃ¬nh nhá», vÃ  sá»­ dá»¥ng cÃ¡c cháº¡y nÃ y Ä‘á»ƒ xÃ¡c Ä‘á»‹nh quy táº¯c má»Ÿ rá»™ng cho
cÃ¡c siÃªu tham sá»‘ nÃ y dá»± Ä‘oÃ¡n giÃ¡ trá»‹ tá»‘i Æ°u cá»§a chÃºng cho cÃ¡c mÃ´ hÃ¬nh 7B vÃ  14B.

Cáº£ ba há» mÃ´ hÃ¬nh Ä‘á»u thá»ƒ hiá»‡n má»‘i quan há»‡ má»Ÿ rá»™ng tuyáº¿n tÃ­nh giá»¯a validation loss vÃ 
training FLOP (xem HÃ¬nh 1(a); lÆ°u Ã½ cáº£ hai trá»¥c Ä‘á»u á»Ÿ thang log), nhÆ° Ä‘Ã£ quan sÃ¡t trÆ°á»›c Ä‘Ã¢y cho Transformer
bá»Ÿi Brown et al. (2020). ÄÃ¡ng chÃº Ã½, Griffin Ä‘áº¡t validation loss tháº¥p hÆ¡n so vá»›i baseline Transformer
trÃªn táº¥t cáº£ ngÃ¢n sÃ¡ch FLOP máº·c dÃ¹ khÃ´ng sá»­ dá»¥ng báº¥t ká»³ lá»›p attention toÃ n cá»¥c nÃ o. Hawk máº·t khÃ¡c Ä‘áº¡t
validation loss cao hÆ¡n má»™t chÃºt, nhÆ°ng khoáº£ng cÃ¡ch nÃ y dÆ°á»ng nhÆ° thu háº¹p khi ngÃ¢n sÃ¡ch huáº¥n luyá»‡n tÄƒng.

2LÆ°u Ã½ ráº±ng chÃºng tÃ´i khá»›p tham sá»‘ vá»›i khá»‘i attention MHA, máº·c dÃ¹ baseline Transformer vÃ  Griffin cuá»‘i cÃ¹ng
dá»±a vÃ o attention MQA Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u quáº£ suy luáº­n. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cÃ¡c khá»‘i há»“i quy cá»§a chÃºng tÃ´i cÃ³ nhiá»u
tham sá»‘ hÆ¡n má»™t chÃºt so vá»›i cÃ¡c khá»‘i MQA tÆ°Æ¡ng á»©ng.
5

--- TRANG 6 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

Báº£ng 1|Äá»™ chÃ­nh xÃ¡c Ä‘Æ°á»£c chuáº©n hÃ³a theo kÃ½ tá»±. Hawk cáº¡nh tranh vá»›i baseline Transformer cá»§a chÃºng tÃ´i, vÃ 
vÆ°á»£t qua hiá»‡u suáº¥t Ä‘Æ°á»£c bÃ¡o cÃ¡o cá»§a Mamba máº·c dÃ¹ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t ná»­a sá»‘ token. Griffin
vÆ°á»£t trá»™i so vá»›i baseline Transformer cá»§a chÃºng tÃ´i, vÃ  Ä‘áº¡t hiá»‡u suáº¥t tÆ°Æ¡ng Ä‘Æ°Æ¡ng Llama-2 máº·c dÃ¹ Ä‘Æ°á»£c huáº¥n luyá»‡n
trÃªn khoáº£ng 7 láº§n Ã­t token hÆ¡n. ChÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c khÃ´ng chuáº©n hÃ³a vá»›i tÃ­nh Ä‘iá»ƒm má»™t pháº§n cho WinoGrande.

Model Model TrainingMMLU HellaSwag PIQA WinoGrande ARC-E ARC-C Average
Type Size Tokens
Mamba 3B 600B 26.2 71.0 78.1 65.9 68.2 41.7 58.5
7B 2T 45.3 77.2 78.8 69.2 75.2 45.9 65.3
Llama-213B 2T 54.8 80.7 80.5 72.8 77.3 49.4 69.3
MQA 1B 300B 28.9 64.8 75.0 62.0 60.2 35.4 54.4
Transformer 3B 300B 31.7 71.0 77.6 66.1 68.1 39.2 59.0
(Baseline) 6B 300B 38.9 77.0 79.5 70.4 74.1 45.2 64.2
1B 300B 29.7 63.3 76.1 57.2 60.6 34.6 53.6
3B 300B 31.3 71.7 78.8 66.5 68.4 40.2 59.5 Hawk
7B 300B 35.0 77.6 80.0 69.9 74.4 45.9 63.8
Griffin1B 300B 29.5 67.2 77.4 65.2 67.0 36.9 57.2
3B 300B 32.6 73.5 78.1 67.2 71.5 41.4 60.7
7B 300B 39.3 78.6 81.0 72.6 75.4 47.9 65.8
14B 300B 49.5 81.4 81.8 74.1 79.1 50.8 69.5

3.2. ÄÃ¡nh giÃ¡ trÃªn cÃ¡c tÃ¡c vá»¥ downstream
Äá»ƒ so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh khÃ¡c trong vÄƒn liá»‡u, chÃºng tÃ´i huáº¥n luyá»‡n táº¥t cáº£ mÃ´ hÃ¬nh trong 300B token trÆ°á»›c khi
Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c tÃ¡c vá»¥ downstream. Hai baseline bÃªn ngoÃ i mÃ  chÃºng tÃ´i so sÃ¡nh lÃ  Mamba-3B (Gu vÃ 
Dao, 2023), mÃ´ hÃ¬nh há»“i quy nhá» máº¡nh nháº¥t Ä‘Æ°á»£c bÃ¡o cÃ¡o trong vÄƒn liá»‡u cho Ä‘áº¿n nay, vÃ  Llama-2 (Touvron
et al., 2023), má»™t mÃ´ hÃ¬nh Transformer má»Ÿ Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i. Cáº£ hai baseline bÃªn ngoÃ i Ä‘á»u Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn
Ä‘Ã¡ng ká»ƒ hÆ¡n 300B tokenâ€“Mamba Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 600B token, gáº¥p Ä‘Ã´i, vÃ  Llama-2
Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 2T token, gáº§n gáº¥p báº£y láº§n. Tuy nhiÃªn chÃºng tÃ´i lÆ°u Ã½ ráº±ng cáº£ Mamba vÃ  Llama-2
Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c bá»™ dá»¯ liá»‡u khÃ¡c nhau vÃ  vá»›i cÃ¡c chiáº¿n lÆ°á»£c Ä‘iá»u chá»‰nh siÃªu tham sá»‘ khÃ¡c nhau, cÃ³ thá»ƒ
giáº£i thÃ­ch má»™t pháº§n hiá»‡u suáº¥t máº¡nh cá»§a chÃºng tÃ´i. Do Ä‘Ã³ chÃºng tÃ´i cÅ©ng bao gá»“m baseline MQA transformer cá»§a riÃªng chÃºng tÃ´i,
Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¹ng dá»¯ liá»‡u vÃ  vá»›i cÃ¹ng ngÃ¢n sÃ¡ch Ä‘iá»u chá»‰nh siÃªu tham sá»‘ nhÆ° Hawk vÃ  Griffin.

ChÃºng tÃ´i cung cáº¥p Ä‘Ã¡nh giÃ¡ trÃªn cÃ¡c tÃ¡c vá»¥ downstream trong Báº£ng 1. ChÃºng tÃ´i tháº¥y ráº±ng cáº£ Hawk vÃ  Griffin Ä‘áº¡t
hiá»‡u suáº¥t ráº¥t máº¡nh. PhÃ¹ há»£p vá»›i cÃ¡c cÃ´ng trÃ¬nh khÃ¡c, chÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c Ä‘Æ°á»£c chuáº©n hÃ³a theo kÃ½ tá»± trÃªn MMLU,
HellaSwag, PIQA, ARC-E vÃ  ARC-C, trong khi chÃºng tÃ´i bÃ¡o cÃ¡o Ä‘á»™ chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i trÃªn WinoGrande vá»›i tÃ­nh Ä‘iá»ƒm
má»™t pháº§n. Hiá»‡u suáº¥t cá»§a Hawk cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ khi chÃºng tÃ´i tÄƒng kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh, vÃ  Hawk-3B
Ä‘áº¡t hiá»‡u suáº¥t máº¡nh hÆ¡n trÃªn cÃ¡c tÃ¡c vá»¥ downstream so vá»›i Mamba-3B, máº·c dÃ¹ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t ná»­a
sá»‘ token. Griffin-3B vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i Mamba-3B, vÃ  Griffin-7B vÃ  Griffin-14B Ä‘áº¡t
hiá»‡u suáº¥t cáº¡nh tranh vá»›i Llama-2, máº·c dÃ¹ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn gáº§n 7 láº§n Ã­t token hÆ¡n. Hawk
cÅ©ng cáº¡nh tranh vá»›i baseline MQA Transformer cá»§a chÃºng tÃ´i, trong khi Griffin vÆ°á»£t trá»™i so vá»›i baseline nÃ y.

4. Huáº¥n luyá»‡n MÃ´ hÃ¬nh Há»“i quy Hiá»‡u quáº£ trÃªn Thiáº¿t bá»‹
ChÃºng tÃ´i gáº·p pháº£i hai thÃ¡ch thá»©c ká»¹ thuáº­t chÃ­nh khi phÃ¡t triá»ƒn vÃ  má»Ÿ rá»™ng cÃ¡c mÃ´ hÃ¬nh. Äáº§u tiÃªn, lÃ m tháº¿ nÃ o
Ä‘á»ƒ phÃ¢n chia hiá»‡u quáº£ cÃ¡c mÃ´ hÃ¬nh qua nhiá»u thiáº¿t bá»‹. Thá»© hai, lÃ m tháº¿ nÃ o Ä‘á»ƒ triá»ƒn khai hiá»‡u quáº£ cÃ¡c há»“i quy tuyáº¿n tÃ­nh
Ä‘á»ƒ tá»‘i Ä‘a hÃ³a hiá»‡u quáº£ huáº¥n luyá»‡n trÃªn TPU. ChÃºng tÃ´i giáº£i quyáº¿t cáº£ hai thÃ¡ch thá»©c nÃ y trong pháº§n nÃ y,
trÆ°á»›c khi cung cáº¥p so sÃ¡nh thá»±c nghiá»‡m vá» tá»‘c Ä‘á»™ huáº¥n luyá»‡n cá»§a Griffin vÃ  baseline MQA cá»§a chÃºng tÃ´i.
6

--- TRANG 7 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

4.1. Model parallelism cho huáº¥n luyá»‡n quy mÃ´ lá»›n
Khi mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i tÄƒng kÃ­ch thÆ°á»›c, chÃºng tÃ´i khÃ´ng thá»ƒ fit mÃ´ hÃ¬nh trÃªn má»™t thiáº¿t bá»‹ duy nháº¥t trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, ngay cáº£ vá»›i
batch size lÃ  1 per-device. Do Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng model parallelism Ä‘á»ƒ phÃ¢n chia cÃ¡c mÃ´ hÃ¬nh lá»›n qua cÃ¡c thiáº¿t bá»‹
trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. VÃ¬ chi phÃ­ giao tiáº¿p qua cÃ¡c thiáº¿t bá»‹ huáº¥n luyá»‡n khÃ¡c nhau ráº¥t Ä‘áº¯t, viá»‡c phÃ¢n chia
mÃ´ hÃ¬nh hiá»‡u quáº£ lÃ  quan trá»ng cho huáº¥n luyá»‡n nhanh á»Ÿ quy mÃ´.

Khá»‘i MLP vÃ  MQA Cho khá»‘i gated-MLP cá»§a chÃºng tÃ´i, chÃºng tÃ´i sá»­ dá»¥ng phÃ¢n chia theo phong cÃ¡ch Megatron (Shoeybi et al., 2019),
yÃªu cáº§u má»™t phÃ©p toÃ¡n all-reduce duy nháº¥t trong cáº£ forward pass vÃ  backward pass. TÆ°Æ¡ng tá»±,
chÃºng tÃ´i Ã¡p dá»¥ng cÃ¹ng chiáº¿n lÆ°á»£c cho cÃ¡c lá»›p tuyáº¿n tÃ­nh trong khá»‘i attention, vÃ  bá»• sung phÃ¢n chia cÆ¡ cháº¿ attention
qua cÃ¡c head cá»§a nÃ³ (Narayanan et al., 2021).

Khá»‘i há»“i quy Khá»‘i há»“i quy chá»©a hai lá»›p tuyáº¿n tÃ­nh trÃªn má»—i nhÃ¡nh. Äiá»u nÃ y cho phÃ©p chÃºng tÃ´i Ã¡p dá»¥ng
phÃ¢n chia Megatron cho cÃ¡c lá»›p nÃ y theo cÃ¡ch tÆ°Æ¡ng Ä‘Æ°Æ¡ng. Lá»›p Conv1D hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p
qua cÃ¡c kÃªnh, cho phÃ©p chÃºng tÃ´i chia tham sá»‘ cá»§a nÃ³ qua cÃ¡c thiáº¿t bá»‹ mÃ  khÃ´ng phÃ¡t sinh chi phÃ­ giao tiáº¿p
nÃ o. Äá»ƒ trÃ¡nh giao tiáº¿p qua thiáº¿t bá»‹ bá»• sung, chÃºng tÃ´i sá»­ dá»¥ng trá»ng sá»‘ block-diagonal cho cÃ¡c
gate trong RG-LRU (xem phÆ°Æ¡ng trÃ¬nh 1 vÃ  2), thay vÃ¬ ma tráº­n dÃ y Ä‘áº·c. Cho táº¥t cáº£ thÃ­ nghiá»‡m trong
bÃ i bÃ¡o nÃ y, chÃºng tÃ´i sá»­ dá»¥ng 16 khá»‘i cho cáº£ recurrence gate vÃ  input gate (sao cho ğ‘Šğ‘¥ vÃ  ğ‘Šğ‘ má»—i cÃ¡i
cÃ³ ğ·2ğ‘…ï¿½ï¿½ğ‘/16 tham sá»‘). Cáº¥u trÃºc Ä‘Æ°á»ng chÃ©o cá»§a há»“i quy cung cáº¥p cÃ¹ng lá»£i Ã­ch nhÆ°
Conv1D, cho phÃ©p phÃ¢n chia tham sá»‘ vÃ  tÃ­nh toÃ¡n mÃ  khÃ´ng cáº§n giao tiáº¿p nÃ o. Vá»›i chiáº¿n lÆ°á»£c nÃ y,
yÃªu cáº§u giao tiáº¿p cá»§a khá»‘i há»“i quy tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i khá»‘i MLP.

CÃ¡c cÃ¢n nháº¯c khÃ¡c Tráº¡ng thÃ¡i optimizer cÃ³ thá»ƒ tiÃªu thá»¥ bá»™ nhá»› Ä‘Ã¡ng ká»ƒ, vÆ°á»£t quÃ¡ kÃ­ch thÆ°á»›c cá»§a
tham sá»‘ mÃ´ hÃ¬nh. Äá»ƒ giáº£i quyáº¿t Ä‘iá»u nÃ y, chÃºng tÃ´i sá»­ dá»¥ng ZeRO parallelism (Rajbhandari et al., 2020),
phÃ¢n phá»‘i cáº£ tráº¡ng thÃ¡i optimizer vÃ  tham sá»‘ mÃ´ hÃ¬nh qua cÃ¡c shard batch. ChÃºng tÃ´i cÅ©ng sá»­ dá»¥ng
biá»ƒu diá»…n bfloat16 cho tham sá»‘ mÃ´ hÃ¬nh vÃ  activation, giáº£m thiá»ƒu báº¥t ká»³ overhead truyá»n dá»¯ liá»‡u nÃ o.

4.2. Há»“i quy tuyáº¿n tÃ­nh hiá»‡u quáº£ trÃªn thiáº¿t bá»‹
CÃ¡c accelerator há»c sÃ¢u hiá»‡n táº¡i Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho cÃ¡c kiáº¿n trÃºc cá»• Ä‘iá»ƒn bao gá»“m chá»§ yáº¿u
phÃ©p nhÃ¢n ma tráº­n vÃ  convolution. CÃ¡c phÃ©p toÃ¡n nÃ y cÃ³ tá»· lá»‡ FLOP-to-byte cao, thÃºc Ä‘áº©y
viá»‡c phÃ¡t triá»ƒn cÃ¡c Ä‘Æ¡n vá»‹ pháº§n cá»©ng chuyÃªn dá»¥ng nhÆ° TensorCore cá»§a Nvidia GPU (Markidis et al., 2018) vÃ 
MXU cá»§a Google TPU (Norrie et al., 2021; Jouppi et al., 2021, 2023). RNN cá»• Ä‘iá»ƒn cÅ©ng hÆ°á»Ÿng lá»£i tá»« Ä‘iá»u nÃ y
do ma tráº­n há»“i quy dÃ y Ä‘áº·c cá»§a chÃºng. NgÆ°á»£c láº¡i, lá»›p RG-LRU Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i, nhÆ° cÃ¡c mÃ´ hÃ¬nh RNN Ä‘Æ°á»ng chÃ©o khÃ¡c,
cÃ³ tá»· lá»‡ FLOP-to-byte tháº¥p. Sá»± khÃ¡c biá»‡t cÆ¡ báº£n nÃ y táº¡o ra thÃ¡ch thá»©c tÃ­nh toÃ¡n,
vÃ¬ cÃ¡c accelerator hiá»‡n táº¡i thiáº¿u tá»‘i Æ°u hÃ³a cho cÃ¡c workload nhÆ° váº­y. VÃ¬ chÃºng tÃ´i cháº¡y táº¥t cáº£ thÃ­ nghiá»‡m trÃªn
TPU-v3, chÃºng tÃ´i táº­p trung vÃ o phÃ¡t triá»ƒn triá»ƒn khai hiá»‡u quáº£ phÃ¹ há»£p vá»›i thiáº¿t bá»‹ nÃ y3.

ThÃ¡ch thá»©c cho RNN Ä‘Æ°á»ng chÃ©o tuyáº¿n tÃ­nh Má»™t trong nhá»¯ng thÃ¡ch thá»©c chÃ­nh cá»§a viá»‡c sá»­ dá»¥ng thiáº¿t bá»‹ nhÆ° TPU-v3
cho RG-LRU lÃ  phÆ°Æ¡ng trÃ¬nh cáº­p nháº­t tráº¡ng thÃ¡i áº©n trong eq.(4) lÃ  phÃ©p toÃ¡n element-wise thuáº§n tÃºy.
Cho má»—i cáº­p nháº­t pháº§n tá»­, nÃ³ yÃªu cáº§u táº£i 6 byte (giáº£ sá»­ bfloat16 cáº§n 2 byte cho má»—i
biáº¿n â„ğ‘¡âˆ’1, ğ‘ğ‘¡, ğ‘¥ğ‘¡) vÃ  ghi 2 byte (tráº¡ng thÃ¡i áº©n â„ğ‘¡) trong khi tÃ­nh toÃ¡n chá»‰ thá»±c hiá»‡n 6 FLOP
(sá»‘ phÃ©p toÃ¡n sá»‘ há»c trong eq.4) cho má»—i pháº§n tá»­. Äiá»u nÃ y dáº«n Ä‘áº¿n tá»· lá»‡ FLOP-to-byte tháº¥p
lÃ  0.75â€“Ä‘Ã¡ng ká»ƒ dÆ°á»›i kháº£ nÄƒng cá»§a thiáº¿t bá»‹ cho cÃ¡c phÃ©p toÃ¡n element-wise lÃ  4.2 (xem Phá»¥ lá»¥c 3).
Thá»i gian thá»±c hiá»‡n do Ä‘Ã³ bá»‹ chi phá»‘i bá»Ÿi viá»‡c truyá»n bá»™ nhá»› giá»¯a HBM vÃ  VMEM, khiáº¿n
tÃ­nh toÃ¡n bá»‹ giá»›i háº¡n bá»Ÿi bá»™ nhá»›.

Má»™t linear scan tÃ¹y chá»‰nh Äá»ƒ giáº£i quyáº¿t Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘Ã£ viáº¿t má»™t kernel Pallas tÃ¹y chá»‰nh cho tÃ­nh toÃ¡n
cá»§a eq.(4) sá»­ dá»¥ng má»™t linear scan. Äiá»u nÃ y cho phÃ©p chÃºng tÃ´i giáº£m thiá»ƒu viá»‡c truyá»n bá»™ nhá»›, báº±ng cÃ¡ch giá»¯ tráº¡ng thÃ¡i áº©n

3CÃ¡c káº¿t luáº­n Ä‘Æ°á»£c rÃºt ra á»Ÿ Ä‘Ã¢y khÃ´ng nháº¥t thiáº¿t Ã¡p dá»¥ng cho cÃ¡c accelerator khÃ¡c.
7

--- TRANG 8 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

2k 4k 8k
Sequence length1.00x1.17x1.46x
0.93x 0.96x 0.98xMQA
Griffin
(a) 400m

2k 4k 8k
Sequence length1.00x1.12x1.33x
0.97x 0.99x 1.01x (b) 1B

2k 4k 8k
Sequence length1.00x1.05x1.13x1.05x 1.07x 1.07x (c) 7B

HÃ¬nh 3|Thá»i gian huáº¥n luyá»‡n má»—i bÆ°á»›c Ä‘Æ°á»£c tÃ­nh tÆ°Æ¡ng Ä‘á»‘i vá»›i baseline MQA cá»§a chÃºng tÃ´i á»Ÿ chiá»u dÃ i chuá»—i 2K khi chÃºng tÃ´i
thay Ä‘á»•i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  chiá»u dÃ i chuá»—i cho Griffin vÃ  MQA. HÃ£y lÆ°u Ã½ ráº±ng khi chÃºng tÃ´i tÄƒng chiá»u dÃ i chuá»—i
chÃºng tÃ´i giáº£m batch size tá»· lá»‡ thuáº­n, sao cho tá»•ng sá»‘ token má»—i batch giá»¯ nguyÃªn cá»‘ Ä‘á»‹nh.

trong VMEM suá»‘t thá»i gian, vÃ  cÅ©ng Ä‘á»ƒ thá»±c hiá»‡n viá»‡c truyá»n bá»™ nhá»› theo tá»«ng khá»‘i lá»›n hÆ¡n thay vÃ¬ tá»«ng cÃ¡i má»™t.
Trong thá»±c táº¿, Ä‘iá»u nÃ y dáº«n Ä‘áº¿n tÄƒng tá»‘c gáº§n 3x so vá»›i triá»ƒn khai Jax native cá»§a linear scan.
NgoÃ i ra, chÃºng tÃ´i quan sÃ¡t 10-20% thá»i gian huáº¥n luyá»‡n tháº¥p hÆ¡n má»—i bÆ°á»›c cá»§a mÃ´ hÃ¬nh Hawk Ä‘áº§y Ä‘á»§, so vá»›i
cÃ¹ng mÃ´ hÃ¬nh sá»­ dá»¥ng triá»ƒn khai Jax native (xem Phá»¥ lá»¥c D.2 Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.)

Táº¡i sao chÃºng tÃ´i khÃ´ng sá»­ dá»¥ng convolution hoáº·c associative scan? Sá»©c háº¥p dáº«n ban Ä‘áº§u cá»§a cÃ¡c mÃ´ hÃ¬nh há»“i quy tuyáº¿n tÃ­nh
xuáº¥t phÃ¡t tá»« kháº£ nÄƒng song song hÃ³a cao, Ä‘Æ°á»£c kÃ­ch hoáº¡t bá»Ÿi tÃ­nh káº¿t há»£p cá»§a cÃ¡c tÃ­nh toÃ¡n. Äiá»u nÃ y
cho phÃ©p thá»±c hiá»‡n hiá»‡u quáº£ trÃªn thiáº¿t bá»‹ thÃ´ng qua convolution (Gu et al., 2021b) hoáº·c thuáº­t toÃ¡n prefix-sum (
associative scan) (Smith et al., 2022). Tuy nhiÃªn, cÆ¡ cháº¿ gating cá»§a RG-LRU trÃªn ğ‘ğ‘¡ khÃ´ng tÆ°Æ¡ng thÃ­ch
vá»›i quan Ä‘iá»ƒm convolutional. Máº·c dÃ¹ chÃºng tÃ´i váº«n cÃ³ thá»ƒ sá»­ dá»¥ng associative scan vá» nguyÃªn táº¯c, associative
scan giáº£m sá»‘ FLOP cáº§n thiáº¿t nhÆ°ng khÃ´ng giáº£m overhead bá»™ nhá»›, Ä‘Ã¢y lÃ  nÃºt tháº¯t cá»• chai chÃ­nh
trong thá»±c táº¿. Vá» máº·t thá»±c nghiá»‡m, chÃºng tÃ´i quan sÃ¡t ráº±ng trÃªn TPU-v3, associative scan cháº­m hÆ¡n Ä‘Ã¡ng ká»ƒ
so vá»›i Jax linear scan native (xem Phá»¥ lá»¥c D.2 Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.) ChÃºng tÃ´i suy Ä‘oÃ¡n ráº±ng tÃ­nh cháº¥t truy cáº­p ngáº«u nhiÃªn
cá»§a viá»‡c tÃ¡i káº¿t há»£p cÃ¢y cá»§a thuáº­t toÃ¡n parallel prefix-sum khiáº¿n nÃ³ khÃ´ng phÃ¹ há»£p vá»›i kiáº¿n trÃºc TPU,
dáº«n Ä‘áº¿n viá»‡c truyá»n bá»™ nhá»› cháº­m hÆ¡nâ€“nÃºt tháº¯t cá»• chai chÃ­nh cá»§a phÃ©p toÃ¡n nÃ y.

4.3. Tá»‘c Ä‘á»™ huáº¥n luyá»‡n trÃªn chuá»—i dÃ i hÆ¡n
ChÃºng tÃ´i so sÃ¡nh tá»‘c Ä‘á»™ huáº¥n luyá»‡n qua cÃ¡c kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh vÃ  chiá»u dÃ i chuá»—i khÃ¡c nhau Ä‘á»ƒ Ä‘iá»u tra
lá»£i Ã­ch tÃ­nh toÃ¡n cá»§a cÃ¡c mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. Cho má»—i kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh, chÃºng tÃ´i giá»¯ tá»•ng sá»‘
token má»—i batch cá»‘ Ä‘á»‹nh, cÃ³ nghÄ©a lÃ  khi chÃºng tÃ´i tÄƒng chiá»u dÃ i chuá»—i, chÃºng tÃ´i giáº£m tá»· lá»‡ thuáº­n
sá»‘ lÆ°á»£ng chuá»—i. Trong HÃ¬nh 3, chÃºng tÃ´i váº½ thá»i gian cháº¡y tÆ°Æ¡ng Ä‘á»‘i cá»§a mÃ´ hÃ¬nh Griffin so vá»›i
baseline MQA á»Ÿ chiá»u dÃ i chuá»—i 2048. á» chiá»u dÃ i chuá»—i tháº¥p nháº¥t, hai mÃ´ hÃ¬nh cÃ³ thá»i gian huáº¥n luyá»‡n
tÆ°Æ¡ng tá»±, nhÆ°ng khi chÃºng tÃ´i tÄƒng chiá»u dÃ i chuá»—i, Transformer trá»Ÿ nÃªn cháº­m hÆ¡n, trong khi thá»i gian cháº¡y cá»§a Griffin
giá»¯ nguyÃªn. Sá»± giáº£m tá»‘c Ä‘á»™ cho baseline rÃµ rÃ ng hÆ¡n á»Ÿ kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh nhá» hÆ¡n vÃ 
giáº£m á»Ÿ kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh lá»›n hÆ¡n. Äiá»u nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch bá»Ÿi thá»±c táº¿ ráº±ng táº¥t cáº£ mÃ´ hÃ¬nh Ä‘á»u chá»©a má»™t sá»‘ lÆ°á»£ng lá»›n
lá»›p tuyáº¿n tÃ­nh. TÃ­nh toÃ¡n cá»§a chÃºng tá»· lá»‡ ğ‘‚(ğ‘‡ğ·2), trong khi RG-LRU lÃ  ğ‘‚(ğ‘‡ğ·) vs ğ‘‚(ğ‘‡2ğ·) cá»§a attention toÃ n cá»¥c.
Äiá»u nÃ y cÃ³ nghÄ©a lÃ  khi chÃºng tÃ´i tÄƒng chiá»u rá»™ng mÃ´ hÃ¬nh ğ· so vá»›i chiá»u dÃ i chuá»—i ğ‘‡, cÃ¡c lá»›p tuyáº¿n tÃ­nh
trá»Ÿ thÃ nh nÃºt tháº¯t cá»• chai tÃ­nh toÃ¡n chÃ­nh, giáº£m thiá»ƒu lá»£i Ã­ch hiá»‡u quáº£ tá»« khá»‘i RNN.

Do Ä‘Ã³, viá»‡c thay tháº¿ Transformer báº±ng Hawk hoáº·c Griffin mang láº¡i cáº£i thiá»‡n thá»i gian wall-clock Ä‘Ã¡ng ká»ƒ nháº¥t
khi chiá»u dÃ i chuá»—i Ä‘á»§ lá»›n so vá»›i chiá»u rá»™ng mÃ´ hÃ¬nh Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh toÃ¡n attention chiáº¿m
má»™t pháº§n lá»›n tá»•ng thá»i gian tÃ­nh toÃ¡n. ChÃºng tÃ´i cÅ©ng lÆ°u Ã½ ráº±ng trong thá»±c táº¿, baseline MQA
8

--- TRANG 9 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

cá»§a chÃºng tÃ´i cÃ³ Ã­t tham sá»‘ hÆ¡n má»™t chÃºt so vá»›i Griffin á»Ÿ cÃ¹ng quy mÃ´ mÃ´ hÃ¬nh (vÃ  thá»±c hiá»‡n Ã­t FLOP hÆ¡n).
Äiá»u nÃ y giáº£i thÃ­ch táº¡i sao Griffin huáº¥n luyá»‡n cháº­m hÆ¡n má»™t chÃºt so vá»›i baseline MQA á»Ÿ 7B cho chuá»—i ngáº¯n.

5. Tá»‘c Ä‘á»™ Suy luáº­n
Suy luáº­n trong LLM bao gá»“m hai giai Ä‘oáº¡n. Trong giai Ä‘oáº¡n "prefill", chÃºng tÃ´i nháº­n vÃ  xá»­ lÃ½ prompt.
BÆ°á»›c nÃ y thá»±c cháº¥t lÃ  thá»±c hiá»‡n forward pass cá»§a mÃ´ hÃ¬nh. VÃ¬ prompt cÃ³ thá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ song song
qua chuá»—i, háº§u háº¿t cÃ¡c phÃ©p toÃ¡n mÃ´ hÃ¬nh bá»‹ giá»›i háº¡n bá»Ÿi tÃ­nh toÃ¡n trong giai Ä‘oáº¡n nÃ y. Do Ä‘Ã³ chÃºng tÃ´i
mong Ä‘á»£i tá»‘c Ä‘á»™ tÆ°Æ¡ng Ä‘á»‘i cá»§a Transformer vÃ  mÃ´ hÃ¬nh há»“i quy trong giai Ä‘oáº¡n prefill tÆ°Æ¡ng tá»±
vá»›i tá»‘c Ä‘á»™ tÆ°Æ¡ng Ä‘á»‘i cá»§a cÃ¹ng mÃ´ hÃ¬nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, mÃ  chÃºng tÃ´i Ä‘Ã£ tháº£o luáº­n trong Pháº§n 4.

Prefill Ä‘Æ°á»£c theo sau bá»Ÿi giai Ä‘oáº¡n "decode", trong Ä‘Ã³ chÃºng tÃ´i láº¥y máº«u token auto-regressively tá»« mÃ´ hÃ¬nh. NhÆ° chÃºng tÃ´i
tháº¥y bÃªn dÆ°á»›i, cÃ¡c mÃ´ hÃ¬nh há»“i quy cÃ³ Ä‘á»™ trá»… tháº¥p hÆ¡n vÃ  thÃ´ng lÆ°á»£ng cao hÆ¡n trong giai Ä‘oáº¡n decode, Ä‘áº·c biá»‡t
cho chiá»u dÃ i chuá»—i dÃ i hÆ¡n nÆ¡i key-value (KV) cache Ä‘Æ°á»£c sá»­ dá»¥ng trong attention cÃ³ thá»ƒ trá»Ÿ nÃªn lá»›n.

CÃ³ hai metric chÃ­nh cáº§n xem xÃ©t khi Ä‘Ã¡nh giÃ¡ tá»‘c Ä‘á»™ suy luáº­n. Thá»© nháº¥t lÃ  Ä‘á»™ trá»…,
Ä‘o thá»i gian cáº§n thiáº¿t Ä‘á»ƒ táº¡o ra má»™t sá»‘ token Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh á»Ÿ má»™t batch size nháº¥t Ä‘á»‹nh. Thá»© hai lÃ 
thÃ´ng lÆ°á»£ng, Ä‘o sá»‘ lÆ°á»£ng token lá»›n nháº¥t má»—i giÃ¢y cÃ³ thá»ƒ Ä‘Æ°á»£c táº¡o ra trÃªn má»™t
thiáº¿t bá»‹ duy nháº¥t khi láº¥y máº«u má»™t sá»‘ token Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh. VÃ¬ thÃ´ng lÆ°á»£ng Ä‘Æ°á»£c tÃ­nh báº±ng token Ä‘Æ°á»£c láº¥y máº«u nhÃ¢n
batch size chia cho Ä‘á»™ trá»…, ngÆ°á»i ta cÃ³ thá»ƒ cáº£i thiá»‡n thÃ´ng lÆ°á»£ng báº±ng cÃ¡ch giáº£m Ä‘á»™ trá»… hoáº·c báº±ng cÃ¡ch giáº£m
viá»‡c sá»­ dá»¥ng bá»™ nhá»› Ä‘á»ƒ cho phÃ©p sá»­ dá»¥ng batch size lá»›n hÆ¡n trÃªn thiáº¿t bá»‹. Äá»™ trá»… cÃ³ thá»ƒ há»¯u Ã­ch Ä‘á»ƒ xem xÃ©t cho
cÃ¡c á»©ng dá»¥ng thá»i gian thá»±c yÃªu cáº§u thá»i gian pháº£n há»“i nhanh. ThÃ´ng lÆ°á»£ng cÅ©ng há»¯u Ã­ch Ä‘á»ƒ xem xÃ©t vÃ¬ nÃ³ cÃ³ thá»ƒ
cho chÃºng ta biáº¿t sá»‘ lÆ°á»£ng token tá»‘i Ä‘a chÃºng ta cÃ³ thá»ƒ láº¥y máº«u tá»« má»™t mÃ´ hÃ¬nh cá»¥ thá»ƒ trong má»™t thá»i gian nháº¥t Ä‘á»‹nh. TÃ­nh cháº¥t
nÃ y há»¯u Ã­ch khi xem xÃ©t cÃ¡c á»©ng dá»¥ng ngÃ´n ngá»¯ khÃ¡c nhÆ° Reinforcement Learning from
Human Feedback (RLHF) hoáº·c cháº¥m Ä‘iá»ƒm Ä‘áº§u ra mÃ´ hÃ¬nh ngÃ´n ngá»¯ nhÆ° Ä‘Æ°á»£c thá»±c hiá»‡n trong AlphaCode (Li et al., 2022)
nÆ¡i kháº£ nÄƒng xuáº¥t ra má»™t sá»‘ lÆ°á»£ng lá»›n token trong má»™t thá»i gian nháº¥t Ä‘á»‹nh lÃ  má»™t tÃ­nh nÄƒng háº¥p dáº«n.

5.1. Má»™t mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n cá»§a bÆ°á»›c decode
Táº¥t cáº£ cÃ¡c thÃ nh pháº§n cá»§a mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘á»u bá»‹ giá»›i háº¡n bá»Ÿi bá»™ nhá»› trong decode miá»…n lÃ  batch size khÃ´ng quÃ¡
lá»›n (tá»©c lÃ  ğµâ‰²128-xem Phá»¥ lá»¥c F.1 Ä‘á»ƒ biáº¿t chi tiáº¿t) vÃ  chÃºng tÃ´i sáº½ giáº£ Ä‘á»‹nh Ä‘iá»u nÃ y cho pháº§n cÃ²n láº¡i cá»§a pháº§n nÃ y.
Overhead bá»™ nhá»› lá»›n nháº¥t cá»§a Transformer thÆ°á»ng Ä‘áº¿n tá»« chÃ­nh cÃ¡c tham sá»‘ vÃ 
cache KV. Do Ä‘Ã³ chÃºng tÃ´i cÃ³ thá»ƒ xáº¥p xá»‰ thá»i gian cáº§n thiáº¿t Ä‘á»ƒ táº¡o ra má»™t token duy nháº¥t cho má»—i chuá»—i
trong batch ğµ trong decode lÃ  thá»i gian cáº§n thiáº¿t Ä‘á»ƒ táº£i hai lÆ°á»£ng nÃ y tá»« bá»™ nhá»›:

Thá»i gian láº¥y máº«u token tiáº¿p theo â‰ˆ kÃ­ch thÆ°á»›c tham sá»‘ + batch size Ã— kÃ­ch thÆ°á»›c cache
bÄƒng thÃ´ng bá»™ nhá»›. (5)

á» Ä‘Ã¢y, kÃ­ch thÆ°á»›c cache Ä‘á» cáº­p Ä‘áº¿n kÃ­ch thÆ°á»›c cache KV á»Ÿ batch size 1 (cho Transformer), hoáº·c
kÃ­ch thÆ°á»›c tráº¡ng thÃ¡i há»“i quy á»Ÿ batch size 1 (cho RNN).

KÃ­ch thÆ°á»›c cache Sá»± khÃ¡c biá»‡t trong kÃ­ch thÆ°á»›c cache so vá»›i tham sá»‘ mÃ´ hÃ¬nh cÃ³ Ã½ nghÄ©a quan trá»ng
cho hiá»‡u quáº£ láº¥y máº«u. Trong cÃ¡c khá»‘i há»“i quy vÃ  attention cá»¥c bá»™, viá»‡c táº£i tham sá»‘ lÃ  nÃºt tháº¯t cá»• chai chÃ­nh,
(vÃ¬ kÃ­ch thÆ°á»›c cache nhá» hÆ¡n Ä‘Ã¡ng ká»ƒ). NgÆ°á»£c láº¡i, cache KV cá»§a attention toÃ n cá»¥c
tá»· lá»‡ vá»›i chiá»u dÃ i chuá»—i ğ‘‡ vÃ  cÃ³ thá»ƒ so sÃ¡nh, hoáº·c tháº­m chÃ­ vÆ°á»£t quÃ¡, kÃ­ch thÆ°á»›c cá»§a tham sá»‘ mÃ´ hÃ¬nh. Äiá»u nÃ y táº¡o ra overhead Ä‘Ã¡ng ká»ƒ khi chiá»u dÃ i chuá»—i ğ‘‡ Ä‘á»§ lá»›n (nhÆ°
tháº¥y trong F.4). Do Ä‘Ã³, má»™t mÃ´ hÃ¬nh há»“i quy cÃ³ kÃ­ch thÆ°á»›c báº±ng nhau cÃ³ thá»ƒ thá»ƒ hiá»‡n Ä‘á»™ trá»… tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ
so vá»›i Transformer khi ğ‘‡ lá»›n. LÆ°u Ã½ tuy nhiÃªn ráº±ng khi kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh tÄƒng, chiá»u dÃ i chuá»—i mÃ 
chÃºng ta tháº¥y lá»£i Ã­ch Ä‘á»™ trá»… (nÆ¡i kÃ­ch thÆ°á»›c cache KV so sÃ¡nh vá»›i kÃ­ch thÆ°á»›c tham sá»‘) cÅ©ng tÄƒng.

Äiá»u quan trá»ng cáº§n lÆ°u Ã½ lÃ , cÅ©ng nhÆ° cáº£i thiá»‡n Ä‘á»™ trá»…, cÃ³ tráº¡ng thÃ¡i há»“i quy nhá» cÅ©ng cÃ³ thá»ƒ tÄƒng
batch size lá»›n nháº¥t fit trong bá»™ nhá»› trÃªn má»™t thiáº¿t bá»‹ duy nháº¥t, dáº«n Ä‘áº¿n thÃ´ng lÆ°á»£ng cao hÆ¡n.
9

--- TRANG 10 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

128 256 512 1024 2048 4096
# Tokens decoded020406080100120140Time taken to sample/s
MQA
Griffin
Hawk
(a) Äá»™ trá»… prefill trá»‘ng

128 256 512 1024 2048 4096
# Tokens decoded050100150Time taken to sample/s
MQA
Griffin
Hawk (b) Äá»™ trá»… prefill 4k

HÃ¬nh 4|Äá»™ trá»… cá»§a cÃ¡c mÃ´ hÃ¬nh 1B tham sá»‘ khÃ¡c nhau cho má»™t pháº¡m vi chiá»u dÃ i chuá»—i cho (a) láº¥y máº«u
tá»« prefill trá»‘ng vÃ  (b) láº¥y máº«u tá»« prefill 4k token.

5.2. Káº¿t quáº£
á» Ä‘Ã¢y, chÃºng tÃ´i xem xÃ©t káº¿t quáº£ suy luáº­n cho cÃ¡c mÃ´ hÃ¬nh cÃ³ kÃ­ch thÆ°á»›c 1B tham sá»‘. Cho baseline cá»§a chÃºng tÃ´i, chÃºng tÃ´i so sÃ¡nh
vá»›i MQA Transformer, nhanh hÆ¡n Ä‘Ã¡ng ká»ƒ trong suy luáº­n so vá»›i MHA Transformer
tiÃªu chuáº©n thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong vÄƒn liá»‡u. CÃ¡c mÃ´ hÃ¬nh mÃ  chÃºng tÃ´i so sÃ¡nh lÃ : i) MQA Transformer, ii)
Hawk, vÃ  iii) Griffin. Äá»ƒ so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau, chÃºng tÃ´i bÃ¡o cÃ¡o cáº£ Ä‘á»™ trá»… vÃ  thÃ´ng lÆ°á»£ng.

Äá»™ trá»… ChÃºng tÃ´i so sÃ¡nh Ä‘á»™ trá»… cho cÃ¡c mÃ´ hÃ¬nh vá»›i batch size 16 vá»›i prefill trá»‘ng cÅ©ng nhÆ°
prefill 4096 token nhÆ° tháº¥y trong HÃ¬nh 4. Hawk vÃ  Griffin Ä‘áº¡t Ä‘á»™ trá»… láº¥y máº«u nhanh hÆ¡n so vá»›i MQA
Transformer cho chuá»—i dÃ i. Äiá»u nÃ y Ä‘áº·c biá»‡t Ä‘Ã¡ng chÃº Ã½ khi chiá»u dÃ i chuá»—i vÃ  chiá»u dÃ i prefill
(áº£nh hÆ°á»Ÿng Ä‘áº¿n kÃ­ch thÆ°á»›c cache KV) Ä‘Æ°á»£c tÄƒng. Griffin Ä‘áº¡t Ä‘á»™ trá»… tÆ°Æ¡ng tá»± nhÆ° Hawk,
thá»ƒ hiá»‡n tÃ­nh tÆ°Æ¡ng thÃ­ch tuyá»‡t vá»i cá»§a há»“i quy tuyáº¿n tÃ­nh vÃ  attention cá»¥c bá»™.

ThÃ´ng lÆ°á»£ng ChÃºng tÃ´i so sÃ¡nh thÃ´ng lÆ°á»£ng tá»‘i Ä‘a (token/s) cho cÃ¹ng mÃ´ hÃ¬nh khi láº¥y máº«u
512, 1024, 2048 vÃ  4196 token theo sau prompt trá»‘ng trong HÃ¬nh 1(b). ChÃºng tÃ´i tháº¥y ráº±ng cáº£ Griffin
vÃ  Hawk Ä‘áº¡t thÃ´ng lÆ°á»£ng cao hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i baseline MQA Transformer. Äiá»u nÃ y má»™t pháº§n
do cÃ¡c mÃ´ hÃ¬nh há»“i quy cÃ³ Ä‘á»™ trá»… tháº¥p hÆ¡n nhÆ°ng cÅ©ng chá»§ yáº¿u xáº£y ra vÃ¬ Griffin vÃ  Hawk cÃ³ thá»ƒ
fit batch size lá»›n hÆ¡n so vá»›i MQA Transformer trÃªn má»™t thiáº¿t bá»‹ duy nháº¥t, vÃ¬ kÃ­ch thÆ°á»›c cache cá»§a chÃºng nhá» hÆ¡n.
Hawk Ä‘áº¡t thÃ´ng lÆ°á»£ng cao hÆ¡n Griffin, vÃ¬ kÃ­ch thÆ°á»›c cache attention cá»¥c bá»™ cuá»‘i cÃ¹ng
trá»Ÿ nÃªn so sÃ¡nh vá»›i kÃ­ch thÆ°á»›c tham sá»‘ khi batch size lá»›n.

6. MÃ´ hÃ¬nh hÃ³a Context DÃ i
Trong pháº§n nÃ y, chÃºng tÃ´i khÃ¡m phÃ¡ hiá»‡u quáº£ cá»§a Hawk vÃ  Griffin trong viá»‡c sá»­ dá»¥ng context dÃ i hÆ¡n Ä‘á»ƒ cáº£i thiá»‡n
dá»± Ä‘oÃ¡n token tiáº¿p theo, vÃ  Ä‘iá»u tra kháº£ nÄƒng ngoáº¡i suy cá»§a chÃºng trong suy luáº­n. NgoÃ i ra,
chÃºng tÃ´i khÃ¡m phÃ¡ hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh trÃªn cÃ¡c tÃ¡c vá»¥ yÃªu cáº§u kháº£ nÄƒng sao chÃ©p vÃ  truy xuáº¥t, cáº£ cho
cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c tÃ¡c vá»¥ nhÆ° váº­y, cÅ©ng nhÆ° khi kiá»ƒm tra cÃ¡c kháº£ nÄƒng nÃ y vá»›i cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Ã£ Ä‘Æ°á»£c pre-train.

6.1. Cáº£i thiá»‡n dá»± Ä‘oÃ¡n token tiáº¿p theo vá»›i context dÃ i hÆ¡n
ChÃºng tÃ´i Ä‘iá»u tra kháº£ nÄƒng cá»§a Hawk vÃ  Griffin cáº£i thiá»‡n dá»± Ä‘oÃ¡n vá»›i context dÃ i hÆ¡n. Cá»¥ thá»ƒ,
chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n báº±ng cÃ¡ch Ä‘o loss trÃªn bá»™ dá»¯ liá»‡u sÃ¡ch held-out qua
má»™t pháº¡m vi chiá»u dÃ i chuá»—i. Sá»­ dá»¥ng cÃ¡c tÃ i liá»‡u dÃ i nÃ y cho phÃ©p chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng cá»§a cÃ¡c mÃ´ hÃ¬nh
10

--- TRANG 11 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

128 256 512 1K 2K 4K 8K 16K 32K
Token position2.42.62.83.03.23.4Mean-so-far NLLGriffin
Hawk
MQA NoPE
MQA RoPE
128 256 512 1K 2K 4K 8K16K 32K 65K131K
Token position2.42.62.83.03.23.4Mean-so-far NLLGriffin-2k
Griffin-8k
Hawk-2k
Hawk-8k

HÃ¬nh 5|Hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh 1B tham sá»‘ khÃ¡c nhau trÃªn bá»™ Ä‘Ã¡nh giÃ¡ held-out cá»§a sÃ¡ch. BÃªn trÃ¡i,
cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i chiá»u dÃ i chuá»—i 2048, vÃ  bÃªn pháº£i vá»›i chiá»u dÃ i chuá»—i
tÆ°Æ¡ng á»©ng 2048 (2k) vÃ  8192 (8k). Hawk vÃ  Griffin cÃ³ thá»ƒ ngoáº¡i suy Ä‘áº¿n cÃ¡c chuá»—i dÃ i hÆ¡n Ä‘Ã¡ng ká»ƒ
so vá»›i cÃ¡c baseline Transformer, vÃ  cáº£i thiá»‡n hiá»‡u suáº¥t thÃªm khi Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn chuá»—i dÃ i hÆ¡n.

ngoáº¡i suy, tá»©c lÃ  kháº£ nÄƒng dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c token tiáº¿p theo vá»›i context dÃ i hÆ¡n so vá»›i
nhá»¯ng gÃ¬ tháº¥y trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

Trong Transformer, kháº£ nÄƒng ngoáº¡i suy nÃ y pháº§n lá»›n Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi positional encoding Ä‘Æ°á»£c sá»­ dá»¥ng cho
cÃ¡c lá»›p attention (Kazemnejad et al., 2024). Äá»‘i vá»›i cÃ¡c mÃ´ hÃ¬nh há»“i quy, nÃ³ thay vÃ o Ä‘Ã³ Ä‘Æ°á»£c chi phá»‘i bá»Ÿi kháº£ nÄƒng
cá»§a mÃ´ hÃ¬nh tiáº¿p tá»¥c tinh chá»‰nh cÃ¡c biá»ƒu diá»…n Ä‘Æ°á»£c lÆ°u trá»¯ trong tráº¡ng thÃ¡i há»“i quy khi context trá»Ÿ nÃªn
dÃ i hÆ¡n. Tá»« biá»ƒu Ä‘á»“ bÃªn trÃ¡i cá»§a HÃ¬nh 5, chÃºng tÃ´i quan sÃ¡t ráº±ng, Ä‘áº¿n má»™t chiá»u dÃ i tá»‘i Ä‘a nÃ o Ä‘Ã³, cáº£ Hawk vÃ 
Griffin Ä‘á»u cáº£i thiá»‡n dá»± Ä‘oÃ¡n token tiáº¿p theo vá»›i context dÃ i hÆ¡n, vÃ  chÃºng cÃ³ thá»ƒ ngoáº¡i suy
Ä‘áº¿n cÃ¡c chuá»—i dÃ i hÆ¡n Ä‘Ã¡ng ká»ƒ (Ã­t nháº¥t 4x dÃ i hÆ¡n) so vá»›i nhá»¯ng gÃ¬ chÃºng Ä‘Æ°á»£c huáº¥n luyá»‡n. Äáº·c biá»‡t, Griffin
ngoáº¡i suy remarkably tá»‘t ngay cáº£ khi sá»­ dá»¥ng RoPE (Su et al., 2021) cho cÃ¡c lá»›p attention cá»¥c bá»™.

Káº¿t quáº£ cho Ä‘áº¿n nay Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn chuá»—i 2048 token. Äá»ƒ Ä‘Ã¡nh giÃ¡
liá»‡u cÃ¡c mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i cÅ©ng cÃ³ thá»ƒ há»c hiá»‡u quáº£ tá»« context dÃ i hÆ¡n, chÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh 1B tham sá»‘ trÃªn
chuá»—i 8192 (8k) token trÃªn Massive Text, vÃ  so sÃ¡nh chÃºng vá»›i cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¹ng bá»™ dá»¯ liá»‡u
nhÆ°ng trÃªn chuá»—i cÃ³ chiá»u dÃ i 2048 (2k) token. ChÃºng tÃ´i giá»¯ tá»•ng sá»‘ token huáº¥n luyá»‡n nhÆ° nhau
qua cÃ¡c mÃ´ hÃ¬nh báº±ng cÃ¡ch giáº£m batch size theo há»‡ sá»‘ 4 cho cÃ¡c mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn chiá»u dÃ i chuá»—i
8192 (trong khi giá»¯ sá»‘ bÆ°á»›c huáº¥n luyá»‡n cá»‘ Ä‘á»‹nh). NhÆ° Ä‘Æ°á»£c minh há»a trong biá»ƒu Ä‘á»“ bÃªn pháº£i cá»§a HÃ¬nh 5,
chÃºng tÃ´i tháº¥y ráº±ng Hawk-8k vÃ  Griffin-8k thá»±c sá»± Ä‘áº¡t loss Ä‘Ã¡nh giÃ¡ tháº¥p hÆ¡n cho chuá»—i cÃ³ chiá»u dÃ i 8192 hoáº·c lá»›n hÆ¡n,
so vá»›i Hawk-2k vÃ  Griffin-2k. Äiá»u nÃ y chá»‰ ra ráº±ng Hawk vÃ  Griffin cÃ³ thá»ƒ há»c sá»­ dá»¥ng context dÃ i hÆ¡n
trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. ThÃº vá»‹, khi Ä‘Ã¡nh giÃ¡ á»Ÿ chiá»u dÃ i chuá»—i ngáº¯n, chÃºng tÃ´i tháº¥y ráº±ng Hawk-2k
vÃ  Griffin-2k hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n má»™t chÃºt so vá»›i Hawk-8k vÃ  Griffin-8k. Äiá»u nÃ y gá»£i Ã½ ráº±ng chiá»u dÃ i chuá»—i huáº¥n luyá»‡n
nÃªn Ä‘Æ°á»£c chá»n cáº©n tháº­n theo má»¥c Ä‘Ã­ch sá»­ dá»¥ng downstream dá»± Ä‘á»‹nh cá»§a mÃ´ hÃ¬nh.

6.2. Kháº£ nÄƒng sao chÃ©p vÃ  truy xuáº¥t
CÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y (Jelassi et al., 2024) Ä‘Ã£ chá»‰ ra ráº±ng Transformer cÃ³ thá»ƒ hiá»‡u quáº£ hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i
state space model (SSM), má»™t há» RNN má»›i phá»• biáº¿n, trong viá»‡c há»c cÃ¡c tÃ¡c vá»¥ tá»•ng há»£p nhÆ° sao chÃ©p
context hoáº·c truy xuáº¥t token liÃªn quan tá»« context. NgoÃ i ra, Jelassi et al. (2024) chá»‰ ra
ráº±ng cÃ¡c Transformer Ä‘Ã£ Ä‘Æ°á»£c pre-train nhÆ° Pythia (Biderman et al., 2023) tá»‘t hÆ¡n nhiá»u trong cÃ¡c tÃ¡c vá»¥ sao chÃ©p vÃ 
truy xuáº¥t táº¡i thá»i Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ so vá»›i cÃ¡c mÃ´ hÃ¬nh SSM Ä‘Ã£ Ä‘Æ°á»£c pre-train nhÆ° Mamba (Gu vÃ  Dao,
2023). Trong pháº§n nÃ y, chÃºng tÃ´i Ä‘iá»u tra hiá»‡u quáº£ cá»§a Griffin vÃ  Hawk trong viá»‡c há»c cÃ¡ch sao chÃ©p vÃ 
truy xuáº¥t token tá»« context. NgoÃ i ra, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Hawk vÃ  Griffin Ä‘Ã£ Ä‘Æ°á»£c pre-train trÃªn
má»™t tÃ¡c vá»¥ tÃ¬m kiáº¿m sá»‘ Ä‘iá»‡n thoáº¡i Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ kiá»ƒm tra cáº£ kháº£ nÄƒng sao chÃ©p vÃ  truy xuáº¥t.
11

--- TRANG 12 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

0 25K 50K 75K 100K
Training Steps0.000.250.500.751.00AccuracyMQA
Hawk
Griffin
(a) TÃ¡c vá»¥ Selective Copying

2729211213
Eval Sequence Length0.000.250.500.751.00Accuracy
MQA
Hawk
Griffin
Train Length (b) TÃ¡c vá»¥ Induction Heads

0 1000 2000 3000
Eval Sequence Length0.000.250.500.751.00AccuracyMQA
Hawk
Griffin
Train Length (c) TÃ¡c vá»¥ Phonebook Lookup

HÃ¬nh 6|KhÃ¡m phÃ¡ kháº£ nÄƒng sao chÃ©p vÃ  truy xuáº¥t cá»§a Hawk vÃ  Griffin trÃªn ba tÃ¡c vá»¥ tá»•ng há»£p.
HÃ¬nh (a) vÃ  (b) hiá»ƒn thá»‹ hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh sÃ¢u 5 lá»›p trÃªn bá»™ eval held out khi Ä‘Æ°á»£c huáº¥n luyá»‡n
rÃµ rÃ ng trÃªn cÃ¡c tÃ¡c vá»¥ nÃ y. HÃ¬nh (c) hiá»ƒn thá»‹ hiá»‡u suáº¥t trÃªn tÃ¡c vá»¥ tÃ¬m kiáº¿m sá»‘ Ä‘iá»‡n thoáº¡i khi
Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Hawk vÃ  Griffin 7B Ä‘Ã£ Ä‘Æ°á»£c pre-train so vá»›i baseline MQA Transformer 6B cá»§a chÃºng tÃ´i.

Huáº¥n luyá»‡n trÃªn cÃ¡c tÃ¡c vá»¥ tá»•ng há»£p Äá»ƒ Ä‘iá»u tra hiá»‡u quáº£ há»c cÃ¡ch sao chÃ©p vÃ  truy xuáº¥t token
liÃªn quan tá»« context, chÃºng tÃ´i huáº¥n luyá»‡n trÃªn hai tÃ¡c vá»¥ tá»•ng há»£p: Selective Copying vÃ  Induction Heads. Äá»ƒ cÃ³ thá»ƒ
so sÃ¡nh Transformer vá»›i Hawk vÃ  Griffin, chÃºng tÃ´i xem xÃ©t cÃ¡c máº¡ng sÃ¢u 5 khá»‘i vá»›i chiá»u mÃ´ hÃ¬nh
64, tá»•ng cá»™ng khoáº£ng 250K tham sá»‘, nÆ¡i Griffin sá»­ dá»¥ng má»™t attention cá»¥c bá»™ duy nháº¥t á»Ÿ
giá»¯a máº¡ng, trong khá»‘i thá»© ba.

â€¢ TÃ¡c vá»¥ selective copying: Trong tÃ¡c vá»¥ nÃ y, mÃ´ hÃ¬nh cáº§n há»c sao chÃ©p cÃ¡c token dá»¯ liá»‡u tá»« má»™t chuá»—i
trong khi bá» qua cÃ¡c token nhiá»…u tá»« context. Xem Phá»¥ lá»¥c H Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t vá» thiáº¿t láº­p cho
tÃ¡c vá»¥ nÃ y. TÃ¡c vá»¥ nÃ y Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« Gu vÃ  Dao (2023), nÆ¡i cÃ¡c tÃ¡c giáº£ chá»‰ ra ráº±ng Mamba
cÃ³ thá»ƒ giáº£i quyáº¿t tÃ¡c vá»¥ nÃ y tá»‘t hÆ¡n cÃ¡c SSM Ä‘Æ°á»£c Ä‘á» xuáº¥t trÆ°á»›c Ä‘Ã¢y. ChÃºng tÃ´i sá»­ dá»¥ng kÃ­ch thÆ°á»›c tá»« vá»±ng
16, vÃ  huáº¥n luyá»‡n trÃªn chuá»—i cÃ³ chiá»u dÃ i 1024, chá»©a 16 token dá»¯ liá»‡u (Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn tá»«
tá»« vá»±ng vÃ  táº¡i cÃ¡c vá»‹ trÃ­ ngáº«u nhiÃªn), vá»›i pháº§n cÃ²n láº¡i cá»§a cÃ¡c token Ä‘Æ°á»£c Ä‘áº·t thÃ nh token nhiá»…u. Griffin
sá»­ dá»¥ng kÃ­ch thÆ°á»›c cá»­a sá»• attention cá»¥c bá»™ 512.

â€¢ Induction heads: Trong tÃ¡c vá»¥ nÃ y, mÃ´ hÃ¬nh cáº§n há»c nhá»› láº¡i token ngay sau
má»™t token Ä‘áº·c biá»‡t. Äiá»u nÃ y yÃªu cáº§u mÃ´ hÃ¬nh há»c token Ä‘áº·c biá»‡t, vÃ  truy xuáº¥t token ngay
sau nÃ³ trong context. Náº¿u mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c tÃ¡c vá»¥, nÃ³ nÃªn cÃ³ thá»ƒ ngoáº¡i suy
Ä‘áº¿n cÃ¡c chuá»—i dÃ i hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i nhá»¯ng gÃ¬ Ä‘Æ°á»£c huáº¥n luyá»‡n. ChÃºng tÃ´i sá»­ dá»¥ng kÃ­ch thÆ°á»›c tá»« vá»±ng 16 vÃ  huáº¥n luyá»‡n
trÃªn chuá»—i cÃ³ chiá»u dÃ i 256 nÆ¡i cÃ¡c token Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn, vÃ  chÃºng tÃ´i láº¥y máº«u ngáº«u nhiÃªn
vá»‹ trÃ­ cá»§a token Ä‘áº·c biá»‡t trong chuá»—i. Griffin sá»­ dá»¥ng cá»­a sá»• attention cá»¥c bá»™ cÃ³ kÃ­ch thÆ°á»›c 128.

ChÃºng tÃ´i hiá»ƒn thá»‹ káº¿t quáº£ trong HÃ¬nh 6. TrÃªn tÃ¡c vá»¥ Selective Copying, chÃºng tÃ´i tháº¥y ráº±ng cáº£ 3 mÃ´ hÃ¬nh Ä‘á»u cÃ³ thá»ƒ
giáº£i quyáº¿t tÃ¡c vá»¥ má»™t cÃ¡ch hoÃ n háº£o. Khi so sÃ¡nh tá»‘c Ä‘á»™ há»c trÃªn tÃ¡c vá»¥ nÃ y, chÃºng tÃ´i tháº¥y Hawk cháº­m hÆ¡n Ä‘Ã¡ng ká»ƒ
so vá»›i Transformer, tÆ°Æ¡ng tá»± nhÆ° quan sÃ¡t Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Jelassi et al. (2024), nÆ¡i cÃ¡c tÃ¡c giáº£
chá»‰ ra ráº±ng Mamba cháº­m hÆ¡n Ä‘Ã¡ng ká»ƒ trong viá»‡c há»c trÃªn cÃ¡c tÃ¡c vá»¥ tÆ°Æ¡ng tá»±. Tuy nhiÃªn thÃº vá»‹, Griffin
háº§u nhÆ° khÃ´ng cÃ³ Ä‘á»™ cháº­m trá»…, hiá»‡u quáº£ khá»›p vá»›i tá»‘c Ä‘á»™ há»c cá»§a Transformer, máº·c dÃ¹ chá»‰ sá»­ dá»¥ng
má»™t lá»›p attention cá»¥c bá»™ duy nháº¥t.

TrÃªn tÃ¡c vá»¥ Induction Heads, trong khi cáº£ 3 mÃ´ hÃ¬nh Ä‘á»u cÃ³ thá»ƒ giáº£i quyáº¿t tÃ¡c vá»¥ má»™t cÃ¡ch hoÃ n háº£o Ä‘áº¿n chiá»u dÃ i chuá»—i huáº¥n luyá»‡n,
baseline Transformer cá»§a chÃºng tÃ´i khÃ´ng thá»ƒ ngoáº¡i suy Ä‘áº¿n chuá»—i dÃ i hÆ¡n trong Ä‘Ã¡nh giÃ¡. Trong khi
baseline MQA cá»§a chÃºng tÃ´i sá»­ dá»¥ng RoPE, Gu vÃ  Dao (2023) cÃ³ quan sÃ¡t tÆ°Æ¡ng tá»± cho Transformer vá»›i má»™t pháº¡m vi
positional encoding. ChÃºng tÃ´i tháº¥y ráº±ng Hawk cÃ³ thá»ƒ ngoáº¡i suy hoÃ n háº£o trÃªn tÃ¡c vá»¥ nÃ y Ä‘áº¿n chuá»—i Ä‘Ã¡nh giÃ¡
dÃ i hÆ¡n nhiá»u báº­c Ä‘á»™ lá»›n so vá»›i chiá»u dÃ i chuá»—i huáº¥n luyá»‡n. ÄÃ¡ng chÃº Ã½, Griffin, vá»›i
attention cá»¥c bá»™ cá»§a nÃ³, cÅ©ng thá»ƒ hiá»‡n kháº£ nÄƒng Ä‘áº·c biá»‡t Ä‘á»ƒ ngoáº¡i suy trÃªn tÃ¡c vá»¥ nÃ y.
12

--- TRANG 13 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

ÄÃ¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c pre-train BÃ¢y giá» chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ liá»‡u kháº£ nÄƒng sao chÃ©p vÃ  truy xuáº¥t cÃ³ tá»± nhiÃªn
xuáº¥t hiá»‡n trong cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c pre-train hay khÃ´ng. ChÃºng tÃ´i xem xÃ©t cÃ¡c mÃ´ hÃ¬nh Hawk vÃ  Griffin 7B
vÃ  baseline MQA Transformer 6B cá»§a chÃºng tÃ´i, táº¥t cáº£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn 300B token trÃªn bá»™ dá»¯ liá»‡u Massive Text.
ChÃºng tÃ´i xem xÃ©t cÃ¹ng tÃ¡c vá»¥ tÃ¬m kiáº¿m phonebook Ä‘Æ°á»£c giá»›i thiá»‡u trong Jelassi et al. (2024), nÆ¡i chÃºng tÃ´i cung cáº¥p cho mÃ´ hÃ¬nh má»™t phonebook tá»•ng há»£p
chá»©a tÃªn vÃ  sá»‘, vÃ  mÃ´ hÃ¬nh Ä‘Æ°á»£c yÃªu cáº§u truy xuáº¥t sá»‘ Ä‘iá»‡n thoáº¡i Ä‘Ãºng cho má»™t tÃªn.
Prompt cho mÃ´ hÃ¬nh lÃ  má»™t phonebook bao gá»“m danh sÃ¡ch tÃªn vÃ  sá»‘ Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn cÃ³
chiá»u dÃ i nháº¥t Ä‘á»‹nh, theo sau bá»Ÿi hai vÃ­ dá»¥ Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn cá»§a tÃ¡c vá»¥, theo sau bá»Ÿi má»™t tÃªn Ä‘Æ°á»£c láº¥y máº«u ngáº«u nhiÃªn
tá»« phonebook mÃ  mÃ´ hÃ¬nh cáº§n truy xuáº¥t sá»‘ Ä‘iá»‡n thoáº¡i Ä‘Ãºng.

Tá»« HÃ¬nh 6(c), chÃºng tÃ´i tháº¥y ráº±ng trong khi Hawk cÃ³ thá»ƒ lÃ m khÃ¡ tá»‘t trÃªn tÃ¡c vá»¥ cho chiá»u dÃ i phonebook ráº¥t ngáº¯n,
nÃ³ tháº¥t báº¡i trong viá»‡c ghi nhá»› vÃ  truy xuáº¥t sá»‘ Ä‘iá»‡n thoáº¡i Ä‘Ãºng khi chiá»u dÃ i phonebook tÄƒng,
tÆ°Æ¡ng tá»± nhÆ° quan sÃ¡t Ä‘Æ°á»£c thá»±c hiá»‡n bá»Ÿi Jelassi et al. (2024) trÃªn hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh Mamba trÃªn tÃ¡c vá»¥ nÃ y.
Äiá»u nÃ y khÃ´ng Ä‘áº·c biá»‡t ngáº¡c nhiÃªn vÃ¬ Hawk sá»­ dá»¥ng tráº¡ng thÃ¡i cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh nhá». Baseline Transformer cá»§a chÃºng tÃ´i cÃ³ thá»ƒ
gáº§n nhÆ° hoÃ n háº£o giáº£i quyáº¿t tÃ¡c vá»¥ nÃ y Ä‘áº¿n chiá»u dÃ i chuá»—i huáº¥n luyá»‡n, nhÆ°ng tháº¥t báº¡i trong viá»‡c truy xuáº¥t sá»‘ Ä‘iá»‡n thoáº¡i Ä‘Ãºng
cho chiá»u dÃ i context dÃ i hÆ¡n chiá»u dÃ i chuá»—i huáº¥n luyá»‡n. ThÃº vá»‹, Griffin cÃ³ thá»ƒ hoÃ n háº£o
giáº£i quyáº¿t tÃ¡c vá»¥ nÃ y Ä‘áº¿n chiá»u dÃ i context khá»›p vá»›i kÃ­ch thÆ°á»›c cá»­a sá»• attention cá»¥c bá»™ 1024,
máº·c dÃ¹ chá»‰ sá»­ dá»¥ng má»™t lá»›p attention cá»¥c bá»™ duy nháº¥t. Má»™t khi chiá»u dÃ i context Ä‘á»§ dÃ i sao cho cá»­a sá»•
attention cá»¥c bá»™ khÃ´ng bao phá»§ toÃ n bá»™ phonebook, hiá»‡u suáº¥t báº¯t Ä‘áº§u giáº£m. Griffin cÅ©ng
cÃ³ thá»ƒ ngoáº¡i suy tá»‘t hÆ¡n Ä‘áº¿n chiá»u dÃ i chuá»—i dÃ i hÆ¡n so vá»›i Transformer. Trong khi hiá»‡u suáº¥t
cá»§a Griffin lÃ  há»©a háº¹n cho kháº£ nÄƒng cá»§a cÃ¡c mÃ´ hÃ¬nh cÃ³ tráº¡ng thÃ¡i cÃ³ kÃ­ch thÆ°á»›c cá»‘ Ä‘á»‹nh giáº£i quyáº¿t cÃ¡c tÃ¡c vá»¥ sao chÃ©p vÃ  truy xuáº¥t,
káº¿t quáº£ cá»§a chÃºng tÃ´i gá»£i Ã½ cáº§n thÃªm cÃ´ng viá»‡c Ä‘á»ƒ cáº£i thiá»‡n cÃ¡c kháº£ nÄƒng nÃ y cho cÃ¡c mÃ´ hÃ¬nh nhÆ° váº­y.

7. CÃ´ng trÃ¬nh LiÃªn quan
Kiáº¿n trÃºc Transformer Ä‘Ã£ trá»Ÿ thÃ nh má»™t lá»±a chá»n thay tháº¿ cÃ³ thá»ƒ má»Ÿ rá»™ng hÆ¡n cho RNN. Transformer Ä‘áº¡t Ä‘Æ°á»£c
kháº£ nÄƒng má»Ÿ rá»™ng vÆ°á»£t trá»™i thÃ´ng qua huáº¥n luyá»‡n song song hoÃ n toÃ n, trÃ¡i ngÆ°á»£c vá»›i cÃ¡c háº¡n cháº¿ vá»‘n cÃ³ cá»§a
RNN. Do cáº¥u trÃºc xá»­ lÃ½ tuáº§n tá»±, RNN cá»• Ä‘iá»ƒn bá»‹ cháº­m tá»‘c Ä‘á»™ huáº¥n luyá»‡n
trong cáº£ forward vÃ  backward propagation (Werbos, 1990). Äá»ƒ giáº£m thiá»ƒu váº¥n Ä‘á» nÃ y, cÃ¡c nhÃ  nghiÃªn cá»©u
Ä‘Ã£ khÃ¡m phÃ¡ cÃ¡c phÆ°Æ¡ng phÃ¡p thay tháº¿ dá»±a trÃªn RNN. CÃ¡c vÃ­ dá»¥ Ä‘Ã¡ng chÃº Ã½ bao gá»“m Quasi-RNN (Bradbury
et al., 2016), káº¿t há»£p convolution vÃ  RNN tuyáº¿n tÃ­nh Ä‘á»ƒ song song hÃ³a lá»›n hÆ¡n, vÃ  viá»‡c sá»­ dá»¥ng
cÃ¡c cÆ¡ cháº¿ gating dá»±a trÃªn Ä‘áº§u vÃ o Ä‘á»ƒ song song hÃ³a huáº¥n luyá»‡n RNN tuyáº¿n tÃ­nh (Martin vÃ  Cundy, 2017).

State-space Model (SSM) gáº§n Ä‘Ã¢y Ä‘Ã£ ná»•i lÃªn nhÆ° má»™t cÃ´ng cá»¥ máº¡nh máº½ Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a chuá»—i Ä‘áº§u vÃ o dÃ i.
ChÃºng thá»ƒ hiá»‡n hiá»‡u suáº¥t máº¡nh trÃªn cÃ¡c tÃ¡c vá»¥ tá»« long-range arena benchmark (Tay et al., 2020),
vÃ  táº¡o Ã¢m thanh (Goel et al., 2022). SSM thÃ nh cÃ´ng tÃ­ch há»£p cÃ¡c khÃ¡i niá»‡m tá»« mÃ´ hÃ¬nh state-space cá»• Ä‘iá»ƒn
(Kalman, 1960) vá»›i nhá»¯ng khÃ¡i niá»‡m cá»§a RNN. Sá»± phá»¥ thuá»™c vÃ o há»“i quy tuyáº¿n tÃ­nh cho phÃ©p
tÃ­nh toÃ¡n tráº¡ng thÃ¡i áº©n hiá»‡u quáº£, thÃ´ng qua cÃ¡c phÃ©p toÃ¡n parallel scan hoáº·c convolution, dáº«n Ä‘áº¿n tá»‘c Ä‘á»™ huáº¥n luyá»‡n
so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh Transformer. MÃ´ hÃ¬nh S4 (Gu et al., 2021a) Ä‘á» xuáº¥t tham sá»‘ hÃ³a tinh vi
Ä‘Æ°á»£c gá»i lÃ  normal plus low-rank Ä‘á»ƒ chÃ©o hÃ³a tÃ­nh toÃ¡n há»“i quy. S4D
tham sá»‘ hÃ³a SSM trá»±c tiáº¿p vá»›i ma tráº­n tráº¡ng thÃ¡i Ä‘Æ°á»ng chÃ©o vÃ  chá»‰ ra ráº±ng nÃ³ hoáº¡t Ä‘á»™ng tá»‘t nhÆ° nhau
trong khi Ä‘Æ¡n giáº£n hÆ¡n nhiá»u (Gu et al., 2022). S5 cÅ©ng chÃ©o hÃ³a há»“i quy, vÃ  chá»‰ ra ráº±ng
há»“i quy cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh báº±ng associative scan (Smith et al., 2022). MÃ´ hÃ¬nh H3 (Dao et al.,
2022b) tá»•ng quÃ¡t hÃ³a diá»…n giáº£i há»“i quy cá»§a linear attention (Katharopoulos et al., 2020). Hyena
(Poli et al., 2023) sá»­ dá»¥ng kiáº¿n trÃºc tÆ°Æ¡ng tá»±, nhÆ°ng thay tháº¿ lá»›p S4D báº±ng kernel convolution toÃ n cá»¥c
Ä‘Æ°á»£c tham sá»‘ hÃ³a bá»Ÿi MLP. RetNet (Sun et al., 2023) sá»­ dá»¥ng thiáº¿t káº¿ SSM Ä‘Æ¡n giáº£n hÆ¡n vá»›i cÆ¡ cháº¿ gating
cho phÃ©p chÃºng song song hÃ³a tÃ­nh toÃ¡n báº±ng má»™t biáº¿n thá»ƒ cá»§a multi-head attention. Orvieto et al.
(2023b) phÃ¢n tÃ­ch vÃ  ablate má»™t cÃ¡ch há»‡ thá»‘ng nhiá»u sá»­a Ä‘á»•i cho RNN tiÃªu chuáº©n. PhÃ¡t hiá»‡n cá»§a há»
chá»‰ ra ráº±ng thÃ´ng qua tham sá»‘ hÃ³a vÃ  khá»Ÿi táº¡o tá»‘t hÆ¡n, RNN tuyáº¿n tÃ­nh Ä‘Æ¡n giáº£n hÃ³a (LRU), hoáº¡t Ä‘á»™ng
tá»‘t nhÆ° cÃ¡c biáº¿n thá»ƒ SSM khÃ¡c trÃªn cÃ¡c tÃ¡c vá»¥ long-range khÃ¡c nhau. RWKV (Peng et al., 2023) lÃ  má»™t
RNN gáº§n Ä‘Ã¢y, Ä‘Æ°á»£c chá»‰ ra lÃ  cáº¡nh tranh trÃªn cÃ¡c tÃ¡c vá»¥ mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯, dá»±a trÃªn má»™t xáº¥p xá»‰ linear attention khÃ¡c
Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« attention-free Transformer (Zhai et al., 2021). Äá»“ng thá»i vá»›i cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i, Gu vÃ  Dao
13

--- TRANG 14 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

(2023) phÃ¡t triá»ƒn kiáº¿n trÃºc SSM gá»i lÃ  Mamba vá»›i cÆ¡ cháº¿ selection phá»¥ thuá»™c Ä‘áº§u vÃ o
vÃ  chá»‰ ra ráº±ng nÃ³ Ä‘áº¡t hiá»‡u suáº¥t so sÃ¡nh vá»›i Transformer vá»›i suy luáº­n hiá»‡u quáº£. Má»™t sá»‘
má»Ÿ rá»™ng cá»§a Mamba Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t (Wang et al., 2024; Zhu et al., 2024) cho cÃ¡c á»©ng dá»¥ng khÃ¡c nhau.
Má»™t gating phá»¥ thuá»™c Ä‘áº§u vÃ o tÆ°Æ¡ng tá»± nhÆ° Mamba cÅ©ng Ä‘Æ°á»£c Ä‘á» xuáº¥t bá»Ÿi Gateloop (Katsch, 2023).

Linear attention (Katharopoulos et al., 2020) cung cáº¥p xáº¥p xá»‰ hiá»‡u quáº£ tÃ­nh toÃ¡n cá»§a
cÆ¡ cháº¿ self-attention báº±ng cÃ¡ch tuyáº¿n tÃ­nh hÃ³a attention, cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh há»“i quy nhÆ° má»™t
RNN tuyáº¿n tÃ­nh. Trong khi cÃ¡ch tiáº¿p cáº­n nÃ y giáº£m Ä‘Ã¡ng ká»ƒ chi phÃ­ tÃ­nh toÃ¡n so vá»›i attention Ä‘áº§y Ä‘á»§, nÃ³ thÆ°á»ng
Ä‘i kÃ¨m vá»›i sá»± Ä‘Ã¡nh Ä‘á»•i vá» hiá»‡u suáº¥t mÃ´ hÃ¬nh. FlashAttention (Dao et al., 2022a) cáº£i thiá»‡n tá»‘c Ä‘á»™ huáº¥n luyá»‡n
cá»§a attention trÃªn GPU báº±ng cÃ¡ch sá»­ dá»¥ng hiá»‡u quáº£ phÃ¢n cáº¥p bá»™ nhá»›. Má»™t cÃ¡ch tiáº¿p cáº­n khÃ¡c Ä‘á»ƒ
giáº£m chi phÃ­ tÃ­nh toÃ¡n cá»§a attention toÃ n cá»¥c, Ä‘ang trá»Ÿ nÃªn ngÃ y cÃ ng phá»• biáº¿n hÆ¡n, lÃ 
sá»­ dá»¥ng sparse-local attention (Child et al., 2019) hoáº·c sliding window attention (Jiang et al., 2023).

8. Káº¿t luáº­n
CÃ´ng trÃ¬nh nÃ y giá»›i thiá»‡u Hawk; má»™t mÃ´ hÃ¬nh há»“i quy káº¿t há»£p lá»›p há»“i quy tuyáº¿n tÃ­nh cÃ³ cá»•ng má»›i, RG-LRU.
ChÃºng tÃ´i cÅ©ng giá»›i thiá»‡u Griffin; má»™t mÃ´ hÃ¬nh lai káº¿t há»£p lá»›p RG-LRU vá»›i attention cá»¥c bá»™.
CÃ¡c mÃ´ hÃ¬nh nÃ y thá»ƒ hiá»‡n hiá»‡u suáº¥t mÃ´ hÃ¬nh hÃ³a ngÃ´n ngá»¯ Ä‘áº·c biá»‡t qua cÃ¡c quy mÃ´ khÃ¡c nhau, vá»›i
loss held-out thá»ƒ hiá»‡n má»Ÿ rá»™ng power-law khi tÃ i nguyÃªn tÃ­nh toÃ¡n tÄƒng. Hawk vÆ°á»£t qua hiá»‡u suáº¥t Ä‘Æ°á»£c bÃ¡o cÃ¡o
cá»§a Mamba trÃªn cÃ¡c tÃ¡c vá»¥ downstream khi Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t ná»­a sá»‘ token, trong khi Griffin hÆ¡i
vÆ°á»£t qua hiá»‡u suáº¥t cá»§a Llama-2 khi Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn Ã­t hÆ¡n 6 láº§n sá»‘ token. HÆ¡n ná»¯a, chÃºng tÃ´i
xÃ¡c thá»±c thá»±c nghiá»‡m cÃ¡c lá»£i Ã­ch thá»i gian suy luáº­n cá»§a Hawk vÃ  Griffin vÃ  quan sÃ¡t giáº£m Ä‘á»™ trá»…
vÃ  tÄƒng thÃ´ng lÆ°á»£ng Ä‘Ã¡ng ká»ƒ so vá»›i cÃ¡c baseline Transformer cá»§a chÃºng tÃ´i. Cuá»‘i cÃ¹ng, Hawk vÃ  Griffin
thá»ƒ hiá»‡n kháº£ nÄƒng ngoáº¡i suy trÃªn cÃ¡c chuá»—i dÃ i hÆ¡n so vá»›i nhá»¯ng gÃ¬ chÃºng Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ  cÃ³ kháº£ nÄƒng
há»c hiá»‡u quáº£ Ä‘á»ƒ sao chÃ©p vÃ  truy xuáº¥t dá»¯ liá»‡u qua khoáº£ng cÃ¡ch dÃ i. Nhá»¯ng phÃ¡t hiá»‡n nÃ y máº¡nh máº½ gá»£i Ã½ ráº±ng cÃ¡c
mÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘á» xuáº¥t cá»§a chÃºng tÃ´i cung cáº¥p má»™t lá»±a chá»n thay tháº¿ máº¡nh máº½ vÃ  hiá»‡u quáº£ cho Transformer vá»›i attention toÃ n cá»¥c.

Lá»i cáº£m Æ¡n
ChÃºng tÃ´i cáº£m Æ¡n Adam Paszke, Sharad Vikram, Trevor Gale, Sebastian Borgeaud, George Scrivener, Raia
Hadsell, Oriol Vinyals, Toby Boyd, Zhifeng Chen, Chris Dyer, Kelvin Xu, Andriy Mnih vÃ¬ sá»± hÆ°á»›ng dáº«n
vÃ  lá»i khuyÃªn cá»§a há». ChÃºng tÃ´i sá»­ dá»¥ng há»‡ sinh thÃ¡i DeepMind Jax (Bradbury et al., 2018) vÃ  Ä‘áº·c biá»‡t cáº£m Æ¡n
Andy Brock vÃ¬ Ä‘Ã£ xÃ¢y dá»±ng framework ná»™i bá»™ mÃ  chÃºng tÃ´i sá»­ dá»¥ng Ä‘á»ƒ huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c mÃ´ hÃ¬nh.

TÃ i liá»‡u tham kháº£o
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,
S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473, 2014.

I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint
arXiv:2004.05150, 2020.

S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit,
U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training
and scaling. In International Conference on Machine Learning, pages 2397â€“2430. PMLR, 2023.

J. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. arXiv preprint
arXiv:1611.01576, 2016.
14

--- TRANG 15 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-
derPlas, S. Wanderman-Milne, vÃ  Q. Zhang. JAX: cÃ¡c biáº¿n Ä‘á»•i cÃ³ thá»ƒ káº¿t há»£p cá»§a chÆ°Æ¡ng trÃ¬nh Python+NumPy, 2018. URL http://github.com/google/jax.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing
Systems, volume 33, pages 1877â€“1901, 2020.

R. Child, S. Gray, A. Radford, vÃ  I. Sutskever. Generating long sequences with sparse transformers.
arXiv preprint arXiv:1904.10509, 2019.

J. Chung, C. Gulcehre, K. Cho, vÃ  Y. Bengio. Empirical evaluation of gated recurrent neural networks
on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

T. Dao, D. Fu, S. Ermon, A. Rudra, vÃ  C. RÃ©. Flashattention: Fast and memory-efficient exact
attention with io-awareness. In Advances in Neural Information Processing Systems, volume 35, pages
16344â€“16359, 2022a.

T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, vÃ  C. RÃ©. Hungry hungry hippos: Towards
language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022b.

Y. N. Dauphin, A. Fan, M. Auli, vÃ  D. Grangier. Language modeling with gated convolutional networks.
In International Conference on Machine Learning, pages 933â€“941. PMLR, 2017.

J. L. Elman. Finding structure in time. Cognitive Science, 14(2):179â€“211, 1990.

Gemini Team Google. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805, 2023.

K. Goel, A. Gu, C. Donahue, vÃ  C. RÃ©. It's raw! audio generation with state-space models. In
International Conference on Machine Learning, pages 7616â€“7633, 2022.

A. Gu vÃ  T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint
arXiv:2312.00752, 2023.

A. Gu, T. Dao, S. Ermon, A. Rudra, vÃ  C. RÃ©. Hippo: Recurrent memory with optimal polynomial
projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474â€“1487, 2020.

A. Gu, K. Goel, vÃ  C. RÃ©. Efficiently modeling long sequences with structured state spaces. arXiv
preprint arXiv:2111.00396, 2021a.

A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, vÃ  C. RÃ©. Combining recurrent, convolutional,
and continuous-time models with linear state space layers. In Advances in Neural Information
Processing Systems, volume 34, pages 572â€“585, 2021b.

A. Gu, A. Gupta, K. Goel, vÃ  C. RÃ©. On the parameterization and initialization of diagonal state space
models. arXiv preprint arXiv:2206.11893, 2022.

D. Hendrycks vÃ  K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.

S. Hochreiter vÃ  J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735â€“1780,
1997.

J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint
arXiv:2203.15556, 2022.
15

--- TRANG 16 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

S. Jelassi, D. Brandfonbrener, S. M. Kakade, vÃ  E. Malach. Repeat after me: Transformers are better
than state space models at copying. arXiv preprint arXiv:2402.01032, 2024.

A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel,
G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

N. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai, N. Patil, S. Subramanian, A. Swing, B. Towles,
et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware
support for embeddings. In Proceedings of the 50th Annual International Symposium on Computer
Architecture, pages 1â€“14, 2023.

N. P. Jouppi, D. H. Yoon, M. Ashcraft, M. Gottscho, T. B. Jablin, G. Kurian, J. Laudon, S. Li, P. Ma, X. Ma,
et al. Ten lessons from three generations shaped google's tpuv4i: Industrial product. In 2021 ACM/IEEE
48th Annual International Symposium on Computer Architecture (ISCA), pages 1â€“14. IEEE, 2021.

R. E. Kalman. A new approach to linear filtering and prediction problems. Journal of Basic Engineering,
82, 1960.

J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, vÃ 
D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

A. Katharopoulos, A. Vyas, N. Pappas, vÃ  F. Fleuret. Transformers are RNNs: Fast autoregressive
transformers with linear attention. In International Conference on Machine Learning, pages
5156â€“5165. PMLR, 2020.

T. Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv preprint
arXiv:2311.01927, 2023.

A. Kazemnejad, I. Padhi, K. Natesan Ramamurthy, P. Das, vÃ  S. Reddy. The impact of positional encoding
on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024.

Y. LeCun, L. Bottou, G. B. Orr, vÃ  K.-R. MÃ¼ller. Efficient backprop. In Neural Networks: Tricks of the
Trade, pages 9â€“50. Springer, 2002.

Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno,
A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):
1092â€“1097, 2022.

I. Loshchilov vÃ  F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,
2017.

S. Markidis, S. W. Der Chien, E. Laure, I. B. Peng, vÃ  J. S. Vetter. Nvidia tensor core programmability,
performance & precision. In 2018 IEEE international parallel and distributed processing symposium
workshops (IPDPSW), pages 522â€“531. IEEE, 2018.

E. Martin vÃ  C. Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint
arXiv:1709.04057, 2017.

H. Mehta, A. Gupta, A. Cutkosky, vÃ  B. Neyshabur. Long range language modeling via gated state
spaces. arXiv preprint arXiv:2206.13947, 2022.

T. Mikolov, M. KarafiÃ¡t, L. Burget, J. CernockÃ½, vÃ  S. Khudanpur. Recurrent neural network based
language model. In INTERSPEECH 11th Annual Conference of the International Speech Communication
Association, pages 1045â€“1048, 2010.
16

--- TRANG 17 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand,
P. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efficient large-scale language model training on
gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, pages 1â€“15, 2021.

T. Norrie, N. Patil, D. H. Yoon, G. Kurian, S. Li, J. Laudon, C. Young, N. Jouppi, vÃ  D. Patterson. The
design process for Google's training chips: TPUv2 and TPUv3. IEEE Micro, 41(2):56â€“63, 2021.

A. Orvieto, S. De, C. Gulcehre, R. Pascanu, vÃ  S. L. Smith. On the universality of linear recurrences
followed by nonlinear projections. arXiv preprint arXiv:2307.11888, 2023a.

A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, vÃ  S. De. Resurrecting recurrent
neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023b.

B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K.
GV, et al. Rwkv: Reinventing RNNs for the transformer era. arXiv preprint arXiv:2305.13048, 2023.

M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, vÃ  C. RÃ©. Hyena
hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023.

J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, et al. Scaling language models: Methods, analysis & insights from training Gopher. arXiv
preprint arXiv:2112.11446, 2021.

S. Rajbhandari, J. Rasley, O. Ruwase, vÃ  Y. He. Zero: Memory optimizations toward training trillion
parameter models. In SC20: International Conference for High Performance Computing, Networking,
Storage and Analysis, pages 1â€“16. IEEE, 2020.

N. Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150,
2019.

N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.

M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, vÃ  B. Catanzaro. Megatron-lm: Training multi-
billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.

H. T. Siegelmann vÃ  E. D. Sontag. Turing computability with neural nets. Applied Mathematics Letters,
4(6):77â€“80, 1991. ISSN 0893-9659.

J. T. Smith, A. Warrington, vÃ  S. W. Linderman. Simplified state space layers for sequence modeling.
arXiv preprint arXiv:2208.04933, 2022.

J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, vÃ  Y. Liu. Roformer: Enhanced transformer with rotary
position embedding. arXiv preprint arXiv:2104.09864, 2021.

Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, vÃ  F. Wei. Retentive network: A successor
to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.

I. Sutskever, O. Vinyals, vÃ  Q. V. Le. Sequence to sequence learning with neural networks. In Advances
in Neural Information Processing Systems, pages 3104â€“3112, 2014.

Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, vÃ  D. Metzler.
Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020.
17

--- TRANG 18 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃ¨re, N. Goyal,
E. Hambro, F. Azhar, et al. LLama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971, 2023.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, vÃ  I. Polosukhin.
Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.

J. Wang, T. Gangavarapu, J. N. Yan, vÃ  A. M. Rush. Mambabyte: Token-free selective state space
model. arXiv preprint arXiv:2401.13660, 2024.

P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE,
78(10):1550â€“1560, 1990.

Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,
et al. Google's neural machine translation system: Bridging the gap between human and machine
translation. arXiv preprint arXiv:1609.08144, 2016.

R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, vÃ  T. Liu. On layer
normalization in the transformer architecture. In International Conference on Machine Learning,
pages 10524â€“10533. PMLR, 2020.

S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, vÃ  J. Susskind. An attention free
transformer. arXiv preprint arXiv:2105.14103, 2021.

B. Zhang vÃ  R. Sennrich. Root mean square layer normalization. Advances in Neural Information
Processing Systems, 32, 2019.

L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, vÃ  X. Wang. Vision mamba: Efficient visual representation
learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024.
18

--- TRANG 19 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

A. RG-LRU Recurrence Gate
Trong HÃ¬nh 7, chÃºng tÃ´i thá»ƒ hiá»‡n hÃ nh vi cá»§a cÃ¡c cÆ¡ cháº¿ gating khÃ¡c nhau Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn trá»ng sá»‘ há»“i quy
ğ‘.

0.0 0.2 0.4 0.6 0.8 1.0
rt0.00.20.40.60.81.0+
a=0.5
RG-LRU
Mamba
GRU
LRU update
0.0 0.2 0.4 0.6 0.8 1.0
rt0.00.20.40.60.81.0+
a=0.9
RG-LRU
Mamba
GRU
LRU update
0.0 0.2 0.4 0.6 0.8 1.0
rt0.00.20.40.60.81.0+
a=0.99
RG-LRU
Mamba
GRU
LRU update

HÃ¬nh 7|HÃ nh vi cá»§a cÃ¡c cÆ¡ cháº¿ gating khÃ¡c nhau Ä‘Æ°á»£c Ã¡p dá»¥ng trÃªn trá»ng sá»‘ há»“i quy ğ‘ (lÆ°u Ã½ ráº±ng
trong kÃ½ hiá»‡u cá»§a Mamba Ä‘Ã¢y lÃ  âˆ’ğ´).

Triá»ƒn khai ChÃºng tÃ´i triá»ƒn khai recurrence gate cá»§a chÃºng tÃ´i, nhÆ° Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong Pháº§n 2.4, theo má»™t hÃ¬nh thá»©c hÆ¡i khÃ¡c,
nhÆ°ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá» máº·t toÃ¡n há»c, Ä‘á»ƒ á»•n Ä‘á»‹nh sá»‘. Cá»¥ thá»ƒ, chÃºng tÃ´i tÃ­nh logarithm
cá»§a ğ‘ğ‘¡ vÃ  sau Ä‘Ã³ mÅ© hÃ³a nÃ³, thay vÃ¬ tÃ­nh sigmoid vÃ  sau Ä‘Ã³ láº¥y lÅ©y thá»«a:

logğ‘ğ‘¡=logğ‘ğ‘ğ‘Ÿğ‘¡=logğœ(Î›)ğ‘ğ‘Ÿğ‘¡=âˆ’ğ‘softplus(Î›)âŠ™ğ‘Ÿğ‘¡. (6)

HÃ nh vi gate Gate cá»§a chÃºng tÃ´i khÃ¡ khÃ¡c so vá»›i cÃ¡c gate tiÃªu chuáº©n khÃ¡c trong vÄƒn liá»‡u. Cá»¥ thá»ƒ,
háº§u háº¿t cÃ¡c cÆ¡ cháº¿ gating, nhÆ° Ä‘Æ°á»£c sá»­ dá»¥ng trong Mamba vÃ  GRU, cho phÃ©p gate ná»™i suy
hoÃ n toÃ n giá»¯a tráº¡ng thÃ¡i áº©n vÃ  quan sÃ¡t má»›i. CÃ²n cá»§a chÃºng tÃ´i máº·t khÃ¡c thiÃªn vá»
viá»‡c giá»¯ láº¡i thÃ´ng tin, vÃ  khÃ´ng cho phÃ©p loáº¡i bá» hoÃ n toÃ n Ä‘Ã³ng gÃ³p cá»§a â„ğ‘¡âˆ’1 (Ä‘iá»u nÃ y phá»¥ thuá»™c, tuy nhiÃªn,
vÃ o giÃ¡ trá»‹ cá»§a Î›). Äá»ƒ thá»ƒ hiá»‡n Ä‘iá»u nÃ y, chÃºng tÃ´i phÃ¢n tÃ­ch trá»ng sá»‘ tÆ°Æ¡ng Ä‘á»‘i cá»§a ğ‘¥ğ‘¡ so vá»›i â„ğ‘¡âˆ’1 trong
Ä‘áº§u ra ğ‘¦ğ‘¡. Cho má»™t há»“i quy tá»•ng quÃ¡t, chÃºng tÃ´i Ä‘á»‹nh nghÄ©a Ä‘iá»u nÃ y lÃ :

â„ğ‘¡=ğ›¼(ğ‘Ÿğ‘¡)â„ğ‘¡âˆ’1+ğ›½(ğ‘Ÿğ‘¡)ğ‘¥ğ‘¡. (7)

Cho mÃ´ hÃ¬nh cá»§a chÃºng tÃ´i, chÃºng tÃ´i cÃ³ ğ›¼(ğ‘Ÿğ‘¡)=ğ‘ğ‘¡=ğ‘ğ‘ğ‘Ÿğ‘¡ vÃ  ğ›½(ğ‘Ÿğ‘¡)=âˆšï¸
1âˆ’ğ›¼(ğ‘Ÿğ‘¡)2. Cho gating kiá»ƒu GRU tiÃªu chuáº©n, chÃºng tÃ´i cÃ³
ğ›¼(ğ‘Ÿğ‘¡)=1âˆ’ğ‘Ÿğ‘¡ vÃ  ğ›½(ğ‘Ÿğ‘¡)=ğ‘Ÿğ‘¡. Cho Mamba, giáº£ sá»­ trong kÃ½ hiá»‡u cá»§a há» ğµ=1,ğ¶=1, thÃ¬ ğ›¼(ğ‘Ÿğ‘¡)=(1âˆ’ğ‘Ÿğ‘¡)âˆ’ğ´
vÃ  ğ›½(ğ‘Ÿğ‘¡)=(1âˆ’ğ›¼)/ğ´. HÃ nh vi cá»§a cÃ¡c cÆ¡ cháº¿ gating khÃ¡c nhau Ä‘Æ°á»£c mÃ´ táº£ trong HÃ¬nh 7, nÆ¡i
Ä‘á»ƒ rÃµ rÃ ng chÃºng tÃ´i cÅ©ng Ä‘Ã£ bao gá»“m giÃ¡ trá»‹ cáº­p nháº­t cá»§a LRU (Orvieto et al., 2023b), khÃ´ng cÃ³
gating. NhÆ° cÃ³ thá»ƒ tháº¥y, gating Mamba gáº§n nhÆ° giá»‘ng há»‡t vá»›i GRU cho cÃ¡c giÃ¡ trá»‹ cá»§a ğ´ gáº§n 1, vá»›i
Ä‘á»™ lá»‡ch nhá» á»Ÿ cÃ¡c giÃ¡ trá»‹ nhá» hÆ¡n. Máº·t khÃ¡c, cÆ¡ cháº¿ gating cá»§a chÃºng tÃ´i thá»±c hiá»‡n má»™t
ná»™i suy phi tuyáº¿n ráº¥t khÃ¡c giá»¯a viá»‡c loáº¡i bá» hoÃ n toÃ n Ä‘áº§u vÃ o ğ‘¥ğ‘¡ vÃ  cáº­p nháº­t cá»§a LRU.

B. Complex-Gated Linear Recurrent Unit (CG-LRU)
Trong Pháº§n 2.4 chÃºng tÃ´i Ä‘Ã£ Ä‘á»‹nh nghÄ©a lá»›p há»“i quy cá»§a chÃºng tÃ´i, tuy nhiÃªn nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c má»Ÿ rá»™ng thÃªm Ä‘á»ƒ sá»­ dá»¥ng
sá»‘ phá»©c. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y, trÆ°á»›c tiÃªn chÃºng tÃ´i tham sá»‘ hÃ³a há»“i quy Ä‘Æ°á»ng chÃ©o phá»©c thÃ´ng qua Ëœğ‘=ğœ(Î›)ğ‘’ğ‘–ğœƒ, trong Ä‘Ã³
ğœƒ lÃ  má»™t tham sá»‘ cÃ³ thá»ƒ há»c. NgoÃ i ra, chÃºng tÃ´i chia Ä‘áº§u vÃ o ğ‘¥ğ‘¡ theo chiá»u kÃªnh, vÃ  diá»…n giáº£i
19

--- TRANG 20 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

ná»­a Ä‘áº§u cá»§a nÃ³ nhÆ° pháº§n thá»±c cá»§a má»™t vector phá»©c, vÃ  pháº§n thá»© hai nhÆ° pháº§n áº£o cá»§a cÃ¹ng
vector phá»©c:

ğ‘¥ğ‘¡=
ğ‘¥1
ğ‘¡
ğ‘¥2
ğ‘¡
(8)

Ëœğ‘¥ğ‘¡=ğ‘¥1
ğ‘¡+ğ‘–ğ‘¥2
ğ‘¡. (9)

Vá»›i Ä‘iá»u nÃ y chÃºng tÃ´i viáº¿t láº¡i cÃ¡c phÆ°Æ¡ng trÃ¬nh cho LRU (xem eq.4) lÃ :

ğ‘Ÿğ‘¡=ğœ(ğ‘Šğ‘ğ‘¥ğ‘¡+ğ‘ğ‘),recurrence gate (10)
ğ‘–ğ‘¡=ğœ(ğ‘Šğ‘¥ğ‘¥ğ‘¡+ğ‘ğ‘¥),input gate (11)
Ëœğ‘ğ‘¡=Ëœğ‘ğ‘ğ‘Ÿğ‘¡, (12)
Ëœâ„ğ‘¡=Ëœğ‘ğ‘¡âŠ™Ëœâ„ğ‘¡âˆ’1+âˆšï¸ƒ
1âˆ’|Ëœğ‘ğ‘¡|2âŠ™(ğ‘–ğ‘¡âŠ™Ëœğ‘¥ğ‘¡). (13)

ChÃºng tÃ´i Ä‘Ã¡nh dáº¥u táº¥t cáº£ cÃ¡c biáº¿n phá»©c vá»›i ËœÂ· Ä‘á»ƒ rÃµ rÃ ng. LÆ°u Ã½ ráº±ng sá»‘ chiá»u cá»§a ğ‘Ÿğ‘¡,ğ‘–ğ‘¡,Ëœğ‘ğ‘¡ vÃ  Ëœâ„ğ‘¡
lÃ  má»™t ná»­a so vá»›i Ä‘áº§u vÃ o thá»±c ğ‘¥ğ‘¡. Cuá»‘i cÃ¹ng, Ä‘á»ƒ tÃ­nh Ä‘áº§u ra chÃºng tÃ´i xáº¿p chá»“ng pháº§n thá»±c vÃ  áº£o
cá»§a â„ğ‘¡ thÃ nh má»™t vector duy nháº¥t ğ‘¦ğ‘¡:

ğ‘¦ğ‘¡=Real(Ëœâ„ğ‘¡)
Imaginary(Ëœâ„ğ‘¡)
(14)

C. SiÃªu tham sá»‘ Quy mÃ´ MÃ´ hÃ¬nh
Trong Báº£ng 2, chÃºng tÃ´i trÃ¬nh bÃ y cÃ¡c siÃªu tham sá»‘ cá»§a cÃ¡c mÃ´ hÃ¬nh á»Ÿ cÃ¡c quy mÃ´ khÃ¡c nhau. CÃ¡c siÃªu tham sá»‘ nÃ y
Ä‘Æ°á»£c chia sáº» cho táº¥t cáº£ cÃ¡c há» mÃ´ hÃ¬nh mÃ  chÃºng tÃ´i khÃ¡m phÃ¡ trong bÃ i bÃ¡o nÃ y.

Báº£ng 2|CÃ¡c siÃªu tham sá»‘ mÃ´ hÃ¬nh chÃ­nh Ä‘Æ°á»£c xem xÃ©t cho cÃ¡c kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh khÃ¡c nhau. CÃ¡c siÃªu tham sá»‘ nÃ y
Ä‘Æ°á»£c chia sáº» qua cÃ¡c kiáº¿n trÃºc khÃ¡c nhau mÃ  chÃºng tÃ´i Ä‘Ã£ kiá»ƒm tra.

Model ModelWidth RNNWidth Depth MLPExpansion Attention TrainingTokens
Size (ğ‘«) ( ğ‘«ğ‘¹ğ‘µğ‘µ )(ğ‘µ)Factor( ğ‘´) Heads (OptimalScaling)
100M 768 1024 12 3 6 1.9B
200M 1024 1536 12 3 8 3.9B
400M 1536 2048 12 3 12 7.8B
1.3B 2048 2560 24 3 16 25B
3B 3072 4096 24 3 24 60B
7B 4096 5632 32 3 32 132.5B
14B 5120 8192 40 3 40 300B

D. Há»“i quy Tuyáº¿n tÃ­nh Hiá»‡u quáº£ trÃªn Thiáº¿t bá»‹
BÆ°á»›c Ä‘áº§u tiÃªn trong tá»‘i Æ°u hÃ³a tÃ­nh toÃ¡n náº±m á»Ÿ viá»‡c xÃ¡c Ä‘á»‹nh nÃºt tháº¯t cá»• chai hiá»‡u suáº¥t chÃ­nh
trÃªn pháº§n cá»©ng má»¥c tiÃªu. Äá»‘i vá»›i háº§u háº¿t cÃ¡c accelerator, cÃ¡c yáº¿u tá»‘ háº¡n cháº¿ chÃ­nh lÃ  thÃ´ng lÆ°á»£ng tÃ­nh toÃ¡n
(FLOP/s) vÃ  bÄƒng thÃ´ng bá»™ nhá»› giá»¯a high-bandwidth memory (HBM) vÃ  fast vector
memory (VMEM). Trong khi cÃ¡c yáº¿u tá»‘ nhÆ° dung lÆ°á»£ng HBM vÃ  giao tiáº¿p host-device cÃ³ liÃªn quan, cÃ¡c ká»¹ thuáº­t
nhÆ° ZeRO sharding vÃ  pipelined data transfer cung cáº¥p cÃ¡c giáº£i phÃ¡p thá»±c táº¿. Thiáº¿t káº¿ accelerator hiá»‡n Ä‘áº¡i
thÆ°á»ng Æ°u tiÃªn tá»· lá»‡ FLOP-to-byte cao Ä‘á»ƒ Ä‘Ã¡p á»©ng cÃ¡c workload nÆ¡i tÃ­nh toÃ¡n
vÆ°á»£t trá»™i hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i viá»‡c truyá»n bá»™ nhá»›. ChÃºng tÃ´i hiá»ƒn thá»‹ thÃ´ng sá»‘ ká»¹ thuáº­t chÃ­nh cá»§a TPU-v3 pod (hai chip
má»—i pod) trong Báº£ng 3, mÃ  chÃºng tÃ´i sá»­ dá»¥ng cho táº¥t cáº£ thÃ­ nghiá»‡m.
20

--- TRANG 21 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

Specification TPU-v3Pod
HBMcapacity 32GB
HBMbandwidth 900GB/s
PeakMXUcompute 123TFLOPs/s(bfloat16)
PeakMXUFLOPs-to-byte-ratio 136
PeakVPUcompute 3.8TFLOPs/s
PeakVPUFLOPs-to-byte-ratio 4.2

Báº£ng 3|ThÃ´ng sá»‘ ká»¹ thuáº­t pháº§n cá»©ng cho TPU-v3 pod.

0K2K4K6K8K10K12K14K16K
Sequence length0102030405060Run time (ms)Linear(Pallas)
Linear(Jax)
Associative(bf16)
Associative(f32)
(a) Thá»i gian cháº¡y Scan

400m 1b 7b
Model size0.85x0.81x0.89x1.00x 1.00x 1.00x1.58x
1.27x1.41x (b) Thá»i gian cháº¡y Hawk

HÃ¬nh 8|a) Thá»i gian cháº¡y cá»§a cÃ¡c triá»ƒn khai khÃ¡c nhau cá»§a phÃ©p toÃ¡n scan trÃªn TPU-v3 á»Ÿ cÃ¡c
chiá»u dÃ i chuá»—i khÃ¡c nhau. Batch size cá»§a Ä‘áº§u vÃ o Ä‘Æ°á»£c cá»‘ Ä‘á»‹nh á»Ÿ 8 vÃ  chiá»u cá»§a má»—i token lÃ  1024.
b) Thá»i gian cháº¡y tÆ°Æ¡ng Ä‘á»‘i cá»§a mÃ´ hÃ¬nh Hawk khi sá»­ dá»¥ng cÃ¡c triá»ƒn khai khÃ¡c nhau cá»§a phÃ©p toÃ¡n scan,
tham chiáº¿u Ä‘áº¿n cÃ¡i cÃ³ triá»ƒn khai Jax scan native.

D.1. TÃ­nh toÃ¡n nhÃ¢n ma tráº­n
Má»™t phÃ©p nhÃ¢n ma tráº­n Ä‘iá»ƒn hÃ¬nh cá»§a ma tráº­n ğ·Ã—ğ· vá»›i ma tráº­n ğ·Ã—ğ‘ cÃ³ 2ğ‘ğ·2 FLOP vÃ  2(ğ·2+2ğ‘ğ·)
byte Ä‘á»ƒ truyá»n (cáº£ Ä‘á»c vÃ  ghi) dáº«n Ä‘áº¿n tá»· lá»‡ FLOP/byte lÃ  ğ‘ğ·
ğ·+ğ‘. Khi ğ·>>ğ‘ vÃ 
cháº¡y trÃªn TPU-v3 Ä‘iá»u nÃ y cÃ³ nghÄ©a lÃ  chiá»u ğ‘ pháº£i Ã­t nháº¥t 136 Ä‘á»ƒ bÃ£o hÃ²a thiáº¿t bá»‹, trong
trÆ°á»ng há»£p Ä‘Ã³ phÃ©p toÃ¡n lÃ  "compute bound", hoáº·c ngÆ°á»£c láº¡i háº§u háº¿t thá»i gian sáº½ Ä‘Æ°á»£c dÃ nh cho viá»‡c chá»
truyá»n bá»™ nhá»›, trong trÆ°á»ng há»£p Ä‘Ã³ phÃ©p toÃ¡n lÃ  "memory bound".

D.2. Thá»i gian cháº¡y Scan
Trong HÃ¬nh 8(a) chÃºng tÃ´i thá»ƒ hiá»‡n ráº±ng trÃªn TPU-v3 kernel Pallas cá»§a chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c tÄƒng tá»‘c gáº§n x3 so vá»›i
triá»ƒn khai Jax naÃ¯ve. NgoÃ i ra, associative scan cháº­m hÆ¡n Ä‘Ã¡ng ká»ƒ, ngay cáº£ khi cháº¡y hoÃ n toÃ n
trong Ä‘á»™ chÃ­nh xÃ¡c bfloat16. HÃ¬nh 8(b) thá»ƒ hiá»‡n ráº±ng nhá»¯ng lá»£i Ã­ch nÃ y cÅ©ng chuyá»ƒn thÃ nh cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ
vá» thá»i gian huáº¥n luyá»‡n tá»•ng thá»ƒ má»—i bÆ°á»›c cá»§a mÃ´ hÃ¬nh Hawk Ä‘áº§y Ä‘á»§ ngay cáº£ á»Ÿ quy mÃ´ 7b. Äá»ƒ hoÃ n chá»‰nh
chÃºng tÃ´i cÅ©ng Ä‘Ã£ thÃªm thá»i gian cháº¡y cá»§a associative scan, cÃ³ thá»ƒ cháº­m hÆ¡n Ä‘áº¿n 50%.

E. KÃ­ch thÆ°á»›c Cá»­a sá»• Attention Cá»¥c bá»™ cá»§a Griffin
Griffin sá»­ dá»¥ng cáº£ cÃ¡c khá»‘i há»“i quy cÅ©ng nhÆ° cÃ¡c lá»›p attention cá»¥c bá»™ trong cÃ¡c khá»‘i temporal mixing cá»§a nÃ³. Cho
táº¥t cáº£ thÃ­ nghiá»‡m trÆ°á»›c Ä‘Ã¢y Ä‘Æ°á»£c hiá»ƒn thá»‹ sá»­ dá»¥ng chiá»u dÃ i chuá»—i huáº¥n luyá»‡n 2048, chÃºng tÃ´i sá»­ dá»¥ng kÃ­ch thÆ°á»›c cá»­a sá»• attention
cá»¥c bá»™ 1024. BÃ¢y giá» chÃºng tÃ´i Ä‘iá»u tra cÃ¡ch hiá»‡u suáº¥t cá»§a cÃ¡c kÃ­ch thÆ°á»›c cá»­a sá»• khÃ¡c nhau cho lá»›p
attention cá»¥c bá»™ thay Ä‘á»•i vá»›i chiá»u dÃ i chuá»—i huáº¥n luyá»‡n.

ChÃºng tÃ´i xem xÃ©t cÃ¡c mÃ´ hÃ¬nh 400M tham sá»‘ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn chiá»u dÃ i chuá»—i 2048, 4096 vÃ  8192 token,
21

--- TRANG 22 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

Train Length 2K Train Length 4K Train Length 8K2.502.522.542.562.582.602.622.642.66Validation Loss512
1K
2K
1K1K
2K4K
1K
2K2K
4K8K1K
2K
4KMQA Transformer
Griffin

HÃ¬nh 9|Hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh Griffin vÃ  MQA Transformer 400M tham sá»‘ sá»­ dá»¥ng cÃ¡c kÃ­ch thÆ°á»›c cá»­a sá»•
attention cá»¥c bá»™ khÃ¡c nhau vÃ  cÃ¡c chiá»u dÃ i chuá»—i huáº¥n luyá»‡n khÃ¡c nhau. KÃ­ch thÆ°á»›c cá»­a sá»• cá»§a cÃ¡c lá»›p attention cá»¥c bá»™
Ä‘Æ°á»£c hiá»ƒn thá»‹ phÃ­a trÃªn má»—i thanh trong biá»ƒu Ä‘á»“. ChÃºng tÃ´i lÆ°u Ã½ ráº±ng MQA Transformer attention toÃ n cá»¥c tá»‘t hÆ¡n nhiá»u
so vá»›i cÃ¡c biáº¿n thá»ƒ attention cá»¥c bá»™ cá»§a MQA Transformer (nÆ¡i kÃ­ch thÆ°á»›c cá»­a sá»• nhá» hÆ¡n
chiá»u dÃ i chuá»—i huáº¥n luyá»‡n). HÆ¡n ná»¯a, chÃºng tÃ´i tháº¥y ráº±ng viá»‡c sá»­ dá»¥ng kÃ­ch thÆ°á»›c cá»­a sá»• attention cá»¥c bá»™ cá»‘ Ä‘á»‹nh
1024 (kÃ½ hiá»‡u '1K' trong biá»ƒu Ä‘á»“) cho mÃ´ hÃ¬nh Griffin vÆ°á»£t trá»™i so vá»›i táº¥t cáº£ baseline MQA Transformer attention toÃ n cá»¥c vÃ  attention cá»¥c bá»™
qua táº¥t cáº£ chiá»u dÃ i chuá»—i huáº¥n luyá»‡n.

nÆ¡i chÃºng tÃ´i giá»¯ tá»•ng sá»‘ token huáº¥n luyá»‡n cá»‘ Ä‘á»‹nh. Cho má»—i chiá»u dÃ i chuá»—i, chÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh Griffin
sá»­ dá»¥ng cÃ¡c kÃ­ch thÆ°á»›c cá»­a sá»• attention cá»¥c bá»™ khÃ¡c nhau. NhÆ° baseline, chÃºng tÃ´i huáº¥n luyá»‡n MQA Transformer sá»­ dá»¥ng
cÃ¡c lá»›p attention toÃ n cá»¥c, cÅ©ng nhÆ° MQA Transformer sá»­ dá»¥ng cÃ¡c lá»›p attention cá»¥c bá»™ vá»›i cÃ¡c kÃ­ch thÆ°á»›c cá»­a sá»•
khÃ¡c nhau. Káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 9, nÆ¡i cÃ¡c kÃ­ch thÆ°á»›c cá»­a sá»• Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘Æ°á»£c hiá»ƒn thá»‹ trÃªn Ä‘áº§u má»—i thanh
(cÃ¡c thanh MQA Transformer vá»›i kÃ­ch thÆ°á»›c cá»­a sá»• báº±ng chiá»u dÃ i chuá»—i huáº¥n luyá»‡n lÃ  baseline MQA Transformer attention toÃ n cá»¥c).

Tá»« HÃ¬nh 9, chÃºng tÃ´i tháº¥y ráº±ng Ä‘Ã¡ng chÃº Ã½, ngay cáº£ khi sá»­ dá»¥ng kÃ­ch thÆ°á»›c cá»­a sá»• cá»‘ Ä‘á»‹nh 1024 cho cÃ¡c
lá»›p attention cá»¥c bá»™ trong Griffin, nÃ³ vÆ°á»£t trá»™i so vá»›i baseline MQA Transformer attention toÃ n cá»¥c qua táº¥t cáº£
chiá»u dÃ i chuá»—i Ä‘Æ°á»£c kiá»ƒm tra. Tuy nhiÃªn, Ä‘Ã¡ng lÆ°u Ã½ ráº±ng khoáº£ng cÃ¡ch hiá»‡u suáº¥t giá»¯a Griffin vá»›i
cá»­a sá»• attention cá»¥c bá»™ 1024 vÃ  MQA Transformer attention toÃ n cá»¥c giáº£m khi chiá»u dÃ i chuá»—i
tÄƒng. Do Ä‘Ã³, náº¿u chiá»u dÃ i chuá»—i tÄƒng thÃªm, cÃ³ thá»ƒ quan trá»ng Ä‘á»ƒ tá»« tá»« cÅ©ng tÄƒng kÃ­ch thÆ°á»›c
cá»­a sá»• attention cá»¥c bá»™. Trong thá»±c táº¿, pháº§n cá»©ng Ä‘Æ°á»£c sá»­ dá»¥ng cÅ©ng sáº½ quyáº¿t Ä‘á»‹nh máº¡nh máº½ kÃ­ch thÆ°á»›c cá»­a sá»• attention cá»¥c bá»™ tá»‘i Æ°u
vá» tá»‘c Ä‘á»™ huáº¥n luyá»‡n vÃ  suy luáº­n. Cuá»‘i cÃ¹ng, chÃºng tÃ´i lÆ°u Ã½ ráº±ng MQA Transformer
sá»­ dá»¥ng attention cá»¥c bá»™ thuáº§n tÃºy (kÃ­ch thÆ°á»›c cá»­a sá»• nhá» hÆ¡n chiá»u dÃ i chuá»—i huáº¥n luyá»‡n) hoáº¡t Ä‘á»™ng Ä‘Ã¡ng ká»ƒ
tá»‡ hÆ¡n so vá»›i cáº£ MQA Transformer attention toÃ n cá»¥c, cÅ©ng nhÆ° Griffin.

F. Tá»‘c Ä‘á»™ Suy luáº­n
F.1. Æ¯á»›c tÃ­nh tÃ­nh bá»‹ giá»›i háº¡n bá»Ÿi bá»™ nhá»›
Tá»‘c Ä‘á»™ suy luáº­n cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ táº¡i thá»i Ä‘iá»ƒm decode bá»‹ giá»›i háº¡n bá»Ÿi viá»‡c táº£i bá»™ nhá»›. NhÆ° Ä‘Ã£ mÃ´ táº£
trong 4.2 RNN tuyáº¿n tÃ­nh bá»‹ giá»›i háº¡n bá»Ÿi bá»™ nhá»›. Trong pháº§n sau chÃºng tÃ´i sáº½ chá»‰ ra Ä‘iá»u nÃ y Ä‘Ãºng cho
cÃ¡c thÃ nh pháº§n khÃ¡c (lÃ  cÃ¡c lá»›p tuyáº¿n tÃ­nh vÃ  self-attention) trong cÃ¡c mÃ´ hÃ¬nh há»“i quy vÃ  Transformer
cá»§a chÃºng tÃ´i.

F.2. Æ¯á»›c tÃ­nh tÃ­nh bá»‹ giá»›i háº¡n bá»Ÿi bá»™ nhá»› cá»§a cÃ¡c lá»›p tuyáº¿n tÃ­nh
NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong D.1 chiá»u ngoÃ i (thÆ°á»ng bao gá»“m batch ğµ vÃ  chiá»u sequence length ğ‘‡)
pháº£i Ã­t nháº¥t 136 Ä‘á»ƒ trá»Ÿ thÃ nh compute bound. Táº¡i thá»i Ä‘iá»ƒm decode ğ‘‡=1 vÃ  náº¿u chÃºng ta giáº£ sá»­ ğµâ‰²128 thÃ¬
báº¥t ká»³ lá»›p tuyáº¿n tÃ­nh nÃ o sáº½ bá»‹ memory bound táº¡i thá»i Ä‘iá»ƒm decode.
22

--- TRANG 23 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

F.3. Æ¯á»›c tÃ­nh tÃ­nh bá»‹ giá»›i háº¡n bá»Ÿi bá»™ nhá»› cá»§a self-attention
Trong pháº§n sau, chÃºng tÃ´i tÃ­nh tá»· lá»‡ truy cáº­p bá»™ nhá»› vá»›i cÃ¡c phÃ©p toÃ¡n sá»‘ há»c cho tÃ­nh toÃ¡n attention
cho bÆ°á»›c decode thá»© ğ¿, Ä‘á»ƒ chá»‰ ra nÃ³ cÅ©ng bá»‹ memory-bound.

Äá»ƒ Ä‘Æ¡n giáº£n hÃ³a phÃ¢n tÃ­ch sau, chÃºng tÃ´i giáº£ sá»­ ráº±ng chÃºng ta báº¯t Ä‘áº§u tá»« má»™t prompt trá»‘ng (hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng
giáº£ sá»­ ráº±ng prefill chá»©a 0 token).

Khi láº¥y máº«u auto-regressively tá»« MHA hoáº·c MQA, thá»±c hÃ nh tiÃªu chuáº©n lÃ  lÆ°u cÃ¡c vector key vÃ  value
trong Key-Value (KV) cache. Cho ğ¿ token Ä‘Ã£ Ä‘Æ°á»£c láº¥y máº«u, KV cache do Ä‘Ã³ sáº½ cÃ³
kÃ­ch thÆ°á»›c 2Ã—ğ¿Ã—ğ»ğ‘˜Ã—ğ‘‘â„ğ‘’ğ‘ğ‘‘ cho má»—i chuá»—i trong batch, trong Ä‘Ã³ ğ»ğ‘˜ biá»ƒu thá»‹ sá»‘ lÆ°á»£ng head Ä‘Æ°á»£c sá»­ dá»¥ng cho
key vÃ  value, vÃ  ğ‘‘â„ğ‘’ğ‘ğ‘‘ biá»ƒu thá»‹ chiá»u cá»§a cÃ¡c vector key vÃ  value trong má»—i head.

Cho viá»‡c láº¥y máº«u token thá»© ğ¿, má»™t khi chÃºng ta tÃ­nh cÃ¡c vector query, key vÃ  value tÆ°Æ¡ng á»©ng vá»›i
token thá»© ğ¿. Trá»ng sá»‘ attention vÃ  Ä‘áº§u ra cá»§a lá»›p attention sau Ä‘Ã³ Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch sá»­ dá»¥ng vector key vÃ  value thá»© ğ¿
trong KV cache. Äiá»u nÃ y yÃªu cáº§u ğ‘‚(ğ¿ğ·) phÃ©p toÃ¡n tá»•ng thá»ƒ vÃ  nÃ³ yÃªu cáº§u táº£i
cache ğ‘‚(ğ¿Ã—ğ»ğ‘˜Ã—ğ‘‘â„ğ‘’ğ‘ğ‘‘) kÃ­ch thÆ°á»›c tá»« HBM, cho má»—i chuá»—i trong minibatch. KÃ­ch thÆ°á»›c cá»§a KV
cache, cÅ©ng nhÆ° sá»‘ lÆ°á»£ng FLOP, tá»· lá»‡ tuyáº¿n tÃ­nh vá»›i batch size ğµ.

Cho MHA, sá»‘ lÆ°á»£ng head cho key vÃ  value ğ»ğ‘˜ thÆ°á»ng báº±ng sá»‘ lÆ°á»£ng head Ä‘Æ°á»£c sá»­ dá»¥ng
cho query ğ». Cho MQA, má»™t head duy nháº¥t Ä‘Æ°á»£c sá»­ dá»¥ng cho key vÃ  value, tá»©c lÃ , ğ»ğ‘˜=1. Do Ä‘Ã³ cho MQA,
kÃ­ch thÆ°á»›c cá»§a KV cache nhá» hÆ¡n má»™t há»‡ sá»‘ ğ»ğ‘˜ (tá»©c lÃ , cÃ³ kÃ­ch thÆ°á»›c 2Ã—ğ¿Ã—ğ‘‘â„ğ‘’ğ‘ğ‘‘).

def attention_sampling(q, k, v):
    """ Auto-regressive sampling via attention.
    For MHA, h_k = h. For MQA, h_k = 1.
    Args:
        q : The q vector for current token of shape [b, h, k]
        k : The keys of the current + previous tokens [b, L, h_k, k]
        v : the values of the current + previous tokens [b, L, h_k, v]
    """
    logits = einsum("bhk,bLk->bhL", q, k) # O(bhLk)
    weights = softmax(logits)
    output = einsum("bhL,bLv->bhv", weights, v) # O(bhLv)
    return output

Cho batch size ğµ, tá»· lá»‡ truy cáº­p bá»™ nhá»› vá»›i FLOP cho tÃ­nh toÃ¡n attention lÃ 
ğ‘‚(ğµÃ—ğ¿Ã—ğ»ğ‘˜Ã—ğ‘‘â„ğ‘’ğ‘ğ‘‘
ğµÃ—ğ¿Ã—ğ·). Cho cÃ¡c kiáº¿n trÃºc Transformer Ä‘iá»ƒn hÃ¬nh, ğ·=ğ»Ã—ğ‘‘â„ğ‘’ğ‘ğ‘‘ vÃ  thÃªm ğ»ğ‘˜=ğ» cho MHA
vÃ  ğ»ğ‘˜=1 cho MQA. Do Ä‘Ã³ tá»· lá»‡ truy cáº­p bá»™ nhá»› vá»›i flops lÃ  ğ‘‚(1) cho MHA vÃ  ğ‘‚(1/ğ») cho MQA.

NhÆ° Ä‘Ã£ giáº£i thÃ­ch trong 3, Ä‘á»ƒ trá»Ÿ thÃ nh compute bound trÃªn TPU-v3 tá»· lá»‡ FLOP-to-byte 136 lÃ  cáº§n thiáº¿t,
vÃ  do Ä‘Ã³ cáº£ MHA vÃ  MQA thÆ°á»ng sáº½ bá»‹ memory bound. Tuy nhiÃªn, MQA tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ
suy luáº­n Transformer (khi so sÃ¡nh vá»›i MHA), vÃ¬ nÃ³ giáº£m tÃ­nh memory boundedness
theo há»‡ sá»‘ ğ».

F.4. KÃ­ch thÆ°á»›c cache
Trong pháº§n sau chÃºng tÃ´i phÃ¢n tÃ­ch kÃ­ch thÆ°á»›c tÆ°Æ¡ng Ä‘á»‘i cá»§a cache Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh há»“i quy vÃ  Transformer cá»§a chÃºng tÃ´i.
Táº¥t cáº£ kÃ­ch thÆ°á»›c cache tá»· lá»‡ tuyáº¿n tÃ­nh vá»›i batch size vÃ  trong pháº§n sau chÃºng tÃ´i giáº£ sá»­ ğµ=1.

F.4.1. KÃ­ch thÆ°á»›c cá»§a KV cache
Cho attention, KV cache cÃ³ kÃ­ch thÆ°á»›c 2ğ‘ğ‘‡â„ğ‘˜ğ‘‘â„ğ‘’ğ‘ğ‘‘, trong Ä‘Ã³ ğ‘ biá»ƒu thá»‹ sá»‘ lÆ°á»£ng lá»›p attention (
Ä‘á»™ sÃ¢u), ğ‘‡ biá»ƒu thá»‹ chiá»u dÃ i cá»§a chuá»—i, â„ğ‘˜ biá»ƒu thá»‹ sá»‘ lÆ°á»£ng KV head vÃ  ğ‘‘â„ğ‘’ğ‘ğ‘‘ biá»ƒu thá»‹
chiá»u head. Trong suá»‘t cÃ´ng trÃ¬nh nÃ y, ğ‘‘â„ğ‘’ğ‘ğ‘‘=128. Cho MHA, â„ğ‘˜ğ‘‘â„ğ‘’ğ‘ğ‘‘=ğ·, trong khi cho MQA, â„ğ‘˜=1.
(Do Ä‘Ã³ chÃºng tÃ´i mong Ä‘á»£i MQA nhanh hÆ¡n khi decode chuá»—i dÃ i so vá»›i MHA, vÃ¬ kÃ­ch thÆ°á»›c cá»§a
KV cache nhá» hÆ¡n Ä‘Ã¡ng ká»ƒ vÃ  Ã­t bá»™ nhá»› cáº§n Ä‘Æ°á»£c di chuyá»ƒn.)
23

--- TRANG 24 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

Cho cáº£ MHA hoáº·c MQA kÃ­ch thÆ°á»›c cá»§a KV cache cÃ³ thá»ƒ vÆ°á»£t quÃ¡ sá»‘ lÆ°á»£ng tham sá»‘ mÃ´ hÃ¬nh khi
chiá»u dÃ i chuá»—i ğ‘‡ lá»›n. Do Ä‘Ã³ chÃºng tÃ´i mong Ä‘á»£i quan sÃ¡t má»™t sá»± chuyá»ƒn Ä‘á»•i tá»« má»™t cháº¿ Ä‘á»™ 'parameter bound'
khi chiá»u dÃ i chuá»—i ngáº¯n, trong Ä‘Ã³ tá»‘c Ä‘á»™ decode bá»‹ chi phá»‘i bá»Ÿi thá»i gian
cáº§n Ä‘á»ƒ táº£i tham sá»‘ mÃ´ hÃ¬nh trÃªn thiáº¿t bá»‹, Ä‘áº¿n má»™t cháº¿ Ä‘á»™ 'cache bound' cho chuá»—i lá»›n, nÆ¡i
tá»‘c Ä‘á»™ decode bá»‹ chi phá»‘i bá»Ÿi thá»i gian cáº§n Ä‘á»ƒ táº£i KV cache.

F.4.2. KÃ­ch thÆ°á»›c cá»§a tráº¡ng thÃ¡i há»“i quy
Tráº¡ng thÃ¡i há»“i quy cá»§a má»™t lá»›p RG-LRU duy nháº¥t cÃ³ kÃ­ch thÆ°á»›c ğ·ğ‘…ğ‘ğ‘, vÃ  tá»•ng kÃ­ch thÆ°á»›c tráº¡ng thÃ¡i cho toÃ n bá»™ mÃ´ hÃ¬nh Hawk
lÃ  ğ‘ğ·ğ‘…ğ‘ğ‘â‰ˆ4ğµğ‘ğ·/3. KhÃ´ng giá»‘ng nhÆ° KV cache, tráº¡ng thÃ¡i nÃ y khÃ´ng tÄƒng vá»›i chiá»u dÃ i chuá»—i vÃ 
ráº¥t nhá» so vá»›i kÃ­ch thÆ°á»›c tham sá»‘. Do Ä‘Ã³ chÃºng tÃ´i mong Ä‘á»£i tá»‘c Ä‘á»™ decode cá»§a RG-LRU
bá»‹ chi phá»‘i bá»Ÿi thá»i gian cáº§n Ä‘á»ƒ táº£i tham sá»‘ mÃ´ hÃ¬nh trÃªn thiáº¿t bá»‹ á»Ÿ táº¥t cáº£ chiá»u dÃ i chuá»—i.

Má»™t cÃ¢n nháº¯c tÆ°Æ¡ng tá»± Ã¡p dá»¥ng cho kÃ­ch thÆ°á»›c tráº¡ng thÃ¡i convolution 1D. Cho chiá»u rá»™ng bá»™ lá»c temporal
kÃ­ch thÆ°á»›c 4, tráº¡ng thÃ¡i cÃ³ kÃ­ch thÆ°á»›c (4âˆ’1)ğ·ğ‘…ğ‘ğ‘=3ğ·ğ‘…ğ‘ğ‘=4ğ· cho má»—i khá»‘i há»“i quy cÅ©ng nhá» hÆ¡n Ä‘Ã¡ng ká»ƒ
so vá»›i kÃ­ch thÆ°á»›c tham sá»‘.

F.4.3. Cache attention cá»¥c bá»™
Má»™t lá»›p MQA cá»¥c bá»™ duy nháº¥t cÃ³ kÃ­ch thÆ°á»›c cache bá»‹ giá»›i háº¡n bá»Ÿi 2ğ‘‡ğ‘Šğ‘†ğ‘‘â„ğ‘’ğ‘ğ‘‘, trong Ä‘Ã³ ğ‘‡ğ‘Šğ‘† biá»ƒu thá»‹ kÃ­ch thÆ°á»›c cá»­a sá»• attention
cá»¥c bá»™. Miá»…n lÃ  ğ‘‡ğ‘Šğ‘†â‰²ğ·2/(ğµğ‘‘â„ğ‘’ğ‘ğ‘‘), kÃ­ch thÆ°á»›c cá»§a cache attention cá»¥c bá»™ cÅ©ng nhá»
so vá»›i sá»‘ lÆ°á»£ng tham sá»‘. Do Ä‘Ã³ chÃºng tÃ´i mong Ä‘á»£i tá»‘c Ä‘á»™ decode cá»§a Griffin tÆ°Æ¡ng tá»± nhÆ°
tá»‘c Ä‘á»™ decode cá»§a mÃ´ hÃ¬nh Hawk.

G. Cáº£i thiá»‡n Dá»± Ä‘oÃ¡n Token Tiáº¿p theo vá»›i Context DÃ i hÆ¡n: Káº¿t quáº£ Bá»• sung
HÃ¬nh 10 hiá»ƒn thá»‹ káº¿t quáº£ bá»• sung thá»ƒ hiá»‡n hiá»‡u suáº¥t dá»± Ä‘oÃ¡n token tiáº¿p theo á»Ÿ cÃ¡c chiá»u dÃ i context khÃ¡c nhau
trÃªn bá»™ dá»¯ liá»‡u held out cá»§a cÃ¡c bÃ i viáº¿t arXiv. ChÃºng tÃ´i tháº¥y ráº±ng káº¿t quáº£ trÃªn bá»™ dá»¯ liá»‡u nÃ y
tÆ°Æ¡ng tá»± vá» máº·t Ä‘á»‹nh tÃ­nh vá»›i káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ trong HÃ¬nh 5.

128 256 512 1K 2K 4K 8K 16K 32K
Token position1.501.752.002.252.502.753.003.253.50Mean-so-far NLLGriffin
Hawk
MQA NoPE
MQA RoPE
128 256 512 1K 2K 4K 8K16K 32K 65K131K
Token position1.501.752.002.252.502.753.003.253.50Mean-so-far NLLGriffin-2k
Griffin-8k
Hawk-2k
Hawk-8k

HÃ¬nh 10|Hiá»‡u suáº¥t Ä‘Ã¡nh giÃ¡ cá»§a cÃ¡c mÃ´ hÃ¬nh 1B tham sá»‘ qua má»™t pháº¡m vi chiá»u dÃ i chuá»—i
trÃªn bá»™ Ä‘Ã¡nh giÃ¡ held-out cá»§a cÃ¡c bÃ i viáº¿t ArXiv. BÃªn trÃ¡i, chÃºng tÃ´i so sÃ¡nh hiá»‡u suáº¥t cá»§a cÃ¡c
mÃ´ hÃ¬nh khÃ¡c nhau Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i chiá»u dÃ i chuá»—i 2048, Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ vá»›i chiá»u dÃ i chuá»—i lÃªn Ä‘áº¿n 32,768. BÃªn
pháº£i, chÃºng tÃ´i so sÃ¡nh Griffin vÃ  Hawk khi Ä‘Æ°á»£c huáº¥n luyá»‡n tÆ°Æ¡ng á»©ng trÃªn chiá»u dÃ i chuá»—i 2048 (2k) vÃ  8192 (8k).
Káº¿t quáº£ tÆ°Æ¡ng tá»± vá» máº·t Ä‘á»‹nh tÃ­nh vá»›i Ä‘Ã¡nh giÃ¡ trÃªn SÃ¡ch Ä‘Æ°á»£c trÃ¬nh bÃ y trong HÃ¬nh 5.

H. Chi tiáº¿t Bá»• sung cá»§a CÃ¡c TÃ¡c vá»¥ Sao chÃ©p vÃ  Truy xuáº¥t
HÃ¬nh 11 lÃ  má»™t minh há»a cá»§a cÃ¡c tÃ¡c vá»¥ Selective Copying vÃ  Induction Heads.

Trong tÃ¡c vá»¥ Selective Copying, mÃ´ hÃ¬nh cáº§n há»c sao chÃ©p cÃ¡c token dá»¯ liá»‡u (token cÃ³ mÃ u trong HÃ¬nh
11) tá»« má»™t chuá»—i trong khi bá» qua cÃ¡c token nhiá»…u (token tráº¯ng trong HÃ¬nh 11). CÃ¡c token bá»‹ gáº¡ch chÃ©o trong
Ä‘áº§u ra trong HÃ¬nh 6 biá»ƒu thá»‹ cÃ¡c token Ä‘Æ°á»£c mask out trong loss.
24

--- TRANG 25 ---
Griffin: Káº¿t há»£p Há»“i quy Tuyáº¿n tÃ­nh CÃ³ cá»•ng vá»›i Attention Cá»¥c bá»™ cho MÃ´ hÃ¬nh NgÃ´n ngá»¯ Hiá»‡u quáº£

Model
(a) TÃ¡c vá»¥ Selective Copying

Model (b) TÃ¡c vá»¥ Induction Heads

HÃ¬nh 11|Minh há»a cá»§a tÃ¡c vá»¥ Selective Copying (trÃ¡i) vÃ  tÃ¡c vá»¥ Induction Heads (pháº£i).

Trong tÃ¡c vá»¥ Induction Heads, mÃ´ hÃ¬nh cáº§n há»c nhá»› láº¡i token ngay sau má»™t token
Ä‘áº·c biá»‡t (token Ä‘en trong HÃ¬nh 11). NhÆ° trÆ°á»›c, cÃ¡c token bá»‹ gáº¡ch chÃ©o trong Ä‘áº§u ra biá»ƒu thá»‹ cÃ¡c token
Ä‘Æ°á»£c mask out trong loss.
25
