# 2402.19427.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rnn/2402.19427.pdf
# Kích thước tệp: 951680 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
2024-3-1
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với
Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả
SohamDe*1,SamuelL.Smith*1,AnushanFernando*1,AleksandarBotev*1,GeorgeCristian-Muraru*1,
AlbertGu2,RubaHaroun1,LeonardBerrada1,YutianChen1,SrivatsanSrinivasan1,GuillaumeDesjardins1,
ArnaudDoucet1,DavidBudden1,YeeWhyeTeh1,RazvanPascanu1,NandoDeFreitas1vàCaglarGulcehre1
*Đóng góp ngang nhau,1GoogleDeepMind,2Công việc được thực hiện khi ở GoogleDeepMind
Mạng nơ-ron hồi quy (RNN) có tốc độ suy luận nhanh và mở rộng hiệu quả trên chuỗi dài, nhưng chúng khó
huấn luyện và khó mở rộng. Chúng tôi đề xuất Hawk, một RNN với hồi quy tuyến tính có cổng, và Griffin,
một mô hình lai kết hợp hồi quy tuyến tính có cổng với attention cục bộ. Hawk vượt qua hiệu suất được báo cáo
của Mamba trên các tác vụ downstream, trong khi Griffin đạt hiệu suất tương đương Llama-2 mặc dù
được huấn luyện trên ít hơn 6 lần số token. Chúng tôi cũng chỉ ra rằng Griffin có thể ngoại suy trên các chuỗi dài
hơn đáng kể so với những gì được thấy trong quá trình huấn luyện. Các mô hình của chúng tôi đạt hiệu quả phần cứng
tương đương với Transformer trong quá trình huấn luyện, và trong suy luận chúng có độ trễ thấp hơn và thông lượng
cao hơn đáng kể. Chúng tôi mở rộng Griffin lên đến 14B tham số, và giải thích cách phân chia mô hình
để huấn luyện phân tán hiệu quả.

1. Giới thiệu
Mạng nơ-ron hồi quy (RNN) đóng vai trò trung tâm trong những ngày đầu của học sâu và nghiên cứu NLP
(Elman, 1990; Siegelmann và Sontag, 1991; Hochreiter và Schmidhuber, 1997; Mikolov et al.,
2010; Bahdanau et al., 2014; Sutskever et al., 2014), và đạt được thành công thực tế trong nhiều ứng dụng,
bao gồm hệ thống dịch máy end-to-end đầu tiên của Google (Wu et al., 2016). Tuy nhiên trong những
năm gần đây, cả học sâu và NLP đều bị thống trị bởi kiến trúc Transformer (Vaswani
et al., 2017), kết hợp các perceptron đa lớp (MLP) và multi-head attention (MHA).
Transformer đạt hiệu suất tốt hơn RNN trong thực tế và cũng rất hiệu quả trong việc sử dụng
phần cứng hiện đại (Kaplan et al., 2020). Các mô hình ngôn ngữ lớn dựa trên Transformer được huấn luyện trên
bộ dữ liệu khổng lồ thu thập từ web đã đạt được thành công đáng chú ý (Brown et al., 2020; Rae et al., 2021;
Hoffmann et al., 2022; Touvron et al., 2023; Achiam et al., 2023; Gemini Team Google, 2023).

Mặc dù thành công, Transformer khó mở rộng hiệu quả cho chuỗi dài do
độ phức tạp bậc hai của attention toàn cục. Ngoài ra, sự tăng trưởng tuyến tính của bộ nhớ cache Key-Value (KV)
với chiều dài chuỗi làm cho Transformer chậm trong suy luận. Mặc dù Multi-Query Attention
(MQA) (Shazeer, 2019) giảm thiểu một phần vấn đề này bằng cách giảm kích thước cache theo một hệ số không đổi,
cache vẫn tăng tuyến tính theo chiều dài chuỗi. Mô hình ngôn ngữ hồi quy trình bày một
lựa chọn hấp dẫn vì chúng nén toàn bộ chuỗi thành một trạng thái ẩn có kích thước cố định được cập nhật
lặp đi lặp lại. Tuy nhiên để thay thế Transformer, các mô hình RNN mới phải chứng minh không chỉ hiệu suất
tương đương ở quy mô mà còn đạt được hiệu quả phần cứng tương tự (Gu et al., 2021a; Mehta et al., 2022;
Smith et al., 2022; Orvieto et al., 2023b; Dao et al., 2022b; Poli et al., 2023; Gu và Dao, 2023).

Trong công trình này, chúng tôi đề xuất lớp RG-LRU, một lớp hồi quy tuyến tính có cổng mới, xung quanh đó chúng tôi thiết kế
một khối hồi quy mới để thay thế MQA. Chúng tôi xây dựng hai mô hình mới sử dụng khối hồi quy này: Hawk,
một mô hình xen kẽ MLP với các khối hồi quy, và Griffin, một mô hình lai xen kẽ
MLP với hỗn hợp các khối hồi quy và attention cục bộ (Beltagy et al., 2020). Chúng tôi chỉ ra rằng:

1. Hawk và Griffin thể hiện quy luật mô rộng power law giữa loss held-out và training FLOP, lên đến và vượt
qua 7B tham số (Hình 1(a)), như đã quan sát trước đây cho Transformer (Kaplan et al., 2020).
2. Griffin đạt loss held-out thấp hơn một chút so với các baseline Transformer mạnh ở tất cả các quy mô mô hình.

Tác giả liên hệ: {sohamde, slsmith}@google.comarXiv:2402.19427v1 [cs.LG] 29 Feb 2024

--- TRANG 2 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

10181019102010211022
Training Flops2.02.53.0Validation Loss
MQA
Hawk
Griffin
(a) Đường cong mở rộng trong huấn luyện

512 1024 2048 4096
Num tokens decoded05000100001500020000Max tokens/s decoded1.0x
1.0x
1.0x
1.0x1.1x 1.8x 3.3x 6.6x2.5x 4.1x 7.3x 14.8x
MQA
Griffin
Hawk (b) Thông lượng tối đa ở quy mô 1B tham số.

Hình 1|a) Hawk, Griffin và baseline MQA Transformer của chúng tôi đều cho thấy mở rộng power law giữa
loss held-out và training FLOP, với Griffin đạt loss held-out thấp nhất ở tất cả ngân sách FLOP.
Mô hình Griffin lớn nhất được hiển thị có 14B tham số. b) Hawk và Griffin đạt thông lượng cao hơn đáng kể
so với MQA Transformer, đặc biệt khi chiều dài của mẫu tăng.

3. Chúng tôi overtrain Hawk và Griffin trên 300B token ở một phạm vi quy mô mô hình. Hawk-3B vượt qua
hiệu suất được báo cáo của Mamba-3B (Gu và Dao, 2023) trên các tác vụ downstream, mặc dù
được huấn luyện trên một nửa số token. Griffin-7B và Griffin-14B đạt hiệu suất tương đương Llama-2
(Touvron et al., 2023) mặc dù được huấn luyện trên khoảng 7 lần ít token hơn (Phần 3.2).

4. Cả Hawk và Griffin đều đạt hiệu quả huấn luyện tương đương với Transformer trên TPU-v3. Vì
các lớp RNN đường chéo bị giới hạn bởi bộ nhớ, chúng tôi đạt được điều này với một kernel cho lớp RG-LRU,
được triển khai trong Pallas (Bradbury et al., 2018), giảm thiểu việc truyền tải bộ nhớ (Phần 4).

5. Trong suy luận, cả Hawk và Griffin đạt thông lượng cao hơn đáng kể so với MQA Trans-
former (Hình 1(b)), và chúng đạt độ trễ thấp hơn khi lấy mẫu chuỗi dài (Phần 5).

6. Griffin hoạt động tốt hơn Transformer khi được đánh giá trên các chuỗi dài hơn so với những gì thấy
trong quá trình huấn luyện, và cũng có thể học hiệu quả các tác vụ sao chép và truy xuất từ dữ liệu huấn luyện
(Phần 6). Tuy nhiên, Hawk và Griffin hoạt động kém hơn Transformer khi chúng tôi đánh giá
các mô hình đã được pre-train trên các tác vụ sao chép và truy xuất chính xác mà không fine-tuning.

2. Kiến trúc Mô hình
Tất cả các mô hình của chúng tôi chứa các thành phần sau: (i) một khối residual, (ii) một khối MLP, và (iii) một
khối temporal-mixing. Trong khi (i) và (ii) giống nhau trên tất cả các mô hình, chúng tôi xem xét ba khối temporal
mixing: Multi-Query Attention (MQA) toàn cục, MQA cục bộ (sliding-window) và khối hồi quy được đề xuất của chúng tôi. Như một phần của khối hồi quy, chúng tôi sử dụng Real-Gated Linear Recurrent Unit (RG-LRU)
– một lớp hồi quy mới được lấy cảm hứng từ Linear Recurrent Unit (Orvieto et al., 2023b).

Khối residual, như được hiển thị trong Hình 2(a), định nghĩa cấu trúc toàn cục của các mô hình và được lấy cảm hứng
từ pre-norm Transformer (Xiong et al., 2020). Sau khi nhúng chuỗi đầu vào, chúng tôi truyền nó qua
𝑁 khối như vậy (𝑁 biểu thị độ sâu mô hình), và sau đó chúng tôi áp dụng RMSNorm (Zhang và Sennrich,
2019) để tạo ra các activation cuối cùng. Để tính xác suất token, chúng tôi áp dụng một lớp tuyến tính cuối cùng
theo sau bởi softmax. Trọng số của lớp này được chia sẻ với lớp nhúng đầu vào.

2.1. Khối residual
Khối residual chứa hai thành phần, được áp dụng theo thứ tự. Thành phần đầu tiên lấy trạng thái ẩn
𝑥 và áp dụng RMSNorm (Zhang và Sennrich, 2019), theo sau bởi khối temporal-mixing.
2

--- TRANG 3 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

Lặp lại 
 N lần 
(a) Khối residual RMSNorm 
RMSNorm Temporal 
mixing block MLP block 
Linear GeLULinear 
Linear 
Linear GeLULinear 
Linear Temporal 
Conv1D RG-LRU (b) Khối MLP có cổng
(c) Khối hồi quy(ví dụ MQA hoặc khối hồi quy) 

Hình 2|a) Xương sống chính của kiến trúc mô hình là khối residual, được xếp chồng 𝑁
lần. b) Khối MLP có cổng mà chúng tôi sử dụng. c) Khối hồi quy mà chúng tôi đề xuất như một thay thế
cho Multi Query Attention (MQA). Nó sử dụng lớp RG-LRU được đề xuất của chúng tôi, được định nghĩa trong Phần 2.4.

Sau đó chúng tôi hợp nhất đầu ra với một kết nối bỏ qua từ 𝑥 thông qua phép cộng. Tương tự, thành phần thứ hai
áp dụng RMSNorm, theo sau bởi khối MLP và sau đó hợp nhất đầu ra của nó với một kết nối bỏ qua
từ đầu vào của RMSNorm. Khối này được minh họa trong Hình 2(a).

2.2. Khối MLP
Chúng tôi sử dụng một khối MLP có cổng (Dauphin et al., 2017) (được minh họa trong Hình 2(b)), tạo ra hai
nhánh từ đầu vào có chiều 𝐷. Chúng tôi áp dụng một lớp tuyến tính với chiều đầu ra 𝑀𝐷 trên mỗi
nhánh, trong đó 𝑀 biểu thị hệ số mở rộng. Để đơn giản, chúng tôi sử dụng 𝑀=3 trong toàn bộ công trình này. Chúng tôi
áp dụng phi tuyến GeLU (Hendrycks và Gimpel, 2016) trên một trong các nhánh trước khi hợp nhất chúng
bằng phép nhân element-wise, tương tự như GeGeLU (Shazeer, 2020). Tuy nhiên, trong khối MLP của chúng tôi,
chúng tôi áp dụng một lớp tuyến tính cuối cùng với chiều đầu ra 𝐷 trên đầu ra của lớp GeGeLU.

2.3. Khối temporal-mixing
Khối temporal-mixing là thành phần của mô hình tổng hợp các activation lớp ẩn
tại các vị trí temporal khác nhau trong chuỗi. Chúng tôi xem xét ba khối temporal-mixing: MQA toàn cục
(Shazeer, 2019), MQA cục bộ (Beltagy et al., 2020) và khối hồi quy được đề xuất của chúng tôi.

Multi-query attention toàn cục Trừ khi được nêu khác, chúng tôi sử dụng MQA thay vì MHA để cải thiện
tốc độ suy luận của các baseline Transformer (Shazeer, 2019). Chúng tôi sử dụng chiều head cố định
𝐷ℎ𝑒𝑎𝑑=128, và chúng tôi cố định số lượng attention head 𝐻 sao cho 𝐻𝐷ℎ𝑒𝑎𝑑=𝐷. Điều này yêu cầu chiều mô hình
𝐷 phải là bội số của 128. Chúng tôi không sử dụng bất kỳ positional embedding tuyệt đối nào, nhưng chúng tôi sử dụng
Rotary Position Embedding (RoPE) (Su et al., 2021) như một positional embedding tương đối.

Sliding window attention cục bộ Một trong những nhược điểm chính của việc sử dụng attention toàn cục là
độ phức tạp tính toán tăng bậc hai theo chiều dài chuỗi. Để giải quyết điều này, một số công trình
đã bắt đầu áp dụng attention cục bộ (Beltagy et al., 2020), còn được gọi là sliding window attention.
Nó cho phép mỗi vị trí chỉ attention đến một số lượng cố định các token trong quá khứ. Điều này không chỉ giảm
FLOP tính toán mà còn giới hạn kích thước cache KV theo kích thước cửa sổ, khiến nó không còn
bậc hai theo chiều dài chuỗi. Tất cả các chi tiết khác giống như MQA toàn cục.

3

--- TRANG 4 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

Khối hồi quy Khối hồi quy của chúng tôi (Hình 2(c)) tương tự như khối GSS (Mehta et al., 2022)
và khối được sử dụng bởi Mamba (Gu và Dao, 2023). Chúng tôi lấy đầu vào có chiều 𝐷 và áp dụng hai
lớp tuyến tính với chiều đầu ra 𝐷𝑅𝑁𝑁 song song, tạo ra hai nhánh. Trên nhánh đầu tiên,
chúng tôi áp dụng một lớp Conv1D separable nhỏ, được lấy cảm hứng từ Shift-SSM trong H3 (Dao et al., 2022b), với
chiều bộ lọc temporal là 4. Lưu ý rằng lớp Conv1D này rất nhỏ, chỉ với 4𝐷𝑅𝑁𝑁 tham số.
Chúng tôi theo sau lớp Conv1D với lớp RG-LRU được đề xuất của chúng tôi (được định nghĩa bên dưới.) Trên nhánh thứ hai,
chúng tôi áp dụng phi tuyến GeLU và sau đó hợp nhất các nhánh bằng phép nhân element-wise. Sau đó
chúng tôi áp dụng một lớp tuyến tính cuối cùng với chiều đầu ra 𝐷.

2.4. Real-Gated Linear Recurrent Unit (RG-LRU)
Lớp RG-LRU được đề xuất của chúng tôi có một hồi quy đơn giản được lấy cảm hứng từ Linear Recurrent Unit (LRU)
(Orvieto et al., 2023b), nhưng kết hợp một cơ chế gating được thúc đẩy bởi văn liệu về
RNN phi tuyến, đặc biệt là LSTM (Hochreiter và Schmidhuber, 1997) và GRU (Chung et al., 2014). Các
phương trình mô tả lớp như sau:

𝑟𝑡=𝜎(𝑊𝑎𝑥𝑡+𝑏𝑎),recurrence gate (1)
𝑖𝑡=𝜎(𝑊𝑥𝑥𝑡+𝑏𝑥),input gate (2)
𝑎𝑡=𝑎𝑐𝑟𝑡, (3)
ℎ𝑡=𝑎𝑡⊙ℎ𝑡−1+√︃
1−𝑎2
𝑡⊙(𝑖𝑡⊙𝑥𝑡). (4)

Đầu ra của lớp là 𝑦𝑡=ℎ𝑡, và phi tuyến 𝜎 trong các phương trình là hàm sigmoid. Trọng số hồi quy
𝑎 trong Phương trình (4) là đường chéo. Do đó tất cả các phép toán đều theo element-wise. Chúng tôi tham số hóa
𝑎 trong Phương trình (3) là 𝑎=𝜎(Λ), trong đó Λ là một tham số có thể học. Điều này đảm bảo rằng 0≤𝑎≤1, đảm bảo
rằng hồi quy ổn định. Biến 𝑐 là một hằng số có giá trị scalar được đặt thành 8. Để ổn định số, trong thực tế chúng tôi tính 𝑎𝑐𝑟𝑡 trong log-space (xem Phụ lục A). Lớp có gate trên cả đầu vào 𝑥 và
trọng số hồi quy 𝑎. Tuy nhiên, không gate nào phụ thuộc vào trạng thái hồi quy ℎ𝑡−1, điều này đảm bảo rằng
tính toán có thể được thực hiện hiệu quả trên thiết bị. Chúng tôi khởi tạo cả 𝑊𝑎 và 𝑊𝑥 bằng LeCun init
(LeCun et al., 2002). Chúng tôi khởi tạo Λ sao cho 𝑎𝑐 được phân bố đều giữa 0.9 và 0.999 tại
đầu huấn luyện, tương tự như Orvieto et al. (2023b). Không giống như nhiều công trình gần đây trong văn liệu SSM,
RG-LRU không sử dụng khởi tạo được lấy cảm hứng từ lý thuyết đa thức trực giao (Gu et al., 2020),
và nó cũng không được định nghĩa như sự rời rạc hóa của một hệ thống liên tục cơ bản (Gu et al., 2021a).

Không giống như lớp LRU gốc, chúng tôi không sử dụng đại số phức trong hồi quy. Trong khi sử dụng
hồi quy phức sẽ dẫn đến một lớp biểu cảm hơn (Orvieto et al., 2023a), chúng tôi thấy rằng
hồi quy phức không có lợi cho mô hình hóa ngôn ngữ trong thực tế, như cũng được quan sát bởi Gu và Dao
(2023).1

Hành vi gate Input gate 𝑖𝑡 tương tự như trong LSTM, có thể lọc (hoặc giảm xuống) đầu vào
𝑥𝑡. Tuy nhiên, theo hiểu biết của chúng tôi, recurrence gate 𝑟𝑡 khác với các cơ chế gating khác
trong văn liệu. Ví dụ, cơ chế selection được đề xuất trong Mamba (Gu và Dao, 2023) có thể
so sánh với update gate của GRU nội suy giữa trạng thái trước đó và quan sát hiện tại
𝑥𝑡. Tác động của nó trên trạng thái ẩn cho phép nó reset trạng thái và quên bất kỳ thông tin nào nó giữ
từ quá khứ, tương tự như forget gate trong LSTM. Ngược lại, recurrence gate của chúng tôi có thể xấp xỉ
nội suy giữa cập nhật LRU tiêu chuẩn từ Orvieto et al. (2023a) và trạng thái ẩn trước đó,
cho phép nó hiệu quả loại bỏ đầu vào và bảo tồn tất cả thông tin từ lịch sử trước đó
(xem Phụ lục A để biết thêm chi tiết). Chúng tôi tin rằng vai trò chính của gate này là cho phép mô hình đạt được
bộ nhớ siêu exponential bằng cách giảm ảnh hưởng của các đầu vào không có thông tin.

1Chúng tôi đề xuất ablating việc sử dụng số phức cho các modality khác và cung cấp thêm thông tin về
phiên bản giá trị phức của lớp RG-LRU trong Phụ lục B.
4

--- TRANG 5 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

3. Mô hình Hồi quy Mở rộng Hiệu quả như Transformer
Nghiên cứu mở rộng cung cấp những hiểu biết quan trọng về cách điều chỉnh các siêu tham số của mô hình và
hành vi của nó ở quy mô. Ở đây, chúng tôi định nghĩa các mô hình được đánh giá trong nghiên cứu của chúng tôi,
và cung cấp các đường cong mở rộng lên đến và vượt qua 7B tham số. Cuối cùng, chúng tôi đánh giá hiệu suất của các mô hình trên các
tác vụ downstream. Chúng tôi xem xét 3 họ mô hình trong công trình này; (1) baseline MQA-Transformer, (2) Hawk; mô hình
RNN thuần túy của chúng tôi, và (3) Griffin; mô hình lai kết hợp các khối hồi quy với attention cục bộ. Chúng tôi
định nghĩa các siêu tham số mô hình chính cho các mô hình trên một phạm vi quy mô trong Phụ lục C.

Baseline MQA Transformer Baseline Transformer của chúng tôi sử dụng mẫu residual và khối MLP có cổng
được mô tả trong Phần 2, kết hợp với MQA (Shazeer, 2019) và RoPE (Su et al., 2021).

Hawk Kiến trúc Hawk sử dụng cùng mẫu residual và khối MLP như baseline Transformer
của chúng tôi, nhưng chúng tôi sử dụng khối hồi quy được giới thiệu trong Phần 2.3 với lớp RG-LRU (xem Phần 2.4)
như khối temporal mixing của chúng tôi, thay vì MQA. Chúng tôi mở rộng chiều rộng của khối hồi quy theo hệ số
khoảng 43 (tức là 𝐷𝑅𝑁𝑁≈4𝐷/3) để đại khái khớp với số lượng tham số của khối MHA
khi cả hai sử dụng cùng chiều mô hình 𝐷.2 Xem Phụ lục C cho các siêu tham số chính xác.

Griffin Lợi ích chính của các khối hồi quy so với attention toàn cục là chúng sử dụng kích thước trạng thái cố định
để tóm tắt chuỗi, trong khi kích thước cache KV của MQA tăng tỉ lệ với chiều dài chuỗi.
Vì attention cục bộ (Phần 2.3) có cùng tính chất này, việc kết hợp các khối hồi quy với attention cục bộ bảo tồn lợi ích này. Chúng tôi thấy sự kết hợp này cực kỳ hiệu quả, vì attention cục bộ mô hình chính xác
quá khứ gần đây, trong khi các lớp hồi quy có thể truyền thông tin qua các chuỗi dài.

Griffin sử dụng cùng mẫu residual và khối MLP như baseline Transformer của chúng tôi. Tuy nhiên không giống
cả baseline MQA Transformer và mô hình Hawk, Griffin sử dụng hỗn hợp các khối hồi quy
và khối MQA. Cụ thể, chúng tôi sử dụng cấu trúc lớp bằng cách xen kẽ hai khối residual với
một khối hồi quy theo sau bởi một khối residual sử dụng khối attention cục bộ (MQA) được mô tả
trong Phần 2.3. Trừ khi được nêu khác, kích thước cửa sổ attention cục bộ được cố định ở 1024 token.

3.1. Đường cong mở rộng
Chúng tôi trình bày kết quả mở rộng chính trong Hình 1(a). Cả ba họ mô hình được huấn luyện ở một phạm vi
quy mô mô hình từ 100M đến 7B tham số, với một mô hình Griffin bổ sung có 14 tỷ tham số.
Chúng tôi tăng số lượng token huấn luyện để tỉ lệ thuận với số lượng tham số
của mô hình, như được quy định bởi luật mở rộng Chinchilla (Hoffmann et al., 2022). Các mô hình được huấn luyện
trên bộ dữ liệu Massive Text (Hoffmann et al., 2022), trước đây được sử dụng để huấn luyện Gopher (Rae et al., 2021)
và Chinchilla (Hoffmann et al., 2022), mặc dù chúng tôi sử dụng phân phối tập con dữ liệu hơi khác.
Chiều dài chuỗi 2048 token được sử dụng (xem Phần 6 cho kết quả với chuỗi dài hơn.) Tất cả
thí nghiệm sử dụng optimizer AdamW (Loshchilov và Hutter, 2017). Chúng tôi điều chỉnh learning rate,
weight decay và tham số 𝛽2 cho các mô hình nhỏ, và sử dụng các chạy này để xác định quy tắc mở rộng cho
các siêu tham số này dự đoán giá trị tối ưu của chúng cho các mô hình 7B và 14B.

Cả ba họ mô hình đều thể hiện mối quan hệ mở rộng tuyến tính giữa validation loss và
training FLOP (xem Hình 1(a); lưu ý cả hai trục đều ở thang log), như đã quan sát trước đây cho Transformer
bởi Brown et al. (2020). Đáng chú ý, Griffin đạt validation loss thấp hơn so với baseline Transformer
trên tất cả ngân sách FLOP mặc dù không sử dụng bất kỳ lớp attention toàn cục nào. Hawk mặt khác đạt
validation loss cao hơn một chút, nhưng khoảng cách này dường như thu hẹp khi ngân sách huấn luyện tăng.

2Lưu ý rằng chúng tôi khớp tham số với khối attention MHA, mặc dù baseline Transformer và Griffin cuối cùng
dựa vào attention MQA để cải thiện hiệu quả suy luận. Điều này có nghĩa là các khối hồi quy của chúng tôi có nhiều
tham số hơn một chút so với các khối MQA tương ứng.
5

--- TRANG 6 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

Bảng 1|Độ chính xác được chuẩn hóa theo ký tự. Hawk cạnh tranh với baseline Transformer của chúng tôi, và
vượt qua hiệu suất được báo cáo của Mamba mặc dù được huấn luyện trên một nửa số token. Griffin
vượt trội so với baseline Transformer của chúng tôi, và đạt hiệu suất tương đương Llama-2 mặc dù được huấn luyện
trên khoảng 7 lần ít token hơn. Chúng tôi báo cáo độ chính xác không chuẩn hóa với tính điểm một phần cho WinoGrande.

Model Model TrainingMMLU HellaSwag PIQA WinoGrande ARC-E ARC-C Average
Type Size Tokens
Mamba 3B 600B 26.2 71.0 78.1 65.9 68.2 41.7 58.5
7B 2T 45.3 77.2 78.8 69.2 75.2 45.9 65.3
Llama-213B 2T 54.8 80.7 80.5 72.8 77.3 49.4 69.3
MQA 1B 300B 28.9 64.8 75.0 62.0 60.2 35.4 54.4
Transformer 3B 300B 31.7 71.0 77.6 66.1 68.1 39.2 59.0
(Baseline) 6B 300B 38.9 77.0 79.5 70.4 74.1 45.2 64.2
1B 300B 29.7 63.3 76.1 57.2 60.6 34.6 53.6
3B 300B 31.3 71.7 78.8 66.5 68.4 40.2 59.5 Hawk
7B 300B 35.0 77.6 80.0 69.9 74.4 45.9 63.8
Griffin1B 300B 29.5 67.2 77.4 65.2 67.0 36.9 57.2
3B 300B 32.6 73.5 78.1 67.2 71.5 41.4 60.7
7B 300B 39.3 78.6 81.0 72.6 75.4 47.9 65.8
14B 300B 49.5 81.4 81.8 74.1 79.1 50.8 69.5

3.2. Đánh giá trên các tác vụ downstream
Để so sánh với các mô hình khác trong văn liệu, chúng tôi huấn luyện tất cả mô hình trong 300B token trước khi
đánh giá trên các tác vụ downstream. Hai baseline bên ngoài mà chúng tôi so sánh là Mamba-3B (Gu và
Dao, 2023), mô hình hồi quy nhỏ mạnh nhất được báo cáo trong văn liệu cho đến nay, và Llama-2 (Touvron
et al., 2023), một mô hình Transformer mở được sử dụng rộng rãi. Cả hai baseline bên ngoài đều được huấn luyện trên
đáng kể hơn 300B token–Mamba được huấn luyện trên 600B token, gấp đôi, và Llama-2
được huấn luyện trên 2T token, gần gấp bảy lần. Tuy nhiên chúng tôi lưu ý rằng cả Mamba và Llama-2
được huấn luyện trên các bộ dữ liệu khác nhau và với các chiến lược điều chỉnh siêu tham số khác nhau, có thể
giải thích một phần hiệu suất mạnh của chúng tôi. Do đó chúng tôi cũng bao gồm baseline MQA transformer của riêng chúng tôi,
được huấn luyện trên cùng dữ liệu và với cùng ngân sách điều chỉnh siêu tham số như Hawk và Griffin.

Chúng tôi cung cấp đánh giá trên các tác vụ downstream trong Bảng 1. Chúng tôi thấy rằng cả Hawk và Griffin đạt
hiệu suất rất mạnh. Phù hợp với các công trình khác, chúng tôi báo cáo độ chính xác được chuẩn hóa theo ký tự trên MMLU,
HellaSwag, PIQA, ARC-E và ARC-C, trong khi chúng tôi báo cáo độ chính xác tuyệt đối trên WinoGrande với tính điểm
một phần. Hiệu suất của Hawk cải thiện đáng kể khi chúng tôi tăng kích thước mô hình, và Hawk-3B
đạt hiệu suất mạnh hơn trên các tác vụ downstream so với Mamba-3B, mặc dù được huấn luyện trên một nửa
số token. Griffin-3B vượt trội đáng kể so với Mamba-3B, và Griffin-7B và Griffin-14B đạt
hiệu suất cạnh tranh với Llama-2, mặc dù được huấn luyện trên gần 7 lần ít token hơn. Hawk
cũng cạnh tranh với baseline MQA Transformer của chúng tôi, trong khi Griffin vượt trội so với baseline này.

4. Huấn luyện Mô hình Hồi quy Hiệu quả trên Thiết bị
Chúng tôi gặp phải hai thách thức kỹ thuật chính khi phát triển và mở rộng các mô hình. Đầu tiên, làm thế nào
để phân chia hiệu quả các mô hình qua nhiều thiết bị. Thứ hai, làm thế nào để triển khai hiệu quả các hồi quy tuyến tính
để tối đa hóa hiệu quả huấn luyện trên TPU. Chúng tôi giải quyết cả hai thách thức này trong phần này,
trước khi cung cấp so sánh thực nghiệm về tốc độ huấn luyện của Griffin và baseline MQA của chúng tôi.
6

--- TRANG 7 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

4.1. Model parallelism cho huấn luyện quy mô lớn
Khi mô hình của chúng tôi tăng kích thước, chúng tôi không thể fit mô hình trên một thiết bị duy nhất trong quá trình huấn luyện, ngay cả với
batch size là 1 per-device. Do đó chúng tôi sử dụng model parallelism để phân chia các mô hình lớn qua các thiết bị
trong quá trình huấn luyện. Vì chi phí giao tiếp qua các thiết bị huấn luyện khác nhau rất đắt, việc phân chia
mô hình hiệu quả là quan trọng cho huấn luyện nhanh ở quy mô.

Khối MLP và MQA Cho khối gated-MLP của chúng tôi, chúng tôi sử dụng phân chia theo phong cách Megatron (Shoeybi et al., 2019),
yêu cầu một phép toán all-reduce duy nhất trong cả forward pass và backward pass. Tương tự,
chúng tôi áp dụng cùng chiến lược cho các lớp tuyến tính trong khối attention, và bổ sung phân chia cơ chế attention
qua các head của nó (Narayanan et al., 2021).

Khối hồi quy Khối hồi quy chứa hai lớp tuyến tính trên mỗi nhánh. Điều này cho phép chúng tôi áp dụng
phân chia Megatron cho các lớp này theo cách tương đương. Lớp Conv1D hoạt động độc lập
qua các kênh, cho phép chúng tôi chia tham số của nó qua các thiết bị mà không phát sinh chi phí giao tiếp
nào. Để tránh giao tiếp qua thiết bị bổ sung, chúng tôi sử dụng trọng số block-diagonal cho các
gate trong RG-LRU (xem phương trình 1 và 2), thay vì ma trận dày đặc. Cho tất cả thí nghiệm trong
bài báo này, chúng tôi sử dụng 16 khối cho cả recurrence gate và input gate (sao cho 𝑊𝑥 và 𝑊𝑎 mỗi cái
có 𝐷2𝑅��𝑁/16 tham số). Cấu trúc đường chéo của hồi quy cung cấp cùng lợi ích như
Conv1D, cho phép phân chia tham số và tính toán mà không cần giao tiếp nào. Với chiến lược này,
yêu cầu giao tiếp của khối hồi quy tương đương với khối MLP.

Các cân nhắc khác Trạng thái optimizer có thể tiêu thụ bộ nhớ đáng kể, vượt quá kích thước của
tham số mô hình. Để giải quyết điều này, chúng tôi sử dụng ZeRO parallelism (Rajbhandari et al., 2020),
phân phối cả trạng thái optimizer và tham số mô hình qua các shard batch. Chúng tôi cũng sử dụng
biểu diễn bfloat16 cho tham số mô hình và activation, giảm thiểu bất kỳ overhead truyền dữ liệu nào.

4.2. Hồi quy tuyến tính hiệu quả trên thiết bị
Các accelerator học sâu hiện tại được tối ưu hóa cho các kiến trúc cổ điển bao gồm chủ yếu
phép nhân ma trận và convolution. Các phép toán này có tỷ lệ FLOP-to-byte cao, thúc đẩy
việc phát triển các đơn vị phần cứng chuyên dụng như TensorCore của Nvidia GPU (Markidis et al., 2018) và
MXU của Google TPU (Norrie et al., 2021; Jouppi et al., 2021, 2023). RNN cổ điển cũng hưởng lợi từ điều này
do ma trận hồi quy dày đặc của chúng. Ngược lại, lớp RG-LRU được đề xuất của chúng tôi, như các mô hình RNN đường chéo khác,
có tỷ lệ FLOP-to-byte thấp. Sự khác biệt cơ bản này tạo ra thách thức tính toán,
vì các accelerator hiện tại thiếu tối ưu hóa cho các workload như vậy. Vì chúng tôi chạy tất cả thí nghiệm trên
TPU-v3, chúng tôi tập trung vào phát triển triển khai hiệu quả phù hợp với thiết bị này3.

Thách thức cho RNN đường chéo tuyến tính Một trong những thách thức chính của việc sử dụng thiết bị như TPU-v3
cho RG-LRU là phương trình cập nhật trạng thái ẩn trong eq.(4) là phép toán element-wise thuần túy.
Cho mỗi cập nhật phần tử, nó yêu cầu tải 6 byte (giả sử bfloat16 cần 2 byte cho mỗi
biến ℎ𝑡−1, 𝑎𝑡, 𝑥𝑡) và ghi 2 byte (trạng thái ẩn ℎ𝑡) trong khi tính toán chỉ thực hiện 6 FLOP
(số phép toán số học trong eq.4) cho mỗi phần tử. Điều này dẫn đến tỷ lệ FLOP-to-byte thấp
là 0.75–đáng kể dưới khả năng của thiết bị cho các phép toán element-wise là 4.2 (xem Phụ lục 3).
Thời gian thực hiện do đó bị chi phối bởi việc truyền bộ nhớ giữa HBM và VMEM, khiến
tính toán bị giới hạn bởi bộ nhớ.

Một linear scan tùy chỉnh Để giải quyết điều này, chúng tôi đã viết một kernel Pallas tùy chỉnh cho tính toán
của eq.(4) sử dụng một linear scan. Điều này cho phép chúng tôi giảm thiểu việc truyền bộ nhớ, bằng cách giữ trạng thái ẩn

3Các kết luận được rút ra ở đây không nhất thiết áp dụng cho các accelerator khác.
7

--- TRANG 8 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

2k 4k 8k
Sequence length1.00x1.17x1.46x
0.93x 0.96x 0.98xMQA
Griffin
(a) 400m

2k 4k 8k
Sequence length1.00x1.12x1.33x
0.97x 0.99x 1.01x (b) 1B

2k 4k 8k
Sequence length1.00x1.05x1.13x1.05x 1.07x 1.07x (c) 7B

Hình 3|Thời gian huấn luyện mỗi bước được tính tương đối với baseline MQA của chúng tôi ở chiều dài chuỗi 2K khi chúng tôi
thay đổi kích thước mô hình và chiều dài chuỗi cho Griffin và MQA. Hãy lưu ý rằng khi chúng tôi tăng chiều dài chuỗi
chúng tôi giảm batch size tỷ lệ thuận, sao cho tổng số token mỗi batch giữ nguyên cố định.

trong VMEM suốt thời gian, và cũng để thực hiện việc truyền bộ nhớ theo từng khối lớn hơn thay vì từng cái một.
Trong thực tế, điều này dẫn đến tăng tốc gần 3x so với triển khai Jax native của linear scan.
Ngoài ra, chúng tôi quan sát 10-20% thời gian huấn luyện thấp hơn mỗi bước của mô hình Hawk đầy đủ, so với
cùng mô hình sử dụng triển khai Jax native (xem Phụ lục D.2 để biết thêm chi tiết.)

Tại sao chúng tôi không sử dụng convolution hoặc associative scan? Sức hấp dẫn ban đầu của các mô hình hồi quy tuyến tính
xuất phát từ khả năng song song hóa cao, được kích hoạt bởi tính kết hợp của các tính toán. Điều này
cho phép thực hiện hiệu quả trên thiết bị thông qua convolution (Gu et al., 2021b) hoặc thuật toán prefix-sum (
associative scan) (Smith et al., 2022). Tuy nhiên, cơ chế gating của RG-LRU trên 𝑎𝑡 không tương thích
với quan điểm convolutional. Mặc dù chúng tôi vẫn có thể sử dụng associative scan về nguyên tắc, associative
scan giảm số FLOP cần thiết nhưng không giảm overhead bộ nhớ, đây là nút thắt cổ chai chính
trong thực tế. Về mặt thực nghiệm, chúng tôi quan sát rằng trên TPU-v3, associative scan chậm hơn đáng kể
so với Jax linear scan native (xem Phụ lục D.2 để biết thêm chi tiết.) Chúng tôi suy đoán rằng tính chất truy cập ngẫu nhiên
của việc tái kết hợp cây của thuật toán parallel prefix-sum khiến nó không phù hợp với kiến trúc TPU,
dẫn đến việc truyền bộ nhớ chậm hơn–nút thắt cổ chai chính của phép toán này.

4.3. Tốc độ huấn luyện trên chuỗi dài hơn
Chúng tôi so sánh tốc độ huấn luyện qua các kích thước mô hình và chiều dài chuỗi khác nhau để điều tra
lợi ích tính toán của các mô hình trong quá trình huấn luyện. Cho mỗi kích thước mô hình, chúng tôi giữ tổng số
token mỗi batch cố định, có nghĩa là khi chúng tôi tăng chiều dài chuỗi, chúng tôi giảm tỷ lệ thuận
số lượng chuỗi. Trong Hình 3, chúng tôi vẽ thời gian chạy tương đối của mô hình Griffin so với
baseline MQA ở chiều dài chuỗi 2048. Ở chiều dài chuỗi thấp nhất, hai mô hình có thời gian huấn luyện
tương tự, nhưng khi chúng tôi tăng chiều dài chuỗi, Transformer trở nên chậm hơn, trong khi thời gian chạy của Griffin
giữ nguyên. Sự giảm tốc độ cho baseline rõ ràng hơn ở kích thước mô hình nhỏ hơn và
giảm ở kích thước mô hình lớn hơn. Điều này có thể được giải thích bởi thực tế rằng tất cả mô hình đều chứa một số lượng lớn
lớp tuyến tính. Tính toán của chúng tỷ lệ 𝑂(𝑇𝐷2), trong khi RG-LRU là 𝑂(𝑇𝐷) vs 𝑂(𝑇2𝐷) của attention toàn cục.
Điều này có nghĩa là khi chúng tôi tăng chiều rộng mô hình 𝐷 so với chiều dài chuỗi 𝑇, các lớp tuyến tính
trở thành nút thắt cổ chai tính toán chính, giảm thiểu lợi ích hiệu quả từ khối RNN.

Do đó, việc thay thế Transformer bằng Hawk hoặc Griffin mang lại cải thiện thời gian wall-clock đáng kể nhất
khi chiều dài chuỗi đủ lớn so với chiều rộng mô hình để đảm bảo tính toán attention chiếm
một phần lớn tổng thời gian tính toán. Chúng tôi cũng lưu ý rằng trong thực tế, baseline MQA
8

--- TRANG 9 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

của chúng tôi có ít tham số hơn một chút so với Griffin ở cùng quy mô mô hình (và thực hiện ít FLOP hơn).
Điều này giải thích tại sao Griffin huấn luyện chậm hơn một chút so với baseline MQA ở 7B cho chuỗi ngắn.

5. Tốc độ Suy luận
Suy luận trong LLM bao gồm hai giai đoạn. Trong giai đoạn "prefill", chúng tôi nhận và xử lý prompt.
Bước này thực chất là thực hiện forward pass của mô hình. Vì prompt có thể được xử lý song song
qua chuỗi, hầu hết các phép toán mô hình bị giới hạn bởi tính toán trong giai đoạn này. Do đó chúng tôi
mong đợi tốc độ tương đối của Transformer và mô hình hồi quy trong giai đoạn prefill tương tự
với tốc độ tương đối của cùng mô hình trong quá trình huấn luyện, mà chúng tôi đã thảo luận trong Phần 4.

Prefill được theo sau bởi giai đoạn "decode", trong đó chúng tôi lấy mẫu token auto-regressively từ mô hình. Như chúng tôi
thấy bên dưới, các mô hình hồi quy có độ trễ thấp hơn và thông lượng cao hơn trong giai đoạn decode, đặc biệt
cho chiều dài chuỗi dài hơn nơi key-value (KV) cache được sử dụng trong attention có thể trở nên lớn.

Có hai metric chính cần xem xét khi đánh giá tốc độ suy luận. Thứ nhất là độ trễ,
đo thời gian cần thiết để tạo ra một số token được chỉ định ở một batch size nhất định. Thứ hai là
thông lượng, đo số lượng token lớn nhất mỗi giây có thể được tạo ra trên một
thiết bị duy nhất khi lấy mẫu một số token được chỉ định. Vì thông lượng được tính bằng token được lấy mẫu nhân
batch size chia cho độ trễ, người ta có thể cải thiện thông lượng bằng cách giảm độ trễ hoặc bằng cách giảm
việc sử dụng bộ nhớ để cho phép sử dụng batch size lớn hơn trên thiết bị. Độ trễ có thể hữu ích để xem xét cho
các ứng dụng thời gian thực yêu cầu thời gian phản hồi nhanh. Thông lượng cũng hữu ích để xem xét vì nó có thể
cho chúng ta biết số lượng token tối đa chúng ta có thể lấy mẫu từ một mô hình cụ thể trong một thời gian nhất định. Tính chất
này hữu ích khi xem xét các ứng dụng ngôn ngữ khác như Reinforcement Learning from
Human Feedback (RLHF) hoặc chấm điểm đầu ra mô hình ngôn ngữ như được thực hiện trong AlphaCode (Li et al., 2022)
nơi khả năng xuất ra một số lượng lớn token trong một thời gian nhất định là một tính năng hấp dẫn.

5.1. Một mô hình đơn giản của bước decode
Tất cả các thành phần của mô hình ngôn ngữ đều bị giới hạn bởi bộ nhớ trong decode miễn là batch size không quá
lớn (tức là 𝐵≲128-xem Phụ lục F.1 để biết chi tiết) và chúng tôi sẽ giả định điều này cho phần còn lại của phần này.
Overhead bộ nhớ lớn nhất của Transformer thường đến từ chính các tham số và
cache KV. Do đó chúng tôi có thể xấp xỉ thời gian cần thiết để tạo ra một token duy nhất cho mỗi chuỗi
trong batch 𝐵 trong decode là thời gian cần thiết để tải hai lượng này từ bộ nhớ:

Thời gian lấy mẫu token tiếp theo ≈ kích thước tham số + batch size × kích thước cache
băng thông bộ nhớ. (5)

Ở đây, kích thước cache đề cập đến kích thước cache KV ở batch size 1 (cho Transformer), hoặc
kích thước trạng thái hồi quy ở batch size 1 (cho RNN).

Kích thước cache Sự khác biệt trong kích thước cache so với tham số mô hình có ý nghĩa quan trọng
cho hiệu quả lấy mẫu. Trong các khối hồi quy và attention cục bộ, việc tải tham số là nút thắt cổ chai chính,
(vì kích thước cache nhỏ hơn đáng kể). Ngược lại, cache KV của attention toàn cục
tỷ lệ với chiều dài chuỗi 𝑇 và có thể so sánh, hoặc thậm chí vượt quá, kích thước của tham số mô hình. Điều này tạo ra overhead đáng kể khi chiều dài chuỗi 𝑇 đủ lớn (như
thấy trong F.4). Do đó, một mô hình hồi quy có kích thước bằng nhau có thể thể hiện độ trễ thấp hơn đáng kể
so với Transformer khi 𝑇 lớn. Lưu ý tuy nhiên rằng khi kích thước mô hình tăng, chiều dài chuỗi mà
chúng ta thấy lợi ích độ trễ (nơi kích thước cache KV so sánh với kích thước tham số) cũng tăng.

Điều quan trọng cần lưu ý là, cũng như cải thiện độ trễ, có trạng thái hồi quy nhỏ cũng có thể tăng
batch size lớn nhất fit trong bộ nhớ trên một thiết bị duy nhất, dẫn đến thông lượng cao hơn.
9

--- TRANG 10 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

128 256 512 1024 2048 4096
# Tokens decoded020406080100120140Time taken to sample/s
MQA
Griffin
Hawk
(a) Độ trễ prefill trống

128 256 512 1024 2048 4096
# Tokens decoded050100150Time taken to sample/s
MQA
Griffin
Hawk (b) Độ trễ prefill 4k

Hình 4|Độ trễ của các mô hình 1B tham số khác nhau cho một phạm vi chiều dài chuỗi cho (a) lấy mẫu
từ prefill trống và (b) lấy mẫu từ prefill 4k token.

5.2. Kết quả
Ở đây, chúng tôi xem xét kết quả suy luận cho các mô hình có kích thước 1B tham số. Cho baseline của chúng tôi, chúng tôi so sánh
với MQA Transformer, nhanh hơn đáng kể trong suy luận so với MHA Transformer
tiêu chuẩn thường được sử dụng trong văn liệu. Các mô hình mà chúng tôi so sánh là: i) MQA Transformer, ii)
Hawk, và iii) Griffin. Để so sánh các mô hình khác nhau, chúng tôi báo cáo cả độ trễ và thông lượng.

Độ trễ Chúng tôi so sánh độ trễ cho các mô hình với batch size 16 với prefill trống cũng như
prefill 4096 token như thấy trong Hình 4. Hawk và Griffin đạt độ trễ lấy mẫu nhanh hơn so với MQA
Transformer cho chuỗi dài. Điều này đặc biệt đáng chú ý khi chiều dài chuỗi và chiều dài prefill
(ảnh hưởng đến kích thước cache KV) được tăng. Griffin đạt độ trễ tương tự như Hawk,
thể hiện tính tương thích tuyệt vời của hồi quy tuyến tính và attention cục bộ.

Thông lượng Chúng tôi so sánh thông lượng tối đa (token/s) cho cùng mô hình khi lấy mẫu
512, 1024, 2048 và 4196 token theo sau prompt trống trong Hình 1(b). Chúng tôi thấy rằng cả Griffin
và Hawk đạt thông lượng cao hơn đáng kể so với baseline MQA Transformer. Điều này một phần
do các mô hình hồi quy có độ trễ thấp hơn nhưng cũng chủ yếu xảy ra vì Griffin và Hawk có thể
fit batch size lớn hơn so với MQA Transformer trên một thiết bị duy nhất, vì kích thước cache của chúng nhỏ hơn.
Hawk đạt thông lượng cao hơn Griffin, vì kích thước cache attention cục bộ cuối cùng
trở nên so sánh với kích thước tham số khi batch size lớn.

6. Mô hình hóa Context Dài
Trong phần này, chúng tôi khám phá hiệu quả của Hawk và Griffin trong việc sử dụng context dài hơn để cải thiện
dự đoán token tiếp theo, và điều tra khả năng ngoại suy của chúng trong suy luận. Ngoài ra,
chúng tôi khám phá hiệu suất của các mô hình trên các tác vụ yêu cầu khả năng sao chép và truy xuất, cả cho
các mô hình được huấn luyện trên các tác vụ như vậy, cũng như khi kiểm tra các khả năng này với các mô hình ngôn ngữ đã được pre-train.

6.1. Cải thiện dự đoán token tiếp theo với context dài hơn
Chúng tôi điều tra khả năng của Hawk và Griffin cải thiện dự đoán với context dài hơn. Cụ thể,
chúng tôi đánh giá các mô hình đã huấn luyện bằng cách đo loss trên bộ dữ liệu sách held-out qua
một phạm vi chiều dài chuỗi. Sử dụng các tài liệu dài này cho phép chúng tôi đánh giá khả năng của các mô hình
10

--- TRANG 11 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

128 256 512 1K 2K 4K 8K 16K 32K
Token position2.42.62.83.03.23.4Mean-so-far NLLGriffin
Hawk
MQA NoPE
MQA RoPE
128 256 512 1K 2K 4K 8K16K 32K 65K131K
Token position2.42.62.83.03.23.4Mean-so-far NLLGriffin-2k
Griffin-8k
Hawk-2k
Hawk-8k

Hình 5|Hiệu suất của các mô hình 1B tham số khác nhau trên bộ đánh giá held-out của sách. Bên trái,
các mô hình được huấn luyện với chiều dài chuỗi 2048, và bên phải với chiều dài chuỗi
tương ứng 2048 (2k) và 8192 (8k). Hawk và Griffin có thể ngoại suy đến các chuỗi dài hơn đáng kể
so với các baseline Transformer, và cải thiện hiệu suất thêm khi được huấn luyện trên chuỗi dài hơn.

ngoại suy, tức là khả năng dự đoán chính xác token tiếp theo với context dài hơn so với
những gì thấy trong quá trình huấn luyện.

Trong Transformer, khả năng ngoại suy này phần lớn được xác định bởi positional encoding được sử dụng cho
các lớp attention (Kazemnejad et al., 2024). Đối với các mô hình hồi quy, nó thay vào đó được chi phối bởi khả năng
của mô hình tiếp tục tinh chỉnh các biểu diễn được lưu trữ trong trạng thái hồi quy khi context trở nên
dài hơn. Từ biểu đồ bên trái của Hình 5, chúng tôi quan sát rằng, đến một chiều dài tối đa nào đó, cả Hawk và
Griffin đều cải thiện dự đoán token tiếp theo với context dài hơn, và chúng có thể ngoại suy
đến các chuỗi dài hơn đáng kể (ít nhất 4x dài hơn) so với những gì chúng được huấn luyện. Đặc biệt, Griffin
ngoại suy remarkably tốt ngay cả khi sử dụng RoPE (Su et al., 2021) cho các lớp attention cục bộ.

Kết quả cho đến nay đánh giá các mô hình được huấn luyện trên chuỗi 2048 token. Để đánh giá
liệu các mô hình của chúng tôi cũng có thể học hiệu quả từ context dài hơn, chúng tôi huấn luyện các mô hình 1B tham số trên
chuỗi 8192 (8k) token trên Massive Text, và so sánh chúng với các mô hình được huấn luyện trên cùng bộ dữ liệu
nhưng trên chuỗi có chiều dài 2048 (2k) token. Chúng tôi giữ tổng số token huấn luyện như nhau
qua các mô hình bằng cách giảm batch size theo hệ số 4 cho các mô hình được huấn luyện trên chiều dài chuỗi
8192 (trong khi giữ số bước huấn luyện cố định). Như được minh họa trong biểu đồ bên phải của Hình 5,
chúng tôi thấy rằng Hawk-8k và Griffin-8k thực sự đạt loss đánh giá thấp hơn cho chuỗi có chiều dài 8192 hoặc lớn hơn,
so với Hawk-2k và Griffin-2k. Điều này chỉ ra rằng Hawk và Griffin có thể học sử dụng context dài hơn
trong quá trình huấn luyện. Thú vị, khi đánh giá ở chiều dài chuỗi ngắn, chúng tôi thấy rằng Hawk-2k
và Griffin-2k hoạt động tốt hơn một chút so với Hawk-8k và Griffin-8k. Điều này gợi ý rằng chiều dài chuỗi huấn luyện
nên được chọn cẩn thận theo mục đích sử dụng downstream dự định của mô hình.

6.2. Khả năng sao chép và truy xuất
Công trình gần đây (Jelassi et al., 2024) đã chỉ ra rằng Transformer có thể hiệu quả hơn đáng kể so với
state space model (SSM), một họ RNN mới phổ biến, trong việc học các tác vụ tổng hợp như sao chép
context hoặc truy xuất token liên quan từ context. Ngoài ra, Jelassi et al. (2024) chỉ ra
rằng các Transformer đã được pre-train như Pythia (Biderman et al., 2023) tốt hơn nhiều trong các tác vụ sao chép và
truy xuất tại thời điểm đánh giá so với các mô hình SSM đã được pre-train như Mamba (Gu và Dao,
2023). Trong phần này, chúng tôi điều tra hiệu quả của Griffin và Hawk trong việc học cách sao chép và
truy xuất token từ context. Ngoài ra, chúng tôi đánh giá các mô hình Hawk và Griffin đã được pre-train trên
một tác vụ tìm kiếm số điện thoại được thiết kế để kiểm tra cả khả năng sao chép và truy xuất.
11

--- TRANG 12 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

0 25K 50K 75K 100K
Training Steps0.000.250.500.751.00AccuracyMQA
Hawk
Griffin
(a) Tác vụ Selective Copying

2729211213
Eval Sequence Length0.000.250.500.751.00Accuracy
MQA
Hawk
Griffin
Train Length (b) Tác vụ Induction Heads

0 1000 2000 3000
Eval Sequence Length0.000.250.500.751.00AccuracyMQA
Hawk
Griffin
Train Length (c) Tác vụ Phonebook Lookup

Hình 6|Khám phá khả năng sao chép và truy xuất của Hawk và Griffin trên ba tác vụ tổng hợp.
Hình (a) và (b) hiển thị hiệu suất của các mô hình sâu 5 lớp trên bộ eval held out khi được huấn luyện
rõ ràng trên các tác vụ này. Hình (c) hiển thị hiệu suất trên tác vụ tìm kiếm số điện thoại khi
đánh giá các mô hình Hawk và Griffin 7B đã được pre-train so với baseline MQA Transformer 6B của chúng tôi.

Huấn luyện trên các tác vụ tổng hợp Để điều tra hiệu quả học cách sao chép và truy xuất token
liên quan từ context, chúng tôi huấn luyện trên hai tác vụ tổng hợp: Selective Copying và Induction Heads. Để có thể
so sánh Transformer với Hawk và Griffin, chúng tôi xem xét các mạng sâu 5 khối với chiều mô hình
64, tổng cộng khoảng 250K tham số, nơi Griffin sử dụng một attention cục bộ duy nhất ở
giữa mạng, trong khối thứ ba.

• Tác vụ selective copying: Trong tác vụ này, mô hình cần học sao chép các token dữ liệu từ một chuỗi
trong khi bỏ qua các token nhiễu từ context. Xem Phụ lục H để biết thêm chi tiết về thiết lập cho
tác vụ này. Tác vụ này được lấy cảm hứng từ Gu và Dao (2023), nơi các tác giả chỉ ra rằng Mamba
có thể giải quyết tác vụ này tốt hơn các SSM được đề xuất trước đây. Chúng tôi sử dụng kích thước từ vựng
16, và huấn luyện trên chuỗi có chiều dài 1024, chứa 16 token dữ liệu (được lấy mẫu ngẫu nhiên từ
từ vựng và tại các vị trí ngẫu nhiên), với phần còn lại của các token được đặt thành token nhiễu. Griffin
sử dụng kích thước cửa sổ attention cục bộ 512.

• Induction heads: Trong tác vụ này, mô hình cần học nhớ lại token ngay sau
một token đặc biệt. Điều này yêu cầu mô hình học token đặc biệt, và truy xuất token ngay
sau nó trong context. Nếu mô hình có thể học tác vụ, nó nên có thể ngoại suy
đến các chuỗi dài hơn đáng kể so với những gì được huấn luyện. Chúng tôi sử dụng kích thước từ vựng 16 và huấn luyện
trên chuỗi có chiều dài 256 nơi các token được lấy mẫu ngẫu nhiên, và chúng tôi lấy mẫu ngẫu nhiên
vị trí của token đặc biệt trong chuỗi. Griffin sử dụng cửa sổ attention cục bộ có kích thước 128.

Chúng tôi hiển thị kết quả trong Hình 6. Trên tác vụ Selective Copying, chúng tôi thấy rằng cả 3 mô hình đều có thể
giải quyết tác vụ một cách hoàn hảo. Khi so sánh tốc độ học trên tác vụ này, chúng tôi thấy Hawk chậm hơn đáng kể
so với Transformer, tương tự như quan sát được thực hiện bởi Jelassi et al. (2024), nơi các tác giả
chỉ ra rằng Mamba chậm hơn đáng kể trong việc học trên các tác vụ tương tự. Tuy nhiên thú vị, Griffin
hầu như không có độ chậm trễ, hiệu quả khớp với tốc độ học của Transformer, mặc dù chỉ sử dụng
một lớp attention cục bộ duy nhất.

Trên tác vụ Induction Heads, trong khi cả 3 mô hình đều có thể giải quyết tác vụ một cách hoàn hảo đến chiều dài chuỗi huấn luyện,
baseline Transformer của chúng tôi không thể ngoại suy đến chuỗi dài hơn trong đánh giá. Trong khi
baseline MQA của chúng tôi sử dụng RoPE, Gu và Dao (2023) có quan sát tương tự cho Transformer với một phạm vi
positional encoding. Chúng tôi thấy rằng Hawk có thể ngoại suy hoàn hảo trên tác vụ này đến chuỗi đánh giá
dài hơn nhiều bậc độ lớn so với chiều dài chuỗi huấn luyện. Đáng chú ý, Griffin, với
attention cục bộ của nó, cũng thể hiện khả năng đặc biệt để ngoại suy trên tác vụ này.
12

--- TRANG 13 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

Đánh giá các mô hình đã được pre-train Bây giờ chúng tôi đánh giá liệu khả năng sao chép và truy xuất có tự nhiên
xuất hiện trong các mô hình đã được pre-train hay không. Chúng tôi xem xét các mô hình Hawk và Griffin 7B
và baseline MQA Transformer 6B của chúng tôi, tất cả được huấn luyện trên 300B token trên bộ dữ liệu Massive Text.
Chúng tôi xem xét cùng tác vụ tìm kiếm phonebook được giới thiệu trong Jelassi et al. (2024), nơi chúng tôi cung cấp cho mô hình một phonebook tổng hợp
chứa tên và số, và mô hình được yêu cầu truy xuất số điện thoại đúng cho một tên.
Prompt cho mô hình là một phonebook bao gồm danh sách tên và số được lấy mẫu ngẫu nhiên có
chiều dài nhất định, theo sau bởi hai ví dụ được lấy mẫu ngẫu nhiên của tác vụ, theo sau bởi một tên được lấy mẫu ngẫu nhiên
từ phonebook mà mô hình cần truy xuất số điện thoại đúng.

Từ Hình 6(c), chúng tôi thấy rằng trong khi Hawk có thể làm khá tốt trên tác vụ cho chiều dài phonebook rất ngắn,
nó thất bại trong việc ghi nhớ và truy xuất số điện thoại đúng khi chiều dài phonebook tăng,
tương tự như quan sát được thực hiện bởi Jelassi et al. (2024) trên hiệu suất của mô hình Mamba trên tác vụ này.
Điều này không đặc biệt ngạc nhiên vì Hawk sử dụng trạng thái có kích thước cố định nhỏ. Baseline Transformer của chúng tôi có thể
gần như hoàn hảo giải quyết tác vụ này đến chiều dài chuỗi huấn luyện, nhưng thất bại trong việc truy xuất số điện thoại đúng
cho chiều dài context dài hơn chiều dài chuỗi huấn luyện. Thú vị, Griffin có thể hoàn hảo
giải quyết tác vụ này đến chiều dài context khớp với kích thước cửa sổ attention cục bộ 1024,
mặc dù chỉ sử dụng một lớp attention cục bộ duy nhất. Một khi chiều dài context đủ dài sao cho cửa sổ
attention cục bộ không bao phủ toàn bộ phonebook, hiệu suất bắt đầu giảm. Griffin cũng
có thể ngoại suy tốt hơn đến chiều dài chuỗi dài hơn so với Transformer. Trong khi hiệu suất
của Griffin là hứa hẹn cho khả năng của các mô hình có trạng thái có kích thước cố định giải quyết các tác vụ sao chép và truy xuất,
kết quả của chúng tôi gợi ý cần thêm công việc để cải thiện các khả năng này cho các mô hình như vậy.

7. Công trình Liên quan
Kiến trúc Transformer đã trở thành một lựa chọn thay thế có thể mở rộng hơn cho RNN. Transformer đạt được
khả năng mở rộng vượt trội thông qua huấn luyện song song hoàn toàn, trái ngược với các hạn chế vốn có của
RNN. Do cấu trúc xử lý tuần tự, RNN cổ điển bị chậm tốc độ huấn luyện
trong cả forward và backward propagation (Werbos, 1990). Để giảm thiểu vấn đề này, các nhà nghiên cứu
đã khám phá các phương pháp thay thế dựa trên RNN. Các ví dụ đáng chú ý bao gồm Quasi-RNN (Bradbury
et al., 2016), kết hợp convolution và RNN tuyến tính để song song hóa lớn hơn, và việc sử dụng
các cơ chế gating dựa trên đầu vào để song song hóa huấn luyện RNN tuyến tính (Martin và Cundy, 2017).

State-space Model (SSM) gần đây đã nổi lên như một công cụ mạnh mẽ để mô hình hóa chuỗi đầu vào dài.
Chúng thể hiện hiệu suất mạnh trên các tác vụ từ long-range arena benchmark (Tay et al., 2020),
và tạo âm thanh (Goel et al., 2022). SSM thành công tích hợp các khái niệm từ mô hình state-space cổ điển
(Kalman, 1960) với những khái niệm của RNN. Sự phụ thuộc vào hồi quy tuyến tính cho phép
tính toán trạng thái ẩn hiệu quả, thông qua các phép toán parallel scan hoặc convolution, dẫn đến tốc độ huấn luyện
so sánh với các mô hình Transformer. Mô hình S4 (Gu et al., 2021a) đề xuất tham số hóa tinh vi
được gọi là normal plus low-rank để chéo hóa tính toán hồi quy. S4D
tham số hóa SSM trực tiếp với ma trận trạng thái đường chéo và chỉ ra rằng nó hoạt động tốt như nhau
trong khi đơn giản hơn nhiều (Gu et al., 2022). S5 cũng chéo hóa hồi quy, và chỉ ra rằng
hồi quy có thể được tính bằng associative scan (Smith et al., 2022). Mô hình H3 (Dao et al.,
2022b) tổng quát hóa diễn giải hồi quy của linear attention (Katharopoulos et al., 2020). Hyena
(Poli et al., 2023) sử dụng kiến trúc tương tự, nhưng thay thế lớp S4D bằng kernel convolution toàn cục
được tham số hóa bởi MLP. RetNet (Sun et al., 2023) sử dụng thiết kế SSM đơn giản hơn với cơ chế gating
cho phép chúng song song hóa tính toán bằng một biến thể của multi-head attention. Orvieto et al.
(2023b) phân tích và ablate một cách hệ thống nhiều sửa đổi cho RNN tiêu chuẩn. Phát hiện của họ
chỉ ra rằng thông qua tham số hóa và khởi tạo tốt hơn, RNN tuyến tính đơn giản hóa (LRU), hoạt động
tốt như các biến thể SSM khác trên các tác vụ long-range khác nhau. RWKV (Peng et al., 2023) là một
RNN gần đây, được chỉ ra là cạnh tranh trên các tác vụ mô hình hóa ngôn ngữ, dựa trên một xấp xỉ linear attention khác
được lấy cảm hứng từ attention-free Transformer (Zhai et al., 2021). Đồng thời với công trình của chúng tôi, Gu và Dao
13

--- TRANG 14 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

(2023) phát triển kiến trúc SSM gọi là Mamba với cơ chế selection phụ thuộc đầu vào
và chỉ ra rằng nó đạt hiệu suất so sánh với Transformer với suy luận hiệu quả. Một số
mở rộng của Mamba đã được đề xuất (Wang et al., 2024; Zhu et al., 2024) cho các ứng dụng khác nhau.
Một gating phụ thuộc đầu vào tương tự như Mamba cũng được đề xuất bởi Gateloop (Katsch, 2023).

Linear attention (Katharopoulos et al., 2020) cung cấp xấp xỉ hiệu quả tính toán của
cơ chế self-attention bằng cách tuyến tính hóa attention, có thể được tính hồi quy như một
RNN tuyến tính. Trong khi cách tiếp cận này giảm đáng kể chi phí tính toán so với attention đầy đủ, nó thường
đi kèm với sự đánh đổi về hiệu suất mô hình. FlashAttention (Dao et al., 2022a) cải thiện tốc độ huấn luyện
của attention trên GPU bằng cách sử dụng hiệu quả phân cấp bộ nhớ. Một cách tiếp cận khác để
giảm chi phí tính toán của attention toàn cục, đang trở nên ngày càng phổ biến hơn, là
sử dụng sparse-local attention (Child et al., 2019) hoặc sliding window attention (Jiang et al., 2023).

8. Kết luận
Công trình này giới thiệu Hawk; một mô hình hồi quy kết hợp lớp hồi quy tuyến tính có cổng mới, RG-LRU.
Chúng tôi cũng giới thiệu Griffin; một mô hình lai kết hợp lớp RG-LRU với attention cục bộ.
Các mô hình này thể hiện hiệu suất mô hình hóa ngôn ngữ đặc biệt qua các quy mô khác nhau, với
loss held-out thể hiện mở rộng power-law khi tài nguyên tính toán tăng. Hawk vượt qua hiệu suất được báo cáo
của Mamba trên các tác vụ downstream khi được huấn luyện trên một nửa số token, trong khi Griffin hơi
vượt qua hiệu suất của Llama-2 khi được huấn luyện trên ít hơn 6 lần số token. Hơn nữa, chúng tôi
xác thực thực nghiệm các lợi ích thời gian suy luận của Hawk và Griffin và quan sát giảm độ trễ
và tăng thông lượng đáng kể so với các baseline Transformer của chúng tôi. Cuối cùng, Hawk và Griffin
thể hiện khả năng ngoại suy trên các chuỗi dài hơn so với những gì chúng được huấn luyện và có khả năng
học hiệu quả để sao chép và truy xuất dữ liệu qua khoảng cách dài. Những phát hiện này mạnh mẽ gợi ý rằng các
mô hình được đề xuất của chúng tôi cung cấp một lựa chọn thay thế mạnh mẽ và hiệu quả cho Transformer với attention toàn cục.

Lời cảm ơn
Chúng tôi cảm ơn Adam Paszke, Sharad Vikram, Trevor Gale, Sebastian Borgeaud, George Scrivener, Raia
Hadsell, Oriol Vinyals, Toby Boyd, Zhifeng Chen, Chris Dyer, Kelvin Xu, Andriy Mnih vì sự hướng dẫn
và lời khuyên của họ. Chúng tôi sử dụng hệ sinh thái DeepMind Jax (Bradbury et al., 2018) và đặc biệt cảm ơn
Andy Brock vì đã xây dựng framework nội bộ mà chúng tôi sử dụng để huấn luyện và đánh giá các mô hình.

Tài liệu tham khảo
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,
S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.

D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473, 2014.

I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv preprint
arXiv:2004.05150, 2020.

S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley, K. O'Brien, E. Hallahan, M. A. Khan, S. Purohit,
U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models across training
and scaling. In International Conference on Machine Learning, pages 2397–2430. PMLR, 2023.

J. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. arXiv preprint
arXiv:1611.01576, 2016.
14

--- TRANG 15 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. Van-
derPlas, S. Wanderman-Milne, và Q. Zhang. JAX: các biến đổi có thể kết hợp của chương trình Python+NumPy, 2018. URL http://github.com/google/jax.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing
Systems, volume 33, pages 1877–1901, 2020.

R. Child, S. Gray, A. Radford, và I. Sutskever. Generating long sequences with sparse transformers.
arXiv preprint arXiv:1904.10509, 2019.

J. Chung, C. Gulcehre, K. Cho, và Y. Bengio. Empirical evaluation of gated recurrent neural networks
on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

T. Dao, D. Fu, S. Ermon, A. Rudra, và C. Ré. Flashattention: Fast and memory-efficient exact
attention with io-awareness. In Advances in Neural Information Processing Systems, volume 35, pages
16344–16359, 2022a.

T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, và C. Ré. Hungry hungry hippos: Towards
language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022b.

Y. N. Dauphin, A. Fan, M. Auli, và D. Grangier. Language modeling with gated convolutional networks.
In International Conference on Machine Learning, pages 933–941. PMLR, 2017.

J. L. Elman. Finding structure in time. Cognitive Science, 14(2):179–211, 1990.

Gemini Team Google. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805, 2023.

K. Goel, A. Gu, C. Donahue, và C. Ré. It's raw! audio generation with state-space models. In
International Conference on Machine Learning, pages 7616–7633, 2022.

A. Gu và T. Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint
arXiv:2312.00752, 2023.

A. Gu, T. Dao, S. Ermon, A. Rudra, và C. Ré. Hippo: Recurrent memory with optimal polynomial
projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474–1487, 2020.

A. Gu, K. Goel, và C. Ré. Efficiently modeling long sequences with structured state spaces. arXiv
preprint arXiv:2111.00396, 2021a.

A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, và C. Ré. Combining recurrent, convolutional,
and continuous-time models with linear state space layers. In Advances in Neural Information
Processing Systems, volume 34, pages 572–585, 2021b.

A. Gu, A. Gupta, K. Goel, và C. Ré. On the parameterization and initialization of diagonal state space
models. arXiv preprint arXiv:2206.11893, 2022.

D. Hendrycks và K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.

S. Hochreiter và J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735–1780,
1997.

J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint
arXiv:2203.15556, 2022.
15

--- TRANG 16 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

S. Jelassi, D. Brandfonbrener, S. M. Kakade, và E. Malach. Repeat after me: Transformers are better
than state space models at copying. arXiv preprint arXiv:2402.01032, 2024.

A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel,
G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.

N. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai, N. Patil, S. Subramanian, A. Swing, B. Towles,
et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware
support for embeddings. In Proceedings of the 50th Annual International Symposium on Computer
Architecture, pages 1–14, 2023.

N. P. Jouppi, D. H. Yoon, M. Ashcraft, M. Gottscho, T. B. Jablin, G. Kurian, J. Laudon, S. Li, P. Ma, X. Ma,
et al. Ten lessons from three generations shaped google's tpuv4i: Industrial product. In 2021 ACM/IEEE
48th Annual International Symposium on Computer Architecture (ISCA), pages 1–14. IEEE, 2021.

R. E. Kalman. A new approach to linear filtering and prediction problems. Journal of Basic Engineering,
82, 1960.

J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, và
D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

A. Katharopoulos, A. Vyas, N. Pappas, và F. Fleuret. Transformers are RNNs: Fast autoregressive
transformers with linear attention. In International Conference on Machine Learning, pages
5156–5165. PMLR, 2020.

T. Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv preprint
arXiv:2311.01927, 2023.

A. Kazemnejad, I. Padhi, K. Natesan Ramamurthy, P. Das, và S. Reddy. The impact of positional encoding
on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024.

Y. LeCun, L. Bottou, G. B. Orr, và K.-R. Müller. Efficient backprop. In Neural Networks: Tricks of the
Trade, pages 9–50. Springer, 2002.

Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno,
A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):
1092–1097, 2022.

I. Loshchilov và F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,
2017.

S. Markidis, S. W. Der Chien, E. Laure, I. B. Peng, và J. S. Vetter. Nvidia tensor core programmability,
performance & precision. In 2018 IEEE international parallel and distributed processing symposium
workshops (IPDPSW), pages 522–531. IEEE, 2018.

E. Martin và C. Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint
arXiv:1709.04057, 2017.

H. Mehta, A. Gupta, A. Cutkosky, và B. Neyshabur. Long range language modeling via gated state
spaces. arXiv preprint arXiv:2206.13947, 2022.

T. Mikolov, M. Karafiát, L. Burget, J. Cernocký, và S. Khudanpur. Recurrent neural network based
language model. In INTERSPEECH 11th Annual Conference of the International Speech Communication
Association, pages 1045–1048, 2010.
16

--- TRANG 17 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand,
P. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efficient large-scale language model training on
gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, pages 1–15, 2021.

T. Norrie, N. Patil, D. H. Yoon, G. Kurian, S. Li, J. Laudon, C. Young, N. Jouppi, và D. Patterson. The
design process for Google's training chips: TPUv2 and TPUv3. IEEE Micro, 41(2):56–63, 2021.

A. Orvieto, S. De, C. Gulcehre, R. Pascanu, và S. L. Smith. On the universality of linear recurrences
followed by nonlinear projections. arXiv preprint arXiv:2307.11888, 2023a.

A. Orvieto, S. L. Smith, A. Gu, A. Fernando, C. Gulcehre, R. Pascanu, và S. De. Resurrecting recurrent
neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023b.

B. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho, H. Cao, X. Cheng, M. Chung, M. Grella, K. K.
GV, et al. Rwkv: Reinventing RNNs for the transformer era. arXiv preprint arXiv:2305.13048, 2023.

M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, và C. Ré. Hyena
hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023.

J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, et al. Scaling language models: Methods, analysis & insights from training Gopher. arXiv
preprint arXiv:2112.11446, 2021.

S. Rajbhandari, J. Rasley, O. Ruwase, và Y. He. Zero: Memory optimizations toward training trillion
parameter models. In SC20: International Conference for High Performance Computing, Networking,
Storage and Analysis, pages 1–16. IEEE, 2020.

N. Shazeer. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150,
2019.

N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.

M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, và B. Catanzaro. Megatron-lm: Training multi-
billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.

H. T. Siegelmann và E. D. Sontag. Turing computability with neural nets. Applied Mathematics Letters,
4(6):77–80, 1991. ISSN 0893-9659.

J. T. Smith, A. Warrington, và S. W. Linderman. Simplified state space layers for sequence modeling.
arXiv preprint arXiv:2208.04933, 2022.

J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, và Y. Liu. Roformer: Enhanced transformer with rotary
position embedding. arXiv preprint arXiv:2104.09864, 2021.

Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, và F. Wei. Retentive network: A successor
to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023.

I. Sutskever, O. Vinyals, và Q. V. Le. Sequence to sequence learning with neural networks. In Advances
in Neural Information Processing Systems, pages 3104–3112, 2014.

Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, và D. Metzler.
Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020.
17

--- TRANG 18 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, et al. LLama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971, 2023.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin.
Attention is all you need. In Advances in Neural Information Processing Systems, volume 30, 2017.

J. Wang, T. Gangavarapu, J. N. Yan, và A. M. Rush. Mambabyte: Token-free selective state space
model. arXiv preprint arXiv:2401.13660, 2024.

P. J. Werbos. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE,
78(10):1550–1560, 1990.

Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey,
et al. Google's neural machine translation system: Bridging the gap between human and machine
translation. arXiv preprint arXiv:1609.08144, 2016.

R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, và T. Liu. On layer
normalization in the transformer architecture. In International Conference on Machine Learning,
pages 10524–10533. PMLR, 2020.

S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, và J. Susskind. An attention free
transformer. arXiv preprint arXiv:2105.14103, 2021.

B. Zhang và R. Sennrich. Root mean square layer normalization. Advances in Neural Information
Processing Systems, 32, 2019.

L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, và X. Wang. Vision mamba: Efficient visual representation
learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024.
18

--- TRANG 19 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

A. RG-LRU Recurrence Gate
Trong Hình 7, chúng tôi thể hiện hành vi của các cơ chế gating khác nhau được áp dụng trên trọng số hồi quy
𝑎.

0.0 0.2 0.4 0.6 0.8 1.0
rt0.00.20.40.60.81.0+
a=0.5
RG-LRU
Mamba
GRU
LRU update
0.0 0.2 0.4 0.6 0.8 1.0
rt0.00.20.40.60.81.0+
a=0.9
RG-LRU
Mamba
GRU
LRU update
0.0 0.2 0.4 0.6 0.8 1.0
rt0.00.20.40.60.81.0+
a=0.99
RG-LRU
Mamba
GRU
LRU update

Hình 7|Hành vi của các cơ chế gating khác nhau được áp dụng trên trọng số hồi quy 𝑎 (lưu ý rằng
trong ký hiệu của Mamba đây là −𝐴).

Triển khai Chúng tôi triển khai recurrence gate của chúng tôi, như được định nghĩa trong Phần 2.4, theo một hình thức hơi khác,
nhưng tương đương về mặt toán học, để ổn định số. Cụ thể, chúng tôi tính logarithm
của 𝑎𝑡 và sau đó mũ hóa nó, thay vì tính sigmoid và sau đó lấy lũy thừa:

log𝑎𝑡=log𝑎𝑐𝑟𝑡=log𝜎(Λ)𝑐𝑟𝑡=−𝑐softplus(Λ)⊙𝑟𝑡. (6)

Hành vi gate Gate của chúng tôi khá khác so với các gate tiêu chuẩn khác trong văn liệu. Cụ thể,
hầu hết các cơ chế gating, như được sử dụng trong Mamba và GRU, cho phép gate nội suy
hoàn toàn giữa trạng thái ẩn và quan sát mới. Còn của chúng tôi mặt khác thiên về
việc giữ lại thông tin, và không cho phép loại bỏ hoàn toàn đóng góp của ℎ𝑡−1 (điều này phụ thuộc, tuy nhiên,
vào giá trị của Λ). Để thể hiện điều này, chúng tôi phân tích trọng số tương đối của 𝑥𝑡 so với ℎ𝑡−1 trong
đầu ra 𝑦𝑡. Cho một hồi quy tổng quát, chúng tôi định nghĩa điều này là:

ℎ𝑡=𝛼(𝑟𝑡)ℎ𝑡−1+𝛽(𝑟𝑡)𝑥𝑡. (7)

Cho mô hình của chúng tôi, chúng tôi có 𝛼(𝑟𝑡)=𝑎𝑡=𝑎𝑐𝑟𝑡 và 𝛽(𝑟𝑡)=√︁
1−𝛼(𝑟𝑡)2. Cho gating kiểu GRU tiêu chuẩn, chúng tôi có
𝛼(𝑟𝑡)=1−𝑟𝑡 và 𝛽(𝑟𝑡)=𝑟𝑡. Cho Mamba, giả sử trong ký hiệu của họ 𝐵=1,𝐶=1, thì 𝛼(𝑟𝑡)=(1−𝑟𝑡)−𝐴
và 𝛽(𝑟𝑡)=(1−𝛼)/𝐴. Hành vi của các cơ chế gating khác nhau được mô tả trong Hình 7, nơi
để rõ ràng chúng tôi cũng đã bao gồm giá trị cập nhật của LRU (Orvieto et al., 2023b), không có
gating. Như có thể thấy, gating Mamba gần như giống hệt với GRU cho các giá trị của 𝐴 gần 1, với
độ lệch nhỏ ở các giá trị nhỏ hơn. Mặt khác, cơ chế gating của chúng tôi thực hiện một
nội suy phi tuyến rất khác giữa việc loại bỏ hoàn toàn đầu vào 𝑥𝑡 và cập nhật của LRU.

B. Complex-Gated Linear Recurrent Unit (CG-LRU)
Trong Phần 2.4 chúng tôi đã định nghĩa lớp hồi quy của chúng tôi, tuy nhiên nó có thể được mở rộng thêm để sử dụng
số phức. Để đạt được điều này, trước tiên chúng tôi tham số hóa hồi quy đường chéo phức thông qua ˜𝑎=𝜎(Λ)𝑒𝑖𝜃, trong đó
𝜃 là một tham số có thể học. Ngoài ra, chúng tôi chia đầu vào 𝑥𝑡 theo chiều kênh, và diễn giải
19

--- TRANG 20 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

nửa đầu của nó như phần thực của một vector phức, và phần thứ hai như phần ảo của cùng
vector phức:

𝑥𝑡=
𝑥1
𝑡
𝑥2
𝑡
(8)

˜𝑥𝑡=𝑥1
𝑡+𝑖𝑥2
𝑡. (9)

Với điều này chúng tôi viết lại các phương trình cho LRU (xem eq.4) là:

𝑟𝑡=𝜎(𝑊𝑎𝑥𝑡+𝑏𝑎),recurrence gate (10)
𝑖𝑡=𝜎(𝑊𝑥𝑥𝑡+𝑏𝑥),input gate (11)
˜𝑎𝑡=˜𝑎𝑐𝑟𝑡, (12)
˜ℎ𝑡=˜𝑎𝑡⊙˜ℎ𝑡−1+√︃
1−|˜𝑎𝑡|2⊙(𝑖𝑡⊙˜𝑥𝑡). (13)

Chúng tôi đánh dấu tất cả các biến phức với ˜· để rõ ràng. Lưu ý rằng số chiều của 𝑟𝑡,𝑖𝑡,˜𝑎𝑡 và ˜ℎ𝑡
là một nửa so với đầu vào thực 𝑥𝑡. Cuối cùng, để tính đầu ra chúng tôi xếp chồng phần thực và ảo
của ℎ𝑡 thành một vector duy nhất 𝑦𝑡:

𝑦𝑡=Real(˜ℎ𝑡)
Imaginary(˜ℎ𝑡)
(14)

C. Siêu tham số Quy mô Mô hình
Trong Bảng 2, chúng tôi trình bày các siêu tham số của các mô hình ở các quy mô khác nhau. Các siêu tham số này
được chia sẻ cho tất cả các họ mô hình mà chúng tôi khám phá trong bài báo này.

Bảng 2|Các siêu tham số mô hình chính được xem xét cho các kích thước mô hình khác nhau. Các siêu tham số này
được chia sẻ qua các kiến trúc khác nhau mà chúng tôi đã kiểm tra.

Model ModelWidth RNNWidth Depth MLPExpansion Attention TrainingTokens
Size (𝑫) ( 𝑫𝑹𝑵𝑵 )(𝑵)Factor( 𝑴) Heads (OptimalScaling)
100M 768 1024 12 3 6 1.9B
200M 1024 1536 12 3 8 3.9B
400M 1536 2048 12 3 12 7.8B
1.3B 2048 2560 24 3 16 25B
3B 3072 4096 24 3 24 60B
7B 4096 5632 32 3 32 132.5B
14B 5120 8192 40 3 40 300B

D. Hồi quy Tuyến tính Hiệu quả trên Thiết bị
Bước đầu tiên trong tối ưu hóa tính toán nằm ở việc xác định nút thắt cổ chai hiệu suất chính
trên phần cứng mục tiêu. Đối với hầu hết các accelerator, các yếu tố hạn chế chính là thông lượng tính toán
(FLOP/s) và băng thông bộ nhớ giữa high-bandwidth memory (HBM) và fast vector
memory (VMEM). Trong khi các yếu tố như dung lượng HBM và giao tiếp host-device có liên quan, các kỹ thuật
như ZeRO sharding và pipelined data transfer cung cấp các giải pháp thực tế. Thiết kế accelerator hiện đại
thường ưu tiên tỷ lệ FLOP-to-byte cao để đáp ứng các workload nơi tính toán
vượt trội hơn đáng kể so với việc truyền bộ nhớ. Chúng tôi hiển thị thông số kỹ thuật chính của TPU-v3 pod (hai chip
mỗi pod) trong Bảng 3, mà chúng tôi sử dụng cho tất cả thí nghiệm.
20

--- TRANG 21 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

Specification TPU-v3Pod
HBMcapacity 32GB
HBMbandwidth 900GB/s
PeakMXUcompute 123TFLOPs/s(bfloat16)
PeakMXUFLOPs-to-byte-ratio 136
PeakVPUcompute 3.8TFLOPs/s
PeakVPUFLOPs-to-byte-ratio 4.2

Bảng 3|Thông số kỹ thuật phần cứng cho TPU-v3 pod.

0K2K4K6K8K10K12K14K16K
Sequence length0102030405060Run time (ms)Linear(Pallas)
Linear(Jax)
Associative(bf16)
Associative(f32)
(a) Thời gian chạy Scan

400m 1b 7b
Model size0.85x0.81x0.89x1.00x 1.00x 1.00x1.58x
1.27x1.41x (b) Thời gian chạy Hawk

Hình 8|a) Thời gian chạy của các triển khai khác nhau của phép toán scan trên TPU-v3 ở các
chiều dài chuỗi khác nhau. Batch size của đầu vào được cố định ở 8 và chiều của mỗi token là 1024.
b) Thời gian chạy tương đối của mô hình Hawk khi sử dụng các triển khai khác nhau của phép toán scan,
tham chiếu đến cái có triển khai Jax scan native.

D.1. Tính toán nhân ma trận
Một phép nhân ma trận điển hình của ma trận 𝐷×𝐷 với ma trận 𝐷×𝑁 có 2𝑁𝐷2 FLOP và 2(𝐷2+2𝑁𝐷)
byte để truyền (cả đọc và ghi) dẫn đến tỷ lệ FLOP/byte là 𝑁𝐷
𝐷+𝑁. Khi 𝐷>>𝑁 và
chạy trên TPU-v3 điều này có nghĩa là chiều 𝑁 phải ít nhất 136 để bão hòa thiết bị, trong
trường hợp đó phép toán là "compute bound", hoặc ngược lại hầu hết thời gian sẽ được dành cho việc chờ
truyền bộ nhớ, trong trường hợp đó phép toán là "memory bound".

D.2. Thời gian chạy Scan
Trong Hình 8(a) chúng tôi thể hiện rằng trên TPU-v3 kernel Pallas của chúng tôi đạt được tăng tốc gần x3 so với
triển khai Jax naïve. Ngoài ra, associative scan chậm hơn đáng kể, ngay cả khi chạy hoàn toàn
trong độ chính xác bfloat16. Hình 8(b) thể hiện rằng những lợi ích này cũng chuyển thành cải thiện đáng kể
về thời gian huấn luyện tổng thể mỗi bước của mô hình Hawk đầy đủ ngay cả ở quy mô 7b. Để hoàn chỉnh
chúng tôi cũng đã thêm thời gian chạy của associative scan, có thể chậm hơn đến 50%.

E. Kích thước Cửa sổ Attention Cục bộ của Griffin
Griffin sử dụng cả các khối hồi quy cũng như các lớp attention cục bộ trong các khối temporal mixing của nó. Cho
tất cả thí nghiệm trước đây được hiển thị sử dụng chiều dài chuỗi huấn luyện 2048, chúng tôi sử dụng kích thước cửa sổ attention
cục bộ 1024. Bây giờ chúng tôi điều tra cách hiệu suất của các kích thước cửa sổ khác nhau cho lớp
attention cục bộ thay đổi với chiều dài chuỗi huấn luyện.

Chúng tôi xem xét các mô hình 400M tham số được huấn luyện trên chiều dài chuỗi 2048, 4096 và 8192 token,
21

--- TRANG 22 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

Train Length 2K Train Length 4K Train Length 8K2.502.522.542.562.582.602.622.642.66Validation Loss512
1K
2K
1K1K
2K4K
1K
2K2K
4K8K1K
2K
4KMQA Transformer
Griffin

Hình 9|Hiệu suất của các mô hình Griffin và MQA Transformer 400M tham số sử dụng các kích thước cửa sổ
attention cục bộ khác nhau và các chiều dài chuỗi huấn luyện khác nhau. Kích thước cửa sổ của các lớp attention cục bộ
được hiển thị phía trên mỗi thanh trong biểu đồ. Chúng tôi lưu ý rằng MQA Transformer attention toàn cục tốt hơn nhiều
so với các biến thể attention cục bộ của MQA Transformer (nơi kích thước cửa sổ nhỏ hơn
chiều dài chuỗi huấn luyện). Hơn nữa, chúng tôi thấy rằng việc sử dụng kích thước cửa sổ attention cục bộ cố định
1024 (ký hiệu '1K' trong biểu đồ) cho mô hình Griffin vượt trội so với tất cả baseline MQA Transformer attention toàn cục và attention cục bộ
qua tất cả chiều dài chuỗi huấn luyện.

nơi chúng tôi giữ tổng số token huấn luyện cố định. Cho mỗi chiều dài chuỗi, chúng tôi huấn luyện các mô hình Griffin
sử dụng các kích thước cửa sổ attention cục bộ khác nhau. Như baseline, chúng tôi huấn luyện MQA Transformer sử dụng
các lớp attention toàn cục, cũng như MQA Transformer sử dụng các lớp attention cục bộ với các kích thước cửa sổ
khác nhau. Kết quả được hiển thị trong Hình 9, nơi các kích thước cửa sổ được sử dụng được hiển thị trên đầu mỗi thanh
(các thanh MQA Transformer với kích thước cửa sổ bằng chiều dài chuỗi huấn luyện là baseline MQA Transformer attention toàn cục).

Từ Hình 9, chúng tôi thấy rằng đáng chú ý, ngay cả khi sử dụng kích thước cửa sổ cố định 1024 cho các
lớp attention cục bộ trong Griffin, nó vượt trội so với baseline MQA Transformer attention toàn cục qua tất cả
chiều dài chuỗi được kiểm tra. Tuy nhiên, đáng lưu ý rằng khoảng cách hiệu suất giữa Griffin với
cửa sổ attention cục bộ 1024 và MQA Transformer attention toàn cục giảm khi chiều dài chuỗi
tăng. Do đó, nếu chiều dài chuỗi tăng thêm, có thể quan trọng để từ từ cũng tăng kích thước
cửa sổ attention cục bộ. Trong thực tế, phần cứng được sử dụng cũng sẽ quyết định mạnh mẽ kích thước cửa sổ attention cục bộ tối ưu
về tốc độ huấn luyện và suy luận. Cuối cùng, chúng tôi lưu ý rằng MQA Transformer
sử dụng attention cục bộ thuần túy (kích thước cửa sổ nhỏ hơn chiều dài chuỗi huấn luyện) hoạt động đáng kể
tệ hơn so với cả MQA Transformer attention toàn cục, cũng như Griffin.

F. Tốc độ Suy luận
F.1. Ước tính tính bị giới hạn bởi bộ nhớ
Tốc độ suy luận của các mô hình ngôn ngữ tại thời điểm decode bị giới hạn bởi việc tải bộ nhớ. Như đã mô tả
trong 4.2 RNN tuyến tính bị giới hạn bởi bộ nhớ. Trong phần sau chúng tôi sẽ chỉ ra điều này đúng cho
các thành phần khác (là các lớp tuyến tính và self-attention) trong các mô hình hồi quy và Transformer
của chúng tôi.

F.2. Ước tính tính bị giới hạn bởi bộ nhớ của các lớp tuyến tính
Như được hiển thị trong D.1 chiều ngoài (thường bao gồm batch 𝐵 và chiều sequence length 𝑇)
phải ít nhất 136 để trở thành compute bound. Tại thời điểm decode 𝑇=1 và nếu chúng ta giả sử 𝐵≲128 thì
bất kỳ lớp tuyến tính nào sẽ bị memory bound tại thời điểm decode.
22

--- TRANG 23 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

F.3. Ước tính tính bị giới hạn bởi bộ nhớ của self-attention
Trong phần sau, chúng tôi tính tỷ lệ truy cập bộ nhớ với các phép toán số học cho tính toán attention
cho bước decode thứ 𝐿, để chỉ ra nó cũng bị memory-bound.

Để đơn giản hóa phân tích sau, chúng tôi giả sử rằng chúng ta bắt đầu từ một prompt trống (hoặc tương đương
giả sử rằng prefill chứa 0 token).

Khi lấy mẫu auto-regressively từ MHA hoặc MQA, thực hành tiêu chuẩn là lưu các vector key và value
trong Key-Value (KV) cache. Cho 𝐿 token đã được lấy mẫu, KV cache do đó sẽ có
kích thước 2×𝐿×𝐻𝑘×𝑑ℎ𝑒𝑎𝑑 cho mỗi chuỗi trong batch, trong đó 𝐻𝑘 biểu thị số lượng head được sử dụng cho
key và value, và 𝑑ℎ𝑒𝑎𝑑 biểu thị chiều của các vector key và value trong mỗi head.

Cho việc lấy mẫu token thứ 𝐿, một khi chúng ta tính các vector query, key và value tương ứng với
token thứ 𝐿. Trọng số attention và đầu ra của lớp attention sau đó được tính bằng cách sử dụng vector key và value thứ 𝐿
trong KV cache. Điều này yêu cầu 𝑂(𝐿𝐷) phép toán tổng thể và nó yêu cầu tải
cache 𝑂(𝐿×𝐻𝑘×𝑑ℎ𝑒𝑎𝑑) kích thước từ HBM, cho mỗi chuỗi trong minibatch. Kích thước của KV
cache, cũng như số lượng FLOP, tỷ lệ tuyến tính với batch size 𝐵.

Cho MHA, số lượng head cho key và value 𝐻𝑘 thường bằng số lượng head được sử dụng
cho query 𝐻. Cho MQA, một head duy nhất được sử dụng cho key và value, tức là, 𝐻𝑘=1. Do đó cho MQA,
kích thước của KV cache nhỏ hơn một hệ số 𝐻𝑘 (tức là, có kích thước 2×𝐿×𝑑ℎ𝑒𝑎𝑑).

def attention_sampling(q, k, v):
    """ Auto-regressive sampling via attention.
    For MHA, h_k = h. For MQA, h_k = 1.
    Args:
        q : The q vector for current token of shape [b, h, k]
        k : The keys of the current + previous tokens [b, L, h_k, k]
        v : the values of the current + previous tokens [b, L, h_k, v]
    """
    logits = einsum("bhk,bLk->bhL", q, k) # O(bhLk)
    weights = softmax(logits)
    output = einsum("bhL,bLv->bhv", weights, v) # O(bhLv)
    return output

Cho batch size 𝐵, tỷ lệ truy cập bộ nhớ với FLOP cho tính toán attention là
𝑂(𝐵×𝐿×𝐻𝑘×𝑑ℎ𝑒𝑎𝑑
𝐵×𝐿×𝐷). Cho các kiến trúc Transformer điển hình, 𝐷=𝐻×𝑑ℎ𝑒𝑎𝑑 và thêm 𝐻𝑘=𝐻 cho MHA
và 𝐻𝑘=1 cho MQA. Do đó tỷ lệ truy cập bộ nhớ với flops là 𝑂(1) cho MHA và 𝑂(1/𝐻) cho MQA.

Như đã giải thích trong 3, để trở thành compute bound trên TPU-v3 tỷ lệ FLOP-to-byte 136 là cần thiết,
và do đó cả MHA và MQA thường sẽ bị memory bound. Tuy nhiên, MQA tăng tốc đáng kể
suy luận Transformer (khi so sánh với MHA), vì nó giảm tính memory boundedness
theo hệ số 𝐻.

F.4. Kích thước cache
Trong phần sau chúng tôi phân tích kích thước tương đối của cache được sử dụng trong các mô hình hồi quy và Transformer của chúng tôi.
Tất cả kích thước cache tỷ lệ tuyến tính với batch size và trong phần sau chúng tôi giả sử 𝐵=1.

F.4.1. Kích thước của KV cache
Cho attention, KV cache có kích thước 2𝑁𝑇ℎ𝑘𝑑ℎ𝑒𝑎𝑑, trong đó 𝑁 biểu thị số lượng lớp attention (
độ sâu), 𝑇 biểu thị chiều dài của chuỗi, ℎ𝑘 biểu thị số lượng KV head và 𝑑ℎ𝑒𝑎𝑑 biểu thị
chiều head. Trong suốt công trình này, 𝑑ℎ𝑒𝑎𝑑=128. Cho MHA, ℎ𝑘𝑑ℎ𝑒𝑎𝑑=𝐷, trong khi cho MQA, ℎ𝑘=1.
(Do đó chúng tôi mong đợi MQA nhanh hơn khi decode chuỗi dài so với MHA, vì kích thước của
KV cache nhỏ hơn đáng kể và ít bộ nhớ cần được di chuyển.)
23

--- TRANG 24 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

Cho cả MHA hoặc MQA kích thước của KV cache có thể vượt quá số lượng tham số mô hình khi
chiều dài chuỗi 𝑇 lớn. Do đó chúng tôi mong đợi quan sát một sự chuyển đổi từ một chế độ 'parameter bound'
khi chiều dài chuỗi ngắn, trong đó tốc độ decode bị chi phối bởi thời gian
cần để tải tham số mô hình trên thiết bị, đến một chế độ 'cache bound' cho chuỗi lớn, nơi
tốc độ decode bị chi phối bởi thời gian cần để tải KV cache.

F.4.2. Kích thước của trạng thái hồi quy
Trạng thái hồi quy của một lớp RG-LRU duy nhất có kích thước 𝐷𝑅𝑁𝑁, và tổng kích thước trạng thái cho toàn bộ mô hình Hawk
là 𝑁𝐷𝑅𝑁𝑁≈4𝐵𝑁𝐷/3. Không giống như KV cache, trạng thái này không tăng với chiều dài chuỗi và
rất nhỏ so với kích thước tham số. Do đó chúng tôi mong đợi tốc độ decode của RG-LRU
bị chi phối bởi thời gian cần để tải tham số mô hình trên thiết bị ở tất cả chiều dài chuỗi.

Một cân nhắc tương tự áp dụng cho kích thước trạng thái convolution 1D. Cho chiều rộng bộ lọc temporal
kích thước 4, trạng thái có kích thước (4−1)𝐷𝑅𝑁𝑁=3𝐷𝑅𝑁𝑁=4𝐷 cho mỗi khối hồi quy cũng nhỏ hơn đáng kể
so với kích thước tham số.

F.4.3. Cache attention cục bộ
Một lớp MQA cục bộ duy nhất có kích thước cache bị giới hạn bởi 2𝑇𝑊𝑆𝑑ℎ𝑒𝑎𝑑, trong đó 𝑇𝑊𝑆 biểu thị kích thước cửa sổ attention
cục bộ. Miễn là 𝑇𝑊𝑆≲𝐷2/(𝐵𝑑ℎ𝑒𝑎𝑑), kích thước của cache attention cục bộ cũng nhỏ
so với số lượng tham số. Do đó chúng tôi mong đợi tốc độ decode của Griffin tương tự như
tốc độ decode của mô hình Hawk.

G. Cải thiện Dự đoán Token Tiếp theo với Context Dài hơn: Kết quả Bổ sung
Hình 10 hiển thị kết quả bổ sung thể hiện hiệu suất dự đoán token tiếp theo ở các chiều dài context khác nhau
trên bộ dữ liệu held out của các bài viết arXiv. Chúng tôi thấy rằng kết quả trên bộ dữ liệu này
tương tự về mặt định tính với kết quả được hiển thị trong Hình 5.

128 256 512 1K 2K 4K 8K 16K 32K
Token position1.501.752.002.252.502.753.003.253.50Mean-so-far NLLGriffin
Hawk
MQA NoPE
MQA RoPE
128 256 512 1K 2K 4K 8K16K 32K 65K131K
Token position1.501.752.002.252.502.753.003.253.50Mean-so-far NLLGriffin-2k
Griffin-8k
Hawk-2k
Hawk-8k

Hình 10|Hiệu suất đánh giá của các mô hình 1B tham số qua một phạm vi chiều dài chuỗi
trên bộ đánh giá held-out của các bài viết ArXiv. Bên trái, chúng tôi so sánh hiệu suất của các
mô hình khác nhau được huấn luyện với chiều dài chuỗi 2048, được đánh giá với chiều dài chuỗi lên đến 32,768. Bên
phải, chúng tôi so sánh Griffin và Hawk khi được huấn luyện tương ứng trên chiều dài chuỗi 2048 (2k) và 8192 (8k).
Kết quả tương tự về mặt định tính với đánh giá trên Sách được trình bày trong Hình 5.

H. Chi tiết Bổ sung của Các Tác vụ Sao chép và Truy xuất
Hình 11 là một minh họa của các tác vụ Selective Copying và Induction Heads.

Trong tác vụ Selective Copying, mô hình cần học sao chép các token dữ liệu (token có màu trong Hình
11) từ một chuỗi trong khi bỏ qua các token nhiễu (token trắng trong Hình 11). Các token bị gạch chéo trong
đầu ra trong Hình 6 biểu thị các token được mask out trong loss.
24

--- TRANG 25 ---
Griffin: Kết hợp Hồi quy Tuyến tính Có cổng với Attention Cục bộ cho Mô hình Ngôn ngữ Hiệu quả

Model
(a) Tác vụ Selective Copying

Model (b) Tác vụ Induction Heads

Hình 11|Minh họa của tác vụ Selective Copying (trái) và tác vụ Induction Heads (phải).

Trong tác vụ Induction Heads, mô hình cần học nhớ lại token ngay sau một token
đặc biệt (token đen trong Hình 11). Như trước, các token bị gạch chéo trong đầu ra biểu thị các token
được mask out trong loss.
25
