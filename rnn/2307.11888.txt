# 2307.11888.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rnn/2307.11888.pdf
# File size: 4779588 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Universality of Linear Recurrences Followed by Non-linear Projections:
Finite-Width Guarantees and Benefits of Complex Eigenvalues
Antonio Orvieto1Soham De2Caglar Gulcehre3Razvan Pascanu2Samuel L. Smith2
Abstract
Deep neural networks based on linear RNNs in-
terleaved with position-wise MLPs are gaining
traction as competitive approaches for sequence
modeling. Examples of such architectures include
state-space models (SSMs) like S4, LRU, and
Mamba: recently proposed models that achieve
promising performance on text, genetics, and
other data that require long-range reasoning. De-
spite experimental evidence highlighting these
architectures’ effectiveness and computational ef-
ficiency, their expressive power remains relatively
unexplored, especially in connection to specific
choices crucial in practice – e.g., carefully de-
signed initialization distribution and potential use
of complex numbers. In this paper, we show that
combining MLPs with both real or complex lin-
ear diagonal recurrences leads to arbitrarily pre-
cise approximation of regular causal sequence-to-
sequence maps. At the heart of our proof, we rely
on a separation of concerns: the linear RNN pro-
vides a lossless encoding of the input sequence,
and the MLP performs non-linear processing on
this encoding. While we show that real diagonal
linear recurrences are enough to achieve universal-
ity in this architecture, we prove that employing
complex eigenvalues near unit disk – i.e., empiri-
cally the most successful strategy in S4 – greatly
helps the RNN in storing information. We con-
nect this finding with the vanishing gradient issue
and provide experiments supporting our claims.
Note: The preliminary version of this manuscript (ICML
workshop version) only contains a subset of the results. Our
follow-up (Cirone et al., 2024) covers expressive power of
gated SSMs such as Mamba (Gu & Dao, 2023).
1ELLIS Institute T ¨ubingen, Max Planck Institute for Intelli-
gent Systems, T ¨ubingen AI Center, T ¨ubingen, Germany2Google
Deepmind3EPFL. Correspondence to: Antonio Orvieto <anto-
nio@tue.ellis.eu >.
Proceedings of the 41stInternational Conference on Machine
Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).1. Introduction
Attention (Vaswani et al., 2017) has supplanted LSTMs
(Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al.,
2014) as the dominant sequence-to-sequence mechanism
for deep learning. However, a promising line of research
sparked by the S4 model (Gu et al., 2021) has initiated a
come-back of recurrent sequence models in simplified linear
form. A growing family of ‘state-space’ models (SSMs) (Gu
et al., 2021; Hasani et al., 2022; Smith et al., 2023; Gu et al.,
2022; Li et al., 2022a; Orvieto et al., 2023; Gu & Dao, 2023)
has emerged which interleave recurrent linear complex-
valued1layers with other components such as position-
wise multi-layer perceptrons (MLPs), gates (Dauphin et al.,
2017), residual connections (He et al., 2016), and normal-
ization layers (Ioffe & Szegedy, 2015; Ba et al., 2016).
Traditional SSMs achieve state-of-the-art results on the long-
range arena (Tay et al., 2020) and show outstanding perfor-
mance in various domain including vision (Nguyen et al.,
2022), audio (Goel et al., 2022), biological signals (Gu
et al., 2021), reinforcement learning (Lu et al., 2023) and
online learning (Zucchet et al., 2023b). SSMs, when aug-
mented with a selectivity mechanism, are also successfully
applied to text (Katsch, 2023; Gu & Dao, 2023; De et al.,
2024), and are sometimes used in combination with softmax
attention (Fu et al., 2023; Wang et al., 2023) or linear atten-
tion (Peng et al., 2023; Sun et al., 2023; Katsch, 2023; Yang
et al., 2023). They gained significant interest in the literature
due to two key factors. First, their computational complexity
scales linearly in sequence length, while transformers scale
quadratically (Vaswani et al., 2017; Katharopoulos et al.,
2020). Second, unlike LSTMs and GRUs, training can be
efficiently parallelized (Martin & Cundy, 2017)2.
1The performance of non-selective SSMs trained from scratch
on classification tasks is greatly affected by the choice of number
field: using complex diagonal recurrences greatly improves perfor-
mance, see discussion in (Gu et al., 2022; Orvieto et al., 2023) and,
in particular, the S4D-Lin initialization. For Mamba on next-token
prediction tasks, real numbers instead are effective. As shown
by Amos et al. (2023), this is an effect due to a different learning
paradigm which necessitates further investigations.
2At the current research state, effective parallelization in the
time dimension is possible for recurrences which are linear in
the state, and where the state-to-state transition is computation-
ally light. Linear dense RNNs are parallelizable in principle but
1arXiv:2307.11888v3  [cs.LG]  5 Jun 2024

--- PAGE 2 ---
Universality of Linear Recurrences Followed by Non-linear Projections
x1=Bu1x2=Λx1+Bu2x3=Λx2+Bu3xL=ΛxL−1+BuL……N=128N=256N=512Reconstruction from last hidden state29/05/2023, 16:42NN SVG
https://alexlenail.me/NN-SVG/ 1/1÷
oon29/05/2023, 16:42NN SVG
https://alexlenail.me/NN-SVG/ 1/1÷
oon29/05/2023, 16:42NN SVG
https://alexlenail.me/NN-SVG/ 1/1÷
oon29/05/2023, 16:42NN SVG
https://alexlenail.me/NN-SVG/ 1/1÷
oon
Output sequence
Input sequenceMLP: from compressed past history to output
Figure 1. Illustration of a Linear RNN + position-wise MLP on flattened MNIST (LeCun, 1998) digits. In our construction, the role
of the linear RNN is to compress (if possible) and store the input sequence into the hidden state: from hidden states one can recover
past tokens using a linear transformation (see §3.1). As the hidden state size Nincreases, the reconstructions becomes more and more
faithful. The MLP (same for all tokens) takes this representation as input, and is able to reproduce the action of any sufficiently regular
sequence-to-sequence model (see §3.2). We provide additional insights and a thorough experimental evaluation and discussion in §4.
The success of these models is surprising, since it was pre-
viously widely thought that non-linearities within the recur-
rence are necessary for RNNs to model complex interac-
tions (Sch ¨afer & Zimmermann, 2006). In this paper, we
therefore step back and study the theoretical expressivity of
models based on stacks of vanilla3linear complex-valued
RNNs interleaved with position-wise MLPs. In contrast to
non-linear RNNs for which we have several expressivity
results, from Turing-completeness (Chung & Siegelmann,
2021; Siegelmann & Sontag, 1992a) to universality (Sch ¨afer
& Zimmermann, 2006; Hanson & Raginsky, 2020), less is
known for architectures based on linear recurrences, with
nonlinearities placed outside of the sequential processing.
Results on SSMs expressivity. The M ¨utz-Sz ´asz Theo-
rem (M ¨untz, 1914; Sz ´asz, 1916) can be used to show that
linear diagonal real-valued RNNs can approximate causal
convolutions of the input with an arbitrary filter in the width
limit (Li et al., 2022b). However, linear filtering alone can-
not provide satisfactory approximations of non-linear causal
sequence-to-sequence maps. Yet, interestingly, Hanson &
Raginsky (2019) showed that staking an exponential (in the
sequence length) number of filters, chained with ReLUs, can
approximate arbitrary time-homogeneous nonlinear causal
operators. Recently, in the more realistic finite-depth regime
of SSMs, Wang & Xue (2023) proved that interleaving five
linear RNNs with nonlinearities leads, in the width limit,
to the approximation of any continuous mappings from
an input sequence to a scalar target. This result uses the
computationally heavy since unit operations involve dense matrix
multiplications (Smith et al., 2023).
3For an analysis of selective SSMs such as Mamba (Gu &
Dao, 2023) and Griffin (De et al., 2024), please check our follow-
up (Cirone et al., 2024).Kolmogorov-Arnold Theorem (Kolmogorov, 1957). Ap-
proximating nonlinear causal sequence-to-sequence map-
pings (in the sense of Definition 1) is, of course, harder:
Wang & Xue (2023) provided an interesting connection
with V olterra-series expansions (Boyd & Chua, 1985) of
time-invariant causal operators: multiplying the output of
Kinfinitely wide linear RNNs provides a K-th order ap-
proximation of smooth non-linear sequence to sequence
mappings. While this result establishes a connection to the
rich literature in dynamical systems, it concerns a specific
architecture design strategy (product of KRNN outputs),
which is far from SSMs practice. Further, the discussion
in Wang & Xue (2023) does not characterize the effect of
choosing complex-valued recurrences (as crucially done in
most SSMs, including S4) on expressivity. In this paper, we
adopt a different strategy compared to Wang & Xue (2023),
and aim instead at characterizing the information content in
the RNN hidden state, further studying how this information
is processed by the MLP across timestamps.
Contributions. We prove that a single linear diagonal
RNN layer followed by a position-wise MLPs can approxi-
mate arbitrarily well any sufficiently regular (see Defini-
tion 1) nonlinear sequence to sequence map over finite
length sequences. Our key insight is that, if the linear RNN
can preserve a lossless compressed representation of the
entire sequence seen so far, the MLP has access to all of the
information in the input. Since MLPs are universal approxi-
mators (Pinkus, 1999), they can process this information to
achieve any desired mapping (see Fig. 1). Along our reason-
ing, we give several insights on the effects of initialization
and characterize the role of complex numbers on memory.
1.In§3.1, using the properties of Vandermonde matrices,
we show that at a fixed timestamp kone can precisely
2

--- PAGE 3 ---
Universality of Linear Recurrences Followed by Non-linear Projections
(zero error) reconstruct the value of each seen input
token from the hidden state of a random diagonal linear
RNN. We discuss how wide the RNN should be for
this property to hold, and how the result adapts to the
setting of compressible inputs. These results follow from
straightforward linear algebra arguments but provide
great insights into RNN memorization mechanisms. Our
discussion has deep links to the HiPPO theory (Gu et al.,
2020) but does not require structured matrices at the cost
of stronger assumptions on the RNN width (§3.1.3).
2.In§3.2, we prove our claim: linear RNNs followed
by position-independent MLPs with one hidden layer
can approximate regular sequence-to-sequence maps.
Starting from the results of §3.1, we use Barron’s the-
ory (Barron, 1993) to provide guarantees on the MLP
width. This result involves technical steps such as the
computation of the Barron constant of the interpolation
of non-linear maps. Crucially, we find that the MLP
width is affected by the ease of reconstruction from the
linear RNN state, quantified by the condition number of
the reconstruction map, which is heavily dependent on
the RNN initialization.
3.In§4, we leverage our framework to understand some
interesting features of basic SSMs such as S4 (Gu et al.,
2021) and the LRU (Orvieto et al., 2023) – such as the
need, e.g. when training from scratch on the Long-range
Arena (LRA) (Tay et al., 2020), for complex-valued
recurrences and initialization near the unit circle in C.
In the same section, we validate our claims and intu-
ition on initialization on synthetic datasets and on LRA
tasks (Tay et al., 2020).
Due to space limitations, proofs and additional related works
are presented in the appendix.
2. Preliminaries
Consider length- Lsequences of real M-dimensional inputs:
¯v= (vi)L
i=1∈ V ⊆ RM×L. We often refer to each vias
a “token”4. Asequence-to-sequence map is a deterministic
transformation of input sequences that produces output se-
quences of the same length: ¯y= (yi)L
i=1∈ Y ⊆ RS×L. We
say that a sequence-to-sequence map is causal if for every
k,ykis blind to the tokens (vi)L
i=k+1.
Definition 1 (Sequence-to-sequence) .A causal sequence-
to-sequence map with length- Lsequential M-dimensional
inputs ¯v= (vi)L
i=1∈ V ⊆ RM×Land length- Lsequential
S-dimensional outputs ¯y= (yi)L
i=1∈ Y ⊆ RS×Lis a
sequence of non-linear continuous functions T= (Tk)L
k=1,
Tk:RM×k→RS∀k∈[L].Tacts as follows:
(vi)L
i=1T7→(yi)L
i=1,s.t.yk=Tk((vi)k
i=1).
4In formal NLP (Cotterell et al., 2023), Vis a finite vocabulary.
Here, we discuss the setting where Vis possibly dense in RM×L.We are going to assume without restating this that each Tk
is a Barron function (Def. 3), with a well-defined integrable
Fourier transform (used in Thm. 4). We consider approxi-
mations ˆTtoTusing a neural network. Schematically:
ˆT=g◦R◦e,
where:
•e:RM→RHis a linear embedding layer with biases,
acting tokenwise. We denote the encoded tokens se-
quence ¯u= (ui)L
i=1∈RH×L, where uk=e(vk)∈
RH, for all k∈[L].
•Ris a linear RNN processing the encoded input to-
kens(ui)L
i=1producing a sequence of N-dimensional
hidden states (xi)L
i=1∈RN×L.
•g:RN→RSis a non-linear function, acting
tokenwise, parametrized by an MLP. We have that
ˆyk=g(xk)∈RS, for all k∈[L].
The combination of gwithRande, which we denote as
g◦R◦e, produces outputs ˆ¯y= (ˆyi)L
i=1∈RP×Lwhich we
desire to be close to ¯y=T(¯v).
The Linear RNN Rprocesses inputs recurrently. This oper-
ation is parametrized by matrices AandB.
Definition 2 (Linear RNN) .RA,B:RH×L→RN×Lpro-
cesses an (encoded) sequence ¯u= (ui)L
i=1producing an
output sequence of hidden states ¯x= (xi)L
i=1∈RN×Lby
means of the following recursion:
xk=Axk−1+Buk, (1)
where A∈RN×N,B∈RN×H, and x0= 0∈RN.
Diagonal Linear RNNs. Linear RNNs have been shown
to be very successful token-mixing components in deep
architectures such as SSMs (Gu et al., 2021; Orvieto et al.,
2023). A crucial feature of SSMs – making them appealing
compared to non-linear variants such as LSTMs (Hochreiter
& Schmidhuber, 1997) or GRUs (Cho et al., 2014)– is that
the forward pass is cheap to compute by means of parallel
scans (Blelloch, 1990). At the root of such fast computation
is the diagonalizability property of linear RNNs.5Indeed,
over the space of N×Nnon-diagonal real matrices, the
set of non-diagonalizable (in the complex domain) matrices
has measure zero (Bhatia, 2013). Hence, up to arbitrarily
small perturbations, Ais diagonalizable over the complex
numbers, i.e. one can write A=QΛQ−1, where Λ =
diag( λ1, . . . , λ N)∈CN×Ngathers the eigenvalues of A,
5In principle, also non-diagonal linear RNNs can be paral-
lelized. Yet, parallelizing non-diagonal RNNs involves multiplying
dense matrices. To achieve a speedup over recurrent computation
on modern hardware, the linear RNN has to be diagonal.
3

--- PAGE 4 ---
Universality of Linear Recurrences Followed by Non-linear Projections
and the columns of Q∈CN×Nare the corresponding
eigenvectors.
xk=QΛQ−1xk−1+Buk
=⇒(Q−1xk) = Λ( Q−1xk−1) + (Q−1B)uk
By renaming xk←Q−1xkandB←Q−1B, one arrives at
the complex-valued diagonal recursion
xk= Λxk−1+Buk. (2)
The corresponding map RΛ,B:RN×L→CN×Lis such
thatRA,B=Q◦RΛ,B, where Qis applied tokenwise. Since
we are interested in architectures of the form ˆf=g◦R◦e,
the linear transformation Qcan be merged into g, at the
price of having inputs for gwith doubled dimension (real
and imaginary parts6):g:R2N→RP. Without loss in
generality (see also Li et al. (2022b)), we therefore assume
from now on our linear RNNs are in diagonal form. Mo-
rover, since the linear transformation Qcan be merged with
the output projection, we consider – as done in S4 and LRU
– initialization directly on the eigenvalues, and not passing
through diagonalization of a Glorot-initialized dense matrix
A. Note that while the role of complex numbers in this
setting is clearly motivated by the construction, we later
show in §4 that using complex eigenvalues induces peculiar
memorization properties.
MLPs and universality. MLPs with one hidden-
layer (1HL-MLPs) are universal non-linear function ap-
proximators (Barron, 1993; Pinkus, 1999). In this paper,
we consider gparametrized by a 1HL-MLP. gis a proxy
for a general non-linear function f, which is regular in the
sense that it is not oscillating too quickly – meaning that its
Fourier transform F(ω)exhibits fast decay as ∥ω∥ → ∞ .
Definition 3 (Barron function) .LetF(ω)be the Fourier
transform of f:Rn→R.fbelongs to the Barron class if
Cf=R
Rn∥ω∥2|F(ω)|dω <∞.
We have the important result due to (Barron, 1993).
Theorem 1 (Universality of 1HL-MLPs) .Consider g(x)
parametrized by a 1HL-MLP (with Dhidden neurons):
g(x) =PD
k=1˜ckσ(⟨˜ak, x⟩+˜bk) + ˜c0, where σis any sig-
moidal function7. Let f:Rn→Rbe continuous with
Barron constant Cf. IfD≥2r2C2
fϵ−2, then there exist
parameters such that sup∥x∥≤r|f(x)−g(x)| ≤ϵ.
Note that ReLU activations also work in the setting of
the theorem above, up to a factor 2in the bound, since
ReLU (x)−ReLU (x−1)is sigmoidal.
Remark 1 (Multidimensional Output) .The result above is
stated for the scalar output case. The S-dimensional outputs
6In practice, some SSM variants drop the imaginary part.
7limx→−∞ σ(x) = 0 andlimx→∞σ(x) = 1 .result can be easily derived by stacking neurons in the hid-
den layer, that becomes of dimension D←DS. Therefore
ifD≥2r2C2
fSϵ−2, then sup∥x∥≤r∥f(x)−g(x)∥1≤ϵS
(since errors accumulate). Let us then call ϵ←ϵSthe
desired accuracy; the number of neurons needed to achieve
that is D≥2r2C2
fS3ϵ−2.
3. Universality Result
In this section, we show that, as the model ˆT=g◦R◦e
grows in width, there exist network parameters such that
ˆT≈T. We state this result informally below.
Theorem 2 (Universality) .Let the inputs set V ⊂RM×Lbe
bounded and ϵ >0be the desired accuracy in approximat-
ingT. LetRbe a diagonal linear real or complex RNN with
width N≥dim(V)and let the MLP width D≥O(L/ϵ2).
Then, ˆT=g◦R◦eapproximates pointwise Twith error ϵ:
sup
¯v∈V∥ˆT(¯v)−T(¯v)∥ ≤ϵ.
Here, by dim(V)we mean the vector-space dimension ofV.
In the worst-case scenario where inputs are not structured,
dim(V) =LM. In practice, we observe that one can work
with smaller dimensions in the hidden state (Fig. 4). The
proof comprises two steps:
•Step 1: Linear RNNs can perform lossless compres-
sion ( §3.1): from the RNN output xkone can per-
fectly (no error) reconstruct the input (vi)k
i=1, assuming
Nis large enough ( N≥dim(V)).
•Step 2: The MLP head gon top of xkcan reconstruct
the ground-truth map Tk:Tk((vi)k
i=1)≃g(xk), as-
suming the number of hidden MLP neurons Dis large
enough ( D≥O(L/ϵ2)).
While step 2 is mainly technical, step 1 – dealing specifi-
cally with the RNN – is less involved and leads to valuable
insights into the architecture.
Remark 2. A few comments on the result are needed:
•In Thm. 2, both the size of the RNN and the MLP agree
with basic information theory reasoning: (a) if inputs
are random and unstructured, the hidden state cannot
perform compression: the RNN has to store O(L)reals,
requiring O(L)hidden dimensions; (b) the MLP size is
O(L)since, in the worst-case, it has to model Ldistinct
maps T1, T2, . . . , T L(Def. 1). This complexity can be
reduced by assuming temporal smoothness (cf. §3.1.3).
•The setting of tokens living in a finite set (Cotterell et al.,
2023) is drastically different and not discussed in this
work. An interesting result using a construction derived
from the first version of this paper can be found in Ding
et al. (2023). Additional recent results can be found
in Jelassi et al. (2024).
4

--- PAGE 5 ---
Universality of Linear Recurrences Followed by Non-linear Projections
•A simple application of Barron’s Theorem (Thm. 1) does
not suffice to prove Step 2. In SSMs, the same MLP
function is applied at each timestamp, ant therefore has
to model the interpolation of T1, T2, . . . , T L. Adapting
Barron’s theory to this time-dependent setting is our main
technical contribution.
This section is dedicated to the proof of Thm 2, with the
main intuition relevant for our discussion is presented in
§3.1.1. Valuable insights on initialization and role of com-
plex numbers can be derived from our proof strategy. These
are discussed in §4.
3.1. Linear RNNs can perfectly memorize inputs
We first state the main result of this subsection.
Theorem 3 (Power of Linear RNNs, informal) .Under no
assumption on the encoded inputs set U ⊆RH×L, if the
hidden dimension scales with HL, then linear RNN-based
processing comes with no information loss : inputs (ui)k
i=1
can be perfectly recovered from xk, for any k= 1,2, . . . , L .
Ifdim(U) =:P < HL – i.e. if Uis linearly compressible –
the hidden dimension can be reduced to O(P).
We encourage the reader to go through this subsection to get
a proof idea. Insights and practical considerations will be
then addressed thoroughly in §4. In §3.1.3, we compare this
simple result with HiPPO theory (Gu et al., 2020): a frame-
work that provides precise guarantees on the approximation
error of structured linear RNNs at any width.
In this subsection, with “input” we refer to the encoded
sequence ¯u=e(¯v)∈RH×L. Note that, as typically
H≥M, accurate reconstruction of ¯uimplies accurate re-
construction of ¯v. We start by unrolling Eq. (2): x1=Bu1,
x2= ΛBu1+Bu2,x3= Λ2Bu1+ ΛBu2+Bu3, and
xk=k−1X
j=0ΛjBuk−j. (3)
It is easy to realize that this operation stacks convolutions
of the sequence B¯u∈CN×Lwith Nindependent one-
dimensional filters parametrized by Λ = diag( λ1, . . . , λ N),
and with form (1, λi, λ2
i, . . . , λL−1
i)fori= 1,2, . . . N .
3.1.1. M AIN IDEA
LetH=M= 1, the encoder ebe the identity, and B=
(1,1, . . . , 1)⊤. Then, Eq. (3) can be written as
xk=
λk−1
1 λk−2
1··· λ11
λk−1
2 λk−2
2··· λ21
...............
λk−1
N λk−2
N··· λN1

u1
u2
...
uk
=Vku⊤
1:k.
(4)where u1:k=v1:k= (vi)k
i=1∈R1×k, and Vkis a Vander-
monde matrix. As long as N≥k, we can hope to recover
u1:kby pseudoinversion of the Vandermonde:
v⊤
1:k=u⊤
1:k=V+
kxk. (5)
Indeed, note that if N=k(number of equations =number
of unknowns), the matrix Vkis invertible under the assump-
tion that all λiare distinct, since (see e.g. (Bhatia, 2013)):
det(Vk) =Q
1≤i<j≤k(λi−λj)̸= 0. IfN > k then under
again the assumption that eigenvalues are distinct, the lin-
ear operation on the hidden state V+
kxkproduces the input
sequence without errors. To make this procedure work in
the simplest setting, a sufficient condition is N≥L. We
summarize below.
Proposition 1 (Bijectivity) .LetM= 1,H= 1 and
let the encoder ebe the identity. Consider input pro-
jection B= (1 ,1, . . . , 1)⊤and recurrent matrix Λ =
diag( λ1, . . . , λ N)withλi̸=λjfor all i̸=j. Fix k∈[L],
letRk
Λ,B:RH×k→RNdenote the map (ui)k
i=17→xk. If
N≥L,Rk
Λ,Bis bijective8, with linear inverse.
The reconstruction map (parametrized by V+
k) is time-
variant and has time-dependent output dimension Rk. Since
the RNN output we consider in this paper is a time-
independent MLP headg, it is essential for the hidden state
to also contain information about time . This is trivial to
achieve with linear RNNs: consider e:RM→RM+1
such that vkis mapped to uk= (1 , vk)for all k. Let
B= ((1 ,0,0, . . . , 0),(0,1,1, . . . , 1))⊤. Consider λ0= 1.
Then, the first hidden state dimension will coincide with the
index k, since xk,0=xk−1,0+1for all k. The construction
above, allows the RNN to count the step it is on. In §3.2,
we will use this fact to conclude the theoretical discussion.
Multidimensional setting. Since the RNN is linear, one
can design Mindependent RNNs acting on separate input
dimensions. Such RNNs can be combined into one single
block using a properly designed Bmatrix.
3.1.2. RNN S CAN COMPRESS INPUTS ,WHEN POSSIBLE
What we discussed in Theorem 3 is a worst-case setting that
requires the RNN hidden state Nto be of size proportional
to the input sequence L. This is not surprising since we
made no assumptions about the inputs ¯u: it is indeed not
possible in general to store O(L)real numbers in a vector of
size smaller than O(L)by means of a linear Vandermonde
projection – unless such inputs are structured (cf. §3.1.3).
However, the RNN hidden state dimension scales efficiently
under low-dimensionality, which we refer to as sparseness
in relation to sparse coding (Lewicki & Sejnowski, 2000).
Assumption A (Low-dimensional, a.k.a sparse , inputs) .Let
Ψ = ( ψi)P
i=1, with ψi∈RM×Lfor all i= 1,2,···, Pbe
8Note that this maps the input sequence to the last hidden state.
5

--- PAGE 6 ---
Universality of Linear Recurrences Followed by Non-linear Projections
an ordered list of basis functions. For every input sequence
¯v, there exist coefficients αv:= (αv
i)P
i=1∈RPsuch that
¯v=PP
i=1αv
iψi. We do not assume such decomposition is
unique, nor to have access to the basis functions.
Let us discuss this assumption in the one-dimensional set-
tingM= 1. Definitions and results carry out effortlessly to
the multidimensional setting. In matrix form, Assumption A
can be written as: there exists a real matrix Ψ∈RL×Psuch
that, for all ¯v∈ V ⊂ R1×Lthere exists αv∈RPsuch that
¯v⊤= Ψαv. Let Ψk∈Rk×Pdenote the first kcolumns
ofΨ. The last equation implies v⊤
1:k= Ψ kαv
k, where αkis
one among the estimates of the coefficients αvholding until
iteration k(e.g. one frequency component might be visible
only after a specific k). Eq. (4) then implies:
xk=Vkv⊤
1:k=VkΨkαv
k, (6)
Under the assumption that Γk:=VkΨkis full rank, αv
k=
(Γ⊤
kΓk)−1Γ⊤
kxk. We therefore get:
v1:k= Ω kxk,Ωk:= Ψ k(Γ⊤
kΓk)−1Γ⊤
k∈Ck×N.(7)
General definition of Ωk.In the non-sparse setting, it is
easy to see that Ωk=V+
k. Therefore, from this point in the
paper we will keep denoting as Ωkthe linear reconstruction
map. To make Ωkact on real numbers, we consider instead
its real/imaginary inputs representation in R2N×N.
Ψmay be unknown. One does not need to assume a
specific Ψa priori: under input sparsity, we can guaran-
tee approximation with a learned linear map on the state,
which may be task-dependent. Even if the input is strictly-
speaking not sparse, not all information might be relevant
for prediction. In a way, sparsity mathematically quantifies
the belief that the information content relevant to a certain
task has a lower dimension compared to the original signal.
3.1.3. C OMPARISON WITH HIPPO
HiPPO theory (Gu et al., 2020) describes the compression
properties of continuous-time linear RNNs of the form
˙xt=Axt+But, where u:R→Ris a smooth in-
putandA∈RN×N, B∈RNhave a specific fixed
form (i.e. are structured ). A common choice for Aand
Bimplements the LegS operator. At time t, the LegS
operator maps the input to the Npolynomial coefficients
ci(t)such that g(t)=PN
i=1ci(t)p(t)
iis the best possible
polynomial approximation of u[0,t]under the uniform mea-
sure in time on the interval [0, t]. If the inputs are Lip-
schitz continuous (smooth), they can indeed be approxi-
mated by an N-degree polynomial: the error is then simply
the sum of the small polynomial coefficients above degree
N:|u[0,t]−g(t)|2=O(t2ℓ2/N). This bound is due to
the fundamental properties of polynomials and can be con-
sidered independently from the HiPPO framework: being
ℓ-Lipschitz implies an error O(t2ℓ2/N)from inputs beinga projection on a specific basis on Northogonal polynomi-
als. In particular, note that assuming the error is O(t2ℓ2/N)
from being described by the first Ncoefficients is actually
a weaker requirement compared to ℓ-Lipshitzness. This
is linked with sparseness : u[0,t]=PN
i=1ci(t)p(t)
ican be
framed as Assumption A. While HiPPO theory, in contrast
to this paper, allows for precise error estimates, our approach
is more general. We show that it is not needed to fix an in-
put measure or to hand-pick A,B: random initialization
on a ring can already lead to successful reconstruction in
practice (Fig. 4). Our worst-case result holds if N≥L,i.e.
for the general non-smooth input setting . Furthermore, as
showcased above, Assumption A can be linked to polyno-
mial regression, and therefore describes a general property
– at the price of missing error quantifications. This can be
resolved: for a fixed basis Ψ, it is possible to provide bounds
using the famous lemma of Johnson et al. (1986): a sim-
ilar analysis is used to quantify the error of randomized
signatures in Rough Paths Theory (Cuchiero et al., 2021;
Compagnoni et al., 2023).
Compared to HiPPO, an advantage of our simplified (yet
less precise) approach is that we can easily move to the
question of expressivity, which we explore next. Further,
our non-structured approach leads to additional insights into
the role of complex eigenvalues, which we explore in §4.
3.2. MLP Reconstruction
Recall that our goal is to approximate the object T=
(Ti)L
i=1, that maps (vi)L
i=1T7→(yi)L
i=1such that yk=
Tk((vi)k
i=1)for all k= 1,2, . . . , L . In the usual SSMs
pipeline, this approximation takes the form ˆT=g◦R◦e,
where compositions are tokenwise (MLP gand embedding
eapplied to each token), and Ris a linear diagonal RNN,
mixing tokens in the temporal direction.
Letxkbe the k-th RNN output. In the settings described
in the last subsection, for every k∈ {1,2, . . . , L }there
exists a linear function Ω′
k(identity on the first coordi-
natexk,0=k, and Ωkon the other coordinates) such that
Ω′
kxk= (k,(vi)k
i=1). Note that under no assumption on
the set of inputs, Ωkcoincides with the Vandermonde in-
verse V+
k. The last operation, (k, v1:k)7→ykneeds to be
performed by the time-independent mapg.
Step 1: fixed timestamp. Let us focus on getting the MLP
to approximate Tk,kfixed, from the RNN output. ghas
to satisfy g(xk)!=Tk(Ωk(xk)) =: fk(xk). The following
result (based on Rmk.1) bounds the width of the MLP gfor
approximation of this quantity.
Proposition 2 (MLP, single timestamp) .The number of
hidden neurons in the MLP gsufficient to approximate fk:=
Tk◦Ωkat level ϵfrom the RNN hidden state xkis bounded
by4r2C2
Tk∥Ωk∥2
2S3ϵ−2where rbounds the hidden state
magnitude, and CTkis the Barron constant of Tk.
6

--- PAGE 7 ---
Universality of Linear Recurrences Followed by Non-linear Projections
The result is easy to parse: to approximate the output at a
specific timestamp k, the width bound is similar to that of
an MLP (vi)k
i=1gk7→ˆyk, which would be 4r2C2
TkS3ϵ−2(see
Thm. 1). The additional factor ∥Ωk∥2
2appears because g
operates on the hidden state and not from the input. Hence,
the MLP has to pay a penalty for reconstruction . The
result is based on the computation of the Barron constant
for the map fk, which is upper bounded by ∥Ωk∥2CTk(see
appendix). In the next paragraph, we study how complexity
is affected by the additional requirement that the map g
should be able to interpolate in-between Tks, based on the
first coordinate in the hidden state xk,1=k.
Step 2: arbitrary timestamp. To construct a time-
independent global approximation, we use recent results
from harmonic analysis (Tlas, 2022) to construct a proper
domain relaxation and operate with a single MLP. This com-
putation leads to a formula for the Barron complexity for
function sequences, where an input dimension determines
the function the MLP should implement. In addition to
the definition Cf=R
Rn∥ω∥2|F(ω)|dω, define the Fourier
norm C′
f=R
Rn|F(ω)|dω.
Theorem 4 (Combination of Barron Maps) .Letf:
[1,2, . . . , L ]×Rn→Rbe such that each f(k,·)is Bar-
ron with constant Cfkand Fourier norm C′
fk. There ex-
ists an extension ˜f:Rn+1→Roff, s.t. ˜fis Barron
with constant ˜C≤ O(PL
k=1Cfk+C′
fk) =O(L)and
˜f(x, k) =f(x, k),∀x∈Rnandk∈1,2, . . . , L .
This result, which essentially confirms the need, under no
further assumptions on T, to blow up the number of hidden
neurons with the sequence length (unless some smoothness
is assumed, see Remark 2), directly leads to our main re-
sult (Thm. 2) under the assumption that the RNN has perfect
memory, guaranteed if N≥dim(V)(Thm. 3).
Proof of Thm. 2. Letfk:=Tk◦Ωk:R2N→RS. By
Prop. 2, Cfk=∥ΩkCTk∥.C′
fkscales similarly. We have
fk(xk) =Tk(v1:k)thanks to Thm. 3. We then apply Thm. 4
to the sequence of fk(one for each timestamp): there ex-
ists an interpolating function ˜f(time as first component
followed by the 2Narguments of fk) with Barron constant
O(PL
k=1Cfk+C′
fk) =O(L). We let this be the function
the 1-HL MLP ghas to approximate, and apply Thm. 1.
4. Validation and Discussion
In this section, we revisit and validate our claims, and further
discuss the practical insights associated with input recon-
struction from the linear RNN hidden state.
4.1. Conditioning and role of complex numbers
In§3.1.1, we discussed the main idea behind reconstruction
from the linear RNN hidden state. Let us restrict atten-tion to the last timestamp: under no sparseness assump-
tion (discussed at the end of this subsection) xL=Vk¯v⊤,
with VL∈CN×L, hence ¯v⊤=V+
LxL, where V+
L=
(V⊤
LVL)−1V⊤
L=: Ω L. Indeed, if the eigenvalues (λi)N
i=1
are distinct and N≥L,V⊤
LVL∈CL×Lis invertible since
the Vandermonde determinant formula ensures VLhas full
column rank9. Such computation does not take into account
anumerical issue : ifVLisill-conditioned ,∥V+
L∥2→ ∞ ,
preventing successful reconstruction. Instead, when ini-
tializing or learning the RNN, not only can we assume
the eigenvalues are distinct, but we can also control them.
Let us pick λisuniformly on the complex ring in between
[rmin, rmax]⊆[0,1]: For i∈[N], we pick λii.i.d with
λi∼T[rmin, rmax] :={λ∈C|rmin≤ |λ| ≤rmax}.
Initialization of Λclose to the unit circle is often used in the
context of modern state-space models to unlock long-range
reasoning (Orvieto et al., 2023). Our theory gives grounding
to this strategy: we see in Fig. 2 that if rmin→1, then the
Vandermonde becomes well conditioned and reconstruction
becomes successful (Fig. 3 + appendix).
0 50 1000
100
200Shown: magnitude of Vk
 rmin=0,rmax=1
 Vk2=4.0e+01
V+
k2=7.8e+09
0 50 1000
100
200Shown: magnitude of Vk
 rmin=0.999,rmax=1
 Vk2=1.8e+02
V+
k2=2.8e+00
0.00.20.40.60.81.0
0.9000.9250.9500.9751.000
Figure 2. Effect of eigenvalue magnitude on conditioning. L=
128,N= 2L= 256 ,λi∼T[rmin, rmax].
One can also link this effect to vanishing gradients: if
|λi| → 0then the hidden state “forgets” inputs at a rate
λk. This is the reason why in Fig. 3 we observe successful
reconstruction only for the recent past if rmin= 0.
Crucial role of complex numbers. IfΛis real the condi-
tion number of Vkgrows exponentially with N(Gautschi
& Inglese, 1987). In the complex setting it is possible to
make the condition number of Vkexactly 1 by e.g. choos-
ing the Nth-roots of unity as eigenvalues of Λ(Gautschi,
1975; C ´ordova et al., 1990). This fact provides a precise
justification for the use of complex numbers in the recurrent
computation : diagonal real recurrent RNNs can be also im-
plemented fast with parallel scans (Smith et al., 2023), but
suffer from an inherent information loss.
9By contradiction, assume V⊤
LVLhas the zero eigenvalue. This
implies there exists xsuch that V⊤
LVLx= 0, hence x⊤V⊤
LVLx=
∥VLx∥= 0, which is true if and only if VLx= 0, meaning VLis
not full column rank.
7

--- PAGE 8 ---
Universality of Linear Recurrences Followed by Non-linear Projections
Reconstruction with Van. Inverse, N=512,rmin=0Reconstruction with Van. Inverse, N=512,rmin=0.999
Figure 3. Reconstruction of MNIST digits from the final linear RNN hidden state using the Vandermonde inverse. For rmin= 0, the
Vandermonde is ill-conditioned (Fig. 2) and hence only the recent past can be reconstructed. For rmin= 0.99we can reconstruct the
whole image. See Fig. 4 for results on learned reconstructions.
641282565121024RNN hidden di ension10)510)410)310)210)1Training Loss (pi(el-wise L2)Pathfinder (MLP head)PathFinder (Linear head)MNIST (MLP head)MNIST (Linear head)
MNIST, Linear HeadMNIST, MLP Head
PathFinder, MLP HeadPathFinder, Linear Head
Figure 4. Reconstruction of MNIST digits and PathFinder data (Tay et al., 2020) Using a trained RNN + MLP or linear reconstruction
decoder from the last hidden state xL. Plotted is average L2 pixel-wise norm (mean of 3 runs). All parameters are trained, hyperparameters
are tuned. rmin= 0.9,rmax= 0.999are found to be best for initialization of the RNN (in line with (Gu et al., 2021)). For large hidden
dimension, linear reconstruction is successful. For smaller hidden dimension, non-linear reconstruction becomes necessary.
Comparison with FFT. The case of equally-spaced eigen-
values on the disk coincides with the computation of the
Fourier Transform. Perfect reconstruction in this setting is
guaranteed by the inverse Fourier Transform Theorem (Fol-
land, 2009). While this would motivate the use of unitary
RNNs (Arjovsky et al., 2016; Helfrich et al., 2018), as we
will see in §4.3 best performing learned architectures do
not have this property: hidden-state features with vanishing
memory are often the best option.
Sparseness improves conditioning. In the last section,
we discussed how input sparseness (on a basis of Pfunc-
tions) results in a reduced hidden state dimension require-
ment. Specifically, if dim(V) = P, i.e. there exists
Ψ∈RL×Psuch that for any ¯v∈ V there exists αv∈RP
such that ¯v⊤= Ψαv, then an RNN with width N=P
achieves perfect recall for inputs in V. In Fig. 12 (appendix)
we show that sparseness also improves conditioning of the
reconstruction map ΩL, appearing in Prop. 2.
4.2. Learned architecture for reconstruction
In the previous subsection we studied the reconstruction
map computed from the parameters of a random linear
RNN. While our theory provides satisfactory approximation
guarantees in this setting, performance is much stronger
if we allow the linear RNN to be trained jointly with thereconstruction map . We use here a simple LRU block (at in-
ference time, a linear RNN (Orvieto et al., 2023)) followed
by a linear reconstruction head Ω. This framework, while
perfectly in line with our settings in this paper, allows the
RNN eigenvalues and the decoder to synchronously adapt to
the specific data structure. Out of curiosity, we also test here
the performance of non-linear reconstruction from the RNN
hidden state with a 1-HL MLP with 2048 hidden units10. We
use data from the long-range arena (Tay et al., 2020), specif-
ically sequential MNIST (784 tokens) and PathFinder (1024
tokens) and train our model11under the usual guidelines
from random initialization (see discussion in (Orvieto et al.,
2023)).
1.For both datasets, the reconstruction error (input of re-
construction map is the last hidden state) vanishes as the
RNN hidden dimension increases. At N= 256 , RNN
width used in practice on the long-range arena (Gu et al.,
2021; Orvieto et al., 2023), reconstruction is nearly per-
fect. For N= 1024 , the error is negligible (as predicted
by Thm. 3) since N≥L.
2.Reconstruction is surprisingly accurate at small values
10This is not the function gthat allows approximation of Tk.
Here the MLP serves as non-linear decoder.
11We train on a single Nvidia RTX A5000 for 100 epochs.
8

--- PAGE 9 ---
Universality of Linear Recurrences Followed by Non-linear Projections
ofNif the decoder is non-linear. Instead, for N= 1024 ,
the error is smallest with a linear decoder, a finding we
believe is rooted in optimization of wide MLPs.
4.3. Approximation of non-linear ODEs.
In§4.1&4.2 we studied the information content of lin-
ear RNNs hidden states and verified empirically that near-
perfect input reconstruction is possible, even if N < L .
We now conclude the paper with results verifying our
main claim (Thm. 2): a single MLP applied to the RNN
hidden states, independent of the timestamp, is able to
reconstruct the output of regular sequence-to-sequence
maps (Def. 1). An example of a sequence-to-sequence
map is the flow of a controlled differential equation, of
form ˙zt=f(zt, vt), yt=h(zt), where (vt)tis the input, f
is a non-linear multidimensional function, and hprojects
the multidimensional state ztinto a one-dimensional out-
put. Such systems can model complex interactions and
stand at the root of physics. We limit our discussion here
to the problem of learning a protein transduction (PT) sys-
tem(Vyshemirsky & Girolami, 2008). The reader can find
results for other non-linear ODEs in the appendix .
In PT, ztis 5-dimensional, and fincludes multiplicative
interactions between components of ztas well as feed-
back (see equations in the appendix). We include an input to
the right-hand-side of the first ODE while integrating with
Runge-Kutta 45, and take has the projection of the first
component of zt. We learn to mimic the PT system with a
linear RNN with N= 128 , followed by a 1-HL MLP with
D= 256 . We sample 10ksmooth inputs and train on the
simulated outputs for L= 2048 integration steps. We test
on1kadditional trajectories. The results in Fig. 5 clearly
show that the architecture studied in this paper is able to
learn the PT sequence-to-sequence map: plotted are two
out-of-sample trajectories (not used during training), show-
casing that our model prediction matches the PT dynamics
up to a small error (as confirmed by the low test loss). We
also include a depiction of the learned eigenvalues, to con-
firm that the RNN has indeed vanishing memory – but this
does not necessarily imply information loss.
Learned eigenvalues /uni039BProtein Transduction: Out-of-sample prediction. point-wise  L2 loss train 0.00166, test 0.0017
Figure 5. Trained linear RNN + MLP architecture (1 layer) learns
a non-linear sequence-to-sequence map. Additional experiments
can be found in the appendix.5. Conclusion
In this paper we studied the expressive power of architec-
tures combining linear RNNs with position-wise MLPs.
These models recently exhibited remarkable performance
in the context of long-range reasoning. In our theoreti-
cal framework, these two blocks work in symbiosis while
having distinct roles: the linear RNN provides a com-
pressed (where possible) representation of the input se-
quence, while the MLP performs non-linear processing.
We believe our separation of concerns principle takes us
one step further in understanding how to design and con-
ceptualize deep state-space models: non-linear sequential
processing can be performed by a time-independent com-
ponent (the MLP). Further, the use of complex numbers
in the recurrence unlocks lossless compression via a well-
conditioned reconstruction map. In addition, our analysis
provides guidelines for asymptotic scaling of architecture
components, and can lead to novel initialization strategies
based on local objectives (e.g. successful reconstruction).
Future work. Our work leaves open interesting avenues
for future research, e.g. the finite-alphabet setting, the finite
precision setting, and the framework of Turing complete-
ness. Regarding the expressivity of recent selective RNNs
such as Mamba and Griffin, an extension of our theory can
be found in Cirone et al. (2024). Last, we note that, as
remarked in §3.1.3, our framework does not model errors in
reconstruction present at moderate hidden state dimensions.
We believe a unified formal analysis, comprising assump-
tions such as smoothness in the inputs and tools such as
the Johnson et al. (1986) lemma, can fill this gap.
Insights from Mamba & Never train from scratch .The
first version of this paper, in the form of a short note, dates
back to May 2023. In late 2023, selective SSMs (Gu & Dao,
2023) were introduced: these SSMs do not use of complex
numbers , have input-controlled state transitions, and are
almost exclusively trained autoregressively (on text). Amos
et al. (2023) brilliantly pointed out that next-token predic-
tion leads to a conceptually distinct objective compared to
standard supervised learning: while training from scratch
requires careful parametrization, initialization, and memo-
rization, pretraining on denoising objectives can be more
forgiving: transformers and less sophisticated S4 variants
are able to get high accuracy on the LRA benchmark (Tay
et al., 2020). Our paper, and specifically our discussion
on complex numbers, is instead motivated by interesting
results and the ablations holding for non-selective SSMs
trained from scratch (Gu et al., 2021; Orvieto et al., 2023).
We believe the supervised learning setting is scientifically
interesting and relevant, and that the analysis presented here
provides a precise motivation for effective design strategies.
While the next-token prediction paradigm is becoming a
standard, the peculiarities of supervised learning can yield
insights into the next generation of generative models.
9

--- PAGE 10 ---
Universality of Linear Recurrences Followed by Non-linear Projections
Acknowledgements
Antonio Orvieto acknowledges the financial support of the
Hector Foundation.
Impact Statement
The authors do not foresee any societal consequences or eth-
ical concerns arising from their work. This is a theoretical
paper.
References
Amos, I., Berant, J., and Gupta, A. Never train from scratch:
Fair comparison of long-sequence models requires data-
driven priors. In International Conference on Learning
Representations , 2023.
Arjovsky, M., Shah, A., and Bengio, Y . Unitary evolution
recurrent neural networks. In International conference
on machine learning . PMLR, 2016.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450 , 2016.
Barron, A. R. Universal approximation bounds for super-
positions of a sigmoidal function. IEEE Transactions on
Information theory , 39(3):930–945, 1993.
Beltagy, I., Peters, M. E., and Cohan, A. Long-
former: The long-document transformer. arXiv preprint
arXiv:2004.05150 , 2020.
Bhatia, R. Matrix analysis , volume 169. Springer Science
& Business Media, 2013.
Blelloch, G. E. Prefix sums and their applications, 1990.
Boyd, S. and Chua, L. Fading memory and the problem of
approximating nonlinear operators with volterra series.
IEEE Transactions on circuits and systems , 32(11):1150–
1161, 1985.
Child, R., Gray, S., Radford, A., and Sutskever, I. Gen-
erating long sequences with sparse transformers. arXiv
preprint arXiv:1904.10509 , 2019.
Cho, K., Van Merri ¨enboer, B., Gulcehre, C., Bahdanau,
D., Bougares, F., Schwenk, H., and Bengio, Y . Learn-
ing phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint
arXiv:1406.1078 , 2014.
Chung, S. and Siegelmann, H. Turing completeness
of bounded-precision recurrent neural networks.
In Ranzato, M., Beygelzimer, A., Dauphin, Y .,
Liang, P., and Vaughan, J. W. (eds.), Advancesin Neural Information Processing Systems , vol-
ume 34, pp. 28431–28441. Curran Associates, Inc.,
2021. URL https://proceedings.neurips.
cc/paper_files/paper/2021/file/
ef452c63f81d0105dd4486f775adec81-Paper.
pdf.
Cirone, N. M., Orvieto, A., Walker, B., Salvi, C., and Lyons,
T. Theoretical foundations of deep selective state-space
models. arXiv preprint arXiv:2402.19047 , 2024.
Compagnoni, E. M., Scampicchio, A., Biggio, L., Orvieto,
A., Hofmann, T., and Teichmann, J. On the effectiveness
of randomized signatures as reservoir for learning rough
dynamics. In 2023 International Joint Conference on
Neural Networks (IJCNN) . IEEE, 2023.
C´ordova, A., Gautschi, W., and Ruscheweyh, S. Vander-
monde matrices on the circle: spectral properties and
conditioning. Numerische Mathematik , 57(1):577–591,
1990.
Cotterell, R., Svete, A., Meister, C., Liu, T., and Du, L.
Formal aspects of language modeling. arXiv preprint
arXiv:2311.04329 , 2023.
Cuchiero, C., Gonon, L., Grigoryeva, L., Ortega, J.-P., and
Teichmann, J. Expressive power of randomized signa-
ture. In The Symbiosis of Deep Learning and Differential
Equations , 2021.
Dao, T. Flashattention-2: Faster attention with bet-
ter parallelism and work partitioning. arXiv preprint
arXiv:2307.08691 , 2023.
Dao, T., Fu, D., Ermon, S., Rudra, A., and R ´e, C. Flashat-
tention: Fast and memory-efficient exact attention with
io-awareness. Advances in Neural Information Process-
ing Systems , 2022.
Dauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan-
guage modeling with gated convolutional networks. In
International conference on machine learning . PMLR,
2017.
De, S., Smith, S. L., Fernando, A., Botev, A., Cristian-
Muraru, G., Gu, A., Haroun, R., Berrada, L., Chen, Y .,
Srinivasan, S., Desjardins, G., Doucet, A., Budden, D.,
Teh, Y . W., Pascanu, R., Freitas, N. D., and Gulcehre,
C. Griffin: Mixing gated linear recurrences with local
attention for efficient language models, 2024.
Ding, Y ., Orvieto, A., He, B., and Hofmann, T. Recurrent
distance-encoding neural networks for graph representa-
tion learning. arXiv preprint arXiv:2312.01538 , 2023.
10

--- PAGE 11 ---
Universality of Linear Recurrences Followed by Non-linear Projections
Dondelinger, F., Husmeier, D., Rogers, S., and Filippone, M.
Ode parameter inference using adaptive gradient match-
ing with gaussian processes. In Artificial intelligence and
statistics , pp. 216–228. PMLR, 2013.
Folland, G. B. Fourier analysis and its applications , vol-
ume 4. American Mathematical Soc., 2009.
Fu, D. Y ., Dao, T., Saab, K. K., Thomas, A. W., Rudra, A.,
and Re, C. Hungry hungry hippos: Towards language
modeling with state space models. In International Con-
ference on Learning Representations , 2023.
Funahashi, K.-I. On the approximate realization of continu-
ous mappings by neural networks. Neural networks , 2(3):
183–192, 1989.
Gautschi, W. Optimally conditioned vandermonde matrices.
Numerische Mathematik , 24:1–12, 1975.
Gautschi, W. and Inglese, G. Lower bounds for the con-
dition number of vandermonde matrices. Numerische
Mathematik , 52(3):241–250, 1987.
Goel, K., Gu, A., Donahue, C., and R ´e, C. It’s raw! au-
dio generation with state-space models. International
Conference on Machine Learning , 2022.
Gu, A. and Dao, T. Mamba: Linear-time sequence
modeling with selective state spaces. arXiv preprint
arXiv:2312.00752 , 2023.
Gu, A., Dao, T., Ermon, S., Rudra, A., and R ´e, C. Hippo:
Recurrent memory with optimal polynomial projections.
Advances in neural information processing systems , 33:
1474–1487, 2020.
Gu, A., Goel, K., and Re, C. Efficiently modeling long
sequences with structured state spaces. In International
Conference on Learning Representations , 2021.
Gu, A., Gupta, A., Goel, K., and R ´e, C. On the parame-
terization and initialization of diagonal state space mod-
els.Advances in Neural Information Processing Systems ,
2022.
Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are
as effective as structured state spaces. In Advances in
Neural Information Processing Systems , 2022.
Haar, A. Zur theorie der orthogonalen funktionensysteme.
Mathematische Annalen , 71(1):38–53, 1911.
Hanson, J. and Raginsky, M. Universal approximation of
input-output maps by temporal convolutional nets. Ad-
vances in Neural Information Processing Systems , 32,
2019.Hanson, J. and Raginsky, M. Universal simulation of stable
dynamical systems by recurrent neural nets. In Learning
for Dynamics and Control , pp. 384–392. PMLR, 2020.
Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini,
A., and Rus, D. Liquid structural state-space models. In
The International Conference on Learning Representa-
tions , 2022.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition ,
pp. 770–778, 2016.
Helfrich, K., Willmott, D., and Ye, Q. Orthogonal recur-
rent neural networks with scaled cayley transform. In
International Conference on Machine Learning . PMLR,
2018.
Hochreiter, S. and Schmidhuber, J. Long short-term memory.
Neural computation , 1997.
Hornik, K. Approximation capabilities of multilayer feed-
forward networks. Neural networks , 4(2):251–257, 1991.
Hornik, K., Stinchcombe, M., and White, H. Multilayer
feedforward networks are universal approximators. Neu-
ral networks , 2(5):359–366, 1989.
Ioffe, S. and Szegedy, C. Batch normalization: Accelerating
deep network training by reducing internal covariate shift.
InInternational Conference on Machine Learning , 2015.
Jelassi, S., Brandfonbrener, D., Kakade, S. M., and
Malach, E. Repeat after me: Transformers are bet-
ter than state space models at copying. arXiv preprint
arXiv:2402.01032 , 2024.
Johnson, W. B., Lindenstrauss, J., and Schechtman, G. Ex-
tensions of lipschitz maps into banach spaces. Israel
Journal of Mathematics , 1986.
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
Transformers are rnns: Fast autoregressive transformers
with linear attention. In International Conference on
Machine Learning , pp. 5156–5165. PMLR, 2020.
Katsch, T. Gateloop: Fully data-controlled linear recurrence
for sequence modeling, 2023.
Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The
efficient transformer. arXiv preprint arXiv:2001.04451 ,
2020.
Kolmogorov, A. N. On the representation of continuous
functions of many variables by superposition of continu-
ous functions of one variable and addition. In Doklady
Akademii Nauk , pp. 953–956. Russian Academy of Sci-
ences, 1957.
11

--- PAGE 12 ---
Universality of Linear Recurrences Followed by Non-linear Projections
LeCun, Y . The mnist database of handwritten digits.
http://yann. lecun. com/exdb/mnist/ , 1998.
Lewicki, M. S. and Sejnowski, T. J. Learning overcomplete
representations. Neural computation , 2000.
Li, Y ., Cai, T., Zhang, Y ., Chen, D., and Dey, D. What makes
convolutional models great on long sequence modeling?
arXiv preprint arXiv:2210.09298 , 2022a.
Li, Z., Han, J., Weinan, E., and Li, Q. Approximation and
optimization theory for linear continuous-time recurrent
neural networks. J. Mach. Learn. Res. , 2022b.
Lorenz, E. N. Deterministic nonperiodic flow. Journal of
atmospheric sciences , 20(2):130–141, 1963.
Lotka, A. J. Elements of physical biology . Williams &
Wilkins, 1925.
Lu, C., Schroecker, Y ., Gu, A., Parisotto, E., Foerster, J.,
Singh, S., and Behbahani, F. Structured state space mod-
els for in-context reinforcement learning. Advances in
Neural Information Processing Systems , 2023.
Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L. The expres-
sive power of neural networks: A view from the width.
Advances in neural information processing systems , 30,
2017.
Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May,
J., and Zettlemoyer, L. Mega: moving average equipped
gated attention. arXiv preprint arXiv:2209.10655 , 2022.
Martin, E. and Cundy, C. Parallelizing linear recur-
rent neural nets over sequence length. arXiv preprint
arXiv:1709.04057 , 2017.
Mashreghi, J., Nazarov, F., and Havin, V . Beurling–
malliavin multiplier theorem: the seventh proof. St. Pe-
tersburg Mathematical Journal , 17(5):699–744, 2006.
M¨untz, C. H. ¨Uber den approximationssatz von Weierstrass .
Springer, 1914.
Nguyen, E., Goel, K., Gu, A., Downs, G. W., Shah, P., Dao,
T., Baccus, S. A., and R ´e, C. S4nd: Modeling images and
videos as multidimensional signals using state spaces. Ad-
vances in Neural Information Processing Systems , 2022.
Orvieto, A., Smith, S. L., Gu, A., Fernando, A., Gulcehre,
C., Pascanu, R., and De, S. Resurrecting recurrent neural
networks for long sequences. International Conference
on Machine Learning , 2023.
Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,
S., Cao, H., Cheng, X., Chung, M., Grella, M., GV , K. K.,
et al. Rwkv: Reinventing rnns for the transformer era.
arXiv preprint arXiv:2305.13048 , 2023.Pinkus, A. Approximation theory of the MLP model in
neural networks. Acta numerica , 8:143–195, 1999.
Sch¨afer, A. M. and Zimmermann, H. G. Recurrent neural
networks are universal approximators. In Artificial Neural
Networks–ICANN 2006: 16th International Conference,
Athens, Greece, September 10-14, 2006. Proceedings,
Part I 16 , pp. 632–640. Springer, 2006.
Siegelmann, H. T. and Sontag, E. D. On the computational
power of neural nets. In Proceedings of the Fifth Annual
Workshop on Computational Learning Theory , COLT ’92,
pp. 440–449, New York, NY , USA, 1992a. Association
for Computing Machinery.
Siegelmann, H. T. and Sontag, E. D. On the computational
power of neural nets. In Proceedings of the fifth annual
workshop on Computational learning theory , pp. 440–
449, 1992b.
Smith, J. T., Warrington, A., and Linderman, S. W. Sim-
plified state space layers for sequence modeling. arXiv
preprint arXiv:2208.04933 , 2022.
Smith, J. T., Warrington, A., and Linderman, S. W. Simpli-
fied state space layers for sequence modeling. Interna-
tional Conference on Learning Representations , 2023.
Sun, Y ., Dong, L., Huang, S., Ma, S., Xia, Y ., Xue, J.,
Wang, J., and Wei, F. Retentive network: A successor to
transformer for large language models. arXiv preprint
arXiv:2307.08621 , 2023.
Sz´asz, O. ¨Uber die approximation stetiger funktionen durch
lineare aggregate von potenzen. Mathematische Annalen ,
77(4):482–496, 1916.
Tay, Y ., Dehghani, M., Abnar, S., Shen, Y ., Bahri, D., Pham,
P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long
range arena: A benchmark for efficient transformers. In
International Conference on Learning Representations ,
2020.
Tlas, T. Bump functions with monotone fourier transforms
satisfying decay bounds. Journal of Approximation The-
ory, 278:105742, 2022.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y ., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., et al. Llama 2: Open foundation and fine-
tuned chat models. arXiv preprint arXiv:2307.09288 ,
2023.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-
tion is all you need. Advances in Neural Information
Processing Systems , 2017.
12

--- PAGE 13 ---
Universality of Linear Recurrences Followed by Non-linear Projections
V olterra, V . Variations and fluctuations of the number of in-
dividuals in animal species living together. ICES Journal
of Marine Science , 3(1):3–51, 1928.
Vyshemirsky, V . and Girolami, M. A. Bayesian ranking
of biochemical system models. Bioinformatics , 24(6):
833–839, 2008.
Wang, J., Yan, J. N., Gu, A., and Rush, A. M. Pretraining
without attention, 2023.
Wang, S. and Xue, B. State-space models with layer-wise
nonlinearity are universal approximators with exponen-
tial decaying memory. arXiv preprint arXiv:2309.13414 ,
2023.
Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H.
Linformer: Self-attention with linear complexity. arXiv
preprint arXiv:2006.04768 , 2020.
Yang, S., Wang, B., Shen, Y ., Panda, R., and Kim, Y . Gated
linear attention transformers with hardware-efficient train-
ing. arXiv preprint arXiv:2312.06635 , 2023.
Zucchet, N., Kobayashi, S., Akram, Y ., von Oswald, J.,
Larcher, M., Steger, A., and Sacramento, J. Gated recur-
rent neural networks discover attention. arXiv preprint
arXiv:2309.01775 , 2023a.
Zucchet, N., Meier, R., Schug, S., Mujika, A., and Sacra-
mento, J. Online learning of long-range dependencies,
2023b.
13

--- PAGE 14 ---
Universality of Linear Recurrences Followed by Non-linear Projections
Appendix
A. Related Works
Issues with attention for long-range reasoning. Efficient processing of long sequences is an important open question in
deep learning. Attention-based transformers (Vaswani et al., 2017) provide a scalable approach but suffer from quadratically
increasing complexity in inference/memory as the sequence length grows. While many approaches exist to alleviate this
issue, e.g. efficient memory management (Dao et al., 2022; Dao, 2023) and architectural modifications (Wang et al., 2020;
Kitaev et al., 2020; Child et al., 2019; Beltagy et al., 2020), the sequence length in large language models is usually kept to
2k/4ktokens for this reason (e.g. Llama2 (Touvron et al., 2023)). In addition, in some long-range reasoning tasks (Tay
et al., 2020) attention does not seem to provide the correct inductive bias , leading to poor performance in addition to high
computational costs.
Success of modern recurrent layers. Due to the issues outlined above, the community has witnessed in the last year the
rise of new, drastically innovative, recurrent alternatives to the attention mechanism, named state-space models (SSMs).
The first SSM, S4, was introduced by (Gu et al., 2021), and since then, a plethora of variants have been proposed:
LiquidS4 (Hasani et al., 2022), DSS (Gupta et al., 2022), S4D (Gu et al., 2022), S5 (Smith et al., 2022), RWKV (Peng et al.,
2023) and RetNet (Sun et al., 2023) among others. These models achieve remarkable performance, surpassing all modern
attention-based transformer variants by an average 20% accuracy on challenging sequence classification tasks (Tay et al.,
2020). SSMs have reached outstanding results in various domains beyond toy datasets (Nguyen et al., 2022; Goel et al.,
2022; Gu et al., 2021; Lu et al., 2023; Zucchet et al., 2023b). SSMs also were successfully applied to language modeling,
and are sometimes used in combination with attention (Fu et al., 2023; Wang et al., 2023; Ma et al., 2022). At inference time,
all SSMs coincide with a stack of linear Recurrent Neural Networks, interleaved with MLPs and normalization. Linearity of
the RNNs allows fast parallel processing with FFTs (Gu et al., 2022) or parallel scans (Smith et al., 2023).
The linear recurrent unit (LRU). Among modern architectures for long-range reasoning based on recurrent modules, the
simplest is perhaps Linear Recurrent Unit (LRU) (Orvieto et al., 2023): while SSMs rely on the discretization of a structured
continuous-time latent dynamical system, the LRU is directly designed for discrete-time systems (token sequences), and
combines easy hyperparameter tuning with solid performance and scalability. The only differences between the LRU and
the standard RNN update xk=Axk−1+Buk(uis the input at a specific layer and xis the hidden-state, then fed into a
position-wise MLP) are (1) the system operates in the complex domain (required for expressivity, see discussion in (Orvieto
et al., 2023)) (2) to enhance stability and better control how quickly gradients vanish, A(diagonal) is learned using polar
parametrization and log-transformed magnitude and phase. Finally, (3) the recurrence is normalized through an extra
optimizable parameter that scales the input to stabilize signal propagation. The parametrization of linear RNNs of (Orvieto
et al., 2023) was found to be effective also in surpassing deep LSTMs and GRUs (Zucchet et al., 2023a). We use the LRU
codebase12as a starting point for our experiments, when the linear RNN is learned.
Approximation theory for MLP and non-linear RNNs. The approximation properties of deep neural networks with
ReLU activations are well studied. While recent advances concern the effect of depth (Lu et al., 2017), the study by Pinkus
(1999), as well as previous works (Funahashi, 1989; Hornik et al., 1989; Hornik, 1991; Barron, 1993), already established
the power of neural networks with a single hidden layer, which can approximate arbitrary continuous non-linear maps on
compacts as the size of the hidden layer grows to infinity. The cleanest result is perhaps the one of Barron (1993), that we
heavily use in this paper.
In the context of non-linear RNN approximation of dynamical systems (e.g. in neuroscience), the state-to-state computation
can be seen as part of an MLP (see e.g. Hanson & Raginsky (2020)): we have xk=σ(Axk−1+Buk), where σis is a
non-linearity. As a result, wide non-linear RNNs can in principle approximate non-linear dyamical systems, as we discuss in
detail in App. B.
Meanwhile, linear RNNs, where xk=Axk−1+Buk, have often been considered of minor interest, as they equivalent in
approximation power to convolutions (Li et al., 2022b) (see App. B). In this paper we take a different approach: we show
that when sufficiently wide, the linear RNNs do not form a bottleneck, and the architecture maintains universality through
the application of the pointwise MLP on the hidden state, as done in recent SSMs achieving state-of-the-art results (Gu
et al., 2021; Smith et al., 2022; Orvieto et al., 2023). As motivated thoroughly in the paper, this architecture unlocks parallel
computation, in contrast to what is possible with directly placing non-linearities in the recurrence.
12https://github.com/NicolasZucchet/minimal-LRU/tree/main/lru
14

--- PAGE 15 ---
Universality of Linear Recurrences Followed by Non-linear Projections
B. Approximation theory for (non-linear) RNNs
We recall a result on universality of MLPs already stated in the main paper.
Definition 3 (Barron function) .LetF(ω)be the Fourier transform of f:Rn→R.fbelongs to the Barron class if
Cf=R
Rn∥ω∥2|F(ω)|dω <∞.
Theorem 1 (Universality of 1HL-MLPs) .Consider g(x)parametrized by a 1HL-MLP (with Dhidden neurons): g(x) =PD
k=1˜ckσ(⟨˜ak, x⟩+˜bk) + ˜c0, where σis any sigmoidal function13. Letf:Rn→Rbe continuous with Barron constant
Cf. IfD≥2r2C2
fϵ−2, then there exist parameters such that sup∥x∥≤r|f(x)−g(x)| ≤ϵ.
B.1. Guarantees for RNNs with recurrent non-linearities
Research on universality of non-linear RNNs dates back to (Siegelmann & Sontag, 1992b). We present here a more recent
result by Hanson & Raginsky (2020).
Theorem 5 (Universality of non-linear RNNs) .Consider the continuous-time non-linear dynamical system with form
˙¯x(t) =¯f(¯x(t), u(t)),¯y(t) =h(¯x(t)), (8)
with¯x(t)∈R¯N,u(t)∈RM. Under some technical assumptions (bounded input, non-chaotic f), for any ϵ >0there exists
a non-linear RNN
˙x(t) =−1
τx(t) +σ(Ax(t) +Bu(t)), y(t) =Cx(t), (9)
for some non-polynomial σ,τ >0,A∈RN×N,B∈RN×M,C∈RM×Nthat approximates the solution to Eq. 8 up to
error ϵuniformly in time, on compact sets of inputs.
The result above typically involves taking N(RNN width) to infinity.
Proof. We briefly outline the idea behind the proof, and invite the reader to refer to (Hanson & Raginsky, 2020) for details.
Approximating the solution to Eq. 8 is equivalent to approximating the infinitesimal solution generator, which is a non-linear
function of (x, u). By Barron’s Theorem (Thm. 1), this generator can be approximated by a one-layer MLP, that is exactly
the right-hand side of Eq. 9. □
B.2. Guarantees for linear RNNs
Simply taking out the non-linearity from the recurrence in Eq. 9 restricts the function class to convolutions. To start, recall
that the linear continuous-time RNN on one-dimensional inputs
˙x(t) =Ax(t) +Bu(t), y(t) =Cx(t),
withA∈RN×N,B∈RN×M,C∈R1×Nhas solutions given by a convolution.
x(t) =Zt
0C⊤eAsBu(t−s)ds=:Zt
0ρ(s)⊤u(t−s)ds=:X
i(ρi⋆ ui)t.
Let us call ˆHNthe class of functionals parametrizable with linear RNNs with hidden state of dimension N, and ˆH=
∪N∈N+HN.
HN:=
{Ht:t∈R}, Ht(u) =Zt
0C⊤eAsBu(t−s)ds, C∈R1×N, A∈RN×N, B∈RN×M
.
It turns out that convolutions are dense in the class of linear functionals. Let U=C0(R,Rd)with norm ∥u∥=
supt∈R∥u(t)∥∞.
Theorem 6 (Linear functionals in C0(R,Rd)are convolutions (Li et al., 2022b)) .Let{Ht:t∈R}be a family of continuous,
linear, causal, regular, and time-homogeneous functionals on U, i.e. such that
13limx→−∞ σ(x) = 0 andlimx→∞σ(x) = 1 .
15

--- PAGE 16 ---
Universality of Linear Recurrences Followed by Non-linear Projections
1. (Continuous) ∀t∈R,sup∥u∥<1Ht(u)<∞.
2. (Linear) ∀t∈R,u, v∈ U andν, λ∈R, we have Ht(λu+νv) =λHt(u) +νHt(v).
3. (Causal) For all u, v∈ U such that u(s) =v(s)for all s≤t, he have Ht(v) =Ht(u).
4.(Regular) Let (un)be a sequence in Us.t.un(s)→0for almost every s∈R, then, for all t∈R, we have
limn→∞Ht(un) = 0 .
5. (Time Homogeneous) For all u∈ U letuτ
t=u(t−τ), then Ht(uτ) =Ht+τ(u).
Then, for any {Ht:t∈R}there exist a function (a kernel) ¯ρ:R+→RMsuch that for all t∈R.
Ht(u) =Z∞
0ρ(s)⊤u(t−s)ds=X
i(¯ρi⋆ ui)t.
Theorem 7 (Linear RNNs can parametrize any convolution (Li et al., 2022b)) .Let{Ht:t∈R}be a family of continuous,
linear, causal, regular, and time-homogeneous functionals on U. Then, for any ϵ >0there exists {ˆHt:t∈R} ∈ˆHsuch
that
sup
t∈Rsup
∥u∥<1∥Ht(u)−ˆHt(u)∥ ≤ϵ.
The result above typically involves taking N(RNN width) to infinity.
This paper. There is a sizable gap between the power of nonlinear and linear RNNs. We show in this paper that placing
a nonlinearity at the output of linear RNNs (unlocking parallel computation) allows approximation of arbitrary regular
non-linear sequence-to-sequence mappings.
16

--- PAGE 17 ---
Universality of Linear Recurrences Followed by Non-linear Projections
C. Details on Multidimensional Input Reconstruction
In the main text, we showed that linear diagonal RNN computations on one-dimensional input sequences can be written
in matrix form using a Vandermonde matrix (Sec. 3.1). For convenience of the reader, we repeat the reasoning here: let
H=M= 1, the encoder ebe the identity, and B= (1,1, . . . , 1)⊤. Then, eq. (3) can be written as
xk=
λk−1
1 λk−2
1··· λ11
λk−1
2 λk−2
2··· λ21
...............
λk−1
N λk−2
N··· λN1

u1
u2
...
uk
=Vku⊤
1:k.
where u1:k=v1:k= (vi)k
i=1∈R1×k, andVkis a Vandermonde matrix. As long as N≥k, we can hope to recover u1:kby
pseudoinversion of the Vandermonde:
v⊤
1:k=u⊤
1:k=V+
kxk,
Here, we give details on the design of input projections such that the RNN output from multidimensional inputs can also be
seen as matrix multiplication. Let us define
vec(v1:k) :=
v⊤
1,1:k
v⊤
2,1:k
...
v⊤
M,1:k
∈RkM,
The matrix Bwe are going to use in our linear diagonal RNN is
B=
1N′×1··· 0N′×10N′×1
0N′×1··· 0N′×10N′×1
............
0N′×1··· 1N′×10N′×1
0N′×1··· 0N′×11N′×1
,
where we select N=MN′. With this choice, the linear diagonal RNN output can be written as
xk=
Vk,1
Vk,2
...
Vk,M

v⊤
1,1:k
v⊤
2,1:k
...
v⊤
M,1:k
=˜Vk1
vec(v:,1:k)
,
where Vk= diag( Vk,1, Vk,2,···, Vk,M), and Vk,j∈CN′×kis the Vandermonde matrix corresponding to the block Λjin
the diagonal recurrent matrix Λ:
Λ = diag(Λ 1,Λ2,···,ΛM)∈C(N′M)×(N′M),
Λj= diag( λ1,j, λ2,j,···, λN′,j)∈CN′×N′,
Vk,j=
λk−1
1,j λk−2
1,j··· λ1,j1
λk−1
2,j λk−2
2,j··· λ2,j1
...............
λk−1
N′,jλk−2
N′,j··· λN′,j1
∈CN′×k.
This construction effectively decouples each input dimension and reduces the discussion to the one-dimensional setting:
invertibility of Vkis guaranteed by invertibility of each block, provided N′≥Land that eigenvalues are distinct. Slight
changes can be made to keep also track of the timestamp (see Sec. 3.1.1) and to adapt to the sparse setting (see Sec. 3.1.2).
17

--- PAGE 18 ---
Universality of Linear Recurrences Followed by Non-linear Projections
D. Expressivity Proofs
One of our main steps involved computation of the Barron constant of the function mapping the RNN hidden state to the
output.
Proposition 2 (MLP, single timestamp) .The number of hidden neurons in the MLP gsufficient to approximate fk:=Tk◦Ωk
at level ϵfrom the RNN hidden state xkis bounded by 4r2C2
Tk∥Ωk∥2
2S3ϵ−2where rbounds the hidden state magnitude,
andCTkis the Barron constant of Tk.
Since Ωkis a matrix, the result can be proved by computing the Barron constant of a function where the argument is the
output of a linear map.
[Change of variables] Let A∈Rp×nandf(x) =g(Ax), then
Cf=∥A∥2Cg.
Proof. The inverse Fourier transform formula directly leads to
f(x) =Z
Rpei⟨p,Ax⟩G(ξ)dξ. (10)
Let us now compute the Fourier Transform of f.
F(ω) =Z
Rne−i⟨ω,x⟩f(x)dx (11)
=Z
Rne−i⟨ω,x⟩Z
Rpei⟨ξ,Ax⟩G(ξ)dξ
dx (12)
=Z
RnZ
Rpe−i⟨ω,x⟩ei⟨A⊤ξ,x⟩G(ξ)dξdx (13)
=Z
RpZ
Rnei⟨A⊤ξ−ω,x⟩dx
G(ξ)dξ. (14)
Recall now the definition of the Dirac delta:
δ(z) =1
2πZ
Reiνzdν. (15)
Therefore
F(ω) =Z
Rpδ(A⊤ξ−ω)G(ξ)dξ. (16)
Note that this is a singular measure in Rn: lives in a linear p-dimensional subspace. Further, note that
Cf=Z
Rn∥ω∥2· |F(ω)|dω (17)
=Z
Rn∥ω∥2·Z
Rpδ(A⊤ξ−ω)G(ξ)dξdω (18)
≤Z
Rn∥ω∥2·Z
Rpδ(A⊤ξ−ω)|G(ξ)|dξdω (19)
≤Z
RpZ
Rn∥ω∥2δ(A⊤ξ−ω)dω
|G(ξ)|dξ (20)
=Z
Rp∥A⊤ξ∥2· |G(ξ)|dξ (21)
=∥A⊤∥2Z
Rp∥ξ∥2· |G(ξ)|dξ. (22)
The proof is done, since ∥A⊤∥2=∥A∥(same singular values), and Cg=R
Rp∥ξ∥2· |G(ξ)|dξ. □
We now proceed proving the complexity for interpolation of sequences of Barron functions. This directly implies our main
result (Thm.2).
18

--- PAGE 19 ---
Universality of Linear Recurrences Followed by Non-linear Projections
Theorem 4 (Combination of Barron Maps) .Letf: [1,2, . . . , L ]×Rn→Rbe such that each f(k,·)is Barron with
constant Cfkand Fourier norm C′
fk. There exists an extension ˜f:Rn+1→Roff, s.t. ˜fis Barron with constant
˜C≤ O(PL
k=1Cfk+C′
fk) =O(L)and˜f(x, k) =f(x, k),∀x∈Rnandk∈1,2, . . . , L .
Proof. Let us consider the following definition for ˜f:
˜f(x, t) =LX
k=1f(x, k)h(t−k), (23)
where h:R→Ris a filter (see discussion after the proof) with support in [−1,1]. Compactness in the support of hleads to
the desired property ˜f(x, k) =f(x, t), for all k= 1,2, . . . L . Let us now compute the Fourier transform of ˜f. Frequencies
are of the form ω= (w, ν), with w∈Rn,ν∈R.
˜F(ω) =1
(2π)n+1Z
RZ
Rn˜f(x, t)e−i⟨w,x⟩e−iνtdxdt (24)
=1
(2π)n+1Z
RZ
Rn"LX
k=1f(x, k)h(t−k)#
e−i⟨w,x⟩e−iνtdxdt (25)
=1
(2π)n+1LX
k=1Z
Rnf(x, k)e−i⟨w,x⟩dx
·Z
Rh(t−k)e−iνtdt
(26)
=LX
k=1˜Fk(w)H(ν)eiνk, (27)
where His the Fourier transform of h, and the factor eiνkcomes from the shift h(· −k). All in all, we get
˜F(w, ν) =H(ν)LX
k=1˜Fk(w)eiνk. (28)
Trivially,
|˜F(w, ν)| ≤ |H (ν)|LX
k=1|˜Fk(w)|. (29)
Therefore:
˜C=Z
Rn+1∥ω∥2|˜F(ω)|dω (30)
≤Z
RnZ
R∥(w, ν)∥2·"
|H(ν)|LX
k=1|˜Fk(w)|#
dwdν (31)
=LX
k=1Z
RnZ
R∥(w, ν)∥2· |H(ν)| · |˜Fk(w)|dwdν. (32)
Using the triangle inequality:
˜C≤Z
RnZ
R(∥w∥2+|ν|)· |H(ν)| · |˜Fk(w)|dwdν (33)
=Z
RnZ
R∥w∥2· |H(ν)| · |˜Fk(w)|dwdν +Z
RnZ
R|v| · |H(ν)| · |˜Fk(w)|dwdν (34)
=CfkC′
h+ChC′
fk. (35)
This concludes the proof since (see next paragraph) ChandC′
hare problem-independent bounded constants. □
The proof of the theorem above concludes stating that ChandC′
hare problem-independent bounded constants. Recall that,
in our proof, h:R→Rhas compact support in [−1,1]. We can design hsuch that its Fourier transform has fast decay:
19

--- PAGE 20 ---
Universality of Linear Recurrences Followed by Non-linear Projections
Theorem 8 ((Tlas, 2022)) .For any δ∈(0,1)and any c >0there is a function h(t)which is C∞, real, even, nonnegative,
supported in [−1,1]and whose Fourier transform H(ν)is monotone decreasing for ν≥0and satisfies the following double
inequality
exp(−(1 +ϵ)cνδ)≲H(ν)≲exp(−(1−ϵ)cνδ), (36)
for any ϵ >0.
This result, rooted in the Beurling-Malliavin multiplier theorem(Mashreghi et al., 2006), ensures that we can design hon a
compact support with exponentially decaying frequencies. This implies that both integrals
Ch=Z
R|ν||H(ν)|dν, C′
h=Z
R|H(ν)|dν (37)
are bounded and independent on the specific form of the function fwe want to approximate.
20

--- PAGE 21 ---
Universality of Linear Recurrences Followed by Non-linear Projections
E. Additional experiments
E.1. Reconstruction using the Vandermonde inverse (support to Section 3.1)
We consider linear diagonal RNNs with the Ndiagonal entries of Λsampled inside the unit disk in C, uniformly in angle in
between radii rminand 1. We consider the hidden state xL∈CNcomputed after LRNN steps, i.e. after the sequence is
completely processed . We want to recover the input sequence from the hidden state using the Vandermonde inverse V+
L(see
Sec. 3.1). If N≥L, since under random initialization the determinant of any set of Lcolumns of VLis positive, we can in
theory achieve perfect reconstruction. In practice, VLis ill conditioned — especially if rminis not close to 1. This causes
some problem in the pseudoinverse computation, which may result in imperfect reconstruction.
In Figure 6 we provide evidence on the MNIST dataset (flattened image =784 tokens): we observe that, if eigenvalues are
sampled with rmin= 0, by multiplying xLbyV+
Lwe are only able to recover recent history. Instead, moving closer to the
unit disk allows almost perfect reconstruction at N= 784 , and a satisfactory result already at N= 512 .
0 100 200 300 400 500 600 700 8000.25
0.000.250.500.751.00Reconstruction via Vandermonde Inverse (N=256, rmin=0)
0 10 200
5
10
15
20
25Result (N=256, rmin=0)
0 100 200 300 400 500 600 700 8000.2
0.00.20.40.60.81.0Reconstruction via Vandermonde Inverse (N=256, rmin=0.999)
0 10 200
5
10
15
20
25Result (N=256, rmin=0.999)
0 100 200 300 400 500 600 700 8000.2
0.00.20.40.60.81.0Reconstruction via Vandermonde Inverse (N=512, rmin=0)
0 10 200
5
10
15
20
25Result (N=512, rmin=0)
0 100 200 300 400 500 600 700 8000.25
0.000.250.500.751.00Reconstruction via Vandermonde Inverse (N=512, rmin=0.999)
0 10 200
5
10
15
20
25Result (N=512, rmin=0.999)
0 100 200 300 400 500 600 700 8000.2
0.00.20.40.60.81.0Reconstruction via Vandermonde Inverse (N=784, rmin=0)
0 10 200
5
10
15
20
25Result (N=784, rmin=0)
0 100 200 300 400 500 600 700 8000.00.20.40.60.81.0Reconstruction via Vandermonde Inverse (N=784, rmin=0.999)
0 10 200
5
10
15
20
25Result (N=784, rmin=0.999)
0 100 200 300 400 500 600 700 8000.2
0.00.20.40.60.81.0Reconstruction via Vandermonde Inverse (N=1024, rmin=0)
0 10 200
5
10
15
20
25Result (N=1024, rmin=0)
0 100 200 300 400 500 600 700 8000.00.20.40.60.81.0Reconstruction via Vandermonde Inverse (N=1024, rmin=0.999)
0 10 200
5
10
15
20
25Result (N=1024, rmin=0.999)
Figure 6. Reconstrucion of MNIST digits (seen as a sequence) from the last linear RNN state. Reconstruction map is not learned but is
instead based on the Vandermonde Pseudoinverse (i.e. worst-case setting, as discussed in the main text). Results are far more accurate if
the reconstruction map is learned (see Fig. 4).
In Figure 7 we clearly show that the average reconstruction error (average over 10kimages samples and 10 random
re-samplings of the linear RNN) is decreasing both as a function of the hidden state size (see discussion in Sec. 3.1) and of
rmin(rmax= 1). The same pattern is observed for the condition number of V⊤
LVL. On the same figure, we show how the
error is distributed over timestamps: it is clear that, for rmin≪1, the reconstruction only covers the last few hundreds of
tokens – a property which is liked to the bad condition number observed in this setting.
Last, in Figure 8 we show what happens when picking the Ndiagonal entries of Λto be the N-th complex roots of 1: as
shown in (C ´ordova et al., 1990), in this setting the Vandermonde condition number is 1. We observe that we can indeed
reconstruct perfectly the output for N= 784 . However, for smaller values of N, the reconstruction presents undesired
artifacts.
21

--- PAGE 22 ---
Universality of Linear Recurrences Followed by Non-linear Projections
0 0.8 0.9 0.99 0.999 1
Value of rmin0.000.020.040.060.080.100.120.140.16Average Reconstruction Error
N=256
N=512
N=784
N=1024
0 0.9 0.99 0.999 0.9999 1
Value of rmin10410710101013101610191022Condition number of VLVL
N=256
N=512
N=1024
N=2048
N=4096
0100 200 300 400 500 600 700 800
timestamp(k)1015
1013
1011
109
107
105
103
101
Average Reconstruction Error (N = 512)
rmin = 0
rmin = 0.8
rmin = 0.9
rmin = 0.99
rmin = 0.999
rmin = 1
0100 200 300 400 500 600 700 800
timestamp(k)1015
1013
1011
109
107
105
103
101
Average Reconstruction Error (N = 784)
rmin = 0
rmin = 0.8
rmin = 0.9
rmin = 0.99
rmin = 0.999
rmin = 1
0100 200 300 400 500 600 700 800
timestamp(k)1015
1013
1011
109
107
105
103
101
Average Reconstruction Error (N = 1024)
rmin = 0
rmin = 0.8
rmin = 0.9
rmin = 0.99
rmin = 0.999
rmin = 1
Figure 7. Error over 10kMNIST images and 10 re-sampling of the RNN. Comment in text. Reconstruction is based on the Vandermonde
Pseudoinverse.
0 100 200 300 400 500 600 700 8000.00.20.40.60.81.0Reconstruction via Vandermonde Inverse (N=256, roots of unity)
0 10 200
5
10
15
20
25Result (N=256, roots of unity)
0 100 200 300 400 500 600 700 8000.00.20.40.60.81.0Reconstruction via Vandermonde Inverse (N=784, roots of unity)
0 10 200
5
10
15
20
25Result (N=784, roots of unity)
Figure 8. Same setting as Figure 6. Eigenvalues are the N-th complex roots of 1. Comment in text. Reconstruction map is not learned
but is instead based on the Vandermonde Pseudoinverse (i.e. worst-case setting, as discussed in the main text).
E.2. Reconstruction under sparsity (support to §3.1.2)
In Figures 10 and 11 we test the discussion in Sec. 3.1.2 in a controlled setting. We consider one-dimensional stream of
L= 4096 random inputs sparse in a basis of P= 32 Haar wavelets (Haar, 1911). The linear diagonal RNN has Λ∈CN×N
with eigenvalues sampled uniformly at random from T(0.95,1)(Fig. 10) or T(0.99,1)(Fig. 11), where
T[rmin, rmax] :={λ∈C|rmin≤ |λ| ≤rmax}.
We use matrix B= (1,1, . . . , 1)⊤. Plotted is the rank of Vk,Ψk,Ωk=VkΨkaskincreases (see notation in Sec. 3.1.2).
We show how the reconstruction error behaves when reconstructing u1:k= Ψ kαu
kwithαu
k= Ω+
kxk. In the figures, we plot
the error for reconstruction of the tokens (ui)k
i=1from xk, for all k≤L. AsNgets larger the reconstruction error gets
uniformly negligible. In particular, if we initialize in T(0.95,1)then the minimum Nwe need for perfect reconstruction of
around N= 256 . If instead we initialize closer to the unit circle, then N= 64 is enough for perfect reconstruction. This
finding is similar to the one presented in Sec. E.1.
On a more fundamental level, we study the condition number of the matrix ΩTΩ, where Ω =VLΨL. This condition number
quantifies the numerical stability of the pseudoinverse Ω+, used in reconstruction. In Figure 9, we show the logarithm of the
condition number for a sequence of length 512sparse in a basis of Peigenfunctions. As rmingets close to 1the condition
number decreases as we saw in §E.1. Crucially however, the condition number also decreases as Pdecreases.
22

--- PAGE 23 ---
Universality of Linear Recurrences Followed by Non-linear Projections
64128 256 512
sparseness parameter P256
512
1024hidden state size Nrmin=0
64128 256 512
sparseness parameter P256
512
1024hidden state size Nrmin=0.9
64128 256 512
sparseness parameter P256
512
1024hidden state size Nrmin=0.99
64128 256 512
sparseness parameter P256
512
1024hidden state size Nrmin=0.999
2.55.07.510.012.515.017.5
Figure 9. Logarithm of the condition number for ΩTΩdecreases as N(hidden state dimension) increases and as P(number of basis
functions) decreases. As always, rmincloser to 1 leads to better conditioning and therefore more stable reconstruction.
0 1000 2000 3000 4000
k050100150200250RankRank of Vk, up to step k (P=32)
N=8
N=16
N=32
N=64
N=128
N=256
0 1000 2000 3000 4000
k051015202530RankRank of k=Vkk, up to step k (P=32)
N=8
N=16
N=32
N=64
N=128
N=256
0 1000 2000 3000 4000
k1016
1013
1010
107
104
101
Relative ErrorInput Reconstruction Error up to step k (P=32)
N=8
N=16
N=32
N=64
N=128
N=256
Figure 10. Reconstruction of a random sparse input. Λinitialized uniformly at random on T(0.95,1). Comment in the text.
0 1000 2000 3000 4000
k050100150200250RankRank of Vk, up to step k (P=32)
N=8
N=16
N=32
N=64
N=128
N=256
0 1000 2000 3000 4000
k051015202530RankRank of k=Vkk, up to step k (P=32)
N=8
N=16
N=32
N=64
N=128
N=256
0 1000 2000 3000 4000
k1016
1013
1010
107
104
101
Relative ErrorInput Reconstruction Error up to step k (P=32)
N=8
N=16
N=32
N=64
N=128
N=256
Figure 11. Reconstruction of a random sparse input. Λinitialized uniformly at random on T(0.99,1). Comment in the text.
23

--- PAGE 24 ---
Universality of Linear Recurrences Followed by Non-linear Projections
8 16 32 64 128 256
Sparseness coefficient P1021051081011L2
Symlets
Haar
Coiflets
Van. Inverse
Figure 12. Conditioning of the reconstruction map for inputs sparse in 3 different wavelet basis ( N= 256 ). Average over 1krandom init.,
rmin= 0.999, rmax= 1. Comment in the main text.
E.3. Approximation of sequence-to-sequence maps (ODE Systems)
We consider approximating sequence-to-sequence maps (vi)L
i=1T7→(yi)L
i=1defined by Runge-Kutta discretization of
the flow of a controlled differential equation ˙zt=f(zt, vt), yt=h(zt), where (vt)tis the input, fis a non-linear
multidimensional function, hprojects the multidimensional state ztinto a one-dimensional output. An example is the
Protein Transduction (PT) system (Vyshemirsky & Girolami, 2008):
˙z1(t) =−k1z1(t)−k2z1(t)z3(t) +k3z4(t) +v(t)
˙z2(t) =k1z1(t)
˙z3(t) =−k2z1(t)z3(t) +k3z4(t) +Vz5(t)
Km+z5(t)
˙z4(t) =k2z1(t)z3(t)−(k3+k4)z4(t)
˙z5(t) =k4z4(t)−Vz5(t)
Km+z5(t)
We identify yk=z1(∆k), where ∆ = 0 .01, and vk=v(∆k). We sample (vi)L
i=1(L= 2048 in PT) from a linear
combination of 16 base wavelets of low frequency ( bias: input is random but has slow variations ), and set the ground
truth (yi)L
i=1to be the result of Runge-Kutta integration with stepsize ∆. As hyperparameters, we use k1= 0.07, k2=
0.6, k3= 0.05, k4= 0.3, V= 0.017, Km= 0.3, z1(0) = 1 , z2(0) = 0 , z3(0) = 1 , z4(0) = z5(0) = 0 as prescribed
by Vyshemirsky & Girolami (2008). Results using approximation of one linear RNN followed by a 1HL-MLP (shared
across timestamps) are shown in Fig. 5 and discussed in the main text. To train, we use 10krandom (low frequency)
trajectories and test on 1ktrajectories with same distribution. Experiments run on a single A5000 using an LRU in
JAX ( https://github.com/NicolasZucchet/minimal-LRU/tree/main/lru ).
In this appendix, we additionally discuss performance in approximating the solution of two other controlled ODEs. Settings
are same as used for PT unless stated otherwise. The first ODE (results in Fig. 15) is a Lotka-Volterra (LV) system (Lotka,
1925; V olterra, 1928):
˙z1(t) =z1(t)(a−b·z2(t))
˙z2(t) =−z2(t)(c−d·z1(t)) +v(t)
where a= 1.0, b= 0.6, c= 1.0, d= 0.7, and we initialize z1(0) = 1 .0, z2(0) = 0 .5as used in Dondelinger et al. (2013).
For integration, we use a stepsize of ∆ = 0 .01andyk=z1(∆k). Here, sequence length is again L= 2048 .
Finally, we consider an extremely challenging scenario : the Lorentz system (LZ) (Lorenz, 1963), which notoriously has
chaotic solutions (named strange attractor, linked to the butterfly effect):
˙z1(t) =σ·(z2(t)−z1(t))
˙z2(t) = (r−z3(t))z1(t)−z2(t) +v(t)
˙z3(t) =z2(t)z1(t)−b·z3(t)
With our parameter choices σ= 10 , r= 26 , b= 8/3and initialization z1(0) = −0.89229143 , z2(0) =
1.08417925 , z3(0) = 2 .34322702 (parameter source: Wikipedia, visited September 2023), the system has a chaotic
24

--- PAGE 25 ---
Universality of Linear Recurrences Followed by Non-linear Projections
0 100 200 300 400 5001.5
1.0
0.5
0.00.51.0outputs
inputs
Figure 13. Behavior of the input-output LV map for random low-frequency inputs controlling the second state equation (output is the
discretized first component). The output has binary nature (up or down) with shifted phase: tiny variations in the input cause drastic effects
on the output (butterfly effect). Plotted is also an illustration of the 3-dimensional dynamics, source: https://www.iaacblog.com/
programs/algorithmic-emergence-chaotic-attractor-equations/ .
behavior as shown in Figure 13. We choose an integration timestep ∆ = 0 .002and, due to the non-linear chaotic and
possibly unstable nature of the controlled attractor, consider L= 512 in this setting.
Note on training. Training one layer of linear RNN + MLP on these ODEs exhibits huge variation across seeds (see
Fig. 14). Since in this paper we want to show that there exist a Linear RNN+MLP configuration able to model non-linear
sequence to sequence maps, we consider the following setup: we run each experiment and hyperparameter sweep on seeds
1−6, and only report the best performance on the test data (train loss always lower). Based on our experience with the
LRU, we conclude that instability is due to the small dimension of our model: performance on standard tasks is more stable
as depth increases (Orvieto et al., 2023). To test our theory, we limit ourselves to a linear RNN with either N= 128 or
N= 256 , followed by an MLP with one hidden layer. We grid-search hyperparameters for each model configuration and
report test error for the best-performing models. Usual best-performing stepsizes are 0.003and0.01. In Lotka-V olterra
experiments (Fig. 15) we train for 200epochs, while we train on the Lorentz System (Fig. 16) for 1000 epochs. No weight
decay, dropout or normalizations are applied. Results are discussed in the relative figure captions and nicely validate
our claims .Code for reproducing the experiments will be provided upon acceptance of this paper .
Lorentz System –  seed variabilityTrain LossEpoch
Figure 14. Variability in training loss dynamics when dealing with randomly initialized linear RNN + 1HL-MLP. Plotted are the dynamics
for seeds 1−6. All other hyperparameters are shared across runs.
25

--- PAGE 26 ---
Universality of Linear Recurrences Followed by Non-linear Projections
 not trained /uni039B trained /uni039BTest MSE = 0.04314Test MSE = 0.002055 
Test MSE =  0.0003543Test MSE = 0.003943 Learnedeigenvalues = 0.999rmin = 1.0rmaxInitalization: linearg:N=128Architecture:
 = 0.999rmin = 1.0rmaxInitalization: 1-HL MLP       ()g:D=512N=128Architecture:
LearnedeigenvaluesLearnedeigenvalues
LearnedeigenvaluesLotka-VolterraLotka-Volterra
Figure 15. Performance of one layer linear RNN followed by a linear layer or a 1HL-MLP (shared accross timestamps), when training or
clamping the RNN at initialization. The encoder and the readout from the RNN hidden state are always trained. Results: The model
greatly benefits from learning the recurrent eigenvalues, but works already quite well with linear projections on the hidden state – indicating
the LV system can be approximated (with some noticeable yet small error) by a linear functional, i.e. a linear system (see Thm 7). When
used, the 1HL-MLP has D= 512 hidden neurons, to double the input size of 2N(real + imaginary part). Even though some elements of
our theory suggest rmin≃1for reconstruction guarantees, here we clearly see that successful learning pushes eigenvalues slightly inside
the unit disk. As (Gu et al., 2021; Smith et al., 2022; Orvieto et al., 2023) we found it benefitial to initialize eigenvalues close to the unit
circle.
26

--- PAGE 27 ---
Universality of Linear Recurrences Followed by Non-linear Projections
 not trained /uni039B trained /uni039BTest MSE = 0.1108
Test MSE = 0.002083 Test MSE = 0.001487Learnedeigenvalues = 0.999rmin = 1.0rmaxInitalization: linearg:N=256Architecture:
 = 0.999rmin = 1.0rmaxInitalization: 1-HL MLP       ()g:D=1024N=256Architecture:
LearnedeigenvaluesLearnedeigenvalues
LearnedeigenvaluesLorentzLorentzTest MSE = 0.1108
Figure 16. Results: compared to Fig. 15, here the task is considerably more challenging: no linear functional is able to learn useful
information about the system — the 1HL-MLP is necessary for decent approximation as the system is highly non-linear. Learning
input-output maps with L= 512 is already challenging: a larger (compared to LV) hidden state N= 256 and an MLP width D= 1024
are therefore chosen for this task. The learned parameters lead to a solution which is not perfect: at the edge of the interval the learned
model struggles. This can be improved with bigger models. However, the learned architecture is able to capture most of the system’s
chaotic behavior. Surprisingly, compared to LV , we found that here learning just the 1HL-MLP already gives great results: we think this is
due to the fact that the sequence length is shorter and the hidden dimension is bigger: the model does not need to adjust eigenvalues as
enough variability is provided in the hidden state.
27
