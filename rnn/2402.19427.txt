# 2402.19427.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rnn/2402.19427.pdf
# File size: 951680 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
2024-3-1
Griffin: MixingGatedLinearRecurrenceswith
LocalAttentionforEfficientLanguageModels
SohamDe*1,SamuelL.Smith*1,AnushanFernando*1,AleksandarBotev*1,GeorgeCristian-Muraru*1,
AlbertGu2,RubaHaroun1,LeonardBerrada1,YutianChen1,SrivatsanSrinivasan1,GuillaumeDesjardins1,
ArnaudDoucet1,DavidBudden1,YeeWhyeTeh1,RazvanPascanu1,NandoDeFreitas1andCaglarGulcehre1
*Equalcontributions,1GoogleDeepMind,2WorkdonewhileatGoogleDeepMind
Recurrentneuralnetworks(RNNs)havefastinferenceandscaleefficientlyonlongsequences,buttheyare
difficulttotrainandhardtoscale. WeproposeHawk,anRNNwithgatedlinearrecurrences,andGriffin,
a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported
performanceofMambaondownstreamtasks,whileGriffinmatchestheperformanceofLlama-2despite
beingtrainedonover6timesfewertokens. WealsoshowthatGriffincanextrapolateonsequencessignifi-
cantlylongerthanthoseseenduringtraining. OurmodelsmatchthehardwareefficiencyofTransformers
duringtraining,andduringinferencetheyhavelowerlatencyandsignificantlyhigherthroughput. We
scaleGriffinupto14Bparameters,andexplainhowtoshardourmodelsforefficientdistributedtraining.
1. Introduction
Recurrentneuralnetworks(RNNs)playedacentralroleintheearlydaysofdeeplearningandNLPre-
search(Elman,1990;SiegelmannandSontag,1991;HochreiterandSchmidhuber,1997;Mikolovetal.,
2010;Bahdanauetal.,2014;Sutskeveretal.,2014),andachievedpracticalsuccessinmanyapplications,
includingGoogle’sfirstendtoendmachinetranslationsystem(Wuetal.,2016). Howeverinrecent
years,bothdeeplearningandNLPhavebeendominatedbytheTransformerarchitecture(Vaswani
et al., 2017), which interleaves multi-layer perceptrons (MLPs) and multi-head attention (MHA).
TransformersachievebetterperformancethanRNNsinpracticeandarealsoveryefficientatutilizing
modernhardware(Kaplanetal.,2020). Transformer-basedlargelanguagemodelstrainedonmassive
datasetscollectedfromthewebhaveachievedremarkablesuccess(Brownetal.,2020;Raeetal.,2021;
Hoffmannetal.,2022;Touvronetal.,2023;Achiametal.,2023;GeminiTeamGoogle,2023).
Despite their successes, Transformers are difficult to scale efficiently to long sequences due to the
quadraticcomplexityofglobalattention. Additionally,thelineargrowthoftheKey-Value(KV)cache
withthesequencelengthmakesTransformersslowduringinference. AlthoughMulti-QueryAttention
(MQA)(Shazeer,2019)partiallymitigatesthisissuebyreducingthecachesizebyaconstantfactor,
the cache still grows linearly in sequence length. Recurrent language models present a compelling
alternative as they compress the entire sequence into a fixed-sized hidden state which is updated
iteratively. HowevertoreplaceTransformers,newRNNmodelsmustdemonstratenotonlycomparable
performanceatscalebutalsoachievesimilarhardwareefficiency(Guetal.,2021a;Mehtaetal.,2022;
Smithetal.,2022;Orvietoetal.,2023b;Daoetal.,2022b;Polietal.,2023;GuandDao,2023).
Inthiswork,weproposetheRG-LRUlayer,anovelgatedlinearrecurrentlayer,aroundwhichwedesign
anewrecurrentblocktoreplaceMQA.Webuildtwonewmodelsusingthisrecurrentblock: Hawk,
amodelwhichinterleavesMLPswithrecurrentblocks,and Griffin,ahybridmodelwhichinterleaves
MLPswithamixtureofrecurrentblocksandlocalattention(Beltagyetal.,2020). Weshowthat:
1.HawkandGriffinexhibitpowerlawscalingbetweenheld-outlossandtrainingFLOPs,uptoandbe-
yond7Bparameters(Figure1(a)),aspreviouslyobservedforTransformers(Kaplanetal.,2020).
2.Griffinachievesslightlylowerheld-outlossthanstrongTransformerbaselinesatallmodelscales.
Corresponding author(s): {sohamde, slsmith}@google.comarXiv:2402.19427v1  [cs.LG]  29 Feb 2024

--- PAGE 2 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
10181019102010211022
Training Flops2.02.53.0Validation Loss
MQA
Hawk
Griffin
(a) Scalingcurveduringtraining
512 1024 2048 4096
Num tokens decoded05000100001500020000Max tokens/s decoded1.0x
1.0x
1.0x
1.0x1.1x 1.8x 3.3x 6.6x2.5x 4.1x 7.3x 14.8x
MQA
Griffin
Hawk (b) Maximumthroughputat1Bparameterscale.
Figure 1|a) Hawk, Griffin and our MQA Transformer baseline all show power law scaling between
held-outlossandtrainingFLOPs,withGriffinachievingthelowestheld-outlossatallFLOPsbudgets.
ThelargestGriffinmodelshownhas14Bparameters. b)HawkandGriffinachievesignificantlyhigher
throughputthanourMQATransformer,especiallywhenthelengthofthesampleincreases.
3.WeovertrainHawkandGriffinon300Btokensatarangeofmodelscales. Hawk-3Bexceedsthe
reportedperformanceofMamba-3B(GuandDao,2023)ondownstreamtasks,despitebeing
trainedonhalfasmanytokens. Griffin-7BandGriffin-14BmatchtheperformanceofLlama-2
(Touvronetal.,2023)despitebeingtrainedonroughly7timesfewertokens(Section3.2).
4.BothHawkandGriffinachievecomparabletrainingefficiencytoTransformersonTPU-v3. Since
diagonal RNN layers are memory bound, we achieve this with a kernel for the RG-LRU layer,
implementedinPallas(Bradburyetal.,2018),thatminimizesmemorytransfers(Section4).
5.Duringinference,bothHawkandGriffinachievesignificantlyhigherthroughputthanMQATrans-
formers(Figure1(b)),andtheyachievelowerlatencywhensamplinglongsequences(Section5).
6.GriffinperformsbetterthanTransformerswhenevaluatedonsequenceslongerthanthoseseen
during training, and can also efficiently learn copying and retrieval tasks from training data
(Section6). However,HawkandGriffinperformlesswellthanTransformerswhenweevaluate
pre-trainedmodelsoncopyingandexact-retrievaltaskswithoutfine-tuning.
2. ModelArchitecture
All our models contain the following components: (i) a residual block , (ii) an MLP block , and (iii) a
temporal-mixing block . While(i)and(ii)arethesameacrossallmodels,weconsiderthreetemporal
mixing blocks: global Multi-Query Attention (MQA), local (sliding-window) MQA and our proposed
recurrent block . AspartoftherecurrentblockweusetheReal-GatedLinearRecurrentUnit(RG-LRU)
–anovelrecurrentlayerinspiredbytheLinearRecurrentUnit(Orvietoetal.,2023b).
Theresidualblock,asshowninFigure2(a),definestheglobalstructureofourmodelsandisinspired
bypre-normTransformers(Xiongetal.,2020). Afterembeddingtheinputsequencewepassitthrough
𝑁such blocks ( 𝑁denoting the model depth), and then we apply RMSNorm (Zhang and Sennrich,
2019)toproducethefinalactivations. Tocomputethetokenprobabilitiesweapplyafinallinearlayer
followedbyasoftmax. Theweightsofthislayeraresharedwiththeinputembeddinglayer.
2.1. Residualblock
Theresidualblockcontainstwocomponents,appliedinorder. Thefirstcomponenttakesthehidden
state𝑥andappliesanRMSNorm(ZhangandSennrich,2019),followedbythetemporal-mixingblock.
2

--- PAGE 3 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
Repeat 
 N times 
(a) Residual block RMSNorm 
RMSNorm Temporal 
mixing block MLP block 
Linear GeLULinear 
Linear 
Linear GeLULinear 
Linear Temporal 
Conv1D RG-LRU (b) Gated MLP block
(c) Recurrent block(e.g. MQA or Recurrent block) 
Figure 2|a) The main backbone of our mode architecture is the residual block, which is stacked 𝑁
times. b)ThegatedMLPblockthatweuse. c)Therecurrentblockthatweproposeasanalternative
toMultiQueryAttention(MQA).ItusesourproposedRG-LRUlayer,definedinSection2.4.
We then merge the output with a skip connection from 𝑥through addition. Similarly, the second
component applies RMSNorm, followed by the MLP block and then merges its output with a skip
connectionfromtheinputoftheRMSNorm. ThisblockisillustratedinFigure2(a).
2.2. MLPblock
We use a gated MLP block (Dauphin et al., 2017) (illustrated in Figure 2(b)), which creates two
branchesfromitsinputofdimension 𝐷. Weapplyalinearlayerwithoutputdimension 𝑀𝐷oneach
branch,where 𝑀denotestheexpansionfactor. Forsimplicity,weuse 𝑀=3throughoutthiswork. We
applyaGeLUnon-linearity(HendrycksandGimpel,2016)ononeofthebranchesbeforemergingthem
byelement-wisemultiplication,similartoGeGeLU(Shazeer,2020). However,inourMLPblock,we
applyafinallinearlayerwithoutputdimension 𝐷ontheoutputsoftheGeGeLUlayer.
2.3. Temporal-mixingblocks
The temporal-mixingblock isthe componentofourmodelthat aggregateshidden layer activations
atdifferenttemporallocationsinthesequence. Weconsiderthreetemporal-mixingblocks: globalMQA
(Shazeer,2019),localMQA(Beltagyetal.,2020)andourproposed Recurrent block .
Globalmulti-queryattention Unlessotherwisestated,weuseMQAratherthanMHAtoimprove
theinferencespeedsofourTransformerbaselines(Shazeer,2019). Weuseafixedheaddimension
𝐷ℎ𝑒𝑎𝑑=128,andwefixthenumberofattentionheads 𝐻suchthat𝐻𝐷ℎ𝑒𝑎𝑑=𝐷. Thisrequiresthemodel
dimension 𝐷tobeamultipleof128. Wedonotuseanyabsolutepositionalembeddings,butweuse
RotaryPositionEmbedding(RoPE)(Suetal.,2021)asarelativepositionalembedding.
Localslidingwindowattention Oneofthekeydisadvantagesofusingglobalattentionisthatits
computationalcomplexitygrowsquadraticallyinthesequencelength. Toaddressthis,severalworks
have started to adopt local attention (Beltagy et al., 2020), also known as sliding window attention.
It allows each position to attend only to a fixed number of tokens in the past. This not only reduces
thecomputationalFLOPsbutalsoboundsthesizeoftheKVcachetothesizeofwindow,makingitno
longerquadraticinthesequencelength. AllotherdetailsarethesameastheglobalMQA.
3

--- PAGE 4 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
Recurrentblock Ourrecurrentblock(Figure2(c))issimilartotheGSSblock(Mehtaetal.,2022)
andtheblockusedbyMamba(GuandDao,2023). Wetaketheinputofdimension 𝐷andapplytwo
linearlayerswithoutputdimension 𝐷𝑅𝑁𝑁inparallel,creatingtwobranches. Onthefirstbranch,we
apply a small separable Conv1D layer, inspired by the Shift-SSM in H3 (Dao et al., 2022b), with a
temporalfilterdimensionof4. NotethatthisConv1Dlayerisverysmall,withjust 4𝐷𝑅𝑁𝑁parameters.
WefollowtheConv1DlayerwithourproposedRG-LRUlayer(definedbelow.) Onthesecondbranch
weapplyaGeLUnonlinearityandthenmergethebranchesbyelement-wisemultiplication. Wethen
applyafinallinearlayerwithoutputdimension 𝐷.
2.4. Real-GatedLinearRecurrentUnit(RG-LRU)
Our proposed RG-LRU layer has a simple recurrence inspired by the Linear Recurrent Unit (LRU)
(Orvietoetal.,2023b),butincorporatesagatingmechanismmotivatedbytheliteratureonnon-linear
RNNs,inparticularLSTMs(HochreiterandSchmidhuber,1997)andGRUs(Chungetal.,2014). The
equationsdescribingthelayerareasfollows:
𝑟𝑡=𝜎(𝑊𝑎𝑥𝑡+𝑏𝑎),recurrence gate (1)
𝑖𝑡=𝜎(𝑊𝑥𝑥𝑡+𝑏𝑥),input gate (2)
𝑎𝑡=𝑎𝑐𝑟𝑡, (3)
ℎ𝑡=𝑎𝑡⊙ℎ𝑡−1+√︃
1−𝑎2
𝑡⊙(𝑖𝑡⊙𝑥𝑡). (4)
Theoutputofthelayeris 𝑦𝑡=ℎ𝑡,andthenon-linearity 𝜎intheequationsisthe sigmoid function. The
recurrentweight 𝑎inEquation (4)isdiagonal. Hencealloperationsareelement-wise. Weparameterize
𝑎inEquation (3)as𝑎=𝜎(Λ),where Λisalearnableparameter. Thisguaranteesthat 0≤𝑎≤1,ensuring
thattherecurrenceisstable. Thevariable 𝑐isascalar-valuedconstantsetto 8. Fornumericalstability,
inpracticewecompute 𝑎𝑐𝑟𝑡inlog-space(seeAppendixA).Thelayerhasgatesonboththeinput 𝑥and
therecurrentweight 𝑎. However,neithergatedependsontherecurrentstate ℎ𝑡−1,whichensuresthat
thecomputationcanbeexecutedefficientlyondevice. Weinitializeboth 𝑊𝑎and𝑊𝑥usingLeCuninit
(LeCunetal.,2002). Weinitialize Λsuchthat𝑎𝑐isuniformlydistributedbetween 0.9and0.999atthe
startoftraining,similartoOrvietoetal.(2023b). UnlikemanyrecentworksintheSSMliterature,the
RG-LRUdoesnotuseinitializationinspiredbythetheoryoforthogonalpolynomials(Guetal.,2020),
anditalsoisnotdefinedasthediscretizationofanunderlyingcontinuoussystem(Guetal.,2021a).
UnliketheoriginalLRUlayer,wedonotusecomplexalgebraintherecurrence. Whileusingcomplex
recurrences would lead to a more expressive layer (Orvieto et al., 2023a) we found that complex
recurrenceswerenotbeneficialforlanguagemodellinginpractice,asalsoobservedbyGuandDao
(2023).1
Gatebehaviour Theinput gate 𝑖𝑡issimilartotheoneinLSTM,whichcanfilter(orscaledown)the
input𝑥𝑡. However,toourknowledge,ourrecurrencegate 𝑟𝑡isdifferentfromothergatingmechanisms
in the literature. For example, the selection mechanism proposed in Mamba (Gu and Dao, 2023) is
comparabletothe update gate ofGRUswhichinterpolatesbetweenthepreviousstateandandthecurrent
observation 𝑥𝑡. Itseffectonthehiddenstateallowsitto reset its state and forget any information it holds
from the past ,similartotheforgetgateintheLSTM.Incontrast,ourrecurrencegatecanapproximately
interpolate between the standard LRU update from Orvieto et al. (2023a) and the previous hidden
state,whichallowsittoeffectively discard the input and preserve all information from the previous history
(seeAppendixAforfurtherdetails). Webelievethekeyroleofthisgateistoenablethemodeltoachieve
super-exponentialmemorybyreducingtheinfluenceofuninformativeinputs.
1We suggest ablating the use of complex numbers for other modalities and provide more information about the
complex-valuedversionoftheRG-LRUlayerinAppendixB.
4

--- PAGE 5 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
3. RecurrentModelsScaleasEfficientlyasTransformers
Scalingstudiesprovideimportantinsightsintohowtotunethehyperparametersofamodelandits
behaviouratscale. Here, wedefinethemodelsevaluatedinourstudies, andprovidescalingcurves
up to and beyond 7Bparameters. Finally, we assess the performance of our models on downstream
tasks. Weconsider3modelfamiliesinthiswork;(1)aMQA-Transformerbaseline,(2)Hawk;ourpure
RNNmodel,and(3)Griffin;ourhybridmodelwhichmixesrecurrentblockswithlocalattention. We
definethekeymodelhyper-parametersformodelsacrossarangeofscalesinAppendixC.
MQATransformerbaseline OurTransformerbaselineusestheresidualpatternandthegatedMLP
blockdescribedinSection2,incombinationwithMQA(Shazeer,2019)andRoPE(Suetal.,2021).
Hawk The Hawk architecture uses the same residual pattern and MLP block as our Transformer
baseline,butweusetherecurrentblockintroducedinSection2.3withaRG-LRUlayer(seeSection2.4)
asourtemporalmixingblock,insteadofMQA.Weexpandthewidthoftherecurrentblockbyafactor
ofapproximately4
3(i.e.𝐷𝑅𝑁𝑁≈4𝐷/3)inordertoroughlymatchtheparametercountofaMHAblock
whenbothusethesamemodeldimension 𝐷.2SeeAppendixCforprecisehyper-parameters.
Griffin Thekeyadvantageofrecurrentblocksoverglobalattentionisthattheyuseafixedstatesize
tosummarizethesequence,whereasthesizeofMQA’sKVcachegrowsproportionaltosequencelength.
Sincelocalattention(Section2.3)hasthesameproperty,mixingrecurrentblockswithlocalattentionpre-
servesthisbenefit. Wehavefoundthiscombinationextremelyeffective,sincelocalattentionaccurately
modelstherecentpast,whiletherecurrentlayerscantransmitinformationacrosslongsequences.
Griffin uses the same residual pattern and MLP block as our Transformer baseline. However unlike
bothourMQATransformerbaselineandtheHawkmodel,Griffinusesamixtureofrecurrentblocks
andMQAblocks. Specifically,weemployalayeredstructurebyalternatingtworesidualblockswith
arecurrentblockfollowedbyoneresidualblockwhichusesthelocal(MQA)attentionblockdescribed
inSection2.3. Unlessotherwisestated,thelocalattentionwindowsizeisfixedto1024tokens.
3.1. Scalingcurves
WepresentourmainscalingresultsinFigure1(a). Allthreemodelfamiliesaretrainedatarangeof
modelscalesfrom100Mto7Bparameters,withanadditionalGriffinmodelwith14billionparameters.
We increase the number of training tokens to be roughly proportional to the number of parameters
ofthemodel,asprescribedbytheChinchillascalinglaws(Hoffmannetal.,2022). Modelsaretrained
ontheMassiveTextdataset(Hoffmannetal.,2022),previouslyusedtotrainGopher(Raeetal.,2021)
andChinchilla(Hoffmannetal.,2022),althoughweuseaslightlydifferentdatasubsetdistribution.
A sequence length of 2048 tokens was used (see Section 6 for results with longer sequences.) All
experiments use the AdamW optimizer (Loshchilov and Hutter, 2017). We tune the learning rate,
weightdecayand 𝛽2parametersforsmallmodels,andusetheserunstoidentifyscalingrulesforthese
hyper-parameterswhichpredicttheiroptimalvaluesforthe7Band14Bmodels.
All three model families demonstrate a linear scaling relationship between the validation loss and
trainingFLOPs(seeFigure1(a);notebothaxesareinlogscale),aspreviouslyobservedforTransformers
byBrownetal.(2020). Notably,GriffinachieveslowervalidationlossthantheTransformerbaseline
acrossallFLOPsbudgetsdespitenotusinganyglobalattentionlayers. Hawkontheotherhandachieves
slightlyhighervalidationloss,butthisgapappearstocloseasthetrainingbudgetincreases.
2Note that we match parameters with MHA attention block, though our Transformer baseline and Griffin ended up
relyingonMQAattentioninordertoimproveinferenceefficiency. Thismeansthatourrecurrentblockshaveslightlymore
parametersthanthecorrespondingMQAblocks.
5

--- PAGE 6 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
Table 1|Character normalized accuracy. Hawk is competitive with our Transformer baseline, and
exceeds the reported performance of Mamba despite being trained on half as many tokens. Griffin
outperformsourTransformerbaseline,andmatchestheperformanceofLlama-2despitebeingtrained
onroughly7timesfewertokens. WereportunnormalizedaccuracywithpartialscoringforWinoGrande.
Model Model TrainingMMLU HellaSwag PIQA WinoGrande ARC-E ARC-C AverageType Size Tokens
Mamba 3B 600B 26.2 71.0 78.1 65.9 68.2 41.7 58.5
7B 2T 45.3 77.2 78.8 69.2 75.2 45.9 65.3Llama-213B 2T 54.8 80.7 80.5 72.8 77.3 49.4 69.3
MQA 1B 300B 28.9 64.8 75.0 62.0 60.2 35.4 54.4
Transformer 3B 300B 31.7 71.0 77.6 66.1 68.1 39.2 59.0
(Baseline) 6B 300B 38.9 77.0 79.5 70.4 74.1 45.2 64.2
1B 300B 29.7 63.3 76.1 57.2 60.6 34.6 53.6
3B 300B 31.3 71.7 78.8 66.5 68.4 40.2 59.5 Hawk
7B 300B 35.0 77.6 80.0 69.9 74.4 45.9 63.8
Griffin1B 300B 29.5 67.2 77.4 65.2 67.0 36.9 57.2
3B 300B 32.6 73.5 78.1 67.2 71.5 41.4 60.7
7B 300B 39.3 78.6 81.0 72.6 75.4 47.9 65.8
14B 300B 49.5 81.4 81.8 74.1 79.1 50.8 69.5
3.2. Evaluationondownstreamtasks
Inordertocomparetoothermodelsintheliterature,wetrainallourmodelsfor300Btokensbefore
evaluatingondownstreamtasks. ThetwoexternalbaselinesthatwecomparetoareMamba-3B(Guand
Dao,2023),thestrongestsmallrecurrentmodelreportedintheliteraturetodate,andLlama-2(Touvron
etal.,2023), awidely usedopenTransformer model. Both externalbaselineshavebeentrainedon
significantlymorethan300Btokens–Mambahasbeentrainedon600Btokens,twicemore,andLlama-2
hasbeentrainedon2Ttokens,nearlyseventimesmore. WenotehoweverthatbothMambaandLlama-2
weretrainedondifferentdatasetsandwithdifferenthyper-parametertuningstrategies,whichmay
partiallyexplainourstrongperformance. WethereforealsoincludeourownMQAtransformerbaseline,
trainedonthesamedataandwiththesamehyper-parametertuningbudgetasHawkandGriffin.
WeprovideanevaluationondownstreamtasksinTable1. WefindthatbothHawkandGriffinachieve
verystrongperformance. Inlinewithotherworks,wereportcharacternormalizedaccuracyonMMLU,
HellaSwag,PIQA,ARC-EandARC-C,whilewereportabsoluteaccuracyonWinoGrandewithpartial
scoring. TheperformanceofHawkimprovessignificantlyasweincreasethemodelsize,andHawk-3B
achievesstrongerperformanceondownstreamtasksthanMamba-3B,despitebeingtrainedonhalfas
manytokens. Griffin-3BsignificantlyoutperformsMamba-3B,andGriffin-7BandGriffin-14Bachieve
performancecompetitivewithLlama-2,despitebeingtrainedonnearly7timesfewertokens. Hawk
isalsocompetitivewithourMQATransformerbaseline,whileGriffinoutperformsthisbaseline.
4. TrainingRecurrentModelsEfficientlyonDevice
Weencounteredtwomainengineeringchallengeswhendevelopingandscalingourmodels. First,how
toefficientlyshardourmodelsacrossmultipledevices. Second,howtoefficientlyimplementlinear
recurrencestomaximizetrainingefficiencyonTPUs. Weaddressbothofthesechallengesinthissection,
beforeprovidinganempiricalcomparisonofthetrainingspeedofGriffinandourMQAbaseline.
6

--- PAGE 7 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
4.1. Modelparallelismforlargescaletraining
Asourmodelincreasesinsize,wecannotfitthemodelonasingledeviceduringtraining,evenwitha
batchsizeof1per-device. Wethereforeusemodelparallelismtoshardourlargemodelsacrossdevices
duringtraining. Sincecommunicationcostsacrossdifferenttrainingdevicesareexpensive,efficiently
shardingthemodeliscriticalforfasttrainingatscale.
MLPandMQAblock Forourgated-MLPblockweuseMegatron-stylesharding(Shoeybietal.,2019),
whichrequiresasingleall-reduceoperationinboththeforwardandthebackwardpass. Similarly,we
applythesamestrategytothelinearlayersintheattentionblock,andadditionallyshardtheattention
mechanismoveritsheads(Narayananetal.,2021).
RecurrentBlock Therecurrentblockcontainstwolinearlayersperbranch. Thisallowsustoapply
Megatronshardingtotheselayersinanequivalentfashion. TheConv1Dlayeroperatesindependently
acrosschannels,enablingustosplititsparametersacrossdeviceswithoutincurringanycommunication
overhead. To avoid additional cross-device communication, we use block-diagonal weights for the
gates in the RG-LRU (see equations 1 and 2), instead of dense matrices. For all experiments in this
paper, we use 16 blocks for both the recurrence gate and the input gate (such that 𝑊𝑥and𝑊𝑎each
have𝐷2
𝑅𝑁𝑁/16parameters). Thediagonalstructureoftherecurrenceoffersthesameadvantageasthe
Conv1D,allowingparametershardingandcomputationwithoutanycommunication. Withthisstrategy,
therecurrentblock’scommunicationrequirementsareequivalenttothoseoftheMLPblock.
Otherconsiderations Optimizerstatescanconsumesignificantmemory,exceedingthesizeofthe
modelparametersthemselves. Toaddressthis,weemployZeROparallelism(Rajbhandarietal.,2020),
distributingbothoptimizerstatesandmodelparametersacrossthebatchshards. Wealsousebfloat16
representationformodelparametersandactivations,minimizinganydatatransferoverhead.
4.2. Efficientlinearrecurrencesondevice
Currentdeeplearningacceleratorsareoptimizedforclassicalarchitectureswhicharecomposedlargely
ofmatrixmultiplicationsandconvolutions. TheseoperationshaveahighFLOPs-to-byteratio,motivating
thedevelopmentofspecializedhardwareunitslikeNvidiaGPUs’TensorCores(Markidisetal.,2018)and
GoogleTPUs’MXUs(Norrieetal.,2021;Jouppietal.,2021,2023). ClassicalRNNsalsobenefitfromthis
duetotheirdenserecurrencematrices. Incontrast,ourproposedRG-LRUlayer,likeotherdiagonalRNN
models,hasalowFLOPs-to-byteratio. Thisfundamentaldifferenceposesacomputationalchallenge,
as existing accelerators lack optimization for such workloads. Since we run all our experiments on
TPU-v3,wefocusondevelopinganefficientimplementationtailoredtothisdevice3.
ChallengesforlineardiagonalRNNs OneofthemainchallengesofutilizingadeviceliketheTPU-v3
fortheRG-LRUisthattheupdateequationofthehiddenstateineq.(4)isapureelementwiseoperation.
Foreachelementupdateitrequirestoload6bytes(assumingbfloat16weneed2bytesforeachofthe
variablesℎ𝑡−1,𝑎𝑡,𝑥𝑡)andwrite2bytes(thehiddenstate ℎ𝑡)whilethecomputationonlyexecutes6FLOPs
(numberofarithmeticoperationsineq.4)perelement. ThistranslatestoalowFLOPs-to-byteratio
of0.75–significantlybelowthedevice’scapacityforelementwiseoperationsof 4.2(seeAppendix3).
ExecutiontimeisthereforedominatedbymemorytransfersbetweenHBMandVMEM,makingthe
computationmemorybound.
Acustomlinearscan ToaddressthiswehavewrittenacustomPallaskernelforthecomputation
ofeq.(4)usinga linear scan . Thisallowsustominimizememorytransfers,bykeepingthehiddenstate
3Theconclusionsdrawnheredonotnecessarilyapplytootheraccelerators.
7

--- PAGE 8 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
2k 4k 8k
Sequence length1.00x1.17x1.46x
0.93x 0.96x 0.98xMQA
Griffin
(a) 400m
2k 4k 8k
Sequence length1.00x1.12x1.33x
0.97x 0.99x 1.01x (b) 1B
2k 4k 8k
Sequence length1.00x1.05x1.13x1.05x 1.07x 1.07x (c) 7B
Figure3|TrainingdurationsperstepcomputedrelativetoourMQAbaselineat2Ksequencelengthaswe
varythemodelsizeandsequencelengthforGriffinandMQA.Letusnotethatasweincreasethesequence
lengthwelowerthebatchsizeproportionally,suchthatthetotalnumberoftokensperbatchstaysfixed.
inVMEMallthetime,andalsotoperformmemorytransfersinlargerchunksratherthanoneatatime.
Inpractice,thistranslatestoalmost3xspeedupoverthenativeJaximplementationofthelinearscan.
Additionally,weobserve10-20%lowertrainingtimesperstepofthefullHawkmodel,relativetothe
samemodelusingthenativeJaximplementation(seeAppendixD.2formoredetails.)
Whywedonotuseconvolutionsorassociativescans? Theinitialappealoflinearrecurrencemodels
stemmed from their high parallelizability, enabled by the associativity of their computations. This
permittedefficientexecutionondeviceviaconvolutions(Guetal.,2021b)orprefix-sumalgorithms(the
associativescan)(Smithetal.,2022). However,theRG-LRU’sgatingmechanismon 𝑎𝑡isnotcompatible
withtheconvolutionalview. Althoughwecanstillusetheassociativescaninprinciple,theassociative
scanreducesthenumberofFLOPsrequiredbutdoesnotreducememoryoverheads,whichisourprimary
bottleneckinpractice. EmpiricallyweobservethatonaTPU-v3theassociativescanissignificantlyslower
thatthenativeJaxlinearscan(seeAppendixD.2formoredetails.) Wespeculatethattherandomaccess
natureofthetreerecombinationoftheparallelprefix-sumalgorithmmakesispoorlysuitedfortheTPU
architecture,leadingtoevenslowermemorytransfers–themainbottleneckofthisoperation.
4.3. Trainingspeedonlongersequences
Wecomparethetrainingspeedsacrossdifferentmodelsizesandsequencelengthstoinvestigatethe
computationaladvantagesofourmodelsduringtraining. Foreachmodelsize,wekeepthetotalnumber
oftokensperbatchfixed,meaningthatasweincreasethesequencelength,weproportionallydecrease
thenumberofsequences. InFigure3,weplottherelativeruntimesofourGriffinmodelcomparedtothat
oftheMQAbaselineat2048sequencelength. Atthelowestsequencelength,thetwomodelshavesimilar
trainingtime,butasweincreasethesequencelengththeTransformerbecomesslower,whileGriffin’srun-
timeremainsthesame. Thedropinspeedforthebaselineismorepronouncedatsmallermodelsizesand
decreasesatlargermodelsizes. Thiscanbeexplainedbythefactthatallmodelscontainalargenumberof
linearlayers. Theircomputationscales 𝑂(𝑇𝐷2),whiletheRG-LRUis 𝑂(𝑇𝐷)vs𝑂(𝑇2𝐷)ofglobalattention.
Thismeansthatasweincreasethemodelwidth 𝐷comparedtothesequencelength 𝑇,thelinearlayers
becometheprimarycomputationalbottleneck,minimizingtheefficiencygainsfromtheRNNblock.
Therefore,replacingTransformerswithHawkorGriffinoffersthemostsignificantwall-clocktimeim-
provementwhensequencelengthissufficientlylargerelativetomodelwidthtoensuretheattentioncom-
putationconstitutesamajorportionofthetotalcomputationtime. Wealsonotethatinpractice,ourMQA
8

--- PAGE 9 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
baselinehasslightlyfewerparametersthanGriffinatthesamemodelscale(andperformsfewerFLOPs).
ThisexplainswhyGriffintrainsslightlyslowerthanourMQAbaselineat7Bforshortsequences.
5. InferenceSpeed
InferenceinLLMsiscomposedoftwostages. Inthe“prefill”stage,wereceiveandprocesstheprompt.
Thisstepiseffectivelyperformingaforwardpassofthemodel. Sincethepromptcanbeprocessedin
parallelacrossthesequence,mostmodeloperationsarecomputeboundduringthisstage. Wetherefore
expecttherelativespeedsofTransformersandrecurrentmodelsduringtheprefillstagetobesimilar
totherelativespeedsofthesamemodelsduringtraining,whichwediscussedinSection4.
Prefillisfollowedbya“decode”stage,inwhichwesampletokensauto-regressivelyfromthemodel. Aswe
showbelow,recurrentmodelshavelowerlatencyandhigherthroughputduringthedecodingstage,espe-
ciallyforlongersequencelengthswherethekey-value(KV)cacheusedinattentioncangetlarge.
Therearetwomainmetricstoconsiderwhenevaluatinginferencespeed. Thefirstislatency,which
measuresthetimetakentogenerateaspecifiednumberoftokensatacertainbatchsize. Thesecondis
throughput,whichmeasuresthelargestnumberoftokenspersecondthatcanbegeneratedonasingle
devicewhensamplingaspecifiednumberoftokens. Sincethroughputisgivenbytokenssampledtimes
batchsizedividedbylatency,onecanimprovethroughputeitherbyreducingthelatencyorbyreducing
memoryusagetoenabletheuseoflargerbatchsizesondevice. Latencycanbeusefultoconsiderfor
real-timeapplicationsthatrequireaquickresponsetime. Throughputisalsousefultoconsiderasitcan
tellusthemaximumnumberoftokenswecouldsamplefromaparticularmodelinagiventime. This
propertyisusefulwhenconsideringotherlanguageapplicationssuchasReinforcementLearningfrom
HumanFeedback(RLHF)orscoringlanguagemodeloutputssuchasdoneinAlphaCode(Lietal.,2022)
wherebeingabletooutputalargenumberoftokensinagiventimeisanappealingfeature.
5.1. Asimplemodelofthedecodestep
Allcomponentsoflanguagemodelsarememoryboundduringdecodingaslongasbatchsizeisn’ttoo
big(i.e.𝐵≲128-seeAppendixF.1fordetails)andwewillassumethisfortheremainderofthissection.
ThelargestmemoryoverheadsofTransformerstypicallycomefromtheparametersthemselvesandthe
KVcache. Thereforewecanapproximatethetimerequiredtogenerateasingletokenforeachsequence
inthebatch 𝐵duringdecodingasthetimeneededtoloadthesetwoquantitiesfrommemory:
Time to sample next token ≈param size+batch size×cache size
memory bandwidth. (5)
Here, cache size refers to eitherthe sizeofthe KV cacheat batch size 1(forTransformers), or tothe
sizeoftherecurrentstateatbatchsize1(forRNNs).
Cachesizes Thedifferenceincachesizerelativetomodelparametershasimportantimplications
for sampling efficiency. In recurrent and local attention blocks, parameter loading is the primary
bottleneck,(becausethecachesizeissubstantiallysmaller). Incontrast,globalattention’sKVcache
scales with the sequence length 𝑇and can be comparable to, or even exceed, the size of the model
parameters. Thisintroducesconsiderableoverheadwhenthesequencelength 𝑇islargeenough(as
showninF.4). Consequently,anequallysizedrecurrentmodelcanexhibitsubstantiallylowerlatency
thanaTransformerwhen 𝑇islarge. Notehoweverthatasthemodelsizegrowsthesequencelengthat
whichweseelatencybenefits(wheretheKVcachesizeiscomparabletoparametersize)alsoincreases.
Itisimportanttonotethat,aswellasimprovinglatency,havingasmallrecurrentstatecanalsoincrease
thelargestbatchsizethatfitsinmemoryonasingledevice,leadingtohigherthroughput.
9

--- PAGE 10 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
128 256 512 1024 2048 4096
# Tokens decoded020406080100120140Time taken to sample/s
MQA
Griffin
Hawk
(a) Latencyemptyprefill
128 256 512 1024 2048 4096
# Tokens decoded050100150Time taken to sample/s
MQA
Griffin
Hawk (b) Latency4kprefill
Figure4|Latencyofdifferent1Bparametermodelsforarangeofsequencelengthsfor(a)sampling
fromanemptyprefilland(b)samplingfromaprefillof4ktokens.
5.2. Results
Here, we look at inference results for models of size 1B parameters. For our baseline, we compare
against a MQA Transformer, which is significantly faster during inference than the standard MHA
Transformer often used in the literature. The models that we compare are: i) MQA Transformer , ii)
Hawk,andiii)Griffin. Forcomparingdifferentmodelswereportbothlatencyandthroughput.
Latency Wecomparethelatencyformodelswithabatchsizeof16withanemptyprefillaswellas
aprefillof4096tokensasseeninFigure4. HawkandGriffinachievefastersamplinglatencythanMQA
Transformersforlongsequences. Thisisparticularlynoticeableasthesequencelengthandtheprefill
length(whichaffectthesizeoftheKVcache)areincreased. GriffinachievessimilarlatencytoHawk,
demonstratingtheexcellentcompatibilityoflinearrecurrencesandlocalattention.
Throughput Wecomparethemaximumthroughput(tokens/s)forthesamemodelswhensampling
512,1024,2048and4196tokensfollowinganemptypromptinFigure1(b). WeseethatbothGriffin
andHawkachievesignificantlyhigherthroughputthantheMQATransformerbaseline. Thisispartially
duetorecurrentmodelshavinglowerlatencybutalsoprimarilyoccursbecauseGriffinandHawkcan
fit larger batch sizes than the MQA Transformer on a single device, since their cache size is smaller.
HawkachieveshigherthroughputsthanGriffin,sincethesizeofthelocalattentioncacheeventually
becomescomparabletothesizeoftheparameterswhenthebatchsizeislarge.
6. LongContextModeling
Inthissection,weexploretheeffectivenessofHawkandGriffintouselongercontextstoimprovetheir
nexttokenprediction,andinvestigatetheirextrapolationcapabilitiesduringinference. Additionally,
weexploreourmodels’performanceontasksthatrequirecopyingandretrievalcapabilities,bothfor
modelsthataretrainedonsuchtasks,aswellaswhentestingforthesecapabilitieswithourpre-trained
languagemodels.
6.1. Improvingnexttokenpredictionwithlongercontexts
We investigate the ability of Hawk and Griffin to improve their predictions with longer contexts. In
particular,weevaluateourtrainedmodelsbymeasuringthelossonaheld-outbooksdatasetacross
arangeofsequencelengths. Usingtheselongdocumentsallowsustoevaluatetheabilityofthemodels
10

--- PAGE 11 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
128 256 512 1K 2K 4K 8K 16K 32K
Token position2.42.62.83.03.23.4Mean-so-far NLLGriffin
Hawk
MQA NoPE
MQA RoPE
128 256 512 1K 2K 4K 8K16K 32K 65K131K
Token position2.42.62.83.03.23.4Mean-so-far NLLGriffin-2k
Griffin-8k
Hawk-2k
Hawk-8k
Figure5|Performanceofvarious1Bparametermodelsonaheld-outevaluationsetofbooks. Onthe
left,themodelshavebeentrainedwithsequencelength2048,andontherightwithsequencelengths
of respectively 2048 (2k) and 8192 (8k). Hawk and Griffin are able to extrapolate to significantly
longer sequences than the Transformer baselines, and further improve performance when trained
onlongersequences.
toextrapolate,i.e. theabilitytoaccuratelypredictthenexttokengivencontextsthatarelongerthan
thoseseenduringtraining.
InTransformers,thisabilitytoextrapolateislargelydeterminedbythepositionalencodingusedforthe
attentionlayers(Kazemnejadetal.,2024). Forrecurrentmodels,itisinsteaddictatedbythecapacity
ofthemodeltokeeprefiningtherepresentationstoredintherecurrencestateasthecontextbecomes
longer. From the left plot of Figure 5, we observe that, up to some maximal length, both Hawk and
Griffinimprovenexttokenpredictiongivenlongercontexts,andtheyareoverallabletoextrapolate
tosignificantlylongersequences(atleast4xlonger)thantheyweretrainedon. Inparticular,Griffin
extrapolatesremarkablywellevenwhenusingRoPE(Suetal.,2021)forthelocalattentionlayers.
Theresultssofarevaluatemodelsthathavebeentrainedonsequencesof2048tokens. Inordertoassess
whetherourmodelscanalsoeffectivelylearnfromlongercontexts,wetrain1Bparametermodelson
sequencesof8192(8k)tokensonMassiveText,andcomparethemtomodelstrainedonthesamedataset
butonsequencesoflength2048(2k)tokens. Wekeepthetotalnumberoftrainingtokensthesame
acrossthemodelsbyreducingthebatchsizebyafactorof4forthemodelstrainedonthesequencelength
of8192(whilekeepingthenumberoftrainingstepsfixed). AsillustratedintherightplotofFigure5,we
findthatHawk-8kandGriffin-8kdoachievelowerevaluationlossforsequencesoflength8192orlarger,
comparedtoHawk-2kandGriffin-2k. ThisindicatesthatHawkandGriffinareabletolearntouselonger
contextsduringtraining. Interestingly,whenevaluatingatshortsequencelengths,wefindthatHawk-2k
andGriffin-2kperformslightlybetterthanHawk-8kandGriffin-8k. Thissuggeststhatthetrainingse-
quencelengthshouldbecarefullychosenaccordingtotheintendeddownstreamuseofthemodel.
6.2. Copyandretrievalcapabilities
Recentwork(Jelassietal.,2024)hasshownthatTransformerscanbesignificantlymoreefficientthan
statespacemodels(SSMs),apopularnewfamilyofRNNs,atlearningsynthetictaskssuchascopying
thecontextorretrievingrelevanttokensfromthecontext. Additionally,Jelassietal.(2024)showed
thatpre-trainedTransformerssuchasPythia(Bidermanetal.,2023)aremuchbetteratcopyingand
retrievaltasksatevaluationtimecomparedtopre-trainedSSMmodelssuchasMamba(GuandDao,
2023). Inthissection,weinvestigatetheefficiencyofGriffinandHawkinlearninghowtocopyand
retrievetokensfromthecontext. Additionally,weevaluatepre-trainedHawkandGriffinmodelson
aphonenumberlookuptaskdesignedtotestbothcopyingandretrievalcapabilities.
11

--- PAGE 12 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
0 25K 50K 75K 100K
Training Steps0.000.250.500.751.00AccuracyMQA
Hawk
Griffin
(a) SelectiveCopyingTask
2729211213
Eval Sequence Length0.000.250.500.751.00Accuracy
MQA
Hawk
Griffin
Train Length (b) InductionHeadsTask
0 1000 2000 3000
Eval Sequence Length0.000.250.500.751.00AccuracyMQA
Hawk
Griffin
Train Length (c) PhonebookLookupTask
Figure6|ExploringthecopyingandretrievalcapabilitiesofHawkandGriffinonthreesynthetictasks.
Figures(a)and(b)showtheperformanceof5layerdeepmodelsonaheldoutevalsetwhenexplicitly
trained on these tasks. Figure (c) shows the performance on a phone number lookup task when
evaluatingourpre-trained7BHawkandGriffinmodelsagainstour6BMQATransformerbaseline.
Trainingonsynthetictasks Toinvestigatetheefficiencyoflearninghowtocopyandretrieverelevant
tokensfromthecontext,wetrainontwosynthetictasks: SelectiveCopyingandInductionHeads. Tobe
abletocompareTransformerswithHawkandGriffin,weconsider5-blockdeepnetworkswithmodel
dimension 64, totalling roughly 250K parameters, where Griffin uses a single local attention in the
middleofthenetwork,inthethirdblock.
•Selectivecopyingtask : Inthistask,themodelneedstolearntocopydatatokensfromasequence
whileignoringnoisetokensfromthecontext. SeeAppendixHformoredetailsonthesetupfor
this task. This task is inspired by Gu and Dao (2023), where the authors showed that Mamba
was able to solve this task better than previously proposed SSMs. We use a vocabulary size of
16,andtrainonsequencesoflength1024,containing16datatokens(randomlysampledfrom
thevocabularyandatrandomlocations),withtherestofthetokenssettothenoisetoken. Griffin
usesalocalattentionwindowsizeof512.
•Inductionheads : Inthistask,themodelneedstolearntorecallthetokenimmediatelyfollowing
aspecialtoken. Thisrequiresthemodeltolearnthespecialtoken,andretrievethetokenimmedi-
atelyfollowingitinthecontext. Ifthemodelisabletolearnthetask,itshouldbeabletoextrapolate
tosignificantlylongersequencesthanitwastrainedfor. Weuseavocabularysizeof16andtrain
onsequencesoflength256wherethetokensaresampledrandomly,andwerandomlysample
thelocationofthespecialtokeninthesequence. Griffinusesalocalattentionwindowofsize128.
WeshowourresultsinFigure6. OntheSelectiveCopyingtask,wefindthatall3modelsareableto
solvethetaskperfectly. Whencomparingspeedoflearningonthistask,wefindHawktobesignificantly
slowerthanTransformers,similartotheobservationmadebyJelassietal.(2024),wheretheauthors
showed that Mamba was significantly slower to learn on similar tasks. Interestingly though, Griffin
showsalmostnoslowdown,effectivelymatchingthespeedoflearningofTransformers,despiteusing
onlyasinglelocalattentionlayer.
OntheInductionHeadstask,whileall3modelscansolvethetaskperfectlyuptothetrainingsequence
length,ourTransformerbaselineisnotabletoextrapolatetolongersequencesduringevaluation. While
ourMQAbaselineusesRoPE,GuandDao(2023)hadsimilarobservationforTransformerswitharange
ofpositionalencodings. WefindthatHawkisabletoperfectlyextrapolateonthistasktoevaluation
sequencesseveralordersofmagnitudelongerthanthetrainingsequencelength. Notably,Griffin,with
itslocalattention,alsodemonstratedexceptionalabilitytoextrapolateonthistask.
12

--- PAGE 13 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
Evaluatingpre-trainedmodels Wenowevaluatewhethercopyingandretrievalcapabilitiesnaturally
emergeinourpre-trainedmodels. Weconsiderour7BHawkandGriffinmodelsandour6BMQATrans-
formerbaseline,alltrainedon300BtokensontheMassiveTextdataset. Weconsiderthesamephonebook
lookuptaskintroducedinJelassietal.(2024),whereweprovidetothemodelasyntheticphonebookcon-
tainingnamesandnumbers,andthemodelisaskedtoretrievethecorrectphonenumbergivenaname.
Theprompttothemodelisaphonebookconsistingofrandomlysampledlistofnamesandnumbersofa
certainlength,followedbytworandomlysampledexamplesofthetask,followedbyarandomlysampled
namefromthephonebookforwhichthemodelneedstoretrievethecorrectphonenumber.
FromFigure6(c),weseethatwhileHawkcandoreasonablywellonthetaskforveryshortphonebook
lengths,itfailstomemorizeandretrievethecorrectphonenumberwhenthephonebooklengthgrows,
similartotheobservationmadebyJelassietal.(2024)ontheMambamodel’sperformanceonthistask.
ThisisnotparticularlysurprisingsinceHawkusesasmallfixed-sizestate. OurTransformerbaselinecan
almostperfectlysolvethistaskuptothetrainingsequencelength,butfailstoretrievethecorrectphone
numberforcontextlengthslongerthanthetrainingsequencelength. Interestingly,Griffincanperfectly
solvethistaskuptoacontextlengththatmatchesitslocalattentionwindowsizeof1024,inspiteof
using only a single local attention layer. Once the context length is long enough such that the local
attentionwindowdoesnotcoverthewholephonebook,performancestartstodegrade. Griffinisalso
abletoextrapolatebettertolongersequencelengthscomparedtoTransformers. Whiletheperformance
ofGriffinispromisingfortheabilityofmodelswithfixed-sizestatetosolvecopyingandretrievaltasks,
ourresultssuggestmoreworkisneededtoimprovethesecapabilitiesforsuchmodels.
7. RelatedWorks
TheTransformerarchitecturehasbecomeamorescalablealternativetoRNNs. Transformersachieve
superior scalability through fully parallelized training, contrasting with the inherent limitations of
RNNs. Duetotheirsequentialprocessingstructure,classicalRNNssufferfromslowtrainingspeeds
duringbothforwardandbackwardpropagation(Werbos,1990). Tomitigatethisissue,researchers
have explored alternative RNN-based methods. Notable examples include Quasi-RNNs (Bradbury
etal.,2016),whichcombineconvolutionsandlinearRNNsforgreaterparallelization,andtheuseof
input-basedgatingmechanismstoparallelizelinearRNNtraining(MartinandCundy,2017).
State-spaceModels(SSMs)haverecentlyemergedasapowerfultoolformodelinglonginputsequences.
Theydemonstratedstrongperformanceontasksfromthelong-rangearenabenchmark(Tayetal.,2020),
andaudiogeneration(Goeletal.,2022). SSMssuccessfullyintegrateconceptsfromclassicalstate-space
models(Kalman,1960)withthoseofRNNs. Theirrelianceonlinearrecurrencesallowsforefficient
hiddenstatecomputation,eitherthroughparallelscanoperationsorconvolutions,resultingintraining
speedscomparabletoTransformermodels. TheS4(Guetal.,2021a)modelproposedasophisticated
parameterizationcalled normalpluslow-rank todiagonalizetherecurrencecomputation. TheS4D
parametrizedtheSSMdirectlywithadiagonalstatematrixandshowedthatitperformedjustaswell
whilebeingmuchsimpler(Guetal.,2022). S5alsodiagonalizedtherecurrence,andshowedthatthe
recurrencecanbecomputedusingtheassociativescan(Smithetal.,2022). TheH3model(Daoetal.,
2022b)generalizestherecurrentinterpretationoflinearattention(Katharopoulosetal.,2020). Hyena
(Polietal.,2023)usesasimilararchitecture,butreplacestheS4Dlayerwithaglobalconvolutionkernel
parametrizedbyanMLP.RetNet(Sunetal.,2023)usesasimplerSSMdesignwithagatingmechanism
whichallowsthemtoparallelizethecomputationusingavariantofmulti-headattention. Orvietoetal.
(2023b)systematicallyanalyzedandablatedmultiplemodificationstostandardRNNs. Theirfinding
showedthatthroughbetterparameterizationandinitializationsimplifiedlinearRNNs(theLRU),per-
formjustaswellasotherSSMsvariantsonvariouslong-rangetasks. RWKV(Pengetal.,2023)isarecent
RNN,showntobecompetitiveonlanguagemodelingtasks,basedonanotherlinearattentionapproxima-
tioninspiredbytheattention-freeTransformer(Zhaietal.,2021). ConcurrenttoourworkGuandDao
13

--- PAGE 14 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
(2023)developedanSSMarchitecturecalledMambawithaninputdependantselectionmechanism
andshowedthatitachievesperformancecomparabletoTransformerswithefficientinference. Several
extensionsofMambahavebeenproposed(Wangetal.,2024;Zhuetal.,2024)fordifferentapplications.
Aninput-dependentgatingsimilartoMambawasalsoproposedbyGateloop(Katsch,2023).
Linearattention(Katharopoulosetal.,2020)offersacomputationallyefficientapproximationofthe
self-attentionmechanismbylinearizingtheattention,whichcanbecomputedrecurrentlyasalinear
RNN.Whilethisapproachsignificantlyreducescomputationalcostcomparedtofullattention,itoften
comeswithatrade-offinmodelperformance. FlashAttention(Daoetal.,2022a)improvesthetraining
speed of attention on GPUs by making efficient use of the memory hierarchy. Another approach to
reducingthecomputationalcostofglobalattention,whichisbecomingincreasinglymorepopular,is
usingsparse-localattention(Childetal.,2019)orslidingwindowattention(Jiangetal.,2023).
8. Conclusion
ThisworkintroducesHawk;arecurrentmodelincorporatinganovelgatedlinearrecurrentlayer,the
RG-LRU.WealsointroduceGriffin;ahybridmodelwhichmixestheRG-LRUlayerwithlocalattention.
Thesemodelsdemonstrateexceptionallanguagemodelingperformanceacrossvaryingscales,with
held-outlossexhibitingpower-lawscalingascomputeresourcesincrease. Hawkexceedsthereported
performanceofMambaondownstreamtaskswhentrainedonhalfasmanytokens,whileGriffinslightly
exceeds the performance of Llama-2 when trained on over 6 times fewer tokens. Furthermore, we
empiricallyvalidatetheinference-timeadvantagesofHawkandGriffinandobservereducedlatency
andsignificantlyincreasedthroughputcomparedtoourTransformerbaselines. Lastly,HawkandGriffin
exhibittheabilitytoextrapolateonlongersequencesthantheyhavebeentrainedonandarecapableof
efficientlylearningtocopyandretrievedataoverlonghorizons. Thesefindingsstronglysuggestthatour
proposedmodelsofferapowerfulandefficientalternativetoTransformerswithglobalattention.
Acknowledgements
We thank Adam Paszke, Sharad Vikram, Trevor Gale, Sebastian Borgeaud, George Scrivener, Raia
Hadsell,OriolVinyals,TobyBoyd,ZhifengChen,ChrisDyer,KelvinXu,AndriyMnihfortheirguidance
andadvice. WemakeuseoftheDeepMindJaxecosystem(Bradburyetal.,2018)andespeciallythank
AndyBrockforbuildingtheinternalframeworkweusedfortrainingandevaluatingourmodels.
References
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt,
S.Altman,S.Anadkat,etal. GPT-4technicalreport. arXiv preprint arXiv:2303.08774 ,2023.
D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and
translate. arXiv preprint arXiv:1409.0473 ,2014.
I.Beltagy,M.E.Peters,andA.Cohan. Longformer: Thelong-documenttransformer. arXiv preprint
arXiv:2004.05150 ,2020.
S.Biderman,H.Schoelkopf,Q.G.Anthony,H.Bradley,K.O’Brien,E.Hallahan,M.A.Khan,S.Purohit,
U.S.Prashanth,E.Raff,etal. Pythia: Asuiteforanalyzinglargelanguagemodelsacrosstraining
andscaling. In International Conference on Machine Learning ,pages2397–2430.PMLR,2023.
J. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. arXiv preprint
arXiv:1611.01576 ,2016.
14

--- PAGE 15 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
J.Bradbury,R.Frostig,P.Hawkins,M.J.Johnson,C.Leary,D.Maclaurin,G.Necula,A.Paszke,J.Van-
derPlas,S.Wanderman-Milne,andQ.Zhang. JAX:composabletransformationsofPython+NumPy
programs,2018. URL http://github.com/google/jax .
T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal,A.Neelakantan,P.Shyam,G.Sastry,
A.Askell,etal. Languagemodelsarefew-shotlearners. In Advances in Neural Information Processing
Systems,volume33,pages1877–1901,2020.
R.Child,S.Gray,A.Radford,andI.Sutskever. Generatinglongsequenceswithsparsetransformers.
arXiv preprint arXiv:1904.10509 ,2019.
J.Chung,C.Gulcehre,K.Cho,andY.Bengio. Empiricalevaluationofgatedrecurrentneuralnetworks
onsequencemodeling. arXiv preprint arXiv:1412.3555 ,2014.
T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré. Flashattention: Fast and memory-efficient exact
attentionwithio-awareness. In Advances in Neural Information Processing Systems ,volume35,pages
16344–16359,2022a.
T. Dao, D. Y. Fu, K. K. Saab, A. W. Thomas, A. Rudra, and C. Ré. Hungry hungry hippos: Towards
languagemodelingwithstatespacemodels. arXiv preprint arXiv:2212.14052 ,2022b.
Y.N.Dauphin,A.Fan,M.Auli,andD.Grangier. Languagemodelingwithgatedconvolutionalnetworks.
InInternational Conference on Machine Learning ,pages933–941.PMLR,2017.
J.L.Elman. Findingstructureintime. Cognitive Science ,14(2):179–211,1990.
Gemini Team Google. Gemini: a family of highly capable multimodal models. arXiv preprint
arXiv:2312.11805 ,2023.
K. Goel, A. Gu, C. Donahue, and C. Ré. It’s raw! audio generation with state-space models. In
International Conference on Machine Learning ,pages7616–7633,2022.
A.GuandT.Dao. Mamba: Linear-timesequencemodelingwithselectivestatespaces. arXiv preprint
arXiv:2312.00752 ,2023.
A. Gu, T. Dao, S. Ermon, A. Rudra, and C. Ré. Hippo: Recurrent memory with optimal polynomial
projections.In Advances in Neural Information Processing Systems ,volume33,pages1474–1487,2020.
A. Gu, K. Goel, and C. Ré. Efficiently modeling long sequences with structured state spaces. arXiv
preprint arXiv:2111.00396 ,2021a.
A.Gu,I.Johnson,K.Goel,K.Saab,T.Dao,A.Rudra,andC.Ré. Combiningrecurrent,convolutional,
and continuous-time models with linear state space layers. In Advances in Neural Information
Processing Systems ,volume34,pages572–585,2021b.
A.Gu,A.Gupta,K.Goel,andC.Ré. Ontheparameterizationandinitializationofdiagonalstatespace
models. arXiv preprint arXiv:2206.11893 ,2022.
D.HendrycksandK.Gimpel. Gaussianerrorlinearunits(gelus). arXiv preprint arXiv:1606.08415 ,2016.
S.HochreiterandJ.Schmidhuber. Longshort-termmemory. Neural Computation ,9(8):1735–1780,
1997.
J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.
Hendricks,J.Welbl,A.Clark,etal. Trainingcompute-optimallargelanguagemodels. arXiv preprint
arXiv:2203.15556 ,2022.
15

--- PAGE 16 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
S.Jelassi,D.Brandfonbrener,S.M.Kakade,andE.Malach. Repeatafterme: Transformersarebetter
thanstatespacemodelsatcopying. arXiv preprint arXiv:2402.01032 ,2024.
A.Q.Jiang,A.Sablayrolles,A.Mensch,C.Bamford,D.S.Chaplot,D.d.l.Casas,F.Bressand,G.Lengyel,
G.Lample,L.Saulnier,etal. Mistral7b. arXiv preprint arXiv:2310.06825 ,2023.
N.Jouppi,G.Kurian,S.Li,P.Ma,R.Nagarajan,L.Nai,N.Patil,S.Subramanian,A.Swing,B.Towles,
et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware
supportforembeddings. In Proceedings of the 50th Annual International Symposium on Computer
Architecture ,pages1–14,2023.
N.P.Jouppi,D.H.Yoon,M.Ashcraft,M.Gottscho,T.B.Jablin,G.Kurian,J.Laudon,S.Li,P.Ma,X.Ma,
etal.Tenlessonsfromthreegenerationsshapedgoogle’stpuv4i: Industrialproduct.In 2021 ACM/IEEE
48th Annual International Symposium on Computer Architecture (ISCA) ,pages1–14.IEEE,2021.
R.E.Kalman. Anewapproachtolinearfilteringandpredictionproblems. Journal of Basic Engineering ,
82,1960.
J.Kaplan,S.McCandlish,T.Henighan,T.B.Brown,B.Chess,R.Child,S.Gray,A.Radford,J.Wu,and
D.Amodei. Scalinglawsforneurallanguagemodels. arXiv preprint arXiv:2001.08361 ,2020.
A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are RNNs: Fast autoregressive
transformers with linear attention. In International Conference on Machine Learning , pages
5156–5165.PMLR,2020.
T. Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv preprint
arXiv:2311.01927 ,2023.
A.Kazemnejad,I.Padhi,K.NatesanRamamurthy,P.Das,andS.Reddy.Theimpactofpositionalencoding
onlengthgeneralizationintransformers. Advances in Neural Information Processing Systems ,36,2024.
Y.LeCun,L.Bottou,G.B.Orr,andK.-R.Müller. Efficientbackprop. In Neural Networks: Tricks of the
Trade,pages9–50.Springer,2002.
Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno,
A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):
1092–1097,2022.
I.LoshchilovandF.Hutter. Decoupledweightdecayregularization. arXiv preprint arXiv:1711.05101 ,
2017.
S.Markidis,S.W.DerChien,E.Laure,I.B.Peng,andJ.S.Vetter. Nvidiatensorcoreprogrammability,
performance&precision. In 2018 IEEE international parallel and distributed processing symposium
workshops (IPDPSW) ,pages522–531.IEEE,2018.
E.MartinandC.Cundy. Parallelizinglinearrecurrentneuralnetsoversequencelength. arXiv preprint
arXiv:1709.04057 ,2017.
H.Mehta, A.Gupta, A.Cutkosky, andB.Neyshabur. Longrangelanguagemodelingviagatedstate
spaces. arXiv preprint arXiv:2206.13947 ,2022.
T. Mikolov, M. Karafiát, L. Burget, J. Cernocký, and S. Khudanpur. Recurrent neural network based
languagemodel. In INTERSPEECH 11th Annual Conference of the International Speech Communication
Association ,pages1045–1048,2010.
16

--- PAGE 17 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, D. Vainbrand,
P. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efficient large-scale language model training on
gpuclustersusingmegatron-lm. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis ,pages1–15,2021.
T.Norrie,N.Patil,D.H.Yoon,G.Kurian,S.Li,J.Laudon,C.Young,N.Jouppi,andD.Patterson. The
designprocessforGoogle’strainingchips: TPUv2andTPUv3. IEEE Micro ,41(2):56–63,2021.
A.Orvieto,S.De,C.Gulcehre,R.Pascanu,andS.L.Smith. Ontheuniversalityoflinearrecurrences
followedbynonlinearprojections. arXiv preprint arXiv:2307.11888 ,2023a.
A.Orvieto,S.L.Smith,A.Gu,A.Fernando,C.Gulcehre,R.Pascanu,andS.De. Resurrectingrecurrent
neuralnetworksforlongsequences. arXiv preprint arXiv:2303.06349 ,2023b.
B.Peng,E.Alcaide,Q.Anthony,A.Albalak,S.Arcadinho,H.Cao,X.Cheng,M.Chung,M.Grella,K.K.
GV,etal. Rwkv: ReinventingRNNsforthetransformerera. arXiv preprint arXiv:2305.13048 ,2023.
M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. Ré. Hyena
hierarchy: Towardslargerconvolutionallanguagemodels. arXiv preprint arXiv:2302.10866 ,2023.
J.W.Rae,S.Borgeaud,T.Cai,K.Millican,J.Hoffmann,F.Song,J.Aslanides,S.Henderson,R.Ring,
S.Young,etal. Scalinglanguagemodels: Methods,analysis&insightsfromtrainingGopher. arXiv
preprint arXiv:2112.11446 ,2021.
S.Rajbhandari,J.Rasley,O.Ruwase,andY.He. Zero: Memoryoptimizationstowardtrainingtrillion
parametermodels. In SC20: International Conference for High Performance Computing, Networking,
Storage and Analysis ,pages1–16.IEEE,2020.
N.Shazeer. Fasttransformerdecoding: Onewrite-headisallyouneed. arXiv preprint arXiv:1911.02150 ,
2019.
N.Shazeer. Gluvariantsimprovetransformer. arXiv preprint arXiv:2002.05202 ,2020.
M.Shoeybi,M.Patwary,R.Puri,P.LeGresley,J.Casper,andB.Catanzaro. Megatron-lm: Trainingmulti-
billionparameterlanguagemodelsusingmodelparallelism. arXiv preprint arXiv:1909.08053 ,2019.
H.T.SiegelmannandE.D.Sontag. Turingcomputabilitywithneuralnets. Applied Mathematics Letters ,
4(6):77–80,1991. ISSN0893-9659.
J.T.Smith,A.Warrington,andS.W.Linderman. Simplifiedstatespacelayersforsequencemodeling.
arXiv preprint arXiv:2208.04933 ,2022.
J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu. Roformer: Enhanced transformer with rotary
positionembedding. arXiv preprint arXiv:2104.09864 ,2021.
Y.Sun,L.Dong,S.Huang,S.Ma,Y.Xia,J.Xue,J.Wang,andF.Wei. Retentivenetwork: Asuccessor
totransformerforlargelanguagemodels. arXiv preprint arXiv:2307.08621 ,2023.
I.Sutskever,O.Vinyals,andQ.V.Le. Sequencetosequencelearningwithneuralnetworks. In Advances
in Neural Information Processing Systems ,pages3104–3112,2014.
Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler.
Longrangearena: Abenchmarkforefficienttransformers. arXiv preprint arXiv:2011.04006 ,2020.
17

--- PAGE 18 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E.Hambro,F.Azhar,etal. LLama: Openandefficientfoundationlanguagemodels. arXiv preprint
arXiv:2302.13971 ,2023.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attentionisallyouneed. In Advances in Neural Information Processing Systems ,volume30,2017.
J. Wang, T. Gangavarapu, J. N. Yan, and A. M. Rush. Mambabyte: Token-free selective state space
model. arXiv preprint arXiv:2401.13660 ,2024.
P.J.Werbos. Backpropagationthroughtime: whatitdoesandhowtodoit. Proceedings of the IEEE ,
78(10):1550–1560,1990.
Y.Wu,M.Schuster,Z.Chen,Q.V.Le,M.Norouzi,W.Macherey,M.Krikun,Y.Cao,Q.Gao,K.Macherey,
etal. Google’sneuralmachinetranslationsystem: Bridgingthegapbetweenhumanandmachine
translation. arXiv preprint arXiv:1609.08144 ,2016.
R.Xiong,Y.Yang,D.He,K.Zheng,S.Zheng,C.Xing,H.Zhang,Y.Lan,L.Wang,andT.Liu. Onlayer
normalization in the transformer architecture. In International Conference on Machine Learning ,
pages10524–10533.PMLR,2020.
S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention free
transformer. arXiv preprint arXiv:2105.14103 ,2021.
B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information
Processing Systems ,32,2019.
L.Zhu,B.Liao,Q.Zhang,X.Wang,W.Liu,andX.Wang. Visionmamba: Efficientvisualrepresentation
learningwithbidirectionalstatespacemodel. arXiv preprint arXiv:2401.09417 ,2024.
18

--- PAGE 19 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
A. RG-LRURecurrenceGate
In Figure 7, we demonstrate the behavior of different gating mechanisms applied on the recurrent
weight𝑎.
0.0 0.2 0.4 0.6 0.8 1.0
rt0.00.20.40.60.81.0+
a=0.5
RG-LRU
Mamba
GRU
LRU update
0.0 0.2 0.4 0.6 0.8 1.0
rt0.00.20.40.60.81.0+
a=0.9
RG-LRU
Mamba
GRU
LRU update
0.0 0.2 0.4 0.6 0.8 1.0
rt0.00.20.40.60.81.0+
a=0.99
RG-LRU
Mamba
GRU
LRU update
Figure7|Thebehaviourofdifferentgatingmechanismsappliedontherecurrentweight 𝑎(notethat
intheMamba’snotationsthisis −𝐴).
Implementation Weimplementourrecurrencegate,asdefinedinSection2.4,inaslightlydifferent,
butmathematicallyequivalentform,fornumericalstability. Inparticular,wecomputethelogarithm
of𝑎𝑡andthenweexponentiateit,insteadofcomputingasigmoidandthentakingapower:
log𝑎𝑡=log𝑎𝑐𝑟𝑡=log𝜎(Λ)𝑐𝑟𝑡=−𝑐softplus(Λ)⊙𝑟𝑡. (6)
Gatebehaviour Ourgateisquitedifferentthanotherstandardgatesintheliterature. Inparticular,
mostgatingmechanisms,liketheoneusedinMambaandGRU,allowthroughthegatetointerpolate
fully between the hidden state and the new observation. Ours on the other hand is biased towards
retaininginformation,anddoesnotallowtofullydiscardthecontributionof ℎ𝑡−1(thisdepends,however,
on the value of Λ). To demonstrate this, we analyze the relative weight of 𝑥𝑡compare to ℎ𝑡−1in the
output𝑦𝑡. Forageneralrecurrencewedefinethisas:
ℎ𝑡=𝛼(𝑟𝑡)ℎ𝑡−1+𝛽(𝑟𝑡)𝑥𝑡. (7)
Forourmodelwehave 𝛼(𝑟𝑡)=𝑎𝑡=𝑎𝑐𝑟𝑡and𝛽(𝑟𝑡)=√︁
1−𝛼(𝑟𝑡)2. ForastandardGRUstylegatingwehave
𝛼(𝑟𝑡)=1−𝑟𝑡and𝛽(𝑟𝑡)=𝑟𝑡. For Mamba, assuming in their notation 𝐵=1,𝐶=1, then𝛼(𝑟𝑡)=(1−𝑟𝑡)−𝐴
and𝛽(𝑟𝑡)=(1−𝛼)/𝐴. ThebehaviourofthedifferentgatingmechanismsisdepictedinFigure7,where
for clarity we have also included the update value of the LRU (Orvieto et al., 2023b), which has no
gating. Ascanbeseen,theMambagatingisalmostidenticaltotheGRUforvaluesof 𝐴closeto 1,with
minordeviationsatsmallervalues. Ontheotherhand,ourgatingmechanismperformsaverydifferent
non-linearinterpolationbetweenfullydiscardingtheinput 𝑥𝑡andtheupdateoftheLRU.
B. Complex-GatedLinearRecurrentUnit(CG-LRU)
InSection2.4wehavedefinedourrecurrentlayer,howeveritcanbefurtherextendedtousecomplex
numbers. Toachievethiswefirstparameterizeacomplexdiagonalrecurrencevia ˜𝑎=𝜎(Λ)𝑒𝑖𝜃,where
𝜃isalearnableparameter. Inaddition,wesplittheinput 𝑥𝑡alongitschanneldimensions,andinterpret
19

--- PAGE 20 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
itsfirsthalfastherealpartofacomplexvector,andthesecondpartastheimaginarypartofthesame
complexvector:
𝑥𝑡=
𝑥1
𝑡
𝑥2
𝑡
(8)
˜𝑥𝑡=𝑥1
𝑡+𝑖𝑥2
𝑡. (9)
WiththiswerewritetheequationsfortheLRU(seeeq.4)as:
𝑟𝑡=𝜎(𝑊𝑎𝑥𝑡+𝑏𝑎),recurrence gate (10)
𝑖𝑡=𝜎(𝑊𝑥𝑥𝑡+𝑏𝑥),input gate (11)
˜𝑎𝑡=˜𝑎𝑐𝑟𝑡, (12)
˜ℎ𝑡=˜𝑎𝑡⊙˜ℎ𝑡−1+√︃
1−|˜𝑎𝑡|2⊙(𝑖𝑡⊙˜𝑥𝑡). (13)
Wemarkallcomplexvariableswith ˜·forclarity. Notethatthenumberofdimensionsof 𝑟𝑡,𝑖𝑡,˜𝑎𝑡and˜ℎ𝑡
arehalfofthoseoftherealinput 𝑥𝑡. Finally, tocomputetheoutputwestacktherealandimaginary
partofℎ𝑡intoasinglevector 𝑦𝑡:
𝑦𝑡=Real(˜ℎ𝑡)
Imaginary(˜ℎ𝑡)
(14)
C. ModelScaleHyper-Parameters
InTable2,wepresentthehyper-parametersofthemodelsatdifferentscales. Thesehyperparameters
aresharedforallthemodelfamiliesthatweexploredinthispaper.
Table2|Keymodelhyper-parametersconsideredfordifferentmodelsizes. Thesehyperparameters
aresharedacrossdifferentarchitectureswetested.
Model ModelWidth RNNWidth Depth MLPExpansion Attention TrainingTokens
Size (𝑫) ( 𝑫𝑹𝑵𝑵 )(𝑵)Factor( 𝑴) Heads (OptimalScaling)
100M 768 1024 12 3 6 1.9B
200M 1024 1536 12 3 8 3.9B
400M 1536 2048 12 3 12 7.8B
1.3B 2048 2560 24 3 16 25B
3B 3072 4096 24 3 24 60B
7B 4096 5632 32 3 32 132.5B
14B 5120 8192 40 3 40 300B
D. EfficientLinearRecurrencesonDevice
Theinitialstepincomputationaloptimizationliesinidentifyingtheprimaryperformancebottleneck
onthetargethardware. Formostaccelerators,thekeylimitingfactorsarecomputationalthroughput
(FLOPs/s)andmemorybandwidthbetweenthehigh-bandwidthmemory(HBM)andthefastvector
memory(VMEM).WhilefactorslikeHBMcapacityandhost-devicecommunicationarerelevant,tech-
niquessuchasZeROshardingandpipelineddatatransferofferpracticalmitigations. Modernaccelerator
designsoftenprioritizeahighFLOPs-to-byteratiotoaccommodateworkloadswherecomputations
significantlyoutnumbermemorytransfers. WeshowthekeyspecificationoftheTPU-v3pod(twochips
perpod)inTable3,whichweuseforallourexperiments.
20

--- PAGE 21 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
Specification TPU-v3Pod
HBMcapacity 32GB
HBMbandwidth 900GB/s
PeakMXUcompute 123TFLOPs/s(bfloat16)
PeakMXUFLOPs-to-byte-ratio 136
PeakVPUcompute 3.8TFLOPs/s
PeakVPUFLOPs-to-byte-ratio 4.2
Table3|HardwarespecificationsforaTPU-v3pod.
0K2K4K6K8K10K12K14K16K
Sequence length0102030405060Run time (ms)Linear(Pallas)
Linear(Jax)
Associative(bf16)
Associative(f32)
(a) Scanruntimes
400m 1b 7b
Model size0.85x0.81x0.89x1.00x 1.00x 1.00x1.58x
1.27x1.41x (b) Hawkruntimes
Figure 8|a) Runtimes of different implementations of the scan operation on a TPU-v3 at different
sequence lengths. The batch size of the input is fixed at 8 and the dimension of each token is 1024.
b)RelativeruntimesoftheHawkmodelwhenusingdifferentimplementationsofthescanoperation,
inreferencetotheonewiththenativeJaxscanimplementation.
D.1. Matrixmultiplicationcomputation
Atypicalmatrixmultiplicationofa 𝐷×𝐷matrixwitha 𝐷×𝑁matrixhas 2𝑁𝐷2FLOPsand 2(𝐷2+2𝑁𝐷)
bytestotransfer(bothreadandwrite)whichtranslatesto𝑁𝐷
𝐷+𝑁FLOPs/byteratio. When 𝐷>>𝑁and
runningonaTPU-v3thisimpliesthatthedimension 𝑁mustbeatleast136tosaturatethedevice,in
whichcasetheoperationis“computebound”,orotherwisemostofthetimewillbespentonwaiting
formemorytransfers,inwhichcasetheoperationis“memorybound”.
D.2. Scanruntimes
InFigure8(a)wedemonstratethatonaTPU-v3ourPallaskernelachievesnearlyx3speedupcompared
tothenaiveJaximplementation. Inaddition,theassociativescanissignificantlyslower,eveniffullyrun
inbfloat16precision. Figure8(b)demonstratesthatthesegainsalsotranslatetosignificantimprove-
mentsoftheoveralltrainingtimeperstepofthefullHawkmodelevenatthe7bscale. Forcompleteness
wehavealsoaddedtheruntimeoftheassociativescan,whichcanbeupto50%slower.
E. TheLocalAttentionWindowSizeofGriffin
Griffin uses both recurrent blocks as well as local attention layers in its temporal mixing blocks. For
allexperimentspreviouslyshownusingatrainingsequencelengthof2048,weusealocalattention
windowsizeof1024. Wenowinvestigatehowtheperformanceofdifferentwindowsizesforthelocal
attentionlayervarieswiththetrainingsequencelength.
We consider 400M parameter models trained on sequence lengths of 2048, 4096 and 8192 tokens,
21

--- PAGE 22 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
Train Length 2K Train Length 4K Train Length 8K2.502.522.542.562.582.602.622.642.66Validation Loss512
1K
2K
1K1K
2K4K
1K
2K2K
4K8K1K
2K
4KMQA Transformer
Griffin
Figure9|Performanceof400MparameterGriffinandMQATransformermodelsusingdifferentlocal
attentionwindowsizesanddifferenttrainingsequencelengths. Thewindowsizesofthelocalattention
layers are shown above each bar in the plot. We notice that a global attention MQA Transformer is
muchbetterthanlocalattentionvariantsoftheMQATransformer(wherethewindowsizeissmaller
than the training sequence length). Furthermore, we see that using a fixed local attention window
sizeof1024(denoted‘1K’intheplot)fortheGriffinmodeloutperformsallglobalattentionandlocal
attentionMQATransformerbaselinesacrossalltrainingsequencelengths.
wherewekeepthetotalnumberoftrainingtokensfixed. Foreachsequencelength,wetrainGriffin
modelsusingdifferentlocalattentionwindowsizes. Asbaselines,wetrainMQATransformersusing
globalattentionlayers,aswellMQATransformersusinglocalattentionlayerswithdifferentwindow
sizes. TheresultsareshowninFigure9,wherethewindowsizesusedareshownontopofeachbar
(MQATransformerbarswithwindowsizeequaltothetrainingsequencelengtharetheglobalattention
MQATransformerbaseline).
From Figure 9, we see that remarkably, even when using a fixed window size of 1024 for the local
attention layers in Griffin, it outperforms the global attention MQA Transformer baseline across all
sequencelengthstested. However,itisworthnotingthattheperformancegapbetweenGriffinwith
localattentionwindow1024andtheglobalattentionMQATransformerreducesasthesequencelength
grows. Therefore,ifthesequencelengthgrowsfurther,itislikelyimportanttoslowlyalsogrowthe
localattentionwindowsize. Inpractice,thehardwareusedwillalsoheavilydeterminetheoptimallocal
attentionwindowsizeintermsoftrainingandinferencespeed. Finally,wenotethatMQATransformers
purelyusinglocalattention(windowsizeslessthanthetrainingsequencelength)performsignificantly
worsethanbothglobalattentionMQATransformers,aswellasGriffin.
F. InferenceSpeeds
F.1. Estimatingmemory-boundedness
Theinferencespeedoflanguagemodelsatdecodetimeisboundedbymemoryloading. Asdescribed
already in 4.2 the linear RNN is memory bound. In the following we will show this is true for the
othercomponents(whicharelinearlayersandself-attention)inourrecurrentmodelsandTransformer
models.
F.2. Estimatingthememoryboundednessoflinearlayers
AsshowninD.1theouterdimension(usuallyconsistingofbatch 𝐵andsequencelength 𝑇dimensions)
mustbeatleast136inordertobecomputebound. Atdecodetime 𝑇=1andifweassume 𝐵≲128then
anylinearlayerswillbememoryboundatdecodetime.
22

--- PAGE 23 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
F.3. Estimatingthememoryboundednessofself-attention
Inthefollowing,wecalculatetheratioofmemoryaccessestoarithmeticoperationsfortheattention
computationforthe 𝐿-thdecodestep,toshowitisalsomemory-bound.
To simplify the following analysis, we assume that we start from an empty prompt (or equivalently
assumethattheprefillcontains0tokens).
Whensamplingauto-regressivelyfromMHAorMQA,standardpracticeistosavethekeyandvalue
vectorsinaKey-Value(KV)cache. For 𝐿tokensalreadysampled,theKVcachewouldthereforebeof
size2×𝐿×𝐻𝑘×𝑑ℎ𝑒𝑎𝑑foreachsequenceinthebatch, where 𝐻𝑘denotesthenumberofheadsusedfor
thekeysandvalues,and 𝑑ℎ𝑒𝑎𝑑denotesthedimensionofthekeyandvaluevectorsineachhead.
Forsamplingthe 𝐿-thtoken,oncewecalculatethequery,keyandvaluevectorscorrespondingtothe
𝐿-thtoken. Theattentionweightsandtheoutputoftheattentionlayerarethencomputedusingthe 𝐿-th
keyandvaluevectorsintheKVcache. Thisrequires 𝑂(𝐿𝐷)operationsoverallanditrequiresloading
the𝑂(𝐿×𝐻𝑘×𝑑ℎ𝑒𝑎𝑑)sizedKVcachefromHBM,foreachsequenceintheminibatch. ThesizeoftheKV
cache,aswellasthenumberofFLOPs,scaleslinearlywiththebatchsize 𝐵.
ForMHA,thenumberofheadsforthekeyandvalues 𝐻𝑘istypicallyequaltothenumberofheadsused
forthequeries 𝐻. ForMQA,asingleheadisusedforkeysandvalues,i.e., 𝐻𝑘=1. ThereforeforMQA,
thesizeoftheKVcacheisafactorof 𝐻𝑘smaller(i.e.,ofsize 2×𝐿×𝑑ℎ𝑒𝑎𝑑).
def attention_sampling(q, k, v):
""" Auto-regressive sampling via attention.
For MHA, h_k = h. For MQA, h_k = 1.
Args:
q : The q vector for current token of shape [b, h, k]
k : The keys of the current + previous tokens [b, L, h_k, k]
v : the values of the current + previous tokens [b, L, h_k, v]
"""
logits = einsum("bhk,bLk->bhL", q, k) # O(bhLk)
weights = softmax(logits)
output = einsum("bhL,bLv->bhv", weights, v) # O(bhLv)
return output
For a batch size of 𝐵, the memory access to FLOPs ratio for the attention computation goes as
𝑂(𝐵×𝐿×𝐻𝑘×𝑑ℎ𝑒𝑎𝑑
𝐵×𝐿×𝐷). For typical Transformer architectures, 𝐷=𝐻×𝑑ℎ𝑒𝑎𝑑and further 𝐻𝑘=𝐻for MHA
and𝐻𝑘=1forMQA.Thereforethememoryaccesstoflopsratiois 𝑂(1)forMHAand 𝑂(1/𝐻)forMQA.
Asexplainedin3,inordertobecomputeboundonTPU-v3aFLOPs-to-byteratioof136isrequired,
andthereforebothMHAandMQAwouldtypicallybememorybound. Nevertheless,MQAsignificantly
speedsupTransformerinference(whencomparedtoMHA),sinceitlowersthememoryboundedness
byafactorof 𝐻.
F.4. Cachesizes
InthefollowingwedoananalysisoftherelativesizesofcachesusedinourrecurrentandTransformers.
Allcachessizesscalelinearlywithbatchsizeandinthefollowingweassume 𝐵=1.
F.4.1. The size of the KV cache
Forattention,theKVcachehassize 2𝑁𝑇ℎ𝑘𝑑ℎ𝑒𝑎𝑑,where𝑁denotesthenumberofattentionlayers(the
depth),𝑇denotes the length of the sequence, ℎ𝑘denotes the number of KV heads and 𝑑ℎ𝑒𝑎𝑑denotes
theheaddimension. Throughoutthiswork, 𝑑ℎ𝑒𝑎𝑑=128. ForMHA, ℎ𝑘𝑑ℎ𝑒𝑎𝑑=𝐷,whileforMQA, ℎ𝑘=1.
(WethereforeexpectMQAtobefasterwhendecodinglongsequencesthanMHA,sincethesizeofthe
KVcacheissignificantlysmallerandlessmemoryneedstobemoved.)
23

--- PAGE 24 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
ForeitherMHAorMQAthesizeoftheKVcachecanexceedthenumberofmodelparameterswhen
thesequencelength 𝑇islarge. Wethereforeexpecttoobserveatransitionfroma‘parameterbound’
regimewhenthesequencelengthisshort,duringwhichthedecodingspeedisdominatedbythetime
takentoloadthemodelparametersondevice,toa‘cachebound’regimeforlargesequences,where
thedecodingspeedisdominatedbythetimetakentoloadtheKVcache.
F.4.2. The size of the recurrent state
TherecurrentstateofasingleRG-LRUlayerhassize 𝐷𝑅𝑁𝑁,andthetotalstatesizefortheentireHawk
model is𝑁𝐷𝑅𝑁𝑁≈4𝐵𝑁𝐷/3. Unlike the KV cache, this state does not grow with sequence length and
isverysmallincomparisontoparametersize. WethereforeexpectthedecodingspeedofRG-LRUto
bedominatedbythetimetakentoloadthemodelparametersondeviceatallsequencelengths.
Asimilarconsiderationappliestothesizeofthe1Dconvolutionstatesize. Foratemporalfilterwidth
ofsize4,thestatehassize (4−1)𝐷𝑅𝑁𝑁=3𝐷𝑅𝑁𝑁=4𝐷foreachrecurrentblockwhichisalsosubstantially
smallerthanparametersizes.
F.4.3. The local attention cache
A single local MQA layer has cache size upper bounded by 2𝑇𝑊𝑆𝑑ℎ𝑒𝑎𝑑, where𝑇𝑊𝑆denotes the local
attentionwindowsize. Solongas 𝑇𝑊𝑆≲𝐷2/(𝐵𝑑ℎ𝑒𝑎𝑑),thesizeofthelocalattentioncacheisalsosmall
relativetotheparametercount. WethereforeexpectthedecodingspeedofGriffintobesimilartothe
decodingspeedoftheHawkmodel.
G.ImprovingNextTokenPredictionwithLongerContexts:AdditionalResults
Figure 10 shows an additional result demonstrating next token prediction performance at different
context lengths on a held out dataset of arXiv articles. We find that the results on this dataset are
qualitativelysimilartotheresultsshowninFigure5.
128 256 512 1K 2K 4K 8K 16K 32K
Token position1.501.752.002.252.502.753.003.253.50Mean-so-far NLLGriffin
Hawk
MQA NoPE
MQA RoPE
128 256 512 1K 2K 4K 8K16K 32K 65K131K
Token position1.501.752.002.252.502.753.003.253.50Mean-so-far NLLGriffin-2k
Griffin-8k
Hawk-2k
Hawk-8k
Figure10|Theevaluationperformanceof1Bparametermodelsacrossarangeofsequencelengths
on held-out evaluation sets of ArXiv articles. On the left, we compare the performance of different
modelstrainedwithsequencelength2048,evaluatedwithasequencelengthofupto32,768. Onthe
right,wecompareGriffinandHawkwhentrainedrespectivelyon2048(2k)and8192(8k)sequence
lengths. ResultsarequalitativelysimilartotheevaluationonBookspresentedinFigure5.
H. AdditionalDetailsoftheCopyandRetrievalTasks
Figure11isanillustrationoftheSelectiveCopyingandInductionHeadstasks.
IntheSelectiveCopyingtask,themodelneedstolearntocopydatatokens(colouredtokensinFigure
11)fromasequencewhileignoringnoisetokens(whitetokensinFigure11). Crossedouttokensin
theoutputinFigure6denotetokensthataremaskedoutintheloss.
24

--- PAGE 25 ---
Griffin: MixingGatedLinearRecurrenceswithLocalAttentionforEfficientLanguageModels
Model
(a) SelectiveCopyingTask
Model (b) InductionHeadsTaskTask
Figure11|AnillustrationoftheSelectiveCopying(left)andtheInductionHeadstasks(right).
IntheInductionHeadstask,themodelneedstolearntorecallthetokenimmediatelyfollowingaspecial
token(blacktokeninFigure11). Asbefore,crossedouttokensintheoutputdenotetokensthatare
maskedoutintheloss.
25
