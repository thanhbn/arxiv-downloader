# 2402.01521.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reasoning/2402.01521.pdf
# Kích thước tệp: 2125275 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Lý luận Cấp độ K: Thiết lập Niềm tin Bậc cao hơn
trong Mô hình Ngôn ngữ Lớn cho Lý luận Chiến lược
Yadong Zhang1,2,*, Shaoguang Mao2,†, Tao Ge2, Xun Wang2,
Yan Xia2,Man Lan1,Furu Wei2,
1Đại học Sư phạm Đông Trung Quốc,2Microsoft Research Asia

Tóm tắt
Lý luận chiến lược là một khả năng phức tạp nhưng thiết yếu
cho các tác nhân thông minh. Nó đòi hỏi
các tác nhân Mô hình Ngôn ngữ Lớn (LLM) phải thích ứng
chiến lược của chúng một cách linh hoạt trong môi trường đa tác nhân.
Khác với các nhiệm vụ lý luận tĩnh, thành
công trong những bối cảnh này phụ thuộc vào việc dự đoán
niềm tin và hành động của các tác nhân khác trong khi liên
tục điều chỉnh chiến lược để đạt được mục
tiêu cá nhân. LLM và các tác nhân LLM thường gặp
khó khăn với lý luận chiến lược do thiếu
một khung lý luận cho phép chúng
suy luận quan điểm của người khác một cách linh hoạt và
thích ứng với môi trường thay đổi. Lấy cảm hứng từ
khung Level-K1 từ lý thuyết trò chơi và
kinh tế học hành vi, mở rộng lý luận
từ phản ứng đơn giản đến độ sâu chiến
lược có cấu trúc, chúng tôi đề xuất một khung mới: "Lý
luận Cấp độ K với Mô hình Ngôn ngữ Lớn
(K-R)." Khung này sử dụng cơ chế đệ quy
để cho phép LLM đạt được
độ sâu chiến lược khác nhau, cho phép các tác nhân
hình thành niềm tin bậc cao hơn—niềm tin về niềm tin
của người khác. Chúng tôi xác thực khung này thông
qua thử nghiệm nghiêm ngặt trên bốn bộ thử nghiệm: hai vấn
đề lý thuyết trò chơi cổ điển và hai nhiệm vụ trí tuệ
xã hội. Kết quả chứng minh
ưu điểm của K-R trong lý luận chiến lược. Công
trình của chúng tôi trình bày việc triển khai đệ quy đầu tiên
về độ sâu chiến lược trong mô hình ngôn ngữ lớn
(LLM). Nó thiết lập nền tảng cho nghiên cứu tương lai
về lý thuyết tâm trí và lý luận chiến
lược trong LLM.

1 Giới thiệu
Lý luận chiến lược—ra quyết định trong môi
trường đa người tham gia—đặt ra những thách thức độc đáo
cho Mô hình Ngôn ngữ Lớn (LLM) và
các tác nhân LLM(Zhang et al., 2024b). Trong những bối cảnh này,

*Công việc được thực hiện khi thực tập tại Microsoft Research
Asia. † Liên hệ với: shaoguang.mao@microsoft.com
1Theo Khung Level-k, tư duy cấp độ k
bao gồm việc xem xét những gì đối thủ/đối tác có thể làm,
họ nghĩ bạn sẽ làm gì, và họ tin bạn nghĩ
họ sẽ làm gì, và cứ thế tiếp tục.

Tư duy Cấp độ đầu tiên Tư duy Cấp độ hai…
Tư duy Cấp độ ba
……

Hình 1: Khung Level-K: Trong tư duy cấp độ đầu tiên,
các tác nhân phản ứng trực tiếp với môi trường. Trong tư duy
cấp độ hai, các tác nhân xem xét tư duy cấp độ đầu tiên của
người khác. Quá trình này tiếp tục lặp lại, với các tác nhân
hình thành niềm tin bậc cao hơn dựa trên giả định về
suy nghĩ của người khác.

các tác nhân phải phản ứng với hành động của người khác trong khi
thích ứng với môi trường động. Chúng cũng cần
điều chỉnh quyết định của mình với mục tiêu riêng trong
những tương tác này. Lý luận chiến lược là thiết yếu
cho các tác nhân thông minh và được áp dụng rộng rãi trong các
nhiệm vụ thực tế, như đầu tư, xây dựng chiến lược kinh doanh(Zhao et al., 2023), đàm phán(Hua et al.,
2023), và xây dựng chính sách(Li et al., 2024).

Lý luận chiến lược hiệu quả dựa vào việc hiểu
quan điểm của người khác và dự đoán
chiến lược của họ. Mặc dù có một số nỗ lực nghiên cứu về
lý luận chiến lược của LLM, hầu hết các phương pháp dựa vào
gợi ý tĩnh (Fu et al., 2023; Xu et al., 2023b).
Điều này thường bao gồm việc hướng dẫn mô hình tính
đến niềm tin và quyết định của người khác trong
quá trình ra quyết định của chính nó trong gợi ý. Tuy
nhiên, những cách tiếp cận này không đủ để cho phép LLM
hình thành niềm tin bậc cao hơn thực sự—niềm tin về
những gì người khác tin, và thiếu tính linh hoạt cần thiết
cho lý luận chiến lược sâu hơn.

Tư duy cấp độ K (Hình 1) (Nagel, 1995; Cui
et al., 2021), một khái niệm cổ điển trong kinh tế học hành vi
và lý thuyết trò chơi, phân loại lý luận thành
độ sâu tư duy chiến lược khác nhau. Nó không chỉ bao gồm việc dự đoán hành động của người khác mà còn xem xét

--- TRANG 2 ---
Vui lòng chọn một số nguyên từ 1 đến 100. Người chơi 
có số được chọn gần nhất với 0.8 * trung bình của tất cả 
các số được chọn sẽ thắng vòng đó. Hãy bắt đầu vòng 1.
Tôi nghĩ trong 
vòng 1...
…
…
…
…
…
…
…
…
(40+50+34+40+36) /5= 40  (Trung bình)
40 *0.8=32 (Mục tiêu) 

Lượng nước có sẵn hàng ngày chỉ có thể đáp ứng nhu cầu của một 
cư dân. Một cuộc đấu giá được tiến hành, và người trả giá cao nhất 
sẽ có được nguồn cung cấp nước hôm nay.
$55
$100
8 1
$60
$ 100
$50
$ 100
$70
$ 100
$30
$ 100

Trong cuộc đấu thầu hôm nay,        đặt mức giá cao nhất $70, 
bảo đảm nguồn nước cho ngày hôm nay.        +2     và 
đặt lại số ngày khát =0. Các cư dân khác 
có HP bị trừ dựa trên số ngày họ 
đã khát (= -). 

Một ngày mới! Tất cả cư dân nhận được lương hàng ngày $100. 
Hôm nay, lượng nước có sẵn hàng ngày chỉ có thể đáp ứng nhu cầu của một 
cư dân...
Hôm nay tôi sẽ đấu giá 
$40 vì...
8 1
 8 1
 8 1
 8 1
…
……
……
……
…
$90
$200
7 2
$80
$ 200
$100
$ 200
$50
$ 130
$80
$ 200

Hôm qua, 
  ………
7 2
 7 2
 10 1
 7 2
…
……
……
……
…

Đoán 0.8 của Trung bình Trò chơi Đấu giá Sinh tồn40 50 34 40 36

Hãy bắt đầu vòng 2.
Trong vòng 
trước...
…
…
…
…
…
…
…
…
31 36 24 25 21Gần nhất với mục tiêu. 
40-32=8 50-32=18 34-32=2 40-32=8 36-32=4

Tiện ích Riêng
Tiện ích Riêng
5
 0 8

Chúng tôi có 5         , 5       , và 5        . 
Vui lòng đàm phán cách phân phối 
chúng.
7
 2 1

Tôi muốn nhiều        hơn, kết quả là, tôi có 
thể chấp nhận ít       hơn, vì vậy tôi đề xuất...

Tôi đề xuất...

Đạt được thỏa thuận. 
      có 4      , 4        , và 0        , 
trong khi     có 1       , 1       , và 5      . 
Tổng tiện ích:  
     :36,      : 45.          thắng!

Đàm phán

Hình 2: Minh họa ba vấn đề lý luận trong môi trường động, tương tác trong bài báo này. Trái:
Đoán 0.8 của Trung bình; Giữa: Trò chơi Đấu giá Sinh tồn; Phải: Đàm phán.

niềm tin của họ về hành động của một người, và thậm chí các
lớp tư duy đệ quy sâu hơn.

Lấy cảm hứng từ tư duy cấp độ K, chúng tôi đề xuất một
khung lý luận chiến lược mới được gọi là "Lý luận
Cấp độ K với LLM (K-R)." K-R tổ chức lý
luận thành các cấp độ phân cấp và sử dụng cơ chế đệ quy
để tích hợp độ sâu chiến lược khác nhau vào ra quyết định. Cụ thể, nó bao gồm: 1)
đệ quy dự đoán hành động của người khác ở các mức độ sâu chiến lược khác nhau với bối cảnh môi trường
và thông tin công khai lịch sử, và 2) lý
luận hành động tối ưu dựa trên những dự đoán này.
Theo hiểu biết của chúng tôi, đây là cách tiếp cận đầu tiên
triển khai các mức độ sâu chiến lược khác nhau trong LLM bằng cơ chế đệ quy và
cho phép lý luận sâu hơn trong các tác nhân LLM thông
qua một khung thuật toán.

Chúng tôi xác thực khung này thông qua thử nghiệm nghiêm ngặt
trên bốn bộ thử nghiệm: hai vấn đề lý thuyết trò chơi cổ điển và hai nhiệm vụ trí tuệ xã hội. Các
vấn đề lý thuyết trò chơi bao gồm Đoán 0.8 của
Trung bình (Hình 2 trái) và Trò chơi Đấu giá Sinh tồn
(Mao et al., 2023) (Hình 2 giữa). Các
nhiệm vụ trí tuệ xã hội bao gồm Đàm phán (Cao
et al., 2018) (Hình 2 phải) và điểm chuẩn SOTOPIA(Zhou et al., 2024). Những thiết lập này phục vụ như mô hình thu nhỏ của các quá trình ra quyết định phức tạp
liên quan đến lý luận chiến lược. Thông qua
các thí nghiệm rộng rãi, chúng tôi chứng minh rằng khung của chúng tôi vượt trội đáng kể so với các phương pháp lý luận hiện có và linh hoạt đạt được các mức độ sâu chiến lược khác nhau. Ngoài bằng chứng thực nghiệm, chúng tôi cung cấp phân tích lý thuyết làm nổi bật
lợi ích của K-R. Chúng tôi cho thấy rằng, tận dụng
khả năng học trong ngữ cảnh của LLM, K-R có thể mô hình hóa hiệu quả hành vi của đối thủ bằng cách sử dụng
thông tin đối thủ công khai và có sẵn tích lũy.

Hơn nữa, chúng tôi điều chỉnh độ sâu chiến lược của
LLM với những người tham gia là con người (Nagel, 1995;
Bosch-Domenech et al., 2002). Sử dụng con người làm
điểm neo, chúng tôi quan sát thấy rằng K-R nâng cao đáng kể
độ sâu chiến lược của LLM từ 0.25 lên
1.89. Đáng chú ý, khi K=3, độ sâu chiến lược (1.89)
của LLM gần đạt tới độ sâu của những người đọc báo tài chính (1.91). Điều này cho thấy mạnh mẽ rằng
K-R thiết lập niềm tin bậc cao hơn trong LLM
cho lý luận chiến lược.

Các đóng góp của công trình này như sau:
•Chúng tôi giới thiệu K-R, một khung mới mở rộng tư duy cấp độ k cho LLM, cho phép
lý luận chiến lược linh hoạt ở các độ sâu khác nhau thông qua cơ chế đệ quy.
•Chúng tôi tiến hành đánh giá toàn diện, bao gồm
vấn đề lý thuyết trò chơi và trí tuệ xã hội, chứng minh rằng K-R vượt trội đáng kể so với
các phương pháp hiện có về tính linh hoạt và hiệu quả, trên cả mô hình nguồn đóng và
nguồn mở.
•Chúng tôi cung cấp phân tích sâu về K-R, xác
nhận khả năng xây dựng niềm tin bậc cao hơn và nâng cao lý luận chiến lược. Điều này đặt
nền móng cho nghiên cứu tương lai về lý thuyết tâm trí và lý luận chiến lược trong LLM.

2 Lý luận Cấp độ K với Mô hình
Ngôn ngữ Lớn

2.1 Phương pháp luận
Lý luận chiến lược đòi hỏi phải xem xét cả bối cảnh ra quyết định và các hành động có thể có của những
người tham gia khác. Chúng tôi sử dụng trò chơi dạng chuẩn đa vòng đa người tham gia để giới thiệu phương pháp được đề xuất. Trong thiết lập này, quá trình ra quyết định của một tác nhân được hình thức hóa như sau: mỗi tác nhân i chọn một hành động a^t_i từ tập hợp A^t_i tại bước thời gian t. Phần thưởng cho tác nhân i, kết quả từ hồ sơ hành động tập thể A^t = (a^t_1, a^t_2, ..., a^t_N) và môi trường E^t, được ký hiệu là U_i(E^t, A^t).

Ở k = 1, các tác nhân quyết định dựa trên môi trường E^t mà không có dự đoán chiến lược:
a^{t,1}_i = arg max_{a_i∈A^t_i} E[U_i(E^t, a_i)] (1)

Ở tư duy cấp độ cao hơn (k ≥ 2), tác nhân i mô phỏng các tác nhân khác hoạt động ở cấp độ k-1 và điều chỉnh chiến lược của họ tương ứng^2:
a^{t,k}_i = arg max_{a_i∈A^t_i} E[U_i(E^t, a_i, â^{t,k-1}_{-i})] (2)

trong đó â^{t,k-1}_{-i} là các hành động được dự đoán của các tác nhân khác dựa trên lý luận cấp độ k-1 của họ.

Chúng tôi đề xuất một khung lý luận chiến lược mới với cơ chế đệ quy, được gọi là "Lý luận Cấp độ K với Mô hình Ngôn ngữ Lớn (K-R)," bao gồm 1) đệ quy dự đoán các hành động â^{t,k}_{-i} của người khác ở các mức tư duy khác nhau sử dụng bối cảnh môi trường và thông tin công khai lịch sử, theo sau bởi 2) lý luận hành động tối ưu a^{t,k}_i dựa trên dự đoán về hành động của người khác.

Quá trình Lý luận Cấp độ K được hình thức hóa như sau:

1)Dự đoán:
â^{t,m}_j = {
LLM(E^t, H^t_j) nếu m = 1
LLM(E^t, H^t_j, â^{t,m-1}_{-j}) nếu m > 1  (3)

trong đó H^t_j = {(E^1, a^1_j), (E^2, a^2_j), ..., (E^{t-1}, a^{t-1}_j)} đại diện cho dữ liệu lịch sử công khai của tác nhân j, và m biểu thị mức tư duy được chỉ định.

2)Lý luận:
a^{t,k}_i = LLM(E^t, H^t_i, â^{t,k-1}_{-i}) (4)

Thuật toán 1 nêu cách thực hiện K-R. Phương pháp đệ quy này cho phép lý luận chiến lược linh hoạt và sâu hơn dần (1, 2, ..., k, k + 1, ...), từ đó nâng cao niềm tin bậc cao hơn trong các tác nhân LLM.

^2Để đơn giản hóa công thức, chúng tôi giả định rằng tất cả đối thủ đều ở cùng mức tư duy. Trong thực tế, mức tư duy khác nhau có thể được triển khai.

Thuật toán 1 Lý luận Cấp độ K với LLM
Yêu cầu: E^t: Bối cảnh quyết định hiện tại tại thời điểm t;
H^t_i: Thông tin lịch sử đến thời điểm t cho tác nhân i;
K: Độ sâu của lý luận chiến lược;
Đảm bảo: a^{t,K}_i: Hành động cho tác nhân i tại thời điểm t sau lý luận cấp độ K.

1:Hàm K_REASONING(i, k):
2:nếu k == 1 thì
3: trả về LLM(E^t, H^t_i)
4:khác
5: cho mỗi tác nhân j ≠ i làm
6: â^{t,k-1}_j = K_REASONING(j, k-1)
7: kết thúc cho
8: trả về LLM(E^t, H^t_i, {â^{t,k-1}_j|j ≠ i})
9:kết thúc nếu
10:a^{t,K}_i = K_REASONING(i, K)
11:trả về a^{t,K}_i

2.2 Phân tích Lý thuyết
Phần này thảo luận về lợi ích từ K-R từ góc độ lý thuyết. Chúng tôi sử dụng khả năng học trong ngữ cảnh của LLM để mô hình hóa hiệu quả hành vi của đối thủ. Giả sử quá trình ra quyết định của tác nhân j tuân theo một chiến lược ẩn θ*_j. Do đó, việc ra quyết định của tác nhân j có thể được biểu thị như:

P(a^t_j|E^t, θ*_j) (5)

Học trong ngữ cảnh của LLM có thể được định nghĩa chính thức như suy luận Bayes ngầm (Xie et al., 2021); do đó, cho môi trường E^t, dự đoán hành động tiếp theo có điều kiện trên H^t_j là:

P(a^t_j|E^t, H^t_j) = ∫ P(a^t_j|E^t, θ_j)P(θ_j|H^t_j)dθ_j (6)

Khi t → ∞, theo định luật số lớn và tính chất của cập nhật Bayes, phân phối hậu nghiệm tập trung xung quanh tham số thực θ*_j:

P(θ_j|H^t_j) → δ(θ_j - θ*_j) (7)

trong đó δ là hàm delta Dirac. Do đó,

∫ P(a^t_j|E^t, θ_j)P(θ_j|H^t_j)dθ_j → P(a^t_j|E^t, θ*_j) (8)

Điều này có nghĩa là khi số lần tương tác tăng lên, K-R có thể dự đoán chính xác hơn hành vi của đối thủ.

Cũng đáng chú ý rằng dữ liệu tương tác không thể vô hạn, và học trong ngữ cảnh liên quan đến hiệu suất của mô hình ngôn ngữ lớn (LLM). Do đó, chúng tôi xác thực thực nghiệm những giả thuyết và lý luận này trong Phần 5.2.

3 Thí nghiệm: Lý thuyết Trò chơi
Để so sánh công bằng khả năng lý luận chiến lược của LLM, chúng tôi đầu tiên áp dụng hai thiết lập lý thuyết trò chơi được sử dụng rộng rãi. Những vấn đề lý thuyết trò chơi được kiểm soát, được định nghĩa rõ ràng này cung cấp đánh giá mạnh mẽ về hiệu suất của LLM, với các thiết lập chi tiết được nêu trong Phụ lục B.

3.1 Định nghĩa Nhiệm vụ và Chỉ số

3.1.1 Đoán 0.8 của Trung bình (G0.8A)
G0.8A (Hình 2 Trái) là một vấn đề lý thuyết trò chơi cổ điển được giới thiệu bởi Alain Ledoux (Ledoux, 1981). Nó bao gồm trò chơi 10 vòng trong đó mỗi người chơi chọn một số từ 1 đến 100. Mục tiêu là chọn một số gần nhất với 80% lựa chọn trung bình của nhóm. Ý tưởng chính là đoán cách người khác sẽ ước tính trung bình và quyết định số để nộp. Khái niệm này cũng được minh họa trong Cuộc thi Sắc đẹp Keynesian (Keynes, 1936).

Trò chơi này phản ánh thách thức dự đoán hành vi tập thể trong thị trường tài chính, nơi các nhà đầu tư phải dự đoán không chỉ giá trị của một tài sản mà còn cách người khác sẽ định giá nó trong tương lai.

Hiệu suất của tác nhân được đánh giá bằng Tỷ lệ Thắng. Cụ thể, Tỷ lệ Thắng được tính dựa trên số lần thắng đạt được bởi tác nhân trong từng vòng riêng lẻ, thay vì toàn bộ tập phim trò chơi.

3.1.2 Trò chơi Đấu giá Sinh tồn (SAG)
SAG (Hình 2 Giữa) được phái sinh từ Thách thức Phân bổ Nước được đề xuất trong (Mao et al., 2023). Mục tiêu của mỗi cư dân là sống sót qua thời kỳ hạn hán 10 ngày bằng cách đấu thầu tài nguyên nước và duy trì điểm sức khỏe trên không. Nếu một người chơi đấu thầu thành công cho nước, họ được điểm sức khỏe; ngược lại, họ mất điểm sức khỏe. Việc tích hợp hệ thống đấu giá với cơ chế điểm sức khỏe tạo ra một môi trường động trong đó người chơi phải cân bằng sức khỏe và tài chính.

Chúng tôi sử dụng Vòng Sinh tồn Trung bình đo lường vòng trung bình mà một người chơi vẫn hoạt động trong trò chơi.

3.2 Kỹ thuật Cơ bản
Chúng tôi thích ứng nhiều cách tiếp cận khác nhau, ban đầu từ lý luận truyền thống và điểm chuẩn tác nhân. Những kỹ thuật cơ bản này bao gồm:

Gợi ý Tiêu chuẩn (Trực tiếp): Đây là phương pháp gợi ý thông thường trong đó LLM tạo ra câu trả lời cuối cùng (Hành động) để đáp ứng với gợi ý thiết lập trò chơi đã cho.

Chuỗi Suy nghĩ (CoT) (Wei et al., 2022): Chúng tôi sử dụng phương pháp lý luận Chuỗi Suy nghĩ không cần ví dụ (Kojima et al., 2022).

Gợi ý Persona (Persona) (Deshpande et al., 2023): Kỹ thuật này sửa đổi quá trình gợi ý tiêu chuẩn bằng cách kết hợp persona "Chuyên gia Trò chơi" để nâng cao khả năng lý luận của LLM.

Reflexion (Reflect) (Shinn et al., 2023): Phương pháp này đề cập đến các tác nhân ngôn ngữ với học tăng cường bằng lời và đã được thích ứng cho các nhiệm vụ động. Các sửa đổi chi tiết được giải thích trong K.

Self-Refine (Refine) (Madaan et al., 2023): Đây là cách tiếp cận lý luận lặp nhiều vòng trong đó một LLM bổ sung đưa ra nhận xét và điều chỉnh trước khi đạt được quyết định cuối cùng. Sự khác biệt giữa Self-Refine và Reflect được trình bày chi tiết trong Phụ lục I.

Chuỗi Suy nghĩ Dự đoán (PCoT): Đây là đường cơ sở mạnh khác với CoT bằng cách yêu cầu LLM dự đoán rõ ràng hành động của đối thủ trước khi ra quyết định. Khác với Lý luận Cấp độ K, bao gồm cách tiếp cận đệ quy, PCoT tập trung vào dự đoán trực tiếp dựa trên ngữ cảnh.

Để biết chi tiết triển khai và ví dụ cụ thể, vui lòng tham khảo Phụ lục K.

3.3 Thiết lập Thí nghiệm
Chúng tôi thiết lập một môi trường có thể kiểm soát và phân biệt giữa hai vai trò: người chơi (trọng tâm chính) và đối thủ. Người chơi được trang bị một phương pháp cụ thể, trong khi tất cả đối thủ sử dụng cách tiếp cận lý luận khác. Thiết lập được định nghĩa rõ ràng này cho phép so sánh rõ ràng hơn khả năng lý luận giữa các phương pháp.

Trong G0.8A và SAG, có một người chơi và bốn đối thủ cho mỗi trò chơi. Thí nghiệm cho mỗi thiết lập được lặp lại 10 lần và đã vượt qua kiểm tra ý nghĩa (Phụ lục H), và mỗi thí nghiệm bao gồm trò chơi 10 vòng.

Tất cả các phương pháp trong thí nghiệm chính được triển khai bằng GPT-4 (Achiam et al., 2023) (gpt4-32k), với nhiệt độ được đặt ở 0.7 và top-p được đặt ở 0.9. Chúng tôi cũng tiến hành thí nghiệm với LLM nguồn mở, Chi tiết được cung cấp trong Phụ lục E. Trừ khi được chỉ định khác, mức tư duy trong Lý luận Cấp độ K được đặt là K=2.

3.4 Kết quả
Để phân biệt giữa "Người chơi" và "Đối thủ" trong bảng, tiêu đề cho Người chơi (đậm) và Đối thủ (nghiêng) được định dạng tương ứng.

Bảng 1: Tỷ lệ Thắng của người chơi chống lại các đối thủ khác nhau trong trò chơi G0.8A.

Direct CoT Persona Reflect Refine PCoT K-R
Direct 0.43 0.67 0.62 0.53 0.43 0.61 0.82
CoT 0.07 0.32 0.35 0.14 0.22 0.45 0.63
Persona 0.05 0.37 0.29 0.05 0.37 0.11 0.46
Reflect 0.42 0.68 0.63 0.39 0.64 0.74 0.78
Refine 0.10 0.34 0.32 0.31 0.23 0.22 0.46
PCoT 0.03 0.44 0.52 0.21 0.51 0.54 0.85
K-R 0.04 0.15 0.14 0.04 0.17 0.14 0.52

Trung bình 0.16 ± 0.18 0.32 ± 0.19 0.41 ± 0.18 0.24 ± 0.18 0.37 ± 0.17 0.40 ± 0.25 0.65 ± 0.17

Bảng 2: Vòng Sinh tồn Trung bình của người chơi chống lại các đối thủ khác nhau trong Trò chơi Đấu giá Sinh tồn.

Direct CoT Persona Reflect Refine PCoT K-R
Direct 5.90 7.00 7.50 4.70 8.70 6.60 9.40
CoT 5.70 6.50 5.30 4.00 8.10 5.30 10.00
Persona 5.70 7.70 7.40 5.20 6.30 7.20 9.30
Reflect 9.40 9.40 9.90 5.20 8.60 8.20 10.00
Refine 6.30 6.40 8.10 4.30 8.20 5.30 7.90
PCoT 8.50 9.60 9.90 6.30 8.50 6.20 9.70
K-R 4.10 5.50 5.00 4.04 5.70 4.40 6.80

Trung bình 6.51 ± 1.82 7.44 ± 1.55 7.59 ± 1.95 4.82 ± 0.82 7.73 ± 1.21 6.17 ± 1.29 9.01 ± 1.21

Bảng 1 trình bày Tỷ lệ Thắng của người chơi sử dụng các phương pháp khác nhau chống lại các đối thủ khác nhau trong trò chơi G0.8A. Đáng chú ý, phương pháp K-R thể hiện Tỷ lệ Thắng vượt trội là 0.65, vượt xa đáng kể tỷ lệ thắng của các chiến lược khác. Bảng 2 cung cấp thông tin chi tiết về Vòng Sinh tồn Trung bình của người chơi trên các chiến lược trò chơi đấu giá khác nhau trong SAG, với phương pháp K-R một lần nữa nổi bật. Phương pháp K-R đạt được vòng sinh tồn trung bình là 9.01, cao hơn đáng kể so với tất cả các phương pháp khác.

Kết quả thí nghiệm nhấn mạnh hiệu quả của phương pháp K-R trong việc nâng cao chiến lược người chơi, gợi ý về sự vượt trội chiến lược của nó trong bối cảnh trò chơi này. Hiệu quả của nó nằm ở khả năng dự đoán nước đi của đối thủ, vượt trội hơn các phương pháp gợi ý khác.

Hiệu suất của Reflect không chứng minh được hiệu quả của phương pháp lý luận. Chúng tôi giả thuyết rằng điều này là do thực tế là, trong môi trường động, Reflect về kinh nghiệm được tóm tắt từ vòng trước (Shinn et al., 2023) có thể không áp dụng được cho vòng tiếp theo của trò chơi. Hơn nữa, trong cả hai trò chơi, Refine không thể hiện ưu thế so với CoT và thấp hơn đáng kể so với K-R. Điều này là do Refine bao gồm các điều chỉnh dựa trên chiến lược của chính mình. Tuy nhiên, những điều chỉnh này không xem xét rõ ràng các chiến lược ẩn của hành vi đối thủ, khiến chúng không thể áp dụng chống lại đối thủ sử dụng các chiến lược khác nhau.

4 Thí nghiệm: Trí tuệ Xã hội
Sau đó chúng tôi đánh giá K-R trong hai điểm chuẩn trí tuệ xã hội để đánh giá hiệu suất của nó trong các tình huống thực tế mở hơn. So với các thiết lập trừu tượng và lý thuyết của Lý thuyết Trò chơi, những tình huống này bao gồm bối cảnh phong phú hơn và theo đuổi mục tiêu phức tạp, điều này thể hiện tốt hơn giá trị của các tác nhân dựa trên LLM trong các ứng dụng thực tế, chẳng hạn như trong chatbot và ra quyết định chiến lược.

4.1 Định nghĩa Nhiệm vụ và Chỉ số

4.1.1 Đàm phán (NEG)
NEG (Hình 2 Phải)(Cao et al., 2018; Duan et al., 2024) là một nhiệm vụ mở và thực tế. Trong thiết lập này, hai tác nhân được trình bày với ba loại vật phẩm: ớt, cherry và dâu tây. Mỗi tác nhân có giá trị tiện ích riêng cho những vật phẩm này và phải đàm phán để phân bổ nhóm vật phẩm công cộng.

Tác nhân bảo đảm được nhiều tiện ích hơn khi đạt được thỏa thuận sẽ thắng trò chơi, và chúng tôi tính toán Tỷ lệ Thắng để đánh giá hiệu suất của các tác nhân khác nhau.

4.1.2 Điểm chuẩn SOTOPIA
SOTOPIA (Zhou et al., 2024) là một môi trường mở để mô phỏng các tương tác xã hội phức tạp giữa các tác nhân nhân tạo và đánh giá trí tuệ xã hội của chúng. Nó bao gồm nhiều tình huống xã hội khác nhau, và mỗi tình huống bao gồm một bối cảnh nền, và các mục tiêu xã hội riêng của mỗi tác nhân. Đồng thời, mỗi tác nhân có hồ sơ nhân vật bao gồm tên, giới tính, tính cách, nghề nghiệp, v.v.

Cho mỗi tập phim, các tác nhân được ghi điểm vào cuối tương tác theo mỗi trong bảy chiều trong SOTOPIA-Eval, bao gồm Hoàn thành Mục tiêu (GOAL), Độ Tin cậy (BEL), Kiến thức (KNO), Bí mật (SEC), Mối quan hệ (REL), Quy tắc Xã hội (SOC), Lợi ích Tài chính và Vật chất (FIN).

4.2 Thiết lập Thí nghiệm
Chúng tôi sử dụng phần lớn các cách tiếp cận lý luận được giới thiệu trong Phần 3.2 như mô hình đường cơ sở để so sánh.

Trong NEG, các thí nghiệm tuân theo thiết lập từ (Cao et al., 2018; Duan et al., 2024). Có một người chơi và một đối thủ cho mỗi trò chơi. Chúng tôi kiểm tra hiệu suất của Các đường cơ sở và Lý luận Cấp độ K trong 100 trò chơi độc lập lặp lại. Để loại bỏ lợi thế vị trí, chúng tôi hoán đổi vị trí của mỗi người chơi cho mỗi thiết lập. Để đảm bảo độ tin cậy, ba thử nghiệm được tiến hành, và kết quả được báo cáo như trung bình với độ lệch chuẩn.

Đồng thời, Chúng tôi tuân thủ thiết lập SOTOPIA-hard (Zhou et al., 2024) bao gồm tổng cộng 100 tập phim, thường được thấy là thách thức đối với LLM, và sử dụng tác nhân dựa trên GPT-4o cố định như đối tác. Thêm vào đó, để đánh giá điểm số của các tác nhân, chúng tôi sử dụng GPT-4 như mô hình đánh giá, vì nó đã được xác định bởi điểm chuẩn SOTOPIA (Zhou et al., 2024) để phục vụ như một proxy đáng tin cậy cho các phán đoán của con người trong việc đánh giá hiệu suất mô hình trên hầu hết các chiều và cho hiệu suất con người trên chiều GOAL.

4.3 Kết quả

Bảng 3: Tỷ lệ Thắng của người chơi chống lại đối thủ trong Thiết lập Đàm phán.

Direct CoT Persona Reflect Refine PCoT K-R
Direct 50.00 61.34 49.58 66.67 65.83 63.03 70.83
CoT 38.66 50.00 36.67 45.83 45.76 47.27 55.36
Persona 50.42 63.33 50.00 70.00 67.50 62.50 70.83
Reflection 33.33 54.17 30.00 50.00 57.14 55.00 55.00
Refine 34.17 54.24 32.50 42.86 50.00 55.77 54.55
PCoT 36.97 52.73 37.50 45.00 44.23 50.00 57.00
K-R 29.17 44.64 29.17 45.00 45.45 43.00 50.00

Trung bình 38.96 ± 2.53 54.35 ± 0.50 37.92 ± 5.84 52.19 ± 1.73 53.70 ± 4.41 53.80 ± 4.34 59.08 ± 2.20

Kết quả được trình bày trong Bảng 3 và Bảng 4 minh họa hiệu quả của Lý luận Cấp độ K trong bối cảnh thiết lập NEG và SOTOPIA-hard, tương ứng.

Trong NEG, phương pháp K-R thể hiện tỷ lệ thắng đáng chú ý là 59.08%, định vị nó cao hơn đáng kể so với tỷ lệ thắng trung bình đạt được bởi các phương pháp khác. Điều này cho thấy rằng, trong hầu hết các trường hợp, các đề xuất được tạo ra thông qua Lý luận Cấp độ K có lợi hơn cho chính nó, cũng như gợi ý xu hướng chấp nhận đề xuất của đối thủ khi lợi ích được nhận thức là đáng kể.

Bảng 4: SOTOPIA-Eval của người chơi chống lại đối thủ trong SOTOPIA-hard.

Direct CoT Refine K-R Direct CoT Refine K-R
Chỉ số [GPT-4o] [LLaMA-3.1-70B]
BEL[0–10] 8.97 9.00 9.00 8.97 8.88 8.85 8.90 8.97
REL[-5–5] 2.38 2.40 2.27 2.67 1.38 1.18 0.82 2.40
KNO[0–10] 6.05 6.05 6.25 6.25 5.88 5.53 5.33 6.12
SEC[-10-0] 0.00 -0.05 0.00 0.00 -0.28 -0.25 -0.18 0.00
SOC[-10–0] -0.05 0.00 -0.05 0.00 -0.70 -0.72 -0.64 0.00
FIN[-5–5] 0.90 0.78 0.80 0.72 0.38 0.35 -0.08 0.75
GOAL [0–10] 6.35 6.60 6.15 6.47 5.35 5.40 4.95 6.38

Tổng thể 3.51 ± 0.09 3.54 ± 0.08 3.49 ± 0.08 3.59 ± 0.09 2.98 ± 0.23 2.90 ± 0.26 2.73 ± 0.25 3.52 ± 0.13

Kết quả từ SOTOPIA tiết lộ một số phát hiện thú vị. Thứ nhất, trong khi K-R thể hiện một số cải thiện so với các phương pháp khác, kết quả không có ý nghĩa thống kê. Chúng tôi giả thuyết rằng điều này có thể do xu hướng vốn có của mô hình dựa trên GPT-4 gán điểm cao hơn cho các phản hồi được tạo ra bởi các tác nhân dựa trên GPT-4. Đáng chú ý, chúng tôi quan sát thấy rằng việc sử dụng các tác nhân dựa trên LLaMA 3.1 70B với K-R có thể dẫn đến nâng cao hiệu suất đáng kể. Đồng thời, các chỉ số tổng thể cho thấy rằng K-R đạt được mức hiệu suất tương đương với mô hình GPT-4, làm nổi bật tiềm năng của K-R trong lĩnh vực trí tuệ xã hội.

5 Thảo luận

5.1 K-R có Thiết lập hiệu quả Niềm tin Bậc cao hơn trong LLM không?

Bảng 5: Hiệu suất con người trong G2/3A.

Thí nghiệm Phòng thí nghiệm Lớp học Làm ở nhà Các nhà lý thuyết Nhóm tin Internet Báo chí
Lựa chọn Trung bình 35.13 26.84 25.20 17.15 22.16 23.08
Độ sâu Chiến lược 0.87 1.53 1.68 2.63 2.01 1.91

Bảng 6: Hiệu suất LLM trong G0.8A ở vòng đầu tiên.

Phương pháp Direct CoT Persona Refine Reflect PCoT KR[k=2] KR[k=3]
Lựa chọn Trung bình 47.29 37.8 41.0 41.0 45.2 44.0 38.42 32.79
Độ sâu Chiến lược 0.25 1.25 0.89 0.89 0.45 0.57 1.18 1.89

Như một vấn đề lý thuyết trò chơi cổ điển, vấn đề G0.8A đã thu hút sự quan tâm nghiên cứu đáng kể trên các ngành khác nhau. Chúng tôi tham chiếu kết quả thí nghiệm của nghiên cứu cổ điển trong số những người tham gia là con người (Nagel, 1995; Bosch-Domenech et al., 2002) như điểm neo và trình bày các quyết định trung bình được đưa ra bởi phương pháp Lý luận Cấp độ K (GPT-4) trong vòng đầu tiên. Thông qua so sánh này, chúng ta có thể quan sát mối quan hệ tương đối giữa mức độ nhận thức của con người và LLM dưới các phương pháp lý luận khác nhau. Phương pháp tính toán cụ thể về độ sâu chiến lược được mô tả trong Phụ lục C. Hiệu suất của con người và LLM được thể hiện trong Bảng 5 và Bảng 6.

Từ những quan sát này, chúng ta có thể kết luận rằng ngay cả khi sử dụng các mô hình SOTA, độ sâu chiến lược của GPT-4 dưới Gợi ý Trực tiếp (0.25) không thể cạnh tranh với độ sâu của sinh viên đại học có khả năng chiến lược thấp hơn trong thiết lập phòng thí nghiệm (0.87). Hơn nữa, cách tiếp cận lý luận Cấp độ K nâng cao đáng kể độ sâu lý luận của mô hình ngôn ngữ lớn, tăng từ 0.25 lên 1.89, và độ sâu chiến lược của mô hình ngôn ngữ lớn (1.89) tiếp cận với của một nhóm độc giả báo tài chính (1.91) khi K=3.

5.2 Lý luận Cấp độ K Dẫn đến Dự đoán Chính xác hơn về Đối thủ

[Biểu đồ thể hiện độ lệch dự đoán trong G0.8A giữa PCoT và Lý luận Cấp độ K qua các vòng]

Vì K-R bao gồm bước trung gian mô hình hóa hành vi của đối thủ, chúng tôi kiểm tra sự tiến triển của độ chính xác dự đoán. Hình 3 minh họa độ lệch dự đoán giữa K-R và PCoT trong G0.8A. K-R thể hiện độ chính xác dự đoán cao hơn PCoT từ Vòng 1, bắt đầu với các dự đoán chính xác hơn và ít ngẫu nhiên hơn. Hơn nữa, các dự đoán hội tụ nhanh chóng và trở nên rất chính xác trong nửa sau của trò chơi. Xu hướng này làm nổi bật sự thành thạo ngày càng tăng của LLM trong việc hiểu niềm tin bậc cao hơn với nhiều ngữ cảnh trò chơi hơn. Về cơ bản, K-R khởi tạo các phiên mới để tính toán hành động tương lai của đối thủ. Cách tiếp cận này tận dụng khả năng học trong ngữ cảnh của LLM hiệu quả hơn so với quá trình dự đoán của PCoT (như đã thảo luận lý thuyết trong Phần 2.2). Kết quả là, K-R đạt được độ chính xác dự đoán tốt hơn.

5.3 Phương pháp Lý luận Tốt hơn so với Mô hình Nền tảng Mạnh hơn
Có sự đồng thuận rằng LLM được huấn luyện với nhiều dữ liệu hơn và có kích thước tham số lớn hơn thể hiện khả năng lý luận mạnh hơn. Chúng tôi khám phá liệu Lý luận Cấp độ K có thể nâng cao đáng kể khả năng lý luận chiến lược của LLM tương đối yếu hơn hay không. Để điều tra, chúng tôi tiến hành thí nghiệm so sánh hiệu suất của K-R với GPT-3.5 (K-R[GPT-3.5]) với các phương pháp lý luận khác dựa trên GPT-4. Tất cả thí nghiệm được lặp lại 10 lần.

Bảng 7: So sánh Lý luận Cấp độ K với GPT-3.5 và các cách tiếp cận lý luận khác với GPT-4. Đối với Đoán 0.8 của Trung bình, chúng tôi báo cáo tỷ lệ thắng; đối với Trò chơi Đấu giá Sinh tồn, chúng tôi báo cáo vòng sinh tồn trung bình.

Đoán 0.8 của Trung bình | Trò chơi Đấu giá Sinh tồn
Đối thủ Direct K-R Direct K-R Direct K-R Direct K-R
[GPT-4] [GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4] [GPT-3.5] [GPT-3.5] [GPT-4] [GPT-4]
Direct 0.18 0.18 0.43 0.82 5.00 9.40 5.90 9.40
CoT 0.14 0.37 0.07 0.63 5.30 8.10 5.70 10.00
Persona 0.10 0.23 0.05 0.46 5.00 7.50 5.70 9.30
Reflect 0.24 0.38 0.42 0.78 5.00 8.50 9.40 10.00
Refine 0.14 0.13 0.10 0.46 5.10 6.70 6.30 7.90
PCoT 0.19 0.46 0.03 0.85 4.10 6.80 8.50 9.70

Trung bình 0.16 0.29 0.18 0.67 4.92 7.83 6.92 9.38

Từ kết quả trong Bảng 7, chúng tôi quan sát thấy rằng K-R[GPT-3.5] vượt trội hơn phương pháp gợi ý tiêu chuẩn của GPT-4 (Direct[GPT4]) từ hiệu suất trung bình. Hơn nữa, khi cạnh tranh với đối thủ sử dụng phương pháp lý luận trên GPT-4, K-R[GPT-3.5] thể hiện khả năng đáng chú ý. K-R, với việc khôi phục tuyệt vời quan điểm của đối thủ, nâng cao khả năng của LLM trong môi trường cạnh tranh. Thêm vào đó, chúng tôi so sánh hiệu suất của mô hình nguồn mở LLAMA2-7B với GPT-3.5/4 trong Phụ lục E, phát hiện rằng K-R nâng cao đáng kể lý luận trong các bối cảnh tương tác trên các LLM khác nhau.

5.4 Mức Tư duy Sâu hơn, Hiệu suất Chiến lược Tốt hơn?

Bảng 8: So sánh giữa Lý luận Cấp độ K[K=2] và Lý luận Cấp độ K[K=3] trong hai trò chơi.

Đoán 0.8 của Trung bình | Trò chơi Đấu giá Sinh tồn
Đối thủ Direct K-R[K=2] K-R[K=3] Direct K-R[K=2] K-R[K=3]
Direct 0.43 0.82 0.77 (-0.05) 5.90 9.40 9.40 (+0.00)
K-R[K=2] 0.04 0.52 0.60 (+0.08) 4.10 6.80 8.30 (+1.50)

K-R mô hình quá trình tư duy của đối thủ một cách đệ quy. Chúng tôi kiểm tra cách mức tư duy ảnh hưởng đến kết quả lý luận bằng cách so sánh K-R[K=2] và K-R[K=3] trong hai trò chơi. Kết quả, được chi tiết trong Bảng 8, tiết lộ tác động của việc tăng mức tư duy. Chống lại phương pháp Trực tiếp (tư duy cấp độ đầu tiên), K-R[K=3] cho thấy tỷ lệ thắng giảm trong G0.8A nhưng duy trì hiệu suất trong SAG, gợi ý có thể suy nghĩ quá mức. Tuy nhiên, K-R[K=3] cải thiện đáng kể chống lại K-R[K=2] trong cả hai trò chơi. Nó gợi ý rằng yếu tố chính trong K-R là độ sâu tư duy tương đối so với đối thủ. Cách tiếp cận sâu hơn một cấp độ mang lại lợi thế chiến lược, nhưng tiến lên hai cấp độ có thể dẫn đến lợi nhuận giảm dần do quá dự đoán. Trong môi trường tương tác, việc xác định mức tư duy của đối thủ là khó khăn. Thích ứng với các mức độ khác nhau và sử dụng Lý luận Cấp độ K cho phân tích sâu hơn là hướng nghiên cứu có giá trị cho tương lai.

Thêm vào đó, mức tư duy cao hơn với việc triển khai gợi ý đệ quy tăng chi phí tính toán. Chi phí tính toán của K-R được thảo luận kỹ lưỡng trong Phụ lục G.

6 Công trình Liên quan

6.1 Lý luận với LLM
Mô hình Ngôn ngữ Lớn (LLM) xuất sắc trong các nhiệm vụ lý luận phức tạp đa dạng, như toán học (Miao et al., 2021; Patel et al., 2021), thường thức (Talmor et al., 2022; Bhakthavatsalam et al., 2021), và lý luận tượng trưng (Srivastava et al., 2022; Suzgun et al., 2022). Một cách tiếp cận lý luận đáng chú ý bao gồm việc chia nhỏ các câu hỏi phức tạp thành một loạt các bước trung gian, một kỹ thuật được biết đến như phương pháp Chuỗi Suy nghĩ (CoT) (Wei et al., 2022; Kojima et al., 2022). Tiếp theo, một số công trình đã xuất hiện để mở rộng CoT, với các đổi mới như Cây Suy nghĩ (ToT) (Yao et al., 2023), Đồ thị Suy nghĩ (GoT) (Besta et al., 2023) và Bộ xương suy nghĩ (Ning et al., 2023). Bên cạnh đó, các cách tiếp cận như Self-Refine (Madaan et al., 2023) và Reflexion (Shinn et al., 2023) nâng cao tính nhất quán của CoT bằng cách có LLM xem xét và tinh chỉnh phản hồi của chúng. Hơn nữa, nghiên cứu gần đây đã tiết lộ rằng việc tích hợp thông tin persona vào LLM cải thiện đáng kể quá trình lý luận của chúng (Deshpande et al., 2023). Một loạt nghiên cứu (Fu et al., 2023; Wang et al., 2023) đã được tiến hành để kết hợp nhiều thông tin persona hơn, nhằm nâng cao tính hợp lý và khả năng kiến thức của quá trình lý luận LLM. Những phương pháp này đã được áp dụng cho các nhiệm vụ tĩnh khác nhau, nhưng chưa được đánh giá đầy đủ trong các vấn đề động (môi trường đa tác nhân) để xác thực hiệu quả của chúng trong khả năng lý luận.

6.2 Lý luận Chiến lược trong Hệ thống Đa Tác nhân
Các vấn đề động phát sinh khi nhiều người tham gia tham gia vào các tương tác đa vòng. Một yếu tố chính là các tương tác đồng thời của nhiều người tham gia với môi trường. Khác với các hệ thống đơn tác nhân, hệ thống đa tác nhân (MAS) gặp phải một loạt vấn đề và thách thức rộng hơn, như được ghi nhận bởi (Wong et al., 2021), bao gồm độ phức tạp tính toán (Ding and Dong, 2020), tính không ổn định (Papoudakis et al., 2019), khả năng quan sát một phần (Mahajan et al., 2019; Foerster et al., 2016), và các thách thức trong phân công tín dụng (Sunehag et al., 2017). Đặc biệt, trong bối cảnh suy luận sử dụng LLM, tính không ổn định của môi trường đặt ra một thách thức riêng biệt.

Gần đây, nghiên cứu về LLM trong lý luận chiến lược đã được tiến hành trên các MAS khác nhau bao gồm hành vi xã hội(Zhou et al., 2024; Hua et al., 2023), mô phỏng kinh tế(Zhao et al., 2023; Li et al., 2023), lý thuyết trò chơi(Duan et al., 2024; Xu et al., 2023a), và chơi trò chơi(Ma et al., 2023; Xu et al., 2023b). Để nâng cao hiệu suất của LLM trong các tình huống lý luận chiến lược, các nhà nghiên cứu đã sử dụng các khái niệm về Lý thuyết Tâm trí (ToM) (Gandhi et al., 2023; Guo et al., 2023) và Học Tăng cường (Xu et al., 2023c; Zhang et al., 2024a) để tối ưu hóa các quá trình lý luận của LLM. Những cách tiếp cận này bao gồm việc gợi ý LLM nhận ra sự phức tạp của các nhiệm vụ chiến lược, giống như đường cơ sở Chuỗi Suy nghĩ Dự đoán được đề xuất của chúng tôi. Tuy nhiên, kết quả thí nghiệm của chúng tôi cho thấy rằng cách tiếp cận này không thể thiết lập một hệ thống phân cấp nhận thức rõ ràng cần thiết cho tư duy đệ quy và chiến lược sâu hơn.

7 Kết luận
Bài báo này đại diện cho một bước tiến đáng kể trong việc hiểu và nâng cao khả năng lý luận chiến lược của LLM. Chúng tôi đề xuất "Lý luận Cấp độ K với LLM." Cách tiếp cận đổi mới này tận dụng cơ chế đệ quy để đạt được mức tư duy khác nhau trong LLM, cho phép chúng tham gia vào tư duy chiến lược sâu hơn. Thông qua các thí nghiệm rộng rãi, chúng tôi xác thực lợi thế được cung cấp bởi phương pháp này. Nó thiết lập nền tảng cho nghiên cứu tương lai về lý thuyết tâm trí và lý luận chiến lược trong LLM.

8 Hạn chế
Chúng tôi xác thực hiệu quả của khung Lý luận Cấp độ K từ hai góc độ: lý thuyết trò chơi và trí tuệ xã hội. Mặc dù kết quả thí nghiệm của chúng tôi cung cấp bằng chứng đáng kể hỗ trợ tính hợp lệ của khung, nghiên cứu thêm là cần thiết để khám phá hiệu suất của mô hình ngôn ngữ lớn (LLM) trong mô hình tác nhân few-shot (He et al., 2016) trên các môi trường, yếu tố chiến lược và tập hành động khác nhau.

Thêm vào đó, K-R dự đoán hành vi có khả năng nhất của đối thủ bằng cách khởi tạo phiên suy luận LLM mới. Cơ chế đệ quy được sử dụng để đạt được các mức độ sâu chiến lược khác nhau chắc chắn tăng chi phí tính toán. Phụ lục G cung cấp thảo luận chi tiết về cách K-R liên quan đến sự gia tăng chi phí tính toán này và so sánh nó trên các phương pháp lý luận khác nhau. Mặc dù có nhu cầu tăng lên, K-R vượt trội hơn các phương pháp khác với chi phí tính toán tương đương.

Tài liệu tham khảo
[Danh sách các tài liệu tham khảo từ trang 9-11...]

[Các phụ lục từ trang 12-24 bao gồm các chi tiết bổ sung về thiết lập thí nghiệm, công thức tính toán, gợi ý được sử dụng, và phân tích thống kê...]
