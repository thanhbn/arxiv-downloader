# 2307.11787.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/reasoning/2307.11787.pdf
# File size: 128467 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
arXiv:2307.11787v2  [cs.CL]  16 Aug 2023LLM Cognitive Judgements Diﬀer From Human
Sotiris Lamprinidis
sotiris@lamprinidis.com
Copenhagen, Denmark
Abstract
Large Language Models (LLMs) have lately been on the spotlig ht of
researchers, businesses, and consumers alike. While the li nguistic capa-
bilities of such models have been studied extensively, ther e is growing
interest in investigating them as cognitive subjects . In the present work I
examine GPT-3 and ChatGPT capabilities on an limited-data i nductive
reasoning task from the cognitive science literature. The r esults suggest
that these models’ cognitive judgements are not human-like .
Keywords— Large Language Models, GPT-3, ChatGPT, Inductive Rea-
soning, Cognitive Judgements
1 Introduction
Large Language Models (LLMs), as typiﬁed by OpenAI ChatGPT [23] have
lately had massive impact on public opinion: out of a maximum of 100, the
terms “ChatGPT” and “AI” surged from 0 and 28 respectively on 20 Nov. 2022
to 73and 99respectivelyon30Apr. 20231This optimism isalsoreﬂectedin the
Markets, as valuations of “AI”-exposed companies have been sur ging [1]. Re-
searchers have expressed mixed opinions about the true abilities of such models,
ranging from strong claims about Artiﬁcial General Intelligence [8]( Microsoft is
a co-owner of OpenAI) and the seemingly anthropomorphic attribu tion of con-
sciousness on such models [20] (which humans partially agree [29]), to dismissal
of any emergent notion of intelligence or human-like behavior [10, 13 , 18].
Perhaps most eloquently presented in [3], it is not clear if LLMs understand
language, or if they are just very proﬁcient at manipulating languag e symbols,
which we humans are vulnerable to implicitly interpreting as coherent. In fact,
even before the current generation of LLMs there has been a foc us on uncover-
ing general linguistic competencies and capabilities [16, 28, 31, 27], as well as
thehuman-likedness of Language Models [16, 12]. In the same spirit, there have
been extensive investigations of ChatGPT performance in Natural Language
Processing (NLP) tasks [2], discovering good overall performance but poor abil-
ity when it comes to reasoning and speciﬁcally inductive reasoning, as well as
extensively accounting ChatGPT failures [5]. Additionally, ChatGPT re sponses
seem to accurately mimic human language use [9]. Could these positive r esults
support claims of intelligence or consciousness of LLMs?
1Data source: Google Trends (https://www.google.com/tren ds).
1

--- PAGE 2 ---
I propose that the strength of the evidence should be proportion al to the
strength of the claims made, and as such any emergent behaviours that seem
to transcend the mere linguistic realm when it comes to the capacity o f LLMs
should be considered from a perspective of cognition .
2 Previous Work
An earlyworkonGPT-3 [6] examined the responsesqualitativelyin wh at is des-
ignated an “Author’s Turing Test” and found inconsistent behaviou r: “[GPT-3
is] not better than our very best writers and philosophers at their peak. Are its
best moments better than many humans and even perhaps, our be st writers at
their worst? Quite possibly [11]. Other works have focused on evaluating LLMs
in experimentsfromthe cognitivepsychologyliteratureandthe human-likedness
of their responses. [4] found GPT-3 to mostly give human-like respo nses. A re-
curring problem, however, with these studies is the explicit memoriza tion of the
materials. Asking GPT-3 about the tasks in [4], I found that many of t he tasks
employed (all of Kahneman & Tversky’s, Wason’s Card Selection task , and the
Blicket task) could be accurately recited by the model.
An assortment of works study the Theory of Mind (ToM) of LLMs - t he
ability to attribute mental states to others. [17] examined both C hatGPT and
GPT-4 ToM using material from Kahneman & Tversky. Similarly to [4], I f ound
that ChatGPT could accurately recite all the tasks. The GPT-4 ToM was also
qualitatively investigated in [14] using 10 philosophical paradoxes. He re too,
I found that ChatGPT (the predecessor model) could recite 9 out o f the 10
paradoxes. Another work focusing on ChatGPT ToM applied clinical m ethods
used to assesspathologicalToM in humans [7]. The model in general performed
poorly and not similarly to humans in most tasks.
Morecloselyrelatedtothepresentwork,[21]performedaneurop sychological
investigation of abilities related to prefrontal functioning and foun d ChatGPT
unable to mimic human cognitive functioning accurately. [19] studied GPT-3 as
pragmatic reasoner and found human-like behaviour, albeit with som e excep-
tions. Finally, [32] investigated ChatGPT and GPT-4 on a range of psy chologi-
cal tasks classiﬁed by the degree of embodiment associated with ea ch and found
that LLM responses deviated from human on sensory and motor ju dgements.
Inthiswork,Ifocuson[15]fromthecognitivescienceliterature. T heauthors
support the hypothesis that when faced with inductive problems wit h limited
data available, human cognitive judgments closely correspond to th ose of a
Bayesian model with ﬁtting priors. The motivation for choosing this p articular
study is ﬁrstly there is no true answer to be determined deductively and as
such it seems ideal to reveal the underlying cognitive workings of a m ind, and
secondly GPT-3 had no knowledge whatsoever about the work, while ChatGPT
was able to give a somewhat accurate high-level overview but could n ot recite
any particular experiment presented in the original work, thus avo iding the
common pitfall of model memorization of human responses.
2

--- PAGE 3 ---
3 Methods
In [15] all tasks involve predicting an extent or duration of a phenom enon given
an intermediate value at an unknown time: given a value t, predict the ﬁnal,
total value ttotal. Responses from 350 participants were compared to a Bayesian
predictorwhichcomputesaprobabilityover ttotal:p(ttotal|t)∝p(t|ttotal)p(ttotal).
The authors found that human judgments are close to the optimal predictions
of the Bayesian predictor.
Speciﬁcally, the tasks and respective tare: cake baking times (minutes), life
spans (years), movie grosses(million US $), poem lengths (number of lines), U.S.
representatives’terms (years), and telephone box oﬃce waiting times (minutes).
I omit the “Movie Runtimes” and “Pharaohs” tasks that had high var iance in
human responses. I inferred the values for the mean human predic tions and
conﬁdence intervals, as well as the predictions of the Bayesian mod els from the
plots in the original work. I refer to [15] for further details on the p rompts and
the Bayesian predictors’ choice of priors.
For each task and prompt value I asked GPT-3 [25] and ChatGPT [24 ] the
equivalent question 20 times, starting a new conversation each time . I skipped
the overallintroductionasit appearsin the study and ﬁnished each prompt with
“The answer must be a single number, not a range, with no expla nation.”. E.g.:
“Imagine you hear about a movie that has taken in 10 million do llars at the box
oﬃce, but don’t know how long it has been running. What would y ou predict
for the total amount of box oﬃce intake for that movie? The ans wer must be a
single number, not a range, with no explanation.”. I manually went through
the results and extracted the response value when applicable, rej ecting answers
where a range or a nonsensical value was given (the number of reje cted answers
is presented in Table 2). The Model answers and relevant code for r eproducing
the results are made available online2
Bayesian Prediction GPT-3 ChatGPT
Cakes 11.5 30.1 16.5
Life Spans 2.5 5.6 3.4
Movie Grosses 20.2 34.9 160.5
Poems 5.6 28.6 20.1
Representatives 16.0 32.0 29.5
Waiting Times 20.7 31.3 37.3
Table 1: Mean Average Percentage Error between the medians of t he proposed
models and the human judgements
4 Results
I present the Mean Average Percentage Error between each mod el and the hu-
man judgements in Table 1. The Bayesian predictors of [15] have con sistently
the lowest error compared to either GPT-3 or ChatGPT. These res ults sug-
gest that LLMs do not align with human cognition when it comes to induc tive
cognitive judgments about everyday phenomena under limited availa ble data.
2https://github.com/sotlampr/llm-cognitive-judgement s
3

--- PAGE 4 ---
0408004080120160Cakes
050100050100150200Life Spans
050100050100150200Movie Grosses
0408004080120160Poems
02040020406080Representatives
01530015304560Waiting Times
tttotal
Bayesian Estimate
Human ParticipantsGPT-3
ChatGPT
Figure 1: Medians and bootstrap 68% conﬁdence interval ( n= 1000)
ComparingGPT-3toChatGPT,thelatterisclosertohumanparticipa ntsfor
most tasks, with the strikingexceptionofthe “MovieGrosses”,wh ereChatGPT
was the most hesitant to answer.
Graphs similar to [15] can be seen in Figure 1. GPT-3 and ChatGPT
demonstrate inconsistent predictions in a few cases regardless of the task, as
seen in the empirical conﬁdence intervals for e.g. “Cakes” at t= 70, “Movie
Grosses” at t= 100 and “Representatives” at t= 7.
I present the percentage of instances where no meaningful resp onse was
given in Table 2. GPT-3 was able to answer meaningfully and succinctly in all
cases, while ChatGPT was hesitant when it comes to “Life Spans” and “Movie
Grosses”. Common responses for both tasks were similar to “The answer is not
possible to determine ...”,“There is not enough information given to accurately
predict...”,“I’m sorry, but I cannot provide ...”,“Since I am an AI language
model, I cannot ...”. Additionally, on the “Life Spans” tasks, ChatGPT an-
swered 6 times that it would not be ethical to make such predictions.
These patterns in refusing to answer also appear in previous works such
4

--- PAGE 5 ---
Task GPT-3 ChatGPT
Cakes 0.0 0.0
Life Spans 0.0 45.0
Movie Grosses 0.0 71.3
Poems 0.0 0.0
Representatives 0.0 3.0
Waiting Times 0.0 0.0
Table 2: Percentage of samples with no satisfactory answer
as [5, 30, 20, 22], possibly revealing the eﬀect of prompt tuning done with
InstructGPT [26], penalizing hallucinatory, counterfactual, or oth erwise unde-
sirable responses
5 Conclusion
I demonstrated a clear failure of popular LLMs when it comes to makin g in-
ductive judgements with limited available data on everyday scenarios . Despite
having a huge number of parameters and being trained on a huge amo unt of
data, these models cannot accurately model basic statistical prin ciples that the
human mind seems to entrust, and which could be more accurately mo delled
with many orders of magnitude less parameters.
References
[1] Surgingstockmarketsarepoweredby artiﬁcial intelligence. The Economist ,
(June 10th 2023), Jun 2023.
[2] Y.Bang,S.Cahyawijaya,N.Lee, W.Dai, D.Su, B.Wilie, H.Lovenia ,Z.Ji,
T. Yu, W. Chung, et al. A multitask, multilingual, multimodal evaluation
of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint
arXiv:2302.04023 , 2023.
[3] E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On t he
dangers of stochastic parrots: Can language models be too big. In Proceed-
ings of the 2021 ACM conference on fairness, accountability , and trans-
parency, pages 610–623, 2021.
[4] M. Binz and E. Schulz. Using cognitive psychology to understand g pt-
3.Proceedings of the National Academy of Sciences , 120(6):e2218523120,
2023.
[5] A. Borji. A categorical archive of chatgpt failures. arXiv preprint
arXiv:2302.03494 , 2023.
[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariw al,
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language mo dels
are few-shot learners. Advances in neural information processing systems ,
33:1877–1901, 2020.
5

--- PAGE 6 ---
[7] E. Brunet-Gouet, N. Vidal, and P. Roux. Can a conversational a gent pass
theory-of-mind tasks? A case study of ChatGPT with the Hinting, F alse
Beliefs, and Strange Stories paradigms. June 2023. doi: 10.5281. U RL
https://hal.science/hal-03991530 .
[8] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvit z, E. Kamar,
P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artiﬁcial gen eral in-
telligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 ,
2023.
[9] Z.G.Cai, D.A.Haslett, X.Duan, S.Wang, andM.J.Pickering. Do eschat-
gpt resemble humans in language use? arXiv preprint arXiv:2303.08014 ,
2023.
[10] N. Chomsky, I. Roberts, and J. Watumull. Noam chomsky: The f alse
promise of chatgpt. The New York Times , 8, 2023.
[11] K. Elkins and J. Chun. Can gpt-3 pass a writer’s turing test? Journal of
Cultural Analytics , 5(2), 2020.
[12] A. Ettinger. What bert is not: Lessons from a new suite of psyc holinguis-
tic diagnostics for language models. Transactions of the Association for
Computational Linguistics , 8:34–48, 2020.
[13] L. Floridi. Ai as agency without intelligence: On chatgpt, large lan guage
models, and other generative models. Philosophy & Technology , 36(1):15,
2023.
[14] L. Freund. Exploring the intersection of rationality, reality, an d theory of
mind in ai reasoning: An analysis of gpt-4’s responses to paradoxes and
tom tests.
[15] T. L. Griﬃths and J. B. Tenenbaum. Optimal predictions in every day
cognition. Psychological science , 17(9):767–773, 2006.
[16] K. Gulordava, P. Bojanowski, E. Grave, T. Linzen, and M. Baro ni. Col-
orless green recurrent networks dream hierarchically. In Proceedings of
the 2018 Conference of the North American Chapter of the Asso ciation
for Computational Linguistics: Human Language Technologi es, Volume 1
(Long Papers) . Association for Computational Linguistics, 2018.
[17] B. Holterman and K. van Deemter. Does chatgpt have theory o f mind?
arXiv preprint arXiv:2305.14020 , 2023.
[18] R. Katzir. Why largelanguagemodelsarepoor theoriesofhuman linguistic
cognition. a reply to piantadosi (2023). Manuscript. Tel Aviv University.
url: https://lingbuzz. net/lingbuzz/007190 , 2023.
[19] B. Lipkin, L. Wong, G. Grand, and J.B. Tenenbaum. Evaluatings tatistical
language models as pragmatic reasoners. arXiv preprint arXiv:2305.01020 ,
2023.
[20] D. Lloyd. What is it like to be a bot?: The world according to gpt-4.
Available at SSRN 4443727 , 2023.
6

--- PAGE 7 ---
[21] R. Loconte, G. Orr` u, M. Tribastone, P. Pietrini, and G. Sarto ri. Chal-
lenging chatgpt’intelligence’with human tools: A neuropsychological in ves-
tigation on prefrontal functioning of a large language model. Intelligence ,
2023.
[22] C. Michaux. Can chat gpt be considered an author? i met with ch at gpt
and asked some questions about philosophy of art and philosophy of mind.
Available at SSRN 4439607 , 2023.
[23] OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt , . Re-
treived 13th June 2023.
[24] OpenAI. Chatgpt. https://chat.openai.com , . May 24 Version.
[25] OpenAI. Gpt-3. https://platform.openai.com/docs/models/gpt-3 , .
Model:davinci-003 .
[26] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,
C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Traininglanguagemode lsto
follow instructions with human feedback. Advances in Neural Information
Processing Systems , 35:27730–27744, 2022.
[27] P. Qian and R. P. Levy. Neural language models as psycholinguist ic sub-
jects: Representations of syntactic state. Association for Com putational
Linguistics, 2019.
[28] M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh. Beyond accurac y: Behav-
ioraltesting of nlp models with checklist. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics . Association for
Computational Linguistics, 2020.
[29] A. E. Scott, D. Neumann, J. Niess, and P. W. Wo´ zniak. Do you m ind?
user perceptions of machine consciousness. In Proceedings of the 2023 CHI
Conference on Human Factors in Computing Systems , pages 1–19, 2023.
[30] V. Taecharungroj. “what can chatgpt do?” analyzing early re actions to the
innovative ai chatbot on twitter. Big Data and Cognitive Computing , 7(1):
35, 2023.
[31] A. Warstadt, A. Singh, and S. R. Bowman. Neural network acc eptability
judgments. Transactions of the Association for Computational Linguis tics,
7:625–641, 2019.
[32] Q. Xu, Y. Peng, M. Wu, F. Xiao, M. Chodorow, and P. Li. Does con ceptual
representation require embodiment? insights from large language m odels.
arXiv preprint arXiv:2305.19103 , 2023.
7
