# 2401.17464.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reasoning/2401.17464.pdf
# Kích thước file: 959345 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Sử dụng Công cụ Hiệu quả với Lý luận Chuỗi-của-Trừu tượng
Silin Gao1,2∗, Jane Dwivedi-Yu2, Ping Yu2, Xiaoqing Ellen Tan2,
Ramakanth Pasunuru2, Olga Golovneva2, Koustuv Sinha2
Asli Celikyilmaz2, Antoine Bosselut1†, Tianlu Wang2†
1EPFL,2FAIR @ Meta
1{silin.gao,antoine.bosselut}@epfl.ch
2{silingao,janeyu,pingyu,ellenxtan}@meta.com
2{rpasunuru,olggol,koustuvs,aslic,tianluwang}@meta.com
Tóm tắt
Để đạt được lý luận trung thực phù hợp với
kỳ vọng của con người, các mô hình ngôn ngữ lớn
(LLM) cần phải dựa lý luận của chúng vào kiến
thức thế giới thực (ví dụ: sự kiện web, quy tắc
toán học và vật lý). Các công cụ giúp LLM truy
cập kiến thức bên ngoài này, nhưng vẫn còn thách
thức trong việc tinh chỉnh các tác nhân LLM (ví
dụ: Toolformer) để gọi công cụ trong các bài toán
lý luận nhiều bước, nơi các lệnh gọi công cụ được
kết nối với nhau đòi hỏi việc lập kế hoạch sử dụng
công cụ toàn diện và hiệu quả.

Trong công trình này, chúng tôi đề xuất một
phương pháp mới cho LLM để tận dụng tốt hơn
các công cụ trong lý luận nhiều bước. Phương
pháp của chúng tôi, Chuỗi-của-Trừu tượng
(CoA), huấn luyện LLM trước tiên giải mã các
chuỗi lý luận với các trình giữ chỗ trừu tượng,
sau đó gọi các công cụ miền để cụ thể hóa mỗi
chuỗi lý luận bằng cách điền vào kiến thức cụ
thể. Việc lập kế hoạch với các chuỗi trừu tượng
này cho phép LLM học các chiến lược lý luận
tổng quát hơn, vững chắc trước những thay đổi
của kiến thức miền (ví dụ: kết quả toán học) liên
quan đến các câu hỏi lý luận khác nhau. Nó cũng
cho phép LLM thực hiện việc giải mã và gọi các
công cụ bên ngoài song song, tránh được độ trễ
suy luận do chờ đợi phản hồi từ công cụ. Trong
các miền lý luận toán học và Wiki QA, chúng tôi
cho thấy phương pháp của chúng tôi liên tục vượt
trội hơn các baseline chuỗi-của-tư duy và tăng
cường công cụ trước đó trên cả tập kiểm tra trong
phân phối và ngoài phân phối, với cải thiện độ
chính xác QA trung bình ~6% tuyệt đối. Các tác
nhân LLM được huấn luyện với phương pháp
của chúng tôi cũng cho thấy việc sử dụng công
cụ hiệu quả hơn, với tốc độ suy luận trung bình
~1.4× nhanh hơn so với LLM tăng cường công
cụ baseline.

1 Giới thiệu
Các mô hình ngôn ngữ lớn gần đây (LLM; Touvron
et al., 2023b; Anil et al., 2023; OpenAI, 2023),
đã có tiến bộ trong việc diễn giải và thực hiện
hướng dẫn (Wei et al., 2021; Chung et al., 2022),
*Công việc thực hiện trong thời gian thực tập tại FAIR.
†Giám sát Bằng nhau.

Trong một trò chơi 90 phút, Mark đã chơi 20 phút, sau đó thêm 35 phút nữa. Anh ấy đã ngồi dự bị bao lâu?
LLM  Công cụ
LLM Mark đã chơi tổng cộng [20 + 35 = y1] phút. Vậy, anh ấy đã ngồi dự bị [90 - y1 = y2] phút.
y1 = 20 + 35 = 55
y2 = 90 – y1 = 90 - 55 = 35
Câu trả lời là 35 phút.

Ralph Hefferline là giáo sư tâm lý học tại một trường đại học. Trường đại học này nằm ở thành phố nào?
Tìm kiếm [trường đại học của Ralph Hefferline -WikiSearch-> y1], đó là [y1 -NER-> y2]. Sau đó tìm [thành phố mà y2 ở -WikiSearch-> y3].
y1: Ralph Hefferline là giáo sư tại Đại học Columbia …
y2: Đại học Columbia
y3: Đại học Columbia là một trường đại học Ivy League ở New York …
Câu trả lời là New York.

Lý luận Toán học  Wiki QA

Hình 1: Tổng quan về lý luận chuỗi-của-trừu tượng
với công cụ. Đưa ra một câu hỏi miền (cuộn xanh lá), một
LLM được tinh chỉnh để trước tiên tạo ra một chuỗi lý luận
trừu tượng nhiều bước (bong bóng xanh dương), sau đó gọi các
công cụ bên ngoài để cụ thể hóa chuỗi với kiến thức cụ thể
miền (nhãn cam). Câu trả lời cuối cùng (bong bóng vàng) được
thu được dựa trên chuỗi lý luận đã được cụ thể hóa.

nhưng vẫn mắc lỗi khi nhớ lại và kết hợp
kiến thức thế giới cho phản hồi của chúng, ví dụ: đưa ra
các tuyên bố không đúng sự thật (Maynez et al., 2020; Ji et al.,
2023), các phép tính sai (Patel et al., 2021), v.v.
Sử dụng các công cụ phụ trợ (ví dụ: một công cụ tìm kiếm để cung
cấp các sự kiện đáng tin cậy, một máy tính cho các phép
toán chính xác, v.v.) tại thời điểm suy luận có thể giảm thiểu
một số lỗi này, tạo động lực cho các mô hình ngôn ngữ
tăng cường công cụ tích hợp các lệnh gọi API bên ngoài
vào việc tạo ra đầu ra của chúng (Parisi et al., 2022;
Schick et al., 2023; Hao et al., 2023b).

Tuy nhiên, chúng tôi cho thấy rằng các LLM tăng
cường công cụ hiện tại, ví dụ: Toolformer (Schick et al., 2023), gặp
khó khăn trong việc tận dụng một cách đáng tin cậy và hiệu quả
các công cụ trong lý luận nhiều bước. Cụ thể, các lệnh gọi công cụ trong
các nhiệm vụ lý luận nhiều bước thường được xen kẽ
(tức là phản hồi của một lệnh gọi API thường là một phần của
truy vấn của một lệnh gọi tiếp theo; như được hiển thị trong Hình 1).
Không mô hình hóa rõ ràng các kết nối tương hỗ này

1arXiv:2401.17464v3  [cs.CL]  8 Jan 2025

--- TRANG 2 ---
trong các chuỗi lý luận, LLM không học được cách
lập kế hoạch hiệu quả cho việc sử dụng công cụ, dẫn đến lý luận
với công cụ kém chính xác hơn.1Trong khi đó, việc xen kẽ
việc tạo ra văn bản với các lệnh gọi API cũng tạo ra "thời gian chờ"
suy luận không hiệu quả, nơi mô hình phải chờ đợi
phản hồi từ lệnh gọi API trước khi tiếp tục
quá trình giải mã. Sự không hiệu quả này trở nên đáng kể hơn
trong các tình huống lý luận nhiều bước, khi thường
cần nhiều vòng lệnh gọi API cho mỗi quá trình lý luận.

Trong công trình này, chúng tôi đề xuất lý luận Chuỗi-
của-Trừu tượng (CoA), một phương pháp vững chắc và hiệu quả cho
LLM để thực hiện lý luận nhiều bước với công cụ.
Như được hiển thị trong Hình 1, LLM được tinh chỉnh với
mục tiêu tạo ra các chuỗi lý luận với các trình giữ chỗ
trừu tượng. Các trình giữ chỗ không ảnh hưởng đến luồng
lý luận của LLM, và sau đó được điền với
kiến thức cụ thể được lấy từ các công cụ chuyên biệt,
để làm cơ sở cho việc tạo ra câu trả lời cuối cùng. Việc lập kế hoạch
chuỗi lý luận trừu tượng khuyến khích LLM
kết nối nhiều lệnh gọi công cụ và áp dụng các
chiến lược lý luận khả thi hơn, vững chắc trước
sự biến đổi của kiến thức miền liên quan trong mỗi
quá trình lý luận, ví dụ: kết quả tính toán cụ thể.

Không giống như các phương pháp trước đây nơi việc giải mã
LLM và các lệnh gọi API được thực hiện theo cách xen kẽ,
phương pháp của chúng tôi tận dụng các công cụ để điền kiến thức
một lần sau khi toàn bộ chuỗi lý luận được tạo ra.
Điều này cho phép việc giải mã hiệu quả hơn trên
nhiều ví dụ (ví dụ: như trong một luồng) vì
các vết CoA cho các ví dụ tiếp theo có thể được
giải mã trong khi các lệnh gọi công cụ được thực hiện cho các
ví dụ trước đó, giảm thiểu thời gian suy luận tổng thể. Chúng tôi
phát triển một pipeline đơn giản để xây dựng dữ liệu tinh chỉnh cho
các mô hình để học CoA, nơi chúng tôi trước tiên nhắc LLM
viết lại các phản hồi hiện có cho các hướng dẫn dưới dạng
chuỗi trừu tượng, sau đó sử dụng các công cụ miền để kiểm tra
tính hợp lệ của việc viết lại, như được hiển thị trong Hình 2.

Sau khi huấn luyện LLM để học lý luận CoA,
chúng tôi đánh giá các mô hình được tinh chỉnh trên hai miền
lý luận nhiều bước đại diện, bao gồm
lý luận toán học (Cobbe et al., 2021; Miao
et al., 2020; Patel et al., 2021; Koncel-Kedziorski
et al., 2016), và Wikipedia (Wiki) QA (Yang et al.,
2018; Berant et al., 2013; Kwiatkowski et al., 2019;
Joshi et al., 2017) liên quan đến lý luận trên kiến thức
mô tả sự kiện. Chúng tôi cho thấy phương pháp của chúng tôi
tăng cường hiệu suất của LLM, với cải thiện độ chính xác
tuyệt đối trung bình ~7.5% và 4.5% trên toán
và Wiki QA, tương ứng. Những cải thiện này

1như được xác minh bởi phân tích của chúng tôi trong §5

là nhất quán trên cả tập kiểm tra trong phân phối và (zero-
shot) ngoài phân phối, và đặc biệt rõ rệt trên các câu hỏi
yêu cầu lý luận chuỗi-của-tư duy phức tạp.2Trong khi đó, phương
pháp của chúng tôi cũng sử dụng công cụ hiệu quả hơn so với các
phương pháp tăng cường trước đây, với tốc độ suy luận trung bình
~1.47× và 1.33× nhanh hơn trên các nhiệm vụ toán và
Wiki QA, tương ứng. Cuối cùng, đánh giá con người mở rộng
chứng minh rằng phương pháp của chúng tôi hướng dẫn
LLM học lý luận chính xác hơn,
dẫn đến ~8% ít lỗi lý luận hơn.

2 Công trình Liên quan
LLM Tăng cường Công cụ Ngày càng có
sự quan tâm trong việc tăng cường LLM bằng các công cụ bên ngoài.
Công trình đáng kể đã cố gắng thích ứng LLM như
những người lý luận sử dụng công cụ thông qua học trong ngữ cảnh,
chứng minh cải thiện hiệu suất đầy hứa hẹn
trong các ứng dụng khác nhau, ví dụ: giải quyết bài toán toán
(Gao et al., 2023; Chen et al., 2022),
trả lời câu hỏi y sinh (Jin et al., 2023)
và tự phê bình (Gou et al., 2023). Tuy nhiên,
việc hướng dẫn LLM sử dụng công cụ hiệu quả bằng
các minh họa trong ngữ cảnh là thách thức, đòi hỏi
kỹ thuật prompt cụ thể cho nhiệm vụ phức tạp
và bị hạn chế bởi khả năng tuân theo hướng dẫn
của mô hình (Jacovi et al., 2023). Nhận thấy những hạn chế
của học trong ngữ cảnh, một số công trình dạy
LLM học cách sử dụng công cụ bằng tinh chỉnh
(Parisi et al., 2022; Schick et al., 2023; Hao et al.,
2023b), cải thiện hiệu suất của LLM một cách
vững chắc hơn. Tuy nhiên, tất cả các phương pháp trên đều áp dụng
tương tác tuần tự với công cụ trong suốt quá trình
lý luận, làm chậm tốc độ suy luận như một hàm
của độ trễ của công cụ (hoặc API) và số lượng
lệnh gọi API được thực hiện.

Một số công trình trước khác tập trung vào việc sử dụng LLM cho
lý luận nhiều bước với các module khác. Cụ thể,
ReAct (Yao et al., 2023b) và FireAct (Chen
et al., 2023) tích hợp LLM với công cụ vào một vòng lặp
đóng của các bước tư duy, hành động và quan sát. Vòng lặp
lý luận dài dòng này làm chậm việc giải mã LLM,
và vẫn kết hợp công cụ thông qua tương tác tuần tự,
dẫn đến suy luận không hiệu quả. Một
hướng công trình khác, Program of Thoughts (Chen
et al., 2022), DECLARATIVE (He-Yueya et al.,
2023) và PAL (Gao et al., 2023) nhắc LLM
tạo ra lý luận dựa trên chương trình và tương tác
với các trình thực thi mã, tuy nhiên phụ thuộc nhiều
vào các mô hình mã nguồn đóng, tức là Codex (Chen

2ví dụ: hơn 3 bước suy dẫn toán học

--- TRANG 3 ---
et al., 2021), và bị hạn chế trong lý luận số học
theo thủ tục. Dựa trên những công trình này, CoA
đề xuất một khung để chuyển đổi các vết lý luận
ngôn ngữ tự nhiên thành các biểu diễn trừu tượng, và
sử dụng các vết lý luận trừu tượng làm dữ liệu tinh chỉnh
để cải thiện LLM tăng cường công cụ. CoA cũng
tăng tốc lý luận tăng cường công cụ, bằng cách lập kế hoạch
toàn diện các vết CoA và chỉ gọi công cụ một lần
tại thời điểm suy luận.

Lập Kế hoạch Sử dụng Công cụ Một số công trình trước đây
nghiên cứu lập kế hoạch sử dụng công cụ trong LLM. Cụ thể,
HuggingGPT (Shen et al., 2023), Chameleon (Lu
et al., 2023), OpenAGI (Ge et al., 2023) và Meta-
Tool (Huang et al., 2023) tập trung vào lập kế hoạch
chuỗi cấp cao của việc sử dụng nhiều công cụ để
giải quyết các nhiệm vụ hỗn hợp đa miền. Tương tự, LATM
(Cai et al., 2023), ML-BENCH (Liu et al., 2023)
và Gorilla (Patil et al., 2023) nhằm mục đích lập kế hoạch
tích hợp cấp chương trình của nhiều API để thiết kế
các script của các nhiệm vụ theo thủ tục, ví dụ: một script
để huấn luyện một mô hình được mô tả bởi một kho GitHub.
ToolChain* (Zhuang et al., 2023) kết hợp
việc lập kế hoạch sử dụng công cụ với lý luận
dựa trên tìm kiếm cây (Yao et al., 2023a; Hao et al., 2023a),
đặc biệt hữu ích cho các nhiệm vụ theo thủ tục (Xu
et al., 2023; Cobbe et al., 2021). Khác với
công trình trên, chúng tôi tập trung vào lập kế hoạch của
lý luận chuỗi-của-tư duy (Wei et al., 2022) tổng quát với
nhận thức về các công cụ chuyên biệt miền.

3 Phương pháp
Lý luận Chuỗi-của-Trừu tượng (CoA) Phương
pháp của chúng tôi tách biệt lý luận tổng quát của LLM
khỏi kiến thức cụ thể miền thu được từ các
công cụ bên ngoài. Hình 1 cho thấy tổng quan về phương
pháp của chúng tôi. Cụ thể, chúng tôi trước tiên tinh chỉnh LLM để
tạo ra các chuỗi lý luận với các trình giữ chỗ
trừu tượng, ví dụ: y1, y2 và y3,3 như được hiển thị trong Hình 1. Trong
giai đoạn thứ hai, chúng tôi cụ thể hóa mỗi chuỗi lý luận bằng cách
thay thế các trình giữ chỗ bằng kiến thức cụ thể miền
thu được từ các công cụ bên ngoài, ví dụ: kết quả tính toán
từ một máy tính, các bài viết liên quan được lấy
từ công cụ tìm kiếm web, v.v. Cuối cùng, câu hỏi
được trả lời dựa trên chuỗi lý luận đã được cụ thể hóa.

Lưu ý rằng vì LLM được huấn luyện để tạo ra
chuỗi lý luận trừu tượng thay vì lý luận
chuỗi-của-tư duy (CoT) thông thường với các giá trị
rõ ràng, điều này cho phép LLM tập trung vào việc học
các chiến lược lý luận tổng quát và toàn diện mà không cần

3Chúng tôi cũng kiểm tra các trình giữ chỗ ở định dạng ký tự đơn, ví dụ:
x, y và z, nhưng chúng dẫn đến kết quả dưới mức tối ưu.

Trong một trò chơi 90 phút, Mark đã chơi 20 phút, sau đó thêm 35 phút nữa. Anh ấy đã ngồi dự bị bao lâu?

LLM  Công cụ
Mark đã chơi tổng cộng [20 + 35 = y1] phút. Vậy, anh ấy đã ngồi dự bị [90 - y1 = y2] phút.
Mark đã chơi tổng cộng 20 + 35 = 55 phút. Vậy, anh ấy đã ngồi dự bị 90 - 55 = 35 phút.

y1 = 20 + 35 = 55
y2 = 90 – y1 = 90 - 55 = 35

Hình 2: Minh họa việc viết lại dữ liệu vàng cho việc
xây dựng dữ liệu tinh chỉnh. Đưa ra một cặp câu hỏi miền
(cuộn xanh lá) và câu trả lời vàng (cuộn vàng), một
LLM được nhắc viết lại câu trả lời vàng thành một chuỗi
lý luận với các biến trừu tượng (bong bóng tím).
Sau đó, các công cụ chuyên biệt miền xác thực tính đúng đắn
của việc viết lại bằng cách kiểm tra xem chuỗi trừu tượng
có thể được cụ thể hóa để có được câu trả lời cuối cùng (nhãn cam).

tạo ra kiến thức cụ thể instance cho các
tham số của mô hình. Hơn nữa, việc tách biệt lý luận
tổng quát và kiến thức cụ thể miền cho phép
việc giải mã LLM tiến hành và chuyển đổi giữa các
mẫu khác nhau song song với việc gọi API (thông qua một
pipeline), tức là LLM có thể bắt đầu tạo ra chuỗi
trừu tượng tiếp theo trong khi công cụ điền vào chuỗi hiện tại,
điều này tăng tốc quá trình suy luận tổng thể.

Xây dựng Dữ liệu Tinh chỉnh Để xây dựng
dữ liệu chuỗi-của-trừu tượng (CoA) cho việc tinh chỉnh
LLM, chúng tôi thu thập các mẫu trả lời câu hỏi (QA)
từ các bộ dữ liệu QA nguồn mở hiện có (Cobbe
et al., 2021; Miao et al., 2020; Yang et al., 2018),
và nhắc LLaMa-70B (Touvron et al., 2023a)
viết lại câu trả lời của mỗi câu hỏi được lấy mẫu,
như được hiển thị trong Hình 2. Cụ thể, chúng tôi nhắc
LLaMa-70B gắn nhãn các đoạn trong câu trả lời vàng
tương ứng với các phép toán kiến thức (ví dụ: suy dẫn toán,
các tuyên bố dựa trên tham khảo Wikipedia) và sau đó
viết lại các câu với các đoạn được gắn nhãn thành
các vết CoA có thể điền, nơi kết quả
phép toán được thay thế bằng các trình giữ chỗ trừu tượng.
Ví dụ: hai suy dẫn trong ví dụ trong
Hình 2 được viết lại thành "[20 + 35 = y1]" và
"[90−y1 = y2]", tương ứng.

Lưu ý rằng kết quả phép toán kiến thức trung gian

--- TRANG 4 ---
có thể xuất hiện nhiều lần trong một câu trả lời, ví dụ:
trong Hình 2, kết quả 55 của phương trình đầu tiên được sử dụng trong
phương trình thứ hai. Chúng tôi nhắc LLaMa-70B
thay thế tất cả các lần xuất hiện của cùng một kết quả
trung gian bằng cùng một trình giữ chỗ, từ đó kết nối
rõ ràng các bước lý luận nhiều. Để đảm bảo
rằng dữ liệu được viết lại là chính xác, chúng tôi sử dụng các
công cụ chuyên biệt miền để xác minh tính đúng đắn của mỗi
vết lý luận CoA.4 Cụ thể, chúng tôi sử dụng
các công cụ để thực hiện các phép toán được gắn nhãn trong mỗi CoA,
và chỉ giữ lại các câu hỏi mà CoA của chúng có thể được điền
với kết quả hợp lệ bởi các công cụ.

4 Cài đặt Thí nghiệm
Chúng tôi tiến hành các thí nghiệm trên hai miền
đại diện: lý luận toán học và Wikipedia
(Wiki) QA, liên quan đến lý luận thông thường và logic
trên kiến thức mô tả sự kiện.

4.1 Lý luận Toán học
Đưa ra một câu hỏi toán, hệ thống QA cần
tạo ra một giải pháp ngôn ngữ tự nhiên cho bài toán
với các suy dẫn số học từng bước (như được
chứng minh trong cột bên trái của Hình 1). Chúng tôi giả định
rằng các suy dẫn liên quan trong giải pháp là
các phép toán kiến thức chuyên biệt cần thiết trong
miền này, được gắn nhãn trong dấu ngoặc vuông
với kết quả suy dẫn được thay thế bằng các
trình giữ chỗ trừu tượng, ví dụ: "[20 + 35 = y1]".

Bộ dữ liệu Chúng tôi xây dựng hầu hết dữ liệu CoA
tinh chỉnh bằng cách viết lại tập huấn luyện GSM8K (Cobbe et al.,
2021), chứa 7473 bài toán toán lớp tiểu học
đa dạng về ngôn ngữ. Vì bộ dữ liệu GSM8K tập trung vào lý luận
nhiều bước, nó thiếu phạm vi bao phủ các bài toán số học
một bước, vì vậy chúng tôi cũng viết lại một tập bổ sung
691 bài toán toán một bước từ bộ dữ liệu ASDiv (Miao et al.,
2020). Trên các bộ dữ liệu được viết lại này, chúng tôi
thấy rằng ~76.6% các vết lý luận CoA được tạo ra
bởi LLaMa-70B được xác minh bởi trình giải phương trình
của chúng tôi (được mô tả bên dưới). Bảng 1 cho thấy phân phối
bước lý luận (tức là số lượng suy dẫn) của
dữ liệu tinh chỉnh được xây dựng.

Để đánh giá trong phân phối, chúng tôi kiểm tra các
mô hình trên GSM8K và ASDiv, chứa 1319 và
2305 bài toán kiểm tra. Để tiếp tục kiểm tra khả năng
tổng quát hóa của các mô hình, chúng tôi cũng tiến hành đánh giá
zero-shot trên các bộ dữ liệu toán đại diện khác,
bao gồm SVAMP (Patel et al., 2021) và MAWPS

4Các triển khai chi tiết của việc xác minh chuỗi lý luận
được mô tả trong Sec. 4.1 và 4.2.

Nguồn | Bước Lý luận
       | 1 | 2 | 3 | 4 | 5 | >5 | Tất cả
GSM8K  | 8 | 1540 | 1648 | 1164 | 666 | 553 | 5579
ASDiv  | 677 | 0 | 0 | 0 | 0 | 0 | 677

Bảng 1: Phân phối bước lý luận của các chuỗi lý luận
được viết lại đúng trong miền toán học.

Câu hỏi | Đạo diễn của bộ phim hài lãng mạn "Big Stone Gap" có trụ sở tại thành phố New York nào?
Trả lời | Greenwich Village
Tham khảo | Big Stone Gap (phim) > Big Stone Gap là một bộ phim hài lãng mạn Mỹ năm 2014 do Adriana Trigiani đạo diễn.
Wikipedia | Adriana Trigiani > Adriana Trigiani là một đạo diễn phim người Mỹ gốc Ý có trụ sở tại Greenwich Village.
Vết CoA | Tìm [đạo diễn của bộ phim hài lãng mạn "Big Stone Gap" -Wiki-> y1]. Tên của đạo diễn bộ phim này là [y1 -NER(person)-> y2]. Sau đó xác định [y2 ở thành phố New York nào -Wiki-> y3].

Bảng 2: Ví dụ về xây dựng dữ liệu tinh chỉnh CoA
trong miền Wiki QA.

(Koncel-Kedziorski et al., 2016), chứa
1000 và 2065 mẫu kiểm tra, tương ứng.5

Công cụ Miền Chúng tôi sử dụng một trình giải phương trình để
thực hiện các suy dẫn số học cần thiết trong
miền toán. Trình giải phương trình của chúng tôi trước tiên trích xuất
các suy dẫn được gắn nhãn trong lý luận CoA, ví dụ:
"[20 + 35 = y1]" và "[90−y1 = y2]", và
kết hợp tất cả các suy dẫn thành một hệ
phương trình. Sau đó hệ phương trình được giải bằng
bộ công cụ SymPy,6 để có được giá trị thực của mỗi
biến (tức là giá trị của trình giữ chỗ trừu tượng).
Cuối cùng, trình giải phương trình của chúng tôi trả về chuỗi
lý luận đã được cụ thể hóa bằng cách thay thế tất cả các biến bằng
giá trị thực đã giải của chúng (bao gồm câu trả lời cuối cùng).

4.2 Wikipedia QA
Đưa ra một câu hỏi dựa trên kiến thức Wikipedia,
mô hình cần trước tiên xác định các bài viết Wikipedia
làm tham khảo liên quan đến câu hỏi, sau đó
lý luận trên kiến thức chính trong các bài viết tham khảo
để trả lời câu hỏi (như được hiển thị trong cột bên phải
của Hình 1). Chúng tôi giả định rằng phép toán
kiến thức chuyên biệt trong miền này là việc lấy
các bài viết Wikipedia liên quan và các thực thể được đặt tên
quan trọng, được viết lại thành các truy vấn tìm kiếm Wikipedia
(WikiSearch) và nhận dạng thực thể được đặt tên (NER)7. Bảng 2 cho thấy một ví dụ về
vết CoA được viết lại cho Wiki QA.

5Đối với benchmark MAWPS, chúng tôi kiểm tra trên 395, 508, 562
và 600 bài toán toán từ các phần AddSub, SingleEq, SingleOp
và MultiArith, tương ứng.
6https://www.sympy.org/en/index.html
7Chúng tôi sử dụng NER để trích xuất các thực thể từ bài viết mà kết nối
kết quả WikiSearch trước đó với các truy vấn WikiSearch sau đó.

--- TRANG 5 ---
Bộ dữ liệu Chúng tôi sử dụng bộ dữ liệu HotpotQA (Yang et al.,
2018) để xây dựng dữ liệu CoA tinh chỉnh trong
miền Wiki QA. HotpotQA chứa 113K
ví dụ QA đa hop, mỗi ví dụ được gắn nhãn với hai
bài viết Wikipedia cung cấp kiến thức hỗ trợ.
Trong số 90447 cặp QA huấn luyện, chúng tôi
xác định 72991 là cặp Bridge QA, nơi một thực thể
trung gian phải được xác định để liên kết câu trả lời
với câu hỏi, như được hiển thị trong Bảng 2. Phần còn lại
17456 là cặp Comparison QA, nơi các
thuộc tính của hai thực thể được so sánh, ví dụ: "Randal Kleiser và Kyle Schickner có cùng quốc tịch không?". Chúng tôi nhắc LLaMa-70B viết lại
các QA huấn luyện này thành CoA với các truy vấn WikiSearch và
NER, và xác minh mỗi CoA với các công
cụ miền của chúng tôi (được mô tả bên dưới), bằng cách kiểm tra xem
tất cả các bài viết được trả về bởi các truy vấn WikiSearch
có khớp với một trong các tiêu đề trong các bài viết vàng không. Cuối cùng,
8956 Bridge QA và 5405 Comparison QA được
sử dụng làm dữ liệu tinh chỉnh, mà các CoA được viết lại của chúng
vượt qua việc xác minh.8 Đối với Wiki QA, chúng tôi lưu ý rằng
ngoài việc huấn luyện một LLM để tạo ra dữ liệu CoA bằng
WikiSearch, chúng tôi cũng tinh chỉnh một LLM thứ hai để
học tạo ra câu trả lời vàng cuối cùng dựa trên một
vết lý luận CoA được cụ thể hóa đúng.

Chúng tôi đánh giá các mô hình trên tập phát triển HotpotQA,
chứa 5918 cặp Bridge QA và
1487 cặp Comparison QA. Tương tự như miền lý luận
toán học, chúng tôi cũng tiến hành đánh giá zero-shot
trên các bộ dữ liệu QA miền mở khác:
WebQuestions (WQ; Berant et al., 2013), Natural-
Questions (NQ; Kwiatkowski et al., 2019) và
TriviaQA (Joshi et al., 2017), chứa 2032,
3610 và 17944 câu hỏi kiểm tra, tương ứng.

Công cụ Miền Các công cụ chuyên biệt cần thiết
cho Wiki QA bao gồm một công cụ tìm kiếm Wikipedia
để lấy các bài viết tham khảo, và một bộ công cụ NER
để trích xuất các thực thể kết nối các truy vấn
tìm kiếm nhiều bước. Chúng tôi theo Toolformer (Schick et al.,
2023) và triển khai một công cụ tìm kiếm Wikipedia như
một trình lấy BM25 (Robertson et al., 1995; Baeza-
Yates et al., 1999) lập chỉ mục cho bản dump Wikipedia
từ benchmark KILT (Petroni et al., 2021).
Chúng tôi sử dụng trình lấy BM25 để tìm kiếm 10 bài viết
hàng đầu liên quan đến truy vấn đầu vào, sau đó sắp xếp lại
các bài viết dựa trên độ tương tự cosine embedding
Sentence-BERT (Reimers and Gurevych, 2019) của chúng với câu hỏi. Sau khi sắp xếp lại, bài viết
top-1 được chọn làm kết quả tìm kiếm cuối cùng.

Lớp Tổng quát | Các loại SpaCy NER được bao gồm trong mỗi Lớp Tổng quát
person | PERSON
group | NORP, ORG, LANGUAGE
location | GPE, FAC, LOC
culture | EVENT, WORK_OF_ART, LAW, PRODUCT
date | DATE, TIME
numeral | CARDINAL, PERCENT, MONEY, QUANTITY, ORDINAL

Bảng 3: Tập hợp các loại SpaCy NER.

Chúng tôi sử dụng SpaCy9 (en_core_web_sm) làm bộ công cụ
NER để trích xuất các thực thể được đặt tên. Để đơn giản hóa NER,
chúng tôi tập hợp nhiều loại SpaCy NER thành
6 lớp tổng quát, như được hiển thị trong Bảng 3. Nếu nhiều
thực thể được đặt tên được nhận dạng, chúng tôi đưa từng thực thể
được nhận dạng vào truy vấn WikiSearch tiếp theo,
và chọn thực thể mà kết quả tìm kiếm tiếp theo của nó
có độ tương tự cosine embedding Sentence-BERT cao nhất
với câu hỏi.

4.3 Baseline
Chúng tôi áp dụng phương pháp lý luận CoA của chúng tôi cho cả
mô hình LLaMa 7B và 70B, và kiểm tra các phiên bản
mô hình khác nhau bao gồm phiên bản đầu tiên của LLaMa (Touvron et al., 2023a) và LLaMa-2 và LLaMa-2-Chat
tiên tiến hơn (Touvron et al., 2023b). Chúng tôi
so sánh phương pháp của chúng tôi với một số baseline, bao gồm:
a) prompting few-shot sử dụng 8 ví dụ QA được lấy mẫu
ngẫu nhiên từ dữ liệu chuỗi-của-tư duy gốc (tức là không được
viết lại) (CoT-FSP), b) tinh chỉnh với dữ liệu chuỗi-của-tư duy
gốc (CoT-FT)10, và c) Toolformer (Schick et al., 2023)
tinh chỉnh LLM trên các văn bản CCNet (Wenzek et al.,
2020) được tăng cường với các lệnh gọi API. Để đánh giá
trên Wiki QA, chúng tôi cũng so sánh phương pháp của chúng tôi
với FireAct (Chen et al., 2023), tinh chỉnh
LLM trên các quỹ đạo ReAct (Yao et al., 2023b) HotpotQA
được chưng cất từ GPT-4 (OpenAI, 2023).

5 Kết quả và Phân tích
5.1 Lý luận Toán học
Bảng 4 cho thấy kết quả đánh giá cho các mô hình LLaMa-
2 và LLaMa-2-Chat.11 Trên các bộ dữ liệu GSM8K
và ASDiv, phương pháp CoA của chúng tôi vượt trội hơn
baseline few-shot CoT-FSP và baseline tinh chỉnh
thông thường CoT-FT, chứng minh rằng tinh chỉnh CoA
với tăng cường công cụ hiệu quả hơn trong việc thích ứng
LLM với các nhiệm vụ lý luận nhiều bước.
Tương tự, khi được đánh giá trên các bộ dữ liệu ngoài phân phối,
SVAMP và MAWPS, CoA cũng liên tục
vượt trội hơn các baseline. Thú vị là, đối với
các bộ dữ liệu ngoài phân phối này, CoT-FT tụt lại
xa hơn so với CoA, đặc biệt đối với các mô hình 7B, cho
thấy rằng lý luận CoA mang lại hiệu suất lý luận
vững chắc hơn về mặt phân phối.

Phương pháp CoA của chúng tôi cũng vượt trội hơn baseline
tăng cường công cụ Toolformer, điều này ngụ ý rằng
lập kế hoạch các biến trừu tượng trong CoA có thể cải thiện
độ chính xác của lý luận với công cụ. Tuy nhiên, vì
Toolformer ban đầu không được huấn luyện với dữ liệu tinh chỉnh
trong miền,12 chúng tôi cũng tinh chỉnh một phiên bản mới
của Toolformer với dữ liệu chuỗi-của-tư duy từ
GSM8K và ASDiv, được ký hiệu là Toolformer
- Math trong Bảng 4. Chúng tôi cũng quan sát thấy rằng CoA
hoạt động tốt hơn Toolformer - Math, xác nhận
rằng việc giới thiệu các biến trừu tượng cho phép
sử dụng công cụ vững chắc hơn so với việc tích hợp trực tiếp
các lệnh gọi API trong lý luận chuỗi-của-tư duy.

Nghiên cứu Ablation Chúng tôi xác minh rằng hiệu suất
tổng quát hóa vững chắc của phương pháp CoA không
chỉ đơn thuần được hưởng lợi từ việc sử dụng các công cụ bổ sung, bằng cách
tinh chỉnh một LLM khác để giải phương trình (từ
cùng một backbone mô hình), thay vì gọi trình giải
phương trình, được ký hiệu là CoA (no Tool) trong Bảng 4.

12Toolformer được tinh chỉnh trên dữ liệu CCNet, có thể không
chứa các mẫu lý luận toán học phong phú.

Chúng tôi thấy rằng CoA (no Tool) hoạt động kém hơn
CoA một cách nhất quán trên tất cả các bộ dữ liệu, xác nhận
rằng việc sử dụng các công cụ chuyên biệt cho phép các tác nhân
LLM tiến hành các phép toán chính xác hơn, thay vì trực tiếp
giải các phép toán tương tự. Tuy nhiên, CoA
(no Tool) vẫn vượt trội hơn tất cả các phương pháp baseline trên
việc tổng quát hóa zero-shot sang các bộ dữ liệu SVAMP và MAWPS,
ngụ ý rằng việc học các chuỗi lý luận trừu tượng cũng
góp phần vào sự vững chắc tốt hơn của CoA,
có lẽ do việc lập kế hoạch tốt hơn của nhiều bước
lý luận được lập chỉ mục bởi các biến trừu tượng.

Các Bước Lý luận Những phát hiện của chúng tôi cho thấy rằng
lợi ích của lý luận chuỗi-của-trừu tượng được phát huy
rõ rệt nhất khi các bài toán yêu cầu chuỗi lý luận dài
để giải quyết. Hình 3 cho thấy hiệu suất phân tầng
của ba mô hình trên GSM8K QA, tương đối với
số lượng bước lý luận trong các chuỗi lý luận được dự đoán
và vàng. So với CoT-FSP few-shot, CoA tạo ra
các chuỗi lý luận thường khớp với độ dài của
các chuỗi lý luận vàng, như được phản ánh bởi thống kê
heat-map (cột bên trái) được tập hợp nhiều hơn xung quanh
đường chéo (có thể so sánh với CoT-FT). Đồng thời,
chúng tôi quan sát thấy rằng các mô hình đạt được độ chính xác QA
tốt hơn khi số lượng bước lý luận trong các câu trả lời
được tạo ra của chúng được căn chỉnh với các tham khảo vàng
(tức là đường chéo của heat-map trong cột bên phải).
Kết quả trên cho thấy rằng các mô hình được tinh chỉnh tốt hơn
trong việc học tạo ra các chuỗi lý luận khớp
với chuỗi lý luận thực cho bài toán.

9https://spacy.io/models/en
10Lưu ý rằng trong miền Wiki QA, dữ liệu HotpotQA được sử dụng
để prompting hoặc tinh chỉnh baseline được tiền xử lý để
chứa cả các bài viết Wikipedia vàng (phục vụ như giải thích
chuỗi-của-tư duy) và câu trả lời cuối cùng.
11Chúng tôi bao gồm kết quả đánh giá tương tự cho mô hình
LLaMa gốc (7B) trong Phụ lục B.

--- TRANG 6 ---
Mô hình | Phương pháp | Sử dụng | GSM8K | ASDiv | SVAMP | MAWPS
        |            | Công cụ |       |       |       | AddSub | SingleEQ | SingleOp | MultiArith | Tất cả
LLaMa-2 | CoT-FSP    | ✗       | 16.38 | 47.85 | 38.40 | 52.41  | 63.39    | 82.03    | 43.33      | 60.53
-7B     | CoT-FT     |         | 35.33 | 57.18 | 48.20 | 66.08  | 74.41    | 85.23    | 65.00      | 73.03
        | Toolformer | ✓       | 17.59 | 48.55 | 37.10 | 47.34  | 58.46    | 79.54    | 50.67      | 59.81
        | CoA        |         | 37.83*| 57.61 | 51.70*| 72.15* | 82.48*   | 86.48*   | 73.17*     | 78.89*
LLaMa-2 | CoT-FSP    | ✗       | 24.03 | 54.14 | 51.30 | 71.90  | 72.44    | 85.41    | 74.00      | 76.32
-Chat-7B| CoT-FT     |         | 35.41 | 59.00 | 46.90 | 58.23  | 72.24    | 85.41    | 73.00      | 73.37
        | CoA (no Tool)|       | 35.03 | 58.79 | 51.50 | 68.10  | 74.21    | 86.48    | 77.67      | 77.38
        | Toolformer | ✓       | 23.65 | 50.85 | 48.80 | 61.01  | 69.09    | 81.85    | 68.50      | 70.85
        | Toolformer - Math|   | 36.01 | 59.18 | 47.60 | 58.99  | 72.44    | 85.94    | 75.50      | 74.43
        | CoA        |         | 38.29*| 59.57 | 54.20*| 72.41  | 81.89*   | 88.26*   | 83.00*     | 82.13*
LLaMa-2 | CoT-FSP    | ✗       | 56.18 | 65.94 | 70.60 | 86.08  | 89.17    | 92.88    | 84.50      | 88.23
-Chat-70B| CoT-FT    |         | 60.50 | 70.24 | 70.40 | 81.52  | 87.60    | 92.35    | 89.17      | 88.18
        | Toolformer | ✓       | 52.54 | 69.07 | 73.60 | 86.84  | 89.76    | 91.46    | 81.50      | 87.26
        | Toolformer - Math|   | 61.03 | 70.59 | 73.20 | 85.57  | 91.34    | 91.99    | 92.00      | 90.60
        | CoA        |         | 62.32*| 71.89*| 73.40 | 86.33  | 94.49*   | 93.06    | 92.33      | 91.91*

Bảng 4: Kết quả đánh giá trên LLaMa-2 và LLaMa-2-Chat cho lý luận toán học. "Tất cả" biểu thị kết quả trung bình trên bốn phần MAWPS. Tỷ lệ khớp chính xác với câu trả lời vàng cuối cùng (tức là độ chính xác) được báo cáo. Đối với mỗi mô hình cơ sở, kết quả tốt nhất và tốt thứ hai được in đậm và gạch chân, tương ứng. Kết quả tốt nhất được gắn nhãn * tốt hơn đáng kể so với kết quả tốt thứ hai tương ứng, với giá trị p kiểm tra ý nghĩa < 0.05.

Số lượng QA | CoT-FSP | CoT-FT | CoA | Độ chính xác (%)

Hình 3: Kết quả đánh giá GSM8K trên LLaMa-2-Chat-
7B w.r.t. số lượng bước lý luận trong chuỗi lý luận được dự đoán
và vàng. (Trái) Số lượng ví dụ kiểm tra thuộc về mỗi tầng. (Phải) Độ chính xác mô hình tương ứng (%) cho những ví dụ đó. Các ô không chéo với ít hơn 15 ví dụ bị bỏ qua.

Phương pháp | Tỷ lệ Lỗi
           | Số học | Lý luận
CoT-FSP    | 17.3   | 70.3
CoT-FT     | 25.2   | 67.8
CoA        | 0.0    | 60.4

Bảng 5: Kết quả đánh giá con người về tỷ lệ lỗi số học và lý luận trên 200 mẫu kiểm tra GSM8K. Các mô hình được phát triển dựa trên LLaMa-2-Chat-7B được trình bày.

Thú vị là, chúng tôi thấy rằng CoA, so với
CoT-FT, đạt được hiệu suất cao hơn đặc biệt trên các câu hỏi
yêu cầu nhiều bước lý luận hơn.
Trong cột bên phải của Hình 3, cải thiện của CoA so với
CoT-FT rõ rệt hơn trên các câu hỏi
với hơn 3 bước trong chuỗi lý luận vàng
(được làm nổi bật bằng ô vuông đỏ). Điều này cho thấy
rằng mô hình được huấn luyện với CoA có khả năng
lý luận chuỗi-của-tư duy dài vững chắc hơn,
được học từ việc lập kế hoạch với các trừu tượng.

Đánh giá Con người Để xác minh toàn diện hơn
rằng CoA cải thiện cả phép toán kiến thức (tức là số học
bằng cách sử dụng công cụ) và độ chính xác lý luận, chúng tôi tiến hành
một đánh giá con người trên các câu trả lời mô hình khác nhau cho 200 câu hỏi GSM8K được lấy mẫu ngẫu nhiên.

Bước Lý luận Vàng | 2 | 3 | 4 | 5 | >5
Thời gian Suy luận mỗi QA (giây) | 2.0 | 2.5 | 3.0 | 3.5 | 4.0 | 4.5 | 5.0

Phương pháp
Toolformer
CoT-FSP
CoA
CoT-FT

Hình 4: Thời gian suy luận wall-clock trên GSM8K (được khởi tạo với LLaMa-2-Chat-7B). Thời gian trung bình trả lời một câu hỏi được đo (tính bằng giây) w.r.t. số lượng bước lý luận vàng cần thiết cho câu hỏi.

Phương pháp | Độ chính xác
CoT-FSP     | 27.90
CoT-FT      | 39.12
Toolformer  | 24.56
Toolformer - Math | 35.25
CoA         | 40.79

Bảng 6: Kết quả đánh giá trên GSM8K với giải mã self-consistency (được khởi tạo với LLaMa-2-Chat-7B). Mỗi mô hình sử dụng bỏ phiếu đa số để tổng hợp câu trả lời của 16 chuỗi lý luận được lấy mẫu.

Cụ thể, đưa ra một câu hỏi GSM8K và câu trả lời của mô hình cho
câu hỏi, chúng tôi yêu cầu các công nhân con người đánh giá xem
câu trả lời có chứa bất kỳ lỗi số học nào (ví dụ: tính toán sai,
phương trình không hợp lệ) hoặc lỗi lý luận không liên quan
đến suy dẫn toán (ví dụ: hiểu sai câu hỏi, chiến lược
không phù hợp để giải quyết câu hỏi), và báo cáo tần suất
mô hình mắc hai loại lỗi này. Trong Bảng 5, chúng tôi
thấy rằng CoA hiệu quả giảm lỗi số học xuống
không, do việc sử dụng trình giải phương trình để thực hiện
các phép tính chính xác. Quan trọng hơn, phương pháp của chúng tôi
cũng mắc ít lỗi lý luận hơn so với các baseline,
xác minh rằng tinh chỉnh CoA hướng dẫn mô hình học
lý luận chính xác hơn thông qua việc lập kế hoạch toàn diện
các chuỗi lý luận trừu tượng. Ngược lại, tinh chỉnh thông thường
(tức là CoT-FT) tạo ra cải thiện lý luận hạn chế hơn
so với CoT-FSP few-shot, đồng thời cũng không thể
ngăn chặn lỗi số học.

--- TRANG 7 ---
Mô hình | Phương pháp | Sử dụng | HotpotQA | | | WQ | NQ | TriviaQA
        |            | Công cụ | Bridge | Comparison | Cả hai | Thời gian
LLaMa-2 | CoT-FSP    | ✗       | 11.69  | 45.46      | 18.47  | 2.074 | 34.65 | 30.91 | 53.48
-Chat-7B| CoT-FT     |         | 14.24  | 56.69      | 22.77  | 1.937 | 33.51 | 25.40 | 51.05
        | Toolformer | ✓       | 12.99  | 44.59      | 20.00  | 2.350 | 36.22 | 30.22 | 54.15
        | Toolformer - Wiki|   | 15.68  | 56.42      | 23.86  | 2.301 | 36.61 | 32.96 | 55.08
        | FireAct    |         | 19.18  | 54.14      | 26.20  | 2.706 | 36.02 | 35.87 | 52.96
        | CoA        |         | 21.00* | 56.96      | 28.22* | 1.896 | 35.97 | 38.67*| 57.90*
LLaMa-2 | CoT-FSP    | ✗       | 21.39  | 56.62      | 28.47  | 6.668 | 34.89 | 37.42 | 63.61
-Chat-70B| CoT-FT    |         | 23.84  | 63.95      | 31.90  | 6.401 | 34.15 | 39.75 | 62.28
        | Toolformer | ✓       | 22.24  | 56.09      | 29.04  | 6.888 | 37.16 | 40.42 | 64.31
        | Toolformer - Wiki|   | 26.38  | 63.82      | 33.90  | 6.855 | 37.70 | 41.25 | 66.64
        | CoA        |         | 27.61* | 64.09      | 34.94* | 6.369 | 36.37 | 43.57*| 69.08*

Bảng 7: Kết quả đánh giá Wiki QA trên các mô hình dựa trên LLaMa-2-Chat. "Cả hai" biểu thị kết quả đánh giá tổng thể trên cả phần bridge và comparison của HotpotQA. "Thời gian" biểu thị số giây trung bình mà mỗi tác nhân cần để trả lời một câu hỏi trong HotpotQA. Tỷ lệ khớp chính xác với câu trả lời vàng cuối cùng (tức là độ chính xác) được báo cáo. Đối với mỗi mô hình cơ sở, kết quả tốt nhất và tốt thứ hai được in đậm và gạch chân, tương ứng. Kết quả tốt nhất được gắn nhãn * tốt hơn đáng kể so với kết quả tốt thứ hai tương ứng, với giá trị p kiểm tra ý nghĩa < 0.05.

Hiệu quả Suy luận Quan trọng là, chúng tôi thấy rằng
lợi ích hiệu suất của lý luận CoA không đi kèm
với chi phí tính toán tăng. Trong Hình 4, chúng tôi cho thấy
thời gian trung bình (giây) mà các tác nhân CoA và baseline
(được khởi tạo với LLaMa-2-Chat-7B) cần để trả lời một câu hỏi
w.r.t. các bước lý luận vàng cần thiết. So với các baseline CoT,
CoA cần ít thời gian hơn baseline few-shot CoT-FSP,
mà việc tạo ra cần được điều kiện hóa trên các ví dụ bổ sung.
Tuy nhiên, CoA kém hiệu quả suy luận hơn một chút so với
CoT-FT, có thể do việc giải mã các token bổ sung (ví dụ: "[" và "]")
cho các tuyên bố trừu tượng.

So với Toolformer, CoA có đường cong thời gian suy luận
thấp hơn và phẳng hơn, cho thấy khả năng mở rộng tốt hơn khi
số lượng bước lý luận tăng.
Sự khác biệt này phát sinh vì CoA tách biệt
việc tạo ra các chuỗi lý luận (trừu tượng) khỏi việc
lấy kiến thức (tức là sử dụng công cụ), cho phép
toàn bộ chuỗi lý luận được giải mã trước khi bất kỳ công cụ nào
được gọi. Quy trình này giảm thiểu chi phí suy luận theo hai cách.
Thứ nhất, các lệnh gọi công cụ được thực hiện sau khi vết CoA
được giải mã, cho phép các lệnh gọi công cụ song song
cho cùng một vết (ví dụ: sử dụng trình giải phương trình một lần
thay vì nhiều lệnh gọi đến máy tính), và tránh
độ trễ thời gian do chờ đợi phản hồi API bên ngoài.
Do đó, mô hình được tinh chỉnh với CoA hiệu quả hơn
trong lý luận nhiều bước, đặc biệt khi số lượng bước
lý luận (tức là lệnh gọi công cụ) tăng. Thứ hai, trên
nhiều ví dụ, mô hình có thể tạo ra vết CoA
của ví dụ tiếp theo trong khi các lệnh gọi công cụ được thực hiện
cho ví dụ trước đó, song song hóa việc giải mã CoA
và lệnh gọi công cụ trên các ví dụ.

Giải mã Self-Consistency Ngoài việc giải mã tham lam,
chúng tôi cũng kiểm tra chiến lược suy luận tiến bộ hơn,
tức là giải mã self-consistency (Wang et al., 2022),
trên phương pháp lý luận CoA của chúng tôi. Chúng tôi kiểm tra
tất cả các phương pháp trên bộ dữ liệu GSM8K được khởi tạo với
LLaMa-2-Chat-7B. Mỗi phương pháp lấy mẫu 16 chuỗi
lý luận và sử dụng bỏ phiếu đa số để tổng hợp
16 câu trả lời được suy ra bởi các chuỗi lý luận,
để có được câu trả lời cuối cùng. Đối với các siêu tham số
của việc lấy mẫu, chúng tôi đặt temperature, top-k và
top-p là 1.0, 40 và 0.5, tương ứng. Bảng 6
cho thấy kết quả đánh giá của chúng tôi. Chúng tôi thấy rằng phương pháp CoA
của chúng tôi liên tục vượt trội hơn tất cả các phương pháp baseline
khi chuyển từ giải mã tham lam sang giải mã
self-consistency. Điều này cho thấy rằng phương pháp của chúng tôi
cũng có tiềm năng tốt hơn để được tổng quát hóa sang
các lược đồ giải mã LLM khác nhau.

5.2 Wiki QA
Bảng 7 cho thấy kết quả Wiki QA của chúng tôi sử dụng các mô hình
LLaMa-2-Chat.13 Tương tự như lý luận toán học, chúng tôi
tinh chỉnh một phiên bản mới của Toolformer với
dữ liệu chuỗi-của-tư duy trong miền từ HotpotQA,
được ký hiệu là Toolformer - Wiki. Trên HotpotQA,
CoA đạt được tỷ lệ khớp chính xác cao hơn với
tham khảo vàng so với các baseline few-shot hoặc tinh chỉnh.
Cụ thể, CoA vượt trội hơn tất cả các baseline trên
các QA kiểu bridge thách thức hơn, nơi hai bước lý luận
trên kiến thức Wikipedia được kết nối liên tiếp,
tức là không thể được thực hiện độc lập song song như trong
các QA kiểu comparison. So với tinh chỉnh FireAct,
CoA cũng đạt được hiệu suất tốt hơn trên cả bridge và
comparison QA, mà không cần dữ liệu được chưng cất
từ GPT-4 nguồn đóng.

Như với lý luận toán học, các tác nhân CoA
cũng thực hiện suy luận hiệu quả hơn so với các tác nhân
Toolformer và FireAct khi trả lời các câu hỏi HotpotQA.
Chúng tôi cũng thấy rằng CoA hiệu quả hơn (cột Thời gian)
so với cả CoT-FSP và CoT-FT, vì CoA không cần
ví dụ few-shot làm đầu vào bổ sung và không cần tạo ra
các bài viết Wiki dài, thay vào đó được cung cấp bởi
công cụ tìm kiếm. Cuối cùng, CoA cải thiện so với
các phương pháp baseline trong các thí nghiệm tổng quát hóa zero-shot
trên các bộ dữ liệu Wiki QA khác, vượt trội hơn
tất cả các baseline trên NaturalQuestions và TriviaQA,
và bằng với các baseline tốt nhất trên WebQuestions.

6 Kết luận
Trong công trình này, chúng tôi đề xuất tách biệt lý luận
tổng quát của các tác nhân LLM khỏi kiến thức chuyên biệt
thu được thông qua các công cụ bên ngoài. Phương pháp của chúng tôi,
chuỗi-của-trừu tượng (CoA), khuyến khích LLM
học lập kế hoạch lý luận trừu tượng nhiều bước,
vững chắc hơn trước những thay đổi kiến thức ngoài phân phối.
CoA cũng đạt được một pipeline hiệu quả hơn
cho việc sử dụng công cụ cải thiện đáng kể tốc độ
của lý luận nhiều bước tăng cường công cụ.
Các triển khai đơn giản nhưng hiệu quả của phương pháp
chúng tôi trên hai nhiệm vụ đa dạng (tức là lý luận toán
và QA miền mở) chứng minh tiềm năng của nó
để được thích ứng với các tình huống lý luận mới.

Hạn chế
Chúng tôi thừa nhận một số hạn chế trong công trình của chúng tôi.
Thứ nhất, các bộ dữ liệu được sử dụng để kiểm tra phương pháp của chúng tôi không thể
có phạm vi bao phủ toàn diện tất cả các tình huống lý luận thế giới thực.
Thay vào đó, chúng tôi xem xét hai miền lý luận
đại diện, tức là lý luận toán học
và QA miền mở (Wikipedia) tổng quát, và sử dụng tiếng Anh
làm ngôn ngữ chính trong việc kiểm tra. Hơn nữa, phương pháp của chúng tôi
được kiểm tra trên cài đặt tinh chỉnh toàn bộ LLM,
đòi hỏi tài nguyên tính toán đáng kể, trong khi các
lược đồ huấn luyện mô hình hiệu quả hơn, ví dụ: LoRA (Hu
et al., 2021), có thể được áp dụng trong công việc tương lai.

Lời cám ơn
Chúng tôi cảm ơn Beatriz Borges, Gail Weiss, Syrielle Montariol, Li Mi và Zeming Chen đã đọc và cung cấp
nhận xét về các bản thảo của bài báo này. Antoine
Bosselut biết ơn sự hỗ trợ của Quỹ Khoa học Quốc gia Thụy Sĩ (No. 215390),
Innosuisse (PFFS-21-29), Quỹ Khoa học EPFL,
Trung tâm Hình ảnh EPFL, Sony Group
Corporation, và Viện Allen cho AI.

Tài liệu tham khảo
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305.10403.

Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999.
Modern information retrieval, volume 463. ACM
press New York.

Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the 2013
conference on empirical methods in natural language
processing, pages 1533–1544.

Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,
and Denny Zhou. 2023. Large language models as
tool makers. arXiv preprint arXiv:2305.17126.

Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier,
Karthik Narasimhan, and Shunyu Yao. 2023. Fireact:
Toward language agent fine-tuning. arXiv preprint
arXiv:2310.05915.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374.

Wenhu Chen, Xueguang Ma, Xinyi Wang, and
William W Cohen. 2022. Program of thoughts
prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint
arXiv:2211.12588.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168.

--- TRANG 10 ---
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language
models. In International Conference on Machine
Learning, pages 10764–10799. PMLR.

Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan,
Shuyuan Xu, and Yongfeng Zhang. 2023. Openagi:
When llm meets domain experts. arXiv preprint
arXiv:2304.04370.

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong
Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.
2023. Critic: Large language models can self-correct
with tool-interactive critiquing. arXiv preprint
arXiv:2305.11738.

Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong,
Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023a.
Reasoning with language model is planning with
world model. arXiv preprint arXiv:2305.14992.

Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting
Hu. 2023b. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings.
arXiv preprint arXiv:2305.11554.

Joy He-Yueya, Gabriel Poesia, Rose E Wang, and
Noah D Goodman. 2023. Solving math word problems by combining language models with symbolic
solvers. arXiv preprint arXiv:2304.09102.

Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685.

Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan
Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan,
Neil Zhenqiang Gong, et al. 2023. Metatool benchmark for large language models: Deciding whether
to use tools and which to use. arXiv preprint
arXiv:2310.03128.

Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee
Aharoni, Bernd Bohnet, and Mor Geva. 2023. A
comprehensive evaluation of tool-assisted generation
strategies. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13856–
13878.

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55:1–38.

Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu.
2023. Genegpt: Augmenting large language models
with domain tools for improved access to biomedical
information. Preprint, arXiv:2304.09667.

Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611.

Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate
Kushman, and Hannaneh Hajishirzi. 2016. Mawps:
A math word problem repository. In Proceedings of
the 2016 conference of the north american chapter of
the association for computational linguistics: human
language technologies, pages 1152–1157.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics, 7:452–
466.

Yuliang Liu, Xiangru Tang, Zefan Cai, Junjie Lu,
Yichi Zhang, Yanjun Shao, Zexuan Deng, Helan
Hu, Zengxian Yang, Kaikai An, et al. 2023. Mlbench: Large language models leverage open-source
libraries for machine learning tasks. arXiv preprint
arXiv:2311.09835.

Ilya Loshchilov and Frank Hutter. 2018. Decoupled
weight decay regularization. In International Conference on Learning Representations.

Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, KaiWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv
preprint arXiv:2304.09842.

Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 1906–1919.

Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020. A diverse corpus for evaluating and developing
english math word problem solvers. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 975–984.

OpenAI. 2023. Gpt-4 technical report. Preprint,
arXiv:2303.08774.

Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm:
Tool augmented language models. arXiv preprint
arXiv:2205.12255.

Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are nlp models really able to solve simple
math word problems? In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2080–2094.

Shishir G Patil, Tianjun Zhang, Xin Wang, and
Joseph E Gonzalez. 2023. Gorilla: Large language
model connected with massive apis. arXiv preprint
arXiv:2305.15334.

--- TRANG 11 ---
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James Thorne,
Yacine Jernite, Vladimir Karpukhin, Jean Maillard,
et al. 2021. Kilt: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2523–2544.

Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
Sentence embeddings using siamese bert-networks.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pages 3982–3992.

Stephen E Robertson, Steve Walker, Susan Jones,
Micheline M Hancock-Beaulieu, Mike Gatford, et al.
1995. Okapi at trec-3. Nist Special Publication Sp,
109:109.

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
arXiv preprint arXiv:2302.04761.

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,
Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in
huggingface. arXiv preprint arXiv:2303.17580.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, et al. 2023b. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,
Ed H Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. 2022. Self-consistency improves
chain of thought reasoning in language models. In
The Eleventh International Conference on Learning
Representations.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural
Information Processing Systems, 35:24824–24837.

Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Édouard Grave. 2020. Ccnet: Extracting high quality monolingual datasets from web
crawl data. In Proceedings of the Twelfth Language
Resources and Evaluation Conference, pages 4003–
4012.

Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu,
Zhengyu Chen, and Jian Zhang. 2023. On the tool
manipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset for
diverse, explainable multi-hop question answering.
In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing. Association for Computational Linguistics.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L Griffiths, Yuan Cao, and Karthik
Narasimhan. 2023a. Tree of thoughts: Deliberate
problem solving with large language models. arXiv
preprint arXiv:2305.10601.

Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2023b.
React: Synergizing reasoning and acting in language
models. Preprint, arXiv:2210.03629.

Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra,
Victor Bursztyn, Ryan A Rossi, Somdeb Sarkhel,
and Chao Zhang. 2023. Toolchain*: Efficient action
space navigation in large language models with a*
search. arXiv preprint arXiv:2310.13227.

A Chi tiết Triển khai
Chi tiết Đánh giá Đối với đánh giá lý luận toán học,
chúng tôi trích xuất số cuối cùng xuất hiện trong
câu trả lời của mỗi mô hình, và kiểm tra xem số đó
có khớp chính xác với tham khảo vàng không. Độ chính xác
được báo cáo như tỷ lệ khớp chính xác trên
tất cả QA trong một tập kiểm tra. Đối với đánh giá Wiki QA, tương tự
như lý luận toán học, chúng tôi trích xuất câu trả lời cuối cùng
của mỗi mô hình và tính tỷ lệ khớp chính xác của nó
với tham khảo vàng. Cụ thể, câu trả lời cuối cùng
được cho là các từ sau "Action: finish[" đối với baseline FireAct,
và các từ sau "The answer is " đối với các mô hình khác. 8 ví dụ
trong miền được sử dụng cho baseline CoT-FSP được hiển thị
trong Bảng 14 và 15, cho phép mô hình cung cấp
câu trả lời với định dạng cần thiết cho đánh giá,
tức là nêu câu trả lời cuối cùng sau "The answer is ".

Đánh giá con người của chúng tôi trên GSM8K được thực hiện bởi
5 chuyên gia miền nội bộ từ nhóm nghiên cứu của chúng tôi.
Đối với mỗi câu hỏi toán, chúng tôi cung cấp cho các chuyên gia

--- TRANG 12 ---
Mô hình | Phương pháp | GSM8K | ASDiv | SVAMP | MAWPS
        |            |       |       |       | AddSub | SingleEQ | SingleOp | MultiArith | Tất cả
LLaMa-7B| CoT-FSP    | 11.90 | 44.69 | 31.80 | 56.20  | 59.65    | 70.28    | 43.00      | 57.05
        | CoT-FT     | 30.71 | 53.19 | 42.30 | 55.70  | 69.09    | 77.05    | 54.17      | 64.36
        | CoA        | 35.71 | 56.36 | 51.10 | 67.59  | 80.51    | 85.94    | 68.33      | 75.98
LLaMa-2-7B| CoT-FSP  | 16.38 | 47.85 | 38.40 | 52.41  | 63.39    | 82.03    | 43.33      | 60.53
        | CoT-FT     | 35.33 | 57.18 | 48.20 | 66.08  | 74.41    | 85.23    | 65.00      | 73.03
        | Toolformer | 17.59 | 48.55 | 37.10 | 47.34  | 58.46    | 79.54    | 50.67      | 59.81
        | CoA        | 37.83 | 57.61 | 51.70 | 72.15  | 82.48    | 86.48    | 73.17      | 78.89
LLaMa-2-Chat-7B| CoT-FSP | 24.03 | 54.14 | 51.30 | 71.90 | 72.44 | 85.41 | 74.00 | 76.32
        | CoT-FT     | 35.41 | 59.00 | 46.90 | 58.23  | 72.24    | 85.41    | 73.00      | 73.37
        | CoT-FT (no ASDiv) | 36.19 | 44.93 | 35.30 | 38.48 | 52.95 | 61.21 | 77.67 | 59.61
        | Toolformer | 23.65 | 50.85 | 48.80 | 61.01  | 69.09    | 81.85    | 68.50      | 70.85
        | Toolformer - Math | 36.01 | 59.18 | 47.60 | 58.99 | 72.44 | 85.94 | 75.50 | 74.43
        | CoA        | 38.29 | 59.57 | 54.20 | 72.41  | 81.89    | 88.26    | 83.00      | 82.13
        | CoA (no ASDiv) | 39.73 | 54.19 | 44.40 | 54.18 | 73.62 | 73.49 | 85.33 | 73.27
        | CoA (no Tool) | 35.03 | 58.79 | 51.50 | 68.10 | 74.21 | 86.48 | 77.67 | 77.38
LLaMa-2-Chat-70B| CoT-FSP | 56.18 | 65.94 | 70.60 | 86.08 | 89.17 | 92.88 | 84.50 | 88.23
        | CoT-FT     | 60.50 | 70.24 | 70.40 | 81.52  | 87.60    | 92.35    | 89.17      | 88.18
        | Toolformer | 52.54 | 69.07 | 73.60 | 86.84  | 89.76    | 91.46    | 81.50      | 87.26
        | Toolformer - Math | 61.03 | 70.59 | 73.20 | 85.57 | 91.34 | 91.99 | 92.00 | 90.60
        | CoA        | 62.32 | 71.89 | 73.40 | 86.33  | 94.49    | 93.06    | 92.33      | 91.91
GPT-J   | Toolformer | - | 40.4 | 29.4 | - | - | - | - | 44.0

Bảng 8: Kết quả đánh giá lý luận toán học.

câu trả lời vàng làm tham khảo, và yêu cầu họ đánh giá
mỗi câu trả lời mô hình theo cách ẩn danh,
tức là các chuyên gia không biết câu trả lời nào đến từ mô hình nào.
Hai câu hỏi yes-or-no được hỏi
để đánh giá mỗi câu trả lời mô hình, bao gồm: a)
liệu câu trả lời có bất kỳ lỗi số học nào không, và
b) liệu câu trả lời có bất kỳ lỗi lý luận nào không, và
các lựa chọn nhị phân từ các chuyên gia được thu thập để
tính tỷ lệ lỗi của việc tạo ra mỗi mô hình.
Chúng tôi trình bày hướng dẫn chi tiết cho đánh giá con người
trong Hình 5. Giao thức thu thập dữ liệu của chúng tôi được
tổ chức của chúng tôi phê duyệt về mặt đạo đức.

Huấn luyện Mô hình Chúng tôi tinh chỉnh các mô hình của chúng tôi với
batch size 8 và learning rate 2e−5 và 1e−5 cho
kích thước mô hình 7B và 70B, tương ứng, sử dụng bộ lập lịch
learning rate cosine với warm-up step 10. Chúng tôi
sử dụng optimizer AdamW (Loshchilov and Hutter, 2018)
cho tất cả các thí nghiệm tinh chỉnh, với β1,
β2 và ϵ được đặt là 0.9, 0.95 và 1e−8, tương ứng.
Weight decay huấn luyện được đặt là 0.1. Đối với lý luận
toán học, chúng tôi sử dụng tổng cộng 400 bước huấn luyện,
và có được các checkpoint mô hình tốt nhất (với
điểm validation cao nhất) tại bước 240 và 200 cho
kích thước mô hình 7B và 70B. Đối với miền Wiki QA, chúng tôi
điều chỉnh tổng số bước huấn luyện thành 500, và có được
các checkpoint tốt nhất tại bước 450 và 300 cho mô hình
7B và 70B. Do đó, chỉ cần ~2K và ~3K QA
trong thực tế để tinh chỉnh các mô hình của chúng tôi
trong miền toán và Wiki QA. Việc huấn luyện
các mô hình 7B và 70B của chúng tôi dựa trên 8 và 64
GPU NVIDIA A100-SXM4 (80GB), với thời gian huấn luyện
khoảng 2 và 5 giờ mỗi mô hình, tương ứng.

B Kết quả Thí nghiệm Đầy đủ
Bảng 8 và 9 cho thấy kết quả đầy đủ của các thí nghiệm
của chúng tôi trên miền toán và Wiki QA. Phương pháp CoA
của chúng tôi đạt được cải thiện nhất quán so với
các baseline trên các phiên bản mô hình LLaMa khác nhau
(LLaMa, LLaMa-2 và LLaMa-2-Chat), kích thước
mô hình (7B và 70B), và các benchmark miền. Điều này
cho thấy tiềm năng lớn của phương pháp chúng tôi được tổng quát hóa
sang các backbone mô hình mới và các nhiệm vụ lý luận.
Chúng tôi cũng trình bày kết quả trên các tập con GSM8K theo
số lượng bước lý luận vàng khác nhau trong
Bảng 10, nơi chúng tôi xác nhận rằng CoA có độ chính xác
lý luận chuỗi-của-tư duy dài vững chắc hơn.

Cân bằng Dữ liệu Tinh chỉnh Trong miền lý luận
toán học, chúng tôi cũng xác thực tầm quan trọng của việc sử dụng
dữ liệu tinh chỉnh được cân bằng trên các bước lý luận
khác nhau. Cụ thể, chúng tôi tiến hành một nghiên cứu ablation
trên CoT-FT và CoA được khởi tạo với mô hình
LLaMa-2-Chat-7B, bằng cách loại bỏ các mẫu QA một bước
của ASDiv khỏi dữ liệu tinh chỉnh (no ASDiv). Chúng tôi
thấy rằng CoT-FT (no ASDiv) và CoA (no ASDiv) có xu hướng
thiên về lý luận nhiều bước, nơi chúng đạt được
hiệu suất tốt hơn trên GSM8K và MultiArith chứa chủ yếu
các QA nhiều bước, nhưng gặp khó khăn với sự suy giảm hiệu suất
nghiêm trọng trên các bộ dữ liệu khác chứa nhiều bài toán
toán một bước. Điều này chứng minh rằng duy trì sự cân bằng
tốt của dữ liệu lý luận một bước và nhiều bước quan trọng
để thích ứng LLM trở thành những người lý luận vững chắc.

Thêm Baseline Prompting Chúng tôi cũng so sánh
phương pháp lý luận CoA của chúng tôi với các phương pháp
dựa trên prompting khác PAL (Gao et al., 2023) và
DECLARATIVE (He-Yueya et al., 2023), sử dụng
các minh họa mã few-shot để nhắc các giải pháp toán
như các chương trình Python hoặc khai báo.
Bảng 11 cho thấy kết quả so sánh của chúng tôi trên
bộ dữ liệu GSM8K, nơi tất cả các phương pháp được khởi tạo
với LLaMa-2-Chat-7B. Không được khởi tạo với các
mô hình mã hóa chuyên dụng (ví dụ: code-davinci-002), PAL
và DECLARATIVE có độ chính xác thấp hơn nhiều trên
GSM8K, hoạt động kém đáng kể so với phương pháp CoA
của chúng tôi, và thậm chí CoT-FSP thông thường.

Ngược lại, phương pháp CoA của chúng tôi ít phụ thuộc vào
các minh họa nhân tạo và sự gần gũi phân phối
của LLM khởi tạo với các nhiệm vụ mục tiêu, vì CoA tinh chỉnh
tác nhân LLM trên các chuỗi lý luận trừu tượng được xác định trước,
thu được từ việc viết lại đơn giản các vết lý luận
ngôn ngữ tự nhiên. Do đó, CoA linh hoạt trong các
định dạng tạo ra khác nhau, ví dụ: mã và văn bản thuần túy,
và tổng quát hóa tốt từ lý luận toán học
sang QA miền mở, đây là một loại nhiệm vụ lý luận
rất khác. Điều này cho thấy khả năng tổng quát hóa của phương pháp
chúng tôi sang các lược đồ lý luận mới cần thiết bởi một miền mới.

Phương pháp | Bước Lý luận Vàng
           | ≤2 | 3 | 4 | 5 | >5
CoT-FSP    | 42.9 | 26.3 | 18.0 | 10.9 | 3.6
CoT-FT     | 55.5 | 42.6 | 25.8 | 19.0 | 10.8
CoA        | 55.8 | 44.4 | 32.5 | 25.3 | 15.1
           | +0.3 | +1.8 | +6.7 | +6.3 | +4.3

Bảng 10: Kết quả đánh giá LLaMa-2-Chat-7B phân tầng
trên GSM8K với các bước lý luận vàng khác nhau.
Hàng cuối báo cáo cải thiện độ chính xác tuyệt đối của
phương pháp CoA so với baseline CoT-FT.

Phương pháp | Độ chính xác
CoT-FSP     | 24.03
PAL         | 20.55
DECLARATIVE | 9.86
CoA         | 38.29

Bảng 11: So sánh CoA với các phương pháp dựa trên prompting
trên GSM8K, được khởi tạo với LLaMa-2-Chat-7B.

--- TRANG 13 ---
Mô hình | Phương pháp | HotpotQA | | | WebQ. | NaturalQ. | TriviaQA
        |            | Bridge | Comparison | Tất cả |
LLaMa-2-7B| CoT-FSP  | 14.43 | 45.26 | 20.62 | 33.96 | 33.35 | 56.95
        | CoT-FT     | 14.85 | 57.36 | 23.39 | 31.50 | 26.93 | 52.32
        | Toolformer | 14.12 | 42.76 | 20.35 | 37.11 | 34.49 | 57.79
        | CoA        | 22.00 | 57.43 | 29.12 | 34.60 | 38.28 | 58.28
LLaMa-2-Chat-7B| CoT-FSP | 11.69 | 45.46 | 18.47 | 34.65 | 30.91 | 53.48
        | CoT-FT     | 14.24 | 56.69 | 22.77 | 33.51 | 25.40 | 51.05
        | Toolformer | 12.99 | 44.59 | 20.00 | 36.22 | 30.22 | 54.15
        | Toolformer - Wiki | 15.68 | 56.42 | 23.86 | 36.61 | 32.96 | 55.08
        | FireAct    | 19.18 | 54.14 | 26.20 | 36.02 | 35.87 | 52.96
        | CoA        | 21.00 | 56.96 | 28.22 | 35.97 | 38.67 | 57.90
LLaMa-2-Chat-70B| CoT-FSP | 21.39 | 56.62 | 28.47 | 34.89 | 37.42 | 63.61
        | CoT-FT     | 23.84 | 63.95 | 31.90 | 34.15 | 39.75 | 62.28
        | Toolformer | 22.24 | 56.09 | 29.04 | 37.16 | 40.42 | 64.31
        | Toolformer - Wiki | 26.38 | 63.82 | 33.90 | 37.70 | 41.25 | 66.64
        | CoA        | 27.61 | 64.09 | 34.94 | 36.37 | 43.57 | 69.08
GPT-J   | Toolformer | - | - | - | 26.3 | 17.7 | 48.8

Bảng 9: Kết quả đánh giá Wiki QA.

C Chi tiết Viết lại Dữ liệu Tinh chỉnh
Bảng 12 và 13 cho thấy các ví dụ prompting cho việc
xây dựng dữ liệu tinh chỉnh của phương pháp chúng tôi. Chúng tôi
nhắc LLaMa-70B viết lại các QA toán và Wiki
hiện có thành các chuỗi lý luận trừu tượng, loại bỏ
việc chưng cất dữ liệu từ LLM nguồn đóng,
nhưng vẫn thu được nguồn dữ liệu cho phép học
lý luận nhiều bước hiệu quả hơn.

--- TRANG 14 ---
Q: Có 15 cây trong lùm cây. Các công nhân lùm cây sẽ trồng cây trong lùm cây hôm nay. Sau khi họ hoàn thành, sẽ có 21 cây. Các công nhân lùm cây sẽ trồng bao nhiêu cây hôm nay?
A: Ban đầu có 15 cây. Sau đó có 21 cây sau khi trồng thêm một số cây. Vậy phải có 21-15=6. Câu trả lời là 6.
C: Ban đầu có 15 cây. Sau đó có 21 cây sau khi trồng thêm một số cây. Vậy phải có [21 - 15 = y1]. Câu trả lời là y1.

Q: Bông hoa có giá $9, chậu đất sét có giá nhiều hơn bông hoa $20, và túi đất có giá ít hơn bông hoa $2. Chi phí trồng hoa là bao nhiêu?
A: Chậu đất sét có giá $20 + $9 = $29. Túi đất có giá $9 - $2 = $7. Chi phí trồng hoa là $9 + $29 + $7 = $45. Câu trả lời là 45.
C: Chậu đất sét có giá [20 + 9 = y1]. Túi đất có giá [9 - 2 = y2]. Chi phí trồng hoa là [9 + y1 + y2 = y3]. Câu trả lời là y3.

Q: Từ tháng 3 đến tháng 8, Sam kiếm được $460 khi làm 23 giờ công việc sân vườn. Tuy nhiên, từ tháng 9 đến tháng 2, Sam chỉ có thể làm việc 8 giờ. Nếu Sam đang tiết kiệm để mua một máy chơi game video có giá $600 và đã chi $340 để sửa xe, anh ấy cần làm việc thêm bao nhiêu giờ nữa trước khi có thể mua máy chơi game video?
A: Sam kiếm được $460 / 23 giờ = $20/giờ. Từ tháng 9 đến tháng 2, Sam kiếm được 8 giờ x $20/giờ = $160. Từ tháng 3 đến tháng 2, Sam kiếm được tổng cộng $460 + $160 = $620. Sau khi sửa xe, anh ấy còn lại $620 - $340 = $280. Sam cần thêm $600 - $280 = $320. Sam cần làm việc thêm $320 / $20/giờ = 16 giờ. Câu trả lời là 16.
C: Sam kiếm được [460 / 23 = y1] đô la mỗi giờ. Từ tháng 9 đến tháng 2, Sam kiếm được [8 * y1 = y2] đô la. Từ tháng 3 đến tháng 2, Sam kiếm được tổng cộng [460 + y2 = y3] đô la. Sau khi sửa xe, anh ấy còn lại [y3 - 340 = y4]. Sam cần thêm [600 - y4 = y5] đô la. Sam cần làm việc thêm [y5 / y1 = y6] giờ. Câu trả lời là y6.

Q: Có chín máy tính trong phòng máy chủ. Năm máy tính nữa được lắp đặt mỗi ngày, từ thứ Hai đến thứ Năm. Bây giờ có bao nhiêu máy tính trong phòng máy chủ?
A: Ban đầu có 9 máy tính. Trong mỗi ngày trong 4 ngày, 5 máy tính nữa được thêm vào. Vậy 5 * 4 = 20 máy tính được thêm vào. 9 + 20 là 29. Câu trả lời là 29.
C: Ban đầu có 9 máy tính. Trong mỗi ngày trong 4 ngày, 5 máy tính nữa được thêm vào. Vậy [5 * 4 = y1] máy tính được thêm vào. [9 + y1 = y2]. Câu trả lời là y2.

Q: Trong số 90 người trên xe buýt của William, 3/5 là người Hà Lan. Trong số 1/2 số người Hà Lan cũng là người Mỹ, 1/3 có chỗ ngồi cạnh cửa sổ. Số người Hà Lan gốc Mỹ ngồi cạnh cửa sổ là bao nhiêu?
A: Trên xe buýt, số người Hà Lan là 3/5 tổng số người, tổng cộng 3/5 x 90 = 54 người. Trong số 54 người Hà Lan, 1/2 là người Hà Lan gốc Mỹ, tổng cộng 1/2 x 54 = 27 người. Nếu 1/3 số hành khách trên xe buýt tự nhận là người Hà Lan gốc Mỹ ngồi cạnh cửa sổ, số lượng của họ là 1/3 x 27 = 9. Câu trả lời là 9.
C: Trên xe buýt, số người Hà Lan là 3/5 tổng số người, tổng cộng [3/5 * 90 = y1] người. Trong số người Hà Lan, 1/2 là người Hà Lan gốc Mỹ, tổng cộng [1/2 * y1 = y2] người. Nếu 1/3 số hành khách trên xe buýt tự nhận là người Hà Lan gốc Mỹ ngồi cạnh cửa sổ, số lượng của họ là [1/3 * y2 = y3]. Câu trả lời là y3.

Bảng 12: Ví dụ prompting cho việc xây dựng dữ liệu tinh chỉnh trong miền lý luận toán học. Đưa ra một câu hỏi (Q) và một câu trả lời vàng (A), LLaMa-70B được nhắc tạo ra việc viết lại câu trả lời thành chuỗi lý luận trừu tượng (C). Dựa trên đó, phương pháp của chúng tôi huấn luyện một LLM tạo ra chuỗi trừu tượng dựa trên câu hỏi, và câu trả lời cuối cùng được suy ra bằng cách cụ thể hóa chuỗi lý luận với công cụ miền (tức là trình giải phương trình).

--- TRANG 15 ---
Q: Fritz von Brodowski bị giết trong cuộc chiến tranh toàn cầu nào kéo dài từ 1939 đến 1945?
A: Câu trả lời là Thế chiến II.
W: Fritz von Brodowski > Friedrich Wilhelm Konrad von Brodowski bị giết một cách gây tranh cãi khi bị Pháp giam giữ trong Thế chiến II.
C: Tìm [cuộc chiến mà Fritz von Brodowski bị giết -Wiki-> y1].

Q: Tay vợt tennis nào đã giành được nhiều danh hiệu Grand Slam hơn, Henri Leconte hay Jonathan Stark?
A: Câu trả lời là Jonathan Stark.
W: Henri Leconte > Anh ấy đã giành được danh hiệu đôi nam French Open năm 1984. Jonathan Stark (tennis) > Trong sự nghiệp của mình, anh ấy đã giành được hai danh hiệu đôi Grand Slam.
C: Trước tiên xác định [số danh hiệu Grand Slam Henri Leconte đã giành được -Wiki-> y1]. Sau đó tìm ra [số danh hiệu Grand Slam Jonathan Stark đã giành được -Wiki-> y2].

Q: Đạo diễn của bộ phim hài lãng mạn "Big Stone Gap" có trụ sở tại thành phố New York nào?
A: Câu trả lời là Greenwich Village.
W: Big Stone Gap (phim) > Big Stone Gap là một bộ phim hài lãng mạn Mỹ năm 2014 do Adriana Trigiani đạo diễn. Adriana Trigiani > Adriana Trigiani là một đạo diễn phim người Mỹ gốc Ý có trụ sở tại Greenwich Village.
C: Trước tiên tìm kiếm [đạo diễn của bộ phim hài lãng mạn "Big Stone Gap" -Wiki-> y1]. Tên của đạo diễn bộ phim này là [y1 -NER(person)-> y2]. Sau đó xác định [y2 ở thành phố New York nào -Wiki-> y3].

Q: Randal Kleiser và Kyle Schickner có cùng quốc tịch không?
A: Câu trả lời là có.
W: Randal Kleiser > John Randal Kleiser (sinh ngày 20 tháng 7 năm 1946) là một đạo diễn và nhà sản xuất phim người Mỹ. Kyle Schickner > Kyle Schickner là một nhà sản xuất phim, nhà viết kịch, đạo diễn, diễn viên người Mỹ.
C: Trước tiên tìm ra [quốc tịch của Randal Kleiser -Wiki-> y1]. Sau đó tìm ra [quốc tịch của Kyle Schickner -Wiki-> y2].

Q: Extras được tạo ra, viết kịch bản và đạo diễn bởi Ricky Dene Gervais, một diễn viên hài, diễn viên, nhà viết kịch, nhà sản xuất, đạo diễn, ca sĩ và nhạc sĩ người Anh, sinh vào ngày nào?
A: Câu trả lời là 25 tháng 6 năm 1961.
W: Ricky Gervais > Ricky Dene Gervais (sinh ngày 25 tháng 6 năm 1961) là một diễn viên hài, diễn viên, nhà viết kịch, nhà sản xuất, đạo diễn, ca sĩ và nhạc sĩ người Anh.
C: Tìm kiếm [khi Ricky Dene Gervais sinh -Wiki-> y1].

Q: Sameera Perera là một vận động viên cricket từ quốc gia đảo nào nằm ở phía đông nam Cộng hòa Ấn Độ và phía đông bắc Maldives?
A: Câu trả lời là Sri Lanka.
W: Sameera Perera > Sameera Perera (sinh ngày 20 tháng 8 năm 1988) là một vận động viên cricket Sri Lanka.
C: Xác định [quốc gia mà vận động viên cricket Sameera Perera đến từ -Wiki-> y1].

Q: Nhà biên kịch nào có tên trong danh sách "Evolution" đã đồng viết một bộ phim có sự tham gia của Nicolas Cage và Téa Leoni?
A: Câu trả lời là David Weissman.
W: The Family Man > The Family Man là một bộ phim hài kịch lãng mạn Mỹ năm 2000 có sự tham gia của Nicolas Cage và Téa Leoni. David Weissman > Các tác phẩm điện ảnh của ông bao gồm "The Family Man" (2000), "Evolution" (2001), và "When in Rome" (2010).
C: Trước tiên tìm ra [bộ phim của Nicolas Cage và Téa Leoni -Wiki-> y1]. Tên của bộ phim này là [y1 -NER(culture)-> y2]. Sau đó tìm ra [ai đã viết y2 với tên trong danh sách "Evolution" -Wiki-> y3].

Q: Ralph Hefferline là giáo sư tâm lý học tại một trường đại học nằm ở thành phố nào?
A: Câu trả lời là New York City.
W: Ralph Hefferline > Ralph Franklin Hefferline là giáo sư tâm lý học tại Đại học Columbia. Columbia University > Đại học Columbia là một trường đại học nghiên cứu Ivy League tư thục ở Upper Manhattan, New York City.
C: Trước tiên xác định [trường đại học của giáo sư tâm lý học Ralph Hefferline -Wiki-> y1]. Trường đại học của giáo sư này là [y1 -NER(group)-> y2]. Sau đó tìm ra [y2 ở thành phố nào -Wiki-> y3].

Bảng 13: Ví dụ prompting cho việc xây dựng dữ liệu tinh chỉnh trong miền Wiki QA. Đưa ra một câu hỏi (Q), một câu trả lời vàng (A) và các bài viết Wikipedia hỗ trợ (W), LLaMa-70B được nhắc tạo ra một chuỗi lý luận trừu tượng (C) với các truy vấn tìm kiếm Wikipedia và NER. Dựa trên đó, phương pháp của chúng tôi trước tiên huấn luyện một LLM tạo ra chuỗi truy vấn trừu tượng dựa trên câu hỏi, sau đó thực hiện các truy vấn bằng các công cụ miền (tức là công cụ tìm kiếm Wikipedia và bộ công cụ NER). Cuối cùng, một LLM thứ hai được huấn luyện để tạo ra câu trả lời cuối cùng dựa trên kết quả tìm kiếm Wikipedia (loại trừ kết quả NER trung gian) trong chuỗi lý luận đã được cụ thể hóa.

--- TRANG 16 ---
Q: Có 15 cây trong lùm cây. Các công nhân lùm cây sẽ trồng cây trong lùm cây hôm nay. Sau khi họ hoàn thành, sẽ có 21 cây. Các công nhân lùm cây sẽ trồng bao nhiêu cây hôm nay?
A: Ban đầu có 15 cây. Sau đó có 21 cây sau khi trồng thêm một số cây. Vậy phải có 21-15=6. Câu trả lời là 6.

Q: Nếu có 3 xe trong bãi đậu xe và 2 xe nữa đến, có bao nhiêu xe trong bãi đậu xe?
A: Ban đầu có 3 xe. 2 xe nữa đến. 3 + 2 = 5. Câu trả lời là 5.

Q: Bông hoa có giá $9, chậu đất sét có giá nhiều hơn bông hoa $20, và túi đất có giá ít hơn bông hoa $2. Chi phí trồng hoa là bao nhiêu?
A: Chậu đất sét có giá $20 + $9 = $29. Túi đất có giá $9 - $2 = $7. Chi phí trồng hoa là $9 + $29 + $7 = $45. Câu trả lời là 45.

Q: Maddie muốn xem mẹ cô ấy chi bao nhiêu tiền cho cà phê mỗi tuần. Cô ấy pha cho mình 2 tách cà phê mỗi ngày. Mỗi tách có 1.5 ounce hạt cà phê. Một túi cà phê có giá $8 và chứa 10.5 ounce hạt. Cô ấy chi bao nhiêu cho cà phê mỗi tuần?
A: Cô ấy sử dụng 3 ounce hạt mỗi ngày vì 2 x 1.5 = 3. Cô ấy sử dụng 21 ounce hạt mỗi tuần vì 7 x 3 = 21. Cô ấy mua 2 túi hạt mỗi tuần vì 21 / 10.5 = 2. Cô ấy chi $16 cho hạt mỗi tuần vì 2 x 8 = 16. Câu trả lời là 16.

Q: Có chín máy tính trong phòng máy chủ. Năm máy tính nữa được lắp đặt mỗi ngày, từ thứ Hai đến thứ Năm. Bây giờ có bao nhiêu máy tính trong phòng máy chủ?
A: Ban đầu có 9 máy tính. Trong mỗi ngày trong 4 ngày, 5 máy tính nữa được thêm vào. Vậy 5 * 4 = 20 máy tính được thêm vào. 9 + 20 là 29. Câu trả lời là 29.

Q: Từ tháng 3 đến tháng 8, Sam kiếm được $460 khi làm 23 giờ công việc sân vườn. Tuy nhiên, từ tháng 9 đến tháng 2, Sam chỉ có thể làm việc 8 giờ. Nếu Sam đang tiết kiệm để mua một máy chơi game video có giá $600 và đã chi $340 để sửa xe, anh ấy cần làm việc thêm bao nhiêu giờ nữa trước khi có thể mua máy chơi game video?
A: Sam kiếm được $460 / 23 giờ = $20/giờ. Từ tháng 9 đến tháng 2, Sam kiếm được 8 giờ x $20/giờ = $160. Từ tháng 3 đến tháng 2, Sam kiếm được tổng cộng $460 + $160 = $620. Sau khi sửa xe, anh ấy còn lại $620 - $340 = $280. Sam cần thêm $600 - $280 = $320. Sam cần làm việc thêm $320 / $20/giờ = 16 giờ. Câu trả lời là 16.

Q: Trong số 90 người trên xe buýt của William, 3/5 là người Hà Lan. Trong số 1/2 số người Hà Lan cũng là người Mỹ, 1/3 có chỗ ngồi cạnh cửa sổ. Số người Hà Lan gốc Mỹ ngồi cạnh cửa sổ là bao nhiêu?
A: Trên xe buýt, số người Hà Lan là 3/5 tổng số người, tổng cộng 3/5 x 90 = 54 người. Trong số 54 người Hà Lan, 1/2 là người Hà Lan gốc Mỹ, tổng cộng 1/2 x 54 = 27 người. Nếu 1/3 số hành khách trên xe buýt tự nhận là người Hà Lan gốc Mỹ ngồi cạnh cửa sổ, số lượng của họ là 1/3 x 27 = 9. Câu trả lời là 9.

Q: Leah có 32 thanh sô cô la và chị gái cô ấy có 42 thanh. Nếu họ ăn 35 thanh, tổng cộng họ còn lại bao nhiêu thanh?
A: Ban đầu, Leah có 32 thanh sô cô la. Chị gái cô ấy có 42 thanh. Vậy tổng cộng họ có 32 + 42 = 74 thanh. Sau khi ăn 35 thanh, họ còn 74-35=39 thanh. Câu trả lời là 39.

Bảng 14: Ví dụ few-shot được sử dụng cho mô hình baseline CoT-FSP trong miền lý luận toán học.

Q: Fritz von Brodowski bị giết trong cuộc chiến tranh toàn cầu nào kéo dài từ 1939 đến 1945?
A: Fritz von Brodowski > Friedrich Wilhelm Konrad von Brodowski bị giết một cách gây tranh cãi khi bị Pháp giam giữ trong Thế chiến II. Câu trả lời là Thế chiến II.

Q: Tay vợt tennis nào đã giành được nhiều danh hiệu Grand Slam hơn, Henri Leconte hay Jonathan Stark?
A: Henri Leconte > Anh ấy đã giành được danh hiệu đôi nam French Open năm 1984. Jonathan Stark (tennis) > Trong sự nghiệp của mình, anh ấy đã giành được hai danh hiệu đôi Grand Slam. Câu trả lời là Jonathan Stark.

Q: Đạo diễn của bộ phim hài lãng mạn "Big Stone Gap" có trụ sở tại thành phố New York nào?
A: Big Stone Gap (phim) > Big Stone Gap là một bộ phim hài lãng mạn Mỹ năm 2014 do Adriana Trigiani đạo diễn. Adriana Trigiani > Adriana Trigiani là một đạo diễn phim người Mỹ gốc Ý có trụ sở tại Greenwich Village. Câu trả lời là Greenwich Village.

Q: Randal Kleiser và Kyle Schickner có cùng quốc tịch không?
A: Randal Kleiser > John Randal Kleiser (sinh ngày 20 tháng 7 năm 1946) là một đạo diễn và nhà sản xuất phim người Mỹ. Kyle Schickner > Kyle Schickner là một nhà sản xuất phim, nhà viết kịch, đạo diễn, diễn viên người Mỹ. Câu trả lời là có.

Q: Extras được tạo ra, viết kịch bản và đạo diễn bởi Ricky Dene Gervais, một diễn viên hài, diễn viên, nhà viết kịch, nhà sản xuất, đạo diễn, ca sĩ và nhạc sĩ người Anh, sinh vào ngày nào?
A: Ricky Gervais > Ricky Dene Gervais (sinh ngày 25 tháng 6 năm 1961) là một diễn viên hài, diễn viên, nhà viết kịch, nhà sản xuất, đạo diễn, ca sĩ và nhạc sĩ người Anh. Câu trả lời là 25 tháng 6 năm 1961.

Q: Sameera Perera là một vận động viên cricket từ quốc gia đảo nào nằm ở phía đông nam Cộng hòa Ấn Độ và phía đông bắc Maldives?
A: Sameera Perera > Sameera Perera (sinh ngày 20 tháng 8 năm 1988) là một vận động viên cricket Sri Lanka. Câu trả lời là Sri Lanka.

Q: Nhà biên kịch nào có tên trong danh sách "Evolution" đã đồng viết một bộ phim có sự tham gia của Nicolas Cage và Téa Leoni?
A: The Family Man > The Family Man là một bộ phim hài kịch lãng mạn Mỹ năm 2000 có sự tham gia của Nicolas Cage và Téa Leoni. David Weissman > Các tác phẩm điện ảnh của ông bao gồm "The Family Man" (2000), "Evolution" (2001), và "When in Rome" (2010). Câu trả lời là David Weissman.

Q: Ralph Hefferline là giáo sư tâm lý học tại một trường đại học nằm ở thành phố nào?
A: Ralph Hefferline > Ralph Franklin Hefferline là giáo sư tâm lý học tại Đại học Columbia. Columbia University > Đại học Columbia là một trường đại học nghiên cứu Ivy League tư thục ở Upper Manhattan, New York City. Câu trả lời là New York City.

Bảng 15: Ví dụ few-shot được sử dụng cho mô hình baseline CoT-FSP trong miền Wiki QA.

--- TRANG 17 ---
Trong nhiệm vụ này, bạn được cung cấp một câu hỏi toán học cấp trung học (Câu hỏi), cùng với một câu trả lời tham khảo vàng (Gold_Answer) làm giải pháp tiêu chuẩn. Ngoài ra còn có 4 câu trả lời ứng viên (Answer_A/B/C/D) được tạo ra bởi các mô hình AI A, B, C và D của chúng tôi, và nhiệm vụ của bạn là đánh giá xem mỗi câu trả lời ứng viên có lỗi hay không.

Trong việc chú thích, bạn sẽ lựa chọn nhị phân "Có" hoặc "Không" cho mỗi câu trả lời ứng viên cho hai loại lỗi (định nghĩa bên dưới). Vì có 4 câu trả lời ứng viên và 2 loại lỗi, đối với mỗi câu hỏi, bạn sẽ thực hiện tổng cộng 8 lựa chọn nhị phân.

Cụ thể, chúng tôi xem xét hai loại lỗi: 1) lỗi số học; 2) lỗi lý luận

1. Câu trả lời có bất kỳ lỗi số học nào không? tức là, các suy dẫn toán được tính toán sai, hoặc các phương trình toán không đúng. Chọn Có hoặc Không. Ví dụ:

Câu hỏi: Một thủy thủ đang đào kho báu bị chôn trên hòn đảo được đánh dấu X trên bản đồ. Họ đã đào mười hố vào ngày đầu tiên, mười ba hố vào ngày thứ hai, và tám hố vào ngày thứ ba. Họ ngừng đào sớm vào ngày thứ ba để lấp đầy hai hố mà các thủy thủ liên tục rơi vào. Bây giờ có bao nhiêu hố trên hòn đảo?

Câu trả lời ứng viên: Vào ngày đầu tiên, các thủy thủ đã đào 10 hố. Vào ngày thứ hai, họ đã đào 13 hố. Vào ngày thứ ba, họ đã đào 8 hố, và lấp đầy 2 hố, vậy có 10 + 13 + 8 - 2 = 27 hố. Câu trả lời là 27.

Phân tích: Có, câu trả lời có lỗi số học, nơi 10 + 13 + 8 - 2 phải là 29.

2. Câu trả lời có bất kỳ lỗi lý luận nào không? tức là, hiểu sai câu hỏi, hoặc có chiến lược giải quyết vấn đề sai, không liên quan đến tính đúng đắn số học. Chọn Có hoặc Không. Ví dụ:

Câu hỏi: Marcus bằng một nửa tuổi của Leo và nhỏ hơn Deanna năm tuổi. Deanna 26 tuổi. Leo bao nhiêu tuổi?

Câu trả lời ứng viên: Marcus bằng một nửa tuổi của Leo. Vậy Marcus là 26 / 2 = 13 tuổi. Leo là 13 + 5 = 18 tuổi. Câu trả lời là 18.

Phân tích: Có, câu trả lời có lỗi lý luận, Leo phải là (26 - 5) * 2 = 42 tuổi.

Ghi chú:
1. Vui lòng bỏ qua bất kỳ lỗi ngữ pháp hoặc chính tả nào trong tất cả câu hỏi và câu trả lời, chúng không được coi là lỗi giải pháp toán.
2. Nếu bạn cảm thấy câu trả lời tham khảo vàng (Gold_Answer) là sai, chỉ cần bỏ qua nó và đưa ra phán đoán dựa trên câu trả lời của chính bạn cho câu hỏi.

Hình 5: Hướng dẫn cho đánh giá con người trên lý luận toán học GSM8K.
