Tài liệu tham khảo

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Báo cáo kỹ thuật palm 2. arXiv preprint arXiv:2305.10403.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Tổng hợp chương trình với các mô hình ngôn ngữ lớn. arXiv preprint arXiv:2108.07732.

Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Hajishirzi, và Ludwig Schmidt. 2022. Khám phá bối cảnh của độ bền phân phối cho các mô hình trả lời câu hỏi. Trong Findings of the Association for Computational Linguistics: EMNLP 2022, trang 5971-5987.

Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, và Diyi Yang. 2023. Một khảo sát thực nghiệm về tăng cường dữ liệu cho học có dữ liệu hạn chế trong nlp. Transactions of the Association for Computational Linguistics, 11:191-211.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Đánh giá các mô hình ngôn ngữ lớn được huấn luyện trên mã. arXiv preprint arXiv:2107.03374.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Mở rộng mô hình ngôn ngữ với pathways. arXiv preprint arXiv:2204.02311.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Huấn luyện các bộ xác minh để giải các bài toán từ toán học. arXiv preprint arXiv:2110.14168.

Fabrizio Gilardi, Meysam Alizadeh, và Maël Kubli. 2023. Chatgpt vượt trội so với người làm việc tự do cho các tác vụ chú thích văn bản. arXiv preprint arXiv:2303.15056.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. 2020. Đo lường hiểu biết ngôn ngữ đa tác vụ khối lượng lớn. Trong International Conference on Learning Representations.

Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, và Jacob Steinhardt. 2021. Đo lường giải quyết vấn đề toán học với bộ dữ liệu math. Trong Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).

Or Honovich, Thomas Scialom, Omer Levy, và Timo Schick. 2022. Hướng dẫn không tự nhiên: Điều chỉnh các mô hình ngôn ngữ với (gần như) không có lao động con người. arXiv preprint arXiv:2212.09689.

Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161.

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. 2019. Roberta: Một cách tiếp cận tiền huấn luyện bert được tối ưu hóa mạnh mẽ. arXiv preprint arXiv:1907.11692.

Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, và Dongmei Zhang. 2023a. Wizardmath: Trao quyền lý luận toán học cho các mô hình ngôn ngữ lớn thông qua evol-instruct được tăng cường. arXiv preprint arXiv:2308.09583.

Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, và Daxin Jiang. 2023b. Wizardcoder: Trao quyền cho các mô hình ngôn ngữ lớn mã với evol-instruct. arXiv preprint arXiv:2306.08568.

Yu Meng, Jiaxin Huang, Yu Zhang, và Jiawei Han. 2022. Tạo dữ liệu huấn luyện với các mô hình ngôn ngữ: Hướng tới hiểu biết ngôn ngữ zero-shot. Advances in Neural Information Processing Systems, 35:462-477.

Yu Meng, Martin Michalski, Jiaxin Huang, Yu Zhang, Tarek Abdelzaher, và Jiawei Han. 2023. Điều chỉnh các mô hình ngôn ngữ như các bộ tạo dữ liệu huấn luyện cho học few-shot được tăng cường bổ sung. Trong International Conference on Machine Learning, trang 24457-24477. PMLR.

Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, và Yanai Elazar. 2023. Tinh chỉnh few-shot so với học trong ngữ cảnh: Một so sánh công bằng và đánh giá. arXiv preprint arXiv:2305.16938.

OpenAI. 2023. Báo cáo kỹ thuật gpt-4. arXiv, trang 2303-08774.

Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. 2023. Điều chỉnh hướng dẫn với gpt-4. arXiv preprint arXiv:2304.03277.

Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Các mô hình nền tảng mở cho mã. arXiv preprint arXiv:2308.12950.

Timo Schick và Hinrich Schütze. 2021. Tạo bộ dữ liệu với các mô hình ngôn ngữ được tiền huấn luyện. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 6943-6951.

Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Lee Boyd-Graber, và Lijuan Wang. 2022. Thúc đẩy gpt-3 trở nên đáng tin cậy. Trong The Eleventh International Conference on Learning Representations.

Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, và Tie-Yan Liu. 2020. Mpnet: Tiền huấn luyện được che mặt và hoán vị cho hiểu biết ngôn ngữ. Advances in Neural Information Processing Systems, 33:16857-16867.

Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, và Chuang Gan. 2023. Tự điều chỉnh các mô hình ngôn ngữ theo nguyên tắc từ đầu với sự giám sát tối thiểu của con người. arXiv preprint arXiv:2305.03047.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, và Jonathan Berant. 2019. Commonsenseqa: Một thách thức trả lời câu hỏi nhắm mục tiêu kiến thức thông thường. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 4149-4158.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Stanford alpaca: Một mô hình llama tuân theo hướng dẫn. https://github.com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Các mô hình chat nền tảng mở và được tinh chỉnh. arXiv preprint arXiv:2307.09288.

Prasetya Utama, Nafise Sadat Moosavi, Victor Sanh, và Iryna Gurevych. 2021. Tránh heuristics suy luận trong việc tinh chỉnh dựa trên prompt few-shot. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 9063-9074.

Laurens Van der Maaten và Geoffrey Hinton. 2008. Trực quan hóa dữ liệu sử dụng t-sne. Journal of machine learning research, 9(11).

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, và Hannaneh Hajishirzi. 2022. Self-instruct: Điều chỉnh mô hình ngôn ngữ với các hướng dẫn tự tạo. arXiv preprint arXiv:2212.10560.

Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, và Gideon Mann. 2023. Bloomberggpt: Một mô hình ngôn ngữ lớn cho tài chính. arXiv preprint arXiv:2303.17564.

Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và Daxin Jiang. 2023. Wizardlm: Trao quyền cho các mô hình ngôn ngữ lớn để tuân theo các hướng dẫn phức tạp. arXiv preprint arXiv:2304.12244.

Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, và Xinyun Chen. 2023. Các mô hình ngôn ngữ lớn như các bộ tối ưu hóa. arXiv preprint arXiv:2309.03409.

Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao Feng, Zhiyong Wu, Tao Yu, và Lingpeng Kong. 2022. Zerogen: Học zero-shot hiệu quả thông qua tạo bộ dữ liệu. arXiv preprint arXiv:2202.07922.

Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, và Yoav Artzi. 2019. Bertscore: Đánh giá tạo văn bản với bert. Trong International Conference on Learning Representations.

Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, và Jimmy Ba. 2022. Các mô hình ngôn ngữ lớn là các kỹ sư prompt ở mức con người. Trong NeurIPS 2022 Foundation Models for Decision Making Workshop.

A Phân tích Chất lượng
Để đánh giá chất lượng của các hướng dẫn được tạo ra, chúng tôi đánh giá xem các hướng dẫn được tạo ra có mạch lạc và hợp lý về logic hay không. Cho đánh giá này, chúng tôi sử dụng ChatGPT làm người chú thích. Chúng tôi lấy mẫu ngẫu nhiên 200 hướng dẫn được tạo ra cho MBPP và CommonsenseQA. Trước tiên chúng tôi cho ChatGPT biết mô tả tác vụ của MBPP và CommonsenseQA, sau đó hỏi ChatGPT, "Bạn có nghĩ hướng dẫn này mạch lạc và hợp lý về logic không? Có hoặc Không." Như một baseline, chúng tôi cũng đánh giá chất lượng của các mẫu thực từ các bộ dữ liệu tương ứng làm giới hạn chất lượng trên.

Như có thể thấy trong Bảng 6, chất lượng của các hướng dẫn được tạo ra có thể so sánh với các mẫu thực, cho thấy rằng các mẫu được tạo ra có độ chính xác đủ. Mặc dù một phần nhỏ các mẫu không chính xác vẫn tồn tại, chúng tôi đã điều tra tác động của những lỗi như vậy trong Mục 4.5.

B Tác động của Độ dài lên Hiệu suất
Khả năng của Ada-Instruct trong việc tạo ra các hướng dẫn dài hơn phù hợp tốt với phân phối mục tiêu góp phần vào sự cải thiện hiệu suất của nó. Để xác thực trực tiếp lợi ích của các hướng dẫn dài hơn thông qua thí nghiệm, chúng tôi chọn HumanEval làm tác vụ mục tiêu. Chúng tôi lấy mẫu ngẫu nhiên hai tập hợp 5k hướng dẫn:

1. Từ tất cả các hướng dẫn được tạo ra bởi Ada-Instruct.
2. Chỉ từ các hướng dẫn có độ dài nhỏ hơn 90 (dựa trên Hình 1, self-instruct hiếm khi tạo ra các hướng dẫn dài hơn 90 token).

Như được thể hiện trong Bảng 7, các hướng dẫn được lấy mẫu từ tập hợp bao gồm các ví dụ dài hơn mang lại điểm pass@1 cao hơn.

C Chi tiết Huấn luyện
Khi tinh chỉnh trong Bước 1, chúng tôi huấn luyện các mô hình trong 40 epoch với 10% bước khởi động cho tất cả các tác vụ. Chúng tôi sử dụng kích thước batch là 10, tỷ lệ học là 1e-6, weight decay là 1e-2, bộ lập lịch tỷ lệ học cosine, và độ chính xác bf16 cho tất cả các tác vụ trừ MATH. Chúng tôi thấy MATH khó hơn nhiều so với các tác vụ khác, vì vậy chúng tôi áp dụng tỷ lệ học thấp hơn là 8e-7 để thích ứng tốt hơn với tác vụ. Đối với tất cả các tác vụ được xem xét, chúng tôi áp dụng checkpoint đầu tiên tại đó giá trị loss nằm trong khoảng từ 0.2 đến 0.4 để tránh overfitting. Checkpoint này được chọn từ epoch huấn luyện thứ 25, 30, 35, và 40.

Trong Bước 1 của quá trình tạo, nhiệt độ được đặt thành 1 cho tất cả các tác vụ. Để tăng cường đa dạng, chúng tôi sử dụng lấy mẫu top-k. Cụ thể, đối với MBPP và CSQA đơn giản hơn, chúng tôi đặt k = 100, trong khi đối với HumanEval, GSM8K, và MATH phức tạp hơn, chúng tôi đặt k = 80.

Khi tinh chỉnh trong Bước 3, đối với tất cả các tác vụ trừ HumanEval và CommonsenseQA, chúng tôi huấn luyện các LLM trong 3 epoch với kích thước batch là 256, tỷ lệ học là 2e-5, weight decay là 1e-2 và độ chính xác bf16. Chúng tôi sử dụng bộ lập lịch cosine với 10% bước khởi động. Đối với HumanEval, chúng tôi áp dụng tỷ lệ học thấp hơn là 1e-5. Đối với CommonsenseQA, chúng tôi áp dụng 2 epoch huấn luyện và tỷ lệ học thấp hơn là 1e-5, cho rằng các điểm dữ liệu trong tác vụ này ngắn hơn nhiều so với các tác vụ khác. Tương tự như (Rozière et al., 2023), chúng tôi áp dụng bộ lập lịch cosine với 15% bước khởi động và đặt tỷ lệ học cuối cùng là 25% của tỷ lệ học đỉnh. Chúng tôi không áp dụng che mặt loss cho hướng dẫn đối với tất cả các tác vụ trừ CommonsenseQA, vì đầu ra cho CommonsenseQA chỉ bao gồm một vài token.

D Nghiên cứu Trường hợp
Trong Bảng 8, chúng tôi trình bày các hướng dẫn được tạo ra bởi Ada-Instruct trên HumanEval. Chúng tôi quan sát thấy rằng các hướng dẫn được tạo ra bởi Self-Instruct chủ yếu ngắn. Mặc dù Evol-Instruct có thể tạo ra các hướng dẫn dài hơn bằng cách lặp đi lặp lại thêm các ràng buộc, những hướng dẫn này có xu hướng không tự nhiên và không phù hợp tốt với phân phối của các tác vụ hạ nguồn. Ngược lại, Ada-Instruct có khả năng tạo ra các hướng dẫn dài hơn phù hợp tốt với tác vụ mục tiêu.

E Giấy phép cho Artifacts
Chúng tôi liệt kê các artifacts được sử dụng trong bài báo này và giấy phép của chúng dưới đây:
• (Touvron et al., 2023), llama2
• (Xu et al., 2023; Luo et al., 2023a,b), llama2
• (Wang et al., 2022), giấy phép Apache-2.0

Công việc này phù hợp với mục đích sử dụng dự định của chúng.

F Chiến lược Đánh giá

F.1 Prompts cho Tác vụ Hạ nguồn
HumanEval:
[INST] Bạn là một lập trình viên Python chuyên nghiệp, hoàn thành hàm dưới đây dựa trên docstring và các test case đã cho:
{Question}
Mã của bạn nên bắt đầu với thẻ [PYTHON] và kết thúc với thẻ [/PYTHON]. [/INST]

MBPP:
[INST] Bạn là một lập trình viên Python chuyên nghiệp, và đây là nhiệm vụ của bạn:
{Question}
Mã của bạn nên vượt qua các test này:
{Test Cases}
Mã của bạn nên bắt đầu với thẻ [PYTHON] và kết thúc với thẻ [/PYTHON]. [/INST]

GSM8k và MATH:
[INST] Bạn là chuyên gia giải quyết các bài toán toán học đòi hỏi lý luận đa bước, và đây là nhiệm vụ của bạn:
{Question} [/INST] Hãy suy nghĩ từng bước.

CommonsenseQA:
[INST] Bạn là chuyên gia về lý luận thông thường, và đây là nhiệm vụ của bạn: {Question}
A. {Text of Label A}
B. {Text of Label B}
C. {Text of Label C}
D. {Text of Label D}
E. {Text of Label E} [/INST] Câu trả lời là:

F.2 Chiến lược Giải mã
Đối với các tác vụ hoàn thành mã, để đảm bảo các đánh giá có thể so sánh được, chúng tôi theo (Rozière et al., 2023) và báo cáo điểm pass@1 của các mô hình của chúng tôi trong các thiết lập giải mã tham lam và zero-shot.

Đối với các tác vụ toán học, để đảm bảo các đánh giá có thể so sánh được, chúng tôi theo (Luo et al., 2023a) và báo cáo điểm pass@1 của các mô hình của chúng tôi trong các thiết lập giải mã tham lam, zero-shot, và chuỗi suy nghĩ.

Đối với CommonsenseQA, việc thiếu tập kiểm tra có sẵn đòi hỏi phải đánh giá mô hình của chúng tôi trên tập phát triển. Đánh giá này được thực hiện trong một khuôn khổ được điều chỉnh từ (Hendrycks et al., 2020), và được thực hiện theo cách zero-shot và chỉ trả lời. Để đảm bảo so sánh công bằng, chúng tôi cũng đánh giá các mô hình cơ sở LLAMA 2 khác trong thiết lập này.

G Định dạng Dữ liệu Tinh chỉnh cho Ada-Instruct

G.1 Bước 1
HumanEval:
[INST] Bạn là một lập trình viên Python chuyên nghiệp, hoàn thành hàm dưới đây dựa trên docstring và các test case đã cho:
{Question}
Mã của bạn nên bắt đầu với thẻ [PYTHON] và kết thúc với thẻ [/PYTHON]. [/INST] [PYTHON]
# pass
[/PYTHON]

MBPP:
[INST] Bạn là một lập trình viên Python chuyên nghiệp, và đây là nhiệm vụ của bạn:
{Question}
Mã của bạn nên vượt qua các test này:
{Test Cases}
Mã của bạn nên bắt đầu với thẻ [PYTHON] và kết thúc với thẻ [/PYTHON]. [/INST] [PYTHON]
# pass
[/PYTHON]

GSM8k và MATH:
[INST] Bạn là chuyên gia giải quyết các bài toán toán học đòi hỏi lý luận đa bước, và đây là nhiệm vụ của bạn:
{Question} [/INST] Hãy suy nghĩ từng bước.

CommonsenseQA:
[INST] Bạn là chuyên gia về lý luận thông thường, và đây là nhiệm vụ của bạn: {Question}
A. {Text of Label A}
B. {Text of Label B}
C. {Text of Label C}
D. {Text of Label D}
E. {Text of Label E} [/INST]

G.2 Bước 3
HumanEval:
[INST] Bạn là một lập trình viên Python chuyên nghiệp, hoàn thành hàm dưới đây dựa trên docstring và các test case đã cho:
{Question}
Mã của bạn nên bắt đầu với thẻ [PYTHON] và kết thúc với thẻ [/PYTHON]. [/INST] [PYTHON]
{Output}
[/PYTHON]

MBPP:
[INST] Bạn là một lập trình viên Python chuyên nghiệp, và đây là nhiệm vụ của bạn:
{Question}
Mã của bạn nên vượt qua các test này:
{Test Cases}
Mã của bạn nên bắt đầu với thẻ [PYTHON] và kết thúc với thẻ [/PYTHON]. [/INST] [PYTHON]
{Output}
[/PYTHON]

GSM8k và MATH:
[INST] Bạn là chuyên gia giải quyết các bài toán toán học đòi hỏi lý luận đa bước, và đây là nhiệm vụ của bạn:
{Question} [/INST] Hãy suy nghĩ từng bước.
{Output}

CommonsenseQA:
[INST] Bạn là chuyên gia về lý luận thông thường, và đây là nhiệm vụ của bạn: {Question}
A. {Text of Label A}
B. {Text of Label B}
C. {Text of Label C}
D. {Text of Label D}
E. {Text of Label E} [/INST] Câu trả lời là: {Output}

H Prompts cho Self-Instruct
Để khuyến khích việc tạo ra các hướng dẫn chất lượng cao và đa dạng, chúng tôi sử dụng các prompt sau trong baseline Self-Instruct.

H.1 Prompts Cho gpt-3.5-turbo-instruct
HumanEval:
Bạn được yêu cầu đưa ra một tập hợp 20 hướng dẫn đa dạng về tác vụ hoàn thành mã. Những hướng dẫn này sẽ được đưa cho một mô hình Codex và chúng tôi sẽ đánh giá mô hình Codex để tạo ra các mã tuân theo các hướng dẫn.

Đây là các yêu cầu:
1. Các hướng dẫn được thiết kế để kiểm tra khả năng lập trình Python để giải quyết các bài toán Python. Mỗi hướng dẫn nên mô tả một bài toán Python với định nghĩa hàm, docstring, và test case.
2. Các hướng dẫn nên kết hợp càng nhiều khái niệm Python càng tốt, cũng như đa dạng và toàn diện.
3. Các hướng dẫn không nên quá dễ. Mỗi bài toán Python nên được giải quyết sử dụng các thư viện tích hợp sẵn hoặc cấu trúc dữ liệu với thuật toán ở mức trung cấp.
4. Các hướng dẫn nên dài ít nhất 1 đến 2 câu. Câu mệnh lệnh hoặc câu hỏi đều được phép.
5. Đầu ra nên là một phản hồi phù hợp với hướng dẫn, và nên tính đến đầy đủ các yêu cầu và test case trong hướng dẫn.
6. Các hướng dẫn không được xuất hiện trong các bộ dữ liệu đánh giá chính thống cho tạo mã, ví dụ HumanEval, MBPP, DS1000 và vân vân.

Danh sách 20 tác vụ:
###
1. {Example 1}
###
2. {Example 2}
###
3. {Example 3}
###
4.

MBPP:
Bạn được yêu cầu đưa ra một tập hợp 20 hướng dẫn đa dạng về tác vụ hoàn thành mã. Những hướng dẫn này sẽ được đưa cho một mô hình Codex và chúng tôi sẽ đánh giá mô hình Codex để tạo ra các mã tuân theo các hướng dẫn.

Đây là các yêu cầu:
1. Các hướng dẫn được thiết kế để kiểm tra khả năng lập trình Python để giải quyết các bài toán Python cơ bản. Mỗi hướng dẫn nên có một giải pháp rõ ràng và riêng biệt.
2. Các hướng dẫn nên kết hợp càng nhiều khái niệm Python càng tốt, cũng như đa dạng và toàn diện.
3. Các hướng dẫn không nên quá phức tạp hoặc quá dễ. Mỗi bài toán Python nên được giải quyết sử dụng các thư viện tích hợp sẵn hoặc cấu trúc dữ liệu với thuật toán ở mức trung cấp.
4. Các hướng dẫn nên dài ít nhất 1 đến 2 câu. Câu mệnh lệnh hoặc câu hỏi đều được phép.
5. Đầu ra nên là một phản hồi phù hợp với hướng dẫn, và nên tính đến đầy đủ các yêu cầu và test case trong hướng dẫn.
6. Các hướng dẫn không được xuất hiện trong các bộ dữ liệu đánh giá chính thống cho tạo mã, ví dụ HumanEval, MBPP, DS1000 và vân vân.

Danh sách 20 tác vụ:
###
1. {Example 1}
###
2. {Example 2}
###
3. {Example 3}
###
4.

GSM8k:
Bạn được yêu cầu đưa ra một tập hợp 20 hướng dẫn đa dạng về tác vụ giải quyết bài toán toán học. Những hướng dẫn này sẽ được đưa cho một mô hình toán học và chúng tôi sẽ đánh giá mô hình toán học để tạo ra các giải pháp tuân theo các hướng dẫn.

Đây là các yêu cầu:
1. Các hướng dẫn được thiết kế để kiểm tra khả năng toán học để giải quyết các bài toán toán học đòi hỏi lý luận đa bước. Mỗi hướng dẫn nên được kèm theo một đường dẫn lý luận chi tiết và một câu trả lời cuối cùng.
2. Các hướng dẫn nên bao gồm các loại bài toán toán học lớp tiểu học đa dạng, cũng như đa dạng và toàn diện.
3. Các hướng dẫn không nên quá phức tạp hoặc quá dễ. Mỗi bài toán toán học nên mất từ 2 đến 8 bước để giải quyết, và các giải pháp chủ yếu liên quan đến việc thực hiện các phép tính sử dụng các phép toán số học cơ bản (+ - / *) để đạt được câu trả lời cuối cùng.
4. Các hướng dẫn nên dài ít nhất 1 đến 2 câu. Câu mệnh lệnh hoặc câu hỏi đều được phép.
5. Đầu ra nên là một phản hồi phù hợp với hướng dẫn có dạng lý luận theo sau là câu trả lời cuối cùng.
6. Các hướng dẫn không được xuất hiện trong các bộ dữ liệu đánh giá chính thống cho toán học, ví dụ GSM8K, MATH và vân vân.

Danh sách 20 tác vụ:
###
1. {Example 1}
###
2. {Example 2}
###
3. {Example 3}
###
4.

MATH:
Bạn được yêu cầu đưa ra một tập hợp 20 hướng dẫn đa dạng về tác vụ giải quyết bài toán toán học. Những hướng dẫn này sẽ được đưa cho một mô hình toán học và chúng tôi sẽ đánh giá mô hình toán học để tạo ra các giải pháp tuân theo các hướng dẫn.

Đây là các yêu cầu:
1. Các hướng dẫn được thiết kế để kiểm tra khả năng toán học để giải quyết các bài toán toán học đòi hỏi lý luận đa bước. Mỗi hướng dẫn nên được kèm theo một đường dẫn lý luận chi tiết và một câu trả lời cuối cùng.
2. Các hướng dẫn nên mô tả các bài toán toán học trong LaTex đòi hỏi kiến thức như giải tích, đại số, lý thuyết số, đếm và xác suất, v.v.
3. Các hướng dẫn nên thách thức, đa dạng và toàn diện. Mỗi bài toán toán học nên mất nhiều bước lý luận phức tạp có thể với một số kiến thức và công cụ toán học tiến bộ để giải quyết.
4. Các hướng dẫn nên dài ít nhất 1 đến 2 câu. Câu mệnh lệnh hoặc câu hỏi đều được phép.
5. Đầu ra nên là một phản hồi phù hợp với hướng dẫn có dạng lý luận theo sau là câu trả lời cuối cùng. Cả lý luận và câu trả lời nên có dạng LaTex. Câu trả lời cuối cùng nên được đặt trong "$\boxed{}$".
6. Các hướng dẫn không được xuất hiện trong các bộ dữ liệu đánh giá chính thống cho toán học, ví dụ GSM8K, MATH và vân vân.

Danh sách 20 tác vụ:
###
1. {Example 1}
###
2. {Example 2}
###
3. {Example 3}
###
4.

H.2 Prompts Cho gpt-4o
HumanEval:
user: Bạn được yêu cầu đưa ra một tập hợp 10 hướng dẫn đa dạng về tác vụ hoàn thành mã. Những hướng dẫn này sẽ được đưa cho một mô hình Codex và chúng tôi sẽ đánh giá mô hình Codex để tạo ra các mã tuân theo các hướng dẫn.

Đây là các yêu cầu:
1. Các hướng dẫn được thiết kế để kiểm tra khả năng lập trình Python để giải quyết các bài toán Python. Mỗi hướng dẫn nên mô tả một bài toán Python với định nghĩa hàm, docstring, và test case.
2. Các hướng dẫn nên kết hợp càng nhiều khái niệm Python càng tốt, cũng như đa dạng và toàn diện.
3. Các hướng dẫn không nên quá dễ. Mỗi bài toán Python nên được giải quyết sử dụng các thư viện tích hợp sẵn hoặc cấu trúc dữ liệu với thuật toán ở mức trung cấp.
4. Các hướng dẫn nên dài ít nhất 1 đến 2 câu. Câu mệnh lệnh hoặc câu hỏi đều được phép.
5. Đầu ra nên là một phản hồi phù hợp với hướng dẫn, và nên tính đến đầy đủ các yêu cầu và test case trong hướng dẫn.
6. Các hướng dẫn không được xuất hiện trong các bộ dữ liệu đánh giá chính thống cho tạo mã, ví dụ HumanEval, MBPP, DS1000 và vân vân.

assistant: ###
1. {Example 1}
###
2. {Example 2}
###
3. {Example 3}
###

user: Tiếp tục tạo ra 7 hướng dẫn còn lại. Số thứ tự của mỗi hướng dẫn phải được đặt trước bằng "###".

GSM8k:
user: Bạn được yêu cầu đưa ra một tập hợp 10 hướng dẫn đa dạng về tác vụ giải quyết bài toán toán học. Những hướng dẫn này sẽ được đưa cho một mô hình toán học và chúng tôi sẽ đánh giá mô hình toán học để tạo ra các giải pháp tuân theo các hướng dẫn.

Đây là các yêu cầu:
1. Các hướng dẫn được thiết kế để kiểm tra khả năng toán học để giải quyết các bài toán toán học đòi hỏi lý luận đa bước. Mỗi hướng dẫn nên được kèm theo một đường dẫn lý luận chi tiết và một câu trả lời cuối cùng.
2. Các hướng dẫn nên bao gồm các loại bài toán toán học lớp tiểu học đa dạng, cũng như đa dạng và toàn diện.
3. Các hướng dẫn không nên quá phức tạp hoặc quá dễ. Mỗi bài toán toán học nên mất từ 2 đến 8 bước để giải quyết, và các giải pháp chủ yếu liên quan đến việc thực hiện các phép tính sử dụng các phép toán số học cơ bản (+ - / *) để đạt được câu trả lời cuối cùng.
4. Các hướng dẫn nên dài ít nhất 1 đến 2 câu. Câu mệnh lệnh hoặc câu hỏi đều được phép.
5. Đầu ra nên là một phản hồi phù hợp với hướng dẫn có dạng lý luận theo sau là câu trả lời cuối cùng.
6. Các hướng dẫn không được xuất hiện trong các bộ dữ liệu đánh giá chính thống cho toán học, ví dụ GSM8K, MATH và vân vân.

assistant: ###
1. {Example 1}
###
2. {Example 2}
###
3. {Example 3}
###

user: Tiếp tục tạo ra 7 hướng dẫn còn lại. Số thứ tự của mỗi hướng dẫn phải được đặt trước bằng "###".
