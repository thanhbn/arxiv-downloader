[Tiếp tục từ trang 7...]

Baseline: Chúng tôi so sánh với Self-Instruct với cùng mẫu ban đầu và cùng lượng dữ liệu SFT. Chúng tôi cũng so sánh với Evol-Instruct với việc triển khai WizardLM (Xu et al., 2023). Để so sánh công bằng, chúng tôi đã sử dụng phiên bản WizardLM-13B-V1.2, cũng sử dụng LLAMA2-13B làm mô hình cơ sở.

Kết quả: Dựa trên kết quả được trình bày trong Bảng 5, chúng tôi quan sát thấy sự cải thiện đáng kể về hiệu suất được quy cho Ada-Instruct. Ada-Instruct cũng thể hiện hiệu suất vượt trội so với cả Self-Instruct và Evol-Instruct.

4.4 Phân tích Tính Nhất quán Phân phối
Chúng tôi đã minh họa trong Hình 1 rằng Ada-Instruct có khả năng tạo ra các hướng dẫn có phân phối độ dài phù hợp với nhiệm vụ mục tiêu. Bây giờ chúng tôi sẽ tiếp tục phân tích thêm tính nhất quán ngữ nghĩa của chúng. Cho rằng chúng tôi chỉ sử dụng 10 mẫu ban đầu, điều tra của chúng tôi đặc biệt tập trung vào hai mối quan tâm quan trọng: (1) mức độ mà các hướng dẫn được tạo ra bao trùm toàn bộ phân phối của nhiệm vụ mục tiêu, thay vì chỉ đơn thuần lặp lại những ví dụ ban đầu này (§ 4.4.1), và (2) tính đa dạng của các hướng dẫn được tạo ra, cụ thể kiểm tra xem chúng có thể hiện một phổ biến thể rộng hay không (§ 4.4.2).

4.4.1 Phân phối Ngữ nghĩa
Chúng tôi vẽ biểu đồ phân phối ngữ nghĩa của các hướng dẫn ban đầu và các hướng dẫn được tạo ra. Thêm vào đó, chúng tôi vẽ phân phối của nhiệm vụ mục tiêu để so sánh, nhằm xác minh xem các hướng dẫn được tạo ra có phù hợp với phân phối mục tiêu hay không. Để so sánh, chúng tôi cũng vẽ phân phối của các hướng dẫn bởi Evol-Instruct. Chúng tôi biểu diễn ngữ nghĩa của các hướng dẫn bằng cách sử dụng text-embedding-ada-002 API từ OpenAI và hình dung phân phối của chúng bằng t-SNE (Van der Maaten và Hinton, 2008).

Hình 3 cho thấy rằng các hướng dẫn được tạo ra thể hiện phân phối nhất quán với nhiệm vụ mục tiêu. Các hướng dẫn của Ada-Instruct không bị giới hạn trong vùng lân cận của mười mẫu ban đầu mà thể hiện khả năng mở rộng đến các vùng rộng hơn, phù hợp với phân phối hướng dẫn thực tế của nhiệm vụ mục tiêu. Ngược lại, phân phối Evol-Instruct cho thấy sai lệch đáng chú ý khỏi phân phối hướng dẫn mục tiêu. Những khoảng cách như vậy không bất thường - Evol-Instruct, dựa trên kỹ thuật thiết kế prompt đa lượt, có thể tạo ra các hướng dẫn dài và phức tạp. Tuy nhiên, việc tạo prompt thủ công mà không học tập làm cho việc phù hợp với phân phối dự định trở nên khó khăn. Ada-Instruct có khả năng học để thích ứng với phân phối hướng dẫn hạ nguồn, điều này rất cần thiết cho việc tạo hướng dẫn. Những quan sát này xác nhận cả tính nhất quán phân phối của Ada-Instruct về mặt ngữ nghĩa, và động lực của việc thích ứng LLM làm bộ tạo hướng dẫn cho các nhiệm vụ dự định.

4.4.2 Tính Đa dạng
Cho rằng bộ tạo hướng dẫn của chúng tôi được huấn luyện chỉ từ 10 ví dụ, một mối quan tâm khác là liệu các hướng dẫn được tạo ra có đủ đa dạng hay chúng bị overfit với một số lượng hạn chế các mẫu huấn luyện. Để giải quyết điều này, chúng tôi đã đánh giá tính đa dạng của các mẫu được tạo ra. Cụ thể, chúng tôi lấy mẫu ngẫu nhiên 10000 cặp mẫu được tạo ra cho MBPP và tính toán điểm tương tự của chúng. Điểm tương tự cao cho một cặp hướng dẫn chỉ ra sự dư thừa. Do đó, cho một tập hợp các mẫu được tạo ra đa dạng hơn, chúng tôi mong muốn một phân phối điểm tương tự thấp hơn. Chúng tôi đã so sánh tính đa dạng của các hướng dẫn được tạo ra bởi Ada-Instruct và bởi Self-Instruct.

Chúng tôi đã tuân theo cách tiếp cận được sử dụng trong nghiên cứu trước đây (Honovich et al., 2022) để sử dụng BERTscore (Zhang et al., 2019) để đo độ tương tự giữa các cặp hướng dẫn. Việc hình dung kết quả có thể được thấy trong Hình 4. Các mẫu từ Ada-Instruct thể hiện độ tương tự thấp hơn giữa các cặp. Điều này chỉ ra rằng Ada-Instruct tạo ra các hướng dẫn với tính đa dạng lớn hơn. Cho rằng khả năng biểu đạt của mô hình cơ sở cho Ada-Instruct (Code LLAMA) rõ ràng yếu hơn so với ChatGPT, điều này nhấn mạnh hiệu quả của Ada-Instruct trong việc tạo ra các hướng dẫn đa dạng.

4.5 Tác động của Chất lượng Hướng dẫn
Ada-Instruct thường sử dụng tinh chỉnh trên các mô hình mã nguồn mở, trong khi Self-Instruct thường sử dụng các mô hình nguồn đóng (như ChatGPT) để tạo hướng dẫn. Điều quan trọng cần lưu ý là, tính đến hiện tại, chất lượng của các mô hình mã nguồn mở nói chung vẫn thua kém so với các mô hình nguồn đóng. Do đó, một mối quan tâm với Ada-Instruct là chất lượng của các hướng dẫn riêng lẻ có thể thấp hơn, đặc biệt đối với các nhiệm vụ phức tạp. Trong phần này, chúng tôi điều tra tác động thực tế đến chất lượng hướng dẫn.

Chúng tôi lấy MBPP làm đối tượng và kiểm tra việc giảm chất lượng hướng dẫn ảnh hưởng đến kết quả như thế nào. Cụ thể, chúng tôi phân tích tác động của việc sử dụng các hướng dẫn có thể sai lầm được tạo ra bởi Ada-Instruct (được ký hiệu là mẫu nhiễu) so với việc sử dụng các hướng dẫn đúng. Để xác định tính đúng đắn của các hướng dẫn, cho rằng các mẫu MBPP bao gồm cả mã và các trường hợp sử dụng, chúng tôi kiểm tra xem mã được tạo ra có vượt qua các trường hợp này thành công hay không. Các hướng dẫn làm được như vậy được coi là mẫu đúng. Trong tất cả các mẫu nhiễu được tạo ra, chúng tôi thấy rằng 46.9% là đúng. Chúng tôi lấy mẫu các quy mô khác nhau của các mẫu nhiễu được tạo ra và mẫu đúng, tương ứng, và so sánh tác động của việc huấn luyện mô hình trên chúng trong Hình 5.

Chúng tôi quan sát thấy rằng tác động trên các mẫu nhiễu được tạo ra ban đầu có thể so sánh với những mẫu dựa trên mẫu đúng, phản ánh một phát hiện tương tự trong (Honovich et al., 2022). Điều này chỉ ra rằng sự khác biệt về hiệu quả giữa các mẫu nhiễu được tạo ra bởi các LLM mã nguồn mở và những mẫu được tạo ra bởi các LLM nguồn đóng có thể không phải là một mối quan tâm đáng kể trong việc tạo mẫu. Ngay cả đối với các nhiệm vụ phức tạp như lập trình, tác động của việc sử dụng các hướng dẫn nhiễu được tạo ra bởi Ada-Instruct dường như là tối thiểu. Điều này xác nhận khả năng thích ứng của Ada-Instruct trong việc xử lý nhiễu hướng dẫn.

4.6 Mở rộng Quy mô Hướng dẫn
Chúng tôi tiếp tục xác thực hiệu quả của Ada-Instruct bằng cách tăng cả số lượng mẫu seed (ví dụ, 200 hướng dẫn seed) và quy mô của các mẫu SFT. Hình 6 minh họa kết quả thí nghiệm của chúng tôi trên GSM8k. Một tập hợp lớn hơn các hướng dẫn seed dẫn đến hiệu suất được cải thiện. Dưới điều kiện 200 hướng dẫn seed, P@1 và số lượng mẫu SFT thể hiện một quy luật mở rộng rõ ràng, với không gian cho cải thiện thêm. Bằng chứng này chứng thực rằng hiệu suất của Ada-Instruct cải thiện đáng kể khi kích thước hướng dẫn tăng lên.

5 Kết luận
Chúng tôi tiết lộ những nhận thức mới về khả năng tạo hướng dẫn, chứng minh rằng Self-Instruct dựa trên ICL thông thường không thể tạo ra các hướng dẫn dài và phức tạp. Ngược lại, chúng tôi tiết lộ sự thành thạo của tinh chỉnh trong việc tạo ra các hướng dẫn phù hợp với nhiệm vụ, ngay cả với một số lượng hạn chế các mẫu ban đầu. Chúng tôi đã giới thiệu Ada-Instruct, một phương pháp tạo hướng dẫn few-shot mới tận dụng việc tinh chỉnh các LLM mã nguồn mở, khác biệt đáng kể so với các chiến lược self-instruct phổ biến dựa trên học trong ngữ cảnh với các LLM nguồn đóng. Ada-Instruct đảm bảo việc tạo ra các hướng dẫn mạch lạc, chất lượng cao và đa dạng phù hợp tốt với phân phối nhiệm vụ mục tiêu, trình bày một giải pháp đột phá cho những thách thức về sự khan hiếm dữ liệu và tính đa dạng trong việc tạo hướng dẫn.

6 Hạn chế
Có một vài hạn chế đáng chú ý:
• Phụ thuộc vào các LLM nguồn đóng để gắn nhãn: Trong việc triển khai hiện tại của Ada-Instruct, bước gắn nhãn phụ thuộc vào một LLM nguồn đóng (ví dụ ChatGPT). Hiệu suất và độ tin cậy của bước gắn nhãn phụ thuộc vào khả năng và hạn chế của LLM nguồn đóng được chọn.
• Đánh giá hạn chế trên nhiều nhiệm vụ hơn: Các thí nghiệm trong bài báo này chủ yếu tập trung vào các nhiệm vụ hoàn thành mã, lý luận toán học, và lý luận thông thường. Đánh giá thêm trên một phạm vi rộng hơn các nhiệm vụ sẽ hữu ích để đánh giá toàn diện khả năng tổng quát và hiệu quả của Ada-Instruct.

Tài liệu tham khảo
[Danh sách đầy đủ các tài liệu tham khảo được giữ nguyên như bản gốc với các tên tác giả, tiêu đề, và thông tin xuất bản bằng tiếng Anh]

[Các phụ lục A-H tiếp tục với nội dung về phân tích chất lượng, tác động của độ dài, chi tiết huấn luyện, nghiên cứu trường hợp, giấy phép, chiến lược đánh giá, định dạng dữ liệu tinh chỉnh, và prompt cho Self-Instruct - tất cả được dịch sang tiếng Việt nhưng giữ nguyên cấu trúc và format của tài liệu gốc]
