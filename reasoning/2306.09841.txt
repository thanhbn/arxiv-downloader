# 2306.09841.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/reasoning/2306.09841.pdf
# File size: 12539306 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Are Large Language Models Really Good
Logical Reasoners? A Comprehensive
Evaluation and Beyond
Fangzhi Xu*, Qika Lin*, Member, IEEE , Jiawei Han, Tianzhe Zhao,
Jun Liu, Senior Member, IEEE , Erik Cambria, Fellow, IEEE
Abstract —Logical reasoning consistently plays a fundamental and significant role in the domains of knowledge engineering and
artificial intelligence. Recently, Large Language Models (LLMs) have emerged as a noteworthy innovation in natural language
processing (NLP). However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual
cognitive inference similar to human intelligence, remains unanswered. To this end, we aim to bridge this gap and provide
comprehensive evaluations in this paper. Firstly, to offer systematic evaluations, we select fifteen typical logical reasoning datasets and
organize them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of
evaluations, we include 3 early-era representative LLMs and 4 trending LLMs. Secondly, different from previous evaluations relying only
on simple metrics (e.g., accuracy ), we propose fine-level evaluations in objective and subjective manners, covering both answers and
explanations, including answer correctness ,explain correctness ,explain completeness andexplain redundancy . Additionally, to
uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., evidence selection
process andreasoning process . Thirdly, to avoid the influences of knowledge bias and concentrate purely on benchmarking the logical
reasoning capability of LLMs, we propose a new dataset with neutral content. Based on the in-depth evaluations, this paper finally
forms a general evaluation scheme of logical reasoning capability from six dimensions (i.e., Correct ,Rigorous ,Self-aware ,Active ,
Oriented andNo hallucination ). It reflects the pros and cons of LLMs and gives guiding directions for future works.
Index Terms —Logical reasoning, large language model, deductive reasoning, inductive reasoning, abductive reasoning.
✦
1 I NTRODUCTION
ASa fundamental and significant topic in the domains
of knowledge engineering and artificial intelligence,
logical reasoning has consistently remained a subject of
intense research interest [1], [2], [3]. Through the integration
of logical reasoning, a wide range of intelligent applications
can be developed (e.g., recommendation systems [4], rela-
tion prediction [5] and question generation [6]), which not
only offer powerful capabilities but also ensure natural in-
terpretability. However, developing efficient and robust log-
ical reasoning systems [7] remains a challenging academic
pursuit due to complexities in handling intricate semantic
and syntactic structures, managing symbolic knowledge,
executing high-level abstractions and inferences, and nav-
igating uncertainty and ambiguity [8], [9].
With the aid of large-scale pre-training, instruction fine-
tuning, and human feedback reinforcement learning strate-
gies [10], Large Language Models (LLMs) have made unpar-
•Fangzhi Xu, Qika Lin, Jiawei Han and Tianzhe Zhao are with
the School of Computer Science and Technology, Xi’an Jiao-
tong University, Xi’an, Shaanxi 710049, China (email: {Leo981106,
tara1208260223 }@stu.xjtu.edu.cn, {qikalin, ztz8758 }@foxmail.com).
•Jun Liu is with the Shaanxi Provincial Key Laboratory of Big Data
Knowledge Engineering, and National Engineering Lab for Big Data
Analytics, Xi’an, Shaanxi 710049, China (e-mail: liukeen@xjtu.edu.cn).
•Erik Cambria is with the School of Computer Science and Engineering,
Nanyang Technological University (e-mail: cambria@ntu.edu.sg).
•Fangzhi Xu and Qika Lin contribute equally to this paper.
•Corresponding author: Qika Lin.alleled strides in the artificial intelligence community, par-
ticularly in the field of natural language processing (NLP).
They have achieved remarkable performance in numerous
traditional NLP tasks [11], [12], such as question answer-
ing, information retrieval, and affective computing. Under
this circumstance, researchers have begun to question the
efficacy of LLMs in addressing complex logical reasoning
tasks [13], [14]and their ability to apply this knowledge
to downstream intelligent applications. As such, a natural
question arises: are large language models really good logical
reasoners ?
There are already several studies to evaluate the capabil-
ity of LLMs from various reasoning perspectives, e.g., mul-
tilingual reasoning [15], commonsense reasoning [16], and
mathematical reasoning [17]. These efforts are meaningful
and inspiring. Nevertheless, all of them are confronted with
one or more of the following defects: (1) There are no com-
prehensive evaluations, which are hampered by the issue of
lacking systematic category, limited LLMs for comparison,
and limited data samples for evaluation; (2) The majority
of evaluation works purely report the accuracy of answers,
which limits fine-level analysis on the reasoning process and
fails to explore the causes of mistakes; (3) Current logical
reasoning benchmarks may fail to purely evaluate logical
reasoning ability since the reasoning of LLMs can be affected
by the content; (4) There lack a complete evaluation system
or well-defined dimensions to comprehensively conclude
the logical reasoning capability of LLMs. Therefore, our
work aims to fill these gaps and provide a comprehensivearXiv:2306.09841v4  [cs.CL]  15 Sep 2024

--- PAGE 2 ---
2
Fig. 1: The overall architecture of the evaluation.
evaluation, where the overall architecture encompasses the
five main points as illustrated in Fig. 1: category with rea-
soning settings, main metrics for evaluation, perspectives of
error analysis, dimensions for measuring logical reasoning
capability, and new benchmark NeuLR with neutral con-
tent. In detail, we address the above limitations from the
following aspects.
Firstly, our work starts from systematic views and pro-
vides comprehensive evaluations. According to the classical
definition [18], logical reasoning can be mainly categorized
into three fundamental types, i.e., deductive, inductive and
abductive reasoning. They together form a complete chain
of reasoning, thus it is meaningful to evaluate LLMs from
these views. Based on it, our comprehensive evaluation is
reflected in three orthogonal dimensions. For the reasoning
type view, all the evaluated datasets are categorized into
four reasoning settings, i.e., deductive, inductive, abductive
and mixed-forms. The former three involve the indepen-
dent reasoning manner. For the dataset view, we include
fifteen typical logical reasoning datasets according to the
above categories. For the model view, we evaluate three
previous LLMs (i.e., text-davinci-003 [19], ChatGPT [20] and
BARD [21]) as well as four up-to-date LLMs (i.e., LLaMA3.1-
Chat1, Mistral-Instruct-v0.32, Claude-33and GPT-44).
Secondly, our work fills the blank in fine-level evalua-
tions of logical reasoning tasks. Current benchmarks only
rely on a few objective metrics (e.g., accuracy) to measure
the model capability. It may not be sufficient in the case of
generative LLMs, since the role of LLMs is not only limited
to correctly answer questions but also serves as practical
tools, which are required to provide reasoning chains or
explanations. Previous works [22], [23] conduct extensive
experiments on popular NLP datasets, but they purely
report the performance results. Since some LLMs (e.g., Chat-
GPT) function as the interactive tools for human use, it is
necessary to introduce subjective metrics to do fine-grained
evaluations. In this paper, we employ four dimensions of
metrics, covering answer correctness ,explanation correctness ,
1. https://llama.meta.com/
2. https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3
3. https://claude.ai/
4. https://openai.com/index/gpt-4/explanation completeness and explanation redundancy . It can
provide more meaningful and complete evaluations from
both objective and subjective views. Considering problem-
atic cases (i.e., wrong answer or wrong explanation) can
reflect obvious logical flaws of LLMs, we further attribute
them to several error types from two dimensions of evidence
selection process and reasoning process and give in-depth anal-
ysis. Notably, the whole evaluation system can be applied
under both human-annotation and automatic modes.
Thirdly, our work focuses on the issue of content neu-
trality and provides new solutions. The current benchmark
for evaluating logical reasoning ability is strongly coupled
with text comprehension; in other words, the rule reason-
ing process of LLMs may be affected by the content in
the inputs, which limits the test of real logical reasoning
ability. Also, LLMs are highly powerful due to their massive
training data, which may overlap with popular benchmarks.
As a result, testing LLMs on these benchmarks may not be
entirely fair, as it can only demonstrate the fitting ability
of LLMs rather than their real logical reasoning capability.
Therefore, language models may be trained to learn a biased
pattern from text, rather than really capture the logical rea-
soning capability. Some previous works [24], [25] propose
to establish complete benchmarks for LLMs. But few works
focus on logical reasoning and fail to attend to the content-
neutral problem. To narrow this gap, we propose a new
dataset named NeuLR, which contains 3,000 content-neutral
samples and covers the deductive, inductive and abductive
reasoning types. It is expected to offer a novel perspective
for benchmarking the logical reasoning ability of LLMs.
Finally, we conclude the extensive performance results of
LLMs and form an evaluation scheme with six key proper-
ties, i.e., Correct ,Rigorous ,Self-aware ,Active ,Oriented and No
Hallucination . Among them, Correct purely measures the ac-
curacy of the answer. Rigorous measures whether LLMs give
both correct answers and complete and correct explanations.
Self-aware is reflected by the redundancy of the generated
content. Active is measured by the proportion of reasoning.
Oriented illustrates whether LLMs can reason from the right
perspectives. No hallucination measures whether LLMs are
more prone to produce hallucinations. The above dimen-
sions can all be quantified from existing evaluation ex-
periments. For deductive, inductive, abductive, and mixed
reasoning settings respectively, we visualize the ability maps
for each LLM. It is meaningful to identify the strengths and
weaknesses of LLMs under the four reasoning settings, thus
guiding future directions.
The main contributions of the paper are listed as follows:
(1) In view of the great success of LLMs in massive
NLP tasks, our work is targeted at answering are LLMs
really good logical reasoners? . In this paper, we provide a
comprehensive evaluation and give potential directions for
future researches.
(2) For a comprehensive evaluation of logical reasoning,
this paper classifies datasets into four reasoning manners,
i.e., deductive, inductive, abductive, and mixed-form. We in-
clude fifteen typical logical reasoning datasets and evaluate
seven representative LLMs under diverse prompting strate-
gies, including 3 early-era LLMs (text-davinci-003, ChatGPT,
and BARD) and 4 trending LLMs (LLaMA3.1-Chat, Mistral-
Instruct-v0.3, Claude-3 and GPT-4).

--- PAGE 3 ---
3
(3) Considering the drawbacks in current objective met-
rics, this paper gives fine-level evaluations including four
dimensions i.e., answer correctness, explanation correctness,
explanation completeness, and explanation redundancy. To
explore the value of failure cases, we attribute them to
several error types and explore the logical flaws of LLMs.
(4) To provide fair evaluations with neutral content and
decouple logical reasoning from text understanding, this
paper proposes a new dataset named NeuLR5. It contains
3,000 content-neutral samples and covers deductive, induc-
tive and abductive reasoning manners.
(5) In view of the evaluation results, this paper forms a
general evaluation scheme for the logical reasoning capabil-
ity of LLMs for the first time, which concludes six key prop-
erties, i.e, Correct ,Rigorous ,Self-aware ,Active ,Oriented and
No hallucination . Furthermore, we derive the ability maps
for each LLM under four reasoning settings respectively and
propose future directions.
2 P RELIMINARY
Logical reasoning aims to generate logical implications that
contain new facts using one- or multi-step inference based
on given premises [26], [27], [28], i.e., premise⇒conclusion .
Elements of logical reasoning typically include knowledge
facts or premises and logical rules, for example:
•rule: All eight-year-old children are in primary school.
•fact1/premise1: Jordan is a child of eight years old.
•fact2/premise2: Jordan is in primary school.
According to the reasoning classification system of clas-
sical logic [18], there are three major types of logical reason-
ing: deductive, inductive and abductive. Drawing upon the
aforementioned rule and facts, we can depict the objective of
these three types of reasoning as forecasting the third item
based on the two provided ones.
Deductive Reasoning . Deductive reasoning is the psy-
chological process of drawing deductive inferences that start
from the given premises and reason with logical rules or
commonsense to obtain certain conclusions [29], [30]. It can
bepremise1 +rule⇒premise2 . Fig. 2 presents an example of de-
ductive reasoning. Its reasoning progress generates specific
knowledge facts from general counterparts, e.g., premise2
and rule are specific and general knowledge, respectively.
Therefore, deductive reasoning is actually a top-down way.
Inductive Reasoning . Distinct from deductive reason-
ing, inductive reasoning derives general principles from
a body of observations which means making broad
generalizations based on specific observations [31], [32].
For example, an example of inductive reasoning can be
premise1 +premise2 ⇒rule, concluding generalized knowledge
rule that is independent with specific item Jordan . Generally,
the truth of the conclusion of an inductive argument is
probable rather than certain in inductive reasoning. Thus, in-
ductive reasoning is a bottom-up approach and is contrasted
with deductive reasoning.
Abductive Reasoning . Formally, abductive reasoning
is similar to deductive reasoning which seeks conclusions
from a set of observations. But differently, its target is to
5. Available at https://github.com/DeepReasoning/NeuLR
Fig. 2: An example of using an LLM to answer reasoning
questions. The red words represent the generated results,
while the blue ones represent the generated explanations.
generate the simplest and most likely explanation for the
given observations [33], [34]. So the result is probable like
in inductive reasoning. An example of abductive reasoning
can be premise2 +rule⇒premise1 , which means premise1 is the
most likely cause of premise2 .
Mixed-form Reasoning. Numerous real-life reasoning
scenarios may require multiple steps and the incorporation
of at least two of these three types. In this paper, they
are vieweds as a more intricate type of reasoning, namely,
mixed-form . For example, given the following fact: 1) James is
a crow and is black; 2) Sophia is a crow and is black; 3) Emily
is a crow. To answer the question what color is Emily? , we
should first get the conclusion that all crows are black using
inductive reasoning and then conduct deductive reasoning
on this conclusion and fact 3) to obtain the final answer black .
3 E VALUATION DETAILS
This section presents the comprehensive experimental set-
tings, including the models, datasets and metrics. More
detailed prompts for zero- or few-shot are shown in Table 3
of the Appendix6.
3.1 Evaluated Models
Because of the rapid emergence of LLMs, it is not real-
istic to include all LLMs in this paper. Thus, we mainly
select three early-era representative LLMs (i.e., text-davinci-
003, ChatGPT, and BARD) and four up-to-date LLMs (i.e.,
LLaMA3.1-Chat, Mistral-Instruct-v0.3, Claude-3 and GPT-
4). The details of these models are listed in the Appendix.
3.2 Evaluated Datasets
According to the previous discussion, the evaluation is
conducted systematically from deductive, inductive, abduc-
tive and mixed-form views. Therefore, this paper selects
fifteen popular datasets in logical reasoning and divides
them into the above four folds. Table 2 in the Appendix
presents detailed information of these datasets. The selected
datasets contain both generation and classification ones
6. Appendix is at: https://github.com/DeepReasoning/NeuLR

--- PAGE 4 ---
4
and there exist diverse forms of tasks, which illustrate
the comprehensiveness of our evaluation. Diverging from
prior works that only employ a limited number of samples,
this paper significantly expands the evaluation size. Since
ChatGPT is one of the most popular LLM for the public,
we give much focus on it. For parts of the datasets, we
keep all the test examples for ChatGPT evaluation (i.e.,
EntailmentBank [35], FOLIO [36], Leap-Of-Thought [37],
CLUTRR [38], ReClor [39], LogiQA [40], LogiQA 2.0 [41]
and LogiQA2NLI [41]), while other large datasets (i.e., bAbI-
15 [42], RuleTaker [43], bAbI-16 [42], α-NLI [44], α-NLG [44],
AbductiveRules [45] and D*-Ab [46]) are sampled to 1,000
examples. As for the evaluation of text-davinci-003 and
BARD, we sample 100 test examples for each dataset.
3.3 Selected Metrics
The majority of prior evaluation studies solely report the
accuracy metric, which may not be comprehensive enough
for evaluating the effectiveness of LLMs. Consequently, we
assert that a more nuanced assessment is necessary. To
this end, we propose to evaluate LLMs from both objec-
tive and subjective perspectives. Specifically, we introduce
four evaluation metrics to reflect the intermediate reasoning
process of LLMs: answer correctness ,explanation correctness ,
explanation completeness and explanation redundancy .
•Answer Correctness . This metric measures the align-
ment between a generated answer and the true label,
emphasizing semantic coherence over exact token
matching. For example, ”the car hit the tree” and
”the tree was struck by the car” demonstrate this
alignment despite differing expression style.
•Explanation Correctness . To reason towards the an-
swer, explanations usually provide a logical and co-
herent narrative that connects the available evidence
or premises to the ultimate conclusion. Thus this
metric indicates whether the generated explanation
is logically correct for the true answer.
•Explanation Completeness . It indicates whether
there are any missing explanations to reason towards
the true answer. An explanation is deemed complete
if it encompasses all elements present in the true
explanation, which means that in the reasoning pro-
cess, the correct answer can be inferred through the
selected known facts and the generated intermediate
facts. But this does not necessarily cause answer
correctness or explanation correctness.
•Explanation Redundancy . This suggests that the
combination of selected established facts and ad-
ditional intermediate facts surpasses the necessary
information for arriving at a precise conclusion, lead-
ing to the inclusion of unnecessary details in the
reasoning process. For instance, only three specific
facts are essential for drawing a conclusion; any extra
facts beyond this set would be redundant.
Notably, the metric values of explanation correctness,
completeness, and redundancy are independent. To identify
prevalent logical flaws in LLMs, we establish error types
to categorize problematic cases. This paper classifies errors
along two primary dimensions: (1) evidence selection processand (2) reasoning process . The first dimension centers on
evaluating the evidence selected by LLMs, whereas the
second dimension emphasizes the logical reasoning process
using the selected evidence. Detailedly, evidence selection
process category can be further divided into wrong selection
and hallucination . The former denotes that LLMs select the
wrong facts or ignore the necessary facts from the beginning
of the reasoning. The latter denotes that LLMs select the
evidence which contradicts the given context or can not
be verified by the context. Reasoning process category can
be further divided into no reasoning ,perspective mistake and
process mistake . The first error type signifies instances where
LLMs fail to conduct reasoning, instead merely listing the
given facts and the final answer. The second denotes LLMs
starting from an irrelevant point or focusing on an improper
perspective for the correct answer. The last refers to LLMs
commencing from a proper viewpoint, but making mistakes
during the reasoning process.
4 O VERALL EXPERIMENTS
In this section, we conduct evaluation experiments on three
early-era LLMs (i.e., text-davinci-003, ChatGPT and BARD)
and four trending LLMs (i.e., LLaMA3.1-Chat, Mistral-
Instruct-v0.3, Claude-3 and GPT-4) under various prompt-
ing methods. Table 1 presents the overall answer correctness
of these three early-stage LLMs on fifteen logical reasoning
datasets. Table 2 supplements the performances of four
popular LLMs. Generally, LLMs’ performances on logical
reasoning tasks still have significant room for improvement
in comparison to the state-of-the-art (SOTA) metric. Most
of the results fall short of those achieved by smaller-sized
SOTA models. We provide a detailed analysis of the results
from the following perspectives. For simplicity, we only in-
clude text-davinci-003, ChatGPT and BARD for discussion.
Firstly, we conduct an analysis of LLMs’ performances
across four reasoning manners, solely focusing on the zero-
shot results to facilitate a clear comparison. Furthermore,
we introduce the relative performance metric (i.e., LLM
accuracy/SOTA) to reflect the relative capability of LLMs
in comparison to SOTA performances in Fig. 3. We also
calculate the weighted results of four reasoning manners in
Fig. 4a. From the results, ChatGPT performs worse in deduc-
tive and inductive settings compared with text-davinci-003
and BARD. In the abductive setting, three LLMs show com-
parable performances and BARD wins with slight advan-
tages. In the mixed-form setting, ChatGPT performs better
and BARD ranks second. Overall, BARD shows consistent
superiority among deductive, inductive and abductive
settings, while text-davinci-003 also does relatively well.
It seems that ChatGPT struggles in the three settings, but is
better at mixed-form reasoning. Also, we compare the LLM
performances between deductive, inductive and abductive
settings. LLMs do best in deductive setting, while they
mostly struggle in inductive setting. We argue that deduc-
tive and abductive reasoning align with typical NLP scenar-
ios, where LLMs have to provide missing facts. Conversely,
inductive reasoning necessitates extracting high-level rules
or knowledge from the given facts, which is more intricate
and may not be readily available in the training corpus.

--- PAGE 5 ---
5
TABLE 1: Overall results of LLMs’ answer correctness across the zero-shot, one-shot and three-shot logical reasoning
settings. The notations De.,In.,Ab.and Mix correspond to deductive, inductive, abductive and mixed-form reasoning,
respectively (as in the following tables and figures). Gen. indicates whether the task is a generation one. The percentage
signs (%) of performance values are omitted for simplicity in the paper.
Dataset Gen.text-davinci-003 ChatGPT BARDSOTA0-shot 1-shot 3-shot 0-shot 1-shot 3-shot 0-shot 1-shot 3-shotDe.bAbI-15 ✓ 85.00 76.00 75.00 38.40 46.40 39.70 79.00 80.00 88.00 100 [42]
EntailmentBank ✓ 93.00 88.00 89.00 83.82 82.06 77.94 96.00 97.00 97.00 100 [35]
RuleTaker 64.00 60.00 62.00 42.00 38.00 40.20 64.00 57.00 70.00 ≈100 [43]
FOLIO 48.00 53.00 52.00 50.00 50.98 54.41 52.00 43.00 49.00 62.11 [36]
Leap-Of-Thought 82.00 90.00 87.00 72.61 74.01 61.21 79.00 72.00 79.00 99.7 [37]In.bAbI-16 ✓ 84.00 81.00 74.00 17.10 24.70 12.90 73.00 44.00 52.00 100 [42]
CLUTRR ✓ 6.00 23.00 20.00 21.99 19.55 12.83 23.00 26.00 24.00 95.0 [47]Ab.α-NLI 74.00 70.00 74.00 80.90 80.00 79.10 75.00 74.00 77.00 68.90 [44]
α-NLG ✓ 9.00 10.00 12.00 21.90 23.40 25.90 10.00 12.00 15.00 45.00 [44]
AbductiveRules ✓ 75.00 42.00 35.00 23.30 35.10 29.80 71.00 49.00 22.00 100 [45]
D*-Ab ✓ 8.00 21.00 23.00 11.60 2.50 1.80 11.00 0.00 0.00 ≥95 [46]MixReClor 53.00 53.00 55.00 58.80 56.00 58.80 56.00 55.00 56.00 75.00 [13]
LogiQA 41.00 35.00 39.00 40.25 39.48 40.86 48.00 46.00 47.00 46.10 [13]
LogiQA 2.0 43.00 42.00 41.00 54.60 50.80 54.80 53.00 46.00 47.00 72.25 [41]
LogiQA2NLI 59.00 55.00 58.00 57.83 53.83 57.00 48.00 50.00 47.00 ≈80 [41]
TABLE 2: Supplementary results on the four trending LLMs across diverse prompt selections. In the implementation, Direct
utilizes the zero-shot prompting to output both explanations and answers, but not prompted with chain-of-thought. The
values in the table represent the answer correctness.
Dataset Gen.LLaMA3.1-Chat Mistral-Ins-v0.3 Claude-3.5 GPT-4
Direct COT Direct COT Direct COT Direct COTDe.bAbI-15 ✓ 89.00 90.00 59.00 51.00 97.00 100.00 98.00 98.00
EntailmentBank ✓ 72.00 74.00 82.00 86.00 85.00 73.00 84.00 77.00
RuleTaker 62.00 64.00 45.00 64.00 61.00 64.00 59.00 67.00
FOLIO 53.00 56.00 48.00 45.00 81.00 84.00 70.00 69.00
Leap-Of-Thought 81.00 80.00 78.00 73.00 55.00 53.00 75.00 80.00In.bAbI-16 ✓ 92.00 88.00 36.00 23.00 80.00 86.00 91.00 92.00
CLUTRR ✓ 39.00 40.00 15.00 20.00 29.00 21.00 33.00 26.00Ab.α-NLI 39.00 37.00 31.00 28.00 36.00 38.00 37.00 38.00
α-NLG ✓ 17.00 17.00 16.00 15.00 26.00 14.00 29.00 23.00
AbductiveRules ✓ 42.00 22.00 35.00 25.00 49.00 41.00 40.00 34.18
D*-Ab ✓ 10.00 24.00 6.00 3.00 38.00 27.00 35.00 33.00MixReClor 67.00 63.00 55.00 52.00 88.00 90.00 88.00 85.00
LogiQA 46.54 44.55 50.23 42.70 64.00 68.00 64.00 65.00
LogiQA 2.0 63.00 55.00 45.00 47.00 74.00 79.00 81.00 82.00
LogiQA2NLI 61.00 51.00 54.00 54.00 60.00 61.00 59.00 48.00
Fig. 3: LLM performances on different datasets.
Secondly, we conduct a detailed analysis of LLMs’ per-
formances from the generation and classification perspec-
tives in Fig. 4b. In general, classification scenarios tend
to yield better performance than generation counterparts.
(a) Different reasoning types.
 (b) Generation/Classification.
Fig. 4: Visualization on the metric of answer correctness.
Notably, ChatGPT exhibits particularly poor results in
generation tasks , such as bAbI-15, bAbI-16, CLUTRR, Ab-

--- PAGE 6 ---
6
(a) Rigorous evaluation.
 (b) Self-aware evaluation.
Fig. 5: Heatmap visualization of rigor and self-awareness.
ductiveRules and D*-Ab. This observation may result from
the fact that ChatGPT is designed to improve chatting
capability rather than complex reasoning, which can lead to
performance degradation in pure generative logic reasoning
scenarios.
Thirdly, few-shot in-context learning (ICL) [48] does not
necessarily bring improvements in logical reasoning tasks.
It is quite inconsistent with the cases in other non-reasoning
NLP tasks, such as topic classification and sentiment anal-
ysis [49]. We count the cases where LLMs can continuously
obtain the performance gains from few-shot ICL (i.e., 0-shot
<1-shot <3-shot). For text-davinci-003, only two (out of
four) abductive datasets continuously benefit from the few-
shot ICL. ChatGPT witnesses performance improvements
only in one (out of five) deductive dataset and one (out of
four) abductive dataset. For BARD, few-shot ICL helps two
(out of five) deductive datasets and one (out of four) abduc-
tive datasets. Remarkably, few-shot ICL fails to provide
consistent benefits for LLMs under inductive reasoning
and mixed-form reasoning manners. We argue that induc-
tive and mixed-form settings require more complex and
high-order reasoning ability, which may be difficult to learn
with a few question-answer samples without rationales. But
the task form of deductive and abductive reasoning is easy
to follow, which provides the application potential for few-
shot ICL.
5 F INE-LEVEL EVALUATIONS
This section presents a detailed analysis of the logical rea-
soning capabilities of LLMs from various perspectives.
5.1 Are LLMs Rigorous Logical Reasoning?
While LLMs may produce correct answers in some cases, it
is unclear whether they perform the correct logical reason-
ing or simply arrive at the right answer by chance. There-
fore, we delve deeper into the reasoning process beyond the
output answers. We view cases where LLMs provide a cor-
rect answer along with a correct and complete explanation
asRigorous . The detailed results are shown in Table 3. Com-
pared with the simple judgment of answer correctness, all
selected LLMs present obvious performance drops. For sim-
plicity, we only take the zero-shot setting into consideration.
In Fig. 5a, we calculate the ratio of rigorous performance and
answer accuracy. The higher values (darker colors) mean
better performance in rigorous reasoning. According to the
results, BARD shows the best capability in rigorous reason-
ing, consistently under four reasoning manners. Meanwhile,ChatGPT still struggles in deductive and inductive settings,
while text-davinci-003 comes last in both abductive and
mixed-form manners.
Further, LLMs are best at keeping rigorous reasoning in
the abductive setting, while they are weak in the deductive
and inductive settings. The finding is a little different from
the analysis of simple accuracy conditions in the previous
section. We argue that the setting of abductive reasoning
requires the LLMs to achieve the reasoning reversely, which
can activate LLMs to provide sufficient reasoning process.
In a deductive reasoning setting, the reasoning chain is
sequential, which may cause LLMs to be in a lazy mode
and harm rigorous reasoning.
In Table 3, we also include the two conditions (1) when
correct answer and correct explanations are satisfied, and
(2) when correct answer and complete explanations are sat-
isfied. Results vary a lot with different datasets and different
LLMs. Overall, ChatGPT performs relatively well in keeping
correct explanations while it may fail to maintain complete
explanations in most cases. In comparison, text-davinci-003
exhibits stronger characteristics in maintaining the com-
pleteness of explanations, compared with the correctness
of explanations. These findings of the respective reasoning
preferences are expected to guide the future utilization of
LLMs.
5.2 Are LLMs Self-aware Logical Reasoners?
From an alternative perspective, the redundancy of the
generated content by LLMs has been a frequently discussed
topic, as it is deemed an important metric for assessing
their practicality. In this paper, we consider LLMs with less
redundant content as more self-aware , as they can effectively
express the necessary information without outputting all
possible answers. Table 4 presents the evaluation results
of LLMs’ self-awareness. Similar to our previous approach,
we compute the weighted results for each reasoning setting
and derive the self-awareness scores shown in Fig. 5b. The
darker color indicates a stronger self-awareness capabil-
ity. Results indicate that text-davinci-003 exhibits notable
advantages, particularly in the inductive, abductive, and
mixed-form reasoning settings. Additionally, it ranks second
in the deductive setting. Conversely, BARD performs poorly
in deductive, abductive, and mixed-form reasoning settings.
In comparison to classification tasks, LLMs tend to
generate redundant answers more frequently in generation
tasks, such as α-NLI vs. α-NLG. This is because open-
ended questions can prompt LLMs to generate content from
various perspectives, which can lead to the inclusion of
redundant information. Furthermore, the mixed-form rea-
soning setting observes significantly fewer instances of re-
dundancy. The tasks in mixed-form reasoning are primarily
based on question answering, which closely resembles real-
life text, and LLMs tend to generate rational and specific
content in such scenarios. However, in other settings, the
input context is elaborately designed for logical reasoning
and may provide sufficient background information. This
can result in LLMs employing embodied commonsense
knowledge to help reason and thus generate additional
explanations. Notably, considering that current LLMs do a
lot of optimizations for preference alignment, it is inevitable

--- PAGE 7 ---
7
TABLE 3: Evaluations on whether LLMs are rigorous reasoners. For each dataset, the first row of results represents the
performances when LLMs give the correct answer, correct explanation as well as complete explanation simultaneously.
The values in the subscripts denote the drops compared with only distinguishing the answer correctness. The second
row of results represents the performances when LLMs give both correct answers and correct explanations, regardless
of the explanation completeness. The third row represents the cases when LLMs give correct answers and list complete
explanations, regardless of the explanation correctness.
Datasettext-davinci-003 ChatGPT BARD
0-shot 1-shot 3-shot 0-shot 1-shot 3-shot 0-shot 1-shot 3-shotDeductivebAbI-1553.00 32.00↓60.00 16.00↓64.00 11.00↓25.50 12.90↓12.10 34.30↓14.10 25.60↓45.00 34.00↓25.00 55.00↓47.00 41.00↓
61.00 66.00 68.00 32.10 12.90 16.40 77.00 74.00 85.00
56.00 60.00 64.00 27.90 18.00 17.60 45.00 25.00 47.00
EntailmentBank29.00 64.00↓37.00 51.00↓30.00 59.00↓25.88 57.94↓20.00 62.06↓10.59 67.35↓26.00 38.00↓25.00 32.00↓33.00 37.00↓
29.00 37.00 30.00 25.88 20.00 10.59 54.00 66.00 71.00
72.00 73.00 75.00 62.06 57.65 31.76 94.00 96.00 97.00
RuleTaker35.30 6.70↓22.50 15.50↓24.80 15.40↓25.88 57.94↓20.00 62.06↓10.59 67.35↓54.00 42.00↓66.00 31.00↓71.00 26.00↓
36.20 24.00 26.00 25.88 20.00 10.59 26.00 28.00 33.00
36.00 23.20 26.00 62.06 57.65 31.76 26.00 25.00 34.00
FOLIO27.00 21.00↓25.00 28.00↓25.00 27.00↓28.92 21.08↓27.94 23.04↓27.94 26.47↓21.00 31.00↓19.00 24.00↓20.00 29.00↓
28.00 25.00 26.00 28.92 27.94 27.94 23.00 19.00 22.00
27.00 28.00 27.00 33.33 32.35 35.29 25.00 22.00 23.00
Leap-of-Thought29.00 53.00↓43.00 47.00↓38.00 49.00↓70.60 2.02↓24.36 49.65↓28.86 32.35↓76.00 3.00↓69.00 3.00↓77.00 2.00↓
63.00 63.00 58.00 71.22 24.83 29.79 76.00 69.00 77.00
32.00 46.00 42.00 70.99 48.33 40.88 79.00 71.00 7.00InductivebAbI-1659.00 25.00↓35.00 46.00↓23.00 51.00↓8.30 8.80↓8.20 16.50↓2.60 10.30↓24.00 49.00↓15.00 29.00↓16.00 36.00↓
67.00 50.00 37.00 10.20 8.40 3.10 58.00 32.00 24.00
65.00 44.00 35.00 10.00 9.40 3.50 32.00 22.00 24.00
CLUTRR2.00 4.00↓7.00 16.00↓6.00 14.00↓7.85 14.14↓4.62 14.92↓2.71 10.12↓19.00 4.00↓25.00 1.00↓23.00 1.00↓
2.00 7.00 6.00 7.94 4.62 2.71 19.00 25.00 23.00
6.00 18.00 18.00 10.56 6.72 3.75 21.00 26.00 23.00Abductiveα-NLI68.00 6.00↓69.00 1.00↓68.00 6.00↓77.50 3.40↓64.50 15.50↓58.40 20.70↓75.00 0.00−71.00 3.00↓77.00 0.00−
70.00 69.00 68.00 78.20 66.70 60.90 75.00 71.00 77.00
68.00 69.00 68.00 77.60 64.50 58.40 75.00 71.00 77.00
α-NLG1.00 8.00↓0.00 10.00↓2.00 10.00↓15.30 6.60↓16.00 7.40↓10.30 15.60↓7.00 3.00↓8.00 4.00↓ 9.00 6.00↓
1.00 0.00 2.00 15.30 16.00 10.40 7.00 9.00 10.00
8.00 8.00 9.00 20.90 22.40 21.40 10.00 9.00 14.00
AbductiveRules50.00 25.00↓5.00 37.00↓0.00 35.00↓12.00 11.30↓18.30 16.80↓5.70 24.10↓57.00 14.00↓30.00 19.00↓10.00 12.00↓
75.00 10.00 0.00 20.50 29.60 9.50 65.00 40.00 18.00
50.00 5.00 0.00 13.00 19.90 5.80 57.00 30.00 10.00
D*-Ab4.00 4.00↓5.00 16.00↓4.00 19.00↓4.10 7.50↓1.20 1.30↓ 1.10 0.70↓ 4.00 7.00↓0.00 0.00−0.00 0.00−
7.00 7.00 7.00 5.50 1.50 1.20 4.00 0.00 0.00
4.00 5.00 4.00 4.50 1.20 1.10 5.00 0.00 0.00Mixed-formReClor5.00 48.00↓0.00 53.00↓0.00 55.00↓28.60 30.20↓25.20 30.80↓29.80 29.00↓38.00 18.00↓34.00 21.00↓33.00 23.00↓
5.00 0.00 0.00 32.40 28.00 32.20 48.00 46.00 42.00
42.00 46.00 42.00 50.00 32.80 46.00 38.00 36.00 38.00
LogiQA5.00 36.00↓2.00 33.00↓1.00 38.00↓25.96 14.29↓21.35 18.13↓20.43 20.43↓29.00 19.00↓26.00 20.00↓19.00 28.00↓
6.00 2.00 1.00 28.42 23.20 23.81 34.00 27.00 23.00
23.00 26.00 27.00 29.19 27.50 26.42 30.00 35.00 27.00
LogiQA2.038.00 5.00↓32.00 10.00↓28.00 13.00↓43.40 11.20↓34.60 16.20↓38.80 16.00↓47.00 6.00↓39.00 7.00↓44.00 3.00↓
39.00 32.00 28.00 43.80 35.20 38.80 47.00 39.00 44.00
39.00 32.00 30.00 44.20 36.80 40.60 47.00 39.00 44.00
LogiQA2NLI57.00 2.00↓51.00 4.00↓56.00 2.00↓43.17 14.67↓36.33 17.50↓36.50 20.50↓38.00 10.00↓41.00 9.00↓37.00 10.00↓
57.00 51.00 56.00 43.50 36.50 36.83 40.00 43.00 40.00
57.00 51.00 56.00 53.50 49.00 50.16 38.00 41.00 37.00
that they would produce some redundant information to
cover diverse conditions.
5.3 Do LLMs Have Obvious Logical Flaws?
Based on the preceding statements, we establish error types
for problematic cases (with incorrect explanations) from two
dimensions: evidence selection process and reasoning process .
The former dimension encompasses two error types: (i)
wrong selection and (ii) hallucination , which are independent
of each other. The latter dimension comprises three error
types: (i) no reasoning , (ii) perspective mistake and (iii) processmistake . Each problematic case can only be attributed to one
of the three errors in the reasoning process dimension.
We visualize the attribution results for fourteen datasets
in Fig. 6, including four deductive, two inductive, four
abductive and four mixed-form ones. Overall speaking,
the types of errors vary between datasets. For the wrong
selection , 33.26% of the problematic cases fail to select the
right answers for reasoning. Also, 27.46% suffers from the
hallucination issue of LLMs. From the dimension of reason-
ing process ,no reasoning error keeps a small portion in most
of the cases, only covering 19.33% of the selected cases in

--- PAGE 8 ---
8
TABLE 4: Evaluation results on the metric of explanation redundancy.
Dataset Gen.text-davinci-003 ChatGPT BARD
0-shot 1-shot 3-shot 0-shot 1-shot 3-shot 0-shot 1-shot 3-shotDe.bAbI-15 ✓ 63.00 56.00 43.00 22.60 39.40 55.70 99.00 84.00 62.00
EntailmentBank ✓ 8.00 6.00 7.00 7.06 5.88 3.24 26.00 25.00 28.00
RuleTaker 26.00 29.00 27.00 21.30 27.80 34.80 80.00 84.00 75.00
FOLIO 14.00 23.00 21.00 31.86 22.55 19.61 60.00 63.00 68.00
Leap-Of-Thought 71.00 55.00 54.00 32.74 5.04 4.73 2.00 2.00 0.00In.bAbI-16 ✓ 60.00 77.00 86.00 93.60 29.80 41.20 96.00 98.00 99.00
CLUTRR ✓ 2.00 28.00 31.00 2.62 1.57 0.87 2.00 6.00 14.00Ab.α-NLI 2.00 2.00 1.00 1.00 0.20 0.10 8.00 16.00 0.00
α-NLG ✓ 63.00 61.00 72.00 70.70 69.70 64.50 24.00 32.00 31.00
AbductiveRules ✓ 1.00 0.00 0.00 42.40 5.40 0.50 67.00 48.00 22.00
D*-Ab ✓ 85.00 27.00 17.00 55.30 27.10 16.70 18.00 16.00 2.00MixReClor 1.00 1.00 1.00 2.00 1.20 1.40 11.00 16.00 24.00
LogiQA 0.00 5.00 0.00 1.54 0.77 1.08 32.00 35.00 43.00
LogiQA 2.0 0.00 0.00 0.00 0.80 4.00 0.8 5.00 5.00 4.00
LogiQA2NLI 0.00 0.00 0.00 0.17 0.50 0.17 11.00 31.00 4.00
total. Meanwhile, perspective mistake occupies 44.47% of the
cases and process mistake covers 36.20%.
The primary challenge evident from the results lies in
the ability of LLMs to locate accurate evidence and perspec-
tives for logical reasoning tasks. Due to the limited content
of inputs, LLMs are also prone to generate hallucinatory
facts to aid the reasoning process, which may affect the
reliability of LLMs in real applications. In addition, LLMs
abandon reasoning in a considerable number of cases. Such
phenomena of laziness are worth noting, especially when
we depend on LLMs to help reason in the downstream
tasks. In the following section, we will provide a detailed
analysis of specific LLM and specific reasoning settings.
Considering the above-mentioned obvious logical flaws, we
will especially focus on the detailed analysis of the activity
in reasoning, the orientation selection of LLMs and model
hallucination.
5.4 Are LLMs Active Logical Reasoners?
In this paper, we consider LLMs with fewer no reasoning
errors as more Active logical reasoners. Fig. 15a presents
the weighted results for measuring active reasoning cases,
where higher values indicate LLMs are more active in rea-
soning, while lower values represent lazier cases. Among
the three LLMs, BARD is the most active logical reasoner,
excelling in deductive, inductive and abductive settings.
ChatGPT, on the other hand, is deemed the lazier reasoner
in deductive and inductive settings, while text-davinci-003
is lazier in abductive reasoning tasks.
Furthermore, we compare the performances of LLMs
across different reasoning modes. In deductive reasoning
tasks, LLMs exhibit more active reasoning, while in ab-
ductive settings, they tend to display lazier performances.
Deductive tasks are in a forward reasoning mode, which is
more natural for both generative LLMs and humans. This
can inspire LLMs to generate effective reasoning chains.
Conversely, abductive reasoning requires LLMs to provide
explanations for the given inputs, which is in a backward
reasoning mode. It is intuitive that LLMs may struggle to
conduct reasoning in some cases.5.5 Are LLMs Oriented Logical Reasoners?
At the outset of the reasoning process, it is crucial to
identify the correct starting points and potential directions
for reasoning. We consider LLMs with this capability as
Oriented logical reasoners and present evaluation results
based on the error type of perspective mistake in Fig. 15b.
From the heatmap results, ChatGPT exhibits better oriented
capability in deductive and inductive tasks compared to
the other two LLMs. However, it frequently fails to identify
the correct reasoning direction in abductive and mixed-form
reasoning. Conversely, BARD performs well in identifying
the right direction in abductive and mixed-form settings but
struggles in deductive and inductive ones. Compared to the
others, text-davinci-003 displays moderate performance in
identifying reasoning perspectives.
In light of the preceding findings, ChatGPT is a lazier
logical reasoner, but it excels at identifying the correct
direction for reasoning. In other words, ChatGPT is prone
to conduct confident reasoning. Conversely, text-davinci-003
and BARD are active in logical reasoning, but they tend to
start from wrong directions, leading to reasoning mistakes.
The balance of active and oriented reasoning gives an
insightful direction in exploring the knowledge and ability
boundary of LLMs. This implies that LLMs must under-
stand their capabilities and limitations, avoiding tasks be-
yond their scope.
5.6 Are LLMs Easy to Induce Hallucination in Logical
Reasoning?
In the typical definition, hallucination refers to generated
content that contradicts commonsense or current facts. To
align with logical reasoning tasks, we expand the definition
to include cases where facts are employed that contradict the
context or are not verified by the context. Fig. 15c displays
the weighted performances of cases with no hallucinations.
The darker color indicates better performance in avoid-
ing hallucinations. Based on the results, ChatGPT exhibits
strong and consistent competitiveness, ranking first in de-
ductive, inductive, and mixed-form settings. It also ranks

--- PAGE 9 ---
9
(a) bAbI15 (Deductive).
 (b) RuleTaker (Deductive).
(c) FOLIO (Deductive).
 (d) Leap-of-Thought (Deductive).
(e) bAbI16 (Inductive).
 (f) CLUTRR (Inductive).
(g)α-NLI (Abductive).
 (h)α-NLG (Abductive).
(i) AbductiveRules (Abductive).
 (j) D*-Ab (Abductive).
(k) ReClor (Mixed-form).
 (l) LogiQA (Mixed-form).
(m) LogiQA2.0 (Mixed-form).
 (n) LogiQA2NLI (Mixed-form).
Fig. 6: Statistics of different error types from evidence selection process and reasoning process view. The light green bar is wrong
selection and the dark green bar means hallucination . From dark to light blue, the bars represent No Reasoning ,Mistakes
(Perspective) , and Mistakes (Process) .

--- PAGE 10 ---
10
(a) Visualization on the activity.
 (b) Visualization on the orientation.
 (c) Visualization on the no-hallucination.
Fig. 7: Heatmap results for the activity, orientation and no-hallucination of LLMs.
(a) EntailmentBank (De.).
 (b) RuleTaker (De.).
(c) FOLIO (De.).
 (d) Leap (De.).
(e) CLUTRR (In.).
 (f) D*-Ab (Ab.).
Fig. 8: The LLM performances with different numbers of
statements. The red line denotes Answer Correct while the
green line is Answer Correct+Explain Correct .
second in abductive reasoning tasks, albeit with slight dis-
advantages. Conversely, BARD displays poor performance
in avoiding hallucinations and maintaining clarity during
reasoning. Across all four reasoning settings, BARD ranks
last with significant gaps.
On average, LLMs induce hallucination in 27.46% of
the failure cases. LLM hallucinations are more prevalent in
deductive reasoning tasks among the settings of deductive,
inductive, and abductive reasoning, whereas LLMs may
exhibit clearer judgment in the inductive setting.
5.7 How Does the Number of Statements Affect the
LLMs’ Performances?
In the following, we explore some of the key factors to affect
the reasoning performances of LLMs. Since the length of the
input context can be different for the datasets, we reportthe model performances with the number of statements in
Fig. 8. We take ChatGPT for analysis and choose six datasets
with specific counts of statements for illustration, covering
the three reasoning manners. The first four subfigures are
related to the deductive reasoning manner, i.e., Entailment-
Bank, RuleTaker, FOLIO, and Leap-of-Thought. Fig. 8e is
CLUTRR in the inductive setting and Fig. 8f is D-Ab in the
abductive setting. The horizontal axis denotes the number
of statements. The left vertical axis denotes the number
of samples for different numbers of statements. And the
right vertical axis represents the performances with different
numbers of statements.
From the overall results, LLMs can keep the correctness
of both the answer and explanations with fewer input
statements. With the statement number increasing, the per-
formances drop a lot and LLMs struggle to give the correct
explanations. Interestingly, five (out of six) datasets witness
performance gains when the number of statements reaches
certain values. For example, in the RuleTaker dataset, the
best performances are achieved when the number of state-
ments is larger than 17. And when the number is between 13
to 16, ChatGPT is capable of keeping stable performances.
We argue that the larger number of statements can provide
richer information and sometimes can help control the rea-
soning direction of LLMs.
Furthermore, we investigate the impact of the number
of tokens in the context. Fig. 9 illustrates the ChatGPT
performances on twelve datasets. Results vary a lot across
different datasets and different reasoning settings. In gen-
eral, as the number of tokens increases, the performances
of ChatGPT tend to decline. Detailedly, ChatGPT performs
stably in deductive settings. Especially for EntailmentBank
and Leap-of-Thought datasets, ChatGPT maintains rela-
tively consistent accuracy with token numbers increasing.
Considering that deductive setting is a more common form
of reasoning in reality, LLMs are well-trained on it to tackle
the various lengths of inputs.
5.8 How Does the Number of Reasoning Hops Affect
the LLMs’ Performances?
Also, it is interesting to explore the influences of reason-
ing hops for LLMs. Among the selected dataset, three of
them offer the number of hops for each sample, which are
EntailmentBank in deductive reasoning, CLUTRR in induc-
tive reasoning and D*-Ab in abductive reasoning. Fig. 10
presents the performance of ChatGPT with different hop

--- PAGE 11 ---
11
(a) EntailmentBank (De.).
 (b) FOLIO (De.).
 (c) Leap-of-Thought (De.).
 (d) CLUTRR (In.).
(e)α-NLI (Ab.).
 (f)α-NLG (Ab.).
 (g) AbductiveRules (Ab.).
 (h) D*-Ab (Ab.).
(i) ReClor (Mix).
 (j) LogiQA (Mix).
 (k) LogiQA2.0 (Mix).
 (l) LogiQA2NLI (Mix).
Fig. 9: The performances of ChatGPT with different tokens on various datasets.The red line denotes Answer Correct while
the green line is Answer Correct+Explain Correct .
(a) Deductive (EntailmentBank).
 (b) Inductive (CLUTRR).
 (c) Abductive (D*-Ab).
Fig. 10: The performances of ChatGPT under different number of hops. Comparison of Deductive, Inductive and Abductive
reasoning settings.The red line denotes Answer Correct while the green line is Answer Correct+Explain Correct .
numbers (Results of other LLMs are listed in the Appendix).
Alongside the simple accuracy results, we also report the
rigorous reasoning cases where both the answer and expla-
nations are correct.
In the deductive setting, with the number of hops in-
creasing, the performances witness obvious drops, partic-
ularly influencing the rigor of the LLM reasoning. It illus-
trates that reasoning hops have great effects on deductive
reasoning. In inductive reasoning, when the hop number
is greater than two, the performance of ChatGPT decreases
sharply. When the number ranges from three to nine, the
performance of ChatGPT keeps stable at a relatively low
level. Combined with the weak performance of ChatGPT in
inductive reasoning tasks, it demonstrates that ChatGPT can
only work on simple induction, and it obviously struggles
in cases when more hops are needed. In the abductive
reasoning setting, the majority of the test samples only need
one-hop reasoning. When the hop number increases, theperformances of ChatGPT witness slight improvements. It
shows that ChatGPT may have the potential capability of
multi-hop reasoning in the abductive setting.
5.9 Does Chain-of-Thought or Program-of-Thought
Help Logical Reasoning?
As a supplement to the original results, we add the com-
parison between chain-of-thought and direct prompting in
Table 2. It is observed that chain-of-thought does not always
lead to evident improvements Especially under abductive
reasoning, COT tends to have adverse effects in the majority
of instances. Given the distinct disparities between logical
reasoning and fact-based reasoning, the model is required
to engage in abstract reasoning. But COT actually works
more for superficial reasoning over facts.
Recently, the program-of-thought (POT) prompting
strategy [50], [51] stands out as another selection to improve
the model performance. Since POT relies heavily on the

--- PAGE 12 ---
12
TABLE 5: Statistics and evaluation results on NeuLR. Num. represents the number of samples in the dataset. #Hop represents
the hop number of samples in the dataset. COT represents the chain-of-thought strategy under the 1-shot setting.
Dataset Num. #Hoptext-davinci-003 ChatGPT BARD
0-shot 1-shot COT 0-shot 1-shot COT 0-shot 1-shot COT
NeuLR 3,000 1∼5 50.93 59.17 67.90 37.27 48.13 48.00 63.67 65.07 66.00
- Deductive 1,000 2 59.00 69.40 86.10 85.20 69.10 68.30 87.40 93.10 91.90
- Inductive 1,000 3 86.90 89.60 95.60 15.10 68.60 69.60 96.00 92.60 96.30
- Abductive 1,000 1∼5 6.90 18.50 22.00 11.50 6.70 6.10 7.60 9.50 9.80
accessibility of external solvers, we merely include several
logical reasoning datasets for evaluation, i.e., ProofWriter
and RuleTaker. In the implementation, LLMs are prompted
to generate First-order Logic , which can be executed by the
Pyke solver to obtain the final answer [52], [53]. We include
the experimental results in Appendix.
It can be concluded that more powerful LLMs can
derive greater advantages from the PoT. It is observed
that open-source LLMs (i.e., LLaMA3.1-Chat and Mistral-
Instruct-v0.3) benefit more from the chain-of-thought strat-
egy, but fail in the program-of-thought method. Conversely,
proprietary powerful LLMs can derive more benefits from
POT than from COT. It demonstrates that POT requires the
basic symbolic generation capability, which is challenging
for most of the current trending LLMs. The symbol-centric
modeling for LLMs is also a promising direction.
6 N EUTRAL -CONTENT LOGICAL REASONING
Considering the current benchmarks may not provide neu-
tral content for fair evaluation, we propose the new dataset
NeuLR to benchmark the neutral-content logical reasoning
tasks. In column 1 ∼3 of TABLE 5, we provide the statistics of
NeuLR. It contains 3k samples in total, with 1k for deductive
reasoning, 1k for inductive reasoning and 1k for abductive
reasoning. Limited by space, we provide the details of the
construction of NeuLR in the Appendix.
To evaluate the performances of LLMs on NeuLR, we
conduct the experiments shown in TABLE 5. Especially, we
provide three different test settings, i.e., zero-shot, one-shot
and chain-of-thought [54] settings. Details of the prompt
forms are included in the Appendix.
Firstly, from the results, few-shot prompting and chain-
of-thought prompting can both boost the performances of
LLMs in most cases. Overall, chain-of-thought helps most
to the model accuracy. Especially for text-davinci-003, it wit-
nesses consistent gains with the aid of few-shot prompting
and chain-of-thought prompting strategies.
Secondly, among the zero-shot results of three LLMs,
BARD achieves the best performances on NeuLR while
ChatGPT ranks last. The differences of zero-shot settings are
significant. However, with the help of few-shot and chain-
of-thought prompting strategies, text-davinci-003 large nar-
rows the gaps with BARD, and it surpasses BARD with
chain-of-thought strategy. Overall, the performances of
LLMs on NeuLR still have great room for improvement.
Thirdly, from the perspective of different reasoning set-
tings, there exist huge differences in results compared with
the previous findings. Generally, the LLMs’ performances
in inductive reasoning are better than deductive reasoningand abductive reasoning. Especially, LLMs present obvi-
ous weakness in the abductive setting. While from fifteen
classical datasets, the performances among the reasoning
settings are sorted as deductive >abductive >inductive . Such
findings can also motivate future studies. Content neutrality
can enhance strategies for boosting inductive reasoning
performance.
7 C ONCLUSION
In this paper, in-depth evaluations are conducted on logical
reasoning tasks, discussing whether LLMs are really good
logical reasoners. First, the logical reasoning evaluations are
organized from deductive, inductive, abductive and mixed-
form views. We select fifteen logical reasoning datasets to
evaluate on three representative LLMs (i.e., text-davinci-003,
ChatGPT and BARD) under both zero-shot and few-shot
settings. Second, this paper provides fine-level evaluations
on four metrics, covering both objective and subjective
views. For problematic cases, extensive error attributions are
conducted from two dimensions, forming five error types.
It uncovers the logical flaws of LLMs and we provide deep
analysis on the results. Third, to achieve a fair and pure
benchmark for logical reasoning capability, we propose a
dataset with neutral content, covering deductive, inductive
and abductive settings.
Based on the evaluation results above, we abstract six
dimensions to measure the logical reasoning capability of
LLMs: (1) Correct , (2) Rigorous , (3) Self-aware , (4) Active , (5)
Oriented and (6) No hallucination . All these properties can
be calculated with the evaluation methods proposed in
this paper. Therefore, we propose an evaluation scheme for
the logical reasoning capability of LLMs. Considering the
different performances of LLMs on deductive, inductive, ab-
ductive and mixed-form settings, we respectively visualize
each ability map in Fig. 11.
According to the results, text-davinci-003 can maintain
balanced performances in deductive and mixed-form set-
tings. But it usually fails to keep oriented for reasoning
in the inductive setting, and it also shows laziness in the
abductive reasoning tasks. Since it is the earliest released
LLM of the three, it is understandable that text-davinci-003
has some limitations in logical reasoning tasks, especially in
the more complex settings (e.g., inductive and abductive).
From the perspective of common evaluations, ChatGPT
is the weakest LLM of the three, since it performs badly in
showing correct and rigorous reasoning under deductive,
inductive and abductive settings. Also, it seems to be the
laziest reasoner in deductive and inductive settings. How-
ever, it surprises us that it shows unique advantages in
maintaining oriented reasoning and avoiding hallucination,

--- PAGE 13 ---
13
(a) Deductive.
 (b) Inductive.
 (c) Abductive.
 (d) Mixed-form.
Fig. 11: Visualization of three early-stage LLM capabilities under four reasoning settings.
CorrectRigorous Self-aware
Active
Oriented No HallucinationLLaMA3.1-Chat
Mistral-Instruct-v0.3Claude-3
GPT-4
(a) Deductive.
CorrectRigorous Self-aware
Active
Oriented No HallucinationLLaMA3.1-Chat
Mistral-Instruct-v0.3Claude-3
GPT-4 (b) Inductive.
CorrectRigorous Self-aware
Active
Oriented No HallucinationLLaMA3.1-Chat
Mistral-Instruct-v0.3Claude-3
GPT-4 (c) Abductive.
CorrectRigorous Self-aware
Active
Oriented No HallucinationLLaMA3.1-Chat
Mistral-Instruct-v0.3Claude-3
GPT-4 (d) Mixed form.
Fig. 12: Visualization of LLM logical reasoning capability under four reasoning settings, including LLaMA3.1-Chat (8B),
Mistral-Instruct-v0.3, Claude-3, and GPT-4 for comparisons.
especially in deductive and inductive settings. In addition,
it shows its comprehensive capability in the mixed-form
setting. We argue that ChatGPT is specially designed for
chatting, thus it does pretty well in keeping rational but is
not good at solving complex reasoning problems.
BARD is the most active reasoner and it keeps great com-
petitiveness as a correct and rigorous reasoner. However,
it also shows obvious flaws compared with other LLMs.
BARD tends to generate redundant content, easily fails to
find the correct reasoning directions and it usually fails to
avoid hallucinations. In short, BARD shows great advan-
tages in current benchmarks with objective metrics, due to
the larger model size and massive training data. But it still
has much room for improvement in some implicit aspects,
i.e., self-awareness, orientation and non-hallucination.
Overall, it can be observed that all LLMs exhibit specific
limitations in logical reasoning, with relative strength in
deductive reasoning but evident struggles in inductive
settings . Moreover, current evaluation benchmarks, which
primarily depend on objective metrics, are not sufficient to
comprehensively evaluate LLMs.
Additionally, we supplement the visualization of four
trending LLMs in Fig. 12. Different from Fig. 11, these
metrics are calculated in the automatic mode. It also verifies
the scalability of our evaluation system. Please refer to
the Appendix for more analysis on these four LLMs and
evaluation settings.
8 F UTURE DIRECTIONS
Based on the evaluation results, this paper concludes six
future directions for logical reasoning tasks.Strengthen the reasoning ability of inductive reasoning.
Inductive reasoning draws broad conclusions from specific
observations, requiring a more abstract and comprehen-
sive understanding of real-world knowledge compared to
deductive or abductive reasoning. However, LLMs have
shown poor performance in this area, as demonstrated in
Section 4.1. Therefore, it is crucial to develop pre-training or
fine-tuning strategies to enhance their inductive reasoning
abilities. One such strategy could be constructing more
inductive instructions to guide LLMs.
Enhance the LLM’s perception of its capability bound-
aries. LLMs excel at providing answers and explanations
for a wide range of reasoning questions, irrespective of
complexity. However, as evidenced in Sections 5.1 and 5.4,
they may introduce irrelevant information or fabrications,
leading to nonsensical or illogical outcomes. A proficient
logical reasoner should recognize its limitations and knows
when to refrain from answering. Future investigations could
explore human self-awareness in cognitive science and
neuroscience to strengthen LLMs’ understanding of their
operational boundaries.
Strengthen the rigorous reasoning to apply to real-world
scenarios. Table 3 illustrates that current LLMs are not suffi-
ciently rigorous for deductive, inductive, and abductive rea-
soning. As a result, there exist significant gaps between their
capabilities and potential applications in real-world scenar-
ios, particularly those that require detailed intermediate ex-
planations. For instance, using LLMs to solve mathematical
problems and provide precise intelligent Q&A services in
the education field remains a significant challenge [55].
Minimize the occurrence of hallucinations. Similar to the

--- PAGE 14 ---
14
behaviors in other problem-solving contexts [56], LLMs may
generate false or irrelevant hallucinations during logical
reasoning tasks. It suggests that LLMs may not fully com-
prehend the question and can not solve it correctly. To
address this issue, future research should develop more
comprehensive evaluation metrics for hallucinations and
explore specific strategies to minimize their occurrence.
Improve the multi-hop reasoning capability, especially
in inductive and abductive settings. Combined with the
results from Fig. 10 and Fig. 1 in the Appendix, the multi-
hop reasoning capability of LLMs still have much room for
improvement. In fact, humans are better at decomposing
complex questions. It can be an interesting topic for LLMs
to capture the ability to divide and conquer questions, thus
benefiting multi-hop reasoning.
Increase explainability. Finally, the explainability of LLMs
will be essential for building trust, detecting and mitigat-
ing biases, improving performance, promoting user under-
standing, and complying with regulations. A commonsense-
based neuro-symbolic AI framework, such as the one pro-
posed by [57] for sentiment analysis, can help increase
the explainability of the reasoning processes required for
decision-making, which is crucial for sensitive applications
involving ethics, privacy and health.
ACKNOWLEDGMENTS
This work was supported by National Key Research and De-
velopment Program of China (2020AAA0108800), National
Natural Science Foundation of China (62137002, 61937001,
62176209, 62176207, 62106190, and 62050194).
REFERENCES
[1] G. Antoniou and A. Bikakis, “Dr-prolog: A system for defeasible
reasoning with rules and ontologies on the semantic web,” IEEE
Transactions on Knowledge and Data Engineering (TKDE) , vol. 19,
no. 2, pp. 233–245, 2007.
[2] T. Lukasiewicz, “A novel combination of answer set programming
with description logics for the semantic web,” IEEE Transactions on
Knowledge and Data Engineering (TKDE) , vol. 22, no. 11, pp. 1577–
1592, 2010.
[3] Q. Lin, J. Liu, L. Zhang, Y. Pan, X. Hu, F. Xu, and H. Zeng, “Con-
trastive graph representations for logical formulas embedding,”
IEEE Transactions on Knowledge and Data Engineering (TKDE) ,
vol. 35, no. 4, pp. 3563–3574, 2023.
[4] L. Wu, Y. Zhou, and D. Zhou, “Towards high-order complemen-
tary recommendation via logical reasoning network,” in IEEE
International Conference on Data Mining (ICDM) , 2022, pp. 1227–
1232.
[5] Q. Lin, J. Liu, F. Xu, Y. Pan, Y. Zhu, L. Zhang, and T. Zhao,
“Incorporating context graph with logical reasoning for inductive
relation prediction,” in The 45th International ACM SIGIR Confer-
ence on Research and Development in Information Retrieval (SIGIR) ,
2022, pp. 893–903.
[6] J. Yu, Q. Su, X. Quan, and J. Yin, “Multi-hop reasoning question
generation and its application,” IEEE Transactions on Knowledge and
Data Engineering (TKDE) , vol. 35, no. 1, pp. 725–740, 2023.
[7] F. Xu, J. Liu, Q. Lin, Y. Pan, and L. Zhang, “Logiformer: A
two-branch graph transformer network for interpretable logical
reasoning,” in The 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR) , 2022, pp.
1055–1065.
[8] H. Bronkhorst, G. Roorda, C. Suhre, and M. Goedhart, “Logical
reasoning in formal and everyday reasoning tasks,” International
Journal of Science and Mathematics Education , vol. 18, pp. 1673–1694,
2020.[9] Z. Yang, X. Du, R. Mao, J. Ni, and E. Cambria, “Logical reasoning
over natural language as knowledge representation: A survey,”
CoRR , vol. abs/2303.12023, 2023.
[10] B. Peng, C. Li, P . He, M. Galley, and J. Gao, “Instruction
tuning with GPT-4,” CoRR , vol. abs/2304.03277, 2023. [Online].
Available: https://doi.org/10.48550/arXiv.2304.03277
[11] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen,
J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P . Liu, J. Nie, and J. Wen,
“A survey of large language models,” CoRR , vol. abs/2303.18223,
2023.
[12] M. M. Amin, E. Cambria, and B. W. Schuller, “Will affective
computing emerge from foundation models and general artificial
intelligence? A first evaluation of chatgpt,” IEEE Intelligent Sys-
tems , vol. 38, no. 2, pp. 15–23, 2023.
[13] F. Jiao, Y. Guo, X. Song, and L. Nie, “Merit: Meta-path guided
contrastive learning for logical reasoning,” in Findings of the As-
sociation for Computational Linguistics (Findings of ACL) , 2022, pp.
3496–3509.
[14] F. Xu, Q. Lin, T. Zhao, J. JiaweiHan, and J. Liu, “PathReasoner:
Modeling reasoning path with equivalent extension for logical
question answering,” in Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long
Papers) . Bangkok, Thailand: Association for Computational
Linguistics, Aug. 2024, pp. 13 413–13 429. [Online]. Available:
https://aclanthology.org/2024.acl-long.724
[15] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Love-
nia, Z. Ji, T. Yu, W. Chung, Q. V . Do, Y. Xu, and P . Fung, “A multi-
task, multilingual, multimodal evaluation of chatgpt on reasoning,
hallucination, and interactivity,” CoRR , vol. abs/2302.04023, 2023.
[16] N. Bian, X. Han, L. Sun, H. Lin, Y. Lu, and B. He, “Chatgpt
is a knowledgeable but inexperienced solver: An investigation
of commonsense problem in large language models,” CoRR , vol.
abs/2303.16421, 2023.
[17] S. Imani, L. Du, and H. Shrivastava, “Mathprompter: Math-
ematical reasoning using large language models,” CoRR , vol.
abs/2303.05398, 2023.
[18] P . A. Flach and A. C. Kakas, “Abductive and inductive reasoning:
background and issues,” Abduction and induction: Essays on their
relation and integration , pp. 1–27, 2000.
[19] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,
P . Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman,
J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P . Welinder,
P . F. Christiano, J. Leike, and R. Lowe, “Training language models
to follow instructions with human feedback,” in NeurIPS , 2022.
[20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin,
C. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language
models to follow instructions with human feedback,” Advances in
Neural Information Processing Systems , vol. 35, pp. 27 730–27 744,
2022.
[21] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,
S. Shakeri, E. Taropa, P . Bailey, Z. Chen et al. , “Palm 2 technical
report,” arXiv preprint arXiv:2305.10403 , 2023.
[22] C. Qin, A. Zhang, Z. Zhang, J. Chen, M. Yasunaga, and D. Yang,
“Is chatgpt a general-purpose natural language processing task
solver?” CoRR , vol. abs/2302.06476, 2023.
[23] S. Tu, C. Li, J. Yu, X. Wang, L. Hou, and J. Li, “Chatlog: Recording
and analyzing chatgpt across time,” CoRR , vol. abs/2304.14106,
2023.
[24] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu,
C. Lv, Y. Zhang, J. Lei, F. Qi, Y. Fu, M. Sun, and J. He, “C-
eval: A multi-level multi-discipline chinese evaluation suite for
foundation models,” CoRR , vol. abs/2305.08322, 2023.
[25] T. Zhang, F. Ladhak, E. Durmus, P . Liang, K. R. McKeown, and
T. B. Hashimoto, “Benchmarking large language models for news
summarization,” CoRR , vol. abs/2301.13848, 2023.
[26] Y. Pan, J. Liu, L. Zhang, T. Zhao, Q. Lin, X. Hu, and Q. Wang,
“Inductive relation prediction with logical reasoning using con-
trastive representations,” in Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing (EMNLP) , 2022,
pp. 4261–4274.
[27] Z. Huang, M. Chiang, and W. Lee, “Line: Logical query reasoning
over hierarchical knowledge graphs,” in 28th SIGKDD Conference
on Knowledge Discovery and Data Mining , 2022, pp. 615–625.
[28] K. Cheng, J. Liu, W. Wang, and Y. Sun, “Rlogic: Recursive logical
rule learning from knowledge graphs,” in 28th SIGKDD Conference
on Knowledge Discovery and Data Mining , 2022, pp. 179–189.

--- PAGE 15 ---
15
[29] P . N. Johnson-Laird, “Deductive reasoning,” Annual review of psy-
chology , vol. 50, no. 1, pp. 109–135, 1999.
[30] V . Goel, “Anatomy of deductive reasoning,” Trends in cognitive
sciences , vol. 11, no. 10, pp. 435–441, 2007.
[31] E. Heit and C. M. Rotello, “Relations between inductive reason-
ing and deductive reasoning.” Journal of Experimental Psychology:
Learning, Memory, and Cognition , vol. 36, no. 3, p. 805, 2010.
[32] F. Yu, H. Zhang, and B. Wang, “Nature language reasoning, a
survey,” arXiv preprint arXiv:2303.14725 , 2023.
[33] J. R. Josephson and S. G. Josephson, Abductive inference: Computa-
tion, philosophy, technology . Cambridge University Press, 1996.
[34] D. Walton, “Abductive, presumptive and plausible arguments,”
Informal Logic , vol. 21, no. 2, 2001.
[35] B. Dalvi, P . Jansen, O. Tafjord, Z. Xie, H. Smith, L. Pipatanangkura,
and P . Clark, “Explaining answers with entailment trees,” in
Proceedings of the 2021 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , 2021, pp. 7358–7370.
[36] S. Han, H. Schoelkopf, Y. Zhao, Z. Qi, M. Riddell, L. Benson,
L. Sun, E. Zubova, Y. Qiao, M. Burtell, D. Peng, J. Fan, Y. Liu,
B. Wong, M. Sailor, A. Ni, L. Nan, J. Kasai, T. Yu, R. Zhang, S. R.
Joty, A. R. Fabbri, W. Kryscinski, X. V . Lin, C. Xiong, and D. Radev,
“FOLIO: natural language reasoning with first-order logic,” CoRR ,
vol. abs/2209.00840, 2022.
[37] A. Talmor, O. Tafjord, P . Clark, Y. Goldberg, and J. Berant, “Leap-
of-thought: Teaching pre-trained models to systematically reason
over implicit knowledge,” in Advances in Neural Information Pro-
cessing Systems (NeurIPS) , 2020.
[38] K. Sinha, S. Sodhani, J. Dong, J. Pineau, and W. L. Hamilton,
“CLUTRR: A diagnostic benchmark for inductive reasoning from
text,” in Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP) , 2019, pp. 4505–
4514.
[39] W. Yu, Z. Jiang, Y. Dong, and J. Feng, “Reclor: A reading compre-
hension dataset requiring logical reasoning,” in 8th International
Conference on Learning Representations (ICLR) , 2020.
[40] J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang, “Logiqa: A
challenge dataset for machine reading comprehension with logical
reasoning,” in Proceedings of the Twenty-Ninth International Joint
Conference on Artificial Intelligence (IJCAI) , 2020, pp. 3622–3628.
[41] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, and Y. Zhang, “Evaluat-
ing the logical reasoning ability of chatgpt and GPT-4,” CoRR , vol.
abs/2304.03439, 2023.
[42] J. Weston, A. Bordes, S. Chopra, and T. Mikolov, “Towards ai-
complete question answering: A set of prerequisite toy tasks,” in
4th International Conference on Learning Representations (ICLR) , 2016.
[43] P . Clark, O. Tafjord, and K. Richardson, “Transformers as soft
reasoners over language,” in Proceedings of the Twenty-Ninth In-
ternational Joint Conference on Artificial Intelligence (IJCAI) , 2020, pp.
3882–3890.
[44] C. Bhagavatula, R. L. Bras, C. Malaviya, K. Sakaguchi, A. Holtz-
man, H. Rashkin, D. Downey, W. Yih, and Y. Choi, “Abductive
commonsense reasoning,” in 8th International Conference on Learn-
ing Representations (ICLR) , 2020.
[45] N. Young, Q. Bao, J. Bensemann, and M. Witbrock, “Abduction-
rules: Training transformers to explain unexpected inputs,” in
Findings of the Association for Computational Linguistics , 2022, pp.
218–227.
[46] O. Tafjord, B. Dalvi, and P . Clark, “Proofwriter: Generating impli-
cations, proofs, and abductive statements over natural language,”
inFindings of the Association for Computational Linguistics , 2021, pp.
3621–3634.
[47] P . Minervini, S. Riedel, P . Stenetorp, E. Grefenstette, and
T. Rockt ¨aschel, “Learning reasoning strategies in end-to-end dif-
ferentiable proving,” in Proceedings of the 37th International Con-
ference on Machine Learning (ICML) , ser. Proceedings of Machine
Learning Research, vol. 119, 2020, pp. 6938–6949.
[48] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,
J. Xu, and Z. Sui, “A survey for in-context learning,” arXiv preprint
arXiv:2301.00234 , 2022.
[49] Z. Wu, Y. Wang, J. Ye, and L. Kong, “Self-adaptive in-context
learning,” arXiv preprint arXiv:2212.10375 , 2022.
[50] W. Chen, X. Ma, X. Wang, and W. W. Cohen, “Program of thoughts
prompting: Disentangling computation from reasoning for numer-
ical reasoning tasks,” arXiv preprint arXiv:2211.12588 , 2022.[51] L. Gao, A. Madaan, S. Zhou, U. Alon, P . Liu, Y. Yang, J. Callan, and
G. Neubig, “Pal: Program-aided language models,” in International
Conference on Machine Learning . PMLR, 2023, pp. 10 764–10 799.
[52] F. Xu, Z. Wu, Q. Sun, S. Ren, F. Yuan, S. Yuan, Q. Lin, Y. Qiao,
and J. Liu, “Symbol-LLM: Towards foundational symbol-centric
interface for large language models,” in Proceedings of the 62nd
Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers) . Bangkok, Thailand: Association for
Computational Linguistics, Aug. 2024, pp. 13 091–13 116. [Online].
Available: https://aclanthology.org/2024.acl-long.707
[53] L. Pan, A. Albalak, X. Wang, and W. Wang, “Logic-lm: Empow-
ering large language models with symbolic solvers for faithful
logical reasoning,” in Findings of the Association for Computational
Linguistics: EMNLP 2023 , 2023, pp. 3806–3824.
[54] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H.
Chi, Q. V . Le, and D. Zhou, “Chain-of-thought prompting elicits
reasoning in large language models,” in NeurIPS , 2022.
[55] I. Drori, S. Zhang, R. Shuttleworth, L. Tang, A. Lu, E. Ke, K. Liu,
L. Chen, S. Tran, N. Cheng et al. , “A neural network solves,
explains, and generates university math problems by program
synthesis and few-shot learning at human level,” Proceedings
of the National Academy of Sciences (PNAS) , vol. 119, no. 32, p.
e2123433119, 2022.
[56] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. Bang,
A. Madotto, and P . Fung, “Survey of hallucination in natural
language generation,” ACM Computing Surveys , vol. 55, no. 12,
pp. 248:1–248:38, 2023.
[57] E. Cambria, Q. Liu, S. Decherchi, F. Xing, and K. Kwok, “Sentic-
Net 7: A commonsense-based neurosymbolic AI framework for
explainable sentiment analysis,” in LREC , 2022, pp. 3829–3839.

--- PAGE 16 ---
16
APPENDIX
The following sections firstly provide comprehensive in-
formation about the seven LLMs analyzed in this study.
Additionally, this file includes a detailed description of the
evaluation methodology, prompt engineering techniques,
NeuLR dataset, and supplementary analyses. Finally, the
case studies for each dataset are presented. Together, these
sections provide a comprehensive and detailed account of
the methods and results of the study.
APPENDIX A
DETAILS OF LARGE LANGUAGE MODELS
In this study, we select three previously representative LLMs
(i.e., text-davinci-003, ChatGPT and BARD) and four up-
to-date LLMs (i.e., LLaMA3.1-Chat, Mistral-Instruct-v0.3,
Claude-3 and GPT-4) for evaluation and analysis. The
specifics are itemized in Table 6, which includes the affilia-
tion, usage charge, pre-trained data, and model size of each
LLM.
TABLE 6: Details of the selected LLMs. Affi. is short for
Affiliation .Charge represents the charges for 1M tokens in
the format of (input charge / output charge). Data is the
latest time of the utilized training data. Bin the last column
represents the one billion.
Model Affi. Charge Data Size
text-davinci-003 Open-AI 20$ Sep. 2021 -
ChatGPT Open-AI 2$ Jun. 2021 -
BARD Google - Not report 540B
LLaMA3.1-Chat Meta - 2023 8B
Mistral-Inst-v0.3 Mistral - 2023 7B
Claude-3 Anthropic 3$/15$ 2023 -
GPT-4 Open-AI 30$/60$ 2023 -
APPENDIX B
DETAILS OF EVALUATIONS
This section supplements some details of the evaluation.
B.1 Evaluated Datasets
Considering the huge annotation load, we make a balance
between the comprehensiveness and annotation cost. We try
to keep the full data of each dataset to evaluate ChatGPT,
except for some large-scale datasets, i.e., RuleTaker, α-NLI,
α-NLG, AbductiveRules, D*-Ab and LogiQA2NLI. We limit
the number of evaluation samples in these datasets to 1000,
1000, 1000, 1000, 1000 and 600 respectively. For text-davinci-
003 and BARD, we randomly sample 100 examples for
each dataset. It is still more comprehensive than previous
works, that only utilize dozens of samples for testing (e.g.,
30 samples for each dataset).
Notably, as for the supplemented experiments on the
four latest LLMs (LLaMA3.1-Chat, Mistral-Instruct-v0.3,
Claude-3 and GPT-4), we randomly sample 100 cases for
each dataset. It is expected to maintain a fair comparison.
For the mixed-form reasoning datasets, we calculate the
fact length distributions of four datasets for reference. It
is observed that most samples have 1-2 or 3-4 facts for
reasoning.TABLE 7: Fact length distributions (%) of mixed-form
datasets.
ModelLength
1-2 3-4 5-6 7-8 ≥9
ReClor 28.20 61.60 9.00 1.00 0.20
LogiQA 76.65 13.36 7.22 1.84 0.92
LogiQA 2.0 39.40 45.80 12.60 1.60 0.60
LogiQA2NLI 34.94 43.56 16.95 3.07 1.46
B.2 Evaluation Modes
As mentioned in the main paper, we introduce four metrics
to evaluate the LLM performances, i.e., Answer Correctness ,
Explain Correctness ,Explain Completeness and Explain Redun-
dancy . Also, we employ five metrics to attribute the error
cases. Although these metrics are subjective, their standards
are clear and well-defined.
As for the three previously representative LLMs, their
evaluations are conducted through manual annotation. In
detail, we output the LLM answers through API calling
and hire ten graduate students (all majoring in Natural
Language Processing) to annotate the answers based on our
metric definitions.
As for the four trending LLMs, their evaluations are
under the automatic mode . Specifically, we utilize GPT-4o
model to act as the LLM judge. The definitions of the metrics
are fed to the LLM judge as instructions, and it outputs the
evaluation scores the same as the human annotators.
APPENDIX C
SUPPLEMENTARY RESULTS ON LATEST LLM S
First of all, we conduct the pilot experiments to demonstrate
the consistency between the manual labeling and GPT-4o.
The analysis is conducted under the four evaluation metrics
(i.e., answer correctness, explanation correctness, explana-
tion completeness, and explanation redundancy) and the
two error categories (i.e., evidence selection and reasoning
errors). Fig. 13 shows the results.
0% 20% 40% 60% 80% 100%CorrectRigorousSelf-awareActiveOrientedNo Hallucination
Matched Not Matched
99.00%95.33%94.00%97.00%97.33%98.33%
1.00%4.67%6.00%3.00%2.67%1.67%
Fig. 13: The evaluation matches between LLM judge and
human annotations on the 300 samples.
It illustrates that LLM-as-judge can well align with hu-
man performance in our evaluation settings.

--- PAGE 17 ---
17
De. In. Ab. Mix
LLaMA3.1
Mistral-v0.3
Claude-3
GPT-40.72 0.66 0.36 0.59
0.63 0.40 0.32 0.49
0.77 0.43 0.46 0.71
0.80 0.59 0.49 0.69
(a) Correctness.
De. In. Ab. Mix
LLaMA3.1
Mistral-v0.3
Claude-3
GPT-40.89 0.85 0.82 0.94
0.78 0.75 0.79 0.94
0.98 0.95 0.97 1.00
0.97 0.95 0.94 1.00 (b) Rigorousness.
De. In. Ab. Mix
LLaMA3.1
Mistral-v0.3
Claude-3
GPT-40.07 0.21 0.01 0.09
0.05 0.00 0.08 0.06
0.43 0.41 0.54 0.56
0.43 0.41 0.54 0.56 (c) Self-awareness.
Fig. 14: Heatmap results for the correctness, rigorousness, and self-awareness metrics.
De. In. Ab. Mix
LLaMA3.1
Mistral-v0.3
Claude-3
GPT-40.62 0.50 0.17 0.59
0.40 0.50 0.49 0.00
0.78 0.50 0.98 0.84
0.82 1.00 0.91 0.87
(a) Active.
De. In. Ab. Mix
LLaMA3.1
Mistral-v0.3
Claude-3
GPT-40.56 0.57 0.08 0.21
0.24 0.00 0.02 0.00
0.43 0.34 0.49 0.44
0.67 0.62 0.43 0.37 (b) Oriented.
De. In. Ab. Mix
LLaMA3.1
Mistral-v0.3
Claude-3
GPT-40.08 0.21 0.13 0.09
0.10 0.10 0.10 0.21
0.46 0.05 0.16 0.42
0.43 0.45 0.32 0.26 (c) No hallucination.
Fig. 15: Heatmap results for the activity, orientation and no-hallucination of LLMs.
Next, we implement the automatic evaluations on
the four trending LLMs (i.e., LLaMA3.1-Chat-8B, Mistral-
Instruct-v0.2-7B, Claude-3, and GPT-4). Fig. 14 and 15 pro-
vide the respective results on the evaluation of six dimen-
sions.
APPENDIX D
PROMPT ENGINEERING
In the implementation, we consider both the zero-shot and
few-shot settings. To obtain appropriate model outputs, we
construct the prompts, shown in Table 10 of Appendix.
APPENDIX E
NEULR DATASET
In this section, we will provide more details on how NeuLR
is constructed and what is the form of the prompt.
E.1 Construction of NeuLR
According to the explanations in the main paper, current
logical reasoning may fail to keep the neutral content for
evaluation. Therefore, we propose the new dataset NeuLR
to benchmark the pure logical reasoning capability of LLMs.
The data is sourced from [42], [46]. Detailedly, we transform
some of the important content (e.g., entities and properties)
into random strings. In NeuLR, we neutralize three types
of words, i.e., name, species and property. We replace each
type of words with the concatenation of a prefix (i.e., NP ,SP , ADP) and six random characters (combination of upper-
case and lowercase letters and numbers). Table 9 illustrates
examples of the replacement process.
TABLE 9: The construction of neutral content. Example col-
umn presents some example words. Prefix represents the
prefix for each type of words. 6-character represents exam-
ples of randomly generated combination of characters.
Type Example Prefix 6-character
Name Bob/Lily NP e.g., uF52pT
Species Dog/Sheep SP e.g., 7gfO2k
Property Big/Red/Smart ADP e.g., PT01mx
E.2 Prompt Forms
Different from previous evaluations, we consider adding
descriptions of content neutrality in the prompt forms.
Specifically, we replace the previous task description pre-
fix with the following form (take abductive reasoning for
illustration):
•This is a neutral-content abductive reasoning task.
The strings starting with SP represent species. The
strings starting with NP represent names. The strings
starting with ADP represent property. Given a con-
text and a fact, it is required to generate a short
missing fact.
Other parts of the prompts are the same as the previous
form.

--- PAGE 18 ---
18
TABLE 8: Evaluated Datasets. Gen. distinguishes whether the predicted answer is generated text or classified labels. Explain
denotes whether the explanation is required in the task. # Davinci, # ChatGPT and # BARD columns represent the number
of evaluation samples of each dataset for the three LLMs.
Categories Dataset Source Gen. Explain # Davinci # ChatGPT # BARD
DeductivebAbI-15 [42] ✓ ✓ 100 1,000 100
EntailmentBank [35] ✓ 100 340 100
RuleTaker [43] ✓ 100 1,000 100
FOLIO [36] 100 204 100
Leap-Of-Thought [37] 100 1,289 100
InductivebAbI-16 [42] ✓ 100 1,000 100
CLUTRR [38] ✓ 100 1,146 100
Abductiveα-NLI [44] 100 1,000 100
α-NLG [44] ✓ 100 1,000 100
AbductiveRules [45] ✓ 100 1,000 100
D*-Ab [46] ✓ ✓ 100 1,000 100
mixed-formReClor [39] 100 500 100
LogiQA [40] 100 651 100
LogiQA 2.0 [41] 100 500 100
LogiQA2NLI [41] 100 600 100
(a) Deductive setting (EntailmentBank) on
text-davinci-003.
(b) Inductive setting (CLUTRR) on text-
davinci-003.
(c) Abductive setting (D*-Ab) on text-
davinci-003.
(d) Deductive setting (EntailmentBank) on
BARD.
(e) Inductive setting (CLUTRR) on BARD.
 (f) Abductive setting (D*-Ab) on BARD.
Fig. 16: The performances of text-davinci-003 and BARD under different number of hops. Comparison of deductive,
inductive and abductive reasoning settings.
Further, we consider both zero-shot setting, few-shot
setting and chain-of-thought strategy in NeuLR. The zero-
shot and few-shot prompt forms are same as the forms
in TABLE 10. For chain-of-thought prompt, we utilize the
explanation provided with the dataset to help generate the
reasoning chains. Next, we will offer examples for each
reasoning setting.
- Deductive Reasoning
[chain-of-thought]: There is one example of deductive rea-
soning: The facts are: SP1Ggwz1 are afraid of SPU4g85d.
NPdmbdKB is a SP1Ggwz1. SPArMPn0 are afraid of
SPU4g85d. SPU4g85d are afraid of SPArMPn0. NPd0WTGi
is a SP1Ggwz1. NP1cTSnm is a SPU4g85d. SPFURodYs are
afraid of SP1Ggwz1. NPpEQFEK is a SPU4g85d. The ques-tion is: What is NPpEQFEK afraid of? Because NPpEQFEK
is a SPU4g85d and SPU4g85d are afraid of SPArMPn0.
Therefore, the answer is: SPArMPn0.
- Inductive Reasoning
[chain-of-thought]: There is one example of inductive rea-
soning: The facts are: NP16WBQQ is a SPQFyx7i. NPerfGLN
is a SP4Vge77. NP0V3buK is a SPGbXv1C. NPU9kOFg
is a SPQFyx7i. NPDHuODI is a SP4Vge77. NPerfGLN is
ADPHQuIP9. NP0V3buK is ADPmBU2ts. NP16WBQQ is
ADP0rI59W. NPDHuODI is ADPHQuIP9.The question is:
What property is NPU9kOFg? Because NPU9kOFg is a
SPQFyx7i, NP16WBQQ is a SPQFyx7i and NP16WBQQ is
ADP0rI59W. Therefore, the answer is: ADP0rI59W.
- Abductive Reasoning

--- PAGE 19 ---
19
[chain-of-thought]: ... Based on rule2 and tripleM, we can
derive a new fact. Combine it with triple4. Based on rule1,
we can derive a new fact. Combine it with rule1. In this way,
we can finally derive the given fact...
APPENDIX F
SUPPLEMENTARY ANALYSIS
We also provide the performances of text-davinci-003 and
BARD with different number of hops in Figure 16. In the
deductive setting of text-davinci-003, model performance
drops with the number of hops increasing. But when the hop
number is over five, it witnesses slight gains in performance,
which illustrates that text-davinci-003 has the potential to
conduct multi-hop reasoning in the deductive reasoning
setting. However, in the inductive and abductive settings,
the performances of text-davinci-003 decrease sharply when
the number of hops increases. Especially, it fails all the cases
when the hop number is over six in inductive reasoning, and
also when the hop number is greater than one in abductive
reasoning. It is inferior to ChatGPT.
For BARD, the situation is quite different. In deductive
reasoning, the performance of BARD increases with the hop
number adds. Especially, when the hop number is over five,
the accuracy reaches 100% without reducing the rigor of
reasoning. In inductive reasoning, the performance of BARD
also drops at first, but it keeps stable and witnesses obvious
gains when the hop number is over six. It demonstrates
that BARD is better at conducting inductive reasoning and
processing multi-hop scenarios compared with text-davinci-
003 and ChatGPT. In abductive reasoning, BARD struggles
a lot, inferior to ChatGPT but is better than text-davinci-003.
In all, in the face of complex multi-hop scenarios, LLMs
still have much room for improvement. From the results,
they do relatively well in deductive reasoning settings. But
they are far from good in the inductive and abductive
settings, which can also inspire future research on it.
APPENDIX G
CASESTUDIES
We show one reasoning case for each dataset in Table 11-25,
where the context and question as well as output of 0-shot
ChatGPT, 1-shot ChatGPT, 3-shot ChatGPT, 0-shot Davinci-
003 and 0-shot BARD are displayed. We also provided the
annotated information about the answer correctness: ,
explain correctness: , explain completeness: , explain
redundancy: , evidence wrong selection: , hallucination:
, no reasoning: , perspective mistake: , and process
mistake: . (the last five indicators are annotated when the
explain explanation is false).

--- PAGE 20 ---
20
TABLE 10: Prompt Engineering.
Dataset Prompt of zero-shot Prompt of k-shot
Deductive Reasoning
bAbI (task 15)[zero-shot prompt]: Given facts: [Context]. Based on the
given facts above, answer the following question using
deductive reasoning and give simple explanations. The
question is: [Question]There are [k] examples of deductive rea-
soning:
Given facts: [Context]
The question is: [Question]
The answer is: [Answer] (display k sam-
ples)
[zero-shot prompt]
EntailmentBank[zero-shot prompt]: Given facts: [Context]. [Question].
Please answer the question in one sentence using deduc-
tive reasoning. And give simple explanations.There are [k] examples of deductive rea-
soning:
Given facts: [Context]
The question is: [Question]
The answer is: [Answer] (display k sam-
ples)
[zero-shot prompt]
RuleTaker[zero-shot prompt]: Given facts: [Context]. Based on the
given facts above, determine whether the following state-
ment is true using deductive reasoning and give simple
explanations. The statement is: [Statement].There are [k] examples of deductive rea-
soning:
Given facts: [Context]
The statement is: [Statement]
The answer is: [Answer] (display k sam-
ples)
[zero-shot prompt]
FOLIO[zero-shot prompt]: Given facts: [Context]. Based on
the given facts above, determine whether the following
statement is true, false, or uncertain using deductive
reasoning and give simple explanations. The statement
is: [Statement].There are [k] examples of deductive rea-
soning:
Given facts: [Context]
The statement is: [Statement]
The answer is: [Answer] (display k sam-
ples)
[zero-shot prompt]
Leap-Of-Thought[zero-shot prompt]: Given facts: [Context]. Based on the
given facts above, determine whether the following state-
ment is true using deductive reasoning and give simple
explanations. The statement is: [Statement].There are [k] examples of deductive rea-
soning:
Given facts: [Context]
The statement is: [Statement]
The answer is: [Answer] (display k sam-
ples)
[zero-shot prompt]
Inductive Reasoning
bAbI-16[zero-shot prompt]: Given facts: [context]. Based on the
given facts above, answer the following question using
inductive reasoning and give simple explanations. The
question is: [Question].There are [k] examples of inductive reason-
ing:
Given facts: [Context]
The question is: [Question]
The answer is: [Answer] (display k sam-
ples)
[zero-shot prompt]
CLUTRR[zero-shot prompt]: Given facts: [context]. [Question].
Please answer the question in one sentence using induc-
tive reasoning. And give simple explanations.There are [k] examples of inductive reason-
ing:
Given facts: [Context]
The question is: [Question]
The answer is: [Answer] (display k sam-
ples)
[zero-shot prompt]
Abductive Reasoning
α-NLI[task description]: Given a context, the abductive reason-
ing task is to choose the more likely explanation from
a given pair of hypotheses choices. And give simple
explanations.
[zero-shot prompt]: The context is: [Context]. The hy-
pothesis choices are: A. [Option A]. B. [Option B].[task description]
Next, I will give you [k] example(s) for test.
The context is [Context]. The hypothesis
choice are: A. [Option A]. B. [Option B].
The correct choice is: [Label].
Next, I will give you an example for test.
[zero-shot prompt]
α-NLG[task description]: Given a context, the abductive reason-
ing task is to generate a valid and short hypothesis.
[zero-shot prompt]: The context is: [Context]. Please gen-
erate a short hypothesis for the context and give simple
explanations.[task description]
Next, I will give you [k] example(s) for
test. The context is [Context]. The correct
answer is [Label].
Next, I will give you an example for test.
[zero-shot prompt]

--- PAGE 21 ---
21
AbductiveRules[task description]: Given a context and an observation,
the abductive reasoning task is to generate a valid and
short explanation.
[zero-shot prompt]: The context is: [Context]. The obser-
vation is: [Observation]. Please generate a short explana-
tion for the given context and observation.[task description]
Next, I will give you [k] example(s) for test.
The context is: [Context]. The observation
is: [Observation]. The explanation is: [Ex-
planation].
Next, I will give you an example for test.
[zero-shot prompt]
D*-Ab[task description]: Given a context and a fact, the abduc-
tive reasoning task is to generate a short missing fact.
[zero-shot prompt]: The context is: [Context+Rule]. The
fact is: [Fact]. Please generate a short missing fact for the
given context and fact. And give simple explanations.[task description]
Next, I will give you [k] example(s) for test.
The context is: [Context+Rule]. The obser-
vation is: [Observation]. The explanation
is: [Explanation].
Next, I will give you an example for test.
[zero-shot prompt]
mixed-form Reasoning
ReClor[task description]: This is a Mahcine Reading Compre-
hension task, given the context and question, you are
required to choose the correct answer from the answer
set and give explanations.
[zero-shot prompt]: The context is: [Context]. The ques-
tion is: [Question]. [Option A]. [Option B]. [Option C].
[Option D]. Please give the correct answer and simple
explanations.[task description]
Next, I will give you [k] example(s) for test.
The context is: [Context]. The question is:
[Question]. The correct choice is: [Label].
Next, I will give you an example for test.
[zero-shot prompt]
LogiQA[task description]: This is a Mahcine Reading Compre-
hension task, given the context and question, you are
required to choose the correct answer from the answer
set and give explanations.
[zero-shot prompt]: The context is: [Context]. The ques-
tion is: [Question]. [Option A]. [Option B]. [Option C].
[Option D]. Please give the correct answer and simple
explanations.[task description]
Next, I will give you [k] example(s) for test.
The context is: [Context]. The question is:
[Question]. The correct choice is: [Label].
Next, I will give you an example for test.
[zero-shot prompt]
LogiQA 2.0[task description]: This is a Mahcine Reading Compre-
hension task, given the context and question, you are
required to choose the correct answer from the answer
set and give explanations.
[zero-shot prompt]: The context is: [Context]. The ques-
tion is: [Question]. [Option A]. [Option B]. [Option C].
[Option D]. Please give the correct answer and simple
explanations.[task description]
Next, I will give you [k] example(s) for test.
The context is: [Context]. The question is:
[Question]. The correct choice is: [Label].
Next, I will give you an example for test.
[zero-shot prompt]
LogiQA2NLI[task description]: This is a Natural Language Inference
task. Please tell whether the premise and conclusion
entailed or NOT entailed. And give simple explanations.
[zero-shot prompt]: The premise is: [Premise]. The con-
clusion is: [Conclusion]. Please give the correct answer
and simple explanations.[task description]
Next, I will give you [k] example(s) for test.
The premise is: [Premise]. The context is:
[Context]. The correct answer is: [Label].
Next, I will give you an example for test.
[zero-shot prompt]

--- PAGE 22 ---
22
TABLE 11: A reasoning case in bAbI-15.Question, answer
and explanationContext: 1 Sheep are afraid of wolves. 2 Mice are afraid of wolves. 3 Winona is a sheep. 4 Gertrude is a sheep. 5
Wolves are afraid of sheep. 6 Emily is a sheep. 7 Cats are afraid of sheep. 8 Jessica is a sheep.
Question: What is jessica afraid of?
Answer: Wolf
Explanation: IsSheep(jessica) (8)1− →Afraid(jessica, wolf)ChatGPT
of 0-shotJessica is afraid of nothing, as it is not specified in the given facts that she has any known natural predator or fear.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotJessica is a sheep and sheep are afraid of wolves. Therefore, it can be deduced that Jessica is afraid of wolves.
Answer and Explain: ChatGPT
of 3-shotSheep are afraid of wolves and Jessica is a sheep.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotJessica is likely afraid of wolves, since wolves are a common source of fear that other animals, including sheep,
mice, and cats, have in common. Wolves are known to be predators of sheep, so Jessica being a sheep, would
likely be scared of wolves.
Answer and Explain: Explain Error Type:Bard
of 0-shotJessica is a sheep. Sheep are afraid of wolves. Therefore, Jessica is afraid of wolves.
Here is the deductive reasoning:
1. Sheep are afraid of wolves.
2. Jessica is a sheep.
3. Therefore, Jessica is afraid of wolves.
The first premise is supported by the fact that sheep are prey animals and wolves are predators. Wolves are known
to prey on sheep, and sheep have evolved to be afraid of wolves as a way of self-preservation.
The second premise is supported by the fact that Jessica is a sheep. This is a given fact in the prompt.
The third premise follows logically from the first two premises. If sheep are afraid of wolves, and Jessica is a
sheep, then Jessica must be afraid of wolves.
Therefore, the conclusion is that Jessica is afraid of wolves.
Answer and Explain: 

--- PAGE 23 ---
23
TABLE 12: A reasoning case in EntailmentBank.Question, answer
and explanationContext: 1 pollination is when pollinating animals / wind / water carry pollen from one flower to another flower.
2 if something is a part of a process then that something is required for that process. 3 helping something has a
positive impact on that something. 4 if something is required for a process then that something positively impacts
that process. 5 an insect is a pollinating animal.
Question: life cycle of some plants includes the process of pollination. Pollination is helped by?
Answer: wind, water, and insects
Explanation: sent1 & sent5 →int1: pollination is when insects / wind / water carry pollen from one flower to another;
int1 & sent2 →int2: insects / wind / water carrying pollen is required for pollination; int2 & sent4 →int3: insects /
wind / water have a positive impact on pollination by carrying pollen; int3 & sent3 →hypothesisChatGPT
of 0-shotPollination in the life cycle of some plants is helped by pollinating animals, such as insects, which are required for
the process of pollination and have a positive impact on it.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotPollination in plants is helped by pollinating animals such as insects, which carry pollen from one flower to
another as a necessary part of the process, positively impacting the plants’ life cycle.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotPollinating animals such as insects, which are required for the process of pollination to occur.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotPollination is helped by pollinating animals, such as insects, carrying pollen from one flower to another.
Answer and Explain: Explain Error Type:Bard
of 0-shotSure, here is the answer to the question in one sentence using deductive reasoning:
Pollination is helped by pollinating animals, such as insects.
Here is the simple explanation:
Pollination is the process of transferring pollen from the male part of a flower to the female part of a flower.
Pollinating animals, such as insects, help pollination by carrying pollen from one flower to another.
Without pollination, plants cannot reproduce and produce seeds.
Therefore, pollinating animals help the life cycle of some plants by helping them to reproduce.
Answer and Explain: Explain Error Type:

--- PAGE 24 ---
24
TABLE 13: A reasoning case in RuleTaker.Question, answer
and explanationContext: 1 If the current runs through the bell then the bell is ringing. 2 The circuit includes the battery. 3 If the
circuit does not include the switch then the circuit is complete. 4 The circuit includes the bell. 5 If the circuit includes
the switch and the switch is on then the circuit is complete. 6 If the current runs through the light bulb then the light
bulb is glowing. 7 If the wire is plastic then the wire is not conducting. 8 The circuit includes the switch. 9 If the
circuit includes the battery and the battery is not flat then the circuit is powered. 10 If the current runs through the
circuit and the circuit includes the bell then the current runs through the bell. 11 The wire is metal. 12 If the wire is
metal then the wire is conducting. 13 If the current runs through the circuit and the circuit includes the light bulb
then the current runs through the light bulb. 14 If the current runs through the radio then the radio is playing. 15
If the current runs through the circuit and the circuit includes the radio then the current runs through the radio. 16
If the circuit is powered and the circuit is complete and the wire is conducting then the current runs through the
circuit.
Statement: The current does not run through the circuit.
Question: Determine whether the statement is true using deductive reasoning and give explanations.
Answer: TrueChatGPT
of 0-shotThe circuit is powered, complete, and the wire is conducting. Therefore, we can use deductive reasoning to
determine that the statement “The current does not run through the circuit” is false based on the given facts.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotIf the battery is not flat and the circuit includes the battery, then the circuit is powered: “If the circuit includes the
battery and the battery is not flat then the circuit is powered.” If the circuit is powered and the wire is metal (which
is conducting), then the current runs through the circuit: “If the circuit is powered and the circuit is complete and
the wire is conducting then the current runs through the circuit.” However, it is stated that the wire is plastic and
if the wire is plastic, then it is not conducting: “If the wire is plastic then the wire is not conducting.” Therefore,
the statement “The current does not run through the circuit” is true.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotThis statement is false based on the given facts. If the circuit is powered, complete, and the wire is conducting,
then the current runs through the circuit, as stated in the first premise. Hence, we can’t conclude that the current
doesn’t run through the circuit.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotPollination is helped by pollinating animals, such as insects, carrying pollen from one flower to another.
Answer and Explain: Explain Error Type:Bard
of 0-shotSure, here is the answer to the question in one sentence using deductive reasoning:
Pollination is helped by pollinating animals, such as insects.
Here is the simple explanation:
Pollination is the process of transferring pollen from the male part of a flower to the female part of a flower.
Pollinating animals, such as insects, help pollination by carrying pollen from one flower to another.
Without pollination, plants cannot reproduce and produce seeds.
Therefore, pollinating animals help the life cycle of some plants by helping them to reproduce.
Answer and Explain: Explain Error Type:

--- PAGE 25 ---
25
TABLE 14: A reasoning case in FOLIO.Question, answer
and explanationContext: 1 All growth companies’ stocks are volatile. 2 If the stock price is volatile, then it is not suitable for
a retirement fund. 3 Some companies’ stocks are growth companies’ stocks. 4 All mature companies’ stocks are
suitable for a retirement fund. 5 KO is a mature company’s stock.
Statement: KO is a company stock and a growth company’s stock.
Question: Determine whether the statement is true, false, or uncertain using deductive reasoning and give
explanations.
Answer: False
Explanation: MatureCompanies’Stocks(KO) (5)4− → SuitableForRetirementRund(KO)2r− →¬ PriceVolatile(KO)1r− →
¬GrowthCompanies’Stocks(KO) (contradictory)ChatGPT
of 0-shotA mature company’s stock cannot also be a growth company’s stock since these are two distinct categories. The
given facts say that some companies’ stocks are growth companies’ stocks and all mature companies’ stocks are
suitable for a retirement fund. Additionally, KO is specifically identified as a mature company’s stock. Therefore,
the statement “KO is a company stock and a growth company’s stock” is false.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotWe know from the given facts that KO is “a mature company’s stock,” but there is no information to suggest that
it is also a growth company’s stock. Therefore, the statement “KO is a company stock and a growth company’s
stock” is uncertain based on the given facts.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotThe given facts state that KO is a mature company’s stock. However, it is not stated whether KO is also a growth
company’s stock. Therefore, the statement “KO is a company stock and a growth company’s stock” is uncertain.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotFalse. While it is true that KO is a company stock, it is not true that it is a growth company’s stock. Growth
companies’ stocks are volatile, whereas the fact given states that mature companies’ stocks are suitable for
retirement funds. Since KO is a mature company’s stock, it follows that it is not a growth company’s stock.
Answer and Explain: Explain Error Type:Bard
of 0-shotThe statement “KO is a company stock and a growth company’s stock” is false.
Explanation:
1. All growth companies’ stocks are volatile. This means that the stock price of a growth company is likely to
fluctuate more than the stock price of a mature company.
2. If the stock price is volatile, then it is not suitable for a retirement fund. This is because retirement funds are
designed to provide long-term growth, and volatile stocks are more likely to lose value over time.
3. Some companies’ stocks are growth companies’ stocks. This means that some companies are growing at a faster
rate than others.
4. All mature companies’ stocks are suitable for a retirement fund. This is because mature companies are less likely
to experience sudden changes in their stock price.
5. KO is a mature company’s stock. This means that KO is a company that is not growing at a rapid pace.
Based on the above facts, we can conclude that KO is not a growth company’s stock. This is because KO is a
mature company, and mature companies’ stocks are not volatile. Therefore, the statement “KO is a company stock
and a growth company’s stock” is false.
Answer and Explain: 

--- PAGE 26 ---
26
TABLE 15: A reasoning case in Leap-Of-Thought.Question, answer
and explanationContext: 1 A bird is not capable of express feelings. 2 A quack-quack is not capable of suffering dehydration. 3 A
privet is capable of suffering dehydration. 4 A music is capable of express feelings.
Statement: A jazz is capable of express feelings.
Question: Determine whether the statement is true using deductive reasoning and give explanations.
Answer: True
Explanation: IsMusic(jazz) (common sense)4− →ExpressFeeling(jazz)ChatGPT
of 0-shotThe statement “A jazz is capable of expressing feelings” can be inferred with a certain level of probability based
on the given facts. The fact that ”A music is capable of expressing feelings” suggests that jazz, which is a genre
of music, is likely to be capable of expressing feelings as well. The fact that ”A bird is not capable of expressing
feelings” and ”A quack-quack is not capable of suffering dehydration” are logically irrelevant to the statement
about whether jazz is capable of expressing feelings. The fact that ”A privet is capable of suffering dehydration”
is also irrelevant, as it does not provide any information or indication one way or the other about the statement.
Therefore, based on the given facts, the statement about jazz being capable of expressing feelings can be inferred
with a certain level of probability and is likely true.
Answer and Explain: ChatGPT
of 1-shotThe statement “A jazz is capable of express feelings” is true using deductive reasoning based on the given facts.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotThis is true, as the given fact “a music is capable of express feelings” includes the subcategory of music that is
jazz. Therefore, we can logically deduce that jazz music is also capable of expressing feelings.
Answer and Explain: Davinci-003
of 0-shotTrue. Although humans usually associate the sound of a quack-quack with birds, the quack-quack mentioned here
is not a bird and is not capable of expressing feelings. Additionally, a privet is capable of suffering dehydration
but not expressing feelings. Therefore, the only species that has been mentioned in the facts that is capable
of expressing feelings is the music. Although a jazz is not explicitly mentioned in the facts, it would also be
categorically considered to be a type of music, so it can logically be assumed that a jazz is capable of expressing
feelings.
Answer and Explain: Bard
of 0-shotThe statement ”A jazz is capable of express feelings” is not true. The fact that a music is capable of expressing
feelings does not mean that a jazz is capable of expressing feelings. A jazz is a type of music, but it is not the
only type of music. There are many other types of music, and not all of them are capable of expressing feelings.
For example, classical music is often seen as being more formal and less emotional than jazz. Therefore, it is not
possible to say that all music is capable of expressing feelings, simply because one type of music is capable of
doing so.
In order to determine whether a jazz is capable of expressing feelings, we would need more information about the
specific jazz in question. We would need to know what the jazz is about, who wrote it, and how it was performed.
Once we have this information, we can then use deductive reasoning to determine whether the jazz is capable of
expressing feelings.
For example, if the jazz is about a personal experience that the author had, then it is more likely that the jazz will
be able to express feelings. This is because the author will have put their own emotions into the jazz, and this will
be reflected in the music. However, if the jazz is about a more general topic, such as the beauty of nature, then it
is less likely that the jazz will be able to express feelings. This is because the author will not have put their own
emotions into the jazz, and the music will therefore be more objective.
Ultimately, whether or not a jazz is capable of expressing feelings is a matter of opinion. There is no scientific way
to determine whether a piece of music is capable of expressing feelings. However, by using deductive reasoning,
we can gain a better understanding of the potential for a jazz to express feelings.
Answer and Explain: Explain Error Type:

--- PAGE 27 ---
27
TABLE 16: A reasoning case in bAbI-16.Question, answer
and explanationContext: 1 Greg is a rhino. 2 Brian is a swan. 3 Bernhard is a frog. 4 Brian is green. 5 Greg is gray. 6 Bernhard is
yellow. 7 Julius is a lion. 8 Julius is yellow. 9 Lily is a rhino.
Question: What color is Lily?
Answer: Gray
Explanation: IsRhino(Greg) (1), IsGray(Greg) (5) →IsGray(rhino), IsRhino(Lily) (9) →IsGray(Lily)ChatGPT
of 0-shotBased on the given facts and inductive reasoning, it is likely that Lily’s color is not definitively determined.
Although Lily is described as a rhino, there is no information given that directly pertains to Lily’s color.
Additionally, there is no other animal descriptions that could be used to make an inference regarding Lily’s
color. Therefore, based on the given facts, it is not possible to make an accurate determination of Lily’s color using
inductive reasoning.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotLily’s color is likely to be unknown as there is no information provided about her color among the other animals.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotBrian, Greg, and Bernhard, which do not directly provide clues about what color Lily may be. Therefore, we
cannot be certain about her color based on the information given alone.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotBased on the given facts, we can use inductive reasoning to conclude that Lily is gray. This is because all of the
other rhinos in the given facts are gray, and there is no evidence to suggest that Lily is any different. In fact, the
only thing that distinguishes Lily from the other rhinos is her name, which is a flower that is typically white or
yellow. However, we can safely assume that Lily’s name is not a reference to her color, as there are many other
animals and plants that share the same name. Therefore, the most likely conclusion is that Lily is gray, just like the
other rhinos. Here is a more detailed explanation of the inductive reasoning process used to reach this conclusion:
1. We are given the following facts: * Greg is a rhino. * Brian is a swan. * Bernhard is a frog. * Brian is green. * Greg
is gray. * Bernhard is yellow. * Julius is a lion. * Julius is yellow. * Lily is a rhino.
2. We notice that all of the rhinos in the given facts are gray.
3. We conclude that there is a strong likelihood that Lily is also gray, as there is no evidence to suggest that she is
any different from the other rhinos.
4. We can safely assume that Lily’s name is not a reference to her color, as there are many other animals and plants
that share the same name.
5. Therefore, the most likely conclusion is that Lily is gray.
Answer and Explain: Bard
of 0-shotBased on inductive reasoning, it is likely that Lily is gray, as both Greg and Julius are gray and they are both
rhinos, like Lily. Since the only other colors mentioned among the animals are green and yellow, and none of the
rhinos have these colors, it is reasonable to assume that Lily is gray.
Answer and Explain: Explain Error Type:

--- PAGE 28 ---
28
TABLE 17: A reasoning case in CLUTRR.Question, answer
and explanationContext: 1 [Chuck] took his daughter [Sara] to the park that afternoon to play. 2 [Judy] took her sister, [Maxine],
out to dinner for her birthday. 3 [Chuck] took his sister [Judy] out to lunch after learning that she got accepted
into her first choice for university. 4 [Maxine] asked her daughter, [Joyce], if she would like to go to a movie with
her on Saturday night. 5 [Maxine] and her daughter [Joyce] went to see a movie yesterday and then got ice cream
afterwards. 6 [Reynaldo]’s wife, [Maxine], was thrilled to learn she was pregnant. 7 [Naomi] went shoe shopping
with her sister [Sara].
Question: What is the kinship between Reynaldo and Naomi?
Answer: niece
Explanation: [(Reynaldo, wife, Maxine) 6, (Maxine, daughter, Joyce) 5] →(Reynaldo, daughter, Joyce) i1
[(Maxine, sister, Judy) 2, (Judy, brother, Chuck) 3] →(Maxine, brother, Chuck) i2
[(Joyce, mother, Maxine) 4, (Maxine, brother, Chuck) i2] →(Joyce, uncle, Chuck) i3
[(Reynaldo, daughter, Joyce) i1, (Joyce, uncle, Chuck) i4] →(Reynaldo, brother, Chuck) i4
[(Chuck, daughter, Sara) 1, (Sara, sister, Naomi) 7] →(Chuck, daughter, Naomi) i5
[(Reynaldo, brother, Chuck) i4, (Chuck, daughter, Naomi) i5] →(Reynaldo, niece, Naomi)ChatGPT
of 0-shotThere is no direct or indirect relation mentioned between Reynaldo and Naomi in the given facts.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotUnclear, as there is not enough information about Naomi’s relation to Reynaldo in the given facts.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotThe given facts do not provide any information about the kinship between Reynaldo and Naomi, so the question
cannot be answered using inductive reasoning.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotReynaldo and Naomi have no direct kinship; they are both related to Maxine, as Reynaldo is her husband and
Naomi is Sara’s sister.
Answer and Explain: Explain Error Type:Bard
of 0-shotReynaldo and Naomi are siblings-in-law.
Inductive reasoning is a type of reasoning that uses specific examples to make a general conclusion. In this
case, we can use the following specific examples to make the general conclusion that Reynaldo and Naomi are
siblings-in-law:
Maxine is the wife of Reynaldo.
Sara is the daughter of Maxine.
Naomi is the sister of Sara.
Therefore, Naomi is the sister-in-law of Reynaldo.
Here are some simple explanations for each of the specific examples:
Maxine is the wife of Reynaldo because we are told that Reynaldo’s wife is Maxine.
Sara is the daughter of Maxine because we are told that Maxine has a daughter named Sara.
Naomi is the sister of Sara because we are told that Naomi went shoe shopping with her sister Sara.
Answer and Explain: Explain Error Type:

--- PAGE 29 ---
29
TABLE 18: A reasoning case in α-NLI.Question, answer
and explanationContext: When I was 19, I enlisted with the USMC. When I got back from the hospital, the USMC sent me home.
Question: Choose the more likely explanation from following two options: A. I got injured the last day during a
training event. B. I was shot in action but didn’t regret it.
Answer: BChatGPT
of 0-shotThe more likely explanation is hypothesis choice A: I got injured the last day during a training event. This is
because the context mentions the narrator enlisting with the USMC and being sent home after getting back from
the hospital, suggesting a military-related scenario. Hypothesis choice A of the narrator getting injured during a
training event aligns with these details and provides a probable explanation for why they may have been sent
home, as injuries sustained during training may prevent soldiers from fulfilling duties and may require medical
attention. Hypothesis choice B of the narrator being shot in action doesn’t fit with the context, as there is no
indication of any mention of being in combat or any explanation for how this would have been related to the
narrator’s hospitalization or being sent home, and it doesn’t provide an explanation for what may have motivated
the narrator’s military-related interests or preferences.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotThe correct choice is A. The context suggests that the person was enlisted in the USMC and got sent home from
the hospital, indicating that they likely got injured during a training event. Choice B suggests an unlikely scenario
where the person got shot in action, which cannot be inferred from the context and contradicts the fact that the
person did not regret it.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotThe correct choice is: A. The context suggests that the narrator enlisted with the USMC and was sent home from
the hospital. Hypothesis A, where the narrator got injured the last day during a training event, is the more likely
explanation because it fits with the idea of being sent home from the hospital after being injured in the line of
duty. Hypothesis B, where the narrator was shot in action but didn’t regret it, does not make sense as a plausible
explanation for being sent home from the hospital expressed in the context.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotThe more likely explanation is A. I got injured the last day during a training event. This could have been caused
by some sort of accident with a weapon or a physical training exercise gone wrong. It is more likely that I would
have been injured on the last day of service, as opposed to being shot in action, which would be less likely in the
US Marine Corps.
Answer and Explain: Explain Error Type:Bard
of 0-shotThe more likely explanation is A. I got injured the last day during a training event.
This is because the context states that you enlisted with the USMC at 19, and that you were sent home from the
hospital after being injured. This suggests that the injury occurred during training, and that it was serious enough
to warrant your discharge from the military.
Hypothesis B is less likely because it is not supported by the context. The context does not mention anything
about you being shot in action, or about you not regretting it. Additionally, it is not clear why the USMC would
send you home if you were not injured.
Here is a simple explanation for why you might have been injured during training:
You were involved in a training accident.
You overexerted yourself and suffered an injury.
You were injured by another Marine during training.
It is important to remember that these are just possible explanations, and we cannot know for sure why you were
injured without more information.
Answer and Explain: Explain Error Type:

--- PAGE 30 ---
30
TABLE 19: A reasoning case in α-NLG.Question, answer
and explanationContext: Joy’s mom sent her in the donut store for a dozen donuts. She was happy as she left with 13 donuts instead
of 12.
Question: Generate a short hypothesis for the context and give simple explanations.
Answer: Joy got an extra donut.ChatGPT
of 0-shotHypothesis: The fact that Joy’s mom sent her to the donut store for a dozen donuts, and Joy left with 13 instead
of 12 might have been a result of the potential personal or environmental factors related to perception, generosity,
or habit.
Explanation: The fact that Joy’s mom sent her to the donut store for a dozen donuts suggests that it might have
some potential personal or environmental factors related to perception, generosity, or habit. Having personal or
environmental factors can involve various types of cognitive or practical factors, such as accuracy, generosity, or
routine, and can reflect one’s sense of responsibility or automaticity. Thus, the fact that Joy’s mom sent her to the
donut store for a dozen donuts might have been a result of the potential personal or environmental factors related
to perception, generosity, or habit. Additionally, the fact that Joy left with 13 donuts instead of 12 implies that
there might have been some additional potential cognitive or contextual factors related to accuracy, availability,
or preference. Having cognitive or contextual factors can involve various types of perception or action, such as
estimation, surplus, or choice, and can reflect one’s sense of intention or opportunity. Furthermore, the fact that
Joy was happy as she left with 13 donuts instead of 12 suggests that there might have been some potential affective
or symbolic factors related to value, satisfaction, or normativity. Having affective or symbolic factors can involve
various types of appraisal or intent, such as pleasure, gratitude, or conformity, and can reflect one’s sense of
social or moral obligation. Therefore, the hypothesis suggests that the fact that Joy’s mom sent her to the donut
store for a dozen donuts, and Joy left with 13 instead of 12 might have been a result of the potential personal or
environmental factors related to perception, generosity, or habit, as well as the potential cognitive or contextual
factors related to accuracy, availability, or preference.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotHypothesis: The reason why Joy left the donut store with 13 instead of 12 donuts may be due to a generous gesture
from the store clerk, a promotional offer that allowed her to receive an extra donut for free, a mistake made by the
store clerk, or Joy’s persuasive negotiation or effective communication skills.
Explanation: The fact that Joy left the donut store with 13 instead of 12 donuts suggests that there may have been
an unexpected addition or alteration to her order. One possibility is that the store clerk may have generously
offered Joy an additional donut as a kind gesture or to show appreciation for her business. Another possibility
is that there may have been a promotional offer where customers who purchased a dozen donuts received an
additional donut for free. Alternatively, a mistake may have been made by the store clerk when fulfilling Joy’s
order, resulting in an extra donut being added to her purchase. Finally, Joy may have effectively persuaded or
negotiated with the store clerk to receive an extra donut, perhaps by demonstrating loyal patronage or simply
asking for an additional donut. These factors may have contributed to Joy’s pleasant surprise at leaving the donut
store with an extra donut, and highlight the importance of customer service, promotions, careful order fulfillment,
and communication skills in the food service industry. Therefore, the hypothesis proposes that the reason why
Joy left the donut store with 13 instead of 12 donuts may be due to a generous gesture from the store clerk, a
promotional offer that allowed her to receive an extra donut for free, a mistake made by the store clerk, or Joy’s
persuasive negotiation or effective communication skills.
Answer and Explain: ChatGPT
of 3-shotHypothesis: The donut shop may have made an error and accidentally given Joy an extra donut, which she was
pleased to receive. Alternatively, the donut store may have had a special promotion where customers receive an
extra donut for free, which Joy was not aware of but happily benefited from. This scenario highlights the potential
for unexpected surprises and bonuses in our daily lives.
Answer and Explain: Davinci-003
of 0-shotHypothesis: The donut store owner gave Joy an extra donut for free.
Explanation: The extra donut could indicate that the store owner gave Joy an extra donut for free as a gesture of
goodwill or appreciation. This could be due to Joy being loyal customer at the store, or due to her being a child
and the store owner having sympathy for her.
Answer and Explain: 

--- PAGE 31 ---
31Bard
of 0-shotSure, here are some possible hypotheses for why Joy was happy when she left the donut store with 13 donuts
instead of 12:
The donut store was giving away a free donut with every dozen purchased.
The donut store made a mistake and gave Joy an extra donut.
Joy was able to negotiate with the donut store owner to get an extra donut.
Joy is a very good friend or family member and the donut store owner wanted to give her a little extra something.
Here are some simple explanations for each hypothesis:
The donut store was giving away a free donut with every dozen purchased because they were trying to promote
their business or because they had too many donuts on hand.
The donut store made a mistake and gave Joy an extra donut because they were busy or because they were not
paying attention.
Joy was able to negotiate with the donut store owner to get an extra donut because she was polite and persuasive.
Joy is a very good friend or family member and the donut store owner wanted to give her a little extra something
because they appreciated her business or because they were close friends or family.
It is impossible to know for sure which hypothesis is correct, but all of them are possible explanations for why Joy
was happy when she left the donut store with 13 donuts instead of 12.
Answer and Explain: 

--- PAGE 32 ---
32
TABLE 20: A reasoning case in AbductiveRules.Question, answer
and explanationContext: 1 Gary is strong. 2 Things that are tiny, are small, and are short, are thin. 3 All people that are quiet, are
poor, and are smart, are clever. 4 Gary is big. 5 If something is short, is tiny, and is nice, it is also thin. 6 If a person
is bad, is sad, and is strong, that person is rough. 7 Things that are strong, are huge, and are big, are high. 8 People
that are bad, are short, and are strong, are also clever. 9 Dave is poor. 10 Dave is imperfect. 11 Charlie is quiet. 12
Gary is heavy. 13 Anne is tiny. 14 All things that are huge, are small, and are big, are also high. 15 All things that are
quiet, are huge, and are poor, are also thin. 16 Anne is little. 17 Charlie is nice. 18 Dave is bad. 19 If a person is tiny,
is nice, and is sad, that person is high. 20 If a person is quiet, is nice, and is smart, that person is clever. 21 All things
that are small, are big, and are smart, are rough. 22 Anne is small. 23 People that are bad, are poor, and are sad, are
also rough.
Question: Please generate a short explanation for the given context and observation. The observation is: Dave is
rough.
Answer: Dave is sad.
Explanation: 9, 1823− →Dave is sad.ChatGPT
of 0-shotBased on the given context, individuals who possess traits such as badness, sadness, and strength are typically
perceived as rough. Dave has been described as poor and imperfect which would suggest a degree of hardship
and inadequacy, and the observation confirms that he is perceived as rough. Therefore, it can be inferred that
Dave’s perceived roughness is a result of his perceived badness, sadness, and poor state in accordance with the
given context.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotThe context indicates that being bad, sad, and strong is associated with being rough, and being poor and imperfect
are also negative qualities. The observation that Dave is rough suggests that he may possess these negative traits
or exhibit aggressive behavior.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotThe given observation contradicts the context, as it states that Dave is rough whereas the context indicates that he
is poor and imperfect. Therefore, there is no valid explanation for this observation within the given context.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotDave is rough because he is bad, poor, and strong, which meets the criteria of people that are bad, poor, and strong
being rough.
Answer and Explain: Explain Error Type:Bard
of 0-shotSure, here is a short explanation for the given context and observation:
Dave is rough because he is bad, sad, and strong. He is also poor and imperfect. This suggests that he is also quiet
and clever. However, he is also tiny and small. This suggests that he is also weak.
Here is a more detailed explanation:
Dave is bad. This is evident from the fact that he is described as “bad” and “rough”.
Dave is sad. This is evident from the fact that he is described as “sad” and “imperfect”.
Dave is strong. This is evident from the fact that he is described as “strong” and “huge”.
Dave is poor. This is evident from the fact that he is described as “poor” and “imperfect”.
Dave is imperfect. This is evident from the fact that he is described as “sad” and “imperfect”.
Dave is quiet. This is evident from the fact that he is described as “quiet” and “clever”.
Dave is clever. This is evident from the fact that he is described as “clever” and “strong”
Dave is tiny. This is evident from the fact that he is described as “tiny” and “small”.
Dave is small. This is evident from the fact that he is described as “small” and “weak”.
Dave’s badness, sadness, strength, poverty, imperfection, quietness, cleverness, tininess, and smallness suggest
that he is a complex and contradictory person. He is capable of great things, but he is also capable of great evil.
He is a force to be reckoned with, and he will not be easily defeated.
Answer and Explain: Explain Error Type:

--- PAGE 33 ---
33
TABLE 21: A reasoning case in D*-Ab.Question, answer
and explanationContext: Fact1: The bear chases the cat. Fact2: The bear is big. Fact3: The bear is cold. Fact4: The bear likes the
squirrel. Fact5: The bear likes the tiger. Fact6: The bear sees the squirrel. Fact7: The cat is big. Fact8: The cat is cold.
Fact9: The cat is young. Fact10: The cat likes the bear. Fact11: The cat likes the tiger. Fact12: The cat sees the bear.
Fact13: The cat sees the tiger. Fact14: The squirrel is young. Fact15: The tiger chases the bear. Fact16: The tiger likes
the squirrel. Rule1: If the tiger is young then the tiger likes the squirrel. Rule2: If something is round then it likes the
bear. Rule3: If something likes the bear then it is cold. Rule4: If the tiger likes the bear and the bear sees the squirrel
then the squirrel is round. Rule5: If something likes the squirrel then it is round. Rule6: If something chases the bear
and the bear likes the cat then the cat likes the squirrel.
Question: Please generate a short missing fact for the given context and fact. And give simple explanations. The fact
is: The cat likes the squirrel.
Answer: The bear likes the cat.
Explanation: Fact15Rule 6− − − − → The bear likes the cat.ChatGPT
of 0-shotMissing fact: The squirrel is round.
Explanation: According to the given rules, if the tiger likes the bear and the bear sees the squirrel, then the squirrel
is round. As the tiger likes the squirrel in the given context, we can conclude that the tiger likes the bear, and since
the bear likes the squirrel, we can infer that the squirrel is round.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotThe fact that the cat likes the squirrel suggests a potential behavioral or social relationship between the two
animals, such as companionship or mutual interest in resources. The missing fact that Anne is young could imply
a new or additional character to the context.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotAccording to the context, the bear likes the squirrel, and the cat likes the bear, which could suggest a predator-prey
relationship between the cat and the squirrel. Moreover, given the fact that the cat likes the squirrel, it is possible
that the cat may try to protect the squirrel from the bear’s chase. Additionally, the context suggests that.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotThe missing fact is: The tiger is young. Explanation: The rule1 states that if the tiger is young then the tiger likes
the squirrel. The fact16 states that the tiger likes the squirrel, so the tiger must also be young.
Answer and Explain: Explain Error Type:Bard
of 0-shotSure, here is a short missing fact for the given context and fact:
The squirrel sees the cat.
This is the missing fact because it is the only fact that can explain all of the given facts and rules. For example,
Rule 4 says that if the tiger likes the bear and the bear sees the squirrel then the squirrel is round. However, we
know that the squirrel is round, so the squirrel must also see the bear.
Here are some simple explanations for the missing fact:
The squirrel is friends with the cat.
The squirrel is related to the cat.
The squirrel is a food source for the cat.
I hope this helps! Let me know if you have any other questions.
However, I think there is a mistake in the given context. The fact that “The cat likes the squirrel” contradicts the
fact that ”The cat chases the squirrel”. If the cat likes the squirrel, then it would not chase the squirrel. Therefore,
one of these facts must be incorrect.
Answer and Explain: Explain Error Type:

--- PAGE 34 ---
34
TABLE 22: A reasoning case in ReClor.Question, answer
and explanationContext: A science class stored one selection of various fruits at 30 degrees Celsius, a similar selection in similar
conditions at 20 degrees, and another similar selection in similar conditions at 10 degrees. Because the fruits stored
at 20 degrees stayed fresh longer than those stored at 30 degrees, and those stored at 10 degrees stayed fresh longest,
the class concluded that the cooler the temperature at which these varieties of fruits are stored, the longer they will
stay fresh.
Question: The class’s reasoning is flawed in that the class A. neglected to offer any explanation for the results it
discovered B. assumed without proof that its thermometer was reliable C. generalized too readily from the fruits it
tested to fruits it did not test D. too readily extrapolated from a narrow range of temperatures to the entire range of
temperatures.
Answer: DChatGPT
of 0-shotThe correct answer is D. Option A is not relevant to the class’s reasoning, as it still made an attempt to draw a
conclusion from its results. Option B is a possible flaw, but it is not stated in the argument whether the thermometer
was reliable or not, so it cannot be the correct answer. Option C is a possible flaw, but it is not stated whether the
class attempted to generalize its results to fruits it did not test, so it cannot be the correct answer. Option D is the
correct answer, as the class too readily made a generalization from a narrow range of temperatures (10 degrees,
20 degrees, and 30 degrees) to the entire range of temperatures at which fruits can be stored, which is likely to be
much broader.
Answer and Explain: ChatGPT
of 1-shotThe correct answer is C. The class’s reasoning is flawed in that it generalized too readily from the fruits it tested to
fruits it did not test. The experiment only tested a specific selection of fruits and did not test all varieties of fruits,
so it is not necessarily true that cooler temperatures will always keep fruits fresh longer. Option A is incorrect
because the class did offer an explanation for the results they discovered. Option B is not relevant to the flaw
in the class’s reasoning. Option D is also not relevant, as the class did not extrapolate from a narrow range of
temperatures to the entire range of temperatures.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotThe correct answer is D. The class’s reasoning is flawed in that they too readily extrapolated from a narrow range
of temperatures to the entire range of temperatures. The class only tested three temperatures, and did not consider
the possibility of a threshold beyond which colder temperatures may not be beneficial for all fruits. Therefore, it is
too hasty to conclude that ¨the cooler the temperature, the longer the fruits will stay fresh ¨based on such a limited
range of temperatures.
Answer and Explain: Davinci-003
of 0-shotThe correct answer is C. The class’s reasoning is flawed in that the class generalized too readily from the fruits it
tested to fruits it did not test. The class was only testing three types of fruit at three different temperatures, which
does not truly demonstrate how all different varieties of fruits can react differently to varying temperatures. Thus,
the class should not make a hasty generalization about how all fruits would respond to different temperatures.
Answer and Explain: Explain Error Type:Bard
of 0-shotThe correct answer is D. Too readily extrapolated from a narrow range of temperatures to the entire range of
temperatures.
The class only tested fruits at three temperatures: 30 degrees Celsius, 20 degrees Celsius, and 10 degrees Celsius.
They concluded that the cooler the temperature, the longer the fruits will stay fresh. However, this conclusion is
based on a very small sample size. It is possible that the relationship between temperature and freshness is not
linear. For example, it is possible that fruits will stay fresh for the same amount of time at 10 degrees Celsius and
5 degrees Celsius. The class would need to test fruits at a wider range of temperatures to confirm their conclusion.
The other options are not correct. Option A is incorrect because the class did offer an explanation for their results:
they said that the cooler the temperature, the longer the fruits will stay fresh. Option B is incorrect because the class
did not assume that their thermometer was unreliable. Option C is incorrect because the class did not generalize
to fruits they did not test. They only tested fruits at three temperatures, and they only made a conclusion about
those three temperatures.
Answer and Explain: 

--- PAGE 35 ---
35
TABLE 23: A reasoning case in LogiQA.Question, answer
and explanationContext: People often use microwave ovens to heat food.Some people think that microwave oven heating changes
the molecular structure of food, producing molecules that the human body cannot recognize.These strange molecular
formulas are unacceptable to the human body, and some are toxic and may even cause cancer. Therefore, people or
animals who often eat microwave foods will have serious physiological changes in their bodies, which will cause
serious health problems.
Question: Which of the following can best question the above view? A. Microwave heating will not cause more
nutrient loss than other cooking methods B. China’s microwave oven production standards are consistent with
international standards and EU standards C. It is also common for developed countries to use microwave ovens D.
The microwave only heats the water molecules in the food, the food does not change chemically.
Answer: DChatGPT
of 0-shotThe answer is D. The given argument suggests that microwave oven heating changes the molecular structure of
food, producing molecules that the human body cannot recognize and causing serious health problems. Option D
provides an alternative explanation for the process of microwave heating, which states that the microwave only
heats the water molecules in the food, and the food does not change chemically. Therefore, option D best questions
the view presented in the argument. Option A is not relevant as it talks about nutrient loss rather than the effects
of microwave heating on food. Option B and option C are not relevant as they provide information about the
production standards of microwave ovens and their usage in different countries, but do not impact the argument
presented in the context.
Answer and Explain: ChatGPT
of 1-shotThe correct choice is: D. The argument suggests that the use of microwave ovens to heat food changes the
molecular structure of the food, producing molecules that the human body cannot recognize, which can lead to
serious health problems. Option D challenges this argument by stating that the microwave only heats the water
molecules in the food and does not change the chemical structure of the food. This suggests that the argument may
be based on a misunderstanding of how microwaves work. Option A, B, and C are not relevant to the argument
or do not challenge the premise that microwaves can cause health problems.
Answer and Explain: ChatGPT
of 3-shotThe correct choice is: D. The microwave only heats the water molecules in the food, the food does not change
chemically, can best question the above view. It suggests that the claim about microwave ovens changing the
molecular structure of food is not accurate and provides a scientific explanation for how microwave heating
works, which does not involve the production of strange or toxic molecules.
Answer and Explain: Davinci-003
of 0-shotThe best answer is B. China’s microwave oven production standards are consistent with international standards
and EU standards. This answer questions the view presented in the context that microwave oven heating changes
the molecular structure of food, since it implies that safety regulations surrounding the production of microwave
ovens would never be put in place if this were true. Therefore, this disproves the view that microwave oven
heating produces molecules that the human body cannot recognize, which could be toxic and may even cause
cancer.
Answer and Explain: Explain Error Type:Bard
of 0-shotThe correct answer is D. The microwave only heats the water molecules in the food, the food does not change
chemically.
The context states that some people think that microwave oven heating changes the molecular structure of food,
producing molecules that the human body cannot recognize. However, this is not true. Microwaves only heat the
water molecules in food, and the food does not change chemically. This is why microwaved food is safe to eat.
The other options are not necessarily correct. Option A is incorrect, as there is some evidence that microwave
heating can cause nutrient loss. Option B is irrelevant, as it does not address the question of whether or not
microwave oven heating changes the molecular structure of food. Option C is also irrelevant, as it does not
address the question of whether or not microwaved food is safe to eat.
Here are some additional information about microwaves and their effects on food:
Microwaves are a form of non-ionizing radiation, which means that they do not have enough energy to damage
DNA.
Microwaves heat food by causing water molecules to vibrate. This vibration produces heat, which cooks the food.
Microwaves do not change the chemical structure of food. This means that microwaved food is safe to eat.
Some studies have shown that microwave heating can cause nutrient loss. However, these studies have been
small and the results have been mixed. More research is needed to determine the extent of nutrient loss from
microwaved food.
Overall, there is no evidence that microwave oven heating is harmful to human health. Microwaved food is safe
to eat.
Answer and Explain: Explain Error Type:

--- PAGE 36 ---
36
TABLE 24: A reasoning case in LogiQA 2.0.Question, answer
and explanationContext: In a track and field qualifier, Zhang Qiang surpassed Li Jin, and Song Zhi’s results were better than Wang
Ping, but not as good as Ma Zheng.
Question: Which of the following options can be inferred? A. Ma Zheng’s grades are better than Zhang Qiang. B. Li
Jin’s results surpass Wang Ping. C. Zhang Qiang’s performance is better than Song Zhi. D. Among the five people,
Wang Ping ranked third at most.
Answer: DChatGPT
of 0-shotIf Zhang Qiang surpassed Li Jin and Song Zhi’s results were better than Wang Ping, but not as good as Ma Zheng,
it can be inferred that Zhang Qiang’s performance is better than Song Zhi. While the relative positions of Zhang
Qiang, Li Jin, Wang Ping, and Ma Zheng are known, there is no information to suggest that Ma Zheng’s grades
are better than Zhang Qiang (option A) or that Li Jin’s results surpass Wang Ping (option B). Option D introduces
a ranking that is not directly supported by the information in the passage.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotThe correct answer is D. From the given information, we know that Zhang Qiang’s result was better than Li Jin’s,
and Song Zhi’s result was better than Wang Ping’s but not as good as Ma Zheng’s. Therefore, we can infer that Ma
Zheng’s result was better than all the other athletes, including Zhang Qiang, and Li Jin’s result was worse than
both Zhang Qiang’s and Wang Ping’s. However, we cannot determine the exact ranking of Wang Ping among the
five athletes with the given information, so option D is the correct choice as it cannot be inferred from the given
context.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotThe correct answer is C. The passage states that Zhang Qiang surpassed Li Jin and that Song Zhi’s results were
better than Wang Ping but not as good as Ma Zheng. Therefore, it can be inferred that Zhang Qiang’s performance
is better than Song Zhi’s since he surpassed Li Jin. Choice A cannot be inferred because Ma Zheng’s performance
isn’t compared to Zhang Qiang’s. Choice B also cannot be inferred because there’s no comparison between Li Jin’s
and Wang Ping’s performance. Choice D cannot be inferred because there’s no information to suggest that Wang
Ping ranked third at most.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotThe correct answer is C. Zhang Qiang ´s performance is better than Song Zhi. This can be inferred from the given
context, as the context stated that ”Zhang Qiang surpassed Li Jin”, which means Zhang Qiang is better than Li Jin,
and ”Song Zhi ´s results were better than Wang Ping”, which means Song Zhi is better than Wang Ping, therefore
Zhang Qiang is better than Song Zhi. Therefore, option C is the correct choice.
Answer and Explain: Explain Error Type:Bard
of 0-shotThe correct answer is D. Among the five people, Wang Ping ranked third at most.
The passage states that Zhang Qiang surpassed Li Jin, and Song Zhi’s results were better than Wang Ping, but not
as good as Ma Zheng. This information can be organized into the following table:
— Athlete — Results —
— Zhang Qiang — Better than Li Jin —
— Song Zhi — Better than Wang Ping —
— Ma Zheng — Better than Song Zhi —
From this table, we can see that Wang Ping’s results are not as good as Ma Zheng’s, so Wang Ping ranked third at
most.
The other answer choices are not supported by the passage. Answer choice A is not supported because the passage
does not say that Ma Zheng’s grades are better than Zhang Qiang’s. Answer choice B is not supported because
the passage does not say that Li Jin’s results surpass Wang Ping’s. Answer choice C is not supported because the
passage does not say that Zhang Qiang’s performance is better than Song Zhi’s.
Answer and Explain: 

--- PAGE 37 ---
37
TABLE 25: A reasoning case in LogiQA2NLI.Question, answer
and explanationContext: The premis is: Researcher: People who participate in opinion surveys often give answers they believe
the opinion surveyor expects to hear, and it is for this reason that some opinion surveys do not reflect the actual
views of those being surveyed. However, in well-constructed surveys, the questions are worded so as to provide
respondents with no indication of which answers the surveyor might expect. So if a survey is well constructed,
survey respondents’ desire to meet surveyors’ expectations has no effect on the survey’s results. The conclusion is:
the answers of opinion-survey respondents can be influenced by beliefs about the surveyor’s expectations even if
those beliefs are unfounded.
Question: Determine whether the premise and conclusion entailed or NOT entailed.
Answer: entailedChatGPT
of 0-shotThe conclusion is not logically entailed by the given premise. The premise discusses the potential for opinion
survey respondents to give answers they believe the surveyor expects to hear, but notes that well-constructed
surveys prevent this by wording questions in a way that does not indicate expected answers. It then concludes
that if a survey is well constructed, respondents’ desire to meet surveyors’ expectations has no effect on the
survey’s results. Therefore, it does not support the conclusion that the answers of opinion-survey respondents can
be influenced by beliefs about the surveyor’s expectations even if those beliefs are unfounded.
Answer and Explain: Explain Error Type:ChatGPT
of 1-shotThe conclusion is not entailed by the given premise. While the premise discusses how well-constructed surveys
are worded so as to provide respondents with no indication of which answers the surveyor might expect, and
how this can mitigate respondents’ desires to meet surveyors’ expectations, it does not provide information to
support or refute the claim that the answers of opinion-survey respondents can be influenced by beliefs about the
surveyor’s expectations even if those beliefs are unfounded. Therefore, the conclusion cannot be reliably inferred
from the given premise.
Answer and Explain: Explain Error Type:ChatGPT
of 3-shotNot entailed. The conclusion about the answers of opinion-survey respondents being influenced by beliefs about
the surveyor’s expectations cannot be logically derived from the given premise.
Answer and Explain: Explain Error Type:Davinci-003
of 0-shotNOT entailed. The premise is implying that because the survey is well constructed with no indication of what
answers the surveyor might expect, the survey respondents’ answers will not be impacted by any beliefs they
have of the surveyor’s expectations. The conclusion is suggesting that even if those beliefs are unfounded, the
responses will still be influenced by them. This goes against the premise as the premise has stated that well-
constructed surveys will give no indication of the surveyor’s expectations, and therefore, their beliefs of the
surveyor’s expectations will have no influence.
Answer and Explain: Explain Error Type:Bard
of 0-shotThe conclusion is entailed by the premise. The premise states that people who participate in opinion surveys often
give answers they believe the opinion surveyor expects to hear, even if those beliefs are unfounded. It also states
that well-constructed surveys are worded so as to provide respondents with no indication of which answers the
surveyor might expect.
This means that even if a survey is well-constructed, respondents may still be influenced by beliefs about the
surveyor’s expectations. This is because people are often motivated to please others, and they may believe that
giving the answer they think the surveyor expects will make them look good.
For example, a respondent may believe that a surveyor expects them to support a certain candidate in an election.
Even if the survey question is not leading, the respondent may still give the answer they think the surveyor
expects, even if they do not actually support that candidate.
This is why it is important for surveyors to be aware of the potential for respondents to be influenced by beliefs
about their expectations. Surveyors can try to minimize this effect by using clear and unbiased language, and by
explaining to respondents that there are no right or wrong answers.
Answer and Explain: Explain Error Type:
