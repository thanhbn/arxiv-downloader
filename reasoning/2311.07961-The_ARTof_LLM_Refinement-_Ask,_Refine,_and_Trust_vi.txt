# Nghệ thuật tinh chỉnh LLM: Hỏi, Tinh chỉnh và Tin tưởng

Kumar Shridhar⋄ ∗Koustuv Sinha♠Andrew Cohen♠Tianlu Wang♠Ping Yu♠
Ram Pasunuru♠Mrinmaya Sachan⋄Jason Weston♠Asli Celikyilmaz♠
⋄ETH Zurich♠Meta AI

## Tóm tắt

Các Mô hình Ngôn ngữ Lớn (LLM) đã thể hiện khả năng sinh văn bản đáng kể, nhưng liệu chúng có thể đánh giá chất lượng của những sinh phẩm của chính mình không? Một khái niệm phổ biến, được gọi là tự tinh chỉnh, cho rằng LLM có thể phát hiện và sửa chữa các lỗi trong sinh phẩm của chúng khi được yêu cầu làm vậy. Tuy nhiên, bằng chứng thực nghiệm gần đây chỉ ra hướng ngược lại, cho thấy LLM thường gặp khó khăn trong việc xác định chính xác các lỗi khi có liên quan đến lý luận. Để giải quyết điều này, chúng tôi đề xuất một chiến lược lý luận với tinh chỉnh gọi là ART: Hỏi, Tinh chỉnh và Tin tưởng, trong đó đặt ra những câu hỏi cần thiết để quyết định khi nào LLM nên tinh chỉnh đầu ra của mình, và khẳng định hoặc từ chối tin tưởng vào việc tinh chỉnh bằng cách xếp hạng việc tinh chỉnh và dự đoán ban đầu. Trên hai nhiệm vụ lý luận đa bước về bài toán văn toán học (GSM8K) và trả lời câu hỏi (StrategyQA), ART đạt được mức tăng hiệu suất 5 điểm so với baseline tự tinh chỉnh, trong khi sử dụng một mô hình nhỏ hơn nhiều làm người ra quyết định. Chúng tôi cũng chứng minh lợi ích của việc sử dụng các mô hình nhỏ hơn để đưa ra quyết định tinh chỉnh như một giải pháp thay thế hiệu quả về chi phí cho việc tinh chỉnh một mô hình lớn hơn.

## 1 Giới thiệu

Khả năng của các Mô hình Ngôn ngữ Lớn (LLM) trong việc sinh ra văn bản mạch lạc và có ý nghĩa đã được cải thiện đáng kể qua các năm (OpenAI, 2023). Tuy nhiên, LLM thường thể hiện những không chính xác trong sinh phẩm ban đầu của chúng, và người ta cho rằng việc tinh chỉnh lặp có thể sửa chữa các lỗi của chúng (Madaan et al.; Shridhar et al., 2023a; Welleck et al.; Zheng et al., 2023). Madaan et al. đã chứng minh tiềm năng của tự tinh chỉnh cho các nhiệm vụ đa dạng như phản hồi đối thoại và đảo ngược cảm xúc; tuy nhiên, phương pháp này ít hiệu quả hơn khi áp dụng cho lý luận toán học. Tương tự, Shridhar et al. (2023a) và Huang et al. (2023) tiếp tục chứng minh những thách thức mà LLM gặp phải trong việc xác định lỗi trong các nhiệm vụ lý luận. Phát triển các mô hình có thể đánh giá và sửa chữa lỗi một cách nhất quán sẽ là một bước có giá trị hướng tới xây dựng các mô hình ngôn ngữ đáng tin cậy hơn.

Thông qua quan sát thực nghiệm trên hai bộ dữ liệu lý luận đa bước, chúng tôi thấy rằng tự tinh chỉnh không cải thiện sinh phẩm ban đầu một cách đáng tin cậy, xác nhận các phát hiện trước đó của Huang et al. (2023). Thực tế, trong phần lớn các trường hợp, tự tinh chỉnh có tác động có hại đến hiệu suất. Mặt khác, tinh chỉnh các mô hình ngôn ngữ thường cải thiện hiệu suất của chúng trên một nhiệm vụ nhất định bằng cách tạo điều kiện thích ứng tốt hơn với các mục tiêu nhiệm vụ (Yuan et al., 2023). Các mô hình nhỏ hơn có thể được huấn luyện trên dữ liệu của LLM để cải thiện hiệu suất của chúng, có thể phục vụ như những giải pháp thay thế hiệu quả về chi phí cho LLM đối với nhiệm vụ đã cho (Magister et al., 2023; Shridhar et al., 2023b; Hsieh et al., 2023). Điều này đã dẫn chúng tôi khám phá khả năng huấn luyện một mô hình nhỏ hơn làm người ra quyết định cho việc tinh chỉnh, có thể xác định một cách nhất quán khi nào cần tinh chỉnh, trong khi mô hình lớn hơn có thể thực hiện quy trình tinh chỉnh sau đó.

Trong công trình của chúng tôi, chúng tôi đề xuất một phương pháp tinh chỉnh gọi là ART: Hỏi, Tinh chỉnh và Tin tưởng, trong đó, khi có một phản hồi LLM ban đầu, hoạt động theo ba giai đoạn sau: (a) đánh giá liệu sinh phẩm ban đầu có cần tinh chỉnh bằng cách đặt một loạt câu hỏi (Hỏi); (b) thực hiện bước tinh chỉnh dựa trên đánh giá (Tinh chỉnh); và cuối cùng (c) chọn kết quả được tinh chỉnh hoặc dự đoán ban đầu (Tin tưởng). Trên hai nhiệm vụ lý luận đa bước, lý luận toán học và trả lời câu hỏi, chúng tôi minh họa hiệu quả của ART bằng cách huấn luyện các mô hình có kích thước khác nhau. Chúng tôi quan sát thấy rằng một mô hình nhỏ hơn nhiều (LLaMA 7B; Touvron et al., 2023) được huấn luyện để quyết định khi nào tinh chỉnh, có thể vượt trội hơn một mô hình lớn gấp 10 lần (LLaMA 70B) trong thiết lập tự tinh chỉnh (lên đến 5 điểm). Ngoài ra, chúng tôi đánh giá sự đánh đổi về chi phí và độ chính xác của việc huấn luyện một mô hình nhỏ hơn với ART để đưa ra quyết định tinh chỉnh cho một LLM được tiền huấn luyện so với tinh chỉnh LLM. Trong nhiều trường hợp, chúng tôi minh họa tính hiệu quả về chi phí của ART như một giải pháp thay thế khả thi cho việc tinh chỉnh LLM. Cuối cùng, chúng tôi cho thấy rằng các mô hình được huấn luyện của chúng tôi (Asker và Truster) có thể hoạt động liền mạch trên một loạt rộng các LLM (LLaMA 70B (Touvron et al., 2023), ChatGPT (Brown et al., 2020) và GPT-4 (OpenAI, 2023)) mà không cần các sửa đổi bổ sung.

## 2 Công trình liên quan

Các chiến lược sử dụng tính toán trung gian để giải quyết các nhiệm vụ lý luận như chuỗi suy nghĩ (Wei et al., 2022; Lewkowycz et al., 2022; Zhang et al., 2022; Kojima et al., 2022; Wang et al., 2022a; LYU et al., 2023) và phân tách câu hỏi phụ (Min et al., 2019; Shridhar et al., 2022; Zhou et al., 2023; Radhakrishnan et al., 2023) đã được chứng minh là rất hiệu quả. Hầu hết các kỹ thuật tinh chỉnh LLM sử dụng một trong hai chiến lược này (Madaan et al.; Welleck et al.; Huang et al., 2023; Paul et al., 2023; Yoran et al., 2023) hoặc thỉnh thoảng là sự kết hợp của cả hai (Shridhar et al., 2023a). Shridhar et al. (2023a) đã thống nhất các phương pháp lý luận với tinh chỉnh trong quá khứ dưới một khái niệm chung về lấy mẫu (cho một truy vấn, LLM sinh phản hồi ban đầu), lấy mẫu lại (LLM tinh chỉnh phản hồi ban đầu), và lựa chọn (chọn việc tinh chỉnh hoặc quay lại phản hồi ban đầu). Tuy nhiên, một LLM duy nhất được sử dụng để thực hiện sinh ban đầu, tinh chỉnh và lựa chọn sau đó bằng cách sử dụng các lời nhắc khác nhau. Ngược lại, chúng tôi đề xuất huấn luyện một mô hình riêng biệt, có dung lượng nhỏ hơn nhiều để đưa ra quyết định tinh chỉnh và sau đó quyết định có tin tưởng việc tinh chỉnh hơn sinh ban đầu hay không và không bị giới hạn bởi các giải pháp dựa trên lời nhắc.

Việc đặt câu hỏi để xác minh các sự kiện có trong dự đoán mô hình đã được nghiên cứu bởi Dhuliawala et al. (2023) trong bối cảnh phát hiện ảo giác. Tuy nhiên, công trình này chỉ xử lý các ảo giác dưới dạng những không chính xác thực tế được nêu trực tiếp. Điều quan trọng cần lưu ý là ảo giác có thể có nhiều hình thức khác, bao gồm các bước lý luận không chính xác. Để giải quyết điều này, chúng tôi huấn luyện một mô hình chuyên gia để xác minh từng bước lý luận bằng cách đặt các câu hỏi có liên quan.

Huấn luyện một mô hình để xếp hạng các đầu ra đã được nghiên cứu trong quá khứ trong nhiều bối cảnh khác nhau (Burges et al., 2005), bao gồm nhưng không giới hạn ở sinh văn bản (Krishna et al., 2022), các bài toán lý luận toán học (Cobbe et al., 2021), dịch máy (Tunstall et al., 2023), và nhiều hơn nữa. Tuy nhiên, chúng tôi không nghiên cứu thiết lập tiêu chuẩn của huấn luyện để xếp hạng chất lượng của các sinh phẩm, mà là để quyết định liệu việc tinh chỉnh có dẫn đến kết quả không chính xác và cần được hoàn tác hay không. Điều này có một số điểm tương đồng với tinh chỉnh lấy mẫu từ chối (Yuan et al., 2023), trong đó một mô hình được huấn luyện để sinh và thu thập các chuỗi lý luận chính xác như các bộ dữ liệu tinh chỉnh được tăng cường. Mặt khác, chúng tôi thu thập cả các chuỗi lý luận chính xác và không chính xác để xếp hạng các đầu ra.

Cuối cùng, công trình của chúng tôi tương tự như việc chưng cất các kỹ năng lý luận vào các mô hình nhỏ hơn (Shridhar et al., 2023b; Magister et al., 2023; Hsieh et al., 2023). Tuy nhiên, thay vì dạy các mô hình nhỏ hơn cách lý luận, chúng tôi huấn luyện các mô hình nhỏ hơn để đặt câu hỏi nhằm xác minh lý luận và quyết định liệu lý luận có chính xác hay không, điều này khác với việc đặt câu hỏi như lập kế hoạch để lý luận (Shridhar et al., 2022).

## 3 ART: Hỏi, Tinh chỉnh và Tin tưởng

Trong phần này, chúng tôi định nghĩa mục tiêu của phương pháp luận được đề xuất ART: Hỏi, Tinh chỉnh và Tin tưởng một cách chi tiết. Cho một truy vấn và một dự đoán ban đầu được sinh bởi LLM, ART sử dụng một pipeline có thể huấn luyện cho việc tinh chỉnh như sau: (a) đánh giá liệu sinh phẩm ban đầu có cần tinh chỉnh bằng cách đặt một loạt câu hỏi (Hỏi); (b) thực hiện bước tinh chỉnh dựa trên đánh giá (Tinh chỉnh); và cuối cùng (c) chọn kết quả được tinh chỉnh hoặc dự đoán ban đầu (Tin tưởng).

### 3.1 Dự đoán ban đầu

Cho một truy vấn nhiệm vụ x, LLM ψ sinh một dự đoán ban đầy y = ψ(x). Đối với các LLM được tiền huấn luyện, truy vấn x được tăng cường với một số ví dụ của nhiệm vụ như các lời nhắc few-shot, trong khi đối với các mô hình được tinh chỉnh, truy vấn được cung cấp trực tiếp mà không có ví dụ nào. Do tính chất lý luận đa bước của các nhiệm vụ trong đó các bước trung gian có lợi cho mô hình để đạt được câu trả lời cuối cùng, chúng tôi xem xét việc sử dụng Chuỗi Suy nghĩ (CoT; Wei et al., 2022) và Phân tách Câu hỏi phụ (Decomp; Shridhar et al., 2022; Zhou et al., 2023) làm hai phương pháp chính của chúng tôi cho dự đoán ban đầu.

### 3.2 Hỏi

Khi dự đoán ban đầu được sinh ra, bước tiếp theo là quyết định khi nào tinh chỉnh đầu ra. Tinh chỉnh mọi mẫu thường dẫn đến hiệu suất tệ hơn nhiều (Huang et al., 2023). Do đó, chúng tôi huấn luyện một Asker để xác định liệu một dự đoán có chính xác hay không, và sau đó chỉ tinh chỉnh những mẫu mà Asker không chắc chắn. Tuy nhiên, trước khi một mô hình nhỏ hơn có thể xác định liệu một câu trả lời được sinh có chính xác hay liệu có cần tinh chỉnh hay không, điều quan trọng là phải điều chỉnh mô hình với kiến thức cụ thể của nhiệm vụ và kết quả mong đợi. Chúng tôi tinh chỉnh mô hình nhỏ hơn theo phong cách CoT (các bước trung gian với câu trả lời cuối cùng, như được hiển thị bởi "Dự đoán Ban đầu" trong Hình 1) trên dữ liệu huấn luyện. Tiếp theo, chúng tôi tạo bộ dữ liệu để huấn luyện mô hình Asker. Chúng tôi sử dụng LLM ψ để sinh k dự đoán cho mỗi ví dụ trên tập huấn luyện, và sau đó gán nhãn "Có" hoặc "Không" cho việc tinh chỉnh dựa trên việc dự đoán có chính xác hay không chính xác (số lượng chính xác được trình bày trong Bảng 1). Đối với mỗi dự đoán, chúng tôi thêm các câu hỏi phụ có trong bộ dữ liệu trước quyết định "Có" hoặc "Không" để tiếp tục huấn luyện mô hình được tinh chỉnh. Theo cách này, Asker học cách đặt các câu hỏi có liên quan trước, ánh xạ chúng với dự đoán và sau đó quyết định liệu tất cả các câu hỏi của nó có được trả lời trong dự đoán hay không, dẫn đến quyết định tinh chỉnh. Một ví dụ được trình bày trong phụ lục Hình 6.

### 3.3 Tinh chỉnh

Nếu Asker dự đoán "Có" (cần tinh chỉnh), thì LLM ψ được sử dụng để tinh chỉnh đầu ra với đầu vào và các câu hỏi phụ từ mô hình Asker, yref = ψ(x; subq). Tương tự như Shridhar et al. (2023a), đối với bộ dữ liệu StrategyQA, các sự kiện bổ sung (facts) cũng được cung cấp cho mô hình ψ trong quá trình tinh chỉnh (yref = ψ(x; subq; facts)). Một ví dụ được trình bày trong phụ lục Hình 7.

### 3.4 Tin tưởng

Cuối cùng, để quyết định liệu đầu ra tinh chỉnh có nên được ưa thích hơn sinh ban đầu hay không, chúng tôi huấn luyện một Truster nhận hai ứng viên (y, yref) cho truy vấn nhiệm vụ x và quyết định cái nào để ưa thích hơn cái kia. Một ví dụ được trình bày trong phần phụ lục 7. Tuy nhiên, trong 80% các trường hợp, câu trả lời cuối cùng của việc tinh chỉnh yref và dự đoán ban đầu y là giống nhau. Mục tiêu của chúng tôi là làm cho Truster học cách xác định chuỗi lý luận với câu trả lời cuối cùng chính xác chứ không phải một chuỗi lý luận trung gian có phong cách cụ thể. Để tạo một bộ dữ liệu huấn luyện có kích thước tốt, chúng tôi đã sử dụng cùng dữ liệu huấn luyện được thu thập cho mô hình Asker (Bảng 1) và chọn các mẫu dự đoán có cả dự đoán chính xác và không chính xác. Chúng tôi xây dựng các cặp ưa thích (dự đoán chính xác) so với không ưa thích (dự đoán không chính xác) và huấn luyện một Truster với mục tiêu phân loại văn bản như:

Lθ = −Ex,yj,yk∼D[log(σ(rθ(x, yj) − rθ(x, yk)))]

trong đó, r là điểm số của mô hình Truster, yj là ứng viên ưa thích (dự đoán chính xác) và yk là ứng viên không ưa thích (dự đoán không chính xác trong trường hợp của chúng tôi) từ bộ dữ liệu D. Dựa trên điểm số cho mỗi mẫu, chúng tôi chọn đầu ra có điểm số tốt nhất.

## 4 Thí nghiệm

### 4.1 Bộ dữ liệu

Chúng tôi kiểm tra chiến lược tinh chỉnh ART trên hai nhiệm vụ lý luận đa bước, GSM8K (Cobbe et al., 2021) và StrategyQA (Geva et al., 2021). Bộ dữ liệu GSM8K là một bộ dữ liệu bài toán văn toán học cấp tiểu học với tập huấn luyện 7473 mẫu và tập kiểm tra 1319 mẫu, mỗi mẫu yêu cầu từ hai đến tám bước để giải quyết. Bộ dữ liệu cũng bao gồm các câu hỏi phụ tương ứng với các bước trong một giải pháp chính xác nhất định. StrategyQA, mặt khác, là một benchmark trả lời câu hỏi tập trung vào các câu hỏi miền mở, yêu cầu các bước lý luận để giải quyết. StrategyQA bao gồm 2290 ví dụ huấn luyện, trong đó 20% đầu tiên được sử dụng làm tập kiểm tra và 80% còn lại làm tập huấn luyện, theo công trình trước đó (Magister et al., 2023; Shridhar et al., 2023a). Mỗi câu hỏi được đi kèm với các câu hỏi được phân tách của nó và kiến thức thực tế chính xác cần thiết để trả lời. Ví dụ của mỗi bộ dữ liệu được trình bày trong phụ lục Hình 7.

### 4.2 Thiết lập thí nghiệm

Chúng tôi sử dụng LLaMA 70B (được tiền huấn luyện và chat) (Touvron et al., 2023), ChatGPT (turbo (gpt-3.5-turbo) và turbo-instruct (gpt-3.5-turbo-instruct)) (Brown et al., 2020), và GPT-4 (gpt-4) (OpenAI, 2023) như các mô hình cơ sở ψ do tính phổ biến và hiệu suất tiên tiến của chúng. Tiếp theo, chúng tôi tinh chỉnh các biến thể của mô hình LLaMA (7B, 13B, và 70B) trên bộ dữ liệu GSM8K và 7B và 13B trên bộ dữ liệu StrategyQA. Tất cả các biến thể được tinh chỉnh đã được huấn luyện thêm để có được mô hình Asker nhằm đặt các câu hỏi có liên quan và quyết định khi nào tinh chỉnh. Cuối cùng, chúng tôi tinh chỉnh mô hình LLaMA 13B để có được Truster quyết định giữa đầu ra ban đầu và được tinh chỉnh. Tất cả các mô hình LLaMA được tiền huấn luyện và tinh chỉnh đều được sử dụng với giải mã tham lam trong quá trình kiểm tra (nhiệt độ = 0 và top p = 1). Để thu thập dữ liệu cho huấn luyện, các nhiệt độ khác nhau được sử dụng để thu thập các mẫu đa dạng (nhiệt độ = {0, 0.3, 0.4, 0.7, 0.8}) và k được đặt thành 5 để sinh 5 dự đoán trên tập huấn luyện. Tất cả việc huấn luyện được thực hiện trên một cụm 8 GPU A100 80GB mỗi cái (trừ việc tinh chỉnh LLaMA 70B, yêu cầu 4 cụm 8 A100 mỗi cụm).

### 4.3 Kết quả

**Tự tinh chỉnh là không đủ** Bảng 2 cho thấy khung tinh chỉnh của dự đoán ban đầu, tinh chỉnh và tin tưởng. Nói chung, hiệu suất của LLaMA 70B thấp hơn nhiều so với mô hình ChatGPT turbo cho bộ dữ liệu GSM8K (59 so với 77 cho CoT và 55 so với 78 cho Phân tách Câu hỏi phụ). Hơn nữa, phương pháp Phân tách Câu hỏi phụ (Decomp) hoạt động tốt hơn CoT cho ChatGPT, nhưng điều ngược lại đúng cho LLaMA 70B. Vì dữ liệu huấn luyện và kiến trúc mô hình của ChatGPT không công khai, khó hiểu được khoảng cách hiệu suất. Cuối cùng, tự tinh chỉnh cải thiện hiệu suất trong một số trường hợp, nhưng dẫn đến hiệu suất tệ hơn trong những trường hợp khác (Các ô màu xanh trong Bảng 2 cho thấy so sánh). Tuy nhiên, việc kết hợp tinh chỉnh với mô-đun tin tưởng cải thiện hiệu suất một cách nhất quán so với dự đoán ban đầu trong hầu hết tất cả các trường hợp. Điều này chứng minh tính hữu ích của các thành phần khác nhau trong phương pháp luận ART được đề xuất của chúng tôi. Lưu ý rằng các baseline của các mô-đun Tự tinh chỉnh và tin tưởng của chúng tôi sử dụng cùng lời nhắc như được trình bày trong Shridhar et al. (2023a) để so sánh công bằng.

**Tầm quan trọng của việc Hỏi** Bảng 2 chứng minh hiệu quả của việc huấn luyện một Asker quyết định khi nào tinh chỉnh các đầu ra. So với chiến lược tự tinh chỉnh (Self), một mô hình nhỏ hơn nhiều như LLaMA 7B (Asker 7B) vượt trội hơn các LLM lớn hơn nhiều như tự tinh chỉnh ChatGPT (Self) hơn 2 điểm (80.89 so với 78.62). LLaMA 13B (Asker 13B) cải thiện nó hơn 4 điểm (78.62 → 82.18). Xu hướng tương tự khi so sánh các tinh chỉnh với khả năng tự tinh chỉnh (Self) của LLaMA 70B, trong đó mô hình 7B (Asker 7B) vượt trội hơn khả năng tự tinh chỉnh được tiền huấn luyện của LLaMA 70B khoảng 2 điểm (61.33 so với 59.83) và hơn 1 điểm cho mô hình chat (58.83 so với 60.12). Mô hình 13B (Asker 13B), mặt khác, cải thiện nó hơn 3 điểm cho mô hình LLaMA 70B được tiền huấn luyện (59.83 → 62.74) và phiên bản chat hơn 4 điểm (58.83 → 63.00). Cuối cùng, việc sử dụng mô hình 70B làm Asker (Asker 70B) cải thiện thêm kết quả 4 điểm cho phiên bản được tiền huấn luyện (59.83 → 63.60) và hơn 5 điểm cho phiên bản chat (58.83 → 63.80). Kết quả theo xu hướng tương tự cho các mô hình GPT-4, trong đó cả mô hình 7B (Asker 7B) và 13B (Asker 13B) đều cải thiện kết quả so với sinh ban đầu khoảng 2 điểm (91.88 → 93.72), cao hơn các baseline khác từ Madaan et al. và Shridhar et al. (2023a). Cuối cùng, lưu ý rằng chiến lược ART được đề xuất của chúng tôi cải thiện hiệu suất tổng thể của ChatGPT lên 82.18 sau khi tinh chỉnh với một lần duy nhất (maj1@1), tương tự như điểm số tự nhất quán của 3 mẫu (maj1@3) (Huang et al., 2023).

Kết quả trên StrategyQA theo xu hướng tương tự, trong đó mô hình 7B Asker 7B cải thiện điểm số tinh chỉnh 1 điểm cho LLaMA 70B (75.15 → 76.22) và hơn 3 điểm cho ChatGPT (70.52 → 73.84), như được hiển thị trong Bảng 3. Lưu ý rằng theo Shridhar et al. (2023a), chúng tôi cũng cung cấp một số thông tin thực tế cùng với các câu hỏi trong quá trình tinh chỉnh để mô hình có thể sửa chữa tính không chính xác thực tế của nó. Các lợi ích lớn hơn đối với mô hình Asker 13B, trong đó hiệu suất cải thiện 3 điểm cho LLaMA 70B (75.15 → 78.38) và 5 điểm cho ChatGPT (70.52 → 75.76), chứng minh tầm quan trọng rõ ràng của việc đặt câu hỏi cho việc ra quyết định tinh chỉnh.

**(Đừng) Luôn Tin tưởng Tinh chỉnh** Bảng 2 chứng minh tính hữu ích của mô-đun tin tưởng quyết định liệu việc tinh chỉnh có cải thiện hay làm giảm dự đoán ban đầu. Chúng tôi huấn luyện mô hình Truster học cách xếp hạng dự đoán ban đầu và đầu ra được tinh chỉnh và quyết định cái nào để chọn cho một đầu vào nhất định. Mô hình Truster được huấn luyện của chúng tôi (LLaMA 13B) đạt được độ chính xác của LLaMA 70B được tiền huấn luyện cao tới 64.24, cao hơn 4 điểm so với baseline (60.43). Xu hướng tương tự cho phiên bản chat, trong đó cải thiện gần 5 điểm so với phương pháp baseline sử dụng cùng LLM để ra quyết định (59.55 → 64.40). Kết quả theo xu hướng tương tự cho ChatGPT trong đó cải thiện so với baseline (cùng LLM) khoảng 4 điểm cho mô hình Turbo so với baseline (78.89 → 82.64) và khoảng 7 điểm từ phương pháp tốt nhất trước đó của Self-Refine (Madaan et al.) (75.10 của Self-Refine → 82.64). Các lợi ích cho GPT-4 rất nhỏ, có thể do hiệu suất cao của mô hình GPT-4, nhưng Truster cải thiện hiệu suất lên 94.08 từ điểm số tinh chỉnh tốt nhất trước đó là 93.10.

Đối với StrategyQA, mô-đun tin tưởng không chứng tỏ rất hữu ích với hiệu suất rất tương tự như điểm số tinh chỉnh. Điều này cho thấy rằng rất khó để huấn luyện một Truster trên các bộ dữ liệu dựa trên sự kiện, vì khó xếp hạng hai phần thông tin thực tế mà không biết các sự kiện đúng.

**Chi phí tinh chỉnh LLM so với tinh chỉnh dựa trên ART** Vì các mẫu huấn luyện có sẵn cho bộ dữ liệu GSM8K, có thể tinh chỉnh mô hình LLaMA 70B. Tinh chỉnh LLaMA 70B đạt được độ chính xác 63.2% trên GSM8K (Yuan et al., 2023), tương tự như những gì mô hình 13B được huấn luyện Asker 13B và Truster có thể đạt được với mô hình LLaMA 70B được tiền huấn luyện, trong khi phát sinh chi phí huấn luyện và yêu cầu tính toán thấp hơn nhiều. Bảng 4 cho thấy rằng huấn luyện mô hình 13B làm Truster rẻ hơn 10 lần so với tinh chỉnh mô hình 70B, và ngay cả với hai mô hình được huấn luyện làm Asker và Truster, ART vẫn rẻ hơn 5 lần. Ngoài ra, tinh chỉnh thường làm cho mô hình chuyên môn hóa hẹp cho bộ dữ liệu được huấn luyện với hiệu suất học trong ngữ cảnh tổng quát giảm (Wang et al., 2022b), điều này sẽ không xảy ra với mô hình được tiền huấn luyện quyết định khi nào tinh chỉnh bằng cách sử dụng khung ART được đề xuất của chúng tôi.

## 5 Nghiên cứu loại bỏ

**Tầm quan trọng của việc Đặt Câu hỏi cho Tinh chỉnh** Chúng tôi huấn luyện Asker để chỉ đưa ra quyết định nhị phân "Có" hoặc "Không" để tinh chỉnh, mà không đặt các câu hỏi có liên quan, và thấy rằng tất cả các phiên bản của mô hình LLaMA luôn tin tưởng các dự đoán và không bao giờ quyết định tinh chỉnh chúng. LLM thường rất tệ trong việc đánh giá dự đoán của chính mình và thường ưa thích dự đoán của riêng chúng (Kadavath et al., 2022), và các thí nghiệm của chúng tôi quan sát được hiện tượng tương tự. Tuy nhiên, việc đặt câu hỏi dẫn đến quyết định tinh chỉnh tốt hơn và một ví dụ định tính được trình bày trong Hình 4.

**Tầm quan trọng của Truster cho việc lựa chọn** Chúng tôi so sánh hiệu suất của mô-đun lựa chọn của LLM (Self) so với Truster được huấn luyện của chúng tôi cho bộ dữ liệu GSM8K và quan sát thấy rằng Truster được huấn luyện có thể đánh giá tốt hơn các lỗi được tạo ra trong dự đoán và yêu cầu mô hình quay lại sinh trước đó nhiều hơn (khoảng 50% nhiều hơn so với tự lựa chọn); dẫn đến hiệu suất vượt trội (Hình 2).

**Khi nào tinh chỉnh?** Đánh giá khi nào tinh chỉnh là một thành phần quan trọng của pipeline tinh chỉnh, vì luôn tinh chỉnh dẫn đến kết quả tệ hơn (Huang et al., 2023). Hình 3 hỗ trợ các phát hiện trước đó và cho thấy rằng luôn tinh chỉnh có thể làm tổn hại hiệu suất tổng thể (100% tinh chỉnh) và tệ hơn dự đoán ban đầu (0% tinh chỉnh). Điểm ngọt ngào nằm ở đâu đó ở giữa (khoảng 30-35% tinh chỉnh có vẻ hoạt động cho cả mô hình ChatGPT và LLaMA 70B trên bộ dữ liệu GSM8K).

**Asker có thể được huấn luyện trên đầu ra của chính nó không?** Thay vì huấn luyện một Asker để đặt câu hỏi về đầu ra của LLM được tiền huấn luyện, chúng ta có thể huấn luyện nó trên dự đoán của chính nó không? Rất ngạc nhiên, chúng tôi thấy rằng Asker được huấn luyện trên dữ liệu của chính nó có thể đưa ra quyết định tinh chỉnh tốt hơn so với tự tinh chỉnh của LLM. Tuy nhiên, như mong đợi, huấn luyện trên dữ liệu mô hình được tiền huấn luyện tỏ ra có lợi hơn trong việc quyết định khi nào tinh chỉnh, do phân phối tương tự của các mẫu kiểm tra và huấn luyện, như được hiển thị trong Bảng 5. Tuy nhiên, đối với các mô hình ChatGPT, Asker được huấn luyện trên dữ liệu của chính nó hoạt động tương tự như được huấn luyện trên mô hình LLaMA 70B, vì cả hai phân phối dữ liệu đều khác với phân phối kiểm tra ChatGPT (82.10 so với 82.18 cho mô hình 13B và 80.69 so với 80.89 cho mô hình 7B).

**Phương pháp luận ART có thể được mở rộng cho các mô hình SoTA không?** Chúng tôi tinh chỉnh MetaMath 7B và 13B (Yu et al., 2023) bằng cách sử dụng cùng chiến lược huấn luyện như các mô hình Asker và so sánh kết quả trong Bảng 6. MetaMath, do được huấn luyện trên dữ liệu bổ sung và khả năng lý luận toán học vượt trội, có thể đánh giá dự đoán tốt hơn và dẫn đến hiệu suất tổng thể tốt hơn sau khi tinh chỉnh (62.31 so với 61.33 cho 7B và 64.06 so với 62.74 cho biến thể 13B). Các mô hình MetaMath cũng hoạt động tốt hơn với ít mẫu hơn (khoảng 27% ít mẫu hơn cho 7B và 25% cho 13B), cho thấy rằng lý luận toán học vượt trội có thể giúp đánh giá dự đoán tốt hơn, dẫn đến ít mẫu không chắc chắn hơn để tinh chỉnh. Vì MetaMath được huấn luyện trên hơn 250K mẫu với lấy mẫu từ chối, chúng tôi không thể chạy tất cả các thí nghiệm trên bộ dữ liệu lớn này, và chúng tôi gắn bó với các mô hình LLaMA cho tất cả các thí nghiệm của chúng tôi.

**Toàn bộ pipeline ART trong một lần** Để kiểm tra liệu toàn bộ pipeline ART của việc đặt các câu hỏi có liên quan, sau đó quyết định liệu các câu hỏi có được trả lời hay không, và sau đó tinh chỉnh có thể được học trong một lần thay vì các mô hình riêng lẻ cho từng bước, chúng tôi huấn luyện mô hình LLaMA 13B và 70B trên toàn bộ chuỗi (tất cả trong một lần). Hình 5 cho thấy rằng tất cả trong một lần (màu xanh lá) hoạt động tệ hơn so với tinh chỉnh (màu cam) cho LLM, chứng minh rằng sinh toàn bộ chuỗi là một nhiệm vụ thách thức hơn cho LLM so với các thành phần riêng lẻ.

## 6 Phát hiện chính

Từ các thí nghiệm, chúng tôi quan sát những điều sau:

• **ART cho phép các mô hình nhỏ hơn đưa ra quyết định tinh chỉnh vượt trội hơn tự tinh chỉnh LLM**: Các mô hình nhỏ hơn được huấn luyện để đưa ra quyết định tinh chỉnh có thể vượt trội hơn mô hình lớn hơn nhiều trong phong cách tự tinh chỉnh (Bảng 2).

• **Đặt câu hỏi trước khi tinh chỉnh**: Đặt câu hỏi là một cách hiệu quả để xác minh chất lượng của các sinh phẩm và cho phép các mô hình đưa ra quyết định tinh chỉnh tốt hơn.

• **Quyết định tinh chỉnh của các mô hình nhỏ hơn là giải pháp thay thế hiệu quả về chi phí cho việc tinh chỉnh LLM**: Quyết định tinh chỉnh của các mô hình nhỏ hơn kết hợp với LLM được tiền huấn luyện hoạt động tương tự như mô hình lớn hơn khi được tinh chỉnh. Điều này tiết kiệm rất nhiều tính toán cần thiết để tinh chỉnh mô hình lớn hơn (Bảng 4) và bảo tồn hiệu suất downstream trên các nhiệm vụ khác.

• **Các mô hình chuyên gia có thể đưa ra đánh giá tốt hơn về tinh chỉnh**: Các mô hình lớn hơn (hiệu suất Asker 13B tốt hơn Asker 7B trong tất cả các trường hợp) cho thấy rằng các mô hình tốt hơn có thể đưa ra quyết định có thông tin hơn về khi nào tinh chỉnh. Bảng 6 cho thấy rằng các mô hình được huấn luyện MetaMath vượt trội hơn các mô hình LLaMA có kích thước tương tự.

• **Truster được huấn luyện có thể xếp hạng quyết định tốt hơn**: Mô hình Truster nhỏ hơn được huấn luyện có thể xếp hạng kết quả tốt hơn so với phiên bản tự lựa chọn của LLM, như được hiển thị trong Hình 2.

## 7 Kết luận

Trong công trình này, chúng tôi đề xuất một chiến lược tinh chỉnh gọi là ART: Hỏi, Tinh chỉnh và Tin tưởng, cho phép các mô hình nhỏ hơn đưa ra quyết định tinh chỉnh cho LLM và xác định liệu những tinh chỉnh này có đáng tin cậy hay không. Chúng tôi chứng minh thực nghiệm hiệu quả của phương pháp của chúng tôi trên hai nhiệm vụ lý luận, bài toán văn toán học và trả lời câu hỏi. Kết quả của chúng tôi cho thấy rằng các mô hình nhỏ hơn, thậm chí nhỏ hơn đến 10 lần, có thể vượt trội hơn các mô hình lớn hơn trong việc đưa ra quyết định tinh chỉnh.

## Hạn chế

Trong công trình này, chúng tôi huấn luyện một Asker để đưa ra quyết định tinh chỉnh bằng cách đặt câu hỏi để xác minh các dự đoán. Chúng tôi đã sử dụng dữ liệu huấn luyện có sẵn cho các bộ dữ liệu GSM8K và StrategyQA. Tuy nhiên, đối với nhiều nhiệm vụ, dữ liệu huấn luyện có thể không có sẵn. Trong những trường hợp như vậy, LLM có thể được sử dụng để sinh dữ liệu và trong nhiều trường hợp nó hoạt động tương tự như dữ liệu thực tế (Magister et al., 2023). Tuy nhiên, chúng tôi chưa kiểm tra điều này với ART do tính khả dụng của bộ dữ liệu huấn luyện. Ngoài ra, đối với StrategyQA, chúng tôi đã sử dụng các sự kiện có sẵn để hỗ trợ quyết định của mô hình khi tinh chỉnh các dự đoán. Những sự kiện này có sẵn trong bộ dữ liệu, nhưng trong thế giới thực có thể được trích xuất với sự giúp đỡ của một số công cụ hoặc từ một số cơ sở dữ liệu. Chúng tôi không kiểm tra phương pháp này trong công trình của chúng tôi và để lại cho công trình tương lai.
