# 2305.07230.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/reasoning/2305.07230.pdf
# File size: 1018377 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Cinnamon AI
WHEN GIANT LANGUAGE BRAINS JUST AREN’T
ENOUGH ! D OMAIN PIZZAZZ WITH KNOWLEDGE
SPARKLE DUST
Minh-Tien Nguyen1,2, Duy-Hung Nguyen1, Shahab Sabahi1, Hung Le3, Jeff Yang1, Hajime Hotta1
1Cinnamon AI, 10th floor, Geleximco building, 36 Hoang Cau, Dong Da, Hanoi, Vietnam.
{ryan.nguyen, hector, sshahab, jeff.yang, hajime }@cinnamon.is
2Hung Yen University of Technology and Education, Vietnam.
tiennm@utehy.edu.vn
3Deakin University, Australia.
thai.le@deakin.edu.au
ABSTRACT
Large language models (LLMs) have significantly advanced the field of natural
language processing, with GPT models at the forefront. While their remarkable
performance spans a range of tasks, adapting LLMs for real-world business sce-
narios still poses challenges warranting further investigation. This paper presents
an empirical analysis aimed at bridging the gap in adapting LLMs to practical use
cases. To do that, we select the question answering (QA) task of insurance as a
case study due to its challenge of reasoning. Based on the task we design a new
model relied on LLMs which are empowered by additional knowledge extracted
from insurance policy rulebooks and DBpedia. The additional knowledge helps
LLMs to understand new concepts of insurance for domain adaptation. Prelimi-
nary results on two QA datasets show that knowledge enhancement significantly
improves the reasoning ability of GPT-3.5 (55.80% and 57.83% in terms of ac-
curacy). The analysis also indicates that existing public knowledge bases, e.g.,
DBPedia is beneficial for knowledge enhancement. Our findings reveal that the
inherent complexity of business scenarios often necessitates the incorporation of
domain-specific knowledge and external resources for effective problem-solving.
1 I NTRODUCTION
As the field of artificial intelligence (AI) continues to advance at an unprecedented pace, Large Lan-
guage Models (LLMs)1(Brown et al., 2020; Lieber et al., 2021; Rae et al., 2021; Thoppilan et al.,
2022; Smith et al., 2022) and Generative Pre-trained Transformers (GPT) (Radford et al., 2018;
Brown et al., 2020) have emerged as breakthrough technologies, revolutionizing natural language
processing (NLP) tasks. These powerful models have demonstrated remarkable capabilities in var-
ious use cases, ranging from AI assistants, chatbots, programming, gaming, question answering,
recommendation systems, and retrieval, to knowledge discovery (Sandur et al., 2022). However,
while LLMs (we use the same concept of LLMs and GPT as LLMs) offer immense potential, they
are not standalone solutions (IBM, 2023), and their full effectiveness can be realized through syner-
gies with other AI-powered technologies (Sandur et al., 2022) and tools (Lu et al., 2023).
In this paper, we will delve into the compelling use cases of LLMs (Radford et al., 2018; Brown
et al., 2020), showcasing their capabilities and impact in different domains. We will highlight their
strengths and limitations, and examine how they can be complemented by other cutting-edge AI
technologies to achieve comprehensive and powerful solutions. Through detailed firsthand findings
from our internal research projects, we will shed light on the synergistic potential of integrating
LLMs with complementary AI-powered tools and techniques.
1We use the concept of LLMs as large language models with billion parameters compared to pre-trained
models such as BERT (Devlin et al., 2019) with million parameters.
1arXiv:2305.07230v2  [cs.CL]  8 Jul 2023

--- PAGE 2 ---
Cinnamon AI
We aim to provide a comprehensive overview of the use cases of LLMs, and highlight how these
advanced language models can be harnessed in conjunction with other AI technologies to unlock
their full potential. By leveraging the collective power of multiple AI tools and techniques, we can
pave the way for innovations and solutions that can revolutionize industries and drive unprecedented
advancements in the field of artificial intelligence.
Embark with us on this rigorous intellectual exploration, as we delve into the synergistic potential of
LLMs and investigate the integration of these state-of-the-art technologies with complementary AI-
driven tools to achieve comprehensive and highly effective solutions. Our aim is to unlock the full
potential of artificial intelligence by harnessing the combined strengths of LLMs, GPT, and other
sophisticated AI methodologies within a cohesive framework.
This paper offers an analysis of the current state of research on LLMs and their applications, drawing
upon a range of relevant sources and methodologies.
1.1 T ECHNICAL ADVANCEMENTS
There are a lot of technical advancements relating to LLMs. This section historically reviews con-
cepts and techniques that are directly relevant to LLMs.
Language modeling is a fundamental NLP task that was originally designed for the probability
estimation of an input sequence (Brown et al., 1992; Pauls & Klein, 2011; Jing & Xu, 2019). The
assumption of LM is based on the probability of the co-occurrence of tokens observed on a huge cor-
pus. Given an input sequence S={w1, w2, ..., w n}withntokens, the probability of Sis computed
as a sequence of conditional probabilities as P(S) =Qn
iP(wi|w1, ..., w i−1). The computation
shows the nature of language modeling that can predict the current token wngiven n−1previ-
ous tokens. However, the computation faces the problem that the probability of a long sequence
is asymptotically zero, so the probability of the whole sequence Stends to be zero. To avoid this
problem, language modeling uses the naive Markov assumption (Lauritzen et al., 1990). Instead of
conditionally depending on the whole n−1previous tokens, the naive Markov assumption states
that the probability appearance of the current token wionly conditions by kprevious tokens.
This leverages the definition of n-grams language models (LMs) (Brown et al., 1992). For example,
the uni-gram LM only considers the probability of the current token wi. The bi-grams LM estimates
the probability of the current token wigiven the previous token wi−1. The tri-grams LM estimates
the probability of the current token wigiven two previous tokens wi−1andwi−2. Figure 1 shows
an example of a bi-grams language model. Given an input sequence “LLM is not all you need” ,
the uni-gram LM ( n= 1) estimates the probability of single tokens and the bi-gram LM ( n= 2)
computes the probability of two consecutive tokens. n-grams LMs play an important role for many
NLP problems such as machine translation, error checking and correction, or text generation. For
example, n-grams LMs can be used to give a higher probability of a sequence “LLM is not all you
need” than a sequence “LLM are not all you need” due to the inconsistency of the subject and the
verb of the second sequence. This is because the bi-gram LM returns a higher probability of the
bi-gram “LLM is” than the bi-gram “LLM are” based on the statistics over a huge training corpus.
n=1LLM
is
not
all
you
needunigrams
LLM is not all you needn=2 LLM  is
is not
not all
all you
you needbigrams
Figure 1: The example of a bi-grams LM.
The concept of n-grams LMs leverages a lot of studies to build LMs by using neural networks
(Bengio et al., 2000; Mikolov et al., 2010; Sundermeyer et al., 2012). Recently, the paradigm shift
has also shown that all NLP tasks can be formulated as text generation handled by LLMs (Radford
2

--- PAGE 3 ---
Cinnamon AI
et al., 2018; Brown et al., 2020; Gururangan et al., 2020; Du et al., 2021).2However, the adaptation
of LLMs to practical applications is a non-trivial task due to the challenges stated in Section 3.
Seq2seq and attention The sequence-to-sequence (seq2seq) architecture was originally designed
for neural machine translation (NMT) (Sutskever et al., 2014). The architecture consists of two
components: an encoder and a decoder. The encoder encodes an input sequence into a fixed-length
context vector V∈Rkand the decoder uses the vector Vfor decoding output tokens.
A B C <EOS>V
V X Y ZX Y Z <EOS>
Figure 2: The architecture of seq2seq.
In Figure 2 the encoder receives an input sequence and encodes the sequence to create the vector V.
The vector Vis the input for the decoder that outputs the next tokens based on previous predicted
tokens. The decoding stops when the decoder outputs the end-of-sequence token. Common recur-
rent neural networks (RNNs) Graves (2013) and their variants (LSTM (Hochreiter & Schmidhuber,
1997) and GRUs (Cho et al., 2014)) can be used for the encoder and decoder.
The seq2seq architecture can be applied to several NLP problems such as NMT (Sutskever et al.,
2014), seq2seq learning (Sutskever et al., 2014), text summarization (Chopra et al., 2016), gram-
matical error correction (Raffel et al., 2020), incomplete utterance restoration (Inoue et al., 2022),
etc. However, this architecture suffers in encoding long sequences that cause long-term dependency
and the bottleneck of information when the whole input sequence is represented by only using the
context vector in the final state of the encoder. It motivated the introduction of attention (Bahdanau
et al., 2015) or pointer networks (Vinyals et al., 2015) (we review attention because it is used in
Transformer). The core concept of attention is that during each step of the decoding process, the
decoder selectively concentrates on a specific segment of the input sequence (Bahdanau et al., 2015).
This selective focus is represented by a weight matrix, which encapsulates the attention distribution
across input tokens. The weight matrix is then integrated with the decoder’s hidden state to facilitate
the prediction of an output token for each time step, ensuring that the generated output is contextu-
ally relevant and accurately captures relationships between input tokens. Figure 3 shows an example
of attention of NMT. Given an input sequence “Attention mechanisms are important” , the attention
a 1Attention
a 2mechanisms
a 3are
a 4important
機構 .... ....
Figure 3: The attention mechanism for NMT from English to Japanese.
assigns a weight aiassociated with each input token. If the weight aiis high, it means that the input
token wiis more important than others to predict the corresponding token in the output sequence.
In essence, the attention mechanism allows the model to dynamically allocate its processing focus
2The evaluation and smoothing methods of n-grams LMs are out-of-scope of this report.
3

--- PAGE 4 ---
Cinnamon AI
to the most salient parts of the input sequence at each decoding step. By adaptively directing its fo-
cus, the decoder can more effectively learn and represent the underlying structure and dependencies
within the input sequence, ultimately leading to improved performance and more accurate output
generation. This sophisticated approach to sequence processing ensures that the model is flexible
and efficient, even when dealing with complex or lengthy input sequences.
Transformer The seq2seq and attention provide an effective architecture for text-to-text problems.
However, the seq2seq uses recurrent models that typically factor computation in terms of memory
constraint limitation and computational efficiency, especially for long sequences. It comes from the
nature of sequential encoding and decoding. In addition, the attention mechanism has proven to be
a highly effective technique for encoding long input sequences, thereby alleviating the information
bottleneck issues that plagued earlier models (Parikh et al., 2016). However, the attention mech-
anism is used as an additional component with a recurrent network. These two shortcomings of
recurrent models and attention leverage the introduction of Transformer (Vaswani et al., 2017).
The Transformer relies on the encoder-decoder architecture (Sutskever et al., 2014), which includes
self-attention, point-wise, and fully connected layers (Vaswani et al., 2017). Its main idea is to
transform input data into encoded vectors with the use of self-attention. The transformation of the
transformer computes attention by using query ( Q), key ( K), and value ( V) matrices. We recom-
mend audiences for reading the original paper (Vaswani et al., 2017) for more detailed information.
The success of Transformer (Vaswani et al., 2017) leverages the growth of pre-trained language
models (PLMs) and LLMs. There are three ways for training PLMs and LLMs (Amatriain, 2023).
•Encoder pretraining : This way only uses the encoder of the Transformer for pretraining.
It is also called bi-directional or auto-encoding. Models in this way are usually pre-trained
by using the masked language model that reconstructs masked tokens of input sequences.
In training, attention can access all input tokens. There are many pre-trained models of this
way such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019).
•Decoder pretraining : Different from the encoder pretraining way, the decoder of pretrain-
ing only uses the decoder of the Transformer for training PLMs and LLMs. Models are
trained by predicting the next tokens (single or a set of tokens called n-grams). Due to their
nature, these models (e.g., GPT (Radford et al., 2018)) are appropriate for text generation
with promising results of many NLP tasks (Brown et al., 2020).
•Encoder-decoder pretraining : The third way follows the encoder-decoder of Transformer
(also called as seq2seq) for pretraining. The attention of the encoder can access all input
tokens while that of the decoder can only access the position of tokens before a given token.
There are several PLMs of this way such as T5 (Raffel et al., 2020), UniLM (Bao et al.,
2020), or ERNIE-GEN (Xiao et al., 2020).
Transfer learning, zero-shot learning, and few-shot learning Transfer learning (TL) is a ma-
chine learning (ML) technique that allows ML models for transfering knowledge (Pan & Yang,
2010). Different from traditional ML methods that work well under the assumption: training and
testing data are drawn from the same feature space and distribution, TL provides a way that ML
models can be adapted for new domains by using its existing knowledge. It usually uses fine-tuning
that adapts pretrained ML models with target training samples (Gururangan et al., 2020). In business
scenarios, TL can help to reduce the number of training samples (Nguyen et al., 2020; 2021; 2023).
Zero-shot and few-shot learning are special types of TL. Zero-shot learning is a type of ML that can
recognize objects whose instances may not have been seen during training (Larochelle et al., 2008;
Xian et al., 2017). While zero-shot learning does not require out-domain samples for prediction,
few-shot learning requires a few out-domain samples for training (Wang et al., 2020). Based on
the nature of generative models, LLMs can work well with both zero-shot and few-shot learning
(Radford et al., 2018; Schick & Schtze, 2021; Brown et al., 2020; Liu et al., 2021).
Prompt-based and in-context learning Prompt-based learning is a relatively new paradigm that
allows AI engineers and researchers to better understand the behavior of LLMs (Liu et al., 2023).
Instead of estimating the conditional probability P(y|x), prompt-based learning models the proba-
bility of text directly based on text generation. The input sequence xis converted by using a template
4

--- PAGE 5 ---
Cinnamon AI
Classify the text  into neutral, negative or positive
Text: I think the food was okey .
Sentiment:Instructions
Context
Input data
Output indicator
Figure 4: An example of a prompt.
to create a prompt x′that includes some unfilled slots. This prompt is put into LLMs for the final
prediction of unfilled slots ˆx. The task of creating prompts is called prompt engineering.3Figure 4
shows an example of a prompt. The prompt includes three parts: instructions, input data, and output
indicator. The context is not required in this classification task.
In-context learning refers to a learning paradigm in which a ML model acquires knowledge by
processing and analyzing data within its relevant context, allowing it to draw meaningful insights and
adapt its understanding based on surrounding information (Liu et al., 2021; Zhang et al., 2023). This
approach contrasts with traditional supervised learning, where ML models are trained on isolated
instances or labeled data. In-context learning enables ML models to better generalize and adapt to
new situations, improving their performance on complex tasks, and facilitating more accurate and
nuanced responses in real-world applications.
LLMs work well with zero-shot and few-shot learning by using prompt-based and in-context learn-
ing (Radford et al., 2018; Schick & Schtze, 2021; Brown et al., 2020; Liu et al., 2021). There are
several prompt engineering techniques such as chain-of-thought (COT) (Wei et al., 2022), zero-shot
COT (Kojima et al., 2022), self-consistency (Wang et al., 2022), generated knowledge prompting
(Liu et al., 2022), and automatic prompt engineer (Zhou et al., 2022) that can be used to distill and
adapt the knowledge of LLMs for practical problems.
1.2 F ACTORS BEHIND THE BREAKTHROUGH OF LLM S
Seq2seq, attention, and Transformer The success of LLMs is technically derived from the
emergence of seq2seq (Sutskever et al., 2014), attention (Bahdanau et al., 2015), and Transformer
(Vaswani et al., 2017). The seq2seq provides a method that can naturally represent text-to-text prob-
lems by using the encoder-decoder architecture. The attention mechanism appears to support the
seq2seq for better using the input via a contextual attention vector that is used for decoding. Trans-
former takes advantage of seq2seq and attention by designing a new encoder-decoder architecture
with new components: self-attention, multi-head attention, and residual connections. The new ar-
chitecture leverages the significant growth of PLMs such as BERT (Devlin et al., 2019) and LLMs
such as GPT-3 (Brown et al., 2020).
Massive data The second factor that leverages the significant growth of LLMs is the availability
of massive unlabeled and annotated data of down-stream tasks. For example, GPT-3 (175B parame-
ters) (Brown et al., 2020) was pretrained with the CommonCrawl corpus4(Raffel et al., 2020). After
pretraining, it works well on many NLP tasks (Brown et al., 2020). While common massive unla-
beled corpora are necessary for training LLMs by using self-training, annotated data provides good
information sources for task-adaptive pre-training of downstream tasks (Gururangan et al., 2020).
Self-training Besides the availability of massive corpora, self-training (self-supervised) is a criti-
cal concept for training LLMs. Different from the traditional ML mechanism that requires annotated
3https://github.com/dair-ai/Prompt-Engineering-Guide
4https://commoncrawl.org/the-data/
5

--- PAGE 6 ---
Cinnamon AI
Human Feedback
Big DataLLMOutput 1
Output 2InputOutput 1 is better
User
Figure 5: Training language models with human feedback. In this framework, the models are trained
using human feedback in the form of preference.
samples created by humans for training, self-training augments unlabelled data to create training
samples. The main idea of self-training is to automatically corrupt input sequences (i.e., masking
some tokens of an input) and the learning process forces the model to reconstruct the original se-
quences based on the corrupted sequences. Self-training allows us for training LLMs without using
annotated data. There are several self-training methods as follows (Amatriain, 2023).
•Language model (LM) is the traditional self-training technique that relies on the concept
of language modeling. Given an input sequence, LM predicts next tokens given the previ-
ous tokens. The prediction includes unidirectional and bidirectional LMs. The unidirec-
tional LMs only predict next tokens of the given input tokens in a uni-direction (forward or
backward) while bidirectional LMs combine both forward or backward LMs for prediction.
•Masked language model (MLM) restores masked tokens (corrupted tokens) by using its
surrounding context. Given an input sequence, the MLM first randomly masks some to-
kens and the training process forces the model to restore the masked tokens by comparing
predicted tokens and gold tokens of the input sequence. For example, BERT (Devlin et al.,
2019) randomly masks 15% of all tokens for training the MLM.
•Permuted language model (PLM) is the same as LM but uses a random permutation of
input sequences.
•Denoising autoencoder (DAE) operates as the autoencoder mechanism. It takes a part
of corrupted input sequences (randomly masking tokens or suffering input sequences in
random order) and recovers the original unsorted input.
•Contrastive learning (CTL) is to learn a scoring function for text pairs of similar and
random sampled sequences. It includes four different types: deep infomax, replaced token
detection, next sentence prediction, and sentence order prediction.
Fine-tuning and human-in-the-loop Despite the significant advancements in early LLMs such
as GPT-2 (Radford et al., 2018), the self-training process still faces certain challenges. The issue
of self-training is that LLMs only have access to unstructured data, and their objective is simply
predicting the next tokens, given certain text. This approach has a problem of no intention, where
the model generates content to make it looks like the training data, without understanding the context
or the purpose behind it, which may lead to unaligned results with users’ needs.
Incorporating human feedback can enhance the quality of the generated content and ensure that it
aligns with the desired outcome. The feedback provided by humans can be used to train the model
more effectively and improve its overall performance. One commonly used form of feedback is
user preference (Ziegler et al., 2019; Nguyen et al., 2022). It is cheaper to evaluate the model’s
output using preference than labelling or a ranking system. In particular, given an input, LLMs will
generate two output candidates and present them to a human evaluator, who will decide which one is
better. The LLMs will utilize this preference feedback to refine its outputs based on the evaluator’s
preferences. Please refer to Figure 5 for more detailed information.
Reinforcement learning with human feedback Reinforcement learning with human feedback
(RLHF) is an innovative approach that integrates human guidance into the reinforcement learning
6

--- PAGE 7 ---
Cinnamon AI
Figure 6: The timeline of LLMs with the number of parameters extracted from Amatriain (2023).
process to improve the performance of an agent in various tasks. By leveraging human expertise,
the RLHF method can help to overcome limitations in designing reward functions, exploring new
strategies, and facilitating faster convergence to optimal solutions (Christiano et al., 2017)
In a typical RLHF setting, the learning agent interacts with an environment and receives evaluative
feedback from a human expert. This feedback is utilized to create a reward function that guides the
agent’s learning process. By incorporating human knowledge into the reward function, RLHF allows
the agent to better understand the underlying objectives and constraints, leading to more efficient and
effective learning. By incorporating RLHF, LLMs can learn to generate more accurate and relevant
content to maximize the feedback rewards. As testified in an Open AI’s paper (Ouyang et al., 2022),
the incorporation of human feedback in the form of preference (i.e., humans choose which output is
better) increases the performance of LLMs consistently by about 50% across different model sizes.
The power of computing resources The power of computing resources (GPUs and TPUs) signif-
icantly contributes to the breakthrough of LLMs. GPUs and TPUs allow us to speed up the training
of LLMs in a practical manner (Hoffmann et al., 2022), which significantly increases the size of
pre-trained models from million to billion parameters.
1.3 LLM S’TIMELINE
Figure 6 shows LLMs’ timeline and the number of parameters extracted from Amatriain (2023). It
is clear to observe the evolution of LLMs from 2018 to 2023. Started by small PLMs such as GPT
and BERT in 2018, training LLMs has received significant attention of research and investment
from both industry and academia (Bowman, 2023). For more detailed information of LLMs and its
catalog, we encourage audiences to read the paper of Amatriain (2023).
The timeline of LLMs pushes the paradigm shift of NLP in which all NLP tasks can be represented
as text generation (Radford et al., 2018; Brown et al., 2020; Gururangan et al., 2020; Du et al.,
2021). For example, GPT-2 (Radford et al., 2018) showed that a generative pretraining LM
improves the performance of 9 out of 12 NLP tasks, i.e., 8.9% on commonsense reasoning (Stories
Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).
GPT-3 with few-shot learning (Brown et al., 2020) is even better on the SuperGLUE benchmark
(Wang et al., 2019) that includes 8 challenging natural language understanding tasks: question
answering, textual entailment, coreference resolution, word sense disambiguation, and causal
reasoning. Du et al. (2021) introduced a general pretraining framework that formulates NLP tasks
7

--- PAGE 8 ---
Cinnamon AI
as text generation. The framework also works well on the SuperGLUE benchmark. LLMs have
continuously improved in terms of quantity and quality. For example, ChatGPT has more than 100
billion parameters and GPT-4 (OpenAI, 2023) with even a bigger model trained by multimodal data
can pass bar exams in the top 10%.
The purpose of this paper is to empirically analyze the behavior of LLMs in actual business cases.
The main research question is “ Do LLMs need knowledge enhancement of specific domains for
working well in business scenarios ?”. To answer the question, we first show the applications of
LLMs in Section 2 and then discuss the challenges of LLMs in Section 3. Based on the challenges,
we introduce our proposal that enhances the domain knowledge of LLMs in Section 4 and describe
experimental settings in Section 5. We finally report preliminary results of the question answering
task in the insurance domain in Section 6 and draw conclusions and future directions in Section 7.
2 LLM S’ APPLICATIONS
We try our best to collect up-to-date applications built by using LLMs.5While some actual ap-
plications are potential such as data annotation and generation, programming, and bug fixing, or
tale-to-your-data, further research is still needed for boosting LLMs to business scenarios.
AI assistant and Chatbot have become one of the most popular consumer use cases for AI tech-
nologies. From scheduling appointments and making notes to responding to emails and helping
with various tasks, AI assistants have become indispensable in our daily lives. Big-tech companies
provide their own AI assistant and chatbot solutions. For example, Quora has even released their
own app Poe, allowing users to chat with multiple LLMs, showcasing the increasing prevalence of
AI assistants in the market (Li et al., 2023). With the support of LLMs, AI assistants and chatbots
can better understand user intents, correctly predict entities, remember the longer context, etc. All
these help to make AI assistants and chatbots to be smarter and faster, and behave like humans.
Error correction, programming, and gaming No doubt that ChatGPT is a cutting-edge artificial
intelligence language model that has an impressive ability to answer follow-up questions. However,
ChatGPT and LLMs have unique strengths that make them as valuable tools for tasks that require
more nuanced language understanding beyond simple error correction. For example, ChatGPT has
adapted for several tasks such as grammatical error correction (Wu et al., 2023), generating and
debugging codes6of different programming languages, debugging written codes, completing missed
parts of codes, rewriting codes in other languages (Kashefi & Mukerji, 2023), solving mathematical
problems, or creating realistic conversations with game characters.7
Data annotation and generation Another potential use case for LLMs and ChatGPT is text anno-
tation commonly used in NLP applications. The study of Gilardi et al. (2023) found that ChatGPT
outperforms crowd-workers and trained annotators for several annotation tasks on 2,382 tweets.
Additionally, ChatGPT’s intercoder agreement exceeds both crowd-workers and trained annotators
for all tasks, and it is twenty times cheaper than MTurk. However, ChatGPT’s performance across
multiple languages and types of text, implementing few-shot learning, semi-automated data labeling
systems, and improving zero-shot reasoning is still an open question.
Learning and education For learning and education, LLMs and ChatGPT can support learners
and teachers in many aspects. For example, LLMs can help to summarizing books or reports, au-
tomatically generating quizzes to test students’ comprehension, grading essays and providing feed-
back, explaining math solutions (Agrawal et al., 2022). This can help to save learners and teachers
time for learning and teaching (Menear, 2023). In addition to learning tasks, LLMs can also serve as
debate partners. It can present arguments from different perspectives on a given topic, allowing stu-
dents to practice critical thinking and persuasive skills by engaging in debates with ChatGPT. This
can be a fun and interactive way for students to develop their argumentation and reasoning abilities.
5The content was partially derived from https://huyenchip.com/2023/04/11/llm-engineering.html
6https://www.youtube.com/watch?v=8y7GRYaYYQg
7https://www.youtube.com/watch?v=-R4PWIkgOSk
8

--- PAGE 9 ---
Cinnamon AI
Talk-to-your-data Talk-to-your-data is currently one of the most popular enterprise applications.
Startups are continuously building tools to allow enterprise users to query their internal data and
policies in natural languages or in a Q&A format. Many of these startups focus on specific verticals
such as legal contracts, resumes, financial data, or customer support (Saeed et al., 2023).
Search and recommendation LLMs have revolutionized the traditional keyword-based search
approach. We have seen recently innovative approaches like Chat-Rec (Gao et al., 2023), which
combines LLMs with recommender systems to enhance interactivity and explainability. With some
modification of GPT models, it is possible to build an app to be used as cross-encoders or bi-encoders
for semantic search (Muennighoff, 2022).
Sales LLMs have numerous applications in the field of business management. LLMs can help
with data querying, visualization (charts and graphs), or storytelling, where users can ask a question
in a natural manner and receive a narrative around the insight associated with the users’ questions.
Another use case is to see LLMs for sales activities such as writing sales emails. In this case, LLMs
can be considered as a powerful tool for reducing the effort of salesman.
SEO AI has revolutionized the field of search engine optimization (SEO) in several ways, as it
enables website owners to focus on creating high-quality content and better services. By providing
more efficient data-driven insights and automating workflow (Cao et al., 2023), LLM-powered apps
can analyze search queries and suggest relevant keywords to users, making it easier to identify
patterns and trends. In addition, LMMs can help to generate data as much as SEO companies want
to increase the rank of webpages processed by ranking algorithms.
3 T HECHALLENGES OF LLM S
Productizing LLMs is no easy feat. Various challenges must be overcome for LLMs to achieve com-
mercial viability, including tackling engineering and technical difficulties, addressing limitations in
capabilities, and preventing ethical or social problems that may arise from their usage. It makes
challenges to integrate LLMs into existing products or create new ones.
3.1 E NGINEERING OBSTACLES
One of the biggest development roadblocks is scalability (Tamkin et al., 2021). The training of
LLMs is a computationally intensive task, requiring massive amounts of data and heavy computing
power. Hence, scaling an LLM to handle business-specific tasks with large data can be daunting.
Most of the time, LLMs are provided by third-party services, which cost fees and complicate large-
scale product customization.
An additional hurdle is ensuring data confidentiality and protection. LLMs are often deployed on
the cloud hosted by third-party companies. So there is no guarantee that the data sent to LLMs is un-
touched. It is dangerous to applications using sensitive data, such as personal conversations, medical
records, and financial transactions. There are solutions to mitigate the issue, but the performance is
unsatisfactory and comes with additional development costs (Yu et al., 2022; Li et al., 2022).
There are also challenges to integrating LLMs into existing systems. LLMs are often used with other
technologies, such as chatbots or virtual assistants. Integrating these different technologies can be
complex, requiring expertise in multiple domains such as natural language processing, machine
learning, and software engineering. Therefore, it is a non-trivial task for ordinary people to use
LLMs to address their business tasks.
3.2 C APABILITY LIMITATIONS
Although LLMs are powerful AI models, they are still far from the human level of intelligence. As
illustrated in Figure 7, when answering the question “Which weighs more, two kilograms of bricks
or a kilogram of feathers?” , GPT-3 (version Mar-2023) insists that both weigh equally because it
is trained with the common questions comparing one-kilogram bricks versus one-kilogram feathers.
Worse still, the model would try to convince it was right by providing a complicated, yet unrelated
9

--- PAGE 10 ---
Cinnamon AI
1KG 2KG
 LLMI am sure that
 they are equal!
Figure 7: A simple question that can cheat LLMs: “Which weighs more, two kilograms of bricks or
a kilogram of feathers?”
explanation: “Two kilograms of bricks and one kilogram of feathers weigh the same amount, which
is two kilograms or 2000 grams. ... So, even though two kilograms of bricks consist of fewer
individual pieces than one kilogram of feathers, both quantities have the same mass and, therefore,
the same weight. ” .
The above example illustrates LLMs’ tendency to produce hallucinations (Azamfirei et al., 2023).
These hallucinations occur when LLMs generate text that is not grounded in reality which is a chal-
lenge to distinguish the real and the imagination. In real-world settings such as insurance businesses
for example; a chatbot trained on policy documents may answer users’ questions by providing fake
content about policy packages or claim conditions that appear genuine, which is confusing, and
potentially leads to harmful consequences.
Another significant technical limitation is the reasoning capability of LLMs (Bang et al., 2023).
While they excel at pattern recognition and generating text based on statistical patterns, they may
lack the ability to reason or understand concepts beyond the surface level. This limits their ability to
understand complex contexts or make logical inferences, which can be crucial for many applications.
For instance, a model that generates legal documents must be able to reason about legal precedent
and apply it to specific cases.
LLMs exhibit extremely low interpretability, posing a significant challenge for building trust in their
applications. LLMs are like black boxes, making it difficult to understand how they predict or make
decisions. This lack of transparency can be a problem in fields such as healthcare, where decisions
made by the model can have a significant impact on people’s life.
Finally, LLMs struggle with handling and generating structured data. While they can generate text,
they struggle to produce structured data like tables or graphs, which can be important for many
applications. This limitation hinders their usefulness in tasks involving the processing of structured
data such as tables, diagrams, equations, and figures.
3.3 E THICAL AND SOCIAL RISKS
There is a rising ethical concern about the potential of LLMs to perpetuate harmful biases or even
create threats to humans. These biases can be present in the data used to train the models, which
can lead to unfair outcomes in decision-making processes. For example, if a language model of a
chatbot is trained on data that is biased against certain racial or gender groups, it can generate biased
dialogues that offend users, which dramatically deteriorates the quality of customer services.
Moreover, LLMs can also pose risks to individual privacy. This is a serious issue in business appli-
cations. For example, if an LLM is trained on personal client data, such as emails or social media
messages, it can learn personal information about individuals and use it in unintended ways. Hack-
10

--- PAGE 11 ---
Cinnamon AI
ers can utilize this loophole to steal secret information from clients. This raises concerns about data
privacy and the potential for misuse of personal information. Keen readers are referred to Isaac et al.
(2021) for details of these ethical and social risks.
4 LLM S+: KNOWLEDGE ENHANCEMENT FOR DOMAIN ADAPTATION
This section first describes a business scenario that requires the reasoning ability of LLMs and shows
the proposal for domain-specific knowledge enhancement that adapts LLMs to actual cases.
4.1 A BUSINESS SCENARIO : QUESTION ANSWERING FOR INSURANCE
As mentioned in Section 2 LLMs can be adapted for many applications. To provide a better un-
derstanding of LLMs in actual business cases, we adapt LLMs for insurance because of two main
reasons. First, insurance is the traditional business with a big market8(Benfield, 2015) where many
insurance companies provide their packets for clients. If AI solutions can be successfully applied to
(partly) automate insurance processes, it can boost the growth of the market and significantly reduce
the cost of labor-expensive tasks. Second, insurance activities usually require a deep understanding
of complex policies, regulations, and legal jargon for insurance processes. These activities are much
more challenging than standalone tasks such as information extraction or information retrieval which
AI models can achieve promising results. We leave the analysis of LLMs for other domains such
as IncidentAI or LegalAI as a future task, after having preliminary results of LLMs for insurance.
While there are many domain-oriented tasks of insurance, the question-answering (QA) task was
selected as a case study for the analysis due to two reasons. First, QA is a common task of insur-
ance companies who have to handle many requests from customers per day for asking information
of relevant procedures, e.g., claim making or the advice of insurance products, etc (Nuruzzaman &
Hussain, 2020). A QA system comes in as a powerful tool that can quickly and accurately answer
complicated insurance-related questions. Second, QA is a challenging NLP task that requires a deep
understanding of the meaning of questions and context, and the implicit mutual relationship between
them for giving correct answers (Rajpurkar et al., 2016; Baradaran et al., 2022).
In practice, end users usually ask questions about their products of insurance by making a phone call
or by using a chat interface. For example, the question “Is there any refund for cancellation?” is
handled by insurance staff who will first confirm the personal information of users. After confirma-
tion, the staff will retrieve relevant information in policy rulebooks or in-house rules to answer the
question. The answer to the question is “Yes” with references extracted from the rulebooks. Please
refer to Table 2 for more examples. However, the large amount of human resources for this QA
process is the pain-point of insurance companies due to two reasons. First, they have to maintain a
lot of people for this QA task. Second, in many cases customers have to wait a quite long time for an
answer and human errors can happen in the operation process. Insurance companies have spent a lot
of effort in building customer care centers, so they want to improve customers’ experience by mak-
ing collaboration between humans and AI. The use of AI in insurance-related tasks is so beneficial
in terms of response time, correctness, and cost saving.
4.2 T HE PROPOSED MODEL
Problem statement As mentioned, LLMs have achieved promising results for many NLP tasks
(Radford et al., 2018; Schick & Schtze, 2021; Brown et al., 2020; Liu et al., 2021). However, we
argue that LLMs need knowledge enhancement for domain adaptation by using an existing enterprise
knowledge base. It comes from the challenges of LLMs mentioned in Section 3. First, LLMs are
trained by using a massive amount of data with billions of parameters. It allows LLMs can work
well in common cases (Section 2). However, it may lack domain knowledge of specific downstream
tasks. For example, if a question includes a specific information, e.g., the maximum amount of
advanced medical care benefits, LLMs alone can only response general information (Section 6.3).
As the result, LLMs need to connect and process policy rule books, in-house rules of insurance,
or external knowledge, e.g., DBpedia when answering questions about claims from users. Second,
the reasoning capability of LLMs is still an open question. Though the reasoning capability can be
significantly improved by using prompt-based learning techniques (Wei et al., 2022; Kojima et al.,
8https://www.oecd.org/pensions/globalinsurancemarkettrends.htm
11

--- PAGE 12 ---
Cinnamon AI
2022; Wang et al., 2022; Liu et al., 2022; Zhou et al., 2022), the lack of domain knowledge still
makes a challenge for LLMs in making correct prediction. For example, given the question of “How
much is the radiation treatment benefit payment?” , GPT can only answer with general information
while combining GPT with insurance policy rulebooks, it can reason the radiation treatment is 10
times of the daily amount of the hospitalization benefit (see Table 2).
LLMs
Answer UsersQuestion LLMs
Answer UsersQuestion
Knowledge base
Domain experts
a. LLMs without domain knowledge b. LLMs with domain knowledge
Figure 8: The proposal of LLMs using a domain knowledge base.
The model We address the lack of domain-specific knowledge of LLMs by introducing a new
architecture in Figure 8. The left hand side is the general process of using LLMs for down-stream
tasks by using zero-shot or few-shot learning with prompting techniques. Users ask questions by
providing prompts and LLMs response appropriate answers. The right hand side, in contrast, shows
our proposal that adapts LLMs for domain adaptation. The knowledge of LLMs is enhanced by
using a domain knowledge base created by domain experts. For example, if users ask a question
about the obesity of insurance contracts, LLMs have a little chance to correctly answer this question
due to the lack of domain knowledge. By leveraging LLMs with domain-specific knowledge, the
system can understand and respond to questions in a conversational manner, making it easy for both
insurance agents and customers for interactions. While any LLMs can be used in the proposed
model, we take GPT-3.5-Turbo9as the main backbone due to its quality for many NLP tasks.10
4.3 D OMAIN KNOWLEDGE IMPLEMENTATION
As mentioned, LLMs are enhanced by using domain knowledge. In this study, we used policy
rulebooks and DBpedia as a type of external domain knowledge. It allows us to investigate the
capability of LLMs to work with different data types.
Policy rulebooks play an important role in insurance. It covers all definitions, terms, policies,
regulations, etc that are necessary for insurance-related tasks. Based on government laws, insurance
companies define and provide their policy rulebooks for their in-house operation. A rulebook is a
document that includes the purpose of the insurance, definitions for terms used in the document,
general provision, payment of benefit, refund policies, and special conditions of the policies.
For data preparation, seven rulebooks published by an insurance company were first collected. Due
to the limitation of LLMs for dealing with structured data, extracting text from PDF files is a crucial
step for data preparation. To ensure the text is machine-readable and usable in the downstream QA
task, preprocessing PDF files is necessary. The preprocessing step involves removing any non-text
elements such as images using PDFMiner,11and parsing tables to the machine-readable format using
Camelot12(results of table processing is mentioned in Section 6). For rulebooks, we only consider
the raw text as LLM’s input rather than the structure of the rulebooks. Once the PDFs have been
preprocessed and converted to the text format, they can be used as input data for inference with
LLMs, i.e., GPT-3.5-Turbo. In practice, the rulebooks were segmented into manageable chunks and
9We utilized both GPT-3.5-Turbo as well as text-davinci-003 and got the same performance, while the cost
of the former one was as 10 times as lower than the later one. We are testing with GPT-4 for the analysis and
results will be available soon.
10https://platform.openai.com/docs/models/overview
11https://github.com/pdfminer
12https://github.com/camelot-dev/camelot
12

--- PAGE 13 ---
Cinnamon AI
each chunk was converted into a corresponding vector for information retrieval. By leveraging the
textual content from policy rulebooks, LLMs can understand new concepts of insurance and then
can appropriately answer questions from end users.
DBpedia as external knowledge Policy rulebooks provide good context for answering input ques-
tions, yet in practice, the questions may include concepts or entities that are not mentioned in the
rulebooks. For example, let’s consider the question, “He was hospitalized for a week due to dia-
betes. How much is his allowance?” . If GPT relies solely on the rulebooks, it may not understand
that diabetes is a lifestyle disease. In this case, external knowledge, e.g., DBPedia13comes as an
additional channel for enhancing information of the context extracted from rulebooks. For instance,
the model can retrieve information such as, “Lifestyle diseases can be defined as diseases linked with
one’s lifestyle... which can lead to heart disease, stroke, obesity, type II diabetes, and lung cancer...
heart disease, hypertension, metabolic syndrome... ” . An example of retrieving additional informa-
tion from DBpedia is shown in Figure 9. By incorporating DBpedia’s knowledge, the context of an
input question is enhanced, resulting in more confident and accurate responses.
abstractLifestyle
diseaseLifestyle diseases
 can be defined as
diseases linked with
one's lifestyle....
CancerwikiPageWikiLink
Metabolic
_syndromewikiPageWikiLink Type_2
_diabeteswikiPageWikiLinkDiseaseshypernym
Figure 9: An example of the DBpedia KG.
DBpedia follows the Resource Description Framework (RDF) format. RDF is a standard for express-
ing structured information on the web using subject-predicate-object triples, which is also known
as the representation of a knowledge graph. In DBpedia, entities are represented as resources (sub-
jects), their attributes are represented as predicates, and the attribute values are represented as ob-
jects. This RDF structure is flexible for querying and linking data. DBpedia offers localized versions
in 125 languages, in which the English version contains 4.58 million entities, with 4.22 million en-
tities classified in a consistent ontology. With the huge amount of facts and its structure, we can
efficiently query, explore, and utilize structured data for various domains.
4.4 K NOWLEDGE ENHANCEMENT FOR QA
In NLP, incorporating external knowledge sources into LLMs has shown efficiency to improve the
performance of QA models (Peng et al., 2023). In our experiments, we employed policy rulebooks
and DBpedia to enhance knowledge of LLMs and define a QA pipeline in Figure 10. Instead of
directly using the knowledge of LLMs, the pipeline first recieves a question by using an adapter that
retrieves relevant context from the knowledge base created by the rulebooks. The retrieved context is
combined with the input question to form a prompt processed by LLMs to generate the final answer.
Information retrieval (IR) IR is an important module of the adapter that retrieves relevant context
for an input question. For doing IR, raw chunks from rulebooks were first stored in a database in
advanced. Given an input question, the adapter first converts it into a vector. For rulebooks, the
13https://dbpedia.org/
13

--- PAGE 14 ---
Cinnamon AI
Policy rulebook
Vectorize
DBpedia KGReturn contextKnowledge
baseAnswerLLMs
Retrieve contextContext  + Question
AnswerAdapterQuestion
Users
Figure 10: The proposed QA pipeline that utilizes domain-specific knowledge.
IR computes the similarity of the question vector to vectors of all chunks. For DBpedia KG, the
IR process begins by extracting entities from input questions using a method that combines part-of-
speech tagging, named entity recognition, and a heuristic-based approach. These extracted entities
act as candidate mentions for linking to the DBpedia KG. Then a text search engine14is used to map
these entities to corresponding entities within the DBpedia KG (an example of a part of DBpedia
KG is shown in Figure 9). This mapping establishes connections between user queries and relevant
entities within the KG. To retrieve specific information of these entities, the SPARQL (SPARQL
Protocol and RDF Query Language) was employed. By formulating SPARQL queries, the model can
interact with the DBpedia KG and retrieve desired information via the DBpedia SPARQL endpoint.
The retrieved chunks from the rulebooks and specific information of entities from DBpedia (Figure
9) were used as the context of an input question.
Prompt creation Once the context has been retrieved, two types of prompts were designed for
utilizing domain knowledge from the rulebooks and DBpedia. This is because the retrieved context
from the rulebooks and DBpedia is different in terms of representation. The context directly re-
trieved from the rulebooks contains consecutive sequences while the context extracted from triples
includes non-consecutive sequences (see Algorithm 2 and 3). For example, the first prompt asks
GPT-3.5 to answer a question of “When will I be notified of payment reason change?” given the
raw context. The second prompt asks the question “He was hospitalized for a week due to diabetes.
How much is his benefit?” based on the context of triples. More precisely, a prompt includes four
sections: (i) the indicator ( “Answer the question in a short and concise way” ), (ii) the question, (iii)
the context, and (iv) external knowledge. Detailed information of the two defined prompts can be
found in Algorithm 2 and 3.
Once the prompts have been defined, it was applied to all the questions. All the prompts were
iteratively fed into GPT by using its APIs to obtain corresponding answers.
5 E XPERIMENTAL SETTINGS
Experimental scenarios We confirm the proposed model in two scenarios. The first is to directly
use GPT-3.5-Turbo for answering questions. The second is to use GPT-3.5-Turbo with additional
domain knowledge (context-aware) for QA. The total cost of experiments is around 250 USD.
•Context-agnostic question answering Given a user’s question, we feed the question di-
rectly to GPT and use GPT’s response as an answer. In this setting, GPT uses its own
knowledge to answer input questions. The prompt of this setting is shown in Algorithm
1 that contains two parts: (i) the indicator ( “Answer the question in a short and concise
way” ) and (ii) the question.
14https://www.elastic.co/
14

--- PAGE 15 ---
Cinnamon AI
Table 1: Samples of human questions. Top five questions with its outputs are showed in Table 2.
Question
What kind of surgery does a hysterectomy involve?
Is there a refund for cancellation?
What is the maximum amount of advanced medical care benefits?
When will I be notified of a payment reason change?
How much is the radiation treatment benefit payment?
What is an endowment insurance policy?
When will I be notified of a payment reason change?
I would like to know the payment amount of sickness hospitalization benefits
How much is the advanced medical care lump-sum benefit?
What kind of surgery does an oophorectomy involve?
How much is the radiation treatment benefit payment?
What kind of surgery does a hysterectomy involve?
Is hospitalization for childbirth covered by hospitalization benefits?
He was hospitalized for a week due to diabetes. How much is his allowance?
•Context-aware question answering Given a user’s question, we feed the question and
relevant external context to GPT and use GPT’s response as an answer. This setting al-
lows GPT to learn from context for knowledge enhancement. The context is information
extracted from rulebooks or the combined information from rulebooks and DBPedia. The
prompt of this setting is mentioned in Algorithm 2 and 3.
Data preparation Two datasets were used for performance evaluation. The first is an expert-
created dataset that consists of 104 gold question-answer pairs from an insurance company (Table 1
shows some samples). Questions refer to different aspects of insurance policies and benefits. Some
of them relate to refunds and surrender values in cases of cancellation, while others inquire about
death payment benefits or hospitalization with specific medical conditions. There are also questions
about the types of surgeries covered by the insurance policy and the amount of benefits payable
for these procedures. Some questions mention the process of changing beneficiaries, payment of
premiums, and the maximum payout limits for certain benefits. The answers to these questions
depend on the specific insurance policy and the terms and conditions outlined in the contract.
The second corpus is a synthesized dataset. This is because the expert-created dataset focuses on
specific questions and answers related to insurance policies and benefits which are usually avail-
able in the rulebooks. However, in practical scenarios, questions may not always explicitly mention
concepts or entities that are present in the rulebooks. To address this aspect, a synthesized dataset
was created by combining commonsense knowledge from DBPedia and domain-specific knowledge
from the rulebooks. This synthesized dataset captures a broader range of questions by generating
queries based on smaller segments of the rulebooks and retrieving relevant information from DB-
Pedia. The generated questions are then adjusted to incorporate both the rulebook information and
the retrieved knowledge from DBPedia. More precisely, the synthesized dataset was created in the
following steps. First, the rulebooks were divided into smaller segments to ensure they adhere to
the token limitations of GPT. After chunking, GPT was employed to generate questions that pertain
to each specific chunk. Next, concepts (entities) in a generated question were extracted and used
as queries for retrieving relevant information from DBPedia. After that, the generated question was
modified by asking GPT to adjust the question by considering both the chunk from the rulebooks
and relevant information from DBPedia. Finally, the GPT was requested to generate the corre-
sponding answer for the synthesized question. The creation process involves humans for manual
checking to ensure the high quality of generated questions. The resulting synthesized dataset con-
tains 87 question-answer pairs after manual checking and removal of duplicates, providing a more
comprehensive and diverse set of questions for evaluation purposes.
Evaluation metrics The evaluation was done by using humans instead of using both automatic
and human evaluation. This is because the quality estimation of answers from the system requires
a deep understanding of policy rulebooks, terms, and conditions relating to input questions rather
15

--- PAGE 16 ---
Cinnamon AI
than simply using token overlapping metrics such as ROUGE-scores (Lin, 2004) or BLEU-scores
(Papineni et al., 2002). Human evaluation was done in two steps. In the first step, two annotators
who had experience with answering insurance questions were involved the evaluation process. Each
annotator is given the guideline of how to judge whether an answer is correct or not. The correctness
of an answer is defined by two indicators: answerability and completeness. The answerability means
the output from the system can answer the input question. The completeness means the content of the
outputted answer covers the meaning of the gold answer. The two annotators made their judgments
by comparing generated answers and gold answers. In the second step, the final judgment (correct
or incorrect) was done based on the discussion of the two annotators (95% agreement). The final
score is accuracy which is the total number of correct answers divided by all gold answers.
6 P RELIMINARY RESULTS AND DISCUSSION
This section shows the preliminary results of knowledge enhancement for LLMs. It first reports
performance comparison of LLMs in context-aware and context-agnostic settings. It then shows an
analysis based on actual outputs.
6.1 P ERFORMANCE COMPARISON
As argued, LLMs need knowledge enhancement for working well in domain-specific tasks. To
validate the argument, this section reports the results of LLMs for insurance QA in two settings.
The first setting directly uses the knowledge of LLMs (context-agnostic - GPT). The second setting
uses knowledge enhancement (context-aware). Note that the same indicators and questions were
used for GPT for fair comparison (Algorithm 1, 2, and 3). This section also shows the contribution
of table processing to improve the capability of LLMs for handling structured data.
Context-aware vs. context-agnostic This section reports the results of GPT in two settings:
context-aware and context-agnostic. To do that, the comparison was done with three models: (i)
GPT with its knowledge using the prompt in Algorithm 1; (ii) GPT+RB : GPT with context from
relevant rulebook’s sections using the prompt in Algorithm 2; and (iii) GPT+RB+DBpedia : GPT
with context from relevant rulebooks’ sections and DBpedia using the prompt in Algorithm 3. More
precisely, the context-agnostic setting includes the GPT model and the contex-aware setting contains
theGPT+RB andGPT+RB+DBpedia models.
(a) Results on the expert-created dataset.
 (b) Results on the synthesized dataset.
Figure 11: Performance of GPT with different context using human evaluation.
Figure 11 shows the results of GPT across the two different settings. The general trend indicates that
knowledge enhancement is beneficial for GPT that can significantly improve the performance on the
two datasets. For the expert-created insurance dataset, there exists huge gaps between context-aware
and context-agnostic settings. It is easy to understand that GPT’s knowledge could not cover specific
questions and concepts of insurance. It turns out that GPT can only answer 10 questions (9.60%)
over 104 questions. We observed that without knowledge enhancement, GPT tends to respond
non-specific answers such as “Please provide more context... ” or“You should check their poli-
cies... ” . In contrast, domain-specific knowledge enhancement allows the GPT model to learn from
a given context, such as a rulebook, and provide more correct answers. The GPT+RB+DBpedia
16

--- PAGE 17 ---
Cinnamon AI
model demonstrates a slight improvement compared to the GPT+RB model (65.40% compared to
61.42%). This improvement can be attributed to the utilization of DBpedia as a bridge between gen-
eral knowledge and domain knowledge from the rulebooks. When relying solely on the rulebooks,
GPT may struggle to understand certain relationships and connections between a question and its an-
swer. For instance, without additional knowledge, GPT might not inherently recognize that diabetes
is considered as a lifestyle disease and should be covered under the lifestyle insurance policy. By
incorporating DBpedia, GPT’s understanding is enriched with supplementary information. DBpe-
dia provides detailed explanation such as “Lifestyle diseases can be defined as diseases linked with
one’s lifestyle... which can lead to heart disease, stroke, obesity, type II diabetes” . This additional
knowledge allows GPT to make more informed connections and draw appropriate conclusions.
The results of the synthesized dataset share the same trend with the expert-created dataset, in which
the performance of the context-aware setting is much better than that of the context-agnostic setting.
The direct use of GPT can only obtain 25.30% accuracy. On the other hand, adding rulebooks and
knowledge from DBpedia, GPT can significantly improve the performance of answering questions
(55.42% for the GPT+RB model and 83.13% for the GPT+RB+DBpedia model). It confirms the
contribution of knowledge enhancement and validates our hypothesis in Section 4. There are two
interesting points when observing Figure 11(a) and 11(b). First, external knowledge, i.e., DBpe-
dia plays a more important role in the synthesized dataset than the expert-created dataset. This is
because, the synthesized dataset contains more questions that the rulebooks could not cover. So
external knowledge helps to add additional information for the rulebooks. In contrast, almost ques-
tions in the expert-created dataset can be answered by only using the rulebooks. In this case, using
external knowledge seems to be unnecessary. In practice, domain-specific and common knowledge
should be combined to provide comprehensive context for LLMs. Second, the performance of GPT
(context-agnostic) on the synthesized dataset is much better than that on the expert-created dataset.
This may come from the creation process in which GPT can operate quite well on its generated
questions. The results in Figure 11 confirm the hypothesis stated in Section 4 that LLMs should not
stand alone and it needs knowledge enhancement for working in domain-specific scenarios. Note
that the results in Figure 11 already include table processing mentioned in the next section.
Handling tables The rulebooks contain tables that make a challenge for GPT because GPT may
not handle structured data well. So the tables need to be pre-processed to retain the columns and
rows structure. After extracting, tables were represented in the form of sequences by concatenating
text in each row and column. This representation helps GPT to understand the structure of tables
and the relationships between individual cells within a table. By leveraging this approach, GPT can
effectively process and analyze tabular data. Detailed information is shown in Algorithm 4.
Accuracy (%)
020406080
Without Table 
pre-procesWith Table pre-
process
Figure 12: Performance of GPT with and without table processing on the expert-created dataset.
Figure 12 reports the comparison of GPT by using table processing. It indicates that employing
table-like data as context improves the performance of GPT. This is because information stated in
tables is required to correctly answer input questions. Given 20 questions related to data in tables,
the model GPT+RB using table processing can answer 16 correct questions (80%) while the model
17

--- PAGE 18 ---
Cinnamon AI
without using the processing can only answer 7 correct questions. It highlights the efficacy of
structured data processing methods for the knowledge enhancement of LLMs. Note that we did not
generate questions that require answer information in tables for the synthesized dataset.
6.2 Q UESTIONS NEED EXTERNAL KNOWLEDGE FOR ANSWERING
We observed that using external knowledge can help to answer input questions. This section shows
the observation how external knowledge helps to improve the reasoning capability of GPT. To do
that, we counted the number of questions that require additional knowledge for answering.
012
GPT GPT+RB GPT+RB+DBpedia
(a) Two questions need external knowledge for
answering in the expert-created dataset.
0510152025
GPT GPT+RB GPT+RB+DBpedia(b) 29 questions need external knowledge for an-
swering in the sythesized dataset.
Figure 13: Performance of GPT on questions that require external knowledge.
Figure 13 shows that the number of questions that require external knowledge for answering in the
expert-created dataset is much smaller than that in the synthesized dataset. For the first dataset,
there only two questions that need external knowledge from rulebooks and DBpedia for answer-
ing. The GPT+RB+DBpedia model can answer these questions because retrieved information from
DBpedia can cover entities that are not mentioned in the rulebooks. The GPT+RB and GPT could
not answer the questions. Answering the two questions contributes 1.92% of the improvement of
GPT+RB+DBpedia in Figure 11(a). In contrast, the synthesized dataset includes 29 questions that
require external knowledge for answering. The GPT+RB+DBpedia can answer 27 questions while
GPT can answer 17 questions. It confirms the significant improvement of the GPT+RB+DBpedia
model compared to GPT. Interestingly, the GPT+RB model can only answer six questions. A possi-
ble reason is that only using the rulebooks may not enough and may provide noisy context for input
questions. However, the GPT+RB model can answer well other questions in Figure 11(b).
6.3 O UTPUT OBSERVATION
Table 2 shows answers of GPT under the three distinct models. GPT can answer general ques-
tions using its knowledge like “ What kind of surgery does a hysterectomy involve ?”, “What types of
surgery are involved in breast reconstruction surgery ?”, or “ If life or death of the insured person is
unknown in whole life insurance, will the death benefit be paid ?” (Table 1). These questions do not
require specific knowledge from policy rulebooks to answer. However, GPT fails to provide accu-
rate responses when confronted with questions requiring specific information from policy rulebooks.
For example, for the first and second questions, GPT could not answer the question and requires to
provide more context. On the other hand, supplemented with additional knowledge from rulebooks
and DBpedia, GPT is able to answer both general and specific questions with detailed explanations.
For the third and fourth questions, GPT can answer with common information. GPT+RB could not
answer and require more context. GPT+RB+DBpedia can correctly answer the questions. This is
because the questions contains entities that are not mentioned in the rulebooks. Therefore, retrieved
information from DBpedia helps to enrich the information of the rulebooks. GPT generates long
outputs that do not directly answer the questions (row 1, 2, and 4) because the questions require
additional knowledge and GPT tries to response as the nature of a generative LLM. In contrast, GPT
with knowledge enhancement can answer correctly with detailed references.
Figure 14 shows the failure analysis where GPT fails to provide accurate answers even after learn-
ing knowledge from relevant rulebooks and DBpedia (the best setting) on the two datasets. For the
18

--- PAGE 19 ---
Cinnamon AI
Table 2: The outputs of GPT models by using different additional context. (ii)is only using GPT;
(ii)stands for GPT+RB; and (iii)denotes GPT+RB+DBpedia.
Question Setting Answer
How much is the
radiation treatment
benefit payment?(i) I’m sorry, as an AI language model, I don’t have access to
current information about radiation treatment benefit pay-
ment. Please provide more context or specify the location
and time frame for a more accurate answer.
(ii) The radiation treatment benefit payment is (Daily hospital-
ization amount) x 10
(iii) The radiation treatment benefit payment is (Daily hospital-
ization amount) x 10
What is the
maximum amount
of advanced
medical care
benefits?(i) As an AI language model, I do not have access to specific in-
formation about a particular policy or insurance plan. Please
provide more context or details about the policy or plan in
question.
(ii) The maximum amount of advanced medical care benefits is
20 million yen.
(iii) The maximum amount of advanced medical care benefits is
20 million yen.
Can I claim a
second bone
marrow donor
benefit?(i) It is possible to claim a second bone marrow donor benefit,
but it would depend on your specific insurance contract and
the terms and conditions of your policy.
(ii) The context provided does not contain information about a
bone marrow donor benefit, so it is not possible to answer
this question.
(iii) No, payment of the bone marrow donor benefit shall be
made only once during the insurance period. (Article 10,
Supplementary Regulations Concerning Payment of Bone
Marrow Donor Benefits)
What types of
surgery are
involved in breast
reconstruction
surgery?(i) Some common techniques used in breast reconstruction
surgeries include breast implants, tissue expanders, flap re-
construction (taking tissue from another part of the body
and using it to rebuild the breast), or a combination of these
methods.
(ii) The context does not provide a clear answer to this question.
(iii) Breast reconstruction surgery involves breast reconstructive
surgery (Table 35) at a hospital or clinic (Table 6) during the
insurance period.
20/3615/36
5/36Wrong context
Complex
questionAmbiguity
question
(a) Failure cases of the expert-created dataset.
Wrong context
Complex
question6/14
8/14 (b) Failure cases of the synthesize dataset.
Figure 14: Error distribution of GPT+RB+DBpedia of failure cases on the two datasets.
expert-created dataset, except for ambiguity cases, which take 55.55% of wrong responses, 13.88%
of error cases are complex questions that GPT cannot infer directly in one-step reasoning. It may
19

--- PAGE 20 ---
Cinnamon AI
come from the nature of answering insurance questions that require multi-steps reasoning with deep
understanding from domain experts. The remaining 41.66% of error cases are caused by incorrect
context retrieval. Our retrieval module uses a simple Cosine similarity method to identify the rel-
evant context for a user’s question. For the synthesized dataset, 57.15% of error cases come from
complex questions. This ratio is quite similar to the expert-created dataset. The ratio of wrong
context retrieval is also large with 42.85%. This shortcoming shares the same cause with the expert-
created dataset in which the IR part uses a simple Cosine similarity method for relevant context
retrieval. It suggests that there is a need to improve IR to reduce the incidence of errors in context
retrieval by considering more sophisticated similarity and vectorization methods. Figure 14 also
shows that the expert-created dataset is more challenging than the synthesized dataset that has no
ambiguity questions. A possible reason comes from the creation process of the synthesized dataset
in which questions are usually generated by using one-step reasoning from GPT.
7 C ONCLUSION AND RESEARCH DIRECTIONS
This paper introduces an empirical analysis of adapting LLMs (GPT) to the QA task of the insurance
domain. To do that, we design a model in which LLMs are empowered by using domain-specific
knowledge. The knowledge enhancement provides an appropriate way that existing knowledge
bases can be integrated into LLMs by using prompt-based learning methods. Preliminary results
on two datasets in the insurance domain show four important points. First, to create powerful AI
applications, LLMs should not operate in isolation. Instead, their knowledge should be augmented
for domain adaptation by integrating AI expertise with external domain-specific knowledge. Second,
the combination of domain-specific knowledge, e.g., policy rulebooks and common knowledge, e.g.,
DBpedia benefits the inference of LLMs. Third, table processing and understanding demonstrate the
importance of table comprehension for achieving the ultimate end value of the target application.
Finally, IR plays an important role that helps to retrieve relevant context for the inference of LLMs.
Our proposal provides an effective and a practical way that can adapt LLMs to business scenarios.
These insights suggest opening several research directions to prepare LLMs for industrial and real-
world applications. One key area of focus could be using knowledge graphs containing open and
closed-domain knowledge to enhance LLMs. By incorporating structured knowledge graphs into
the model’s training and inference processes, LLMs could better capture the relationships between
entities and concepts, leading to more accurate and robust predictions. Another promising research
direction is using chain-of-thought prompts to improve reasoning procedures. This approach in-
volves providing a series of interconnected prompts to iteratively guide the model’s reasoning pro-
cess, allowing it to make more nuanced and sophisticated inferences. The possible final direction is
to improve the IR part that helps to retrieve more relevant context for the reasoning of GPT.
REFERENCES
Ayush Agrawal, Siddhartha Gadgil, Navin Goyal, Ashvni Narayanan, and Anand Tadipatri. Towards
a mathematics formalisation assistant using large language models, 2022.
Xavier Amatriain. Transformer models: an introduction and catalog. In arXiv preprint
arXiv:2302.07730 , 2023.
Razvan Azamfirei, Sapna R Kudchadkar, and James Fackler. Large language models and the perils
of their hallucinations. Critical Care , 27(1):1–2, 2023.
Dzmitry Bahdanau, Kyung Hyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In 3rd International Conference on Learning Representations,
ICLR 2015 , 2015.
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, and Holy Love-
nia et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination,
and interactivity. In arXiv preprint arXiv:2302.04023 , 2023.
Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jianfeng
Gao, Songhao Piao, Ming Zhou, et al. Unilmv2: Pseudo-masked language models for unified
language model pre-training. In International Conference on Machine Learning , pp. 642–652.
PMLR, 2020.
20

--- PAGE 21 ---
Cinnamon AI
Razieh Baradaran, Razieh Ghiasi, and Hossein Amirkhani. A survey on machine reading compre-
hension systems. Natural Language Engineering , 28(6):683–732, 2022.
Aon Benfield. Global insurance market opportunities. Risk. Reinsurance. Human Resources , (1):
63, 2015.
Yoshua Bengio, R ´ejean Ducharme, and Pascal Vincent. A neural probabilistic language model.
Advances in neural information processing systems , 13, 2000.
Samuel R Bowman. Eight things to know about large language models. arXiv preprint
arXiv:2304.00612 , 2023.
Peter F Brown, Vincent J Della Pietra, Peter V Desouza, Jennifer C Lai, and Robert L Mercer. Class-
based n-gram models of natural language. Computational linguistics , 18(4):467–480, 1992.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, and Lichao Sun. A com-
prehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt,
2023.
Kyunghyun Cho, Bart van Merri ¨enboer, C ¸ a ˘glar Gulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder–decoder
for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pp. 1724–1734, 2014.
Sumit Chopra, Michael Auli, and Alexander M Rush. Abstractive sentence summarization with
attentive recurrent neural networks. In Proceedings of the 2016 conference of the North American
chapter of the association for computational linguistics: human language technologies , pp. 93–
98, 2016.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing sys-
tems, 30, 2017.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186 , 2019.
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. All nlp
tasks are generation tasks: A general pretraining framework. In arXiv preprint arXiv:2103.10360 ,
2021.
Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. Chat-rec:
Towards interactive and explainable llms-augmented recommender system, 2023.
Fabrizio Gilardi, Meysam Alizadeh, and Mal Kubli. Chatgpt outperforms crowd-workers for text-
annotation tasks, 2023.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850 , 2013.
Suchin Gururangan, Ana Marasovi, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and
Noah A. Smith. Dont stop pretraining: Adapt language models to domains and tasks. In In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.
83428360 , 2020.
Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):
1735–1780, 1997.
21

--- PAGE 22 ---
Cinnamon AI
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. An
empirical analysis of compute-optimal large language model training. Advances in Neural Infor-
mation Processing Systems , 35:30016–30030, 2022.
IBM. Corporate governed natural language generation using gpt based llm for wat-
son assistant, 2023. URL https://community.ibm.com/community/
user/ai-datascience/blogs/rachana-vishwanathula/2023/03/04/
gpt-based-nlg-with-corporateknowledgebase .
Shumpei Inoue, Tsungwei Liu, Son Nguyen, and Minh-Tien Nguyen. Enhance incomplete utterance
restoration by joint learning token extraction and text generation. In Proceedings of the 2022
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies , pp. 3149–3158, 2022.
Sean Legassick Isaac, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from
language models. arXiv preprint arXiv:2112.04359 , 2021.
Kun Jing and Jungang Xu. A survey on neural network language models. arXiv preprint
arXiv:1906.03591 , 2019.
Ali Kashefi and Tapan Mukerji. Chatgpt for programming numerical methods, 2023.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916 , 2022.
Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zero-data learning of new tasks. In AAAI ,
volume 1, pp. 3, 2008.
Steffen L Lauritzen, A Philip Dawid, Birgitte N Larsen, and H-G Leimer. Independence properties
of directed markov fields. Networks , 20(5):491–505, 1990.
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be
strong differentially private learners. In International Conference on Learning Representations ,
2022.
Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and You Zhang. Chatdoctor: A medical chat model
fine-tuned on llama model using medical domain knowledge, 2023.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evalua-
tion. White Paper. AI21 Labs , 1, 2021.
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out , pp. 74–81, 2004.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What
makes good in-context examples for gpt-3. In arXiv preprint arXiv:2101.06804 , 2021.
Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and
Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. In Proceed-
ings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , pp. 3154–3169, 2022.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-
train, prompt, and predict: A systematic survey of prompting methods in natural language pro-
cessing. ACM Computing Surveys, 55(9), pp.1-35 , 2023.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692 , 2019.
Albert Lu, Hongxin Zhang, Yanzhe Zhang, Xuezhi Wang, and Diyi Yang. Bounding the capabilities
of large language models in open text generation with prompt constraints, 2023.
22

--- PAGE 23 ---
Cinnamon AI
Kevin Menear. Unleashing the potential of large language models in the classroom: A comprehen-
sive guide for teachers. Medium , 2023.
Tomas Mikolov, Martin Karafi ´at, Lukas Burget, Jan Cernock `y, and Sanjeev Khudanpur. Recurrent
neural network based language model. In Interspeech , volume 2, pp. 1045–1048. Makuhari, 2010.
Niklas Muennighoff. Sgpt: Gpt sentence embeddings for semantic search, 2022.
Duy-Hung Nguyen, Nguyen Viet Dung Nghiem, Bao-Sinh Nguyen, Dung Tien Tien Le, Shahab
Sabahi, Minh-Tien Nguyen, and Hung Le. Make the most of prior data: A solution for interactive
text summarization with preference feedback. In Findings of the Association for Computational
Linguistics: NAACL 2022 , pp. 1919–1930, 2022.
Minh-Tien Nguyen, Viet-Anh Phan, Le Thai Linh, Nguyen Hong Son, Le Tien Dung, Miku Hirano,
and Hajime Hotta. Transfer learning for information extraction with limited data. In Computa-
tional Linguistics: 16th International Conference of the Pacific Association for Computational
Linguistics, PACLING 2019, Hanoi, Vietnam, October 11–13, 2019, Revised Selected Papers 16 ,
pp. 469–482. Springer, 2020.
Minh-Tien Nguyen, Dung Tien Le, and Linh Le. Transformers-based information extraction with
limited data for domain-specific business documents. Engineering Applications of Artificial In-
telligence , 97:104100, 2021.
Minh-Tien Nguyen, Nguyen Hong Son, et al. Gain more with less: Extracting information from
business documents with small data. Expert Systems with Applications , 215:119274, 2023.
Mohammad Nuruzzaman and Omar Khadeer Hussain. Intellibot: A dialogue-based chatbot for the
insurance industry. Knowledge-Based Systems , 196:105810, 2020.
OpenAI. Gpt-4 technical report. arXiv preprint 2303.08774, 2023b. URL
https://doi.org/10.48550/arX iv.2303.08774 , 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems , 35:
27730–27744, 2022.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge
and data engineering , 22(10):1345–1359, 2010.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics , pp. 311–318, 2002.
Ankur Parikh, Oscar T ¨ackstr ¨om, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model for natural language inference. In Proceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing , pp. 2249–2255, 2016.
Adam Pauls and Dan Klein. Faster and smaller n-gram language models. In Proceedings of the 49th
annual meeting of the Association for Computational Linguistics: Human Language Technolo-
gies, pp. 258–267, 2011.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars
Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language
models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813 ,
2023.
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under-
standing by generative pre-training. 2018.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446 , 2021.
23

--- PAGE 24 ---
Cinnamon AI
Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified
text-to-text transformer. ArXiv , abs/1910.10683, 2020.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for
machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing , pp. 2383–2392, 2016.
Mohammed Saeed, Nicola De Cao, and Paolo Papotti. Querying large language models with sql,
2023.
Atul Sandur, ChanHo Park, Stavros V olos, Gul Agha, and Myeongjae Jeon. Jarvis: Large-scale
server monitoring with adaptive near-data processing. In 2022 IEEE 38th International Confer-
ence on Data Engineering (ICDE) , pp. 1408–1422, 2022. doi: 10.1109/ICDE53745.2022.00110.
Timo Schick and Hinrich Schtze. Its not just size that matters: Small language models are also
few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies, pp. 2339-2352 ,
2021.
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deep-
speed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.
arXiv preprint arXiv:2201.11990 , 2022.
Martin Sundermeyer, Ralf Schl ¨uter, and Hermann Ney. Lstm neural networks for language mod-
eling. In Thirteenth annual conference of the international speech communication association ,
2012.
Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
Advances in neural information processing systems , 27, 2014.
Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. Understanding the capabilities,
limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503 ,
2021.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog
applications. arXiv preprint arXiv:2201.08239 , 2022.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, pp. 60006010 , 2017.
Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. Advances in neural informa-
tion processing systems , 28, 2015.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer
Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language
understanding systems. Advances in neural information processing systems , 32, 2019.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency
improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 , 2022.
Yaqing Wang, Quanming Yao, James T Kwok, and Lionel M Ni. Generalizing from a few examples:
A survey on few-shot learning. ACM computing surveys (csur) , 53(3):1–34, 2020.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903 , 2022.
Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael Lyu. Chatgpt or gram-
marly? evaluating chatgpt on grammatical error correction benchmark, 2023.
24

--- PAGE 25 ---
Cinnamon AI
Yongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the
ugly. In Proceedings of the IEEE conference on computer vision and pattern recognition , pp.
4582–4591, 2017.
Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-gen:
An enhanced multi-flow pre-training and fine-tuning framework for natural language generation.
arXiv preprint arXiv:2001.11314 , 2020.
Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan
Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private fine-tuning
of language models. In International Conference on Learning Representations , 2022.
Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context
learning? arXiv preprint arXiv:2301.13670 , 2023.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,
and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint
arXiv:2211.01910 , 2022.
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv
preprint arXiv:1909.08593 , 2019.
A A PPENDIX
A.1 S AMPLE PROMPTS
Algorithm 1: A sample prompt without knowledge enhancement.
prompt = ‘ ‘ Answer t h e q u e s t i o n i n a s h o r t and c o n c i s e way : ‘When
w i l l I be n o t i f i e d of payment r e a s o n change ? ’”
Algorithm 2: A sample prompt using relevant rulebooks.
prompt = ‘ ‘ Answer t h e q u e s t i o n i n a s h o r t and c o n c i s e way : ‘When
w i l l I be n o t i f i e d of payment r e a s o n change ? ’ , b as e on t h e
c o n t e x t : ‘ 2 . I f t h e Company changes t h e p r o v i s i o n s c o n c e r n i n g
t h e grounds f o r payment of advanced m e d i c a l c a r e b e n e f i t s , e t c
.
p u r s u a n t t o t h e p r o v i s i o n s of P a r a g r a p h 1 , t h e Company s h a l l
n o t i f y t h e p o l i c y h o l d e r of such change a t l e a s t two months
p r i o r t o t h e d a t e of such change . ’ ”
Algorithm 3: A sample prompt using the relevant information from the DBpedia KG.
prompt = ‘ ‘ Answer t h e q u e s t i o n i n a s h o r t and c o n c i s e way based on
t h e c o n t e x t and e x t e r n a l i n f o r m a t i o n : ‘ ‘He was h o s p i t a l i z e d
f o r a week due t o d i a b e t e s . how much i s h i s b e n e f i t amount h e r
a l l o w a n c e ? ’ −−− C o n t e x t : ‘ ‘ H o s p i t a l i z a t i o n b e n e f i t s f o r
l i f e s t y l e − r e l a t e d d i s e a s e s , Payment amount , The f o l l o w i n g
amount p e r h o s p i t a l i z a t i o n ( d a i l y amount of h o s p i t a l i z a t i o n
b e n e f i t s ) x number of days of h o s p i t a l i z a t i o n d u r i n g t h e
i n s u r a n c e p e r i o d f o r t r e a t m e n t of l i f e s t y l e − r e l a t e d d i s e a s e s
l i s t e d on t h e l e f t ’ −−− E x t e r n a l i n f o r m a t i o n : ‘ ‘ l i f e s t y l e
d i s e a s e |a b s t r a c t |. . . l e a d t o h e a r t d i s e a s e , s t r o k e ,
o b e s i t y , t y p e I I d i a b e t e s and lu ng c a n c e r . . . . ”
25

--- PAGE 26 ---
Cinnamon AI
A.2 P ROMPTS OF USING TABLE DATA
Algorithm 4: Table representation for GPT.
” Table name ” : [
{
‘ ‘ row ” : name of row c o r r e s p o n d i n g t o t h e c e l l
‘ ‘ column ” : name of column c o r e s s p o n d i n g t o t h e c e l l
‘ ‘ v a l u e ” : c o n t e n t of c e l l
},
]
Example :
‘ ‘Women’ s S p e c i f i c I n s u r a n c e ” : [
{
‘ ‘ row ” : ‘ ‘ Female S p e c i f i c S u r g e r y B e n e f i t s ” ,
‘ ‘ column ” : ‘ ‘ D e t a i l s of b e n e f i t s ” ,
‘ ‘ v a l u e ” : ‘ ‘ S u r g e r y i n v o l v i n g t h e b r e a s t , u t e r u s ”
},
{
‘ ‘ row ” : ‘ ‘ B r e a s t R e c o n s t r u c t i o n B e n e f i t s ” ,
‘ ‘ column ” : ‘ ‘ D e t a i l s of b e n e f i t s ” ,
‘ ‘ v a l u e ” : ” B r e a s t r e c o n s t r u c t i o n s u r g e r y f o r t h e b r e a s t ”
}
]
26
