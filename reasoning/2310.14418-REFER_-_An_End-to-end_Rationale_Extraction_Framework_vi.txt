# REFER: Khung Trích Xuất Lý Do Từ Đầu Đến Cuối
cho Điều Chỉnh Giải Thích

Mohammad Reza Ghasemi Madani
Đại học Bologna
mohammadreza.ghasemi@studio.unibo.it

Pasquale Minervini
Đại học Edinburgh
p.minervini@ed.ac.uk

## Tóm tắt

Các lời giải thích bằng văn bản được chú thích bởi con người đang trở nên ngày càng quan trọng trong Xử lý Ngôn ngữ Tự nhiên Có thể Giải thích. Trích xuất lý do nhằm cung cấp các lời giải thích trung thực (tức là phản ánh hành vi của mô hình) và hợp lý (tức là thuyết phục đối với con người) bằng cách làm nổi bật các đầu vào có tác động lớn nhất đến dự đoán mà không làm giảm hiệu suất của mô hình tác vụ. Trong các nghiên cứu gần đây, trọng tâm của việc huấn luyện các bộ trích xuất lý do chủ yếu tập trung vào tối ưu hóa tính hợp lý bằng cách sử dụng các điểm nổi bật của con người, trong khi mô hình tác vụ được huấn luyện để tối ưu hóa đồng thời độ chính xác dự đoán tác vụ và tính trung thực. Chúng tôi đề xuất REFER, một khung sử dụng bộ trích xuất lý do có thể vi phân cho phép lan truyền ngược qua quá trình trích xuất lý do. Chúng tôi phân tích tác động của việc sử dụng các điểm nổi bật của con người trong quá trình huấn luyện bằng cách huấn luyện đồng thời mô hình tác vụ và bộ trích xuất lý do. Trong các thí nghiệm của chúng tôi, REFER mang lại kết quả tốt hơn đáng kể về mặt tính trung thực, tính hợp lý và độ chính xác tác vụ xuôi dòng trên cả dữ liệu trong phân phối và ngoài phân phối. Trên cả e-SNLI và CoS-E, cài đặt tốt nhất của chúng tôi tạo ra kết quả tốt hơn về mặt lợi ích tương đối chuẩn hóa tổng hợp so với các đường cơ sở trước đây lần lượt 11% và 3%.

## 1. Giới thiệu

Các Mô hình Ngôn ngữ Neural đã nổi lên như những người thực hiện Hiện đại nhất (SoTA) trong một loạt các tác vụ Xử lý Ngôn ngữ Tự nhiên (NLP) (Devlin et al., 2019; Liu et al., 2019). Tuy nhiên, chúng thường được nhận thức là mờ đục (Rudin, 2019; Doshi-Velez và Kim, 2017; Lipton, 2018), khơi dậy sự quan tâm đáng kể đến việc phát triển các thuật toán có thể tự động giải thích hành vi của những mô hình này (Denil et al., 2015; Sundararajan et al., 2017; Camburu et al., 2018; Rajani et al., 2019; Luo et al., 2022).

Trong lĩnh vực các mô hình neural tự giải thích, hai phương pháp nổi bật đã xuất hiện: (i) Lý do Trích xuất (ERs, Zaidan et al., 2007; Bastings và Filippova, 2020), bao gồm việc chọn một tập con các đặc trưng đầu vào có trách nhiệm cho một dự đoán, và (ii) Lời giải thích Ngôn ngữ Tự nhiên (NLEs, Park et al., 2018; Hendricks et al., 2016; Kayser et al., 2021; Camburu et al., 2018), tạo ra các lý giải có thể đọc được bởi con người cho các dự đoán. Các khía cạnh quan trọng cần quan tâm cho cả ERs và NLEs là tính hợp lý, đo lường sự phù hợp giữa các lời giải thích của mô hình và sự thật cơ bản, và tính trung thực, đo lường mức độ chính xác của các lời giải thích phản ánh quá trình ra quyết định của mô hình. ERs cung cấp các lời giải thích ngắn gọn, phục vụ như một phương tiện để người dùng đánh giá tính đáng tin cậy của một mô hình. Tuy nhiên, ERs có thể thiếu các chi tiết lý luận quan trọng, chẳng hạn như mối quan hệ đặc trưng (Wiegreffe et al., 2021). Mặt khác, NLEs cung cấp các lý giải chi tiết bằng ngôn ngữ tự nhiên, bổ sung cho ERs bằng cách có thể cung cấp các lời giải thích toàn diện hơn.

Việc đánh giá ERs bao gồm việc đánh giá tính hợp lý và tính trung thực của chúng. Tính hợp lý đề cập đến mức độ mà một điểm nổi bật giải thích một nhãn được dự đoán, như được đánh giá bởi các đánh giá viên con người, hoặc theo sự tương tự với các điểm nổi bật vàng (Yang et al., 2020; DeYoung et al., 2020). Tính trung thực đo lường mức độ chính xác mà một điểm nổi bật đại diện cho quá trình quyết định của mô hình - ví dụ, bằng cách đo lường mức độ mà sự tin tưởng vào nhãn được dự đoán thay đổi sau khi loại bỏ các từ được làm nổi bật (tính toàn diện) hoặc khi chỉ xem xét các từ được làm nổi bật (tính đầy đủ) (Alvarez Melis và Jaakkola, 2018; Wiegreffe và Pinter, 2019).

Các nghiên cứu trước đây chủ yếu tập trung vào trích xuất lý do, bao gồm việc giải thích đầu ra của một mô hình bằng cách xác định các token đầu vào gây ra ảnh hưởng lớn nhất đến dự đoán của mô hình (Denil et al., 2015; Sundararajan et al., 2017; Jin et al., 2020; Lundberg và Lee, 2017) và cung cấp tín hiệu giám sát bổ sung (Hase và Bansal, 2022).

Phần lớn các nghiên cứu trước đây trong lĩnh vực này đã xoay quanh điều chỉnh giải thích, một kỹ thuật nhằm cải thiện khả năng tổng quát hóa trong các mô hình neural bằng cách điều chỉnh các lý do máy với các lý do con người (Ross et al., 2017; Huang et al., 2021; Ghaeini et al., 2019; Kennedy et al., 2020; Rieger et al., 2020; Liu và Avci, 2019). Tuy nhiên, ERs là các phân phối rời rạc trên văn bản đầu vào, có thể khó học bởi các mô hình neural thông qua lan truyền ngược (Niepert et al., 2021).

Trong nghiên cứu này, chúng tôi đề xuất REFER, một Khung Trích xuất Lý do Từ đầu đến cuối cho Điều chỉnh Giải thích, cho phép lan truyền ngược qua quá trình trích xuất lý do. Cụ thể, REFER bao gồm một bộ trích xuất lý do có thể vi phân, chọn k% từ quan trọng nhất từ đầu vào văn bản, sau đó được sử dụng bởi mô hình để tạo ra dự đoán.

## 2. Các Nghiên cứu Liên quan

Sự phức tạp vốn có của các mô hình neural đã tạo ra những lo ngại về tính mờ đục của chúng (Rudin, 2019), đặc biệt về những tác động xã hội của việc sử dụng các mô hình neural trong các tình huống ra quyết định có tính cược cao (Bender et al., 2021). Do đó, khả năng giải thích có tầm quan trọng tối cao để thúc đẩy niềm tin, đảm bảo thực hành đạo đức và duy trì sự an toàn của các hệ thống NLP (Doshi-Velez và Kim, 2017; Lipton, 2018).

**Học để Giải thích** Hợp lý hóa cung cấp các lời giải thích cục bộ bằng cách cung cấp một lời giải thích duy nhất cho mỗi dự đoán thay vì một lời giải thích toàn cục bao phủ toàn bộ mô hình (Baehrens et al., 2010; Ribeiro et al., 2016). Những lời giải thích này mang lại những hiểu biết có giá trị cho các mục đích khác nhau, bao gồm gỡ lỗi, định lượng độ thiên vị và công bằng, hiểu hành vi mô hình, và đảm bảo tính mạnh mẽ và quyền riêng tư (Molnar, 2022). Tuy nhiên, việc có được giám sát trực tiếp dưới dạng các lý do được gắn nhãn bởi con người trong quá trình huấn luyện không phải lúc nào cũng khả thi, điều này đã dẫn đến việc phát triển các bộ dữ liệu bao gồm các lý giải của con người cho các nhãn đúng. Những nỗ lực này nâng cao khả năng diễn giải của các mô hình NLP và giải quyết các hạn chế liên quan đến giám sát trực tiếp trong việc học giải thích.

**Lời giải thích Hậu hoc** Lời giải thích hậu hoc là một nhánh khác của nghiên cứu khả năng diễn giải. Những lời giải thích này thường bao gồm điểm số tầm quan trọng ở cấp độ token. Trong việc tìm kiếm các lời giải thích hậu hoc hiệu quả, cần phải đạt được sự cân bằng giữa sự rõ ràng của ngữ nghĩa và việc tránh các hành vi trái trực giác. Lời giải thích dựa trên gradient (Sundararajan et al., 2017; Smilkov et al., 2017) cung cấp ngữ nghĩa rõ ràng bằng cách mô tả tác động cục bộ của các nhiễu loạn đầu vào đến đầu ra của mô hình. Tuy nhiên, đôi khi chúng có thể thể hiện các hành vi không nhất quán (Feng et al., 2018), và hiệu quả của chúng dựa vào tính khả vi của mô hình. Ngoài ra, có những phương pháp bất khả tri mô hình không dựa vào các tính chất mô hình cụ thể. Một ví dụ đáng chú ý là Lời giải thích Địa phương Có thể Diễn giải Bất khả tri Mô hình (LIME, Ribeiro et al., 2016). Những phương pháp này xấp xỉ hành vi của mô hình cục bộ bằng cách lặp đi lặp lại việc đưa ra dự đoán trên các đầu vào bị nhiễu loạn và khớp một mô hình đơn giản, có thể giải thích được trên các đầu ra kết quả.

**Học từ Lý do Con người** Nghiên cứu gần đây đã tập trung vào việc tận dụng các lý do để nâng cao việc huấn luyện các bộ phân loại văn bản neural. Zhang et al. (2016) giới thiệu một Mạng Neural Tích chập được tăng cường lý do mà rõ ràng xác định các câu hỗ trợ cho việc phân loại. Strout et al. (2019) chứng minh rằng việc kết hợp các lý do trong quá trình huấn luyện cải thiện chất lượng của các lý do được dự đoán, như được con người ưa thích hơn so với các mô hình được huấn luyện mà không có giám sát rõ ràng (Strout et al., 2019). Ngoài các mô hình tích hợp, các phương pháp pipeline đã được đề xuất, trong đó các mô hình riêng biệt được huấn luyện để trích xuất lý do và phân loại dựa trên những lý do được trích xuất này (Lehman et al., 2019; Chen et al., 2019). Những phương pháp này giả định sự có sẵn của dữ liệu huấn luyện rõ ràng cho việc trích xuất lý do.

**Mục tiêu Lý do Trích xuất** Một số nghiên cứu trước đây đã nhằm nâng cao tính trung thực của các lý do trích xuất bằng cách sử dụng các Thuật toán Quy kết (AAs), trích xuất lý do thông qua các hàm được tạo thủ công (Sundararajan et al., 2017; Ismail et al., 2021; Situ et al., 2021). Tuy nhiên, AAs không dễ dàng tối ưu hóa và thường đòi hỏi các tài nguyên tính toán đáng kể. Situ et al. (2021); Schwarzenberg et al. (2021) giải quyết chi phí tính toán bằng cách huấn luyện một mô hình để mô phỏng hành vi của một AA. Jain et al. (2020); Yu et al. (2021); Paranjape et al. (2020); Bastings và Filippova (2020); Yu et al. (2019); Lei et al. (2016) sử dụng các Pipeline Chọn-Dự đoán (SPPs) để tạo ra các lý do trung thực. Tuy nhiên, SPPs chỉ đảm bảo tính đầy đủ mà không đảm bảo tính toàn diện (DeYoung et al., 2020), và thường tạo ra kết quả ít chính xác hơn, vì chúng chỉ có thể quan sát một phần của đầu vào, và do những thách thức liên quan đến tối ưu hóa dựa trên gradient và các phân phối rời rạc.

Về tính hợp lý của các lý do, các phương pháp hiện tại thường bao gồm việc giám sát các bộ trích xuất lý do neural (Bhat et al., 2021) và SPPs (Jain et al., 2020; Paranjape et al., 2020; DeYoung et al., 2020) bằng cách sử dụng các lý do vàng. Tuy nhiên, các bộ trích xuất dựa trên LM thiếu huấn luyện cho tính trung thực, và SPPs hy sinh hiệu suất tác vụ để đạt được tính trung thực bằng cách xây dựng. Các nghiên cứu khác chủ yếu tập trung vào cải thiện tính hợp lý của các lý do (Narang et al., 2020; Lakhotia et al., 2021; Camburu et al., 2018), thường sử dụng các pipeline cụ thể cho tác vụ (Rajani et al., 2019; Kumar và Talukdar, 2020). Ngược lại, REFER tối ưu hóa đồng thời cả mô hình tác vụ và bộ trích xuất lý do cho tính trung thực, tính hợp lý và hiệu suất tác vụ và đạt được sự cân bằng tốt hơn về những tiêu chí này mà không gặp phải nhược điểm từ các phương pháp dựa trên heuristic (ví dụ, AAs).

## 3. Kiến trúc Mô hình

**Mô hình Tác vụ** Xem xét Ftask là mô hình tác vụ cho phân loại văn bản, trong đó nó bao gồm một bộ mã hóa (Vaswani et al., 2017) và một đầu. Đặt xi = [xti]nt=1 là chuỗi đầu vào thứ i với độ dài n, và Ftask(xi) ∈ RM là vector logit cho đầu ra của mô hình tác vụ. Chúng tôi sử dụng yi = arg max j[Ftask(xi)]j để biểu thị lớp được dự đoán bởi mô hình tác vụ. Cho rằng mất mát entropy chéo được sử dụng để huấn luyện Ftask dự đoán y*i, mất mát tác vụ được định nghĩa như sau:

Ltask = LCE(Ftask(xi), y*i) (1)

**Bộ Trích xuất Lý do** Đặt Fext biểu thị một bộ trích xuất lý do, sao cho si = Fext(xi). Cho Ftask, xi, và yi, mục tiêu của việc trích xuất lý do là đưa ra vector si = [sti]nt=1 ∈ Rn, sao cho mỗi sti là một điểm số tầm quan trọng cho biết mức độ mạnh mẽ mà token xti đã ảnh hưởng đến Ftask để dự đoán lớp yi. Các lý do cuối cùng thường được thu được bằng cách nhị phân hóa si thành r(k)i ∈ {0,1}n, thông qua chiến lược k% hàng đầu (DeYoung et al., 2020; Jain et al., 2020; Pruthi et al., 2022; Chan et al., 2021).

Để nắm bắt mức độ mà các đoạn trong các lý do được trích xuất đủ để một mô hình đưa ra dự đoán, chúng tôi đo sự khác biệt trong độ tin cậy của mô hình khi xem xét đầu vào hoàn chỉnh so với chỉ các lý do được trích xuất. Một sự khác biệt nhỏ cho thấy tầm quan trọng cao của các lý do được trích xuất.

Lsuff-diff = LCE(Ftask(r(k)i), y*i) - LCE(Ftask(xi), y*i) (2)

Theo Chan et al. (2022), để tránh các mất mát âm, chúng ta có thể sử dụng lề ms để áp đặt một giới hạn dưới cho Lsuff-diff, tạo ra tiêu chí lề sau:

Lsuff = max(-ms, Lsuff-diff) + ms (3)

Để tính toán tính toàn diện, chúng tôi tạo các ví dụ đối chiếu cho xi, x̃i = xi \ r(k)i, là xi với các lý do được dự đoán ri bị loại bỏ (Zaidan et al., 2007). Tương tự như Phương trình (2), chúng tôi đo sự khác biệt trong độ tin cậy của mô hình giữa việc xem xét đầu vào hoàn chỉnh và tập hợp đối chiếu x̃i. Một điểm số cao ở đây ngụ ý rằng các lý do có ảnh hưởng trong dự đoán.

Lcomp-diff = LCE(Ftask(xi), y*i) - LCE(Ftask(x̃i), y*i) (4)

Lặp lại, chúng tôi bắt buộc Lcomp-diff phải dương như sau:

Lcomp = max(-mc, Lcomp-diff) + mc (5)

Cuối cùng, việc lựa chọn các token để khớp với các điểm nổi bật của con người có thể được coi là một bài toán phân loại nhị phân, và mất mát tính hợp lý được tính bằng cách sử dụng hàm mất mát entropy chéo nhị phân (BCE):

Lplaus = -Σt r*,ti log(Fext(xti)) (6)

trong đó r*i là lý do vàng cho đầu vào xi có độ dài t. Điều này dẫn đến mục tiêu học đa tác vụ sau:

L = Ltask + αfLfaith + αpLplaus
= Ltask + αcLcomp,K + αsLsuff,K + αpLplaus

**Lan truyền Ngược Qua Trích xuất Lý do** Để lan truyền ngược qua quá trình trích xuất lý do, chúng tôi sử dụng Ước lượng Khả năng Tối đa Ngầm Định Thích ứng (AIMLE, Minervini et al., 2023), một phương pháp ước lượng gradient có phương sai thấp và độ thiên vị thấp được đề xuất gần đây cho phân phối rời rạc mà không đòi hỏi điều chỉnh siêu tham số đáng kể. AIMLE là một phần mở rộng của Ước lượng Khả năng Tối đa Ngầm Định (IMLE, Niepert et al., 2021), một bộ ước lượng gradient dựa trên nhiễu loạn trong đó gradient của mất mát đối với điểm số token ∇sL được ước lượng là ∇sL ≈ r(s+ε) - r(s+λ∇rL+ε), trong đó ε biểu thị nhiễu Gumbel, r biểu thị hàm k% hàng đầu, và λ là một siêu tham số được người dùng chọn. AIMLE loại bỏ nhu cầu người dùng chọn λ bằng cách tự động xác định λ tối ưu cho một tác vụ học cho trước.

## 4. Các Câu hỏi Nghiên cứu

**RQ1: Việc huấn luyện mô hình trên các điểm nổi bật của con người có cải thiện các tính chất tổng quát hóa của mô hình không?** Ngày nay, các hệ thống học máy có thể học để nắm bắt các tương quan giả trong dữ liệu để giải quyết bất kỳ tác vụ cho trước nào, và thường gặp khó khăn trong các trường hợp thách thức hơn (McCoy et al., 2019). Khi các mô hình được phép đưa ra dự đoán mà không xem xét các tiêu chí liên quan đến lý do - tính trung thực và tính hợp lý - các lý do được trích xuất bởi mô hình có thể không thể hiểu được và thiếu các diễn giải có ý nghĩa (Vig và Belinkov, 2019). Mà không hiểu các yếu tố và thông tin ảnh hưởng đến dự đoán của mô hình, việc tin tưởng hoặc giải thích đầu ra của nó trở nên khó khăn. Trong một số bối cảnh, các lời giải thích trung thực là rất quan trọng - ví dụ, chúng có thể được sử dụng để xác định xem một mô hình có dựa vào các thuộc tính được bảo vệ hay không, chẳng hạn như giới tính hoặc nhóm tôn giáo (Pruthi et al., 2020). McCoy et al. (2019) đề xuất giả thuyết rằng các mô hình suy luận ngôn ngữ tự nhiên neural (NLI) có thể dựa vào ba heuristic cú pháp có thể sai: (i) sự chồng chéo từ vựng, (ii) các chuỗi con, và (iii) các thành phần. Để đánh giá xem các mô hình có thực sự áp dụng những heuristic này hay không, chúng tôi sử dụng Phân tích Heuristic cho Hệ thống NLI (HANS, McCoy et al., 2019), bao gồm nhiều ví dụ trong đó những heuristic như vậy thất bại, cung cấp một phương tiện để đánh giá sự phụ thuộc của mô hình vào những heuristic này. Bảng 7 hiển thị các trường hợp của những heuristic này trong bộ dữ liệu HANS.

Tính trung thực đề cập đến mức độ mà một lời giải thích được cung cấp bởi một mô hình phản ánh chính xác thông tin được sử dụng bởi mô hình để đưa ra quyết định (Jacovi và Goldberg, 2020). chúng có thể được sử dụng để xác định xem một mô hình có dựa vào các thuộc tính được bảo vệ hay không, chẳng hạn như giới tính hoặc nhóm tôn giáo (Pruthi et al., 2020).

**RQ2: Làm thế nào chúng ta có thể khiến máy móc bắt chước các lý do của con người?** Các lý do của con người thường được bắt nguồn từ kiến thức nền rộng lớn và sự hiểu biết về các khái niệm khác nhau của họ. Trong khi các mô hình ngôn ngữ (LMs) sở hữu một số mức độ kiến thức này, chúng đối mặt với thách thức trong việc cân bằng giữa tối ưu hóa hiệu suất tác vụ và đáp ứng các tiêu chí cho lời giải thích trích xuất. Do đó, việc cân bằng tính hợp lý, tính trung thực và độ chính xác tác vụ đặt ra một nhiệm vụ thách thức. Một mô hình có thể phản ánh quá trình bên trong của nó để đưa ra dự đoán (trung thực), nhưng nó có thể không có ý nghĩa đối với con người (không hợp lý). Mặt khác, một mô hình trả về các lý do thuyết phục (hợp lý) mà không sử dụng chúng trong quá trình ra quyết định là không hữu ích lắm (không trung thực).

**RQ3: Việc huấn luyện mô hình trên một số lượng nhỏ các điểm nổi bật của con người có cải thiện các tính chất tổng quát hóa của nó không?** Con người có thể học hiệu quả các tác vụ mới chỉ với một vài ví dụ bằng cách tận dụng kiến thức trước của họ. Các phương pháp gần đây để hợp lý hóa dựa vào một số lượng lớn các ví dụ huấn luyện có nhãn, bao gồm nhãn tác vụ và các lý do được chú thích cho mỗi trường hợp. Việc có được những chú thích rộng rãi như vậy thường không khả thi cho nhiều tác vụ. Ngoài ra, việc tinh chỉnh LMs, thường có hàng tỷ tham số, có thể tốn kém và dễ bị overfitting. Với chi phí cao của các chú thích con người, một phương pháp thực tế hơn bao gồm việc kết hợp một lượng giám sát con người hạn chế. Chúng tôi điều tra các đặc tính của các lý do hiệu quả và chứng minh rằng việc làm cho mô hình neural nhận thức được các dự đoán được hợp lý hóa của nó có thể nâng cao đáng kể hiệu suất của nó, đặc biệt trong các kịch bản tài nguyên thấp.

**RQ4: Các bộ trích xuất lý do đã học có tổng quát hóa đến dữ liệu OOD không?** Hiệu suất kém của các mô hình trên các bộ dữ liệu OOD có thể bắt nguồn từ những hạn chế trong kiến trúc của mô hình, tín hiệu không đủ trong tập huấn luyện OOD, hoặc sự kết hợp của cả hai (McCoy et al., 2019). Một hệ thống NLI gắn nhãn đúng một ví dụ có thể không làm như vậy bằng cách hiểu ý nghĩa của các câu mà bằng cách dựa vào giả định rằng bất kỳ giả thuyết nào có từ xuất hiện trong tiền đề đều được tiền đề kéo theo (Dasgupta et al., 2018; Naik et al., 2018). Gururangan et al. (2018) đặt ra nghi ngờ về việc liệu các mô hình được huấn luyện trên bộ dữ liệu SNLI có thực sự học hiểu ngôn ngữ hay chủ yếu dựa vào các tương quan giả, còn được gọi là artifacts. Ví dụ, các từ như "friends" và "old" thường xuất hiện trong các giả thuyết trung tính. Để phân tích điều này, chúng tôi đánh giá mô hình của chúng tôi trên các tập đối chiếu (Gardner et al., 2020) cũng như dữ liệu chưa thấy, là những nhiễu loạn nhỏ thay đổi nhãn (chủ yếu) trên các trường hợp để hiểu ranh giới địa phương thực sự của bộ dữ liệu. Về cơ bản, chúng giúp chúng ta hiểu xem bộ trích xuất lý do có học bất kỳ đường tắt cụ thể cho bộ dữ liệu nào không. Bảng 9 hiển thị các mẫu cho cả các trường hợp thay đổi nhãn và không thay đổi nhãn được sửa đổi bởi Li et al. (2020).

## 5. Thí nghiệm

### 5.1 Đường cơ sở

Lớp đầu tiên của các đường cơ sở là AAs, không bao gồm việc huấn luyện Fext và được áp dụng hậu hoc (tức là chúng không ảnh hưởng đến việc huấn luyện Ftask). Đường cơ sở Gradient Tích hợp (AA (IG), Sundararajan et al., 2017) được sử dụng như một đường cơ sở cho lớp này. Huấn luyện Hướng dẫn Saliency (SGT, Ismail et al., 2021) là một đường cơ sở khác sử dụng tiêu chí dựa trên tính đầy đủ để điều chỉnh Ftask, sao cho AA mang lại các lý do trung thực cho Ftask.

Một phương pháp khác là Pipeline Chọn-Dự đoán (SPP), trong đó Ftask được huấn luyện để giải quyết một tác vụ cho trước chỉ sử dụng các token được chọn bởi Fext (Jain et al., 2020; Yu et al., 2019; Paranjape et al., 2020); do đó, SPPs nhằm mục đích "tính trung thực bằng cách xây dựng". FRESH (Jain et al., 2020) và A2R (Yu et al., 2019) đã được đề xuất để tạo ra các lý do trung thực: FRESH dựa vào việc huấn luyện Ftask và Fext riêng biệt, trong khi A2R nhằm cải thiện hiệu suất tác vụ của Ftask bằng cách điều chỉnh nó với một bộ dự đoán dựa trên attention sử dụng đầu vào đầy đủ (Jain et al., 2020; Yu et al., 2019).

Pipeline gần đây nhất là UNIREX (Chan et al., 2022), xem xét hai biến thể kiến trúc chính: (i) Dual LM (DLM), trong đó Ftask và Fext là hai LM dựa trên Transformer riêng biệt với cùng kiến trúc encoder (ii) Shared LM (SLM), trong đó Ftask và Fext chia sẻ encoder, trong khi Fext có đầu ra riêng. Hình 10 hiển thị kiến trúc cho DLM và SLM trong UNIREX. DLM cung cấp nhiều năng lực hơn cho Fext, có thể giúp Fext cung cấp các lý do hợp lý. Trong khi SLM tận dụng học đa tác vụ và cải thiện tính trung thực vì Fext có quyền truy cập lớn hơn vào thông tin về quá trình lý luận của Ftask (Chan et al., 2022). REFER hưởng lợi từ cả kiến trúc SLM và DLM bằng cách thiết lập giao tiếp giữa Ftask và Fext riêng biệt sử dụng lan truyền ngược.

### 5.2 Metrics

Để đánh giá tính trung thực, tính hợp lý và hiệu suất tác vụ, chúng tôi áp dụng các metrics được thiết lập trong benchmark ERASER (DeYoung et al., 2020) và UNIREX (Chan et al., 2022). Để đánh giá tính trung thực, chúng tôi sử dụng tính toàn diện và tính đầy đủ, và tính toán các metrics tính toàn diện và tính đầy đủ cuối cùng bằng cách sử dụng đường cong diện tích trên độ chính xác (AOPC). Đo lường sự khớp chính xác giữa các lý do được dự đoán và tham chiếu có thể quá nghiêm ngặt; do đó, DeYoung et al. (2020) cũng xem xét Intersection-Over-Union (IOU) cho phép gán điểm cho các khớp một phần. Chúng tôi sử dụng những khớp một phần này để tính toán Diện tích Dưới Đường cong Precision-Recall (AUPRC) và Token F1 (TF1) để định lượng sự tương tự giữa các lý do được trích xuất và các lý do vàng (DeYoung et al., 2020; Narang et al., 2020). Ngoài ra, chúng tôi sử dụng độ chính xác và macro F1 để đánh giá hiệu suất mô hình tác vụ trên CoS-E và e-SNLI, tương ứng. Để so sánh các phương pháp khác nhau đối với tất cả ba tiêu chí, Chan et al. (2022) sử dụng metric Lợi ích Tương đối Chuẩn hóa (NRG) ánh xạ tất cả điểm số thô đến phạm vi [0,1] - càng cao càng tốt. Cuối cùng, để tóm tắt tất cả các metrics thô, chúng tôi tính điểm NRG đơn bằng cách lấy trung bình điểm NRG cho tính trung thực, tính hợp lý và độ chính xác tác vụ.

### 5.3 Bộ dữ liệu

Chúng tôi chủ yếu thí nghiệm với các bộ dữ liệu CoS-E (Rajani et al., 2019) và e-SNLI (Camburu et al., 2018), tất cả đều có các chú thích lý do vàng từ ERASER (DeYoung et al., 2020). Để đánh giá tổng quát hóa OOD, chúng tôi xem xét MNLI (Williams et al., 2018) và HANS (McCoy et al., 2019).

CoS-E (Rajani et al., 2019) bao gồm các câu hỏi và câu trả lời trắc nghiệm được lấy từ công việc của (Talmor et al., 2019). Nó bao gồm các lý do hỗ trợ cho mỗi cặp câu hỏi-câu trả lời dưới hai dạng. Các đoạn hỗ trợ được trích xuất và các mô tả văn bản tự do cung cấp lời giải thích chi tiết hơn về lý luận đằng sau lựa chọn câu trả lời.

e-SNLI (Camburu et al., 2018) là một phần tăng cường của corpus SNLI (Bowman et al., 2015) và bao gồm các lý do con người cũng như lời giải thích ngôn ngữ tự nhiên. Đối với các cặp trung tính, người chú thích chỉ có thể làm nổi bật các từ trong giả thuyết. Hơn nữa, họ xem xét các lời giải thích liên quan đến mâu thuẫn hoặc trung tính là đúng miễn là ít nhất một phần bằng chứng trong đầu vào được làm nổi bật. Việc tập trung vào giả thuyết và cho phép làm nổi bật một phần bằng chứng dẫn đến việc thu thập các điểm nổi bật không toàn diện trong bộ dữ liệu.

MNLI (Williams et al., 2018) bao phủ phạm vi rộng hơn của văn bản viết và nói, chủ đề, phong cách và mức độ trang trọng so với SNLI. Nó được giới thiệu để xác định mối quan hệ logic giữa hai câu cho trước. Để đánh giá các metrics tính hợp lý trên dữ liệu OOD, chúng tôi thực hiện lấy mẫu ngẫu nhiên 50 trường hợp từ phần validation của MNLI và chú thích chúng thủ công đối với nhãn vàng. Chúng tôi gọi tập con dữ liệu cụ thể này là e-MNLI. Bảng 6 hiển thị các trường hợp từ e-MNLI cho các nhãn khác nhau. Để tiến hành đánh giá tổng quát hóa OOD bổ sung, chúng tôi sử dụng hai Tập Đối chiếu OOD được gọi là MNLI-Contrast và MNLI-Original. Những tập đối chiếu này được tạo ra bằng cách sửa đổi nhẹ các trường hợp MNLI gốc (Li et al., 2020). Trong MNLI-Contrast, việc sửa đổi thay đổi nhãn gốc, trong khi trong MNLI-Original, nhãn gốc vẫn giữ nguyên. Các ví dụ của những tập đối chiếu này được hiển thị trong Bảng 9.

HANS (McCoy et al., 2019) được thiết kế để đánh giá khả năng của các hệ thống NLI dựa vào heuristics và patterns thay vì hiểu biết thực sự. HANS bao gồm các cặp câu được tạo ra một cách cẩn thận để đánh lừa các mô hình bằng cách sử dụng ba danh mục heuristic: Chồng chéo Từ vựng, Chuỗi con và Thành phần. Các trường hợp cho mỗi heuristic được đưa ra trong Bảng 7. Bằng cách đánh giá các mô hình trên bộ dữ liệu HANS, các nhà nghiên cứu có thể có được hiểu biết sâu sắc về các hạn chế và tính mạnh mẽ của các hệ thống NLI.

## 6. Kết quả

**RQ1: Việc huấn luyện mô hình trên các điểm nổi bật của con người có cải thiện các tính chất tổng quát hóa của mô hình không?** Chúng tôi gắn nhãn với +P và +FP các mô hình được huấn luyện bằng cách tối ưu hóa cho tính hợp lý và đồng thời tính trung thực và tính hợp lý, tương ứng. Hình 6 hiển thị kết quả chính cho e-SNLI về NRG. Nhìn chung, REFER+FP đạt được NRG tổng hợp cao nhất, cải thiện so với đường cơ sở mạnh nhất (UNIREX SLM+FP) 12%. Về tính hợp lý, các mô hình được huấn luyện rõ ràng cho tính hợp lý (+P) hoặc cả tính trung thực và tính hợp lý (+FP) đạt được kết quả tương tự, với REFER+FP vượt trội so với mô hình tốt thứ hai 3%. Về tính trung thực, REFER đạt được điểm số cao nhất trong tất cả ba cấu hình. Một phát hiện thú vị là ngay cả khi huấn luyện REFER và A2R chỉ cho tính hợp lý (REFER+P và A2R+P), điểm NRG tính trung thực của chúng vẫn cao hơn đáng kể so với tất cả các phương pháp khác. Kết quả chi tiết được hiển thị trong Bảng 10 và Bảng 11. Ngoài ra, chúng tôi phân tích dự đoán của mô hình trên các trường hợp được gắn nhãn đúng so với những trường hợp được gắn nhãn sai, như được trình bày trong Bảng 1. Đáng ngạc nhiên, mặc dù mô hình đạt được điểm tính hợp lý tương đối cao, các metrics tính đầy đủ và tính toàn diện thấp khi mô hình dự đoán nhãn sai. Điều này cho thấy rằng ngay cả khi các lý do con người được trích xuất từ đầu vào, mô hình không dựa mạnh vào chúng trong đầu vào được gắn nhãn sai.

Các lý do được trích xuất bởi mô hình, được hiển thị trong Bảng 2, chứng minh tác động của điều chỉnh trên điều chỉnh giải thích. Mà không có điều chỉnh ER, lý luận của mô hình có xu hướng dựa vào các mẫu dữ liệu cụ thể và heuristics thay vì các lời giải thích có ý nghĩa. Ngược lại, khi mô hình được điều chỉnh trên ER, chất lượng của các lý do cải thiện đáng kể về mặt tính trung thực và tính hợp lý. Ví dụ, ví dụ làm nổi bật việc lựa chọn "man pushing cart" và "woman smoking cigarette" như các lý do để dự đoán nhãn contradiction. Các metrics đánh giá cho tính trung thực trên e-SNLI trong Bảng 4 hỗ trợ thêm cho quan niệm rằng mô hình thực sự dựa vào những lý do này cho dự đoán của nó.

**RQ2: Làm thế nào chúng ta có thể khiến máy móc bắt chước các lý do của con người?** Hình 7 hiển thị phân phối kết quả cho các kết hợp khác nhau của trọng số mất mát tính trung thực và tính hợp lý trên tập validation CoS-E. Chúng tôi huấn luyện mô hình cho (αf, αp) ∈ {0.0, 0.5, 1.0}². Dựa trên kết quả, có một tương quan ngược nhẹ giữa tính hợp lý và tính trung thực. Tuy nhiên, tác vụ cho thấy hành vi tương đối ổn định qua biến thiên tính trung thực và tính hợp lý. Điều này có nghĩa là, với pipeline của chúng tôi, chúng ta không thể đạt được sự cân bằng tính hợp lý và tính trung thực cao hơn từ một mức độ nhất định trên CoS-E.

**RQ3: Giám sát nhỏ của điểm nổi bật con người sẽ giúp như thế nào?** Chúng tôi tiến hành các thí nghiệm để điều tra cách mô hình của chúng tôi hoạt động khi các tỷ lệ phần trăm khác nhau của dữ liệu được chú thích bởi con người được bao gồm trong tập huấn luyện. Hình 8 trình bày kết quả thu được cho tất cả các tiêu chí huấn luyện khi sử dụng các tỷ lệ phần trăm khác nhau của chú thích con người: 0.1%, 1%, 10%, 20%, 50%, và 100%. Kết quả cho thấy cho đến 10% dữ liệu được chú thích bởi con người, tính hợp lý vẫn nhất quán. Mặt khác, REFER đạt được tính hợp lý tương đương với 100% giám sát con người chỉ với 50% chú thích con người. Điều này có nghĩa REFER cho phép tối ưu hóa tính hợp lý hiệu quả bằng cách sử dụng giám sát lý do vàng tối thiểu. Ngược lại, hiệu suất tác vụ bị giảm bởi việc tăng giám sát lý do con người vì mô hình nên học từ các điểm nổi bật của con người thay vì các mẫu lặp lại. Tính trung thực không thể hiện mối quan hệ rõ ràng với sự có sẵn của các lý do vàng, vì nó dựa vào các đặc trưng nội tại của mô hình thay vì các lý do được cung cấp bởi con người.

Một phát hiện thú vị khác là mô hình được huấn luyện cho một k% cụ thể hoạt động tốt trên các k% khác trong quá trình suy luận đối với tính hợp lý. Hình 9 hiển thị hành vi khá ổn định của mô hình được huấn luyện cho top-50% và được đánh giá cho các k% khác đối với tính hợp lý TF1. Điều này có nghĩa mô hình có xu hướng chọn các lý do trong số các điểm nổi bật của con người ngay cả với số lượng k thấp. Bảng 8 minh họa lý do được chọn bởi mô hình được huấn luyện cho top-50% và được đánh giá cho các k khác nhau.

**RQ4: Bộ trích xuất lý do đã học có tổng quát hóa qua dữ liệu OOD không?** Bảng 3 và Bảng 4 hiển thị kết quả REFER trên các bộ dữ liệu ID và OOD. Trong cả hai Bảng, REFER được huấn luyện trên bộ dữ liệu ID và được đánh giá trên các tập ID và OOD. Chúng tôi xem xét kết quả từ Bảng 3 như đường cơ sở và phân tích hiệu ứng của điều chỉnh ER trong Bảng 4. Khi chúng tôi huấn luyện mô hình với điều chỉnh giải thích, tính trung thực và tính đầy đủ được nâng cao. Trên MNLI, tính đầy đủ cải thiện từ 0.206 đến 0.109, trong khi trên HANS, nó từ 0.249 đến 0.071. Về Tính toàn diện, việc huấn luyện mô hình cùng với điều chỉnh ER cải thiện đường cơ sở từ 0.212 đến 0.310 trên MNLI và từ 0.272 đến 0.320 trên HANS. Bên cạnh đó, kết quả trên e-MNLI trong Bảng 4 cho thấy tính hợp lý của OOD đáng kể và có thể so sánh với dữ liệu ID. Tương tự, tính toàn diện và tính đầy đủ cải thiện trên cả MNLI-Contrast và MNLI-Original. Tuy nhiên, kết quả trên MNLI-Original dường như tốt hơn, đặc biệt đối với task macro F1, có nghĩa mô hình hoạt động tốt như nhau trong việc dự đoán các nhãn khác nhau.

## 7. Kết luận

Trong bài báo này, chúng tôi đề xuất REFER, một khung trích xuất lý do huấn luyện đồng thời mô hình tác vụ và bộ trích xuất lý do để tối ưu hóa hiệu suất tác vụ xuôi dòng, tính trung thực và tính hợp lý. Là hoàn toàn từ đầu đến cuối, nhờ vào Ước lượng Khả năng Tối đa Ngầm Định Thích ứng (Minervini et al., 2023), cho phép mô hình tác vụ và bộ trích xuất lý do được tối ưu hóa đồng thời cho những tiêu chí này, do đó nhận thức về hành vi của nhau và điều chỉnh tham số của chúng để cải thiện hiệu suất và đạt được sự cân bằng tốt hơn. Sau đó chúng tôi phân tích một số khía cạnh của quá trình trích xuất lý do, điều tra cách các lý do con người ảnh hưởng đến hành vi mô hình; cách mô hình có thể bắt chước các lý do được tạo ra bởi con người; và mức độ mà các mô hình đã học có thể tổng quát hóa trên các bộ dữ liệu OOD. Cuối cùng, bằng cách trả lời tất cả những câu hỏi này, chúng tôi so sánh hiệu suất REFER với các phương pháp và kiến trúc khác và minh họa rằng mô hình của chúng tôi vượt trội so với các mô hình trước đây trong hầu hết các trường hợp.
