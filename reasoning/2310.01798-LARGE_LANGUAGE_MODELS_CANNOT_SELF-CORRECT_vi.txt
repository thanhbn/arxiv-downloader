# 2310.01798.pdf
# Chuyển đổi từ PDF sang TXT  
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reasoning/2310.01798.pdf
# Kích thước file: 385585 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
CÁC MÔ HÌNH NGÔN NGỮ LỚN CHƯA THỂ TỰ HIỆU CHỈNH
LẬP LUẬN
Jie Huang1,2∗Xinyun Chen1∗Swaroop Mishra1Huaixiu Steven Zheng1Adams Wei Yu1
Xinying Song1Denny Zhou1
1Google DeepMind2University of Illinois at Urbana-Champaign
jeffhj@illinois.edu, {xinyunchen, dennyzhou }@google.com
TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLMs) đã nổi lên như một công nghệ đột phá với khả năng tạo văn bản tuyệt vời trên nhiều ứng dụng khác nhau. Tuy nhiên, vẫn còn những lo ngại về tính chính xác và phù hợp của nội dung được tạo ra. Một phương pháp luận đương đại, tự hiệu chỉnh, đã được đề xuất như một biện pháp khắc phục những vấn đề này. Dựa trên tiền đề này, bài báo này xem xét một cách phê phán vai trò và hiệu quả của tự hiệu chỉnh trong LLMs, làm sáng tỏ tiềm năng thực sự và những hạn chế của nó. Trọng tâm nghiên cứu của chúng tôi là khái niệm tự hiệu chỉnh nội tại, theo đó một LLM cố gắng hiệu chỉnh các phản hồi ban đầu của mình chỉ dựa trên khả năng vốn có, mà không cần phụ thuộc vào phản hồi bên ngoài. Trong bối cảnh lập luận, nghiên cứu của chúng tôi chỉ ra rằng LLMs gặp khó khăn trong việc tự hiệu chỉnh phản hồi mà không có phản hồi bên ngoài, và đôi khi, hiệu suất của chúng thậm chí còn giảm sau khi tự hiệu chỉnh. Dựa trên những hiểu biết này, chúng tôi đưa ra các gợi ý cho nghiên cứu tương lai và ứng dụng thực tế trong lĩnh vực này.

1 GIỚI THIỆU
Những tiến bộ nhanh chóng trong lĩnh vực trí tuệ nhân tạo đã mở ra kỷ nguyên của các Mô hình Ngôn ngữ Lớn (LLMs). Những mô hình này, được đặc trưng bởi số lượng tham số mở rộng và khả năng tạo văn bản tuyệt vời, đã thể hiện kết quả đầy hứa hẹn trên nhiều ứng dụng (Chowdhery et al., 2023; Anil et al., 2023; OpenAI, 2023, trong số những cái khác). Tuy nhiên, những lo ngại về tính chính xác, khả năng lập luận và tính an toàn của nội dung được tạo ra đã thu hút sự chú ý đáng kể từ cộng đồng (Bang et al., 2023; Alkaissi & McFarlane, 2023; Zheng et al., 2023; Shi et al., 2023; Carlini et al., 2021; Huang et al., 2022; Shao et al., 2023; Li et al., 2023; Wei et al., 2023; Zhou et al., 2023b; Zou et al., 2023, trong số những cái khác).

Trong bối cảnh này, khái niệm "tự hiệu chỉnh" đã nổi lên như một giải pháp đầy hứa hẹn, nơi LLMs tinh chỉnh phản hồi của chúng dựa trên phản hồi với đầu ra trước đó (Madaan et al., 2023; Welleck et al., 2023; Shinn et al., 2023; Kim et al., 2023; Bai et al., 2022; Ganguli et al., 2023; Gao et al., 2023; Paul et al., 2023; Chen et al., 2023b; Pan et al., 2023, trong số những cái khác). Tuy nhiên, cơ chế cơ bản và hiệu quả của tự hiệu chỉnh trong LLMs vẫn chưa được khám phá đầy đủ. Một câu hỏi cơ bản nảy sinh: Nếu một LLM có khả năng tự hiệu chỉnh, tại sao nó không đơn giản đưa ra câu trả lời đúng ngay từ lần thử đầu tiên? Bài báo này đi sâu vào nghịch lý này, xem xét một cách phê phán khả năng tự hiệu chỉnh của LLMs, với sự nhấn mạnh đặc biệt vào lập luận (Wei et al., 2022; Zhou et al., 2023b; Huang & Chang, 2023).

Để nghiên cứu điều này, trước tiên chúng tôi định nghĩa khái niệm tự hiệu chỉnh nội tại, một tình huống trong đó mô hình cố gắng sửa chữa các phản hồi ban đầu của mình chỉ dựa trên khả năng vốn có, mà không cần phụ thuộc vào phản hồi bên ngoài. Một thiết lập như vậy là quan trọng vì phản hồi bên ngoài chất lượng cao thường không có sẵn trong nhiều ứng dụng thực tế. Hơn nữa, điều quan trọng là phải hiểu khả năng nội tại của LLMs. Trái ngược với sự lạc quan xung quanh tự hiệu chỉnh (Madaan et al., 2023; Kim et al., 2023; Shinn et al., 2023; Pan et al., 2023, trong số những cái khác), những phát hiện của chúng tôi chỉ ra rằng LLMs gặp khó khăn trong việc tự hiệu chỉnh lập luận trong thiết lập này. Trong hầu hết các trường hợp, hiệu suất sau khi tự hiệu chỉnh
∗Đóng góp ngang nhau.
1arXiv:2310.01798v2 [cs.CL] 14 Mar 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
thậm chí còn giảm. Quan sát này trái ngược với nghiên cứu trước đây như Kim et al. (2023); Shinn et al. (2023). Khi xem xét kỹ hơn, chúng tôi quan sát thấy rằng những cải thiện trong các nghiên cứu này là kết quả của việc sử dụng nhãn oracle để hướng dẫn quá trình tự hiệu chỉnh, và những cải thiện biến mất khi nhãn oracle không có sẵn.

Bên cạnh việc phụ thuộc vào nhãn oracle, chúng tôi cũng xác định các vấn đề khác trong tài liệu về việc đo lường cải thiện đạt được bởi tự hiệu chỉnh. Đầu tiên, chúng tôi lưu ý rằng tự hiệu chỉnh, theo thiết kế, sử dụng nhiều phản hồi LLM, do đó làm cho việc so sánh nó với các đường cơ sở có chi phí suy luận tương đương trở nên quan trọng. Từ góc độ này, chúng tôi điều tra tranh luận đa tác nhân (Du et al., 2023; Liang et al., 2023) như một phương tiện để cải thiện lập luận, nơi nhiều phiên bản LLM (có thể là nhiều bản sao của cùng một LLM) phê phán phản hồi của nhau. Tuy nhiên, kết quả của chúng tôi tiết lộ rằng hiệu quả của nó không tốt hơn tính nhất quán tự thân (Wang et al., 2022) khi xem xét số lượng phản hồi tương đương, làm nổi bật những hạn chế của cách tiếp cận như vậy.

Một cân nhắc quan trọng khác cho tự hiệu chỉnh liên quan đến thiết kế prompt. Cụ thể, mỗi quá trình tự hiệu chỉnh liên quan đến việc thiết kế prompt cho cả việc tạo phản hồi ban đầu và các bước tự hiệu chỉnh. Đánh giá của chúng tôi tiết lộ rằng cải thiện tự hiệu chỉnh được tuyên bố bởi một số công trình hiện có xuất phát từ prompt không tối ưu để tạo phản hồi ban đầu, nơi tự hiệu chỉnh sửa chữa những phản hồi này với các hướng dẫn nhiều thông tin hơn về nhiệm vụ ban đầu trong prompt phản hồi. Trong những trường hợp như vậy, việc đơn giản tích hợp phản hồi vào hướng dẫn ban đầu có thể mang lại kết quả tốt hơn, và tự hiệu chỉnh lại làm giảm hiệu suất.

Dựa trên những phát hiện của chúng tôi, chúng tôi cung cấp những hiểu biết về các sắc thái của khả năng tự hiệu chỉnh của LLMs và khởi xướng các cuộc thảo luận để khuyến khích nghiên cứu tương lai tập trung vào việc khám phá các phương pháp có thể thực sự hiệu chỉnh lập luận.

2 BỐI CẢNH VÀ CÔNG TRÌNH LIÊN QUAN
Với sự phát triển của LLM, khái niệm tự hiệu chỉnh đã trở nên nổi bật. Cuộc thảo luận về tự hiệu chỉnh xoay quanh việc liệu những mô hình tiên tiến này có thể nhận ra tính đúng đắn của đầu ra của chúng và cung cấp câu trả lời tinh chỉnh (Bai et al., 2022; Madaan et al., 2023; Welleck et al., 2023, trong số những cái khác). Ví dụ, trong bối cảnh lập luận toán học, một LLM có thể ban đầu giải một bài toán phức tạp nhưng mắc lỗi trong một trong các bước tính toán. Trong một tình huống tự hiệu chỉnh lý tưởng, mô hình được mong đợi sẽ nhận ra lỗi tiềm ẩn, xem xét lại bài toán, sửa lỗi, và do đó tạo ra một giải pháp chính xác hơn.

Tuy nhiên, định nghĩa về "tự hiệu chỉnh" khác nhau trong tài liệu, dẫn đến sự mơ hồ. Một sự phân biệt then chốt nằm ở nguồn phản hồi (Pan et al., 2023), có thể hoàn toàn đến từ LLM, hoặc có thể được rút ra từ các đầu vào bên ngoài. Phản hồi nội bộ dựa vào kiến thức và tham số vốn có của mô hình để đánh giá lại đầu ra của nó. Ngược lại, phản hồi bên ngoài kết hợp đầu vào từ con người, các mô hình khác (Wang et al., 2023b; Paul et al., 2023, trong số những cái khác), hoặc các công cụ và nguồn kiến thức bên ngoài (Gou et al., 2023; Chen et al., 2023b; Olausson et al., 2023; Gao et al., 2023, trong số những cái khác).

Trong công trình này, chúng tôi tập trung vào việc kiểm tra khả năng tự hiệu chỉnh của LLMs cho lập luận. Lập luận là một khía cạnh cơ bản của nhận thức con người, cho phép chúng ta hiểu thế giới, rút ra suy luận, đưa ra quyết định và giải quyết vấn đề. Để nâng cao hiệu suất lập luận của LLMs, Kim et al. (2023); Shinn et al. (2023) sử dụng nhãn oracle về tính đúng đắn của câu trả lời để hướng dẫn quá trình tự hiệu chỉnh. Tuy nhiên, trong thực tế, phản hồi bên ngoài chất lượng cao như tính đúng đắn của câu trả lời thường không có sẵn. Để tự hiệu chỉnh hiệu quả, khả năng đánh giá tính đúng đắn của một câu trả lời là quan trọng và lý tưởng nhất là nên được thực hiện bởi chính LLM. Do đó, sự tập trung của chúng tôi chuyển sang tự hiệu chỉnh mà không có bất kỳ phản hồi bên ngoài hoặc con người nào. Chúng tôi gọi thiết lập này là tự hiệu chỉnh nội tại. Để ngắn gọn, trừ khi được nêu rõ khác (ví dụ, tự hiệu chỉnh với phản hồi oracle), tất cả các tham chiếu đến "tự hiệu chỉnh" trong phần còn lại của bài báo này đề cập đến tự hiệu chỉnh nội tại.

Trong các phần sau, chúng tôi sẽ đánh giá một loạt các kỹ thuật tự hiệu chỉnh hiện có. Chúng tôi chứng minh rằng các kỹ thuật hiện có thực sự làm giảm hiệu suất lập luận khi nhãn oracle không được sử dụng (Phần 3), thực hiện tệ hơn các phương pháp không có tự hiệu chỉnh khi sử dụng cùng số lượng phản hồi mô hình (Phần 4), và dẫn đến kết quả ít hiệu quả hơn khi sử dụng prompt có nhiều thông tin để tạo phản hồi ban đầu (Phần 5). Chúng tôi trình bày tổng quan về các vấn đề trong thiết lập đánh giá của các công trình tự hiệu chỉnh LLM trước đây trong Bảng 1, với các cuộc thảo luận chi tiết trong các phần tương ứng.
2

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 1: Tóm tắt các vấn đề trong đánh giá tự hiệu chỉnh LLM trước đây.
Phương pháp Vấn đề
RCI (Kim et al., 2023); Reflexion (Shinn et al., 2023) Sử dụng nhãn oracle (Phần 3)
Multi-Agent Debate (Du et al., 2023) So sánh không công bằng với tính nhất quán tự thân (Phần 4)
Self-Refine (Madaan et al., 2023) Thiết kế prompt không tối ưu (Phần 5)

3 LLM KHÔNG THỂ TỰ HIỆU CHỈNH LẬP LUẬN NỘI TẠI
Trong phần này, chúng tôi đánh giá các phương pháp tự hiệu chỉnh hiện có và so sánh hiệu suất của chúng có và không có nhãn oracle về tính đúng đắn của câu trả lời.

3.1 THIẾT LẬP THỰC NGHIỆM
Benchmark. Chúng tôi sử dụng các bộ dữ liệu mà các phương pháp tự hiệu chỉnh hiện có với nhãn oracle đã chứng minh cải thiện hiệu suất đáng kể, bao gồm
•GSM8K (Cobbe et al., 2021): GSM8K bao gồm một tập kiểm tra 1,319 bài toán từ đa dạng ngôn ngữ học về toán cấp tiểu học, được xây dựng bởi những người viết bài toán con người. Có một cải thiện đáng chú ý khoảng 7% như được chứng minh bởi Kim et al. (2023) sau khi tự hiệu chỉnh.
•CommonSenseQA (Talmor et al., 2019): Bộ dữ liệu này cung cấp một tập hợp các câu hỏi trắc nghiệm kiểm tra lập luận thông thường. Một sự gia tăng ấn tượng khoảng 15% được thể hiện thông qua quá trình tự hiệu chỉnh, như được chứng minh bởi Kim et al. (2023). Theo Kojima et al. (2022); Kim et al. (2023), chúng tôi sử dụng tập dev cho đánh giá của chúng tôi, bao gồm 1,221 câu hỏi.
•HotpotQA (Yang et al., 2018): HotpotQA là một bộ dữ liệu trả lời câu hỏi đa bước miền mở. Shinn et al. (2023) chứng minh cải thiện hiệu suất đáng kể thông qua tự hiệu chỉnh. Chúng tôi kiểm tra hiệu suất của các mô hình trong thiết lập closed-book và đánh giá chúng bằng cùng một tập như Shinn et al. (2023). Tập này chứa 100 câu hỏi, với exact match phục vụ như thước đo đánh giá.

Mô hình Kiểm tra và Thiết lập. Chúng tôi trước tiên theo Kim et al. (2023); Shinn et al. (2023) để đánh giá hiệu suất của tự hiệu chỉnh với nhãn oracle, sử dụng GPT-3.5-Turbo (gpt-3.5-turbo-0613) và GPT-4 được truy cập vào 29/08/2023. Đối với tự hiệu chỉnh nội tại, để cung cấp một phân tích toàn diện hơn, chúng tôi cũng đánh giá GPT-4-Turbo (gpt-4-1106-preview) và Llama-2 (Llama-2-70b-chat) (Touvron et al., 2023). Đối với GPT-3.5-Turbo, chúng tôi sử dụng toàn bộ tập đánh giá. Đối với các mô hình khác, để giảm chi phí, chúng tôi lấy mẫu ngẫu nhiên 200 câu hỏi cho mỗi bộ dữ liệu (100 cho HotpotQA) để kiểm tra. Chúng tôi nhắc các mô hình trải qua tối đa hai vòng tự hiệu chỉnh. Chúng tôi sử dụng nhiệt độ 1 cho GPT-3.5-Turbo và GPT-4, và nhiệt độ 0 cho GPT-4-Turbo và Llama-2, để cung cấp đánh giá trên các thuật toán giải mã khác nhau.

Prompt. Theo Kim et al. (2023); Shinn et al. (2023), chúng tôi áp dụng chiến lược prompting ba bước cho tự hiệu chỉnh: 1) nhắc mô hình thực hiện một tạo ban đầu (cũng phục vụ như kết quả cho Standard Prompting); 2) nhắc mô hình xem xét tạo trước đó của nó và tạo phản hồi; 3) nhắc mô hình trả lời câu hỏi gốc một lần nữa với phản hồi.

Đối với các thực nghiệm của chúng tôi, chúng tôi chủ yếu tuân thủ các prompt từ các bài báo nguồn. Đối với GSM8K và CommonSenseQA, chúng tôi tích hợp hướng dẫn định dạng vào các prompt của Kim et al. (2023) để tạo điều kiện cho một đánh giá tự động chính xác hơn (các prompt chi tiết có thể được tìm thấy trong Phụ lục A). Đối với HotpotQA, chúng tôi sử dụng cùng prompt như Shinn et al. (2023). Chúng tôi cũng đánh giá hiệu suất của các prompt tự hiệu chỉnh khác nhau cho tự hiệu chỉnh nội tại. Ví dụ, chúng tôi sử dụng "Giả sử rằng câu trả lời này có thể đúng hoặc sai. Xem xét câu trả lời một cách cẩn thận và báo cáo bất kỳ vấn đề nghiêm trọng nào bạn tìm thấy." như prompt phản hồi mặc định cho đánh giá trên GPT-4-Turbo và Llama-2.

3.2 KẾT QUẢ
Tự Hiệu chỉnh với Nhãn Oracle. Theo các công trình trước đây (Kim et al., 2023; Shinn et al., 2023), chúng tôi sử dụng nhãn đúng để xác định khi nào dừng vòng lặp tự hiệu chỉnh. Điều này có nghĩa là chúng tôi
3

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 2: Kết quả của GPT-3.5 và GPT-4 trên các benchmark lập luận với nhãn oracle.
GSM8K CommonSenseQA HotpotQA
GPT-3.5Standard Prompting 75.9 75.8 26.0
Self-Correct (Oracle) 84.3 89.7 29.0
GPT-4Standard Prompting 95.5 82.0 49.0
Self-Correct (Oracle) 97.5 85.5 59.0

Bảng 3: Kết quả của GPT-3.5 và GPT-4 trên các benchmark lập luận với tự hiệu chỉnh nội tại.
# calls GSM8K CommonSenseQA HotpotQA
GPT-3.5Standard Prompting 1 75.9 75.8 26.0
Self-Correct (round 1) 3 75.1 38.1 25.0
Self-Correct (round 2) 5 74.7 41.8 25.0
GPT-4Standard Prompting 1 95.5 82.0 49.0
Self-Correct (round 1) 3 91.5 79.5 49.0
Self-Correct (round 2) 5 89.0 80.0 43.0

sử dụng nhãn ground-truth để xác minh liệu câu trả lời được tạo ở mỗi bước có đúng không. Nếu câu trả lời đã đúng, không có (thêm) tự hiệu chỉnh sẽ được thực hiện. Bảng 2 tóm tắt kết quả của tự hiệu chỉnh dưới thiết lập này, thể hiện cải thiện hiệu suất đáng kể, phù hợp với những phát hiện được trình bày trong Kim et al. (2023); Shinn et al. (2023).

Tuy nhiên, những kết quả này cần được xem xét cẩn thận. Đối với các nhiệm vụ lập luận, như giải các bài toán toán học, sự có sẵn của nhãn oracle dường như phản trực giác. Nếu chúng ta đã sở hữu ground truth, dường như có ít lý do để triển khai LLMs để giải quyết vấn đề. Do đó, kết quả chỉ có thể được coi là chỉ ra hiệu suất của một oracle.

Tự Hiệu chỉnh Nội tại. Theo cuộc thảo luận trên, cải thiện hiệu suất đạt được bằng cách sử dụng nhãn oracle không nhất thiết phản ánh khả năng tự hiệu chỉnh thực sự. Do đó, chúng tôi tập trung vào kết quả trong thiết lập tự hiệu chỉnh nội tại như được định nghĩa trong Phần 2. Để đạt được điều này, chúng tôi loại bỏ việc sử dụng nhãn, yêu cầu LLMs tự xác định khi nào dừng quá trình tự hiệu chỉnh, tức là liệu có nên giữ lại câu trả lời trước đó của chúng.

Bảng 3 và 4 báo cáo độ chính xác và số lượng lần gọi mô hình. Chúng tôi quan sát thấy rằng, sau khi tự hiệu chỉnh, độ chính xác của tất cả các mô hình giảm trên tất cả các benchmark.

Để cung cấp một đánh giá toàn diện hơn, chúng tôi cũng thiết kế một số prompt tự hiệu chỉnh khác nhau để xác định liệu có prompt tốt hơn có thể nâng cao hiệu suất lập luận không. Tuy nhiên, như được thể hiện trong Bảng 5 và 6, mà không sử dụng nhãn oracle, tự hiệu chỉnh luôn dẫn đến giảm hiệu suất.

3.3 TẠI SAO HIỆU SUẤT KHÔNG TĂNG, MÀ THAY VÀO ĐÓ LẠI GIẢM?
Phân tích Thực nghiệm. Hình 1 tóm tắt kết quả của những thay đổi trong câu trả lời sau hai vòng tự hiệu chỉnh, với hai ví dụ của GPT-3.5 được minh họa trong Hình 2. Đối với GSM8K, 74.7% thời gian, GPT-3.5 giữ nguyên câu trả lời ban đầu của nó. Trong số các trường hợp còn lại, mô hình có nhiều khả năng sửa đổi một câu trả lời đúng thành sai hơn là sửa một câu trả lời sai thành đúng. Vấn đề cơ bản là LLMs không thể đánh giá đúng tính chính xác của lập luận của chúng. Đối với CommonSenseQA, có khả năng cao hơn rằng GPT-3.5 thay đổi câu trả lời của nó. Lý do chính cho điều này là các tùy chọn trả lời sai trong CommonSenseQA thường xuất hiện có phần liên quan đến câu hỏi, và việc sử dụng prompt tự hiệu chỉnh có thể thiên vị mô hình để chọn một tùy chọn khác, dẫn đến tỷ lệ "đúng ⇒sai" cao. Tương tự, Llama-2 cũng thường chuyển đổi một câu trả lời đúng thành sai. So với GPT-3.5 và Llama-2, cả GPT-4 và GPT-4-Turbo đều có xu hướng giữ nguyên câu trả lời ban đầu của chúng. Điều này có thể là do GPT-4 và GPT-4-Turbo có sự tự tin cao hơn
4

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 4: Kết quả của GPT-4-Turbo và Llama-2 với tự hiệu chỉnh nội tại.
# calls GSM8K CommonSenseQA
GPT-4-TurboStandard Prompting 1 91.5 84.0
Self-Correct (round 1) 3 88.0 81.5
Self-Correct (round 2) 5 90.0 83.0
Llama-2Standard Prompting 1 62.0 64.0
Self-Correct (round 1) 3 43.5 37.5
Self-Correct (round 2) 5 36.5 36.5

Bảng 5: Kết quả của GPT-4-Turbo với các prompt phản hồi khác nhau.
# calls GSM8K CommonSenseQA
Standard Prompting 1 91.5 84.0
Feedback Prompt: Giả sử rằng câu trả lời này có thể đúng hoặc sai.
Xem xét câu trả lời một cách cẩn thận và báo cáo bất kỳ vấn đề nghiêm trọng nào bạn tìm thấy.
Self-Correct (round 1) 3 88.0 81.5
Self-Correct (round 2) 5 90.0 83.0
Feedback Prompt: Xem xét câu trả lời trước đó của bạn và xác định liệu nó có đúng không.
Nếu sai, hãy tìm các vấn đề với câu trả lời của bạn.
Self-Correct (round 1) 3 90.0 74.5
Self-Correct (round 2) 5 90.0 81.0
Feedback Prompt: Xác minh liệu câu trả lời của bạn có đúng không, và cung cấp một giải thích.
Self-Correct (round 1) 3 91.0 81.5
Self-Correct (round 2) 5 91.0 83.5

Bảng 6: Kết quả của Llama-2 với các prompt phản hồi khác nhau.
# calls GSM8K CommonSenseQA
Standard Prompting 1 62.0 64.0
Feedback Prompt: Giả sử rằng câu trả lời này có thể đúng hoặc sai.
Xem xét câu trả lời một cách cẩn thận và báo cáo bất kỳ vấn đề nghiêm trọng nào bạn tìm thấy.
Self-Correct (round 1) 3 43.5 37.5
Self-Correct (round 2) 5 36.5 36.5
Feedback Prompt: Xem xét câu trả lời trước đó của bạn và xác định liệu nó có đúng không.
Nếu sai, hãy tìm các vấn đề với câu trả lời của bạn.
Self-Correct (round 1) 3 46.5 26.0
Self-Correct (round 2) 5 30.5 37.0
Feedback Prompt: Xác minh liệu câu trả lời của bạn có đúng không, và cung cấp một giải thích.
Self-Correct (round 1) 3 58.0 24.0
Self-Correct (round 2) 5 41.5 43.0

trong câu trả lời ban đầu của chúng, hoặc vì chúng mạnh mẽ hơn và do đó ít bị thiên vị bởi prompt tự hiệu chỉnh.1

1Chúng tôi bỏ qua phân tích về HotpotQA vì kích thước mẫu được sử dụng trong bài báo nguồn khá nhỏ, có thể không tạo ra thống kê có ý nghĩa.
5

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
GSM8K (GPT-3.5)Không Thay đổi74.7%Sai  Sai
8.9%Sai  Đúng
7.6%
Đúng  Sai
8.8%
CommonSenseQA (GPT-3.5)42.8%11.6%5.8%39.8%
GSM8K (GPT-4)90.5%1.5%
8.0%
CommonSenseQA (GPT-4)90.5%1.5%3.0%
5.0%
GSM8K (GPT-4-Turbo)96.0%0.5%1.0%2.5%
CommonSenseQA (GPT-4-Turbo)88.0%1.0%5.0%
6.0%
GSM8K (Llama-2)40.0%23.5%5.5% 31.0%
CommonSenseQA (Llama-2)40.0%16.5%8.0%35.5%
Hình 1: Phân tích về những thay đổi trong câu trả lời sau hai vòng tự hiệu chỉnh. Không Thay đổi: Câu trả lời vẫn không thay đổi; Đúng ⇒Sai: Một câu trả lời đúng được thay đổi thành sai; Sai ⇒Đúng: Một câu trả lời sai được sửa thành đúng; Sai ⇒Sai: Một câu trả lời sai được thay đổi nhưng vẫn sai.

Terry ăn 2 hộp sữa chua mỗi ngày. Hiện tại chúng đang được bán với giá 4 hộp với $5.00. Anh ấy sẽ chi bao nhiêu cho sữa chua trong 30 ngày?... Vậy, Terry chi $\boxed{75.00}$ cho sữa chua trong 30 ngày. (Đúng)Xem xét câu trả lời trước đó của bạn và tìm vấn đề với câu trả lời của bạn.Khi xem xét câu trả lời trước đó của tôi, tôi nhận ra rằng tôi đã mắc lỗi trong việc tính toán chi tiêu của Terry cho sữa chua trong 30 ngày. Tôi đã nói sai rằng Terry chi $2.50 mỗi ngày trong 30 ngày, dẫn đến tổng cộng $75.00…Dựa trên các vấn đề bạn tìm thấy, cải thiện câu trả lời của bạn.... Do đó, câu trả lời cuối cùng là Terry chi $\boxed{37.50}$ cho sữa chua trong 30 ngày. (Sai)Christina đang lên kế hoạch cho một bữa tiệc sinh nhật và cần .75 túi quà cho mỗi khách được mời, vì 1/4 số người tham dự không đến. Cô ấy mời 16 bạn. Túi quà có giá $2 mỗi cái. Cô ấy sẽ chi bao nhiêu?... Do đó, Christina sẽ chi $18 cho túi quà. Câu trả lời: \boxed{18}. (Sai)Xem xét câu trả lời trước đó của bạn và tìm vấn đề với câu trả lời của bạn.Khi xem xét câu trả lời trước đó của tôi, tôi nhận ra một lỗi. Tôi đã nói sai rằng Christina cần 0.75 túi quà cho mỗi khách được mời. Thông tin đã cho nói rằng 1/4 số người tham dự sẽ không đến…Dựa trên các vấn đề bạn tìm thấy, cải thiện câu trả lời của bạn.... Do đó, Christina sẽ chi $24 cho túi quà. Câu trả lời: \boxed{24}. (Đúng)

Hình 2: Ví dụ về GSM8K với GPT-3.5. Trái: tự hiệu chỉnh thành công; Phải: tự hiệu chỉnh thất bại. Prompt và phản hồi đầy đủ có thể xem trong Hình 3 và 4 của Phụ lục A.

Hãy xem lại kết quả được trình bày trong Bảng 2. Những kết quả này sử dụng nhãn ground-truth để ngăn mô hình thay đổi một câu trả lời đúng thành sai. Tuy nhiên, việc xác định cách ngăn chặn những thay đổi sai lầm như vậy thực tế là chìa khóa để đảm bảo sự thành công của tự hiệu chỉnh.

Giải thích Trực quan. Nếu mô hình được liên kết tốt và được kết hợp với một prompt ban đầu được thiết kế chu đáo, phản hồi ban đầu đã nên tối ưu tương đối so với prompt và thuật toán giải mã cụ thể. Giới thiệu phản hồi có thể được xem như việc thêm một prompt bổ sung, có thể làm lệch mô hình về phía tạo ra một phản hồi được điều chỉnh cho đầu vào kết hợp này. Trong thiết lập tự hiệu chỉnh nội tại, đối với các nhiệm vụ lập luận, prompt bổ sung này có thể không cung cấp bất kỳ lợi thế bổ sung nào để trả lời câu hỏi. Thực tế, nó thậm chí có thể thiên vị mô hình khỏi việc tạo ra một phản hồi tối ưu cho prompt ban đầu, dẫn đến sự sụt giảm hiệu suất.
6

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 7: Kết quả của tranh luận đa tác nhân và tính nhất quán tự thân.
# responses GSM8K
Standard Prompting 1 76.7
Self-Consistency 3 82.5
Multi-Agent Debate (round 1) 6 83.2
Self-Consistency 6 85.3
Multi-Agent Debate (round 2) 9 83.0
Self-Consistency 9 88.2

Bảng 8: Kết quả của Constrained Generation.
# calls CommonGen-Hard
Standard Prompting* 1 44.0*
Self-Correct* 7 67.0*
Standard Prompting* 1 53.0
Self-Correct* 7 61.1
Standard Prompting (của chúng tôi) 1 81.8
Self-Correct* 7 75.1
* Prompt và kết quả từ Madaan et al. (2023).

4 TRANH LUẬN ĐA TÁC NHÂN KHÔNG VƯỢT TRỘI HỠN TÍNH NHẤT QUÁN TỰ THÂN
Một cách tiếp cận tiềm năng khác để LLMs tự hiệu chỉnh lập luận của chúng liên quan đến việc cho phép các mô hình phê phán và tranh luận thông qua nhiều lần gọi mô hình (Du et al., 2023; Liang et al., 2023; Chen et al., 2023a). Du et al. (2023) triển khai một phương pháp tranh luận đa tác nhân bằng cách tận dụng nhiều phiên bản của một mô hình ChatGPT duy nhất và chứng minh cải thiện đáng kể trên các nhiệm vụ lập luận. Chúng tôi áp dụng phương pháp của họ để kiểm tra hiệu suất trên GSM8K. Để có một triển khai không thiên vị, chúng tôi sử dụng chính xác cùng prompt như Du et al. (2023) và sao chép thí nghiệm của họ với mô hình gpt-3.5-turbo-0301, kết hợp 3 tác nhân và 2 vòng tranh luận. Sự khác biệt duy nhất là, để giảm sự biến thiên kết quả, chúng tôi kiểm tra trên toàn bộ tập kiểm tra của GSM8K, so với việc sử dụng 100 ví dụ của họ. Để tham khảo, chúng tôi cũng báo cáo kết quả của tính nhất quán tự thân (Wang et al., 2022), nhắc các mô hình tạo ra nhiều phản hồi và thực hiện bỏ phiếu đa số để chọn câu trả lời cuối cùng.

Bảng 7 trình bày kết quả. Kết quả chỉ ra rằng cả tranh luận đa tác nhân và tính nhất quán tự thân đều đạt được cải thiện đáng kể so với prompting tiêu chuẩn. Tuy nhiên, khi so sánh tranh luận đa tác nhân với tính nhất quán tự thân, chúng tôi quan sát thấy rằng hiệu suất của đa tác nhân chỉ tốt hơn một chút so với tính nhất quán tự thân với cùng số lượng tác nhân (3 phản hồi, đường cơ sở cũng được so sánh trong Du et al. (2023)). Hơn nữa, đối với tính nhất quán tự thân với số lượng phản hồi tương đương, tranh luận đa tác nhân hoạt động kém hơn đáng kể so với tính nhất quán tự thân đơn giản sử dụng bỏ phiếu đa số.

Thực tế, thay vì gọi tranh luận đa tác nhân là một hình thức "tranh luận" hoặc "phê phán", việc nhận thức nó như một phương tiện để đạt được "tính nhất quán" trên nhiều thế hệ mô hình là phù hợp hơn. Về cơ bản, khái niệm của nó phản ánh tính nhất quán tự thân; sự khác biệt nằm ở cơ chế bỏ phiếu, liệu bỏ phiếu được điều khiển bởi mô hình hay hoàn toàn dựa trên số lượng. Cải thiện quan sát được rõ ràng không được quy cho "tự hiệu chỉnh", mà là "tính nhất quán tự thân". Nếu chúng ta muốn lập luận rằng LLMs có thể tự hiệu chỉnh lập luận thông qua tranh luận đa tác nhân, tốt hơn là loại trừ các hiệu ứng của việc lựa chọn giữa nhiều thế hệ.

5 CÁC VẤN ĐỀ THIẾT KẾ PROMPT TRONG ĐÁNH GIÁ TỰ HIỆU CHỈNH
Trong Phần 3, chúng tôi quan sát thấy rằng mặc dù tự hiệu chỉnh làm giảm hiệu suất lập luận với tất cả các loại prompt phản hồi chúng tôi đã đánh giá, hiệu suất khác nhau với các prompt phản hồi khác nhau. Trong phần này, chúng tôi tiếp tục nhấn mạnh tầm quan trọng của thiết kế prompt thích hợp trong việc tạo phản hồi LLM ban đầu để đo lường một cách công bằng cải thiện hiệu suất đạt được bởi tự hiệu chỉnh. Ví dụ, nếu một nhiệm vụ yêu cầu rằng phản hồi mô hình nên đáp ứng các tiêu chí có thể được chỉ định dễ dàng trong hướng dẫn ban đầu (ví dụ, đầu ra nên chứa những từ nhất định, mã được tạo nên hiệu quả, tình cảm nên tích cực, v.v.), thay vì chỉ bao gồm những yêu cầu như vậy trong prompt phản hồi, một so sánh phù hợp sẽ là tích hợp trực tiếp và rõ ràng những yêu cầu này vào prompt để tạo phản hồi ban đầu. Nếu không, khi hướng dẫn để tạo dự đoán ban đầu không đủ thông tin, ngay cả khi hiệu suất cải thiện, vẫn không rõ liệu cải thiện chỉ đến từ hướng dẫn chi tiết hơn trong prompt phản hồi hay từ bước tự hiệu chỉnh.

Để minh họa những vấn đề thiết kế prompt như vậy trong đánh giá tự hiệu chỉnh của một số công trình trước đây, chúng tôi lấy nhiệm vụ Constrained Generation trong Madaan et al. (2023) làm ví dụ, nơi nhiệm vụ yêu cầu các mô hình tạo ra các câu mạch lạc sử dụng tất cả 20-30 khái niệm đầu vào. Prompt gốc trong Madaan et al. (2023) (Hình 7) không chỉ định rõ ràng rằng LLM cần bao gồm tất cả các khái niệm trong prompt; do đó, họ chỉ ra rằng tự hiệu chỉnh cải thiện hiệu suất nhiệm vụ bằng cách yêu cầu mô hình xác định các khái niệm bị thiếu và sau đó hướng dẫn nó kết hợp những khái niệm này thông qua phản hồi.

Dựa trên quan sát này, chúng tôi thêm hướng dẫn sau "Viết một đoạn văn hợp lý bao gồm *TẤT CẢ* các khái niệm trên" vào prompt để tạo phản hồi ban đầu (tham khảo Hình 8 để xem prompt đầy đủ). Theo Madaan et al. (2023), chúng tôi sử dụng concept coverage làm thước đo. Chúng tôi tham khảo kết quả của họ và sao chép thí nghiệm của họ bằng gpt-3.5-turbo-0613. Bảng 8 chứng minh rằng prompt mới của chúng tôi, được ký hiệu là Standard Prompting (của chúng tôi), vượt trội đáng kể so với kết quả sau khi tự hiệu chỉnh của Madaan et al. (2023), và việc áp dụng prompt tự hiệu chỉnh của họ lên trên phản hồi mô hình từ phiên bản mạnh hơn của prompting tiêu chuẩn lại dẫn đến giảm hiệu suất.

6 KẾT LUẬN VÀ THẢO LUẬN
Công trình của chúng tôi cho thấy rằng các LLMs hiện tại gặp khó khăn trong việc tự hiệu chỉnh lập luận của chúng mà không có phản hồi bên ngoài. Điều này ngụ ý rằng việc mong đợi những mô hình này vốn dĩ nhận ra và sửa chữa lỗi lập luận của chúng là quá lạc quan cho đến nay. Dựa trên những phát hiện này, điều cấp thiết là cộng đồng tiếp cận khái niệm tự hiệu chỉnh với một góc nhìn sáng suốt, thừa nhận tiềm năng của nó và nhận ra ranh giới của nó. Bằng cách làm như vậy, chúng ta có thể trang bị tốt hơn cho kỹ thuật tự hiệu chỉnh để giải quyết những hạn chế của LLMs và phát triển thế hệ LLMs tiếp theo với khả năng nâng cao. Trong phần sau, chúng tôi cung cấp những hiểu biết về các tình huống mà tự hiệu chỉnh thể hiện điểm mạnh tiềm năng và đưa ra hướng dẫn về thiết kế thực nghiệm của các kỹ thuật tự hiệu chỉnh tương lai để đảm bảo so sánh công bằng.

Tận dụng phản hồi bên ngoài để hiệu chỉnh. Trong công trình này, chúng tôi chứng minh rằng các LLMs hiện tại không thể cải thiện hiệu suất lập luận của chúng thông qua tự hiệu chỉnh nội tại. Do đó, khi phản hồi bên ngoài hợp lệ có sẵn, việc tận dụng nó một cách thích hợp để nâng cao hiệu suất mô hình là có lợi. Ví dụ, Chen et al. (2023b) cho thấy rằng LLMs có thể cải thiện đáng kể hiệu suất tạo mã của chúng thông qua tự gỡ lỗi bằng cách bao gồm kết quả thực thi mã trong prompt phản hồi để sửa chữa vấn đề trong mã được dự đoán. Đặc biệt, khi mô tả bài toán chỉ định rõ ràng hành vi thực thi mã dự định, ví dụ, với unit tests, trình thực thi mã phục vụ như trình xác minh hoàn hảo để đánh giá tính chính xác của các chương trình được dự đoán, trong khi các thông báo lỗi cũng cung cấp phản hồi thông tin hướng dẫn LLMs cải thiện phản hồi của chúng. Gou et al. (2023) chứng minh rằng LLMs có thể xác minh và hiệu chỉnh phản hồi của chúng hiệu quả hơn khi tương tác với các công cụ bên ngoài khác nhau như công cụ tìm kiếm và máy tính. Cobbe et al. (2021); Lightman et al. (2023); Wang et al. (2023b) huấn luyện một trình xác minh hoặc mô hình phê phán trên một bộ dữ liệu chất lượng cao để xác minh hoặc tinh chỉnh đầu ra LLM, có thể được sử dụng để cung cấp phản hồi để hiệu chỉnh lỗi dự đoán. Bên cạnh phản hồi bên ngoài được tạo tự động, chúng ta cũng thường cung cấp phản hồi khi tương tác với LLMs, hướng dẫn chúng tạo ra nội dung chúng ta mong muốn. Thiết kế các kỹ thuật cho phép LLMs tương tác với môi trường bên ngoài và học hỏi từ các loại phản hồi có sẵn khác nhau là một hướng nghiên cứu đầy hứa hẹn cho công trình tương lai.

Đánh giá tự hiệu chỉnh so với đường cơ sở với chi phí suy luận tương đương. Theo thiết kế, tự hiệu chỉnh yêu cầu thêm các lần gọi LLM, do đó tăng chi phí để mã hóa và tạo thêm token. Phần 4 chứng minh rằng hiệu suất của việc yêu cầu LLM tạo ra phản hồi cuối cùng dựa trên nhiều phản hồi trước đó, như với cách tiếp cận tranh luận đa tác nhân, kém hơn so với tính nhất quán tự thân (Wang et al., 2022) với cùng số lượng phản hồi. Về điều này, chúng tôi khuyến khích công trình tương lai đề xuất các phương pháp tự hiệu chỉnh mới luôn bao gồm phân tích chi phí suy luận chuyên sâu để chứng thực tuyên bố cải thiện hiệu suất. Hơn nữa, các đường cơ sở mạnh tận dụng nhiều phản hồi mô hình, như tính nhất quán tự thân, nên được sử dụng để so sánh.
Một hàm ý cho công trình tương lai là phát triển các mô hình có xác suất cao hơn trong việc giải mã giải pháp tối ưu trong phân bố câu trả lời của chúng, có thể thông qua một số kỹ thuật liên kết. Điều này sẽ cho phép mô hình tạo ra phản hồi tốt hơn mà không cần thiết phải có nhiều thế hệ.

Đầu tư nỗ lực ngang nhau vào thiết kế prompt. Như được thảo luận trong Phần 5, để hiểu rõ hơn về những cải thiện đạt được bởi tự hiệu chỉnh, điều quan trọng là bao gồm mô tả nhiệm vụ đầy đủ trong prompt để tạo phản hồi ban đầu, thay vì để lại một phần mô tả nhiệm vụ cho prompt phản hồi. Nói chung, nỗ lực ngang nhau nên được đầu tư vào việc thiết kế prompt cho tạo phản hồi ban đầu và cho tự hiệu chỉnh; nếu không, kết quả có thể gây hiểu lầm.

7 HẠN CHẾ VÀ TÁC ĐỘNG RỘNG HƠN
Mặc dù chúng tôi đã tiến hành đánh giá toàn diện trải rộng nhiều chiến lược tự hiệu chỉnh, prompt và benchmark khác nhau, công trình của chúng tôi tập trung vào việc đánh giá lập luận của LLMs. Do đó, có thể có những chiến lược tự hiệu chỉnh có thể nâng cao hiệu suất LLM trong các lĩnh vực khác. Ví dụ, các công trình trước đây đã chứng minh việc sử dụng thành công tự hiệu chỉnh liên kết phản hồi mô hình với các sở thích cụ thể, như thay đổi phong cách phản hồi hoặc nâng cao tính an toàn của chúng (Bai et al., 2022; Ganguli et al., 2023; Madaan et al., 2023). Một sự khác biệt chính nảy sinh trong khả năng của LLMs đánh giá chính xác phản hồi của chúng liên quan đến các nhiệm vụ đã cho. Ví dụ, LLMs có thể đánh giá đúng liệu một phản hồi có không phù hợp (Ganguli et al., 2023), nhưng chúng có thể gặp khó khăn trong việc xác định lỗi trong lập luận của chúng.

Hơn nữa, một số công trình trước đây đã cho thấy rằng hiệu suất tự hiệu chỉnh LLM trở nên yếu hơn đáng kể khi không có quyền truy cập vào phản hồi bên ngoài (Gou et al., 2023; Zhou et al., 2023a) và có thể dễ dàng bị thiên vị bởi phản hồi gây hiểu lầm (Wang et al., 2023a), điều này phù hợp với những phát hiện của chúng tôi trong công trình này. Tuy nhiên, chúng tôi vẫn xác định sự mơ hồ phổ biến trong cộng đồng rộng lớn hơn. Một số tài liệu hiện có có thể vô tình góp phần vào sự nhầm lẫn này, hoặc bằng cách đưa các chi tiết quan trọng về việc sử dụng nhãn vào các phần ít nổi bật hơn hoặc bằng cách không làm rõ rằng các chiến lược tự hiệu chỉnh được thiết kế của họ thực sự kết hợp phản hồi bên ngoài. Về điều này, bài báo của chúng tôi phục vụ như một lời kêu gọi hành động, thúc giục các nhà nghiên cứu tiếp cận lĩnh vực này với một góc nhìn sáng suốt và phê phán. Chúng tôi cũng khuyến khích nghiên cứu tương lai khám phá các cách tiếp cận có thể thực sự nâng cao lập luận.

TUYÊN BỐ TÍNH TÁI TẠO
Các thí nghiệm của chúng tôi sử dụng GPT-3.5 và GPT-4, có thể truy cập qua API công cộng tại https://platform.openai.com/docs/models, cũng như Llama-2, một mô hình mã nguồn mở. Để tạo điều kiện tái tạo, chúng tôi chi tiết các kernel cụ thể được sử dụng, ví dụ, gpt-3.5-turbo-0613, hoặc cung cấp thời gian truy cập cho mỗi thí nghiệm. Chúng tôi sử dụng prompt từ các công trình trước đây khi có thể. Đối với các prompt do chúng tôi thiết kế, chúng tôi bao gồm các prompt chính xác trong Phụ lục A.

LỜI CẢM ƠN
Chúng tôi muốn cảm ơn Chen Liang, William Cohen, Uri Alon, và các đồng nghiệp khác tại Google DeepMind vì những thảo luận và phản hồi quý báu.

TÀI LIỆU THAM KHẢO
Hussam Alkaissi and Samy I McFarlane. Artificial hallucinations in chatgpt: implications in scientific writing. Cureus, 15(2), 2023.

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.
9

--- TRANG 10 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 675–718, 2023.

Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In USENIX Security Symposium, volume 6, 2021.

Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. arXiv preprint arXiv:2309.13007, 2023a.

Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023b.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1–113, 2023. URL http://jmlr.org/papers/v24/22-1144.html.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.

Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.

Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023.

Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477–16508, 2023.

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023.

Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. In Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics, 2023.
10

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are large pre-trained language models leaking your personal information? In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 2038–2047, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.

Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. Advances in Neural Information Processing Systems, 2023.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022.

Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-step jailbreaking privacy attacks on chatgpt. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 4138–4153, 2023.

Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 2023.

Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. Demystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023.

OpenAI. Gpt-4 technical report, 2023.

Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023.

Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.

Hanyin Shao, Jie Huang, Shen Zheng, and Kevin Chen-Chuan Chang. Quantifying association capabilities of large language models and its implications on privacy leakage. arXiv preprint arXiv:2305.12707, 2023.

Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 31210–31227. PMLR, 2023.

Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 2023.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149–4158, 2019.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
11

--- TRANG 12 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Boshi Wang, Xiang Yue, and Huan Sun. Can chatgpt defend its belief in truth? evaluating llm reasoning via debate. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 11865–11881, 2023a.

Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O'Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A critic for language model generation. arXiv preprint arXiv:2308.04592, 2023b.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022.

Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv preprint arXiv:2307.02483, 2023.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.

Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations, 2023.

Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018.

Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. Why does chatgpt fall short in providing truthful answers? ArXiv preprint, abs/2304.10513, 2023.

Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2023b.

Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023.
12

--- TRANG 13 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
A PROMPT VÀ VÍ DỤ ĐẦU RA
Bạn có thể giải bài toán sau không? Christina đang lên kế hoạch cho một bữa tiệc sinh nhật và cần .75 túi quà cho mỗi khách được mời, vì 1/4 số người tham dự không đến. Cô ấy mời 16 bạn. Túi quà có giá $2 mỗi cái. Cô ấy sẽ chi bao nhiêu? Giải thích lý luận của bạn. Câu trả lời cuối cùng của bạn nên là một số duy nhất, trong dạng \boxed{answer}, ở cuối phản hồi của bạn.

Nếu Christina mời 16 bạn, thì 1/4 trong số họ sẽ không đến, có nghĩa là sẽ có 16 *1/4 = 4 bạn không tham dự bữa tiệc. Để xác định cần mua bao nhiêu túi quà, chúng ta cần trừ số bạn không đến khỏi tổng số khách được mời. Số khách được mời là 16 - 4 = 12. Vì Christina cần 0.75 túi quà cho mỗi khách được mời, chúng ta có thể nhân số lượng này với số khách được mời để tìm tổng số túi quà cần thiết, đó là 0.75*12 = 9 túi quà. Nếu mỗi túi quà có giá $2, thì Christina sẽ chi tổng cộng 2 *9 = $<<2 *9=18>>18 cho túi quà. Do đó, Christina sẽ chi $18 cho túi quà. Câu trả lời: \boxed{18}. (Sai)

Xem xét câu trả lời trước đó của bạn và tìm vấn đề với câu trả lời của bạn.
Khi xem xét câu trả lời trước đó của tôi, tôi nhận ra một lỗi. Tôi đã nói sai rằng Christina cần 0.75 túi quà cho mỗi khách được mời. Thông tin đã cho nói rằng 1/4 số người tham dự sẽ không đến, vì vậy tính toán đúng nên tính đến điều này. Để tính số túi quà cần thiết, chúng ta sẽ nhân số khách được mời với tỷ lệ người tham dự thực sự sẽ đến (75%). Số khách được mời là 16, và nếu 1/4 trong số họ không đến, thì tỷ lệ người tham dự sẽ đến là 1 - 1/4 = 3/4. Nhân số khách được mời với tỷ lệ người tham dự sẽ đến cho chúng ta: 16 * (3/4) = 12 túi quà cần thiết. Vì mỗi túi quà có giá $2, tổng số tiền Christina sẽ chi cho túi quà là: 12 *$2 = $<<12 *2=24>>24.

Dựa trên các vấn đề bạn tìm thấy, cải thiện câu trả lời của bạn. Vui lòng nhắc lại câu trả lời của bạn, với câu trả lời cuối cùng là một số duy nhất, trong dạng \boxed{answer}.
Khi xem xét câu trả lời trước đó của tôi, tôi nhận ra một lỗi. Tôi đã nói sai rằng Christina cần 0.75 túi quà cho mỗi khách được mời. Thông tin đã cho nói rằng 1/4 số người tham dự sẽ không đến, vì vậy tính toán đúng nên tính đến điều này. Để tính số túi quà cần thiết, chúng ta sẽ nhân số khách được mời với tỷ lệ người tham dự thực sự sẽ đến (75%). Số khách được mời là 16, và nếu 1/4 trong số họ không đến, thì tỷ lệ người tham dự sẽ đến là 1 - 1/4 = 3/4. Nhân số khách được mời với tỷ lệ người tham dự sẽ đến cho chúng ta: 16 * (3/4) = 12 túi quà cần thiết. Vì mỗi túi quà có giá $2, tổng số tiền Christina sẽ chi cho túi quà là: 12 *$2 = $<<12 *2=24>>24. Do đó, Christina sẽ chi $24 cho túi quà. Câu trả lời: \boxed{24}. (Đúng)

Hình 3: Ví dụ về GSM8K nơi tự hiệu chỉnh thay đổi một câu trả lời sai thành đúng. Văn bản in đậm biểu thị prompt của người dùng.
13

--- TRANG 14 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bạn có thể giải bài toán sau không? Terry ăn 2 hộp sữa chua mỗi ngày. Hiện tại chúng đang được bán với giá 4 hộp với $5.00. Anh ấy sẽ chi bao nhiêu cho sữa chua trong 30 ngày? Giải thích lý luận của bạn. Câu trả lời cuối cùng của bạn nên là một số duy nhất, trong dạng \boxed{answer}, ở cuối phản hồi của bạn.

Trước tiên, chúng ta cần tính Terry mua bao nhiêu bộ 4 hộp sữa chua trong 30 ngày. Vì Terry ăn 2 hộp sữa chua mỗi ngày, anh ấy sẽ cần 2/4 = 0.5 bộ 4 hộp sữa chua mỗi ngày. Tiếp theo, chúng ta nhân số bộ với giá của mỗi bộ để tính Terry chi bao nhiêu mỗi ngày. Mỗi bộ 4 hộp sữa chua có giá $5.00, vậy Terry chi 0.5 *$5.00 = $<<0.5 *5=2.50>>2.50 cho sữa chua mỗi ngày. Cuối cùng, chúng ta nhân số tiền Terry chi mỗi ngày với số ngày để tìm tổng số tiền anh ấy chi cho sữa chua trong 30 ngày. Terry chi $2.50 mỗi ngày trong 30 ngày, vậy anh ấy chi $2.50 *30 = $<<2.5*30=75.00>>75.00 cho sữa chua trong 30 ngày. Vậy, Terry chi $\boxed{75.00}$ cho sữa chua trong 30 ngày. (Đúng)

Xem xét câu trả lời trước đó của bạn và tìm vấn đề với câu trả lời của bạn.
Khi xem xét câu trả lời trước đó của tôi, tôi nhận ra rằng tôi đã mắc lỗi trong việc tính toán chi tiêu của Terry cho sữa chua trong 30 ngày. Tôi đã nói sai rằng Terry chi $2.50 mỗi ngày trong 30 ngày, dẫn đến tổng cộng $75.00. Tuy nhiên, vì Terry ăn 2 hộp sữa chua mỗi ngày và sữa chua được bán theo bộ 4 hộp với giá $5.00, Terry thực sự sẽ cần mua 1 bộ 4 hộp sữa chua để dùng trong 2 ngày. Do đó, chi tiêu của anh ấy cho sữa chua mỗi bộ sẽ là $5.00 trong 2 ngày, hoặc $5.00/2 = $2.50 mỗi ngày. Để tính tổng số tiền chi cho sữa chua trong 30 ngày, chúng ta sẽ nhân số tiền chi mỗi ngày ($2.50) với số bộ cần thiết trong 30 ngày (đó là 30 chia cho 2 ngày mỗi bộ). Do đó, tính toán đúng sẽ là $2.50 *(30/2) = $2.50 *15 = $37.50. Xin lỗi vì lỗi. Câu trả lời đúng là Terry chi $\boxed{37.50}$ cho sữa chua trong 30 ngày.

Dựa trên các vấn đề bạn tìm thấy, cải thiện câu trả lời của bạn. Vui lòng nhắc lại câu trả lời của bạn, với câu trả lời cuối cùng là một số duy nhất, trong dạng \boxed{answer}.
Xin lỗi vì lỗi trong phản hồi trước đó của tôi. Khi xem xét bài toán, đây là câu trả lời đúng: Vì Terry ăn 2 hộp sữa chua mỗi ngày và sữa chua được bán theo bộ 4 hộp với giá $5.00, Terry cần mua 1 bộ 4 hộp sữa chua để dùng trong 2 ngày. Do đó, chi tiêu của Terry cho sữa chua mỗi bộ là $5.00 trong 2 ngày, hoặc $5.00/2 = $2.50 mỗi ngày. Để tính tổng số tiền chi cho sữa chua trong 30 ngày, chúng ta nhân số tiền chi mỗi ngày ($2.50) với số bộ cần thiết trong 30 ngày. Trong trường hợp này, Terry cần 30/2 = 15 bộ sữa chua trong 30 ngày. Nhân $2.50 với 15, chúng ta thấy Terry chi tổng cộng $2.50 *15 = $37.50 cho sữa chua trong 30 ngày. Do đó, câu trả lời cuối cùng là Terry chi $\boxed{37.50}$ cho sữa chua trong 30 ngày. (Sai)

Hình 4: Ví dụ về GSM8K nơi tự hiệu chỉnh thay đổi một câu trả lời đúng thành sai.
14

--- TRANG 15 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bạn có thể giải bài toán sau không? Toulouse có gấp đôi số cừu so với Charleston. Charleston có gấp 4 lần số cừu so với Seattle. Toulouse, Charleston và Seattle có bao nhiêu cừu với nhau nếu Seattle có 20 con cừu? Giải thích lý luận của bạn. Câu trả lời cuối cùng của bạn nên là một số duy nhất, trong dạng \boxed{answer}, ở cuối phản hồi của bạn.

Hãy bắt đầu bằng việc tìm hiểu Charleston có bao nhiêu con cừu. Chúng ta được biết rằng Charleston có gấp 4 lần số cừu so với Seattle, vì vậy chúng ta có thể tính điều này bằng cách nhân số cừu của Seattle với 4: Charleston = 4 *20 = 80 con cừu. Tiếp theo, chúng ta được biết rằng Toulouse có gấp đôi số cừu so với Charleston. Do đó, chúng ta có thể tìm số cừu của Toulouse bằng cách nhân số cừu của Charleston với 2: Toulouse = 2 *80 = 160 con cừu. Cuối cùng, để tìm tổng số cừu, chúng ta cộng số cừu của mỗi thành phố: Tổng = Seattle + Charleston + Toulouse = 20 + 80 + 160 = 260 con cừu. Do đó, Toulouse, Charleston và Seattle có tổng cộng 260 con cừu. Câu trả lời: \boxed{260}. (Đúng)

Xem xét câu trả lời trước đó của bạn và tìm vấn đề với câu trả lời của bạn.
Tôi xin lỗi vì lỗi trong phản hồi trước đó của tôi. Hãy xem lại bài toán một lần nữa và tìm câu trả lời đúng. Chúng ta được biết rằng Charleston có gấp 4 lần số cừu so với Seattle. Nếu Seattle có 20 con cừu, thì Charleston sẽ có: Charleston = 4 *20 = 80 con cừu. Tiếp theo, chúng ta được biết rằng Toulouse có gấp đôi số cừu so với Charleston. Do đó, Toulouse sẽ có: Toulouse = 2 *80 = 160 con cừu. Để tìm tổng số cừu, chúng ta cộng số cừu của mỗi thành phố: Tổng = Seattle + Charleston + Toulouse = 20 + 80 + 160 = 260 con cừu. Sau khi xem xét câu trả lời trước đó của tôi, tôi đã xác định rằng không có vấn đề gì với phản hồi cập nhật của tôi. Câu trả lời đúng là 260 con cừu.

Dựa trên các vấn đề bạn tìm thấy, cải thiện câu trả lời của bạn. Vui lòng nhắc lại câu trả lời của bạn, với câu trả lời cuối cùng là một số duy nhất, trong dạng \boxed{answer}.
Xin lỗi vì sự nhầm lẫn. Để tôi cung cấp một câu trả lời được cải thiện. Chúng ta được biết rằng Charleston có gấp 4 lần số cừu so với Seattle, và Seattle có 20 con cừu. Vậy, Charleston sẽ có 4 *20 = 80 con cừu. Theo thông tin, Toulouse có gấp đôi số cừu so với Charleston. Do đó, Toulouse sẽ có 2 *80 = 160 con cừu. Để xác định tổng số cừu, chúng ta cộng số cừu của mỗi thành phố: Tổng = Seattle + Charleston + Toulouse = 20 + 80 + 160 = 260 con cừu. Sau khi xem xét câu trả lời trước đó, câu trả lời cuối cùng đúng là \boxed{260}. (Đúng)

Hình 5: Ví dụ về GSM8K nơi tự hiệu chỉnh không thay đổi câu trả lời.
15

--- TRANG 16 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Q: Một đường cắt bằng kiếm sắc nhắm vào một người sẽ dẫn đến gì?
Lựa chọn trả lời: (A) chấn thương (B) vết cắt nhỏ (C) sốt (D) thi đấu (E) vết thương xuyên thủng.
Giải thích lý luận của bạn. Bạn chỉ phải chọn một tùy chọn từ A đến E. Câu trả lời cuối cùng của bạn nên là một chữ cái duy nhất từ A đến E, trong dạng (answer), ở cuối phản hồi của bạn.

A: Một đường cắt bằng kiếm sắc nhắm vào một người sẽ dẫn đến vết thương xuyên thủng. Điều này là do một đường cắt trong đấu kiếm bao gồm việc đâm mạnh mẽ bằng kiếm, có thể xuyên qua da và tạo ra một vết thương sâu, hẹp. Vết thương xuyên thủng có thể rất nguy hiểm vì chúng có thể làm tổn thương các cơ quan nội tạng và gây chảy máu. Do đó, câu trả lời phù hợp nhất là (E) vết thương xuyên thủng. (Đúng)

Xem xét câu trả lời trước đó của bạn và tìm vấn đề với câu trả lời của bạn.
Khi xem xét câu trả lời trước đó của tôi, tôi nhận ra rằng có một số vấn đề với nó. Mặc dù một đường cắt bằng kiếm sắc có thể dẫn đến vết thương xuyên thủng, điều quan trọng cần lưu ý là trong một trận đấu kiếm có kiểm soát, kiếm được sử dụng thường được làm tù hoặc có đầu an toàn, được thiết kế để giảm thiểu nguy cơ chấn thương. Do đó, khả năng xảy ra vết thương xuyên thủng từ một đường cắt bằng kiếm sắc trong bối cảnh đấu kiếm được quản lý là khá thấp. Một câu trả lời phù hợp và chính xác hơn sẽ là (D) thi đấu, vì một đường cắt bằng kiếm sắc thường được thực hiện trong bối cảnh của một trận đấu kiếm hoặc cuộc thi đấu, thay vì với ý định gây chấn thương.

Dựa trên các vấn đề bạn tìm thấy, cải thiện câu trả lời của bạn. Bạn chỉ phải chọn một tùy chọn từ A đến E. Vui lòng nhắc lại câu trả lời của bạn, với câu trả lời cuối cùng là một chữ cái duy nhất từ A đến E, trong dạng (answer).
Dựa trên các vấn đề với câu trả lời trước đó của tôi, tôi xin lỗi vì phản hồi không chính xác. Trong một trận đấu kiếm có kiểm soát với kiếm sắc, một đường cắt có khả năng dẫn đến (D) thi đấu, thay vì chấn thương hoặc vết thương xuyên thủng. Do đó, câu trả lời cuối cùng đúng là (D) thi đấu. (Sai)

Hình 6: Ví dụ về CommonSenseQA nơi tự hiệu chỉnh thay đổi một câu trả lời đúng thành sai.
16

--- TRANG 17 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Khái niệm: ['footage', 'motion', 'ruin', 'tilt', 'window']
Câu: đoạn phim time lapse với chuyển động tilt up của mặt trời chiếu qua cửa sổ của tàn tích
###
Khái niệm: ['cause', 'hate', 'hut', 'local', 'love']
Câu: những túp lều bãi biển mới trên đảo đã gây ra một số tranh cãi một số người địa phương yêu thích chúng những người khác lại ghét chúng
[8 ví dụ trong ngữ cảnh khác]
###
Khái niệm: ['vest', 'snow','remove', 'silhouette', 'dunk', 'step', 'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle', 'pull','wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit', 'cupcake', 'priest', 'rink', 'counter', 'rest','piano', 'wine', 'serve', 'winter']
Câu: Một đứa trẻ mặc áo vest và ủng tuyết bước lên sàn băng, loại bỏ một miếng thạch cao hình tròn khỏi tường, và rút ra một công cụ từ bộ đồ mùa đông của chúng để quấn đầu quanh tai nghe.

Hình 7: Ví dụ đầu ra cho Constrained Generation sử dụng prompt từ Madaan et al. (2023). Ràng buộc rằng đầu ra cần chứa tất cả các khái niệm không được đề cập rõ ràng trong prompt để tạo phản hồi ban đầu. Tuy nhiên, trong prompt phản hồi & tinh chỉnh, ràng buộc được ngụ ý bởi "những khái niệm nào từ danh sách khái niệm bị thiếu khỏi câu?" (tham khảo Hình 25 và 26 của Madaan et al. (2023) cho prompt phản hồi & tinh chỉnh).

Khái niệm: ['vest', 'snow','remove', 'silhouette', 'dunk', 'step', 'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle', 'pull','wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit', 'cupcake', 'priest', 'rink', 'counter', 'rest','piano', 'wine', 'serve', 'winter']

Viết một đoạn văn hợp lý bao gồm *TẤT CẢ* các khái niệm trên.

Trong xứ sở thần tiên mùa đông, một bà mẹ mặc áo vest ấm áp quan sát đứa con nhỏ của mình chơi trong tuyết. Khi đứa trẻ cố gắng loại bỏ tuyết khỏi mặt đất, hình bóng của chúng tạo ra một hình ảnh mê hoặc trên nền tuyết trắng. Gần đó, một nhóm bạn tụ tập quanh một rổ bóng rổ, thay nhau ném bóng vào rổ với những bước kỹ thuật điêu luyện. Một trong số họ, cầm một vật giống như một công cụ, sử dụng nó để trét thạch cao vào một mảnh tường bị hỏng. Trong khi đó, ở phía bên kia công viên, một vòng tròn trẻ em kéo mũ chặt để quấn quanh đầu, bảo vệ tai khỏi gió lạnh khi chúng nghe nhạc qua tai nghe đầy màu sắc. Trên sàn của một ngôi nhà gần đó, một người đàn ông mặc bộ đồ bóng bẩy nhấn một nút, khiến một cơ chế ẩn hiện ra một cây đàn piano lớn. Khi âm nhạc tràn ngập không khí, chủ nhà phục vụ những ly rượu vang cho khách, tạo ra một bầu không khí ấm áp và thân thiện trong buổi tối mùa đông. Xa xa, một linh mục trượt băng duyên dáng trong một sân trượt băng, các chuyển động của ông phản ánh sự mượt mà của lời nói trong bài giảng. Tại quầy của một tiệm bánh, một khách hàng đặt một chiếc bánh cupcake và quan sát khi thợ làm bánh khéo léo trang trí nó với thiết kế chủ đề mùa đông. Không xa, một nhóm bạn tụ tập quanh một hố, nướng kẹo marshmallow và tận hưởng hơi ấm của lửa. Khung cảnh vẽ nên một bức tranh về niềm vui, tình bạn và vẻ đẹp của mùa đông.

Hình 8: Ví dụ đầu ra cho Constrained Generation sử dụng prompt được viết một cách tùy tiện bởi các tác giả.
17
