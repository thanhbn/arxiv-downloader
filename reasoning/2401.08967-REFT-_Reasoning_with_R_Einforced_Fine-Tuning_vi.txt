# REFT: Lý luận với Tinh chỉnh Có Tăng cường

Trung Quoc Luong∗, Xinbo Zhang∗, Zhanming Jie*, Peng Sun†, Xiaoran Jin, Hang Li
ByteDance Research
{trung.luong, zhangxinbo.freya, allan.jie}@bytedance.com
{wanhesong, xiaoran.jin, lihang.lh}@bytedance.com

## Tóm tắt

Một cách để tăng cường khả năng lý luận của các Mô hình Ngôn ngữ Lớn (LLMs) là thực hiện Tinh chỉnh Có giám sát (SFT) sử dụng các chú thích Chuỗi-Suy nghĩ (CoT). Tuy nhiên, phương pháp này không thể hiện khả năng tổng quát hóa đủ mạnh, vì quá trình huấn luyện chỉ dựa vào dữ liệu CoT được cho. Ví dụ trong giải toán, thường chỉ có một đường lý luận được chú thích cho mỗi câu hỏi trong dữ liệu huấn luyện. Trực quan, sẽ tốt hơn nếu thuật toán học từ nhiều đường lý luận được chú thích cho một câu hỏi. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả gọi là Tinh chỉnh Có Tăng cường (ReFT) để nâng cao khả năng tổng quát hóa của việc học LLMs cho lý luận, với giải toán làm ví dụ. ReFT đầu tiên khởi động mô hình với SFT, sau đó sử dụng học tăng cường trực tuyến, cụ thể là thuật toán PPO trong bài báo này, để tinh chỉnh thêm mô hình, trong đó nhiều đường lý luận được tự động lấy mẫu cho câu hỏi và phần thưởng được tự nhiên suy ra từ các câu trả lời đúng. Các thí nghiệm mở rộng trên tập dữ liệu GSM8K, MathQA, và SVAMP cho thấy ReFT vượt trội đáng kể so với SFT, và hiệu suất có thể được cải thiện thêm bằng cách kết hợp các chiến lược thời gian suy luận như bỏ phiếu đa số và xếp hạng lại. Lưu ý rằng ReFT đạt được cải thiện bằng cách học từ cùng các câu hỏi huấn luyện như SFT, mà không dựa vào câu hỏi huấn luyện bổ sung hoặc tăng cường. Điều này cho thấy khả năng tổng quát hóa vượt trội của ReFT.

## 1 Giới thiệu

Các phương pháp tiên tiến nhất để giải toán (Luo et al., 2023; Wang et al., 2023a) sử dụng Tinh chỉnh Có giám sát (SFT) để huấn luyện mô hình bằng các chú thích Chuỗi-Suy nghĩ (CoT) (Wei et al., 2022). Như được thể hiện trong Hình 1, một chú thích CoT nêu ra các bước lý luận trung gian hướng tới việc giải quyết một bài toán.

Thường có một chú thích CoT cho mỗi câu hỏi trong dữ liệu huấn luyện, tức là một đường lý luận đúng, được sử dụng trong SFT. Chúng tôi quan sát thấy điều này có thể dẫn đến khả năng tổng quát hóa tương đối yếu của các mô hình SFT. Thường xảy ra trường hợp nhiều chú thích CoT hợp lệ tồn tại cho cùng một câu hỏi (Cobbe et al., 2021a; Zhang et al., 2023), nhấn mạnh nhu cầu về một phương pháp tinh chỉnh mạnh mẽ hơn. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp đơn giản nhưng hiệu quả gọi là Tinh chỉnh Có Tăng cường (ReFT) (Hình 1 dưới).

ReFT bắt đầu với giai đoạn khởi động bao gồm Tinh chỉnh Có giám sát (SFT) trong một hoặc hai epoch (Hình 1, hộp tô màu). Giai đoạn ban đầu này trang bị cho mô hình khả năng tạo ra các phản hồi đúng cho các bài toán toán học ở một mức độ nhất định, như đã được chứng minh trong các nghiên cứu trước (Cobbe et al., 2021a). Tiếp theo, ReFT tiến hành tinh chỉnh thêm mô hình thông qua việc sử dụng thuật toán Học Tăng cường (RL) trực tuyến (Sutton and Barto, 2018), cụ thể là Tối ưu hóa Chính sách Gần đúng (PPO) (Schulman et al., 2017) trong bài báo này. Bằng cách này, ReFT có thể lấy mẫu nhiều đường lý luận đúng hoặc chú thích CoT và học từ chúng (Hình 2, phải).

Vì dữ liệu huấn luyện bao gồm các câu trả lời đúng, phần thưởng vàng có thể được tự nhiên suy ra từ chúng khi huấn luyện PPO. Do đó, không cần yêu cầu một mô hình phần thưởng được huấn luyện riêng. Ngược lại, RLHF (Ouyang et al., 2022) phải sử dụng một mô hình phần thưởng được học từ dữ liệu được gán nhãn bởi con người.

Trong giai đoạn khởi động, ReFT đạt được một mức độ chính xác nhất định bằng học có giám sát. Trong giai đoạn RL, ReFT tiếp tục nâng cao khả năng của mình bằng học tăng cường thông qua việc lấy mẫu các đường lý luận CoT khác nhau. Bằng cách này, ReFT nhận được tín hiệu giám sát phong phú hơn nhiều so với SFT. Phương pháp này cho phép ReFT cải thiện đáng kể khả năng tổng quát hóa trong giải toán (Gao et al., 2018; Brown et al., 2020). Lưu ý rằng ReFT vượt trội so với SFT bằng cách sử dụng cùng các câu hỏi huấn luyện, mà không dựa vào câu hỏi huấn luyện bổ sung hoặc tăng cường. Thực tế, ReFT không xung đột với kỹ thuật dữ liệu như vậy và có thể được kết hợp một cách liền mạch với nó.

Những đóng góp của chúng tôi như sau:
• Chúng tôi giới thiệu một phương pháp tinh chỉnh mới, tinh chỉnh có tăng cường (ReFT), sử dụng học tăng cường để giải toán. ReFT thể hiện khả năng tổng quát hóa được cải thiện so với tinh chỉnh có giám sát thông thường khi được huấn luyện trên cùng tập dữ liệu.

• Chúng tôi thực hiện các thí nghiệm mở rộng sử dụng hai mô hình nền, CodeLLAMA (Roziere et al., 2023) và Galactica (Taylor et al., 2022), trên ba tập dữ liệu chuẩn: GSM8K (Cobbe et al., 2021a), MathQA (Amini et al., 2019), và SVAMP (Patel et al., 2021). Các thí nghiệm của chúng tôi bao gồm cả CoT ngôn ngữ tự nhiên và dựa trên chương trình, chứng minh hiệu suất và khả năng tổng quát hóa được cải thiện đáng kể của ReFT.

• Ngoài ra, chúng tôi chứng minh rằng ReFT hưởng lợi từ cả bỏ phiếu đa số (Wang et al., 2023b) và xếp hạng lại mô hình phần thưởng (Uesato et al., 2022) tại thời gian suy luận, cải thiện thêm hiệu suất của nó.

## 2 Công trình liên quan

**Giải Toán** Các nỗ lực nghiên cứu gần đây tập trung vào thiết kế prompt CoT và kỹ thuật dữ liệu. Hầu hết chúng đều cố gắng làm cho CoT toàn diện và chi tiết để trình bày các giải pháp lý luận từng bước (Nye et al., 2021; Fu et al., 2023; Zhou et al., 2023b; Khot et al., 2023; Zelikman et al., 2022; Imani et al., 2023; Miao et al., 2023). Gao et al. (2023) tiếp tục đề xuất sử dụng chương trình Python làm prompt CoT, chứng minh các bước lý luận chính xác hơn và cải thiện đáng kể so với CoT ngôn ngữ tự nhiên (Wei et al., 2022). Zhou et al. (2023a) giới thiệu một phương pháp prompting tạo mã để xác minh bước lý luận trung gian với GPT-4 (OpenAI, 2023), từ đó đạt được hiệu suất tiên tiến trên GSM8K (Cobbe et al., 2021a) và MATH (Hendrycks et al., 2021). Một hướng nghiên cứu khác tập trung vào cải thiện chất lượng CoT (Wang et al., 2023a; Liu et al., 2023; Yu et al., 2023) và tăng lượng dữ liệu CoT (Luo et al., 2023; Yue et al., 2023) từ ChatGPT (gpt-3.5-turbo) hoặc GPT-4 của OpenAI.

**Học Tăng cường** Công trình của chúng tôi chủ yếu liên quan đến nghiên cứu gần đây áp dụng PPO (Schulman et al., 2017) vào xử lý ngôn ngữ tự nhiên để căn chỉnh sở thích của con người (Ouyang et al., 2022). Kể từ đó, một số thuật toán huấn luyện đã được đề xuất để cải thiện hiệu quả việc căn chỉnh, bao gồm tối ưu hóa sở thích trực tiếp (DPO) (Rafailov et al., 2023), tối ưu hóa sở thích nhận dạng (IPO) (Azar et al., 2023), và tối ưu hóa Kahneman-Tversky (KTO) (Ethayarajh et al., 2023). Khác với mục đích căn chỉnh, chúng tôi hướng đến việc áp dụng học tăng cường như một mô hình tinh chỉnh để cải thiện hiệu suất so với tinh chỉnh có giám sát thông thường.

Cụ thể cho việc giải toán, Uesato et al. (2022) và Lightman et al. (2023) đã huấn luyện một mô hình phần thưởng dựa trên kết quả hoặc quy trình để thực hiện xếp hạng lại (Cobbe et al., 2021a) nhằm đạt được hiệu suất tốt hơn nhiều so với SFT và bỏ phiếu đa số (Wang et al., 2023b). Trong khi phương pháp của chúng tôi nhằm cải thiện hiệu suất của chính sách đó, các phương pháp xếp hạng lại mô hình phần thưởng này có thể dễ dàng tích hợp vào mô hình chính sách kết quả.

## 3 Phương pháp

Trong nghiên cứu này, chúng tôi tập trung vào CoT ngôn ngữ tự nhiên (N-CoT) (Wei et al., 2022) (Hình 1) và CoT dựa trên chương trình (Gao et al., 2023) (P-CoT) sử dụng Python. Gao et al. (2023) đã đề xuất CoT dựa trên chương trình cho việc giải toán. Chúng ta có thể đơn giản thực thi chương trình để có được câu trả lời. Để đảm bảo rõ ràng và tránh sự mơ hồ, chúng tôi sử dụng các thuật ngữ N-CoT và P-CoT để đại diện cho CoT ngôn ngữ tự nhiên và dựa trên chương trình, tương ứng.

### 3.1 Tinh chỉnh Có Tăng cường

Quá trình Tinh chỉnh Có Tăng cường (ReFT) được đề xuất bao gồm hai giai đoạn: giai đoạn khởi động và giai đoạn học tăng cường. Thuật toán tổng thể được thể hiện trong Thuật toán 1.

**Khởi động** Trong giai đoạn này, chính sách được tinh chỉnh trong vài epoch trên tập dữ liệu bao gồm các bộ "(câu hỏi, CoT)": (x,e). Nó cho phép mô hình có kỹ năng giải quyết vấn đề cơ bản để tạo ra phản hồi phù hợp. Chính thức, quá trình tạo CoT có thể được phân tách thành một chuỗi các hành động dự đoán token tiếp theo. Token hành động cuối cùng, <eos>, báo hiệu quá trình tạo kết thúc. CoT e được viết là:

e = [a₁, a₂, ..., aₗ₋₁, aₗ=<eos>]

trong đó L đại diện cho độ dài tối đa. Tại thời điểm t, hành động aₜ được lấy mẫu từ chính sách πθ(·|sₜ) trong đó aₜ có thể là bất kỳ token nào trong từ vựng và trạng thái sₜ bao gồm tất cả tokens trong câu hỏi và tất cả tokens được tạo ra cho đến nay. Sau mỗi hành động, trạng thái kết quả sₜ₊₁ là sự nối của trạng thái hiện tại sₜ và hành động aₜ:

sₜ₊₁ = {
    x, t = 0
    [sₜ, aₜ], 1 ≤ t ≤ L
}

Khi hành động được tạo ra là token <eos>, trạng thái kết quả sₗ₊₁ là trạng thái kết thúc và quá trình tạo hoàn thành. Với ký hiệu này, hàm mất mát cho một mẫu có thể được viết là:

LSFT(θ) = -𝔼e~D[∑ᴸₜ₌₁ log(πθ(aₜ|sₜ))]     (1)

**Học Tăng cường** Trong giai đoạn này, chính sách cải thiện hiệu suất của mình thông qua một hình thức tự học trực tuyến sử dụng tập dữ liệu bao gồm các bộ (câu hỏi, câu trả lời): (x,y). Cụ thể, mô hình chính sách học bằng cách liên tục lấy mẫu các phản hồi (Hình 2), đánh giá tính đúng đắn của câu trả lời và cập nhật các tham số của nó theo cách trực tuyến (dòng 7-14 trong Thuật toán 1). Chúng tôi sử dụng PPO (Schulman et al., 2017) với thuật toán mục tiêu bị cắt để huấn luyện. Theo Ziegler et al. (2019), mô hình giá trị Vφ được xây dựng bằng cách thêm một đầu giá trị tuyến tính trên các trạng thái ẩn cuối cùng của mô hình chính sách πθ, đây là mô hình sau giai đoạn khởi động. Phần thưởng 0 được đưa ra cho tất cả hành động dẫn đến trạng thái không kết thúc. Tại trạng thái kết thúc, chúng tôi sử dụng một hàm phần thưởng trực tiếp so sánh câu trả lời được trích xuất từ CoT của trạng thái và câu trả lời đúng y. Ở đây, hàm phần thưởng trả về 1 nếu câu trả lời được coi là đúng, ngược lại trả về 0. Trên tập dữ liệu có câu trả lời đều là số, phần thưởng một phần (Zhong et al., 2017; Le et al., 2022) là 0.1 có thể được áp dụng khi câu trả lời có thể được trích xuất và là loại số. Cho 1 ≤ t ≤ L, chúng ta viết:

r(sₜ, aₜ, sₜ₊₁) = {
    1, EXTRACT(sₜ₊₁) = y
    0.1, EXTRACT(sₜ₊₁) ≠ null, ≠ y
    0, EXTRACT(sₜ₊₁) = null
}

Phần thưởng một phần như vậy có thể giúp giảm hiệu ứng học từ phần thưởng thưa thớt (Riedmiller et al., 2018; Trott et al., 2019). Ngoài ra, theo Zheng et al. (2023), tổng phần thưởng của chúng tôi là tổng của điểm hàm phần thưởng và phân kỳ Kullback-Leibler (KL) (Kullback and Leibler, 1951) giữa chính sách RL đã học và chính sách ban đầu được chia tỷ lệ bởi hệ số β.

rₜₒₜₐₗ(sₜ, aₜ, sₜ₊₁) = r(sₜ, aₜ, sₜ₊₁) - βKL(πθ(·|sₜ), π⁽⁰⁾θ(·|sₜ))

Ước lượng lợi thế tổng quát (Schulman et al., 2018) được sử dụng để tính toán lợi thế:

Âₜ = ∑ᴸ⁻ᵗₗ₌₀ (γλ)ˡδₜ₊ₗ

trong đó Temporal Difference (TD) được định nghĩa là:
δₜ′ = -Vφ(sₜ′) + rₜₒₜₐₗ(sₜ′, aₜ′, sₜ′₊₁) + γVφ(sₜ′₊₁)

với giá trị trạng thái kết thúc Vφ(sₗ₊₁) := 0, λ ∈ (0,1] là hệ số chiết khấu cho phần thưởng, và γ ∈ [0,1] là hệ số chiết khấu cho TD. Đối với ước lượng lợi nhuận, chúng tôi tận dụng λ-return R̂ₜ, có thể được viết là tổng của ước lượng lợi thế tổng quát và ước lượng giá trị:

R̂ₜ = Âₜ + Vφ(sₜ)

Cuối cùng, các mục tiêu chính sách và giá trị có thể được viết trong hai phương trình dưới đây:

Lₚₒₗᵢcᵧ(θ) = -𝔼e~πθₒₗd[min(πθ(aₜ|sₜ)/πθₒₗd(aₜ|sₜ)Âₜ, clip(πθ(aₜ|sₜ)/πθₒₗd(aₜ|sₜ), 1-ε, 1+ε)Âₜ)]

Lᵥₐₗᵤₑ(φ) = ½𝔼e~πθₒₗd[max(|Vφ(sₜ) - R̂ₜ|², |clip(R̂ₜ - Vφ(sₜ), Âₜ - ε, Âₜ + ε)|²)]

trong đó πθₒₗd, Vφₒₗd được sử dụng để lấy mẫu CoT và tính toán Âₜ, R̂ₜ. Hàm mất mát thống nhất là tổng có trọng số của các mục tiêu trên.

LRL(θ,φ) = Lₚₒₗᵢcᵧ + αLᵥₐₗᵤₑ     (2)

trong đó α là hệ số cho mục tiêu giá trị.

## 4 Thí nghiệm

### 4.1 Tập dữ liệu

Chúng tôi thực hiện thí nghiệm trên ba tập dữ liệu bài toán: GSM8K (Cobbe et al., 2021a), SVAMP (Patel et al., 2021) và MathQA (Amini et al., 2019). Đối với cả GSM8K và SVAMP, định dạng câu trả lời là một giá trị số. Trong MathQA, định dạng thay vào đó là danh sách nhiều lựa chọn (tức là ABCD). Bảng 1 trình bày thống kê của tất cả tập dữ liệu. Chúng tôi thực hiện prompting few-shot (Wei et al., 2022; Gao et al., 2023) sử dụng GPT-3.5-turbo để có được cả chú thích N-CoT và P-CoT. Các chú thích N-CoT và P-CoT được thu thập theo Jie et al. (2023). Chúng tôi cũng đã thực hiện một thí nghiệm bổ sung trên phiên bản số của MathQA (Jie and Lu, 2023) trong đó định dạng cũng là một giá trị số. Những thí nghiệm như vậy được sử dụng để chứng minh giả định của chúng tôi về hiện tượng reward hacking tiềm năng (Skalse et al., 2022) trên MathQA (§4.4).

### 4.2 Baseline

Chúng tôi so sánh ReFT với các baseline SFT và self-training (Xie et al., 2020; Amini et al., 2022). SFT đơn giản tinh chỉnh mô hình ngôn ngữ trên dữ liệu huấn luyện. Các thí nghiệm với phương pháp self-training đảm bảo so sánh tương đối công bằng vì những phương pháp này chia sẻ cơ chế mà các mẫu được tạo từ mô hình được sử dụng để huấn luyện.

Chúng tôi đã triển khai Offline Self-Training (Offline-ST) (He et al., 2020), và Online (Hoi et al., 2021) Self-Training (Online-ST). Phương pháp Offline-ST tương tự như expert iteration (Anthony et al., 2017; Uesato et al., 2022; Zelikman et al., 2022). Chúng tôi đầu tiên sử dụng checkpoint SFT từ checkpoint sớm để lấy mẫu CoTs và xác minh chúng với ground truth. Chúng tôi chỉ giữ lại những mẫu expert có câu trả lời đúng. Chúng tôi thực hiện SFT trên sự kết hợp của dữ liệu huấn luyện gốc và các mẫu expert.

Phương pháp Online-ST được tạo ra để có thể so sánh chặt chẽ với ReFT. Theo ReFT, Online-ST có cùng quá trình khởi động. Sau đó, chúng tôi thực hiện huấn luyện liên tục với các mẫu được tạo ra ngay lập tức. Tại mỗi bước huấn luyện, mô hình đầu tiên lấy mẫu CoTs cho một batch và chỉ giữ lại những cái có câu trả lời đúng. Batch kết quả bao gồm cả CoTs được lấy mẫu và ground-truth. Sau đó chúng tôi cập nhật các tham số mô hình trên batch này với mục tiêu tinh chỉnh có giám sát LSFT. So với ReFT, Online-ST không sử dụng các phản hồi tiêu cực (với câu trả lời sai) cũng không có cơ chế chuyên dụng để ngăn mô hình phân kỳ đáng kể khỏi mô hình ban đầu, điều này có thể biểu hiện như overfitting cụ thể cho tác vụ và bất ổn định huấn luyện.

### 4.3 Thiết lập thí nghiệm

Chúng tôi thực hiện thí nghiệm với hai mô hình nền: Galactica-6.7B (Taylor et al., 2022) và CodeLLAMA-7B (Roziere et al., 2023). Cả hai mô hình đều được báo cáo có hiệu suất mạnh trong giải toán và thường được áp dụng trong văn hệ gần đây về các tác vụ lý luận (Yue et al., 2023; Luo et al., 2023).

Ngoài việc so sánh với baselines, chúng tôi cũng áp dụng các kỹ thuật phổ biến, bỏ phiếu đa số (Wang et al., 2023b) và xếp hạng lại mô hình phần thưởng (Lightman et al., 2023) trên GSM8K.

**Siêu tham số** Trong tất cả thí nghiệm, việc huấn luyện được thực hiện với 8 GPU A100-80GB sử dụng DeepSpeed (Rajbhandari et al., 2020; Rasley et al., 2020) Zero stage 2 và HuggingFace Accelerate (Gugger et al., 2022). Trong giai đoạn khởi động của ReFT, chúng tôi sử dụng optimizer AdamW (Loshchilov and Hutter, 2017) với tỷ lệ khởi động 10%. Kích thước batch là 48 và learning rate là 1e-5. Độ dài tối đa được đặt là 1024. Số epoch trong giai đoạn khởi động là 2 trong tất cả thiết lập trừ trên MathQA MCQ và MathQA numeric trong đó chúng tôi sử dụng lên đến 5 và 10 tương ứng. Mô hình được huấn luyện trong 300 epoch với learning rate 3e-7. Theo Ziegler et al. (2019), λ, γ, α, ε và U trong PPO được đặt lần lượt là 1, 0.95, 5, 0.2, và 2. Hệ số KL β được đặt là 0.01 cho P-CoT và được đặt là 0.05 cho các thí nghiệm N-CoT. Các thiết lập siêu tham số khác về ReFT có thể được tìm thấy trong Phụ lục B.

Đối với baseline SFT, chúng tôi huấn luyện mô hình trong 40 epoch và chọn checkpoint có hiệu suất tốt nhất. Số epoch này đã được chọn đủ lớn để đảm bảo SFT hội tụ. Đối với baseline Offline-ST, chúng tôi lấy mẫu CoTs bằng cách sử dụng checkpoint từ giai đoạn khởi động ReFT. Sử dụng nhiệt độ tạo 1.0 và độ dài tối đa 1024, chúng tôi lấy mẫu 100 CoTs cho mỗi câu hỏi và chỉ giữ những cái có câu trả lời đúng. Theo Singh et al. (2023), sau đó chúng tôi lấy mẫu phụ CoTs thành 10 CoTs ngẫu nhiên duy nhất cho mỗi câu hỏi để cân bằng độ khó của các câu hỏi. Số epoch tinh chỉnh được đặt là 20, đủ lớn để đảm bảo huấn luyện hội tụ. Như đã đề cập trong §4.2, baseline Online-ST cố gắng bắt chước cùng thiết lập như trong ReFT. Chúng tôi có cùng quá trình khởi động và thiết lập siêu tham số gần giống như ReFT.

**Xếp hạng lại Mô hình Phần thưởng** Theo (Cobbe et al., 2021a; Uesato et al., 2022), chúng tôi huấn luyện một mô hình phần thưởng (RM) để xác định tính đúng đắn của CoT. Để xây dựng dữ liệu huấn luyện RM, chúng tôi sử dụng mô hình từ giai đoạn khởi động và thực hiện lấy mẫu để thu được 100 CoTs cho mỗi câu hỏi trong tập huấn luyện. Các CoTs được khử trùng và các nhãn nhị phân có thể được thu được bằng cách so sánh câu trả lời được trích xuất với ground truth.

Như một thực hành phổ biến, mô hình phần thưởng là một mô hình ngôn ngữ được khởi tạo từ checkpoint SFT tốt nhất (Cobbe et al., 2021a; Ouyang et al., 2022). Tương tự như mô hình phần thưởng dựa trên kết quả (ORM) (Uesato et al., 2022), mô hình phần thưởng được huấn luyện để dự đoán một nhãn nhị phân chỉ ra giải pháp "đúng" hoặc "sai". Khi đầu vào đi qua mô hình phần thưởng, phân loại được thực hiện với một bộ phân loại tuyến tính trên trạng thái ẩn của token cuối cùng. Cuối cùng, giải pháp có điểm "đúng" cao nhất trong số các ứng viên được chọn làm câu trả lời cuối cùng. Chúng tôi huấn luyện mô hình RM trong 3 epoch sử dụng kích thước batch 24, độ dài tối đa 700 và lịch learning rate tuyến tính với giai đoạn khởi động 10% và learning rate tối đa 1e-6.

**Đánh giá** Chúng tôi báo cáo độ chính xác giá trị cho cả N-CoT và P-CoT trên tất cả tập dữ liệu. Đối với bỏ phiếu đa số và xếp hạng lại (Bảng 4), chúng tôi lấy mẫu 100 CoTs để đánh giá. Trong bỏ phiếu, câu trả lời hợp lệ có số phiếu đa số được chọn làm câu trả lời cuối cùng để tính độ chính xác. Trong xếp hạng lại, chúng tôi chọn CoT có điểm cao nhất và trích xuất câu trả lời.

### 4.4 Kết quả

**ReFT Vượt trội so với SFT** Bảng 2 so sánh hiệu suất giữa các baseline và ReFT được đề xuất trên tập dữ liệu GSM8K, SVAMP, và MathQA. Chúng ta có thể quan sát thấy ReFT liên tục đạt được hiệu suất tốt hơn nhiều so với SFT trừ trên MathQA MCQ N-CoT. Cụ thể, chúng tôi có cải thiện gần 10 điểm và 12 điểm so với SFT với CodeLLAMA trên GSM8K N-CoT và P-CoT, tương ứng. Trung bình, chúng tôi đạt được cải thiện 6.7 điểm và 7.4 điểm với CodeLLAMA trên tất cả tập dữ liệu trong N-CoT và P-CoT, tương ứng. Đáng chú ý, không có chú thích bổ sung hoặc mô hình phần thưởng nào được sử dụng trong ReFT. Những kết quả mạnh mẽ như vậy chứng minh khả năng tổng quát hóa bền vững của ReFT (xem Phân tích §5) và tiềm năng to lớn để khám phá thêm việc huấn luyện với học tăng cường (Lu et al., 2023).

Self-training offline bao gồm dữ liệu lấy mẫu từ chính sách ban đầu để tinh chỉnh. Chúng ta có thể thấy baseline đơn giản này có thể cải thiện hiệu suất so với SFT (He et al., 2020; Gulcehre et al., 2023) nhưng những cải thiện này còn kém xa so với cải thiện của ReFT. Những so sánh như vậy cho thấy "khám phá" là cần thiết trong ReFT để có hiệu suất tốt. Mặc dù online self-training đạt được thêm một số cải thiện với Galactica, nó vẫn kém xa ReFT trung bình. Kết quả này cho thấy các instance sai cũng rất cần thiết để hướng dẫn mô hình khám phá tốt hơn. Các so sánh với self-training cũng gợi ý rằng phương pháp được đề xuất với lấy mẫu on-policy và học tăng cường tốt hơn các phương pháp tăng cường dữ liệu tiêu chuẩn.

**Reward Hacking cho MathQA** Điều tra của chúng tôi về kết quả tiêu cực trên MathQA MCQ cho thấy ReFT gặp phải reward hacking (Skalse et al., 2022) trên câu hỏi nhiều lựa chọn trong quá trình huấn luyện. Hình 3 cho thấy cách các giải pháp được lấy mẫu tạo ra "phần thưởng không chính xác", làm cho việc huấn luyện RL gặp khó khăn. Như chúng ta có thể thấy, CoT được lấy mẫu thu được câu trả lời sai "172" không phải là một nửa tích của "18" và "22". Tuy nhiên, bước lý luận cuối cùng vẫn dự đoán tùy chọn "C" làm câu trả lời cuối cùng vì mô hình sẽ luôn dự đoán một trong các tùy chọn từ {A, B, C, D, E} bất kể tính đúng đắn của CoT trung gian. Do đó, CoT gây hiểu lầm như vậy sẽ nhận được phần thưởng tích cực "1" và dẫn mô hình sai lầm để coi đây là CoT đúng. Hiện tượng reward hacking cơ bản này làm hỏng nghiêm trọng việc huấn luyện mô hình (Everitt et al., 2021). Đây cũng là lý do chúng tôi chọn checkpoint với các bước khởi động dài hơn cho MathQA N-CoT để giảm hiệu ứng reward hacking.

Để chứng minh thêm hiệu ứng tiêu cực của câu hỏi MCQ, chúng tôi thí nghiệm trên biến thể MathQA của Jie and Lu (2023), MathQA numeric (Bảng 1), loại bỏ các tùy chọn trong câu hỏi và trực tiếp dự đoán câu trả lời số. Bảng 3 trình bày so sánh với các baseline. Chúng ta có thể quan sát thấy ReFT liên tục vượt trội so với các baseline sử dụng cả Galactica và CodeLLAMA.

Lý tưởng, chúng ta có thể giảm hiệu ứng reward hacking trên MathQA MCQ nếu chúng ta có thể thu được phần thưởng chi tiết hơn (ví dụ, phần thưởng dựa trên quy trình (Lightman et al., 2023)) cho các bước lý luận trung gian. Tuy nhiên, việc phát triển một mô hình phần thưởng dựa trên quy trình đáng tin cậy rất tốn kém và đòi hỏi chú thích thủ công rộng rãi của các bước lý luận. Nhận thức được những thách thức này, chúng tôi coi việc kiểm soát reward hacking và phân tích của nó là một vấn đề quan trọng cần được giải quyết trong công việc tương lai.

**Bỏ phiếu Đa số và Xếp hạng lại Có lợi cho ReFT** Theo Wang et al. (2023b); Uesato et al. (2022); Lightman et al. (2023), chúng tôi cũng thực hiện bỏ phiếu đa số và xếp hạng lại mô hình phần thưởng để cho thấy ReFT có thể hưởng lợi từ các kỹ thuật phổ biến này. Cụ thể, chúng tôi thực hiện lấy mẫu từ cả chính sách SFT và ReFT. Chúng tôi lấy mẫu 100 giải pháp CoT cho mỗi câu hỏi và sử dụng mô hình phần thưởng được mô tả trong §4.3 để thực hiện xếp hạng lại. Kết quả trong Bảng 4 chứng minh rằng ReFT liên tục đạt được hiệu suất tốt nhất trên GSM8K bằng xếp hạng lại mô hình phần thưởng. ReFT + Voting vượt trội đáng kể so với SFT + Voting 8.6 điểm trung bình trên tất cả thiết lập. ReFT với xếp hạng lại vượt trội so với SFT với xếp hạng lại hơn 3 điểm.

So với các phương pháp mã nguồn mở hiện có (Luo et al., 2023; Wang et al., 2023a; Yue et al., 2023) (Bảng 4 dưới), biến thể P-CoT tốt nhất của chúng tôi đạt được hiệu suất tốt nhất với độ chính xác 81.2 trên GSM8K. Ngoài ra, những phương pháp này chủ yếu bao gồm dữ liệu bổ sung được tạo từ ChatGPT và thực hiện chưng cất trong quá trình tinh chỉnh. Ngược lại, chúng tôi cải thiện chính sách đó bằng cách khai thác tiềm năng của dữ liệu huấn luyện hiện có và đẩy giới hạn hiệu suất chính sách. Kết quả tốt nhất của chúng tôi được báo cáo trong Bảng 4, tức là thiết lập CodeLLAMA + ReFT + Reranking với P-CoT, thậm chí còn vượt qua GPT-3.5-turbo. Tuy nhiên, chúng tôi đạt được kết quả với một mô hình chỉ có kích thước 7B.

**Thí nghiệm với Mô hình Nhỏ** Trực quan, khám phá có thể dẫn đến demonstration không hoàn hảo với mô hình ngôn ngữ nhỏ. Chúng tôi thực hiện thí nghiệm trên dữ liệu P-CoT sử dụng Galactica-125M, Codeparrot-small và Codegen-350M. Bảng 5 cho thấy so sánh hiệu suất giữa SFT và ReFT. Đáng ngạc nhiên, ReFT vẫn vượt trội so với SFT trên ba tập dữ liệu. Những cải thiện như vậy chứng minh sự bền vững của ReFT trong quá trình khám phá các chương trình hợp lý.

**Nghiên cứu Ablation** Chúng tôi thực hiện nghiên cứu ablation sử dụng CodeLLAMA trên GSM8K P-CoT (Bảng 6). Không có phần thưởng một phần, ReFT đạt được độ chính xác thấp hơn 74.4 nhưng vẫn tốt hơn nhiều so với SFT. Như đã đề cập trong §3.1, phần thưởng một phần như vậy có thể giúp giảm hiệu ứng phần thưởng thưa thớt (Trott et al., 2019) trong quá trình huấn luyện. Ngoài ra, phân phối chính sách sẽ dễ dàng sụp đổ để tạo ra kết quả không mong muốn (tức là độ chính xác 0) nếu chúng ta đặt hệ số KL β về 0. Chắc chắn là quan trọng để áp đặt ràng buộc trên không gian mà chính sách khám phá (Ouyang et al., 2022). Bước khởi động ban đầu về cơ bản tạo ra những ràng buộc như vậy và cho phép chính sách khám phá thêm trong phạm vi được điều chỉnh bởi β. Chúng tôi cũng thí nghiệm với một mô hình giá trị riêng biệt (Andrychowicz et al., 2021; Cobbe et al., 2021b), trong đó các tham số thân được khởi tạo giống như mô hình chính sách. Chúng tôi thấy rằng thiết lập như vậy cho phép chính sách hội tụ nhanh hơn trong huấn luyện RL sớm, nhưng cuối cùng đạt được hiệu suất ngang bằng. So với thiết lập ban đầu của mô hình giá trị chia sẻ, tuy nhiên, nó gấp đôi chi phí tính toán do một forward-pass bổ sung, cũng như gấp đôi chi phí bộ nhớ do lưu trữ mạng giá trị riêng biệt. Cuối cùng, trong Phụ lục C chúng tôi đưa ra một nghiên cứu trường hợp để cho thấy P-CoT được tạo ra phát triển như thế nào cho SFT và ReFT.

## 5 Phân tích

**Tổng quát hóa** Hình 4 cho thấy phần thưởng trung bình, độ chính xác đánh giá, và phân kỳ KL trong quá trình huấn luyện ReFT trên GSM8K P-CoT sử dụng CodeLLAMA làm mô hình nền. SFT hội tụ và trở nên overfitting khi tiến gần epoch thứ 40. Tuy nhiên, chúng ta có thể thấy phần thưởng trung bình khoảng 80% đến 90% cho chính sách ReFT tại epoch thứ 40, và độ chính xác giá trị cũng đang tăng. Ngoài ra, chúng ta có thể thấy phân kỳ KL (Hình 4 (c)) rất lớn ở đầu và sau đó duy trì giá trị hợp lý giữa 0 và 10. Phân kỳ KL ổn định cho thấy chính sách của chúng tôi thực hiện khám phá trong không gian chứa các chương trình thích hợp. Cơ chế học tăng cường cơ bản cải thiện đáng kể khả năng tổng quát hóa của ReFT (Brown et al., 2020).

**Đánh giá Định tính** Chúng tôi thực hiện đánh giá con người để đánh giá định tính đầu ra từ mô hình SFT, checkpoint Warmup, và mô hình ReFT. Đánh giá sử dụng 50 câu hỏi và lấy mẫu các giải pháp trong tập test GSM8K có thể được giải đúng bởi cả ba mô hình. Chúng tôi yêu cầu bốn người chú thích khác nhau chấm điểm đường lý luận theo các tiêu chí sau, mỗi tiêu chí được chấm điểm từ 0 đến 1.

• **Logic**: đánh giá xem logic dẫn đến câu trả lời có đúng không.
• **Naming**: đánh giá xem biến có truyền đạt ngữ nghĩa thích hợp và hợp lý không
• **Compactness**: đánh giá xem đường lý luận có chứa thông tin dư thừa không.

Điểm hoàn hảo là 3 cho thấy hiệu suất tốt trên ba khía cạnh này. Để đảm bảo đánh giá công bằng và trung thực, chúng tôi tuân thủ nghiêm ngặt thiết lập: (1) Nguồn gốc của mỗi đường lý luận (từ SFT, Warmup, hoặc ReFT) được ẩn danh để ngăn thiên vị của người chú thích. (2) Bốn người chú thích khác nhau chịu trách nhiệm cho các phần khác nhau của mẫu. Như thấy trong bảng 7, mặc dù điểm tổng thể khá gần nhau, ReFT hoạt động hơi tốt hơn SFT, và vượt trội so với biến thể Warmup. Lưu ý rằng SFT vốn được huấn luyện để học từ ground truth, do đó, có khả năng có điểm cao. Phân tích so sánh này nhấn mạnh sự bền vững của ReFT trong việc tạo ra các đường lý luận chính xác và mạch lạc về mặt ngữ nghĩa.

**Khi nào ReFT vượt trội so với SFT?** Để điều tra thêm mối quan hệ giữa ReFT và SFT, chúng tôi thực hiện huấn luyện ReFT với số bước khởi động khác nhau từ SFT. Hình 5 cho thấy độ chính xác giá trị của các biến thể ReFT khác nhau so với SFT. Cụ thể, nếu bước warmup là 3, điều đó có nghĩa là chính sách khởi tạo từ checkpoint SFT epoch thứ 3. Chúng ta có thể thấy hiệu suất của tất cả chính sách ReFT giảm ngay sau warmup ở đầu, cho đến khi epoch huấn luyện đạt khoảng 8. Vì lớp tuyến tính trong mô hình giá trị chia sẻ được khởi tạo ngẫu nhiên, và có thể mất vài epoch để điều chỉnh phân phối. Bắt đầu từ epoch thứ 30, SFT hội tụ và tất cả biến thể ReFT vẫn đang cải thiện. Chúng ta cũng có thể thấy tất cả biến thể đều vượt trội so với SFT với biên độ đáng kể và không có lợi thế rõ ràng của bất kỳ biến thể ReFT cụ thể nào.

## 6 Kết luận

Chúng tôi đã giới thiệu tinh chỉnh có tăng cường (ReFT) như một phương pháp mới để tinh chỉnh mô hình giải toán. Ngược lại với SFT, ReFT tối ưu hóa một mục tiêu không khả vi bằng cách khám phá nhiều chú thích CoT trong việc tìm kiếm câu trả lời đúng, thay vì dựa vào một chú thích duy nhất.

Thông qua thí nghiệm mở rộng trên ba tập dữ liệu sử dụng hai mô hình nền, chúng tôi đã chứng minh rằng ReFT vượt trội so với SFT về hiệu suất và khả năng tổng quát hóa. Hơn nữa, chúng tôi đã thể hiện tính tương thích của các mô hình được huấn luyện với ReFT với các kỹ thuật như bỏ phiếu đa số (Wang et al., 2023b) và xếp hạng lại mô hình phần thưởng (Cobbe et al., 2021a; Uesato et al., 2022).

Hơn nữa, ReFT đã thể hiện hiệu suất vượt trội so với một số mô hình mã nguồn mở có sẵn công khai có kích thước tương đương trong giải toán. Điều này chứng minh tính hiệu quả và giá trị thực tiễn của phương pháp ReFT.

## 7 Nghiên cứu tương lai

Chúng tôi đã thực hiện nỗ lực đầu tiên áp dụng học tăng cường, cụ thể là thuật toán PPO (Schulman et al., 2017), vào tinh chỉnh LLMs cho giải toán. Nghiên cứu tương lai của chúng tôi bao gồm việc sử dụng các kỹ thuật học tăng cường offline (Levine et al., 2020; Gulcehre et al., 2023), phát triển phương pháp không cần khởi động để nâng cao hiệu quả và hiệu suất huấn luyện, từ đó giảm khoảng cách với phương pháp xếp hạng lại. Ngoài ra, Lightman et al. (2023) gợi ý rằng một mô hình phần thưởng dựa trên quy trình (PRM) được huấn luyện tốt có thể nâng cao đáng kể hiệu suất. Do đó, sẽ đáng để khám phá việc triển khai phần thưởng dựa trên quy trình trong huấn luyện học tăng cường. Cuối cùng, vì ReFT là một phương pháp linh hoạt, chúng tôi dự định áp dụng nó cho các tác vụ lý luận tổng quát hơn trong đó suy luận có thể được hình thức hóa với CoT.

## Hạn chế

**Hiệu quả Huấn luyện** Như được mô tả trong Hình 4 (b), rõ ràng là ReFT cần số epoch lớn hơn để đạt hội tụ so với SFT. Điều này chủ yếu do ReFT tối ưu hóa một mục tiêu không khả vi và yêu cầu khám phá không gian tạo để đạt được câu trả lời đúng. Trong khi learning rate lớn hơn có thể đẩy nhanh hội tụ, nó cũng làm cho chính sách dễ bị bất ổn định và có khả năng sụp đổ. Thay vào đó, sử dụng kích thước batch lớn hơn là một lựa chọn khả thi; tuy nhiên, nó có chi phí tính toán tăng lên.

**Reward Hacking** Hàm phần thưởng của chúng tôi chỉ dựa vào câu trả lời cuối cùng để xác định phần thưởng. Tuy nhiên, như đã chứng minh trong các thí nghiệm được thực hiện trên tập dữ liệu MathQA MCQ N-CoT, chính sách có thể dễ dàng bị thao túng nếu không gian có thể của câu trả lời cuối cùng bị hạn chế, chẳng hạn như A,B,C,D. Để giảm thiểu vấn đề reward hacking, có thể cần sử dụng hàm phần thưởng chi tiết hơn hoặc dựa trên quy trình có tính đến phạm vi yếu tố rộng hơn.
