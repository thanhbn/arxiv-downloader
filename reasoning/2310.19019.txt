# 2310.19019.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/reasoning/2310.19019.pdf
# File size: 348157 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish,
Language Modeling Likewise
Nan He∗1Hanyu Lai∗2Chenyang Zhao∗2Zirui Cheng2Junting Pan3Ruoyu Qin2Ruofan Lu2Rui Lu2
Yunchen Zhang4Gangming Zhao5Zhaohui Hou6Zhiyuan Huang6Shaoqing Lu7Ding Liang7
Mingjie Zhan7
Abstract
Large Language Models (LLMs) exhibit impres-
sive reasoning and data augmentation capabilities
in various NLP tasks. However, what about small
models? In this work, we propose TeacherLM-
7.1B, capable of annotating relevant fundamen-
tals, chain of thought, and common mistakes
for most NLP samples, which makes annotation
more than just an answer, thus allowing other
models to learn “why” instead of just “what”.
The TeacherLM-7.1B model achieved a zero-shot
score of 52.3 on MMLU, surpassing most models
with over 100B parameters. Even more remark-
able is its data augmentation ability. Based on
TeacherLM-7.1B, we augmented 58 NLP datasets
and taught various student models with differ-
ent parameters from OPT and BLOOM series
in a multi-task setting. The experimental results
indicate that the data augmentation provided by
TeacherLM has brought significant benefits. We
will release the TeacherLM series of models and
augmented datasets as open-source.
1. Introduction
Large Language Models have recently revolutionized the
NLP landscape (Brown et al., 2020; Rae et al., 2021; Chowd-
hery et al., 2022; Hoffmann et al., 2022; Zeng et al., 2022;
Black et al., 2022; Wei et al., 2022a; Taylor et al., 2022).
Compared to the increase in model size, an essential thing
for achieving a deeper understanding of language is to utilize
data effectively. Unfortunately, most NLP datasets have sim-
*Equal contribution1University of the Chinese Academy of
Sciences2Tsinghua University3The Chinese University of Hong
Kong4University of Electronic Science and Technology of China
5The University of Hong Kong6Beijing University of Posts and
Telecommunications7SenseTime Research. Correspondence to:
Mingjie Zhan <zmjdll@gmail.com >.ple input-output formats, inconsistent with the data seen dur-
ing the pre-training of language models. As a result, directly
finetuning such data is insufficient for the model to fully
understand the comprehensive content of samples, inducing
difficulty in fully utilizing its learning capabilities. Thus,
two leading data augmentation strategies have emerged to
address this issue, including task-level and instance-level
approaches.
In task-level data augmentation, the aim is to map any natu-
ral language task into multiple human-readable prompted
forms with diverse wording (Wei et al., 2021; Bach et al.,
2022; Wang et al., 2022b). Furthermore, combining task-
level augmentation with multi-task training leads to a more
comprehensive ability. Finetuning a pre-trained model on
this multi-task mixture covering various tasks (Sanh et al.,
2021; Chung et al., 2022; Muennighoff et al., 2022; Iyer
et al., 2022) induces the language model a more substantial
zero-shot and few-shot capabilities.
Though task-level augmentation has pushed the language
model’s generalizability to a new height, each sample repre-
sents an individual entity, limiting the augmentation, which
treats the entire dataset using a unified data augmentation
method. Hence a more effective approach is to augment
each sample individually based on its unique characteristics,
with a new stage flourishing in instance-level data augmenta-
tion. For this approach, retrieval-based pre-trained language
models (Borgeaud et al., 2021; Izacard et al., 2022) utilize
small models in conjunction with a massive database to
introduce more relevant knowledge for each sample. Mean-
while, LLMs can use their solid zero-shot ability to augment
each sample based on different prompts (Wang et al., 2021;
Ho et al., 2022). Nevertheless, these two methods bring
huge usage costs, and some of the current best-performing
models are not open-source.
In order to reduce the cost of data augmentation, in this work,
we propose to open-source a series of small TeacherLM
models, which rival human annotation and LLMs. As an
adage goes, “It is better to teach someone how to fish than
to give them the fish,” which also applies to language mod-
els. To achieve this purpose, our approach has two primaryarXiv:2310.19019v3  [cs.CL]  16 Jul 2024

--- PAGE 2 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
Question: Determine following sentence’s correctness. Boyle’s law
correlates the pressure and volume of a gas. (A) True (B) False
Answer: The answer is (A).Input
TeacherLM
Step 1: Boyle’s law states that when the remaining conditions are
constant, then P1V1= P 2V2. Step 2: The given statement is Boyle’s
law correlates the pressure and volume of a gas. Hence the statement
is correct.Output by
prompting Let’s
think step by
step.
The mathematical expression for Boyle’s law: P∝1
V, Here, Pis
the pressure of the gas and Vis the volume of the gas. It can also be
expressed as follows: P = K1
V. Here, K = Constant.Output by
prompting The
fundamental of
this question is:
Do not get confused between Boyles’s law and Charles’s law.
Charles’s law states the relationship between the volume and tem-
perature of a gas under constant other conditions. It states as follows:
V1T2= V 2T1.Output by
prompting The
common mis-
takes are:Dataset
Multiple-Choice QA,
Closed-Book QA,
Natural Language
Inference, Sentiment,
Topic Classification...
TeacherLM
Augmented Dataset
Student Model
−10010∆Accuracy (%)Augmented v.s. Unaugmented
Augmented >Unaugmented Augmented <Unaugmented
high school statistics
high school government politicssecurity studies
high school macroeconomicsmarketing
us foreign policyconceptual physics
high school chemistry
college computer sciencehigh school biologyprehistory
college medicine
high school european historycomputer securitypublic relationscollege biology
high school microeconomicscollege physicsinternational lawnutrition
electrical engineeringeconometricsanatomy
high school world historycollege chemistry
college mathematics
high school computer sciencemedical genetics
high school psychologyclinical knowledgeformal logic
human sexualitysociology
management
moral disputesphilosophy
miscellaneous
high school geographyvirology
world religions
professional medicineprofessional accountingglobal factshuman aging
professional law
high school us historyprofessional psychologybusiness ethicsmoral scenarios
elementary mathematicslogical fallacies
high school mathematicshigh school physicsjurisprudence
abstract algebramachine learningastronomy
Figure 1: TeacherLMs can perform augmentation on a wide range of datasets. They can leverage three different prompts
to generate augmentations, including fundamentals, CoT, and common mistakes, providing complete information needed
to solve the problem. The results of multitask training using augmented and unaugmented P3-Sense-3K data show that
the augmented data can make the BLOOM-7.1B student model perform better in zero-shot performance on 47 tasks in the
MMLU(57 tasks) benchmark.
intuitions. Firstly, we believe that the real need for aug-
mentation in a dataset lies in each sample’s label, where
the model should learn “why” instead of just remember-
ing “what”. Our goal is to shift the learning objectives of
language models from results-oriented to process-oriented,
moving away from rote memorization towards a more holis-tic understanding to break through current limitations im-
posed on language model capabilities. Secondly, language
models should mimic humans’ learning process by simul-
taneously grasping each sample’s relevant fundamentals,
chain of thought, and common mistakes to understand the
training objectives more comprehensively.

--- PAGE 3 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
To achieve this, we define the learning process as com-
prising three dimensions: fundamental, chain of thought,
and common mistake. Our ultimate goal is to annotate
this information for every sample in any NLP dataset. To
ensure that TeacherLM has strong zero-shot capabilities
for most natural language processing tasks, we have col-
lected 2 million samples from multiple domains for train-
ing. Furthermore, we combined manual annotation and
STaR (Zelikman et al., 2022) strategy to construct a com-
plete “ {Question } {Answer } {Fundamentals } {Chain of
Thought } {Common Mistakes }” five-element training ob-
ject for each sample.
In sum, our key contributions are:
•Comprehensive TeacherLM can generate fundamen-
tals, chain of thought, and common mistakes, providing
comprehensive information tailored to the task’s char-
acteristics and allowing each task to learn the most
relevant knowledge.
•Generalizability TeacherLM is suitable for a vari-
ety of datasets and models. As shown in Figure
1, we augmented 58 NLP datasets and taught vari-
ous student models with different parameters from
OPT (Zhang et al., 2022), and BLOOM (Scao et al.,
2022) series in a multi-task setting. Compared to non-
augmented versions, the experimental results indicate
that TeacherLM’s data augmentation gains clear bene-
fits.
•Fight big with small TeacherLM-7.1B model achieved
a zero-shot score of 52.3 on MMLU, surpassing most
models with over 100B parameters.
•Cost friendly TeacherLM-7.1B has only 7.1 billion
parameters; compared to models such as text-davinci-
003 , it has efficient inference speed and lower running
configuration requirements. Therefore, with the sig-
nificant cost reduction, we can augment NLP datasets
of millions of levels, further opening the door to the
reasoning world.
•Open source We will release the TeacherLM series of
models and augmented datasets as open-source.
2. Related Work
This paper explores the intersection of various NLP re-
search fields, including multi-task learning, instruction tun-
ing, multi-step reasoning, and data augmentation. In this
section, we will introduce several key related works.
Reasoning via finetuning In this study, we construct a
large-scale Reasoning dataset for training TeacherLM. Pre-
vious works have utilized manually annotated multi-stepreasoning to improve model performance (Ling et al., 2017;
Camburu et al., 2018; Rajani et al., 2019; Hu et al., 2024;
Talmor et al., 2020; Cobbe et al., 2021; Nye et al., 2021;
Zelikman et al., 2022; Chung et al., 2022). Compared to
these works, TeacherLM-7.1B has certain advantages when
compared to models of the same scale.
Using large models as zero-Shot data augmentation gen-
erators Combining chain of thought and in-context learning
has unlocked more robust reasoning capabilities for LLMs
(Wei et al., 2022b; Viswanathan et al., 2023; Suzgun et al.,
2022; Lampinen et al., 2022), guiding models to move from
learning to rote information to now learning to think crit-
ically. Furthermore, by adding the sentence “Let’s think
step by step” before LLMs generate answers, the model can
generate a step-by-step thought process and significantly
improve accuracy in solving reasoning tasks(Kojima et al.,
2022). Therefore, Large Language Models are zero-shot
Reasoners and can also be considered as Zero-Shot data
augmentation generators.
Instruction finetuning Multi-task learning improves the
performance of language models in zero-shot settings. Many
works have found that designing elaborated natural language
templates with instructions for each NLP task and connect-
ing them breaks down barriers between tasks and allows the
language model to understand the data better (Wei et al.,
2021; Sanh et al., 2021; Ouyang et al., 2022; Chen et al.,
2024; Wang et al., 2022b; Scialom et al., 2022; Chung et al.,
2022; Muennighoff et al., 2022; Iyer et al., 2022). In our
work, we combine instruction finetuning and reasoning to
unlock more potential in language models.
3. Training TeacherLM
A good teacher can be a beacon, guiding students toward
mastering the methods to solve problems. We aim the same
for the TeacherLM. In this regard, we make two primary
efforts. Firstly, we construct a dataset comprising two mil-
lion detailed explanations. Secondly, we adopt a multi-stage
progressive training mode, moving from generality towards
specialization.
3.1. Dataset Construction
P3-Sense-3K We extract 58 supervised datasets from P3
(Sanh et al., 2021). Each dataset contains multiple prompts,
resulting in 529 tasks. To ensure sample balance, We select
at most 3,000 samples of less than 1,200 tokens for each
task. There are 1,400,364 samples in total. Then we for-
mat multiple choice tasks in the form of “ Q:{question }
{options }A:{answer }”. All other tasks are changed to the
form of “ Q:{question }A:{answer }”.
Muffin-3W We extract 56 supervised datasets from Muffin
(Wei et al., 2021), and each dataset includes ten prompts.

--- PAGE 4 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
0.5 2 10 40 160 64020406080Average Human Expert
Average Human Rateroursoursoursoursours
Parameters (Billion)Accuracy (%)
TeacherLM-560M TeacherLM-1.1B TeacherLM-3B TeacherLM-7.1B
TeacherLM-176B Flan-PaLM-8B Flan-PaLM-62B Flan-PaLM-540B
Gopher-1.4B Gopher-7.1B Gopher-280B GPT-3-13B
GPT-3-175B Atlas-11B GPT-Neox-20B Chinchilla-70B
GAL-120B GLM-130B BLOOM-176B
Figure 2: Average MMLU scores (%) for 57 tasks with model and human accuracy comparisons. TeacherLMs are in the
0-shot setting, and the rest are in the 5-shot setting.
Similarly, We select at most 30,000 samples of less than
1,200 tokens for each task. There are 1,155,767 samples in
total. All tasks are changed to the form of “ Q:{question }
A:{answer }”.
TeacherData-2M Furthermore, we collect 2 million pieces
of multi-domain data not included in any public datasets for
training. We utilize manual annotation and the STaR (Zelik-
man et al., 2022) strategy to construct a five-element fixed
paradigm for each sample, including the question, answer,
fundamentals, chain of thought, and common mistakes. We
select samples of less than 2048 tokens.
3.2. Training Procedure
Training the teacher models aims to obtain checkpoints
excelling at generating comprehensive explanations with
solid generalization ability. In this section, we introduce our
base models and the details of the multi-stage training.
3.2.1. M ODELS
In this work, our base models are the BLOOM (Scao et al.,
2022) series ranging from 560 million to 176 billion pa-
rameters, which are pre-trained on the ROOTS (Lauren c ¸on
et al., 2022) corpus in 46 natural languages and 13 program-
ming languages. BLOOM models are large decoder-only
language models pre-trained for 350 billion tokens.3.2.2. M ULTI -STAGE TRAINING
For each model, we adopt a multi-stage training procedure.
Multi-task training Previous literature has shown that train-
ing on a large number of tasks with instructions can improve
the model’s zero-shot ability and allow it to generalize well
on unseen tasks (Sanh et al., 2021; Wei et al., 2021). There-
fore, we combine the mixture of P3-Sense-3K and Muffin-
3W to conduct multi-task training.
Personalization training Using the TeacherData-2M
dataset, the model simultaneously learns to analyze each
sample’s fundamentals, chain of thought, and common mis-
takes. Through this process, the models can generate three
types of explanations.
Specialization learning We split the TeacherData-2M
dataset and train three independent models to focus on
learning fundamentals, chain of thought, and common
mistakes, respectively. The resulting models are re-
ferred to as TeacherLM-Fundamental, TeacherLM-COT,
and TeacherLM-CommonMistake.
In each stage of training, we use packing (Raffel et al.,
2020) to combine multiple texts, and terminators sepa-
rate different texts. The interaction between texts is elim-
inated by setting the attention mask and resetting the po-
sition id. Models of different sizes use different learning
rates and batchsizes. According to the number of param-
eters from small to large, they are 3e-4/256(TeacherLM-

--- PAGE 5 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
560M), 1e-4/512(TeacherLM-1.1B), 4e-5/512(TeacherLM-
3B), 2e-5/768(TeacherLM-7.1B), 6e-5/1024(TeacherLM-
176B). More information can be seen in Appendix A.1. Af-
ter the first stage, we evaluate the models’ performance on
the MMLU (Hendrycks et al., 2020) benchmark for select-
ing checkpoints for further training. Then, after the second
and third stages, apart from evaluating performance on the
MMLU benchmark, we conduct a manual evaluation as a
reference for selecting checkpoints.
3.3. Evaluation Protocol
3.3.1. B ENCHMARK
For the teacher models, we focus on the performance on the
held-out tasks not included in the training procedure. Specif-
ically, we evaluate the teacher models on the MMLU(57
tasks) benchmark in the zero-shot setting. The benchmark
consists of exam-like questions on academic subjects such
as mathematics, history, law, and medicine.
3.3.2. E VALUATION METHOD
The evaluation adopts the method of rank classification. In
more detail, assume answers are from a fixed set A, and
answer i∈A, where i= 1, ..., n ,nindexes ncandidate
answers. Calculate the probability of each candidate answer
using the Formula:
P(answer i|q) =1
KKX
k=1logP(ak|q, a 1, ..., a k−1)(1)
HerelogP(ak|q, a 1, ..., a k−1)is the log probability of gen-
erating the k-th token akinanswer iconditioned on the
previous tokens. Kis the total number of tokens in answer i
andqis the question. Choose the answer with the highest
probability and calculate average accuracy on all tasks.
3.4. Results
3.4.1. Z ERO-SHOT SETTING
We show the MMLU scores with model and human compar-
isons in Figure 2. There are some key points.
First, models with few parameters can still benefit from
our datasets. Even our 560M model exceeds BLOOM-
176B. Moreover, scaling the CoT data to a larger size can
improve zero-shot performance. In FLAN-PaLM (Chung
et al., 2022), nine CoT datasets are added to improve the
performance on the held-out tasks. Further scaling the num-
ber of CoT samples to 2 million in this work shows fantastic
improvement. The TeacherLM-7.1B model has achieved a
zero-shot score of 52.3 on the MMLU, surpassing the 5-shot
performance of most hundred billion parameter models. Ad-
ditionally, the zero-shot score of TeacherLM-176B is 59.8,
comparable to the 5-shot score of gopher-280B (Rae et al.,2021). The full results are in Appendix A.2.
3.4.2. M ULTI -STAGE TRAINING HELPS A L OT
We show the evaluation results during multi-stage training
in Table 1. In contrast to multi-stage training, we also
mixed all datasets and trained on them directly. The training
hyperparameters and the number of steps are consistent. We
find that directly blending all datasets scores much lower
than multi-stage training. However, only the models using
CoT data obtained continuous improvement in the third
stage. This phenomenon may indicate that tasks of the
MMLU benchmark require higher reasoning ability.
3.4.3. A DDSUBJECT TOPROMPT
Inspired by the fact that the MMLU benchmark is composed
of different subject tasks, we added the prompt “ The fol-
lowing are multiple choice questions (with answers) about
{subject name }” at the beginning of each text. We labeled
the subject of each sample of TeacherData-2M. This ap-
proach increases the score of TeacherLM-7.1B by 2%. We
think such a prompt is more helpful for the model to use the
correct knowledge to answer the question.
4. Teaching Student
In this section, we select TeacherLM-7.1B as the teacher for
the following experiments in order to balance efficiency and
performance. Our experiment contains three dimensions to
verify the ability of TeacherLM. The first dimension is the
choice of different training modes, divided into multi-task
training followed by zero-shot testing and single-task fine-
tuning followed by testing in the corresponding task. The
second dimension is the scaling of the model size. The third
dimension is the diversity of the test tasks.
4.1. Datasets
We selected P3-Sense-3K for multi-task training. More-
over, for single-task fine-tuning, we aimed to compare
TeacherLM-7.1B’s enhancement effects with manually an-
notated rationales. Thus we selected (1) StrategyQA (Geva
et al., 2021), a question-answering benchmark with implicit
reasoning strategies. Due to the unavailability of the test
dataset, the training dataset was was randomly redivided
into training, validation, and test sets in an 8:1:1 ratio. (2)
CREAK (Onoe et al., 2021), a dataset for commonsense rea-
soning over entity knowledge. Since the test dataset lacks
labels, we directly evaluated the dev dataset. (3) ECQA
(Aggarwal et al., 2021), a dataset containing explanations
for commonsenseQA.

--- PAGE 6 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
Table 1: Average MMLU scores(%) for 57 tasks with multi-stage training and no-stage training comparisons. C, F, M
represent CoT, fundamentals, common mistakes respectively. No stage represents directly mixing all datasets for training.
TeacherLM-176B only completed part of the training process and only trained on TeacherData-2M.
parameters 1st stage 2nd stage 3rd stage(C) 3rd stage(F) 3rd stage(M) No stage
560M 31.00 36.47 38.39 34.65 34.85 35.27
1.1B 32.87 41.36 41.38 39.25 38.75 40.26
3B 36.51 46.34 46.74 45.22 45.1 46.01
7.1B 41.47 51.11 52.30 50.32 49.00 47.00
176B / 59.80 / / / /
Table 2: Dataset settings in section 4. Here shows option amounts, example amounts, and average tokens in the training split
of each dataset. Manual denotes human annotated rationales in original datasets. CoT ,Fud, and Mis are TeacherLM’s
generated CoT, fundamentals, and common mistakes, while CoT-D ,Fud-D , and Mis-D are text-davinci-003’s generated
CoT, fundamentals and common mistakes, through the same prompts inputted to TeacherLM.
DATASET OPTIONSEXMAPLE AMOUNTS AVERAGE TOKENS
TRAIN TEST MANUAL COT F UD MIS COT-D F UD-D M IS-D
ECQA 5 7598 2194 59 75 186 53 47 27 32
STRATEGY QA 2 1832 228 28 90 192 55 73 48 31
CREAK 2 10174 1371 15 71 190 48 42 25 19
P3- SENSE -3K / 1400364 / / 49 135 54 / / /
4.2. Models and training details
We used the three models in the TeacherLM-7.1B series
to augment each sample in the above four datasets. The
augmented samples, as shown in Figure 4, include five parts
of information: question, answer, fundamentals, chain of
thought, and common mistakes. For more information on
the above datasets, please refer to Table 2.
In the multi-task training mode, we evaluated the benefit of
the P3-Sense-3K dataset augmented by TeacherLM-7.1B
on various models. The three parts of TeacherLM’s explana-
tions are concatenated into sequence with the answer. The
control group consisted of the original P3-Sense-3k dataset,
containing only question and answer pairs. We increased the
student model size from 1.1B to 7.1B and set the learning
rate for all experiments in the multi-task training mode to
2e-5 and the batch size to 256.
For the single-task fine-tuning, we select BLOOMZ-7.1B
as the student model, which has been fine-tuned on xP3,
a composite of supervised datasets in 46 languages with
English and machine-translated prompts. In the single-task
fine-tuning, we set the learning rate of all experiments to
6e-6 and the batch size to 64.
4.3. Comparison with human and text-davinci-003
To further validate model-generated explanations’ qual-
ity, we include human annotation and text-davinci-003as control groups in our experiments, where text-davinci-
003 serves as the teacher and augments the Strate-
gyQA, CREAK, and ECQA datasets in the same way as
TeacherLM-7.1B.
Apart from training results in section 4.4, we found that
manual explanations and text-davinci-003’s augmentation
both read smoothly but are inherently less detailed than
TeacherLM-7.1B’s. Nevertheless, text-davinci-003 some-
times reiterates the content of the question, rendering limited
augmented information that could help the student model
improve its abilities.
4.4. Results
Multi-task training In this experiment section, the model
underwent extensive data training and thoroughly learned
the rationales of data augmentation. As shown in Figure
3, models of different sizes were able to bring significant
benefits in general. The full experiment results can be seen
in appendix B.
Single-task training In this section, we divided the experi-
ment into two parts. First, we directly fine-tune BLOOMZ-
7.1B for each task. In the StrategyQA and CREAK datasets,
TeacherLM-7.1B performs beyond manual annotation and
has a more robust augmentation ability than text-davinci-
003 in the CREAK task.
Second, we train BLOOMZ-7.1B on the augmented P3-

--- PAGE 7 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
0 2 42530
+1.84Bloom-1B1MMLU
0 2 450556065
+3.94CREAK
0 2 420304050
+3.42ECQA
0 2 445505560
+3.07StrategyQA
0 2 4253035
+1.97Bloom-3B
0 2 4506070
+1.09
0 2 4204060
+2.14
0 2 455606570
-0.44
0 2 42530354045
+3.98Bloom-7B10 2 4506070
+5.32
0 2 4204060
+1.96
0 2 4506070
+3.95
0 2 4253035
+2.66OPT-1B3
0 2 4506070
+1.97
0 2 4204060
+2.55
0 2 45060
+3.95
0 2 425303540
+1.84OPT-2B7
0 2 4506070
+3.28
0 2 4204060
+0.77
0 2 4506070
+3.07
0 2 425303540
+3.32OPT-6B7
0 2 4506070
+2.12
0 2 4204060
0.00
0 2 4506070
+2.63
Number of Tokens (Million)Accuracy (%)
Plain Multi-task Unaugmented Multi-task Augmented
Figure 3: Different student models’ performance in terms of augmentation of TeacherLM-7.1B. We augmented P3-Sense-
3K(58 NLP datasets) and trained student models using the multi-task method. The x-axis indicates training token numbers,
while the y-axis indicates zero-shot scores on various datasets. The gray line labeled “Plain” represents student models’
accuracy without training. The orange line represents accuracy fluctuation when student models are trained directly on the
original P3-Sense-3K, and the blue line represents the effect of training on the augmented P3-Sense-3K dataset.
Sense-3K dataset, obtaining P3-Augmented-BLOOMZ-
7.1B, and then fine-tune it on single tasks. The experiment
shows that this further improved the accuracy of the task
and, to some extent, solved the problem of insufficient data.
In StrategyQA and CREAK, TeacherLM-7.1B also shows
the ability to enhance data beyond human annotation and
text-davinci-003.
In addition, we found that including manual rationales dur-
ing training can harm some datasets, such as ECQA. Our
model was able to alleviate some problems to some extent,
but it could not completely solve them.5. Discussion & Conclusion
Which part of data augmentation is the most important?
It cannot be generalized to all datasets. There are unavoid-
able differences between datasets, which often lead to vary-
ing requirements for data augmentation content, as shown
in Table 3. Therefore, CoT, fundamentals, and common mis-
takes form a complementary relationship, and these three
elements can be separated or combined in different orders
to enhance samples. We conducted detailed experiments, as
shown in Appendix B. However, in general, CoT brings the
most benefits.
How can data augmentation effects be maintained in the
case of limited data?

--- PAGE 8 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
Table 3: Single-task finetuning results for the BLOOMZ-7.1B model and the P3-Augmented-BLOOMZ-7.1B model. Text
means no data augmentation for the corresponding task, and manual means using manual CoT. Each result of TeacherLM-
7.1B and text-davinci-003, from left to right, represents the score after data augmentation using the corresponding model to
generate CoT, fundamentals, and common mistakes.
TASK TEXT MANUAL TEACHER LM-7.1B TEXT -DAVINCI -003
BLOOMZ-7.1B
ECQA 71.9 61.9 61.9 / 61.9 / 61.9 64.0 / 64.0 / 64.4
STRATEGY QA 71.1 71.1 65.8 / 73.3 / 57.0 (+2.2) 66.7 / 79.0 / 69.3 (+7.9)
CREAK 64.0 73.3 (+9.3) 77.0 / 54.8 / 58.2 (+13.0) 74.6 / 69.6 / 76.7 (+12.7)
P3-A UGMENTED -BLOOMZ-7.1B
ECQA 68.6 53.1 60.5 / 58.3 / 65.0 60.1 / 67.7 / 56.6
STRATEGY QA 72.4 68.9 74.1 / 77.2 / 74.1 (+4.8) 65.8 / 75.4 / 71.9 (+3.0)
CREAK 76.7 78.6 (+1.9) 75.9 / 75.8 / 79.0 (+2.3) 70.9 / 70.0 / 70.3
Question: Fact 1: Climate is generally described in terms of temperature and moisture. Fact 2: Humidity is a measure of mois-
ture content of air. Climate is generally described in terms of temperature and what?
(A) humidity (B) wind (C) warmer air (D) calibrated (E) percentages (F) matter (G) color (H) SI units
Answer: The answer is (A).
Manual Explanations
Climate is generally described in terms of temperature and humidity.
text-davinci-003 by prompting Let’s think step by step.
Fact 1 states that climate is generally described in terms of temperature and moisture. Fact 2 states that humidity is a measure of
moisture content of air. Therefore, the answer is (A) humidity.
text-davinci-003 by prompting The fundamental of this question is:
Climate is generally described in terms of temperature and moisture. Humidity is a measure of moisture content of air, so the
correct answer is (A) humidity.
text-davinci-003 by prompting The common mistakes are:
(B) wind, (C) warmer air, (D) calibrated, (E) percentages, (F) matter, (G) color, and (H) SI units.
TeacherLM-7.1B by prompting Let’s think step by step.
Various physical properties describe the climate. One of them is humidity. The climate is generally described in terms of tempera-
ture and humidity. So, option humidity is the correct answer.
TeacherLM-7.1B by prompting The fundamental of this question is:
Climate refers to the weather pattern in a long-term condition in a specific region. Several factors influence the climate of a spe-
cific region. Temperature, wind speed, humidity, precipitation, and solar radiation are the important significant factors that influ-
ence the climate of a specific region.
TeacherLM-7.1B by prompting The common mistakes are:
Do not think that climate is simply determined by temperature alone. Temperature, humidity, wind speed, and other factors also
play an important role in the climate of a region. Do not consider that climate is not measured in humid regions. Climate is mea-
sured in terms of humidity also.
Figure 4: Example showing a common sense question, with its manual explanations from the original dataset and CoT,
fundamentals, and common mistakes generated by text-davinci-003 and TeacherLM-7.1B-7.1B, from top to bottom.
In addition to the TeacherLM-7.1B model, we also released
the augmented P3-Sense-3K dataset. If the data is limited,
multi-task training can first be performed on this dataset and
then fine-tuned on the small dataset, as Table 3 shows.
What did the student model learn from the augmented
data?
Learn by analogy. Though many of the rationales gener-ated by TeacherLM are not always correct, in this paper,
no measures were taken to filter the augmented content,
and the student model still produces significant benefits.
The correctness of the augmented content is not the only
important factor. The relevance of the content and the con-
sistency of the reasoning logic are also significant (Wang
et al., 2022a), which allows the student models to learn to
think critically and truly enhance the model’s generalization

--- PAGE 9 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
ability on unseen tasks.
In comparison to human annotation and text-davinci-
003, what are the characteristics and deficiencies of the
generation content of TeacherLM-7.1B?
As shown in Figure 4 and Appendix C, TeacherLM-7.1B’s
explanations are generally more comprehensive and detailed
than human annotations and text-davinci-003’s explanations.
However, it falls behind text-davinci-003 in solving mathe-
matical problems, probably related to the model’s size.
References
Aggarwal, S., Mandowara, D., Agrawal, V ., Khandelwal,
D., Singla, P., and Garg, D. Explanations for common-
senseqa: New dataset and models. In Workshop on Com-
monsense Reasoning and Knowledge Bases , 2021.
Bach, S. H., Sanh, V ., Yong, Z.-X., Webson, A., Raffel, C.,
Nayak, N. V ., Sharma, A., Kim, T., Bari, M. S., Fevry, T.,
et al. Promptsource: An integrated development environ-
ment and repository for natural language prompts. arXiv
preprint arXiv:2202.01279 , 2022.
Black, S., Biderman, S., Hallahan, E., Anthony, Q., Gao,
L., Golding, L., He, H., Leahy, C., McDonell, K., Phang,
J., et al. Gpt-neox-20b: An open-source autoregressive
language model. arXiv preprint arXiv:2204.06745 , 2022.
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Ruther-
ford, E., Millican, K., Driessche, G. v. d., Lespiau, J.-B.,
Damoc, B., Clark, A., et al. Improving language mod-
els by retrieving from trillions of tokens. arXiv preprint
arXiv:2112.04426 , 2021.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:
1877–1901, 2020.
Camburu, O.-M., Rockt ¨aschel, T., Lukasiewicz, T., and
Blunsom, P. e-snli: Natural language inference with
natural language explanations. Advances in Neural Infor-
mation Processing Systems , 31, 2018.
Chen, W., You, Z., Li, R., Guan, Y ., Qian, C., Zhao, C.,
Yang, C., Xie, R., Liu, Z., and Sun, M. Internet of agents:
Weaving a web of heterogeneous agents for collaborative
intelligence. arXiv preprint arXiv:2407.07061 , 2024.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311 , 2022.Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y .,
Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma,
S., et al. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 , 2022.
Cobbe, K., Kosaraju, V ., Bavarian, M., Chen, M., Jun, H.,
Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,
R., et al. Training verifiers to solve math word problems.
arXiv preprint arXiv:2110.14168 , 2021.
Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and
Berant, J. Did aristotle use a laptop? a question answering
benchmark with implicit reasoning strategies. Transac-
tions of the Association for Computational Linguistics , 9:
346–361, 2021.
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
M., Song, D., and Steinhardt, J. Measuring mas-
sive multitask language understanding. arXiv preprint
arXiv:2009.03300 , 2020.
Ho, N., Schmid, L., and Yun, S.-Y . Large language models
are reasoning teachers. arXiv preprint arXiv:2212.10071 ,
2022.
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,
Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A.,
Welbl, J., Clark, A., et al. Training compute-optimal
large language models. arXiv preprint arXiv:2203.15556 ,
2022.
Hu, S., Tu, Y ., Han, X., He, C., Cui, G., Long, X., Zheng, Z.,
Fang, Y ., Huang, Y ., Zhao, W., et al. Minicpm: Unveiling
the potential of small language models with scalable train-
ing strategies. arXiv preprint arXiv:2404.06395 , 2024.
Iyer, S., Lin, X. V ., Pasunuru, R., Mihaylov, T., Simig,
D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S.,
et al. Opt-iml: Scaling language model instruction meta
learning through the lens of generalization. arXiv preprint
arXiv:2212.12017 , 2022.
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,
F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and
Grave, E. Few-shot learning with retrieval augmented lan-
guage models. arXiv preprint arXiv:2208.03299 , 2022.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,
Y . Large language models are zero-shot reasoners. arXiv
preprint arXiv:2205.11916 , 2022.
Lampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson,
K., Tessler, M. H., Creswell, A., McClelland, J. L., Wang,
J. X., and Hill, F. Can language models learn from ex-
planations in context? arXiv preprint arXiv:2204.02329 ,
2022.

--- PAGE 10 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
Lauren c ¸on, H., Saulnier, L., Wang, T., Akiki, C., del Moral,
A. V ., Le Scao, T., V on Werra, L., Mou, C., Ponferrada,
E. G., Nguyen, H., et al. The bigscience roots corpus:
A 1.6 tb composite multilingual dataset. In Thirty-sixth
Conference on Neural Information Processing Systems
Datasets and Benchmarks Track , 2022.
Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Pro-
gram induction by rationale generation: Learning to solve
and explain algebraic word problems. arXiv preprint
arXiv:1705.04146 , 2017.
Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Bi-
derman, S., Scao, T. L., Bari, M. S., Shen, S., Yong, Z.-X.,
Schoelkopf, H., et al. Crosslingual generalization through
multitask finetuning. arXiv preprint arXiv:2211.01786 ,
2022.
Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,
Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma,
M., Luan, D., et al. Show your work: Scratchpads for
intermediate computation with language models. arXiv
preprint arXiv:2112.00114 , 2021.
Onoe, Y ., Zhang, M. J., Choi, E., and Durrett, G. Creak:
A dataset for commonsense reasoning over entity knowl-
edge. arXiv preprint arXiv:2109.01653 , 2021.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,
K., Ray, A., et al. Training language models to fol-
low instructions with human feedback. arXiv preprint
arXiv:2203.02155 , 2022.
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann,
J., Song, F., Aslanides, J., Henderson, S., Ring, R.,
Young, S., et al. Scaling language models: Methods,
analysis & insights from training gopher. arXiv preprint
arXiv:2112.11446 , 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., Liu, P. J., et al. Exploring
the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res. , 21(140):1–67, 2020.
Rajani, N. F., McCann, B., Xiong, C., and Socher, R. Ex-
plain yourself! leveraging language models for common-
sense reasoning. arXiv preprint arXiv:1906.02361 , 2019.
Sanh, V ., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,
Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,
A., et al. Multitask prompted training enables zero-shot
task generalization. arXiv preprint arXiv:2110.08207 ,
2021.
Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili ´c, S., Hesslow,
D., Castagn ´e, R., Luccioni, A. S., Yvon, F., Gall ´e, M.,
et al. Bloom: A 176b-parameter open-access multilingual
language model. arXiv preprint arXiv:2211.05100 , 2022.Scialom, T., Chakrabarty, T., and Muresan, S. Continual-t0:
Progressively instructing 50+ tasks to language models
without forgetting. arXiv preprint arXiv:2205.12393 ,
2022.
Suzgun, M., Scales, N., Sch ¨arli, N., Gehrmann, S., Tay,
Y ., Chung, H. W., Chowdhery, A., Le, Q. V ., Chi,
E. H., Zhou, D., et al. Challenging big-bench tasks and
whether chain-of-thought can solve them. arXiv preprint
arXiv:2210.09261 , 2022.
Talmor, A., Tafjord, O., Clark, P., Goldberg, Y ., and Be-
rant, J. Leap-of-thought: Teaching pre-trained models
to systematically reason over implicit knowledge. Ad-
vances in Neural Information Processing Systems , 33:
20227–20237, 2020.
Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn,
A., Saravia, E., Poulton, A., Kerkez, V ., and Stojnic, R.
Galactica: A large language model for science. arXiv
preprint arXiv:2211.09085 , 2022.
Viswanathan, V ., Zhao, C., Bertsch, A., Wu, T., and Neu-
big, G. Prompt2model: Generating deployable mod-
els from natural language instructions. arXiv preprint
arXiv:2308.12261 , 2023.
Wang, B., Min, S., Deng, X., Shen, J., Wu, Y ., Zettlemoyer,
L., and Sun, H. Towards understanding chain-of-thought
prompting: An empirical study of what matters. arXiv
preprint arXiv:2212.10001 , 2022a.
Wang, Y ., Mishra, S., Alipoormolabashi, P., Kordi, Y .,
Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran,
A. S., Naik, A., Stap, D., et al. Super-naturalinstructions:
Generalization via declarative instructions on 1600+ nlp
tasks. URL https://arxiv. org/abs/2204.07705 , 2022b.
Wang, Z., Yu, A. W., Firat, O., and Cao, Y . To-
wards zero-label language learning. arXiv preprint
arXiv:2109.09193 , 2021.
Wei, J., Bosma, M., Zhao, V . Y ., Guu, K., Yu, A. W., Lester,
B., Du, N., Dai, A. M., and Le, Q. V . Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 , 2021.
Wei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B.,
Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Met-
zler, D., et al. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682 , 2022a.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le, Q., and Zhou, D. Chain of thought prompting elic-
its reasoning in large language models. arXiv preprint
arXiv:2201.11903 , 2022b.

--- PAGE 11 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
Zelikman, E., Wu, Y ., and Goodman, N. D. Star: Boot-
strapping reasoning with reasoning. arXiv preprint
arXiv:2203.14465 , 2022.
Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,
Yang, Z., Xu, Y ., Zheng, W., Xia, X., et al. Glm-130b:
An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414 , 2022.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V .,
et al. Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 , 2022.

--- PAGE 12 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
A. TeacherLMs
A.1. Training hyperparameters
We show the training hyperparameters of TeacheLMs and the amount of training data at each stage in Table 4.
Table 4: Training hyperparameter settings for each model size. TeacherLM-176B completed part of the training process and
only trained on TeacherData-2M.
parameters learning rate batch size tokens in 1st stage tokens in 2nd stage tokens in 3rd stage
560M 3e-4 256 5B 7.5B 0.5B
1.1B 1e-4 512 5B 7.5B 0.5B
3B 4e-5 512 5B 7.5B 0.5B
7.1B 2e-5 768 0.5B 8B 1.5B
176B 6e-5 1024 / 1B /
A.2. Full experimental results
The evaluation results of TeacherLMs can be seen at Table 5 and Table 6, where we show the MMLU individual task
performance of TeacherLM-560M, TeacherLM-1.1B, TeacherLM-3B, FLAN-PALM-8B, TeacherLM-7.1B, TeacherLM-
176B, GLM-130B, BLOOM-176B and Gopher-280B. Here, we report the “validation” set performance of individual tasks
in MMLU.
B. Complete Benchmarks for All Augmented Trained Models
We evaluate all the augmented trained models on four benchmarks, MMLU, ECQA, CREAK and StrategyQA. We show the
results in Table 7. We evaluate ten kinds of checkpoints, including original pretrained models, models trained with original
P3 dataset, models trained with P3-Sense-3K, and models trained with seven kinds of augmented dataset.
C. More Comparison Examples
Figure 4 shows TeacherLM-7.1B’s explanations for a Physics problem. Apart from Physics, TeacherLM-7.1B also has a
sound ability to analyze History and Geography problems, which we show in Figure 5.

--- PAGE 13 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
Table 5: MMLU individual task performance of TeacherLM-7.1B, TacherLM-176B, GLM-130B, BLOOM-176B, and
Gopher-280B. Furthermore, we denote “college” as “CO” and “high school” as “HS”.
TASK TEACHER LM-7.1B T EACHER LM-176B GLM-130B BLOOM-176B G OPHER -280B
ABSTRACT ALGEBRA 30.00 32.00 24.00 24.00 25.00
ANATOMY 46.67 54.81 48.90 38.52 56.30
ASTRONOMY 51.32 67.76 48.03 34.87 65.80
BUSINESS ETHICS 65.00 67.00 51.00 34.00 70.00
CLINICAL KNOWLEDGE 57.74 62.26 48.68 35.85 67.20
CO BIOLOGY 59.72 66.67 47.22 37.50 70.80
CO CHEMISTRY 42.00 42.00 34.00 19.00 45.00
CO COMPUTER SCIENCE 40.00 35.00 44.00 1.00 49.00
CO MATHEMATICS 34.00 35.00 27.00 31.00 37.00
CO MEDICINE 50.29 58.96 43.35 28.90 60.10
CO PHYSICS 41.18 44.12 30.39 24.50 34.30
COMPUTER SECURITY 64.00 76.00 61.00 40.00 65.00
CONCEPTUAL PHYSICS 51.49 51.06 38.72 31.49 49.40
ECONOMETRICS 35.09 44.74 26.32 26.32 43.00
ELECTRICAL ENGINEERING 53.10 60.00 45.52 32.41 60.00
ELEMENTARY MATHEMATICS 39.42 42.86 31.75 29.63 33.60
FORMAL LOGIC 38.10 39.68 27.78 23.02 35.70
GLOBAL FACTS 29.00 32.00 35.00 23.00 38.00
HSBIOLOGY 62.90 70.97 51.29 27.42 71.30
HSCHEMISTRY 43.35 50.74 34.98 27.09 47.80
HSCOMPUTER SCIENCE 62.00 64.00 53.00 30.00 54.00
HSEUROPEAN HISTORY 61.21 75.76 58.18 35.76 72.10
HSGEOGRAPHY 67.68 78.28 53.54 36.36 76.80
HSGOVERNMENT AND POLITICS 68.39 75.13 62.18 40.41 83.90
HSMACROECONOMICS 55.64 62.82 42.56 30.77 65.10
HSMATHEMATICS 29.26 33.33 28.15 25.93 23.70
HSMICROECONOMICS 60.50 72.27 45.80 26.89 66.40
HSPHYSICS 30.46 37.09 29.80 30.46 33.80
HSPSYCHOLOGY 70.83 82.20 54.13 39.27 81.80
HSSTATISTICS 44.91 50.00 38.43 26.39 50.00
HSUSHISTORY 54.90 66.18 58.33 40.69 78.90
HSWORLD HISTORY 64.98 73.42 67.09 32.07 75.10
HUMAN AGING 56.95 67.26 45.29 32.29 66.40
HUMAN SEXUALITY 61.07 67.18 51.15 35.11 67.20
INTERNATIONAL LAW 71.90 78.51 56.20 42.15 77.70
JURISPRUDENCE 62.96 70.37 43.52 35.19 71.30
LOGICAL FALLACIES 52.15 61.96 57.06 31.29 72.40
MACHINE LEARNING 29.46 42.86 40.18 29.46 41.10
MANAGEMENT 75.73 73.79 56.31 27.18 77.70
MARKETING 79.06 86.75 67.52 39.74 83.30
MEDICAL GENETICS 57.00 71.00 48.00 45.00 69.00
MISCELLANEOUS 60.54 71.14 61.18 40.23 75.70
MORAL DISPUTES 55.20 63.87 47.11 36.71 66.80
MORAL SCENARIOS 22.35 29.72 24.25 24.36 40.20
NUTRITION 56.86 64.38 50.65 32.35 69.90
PHILOSOPHY 52.73 66.24 45.34 35.37 68.80
PREHISTORY 50.62 69.14 50.93 40.43 67.60
PROFESSIONAL ACCOUNTING 36.17 45.74 35.46 28.72 44.30
PROFESSIONAL LAW 34.16 42.31 37.94 29.53 44.50
PROFESSIONAL MEDICINE 47.79 55.15 43.38 18.01 64.00
PROFESSIONAL PSYCHOLOGY 47.06 63.07 42.48 31.54 68.10
PUBLIC RELATIONS 66.36 64.55 55.46 33.64 71.80
SECURITY STUDIES 63.67 68.16 44.90 34.29 64.90
SOCIOLOGY 70.15 81.09 51.74 31.84 84.10
USFOREIGN POLICY 70.00 78.00 61.00 46.00 81.00
VIROLOGY 42.17 46.99 39.16 28.31 47.00
WORLD RELIGIONS 55.56 73.68 55.56 42.11 84.20
AVERAGE 52.30 59.80 45.70 31.90 60.00

--- PAGE 14 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
Table 6: MMLU individual task performance of TeacherLM-560M, TacherLM-1.1B, TeacherLM-3B, Flan-PaLM-8B.
Furthermore, we denote “college” as “CO” and “high school” as “HS”.
TASK TEACHER LM-560M T EACHER LM-1.1B T EACHER LM-3B F LAN-PALM -8B
ABSTRACT ALGEBRA 29.00 33.00 32.00 36.40
ANATOMY 42.96 39.26 40.74 42.90
ASTRONOMY 44.08 42.11 46.71 43.80
BUSINESS ETHICS 30.00 44.00 51.00 36.40
CLINICAL KNOWLEDGE 47.92 45.66 53.21 48.30
CO BIOLOGY 36.11 35.42 47.92 56.20
CO CHEMISTRY 37.00 31.00 42.00 25.00
CO COMPUTER SCIENCE 35.00 40.00 30.00 54.50
CO MATHEMATICS 30.00 32.00 41.00 18.20
CO MEDICINE 47.40 40.46 47.40 50.00
CO PHYSICS 32.35 30.39 37.25 45.50
COMPUTER SECURITY 42.00 49.00 58.00 72.70
CONCEPTUAL PHYSICS 37.02 41.28 44.68 38.50
ECONOMETRICS 28.07 33.33 26.32 33.30
ELECTRICAL ENGINEERING 46.20 46.90 49.66 37.50
ELEMENTARY MATHEMATICS 29.89 34.13 34.66 34.10
FORMAL LOGIC 29.37 26.98 32.54 21.40
GLOBAL FACTS 24.00 30.00 30.00 30.00
HSBIOLOGY 40.32 50.65 57.42 50.00
HSCHEMISTRY 39.90 39.41 43.35 18.20
HSCOMPUTER SCIENCE 40.00 38.00 47.00 44.40
HSEUROPEAN HISTORY 42.42 41.21 52.73 72.20
HSGEOGRAPHY 44.95 51.52 56.57 68.20
HSGOVERNMENT AND POLITICS 40.93 47.67 53.37 57.10
HSMACROECONOMICS 38.46 41.28 53.08 44.20
HSMATHEMATICS 28.52 27.04 26.67 17.20
HSMICROECONOMICS 41.60 50.00 60.08 57.70
HSPHYSICS 29.14 29.14 29.14 17.60
HSPSYCHOLOGY 44.22 55.23 63.85 68.30
HSSTATISTICS 35.65 34.26 41.20 39.10
HSUSHISTORY 39.22 40.69 47.55 72.70
HSWORLD HISTORY 40.51 45.15 53.59 61.50
HUMAN AGING 39.46 40.81 55.61 52.20
HUMAN SEXUALITY 44.27 43.51 48.09 66.70
INTERNATIONAL LAW 58.68 65.29 62.81 76.90
JURISPRUDENCE 44.44 49.07 50.00 72.70
LOGICAL FALLACIES 32.52 34.97 46.01 61.10
MACHINE LEARNING 28.57 27.68 36.61 45.50
MANAGEMENT 44.66 63.11 61.17 81.80
MARKETING 50.85 63.25 67.95 72.00
MEDICAL GENETICS 42.00 39.00 48.00 63.60
MISCELLANEOUS 37.42 43.93 54.92 68.60
MORAL DISPUTES 45.95 45.09 52.02 39.50
MORAL SCENARIOS 27.15 23.58 24.69 25.00
NUTRITION 45.42 47.06 50.33 57.60
PHILOSOPHY 35.05 44.37 47.59 61.80
PREHISTORY 37.04 38.27 42.90 45.70
PROFESSIONAL ACCOUNTING 35.46 32.98 35.82 35.50
PROFESSIONAL LAW 29.40 32.40 33.64 32.40
PROFESSIONAL MEDICINE 29.78 33.82 42.65 51.60
PROFESSIONAL PSYCHOLOGY 33.82 40.36 43.46 46.40
PUBLIC RELATIONS 32.73 42.73 50.91 50.00
SECURITY STUDIES 44.08 47.76 54.69 40.70
SOCIOLOGY 54.72 62.69 65.67 72.70
USFOREIGN POLICY 52.00 57.00 67.00 63.60
VIROLOGY 33.13 36.14 40.96 44.40
WORLD RELIGIONS 35.67 38.01 50.29 68.40
AVERAGE 38.39 41.38 46.74 49.29

--- PAGE 15 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
Table 7: Comparing different benchmarks of the original six models we use and the scores of each model after training
with P3 dataset, P3-Sense-3K, and seven augmented datasets. PRETRAINED in the figure represents the original models;
ORIGIN represents models trained on origin P3 dataset; SENSE represents models trained on P3-Sense-3K; The rest
represent models trained on augmented P3 dataset in different prompt format. C represents the CoT field; M represents the
common mistakes field; F represents the fundamental field.
MMLU BL-1B1 BL-3B BL-7B1 BZ-1B1 BZ-3B BZ-7B1 OPT-1B3 OPT-2B7 OPT-6B7
PRETRAINED 23.1 25.1 24.7 23.7 35.2 39.6 24.8 22.2 24.9
ORIGIN 25.8 26.3 32.8 24.8 28.9 32.2 27.8 28.3 31.0
SENSE 31.0 36.1 39.2 32.2 38.6 41.3 33.1 37.0 39.1
C 32.9 38.1 42.6 32.8 40.1 44.0 35.8 38.4 42.2
CM 32.5 37.4 42.3 33.1 40.2 44.1 34.9 38.4 41.3
CMF 33.0 38.0 43.0 33.5 40.3 44.7 34.3 39.4 42.7
SHUFFLE 33.1 38.2 41.0 32.9 40.1 44.1 35.3 38.2 41.6
M 31.9 37.3 41.5 32.8 39.0 43.1 34.8 38.1 39.9
F 31.7 37.0 42.0 32.7 39.7 43.5 33.2 37.9 40.9
FCM 32.5 38.0 43.2 32.6 40.2 44.5 33.4 39.5 42.2
ECQA BL-1B1 BL-3B BL-7B1 BZ-1B1 BZ-3B BZ-7B1 OPT-1B3 OPT-2B7 OPT-6B7
PRETRAINED 20.6 19.3 21.2 20.2 62.8 74.3 21.2 20.4 20.8
ORIGIN 22.1 46.0 61.3 24.8 54.5 64.3 27.6 48.5 64.9
SENSE 50.0 63.4 70.0 32.2 76.0 78.2 59.5 69.6 71.9
C 51.5 65.3 71.9 32.8 75.6 80.1 59.5 70.4 68.8
CM 53.1 64.7 71.7 33.1 76.4 79.5 58.8 70.2 71.9
CMF 49.4 64.8 70.9 33.5 74.5 78.2 51.9 69.7 71.7
SHUFFLE 55.7 67.9 71.3 32.9 75.8 78.2 58.1 71.6 72.9
M 51.8 65.5 71.4 32.8 76.6 78.6 62.1 70.2 71.6
F 49.1 63.4 70.1 32.7 75.0 78.6 49.7 69.3 70.3
FCM 46.5 64.8 70.6 32.6 75.3 78.2 50.0 68.4 71.8
CREAK BL-1B1 BL-3B BL-7B1 BZ-1B1 BZ-3B BZ-7B1 OPT-1B3 OPT-2B7 OPT-6B7
PRETRAINED 50.6 49.6 49.6 50.4 49.6 49.6 49.7 53.7 64.6
ORIGIN 51.3 54.6 51.6 51.3 51.4 51.1 52.2 51.2 51.9
SENSE 59.2 68.2 67.9 58.8 66.7 71.3 67.1 70.2 73.6
C 61.4 66.8 71.3 63.7 67.0 70.6 68.8 73.2 74.4
CM 62.4 68.8 71.8 63.7 68.8 72.2 68.3 72.9 75.7
CMF 61.9 70.2 70.5 63.9 69.0 70.8 65.9 73.5 75.0
SHUFFLE 60.0 68.6 72.0 63.7 69.0 70.8 67.9 72.6 75.0
M 60.7 67.0 73.2 62.7 66.7 71.7 66.4 71.9 75.2
F 57.4 66.9 70.8 60.7 67.6 71.1 69.1 72.6 73.8
FCM 64.1 69.3 70.4 62.9 68.5 70.9 65.0 73.2 75.9
STRATEGY QA BL-1B1 BL-3B BL-7B1 BZ-1B1 BZ-3B BZ-7B1 OPT-1B3 OPT-2B7 OPT-6B7
PRETRAINED 55.3 53.5 52.2 50.0 52.2 47.4 53.5 63.6 71.5
ORIGIN 57.0 55.7 60.5 56.6 52.6 63.2 57.9 54.4 63.2
SENSE 57.5 69.7 70.6 61.8 71.5 73.7 62.3 65.8 71.9
C 58.8 68.9 72.4 64.0 70.6 73.2 65.4 66.7 74.6
CM 59.6 70.2 69.7 62.7 71.9 73.7 65.8 66.7 73.7
CMF 58.8 65.4 72.4 61.8 69.3 75.0 62.7 65.4 72.4
SHUFFLE 62.3 71.5 71.9 61.8 71.1 73.7 64.0 65.4 73.2
M 61.8 66.2 74.6 64.5 72.4 71.5 66.2 68.4 74.1
F 60.1 68.0 71.9 63.6 70.2 75.0 63.2 68.9 70.6
FCM 60.1 65.8 71.1 64.0 70.2 72.8 60.5 66.2 71.5

--- PAGE 16 ---
TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
Question: Determine the correctness of the following sentence. Sam Cooke became a member of the Rock & Roll Hall of Fame.
(A) True (B) False
Answer: The answer is (A).
Manual Explanations
I know this to be true.
text-davinci-003 by prompting Let’s think step by step.
Sam Cooke was an American singer, songwriter, and entrepreneur. He was one of the most influential musical artists of the 20th
century. He was inducted into the Rock and Roll Hall of Fame in 1986. Therefore, the statement is true.
text-davinci-003 by prompting The fundamental of this question is:
True. Sam Cooke was inducted into the Rock and Roll Hall of Fame in 1986.
text-davinci-003 by prompting The common mistakes are:
Sam Cooke became a member of the Rock and Roll Hall of Fame.
TeacherLM-7.1B by prompting Let’s think step by step.
Step 1: The early 1970s saw the emergence of Rock & Roll as the leading music form in the United States. Step 2: Sam Cooke is
known as the King of Soul. He became a Rock & Roll Hall of Fame member in 1995.
TeacherLM-7.1B by prompting The fundamental of this question is:
The United States established several organizations to recognize excellence in achievements. These organizations are the National
Foundation for the Advancement of Arts and Sciences and the Rock and Roll Hall of Fame. In 1972, the Country Music Asso-
ciation introduced the first-ever Rock and Roll Hall of Fame. A rock music awards show named the BBC Radio 1 Big Weekend
also emerged. This award show had several celebrities competing for the top honor: the Rock and Roll Hall of Fame. The first
inductee was the band, the Beach Boys. The recording industry’s successes maintain the honor.TeacherLM-7.1B by prompting The common mistakes are:
Rock & Roll songs emerged in the 1950s and 1960s. Do not forget that Ray Charles and Sam Cooke are both in the Rock & Roll
Hall of Fame.
Question: Determine the correctness of the following sentence. Turmeric grows in the sub-Asian region of the world.
(A) True (B) False
Answer: The answer is (A).
Manual Explanations
Because they are facts.
text-davinci-003 by prompting Let’s think step by step.
Turmeric is a plant that is native to South Asia. Therefore, it grows in the sub-Asian region of the world. This means that the
statement is true.
text-davinci-003 by prompting The fundamental of this question is: Turmeric is a plant native to the subcontinent of India, which
is part of the Asian region of the world. Therefore, the statement is true.
text-davinci-003 by prompting The common mistakes are:
True.
TeacherLM-7.1B by prompting Let’s think step by step.
Step 1: The botanical name of Turmeric is Curcuma Longa. The root of this plant is used for many purposes, like its anticancer-
ous, analgesic, and antimicrobial properties. Step 2: Turmeric is also known as Indian ginger because the herbaceous root veg-
etable is native to south Asia, also called sub-Asian. So the statement is true.
TeacherLM-7.1B by prompting The fundamental of this question is:
India has a tropical monsoon climate with four seasons throughout the year, a cool season from January to February, a summer
season from March to May, a southwest monsoon rainy season from June to September, and a northeast monsoon period from
October to December. The temperature is as low as 150C in the north and as high as 27C in the south. Almost all year is frost-
free, crops can be grown throughout the year, and the heat resources are abundant. In this way, very many tropical plants can
survive in India.
TeacherLM-7.1B by prompting The common mistakes are:
Make sure to distinguish the term sub-Asia from Asia. Although sub-Asia is part of Asia, people generally refer to India as sub-
Asia.
Figure 5: More examples generated by TeacherLM-7.1B and text-davinci-003, with its manual explanations from the
original dataset and CoT, fundamentals, and common mistakes generated by text-davinci-003 and TeacherLM-7.1B, from
top to bottom.
