# CodeCoT: Giải quyết lỗi cú pháp code trong suy luận CoT cho sinh code

Dong Huang∗1Qingwen Bu∗2Yuhao Qing1Heming Cui1

Tóm tắt
Chain-of-thought (CoT) đã nổi lên như một công cụ đột phá trong NLP, đáng chú ý với hiệu quả trong các tác vụ suy luận phức tạp, như chứng minh toán học. Tuy nhiên, việc áp dụng nó trong sinh code phải đối mặt với thách thức riêng biệt, tức là, mặc dù code được sinh ra với suy luận CoT là đúng về mặt logic, nhưng nó gặp vấn đề lỗi cú pháp (ví dụ: báo lỗi cú pháp không hợp lệ) trong quá trình thực thi code, điều này khiến kết quả pass@1 của CoT trong HumanEval thậm chí thấp hơn kết quả zero-shot. Trong bài báo này, chúng tôi trình bày Code Chain-of-Thought (CodeCoT) tích hợp CoT với quy trình tự kiểm tra cho sinh code. CodeCoT bắt đầu với LLM sử dụng CoT để phát triển code ban đầu nhằm đảm bảo code được sinh ra tuân theo luồng logic đúng. Sau đó, CodeCoT sẽ sinh test case để xác thực liệu code có lỗi cú pháp trong quá trình thực thi hay không. CodeCoT sau đó sử dụng giai đoạn tự kiểm tra, trong đó code được sinh ra được thực thi với các test case này trong môi trường local. Nếu môi trường local báo thông tin lỗi (ví dụ: lỗi cú pháp không hợp lệ), CodeCoT sẽ tinh chỉnh code một cách lặp đi lặp lại dựa trên thông tin phản hồi. Trong vòng lặp này, CodeCoT có thể đảm bảo code được sinh ra không chỉ tuân theo luồng logic của mô tả code, mà lỗi cú pháp cũng sẽ được giải quyết với quy trình tự kiểm tra. Kết quả đánh giá của chúng tôi cho thấy CodeCoT cải thiện hiệu quả của sinh code. Ví dụ, CodeCoT tăng pass@1 từ 75.6% lên 79.3% cho dataset HumanEval.

1. Giới thiệu
Bắt đầu với những tiến bộ được trình bày bởi các mô hình GPT-x được phát triển bởi OpenAI, các mô hình ngôn ngữ lớn (LLM) dựa trên transformer hiện cung cấp hiệu suất tối ưu trong nhiều tác vụ NLP tiêu chuẩn. Một trong những LLM mới nhất, GPT-3 (Brown et al., 2020b) sử dụng khoảng 175 tỷ tham số và được huấn luyện trên kho dữ liệu huấn luyện ngôn ngữ tự nhiên cực lớn, bao gồm, trong số những thứ khác, các đoạn trích từ Wikipedia. Lấy cảm hứng từ GPT-3, nhiều mô hình ngôn ngữ lớn đã được phát triển, là những biến thể khác nhau của kiến trúc transformer. Một số mô hình mạnh nhất là PaLM (Chowdhery et al., 2022), GLaM (Du et al., 2021), MegatronTuring NLG (Smith et al., 2022), Meta-OPT (Zhang et al., 2022a), Gopher (Rae et al., 2021), LaMDA (Thoppilan et al., 2022), Chinchilla (Hoffmann et al., 2022), ChatGPT, và GPT4 (OpenAI, 2023). GPT-4 hiện cung cấp hiệu suất tối ưu trong các tác vụ NLP như dịch ngôn ngữ tự nhiên (Li et al., 2023b) và thậm chí dịch sang biểu diễn có cấu trúc (Olmo et al., 2021).

Gần đây, Wei et al. (2022) đã giới thiệu khái niệm sáng tạo về Chain-of-Thought Prompting. Kỹ thuật này hướng dẫn mô hình ngôn ngữ tạo ra một chuỗi câu ngắn gọn phản ánh các bước nhận thức mà con người có thể thực hiện khi giải quyết vấn đề. Như được minh họa trong Hình 1 Math CoT, khi người dùng yêu cầu truy vấn số học "vui lòng tính tổng của 128 và 367", thay vì trả lời trực tiếp bằng "495", mô hình sử dụng CoT sẽ được hướng dẫn trả lời câu hỏi thông qua toàn bộ quá trình suy luận. Phương pháp này đã chứng minh sự cải thiện đáng kể trong hiệu suất mô hình cho các thách thức suy luận đa bước khác nhau. Sự xuất hiện của CoT đã mở khóa tiềm năng mới cho LLM, đặc biệt trong các tác vụ downstream. CoT không chỉ cải thiện hiệu suất trong các tác vụ suy luận số học, thường thức và ký hiệu (Zelikman et al., 2022; Li et al., 2022b) mà còn mở đường cho những đổi mới tổng quát hơn. Những điều này từ việc tạo đối thoại nắm bắt ý định người dùng đến tham gia vào các tác vụ đa phương thức kết hợp thông tin văn bản và thị giác (Liu et al., 2022). Sự quan tâm ngày càng tăng đối với CoT nhấn mạnh tiềm năng của nó trong việc định hình cách LLM tương tác và suy luận.

Việc sử dụng rộng rãi CoT trong suy luận toán học khuyến khích chúng tôi thảo luận về việc áp dụng CoT trong sinh code. Tuy nhiên, như đã đề cập bởi Dong et al. (2023b); Wang et al. (2023); Shinn et al. (2023), việc áp dụng trực tiếp CoT trong sinh code sẽ làm giảm hiệu quả sinh code, được minh chứng bằng điểm pass@1 thấp hơn kết quả zero-shot. Như được hiển thị trong Hình 1 CoT in Code, chúng tôi quan sát thấy lý do chính là code được sinh ra với suy luận CoT mặc dù tuân theo luồng logic (ví dụ: pseudocode) của mô tả tác vụ, nhưng nó bỏ qua việc tuân theo các yêu cầu cú pháp (ví dụ: tránh lỗi cú pháp), điều này sau đó làm giảm pass@1 của suy luận CoT trong các tác vụ sinh code (ví dụ: dataset HumanEval và MBPP).

Để giải quyết thách thức của CoT trong sinh code, chúng tôi đề xuất CodeCoT, một framework mới kết hợp suy luận CoT và tự kiểm tra để giảm thiểu các vấn đề phát sinh từ sự ngắt kết nối giữa suy luận tường thuật và yêu cầu cú pháp code nghiêm ngặt. Trong quá trình sinh code, CodeCoT đầu tiên sử dụng suy luận CoT để sinh code với luồng logic đúng và sau đó sử dụng quy trình tự kiểm tra để phát hiện và sửa các lỗi cú pháp trong code của nó. Cách tiếp cận tập trung kép này hiệu quả thu hẹp khoảng cách được quan sát trong các ứng dụng CoT truyền thống cho sinh code. Như được hiển thị trong Hình 1 CodeCoT, chúng tôi phân biệt framework CodeCoT thành CoT in Code và tự kiểm tra. CoT in Code gần gũi với các kỹ thuật CoT khác được sử dụng trong các tác vụ downstream khác nhau của LLM. Nó chứa một mô tả tác vụ, suy luận CoT, đầu ra tác vụ, và một mô tả tác vụ khác. Thành phần tự kiểm tra sẽ giới thiệu một lớp tự kiểm tra mới vào quy trình. Cụ thể, đối với thành phần tự kiểm tra, LLM sẽ sinh ra một vài test case. Sau đó thành phần tự kiểm tra sẽ thực thi code với các test case này trong môi trường local để đánh giá liệu có lỗi cú pháp trong code hay không. Nếu việc thực thi báo lỗi cú pháp, LLM sau đó sẽ sinh lại hàm code để lặp và tinh chỉnh code tương ứng. Quy trình này kết thúc với LLM tạo ra code được đánh bóng không chỉ đúng về logic mà còn không chứa lỗi cú pháp.

Các thí nghiệm rộng rãi minh họa rằng CodeCoT cho thấy sự gia tăng đáng kể trong độ chính xác pass@1 cho các dataset đánh giá. Ví dụ, CodeCoT tăng pass@1 từ 75.6% và 69.8% lên 79.3% và 89.5% cho dataset HumanEval và MBPP. CodeCoT cũng đạt hiệu suất SOTA trong dataset HumanEval-ET và MBPP-ET. Ví dụ, CodeCoT đạt 69.5% và 63.0% pass@1 trong khi các baseline chỉ đạt 56.1% và 49.5% pass@1 trong HumanEval-ET và MBPP-ET. Các đóng góp chính của chúng tôi như sau:

• Chúng tôi đề xuất CodeCoT, sử dụng quy trình tự kiểm tra để giải quyết thách thức của Chain-of-Thought (CoT) trong sinh code (tức là chuyển đổi suy luận logic dựa trên tường thuật thành code chính xác và có thể thực thi).

• CodeCoT đạt hiệu suất SOTA trên benchmark HumanEval và cải thiện đáng kể độ chính xác pass@1 so với các phương pháp hiện có. Ví dụ, CodeCoT tăng pass@1 từ 75.6% và 69.8% lên 79.3% và 89.5% cho dataset HumanEval và MBPP.

2. Công việc liên quan

2.1. Mô hình ngôn ngữ lớn
Quỹ đạo phát triển mô hình ngôn ngữ đã chứng kiến sự nhấn mạnh nhất quán về mở rộng quy mô, cả về kiến trúc mô hình và các dataset mà chúng được huấn luyện. Chronology tăng trưởng này có thể được truy ngược về các công trình của Brants et al. (2007), những người đã chứng minh lợi thế của các mô hình được huấn luyện trên 2 nghìn tỷ token khổng lồ, dẫn đến việc tạo ra 300 tỷ n-gram. Bước nhảy vọt đáng kể này đặc biệt phù hợp để nâng cao chất lượng dịch máy. Mặc dù các kỹ thuật ban đầu, như "Stupid Backoff" để làm mịn, còn thô sơ, những tiến bộ đã được thực hiện bởi Heafield et al. (2013). Tiềm năng chuyển đổi của việc mở rộng quy mô được nhấn mạnh thêm với sự phát triển của kiến trúc transformer, đã tạo ra các tiêu chuẩn mới trong nhiều thách thức NLP. Một số mô hình tiên phong trong kỷ nguyên này bao gồm BERT của Devlin et al. (2019), GPT-2 của Radford et al. (2019), MegatronLM của Shoeybi et al. (2019), và T5 của Raffel et al. (2019). Cảnh quan trải qua một sự thay đổi to lớn với sự giới thiệu của GPT-3 của Brown et al. (2020a), một gã khổng lồ với 175 tỷ tham số. Điều này thúc đẩy sự phát triển của một dòng dõi các Mô hình Ngôn ngữ Lớn như Jurassic-1, Megatron-Turing NLG, Gopher, Chinchilla, PaLM, OPT, và GLM, được giới thiệu từ 2021 đến 2022. Đi sâu vào cơ chế của việc mở rộng quy mô, các nghiên cứu như của Hestness et al. (2017) và Rosenfeld et al. (2019) đánh giá mối quan hệ giữa kích thước mô hình và dataset và hiệu suất kết quả, phát hiện sự hiện diện của định luật lũy thừa.

2.2. Chain of Thought Prompting
Khái niệm chain-of-thought prompting được giới thiệu để khai thác khả năng suy luận của các mô hình ngôn ngữ lớn, trình bày một cách tiếp cận mới để tinh chỉnh hiệu suất của các mô hình này trong các tác vụ phức tạp. Ban đầu được đề xuất bởi (Wei et al., 2022), kỹ thuật này nhằm bổ sung các ví dụ few-shot với các bước suy luận chi tiết, dẫn đến những cải thiện hiệu suất đáng kể trong các tác vụ phức tạp. Theo thời gian, cách tiếp cận này đã truyền cảm hứng cho rất nhiều nghiên cứu, mỗi nghiên cứu tìm cách tinh chỉnh và nâng cao khái niệm nền tảng của CoT. Những tiến bộ đáng chú ý bao gồm các đổi mới trong self-consistency (Wang et al., 2022), tiến bộ trong least-to-most prompting và biến thể động của nó (Zhou et al., 2022; 2023), cũng như những đột phá trong bootstrapping (Zelikman et al., 2022) và self-training (Huang et al., 2022). Phương pháp verifier (Li et al., 2022b) cũng nổi bật như một đóng góp đáng chú ý trong lĩnh vực này. Một ngoại lệ đáng chú ý trong cảnh quan khả năng thích ứng là Auto-CoT (Zhang et al., 2022c). Phương pháp này phân loại các câu hỏi test thành các cụm riêng biệt để tăng cường tính đa dạng, sau đó tạo ra câu trả lời thông qua zero-shot prompting.

2.3. Ứng dụng chain of thought trong LLM
Theo sau chain of thought prompting ban đầu được đề xuất bởi Wei et al. (2022) được sử dụng trong suy luận số học, thường thức và ký hiệu, rất nhiều công trình xuất hiện nhằm cải thiện các phần khác nhau của quy trình suy luận gốc, bao gồm autocot (Zhang et al., 2022c), self-consistency (Wang et al., 2022), active prompt (Diao et al., 2023), automate-cot (Shum et al., 2023). Bên cạnh đó, có một số nhà tiên phong áp dụng những ý tưởng tương tự vào việc tạo đối thoại có kiến thức và các tác vụ khác. (Liu et al., 2022) sử dụng cách tiếp cận prompting đa giai đoạn để tạo kiến thức trước và sau đó phản hồi, đạt hiệu suất tốt hơn fine-tuning về mặt khả năng kiến thức phản hồi và sự tham gia. (Tan et al., 2021) kết hợp prompting với tuning để chuyển các mô hình được pre-trained sang các tác vụ dịch thuật. Công việc liên quan gần đây chuyển đổi hồ sơ người dùng và các lần lặp lịch sử thành prompt để xây dựng hệ thống khuyến nghị đối thoại với backbone là ChatGPT (Gao et al., 2023). Cobbe et al. (2021) sử dụng máy tính cho các phép toán số học như xử lý hậu kỳ, và Demeter et al. (2020) thêm các module chuyên biệt để tạo thành phố và ngày tháng. Khác với những công việc này, PAL tạo code cho trình thông dịch Python, đủ tổng quát để xử lý cả tính toán số học và ngày tháng, mà không cần modules chuyên biệt và sửa chữa ad-hoc. Chowdhery et al. (2022) cũng đã thử nghiệm với máy tính bên ngoài. Pi et al. (2022) pre-train mô hình trên kết quả thực thi của các biểu thức ngẫu nhiên trên máy tính.

2.4. Nâng cao sinh code thông qua Prompt Engineering
(Chen et al., 2021) giới thiệu một cách tiếp cận lọc đơn giản bằng cách chỉ chọn các mẫu đầu ra vượt qua thành công các test case công khai. AlphaCode (Li et al., 2022a), CodeT (Chen et al., 2022), và MBR-Exec (Shi et al., 2022) đề xuất tạo ra nhiều test case hơn và sử dụng các phương pháp dựa trên quy tắc tinh vi hơn để xếp hạng các mẫu tạo ra theo hành vi thực thi của chúng. LEVER (Ni et al., 2023), Coder-Reviewer (Zhang et al., 2022b) và Code Rankers (Inala et al., 2022) tuân theo nguyên tắc tương tự nhưng giới thiệu các phương pháp xếp hạng dựa trên mô hình nhiều hơn. Gần đây, nhiều công việc liên quan đã được đề xuất để tăng chất lượng tạo ra thông qua các tự sửa đổi lặp. Self-Edit (Zhang et al., 2023) sử dụng kết quả test từ các test case công khai làm phản hồi cho các mô hình tự sửa đổi code của chúng. Self-correct (Welleck et al., 2022) và CodeRL (Le et al., 2022) giới thiệu các mô hình thứ cấp để dự đoán tính đúng đắn của các chương trình đầu ra và sửa đổi chúng tương ứng. Self-debug (Chen et al., 2023), Self-refine (Madaan et al., 2023), và Reflexion (Shinn et al., 2023) đề xuất tạo điều kiện cho việc sửa đổi code tốt hơn với giải thích ngôn ngữ tự nhiên tổng hợp hoặc phản chiếu tự tạo bởi LLM. Self-repair (Olausson et al., 2023) và ILF (Chen, 2023) tuân theo chiến lược tương tự nhưng nhấn mạnh việc sử dụng giải thích ngôn ngữ tự nhiên được cung cấp bởi các chuyên gia con người.

Gần đây, một số chiến lược CoT được đề xuất song song với CodeCoT, đã được sử dụng để nâng cao hiệu quả của sinh code. Ví dụ, (Li et al., 2023a) đề xuất SCOT để sử dụng suy luận CoT cho sinh code cấp cấu trúc. CodeChain (Le et al., 2023) mở rộng SCOT và tạo code từ cấp sub-module chi tiết hơn so với SCOT. Khác với những chiến lược CoT song song này, tập trung vào sinh code cấp sub-module/cấu trúc với suy luận CoT, CodeCoT suy nghĩ lại thách thức của sinh code được hướng dẫn bởi suy luận CoT, tức là suy luận CoT hiện tại giải quyết yêu cầu logic của sinh code trong khi bỏ qua yêu cầu cú pháp cùng lúc. CodeCoT giải quyết thách thức trên bằng cách sử dụng suy luận CoT và quy trình tự kiểm tra để đạt được tính đúng đắn về logic và cú pháp trong quá trình sinh code.

3. Phương pháp

3.1. Tổng quan
Hình 2 cung cấp pipeline của CodeCoT, được chia thành bốn thành phần quan trọng: CoT Prompt, Test Cases Generation, Code Generation, và Self-Examination. Quy trình bắt đầu với CoT Prompt, nơi LLM được giao một tác vụ, ví dụ, "tính giai thừa của n." LLM sau đó phân tích tác vụ này, cung cấp một chuỗi suy luận logic chi tiết cách tiếp cận để giải quyết tác vụ, dẫn đến một hàm code được tạo ra. Tiếp theo, trong giai đoạn Test Cases Generation, LLM sẽ tạo ra một tập hợp các test được sử dụng để đánh giá liệu code có thể thực thi hay không. Trong giai đoạn Code Function Generation, LLM sẽ đầu tiên tạo ra với luồng logic đúng. Sau đó nó sẽ dựa trên thông tin phản hồi tinh chỉnh code để sửa các lỗi cú pháp. Trong giai đoạn Self-Examination, code sẽ được thực thi trong môi trường local với test case để phân tích liệu code có lỗi (ví dụ: lỗi cú pháp không hợp lệ) hay không. Nếu lỗi phát sinh trong quá trình thực thi local, thông tin lỗi (thông tin phản hồi) sẽ được đưa vào LLM để tinh chỉnh code, để đảm bảo code có cả luồng logic và cú pháp đúng. Quy trình lặp của tự kiểm tra sẽ được tiến hành thông qua một loạt các lần lặp đa bước, cho phép người dùng chỉ định số lượng lần lặp, trong khi mặc định là năm lần lặp.

3.2. CoT in Code
CodeCoT prompt. Giai đoạn đầu tiên trong quy trình CodeCoT là cung cấp cho mô hình một prompt rõ ràng và có cấu trúc, như được hiển thị trong Hình 2. CoT prompt chứa hai yếu tố chính, tức là mô tả tác vụ và ví dụ để hướng dẫn. Mô tả tác vụ sẽ có một tuyên bố ngắn gọn phác thảo mục tiêu chính của code cần được tạo ra. Sử dụng ví dụ đã cho, tác vụ có thể là "Trả về danh sách đã sắp xếp các phần tử duy nhất từ danh sách đầu vào." Điều này đặt ra một mục tiêu rõ ràng cho quy trình sinh code và hướng dẫn LLM trong tác vụ của nó. Ví dụ để hướng dẫn cung cấp một ví dụ sinh code để phục vụ như một bài hướng dẫn cho LLM. Trong trường hợp này, ví dụ được đưa ra là "triển khai hàm giai thừa." Mặc dù ví dụ có thể không tương quan trực tiếp với tác vụ chính, nó cung cấp cái nhìn về loại logic hoặc cấu trúc mong đợi trong giải pháp. Ví dụ này hỗ trợ mô hình hiểu độ sâu, độ phức tạp, hoặc cách tiếp cận mà người dùng dự đoán cho tác vụ hiện tại.

Code Function Generation. Sau khi nhận được một prompt bao gồm cả ví dụ tác vụ và mô tả tác vụ, LLM sau đó sẽ tạo code, chủ yếu được hướng dẫn bởi mô tả tác vụ. Luồng logic code ban đầu này sẽ được tạo ra bởi suy luận Chain of Thought (CoT), đảm bảo rằng luồng logic của code phù hợp với quy trình giải quyết vấn đề. Một khi code được tạo ra chứa lỗi cú pháp, một cơ chế tự kiểm tra được kích hoạt, xác định những lỗi này trong môi trường local. Phản hồi lỗi này sau đó được phản hồi lại cho LLM, cho phép nó tinh chỉnh và sửa cú pháp. Quy trình lặp này của tự kiểm tra và phản hồi đảm bảo code không chỉ đúng về logic mà còn chính xác về cú pháp.

3.3. CodeCoT
Test case generation. Để đánh giá code được tạo ra bởi CoT prompt trong môi trường local, chúng tôi cũng sẽ yêu cầu LLM tạo ra test case được sử dụng để đánh giá cú pháp của code được tạo ra. Cụ thể, như được hiển thị trong Hình 2, trong quy trình sinh code, chúng tôi cũng sẽ yêu cầu LLM tạo test case cho code đã cho1. Để giảm chi phí giao tiếp API, chúng tôi yêu cầu sinh code và tạo test case trong cùng một prompt (xem Hình 4).

Self-examination. CoT prompt làm cho code được tạo ra bởi LLM đôi khi đúng về logic, nhưng code đôi khi chứa lỗi cú pháp, khiến code không thể được thực thi đúng. Như được hiển thị trong Hình 2 tự kiểm tra với test case, chúng tôi giải quyết vấn đề trên bằng cách cung cấp một giai đoạn tự kiểm tra sử dụng kiểm tra liên tục và tinh chỉnh lặp để giải quyết vấn đề này. Cụ thể, một khi LLM tạo ra một bản nháp ban đầu của hàm code, nó sẽ phải chịu sự xem xét nghiêm ngặt với các test tự tạo bằng cách thực thi code với test trong môi trường local (terminal local). Sau đó CodeCoT sẽ thu được phản hồi từ môi trường local. Nếu các thông điệp lỗi, ví dụ lỗi cú pháp không hợp lệ, phát sinh trong giai đoạn này, CodeCoT sẽ sửa đổi code dựa trên code được tạo ra lần cuối và phản hồi từ môi trường local. Ví dụ, như được minh họa trong Hình 3, chúng ta có thể quan sát thấy rằng hàm code ở trên (phiên bản đầu tiên) có lỗi cú pháp ở dòng 4. Lý do chính là dấu ngoặc kép ba phải ở dòng thứ 8. Chúng ta có thể thấy rằng mặc dù code tuân theo suy luận CoT, lỗi cú pháp khiến code không thể thực thi, vì vậy Self-exam CodeCoT sẽ đưa lỗi được báo cho LLM, sau đó nó sửa lỗi này và báo một phiên bản đúng ở dưới. Hàm được sửa đổi sau đó được test lại. Một khi hàm được sửa đổi vượt qua tất cả test, chúng ta có thể coi nó là đáng tin cậy về chức năng và đúng về cú pháp.

4. Đánh giá
Trong phần này, chúng tôi đánh giá CodeCoT để trả lời các câu hỏi sau:

• RQ1. CodeCoT hoạt động như thế nào?
• RQ2. CodeCoT có sửa lỗi cú pháp trong quá trình tự kiểm tra không?
• RQ3. Các bước tự kiểm tra ảnh hưởng đến hiệu quả của CodeCoT như thế nào?
• RQ4. Mỗi thành phần của CodeCoT hoạt động như thế nào?

Chúng tôi sử dụng pass@1 làm metric đánh giá độ đúng của code, metric được áp dụng rộng rãi nhất trong tài liệu về sinh code tự động (Chen et al., 2021; Austin et al., 2021; Dong et al., 2023a; Zhang et al., 2023; Dong et al., 2023b).

Dataset. Trong bài báo này, chúng tôi đánh giá hiệu quả của CodeCoT với bốn dataset sinh code được sử dụng rộng rãi, tức là HumanEval (Chen et al., 2021) và MBPP (Austin et al., 2021), và các phiên bản nâng cao của chúng, tức là HumanEval-ET và MBPP-ET (Dong et al., 2023a). HumanEval và HumanEval-ET tập trung vào một loạt các thách thức lập trình, cung cấp một tập hợp vấn đề đa dạng để kiểm tra kỹ năng giải quyết vấn đề và khả năng thích ứng của mô hình. Mặt khác, MBPP và MBPP-ET cung cấp một bộ sưu tập toàn diện các vấn đề lập trình Python, được thiết kế để đánh giá thành thạo của mô hình trong cú pháp Python và khả năng xử lý nhiều tình huống mã hóa khác nhau. Các phiên bản nâng cao, HumanEval-ET và MBPP-ET, bao gồm nhiều test case đầy đủ hơn, làm cho chúng thách thức hơn và phù hợp hơn để đánh giá các mô hình tiên tiến.

Baseline. Để minh họa hiệu quả của CodeCoT, trong bài báo này, chúng tôi so sánh CodeCoT với một số mô hình ngôn ngữ lớn (LLM), bao gồm cả mô hình mã nguồn mở và mã nguồn đóng, như AlphaCode (Li et al., 2022a), Incoder (Fried et al., 2022), CodeGeeX (Zheng et al., 2023), StarCoder (Li et al., 2023c), CodeGen-Mono (Nijkamp et al., 2022), CodeX, CodeX với CodeT (Chen et al., 2022), ChatGPT, và GPT4 (OpenAI, 2023). Hơn nữa, chúng tôi đánh giá CodeCoT với các phương pháp prompt engineering SOTA hiện tại, tức là Few-shot, ReAct (Yao et al., 2022), Reflexion (Shinn et al., 2023), ToT (Yao et al., 2023), RAP (Hao et al., 2023), Self-Edit (Zhang et al., 2023), Self-Planing (Jiang et al., 2023), Self-Debugging (Chen et al., 2023), Self-Collaboration (Dong et al., 2023b), SCOT (Li et al., 2023a), CodeChain (Le et al., 2023), và INTERVENOR (Wang et al., 2023). Mô hình cơ sở được sử dụng trong các chiến lược prompt engineering của chúng tôi là ChatGPT. Những chiến lược này đã được chứng minh là cải thiện đáng kể hiệu suất của LLM trong các tình huống sinh code phức tạp2.

4.1. RQ1. CodeCoT hoạt động như thế nào?
Kết quả đánh giá của CodeCoT và các baseline được hiển thị trong Bảng 1, nơi chúng ta có thể thấy rằng CodeCoT đạt hiệu suất SOTA so với các mô hình baseline và chiến lược prompt engineering trong dataset HumanEval và MBPP. Ví dụ, chúng ta có thể thấy rằng ChatGPT đạt 57.3% và 52.2% pass@1 trong dataset HumanEval và MBPP. Trong khi CodeCoT đạt 79.3% và 89.5% pass@1 trong những dataset này, tăng 22% và 37.3% pass@1 trong dataset HumanEval và MBPP, điều này minh họa rằng CodeCoT có thể cải thiện hiệu quả sinh code của mô hình cơ sở của nó. Sau đó, khi chúng tôi so sánh CodeCoT với các chiến lược prompt engineering baseline, chúng ta có thể thấy rằng CodeCoT vẫn đạt hiệu suất SOTA. Ví dụ, so với Self-Collaboration và INTERVENOR, CodeCoT cải thiện pass@1 từ 74.4% và 75.6% lên 79.3% trong dataset HumanEval, và CodeCoT cũng cải thiện pass@1 từ 68.2% và 69.8% lên 89.5% trong dataset MBPP. Đối với dataset HumanEval-ET và MBPP-ET, CodeCoT cũng tăng pass@1 từ 56.1% và 49.5% lên 69.5% và 63.0% so với Self-Collaboration. So với kết quả CoT hiện tại, chúng ta cũng có thể thấy rằng SCOT (Li et al., 2023a) và CodeChain (Le et al., 2023) cũng thấp hơn CodeCoT, điều này là do SCOT và CodeChain không giải quyết thách thức của lỗi cú pháp trong quy trình sinh code.

4.2. RQ2. CodeCoT có sửa lỗi cú pháp trong quá trình tự kiểm tra không?
Để minh họa liệu CodeCoT có giảm lỗi cú pháp trong quá trình tự kiểm tra so với các chiến lược CoT khác hay không, chúng tôi phân tích thêm phân phối lỗi khi chúng tôi tính pass@1 trong thí nghiệm. Chúng tôi chia các loại lỗi thành AssertError (thường tồn tại trong quy trình tính pass@1 vì các đoạn code không vượt qua test case.) và các lỗi khác, có nghĩa là code không thể được thực thi vì lỗi cú pháp, trình biên dịch, và các lỗi khác không phát sinh do assertion trong code. Kết quả đánh giá được hiển thị trong Bảng 2, nơi chúng ta có thể quan sát thấy rằng tỷ lệ RuntimeError (ví dụ: SyntaxError) thấp hơn baseline của chúng tôi. Ví dụ, trong SCOT và CodeChain, RuntimeError có 30% và 35% trong dataset HumanEval, trong khi CodeCoT 1-step chỉ có 27%, và khi chúng tôi tăng các bước tự kiểm tra, RuntimeError giảm thêm xuống 2% cho 5-step. Những kết quả này minh họa rằng so với các chiến lược suy luận CoT khác, CodeCoT có thể giảm các lỗi gây ra bởi lỗi cú pháp.

4.3. Các bước tự kiểm tra ảnh hưởng đến hiệu quả của CodeCoT như thế nào?
Để đánh giá ảnh hưởng của các bước tự kiểm tra lặp đối với hiệu suất sinh code, chúng tôi tăng một cách có hệ thống số vòng tự kiểm tra và theo dõi những cải thiện độ chính xác kết quả. Kết quả đánh giá được hiển thị trong Bảng 3, nơi chúng ta có thể thấy rằng tăng các bước tự kiểm tra có thể cải thiện hiệu quả sinh code. Ví dụ, khi chúng tôi tăng bước từ 1 đến 5, pass@1 của CodeCoT tăng từ 71.3% và 81.7% lên 79.3% và 89.5% cho dataset HumanEval và MBPP. Những hành vi này cũng được hiển thị trong dataset ET. Ví dụ, pass@1 của CodeCoT cũng tăng từ 60.4% và 58.4% lên 69.5% và 63.0% cho dataset HumanEval-ET và MBPP-ET.

4.4. RQ4. Mỗi thành phần của CodeCoT hoạt động như thế nào?
Như được hiển thị trong Hình 2, trong quy trình sinh code, CodeCoT sẽ đầu tiên sử dụng CoT prompt để sinh code và test case, sau đó thực thi trong môi trường local để phân tích liệu code có đúng hay không và sau đó tinh chỉnh code nếu có lỗi run-time. Trong phần này, chúng tôi sẽ phân tích cách các thành phần khác nhau của CodeCoT ảnh hưởng đến hiệu quả của nó. Cụ thể, chúng tôi sẽ so sánh hiệu quả lành tính của few-shot ChatGPT (Coder), Coder + CoT prompt, Coder + quy trình Self-examination (tạo test case và thực thi trong môi trường local), Coder + CoT prompt + Self-examination (CodeCoT).

Kết quả đánh giá được hiển thị trong Bảng 4. Chúng ta có thể thấy rằng đầu tiên, với sự hỗ trợ của mỗi thành phần, ví dụ: CoT prompt và tự kiểm tra, pass@1 sẽ tăng so với kết quả của chỉ Coder. Ví dụ, một khi chúng tôi sử dụng Coder và CoT prompt, pass@1 tăng từ 67.7% và 65.8% lên 69.5% và 67.7% cho dataset HumanEval và MBPP. Sau đó, khi chúng tôi kết hợp Coder+Self-examination, pass@1 được tăng thêm. Chẳng hạn, pass@1 tăng từ 67.7% và 65.8% lên 70.1% và 79.0% cho dataset HumanEval và MBPP. Chúng ta cũng có thể quan sát thấy rằng pass@1 của phiên bản ET cũng được tăng, ví dụ: pass@1 tăng từ 54.9% và 48.3% lên 57.9% và 56.4% cho dataset HumanEval-ET và MBPP-ET. Mặc dù Coder+CoT prompt và Coder+Self-examination cải thiện hiệu quả sinh code của ChatGPT, chúng ta có thể thấy rằng chúng không đạt hiệu suất SOTA so với baseline của chúng tôi. Ví dụ, Self-Collaboration đạt 74.4% và 68.2% pass@1 trong dataset HumanEval và MBPP. Tuy nhiên, một khi chúng tôi kết hợp ba thành phần vào CodeCoT, chúng ta có thể thấy rằng pass@1 tiếp tục đạt 79.3% và 89.5% trong dataset HumanEval và MBPP, cao hơn các chiến lược baseline của chúng tôi, ví dụ: 75.6% và 69.8% pass@1 trong HumanEval và MBPP, chứng minh rằng trong CodeCoT mỗi thành phần đều quan trọng và không thể bỏ qua.

4.5. Thảo luận thêm về hiệu quả của CodeCoT với các thành phần của nó
Trong phần này, chúng tôi muốn thảo luận tại sao pass@1 của CodeCoT sẽ tăng từ 68.3%/70.1% và 73.9%/79.0% lên 79.3% và 89.5% trong Bảng 4. Cụ thể, chúng tôi phân tích test case được tạo ra bởi Coder+Self-examination và CodeCoT trong Bảng 5. Chúng ta có thể thấy rằng đối với Coder+Self-examination, pass@1 của test case trên giải pháp chính tắc chỉ có 47.0% và 57.2% trong HumanEval và MBPP. Tuy nhiên, khi CoT prompt được bao gồm trong CodeCoT, pass@1 của test case trên giải pháp chính tắc tăng lên 67.1% và 79.0% trong những dataset này, điều này minh họa tại sao CodeCoT đạt hiệu suất cao hơn Coder+Self-examination. Cụ thể, CodeCoT đạt được các test case chính xác hơn để tự test và những test case này có thể hướng dẫn chính xác Coder tinh chỉnh code được tạo ra trong quy trình tự kiểm tra.

5. Kết luận
Trong bài báo này, chúng tôi giải quyết các lỗi runtime thường tồn tại khi sử dụng CoT prompt trong sinh code bằng cách đề xuất CodeCoT, sử dụng quy trình tự kiểm tra để phát hiện lỗi runtime trong code được tạo ra với các test được tạo ra của nó. Các đánh giá của chúng tôi cho thấy rằng CodeCoT được đề xuất cải thiện đáng kể pass@1 của sinh code trên các LLM khác nhau. Ví dụ, CodeCoT đạt 79.3% và 89.5% pass@1 trong dataset HumanEval và MBPP cho ChatGPT với quy trình tự kiểm tra. Công việc tương lai có thể điều tra các cải tiến thêm cho cách tiếp cận của chúng tôi, khám phá khả năng áp dụng của nó cho các lĩnh vực khác, và đi sâu hơn vào các cơ chế cơ bản làm cho CoT rất hiệu quả cho sinh code dựa trên LLM.

6. Hạn chế
Chúng tôi thảo luận về những hạn chế của công việc của chúng tôi có thể hy vọng truyền cảm hứng cho nghiên cứu tương lai trong hướng này. Thứ nhất, trong bài báo này, chúng tôi tập trung vào các mô hình đóng nguồn, ví dụ: ChatGPT (trong hầu hết các thí nghiệm), GPT-4 (trong phụ lục). Các mô hình mở nguồn, ví dụ: CodeT5+, StarCoder, CodeGen, và những mô hình khác không được đánh giá trong bài báo của chúng tôi. Lý do chính là CoT yêu cầu tham số lớn (>175B) có thể minh họa hiệu quả của nó (Wei et al., 2022), điều này hạn chế việc áp dụng CodeCoT trong mô hình mã nguồn mở. Do đó, chúng tôi khuyến khích điều tra tương lai cho các chiến lược suy luận khác (ví dụ: ToT, GoT) trong mô hình mã nguồn mở với các bước tự kiểm tra của chúng tôi. Thứ hai, CodeCoT yêu cầu nhiều tương tác với LLM cho quy trình tự kiểm tra, điều này sẽ tăng chi phí sinh code cả về thời gian và phí API. Vì vậy chúng tôi khuyến khích các nghiên cứu tương lai giảm thời gian tương tác. Cuối cùng, như được hiển thị trong Bảng 5, tester trong CodeCoT không thể đảm bảo tất cả test case đều đúng, có nghĩa là một số đoạn code được tạo ra bởi CodeCoT có thể sau đó sửa đổi thành một phiên bản lỗi. Trong tương lai, chúng tôi sẽ cố gắng định lượng các hàm được sửa đổi và điều tra cách tránh những hành vi này.
