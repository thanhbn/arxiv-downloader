# 2310.12426.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reasoning/2310.12426.pdf
# Kích thước tệp: 2524685 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
MAF: Phản hồi Đa khía cạnh để Cải thiện
Lý luận trong các Mô hình Ngôn ngữ Lớn
Deepak Nathani , David Wang , Liangming Pan , William Yang Wang
Đại học California, Santa Barbara, Santa Barbara, CA
{dnathani, d_wang, liangmingpan, william}@cs.ucsb.edu

Tóm tắt
Các Mô hình Ngôn ngữ (LM) đã cho thấy hiệu suất ấn tượng trong nhiều tác vụ ngôn ngữ tự nhiên khác nhau. Tuy nhiên, khi đến với lý luận ngôn ngữ tự nhiên, LM vẫn gặp phải những thách thức như ảo giác, tạo ra các bước lý luận trung gian không chính xác, và mắc lỗi toán học. Nghiên cứu gần đây đã tập trung vào việc nâng cao LM thông qua tự cải thiện bằng cách sử dụng phản hồi. Tuy nhiên, các phương pháp hiện tại dựa vào một nguồn phản hồi chung chung duy nhất không thể giải quyết được các loại lỗi đa dạng được tìm thấy trong chuỗi lý luận do LM tạo ra. Trong nghiên cứu này, chúng tôi đề xuất Phản hồi Đa khía cạnh, một khung tinh chỉnh lặp đi lặp lại tích hợp nhiều mô-đun phản hồi, bao gồm LM đóng băng và các công cụ bên ngoài, mỗi mô-đun tập trung vào một danh mục lỗi cụ thể. Kết quả thử nghiệm của chúng tôi chứng minh hiệu quả của phương pháp chúng tôi trong việc giải quyết một số lỗi trong chuỗi lý luận do LM tạo ra và do đó cải thiện hiệu suất tổng thể của LM trong một số tác vụ lý luận. Chúng tôi thấy một cải thiện tương đối lên đến 20% trong Lý luận Toán học và lên đến 18% trong Phát sinh Logic. Chúng tôi phát hành mã nguồn, prompt và dữ liệu để thúc đẩy nghiên cứu trong tương lai.

1 Giới thiệu
Nghiên cứu gần đây về Mô hình Ngôn ngữ đã tập trung vào việc tăng cường chúng với các công cụ bên ngoài (Schick et al., 2023; Paranjape et al., 2023), học từ phản hồi (Ouyang et al., 2022; Akyurek et al., 2023) và tinh chỉnh lặp đi lặp lại (Madaan et al., 2023; Paul et al., 2023; Shinn et al., 2023). Tinh chỉnh lặp đi lặp lại đã là một công cụ cần thiết trong tiến hóa và giải quyết vấn đề của con người. Hơn nữa, con người tìm kiếm phản hồi từ các nguồn kiến thức chuyên về lĩnh vực cụ thể. Ví dụ, một kiến trúc sư được giao nhiệm vụ tạo ra một thiết kế tòa nhà thân thiện với môi trường và vững chắc về mặt cấu trúc sẽ cần phản hồi có mục tiêu từ các kỹ sư xây dựng về tính toàn vẹn cấu trúc, và các chuyên gia bền vững về các nguyên tắc thiết kế thân thiện với môi trường. Tuy nhiên, các nghiên cứu trước đây về tinh chỉnh lặp đi lặp lại bỏ qua yêu cầu này và thu thập phản hồi chung chung từ nhiều nguồn hoặc thu thập một phản hồi duy nhất từ chính Mô hình Ngôn ngữ.

Trong nghiên cứu này, đầu tiên chúng tôi điều tra xem việc sử dụng phản hồi chung chung có phải là một nút cổ chai trong việc giải quyết phạm vi đa dạng các lỗi có mặt trong chuỗi lý luận được tạo ra bởi LM hay không. Chúng tôi cho rằng việc sử dụng phản hồi chung chung cho một phổ rộng các lỗi có thể dẫn đến phản hồi mơ hồ hoặc không thể hành động được đối với các lỗi cụ thể. Điều này là do hai yếu tố chính: (i) việc bao gồm nhiều danh mục lỗi trong một prompt duy nhất, điều này không chỉ làm tăng kích thước của prompt mà còn đặt ra thách thức cho các mô hình hiện tại có độ dài ngữ cảnh hạn chế và gặp khó khăn với văn bản dài, và (ii) mô hình bị gánh nặng với việc xác định nhiều loại lỗi trong một trường hợp duy nhất, điều này làm giảm chất lượng của phản hồi được tạo ra.

Để vượt qua những thách thức này, chúng tôi giới thiệu Phản hồi Đa khía cạnh (MAF), một khung tinh chỉnh lặp đi lặp lại mục đích chung mới sử dụng một tập hợp các mô-đun phản hồi chuyên biệt, bao gồm LM được đào tạo trước và các công cụ bên ngoài, mỗi mô-đun được thiết kế riêng để giải quyết một danh mục lỗi cụ thể. Khung của chúng tôi bao gồm một mô hình cơ sở để tạo ra giải pháp ban đầu, nhiều mô-đun phản hồi, và một mô hình tinh chỉnh để sửa đổi giải pháp bằng cách sử dụng phản hồi. Trái ngược với các nghiên cứu trước đây, các mô-đun phản hồi của chúng tôi có thể sử dụng một trong hai chiến lược tinh chỉnh: Tinh chỉnh Nhanh và Tinh chỉnh Chậm §3.4. Khi sử dụng chế độ Tinh chỉnh Nhanh, giải pháp được sửa đổi ngay lập tức trước khi chuyển sang phản hồi tiếp theo, tuy nhiên, đối với Tinh chỉnh Chậm, chúng tôi đầu tiên thu thập phản hồi từ nhiều nguồn phản hồi và sau đó tinh chỉnh giải pháp bằng cách sử dụng phản hồi tập thể này.

Trong việc thiết kế MAF, chúng tôi đầu tiên đã phân loại các lỗi trong chuỗi lý luận do LM tạo ra dựa trên Golovneva et al. (2022) và cũng xác định một số danh mục lỗi mới.

--- TRANG 2 ---
Input
Mô hình M
Bộ tinh chỉnh R
Máy tính
Cú pháp Lập trình
Redundancy
12
433Đầu vào được truyền đến Mô hình ban đầu MĐầu ra ban đầu O sau đó được truyền đến một tập hợp các mô-đun phản hồi FPhản hồi được tạo ra và giải pháp trước đó được truyền đến Bộ tinh chỉnh R
Giải pháp được tinh chỉnh bây giờ được sử dụng để tạo ra giải pháp mới cho đến khi chúng ta đạt được giải pháp đúng O'

Hình 1: Tổng quan về khung Phản hồi Đa khía cạnh. Chúng tôi lặp lại các bước 3 và 4 cho đến khi chúng ta đạt được giải pháp cuối cùng O' hoặc cho đến một số lần lặp đã đặt.

Hạng mục. Sau đó, chúng tôi đã tách biệt các mô-đun phản hồi của mình sao cho mỗi mô-đun tập trung vào một danh mục lỗi duy nhất. Chiến lược này không chỉ nâng cao hiệu suất mà còn cho phép tận dụng các công cụ chuyên biệt được thiết kế riêng cho các loại lỗi khác nhau, chẳng hạn như sử dụng trình thông dịch mã cho lỗi cú pháp trong các chương trình Python được tạo ra thay vì dựa vào phản hồi LM chung chung. Thông qua thiết kế modular, chúng tôi đảm bảo rằng mỗi mô-đun của khung của chúng tôi có thể thay thế được tùy thuộc vào tác vụ. Chúng tôi tiến hành thử nghiệm rộng rãi trên nhiều tác vụ lý luận phức tạp như Lý luận Toán học (Shi et al., 2023), Lý luận Logic (Dalvi et al., 2021), và Trả lời Câu hỏi Đa bước nhảy (Dua et al., 2019), và thiết lập rằng MAF hiệu quả giải quyết các thách thức đã thảo luận trước đây, và cải thiện đáng kể hiệu suất của Mô hình Ngôn ngữ trong các tác vụ lý luận phức tạp. Chúng tôi so sánh phương pháp của mình với LM Cơ sở và Self-Refine (Madaan et al., 2023), cũng thực hiện tinh chỉnh lặp đi lặp lại. Trong quá trình thử nghiệm, chúng tôi phát hiện ra rằng Tinh chỉnh Quá mức §4 giải pháp có thể dẫn đến hiệu suất tệ hơn, và do đó chúng tôi coi số lần lặp như một siêu tham số. Chúng tôi cũng đề xuất một thiết lập Oracle Verifier §4, nơi chúng tôi giả định rằng chúng tôi có quyền truy cập vào một "bộ xác minh câu trả lời" và dừng quá trình tinh chỉnh một khi chúng tôi đã đạt được câu trả lời đúng. Để tránh so sánh không công bằng, chúng tôi kiểm tra Self-Refine dưới thiết lập này và thấy rằng MAF cung cấp phản hồi có thể hành động và cụ thể.

Để tóm tắt, những đóng góp chính của nghiên cứu này như sau:

1. Chúng tôi đề xuất một khung mới tách biệt quá trình tạo phản hồi cho các danh mục lỗi khác nhau. Điều này cho phép chúng tôi sử dụng các công cụ và LM cụ thể cho lỗi. Hơn nữa, khung được đề xuất là modular và tất cả các mô-đun đều có thể Cắm và Chạy. Điều này cho phép linh hoạt tốt hơn trên các tác vụ mới và việc sử dụng các mô hình tiên tiến khi chúng trở nên có sẵn.

2. Ngoài ra, chúng tôi đề xuất hai loại chiến lược tinh chỉnh khác nhau, Tinh chỉnh Nhanh và Tinh chỉnh Chậm. Tinh chỉnh Nhanh cho phép sửa đổi ngay lập tức một giải pháp bằng mô-đun phản hồi để tránh xung đột, trong khi Tinh chỉnh Chậm cải thiện hiệu quả bằng cách thực hiện sửa đổi từ nhiều mô-đun phản hồi cùng nhau.

3. Chúng tôi cho thấy rằng khung của chúng tôi vượt trội hơn so với Mô hình Ngôn ngữ cơ sở và các baseline Tinh chỉnh Lặp đi lặp lại tương tự trên một số tác vụ lý luận phức tạp như lý luận Toán học (GSM-IC), Lý luận Logic (Entailment Bank), và Trả lời Câu hỏi Đa bước nhảy (DROP).

--- TRANG 3 ---
2 Nghiên cứu Liên quan

Lý luận Chuỗi Suy nghĩ Đã có rất nhiều nghiên cứu về việc khích lệ các Mô hình Ngôn ngữ Lớn để cải thiện khả năng lý luận của chúng. Wei et al. (2023) phát hiện ra rằng việc khích lệ các mô hình tạo ra một chuỗi lý luận trong thiết lập few-shot trước khi giải quyết một vấn đề cải thiện hiệu suất một cách đáng kể. Hơn nữa, Kojima et al. (2022) phát hiện ra rằng trong thiết lập zero-shot, việc thêm tiền tố "Let's think step-by-step" vào việc tạo giải pháp có tác dụng tương tự như việc tạo ra chuỗi lý luận trung gian và cải thiện hiệu suất. Theo nghiên cứu này, Wang et al. (2022) đề xuất lấy mẫu nhiều đầu ra từ mô hình và chọn câu trả lời cuối cùng bằng bỏ phiếu đa số. Zhou et al. (2023) cũng cho thấy rằng việc phân tách câu hỏi gốc thành các tiểu vấn đề nhỏ hơn có thể mang lại sự cải thiện hiệu suất. Madaan et al. (2022) và Chen et al. (2022) cho thấy rằng các mô hình được đào tạo trên mã có thể được sử dụng để tạo ra một chương trình suy nghĩ tương tự như chuỗi suy nghĩ, cho phép mô hình sử dụng trình thông dịch ngôn ngữ để chuyển đổi các phép tính toán học thành mã. Trong nghiên cứu này, chúng tôi sử dụng khả năng học trong ngữ cảnh của LM để thực hiện các mô-đun phản hồi và tinh chỉnh dựa trên LM.

LLM được Tăng cường bằng Công cụ Tuy nhiên, ngay cả với những kỹ thuật prompt tiên tiến này, LM vẫn thất bại đối với các vấn đề đòi hỏi kiến thức bên ngoài và vẫn dễ mắc phải các vấn đề như ảo giác và lý luận không chính xác. Để khắc phục những vấn đề này, một số nghiên cứu gần đây đã giới thiệu việc sử dụng các công cụ bên ngoài như máy tính, trình thông dịch mã, truy xuất kiến thức (Karpas et al., 2022; Chen et al., 2022; Schick et al., 2023). Yang et al. (2023); Patil et al. (2023); Shen et al. (2023) cho thấy rằng có thể dạy LLM sử dụng công cụ bằng cách tạo ra các lời gọi API. Li et al. (2023) cũng phát hành một tập dữ liệu lớn để cho phép nghiên cứu trong lĩnh vực Mô hình Ngôn ngữ được tăng cường. Hao et al. (2023) đề xuất biểu diễn mỗi công cụ như một token và học các embedding cụ thể cho công cụ để tăng tính robust của những mô hình ngôn ngữ này cho việc sử dụng API bên ngoài. Tuy nhiên, lý luận là một tác vụ lặp đi lặp lại và khó có thể tìm ra một câu trả lời hợp lý cho một số vấn đề trong một lần thử ngay cả khi những LLM này được tăng cường bằng công cụ. Điều này đã truyền cảm hứng cho một số nghiên cứu gần đây sử dụng khung tinh chỉnh lặp đi lặp lại. Trong nghiên cứu của chúng tôi, chúng tôi sử dụng các công cụ bên ngoài để tạo phản hồi cho các danh mục lỗi nhất định như Lỗi Cú pháp Lập trình, Máy tính cho các phương trình toán học, v.v.

Loại Tác vụ Phản hồi Loại ER
Lập trình Cú pháp Trình thông dịch !
Đặt tên Biến OpenAI !
Toán học Redundancy OpenAI #
Lý luận Commonsense OpenAI #
Bước Thiếu OpenAI #
Logic Redundancy OpenAI #
Lý luận Lặp lại OpenAI #
Ảo giác OpenAI #
Redundancy OpenAI #
Trả lời Factuality OpenAI #
Câu hỏi Commonsense OpenAI #
Bước Thiếu OpenAI #

Bảng 1: Các mô-đun phản hồi được sử dụng cho mỗi tác vụ và loại của chúng. Các mô-đun loại OpenAI sử dụng LLM để cung cấp phản hồi. Các Mô-đun Phản hồi với Eager-Refine (ER) được kích hoạt, tinh chỉnh giải pháp mà không chờ phản hồi từ các mô-đun phản hồi khác. (§3.4).

Học từ Phản hồi Schick et al. (2022) đã thực hiện bước đầu tiên hướng tới việc sửa chữa các vấn đề một cách lặp đi lặp lại và giới thiệu ý tưởng đào tạo nhiều phiên bản của cùng một mô hình để hỗ trợ trong các giai đoạn khác nhau của việc giải quyết vấn đề. Nó được giới thiệu như một khung chỉnh sửa cộng tác. (Madaan et al., 2023) đề xuất sử dụng cùng một mô hình để tạo giải pháp ban đầu, tạo phản hồi, và tinh chỉnh. Hơn nữa, gần đây đã có nhiều nghiên cứu khám phá việc sử dụng phản hồi ngôn ngữ tự nhiên để cải thiện hiệu suất. Shinn et al. (2023) đề xuất chuyển đổi các loại phản hồi khác nhau thành ngôn ngữ tự nhiên và lưu trữ phản hồi trong bộ đệm bộ nhớ để xem xét lại sau này. Akyurek et al. (2023) đào tạo một bộ tạo phê bình để tối đa hóa hiệu suất tác vụ cuối của một mô hình cơ sở lớn hơn bằng cách sử dụng học tăng cường. Paul et al. (2023) đề xuất một khung để tinh chỉnh LM để tạo ra các bước lý luận trung gian trong khi tương tác với một mô hình phê bình được đào tạo để thu hẹp khoảng cách giữa các mô hình nhỏ và lớn. Nghiên cứu gần đây (Wu et al., 2023) cũng đã chứng minh lợi ích của việc kết hợp phản hồi tinh vi tách biệt cho mỗi danh mục lỗi bằng cách sử dụng một tập hợp các mô hình phần thưởng được tinh chỉnh.

Mặc dù có những tiến bộ này, vẫn thiếu bằng chứng thực nghiệm hỗ trợ hiệu quả của phản hồi đa khía cạnh tách biệt cho tinh chỉnh lặp đi lặp lại. Nghiên cứu của chúng tôi giải quyết khoảng trống này bằng cách chứng minh rằng một mô-đun phản hồi chung chung, là không đủ để giải quyết phạm vi đa dạng các lỗi tiềm ẩn trong phản hồi của mô hình ngôn ngữ. Để vượt qua hạn chế này, chúng tôi đề xuất một bộ mô-đun phản hồi, mỗi mô-đun nhắm mục tiêu cụ thể vào một danh mục lỗi cụ thể và cung cấp phản hồi chi tiết để cải thiện cả chất lượng lý luận và giải pháp.

--- TRANG 4 ---
3 MAF: Phản hồi Đa khía cạnh

3.1 Tổng quan

Trong nghiên cứu này, chúng tôi trình bày một khung tinh chỉnh lặp đi lặp lại với trọng tâm rõ ràng vào việc tách biệt việc tạo phản hồi cho các danh mục lỗi khác nhau. Phương pháp này cho phép chúng tôi giải quyết một cách có hệ thống các loại lỗi khác nhau trong các giải pháp được tạo ra. Khung được đề xuất của chúng tôi có ba thành phần quan trọng: một mô hình ngôn ngữ cơ sở M tạo ra giải pháp ban đầu O. Một tập hợp n mô-đun phản hồi {f0, f1, f2...fn}, mỗi mô-đun tập trung vào một danh mục lỗi duy nhất, tập thể các mô-đun này tạo ra phản hồi đa khía cạnh F. Và một Bộ tinh chỉnh R tạo ra giải pháp tinh chỉnh O' dựa trên giải pháp ban đầu O và phản hồi F.

Mặc dù có những nghiên cứu khác cũng sử dụng tinh chỉnh lặp đi lặp lại (Madaan et al., 2023; Akyurek et al., 2023; Peng et al., 2023), theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên khám phá tác động của việc tách biệt các loại phản hồi khác nhau. Lấy cảm hứng từ ROSCOE (Golovneva et al., 2022), chúng tôi phân loại phản hồi thành mười danh mục khác biệt: số học, cú pháp lập trình, đặt tên biến, bước thiếu, tính nhất quán, redundancy, lặp lại, ảo giác, commonsense, và factuality. Định nghĩa của những danh mục lỗi này được cung cấp trong Phụ lục A. Hơn nữa, một mô-đun phản hồi có thể là một công cụ như trình thông dịch mã cho phản hồi cú pháp, máy tính cho phản hồi số học, đồ thị kiến thức cho phản hồi factuality hoặc commonsense, Mô hình Ngôn ngữ, hoặc thậm chí là một mô hình được tinh chỉnh. Điều quan trọng cần lưu ý là quá trình tạo phản hồi không giới hạn ở những danh mục này và có thể được mở rộng để bao gồm các danh mục khác. Quá trình tổng thể được minh họa trong Hình 1 và Thuật toán 1.

3.2 Tạo Ban đầu

Chúng tôi sử dụng một mô hình lớn như GPT3.5, GPT4 (OpenAI, 2023), để tạo ra một giải pháp ban đầu. Tuy nhiên, việc chỉ tạo ra một giải pháp không lý tưởng cho quá trình lý luận vì lý luận là một quá trình lặp đi lặp lại. Giải pháp thường cần được tinh chỉnh theo thời gian với mỗi lần lặp đưa chúng ta đến gần hơn với câu trả lời đúng. Chúng tôi tuân theo nguyên tắc tương tự trong nghiên cứu này, nơi chúng tôi ban đầu tạo ra một giải pháp và sau đó tiến hành tinh chỉnh nó dựa trên phản hồi có thể hành động.

3.3 Mô-đun Phản hồi

Tạo phản hồi là một tác vụ phức tạp, cung cấp một danh sách toàn diện các vấn đề phổ biến gặp phải trong đầu ra của mô hình. Việc tạo phản hồi được thực hiện thông qua nhiều công cụ khác nhau được gọi là mô-đun phản hồi, có thể bao gồm các công cụ bên ngoài, LLM đóng băng, mô hình được tinh chỉnh, và bộ chấm điểm. Những mô-đun này được sử dụng để cung cấp phản hồi có thể hành động dựa trên giải pháp ban đầu.

Mỗi loại mô-đun phản hồi phù hợp để giải quyết các loại lỗi cụ thể. Ví dụ, một công cụ bên ngoài như trình thông dịch mã sẽ lý tưởng để cung cấp phản hồi về lỗi cú pháp trong mã, trong khi một mô hình được tinh chỉnh có thể cung cấp phản hồi tinh tế hơn về các vấn đề như redundancy hoặc ảo giác. Phương pháp tách biệt này cho phép chúng tôi giải quyết các lỗi một cách có mục tiêu hơn, cải thiện chất lượng tổng thể của giải pháp được tinh chỉnh. Bảng 1 cho thấy tất cả các mô-đun phản hồi được sử dụng trong nghiên cứu của chúng tôi.

Hơn nữa, điều đáng chú ý là việc chỉ đơn giản chỉ định các danh mục lỗi trong một prompt duy nhất không mang lại kết quả thỏa đáng. Chúng tôi thấy hai lý do chính cho điều này, thứ nhất vì mô hình được giao nhiệm vụ tập trung vào nhiều lỗi đồng thời và thứ hai, khi số lượng danh mục lỗi tăng lên, độ dài ngữ cảnh cũng tăng lên, dẫn đến phản hồi chất lượng cao cho một vài danh mục lỗi đầu tiên và sự suy giảm mạnh về chất lượng cho phần còn lại. Trong khi một số nghiên cứu trước đây (Wu et al., 2023; Paul et al., 2023) đã khám phá việc sử dụng phản hồi tinh vi và đạt được kết quả đầy hứa hẹn.

Hơn nữa, các mô-đun phản hồi của chúng tôi có thể chọn giữa hai chiến lược tinh chỉnh: Eager-Refine hoặc Lazy-Refine. Chúng tôi thảo luận về sự phân biệt này chi tiết hơn trong phần tiếp theo.

3.4 Tinh chỉnh

Trong nghiên cứu này, chúng tôi tái sử dụng mô hình ban đầu M như một Bộ tinh chỉnh. Trong giai đoạn tinh chỉnh, bộ tinh chỉnh được cung cấp một giải pháp và phản hồi đa khía cạnh và được yêu cầu sửa đổi giải pháp theo phản hồi. Chúng tôi phát hiện ra rằng những mô hình lớn này thành thạo trong tác vụ tinh chỉnh, thể hiện sự cải thiện rõ rệt về chất lượng của giải pháp cuối cùng so với đầu ra ban đầu.

--- TRANG 5 ---
GIẢ THUYẾT: New York State có ánh nắng mặt trời lớn nhất trong tháng 6
VẢN BẢN: sent1: lượng ánh sáng ban ngày lớn nhất vào mùa hè
sent2: hoa kỳ nằm ở bán cầu bắc
sent3: new york / new york state là một bang nằm ở hoa kỳ
sent4: tháng 6 là trong mùa hè ở bán cầu bắc
sent5: lượng ánh sáng ban ngày lớn nhất vào ngày hạ chí
sent6: lượng ánh sáng ban ngày ít nhất vào mùa đông

TẠO BAN ĐẦU
sent2 và sent3->int1: new york nằm ở bán cầu bắc
int1 và sent4->int2: Tháng 6 là trong mùa hè ở bán cầu bắc
sent1 và sent6->int3: có nhiều ánh nắng mặt trời hơn vào mùa hè so với mùa đông
int1 và sent1->giả thuyết: new york state có ánh nắng mặt trời lớn nhất trong tháng 6

PHẢN HỒI BƯỚC THIẾU
int1 và sent1-> giả thuyết: new york state có ánh nắng mặt trời lớn nhất trong tháng 6
# sai! Cây suy luận thiếu một tiền đề trung gian thiết lập rằng đó là mùa hè ở new york vào tháng 6. Chúng ta có thể thêm tiền đề này như một kết luận trung gian.

PHẢN HỒI LẶP LẠI
int1 và sent4->int2: Tháng 6 là trong mùa hè ở bán cầu bắc
# sai! Bước suy luận này đơn giản lặp lại tiền đề trong kết luận trung gian int2. Chúng ta có thể loại bỏ hoàn toàn bước này.

PHẢN HỒI REDUNDANCY
sent1 và sent6->int3: có nhiều ánh nắng mặt trời hơn vào mùa hè so với mùa đông
# sai! Bước suy luận này hợp lệ nhưng không liên quan đến giả thuyết. Chúng ta có thể loại bỏ hoàn toàn bước này.

PHẢN HỒI SELF-REFINE
int1 và sent4->int2: Tháng 6 là trong mùa hè ở bán cầu bắc
# sai! Tiền đề int1 rằng new york nằm ở bán cầu bắc không dẫn đến kết luận trung gian int2 rằng Tháng 6 là trong mùa hè ở bán cầu bắc.
int1 và sent1->giả thuyết: new york state có ánh nắng mặt trời lớn nhất trong tháng 6
# sai! Bước này thiếu tiền đề chính rằng đó là mùa hè ở new york trong tháng 6. Chúng ta cần thêm điều này.

TẠO SELF-REFINE (KHÔNG CHÍNH XÁC)
sent2 và sent3 ->int1: new york nằm ở bán cầu bắc
sent4 ->int2: tháng 6 là trong mùa hè ở new york
sent1 và int2 ->giả thuyết: new york state có ánh nắng mặt trời lớn nhất trong tháng 6

TẠO MAF-REFINED (CHÍNH XÁC)
sent2 và sent3->int1: new york nằm ở bán cầu bắc
int1 và sent4->int2: tháng 6 là trong mùa hè ở new york
sent1 và int2->giả thuyết: new york state có ánh nắng mặt trời lớn nhất trong tháng 6

BỘ TINH CHỈNH

Hình 2: So sánh phản hồi được tạo ra bởi MAF và Self-Refine.

Như đã đề cập trong phần trước, chúng tôi sử dụng hai chiến lược tinh chỉnh: Tinh chỉnh Nhanh và Tinh chỉnh Chậm. Phương pháp Tinh chỉnh Nhanh được sử dụng cho các loại phản hồi có thể gây ra xung đột trong quá trình tinh chỉnh. Một ví dụ về mô-đun phản hồi như vậy sẽ là Đặt tên Biến (VN), được sử dụng để sửa tên biến trong mã được tạo ra. Mô-đun này có thể gây ra xung đột với các mô-đun khác vì khi các mô-đun khác đang tham chiếu đến một biến mà được cho là sẽ thay đổi theo phản hồi VN và có thể làm cho chương trình không thể thực thi được nếu được tinh chỉnh không chính xác. Trong khi đó, trong chiến lược tinh chỉnh chậm, phản hồi từ nhiều mô-đun được nối cùng nhau cùng với các danh mục lỗi thích hợp để tạo thành một phản hồi đa khía cạnh duy nhất. Phản hồi tập thể này sau đó được truyền đến mô hình Bộ tinh chỉnh để có được giải pháp được sửa đổi. Một lợi thế khác của phương pháp này là hiệu quả tăng lên bằng cách tinh chỉnh một lần cho nhiều lỗi và sự linh hoạt được thêm vào.

Tuy nhiên, chúng tôi cũng phát hiện ra rằng các mô hình mã nguồn mở nhỏ hơn như LLaMA (Touvron et al., 2023), Alpaca (Taori et al., 2023) và Vicuna thường không tuân thủ phản hồi được cung cấp, do đó làm cho quá trình tinh chỉnh không hiệu quả. Những mô hình nhỏ hơn này cũng có độ dài ngữ cảnh nhỏ hơn, làm cho việc bao gồm tất cả phản hồi trong prompt trở nên khó khăn. Để giải quyết vấn đề này, chúng tôi sử dụng phương pháp Tóm tắt Có chọn lọc. Chúng tôi chỉ chọn các phần phản hồi chỉ ra một vấn đề. Hình 2 cho thấy phản hồi "được tóm tắt" được tạo ra bởi phương pháp của chúng tôi. Phương pháp tóm tắt có chọn lọc đơn giản này làm cho phản hồi ngắn gọn và cũng chứng minh là ít gây xao nhãng cho các mô hình trong giai đoạn tinh chỉnh. Phương pháp này cho phép chúng tôi kết hợp hiệu quả tất cả phản hồi lại với nhau và sử dụng các mô hình có độ dài ngữ cảnh nhỏ hơn ở một mức độ nào đó.

4 Thử nghiệm

4.1 Tập dữ liệu và Phép đo

Lý luận Toán học Tập dữ liệu GSM8K, được trình bày bởi Cobbe et al. (2021), là một tập hợp toàn diện các bài toán toán học chất lượng cao cấp tiểu học. GSM8K đã chứng minh là một nguồn tài nguyên tuyệt vời để kiểm tra LLM trên các bài toán từ toán học, tuy nhiên, hiệu suất trên tập dữ liệu này đã bão hòa trong một thời gian.

Để tránh vấn đề đó, chúng tôi tiến hành thử nghiệm trên một biến thể khó hơn của tập dữ liệu này, GSM-IC (Grade-School Math with Irrelevant Context), được giới thiệu bởi Shi et al. (2023). Chúng tôi chạy các thử nghiệm chính của mình trên một tập con 500 bài toán được lấy mẫu ngẫu nhiên từ GSM-IC và sử dụng % tỷ lệ giải như là phép đo.

--- TRANG 6 ---
Thuật toán 1 Thuật toán MAF
Yêu cầu: Đầu vào x, mô hình M, Bộ tinh chỉnh R, số lần lặp T
Yêu cầu: nEager-refine {s1, s2, ...s n} và mLazy-Refine {p1, p2, ...p m} Mô-đun Phản hồi
1: Khởi tạo đầu ra y0 từ M
2: while i < T do
3: for j←1 to n do ▷ Phản hồi Eager-refine
4: fsj←generate _feedback (sj, yi) ▷ Tạo Phản hồi
5: if fsj chỉ ra cần sửa đổi then
6: yi←revise (R, fs j, yi) ▷ Sửa đổi giải pháp
7: F←"" ▷ Khởi tạo phản hồi trống
8: for k←1 to m do ▷ Phản hồi Lazy-refine
9: fpk←generate _feedback (pk, yi)
10: if fpj chỉ ra cần sửa đổi then
11: F←F+fpk
12: if F không trống then
13: yi+1←revise (R, fp k, yi)
14: else
15: yi+1=yi

Lý luận Logic EntailmentBank, như được mô tả bởi Dalvi et al. (2021), là một tập dữ liệu chứa các cây suy luận đa bước. Chúng tôi sử dụng tập validation của Tác vụ 1 được cung cấp bởi các tác giả cho thử nghiệm của chúng tôi. Đối với các phép đo, chúng tôi không sử dụng các phép đo tự động được cung cấp bởi nghiên cứu gốc vì những phép đo này mong đợi các cây có cấu trúc tương tự với các cây vàng. Tuy nhiên, đó không phải là một so sánh công bằng vì chúng tôi thấy rằng tồn tại nhiều cây suy luận đúng cho một giả thuyết và thông tin nhất định. Do đó, chúng tôi tiến hành đánh giá bằng con người và yêu cầu con người đánh giá xem giả thuyết có thể được suy ra từ cây dự đoán hay không.

Trả lời Câu hỏi DROP (Dua et al., 2019) là một tập dữ liệu trả lời câu hỏi. Tập dữ liệu Discrete Reasoning Over the Content of Paragraphs (DROP) được thiết kế cho các tác vụ trả lời câu hỏi phức tạp đòi hỏi lý luận đa bước trên các đoạn văn bản. Nó trình bày một benchmark có giá trị cho khung tinh chỉnh lặp đi lặp lại của chúng tôi, vì bản chất đa bước của các câu hỏi cung cấp nhiều cơ hội để tạo phản hồi và tinh chỉnh để hướng dẫn mô hình hướng tới giải pháp đúng. Chúng tôi sử dụng phân tích đầu ra cho câu trả lời và sử dụng % câu trả lời đúng làm phép đo cuối cùng.

4.2 Baseline

Trong nghiên cứu này, chúng tôi tập trung vào việc so sánh phương pháp của mình với LM Cơ sở sử dụng OpenAI API và một khung tinh chỉnh lặp đi lặp lại được đề xuất gần đây, Self-Refine (Madaan et al., 2023). Để cung cấp so sánh công bằng và tránh tính ngẫu nhiên trong câu trả lời được tạo ra, chúng tôi đã sử dụng Greedy Decoding cho tất cả các thử nghiệm của mình. Chúng tôi tuân theo (Madaan et al., 2023) và sử dụng 8-shot Program of Thought (Chen et al., 2022) cho GSM-IC vì chương trình Python được viết bởi mô hình hoạt động như một "máy tính" cho các phương trình toán học. Đầu vào cho Entailment Bank bao gồm một giả thuyết và văn bản hỗ trợ, và chúng tôi bị giới hạn bởi độ dài ngữ cảnh của các mô hình hiện tại. Do đó, chúng tôi sử dụng 4-shot prompting cho tập dữ liệu này. Chúng tôi sử dụng few-shot prompting tiêu chuẩn (Brown et al., 2020), vì Cây Suy luận tự nó hoạt động như một chuỗi lý luận.

Tương tự, các ví dụ trong tập dữ liệu DROP có một đoạn văn và một câu hỏi đi kèm, vì vậy chúng tôi sử dụng 3-shot Chain of Thought Wei et al. (2023) prompting. Chúng tôi cũng cung cấp một hướng dẫn chỉ định rằng mô hình nên chọn một số, ngày tháng, hoặc đoạn từ đoạn văn để trả lời câu hỏi như được hiển thị trong Phụ lục C.

Đối với Self-Refine, chúng tôi sử dụng các prompt được cung cấp bởi các tác giả cho GSM8K trong nghiên cứu của họ cho GSM-IC và viết prompt riêng của chúng tôi cho DROP và Entailment Bank. Self-Refine có ba mô-đun, tạo ban đầu, phản hồi, và bộ tinh chỉnh. Chúng tôi sử dụng cùng tham số và chiến lược prompting như baseline tương ứng cho việc tạo ban đầu. Mô-đun phản hồi và mô-đun bộ tinh chỉnh được thực hiện bằng cách sử dụng few-shot prompting tiêu chuẩn với 3 ví dụ trong ngữ cảnh cho tất cả các tập dữ liệu. Chi tiết thêm về việc thực hiện các baseline này có thể được tìm thấy trong Phụ lục C.

--- TRANG 7 ---
Mô hình EB GSMIC GSM8K DROP
GPT3.5 56.1 76.2 69.2 72.3
+SR 53.5↓2.6 77.0↑0.8 69.2↓0.0 62.0↓8.3
+SR★ 54.5↓1.6 87.0↑10.8 77.4↑8.2 77.5↑4.2
+MAF 54.5↓1.6 77.4↑1.2 69.8↑0.6 66.2↓6.1
+MAF★ 60.4↑4.3 91.4↑15.2 73.4↑4.2 76.4↑4.1
ChatGPT 60.4 72.0 71.8 70.7
+SR 65.8↑5.4 76.0↑4.0 74.6↑2.8 45.5↓25.2
+SR★ 67.4↑7.0 78.0↑6.0 79.4↑7.6 73.2↑2.5
+MAF 68.4↑8.0 77.8↑5.8 73.2↑1.4 67.9↓2.8
+MAF★ 71.7↑11.3 82.8↑10.8 76.6↑4.8 72.7↑2.0

Bảng 2: Kết quả thử nghiệm cho Entailment Bank (EB), GSMIC, GSM8K, tập dữ liệu DROP như được mô tả trong §4. SR đại diện cho Self-Refine (Madaan et al., 2023), và MAF đại diện cho phương pháp của chúng tôi. ★ đại diện cho thiết lập Oracle Verifier (§4.4). Điểm số tốt nhất cho thiết lập tiêu chuẩn được in đậm và gạch chân cho thiết lập Oracle Verifier.

[BIỂU ĐỒ: Hiển thị độ chính xác cho các baseline và MAF trên GSM-IC sử dụng ChatGPT dưới thiết lập Tiêu chuẩn và Oracle Verifier]

Hình 3: Độ chính xác cho baseline và MAF trên GSM-IC sử dụng ChatGPT dưới thiết lập Tiêu chuẩn và Oracle Verifier §4.4.

4.3 Thực hiện

Chúng tôi thực hiện MAF (Multi-Aspect Feedback) theo cấu trúc cơ bản được định nghĩa trong §3 và Thuật toán 1. Quá trình tinh chỉnh lặp đi lặp lại tiếp tục cho đến khi chúng tôi đạt được chất lượng đầu ra mong muốn hoặc một tiêu chí dừng cụ thể cho tác vụ, tối đa 4 lần lặp. Phương pháp của chúng tôi bao gồm một mô-đun tạo ban đầu, bộ tinh chỉnh, và các mô-đun phản hồi như được hiển thị trong Bảng 1. Mô-đun tạo ban đầu sử dụng cùng tham số và chiến lược prompting như baseline tương ứng. Mô-đun Eager-Refine của chúng tôi sử dụng 3-shot prompting tiêu chuẩn và Lazy-Refiner của chúng tôi, bao gồm phản hồi từ nhiều mô-đun, sử dụng phương pháp 2-shot prompting tiêu chuẩn. Các mô-đun phản hồi dựa trên OpenAI của chúng tôi cũng sử dụng 3-shot prompting tiêu chuẩn theo (Madaan et al., 2023) với hướng dẫn cụ thể cho lỗi.

Để cung cấp so sánh công bằng, chúng tôi sử dụng cùng prompt cho Baseline và Self-Refine bất cứ khi nào có thể. Chúng tôi sử dụng Greedy Decoding cho tất cả các mô-đun dựa trên OpenAI của chúng tôi để tránh bất kỳ tính ngẫu nhiên nào. Như đã đề cập trong §3.3, chúng tôi sử dụng phương pháp tóm tắt có chọn lọc để phù hợp với tất cả các phản hồi lazy-refine trong mô-đun bộ tinh chỉnh của chúng tôi. Chúng tôi sử dụng chiến lược dựa trên quy tắc cơ bản để tóm tắt phản hồi. Vì các mô-đun phản hồi của chúng tôi được hướng dẫn kiểm tra từng bước trung gian, chúng cũng bao gồm các bước không có lỗi trong phản hồi được tạo ra. Tuy nhiên, phản hồi về những dòng này không hữu ích vì không có thay đổi nào trong những bước đó. Do đó, chúng tôi tìm kiếm các bước có phản hồi "trông tốt" và loại bỏ những bước đó. Phương pháp đơn giản này giúp chúng tôi tăng hiệu quả của phương pháp bằng cách có thể bao gồm nhiều phản hồi trong lazy-refiner của chúng tôi.

4.4 Kết quả

Trong phần này, chúng tôi thảo luận về hiệu suất của phương pháp chúng tôi và so sánh nó với baseline. Chúng tôi cũng thảo luận về một thử nghiệm ablation nghiên cứu đóng góp của mỗi mô-đun phản hồi. Kết quả chính của nghiên cứu này được hiển thị trong Bảng 2. Chúng tôi thấy rằng phương pháp của chúng tôi có thể vượt trội hơn LM cơ sở và Self-Refine trên một tập hợp đa dạng các tập dữ liệu lý luận. Đối với MAF, chúng tôi báo cáo kết quả sau 2 lần lặp, và theo Madaan et al. (2023) báo cáo kết quả Self-Refine sau 4 lần lặp.

GPT3.5 so với ChatGPT Chúng tôi thấy rằng GPT3.5 có thể tạo ra phản hồi tốt hơn nếu có lỗi trong giải pháp, tuy nhiên thường chỉ ra các lỗi giả mao ngay cả khi chúng không có mặt. Mặt khác, ChatGPT thận trọng hơn trong việc tạo phản hồi và thường không phát hiện ra lỗi ngay cả trong một giải pháp có lỗi. Do hành vi mâu thuẫn này từ hai mô hình, phương pháp của chúng tôi có thể đạt được hiệu suất tương tự khi sử dụng cả hai mô hình, mặc dù ChatGPT thường được coi là một mô hình tốt hơn GPT3.5.

--- TRANG 8 ---
Tinh chỉnh Quá mức Do hành vi được mô tả ở trên, chúng tôi gặp phải vấn đề Tinh chỉnh Quá mức. Điều này có nghĩa là một khi chúng tôi đã đạt được một giải pháp tối ưu cho một vấn đề lý luận, việc buộc LM tinh chỉnh nó thêm sẽ làm giảm hiệu suất và chất lượng của chuỗi lý luận. Như được hiển thị trong Bảng 2 và Hình 3, Self-Refine (Madaan et al., 2023) cũng gặp phải vấn đề này trong tập dữ liệu DROP. Hơn nữa, tất cả các khung tinh chỉnh lặp đi lặp lại khác như Self-Refine sử dụng điều kiện dừng và dừng quá trình tinh chỉnh nếu phản hồi được tạo ra không chỉ ra bất kỳ vấn đề nào. Tuy nhiên, chúng tôi không thể tận dụng điều kiện dừng này vì sự tương tác giữa nhiều mô-đun phản hồi. Ví dụ, giả sử mô-đun Bước Thiếu của chúng tôi dừng ở lần lặp k do không có bước thiếu tại thời điểm đó vì các mô-đun phản hồi khác tiếp tục tinh chỉnh giải pháp và tự tinh chỉnh là một quá trình không hoàn hảo, nó có thể giới thiệu lỗi Bước Thiếu trong các lần lặp tiếp theo.

Do đó, chúng tôi coi số lần lặp như một siêu tham số và thấy rằng 2 lần lặp hoạt động tốt nhất cho phương pháp của chúng tôi.

Oracle Verifier Vì trọng tâm chính của nghiên cứu này là cải thiện các thế hệ ban đầu không chính xác từ mô hình, chúng tôi cũng đánh giá các mô hình dưới thiết lập Oracle Verifier. Trong thiết lập này, chúng tôi giả định quyền truy cập vào một "Oracle Verifier" có thể đánh giá câu trả lời cuối cùng được tạo ra bởi mô hình. Nếu câu trả lời cuối cùng là đúng, chúng tôi dừng quá trình tinh chỉnh cho mẫu kiểm tra đó, nếu không, chúng tôi để mô hình tiếp tục tinh chỉnh giải pháp. Điều quan trọng cần lưu ý tuy nhiên là mô hình không được biết về việc xác minh này, và do đó vẫn có thể dừng tinh chỉnh thêm bằng cách không tạo ra phản hồi có thể hành động trong lần lặp tiếp theo. Dưới thiết lập sửa đổi này, chúng tôi báo cáo kết quả cho cả Self-Refine và MAF sau 4 lần lặp. Ngay cả dưới thiết lập sửa đổi này, chúng tôi thấy rằng phương pháp của chúng tôi có thể vượt trội hơn Self-Refine vì chúng tôi tạo ra phản hồi đa dạng và do đó có thể cải thiện nhiều giải pháp hơn trong tập dữ liệu GSM-IC.

4.5 Ablation

Trong phần này, chúng tôi thực hiện hai nghiên cứu ablation. Thứ nhất, chúng tôi phân tích tác động của mỗi mô-đun phản hồi, xác định những mô-đun có ảnh hưởng đáng kể nhất đến hiệu suất. Thứ hai, chúng tôi so sánh kết quả của hai chiến lược tinh chỉnh được đề xuất: Lazy so với Eager Refinement. Các nghiên cứu ablation được tiến hành trên tập dữ liệu GSM-IC. Do hạn chế về tài nguyên, chúng tôi tiến hành các nghiên cứu ablation sau đây trên một tập con ngẫu nhiên nhỏ hơn có kích thước 100.

Mô hình Tiêu chuẩn Oracle
ChatGPT 73.0 −
+MAF 79.0 84.0
−Variable Naming (VN) 79.0 ↓0.0 83.0 ↓1.0
−Redundancy (Red) 70.0 ↓9.0 80.0 ↓4.0
−Commonsense (Com) 73.0 ↓6.0 80.0 ↓4.0
−Missing Step (MS) 79.0 ↓0.0 84.0 ↓0.0
MAF (VN,Red,Com) 79.0 85.0

Bảng 3: Bảng này cho thấy đóng góp của mỗi mô-đun phản hồi trong MAF cho tập dữ liệu GSM-IC. Ký hiệu − trước một mô-đun biểu thị độ chính xác của phương pháp chúng tôi sau khi loại bỏ mô-đun phản hồi đó. MAF (VN,Red,Com) cho thấy độ chính xác của phương pháp chúng tôi chỉ sử dụng các mô-đun phản hồi hoạt động tốt nhất.

Đóng góp của các Mô-đun Phản hồi khác nhau
Chúng tôi kiểm tra đóng góp của mỗi mô-đun phản hồi bằng cách loại bỏ mô-đun đó và tính toán độ chính xác dưới thiết lập tiêu chuẩn và oracle verifier. Trong khi thiết lập tiêu chuẩn làm nổi bật các tác động tiêu cực tiềm ẩn của một số mô-đun, thiết lập oracle verifier làm nổi bật đóng góp tuyệt đối của mỗi mô-đun. Điều này làm nổi bật một phát hiện quan trọng rằng việc sử dụng các danh mục lỗi là tối quan trọng để đạt được hiệu suất. Kết quả cho nghiên cứu ablation này có thể được tìm thấy trong Bảng 3.

Chúng tôi cũng tính toán độ chính xác của phương pháp chúng tôi khi sử dụng ba mô-đun hoạt động tốt nhất như nguồn phản hồi duy nhất. Chúng tôi thấy rằng điều này thực sự khôi phục hiệu suất của phương pháp chúng tôi. Mặc dù các mô-đun Variable Naming và Missing Step không ảnh hưởng đến hiệu suất của MAF một cách lớn, nó vẫn làm cho phương pháp của chúng tôi robust hơn đối với sự thay đổi phân phối có thể xảy ra. Hơn nữa, đóng góp chính của mô-đun Variable Naming không phải là tăng hiệu suất, mà là tăng tính dễ đọc của mã và không làm bối rối người dùng với tên không rõ ràng.

Lưu ý rằng chúng tôi không loại bỏ mô-đun Programming Syntax checker vì chúng tôi sử dụng Program of Thoughts đòi hỏi trình thông dịch Python.

Lazy so với Eager Refinement Để minh họa bản chất bổ sung của hai chiến lược tinh chỉnh được đề xuất, chúng tôi đánh giá hiệu suất của khung tinh chỉnh lặp đi lặp lại của chúng tôi sử dụng tất cả các mô-đun phản hồi ở chế độ Lazy hoặc Eager. Kết quả được trình bày trong Bảng 4, cho thấy hiệu suất của khung của chúng tôi dưới các thiết lập tinh chỉnh khác nhau. Trong bảng này, 'MAF' tương ứng với kết quả thu được bằng cách kết hợp cả chiến lược Lazy và Eager Refinement, như được định nghĩa trong Bảng 1, trong khi 'Only Lazy/Eager-Refine' hiển thị kết quả của khung của chúng tôi khi chỉ sử dụng một loại chiến lược tinh chỉnh.

--- TRANG 9 ---
Mô hình Tiêu chuẩn Oracle
GPT3.5 71.0 71.0
+MAF 67.0↓4.0 85.0↑14.0
+Only Lazy-Refine 66.0↓5.0 81.0↑10.0
+Only Eager-Refine 66.0↓5.0 85.0↑14.0
ChatGPT 73.0 73.0
+MAF 78.0↑5.0 81.0↑8.0
+Only Lazy-Refine 69.0↓4.0 80.0↑7.0
+Only Eager-Refine 73.0↓0.0 80.0↑7.0

Bảng 4: Độ chính xác trên GSM-IC khi sử dụng các chiến lược tinh chỉnh khác nhau dưới thiết lập Tiêu chuẩn và Oracle. MAF cho thấy hiệu suất của phương pháp chúng tôi khi sử dụng cả Lazy và Eager refine kết hợp. Only Lazy-Refine có nghĩa là tất cả các mô-đun phản hồi sử dụng Lazy-Refine và tương tự Only Eager-Refine có nghĩa là tất cả các mô-đun sử dụng chiến lược Eager-Refine. Điểm số tốt nhất cho mỗi thiết lập được in đậm.

Kết quả rõ ràng chứng minh rằng khung của chúng tôi, tận dụng sự kết hợp của tinh chỉnh eager và lazy, liên tục bằng hoặc vượt trội hơn so với việc sử dụng một chiến lược riêng lẻ, trên cả thiết lập tiêu chuẩn và oracle verifier.

Các cân nhắc thực tế cũng ủng hộ việc sử dụng phương pháp lai kết hợp cả phản hồi eager và lazy. Chỉ dựa vào phản hồi lazy có thể dẫn đến tình huống mà nhiều danh mục phản hồi được nén thành một prompt duy nhất. Mặc dù có kỹ thuật tóm tắt phản hồi của chúng tôi, điều này có thể dẫn đến prompt lặp vượt quá giới hạn cửa sổ ngữ cảnh của nhiều mô hình có sẵn rộng rãi. Ngược lại, việc chỉ sử dụng phản hồi eager có thể dẫn đến việc viết lại giải pháp cho mỗi mô-đun phản hồi, dẫn đến việc sử dụng token cao. Trong khi sử dụng tất cả các mô-đun trong chế độ Eager-Refine có thể tiếp cận gần hiệu suất của 'MAF' (như được hiển thị trong Bảng 4), nó không có thể mở rộng khi xử lý một số lượng lớn các mô-đun phản hồi.

5 Kết luận

Trong nghiên cứu này, chúng tôi trình bày Multi-Aspect Feedback (MAF), một khung tinh chỉnh lặp đi lặp lại mới tách biệt các mô-đun phản hồi và tận dụng các công cụ cụ thể cho lỗi để tạo phản hồi. Chúng tôi chứng minh hiệu suất của khung của chúng tôi trên một tập hợp các tác vụ lý luận đa dạng và so sánh nó với các baseline tinh chỉnh lặp đi lặp lại khác.

Trái ngược với các nghiên cứu trước đây, chúng tôi thấy rằng Tinh chỉnh Quá mức có thể là một vấn đề trong các khung tinh chỉnh lặp đi lặp lại vì các mô hình không chắc chắn liệu câu trả lời của chúng có đúng hay không. Nghiên cứu của chúng tôi cũng tập trung vào sự cần thiết phải thiết kế các phương pháp phản hồi tốt hơn và kêu gọi tăng cường Mô hình Ngôn ngữ với chúng. Chúng tôi hy vọng nghiên cứu này sẽ truyền cảm hứng cho nghiên cứu sâu hơn trong lĩnh vực này và để đạt được mục tiêu đó, chúng tôi cung cấp tất cả mã, dữ liệu và prompt của chúng tôi.

Hạn chế

Hạn chế chính của phương pháp chúng tôi là các mô hình cơ sở cần có khả năng học trong ngữ cảnh đủ để xử lý phản hồi và tinh chỉnh giải pháp. Ngay cả với khả năng học trong ngữ cảnh, những mô hình này không hoàn hảo và do đó vẫn có thể mắc lỗi khi tinh chỉnh giải pháp ngay cả khi được cung cấp phản hồi đúng.

Tất cả các thử nghiệm được tiến hành trong nghiên cứu này sử dụng các mô hình lớn mạnh mẽ được cung cấp bởi OpenAI. Chúng tôi thấy rằng các LM mã nguồn mở như Vicuna và Alpaca có thể tạo ra giải pháp ban đầu khá tốt nhưng không có khả năng tinh chỉnh giải pháp của chính chúng. Do đó, chúng tôi để việc điều tra cải thiện các mô hình mã nguồn mở cho nghiên cứu tương lai.

Một hạn chế khác vốn có trong phương pháp của chúng tôi là sự phụ thuộc vào một tập hợp cố định các Mô-đun Phản hồi. Phương pháp của chúng tôi đòi hỏi việc chọn trước các mô-đun phản hồi trước khi thực hiện, điều này đến lượt nó đòi hỏi sự can thiệp của con người để xác định các danh mục phản hồi thích hợp cho mỗi tập dữ liệu hoặc lĩnh vực mới. Nghiên cứu tương lai có thể khám phá các phương pháp mới có thể xác định động và tự động các mô-đun phản hồi phù hợp nhất cho các vấn đề cụ thể trong thời gian thực.

Tuyên bố Đạo đức

Các thử nghiệm trong nghiên cứu này được thực hiện với các mô hình không phải mã nguồn mở và luôn thay đổi. Hơn nữa, những mô hình này đắt đỏ để sử dụng, và do đó nghiên cứu sử dụng những mô hình này đòi hỏi một lượng tài trợ khổng lồ. Tài liệu hiện có thiếu chi tiết về các tập dữ liệu đang được sử dụng để đào tạo những mô hình khổng lồ này hoặc cơ chế lọc đang được sử dụng để làm sạch các corpora bị ô nhiễm.

Hơn nữa, luôn có khả năng các tác nhân xấu sử dụng phương pháp của chúng tôi để tạo ra văn bản độc hại hoặc có hại hơn. Phương pháp của chúng tôi không bảo vệ chống lại điều này.

Lời cảm ơn

Nghiên cứu này được hỗ trợ bởi giải thưởng Quỹ Khoa học Quốc gia #2048122. Các quan điểm được bày tỏ là của các tác giả và không phản ánh chính sách hoặc lập trường chính thức của chính phủ Hoa Kỳ. Ngoài ra, chúng tôi cảm ơn các reviewer về phản hồi chi tiết và hữu ích của họ.

Tài liệu tham khảo

Afra Feyza Akyurek, Ekin Akyürek, Aman Madaan, A. Kalyan, Peter Clark, D. Wijaya, và Niket Tandon. 2023. Rl4f: Generating natural language feedback with reinforcement learning for repairing model outputs. ArXiv, abs/2305.08844. 1, 3, 4

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Language models are few-shot learners. ArXiv, abs/2005.14165. 6

Wenhu Chen, Xueguang Ma, Xinyi Wang, và William W. Cohen. 2022. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. 3, 6

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, và John Schulman. 2021. Training verifiers to solve math word problems. ArXiv, abs/2110.14168. 5

Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, và Peter Clark. 2021. Explaining answers with entailment trees. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 7358–7370, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. 2, 6

Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, và Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Trong Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), trang 2368–2378, Minneapolis, Minnesota. Association for Computational Linguistics. 2, 6

Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, và Asli Celikyilmaz. 2022. Roscoe: A suite of metrics for scoring step-by-step reasoning. 1, 4, 12, 13

Shibo Hao, Tianyang Liu, Zhen Wang, và Zhiting Hu. 2023. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. ArXiv, abs/2305.11554. 3

Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit

--- TRANG 11 ---
Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, và Moshe Tenenholtz. 2022. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. 3

Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, và Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Trong Advances in Neural Information Processing Systems, tập 35, trang 22199–22213. Curran Associates, Inc. 3

Minghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, và Yongbin Li. 2023. Apibank: A benchmark for tool-augmented llms. ArXiv, abs/2304.08244. 3

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, và Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. 1, 2, 3, 4, 6, 7, 8

Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, và Graham Neubig. 2022. Language models of code are few-shot commonsense learners. ArXiv, abs/2210.07128. 3

OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774. 4

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, và Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155. 1

Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, và Marco Tulio Ribeiro. 2023. Art: Automatic multi-step reasoning and tool-use for large language models. 1

Shishir G. Patil, Tianjun Zhang, Xin Wang, và Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive apis. ArXiv, abs/2305.15334. 3

Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, và Boi Faltings. 2023. Refiner: Reasoning feedback on intermediate representations. ArXiv, abs/2304.01904. 1, 3, 4

Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, và Jianfeng Gao. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. 4

Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, và Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. 1, 3

Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christoforos Nalmpantis, Edouard Grave, và Sebastian Riedel. 2022. Peer: A collaborative language model. ArXiv, abs/2208.11663. 3

Yongliang Shen, Kaitao Song, Xu Tan, Dong Sheng Li, Weiming Lu, và Yue Ting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. ArXiv, abs/2303.17580. 3

Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, và Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context. 2, 5

Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, và Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. 1, 3

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, và Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca. 5

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. 2023. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971. 5

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, và Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. ArXiv, abs/2203.11171. 3

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, và Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. 3, 6

Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, và Hanna Hajishirzi. 2023. Fine-grained human feedback gives better rewards for language model training. 3, 4

Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, và Ying Shan. 2023. Gpt4tools: Teaching large language model to use tools via self-instruction. ArXiv, abs/2305.18752. 3

--- TRANG 12 ---
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, và Ed Chi. 2023. Least-to-most prompting enables complex reasoning in large language models. 3

A Danh mục Lỗi

Trong phần này chúng tôi định nghĩa tất cả các danh mục lỗi được triển khai trong nghiên cứu này. Bảng 5 cho thấy các danh mục lỗi được giới thiệu bởi Golovneva et al. (2022).

A.1 Phản hồi Cú pháp Lập trình

Mô-đun phản hồi cú pháp lập trình được triển khai như Trình thông dịch Python. Mô-đun này nhằm mục đích sửa bất kỳ lỗi cú pháp nào trong mã được tạo ra. Mô-đun cụ thể này được hưởng lợi từ phương pháp Tinh chỉnh Nhanh.

A.2 Phản hồi Đặt tên Biến

Tên biến tốt trong mã có thể cải thiện tính dễ đọc của mã và có thể cải thiện khả năng hiểu chương trình của chính mô hình trong các lần lặp tiếp theo. Phản hồi Đặt tên Biến là một mô-đun khác được hưởng lợi từ phương pháp eager-refine.

A.3 Phản hồi Redundancy

Thông tin dư thừa là bất kỳ thông tin nào được bao gồm trong lý luận mà không giúp trả lời câu hỏi. Thông tin bổ sung này có thể làm xao nhãng mô hình khỏi việc trả lời đúng và do đó nên được loại bỏ.

A.4 Phản hồi Commonsense

Lỗi lý luận commonsense là lỗi về bất kỳ mối quan hệ hoặc kiến thức nào mà nên được biết từ thế giới chung như "tất cả vịt đều là chim".

A.5 Bước Thiếu

Lỗi bước thiếu là bất kỳ khoảng trống nào trong lý luận hoặc thông tin thiếu ngăn chuỗi lý luận trở nên chính xác. Điều này cũng xác định mô hình nói rằng câu hỏi không thể trả lời như một lỗi bước thiếu vì điều đó có nghĩa là cần thêm các bước lý luận để trả lời câu hỏi từ đoạn văn.

A.6 Phản hồi Factuality

Lỗi factuality là các trường hợp mà lý luận câu trả lời nêu thông tin không có thật. Điều này có thể là thông tin mâu thuẫn với thông tin được đưa ra trong đoạn văn hoặc bị ảo giác.

A.7 Phản hồi Ảo giác

LLM dễ bị ảo giác, tuy nhiên, đã được chứng minh rằng việc lấy mẫu từ LLM nhiều lần và sau đó chọn câu trả lời đa số có thể cải thiện kết quả. Do đó phản hồi Ảo giác nhằm mục đích sửa bất kỳ sự thật bị ảo giác nào trong thế hệ ban đầu. Mô-đun này có thể được cải thiện bằng cách sử dụng một công cụ bên ngoài như Nguồn Kiến thức thay vì LLM.

B Tham số Triển khai

Bảng 6 cung cấp các tham số được sử dụng cho Self-Refine và Bảng 7 cung cấp các tham số được sử dụng cho MAF.

C Ví dụ Prompt Few-Shot

Chúng tôi thêm các mẫu cho tất cả các prompt được sử dụng trong nghiên cứu của chúng tôi. Prompt hoàn chỉnh có thể được tìm thấy trong mã nguồn của chúng tôi. Lưu ý rằng đối với tất cả các ví dụ prompt phản hồi, có ít nhất một ví dụ không có lỗi, trong trường hợp đó phản hồi sẽ nêu rằng không có lỗi và lý luận là đúng. Điều này giúp giảm khả năng mô hình xác định lỗi trong giải pháp thực sự đúng.

--- TRANG 13 ---
Loại Lỗi Định nghĩa
Cú pháp Lập trình Lỗi cú pháp trong mã
Số học Lỗi trong tính toán toán học
Ngữ pháp Cách sử dụng ngữ pháp có lỗi, không thông thường, hoặc gây tranh cãi
Tính nhất quán Các bước mâu thuẫn với nhau hoặc không tuân theo một câu chuyện mạch lạc
Đặt tên Biến Tên biến trong chương trình không cung cấp thông tin đầy đủ hoặc bị sai
Lặp lại Bước diễn giải thông tin đã được đề cập trong các bước lý luận trước đó
Ảo giác Thông tin không được cung cấp trong câu nói của vấn đề và không liên quan hoặc sai
Commonsense Mô hình thiếu các mối quan hệ mà nên được biết từ thế giới chung (ví dụ: "1 tá = 12")
Factuality Thông tin về một đối tượng (tức là số lượng, đặc điểm) hoặc một thực thể được đặt tên không khớp với ngữ cảnh đầu vào
Bước Thiếu Nội dung của lý luận được tạo ra không hoàn chỉnh và thiếu thông tin cần thiết để đưa ra câu trả lời đúng
Redundancy Giải thích chứa thông tin dư thừa, mặc dù có thể là thật, nhưng không cần thiết để trả lời câu hỏi

Bảng 5: Các loại lỗi như được định nghĩa trong Golovneva et al. (2022). Mỗi danh mục lỗi được định nghĩa cho một bước duy nhất trong chuỗi lý luận.

Tập dữ liệu LM Cơ sở Phản hồi Bộ tinh chỉnh
EB 300 600 600
GSM-IC 300 600 600
GSM8K 300 600 600
DROP 450 600 800

Bảng 6: Giá trị của tham số Số Token Tối đa cho Self-Refine trên các tập dữ liệu khác nhau.

Tập dữ liệu LM Cơ sở Phản hồi Bộ tinh chỉnh
EB 300 600 600
GSM-IC 300 600 600
GSM8K 300 600 600
DROP 450 600 800

Bảng 7: Giá trị của tham số Số Token Tối đa cho MAF trên các tập dữ liệu khác nhau.

--- TRANG 14 ---
[Nội dung này chứa các ví dụ prompt chi tiết cho việc khởi tạo, phản hồi và tinh chỉnh trong các tác vụ lý luận toán học. Do độ dài và tính chất kỹ thuật, tôi sẽ tóm tắt rằng đây là các prompt mẫu được sử dụng để hướng dẫn mô hình trong việc giải quyết các bài toán toán học theo từng bước.]

Hình 4: Prompt khởi tạo cho Lý luận Toán học

--- TRANG 15-26 ---
[Các trang này chứa nhiều ví dụ prompt chi tiết cho các loại phản hồi khác nhau (Commonsense, Missing Step, Redundancy, Variable Naming) và các prompt tinh chỉnh cho cả MAF và Self-Refine trên các tác vụ lý luận toán học và trả lời câu hỏi. Mỗi prompt bao gồm các ví dụ cụ thể về cách mô hình nên phân tích và cải thiện các giải pháp được đưa ra.]
