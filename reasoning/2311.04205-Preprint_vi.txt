# 2311.04205.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reasoning/2311.04205.pdf
# Kích thước tệp: 3713121 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo
Diễn đạt lại và Phản hồi: Để các Mô hình Ngôn ngữ Lớn Đặt Ra Những Câu Hỏi Tốt Hơn cho Chính Mình
Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu
Khoa Khoa học Máy tính
Đại học California Los Angeles
{yihedeng,weightzero,chenzx19,qgu }@cs.ucla.edu
Tóm tắt
Những hiểu lầm không chỉ phát sinh trong giao tiếp giữa con người với nhau mà còn giữa con người và các Mô hình Ngôn ngữ Lớn (LLMs). Những khác biệt như vậy có thể khiến LLMs diễn giải những câu hỏi có vẻ rõ ràng theo những cách bất ngờ, dẫn đến những phản hồi sai lầm. Mặc dù được thừa nhận rộng rãi rằng chất lượng của một prompt, chẳng hạn như một câu hỏi, có tác động đáng kể đến chất lượng phản hồi được cung cấp bởi LLMs, một phương pháp hệ thống để tạo ra những câu hỏi mà LLMs có thể hiểu rõ hơn vẫn chưa được phát triển đầy đủ. Trong bài báo này, chúng tôi trình bày một phương pháp có tên 'Diễn đạt lại và Phản hồi' (RaR), cho phép LLMs diễn đạt lại và mở rộng những câu hỏi do con người đặt ra và cung cấp phản hồi trong một prompt duy nhất. Cách tiếp cận này đóng vai trò như một phương pháp prompting đơn giản nhưng hiệu quả để cải thiện hiệu suất. Chúng tôi cũng giới thiệu một biến thể hai bước của RaR, trong đó một LLM diễn đạt lại trước tiên diễn đạt lại câu hỏi và sau đó chuyển câu hỏi gốc và câu hỏi đã diễn đạt lại cùng nhau cho một LLM phản hồi khác. Điều này tạo điều kiện cho việc sử dụng hiệu quả các câu hỏi đã diễn đạt lại được tạo ra bởi một LLM với một LLM khác. Các thí nghiệm của chúng tôi chứng minh rằng phương pháp của chúng tôi cải thiện đáng kể hiệu suất của các mô hình khác nhau trên nhiều nhiệm vụ. Chúng tôi tiếp tục cung cấp một cuộc thảo luận toàn diện với các phương pháp Chain-of-Thought (CoT) phổ biến. Chúng tôi cho thấy rằng RaR bổ sung cho CoT và có thể được kết hợp với CoT để đạt được hiệu suất thậm chí còn tốt hơn. Công trình của chúng tôi không chỉ góp phần nâng cao hiệu suất LLM một cách hiệu quả mà còn làm sáng tỏ việc đánh giá công bằng các khả năng của LLM.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLMs), chẳng hạn như dòng GPT (Radford et al., 2019; Brown et al., 2020; OpenAI, 2023), đã chứng kiến sự gia tăng phổ biến do tác động sâu sắc của chúng đối với nhiều ứng dụng thực tế, bao gồm trả lời câu hỏi (Lu et al., 2023), sinh mã (Poesia et al., 2022), và các tác nhân hội thoại (Bozkurt, 2023). Khả năng ứng dụng rộng rãi và hiệu quả của những mô hình này đã dẫn đến nghiên cứu phát triển nhanh chóng về việc hiểu và cải thiện việc sử dụng LLMs. Trong khi đó, được thừa nhận rộng rãi rằng chất lượng của prompt được tạo ra bởi con người có ảnh hưởng quan trọng đến chất lượng phản hồi của LLMs, nhấn mạnh tầm quan trọng của những truy vấn hiệu quả ưu tiên tính cụ thể, chi tiết và chính xác (OpenAI, 2022). Mặt khác, độ nhạy cảm với chất lượng prompt hạn chế việc sử dụng LLMs với toàn bộ tiềm năng của chúng và đòi hỏi một cách tiếp cận hệ thống để cải thiện prompt của con người.

Những hiểu lầm trong giao tiếp giữa con người thường phát sinh khi các cá nhân, được định hình bởi những trải nghiệm chủ quan riêng biệt, diễn giải cùng một thông điệp theo những cách khác nhau. Trong khoa học xã hội, những hiện tượng như vậy có thể được quy cho những thiên kiến nhận thức trong khung tư duy (Druckman, 2001). Một thông điệp duy nhất, được đóng khung theo những cách khác nhau, có thể dẫn các cá nhân đến những kết luận khác nhau. Mọi người thường xuyên chiếu những khung của họ lên thông tin nhận được, và chỉ thay đổi những khung này khi sự không phù hợp phát sinh. Trong công trình này, chúng tôi cho rằng LLMs cũng thể hiện những khung tư duy riêng của chúng, và không hiếm khi quan sát thấy sự khác biệt giữa các khung được sử dụng bởi con người và LLMs gây ra độ nhạy cảm của LLM đối với chất lượng prompt. Trong khi đó, có thể khó khăn cho con người để đánh giá độ rõ ràng của câu hỏi của họ và để căn chỉnh các khung của họ với những khung của LLMs. Để minh họa điều này, chúng tôi trước tiên trình bày một ví dụ động lực bằng cách điều tra chi tiết một công trình gần đây (Allen-Zhu & Li, 2023).

GPT-4 Có phải Mother Teresa sinh vào một tháng chẵn?
Không, Mother Teresa sinh vào ngày 26 tháng 8 năm 1910. 
Tháng 8 là tháng thứ 8 trong năm.
Có phải Ludwig van Beethoven sinh vào một tháng chẵn?
Không, Ludwig van Beethoven sinh vào ngày 17 tháng 12 năm 1770. Mặc dù tháng 12 là tháng thứ 12 trong năm, và do đó là chẵn, nó thường được phân loại là một tháng lẻ vì số ngày của nó là 31, đó là một số lẻ.
GPT-4
Có phải Ed Sheeran sinh vào một tháng chẵn?
Không, Ed Sheeran không sinh vào một tháng chẵn. Anh ấy sinh vào ngày 17 tháng 2 năm 1991. Mặc dù tháng 2 là tháng thứ hai trong năm, nó truyền thống được coi là một tháng lẻ vì nó là tháng duy nhất có số ngày không đều.
GPT-4
Có phải Bob Marley sinh vào một tháng chẵn?
Không, Bob Marley sinh vào ngày 6 tháng 2 năm 1945. Mặc dù tháng 2 là tháng thứ 2 trong năm, đó là một số chẵn, khái niệm chẵn và lẻ áp dụng cho các số nguyên, không phải cho các tháng trong lịch hoặc ngày tháng.
GPT-4
Sự mơ hồ 1: Tháng 2 là "lẻ" vì số ngày không đều
Sự mơ hồ 2: Tháng "Chẵn/Lẻ" đề cập đến tháng có số ngày chẵn/lẻ.
Sự mơ hồ 3: Khái niệm chẵn hoặc lẻ không thể được áp dụng cho lịch.

Hình 1: Ví dụ động lực của nghiên cứu chúng tôi. Chúng ta có thể quan sát một số sự mơ hồ trong câu hỏi cho LLMs. Một LLM có thể diễn giải "tháng chẵn" là tháng có số ngày chẵn, điều này khác với ý định của con người.

Ví dụ Động lực. Allen-Zhu & Li (2023) phát hiện rằng LLMs như GPT-4 có thể không lý luận hiệu quả với kiến thức nội bộ của chúng ngay cả khi chúng có thể truy xuất thông tin một cách chính xác. Như được thể hiện trong Hình 1, khi được đặt câu hỏi, "Có phải Mother Teresa sinh vào một tháng chẵn?" GPT-4 có thể khẳng định sai lầm rằng tháng 8 là một tháng lẻ. Dựa trên quan sát này, Allen-Zhu & Li (2023) đề xuất rằng GPT-4 thay vào đó đòi hỏi một quy trình Chain-of-Thought—dựa vào các câu hỏi tiếp theo do người dùng dẫn dắt—để sửa chữa những câu trả lời sai trước đó của nó.

Tuy nhiên, chúng tôi tiến thêm một bước để điều tra lý do nội tại cho sự kém hiệu quả của LLM trong việc trả lời những câu hỏi như vậy. Như được thể hiện trong các phản hồi khác trong Hình 1, khi GPT-4 giải thích lý luận của nó, mô hình có một số sự mơ hồ đối với các câu hỏi. Ví dụ, nó có thể coi tháng 2 là lẻ do số ngày không đều của nó và đôi khi coi một tháng chẵn/lẻ là những tháng có số ngày chẵn/lẻ.

Sự mơ hồ trong câu hỏi (Cole et al., 2023; Kim et al., 2023a) là một mối quan tâm được công nhận trong các bộ dữ liệu chuẩn. Ví dụ, đã được quan sát thấy rằng các bộ dữ liệu NLI như MultiNLI (Williams et al., 2018) chứa sự mơ hồ, điều này thậm chí còn thách thức đối với những người diễn giải con người (Liu et al., 2023). Hơn nữa, nghiên cứu của chúng tôi phát hiện ra rằng các bộ dữ liệu chuẩn thường được sử dụng để đánh giá LLM (Wei et al., 2022; bench authors, 2023) sở hữu những sự mơ hồ không thể nhận thức được đối với con người nhưng thách thức đối với các mô hình ngôn ngữ. Những sự mơ hồ này khiến LLMs cung cấp những phản hồi sai lầm cho những truy vấn không mong muốn.

Câu hỏi gốc | Câu hỏi tự diễn đạt lại
Có phải {người} sinh vào một ngày chẵn?
Có phải {người} sinh vào một tháng chẵn?
Có phải {người} sinh vào một năm chẵn? | Bạn có thể cung cấp thêm thông tin về việc liệu cá nhân tên {người} có sinh vào một ngày là số chẵn không? Điều này đề cập đến các ngày như ngày 2, 4, 6, 8, và tiếp tục trong một tháng nhất định.
Bạn có thể cung cấp tháng cụ thể trong năm mà {người} sinh ra để xác định liệu nó có rơi vào một tháng có số chẵn như tháng 2, tháng 4, tháng 6, tháng 8, tháng 10, hoặc tháng 12 không?
Năm sinh của {người} là gì và nó có phải là một số chẵn không?

Hình 2: So sánh với câu hỏi gốc (Allen-Zhu & Li, 2023) và một câu hỏi tự diễn đạt lại có thể của GPT-4. Chúng ta có thể quan sát sự cải thiện đáng kể về độ chính xác trong việc trả lời những câu hỏi đã diễn đạt lại này so với truy vấn của 105 người nổi tiếng cho nhiệm vụ này.

Cách tiếp cận của chúng tôi. Trong bài báo này, chúng tôi làm nổi bật một khía cạnh thường bị bỏ qua trong các nghiên cứu về LLMs: sự khác biệt giữa khung tư duy của con người và LLM. Nghiên cứu của chúng tôi minh họa rằng sự khác biệt này có tác động đáng kể đến hiệu suất của LLMs trong thiết lập zero-shot. Để giải quyết vấn đề này, chúng tôi đề xuất để LLM diễn đạt lại câu hỏi và kết hợp thêm chi tiết để trả lời tốt hơn. Chúng tôi quan sát thấy rằng, trái ngược với những câu hỏi được con người hỏi một cách bình thường, những câu hỏi được diễn đạt lại tự động có xu hướng tăng cường độ rõ ràng về ngữ nghĩa và hỗ trợ giải quyết sự mơ hồ vốn có. Kỹ thuật tự diễn đạt lại này dẫn đến sự cải thiện đáng kể về độ chính xác, như được thể hiện trong biểu đồ cột của Hình 2.

Dựa trên những hiểu biết này, chúng tôi giới thiệu một phương pháp có tên Diễn đạt lại và Phản hồi (RaR), nhắc LLM diễn đạt lại câu hỏi đã cho và phản hồi trong một prompt duy nhất. Kết quả thực nghiệm của chúng tôi trên các nhiệm vụ lý luận đa dạng cho thấy hiệu quả của cách tiếp cận này. Ngoài ra, chúng tôi tận dụng RaR để tạo điều kiện chuyển giao các câu hỏi đã diễn đạt lại từ các LLMs có khả năng cao hơn để làm rõ sự mơ hồ cho các mô hình kém tiến bộ hơn. Cuối cùng, chúng tôi trình bày những thảo luận kỹ lưỡng với phương pháp Chain-of-Thought (CoT) (Kojima et al., 2022; Wei et al., 2022). RaR bổ sung cho CoT và có thể dễ dàng được kết hợp để cải thiện, như được xác nhận bởi kết quả thực nghiệm của chúng tôi. Không giống như các phương pháp sử dụng nhiều LLMs cho kỹ thuật prompt lặp đi lặp lại dựa trên điểm số chính xác (Zhou et al., 2022b; Pryzant et al., 2023), phương pháp của chúng tôi vừa không giám sát vừa không cần huấn luyện, làm cho nó tiết kiệm và áp dụng được cho tất cả các câu hỏi trong thiết lập zero-shot. Cuối cùng, công trình của chúng tôi kêu gọi tầm quan trọng rằng việc thiết kế các nhiệm vụ do con người tạo ra nhắm vào các khả năng LLM cụ thể nên được xem xét nghiêm ngặt bởi cả con người và LLMs để đảm bảo sự rõ ràng trong ý định.

2 Công trình liên quan

2.1 Phương pháp Prompting cho LLMs
Kể từ sự ra đời của các LLMs gần đây (OpenAI, 2023; Touvron et al., 2023; Chiang et al., 2023), một khối lượng nghiên cứu ngày càng tăng đã tập trung vào kỹ thuật prompt cho LLMs (Brown et al., 2020; Schick & Schütze, 2021; Zhou et al., 2022b; White et al., 2023; Wang et al., 2023). Những hướng dẫn thủ công đã xuất hiện để hướng dẫn người dùng thiết kế và sửa đổi prompts của họ (Reynolds & McDonell, 2021; Saravia, 2022). OpenAI (2022) cũng đã đưa ra những khuyến nghị chung để tạo ra các truy vấn, nhấn mạnh tính cụ thể, chi tiết và chính xác. Tuy nhiên, các cá nhân thường thấy khó khăn trong việc tinh chỉnh những câu hỏi của riêng họ để có độ rõ ràng hoặc bao gồm các chi tiết cần thiết cho LLMs, vì những câu hỏi đó đã đủ rõ ràng đối với chính con người. Nghiên cứu tiếp theo (Zhou et al., 2022b; Sorensen et al., 2022; Pryzant et al., 2023) đã tập trung vào việc tinh chỉnh tự động các prompts. Những phương pháp này thường sử dụng nhiều LLMs để tạo ra các prompts ứng cử viên, đánh giá và chấm điểm những prompts này, và lặp đi lặp lại tinh chỉnh chúng cho đến khi một prompt thỏa mãn được tạo ra. Việc đánh giá một prompt thường dựa vào độ chính xác của phản hồi của LLM (Zhou et al., 2022b; Pryzant et al., 2023) hoặc thông tin tương hỗ của câu hỏi (Sorensen et al., 2022). Với bản chất của tính toán lặp và sự cần thiết cho đánh giá định tính, những phương pháp như vậy được sử dụng để tinh chỉnh các mẫu prompt đơn lẻ; áp dụng chúng một cách phổ biến cho tất cả các câu hỏi sẽ tốn kém.

Phương pháp được người dùng sử dụng thường xuyên nhất và phù hợp chặt chẽ với cách tiếp cận của chúng tôi là prompting zero-shot Chain-of-Thought (CoT) (Kojima et al., 2022). Trong thiết lập few-shot mạnh hơn, few-shot CoT (Wei et al., 2022) cũng đã là một phương pháp prompting mạnh. Với việc những kỹ thuật này không đòi hỏi đánh giá và lựa chọn lặp, chúng đã trở nên phổ biến rộng rãi và truyền cảm hứng cho một loạt các nghiên cứu tiếp theo (Wang et al., 2022; Zhou et al., 2022a; Press et al., 2022; Yao et al., 2023; Zhang et al., 2023b; Shao et al., 2023). Tuy nhiên, các phương pháp CoT không phải không có những hạn chế của chúng, như được quan sát trong nghiên cứu của chúng tôi. Các điều tra gần đây cũng đã làm nổi bật những thách thức với độ tin cậy của cả zero-shot CoT (Turpin et al., 2023) và few-shot CoT (Pawelczyk et al., 2023). Gần đây nhất, Zhou et al. (2023) đề xuất Foresee and Reflect tương tự như một phương pháp prompting zero-shot nhắm vào nhiệm vụ được đề xuất Thinking for Doing (T4D). Cuối cùng, đáng chú ý rằng phương pháp của chúng tôi bổ sung cho tất cả các kỹ thuật prompting được đề cập ở trên và có thể được kết hợp.

2.2 Phương pháp Tự sửa chữa cho LLMs
Một hướng công trình khác nhằm nâng cao hiệu suất LLM (Madaan et al., 2023; Welleck et al., 2022; Kim et al., 2023b; Pan et al., 2023; Shinn et al., 2023) bằng cách tận dụng LLM để tinh chỉnh phản hồi của chính nó, một khái niệm được gọi là prompting hậu kỳ. Điều này bao gồm các thuật ngữ như "tự sửa chữa", "tự tinh chỉnh", và "tự phê bình", trong đó LLMs sửa đổi phản hồi của riêng chúng dựa trên nhiều nguồn phản hồi hoặc mô hình phê bình khác nhau. Như được phân loại bởi Pan et al. (2023), các mô hình phê bình tự động thường sử dụng phản hồi tự thân của LLM (Madaan et al., 2023; Shinn et al., 2023; Yan et al., 2023), các LLMs được huấn luyện khác (Yang et al., 2022; Lightman et al., 2023), hoặc tài liệu tham khảo bên ngoài (Jung et al., 2022; Gao et al., 2023; Yu et al., 2023b; Welleck et al., 2022). Tuy nhiên, các nghiên cứu gần đây (Huang et al., 2023; Stechly et al., 2023) xem xét khả năng tự sửa chữa của LLMs và tìm thấy những hạn chế tiềm năng, đề xuất rằng LLMs có thể không thể tự sửa chữa các quy trình lý luận của chúng. Những phát hiện của họ tiết lộ rằng tự sửa chữa không tốt hơn tự nhất quán (Wang et al., 2022). Trái ngược với việc cho phép LLM tự tinh chỉnh phản hồi của nó, phương pháp của chúng tôi để LLM thay vào đó diễn đạt lại những câu hỏi ban đầu được tạo ra bởi con người.

Công trình đồng thời. Theo hiểu biết tốt nhất của chúng tôi, công trình của chúng tôi là một trong những công trình đầu tiên xem xét sự hiểu lầm giữa con người và LLMs, cũng như khả năng của các mô hình để diễn đạt lại đầu vào như một phương tiện tự cải thiện. Đồng thời, một số nghiên cứu đã xuất hiện cũng khám phá và sử dụng khả năng diễn đạt lại của LLMs. Thay vì prompting zero-shot, Yu et al. (2023a) sử dụng LLMs để diễn đạt lại câu hỏi cho việc tạo dữ liệu tổng hợp và tinh chỉnh tiếp theo. Trong lĩnh vực các mô hình thị giác-ngôn ngữ, Prasad et al. (2023) tương tự cho thấy rằng khả năng diễn đạt lại của các mô hình ngôn ngữ có thể cải thiện hiệu quả quy trình lý luận của chính nó. Hơn nữa, Weston & Sukhbaatar (2023) giới thiệu System 2 Attention, một phương pháp để LLM tự lọc thông tin không liên quan từ prompt gốc và tái tạo truy vấn.

3 Căn chỉnh các Khung với Diễn đạt lại và Phản hồi
Để giảm thiểu một cách hệ thống sự mơ hồ của prompt và bối cảnh hóa thông tin theo cách phù hợp với khung hiện có của LLMs, chúng tôi đề xuất Diễn đạt lại và Phản hồi (RaR) như một phương tiện tự làm rõ. Trong giao tiếp giữa con người, diễn đạt lại là một kỹ thuật được biết đến thường xuyên trong đó mọi người diễn đạt lại câu hỏi của người khác như một quá trình hiểu, để đảm bảo sự rõ ràng và mạch lạc trong phản hồi. Một chiến lược giao tiếp như vậy do đó có thể được áp dụng tương tự cho một LLM, để nó tạo ra một câu hỏi đã diễn đạt lại trước và cung cấp một câu trả lời sau đó. Theo trực giác này, RaR yêu cầu LLMs Diễn đạt lại các câu hỏi đã cho và sau đó Phản hồi trong một truy vấn duy nhất. Cách tiếp cận này có thể được xem như một chiến lược để trực tiếp nâng cao chất lượng phản hồi của LLM. Chi tiết, chúng tôi giới thiệu prompt sau cho nhiệm vụ trả lời câu hỏi:

"{question }"\nDiễn đạt lại và mở rộng câu hỏi, và phản hồi. (3.1)

Thiết kế prompt của chúng tôi nhằm mục đích ngắn gọn và hiệu quả. Trong khi đó, nhiều biến thể khác của prompt vẫn nằm trong phương pháp của chúng tôi và cũng cung cấp cải thiện hiệu suất, điều này chia sẻ cùng ý định diễn đạt lại trước khi phản hồi. Những prompts như vậy bao gồm nhưng không giới hạn ở những điều sau đây,

•Diễn đạt lại và elabor về cuộc điều tra, sau đó cung cấp câu trả lời.
•Định khung lại câu hỏi với bối cảnh và chi tiết bổ sung, sau đó cung cấp câu trả lời.
•Sửa đổi câu hỏi gốc để có độ rõ ràng và chi tiết, sau đó đưa ra câu trả lời.
•Phát biểu lại và elabor về cuộc điều tra trước khi tiến hành với phản hồi.

Trong các thí nghiệm sau này, chúng tôi sẽ cho thấy rằng RaR vẫn hiệu quả đối với tất cả các biến thể của những prompts này. Với việc diễn đạt lại trước khi trả lời, RaR cung cấp một số tính chất mong muốn bao gồm (1) để LLM tự cải thiện prompts trong khi duy trì bối cảnh của truy vấn gốc, (2) căn chỉnh tốt hơn truy vấn dự định của con người với phong cách câu hỏi ưa thích của LLM, (3) mở rộng quy trình tư duy của LLM và thêm một bước sẽ không xuất hiện tự nhiên khi sử dụng CoT và (4) cung cấp một cách tiếp cận cho con người để diễn giải cách LLMs hiểu các câu hỏi.

4 RaR Cải thiện Hiệu quả Phản hồi LLM
Trong phần này, chúng tôi cung cấp một đánh giá toàn diện về khả năng áp dụng và hiệu quả của RaR. Kết quả được trình bày trong bốn chiều kích chính: (1)RaR là một prompt đơn giản và hiệu quả để cải thiện hiệu suất LLM; (2)các biến thể của prompts vẫn rơi vào phương pháp của RaR và vẫn hiệu quả một cách mạnh mẽ; (3)LLMs, trong khi tất cả đều hưởng lợi từ RaR, có trình độ khác nhau trong việc diễn đạt lại và hưởng lợi ở các mức độ khác nhau.

RaR: Diễn đạt lại và Phản hồi trong một prompt duy nhất
LLM"Lấy những chữ cái cuối cùng của các từ trong 'Edgar Bob' và nối chúng lại."
Diễn đạt lại và mở rộng câu hỏi, và phản hồi
Bạn có thể tạo một chuỗi hoặc loạt ký tự mới bằng cách kết hợp các chữ cái cuối cùng từ mỗi từ trong cụm từ "Edgar Bob"?
Những chữ cái cuối cùng trong các từ "Edgar" và "Bob" là "r" và "b", do đó khi được nối, nó tạo thành "rb".

Hình 3: Demonstrating RaR: một prompt duy nhất để cải thiện chất lượng phản hồi của LLM.

4.1 Nhiệm vụ Chuẩn
Chúng tôi đánh giá khả năng zero-shot của LLMs trên nhiều nhiệm vụ chuẩn trong các danh mục khác nhau. Như được đề xuất bởi Allen-Zhu & Li (2023), chúng tôi xem xét các nhiệm vụ sau mà ngay cả các LLMs tiên tiến như GPT-4 được phát hiện phản hồi kém với nhiều câu hỏi này¹.

•Phân loại Kiến thức (Allen-Zhu & Li, 2023). Lấy mẫu một nhóm các cá nhân có trang Wikipedia, nhiệm vụ này thách thức LLM quyết định liệu một người nổi tiếng có sinh vào một ngày, tháng, hoặc năm chẵn.

•So sánh Kiến thức (Allen-Zhu & Li, 2023). Sử dụng cùng nhóm cá nhân, nhiệm vụ này hướng dẫn LLM so sánh tuổi của hai người và quyết định ai sinh sớm hơn.

Hơn nữa, chúng tôi xem xét các bộ dữ liệu được sử dụng rộng rãi sau đây cho một đánh giá toàn diện, điều này cũng được xem xét rộng rãi trong các công trình prompting trước đây (Wei et al., 2022).

•CSQA (Talmor et al., 2019). Dữ liệu CommonSense QA bao gồm một loạt các câu hỏi đánh giá khả năng hiểu biết thường thức về thế giới và liên quan đến ngữ nghĩa phức tạp.

•Hiểu Ngày tháng (bench authors, 2023). Nguồn từ Big-bench (bench authors, 2023), nhiệm vụ Hiểu Ngày tháng nhấn mạnh lý luận thường thức và suy luận một ngày từ một bối cảnh được cung cấp. Chúng tôi xem xét một phiên bản khó hơn trong đó chúng tôi không cung cấp các lựa chọn của những câu trả lời tiềm năng và để LLM trả lời trực tiếp.

•Nối Chữ cái Cuối (Fortes, 2023). Nhiệm vụ tập trung vào lý luận tượng trưng, và yêu cầu LLM nối các chữ cái cuối cùng của một danh sách các tên được cho. Chúng tôi xem xét việc nối cho hai tên cũng như một nhiệm vụ khó hơn của việc nối cho bốn tên.

•Tung Đồng xu². Nguồn từ Hugging Face, nhiệm vụ hỏi LLM liệu đồng xu vẫn ngửa lên, với điều kiện ban đầu của nó và các hành động tiếp theo của những người đã tung hoặc không tung đồng xu. Chúng tôi thêm một "Tung có nghĩa là đảo ngược." bổ sung vào các câu hỏi.

•Thể thao (bench authors, 2023). Nguồn từ Big-bench (bench authors, 2023), nhiệm vụ Hiểu Thể thao chủ yếu hỏi liệu một câu có hợp lý hay không hợp lý, trong đó một nhân vật thể thao nổi bật được mô tả thực hiện các hành động liên quan đến thể thao cụ thể.

Chi tiết của tất cả các nhiệm vụ được đánh giá được tóm tắt trong Bảng 5 của Phụ lục A.

4.2 Kết quả Chính
Chúng tôi tiến hành thí nghiệm trên các bộ dữ liệu chuẩn đã đề cập sử dụng GPT-4³(OpenAI, 2023). Chúng tôi điều tra hiệu suất của RaR, cho phép LLM vừa diễn đạt lại vừa phản hồi câu hỏi trong một truy vấn duy nhất. Một cách tiếp cận như vậy có thể được coi như một chiến lược zero-shot đơn giản để cải thiện hiệu suất của LLM trên bất kỳ câu hỏi nào. Trong Hình 4 và chi tiết trong Bảng 1, chúng tôi so sánh độ chính xác của GPT-4 có hoặc không có RaR. Trên một khoảng đa dạng của 10 nhiệm vụ nhấn mạnh các khía cạnh khác nhau của khả năng LLM, RaR nhất quán tạo ra những cải thiện có thể phân biệt được cho GPT-4. Đáng chú ý, đối với những nhiệm vụ mà GPT-4 ban đầu thấy rất thách thức (ví dụ, nối chữ cái cuối), RaR thể hiện sự cải thiện đáng kể thậm chí đến gần 100% độ chính xác. Thật vậy, tương tự như giao tiếp con người, diễn đạt lại và elabor một câu hỏi và sau đó trả lời là một cách tiếp cận hiệu quả. Tóm tắt các kết quả, RaR cung cấp một prompt zero-shot phổ quát, cắm và chơi cho phép cải thiện hiệu suất hiệu quả và hiệu quả của LLMs trên các nhiệm vụ chung.

93.33
83.96
89.52
100
85.45
84.54
75.88
99.09
86.82
99.09
54.29
58.10
57.14
98.44
84.09
79.55
69.92
52.05
21.36
74.55

Hình 4: So sánh độ chính xác (%) của các prompts khác nhau sử dụng GPT-4. RaR cải thiện độ chính xác của GPT-4 trên 10 nhiệm vụ. Một biên độ lớn hơn chỉ ra một câu hỏi được thiết kế kém hơn. Đối với các nhiệm vụ như CSQA trong đó chất lượng câu hỏi đủ tốt, phương pháp của chúng tôi vẫn cung cấp một cải thiện biên.

Biến thể của Prompt. Những phát hiện chính của chúng tôi, như được minh họa trong Hình 4, được thu được sử dụng prompt cụ thể (3.1). Tuy nhiên, quan trọng cần lưu ý rằng các biến thể của prompt này với các cụm từ được sửa đổi vẫn phù hợp với các nguyên tắc cơ bản của phương pháp RaR. Trong Bảng 1, chúng tôi cho thấy hiệu suất của GPT-4 trên bốn biến thể của prompt RaR, như được liệt kê trong Phần 3, cùng với độ lệch hiệu suất trung bình từ kết quả của prompt chính của chúng tôi. Mặc dù việc thay đổi từ ngữ prompt gây ra những dao động nhỏ trong hiệu suất zero-shot của GPT-4, một sự nâng cao nhất quán so với kết quả cơ sở được quan sát trên tất cả các biến thể.

Bảng 1: Độ chính xác test zero-shot của GPT-4 trên các câu hỏi gốc của mỗi nhiệm vụ, sử dụng các biến thể khác nhau của prompts. Chúng tôi ký hiệu sự khác biệt trong độ chính xác zero-shot trung bình cho các biến thể prompt với prompt chính.

Ngày | Tháng | Năm | Tuổi | CSQA | Ngày tháng | Thể thao | Cuối (2) | Cuối (4) | Tung đồng xu | Trung bình
Gốc | 54.29 | 58.10 | 57.14 | 98.44 | 84.09 | 69.92 | 79.55 | 52.05 | 21.36 | 74.55 | 64.95
RaR | 93.33 | 83.96 | 89.52 | 100 | 85.45 | 75.88 | 84.54 | 99.09 | 86.82 | 99.09 | 89.77
RaR 1 | 94.29 | 82.08 | 85.71 | 100 | 87.50 | 75.34 | 85.91 | 95.89 | 78.64 | 93.18 | 87.85(−1.92)
RaR 2 | 93.33 | 88.68 | 88.57 | 100 | 85.83 | 75.61 | 88.64 | 91.32 | 84.54 | 93.18 | 88.97(−0.8)
RaR 3 | 85.71 | 83.96 | 92.38 | 100 | 83.33 | 79.13 | 85.45 | 86.76 | 80.00 | 95.00 | 87.17(−2.6)
RaR 4 | 92.14 | 87.74 | 93.33 | 100 | 87.50 | 78.64 | 86.36 | 92.24 | 75.45 | 93.64 | 88.70(−1.07)

4.3 Hiệu suất trên Các LLMs Khác nhau
Chúng tôi tiếp tục xem xét hiệu suất của RaR trên các LLMs khác nhau, bao gồm GPT-3.5 và Vicuna (Chiang et al., 2023), một mô hình được tinh chỉnh từ LLaMA-2, để điều tra liệu các kiến trúc LLM khác nhau có thể cung cấp cải thiện phản hồi nhất quán bằng cách diễn đạt lại các câu hỏi. Cụ thể, chúng tôi xem xét GPT-4-0613, GPT-3.5-turbo-0613, và Vicuna-13b-v1.5.

Chúng tôi trình bày kết quả thí nghiệm trong Hình 5. Do hiệu suất gần bằng không của Vicuna-13b-v1.5 trên Nối Chữ cái Cuối (4), chúng tôi loại trừ nhiệm vụ này khỏi đánh giá của Vicuna-13b-v1.5. Đáng chú ý, tất cả các LLMs được xem xét đều thể hiện hiệu suất nâng cao với RaR, dẫn đến sự gia tăng đáng kể về độ chính xác trên đa số các nhiệm vụ. Các mô hình tiên tiến hơn, như GPT-4, hưởng lợi từ những cải thiện đáng kể nhất trên tất cả các nhiệm vụ, trong khi các mô hình ít phức tạp hơn, như Vicuna, đạt được những cải thiện khiêm tốn sử dụng cách tiếp cận của chúng tôi. Trên một số nhiệm vụ như Thể thao, GPT-3.5 và Vicuna thậm chí thể hiện hiệu suất hơi giảm. Trong Phụ lục B.1, chúng tôi cung cấp các ví dụ cụ thể của các câu hỏi đã diễn đạt lại trên các mô hình khác nhau để minh họa sự khác biệt hiệu suất. Chúng tôi kết thúc thí nghiệm này với hiểu biết quan trọng sau: tất cả các mô hình có thể hưởng lợi từ việc diễn đạt lại câu hỏi, với các mô hình tiên tiến hơn dự kiến đạt được một cải thiện lớn hơn.

GPT-4-0613 | GPT-3.5-turbo-0613 | Vicuna-13b-v1.5

Hình 5: Độ chính xác (%) của GPT-4-0613, GPT-3.5-turbo-0613 và Vicuna-13b khi kiểm tra trên các câu hỏi gốc và tự diễn đạt lại sử dụng Two-step RaR. GPT-4 có được cải thiện lớn hơn trên các nhiệm vụ khác nhau so với GPT-3.5 và Vicuna.

5 Để LLMs Mạnh hơn Diễn đạt lại cho LLMs Yếu hơn Phản hồi
Trong khi các LLMs khác nhau sở hữu khả năng diễn đạt lại khác nhau và hưởng lợi ở các mức độ khác nhau từ RaR, chúng tôi tiếp tục đặt câu hỏi rằng: liệu LLMs mạnh hơn có thể được tận dụng để giúp LLMs yếu hơn trong việc làm rõ các prompts? Để tiếp tục sử dụng sự cải thiện chất lượng của các câu hỏi được diễn đạt lại bởi các mô hình lớn hơn, như GPT-4, chúng tôi giới thiệu một biến thể của RaR gọi là Two-step RaR tách bước diễn đạt lại khỏi bước phản hồi trong RaR. Trực quan, ngay cả giữa con người, một câu hỏi chi tiết và chính xác hơn sẽ thu được những phản hồi chính xác và quyết đoán hơn. Two-step RaR theo trực giác này bằng cách thiết kế một quy trình hai bước để cải thiện chất lượng của các câu hỏi: trong bước đầu tiên, với một câu hỏi truy vấn, chúng tôi tạo một truy vấn tự diễn đạt lại câu hỏi đã diễn đạt lại bằng cách nhắc một LLM diễn đạt lại với prompt sau:

"{question }"
Với câu hỏi trên, diễn đạt lại và mở rộng nó để giúp bạn trả lời tốt hơn. Duy trì tất cả thông tin trong câu hỏi gốc.

Sau đó câu hỏi gốc và câu hỏi đã diễn đạt lại được kết hợp để nhắc một LLM phản hồi với prompt sau:

(gốc) {question }
(đã diễn đạt lại) {rephrased question }
Sử dụng câu trả lời của bạn cho câu hỏi đã diễn đạt lại để trả lời câu hỏi gốc.

Đáng chú ý, LLM diễn đạt lại và LLM phản hồi có thể là cùng một mô hình hoặc các mô hình khác nhau. Như chúng tôi thể hiện trong các thí nghiệm, các LLMs khác nhau thể hiện trình độ khác biệt trong việc diễn đạt lại câu hỏi. Ở đây, chúng tôi xem xét liệu các câu hỏi đã diễn đạt lại được tạo ra bởi một LLM mạnh hơn (GPT-4) có thể hưởng lợi cho một LLM yếu hơn (Vicuna). Chúng tôi chi tiết hiệu suất của Vicuna-13b-v1.5 trên các câu hỏi được diễn đạt lại bởi GPT-4, so với các câu hỏi tự diễn đạt lại của chính nó trong Bảng 2. Nhất quán với kỳ vọng của chúng tôi rằng GPT-4 có thể căn chỉnh tốt hơn với ý định con người và làm rõ câu hỏi, chúng tôi quan sát thấy rằng các câu hỏi đã diễn đạt lại của nó cải thiện đáng kể hiệu suất của Vicuna-13b-v1.5 trên một số nhiệm vụ, đặc biệt khi các câu hỏi tự diễn đạt lại của Vicuna thể hiện chất lượng thấp. Thật vậy, các câu hỏi có thể được làm rõ thêm cho Vicuna, nhưng cần có thêm khám phá về khả năng của nó đối với các câu hỏi tự diễn đạt lại.

Bảng 2: So sánh các câu hỏi đã diễn đạt lại của GPT-4 với các câu hỏi tự diễn đạt lại của Vicuna. Chúng tôi bỏ nhiệm vụ Năm Chẵn vì các câu hỏi đã diễn đạt lại bởi Vicuna đủ rõ ràng như những câu của GPT-4.

Ngày chẵn | Tháng chẵn | CSQA | Thể thao | Ngày tháng | Chữ cái cuối (2)
Vicuna-đã diễn đạt lại | 58.10 | 56.19 | 51.36 | 65.00 | 32.79 | 5.45
GPT-4-đã diễn đạt lại | 61.90 | 60.95 | 55.00 | 73.64 | 37.67 | 10.45

6 Thảo luận với Chain-of-Thought

6.1 Thảo luận với Zero-shot CoT
Được biết rộng rãi rằng zero-shot CoT, bằng cách thêm hướng dẫn "Hãy nghĩ từng bước một." vào các truy vấn, có thể cải thiện hiệu quả hiệu suất của LLMs trên các nhiệm vụ lý luận. Tuy nhiên, chúng tôi làm nổi bật một số ví dụ trong đó zero-shot CoT không thể cung cấp cải thiện, đôi khi thậm chí dẫn đến hiệu suất giảm. Ngược lại, RaR nhất quán thể hiện hiệu quả. Cuối cùng, chúng tôi lưu ý rằng phương pháp của chúng tôi bổ sung cho zero-shot CoT và có thể được kết hợp cùng nhau bằng cách đơn giản thêm "hãy nghĩ từng bước một" vào (3.1).

Nhiệm vụ. Chúng tôi xem xét nhiệm vụ Thành ngữ Trung Quốc như được giới thiệu trong Allen-Zhu & Li (2023), cụ thể là nhiệm vụ khó nhất của việc suy luận chữ cái đầu tiên. Nhiệm vụ này liên quan đến việc lấy những thành ngữ bốn ký tự Trung Quốc được công nhận rộng rãi và che một ký tự ở một vị trí để LLM suy luận đúng ký tự bị che. Hơn nữa, chúng tôi cũng sử dụng nhiệm vụ StereoSet (Nadeem et al., 2021), đánh giá các thiên kiến rập khuôn có mặt trong LLMs liên quan đến giới tính, chủng tộc, nghề nghiệp và tôn giáo. Từ dữ liệu inter-sentence, chúng tôi lấy mẫu 220 ví dụ, mỗi ví dụ bao gồm một câu bối cảnh và ba lựa chọn: một rập khuôn, một chống rập khuôn, và một không liên quan. Chúng tôi áp dụng định dạng prompt được sử dụng bởi Shaikh et al. (2022). Chúng tôi hoãn giải thích chi tiết về đánh giá đến Phụ lục A.

Kết quả. Như được minh họa trong Bảng 3, mặc dù RaR nâng cao hiệu suất của LLM, việc suy luận chính xác ký tự đầu tiên của nhiệm vụ Thành ngữ Trung Quốc vẫn là một thách thức. Người ta có thể hỏi: liệu zero-shot CoT có cung cấp cải thiện nhất quán cho LLM trong những nhiệm vụ như vậy như nó làm trên các nhiệm vụ lý luận khác? Phát hiện của chúng tôi là, thực tế, zero-shot CoT có thể dẫn đến hiệu suất tệ hơn (31.43%) cho những nhiệm vụ khó như vậy, vì LLM có xu hướng ảo giác trong các bước trung gian—một hiện tượng tương tự như việc ảo giác tăng dần (Zhang et al., 2023a). Hơn nữa, như Shaikh et al. (2022) phát hiện trên các mô hình ngôn ngữ khác, zero-shot CoT có thể dẫn đến lý luận không mong muốn hướng tới thiên kiến và độc tính. Cũng trong Bảng 3, chúng tôi thể hiện hiệu suất của GPT-4 trên StereoSet. Chúng ta có thể quan sát thấy rằng, trong khi zero-shot CoT không cải thiện được Language Modeling Score, các câu hỏi đã diễn đạt lại cải thiện nó đáng kể đến 97.73%. Điều này ngụ ý rằng, với RaR, LLM hiếm khi chọn các lựa chọn không liên quan. Hơn nữa, trong khi zero-shot CoT cải thiện tỷ lệ phần trăm của các phản hồi công bằng (chọn không ai trong số họ), RaR đạt được hiệu suất tốt nhất.

Bảng 3: Hiệu suất của GPT-4 sử dụng câu hỏi gốc, prompt zero-shot CoT và RaR. Zero-shot CoT dẫn đến độ chính xác thậm chí tệ hơn của Thành ngữ Trung Quốc do ảo giác trong các bước lý luận. Trên StereoSet, các đánh giá bao gồm Language Modeling Score và Fair Score, trong đó RaR thể hiện hiệu suất tốt nhất.

Bộ dữ liệu | Đánh giá | Gốc | Zero-shot CoT | RaR
Thành ngữ Trung Quốc | Độ chính xác (%) | 32.38 | 31.43 | 35.24
StereoSet | Language Modeling Score (%) | 84.09 | 84.09 | 97.73
StereoSet | Fair Score (%) | 6.82 | 35.00 | 42.27

6.2 Cải thiện trên Few-Shot CoT
Few-shot CoT (Wei et al., 2022), mặc dù khác với thiết lập zero-shot mà chúng tôi xem xét, đã là kỹ thuật CoT hiệu quả nhất cho việc sử dụng các ví dụ few-shot. Thay vì căn chỉnh câu hỏi với những gì LLM nhận tốt nhất, few-shot CoT hướng dẫn LLM lý luận sử dụng logic con người được cung cấp. Tuy nhiên, một mối quan tâm xuất hiện: LLMs phản hồi như thế nào khi các ví dụ do con người tạo ra có lỗi hoặc chứa sai lầm? Như được chứng thực bởi một nghiên cứu song song gần đây (Pawelczyk et al., 2023), chúng tôi tương tự quan sát thấy rằng LLMs có thể bị ảnh hưởng bất lợi bởi các ví dụ few-shot xấu.

Nhiệm vụ. Chúng tôi xem lại nhiệm vụ Nối Chữ cái Cuối và tham khảo các ví dụ few-shot được cung cấp trong Wei et al. (2022). Như được thể hiện trong Hình 6, các ví dụ theo một logic cụ thể: lấy chữ cái cuối cùng của từ thứ nhất và thứ hai, và nối những chữ cái này, dẫn đến câu trả lời. Những ví dụ few-shot như vậy đã được chứng minh là nâng cao hiệu quả nhất hiệu suất của một mô hình ngôn ngữ, đạt được độ chính xác 100% khi nối các chữ cái cuối cùng của hai từ. Ngược lại, chúng tôi khám phá một ví dụ sử dụng logic sau: lấy chữ cái đầu tiên của từ thứ nhất và thứ hai, và nối những chữ cái này, cung cấp câu trả lời cho việc nối chữ cái cuối. Mục tiêu của chúng tôi là điều tra cách prompt few-shot thay thế này, mặc dù mang một logic tương tự như prompt gốc và câu trả lời đúng, ảnh hưởng đến hiệu suất của GPT-4.

Kết quả. Như được minh họa trong Hình 6, GPT-4 có xu hướng bám vào logic của prompt đã sửa đổi của chúng tôi, dẫn đến một câu trả lời không chính xác. Nó nối chính xác tất cả các chữ cái đầu tiên, nhưng kết luận với một câu trả lời cuối cùng có vẻ tùy ý. Trong Bảng 4, chúng tôi thể hiện kết quả của one-shot và four-shot CoT sử dụng những ví dụ như vậy. Chúng tôi quan sát thấy rằng hiệu suất của one-shot CoT rõ ràng giảm với chỉ một ví dụ có lỗi. Khi số lượng của những ví dụ có lỗi này tăng, hiệu suất của GPT-4 trong thiết lập 4-shot cho việc nối chữ cái cuối của bốn từ giảm xuống chỉ 52.27%. Quan sát này tiết lộ một cạm bẫy tiềm năng trong việc sử dụng few-shot CoT: với việc những ví dụ này được người dùng tạo ra, chất lượng của chúng trở nên quan trọng. Trong khi đó, chúng tôi phát hiện rằng RaR cho phép GPT-4 sửa chữa bất kỳ cạm bẫy nào trong logic của các ví dụ đã cho.

GPT-4 Q: Lấy những chữ cái cuối cùng của các từ trong "Elon Musk" và nối chúng lại.
A: Chữ cái đầu tiên của "Elon" là "E". Chữ cái đầu tiên của "Musk" là "M". Nối chúng lại là "EM". Câu trả lời là nk.
Q: Lấy những chữ cái cuối cùng của các từ trong "Annette Erin Marisol Esperanza" và nối chúng lại.
Chữ cái đầu tiên của "Annette" là "A". Chữ cái đầu tiên của "Erin" là "E". Chữ cái đầu tiên của "Marisol" là "M". Chữ cái đầu tiên của "Esperanza" là "E". Nối chúng lại là "AEME". Câu trả lời là etaz.
GPT-4

Q: Lấy những chữ cái cuối cùng của các từ trong "Elon Musk" và nối chúng lại.
A: Chữ cái cuối cùng của "Elon" là "n". Chữ cái cuối cùng của "Musk" là "k". Nối chúng lại là "nk". Câu trả lời là nk.
Q: Lấy những chữ cái cuối cùng của các từ trong "Annette Erin Marisol Esperanza" và nối chúng lại.
Chữ cái cuối cùng của "Annette" là "e". Chữ cái cuối cùng của "Erin" là "n". Chữ cái cuối cùng của "Marisol" là "l". Chữ cái cuối cùng của "Esperanza" là "a". Nối chúng lại là "enla". Câu trả lời là enla.

"Q: Lấy những chữ cái cuối cùng của các từ trong "Elon Musk" và nối chúng lại.
A: Chữ cái đầu tiên của "Elon" là "E". Chữ cái đầu tiên của "Musk" là "M". Nối chúng lại là "EM". Câu trả lời là nk."
Với câu hỏi trên, diễn đạt lại và mở rộng nó để giúp bạn trả lời tốt hơn. Duy trì tất cả thông tin trong câu hỏi gốc.
"Q: Xác định những chữ cái cuối cùng của mỗi từ trong tên "Elon Musk", sau đó ghép những chữ cái đó lại với nhau.
A: Chữ cái cuối cùng của 'Elon' là 'n'. Chữ cái cuối cùng của 'Musk' là 'k'. Khi bạn ghép những chữ cái này lại với nhau, kết quả là 'nk'"

Hình 6: Few-shot CoT có thể hướng dẫn hiệu quả LLM lý luận theo cách tương tự như các ví dụ được cung cấp. Một ví dụ QA được tạo ra kém, như được thể hiện bằng màu đỏ, dẫn đến LLM theo logic được cung cấp nhưng đạt đến một câu trả lời tùy ý.

Bảng 4: Độ chính xác (%) của GPT-4 trên nhiệm vụ Nối Chữ cái Cuối. Chúng tôi sử dụng các ví dụ one-shot và four-shot từ (Wei et al., 2022), chỉ thay đổi các bước trung gian thành "chữ cái đầu tiên" như được thể hiện trong ví dụ của chúng tôi.

Chữ cái cuối (2) | Chữ cái cuối (4)
One-shot CoT | 89.04 | 78.18
RaR+One-shot CoT | 100 | 93.64
Four-shot CoT | 87.21 | 52.27
RaR+Four-shot CoT | 100 | 95.45

7 Kết luận
Trong bài báo này, chúng tôi đã điều tra những hiểu lầm hiện có xảy ra giữa con người và LLMs và chứng minh rằng những câu hỏi có vẻ rõ ràng đối với con người vẫn có thể bị diễn giải sai bởi LLMs. Dựa trên hiểu biết này, chúng tôi giới thiệu Diễn đạt lại và Phản hồi (RaR), một cách tiếp cận mới nhắc một LLM trước tiên diễn đạt lại và làm rõ câu hỏi trước khi trả lời nó. Các đánh giá thực nghiệm của chúng tôi, được tiến hành trên một loạt các bộ dữ liệu chuẩn, xác nhận hiệu quả của các phương pháp được đề xuất của chúng tôi. Phân tích sâu hơn tiết lộ rằng trong khi tất cả các mô hình đạt được hiệu suất nâng cao thông qua việc diễn đạt lại câu hỏi, các mô hình tinh vi hơn thể hiện những cải thiện đáng kể hơn. Dựa trên quan sát này, chúng tôi trình bày Two-step RaR, một biến thể của RaR sử dụng một LLM diễn đạt lại để tinh chỉnh câu hỏi cho việc sử dụng tiếp theo bởi bất kỳ LLM phản hồi nào. Quan trọng, chúng tôi đã phát hiện rằng sự nâng cao trong chất lượng câu hỏi đạt được thông qua diễn đạt lại có thể chuyển giao được trên các mô hình. Ngoài những phát hiện này, chúng tôi đã thực hiện so sánh với các phương pháp CoT thông qua cả công thức toán học và điều tra thực nghiệm. Chúng tôi cũng chứng minh rằng RaR bổ sung cho CoT, và có thể được tận dụng để đạt được những cải thiện hiệu suất bổ sung.

Tài liệu tham khảo
Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge manipulation, 2023.

BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj.

Aras Bozkurt. Generative artificial intelligence (ai) powered conversational educational agents: The inevitable paradigm shift. Asian Journal of Distance Education, 18(1), 2023.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.

Jeremy R Cole, Michael JQ Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions. arXiv preprint arXiv:2305.14613, 2023.

James N Druckman. The implications of framing effects for citizen competence. Political behavior, 23:225–256, 2001.

Armando Fortes. simple-dataset-generation, February 2023. URL https://github.com/atfortes/simple-dataset-generation.

Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477–16508, 2023.

Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023.

Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1266–1279, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.82. URL https://aclanthology.org/2022.emnlp-main.82.

Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models. arXiv preprint arXiv:2310.14696, 2023a.

Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023b.

Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199–22213, 2022.

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step, 2023.

Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A Smith, and Yejin Choi. We're afraid language models aren't modeling ambiguity. arXiv preprint arXiv:2304.14399, 2023.

Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2023.

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.

Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 5356–5371, 2021.

OpenAI. Best practices for prompt engineering with openai api. https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api, 2022.

OpenAI. Gpt-4 technical report, 2023.

Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv preprint arXiv:2308.03188, 2023.

Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language models as few shot unlearners, 2023.

Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models. arXiv preprint arXiv:2201.11227, 2022.

Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Rephrase, augment, reason: Visual grounding of questions for vision-language models. arXiv preprint arXiv:2310.05861, 2023.

Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.

Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with" gradient descent" and beam search. arXiv preprint arXiv:2305.03495, 2023.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, CHI EA '21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380959. doi: 10.1145/3411763.3451760. URL https://doi.org/10.1145/3411763.3451760.

Elvis Saravia. Prompt Engineering Guide. https://github.com/dair-ai/Prompt-Engineering-Guide, 12 2022.

Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 255–269, 2021.

Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. On second thought, let's not think step by step! bias and toxicity in zero-shot reasoning. arXiv preprint arXiv:2212.08061, 2022.

Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. arXiv preprint arXiv:2302.00618, 2023.

Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.

Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers, Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. An information-theoretic approach to prompt engineering without ground truth labels. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 819–862, 2022.

Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems, 2023.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4149–4158, 2019.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting, 2023.

Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai, Qiushi Yang, Yanqing Kang, Jinru Wu, Huawen Hu, et al. Prompt engineering for healthcare: Methodologies and applications. arXiv preprint arXiv:2304.14670, 2023.

Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824–24837, 2022.

Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct. arXiv preprint arXiv:2211.00053, 2022.

Jason Weston and Sainbayar Sukhbaatar. System 2 attention (is something you might need too). arXiv preprint arXiv:2311.11829, 2023.

Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382, 2023.

Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://aclanthology.org/N18-1101.

Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I Wang, Wen-tau Yih, and Ziyu Yao. Learning to simulate natural language feedback for interactive semantic parsing. arXiv preprint arXiv:2305.08195, 2023.

Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 4393–4479, 2022.

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.

Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023a.

Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving language models via plug-and-play retrieval feedback. arXiv preprint arXiv:2305.14002, 2023b.

Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023a.

Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923, 2023b.

Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations, 2022a.

Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R. McKee, Ari Holtzman, Jay Pujara, Xiang Ren, Swaroop Mishra, Aida Nematzadeh, Shyam Upadhyay, and Manaal Faruqui. How far are large language models from agents with theory-of-mind?, 2023.

Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022b.

A Chi tiết Thí nghiệm
Chúng tôi sử dụng toàn bộ bộ dữ liệu cho Dates Understanding, và ngẫu nhiên rút các tập con có kích thước 220 cho các nhiệm vụ còn lại. Chúng tôi sử dụng độ chính xác để đánh giá hiệu suất của LLM. Độ chính xác trước tiên được ước tính sử dụng khớp chính xác trên các từ được tạo ra bởi LLM. Cụ thể, một câu trả lời được coi là đúng nếu nó chứa từ chính xác của phản hồi đúng và không có bất kỳ phản hồi sai nào. Chúng tôi sau đó xác minh và sửa chữa các tính toán thông qua kiểm tra thủ công. Đối với một số nhiệm vụ, để hạn chế định dạng phản hồi (ví dụ, nhiều lựa chọn), chúng tôi thêm một prompt nhất quán khi đánh giá câu hỏi gốc và RaR, chẳng hạn như "Chọn câu trả lời thích hợp nhất duy nhất". Chi tiết của các prompts được trình bày trong Bảng 6.

Các thí nghiệm của chúng tôi được thực hiện sử dụng API GPT-4 có sẵn công khai, cũng như phiên bản lịch sử của GPT-4-0613 và GPT-3.5-turbo-0613. Chúng tôi cũng đang xem xét một mô hình LLM mã nguồn mở, Vicuna-13B-v1.5.

Bảng 5: Chi tiết của tất cả các nhiệm vụ được xem xét trong các thí nghiệm của chúng tôi, bao gồm tên bộ dữ liệu, danh mục, số lượng câu hỏi, và một ví dụ câu hỏi.

Bộ dữ liệu | Danh mục | Số lượng | Ví dụ câu hỏi
Ngày chẵn | Phân loại Kiến thức | 105 | Có phải Donald Trump sinh vào một ngày chẵn?
Tháng chẵn | Phân loại Kiến thức | 105 | Có phải Joe Biden sinh vào một tháng chẵn?
Năm chẵn | Phân loại Kiến thức | 105 | Có phải Elon Musk sinh vào một năm chẵn?
So sánh tuổi | So sánh Kiến thức | 104 | Có phải Joe Biden sinh sớm hơn Donald Trump?
CSQA | Lý luận Thường thức | 220 | Mọi người đều có một số loại sáng tạo, đó là cách tốt nhất để làm gì? Lựa chọn: A. Tạo nghệ thuật B. Viết C. May D. Tạo ý tưởng E. Thể hiện bản thân
Ngày tháng | Lý luận Thường thức | 369 | Đây là ngày cuối cùng của năm 1899. Ngày mai là ngày nào theo định dạng MM/DD/YYYY?
Chữ cái cuối (2) | Lý luận Tượng trưng | 220 | Lấy những chữ cái cuối cùng của các từ trong "Kristen Andrea" và nối chúng lại.
Chữ cái cuối (4) | Lý luận Tượng trưng | 220 | Lấy những chữ cái cuối cùng của các từ trong "Mia Mariana Marcos Xavier" và nối chúng lại.
Tung đồng xu | Lý luận Tượng trưng | 220 | Một đồng xu đang ngửa lên. Roxas không tung đồng xu. Schneiderman không tung đồng xu. Đồng xu vẫn ngửa lên không? Tung có nghĩa là đảo ngược.
Thể thao | Lý luận Thường thức | 220 | Câu sau có hợp lý không? "Juan Soto bắn từ ngoài vòng cung"

Bảng 6: Prompts định dạng được thêm vào cả câu hỏi gốc và prompts RaR.

Nhiệm vụ | Prompt định dạng
Ngày tháng | Định dạng câu trả lời cuối cùng nên là MM/DD/YYYY.
CSQA | Chọn câu trả lời *duy nhất* hợp lý nhất.
StereoSet | Chọn câu trả lời *duy nhất* hợp lý nhất.
Thể thao | Trả lời câu hỏi Có hoặc Không.
Tung đồng xu | Trả lời câu hỏi Có hoặc Không.

Đánh giá. Đối với nhiệm vụ Thành ngữ Trung Quốc, chúng tôi đánh giá độ chính xác zero-shot của phản hồi GPT-4, với ước tính độ chính xác tự động và kiểm tra thủ công thêm. Đối với StereoSet, như được đề xuất bởi Nadeem et al. (2021), hai chỉ số đánh giá quan trọng nên được xem xét: Language Modeling Score, đánh giá liệu LLM có chọn các tùy chọn liên quan thay vì không liên quan, và Stereotype Score, định lượng tỷ lệ phần trăm dữ liệu mà một mô hình ưa thích các lựa chọn rập khuôn thay vì chống rập khuôn. Như được xác định bởi các tác giả, một mô hình lý tưởng sẽ không thể hiện thiên kiến nào đối với các liên kết rập khuôn hoặc chống rập khuôn, tạo ra điểm tối ưu là 0.5 cho Stereotype Score. Trong việc kiểm tra đầu ra của GPT-4, chúng tôi quan sát khả năng của nó để thực sự xác định rằng không ai trong hai tùy chọn liên quan có thể được kết luận chỉ từ câu bối cảnh. Do đó, chúng tôi phân loại những đầu ra như vậy là các phản hồi công bằng và giới thiệu Fair Score, được xác định bởi tỷ lệ của những phản hồi này, bổ sung cho Language Modeling Score. Chúng tôi cung cấp một ví dụ của phản hồi như vậy dưới đây.

Câu hỏi không cung cấp đủ thông tin để xác định độ chính xác của tùy chọn A và C. Tùy chọn B không liên quan đến bối cảnh đã cho. Do đó, không có tùy chọn nào được cung cấp có thể được chọn làm câu trả lời hợp lý nhất dựa trên thông tin có sẵn.

Bảng 7: Ví dụ few-shot được cải thiện bởi RaR được sử dụng trong Phần 6.2 cho Nối Chữ cái Cuối.

Q: Lấy những chữ cái cuối cùng của các từ trong "Elon Musk" và nối chúng lại.
A: Chữ cái đầu tiên của "Elon" là "E". Chữ cái đầu tiên của "Musk" là "M". Nối chúng lại là "EM". Câu trả lời là nk.

Q: Lấy những chữ cái cuối cùng của các từ trong "Larry Page" và nối chúng lại.
A: Chữ cái đầu tiên của "Larry" là "L". Chữ cái đầu tiên của "Page" là "P". Nối chúng lại là "LP". Câu trả lời là ye.

Q: Lấy những chữ cái cuối cùng của các từ trong "Sergey Brin" và nối chúng lại.
A: Chữ cái đầu tiên của "Sergey" là "S". Chữ cái đầu tiên của "Brin" là "B". Nối chúng lại là "SB". Câu trả lời là yn.

Q: Lấy những chữ cái cuối cùng của các từ trong "Bill Gates" và nối chúng lại.
A: Chữ cái đầu tiên của "Bill" là "B". Chữ cái đầu tiên của "Gates" là "G". Nối chúng lại là "BG". Câu trả lời là ls.

Bảng 8: Ví dụ few-shot được cải thiện bởi RaR được sử dụng trong Phần 6.2 cho Nối Chữ cái Cuối.

Q: Xác định những chữ cái cuối cùng của mỗi từ trong tên "Elon Musk", sau đó ghép những chữ cái đó lại với nhau.
A: Chữ cái cuối cùng của 'Elon' là 'n'. Chữ cái cuối cùng của 'Musk' là 'k'. Khi bạn ghép những chữ cái này lại với nhau, kết quả là 'nk'

Q: Xác định những chữ cái cuối cùng của mỗi từ trong tên "Larry Page", sau đó ghép những chữ cái đó lại với nhau.
A: Chữ cái cuối cùng của 'Larry' là 'y'. Chữ cái cuối cùng của 'Page' là 'e'. Khi bạn ghép những chữ cái này lại với nhau, kết quả là 'ye'

Q: Xác định những chữ cái cuối cùng của mỗi từ trong tên "Sergey Brin", sau đó ghép những chữ cái đó lại với nhau.
A: Chữ cái cuối cùng của 'Sergey' là 'y'. Chữ cái cuối cùng của 'Brin' là 'n'. Khi bạn ghép những chữ cái này lại với nhau, kết quả là 'yn'

Q: Xác định những chữ cái cuối cùng của mỗi từ trong tên "Bill Gates", sau đó ghép những chữ cái đó lại với nhau.
A: Chữ cái cuối cùng của 'Bill' là 'l'. Chữ cái cuối cùng của 'Gates' là 's'. Khi bạn ghép những chữ cái này lại với nhau, kết quả là 'ls'

B Kết quả Thí nghiệm Bổ sung

[Tiếp tục với các bảng và hình vẽ như trong văn bản gốc...]

B.1 Diễn đạt lại từ Các LLMs Khác nhau

Các quan sát ban đầu cho thấy rằng các câu hỏi đã diễn đạt lại của Vicuna-13b-v1.5 hiếm khi cung cấp sự làm rõ đáng kể, thường phản ánh sự đơn giản của các câu hỏi gốc của chúng. Trong trường hợp cuối cùng của Bảng 9, Vicuna-13b-v1.5 làm rối loạn ý định của câu hỏi bằng cách thay đổi "hôm qua" thành "hôm nay". Trong khi cả GPT-3.5 và GPT-4 có thể làm rõ câu hỏi, GPT-3.5 đôi khi giới thiệu chi tiết thêm hoặc hiểu sai. Như được thể hiện trong ví dụ thứ hai của Bảng 9, GPT-3.5 hiểu sai khái niệm tháng chẵn là "một tháng có số ngày chẵn". Tương tự, trong ví dụ thứ ba, GPT-3.5 giới thiệu một ràng buộc sai của trò chơi "gần đây". GPT-4, ngược lại, có thể làm rõ gần gũi với ý định con người. Chúng tôi cũng quan sát thấy rằng GPT-3.5 có xu hướng giới thiệu cụm từ sau vào các câu hỏi đã diễn đạt lại trong Thể thao (45 trên 220) và Ngày tháng (21 trên 369): "Vui lòng diễn đạt lại và cung cấp chi tiết bổ sung nếu cần thiết để nâng cao độ chính xác phản hồi của bạn.", dẫn đến một câu trả lời chỉ với một câu hỏi đã diễn đạt lại khác chứ không phải câu trả lời thực tế. Do đó, chúng tôi loại bỏ tất cả các câu chứa "diễn đạt lại" cho GPT-3.5 trên hai bộ dữ liệu này.

[Tiếp tục với phần còn lại của tài liệu...]
