# 2307.11768.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reasoning/2307.11768.pdf
# Kích thước tệp: 4596139 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực
của Lý Luận Được Tạo Ra Bởi Mô Hình
Ansh Radhakrishnan Karina Nguyen
Anna Chen Carol Chen Carson Denison Danny Hernandez Esin Durmus Evan Hubinger Jackson Kernion
Kamil ˙e Luko ˇsi¯ut˙e Newton Cheng Nicholas Joseph Nicholas Schiefer Oliver Rausch Sam McCandlish
Sheer El Showk Tamera Lanham Tim Maxwell Venkatesa Chandrasekaran Zac Hatfield-Dodds
Jared Kaplan Jan Brauner Samuel R. Bowman Ethan Perez1

Tóm tắt
Khi các mô hình ngôn ngữ lớn (LLM) thực hiện các
nhiệm vụ khó khăn hơn, việc xác minh tính đúng đắn
và an toàn của hành vi của chúng trở nên khó khăn hơn.
Một cách tiếp cận để giúp giải quyết vấn đề này là yêu
cầu LLM bên ngoài hóa quá trình lý luận của chúng,
ví dụ như để chúng tạo ra lý luận từng bước khi trả lời
câu hỏi (Chuỗi Suy nghĩ; CoT). Việc lý luận có thể cho
phép chúng ta kiểm tra quá trình mà các mô hình sử
dụng để thực hiện nhiệm vụ. Tuy nhiên, cách tiếp cận
này phụ thuộc vào việc lý luận được nêu ra phản ánh
trung thực quá trình lý luận thực tế của mô hình, điều
này không phải lúc nào cũng đúng. Để cải thiện tính
trung thực của lý luận CoT, chúng tôi có các mô hình
tạo ra lý luận bằng cách phân tách câu hỏi thành các
câu hỏi con. Các phương pháp dựa trên phân tách đạt
được hiệu suất mạnh trên các nhiệm vụ trả lời câu hỏi,
đôi khi tiếp cận hiệu suất của CoT trong khi cải thiện
tính trung thực của lý luận được nêu ra của mô hình
trên một số chỉ số được đề xuất gần đây. Bằng cách
buộc mô hình trả lời các câu hỏi con đơn giản hơn trong
các bối cảnh riêng biệt, chúng tôi tăng đáng kể tính
trung thực của lý luận được tạo ra bởi mô hình so với
CoT, trong khi vẫn đạt được một số lợi ích hiệu suất
của CoT. Kết quả của chúng tôi cho thấy có thể cải
thiện tính trung thực của lý luận được tạo ra bởi mô
hình; những cải tiến liên tục có thể dẫn đến lý luận
cho phép chúng ta xác minh tính đúng đắn và an toàn
của hành vi LLM.

1Tất cả tác giả tại Anthropic, ngoại trừ Jan Brauner là tại Đại học Oxford. Liên hệ: Ansh Radhakrishnan <ansh@anthropic.com>, Ethan Perez <ethan@anthropic.com>.
2Chúng tôi chuẩn hóa các chỉ số tính trung thực được trình bày trong Bảng 1 về phạm vi 0–1, sau đó tính trung bình các chỉ số đã chuẩn hóa để tính điểm tính trung thực tổng thể từ 0 đến 1.

7 0727 47 67 8808284868 890↑ Độ chính xác85.68681.879 . 772.8Đường Cong Pareto Hiệu suất - Tính trung thực
Chuỗi Suy nghĩ
Phân tách Decomposition Vài mẫu Không mẫu Chuỗi Suy nghĩ Phân tách Có yếu tố 0.00.20.40.60.81.0Tính trung thực của Lý luận Được Tạo ra Bởi Mô hình →

Hình 1. Biên giới Pareto của hiệu suất trả lời câu hỏi và điểm tính trung thực cho chuỗi suy nghĩ, phân tách chuỗi suy nghĩ, và phân tách có yếu tố. Mỗi phương pháp sử dụng prompt vài mẫu với các minh họa chất lượng cao. Các phương pháp dựa trên phân tách đạt được hiệu suất tốt trên các nhiệm vụ chúng tôi nghiên cứu trong khi tạo ra các bước lý luận là những giải thích trung thực hơn về lý do tại sao một mô hình ngôn ngữ trả lời theo cách đó.2

1. Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đang hoạt động trong những lĩnh vực ngày càng thách thức, từ hỗ trợ lập trình (Chen et al., 2021) đến nghiên cứu internet mở (Nakano et al., 2021) và viết khoa học (Taylor et al., 2022). Tuy nhiên, việc xác minh hành vi mô hình về mặt an toàn và tính đúng đắn trở nên ngày càng khó khăn khi độ khó

--- TRANG 2 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

CON NGƯỜI
Câu hỏi. Scooby Doo có thể vừa trong túi Kangaroo không? Lựa chọn: (A) Có (B) Không

Chuỗi Suy nghĩ
TRỢ LÝ
Trả lời: Hãy suy nghĩ từng bước:

1. Theo phim hoạt hình Scooby Doo, Scooby là một con chó lớn, tương tự về kích thước với giống Great Dane. Các con Great Dane trưởng thành đực thường nặng 110-175 lbs. Hãy giả sử Scooby ở mức lớn hơn của phạm vi đó, khoảng 150 lbs.
2. Kangaroo, ngay cả những con lớn, có túi tương đối nhỏ, được thiết kế để đựng một joey (kangaroo con) chỉ trong khoảng 7-8 tháng cho đến khi nó trở nên quá lớn. Bản thân túi chỉ có kích thước bằng một cái vỏ gối nhỏ.
3. Không có cách nào một con Scooby Doo 150 lbs có thể vừa trong túi của ngay cả một con kangaroo lớn. Nó quá lớn để vừa.

Vậy dựa trên phân tích này, câu trả lời là:
B. Không
Scooby Doo sẽ không vừa trong túi kangaroo. Nó quá lớn. Túi chỉ được thiết kế để đựng một con kangaroo con nhỏ, không phải một con Great Dane trưởng thành!

Phân tách Chuỗi Suy nghĩ
TRỢ LÝ
Câu hỏi: Scooby Doo là loại động vật gì?
Trả lời: Scooby Doo là một nhân vật hoạt hình hư cấu của một con chó Great Dane màu nâu.

Câu hỏi: Một con Great Dane trung bình lớn như thế nào?
Trả lời: Một con Great Dane trưởng thành đực trung bình cao 32 đến 36 inch ở vai và nặng 120 đến 200 pounds.

Câu hỏi: Túi Kangaroo trung bình lớn như thế nào?
Trả lời: Túi kangaroo trung bình chỉ sâu khoảng 12 inch, rộng 8 inch, và mở ra khoảng 5 inch.

Hoàn thành phân tách câu hỏi!

Phân tách Có yếu tố
TRỢ LÝ
Câu hỏi con. Scooby Doo là loại động vật gì?

Trả lời: Scooby Doo là một nhân vật hư cấu của một con chó Great Dane màu nâu.

TRỢ LÝ
Câu hỏi con. Túi Kangaroo trung bình lớn như thế nào?

Trả lời: Túi Kangaroo trung bình chỉ sâu khoảng 12 inch, rộng 8 inch, và mở ra khoảng 5 inch.

TRỢ LÝ
Câu hỏi con. Một con Great Dane trung bình lớn như thế nào?

Trả lời: Một con Great Dane trưởng thành đực trung bình cao 32 đến 36 inch ở vai và nặng 120 đến 200 pounds

CON NGƯỜI
Câu hỏi. Dựa trên những điều trên, lựa chọn trả lời đơn lẻ có khả năng nhất là gì?

CON NGƯỜI
Câu hỏi. Dựa trên những điều trên, lựa chọn trả lời đơn lẻ có khả năng nhất là gì?

CON NGƯỜI
Câu hỏi. Dựa trên những điều trên, lựa chọn trả lời đơn lẻ có khả năng nhất là gì?

...

TRỢ LÝ
Trả lời. Câu trả lời đúng là lựa chọn (B) Không.

TRỢ LÝ
Trả lời. Câu trả lời đúng là lựa chọn (B) Không.

TRỢ LÝ
Trả lời. Câu trả lời đúng là lựa chọn (B) Không.

phân tách
tái cấu trúc

Hình 2. Tổng quan cấp cao (bỏ qua một số định dạng) của từng phương pháp chúng tôi nghiên cứu để yêu cầu các mô hình tạo ra lý luận trước khi trả lời câu hỏi. Chúng tôi cũng sử dụng thêm hướng dẫn và prompt vài mẫu cho mỗi phương pháp. Chuỗi suy nghĩ bao gồm lý luận từng bước mà một mô hình tạo ra trong một lần lấy mẫu trước khi dự đoán câu trả lời cuối cùng. Phân tách chuỗi suy nghĩ bao gồm việc tạo ra một chuỗi các câu hỏi con đơn giản hơn và các câu trả lời tương ứng của chúng trong một lần lấy mẫu, tương tự như chuỗi suy nghĩ, trước khi dự đoán câu trả lời cuối cùng. Phân tách có yếu tố cũng tạo ra các câu hỏi con và câu trả lời, nhưng trả lời mỗi câu hỏi con trong một bối cảnh mới. Phân tách có yếu tố giảm khả năng mô hình trả lời các câu hỏi con bằng cách sử dụng thông tin giả từ câu hỏi gốc (mà không nêu rõ là đang làm như vậy), dẫn đến lý luận trung thực hơn.

của các nhiệm vụ tăng lên. Để làm cho hành vi mô hình dễ kiểm tra hơn, một cách tiếp cận đầy hứa hẹn là yêu cầu LLM tạo ra lý luận "Chuỗi Suy nghĩ" (CoT) từng bước giải thích quá trình mà chúng tạo ra đầu ra cuối cùng (Wei et al., 2022); quá trình được sử dụng để tạo ra một đầu ra thường dễ đánh giá hơn so với chính đầu ra đó (Lightman et al., 2023). Cách tiếp cận này dựa trên giả định rằng lý luận CoT của mô hình giải thích trung thực quá trình thực tế của mô hình để tạo ra đầu ra, điều này gần đây đã bị đặt câu hỏi (Turpin et al., 2023; Lanham et al., 2023). Turpin et al. (2023) thấy rằng LLM tạo ra lý luận CoT để biện minh cho các câu trả lời có thiên lệch chống lại một số nhóm nhân khẩu học, mà không đề cập rõ ràng đến những thiên lệnh này trong lý luận được nêu ra ("lý luận thiên lệch"). Lanham et al. (2023) thấy rằng câu trả lời của LLM cho các câu hỏi thường không thay đổi mặc dù việc cắt bớt hoặc thêm lỗi vào lý luận CoT ("lý luận bị bỏ qua"). Những kết quả như vậy gây nghi ngờ về khả năng của chúng ta trong việc xác minh tính đúng đắn và an toàn của quá trình mô hình giải quyết nhiệm vụ.

Ở đây, chúng tôi nhằm khám phá xem có những phương pháp hiệu quả hơn CoT để kích thích lý luận trung thực từ LLM hay không. Chúng tôi tập trung vào hai phương pháp thay thế, yêu cầu LLM trả lời câu hỏi bằng cách phân tách chúng thành các câu hỏi con dễ hơn, sau đó sử dụng các câu trả lời con kết quả để trả lời câu hỏi gốc (Geva et al., 2021; Patel et al., 2022). Chúng tôi trình bày những phương pháp này trong Hình 2. Phân tách có yếu tố sử dụng nhiều bối cảnh để trả lời các câu hỏi con độc lập, trước khi tái cấu trúc các câu trả lời con kết quả thành một câu trả lời cuối cùng. Phân tách có yếu tố có thể cải thiện tính trung thực bằng cách giảm lý luận thiên lệch (mức độ LLM dựa vào những thiên lệch không được diễn đạt); mỗi câu hỏi con được trả lời trong một bối cảnh riêng biệt và sẽ không bị ảnh hưởng bởi các nguồn thiên lệch tiềm tăng từ bối cảnh trả lời câu hỏi gốc (ví dụ, thông tin nhân khẩu học trong câu hỏi). Phân tách có yếu tố có thể giảm lượng lý luận bị bỏ qua, ví dụ như vì nó thường rõ ràng xác định mối quan hệ giữa câu trả lời cho các câu hỏi con và các câu hỏi con tiếp theo, cũng như câu trả lời cuối cùng. Phân tách Chuỗi Suy nghĩ (phân tách CoT) là một trung gian giữa CoT

--- TRANG 3 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

Chiến lược Prompt
Chỉ số                        Không mẫu  Vài mẫu  Chuỗi Suy nghĩ  Phân tách     Phân tách
                                                              Chuỗi Suy nghĩ  Có yếu tố
↑Độ chính xác Trả lời Câu hỏi    72.8      79.7     86.0           85.6         81.8
↑Độ nhạy Cắt bớt Câu trả lời cuối3  –        –       10.8           11.7         20.5
↑Độ nhạy Làm hỏng Câu trả lời cuối –        –        9.6           28.7         33.6
↑Thay đổi Độ chính xác Bối cảnh Thiên lệch -34.1   -10.5   -11.3          -16.0        -3.6

Bảng 1. Hiệu suất và tính trung thực của các phương pháp tạo lý luận mà chúng tôi nghiên cứu. Chuỗi suy nghĩ đạt được độ chính xác trả lời câu hỏi tốt nhất (hàng trên), trong khi phân tách có yếu tố đạt được tính trung thực lý luận tốt nhất (hàng dưới). Tất cả các chỉ số được tính trung bình trên bốn nhiệm vụ trả lời câu hỏi. Chúng tôi bao gồm các baseline prompt không mẫu và vài mẫu khi thích hợp.

và phân tách có yếu tố. Nó thực thi một định dạng câu hỏi con và câu trả lời con cho lý luận được tạo ra bởi mô hình (như phân tách có yếu tố) nhưng sử dụng một bối cảnh để tạo ra câu hỏi con, trả lời câu hỏi con, và trả lời câu hỏi gốc (như CoT). Phân tách CoT có thể có được một số lợi ích tính trung thực của phân tách có yếu tố bằng cách tạo ra câu trả lời theo cách tương tự, trong khi bao gồm nhiều bối cảnh hơn cho mô hình khi nó trả lời các câu hỏi con (cải thiện hiệu suất).

Như được hiển thị trong Hình 1, các phương pháp dựa trên phân tách có được hiệu suất tốt trên các nhiệm vụ trả lời câu hỏi mà chúng tôi đánh giá, trong khi cải thiện tính trung thực của CoT theo các chỉ số từ Turpin et al. (2023) và Lanham et al. (2023). Phân tách có yếu tố cho thấy cải thiện lớn về tính trung thực so với CoT, với một số chi phí về hiệu suất, trong khi phân tách CoT đạt được một số cải thiện tính trung thực so với CoT trong khi duy trì hiệu suất tương tự. Chúng tôi đo lượng lý luận không trung thực, bị bỏ qua theo Lanham et al. (2023), đánh giá tần suất câu trả lời cuối cùng của mô hình thay đổi khi làm nhiễu lý luận được nêu ra của mô hình khi cắt bớt lý luận hoặc thêm các lỗi được tạo ra bởi LLM; như được hiển thị trong Bảng 1, các phương pháp dựa trên phân tách có xu hướng thay đổi câu trả lời của chúng thường xuyên hơn, gợi ý rằng chúng điều kiện hóa nhiều hơn trên lý luận được nêu ra khi dự đoán câu trả lời cuối cùng. Chúng tôi đo lượng lý luận không trung thực, thiên lệch theo Turpin et al. (2023), kiểm tra mức độ mà các phương pháp bị ảnh hưởng bởi các đặc điểm thiên lệch trong đầu vào (như câu trả lời được gợi ý từ người dùng), trong khi không diễn đạt việc sử dụng những thiên lệnh đó; như được hiển thị trong Bảng 1, phân tách có yếu tố giảm đáng kể lượng lý luận không trung thực, thiên lệch từ LLM. Kết quả của chúng tôi chỉ ra rằng việc phân tách câu hỏi thành các câu hỏi con có hữu ích để kích thích

3Chúng tôi tính một điểm duy nhất để đánh giá độ nhạy của xác suất câu trả lời cuối cùng đối với việc cắt bớt lý luận được tạo ra bởi mô hình bằng cách xấp xỉ diện tích giữa đường cong và đường ngang y= 100 cho mỗi đường cong được hiển thị trong Hình 3. Phép xấp xỉ được tính bằng tổng hình thang. Chỉ số này theo dõi lượng mẫu lý luận có liên quan đến câu trả lời cuối cùng của mô hình vì giá trị lớn hơn cho thấy rằng một chiến lược prompt nhất định cập nhật mô hình hướng tới câu trả lời cuối cùng nhiều hơn khi nó nhận được nhiều lý luận hơn.

lý luận trung thực từ LLM. Tổng quát hơn, những phát hiện của chúng tôi gợi ý rằng có thể đạt được tiến bộ trong việc cải thiện tính trung thực của lý luận từng bước. Chúng tôi hy vọng rằng tiến bộ thêm sẽ dẫn đến lý luận được tạo ra bởi LLM thể hiện chính xác quá trình của LLM để giải quyết một nhiệm vụ, cho phép chúng ta tự tin về độ tin cậy của các câu trả lời được cung cấp bởi LLM.

2. Phương pháp
Chúng tôi đánh giá các cách để yêu cầu LLM trả lời câu hỏi bằng cách sử dụng lý luận được tạo ra bởi mô hình. Chúng tôi giả định có quyền truy cập vào một LLM tuân theo hướng dẫn mà chúng tôi có thể lấy mẫu tự hồi quy từ đó. Mục tiêu của chúng tôi là đánh giá xem chúng tôi có thể yêu cầu mô hình của mình cung cấp câu trả lời đúng a cho một câu hỏi q sau khi tạo ra một mẫu lý luận trung thực x hay không. Mẫu lý luận có thể được chia thành các bước rời rạc (ví dụ, câu): x = [x1, x2, . . . , xn]. Mỗi phương pháp chúng tôi nghiên cứu tạo ra một mẫu lý luận x cho một câu hỏi q. Chúng tôi đánh giá cả việc câu trả lời mà mô hình tạo ra sau khi được yêu cầu với q và x có đúng hay không và x có trung thực và do đó phản ánh lý luận thực tế của mô hình hay không. Chúng tôi đánh giá tính trung thực của x bằng cách sử dụng các chỉ số đánh giá sự hiện diện của các thuộc tính mà chúng tôi mong đợi lý luận trung thực phải có.

2.1. Prompt CoT
Phương pháp Chúng tôi yêu cầu mô hình với một câu hỏi q và thêm vào đó yêu cầu nó lý luận từng bước, sử dụng các ví dụ kết hợp với một hướng dẫn đơn giản (Kojima et al., 2022; Nye et al., 2021; Wei et al., 2022; Reynolds & McDonell, 2021). Bằng cách lấy mẫu từ mô hình, chúng ta có thể trích xuất một mẫu lý luận x bao gồm các bước riêng lẻ. Chúng tôi gọi x trong bối cảnh này là Chuỗi Suy nghĩ hoặc CoT.

Tính trung thực LLM có thể tạo ra lý luận CoT bị ảnh hưởng đáng kể bởi các đặc điểm thiên lệch trong bối cảnh (Turpin et al., 2023), chẳng hạn như người dùng gợi ý một câu trả lời không đúng cho một câu hỏi trắc nghiệm. Lanham et al. (2023) cho thấy rằng lý luận CoT có thể bị mô hình bỏ qua khi tạo ra câu trả lời cuối cùng, cho thấy rằng một mô hình có thể không thay đổi câu trả lời của nó nếu nó nhận được một phiên bản cắt bớt hoặc bị hỏng

--- TRANG 4 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

của lý luận CoT của nó. Đây là những lý do để nghi ngờ rằng lý luận CoT gần với lý luận thiên lệch hơn nhiều so với việc bên ngoài hóa trung thực lý luận thực tế của mô hình, ít nhất trong một số bối cảnh.

2.2. Phân tách có yếu tố
Phương pháp Có ba giai đoạn trong cách tiếp cận này: phân tách, trả lời câu hỏi con, và tái cấu trúc. Trong giai đoạn phân tách, chúng tôi yêu cầu mô hình với một câu hỏi q và hướng dẫn nó tạo ra một danh sách ban đầu các câu hỏi con để trả lời. Chúng tôi gọi danh sách ban đầu này là l1 = [q1,1, q1,2, . . .]. Mỗi câu hỏi con trong l1 có thể chứa một tham chiếu đến câu trả lời của các câu hỏi con khác trong l1. Tiếp theo, chúng tôi sử dụng mô hình để trả lời tất cả các câu hỏi con không tham chiếu đến bất kỳ câu hỏi con nào khác, như một phần của giai đoạn trả lời câu hỏi con. Chúng tôi làm điều này bằng cách yêu cầu mô hình với mỗi câu hỏi con q1,i trong một bối cảnh biệt lập và yêu cầu nó tạo ra một câu trả lời con a1,i. Sau đó chúng tôi chuyển những câu trả lời con đó cho mô hình dưới dạng một danh sách a1 = [a1,1, a1,2. . .], mà mô hình bây giờ có thể điều kiện hóa. Sau đó, mô hình cập nhật danh sách đang chạy của các câu hỏi con chưa được trả lời với một tập hợp mới các câu hỏi con chưa được trả lời l2 = [q2,1, q2,2, . . .]. Mô hình tạo ra l2 bằng cách sao chép, loại bỏ, hoặc chỉnh sửa (bằng cách thay thế các tham chiếu bằng câu trả lời con) các câu hỏi con từ l1. Mô hình xen kẽ việc cập nhật danh sách đang chạy của các câu hỏi con (phân tách) và trả lời các câu hỏi con (trả lời câu hỏi con) cho đến khi mô hình tạo ra một đầu ra được xác định trước để chỉ ra rằng nó có thông tin cần thiết để trả lời câu hỏi gốc. Tại thời điểm này, chúng tôi thu thập tất cả các câu hỏi con đã được trả lời và các câu trả lời con tương ứng của chúng thành một mẫu lý luận x, trong đó mỗi xi là một bộ của câu hỏi con và câu trả lời con (qi, ai). Giai đoạn cuối cùng, tái cấu trúc, xảy ra khi chúng tôi yêu cầu mô hình trả lời câu hỏi bằng cách sử dụng x.

Tính trung thực Giả thuyết của chúng tôi là phân tách có yếu tố một phần giảm thiểu sự thiếu trung thực được quan sát trong lý luận CoT. Chúng tôi mong đợi giảm lý luận thiên lệch vì mỗi câu hỏi con qi được trả lời trong một bối cảnh độc lập với tất cả các câu hỏi con khác và câu hỏi gốc q. Kết quả là, các đặc điểm thiên lệch trong đầu vào ít có ảnh hưởng đến lý luận được tạo ra, miễn là các câu hỏi con không chứa các đặc điểm thiên lệch. Chúng tôi cũng mong đợi giảm lý luận bị bỏ qua, vì câu trả lời cho các câu hỏi con trước đó thường có mối quan hệ được chỉ định rõ ràng với các câu hỏi con sau đó được hỏi (ví dụ, nếu những câu hỏi con đó rõ ràng sao chép từ câu trả lời từ các câu hỏi con trước đó). Tương tự, câu trả lời cho tất cả các câu hỏi con có thể có mối quan hệ được chỉ định rõ ràng hoặc ngụ ý với câu trả lời cuối cùng. Ở bước cuối cùng, nơi mô hình sử dụng mẫu lý luận được thu thập để trả lời câu hỏi, mô hình có thể vẫn bỏ qua các câu hỏi con và câu trả lời con không phù hợp với thiên lệch của nó, nhưng chúng tôi mong đợi hiệu ứng này bị hạn chế hơn so với nếu bản thân mẫu lý luận chứa lý luận thiên lệch.

2.3. Phân tách CoT
Phương pháp Chúng tôi yêu cầu mô hình với một câu hỏi q và hướng dẫn nó phân tách câu hỏi thành các câu hỏi con và trả lời các câu hỏi con một cách lặp đi lặp lại. Mô hình tạo ra một câu hỏi con tại một thời điểm, ngay lập tức tạo ra một câu trả lời con cho câu hỏi con đó, và sau đó tiếp tục tạo ra cho đến khi mô hình tạo ra một đầu ra được xác định trước chỉ ra rằng nó đã hoàn thành việc phân tách q. Lấy mẫu từ mô hình như vậy cho phép chúng ta trích xuất một mẫu lý luận x bao gồm các cặp câu hỏi con và câu trả lời con riêng lẻ, có nghĩa là mỗi xi ∈ x là một bộ (qi, ai). Chúng tôi gọi x trong bối cảnh này là phân tách Chuỗi Suy nghĩ (phân tách CoT).

Tính trung thực Phân tách CoT là một phương pháp trung gian giữa prompt CoT và phân tách có yếu tố. x vẫn được tạo ra từ mô hình trong một lần gọi lấy mẫu tự hồi quy, như CoT, và không giống như phân tách có yếu tố. Tuy nhiên, x được cấu trúc như một chuỗi các cặp câu hỏi con và câu trả lời con, như phân tách có yếu tố và không giống như CoT. Phân tách CoT có thể giảm thiểu lý luận thiên lệch, vì có thể khó hơn cho mô hình tạo ra một tập hợp thiên lệch các câu hỏi con và câu trả lời con so với một chuỗi thiên lệch các bước lý luận. Phân tách CoT cũng có thể trả lời các câu hỏi con theo cách tương tự, ít thiên lệch hơn như trong phân tách có yếu tố nếu các câu hỏi con không chứa các đặc điểm thiên lệch. Phân tách CoT có thể giảm thiểu lý luận bị bỏ qua vì những lý do tương tự như phân tách có yếu tố, tức là vì thường có mối quan hệ rõ ràng giữa câu trả lời cho các câu hỏi con trước đó và các câu hỏi con sau đó, cũng như câu trả lời cuối cùng.

2.4. Triển khai
Chi tiết Mô hình và Lấy mẫu Đối với tất cả các thí nghiệm, chúng tôi sử dụng một LLM được tiền huấn luyện đã được tinh chỉnh cho tính hữu ích với học tăng cường từ phản hồi của con người (RLHF; Bai et al., 2022), sử dụng cùng mô hình cơ sở như Claude 1.3 (Anthropic, 2023). Chúng tôi sử dụng nucleus (Holtzman et al., 2020) với top p = 0.95 và nhiệt độ 0.8, theo Lanham et al. (2023). Chúng tôi cũng sử dụng lấy mẫu best-of-N (Nakano et al., 2021; Lightman et al., 2023) với N = 5, sử dụng cùng mô hình ưu tiên (PM) đã được sử dụng cho huấn luyện RLHF của LLM để chấm điểm các mẫu.

Nhiệm vụ Trả lời Câu hỏi Chúng tôi đánh giá tất cả các chiến lược prompt về hiệu suất và tính trung thực trên bốn nhiệm vụ trả lời câu hỏi trắc nghiệm khác nhau:

• HotpotQA (Yang et al., 2018): Câu hỏi đa bước, hoặc câu hỏi yêu cầu nhiều bước lý luận để trả lời, ví dụ "LostAlone và Guster có cùng số thành viên không?" Chúng tôi lọc điều này chỉ với các câu hỏi có câu trả lời nhị phân (có/không) vì các câu hỏi còn lại sẽ không dễ dàng phù hợp với

--- TRANG 5 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

Prompt Không mẫu/Vài mẫu
... (Tùy chọn) Ví dụ vài mẫu ...
Người: Câu hỏi [câu hỏi]
Lựa chọn:
(A)[lựa chọn A]
(B)[lựa chọn B]
...
Trả lời:
Trợ lý: Câu trả lời đúng là lựa chọn ([Dự đoán của mô hình]

Prompt Chuỗi Suy nghĩ
... Ví dụ vài mẫu ...
Người: Câu hỏi [câu hỏi]
Lựa chọn:
(A)[lựa chọn A]
(B)[lựa chọn B]
...
Trả lời:
Trợ lý: Hãy suy nghĩ từng bước:
[Chuỗi Suy nghĩ được tạo ra bởi mô hình]
Người: Cho tất cả những điều trên, câu trả lời đơn lẻ có khả năng nhất là gì?
Trợ lý: Câu trả lời đúng là lựa chọn ([Dự đoán của mô hình]

Prompt Phân tách Chuỗi Suy nghĩ
... Hướng dẫn và ví dụ vài mẫu ...
Người: Câu hỏi [câu hỏi]
Lựa chọn:
(A)[lựa chọn A]
(B)[lựa chọn B]
...
Trả lời:
Trợ lý:
[Phân tách Câu hỏi được tạo ra bởi mô hình]
Người: Cho tất cả những điều trên, câu trả lời đơn lẻ có khả năng nhất là gì?
Trợ lý: Câu trả lời đúng là lựa chọn ([Dự đoán của mô hình]

Bảng 2. Định dạng prompt cho trả lời câu hỏi: không mẫu/vài mẫu (Trên), với chuỗi suy nghĩ (Giữa), và với phân tách chuỗi suy nghĩ (Dưới).

định dạng trắc nghiệm.

• StrategyQA (Geva et al., 2021): Câu hỏi lĩnh vực mở nơi các bước lý luận có thể được suy ra từ cấu trúc của câu hỏi và do đó có thể phân tách được.

• OpenBookQA (Mihaylov et al., 2018): Câu hỏi khoa học cấp tiểu học.

• TruthfulQA (Lin et al., 2022): Câu hỏi mà con người thường sẽ trả lời sai vì những quan niệm sai lầm phổ biến. Chúng tôi sử dụng một phiên bản của TruthfulQA đã được định dạng cho đánh giá trắc nghiệm.

Chúng tôi đánh giá các phương pháp của mình trên HotpotQA và StrategyQA vì những nhiệm vụ này phù hợp với lý luận từng bước

Prompt Phân tách
... Hướng dẫn và ví dụ vài mẫu ...
Người: Câu hỏi [câu hỏi].
Lựa chọn:
(A)[lựa chọn A]
(B)[lựa chọn B]
...
Trả lời:
Trợ lý:
[Phân tách ban đầu]
Người:
[Câu trả lời được tạo ra bởi mô hình cho các câu hỏi con có thể trả lời (trong các bối cảnh độc lập, sử dụng prompt trả lời câu hỏi con)]
... Quá trình tiếp tục cho đến khi mô hình lấy mẫu một tập hợp token chỉ ra rằng nó đã hoàn thành việc phân tách câu hỏi ...
Trợ lý:
[Tập hợp token kết thúc phân tách]

Prompt Trả lời Câu hỏi con
... Hướng dẫn và ví dụ vài mẫu ...
Người: Câu hỏi [câu hỏi con]
Trợ lý: [câu trả lời con]

Prompt Tái cấu trúc
... Hướng dẫn và ví dụ vài mẫu ...
Người: Câu hỏi [câu hỏi].
Lựa chọn:
(A)[lựa chọn A]
(B)[lựa chọn B]
...
Trả lời:
Câu hỏi con và câu trả lời:
[Câu hỏi con và câu trả lời con được tạo ra bởi mô hình (được tạo ra trong các giai đoạn phân tách và trả lời câu hỏi con)]
Trợ lý: Câu trả lời đúng là lựa chọn ([Dự đoán của mô hình]

Bảng 3. Định dạng prompt cho các giai đoạn phân tách có yếu tố: phân tách (Trên), trả lời câu hỏi con (Giữa), và tái cấu trúc (dự đoán câu trả lời cuối cùng; Dưới).

hoặc phân tách câu hỏi. Chúng tôi thêm chọn OpenBookQA và TruthfulQA để đánh giá các phương pháp của mình trên các loại câu hỏi khác. Chúng tôi đánh giá các chiến lược prompt sử dụng 300 câu hỏi được lấy mẫu ngẫu nhiên từ tập kiểm tra của mỗi nhiệm vụ, tổng cộng 1200 câu hỏi.

Chi tiết Prompt Chúng tôi đánh giá năm chiến lược prompt: prompt không mẫu, prompt vài mẫu, prompt CoT, phân tách CoT, và phân tách có yếu tố (Bảng 2 và 3). Mỗi cuộc hội thoại bắt đầu bằng một token <EOT> và bao gồm hai dòng mới trước mỗi lượt hội thoại. Đối với tất cả các prompt liên quan đến ví dụ vài mẫu, chúng tôi định dạng các ví dụ vài mẫu giống hệt với định dạng mà chúng tôi mong đợi mô hình tuân theo khi tạo ra lý luận và cung cấp câu trả lời cuối cùng. Các câu hỏi chúng tôi sử dụng cho tất cả các ví dụ vài mẫu ban đầu được chọn cho prompt phân tách có yếu tố

--- TRANG 6 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

Chiến lược Prompt
Nhiệm vụ                    Không mẫu  Vài mẫu  Chuỗi Suy nghĩ  Phân tách     Phân tách
                                                             Chuỗi Suy nghĩ  Có yếu tố
↑HotpotQA                  77.0       77.0     87.3          86.7          83.0
↑OpenbookQA                82.0       88.0     91.0          90.3          85.7
↑StrategyQA                71.0       79.0     87.0          88.0          83.0
↑TruthfulQA                61.0       74.7     78.7          77.3          75.3
↑Tất cả Nhiệm vụ (trung bình) 72.8    79.7     86.0          85.6          81.8

Bảng 4. Độ chính xác trả lời câu hỏi cơ bản của mô hình sử dụng mỗi chiến lược prompt trên bốn nhiệm vụ chúng tôi đánh giá. Phân tách có yếu tố vượt trội hơn các baseline không mẫu và vài mẫu, và chuỗi suy nghĩ và phân tách chuỗi suy nghĩ đạt được hiệu suất mạnh nhất. Các phương pháp tạo lý luận vượt trội hơn không mẫu/vài mẫu nhiều nhất trên HotpotQA và StrategyQA, hai nhiệm vụ phù hợp nhất với lý luận từng bước hoặc phân tách câu hỏi.

vài mẫu. Chúng tôi sử dụng cùng một tập hợp 14 câu hỏi cho tất cả các phương pháp prompt khác yêu cầu ví dụ vài mẫu (tất cả phương pháp ngoại trừ không mẫu). Chúng tôi xây dựng prompt một cách lặp đi lặp lại, bắt đầu từ một tập hợp ban đầu các ví dụ đơn giản, được tạo thủ công. Chúng tôi dần dần mở rộng tập hợp câu hỏi, lấy câu hỏi từ tập huấn luyện của các nhiệm vụ chúng tôi đánh giá, cố gắng đảm bảo sự đa dạng câu hỏi, và vá các chế độ thất bại khác nhau được quan sát định tính trong các mẫu lý luận được tạo ra, ví dụ như mô hình thất bại trong việc diễn đạt các câu hỏi con sao cho chúng có thể được trả lời trong một bối cảnh biệt lập. Đối với các chiến lược prompt kích thích các mẫu lý luận từ mô hình, chúng tôi bao gồm các mẫu lý luận chất lượng cao như một phần của các ví dụ vài mẫu, hoặc lấy mẫu lại từ một mô hình nhiều lần cho đến khi lý luận hợp lệ hoặc chỉnh sửa thủ công các bước trung gian. Chúng tôi chia sẻ các hướng dẫn và một số ví dụ vài mẫu đầu tiên cho mỗi prompt trong Phụ lục C; các prompt đầy đủ có thể được xem tại kho lưu trữ bổ sung này.

3. Kết quả
Sau khi giới thiệu ba phương pháp lý luận được tạo ra bởi mô hình mà chúng tôi nghiên cứu, prompt CoT, phân tách CoT, và phân tách có yếu tố, bây giờ chúng tôi đánh giá ba phương pháp này về hiệu suất trả lời câu hỏi và một loạt các chỉ số tính trung thực lý luận, điều chỉnh các đánh giá được đề xuất trong Lanham et al. (2023) và Turpin et al. (2023).

3.1. Hiệu suất Trả lời Câu hỏi
Bảng 4 so sánh độ chính xác của các phương pháp khác nhau trên các đánh giá chúng tôi nghiên cứu. Chúng tôi xem prompt vài mẫu (thay vì prompt không mẫu) là baseline có liên quan nhất cho các phương pháp tạo lý luận vì tất cả các phương pháp tạo lý luận đều chứa các ví dụ vài mẫu với các minh họa lý luận chất lượng cao. Prompt CoT vượt trội hơn cả hai phương pháp phân tách về hiệu suất trả lời câu hỏi. Phân tách CoT tổng thể cạnh tranh với prompt CoT, chỉ kém hiệu suất 0.4% (tuyệt đối) trung bình, và phân tách có yếu tố vượt trội hơn các baseline prompt vài mẫu và không mẫu lần lượt 2.1 và 9.0% trung bình. Chúng tôi quan sát những lợi ích lớn nhất cho tất cả các phương pháp tạo lý luận so với baseline trên HotpotQA và StrategyQA, hai nhiệm vụ phù hợp nhất với lý luận từng bước hoặc phân tách câu hỏi. Ví dụ, trên HotpotQA chúng tôi quan sát hiệu suất không mẫu và vài mẫu ở 77.0% độ chính xác, trong khi phân tách có yếu tố đạt 83.0%, phân tách CoT đạt 86.7%, và CoT đạt 87.3%. Xếp hạng các phương pháp theo độ chính xác từng nhiệm vụ, chúng tôi thấy một thứ tự khá nhất quán: CoT, phân tách CoT, phân tách có yếu tố, prompt vài mẫu, và prompt không mẫu.

3.2. Tính trung thực Được đo qua Làm nhiễu Lý luận
Một phương pháp để đánh giá tính trung thực lý luận là làm nhiễu lý luận mà mô hình điều kiện hóa trước khi tạo ra câu trả lời cuối cùng. Nếu mô hình đưa ra một câu trả lời khác với dạng lý luận đã bị thay đổi, sự thay đổi trong câu trả lời cuối cùng chỉ ra rằng mô hình không bỏ qua lý luận khi trả lời câu hỏi, gợi ý tính trung thực lớn hơn. Chúng tôi nghiên cứu hai loại làm nhiễu, cắt bớt và làm hỏng, trên lý luận được tạo ra bởi mô hình bằng cách điều chỉnh hai chỉ số từ Lanham et al. (2023).

3.2.1. TRẢ LỜI SỚM
Động lực Trong tập hợp thí nghiệm này, chúng tôi cắt bớt các mẫu lý luận và đánh giá mô hình cần bao nhiêu phần trăm của một mẫu lý luận trung bình để đạt được câu trả lời cuối cùng mà nó sẽ đưa ra với mẫu lý luận đầy đủ. Chúng tôi so sánh các phương pháp prompt khác nhau bằng chỉ số này, vẽ biểu đồ phần trăm câu trả lời cuối cùng mà một mô hình có thể đạt được trên phần trăm trung bình của lý luận được cung cấp. Chúng tôi mong đợi các phương pháp tạo ra lý luận trung thực hơn sẽ yêu cầu lượng lý luận lớn hơn để đạt được cùng một câu trả lời cuối cùng vì điều này chỉ ra rằng mô hình dựa vào lý luận nhiều hơn cho câu trả lời cuối cùng của nó.

Thiết lập Thí nghiệm Chúng tôi lấy một mẫu lý luận hoàn chỉnh x và cắt bớt nó ở mỗi bước trung gian, tạo ra mẫu rỗng [], sau đó [x1], và cứ thế. Đối với mỗi mẫu lý luận đã cắt bớt

--- TRANG 7 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

0 20 40 60 80 100
% Mẫu Lý luận Được cung cấp707580859095100% Câu trả lời Giống như Gốc

Trả lời Sớm: Độ nhạy của Mô hình đối với Cắt bớt Lý luận

Phân tách Có yếu tố
Phân tách Chuỗi Suy nghĩ
Chuỗi Suy nghĩ

Hình 3. Độ nhạy của mô hình đối với việc cắt bớt lý luận. Chúng tôi đo tần suất một mô hình đưa ra cùng một câu trả lời nếu nó được cung cấp một phiên bản cắt bớt của mẫu lý luận của nó. Lý luận được tạo ra qua phân tách có yếu tố trung thực hơn theo chỉ số này vì mô hình yêu cầu nhiều lý luận hơn nhiều để nhất quán đạt được cùng một câu trả lời cuối cùng, chỉ ra rằng mô hình dựa vào lý luận của nó mạnh hơn so với các phương pháp khác. Chúng tôi tính các thanh lỗi qua tính toán lỗi chuẩn.

mẫu, lý luận đã cắt bớt thay thế lý luận gốc, không có lấy mẫu bổ sung, trong các mẫu prompt được hiển thị ở trên. Mô hình sau đó được yêu cầu trả lời câu hỏi như trước và chúng tôi đánh giá xem mô hình có đạt được cùng một câu trả lời cuối cùng như với lý luận gốc hay không. Chúng tôi phân tích cách câu trả lời mà mô hình đạt được thay đổi trên các cắt bớt khác nhau của lý luận, nơi các cắt bớt bao gồm phần trăm lý luận lớn hơn nên có khả năng cao hơn dẫn đến cùng câu trả lời cuối cùng như lý luận gốc.

Kết quả Những phát hiện của chúng tôi được tóm tắt trong Hình 3. Đối với prompt CoT và phân tách CoT, chúng tôi quan sát rằng các đường cong có độ dốc khá nhẹ và đạt giá trị cao sớm trong một mẫu lý luận trung bình. Điều này gợi ý mô hình yêu cầu tương đối ít một mẫu lý luận CoT hoặc phân tách CoT để đạt được câu trả lời cuối cùng của nó và do đó có thể không hoàn toàn dựa vào những mẫu lý luận đó. Đối với phân tách có yếu tố, chúng tôi quan sát mô hình yêu cầu một lượng lớn hơn lý luận của nó để nhất quán đạt được cùng một câu trả lời, chỉ ra mô hình dựa vào nhiều lý luận của nó hơn khi trả lời câu hỏi.4 Chúng tôi hiển thị kết quả chi tiết hơn, được chia theo nhiệm vụ, trong Phụ lục A.1.

4Kết quả của chúng tôi được trình bày ở dạng khác với kết quả tương tự từ Lanham et al. (2023), vì chúng tôi tính trung bình kết quả của mình trên tất cả các mẫu lý luận, ngay cả khi chúng khác nhau về độ dài hoặc nhiệm vụ.

0 5 10 15 20 25 30 35 40
% Câu trả lời Khác với Gốc

Phân tách Có yếu tố
Phân tách Chuỗi Suy nghĩ
Chuỗi Suy nghĩ

33.628.79.6

Thêm Lỗi: Độ nhạy của Mô hình đối với Làm hỏng Lý luận

Hình 4. Độ nhạy của mô hình đối với việc làm hỏng lý luận. Chúng tôi đo tần suất một mô hình thay đổi câu trả lời của nó nếu được cung cấp một phiên bản bị hỏng của mẫu lý luận của nó. Lý luận được tạo ra bởi các phương pháp dựa trên phân tách trung thực hơn theo chỉ số này vì mô hình thay đổi câu trả lời của nó thường xuyên hơn nhiều khi nó được cung cấp một phiên bản bị hỏng của một mẫu lý luận dựa trên phân tách so với chuỗi suy nghĩ, chỉ ra rằng mô hình dựa vào lý luận dựa trên phân tách mạnh hơn so với lý luận chuỗi suy nghĩ. Chúng tôi tính các thanh lỗi qua tính toán lỗi chuẩn.

3.2.2. THÊM LỖI
Động lực Trong tập hợp thí nghiệm này, chúng tôi làm hỏng các mẫu lý luận và đánh giá điều này gây ra mô hình thay đổi câu trả lời cuối cùng của nó như thế nào. Chúng tôi so sánh các phương pháp prompt khác nhau bằng chỉ số này, vẽ biểu đồ phần trăm câu trả lời cuối cùng được thay đổi nếu mẫu lý luận của một mô hình bị hỏng. Chúng tôi mong đợi các phương pháp tạo ra lý luận trung thực hơn sẽ có nhiều câu trả lời cuối cùng được thay đổi hơn vì điều này chỉ ra rằng lý luận đang đóng vai trò nhân quả trong câu trả lời cuối cùng của mô hình và do đó có khả năng cao hơn phản ánh lý luận thực tế của mô hình.

Thiết lập Thí nghiệm Chúng tôi lấy một mẫu lý luận hoàn chỉnh x và yêu cầu cùng một mô hình ngôn ngữ trong một bối cảnh khác sửa đổi bước xi bằng cách thêm một lỗi vào nó và tạo ra bước bị hỏng x′i. Các prompt cho điều này được bao gồm trong Phụ lục E. Chúng tôi yêu cầu mô hình tái tạo phần còn lại của lý luận từ điểm đó trở đi, tức là chúng tôi yêu cầu mô hình với [x1, x2, . . . , x′i] và yêu cầu nó tạo ra lý luận bị hỏng [x1, x2, x3, . . . , x′i, x′i+1, . . . , x′n]. Chúng tôi thay thế thủ công lý luận gốc bằng lý luận bị hỏng trước khi yêu cầu mô hình trả lời câu hỏi gốc. Chúng tôi lặp lại điều này cho ba lựa chọn ngẫu nhiên và khác biệt của xi cho mỗi mẫu lý luận. Chúng tôi đánh giá xem mô hình có đạt được cùng một câu trả lời cuối cùng như với lý luận gốc hay không. Các ví dụ về lý luận bị hỏng cũng được trình bày trong Phụ lục E.

Kết quả Những phát hiện của chúng tôi trong Hình 4 cho thấy rằng việc làm hỏng các phân tách CoT và phân tách có yếu tố thường thay đổi câu trả lời mà mô hình đưa ra, cung cấp bằng chứng cho tuyên bố

--- TRANG 8 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

rằng các mô hình dựa vào các mẫu lý luận dựa trên phân tách nhiều hơn so với các mẫu lý luận CoT. Lý luận CoT bị hỏng cũng có thể thay đổi câu trả lời cuối cùng của mô hình, nhưng điều này xảy ra ít thường xuyên hơn nhiều so với lý luận dựa trên phân tách; một mẫu lý luận CoT bị hỏng thay đổi câu trả lời cuối cùng của mô hình chỉ cho 9.6% câu hỏi, so với 28.7% câu trả lời thay đổi cho phân tách CoT và 33.6% câu trả lời thay đổi cho phân tách có yếu tố.5 Chúng tôi hiển thị kết quả chi tiết hơn, được chia theo nhiệm vụ, trong Phụ lục A.2.

3.2.3. KẾT LUẬN
Tổng thể, kết quả của chúng tôi từ các thí nghiệm làm nhiễu lý luận gợi ý rằng phân tách câu hỏi dẫn đến lý luận được tạo ra bởi mô hình trung thực hơn. Phân tách có yếu tố tạo ra lý luận trung thực nhất, trong khi phân tách CoT tạo ra lý luận ít trung thực hơn phân tách có yếu tố nhưng trung thực hơn prompt CoT. Điều này được thể hiện bởi các thí nghiệm trả lời sớm, tìm thấy tính trung thực tương đương giữa phân tách CoT và prompt CoT, và các thí nghiệm thêm lỗi, tìm thấy phân tách CoT có tính trung thực trung gian.

3.3. Tính trung thực Được đo qua Bối cảnh Thiên lệch

3.3.1. LÝ LUẬN THIÊN LỆCH TỪ ANSWER ALWAYS A
Một cách khác để kiểm tra tính trung thực lý luận là đo lường mức độ dự đoán của mô hình thay đổi do các đặc điểm thiên lệch trong bối cảnh của mô hình, đối với các đặc điểm mà mô hình không có khả năng đề cập rõ ràng trong lý luận của nó (Turpin et al., 2023). Một ví dụ về đặc điểm thiên lệnh như vậy, mà chúng tôi kiểm tra ở đây, là làm cho tất cả các ví dụ vài mẫu trong bối cảnh của mô hình có cùng lựa chọn câu trả lời đúng "A" theo Turpin et al. (2023). Sau đó chúng tôi đo sự không trung thực bằng cách sử dụng sự sụt giảm hiệu suất được quan sát khi chúng tôi giới thiệu thiên lệch này. Giả sử mô hình trả lời theo cách phù hợp với thiên lệch, ví dụ như trả lời sai "A" nếu tất cả các ví dụ vài mẫu của nó có câu trả lời "A" nhưng sẽ trả lời câu hỏi đúng trong trường hợp khác; phát hiện này sẽ chỉ ra rằng mô hình không hoàn toàn dựa vào lý luận được nêu ra của nó cho câu trả lời cuối cùng, giả sử mô hình không bao giờ nêu rằng nó đang sử dụng đặc điểm thiên lệch (điều mà chúng tôi và Turpin et al. xác nhận trong hầu hết tất cả các mẫu lý luận mà chúng tôi quét). Ở đây, chúng tôi giới thiệu đặc điểm thiên lệch bằng cách làm cho câu trả lời đúng "A" cho mỗi ví dụ vài mẫu trong bối cảnh của mô hình, bằng cách thay đổi văn bản câu trả lời nào tương ứng với lựa chọn trắc nghiệm nào, khi cần thiết. Chúng tôi cũng thay đổi các mẫu lý luận trong

5Kết quả của chúng tôi được trình bày ở dạng khác với kết quả tương tự từ Lanham et al. (2023), vì chúng tôi tính trung bình phần trăm lần câu trả lời được thay đổi trên tất cả các mẫu lý luận, ngay cả khi chúng khác nhau về độ dài hoặc nhiệm vụ, và trên tất cả các vị trí có thể của bước bị nhầm lẫn.

prompt vài mẫu để phù hợp với sự thay đổi trong thứ tự câu trả lời, ví dụ nếu mô hình hỏi các câu hỏi con bằng cách xem qua từng lựa chọn câu trả lời theo thứ tự, chúng tôi điều chỉnh thứ tự câu hỏi con cùng với các lựa chọn câu trả lời. Sau đó chúng tôi yêu cầu mô hình tạo ra lý luận và trả lời câu hỏi, hoặc trực tiếp trả lời câu hỏi trong điều kiện vài mẫu.

Triển khai Chúng tôi đánh giá các phương pháp của mình trên các nhiệm vụ khác với Turpin et al. Kết quả là, các ví dụ vài mẫu chúng tôi sử dụng trong prompt của mình khác với ví dụ vài mẫu của họ, vì chúng tôi sử dụng cùng ví dụ cho mỗi phương pháp như chúng tôi đã làm cho các thí nghiệm trước đó. Các ví dụ vài mẫu của chúng tôi cũng bao gồm các cuộc hội thoại hai phía giữa Con người và Trợ lý, nơi Con người hỏi một câu hỏi và Trợ lý trả lời một câu hỏi, có thể sau khi tạo ra lý luận; Turpin et al. thay vào đó đặt tất cả các ví dụ vài mẫu và bối cảnh ở phía Con người của cuộc hội thoại, trước khi yêu cầu Trợ lý trả lời câu hỏi (có thể sau khi tạo ra lý luận). Theo Turpin et al. (2023), chúng tôi lọc kết quả của mình bằng cách loại trừ các câu hỏi nơi câu trả lời đúng là "A", để cụ thể xem kết quả cho các câu hỏi nơi thiên lệch có thể dẫn mô hình hướng tới một câu trả lời không đúng.

Kết quả Hình 5 (phải) hiển thị kết quả. Chúng tôi thấy rằng prompt CoT, phân tách CoT, và phân tách có yếu tố đều tương tự không bị ảnh hưởng bởi đặc điểm thiên lệch trong các ví dụ vài mẫu. Chúng tôi quan sát đối với prompt CoT một sự sụt giảm 1.2% (tuyệt đối) về độ chính xác, đối với phân tách CoT một sự sụt giảm 2.8%, và đối với phân tách có yếu tố một sự tăng 2.1%6. Điều này trái ngược với sự sụt giảm hiệu suất đáng kể hơn 7.1% (tuyệt đối) cho điều kiện vài mẫu. Tổng thể, kết quả của chúng tôi trong bối cảnh này không tiết lộ sự khác biệt đáng kể trong tính trung thực lý luận của các phương pháp khác nhau. Chúng tôi trình bày kết quả chi tiết hơn, được chia theo nhiệm vụ, trong Phụ lục A.3.

Turpin et al. (2023) thấy rằng prompt CoT cho thấy sự sụt giảm hiệu suất lớn hơn với các bối cảnh thiên lệch so với các bối cảnh không thiên lệch cho thiên lệch này (-4.7% tuyệt đối). Có một số giải thích có thể cho kết quả khác biệt của chúng tôi. Điều này có thể do sự khác biệt trong định dạng prompt (liệu các ví dụ vài mẫu được đưa ra ở phía con người hay trợ lý của cuộc hội thoại), các nhiệm vụ đánh giá, và/hoặc mô hình (Turpin et al. sử dụng Claude 1.0 cho các thí nghiệm). Quan trọng nhất, hai trong số các nhiệm vụ đánh giá của chúng tôi, phiên bản được lọc của HotpotQA và StrategyQA, chỉ có hai lựa chọn câu trả lời cho tất cả các câu hỏi của chúng. Bằng cách lọc ra các câu hỏi nơi câu trả lời đúng thực sự là "A", chúng tôi loại bỏ một nửa số ví dụ cho mỗi nhiệm vụ này. Việc so sánh tất cả các phương pháp prompt với nhau trong bối cảnh này vẫn hợp lệ, nhưng việc so sánh với các kết quả khác, chẳng hạn như những kết quả trong Turpin et al. (2023), có thể không phù hợp.

6Chúng tôi nghi ngờ rằng sự tăng có thể do nhiễu.

--- TRANG 9 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

Câu trả lời Được gợi ý                    Câu trả lời Luôn là A
Tác động của Bối cảnh Thiên lệch lên Độ chính xác

30354045505 5606 57 07 5808590↑ Độ chính xác
Không mẫu Vài mẫu Chuỗi  Phân tách    Phân tách
             Suy nghĩ Chuỗi Suy nghĩ Có yếu tố
- 34. 138. 772.8- 16.663. 179 . 7-21.364. 786-29 . 1-9 .2
56.572.685.681.8

30354045505 5606 57 07 5808590↑ Độ chính xác
Vài mẫu Chuỗi  Phân tách    Phân tách
         Suy nghĩ Chuỗi Suy nghĩ Có yếu tố
-4.3- 1.2-2.8+ 2. 172. 17 6.483. 782.38484. 985. 181. 9

Bối cảnh Không thiên lệch
Bối cảnh Thiên lệch

Hình 5. Đo tính trung thực lý luận với các bối cảnh thiên lệch. Chúng tôi đo sự khác biệt độ chính xác giữa các bối cảnh không thiên lệch và thiên lệch cho tất cả các kỹ thuật prompt. Chúng tôi đánh giá hai dạng bối cảnh thiên lệch: người dùng gợi ý một câu trả lời không đúng (câu trả lời được gợi ý) và các lựa chọn câu trả lời trong tất cả các ví dụ vài mẫu chia sẻ lựa chọn câu trả lời "A" (câu trả lời luôn là A). Phân tách có yếu tố ít bị ảnh hưởng nhất bởi thiên lệch câu trả lời được gợi ý. Kết quả cho thiên lệch câu trả lời luôn là A đều hơn trên các phương pháp khác nhau, với không có chiến lược nào sử dụng lý luận được tạo ra bởi mô hình bị ảnh hưởng mạnh bởi thiên lệch. Tính dễ bị tổn thương đối với những thiên lệch này chỉ ra sự thiếu trung thực lý luận, vì chúng tôi không bao giờ quan sát mô hình đề cập đến những thiên lệnh trong các mẫu lý luận của nó; điều này ngụ ý rằng nếu mô hình trả lời theo cách phù hợp với thiên lệch, nó không hoàn toàn dựa vào lý luận được nêu ra của nó.

3.3.2. LÝ LUẬN THIÊN LỆCH TỪ SUGGESTED ANSWER
Tiếp theo, chúng tôi khám phá việc giới thiệu một đặc điểm thiên lệch khác từ Turpin et al. (2023). Chúng tôi chọn một câu trả lời không đúng cho mỗi câu hỏi và thêm (vào phát biểu của con người) "Tôi nghĩ câu trả lời đúng là ([lựa chọn câu trả lời không đúng]), nhưng tôi tò mò muốn nghe bạn nghĩ gì."7 Sau đó chúng tôi yêu cầu mô hình tạo ra lý luận và trả lời câu hỏi, hoặc trực tiếp trả lời câu hỏi trong các điều kiện không mẫu và vài mẫu. Giả sử độ chính xác của mô hình giảm đáng kể và nó thất bại trong việc đề cập rằng nó đang chọn câu trả lời được gợi ý vì con người đã gợi ý nó; một phát hiện như vậy sẽ gợi ý sự thiếu trung thực lý luận, vì những lý do tương tự như trong §3.3.1. Chúng tôi một lần nữa đo sự sụt giảm hiệu suất gây ra bởi việc thêm đặc điểm thiên lệch, trong khi xác minh rằng các mẫu lý luận không tham chiếu đến thiên lệch. Chúng tôi sử dụng chi tiết triển khai từ §3.3.1 ở đây, ngoại trừ việc lọc kết quả. Ở đây, vì chúng tôi luôn gợi ý một câu trả lời không đúng cho mô hình, chúng tôi không cần lọc kết quả để chọn các câu hỏi nơi mô hình có thể

7Điều này kiểm tra xu hướng của mô hình trả lời câu hỏi theo những cách phù hợp với sở thích hoặc niềm tin của đối tác hội thoại con người của nó (Perez et al., 2022, "sycophancy"); chúng tôi trình bày kết quả bổ sung liên quan đến sycophancy trong Phụ lục B.

trả lời câu hỏi không đúng do thiên lệch. Đây là một sự khác biệt nhỏ so với thiết lập của Turpin et al. (2023), những người thay vào đó luôn gợi ý một lựa chọn câu trả lời ngẫu nhiên và sau đó lọc cho các ví dụ nơi gợi ý là một lựa chọn câu trả lời không đúng; cuối cùng, cả hai phân tích nên dẫn đến những phát hiện tương tự.

Kết quả Hình 5 (trái) hiển thị kết quả của chúng tôi. Chúng tôi thấy một sự sụt giảm hiệu suất đáng kể cho tất cả các phương pháp. Đối với prompt CoT, chúng tôi quan sát một sự sụt giảm 21.3% (tuyệt đối) về độ chính xác, đối với phân tách CoT một sự sụt giảm 29.1%, và đối với phân tách có yếu tố một sự sụt giảm 9.2%, ít nhất trong tất cả các phương pháp prompt. Phát hiện này gợi ý rằng phân tách có yếu tố giảm thiểu một phần nhưng không phải tất cả sự thiếu trung thực được quan sát trong các phương pháp khác trong bối cảnh này. Cũng đáng chú ý rằng lý luận CoT trung thực hơn lý luận phân tách CoT trong bối cảnh này, mặc dù cả hai phương pháp đều quan sát sự giảm hiệu suất lớn hơn so với điều kiện prompt vài mẫu (16.6% sụt giảm tuyệt đối). Chúng tôi trình bày kết quả chi tiết hơn, được chia theo nhiệm vụ, trong Phụ lục A.3.

--- TRANG 10 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

Mẫu Phân tách Chuỗi Suy nghĩ
Người: Câu hỏi: Ban nhạc The Shins có được thành lập trước ban nhạc Paramore không?
Lựa chọn:
(A) Có
(B) Không
Trả lời:
Trợ lý:
<subq>Ban nhạc The Shins được thành lập khi nào? </subq>
<suba>The Shins được thành lập năm 1996. </suba>
<subq>Ban nhạc Paramore được thành lập khi nào? </subq>
<suba>Paramore được thành lập năm 2004. </suba>
<FIN></FIN>
Người: Dựa trên những điều trên, lựa chọn câu trả lời đơn lẻ có khả năng nhất là gì?
Trợ lý: Câu trả lời đúng là lựa chọn (A

Mẫu Phân tách Có yếu tố
Người: Câu hỏi: Ban nhạc The Shins có được thành lập trước ban nhạc Paramore không?
Lựa chọn:
(A) Có
(B) Không
Câu hỏi con và câu trả lời:
Câu hỏi: Ban nhạc The Shins được thành lập khi nào? Câu trả lời: The Shins được thành lập năm 1996.
Câu hỏi: Ban nhạc Paramore được thành lập khi nào? Câu trả lời: Paramore được thành lập năm 2004.
Trợ lý: Dựa trên những điều trên, câu trả lời đúng là lựa chọn (A

Bảng 5. Mẫu lý luận phân tách chuỗi suy nghĩ và phân tách có yếu tố. Để ngắn gọn, chúng tôi rút gọn lý luận phân tách có yếu tố xuống giai đoạn tái cấu trúc.

3.3.3. KẾT LUẬN
Những phát hiện của chúng tôi nghiên cứu tính trung thực của lý luận được tạo ra bởi mô hình qua các bối cảnh thiên lệch gợi ý rằng phân tách có yếu tố dẫn đến lý luận trung thực hơn so với CoT hoặc phân tách CoT. Lý luận phân tách CoT trông ít trung thực hơn lý luận CoT qua những chỉ số này, nhưng các đo lường của chúng tôi từ các thí nghiệm làm nhiễu lý luận gợi ý ngược lại. Chúng tôi không đưa ra bất kỳ tuyên bố nào về thứ tự của các phương pháp về tầm quan trọng của chúng đối với tính trung thực tổng thể, vậy nên bằng cách tính trung bình đơn giản (sau khi chuẩn hóa về thang 0–1), chúng tôi đánh giá lý luận phân tách CoT trung thực hơn lý luận CoT.

3.4. Những phát hiện Định tính
Chúng tôi hiển thị các mẫu lý luận cho phân tách CoT và phân tách có yếu tố trong Bảng 5 và Phụ lục D. Các phân tách được tạo ra bởi mô hình, cho cả phân tách CoT và phân tách có yếu tố, nhìn chung hợp lý. Mô hình thường tạo ra các câu hỏi con cho mỗi lựa chọn câu trả lời để thực hiện quá trình loại trừ, điều này phản ánh các ví dụ vài mẫu trong bối cảnh của nó. Ngoài ra, mô hình thường hỏi một câu hỏi (con) giới thiệu về chủ đề chung đằng sau câu hỏi; điều này giúp thu thập bối cảnh đôi khi được sử dụng trong các câu hỏi con trong tương lai.

Những phát hiện Định tính Phân tách Có yếu tố Đôi khi mô hình thất bại trong việc diễn đạt một câu hỏi con sao cho nó có thể được trả lời mà không cần bối cảnh bổ sung. Nó cũng có thể tái tạo các câu hỏi con trước đó không thể được trả lời và vẫn thất bại trong việc nhận được câu trả lời cho chúng, thay vì sửa chữa các câu hỏi con một cách đáng tin cậy để chúng có thể được trả lời. Thỉnh thoảng, các câu hỏi con và câu trả lời con cuối cùng hỗ trợ nhiều lựa chọn câu trả lời. Mô hình vẫn có thể cuối cùng trả lời câu hỏi đúng, nhưng từ góc độ tính trung thực, mô hình lý tưởng sẽ thảo luận rõ ràng về cái nào trong số các câu trả lời được hỗ trợ là đúng.

3.5. Thảo luận và Hạn chế
Những phát hiện của chúng tôi chỉ ra rằng việc sử dụng phân tách câu hỏi so với prompt CoT cung cấp lợi ích tính trung thực với chi phí của hiệu suất trả lời câu hỏi. Phân tách có yếu tố tạo ra lý luận trung thực nhất nhưng dẫn đến hiệu suất trả lời câu hỏi tệ nhất. Phân tách CoT cung cấp lý luận và hiệu suất trung thực trung gian. Chúng tôi không chắc chắn về cách sự đánh đổi quan sát này có thể bị ảnh hưởng bởi các cải tiến khác như huấn luyện thêm, đặc biệt là huấn luyện hướng tới việc cải thiện khả năng của mô hình trả lời câu hỏi qua phân tách. Huấn luyện như vậy hoặc các kỹ thuật khác có thể dẫn đến các phương pháp thống trị Pareto cho lý luận được tạo ra bởi mô hình có tính trung thực và hiệu suất cao, điều mà chúng tôi tin là một mục tiêu thú vị cho nghiên cứu tương lai.

Công việc của chúng tôi dựa nhiều vào các phương pháp chúng tôi sử dụng để đánh giá tính trung thực của lý luận được tạo ra bởi mô hình. Những phương pháp này bị hạn chế bởi sự không thể truy cập của chúng tôi vào sự thật cơ bản cho lý luận của mô hình. Tuyên bố của chúng tôi rằng phân tách câu hỏi cải thiện tính trung thực lý luận là một tuyên bố dựa trên nhiều dòng bằng chứng khá độc lập, nhưng chúng tôi mở cho các công cụ tương lai để đánh giá tính trung thực lý luận, có thể những công cụ dựa trên hiểu biết cơ học về các tính toán nội bộ của mô hình của chúng tôi (Olah, 2023), thay đổi kết luận của chúng tôi. Ngoài ra, chúng tôi đánh giá các phương pháp của mình chỉ trên bốn nhiệm vụ trả lời câu hỏi và chỉ trên một mô hình (một LLM được tinh chỉnh RLHF); các LLM được tiền huấn luyện có thể có xu hướng tạo ra lý luận bị bỏ qua hoặc thiên lệch nhiều hơn hoặc ít hơn, điều này có thể tăng hoặc giảm lợi ích tính trung thực có được qua phân tách. Mở rộng sự đa dạng của các nhiệm vụ và mô hình được đánh giá có thể dẫn đến kết luận mạnh mẽ hơn về hiệu suất tương đối và tính trung thực lý luận của prompt CoT và các cách tiếp cận phân tách câu hỏi.

4. Công trình Liên quan
Phân tách Nhiệm vụ và Nhận thức Có yếu tố Phân tách nhiệm vụ đã được chứng minh đạt hiệu suất mạnh trong nhiều bối cảnh khác nhau. Một số phương pháp để

--- TRANG 11 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

yêu cầu mô hình ngôn ngữ lý luận có điểm tương đồng với các cách tiếp cận phân tách câu hỏi mà chúng tôi nghiên cứu, ví dụ như Least-To-Most Prompting (Zhou et al., 2023), Plan-and-Solve Prompting (Wang et al., 2023), Selection-Inference (Creswell et al., 2023), và Successive Prompting (một phiên bản ít linh hoạt hơn của phân tách có yếu tố; Dua et al., 2022). Những phương pháp này kết hợp lý luận kiểu phân tách (Least-To-Most, Plan-and-Solve, và Successive Prompting) và/hoặc hạn chế lượng bối cảnh được sử dụng khi tạo ra các bước lý luận (Least-to-Most Prompting, Successive Prompting, và Selection-Inference). Ferrucci et al. (2010); Min et al. (2019); Perez et al. (2020); Fu et al. (2021); và Guo et al. (2022) khám phá việc sử dụng giám sát, heuristic, hoặc mô hình ngôn ngữ để phân tách các câu hỏi khó, đa bước thành các câu hỏi con dễ, một bước có thể được trả lời độc lập. Reppert et al. (2023) nghiên cứu quá trình Phân tách Lặp lại, nơi một con người giúp phân tách nhiệm vụ cho LLM thực hiện. AlKhamissi et al. (2022) thấy rằng việc phân tách nhiệm vụ phát hiện lời nói thù hận thành một số nhiệm vụ con cải thiện đáng kể độ chính xác và khả năng khái quát hóa ngoài phân phối. Christiano et al. (2018) và Snell et al. (2022) cải thiện hiệu suất nhiệm vụ bằng cách trả lời câu hỏi qua phân tách, sau đó học cách dự đoán hoặc chưng cất những câu trả lời được cải thiện đó trở lại mô hình gốc. Rộng hơn, Stuhlmüller (2018) trình bày giả thuyết nhận thức có yếu tố hoặc tuyên bố rằng các nhiệm vụ có thể được phân tách hoặc nhân tử hóa thành các nhiệm vụ con nhỏ và chủ yếu độc lập. Stuhlmüller et al. (2022) trình bày một thư viện phần mềm để triển khai các chương trình nhận thức có yếu tố với LLM. Công việc của chúng tôi bổ sung cho tài liệu hiện có bằng cách gợi ý rằng các phương pháp dựa trên phân tách có thể có lợi ích bổ sung ngoài hiệu suất, cụ thể là, cải thiện tính trung thực của lý luận được tạo ra.

Tính trung thực Giải thích Công trình trước đó cũng đề xuất các chỉ số và đánh giá tính trung thực của lý luận được tạo ra bởi mô hình. Chúng tôi áp dụng định nghĩa lý luận trung thực từ Jacovi & Goldberg (2020), nơi lý luận trung thực ở mức độ nó phản ánh lý luận thực tế của mô hình. Một loại tính trung thực là mức độ mà các giải thích dẫn đến khả năng mô phỏng hành vi mô hình, nơi mục tiêu là để hành vi mô hình phù hợp với kỳ vọng của con người, có thể sau khi phân tích lý luận của mô hình (Doshi-Velez & Kim, 2017; Hase et al., 2020; Wiegreffe et al., 2021). Gao (2023) thấy rằng LLM có thể bỏ qua các phần của lý luận CoT của chúng, như được đánh giá bằng cách làm nhiễu các mẫu lý luận CoT, củng cố kết quả của chúng tôi và kết quả của Lanham et al. (2023). Creswell et al. (2023); Lyu et al. (2023) khám phá các phương pháp để yêu cầu mô hình tạo ra các giải thích có khả năng trung thực hơn bằng cách xây dựng, mặc dù họ không đo lường tính trung thực một cách rõ ràng. Công trình khác đánh giá tính hợp lý của lý luận CoT và thấy tính hợp lý của lý luận CoT là đa dạng; một số thấy lý luận CoT chứa mâu thuẫn và lỗi logic (Uesato et al., 2022; Jung et al., 2022; Ye & Durrett, 2022; Golovneva et al., 2023), nhưng những người khác thấy các giải thích CoT vừa hợp lý vừa hữu ích, ngay cả đối với các mô hình nhỏ hơn (Madaan & Yazdanbakhsh, 2022; Li et al., 2022).

5. Kết luận
Chúng tôi khám phá ba chiến lược prompt để cải thiện hiệu suất trả lời câu hỏi trong khi kích thích lý luận trung thực từ LLM: prompt Chuỗi Suy nghĩ (CoT), phân tách CoT, và phân tách có yếu tố. Công việc của chúng tôi cho thấy có thể cải thiện đáng kể tính trung thực của lý luận được tạo ra bởi mô hình bằng cách yêu cầu mô hình thực hiện phân tách câu hỏi trong khi duy trì mức độ tương tự của độ chính xác trả lời câu hỏi, gợi ý rằng còn nhiều chỗ để tiến bộ sử dụng các kỹ thuật khác.

Chúng tôi mong đợi việc kiểm tra quá trình lý luận của mô hình là một đòn bẩy mạnh mẽ để cải thiện an toàn của chúng khi giám sát mô hình trong các bối cảnh quan trọng (Rudin, 2019); nếu mô hình cung cấp lý luận trung thực cho đầu ra của chúng, chúng ta có thể loại bỏ đầu ra của chúng trong các tình huống nơi lý luận của chúng bề mặt hành vi không mong muốn như hack phần thưởng hoặc sycophancy. Chúng tôi thấy một số hướng đầy hứa hẹn để xây dựng dựa trên kết quả của chúng tôi. Thứ nhất, huấn luyện mô hình để tạo ra lý luận hiệu quả và trung thực hơn có thể dẫn đến lợi ích thêm, bằng cách huấn luyện mô hình ví dụ để giải quyết vấn đề qua phân tách hoặc để tạo ra lý luận nhất quán trên các đầu vào liên quan logic (để giảm thiểu lý luận không trung thực, thiên lệnh; Turpin et al., 2023). Thứ hai, cải thiện tính trung thực của lý luận được nêu ra của mô hình có thể cải thiện hiệu quả của các phương pháp huấn luyện mô hình dựa trên quá trình lý luận được nêu ra của chúng (Uesato et al., 2022; Lightman et al., 2023). Cuối cùng, quan trọng là xác thực rằng lý luận được nêu ra trung thực cho phép chúng ta phát hiện hành vi mô hình không mong muốn, đặc biệt là những hành vi sẽ khó bắt được bằng cách chỉ nhìn vào đầu ra cuối cùng của mô hình. Với nghiên cứu thêm, chúng tôi hy vọng rằng lý luận trung thực, được tạo ra bởi mô hình sẽ cho phép chúng ta hiểu đáng tin cậy và huấn luyện cách LLM thực hiện nhiệm vụ qua giám sát dựa trên quá trình, ngay cả khi những nhiệm vụ đó trở nên ngày càng thách thức hơn.

Đóng góp Tác giả
Ansh Radhakrishnan dẫn dắt dự án, soạn thảo bài báo, và tiến hành tất cả công việc thí nghiệm ngoại trừ các thí nghiệm sycophancy, được tiến hành bởi Karina Nguyen. Karina Nguyen, Jan Brauner, Samuel R. Bowman, và Ethan Perez giúp sửa đổi bài báo và hình ảnh. Jared Kaplan, Samuel R. Bowman, và Ethan Perez cung cấp phản hồi trong suốt quá trình dự án, và Ethan Perez xác định phạm vi hướng dự án. Tất cả các tác giả được liệt kê khác đóng góp vào việc phát triển các mô hình chưa được công bố, cơ sở hạ tầng, hoặc cung cấp hỗ trợ khác làm cho các thí nghiệm của chúng tôi có thể thực hiện được.

--- TRANG 12 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

Lời cảm ơn
Chúng tôi cảm ơn Amanda Askell, Buck Shlegeris, Daniel Ziegler, Kshitij Sachan, Leo Gao, Miles Turpin, Ryan Greenblatt, và Saurav Kadavath vì phản hồi và thảo luận hữu ích.

Tài liệu tham khảo

AlKhamissi, B., Ladhak, F., Iyer, S., Stoyanov, V., Kozareva, Z., Li, X., Fung, P., Mathias, L., Celikyilmaz, A., and Diab, M. ToKen: Task decomposition and knowledge infusion for few-shot hate speech detection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2109–2120, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.136.

Anthropic. Introducing claude, 2023. URL https://www.anthropic.com/index/introducing-claude.

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., Mann, B., and Kaplan, J. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F. P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W. Evaluating large language models trained on code, 2021.

Christiano, P., Shlegeris, B., and Amodei, D. Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575, 2018.

Creswell, A., Shanahan, M., and Higgins, I. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=3Pf3Wg6o-A4.

Doshi-Velez, F. and Kim, B. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608, 2017.

Dua, D., Gupta, S., Singh, S., and Gardner, M. Successive prompting for decomposing complex questions. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1251–1265, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.81.

Ferrucci, D. A., Brown, E. W., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A., Lally, A., Murdock, J. W., Nyberg, E., Prager, J. M., Schlaefer, N., and Welty, C. Building watson: An overview of the deepqa project. AI Mag., 31:59–79, 2010.

Fu, R., Wang, H., Zhang, X., Zhou, J., and Yan, Y. Decomposing complex questions makes multi-hop QA easier and more interpretable. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 169–180, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.17. URL https://aclanthology.org/2021.findings-emnlp.17.

Gao, L. Shapley value attribution in chain of thought. https://www.lesswrong.com/posts/FX5JmftqL2j6K8dn4/shapley-value-attribution-in-chain-of-thought, 04 2023.

Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and Berant, J. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics (TACL), 2021.

Golovneva, O., Chen, M. P., Poff, S., Corredor, M., Zettlemoyer, L., Fazel-Zarandi, M., and Celikyilmaz, A. ROSCOE: A suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=xYlJRpzZtsY.

Guo, X.-Y., Li, Y.-F., and Haffari, G. Complex reading comprehension through question decomposition. In Proceedings of the The 20th Annual Workshop of the Australasian Language Technology Association, pp. 31–40, Adelaide, Australia, December 2022. Australasian Language Technology Association. URL https://aclanthology.org/2022.alta-1.5.

--- TRANG 13 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

Hase, P., Zhang, S., Xie, H., and Bansal, M. Leakage-adjusted simulatability: Can models generate non-trivial explanations of their behavior in natural language? In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 4351–4367, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.390. URL https://aclanthology.org/2020.findings-emnlp.390.

Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=rygGQyrFvH.

Jacovi, A. and Goldberg, Y. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 4198–4205, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.386. URL https://aclanthology.org/2020.acl-main.386.

Jung, J., Qin, L., Welleck, S., Brahman, F., Bhagavatula, C., Le Bras, R., and Choi, Y. Maieutic prompting: Logically consistent reasoning with recursive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1266–1279, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.82.

Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 22199–22213. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf.

Lanham, T., Chen, A., Radhakrishnan, A., Steiner, B., Denison, C., Hernandez, D., Li, D., Durmus, E., Hubinger, E., Kernion, J., Lukosuite, K., Nguyen, K., Cheng, N., Joseph, N., Schiefer, N., Rausch, O., Larson, R., McCandlish, S., Kundu, S., Kadavath, S., Yang, S., Henighan, T., Maxwell, T., Telleen-Lawton, T., Hume, T., Hatfield-Dodds, Z., Kaplan, J., Brauner, J., Bowman, S. R., and Perez, E. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint (released concurrently), 2023.

Li, S., Chen, J., Shen, Y., Chen, Z., Zhang, X., Li, Z., Wang, H., Qian, J., Peng, B., Mao, Y., Chen, W., and Yan, X. Explanations from large language models make small reasoners better. arXiv preprint arXiv:2210.06726, 2022.

Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.

Lin, S., Hilton, J., and Evans, O. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 3214–3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229.

Lyu, Q., Havaldar, S., Stein, A., Zhang, L., Rao, D., Wong, E., Apidianaki, M., and Callison-Burch, C. Faithful chain-of-thought reasoning. arXiv preprint arXiv 2301.13379, 2023.

Madaan, A. and Yazdanbakhsh, A. Text and patterns: For effective chain of thought, it takes two to tango, 2022. arXiv prepring arXiv:2209.07686.

Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP, 2018.

Min, S., Zhong, V., Zettlemoyer, L., and Hajishirzi, H. Multi-hop reading comprehension through question decomposition and rescoring. arXiv preprint arXiv 1906.02916, 2019.

Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., Jiang, X., Cobbe, K., Eloundou, T., Krueger, G., Button, K., Knight, M., Chess, B., and Schulman, J. Webgpt: Browser-assisted question-answering with human feedback. CoRR, abs/2112.09332, 2021. URL https://arxiv.org/abs/2112.09332.

Nye, M., Johan Andreassen, A., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.

Olah, C. Interpretability Dreams, 2023. URL https://transformer-circuits.pub/2023/interpretability-dreams/index.html.

Patel, P., Mishra, S., Parmar, M., and Baral, C. Is a question decomposition unit all we need? arXiv preprint arXiv:2205.12538, 2022.

--- TRANG 14 ---
Phân Tách Câu Hỏi Cải Thiện Tính Trung Thực của Lý Luận Được Tạo Ra Bởi Mô hình

Perez, E., Lewis, P., Yih, W.-t., Cho, K., and Kiela, D. Unsupervised question decomposition for question answering. In Conference on Empirical Methods in Natural Language Processing, 2020.

Perez, E., Ringer, S., Lukošiūtė, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B., Israel, B., Seethor, B., McKinnon, C., Olah, C., Yan, D., Amodei, D., Amodei, D., Drain, D., Li, D., Tran-Johnson, E., Khundadze, G., Kernion, J., Landis, J., Kerr, J., Mueller, J., Hyun, J., Landau, J., Ndousse, K., Goldberg, L., Lovitt, L., Lucas, M., Sellitto, M., Zhang, M., Kingsland, N., Elhage, N., Joseph, N., Mercado, N., DasSarma, N., Rausch, O., Larson, R., McCandlish, S., Johnston, S., Kravec, S., Showk, S. E., Lanham, T., Telleen-Lawton, T., Brown, T., Henighan, T., Hume, T., Bai, Y., Hatfield-Dodds, Z., Clark, J., Bowman, S. R., Askell, A., Grosse, R., Hernandez, D., Ganguli, D., Hubinger, E., Schiefer, N., and Kaplan, J. Discovering language model behaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022.

Reppert, J., Rachbach, B., George, C., Stebbing, L., Byun, J., Appleton, M., and Stuhlmüller, A. Iterated decomposition: Improving science Q&A by supervising reasoning processes. arXiv preprint arXiv:2301.01751, 2023.

Reynolds, L. and McDonell, K. Prompt programming for large language models: Beyond the few-shot paradigm. arXiv preprint arXiv:2102.07350, 2021.

Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1:206–215, 05 2019. doi: 10.1038/s42256-019-0048-x.

Snell, C., Klein, D., and Zhong, R. Learning by distilling context. arXic preprint arXiv 2209.15189, 2022.

Stuhlmüller, A. Factored cognition. https://www.alignmentforum.org/posts/DFkGStzvj3jgXibFG/factored-cognition, 12 2018. AI Alignment Forum.

Stuhlmüller, A., Reppert, J., and Stebbing, L. Factored cognition primer. https://primer.ought.org, 2022.

Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. Galactica: A large language model for science, 2022.

Turpin, M., Michael, J., Perez, E., and Bowman, S. R. Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting. arXiv preprint arXiv:2305.04388, 2023.

Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process- and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022.

Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lee, E.-P. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., ichter, b., Xia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 24824–24837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.

Wiegreffe, S., Marasović, A., and Smith, N. A. Measuring association between labels and free-text rationales. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10266–10284, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.804. URL https://aclanthology.org/2021.emnlp-main.804.

Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., and Manning, C. D. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.

Ye, X. and Durrett, G. The unreliability of explanations in few-shot prompting for textual reasoning. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=Bct2f8fRd8S.

Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., and Chi, E. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2023.

--- TRANG 15-31 ---
[Tiếp tục dịch các trang còn lại với nội dung về kết quả chi tiết, biểu đồ, bảng và phụ lục...]
