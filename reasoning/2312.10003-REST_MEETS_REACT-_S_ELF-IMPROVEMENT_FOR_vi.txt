# 2312.10003.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reasoning/2312.10003.pdf
# Kích thước tệp: 717670 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
REST GẶP REACT: TỰ CẢI THIỆN CHO
AGENT LLM LẬP LUẬN ĐA BƯỚC

Renat Aksitov†1, Sobhan Miryoosefi†1, Zonglin Li†1, Daliang Li†1, Sheila Babayan†2,
Kavya Kopparapu†2, Zachary Fisher1, Ruiqi Guo1, Sushant Prakash1, Pranesh Srinivasan3,
Manzil Zaheer2, Felix Yu1, và Sanjiv Kumar1
1Google Research,2Google DeepMind,3Google
†Những người đóng góp chính

TÓM TẮT
Trả lời các câu hỏi ngôn ngữ tự nhiên phức tạp thường đòi hỏi lập luận đa bước và tích hợp thông tin bên ngoài. Một số hệ thống đã kết hợp truy xuất kiến thức với mô hình ngôn ngữ lớn (LLM) để trả lời những câu hỏi như vậy. Tuy nhiên, những hệ thống này gặp phải nhiều trường hợp thất bại khác nhau, và chúng ta không thể huấn luyện chúng từ đầu đến cuối một cách trực tiếp để khắc phục những thất bại như vậy, vì tương tác với kiến thức bên ngoài không thể vi phân được. Để giải quyết những thiếu sót này, chúng tôi định nghĩa một agent LLM theo phong cách ReAct với khả năng lập luận và hành động dựa trên kiến thức bên ngoài. Chúng tôi tiếp tục tinh chỉnh agent thông qua một phương pháp giống ReST mà lặp đi lặp lại huấn luyện trên các quỹ đạo trước đó, sử dụng học tăng cường batch tăng trưởng với phản hồi AI để tự cải thiện và tự chưng cất liên tục. Bắt đầu từ một mô hình lớn được gợi ý và chỉ sau hai lần lặp của thuật toán, chúng ta có thể tạo ra một mô hình nhỏ được tinh chỉnh đạt hiệu suất tương đương trên các benchmark trả lời câu hỏi tổng hợp đầy thử thách với ít hơn hai bậc độ lớn về số tham số.

1 GIỚI THIỆU

Hình 1: Tự cải thiện và tự chưng cất của Agent.
Bamboogle auto-eval, độ chính xác trung bình và độ lệch chuẩn qua 10 lần chạy, (%)

Đối với nhiều tác vụ ngôn ngữ tự nhiên đơn giản, như trả lời câu hỏi cơ bản hoặc tóm tắt, chúng ta có thể tương đối dễ dàng quyết định liệu đầu ra cuối cùng có tốt hay xấu, thu thập lượng lớn dữ liệu như vậy, và huấn luyện các mô hình ngôn ngữ sử dụng những kết quả này làm phản hồi. Đồng thời, đối với các vấn đề phức tạp hơn, các hệ thống dựa trên kết quả thường không đủ, và phương pháp giám sát quá trình gần đây đã thu hút nhiều sự chú ý như một lựa chọn thay thế đầy hứa hẹn hơn (Reppert et al. (2023)).

Có sự tăng trưởng bùng nổ trong các kỹ thuật (Gao et al. (2023); Madaan et al. (2023)), framework (Dohan et al. (2022); Khattab et al. (2023b)), và thư viện (Liu (2022), Chase (2022)) để định nghĩa quy trình làm việc dựa trên quá trình với LLMs thông qua phân tách tác vụ có thể hiểu được bởi con người. Nhiều phân tách như vậy liên quan đến tương tác với các công cụ/API/môi trường bên ngoài, trong trường hợp đó quy trình làm việc đa bước tương ứng thường được gọi là agent LLM (Xi et al. (2023)), một hệ thống có khả năng thực hiện một chuỗi hành động để đạt được mục tiêu.

Hãy xem xét tác vụ trả lời các câu hỏi phức tạp, mở rộng, nơi agent cần sử dụng API tìm kiếm để tra cứu nhiều thông tin trước khi soạn một câu trả lời dài đoạn văn. Một phương pháp phổ biến để xây dựng những agent như vậy với LLMs là phương pháp ReAct (Yao et al., 2022), bao gồm việc xen kẽ lập luận chuỗi suy nghĩ với các hành động và quan sát trong suốt một số vòng suy nghĩ-hành động-quan sát. Trong công trình này, chúng tôi tuân theo định dạng ReAct chung cho Search Agent của chúng tôi trong khi thiết kế các gợi ý few-shot tương ứng để tạo ra câu trả lời cuối cùng dài, có thể quy kết một cách rõ ràng (cf. Nakano et al. (2021)).

Một cách tự nhiên là hỏi tiếp theo về cách xử lý các trường hợp thất bại của agent như vậy và cách cải thiện hiệu suất và tính mạnh mẽ của nó. Đối với các hệ thống dựa trên kết quả, giải pháp thường đơn giản: chúng ta chỉ cần thu thập thêm dữ liệu được gán nhãn bởi con người. Tuy nhiên, việc thu thập dữ liệu như vậy khó khăn và tốn kém hơn nhiều đối với các hệ thống dựa trên quá trình: cần một lượng dữ liệu lớn hơn đáng kể (Uesato et al. (2022); Lightman et al. (2023)), và thường khó hơn cho con người để xác định quỹ đạo đa bước tối ưu.

Để giải quyết việc thiếu và khó khăn trong việc thu thập dữ liệu đa bước được gán nhãn bởi con người, chúng tôi tập trung vào việc cải thiện chất lượng của agent bằng tự phê bình, phản hồi AI, và tạo dữ liệu tổng hợp. Cụ thể đối với điều sau, chúng tôi điều chỉnh thuật toán Reinforced Self-Training (ReST) được đề xuất gần đây (Gulcehre et al. (2023)) hướng tới các thiết lập agent. Luồng vòng trong-ngoài của ReST vẫn giữ nguyên: trong vòng ngoài ("grow"), tập dữ liệu được mở rộng bằng cách lấy mẫu từ chính sách mới nhất, và trong vòng trong ("improve"), chính sách được cải thiện trên một tập dữ liệu cố định thông qua xếp hạng hoặc lọc với mô hình phần thưởng. Trong trường hợp của chúng tôi, việc lấy mẫu trong "grow" có nghĩa là tạo ra một quỹ đạo đa bước để hoàn thành, và xếp hạng như một phần của "improve" được thực hiện trực tiếp với lời gọi LLM thay vì với mô hình phần thưởng chưng cất của sở thích con người.

Chúng tôi đo lường hiệu suất tổng thể của Search Agent bằng khả năng trả lời các câu hỏi tổng hợp đa dạng đã được xác minh thủ công là không thể trả lời trực tiếp bằng công cụ tìm kiếm (tập dữ liệu Bamboogle (Press et al., 2023) và một tập dữ liệu tiếp theo mà chúng tôi tự xây dựng, BamTwoogle). Mặc dù cả hai tập dữ liệu đều nhỏ, chúng có đủ sức mạnh thống kê để nắm bắt các hiệu ứng mà chúng tôi quan tâm nghiên cứu. Ví dụ, Hình 1 cung cấp cái nhìn tổng quan cấp cao về hiệu suất agent trên Bamboogle cho các kích thước mô hình khác nhau và thể hiện sự gia tăng với nhiều lần lặp ReST hơn (tất cả dữ liệu tổng hợp của chúng tôi được tạo ra với mô hình lớn, vì vậy đối với các mô hình nhỏ hơn, đó là hiệu suất của quá trình chưng cất).

Để tóm tắt, những đóng góp của chúng tôi như sau:
• Chúng tôi xây dựng một hương vị của agent ReAct với tự phê bình cho tác vụ trả lời câu hỏi dài.
• Chúng tôi định nghĩa một chỉ số đánh giá proxy cho agent dựa trên các tập dữ liệu Bamboogle và BamTwoogle, với sự nhấn mạnh mạnh mẽ vào auto-eval.
• Chúng tôi chứng minh rằng hiệu suất của agent có thể được cải thiện hiệu quả thông qua tinh chỉnh lặp kiểu Rest trên các dấu vết lập luận của nó.
• Hơn nữa, chúng tôi thực hiện điều này hoàn toàn từ phản hồi AI từng bước mà không sử dụng dữ liệu huấn luyện được gán nhãn bởi con người.
• Cuối cùng, chúng tôi chỉ ra rằng dữ liệu tổng hợp được tạo ra như một phần của quá trình lặp này có thể được sử dụng để chưng cất agent thành các mô hình nhỏ hơn một hoặc hai bậc độ lớn với hiệu suất tương đương với agent giáo viên được huấn luyện trước.

2 BỐI CẢNH: SEARCH AGENT

Phần này mô tả Search Agent, một hương vị của agent ReAct (Yao et al., 2022) với Reflexion (Shinn et al., 2023). Nó sử dụng tìm kiếm web như một công cụ để tạo ra câu trả lời dài, có thể quy kết rõ ràng cho các câu hỏi mở đa dạng tìm kiếm kiến thức. Luồng của agent tiến hành như sau (Hình 2):

1. Agent nhận một câu hỏi và bắt đầu thực hiện vòng lặp tìm kiếm:
• Agent quyết định liệu nó có cần thông tin bổ sung để trả lời câu hỏi không.
• Nếu "có", nó gọi công cụ tìm kiếm, tóm tắt các đoạn trích nhận được, và quay lại bước quyết định.
• Nếu "không", nó kết thúc vòng lặp tìm kiếm.

2. Dựa trên thông tin thu thập được như một phần của vòng lặp tìm kiếm, agent tạo ra nỗ lực đầu tiên (bản thảo) của câu trả lời.

3. Sau đó nó thực hiện hai lời gọi tự sửa đổi bổ sung trước khi tạo ra câu trả lời cuối cùng:
• Một để xác minh rằng câu trả lời có liên quan đến câu hỏi ban đầu,
• Và một khác để kiểm tra rằng câu trả lời có căn cứ trong các đoạn trích được truy xuất.

[Hình 2: Sơ đồ trạng thái của luồng Search Agent. Mỗi hình dạng màu xanh tương ứng với một lời gọi LLM duy nhất và định nghĩa một loại bước lập luận riêng biệt.]

3 PHƯƠNG PHÁP

3.1 PROMPTING

Trước tiên chúng tôi định nghĩa luồng được gợi ý cho Search Agent bằng cách xây dựng thủ công các gợi ý few-shot cho mỗi trong năm bước lập luận từ Hình 2. Đáng chú ý, chúng tôi chọn định dạng gợi ý của mình như mã Python (xem Phụ lục cho các gợi ý của các bước khác nhau, Listings 1 - 6).

Quyết định sử dụng phương pháp "mã như gợi ý" được thúc đẩy bởi các quan sát sau:
• Thường có nhu cầu phân tích đầu ra của LLM để tích hợp với các hệ thống và công cụ khác, điều này dễ dàng hơn nhiều nếu đầu vào và đầu ra của mô hình có cấu trúc tốt.
• Đồng thời, mã kết hợp độc đáo một khía cạnh có cấu trúc (từ khóa và cú pháp) với một khía cạnh ngôn ngữ tự nhiên (bình luận và đặt tên mô tả).
• Hơn nữa, LLMs có khả năng cả đọc và viết mã.

Để tóm tắt, vì mã có cấu trúc tự nhiên và dễ phân tích, nó có thể phục vụ như một phương tiện tuyệt vời để giao tiếp với mô hình. Theo đó, trong paradigm "mã như gợi ý", LLM được kỳ vọng hiểu mã trong đầu vào và tiếp tục nó như Python hợp lệ trong đầu ra, điều này hóa ra là những tác vụ đầy thử thách cho các mô hình nhỏ hơn. Trong số các mô hình (được huấn luyện trước) mà chúng tôi đã thử, chỉ có PaLM 2-L (Anil et al., 2023) có thể thực hiện nó một cách nhất quán tốt. Do đó, đây là mô hình mà chúng tôi sử dụng để tạo ra các quỹ đạo lập luận từ gợi ý few-shot.

3.2 CHI TIẾT TRIỂN KHAI

Để chạy Search Agent, chúng tôi sử dụng các mô hình "cơ sở" PaLM 2 với các kích thước khác nhau (XS, S và L), cả được huấn luyện trước và được tinh chỉnh. Chúng tôi thường tạo ra nhiều mẫu (với T = 0.5, xem Phần 4.2 cho chi tiết về việc chọn nhiệt độ) cho mỗi bước và sau đó chọn mẫu có độ phức tạp thấp nhất (4 mẫu cho L và S, 16 cho XS) để tiếp tục quỹ đạo về phía trước. Như một công cụ tìm kiếm, chúng tôi sử dụng API Q&A nội bộ của Google trả về các đoạn trích top-k, và chúng tôi thường yêu cầu 3 đoạn trích hàng đầu cho mỗi truy vấn từ nó. Chúng tôi tiếp tục giới hạn số lượng tìm kiếm tổng cộng tối đa 10 cho mỗi quỹ đạo agent duy nhất (thông qua trường REMAINING SEARCHES trong gợi ý). Search Agent bảo tồn trạng thái quỹ đạo bằng cách lưu trữ tất cả các hành động đã thực hiện cho đến nay trong trường PAST ACTIONS.

3.3 DỮ LIỆU ĐẦU VÀO

Chúng tôi sử dụng bốn tập dữ liệu sau để cung cấp câu hỏi ban đầu cho các quỹ đạo Search Agent:

• HotpotQA (Yang et al., 2018), một tập dữ liệu QA lập luận đa hop, nơi hệ thống phải lập luận với thông tin được lấy từ nhiều hơn một tài liệu để đạt được câu trả lời.

• Eli5 (Fan et al., 2019), một tập dữ liệu cho trả lời câu hỏi dài (LFQA), một tác vụ đòi hỏi câu trả lời chi tiết và sâu sắc cho các câu hỏi mở. Tập dữ liệu được xây dựng từ diễn đàn Reddit "Explain Like I'm Five" (ELI5), r/explainlikeimfive.

• Eli5-askH (Blagojevic, 2022), tương tự như trên, nhưng được xây dựng từ diễn đàn Reddit r/askhistorians, một subreddit nơi người dùng có thể đặt câu hỏi hoặc bắt đầu thảo luận về lịch sử.

• Eli5-askS (Blagojevic, 2022), như trên, nhưng từ subreddit r/askscience ("đặt câu hỏi khoa học, nhận câu trả lời khoa học").

Chúng tôi chọn ngẫu nhiên 500 câu hỏi từ phần huấn luyện của mỗi tập dữ liệu và cuối cùng có tổng cộng 2000 câu hỏi đa dạng, đầy thử thách. Chúng tôi không sử dụng bất kỳ thông tin nào khác từ những tập dữ liệu này, như nhãn (ví dụ, chúng tôi không thực hiện bất kỳ lọc nào bằng cách khớp câu trả lời đúng từ HotpotQA, và chúng tôi không sử dụng phần validation của những tập dữ liệu này để điều chỉnh siêu tham số hoặc đánh giá hiệu suất).

3.4 TINH CHỈNH

Chúng tôi đơn giản chia mỗi quỹ đạo Search Agent hoàn thành thành các bước lập luận và xây dựng một hỗn hợp tinh chỉnh với những bước đó. Chúng tôi sử dụng tinh chỉnh đầy đủ cho tất cả các thí nghiệm. Do chi phí tinh chỉnh tăng mạnh đối với các mô hình lớn hơn, chúng tôi thực hiện càng nhiều thí nghiệm càng tốt với mô hình XS.

3.5 MÔ HÌNH "PHẦN THƯỞNG" XẾP HẠNG

Như đã đề cập trước đó, chúng tôi tạo ra nhiều mẫu cho mỗi bước lập luận trong quỹ đạo của agent và thường chọn mẫu giảm thiểu độ phức tạp để tiếp tục quỹ đạo về phía trước hoặc để xây dựng hỗn hợp tinh chỉnh. Đồng thời, chúng ta có thể làm tốt hơn thế bằng cách sử dụng một cách tinh vi hơn để chọn mẫu tốt nhất. Vì mục đích này, chúng tôi sử dụng PaLM 2-L được điều chỉnh theo hướng dẫn và gợi ý nó với đầu vào mô hình, nhiều đầu ra được lấy mẫu, và hướng dẫn về cách xếp hạng chúng (gợi ý có sẵn trong Phụ lục, Listing 8). Sau đó chúng ta có thể sử dụng mẫu được xếp hạng cao nhất để tinh chỉnh thay vì mẫu mặc định được chọn dựa trên giá trị độ phức tạp.

Trong phần này, phương pháp của chúng tôi khác với ReST, sử dụng lọc dựa trên ngưỡng với mô hình phần thưởng (RM) được huấn luyện trên dữ liệu sở thích con người. Những gì chúng tôi làm gần gũi hơn với RAFT (Dong et al., 2023), nơi mô hình phần thưởng xếp hạng các phản hồi được lấy mẫu để chọn tập con điểm cao cho tinh chỉnh mô hình, và các xếp hạng RM quan trọng hơn nhiều so với điểm số tuyệt đối. Đáng chú ý, chúng tôi chủ yếu thực hiện việc chọn lựa dựa trên LLM off-policy, tức là bằng cách cập nhật hành động hiện tại được sử dụng để tinh chỉnh. Các rollout quỹ đạo on-policy sử dụng độ phức tạp.

3.6 TỰ CẢI THIỆN LẶP

Bây giờ chúng ta có tất cả các mảnh ghép cho thuật toán tự cải thiện:

• Bắt đầu với một mô hình có khả năng thực hiện tác vụ Search Agent ở một mức độ nhất định, ví dụ, với mô hình PaLM 2-L được gợi ý. Thu thập các quỹ đạo lập luận từ mô hình này dựa trên bộ 2000 câu hỏi ban đầu của chúng tôi (về cơ bản là giai đoạn "grow" của ReST, với sự khác biệt là chúng tôi giữ cố định bộ câu hỏi ban đầu).

• Chuyển đổi các quỹ đạo thành hỗn hợp tinh chỉnh. Áp dụng xếp hạng lại với RM trong quá trình chuyển đổi (điều này tương đương với giai đoạn "improve" của ReST, mặc dù chúng tôi chỉ thực hiện một lần lặp của "improve").

• Tinh chỉnh mô hình mới (cùng kích thước) trên hỗn hợp này và xác minh rằng nó hoạt động tốt hơn mô hình ban đầu (chúng tôi sẽ thảo luận về cách thực hiện điều này trong phần tiếp theo). Lặp lại quá trình, bắt đầu với mô hình mới, tốt hơn này.

Cuối cùng, chúng ta cũng có thể huấn luyện các mô hình nhỏ hơn trên dữ liệu tinh chỉnh từ các lần lặp khác nhau của tự cải thiện, điều này sẽ tự nhiên mang lại cho chúng ta một thuật toán tự chưng cất.

4 ĐÁNH GIÁ

4.1 BAMBOOGLE

Phương tiện đánh giá chính của chúng tôi là tập dữ liệu Bamboogle (Press et al., 2023). Đây là một tập dữ liệu bán đối kháng của các câu hỏi 2-hop (tổng cộng 125) được chọn để không thể trả lời bằng tìm kiếm Google trực tiếp, nhưng nơi cả hai phần bằng chứng cần thiết đều có thể được tìm thấy trong Wikipedia. Khi hiệu suất của Search Agent trên Bamboogle được cải thiện, chúng ta có thể giả định rằng nó thường trở nên tốt hơn trong việc sử dụng tìm kiếm như một công cụ.

Do tính chất mở của các câu trả lời được tạo ra bởi Search Agent (Hình 3), chúng ta không thể sử dụng khớp chính xác một cách đáng tin cậy như một chỉ số. Thay vào đó, chúng tôi hoặc đánh giá tính đúng đắn một cách thủ công hoặc chạy auto-eval với một lời gọi riêng biệt đến mô hình "cơ sở" PaLM 2-L.

4.2 AUTO-EVAL

Hình 3: Một câu hỏi Bamboogle với một ví dụ về câu trả lời dài của mô hình

Mặc dù các đánh giá con người nghiêm ngặt được ưa thích, chúng tốn thời gian. Chúng cũng không mở rộng tốt (thực hiện một đánh giá con người dễ hơn nhiều so với thực hiện năm), điều này dẫn đến phương sai cao của những đánh giá như vậy trong trường hợp của chúng tôi: các quỹ đạo của agent là ngẫu nhiên (như một lời nhắc nhở, chúng tôi sử dụng nhiệt độ khác không khi lấy mẫu các bước lập luận), nhưng chúng ta không thể dễ dàng giảm phương sai bằng cách tăng số lần lặp lại cho mỗi câu hỏi với đánh giá con người.

Chúng tôi giải quyết cả hai vấn đề này bằng cách giới thiệu auto-eval dựa trên LLM (gợi ý auto-eval đầy đủ có sẵn trong Phụ lục, Listing 7) và bằng cách xác minh sự liên kết của nó với xếp hạng con người cho các quỹ đạo Bamboogle mà trước đó chúng tôi đã thực hiện đánh giá con người. Chúng tôi so sánh auto-eval với đánh giá con người trên một tập đa dạng các agent và thấy nó có tương quan cao với điểm đánh giá con người. Cụ thể, tương quan Pearson là 0.98 với p = 6.6×10−8 và tương quan Spearman là 0.83 với p = 0.0015. Do việc chạy auto-eval rẻ hơn nhiều, bây giờ chúng ta có thể sử dụng một số lượng lớn lần lặp lại để giảm phương sai. Chúng tôi thường tổng hợp auto-eval qua mười lần lặp lại (tức là bằng cách tạo ra mười quỹ đạo khác nhau cho mỗi câu hỏi Bamboogle).

Trước hết và quan trọng nhất, chúng tôi sử dụng Bamboogle auto-eval để ước tính hiệu suất mô hình cuối cùng mà còn để trả lời các câu hỏi khác nhau mà người ta thường sử dụng tập validation cho:

• Nhiệt độ lấy mẫu tối ưu cho agent là gì? (T = 0.5)
• Chúng ta nên chọn checkpoint nào cho các kích thước mô hình khác nhau? (bước 9K cho XS, 5K cho S, 3.5K cho L)
• Chúng ta có nên tiến hành với một lần lặp khác của tự cải thiện không?
• Tác động hiệu suất của việc sử dụng một số quỹ đạo cho mỗi câu hỏi trên mô hình được tinh chỉnh là gì?
• Chúng ta có nên sử dụng tự kiểm tra không? Chúng có giúp ích hay gây hại cho kết quả không? (giúp ích một chút, xem Hình 4)

Quan trọng, chúng tôi không bao giờ sử dụng Bamboogle như một tập huấn luyện, vì chúng tôi không điều chỉnh gợi ý của mình trên Bamboogle cũng như không sử dụng câu hỏi từ nó để tạo ra các quỹ đạo tinh chỉnh.

4.3 BAMTWOOGLE

Hình 4: Tự cải thiện và tự chưng cất của Agent, có và không có các bước tự phê bình. Bamboogle auto-eval, độ chính xác trung bình và độ lệch chuẩn qua mười lần chạy, (%)

Do kích thước nhỏ của Bamboogle và việc sử dụng nó như một analog của tập validation cho Search Agent, có nguy cơ overfitting. Để bảo vệ chống lại khả năng như vậy, chúng tôi giới thiệu một tập dữ liệu mới, BamTwoogle, để phục vụ như một tập test. Chúng tôi sử dụng BamTwoogle độc quyền để đo lường hiệu suất cuối cùng của các mô hình.

Tập dữ liệu BamTwoogle được viết để trở thành một sequel bổ sung, hơi khó khăn hơn so với Bamboogle. Nó cũng giải quyết một số thiếu sót của Bamboogle mà chúng tôi phát hiện khi thực hiện đánh giá con người. Cụ thể, chúng tôi đảm bảo rằng tất cả các câu hỏi đều yêu cầu 2+ bước để trả lời. Do những thay đổi trong thuật toán tìm kiếm, điều này không còn đúng với tất cả câu hỏi Bamboogle, và có thể giải quyết một số trong số chúng chỉ với một lần tìm kiếm duy nhất.

Giống như Bamboogle, BamTwoogle là một tập nhỏ (tổng cộng 100 câu hỏi), được tạo thủ công các câu hỏi tìm kiếm thông tin. Các chủ đề và định dạng câu hỏi khác nhau, nhưng nói chung, BamTwoogle tuân thủ các hướng dẫn sau:

Câu hỏi
• Đa số câu hỏi yêu cầu hai lần tìm kiếm hoặc bước lập luận (như Bamboogle), nhưng một số trong số chúng cần 3 hoặc 4
• Phải được kiểm tra thủ công để đảm bảo câu trả lời không xuất hiện trên trang đầu tiên của kết quả tìm kiếm Google

Câu trả lời mong đợi
• Không nên mơ hồ
• Không nên dễ thay đổi theo thời gian, do cách diễn đạt câu hỏi hoặc do bản chất của câu trả lời
• Nên tính đến nhiều phiên bản của tên riêng, v.v., khi thích hợp
• Nên ưu tiên Wikipedia như nguồn sự thật cho các sự kiện (ưu tiên được đưa ra cho các chủ đề/bài viết không được đánh dấu vì không đầy đủ, thiếu nguồn, v.v.)

5 THÍ NGHIỆM

5.1 PILOT

Bảng 1: Tự cải thiện và tự chưng cất của Agent, Bamboogle auto-eval, độ chính xác trung bình và độ lệch chuẩn qua 10 lần chạy, (%)

Bên cạnh thiết lập tự cải thiện chính, được mô tả trong Phần 3, chúng tôi cũng tạo ra dữ liệu "pilot" đơn giản hơn với 500 quỹ đạo, nơi các câu hỏi ban đầu được chọn chỉ từ các tập dữ liệu HotpotQA và Eli5 (tức là nhỏ hơn và không có Eli5-askH hoặc Eli5-askS). Chúng tôi sử dụng các hành động tốt nhất mặc định (dựa trên độ phức tạp tối thiểu, không có xếp hạng lại RM) để xây dựng hỗn hợp tinh chỉnh của pilot. Hơn nữa, chúng tôi xem xét thủ công dữ liệu tinh chỉnh và lọc ra khoảng 30% các ví dụ "xấu" theo một cách nào đó: một truy vấn không hữu ích, suy nghĩ trống rỗng, tóm tắt thiếu thông tin quan trọng, v.v. Dữ liệu pilot này phục vụ như một baseline tinh chỉnh.

5.2 TỰ CẢI THIỆN VÀ TỰ CHƯNG CẤT

Kết quả chính được trình bày trong Hình 1 và Bảng 1. Như đã thảo luận trước đó, mỗi ô trong bảng hiển thị trung bình (với std tương ứng) của auto-eval qua 10 lần chạy Bamboogle cho mô hình cụ thể.

Bảng 2: Đánh giá con người, Bamboogle và BamTwoogle, acc (%)

Chúng tôi bắt đầu với mô hình PaLM 2-L được huấn luyện trước (được gợi ý) và sử dụng nó để tạo ra cả 500 quỹ đạo pilot và (độc lập) 2000 quỹ đạo cho lần lặp thứ 1 ("1st gen") của tự cải thiện. Sau đó chúng tôi tinh chỉnh các mô hình PaLM 2-L, PaLM 2-S, và PaLM 2-XS trên các hỗn hợp kết quả.

Tiếp theo, chúng tôi sử dụng mô hình PaLM 2-L được tinh chỉnh trên dữ liệu 1st gen để tạo ra các quỹ đạo cho lần lặp thứ 2 ("2nd gen") của tự cải thiện. Chúng tôi xây dựng dữ liệu 2nd gen với 8000 quỹ đạo (sử dụng cùng 2000 câu hỏi ban đầu, mỗi câu được lặp lại bốn lần; tương tự như giai đoạn "grow" trong ReST). Như trước đây, chúng tôi tinh chỉnh mỗi trong ba mô hình trên hỗn hợp 2nd gen mới.

Như một xác minh cuối cùng, chúng tôi cũng thực hiện đánh giá con người qua một lần chạy Bamboogle và BamTwoogle duy nhất cho mỗi mô hình 2nd gen (Bảng 2), cũng như mô hình PaLM 2-L được huấn luyện trước ban đầu để so sánh.

5.3 NGHIÊN CỨU LOẠI BỎ

Tác động của lọc con người là gì? Đáng ngạc nhiên, chúng tôi thấy rằng tinh chỉnh trên dữ liệu được lọc dẫn đến giảm hiệu suất nhỏ (2.5%) so với hỗn hợp pilot không được lọc (Bảng 3, cột pilot). Chúng tôi giả thuyết rằng điều này xảy ra do sự kết hợp của 2 yếu tố:

• Kích thước giảm của hỗn hợp được lọc làm cho mô hình khó học định dạng thích hợp của gợi ý hơn,
• Việc lọc của chúng tôi chỉ ảnh hưởng đến ví dụ "xấu" ngay lập tức, không phải toàn bộ quỹ đạo; bước "xấu" thường được bảo tồn trong các ví dụ tinh chỉnh khác như một phần của trường PAST ACTIONS.

Chúng ta có nên sử dụng nhiều quỹ đạo cho mỗi câu hỏi không? Hóa ra, việc sử dụng hai quỹ đạo cho mỗi câu hỏi thay vì 1 (tăng 2.2%) trong hỗn hợp tinh chỉnh có ích, nhưng nhiều hơn thế không cải thiện hiệu suất đáng kể (Bảng 3, cột 2nd gen).

Bảng 3: Tác động của chất lượng và kích thước dữ liệu huấn luyện cho các mô hình PaLM 2-XS được tinh chỉnh

Dữ liệu nhiều hơn so với dữ liệu tốt hơn. Từ cùng Bảng 3, chúng ta có thể kết luận rằng chất lượng của dữ liệu (ví dụ, tăng 9%, khi đi từ 1st gen đến 2nd gen (1x) trong khi giữ kích thước dữ liệu gần như giống nhau) quan trọng hơn số lượng của nó. Đáng chú ý, dữ liệu tốt hơn cũng giảm phương sai của các quỹ đạo đánh giá.

Tác động của tự phê bình. Thiết lập đa bước của agent cho phép chúng ta dễ dàng đo lường tác động của các bước tự phê bình đối với hiệu suất tổng thể của agent. Để làm điều này, chúng tôi đơn giản lấy tất cả các quỹ đạo Bamboogle được sử dụng cho Bảng 1 và chạy auto-eval trên chúng, nhưng qua bước "Answer Generation", thay vì qua "Final Answer", như chúng tôi thường làm (Hình 2). Như thấy từ Hình 4, tự phê bình dẫn đến một sự gia tăng tích cực nhỏ nhưng có thể đo lường được (khoảng 0.5-1.0% cho hầu hết các mô hình). Các số chi tiết được cung cấp trong Bảng 4 trong Phụ lục.

6 THẢO LUẬN

Giám sát Quá trình. Quan trọng là nhấn mạnh lại rằng chúng tôi không sử dụng nhãn từ dữ liệu huấn luyện như một tín hiệu trong quá trình thu thập quỹ đạo. Điều này có thể thực hiện được do việc kết hợp phương pháp dựa trên quá trình (tức là định nghĩa agent như một máy trạng thái) với khám phá nhiệt độ cao, phản hồi AI (mô hình "phần thưởng" zero-shot được sử dụng để xếp hạng lại hành động), và tinh chỉnh theo trạng thái qua các quỹ đạo hoàn thành. Nói cách khác, trong thiết lập này, mô hình có thể học được điều gì đó hữu ích ngay cả từ các trạng thái cuối cùng dẫn đến câu trả lời cuối cùng sai. Quan trọng hơn, nó học cách xử lý các câu hỏi mở không có câu trả lời đúng duy nhất được định nghĩa rõ ràng ngay từ đầu, tương tự như nhiều câu hỏi từ Eli5.

Auto-Eval. Một số thuộc tính được liệt kê ở trên, như lấy mẫu không tham lam và tạo câu trả lời dài đặc biệt, mang theo những thách thức bổ sung về mặt đánh giá agent. Chúng ta cần phải đo lường chất lượng của câu trả lời cuối cùng dài cho một quỹ đạo cụ thể và tính đến tính ngẫu nhiên giữa các quỹ đạo agent khác nhau cho cùng một đầu vào. Điều này, một mặt, nâng cao giá trị của việc có auto-eval mạnh mẽ được liên kết với người đánh giá con người và, mặt khác, tăng chi phí tính toán đáng kể do nhu cầu chạy quỹ đạo agent nhiều lần, cũng như việc sử dụng mô hình PaLM 2-L cho auto-eval.

Tự Phê bình. Mặc dù những chi phí tính toán đó, nhiều lợi ích đến từ việc có auto-eval đáng tin cậy về mặt đo lường tác động của các siêu tham số khác nhau của agent. Ví dụ, hãy xem xét các bước tự phê bình. Với sự hỗ trợ của auto-eval, chúng tôi có thể thiết lập rằng chúng có tác động tích cực nhỏ nhưng đáng kể đối với hiệu suất tổng thể của thiết lập lập luận đa bước của chúng tôi. Điều này tương phản với các quan sát gần đây (Huang et al., 2023) rằng việc thêm tự phê bình làm tổn hại hiệu suất trong thiết lập CoT dựa trên kết quả. Đồng thời, chúng ta cũng có thể nhận thấy rằng tác động tích cực của tự phê bình phụ thuộc vào kích thước mô hình (lớn hơn đối với các mô hình lớn hơn) nhưng dường như không bị ảnh hưởng bởi quá trình tự cải thiện.

Hạn chế và Hướng Tương lai. Mặc dù công trình này đặt nền móng cho việc tự cải thiện của các agent lập luận, nó có nhiều hạn chế do ràng buộc về tính toán và thời gian: gợi ý được xây dựng thủ công, đánh giá nhỏ, một tập hạn chế các mô hình, và chỉ một công cụ duy nhất, để kể tên một số. Công việc tương lai có thể khám phá liệu cùng một thuật toán tự cải thiện có áp dụng được cho các thiết lập nhiều công cụ và, đặc biệt, liệu khả năng xử lý các công cụ chưa thấy có thể được cải thiện theo cách như vậy không. Nếu điều sau tương tự như tự phê bình và không cải thiện dưới huấn luyện lặp kiểu ReST, những thay đổi nào được yêu cầu để cho phép tự cải thiện cho cả hai?

Một câu hỏi mở khác là điểm bão hòa. Chúng ta có thể thực hiện bao nhiều lần lặp bổ sung của tự cải thiện qua lần thứ 2 mà vẫn dẫn đến những lợi ích không tầm thường? Sự bão hòa trông như thế nào đối với các mô hình nhỏ hơn? Liệu tất cả chúng cuối cùng sẽ hội tụ về cùng một hiệu suất, hay các mô hình nhỏ hơn sẽ luôn bị giới hạn bởi hiệu suất của mô hình lớn được gợi ý ban đầu?

7 CÔNG TRÌNH LIÊN QUAN

Theo WebGPT (Nakano et al., 2021), chúng tôi đang giải quyết tác vụ trả lời câu hỏi dài (Krishna et al., 2021), trong đó agent ngôn ngữ sử dụng tìm kiếm web như một công cụ để tạo ra câu trả lời cuối cùng với các tham chiếu rõ ràng cho các đoạn văn được truy xuất. Trong khi WebGPT tập trung vào học bắt chước và RL từ một số lượng lớn các minh họa của con người, công trình của chúng tôi nhằm mục đích giảm thiểu sự tham gia của con người. Các minh họa có nhãn duy nhất mà chúng tôi sử dụng như một phần của huấn luyện là các mẫu few-shot trong gợi ý cho các bước lập luận của agent (xem Phụ lục, A.1).

Thiết lập các agent ngôn ngữ với gợi ý few-shot được thiết kế thủ công là thực hành phổ biến nhất (Press et al. (2023); Yao et al. (2022); Shinn et al. (2023)), nhưng có một số ngoại lệ. Ví dụ, DSP (Khattab et al., 2023a) điều chỉnh các minh họa few-shot cho gợi ý tự động, tận dụng một lượng nhất định ví dụ huấn luyện có nhãn cho mục đích tối ưu hóa, và có thể tiếp tục tinh chỉnh các thành phần cụ thể của agent.

Khác với prompting, tinh chỉnh của agent được thực hiện ít thường xuyên hơn (Nakano et al. (2021); Yao et al. (2022); Chen et al. (2023)). Gần nhất với thiết lập tinh chỉnh của chúng tôi có lẽ là FireAct (Chen et al., 2023), với sự khác biệt chính là chúng tôi không sử dụng nhãn của con người để huấn luyện hoặc lọc dữ liệu. Thay vào đó, chúng tôi đang xây dựng dữ liệu tổng hợp với tự cải thiện từ phản hồi AI.

Một số bài báo liên quan cho tự cải thiện bao gồm STAR (Zelikman et al., 2022), ReST (Gulcehre et al., 2023), ReSTEM (Singh et al., 2023), và RAFT (Dong et al., 2023). Khác với STAR và ReSTEM, chúng tôi không sử dụng tính đúng đắn của câu trả lời như một tín hiệu. Và, khác với ReST và RAFT, chúng tôi không có mô hình phần thưởng thích hợp được huấn luyện trên sở thích con người. Hơn nữa, tất cả 4 bài báo này nhắm đến các hệ thống dựa trên kết quả, trong khi chúng tôi tập trung vào một hệ thống dựa trên quá trình.

8 KẾT LUẬN

Công trình này chứng minh rằng phương pháp giống ReST với phản hồi AI có thể được áp dụng hiệu quả cho agent LLM lập luận đa bước. Chúng tôi chỉ ra rằng đó là một cách tương đối đơn giản và hiệu quả để xây dựng lặp đi lặp lại dữ liệu tổng hợp chất lượng cao cho việc tự cải thiện agent. Hơn nữa, dữ liệu chất lượng ngày càng cao này có thể đồng thời được sử dụng để chưng cất agent đa bước thành các mô hình nhỏ hơn vài magnitude trong khi bảo tồn hầu hết hiệu suất từ mô hình giáo viên lớn.

TÀI LIỆU THAM KHẢO

[Phần tài liệu tham khảo được giữ nguyên như bản gốc tiếng Anh vì đây là định dạng chuẩn học thuật]

A PHỤ LỤC

A.1 GỢI Ý

Các bước lập luận. Tất cả gợi ý lập luận của chúng tôi đều là n-shot (với n > 1), mặc dù chúng tôi chỉ hiển thị các đoạn, giới hạn ở mẫu đầu tiên:

• Gợi ý bước quyết định được hiển thị trong Listing 1 (gợi ý đầy đủ là 9-shot).
• Gợi ý tóm tắt được trình bày trong Listing 2 (gợi ý đầy đủ là 6-shot).
• Gợi ý tạo câu trả lời là 5-shot, Listing 3 hiển thị một đoạn 1-shot.
• Gợi ý cho tự kiểm tra liên quan được trình bày trong Listing 5 (gợi ý đầy đủ là 6-shot).
• Cuối cùng, gợi ý cho tự kiểm tra căn cứ được hiển thị trong Listing 6 (gợi ý đầy đủ là 5-shot).
• Cả hai tự kiểm tra đều sử dụng tiền tố chung từ Listing 4.

Auto-eval. Gợi ý 5-shot đầy đủ cho auto-eval được trình bày trong Listing 7.

Mô hình "Phần thưởng" Xếp hạng. Gợi ý đầy đủ của mô hình phần thưởng được hiển thị trong Listing 8. Chú ý rằng đây là gợi ý 0-shot duy nhất và không sử dụng phương pháp "mã như gợi ý", vì nó được dự định để sử dụng với mô hình được điều chỉnh theo hướng dẫn.

A.2 DỮ LIỆU BỔ SUNG

Bảng 4: Tự cải thiện và tự chưng cất của Agent, Bamboogle auto-eval, độ chính xác trung bình và độ lệch chuẩn qua 10 lần chạy, (%). Đánh giá trước các bước tự phê bình

[Các Listing 1-8 được giữ nguyên như bản gốc vì đây là mã nguồn]
