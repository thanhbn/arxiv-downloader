# 2311.05584.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reasoning/2311.05584.pdf
# Kích thước tệp: 992290 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
HỘI THOẠI ĐỊNH HƯỚNG MỤC TIÊU ZERO-SHOT THÔNG QUA RL TRÊN
CÁC CUỘC HỘI THOẠI TƯỞNG TƯỢNG

Joey Hong Sergey Levine Anca Dragan
UC Berkeley
{joey hong,sergey.levine,anca }@berkeley.edu

TÓM TẮT
Các mô hình ngôn ngữ lớn (LLM) đã nổi lên như những giải pháp mạnh mẽ và tổng quát cho
nhiều tác vụ ngôn ngữ tự nhiên. Tuy nhiên, nhiều ứng dụng quan trọng nhất của việc tạo sinh
ngôn ngữ là tương tác, nơi một agent phải nói chuyện với một người để đạt được kết quả mong
muốn. Ví dụ, một giáo viên có thể cố gắng hiểu mức độ hiểu biết hiện tại của học sinh để điều
chỉnh hướng dẫn cho phù hợp, và một đại lý du lịch có thể đặt câu hỏi cho khách hàng để hiểu
sở thích của họ nhằm đề xuất các hoạt động mà họ có thể thích. Các LLM được huấn luyện với
fine-tuning có giám sát hoặc RL "một bước", như với RLHF tiêu chuẩn, có thể gặp khó khăn với
các tác vụ đòi hỏi hành vi định hướng mục tiêu như vậy, vì chúng không được huấn luyện để tối
ưu hóa cho các kết quả hội thoại tổng thể sau nhiều lượt tương tác. Trong công trình này, chúng
tôi khám phá một phương pháp mới để điều chỉnh LLM với RL cho hội thoại định hướng mục
tiêu như vậy. Hiểu biết chính của chúng tôi là, mặc dù LLM có thể không giải quyết hiệu quả
các tác vụ hội thoại định hướng mục tiêu ngay từ đầu, chúng có thể cung cấp dữ liệu hữu ích
để giải quyết các tác vụ đó bằng cách mô phỏng các hành vi dưới tối ưu nhưng giống con người.
Với mô tả văn bản về một tác vụ hội thoại định hướng mục tiêu, chúng tôi tận dụng LLM để lấy
mẫu các rollout tổng hợp đa dạng của các tương tác giả định trong miền giữa con người với con
người. Thuật toán của chúng tôi sau đó sử dụng tập dữ liệu này với học tăng cường ngoại tuyến để
huấn luyện một agent hội thoại tương tác có thể tối ưu hóa các mục tiêu định hướng mục tiêu qua
nhiều lượt. Trên thực tế, LLM tạo ra các ví dụ về những tương tác có thể, và RL sau đó xử lý
những ví dụ này để học cách thực hiện những tương tác tối ưu hơn. Thực nghiệm cho thấy phương
pháp đề xuất của chúng tôi đạt được hiệu suất tiên tiến trong các tác vụ hội thoại định hướng mục
tiêu khác nhau bao gồm giảng dạy và khám phá sở thích.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) đã trở nên rất hiệu quả trong việc thực hiện nhiều tác vụ ngôn
ngữ tự nhiên trong thế giới thực, bao gồm trả lời câu hỏi mở (Pyatkin et al., 2022), tóm tắt (Paulus
et al., 2017; Wu & Hu, 2018; Böhm et al., 2019), tạo mã (Chen et al., 2021b; Rozière et al., 2023;
Zhong & Wang, 2023), và giải quyết vấn đề tổng quát (Wei et al., 2023).

Hình 1: Ví dụ minh họa về cách các LLM hiện tại hoạt động khi
được nhắc nhở để hành động như đại lý du lịch so với cách các
đại lý du lịch hiệu quả nên hoạt động.

Trong khi LLM xuất sắc trong việc tạo ra các phản hồi hấp dẫn và chính xác cho các truy vấn cá nhân, khả năng tham gia vào cuộc hội thoại định hướng mục tiêu của chúng vẫn còn hạn chế. Chúng có thể bắt chước dòng chảy của một cuộc hội thoại, nhưng thường không nhằm mục đích hoàn thành một mục tiêu thông qua việc đối thoại. Ví dụ, như được hiển thị trong Hình 1, chúng ta có thể nhắc nhở LLM hành động như một đại lý du lịch, và nó sẽ tạo ra những phản hồi thực tế mà con người có thể đánh giá là hữu ích. Nhưng nó sẽ không cố ý cố gắng tối đa hóa cơ hội lập kế hoạch một lịch trình mong muốn cho con người.

--- TRANG 2 ---
Trong thực tế, điều này biểu hiện như việc thiếu câu hỏi làm rõ, thiếu dòng chảy hội thoại định hướng mục tiêu, và nhìn chung là các phản hồi dài dòng và không được cá nhân hóa.

Sự khác biệt giữa một agent chỉ đơn giản bắt chước dòng chảy của cuộc hội thoại và một agent theo đuổi mục tiêu hội thoại trở nên rõ ràng hơn khi chúng ta xem xét cách sự không chắc chắn ảnh hưởng đến cuộc hội thoại. Dù bạn là người dùng cuối đang yêu cầu agent hướng dẫn bạn về một khái niệm AI mới, hay lập kế hoạch cho một kỳ nghỉ sắp tới, bạn có thông tin đặc quyền mà agent không biết, nhưng điều đó quan trọng để agent thực hiện tác vụ tốt; ví dụ, nền tảng hiểu biết AI hiện tại của bạn quan trọng khi học một khái niệm mới, và sở thích du lịch của bạn quan trọng khi bạn lập kế hoạch kỳ nghỉ. Như được minh họa trong Hình 1, một agent định hướng mục tiêu sẽ thu thập thông tin mà nó cần để thành công, có thể bằng cách đặt câu hỏi làm rõ (ví dụ, bạn có phải là người năng động không?) và đề xuất các giải pháp một phần để nhận phản hồi (ví dụ, đi biển nghe như thế nào?). Tuy nhiên, các LLM ngày nay phần lớn thất bại ở điều này, và có khả năng cố gắng đưa ra một phỏng đoán hữu ích nhưng kém thông tin ngay lập tức hơn là đặt những câu hỏi thích hợp. Và như chúng tôi sẽ chỉ ra trong các thí nghiệm, ngay cả khi được nhắc nhở cẩn thận để thu thập thông tin, chúng tuân thủ nhưng tạo ra những câu hỏi dài dòng và áp đảo không giỏi trong việc có được thông tin đúng.

Về nguyên tắc, học tăng cường (RL) có thể cung cấp một công cụ rất mạnh mẽ để thu hẹp khoảng cách này: LLM được huấn luyện với RL để đạt được mục tiêu hội thoại (chẳng hạn như tối đa hóa xác suất người dùng sẽ chấp nhận lịch trình được lập kế hoạch) có thể thực hiện các bước định hướng mục tiêu, đặt câu hỏi làm rõ, khơi gợi sở thích, rất rõ ràng và ngắn gọn trong phản hồi, và thậm chí có thể xây dựng mối quan hệ với người dùng. Nhưng RL cần dữ liệu, hoặc dưới dạng tương tác trực tuyến với một bộ mô phỏng con người, hoặc tương tác ngoại tuyến giữa con người với con người. Dữ liệu trực tuyến có thể khó khăn về mặt tính toán để có được, và dữ liệu ngoại tuyến phải được tuyển chọn cẩn thận để tối ưu hóa các thuộc tính mong muốn như độ bao phủ và đa dạng (Fu et al., 2020b; Gulcehre et al., 2020; Kumar et al., 2022).

Ý tưởng chính của chúng tôi là chúng ta có thể tạo ra các agent hội thoại định hướng mục tiêu zero-shot bằng cách khai thác những gì LLM giỏi — bắt chước các cuộc hội thoại thực tế đa dạng; và khai thác những gì RL giỏi — tối ưu hóa các mục tiêu nhiều bước. Chúng tôi đề xuất sử dụng LLM để "tưởng tượng" một loạt các cuộc hội thoại cụ thể tác vụ có thể thường thực tế, nhưng nơi LLM không giải quyết tối ưu tác vụ. Trên thực tế, LLM có thể tưởng tượng những gì con người có thể làm, nhưng không phải những gì một agent tối ưu nên làm. Các cuộc hội thoại sau đó được tạo ra dựa trên các trạng thái ẩn được lấy mẫu. Chúng tôi huấn luyện một agent để tham gia vào cuộc hội thoại định hướng mục tiêu bằng cách huấn luyện RL ngoại tuyến trên tập dữ liệu kết quả.

Đóng góp chính của chúng tôi là một thuật toán RL zero-shot hiệu quả tối ưu hóa cho các tác vụ hội thoại định hướng mục tiêu. Thay vì trực tiếp sử dụng LLM được huấn luyện trước như các agent tối ưu, phương pháp của chúng tôi nhằm tận dụng sức mạnh của chúng trong việc bắt chước các cuộc hội thoại đa dạng, giống con người, nhưng dưới tối ưu để tạo dữ liệu, sau đó có thể được cung cấp cho thuật toán RL để thực sự khám phá ra những hành vi tối ưu hơn. Chúng tôi đề xuất một hệ thống mới được gọi là động cơ tưởng tượng (IE) tạo ra một tập dữ liệu các cuộc hội thoại đa dạng, liên quan đến tác vụ và có tính hướng dẫn để được sử dụng để huấn luyện các agent phụ thuộc. Chúng tôi đánh giá phương pháp của mình trên các tác vụ liên quan đến giảng dạy một khái niệm mới, thuyết phục và khám phá sở thích. Kết quả thí nghiệm của chúng tôi bao gồm một nghiên cứu người dùng so sánh các agent được huấn luyện với phương pháp của chúng tôi với các LLM tiên tiến được nhắc nhở, cho thấy phương pháp của chúng tôi có thể đạt được kết quả tốt hơn đáng kể trong các cuộc hội thoại tương tác ngay cả khi sử dụng các mô hình nhỏ hơn nhiều bậc so với baseline dựa trên prompt.

2 CÔNG TRÌNH LIÊN QUAN

Mô hình ngôn ngữ. Các mô hình ngôn ngữ, đặc biệt là LLM, đã thể hiện khả năng ấn tượng trong tạo văn bản (Ghazvininejad et al., 2017; Li et al., 2017; Holtzman et al., 2018; Radford et al., 2019; Yang & Klein, 2021), dịch thuật (Gu et al., 2017), trả lời câu hỏi (Pyatkin et al., 2022), tóm tắt (Paulus et al., 2017; Wu & Hu, 2018; Böhm et al., 2019), và tạo mã (Chen et al., 2021b; Zhong & Wang, 2023). Tuy nhiên, thành công ở hầu hết các tác vụ này phần lớn được thực hiện bởi học có giám sát, và không đòi hỏi lý luận qua nhiều bước tương tác để tối ưu hóa một mục tiêu dài hạn. LLM đã được fine-tune thông qua học có giám sát để tham gia hội thoại với người dùng con người với một số thành công (He et al., 2018; Shuster et al., 2022b;a), nhưng chủ yếu để tạo ra phản hồi thực tế chứ không phải để hoàn thành một mục tiêu cơ bản.

RL cho mô hình ngôn ngữ. Nhiều LLM hiện tại tận dụng fine-tuning học tăng cường (RL), nơi một mô hình phần thưởng được học từ phản hồi trực tiếp từ các chuyên gia con người (Ziegler et al., 2020; Stiennon et al., 2020; Wu et al., 2021; Nakano et al., 2022; Bai et al., 2022a; Christiano et al., 2023) hoặc gián tiếp từ một hệ thống AI được chế tạo thủ công (Bai et al., 2022b), và sau đó được sử dụng để fine-tune LLM thông qua một mục tiêu RL. Trong khi fine-tuning chủ yếu được thực hiện thông qua RL trực tuyến, các phương pháp gần đây đề xuất điều chỉnh LLM từ dữ liệu ngoại tuyến (Rafailov et al., 2023; Gulcehre et al., 2023). Bằng cách làm như vậy, LLM có thể tuân thủ một cách trung thực các hướng dẫn của con người, hoặc prompts, và do đó có thể hoạt động như những bộ giải quyết vấn đề tổng quát bằng kỹ thuật prompt engineering (Ouyang et al., 2022). Mặc dù hiệu quả, một nhược điểm rõ ràng của các phương pháp fine-tuning RL là chúng chỉ xem xét các mục tiêu bandit. Cụ thể, trong fine-tuning RL, LLM được huấn luyện để tối đa hóa mô hình phần thưởng đã học trong một phản hồi một bước, và không phải trong suốt một cuộc hội thoại nhiều bước. Do đó, nếu phản hồi tốt nhất cho một truy vấn không được biết do thông tin tiềm ẩn, chẳng hạn như ý định hoặc sở thích, của người dùng, các LLM truyền thống sẽ chỉ cung cấp phản hồi "phỏng đoán" tốt nhất có thể trong một bước, và không cố gắng thu thập thông tin bổ sung để phản hồi tối ưu hơn. Đáng chú ý, Glaese et al. (2022) đề xuất học một agent tìm kiếm thông tin, nhưng lại xem xét một mục tiêu một bước dựa trên việc tối đa hóa tính hữu ích, và không xem xét cũng như đánh giá trên các tác vụ nơi việc thu thập thông tin được sử dụng để hoàn thành một mục tiêu dài hạn; phương pháp này cũng dựa vào việc người đánh giá con người có thể xác định các hành động tìm kiếm thông tin hữu ích.

Hội thoại định hướng mục tiêu. Đã có nhiều công trình trước đây về việc học các mô hình để hoàn thành các tác vụ thông qua cuộc hội thoại ngoài việc tối đa hóa tính thông tin hoặc tính con người. Hội thoại định hướng mục tiêu, hoặc tương tự hội thoại định hướng tác vụ, có thể được công thức hóa như một MDP mà từ đó các agent có thể được huấn luyện bằng RL. Các phương pháp RL trực tuyến để tối ưu hóa các agent hội thoại thường yêu cầu một bộ mô phỏng hành vi con người, thường được chế tạo thủ công hoặc học như một mô hình cố định (Carta et al., 2023; He et al., 2018; Gašić et al., 2011). Hơn nữa, chúng liên quan đến việc thu thập liên tục các mẫu mới, điều này gây ra chi phí tính toán lớn trong các tác vụ nơi con người thể hiện hành vi phức tạp và tinh tế, và thường dễ bị "hack" phần thưởng (Skalse et al., 2022). Thay vào đó, các phương pháp RL ngoại tuyến cũng đã được xem xét chỉ yêu cầu một tập dữ liệu tĩnh của các cuộc hội thoại (Jaques et al., 2019; Jang et al., 2022; Verma et al., 2022; Snell et al., 2023). Đáng chú ý, Verma et al. (2022) đề xuất một thuật toán RL ngoại tuyến để giải quyết hội thoại định hướng mục tiêu dựa trên đàm phán sử dụng một tập dữ liệu các cuộc hội thoại giữa những người nói con người. Tuy nhiên, để RL ngoại tuyến cải thiện so với học có giám sát, tập dữ liệu phải được tuyển chọn cẩn thận để tối ưu hóa các thuộc tính mong muốn như độ bao phủ và đa dạng (Fu et al., 2020b; Gulcehre et al., 2020; Kumar et al., 2022), điều này có thể hạn chế tính thực tiễn của nó. Một cách trực giao, các tập dữ liệu benchmark hội thoại đã được tạo ra nhằm đánh giá khả năng của các agent trong việc hoàn thành các tác vụ khác nhau như trả lời câu hỏi (Budzianowski et al., 2020), dịch vụ khách hàng (Chen et al., 2021a), và đàm phán (He et al., 2018). Tuy nhiên, nhiều tập dữ liệu như vậy dành cho các tác vụ không đòi hỏi việc cá nhân hóa phản hồi của agent cho từng con người. Trong bài báo này, chúng tôi xem xét các tác vụ hội thoại định hướng mục tiêu nơi con người hành xử khác nhau do các yếu tố tiềm ẩn, và các agent phải thu thập thông tin và cá nhân hóa cho từng con người. Do độ phức tạp bổ sung này, việc tuyển chọn một tập dữ liệu con người-con người với các hành vi con người đủ đa dạng có thể khó khăn một cách cấm đoán.

Chưng cất kiến thức. Động cơ tưởng tượng đề xuất của chúng tôi có thể được coi là một trường hợp của chưng cất kiến thức (Hinton et al., 2015), nơi kiến thức từ một mô hình lớn (trong trường hợp của chúng tôi, LLM) được sử dụng để huấn luyện một mô hình nhỏ hơn. Gần đây, điều này đã trở nên phổ biến với LLM hoạt động như mô hình giáo viên, và tổng hợp tạo ra các ví dụ huấn luyện mới cho mô hình nhỏ hơn (Taori et al., 2023; Chiang et al., 2023; Kim & Rush, 2016). Trong khi phương pháp của chúng tôi tương tự về nguyên tắc, tất cả các phương pháp trước đây chỉ xem xét các mục tiêu học có giám sát phụ thuộc. Theo hiểu biết của chúng tôi, chúng tôi là những người đầu tiên thực hiện tạo sinh hội thoại tổng hợp cho RL.

3 KIẾN THỨC CƠ BẢN

Quá trình quyết định Markov. Để công thức hóa hội thoại như một vấn đề ra quyết định, chúng tôi sử dụng chủ nghĩa hình thức của quá trình quyết định Markov (MDP), được cho bởi một tuple M = (S,A, P, r, ρ, γ), nơi S là không gian trạng thái, A là không gian hành động, P là hàm chuyển tiếp, r là hàm phần thưởng, ρ là phân phối trạng thái ban đầu, và γ là yếu tố chiết khấu. Khi hành động a ∈ A được thực thi tại trạng thái s ∈ S, trạng thái tiếp theo được lấy mẫu s′ ∼ P(·|s, a), và agent nhận phần thưởng r với giá trị trung bình r(s, a).

Hội thoại định hướng mục tiêu như MDP. Hội thoại định hướng mục tiêu có thể được xem như một MDP, nơi các trạng thái là chuỗi các token từ một từ vựng hữu hạn V (Ramamurthy et al., 2023). Tất cả các token mà agent ban đầu quan sát được sử dụng như trạng thái ban đầu của chúng ta, s₀ = (x₀, . . . , xₘ), nơi xᵢ ∈ V, ∀i ∈ [m]. Tại bước thời gian t, một hành động aₜ ∈ V là một token nào đó trong từ vựng. Miễn là aₜ không phải là một token kết thúc chuỗi đặc biệt <EOS>, hàm chuyển tiếp một cách xác định sẽ thêm aₜ vào trạng thái sₜ để hình thành sₜ₊₁. Nếu không, agent quan sát (có thể ngẫu nhiên) các phản hồi từ đối tác hội thoại của họ oₜ = (y₀, . . . , yₙ), cũng bao gồm các token trong từ vựng; sau đó, hàm chuyển tiếp thêm cả aₜ và các phản hồi oₜ vào trạng thái sₜ. Điều này tiếp tục cho đến bước thời gian cuối cùng T nơi chúng ta có được một trạng thái sₜ và agent nhận một phần thưởng xác định r(sₜ).

Trong nhiều tác vụ thế giới thực đòi hỏi hội thoại với con người, con người thể hiện một loạt các hành vi khác nhau. Ví dụ, trong một tác vụ đại lý du lịch, con người sẽ phản hồi khác nhau với agent theo sở thích hoạt động, ngân sách và các yếu tố cá nhân khác của riêng họ. Những yếu tố như vậy thường tiềm ẩn, nhưng ảnh hưởng đến cách một agent tối ưu nên phản hồi. Thay vì MDP thông thường, những tác vụ này thay vào đó có thể được công thức hóa như MDP tham số ẩn (Doshi-Velez & Konidaris, 2013), được cho bởi một tuple M = (S,A,Z, P, r, ρ, γ), nơi Z cũng tham số hóa các hàm chuyển tiếp và phần thưởng. Trong thực tế, các giải pháp cho MDP tham số ẩn không cần mô hình hóa Z một cách rõ ràng, và thay vào đó sử dụng một mô hình chuỗi (tức là, một mô hình ngôn ngữ tiêu chuẩn) để xử lý việc suy luận ngầm nó từ lịch sử các quan sát. Tuy nhiên, chúng tôi xem Z như chủ nghĩa hình thức hữu ích để hiểu tại sao việc thu thập thông tin lại quan trọng trong các agent hội thoại hiệu quả.

Học tăng cường. Mục tiêu của học tăng cường (RL) là học một chính sách π tối đa hóa kỳ vọng return chiết khấu ∑∞ₜ₌₀ γᵗrₜ trong một MDP. Hàm Q Qᵗ(s, a) cho một chính sách π đại diện cho phần thưởng dài hạn chiết khấu đạt được bằng cách thực thi a trong trạng thái s và sau đó tuân theo chính sách π sau đó. Qᵗ thỏa mãn đệ quy Bellman:

Qᵗ(s, a) = BᵗQᵗ(s, a) = r(s, a) + γEs′∼P(·|s,a),a′∼π(·|s′)[Q(s′, a′)]

Hàm giá trị Vᵗ là kỳ vọng của hàm Q Vᵗ(s) = Ea∼π(·|s)[Qᵗ(s, a)]. Kỳ vọng return chiết khấu có thể được biểu diễn như J(π) = Es₀∼ρ[Vᵗ(s₀)]. Trong RL ngoại tuyến, chúng ta được cung cấp một tập dữ liệu D = {(sᵢ, aᵢ, s′ᵢ, rᵢ)}ᵢ∈[N] có kích thước |D| = N, được tạo ra bởi một chính sách hành vi không biết πβ (có thể tương ứng với một hỗn hợp của nhiều chính sách). Thiết lập RL ngoại tuyến đặc biệt hữu ích khi tương tác trực tuyến với thế giới thực tốn kém hoặc không khả dụng.

4 HỌC TĂNG CƯỜNG TRÊN CÁC CUỘC HỘI THOẠI TƯỞNG TƯỢNG

Trong bài báo này, chúng tôi trình bày một phương pháp huấn luyện zero-shot các agent trong một tác vụ hội thoại định hướng mục tiêu. Thay vì các phương pháp RL ngoại tuyến truyền thống đòi hỏi một tập dữ liệu được tuyển chọn D của dữ liệu con người-con người, đầu vào duy nhất mà hệ thống của chúng tôi yêu cầu là một mô tả tác vụ D. Tính mới chính của phương pháp đề xuất của chúng tôi là một động cơ tưởng tượng (IE) cho phép tạo ra một tập dữ liệu đa dạng D̂ của các cuộc hội thoại liên quan đến tác vụ cho bất kỳ mô tả tác vụ D nào. Sau đó, một khi tập dữ liệu hội thoại D̂ được tạo ra, chúng tôi tối ưu hóa cho một agent π̂ thông qua RL ngoại tuyến trên các cuộc hội thoại tưởng tượng. Chúng tôi đi qua cả hai phần một cách chi tiết.

--- TRANG 5 ---
4.1 ĐỘNG CƠ TƯỞNG TƯỢNG: TỔNG HỢP CÁC CUỘC HỘI THOẠI ĐA DẠNG LIÊN QUAN ĐẾN TÁC VỤ

Chúng tôi giả định có quyền truy cập vào một LLM PLLM(· |p) có thể được sử dụng để tạo ra một phản hồi cho bất kỳ prompt p nào. IE bao gồm ba bước, được phác thảo trong Hình 1 và chúng tôi mô tả dưới đây. Chúng tôi cũng cung cấp các ví dụ rõ ràng về quá trình (bao gồm các prompt được sử dụng) cho hai tác vụ khác nhau trong Phụ lục A.

Bước lý luận: tổng hợp các persona liên quan đến tác vụ. Nhớ lại rằng các cuộc hội thoại định hướng mục tiêu có thể được công thức hóa như MDP tham số ẩn với không gian ẩn Z, nơi mỗi con người có một z ∈ Z khác nhau ảnh hưởng đến cách họ hành xử, và cách một agent nên phản hồi tối ưu. Không có quyền truy cập vào dữ liệu trước đó, chúng ta sẽ một cách ngây thơ dựa vào việc có kiến thức cụ thể tác vụ về Z. Tuy nhiên, hiểu biết của chúng tôi là LLM chứa một lĩnh vực kiến thức rộng hơn nhiều so với bất kỳ cá nhân con người nào, và do đó có thể cung cấp kiến thức cụ thể tác vụ khi con người không thể. Do đó, chúng tôi đề xuất truy vấn PLLM(· |fr(D)), nơi fr(D) là một prompt lý luận sử dụng mô tả tác vụ D; prompt yêu cầu LLM xuất các mô tả văn bản của các persona φ(z) cho z ∈ Z. Những mô tả φ(z) này cuối cùng có thể được sử dụng như "persona prompts" để tạo ra các hành vi khác nhau khi mô phỏng con người trong các cuộc hội thoại tổng hợp (Serapio-García et al., 2023; Park et al., 2023).

Bước tưởng tượng: tạo ra các cuộc hội thoại tổng hợp. Mục tiêu trong bước này là tưởng tượng các cuộc hội thoại τ giữa một agent (có thể dưới tối ưu) và một con người. Chính thức, điều này liên quan đến việc tạo ra các rollout tổng hợp trong MDP tham số ẩn cơ bản. Lưu ý rằng trong thế giới thực, các mẫu từ cả hàm chuyển tiếp P và chính sách hành vi πβ của MDP chỉ đơn giản là phản hồi của con người. Do đó, việc tổng hợp thành công các quỹ đạo có thể được giảm xuống thành việc mô phỏng hội thoại con người-con người.

Để hoàn thành điều này, chúng tôi tận dụng LLM để tạo ra các cuộc hội thoại tổng hợp giữa một agent và con người, nơi chúng tôi điều kiện việc tạo sinh dựa trên cách con người hành xử, và phần thưởng mà agent đạt được. Điều này được thực hiện như sau. Đầu tiên, chúng tôi lấy mẫu φ(z) cho một persona z ∈ Z nào đó mà chúng tôi có được trong bước lý luận, và cũng lấy mẫu r ∈ {0,1} chỉ ra liệu agent thất bại hay thành công trong cuộc hội thoại được tạo ra. Giả định về phần thưởng nhị phân chỉ để không phụ thuộc vào tác vụ, và các phần thưởng biểu cảm hơn có thể được xem xét nếu chúng tồn tại cho một tác vụ cụ thể. Một cuộc hội thoại có điều kiện có thể được lấy mẫu τ ∼ PLLM(· |fi(D, φ(z), r)) nơi fi(D, φ(z), r) là một prompt tưởng tượng yêu cầu LLM xuất một cuộc hội thoại giữa hai con người có liên quan đến tác vụ, và nơi con người hành xử theo z và agent cuối cùng đạt được phần thưởng r.

Bước phê bình: tinh chỉnh các cuộc hội thoại. Mặc dù các cuộc hội thoại tổng hợp từ bước tưởng tượng hầu hết là những tương tác hợp lý, con người trong cuộc hội thoại đôi khi tạo ra những phản hồi có thể không thực tế. Ví dụ, con người trong các cuộc hội thoại thường tiết lộ các persona cơ bản của họ, mà không cần agent đặt bất kỳ câu hỏi nào, hoặc thậm chí xây dựng mối quan hệ chung với họ. Vì việc suy luận persona của con người là một kỹ năng quan trọng mà chúng tôi muốn các agent học phụ thuộc có được, chúng tôi muốn các chiến lược thu thập thông tin được phản ánh trong các cuộc hội thoại tưởng tượng, ngay cả khi chúng không được triển khai một cách tối ưu hoặc chiến lược (như sẽ được yêu cầu cho agent tối ưu).

Để khắc phục điều này, chúng tôi đề xuất chỉnh sửa các cuộc hội thoại tưởng tượng dựa trên một tập hợp các tiêu chí về những gì cấu thành các cuộc hội thoại sư phạm cho việc học phụ thuộc của chúng tôi. Các tiêu chí của chúng tôi quan tâm đến tác vụ cụ thể nhưng thường bao gồm các nguyên tắc sau: (1) "con người" không nên tiết lộ hành vi tiềm ẩn của họ ngay lập tức, mà chỉ làm cho nó rõ ràng dần dần thông qua cuộc hội thoại qua lại với agent; (2) tình cảm của con người ở cuối cuộc hội thoại nên phản ánh chính xác phần thưởng mà agent đạt được. Các tiêu chí của chúng tôi có thể được sử dụng tương tự như một hiến pháp để cải thiện chất lượng của các phản hồi được tạo ra (Bai et al., 2022b). Chính thức, chúng tôi lấy mẫu một cuộc hội thoại được chỉnh sửa τ′ ∼ PLLM(· |fc(D, τ, c)) từ cùng LLM nơi prompt phê bình fc(D, τ, c) bao gồm cuộc hội thoại gốc và tiêu chí c. Việc lặp lại các bước tưởng tượng và phê bình cho phép chúng tôi tuyển chọn một tập dữ liệu D̂ của các cuộc hội thoại đa dạng và có tính hướng dẫn.

4.2 TỐI ƯU HÓA RL TRÊN TẬP DỮ LIỆU TƯỞNG TƯỢNG

Trong khi động cơ tưởng tượng có thể tạo ra các cuộc hội thoại hợp lý, điều này tự nó không tạo ra các agent hiệu quả – tức là, chúng tôi sử dụng LLM để tổng hợp các kịch bản hợp lý, bao gồm các chiến lược mà một agent có thể thực hiện, nhưng không nhất thiết là những gì một agent tối ưu nên làm. Để xác định chiến lược tối ưu mà một agent thực sự nên thực hiện để đạt được kết quả mong muốn, chúng tôi yêu cầu RL nhiều bước để tối ưu hóa một agent nhằm tối đa hóa xác suất thành công trong tác vụ mong muốn. Do đó, câu hỏi chính mà chúng tôi nhằm trả lời trong phần này như sau: Làm thế nào chúng ta sử dụng một tập dữ liệu tĩnh của các cuộc hội thoại tổng hợp để huấn luyện một agent RL? Giải pháp của chúng tôi liên quan đến việc chạy RL dựa trên giá trị ngoại tuyến để học một chính sách hoàn toàn từ tập dữ liệu tổng hợp, mà không có bất kỳ mẫu on-policy nào.

--- TRANG 6 ---
Trước khi chạy RL ngoại tuyến, chúng tôi cần xử lý sau tập dữ liệu các cuộc hội thoại tổng hợp thành các ví dụ huấn luyện RL. Nhớ lại rằng chúng tôi đã xây dựng một tập dữ liệu D̂ = {(τᵢ, rᵢ)}ᵢ∈[N] của N cuộc hội thoại tưởng tượng, nơi mỗi cuộc hội thoại τᵢ là một chuỗi các token trong từ vựng V tạo thành các phát ngôn giữa một agent mô phỏng và con người mô phỏng. Đối với mỗi cuộc hội thoại τᵢ, chúng tôi cô lập tất cả các token a bởi agent, sau đó tạo ra (s, a, s′, r) nơi trạng thái s bao gồm tất cả các token trước a, trạng thái tiếp theo s′ bao gồm tất cả các token trước token tiếp theo a′ bởi agent, và r = rᵢ chỉ khi s′ = τᵢ là cuộc hội thoại đầy đủ. Sử dụng thủ tục này, chúng tôi xây dựng một tập dữ liệu D̂′ = {(sᵢ, aᵢ, s′ᵢ, rᵢ)}ᵢ∈[N′].

Sau đó, chúng tôi chạy RL dựa trên giá trị để học một chính sách π̂. Cụ thể, chúng tôi học các hàm Q̂ và V̂ ước tính hàm Q tối ưu và hàm giá trị, tương ứng, và sau đó sử dụng những hàm này để trích xuất một chính sách π̂. Các hàm có thể được học bằng cách sử dụng đệ quy Bellman với các giá trị mục tiêu được bootstrap:

Q̂ = arg min Q E(s,a,s′,r)∼D̂′ [r + γV̂(s′) − Q(s, a)]², V̂ = arg min V Es∼D̂′ [max a′ Q̂(s, a′) − V(s)]².

Khi π̂ là một mô hình ngôn ngữ, chúng tôi sử dụng những hàm này kết hợp với mô hình ngôn ngữ cơ sở π̂LM để trích xuất chính sách (Snell et al., 2022), thông qua π̂(a|s) ∝ πLM(a|s)eβ(Q̂(s,a)−V̂(s)).

Nếu chính sách được học hoàn toàn từ dữ liệu ngoại tuyến, việc huấn luyện một cách ngây thơ từ RL dựa trên giá trị có thể gặp phải dịch chuyển phân phối (Fujimoto et al., 2018; Kumar et al., 2019), mà các thuật toán RL ngoại tuyến khắc phục bằng cách đảm bảo rằng các hàm Q̂, V̂ đã học là bi quan (Kumar et al., 2020; Kostrikov et al., 2021). Lưu ý rằng động cơ tưởng tượng của chúng tôi không phụ thuộc vào thuật toán RL; trong các thí nghiệm của chúng tôi, chúng tôi sử dụng Implicit Language Q-Learning (ILQL) (Snell et al., 2022) vì hiệu suất của nó trên các tác vụ ngôn ngữ tự nhiên.

5 THÍ NGHIỆM

Giả thuyết. Các thí nghiệm của chúng tôi đánh giá quy trình huấn luyện agent hội thoại zero-shot đề xuất của chúng tôi trên hai tác vụ hội thoại định hướng mục tiêu. Các tác vụ yêu cầu agent thực hiện thu thập thông tin để cá nhân hóa phản hồi của họ cho người dùng, điều này đòi hỏi các chiến lược hội thoại định hướng mục tiêu như đặt câu hỏi làm rõ, hoặc xây dựng mối quan hệ với người dùng để hiểu rõ hơn ý định của họ. Chúng tôi nhằm trả lời các câu hỏi nghiên cứu sau:

1. Việc tận dụng LLM trong động cơ tưởng tượng của chúng tôi để tạo ra dữ liệu tổng hợp để huấn luyện các agent phụ thuộc có được ưa thích hơn việc sử dụng chúng một cách ngây thơ để hành xử như các agent không?

2. RL ngoại tuyến trên dữ liệu tưởng tượng có tốt hơn việc chỉ sử dụng học bắt chước trên cùng dữ liệu không?

Câu hỏi nghiên cứu đầu tiên nhắm vào giả thuyết chính của chúng tôi, rằng LLM nên được tận dụng để tạo ra dữ liệu thay vì để giải quyết các tác vụ định hướng mục tiêu. Câu hỏi thứ hai nhắm vào việc liệu các chi tiết cụ thể về cách chúng tôi huấn luyện trên dữ liệu tưởng tượng có quan trọng không. Chúng tôi giả thuyết rằng trong khi trường hợp trung bình cả BC và RL đều hoạt động tương tự, sự tương phản giữa các agent RL và BC đáng chú ý trong các tình huống không được đại diện tốt trong các cuộc hội thoại tưởng tượng. Đặc biệt, các agent RL nên mạnh mẽ hơn khi con người hành xử theo những cách không được đại diện trong bất kỳ cuộc hội thoại nào trong tập dữ liệu tưởng tượng của chúng tôi, mà có thể trong các nối tiếp của nhiều cuộc hội thoại. Điều này bởi vì các agent RL được thể hiện để tạo ra các chiến lược mới từ các thành phần trong tập dữ liệu thông qua một hiện tượng được gọi là "trajectory stitching" (Fu et al., 2020a; Levine et al., 2020).

Các phương pháp. Để trả lời cả hai câu hỏi, chúng tôi xem xét các phương pháp sau:

GPT. Phương pháp này nhắc nhở GPT-3.5 (OpenAI, 2022), là một LLM mạnh mẽ được chỉ ra trong công trình trước đây có thể giải quyết hiệu quả nhiều tác vụ ngôn ngữ tự nhiên (Ouyang et al., 2022), để trực tiếp hành xử như agent. Prompt bao gồm cả mô tả tác vụ, cũng như hiểu biết rằng agent kết quả cần thu thập thông tin về người dùng con người để phản hồi tối ưu cho họ. Đây là cách sử dụng truyền thống của LLM để giải quyết các tác vụ hội thoại.

IE+BC (ablation). Phiên bản này của phương pháp chúng tôi huấn luyện một agent trên tập dữ liệu tưởng tượng được tạo ra bởi động cơ tưởng tượng đề xuất của chúng tôi, nhưng thông qua một mục tiêu behavioral cloning (BC), nơi agent một cách thẳng thắn bắt chước dữ liệu tổng hợp. Điều này tương đương với supervised fine-tuning trên tập dữ liệu tưởng tượng. Đây là một ablation của phương pháp đề xuất của chúng tôi nhằm điều tra câu hỏi nghiên cứu thứ hai.

--- TRANG 7 ---
IE+FBC (ablation). Thay vì BC trên toàn bộ tập dữ liệu tưởng tượng, phương pháp này huấn luyện agent sử dụng filtered BC thay vào đó, bắt chước chỉ các quỹ đạo thành công trong tập dữ liệu. Đây là một ablation khác của phương pháp đề xuất của chúng tôi.

IE+RL. Đây là phiên bản đầy đủ của phương pháp chúng tôi, huấn luyện agent sử dụng RL ngoại tuyến. Cụ thể, chúng tôi sử dụng ILQL (Snell et al., 2022) như thuật toán RL ngoại tuyến.

Trên các phương pháp sử dụng động cơ tưởng tượng, chúng tôi sử dụng GPT-3.5 (OpenAI, 2022) như LLM trong động cơ tưởng tượng để tạo ra dữ liệu tổng hợp. Tuy nhiên, các agent phụ thuộc của chúng tôi được huấn luyện trên dữ liệu tưởng tượng sử dụng một mô hình GPT-2 decoder-only nhỏ hơn nhiều (Radford et al., 2019). Điều này để cho thấy rằng chúng tôi chỉ cần tận dụng LLM tiên tiến để tạo ra dữ liệu, vì các agent kết quả có thể nhỏ hơn nhiều; điều này làm cho phương pháp của chúng tôi thực tế nếu chi phí tính toán là một cân nhắc quan trọng, hoặc nếu các LLM hiện đại được triển khai chỉ với API suy luận, cả hai đều là những rào cản gặp phải trong các đánh giá thực nghiệm của chúng tôi. Đối với mỗi tác vụ, chúng tôi trì hoãn các chi tiết triển khai của động cơ tưởng tượng (bao gồm các prompt được sử dụng và kết quả) và tối ưu hóa RL (bao gồm các siêu tham số huấn luyện) đến Phụ lục A.

Thiết lập nghiên cứu người dùng. Chúng tôi kiểm tra hai giả thuyết của mình trong một nghiên cứu người dùng hai phần với 12 người dùng. Để kiểm tra liệu việc tận dụng LLM cho động cơ tưởng tượng có tốt hơn việc trực tiếp sử dụng LLM như các agent thông qua prompting, chúng tôi sử dụng thiết kế within-subjects và để người dùng tương tác với GPT và IE+RL (theo thứ tự ngẫu nhiên để tránh hiệu ứng thứ tự) trên hai tác vụ, mà chúng tôi mô tả dưới đây. Sau đó chúng tôi yêu cầu họ đánh giá các agent, cho mỗi tác vụ, theo bốn tiêu chí, trên thang điểm Likert 5 điểm:

(A) Agent hoàn thành tác vụ hiện có tốt như thế nào.
(B) Cuộc hội thoại kết quả thực tế và tự nhiên như thế nào.
(C) Agent hiệu quả như thế nào trong việc đặt câu hỏi liên quan cho thu thập thông tin.
(D) Sự hài lòng tổng thể với agent.

Để cung cấp một đánh giá chi tiết hơn so sánh các agent dựa trên RL và BC của IE, chúng tôi cụ thể phân tích các kịch bản thách thức nơi con người thể hiện các persona đặc biệt bất thường hoặc khó khăn. Những kịch bản như vậy làm trầm trọng thêm những thiếu sót của các phương pháp dựa trên BC, chỉ đơn giản bắt chước dòng chảy hội thoại trong dữ liệu thay vì tối ưu hóa cho phần thưởng tác vụ. Để kiểm tra liệu agent RL có mạnh mẽ hơn khi con người hành xử theo những cách không được phản ánh trong bất kỳ cuộc hội thoại nào trong tập dữ liệu, chúng tôi phân tích dữ liệu được tạo ra, xác định các hành vi không được đại diện (như người dùng mơ hồ hoặc không hài lòng), và bắt chước chúng để tạo ra các cuộc hội thoại với các agent IE+BC, IE+FBC, và IE+ILQL. Chúng tôi làm điều này vì đây là những hành vi ít có khả năng xảy ra một cách tự nhiên thông qua các tương tác tự do. Chúng tôi cho những người dùng xem những cuộc hội thoại này và yêu cầu họ đánh giá các agent như trên. Chúng tôi báo cáo kết quả tóm tắt và các đoạn của các cuộc hội thoại được đánh giá trong bài báo chính, và trì hoãn các cuộc hội thoại được đánh giá đầy đủ đến Phụ lục B.

5.1 MÔ TẢ TÁC VỤ

Chúng tôi xem xét hai vấn đề hội thoại định hướng mục tiêu dựa trên các tác vụ thế giới thực của hướng dẫn cá nhân hóa và khám phá sở thích và đề xuất:

Hướng dẫn. Trong tác vụ này, một con người yêu cầu một agent dạy họ về một số khái niệm mà họ không quen thuộc. Cụ thể, con người sẽ hỏi agent về một trong năm khái niệm trong RL: "behavior cloning", "policy gradient", "actor-critic", "model-based reinforcement learning" và "offline reinforcement learning". Mặc dù tác vụ này tương tự như trả lời câu hỏi tổng quát (Budzianowski et al., 2020), chúng tôi xem xét trường hợp nơi agent cần thêm vào đó điều chỉnh hướng dẫn của họ cho kiến thức nền của con người, tức là, họ quen thuộc với các khái niệm tương tự như thế nào. Do đó, trong tác vụ này, kiến thức nền của con người tạo thành Z của MDP tham số ẩn. Các ví dụ cuộc hội thoại cho tác vụ này được hiển thị trong Hình 3.

Khám phá sở thích. Ở đây, agent phải xây dựng mối quan hệ với con người với mục tiêu khám phá sở thích cơ bản của họ. Chúng tôi cụ thể xem xét một tác vụ đại lý du lịch được ám chỉ trước đó trong bài báo của chúng tôi, nơi agent phải đề xuất các hoạt động cá nhân hóa cho con người từ một tập hợp các hoạt động cố định được cung cấp trong mô tả tác vụ. Chúng tôi có một tập hợp 18 hoạt động được nhóm thành sáu danh mục: khám phá thiên nhiên, bãi biển, sức khỏe, ẩm thực, mua sắm, và trải nghiệm văn hóa. Trái ngược với tác vụ hướng dẫn, không gian của các tham số ẩn Z ảnh hưởng đến hành vi con người phức tạp hơn nhiều. Cụ thể, ngoài việc khám phá sở thích hoạt động của con người, agent cũng phải tìm ra và tính đến các yếu tố bổ sung như tính cách, ngân sách, hoặc động lực gia đình.

--- TRANG 8 ---
Ví dụ, nếu con người có bản chất do dự, agent nên cẩn thận không làm họ choáng ngợp với quá nhiều lựa chọn, mà thay vào đó đưa ra một vài lựa chọn mỗi lần.

5.2 IE CÓ TỐT HƠN PROMPTING KHÔNG?

Chúng tôi đầu tiên nhằm giải quyết định lượng và định tính câu hỏi nghiên cứu đầu tiên: việc tận dụng LLM như bộ tạo dữ liệu cho RL có hiệu quả hơn việc trực tiếp sử dụng chúng như các agent thông qua prompting không?

[THIS IS TABLE: Bảng 1 showing mean and standard error ratings from users interacting with agents across both tasks, with metrics (A)-(D) for GPT Agent vs IE+RL Agent across Instruction and Preference Elicitation tasks]

Đánh giá nghiên cứu người dùng. Đối với mỗi tác vụ được đánh giá, chúng tôi tiến hành một nghiên cứu người dùng nơi người dùng tương tác với agent GPT và IE+RL ba lần mỗi agent, nơi các agent là ẩn danh và thứ tự mà người dùng tương tác với họ là ngẫu nhiên. Sau đó, mỗi người dùng báo cáo đánh giá cho các chỉ số (A) đến (D). Kết quả được hiển thị trong Bảng 1. Trong cả hai tác vụ, agent IE+RL đề xuất của chúng tôi vượt trội hơn agent GPT trên tất cả các chỉ số, đặc biệt về tính tự nhiên của cuộc hội thoại kết quả và sự hài lòng của người dùng.

Tiếp theo, chúng tôi đánh giá định tính các cuộc hội thoại giữa người dùng con người và mỗi agent. Trong Hình 3, chúng tôi hiển thị một đoạn cuộc hội thoại giữa một agent trong tác vụ hướng dẫn và một con người dường như là người thường không quen thuộc với AI và các khái niệm RL. Trong ví dụ này, mặc dù agent GPT thực sự cố gắng thu thập thông tin (vì nó được hướng dẫn rõ ràng để làm như vậy trong prompt), nó cố gắng làm như vậy trong một cuộc khảo sát dài, điều này cồng kềnh và bất tiện cho con người. Ngược lại, agent IE+RL của chúng tôi thu thập thông tin từng bước bằng cách đặt những câu hỏi tự nhiên, dễ trả lời, với mỗi câu hỏi xây dựng một cách thông minh dựa trên câu hỏi trước đó. Agent IE+RL đầu tiên hỏi liệu con người có quen thuộc với AI không, và khi con người phản hồi tiêu cực, đặt một câu hỏi thân thiện tiếp theo để đánh giá họ quen thuộc với công nghệ nói chung như thế nào. Tiếp theo, trong Hình 4, chúng tôi hiển thị các đoạn cuộc hội thoại giữa một agent trong tác vụ khám phá sở thích và một con người dường như do dự và đưa ra những câu trả lời mơ hồ. Agent IE+LQL của chúng tôi thích ứng và đưa ra những câu hỏi hẹp hơn để con người trả lời, trong khi agent GPT tiếp tục đặt những câu hỏi quá dài dòng và không được cá nhân hóa. Các cuộc hội thoại đầy đủ cho cả hai ví dụ có thể được tìm thấy trong Phụ lục B.

Đánh giá mô phỏng quy mô lớn. Ngoài nghiên cứu người dùng trên 12 người dùng con người, chúng tôi cũng tiến hành một đánh giá quy mô lớn hơn của các agent GPT và IE-RL trong mô phỏng. Thay vì ghép nối các agent với con người thực, chúng tôi thay vào đó xem xét những "con người mô phỏng" có phản hồi được tạo ra bởi GPT-3.5 (OpenAI, 2022). Chúng tôi chỉ làm điều này trên tác vụ khám phá sở thích nơi có một thước đo thành công rõ ràng – liệu agent có đề xuất một hoạt động mà con người mô phỏng thích không. Cụ thể, chúng tôi nhắc nhở GPT với một persona được lấy mẫu bao gồm một hoạt động cụ thể mà họ sẽ thích, cũng như một loại tính cách ảnh hưởng đến hành vi hoặc sở thích của họ (ví dụ, thích phiêu lưu, tìm kiếm sang trọng, do dự, v.v.), và yêu cầu nó phản hồi cuộc hội thoại cho đến nay theo cách phù hợp với persona của họ, mà không tiết lộ thẳng thắn nó cho agent. Chúng tôi đo lường liệu mỗi agent có thể đề xuất hoạt động ground-truth có trong prompt của con người mô phỏng trong vòng 15 phát ngôn tổng cộng (bao gồm cả agent và con người mô phỏng).

[THIS IS TABLE: Bảng 2 showing mean results of agents in preference elicitation task interacting with 50 simulated humans, including metrics for tokens per utterance, one-shot success, and final success]

Chúng tôi báo cáo kết quả trong Bảng 2 trên 50 con người mô phỏng, có persona được lấy mẫu đồng đều ngẫu nhiên. Chúng tôi phát hiện rằng vì con người mô phỏng được nhắc nhở với một hoạt động ground-truth, việc các agent khơi gợi sở thích của họ dễ dàng hơn nhiều, vì con người mô phỏng thường đưa ra những phản hồi thẳng thắn về những gì họ thích (trong khi con người thực không chắc chắn về những gì họ thích sẽ đưa ra những phản hồi mơ hồ hơn đòi hỏi việc thăm dò dần dần bởi agent). Do đó, tỷ lệ thành công cuối cùng của cả hai agent đều cao, với agent IE-RL chỉ vượt trội một chút so với agent GPT. Tuy nhiên, định tính, agent GPT sẽ tạo ra những phản hồi cực kỳ dài dòng bao gồm một danh sách kiểm tra các câu hỏi tẻ nhạt để trả lời, hoặc danh sách các hoạt động được đề xuất không đặc biệt được cá nhân hóa cho người dùng. Do đó, chúng tôi xác định hai chỉ số khác chứng minh các tương tác chất lượng thấp hơn bởi agent GPT so với agent IE-RL của chúng tôi. Đầu tiên mà chúng tôi đo lường là số lượng token trung bình trong mỗi phát ngôn của agent. Chúng tôi thấy rằng những phản hồi quá dài dòng bởi agent GPT dẫn đến số lượng token bất thường cao mỗi phát ngôn, trong khi agent IE-RL của chúng tôi tạo ra những phát ngôn ngắn gọn hơn đáng kể với ít token hơn. Ngoài ra, chúng tôi đo lường tỷ lệ thành công one-shot: tỷ lệ phần trăm các lần hoạt động đầu tiên được đề xuất bởi agent là ground-truth. Trong thực tế, một hệ thống đề xuất quá nhiều hoạt động có thể có hại ngay cả khi một trong những đề xuất cuối cùng thành công, vì con người có thể mất kiên nhẫn hoặc cuối cùng không tin tưởng khả năng của hệ thống. Chúng tôi phát hiện rằng agent GPT, thường sử dụng việc đề xuất một loạt các hoạt động, có tỷ lệ thành công zero-shot thấp hơn gần 30% so với agent IE-RL của chúng tôi, agent này thực hiện thu thập thông tin có mục tiêu và có thể thường xuyên hơn nhiều xác định hoạt động đúng để đề xuất trong lần thử đầu tiên.

5.3 RL NGOẠI TUYẾN CÓ TỐT HƠN BC KHÔNG?

Tiếp theo, chúng tôi giải quyết câu hỏi nghiên cứu thứ hai: việc huấn luyện trên dữ liệu tưởng tượng với RL có hiệu quả hơn việc trực tiếp bắt chước nó với học có giám sát không? Nhớ lại rằng chúng tôi cho rằng tối ưu hóa RL vượt trội hơn học bắt chước trong các kịch bản thách thức nơi các chiến lược được phản ánh chính xác trong dữ liệu không đủ. Để có được những ví dụ như vậy, chúng tôi đóng vai như con người thể hiện những hành vi thách thức tiềm năng và tương tác với các agent. Cụ thể, trong tác vụ hướng dẫn, chúng tôi xem xét con người đánh giá quá cao hiểu biết của họ về một khái niệm cụ thể. Bằng cách làm như vậy, hiểu biết của agent về nền tảng kiến thức của con người sẽ không phù hợp với nền tảng thực sự của họ, dẫn đến

--- TRANG 9 ---
người dùng không hiểu giải thích của agent. Trong khi đó, trong tác vụ khám phá sở thích, chúng tôi xem xét người dùng bày tỏ sự không hài lòng với đề xuất ban đầu của agent. Đối với mỗi tác vụ, chúng tôi tạo ra ba tiền tố cuộc hội thoại của các tình huống thách thức, sau đó đánh giá khả năng của các agent IE+BC, IE+FBC, và IE+RL để phục hồi từ chúng. Sau đó, chúng tôi cho mỗi người dùng trong nghiên cứu người dùng xem những cuộc hội thoại như vậy, và yêu cầu người dùng đánh giá khả năng của mỗi agent cho cùng các chỉ số (A) đến (D). Kết quả được báo cáo trong Bảng 3, nơi chúng tôi thấy sự cải thiện rõ ràng của agent IE+RL, đặc biệt trong việc đặt những câu hỏi thu thập thông tin hiệu quả. Trong Hình 5, chúng tôi hiển thị các đoạn tương ứng của các cuộc hội thoại với các agent IE+FBC và IE+RL trong tác vụ hướng dẫn. Ở đây, người dùng bày tỏ sự bối rối với giải thích của agent. Agent IE+FBC quyết định diễn giải lại giải thích trước đó, trong khi agent IE+RL quyết định đặt thêm câu hỏi để hiểu rõ hơn nền tảng của người dùng. Sau đó, trong Hình 6, chúng tôi hiển thị các ví dụ tương ứng trong tác vụ khám phá sở thích. Ở đây, người dùng bày tỏ sự không hài lòng với đề xuất đắt đỏ của agent. Chỉ agent IE+RL quyết định đưa ra các lựa chọn thay thế rẻ hơn, trong khi agent IE+FBC dường như bỏ qua sự không hài lòng của người dùng. Các cuộc hội thoại đầy đủ cho cả hai ví dụ có thể được tìm thấy trong Phụ lục B.

[THIS IS TABLE: Bảng 3 showing ratings comparison between IE+BC, IE+FBC, and IE+RL agents across different metrics for Instruction and Preference Elicitation tasks]

6 THẢO LUẬN

Trong bài báo này, chúng tôi đề xuất một thuật toán đạt được việc thu thập zero-shot các agent hội thoại định hướng mục tiêu. Phương pháp này tận dụng một động cơ tưởng tượng mới, tạo ra một tập dữ liệu hội thoại tổng hợp có liên quan đến tác vụ, thực tế và thể hiện các hành vi đa dạng. Tập dữ liệu tưởng tượng sau đó có thể được sử dụng để huấn luyện các agent hội thoại thông qua tối ưu hóa RL ngoại tuyến. Giả thuyết chính mà công trình của chúng tôi chứng minh là LLM không nên được sử dụng trực tiếp như các agent hội thoại định hướng mục tiêu, mà thay vào đó như các bộ tạo cho hội thoại có thể được sử dụng cho tối ưu hóa phụ thuộc. Chúng tôi chỉ ra, trên nhiều tác vụ hội thoại bao gồm giảng dạy và khám phá sở thích, rằng phương pháp của chúng tôi là cách sử dụng LLM hiệu quả hơn nhiều so với các phương pháp truyền thống nhắc nhở LLM để hành động trực tiếp như các agent.

Nhìn chung, phương pháp của chúng tôi tránh việc tuyển chọn cẩn thận hội thoại con người-con người được sử dụng truyền thống để huấn luyện các agent hội thoại thông qua RL. Tuy nhiên, chúng tôi vẫn yêu cầu sự can thiệp của con người dưới dạng các prompt cụ thể tác vụ. Công trình tương lai có thể nhằm tự động hóa quá trình này hơn nữa, để một agent hội thoại zero-shot có thể được huấn luyện từ bất kỳ mô tả tác vụ nào.

LỜI CẢM ƠN

Chúng tôi cảm ơn các thành viên của RAIL và InterACT tại UC Berkeley cho sự hỗ trợ và đề xuất của họ. Chúng tôi cảm ơn các nhà phê bình ẩn danh cho phản hồi về phiên bản đầu của bài báo này. Công trình này được hỗ trợ một phần bởi Berkeley DeepDrive, Cooperative AI Foundation, Office of Naval Research thông qua N00014-21-1-2838, và NSF thông qua IIS-2246811.

TÀI LIỆU THAM KHẢO

[Tiếp theo là danh sách tài liệu tham khảo dài với nhiều citations khoa học...]

--- TRANG 25 ---
[Phần này chứa thêm nhiều tài liệu tham khảo khoa học được tiếp tục từ trang trước]
