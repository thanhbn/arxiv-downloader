# 2311.04205.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/reasoning/2311.04205.pdf
# File size: 3713121 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preprint
Rephrase and Respond: Let Large Language Models Ask Bet-
ter Questions for Themselves
Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu
Department of Computer Science
University of California Los Angeles
{yihedeng,weightzero,chenzx19,qgu }@cs.ucla.edu
Abstract
Misunderstandings arise not only in interpersonal communication but also
between humans and Large Language Models (LLMs). Such discrepancies
can make LLMs interpret seemingly unambiguous questions in unexpected
ways, yielding incorrect responses. While it is widely acknowledged that
the quality of a prompt, such as a question, significantly impacts the qual-
ity of the response provided by LLMs, a systematic method for crafting
questions that LLMs can better comprehend is still underdeveloped. In
this paper, we present a method named ‘Rephrase and Respond’ (RaR),
which allows LLMs to rephrase and expand questions posed by humans
and provide responses in a single prompt. This approach serves as a simple
yet effective prompting method for improving performance. We also intro-
duce a two-step variant of RaR, where a rephrasing LLM first rephrases
the question and then passes the original and rephrased questions together
to a different responding LLM. This facilitates the effective utilization of
rephrased questions generated by one LLM with another. Our experiments
demonstrate that our methods significantly improve the performance of
different models across a wide range to tasks. We further provide a com-
prehensive discussion with the popular Chain-of-Thought (CoT) methods.
We show that RaR is complementary to CoT and can be combined with
CoT to achieve even better performance. Our work not only contributes to
enhancing LLM performance efficiently and effectively but also sheds light
on a fair evaluation of LLM capabilities.
1 Introduction
Large Language Models (LLMs), such as the GPT series (Radford et al., 2019; Brown et al.,
2020; OpenAI, 2023), have witnessed a surge in popularity due to their profound impact
on various real-world applications, including question answering (Lu et al., 2023), code
generation (Poesia et al., 2022), and conversational agents (Bozkurt, 2023). The wide applica-
bility and efficacy of these models have led to rapidly growing research on understanding
and improving the use of LLMs. Meanwhile, it is widely acknowledged that the quality
of the prompt generated by humans critically influences the response quality of the LLMs,
emphasizing the importance of effective queries that prioritize specificity, detail, and preci-
sion (OpenAI, 2022). The sensitivity to prompt qualities, on the other hand, limits the usage
of LLMs to their full potential and requires a systematic way to improve human’s prompt.
Misunderstandings in interpersonal communications often arise when individuals, shaped
by distinct subjective experiences, interpret the same message differently. In social science,
such phenomena can be attributed to cognitive biases in frames in thought (Druckman, 2001).
A single message, framed in different ways, can lead individuals to different conclusions.
People habitually project their frames onto their received information, and only shift these
frames when incongruence arises. In this work, we posit that LLMs also exhibit their own
frames in thought, and it is not uncommon to observe a disparity between the frames used
by humans and LLMs that caused the LLM’s sensitivity to prompt qualities. Meanwhile,
it can be challenging for humans to assess the clarity of their questions and to align their
1arXiv:2311.04205v2  [cs.CL]  18 Apr 2024

--- PAGE 2 ---
Preprint
frames with those of LLMs. To illustrate this, we first present a motivating example by
investigating a recent work (Allen-Zhu & Li, 2023) in detail.
GPT-4Was Mother Teresa born in an even month?
No, Mother Teresa was born on August 26, 1910. 
August is the 8th month of the year.
Was Ludwig van Beethoven born in an even month?
No, Ludwig van Beethoven was born on December 
17, 1770. Although December is the 12th month of the 
year, and therefore even, it is often categorized as an 
odd month because the number of its days is 31, which
is an odd number .
GPT-4
Was Ed Sheeran born in an even month?
No, Ed Sheeran was not born in an even month. He 
was born on February 17, 1991. Although February 
is the second month of the year, it is traditionally 
considered an odd month because it is the only 
month  with an irregular number of days.
GPT-4
Was Bob Marley born in an even month?
No, Bob Marley was born on February 6, 1945. Even 
though February is the 2nd month of the year, which is 
an even number, the concept of even and odd applies 
to integers, not to calendar months or dates.
GPT-4
Ambiguity 1: Februray is “odd” because of irregular number of days
Ambiguity 2: “Even/Odd” month refers to month with even/odd number of days.
Ambiguity 3: Concept of even or odd cannot be applied to calendar.
Figure 1: Motivating examples of our study. We can observe several ambiguities in the
question for LLMs. An LLM can interpret “even month” as the month with an even number
of days, which diverges from human intention.
Motivating Example. Allen-Zhu & Li (2023) found that LLMs such as GPT-4 may not effi-
ciently reason with their internal knowledge even if they can retrieve information accurately.
As shown in Figure 1, when posed with the query, “Was Mother Teresa born on an even
month?” GPT-4 might mistakenly assert that August is an odd month. Based on this obser-
vation, Allen-Zhu & Li (2023) suggested that GPT-4 instead requires a Chain-of-Thought
process—relying on user-led follow-up questions—to correct its previous wrong answers.
However, we take a step further to investigate the intrinsic reason for LLM’s inefficiency
in answering such questions. As shown in the other responses in Figure 1, when GPT-4
explains its reasoning, the model has several ambiguities toward the questions. For example,
it may consider February as odd due to its irregular number of days and sometimes consider
an even/odd month to be months with an even/odd number of days.
Ambiguity in questions (Cole et al., 2023; Kim et al., 2023a) is a recognized concern in
benchmark datasets. For instance, it has been observed that the NLI datasets such as
MultiNLI (Williams et al., 2018) contain ambiguities, which are challenging even for human
interpreters (Liu et al., 2023). Furthermore, our study uncovers that benchmark datasets
commonly used for LLM evaluation (Wei et al., 2022; bench authors, 2023) possess am-
biguities that are imperceptible to humans but challenging for language models. These
ambiguities cause LLMs to provide mistaken responses to unintended queries.
Original question Self-rephrased question
Was {person} born in an even day?
Was {person} born in an even month?
Was {person} born in an even year?Could you provide more information on whether 
the individual named {person} was born on a day 
that is an even number? This refers to dates such 
as the 2nd, 4th, 6th, 8th, and so on within a given 
month.
Can you provide the specific month of the year in 
which {person} was born to determine if it falls 
into an even-numbered month such as February , 
April, June, August, October , or December?
What is the birth year of {person} and is it an 
even number?
Figure 2: Comparison with the original question (Allen-Zhu & Li, 2023) and one possible
self-rephrased question of GPT-4. We can observe a marked improvement of accuracy in
answering these rephrased questions over the query of 105 celebrities for this task.
Our Approach. In this paper, we highlight an often-overlooked aspect of studies in LLMs:
the disparity between human and LLM thought frames. Our research illustrates that this
disparity significantly impacts the performance of LLMs in the zero-shot setting . To tackle
this problem, we propose to let the LLM rephrase the question and incorporate additional
2

--- PAGE 3 ---
Preprint
details for better answering. We observe that, as opposed to questions asked casually
by human, the automatically rephrased questions tend to enhance semantic clarity and
aid in resolving inherent ambiguity. This self-rephrasing technique leads to significant
improvement in accuracy, as shown in the barplot of Figure 2.
Building upon these insights, we introduce a method named Rephrase and Respond (RaR),
which prompts the LLM to rearticulate the given question and respond in a single prompt.
Our empirical results across diverse reasoning tasks show the effectiveness of the approach.
Additionally, we leverage RaR to facilitate the transfer of rephrased questions from more
capable LLMs to clarify ambiguities for the less advanced models. Lastly, we present
thorough discussions with the Chain-of-Thought (CoT) method (Kojima et al., 2022; Wei
et al., 2022). RaR is complementary to CoT and can be easily combined for improvement,
as confirmed by our experimental results. Unlike methods that employ multiple LLMs
for iterative prompt engineering based on accuracy scores (Zhou et al., 2022b; Pryzant
et al., 2023), our method is both unsupervised and training-free , making it economical
and applicable to all questions in the zero-shot setting. Lastly, our work calls forth the
importance that the design of human-crafted tasks targeting specific LLM capabilities should
be rigorously reviewed by both humans and LLMs to ensure clarity in intention.
2 Related Work
2.1 Prompting Methods for LLMs
Since the advent of recent LLMs (OpenAI, 2023; Touvron et al., 2023; Chiang et al., 2023), a
growing body of research has focused on prompt engineering for LLMs (Brown et al., 2020;
Schick & Sch ¨utze, 2021; Zhou et al., 2022b; White et al., 2023; Wang et al., 2023). Manual
guidelines have emerged to guide users in designing and revising their prompts (Reynolds
& McDonell, 2021; Saravia, 2022). OpenAI (2022) has also offered general recommendations
for crafting queries, emphasizing specificity, detail, and precision. However, individuals
often find it challenging to refine their own questions for clarity or to include necessary
details for LLMs, as the questions are clear enough for humans themselves. Subsequent
research (Zhou et al., 2022b; Sorensen et al., 2022; Pryzant et al., 2023) has concentrated on
the autonomous refinement of prompts. These methods often employ multiple LLMs to
generate candidate prompts, evaluate and score these prompts, and iteratively refine them
until a satisfactory prompt is produced. The evaluation of a prompt typically relies on either
the accuracy of an LLM’s response (Zhou et al., 2022b; Pryzant et al., 2023) or the mutual
information of the question (Sorensen et al., 2022). Given the nature of iterative computation
and the necessity for qualitative evaluation, such methods are employed for refining single
prompt templates; applying them universally to all questions would be expensive.
The method most frequently used by users and closely aligned with our approach is the
zero-shot Chain-of-Thought (CoT) prompting (Kojima et al., 2022). In the stronger few-
shot setting, few-shot CoT (Wei et al., 2022) has also been a strong prompting method.
Given that these techniques do not require evaluation and iterative selection, they have
gained widespread popularity and inspired a series of subsequent studies (Wang et al.,
2022; Zhou et al., 2022a; Press et al., 2022; Yao et al., 2023; Zhang et al., 2023b; Shao et al.,
2023). However, CoT methods are not without their limitations, as observed in our study.
Recent investigations have also highlighted challenges with the reliability of both zero-shot
CoT (Turpin et al., 2023) and few-shot CoT (Pawelczyk et al., 2023). Most recently, Zhou et al.
(2023) propose Foresee and Reflect similarly as a zero-shot prompting method that targets
the proposed task Thinking for Doing (T4D). Lastly, it is worth noting that our method is
complementary to all the prompting techniques mentioned above and can be combined.
2.2 Self-correction Methods for LLMs
Another line of work aims at enhancing LLM performance (Madaan et al., 2023; Welleck
et al., 2022; Kim et al., 2023b; Pan et al., 2023; Shinn et al., 2023) by leveraging the LLM to
refine its own responses , a concept known as post-hoc prompting. This encompasses terms
such as “self-correction”, “self-refine”, and “self-critique”, where LLMs revise their own
responses drawing upon various feedback sources or critic models . As classified by Pan et al.
(2023), automated critic models generally employ the LLM’s self-feedback (Madaan et al.,
3

--- PAGE 4 ---
Preprint
2023; Shinn et al., 2023; Yan et al., 2023), other trained LLMs (Yang et al., 2022; Lightman et al.,
2023), or external references (Jung et al., 2022; Gao et al., 2023; Yu et al., 2023b; Welleck et al.,
2022). Yet, recent studies (Huang et al., 2023; Stechly et al., 2023) examine the self-correction
capacities of LLMs and find potential limitations, suggesting that LLMs may not be able to
self-correct their reasoning processes. Their findings reveal that self-correction is no better
than self-consistency (Wang et al., 2022). Contrary to allowing the LLM to self-refine its
responses, our method let the LLM instead rephrase questions originally crafted by humans.
Concurrent works. To the best of our knowledge, our work is one of the first works to
examine the misunderstandings between humans and LLMs, as well as the models’ ability
to rephrase inputs as a means of self-improvement. Concurrently, several studies have
emerged that also explore and utilize the rephrasing capabilities of LLMs. Instead of zero-
shot prompting, Yu et al. (2023a) employs LLMs to rephrase questions for the generation of
synthetic data and subsequent fine-tuning. In the domain of vision-language models, Prasad
et al. (2023) similarly shows that the rephrasing ability of language models can effectively
improve its own reasoning process. Furthermore, Weston & Sukhbaatar (2023) introduces
System 2 Attention, a method that let the LLM itself filter out irrelevant information from
the original prompt and to regenerate the query.
3 Aligning the Frames with Rephrase and Respond
To systematically reduce prompt ambiguity and contextualize information in a way that
aligns with the existing frame of the LLMs, we propose Rephrase and Respond (RaR) as
a means of self-clarification. In interpersonal communication, rephrasing is a commonly
known technique where people rephrase another person’s question as a process of under-
standing, to ensure clarity and coherence in responding. Such a communication strategy can
therefore be similarly applied to an LLM, letting it generate a rephrased question first and
provide an answer subsequently. Following this intuition, RaR asks the LLMs to Rephrase
the given questions and then Respond within a single query. This approach can be viewed
as a strategy to directly enhance the quality of the LLM’s response. In detail, we introduce
the following prompt for the question-answering task:
"{question }"\nRephrase and expand the question, and respond. (3.1)
Our prompt design aimed for conciseness and effectiveness. Meanwhile, many other
variations of the prompt remain within our methodology and also provide improvement
in performance, which share the same intention of rephrasing before responding. Such
prompts include but not limited to the following,
•Reword and elaborate on the inquiry, then provide an answer.
•Reframe the question with additional context and detail, then
provide an answer.
•Modify the original question for clarity and detail, then offer an
answer.
•Restate and elaborate on the inquiry before proceeding with a
response.
In later experiments, we will show that RaR remains effective for all variations of these
prompts. With rephrasing before answering, RaR offers several desirable properties in-
cluding (1) letting the LLM self-improve the prompts while maintaining the context of the
original query, (2) better aligning the human’s intended query with LLM’s preferred style of
question, (3) expanding the LLM’s thought process and adding a step that will not naturally
appear when using CoT and (4) providing an approach for humans to interpret how LLMs
understand the questions.
4 RaR Effectively Improves LLM Responses
In this section, we provide a comprehensive assessment of the applicability and efficacy
of RaR. The results are presented in four primary dimensions: (1)RaR is a simple and
effective prompt to improve LLM performances; (2)variations of prompts still fall in the
methodology of RaR and remain robustly effective; (3)LLMs, while all benefit from RaR,
have different proficiency in rephrasing and benefit at different levels.
4

--- PAGE 5 ---
Preprint
RaR: Rephrase and Respond in a single prompt
LLM“Take the last letters of the words in ‘Edgar Bob’ and 
concatenate them.”
Rephrase and expand the question, and respond
Could you please form a new string or series of characters 
by joining together the final letters from each word in the 
phrase “Edgar Bob”?
The last letters in the words “Edgar” and “Bob” are “r”  
and “b”, hence when concatenated, it forms “rb”.
Figure 3: Demonstration of RaR: one single prompt to improve LLM’s response quality.
4.1 Benchmark Tasks
We evaluate the zero-shot capabilities of LLMs across multiple benchmark tasks in different
categories. As proposed by Allen-Zhu & Li (2023), we consider the following tasks that even
advanced LLMs such as GPT-4 are found to respond poorly to many of these questions1.
•Knowledge Classification (Allen-Zhu & Li, 2023). Sampling a pool of individuals with
Wikipedia pages, this task challenges the LLM to decide if a renowned person was born
on an even day, month, or year.
•Knowledge Comparison (Allen-Zhu & Li, 2023). Using the same pool of individuals,
this task instructs the LLM to compare the ages of two people and decide who was born
earlier.
Furthermore, we consider the following widely-used datasets for a comprehensive evalua-
tion, which are also widely considered in previous prompting works (Wei et al., 2022).
•CSQA (Talmor et al., 2019). The CommonSense QA data encompasses a range of ques-
tions that evaluate the ability of commonsense understanding of the world and involves
intricate semantics.
•Date Understanding (bench authors, 2023). Sourced from Big-bench (bench authors,
2023), the Date Understanding task emphasizes commonsense reasoning and deducing
a date from a provided context. We consider a more difficult version where we do not
provide the choices of potential answers and let the LLM answer directly.
•Last Letter Concatenation (Fortes, 2023). The task centers on symbolic reasoning, and
asks the LLM to concatenate the final letters of a given list of names. We consider
concatenation for two names as well as a more difficult task of concatenation for four
names.
•Coin Flip2. Sourced from Hugging Face, the task asks the LLM if the coin still heads up,
given its initial condition and subsequent actions of people who either flipped or did not
flip the coin. We add an additional “Flip means reverse.” to the questions.
•Sports (bench authors, 2023). Sourced from Big-bench (bench authors, 2023), the Sports
Understanding task primarily asks if a sentence is plausible or implausible, where a
prominent sports figure is depicted performing specific sports-related actions.
The details of all evaluated tasks are summarized in Table 5 of Appendix A.
4.2 Main Results
We conduct experiments on the aforementioned benchmark datasets using GPT-43(OpenAI,
2023). We investigate the performance of RaR, which allows the LLM to both rephrase
and respond to the question in a single query. Such an approach can be considered as a
simple zero-shot strategy to improve the LLM’s performance on any question. In Figure 4
1As the data are not open-sourced, we let GPT-4 generate 105 famous individuals with their birth
dates and 105 Chinese idioms in its knowledge.
2https://huggingface.co/datasets/skrishna/coin_flip
3We note that all our experiments accessed GPT-4 during 10/01-10/30. We also include results on
GPT-4-0613 .
5

--- PAGE 6 ---
Preprint
and detailed in Table 1, we compare the accuracy of GPT-4 with or without RaR. Across
a diverse span of 10 tasks that emphasize different aspects of LLM’s capabilities, RaR
consistently yields distinguishable improvements for GPT-4. Notably, for tasks that GPT-4
originally finds highly challenging (e.g., last letter concatenation), RaR exhibits remarkable
improvement even to almost 100% accuracy. Indeed, similar to human communication,
rephrasing and elaborating a question and then answering is an effective approach. In
summary of the results, RaR provides a universal, plug-and-play zero-shot prompt that
allows for efficient and effective performance improvement of LLMs on general tasks.
93.3383.9689.52100
85.45
84.54
75.88
99.0986.8299.0954.2958.1057.1498.44
84.09
79.55
69.92
52.0521.36 74.55
Figure 4: Accuracy (%) comparison of different prompts using GPT-4. RaR improves the
accuracy of GPT-4 across 10 tasks. A larger margin indicates a less well-designed question.
For tasks such as CSQA where the question quality is good enough, our method still offers
a marginal improvement.
Variations of Prompt. Our primary findings, as illustrated in Figure 4, were obtained
using the specific prompt (3.1) . However, it is important to note that variations of this
prompt with modified phrases remain aligned with the RaR methodology’s underlying
principles. In Table 1, we show the performance of GPT-4 across the four variations of the
RaR prompt, as listed in Section 3, along with the average performance deviations from
the results of our main prompt. Despite the fact that changes in prompt wording induce
minor fluctuations in GPT-4’s zero-shot performance, a consistent enhancement relative to
the baseline results is observed across all variations.
Table 1: Zero-shot test accuracy of GPT-4 on the original questions of each task, using
different variations of prompts. We denote the difference in average zero-shot accuracy for
prompt variants with the main prompt.
Day Month Year Age CSQA Dates Sports Last (2) Last (4) Coin flip Avg
Original 54.29 58.10 57.14 98.44 84.09 69.92 79.55 52.05 21.36 74.55 64.95
RaR 93.33 83.96 89.52 100 85.45 75.88 84.54 99.09 86.82 99.09 89.77
RaR 1 94.29 82.08 85.71 100 87.50 75.34 85.91 95.89 78.64 93.18 87.85(−1.92 )
RaR 2 93.33 88.68 88.57 100 85.83 75.61 88.64 91.32 84.54 93.18 88.97(−0.8)
RaR 3 85.71 83.96 92.38 100 83.33 79.13 85.45 86.76 80.00 95.00 87.17(−2.6)
RaR 4 92.14 87.74 93.33 100 87.50 78.64 86.36 92.24 75.45 93.64 88.70(−1.07 )
4.3 Performance across Various LLMs
We further examine the performance of RaR on various LLMs, including GPT-3.5 and Vi-
cuna (Chiang et al., 2023), a fine-tuned model from LLaMA-2, to investigate if different
LLM architectures can provide consistent response improvement by rephrasing the ques-
tions. Specifically, we consider GPT-4-0613 ,GPT-3.5-turbo-0613 , and Vicuna-13b-v1.5 .
We present the experiment results in Figure 5. Due to Vicuna-13b-v1.5 ’s near-zero per-
formance on Last Letter Concatenation (4), we exclude this task from the evaluation of
Vicuna-13b-v1.5 . Remarkably, all examined LLMs demonstrate enhanced performance
with RaR, resulting in a notable increase in accuracy across the majority of the tasks. More
6

--- PAGE 7 ---
Preprint
advanced models, such as GPT-4, benefit from the most significant gains across all tasks,
while models of lesser complexity, like Vicuna, achieve modest improvements using our
approach. On certain tasks such as Sports, GPT-3.5 and Vicuna even exhibit slightly di-
minished performance. In Appendix B.1, we provide specific examples of the rephrased
questions across different models to illustrate the performance differences. We wrap up this
experiment with the following key insight: all models can benefit from rephrasing questions,
with more advanced models expected to gain a larger improvement.
GPT-4-0613 GPT-3.5-turbo-0613 Vicuna-13b-v1.5
Figure 5: Accuracy (%) of GPT-4-0613 ,GPT-3.5-turbo-0613 and Vicuna-13b when testing
on original and self-rephrased questions using Two-step RaR. GPT-4 enjoys larger improve-
ment on various tasks compared with GPT -3.5 and Vicuna.
5 Let Stronger LLMs Rephrase for Weaker LLMs to Respond
While different LLMs possess different rephrasing capabilities and benefit at different levels
from RaR, we further pose the question that: can stronger LLMs be leveraged to help weaker
LLMs in clarification of the prompts? To further utilize the quality improvement of the
questions rephrased by larger models, like GPT-4, we introduce a variation of RaR called
Two-step RaR that seprates the rephasing step from responding step in RaR. Intuitively, even
among humans, a more detailed and precise question elicits more accurate and decisive
responses. Two-step RaR follows this intuition by designing a two-step procedure to
improve the quality of the questions: in the first step, given a query question , we generate
a self-rephrased query rephrased question by prompting a rephrasing LLM with the
following prompt:
"{question }"
Given the above question, rephrase and expand it to help you do
better answering. Maintain all information in the original question.
Then the original question and the rephrased question are combined to prompt a responding
LLM with the following prompt:
(original) {question }
(rephrased) {rephrased question }
Use your answer for the rephrased question to answer the original question.
Notably, the rephrasing LLM and the responding LLM can be either the same or different
models. As we shown in experiments, different LLMs exhibit distinct proficiency in question
rephrasing. Here, we examine if the rephrased questions generated by a stronger LLM
(GPT-4) can benefit a weaker LLM (Vicuna). We detail Vicuna-13b-v1.5 ’s performance on
questions rephrased by GPT-4, as compared to its own rephrased questions in Table 2. Con-
sistent with our expectation that GPT-4 can better align with human intention and clarify the
question, we observe that its rephrased questions remarkably enhance Vicuna-13b-v1.5 ’s
performance on several tasks, especially when Vicuna’s self-rephrased questions exhibit
low quality. Indeed, the questions can be clarified further for Vicuna, but more exploration
needs to be made on its capability of self-rephrased questions.
7

--- PAGE 8 ---
Preprint
Table 2: Comparison of GPT-4’s rephrased questions with Vicuna’s self-rephrased questions.
We drop the Even Year task since the rephrased questions by Vicuna are clear enough as
those by GPT-4.
Even day Even month CSQA Sports Dates Last letter (2)
Vicuna-rephrased 58.10 56.19 51.36 65.00 32.79 5.45
GPT-4-rephrased 61.90 60.95 55.00 73.64 37.67 10.45
6 Discussion with Chain-of-Thought
6.1 Discussion with Zero-shot CoT
It is widely known that zero-shot CoT, by appending the instruction “Let’s think step by
step.” to queries, can effectively improve the performance of LLMs on reasoning tasks.
However, we highlight some examples where zero-shot CoT fails to deliver improvements,
sometimes even leading to diminished performance. In contrast, RaR consistently demon-
strates effectiveness. Lastly, we note that our method is complementary to zero-shot CoT
and can be combined together by simply adding “let’s think step by step” to (3.1).
Tasks. We examine the Chinese Idiom task as introduced in Allen-Zhu & Li (2023), specif-
ically the most difficult task of inferring the first letter. This task involves taking widely
recognized four-character Chinese idioms and masking one character at one position to
let LLM correctly infer the masked character. Furthermore, we also use the StereoSet
task (Nadeem et al., 2021), which assesses the stereotypical biases present in LLMs with
respect to gender, race, profession, and religion. From the inter-sentence data, we sample
220 examples, each comprising a context sentence and three choices: one stereotypical, one
anti-stereotypical, and one unrelated. We adopt the prompt format used by Shaikh et al.
(2022). We defer the detailed explanation of evaluation to Appendix A.
Results. As illustrated in Table 3, even though RaR enhances LLM’s performance, accurately
inferring the first character of the Chinese Idiom task remains a challenge. One might then
ask: does zero-shot CoT provide consistent improvement to LLM in such tasks as it does
on other reasoning tasks? Our discovery is, in fact, zero-shot CoT may result in worse
performances (31.43%) for such hard tasks, as the LLM tends to hallucinate during the
intermediate steps—a phenomenon similar to hallucination snowballing (Zhang et al.,
2023a). Furthermore, as Shaikh et al. (2022) discovered on other language models, zero-
shot CoT may result in undesired reasoning towards bias and toxicity. Also in Table 3,
we demonstrate the performance of GPT-4 on StereoSet. We can observe that, while zero-
shot CoT fails to improve the Language Modeling Score, rephrased questions improve it
significantly to 97.73%. This implies that, with RaR, the LLM rarely opts for unrelated
choices. Moreover, while zero-shot CoT improves the percentage of fair responses (choosing
neither of them), RaR achieves the best performance.
Table 3: Performance of GPT-4 using original questions, zero-shot CoT prompt and RaR.
Zero-shot CoT results in even worse accuracy of Chinese Idiom due to hallucination in the
reasoning steps. On StereoSet, evaluations include Language Modeling Score and Fair Score,
where RaR demonstrated the best performance.
Dataset Evaluation Original Zero-shot CoT RaR
Chinese Idiom Accuracy (%) 32.38 31.43 35.24
StereoSetLanguage Modeling Score (%) 84.09 84.09 97.73
Fair Score (%) 6.82 35.00 42.27
6.2 Improvement on Few-Shot CoT
Few-shot CoT (Wei et al., 2022), while different from the zero-shot setting that we consider,
has been the most effective CoT technique for its usage of few-shot examples. Instead of
aligning the question to what the LLM best receives, few-shot CoT guides the LLM to reason
8

--- PAGE 9 ---
Preprint
using the supplied human logic. Nonetheless, a concern emerges: How do LLMs respond
when the human-crafted examples are flawed or contain errors? As corroborated by a recent
parallel study (Pawelczyk et al., 2023), we similarly observe that LLMs can be adversely
influenced by bad few-shot examples.
Tasks. We revisit the Last Letter Concatenation task and refer to the few-shot examples
provided in Wei et al. (2022). As shown in Figure 6, the examples follow a specific logic:
obtain the last letter of the first and second word, and concatenate these letters, resulting in
the answer. Such few-shot examples have been demonstrated to most effectively enhance
the performance of a language model, achieving an accuracy of 100% when concatenating
the last letters of two words. Conversely, we explore an example that employs the following
logic: obtain the first letter of the first and second word, and concatenate these letters,
providing the answer for last letter concatenation. Our aim is to investigate how this
alternative few-shot prompt, despite bearing a logic similar to the original prompt and the
correct answer, influences the performance of the GPT-4.
Results. As illustrated in Figure 6, GPT-4 tends to stick to the logic of our modified prompt,
resulting in an incorrect answer. It accurately concatenates all first letters, but concludes
with a seemingly arbitrary final answer. In Table 4, we demonstrate the results of one-
shot and four-shot CoT using such examples. We observe that the performance of the
one-shot CoT evidently degraded with just one flawed example. As the number of these
flawed examples increases, the performance of GPT-4 in a 4-shot setting for last letter
concatenation of four words drops to only 52.27%. This observation reveals a potential
pitfall in employing few-shot CoT: given that these examples are user-crafted, their quality
becomes vital. Meanwhile, we discovered that RaR enables GPT-4 to correct any pitfalls in
the logic of the given examples.
GPT-4Q: Take the last letters of the words in "Elon Musk" and concatenate them.
A: The first letter of "Elon" is "E". The first letter of "Musk" is "M". Conca-
-tenating them is "EM". The answer is nk. 
Q: Take the last letters of the words in "Annette Erin Marisol Esperanza" 
and concatenate them. 
The first letter of "Annette" is "A". The first letter of "Erin" is "E". The first 
letter of "Marisol" is "M". The first letter of "Esperanza" is "E". Concatena-
-ting them is "AEME". The answer is etaz.GPT-4
Q: Take the last letters of the words in "Elon Musk" and concatenate them.
A: The last letter of "Elon" is "n". The last letter of "Musk" is "k". Conca-
-tenating them is "nk". The answer is nk. 
Q: Take the last letters of the words in "Annette Erin Marisol Esperanza" 
and concatenate them. 
The last letter of "Annette" is "e". The last letter of "Erin" is "n". The last 
letter of "Marisol" is "l". The last letter of "Esperanza" is "a". Concatena-
-ting them is "enla". The answer is enla.“Q: Take the last letters of the words in "Elon Mu-
-sk" and concatenate them.
A: The first letter of "Elon" is "E". The first letter 
of "Musk" is "M". Concatenating them is "EM". 
The answer is nk. ”
Given the above question, rephrase and expand it 
to help you do better answering. Maintain all inf-
-ormation in the original question. 
“Q: Identify the last letters of each word in the n-
-ame "Elon Musk", then put those letters together.
A: The last letter of 'Elon' is 'n'. The last letter of 
'Musk' is 'k'. When you put these letters together, 
the result is 'nk'.”
Figure 6: Few-shot CoT can effectively instruct the LLM to reason in a similar manner to
the provided examples. A badly crafted QA example, as shown in red, results in the LLM
following the provided logic but reaching an arbitrary answer.
Table 4: Accuracy (%) of GPT-4 on Last Letter Concatenation task. We use one-shot and
four-shot examples from (Wei et al., 2022), only changing the intermediate steps to be “first
letter” as shown in our example.
Last letter (2) Last letter (4)
One-shot CoT 89.04 78.18
RaR+One-shot CoT 100 93.64
Four-shot CoT 87.21 52.27
RaR+Four-shot CoT 100 95.45
7 Conclusion
In this paper, we have investigated the existing misunderstandings that occur between
humans and LLMs and demonstrated that questions that appear clear to humans may still
be misinterpreted by LLMs. Building on this insight, we introduced Rephrase and Respond
9

--- PAGE 10 ---
Preprint
(RaR), a novel approach that prompts an LLM to first rephrase and clarify the question
before answering it. Our empirical evaluations, conducted across a range of benchmark
datasets, confirm the effectiveness of our proposed methods. Further analysis reveals
that while all models gain enhanced performance through question rephrasing, the more
sophisticated models exhibit more substantial improvements. Based on the observation,
We presented Two-step RaR, a variation of RaR that employs a rephrasing LLM to refine
questions for subsequent use by any responding LLM. Crucially, we have found that the
enhancement in question quality achieved through rephrasing is transferable across models.
In addition to these findings, we have made comparisons with CoT methods through both
mathematical formulation and empirical investigations. We also demonstrated that RaR is
complementary to CoT, and can be leveraged to achieve additional performance gains.
References
Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.2, knowledge
manipulation, 2023.
BIG bench authors. Beyond the imitation game: Quantifying and extrapolating the capabili-
ties of language models. Transactions on Machine Learning Research , 2023. ISSN 2835-8856.
URL https://openreview.net/forum?id=uyTL5Bvosj .
Aras Bozkurt. Generative artificial intelligence (ai) powered conversational educational
agents: The inevitable paradigm shift. Asian Journal of Distance Education , 18(1), 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. Advances in neural information processing systems ,
33:1877–1901, 2020.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P . Xing.
Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
URL https://lmsys.org/blog/2023-03-30-vicuna/ .
Jeremy R Cole, Michael JQ Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan
Dhingra, and Jacob Eisenstein. Selectively answering ambiguous questions. arXiv preprint
arXiv:2305.14613 , 2023.
James N Druckman. The implications of framing effects for citizen competence. Political
behavior , 23:225–256, 2001.
Armando Fortes. simple-dataset-generation, February 2023. URL https://github.com/
atfortes/simple-dataset-generation .
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng
Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and
revising what language models say, using language models. In Proceedings of the 61st
Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp.
16477–16508, 2023.
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying
Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv
preprint arXiv:2310.01798 , 2023.
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan
Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recur-
sive explanations. In Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing , pp. 1266–1279, Abu Dhabi, United Arab Emirates, December 2022.
Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.82. URL
https://aclanthology.org/2022.emnlp-main.82 .
10

--- PAGE 11 ---
Preprint
Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. Tree of
clarifications: Answering ambiguous questions with retrieval-augmented large language
models. arXiv preprint arXiv:2310.14696 , 2023a.
Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer
tasks. arXiv preprint arXiv:2303.17491 , 2023b.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
Large language models are zero-shot reasoners. Advances in neural information processing
systems , 35:22199–22213, 2022.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee,
Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step, 2023.
Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha
Swayamdipta, Noah A Smith, and Yejin Choi. We’re afraid language models aren’t
modeling ambiguity. arXiv preprint arXiv:2304.14399 , 2023.
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao
Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathe-
matical reasoning of foundation models in visual contexts, 2023.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe,
Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative
refinement with self-feedback. arXiv preprint arXiv:2303.17651 , 2023.
Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in
pretrained language models. In Proceedings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pp. 5356–5371, 2021.
OpenAI. Best practices for prompt engineering with openai api.
https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-
openai-api , 2022.
OpenAI. Gpt-4 technical report, 2023.
Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang
Wang. Automatically correcting large language models: Surveying the landscape of
diverse self-correction strategies. arXiv preprint arXiv:2308.03188 , 2023.
Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language
models as few shot unlearners, 2023.
Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher
Meek, and Sumit Gulwani. Synchromesh: Reliable code generation from pre-trained
language models. arXiv preprint arXiv:2201.11227 , 2022.
Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. Rephrase, augment, reason: Visual
grounding of questions for vision-language models. arXiv preprint arXiv:2310.05861 , 2023.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.
Measuring and narrowing the compositionality gap in language models. arXiv preprint
arXiv:2210.03350 , 2022.
Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Auto-
matic prompt optimization with” gradient descent” and beam search. arXiv preprint
arXiv:2305.03495 , 2023.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Language models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
11

--- PAGE 12 ---
Preprint
Laria Reynolds and Kyle McDonell. Prompt programming for large language models:
Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on
Human Factors in Computing Systems , CHI EA ’21, New York, NY, USA, 2021. Association
for Computing Machinery. ISBN 9781450380959. doi: 10.1145/3411763.3451760. URL
https://doi.org/10.1145/3411763.3451760 .
Elvis Saravia. Prompt Engineering Guide. https://github.com/dair-ai/Prompt-Engineering-Guide ,
12 2022.
Timo Schick and Hinrich Sch ¨utze. Exploiting cloze-questions for few-shot text classification
and natural language inference. In Proceedings of the 16th Conference of the European Chapter
of the Association for Computational Linguistics: Main Volume , pp. 255–269, 2021.
Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang. On second
thought, let’s not think step by step! bias and toxicity in zero-shot reasoning. arXiv
preprint arXiv:2212.08061 , 2022.
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen.
Synthetic prompting: Generating chain-of-thought demonstrations for large language
models. arXiv preprint arXiv:2302.00618 , 2023.
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan,
and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.
Taylor Sorensen, Joshua Robinson, Christopher Rytting, Alexander Shaw, Kyle Rogers,
Alexia Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. An information-
theoretic approach to prompt engineering without ground truth labels. In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers) , pp. 819–862, 2022.
Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesn’t know it’s
wrong: An analysis of iterative prompting for reasoning problems, 2023.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A
question answering challenge targeting commonsense knowledge. In Proceedings of the
2019 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers) , pp. 4149–4158, 2019.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language models don’t
always say what they think: Unfaithful explanations in chain-of-thought prompting, 2023.
Jiaqi Wang, Enze Shi, Sigang Yu, Zihao Wu, Chong Ma, Haixing Dai, Qiushi Yang, Yanqing
Kang, Jinru Wu, Huawen Hu, et al. Prompt engineering for healthcare: Methodologies
and applications. arXiv preprint arXiv:2304.14670 , 2023.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha
Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in
language models. arXiv preprint arXiv:2203.11171 , 2022.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language
models. Advances in Neural Information Processing Systems , 35:24824–24837, 2022.
Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi,
and Yejin Choi. Generating sequences by learning to self-correct. arXiv preprint
arXiv:2211.00053 , 2022.
Jason Weston and Sainbayar Sukhbaatar. System 2 attention (is something you might need
too). arXiv preprint arXiv:2311.11829 , 2023.
12

--- PAGE 13 ---
Preprint
Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf
Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. A prompt pattern catalog to
enhance prompt engineering with chatgpt. arXiv preprint arXiv:2302.11382 , 2023.
Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus
for sentence understanding through inference. In Marilyn Walker, Heng Ji, and Amanda
Stent (eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pp.
1112–1122, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.
doi: 10.18653/v1/N18-1101. URL https://aclanthology.org/N18-1101 .
Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I Wang, Wen-tau Yih, and Ziyu Yao. Learning
to simulate natural language feedback for interactive semantic parsing. arXiv preprint
arXiv:2305.08195 , 2023.
Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories
with recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing , pp. 4393–4479, 2022.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and
Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language
models. arXiv preprint arXiv:2305.10601 , 2023.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T
Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own
mathematical questions for large language models. arXiv preprint arXiv:2309.12284 , 2023a.
Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving
language models via plug-and-play retrieval feedback. arXiv preprint arXiv:2305.14002 ,
2023b.
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language
model hallucinations can snowball. arXiv preprint arXiv:2305.13534 , 2023a.
Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multi-
modal chain-of-thought reasoning in language models. arXiv preprint arXiv:2302.00923 ,
2023b.
Denny Zhou, Nathanael Sch ¨arli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale
Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting en-
ables complex reasoning in large language models. In The Eleventh International Conference
on Learning Representations , 2022a.
Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R. McKee, Ari
Holtzman, Jay Pujara, Xiang Ren, Swaroop Mishra, Aida Nematzadeh, Shyam Upadhyay,
and Manaal Faruqui. How far are large language models from agents with theory-of-
mind?, 2023.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,
and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint
arXiv:2211.01910 , 2022b.
13

--- PAGE 14 ---
Preprint
A Experiment Details
We use the entire dataset for Dates Understanding , and randomly draw subsets of size 220
for the rest tasks. We use accuracy to evaluate the performance of the LLM. The accuracy is
firstly estimated using exact matching on the words generated by the LLM. Specifically, an
answer is considered correct if it contains the exact word of the correct response and without
any incorrect responses. We subsequently verify and correct the calculations through
manual inspection. For certain tasks, to constrain the response format (e.g., multiple-choice),
we append a consistent prompt when evaluating the original question and RaR, such as
“Select the single most appropriate answer”. Details of the prompts are presented in Table 6.
Our experiments are done using the publicly available GPT-4 API, as well as the historical
version of GPT-4-0613 and GPT-3.5-turbo-0613. We are also considering an open-source
LLM model, Vicuna-13B-v1.5.
Table 5: Details of all the tasks considered in our experiments, including dataset name,
category, number of questions, and an example question.
Dataset Category Number Question example
Even day Knowledge Classification 105 Was Donald Trump born on an even day?
Even month Knowledge Classification 105 Was Joe Biden born in an even month?
Even year Knowledge Classification 105 Was Elon Musk born in an even year?
Compare age Knowledge Comparison 104 Was Joe Biden born earlier than Donald Trump?
CSQA Commonsense Reasoning 220Everybody has some sort of creativity, it’s the best way to what?
Choices: A. Create art B. Write C. Sew D. Create idea E. Express yourself
Dates Commonsense Reasoning 369 This is the last day of 1899. What is the date tomorrow in MM/DD/YYYY?
Last letter (2) Symbolic Reasoning 220 Take the last letters of the words in “Kristen Andrea” and concatenate them.
Last letter (4) Symbolic Reasoning 220 Take the last letters of the words in “Mia Mariana Marcos Xavier” and concatenate them.
Coin flip Symbolic Reasoning 220A coin is heads up. Roxas does not flip the coin.
Schneiderman does not flip the coin. Is the coin still heads up? Flip means reverse.
Sports Commonsense Reasoning 220 Is the following sentence plausible? “Juan Soto shot from beyond the arc”
StereoSet Stereotypical Bias 220Morocco is a country in Africa. Which of these is more accurate?
A. It’s a very poor country with low standards of living.
B. There are many cultures and classes of people that live there.
C. Blue is her favorite color.
Table 6: Formatting prompts that are appended to both the original questions and RaR
prompts.
Task Format prompt
Date Final answer format should be MM/DD/YYYY.
CSQA Select the *single* most sensible answer.
StereoSet Select the *single* most sensible answer.
Sports Answer the Yes or No question.
Coin flip Answer the Yes or No question.
Evaluation. For the Chinese Idiom task, we evaluate the zero-shot accuracy of GPT-4’s
responses, with automated accuracy estimation and further manual checking. For StereoSet,
as suggested by Nadeem et al. (2021), two crucial evaluation metrics should be considered:
theLanguage Modeling Score , which assesses whether the LLM selects related options
over unrelated ones, and the Stereotype Score , which quantifies the percentage of data
that a model favors stereotypical choices over anti-stereotypical ones. As identified by
the authors, an ideal model would display no bias toward either stereotypical or anti-
stereotypical associations, yielding an optimal score of 0.5 for the Stereotype Score. In our
examination of GPT-4’s outputs, we observe its capability to actually determine that neither
of the two related options can be concluded solely from the context sentence. Consequently,
we categorize such outputs as fair responses and introduce a Fair Score , determined by the
proportion of these responses, complementing the Language Modeling Score . We provide
an example of such a response below.
The question does not provide enough information to determine the accuracy of
options A and C. Option B is irrelevant to the given context. Therefore, none
of the provided options can be selected as the most sensible answer based on
the information available.
14

--- PAGE 15 ---
Preprint
Table 7: Few-shot examples improved by RaR used in Sec 6.2 for Last Letter Concatenation.
Q: Take the last letters of the words in “Elon Musk” and concatenate them.
A: The first letter of “Elon” is “E”. The first letter of “Musk” is “M”. Concatenating them
is “EM”. The answer is nk.
Q: Take the last letters of the words in “Larry Page” and concatenate them.
A: The first letter of “Larry” is “L”. The first letter of “Page” is “P”. Concatenating them
is “LP”. The answer is ye.
Q: Take the last letters of the words in “Sergey Brin” and concatenate them.
A: The first letter of “Sergey” is “S”. The first letter of “Brin” is “B”. Concatenating them
is “SB”. The answer is yn.
Q: Take the last letters of the words in “Bill Gates” and concatenate them.
A: The first letter of “Bill” is “B”. The first letter of “Gates” is “G”. Concatenating them is
“BG”. The answer is ls.
Table 8: Few-shot examples improved by RaR used in Sec 6.2 for Last Letter Concatenation.
Q: Identify the last letters of each word in the name “Elon Musk”, then put those letters
together.
A: The last letter of ‘Elon’ is ‘n’. The last letter of ‘Musk’ is ‘k’. When you put these letters
together, the result is ‘nk’
Q: Identify the last letters of each word in the name “Larry Page”, then put those letters
together.
A: The last letter of ‘Larry’ is ‘y’. The last letter of ‘Page’ is ‘e’. When you put these letters
together, the result is ‘ye’
Q: Identify the last letters of each word in the name “Sergey Brin”, then put those letters
together.
A: The last letter of ‘Sergey’ is ‘y’. The last letter of ‘Brin’ is ‘n’. When you put these
letters together, the result is ‘yn’
Q: Identify the last letters of each word in the name “Bill Gates”, then put those letters
together.
A: The last letter of ‘Bill’ is ‘l’. The last letter of ‘Gates’ is ‘s’. When you put these letters
together, the result is ‘ls’
B Additional Experiment Results
99.04
56.19
64.15
50.48
74.0926.36
50.4570.7378.6484.55
98.1888.6498.6486.3686.3694.5599.54
81.82
74.80GPT-4-0613 Vicuna-13b-v1.5 GPT-3.5-turbo-0613
51.4367.6253.3367.72 48.5763.81
51.4375.24
67.2761.3678.18
72.73
73.64 67.27
52.5752.0322.38
35.24
59.36
82.656.82
57,73
49.52 58.1051.4356.1948.5752.38
66.3575.96
60.45
72.2719.51
32.7969.0965.0055.9151.36
2.73
5.45
99.04
56.19
64.15
50.48
74.0926.36
52.0570.7378.6484.55
88.64
98.1874.8081.8288.57
71.42
71.42
98.64
51.4367.6253.3367.72 48.5763.81
51.4375.24
67.2761.3678.18
72.73
73.6467.27
52.5752.03
59.36
82.656.82
57,73
Figure 7: Accuracy (%) of GPT-4-0613 ,GPT-3.5-turbo-0613 and Vicuna-13b when testing
on original and self-rephrased questions using Two-step RaR. GPT-4 enjoys larger improve-
ment on various tasks compared with GPT -3.5 and Vicuna.
B.1 Rephrasings from Different LLMs
Initial observations suggest that Vicuna-13b-v1.5 ’s rephrased questions seldom offer sub-
stantial clarification, often mirroring the simplicity of their original questions. In the last
instance of Table 9, Vicuna-13b-v1.5 perturbs the question’s intent by changing “yesterday”
15

--- PAGE 16 ---
Preprint
to “today”. While both GPT-3.5 and GPT-4 can elucidate questions, GPT-3.5 occasionally
introduces extra details or misinterpretations. As shown in the second example of Table 9,
GPT-3.5 misinterprets the concept of even month as “a month with an even number of
days”. Similarly, in the third example, GPT-3.5 introduces a wrong constraint of “recent”
game. GPT-4, on the contrary, is able to make clarifications that are mostly close to human
intention. We also observe that GPT-3.5 tends to introduce the following phrase to rephrased
questions in Sports (45 out of 220) and Dates (21 out of 369): “Please rephrase and provide
additional details if necessary to enhance your response accuracy.”, resulting in an answer
with just another rephrased question but not the actual answer. Therefore, we remove all
sentences containing “rephrase” for GPT-3.5 on these two datasets.
Table 9: Examples of the self-rephrased questions generated by different LLM models. A
stronger model makes fewer mistakes in rephrasing the questions.
Model Self-rephrased Question
Original Was Barack Obama born in an even day?
GPT-4-0613 Did Barack Obama’s birth occur on a day of the month that is considered an even number?
GPT-3.5-turbo-0613Was Barack Obama born on a day that is evenly divisible by two, resulting in a whole number
with no remainder when divided by two?
Vicuna-13b-v1.5 What is the birthday of Barack Obama, and is it an even day?
Original Was Abraham Lincoln born in an even month?
GPT-4-0613 Was the former United States president Abraham Lincoln born in a month that is represented
by an even number, from January being ‘1’ to December being ‘12’ ?
GPT-3.5-turbo-0613Can you please clarify whether Abraham Lincoln, the 16th President of the United States, was
born in a month with an even number of days?
Vicuna-13b-v1.5 What was the month in which Abraham Lincoln was born, and is that month even or odd?
Original Is the following sentence plausible? “Amari Cooper scored a touchdown”
GPT-4-0613Is it believable or likely that Amari Cooper, who is known for playing football , scored a
touchdown?
GPT-3.5-turbo-0613Can you confirm if it is reasonable to say, based on current facts or observations, that Amari
Cooper, the football player , successfully achieved a touchdown during a recent game or event?
Vicuna-13b-v1.5 Is it plausible that Amari Cooper scored a touchdown?
Original Yesterday was April 30, 2021. What is the date tomorrow in MM/DD/YYYY?
GPT-4-0613If yesterday was the last day of April in the year 2021, which is 04/30/2021, can we figure
out what the date will be the day after today, using the format of the month first, then day, and
lastly year (MM/DD/YYYY)?
GPT-3.5-turbo-0613Yesterday, which was April 30, 2021, just passed. I am looking to determine the date that
comes after today. To provide an accurate response, I need to express the date format as
MM/DD/YYYY. Can you please assist me in figuring out the date?
Vicuna-13b-v1.5 If today is April 30, 2021, what will be the date tomorrow in the format of MM/DD/YYYY?
B.2 Multiple Rephrasings: Will the Questions Converge?
In this subsection, we explore whether iterative self-rephrasing by GPT-4 yields consistent
clarifications when using Two-step RaR. Specifically, we utilize prompt (??)in Two-step
RaR to enable GPT-4 to rephrase a question, then feed its output back into the same prompt
(??)for a second and third round of rephrasing. In Table 10, we consider “Was Abraham
Lincoln born on an even day?” as an example question and use it for three successive
self-rephrasings by GPT-4 across different runs. The key clarification that needs to be made
here is on the concept of “even day”. While humans understand that “even day” refers to
whether the day of the month is even, LLMs may understand it as either an even day of the
week or year. We observe that although GPT-4 sometimes might not clarify this concept in
its initial attempt, by the third rephrasing, it converges to a consistent explanation of “even
day”. Meanwhile, the question gets more and more elaborate after multiple rephrasings.
This conveys the following key message.
C Input/Output Examples
In this section, we provide specific input and output examples of GPT-4 on each of task we
considered, using either original questions or RaR.
16

--- PAGE 17 ---
Preprint
Table 10: Examples of the self-rephrased questions generated by GPT-4 after the 1st, 2nd
and 3rd self-rephrasing. The original question is “Was Abraham Lincoln born in an even
day?” The key clarification that needs to be made is on the concept of “even day” of the
month instead of even day of the year or the week.
Rephrase times Self-rephrased Question
1stDid the birth of Abraham Lincoln, the sixteenth president of the United States, occur on a day
of the month that is considered an even number?
2ndCan you confirm if the birth date of Abraham Lincoln, who was the sixteenth person to hold the
office of the President of the United States, falls on an even number in the monthly calendar ?
3rdCould you verify whether the day of the month on which Abraham Lincoln, the sixteenth
president of the United States, was born, is an even number? The focus here is on identifying the
specific date and determining if it falls on an even or odd day according to the calendar.
1stIs the date of Abraham Lincoln’s birth an even day? Provide more details, if possible, about his
birth date.
2ndCan you confirm whether the day on which Abraham Lincoln was born is an even number or
not? Please provide as much detailed information as possible concerning his birth date.
3rdCould you verify if the day of the month on which Abraham Lincoln was born falls on an even
number, please provide a comprehensive description around his birth date, while emphasizing on
the day of his birth especially, to assist with this confirmation?
Table 11: Examples of correct and incorrect responses by GPT-4 to the original questions of
Even day .
Question : Was Barack Obama born in an even day?
GPT-4 (Incorrect): No, Barack Obama was born on August 4, 1961, which is an even date
but an odd day.
Question : Was Kanye West born in an even day?
GPT-4 (Incorrect): No, Kanye West was born on June 8, 1977, which is an even day.
Question : Was Stephen Hawking born in an even day?
GPT-4 (Incorrect): No, Stephen Hawking was born on January 8, 1942. Though the day,
8th, is an even number, the term ”even day” isn’t typically used to describe dates.
Question : Was Frida Kahlo born in an even day?
GPT-4 (Incorrect): No, Frida Kahlo was born on July 6, 1907. Therefore, she was born on
an odd day.
Question : Was Leonardo da Vinci born in an even day?
GPT-4 (Correct): No, Leonardo da Vinci was born on April 15, 1452, which is an odd day.
Question : Was Ludwig van Beethoven born in an even day?
GPT-4 (Correct): No, Ludwig van Beethoven was born on December 17, 1770, which is
an odd day.
17

--- PAGE 18 ---
Preprint
Table 12: Examples of correct and incorrect responses by GPT-4 to 1-step RaR on Even day .
We omit “ \n” and leave it as a blank space for better readability.
Question : “Was Barack Obama born in an even day?” Rephrase and expand the question,
and respond.
GPT-4 (Correct): “Did the former United States President, Barack Obama, have his
birthday fall on an even numbered day of a month?” Barack Obama was born on August
4, 1961. So yes, he was born on an even numbered day.
Question : “Was Kanye West born in an even day?” Rephrase and expand the question,
and respond.
GPT-4 (Correct): The question asks, “Was the day of Kanye West’s birth an even-
numbered day?” Kanye West was born on June 8, 1977. Thus, yes, he was born on
an even day, the 8th of June.
Question : “Was Stephen Hawking born in an even day?” Rephrase and expand the
question, and respond.
GPT-4 (Correct): The expanded version of the question could be phrased as, “Was
renowned physicist and author Stephen Hawking born on a date that is classified as an
even day?”. Stephen Hawking was born on January 8, 1942, so the answer is yes; he was
born on an even day.
Question : “Was Frida Kahlo born in an even day?” Rephrase and expand the question,
and respond.
GPT-4 (Correct): “Did the famous Mexican artist Frida Kahlo have her birth date fall on
an even numbered day of the month?” Yes, she was. Frida Kahlo was born on July 6th,
1907, which is an even day of the month.
Question : “Was Leonardo da Vinci born in an even day?” Rephrase and expand the
question, and respond.
GPT-4 (Incorrect): “Did the day of birth for the renowned artist and inventor, Leonardo
da Vinci, fall on an even-numbered day?” Yes, Leonardo da Vinci was born on April 15,
1452, which is an odd-numbered day.
Question : “Was Ludwig van Beethoven born in an even day?” Rephrase and expand the
question, and respond.
GPT-4 (Correct): “Was the celebrated composer Ludwig van Beethoven born on a day
that falls under an even-numbered date?” Ludwig van Beethoven was born on December
17, 1770. Since 17 is an odd number, the answer is no, Beethoven was not born on an
even day.
18

--- PAGE 19 ---
Preprint
Table 13: Examples of correct and incorrect responses by GPT-4 to 1-step RaR on Even day .
We omit “ \n” and leave it as a blank space for better readability.
Question : (original) Was Barack Obama born in an even day? (rephrased) Was the 44th
President of the United States, Barack Obama, born on a day of the month that is classified
as an even number? Use your answer for the rephrased question to answer the original
question.
GPT-4 (Incorrect): No, Barack Obama was not born on an even day. He was born on
August 4, 1961.
Question : (original) Was Kanye West born in an even day? (rephrased) Can you tell me
if the day of the month on which Kanye West was born falls on an even number? Use
your answer for the rephrased question to answer the original question.
GPT-4 (Correct): Kanye West was born on June 8, 1977. Since 8 is an even number, the
answer to both questions is yes.
Question : (original) Was Stephen Hawking born in an even day? (rephrased) Stephen
Hawking, did his birth take place on a day that falls into the category of even numbers?
Use your answer for the rephrased question to answer the original question.
GPT-4 (Correct): Stephen Hawking was born on January 8, 1942. Therefore, the answer
to both the original and rephrased questions is “yes.”
Question : (original) Was Frida Kahlo born in an even day? (rephrased) Was the famous
artist Frida Kahlo born on a day of the month that is an even number? Use your answer
for the rephrased question to answer the original question.
GPT-4 (Incorrect): Frida Kahlo was born on July 6, 1907. Thus, she was not born on an
even day.
Question : (original) Was Leonardo da Vinci born in an even day? (rephrased) Was the
historical figure Leonardo da Vinci, renowned for his artistic and scientific achievements,
born on a day that falls under an even number? Use your answer for the rephrased
question to answer the original question.
GPT-4 (Correct): Leonardo da Vinci was born on April 15, 1452. 15 is an odd number, so
he was not born on an even day.
Question : (original) Was the celebrated composer Ludwig van Beethoven born on a
day that falls under an even-numbered date? (rephrased) Did the birth of the famous
composer Ludwig van Beethoven fall on a day of the month that is considered an even
number? Use your answer for the rephrased question to answer the original question.
GPT-4 (Correct): Ludwig van Beethoven was born on December 17, 1770. 17 is an odd
number, not an even one. Therefore, the answer to both the original and rephrased
question is no, Ludwig van Beethoven was not born on an even day.
Table 14: Examples of correct and incorrect responses by GPT-4 to the original questions of
Coin flip .
Question : A coin is heads up. aluino flips the coin. arthor flips the coin. Is the coin still
heads up? Flip means reverse. Answer the Yes or No question.
GPT-4 (Incorrect): No
Question : A coin is heads up. jumaimah flips the coin. buity flips the coin. Is the coin
still heads up? Flip means reverse. Answer the Yes or No question.
GPT-4 (Incorrect): No
Question : A coin is heads up. kinta does not flip the coin. renit does not flip the coin. Is
the coin still heads up? Flip means reverse. Answer the Yes or No question.
GPT-4 (Correct): Yes
Question : A coin is heads up. brandarius does not flip the coin. gitta flips the coin. Is the
coin still heads up? Flip means reverse. Answer the Yes or No question.
GPT-4 (Correct): No.
19

--- PAGE 20 ---
Preprint
Table 15: Examples of correct and incorrect responses by GPT-4 using 1-step RaR on Coin
flip.
Question : “A coin is heads up. aluino flips the coin. arthor flips the coin. Is the coin
still heads up? Flip means reverse.” Rephrase and expand the question, and respond.
Answer the Yes or No question.
GPT-4 (Correct): The coin started out being heads up. Then, Aluino flipped the coin,
reversing its side. After that, Arthor also flipped the coin, reversing its side again. Is the
coin facing heads up now? Considering the action of flipping the coin means to reverse
its side, after two flips, the coin would indeed be back in its original position. So, yes, the
coin is still heads up.
Question : “A coin is heads up. jumaimah flips the coin. buity flips the coin. Is the coin
still heads up? Flip means reverse.” Rephrase and expand the question, and respond.
Answer the Yes or No question.
GPT-4 (Correct): The coin is initially facing heads up. Then, it is flipped once by Ju-
maimah and again by Buity. After these two flips, is the coin still facing heads up? Taking
into consideration that flipping the coin means to turn it over to its opposite side, the
answer is Yes, the coin is still heads up after two flips.
Question : “A coin is heads up. kinta does not flip the coin. renit does not flip the coin.
Is the coin still heads up? Flip means reverse.” Rephrase and expand the question, and
respond. Answer the Yes or No question.
GPT-4 (Correct): Has the position of the coin, which was initially heads up, changed
considering that neither Kinta nor Renit have flipped, implying reversed, the coin? The
answer is yes, the coin is still heads up.
Question : “A coin is heads up. brandarius does not flip the coin. gitta flips the coin. Is
the coin still heads up? Flip means reverse.” Rephrase and expand the question, and
respond. Answer the Yes or No question.
GPT-4 (Correct): Rephrased and expanded question: Initially, the coin was facing heads
up. The scenario suggests that brandarius doesn’t flip or reverse the coin but gitta does.
Considering the definition of the word ’flip’ as to reverse, we can understand that a coin
flipped changes its position from head to tail or vice versa. So, would the coin maintain
its initial position of being heads up after gitta flipped it?Answer: No.
20

--- PAGE 21 ---
Preprint
Table 16: Examples of correct and incorrect responses by GPT-4 using 2-step RaR on Coin
flip.
Question : (original) A coin is heads up. aluino flips the coin. arthor flips the coin. Is the
coin still heads up? Flip means reverse. (rephrased) The initial state of a coin is that it is
face up with heads. Then, Aluino performs an action of flipping the coin, which implies
reversing its current face from heads to tails. Following this, Arthor takes his turn and
also flips the coin. Now, can it be determined whether the coin reverts to its original
state of being heads up after these sequential flips? Remember, in this context, flipping
pertains to reversing the side that the coin is currently showing. Answer the Yes or No
question.
GPT-4 (Correct): Yes.
Question : (original) A coin is heads up. jumaimah flips the coin. buity flips the coin. Is
the coin still heads up? Flip means reverse. (rephrased) The initial position of the coin is
heads up. Jumaimah proceeds to perform an action on the coin known as a flip, which
changes the coin from its current position to its opposite position. Following this, Buity
also conducts the same action on the coin as Jumaimah did, which is a flip. With this
sequence of events, can we determine if the final position of the coin is still heads up?
Remember, the term ’flip’ is defined as reversing the current position of the coin. Answer
the Yes or No question.
GPT-4 (Correct): Yes.
Question : (original) A coin is heads up. kinta does not flip the coin. renit does not
flip the coin. Is the coin still heads up? Flip means reverse. (rephrased) The original
question gives us a scenario wherein a coin is initially placed in a heads up position. It
also states that two individuals, Kinta and Renit, have not interacted or manipulated the
coin, specifically, they have not flipped it. To flip, as described, is to reverse the coin’s
present state. Considering these points, is the coin’s position unchanged and is it still
facing heads up? Answer the Yes or No question.
GPT-4 (Correct): Yes.
Question : (original) A coin is heads up. brandarius does not flip the coin. gitta flips the
coin. Is the coin still heads up? Flip means reverse. (rephrased) The current state of a
coin is that it is facing heads up. An individual named brandarius does not alter this state
by flipping the coin. However, another individual named gitta does change the state of
the coin by flipping it. A flip in this context refers to reversing the side of the coin that is
facing up. Hence, is it possible that following gitta’s action on the coin, it remains heads
up? Answer the Yes or No question.
GPT-4 (Correct): No.
Table 17: Examples of correct and incorrect responses by GPT-4 to the original questions of
Last letter concatenation (2) .
Question : Take the last letters of the words in “Beth Joel” and concatenate them.
GPT-4 (Incorrect): “hel”
Question : Take the last letters of the words in “Patrick Lorena” and concatenate them.
GPT-4 (Incorrect): kkna
Question : Take the last letters of the words in “Edgar Bob” and concatenate them.
GPT-4 (Correct): “rb”
Question : Take the last letters of the words in “Alexandra Betty” and concatenate them.
GPT-4 (Incorrect): ara
Question : Take the last letters of the words in “Paul Liz” and concatenate them.
GPT-4 (Incorrect): uliz
Question : Take the last letters of the words in “Sheila Alberto” and concatenate them.
GPT-4 (Correct): “ao”
21

--- PAGE 22 ---
Preprint
Table 18: Examples of correct and incorrect responses by GPT-4 using 1-step RaR on Last
letter concatenation (2) .
Question : “Take the last letters of the words in “Beth Joel” and concatenate them.”
Rephrase and expand the question, and respond.
GPT-4 (Correct): Can you merge the last letters from each of the words in the name “Beth
Joel” together? What would the resultant combination look like? The last letters from the
words “Beth” and “Joel” are “h” and “l” respectively. When we merge them together, the
resultant combination is “hl”.
Question : “Take the last letters of the words in “Patrick Lorena” and concatenate them.”
Rephrase and expand the question, and respond.
GPT-4 (Correct): Could you please look at each word in the term “Patrick Lorena,” find
the last letter of each word, and then join these letters together? Response: The last letters
of “Patrick” and “Lorena” are “k” and “a”, respectively. When we concatenate them, we
get “ka”.
Question : “Take the last letters of the words in “Edgar Bob” and concatenate them.”
Rephrase and expand the question, and respond.
GPT-4 (Correct): Could you please form a new string or series of characters by joining
together the final letters from each word in the phrase “Edgar Bob”? The last letters in
the words “Edgar” and “Bob” are “r” and “b”, hence when concatenated, it forms “rb”.
Question : “Take the last letters of the words in “Alexandra Betty” and concatenate them.”
Rephrase and expand the question, and respond.
GPT-4 (Correct): Could you take the final letters that are from both words in “Alexandra
Betty”, combine these letters, and present them together in one word? The response
would be “ay”.
Question : “Take the last letters of the words in “Paul Liz” and concatenate them.”
Rephrase and expand the question, and respond.
GPT-4 (Incorrect): Can you take the final letters from each word in the phrase “Paul Liz”
and put them together sequentially? Yes, by doing so, you get “ulz”.
Question : “Take the last letters of the words in “Sheila Alberto” and concatenate them.”
Rephrase and expand the question, and respond.
GPT-4 (Correct): Can you identify the last letter from each word in the name “Sheila
Alberto” and then join them together? In the name “Sheila Alberto,” the last letters of
each word are “a” from Sheila and “o” from Alberto. If you concatenate or combine these
letters, you get “ao”.
22

--- PAGE 23 ---
Preprint
Table 19: Examples of correct and incorrect responses by GPT-4 using 2-step RaR on Last
letter concatenation (2) .
Question : (original) Take the last letters of the words in “Beth Joel” and concatenate
them. (rephrased) Can you extract the final letters from both “Beth” and “Joel”, and then
combine them to form a new arrangement of letters, maintaining the original instruction
to concatenate them? Use your answer for the rephrased question to answer the original
question.
GPT-4 (Correct): The last letter of “Beth” is “h” and the last letter of “Joel” is “l”. Con-
catenate them, the result is “hl”.
Question : (original) Take the last letters of the words in “Patrick Lorena” and concatenate
them. (rephrased) Identify the final letters in each of the words in ‘Patrick Lorena’. After
you’ve identified those, combine them all together, in the order they appeared, to form a
new word or string of letters. Use your answer for the rephrased question to answer the
original question.
GPT-4 (Correct): The final letters of “Patrick” and “Lorena” are “k” and “a” respectively.
When combined together, they form “ka”.
Question : (original) Take the last letters of the words in “Edgar Bob” and concatenate
them.” Rephrase and expand the question, and respond. (rephrased) Can you identify
and extract the final letters in both the words that form “Edgar Bob”, and then join them
together in the order they appear? Use your answer for the rephrased question to answer
the original question.
GPT-4 (Correct): The last letters in the words “Edgar Bob” are “r” and “b”. Concatenating
them in the order they appear would be “rb”.
Question : (original) Take the last letters of the words in “Alexandra Betty” and concate-
nate them. (rephrased) To answer the question, you would need to look at the words
“Alexandra” and “Betty”. Identify the last letter in each of these words. Once you have
these last letters, join them together in the order they appear in the original words. The
question is asking you to perform these actions. Use your answer for the rephrased
question to answer the original question.
GPT-4 (Correct): The last letter of “Alexandra” is “a” and the last letter of “Betty” is “y”.
When these are concatenated, or joined together, the result is “ay”.
Question : (original) Take the last letters of the words in “Paul Liz” and concatenate
them. (rephrased) Identify the last letters of each word in the phrase “Paul Liz”. After
identifying, combine these letters together to form a new word or string. This process is
known as concatenation. In other words, you will merge the last characters from ‘Paul’
and ‘Liz’ without any spaces between them. Use your answer for the rephrased question
to answer the original question.
GPT-4 (Correct): The last letters of “Paul” and “Liz” are “l” and “z” respectively. Con-
catenating these letters would form “lz”.
Question : (original) Take the last letters of the words in “Sheila Alberto” and concatenate
them. (rephrased) Identify the final letters in each of the words ‘Sheila’ and ‘Alberto’.
After recognizing these, join or link them together in the same order they appear. This
process is referred to as concatenation. Use your answer for the rephrased question to
answer the original question.
GPT-4 (Correct): The last letter of “Sheila” is “a” and the last letter of “Alberto” is “o”.
So, when concatenated, the resulting output is “ao”.
23

--- PAGE 24 ---
Preprint
Table 20: Examples of correct and incorrect responses by GPT-4 to the original questions of
Sports .
Question : Is the following sentence plausible? “Javier Zanetti earned a direct kick”
Answer the Yes or No question.
GPT-4 (Incorrect): No
Question : Is the following sentence plausible? “Jayson Tatum nutmegged the defender”
Answer the Yes or No question.
GPT-4 (Incorrect): Yes
Question : Is the following sentence plausible? “Sean Couturier passed the puck in the
Stanley Cup” Answer the Yes or No question.
GPT-4 (Correct): Yes
Question : Is the following sentence plausible? “Tuukka Rask hit a double” Answer the
Yes or No question.
GPT-4 (Correct): No
Question : Is the following sentence plausible? “Sam Darnold struck out” Answer the
Yes or No question.
GPT-4 (Incorrect): Yes
Table 21: Examples of correct and incorrect responses by GPT-4 using One-step RaR on
Sports .
Question : ‘Is the following sentence plausible? “Javier Zanetti earned a direct kick”’
Rephrase and expand the question, and respond. Answer the Yes or No question.
GPT-4 (Correct): “Is it plausible or possible that Javier Zanetti received a direct kick
during a soccer game?” Yes, it is plausible.
Question : ‘Is the following sentence plausible? “Jayson Tatum nutmegged the defender”’
Rephrase and expand the question, and respond. Answer the Yes or No question.
GPT-4 (Incorrect): Could it be possible that Jayson Tatum managed to play a nutmeg
move on a defender during a basketball game? Yes, it’s plausible as nutmegging is a
commonly used trick in basketball and Jayson Tatum is known for his skills and tricks in
the game.
Question : ‘Is the following sentence plausible? “Sean Couturier passed the puck in the
Stanley Cup”’ Rephrase and expand the question, and respond. Answer the Yes or No
question.
GPT-4 (Correct): Is it possible for the statement “Sean Couturier maradona’d the defender”
to make sense or be meaningful? In context of sports language, particularly soccer, the
term “maradona’d” could be used to describe a soccer player performing a maneuver
similar to that of Diego Maradona, a renowned soccer player. Sean Couturier is a
professional ice hockey player, and although the sports are different, it is plausible that
he could carry out a maneuver in hockey analogous to one performed by Maradona in
soccer, assuming “maradona’d” means he skillfully bypassed or tricked the defender in
some way. So, yes, the sentence could be plausible.
Question : ‘Is the following sentence plausible? “Tuukka Rask hit a double”’ Rephrase
and expand the question, and respond. Answer the Yes or No question.
GPT-4 (Correct): Is it possible that Tuukka Rask, a professional ice hockey player, could
have hit a double in a game? No, Tuukka Rask is a professional ice hockey goaltender for
the Boston Bruins. He would not be involved in a sport where he would hit a double,
such as baseball. The phrase “hitting a double” isn’t applicable to ice hockey, making the
sentence implausible.
Question : ‘Is the following sentence plausible? “Sam Darnold struck out”’ Rephrase and
expand the question, and respond. Answer the Yes or No question.
GPT-4 (Correct): Is it reasonable or believable to say “Sam Darnold struck out”? No, it’s
not plausible because Sam Darnold is a known professional football player, and the term
“struck out” is used in baseball, not football.
24

--- PAGE 25 ---
Preprint
Table 22: Examples of correct and incorrect responses by GPT-4 using Two-step RaR on
Sports .
Question : (original) Is the following sentence plausible? “Javier Zanetti earned a direct
kick” (rephrased) Is it believable or possible that Javier Zanetti secured a direct free kick
in a match? Use your answer for the rephrased question to answer the original question.
Answer the Yes or No question.
GPT-4 (Correct): Yes
Question : (original) Is the following sentence plausible? “Jayson Tatum nutmegged the
defender” (rephrased) Is it believable that Jayson Tatum, a professional basketball player,
could have performed a move where he passed the ball through a defender’s legs, also
known as a ‘nutmeg’? Use your answer for the rephrased question to answer the original
question. Answer the Yes or No question.
GPT-4 (Incorrect): Yes
Question : (original) Is the following sentence plausible? “Sean Couturier passed the
puck in the Stanley Cup” (rephrased) Could it be possible that Sean Couturier passed the
puck during a game in the Stanley Cup tournament? Use your answer for the rephrased
question to answer the original question. Answer the Yes or No question.
GPT-4 (Correct): Yes
Question : (original) Is the following sentence plausible? “Tuukka Rask hit a double”
(rephrased) Is it feasible that Tuukka Rask, presumably known for a non-baseball sport,
hit a double in a baseball game? Use your answer for the rephrased question to answer
the original question. Answer the Yes or No question.
GPT-4 (Correct): No
Question : (original) Is the following sentence plausible? “Sam Darnold struck out”
(rephrased) Is it believable that Sam Darnold has struck out in a game? Please provide a
detailed explanation supporting your answer without omitting any information from
the initial question. Use your answer for the rephrased question to answer the original
question. Answer the Yes or No question.
GPT-4 (Correct): No
25
