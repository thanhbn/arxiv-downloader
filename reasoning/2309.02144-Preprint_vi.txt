Tôi muốn bạn đóng vai trò như một giáo viên toán tiểu học, và đánh giá chất lượng của câu trả lời được cung cấp bởi một trợ lý AI cho Câu hỏi toán học được hiển thị bên dưới.
Bạn sẽ được cung cấp một câu trả lời tham chiếu và câu trả lời của trợ lý, và Đánh giá của bạn nên xem xét tính đúng đắn của câu trả lời của trợ lý.
Bắt đầu đánh giá của bạn bằng cách so sánh câu trả lời của trợ lý với câu trả lời tham chiếu từng bước. Xác định và sửa chữa bất kỳ lỗi nào.
Câu trả lời được chấm điểm trên 10 điểm, với một điểm bị trừ cho mỗi bước sai. Hãy khách quan nhất có thể.
Bạn cần đưa ra Bằng chứng Đánh giá trước rồi mới đánh giá phản hồi trên thang điểm từ 1 đến 10.
[Câu hỏi]:
{question}
[Bắt đầu Câu trả lời Tham chiếu]
{reference}
[Kết thúc Câu trả lời Tham chiếu]
[Bắt đầu Câu trả lời của Trợ lý]
{answer}
[Kết thúc Câu trả lời của Trợ lý]
Bạn PHẢI xuất ra với hai dòng:
Bằng chứng Đánh giá: <Giải thích>
Đánh giá: <CHỈ một chữ số duy nhất>

Bảng 6: Mẫu đánh giá nhắc ChatGPT chấm điểm từng COT ứng viên.

A BỘ DỮ LIỆU

Chúng tôi tiến hành thí nghiệm trên ba bộ dữ liệu suy luận được sử dụng rộng rãi với chuỗi tư duy được chú thích bởi con người, bao gồm nhiệm vụ suy luận toán học GSM8K (Cobbe et al., 2021), AQUA-RAT (Ling et al., 2017), nhiệm vụ suy luận thông thường ECQA (Aggarwal et al., 2021):

GSM8K GSM8K là một bộ dữ liệu suy luận toán học được sử dụng rộng rãi, bao gồm 8.5K bài toán từ đa dạng dành cho tiểu học, được phát triển bởi các tác giả con người. Nó được chia thành 7.5K bài toán huấn luyện và 1K bài toán kiểm tra. Chúng tôi lấy mẫu 400 bài toán từ tập kiểm tra để tạo thành tập xác thực, và do đó chúng tôi có lần lượt 7,473, 400, và 919 ví dụ trong tập huấn luyện, xác thực, và kiểm tra.

AQUA-RAT AQUA-RAT bao gồm khoảng 100,000 bài toán từ dựa trên đại số, mỗi bài được kèm theo một lý luận ngôn ngữ tự nhiên. Mỗi ví dụ trong bộ dữ liệu bao gồm bốn thành phần: 1) câu hỏi, được viết bằng ngôn ngữ tự nhiên, 2) lựa chọn, một tập hợp năm câu trả lời tiềm năng với một câu đúng, 3) lý luận, một giải thích ngôn ngữ tự nhiên về lời giải của bài toán, và 4) đúng, lựa chọn câu trả lời đúng. Để hiệu quả, chúng tôi lấy mẫu ngẫu nhiên lần lượt 5,000, 400, và 1,254 ví dụ làm tập huấn luyện, xác thực, và kiểm tra.

ECQA ECQA được dẫn xuất từ CommonsenseQA (CQA) (Saha et al., 2018) bằng cách tạo ra một giải thích tự do cho mỗi cặp QA trong CQA. CQA là một bộ dữ liệu toàn diện cho suy luận thông thường, chứa các cặp QA với năm lựa chọn và một câu trả lời đúng duy nhất. ECQA bao gồm tổng cộng 11K cặp QA và có lần lượt 7,598, 1,090, và 2,194 ví dụ trong tập huấn luyện, xác thực, và kiểm tra.

GSM8K-RANK Để đánh giá hiệu quả của AFT trong tình huống xếp hạng, chúng tôi chọn ngẫu nhiên 1,000 ví dụ từ tập huấn luyện của GSM8K và tạo ra 8 COT ứng viên cho mỗi câu hỏi. Sau đó chúng tôi nhắc ChatGPT đánh giá những ứng viên này bằng cách cung cấp câu hỏi, câu trả lời tham chiếu, và COT cần được đánh giá và do đó chúng ta có thể đạt được một chuỗi xếp hạng chất lượng cho các COT được tạo ra khác nhau. Chúng tôi lấy mẫu ngẫu nhiên 20 ví dụ và phát hiện rằng kết quả chấm điểm của ChatGPT phù hợp tốt với đánh giá của con người. ChatGPT được hướng dẫn gán điểm từ 1 đến 10, biểu thị chất lượng của mỗi COT. Để đảm bảo độ tin cậy của các đánh giá, theo (Wang et al., 2023b), chúng tôi yêu cầu ChatGPT trình bày bằng chứng đánh giá trước khi gán điểm, và đơn giản 3 điểm cho mỗi ví dụ. Chúng tôi lấy điểm trung bình làm điểm cuối cùng cho mỗi COT.

B THIẾT LẬP THAM SỐ

Chúng tôi tiến hành thí nghiệm trên bốn mô hình ngôn ngữ lớn, LLama-7B, LLama-13B, LLama2-7B, và LLama2-13B. Chúng tôi không tiến hành thí nghiệm trên các mô hình lớn hơn do hạn chế tài nguyên. Chúng tôi lấy mẫu k = 6 COT từ VFT-LLM với nhiệt độ lấy mẫu là 1. Hàm mất mát căn chỉnh ràng buộc tách rời của chúng tôi không giới thiệu bất kỳ siêu tham số nào, và chúng tôi tìm kiếm siêu tham số của hàm mất mát ràng buộc biên giới trong phạm vi (0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5) trên tập xác thực. Giá trị beta của các mô hình và bộ dữ liệu khác nhau được cung cấp trong Bảng 7. Trên GSM8K, AQUA, và ECQA, các mô hình được huấn luyện lần lượt trong 3, 3, và 1 epoch. Tốc độ học được đặt ở 2e-5, với sự phân rã tuyến tính và khởi động tuyến tính cho 3% tổng số bước huấn luyện. Các mô hình 7B và 13B được huấn luyện lần lượt trên 8 và 32 GPU V100 với bộ nhớ 32GB. Chúng tôi sử dụng độ dài chuỗi tối đa là 512 và sử dụng thư viện DeepSpeed và trình tối ưu hóa ZeRO trong quá trình huấn luyện.

Models | GSM8K | AQUA | ECQA | GSM8K-RANK
-------|--------|-------|-------|------------
LLama-7B | 0.15 | 0.15 | 0.15 | 0.05
LLama2-7B | 0.15 | 0.40 | 0.35 | 0.15
LLama-13B | 0.15 | 0.15 | 0.15 | 0.15
LLama2-13B | 0.15 | 0.15 | 0.20 | 0.15

Bảng 7: Giá trị của siêu tham số beta cho căn chỉnh ràng buộc biên giới.

C HÀM MẤT MÁT XẾP HẠNG RÀNG BUỘC TÁCH RỜI

Cho một chuỗi xếp hạng c1 ⪰ c2 ⪰ ··· ⪰ ck, bên cạnh việc mở rộng LBC
A (Phương trình 8) thành hàm mất mát xếp hạng LRBC
A (Phương trình 14), chúng tôi cũng thử mở rộng RDC
A thành hai loại hàm mất mát xếp hạng ràng buộc tách rời như sau:

LRDC1
A = log[1 + ΣciϮcj exp(D(scj
theta) − sci
theta)] (12)

LRDC2
A = log[1 + Σci≻cj,cj∉cmin exp(scj
theta − sci
theta) + Σci≻cj,cj∈cmin exp(D(scj
theta) − sci
theta)] (13)

trong đó cmin là tập hợp tất cả các ví dụ chất lượng thấp nhất. Cụ thể, LRDC1
A tách rời điểm số của c khi nó phục vụ như một ví dụ tiêu cực, trong khi LRDC2
A chỉ tách rời điểm số của các ví dụ chất lượng thấp nhất. Chúng tôi thiết kế LRDC2
A vì chúng tôi xem xét rằng trong tình huống xếp hạng, các ví dụ chất lượng cao hơn được ràng buộc vốn dĩ bởi các ví dụ chất lượng thấp hơn. Do đó, chúng tôi giả thuyết rằng việc ràng buộc chỉ các ví dụ thấp nhất có thể ngăn chặn suy thoái mô hình.

Chúng tôi cũng xem xét một đường cơ sở xếp hạng không có bất kỳ ràng buộc nào:

LR
A = log[1 + Σci≻cj exp(scj
theta − sci
theta)] (14)

Methods | LV FT | +LRBC
A | +LRDC1
A | +LRDC2
A | +LR
A
---------|--------|------------|------------|------------|--------
Accuracy | 20.82 ±0.71 | 26.08 ±1.05 | 25.68 ±0.49 | 12.57 ±1.34 | 7.03 ±0.98

Bảng 8: Kết quả của LLama-7B trên GSM8K được tinh chỉnh bởi các phương pháp khác nhau.

Bảng 8 minh họa kết quả của LLama7B được tinh chỉnh bởi các phương pháp khác nhau trên GSM8K-RANK. Như được hiển thị: 1): Phương pháp không đặt bất kỳ ràng buộc nào LA chỉ đạt được độ chính xác 7.03, cho thấy tầm quan trọng của việc thêm ràng buộc vào hàm mất mát căn chỉnh. 2): LRDC2
A, áp dụng ràng buộc tách rời chỉ cho các ví dụ chất lượng thấp nhất, đạt được độ chính xác cải thiện nhẹ là 12.57. Tuy nhiên, nó cũng làm giảm đáng kể hiệu suất tổng thể của mô hình so với VFT, cho thấy rằng việc ràng buộc chỉ các ví dụ chất lượng thấp nhất là không đủ. 3): LRDC1
A tốt hơn nhiều so với VFT, LRDC2
A và LA, chúng tôi nghĩ lý do là sau khi tách rời tất cả điểm số tiêu cực, LRDC1
A ngăn chặn suy thoái mô hình, tuy nhiên, nó tệ hơn LRBC
A, chúng tôi giả thuyết rằng LRDC1
A chỉ cố gắng cải thiện tất cả điểm số, mặc dù với mức độ khác nhau, điều này không đủ tốt trong tình huống xếp hạng.

D ĐI SÂU VÀO CÁC HÀM MẤT MÁT XẾP HẠNG TRƯỚC ĐÂY ĐỂ CĂN CHỈNH

Trong phần này, chúng tôi đi sâu vào các hàm mất mát xếp hạng được sử dụng rộng rãi trước đây để căn chỉnh, DPO (Rafailov et al., 2023), RRHF (Yuan et al., 2023b) và PRO (Song et al., 2023a), và chỉ ra rằng tất cả chúng đều thiếu một thuật ngữ ràng buộc.

Cho một chuỗi xếp hạng c1 ⪰ c2 ⪰ ··· ⪰ ck, tất cả các hàm mất mát xếp hạng được đề xuất để đảm bảo điểm số của các ví dụ chất lượng cao lớn hơn so với các ví dụ chất lượng thấp. Các hàm mất mát xếp hạng thường sử dụng log-likelihood trung bình theo token để biểu diễn điểm số của một ví dụ c được đưa ra bởi một LLM được tham số hóa bởi theta:

sc
theta = (1/|c|) Σj=1|c| logP(cj|c<j, q;theta), (15)

D.1 DPO

Tối ưu hóa Sở thích Trực tiếp (DPO) (phiên bản xếp hạng) tối ưu hóa LLM với hàm mất mát xếp hạng sau:

LDPO = −Σci log[exp(βsci
theta − βsci
thetaref)/(exp(βsci
theta − βsci
thetaref) + Σcj≺ci exp(βscj
theta − βscj
thetaref))]
= Σci log[1 + Σcj≺ci exp(βscj
theta − βscj
thetaref − βsci
theta + βsci
thetaref)] (16)

trong đó theta và thetaref là tham số của mô hình huấn luyện và mô hình tham chiếu, tương ứng. Mô hình huấn luyện và mô hình tham chiếu thường được khởi tạo bởi cùng một LLM, và DPO đóng băng mô hình tham chiếu trong quá trình tinh chỉnh. β là một siêu tham số của DPO.

Để phân tích hiệu quả của DPO, chúng tôi tính gradient đối với tham số theta:

∇thetaLDPO = −Σci [Σcj≺ci [βexp(βscj
theta − βscj
thetaref − βsci
theta + βsci
thetaref)∇thetasci
theta − βexp(βscj
theta − βscj
thetaref − βsci
theta + βsci
thetaref)∇thetascj
theta]]/(1 + Σcj≺ci exp(βscj
theta − βscj
thetaref − βsci
theta + βsci
thetaref)) (17)

Dựa trên ∇thetaLDPO, đối với mỗi cặp (ci, cj), LDPO sẽ giảm scj
theta với trọng số gradient βexp(βscj
theta − βscj
thetaref − βsci
theta + βsci
thetaref)/(1 + Σcj≺ci exp(βscj
theta − βscj
thetaref − βsci
theta + βsci
thetaref)), điều này có thể dẫn đến suy thoái mô hình.

Trong bài báo DPO gốc (Rafailov et al., 2023), họ quan sát thấy thảm họa này và giảm thiểu nó bằng cách đặt β rất nhỏ (ví dụ, 0.1) để đạt được trọng số gradient nhỏ. Vui lòng tham khảo bài báo gốc để biết thêm chi tiết. Tuy nhiên, dựa trên Phương trình 17, β nhỏ cũng cản trở việc cải thiện các ví dụ tích cực, điều này cũng có thể cản trở hiệu suất của mô hình. Hơn nữa, việc chỉ dựa vào việc giảm trọng số gradient có thể không đủ để ngăn chặn suy thoái mô hình, như được chứng minh trong phân tích tiếp theo về RRHF và PRO. Trong bài báo này, chúng tôi không sao chép DPO vì không có mã công khai chính thức cho xếp hạng.

Scaling Factor β | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1
-----------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|----
Accuracy | 18.75 | 18.01 | 15.05 | 13.20 | 11.79 | 11.79 | 9.83 | 8.78 | 8.62 | 7.51

Bảng 9: Ảnh hưởng của hệ số tỷ lệ trọng số gradient β cho RRHF.

D.2 RRHF

Xếp hạng Phản hồi để căn chỉnh Phản hồi của Con người (RRHF), tính đến xếp hạng ứng viên và phân biệt các ứng viên khác nhau thông qua hàm mất mát xếp hạng cặp đôi:

LRRHF = Σci≻cj max(0, scj
theta − sci
theta) (18)

Chúng tôi tính gradient của LRRHF đối với theta:

∇thetaLRRHF = −Σci≻cj [I(scj
theta > sci
theta)∇thetasci
theta|{z}
tăng sci
theta − I(scj
theta > sci
theta)∇thetascj
theta|{z}
giảm scj
theta] (19)

Dựa trên ∇thetaLRRHF, chúng ta có thể thấy rằng mặc dù RRHF ngầm giới thiệu một ràng buộc bằng cách đặt hàm mất mát về 0 khi điểm số tích cực lớn hơn điểm số tiêu cực, nó vẫn có một nhược điểm: Bất cứ khi nào scj
theta > sci
theta, LRRHF sẽ giảm scj
theta với cùng trọng số gradient I(scj
theta > sci
theta) = 1. Trọng số này có thể quá lớn, có thể làm hại hiệu suất của mô hình.

Để minh họa điều này, chúng tôi khám phá hiệu suất của RRHF với hệ số tỷ lệ β trên trọng số gradient của nó. Như được hiển thị trong Bảng 9, rõ ràng rằng khi trọng số tăng (β lớn hơn), hiệu suất của mô hình giảm, cho thấy rằng: 1) Ràng buộc của RRHF không đủ hiệu quả để ngăn chặn suy thoái mô hình; 2) Chúng ta có thể giảm thiểu suy thoái mô hình bằng cách làm cho trọng số gradient nhỏ hơn như được đề xuất bởi DPO (Rafailov et al., 2023); 3) Mặc dù chúng tôi đã thử β rất nhỏ = 0.1, RRHF vẫn làm hại hiệu suất, điều này cho thấy việc chỉ dựa vào việc giảm trọng số gradient có thể không đủ để ngăn chặn suy thoái mô hình.

Thực tế, trong bài báo RRHF gốc (Yuan et al., 2023b), các tác giả đã quan sát thấy rằng một trọng số xếp hạng lớn, như 10 hoặc 100, làm giảm đáng kể hiệu suất mô hình, khiến họ thử một trọng số nhỏ hơn (tức là 1). Tuy nhiên, họ không phân tích lý do tiềm ẩn. Trong bài báo này, chúng tôi nhấn mạnh rằng một yếu tố chính gây ra sự khác biệt này là việc giảm không chính đáng điểm số ví dụ tiêu cực, điều này đòi hỏi phải áp đặt ràng buộc lên hàm mất mát xếp hạng. Ngoài ra, chúng tôi phát hiện rằng trọng số 1 cũng có thể làm hại đáng kể hiệu suất của mô hình trong nhiệm vụ suy luận. Chúng tôi tin rằng trọng số tối ưu của RRHF thay đổi giữa các nhiệm vụ.

D.3 PRO

Tối ưu hóa Xếp hạng Sở thích (PRO), tính đến xếp hạng ứng viên và phân biệt các ứng viên khác nhau thông qua hàm mất mát xếp hạng với nhiệt độ động:

LPRO = −Σci log[exp(τmax
i sci
theta)/(exp(τmax
i sci
theta) + Σcj≺ci exp(τj
i scj
theta))] = Σci log[1 + Σcj≺ci exp(τj
i scj
theta − τmax
i sci
theta)] (20)

τj
i = rci − rcj > 0, τmax
i = max cj≺ci τj
i (21)

trong đó rc là điểm số của c được đưa ra bởi một mô hình phần thưởng. τj
i là nhiệt độ động cho điểm số scj
theta. Chúng tôi tính gradient đối với tham số theta:

∇thetaLPRO = −Σci [Σcj≺ci [τmax
i exp(τj
i scj
theta − τmax
i sci
theta)∇thetasci
theta − τj
i exp(τj
i scj
theta − τmax
i sci
theta)∇thetascj
theta]]/(1 + Σcj≺ci exp(τj
i scj
theta − τmax
i sci
theta)) (22)

Dựa trên ∇thetaLPRO, chúng ta có thể thấy rằng đối với mỗi cặp (ci, cj), LPRO sẽ giảm scj
theta với trọng số gradient động:

DGWj
i = [τj
i exp(τj
i scj
theta − τmax
i sci
theta)]/(1 + Σcj≺ci exp(τj
i scj
theta − τmax
i sci
theta)), (23)

điều này có thể làm hại hiệu suất của mô hình. Tuy nhiên, trọng số gradient động được tính dựa trên phần thưởng hợp lý hơn so với giá trị hằng số 1 được sử dụng trong RRHF, và do đó PRO vượt trội hơn RRHF. Cụ thể, khi có khoảng cách phần thưởng đáng kể giữa chất lượng cao hơn và chất lượng thấp hơn, được biểu thị bởi một giá trị τj
i lớn. Điều hợp lý là tăng hình phạt cho điểm số ví dụ tiêu cực (DGWj
i lớn), và ngược lại. Để chứng minh điều này, chúng tôi loại bỏ thuật ngữ nhiệt độ động, tức là τj
j và τmax
i, từ PRO. Như được hiển thị trong Bảng 10, chúng ta có thể thấy rằng PRO vượt trội đáng kể so với PRO (remove tau) khi không có ràng buộc. Tuy nhiên, khoảng cách hiệu suất thu hẹp khi thêm ràng buộc tách rời của chúng tôi. Những kết quả này cho thấy: 1) Ở một mức độ nhất định, hiệu quả của nhiệt độ động xuất phát từ khả năng làm cho PRO giảm điểm số tiêu cực một cách hợp lý hơn. 2) Nhiệt độ động hữu ích để ngăn chặn suy thoái mô hình nhưng không đủ tốt.

Methods | PRO | PRO (remove tau) | PRO + RDC1 | PRO (remove tau) + RDC1
---------|-----|------------------|------------|-------------------------
Accuracy | 18.73 ±0.31 | 7.18 ±0.78 | 25.84±0.48 | 25.43 ±0.98

Bảng 10: Tầm quan trọng của nhiệt độ động của PRO. "remove tau" biểu thị loại bỏ thuật ngữ nhiệt độ động, tức là τj
j và τmax
i từ PRO. "+RDC1" biểu thị thêm kỹ thuật tách rời xếp hạng của chúng tôi (Phương trình 12).

D.4 TÓM TẮT

Phân tích của chúng tôi tiết lộ rằng các nghiên cứu căn chỉnh dựa trên xếp hạng trước đây có một số hạn chế:
1) Mặc dù các phương pháp của họ bao gồm một số chiến lược để ngăn chặn suy thoái mô hình (tức là sử dụng hệ số tỷ lệ để giảm trọng số gradient cho DPO, đặt hàm mất mát về 0 cho RRHF, và kết hợp nhiệt độ động cho PRO), họ không nhận ra tầm quan trọng của các ràng buộc đối với các phương pháp căn chỉnh dựa trên xếp hạng trong các bài báo của họ.
2) Các chiến lược của họ về cơ bản liên quan đến việc giảm độ lớn của trọng số gradient, được chứng minh là không đủ (ít nhất trong các nhiệm vụ suy luận).

Khác với các nghiên cứu trước đây, trong bài báo này:
1) Chúng tôi chỉ ra vấn đề sai lệch đánh giá của VFT đối với suy luận và nhấn mạnh tầm quan trọng của ràng buộc đối với căn chỉnh để ngăn chặn suy thoái mô hình.
2) Chúng tôi giới thiệu hàm mất mát căn chỉnh ràng buộc mới. Hàm mất mát căn chỉnh ràng buộc với thuật ngữ ràng buộc biên giới không chỉ thay đổi độ lớn mà còn điều chỉnh hướng của trọng số gradient tùy thuộc vào điều kiện, được chứng minh là rất hiệu quả trong việc ngăn chặn suy thoái mô hình và nâng cao khả năng suy luận của LLM.

E NGHIÊN CỨU TÌNH HUỐNG

Chúng tôi cũng tiến hành một nghiên cứu tình huống để trực quan cho thấy tầm quan trọng của căn chỉnh ràng buộc của chúng tôi. Như được hiển thị trong Bảng 11, cho câu hỏi, AFT của chúng tôi thành công đưa ra COT và câu trả lời đúng, trong khi VFT đưa ra COT sai ở bước thứ hai (tô màu đỏ), chứng minh sự vượt trội của AFT. Quan trọng hơn, sau khi loại bỏ ràng buộc biên giới, khả năng sinh của LLM dường như suy thoái, dẫn đến việc xuất ra nhiều token lặp lại và vô nghĩa.

Câu hỏi | Vào cuối năm học, Kate hỏi giáo viên của cô ấy xem có thể có 3 hộp 64 bút chì màu được không vì tất cả chúng đều đã mòn thành những mảnh nhỏ. Giáo viên đồng ý. Kate mang bút chì về nhà và quyết định làm bánh nướng bút chì bằng cách nấu chảy 8 mảnh nhỏ bút chì lại với nhau trong khuôn bánh nướng mà cô đặt trong lò nướng trong vài phút. Nếu cô có thể bán bánh nướng bút chì với giá 1,50 đô la mỗi cái, Kate có thể kiếm được bao nhiều tiền?
-------|---
Câu trả lời | Cô ấy có thể lấy 3 hộp 64 bút chì nên cô ấy có 3*64 = <<3*64=192>>192 bút chì Mỗi bánh nướng bút chì cần 8 bút chì và cô ấy có 192 bút chì nên cô ấy có thể làm 192/8 = <<192/8=24>>24 bánh nướng bút chì Nếu cô ấy làm 24 bánh nướng bút chì và bán chúng với giá 1,50 đô la mỗi cái, cô ấy có thể kiếm 24*1.5 = $<<24*1.5=36.00>>36.00 [ANS] 36.
VFT | Cô ấy có 3 hộp bút chì đều đã mòn thành những mảnh nhỏ nên cô ấy có 3*64 = <<3*64=192>>192 mảnh bút chì Cô ấy nấu chảy 8 mảnh bút chì lại với nhau trong khuôn bánh nướng nên cô ấy có 8+192 = <<8+192=200>>200 bánh nướng bút chì Cô ấy có thể bán bánh nướng bút chì với giá 1,50 đô la mỗi cái nên cô ấy có thể kiếm 200*1.5 = $<<200*1.5=300>>300 [ANS] 300.
AFT | Kate có 3 hộp * 64 bút chì = <<3*64=192>>192 bút chì. Cô ấy nấu chảy 192 bút chì / 8 bút chì mỗi bánh nướng = <<192/8=24>>24 bánh nướng. Cô ấy bán bánh nướng với giá 1,50 đô la mỗi cái và có 24 bánh nướng, vậy cô ấy kiếm được 24 bánh nướng * 1,50 đô la/bánh nướng = $<<24*1.5=36>>36. [ANS] 36.
AFT (loại bỏ ràng buộc) | Cô ấy có thể làm 8 mảnh bút chì cho mỗi khuôn bánh nướng nên 3 hộp * 8 mảnh/khuôn = 8 * 8 = <<3*8=24>>24 mảnh vừa trong 1 khuôn Mỗi bút chì nấu chảy có giá trị 1,50 đô la vì 24 mảnh * 1,50/64 mảnh = 1,50/1,50/24 mảnh = 1,50/1,50 Cô ấy có thể làm 4 khuôn bút chì nấu chảy vì 24 mảnh / 1,50 / 1,5 / 24 mảnh / 1,50 / 1,50 / 24 mảnh / 1,50 / 1,50 / 24 mảnh / 1,50 / 1,50 / 24 mảnh / 1,50 / 1,50 / 24 mảnh / 1,50 / 1,50 / 24 mảnh / 1,50 / 1,50

Bảng 11: Một nghiên cứu tình huống để trực quan cho thấy hiệu quả của AFT với ràng buộc biên giới. các bước đúng và sai được tô màu xanh và đỏ, tương ứng.
