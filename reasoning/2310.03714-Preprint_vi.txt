# 2310.03714.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/reasoning/2310.03714.pdf
# Kích thước tệp: 460814 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bản thảo
DSP Y: BIÊN DỊCH CÁC LỜI GỌI MÔ HÌNH NGÔN NGỮ KHAI BÁO
THÀNH CÁC PIPELINE TỰ CẢI THIỆN
Omar Khattab,1Arnav Singhvi,2
Paridhi Maheshwari,4Zhiyuan Zhang,1
Keshav Santhanam,1Sri Vardhamanan,6Saiful Haq,6
Ashutosh Sharma,6Thomas T. Joshi,7Hanna Moazam,8
Heather Miller,3,9Matei Zaharia,2Christopher Potts1
1Đại học Stanford,2UC Berkeley,3Đại học Carnegie Mellon,
4Amazon Alexa AI,5Dashworks Technologies, Inc.,
6IIT Bombay,7Calera Capital,8Microsoft,9Two Sigma Investments
okhattab@cs.stanford.edu
TÓM TẮT
Cộng đồng ML đang nhanh chóng khám phá các kỹ thuật để nhắc nhở các mô hình ngôn ngữ (LM) và để xếp chúng thành các pipeline giải quyết các tác vụ phức tạp. Thật không may, các pipeline LM hiện tại thường được triển khai bằng cách sử dụng các "mẫu prompt" được mã hóa cứng, tức là các chuỗi dài được khám phá thông qua thử và sai. Hướng tới một phương pháp tiếp cận hệ thống hơn để phát triển và tối ưu hóa các pipeline LM, chúng tôi giới thiệu DSPy, một mô hình lập trình trừu tượng hóa các pipeline LM như các đồ thị chuyển đổi văn bản, tức là các đồ thị tính toán mệnh lệnh trong đó LM được gọi thông qua các module khai báo. Các module DSPy được tham số hóa, có nghĩa là chúng có thể học (bằng cách tạo và thu thập các minh chứng) cách áp dụng các kết hợp của các kỹ thuật nhắc nhở, tinh chỉnh, tăng cường và lý luận. Chúng tôi thiết kế một trình biên dịch sẽ tối ưu hóa bất kỳ pipeline DSPy nào để tối đa hóa một chỉ số nhất định. Chúng tôi tiến hành hai nghiên cứu tình huống, cho thấy rằng các chương trình DSPy ngắn gọn có thể biểu đạt và tối ưu hóa các pipeline LM tinh vi mà lý luận về các bài toán từ toán học, giải quyết truy xuất đa bước, trả lời các câu hỏi phức tạp và kiểm soát các vòng lặp agent. Trong vòng vài phút biên dịch, một vài dòng DSPy cho phép GPT-3.5 và llama2-13b-chat tự khởi tạo các pipeline vượt trội hơn việc nhắc nhở few-shot tiêu chuẩn (thường là hơn 25% và 65%, tương ứng) và các pipeline với các minh chứng do chuyên gia tạo (lên đến 5–46% và 16–40%, tương ứng). Trên hết, các chương trình DSPy được biên dịch cho các LM mở và tương đối nhỏ như T5 770M tham số và llama2-13b-chat có thể cạnh tranh với các phương pháp dựa trên các chuỗi prompt do chuyên gia viết cho GPT-3.5 độc quyền.
DSPy có sẵn tại https://github.com/stanfordnlp/dspy.

1 GIỚI THIỆU
Các mô hình ngôn ngữ (LM) đang cho phép các nhà nghiên cứu xây dựng các hệ thống NLP ở mức độ trừu tượng cao hơn và với yêu cầu dữ liệu thấp hơn bao giờ hết (Bommasani et al., 2021). Điều này đang thúc đẩy một không gian bùng nổ của các kỹ thuật "nhắc nhở" - và các kỹ thuật tinh chỉnh nhẹ - để thích ứng LM với các tác vụ mới (Kojima et al., 2022), gợi ra lý luận hệ thống từ chúng (Wei et al., 2022; Wang et al., 2022b), và tăng cường chúng với các nguồn được truy xuất (Guu et al., 2020; Lazaridou et al., 2022; Khattab et al., 2022) hoặc với các công cụ (Yao et al., 2022; Schick et al., 2023). Hầu hết các kỹ thuật này được khám phá riêng lẻ, nhưng sự quan tâm đã tăng lên trong việc xây dựng các pipeline và agent đa giai đoạn phân tách các tác vụ phức tạp thành các lời gọi LM dễ quản lý hơn trong nỗ lực cải thiện hiệu suất (Qi et al., 2019; Khattab et al., 2021a; Karpas et al., 2022; Dohan et al., 2022; Khot et al., 2022; Khattab et al., 2022; Chen et al., 2022; Pourreza & Rafiei, 2023; Shinn et al., 2023).
Thật không may, LM được biết là nhạy cảm với cách chúng được nhắc nhở cho mỗi tác vụ, và điều này được gia tăng trong các pipeline nơi nhiều lời gọi LM phải tương tác hiệu quả. Kết quả là, các lời gọi LM trong các pipeline LM hiện tại và trong các framework phát triển phổ biến thường được triển khai bằng cách sử dụng các 'mẫu prompt' được mã hóa cứng, tức là các chuỗi dài của hướng dẫn và minh chứng được chế tác thủ công thông qua thử nghiệm và sai lầm thủ công. Chúng tôi lập luận rằng phương pháp này, mặc dù phổ biến, có thể mong manh và không thể mở rộng - về mặt khái niệm giống như việc điều chỉnh thủ công các trọng số cho một bộ phân loại. Một prompt chuỗi nhất định có thể không tổng quát hóa cho các pipeline khác nhau hoặc qua các LM, miền dữ liệu khác nhau, hoặc thậm chí các đầu vào.
Hướng tới một phương pháp tiếp cận hệ thống hơn để thiết kế các pipeline AI, chúng tôi giới thiệu mô hình lập trình DSPy.1 DSPy đẩy việc xây dựng các pipeline LM mới ra khỏi việc thao tác các chuỗi dạng tự do và gần hơn với lập trình (kết hợp các toán tử mô-đun để xây dựng các đồ thị chuyển đổi văn bản) nơi một trình biên dịch tự động tạo ra các chiến lược gọi LM và prompt được tối ưu hóa từ một chương trình.
Chúng tôi lấy cảm hứng từ sự đồng thuận đã xuất hiện xung quanh các trừu tượng mạng thần kinh (Bergstra et al., 2013), nơi (1) nhiều lớp đa mục đích có thể được kết hợp mô-đun trong bất kỳ kiến trúc phức tạp nào và (2) các trọng số mô hình có thể được huấn luyện bằng cách sử dụng các bộ tối ưu thay vì được điều chỉnh thủ công.
Để đạt được điều này, chúng tôi đề xuất mô hình lập trình DSPy (Phần 3). Đầu tiên, chúng tôi dịch các kỹ thuật nhắc nhở dựa trên chuỗi, bao gồm các kỹ thuật phức tạp và phụ thuộc vào tác vụ như Chain of Thought (Wei et al., 2022) và ReAct (Yao et al., 2022), thành các module khai báo mang các chữ ký được định kiểu bằng ngôn ngữ tự nhiên. Các module DSPy là các thành phần thích ứng với tác vụ - giống như các lớp mạng thần kinh - trừu tượng hóa bất kỳ chuyển đổi văn bản cụ thể nào, như trả lời một câu hỏi hoặc tóm tắt một bài báo. Sau đó, chúng tôi tham số hóa mỗi module để nó có thể học hành vi mong muốn bằng cách lặp đi lặp lại khởi tạo các minh chứng hữu ích trong pipeline. Lấy cảm hứng trực tiếp từ các trừu tượng PyTorch (Paszke et al., 2019), các module DSPy được sử dụng thông qua các đồ thị tính toán define-by-run biểu cảm. Các pipeline được biểu đạt bằng (1) khai báo các module cần thiết và (2) sử dụng các module này trong bất kỳ luồng điều khiển logic nào (ví dụ, câu lệnh if, vòng lặp for, ngoại lệ, v.v.) để kết nối logic các module.

Sau đó, chúng tôi phát triển trình biên dịch DSPy (Phần 4), tối ưu hóa bất kỳ chương trình DSPy nào để cải thiện chất lượng hoặc chi phí. Các đầu vào của trình biên dịch là chương trình, một vài đầu vào huấn luyện với nhãn tùy chọn, và một chỉ số xác thực. Trình biên dịch mô phỏng các phiên bản của chương trình trên các đầu vào và khởi tạo các dấu vết ví dụ của mỗi module để tự cải thiện, sử dụng chúng để xây dựng các prompt few-shot hiệu quả hoặc tinh chỉnh các LM nhỏ cho các bước của pipeline. Tối ưu hóa trong DSPy có tính mô-đun cao: nó được tiến hành bởi các teleprompter,2 là các chiến lược tối ưu hóa đa mục đích xác định cách các module nên học từ dữ liệu. Theo cách này, trình biên dịch tự động ánh xạ các module khai báo thành các kết hợp chất lượng cao của nhắc nhở, tinh chỉnh, lý luận và tăng cường.

Các mô hình lập trình như DSPy có thể được đánh giá theo nhiều chiều, nhưng chúng tôi tập trung vào vai trò của các prompt do chuyên gia chế tác trong việc định hình hiệu suất hệ thống. Chúng tôi đang tìm cách giảm hoặc thậm chí loại bỏ vai trò của chúng thông qua các module DSPy (ví dụ, các phiên bản của các kỹ thuật phổ biến như Chain of Thought) và teleprompter. Chúng tôi báo cáo về hai nghiên cứu tình huống mở rộng: các bài toán từ toán học (GMS8K; Cobbe et al. 2021) và trả lời câu hỏi đa bước (HotPotQA; Yang et al. 2018) với các khám phá về chain of thought, phản ánh đa chuỗi, truy xuất đa bước, trả lời câu hỏi tăng cường truy xuất, và các vòng lặp agent. Các đánh giá của chúng tôi sử dụng một số chiến lược biên dịch khác nhau một cách hiệu quả và cho thấy rằng các chương trình DSPy đơn giản vượt trội hơn các hệ thống sử dụng prompt thủ công, đồng thời cho phép các chương trình của chúng tôi sử dụng các LM nhỏ hơn nhiều và do đó hiệu quả hơn một cách hiệu quả.

Nhìn chung, công trình này đề xuất mô hình lập trình đầu tiên dịch các kỹ thuật nhắc nhở thành các module khai báo được tham số hóa và giới thiệu một trình biên dịch hiệu quả với các chiến lược tối ưu hóa tổng quát (teleprompter) để tối ưu hóa các pipeline tùy ý của các module này. Các đóng góp chính của chúng tôi là thực nghiệm và thuật toán: với DSPy, chúng tôi đã phát hiện ra rằng chúng tôi có thể triển khai các chương trình rất ngắn có thể khởi tạo các hệ thống NLP đa giai đoạn tự cải thiện sử dụng các LM nhỏ như llama2-13b-chat và T5-Large (770M tham số). Không có prompt thủ công và trong vòng vài phút đến hàng chục phút biên dịch, các kết hợp của các module DSPy có thể nâng chất lượng của các chương trình đơn giản từ 33% lên 82% (Phần 6) và từ 32% lên 46% (Phần 7) cho GPT-3.5 và, tương tự, từ 9% lên 47% (Phần 6) và từ 22% lên 41% (Phần 7) cho llama2-13b-chat.

1DSPy được phát âm là dee-ess-pie. Đây là phiên bản thứ hai của framework Demonstrate–Search–Predict trước đó của chúng tôi (DSP; Khattab et al. 2022). Bài báo này giới thiệu các khái niệm chính trong DSPy. Để có tài liệu mở rộng và cập nhật hơn về framework, chúng tôi khuyến khích độc giả tham khảo https://github.com/stanfordnlp/dspy.
2Chúng tôi có tên tele-prompter từ khái niệm trừu tượng hóa và tự động hóa tác vụ nhắc nhở, đặc biệt là để nó xảy ra ở khoảng cách, không có sự can thiệp thủ công.

--- TRANG 2 ---
Bản thảo
các lời gọi trong các pipeline LM hiện tại và trong các framework phát triển phổ biến thường được triển khai bằng cách sử dụng các 'mẫu prompt' được mã hóa cứng, tức là các chuỗi dài của hướng dẫn và minh chứng được chế tác thủ công thông qua thử nghiệm và sai lầm thủ công. Chúng tôi lập luận rằng phương pháp này, mặc dù phổ biến, có thể mong manh và không thể mở rộng - về mặt khái niệm giống như việc điều chỉnh thủ công các trọng số cho một bộ phân loại. Một prompt chuỗi nhất định có thể không tổng quát hóa cho các pipeline khác nhau hoặc qua các LM, miền dữ liệu khác nhau, hoặc thậm chí các đầu vào.

Hướng tới một phương pháp tiếp cận hệ thống hơn để thiết kế các pipeline AI, chúng tôi giới thiệu mô hình lập trình DSPy.1 DSPy đẩy việc xây dựng các pipeline LM mới ra khỏi việc thao tác các chuỗi dạng tự do và gần hơn với lập trình (kết hợp các toán tử mô-đun để xây dựng các đồ thị chuyển đổi văn bản) nơi một trình biên dịch tự động tạo ra các chiến lược gọi LM và prompt được tối ưu hóa từ một chương trình.

Chúng tôi lấy cảm hứng từ sự đồng thuận đã xuất hiện xung quanh các trừu tượng mạng thần kinh (Bergstra et al., 2013), nơi (1) nhiều lớp đa mục đích có thể được kết hợp mô-đun trong bất kỳ kiến trúc phức tạp nào và (2) các trọng số mô hình có thể được huấn luyện bằng cách sử dụng các bộ tối ưu thay vì được điều chỉnh thủ công.

Để đạt được điều này, chúng tôi đề xuất mô hình lập trình DSPy (Phần 3). Đầu tiên, chúng tôi dịch các kỹ thuật nhắc nhở dựa trên chuỗi, bao gồm các kỹ thuật phức tạp và phụ thuộc vào tác vụ như Chain of Thought (Wei et al., 2022) và ReAct (Yao et al., 2022), thành các module khai báo mang các chữ ký được định kiểu bằng ngôn ngữ tự nhiên. Các module DSPy là các thành phần thích ứng với tác vụ - giống như các lớp mạng thần kinh - trừu tượng hóa bất kỳ chuyển đổi văn bản cụ thể nào, như trả lời một câu hỏi hoặc tóm tắt một bài báo. Sau đó, chúng tôi tham số hóa mỗi module để nó có thể học hành vi mong muốn bằng cách lặp đi lặp lại khởi tạo các minh chứng hữu ích trong pipeline. Lấy cảm hứng trực tiếp từ các trừu tượng PyTorch (Paszke et al., 2019), các module DSPy được sử dụng thông qua các đồ thị tính toán define-by-run biểu cảm. Các pipeline được biểu đạt bằng (1) khai báo các module cần thiết và (2) sử dụng các module này trong bất kỳ luồng điều khiển logic nào (ví dụ, câu lệnh if, vòng lặp for, ngoại lệ, v.v.) để kết nối logic các module.

Sau đó, chúng tôi phát triển trình biên dịch DSPy (Phần 4), tối ưu hóa bất kỳ chương trình DSPy nào để cải thiện chất lượng hoặc chi phí. Các đầu vào của trình biên dịch là chương trình, một vài đầu vào huấn luyện với nhãn tùy chọn, và một chỉ số xác thực. Trình biên dịch mô phỏng các phiên bản của chương trình trên các đầu vào và khởi tạo các dấu vết ví dụ của mỗi module để tự cải thiện, sử dụng chúng để xây dựng các prompt few-shot hiệu quả hoặc tinh chỉnh các LM nhỏ cho các bước của pipeline. Tối ưu hóa trong DSPy có tính mô-đun cao: nó được tiến hành bởi các teleprompter,2 là các chiến lược tối ưu hóa đa mục đích xác định cách các module nên học từ dữ liệu. Theo cách này, trình biên dịch tự động ánh xạ các module khai báo thành các kết hợp chất lượng cao của nhắc nhở, tinh chỉnh, lý luận và tăng cường.

Các mô hình lập trình như DSPy có thể được đánh giá theo nhiều chiều, nhưng chúng tôi tập trung vào vai trò của các prompt do chuyên gia chế tác trong việc định hình hiệu suất hệ thống. Chúng tôi đang tìm cách giảm hoặc thậm chí loại bỏ vai trò của chúng thông qua các module DSPy (ví dụ, các phiên bản của các kỹ thuật phổ biến như Chain of Thought) và teleprompter. Chúng tôi báo cáo về hai nghiên cứu tình huống mở rộng: các bài toán từ toán học (GMS8K; Cobbe et al. 2021) và trả lời câu hỏi đa bước (HotPotQA; Yang et al. 2018) với các khám phá về chain of thought, phản ánh đa chuỗi, truy xuất đa bước, trả lời câu hỏi tăng cường truy xuất, và các vòng lặp agent. Các đánh giá của chúng tôi sử dụng một số chiến lược biên dịch khác nhau một cách hiệu quả và cho thấy rằng các chương trình DSPy đơn giản vượt trội hơn các hệ thống sử dụng prompt thủ công, đồng thời cho phép các chương trình của chúng tôi sử dụng các LM nhỏ hơn nhiều và do đó hiệu quả hơn một cách hiệu quả.

Nhìn chung, công trình này đề xuất mô hình lập trình đầu tiên dịch các kỹ thuật nhắc nhở thành các module khai báo được tham số hóa và giới thiệu một trình biên dịch hiệu quả với các chiến lược tối ưu hóa tổng quát (teleprompter) để tối ưu hóa các pipeline tùy ý của các module này. Các đóng góp chính của chúng tôi là thực nghiệm và thuật toán: với DSPy, chúng tôi đã phát hiện ra rằng chúng tôi có thể triển khai các chương trình rất ngắn có thể khởi tạo các hệ thống NLP đa giai đoạn tự cải thiện sử dụng các LM nhỏ như llama2-13b-chat và T5-Large (770M tham số). Không có prompt thủ công và trong vòng vài phút đến hàng chục phút biên dịch, các kết hợp của các module DSPy có thể nâng chất lượng của các chương trình đơn giản từ 33% lên 82% (Phần 6) và từ 32% lên 46% (Phần 7) cho GPT-3.5 và, tương tự, từ 9% lên 47% (Phần 6) và từ 22% lên 41% (Phần 7) cho llama2-13b-chat.

1DSPy được phát âm là dee-ess-pie. Đây là phiên bản thứ hai của framework Demonstrate–Search–Predict trước đó của chúng tôi (DSP; Khattab et al. 2022). Bài báo này giới thiệu các khái niệm chính trong DSPy. Để có tài liệu mở rộng và cập nhật hơn về framework, chúng tôi khuyến khích độc giả tham khảo https://github.com/stanfordnlp/dspy.
2Chúng tôi có tên tele-prompter từ khái niệm trừu tượng hóa và tự động hóa tác vụ nhắc nhở, đặc biệt là để nó xảy ra ở khoảng cách, không có sự can thiệp thủ công.

--- TRANG 3 ---
Bản thảo

2 CÔNG TRÌNH LIÊN QUAN

Công trình này được lấy cảm hứng từ vai trò mà Torch (Collobert et al., 2002), Theano (Bergstra et al., 2010; 2011; Al-Rfou et al., 2016), Chainer (Tokui et al., 2015), và các framework khác đóng trong sự phát triển của deep learning bằng cách cung cấp các trừu tượng mạnh mẽ. Một sự chuyển đổi tương tự đang nổi lên với các pipeline cấp cao hơn của LM, và chúng tôi đang tìm cách cung cấp một framework khái niệm vững chắc và các trừu tượng lập trình cho những gì chúng tôi gọi là lập trình mô hình nền tảng. Chúng tôi dựa trên lập trình khả vi (Wang et al., 2018) nhưng áp dụng cho các lời gọi LM thay vì mạng thần kinh, và mượn các yếu tố cú pháp từ PyTorch (Paszke et al., 2019).

Học trong ngữ cảnh (McCann et al. 2018; Radford et al. 2018; Brown et al. 2020) là một cơ chế quan trọng cho lập trình mô hình nền tảng. Một lượng lớn công trình đang phát triển đã tiết lộ rằng, đặc biệt với việc điều chỉnh hướng dẫn (Ouyang et al., 2022), chúng ta có thể gợi ra hành vi tinh vi thông qua việc nhắc nhở (Wei et al., 2022; Wang et al., 2022b; Press et al., 2022; Yao et al., 2022; Khot et al., 2022; Madaan et al., 2023). Tương tự, các hình thức giám sát yếu thường đòi hỏi các heuristic cụ thể cho tác vụ (Khattab et al., 2021a;b) hoặc được xây dựng thủ công (Ratner et al., 2016; Hancock et al., 2018) hiện được thực hiện bởi LM (Wang et al., 2022b; Zelikman et al., 2022; Zhang et al., 2022; Shao et al., 2023).

Các phương pháp học trong ngữ cảnh hiện nay thường xuyên gọi các công cụ, dẫn đến các pipeline LM sử dụng các mô hình truy xuất (Chen et al., 2017; Lewis et al., 2020; Guu et al., 2020; Lazaridou et al., 2022; Izacard et al., 2022), các mô hình nền tảng đa phương tiện, và các công cụ truyền thống hơn như API (Nakano et al., 2021) và máy tính. Một số bộ công cụ đã được phát triển để hỗ trợ điều này, bao gồm LangChain (Chase, 2022), Semantic Kernel (Microsoft, 2023), LlamaIndex (Liu, 2022), và nhiều thư viện truy xuất và agent khác. Các bộ công cụ này cung cấp các chuỗi và agent được đóng gói sẵn kết nối LM với nhiều công cụ có thể truy cập. Tuy nhiên, chúng gặp phải các thách thức về kỹ thuật prompt engineering phổ biến mà DSPy giải quyết: chúng biểu đạt hành vi cụ thể cho tác vụ thông qua các mẫu prompt viết tay (để thảo luận chi tiết, xem Phụ lục B).

Các nhà nghiên cứu đang bắt đầu áp dụng tối ưu hóa rời rạc và RL để tìm các prompt hiệu quả, thường cho một lời gọi LM logic duy nhất (Guo et al., 2023; Pryzant et al., 2023; Huang et al., 2022; Yang et al., 2023). DSPy tìm cách tổng quát hóa không gian này: nó cung cấp một framework phong phú để tối ưu hóa các pipeline tùy ý từ các chữ ký khai báo cấp cao, bằng cách khởi tạo các minh chứng đa giai đoạn chất lượng cao với các ràng buộc. Trong framework này, các teleprompter DSPy có thể áp dụng tối ưu hóa bằng cách sử dụng các kỹ thuật lựa chọn mô hình như cross-validation hoặc, về nguyên tắc, với các kỹ thuật tinh vi liên quan đến RL và phản hồi LM (Hu et al., 2023; Zhao et al., 2023a; Shinn et al., 2023) hoặc các phương pháp tối ưu hóa siêu tham số học hoặc Bayesian (Bergstra et al., 2013; Akiba et al., 2019).

Bài báo hiện tại tìm cách thúc đẩy DSPy như một mô hình lập trình và báo cáo các phát hiện thực nghiệm mới từ việc áp dụng trình biên dịch DSPy. Điều này được lấy cảm hứng từ công trình định hình của Bergstra et al. (2010; 2013), Paszke et al. (2019), và Wolf et al. (2020), những người hỗ trợ các mô hình lập trình tương ứng của họ với sự kết hợp của các số liệu benchmark và một số biện pháp định tính. Đối với bài báo hiện tại, chúng tôi tập trung vào việc cho thấy rằng DSPy và trình biên dịch của nó cho phép chúng tôi xây dựng các hệ thống LM xuất sắc mà không cần các chuỗi prompt thủ công, mà thay vào đó từ các đơn vị thực sự mô-đun, và điều này mở ra cánh cửa để khám phá một cách có hệ thống một không gian thiết kế phong phú ở mức độ trừu tượng lập trình rất cao.

3 MÔ HÌNH LẬP TRÌNH DSPY

Chúng tôi trình bày DSPy, xử lý LM như các thiết bị trừu tượng để tạo văn bản,3 và tối ưu hóa việc sử dụng chúng trong các đồ thị tính toán tùy ý. Các chương trình DSPy được biểu đạt bằng Python: mỗi chương trình nhận đầu vào tác vụ (ví dụ, một câu hỏi để trả lời hoặc một bài báo để tóm tắt) và trả về đầu ra (ví dụ, một câu trả lời hoặc một bản tóm tắt) sau một loạt các bước. DSPy đóng góp ba trừu tượng hướng tới tối ưu hóa tự động: signatures, modules, và teleprompters. Signatures trừu tượng hóa hành vi đầu vào/đầu ra của một module; modules thay thế các kỹ thuật prompt thủ công hiện tại và có thể được kết hợp trong các pipeline tùy ý; và teleprompters tối ưu hóa tất cả các module trong pipeline để tối đa hóa một chỉ số.

3Chúng tôi giả định có quyền truy cập vào một hoặc nhiều LM, tiêu thụ một chuỗi prompt và trả về các hoàn thành văn bản. Đây có thể là một LM có thể nhắc nhở có khả năng học trong ngữ cảnh (ví dụ, GPT-3.5 hoặc Llama2-7b) hoặc một LM nhỏ hơn có thể tinh chỉnh (ví dụ, T5-base). Một LM có thể được chọn làm mặc định; các hoạt động sẽ sử dụng nó trừ khi được cấu hình khác.

--- TRANG 4 ---
Bản thảo

3.1 CÁC CHỮ KÝ NGÔN NGỮ TỰ NHIÊN CÓ THỂ TRỪU TƯỢNG HÓA PROMPTING & FINETUNING

Thay vì các prompt chuỗi tự do, các chương trình DSPy sử dụng các chữ ký ngôn ngữ tự nhiên để giao việc cho LM. Một chữ ký DSPy là một khai báo hàm được định kiểu bằng ngôn ngữ tự nhiên: một đặc tả khai báo ngắn gọn nói với DSPy rằng một chuyển đổi văn bản cần làm gì (ví dụ, "tiêu thụ câu hỏi và trả về câu trả lời"), thay vì cách một LM cụ thể nên được nhắc nhở để triển khai hành vi đó. Chính thức hơn, một chữ ký DSPy là một tuple của các trường đầu vào và các trường đầu ra (và một hướng dẫn tùy chọn). Một trường bao gồm tên trường và metadata tùy chọn.4 Trong việc sử dụng điển hình, các vai trò của các trường được DSPy suy luận như một hàm của tên trường. Ví dụ, trình biên dịch DSPy sẽ sử dụng học trong ngữ cảnh để diễn giải question khác với answer và sẽ tinh chỉnh lặp đi lặp lại việc sử dụng các trường này.

Các chữ ký cung cấp hai lợi ích so với prompt: chúng có thể được biên dịch thành các prompt hoặc tinh chỉnh tự cải thiện và thích ứng với pipeline. Điều này chủ yếu được thực hiện bằng cách khởi tạo (Phần 4) các ví dụ minh chứng hữu ích cho mỗi chữ ký. Ngoài ra, chúng xử lý logic định dạng và phân tích có cấu trúc để giảm (hoặc, lý tưởng là, tránh) thao tác chuỗi mong manh trong các chương trình người dùng.

Trong thực tế, các chữ ký DSPy có thể được biểu đạt bằng ký hiệu tắt như question -> answer, để dòng 1 trong phần sau là một chương trình DSPy hoàn chỉnh cho một hệ thống hỏi đáp cơ bản (với dòng 2 minh họa việc sử dụng và dòng 3 phản hồi khi GPT-3.5 là LM):

1qa = dspy.Predict("question -> answer")
2qa(question="Guaraní được nói ở đâu?")
3# Out: Prediction(answer='Guaraní chủ yếu được nói ở Nam Mỹ.')

Trong ký hiệu tắt, tên của mỗi trường chỉ ra vai trò ngữ nghĩa mà trường đầu vào (hoặc đầu ra) đóng trong chuyển đổi. DSPy sẽ phân tích ký hiệu này và mở rộng tên trường thành các hướng dẫn có ý nghĩa cho LM, để english document -> french translation sẽ nhắc nhở cho việc dịch từ tiếng Anh sang tiếng Pháp. Khi cần thiết, DSPy cung cấp các giao diện lập trình nâng cao hơn để biểu đạt các ràng buộc rõ ràng hơn trên các chữ ký (Phụ lục A).

3.2 CÁC MODULE ĐƯỢC THAM SỐ HÓA & TEMPLATED CÓ THỂ TRỪU TƯỢNG HÓA CÁC KỸ THUẬT PROMPTING

Giống như các chữ ký kiểu trong các ngôn ngữ lập trình, các chữ ký DSPy chỉ đơn giản định nghĩa một giao diện và cung cấp các gợi ý giống kiểu về hành vi mong đợi. Để sử dụng một chữ ký, chúng ta phải khai báo một module với chữ ký đó, như chúng ta đã khởi tạo một module Predict ở trên. Một khai báo module như thế này trả về một hàm có chữ ký đó.

Module Predict Module cốt lõi để làm việc với các chữ ký trong DSPy là Predict (pseudocode đơn giản hóa trong Phụ lục D.1). Bên trong, Predict lưu trữ chữ ký được cung cấp, một LM tùy chọn để sử dụng (ban đầu là None, nhưng ngược lại ghi đè LM mặc định cho module này), và một danh sách các minh chứng cho việc nhắc nhở (ban đầu trống). Giống như các lớp trong PyTorch, module được khởi tạo hoạt động như một hàm có thể gọi: nó nhận các đối số từ khóa tương ứng với các trường đầu vào của chữ ký (ví dụ, question), định dạng một prompt để triển khai chữ ký và bao gồm các minh chứng thích hợp, gọi LM, và phân tích các trường đầu ra. Khi Predict phát hiện nó đang được sử dụng trong chế độ biên dịch, nó cũng sẽ theo dõi bên trong các dấu vết đầu vào/đầu ra để hỗ trợ teleprompter trong việc khởi tạo các minh chứng.

Các Module Tích hợp Khác Các module DSPy dịch các kỹ thuật nhắc nhở thành các hàm mô-đun hỗ trợ bất kỳ chữ ký nào, tương phản với phương pháp tiêu chuẩn của việc nhắc nhở LM với các chi tiết cụ thể cho tác vụ (ví dụ, các ví dụ few-shot viết tay). Để đạt được điều này, DSPy bao gồm một số module tinh vi hơn như ChainOfThought, ProgramOfThought, MultiChainComparison, và ReAct.5 Tất cả những module này đều có thể được sử dụng thay thế cho nhau để triển khai một chữ ký DSPy. Ví dụ, chỉ cần thay đổi Predict thành ChainOfThought trong chương trình trên dẫn đến một hệ thống suy nghĩ từng bước trước khi cam kết với trường đầu ra của nó.

Quan trọng là, tất cả các module này được triển khai trong vài dòng mã bằng cách mở rộng chữ ký do người dùng định nghĩa và gọi Predict một hoặc nhiều lần trên các chữ ký mới khi thích hợp. Ví dụ, chúng tôi trình bày một triển khai đơn giản hóa của ChainOfThought tích hợp bên dưới.

1class ChainOfThought(dspy.Module):
2def __init__(self, signature):
3 # Sửa đổi chữ ký từ '*inputs -> *outputs' thành '*inputs -> rationale, *outputs'.
4 rationale_field = dspy.OutputField(prefix="Reasoning: Let's think step by step.")
5 signature = dspy.Signature(signature).prepend_output_field(rationale_field)
6
7 # Khai báo một sub-module với chữ ký đã sửa đổi.
8 self.predict = dspy.Predict(signature)
9
10def forward(self, **kwargs):
11 # Chỉ chuyển tiếp các đầu vào cho sub-module.
12 return self.predict(**kwargs)

Đây là một module đầy đủ chức năng có khả năng học việc nhắc nhở few-shot hiệu quả cho bất kỳ LM hoặc tác vụ nào. Chúng tôi tương phản điều đó với Phụ lục C, sao chép các prompt lý luận dài viết tay bởi các nguồn từ nghiên cứu gần đây đến các thư viện nhắc nhở phổ biến.

Tham số hóa Một cách độc đáo, DSPy tham số hóa các kỹ thuật nhắc nhở này. Để hiểu sự tham số hóa này, quan sát rằng bất kỳ lời gọi LM nào tìm cách triển khai một chữ ký cụ thể cần chỉ định các tham số bao gồm: (1) LM cụ thể để gọi (Chen et al., 2023), (2) các hướng dẫn prompt (Yang et al., 2023) và tiền tố chuỗi của mỗi trường chữ ký và, quan trọng nhất, (3) các minh chứng được sử dụng như prompt few-shot (cho LM đông lạnh) hoặc như dữ liệu huấn luyện (cho tinh chỉnh).

Chúng tôi tập trung chủ yếu vào việc tự động tạo và lựa chọn các minh chứng hữu ích. Trong các nghiên cứu tình huống của chúng tôi, chúng tôi thấy rằng việc khởi tạo các minh chứng tốt cho chúng tôi một cách mạnh mẽ để dạy các pipeline tinh vi của LM các hành vi mới một cách có hệ thống.

Công cụ Các chương trình DSPy có thể sử dụng các công cụ, là các module thực thi tính toán. Chúng tôi hỗ trợ các mô hình truy xuất thông qua một module dspy.Retrieve. Tại thời điểm viết, DSPy có hỗ trợ tích hợp cho các retriever ColBERTv2, Pyserini, và Pinecone, và chúng tôi đã khám phá dspy.SQL thực nghiệm để thực thi các truy vấn SQL và dspy.PythonInterpreter để thực thi mã Python trong một sandbox.

Chương trình Các module DSPy có thể được kết hợp trong các pipeline tùy ý trong một giao diện define-by-run. Lấy cảm hứng trực tiếp từ PyTorch và Chainer, người ta đầu tiên khai báo các module cần thiết tại lúc khởi tạo, cho phép DSPy theo dõi chúng để tối ưu hóa, và sau đó người ta biểu đạt pipeline với mã tùy ý gọi các module trong một phương thức forward. Như một minh họa đơn giản, chúng tôi cung cấp hệ thống tăng cường truy xuất-tạo sinh (RAG) đơn giản nhưng hoàn chỉnh sau đây.

1class RAG(dspy.Module):
2def __init__(self, num_passages=3):
3 # 'Retrieve' sẽ sử dụng cài đặt truy xuất mặc định của người dùng trừ khi bị ghi đè.
4 self.retrieve = dspy.Retrieve(k=num_passages)
5 # 'ChainOfThought' với chữ ký tạo câu trả lời đưa ra truy xuất & câu hỏi.
6 self.generate_answer = dspy.ChainOfThought("context, question -> answer")
7
8def forward(self, question):
9 context = self.retrieve(question).passages
10 return self.generate_answer(context=context, question=question)

Để làm nổi bật tính mô-đun, chúng tôi sử dụng ChainOfThought như một thay thế drop-in của Predict cơ bản. Người ta giờ có thể đơn giản viết RAG()("Guaraní được nói ở đâu?") để sử dụng nó. Lưu ý rằng, nếu chúng ta sử dụng chữ ký "context, question -> search query", chúng ta có được một hệ thống tạo ra các truy vấn tìm kiếm thay vì câu trả lời.

3.3 TELEPROMPTERS CÓ THỂ TỰ ĐỘNG HÓA PROMPTING CHO CÁC PIPELINE TÙY Ý

Khi biên dịch một chương trình DSPy, chúng ta thường gọi một teleprompter, là một bộ tối ưu nhận chương trình, một tập huấn luyện, và một chỉ số - và trả về một chương trình tối ưu mới. Các teleprompter khác nhau (Phần 4) áp dụng các chiến lược khác nhau cho tối ưu hóa.

--- TRANG 5 ---
Bản thảo

Trong DSPy, các tập huấn luyện có thể nhỏ, có thể là một số ít ví dụ, mặc dù dữ liệu lớn hơn cho phép tối ưu hóa mạnh mẽ hơn. Các ví dụ huấn luyện có thể không đầy đủ, tức là chỉ cần các giá trị đầu vào. Nhãn cho các bước pipeline không được yêu cầu, trừ khi chúng cần được sử dụng trong chỉ số. Trong thực tế, chúng tôi thường giả định nhãn chỉ cho (nhiều nhất) đầu ra cuối cùng của chương trình, không phải các bước trung gian. Hiệu quả nhãn này rất quan trọng đối với tính mô-đun: xây dựng một pipeline mới trong DSPy chỉ yêu cầu đơn giản biên dịch lại mã của pipeline mới, không phải chú thích dữ liệu cụ thể cho pipeline mới.

Các chỉ số có thể là các khái niệm đơn giản như exact match (EM) hoặc F1, nhưng chúng có thể là toàn bộ các chương trình DSPy cân bằng nhiều mối quan tâm. Ví dụ, chúng ta có thể biên dịch module RAG ở trên đối với một bộ dữ liệu các cặp câu hỏi-trả lời qatrainset và chỉ số EM. Mục tiêu của tối ưu hóa ở đây là để khởi tạo hiệu quả các minh chứng few-shot. Mã sau đây đạt được điều này:

1# Tập huấn luyện nhỏ chỉ với câu hỏi và câu trả lời cuối cùng.
2qa_trainset = [dspy.Example(question="Thủ đô của Pháp là gì?", answer="Paris")]
3
4# Teleprompter sẽ khởi tạo các nhãn thiếu: chuỗi lý luận và ngữ cảnh truy xuất.
5teleprompter = dspy.BootstrapFewShot(metric=dspy.evaluate.answer_exact_match)
6compiled_rag = teleprompter.compile(RAG(), trainset=qa_trainset)

Trong ví dụ này, teleprompter BootstrapFewShot (Phần 4, Phụ lục E.1) mô phỏng RAG trên (các) ví dụ huấn luyện. Nó sẽ thu thập các minh chứng của mỗi module (tức là các ví dụ về hành vi đầu vào-đầu ra của nó) mà cùng nhau dẫn đến đầu ra hợp lệ (tức là tôn trọng các chữ ký và chỉ số).

Nếu người ta muốn đẩy chương trình đã biên dịch để trở nên trích xuất đưa ra các ngữ cảnh được truy xuất của nó, người ta có thể định nghĩa một chỉ số tùy chỉnh để sử dụng thay cho dspy.evaluate.answer_exact_match:

1def answer_and_context_match(example, pred, trace=None):
2answer_match = dspy.evaluate.answer_exact_match(example, pred)
3
4# Dự đoán có phải là một chuỗi con của một đoạn văn nào không?
5context_match = any((pred.answer.lower() in c) for c in pred.context)
6
7return answer_match and context_match

Lưu ý rằng hành vi như thế này có thể được kiểm tra chính xác hơn bởi một chương trình DSPy khác kiểm tra việc grounding trung thực của các câu trả lời. Các chỉ số như vậy được hỗ trợ và khuyến khích đầy đủ trong DSPy.

Các teleprompter có thể được kết hợp bằng cách chỉ định một chương trình giáo viên. DSPy sẽ lấy mẫu các minh chứng từ chương trình này để tối ưu hóa prompt. Sự kết hợp này có thể cho phép các pipeline rất phong phú, nơi các chương trình đắt đỏ (ví dụ, các ensemble phức tạp đắt đỏ sử dụng LM lớn) giám sát các chương trình rẻ (ví dụ, các pipeline đơn giản sử dụng LM nhỏ hơn). Người ta có thể bắt đầu với compiled rag từ trên (giả sử, được biên dịch để sử dụng một LM Llama2-13b-chat lớn) nhưng giờ tinh chỉnh Flan-T5-large để tạo một chương trình hiệu quả:

1# Tập lớn hơn các câu hỏi *không có nhãn*. Nhãn cho tất cả các bước sẽ được khởi tạo.
2unlabeled_questions = [dspy.Example(question="Thủ đô của Đức là gì?"), ...]
3
4# Vì chúng ta giả định không có answer, chúng ta sử dụng 'answer_passage_match' để lọc các câu trả lời không có căn cứ.
5finetuning_teleprompter = BootstrapFinetune(metric=dspy.evaluate.answer_passage_match)
6
7# Chúng ta đặt 'teacher=compiled_rag' để kết hợp. Khởi tạo giờ sẽ sử dụng 'compiled_rag'.
8compiled_rag_via_finetune = finetuning_teleprompter.compile(RAG(), teacher=compiled_rag,
trainset=unlabeled_questions, target='google/flan-t5-large')

4 TRÌNH BIÊN DỊCH DSPY

Một nguồn sức mạnh biểu cảm quan trọng của DSPy là khả năng biên dịch - hoặc tự động tối ưu hóa - bất kỳ chương trình nào trong mô hình lập trình này. Biên dịch dựa trên một teleprompter, là một bộ tối ưu cho các chương trình DSPy cải thiện chất lượng (hoặc chi phí) của các module thông qua nhắc nhở hoặc tinh chỉnh, được thống nhất trong DSPy. Mặc dù DSPy không thực thi điều này khi tạo các teleprompter mới, các teleprompter điển hình trải qua ba giai đoạn.

Giai đoạn 1: Tạo Ứng viên Trình biên dịch đầu tiên (đệ quy) tìm tất cả các module Predict duy nhất (predictors) trong một chương trình, bao gồm những module được lồng dưới các module khác. Đối với mỗi predictor duy nhất p, teleprompter có thể tạo ra các giá trị ứng viên cho các tham số của p: các hướng dẫn, mô tả trường, hoặc - quan trọng nhất - các minh chứng (tức là các cặp ví dụ đầu vào-đầu ra). Trong lần lặp này của DSPy, chúng tôi tập trung vào các minh chứng và thấy rằng các phương pháp giống như rejection-sampling đơn giản có thể giúp khởi tạo các hệ thống đa giai đoạn rất hiệu quả.

Xem xét teleprompter không tự nhiên đơn giản nhất trong DSPy, BootstrapFewShot (pseudocode đơn giản hóa trong Phụ lục E.1). Teleprompter này sẽ mô phỏng một chương trình giáo viên (hoặc, nếu không được đặt, phiên bản zero-shot của chương trình đang được biên dịch) trên một số đầu vào huấn luyện, có thể một hoặc nhiều lần với nhiệt độ cao. Khi chạy trong chế độ biên dịch, các dấu vết đa giai đoạn được theo dõi một cách minh bạch và an toàn cho luồng trong suốt quá trình thực thi. Chỉ số của chương trình được sử dụng để lọc các dấu vết đa giai đoạn mà cùng nhau giúp pipeline vượt qua chỉ số. Do đó chúng ta có được các nhãn tiềm năng cho tất cả các chữ ký trong chương trình bằng cách loại bỏ các ví dụ xấu và sử dụng các ví dụ tốt như các minh chứng tiềm năng, mặc dù các quyết định thiết kế này dưới sự kiểm soát của người dùng.

Mặc dù LM có thể rất không đáng tin cậy, chúng tôi thấy chúng có thể khá hiệu quả trong việc tìm kiếm không gian các giải pháp cho các thiết kế đa giai đoạn. Một chương trình được phân tách tốt thường có thể tìm thấy ít nhất một số ví dụ huấn luyện nơi LM có thể vượt qua các ràng buộc được thực thi bởi các chữ ký và chỉ số, cho phép chúng ta khởi tạo lặp đi lặp lại nếu cần.

Giai đoạn 2: Tối ưu hóa Tham số Giờ mỗi tham số có một tập rời rạc các ứng viên: minh chứng, hướng dẫn, v.v. Nhiều thuật toán điều chỉnh siêu tham số (ví dụ, tìm kiếm ngẫu nhiên hoặc Tree-structured Parzen Estimators như trong HyperOpt (Bergstra et al., 2013) và Optuna (Akiba et al., 2019)) có thể được áp dụng để lựa chọn giữa các ứng viên. Chúng tôi báo cáo các triển khai đơn giản hóa của BootstrapFewShotWithRandomSearch và BootstrapFewShotWithOptuna của DSPy trong Phụ lục E.2 và Phụ lục E.3.

Một loại tối ưu hóa khác là tinh chỉnh với BootstrapFinetune, nơi các minh chứng được sử dụng để cập nhật trọng số LM cho mỗi predictor. Khi điều này được áp dụng, tham số LM của mỗi module được cập nhật thành trọng số LM mới. Thường, chúng ta đang tối ưu hóa chất lượng trung bình bằng cách sử dụng chỉ số với cross-validation trên tập huấn luyện hoặc một tập xác thực. Điều này có thể áp dụng ngay cả khi không có nhãn cho bất kỳ giai đoạn nào, tùy thuộc vào bản chất của chỉ số.

Giai đoạn 3: Tối ưu hóa Chương trình Bậc cao Một loại tối ưu hóa khác mà trình biên dịch DSPy hỗ trợ là sửa đổi luồng điều khiển của chương trình. Một trong những hình thức đơn giản nhất của những điều này là ensemble, mà chúng tôi sử dụng trong các nghiên cứu tình huống trong công trình này. Một ensemble sẽ khởi tạo nhiều bản sao của cùng một chương trình, và sau đó thay thế chương trình bằng một chương trình mới chạy tất cả chúng song song và giảm dự đoán của chúng thành một với một hàm tùy chỉnh (ví dụ, bỏ phiếu đa số). Trong công trình tương lai, giai đoạn này có thể dễ dàng chứa các kỹ thuật cho việc khởi tạo động hơn (tức là thời gian kiểm tra) cũng như logic backtracking tự động.

5 MỤC TIÊU ĐÁNH GIÁ

Các framework lập trình có thể được đánh giá theo nhiều chiều: hiệu quả tính toán, hiệu quả nhà phát triển, tính trực quan của mã và khái niệm, và nhiều hơn nữa. Trong bài báo này, chúng tôi tập trung vào có lẽ vấn đề cấp bách nhất đối với các pipeline LM hiện tại: vai trò của các prompt cụ thể cho tác vụ, viết tay trong việc đạt được các hệ thống hiệu suất. Các đánh giá của chúng tôi tìm cách kiểm tra các giả thuyết sau:

H1 Với DSPy, chúng ta có thể thay thế các chuỗi prompt thủ công bằng các module ngắn gọn và được định nghĩa rõ ràng, mà không giảm chất lượng hoặc sức mạnh biểu cảm.

H2 Tham số hóa các module và xử lý prompting như một vấn đề tối ưu hóa làm cho DSPy tốt hơn trong việc thích ứng với các LM khác nhau, và nó có thể vượt trội hơn các prompt do chuyên gia viết.

H3 Tính mô-đun kết quả làm cho có thể khám phá kỹ lưỡng hơn các pipeline phức tạp có các đặc tính hiệu suất hữu ích hoặc phù hợp với các chỉ số tinh tế.

Đánh giá của chúng tôi sẽ khám phá các giả thuyết này bằng cách sử dụng các cặp tác vụ-chương trình đa dạng. Chúng tôi hy vọng điều này bắt đầu một sự chuyển đổi từ các câu hỏi không được chỉ định rõ như "các LM khác nhau so sánh như thế nào trên GSM8K" hướng tới "chúng so sánh như thế nào trên GSM8K với chương trình P khi được biên dịch với chiến lược S", đây là một chạy được định nghĩa rõ và có thể tái tạo. Cuối cùng, mục tiêu của chúng tôi là giảm vai trò của việc xây dựng prompt khéo léo trong AI hiện đại có lợi cho việc phát triển các chương trình và bộ tối ưu mô-đun, có thể kết hợp mới.

--- TRANG 6 ---
Bản thảo

Bảng 1: Kết quả với học trong ngữ cảnh trên các bài toán từ toán học GSM8K. Mỗi hàng đại diện cho một pipeline riêng biệt: module trong cột Program được biên dịch đối với các ví dụ trong tập Training. Các chương trình, trình biên dịch, và (nhỏ) tập huấn luyện được định nghĩa trong Phần 6. Các hàng với ensemble xây dựng trên hàng ngay trước đó. Đáng chú ý, tất cả các chương trình trong bảng này được biểu đạt bằng cách kết hợp hai đến bốn module DSPy và teleprompter. Biên dịch các module đúng, thay vì prompt chuỗi, cải thiện các LM khác nhau từ độ chính xác 4-20% lên 49-88%.

GPT-3.5 Llama2-13b-chat
Program Compilation Training Dev Test Dev Test
vanilla none n/a 24.0 25.2 7.0 9.4
fewshot trainset 33.1 – 4.3 –
bootstrap trainset 44.0 – 28.0 –
bootstrap ×2 trainset 64.7 61.7 37.3 36.5
+ensemble trainset 62.7 61.9 39.0 34.6
CoT none n/a 50.0 – 26.7 –
fewshot trainset 63.0 – 27.3 –
fewshot +humanCoT 78.6 72.4 34.3 33.7
bootstrap trainset 80.3 72.9 43.3 –
+ensemble trainset 88.3 81.6 43.7 –
reflection none n/a 65.0 – 36.7 –
fewshot trainset 71.7 – 36.3 –
bootstrap trainset 83.0 76.0 44.3 40.2
+ensemble trainset 86.7 – 49.0 46.9

6 NGHIÊN CỨU TÌNH HUỐNG: CÁC BÀI TOÁN TỪ TOÁN HỌC

Chúng tôi đánh giá trên bộ dữ liệu GSM8K phổ biến với các câu hỏi toán học cấp tiểu học (Cobbe et al., 2021). Chúng tôi lấy mẫu 200 và 300 cặp câu hỏi-trả lời từ tập huấn luyện chính thức cho huấn luyện và phát triển, tương ứng. Các đánh giá cuối cùng của chúng tôi sử dụng 1.3k ví dụ tập kiểm tra chính thức. Chúng tôi báo cáo các so sánh mở rộng trên tập phát triển để tránh overfitting trên kiểm tra. Theo công trình trước đó trên GSM8K, chúng tôi đánh giá độ chính xác của giá trị số cuối cùng xuất hiện trong đầu ra LM.

Các Chương trình Được Xem xét Đối với tác vụ này, chúng tôi xem xét ba chương trình DSPy đơn giản: một module Predict một bước (vanilla), một module ChainOfThought hai bước (CoT), và cuối cùng là một module ComparerOfThoughts đa giai đoạn (ThoughtReflection). Những điều này được định nghĩa đầy đủ bởi mã sau:

1vanilla = dspy.Predict("question -> answer") # GSM8K Program 'vanilla'
2
3CoT = dspy.ChainOfThought("question -> answer") # GSM8K Program 'CoT'

1class ThoughtReflection(dspy.Module):
2def __init__(self, num_attempts):
3 self.predict = dspy.ChainOfThought("question -> answer", n=num_attempts)
4 self.compare = dspy.MultiChainComparison('question -> answer', M=num_attempts)
5
6def forward(self, question):
7 completions = self.predict(question=question).completions
8 return self.compare(question=question, completions=completions)
9
10reflection = ThoughtReflection(num_attempts=5) # GSM8K Program 'reflection'

Trong reflection, năm chuỗi lý luận được lấy mẫu từ LM (cùng với các câu trả lời của chúng) và chúng được so sánh song song bởi một module MultiChainComparison tích hợp, tổng quát hóa Yoran et al. (2023). Điều này tạo ra một câu trả lời mới có tính đến các mẫu từ năm lần thử. Quan trọng, các module được sử dụng đều là chung, không có module nào cụ thể cho các bài toán toán học hoặc LM cụ thể.

Biên dịch Như chúng tôi đã thảo luận trong Phần 4, các chương trình DSPy có thể được biên dịch thành các chương trình mới, được tối ưu hóa. Trong các thí nghiệm của chúng tôi, chúng tôi đánh giá các chương trình zero-shot (không biên dịch) cũng như một số chiến lược để biên dịch. Trình biên dịch đơn giản nhất của chúng tôi là LabeledFewShot:

1fewshot = dspy.LabeledFewShot(k=8).compile(program, trainset=trainset)

Ở đây, program có thể là bất kỳ module DSPy nào. Điều này chỉ đơn giản lấy mẫu k=8 minh chứng ngẫu nhiên từ trainset cho các trường chung cho các ví dụ huấn luyện và (các) chữ ký, trong trường hợp này, question và answer, nhưng không phải lý luận chẳng hạn. Chúng tôi báo cáo trung bình của 3-5 lần chạy (tùy thuộc vào cài đặt) khi áp dụng việc lấy mẫu ngẫu nhiên như vậy.

Tiếp theo, chúng tôi cũng xem xét việc khởi tạo các ví dụ few-shot với tìm kiếm ngẫu nhiên:

1tp = BootstrapFewShotWithRandomSearch(metric=gsm8k_accuracy)
2bootstrap = tp.compile(program, trainset=trainset, valset=devset)

Điều này sẽ tạo ra các chuỗi minh chứng cho các ví dụ trong tập huấn luyện và tối ưu hóa việc lựa chọn minh chứng (từ tập này) để tự cải thiện các module của chương trình. Như tên gọi, điều này được thực hiện với tìm kiếm ngẫu nhiên, xử lý việc lựa chọn minh chứng như một tham số để tối ưu hóa.

Tiếp theo, nếu mong muốn, quá trình khởi tạo này có thể được lồng trong DSPy. Đặc biệt, chúng ta có thể sử dụng chương trình bootstrap tối ưu để khởi tạo thêm một chương trình khác. Điều này có liên quan, ví dụ, bất cứ khi nào chương trình zero-shot ban đầu hoạt động tương đối kém.

1bootstrap2 = tp.compile(program, teacher=bootstrap, trainset=trainset, valset=devset)

Và cuối cùng, chúng tôi xem xét việc ensemble các bootstrap này:

1# Một chương trình ensemble 7 chương trình ứng viên hàng đầu từ một lần chạy trình biên dịch khởi tạo (đặc biệt là 'bootstrap' hoặc, khi có thể áp dụng, 'bootstrap2') với bỏ phiếu đa số.
2ensemble = Ensemble(reduce_fn=dspy.majority).compile(bootstrap.programs[:7])

GSM8K bao gồm các chuỗi lý luận của con người. Ở trên, trainset không bao gồm các chuỗi lý luận này. Chúng tôi cũng đánh giá với trainset humanCoT, mở rộng các ví dụ trong trainset với chuỗi lý luận của con người. Hai bộ dữ liệu này có thể được sử dụng thay thế cho nhau như giá trị cho tham số trainset ở trên. Chúng tôi lưu ý ở đây rằng biên dịch thường chạy trong khoảng vài phút (hoặc hàng chục phút) vì ngay cả các cài đặt đắt đỏ hơn chỉ yêu cầu chạy chương trình một vài nghìn lần (ví dụ, 10-20 thử nghiệm trên 150-300 ví dụ xác thực) và chúng có thể xảy ra song song.

Kết quả Kết quả của chúng tôi được tóm tắt trong Bảng 1, bao gồm kết quả dev cũng như đánh giá của chúng tôi về các đại diện hứa hẹn của mỗi phương pháp trên tập kiểm tra. Đầu tiên, kết quả chương trình vanilla cho thấy rằng GPT-3.5 và llama2-13b-chat gặp khó khăn với các bài toán từ toán học khi chúng phải dự đoán câu trả lời trực tiếp, tức là không sử dụng chuỗi lý luận trước. Điều này rõ ràng nhất trong việc không có minh chứng tốt, có thể được thấy trong cài đặt biên dịch none (tức là hướng dẫn zero-shot) và cài đặt fewshot (tức là lấy mẫu các cặp câu hỏi-trả lời ngẫu nhiên). Thú vị, tuy nhiên, vanilla được giúp đỡ đáng kể bởi việc biên dịch với bootstrap và bằng cách lặp lại quá trình này thành bootstrap ×2. Khi kiểm tra các prompt được khởi tạo (Phụ lục F), chúng ta thấy rằng prompt cho phép LM tận dụng trường answer để lý luận trước, điều này được cho phép vì chỉ số trích xuất giá trị số cuối cùng để đánh giá.

Tiếp theo, chúng tôi xem xét chương trình CoT. Mặc dù các chuỗi lý luận của chuyên gia con người (+humanCoT) cung cấp một tăng cường lớn khi có sẵn, chúng ta có thể khớp hoặc vượt qua điều này bằng cách sử dụng bootstrap, củng cố giả thuyết của chúng tôi rằng DSPy có thể cắt giảm nhu cầu về các prompt thủ công. Ngoài điều này, chúng ta thấy rằng chương trình reflection, mặc dù chỉ dài hơn vài dòng so với các chương trình khác, là một người chiến thắng rõ ràng, mặc dù CoT khá hiệu quả với ensemble. Nhìn chung, thủ tục biên dịch bootstrap dẫn đến các lợi ích lớn cho mọi chương trình, qua cả hai LM. Thực vậy, tất cả các chương trình trong bảng này được biểu đạt bằng cách kết hợp hai đến bốn module DSPy và teleprompter, và chúng tiết lộ tổng thể rằng - trong paradigm mới được quy định bởi DSPy - đó là việc kết hợp các module chung đúng, thay vì thao tác prompt chuỗi, cải thiện các LM khác nhau từ độ chính xác 4-20% lên 49-88%.

Chúng ta có thể so sánh một cách không chính thức với những điều sau. Zhang et al. (2022) báo cáo 48% cho text-davinci-002, phù hợp chặt chẽ với kết quả llama2-13b-chat của chúng tôi, và báo cáo 59.4% với codex khi sử dụng phương pháp CoT thủ công và 62.8% với phương pháp CoT tự động. Wang et al. (2022b) báo cáo 57% cho prompting CoT với PaLM 540-B, trở thành 74% khi thêm self-consistency. Các tác giả Llama2 (Touvron et al., 2023) trình bày 28.7% cho llama2-13b, 42.2% cho llama2-34b, và 56.8% cho llama2-70b. Thú vị, chương trình của chúng tôi với biến thể 13b của mô hình có khả năng cạnh tranh với kết quả dựa trên 34b của họ mặc dù chúng tôi không sử dụng các chuỗi lý luận của con người trong chương trình của mình. Zhao et al. (2023b) báo cáo 80.8% cho CoT với gpt-3.5-turbo từ tháng 4 năm 2023. Các tác giả GPT-4 (OpenAI, 2023) báo cáo rằng GPT-3.5 đạt 57.1% và GPT-4 nâng điều này lên 92% nhưng họ lưu ý rằng GPT-4 thực sự được tiền huấn luyện trên một tập con của tập huấn luyện GSM8K.

--- TRANG 7 ---
Bản thảo

7 NGHIÊN CỨU TÌNH HUỐNG: TRẢ LỜI CÂU HỎI PHỨC TẠP

Trong nghiên cứu tình huống này, chúng tôi khám phá tác vụ trả lời câu hỏi đa bước với bộ dữ liệu HotPotQA (Yang et al., 2018) trong cài đặt miền mở "fullwiki". Đối với truy xuất, chúng tôi sử dụng một chỉ mục tìm kiếm của bản dump "abstracts" Wikipedia 2017 chính thức của HotPotQA. Tìm kiếm được tiến hành bởi một retriever ColBERTv2 (Santhanam et al., 2021). Tập kiểm tra HotPotQA bị ẩn, vì vậy chúng tôi dành tập xác thực chính thức cho việc kiểm tra của chúng tôi, và lấy mẫu 1000 ví dụ cho điều đó. Chúng tôi chia tập huấn luyện thành các phần chia train/validation 70%/30%. Trong phần chia huấn luyện (và do đó xác thực), chúng tôi chỉ giữ các ví dụ được đánh dấu là "hard" trong bộ dữ liệu gốc, phù hợp với việc chỉ định của các tập xác thực và kiểm tra chính thức. Để huấn luyện và để báo cáo kết quả phát triển, chúng tôi lấy mẫu 200 và 300 ví dụ tương ứng.

Các Chương trình Được Xem xét Baseline đơn giản nhất của chúng tôi là chương trình vanilla được sử dụng trong nghiên cứu tình huống trước đó trên GSM8K (Phần 6); chữ ký "question -> answer" đủ chung để nó sẽ hoạt động cho tác vụ này (và nhiều tác vụ khác) khi được biên dịch thích hợp.

Chương trình RAG baseline của chúng tôi là chương trình được đưa ra trong Phần 3.2 như một ví dụ đơn giản về RAG với một lớp dspy.ChainOfThought. Chúng ta sẽ thấy rằng chương trình này không xuất sắc trên HotPotQA, và điều này thúc đẩy chúng tôi đánh giá hai chương trình đa bước.

Để đạt được điều đó, chúng tôi đầu tiên kiểm tra ReAct (Yao et al., 2022), một agent đa bước cho việc sử dụng công cụ, được triển khai như một module tích hợp trong DSPy. Trong trường hợp đơn giản nhất, một module ReAct cho một chữ ký cụ thể có thể được khai báo như sau trong DSPy:

1react = dspy.ReAct("question -> answer", tools=[dspy.Retrieve(k=1)], max_iters=5)

Chúng tôi cũng kiểm tra chương trình tùy chỉnh sau, mô phỏng luồng thông tin trong Baleen (Khattab et al., 2021a) và IRRR (Qi et al., 2020) và có những điểm tương đồng với IRCoT (Trivedi et al., 2022).

1class BasicMultiHop(dspy.Module):
2def __init__(self, passages_per_hop):
3 self.retrieve = dspy.Retrieve(k=passages_per_hop)
4 self.generate_query = dspy.ChainOfThought("context, question -> search_query")
5 self.generate_answer = dspy.ChainOfThought("context, question -> answer")
6
7def forward(self, question):
8 context = []
9
10 for hop in range(2):
11 query = self.generate_query(context=context, question=question).search_query
12 context += self.retrieve(query).passages
13
14 return self.generate_answer(context=context, question=question)
15
16multihop = BasicMultiHop(passages_per_hop=3)

Biên dịch Đối với các trình biên dịch, chúng tôi tiếp tục sử dụng những trình biên dịch mà chúng tôi đã sử dụng cho GSM8K (xem Phần 6). Chúng tôi cũng xem xét hai kết hợp của các teleprompter của chúng tôi. Đối với ReAct, chúng tôi xem xét việc khởi tạo với BootstrapFewShotWithRandomSearch bắt đầu từ một bootstrap trước đó của chương trình ReAct. Đối với chương trình multihop đơn giản, chúng tôi cũng xem xét tinh chỉnh với T5-Large bắt đầu từ bootstrap trước đó của chương trình đó.

1multihop_t5 = dspy.BootstrapFinetune(metric=answer_exact_match).compile(program,
teacher=bootstrap, trainset=trainset, target='t5-large')

Kết quả Bảng 2 tóm tắt kết quả của chúng tôi. So với việc nhắc nhở few-shot vanilla, một chương trình chain-of-thought và retrieval-augmented generation (CoTRAG) có thể tự khởi tạo trong DSPy để tăng answer EM đáng kể. Tuy nhiên, điều này hoàn toàn dựa vào retriever ColBERTv2 để tìm các đoạn văn liên quan trực tiếp từ các câu hỏi gốc, hạn chế khả năng thu hồi đoạn văn của nó. Điều này được giải quyết trong các chương trình react và multihop, sẽ tạo ra các truy vấn cho retriever trong nhiều "hop" lặp đi lặp lại. Thực vậy, nhìn chung, một chương trình multihop đơn giản hoạt động tốt nhất, và nói chung bootstrap lại một lần nữa chứng tỏ rất hiệu quả trong việc nâng chất lượng của nó so với biến thể fewshot cho cả hai LM.

Đặc biệt, chúng ta có thể thấy rằng bootstrap (và/hoặc bootstrap ×2) có thể vượt trội hơn cả việc nhắc nhở fewshot (cho multihop) và lý luận của chuyên gia con người (cho react; được điều chỉnh một chút từ Yao et al. (2022) cho cài đặt truy xuất của chúng tôi). Có lẽ quan trọng nhất, chúng ta có thể làm cho llama2-13b-chat cạnh tranh với GPT-3.5 bằng cách đơn giản biên dịch các chương trình của chúng tôi.

Để đánh giá khả năng tinh chỉnh của DSPy, chúng tôi cũng đánh giá trình biên dịch multihop t5 được định nghĩa ở trên tạo ra một mô hình T5-Large (770M tham số). Chương trình này đạt 39.3% answer EM và 46.0% passage accuracy trên tập dev, chỉ sử dụng 200 đầu vào có nhãn và 800 câu hỏi không có nhãn.

--- TRANG 8 ---
Bản thảo

Bảng 2: Kết quả với học trong ngữ cảnh trên HotPotQA trả lời câu hỏi truy xuất đa bước. Chúng tôi báo cáo exact match của câu trả lời (Ans) và độ chính xác truy xuất cặp (Psg). Mỗi hàng đại diện cho một pipeline riêng biệt: module trong cột Program được biên dịch đối với các ví dụ trong tập Training. Các chương trình, trình biên dịch, và (nhỏ) tập huấn luyện được định nghĩa trong văn bản chính. Đối với HotPotQA, chúng tôi sử dụng tập huấn luyện (và không phải dev) trực tiếp cho cross-validation. ∗Kết quả được đánh dấu được đánh giá trên 50% tập kiểm tra của chúng tôi do chi phí.

GPT-3.5 Llama2-13b-chat
Program Compiler Dev Test Dev Test
Ans Psg Ans Psg Ans Psg Ans Psg
vanilla fewshot 34.3 n/a 31.5 n/a 27.5 n/a 21.8 n/a
CoTRAG fewshot 36.4 36.0 29.8 34.4 34.5 36.0 28.0 34.4
bootstrap 42.3 36.0 – – 38.3 36.0 32.9 34.4
react none 20.3 – – – 20.0 – – –
+human r 33.0 – – – 28.3 – – –
bootstrap 31.0 – – – 24.7 – – –
bootstrap ×2 39.0 – – – 40.0 – – –
multihop fewshot 36.9 38.3 31.2 40.8 34.7 32.0 31.3 30.8
bootstrap 48.7 47.0 39.6 43.8 42.0 48.3 36.4 43.5
ensemble 54.7 – 45.6∗ – 50.0 – 41.0 –

câu hỏi. Để biên dịch, chúng tôi sử dụng một chương trình giáo viên bao gồm một ensemble (union) của hai multihop với llama2-13b-chat. Xem xét kích thước cực nhỏ và tính khả dụng cục bộ của nó, chương trình được biên dịch này với T5-Large sẽ áp đặt chi phí thấp hơn hàng bậc độ lớn cho suy luận so với một LM độc quyền như GPT-3.5.

Kết quả của chúng tôi có thể được gắn với đánh giá trên HotPotQA trong một số bài báo gần đây, mặc dù có sự biến đổi đáng kể trong phương pháp đánh giá và mẫu tập kiểm tra qua các nghiên cứu trong không gian này. Sử dụng prompting CoT, Si et al. (2022) đạt 25.2% EM. Với kỹ thuật "recite-and-answer" sử dụng PaLM-62B (Chowdhery et al., 2022) để đọc thuộc lòng các đoạn văn bằng chứng, Sun et al. (2022) đạt 26.5% EM. Wang et al. (2022a) đạt 33.8% EM và 44.6% F1 khi áp dụng self-consistency cho PaLM-540B. Yao et al. (2022) đạt 27.4% EM sử dụng ReAct với PaLM-540B và 30.8 với text-davinci-002, với một công cụ cho nó khả năng tìm kiếm bằng cách sử dụng API Wikipedia. Họ đẩy kết quả PaLM của họ lên 35.1% EM bằng cách áp dụng một bước CoT bổ sung với self-consistency, có thể giống với phương pháp ensemble của chúng tôi theo nghĩa tổng hợp nhiều câu trả lời. Trivedi et al. (2022) báo cáo 49% sử dụng một pipeline với LM code-davinci-002 trên một mẫu 500 câu hỏi HotPotQA.

8 KẾT LUẬN

Bài báo này giới thiệu DSPy, một mô hình lập trình mới để thiết kế các hệ thống AI sử dụng các pipeline của LM được tiền huấn luyện và các công cụ khác. Chúng tôi trình bày ba khái niệm mới được giới thiệu trong trừu tượng này (DSPy signatures, modules, và teleprompters), và cho thấy trong hai nghiên cứu tình huống rất khác nhau rằng nó hỗ trợ phát triển nhanh chóng các hệ thống hiệu quả cao sử dụng các LM tương đối nhỏ. Chúng tôi đã duy trì các phiên bản mã nguồn mở của framework này trong gần một năm. Trong thời gian này, chúng tôi đã thấy và tạo ra một số lượng lớn các chương trình được biên dịch thành các hệ thống chất lượng cao bởi DSPy, trải dài các tác vụ từ trích xuất thông tin đến tạo dữ liệu tổng hợp tài nguyên thấp. Vì lợi ích của không gian và để duy trì phạm vi hợp lý trong bài báo này, chúng tôi để lại việc báo cáo về các tác vụ như vậy dưới điều kiện thực nghiệm kiểm soát cho công trình tương lai. Mặc dù học trong ngữ cảnh đã chứng tỏ là chuyển đổi trong 2-3 năm qua của nghiên cứu LM, chúng tôi lập luận rằng sức mạnh biểu cảm thực sự trong paradigm đang nổi lên này là trong việc xây dựng các đồ thị chuyển đổi văn bản tinh vi trong đó các module có thể kết hợp và bộ tối ưu (teleprompter) kết hợp để tận dụng LM theo những cách có hệ thống và đáng tin cậy hơn.

LỜI CẢM ƠN

Chúng tôi cảm ơn Josh Purtell vì đã gợi ý tên phù hợp "text transformation graph" cho trừu tượng đồ thị tính toán của DSPy. Chúng tôi cảm ơn Rick Battle, Igor Kotenkov, Lisa Li, David Hall, Ashwin Paranjape, Chris Manning, Percy Liang, và nhiều nhà nghiên cứu, nhà phát triển, và người dùng khác cho các cuộc thảo luận và phản hồi có giá trị. Chúng tôi cảm ơn Giuseppe Attanasio cho gist định dạng mã Python GitHub-style LaTeX công khai của anh ấy.6

Công trình này được hỗ trợ một phần bởi IBM như một thành viên sáng lập của Stanford Institute for Human-Centered Artificial Intelligence (HAI), Oracle, Virtusa, và Cigna Healthcare. Nó cũng được hỗ trợ một phần bởi một khoản tài trợ Azure compute của HAI. Nghiên cứu này được hỗ trợ một phần bởi các thành viên liên kết và những người ủng hộ khác của dự án Stanford DAWN – Facebook, Google, và VMware – cũng như NSF dưới khoản tài trợ CAREER CNS-1651570. Bất kỳ ý kiến, phát hiện, và kết luận hoặc khuyến nghị nào được biểu đạt trong tài liệu này là của các tác giả và không nhất thiết phản ánh quan điểm của National Science Foundation. Omar Khattab được hỗ trợ bởi học bổng Apple Scholars in AI/ML.

TÀI LIỆU THAM KHẢO

[Phần tài liệu tham khảo tiếp tục với danh sách các trích dẫn học thuật...]

--- TRANG 9 ---
[Nội dung tiếp tục với các trang còn lại của tài liệu, bao gồm các phụ lục chi tiết về signatures nâng cao, so sánh với các thư viện hiện có, ví dụ về các prompt lớn, modules, teleprompters, và các ví dụ về prompt được tạo tự động bởi DSPy...]
