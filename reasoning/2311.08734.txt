# 2311.08734.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/reasoning/2311.08734.pdf
# File size: 1180729 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Thread of Thought Unraveling Chaotic Contexts
Yucheng Zhou1∗, Xiubo Geng2, Tao Shen3, Chongyang Tao2,
Guodong Long3,Jian-Guang Lou2†,Jianbing Shen1†
1SKL-IOTSC, CIS, University of Macau,
2Microsoft Corporation,3AAII, FEIT, University of Technology Sydney
yucheng.zhou@connect.um.edu.mo, {xigeng,chongyang.tao,jlou}@microsoft.com
{tao.shen, guodong.long}@uts.edu.au, jianbingshen@um.edu.mo
Abstract
Large Language Models (LLMs) have ushered
in a transformative era in the field of natu-
ral language processing, excelling in tasks re-
lated to text comprehension and generation.
Nevertheless, they encounter difficulties when
confronted with chaotic contexts (e.g., distrac-
tors rather than long irrelevant context), lead-
ing to the inadvertent omission of certain de-
tails within the chaotic context. In response to
these challenges, we introduce the “Thread of
Thought” (ThoT) strategy, which draws inspi-
ration from human cognitive processes. ThoT
systematically segments and analyzes extended
contexts while adeptly selecting pertinent in-
formation. This strategy serves as a versatile
“plug-and-play” module, seamlessly integrating
with various LLMs and prompting techniques.
In the experiments, we utilize the PopQA and
EntityQ datasets, as well as a Multi-Turn Con-
versation Response dataset (MTCR) we col-
lected, to illustrate that ThoT significantly im-
proves reasoning performance compared to
other prompting techniques.
1 Introduction
Large Language Models (LLMs) represent a sig-
nificant advancement in the field of artificial in-
telligence. They have achieved notable accom-
plishments in natural language understanding and
generation (Brown et al., 2020; Wei et al., 2022).
The development of LLMs has had a far-reaching
impact, drawing significant attention in academia.
These models demonstrate proficiency in a wide ar-
ray of natural language processing tasks, including
sentiment analysis (Zhang et al., 2023), machine
translation (Moslem et al., 2023), and summariza-
tion (Tam et al., 2023). Moreover, they exert a
profound influence across various industries and
offer promising solutions for intricate issues, such
∗Work is done during internship at Microsoft.
†Corresponding author.as aiding in legal consultations (Yue et al., 2023)
and assisting in medical diagnostics (Wang et al.,
2023a).
With the growing complexity and diversity of
tasks demanding extensive information process-
ing and reasoning, particularly in the context of
Retrieval-Augmented Generation (RAG) (Lewis
et al., 2020) and conversational (Xu et al., 2022)
scenarios, the input text often comprises a wealth
of information from various sources, including user
queries, conversation history, external knowledge
bases, and more. This information may be inter-
connected or entirely unrelated. Moreover, the sig-
nificance of this information can fluctuate based
on the context, with certain pieces being critical
for addressing specific questions and others being
extraneous. This situation can aptly be character-
ized as a “Chaotic Context”. Similar to but distinct
from “Long Context”, “Chaotic Context” under-
scores the complexity and volume of information,
going beyond the mere length of the context. More-
over, Liu et al. (2023) found that existing LLMs of-
ten encounter difficulties in effectively identifying
relevant information from the context augmented
through retrieval, particularly when it is located in
the middle position.
Recent studies (Xu et al., 2023; Jiang et al.,
2023) have proposed various solutions to enhance
the performance of LLMs in long-context sce-
narios, upon an intuition of input capacity opti-
mization. Xu et al. (2023) proposes a method to
compare and combine retrieval-augmentation and
long context extension for large language mod-
els (LLMs) on various long text tasks. How-
ever, this approach necessitates the retraining of
LLMs with positional interpolation. In addition,
Jiang et al. (2023) introduces LongLLMLingua, a
method that streamlines the input prompt by culling
irrelevant or redundant information. Nonetheless,
this method mandates the fine-tuning of auxiliary
models (e.g., LLaMA-7B (Touvron et al., 2023a))arXiv:2311.08734v1  [cs.CL]  15 Nov 2023

--- PAGE 2 ---
Walk  me through  this context  in manageable  
parts  step by step, summarizing  and 
analyzing  as we go.
Let's think step by step.Chain of Thought
As a content reviewer, I provide multiple retrieved passages about 
this question; you need to answer the question.
retrieved Passage 1 is: to introduce book vending machines, …
retrieved Passage 2 is: of the Carlsbad Decrees, it quickly …
retrieved Passage 3 is: It is at " Kreuzstrae  12", opposite …
retrieved Passage 4 is: house in Leipzig remained there, …
retrieved Passage 5 is: Delcam  Delcam  is a supplier of …
retrieved Passage 6 is: inhabitants are called recreinos . …
retrieved Passage 7 is: 'politically unreliable' writers like …
retrieved Passage 8 is: Recheio  Recheio  is the biggest …
retrieved Passage 9 is: via the Internet, large volumes of …
retrieved Passage 10 is: without needing to pay any royalties, …
Q: Where was Reclam  founded?
{Prompting Method}
A:Input
Thread of Thought
Prompting Method
Reclam  was founded  by Anton  Philipp  Reclam . 
According  to retrieved  Passage  2, he founded  his own 
publishing  house  called  "Verlag  des literarischen  
Museums"  in Carlsbad,  which  quickly  became  a center  of 
intellectual  and literary  circles .
According  to retrieved  Passage  2, Anton  Philipp  Reclam  
founded  his own publishing  …
In retrieved  Passage  4, it is mentioned  that the Reclam  
publishing  house  in Leipzig  …
Finally,  in retrieved  Passage  7, it is stated  that after the 
partition  of Germany  …
Therefore,  Reclam  was originally  founded  in Leipzig , 
Germany,  and later moved  its main  office  to Stuttgart .Output
Output
Figure 1: Thread of Thought prompting enables large language models to tackle chaotic context problems. In the
output depicted, green text denotes the correct answer, while red text indicates the erroneous prediction.
for prompt compression. The utility of these aux-
iliary models may prove insufficient in address-
ing unknown or intricate content, and it imposes
limitations on the length of text that can be ef-
fectively processed. Moreover, its non-end-to-end
framework can lead to error propagation. In con-
trast, chain-of-thought (CoT) prompting (Wei et al.,
2022) can enhance a model’s reasoning ability with-
out requiring any retraining or fine-tuning of LLMs.
However, due to the massive amount of informa-
tion contained within chaotic contexts, CoT still
encounters information missing in reasoning, as
shown in Figure 1.
To address these challenges, we introduce the
“Thread of Thought” (ThoT) strategy. ThoT, draw-
ing inspiration from human cognitive processes, en-
ables Large Language Models (LLMs) to methodi-
cally segment and analyze extended contexts. This
segmentation enhances the extraction of pertinent
content for responding to queries. ThoT represents
the unbroken continuity of ideas that individuals
maintain while sifting through vast information,
allowing for the selective extraction of relevant de-
tails and the dismissal of extraneous ones. This
balance of attention across a document’s sections
is crucial for accurately interpreting and respond-
ing to the information presented. Moreover, the
stepwise analysis and summarization of segmented
information improve comprehension over multiple
paragraphs and protect LLMs against misleading
yet seemingly relevant data.
In comparison to existing methods that require
complex multi-stage prompting (Zhou et al., 2023)
or multi-path sampling (Wang et al., 2023b), ThoT
is a simpler, more universal, and efficient solution.
It integrates seamlessly as a “plug-and-play” mod-
ule with various pre-trained language models and
prompting strategies, avoiding complex procedures.ThoT not only improves LLMs’ performance in
chaotic contexts but also enhances their reasoning
abilities.
To evaluate ThoT’s effectiveness in handling
chaotic contextual information, we used long-tail
question answering datasets, specifically PopQA
(Mallen et al., 2023) and EntityQ (Sciavolino et al.,
2021). These datasets feature knowledge often un-
familiar to large models, thereby reducing the im-
pact of their inherent knowledge retention on our re-
sults. Additionally, we construct a Multi-Turn Con-
versation Response (MTCR) dataset based on ev-
eryday conversations to further assess our method.
Comparative analyses with other prompting tech-
niques show that ThoT markedly improves reason-
ing performance, evidencing its effectiveness. We
also explored various prompts to determine optimal
prompting strategies.
2 Related Work
2.1 Long Context Large Language Models
Recent advancements in Large Language Models
(LLMs) have made significant strides in managing
extended contexts, moving beyond the limitations
of traditional pre-defined context windows. Ratner
et al. (2023) introduce the Parallel Context Win-
dows (PCW) method, which segments extensive
contexts into multiple windows, employing inde-
pendent attention mechanisms. Building on this
concept, Chen et al. (2023) facilitate substantially
longer context windows with minimal fine-tuning
by aligning position indices with the maximum po-
sition index from the pre-training phase. Moreover,
a different approach, LongNet, utilizes dilated at-
tention, allowing the attention field to expand expo-
nentially with distance (Ding et al., 2023). In addi-
tion, Xiao et al. (2023) underscore the phenomenon
of attention convergence, where maintaining the

--- PAGE 3 ---
Key-Value (KV) states of initial tokens significantly
enhances window attention performance. Lastly,
Press et al. (2022) introduce Attention with Linear
Biases (ALiBi), a method that biases the query-
key attention scores based on distance, achieving
comparable perplexity to models trained on longer
sequences. However, these methods predominantly
concentrate on long contexts. In contrast, chaotic
contexts are characterized by their overloaded in-
formation, often cluttered with numerous similar
and unrelated elements.
2.2 Reasoning with Large Language Models
Advancements in large language models (LLMs)
have significantly impacted AI, notably in complex
reasoning tasks. The enhancement of LLMs’ rea-
soning capabilities is exemplified in (Wei et al.,
2022), where Chain-of-Thought (CoT) prompting
is introduced. This method improves arithmetic,
common sense, and symbolic reasoning by gen-
erating intermediate steps. Building on this, the
Graph of Thoughts (GoT) framework conceptual-
izes LLM outputs as graphs, leading to notable
improvements in task performance and efficiency
(Besta et al., 2023). Extending the CoT concept,
Yao et al. (2023a) propose the Tree of Thoughts
(ToT) framework, which has shown remarkable
success in complex problem-solving tasks like the
24-point game. In addition, Zhou et al. (2023) in-
troduce the least-to-most prompting strategy, break-
ing down complex problems into simpler sub-
problems and showing effectiveness in tasks re-
quiring advanced symbolic manipulation. Lastly,
Yao et al. (2023b) explore non-linear thought pro-
cesses through GoT reasoning, outperforming the
linear CoT approach in both mathematical and fi-
nancial problem datasets. However, these methods
are effective but overlook chaotic context scenar-
ios.
2.3 Knowledge Following in Long Context
LLMs can process extensive input contexts, but
their performance significantly deteriorates when
extracting relevant information buried in these con-
texts, challenging their efficiency in managing long
contexts (Liu et al., 2023). To address deploy-
ing LLMs in streaming applications, Xiao et al.
(2023) introduce the StreamingLLM framework,
enabling LLMs with limited attention windows to
handle indefinitely long sequences without addi-
tional fine-tuning. Some study finds that retrieval
augmentation enables a 4K context window LLM
Reclam  was originally  founded  
in Leipzig
As a content reviewer, I provide multiple retrieved passages about 
this question; you need to answer the question.
retrieved Passage 1 is: to introduce book vending machines, …
retrieved Passage 2 is: of the Carlsbad Decrees, it quickly …
retrieved Passage 3 is: It is at " Kreuzstrae  12", opposite …
retrieved Passage 4 is: house in Leipzig remained there, …
retrieved Passage 5 is: Delcam  Delcam  is a supplier of …
retrieved Passage 6 is: inhabitants are called recreinos . …
retrieved Passage 7 is: 'politically unreliable' writers like …
retrieved Passage 8 is: Recheio  Recheio  is the biggest …
retrieved Passage 9 is: via the Internet, large volumes of …
retrieved Passage 10 is: without needing to pay any royalties, …
Q: Where was Reclam  founded?
Walk  me through  this context  in manageable  parts  step by step, 
summarizing  and analyzing  as we go.
A:Input
According  to retrieved  Passage  2, Anton  Philipp  Reclam  founded  
his own publishing  …
In retrieved  Passage  4, it is mentioned  that the Reclam  publishing  
house  in Leipzig  …
Finally,  in retrieved  Passage  7, it is stated  that after the partition  of 
Germany  …
Therefore,  Reclam  was originally  founded  in Leipzig , Germany,  
and later moved  its main  office  to Stuttgart .First Output
Therefore,  the answer :Second OutputFigure 2: Thread of Thought for zero-shot reasoning.
to equal the performance of a 16K context win-
dow LLM fine-tuned with positional interpolation
in long-context tasks, underscoring the potential of
retrieval methods in augmenting LLM capabilities
(Xu et al., 2023). Moreover, LongLLMLingua in-
troduces prompt compression to improve LLMs’
key information perception, significantly boosting
performance (Jiang et al., 2023).
3 Methodology
We present an innovative method for template-
based prompting that is specifically designed to
enhance Thread of Thought (ThoT) reasoning. This
novel strategy stands distinct from the traditional
chain of thought prompting (Wei et al., 2022), adept
at navigating through disordered contexts in which
the information may be either interwoven or dis-
parate. ThoT prompting can be seamlessly inte-
grated with a variety of existing language mod-
els and prompting techniques, offering a modular
“plug-and-play” improvement that eliminates the
need for elaborate prompting strategies or sampling
methods. Our approach’s underlying principle is
both simple and efficient, as exemplified in Fig-
ure 2: inserting “Walk me through this context in
manageable parts step by step, summarizing and an-
alyzing as we go” into the prompt facilitates ThoT
reasoning.
As illustrated in Figure 2, in contrast to Chain

--- PAGE 4 ---
of Thought (CoT) prompting, which struggles with
complex and chaotic contexts, ThoT prompting
adeptly maintains the logical progression of rea-
soning without being overwhelmed. While prompt
compressors and similar strategies have sought to
address these complexities, they often underper-
form with unfamiliar or particularly complex mate-
rial and typically necessitate significant modifica-
tions to the Large Language Models (LLMs), such
as retraining or fine-tuning with additional datasets
(Xu et al., 2023; Jiang et al., 2023). ThoT, however,
not only effectively manages chaotic contexts but
also simplifies the prompting process, requiring
just two prompting efforts compared to CoT.
3.1 First Step: Initiating the Reasoning
The initial prompt is designed to guide the LLM
through an analytical dissection of the context, us-
ing the directive “Walk me through this context in
manageable parts step by step, summarizing and
analyzing as we go”. Specifically, we employ a
template that incorporates the chaotic context X
and query Qinto the prompt Pas “[X] Q: [Q] [T]
A:”, where [T]denotes the trigger sentence tthat
initiates the reasoning process. For instance, utiliz-
ing “Walk me through this context in manageable
parts step by step, summarizing and analyzing as
we go” as the trigger, the prompt Pbecomes “[ X]
Q: [Q] Walk me through this context in manageable
parts step by step, summarizing and analyzing as
we go. A:”. This prompted text Pis then inputted
into an LLM, which generates the subsequent sen-
tences Z. This procedure is modeled after the cog-
nitive strategies humans employ when confronted
with complex information, breaking it down into
digestible segments, distilling key points, and nav-
igating through the material with sustained focus.
This incremental method fosters a more structured
and coherent line of reasoning, proving particularly
advantageous in chaotic contexts.
3.2 Second Step: Refining the Conclusion
The second prompt builds upon the structured
reasoning established earlier, employing another
prompt to distill the analysis into a definitive an-
swer. By leveraging the organized thought se-
quence initiated by the first prompt, this step aims
to succinctly capture the essence of the conclusion.
Specifically, we use a simple template to combine
the initial prompted text P, the response Z, and the
conclusion marker [A], as in “[ P] [Z][A]”, where
[A]signifies the trigger sentence designed to extract
As a writer,  your task is to continue  the following  dialogue . Start  by 
inferring  the possible  current  situation  of Speaker  2 based  on their persona . 
Then,  have  Speaker  1 ask a question  that indirectly  alludes  to Speaker  2's 
situation  without  directly  mentioning  the words  from  their persona . Finally,  
Speaker  2 should  respond,  incorporating  his persona  into the answer .
These  are the conversations  of the two speakers :
{Conversation}
These  are the personas  of the speaker  2:
{Speaker 2’spersonas}
Please  continue  the above  conversation,  with one persona,   one response  
from  Speaker  1 and one response  from  Speaker  2. Think  step-by-step, then 
explain .
The final output  includes  the persona  and dialogue  between  speaker  1 and 
speaker  2 in the following  format :
Persona : {persona}
S1: {response 1}
S2: {response 2} Generate Responds Based on Conversation and Persona
Extract Responds From Generated ConversationFigure 3: Prompt for MTCR Dataset Construction.
the answer, such as “Therefore, the answer:”. This
extraction prompt perpetuates the thought process,
prompting the model to sift through the analysis
and isolate the principal conclusion as the final an-
swer. The prompt’s design is a deliberate tactic to
sharpen the model’s focus, fostering precision and
explicitness in the response.
This two-tiered prompting system effectively ad-
dresses the limitations of prior methods while ob-
viating the need for intensive model retraining or
complex modifications. Our methodology not only
enhances the model’s capacity to navigate chaotic
contexts but also more closely aligns its reasoning
processes with human cognitive patterns.
4 Experiments
4.1 Experimental Settings
Dataset. We evaluated our method across two
chaotic context scenarios: retrieval-augmented gen-
eration and multi-turn conversation response. Our
assessment utilized three datasets: the PopQA
dataset (Mallen et al., 2023), the EntityQ dataset
(Sciavolino et al., 2021), and our own Multi-Turn
Conversation Response (MTCR) dataset. Specifi-
cally, the PopQA and EntityQ datasets, designed
to contain long-tail knowledge, were chosen to
minimize interference from the extensive internal
knowledge of large models, thereby facilitating a
more effective comparison of different methodolo-
gies. Distinct from the original PopQA and EntityQ
datasets, we randomly selected a test set of 1,000
samples for our analysis. For the evaluation of the
PopQA and EntityQ datasets, we adhered to the
original datasets’ metric, namely the exact match
(EM). Furthermore, the MTCR dataset, used to
assess multi-turn conversation response, was de-

--- PAGE 5 ---
Assessment  of the Quality  of Generated  Speaker 2's Response
Conversation  Content : 
{conversation}
Generated  Respond : 
{generated  speaker 2's response}
Persona :
{speaker 2's persona}
Comprehensive  Evaluation  Guide :
Score  each of the following  three  criteria  separately .
Relevance :
- 1 point : Not relevant ; the response  does not relate  to Speaker 1's dialogue .
- 2 points : Slightly  relevant ; the response  touches  on the subject  but misses  key 
points  or deviates  significantly .
- 3 points : Somewhat  relevant ; the response  is related  to Speaker 1's dialogue  but 
may miss some  nuances  or details .
- 4 points : Relevant ; the response  is on topic  and addresses  most  points  made  by 
Speaker 1.
- 5 points : Highly  relevant ; the response  is fully on topic,  directly  addresses  all 
elements  of Speaker 1's dialogue .
Accuracy :
- 1 point : Inaccurate ; the response  contains  significant  errors  or shows  
misunderstanding  of the topic .
- 2 points : Somewhat  inaccurate ; the response  contains  multiple  errors,  though  it 
grasps  the basic  idea.
- 3 points : Moderately  accurate ; the response  has minor  errors  but generally  
understands  the topic .
- 4 points : Mostly  accurate ; the response  contains  minimal,  inconsequential  errors .
- 5 points : Fully  accurate ; the response  is free from  errors  and fully understands  
the topic .
Persona  Representation :
- 1 point : No representation ; Speaker 2's persona  is not reflected  in the response .
- 2 points : Weak  representation ; Speaker 2's persona  is hinted  at but largely  absent  
or incorrect .
- 3 points : Adequate  representation ; Speaker 2's persona  is present  but some  traits  
may be missing  or not fully  captured .
- 4 points : Strong  representation ; Speaker 2's persona  is clear  and most  traits  are 
well represented .
- 5 points : Full representation ; Speaker 2's persona  is fully and accurately  
portrayed  throughout  the response .
Example  Output  Form :
Score :
Relevance  Score : {score}
Accuracy  Score : {score}
Persona  Representation  Score : {score}
Scoring  Rationale :
Relevance  Score : {scoring  rationale}
Accuracy  Score : {scoring  rationale}
Persona  Representation  Score : {scoring  rationale}Generate Responds Based on Conversation and PersonaFigure 4: Prompt Evaluation Metric for MTCR Dataset.
veloped based on the Multi-Session Chat (MSC)
dataset (Xu et al., 2022). The dataset construc-
tion involved sequentially using two prompts, as
shown in Figure 3. The input of prompts is the
MSC dataset’s conversation and Speaker2’s per-
sona to generate a response for Speaker1. During
the inference phase, the model was required to
consider the multi-turn conversation contextual de-
tails mentioned previously to generate a response
for speaker2, coping with the response created for
speaker1. Following this, a manual screening pro-
cess was conducted to eliminate samples that did
not meet certain criteria, such as persona content
leakage and irrelevance to the context or persona,
culminating in a refined selection of 304 samples.
For the MTCR dataset’s evaluation, we merge the
persona as a known condition along with the model-
generated response for Speaker2 in the prompt, as
depicted in Figure 4, and then pass them into GPT-4
(OpenAI, 2023), obtaining scoring.
Prompt. In the experimental comparison, we
consider four distinct prompts for retrieval-Method GPT-3.5-turbo LLaMA 2 Chat (70B)
Vanilla 0.398 0.330
Retrieval 0.475 0.510
CoT 0.482 0.525
ThoT 0.574 0.561
Table 1: Performance Comparison on PopQA.
Method GPT-3.5-turbo LLaMA 2 Chat (70B)
Vanilla 0.497 0.430
Retrieval 0.512 0.522
CoT 0.517 0.547
ThoT 0.565 0.559
Table 2: Performance Comparison on EntityQ.
augmented generation. (1) “Vanilla” entails using
the instruction and question as the prompt without
providing any retrieval results, i.e., “{instruction}
{question}.”. (2) “Retrieval” includes retrieval re-
sults within the prompt, formatted as “{instruction}
{retrieval results} {question}.”. (3) “CoT” (Chain
of Thought) incorporates the retrieval results and
appends the phrase “Let’s think step by step” to
the instruction and question, resulting in “{instruc-
tion} {retrieval results} {question} Let’s think step
by step.”. (4)“ThoT” (Thought-by-Thought) also
integrates retrieval results and follows a more de-
tailed prompt structure: “{instruction} {retrieval
results} {question} Walk me through this context
in manageable parts step by step, summarizing
and analyzing as we go.”. For the MTCR dataset,
we employ only the “Vanilla”, “CoT”, and “ThoT”
prompts. Their formats are, respectively: “{instruc-
tion} {conversation}”, “{instruction} Let’s think
step by step. {conversation}”, and “{instruction}
Walk me through this context in manageable parts
step by step, summarizing and analyzing as we go.
{conversation}”.
Language models. We evaluated four large-scale
language models: GPT-3.5-turbo (Schulman et al.,
2022), GPT-4 (OpenAI, 2023), LLaMA 2 Chat
(Touvron et al., 2023b), and Vicuna (Chiang et al.,
2023). Due to the GPT-3.5-turbo and GPT-4 are not
open-source, the details of their model parameters
remain undisclosed. For the LLaMA 2 Chat model,
we utilized variants with 7B, 13B, and 70B param-
eters in our experiments. Similarly, versions with
7B, 13B, and 33B parameters of the Vicuna model
were employed. Sampling from these models was
conducted using a greedy decoding strategy.

--- PAGE 6 ---
MethodGPT-3.5-turbo LLaMA 2 Chat (70B)
Relevance Accuracy Persona Average Relevance Accuracy Persona Average
Vanilla 3.211 3.135 3.345 3.230 2.819 2.901 2.914 2.878
CoT 3.352 3.220 3.349 3.307 2.783 2.806 2.882 2.823
ThoT 3.849 3.921 3.645 3.805 3.158 3.295 3.268 3.240
Table 3: Performance Comparison on MTCR dataset.
MethodPopQA EntityQ
GPT-4 GPT-3.5-turbo LLaMA 2 Chat (70B) GPT-4 GPT-3.5-turbo LLaMA 2 Chat (70B)
Vanilla 0.430 0.391 0.314 0.405 0.405 0.369
Retrieval 0.360 0.477 0.430 0.571 0.560 0.643
CoT 0.442 0.465 0.558 0.560 0.583 0.667
ThoT 0.651 0.674 0.663 0.643 0.667 0.702
Table 4: Study of “Lost in Middle” in PopQA and EntityQ.
4.2 Results
Tables 1 and Tables 2 show the performance of
retrieval-augmented generation. In PopQA and En-
tityQ datasets, we notice a consistent pattern where
the Thought-by-Thought (ThoT) prompt configu-
ration outperforms the other methods. The intro-
duction of CoT also demonstrates a positive effect,
indicating that prompting models to follow a me-
thodical problem-solving approach can improve
performance metrics. It is particularly notewor-
thy that ThoT exhibits a marked improvement in
results over the CoT configuration, highlighting
the efficacy of stepwise contextual processing in
enhancing the quality of generated responses. In
Tables 3, a similar trend emerges. ThoT retains its
lead, suggesting that its detailed prompt structure,
which encourages summarizing and analyzing in-
formation in a structured manner, is particularly
effective in complex conversational contexts. It
underscores the importance of a methodical break-
down of context in generating relevant, accurate,
and persona-consistent responses. The structured
approach of ThoT prompts, which guide the model
through a detailed, step-by-step analysis, consis-
tently yields the best performance across chaotic
contexts.
4.3 Lost in Middle
As shown in Table 4, we delves into the phenomena
termed “Lost in Middle” (Liu et al., 2023), where
the focus is to examine the performance of vari-
ous models on two different question-answering
datasets, PopQA and EntityQ. The presented re-
sults draw a comparison between four methodolo-
gies: Vanilla, Retrieval, Chain of Thought (CoT),and Theory of Mind (ThoT), as applied to three
advanced language models: GPT-4, GPT-3.5-turbo,
and LLaMA 2 Chat (70B).
Performance on PopQA : The results indicate
that ThoT significantly outperforms the other meth-
ods across all three models. With GPT-4 lead-
ing at a score of 0.651, closely followed by GPT-
3.5-turbo and LLaMA 2 Chat (70B) at 0.674 and
0.663, respectively. This suggests that ThoT’s ad-
vanced technique, potentially incorporating more
nuanced understandings of context and reasoning,
has a definitive edge in handling the complexities of
PopQA. The Vanilla approach yields moderate per-
formance with GPT-4, which surpasses the scores
of the other two models, hinting at the superior
reasoning capabilities of the latest model iteration.
Performance on EntityQ : Similar to PopQA,
the ThoT methodology again tops the charts, in-
dicating its robustness across different datasets.
GPT-4’s performance, while still the highest in
the Vanilla method, sees a significant jump to
0.643 when applying ThoT, suggesting a better
synergy between GPT-4’s capabilities and ThoT’s
advanced reasoning framework. Notably, the Re-
trieval method showcases a stark improvement over
Vanilla for all models, with LLaMA 2 Chat (70B)
achieving the highest score of 0.643.
4.4 Impact of Model Scale
As shown in Figure 5, results demonstrate a clear
correlation between the scale of the model and its
performance across different prompting strategies.
As we scale up from 7 billion parameters to 70
billion parameters in the LLama2, there is a notice-

--- PAGE 7 ---
No. Template EM
1 Let’s read through the document section by section, analyzing each part carefully as we go. 0.43
2 Take me through this long document step-by-step, making sure not to miss any important details. 0.47
3Divide the document into manageable parts and guide me through each one, providing insights as we move0.51along.
4 Analyze this extensive document in sections, summarizing each one and noting any key points. 0.47
5 Let’s go through this document piece by piece, paying close attention to each section. 0.50
6 Examine the document in chunks, evaluating each part critically before moving to the next. 0.49
7 Walk me through this lengthy document segment by segment, focusing on each part’s significance. 0.52
8 Let’s dissect this document bit by bit, making sure to understand the nuances of each section. 0.45
9 Systematically work through this document, summarizing and analyzing each portion as we go. 0.45
10Navigate through this long document by breaking it into smaller parts and summarizing each, so we don’t0.48miss anything.
11 Let’s explore the context step-by-step, carefully examining each segment. 0.44
12 Take me through the context bit by bit, making sure we capture all important aspects. 0.49
13 Let’s navigate through the context section by section, identifying key elements in each part. 0.47
14 Systematically go through the context, focusing on each part individually. 0.46
15 Let’s dissect the context into smaller pieces, reviewing each one for its importance and relevance. 0.47
16 Analyze the context by breaking it down into sections, summarizing each as we move forward. 0.49
17 Guide me through the context part by part, providing insights along the way. 0.52
18 Examine each segment of the context meticulously, and let’s discuss the findings. 0.44
19 Approach the context incrementally, taking the time to understand each portion fully. 0.42
20 Carefully analyze the context piece by piece, highlighting relevant points for each question. 0.47
21 In a step-by-step manner, go through the context, surfacing important information that could be useful. 0.53
22 Methodically examine the context, focusing on key segments that may answer the query. 0.45
23 Progressively sift through the context, ensuring we capture all pertinent details. 0.46
24 Navigate through the context incrementally, identifying and summarizing relevant portions. 0.48
25 Let’s scrutinize the context in chunks, keeping an eye out for information that answers our queries. 0.42
26 Take a modular approach to the context, summarizing each part before drawing any conclusions. 0.47
27 Read the context in sections, concentrating on gathering insights that answer the question at hand. 0.48
28Proceed through the context systematically, zeroing in on areas that could provide the answers we’re0.49seeking.
29Let’s take a segmented approach to the context, carefully evaluating each part for its relevance to the0.39questions posed.
30 Walk me through this context in manageable parts step by step, summarizing and analyzing as we go. 0.55
Table 5: Prompt Selection Analysis.
7B 13B 70B
Model Size0.30.40.5EM
Vanilla
Retrieval
CoT
ThoT
(a) LLaMA 2
7B 13B 33B
Model Size0.300.350.400.450.500.55EM
Vanilla
Retrieval
CoT
ThoT (b) Vicuna
Figure 5: PopQA performance on different scales of
LLaMA 2 and Vicuna.
able increase in the EM scores across all prompt
configurations. The same trend is observed with
the Vicuna model. This increase corroborates the
hypothesis that larger models have a greater capac-
ity for understanding and generating accurate re-
sponses. The Thought-by-Thought (ThoT) configu-
ration maintains a lead in performance at all model
sizes, which demonstrates ThoT prompts appear to
leverage the model’s capabilities more efficiently,
regardless of model size. In addition, the exper-imental data suggests that model scale positively
impacts the performance of retrieval-augmented
generation methods, with prompts such as CoT and
ThoT showing the most significant benefits. This
trend underscores the importance of prompt design
in harnessing the full potential of large language
models.
4.5 Prompt Selection
The prompt analysis, as depicted in Table 5, re-
veals strategic variations in phrasing that have a
direct impact on the performance of the language
models, as indicated by the Exact Match (EM)
scores. The prompts are designed to guide the
model through the process of analyzing a document
or context systematically and step by step. A com-
mon theme among the more successful prompts,
particularly those with EM scores above 0.50, is
the explicit instruction to the model to not only
analyze each part of the document but also to sum-
marize and note key insights as it progresses. For
instance, prompts that instructed the model to sum-

--- PAGE 8 ---
As a content reviewer, I provide multiple retrieved passages about this question; you need to answer the 
question .
  
retrieved Passage 1 is: The Red Romance The Red Romance is a New York City band formed in late 
2006. …
retrieved Passage 2 is: live show. In November 2006 Cardenas left the band to play in the Portland…
retrieved Passage 3 is: A Tribe Called Red A Tribe Called Red (often abbreviated as ATCR) is a …
retrieved Passage 4 is: barbershop to rock and roll. Scottish folk -rock band Runrig  have made …
retrieved Passage 5 is: Cagg  did shows in West Germany (Bonn & Frankfurt). In September Abby 
Hick… \ 
retrieved Passage 6 is: The Red Hearts The Red Hearts are a garage punk band from Los Angeles, …
retrieved Passage 7 is: drums Adam Chilenski  – bass, backing vocals The Red Romance The Red 
Romance …
retrieved Passage 8 is: April 2008. Pops is a former member of the … The Red Hearts are a garage punk 
band …
retrieved Passage 9 is: stage by two additional musicians : Ronan Le Bars (bagpipes, whistles), (Dan Ar 
Braz …
retrieved Passage 10 is: Red (band) Red (also stylized R3D or RED) is an American rock band from 
Nashville, …
Q: What type of music does The Red Hearts play?
{Prompting Method}
A:Input
The retrieved  passages  provide  information  about  various  bands,  but none  specifically  state the type of 
music  that The Red Hearts  play. Therefore,  we cannot  determine  the type of music  that The Red Hearts  
play based  on the given  information .
The passages  retrieved  for The Red Hearts  provide  information  about  the band's  formation,  members,  
and musical  style . 
Passage  2 mentions  that The Red Hearts  went  through  a series  of drummers,  including  Eric Tretbar  and 
Justino  Polimeni , before  Kirk Podell  joined  as the current  drummer . The band  embarked  on a tour of the 
Southern  States .
According  to Passage  6, The Red Hearts,  a garage  punk  band,  were  formed  in 2004  by Lenny  Pops  after 
the breakup  of his previous  band,  The Snakes . The band  was described  as a minimal  mod/punk  three -
piece . They  played  local  shows  in Los Angeles  and Long  Beach .
Passage  8 states  that Lenny  Pops,  the founder  of The Red Hearts,  was a former  member  of the Brian  
Jonestown  Massacre  and played  with Blake  Miller  of Moving  Units  in a band  called  Spectacle . The Red 
Hearts  are a garage  punk  band .
Based  on these  passages,  The Red Hearts  can be described  as a garage  punk  band  with mod and punk  
influences . They  have  gone  through  various  drummers  and have toured  extensively . Lenny  Pops,  the 
founder,  has a background  in other  bands  like The Snakes  and Spectacle .CoT Output
ThoT  Output
Figure 6: Case Study.
As a content reviewer, I provide multiple retrieved passages about this question; you need to answer the question.
retrieved Passage 1 is: claims that ever since her father‘s death she has been …
retrieved Passage 2 is: …Andi Eigenmann  is his daughter with actress Jaclyn Jose...
retrieved Passage 3 is: several tokens (e.g., a lock of hair, some black beads …
retrieved Passage 4 is: 2017, he took part in the German TV series " Let's Dance…
retrieved Passage 5 is: engaged. When Jorja  Fox decided to leave the show, …
retrieved Passage 6 is: as well as in the 420 discipline in 2007 and 2008. …
retrieved Passage 7 is: to be to transfix a man with a harpoon. Grissom …
retrieved Passage 8 is: Rivera on June 1, 2009. The next year, Rivera …
retrieved Passage 9 is: the relationship, saying that it was in her best interest. …
retrieved Passage 10 is: Sidle and Grissom were interested in each other …
Q: Who is Mark Gil married to?
Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.
A:Input
The question  is about  Mark  Gil and who he is married  to. Mark  Gil is mentioned  in 
retrieved  Passage  2 and retrieved  Passage  8. 
In retrieved  Passage  2, it is mentioned  that Mark  Gil was married  to Maricar  Jacinto  in 
1996  and they had one child  named  Stephanie . However,  this passage  does not provide  
any information  about  Mark  Gil's current  marital  status  or who he is currently  married  to. 
In retrieved  Passage  8, it is not mentioned  who Mark  Gil is married  to. Instead,  this 
passage  provides  information  about  his previous  marriages  and divorces . It states  that he 
was previously  married  to a woman  named  Elisabeth  Rivera  in 2009 , but they divorced  
in 2011 . It also mentions  that he later married  Kim Perks  in 2012  and they had a 
daughter  in 2014 . This passage  does not provide  any information  on Mark  Gil's current  
marital  status  or who he is currently  married  to.
Therefore,  based  on the provided  information,  it is not possible  to determine  who Mark  
Gil is currently  married  to.Output
Figure 7: Error Analysis.
marize each section and not miss important de-
tails, such as prompt 2 and prompt 4, resulted in
higher EM scores. Prompts that encouraged a more
granular approach, directing the model to focus
on individual parts and their significance or rel-
evance, also performed well. This is evidenced
by prompt 14, which achieved a relatively high
EM score. The more detailed the instruction for
the model to dissect and analyze the context, the
better the model performed. Conversely, prompts
that were less directive or less structured, such as
prompt 29, tended to result in lower EM scores.
This suggests that models benefit from clear, spe-
cific, and action-oriented instructions that leave
little room for ambiguity in the analytical process.
The highest-scoring prompt, number 30, combines
several elements of successful prompts. It asks the
model to manage the complexity by breaking it
down into parts, which implies a thorough analy-
sis, and also to summarize and analyze, indicating
an active engagement with the material that goes
beyond mere reading or passive understanding. In
summary, the results suggest that prompts that are
structured to enforce a detailed analytical process,
encouraging step-by-step dissection, summariza-
tion, and critical evaluation, lead to better model
performance.4.6 Case Study
The case study presented in Figure 6 shows a com-
parative analysis between the CoT and ThoT in
PopQA. CoT only stated that the passages con-
tained information about various bands without
specifying the genre of “The Red Hearts”. This il-
lustrates a potential limitation of the CoT approach:
it might not effectively synthesize information from
multiple sources when the answer is not explicitly
stated but rather needs to be inferred from the given
data. On the contrary, the ThoT method success-
fully identified that “The Red Hearts play garage
punk music”. This outcome showcases the strength
of the ThoT approach. ThoT is adept at synthe-
sizing and correlating information across multiple
pieces of text. It pieced together relevant details
from passages 6 and 8, noting that “The Red Hearts”
were described as “a garage punk band”.
4.7 Error Analysis
From Figure 7, the ThoT method can not conclude
the answer for this case. The passage stating, “Andi
Eigenmann is his daughter with actress Jaclyn Jose”
holds the key to the correct inference that Mark Gil
was married to Jaclyn Jose. The ThoT method’s
failure to make this inference suggests that while
the model is adept at extracting explicit informa-

--- PAGE 9 ---
tion, it struggles with implicit reasoning that re-
quires understanding nuanced relationships. The
oversight may be attributed to the model’s inferen-
tial reasoning capabilities, specifically regarding
relationship inference—a known shortcoming in
large models as also identified in prior research
(Berglund et al., 2023). The case study highlights
the need for models to not only parse and sum-
marize information but also engage in a level of
deductive reasoning that resembles human cogni-
tion. Therefore, enhancing the model’s ability to
infer and reason about entity relationships is very
important.
5 Conclusion
This paper presented the “Thread of Thought”
(ThoT) strategy, a novel approach designed to en-
hance the performance of Large Language Mod-
els (LLMs) in processing chaotic contextual infor-
mation. ThoT, inspired by human cognitive pro-
cesses, significantly improves the ability of LLMs
to segment and analyze extended contexts. We
compared ThoT with existing methods, which of-
ten require complex retraining, fine-tuning, or are
limited in their ability to handle large volumes of
intricate information. ThoT, in contrast, offers a
more straightforward and efficient solution. It acts
as a “plug-and-play” module, seamlessly integrat-
ing with various pre-trained language models and
prompting strategies without necessitating com-
plex procedures. The effectiveness of ThoT was
rigorously tested using long-tail question answer-
ing datasets, such as PopQA and EntityQ, and a
Multi-Turn Conversation Response dataset based
on everyday conversations. The results from these
evaluations were clear: ThoT not only excelled in
handling chaotic contexts but also enhanced the
reasoning capabilities of LLMs.
References
Lukas Berglund, Meg Tong, Max Kaufmann, Mikita
Balesni, Asa Cooper Stickland, Tomasz Korbak, and
Owain Evans. 2023. The reversal curse: Llms
trained on "a is b" fail to learn "b is a". CoRR ,
abs/2309.12288.
Maciej Besta, Nils Blach, Ales Kubicek, Robert Ger-
stenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz
Lehmann, Michal Podstawski, Hubert Niewiadom-
ski, Piotr Nyczyk, and Torsten Hoefler. 2023. Graph
of thoughts: Solving elaborate problems with large
language models. CoRR , abs/2308.09687.Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual .
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023. Extending context window of
large language models via positional interpolation.
CoRR , abs/2306.15595.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,
Shaohan Huang, Wenhui Wang, Nanning Zheng, and
Furu Wei. 2023. Longnet: Scaling transformers to 1,
000, 000, 000 tokens. CoRR , abs/2307.02486.
Huiqiang Jiang, Qianhui Wu, , Xufang Luo, Dongsheng
Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023.
Longllmlingua: Accelerating and enhancing llms
in long context scenarios via prompt compression.
ArXiv preprint , abs/2310.06839.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in Neu-
ral Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual .
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023. Lost in the middle: How language
models use long contexts. CoRR , abs/2307.03172.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023 , pages 9802–9822. Association for
Computational Linguistics.
Yasmin Moslem, Rejwanul Haque, John D. Kelleher,
and Andy Way. 2023. Adaptive machine translation

--- PAGE 10 ---
with large language models. In Proceedings of the
24th Annual Conference of the European Associa-
tion for Machine Translation, EAMT 2023, Tampere,
Finland, 12-15 June 2023 , pages 227–237. European
Association for Machine Translation.
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train
short, test long: Attention with linear biases enables
input length extrapolation. In The Tenth International
Conference on Learning Representations, ICLR 2022,
Virtual Event, April 25-29, 2022 . OpenReview.net.
Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram,
Inbal Magar, Omri Abend, Ehud Karpas, Amnon
Shashua, Kevin Leyton-Brown, and Yoav Shoham.
2023. Parallel context windows for large language
models. In Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), ACL 2023, Toronto, Canada,
July 9-14, 2023 , pages 6383–6402. Association for
Computational Linguistics.
John Schulman, Barret Zoph, Christina Kim, Jacob
Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron
Uribe, Liam Fedus, Luke Metz, Michael Pokorny,
et al. 2022. Chatgpt: Optimizing language models
for dialogue. OpenAI blog .
Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,
and Danqi Chen. 2021. Simple entity-centric ques-
tions challenge dense retrievers. In Proceedings
of the 2021 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2021, Vir-
tual Event / Punta Cana, Dominican Republic, 7-11
November, 2021 , pages 6138–6148. Association for
Computational Linguistics.
Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah
Kwan, Mohit Bansal, and Colin Raffel. 2023. Evalu-
ating the factual consistency of large language mod-
els through news summarization. In Findings of
the Association for Computational Linguistics: ACL
2023, Toronto, Canada, July 9-14, 2023 , pages 5220–
5255. Association for Computational Linguistics.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. CoRR ,
abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models. CoRR , abs/2307.09288.
Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang,
and Dinggang Shen. 2023a. Chatcad: Interactive
computer-aided diagnosis on medical image using
large language models. CoRR , abs/2302.07257.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .
Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd-
hery, and Denny Zhou. 2023b. Self-consistency
improves chain of thought reasoning in language
models. In The Eleventh International Conference
on Learning Representations, ICLR 2023, Kigali,
Rwanda, May 1-5, 2023 . OpenReview.net.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022. Chain-of-thought prompt-
ing elicits reasoning in large language models. In
NeurIPS .
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. 2023. Efficient stream-
ing language models with attention sinks. CoRR ,
abs/2309.17453.
Jing Xu, Arthur Szlam, and Jason Weston. 2022. Be-
yond goldfish memory: Long-term open-domain con-
versation. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,
May 22-27, 2022 , pages 5180–5197. Association for
Computational Linguistics.
Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee,
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina
Bakhturina, Mohammad Shoeybi, and Bryan Catan-
zaro. 2023. Retrieval meets long context large lan-
guage models. CoRR , abs/2310.03025.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. 2023a. Tree of thoughts: Deliberate
problem solving with large language models. CoRR ,
abs/2305.10601.
Yao Yao, Zuchao Li, and Hai Zhao. 2023b. Be-
yond chain-of-thought, effective graph-of-thought
reasoning in large language models. CoRR ,
abs/2305.16582.
Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li,
Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao,

--- PAGE 11 ---
Song Yun, Xuanjing Huang, and Zhongyu Wei. 2023.
Disc-lawllm: Fine-tuning large language models for
intelligent legal services. CoRR , abs/2309.11325.
Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan,
and Lidong Bing. 2023. Sentiment analysis in the
era of large language models: A reality check. CoRR ,
abs/2305.15005.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Claire Cui, Olivier Bousquet, Quoc V . Le, and Ed H.
Chi. 2023. Least-to-most prompting enables com-
plex reasoning in large language models. In The
Eleventh International Conference on Learning Rep-
resentations, ICLR 2023, Kigali, Rwanda, May 1-5,
2023 . OpenReview.net.
