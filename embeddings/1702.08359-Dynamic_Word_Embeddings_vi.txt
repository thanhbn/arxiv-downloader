# Embedding từ động

Robert Bamler1, Stephan Mandt1

## Tóm tắt

Chúng tôi trình bày một mô hình ngôn ngữ xác suất cho dữ liệu văn bản có dấu thời gian để theo dõi sự tiến hóa ngữ nghĩa của các từ riêng lẻ theo thời gian. Mô hình biểu diễn các từ và ngữ cảnh bằng các quỹ đạo tiềm ẩn trong không gian embedding. Tại mỗi thời điểm, các vector embedding được suy luận từ một phiên bản xác suất của word2vec (Mikolov et al., 2013b). Các vector embedding này được kết nối theo thời gian thông qua một quá trình khuếch tán tiềm ẩn. Chúng tôi mô tả hai thuật toán suy luận biến phân có thể mở rộng—làm mượt skip-gram và lọc skip-gram—cho phép chúng tôi huấn luyện mô hình một cách kết hợp trên tất cả các thời điểm; do đó học trên tất cả dữ liệu trong khi đồng thời cho phép các vector từ và ngữ cảnh trôi dạt. Kết quả thực nghiệm trên ba corpus khác nhau cho thấy mô hình động của chúng tôi suy luận các quỹ đạo embedding từ có thể diễn giải hơn và dẫn đến khả năng dự đoán cao hơn so với các phương pháp cạnh tranh dựa trên các mô hình tĩnh được huấn luyện riêng biệt trên các lát thời gian.

## 1. Giới thiệu

Ngôn ngữ tiến hóa theo thời gian và các từ thay đổi ý nghĩa do sự chuyển dịch văn hóa, các phát minh công nghệ, hoặc các sự kiện chính trị. Chúng tôi xem xét vấn đề phát hiện sự thay đổi trong ý nghĩa và cách sử dụng của các từ trong một khoảng thời gian nhất định dựa trên dữ liệu văn bản. Việc nắm bắt những thay đổi ngữ nghĩa này đòi hỏi một mô hình ngôn ngữ động.

Word embedding là một công cụ mạnh mẽ để mô hình hóa các mối quan hệ ngữ nghĩa giữa các từ riêng lẻ (Bengio et al., 2003; Mikolov et al., 2013a; Pennington et al., 2014; Mnih & Kavukcuoglu, 2013; Levy & Goldberg, 2014; Vilnis & McCallum, 2014; Rudolph et al., 2016). Word embedding mô hình hóa phân phối của các từ dựa trên các từ xung quanh chúng trong corpus huấn luyện, và tóm tắt những thống kê này dưới dạng biểu diễn vector chiều thấp. Khoảng cách hình học giữa các vector từ phản ánh sự tương tự ngữ nghĩa (Mikolov et al., 2013a) và các vector khác biệt mã hóa các mối quan hệ ngữ nghĩa và cú pháp (Mikolov et al., 2013c), điều này cho thấy chúng là những biểu diễn hợp lý của ngôn ngữ. Các word embedding được huấn luyện trước hữu ích cho nhiều tác vụ có giám sát, bao gồm phân tích cảm xúc (Socher et al., 2013b), phân tích cú pháp ngữ nghĩa (Socher et al., 2013a), và thị giác máy tính (Fu & Sigal, 2016). Như các mô hình không giám sát, chúng cũng được sử dụng để khám phá các phép tương tự từ và ngôn ngữ học (Mikolov et al., 2013c).

Word embedding hiện tại được công thức hóa như các mô hình tĩnh, giả định rằng ý nghĩa của bất kỳ từ nào đều giống nhau trên toàn bộ corpus văn bản. Trong bài báo này, chúng tôi đề xuất một tổng quát hóa của word embedding cho dữ liệu tuần tự, chẳng hạn như corpus của các văn bản lịch sử hoặc các luồng văn bản trên mạng xã hội.

Các cách tiếp cận hiện tại để học word embedding trong bối cảnh động dựa vào việc nhóm dữ liệu thành các bin thời gian và huấn luyện embedding riêng biệt trên các bin này (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016). Tuy nhiên, cách tiếp cận này đặt ra ba vấn đề cơ bản. Thứ nhất, vì các mô hình word embedding không lồi, việc huấn luyện chúng hai lần trên cùng một dữ liệu sẽ dẫn đến các kết quả khác nhau. Do đó, các vector embedding tại các thời điểm liên tiếp chỉ có thể liên quan một cách gần đúng với nhau, và chỉ khi chiều embedding lớn (Hamilton et al., 2016). Thứ hai, việc chia corpus thành các bin thời gian riêng biệt có thể dẫn đến các tập huấn luyện quá nhỏ để huấn luyện mô hình word embedding. Do đó, người ta có nguy cơ overfitting với ít dữ liệu bất cứ khi nào độ phân giải thời gian cần thiết là tinh vi, như chúng tôi cho thấy trong phần thực nghiệm. Thứ ba, do kích thước corpus hữu hạn, các vector word embedding học được chịu nhiễu ngẫu nhiên. Rất khó để phân biệt nhiễu này với các sự trôi dạt ngữ nghĩa có hệ thống giữa các thời điểm tiếp theo, đặc biệt là trong các khoảng thời gian ngắn, nơi chúng ta chỉ mong đợi sự trôi dạt ngữ nghĩa nhỏ.

Trong bài báo này, chúng tôi vượt qua những vấn đề này bằng cách giới thiệu một mô hình word embedding động. Những đóng góp của chúng tôi như sau:

• Chúng tôi suy ra một mô hình không gian trạng thái xác suất trong đó các embedding từ và ngữ cảnh tiến hóa theo thời gian theo một quá trình khuếch tán. Nó tổng quát hóa mô hình skip-gram (Mikolov et al., 2013b; Barkan, 2017) cho một thiết lập động, cho phép huấn luyện đầu cuối. Điều này dẫn đến các quỹ đạo embedding liên tục, làm mượt nhiễu trong thống kê từ-ngữ cảnh, và cho phép chúng tôi chia sẻ thông tin trên tất cả các thời gian.

• Chúng tôi đề xuất hai thuật toán suy luận biến phân hộp đen có thể mở rộng (Ranganath et al., 2014; Rezende et al., 2014) cho lọc và làm mượt. Những thuật toán này tìm ra word embedding tổng quát hóa tốt hơn cho dữ liệu giữ lại. Thuật toán làm mượt của chúng tôi thực hiện suy luận biến phân hộp đen hiệu quả cho các phân phối biến phân Gaussian có cấu trúc với ma trận độ chính xác tam giác, và áp dụng rộng rãi hơn.

• Chúng tôi phân tích ba corpus văn bản khổng lồ kéo dài qua các giai đoạn thời gian dài. Cách tiếp cận của chúng tôi cho phép chúng tôi tự động tìm ra những từ có ý nghĩa thay đổi nhiều nhất. Nó tạo ra các quỹ đạo word embedding mượt mà và do đó cho phép chúng tôi đo lường và hình dung động lực liên tục của toàn bộ đám mây embedding khi nó biến dạng theo thời gian.

Hình 1 minh họa phương pháp của chúng tôi. Biểu đồ cho thấy sự khớp của mô hình skip-gram động với Google books (chúng tôi đưa ra chi tiết trong phần 5). Chúng tôi cho thấy mười từ có ý nghĩa thay đổi mạnh nhất về khoảng cách cosine trong 150 năm qua. Qua đó chúng tôi tự động khám phá những từ như "computer" hoặc "radio" có ý nghĩa thay đổi do tiến bộ công nghệ, nhưng cũng những từ như "peer" và "notably" có sự thay đổi ngữ nghĩa ít rõ ràng hơn.

Bài báo của chúng tôi được cấu trúc như sau. Trong phần 2 chúng tôi thảo luận về các công trình liên quan, và chúng tôi giới thiệu mô hình trong phần 3. Trong phần 4 chúng tôi trình bày hai thuật toán suy luận biến phân hiệu quả cho mô hình động của chúng tôi. Chúng tôi cho thấy kết quả thực nghiệm trong phần 5. Phần 6 tóm tắt các phát hiện của chúng tôi.

## 2. Công trình liên quan

Các mô hình xác suất đã được mở rộng thành các mô hình chuỗi thời gian tiềm ẩn rất phổ biến (Blei & Lafferty, 2006; Wang et al., 2008; Sahoo et al., 2012; Gultekin & Paisley, 2014; Charlin et al., 2015; Ranganath et al., 2015; Jerfel et al., 2017), nhưng không có mô hình nào liên quan đến word embedding. Gần nhất với những mô hình này là mô hình chủ đề động (Blei & Lafferty, 2006; Wang et al., 2008), học sự tiến hóa của các chủ đề tiềm ẩn theo thời gian. Các mô hình chủ đề dựa trên biểu diễn túi từ và do đó coi các từ như các ký hiệu mà không mô hình hóa mối quan hệ ngữ nghĩa của chúng. Do đó chúng phục vụ một mục đích khác.

Mikolov et al. (2013a;b) đã đề xuất mô hình skip-gram với lấy mẫu âm (word2vec) như một cách tiếp cận word embedding có thể mở rộng dựa trên gradient descent ngẫu nhiên. Cách tiếp cận này đã được công thức hóa trong thiết lập Bayesian (Barkan, 2017), mà chúng tôi thảo luận riêng trong phần 3.1. Tuy nhiên, những mô hình này không cho phép các vector word embedding thay đổi theo thời gian.

Một số tác giả đã phân tích các thống kê khác nhau của dữ liệu văn bản để phân tích các thay đổi ngữ nghĩa của từ theo thời gian (Mihalcea & Nastase, 2012; Sagi et al., 2011; Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016). Không có ai trong số họ mô hình hóa một cách rõ ràng một quá trình động; thay vào đó, họ cắt dữ liệu thành các bin thời gian khác nhau, khớp mô hình riêng biệt trên mỗi bin, và tiếp tục phân tích các vector embedding trong hậu xử lý. Theo thiết kế, những mô hình tĩnh này do đó không thể chia sẻ sức mạnh thống kê qua thời gian. Điều này hạn chế khả năng áp dụng của các mô hình tĩnh cho các corpus rất lớn.

Gần nhất với cách tiếp cận của chúng tôi là các phương pháp dựa trên word embedding. Kim et al. (2014) khớp word2vec riêng biệt trên các bin thời gian khác nhau, trong đó các vector từ thu được cho bin trước đó được sử dụng để khởi tạo thuật toán cho bin thời gian tiếp theo. Các bin phải đủ lớn và các quỹ đạo tìm được không mượt mà như của chúng tôi, như chúng tôi chứng minh trong bài báo này. Hamilton et al. (2016) cũng huấn luyện word2vec riêng biệt trên một số corpus lớn từ các thập kỷ khác nhau. Nếu chiều embedding đủ lớn (và do đó bài toán tối ưu ít không lồi hơn), các tác giả lập luận rằng word embedding tại các thời gian gần nhau gần đúng khác nhau bởi một phép quay toàn cục ngoài một sự trôi dạt ngữ nghĩa nhỏ, và họ tính toán gần đúng phép quay này. Vì phép quay sau không tồn tại theo nghĩa nghiêm ngặt, rất khó để phân biệt các hiện tượng của phép quay gần đúng với sự trôi dạt ngữ nghĩa thực sự. Như đã thảo luận trong bài báo này, cả hai biến thể đều tạo ra các quỹ đạo nhiễu hơn.

## 3. Mô hình

Chúng tôi đề xuất mô hình skip-gram động, một tổng quát hóa của mô hình skip-gram (word2vec) (Mikolov et al., 2013b) cho dữ liệu văn bản tuần tự. Mô hình tìm ra các vector word embedding liên tục trôi dạt theo thời gian, cho phép theo dõi các thay đổi trong ngôn ngữ và cách sử dụng từ trong các giai đoạn ngắn và dài. Skip-gram động là một mô hình xác suất kết hợp một phiên bản Bayesian của mô hình skip-gram (Barkan, 2017) với một chuỗi thời gian tiềm ẩn. Nó được huấn luyện kết hợp từ đầu đến cuối và mở rộng cho dữ liệu khổng lồ bằng suy luận Bayesian gần đúng.

Dữ liệu quan sát bao gồm các chuỗi từ từ một từ vựng hữu hạn có kích thước L. Trong phần 3.1, tất cả các chuỗi (câu từ sách, bài báo, hoặc tweet) được coi là độc lập về thời gian; trong phần 3.2 chúng sẽ được liên kết với các dấu thời gian khác nhau. Mục tiêu là tối đa hóa xác suất của mỗi từ xuất hiện trong dữ liệu cho các từ xung quanh nó trong một cửa sổ ngữ cảnh. Như được chi tiết dưới đây, mô hình học hai vector ui, vi ∈ Rd cho mỗi từ i trong từ vựng, trong đó d là chiều embedding. Chúng tôi gọi ui là vector word embedding và vi là vector context embedding.

### 3.1. Mô hình Skip-Gram Bayesian

Mô hình skip-gram Bayesian (Barkan, 2017) là một phiên bản xác suất của word2vec (Mikolov et al., 2013b) và tạo thành cơ sở cho cách tiếp cận của chúng tôi. Mô hình đồ thị được hiển thị trong Hình 2a). Đối với mỗi cặp từ i,j trong từ vựng, mô hình gán xác suất rằng từ i xuất hiện trong ngữ cảnh của từ j. Xác suất này là σ(ui⊤vj) với hàm sigmoid σ(x) = 1/(1 + e^(-x)). Đặt zij ∈ {0,1} là một biến chỉ số biểu thị một lần rút từ phân phối xác suất đó, do đó p(zij = 1) = σ(ui⊤vj). Mô hình sinh giả định rằng nhiều cặp từ-từ (i,j) được rút đều từ từ vựng và kiểm tra để trở thành cặp từ-ngữ cảnh; do đó một chỉ số ngẫu nhiên zij riêng biệt được liên kết với mỗi cặp được rút.

Tập trung vào các từ và các láng giềng của chúng trong cửa sổ ngữ cảnh, chúng tôi thu thập bằng chứng của các cặp từ-từ mà zij = 1. Chúng được gọi là các ví dụ tích cực. Ký hiệu n+ij số lần một cặp từ-ngữ cảnh (i,j) được quan sát trong corpus. Đây là một thống kê đủ của mô hình, và đóng góp của nó vào khả năng là p(n+ij|ui,vj) = σ(ui⊤vj)^n+ij. Tuy nhiên, quá trình sinh cũng giả định khả năng từ chối các cặp từ-từ nếu zij = 0. Do đó, người ta cần xây dựng một tập huấn luyện thứ hai giả tưởng của các cặp từ-từ bị từ chối, được gọi là các ví dụ âm. Đặt các số đếm tương ứng là n-ij. Tổng khả năng của cả ví dụ tích cực và âm là

p(n+,n-|U,V) = ∏(i,j=1 to L) σ(ui⊤vj)^n+ij σ(-ui⊤vj)^n-ij. (1)

Ở trên chúng tôi đã sử dụng tính chất phản đối xứng σ(-x) = 1-σ(x). Trong ký hiệu của chúng tôi, việc bỏ chỉ số con cho n+ và n- biểu thị toàn bộ ma trận L×L, U = (u1,···,uL) ∈ Rd×L là ma trận của tất cả các vector word embedding, và V được định nghĩa tương tự cho các vector ngữ cảnh. Để xây dựng các ví dụ âm, người ta thường chọn n-ij ∝ P(i)P(j)^(3/4) (Mikolov et al., 2013b), trong đó P(i) là tần suất của từ i trong corpus huấn luyện. Do đó, n- được định nghĩa rõ ràng cho đến một hằng số cần được điều chỉnh.

Định nghĩa n± = (n+,n-) sự kết hợp của cả ví dụ tích cực và âm, log khả năng kết quả là

log p(n±|U,V) = ∑(i,j=1 to L) [n+ij log σ(ui⊤vj) + n-ij log σ(-ui⊤vj)]. (2)

Đây chính xác là mục tiêu của mô hình skip-gram (không Bayesian), xem (Mikolov et al., 2013b). Các ma trận đếm n+ và n- hoặc được tính toán trước cho toàn bộ corpus, hoặc ước lượng dựa trên các mẫu con ngẫu nhiên từ dữ liệu theo cách tuần tự, như word2vec thực hiện. Barkan (2017) đưa ra một xử lý Bayesian gần đúng của mô hình với các prior Gaussian trên các embedding.

### 3.2. Mô hình Skip-Gram Động

Phần mở rộng chính của cách tiếp cận của chúng tôi là sử dụng bộ lọc Kalman như một prior cho sự tiến hóa thời gian của các embedding tiềm ẩn (Welch & Bishop, 1995). Điều này cho phép chúng tôi chia sẻ thông tin qua tất cả các thời gian trong khi vẫn cho phép các embedding trôi dạt.

**Ký hiệu.** Chúng tôi xem xét một corpus gồm T tài liệu được viết tại các dấu thời gian τ1 < ... < τT. Đối với mỗi bước thời gian t ∈ {1,...,T}, các thống kê đủ của các cặp từ-ngữ cảnh được mã hóa trong các ma trận L×L n+t, n-t của các số đếm tích cực và âm với các phần tử ma trận n+ij,t và n-ij,t, tương ứng. Ký hiệu Ut = (u1,t,···,uL,t) ∈ Rd×L ma trận của word embedding tại thời gian t, và định nghĩa Vt tương ứng cho các vector ngữ cảnh. Đặt U,V ∈ RT×d×L biểu thị các tensor của word embedding và context embedding qua tất cả các thời gian, tương ứng.

**Mô hình.** Mô hình đồ thị được hiển thị trong Hình 2b). Chúng tôi xem xét một quá trình khuếch tán của các vector embedding theo thời gian. Phương sai σ2t của kernel chuyển tiếp là

σ2t = D(τt+1 - τt), (3)

trong đó D là một hằng số khuếch tán toàn cục và (τt+1 - τt) là thời gian giữa các quan sát tiếp theo (Welch & Bishop, 1995). Tại mỗi bước thời gian t, chúng tôi thêm một prior Gaussian bổ sung với trung bình bằng không và phương sai σ20 ngăn các vector embedding trở nên rất lớn, do đó

p(Ut+1|Ut) ∝ N(Ut,σ2t)N(0,σ20). (4)

Tính toán chuẩn hóa, điều này dẫn đến

Ut+1|Ut ~ N(Ut/(1 + σ2t/σ20), 1/(σ-2t + σ-20)I), (5)

Vt+1|Vt ~ N(Vt/(1 + σ2t/σ20), 1/(σ-2t + σ-20)I). (6)

Trong thực tế, σ0 >> σt, do đó việc giảm chấn về gốc rất yếu. Điều này cũng được gọi là quá trình Ornstein-Uhlenbeck (Uhlenbeck & Ornstein, 1930). Chúng tôi khôi phục quá trình Wiener cho σ0 → ∞, nhưng σ0 < ∞ ngăn chuỗi thời gian tiềm ẩn phân kỳ đến vô cùng. Tại chỉ số thời gian t = 1, chúng tôi định nghĩa p(U1|U0) ≡ N(0,σ20I) và làm tương tự cho V1.

Phân phối kết hợp của chúng tôi phân tích như sau:

p(n±,U,V) = ∏(t=0 to T-1) p(Ut+1|Ut)p(Vt+1|Vt) × ∏(t=1 to T) ∏(i,j=1 to L) p(n±ij,t|ui,t,vj,t) (7)

Mô hình prior thúc đẩy mô hình học các vector embedding thay đổi mượt mà qua thời gian. Điều này cho phép liên kết các từ một cách rõ ràng với nhau và phát hiện các thay đổi ngữ nghĩa. Mô hình chia sẻ thông tin một cách hiệu quả qua miền thời gian, cho phép khớp mô hình ngay cả trong các thiết lập mà dữ liệu tại mỗi thời điểm nhất định nhỏ, miễn là tổng dữ liệu lớn.

## 4. Suy luận

Chúng tôi thảo luận hai thuật toán suy luận gần đúng có thể mở rộng. Lọc chỉ sử dụng thông tin từ quá khứ; nó được yêu cầu trong các ứng dụng streaming nơi dữ liệu được tiết lộ cho chúng tôi theo trình tự. Làm mượt là phương pháp suy luận khác, học các embedding tốt hơn nhưng yêu cầu toàn bộ chuỗi tài liệu trước.

Trong suy luận Bayesian, chúng tôi bắt đầu bằng cách công thức hóa phân phối kết hợp (Eq. 7) trên các quan sát n± và tham số U và V, và chúng tôi quan tâm đến phân phối hậu nghiệm trên tham số có điều kiện trên quan sát,

p(U,V|n±) = p(n±,U,V) / ∫ p(n±,U,V)dUdV (8)

Vấn đề là chuẩn hóa không thể giải được. Trong suy luận biến phân (VI) (Jordan et al., 1999; Blei et al., 2016), người ta tránh vấn đề này và xấp xỉ hậu nghiệm bằng một phân phối biến phân đơn giản hơn qλ(U,V) bằng cách tối thiểu hóa divergence Kullback-Leibler (KL) với hậu nghiệm. Ở đây, λ tóm tắt tất cả tham số của phân phối biến phân, chẳng hạn như trung bình và phương sai của một Gaussian, xem dưới đây. Việc tối thiểu hóa divergence KL tương đương với việc tối ưu hóa evidence lower bound (ELBO) (Blei et al., 2016),

L(λ) = Eqλ[log p(n±,U,V)] - Eqλ[log qλ(U,V)]. (9)

Đối với một lớp mô hình hạn chế, ELBO có thể được tính toán dưới dạng đóng (Hoffman et al., 2013). Mô hình của chúng tôi không liên hợp và thay vào đó yêu cầu VI hộp đen sử dụng thủ thuật tham số hóa lại (Rezende et al., 2014; Kingma & Welling, 2014).

### 4.1. Lọc Skip-Gram

Trong nhiều ứng dụng như streaming, dữ liệu đến tuần tự. Do đó, chúng tôi chỉ có thể điều kiện mô hình của chúng tôi trên các quan sát quá khứ chứ không phải tương lai. Trước tiên chúng tôi sẽ mô tả suy luận trong thiết lập lọc (Kalman) như vậy (Kalman et al., 1960; Welch & Bishop, 1995).

Trong kịch bản lọc, thuật toán suy luận cập nhật lặp lại phân phối biến phân q khi bằng chứng từ mỗi bước thời gian t trở nên khả dụng. Chúng tôi sử dụng một phân phối biến phân phân tích qua tất cả các thời gian, q(U,V) = ∏(t=1 to T) q(Ut,Vt) và chúng tôi cập nhật hệ số biến phân tại thời gian t nhất định dựa trên bằng chứng tại thời gian t và hậu nghiệm gần đúng của bước thời gian trước đó. Hơn nữa, tại mỗi thời gian t chúng tôi sử dụng phân phối phân tích hoàn toàn:

q(Ut,Vt) = ∏(i=1 to L) N(ui,t; μui,t, Σui,t) N(vi,t; μvi,t, Σvi,t)

Các tham số biến phân là các trung bình μui,t, μvi,t ∈ Rd và các ma trận hiệp phương sai Σui,t và Σvi,t, mà chúng tôi hạn chế là đường chéo (xấp xỉ mean-field).

Chúng tôi mô tả cách chúng tôi tính toán tuần tự q(Ut,Vt) và sử dụng kết quả để tiến hành bước thời gian tiếp theo. Như các hệ thống động Markov khác, mô hình của chúng tôi giả định đệ quy sau,

p(Ut,Vt|n±1:t) ∝ p(n±t|Ut,Vt)p(Ut,Vt|n±1:t-1). (10)

Trong xấp xỉ biến phân của chúng tôi, ELBO (Eq. 9) do đó tách thành tổng của T thuật ngữ, L = ∑t Lt với

Lt = E[log p(n±t|Ut,Vt)] + E[log p(Ut,Vt|n±1:t-1)] - E[log q(Ut,Vt)], (11)

trong đó tất cả kỳ vọng được lấy dưới q(Ut,Vt). Chúng tôi tính toán thuật ngữ entropy -E[log q] trong Eq. 11 một cách phân tích và ước lượng gradient của log likelihood bằng cách lấy mẫu từ phân phối biến phân và sử dụng thủ thuật tham số hóa lại (Kingma & Welling, 2014; Salimans & Kingma, 2016). Tuy nhiên, thuật ngữ thứ hai của Eq. 11, chứa prior tại thời gian t, vẫn không thể giải được. Chúng tôi xấp xỉ prior như

p(Ut,Vt|n±1:t-1) ≡ Ep(Ut-1,Vt-1|n±1:t-1)[p(Ut,Vt|Ut-1,Vt-1)] ≈ Eq(Ut-1,Vt-1)[p(Ut,Vt|Ut-1,Vt-1)]. (12)

Kỳ vọng còn lại chỉ liên quan đến Gaussian và có thể được thực hiện một cách phân tích. Prior gần đúng kết quả là một phân phối phân tích hoàn toàn p(Ut,Vt|n±1:t-1) ≈ ∏(i=1 to L) N(ui,t; μ̃ui,t, Σ̃ui,t) N(vi,t; μ̃vi,t, Σ̃vi,t) với

μ̃ui,t = Σ̃ui,t (Σui,t-1 + σ2tI)^(-1) μui,t-1;
Σ̃ui,t = [(Σui,t-1 + σ2tI)^(-1) + (1/σ20)I]^(-1) (13)

Các phương trình cập nhật tương tự áp dụng cho μ̃vi,t và Σ̃vi,t. Do đó, đóng góp thứ hai trong Eq. 11 (prior) tạo ra một biểu thức dạng đóng. Chúng tôi do đó có thể tính toán gradient của nó.

### 4.2. Làm mượt Skip-Gram

Trái ngược với lọc, trong đó suy luận được điều kiện trên các quan sát quá khứ cho đến thời gian t nhất định, làm mượt (Kalman) thực hiện suy luận dựa trên toàn bộ chuỗi quan sát n±1:T. Cách tiếp cận này tạo ra các quỹ đạo mượt mà hơn và thường có khả năng cao hơn so với lọc, vì bằng chứng được sử dụng từ cả quan sát tương lai và quá khứ.

Bên cạnh sơ đồ suy luận mới, chúng tôi cũng sử dụng một phân phối biến phân khác. Vì mô hình được khớp kết hợp với tất cả các bước thời gian, chúng tôi không còn bị hạn chế bởi phân phối biến phân phân tích theo thời gian. Để đơn giản, chúng tôi tập trung vào phân phối biến phân cho word embedding U; context embedding V được xử lý giống hệt. Chúng tôi sử dụng phân phối phân tích trên cả không gian embedding và không gian từ vựng,

q(U1:T) = ∏(i=1 to L) ∏(k=1 to d) q(uik,1:T). (14)

Trong miền thời gian, xấp xỉ biến phân của chúng tôi có cấu trúc. Để đơn giản hóa ký hiệu, chúng tôi bỏ chỉ số cho từ i và chiều embedding k, do đó chúng tôi viết q(u1:T) cho q(uik,1:T) trong đó chúng tôi tập trung vào một hệ số duy nhất. Hệ số này là phân phối Gaussian đa biến trong miền thời gian với ma trận độ chính xác tam giác Λ,

q(u1:T) = N(μ,Λ^(-1)) (15)

Cả trung bình μ = μ1:T và các phần tử của ma trận độ chính xác tam giác Λ ∈ RT×T đều là tham số biến phân. Điều này mang lại cho phân phối biến phân của chúng tôi sự diễn giải của một hậu nghiệm của bộ lọc Kalman (Blei & Lafferty, 2006), nắm bắt các tương quan theo thời gian.

Chúng tôi khớp các tham số biến phân bằng cách huấn luyện mô hình kết hợp trên tất cả các bước thời gian, sử dụng VI hộp đen và thủ thuật tham số hóa lại. Vì độ phức tạp tính toán của một bước cập nhật tỷ lệ như Θ(L2), trước tiên chúng tôi pretrain mô hình bằng cách rút các minibatch của L' < L từ ngẫu nhiên và L' ngữ cảnh ngẫu nhiên từ từ vựng (Hoffman et al., 2013). Sau đó chúng tôi chuyển sang batch đầy đủ để giảm nhiễu lấy mẫu. Vì phân phối biến phân không phân tích trong miền thời gian, chúng tôi luôn bao gồm tất cả các bước thời gian {1,...,T} trong minibatch.

Chúng tôi cũng suy ra một thuật toán hiệu quả cho phép chúng tôi ước lượng gradient tham số hóa lại sử dụng thời gian và bộ nhớ Θ(T), trong khi việc triển khai ngây thơ suy luận biến phân hộp đen với phân phối biến phân có cấu trúc của chúng tôi sẽ yêu cầu Θ(T2) của cả hai tài nguyên. Ý tưởng chính là tham số hóa Λ = B⊤B theo phân rã Cholesky B, là song đường chéo (Kılıç & Stanica, 2013), và biểu thị gradient của B^(-1) theo gradient của B. Chúng tôi sử dụng mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) để thúc đẩy tính xác định dương của B. Thuật toán được chi tiết trong tài liệu bổ sung của chúng tôi.

## 5. Thực nghiệm

Chúng tôi đánh giá phương pháp của chúng tôi trên ba corpus văn bản có dấu thời gian. Chúng tôi chứng minh rằng các thuật toán của chúng tôi tìm ra các quỹ đạo embedding mượt mà hơn so với các phương pháp dựa trên mô hình tĩnh. Điều này cho phép chúng tôi theo dõi các thay đổi ngữ nghĩa của các từ riêng lẻ bằng cách theo dõi các mối quan hệ láng giềng gần nhất theo thời gian. Trong phân tích định lượng của chúng tôi, chúng tôi tìm thấy khả năng dự đoán cao hơn trên dữ liệu giữ lại so với các baseline của chúng tôi.

**Thuật toán và Baseline.** Chúng tôi báo cáo kết quả từ các thuật toán đề xuất của chúng tôi từ phần 4 và so sánh với các baseline từ phần 2:

• **SGI** biểu thị mô hình skip-gram không Bayesian với khởi tạo ngẫu nhiên độc lập của các vector từ (Mikolov et al., 2013b). Chúng tôi đã sử dụng triển khai riêng của mô hình bằng cách bỏ prior lọc Kalman và ước lượng điểm các vector embedding. Các vector từ tại các thời gian gần nhau được làm cho có thể so sánh bằng các phép biến đổi trực giao gần đúng, tương ứng với Hamilton et al. (2016).

• **SGP** biểu thị cách tiếp cận tương tự như trên, nhưng với các vector từ và ngữ cảnh được khởi tạo trước với các giá trị từ năm trước, như trong Kim et al. (2014).

• **DSG-F**: lọc skip-gram động (đề xuất).

• **DSG-S**: làm mượt skip-gram động (đề xuất).

**Dữ liệu và tiền xử lý.** Ba corpus của chúng tôi minh họa các giới hạn đối lập cả về khoảng thời gian được bao phủ và lượng văn bản mỗi bước thời gian.

1. Chúng tôi đã sử dụng dữ liệu từ corpus Google books (Michel et al., 2011) từ hai thế kỷ qua (T = 209). Điều này bao gồm 5 triệu sách được số hóa và khoảng 10^10 từ quan sát. Corpus bao gồm các bảng n-gram với n ∈ {1,...,5}, được chú thích theo năm xuất bản. Chúng tôi xem xét các năm từ 1800 đến 2008 (mới nhất có sẵn). Vào năm 1800, kích thước dữ liệu là khoảng ~7·10^7 từ. Chúng tôi đã sử dụng số đếm 5-gram, tạo ra kích thước cửa sổ ngữ cảnh là 4.

2. Chúng tôi đã sử dụng các bài phát biểu "State of the Union" (SoU) của các tổng thống Mỹ, kéo dài hơn hai thế kỷ, tạo ra T = 230 bước thời gian khác nhau và khoảng 10^6 từ quan sát. Một số tổng thống đã đưa ra cả bài phát biểu bằng văn bản và bằng miệng; nếu những điều này cách nhau ít hơn một tuần, chúng tôi nối chúng lại và sử dụng ngày trung bình. Chúng tôi chuyển đổi tất cả các từ thành chữ thường và xây dựng số đếm mẫu dương n+ij sử dụng kích thước cửa sổ ngữ cảnh là 4.

3. Chúng tôi đã sử dụng corpus Twitter của các tweet tin tức cho 21 ngày được rút ngẫu nhiên từ 2010 đến 2016. Số lượng tweet trung bình mỗi ngày là 722. Chúng tôi chuyển đổi tất cả các tweet thành chữ thường và sử dụng kích thước cửa sổ ngữ cảnh là 4, mà chúng tôi hạn chế trong các tweet đơn lẻ.

**Siêu tham số.** Từ vựng cho mỗi corpus được xây dựng từ 10.000 từ thường xuyên nhất trong giai đoạn thời gian nhất định. Trong corpus Google books, số lượng từ mỗi năm tăng lên gấp 200 lần từ năm 1800 đến 2008. Để tránh từ vựng bị chi phối bởi các từ hiện đại, chúng tôi chuẩn hóa tần suất từ riêng biệt cho mỗi năm trước khi cộng chúng lại.

Đối với corpus Google books, chúng tôi chọn chiều embedding d = 200, cũng được sử dụng trong Kim et al. (2014). Chúng tôi đặt d = 100 cho SoU và Twitter, vì d = 200 dẫn đến overfitting trên những corpus nhỏ hơn nhiều này. Tỷ lệ η = ∑ij n-ij,t / ∑ij n+ij,t của các cặp từ-ngữ cảnh âm với dương là η = 1. Việc xây dựng chính xác các ma trận n±t được giải thích trong tài liệu bổ sung.

Chúng tôi đã sử dụng phương sai prior toàn cục σ20 = 1 cho tất cả corpus và tất cả thuật toán, bao gồm các baseline. Hằng số khuếch tán D kiểm soát thang thời gian mà thông tin được chia sẻ giữa các bước thời gian. Giá trị tối ưu cho D phụ thuộc vào ứng dụng. Một corpus duy nhất có thể thể hiện các thay đổi ngữ nghĩa của các từ trên các thang thời gian khác nhau, và sự lựa chọn tối ưu cho D phụ thuộc vào thang thời gian mà người ta quan tâm. Chúng tôi đã sử dụng D = 10^(-3) mỗi năm cho Google books và SoU, và D = 1 mỗi năm cho corpus Twitter, kéo dài một phạm vi thời gian ngắn hơn nhiều. Trong tài liệu bổ sung, chúng tôi cung cấp chi tiết về quy trình tối ưu hóa.

**Kết quả định tính.** Chúng tôi cho thấy rằng cách tiếp cận của chúng tôi tạo ra các quỹ đạo word embedding mượt mà trên tất cả ba corpus. Chúng tôi có thể tự động phát hiện các từ trải qua thay đổi ngữ nghĩa đáng kể theo thời gian.

Hình 1 trong phần giới thiệu cho thấy sự khớp của thuật toán lọc skip-gram động với corpus Google books. Ở đây, chúng tôi cho thấy mười từ có vector từ thay đổi mạnh nhất trong 150 năm qua về khoảng cách cosine. Hình 3 hình dung các đám mây word embedding qua bốn năm liên tiếp của Google books, nơi chúng tôi so sánh DSG-F với SGI. Chúng tôi ánh xạ các vector embedding chuẩn hóa thành hai chiều sử dụng t-SNE động (Rauber et al., 2016) (xem phần bổ sung để biết chi tiết). Các đường chỉ ra sự thay đổi của vector từ so với năm trước. Trong mô hình của chúng tôi, chỉ có vài từ thay đổi vị trí của chúng trong không gian embedding một cách nhanh chóng, trong khi các embedding sử dụng SGI cho thấy sự dao động mạnh, làm cho chuyển động của đám mây khó theo dõi.

Hình 4 hình dung độ mượt mà của các quỹ đạo trực tiếp trong không gian embedding (không có phép chiếu thành hai chiều). Chúng tôi xem xét sự khác biệt giữa các vector từ trong năm 1998 và 10 năm tiếp theo. Cụ thể hơn, chúng tôi tính toán biểu đồ của khoảng cách Euclidean ||uit - ui,t+δ|| trên các chỉ số từ i, trong đó δ = 1,...,10 (như đã thảo luận trước đó, SGI sử dụng phép quay toàn cục để căn chỉnh embedding một cách tối ưu trước). Trong mô hình của chúng tôi, các vector embedding dần dần di chuyển ra khỏi vị trí ban đầu khi thời gian tiến triển, cho thấy chuyển động có hướng. Ngược lại, cả hai mô hình baseline chỉ cho thấy ít chuyển động có hướng sau bước thời gian đầu tiên, gợi ý rằng hầu hết các thay đổi thời gian là do dao động kích thước hữu hạn của n±ij,t.

Cách tiếp cận của chúng tôi cho phép chúng tôi phát hiện các thay đổi ngữ nghĩa trong việc sử dụng các từ cụ thể. Hình 5 và 1 đều cho thấy khoảng cách cosine giữa một từ nhất định và các từ láng giềng của nó (đường màu) như một hàm của thời gian. Hình 5 cho thấy kết quả trên tất cả ba corpus và tập trung vào so sánh các phương pháp. Chúng tôi thấy rằng DSG-S và DSG-F (cả hai đều được đề xuất) tạo ra các quỹ đạo hiển thị ít nhiễu hơn so với các baseline SGP và SGI. Thực tế là các baseline dự đoán khoảng cách cosine bằng không (không có tương quan) giữa các cặp từ được chọn trên corpus SoU và Twitter cho thấy rằng những corpus này quá nhỏ để khớp thành công các mô hình này, trái ngược với cách tiếp cận của chúng tôi chia sẻ thông tin trong miền thời gian. Lưu ý rằng như trong các mô hình chủ đề động, làm mượt skip-gram (DSG-S) có thể khuếch tán thông tin vào quá khứ (xem "presidential" thành "clinton-trump" trong Hình 5).

**Kết quả định lượng.** Chúng tôi cho thấy rằng cách tiếp cận của chúng tôi tổng quát hóa tốt hơn cho dữ liệu chưa thấy. Qua đó chúng tôi phân tích khả năng dự đoán giữ lại trên các cặp từ-ngữ cảnh tại thời gian t nhất định, trong đó t bị loại khỏi tập huấn luyện,

1/|n±t| log p(n±t|Ũt,Ṽt). (16)

Ở trên, |n±t| = ∑i,j (n+ij,t + n-ij,t) biểu thị tổng số cặp từ-ngữ cảnh tại thời gian τt. Vì suy luận khác nhau trong tất cả các cách tiếp cận, các định nghĩa của ma trận word và context embedding Ũt và Ṽt trong Eq. 16 phải được điều chỉnh:

• Đối với SGI và SGP, chúng tôi đã thực hiện một lượt theo thứ tự thời gian qua chuỗi thời gian và sử dụng các embedding Ũt = Ut-1 và Ṽt = Vt-1 từ bước thời gian trước đó để dự đoán thống kê n±ij,t tại bước thời gian t.

• Đối với DSG-F, chúng tôi đã thực hiện cùng một lượt để kiểm tra n±ij,t. Qua đó chúng tôi đặt Ũt và Ṽt là các mode Ut-1, Vt-1 của hậu nghiệm gần đúng tại bước thời gian trước đó.

• Đối với DSG-S, chúng tôi giữ lại 10%, 10% và 20% tài liệu từ corpus Google books, SoU và Twitter để kiểm tra, tương ứng. Sau khi huấn luyện, chúng tôi ước lượng word embedding (context embedding) Ũt (Ṽt) trong Eq. 16 bằng nội suy tuyến tính giữa các giá trị của Ut-1 (Vt-1) và Ut+1 (Vt+1) trong mode của phân phối biến phân, có tính đến rằng các dấu thời gian τt nói chung không được đặt cách đều nhau.

Các khả năng dự đoán như một hàm của thời gian τt được hiển thị trong Hình 6. Đối với corpus Google Books (bảng trái trong hình 6), log khả năng dự đoán tăng theo thời gian với tất cả bốn phương pháp. Đây phải là một hiện tượng của corpus vì SGI không mang thông tin nào của quá khứ. Một giải thích có thể là số lượng từ tăng lên mỗi năm từ 1800 đến 2008 trong corpus Google Books. Trên tất cả ba corpus, sự khác biệt giữa hai triển khai của mô hình tĩnh (SGI và SGP) là nhỏ, điều này gợi ý rằng việc khởi tạo trước embedding với kết quả trước đó có thể cải thiện tính liên tục của chúng nhưng dường như có ít tác động đến khả năng dự đoán. Log khả năng cho bộ lọc skip-gram (DSG-F) tăng trong vài bước thời gian đầu tiên khi bộ lọc thấy nhiều dữ liệu hơn, sau đó bão hòa. Sự cải thiện của cách tiếp cận chúng tôi so với mô hình tĩnh đặc biệt rõ ràng trong corpus SoU và Twitter, nhỏ hơn nhiều so với corpus Google books khổng lồ. Ở đó, việc chia sẻ thông tin qua thời gian là quan trọng vì có ít dữ liệu tại mỗi lát thời gian. Làm mượt skip-gram vượt trội hơn lọc skip-gram vì nó chia sẻ thông tin theo cả hai hướng thời gian và sử dụng phân phối biến phân linh hoạt hơn.

## 6. Kết luận

Chúng tôi đã trình bày mô hình skip-gram động: một mô hình xác suất Bayesian kết hợp word2vec với chuỗi thời gian liên tục tiềm ẩn. Chúng tôi đã chỉ ra thực nghiệm rằng cả lọc skip-gram động (chỉ điều kiện trên các quan sát quá khứ) và làm mượt skip-gram động (sử dụng tất cả dữ liệu) đều dẫn đến các vector embedding thay đổi mượt mà tốt hơn trong việc dự đoán thống kê từ-ngữ cảnh tại các bước thời gian giữ lại. Lợi ích mạnh nhất khi dữ liệu tại các bước thời gian riêng lẻ nhỏ, sao cho việc khớp mô hình word embedding tĩnh khó khăn. Cách tiếp cận của chúng tôi có thể được sử dụng như một công cụ khai thác dữ liệu và phát hiện bất thường khi streaming văn bản trên mạng xã hội, cũng như một công cụ cho các nhà sử học và nhà khoa học xã hội quan tâm đến sự tiến hóa của ngôn ngữ.
