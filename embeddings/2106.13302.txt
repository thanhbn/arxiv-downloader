# 2106.13302.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/embeddings/2106.13302.pdf
# File size: 495624 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
byteSteady: Fast Classification Using Byte-Level n-Gram
Embeddings
Xiang Zhang
fancyzhx@gmail.com
Element AI
Montreal, Quebec, CanadaAlexandre Drouin
alexandre.drouin@servicenow.com
Element AI, ServiceNow
Montreal, Quebec, CanadaRaymond Li
raymond.li@servicenow.com
Element AI, ServiceNow
Montreal, Quebec, Canada
ABSTRACT
This article introduces byteSteady – a fast model for classification
using byte-level 𝑛-gram embeddings. byteSteady assumes that each
input comes as a sequence of bytes. A representation vector is pro-
duced using the averaged embedding vectors of byte-level 𝑛-grams,
with a pre-defined set of 𝑛. The hashing trick is used to reduce the
number of embedding vectors. This input representation vector
is then fed into a linear classifier. A straightforward application
of byteSteady is text classification. We also apply byteSteady to
one type of non-language data – DNA sequences for gene classifi-
cation. For both problems we achieved competitive classification
results against strong baselines, suggesting that byteSteady can
be applied to both language and non-language data. Furthermore,
we find that simple compression using Huffman coding does not
significantly impact the results, which offers an accuracy-speed
trade-off previously unexplored in machine learning.
1 INTRODUCTION
Classification of sequential data such as text is a fundamental task in
machine learning. Recently, tools such as fastText [ 17] and Vowpal
Wabbit [ 27] have shown that simple models can achieve state-of-
the-art results for text classification. Moreover, it was shown that
fastText could be trained at the character level and still produce
results competitive with a word-level model [ 29]. This prompts us
to explore an even lower level of data organization: byte level and
investigate whether training models at such a primal level allows
their applicability beyond textual data.
In this article, we demonstrate a fastText-like model that is hard-
coded to process input at the level of bytes, named byteSteady , and
show that it can achieve competitive classification results com-
pared to word-level fastText models and byte-level convolutional
networks. byteSteady assumes that inputs come as sequences of
bytes. Like in fastText [ 17], a representation vector is produced
using the averaged embedding vectors of byte-level 𝑛-grams, with
a pre-defined set of 𝑛. The hashing trick [ 27] is used to reduce the
number of embedding vectors. This input representation vector is
then fed into a linear classifier to produce the desired class.
A byte-level sequence classification model has the potential to
work beyond language data. In this article, we also apply byteSteady
to gene classification for which the data comes as sequences of
nucleotides. We show that byteSteady not only requires far less data
pre-processing than previous standard models in bioinformatics,
but also achieved better results for a large-scale gene classification
data. Coincidentally, byte-level 𝑛-grams of nucleotides are also
called𝑘-mers in genomics [5].
When using a simple classifier such as logistic regression, much
labor is usually required to design features under the assumptionthat raw signals are unlikely to be linearly separable. Conversely,
deep learning models such as convolutional and recurrent networks
are usually applied to sequential data at the raw signal level to ex-
tract hierarchical representation for classification. Results using
byteSteady for text classification and gene classification have weak-
ened this assumption and the methodological dichotomy, and paved
the way for more exploration in using simple models for low-level
inputs.
Byte-level inputs can also be compressed using standard data
compression algorithms. In this article, we also show that for both
text classification and gene classification tasks, the performance is
not significantly impacted by compressing inputs using Huffman
coding [ 16]. This provides a unique accuracy-speed trade-off that
was not previously explored for machine learning.
Prior work has explored byte-level text processing. For example,
in [14], an LSTM-based [ 15] sequence-to-sequence [ 6] [24] model
is applied at byte level for a variety of natural language processing
(NLP) tasks in 4 Romance languages. For NLP, the advantage of
byte-level processing is that models can be immediately applied to
any language regardless of whether there are too many entities at
character or word levels. It alleviates the curse-of-dimensionality
problem [ 2] with large vocabularies. Building on these advantages,
we believe that this work is the first to explore the intersection of
simple models (e.g., fastText-like) and byte-level processing, and
demonstrate applicability outside the realm of language-based data.
Contributions: In a summary, this paper makes the following
contributions: 1) demonstrate that byteSteady’s byte-level encod-
ing can achieve competitive performance for text classification; 2)
extend its applicability beyond the scope of language data using
gene classification as an example; 3) show that trivial speed-up and
a new type of accuracy-speed tradeoff is possible using standard
compression algorithms like Huffman coding. In addition, 4) a new
large-scale gene classification dataset was built to promote further
research in this domain.
2 BYTESTEADY MODEL
As shown in figure 1, byteSteady assumes that each sample consists
of an input byte sequence and a output class label. The byte se-
quence is parsed into many byte-level 𝑛-grams using a pre-defined
set of𝑛, which we call the 𝑛-gram set. Note that these 𝑛-grams are
just sub-sequences of the input. For each 𝑛-gram, we compute a
hash value, and use its modulo with respect to the pre-defined total
number of embeddings as the index to query an embedding vector.
This pre-defined number of embeddings is called the hash table size.
Then, all of these embedding vectors are averaged together to pro-
duce the representation vector for the whole input byte sequence.arXiv:2106.13302v1  [cs.CL]  24 Jun 2021

--- PAGE 2 ---
Xiang Zhang, Alexandre Drouin, and Raymond Li
62 79 74 65 53 74 65 61 64 79 
62 79 
79 74 
74 65 
65 53 
...0
3
3
4
...Input bytes 
n-grams Hash & modulo 
Embedding Vectors Representation Vector 
Query & 
average Linear Classifier 
Output label 
Figure 1: Illustration of byteSteady
This input representation vector is fed into a linear classifier to
produce the desired class using the negative log-likelihood loss.
Except for the hard-coded byte-level sequence processing, byteSteady
uses the same machine learning model as fastText [ 17]. Combining
the embedding table and the linear classifier, the entire model can
be thought of as a 2-layer neural network without non-linearity.
Assuming𝐴is the embedding matrix and 𝐵is the linear classifier
parameter matrix, the loss for each sample can be represented as
−𝑦log(softmax(𝐵𝐴𝑥)), (1)
in which𝑦is the one-hot encoded label and 𝑥is the frequency-of-
grams vector for the sample. 𝐴and𝐵are the parameters to learn.
It is well known that such a 2-layer neural network does not have
more representational capacity than a linear model. In the case that
the embedding dimension is larger than or equal to the number of
classes, the capacity is the same.
For byte sequences such as texts and DNAs, 𝑥is usually sparse.
This makes it possible to use online hashing, sparse matrix-vector
multiplication and parallelization across samples for speeding up.
Similar to fastText, we use the HogWILD! [ 21] algorithm for fast
learning on the CPU. The hashing trick [ 27] used in byteSteady has
been shown to work well for word and character level text classifi-
cation. We implemented Fowler–Noll–Vo (FNV) and CityHash as
2 hash function variants for byteSteady, and did not observe any
difference in accuracy and speed.
Besides showing that such bag of tricks work well for text classi-
fication at the level of bytes, byte-level processing opens some other
unique possibilities for classification task in general. The first is
that byte-level processing can be applied to types of data other than
text – after all, all data in our computers are encoded as sequences
of bytes. In this article, we explore gene classification as an example.
Secondly, compression can be applied to byte-sequences. Since its
outputs are also bytes, we could attempt to apply byteSteady on the
shorter compressed byte sequence for classification. For both text
classification and gene classification, we show that it is possible to
find𝑛-gram set configurations that work well for compressed data
using Huffman coding [ 16]. This offers an accuracy-speed trade-off
that is unexplored in machine learning before.
3 TASKS
In this section, we introduce the text classification and gene classi-
fication tasks with comparison between byteSteady and previous
state-of-the-art models. Ablation studies for byteSteady on both of
these tasks are presented in the next sections.3.1 Text Classification
The text classification datasets used in this article are the same as
in [29]. In total, there are 14 large-scale datasets in 4 languages
including Chinese, English, Japanese and Korean. There are 2 kinds
of tasks in these datasets, which are sentiment analysis and topic
classification. Moreover, 2 of the datasets are constructed by com-
bining samples in all 4 languages to test the model’s ability to handle
different languages in the same fashion. Table 1 is a summarization.
In table 1, the first 9 datasets are sentiment analysis datasets in
either Chinese, English, Japanese or Korean, from either restaurant
review or online shopping websites. The following 3 datasets are
topic classification datasets in either Chinese or English, from online
news websites. The last two are multi-lingual datasets by combining
online shopping reviews from 4 languages.
It is worth noting that word-level or character-level models re-
quire significant pre-preprocessing for these languages, and the
resulting vocabulary is large. It bears the risk of the curse-of-
dimensionality [ 2]. Processing text at the level of bytes and using
the hashing trick [ 27] alleviated these problems, and previous re-
sults [ 14] [29] suggest that byte-level deep learning models could
achieve competitive accuracy. However, as far as we know, we are
the first to present results on byte-level text processing using simple
models.
For hyper-parameter search, a development-validation split is
constructed using a 90%-10% split on the training subset of each
dataset. In the ablation study later, we will show that byteSteady
results are insensitive to the hash table size and the embedding
dimension as long as they are large enough. In this section, we use
a hash table size of 16,777,216 (equals to 224) and an embedding
dimension of 16. Later ablation study will show that byteSteady
results are sensitive to the 𝑛-gram set and the weight decay. For all
datasets, the hyper-parameter search experiments suggest that the
best𝑛-gram set configuration is {4,8,12,16}, and the best weight
decay is 0.001.
Using these settings and the same comparison protocol as [ 29],
we present the testing errors in table 1 with comparisons to word-
level 5-gram fastText models and byte-level one-hot encoded con-
volutional networks (ConvNets). These comparison models are the
best of their kind for these datasets [ 29], and represent strong base-
lines. The results in Table 1 suggest that byte-level simple models
can achieve competitive results for text classification, sometimes
surpassing previous state-of-the-arts. When byteSteady is not the
state-of-the-art, it is not far from the best model.

--- PAGE 3 ---
byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings
Table 1: Text classification datasets and comparisons. For byteSteady, only one testing error is presented for each dataset in
this article. The byteSteady model uses 𝑛-gram set{4,8,12,16}and a weight decay of 0.001, which was chosen after a hyper-
parameter search using a development-validation split on each dataset. The embedding dimension is 16, and the hash table
size is 224=16,777,216The comparison models are the 5-gram word-level fastText model and large byte-level convolutional
network (ConvNet) from [29]. Best result for each dataset is highlighted by a bold font.
Dataset Language Class Train Test byteSteady fastText ConvNet
Dianping Chinese 2 2,000K 500K 22.61% 22.62% 23.17%
JD f. Chinese 5 3,000K 250K 50.55% 48.59% 48.10%
JD b. Chinese 2 4,000K 360K 9.66% 9.93% 9.33%
Rakuten f. Japanese 5 4,000K 500K 44.66% 46.31% 45.10%
Rakuten b. Japanese 2 3,400K 400K 5.44% 5.45% 5.93%
11st f. Korean 5 750K 100K 37.36% 38.67% 32.56%
11st b. Korean 2 4,000K 400K 13.78% 13.23% 13.30%
Amazon f. English 5 3,000K 650K 41.88% 40.02% 42.21%
Amazon b. English 2 3,600K 400K 6.22% 5.41% 6.52%
Ifeng Chinese 5 800K 50K 16.92% 16.95% 16.70%
Chinanews Chinese 7 1,400K 112K 10.70% 9.24% 10.62%
NYTimes English 7 1,400K 105K 14.81% 13.23% 14.30%
Joint full (Multiple) 5 10,750K 1,500K 44.04% 43.29% 42.93%
Joint binary (Multiple) 2 15,000K 1,560K 8.72% 8.74% 8.79%
3.2 Gene Classification
We now present an application of byteSteady to a kind of text-like
non-language data – DNA sequences for gene classification. The use
of𝑛-gram representations of DNA sequences is increasingly preva-
lent in the field of genomics, as it allows to circumvent the com-
putationally intensive process of multiple sequence alignment [5].
Such alignment-free sequence analyses have been widely successful
in problems such as DNA sequence assembly [ 3,7], phylogeny
reconstruction and genome comparison [ 10,18,28], genome-wide
phenotype prediction [ 8,11,12,19], regulatory sequence predic-
tion [ 1,13,23], and taxonomic profiling in metagenomics [ 4,20,25].
The coupling of such representations with machine learning
models has led to state-of-the-art results, but comes at the cost
of gigantic feature spaces [ 12,25]. Factors that inflate the number
of observed 𝑛-grams include the natural diversity of sequences
(e.g., mutations) and random variations due to sequencing errors.
To tackle such huge feature spaces, some methods have relied on
filtering𝑛-grams as a preprocessing step [ 22], while others have
turned to out-of-core data processing [ 11,25]. The application of
byteSteady to genomic data is therefore natural, as it can efficiently
handle large 𝑛-gram feature spaces and can be directly applied
to the DNA sequences in byte representation. In genomics, the
𝑛-grams of nucleotides are also called 𝑘-mers.
Our gene classification task consists of bacterial gene sequences
in six high-level categories: antibiotic resistance, transporter, hu-
man homolog, essential gene, virulance factor, and drug target. An
accurate predictor for this task could improve automatic genome
annotation and help detect important genes, such as those associ-
ated with the drug resistance and virulence of pathogenic bacteria.
We rely on a dataset extracted from the Pathosystems Resource
Integration Center (PATRIC) database [ 9,26], which contains high-
quality gene annotations for a large set of publicly available bacte-
rial genomes.The dataset contains a total of 5,111,616 DNA sequences, which
are composed of nucleotides encoded as ASCII characters A, C, G,
and T. The data is distributed evenly into 6classes with 851,936
examples per class. We randomly select 90%of the data for training
and use the remaining 10% for testing. We further partition the
training set using the same proportions to produce a validation
set for hyper-parameter search. Again, we use a hash table size of
16,777,216 (equals to 224) and an embedding dimension of 16. The
hyper-parameter search experiments suggest that the best 𝑛-gram
set configuration is {2,4,6,8,10,12,14,16}, and the best weight
decay is 0.000001.
We show the benefits of using byteSteady in this context, by com-
paring to another linear classifier that requires manual 𝑛-gram fea-
ture selection due to computational constraints. This corresponds
to the reality of practitioners in genomics, which must often resort
to feature selection in order to apply standard machine learning
algorithms to extremely large feature spaces [ 22]. We consider ver-
sions of this baseline that use the top 100,000 and 1,000,000 most
frequent𝑛-grams in the training data.
Our results in Table 2, show that high prediction accuracies can
be achieved, but only when considering large 𝑛-gram sizes (e.g.,
16). This is only achievable for models like byteSteady, that have
the ability to efficiently process the full set of features using the
hashing trick [ 27]. For reference, the best byteSteady model, using
𝑛-gram set 2[1−8]={2,4,6,8,10,12,14,16}and weight decay
10−6, achieves 3.73% testing error. This is the only testing error for
gene classification in this article.
4 ABLATION STUDY
This section presents the ablation studies on 4 hyper-parameters
in byteSteady - the 𝑛-gram set, the weight decay, the embedding
dimension, and the hash table size. The general conclusion is that
the results of byteSteady are highly sensitive to the 𝑛-gram set and

--- PAGE 4 ---
Xiang Zhang, Alexandre Drouin, and Raymond Li
Table 2: Results for the gene classification task. For each model, validation errors are shown for different 𝑛-gram sets. Arith-
metic and set notations are used to provide shortened text. For example, 2[1-8]={2,4,6,8,10,12,14,16}, and 4[0-2]={1,4,16}.
𝑛-gram set{1} { 2} { 4} { 8} { 16} 2[1-8]4[1-4]2[0-4]4[0-2]
Top-100K 70.59% 64.41% 51.99% 12.87% 43.50% 12.89% 12.85% 12.78% 32.68%
Top-1M 70.59% 64.41% 51.99% 12.87% 23.83% 13.88% 13.11% 12.49% 20.21%
byteSteady 71.50% 64.70% 52.57% 11.57% 6.81% 3.79% 4.12% 7.03% 13.76%
the weight decay, but they do not improve much with increasing
embedding dimension and the hash table size after a certain point.
For all of the ablation studies, we perform experiments on both
text classification and gene classification. For the text classification
task, we use the training subset of Dianping and make a 90%-10%
development-validation split. All of the errors reported are on the
validation datasets for both tasks.
4.1𝑛-Gram Set and Weight Decay
When using the byteSteady model for training, we need to provide
an𝑛-gram set for consideration. This is in contrast to some word-
level models such as fastText [ 17], for which we only provide a
single𝑛. In such a case, either only the gram set of {𝑛}or[1−𝑛]
is considered. Instead, we find that byteSteady is sensitive to the
𝑛-gram set, and the configuration [1−𝑛]does not perform the best.
Meanwhile, machine learning models are generally sensitive to
the weight decay parameter. It is often used for the purpose of
regularization, such that the gap between training and testing can
become closer. This also applies to byteSteady. The best parameters
depend on the task and the sample size.
Table 3 details the results on the 𝑛-gram set and weight decay
parameters. All of the numbers are validation errors. Variations
of𝑛-gram sets include the sets of a single 𝑛, the sets of linearly
increasing𝑛, and the sets of exponentially increasing 𝑛. They all
range up to 𝑛=16. Due to the different in sample size, the best
weight decay is different for each task. Text classification results are
shown in{10−2,10−3,10−4}, whereas for gene classification they
are in{10−5,10−6,10−7}. All of these experiments use an embedding
dimension of 16 and a hash table size of 224=16,777,216.
There are a few conclusions from the results. The first is that
longer𝑛-grams give better results in the case of a single valued
set. The second is that for both tasks, the best results come from a
set of linearly increasing 𝑛, albeit not[1−16]which includes all
𝑛-grams in the range. We believe that this is because rich features
like[1−16]bear more risk of over-fitting.
The third conclusion is that the exponentially increasing 𝑛-gram
set2[0−4]={1,2,4,8,16}does not perform significantly worse than
the the linear sets. This offers an additional speed-accuracy trade-
off, for which the computational complexity is O(𝑛)for the linear 𝑛-
gram sets, and O(log(𝑛))for the exponential alternatives. Whether
the decrease in accuracy due to such reduction in computational
complexity is acceptable should be dependent on the problem.
4.2 Embedding Dimension and Hash Table Size
Table 4 details the validation errors for the ablation study on the
embedding dimension for both text classification and gene clas-
sification. For these experiments, we use the 𝑛-gram set 2[0−4]={1,2,4,8,16}and a hash table size of 224=16,777,216. Different
weight decay parameters are used for each task according to the
previous ablation study. These results suggest that the embedding
dimension does not significantly impact the model performance in
terms of accuracy. However, it does directly impact the amount of
memory needed to store the model parameters. Therefore, all of
other experiments in this article always use an embedding dimen-
sion of 16 – a moderate choice.
Table 5 details the validation errors for the ablation study on
the hash table size, for both text classification and gene classifica-
tion. We choose the hash table size ranging from 216=65,536to
226=67,108,864. These experiments use the 𝑛-gram set 2[0−4]=
{1,2,4,8,16}and an embedding dimension of 16. Different weight
decay parameters are used for each task according to the previous
ablation study. From these results, we could conclude that the im-
provement can be observed when we increase the hash table size,
but it becomes marginal after a certain point. As a result, all other
experiments in this article use a moderately large hash table size
224=16,777,216.
Both ablation studies on the embedding dimension and the hash
table size suggest that the byteSteady results are insensitive to these
hyper-parameters when they are large enough. Therefore, future
experiments will only focus on ablation studies for the 𝑛-gram set
and the weight decay.
5 COMPRESSION USING HUFFMAN CODING
This section presents a unique exploration enabled by byte-level
data processing – applying compression on the input byte se-
quences and presenting the resulting shorter sequences to byteSteady
for classification. For text, previous character-level and word-level
models could not apply because compression will render the char-
acter or word boundaries non-existent.
The compression algorithm we use here is Huffman coding [ 16],
using 2 variants for which the output are bits and bytes respectively.
In both cases, we control the compression rate by limiting the byte
length of symbols. We find that the model can perform well in low
compression rates.
5.1 Bit-Level and Byte-Level Huffman Coding
Huffman coding [ 16] works by giving shorter codes for higher
frequency symbols, using a frequency-sorted tree for which the leaf
nodes are symbols. In this article, symbols are defined as a byte sub-
sequences of length 𝑚. When the tree is binary, Huffman coding
outputs binary codes (bits) one at a time. We call this variant the
bit-level Huffman coding. On the other hand, if the tree is 256-ary,
the codes can be generated one byte at a time. We name this variant
the byte-level Huffman coding.

--- PAGE 5 ---
byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings
Table 3: Ablation study on 𝑛-gram set and weight decay. All numbers are validation errors. The rows iterate through 𝑛-gram sets.
Arithmetic and set notations are used to provide shortened text. For example, 2[1-8]={2,4,6,8,10,12,14,16}, and 4[0-2]={1,4,16}.
The columns are the weight decay parameters, which are differently chosen depending on the tasks. All of these experiments
use an embedding dimension of 16 and a hash table size of 224=16,777,216. The best result for each task is highlighted using
a bold font.
Text Classification Gene Classification
10−210−310−410−510−610−7
{1}49.44% 41.54% 39.11% 71.28% 71.50% 71.38%
{2}40.48% 30.51% 28.19% 64.78% 64.70% 64.71%
{4}31.25% 27.11% 28.29% 52.60% 52.57% 52.55%
{8}26.83% 25.37% 26.96% 11.38% 11.57% 11.70%
{16}35.66% 32.77% 34.05% 5.38% 6.81% 7.93%
[1−16]31.61% 26.16% 24.86% 4.27% 3.90% 4.17%
2[1−8]31.50% 25.50% 24.69% 3.86% 3.79% 4.32%
4[1−4]28.97% 24.09% 25.10% 3.90% 4.12% 4.76%
8[1−2]26.07% 24.73% 26.05% 4.79% 5.20% 5.89%
2[0−4]33.99% 26.22% 26.94% 7.15% 7.03% 7.30%
4[0−2]33.57% 27.44% 28.12% 15.97% 13.76% 14.26%
16[0−1]45.97% 37.02% 38.66% 18.00% 22.23% 21.16%
Table 4: Ablation study on embedding dimension. All of the
numbers are validation errors. These experiments use the 𝑛-
gram set 2[0−4]and a hash table size of 224=16,777,216. For
text classification, the weight decay used is 0.001. For gene
classification, it is 0.000001.
4 8 16 32 64
Text 26.13% 26.55% 26.22% 26.88% 26.16%
Gene 7.14% 6.87% 7.30% 6.87% 7.27%
The symbol length 𝑚can exponentially impact the size of the
frequency table. Naturally, larger 𝑚ensures longer symbols can
be considered for shorter codes, and results in better compression
rate. Therefore, 𝑚can be used to control the compression level of
data, and offers a speed-accuracy trade-off that was not explored
in machine learning before. Note that using 𝑚=1for byte-level
Huffman coding will not compress because the symbols and the
codes have the same length.
Table 7 illustrates this property by presenting the compression
ratio for the development and validation datasets of each task. The
difference between development and validation is small in spite of
using the development dictionary to compress the validation data.
These ratio numbers can be directly translated to the reduction in
training time when using compressed input.
The results for Huffman coding experiments are presented in ta-
ble 6. All of the experiments use the 𝑛-gram set 2[0−4]={1,2,4,8,16}.
According to previous results, the best uncompressed model for
this configuration achieves 26.22% for text classification, and 7.01%
for gene classification. Given the best compressed result of 26.49%
and 7.76% using the byte-level Huffman coding with symbol length
𝑚=2, we know that a low level of compression does not affect the
results significantly.On the other hand, for both bit-level and byte-level Huffman
coding, we could observe that the results become worse as more
aggressive compression is used. This offers a unique speed-accuracy
trade-off based on input compression, which is an idea previously
unexplored in machine learning. This could be useful for devices
in a constrained computation and network environment.
If we compare between bit-level Huffman coding of symbol
length 1 with byte-level of length 2 – the lowest compression rates
respectively – byte-level Huffman coding gives better results for
all the tasks. This is because generating byte-level codes preserves
byte boundaries – that is, byte boundaries in the compressed data
were still byte boundaries in the original data. Bit-level Huffman
coding does not preserve boundaries, which may be challenging
for byteSteady to perform well.
5.2 Ablation Study on 𝑛-Gram Set and Weight
Decay
Table 8 details the results for an ablation study on 𝑛-gram set and
weight decay, using the bit-level Huffman coding of symbol length
1. Similar to previous ablation study with uncompressed input, a
composite𝑛-gram set usually works better than a single choice of
𝑛.
However, unlike in the uncompressed ablation study, the results
for a single choice of 𝑛does not necessarily improve as 𝑛increases.
Meanwhile, linearly increasing 𝑛-gram set also does not necessarily
perform the best. These results suggest that hyper-parameter search
should conduct differently when compression is used to speed up
byteSteady at the input level.
6 CONCLUSION
In this article, we introduce byteSteady – a fast model for classifi-
cation using byte-level 𝑛-gram embeddings. The model produces
a representation vector using the averaged embedding vectors of

--- PAGE 6 ---
Xiang Zhang, Alexandre Drouin, and Raymond Li
Table 5: Ablation study on hash table size. All of the numbers are validation errors. These experiments use the 𝑛-gram set
2[0−4]and an embedding dimension of 16. For text classification, the weight decay used is 0.001. For gene classification, it is
0.00001.
216218220222224226
Text 27.11% 27.28% 27.14% 26.23% 26.22% 26.21%
Gene 17.14% 9.61% 8.15% 7.12% 7.30% 7.01%
Table 6: Huffman coding results. All numbers are validation errors. The rows iterate through different Huffman coding mech-
anism. The columns are the weight decay parameters, which are differently chosen depending on the tasks. All of these exper-
iments use the 𝑛-gram set 2[0−4]={1,2,4,8,16}. The best result for each task in each coding mechanism is highlighted using a
bold font. The best results using the same configuration without compression was 26.22% for text classification, and 7.03% for
gene classification.
Text Classification Gene Classification
10−210−310−410−510−610−7
Bit 1 36.88% 27.72% 26.83% 11.48% 13.13% 14.85%
Bit 2 37.58% 29.03% 28.50% 16.48% 18.40% 20.45%
Bit 4 50.13% 35.66% 35.90% 24.28% 26.67% 29.03%
Bit 8 50.13% 47.93% 47.36% 31.89% 34.34% 36.44%
Byte 2 33.25% 27.09% 26.49% 7.76% 8.43% 8.67%
Byte 4 37.16% 28.78% 28.07% 10.47% 11.75% 13.10%
Byte 8 50.13% 38.70% 37.50% 15.37% 17.00% 18.56%
Table 7: Compression ratio for bit-level and byte-level Huffman coding. The compression ratio is defined as the faction of
compressed size divided by the uncompressed size. The second row is the symbol length 𝑚in the dictionary. For both text
classification and gene classification tasks, we show the numbers for both the development and the validation subsets. For
your reference the validation sets use the same compression dictionary obtained from the development sets. Symbol length 1
is ignored for byte-level Huffman coding because it offers no compression.
Bit level Byte level
1 2 4 8 2 4 8
TextDevelopment 0.8615 0.6708 0.4894 0.3534 0.7133 0.5137 0.3644
Validation 0.8616 0.6707 0.4890 0.3454 0.7131 0.5155 0.4064
GeneDevelopment 0.4041 0.3161 0.2796 0.2606 0.5008 0.2527 0.2531
Validation 0.4041 0.3161 0.2796 0.2606 0.5008 0.2527 0.2531
byte-level𝑛-grams, and feeds it into a linear classifier for classi-
fication. The model is the same as fastText [ 17], except for the
hard-coded byte-level input processing mechanism.
byteSteady can be applied to language and non-language data. In
this article, we show experiments in text classification and gene clas-
sification. Competitive results against strong baselines are achieved.
Since byteSteady reads input at the level of bytes, compression can
be applied to the input for speeding up. We show that a low-level of
compression using Huffman coding does not significantly impact
the results, and provide a new speed-accuracy trade-off previously
unexplored in machine learning.
In the future, we hope to extend byteSteady to unsupervised
embedding learning for byte-level 𝑛-grams, and use it for more
kinds of data and problems.REFERENCES
[1]Aaron Arvey, Phaedra Agius, William Stafford Noble, and Christina Leslie. 2012.
Sequence and chromatin determinants of cell-type–specific transcription factor
binding. Genome research 22, 9 (2012), 1723–1734.
[2]Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A
neural probabilistic language model. Journal of machine learning research 3, Feb
(2003), 1137–1155.
[3]Sébastien Boisvert, François Laviolette, and Jacques Corbeil. 2010. Ray: simulta-
neous assembly of reads from a mix of high-throughput sequencing technologies.
Journal of computational biology 17, 11 (2010), 1519–1533.
[4]Sébastien Boisvert, Frédéric Raymond, Élénie Godzaridis, François Laviolette,
and Jacques Corbeil. 2012. Ray Meta: scalable de novo metagenome assembly
and profiling. Genome biology 13, 12 (2012), R122.
[5]Oliver Bonham-Carter, Joe Steele, and Dhundy Bastola. 2014. Alignment-free
genetic sequence comparisons: a review of recent approaches by word analysis.
Briefings in bioinformatics 15, 6 (2014), 890–905.
[6]Kyunghyun Cho, Bart van Merriënboer, Çağlar Gülçehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
Representations using RNN Encoder–Decoder for Statistical Machine Translation.
InProceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP) . Association for Computational Linguistics, Doha, Qatar,
1724–1734.

--- PAGE 7 ---
byteSteady: Fast Classification Using Byte-Level n-Gram Embeddings
Table 8: Ablation study on 𝑛-gram set and weight decay using a bit-level Huffman coding of symbol length 1. All numbers
are validation errors. The rows iterate through 𝑛-gram sets. Arithmetic and set notations are used to provide shortened text.
For example, 2[1-8]={2,4,6,8,10,12,14,16}, and 4[0-2]={1,4,16}. The columns are the weight decay parameters, which are
differently chosen depending on the tasks. The best result for each task is highlighted using a bold font.
Text Classification Gene Classification
10−210−310−410−510−610−7
{1}50.13% 43.63% 42.04% 64.33% 64.33% 64.33%
{2}37.33% 31.35% 30.59% 51.65% 51.53% 51.50%
{4}29.79% 26.93% 28.99% 10.21% 11.69% 13.10%
{8}29.50% 28.54% 29.90% 22.05% 24.89% 26.72%
{16}50.12% 48.50% 47.54% 33.35% 35.65% 37.14%
[1−16]38.76% 27.30% 26.12% 12.62% 14.04% 16.30%
2[1−8]34.42% 27.09% 26.38% 13.28% 15.28% 17.60%
4[1−4]30.54% 26.06% 26.80% 12.87% 15.57% 18.29%
8[1−2]34.81% 29.22% 30.54% 24.80% 27.69% 30.02%
2[0−4]36.88% 27.72% 26.83% 11.48% 13.13% 14.85%
4[0−2]35.96% 27.55% 27.36% 11.15% 12.84% 14.37%
16[0−1]50.13% 43.61% 42.24% 28.97% 30.19% 30.78%
[7]Phillip EC Compeau, Pavel A Pevzner, and Glenn Tesler. 2011. How to apply de
Bruijn graphs to genome assembly. Nature biotechnology 29, 11 (2011), 987–991.
[8]James J Davis, Sébastien Boisvert, Thomas Brettin, Ronald W Kenyon, Chunhong
Mao, Robert Olson, Ross Overbeek, John Santerre, Maulik Shukla, Alice R Wattam,
et al.2016. Antimicrobial resistance prediction in PATRIC and RAST. Scientific
reports 6 (2016), 27930.
[9]James J Davis, Alice R Wattam, Ramy K Aziz, Thomas Brettin, Ralph Butler,
Rory M Butler, Philippe Chlenski, Neal Conrad, Allan Dickerman, Emily M
Dietrich, et al .2020. The PATRIC Bioinformatics Resource Center: expanding
data and analysis capabilities. Nucleic acids research 48, D1 (2020), D606–D612.
[10] Maxime Déraspe, Frédéric Raymond, Sébastien Boisvert, Alexander Culley, Paul H
Roy, François Laviolette, and Jacques Corbeil. 2017. Phenetic comparison of
prokaryotic genomes using k-mers. Molecular biology and evolution 34, 10 (2017),
2716–2729.
[11] Alexandre Drouin, Sébastien Giguère, Maxime Déraspe, Mario Marchand, Michael
Tyers, Vivian G Loo, Anne-Marie Bourgault, François Laviolette, and Jacques
Corbeil. 2016. Predictive computational phenotyping and biomarker discovery
using reference-free genome comparisons. BMC genomics 17, 1 (2016), 1–15.
[12] Alexandre Drouin, Gaël Letarte, Frédéric Raymond, Mario Marchand, Jacques
Corbeil, and François Laviolette. 2019. Interpretable genotype-to-phenotype
classifiers with performance guarantees. Scientific reports 9, 1 (2019), 1–13.
[13] Mahmoud Ghandi, Dongwon Lee, Morteza Mohammad-Noori, and Michael A
Beer. 2014. Enhanced regulatory sequence prediction using gapped k-mer features.
PLoS computational biology 10, 7 (2014).
[14] Dan Gillick, Cliff Brunk, Oriol Vinyals, and Amarnag Subramanya. 2016. Multi-
lingual Language Processing From Bytes. In Proceedings of NAA-HLT . 1296–1306.
[15] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[16] D. A. Huffman. 1952. A Method for the Construction of Minimum-Redundancy
Codes. Proceedings of the IRE 40, 9 (1952), 1098–1101.
[17] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag
of tricks for efficient text classification. arXiv preprint arXiv:1607.01759 (2016).
[18] Chris-Andre Leimeister and Burkhard Morgenstern. 2014. Kmacs: the k-mismatch
average common substring approach to alignment-free sequence comparison.
Bioinformatics 30, 14 (2014), 2000–2008.
[19] Marcus Nguyen, Thomas Brettin, S Wesley Long, James M Musser, Randall J
Olsen, Robert Olson, Maulik Shukla, Rick L Stevens, Fangfang Xia, Hyunseung
Yoo, et al .2018. Developing an in silico minimum inhibitory concentration panel
test for Klebsiella pneumoniae. Scientific reports 8, 1 (2018), 1–11.
[20] Frédéric Raymond, Maxime Déraspe, Maurice Boissinot, Michel G Bergeron, and
Jacques Corbeil. 2016. Partial recovery of microbiomes after antibiotic treatment.
Gut Microbes 7, 5 (2016), 428–434.
[21] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hog-
wild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In
Advances in Neural Information Processing Systems 24 , J. Shawe-Taylor, R. S. Zemel,
P. L. Bartlett, F. Pereira, and K. Q. Weinberger (Eds.). Curran Associates, Inc.,
693–701. http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-
parallelizing-stochastic-gradient-descent.pdf[22] Yvan Saeys, Iñaki Inza, and Pedro Larrañaga. 2007. A review of feature selection
techniques in bioinformatics. bioinformatics 23, 19 (2007), 2507–2517.
[23] Mahfuza Sharmin, Héctor Corrada Bravo, and Sridhar Hannenhalli. 2016. Het-
erogeneity of transcription factor binding specificity models within and across
cell lines. Genome research 26, 8 (2016), 1110–1123.
[24] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning
with neural networks. In Advances in neural information processing systems . 3104–
3112.
[25] Kévin Vervier, Pierre Mahé, Maud Tournoud, Jean-Baptiste Veyrieras, and Jean-
Philippe Vert. 2016. Large-scale machine learning for metagenomics sequence
classification. Bioinformatics 32, 7 (2016), 1023–1032.
[26] Alice R Wattam, David Abraham, Oral Dalay, Terry L Disz, Timothy Driscoll,
Joseph L Gabbard, Joseph J Gillespie, Roger Gough, Deborah Hix, Ronald Kenyon,
et al.2014. PATRIC, the bacterial bioinformatics database and analysis resource.
Nucleic acids research 42, D1 (2014), D581–D591.
[27] Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh
Attenberg. 2009. Feature hashing for large scale multitask learning. In Proceedings
of the 26th Annual International Conference on Machine Learning . ACM, 1113–
1120.
[28] Jia Wen, Raymond HF Chan, Shek-Chung Yau, Rong L He, and Stephen ST Yau.
2014. K-mer natural vector and its application to the phylogenetic analysis of
genetic sequences. Gene 546, 1 (2014), 25–34.
[29] Xiang Zhang and Yann LeCun. 2017. Which Encoding is the Best for Text
Classification in Chinese, English, Japanese and Korean? CoRR abs/1708.02657
(2017). arXiv:1708.02657 http://arxiv.org/abs/1708.02657
