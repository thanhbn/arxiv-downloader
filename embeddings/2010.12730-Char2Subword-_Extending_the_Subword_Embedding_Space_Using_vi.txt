# 2010.12730.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/embeddings/2010.12730.pdf
# Kích thước tệp: 1561237 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Char2Subword: Mở rộng Không gian Nhúng Từ con bằng
Tính Kết hợp Ký tự Mạnh mẽ
Gustavo Aguilar,;Bryan McCann,:Tong Niu,:
Nazneen Rajani,:Nitish Keskar,:Thamar Solorio;
;University of Houston, Houston, TX
:Salesforce Research, Palo Alto, CA
;{gaguilaralas, tsolorio}@uh.edu
:{bmccann, tniu, nazneen.rajani, nkeskar}@salesforce.com
Tóm tắt
Mã hóa cặp byte (BPE) là một thuật toán phổ biến trong quá trình token hóa từ con của các mô hình ngôn ngữ vì nó cung cấp nhiều lợi ích. Tuy nhiên, quá trình này chỉ dựa trên thống kê dữ liệu tiền huấn luyện, khiến tokenizer khó xử lý các chính tả không thường xuyên. Mặt khác, mặc dù mạnh mẽ trước các lỗi chính tả, các mô hình thuần ký tự thường dẫn đến chuỗi dài không hợp lý và khiến mô hình khó học được các từ có nghĩa. Để giảm thiểu những thách thức này, chúng tôi đề xuất một mô-đun từ con dựa trên ký tự (char2subword)¹ học bảng nhúng từ con trong các mô hình tiền huấn luyện như BERT. Mô-đun char2subword của chúng tôi xây dựng biểu diễn từ các ký tự ngoài từ vựng từ con, và nó có thể được sử dụng như một thay thế thả vào cho bảng nhúng từ con. Mô-đun này mạnh mẽ trước các thay đổi cấp độ ký tự như lỗi chính tả, biến đổi từ, chữ hoa-thường, và dấu câu. Chúng tôi tích hợp nó sâu hơn với BERT thông qua tiền huấn luyện trong khi giữ cố định các tham số transformer của BERT – và do đó, cung cấp một phương pháp thực tế. Cuối cùng, chúng tôi cho thấy rằng việc kết hợp mô-đun của chúng tôi vào mBERT cải thiện đáng kể hiệu suất trên benchmark đánh giá chuyển đổi mã ngôn ngữ truyền thông xã hội (LinCE).

1 Giới thiệu
Mã hóa cặp byte (BPE) là một thuật toán phổ biến trong quá trình token hóa giữa các mô hình ngôn ngữ dựa trên transformer như BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), RoBERTa (Liu et al., 2019), và CTRL (Keskar et al., 2019). Phương pháp này giải quyết vấn đề từ vựng mở bằng cách phân đoạn các từ không thấy hoặc hiếm thành các đơn vị từ con nhỏ hơn trong khi giữ kích thước từ vựng hợp lý (Huck et al., 2017; Kudo, 2018; Wang et al., 2019). Tuy nhiên, BPE và các biến thể của nó nhạy cảm với những nhiễu loạn nhỏ trong văn bản, có thể làm biến dạng ý nghĩa của câu (Jones et al., 2020) (xem Hình 1). Hơn nữa, quá trình token hóa này cứng nhắc trước các thay đổi như thêm từ con vào từ vựng hoặc sửa các phân đoạn. Đó là vì token hóa phụ thuộc vào corpus gốc nơi từ vựng được tạo ra (ví dụ, Wikipedia), dẫn đến một tập hợp cố định các mảnh từ con gắn với bảng tra cứu nhúng (Bostrom and Durrett, 2020).

Mặc dù những khía cạnh này không phải là vấn đề với văn bản sạch và định dạng đúng, nhưng đó không phải là trường hợp khi văn bản có nhiều tiếng ồn đáng kể (ví dụ, Wikipedia so với truyền thông xã hội). Văn bản nhiễu có thể dẫn đến các mảnh từ con rộng rãi mỗi từ (xem Hình 1), ngăn cản các mô hình nắm bắt ý nghĩa hiệu quả và thích nghi với các miền như vậy. Điều này đặc biệt nổi bật trên văn bản truyền thông xã hội (Baldwin et al., 2015; Eisenstein, 2013a,b), nơi tiếng ồn thậm chí xuyên suốt các ngôn ngữ và trong các tình huống chuyển đổi mã (Singh et al., 2018; Aguilar et al., 2018; Molina et al., 2016; Das, 2016).

Bài báo này đề xuất một mô-đun từ-ký-tự-đến-từ-con (char2subword) được huấn luyện để xử lý các chính tả hiếm hoặc không thấy một cách mạnh mẽ trong khi ít hạn chế hơn với một phương pháp token hóa cụ thể. Phương pháp của chúng tôi hoạt động như một thay thế thả vào cho bảng nhúng trong các mô hình ngôn ngữ tiền huấn luyện như mBERT. Nó cải thiện hiệu suất và giảm số lượng tham số nhúng 45% mà không hy sinh tốc độ suy luận. Chúng tôi huấn luyện mô-đun của chúng tôi để xấp xỉ bảng nhúng sử dụng các ký tự từ các từ và từ con từ vựng gốc. Thủ tục này tận dụng transfer learning từ bảng nhúng tiền huấn luyện thay vì bắt đầu từ đầu – do đó, tiết kiệm thời gian và tài nguyên tính toán quý giá. Bên cạnh đó, từ vựng từ con cung cấp đủ các mẫu cấp độ ký tự để học từ các token đã được phân đoạn. Chúng tôi tích hợp mô-đun của chúng tôi với các lớp transformer của mBERT thậm chí sâu hơn bằng cách tiếp tục huấn luyện với dữ liệu tiền huấn luyện và mục tiêu MLM. Khi char2subword của chúng tôi được thích nghi với mô hình ngôn ngữ tiền huấn luyện, chúng tôi đánh giá hiệu suất tổng thể của mô hình bằng cách tinh chỉnh nó trên các tác vụ downstream. Chúng tôi thể hiện hiệu quả của phương pháp bằng cách vượt trội mBERT trên benchmark chuyển đổi mã ngôn ngữ truyền thông xã hội (LinCE) (Aguilar et al., 2020), nơi miền tinh chỉnh khác biệt đáng kể so với miền tiền huấn luyện. Kết quả cho thấy mô-đun char2subword cũng có thể nắm bắt chuyển đổi mã trong từ. Ở cấp độ câu, mô hình có thể liên kết các từ từ cùng ngôn ngữ để hỗ trợ dự đoán ngôn ngữ.

Chúng tôi nêu bật các đóng góp chính như sau:
1. Chúng tôi giới thiệu char2subword, một mô-đun mở-từ-vựng và tiết kiệm tham số mới mở rộng từ vựng bị hạn chế miền và cố định trong mBERT (hoặc bất kỳ mô hình tiền huấn luyện nào dựa trên từ con) trong khi bảo tồn ngữ nghĩa của không gian nhúng đa ngôn ngữ.

2. Chúng tôi thể hiện khả năng kết hợp ký tự của mô-đun bằng cách xử lý tiếng ồn mạnh mẽ ở cấp độ ký tự trong khi độc lập ngôn ngữ và linh hoạt với token hóa khác nhau.

3. Chúng tôi phân tích các ưu điểm của mô hình trên các tác vụ downstream và chứng minh việc sử dụng thực tế và khả năng thích nghi với các miền khác bất chấp thay đổi từ vựng.

2 Công trình liên quan
Biểu diễn từ Hầu hết các tiến bộ đột phá ban đầu trong NLP dựa trên biểu diễn nhúng từ từ các phương pháp như word2vec (Mikolov et al., 2013) và GloVe (Pennington et al., 2014). Chúng cho thấy khả năng sắp xếp các từ trong không gian liên tục nhiều chiều mã hóa các mối quan hệ ngữ nghĩa và ý nghĩa (Goldberg và Levy, 2014). Tuy nhiên, các từ hiếm được biểu diễn yếu trong không gian như vậy, và các từ OOV không thể biểu diễn được. Để giảm thiểu điều đó, các nhà nghiên cứu đã đề xuất biểu diễn từ sử dụng mạng neural đệ quy được hướng dẫn bởi hình thái học (Luong et al., 2013), cũng như nhúng hình tố như một phân phối tiên nghiệm trên nhúng từ xác suất (Bhatia et al., 2016). Bất kể, các thách thức vẫn tồn tại trong văn bản nhiễu, nơi người dùng không tuân theo các dạng từ chuẩn (Eisenstein, 2013b). Những vấn đề như vậy trở nên trầm trọng hơn trong truyền thông xã hội do môi trường vốn dĩ đa ngôn ngữ. Cần thêm từ mỗi ngôn ngữ, trong khi tiếng ồn chính tả tồn tại xuyên suốt các ngôn ngữ.

Biểu diễn ký tự Trong khi các hệ thống cấp độ ký tự tỏ ra mạnh mẽ cho phân loại văn bản (Conneau et al., 2017), chúng không thành công trên các tác vụ đa ngôn ngữ như dịch máy neural (NMT) ban đầu (Neubig et al., 2013; Chung et al., 2016). Ngay cả khi hiệu suất thỏa mãn, các hệ thống như vậy phải xử lý các chuỗi ký tự dài dẫn đến một quá trình rất chậm (Costa-jussà và Fonollosa, 2016; Ling et al., 2015b). Ngoài ra, các ngôn ngữ có các hệ thống viết khác nhau và các thuộc tính cụ thể được mã hóa ở cấp độ ký tự. Trong khi một số thuộc tính đó có thể được nắm bắt hiệu quả trên các ngôn ngữ giàu hình thái (ví dụ, Czech và Arabic), các thuộc tính từ các ngôn ngữ khác không có tác động nhiều hơn việc sử dụng từ (ví dụ, tiếng Anh) (Cherry et al., 2018). Những thách thức này cũng áp dụng cho trường hợp của chúng tôi vì chúng tôi tiến hành nghiên cứu trên dữ liệu đa ngôn ngữ với các ngôn ngữ khác nhau về loại hình.

Biểu diễn hybrid Sử dụng từ hoặc ký tự đã thể hiện ưu điểm và nhược điểm ở cả hai phía. Các nhà nghiên cứu đã cố gắng có được điều tốt nhất của cả hai thế giới bằng cách kết hợp ký tự và từ trong một kiến trúc hybrid (Luong và Manning, 2016) nơi mặc định dựa trên nhúng từ tĩnh rút lui về ký tự nếu từ không biết. Các nỗ lực song song tập trung vào các mô hình ngôn ngữ neural nhận thức ký tự (Kim et al., 2016) nơi ý nghĩa được làm giàu bối cảnh bằng mạng highway (Srivastava et al., 2015), cũng như các mô hình ngôn ngữ LSTM dựa trên ký tự xây dựng biểu diễn từ trung gian từ LSTM cấp độ ký tự (Ling et al., 2015a). Các nhúng từ được bối cảnh hóa thành công nhất được xây dựng từ ký tự là các mô hình ngôn ngữ ELMo (Peters et al., 2018) và Flair (Akbik et al., 2018). Xây dựng mô hình từ ký tự có thể dễ dàng thích nghi với các miền truyền thông xã hội (Akbik et al., 2019), bao gồm dữ liệu chuyển đổi mã (Aguilar và Solorio, 2020).

Các mô hình từ con Sennrich et al. (2016) đề xuất token hóa từ con sử dụng thuật toán mã hóa cặp byte (BPE) để cân bằng việc sử dụng ký tự và từ. BPE tự động chọn một từ vựng của từ con cho kích thước từ vựng mong muốn. Thủ tục này đệ quy xây dựng từ con dựa trên ký tự sử dụng tần suất từ (Sennrich et al., 2016). Một biến thể tham lam khác của BPE có thể chọn tiền tố dài nhất để phân đoạn từ (Wu et al., 2016). Thay vì các phiên bản tham lam, phân đoạn có thể xảy ra theo cách ngẫu nhiên; vẽ các ứng viên phân đoạn tại các điểm khác nhau của một từ có thể cải thiện tổng quát hóa (Kudo, 2018). Biến thể WordPiece của BPE được sử dụng trong NMT và các mô hình ngôn ngữ như multilingual BERT (Devlin et al., 2019). Bất kể biến thể nào, những phương pháp này xử lý vấn đề ngoài-từ-vựng bằng cách chia nhỏ các từ không thấy hoặc hiếm thành các mảnh có trong từ vựng. Vấn đề là BPE có thể tạo ra các mảnh từ con không hợp lý về mặt ngôn ngữ. Token hóa BPE không lý tưởng cho các miền truyền thông xã hội vì các quy tắc của nó không nhất thiết áp dụng xuyên suốt các miền, đặc biệt là những miền có tiếng ồn đáng kể và khác biệt chính tả (Bostrom và Durrett, 2020).

Các mô hình kết hợp Ý tưởng sáng tác các vectơ OOV đã được khám phá trước đây (Ling et al., 2015a; Plank et al., 2016). Tuy nhiên, học các vectơ như vậy đòi hỏi một corpus lớn và thời gian tính toán lâu (tức là xử lý ký tự). Pinter et al. (2017) đề xuất học các từ OOV từ một từ điển nhúng từ tiền huấn luyện. Họ xử lý mỗi từ từ từ điển như một chuỗi ký tự và xuất ra một vectơ duy nhất bắt chước nhúng từ liên quan trong từ điển. Schick và Schütze (2019) cải thiện phương pháp này bằng cách giới thiệu bắt chước có chú ý để tính đến bối cảnh, bên cạnh dạng bề mặt của từ.

--- TRANG 2 ---
3 Phương pháp
Cho một từ w, một mô hình từ con tạo ra một chuỗi các mảnh từ con s = {s₀, s₁, ..., sₙ}, sao cho việc nối tất cả các đoạn từ s hoàn toàn tái tạo lại từ w. Bất kể một mảnh từ con có biểu diễn một ký tự trong từ hay không, tất cả các mảnh được xử lý như các đơn vị ngữ nghĩa trong một câu.² Những mảnh như vậy đến từ một hệ thống dựa trên quy tắc không tính đến ngữ nghĩa hoặc hình thái học trong quá trình token hóa. Do đó, token hóa từ con có tác động đáng kể đến sự trừu tượng ngữ nghĩa từ các lớp trên trong các mô hình tiền huấn luyện như mBERT.

Để giảm thiểu những vấn đề như vậy, chúng tôi xây dựng biểu diễn từ từ ký tự. Mô-đun char2subword cho phép các mẫu token hóa linh hoạt, nơi mô hình có thể chia bằng khoảng trắng, sử dụng phương pháp token hóa gốc, hoặc sử dụng một quy trình token hóa khác được định nghĩa bởi người dùng. Có hai giai đoạn chính trong phương pháp đề xuất của chúng tôi: xấp xỉ nhúng từ con với mô-đun char2subword (tức là lý tưởng nhất là nhân bản không gian nhúng E) (Phần 3.1), và tích hợp bối cảnh mô-đun char2subword vào mô hình tiền huấn luyện (Phần 3.2).

3.1 Xấp xỉ nhúng từ con
Xét một từ con sᵢ từ từ vựng V và ma trận nhúng từ con E ∈ R^(|V|×d). Chúng tôi học một hàm có tham số f: R^(|c|×1) → R^d ánh xạ chuỗi ký tự cᵢ = {cᵢ₁, cᵢ₂, ..., cᵢ|sᵢ|} từ từ con sᵢ đến vectơ nhúng tương ứng eᵢ ∈ E:

êᵢ = f(cᵢ) s.t. êᵢ ≈ eᵢ

Để thực hiện điều này, chúng tôi thiết kế một hàm mục tiêu thỏa mãn bốn yêu cầu của chúng tôi; chúng tôi muốn các nhúng: (i) bảo tồn khoảng cách góc của chúng, (ii) tương tự trong chuẩn L2 để ngăn ngừa gián đoạn độ lớn trong các lớp trên của mBERT, (iii) có các láng giềng tương tự trong không gian khoảng cách cosine, và (iv) cuối cùng ánh xạ đến các token giống nhau trong không gian nhúng. Do đó chúng tôi tối ưu f bằng cách tối thiểu hóa hàm mục tiêu tổng thể L(·):

L(cᵢ, eᵢ, yᵢ, f) = L_cos(eᵢ, f(cᵢ)) + L_2(eᵢ, f(cᵢ)) + L_nbr(eᵢ, f(cᵢ)) + L_ce(yᵢ, f(cᵢ))

Bốn mục tiêu của hàm mất mát tương ứng với bốn thuộc tính mong muốn đã nêu. Mục tiêu đầu tiên, L_cos(·), là khoảng cách cosine giữa các vectơ nhúng mục tiêu và dự đoán eᵢ và êᵢ. Bằng cách sử dụng hàm khoảng cách góc, chúng tôi khuyến khích mô hình nhân bản các mối quan hệ ngữ nghĩa và sắp xếp vectơ trong không gian nhúng gốc của E:

L_cos(eᵢ, êᵢ) = 1 - (eᵢ · êᵢ)/(||eᵢ|| ||êᵢ||)

Mục tiêu thứ hai là chuẩn L2 hoặc khoảng cách Euclidean giữa các vectơ eᵢ và êᵢ. Các mục tiêu trước đó không điều chỉnh độ lớn của vectơ dự đoán êᵢ, cho phép đó là một mức độ tự do cho f. Bằng cách sử dụng chuẩn L2, chúng tôi phạt mô hình vì tạo ra một vectơ êᵢ với độ lớn khác với eᵢ. Điều chỉnh độ lớn quan trọng để xấp xỉ sắp xếp vectơ trong không gian nhúng. Chúng tôi giả thuyết rằng các thuộc tính hơi khác nhau trong nhúng E có thể phóng đại sự khác biệt ở các lớp trên của mBERT.

Mục tiêu thứ ba, L_nbr(·), là sai số bình phương trung bình (MSE) của các khoảng cách cosine được tạo ra giữa k láng giềng gần nhất với eᵢ so với khoảng cách của cùng láng giềng đối với êᵢ:

{n₁, ..., nₖ} = topk(eᵢ, E)
L_nbr(eᵢ, êᵢ) = (1/k) Σⱼ₌₁ᵏ (dis(eᵢ, nⱼ) - dis(êᵢ, nⱼ))²

trong đó topk(·, ·) lấy k láng giềng gần nhất theo khoảng cách cosine trong tất cả các vectơ từ con trong E. Ý tưởng cốt lõi của mục tiêu này là buộc khoảng cách giữa êᵢ và các láng giềng n tương tự nhất có thể với khoảng cách giữa cùng láng giềng và eᵢ.

Mục tiêu cuối cùng là mất mát cross-entropy L_ce(·). Chúng tôi sử dụng E như các tham số cố định để chiếu tuyến tính từ nhúng đến từ vựng. Hạng mục mất mát này buộc mô hình học các biểu diễn nhúng chính xác sao cho chúng ánh xạ đến các từ con gốc từ từ vựng V:

L_ce(yᵢ, êᵢ) = -Σⱼ₌₁|V| yᵢⱼ log ŷᵢⱼ
s.t. ŷᵢ = softmax(êᵢ EᵀT)

Mô-đun Char2subword Chúng tôi mô hình hóa f sử dụng kiến trúc transformer (Vaswani et al., 2017). Mô-đun xử lý một mẫu như một chuỗi ký tự cᵢ = {cᵢ₁, cᵢ₂, ..., cᵢₘ} của một từ con sᵢ có độ dài M.³ Chúng tôi biểu diễn chuỗi cᵢ như tổng giữa nhúng ký tự và mã hóa vị trí hình sin. Chúng tôi truyền chuỗi vectơ ký tự kết quả X⁰ đến một chồng l lớp attention, mỗi lớp với k đầu attention. Trên đỉnh l lớp attention, chúng tôi thêm một lớp tuyến tính Wₑ ∈ R^(d×d) theo sau bởi max-pooling và layer normalization cho đầu ra cuối cùng êᵢ (xem định nghĩa đầy đủ trong Phụ lục A).

Tính mạnh mẽ cấp độ ký tự Tính linh hoạt của mô-đun char2subword làm cho việc dạy mô hình tính bất biến văn bản dễ dàng hơn vì các đầu vào hiện được xử lý ở cấp độ ký tự. Chúng tôi tăng cường từ vựng từ con V bằng cách giới thiệu các lỗi chính tả ký tự đơn tự nhiên trong quá trình huấn luyện. Chúng tôi áp dụng một thao tác tại một thời điểm và chỉ với các từ con vượt quá bốn ký tự để giảm cơ hội nhập nhằng giữa các từ con hợp lệ. Các thao tác được mô tả trong Bảng 1 và cái nhìn tổng quan được hiển thị trong Hình 2.⁴

³ Để phân biệt giữa từ và từ con, chúng tôi thêm tiền tố '##' vào chuỗi cᵢ trong trường hợp từ đầy đủ.
⁴ Cho thao tác mistype, chúng tôi sử dụng hơn 100 bố cục bàn phím để đối phó với các ngôn ngữ trong mBERT.

--- TRANG 3 ---
3.2 Tiền huấn luyện với char2subword
Các kỹ thuật trước đây tận dụng kiến thức tiền huấn luyện trong ma trận nhúng E. Tuy nhiên, mô-đun char2subword có thể không được tích hợp với các lớp trên của mBERT tiền huấn luyện vì nó chỉ thấy các từ con riêng lẻ mà không có bối cảnh. Để giảm thiểu điều đó, chúng tôi tiền huấn luyện mô-đun char2subword cùng với mBERT (Gururangan et al., 2020). Chúng tôi không cập nhật tham số trong các lớp trên của mBERT vì mục tiêu là cung cấp mô-đun char2subword như một thay thế thả vào cho E trên các mô hình tiền huấn luyện có sẵn công khai.⁵

Theo Liu et al. (2019), chúng tôi sử dụng mục tiêu mô hình ngôn ngữ che (MLM) động (xem Hình 3). Chúng tôi ngẫu nhiên chọn 15% token từ con và che chúng ở cấp độ ký tự. Chúng tôi thay thế 80% ký tự bằng [MASK], 10% bằng ký tự được chọn ngẫu nhiên và 10% còn lại được giữ nguyên. Chúng tôi cung cấp ký tự cho mô-đun char2subword và đưa ra dự đoán từ từ vựng từ con V.⁶ Chúng tôi tiền huấn luyện mô hình char2subword với 1M chuỗi 512 token từ con từ Wikipedia (200K chuỗi cho mỗi văn bản tiếng Anh, Tây Ban Nha, Hindi, Nepal, và Ả Rập). Sử dụng tích lũy gradient, chúng tôi cập nhật tham số với kích thước batch hiệu quả là 2.000 mẫu. Lưu ý rằng mô hình không yêu cầu tiền huấn luyện rộng rãi vì 1) các tham số lớp trên được khởi tạo từ checkpoint mBERT tiền huấn luyện và giữ cố định trong quá trình huấn luyện, và 2) mô-đun char2subword được khởi tạo từ giai đoạn xấp xỉ nhúng. Do đó, tiền huấn luyện mô hình trong vài epoch là đủ.

3.3 Tinh chỉnh
Khi mô-đun char2subword đã được tối ưu, chúng tôi đánh giá mô hình tiền huấn luyện với mô-đun char2subword trên các tác vụ NLP downstream. Cụ thể, chúng tôi thử nghiệm với hai tình huống: chế độ đầy đủ và chế độ hybrid.

Chế độ đầy đủ Chế độ này hoàn toàn thay thế bảng nhúng từ con trong mBERT (tức là tập hợp các tham số và vectơ) bằng mô-đun char2subword. Ý tưởng của thiết lập này là đánh giá mức độ tốt của việc xấp xỉ không gian nhúng ban đầu trong E. Theo trực giác, nếu char2subword nhân bản hoàn hảo không gian nhúng trong E, thì mô hình tổng thể sẽ hoạt động tương tự như mô hình mBERT gốc. Tuy nhiên, thiết lập này không token hóa thêm một từ; do đó, chuỗi đầu vào có xu hướng ngắn hơn và bảo tồn ý nghĩa hơn (tức là quá nhiều mảnh từ con cho một từ duy nhất có thể làm suy giảm ý nghĩa của nó).

Chế độ hybrid Không giống như chế độ đầy đủ, chế độ này không thay thế bảng nhúng từ con. Thay vào đó, nó sử dụng các vectơ nhúng từ con mặc định cho các từ đầy đủ (tức là không phải mảnh từ con). Mô hình rút lui về nhúng dựa trên ký tự từ mô-đun char2subword khi một từ như một toàn thể không xuất hiện trong từ vựng. Phương pháp này tập trung cụ thể vào từ con thay vì từ đầy đủ, hiệu quả ngăn chặn từ bị chia nhỏ thành mảnh.

4 Thí nghiệm
Xấp xỉ nhúng Mục tiêu của các thí nghiệm xấp xỉ là nhân bản bảng nhúng từ con gốc trong khi đảm bảo tính mạnh mẽ ở cấp độ ký tự. Chúng tôi thử nghiệm với các hàm mục tiêu được mô tả trong Phần 3.1. Chúng tôi sử dụng độ chính xác trung bình để xác định phương pháp tốt nhất (chúng tôi cũng cung cấp độ chính xác để tham khảo).⁷ Các thí nghiệm 1.1-1.4 cho thấy kết quả của mỗi mục tiêu riêng biệt (xem Bảng 2). Đáng chú ý, mục tiêu cross-entropy liên quan nhất để đảm bảo độ chính xác cao (58% so với 28,5% của mục tiêu cosine). Kết hợp tất cả các mục tiêu cho độ chính xác trung bình 60% (thí nghiệm 1.9). Mặc dù thí nghiệm 1.6 và 1.9 hoạt động rất gần nhau (59,9% so với 60%), thí nghiệm sau vẫn bảo tồn nhiều láng giềng hơn dọc theo top k láng giềng dự kiến.

Sau khi tối ưu một mô-đun char2subword (thí nghiệm 1.9), chúng tôi bối cảnh hóa nó trong giai đoạn tiền huấn luyện (Phần 3.2). Kết quả cho thấy độ chính xác tại k giảm đáng kể (Hình 4, "Approx. → Pre-training"). Tuy nhiên, khi khởi động lại xấp xỉ sau tiền huấn luyện, mô hình hoạt động tốt hơn nhiều so với phiên bản xấp xỉ ban đầu đạt độ chính xác trung bình 82,4% (Hình 4, "Approx.→Pre-training→Approx."). Sự cải thiện này cho thấy nhu cầu bối cảnh hóa cho mô-đun char2subword gốc. Bối cảnh hóa bản thân không đảm bảo rằng mô-đun sẽ giống không gian nhúng như trong E (tức là không có gì buộc mô-đun tối ưu cho điều đó). Tuy nhiên, nó căn chỉnh tốt hơn ngữ nghĩa của không gian tạo điều kiện cho việc xấp xỉ.

Tính mạnh mẽ cấp độ ký tự là một khía cạnh thiết yếu khác khi tối ưu char2subword. Chúng tôi thêm các chỉnh sửa ký tự đơn vào giai đoạn huấn luyện được mô tả trong Phần 3.1. Bảng 3 hiển thị các láng giềng của từ business và các biến thể của nó. Khi được cung cấp business mà không có tiếng ồn, các mô-đun char2subword (có và không có tiếng ồn) lấy các láng giềng liên quan ngữ nghĩa. Tuy nhiên, khi từ được viết hoa, các láng giềng không liên quan đến từ business cho char2subword không có tiếng ồn. Ngoài ra, token hóa từ con cho BUSINESS trở thành B-US-INE-SS, làm biến dạng ý nghĩa của từ gốc. Bất kể, char2subword với tiếng ồn có khả năng chống chịu với mẫu viết hoa và có khả năng duy trì ý nghĩa.⁸

Thí nghiệm tinh chỉnh Khi mô-đun được thích nghi với mBERT, chúng tôi benchmark mô hình trong chế độ đầy đủ và hybrid (xem Phần 3.3) sử dụng benchmark LinCE (Aguilar et al., 2020). Đặc biệt, chúng tôi tập trung vào nhận dạng ngôn ngữ (LID), gán nhãn từ loại (POS), nhận dạng thực thể có tên (NER), và phân tích cảm xúc (SA). Bảng 4 hiển thị kết quả của các thí nghiệm sử dụng chế độ đầy đủ và hybrid. Ngoài ra, chúng tôi bao gồm điểm số test của ELMo⁹ như baseline vì ELMo sáng tác biểu diễn của nó từ ký tự. Cho mỗi mô hình đề xuất, chúng tôi sử dụng các phiên bản xấp xỉ và tiền huấn luyện (tức là "Approx. →Pre-training→Approx.") của mô-đun char2subword. Kết quả nhận dạng ngôn ngữ không phải là chỉ báo mạnh mẽ của sự cải thiện vì điểm số đều rất gần nhau.¹⁰ Tuy nhiên, quan trọng là lưu ý rằng mô hình, bất kể phiên bản nào, có thể thực hiện ngang bằng với baseline mBERT. Điều này cho thấy rằng các biểu diễn char2subword tương thích với phần còn lại của mô hình mBERT (tức là các lớp transformer mBERT).

Cho các tác vụ POS và NER, chúng tôi thấy cải thiện so với mBERT. Thí nghiệm hybrid tiền huấn luyện cho Hindi-English tốt hơn đáng kể so với baseline cho cả POS (89,64% so với 87,86%) và NER (74,91% so với 72,94%). Một trong những lý do cho sự tăng hiệu suất này là do tiếng ồn mà việc chia Hindi chuyển tự (tức là Hindi La tinh hóa) tạo ra cho baseline. Ngược lại, char2subword nén các từ chuyển tự thành một vectơ duy nhất, giảm tiếng ồn trong mô hình. Kết quả NER cho Spanish-English (es-en) và Modern Standard Arabic-Egyptian Arabic (msa-arz) cũng vượt qua baseline (64,26% so với 62,66%). Mặc dù không có chuyển tự trong các cặp ngôn ngữ này, vẫn có nhiều tiếng ồn đến từ ngôn ngữ do người dùng tạo ra trên truyền thông xã hội. Ngoài ra, tiền huấn luyện char2subword trên dữ liệu tiếng Tây Ban Nha và Ả Rập cải thiện biểu diễn và tính mạnh mẽ của mô hình cho những ngôn ngữ như vậy.

--- TRANG 4 ---
5 Phân tích
Attention cho nhận dạng ngôn ngữ Hình 5 cho thấy trực quan hóa cho tác vụ LID Spanish-English với một ví dụ chuyển đổi mã nội câu (tức là chuyển đổi mã ở cấp độ mệnh đề của một câu phát biểu). Ví dụ cho thấy các kết nối mạnh nhất ở cấp độ từ (Hình 5 (trái)) xảy ra cho các từ cùng ngôn ngữ. Đặc biệt, từ consequencias hơi mơ hồ vì hình thái của nó chồng lắp đáng kể với cả phiên bản tiếng Anh và tiếng Tây Ban Nha. Với bối cảnh từ các từ tiếng Tây Ban Nha xung quanh, mô hình có thể xác định từ đó là tiếng Tây Ban Nha.

Mặc dù có nhiều mẫu được nắm bắt trong tất cả các đầu trong mBERT, mẫu này cho thấy rằng các từ cùng ngôn ngữ có thể cung cấp hỗ trợ bối cảnh cùng với câu.

Ngoài hỗ trợ bối cảnh, attention cấp độ ký tự đóng vai trò quan trọng khi xây dựng biểu diễn từ. Đặc biệt cho từ này, sự mơ hồ được giới thiệu do chữ cái q. Lưu ý rằng mô-đun char2subword tạo ra các kết nối mạnh mẽ với chữ cái này và các phần nơi có thể xảy ra nhiều mơ hồ hơn. Ví dụ, chữ cái i xảy ra nơi các hậu tố -cias (tiếng Tây Ban Nha) và -ces (tiếng Anh) có thể hoàn thành từ).

Phân tích lỗi Bằng cách kiểm tra các lỗi của mô hình trong ma trận nhầm lẫn cho tập phát triển LID Spanish-English, chúng tôi nhận thấy 112 từ tiếng Anh được dự đoán là tiếng Tây Ban Nha, và 101 từ tiếng Tây Ban Nha được dự đoán là tiếng Anh (xem Bảng 6 trong Phụ lục B cho ma trận nhầm lẫn). Trong số 101 từ tiếng Anh, 63 được xử lý bởi mô-đun char2subword (tức là thông qua backoff). Hầu hết các lỗi này đến từ các từ chồng lắp nhiều về hình thái giữa hai ngôn ngữ. Ví dụ, các từ imagine, rodeos, superego, tacos là chính tả chính xác giữa các ngôn ngữ, trong khi các từ apetite và pajamas thay đổi một chữ cái giữa các ngôn ngữ (ví dụ, apetito, pijamas trong tiếng Tây Ban Nha). Những lỗi này gợi ý rằng tính mạnh mẽ có thể tạo ra một số mơ hồ khi phát hiện ngôn ngữ của văn bản. Đó là, sự khác biệt ký tự đơn có thể biểu thị một ngôn ngữ hoặc ngôn ngữ khác, nhưng các thao tác mạnh mẽ (Bảng 1) có thể làm mờ sự phân biệt như vậy trong giai đoạn xấp xỉ. Các từ khác là thán từ được đánh vần giống nhau (ví dụ, oh, eh, và Muahahahahaha). Ngoài ra, có những trường hợp nhãn ground-truth sai. Ví dụ, từ larges trong câu "La puerta esta abierta para que te larges porque no te has ido"¹¹ được dự đoán chính xác là tiếng Tây Ban Nha dựa trên bối cảnh (tức là chính tả đúng là largues, có nghĩa là đi ra).

Độ dài chuỗi từ con Các chuỗi từ token hóa từ con có cùng độ dài hoặc dài hơn chuỗi token gốc. Định lượng điều đó cho chúng ta biết về cơ hội mà mô hình char2subword mBERT có trong thực tế. Bảng 5 hiển thị thống kê về độ dài chuỗi gốc (Tokens) và độ dài chuỗi sau token hóa từ con (Subword). Lưu ý rằng độ dài chuỗi trung bình có xu hướng nhân đôi trên các tập dữ liệu. Điều này có thể giải thích khoảng cách hiệu suất lớn hơn cho các tác vụ NER và POS tagging so với LID. Các tác vụ trước yêu cầu nhiều ngữ nghĩa hơn, phù hợp với thực tế là từ con làm suy giảm ý nghĩa bằng cách chia thành nhiều mảnh.

Tham số so với hiệu quả Bảng tra cứu từ con trong mBERT cung cấp truy cập ngay lập tức cho văn bản được token hóa vào không gian nhúng, làm cho bảng như vậy rất thuận tiện. Tuy nhiên, quyền truy cập này bị hạn chế cao với một từ vựng được xác định trước, và, trong trường hợp các mô hình đa ngôn ngữ, từ vựng như vậy phải có phạm vi bao phủ đầy đủ cho tất cả các ngôn ngữ liên quan. Các mô hình như mBERT hoặc XLM-R (Conneau et al., 2020) sử dụng hơn 100 ngôn ngữ, dịch thành số lượng lớn tham số chỉ để cho phép văn bản được vector hóa. Cụ thể hơn, mBERT có 177M tham số tổng cộng trong khi chỉ bảng nhúng từ con của nó (|V| ≈ 119K) chiếm 91M tham số—hơn 50% tất cả tham số của mô hình.¹² Mặt khác, mô-đun char2subword giảm số lượng tham số xuống 50M, khoảng 45% ít hơn bảng nhúng từ con, trong khi cũng có khả năng xử lý lỗi chính tả và biến đổi một cách mạnh mẽ. Tuy nhiên, mô-đun này yêu cầu thời gian tính toán nhiều hơn để đưa ra các biểu diễn nhúng cấp độ từ con.

Tấn công đối nghịch Chúng tôi đánh giá tính mạnh mẽ của char2subword bằng cách sử dụng thư viện TextAttack (Morris et al., 2020). Đặc biệt, chúng tôi áp dụng công thức DeepWordBug (Gao et al., 2018) cho tập validation phân tích cảm xúc es-en. Cuộc tấn công bao gồm các biến đổi cấp độ ký tự trên các từ được xếp hạng cao nhất tối thiểu hóa khoảng cách chỉnh sửa của nhiễu loạn. Đáng chú ý, mô-đun char2subword có khả năng chống chịu hơn mBERT trước những cuộc tấn công này; mBERT mất 16,78 điểm độ chính xác có trọng số (56,10 → 39,32), trong khi char2subword + mBERT giảm 12,41 điểm (57,71 → 45,30). Hầu hết các cuộc tấn công ảnh hưởng đến dự đoán trên mBERT là thực thể. Theo trực giác, điều này hợp lý vì BPE chia các trường hợp như vậy thành nhiều mảnh từ con, trong khi char2subword gắn bó với các từ tên và tận dụng bối cảnh.

6 Kết luận
Chúng tôi cung cấp một phương pháp mới lạ, linh hoạt và mạnh mẽ để mở rộng bảng nhúng từ con mBERT. Mô-đun char2subword cung cấp nhiều kiểm soát hơn ở cấp độ token hóa, và nó có thể tạo ra nhúng từ mà không bị hạn chế bởi một từ vựng hoặc phương pháp phân đoạn cố định. Ngoài ra, mô-đun char2subword cho phép tinh chỉnh một ngôn ngữ hoặc miền quan tâm (tức là bằng cách tiền huấn luyện mô-đun char2subword) trong khi bảo tồn các thuộc tính đa ngôn ngữ của nó. Cuối cùng, phương pháp này không giới hạn ở chuyển đổi mã; mô-đun char2subword là một phương pháp tổng quát có thể được áp dụng cho bất kỳ mô hình tiền huấn luyện dựa trên từ hoặc từ con nào.

Lời cảm ơn
Công trình này được tài trợ một phần bởi Quỹ Khoa học Quốc gia dưới grant #1910192.

--- TRANG 5-12 ---
[Phần tham khảo và phụ lục được giữ nguyên định dạng gốc với các chi tiết kỹ thuật, công thức toán học, và bảng biểu]
