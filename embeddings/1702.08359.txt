# 1702.08359.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/embeddings/1702.08359.pdf
# File size: 1722898 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Dynamic Word Embeddings
Robert Bamler1Stephan Mandt1
Abstract
We present a probabilistic language model for
time-stamped text data which tracks the se-
mantic evolution of individual words over time.
The model represents words and contexts by
latent trajectories in an embedding space. At
each moment in time, the embedding vectors
are inferred from a probabilistic version of
word2vec (Mikolov et al., 2013b). These em-
bedding vectors are connected in time through
a latent diffusion process. We describe two
scalable variational inference algorithms—skip-
gram smoothing and skip-gram ﬁltering—that al-
low us to train the model jointly over all times;
thus learning on all data while simultaneously al-
lowing word and context vectors to drift. Experi-
mental results on three different corpora demon-
strate that our dynamic model infers word em-
bedding trajectories that are more interpretable
and lead to higher predictive likelihoods than
competing methods that are based on static mod-
els trained separately on time slices.
1. Introduction
Language evolves over time and words change their mean-
ing due to cultural shifts, technological inventions, or po-
litical events. We consider the problem of detecting shifts
in the meaning and usage of words over a given time span
based on text data. Capturing these semantic shifts requires
a dynamic language model.
Word embeddings are a powerful tool for modeling se-
mantic relations between individual words (Bengio et al.,
2003; Mikolov et al., 2013a; Pennington et al., 2014; Mnih
& Kavukcuoglu, 2013; Levy & Goldberg, 2014; Vilnis &
McCallum, 2014; Rudolph et al., 2016). Word embed-
1Disney Research, 4720 Forbes Avenue, Pittsburgh,
PA 15213, USA. Correspondence to: Robert Bamler
<Robert.Bamler@disneyresearch.com >, Stephan Mandt
<Stephan.Mandt@disneyresearch.com >.
Proceedings of the 34thInternational Conference on Machine
Learning , Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).dings model the distribution of words based on their sur-
rounding words in a training corpus, and summarize these
statistics in terms of low-dimensional vector representa-
tions. Geometric distances between word vectors reﬂect
semantic similarity (Mikolov et al., 2013a) and difference
vectors encode semantic and syntactic relations (Mikolov
et al., 2013c), which shows that they are sensible represen-
tations of language. Pre-trained word embeddings are use-
ful for various supervised tasks, including sentiment anal-
ysis (Socher et al., 2013b), semantic parsing (Socher et al.,
2013a), and computer vision (Fu & Sigal, 2016). As un-
supervised models, they have also been used for the ex-
ploration of word analogies and linguistics (Mikolov et al.,
2013c).
Word embeddings are currently formulated as static mod-
els, which assumes that the meaning of any given word is
the same across the entire text corpus. In this paper, we
propose a generalization of word embeddings to sequential
data, such as corpora of historic texts or streams of text in
social media.
Current approaches to learning word embeddings in a dy-
namic context rely on grouping the data into time bins
and training the embeddings separately on these bins (Kim
et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016).
This approach, however, raises three fundamental prob-
lems. First, since word embedding models are non-convex,
training them twice on the same data will lead to different
results. Thus, embedding vectors at successive times can
only be approximately related to each other, and only if the
embedding dimension is large (Hamilton et al., 2016). Sec-
ond, dividing a corpus into separate time bins may lead to
training sets that are too small to train a word embedding
model. Hence, one runs the risk of overﬁtting to few data
whenever the required temporal resolution is ﬁne-grained,
as we show in the experimental section. Third, due to the
ﬁnite corpus size the learned word embedding vectors are
subject to random noise. It is difﬁcult to disambiguate this
noise from systematic semantic drifts between subsequent
times, in particular over short time spans, where we expect
only minor semantic drift.
In this paper, we circumvent these problems by introducing
a dynamic word embedding model. Our contributions are
as follows:arXiv:1702.08359v2  [stat.ML]  17 Jul 2017

--- PAGE 2 ---
Dynamic Word Embeddings
date0:00:20:40:60:81:0cosine distancetext
photographs
preface
references
memorandumproductivity
diminishing
elasticity
aggregate
utility1. marginal
dateexact
accurate
sampling
observer
cleversoftware
user
machine
device
printer2. computer
datequam
aut
quod
Arizona
aufEffects
Effect
Inﬂuence
Amer
pp.3. versus
datenomination
offender
custody
assignment
votingwillingness
loyalty
dedication
adherence
devotion4. commitment
dateperipheral
basal
os
cortex
nuclearTV
telephone
newspapers
phone
computer5. radio
1850 1900 1950 2000
date0:00:20:40:60:81:0cosine distanceobjective
perception
subjective
verb
verbspossibilities
potentially
possibility
risks
likelihood6. potential
1850 1900 1950 2000
datematerially
faithfully
effectually
clearly
abundantlyespecially
particularly
including
notable
exempliﬁed7. notably
1850 1900 1950 2000
datenobleman
lawyer
knight
member
nobilityclassroom
cognitive
networks
teacher
parent8. peer
1850 1900 1950 2000
dateemphatically
signiﬁcant
gestures
smiled
sharplyconsiderably
substantially
greatly
materially
slightly9. signiﬁcantly
1850 1900 1950 2000
dateprocessing
Planning
Freud
enzyme
specializedcomputer
Web
technology
applications
design10. software
Figure 1. Evolution of the 10words that changed the most in cosine distance from 1850 to 2008 on Google books, using skip-gram
ﬁltering (proposed). Red (blue) curves correspond to the ﬁve closest words at the beginning (end) of the time span, respectively.
•We derive a probabilistic state space model where
word and context embeddings evolve in time accord-
ing to a diffusion process. It generalizes the skip-gram
model (Mikolov et al., 2013b; Barkan, 2017) to a dy-
namic setup, which allows end-to-end training. This
leads to continuous embedding trajectories, smoothes
out noise in the word-context statistics, and allows us
to share information across all times.
•We propose two scalable black-box variational in-
ference algorithms (Ranganath et al., 2014; Rezende
et al., 2014) for ﬁltering and smoothing. These al-
gorithms ﬁnd word embeddings that generalize bet-
ter to held-out data. Our smoothing algorithm carries
out efﬁcient black-box variational inference for struc-
tured Gaussian variational distributions with tridiago-
nal precision matrices, and applies more broadly.
•We analyze three massive text corpora that span over
long periods of time. Our approach allows us to auto-
matically ﬁnd the words whose meaning changes the
most. It results in smooth word embedding trajecto-
ries and therefore allows us to measure and visual-
ize the continuous dynamics of the entire embedding
cloud as it deforms over time.
Figure 1 exempliﬁes our method. The plot shows a ﬁt of
our dynamic skip-gram model to Google books (we give
details in section 5). We show the ten words whose mean-
ing changed most drastically in terms of cosine distance
over the last 150 years. We thereby automatically dis-
cover words such as “computer” or “radio” whose meaning
changed due to technological advances, but also words like“peer” and “notably” whose semantic shift is less obvious.
Our paper is structured as follows. In section 2 we discuss
related work, and we introduce our model in section 3. In
section 4 we present two efﬁcient variational inference al-
gorithms for our dynamic model. We show experimental
results in section 5. Section 6 summarizes our ﬁndings.
2. Related Work
Probabilistic models that have been extended to latent time
series models are ubiquitous (Blei & Lafferty, 2006; Wang
et al., 2008; Sahoo et al., 2012; Gultekin & Paisley, 2014;
Charlin et al., 2015; Ranganath et al., 2015; Jerfel et al.,
2017), but none of them relate to word embeddings. The
closest of these models is the dynamic topic model (Blei &
Lafferty, 2006; Wang et al., 2008), which learns the evo-
lution of latent topics over time. Topic models are based
on bag-of-word representations and thus treat words as
symbols without modelling their semantic relations. They
therefore serve a different purpose.
Mikolov et al. (2013a;b) proposed the skip-gram model
with negative sampling (word2vec) as a scalable word em-
bedding approach that relies on stochastic gradient de-
scent. This approach has been formulated in a Bayesian
setup (Barkan, 2017), which we discuss separately in sec-
tion 3.1. These models, however, do not allow the word
embedding vectors to change over time.
Several authors have analyzed different statistics of text
data to analyze semantic changes of words over time (Mi-
halcea & Nastase, 2012; Sagi et al., 2011; Kim et al., 2014;

--- PAGE 3 ---
Dynamic Word Embeddings
Figure 2. a) Bayesian skip-gram model (Barkan, 2017). b) The
dynamic skip-gram model (proposed) connects Tcopies of the
Bayesian skip-gram model via a latent time series prior on the
embeddings.
Kulkarni et al., 2015; Hamilton et al., 2016). None of them
explicitly model a dynamical process; instead, they slice
the data into different time bins, ﬁt the model separately
on each bin, and further analyze the embedding vectors in
post-processing. By construction, these static models can
therefore not share statistical strength across time. This
limits the applicability of static models to very large cor-
pora.
Most related to our approach are methods based on word
embeddings. Kim et al. (2014) ﬁt word2vec separately on
different time bins, where the word vectors obtained for
the previous bin are used to initialize the algorithm for the
next time bin. The bins have to be sufﬁciently large and the
found trajectories are not as smooth as ours, as we demon-
strate in this paper. Hamilton et al. (2016) also trained
word2vec separately on several large corpora from differ-
ent decades. If the embedding dimension is large enough
(and hence the optimization problem less non-convex), the
authors argue that word embeddings at nearby times ap-
proximately differ by a global rotation in addition to a small
semantic drift, and they approximately compute this ro-
tation. As the latter does not exist in a strict sense, it is
difﬁcult to distinguish artifacts of the approximate rotation
from a true semantic drift. As discussed in this paper, both
variants result in trajectories which are noisier.1
3. Model
We propose the dynamic skip-gram model, a generaliza-
tion of the skip-gram model (word2vec) (Mikolov et al.,
2013b) to sequential text data. The model ﬁnds word em-
bedding vectors that continuously drift over time, allowing
to track changes in language and word usage over short and
long periods of time. Dynamic skip-gram is a probabilistic
model which combines a Bayesian version of the skip-gram
model (Barkan, 2017) with a latent time series. It is jointly
1Rudolph & Blei (2017) independently developed a similar
model, using a different likelihood model. Their approach uses
a non-Bayesian treatment of the latent embedding trajectories,
which makes the approach less robust to noise when the data per
time step is small.trained end-to-end and scales to massive data by means of
approximate Bayesian inference.
The observed data consist of sequences of words from a
ﬁnite vocabulary of size L. In section 3.1, all sequences
(sentences from books, articles, or tweets) are considered
time-independent; in section 3.2 they will be associated
with different time stamps. The goal is to maximize the
probability of every word that occurs in the data given its
surrounding words within a so-called context window. As
detailed below, the model learns two vectors ui,vi∈Rd
for each word iin the vocabulary, where dis the embed-
ding dimension. We refer to uias the word embedding
vector and to vias the context embedding vector.
3.1. Bayesian Skip-Gram Model
The Bayesian skip-gram model (Barkan, 2017) is a prob-
abilistic version of word2vec (Mikolov et al., 2013b) and
forms the basis of our approach. The graphical model is
shown in Figure 2a). For each pair of words i,jin the
vocabulary, the model assigns probabilities that word iap-
pears in the context of word j. This probability is σ(u/latticetop
ivj)
with the sigmoid function σ(x) = 1/(1 +e−x). Letzij∈
{0,1}be an indicator variable that denotes a draw from that
probability distribution, hence p(zij= 1) =σ(u/latticetop
ivj). The
generative model assumes that many word-word pairs (i,j)
are uniformly drawn from the vocabulary and tested for be-
ing a word-context pair; hence a separate random indicator
zijis associated with each drawn pair.
Focusing on words and their neighbors in a context win-
dow, we collect evidence of word-word pairs for which
zij= 1. These are called the positive examples. De-
noten+
ijthe number of times that a word-context pair
(i,j)is observed in the corpus. This is a sufﬁcient statis-
tic of the model, and its contribution to the likelihood is
p(n+
ij|ui,vj) =σ(u/latticetop
ivj)n+
ij. However, the generative pro-
cess also assumes the possibility to reject word-word pairs
ifzij= 0. Thus, one needs to construct a ﬁctitious sec-
ond training set of rejected word-word pairs, called nega-
tive examples. Let the corresponding counts be n−
ij. The
total likelihood of both positive and negative examples is
then
p(n+,n−|U,V) =L/productdisplay
i,j=1σ(u/latticetop
ivj)n+
ijσ(−u/latticetop
ivj)n−
ij.(1)
Above we used the antisymmetry σ(−x) = 1−σ(x). In
our notation, dropping the subscript indices for n+andn−
denotes the entire L×Lmatrices,U= (u1,···,uL)∈
Rd×Lis the matrix of all word embedding vectors, and V
is deﬁned analogously for the context vectors. To con-
struct negative examples, one typically chooses n−
ij∝
P(i)P(j)3/4(Mikolov et al., 2013b), where P(i)is the

--- PAGE 4 ---
Dynamic Word Embeddings
frequency of word iin the training corpus. Thus, n−is
well-deﬁned up to a constant factor which has to be tuned.
Deﬁningn±= (n+,n−)the combination of both positive
and negative examples, the resulting log likelihood is
logp(n±|U,V) =
L/summationdisplay
i,j=1/parenleftbig
n+
ijlogσ(u/latticetop
ivj) +n−
ijlogσ(−u/latticetop
ivj)/parenrightbig
.(2)
This is exactly the objective of the (non-Bayesian) skip-
gram model, see (Mikolov et al., 2013b). The count ma-
tricesn+andn−are either pre-computed for the entire
corpus, or estimated based on stochastic subsamples from
the data in a sequential way, as done by word2vec. Barkan
(2017) gives an approximate Bayesian treatment of the
model with Gaussian priors on the embeddings.
3.2. Dynamic Skip-Gram Model
The key extension of our approach is to use a Kalman ﬁl-
ter as a prior for the time-evolution of the latent embed-
dings (Welch & Bishop, 1995). This allows us to share
information across all times while still allowing the em-
beddings to drift.
Notation. We consider a corpus of Tdocuments which
were written at time stamps τ1<...<τ T. For each time
stept∈{1,...,T}the sufﬁcient statistics of word-context
pairs are encoded in the L×Lmatricesn+
t,n−
tof positive
and negative counts with matrix elements n+
ij,tandn−
ij,t,
respectively. Denote Ut= (u1,t,···,uL,t)∈Rd×Lthe
matrix of word embeddings at time t, and deﬁne Vtcorre-
spondingly for the context vectors. Let U,V∈RT×d×L
denote the tensors of word and context embeddings across
all times, respectively.
Model. The graphical model is shown in Figure 2b). We
consider a diffusion process of the embedding vectors over
time. The variance σ2
tof the transition kernel is
σ2
t=D(τt+1−τt), (3)
whereDis a global diffusion constant and (τt+1−τt)is the
time between subsequent observations (Welch & Bishop,
1995). At every time step t, we add an additional Gaussian
prior with zero mean and variance σ2
0which prevents the
embedding vectors from growing very large, thus
p(Ut+1|Ut)∝N(Ut,σ2
t)N(0,σ2
0). (4)
Computing the normalization, this results in
Ut+1|Ut∼N/parenleftbiggUt
1 +σ2
t/σ2
0,1
σ−2
t+σ−2
0I/parenrightbigg
,(5)
Vt+1|Vt∼N/parenleftbiggVt
1 +σ2
t/σ2
0,1
σ−2
t+σ−2
0I/parenrightbigg
.(6)In practice,σ0/greatermuchσt, so the damping to the origin is very
weak. This is also called Ornstein-Uhlenbeck process (Uh-
lenbeck & Ornstein, 1930). We recover the Wiener process
forσ0→∞ , butσ0<∞prevents the latent time series
from diverging to inﬁnity. At time index t= 1, we deﬁne
p(U1|U0)≡N(0,σ2
0I)and do the same for V1.
Our joint distribution factorizes as follows:
p(n±,U,V ) =T−1/productdisplay
t=0p(Ut+1|Ut)p(Vt+1|Vt)
×T/productdisplay
t=1L/productdisplay
i,j=1p(n±
ij,t|ui,t,vj,t)(7)
The prior model enforces that the model learns embedding
vectors which vary smoothly across time. This allows to as-
sociate words unambiguously with each other and to detect
semantic changes. The model efﬁciently shares informa-
tion across the time domain, which allows to ﬁt the model
even in setups where the data at every given point in time
are small, as long as the data in total are large.
4. Inference
We discuss two scalable approximate inference algorithms.
Filtering uses only information from the past; it is required
in streaming applications where the data are revealed to
us sequentially. Smoothing is the other inference method,
which learns better embeddings but requires the full se-
quence of documents ahead of time.
In Bayesian inference, we start by formulating a joint dis-
tribution (Eq. 7) over observations n±and parameters U
andV, and we are interested in the posterior distribution
over parameters conditioned on observations,
p(U,V|n±) =p(n±,U,V )/integraltext
p(n±,U,V )dUdV(8)
The problem is that the normalization is intractable. In vari-
ational inference (VI) (Jordan et al., 1999; Blei et al., 2016)
one sidesteps this problem and approximates the posterior
with a simpler variational distribution qλ(U,V)by mini-
mizing the Kullback-Leibler (KL) divergence to the poste-
rior. Here,λsummarizes all parameters of the variational
distribution, such as the means and variances of a Gaussian,
see below. Minimizing the KL divergence is equivalent to
optimizing the evidence lower bound (ELBO) (Blei et al.,
2016),
L(λ) =Eqλ[logp(n±,U,V )]−Eqλ[logqλ(U,V)].(9)
For a restricted class of models, the ELBO can be com-
puted in closed-form (Hoffman et al., 2013). Our model is

--- PAGE 5 ---
Dynamic Word Embeddings
non-conjugate and requires instead black-box VI using the
reparameterization trick (Rezende et al., 2014; Kingma &
Welling, 2014).
4.1. Skip-Gram Filtering
In many applications such as streaming, the data arrive se-
quentially. Thus, we can only condition our model on past
and not on future observations. We will ﬁrst describe in-
ference in such a (Kalman) ﬁltering setup (Kalman et al.,
1960; Welch & Bishop, 1995).
In the ﬁltering scenario, the inference algorithm iteratively
updates the variational distribution qas evidence from each
time steptbecomes available. We thereby use a variational
distribution that factorizes across all times, q(U,V) =/producttextT
t=1q(Ut,Vt)and we update the variational factor at a
given timetbased on the evidence at time tand the approx-
imate posterior of the previous time step. Furthermore, at
every timetwe use a fully-factorized distribution:
q(Ut,Vt) =L/productdisplay
i=1N(ui,t;µui,t,Σui,t)N(vi,t;µvi,t.Σvi,t),
The variational parameters are the means µui,t,µvi,t∈Rd
and the covariance matrices Σui,tandΣvi,t, which we re-
strict to be diagonal (mean-ﬁeld approximation).
We now describe how we sequentially compute q(Ut,Vt)
and use the result to proceed to the next time step. As other
Markovian dynamical systems, our model assumes the fol-
lowing recursion,
p(Ut,Vt|n±
1:t)∝p(n±
t|Ut,Vt)p(Ut,Vt|n±
1:t−1).(10)
Within our variational approximation, the ELBO (Eq. 9)
therefore separates into a sum of Tterms,L=/summationtext
tLtwith
Lt=E[logp(n±
t|Ut,Vt)] +E[logp(Ut,Vt|n±
1:t−1)]
−E[logq(Ut,Vt)],(11)
where all expectations are taken under q(Ut,Vt). We com-
pute the entropy term −E[logq]in Eq. 11 analytically and
estimate the gradient of the log likelihood by sampling
from the variational distribution and using the reparam-
eterization trick (Kingma & Welling, 2014; Salimans &
Kingma, 2016). However, the second term of Eq. 11, con-
taining the prior at time t, is still intractable. We approxi-
mate the prior as
p(Ut,Vt|n±
1:t−1)≡
Ep(Ut−1,Vt−1|n±
1:t−1)/bracketleftbig
p(Ut,Vt|Ut−1,Vt−1)/bracketrightbig
≈Eq(Ut−1,Vt−1)/bracketleftbig
p(Ut,Vt|Ut−1,Vt−1)/bracketrightbig
.(12)
The remaining expectation involves only Gaussians and
can be carried-out analytically. The resulting approximateprior is a fully factorized distribution p(Ut,Vt|n±
1:t−1)≈/producttextL
i=1N(ui,t; ˜µui,t,˜Σui,t)N(vi,t; ˜µvi,t,˜Σvit)with
˜µui,t=˜Σui,t/parenleftbig
Σui,t−1+σ2
tI/parenrightbig−1µui,t−1;
˜Σui,t=/bracketleftBig/parenleftbig
Σui,t−1+σ2
tI/parenrightbig−1+ (1/σ2
0)I/bracketrightBig−1
.(13)
Analogous update equations hold for ˜µvi,tand˜Σvi,t. Thus,
the second contribution in Eq. 11 (the prior) yields a closed-
form expression. We can therefore compute its gradient.
4.2. Skip-Gram Smoothing
In contrast to ﬁltering, where inference is conditioned on
past observations until a given time t, (Kalman) smoothing
performs inference based on the entire sequence of obser-
vationsn±
1:T. This approach results in smoother trajectories
and typically higher likelihoods than with ﬁltering, because
evidence is used from both future and past observations.
Besides the new inference scheme, we also use a different
variational distribution. As the model is ﬁtted jointly to all
time steps, we are no longer restricted to a variational distri-
bution that factorizes in time. For simplicity we focus here
on the variational distribution for the word embeddings U;
the context embeddings Vare treated identically. We use a
factorized distribution over both embedding space and vo-
cabulary space,
q(U1:T) =L/productdisplay
i=1d/productdisplay
k=1q(uik,1:T). (14)
In the time domain, our variational approximation is struc-
tured. To simplify the notation we now drop the indices
for wordsiand embedding dimension k, hence we write
q(u1:T)forq(uik,1:T)where we focus on a single factor.
This factor is a multivariate Gaussian distribution in the
time domain with tridiagonal precision matrix Λ,
q(u1:T) =N(µ,Λ−1) (15)
Both the means µ=µ1:Tand the entries of the tridiago-
nal precision matrix Λ∈RT×Tare variational parameters.
This gives our variational distribution the interpretation of a
posterior of a Kalman ﬁlter (Blei & Lafferty, 2006), which
captures correlations in time.
We ﬁt the variational parameters by training the model
jointly on all time steps, using black-box VI and the repa-
rameterization trick. As the computational complexity of
an update step scales as Θ(L2), we ﬁrst pretrain the model
by drawing minibatches of L/prime< L random words and
L/primerandom contexts from the vocabulary (Hoffman et al.,
2013). We then switch to the full batch to reduce the sam-
pling noise. Since the variational distribution does not fac-
torize in the time domain we always include all time steps
{1,...,T}in the minibatch.

--- PAGE 6 ---
Dynamic Word Embeddings
We also derive an efﬁcient algorithm that allows us to es-
timate the reparametrization gradient using Θ(T)time and
memory, while a naive implementation of black-box varia-
tional inference with our structured variational distribution
would require Θ(T2)of both resources. The main idea is to
parametrize Λ =B/latticetopBin terms of its Cholesky decompo-
sitionB, which is bidiagonal (Kılıc ¸ & Stanica, 2013), and
to express gradients of B−1in terms of gradients of B. We
use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle,
2003) to enforce positive deﬁniteness of B. The algorithm
is detailed in our supplementary material.
5. Experiments
We evaluate our method on three time-stamped text cor-
pora. We demonstrate that our algorithms ﬁnd smoother
embedding trajectories than methods based on a static
model. This allows us to track semantic changes of indi-
vidual words by following nearest-neighbor relations over
time. In our quantitative analysis, we ﬁnd higher predictive
likelihoods on held-out data compared to our baselines.
Algorithms and Baselines. We report results from our
proposed algorithms from section 4 and compare against
baselines from section 2:
•SGI denotes the non-Bayesian skip-gram model
with independent random initializations of word vec-
tors (Mikolov et al., 2013b). We used our own imple-
mentation of the model by dropping the Kalman ﬁl-
tering prior and point-estimating embedding vectors.
Word vectors at nearby times are made comparable by
approximate orthogonal transformations, which corre-
sponds to Hamilton et al. (2016).
•SGP denotes the same approach as above, but with
word and context vectors being pre-initialized with the
values from the previous year, as in Kim et al. (2014).
•DSG-F: dynamic skip-gram ﬁltering (proposed).
•DSG-S: dynamic skip-gram smoothing (proposed).
Data and preprocessing. Our three corpora exemplify
opposite limits both in the covered time span and in the
amount of text per time step.
1. We used data from the Google books corpus2(Michel
et al., 2011) from the last two centuries ( T= 209 ).
This amounts to 5million digitized books and approx-
imately 1010observed words. The corpus consists of
n-gram tables with n∈{1,..., 5}, annotated by year
of publication. We considered the years from 1800 to
2http://storage.googleapis.com/books/
ngrams/books/datasetsv2.html2008 (the latest available). In 1800 , the size of the data
is approximately∼7·107words. We used the 5-gram
counts, resulting in a context window size of 4.
2. We used the “State of the Union” ( SoU) addresses of
U.S. presidents, which spans more than two centuries,
resulting inT= 230 different time steps and approx-
imately 106observed words.3Some presidents gave
both a written and an oral address; if these were less
than a week apart we concatenated them and used the
average date. We converted all words to lower case
and constructed the positive sample counts n+
ijusing
a context window size of 4.
3. We used a Twitter corpus of news tweets for 21ran-
domly drawn dates from 2010 to2016 . The median
number of tweets per day is 722. We converted all
tweets to lower case and used a context window size
of4, which we restricted to stay within single tweets.
Hyperparameters. The vocabulary for each corpus was
constructed from the 10,000most frequent words through-
out the given time period. In the Google books corpus, the
number of words per year grows by a factor of 200from the
year1800 to2008 . To avoid that the vocabulary is domi-
nated by modern words we normalized the word frequen-
cies separately for each year before adding them up.
For the Google books corpus, we chose the embedding
dimensiond= 200 , which was also used in Kim et al.
(2014). We set d= 100 for SoU and Twitter, as d= 200
resulted in overﬁtting on these much smaller corpora. The
ratioη=/summationtext
ijn−
ij,t//summationtext
ijn+
ij,tof negative to positive word-
context pairs was η= 1. The precise construction of the
matricesn±
tis explained in the supplementary material.
We used the global prior variance σ2
0= 1 for all corpora
and all algorithms, including the baselines. The diffusion
constantDcontrols the time scale on which information
is shared between time steps. The optimal value for D
depends on the application. A single corpus may exhibit
semantic shifts of words on different time scales, and the
optimal choice for Ddepends on the time scale in which
one is interested. We used D= 10−3per year for Google
books and SoU, and D= 1per year for the Twitter corpus,
which spans a much shorter time range. In the supplemen-
tary material, we provide details of the optimization proce-
dure.
Qualitative results. We show that our approach results in
smooth word embedding trajectories on all three corpora.
We can automatically detect words that undergo signiﬁcant
semantic changes over time.
Figure 1 in the introduction shows a ﬁt of the dynamic
skip-gram ﬁltering algorithm to the Google books corpus.
3http://www.presidency.ucsb.edu/sou.php

--- PAGE 7 ---
Dynamic Word Embeddings
Figure 3. Word embeddings over a sequence of years trained on
Google books, using DSG-F (proposed, top row) and compared
to the static method by Hamilton et al. (2016) (bottom). We used
dynamic t-SNE (Rauber et al., 2016) for dimensionality reduc-
tion. Colored lines in the second to fourth column indicate the tra-
jectories from the previous year. Our method infers smoother tra-
jectories with only few words that move quickly. Figure 4 shows
that these effects persist in the original embedding space.
Here, we show the ten words whose word vectors change
most drastically over the last 150 years in terms of co-
sine distance. Figure 3 visualizes word embedding clouds
over four subsequent years of Google books, where we
compare DSG-F against SGI. We mapped the normal-
ized embedding vectors to two dimensions using dynamic
t-SNE (Rauber et al., 2016) (see supplement for details).
Lines indicate shifts of word vectors relative to the preced-
ing year. In our model only few words change their position
in the embedding space rapidly, while embeddings using
SGI show strong ﬂuctuations, making the cloud’s motion
hard to track.
Figure 4 visualizes the smoothness of the trajectories di-
rectly in the embedding space (without the projection to
two dimensions). We consider differences between word
vectors in the year 1998 and the subsequent 10 years.
In more detail, we compute histograms of the Euclidean
distances||uit−ui,t+δ||over the word indexes i, where
δ= 1,..., 10(as discussed previously, SGI uses a global
rotation to optimally align embeddings ﬁrst). In our model,
embedding vectors gradually move away from their origi-
nal position as time progresses, indicating a directed mo-
tion. In contrast, both baseline models show only little di-
rected motion after the ﬁrst time step, suggesting that most
temporal changes are due to ﬁnite-size ﬂuctuations of n±
ij,t.
Initialization schemes alone, thus, seem to have a minor
effect on smoothness.
Our approach allows us to detect semantic shifts in the us-
age of speciﬁc words. Figures 5 and 1 both show the cosine
distance between a given word and its neighboring words
(colored lines) as a function of time. Figure 5 shows results
on all three corpora and focuses on a comparison across
methods. We see that DSG-S and DSG-F (both proposed)
Figure 4. Histogram of distances between word vectors in the
year 1998 and their positions in subsequent years (colors).
DSG-F (top panel) displays a continuous growth of these dis-
tances over time, indicating a directed motion. In contrast, in
SGP (middle) (Kim et al., 2014) and SGI (bottom) (Hamilton
et al., 2016), the distribution of distances jumps from the ﬁrst to
the second year but then remains largely stationary, indicating ab-
sence of a directed drift; i.e. almost all motion is random.
result in trajectories which display less noise than the base-
lines SGP and SGI. The fact that the baselines predict zero
cosine distance (no correlation) between the chosen word
pairs on the SoU and Twitter corpora suggests that these
corpora are too small to successfully ﬁt these models, in
contrast to our approach which shares information in the
time domain. Note that as in dynamic topic models, skip-
gram smoothing (DSG-S) may diffuse information into the
past (see ”presidential” to ”clinton-trump” in Fig. 5).
Quantitative results. We show that our approach gener-
alizes better to unseen data. We thereby analyze held-out
predictive likelihoods on word-context pairs at a given time
t, wheretis excluded from the training set,
1
|n±
t|logp(n±
t|˜Ut,˜Vt). (16)
Above,|n±
t|=/summationtext
i,j/parenleftbig
n+
ij,t+n−
ij,t/parenrightbig
denotes the total num-
ber of word-context pairs at time τt. Since inference is dif-
ferent in all approaches, the deﬁnitions of word and con-
text embedding matrices ˜Utand˜Vtin Eq. 16 have to be
adjusted:
•For SGI and SGP, we did a chronological pass
through the time sequence and used the embeddings
˜Ut=Ut−1and˜Vt=Vt−1from the previous time
step to predict the statistics n±
ij,tat time stept.
•For DSG-F, we did the same pass to test n±
ij,t. We
thereby set ˜Utand˜Vtto be the modes Ut−1,Vt−1of
the approximate posterior at the previous time step.
•For DSG-S, we held out 10%,10% and20% of the
documents from the Google books, SoU, and Twitter
corpora for testing, respectively. After training, we
estimated the word (context) embeddings ˜Ut(˜Vt) in

--- PAGE 8 ---
Dynamic Word Embeddings
date0.00.51.0cos. distance"computer" to "accurate"Google Books
date"community" to "nature""State of the Union" addresses
2010 2011 2012 2013 2014 2015 2016
date"presidential" to "barack"Twitter
DSG-S
DSG-F
SGI
SGP
1800 1850 1900 1950 2000
date0.00.51.0cos. distance"computer" to "machine"
1800 1850 1900 1950 2000
date"community" to "transportation"
2010 2011 2012 2013 2014 2015 2016
date"presidential" to "clinton-trump"
Figure 5. Smoothness of word embedding trajectories, compared across different methods. We plot the cosine distance between two
words (see captions) over time. High values indicate similarity. Our methods (DSG-S and DSG-F) ﬁnd more interpretable trajectories
than the baselines (SGI and SGP). The different performance is most pronounced when the corpus is small (SoU and Twitter).
1800 1850 1900 1950 2000
date of test books−0.56−0.55−0.54−0.53−0.52normalized log(predictive likelihood)
Google books
DSG-S
DSG-F
SGI
SGP
1800 1850 1900 1950 2000
date of test SoU address−0.85−0.80−0.75−0.70−0.65"State of the Union" addresses
DSG-S
DSG-F
SGI
SGP
2010 2011 2012 2013 2014 2015 2016
date of test tweets−0.85−0.80−0.75−0.70−0.65Twitter
DSG-S
DSG-F
SGI
SGP1800 1820−0.56−0.55−0.54
1980 2000−0.53−0.52
2015−0.845−0.840−0.835
1882 1884−0.770−0.768
Figure 6. Predictive log-likelihoods (Eq. 16) for two proposed versions of the dynamic skip-gram model (DSG-F & DSG-S) and two
competing methods SGI (Hamilton et al., 2016) and SGP (Kim et al., 2014) on three different corpora (high values are better).
Eq. 16 by linear interpolation between the values of
Ut−1(Vt−1) andUt+1(Vt+1) in the mode of the vari-
ational distribution, taking into account that the time
stampsτtare in general not equally spaced.
The predictive likelihoods as a function of time τtare
shown in Figure 6. For the Google Books corpus (left panel
in ﬁgure 6), the predictive log-likelihood grows over time
with all four methods. This must be an artifact of the cor-
pus since SGI does not carry any information of the past.
A possible explanation is the growing number of words per
year from 1800 to2008 in the Google Books corpus. On
all three corpora, differences between the two implementa-
tions of the static model (SGI and SGP) are small, which
suggests that pre-initializing the embeddings with the pre-
vious result may improve their continuity but seems to have
little impact on the predictive power. Log-likelihoods for
the skip-gram ﬁlter (DSG-F) grow over the ﬁrst few time
steps as the ﬁlter sees more data, and then saturate. The
improvement of our approach over the static model is par-
ticularly pronounced in the SoU and Twitter corpora, which
are much smaller than the massive Google books corpus.
There, sharing information between across time is crucial
because there is little data at every time slice. Skip-gram
smoothing outperforms skip-gram ﬁltering as it shares in-formation in both time directions and uses a more ﬂexible
variational distribution.
6. Conclusions
We presented the dynamic skip-gram model: a Bayesian
probabilistic model that combines word2vec with a latent
continuous time series. We showed experimentally that
both dynamic skip-gram ﬁltering (which conditions only
on past observations) and dynamic skip-gram smoothing
(which uses all data) lead to smoothly changing embedding
vectors that are better at predicting word-context statistics
at held-out time steps. The beneﬁts are most drastic when
the data at individual time steps is small, such that ﬁtting a
static word embedding model is hard. Our approach may
be used as a data mining and anomaly detection tool when
streaming text on social media, as well as a tool for his-
torians and social scientists interested in the evolution of
language.
Acknowledgements
We would like to thank Marius Kloft, Cheng Zhang,
Andreas Lehrmann, Brian McWilliams, Romann Weber,
Michael Clements, and Ari Pakman for valuable feedback.

--- PAGE 9 ---
Dynamic Word Embeddings
References
Barkan, Oren. Bayesian Neural Word Embedding. In Pro-
ceedings of the Thirty-First AAAI Conference on Artiﬁ-
cial Intelligence , 2017.
Beck, Amir and Teboulle, Marc. Mirror Descent and Non-
linear Projected Subgradient Methods for Convex Opti-
mization. Operations Research Letters , 31(3):167–175,
2003.
Ben-Tal, Aharon, Margalit, Tamar, and Nemirovski,
Arkadi. The Ordered Subsets Mirror Descent Optimiza-
tion Method with Applications to Tomography. SIAM
Journal on Optimization , 12(1):79–108, 2001.
Bengio, Yoshua, Ducharme, R ´ejean, Vincent, Pascal, and
Jauvin, Christian. A Neural Probabilistic Language
Model. Journal of Machine Learning Research , 3:1137–
1155, 2003.
Blei, David M and Lafferty, John D. Dynamic Topic Mod-
els. In Proceedings of the 23rd International Conference
on Machine Learning , pp. 113–120. ACM, 2006.
Blei, David M., Kucukelbir, Alp, and McAuliffe, Jon D.
Variational Inference: A Review for Statisticians. arXiv
preprint arXiv:1601.00670 , 2016.
Charlin, Laurent, Ranganath, Rajesh, McInerney, James,
and Blei, David M. Dynamic Poisson Factorization.
InProceedings of the 9th ACM Conference on Recom-
mender Systems , pp. 155–162, 2015.
Fu, Yanwei and Sigal, Leonid. Semi-Supervised
V ocabulary-Informed Learning. In Proceedings of
the IEEE Conference on Computer Vision and Pattern
Recognition , pp. 5337–5346, 2016.
Gultekin, San and Paisley, John. A Collaborative Kalman
Filter for Time-Evolving Dyadic Processes. In Proceed-
ings of the 2nd International Conference on Data Min-
ing, pp. 140–149, 2014.
Hamilton, William L, Leskovec, Jure, and Jurafsky, Dan.
Diachronic word embeddings reveal statistical laws of
semantic change. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguis-
tics, pp. 1489–1501, 2016.
Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John William. Stochastic Variational Inference.
Journal of Machine Learning Research , 14(1):1303–
1347, 2013.
Jerfel, Ghassen, Basbug, Mehmet E, and Engelhardt, Bar-
bara E. Dynamic Compound Poisson Factorization. In
Artiﬁcial Intelligence and Statistics , 2017.Jordan, Michael I, Ghahramani, Zoubin, Jaakkola,
Tommi S, and Saul, Lawrence K. An Introduction to
Variational Methods for Graphical Models. Machine
learning , 37(2):183–233, 1999.
Kalman, Rudolph Emil et al. A New Approach to Linear
Filtering and Prediction Problems. Journal of Basic En-
gineering , 82(1):35–45, 1960.
Kılıc ¸, Emrah and Stanica, Pantelimon. The Inverse of
Banded Matrices. Journal of Computational and Applied
Mathematics , 237(1):126–135, 2013.
Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Dar-
shan, and Petrov, Slav. Temporal Analysis of Language
Through Neural Language Models. In Proceedings of
the ACL 2014 Workshop on Language Technologies and
Computational Social Science , pp. 61–65, 2014.
Kingma, Diederik P and Welling, Max. Auto-Encoding
Variational Bayes. In Proceedings of the 2nd Interna-
tional Conference on Learning Representations (ICLR) ,
2014.
Kulkarni, Vivek, Al-Rfou, Rami, Perozzi, Bryan, and
Skiena, Steven. Statistically Signiﬁcant Detection of
Linguistic Change. In Proceedings of the 24th Inter-
national Conference on World Wide Web , pp. 625–635,
2015.
Levy, Omer and Goldberg, Yoav. Neural Word Embedding
as Implicit Matrix Factorization. In Advances in Neural
Information Processing Systems , pp. 2177–2185, 2014.
Michel, Jean-Baptiste, Shen, Yuan Kui, Aiden,
Aviva Presser, Veres, Adrian, Gray, Matthew K,
Pickett, Joseph P, Hoiberg, Dale, Clancy, Dan, Norvig,
Peter, Orwant, Jon, et al. Quantitative Analysis of
Culture Using Millions of Digitized Books. Science ,
331(6014):176–182, 2011.
Mihalcea, Rada and Nastase, Vivi. Word Epoch Dis-
ambiguation: Finding how Words Change Over Time.
InProceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics: Short Papers-
Volume 2 , pp. 259–263, 2012.
Mikolov, Tomas, Chen, Kai, Corrado, Greg, and Dean, Jef-
frey. Efﬁcient Estimation of Word Representations in
Vector Space. arXiv preprint arXiv:1301.3781 , 2013a.
Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed Representations of
Words and Phrases and their Compositionality. In Ad-
vances in Neural Information Processing Systems 26 , pp.
3111–3119. 2013b.

--- PAGE 10 ---
Dynamic Word Embeddings
Mikolov, Tomas, Yih, Wen-tau, and Zweig, Geoffrey. Lin-
guistic Regularities in Continuous Space Word Repre-
sentations. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies
(NAACL-HLT-2013) , pp. 746–751, 2013c.
Mnih, Andriy and Kavukcuoglu, Koray. Learning Word
Embeddings Efﬁciently with Noise-Contrastive Estima-
tion. In Advances in Neural Information Processing Sys-
tems, pp. 2265–2273, 2013.
Pennington, Jeffrey, Socher, Richard, and Manning,
Christopher D. Glove: Global Vectors for Word Rep-
resentation. In EMNLP , volume 14, pp. 1532–43, 2014.
Ranganath, Rajesh, Gerrish, Sean, and Blei, David M.
Black Box Variational Inference. In AISTATS , pp. 814–
822, 2014.
Ranganath, Rajesh, Perotte, Adler J, Elhadad, No ´emie, and
Blei, David M. The Survival Filter: Joint Survival Anal-
ysis with a Latent Time Series. In UAI, pp. 742–751,
2015.
Rauber, Paulo E., Falc ˜ao, Alexandre X., and Telea, Alexan-
dru C. Visualizing Time-Dependent Data Using Dy-
namic t-SNE. In EuroVis 2016 - Short Papers , 2016.
Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic Backpropagation and Approximate In-
ference in Deep Generative Models. In The 31st Interna-
tional Conference on Machine Learning (ICML) , 2014.
Rudolph, Maja and Blei, David. Dynamic Bernoulli
Embeddings for Language Evolution. arXiv preprint
arXiv:1703.08052 , 2017.
Rudolph, Maja, Ruiz, Francisco, Mandt, Stephan, and Blei,
David. Exponential Family Embeddings. In Advances
in Neural Information Processing Systems , pp. 478–486,
2016.
Sagi, Eyal, Kaufmann, Stefan, and Clark, Brady. Trac-
ing Semantic Change with Latent Semantic Analysis.
Current Methods in Historical Semantics , pp. 161–183,
2011.
Sahoo, Nachiketa, Singh, Param Vir, and Mukhopadhyay,
Tridas. A Hidden Markov Model for Collaborative Fil-
tering. MIS Quarterly , 36(4):1329–1356, 2012.
Salimans, Tim and Kingma, Diederik P. Weight Normaliza-
tion: A Simple Reparameterization to Accelerate Train-
ing of Deep Neural Networks. In Advances in Neural
Information Processing Systems , pp. 901–901, 2016.Socher, Richard, Bauer, John, Manning, Christopher D,
and Ng, Andrew Y . Parsing with Compositional Vector
Grammars. In ACL (1) , pp. 455–465, 2013a.
Socher, Richard, Perelygin, Alex, Wu, Jean Y , Chuang, Ja-
son, Manning, Christopher D, Ng, Andrew Y , and Potts,
Christopher. Recursive Deep Models for Semantic Com-
positionality over a Sentiment Treebank. In Proceedings
of the 2013 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , volume 1631, pp.
1642, 2013b.
Uhlenbeck, George E and Ornstein, Leonard S. On the
Theory of the Brownian Motion. Physical Review , 36
(5):823, 1930.
Vilnis, Luke and McCallum, Andrew. Word Representa-
tions via Gaussian Embedding. In Proceedings of the
2nd International Conference on Learning Representa-
tions (ICLR) , 2014.
Wang, Chong, Blei, David, and Heckerman, David. Con-
tinuous time dynamic topic models. In Proceedings of
the Twenty-Fourth Conference on Uncertainty in Artiﬁ-
cial Intelligence , pp. 579–586, 2008.
Welch, Greg and Bishop, Gary. An Introduction to the
Kalman Filter. 1995.

--- PAGE 11 ---
Supplementary Material to “Dynamic Word Embeddings”
Robert Bamler1Stephan Mandt1
Table S1. Hyperparameters for skip-gram ﬁltering and skip-gram
smoothing.
PARAMETER COMMENT
L= 104vocabulary size
L/prime= 103batch size for smoothing
d= 100 embedding dimension for SoU and Twitter
d= 200 embedding dimension for Google books
Ntr= 5000 number of training steps for each t(ﬁltering)
N/prime
tr= 5000 number of pretraining steps with minibatch
sampling (smoothing; see Algorithm 2)
Ntr= 1000 number of training steps without minibatch
sampling (smoothing; see Algorithm 2)
cmax= 4 context window size for positive examples
η= 1 ratio of negative to positive examples
γ= 0.75 context exponent for negative examples
D= 10−3diffusion const. per year (Google books & SoU)
D= 1 diffusion const. per year (Twitter)
σ2
0= 1 variance of overall prior
α= 10−2learning rate (ﬁltering)
α/prime= 10−2learning rate during minibatch phase (smoothing)
α= 10−3learning rate after minibatch phase (smoothing)
β1= 0.9 decay rate of 1stmoment estimate
β2= 0.99 decay rate of 2ndmoment estimate (ﬁltering)
β2= 0.999 decay rate of 2ndmoment estimate (smoothing)
δ= 10−8regularizer of Adam optimizer
1. Dimensionality Reduction in Figure 1
To create the word-clouds in Figure 1 of the main text we
mapped the ﬁtted word embeddings from Rdto the two-
dimensional plane using dynamic t-SNE (Rauber et al.,
2016). Dynamic t-SNE is a non-parametric dimension-
ality reduction algorithm for sequential data. The algo-
rithm ﬁnds a projection to a lower dimension by solving
a non-convex optimization problem that aims at preserving
nearest-neighbor relations at each individual time step. In
1Disney Research, 4720 Forbes Avenue, Pittsburgh,
PA 15213, USA. Correspondence to: Robert Bamler
<Robert.Bamler@disneyresearch.com >, Stephan Mandt
<Stephan.Mandt@disneyresearch.com >.
Proceedings of the 34thInternational Conference on Machine
Learning , Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).addition, projections at neighboring time steps are aligned
with each other by a quadratic penalty with prefactor λ≥0
for sudden movements.
There is a trade-off between ﬁnding good local projections
for each individual time step ( λ→0), and ﬁnding smooth
projections (large λ). Since we want to analyze the smooth-
ness of word embedding trajectories, we want to avoid
bias towards smooth projections. Unfortunately, setting
λ= 0is not an option since, in this limit, the optimization
problem is invariant under independent rotations at each
time, rendering trajectories in the two-dimensional projec-
tion plane meaningless. To still avoid bias towards smooth
projections, we anneal λexponentially towards zero over
the course of the optimization. We start the optimizer with
λ= 0.01, and we reduce λby5%with each training step.
We run 100optimization steps in total, so that λ≈6×10−6
at the end of the training procedure. We used the open-
source implementation,1set the target perplexities to 200,
and used default values for all other parameters.
2. Hyperparemeters and Construction of n±
1:T
Table S1 lists the hyperparameters used in our experiments.
For the Google books corpus, we used the same context
window size cmaxand embedding dimension das in (Kim
et al., 2014). We reduced dfor the SoU and Twitter corpora
to avoid overﬁtting to these much smaller data sets.
In constrast to word2vec, we construct our positive and
negative count matrices n±
ij,tdeterministically in a prepro-
cessing step. As detailed below, this is done such that it
resembles as closely as possible the stochastic approach in
word2vec (Mikolov et al., 2013). In every update step,
word2vec stochastically samples a context window size
uniformly in an interval [1,···,cmax], thus the context size
ﬂuctuates and nearby words appear more often in the same
context than words that are far apart from each other in
the sentence. We follow a deterministic scheme that re-
sults in similar statistics. For each pair of words (w1,w2)
in a given sentence, we increase the counts n+
iw1jw2by
max (0,1−k/c max), where 0≤k≤cmax is the num-
ber of words that appear between w1andw2, andiw1and
jw2are the words’ unique indices in the vocabulary.
1https://github.com/paulorauber/thesnearXiv:1702.08359v2  [stat.ML]  17 Jul 2017

--- PAGE 12 ---
Supplementary Material to “Dynamic Word Embeddings”
Algorithm 1 Skip-gram ﬁltering; see section 4 of the main
text.
Remark: All updates are analogous for word and con-
text vectors; we drop their indices for simplicity.
Input: number of time steps T, time stamps τ1:T, posi-
tive and negative examples n±
1:T, hyperparameters.
Init. prior means ˜µik,1←0and variances ˜Σi,1=Id×d
Init. variational means µik,1←0and var. Σi,1=Id×d
fort= 1toTdo
ift/negationslash= 1then
Update approximate Gaussian prior with params.
˜µik,tand˜Σi,tusingµik,t−1andΣi,t−1, see Eq. 13.
end if
Compute entropy Eq[logq(·)]analytically.
Compute expected log Gaussian prior with parameters
˜µik,tand˜Σk,tanalytically.
MaximizeLtin Eq. 11, using black-box VI with the
reparametrization trick.
Obtainµik,tandΣi,tas outcome of the optimization.
end for
We also used a deterministic variant of word2vec to con-
struct the negative count matrices n−
t. In word2vec, ηnega-
tive samples (i,j)are drawn for each positive sample (i,j/prime)
by drawingηindependent values for jfrom a distribution
P/prime
t(j)deﬁned below. We deﬁne n−
ij,tsuch that it matches
the expectation value of the number of times that word2vec
would sample the negative word-context pair (i,j). Specif-
ically, we deﬁne
Pt(i) =/summationtextL
j=1n+
ij,t/summationtextL
i/prime,j=1n+
i/primej,t, (S1)
P/prime
t(j) =/parenleftbig
Pt(j)/parenrightbigγ
/summationtextL
j/prime=1/parenleftbig
Pt(j/prime)/parenrightbigγ, (S2)
n−
ij,t=/parenleftbiggL/summationdisplay
i/prime,j/prime=1n+
i/primej/prime,t/parenrightbigg
ηPt(i)P/prime
t(j). (S3)
We choseγ= 0.75as proposed in (Mikolov et al., 2013),
and we setη= 1. In practice, it is not necessary to explic-
itly construct the full matrices n−
t, and it is more efﬁcient
to keep only the distributions Pt(i)andP/prime
t(j)in memory.
3. Skip-gram Filtering Algorithm
The skip-gram ﬁltering algorithm is described in section 4
of the main text. We provide a formulation in pseudocode
in Algorithm 1.
4. Skip-gram Smoothing Algorithm
In this section, we give details for the skip-gram smoothing
algorithm, see section 4 of the main text. A summary isAlgorithm 2 Skip-gram smoothing; see section 4. We drop
indicesi,j, andkfor word, context, end embedding dimen-
sion, respectively, when they are clear from context.
Input: number of time steps T, time stamps τ1:T, word-
context counts n+
1:T, hyperparameters in Table S1
Obtainn−
t∀tusing Eqs. S1–S3
Initializeµu,1:T,µv,1:T←0
Initializeνu,1:T,νv,1:T,ωu,1:T−1, andωv,1:T−1such
thatB/latticetop
uBu=B/latticetop
vBv= Π (see Eqs. S5 and S11)
forstep= 1toN/prime
trdo
DrawI⊂{ 1,...,L/prime}with|I|=L/primeuniformly
DrawJ⊂{ 1,...,L/prime}with|J|=L/primeuniformly
for alli∈Ido
Draw/epsilon1[s]
ui,1:T∼N(0,I)
SolveBu,ixui,1:T=/epsilon1ui,1:Tforxui,1:T
end for
Obtainxvj,1:Tby repeating last loop ∀j∈J
Calculate gradient estimates of Lfor minibatch
(I,J)using Eqs. S10, S14, and S15
Obtain update steps d[·]for all variational parameters
using Adam optimizer with parameters in Table S1
Updateµu,1:T←µu,1:T+d[µu,1:T], and analogously
forµv,1:T,ωu,1:T−1, andωv,1:T−1
Updateνu,1:Tandνv,1:Taccording to Eq. S18
end for
Repeat above loop for Ntrmore steps, this time without
minibatch sampling (i.e., setting L/prime=L)
provided in Algorithm 2.
Variational distribution. For now, we focus on the word
embeddings, and we simplify the notation by dropping the
indices for the vocabulary and embedding dimensions. The
variational distribution for a single embedding dimension
of a single word embedding trajectory is
q(u1:T) =N(µu,1:T,(B/latticetop
uBu)−1). (S4)
Here,µu,1:Tis the vector of mean values, and Buis the
Cholesky decomposition of the precision matrix. We re-
strict the latter to be bidiagonal,
Bu=
νu,1ωu,1
νu,2ωu,2
......
νu,T−1ωu,T−1
νT
(S5)
withνu,t>0for allt∈{1,...,T}. The variational pa-
rameters are µu,1:T,νu,1:T, andω1:T−1. The variational
distribution of the context embedding trajectories v1:Thas
the same structure.

--- PAGE 13 ---
Supplementary Material to “Dynamic Word Embeddings”
With the above form of Bu, the variational distribution is a
Gaussian with an arbitrary tridiagonal symmetric precision
matrixB/latticetop
uBu. We chose this variational distribution be-
cause it is the exact posterior of a hidden time-series model
with a Kalman ﬁltering prior and Gaussian noise (Blei &
Lafferty, 2006). Note that our variational distribution is a
generalization of a fully factorized (mean-ﬁeld) distribu-
tion, which is obtained for ωu,t= 0∀t. In the general
case,ωu,t/negationslash= 0, the variational distribution can capture cor-
relations between all time steps, with a dense covariance
matrix (B/latticetop
uBu)−1.
Gradient estimation. The skip-gram smoothing algo-
rithm uses stochastic gradient ascent to ﬁnd the variational
parameters that maximize the ELBO,
L=Eq/bracketleftbig
logp(U1:T,V1:T,n±
1:T)/bracketrightbig
−Eq/bracketleftbig
logq(U1:T,V1:T)/bracketrightbig
.
(S6)
Here, the second term is the entropy, which can be evalu-
ated analytically. We obtain for each component in vocab-
ulary and embedding space,
−Eq[logq(u1:T)] =−/summationdisplay
tlog(νu,t) +const. (S7)
and analogously for −Eq[logq(v1:T)].
The ﬁrst term on the right-hand side of Eq. S6 can-
not be evaluated analytically. We approximate its gra-
dient by sampling from qusing the reparameterization
trick (Kingma & Welling, 2014; Rezende et al., 2014).
A naive calculation would require Ω(T2)computing time
since the derivatives of Lwith respect to νu,tandωu,tfor
eachtdepend on the count matrices n±
t/primeof allt/prime. However,
as we show next, there is a more efﬁcient way to obtain all
gradient estimates in Θ(T)time.
We focus again on a single dimension of a single word em-
bedding trajectory u1:T, and we drop the indices iandk.
We drawSindependent samples u[s]
1:Twiths∈{1,...,S}
fromq(u1:T)by parameterizing
u[s]
1:T=µu,1:T+x[s]
u,1:T (S8)
with
x[s]
u,1:T=B−1
u/epsilon1[s]
u,1:Twhere/epsilon1[s]
u,1:T∼N(0,I).(S9)
We obtainx[s]
u,1:TinΘ(T)time by solving the bidiagonal
linear system Bux[s]
u,1:T=/epsilon1[s]
u,1:T. Samplesv[s]
1:Tfor the
context embedding trajectories are obtained analogously.
Our implementation uses S= 1, i.e., we draw only a sin-
gle sample per training step. Averaging over several sam-
ples is done implicitly since we calculate the update stepsusing the Adam optimizer (Kingma & Ba, 2014), which ef-
fectively averages over several gradient estimates in its ﬁrst
moment estimate.
The derivatives ofLwith respect to µu,1:Tcan be obtained
using Eq. S8 and the chain rule. We ﬁnd
∂L
∂µu,1:T≈1
SS/summationdisplay
s=1/bracketleftBig
Γ[s]
u,1:T−Πu[s]
1:T/bracketrightBig
. (S10)
Here, Π∈RT×Tis the precision matrix of the prior
u1:T∼ N (0,Π−1). It is tridiagonal and therefore the
matrix-multiplication Πu[s]
1:Tcan be carried out efﬁciently.
The non-zero matrix elements of Πare
Π11=σ−2
0+σ−2
1
ΠTT=σ−2
0+σ−2
T−1
Πtt=σ−2
0+σ−2
t−1+σ−2
t∀t∈{2,...,T−1}
Π1,t+1= Πt+1,1=−σ−2
t. (S11)
The term Γ[s]
u,1:Ton the right-hand side of Eq. S10 comes
from the expectation value of the log-likelihood under q. It
is given by
Γ[s]
ui,t=L/summationdisplay
j=1/bracketleftBig/parenleftbig
n+
ij,t+n−
ij,t/parenrightbig
σ/parenleftBig
−u[s]/latticetop
i,tv[s]
j,t/parenrightBig
−n−
ij,t/bracketrightBig
v[s]
j,t
(S12)
where we temporarily restored the indices iandjfor words
and contexts, respectively. In deriving Eq. S12, we used the
relations∂logσ(x)/∂x=σ(−x)andσ(−x) = 1−σ(x).
The derivatives ofLwith respect to νu,tandωu,tare more
intricate. Using the parameterization in Eqs. S8–S9, the
derivatives are functions of ∂B−1
u/∂νtand∂B−1
u/∂ωt, re-
spectively, where B−1
uis a dense (upper triangular) T×T
matrix. An efﬁcient way to obtain these derivatives is to
use the relation
∂B−1
u
∂νt=−B−1
u∂Bu
∂νtB−1
u (S13)
and similarly for ∂B−1
u/∂ωt. Using this relation and
Eqs. S8–S9, we obtain the gradient estimates
∂L
∂νu,t≈−1
SS/summationdisplay
s=1y[s]
u,tx[s]
u,t−1
νu,t, (S14)
∂L
∂ωu,t≈−1
SS/summationdisplay
s=1y[s]
u,tx[s]
u,t+1. (S15)
The second term on the right-hand side of Eq. S14 is the
derivative of the entropy, Eq. S7, and
y[s]
u,1:T= (B/latticetop
u)−1/bracketleftBig
Γ[s]
u,1:T−Πu[s]
1:T/bracketrightBig
. (S16)

--- PAGE 14 ---
Supplementary Material to “Dynamic Word Embeddings”
The valuesy[s]
u,1:Tcan again be obtained in Θ(T)time by
bringingB/latticetop
uto the left-hand side and solving the corre-
sponding bidiagonal linear system of equations.
Sampling in vocabulary space. In the above paragraph,
we described an efﬁcient strategy to obtain gradient esti-
mates in only Θ(T)time. However, the gradient estimation
scales quadratic in the vocabulary size Lbecause allL2el-
ements of the positive count matrices n+
tcontribute to the
gradients. In order speed up the optimization, we pretrain
the model using a minibatch of size L/prime< L in vocabulary
space as explained below. The computational complexity
of a single training step in this setup scales as (L/prime)2rather
thanL2. AfterN/prime
tr= 5000 training steps with minibatch
sizeL/prime, we switch to the full batch size of Land train the
model for another Ntr= 1000 steps.
The subsampling in vocabulary space works as follows. In
each training step, we independently draw a set IofL/prime
random distinct words and a set JofL/primerandom distinct
contexts from a uniform probability over the vocabulary.
We then estimate the gradients of Lwith respect to only
the variational parameters that correspond to words i∈I
and contexts j∈J. This is possible because both the prior
of our dynamic skip-gram model and the variational distri-
bution factorize in the vocabulary space. The likelihood of
the model, however, does not factorize. This affects only
the deﬁnition of Γ[s]
uik,tin Eq. S12. We replace Γ[s]
uik,tby
an estimate Γ[s]/prime
uik,tbased on only the contexts j∈J in the
current minibatch,
Γ[s]
ui,t=L
L/prime/summationdisplay
j∈J/bracketleftBig/parenleftbig
n+
ij,t+n−
ij,t/parenrightbig
σ/parenleftBig
−u[s]/latticetop
i,tv[s]
j,t/parenrightBig
−n−
ij,t/bracketrightBig
v[s]
j,t. (S17)
Here, the prefactor L/L/primerestores the correct ratio between
evidence and prior knowledge (Hoffman et al., 2013).
Enforcing positive deﬁniteness. We update the varia-
tional parameters using stochastic gradient ascent with the
Adam optimizer (Kingma & Ba, 2014). The parame-
tersνu,1:Tare the eigenvalues of the matrix Bu, which
is the Cholesky decomposition of the precision matrix
ofq. Therefore, νu,thas to be positive for all t∈
{1,...,T}. We use mirror ascent (Ben-Tal et al., 2001;
Beck & Teboulle, 2003) to enforce νu,t>0. Speciﬁcally,
we updateνtto a new value ν/prime
tdeﬁned by
ν/prime
u,t=1
2νu,td[νu,t] +/radicalBigg/parenleftbigg1
2νu,td[νu,t]/parenrightbigg2
+ν2
u,t(S18)
whered[νu,t]is the step size obtained from the Adam op-
timizer. Eq. S18 can be derived from the general mirror
ascent update rule Φ/prime(ν/prime
u,t) = Φ/prime(νu,t) +d[νu,t]with themirror map Φ :x/mapsto→−c1log(x)+c2x2/2, where we set the
parameters to c1=νu,tandc2= 1/νu,tfor dimensional
reasons. The update step in Eq. S18 increases (decreases)
νu,tfor positive (negative) d[νu,t], while always keeping its
value positive.
Natural basis. As a ﬁnal remark, let us discuss an op-
tional extension to the skip-gram smoothing algorithm that
converges in less training steps. This extension only in-
creases the efﬁciency of the algorithm. It does not change
the underlying model or the choice of variational distri-
bution. Observe that the prior of the dynamic skip-gram
model connects only neighboring time-steps with each
other. Therefore, the gradient of Lwith respect to µu,t
depends only on the values of µu,t−1andµu,t+1. A naive
implementation of gradient ascent would thus require T−1
update steps until a change of µu,1affects updates of µu,T.
This problem can be avoided with a change of basis from
µu,1:Tto new parameters ρu,1:T,
µu,1:T=Aρu,1:T (S19)
with an appropriately chosen invertible matrix A∈RT×T.
Derivatives ofLwith respect to ρu,1:Tare given by the
chain rule,∂L/∂ρu,1:T= (∂L/∂µu,1:T)A. A natural (but
inefﬁcient) choice for Ais to stack the eigenvectors of the
prior precision matrix Π, see Eq. S11, into a matrix. The
eigenvectors of Πare the Fourier modes of the Kalman ﬁl-
tering prior (with a regularization due to σ0). Therefore,
there is a component ρu,tthat corresponds to the zero-mode
ofΠ, and this component learns an average word embed-
ding over all times. Higher modes correspond to changes
of the embedding vector over time. A single update to the
zero immediately affects all elements of µu,1:T, and there-
fore changes the word embeddings at all time steps. Thus,
information propagates quickly along the time dimension.
The downside of this choice for Ais that the transforma-
tion in Eq. S19 has complexity Ω(T2), which makes update
steps slow.
As a compromise between efﬁciency and a natural ba-
sis, we propose to set Ain Eq. S19 to the Cholesky de-
composition of the prior covariance matrix Π−1≡AA/latticetop.
Thus,Ais still a dense (upper triangular) matrix, and, in
our experiments, updates to the last component ρu,Taf-
fect all components of µu,1:Tin an approximately equal
amount. Since Πis tridiagonal, the inverse of Ais bidiago-
nal, and Eq. S19 can be evaluated in Θ(T)time by solving
Aµu,1:T=ρu,1:Tforµu,1:T. This is the parameterization
we used in our implementation of the skip-gram smoothing
algorithm.

--- PAGE 15 ---
Supplementary Material to “Dynamic Word Embeddings”
References
Beck, Amir and Teboulle, Marc. Mirror Descent and Non-
linear Projected Subgradient Methods for Convex Opti-
mization. Operations Research Letters , 31(3):167–175,
2003.
Ben-Tal, Aharon, Margalit, Tamar, and Nemirovski,
Arkadi. The Ordered Subsets Mirror Descent Optimiza-
tion Method with Applications to Tomography. SIAM
Journal on Optimization , 12(1):79–108, 2001.
Blei, David M and Lafferty, John D. Dynamic Topic Mod-
els. In Proceedings of the 23rd International Conference
on Machine Learning , pp. 113–120. ACM, 2006.
Hoffman, Matthew D, Blei, David M, Wang, Chong, and
Paisley, John William. Stochastic Variational Inference.
Journal of Machine Learning Research , 14(1):1303–
1347, 2013.
Kim, Yoon, Chiu, Yi-I, Hanaki, Kentaro, Hegde, Dar-
shan, and Petrov, Slav. Temporal Analysis of Language
Through Neural Language Models. In Proceedings of
the ACL 2014 Workshop on Language Technologies and
Computational Social Science , pp. 61–65, 2014.
Kingma, Diederik and Ba, Jimmy. Adam: A Method
for Stochastic Optimization. arXiv preprint
arXiv:1412.6980 , 2014.
Kingma, Diederik P and Welling, Max. Auto-Encoding
Variational Bayes. In Proceedings of the 2nd Interna-
tional Conference on Learning Representations (ICLR) ,
2014.
Mikolov, Tomas, Sutskever, Ilya, Chen, Kai, Corrado,
Greg S, and Dean, Jeff. Distributed Representations of
Words and Phrases and their Compositionality. In Ad-
vances in Neural Information Processing Systems 26 , pp.
3111–3119. 2013.
Rauber, Paulo E., Falc ˜ao, Alexandre X., and Telea, Alexan-
dru C. Visualizing Time-Dependent Data Using Dy-
namic t-SNE. In EuroVis 2016 - Short Papers , 2016.
Rezende, Danilo Jimenez, Mohamed, Shakir, and Wierstra,
Daan. Stochastic Backpropagation and Approximate In-
ference in Deep Generative Models. In The 31st Interna-
tional Conference on Machine Learning (ICML) , 2014.
