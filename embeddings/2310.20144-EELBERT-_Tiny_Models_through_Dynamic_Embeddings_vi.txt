# EELBERT: Các mô hình nhỏ gọn thông qua Embeddings động

Gabrielle Cohn, Rishika Agarwal, Deepanshu Gupta, và Siddharth Patwardhan
Apple
Cupertino, CA 95014
{gcohn,rishika_agarwal,dkg,patwardhan.s}@apple.com

## Tóm tắt

Chúng tôi giới thiệu EELBERT, một phương pháp nén các mô hình dựa trên transformer (ví dụ: BERT), với tác động tối thiểu đến độ chính xác của các tác vụ downstream. Điều này được thực hiện bằng cách thay thế lớp embedding đầu vào của mô hình bằng các tính toán embedding động, tức là tính toán ngay lập tức. Vì lớp embedding đầu vào chiếm một phần đáng kể của kích thước mô hình, đặc biệt là đối với các biến thể BERT nhỏ hơn, việc thay thế lớp này bằng một hàm tính toán embedding giúp chúng tôi giảm kích thước mô hình đáng kể. Đánh giá thực nghiệm trên benchmark GLUE cho thấy các biến thể BERT của chúng tôi (EELBERT) chỉ bị suy giảm tối thiểu so với các mô hình BERT truyền thống. Thông qua phương pháp này, chúng tôi có thể phát triển mô hình nhỏ nhất UNO-EELBERT, đạt điểm GLUE trong phạm vi 4% so với BERT-tiny được huấn luyện đầy đủ, trong khi chỉ nhỏ hơn 15 lần (1.2 MB) về kích thước.

## 1 Giới thiệu

Trong vài năm qua, việc xây dựng các hệ thống hiểu ngôn ngữ tự nhiên dựa trên các mô hình ngôn ngữ tiền huấn luyện mạnh mẽ như BERT (Devlin et al., 2019), T5 (Raffel et al., 2020), mT5 (Xue et al., 2021), và RoBERTa (Liu et al., 2019) đã trở thành thông lệ. Các mô hình ngôn ngữ này bao gồm một chuỗi các lớp dựa trên transformer, mỗi lớp biến đổi biểu diễn ở đầu vào thành một biểu diễn mới ở đầu ra. Các transformer như vậy hoạt động như "xương sống" để giải quyết một số tác vụ ngôn ngữ tự nhiên, như phân loại văn bản, gán nhãn chuỗi, và sinh văn bản, và chủ yếu được sử dụng để ánh xạ (hoặc mã hóa) văn bản ngôn ngữ tự nhiên thành không gian vector đa chiều biểu diễn ngữ nghĩa của ngôn ngữ đó.

Các thí nghiệm trong nghiên cứu trước đó (Kaplan et al., 2020) đã chứng minh rằng kích thước của mô hình ngôn ngữ (tức là số lượng tham số) có tác động trực tiếp đến hiệu suất tác vụ, và việc tăng kích thước mô hình ngôn ngữ cải thiện khả năng hiểu ngôn ngữ của nó. Hầu hết các kết quả tiên tiến gần đây trong các tác vụ NLP đều được đạt được với các mô hình rất lớn. Tuy nhiên, cùng với việc các mô hình ngôn ngữ khổng lồ ngày càng phổ biến, đã có một xu hướng song song để tạo ra các mô hình nhỏ hơn nhiều, có thể được triển khai trong các môi trường hạn chế tài nguyên như điện thoại thông minh hoặc đồng hồ.

Một số câu hỏi quan trọng nảy sinh khi xem xét các môi trường như vậy: Làm thế nào để tận dụng sức mạnh của các mô hình ngôn ngữ lớn như vậy trên các thiết bị công suất thấp này? Liệu có thể nhận được lợi ích của các mô hình ngôn ngữ lớn mà không cần yêu cầu về đĩa, bộ nhớ và tính toán khổng lồ? Nhiều nghiên cứu gần đây trong các lĩnh vực cắt tỉa mô hình (Gordon et al., 2020), lượng tử hóa (Zafrir et al., 2019), chưng cất (Jiao et al., 2020; Sanh et al., 2020) và các phương pháp mục tiêu hơn như giả thuyết vé số (Chen et al., 2020) nhằm tạo ra các mô hình nhỏ hơn nhưng hiệu quả. Công việc của chúng tôi áp dụng một phương pháp khác bằng cách thu hồi tài nguyên cần thiết để biểu diễn từ vựng lớn của mô hình.

Cảm hứng cho công việc của chúng tôi đến từ Ravi và Kozareva (2018a), người đã giới thiệu các embedding động, tức là các embedding được tính toán ngay lập tức thông qua các hàm hash. Chúng tôi mở rộng việc sử dụng các embedding động cho các mô hình ngôn ngữ dựa trên transformer. Chúng tôi quan sát thấy rằng 21% số tham số có thể huấn luyện trong BERT-base (Turc et al., 2019) nằm trong lớp tra cứu embedding. Bằng cách thay thế lớp embedding đầu vào này bằng các embedding được tính toán trong thời gian chạy, chúng tôi có thể giảm kích thước mô hình cùng một tỷ lệ phần trăm.

Trong bài báo này, chúng tôi giới thiệu một mô hình "không có embedding" - EELBERT - sử dụng chiến lược tính toán embedding động để đạt được kích thước nhỏ hơn. Chúng tôi thực hiện một loạt thí nghiệm để đánh giá thực nghiệm chất lượng của các mô hình "không có embedding" này cùng với mức độ giảm kích thước tương đối. Mức giảm kích thước lên đến 88% được quan sát trong các thí nghiệm của chúng tôi, với sự suy giảm tối thiểu về chất lượng mô hình, và phương pháp này hoàn toàn bổ sung cho các kỹ thuật nén mô hình khác. Vì EELBERT tính toán embedding trong thời gian chạy, chúng tôi phải chấp nhận độ trễ bổ sung, điều mà chúng tôi đo lường trong các thí nghiệm. Chúng tôi thấy rằng độ trễ của EELBERT tăng tương đối so với BERT khi kích thước mô hình giảm, nhưng có thể được giảm thiểu thông qua các tối ưu hóa kiến trúc và kỹ thuật cẩn thận. Xem xét những lợi ích trong nén mô hình mà EELBERT mang lại, đây không phải là một sự đánh đổi bất hợp lý.

## 2 Nghiên cứu liên quan

Có một lượng lớn nghiên cứu mô tả các chiến lược để tối ưu hóa bộ nhớ và hiệu suất của các mô hình BERT (Ganesh et al., 2021). Trong phần này, chúng tôi nêu bật các nghiên cứu liên quan nhất đến công việc của chúng tôi, tập trung vào việc giảm kích thước của các embedding token được sử dụng để ánh xạ các token đầu vào thành một biểu diễn vector giá trị thực. Chúng tôi cũng xem xét nghiên cứu trước đây về hash embeddings hoặc randomized embeddings được sử dụng trong các ứng dụng ngôn ngữ (ví dụ: Tito Svenstrup et al. (2017)).

Nhiều nghiên cứu trước đây đã được thực hiện để giảm kích thước của các embedding tĩnh tiền huấn luyện như GloVe và Word2Vec. Lebret và Collobert (2014) áp dụng Phân tích Thành phần Chính (PCA) để giảm số chiều của word embedding. Để nén GloVe embeddings, Arora et al. (2018) đề xuất LASPE, tận dụng phân tích ma trận để biểu diễn các embedding gốc như một sự kết hợp của các embedding cơ sở và các biến đổi tuyến tính. Lam (2018) đề xuất một phương pháp gọi là Word2Bits sử dụng lượng tử hóa để nén Word2Vec embeddings. Tương tự, Kim et al. (2020) đề xuất sử dụng các khối mã kích thước biến đổi để biểu diễn mỗi từ, trong đó các mã được học thông qua một mạng feedforward với ràng buộc nhị phân.

Tuy nhiên, các nghiên cứu liên quan nhất đến bài báo này là của Ravi và Kozareva (2018b) và Ravi (2017). Ý tưởng chính trong phương pháp của Ravi và Kozareva (2018b) là sử dụng các mạng projection như một hàm xác định để tạo ra một vector embedding từ một chuỗi văn bản, trong đó hàm generator này thay thế lớp embedding.

Ý tưởng đó đã được mở rộng cho các word-level embeddings bởi Sankar et al. (2021) và Ravi và Kozareva (2021), sử dụng kỹ thuật dựa trên LSH cho hàm projection. Các bài báo này chứng minh hiệu quả của projection embeddings, kết hợp với một lớp chồng của CNN, BiLSTM và CRF, trên một tác vụ phân loại văn bản nhỏ. Trong công việc của chúng tôi, chúng tôi nghiên cứu tiềm năng của các phương pháp projection và hash embedding này để đạt được nén trong các mô hình transformer như BERT.

## 3 Mô hình hóa EELBERT

EELBERT được thiết kế với mục tiêu giảm kích thước (và do đó giảm yêu cầu bộ nhớ) của các lớp embedding đầu vào của BERT và các mô hình dựa trên transformer khác. Trong phần này, chúng tôi trước tiên mô tả các quan sát của chúng tôi về BERT để thông báo cho các lựa chọn kiến trúc trong EELBERT, sau đó trình bày mô hình EELBERT chi tiết.

### 3.1 Các quan sát về BERT

Các mô hình ngôn ngữ giống BERT nhận một chuỗi token làm đầu vào, mã hóa chúng thành một biểu diễn không gian vector ngữ nghĩa. Các token đầu vào được tạo ra bởi một tokenizer, phân đoạn một câu ngôn ngữ tự nhiên thành các đơn vị chuỗi con rời rạc w1, w2, ..., wn. Trong BERT, mỗi token trong từ vựng của mô hình được ánh xạ thành một chỉ số, tương ứng với một hàng trong bảng embedding đầu vào (còn được gọi là lớp embedding đầu vào). Hàng này biểu diễn vector embedding kích thước d của token ewi ∈ Rd, cho một token wi đã cho.

Quá trình tra cứu bảng để ánh xạ các token trong từ vựng thành các biểu diễn vector số sử dụng lớp embedding đầu vào là một thao tác "không thể huấn luyện", và do đó không bị ảnh hưởng bởi các kỹ thuật nén mô hình tiêu chuẩn, thường nhắm vào các tham số có thể huấn luyện của mô hình. Điều này dẫn đến một nút thắt cổ chai nén, vì việc phân tích các mô hình giống BERT cho thấy lớp embedding đầu vào chiếm một phần lớn của các tham số mô hình.

Chúng tôi xem xét ba mô hình BERT có sẵn công khai với kích thước khác nhau, tất cả đều được tiền huấn luyện cho tiếng Anh (Turc et al., 2019) - BERT-base, BERT-mini và BERT-tiny. BERT-base có 12 lớp với kích thước lớp ẩn 768, dẫn đến khoảng 110M tham số có thể huấn luyện. BERT-mini có 4 lớp và kích thước lớp ẩn 256, với khoảng 11M tham số, và BERT-tiny có 2 lớp và kích thước lớp ẩn 128, tổng cộng khoảng 4.4M tham số.

Hình 1 cho thấy tỷ lệ kích thước mô hình được lớp embedding đầu vào chiếm (phần được tô màu xanh của các thanh) so với các lớp encoder (phần không được tô màu của các thanh). Lưu ý rằng trong biến thể BERT nhỏ nhất, BERT-tiny, lớp embedding đầu vào chiếm gần 90% mô hình. Bằng cách áp dụng một phương pháp khác để nén mô hình, tập trung không phải vào việc giảm các tham số có thể huấn luyện mà thay vào đó là loại bỏ lớp embedding đầu vào, có thể mang lại mức giảm kích thước mô hình lên đến 9 lần.

### 3.2 Kiến trúc EELBERT

EELBERT chỉ khác BERT trong quá trình chuyển từ token đầu vào thành embedding đầu vào. Thay vì tra cứu mỗi token đầu vào trong lớp embedding đầu vào như bước đầu tiên, chúng tôi tính toán động một embedding cho token wi bằng cách sử dụng hàm hash n-gram pooling. Đầu ra là một biểu diễn vector kích thước d, ewi ∈ Rd, giống như chúng tôi sẽ nhận được từ lớp embedding trong BERT tiêu chuẩn. Lưu ý rằng EELBERT chỉ tác động đến token embeddings, không phải segment hoặc position embeddings, và tất cả các đề cập đến "embeddings" từ đây về sau đều đề cập đến token embeddings.

Khía cạnh chính của phương pháp này là nó không dựa vào bảng embedding đầu vào được lưu trữ trong bộ nhớ, thay vào đó sử dụng hàm hash để ánh xạ các token đầu vào thành các vector embedding trong thời gian chạy. Kỹ thuật này không nhằm mục đích tạo ra các embedding xấp xỉ BERT embeddings. Không giống như các embedding đầu vào của BERT, các embedding động không cập nhật trong quá trình huấn luyện.

Phương pháp hàm hash n-gram pooling của chúng tôi được hiển thị trong Hình 2, với các thao tác trong hộp đen, và các đường đen đi từ đầu vào đến đầu ra của các thao tác đó. Các giá trị đầu vào và đầu ra được đóng khung màu xanh. Để dễ ký hiệu, chúng tôi gọi các n-gram có độ dài i là i-gram, trong đó i = 1, ..., N, và N là kích thước n-gram tối đa.

Các bước của thuật toán như sau:

1. Khởi tạo các hash seed ngẫu nhiên h ∈ Zd. Có tổng cộng d hash seeds, trong đó d là kích thước của embedding chúng tôi muốn thu được, ví dụ 768 cho BERT-base. Các d hash seeds được tạo ra thông qua một trạng thái ngẫu nhiên cố định, vì vậy chúng tôi chỉ cần lưu một số nguyên duy nhất xác định trạng thái ngẫu nhiên.

2. Hash i-grams để lấy các chữ ký i-gram si. Có ki = l - i + 1 số lượng i-grams, trong đó l là độ dài của token. Sử dụng hàm hash cuốn (Wikipedia contributors, 2023), chúng tôi tính toán các vector chữ ký i-gram, si ∈ Zki.

3. Tính toán ma trận projection cho i-grams. Đối với mỗi i, chúng tôi tính toán ma trận projection Pi sử dụng một tập con của các hash seeds. Vector hash seed h được phân vùng thành N vectors, được đóng khung màu hồng trong sơ đồ. Mỗi phân vùng hi có độ dài di, trong đó ∑(i=1 to N) di = d, với các giá trị i lớn hơn tương ứng với di lớn hơn. Cho vector hash seed hi và vector chữ ký i-gram si, ma trận projection Pi ∈ Zki×di là tích ngoài si × hi. Để đảm bảo rằng các giá trị ma trận được giới hạn trong khoảng [-1,1], chúng tôi thực hiện một chuỗi biến đổi trên Pi:

Pi = Pi % B
Pi = Pi - (Pi > B/2) * B
Pi = Pi / (B/2)

trong đó B là kích thước bucket của chúng tôi (số vô hướng).

4. Tính toán embedding, ei, cho mỗi i-grams. Chúng tôi thu được ei ∈ Rdi bằng cách lấy trung bình Pi qua ki hàng của nó để tạo ra một vector di chiều duy nhất.

5. Nối ei để lấy token embedding e. Chúng tôi nối N vectors {ei}(i=1 to N) để lấy vector embedding cuối cùng của token, e ∈ Rd.

Đối với kích thước embedding cố định d, các siêu tham số có thể điều chỉnh của thuật toán này là: N, B, và lựa chọn hàm hashing. Chúng tôi đã sử dụng N = 3, B = 10^9 + 7 và hàm hash cuốn.

Vì EELBERT thay thế lớp embedding đầu vào bằng các embedding động, kích thước mô hình được xuất giảm đi kích thước của lớp embedding đầu vào: O(d × V) trong đó V là kích thước từ vựng, và d là kích thước embedding.

Chúng tôi đặc biệt đề cập đến kích thước được xuất ở đây, vì trong quá trình tiền huấn luyện, mô hình cũng sử dụng một lớp embedding đầu ra để ánh xạ các vector embedding trở lại thành tokens. Trong tiền huấn luyện BERT điển hình, các trọng số được chia sẻ giữa lớp embedding đầu vào và đầu ra, vì vậy lớp embedding đầu ra không đóng góp vào kích thước mô hình. Tuy nhiên, đối với EELBERT, không có lớp embedding đầu vào để chia sẻ trọng số, vì vậy lớp embedding đầu ra có đóng góp vào kích thước mô hình. Ngay cả khi chúng tôi tính toán trước và lưu trữ các embedding token động như một bảng tra cứu embedding, việc sử dụng các embedding động được chuyển vị như một lớp đầu ra cố định sẽ làm mất mục đích của việc học các biểu diễn được ngữ cảnh hóa. Tóm lại, việc sử dụng các lớp embedding đầu vào và đầu ra liên kết trong EELBERT là không khả thi, vì vậy BERT và EELBERT có cùng kích thước trong quá trình tiền huấn luyện. Khi tiền huấn luyện hoàn thành, lớp embedding đầu ra trong cả hai mô hình đều được loại bỏ, và các mô hình được xuất được sử dụng cho các tác vụ downstream, đây là khi chúng tôi thấy được lợi thế về kích thước của EELBERT.

## 4 Thiết lập thí nghiệm

Trong phần này, chúng tôi đánh giá hiệu quả của EELBERT. Các câu hỏi chính mà chúng tôi quan tâm là: chúng tôi có thể đạt được bao nhiều nén mô hình và tác động của việc nén như vậy đến chất lượng mô hình đối với hiểu ngôn ngữ là gì? Chúng tôi thực hiện các thí nghiệm trên một tập hợp các tác vụ NLP benchmark để trả lời các câu hỏi này một cách thực nghiệm.

Trong mỗi thí nghiệm của chúng tôi, chúng tôi so sánh EELBERT với mô hình BERT tiêu chuẩn tương ứng - tức là một mô hình có cùng cấu hình nhưng với lớp embedding đầu vào có thể huấn luyện tiêu chuẩn thay vì các embedding động của chúng tôi. Mô hình tiêu chuẩn này phục vụ như baseline để so sánh, để quan sát tác động của phương pháp của chúng tôi.

### 4.1 Tiền huấn luyện

Đối với các thí nghiệm của chúng tôi, chúng tôi tiền huấn luyện cả BERT và EELBERT từ đầu trên tập dữ liệu OpenWebText (Radford et al., 2019; Gokaslan và Cohen, 2019), sử dụng pipeline tiền huấn luyện được phát hành bởi Hugging Face Transformers (Wolf et al., 2019). Mỗi mô hình của chúng tôi được tiền huấn luyện trong 900,000 bước với độ dài token tối đa 128 sử dụng tokenizer bert-base-uncased. Chúng tôi tuân theo quy trình tiền huấn luyện được mô tả trong Devlin et al. (2019), với một số khác biệt. Cụ thể, (a) chúng tôi sử dụng OpenWeb Corpus để tiền huấn luyện, trong khi công việc gốc sử dụng tập dữ liệu kết hợp của Wikipedia và BookCorpus, và (b) chúng tôi chỉ sử dụng mục tiêu tiền huấn luyện masked language model, trong khi công việc gốc sử dụng cả mục tiêu masked language model và next sentence prediction.

Đối với BERT, các lớp embedding đầu vào và đầu ra được liên kết và có thể huấn luyện. Vì EELBERT không có lớp embedding đầu vào, lớp embedding đầu ra của nó được tách rời và có thể huấn luyện.

### 4.2 Fine-tuning

Đối với fine-tuning và đánh giá downstream, chúng tôi chọn benchmark GLUE (Wang et al., 2018) để đánh giá chất lượng của các mô hình. GLUE là một tập hợp chín tác vụ hiểu ngôn ngữ, bao gồm các tác vụ câu đơn (phân tích cảm xúc, tính chấp nhận ngôn ngữ), các tác vụ tương tự/paraphrase, và các tác vụ suy luận ngôn ngữ tự nhiên. Sử dụng mỗi mô hình của chúng tôi như một backbone, chúng tôi fine-tune riêng lẻ cho từng tác vụ GLUE dưới một cài đặt tương tự như được mô tả trong Devlin et al. (2019). Các metrics trên các tác vụ này phục vụ như một proxy cho chất lượng của các mô hình embedding. Vì các metrics GLUE được biết là có phương sai cao, chúng tôi chạy mỗi thí nghiệm 5 lần sử dụng 5 seeds khác nhau, và báo cáo median của các metrics trên tất cả các lần chạy, như được thực hiện trong Lan et al. (2020).

Chúng tôi tính toán điểm GLUE tổng thể cho mỗi mô hình. Đối với BERT-base và EELBERT-base, chúng tôi sử dụng phương trình sau:

AVERAGE (CoLA Matthews corr, SST-2 accuracy, MRPC accuracy, STSB Pearson corr, QQP accuracy, AVERAGE (MNLI match accuracy, MNLI mismatch accuracy), QNLI accuracy, RTE accuracy)

Giống như Devlin et al. (2019), chúng tôi không bao gồm tác vụ WNLI trong các tính toán của chúng tôi. Đối với tất cả các biến thể BERT nhỏ hơn, tức là BERT-mini, BERT-tiny, EELBERT-mini, EELBERT-tiny, và UNO-EELBERT, chúng tôi sử dụng:

AVERAGE (SST-2 accuracy, MRPC accuracy, QQP accuracy, AVERAGE (MNLI match accuracy, MNLI mismatch accuracy), QNLI accuracy, RTE accuracy)

Lưu ý rằng chúng tôi loại trừ CoLA và STSB khỏi điểm của các mô hình nhỏ hơn, vì các mô hình (cả baseline và EELBERT) dường như không ổn định trên các tác vụ này. Chúng tôi thấy một sự loại trừ tương tự của các tác vụ này trong Sun et al. (2019).

Cũng lưu ý rằng trong các bảng, chúng tôi viết tắt MNLI match và mismatch accuracy là MNLI (M, MM Acc.), CoLA Matthews correlation là CoLA (M Corr.), và STSB Pearson và Spearman correlation là STSB (P, S Corr.).

## 5 Kết quả

Chúng tôi trình bày kết quả các thí nghiệm đánh giá các khía cạnh khác nhau của mô hình với mục đích triển khai và sử dụng trong sản xuất.

### 5.1 Kích thước mô hình vs. Chất lượng

Thí nghiệm đầu tiên của chúng tôi trực tiếp đánh giá các embedding động của chúng tôi bằng cách so sánh các mô hình EELBERT với các baseline BERT tiêu chuẩn tương ứng trên các tác vụ benchmark GLUE. Chúng tôi bắt đầu bằng cách tiền huấn luyện các mô hình như được mô tả trong Phần 4.1 và fine-tune các mô hình trên các tác vụ GLUE downstream, như được mô tả trong Phần 4.2.

Bảng 1 tóm tắt kết quả của thí nghiệm này. Lưu ý rằng việc thay thế lớp embedding có thể huấn luyện bằng embedding động thực sự có tác động tương đối nhỏ đến điểm GLUE. EELBERT-base đạt được giảm khoảng 21% số lượng tham số trong khi chỉ suy giảm 1.5% trên điểm GLUE.

Như một theo dõi cho điều này, chúng tôi nghiên cứu tác động của embedding động trên các mô hình có kích thước nhỏ hơn đáng kể. Bảng 2 hiển thị kết quả cho BERT-mini và BERT-tiny, có 11 triệu và 4.4 triệu tham số có thể huấn luyện, tương ứng. Các mô hình EELBERT-mini và EELBERT-tiny tương ứng có 3.4 triệu và 0.5 triệu tham số có thể huấn luyện, tương ứng. EELBERT-mini chỉ có 0.7% suy giảm tuyệt đối so với BERT-mini, trong khi nhỏ hơn khoảng 3 lần. Tương tự, EELBERT-tiny gần như ngang bằng với BERT-tiny, với 0.5% suy giảm tuyệt đối, trong khi nhỏ hơn khoảng 9 lần.

Ngoài ra, khi chúng tôi so sánh các mô hình EELBERT-mini và BERT-tiny, có khoảng cùng số lượng tham số có thể huấn luyện, chúng tôi nhận thấy rằng EELBERT-mini có điểm GLUE cao hơn đáng kể so với BERT-tiny. Điều này dẫn chúng tôi đến kết luận rằng trong các điều kiện hạn chế không gian, sẽ tốt hơn khi huấn luyện một mô hình với embedding động và số lượng lớp ẩn lớn hơn thay vì một mô hình nông hơn với lớp embedding có thể huấn luyện và ít lớp ẩn hơn.

### 5.2 Vượt qua giới hạn: UNO-EELBERT

Các kết quả được thảo luận trong phần trước cho thấy rằng các embedding động của chúng tôi có tiện ích nhất đối với các mô hình cực kỳ nhỏ, nơi chúng hoạt động tương đương với BERT tiêu chuẩn trong khi cung cấp nén mạnh mẽ. Theo dõi dòng suy nghĩ này, chúng tôi cố gắng vượt qua ranh giới của nén mô hình. Chúng tôi huấn luyện UNO-EELBERT, một mô hình với cấu hình tương tự như EELBERT-tiny, nhưng với kích thước intermediate giảm xuống 128. Chúng tôi lưu ý rằng mô hình này nhỏ hơn gần 15 lần so với BERT-tiny, với suy giảm điểm GLUE tuyệt đối ít hơn 4%. Nó cũng nhỏ hơn 350 lần so với BERT-base, với suy giảm tuyệt đối ít hơn 20%. Lưu ý rằng đối với các tính toán suy giảm này, tất cả các điểm GLUE đều được tính toán sử dụng phương trình điểm GLUE mô hình nhỏ, loại trừ CoLA và STSB, để các điểm có thể so sánh được. Chúng tôi tin rằng với kích thước mô hình 1.2 MB, UNO-EELBERT có thể là một ứng cử viên mạnh mẽ cho các thiết bị edge có bộ nhớ thấp như IoT, và các ứng dụng quan trọng về bộ nhớ khác.

### 5.3 Tác động của hàm Hash

Kết quả của chúng tôi cho đến nay cho thấy rằng lớp embedding có thể huấn luyện có thể được thay thế bằng một hàm hash xác định với tác động tối thiểu đến chất lượng downstream. Hàm hash mà chúng tôi sử dụng pools các đặc trưng n-gram của một từ để tạo ra embedding của nó, vì vậy các từ có hình thái tương tự, như "running" và "runner", sẽ dẫn đến các embedding tương tự. Trong thí nghiệm này, chúng tôi nghiên cứu liệu lựa chọn hàm hash cụ thể của chúng tôi có đóng vai trò quan trọng trong chất lượng mô hình hay không, hoặc liệu một hàm hash hoàn toàn ngẫu nhiên không bảo tồn thông tin hình thái học nào sẽ mang lại kết quả tương tự.

Để mô phỏng một hàm hash ngẫu nhiên, chúng tôi khởi tạo lớp embedding của BERT với một phân phối chuẩn ngẫu nhiên (sơ đồ khởi tạo mặc định của BERT), và sau đó đóng băng lớp embedding, vì vậy mỗi từ trong từ vựng được ánh xạ thành một embedding ngẫu nhiên. Kết quả được trình bày trong Bảng 3 cho thấy rằng đối với các mô hình lớn hơn như BERT-base, hàm hashing không có ý nghĩa nhiều, vì các mô hình được huấn luyện với hàm hash ngẫu nhiên vs n-gram pooling hoạt động tương tự trên các tác vụ GLUE. Tuy nhiên, đối với mô hình BERT-mini nhỏ hơn, hàm hash n-gram pooling của chúng tôi mang lại điểm số tốt hơn. Các kết quả này cho thấy rằng tầm quan trọng của hàm hash n-gram pooling, so với một hàm hash hoàn toàn ngẫu nhiên, tăng lên khi kích thước mô hình giảm. Đây là một phát hiện hữu ích, vì lợi ích chính của dynamic hashing là phát triển các mô hình nhỏ có thể chạy trên thiết bị.

### 5.4 Hàm Hash như một Initializer

Dựa trên kết quả của thí nghiệm trước, chúng tôi xem xét một vai trò thay thế tiềm năng cho các embedding được tạo ra bởi hàm hash của chúng tôi. Chúng tôi nghiên cứu liệu hàm hash n-gram pooling của chúng tôi có thể là một initializer tốt hơn cho lớp embedding có thể huấn luyện, so với initializer phân phối chuẩn ngẫu nhiên thường được sử dụng. Để trả lời câu hỏi này, chúng tôi thực hiện một thí nghiệm với BERT-base, bằng cách khởi tạo một mô hình với khởi tạo ngẫu nhiên chuẩn mặc định và mô hình khác với các embedding được tạo ra bằng hàm hash n-gram pooling của chúng tôi (cột hash trong Bảng 4). Lưu ý rằng trong thí nghiệm này, các lớp embedding đầu vào và đầu ra được liên kết, và các lớp embedding có thể huấn luyện cho cả hai sơ đồ khởi tạo.

Kết quả của thí nghiệm này được hiển thị trong Bảng 4. Mô hình được khởi tạo hash cho thấy tăng 0.5% tuyệt đối trong điểm GLUE so với mô hình được khởi tạo ngẫu nhiên. Chúng tôi cũng thực hiện so sánh này cho BERT-mini (không được hiển thị trong bảng), và quan sát một kết quả tương tự. Thực tế, đối với BERT-mini, mô hình được khởi tạo hash có tăng tuyệt đối 1.6% trong điểm GLUE tổng thể, cho thấy rằng lợi thế của khởi tạo hash n-gram pooling có thể còn lớn hơn đối với các mô hình nhỏ hơn.

### 5.5 Đánh đổi giữa Bộ nhớ và Độ trễ

Một hậu quả của việc sử dụng embedding động là chúng tôi về cơ bản đang đánh đổi thời gian tính toán cho bộ nhớ. Thời gian tra cứu embedding cho một token là O(1) trong các mô hình BERT. Trong EELBERT, embedding token phụ thuộc vào số lượng n-gram ký tự trong token, cũng như kích thước của các phân vùng hash seed. Do tích ngoài giữa các chữ ký n-gram và các hash seeds được phân vùng, độ phức tạp thời gian tổng thể được chi phối bởi l × d, trong đó l là độ dài của một token, và d là kích thước embedding, dẫn đến độ phức tạp thời gian O(l × d) để tính toán embedding hash động cho một token. Đối với tiếng Anh, số lượng chữ cái trung bình trong một từ tuân theo phân phối hơi Poisson, với trung bình khoảng 4.79 (Norvig, 2012), và kích thước embedding d cho các mô hình BERT thường dao động từ 128 đến 768.

Thời gian suy luận cho BERT-base vs EELBERT-base về thực tế không thay đổi, vì phần lớn thời gian tính toán đi vào các khối encoder đối với các mô hình lớn với nhiều khối encoder. Tuy nhiên, các thí nghiệm của chúng tôi trong Bảng 5 cho thấy rằng EELBERT-tiny có thời gian suy luận khoảng 2.3 lần so với BERT-tiny, vì thời gian tính toán trong các khối encoder giảm đối với các mô hình nhỏ hơn, và tính toán embedding bắt đầu tạo thành một phần đáng kể của độ trễ tổng thể. Các đo lường độ trễ này được thực hiện trên MacBook Pro M1 tiêu chuẩn với RAM 32GB. Chúng tôi thực hiện suy luận trên một tập hợp 10 câu (với độ dài từ trung bình 4.8) cho mỗi mô hình, báo cáo độ trễ trung bình để có được các embedding cho một câu (độ trễ tokenization giống nhau cho tất cả các mô hình, và được loại trừ khỏi các đo lường).

Để cải thiện độ trễ suy luận, chúng tôi đề xuất một số tối ưu hóa kiến trúc và kỹ thuật. Tích ngoài giữa các giá trị hash n-gram O(l) chiều và các hash seeds O(d) chiều, dẫn đến một ma trận có kích thước O(l × d), là nút thắt cổ chai tính toán trong tính toán embedding động. Một mặt nạ thưa thớt với số lượng 1 cố định trong mỗi hàng có thể giảm độ phức tạp của bước này xuống O(l × s), trong đó s là số lượng ones trong mỗi hàng, và s ≪ d. Điều này có nghĩa là mỗi n-gram sẽ chỉ chú ý đến một số hash seeds. Mặt nạ này có thể được học trong quá trình huấn luyện, và được lưu với các tham số mô hình mà không có nhiều overhead bộ nhớ, vì nó sẽ có kích thước O(k × s), k là số lượng n-grams tối đa mong đợi từ một token. Công việc tương lai có thể khám phá tác động của phương pháp này đến chất lượng mô hình. Embedding hash của các token cũng có thể được tính toán song song, vì chúng độc lập với nhau. Ngoài ra, chúng tôi quan sát thấy rằng 1, 2 và 3-grams tuân theo phân phối Zipf-ian. Bằng cách sử dụng một cache nhỏ của các embedding cho các n-grams phổ biến nhất, chúng tôi có thể tăng tốc tính toán với chi phí là tăng nhỏ dấu chân bộ nhớ.

## 6 Kết luận

Trong công việc này, chúng tôi đã khám phá việc áp dụng embedding động cho kiến trúc mô hình BERT, như một thay thế cho lớp embedding đầu vào có thể huấn luyện tiêu chuẩn. Các thí nghiệm của chúng tôi cho thấy rằng việc thay thế lớp embedding đầu vào bằng các embedding được tính toán động là một phương pháp nén mô hình hiệu quả, với suy giảm tối thiểu trên các tác vụ downstream. Embedding động dường như đặc biệt hiệu quả đối với các biến thể BERT nhỏ hơn, nơi lớp embedding đầu vào bao gồm một tỷ lệ phần trăm lớn hơn của các tham số có thể huấn luyện.

Chúng tôi cũng thấy rằng đối với các mô hình BERT nhỏ hơn, một mô hình sâu hơn với embedding động mang lại kết quả tốt hơn so với một mô hình nông hơn có kích thước tương đương với lớp embedding có thể huấn luyện. Vì kỹ thuật embedding động được sử dụng trong EELBERT bổ sung cho các kỹ thuật nén mô hình hiện có, chúng tôi có thể áp dụng nó kết hợp với các phương pháp nén khác để tạo ra các mô hình cực kỳ nhỏ. Đáng chú ý, mô hình nhỏ nhất của chúng tôi, UNO-EELBERT, chỉ có kích thước 1.2 MB, nhưng đạt được điểm GLUE trong phạm vi 4% so với một mô hình tiêu chuẩn được huấn luyện đầy đủ gần 15 lần kích thước của nó.
