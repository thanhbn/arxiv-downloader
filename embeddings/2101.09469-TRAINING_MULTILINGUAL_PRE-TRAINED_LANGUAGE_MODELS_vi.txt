# 2101.09469.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/embeddings/2101.09469.pdf
# Kích thước tệp: 875418 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
HUẤN LUYỆN CÁC MÔ HÌNH NGÔN NGỮ ĐA NGÔN NGỮ ĐÃ ĐƯỢC TIỀN HUẤN LUYỆN
VỚI CÁC TỪ CON CẤP BYTE
BÁO CÁO KỸ THUẬT
Junqiu Wei, Qun Liu, Yinpeng Guo, Xin Jiang
Phòng thí nghiệm Noah's Ark, Huawei Technologies
{weijunqiu, qun.liu, guo.yinpeng, jiang.xin}@huawei.com
Ngày 4 tháng 6 năm 2021
TÓM TẮT
Các mô hình ngôn ngữ đã được tiền huấn luyện đã đạt được những thành công lớn trong nhiều tác vụ hiểu ngôn ngữ tự nhiên (NLU) nhờ khả năng nắm bắt thông tin ngữ cảnh sâu trong văn bản thông qua tiền huấn luyện trên các kho dữ liệu quy mô lớn. Một trong những thành phần cơ bản trong các mô hình ngôn ngữ đã được tiền huấn luyện là từ vựng, đặc biệt là để huấn luyện các mô hình đa ngôn ngữ trên nhiều ngôn ngữ khác nhau. Trong báo cáo kỹ thuật này, chúng tôi trình bày các thực hành của mình trong việc huấn luyện các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện với BBPE: BPE cấp Byte (tức là, Byte Pair Encoding). BBPE đã được áp dụng bởi các mô hình ngôn ngữ đã được tiền huấn luyện như GPT-2/3 [1,2] và Roberta [3] và việc sử dụng nó trong dịch máy đã được thảo luận trong [4]. Chúng tôi so sánh từ vựng cấp byte với từ vựng cấp ký tự được áp dụng trong mô hình BERT đa ngôn ngữ của Google thông qua các nghiên cứu tình huống chuyên sâu về token hóa trong nhiều ngôn ngữ khác nhau. Trong thí nghiệm, chúng tôi áp dụng kiến trúc của NEZHA [5] làm mô hình ngôn ngữ đã được tiền huấn luyện cơ bản và kết quả cho thấy NEZHA được huấn luyện với các từ con cấp byte luôn vượt trội hơn Google multilingual BERT và NEZHA vanilla với khoảng cách đáng kể trong một số tác vụ NLU đa ngôn ngữ. Chúng tôi phát hành mã nguồn của các công cụ xây dựng từ vựng cấp byte và các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện tại các URL12.
Từ khóa: Mô hình Ngôn ngữ Đã được Tiền Huấn luyện, Token hóa, Đa ngôn ngữ, Từ con cấp Byte

1 Giới thiệu
Các mô hình ngôn ngữ đã được tiền huấn luyện đã chứng minh thành công tuyệt vời nhờ hiệu suất xuất sắc trong nhiều tác vụ hiểu ngôn ngữ tự nhiên (NLU). Trong giai đoạn tiền huấn luyện, nó sử dụng các tác vụ mô hình hóa ngôn ngữ và học các biểu diễn từ có ngữ cảnh bằng cách sử dụng lượng lớn văn bản huấn luyện. Một lượng lớn các nỗ lực nghiên cứu đã được dành cho các mô hình ngôn ngữ đã được tiền huấn luyện như ELMo [6], BERT [7], ERNIE-Baidu [8,9], ERNIE-Tsinghua [10], XLNet [11], RoBERTa [3], NEZHA [5], ALBERT [12], ELECTRA [13] và MegatronLM3. Như một kỹ thuật cơ bản trong xử lý ngôn ngữ tự nhiên (NLP), các mô hình ngôn ngữ được tiền huấn luyện trên văn bản có thể dễ dàng được chuyển giao để học các tác vụ NLP hạ nguồn với điều chỉnh tinh, đạt được hiệu suất tốt nhất hiện tại trên nhiều tác vụ bao gồm phân tích tình cảm, đọc hiểu máy, khớp câu, nhận dạng thực thể có tên và suy luận ngôn ngữ tự nhiên.

Mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện hoạt động như một bộ mã hóa đa ngôn ngữ phổ quát nhúng bất kỳ câu/từ nào vào một không gian nhúng được chia sẻ. Nỗ lực đầu tiên về chủ đề này là Google's multilingual BERT [7] nhúng hơn 100 ngôn ngữ và cải thiện đáng kể hiệu suất trên các ngôn ngữ ít tài nguyên. Sau đó, XLM (Cross-Lingual Language Pre-training) [14] tiếp tục kết hợp dữ liệu song song có giám sát (với một mục tiêu mô hình ngôn ngữ đa ngôn ngữ mới) vào các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện. Trong báo cáo kỹ thuật này, chúng tôi trình bày chi tiết thực hành của mình trong việc huấn luyện các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện với Từ con cấp Byte. Một thành phần quan trọng trong các mô hình ngôn ngữ đã được tiền huấn luyện, đặc biệt là các mô hình đa ngôn ngữ, là từ vựng. Chúng tôi quan sát thấy rằng từ vựng cấp ký tự, ví dụ như từ vựng BERT đa ngôn ngữ của Google (mBERT vocab), có hai điểm yếu. Trước hết, nó có quá nhiều từ con hiếm chứa các biểu diễn khó học trong các mô hình học sâu. Như thể hiện trong Hình 1, từ vựng mBERT có khoảng 30% từ con với tần suất nhỏ hơn 100 và khoảng 18% từ con không bao giờ xuất hiện. Chúng tôi tính toán tần suất của các từ con trong bộ dữ liệu Wikipedia với 14 ngôn ngữ như thể hiện trong Phần 4.4.3 và sắp xếp các từ con theo tần suất của chúng. Lưu ý rằng chúng tôi chỉ xem xét các từ con thuộc về 14 ngôn ngữ trong từ vựng mBERT. Chúng tôi quan sát thấy rằng những từ con hiếm này chủ yếu là các ký tự Trung Quốc hiếm và các từ con Latin hiếm. Những từ con này lãng phí các vị trí của từ vựng. Thứ hai, mặc dù từ vựng mBERT có nhiều từ con hiếm, nhưng không thể tránh khỏi vấn đề từ không xác định (tức là, [UNK]). Theo tiêu chuẩn Unicode mới nhất4, có hơn 14 triệu ký tự (hầu hết trong số đó hiếm khi xuất hiện). Và do đó, không thể bao gồm tất cả chúng trong một từ vựng sẽ dẫn đến vấn đề [UNK] trong văn bản chứa những ký tự không được bao gồm này. Ví dụ "莫" là tên của một nhà văn Trung Quốc nổi tiếng, tuy nhiên, ký tự "莫" không có trong từ vựng mBERT.

Được thúc đẩy bởi điều này, chúng tôi sử dụng một kỹ thuật, cụ thể là Từ con cấp Byte thể hiện thành công tuyệt vời trong dịch máy thần kinh [4], trong việc xây dựng từ vựng cho các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện. Cụ thể, kỹ thuật này trước tiên chuyển đổi văn bản thành các mã UTF-8 tương ứng và sau đó áp dụng một thuật toán xây dựng từ vựng cấp byte trên các mã UTF-8. Trong nghiên cứu này, chúng tôi sử dụng Byte Pair Encoding (BPE) như một ví dụ về thuật toán xây dựng từ vựng cấp ký tự cơ bản này và do đó, nó được gọi là "Byte-Level BPE" (BBPE). Có những bằng chứng thực nghiệm mạnh mẽ [4] cho thấy rằng việc xây dựng từ vựng đa ngôn ngữ ở cấp byte sẽ khuyến khích mạnh mẽ việc chia sẻ từ con giữa các ngôn ngữ khác nhau một cách tinh tế, kết quả là các từ con thu được trong từ vựng có tần suất cao hơn nhiều và việc học các biểu diễn của các từ con có thể được cải thiện. Như thể hiện trong Hình 1, từ vựng BBPE của chúng tôi có tất cả các từ con với tần suất hơn 10.000 trên bộ dữ liệu Wikipedia của 14 ngôn ngữ. Bên cạnh đó, chúng tôi hoàn toàn tránh vấn đề từ không xác định (tức là, [UNK]) bằng cách bao gồm tất cả các byte vào từ vựng. Đáng chú ý rằng chỉ có 256 byte tổng cộng và do đó, chi phí bao gồm tất cả chúng trong một từ vựng là không đáng kể. Để trực quan đưa ra ý tưởng về Từ con cấp Byte này, chúng tôi trình bày một ví dụ trong Hình 2. Trong hình này, có bốn dòng tương ứng với văn bản thô, mã hóa UTF-8 của văn bản, mã hóa UTF-8 đã được token hóa của văn bản và văn bản tương ứng của mã hóa UTF-8 đã được token hóa, tương ứng. Mỗi từ chứa nhiều ký tự và chúng tôi chuyển đổi mỗi ký tự thành byte và bảo toàn ranh giới của mỗi từ (tức là, khoảng trắng). Bằng cách áp dụng thuật toán xây dựng từ vựng (ví dụ: BPE) vào mã hóa UTF-8 của văn bản, chúng tôi thu được từ vựng cấp byte. Token hóa sử dụng từ vựng được thể hiện trong dòng thứ ba và dòng cuối cùng thể hiện văn bản tương ứng của mã hóa UTF-8 đã được token hóa. Lưu ý rằng ký hiệu "##" biểu thị từ con là một từ con theo sau (tức là, từ con không ở đầu từ mà nó thuộc về). Trong công việc này, chúng tôi thủ công tạo ra một số kỹ thuật để hiểu rõ hơn về các từ con cấp byte đã học được sẽ được trình bày chi tiết trong các phần sau. Chúng tôi cũng cung cấp một số nhận xét sâu sắc về những kỹ thuật này giúp làm sáng tỏ thực hành huấn luyện mô hình đa ngôn ngữ.

Đóng góp của báo cáo kỹ thuật này có ba mặt. Thứ nhất, chúng tôi trình bày chi tiết việc triển khai Từ con cấp Byte trong huấn luyện từ vựng cùng với một số kỹ thuật thủ công hiệu quả được tích hợp cho các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện. Thứ hai, chúng tôi tiến hành các nghiên cứu tình huống toàn diện về token hóa trên nhiều ngôn ngữ và chứng minh hiệu quả của BBPE, đặc biệt là đối với các ngôn ngữ ít tài nguyên. Thứ ba, chúng tôi tiến hành thí nghiệm dựa trên kiến trúc NEZHA được phát hành trước đây của chúng tôi và kết quả của chúng tôi cho thấy NEZHA với BBPE luôn vượt trội hơn NEZHA vanilla và Google multilingual BERT với khoảng cách đáng kể trong tác vụ suy luận ngôn ngữ tự nhiên (XNLI). Thứ tư, chúng tôi mã nguồn mở các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện và công cụ BBPE.

Phần còn lại của bài báo này được tổ chức như sau. Trong Phần 2, chúng tôi xem xét các công trình liên quan đến công việc của chúng tôi. Phần 3 và Phần 4 trình bày thuật toán xây dựng từ vựng cấp byte được đề xuất và hiệu suất thực nghiệm của nó trong việc huấn luyện các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện. Cuối cùng, Phần 5 kết luận bài báo này.

2 Các công trình liên quan
Phần này xem xét tài liệu liên quan đến công việc của chúng tôi. Phần 2.1, Phần 2.2 và Phần 2.3 trình bày các công trình hiện có về các mô hình ngôn ngữ đã được tiền huấn luyện và các thuật toán xây dựng từ vựng, tương ứng.

2.1 Unicode và Mã hóa UTF-8
Unicode5 là một tiêu chuẩn thống nhất để mã hóa, biểu diễn và xử lý dữ liệu văn bản nhất quán. Thành công của nó trong việc thống nhất biểu diễn văn bản khiến nó được lan rộng và áp dụng trong phần mềm máy tính. UTF-8 (8-bit Unicode Transformation Format)6 là một trong những định dạng quan trọng nhất trong unicode. Và UTF-8 là mã hóa ký tự có độ dài thay đổi và mã hóa các ký tự với 1-4 byte. Đáng chú ý rằng UTF-8 được thiết kế để tương thích ngược với ASCII. Cụ thể, byte đầu tiên của UTF-8 được mã hóa bằng một byte duy nhất với cùng giá trị nhị phân như ASCII. Như vậy, mỗi văn bản ASCII hợp lệ cũng là Unicode được mã hóa UTF-8 hợp lệ. Và byte đầu tiên của UTF-8 xác định độ dài của mã hóa ký tự. Đến năm 2021, UTF-8 là mã hóa được sử dụng phổ biến nhất cho World Wide Web7.

2.2 Các mô hình ngôn ngữ đã được tiền huấn luyện
Các mô hình ngôn ngữ đã được tiền huấn luyện [6,7,8,9,10,11,3,5,12,13] sử dụng một lượng lớn dữ liệu văn bản không giám sát trong giai đoạn tiền huấn luyện và do đó, các biểu diễn từ có ngữ cảnh sâu được thu được. Trong giai đoạn điều chỉnh tinh, các biểu diễn có thể được chuyển giao thành công cho một loạt rộng các tác vụ NLP bao gồm Phân loại Văn bản, Suy luận Ngôn ngữ Tự nhiên, Đọc hiểu Máy, v.v. Các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện [7,14] hoạt động như một bộ mã hóa phổ quát áp dụng cho nhiều ngôn ngữ khác nhau và nhúng chúng vào một không gian vector thống nhất. So với các mô hình đơn ngôn ngữ, các mô hình đa ngôn ngữ tận hưởng việc chia sẻ thông tin ngữ nghĩa giữa các ngôn ngữ khác nhau (đặc biệt là, phần không phụ thuộc vào ngôn ngữ) và do đó, tăng cường đáng kể hiệu suất trên các ngôn ngữ ít tài nguyên.

2.3 Các thuật toán xây dựng từ vựng
Phần này khảo sát các công trình hiện có về xây dựng từ vựng có hai loại, cụ thể là Từ vựng cấp Ký tự và Từ vựng cấp Byte. Từ vựng cấp ký tự xem mỗi ký tự là đơn vị cơ bản nhất trong việc xây dựng từ vựng và xây dựng từ vựng trên biểu diễn văn bản thô của kho dữ liệu trực tiếp. Nhưng từ vựng cấp byte thay vào đó xem mỗi byte là đơn vị cơ bản nhất và xây dựng từ vựng dựa trên biểu diễn byte của kho dữ liệu tương ứng.

2.3.1 Từ vựng cấp Ký tự
Từ vựng Từ. Loại từ vựng này được triển khai trong các mô hình Dịch máy Thần kinh dựa trên từ trong những ngày đầu. Từ vựng từ bao gồm nhiều từ và chỉ đơn giản xem mỗi từ là một token trong quá trình token hóa [15,16,17,18]. Nhưng phương pháp này gặp phải vấn đề từ không xác định hoặc hiếm và cực kỳ ngăn cản việc chia sẻ từ con giữa các từ khác nhau và do đó, trở nên ít phổ biến hơn trong các mô hình học sâu dễ bị tổn thương bởi từ không xác định hoặc hiếm.

Từ vựng Ký tự. Do những hạn chế của từ vựng từ, từ vựng ký tự được triển khai chứa nhiều ký tự và xem mỗi ký tự là một token trong quy trình phân đoạn văn bản [19]. Ưu điểm của loại từ vựng này có hai mặt. Thứ nhất, nó có kích thước rất nhỏ và do đó, có thể dành nhiều bộ nhớ GPU hơn để lưu trữ nhiều mẫu huấn luyện hơn (tức là, kích thước batch có thể được tăng). Thứ hai, nó khuyến khích mạnh mẽ việc chia sẻ thông tin từ con giữa các từ khác nhau. Mặc dù có những ưu điểm, hạn chế của nó cũng rõ ràng. Từ vựng ký tự dẫn đến các chuỗi token dài hơn nhiều sau khi token hóa khiến việc huấn luyện các mô hình khó khăn hơn.

Từ vựng Từ con. Từ vựng từ con bao gồm nhiều từ con và do đó, cân bằng hợp lý giữa từ vựng từ và từ vựng ký tự và trở thành tiêu chuẩn de facto của việc xây dựng từ vựng. Có ba thuật toán xây dựng từ vựng từ con chính, cụ thể là Byte Pair Encoding (BPE) [20,21], Unigram [22] và WordPiece trong việc xây dựng từ vựng từ con. BPE và WordPiece khởi tạo từ vựng như một tập hợp các ký tự và sau đó lặp lại hợp nhất một cặp token trong từ vựng và chèn cặp đã hợp nhất (tức là, một token mới được tạo) vào từ vựng cho đến khi kích thước từ vựng đạt đến một giá trị được định trước. Sự khác biệt của chúng nằm ở phương pháp lựa chọn cặp token trong mỗi lần lặp. BPE lặp lại thay thế cặp ký tự liên tiếp xuất hiện thường xuyên nhất trong một văn bản bằng một ký tự duy nhất không xuất hiện trong văn bản. Bên cạnh đó, nó duy trì một bảng ánh xạ để liên kết mỗi cặp được thay thế với ký tự tương ứng của chúng để sử dụng trong giải mã. Trong huấn luyện từ vựng, mỗi ký tự được xem là đơn vị cơ bản nhất. Xem xét ví dụ như thể hiện trong Bảng 1. Văn bản thô là "ABABABCABC" và cặp ký tự thường xuyên nhất là "AB". Do đó, trong lần lặp đầu tiên, cặp "AB" được thay thế bằng một ký tự mới Z và "Z=AB" được chèn vào bảng ánh xạ (tức là, từ vựng). Tương tự, trong ba lần lặp cuối cùng, "ZC", "ZZ", "YY" được thay thế bằng "Y", "X" và "M", tương ứng. Đáng chú ý rằng khoảng trắng không được phép hợp nhất với bất kỳ ký tự nào khác. Điều này có nghĩa là các ký tự được hợp nhất trong bất kỳ lần lặp nào phải nằm trong cùng một từ và mỗi mục trong từ vựng phải là một chuỗi con của một từ. WordPiece giống như BPE ngoại trừ việc lựa chọn cặp ký tự trong mỗi lần lặp. WordPiece chọn cái nào tối đa hóa khả năng xảy ra của một mô hình ngôn ngữ đã cho sau khi hợp nhất cặp. Nói cách khác, BPE và WordPiece xây dựng từ vựng theo cách từ dưới lên bắt đầu từ từ vựng cấp ký tự và lặp lại làm phong phú từ vựng bằng cách hợp nhất hai token hiện có.

Bảng 1: Một ví dụ về BPE
Văn bản Bảng ánh xạ (Từ vựng)
Văn bản thô ABABABCABC -
Lần lặp 1 ZZZCZC Z=AB
Lần lặp 2 ZZYY Z=AB, Y=ZC
Lần lặp 3 XYY Z=AB, Y=ZC, X=ZZ
Lần lặp 4 XM Z=AB, Y=ZC, X=ZZ, M=YY

Ngược lại, Unigram xây dựng từ vựng theo cách từ trên xuống bắt đầu từ một tập hợp các từ/từ con và làm phong phú từ vựng bằng cách chia tách các token hiện có thay vì hợp nhất chúng. Cụ thể, ban đầu nó duy trì một tập hợp các ứng cử viên của token (thường là tất cả các từ trong kho dữ liệu) và sau đó lặp lại chia tách các ứng cử viên bằng một mô hình xác suất và chèn những cái đã chia tách vào danh sách ứng cử viên cho đến khi danh sách ứng cử viên đạt đến một kích thước nhất định. Một số kỹ thuật cắt tỉa cũng được kết hợp để cắt tỉa các ứng cử viên và thúc đẩy việc xây dựng từ vựng. BPE và Unigram được tích hợp vào thư viện nổi tiếng, cụ thể là thư viện SentencePiece [23]. Nhưng việc triển khai WordPiece được phát triển bởi Google chưa được phát hành do các vấn đề thương mại và không có sẵn. Từ vựng từ con lần đầu tiên được triển khai vào lĩnh vực Dịch máy Thần kinh để huấn luyện từ vựng [21] và sau đó được giới thiệu cho các mô hình ngôn ngữ đã được tiền huấn luyện. BERT [7] sử dụng WordPiece như thuật toán xây dựng từ vựng cơ bản của họ và ERNIE-Baidu, ERNIE-Tsinghua, NEZHA, ELECTRA chỉ đơn giản sử dụng từ vựng của BERT được phát hành bởi Google trong việc huấn luyện mô hình của họ. ALBERT [12] và XLNET [11] sử dụng Unigram và BPE tương ứng bằng cách sử dụng thư viện SentencePiece [23].

2.3.2 Từ vựng cấp Byte
Như thể hiện trong Hình 2, dòng thứ hai thể hiện văn bản cấp byte (trong mã UTF-8) của câu gốc thể hiện trong dòng đầu tiên. Từ vựng cấp Byte được xây dựng và áp dụng trong văn bản cấp byte đã chuyển đổi [1,3,4]. Trong biểu diễn cấp byte, mỗi ký tự trong văn bản gốc được chuyển đổi thành nhiều byte trong văn bản cấp byte và do đó, từ vựng cấp byte cho phép việc chia sẻ các từ khác nhau một cách tinh tế hơn (tức là, ở cấp độ dưới ký tự), kết quả là các từ vựng thu được có ít token hiếm hơn và nhiều token tần suất cao hơn (ví dụ, mỗi ký tự Trung Quốc/Nhật Bản/Hàn Quốc hiếm được token hóa thành nhiều byte tần suất cao). [4] triển khai BPE cấp byte trên các hệ thống dịch máy thần kinh và GPT-2/3 [1,2] và RoBERTa [3] tuyên bố từ vựng của họ được xây dựng bằng cách sử dụng BPE trên văn bản cấp byte. Nhưng họ không cung cấp chi tiết về quy trình xây dựng từ vựng của họ. Một từ vựng bao gồm hoàn toàn 256 byte cũng đã được sử dụng trong nhiều tác vụ ngôn ngữ và giọng nói như gắn thẻ từ loại và nhận dạng thực thể có tên [24], dịch thuật [25], đọc máy [26] và nhận dạng giọng nói [27] nhưng những công trình này không áp dụng BPE trong mã hóa văn bản.

3 Xây dựng từ vựng với các từ con cấp Byte
Trong phần này, chúng tôi trình bày chi tiết thực hành của chúng tôi về việc xây dựng từ vựng với các từ con cấp byte. Phần 3.1 trình bày thuật toán từ vựng cấp byte tổng thể từng bước. Phần 3.2 cung cấp những hiểu biết về các chi tiết kỹ thuật liên quan đến thuật toán của chúng tôi để đưa ra một ý tưởng cấp cao. Phần 3.3 thảo luận từ góc độ lý thuyết về lý do tại sao các từ con cấp byte hoạt động và khi nào kỹ thuật này hoạt động.

3.1 Thuật toán xây dựng từ vựng
Trong phần này, chúng tôi trình bày chi tiết về thuật toán xây dựng từ vựng cấp byte của chúng tôi và cung cấp những hiểu biết về các thủ thuật quan trọng được triển khai giúp làm sáng tỏ thực hành xây dựng các từ con cấp byte. Đáng chú ý rằng trong phần này, chúng tôi sử dụng BPE như công cụ xây dựng từ vựng cơ bản trên văn bản cấp byte nhưng thuật toán của chúng tôi tương thích với bất kỳ công cụ xây dựng từ vựng nào khác (thuật toán cấp ký tự được đề cập trong Phần 2.3.1) như Unigram, WordPiece, v.v. (tức là, BPE được đề cập trong phần này có thể được thay thế bằng bất kỳ thuật toán xây dựng từ vựng cấp ký tự nào khác trong Phần 2.3.1 như WordPiece, v.v.). Vì chúng tôi xem xét văn bản như một chuỗi byte, chúng tôi gọi phương pháp huấn luyện là Byte-Level Byte Pair Encoding (BBPE). Lưu ý rằng chúng tôi giả định rằng thuật toán của chúng tôi lấy văn bản thô làm đầu vào và do đó, việc xây dựng từ vựng được đề cập trong phần này chứa quy trình chuyển đổi văn bản thô thành chuỗi byte và áp dụng BPE vào văn bản cấp byte. Trong phần còn lại của bài báo này, chúng tôi gọi thuật ngữ "từ" là một chuỗi các ký tự/byte được phép hợp nhất trong BPE (tức là, chuỗi không có khoảng trắng bên trong nhưng có khoảng trắng trước và sau chính nó).

Thuật toán 1: Thuật toán xây dựng từ vựng với BPE cấp Byte
Dữ liệu: Dữ liệu văn bản đa ngôn ngữ thô
Kết quả: Một từ vựng chứa các từ con cấp Byte
Bước 1: Tiền xử lý văn bản;
• Bước 1.1: Chèn khoảng trắng trước và sau mỗi ký tự CJK và mỗi dấu câu;
• Bước 1.2: Chuyển đổi mỗi ký tự thành chuỗi byte UTF-8 tương ứng ngoại trừ khoảng trắng và dấu câu. Bước này chuyển đổi văn bản thô thành biểu diễn cấp byte. Nhưng chúng tôi giữ nguyên khoảng trắng và dấu câu cung cấp thông tin ranh giới của mỗi từ và câu, tương ứng;
• Bước 1.3: Gán nhãn "leading" cho byte đầu tiên trong mỗi từ cấp byte đã chuyển đổi biểu thị nó là byte đầu tiên của một từ;
Bước 2: Áp dụng BPE vào văn bản cấp byte;
• Bước này áp dụng BPE như đã đề cập trong Phần 2.3.1 vào văn bản cấp byte được đưa ra bởi Bước 3 và thu được một từ vựng chứa các từ con cấp byte.
Bước 3: Hậu xử lý từ vựng;
• Bước 3.1: Chèn tất cả các byte (tổng cộng 256) vào từ vựng nếu chúng không có trong từ vựng;
• Bước 3.2: Chèn tất cả các byte (tổng cộng 256) mỗi cái được gán nhãn "leading" vào từ vựng nếu chúng không có trong từ vựng;
• Bước 3.3: Chèn "##" trước mỗi từ con theo sau và loại bỏ các nhãn được thêm vào trong Bước 1.3 và Bước 3.2.

Chúng tôi tiến hành trình bày mô tả từng bước của thuật toán bao gồm ba bước sau đây như thể hiện trong Thuật toán 1. Bước 1 tiền xử lý dữ liệu đầu vào thô và chuyển đổi nó thành biểu diễn cấp byte tương ứng. Nó bao gồm ba bước con (1) chèn khoảng trắng trước và sau mỗi ký tự CJK (Trung Quốc, Nhật Bản hoặc Hàn Quốc) và mỗi dấu câu, (2) chuyển đổi mỗi ký tự thành mã byte UTF-8 (ngoại trừ khoảng trắng và dấu câu) (3) gán nhãn cho mỗi byte đầu (như thể hiện trong Bước 1.1, Bước 1.2 và Bước 1.3). Bước 2 áp dụng BPE vào văn bản cấp byte và cuối cùng thu được một từ vựng chứa các từ con cấp byte. Bước 3 hậu xử lý từ vựng để đảm bảo (1) từ vựng có 256 từ con đầu một byte và 256 từ con theo sau một byte (2) mỗi từ con theo sau có ký hiệu '##' ở đầu.

3.2 Chi tiết kỹ thuật để xây dựng từ vựng cấp Byte
Trong phần này, chúng tôi trình bày ba chi tiết kỹ thuật được triển khai trong thuật toán BBPE của chúng tôi và cung cấp những hiểu biết về từng cái như sau.

3.2.1 Đơn vị cơ bản: Byte
Xem mỗi byte như đơn vị cơ bản. Mã hóa UTF-8 biểu diễn mỗi ký tự unicode với 1-4 byte. Theo [4], trong thuật toán của chúng tôi, chúng tôi xem xét mã hóa UTF-8 của văn bản và đơn vị cơ bản được sử dụng là byte. Điều này phân tách mỗi ký tự thành nhiều mảnh (tức là, byte) và do đó, cho phép tất cả các từ chia sẻ các từ con ở cấp độ tinh tế hơn (tức là, lên đến cấp độ dưới ký tự) và cũng có nhiều cơ hội chia sẻ hơn. Bên cạnh đó, nó phân tách các từ/ký tự hiếm (đặc biệt là ký tự) thành nhiều byte với tần suất khá cao để các mô hình ngôn ngữ đã được tiền huấn luyện có thể học các biểu diễn có ngữ cảnh của chúng đủ tốt. Chúng tôi sẽ thấy trong Phần 4.2 thông qua các nghiên cứu tình huống rằng ngay cả khi chúng tôi chia tách mỗi ký tự thành byte, unicode của hầu hết ký tự có thể được hợp nhất như một từ con trong từ vựng BBPE của chúng tôi. Và unicode của mỗi ký tự hiếm được chia thành nhiều từ con trong BBPE. Mặt khác, chúng tôi không chia mỗi byte thành các mảnh chi tiết hơn (tức là, bit) vì cấp byte đủ tinh (lưu ý rằng chỉ có 256 byte khác biệt).

Từ vựng phải chứa tất cả các từ con một byte (trong cả phiên bản đầu và theo sau được đề cập trong Phần 3.2.3). Theo [4], trong trường hợp kho dữ liệu trong giai đoạn tiền huấn luyện hoặc điều chỉnh tinh chứa các từ/ký tự chưa thấy không bao giờ xuất hiện trong kho dữ liệu để huấn luyện từ vựng, chúng tôi đảm bảo rằng từ vựng chứa tất cả các ký tự một byte (trong cả hai phiên bản đã đề cập và do đó, có tổng cộng 512 từ con một byte trong từ vựng của chúng tôi). Kết quả là, chúng tôi luôn có thể phân tách bất kỳ chuỗi ký tự UTF-8 nào thành một chuỗi token chứa trong từ vựng của chúng tôi (tức là, sẽ không có token không xác định nào trong kết quả token hóa với từ vựng của chúng tôi). Thủ thuật này tương ứng với Bước 3.1 và Bước 3.2 đảm bảo từ vựng chứa tất cả các từ con một byte trong cả hai phiên bản và như vậy, vấn đề token không xác định sẽ được tránh.

3.2.2 Đơn vị tối đa: Từ, Ký tự CJK và Dấu câu
Mã hóa UTF-8 của mỗi từ được xem là một đơn vị tối đa trong BBPE. Xem xét một từ trong văn bản gốc có khoảng trắng trước và sau chính nó. Chúng tôi giữ nguyên ranh giới của từ (tức là, khoảng trắng) khi chuyển đổi văn bản thô thành mã UTF-8. Giống như BPE vanilla, chúng tôi không cho phép việc hợp nhất xuyên từ và giả định rằng từ là token lớn nhất được sử dụng, kết quả là nó giảm sự xuất hiện của các token dài thường có tần suất thấp. Các ngôn ngữ như Latin (bao gồm tiếng Anh, Pháp, Ý) và Slavic (bao gồm tiếng Nga, Ba Lan) tự nhiên có ranh giới cho mỗi từ và chúng tôi bảo toàn ranh giới từ (tức là, được biểu thị bằng khoảng trắng) trong tiền xử lý dữ liệu. Thủ thuật này tương ứng với Bước 1.2 bảo toàn ranh giới từ trong văn bản thô. Thủ thuật này cũng được triển khai trong [4] trong dịch máy thần kinh cấp byte.

Mã hóa UTF-8 của mỗi ký tự CJK (Trung Quốc, Nhật Bản, Hàn Quốc) và mỗi dấu câu cũng được xem là một đơn vị tối đa. Khác với các ngôn ngữ Latin, văn bản thô của CJK không có thông tin về ranh giới của các từ và mỗi câu trong CJK chỉ đơn giản là một chuỗi ký tự (không chứa khoảng trắng). Xem xét rằng có sự mơ hồ trong việc phân đoạn (tức là, token hóa) văn bản CJK và một phân đoạn kém trên văn bản có thể dẫn đến mất mát hiệu suất đáng kể trên nhiều tác vụ hạ nguồn, đặc biệt là Nhận dạng Thực thể Có tên (NER), trong các mô hình ngôn ngữ đã được tiền huấn luyện như BERT [7], RoBERTa [3], v.v., đây là một thực hành phổ biến rằng mỗi ký tự CJK được xem là một token. Chúng tôi cũng tuân theo thực hành này và chỉ đơn giản xem mỗi ký tự CJK như một từ trong huấn luyện từ vựng và an toàn để các mô hình ngôn ngữ tiền huấn luyện học thông tin cấp từ/cụm từ (tức là, các phụ thuộc giữa các ký tự) với cơ chế chú ý. Bên cạnh đó, trong văn bản không phải CJK, mỗi số như "233000", "2019" là một từ (tức là, được bao quanh bởi hai khoảng trắng trước và sau nó) nhưng trong văn bản CJK, mỗi số không được phân tách với các ký tự khác. Do đó, một lợi ích khác của thủ thuật này là làm cho mỗi số trong văn bản CJK tách biệt khỏi văn bản khác và được xem là một từ. Thủ thuật này tương ứng với Bước 1.1 đảm bảo rằng các byte bên trong mỗi ký tự CJK (dấu câu) sẽ không được hợp nhất với bất kỳ byte nào bên ngoài ký tự CJK (dấu câu) này.

3.2.3 Phân biệt từ con theo sau với từ con đầu
Xem xét một chuỗi từ con {"whats", "app"}. Văn bản gốc của nó có hai khả năng: "whats app" và "whatsapp" nếu chúng tôi không phân biệt một từ con theo sau và một từ con đầu với cùng cách viết. Do đó, chúng tôi quan sát rằng mặc dù "app" và "##app" có cùng cách viết, thông tin ngữ nghĩa của chúng khác nhau. Bên cạnh đó, một lợi ích khác của thực hành này là làm cho token hóa không mất mát. Điều này có nghĩa là sau khi một câu được token hóa thành nhiều token, chúng tôi có thể khôi phục câu gốc bằng cách sử dụng ký hiệu ("##") biểu thị một từ con theo sau. Được thúc đẩy bởi điều này, chúng tôi phân biệt hai trường hợp trong thuật toán của chúng tôi. Đáng chú ý rằng đây là một kỹ thuật thường được sử dụng trong Dịch máy Thần kinh [21,4] mà chúng tôi cũng triển khai trong báo cáo này. Trong Phần 4.4.2, chúng tôi cũng tiến hành nghiên cứu ablation về BBPE mà không sử dụng kỹ thuật này. Chi tiết kỹ thuật này tương ứng với Bước 1.3 đảm bảo rằng chúng tôi có thể phân biệt một từ con theo sau và một từ con đầu với cùng cách viết. Chúng tôi sẽ thể hiện trong thí nghiệm rằng kỹ thuật này rất hiệu quả trong việc huấn luyện các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện ngụ ý rằng một từ con theo sau và một từ con đầu có ý nghĩa ngữ nghĩa khác nhau.

3.3 Thảo luận về tại sao và khi nào các từ con cấp Byte hoạt động
Đây là một nhận thức chung rằng các mạng thần kinh sâu cho xử lý ngôn ngữ tự nhiên, đặc biệt là các mô hình ngôn ngữ đã được tiền huấn luyện có hàng triệu hoặc thậm chí hàng nghìn tỷ tham số, dễ bị tổn thương bởi các từ hiếm và không xác định. Điều này là do sự khan hiếm của các từ hiếm khiến việc học các biểu diễn của chúng khá khó khăn trong các mạng thần kinh sâu vì chúng hiếm khi được tiếp xúc với mô hình trong quy trình huấn luyện và lãng phí khá nhiều vị trí trong từ vựng. Và các từ không xác định chỉ đơn giản được xem là một token đặc biệt như '[UNK]' mà không phân biệt thêm cách viết. Vấn đề này đặc biệt nghiêm trọng trong các ngôn ngữ giàu ký tự như Thái, Ả Rập, Nhật Bản, v.v. Biểu diễn văn bản ở cấp byte là một giải pháp hiệu quả vì mỗi ký tự gốc được chuyển đổi thành 1-4 byte và do đó, việc chia sẻ cấp dưới ký tự giữa các từ hoặc ký tự gốc khác nhau là có thể. Với các từ con cấp byte, một ký tự gốc hiếm hoặc không xác định có thể được chia thành nhiều byte thường xuyên và tương đương, các vị trí của các từ hiếm trong từ vựng có thể được giải phóng để lưu trữ các ký hiệu được sử dụng thường xuyên hơn và có ý nghĩa hơn. Do đó, vấn đề token hiếm/không xác định được giảm thiểu đáng kể. Và biểu diễn byte là bất khả tri ngôn ngữ và thậm chí cho phép việc chia sẻ giữa các ngôn ngữ mà không có bất kỳ sự chồng chéo nào trên các bộ ký tự của chúng.

Từ thảo luận trên, chúng tôi có thể thu được rằng các từ con cấp byte hoạt động trong các tình huống mà (1) có các ký tự hiếm trong văn bản cấp ký tự (đặc biệt là các ngôn ngữ giàu ký tự như Thái, Ả Rập, Nhật Bản, v.v.) (2) ký tự hiếm có hơn 1 byte trong biểu diễn byte (để nó có thể được phân tách thành nhiều hơn một byte xuất hiện thường xuyên trong kho dữ liệu huấn luyện). Như chúng tôi sẽ trình bày trong thí nghiệm, các từ con cấp byte đạt được thành công tuyệt vời trong các ngôn ngữ mà ký tự tương ứng với nhiều byte và có một số lượng lớn ký tự nhưng có được cải thiện gần như không đáng kể trên các ngôn ngữ Latin (vì các ngôn ngữ Latin có số lượng ký tự rất hạn chế và unicode UTF-8 của hầu hết ký tự trong Latin chỉ có 1 byte) so với các từ con cấp ký tự. Kết quả này xác nhận thảo luận của chúng tôi trong phần này.

Cuối cùng, đáng chú ý rằng trong từ vựng BBPE, có thể một token cấp byte không có văn bản gốc tương ứng (ví dụ: một chuỗi con của unicode UTF-8 của một ký tự Trung Quốc chứa hơn 1 byte). Điều này sẽ không dẫn đến bất kỳ vấn đề nào trong việc hiểu ngôn ngữ tự nhiên như đã nghiên cứu trong bài báo này. Nhưng trong các tác vụ tạo ngôn ngữ tự nhiên, nó có thể dẫn đến từ không khả thi trong quy trình giải mã. Nhưng vấn đề này có thể được giảm thiểu đáng kể bằng nhiều giải pháp hiệu quả như tăng cường dữ liệu giảm xác suất tạo ra các từ không khả thi.

4 Thí nghiệm
Trong phần này, chúng tôi báo cáo kết quả thí nghiệm của các phương pháp của chúng tôi. Phần còn lại của phần này được tổ chức như sau. Phần 4.1 cung cấp thông tin về các thiết lập thí nghiệm của chúng tôi. Phần 4.2 trình bày các nghiên cứu tình huống (tức là, token hóa) trên nhiều ngôn ngữ khác nhau bằng cách sử dụng từ vựng của chúng tôi và so sánh với các từ vựng được sử dụng trong các công trình khác. Phần 4.3 trình bày kết quả thí nghiệm trên các tác vụ XNLI đa ngôn ngữ. Cuối cùng, Phần 4.4 nghiên cứu thêm về ảnh hưởng của một số biến thể của thuật toán xây dựng từ vựng của chúng tôi và cũng trình bày nghiên cứu thực nghiệm về các thí nghiệm với nhiều ngôn ngữ hơn.

4.1 Thiết lập thí nghiệm
Chi tiết tiền huấn luyện: Chúng tôi huấn luyện các mô hình NEZHA trên 10 máy chủ trên Huawei Cloud8, mỗi máy có 8 GPU NVIDIA Tesla V100 với bộ nhớ 32GB. Thuật toán huấn luyện phân tán là Ring-AllReduce9 và được sử dụng với framework có tên Horovod [28]. Chúng tôi huấn luyện mỗi mô hình từ đầu và tiền huấn luyện trong 80k bước. Kích thước batch trên mỗi GPU là 64, và do đó tổng kích thước batch là 64 * 8 * 10 = 5120. Đối với mỗi mô hình được kiểm tra, chúng tôi đặt tốc độ học tối đa là 1e-4 (với 1800 bước warm-up và suy giảm đa thức). Ngoài ra, chúng tôi áp dụng huấn luyện độ chính xác hỗn hợp sử dụng FP16 [29] trong giai đoạn tiền huấn luyện. Chúng tôi sử dụng bộ dữ liệu wikipedia chứa 11 ngôn ngữ bao gồm tiếng Anh, Ả Rập, Tây Ban Nha, Thái, Nga, Đức, Ý, Pháp, Mã Lai, Bồ Đào Nha và Ba Lan và tăng mẫu các ngôn ngữ ít tài nguyên để làm cho bộ dữ liệu cân bằng hơn.

4.2 Nghiên cứu tình huống về kết quả token hóa và phân tích từ vựng
Hình 3-Hình 6 chứng minh token hóa của nhiều ngôn ngữ khác nhau bằng cách sử dụng từ vựng của chúng tôi và từ vựng được sử dụng trong Google multilingual BERT. Từ vựng BBPE của chúng tôi được xây dựng bằng cách sử dụng bộ dữ liệu wikipedia chứa 11 ngôn ngữ bao gồm tiếng Anh, Ả Rập, Tây Ban Nha, Thái, Nga, Đức, Ý, Pháp, Mã Lai, Bồ Đào Nha và Ba Lan và tăng mẫu các ngôn ngữ ít tài nguyên để làm cho bộ dữ liệu cân bằng hơn. Mỗi hình có bốn dòng tương ứng với văn bản thô, mã hóa UTF-8 của văn bản thô được chuyển đổi bằng 6 bước đã đề cập, kết quả token hóa trên mã hóa UTF-8, văn bản tương ứng của mã hóa UTF-8 đã được token hóa và kết quả token hóa trên văn bản thô bằng cách sử dụng từ vựng của Google. Từ Hình 3, Hình 4 và Hình 6, chúng tôi quan sát rằng đối với cùng một câu bằng tiếng Anh, Thái hoặc Ả Rập, đầu ra của tokenizer của chúng tôi có ít token hơn và điều này ngụ ý rằng BBPE được triển khai của chúng tôi tận hưởng việc chia sẻ từ con tốt hơn.

Hình 7 thể hiện so sánh theo ngôn ngữ giữa từ vựng đa ngôn ngữ Google và từ vựng BBPE của chúng tôi. Mỗi thanh trong hình thể hiện sự khác biệt tương đối của các token trong ngôn ngữ tương ứng (tức là, (A - B) / A, trong đó A (B tương ứng) là số lượng token trong ngôn ngữ trong từ vựng BBPE (Google multilingual tương ứng)). Như quan sát từ hình, từ vựng BBPE có nhiều token hơn trong tiếng Tây Ban Nha, Ả Rập, Mã Lai và Thái và đây là lý do tại sao từ vựng BBPE của chúng tôi cho ra ít token hơn trong token hóa tiếng Ả Rập và Thái so với từ vựng đa ngôn ngữ Google. Và chúng tôi cũng quan sát rằng mặc dù từ vựng BBPE có ít token hơn trong tiếng Anh và Nga, nó có kết quả token hóa gần như giống nhau trên hai ngôn ngữ như từ vựng Google. Điều này ngụ ý rằng từ vựng BBPE của chúng tôi loại bỏ các token dư thừa (đặc biệt là các từ/từ con hiếm) trong các ngôn ngữ như tiếng Anh, Nga, Pháp, v.v. và dành các token cho các ngôn ngữ ít tài nguyên như Ả Rập và Thái. Xem xét lại từ Trung Quốc "莫" được đề cập trong Phần 1 là tên của một nhà văn Trung Quốc nổi tiếng, tuy nhiên, ký tự "莫" không có trong từ vựng mBERT và sẽ được xem là "[UNK]" (tức là, token không xác định). Nhưng với từ vựng BBPE của chúng tôi, mã hóa UTF-8 của ký tự (tức là, E8A992) sẽ được token hóa thành "E8", "##A9" và "##92" và vấn đề từ không xác định được tránh một cách thích hợp. Xem xét một ví dụ khác "春晓" là một câu từ một bài thơ Trung Quốc cổ. Kết quả token hóa của mBERT là "春晓", trong đó từ "春" không có trong từ vựng của nó và được xem là "[UNK]". Nhưng kết quả token hóa của BBPE của chúng tôi là "E585B0 E58FB6 E698A5 E891 ##B3 E895 ##A4 EFBC8C E6A182 E58D8E E7A78B E79A ##8E E6B481 E38082" trong đó từ "春" được token hóa thành hai từ con "E895" và "##A4" và vấn đề "[UNK]" được phá vỡ một cách thích hợp một lần nữa. Bên cạnh đó, "春" và "晓" là hai ký tự Trung Quốc hiếm nhưng chúng được bao gồm trong từ vựng Google mBERT chiếm hai mục của từ vựng và lãng phí không gian. Từ vựng BBPE của chúng tôi không chứa hai ký tự hiếm này dành không gian từ vựng cho các từ con thường xuyên hơn và mỗi trong hai ký tự được chia thành hai từ con với tần suất cao hơn trong token hóa.

4.3 Kết quả thí nghiệm
Chúng tôi kiểm tra các phương pháp và một số baseline trên tác vụ Cross-Lingual Natural Language Inference (XNLI) trên bảy ngôn ngữ, cụ thể là tiếng Anh (En), Ả Rập (Ar), Đức (De), Tây Ban Nha (Es), Pháp (Fr), Nga (Ru) và Thái (Th). Chúng tôi huấn luyện hai từ vựng dựa trên BBPE với các kích thước khác nhau 50k và 100k, tương ứng và huấn luyện NEZHA với hai từ vựng cấp byte. Và chúng tôi so sánh với Google's multilingual BERT, vanilla NEZHA (được huấn luyện với cùng từ vựng như Google's multilingual BERT) và NEZHA được huấn luyện với từ vựng dựa trên BPE gốc. Kết quả như thể hiện trong Bảng 2. Như bảng thể hiện, NEZHA (BBPE) với từ vựng kích thước 100k luôn vượt trội hơn BERT (Google), vanilla NEZHA và NEZHA (BPE) trên bảy ngôn ngữ được kiểm tra với khoảng cách đáng kể. Các cải thiện của NEZHA (BBPE) so với BERT (Google) và NEZHA (BPE) đáng kể hơn trên Ar, Ru và Th là các ngôn ngữ ít tài nguyên và kết quả thí nghiệm này chứng minh rằng những ngôn ngữ này tận hưởng việc chia sẻ từ con cấp byte. Và kết quả này cũng nhất quán với các nghiên cứu tình huống về token hóa như thể hiện trong Hình 4.2 (BBPE của chúng tôi cho ra ít token hơn trong kết quả token hóa trên những ngôn ngữ này so với những kết quả bằng cách sử dụng từ vựng đa ngôn ngữ Google). Cải thiện được mang lại bởi BBPE trên các ngôn ngữ Latin như tiếng Anh không rất đáng kể vì hầu hết ký tự Latin là ký tự một byte và không có sự khác biệt giữa cấp ký tự và cấp byte. Bên cạnh đó, NEZHA (BBPE) chỉ chứa 50k từ con trong từ vựng của nó (tức là, ít hơn hoặc bằng một nửa từ vựng của Google và của NEZHA (BPE)) vượt trội hơn BERT (Google) và NEZHA (BPE) và hiệu suất của nó rất gần với vanilla NEZHA.

Bảng 2: Kết quả trên XNLI
Mô hình En Ar De Es Fr Ru Th Avg. Vocab
BERT (google) 82.4 72.0 76.1 78.4 76.9 74.5 67.1 75.3 120k
NEZHA 83.0 75.9 79.1 81.1 79.3 76.3 68.8 77.6 120k
NEZHA (BPE) 81.2 72.7 77.0 78.8 79.0 73.8 67.3 77.0 100k
NEZHA (BBPE) 83.2 73.9 79.8 81.5 80.8 77.1 71.5 78.3 100k
NEZHA (BBPE) 82.6 74.1 78.3 80.5 79.6 76.1 69.6 77.3 50k

4.4 Nghiên cứu Ablation

4.4.1 So sánh giữa BBPE và BUnigram
Trong phần này, chúng tôi so sánh thực nghiệm hai thuật toán phân đoạn từ khác nhau về việc phân đoạn/token hóa văn bản cấp byte. Một là BPE mà chúng tôi áp dụng trong thí nghiệm của mình và cái còn lại là Unigram [22]. Hai thuật toán này thường được sử dụng cho phân đoạn từ và được tích hợp trong một công cụ nổi tiếng, cụ thể là SentencePiece10. Vì cả hai đều xử lý văn bản cấp byte, chúng tôi gọi chúng là BBPE và BUnigram, tương ứng. Đáng chú ý rằng BUnigram tiêu tốn không gian và không thể xử lý tất cả các kho dữ liệu đa ngôn ngữ. Do đó, đối với mỗi ngôn ngữ trong 11 ngôn ngữ chúng tôi áp dụng cho tiền huấn luyện, chúng tôi lấy mẫu một tập con và xây dựng một phiên bản nhỏ của các kho dữ liệu tiền huấn luyện. Kết quả như thể hiện trong Bảng 3. Từ kết quả, chúng tôi có thể quan sát rằng BBPE luôn vượt trội hơn BUnigram trên bảy ngôn ngữ được kiểm tra.

Bảng 3: So sánh giữa BBPE và BUnigram (Dựa trên phiên bản nhỏ của dữ liệu)
Mô hình En Ar De Es Fr Ru Th Vocab
NEZHA (wiki sample, BBPE) 80.3 67.6 75.3 78.1 76.6 72.6 69.5 100k
NEZHA (wiki sample, BUnigram) 78.5 60.6 72.4 76.6 76.2 62.3 53.7 100k

4.4.2 So sánh giữa BBPE và BBPE mà không phân biệt từ con đầu và từ con theo sau có cùng cách viết
Trong phần này, chúng tôi so sánh kết quả thí nghiệm trên BBPE mà không sử dụng kỹ thuật trong Phần 3.2.3 (chúng tôi gọi phiên bản này là BBPE, w/o D tắt) và BBPE gốc của chúng tôi được trình bày trước đó. Chúng tôi xem xét ba triển khai khác nhau của BBPE, w/o D được thể hiện như sau.

•(BBPE, w/o D, WWB (Without Word Boundary)): trong việc xây dựng từ vựng, chúng tôi không sử dụng kỹ thuật trong Phần 3.2.3.

•(BBPE, w/o D, WBL (Word Boundary Label)): trong việc xây dựng từ vựng, chúng tôi không sử dụng kỹ thuật trong Phần 3.2.3 nhưng trong token hóa, chúng tôi thêm một ký tự đặc biệt "##" trước mỗi từ con theo sau. Kết quả là, mỗi từ con theo sau sẽ có ngữ cảnh khác so với từ con đầu có cùng cách viết.

•(BBPE, w/o D, STE (Subword Type Embedding)): trong việc xây dựng từ vựng, chúng tôi không sử dụng kỹ thuật trong Phần 3.2.3 nhưng gán một nhúng loại từ con có thể huấn luyện cho mỗi loại từ con được thêm vào với nhúng từ trong đầu vào của mô hình. Trong công việc này, chúng tôi xem xét 7 loại từ con khác nhau: từ con theo sau, từ con đầu, từ con kết thúc, từ nguyên, "[SEP]", "[CLS]" và khác.

Bảng 4 chứng minh kết quả thí nghiệm. Ba phương pháp trên có hiệu suất kém hơn so với phương pháp BBPE gốc của chúng tôi. Điều này ngụ ý rằng thông tin rõ ràng về các từ con đầu/theo sau trong từ vựng chứa thông tin ngữ nghĩa đáng kể và có tác động đến hiệu suất. Đáng chú ý rằng BBPE, w/o D, WBL dẫn đến sụt giảm hiệu suất đáng kể. Điều này là do ký tự bổ sung "##" được giới thiệu làm cho câu dài hơn và các ký tự được giới thiệu không có thông tin để phân biệt các từ con đầu và theo sau như mong đợi.

Bảng 4: So sánh với BBPE mà không sử dụng kỹ thuật trong Phần 3.2.3
Mô hình En Ar De Es Fr Ru Th Avg. Vocab
NEZHA (BBPE) 83.21 73.95 79.80 81.50 80.78 77.09 71.54 78.3 100k
NEZHA (BBPE, w/o D, WWB) 82.48 72.51 78.96 80.10 80.32 77.27 72.63 77.75 100k
NEZHA (BBPE, w/o D, WBL) 79.56 65.07 77.69 78.04 76.39 72.79 69.90 74.21 100k
NEZHA (BBPE, w/o D, STE) 82.93 72.95 79.13 80.54 80.44 76.83 73.95 77.94 100k

4.4.3 Thí nghiệm với nhiều ngôn ngữ hơn (Trung Quốc, Hàn Quốc và Nhật Bản)
Chúng tôi sử dụng bộ dữ liệu wikipedia của Trung Quốc, Hàn Quốc và Nhật Bản cùng với 11 ngôn ngữ được đề cập trước đó và cũng sử dụng kỹ thuật tăng mẫu để làm cho bộ dữ liệu cân bằng hơn. Vì không có tác vụ XNLI Hàn Quốc và Nhật Bản, chúng tôi chỉ thêm tác vụ XNLI Trung Quốc (Zh) trong đánh giá tác vụ hạ nguồn. Kết quả như thể hiện trong Hình 5. Từ Bảng 2, chúng tôi quan sát rằng các cải thiện của NEZHA (BBPE) so với những cái khác trên De, Th và Zh vẫn đáng kể. Nhưng chúng tôi quan sát rằng NEZHA (BBPE) được huấn luyện trên 14 ngôn ngữ có sụt giảm hiệu suất đáng kể trên Ar, Ru so với NEZHA (BBPE) được huấn luyện trên 11 ngôn ngữ như thể hiện trong Bảng 1. Điều này là do ba ngôn ngữ được thêm vào (Trung Quốc, Hàn Quốc và Nhật Bản) có ít chia sẻ với Ar và Ru nhưng tiêu thụ một số token trong từ vựng và điều này làm cho các token trong Ar và Ru ít hơn.

Bảng 5: Kết quả trên XNLI (14 ngôn ngữ được tiền huấn luyện)
Mô hình En Ar De Es Fr Ru Th Zh Avg. Vocab
BERT (google) 82.4 72.0 76.1 78.4 76.9 74.5 67.1 76.6 75.5 120k
NEZHA 82.78 75.9 79.1 81.1 79.3 76.3 68.8 76.8 77.5 120k
NEZHA (BPE) 80.7 70.4 77.5 78.3 78.6 72.9 67.5 77.2 75.4 100k
NEZHA (BBPE) 82.4 72.1 79.6 80.2 79.3 75.1 70.2 77.8 77.1 100k

Chúng tôi cũng so sánh từ vựng BBPE trên 14 ngôn ngữ với từ vựng đa ngôn ngữ Google. Hình 8 thể hiện so sánh theo ngôn ngữ giữa từ vựng đa ngôn ngữ Google và từ vựng BBPE của chúng tôi. Mỗi thanh trong hình thể hiện sự khác biệt tương đối của các token trong ngôn ngữ tương ứng (tức là, (A - B) / A, trong đó A (B tương ứng) là số lượng token trong ngôn ngữ trong từ vựng BBPE (Google multilingual tương ứng)). Chúng tôi quan sát rằng từ vựng BBPE có nhiều token hơn trong tiếng Tây Ban Nha, Ả Rập, Bồ Đào Nha, Mã Lai và Thái và ít token hơn trong các ngôn ngữ khác so với từ vựng đa ngôn ngữ Google. Điều này ngụ ý rằng từ vựng BBPE của chúng tôi loại bỏ các token dư thừa (đặc biệt là các từ/từ con hiếm) trong các ngôn ngữ như tiếng Anh, Nga, Pháp, v.v. (đặc biệt là Nhật Bản, Hàn Quốc và Trung Quốc chứa nhiều ký tự/từ hiếm) và dành các token cho các ngôn ngữ ít tài nguyên như Ả Rập, Mã Lai và Thái.

5 Kết luận
Trong báo cáo kỹ thuật này, chúng tôi triển khai byte-level byte pair encoding (BBPE) trong việc xây dựng từ vựng cho các mô hình ngôn ngữ đa ngôn ngữ đã được tiền huấn luyện. Chúng tôi trình bày chi tiết một số kỹ thuật quan trọng trong việc phát triển từ vựng cấp byte và cung cấp những hiểu biết về thực hành của nó. Thí nghiệm của chúng tôi cho thấy rằng phương pháp của chúng tôi vượt trội đáng kể so với một số baseline chỉ huấn luyện các mô hình ở cấp văn bản (bao gồm Google's multilingual BERT, vanilla NEZHA và NEZHA với BPE cấp văn bản) và xác minh hiệu quả của các kỹ thuật cấp byte mà chúng tôi triển khai.

Tài liệu tham khảo
[1] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, và Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 2019.
[2] TB Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, et al. Language models are few-shot learners. arxiv 2020. arXiv preprint arXiv:2005.14165, 4.
[3] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, và Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. 2019.
[4] Changhan Wang, Kyunghyun Cho, và Jiatao Gu. Neural machine translation with byte-level subwords. arXiv preprint arXiv:1909.03341, 2019.
[5] Junqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong Huang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin Jiang, Xiao Chen, và Qun Liu. Nezha: Neural contextualized representation for chinese language understanding. arXiv preprint arXiv:1909.00204, 2019.
[6] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, và Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227–2237, 2018.
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, và Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, 2019.
[8] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, và Hua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019.
[9] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, và Haifeng Wang. Ernie 2.0: A continual pre-training framework for language understanding. arXiv preprint arXiv:1907.12412, 2019.
[10] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, và Qun Liu. Ernie: Enhanced language representation with informative entities. arXiv preprint arXiv:1905.07129, 2019.
[11] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, và Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.
[12] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, và Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.
[13] Kevin Clark, Minh-Thang Luong, Quoc V Le, và Christopher D Manning. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations, 2019.
[14] Alexis Conneau và Guillaume Lample. Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems, pages 7057–7067, 2019.
[15] Nal Kalchbrenner và Phil Blunsom. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, 2013.
[16] Dzmitry Bahdanau, Kyunghyun Cho, và Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, 2015.
[17] Minh-Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, và Wojciech Zaremba. Addressing the rare word problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 11–19, 2015.
[18] Sebastien Jean, Kyunghyun Cho, Roland Memisevic, và Yoshua Bengio. On using very large target vocabulary for neural machine translation. In 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015, pages 1–10. Association for Computational Linguistics (ACL), 2015.
[19] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, và Llion Jones. Character-level language modeling with deeper self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3159–3166, 2019.
[20] Philip Gage. A new algorithm for data compression. In The C Users Journal, 1994.
[21] Rico Sennrich, Barry Haddow, và Alexandra Birch. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725, 2016.
[22] Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword candidates. arXiv preprint arXiv:1804.10959, 2018.
[23] Taku Kudo và John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics.
[24] Dan Gillick, Cliff Brunk, Oriol Vinyals, và Amarnag Subramanya. Multilingual language processing from bytes. In Proceedings of NAACL-HLT, pages 1296–1306, 2016.
[25] Marta Ruiz Costa-Jussà, Carlos Escolano Peinado, và José Adrián Rodríguez Fonollosa. Byte-based neural machine translation. In Proceedings of the First Workshop on Subword and Character Level Models in NLP, pages 154–158. Association for Computational Linguistics, 2017.
[26] Tom Kenter, Llion Jones, và Daniel Hewlett. Byte-level machine reading across morphologically varied languages. In AAAI, 2018.
[27] Bo Li, Yu Zhang, Tara Sainath, Yonghui Wu, và William Chan. Bytes are all you need: End-to-end multilingual speech recognition and synthesis with bytes. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5621–5625. IEEE, 2019.
[28] Alexander Sergeev và Mike Del Balso. Horovod: fast and easy distributed deep learning in TensorFlow. arXiv preprint arXiv:1802.05799, 2018.
[29] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017.
