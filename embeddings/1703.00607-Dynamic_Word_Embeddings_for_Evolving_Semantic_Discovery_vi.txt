# Embedding Từ Động để Khám Phá Ngữ Nghĩa Tiến Hóa

Zijun Yao
Đại học Rutgers
zijun.yao@rutgers.edu

Yifan Sun
Technicolor Research
yifan.sun@technicolor.com

Weicong Ding
Amazon
20008005dwc@gmail.com

Nikhil Rao
Amazon
nikhilrao86@gmail.com

Hui Xiong
Đại học Rutgers
hxiong@rutgers.edu

TÓM TẮT
Tiến hóa từ ngữ đề cập đến việc thay đổi ý nghĩa và liên kết của các từ theo thời gian, như một sản phẩm phụ của quá trình tiến hóa ngôn ngữ con người. Bằng cách nghiên cứu tiến hóa từ ngữ, chúng ta có thể suy ra các xu hướng xã hội và cấu trúc ngôn ngữ qua các giai đoạn khác nhau của lịch sử nhân loại. Tuy nhiên, các kỹ thuật truyền thống như học biểu diễn từ không nắm bắt đầy đủ cấu trúc ngôn ngữ và từ vựng đang phát triển. Trong bài báo này, chúng tôi phát triển một mô hình thống kê động để học biểu diễn vector từ nhận biết thời gian. Chúng tôi đề xuất một mô hình đồng thời học embedding nhận biết thời gian và giải quyết "bài toán căn chỉnh" kết quả. Mô hình này được huấn luyện trên tập dữ liệu NYTimes thu thập được. Ngoài ra, chúng tôi phát triển nhiều chiến lược đánh giá trực quan cho embedding từ theo thời gian. Các thử nghiệm định tính và định lượng của chúng tôi chỉ ra rằng phương pháp của chúng tôi không chỉ nắm bắt đáng tin cậy sự tiến hóa này theo thời gian, mà còn luôn vượt trội so với các phương pháp embedding thời gian tiên tiến về cả độ chính xác ngữ nghĩa và chất lượng căn chỉnh.

Định dạng tham chiếu ACM:
Zijun Yao, Yifan Sun, Weicong Ding, Nikhil Rao, và Hui Xiong. 2018. Dynamic Word Embeddings for Evolving Semantic Discovery. Trong WSDM 2018: The Eleventh ACM International Conference on Web Search and Data Mining, February 5–9, 2018, Marina Del Rey, CA, USA. ACM, New York, NY, USA, 9 trang. https://doi.org/10.1145/3159652.3159703

1 GIỚI THIỆU
Ngôn ngữ con người là một cấu trúc tiến hóa, với các liên kết ngữ nghĩa của từ thay đổi theo thời gian. Ví dụ, apple (táo) truyền thống chỉ được liên kết với trái cây, giờ đây cũng được liên kết với một công ty công nghệ. Tương tự, sự liên kết của tên các nhân vật nổi tiếng (ví dụ, trump) thay đổi theo sự thay đổi vai trò của họ. Vì lý do này, việc hiểu và theo dõi tiến hóa từ ngữ hữu ích cho các nhiệm vụ trích xuất kiến thức nhận biết thời gian (ví dụ, phân tích tình cảm công chúng), và các ứng dụng khác trong khai thác văn bản. Vì vậy, chúng tôi nhằm học embedding từ có khuynh hướng thời gian, để nắm bắt ý nghĩa nhận biết thời gian của từ.

Embedding từ nhằm biểu diễn từ bằng các vector chiều thấp, trong đó các từ có ngữ nghĩa tương tự gần nhau về mặt hình học (ví dụ, red và blue gần nhau hơn red và squirrel). Các kỹ thuật embedding từ cổ điển bắt đầu từ những năm 90 và dựa trên các phương pháp thống kê [9,20]. Sau đó, các phương pháp mạng nơ-ron [5], cũng như những tiến bộ gần đây như word2vec [24,25] và GloVE [27] đã cải thiện đáng kể hiệu suất của việc học biểu diễn từ. Tuy nhiên, các kỹ thuật này thường không xem xét yếu tố thời gian, và giả định rằng từ tĩnh theo thời gian.

Trong bài báo này, chúng tôi quan tâm đến việc tính toán embedding từ nhận biết thời gian. Cụ thể, mỗi từ trong một khung thời gian khác nhau (ví dụ, năm) được biểu diễn bởi một vector khác nhau. Từ những embedding này, chúng ta có một khái niệm tốt hơn về "khoảng cách" (khoảng cách cosine giữa các vector embedding từ), và bằng cách nhìn vào "vùng lân cận" của từ (được định nghĩa thông qua khoảng cách này), chúng ta có thể hiểu rõ hơn về liên kết từ cũng như ý nghĩa từ, khi chúng phát triển theo thời gian. Ví dụ, bằng cách xác định vị trí embedding của tên nhân vật như trump và các từ gần nhất, chúng ta có thể thấy rằng anh ta có thể được liên kết với quỹ đạo: bất động sản → truyền hình → đảng cộng hòa. Tương tự, quỹ đạo của apple di chuyển từ vùng lân cận của strawberry, mango đến vùng lân cận của iphone, ipad.

Một vấn đề thực tế quan trọng trong việc học các embedding từ khác nhau cho các giai đoạn thời gian khác nhau là căn chỉnh. Cụ thể, hầu hết các hàm chi phí để huấn luyện không biến với phép quay, như một sản phẩm phụ, các embedding học được qua thời gian có thể không được đặt trong cùng một không gian tiềm ẩn. Chúng tôi gọi đây là bài toán căn chỉnh, đây là một vấn đề nói chung nếu embedding được học độc lập cho mỗi lát thời gian.

Khác với các phương pháp truyền thống, tài liệu về học embedding từ thời gian tương đối ngắn: [12,15,19,40]. Nói chung, các phương pháp trong những công trình này tuân theo một mô hình hai bước tương tự: đầu tiên tính toán embedding từ tĩnh trong mỗi lát thời gian riêng biệt, sau đó tìm cách căn chỉnh embedding từ qua các lát thời gian. Để đạt được căn chỉnh, [15] tìm một phép biến đổi tuyến tính của từ giữa hai lát thời gian bất kỳ bằng cách giải quyết một bài toán bình phương nhỏ nhất d chiều của k từ lân cận gần nhất (trong đó d là chiều embedding). Ngoài ra, [40] cũng sử dụng phương pháp biến đổi tuyến tính giữa lát thời gian cơ sở và mục tiêu, và tính toán phép biến đổi tuyến tính sử dụng từ neo, không thay đổi ý nghĩa giữa hai lát thời gian. Phương pháp này yêu cầu kiến thức tiên nghiệm về các từ thực sự tĩnh, đòi hỏi giám sát chuyên gia bổ sung. Cuối cùng, [12] áp đặt phép biến đổi trực giao, và giải quyết một bài toán Procrustes d chiều giữa mọi hai lát thời gian liền kề.

Điểm mới chính của chúng tôi trong bài báo này là học embedding từ qua thời gian một cách kết hợp, do đó loại bỏ nhu cầu giải quyết bài toán căn chỉnh riêng biệt. Cụ thể, chúng tôi đề xuất học embedding thời gian trong tất cả các lát thời gian đồng thời, và áp dụng các điều kiện chính quy hóa để làm mịn thay đổi embedding qua thời gian. Có ba lợi thế chính của phương pháp mô hình hóa kết hợp được đề xuất này:

• Thứ nhất, điều này có thể được xem như một cải tiến so với các phương pháp truyền thống "một thời gian" như word2vec.
• Thứ hai, kết quả thực nghiệm của chúng tôi cho thấy việc thực thi căn chỉnh thông qua chính quy hóa mang lại kết quả tốt hơn so với phương pháp hai bước.
• Thứ ba, chúng ta có thể chia sẻ thông tin qua các lát thời gian cho phần lớn từ vựng. Kết quả là, phương pháp của chúng tôi mạnh mẽ chống lại sự thưa thớt dữ liệu - chúng ta có thể có các lát thời gian trong đó một số từ hiếm khi xuất hiện, hoặc thậm chí thiếu. Đây là một lợi thế thực tế quan trọng mà phương pháp của chúng tôi mang lại.

Vì mô hình của chúng tôi yêu cầu embedding qua tất cả các lát thời gian được học cùng một lúc, nó có thể thách thức về mặt tính toán. Để giảm thiểu điều này, chúng tôi sử dụng phương pháp giảm tọa độ khối có thể xử lý kích thước từ vựng lớn thông qua phân tách.

Trong nghiên cứu thực nghiệm, chúng tôi học embedding thời gian của từ từ các bài báo The New York Times từ 1990 đến 2016. Ngược lại, các công trình embedding từ thời gian trước đó đã tập trung vào tiểu thuyết có dấu thời gian và bộ sưu tập tạp chí (như Google N-Gram và COHA). Tuy nhiên, kho ngữ liệu tin tức có lợi thế tự nhiên trong việc nghiên cứu tiến hóa ngôn ngữ qua lăng kính của các sự kiện hiện tại. Ngoài ra, nó cho phép chúng tôi làm việc với kho ngữ liệu duy trì tính nhất quán trong phong cách kể chuyện và ngữ pháp, trái ngược với các bài đăng Facebook và Twitter. Để đánh giá embedding của chúng tôi, chúng tôi phát triển cả thước đo định tính và định lượng.

Về định tính, chúng tôi minh họa các lợi thế của embedding thời gian cho khám phá ngữ nghĩa tiến hóa bằng cách 1) vẽ quỹ đạo vector từ để tìm ý nghĩa và liên kết đang phát triển, 2) sử dụng căn chỉnh qua thời gian để xác định từ liên quan qua thời gian, và 3) xem xét norm như một đại diện cho độ phổ biến khái niệm.

Về định lượng, trước tiên chúng tôi sử dụng các chủ đề ngữ nghĩa được trích xuất từ Phần (ví dụ, Technology, World News) của các bài báo tin tức làm chuẩn gốc để đánh giá độ chính xác ngữ nghĩa của embedding thời gian. Ngoài ra, chúng tôi cung cấp hai bộ thử nghiệm để đánh giá chất lượng căn chỉnh qua thời gian: một bao gồm các vai trò thay đổi đã biết (ví dụ, tổng thống Hoa Kỳ), được xác định khách quan, và một về thay thế khái niệm (ví dụ, đĩa compact thành mp3), được xác định chủ quan hơn. Kết quả thực nghiệm cho thấy hiệu quả của mô hình đề xuất và chứng minh cải thiện đáng kể so với các phương pháp cơ sở.

Phần còn lại của bài báo này được tổ chức như sau. Trong Phần 2, chúng tôi trình bày mô hình đề xuất để học embedding thời gian, và trong Phần 3, chúng tôi mô tả một thuật toán có thể mở rộng để huấn luyện nó. Trong Phần 4, chúng tôi mô tả tập dữ liệu kho ngữ liệu tin tức và chi tiết thiết lập thí nghiệm. Chúng tôi thực hiện đánh giá định tính trong Phần 5. Cuối cùng, chúng tôi so sánh định lượng embedding của chúng tôi với các embedding thời gian tiên tiến khác trong Phần 6.

2 PHƯƠNG PHÁP LUẬN
Bây giờ chúng tôi thiết lập mô hình embedding từ thời gian. Chúng tôi xem xét một kho ngữ liệu văn bản được thu thập qua thời gian. Những loại kho ngữ liệu như bộ sưu tập bài báo tin tức với ngày xuất bản hoặc thảo luận truyền thông xã hội với dấu thời gian rất phổ biến. Chính thức, chúng tôi ký hiệu bằng D=(D₁, ..., Dₜ) kho ngữ liệu văn bản của chúng tôi trong đó mỗi Dₜ, t=1, ..., T, là kho ngữ liệu của tất cả tài liệu trong lát thời gian thứ t. Không mất tổng quát, chúng tôi giả định các lát thời gian được sắp xếp theo thứ tự thời gian. Độ dài của các lát thời gian này có thể theo thứ tự tháng, năm, hoặc thập kỷ. Hơn nữa, độ dài của tất cả các lát thời gian có thể khác nhau. Chúng tôi xem xét từ vựng tổng thể V={w₁, ..., wᵥ} có kích thước V. Chúng tôi lưu ý rằng từ vựng V bao gồm các từ có mặt trong kho ngữ liệu tại bất kỳ thời điểm nào, và do đó có thể có một số w∈V hoàn toàn không xuất hiện trong một số Dₜ. Điều này bao gồm từ nổi lên và từ chết điển hình trong kho ngữ liệu tin tức thế giới thực.

Với kho ngữ liệu được gắn thẻ thời gian như vậy, mục tiêu của chúng tôi là tìm biểu diễn vector dày đặc, chiều thấp uᵥ(t)∈ℝᵈ, d≪V cho mỗi từ w∈V và mỗi giai đoạn thời gian t=1, ..., T. Chúng tôi ký hiệu bằng uᵥ embedding tĩnh cho từ w (ví dụ, học qua word2vec), và d là chiều embedding (thường 50≤d≤200). Gọn gàng, chúng tôi ký hiệu bằng U(t) (có kích thước V×d) ma trận embedding của tất cả từ có hàng thứ i tương ứng với vector embedding của từ thứ i uᵥᵢ(t).

2.1 Embedding từ không nhận biết thời gian
Một quan sát cơ bản trong tài liệu embedding từ tĩnh là các từ có ngữ nghĩa tương tự thường có từ lân cận tương tự trong kho ngữ liệu [10]. Đây là ý tưởng đằng sau việc học biểu diễn từ dày đặc chiều thấp cả truyền thống [5,9,20] và gần đây [24,27]. Trong một số phương pháp này, cấu trúc lân cận được nắm bắt bởi tần suất mà các cặp từ đồng xuất hiện trong một cửa sổ địa phương nhỏ.

Chúng tôi tính toán ma trận thông tin lẫn nhau từng điểm (PMI) V×V cụ thể cho kho ngữ liệu D, có mục thứ w,c là:

PMI(D,L)ᵥ,c = log(#(w,c)·|D|)/(#(w)·#(c))  (1)

trong đó #(w,c) đếm số lần từ w và c đồng xuất hiện trong cửa sổ kích thước L trong kho ngữ liệu D, và #(w), #(c) đếm số lần xuất hiện của từ w và c trong D. |D| là tổng số token từ trong kho ngữ liệu. L thường khoảng 5 đến 10; chúng tôi đặt L=5 trong suốt bài báo này.

Ý tưởng chính đằng sau cả word2vec [24] và GloVE [27] là tìm vector embedding uᵥ và uc sao cho với bất kỳ tổ hợp w,c nào,

uᵥᵀuc ≈ PMI(D,L)ᵥ,c  (2)

trong đó mỗi uᵥ có độ dài d≪V. Trong khi cả [24] và [27] đều cung cấp các thuật toán có thể mở rộng cao như lấy mẫu âm để làm điều này một cách ngầm, [17] cho thấy những điều này tương đương với nhân tử hóa thứ hạng thấp của PMI(D,L). Phương pháp của chúng tôi chủ yếu được thúc đẩy bởi quan sát này. Chúng tôi lưu ý rằng mặc dù các ma trận PMI có kích thước V×V, trong các tập dữ liệu thế giới thực nó thường thưa thớt như quan sát trong [27], mà các phương pháp nhân tử hóa hiệu quả tồn tại [39].

2.2 Embedding từ thời gian
Một phần mở rộng tự nhiên của trực giác embedding từ tĩnh là sử dụng kỹ thuật nhân tử hóa ma trận này trên mỗi lát thời gian Dₜ riêng biệt. Cụ thể, cho mỗi lát thời gian t, chúng tôi định nghĩa mục w,c của ma trận PMI dương (PPMI(t,L)) như:

PPMI(t,L)ᵥ,c = max{PMI(Dₜ,L)ᵥ,c, 0} := Y(t)  (3)

Embedding từ thời gian U(t) phải thỏa mãn

U(t)U(t)ᵀ ≈ PPMI(t,L)  (4)

Một cách để tìm U(t) như vậy là với mỗi t, nhân tử hóa PPMI(t,L) bằng cách sử dụng phương pháp giá trị riêng hoặc giải quyết bài toán nhân tử hóa ma trận lặp.

Bài toán căn chỉnh: Áp đặt (4) không đủ cho một embedding duy nhất, vì các nghiệm không biến dưới phép quay; tức là, với bất kỳ ma trận trực giao d×d R nào, chúng ta có embedding Û(t) = U(t)R, lỗi xấp xỉ trong (4) giống nhau vì Û(t)Û(t)ᵀ = U(t)RRᵀU(t)ᵀ = U(t)U(t)ᵀ.

Vì lý do này, quan trọng là thực thi căn chỉnh; nếu từ w không thay đổi ngữ nghĩa từ t đến t+1, thì chúng ta thêm yêu cầu uᵥ(t) ≈ uᵥ(t+1).

Để làm điều này, [12, 15] đề xuất quy trình hai bước; đầu tiên, họ nhân tử hóa mỗi Y(t) riêng biệt, và sau đó thực thi căn chỉnh bằng cách sử dụng ánh xạ tuyến tính địa phương [15] hoặc giải quyết bài toán procrustes trực giao [12]. Lưu ý rằng trong các phương pháp này, việc căn chỉnh U(t) với U(t′) giả định rằng chúng ta mong muốn U(t) ≈ U(t′). Nếu chúng ta chỉ chọn t′=t+1 (như làm trong [12]), giả định này hợp lý vì giữa hai năm bất kỳ, chỉ một số từ trải qua thay đổi ngữ nghĩa, xuất hiện, hoặc chết. Tuy nhiên, điều này trở nên có vấn đề nếu U(t) là kết quả của giai đoạn thời gian với dữ liệu cực kỳ thưa thớt (và do đó học kém); tất cả embedding năm tiếp theo và embedding năm trước sẽ được căn chỉnh kém.

2.3 Mô hình của chúng tôi
Chúng tôi đề xuất tìm embedding từ thời gian như nghiệm của bài toán tối ưu hóa kết hợp sau:

min(U(1), ..., U(T)) [1/2 ∑ᵗ₌₁ᵀ ||Y(t) - U(t)U(t)ᵀ||²F + λ/2 ∑ᵗ₌₁ᵀ ||U(t)||²F + τ/2 ∑ᵗ₌₂ᵀ ||U(t-1) - U(t)||²F]  (5)

trong đó Y(t) = PPMI(t,L) và λ,τ > 0. Ở đây điều kiện phạt ||U(t)||²F thực thi trung thành dữ liệu thứ hạng thấp như được áp dụng rộng rãi trong tài liệu trước đây. Điều kiện làm mịn chính ||U(t-1) - U(t)||²F khuyến khích embedding từ được căn chỉnh. Tham số τ kiểm soát tốc độ chúng ta cho phép embedding thay đổi; τ=0 không thực thi căn chỉnh nào, và chọn τ→∞ hội tụ đến embedding tĩnh với U(1) = U(2) = ... = U(T). Lưu ý rằng các phương pháp của [12,15] có thể được xem như các nghiệm không tối ưu của (5), ở chỗ chúng tối ưu hóa cho mỗi điều kiện riêng biệt. Một mặt, trong khi các chiến lược trong [15] và [12] thực thi căn chỉnh theo cặp, chúng ta thực thi căn chỉnh qua tất cả các lát thời gian; tức là, nghiệm căn chỉnh cuối cùng U(t) bị ảnh hưởng không chỉ bởi U(t-1) và U(t+1), mà bởi mọi embedding khác. Điều này tránh sự truyền lỗi căn chỉnh gây ra bởi việc lấy mẫu con của khung thời gian cụ thể. Ngoài ra, xem xét trường hợp cực đoan trong đó từ w vắng mặt khỏi Dₜ nhưng có ý nghĩa tương tự trong cả t-1 và t+1. Áp dụng trực tiếp bất kỳ kỹ thuật nhân tử hóa ma trận nào cho mỗi thời điểm sẽ thực thi uᵥ(t) ≈ 0. Tuy nhiên, với lựa chọn τ đúng, nghiệm uᵥ(t) của (5) sẽ gần với uᵥ(t-1) và uᵥ(t+1). Nhìn chung, phương pháp của chúng tôi đạt được embedding trung thành cao với kho ngữ liệu nhỏ hơn nhiều, và đặc biệt, trong Phần 6, chúng tôi chứng minh rằng embedding của chúng tôi mạnh mẽ chống lại việc lấy mẫu con đột ngột của các lát thời gian cụ thể.

3 TỐI ƯU HÓA
Một thách thức chính trong việc giải quyết (5) là với V và T lớn, người ta có thể không thể khớp tất cả ma trận PPMI Y(1), ..., Y(T) trong bộ nhớ, mặc dù Y(t) thưa thớt. Do đó, để có thể mở rộng, một giải pháp rõ ràng là đầu tiên phân tách mục tiêu theo thời gian, sử dụng tối thiểu hóa xen kẽ để giải cho U(t) tại mỗi bước:

min(U(t)) f(U(t)) = [1/2 ||Y(t) - U(t)U(t)ᵀ||²F + λ/2 ||U(t)||²F + τ/2 (||U(t-1) - U(t)||²F + ||U(t) - U(t+1)||²F)]  (6)

cho một t cụ thể. Lưu ý rằng f(U(t)) là bậc bốn trong U(t), và do đó ngay cả khi chúng ta chỉ giải cho một t cố định, (6) không thể được tối thiểu hóa phân tích. Do đó, lựa chọn duy nhất của chúng ta là giải (6) lặp sử dụng phương pháp bậc một nhanh như gradient descent. Gradient của điều kiện đầu tiên một mình được cho bởi

∇f(U(t)) = -2Y(t)U(t) + 2U(t)U(t)ᵀU(t)  (7)

Mỗi tính toán gradient có thứ tự O(nnz(Y(t))d + d²V) (sau đó được lồng trong tối ưu hóa lặp U(t) cho mỗi t). Trong các ứng dụng thực tế, V theo thứ tự hàng chục nghìn đến hàng trăm nghìn, và T theo thứ tự hàng chục đến hàng trăm.

Thay vào đó, hãy xem xét bài toán hơi nới lỏng của việc tối thiểu hóa

min(U(t),W(t)) [1/2 ∑ᵗ₌₁ᵀ ||Y(t) - U(t)W(t)ᵀ||²F + γ/2 ∑ᵗ₌₁ᵀ ||U(t) - W(t)||²F + λ/2 ∑ᵗ₌₁ᵀ ||U(t)||²F + τ/2 ∑ᵗ₌₂ᵀ ||U(t-1) - U(t)||²F + λ/2 ∑ᵗ₌₁ᵀ ||W(t)||²F + τ/2 ∑ᵗ₌₂ᵀ ||W(t-1) - W(t)||²F]  (8)

trong đó các biến W(t), t=1, ..., T được giới thiệu để phá vỡ tính đối xứng của việc nhân tử hóa Y(t). Bây giờ, tối thiểu hóa cho mỗi U(t) (và tương tự W(t)) chỉ là nghiệm của bài toán hồi quy ridge, và có thể được giải trong một bước bằng cách đặt gradient của mục tiêu của (8) bằng 0, tức là U(t)A = B trong đó

A = W(t)ᵀW(t) + (γ + λ + 2τ)I,
B = Y(t)W(t) + γW(t) + τ(U(t-1) + U(t+1))

cho t=2, ..., T-1, và các hằng số điều chỉnh cho t=0,T. Tạo A và B yêu cầu O(Vd² + nnz(Y(t))d), và giải U(t)A = B có thể được thực hiện trong tính toán O(d³), trong một bước. Điều này có thể được phân tách thêm thành các khối từng hàng có kích thước b, bằng cách tối thiểu hóa trên một khối hàng của U(t) tại một thời điểm, điều này làm giảm độ phức tạp tạo A và B thành O(bd² + nnz(Y(t)[:b,:])d), tức là độc lập với V. Điều này cho phép mở rộng cho V rất lớn, vì chỉ một khối hàng của Y(t) phải được tải tại một thời điểm.

Giảm tọa độ khối so với gradient descent ngẫu nhiên: Phương pháp được mô tả ở đây thường được gọi là giảm tọa độ khối (BCD) vì nó tối thiểu hóa đối với một khối duy nhất (U(t) hoặc W(t)) tại một thời điểm, và kích thước khối có thể được làm nhỏ hơn (một số hàng của U(t) hoặc W(t)) để duy trì khả năng mở rộng. Sức hấp dẫn chính của BCD là khả năng mở rộng [39]; tuy nhiên, nhược điểm chính là thiếu bảo đảm hội tụ, ngay cả trong trường hợp tối ưu hóa lồi [29]. Tuy nhiên, trong thực tế, BCD rất thành công và đã được sử dụng trong nhiều ứng dụng [38]. Lựa chọn tối ưu hóa khác là gradient descent ngẫu nhiên (SGD), phân tách mục tiêu thành tổng các điều kiện nhỏ hơn. Ví dụ, điều kiện đầu tiên của (8) có thể được viết như tổng của các điều kiện, mỗi điều kiện chỉ sử dụng một hàng của Y(t):

f(U(t)) = ∑ᵢ₌₁ᵛ ||Y(t)[i,:] - uᵢ(t)ᵀW(t)||²F  (9)

Độ phức tạp thoạt nhìn nhỏ hơn so với BCD; tuy nhiên, SGD đi kèm với các vấn đề được ghi chép tốt về tiến trình chậm và kích thước bước khó điều chỉnh, và trong thực tế, có thể chậm hơn nhiều cho các ứng dụng nhân tử hóa ma trận [30,39]. Tuy nhiên, chúng tôi chỉ ra rằng việc lựa chọn phương pháp tối ưu hóa không quan trọng đối với mô hình của chúng tôi; bất cứ thứ gì giải quyết thành công (5) sẽ dẫn đến embedding thành công như nhau.

4 TẬP DỮ LIỆU THỰC NGHIỆM VÀ THIẾT LẬP
Trong phần này, chúng tôi mô tả quy trình cụ thể được sử dụng để tạo embedding cho hai phần tiếp theo.

Tập dữ liệu bài báo tin tức: Đầu tiên, chúng tôi thu thập tổng cộng 99.872 bài báo từ The New York Times, được xuất bản từ tháng 1 năm 1990 đến tháng 7 năm 2016. Ngoài văn bản, chúng tôi cũng thu thập metadata bao gồm tiêu đề, tác giả, ngày phát hành, và nhãn phần (ví dụ, Business, Sports, Technology); tổng cộng có 59 phần như vậy. Chúng tôi sử dụng lát thời gian hàng năm, chia kho ngữ liệu thành T=27 phân vùng. Sau khi loại bỏ từ hiếm (ít hơn 200 lần xuất hiện trong tất cả bài báo qua thời gian) và từ dừng, từ vựng của chúng tôi bao gồm V=20.936 từ duy nhất. Sau đó chúng tôi tính toán ma trận đồng xuất hiện cho mỗi lát thời gian t với kích thước cửa sổ L=5, sau đó được sử dụng để tính toán ma trận PPMI như được nêu trong (3). Tất cả các phương pháp embedding mà chúng tôi so sánh đều được huấn luyện trên cùng tập dữ liệu này.

Chi tiết huấn luyện cho thuật toán của chúng tôi: Chúng tôi thực hiện tìm kiếm lưới để tìm các tham số chính quy hóa và tối ưu hóa tốt nhất. Kết quả tìm kiếm của chúng tôi, chúng tôi thu được λ=10, τ=γ=50, và chạy 5 epoch (5 lần hoàn thành qua tất cả lát thời gian, và tất cả hàng và cột của Y(t)). Thú vị là, đặt λ=0 cũng cho kết quả tốt, nhưng yêu cầu nhiều lần lặp hơn để hội tụ. Biến khối là một ma trận (U(t) hoặc V(t) cho một t cụ thể).

Thước đo khoảng cách: Tất cả khoảng cách giữa hai từ được tính toán bằng độ tương tự cosine giữa các vector embedding:

similarity(a,b) = cosine(uₐ, uᵦ) = uₐᵀuᵦ/(||uₐ||₂ · ||uᵦ||₂)  (10)

trong đó uₐ và uᵦ là embedding của từ a và b.

5 ĐÁNH GIÁ ĐỊNH TÍNH
Các embedding chúng tôi học tiết lộ các mẫu thú vị trong sự thay đổi ngữ nghĩa từ, tương tự ngữ nghĩa qua thời gian, và xu hướng phổ biến của các khái niệm từ kho ngữ liệu tin tức.

5.1 Trực quan hóa quỹ đạo
Quỹ đạo của một từ trong không gian nhúng (được căn chỉnh đúng cách) cung cấp công cụ để hiểu sự thay đổi trong ý nghĩa của từ theo thời gian. Điều này có thể giúp các ứng dụng rộng hơn, như nắm bắt và định lượng tiến hóa ngôn ngữ, đặc trưng thương hiệu và con người, và phân tích liên kết nổi lên giữa các từ nhất định.

Hình 1 cho thấy quỹ đạo của một tập hợp từ ví dụ. Chúng tôi vẽ phép chiếu t-SNE 2-D của embedding thời gian của mỗi từ qua thời gian. Chúng tôi cũng vẽ những từ gần nhất với từ mục tiêu từ mỗi lát thời gian. Chúng tôi chọn bốn từ quan tâm: apple và amazon như tên công ty mới nổi trong khi ban đầu đề cập đến trái cây và rừng mưa, và obama và trump như những người có vai trò nghề nghiệp thay đổi.

Trong tất cả trường hợp, các embedding minh họa sự thay đổi ngữ nghĩa đáng kể của các từ quan tâm trong khung thời gian 27 năm này. Chúng ta thấy apple thay đổi từ trái cây và nguyên liệu bánh tráng miệng đến không gian công nghệ. Thú vị là, có một đỉnh trong năm 1994 trong quỹ đạo, khi Apple dẫn đầu một làn sóng thảo luận ngắn vì việc thay thế CEO và hợp tác với IBM; sau đó liên kết chuyển về vùng lân cận trái cây và bánh tráng miệng cho đến khi phục hồi bởi Steve Jobs vào đầu những năm 2000. Tương tự, amazon thay đổi từ rừng thành công ty thương mại điện tử, cuối cùng hạ cánh vào năm 2016 như nhà cung cấp sáng tạo nội dung và phát trực tuyến trực tuyến do sự phổ biến của dịch vụ Prime Video. Tên tổng thống Hoa Kỳ, obama và trump, rất có ý nghĩa, thay đổi từ cuộc sống tiền tổng thống của họ (Obama như luật sư dân quyền và giáo sư đại học; Trump như nhà phát triển bất động sản và người nổi tiếng TV) đến lĩnh vực chính trị. Nhìn chung, Hình 1 chứng minh rằng thứ nhất, embedding từ thời gian của chúng tôi có thể nắm bắt tốt sự thay đổi ngữ nghĩa của từ qua thời gian, và thứ hai, mô hình của chúng tôi cung cấp chất lượng căn chỉnh cao ở chỗ các từ cùng ý nghĩa qua các năm khác nhau có embedding gần nhau về mặt hình học, mà không cần giải quyết bài toán tối ưu hóa riêng biệt cho căn chỉnh.

Bảng 1: Công nghệ tương đương qua thời gian: iphone, twitter, và mp3.

Truy vấn: iphone, 2012 | twitter, 2012 | mp3, 2000
90-94: desktop, pc, dos, macintosh, software | broadcast, cnn, bulletin, tv, radio, messages, correspondents | stereo, disk, disks, audio
95-96: | | mp3
97: | chat, messages, emails, web | 
98-02: | | pc
03: | | napster
04: | | mp3
05-06: | | ipod
07-08: | blog, posted | itunes, downloaded
09-12: iphone | | 
13-16: smartphone, iphone | twitter | 

5.2 Tìm kiếm tương đương
Một lợi thế chính khác của căn chỉnh từ là khả năng tìm các mục hoặc người "tương đương" về mặt khái niệm theo thời gian. Chúng tôi cung cấp ví dụ trong lĩnh vực công nghệ, vai trò chính thức, và chuyên gia thể thao. Trong loại thử nghiệm này, chúng tôi tạo truy vấn bao gồm cặp từ-năm đặc biệt đại diện cho từ đó trong năm đó, và tìm kiếm các cặp từ-năm khác trong vùng lân cận của nó, cho các năm khác nhau.

Bảng 1 liệt kê những từ gần nhất (top-1) của mỗi năm với vector truy vấn. Để mục đích trực quan hóa, chúng tôi nhóm các từ có ngữ nghĩa tương tự lại với nhau. Ví dụ, cột đầu tiên cho thấy iphone năm 2012 gần gũi với smartphone trong những năm gần đây, nhưng gần với các từ như desktop và macintosh trong những năm 90; thú vị là, telephone không bao giờ xuất hiện, cho thấy iPhone phục vụ mọi người nhiều hơn như một máy tính xách tay hơn là thiết bị gọi. Như một ví dụ khác, bằng cách xem xét quỹ đạo của twitter, chúng ta thấy sự tiến hóa của nguồn tin tức, từ phát sóng tin tức TV & radio trong những năm 90 đến phòng trò chuyện, trang web, và email vào đầu những năm 2000, blog vào cuối những năm 2000, và cuối cùng là tweet ngày nay. Ví dụ cuối cùng khá rõ ràng; mp3 đại diện cho hình thức chính mà âm nhạc được tiêu thụ năm 2000, thay thế disk và stereo trong những năm 1990 (cassette cũng xuất hiện trong top-3) và sau đó bị thay thế bởi streaming trực tuyến. Chúng ta có thể thấy một đỉnh một năm của Napster bị đóng cửa vì vi phạm bản quyền, và sau đó một dịch vụ streaming mới - iTunes.

Tiếp theo, chúng tôi sử dụng embedding để xác định những người trong vai trò chính trị. Bảng 2 cố gắng khám phá ai là tổng thống Hoa Kỳ và thị trưởng thành phố New York trong thời gian đó, sử dụng như truy vấn obama năm 2016 và blasio năm 2015. Đối với tổng thống, chỉ từ gần nhất từ mỗi năm được liệt kê, và luôn đúng (tính đến các năm bầu cử). Đối với thị trưởng, từ top-1 gần nhất được hiển thị trừ khi nó là mayor, trong trường hợp đó từ thứ hai được hiển thị. Chúng ta có thể thấy rằng cả vai trò tổng thống Hoa Kỳ và thị trưởng NYC đều được tìm kiếm tốt cho những người khác nhau trong nhiệm kỳ phục vụ của họ. Chúng ta thấy rằng embedding cho Tổng thống nhất quán, và phần lớn, thị trưởng NYC cũng vậy. Năm 2011, cuomo cũng một phần liên quan vì ông là thống đốc bang NY. Chúng tôi không tìm thấy bất kỳ từ liên quan nào trong truy vấn thị trưởng NYC năm 2006.

Cuối cùng, chúng tôi tìm kiếm sự tương đương trong thể thao, lặp lại thí nghiệm cho tay vợt nam ATP xếp hạng 1 như được hiển thị trong Bảng 3. Trong trường hợp tổng thống và thị trưởng, chúng tôi được hỗ trợ đáng kể bởi thực tế rằng họ thường được gọi bằng chức danh: "Tổng thống Obama" và "Thị trưởng de Blasio". Mặt khác, các vô địch quần vợt không được gọi bằng chức danh. Tuy nhiên, một số lượng đáng ngạc nhiên các vô địch đúng xuất hiện như những từ gần nhất, và tất cả các tên đều là những tay vợt nổi tiếng trong giai đoạn thời gian nhất định. Một nghiên cứu thực nghiệm toàn diện hơn về chất lượng căn chỉnh được cung cấp trong Phần 6.

Bảng 2: "Ai cai trị?" Từ gần nhất với obama năm 2016 (vai trò là tổng thống Hoa Kỳ) và blasio năm 2015 (vai trò là thị trưởng thành phố New York (NYC)). Dấu sao chỉ câu trả lời không chính xác.

Câu hỏi: Tổng thống Hoa Kỳ | Thị trưởng NYC
Truy vấn: obama, 2016 | blasio, 2015
90-92: bush | dinkins
93: clinton | 
94-00: | giuliani
01: bush | 
02-05: | bloomberg
06: | n/a*
07: | bloomberg
08: | 
09-10: obama | 
11: | cuomo*
12: | bloomberg
13-16: | blasio

Bảng 3: "Ai là tay vợt nam ATP xếp hạng No.1?" Từ gần nhất với nadal năm 2010 cho mỗi năm được liệt kê. Câu trả lời chính xác dựa trên xếp hạng cuối năm ATP và được in đậm trong bảng.

năm: 1990 | 1991 | 1992 | 1993
từ: edberg | lendl | sampras | sampras
1994 | 1995 | 1996 | 1997 | 1998
sampras | sampras | ivanisevic | sampras | sampras
1999 | 2000 | 2001 | 2002 | 2003
sampras | sampras | agassi | capriati | roddick
2004 | 2005 | 2006 | 2007 | 2008
federer | federer | roddick | federer | nadal
2009 | 2010 | 2011 | 2012 | 2013
federer | nadal | djokovic | federer | federer
2014 | 2015
federer | djokovic

5.3 Xác định độ phổ biến
Thường được quan sát rằng embedding từ được tính toán bằng cách nhân tử hóa ma trận PMI có norm tăng với tần suất từ [2,27]. Các norm vector từ này qua thời gian có thể được xem như một chuỗi thời gian để phát hiện các khái niệm xu hướng (ví dụ, sự thay đổi ngữ nghĩa đột ngột hoặc xuất hiện) đằng sau các từ, với tính mạnh mẽ hơn so với tần suất từ.

Hình 2 và 3 minh họa so sánh giữa norm embedding và tần suất để xác định độ phổ biến khái niệm mỗi năm, được xác định bởi từ khóa chính trong kho ngữ liệu The New York Times. Nói chung, so sánh với tần suất rời rạc và nhiều nhiễu hơn, chúng tôi lưu ý rằng norm của embedding của chúng tôi khuyến khích sự mượt mà và chuẩn hóa trong khi chỉ thị các giai đoạn khi các từ tương ứng đang làm tin tức. Trong Hình 2, các norm embedding hiển thị gần như các gò 4 năm đều tương ứng với nhiệm kỳ của mỗi tổng thống. Trong mỗi nhiệm kỳ, tên của mỗi tổng thống hiện tại trở thành khái niệm xu hướng đóng vai trò quan trọng trong cấu trúc thông tin tại thời điểm đó. Hai quan sát thú vị có thể thu được. Thứ nhất, vì Hillary Clinton liên tục phục vụ như Ngoại trưởng trong 2009-2013, sự phổ biến của clinton được bảo tồn; tuy nhiên nó vẫn không phổ biến như tổng thống obama. Thứ hai, vì chiến dịch tổng thống, trump năm 2016 có sự phổ biến tăng vượt xa so với vai trò trước đây như doanh nhân, và cuối cùng vượt qua đối thủ clinton về độ bao phủ tin tức.

Trong Hình 3, chúng ta có thể thấy sự tăng giảm mượt mà của các hiện tượng tạm thời (vụ bê bối enron và qaeda tăng). Đối với qaeda, chúng ta thấy có một bước nhảy năm 2001, và sau đó nó vẫn ổn định với sự suy giảm nhỏ. Ngược lại, enron cho thấy sự suy giảm mạnh hơn, vì bất chấp sự cường điệu tạm thời, nó không kéo dài trong tin tức lâu. Cũng lưu ý sự ổn định của việc sử dụng norm để theo dõi sự phổ biến so với tần suất, tăng đột biến cho enron trên qaeda, mặc dù 11/9 phổ biến tin tức hơn nhiều so với sự suy giảm bê bối của tập đoàn. Đối với ngôi sao bóng rổ pippen, mặc dù công khai của anh ta (ví dụ, tần suất) tương đối ít hơn so với các thuật ngữ kinh doanh, sự phổ biến của anh ta vẫn được công nhận bởi sự tăng cường trong norm vector. Đối với thuật ngữ khác isis, chúng ta có thể thấy nó bắt đầu thay thế qaeda như "tổ chức khủng bố xu hướng" trong phương tiện truyền thông tin tức.

Hình 2: Norm (trên) và tần suất tương đối (dưới) trong suốt các năm 1990-2016. Chúng tôi chọn tên các tổng thống Hoa Kỳ trong khung thời gian này - clinton, bush, obama, và trump. Chúng tôi lưu ý rằng bush có thể đề cập đến George H. W. Bush (1989-1993) hoặc George W. Bush (2001-2009). Clinton có thể đề cập đến Bill Clinton (1993-2001) hoặc Hillary Clinton (Ngoại trưởng Hoa Kỳ năm 2009-2013 và ứng viên tổng thống Hoa Kỳ năm 2014-2016).

Hình 3: Norm (trên) và tần suất tương đối (dưới) mỗi năm trong kho ngữ liệu của từ khóa sự kiện chính: pippen cho sự nổi tiếng bóng rổ, enron cho vụ bê bối tập đoàn, qaeda và isis cho các nhóm khủng bố nổi lên.

6 ĐÁNH GIÁ ĐỊNH LƯỢNG
Trong phần này, chúng tôi đánh giá thực nghiệm mô hình Dynamic Word2Vec (DW2V) đề xuất của chúng tôi so với các phương pháp embedding từ thời gian khác. Trong tất cả trường hợp, chúng tôi đặt chiều embedding là d=50. Chúng tôi có các baseline sau:

• Static-Word2Vec (SW2V): embedding word2vec tiêu chuẩn [25], được huấn luyện trên toàn bộ kho ngữ liệu và bỏ qua thông tin thời gian.
• Transformed-Word2Vec (TW2V) [15]: embedding U(t) đầu tiên được huấn luyện riêng biệt bằng cách nhân tử hóa ma trận PPMI cho mỗi năm t, và sau đó được biến đổi bằng cách tối ưu hóa ma trận biến đổi tuyến tính tối thiểu hóa khoảng cách giữa uw(t) và uw(t') cho embedding từ gần nhất k=30 với từ truy vấn w.
• Aligned-Word2Vec (AW2V) [12]: embedding U(t) đầu tiên được huấn luyện bằng cách nhân tử hóa ma trận PPMI cho mỗi năm t, và sau đó được căn chỉnh bằng cách tìm kiếm phép biến đổi trực chuẩn tốt nhất giữa U(t) và U(t+1).

6.1 Tương tự ngữ nghĩa
Một trong những tính chất quan trọng nhất của embedding từ là nó mang ý nghĩa của từ chính xác như thế nào. Do đó, chúng tôi phát triển một thử nghiệm để xem các từ có thể được phân loại theo ý nghĩa dựa trên embedding hay không. Các bài báo tin tức chúng tôi thu thập được gắn thẻ với "phần" của chúng như Business, Sports. Thông tin này có thể được sử dụng để xác định ý nghĩa từ thời gian. Quan trọng là lưu ý rằng thông tin này không được sử dụng trong việc học embedding từ. Ví dụ, chúng ta thấy amazon xuất hiện 41% thời gian trong World năm 1995, liên kết mạnh với rừng mưa (không phải một phần của Hoa Kỳ), và 50% thời gian trong Technology năm 2012, liên kết mạnh với thương mại điện tử. Do đó chúng tôi sử dụng điều này để thiết lập sự thật cơ bản của loại từ, bằng cách xác định các từ trong những năm đặc biệt nhiều trong một phần tin tức cụ thể. Tức là, nếu một từ cực kỳ thường xuyên trong một phần cụ thể, chúng tôi liên kết từ đó với phần đó và sử dụng điều đó như sự thật cơ bản. Chúng tôi chọn 11 phần phổ biến và phân biệt nhất của The New York Times, và cho mỗi phần s và mỗi từ w trong năm t, chúng tôi tính toán phần trăm p xuất hiện của nó trong mỗi phần. Để tránh các bộ ba <w,t,s> từ-thời gian-phần trùng lặp, cho một w và s cụ thể, chúng tôi chỉ giữ năm có sức mạnh lớn nhất, và thêm lọc bỏ bất kỳ bộ ba nào có sức mạnh ít hơn p=35%. Lưu ý rằng phân bố đều ngẫu nhiên của từ sẽ dẫn đến nó xuất hiện khoảng 9% thời gian trong mỗi phần, và ngưỡng của chúng tôi khoảng 4 lần số lượng đó. Chúng tôi làm điều này để nói với sự tin cậy đầy đủ rằng các liên kết như vậy có thể được coi là sự thật cơ bản.

Để hạn chế sự khác biệt kích thước giữa các loại, cho mỗi phần s có hơn 200 bộ ba đủ tiêu chuẩn, chúng tôi giữ top-200 từ theo sức mạnh. Tổng cộng, điều này dẫn đến 1888 bộ ba qua 11 phần, trong đó mỗi cặp từ-năm được liên kết mạnh mẽ với một phần như loại thực của nó.

Sau đó chúng tôi áp dụng k-means cầu, sử dụng độ tương tự cosine giữa các embedding như hàm khoảng cách để phân cụm, với K=10, 15, và 20 cụm. Chúng tôi sử dụng hai thước đo để đánh giá kết quả phân cụm:

• Thông tin lẫn nhau chuẩn hóa (NMI), được định nghĩa là
NMI(L,C) = I(L;C) / [H(L)+H(C)]/2  (11)

trong đó L biểu thị tập hợp nhãn và C tập hợp cụm. I(L;C) biểu thị tổng thông tin lẫn nhau giữa bất kỳ cụm ci và bất kỳ nhãn lj nào, và H(L) và H(C) entropy cho nhãn và cụm, tương ứng. Thước đo này đánh giá tính tinh khiết của kết quả phân cụm từ góc độ lý thuyết thông tin.

• Thước đo Fβ (Fβ), được định nghĩa là
Fβ = (β²+1)PR / (β²P+R)  (12)

trong đó P = TP/(TP+FP) biểu thị độ chính xác và R = TP/(TP+FN) biểu thị độ nhớ. (TP/FP = tích cực đúng/sai, TN/FN = tiêu cực đúng/sai.) Như một phương pháp thay thế để đánh giá phân cụm, chúng ta có thể xem mỗi cặp từ như một chuỗi quyết định. Chọn hai cặp (w,t) bất kỳ. Nếu chúng được phân cụm cùng nhau và thêm vào có cùng nhãn phần, đây là quyết định đúng; ngược lại, phân cụm thực hiện quyết định sai. Thước đo Fβ đo độ chính xác như trung bình điều hòa (có trọng số β) của độ chính xác và độ nhớ. Chúng tôi đặt β=5 để đưa trọng số nhiều hơn cho độ nhớ bằng cách phạt tiêu cực sai mạnh hơn.

Bảng 4 và 5 cho thấy đánh giá phân cụm. Chúng ta có thể thấy DW2V đề xuất của chúng tôi luôn vượt trội so với các baseline khác cho tất cả giá trị của K. Những kết quả này cho thấy hai lợi thế. Thứ nhất, sự thay đổi ngữ nghĩa từ đã được nắm bắt bởi embedding thời gian (ví dụ, bằng cách tương quan chính xác với nhãn phần của amazon, thay đổi từ World thành Technology). Thứ hai, vì embedding của từ của tất cả các năm được sử dụng cho phân cụm, kết quả phân cụm tốt chỉ ra căn chỉnh tốt qua các năm. Chúng ta cũng có thể thấy rằng AW2V cũng hoạt động tốt, vì nó cũng áp dụng căn chỉnh giữa các lát thời gian liền kề cho tất cả từ. Tuy nhiên, TW2V không hoạt động tốt như những cái khác, cho thấy rằng căn chỉnh cục bộ (chỉ một số từ) không đủ cho chất lượng căn chỉnh cao.

Bảng 4: Thông tin lẫn nhau chuẩn hóa (NMI).
Phương pháp | 10 Cụm | 15 Cụm | 20 Cụm
SW2V | 0.6736 | 0.6867 | 0.6713
TW2V | 0.5175 | 0.5221 | 0.5130
AW2V | 0.6580 | 0.6618 | 0.6386
DW2V | 0.7175 | 0.7162 | 0.6906

Bảng 5: Thước đo F (Fβ).
Phương pháp | 10 Cụm | 15 Cụm | 20 Cụm
SW2V | 0.6163 | 0.7147 | 0.7214
TW2V | 0.4584 | 0.5072 | 0.5373
AW2V | 0.6530 | 0.7115 | 0.7187
DW2V | 0.6949 | 0.7515 | 0.7585

6.2 Chất lượng căn chỉnh
Bây giờ chúng tôi đánh giá chất lượng căn chỉnh trực tiếp hơn, tức là tính chất rằng phân bố ngữ nghĩa trong không gian embedding thời gian nên nhất quán theo thời gian. Ví dụ, nếu một từ như estate hoặc republican không thay đổi nhiều trong suốt thời gian, embedding của nó nên giữ tương đối không đổi cho t khác nhau. Bằng logic tương tự, nếu một từ như trump thay đổi liên kết trong suốt thời gian, embedding của nó nên phản ánh sự thay đổi này bằng cách di chuyển từ một vị trí đến vị trí khác (ví dụ, estate→republican). Chúng ta đã thấy điều này trong phần trước cho các từ tĩnh như president hoặc mayor; chúng không thay đổi ý nghĩa, mặc dù chúng được đi kèm với tên thay đổi thành chúng mỗi vài năm.

Để kiểm tra chất lượng căn chỉnh embedding, chúng tôi tạo một nhiệm vụ để truy vấn sự tương đương qua các năm. Ví dụ, cho obama-2012, chúng tôi muốn truy vấn từ tương đương của nó năm 2002. Như chúng ta biết obama là tổng thống Hoa Kỳ năm 2012; tương đương của nó năm 2002 là bush, người là tổng thống Hoa Kỳ vào thời điểm đó. Theo cách này, chúng tôi tạo hai testset. Cái đầu tiên dựa trên kiến thức được ghi chép công khai mà cho mỗi năm liệt kê các tên khác nhau cho một vai trò cụ thể, như tổng thống Hoa Kỳ, thủ tướng Vương quốc Anh, đội vô địch NFL superbowl, v.v. Cho mỗi năm (ví dụ, 2012), chúng tôi đặt từ của nó (ví dụ, obama) vào tập embedding của mỗi năm khác để truy vấn sự tương đương của nó trong các từ gần nhất.

Thử nghiệm thứ hai được tạo ra bởi con người, để khám phá các khái niệm thú vị hơn như công nghệ mới nổi, thương hiệu và sự kiện chính (ví dụ, bùng phát bệnh dịch và khủng hoảng tài chính). Để xây dựng các cặp từ thử nghiệm, trước tiên chúng tôi chọn các thuật ngữ mới nổi chưa được phổ biến trước 1994, sau đó truy vấn những tiền lệ nổi tiếng của chúng trong 1990 đến 1994 (ví dụ, app-2012 có thể tương ứng với software-1990). Cho từ mới nổi (ví dụ, app) chúng tôi trích xuất năm phổ biến nhất (ví dụ, 2012) với tần suất tối đa, và đặt embedding của nó vào mỗi năm từ 1990 đến 1994 để truy vấn tiền lệ của nó (ví dụ, software). Mỗi cặp từ-năm bây giờ tạo thành một truy vấn và một câu trả lời; tổng cộng chúng tôi có N=11028 cặp như vậy trong testset đầu tiên, và N=445 trong cái thứ hai.

Chúng tôi sử dụng hai thước đo để đánh giá hiệu suất.

• Cho mỗi thử nghiệm i, từ câu trả lời đúng được xác định tại vị trí rank[i] cho các từ gần nhất. Thứ hạng đảo ngược trung bình (MRR) được định nghĩa là
MRR = (1/N) ∑ᵢ₌₁ᴺ (1/rank[i])  (13)

trong đó 1/rank[i] = 0 nếu câu trả lời đúng không được tìm thấy trong top-10. MRR cao hơn có nghĩa là câu trả lời đúng xuất hiện gần hơn và rõ ràng hơn với embedding truy vấn.

• Ngoài ra, cho thử nghiệm i bao gồm cặp từ-năm truy vấn và mục tiêu, xem xét K từ gần nhất với embedding truy vấn trong năm mục tiêu. Nếu từ mục tiêu nằm trong số K từ này, thì Precision@K cho thử nghiệm i (ký hiệu P@K[i]) là 1; ngược lại, nó là 0. Thì Mean Precision@K được định nghĩa là
MP@K = (1/N) ∑ᵢ₌₁ᴺ (P@K[i])  (14)

Độ chính xác cao hơn chỉ ra khả năng tốt hơn để có được câu trả lời đúng sử dụng embedding gần.

Bảng 6 và 7 cho thấy đánh giá của thử nghiệm căn chỉnh. Chúng ta có thể thấy rằng phương pháp đề xuất của chúng tôi vượt trội so với những cái khác và cho thấy chất lượng căn chỉnh tốt, đôi khi theo thứ tự độ lớn. So sánh với testset 1 có một lượng lớn truy vấn với căn chỉnh tầm ngắn đáng kể (ví dụ, từ 2012 đến 2013), testset 2 chủ yếu bao gồm ít căn chỉnh tầm dài hơn (ví dụ, 2012 đến 1990). Do đó, chúng ta có thể thấy rằng hiệu suất của SW2V tương đối tốt trong testset 1 vì phân bố ngữ nghĩa không thay đổi nhiều trong tầm ngắn khiến thử nghiệm này thuận lợi cho embedding tĩnh. Tuy nhiên, SW2V suy giảm mạnh trong testset 2, nơi căn chỉnh tầm dài cần thiết hơn. Đối với TW2V, vì nó thực hiện biến đổi năm-đối-năm cá thể (ví dụ, 2012-đến-1990) bằng cách giả định rằng cấu trúc địa phương của từ mục tiêu không thay đổi, chất lượng căn chỉnh tổng thể của toàn bộ tập embedding không thỏa mãn trong testset 1 với nhiều căn chỉnh. Tuy nhiên, nó hoạt động tương tự như AW2V trong testset 2 vì biến đổi năm-đối-năm cá thể của nó làm cho nó có khả năng hơn cho căn chỉnh tầm dài. AW2V, thực thi căn chỉnh cho toàn bộ tập embedding giữa các lát thời gian liền kề, cung cấp hiệu suất khá đáng tin cậy. Tuy nhiên, chất lượng căn chỉnh của nó vẫn dưới chúng tôi, cho thấy rằng phương pháp hai bước của họ không thành công trong việc thực thi căn chỉnh toàn cục.

Bảng 6: Thứ hạng đảo ngược trung bình (MRR) và Độ chính xác trung bình (MP) cho Testset 1.
Phương pháp | MRR | MP@1 | MP@3 | MP@5 | MP@10
SW2V | 0.3560 | 0.2664 | 0.4210 | 0.4774 | 0.5612
TW2V | 0.0920 | 0.0500 | 0.1168 | 0.1482 | 0.1910
AW2V | 0.1582 | 0.1066 | 0.1814 | 0.2241 | 0.2953
DW2V | 0.4222 | 0.3306 | 0.4854 | 0.5488 | 0.6191

Bảng 7: Thứ hạng đảo ngược trung bình (MRR) và Độ chính xác trung bình (MP) cho Testset 2.
Phương pháp | MRR | MP@1 | MP@3 | MP@5 | MP@10
SW2V | 0.0472 | 0.0000 | 0.0787 | 0.0787 | 0.2022
TW2V | 0.0664 | 0.0404 | 0.0764 | 0.0989 | 0.1438
AW2V | 0.0500 | 0.0225 | 0.0517 | 0.0787 | 0.1416
DW2V | 0.1444 | 0.0764 | 0.1596 | 0.2202 | 0.3820

6.3 Tính mạnh mẽ
Cuối cùng, chúng tôi khám phá tính mạnh mẽ của mô hình embedding chống lại việc lấy mẫu con từ cho các năm được chọn. Bảng 8 cho thấy kết quả của nhiệm vụ căn chỉnh (testset 1) cho các vector được tính toán từ ma trận đồng xuất hiện được lấy mẫu con cho mỗi ba năm từ 1991 đến 2015. Để lấy mẫu con, mỗi phần tử Cᵢⱼ được thay thế bằng một số nguyên được rút ra ngẫu nhiên Ĉᵢⱼ từ phân bố Binomial với tốc độ r và n=Cᵢⱼ thử nghiệm; điều này mô phỏng số lần đồng xuất hiện được đo nếu chúng bị bỏ lỡ với xác suất r. Tần suất mới f̂ sau đó được chuẩn hóa lại để f̂ᵢ/fᵢ = (∑ⱼ Ĉᵢⱼ)/(∑ⱼ Cᵢⱼ). Được liệt kê là kết quả thử nghiệm căn chỉnh cho r=1, 0.1, 0.01, và 0.001 so với [12], hoạt động tương đương với embedding của chúng tôi. Không ngạc nhiên, đối với các cuộc tấn công cực đoan (chỉ để lại 1% hoặc 0.1% đồng xuất hiện), hiệu suất của [12] suy giảm mạnh; tuy nhiên, vì phương pháp tối ưu hóa kết hợp của chúng tôi, hiệu suất của embedding của chúng tôi dường như giữ ổn định.

Bảng 8: MRR và MP cho căn chỉnh với lấy mẫu con mỗi 3 năm.
Phương pháp | r | MRR | MP@1 | MP@3 | MP@5 | MP@10
AW2V | 100% | 0.1582 | 0.1066 | 0.1814 | 0.2241 | 0.2953
AW2V | 10% | 0.0884 | 0.0567 | 0.1020 | 0.1287 | 0.1727
AW2V | 1% | 0.0409 | 0.0255 | 0.0475 | 0.0605 | 0.0818
AW2V | 0.1% | 0.0362 | 0.0239 | 0.0416 | 0.0532 | 0.0690
DW2V | 100% | 0.4222 | 0.3306 | 0.4854 | 0.5488 | 0.6191
DW2V | 10% | 0.4394 | 0.3489 | 0.5036 | 0.5628 | 0.6292
DW2V | 1% | 0.4418 | 0.3522 | 0.5024 | 0.5636 | 0.6310
DW2V | 0.1% | 0.4427 | 0.3550 | 0.5006 | 0.5612 | 0.6299

7 CÔNG TRÌNH LIÊN QUAN
Hiệu ứng thời gian trong xử lý ngôn ngữ tự nhiên: Có một số nghiên cứu điều tra các đặc trưng thời gian của ngôn ngữ tự nhiên. Một số sử dụng mô hình chủ đề trên kho ngữ liệu tin tức [1] hoặc tạp chí khoa học có dấu thời gian [6,33,36] để tìm đỉnh và xuất hiện của chủ đề và quan điểm. Các đặc trưng đếm từ đơn giản hơn được sử dụng trong [7,13,22] để tìm các khái niệm được thảo luận nóng bỏng và hiện tượng văn hóa, trong [21,32,34] để phân tích hành vi thiếu niên trong phòng trò chuyện, và trong [13] để khám phá sự cố cúm.

Học embedding từ: Ý tưởng về embedding từ đã tồn tại ít nhất từ những năm 90, với các vector được tính toán như các hàng của đồng xuất hiện [20], thông qua nhân tử hóa ma trận [9], và nổi tiếng nhất thông qua mạng nơ-ron sâu [5,8]. Chúng gần đây đã được phổ biến lại với thành công của embedding chiều thấp như GloVE [27] và word2vec [24,25], đã được chứng minh là cải thiện đáng kể hiệu suất trong các nhiệm vụ NLP chính, như phân cụm tài liệu [16], LDA [28], và tương tự từ [3,18]. Có mối liên hệ chặt chẽ giữa các phương pháp gần đây này và phương pháp đề xuất của chúng tôi, ở chỗ cả word2vec và GloVE đều được chứng minh là tương đương với nhân tử hóa ma trận của ma trận PMI dịch chuyển [17].

Embedding từ thời gian và đánh giá: Trong khi các công cụ NLP đã được sử dụng thường xuyên để khám phá ý nghĩa từ mới nổi và xu hướng xã hội, nhiều trong số chúng dựa trên thay đổi trong ma trận đồng xuất hiện hoặc PMI [11,13,22,26,37], thay đổi trong phần từ loại [23] hoặc các phương pháp thống kê khác [4,15,35]. Một số công trình sử dụng embedding từ chiều thấp, nhưng hoặc không làm mịn [31], hoặc sử dụng phương pháp hai bước [12,14,15]. Sự thay đổi ngữ nghĩa và xuất hiện cũng được đánh giá theo nhiều cách khác nhau. Trong [31], sự thay đổi từ được xác định bằng cách theo dõi góc trung bình giữa một từ và các từ lân cận của nó. Một trong số các thử nghiệm trong [15] tạo dữ liệu tổng hợp với sự thay đổi ngữ nghĩa được tiêm, và định lượng độ chính xác của việc nắm bắt chúng sử dụng các thước đo chuỗi thời gian khác nhau. Trong [23], các tác giả cho thấy tính có ý nghĩa ngữ nghĩa của các đặc trưng từ vựng chính bằng cách sử dụng chúng để dự đoán dấu thời gian của một cụm từ cụ thể. Và, [26] tạo kết nối rằng ý nghĩa mới nổi thường cùng tồn tại với ý nghĩa trước đó, và sử dụng embedding động để khám phá và xác định đa nghĩa, được đánh giá chống lại WordNet. Chủ yếu, embedding từ thời gian được đánh giá chống lại cơ sở dữ liệu do con người tạo ra các từ đã biết thay đổi ngữ nghĩa [12,15,35] đây cũng là phương pháp của chúng tôi.

8 KẾT LUẬN
Chúng tôi nghiên cứu sự tiến hóa của ngữ nghĩa từ như một bài toán học embedding từ động. Chúng tôi đề xuất một mô hình để học embedding từ nhận biết thời gian và sử dụng nó để khai thác động kho ngữ liệu văn bản. Phương pháp đề xuất của chúng tôi đồng thời học embedding và căn chỉnh chúng qua thời gian, và có một số lợi ích: khả năng diễn giải cao hơn cho embedding, chất lượng tốt hơn với ít dữ liệu hơn, và căn chỉnh đáng tin cậy hơn cho truy vấn qua thời gian. Chúng tôi giải quyết bài toán tối ưu hóa kết quả sử dụng phương pháp giảm tọa độ khối có thể mở rộng. Chúng tôi thiết kế các phương pháp định tính và định lượng để đánh giá embedding thời gian cho ngữ nghĩa từ tiến hóa, và cho thấy rằng phương pháp embedding động của chúng tôi hoạt động thuận lợi so với các phương pháp embedding thời gian khác.

TÀI LIỆU THAM KHẢO
[1] James Allan, Rahul Gupta, và Vikas Khandelwal. 2001. Temporal summaries of new topics. Trong Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 10–18.
[2] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, và Andrej Risteski. 2015. Rand-walk: A latent variable model approach to word embeddings. arXiv preprint arXiv:1502.03520 (2015).
[3] Marco Baroni, Georgiana Dinu, và Germán Kruszewski. 2014. Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.. Trong ACL (1). 238–247.
[4] Pierpaolo Basile, Annalina Caputo, và Giovanni Semeraro. 2014. Analysing word meaning over time by exploiting temporal random indexing. Trong First Italian Conference on Computational Linguistics CLiC-it.
[5] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, và Christian Jauvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research 3 (2003), 1137–1155.
[6] David M Blei và John D Lafferty. 2006. Dynamic topic models. Trong Proceedings of the 23rd international conference on Machine learning. ACM, 113–120.
[7] Hyunyoung Choi và Hal Varian. 2012. Predicting the present with Google Trends. Economic Record 88, s1 (2012), 2–9.
[8] Ronan Collobert và Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. Trong Proceedings of the 25th international conference on Machine learning. ACM, 160–167.
[9] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, và Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American society for information science 41, 6 (1990), 391.
[10] John R Firth. 1957. {A synopsis of linguistic theory, 1930-1955}. (1957).
[11] Kristina Gulordava và Marco Baroni. 2011. A distributional similarity approach to the detection of semantic change in the Google Books Ngram corpus. Trong Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics. Association for Computational Linguistics, 67–71.
[12] William L Hamilton, Jure Leskovec, và Dan Jurafsky. 2016. Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change. arXiv preprint arXiv:1605.09096 (2016).
[13] Gerhard Heyer, Florian Holz, và Sven Teresniak. 2009. Change of Topics over Time-Tracking Topics by their Change of Meaning. KDIR 9 (2009), 223–228.
[14] Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, và Slav Petrov. 2014. Temporal analysis of language through neural language models. arXiv preprint arXiv:1405.3515 (2014).
[15] Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, và Steven Skiena. 2015. Statistically significant detection of linguistic change. Trong Proceedings of the 24th International Conference on World Wide Web. ACM, 625–635.
[16] Matt J Kusner, Yu Sun, Nicholas I Kolkin, Kilian Q Weinberger, và others. 2015. From Word Embeddings To Document Distances.. Trong ICML, Vol. 15. 957–966.
[17] Omer Levy và Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. Trong Advances in neural information processing systems. 2177–2185.
[18] Omer Levy, Yoav Goldberg, và Ido Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics 3 (2015), 211–225.
[19] Xuanyi Liao và Guang Cheng. 2016. Analysing the Semantic Change Based on Word Embedding. Trong International Conference on Computer Processing of Oriental Languages. Springer, 213–223.
[20] Kevin Lund và Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, Instruments, & Computers 28, 2 (1996), 203–208.
[21] Guy Merchant. 2001. Teenagers in cyberspace: An investigation of language use and language change in internet chatrooms. Journal of Research in Reading 24, 3 (2001), 293–306.
[22] Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K Gray, Joseph P Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, và others. 2011. Quantitative analysis of culture using millions of digitized books. science 331, 6014 (2011), 176–182.
[23] Rada Mihalcea và Vivi Nastase. 2012. Word epoch disambiguation: Finding how words change over time. Trong Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2. Association for Computational Linguistics, 259–263.
[24] Tomas Mikolov, Kai Chen, Greg Corrado, và Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).
[25] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, và Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. Trong Advances in neural information processing systems. 3111–3119.
[26] Sunny Mitra, Ritwik Mitra, Martin Riedl, Chris Biemann, Animesh Mukherjee, và Pawan Goyal. 2014. That's sick dude!: Automatic identification of word sense change across different timescales. arXiv preprint arXiv:1405.4392 (2014).
[27] Jeffrey Pennington, Richard Socher, và Christopher D Manning. 2014. Glove: Global Vectors for Word Representation.. Trong EMNLP, Vol. 14. 1532–1543.
[28] James Petterson, Wray Buntine, Shravan M Narayanamurthy, Tibério S Caetano, và Alex J Smola. 2010. Word features for latent dirichlet allocation. Trong Advances in Neural Information Processing Systems. 1921–1929.
[29] Michael JD Powell. 1973. On search directions for minimization algorithms. Mathematical Programming 4, 1 (1973), 193–201.
[30] Nikhil Rao, Hsiang-Fu Yu, Pradeep K Ravikumar, và Inderjit S Dhillon. 2015. Collaborative filtering with graph information: Consistency and scalable methods. Trong Advances in neural information processing systems. 2107–2115.
[31] Eyal Sagi, Stefan Kaufmann, và Brady Clark. 2011. Tracing semantic change with latent semantic analysis. Current methods in historical semantics (2011), 161–183.
[32] Diane J Schiano, Coreena P Chen, Ellen Isaacs, Jeremy Ginsberg, Unnur Gretarsdottir, và Megan Huddleston. 2002. Teen use of messaging media. Trong CHI'02 extended abstracts on Human factors in computing systems. ACM, 594–595.
[33] Ruben Sipos, Adith Swaminathan, Pannaga Shivaswamy, và Thorsten Joachims. 2012. Temporal corpus summarization using submodular word coverage. Trong Proceedings of the 21st ACM international conference on Information and knowledge management. ACM, 754–763.
[34] Sali A Tagliamonte và Derek Denis. 2008. Linguistic ruin? LOL! Instant messaging and teen language. American speech 83, 1 (2008), 3–34.
[35] Xuri Tang, Weiguang Qu, và Xiaohe Chen. 2016. Semantic change computation: A successive approach. World Wide Web 19, 3 (2016), 375–415.
[36] Xuerui Wang và Andrew McCallum. 2006. Topics over time: a non-Markov continuous-time model of topical trends. Trong Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 424–433.
[37] Derry Tanti Wijaya và Reyyan Yeniterzi. 2011. Understanding semantic change of words over centuries. Trong Proceedings of the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web. ACM, 35–40.
[38] Stephen J Wright. 2015. Coordinate descent algorithms. Mathematical Programming 151, 1 (2015), 3–34.
[39] Hsiang-Fu Yu, Cho-Jui Hsieh, Si Si, và Inderjit Dhillon. 2012. Scalable coordinate descent approaches to parallel matrix factorization for recommender systems. Trong 12th IEEE International Conference on Data Mining (ICDM). IEEE, 765–774.
[40] Yating Zhang, Adam Jatowt, Sourav S Bhowmick, và Katsumi Tanaka. 2016. The Past is Not a Foreign Country: Detecting Semantically Similar Terms across Time. IEEE Transactions on Knowledge and Data Engineering 28, 10 (2016), 2793–2807.
