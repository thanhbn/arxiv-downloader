# 2311.08981.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/contrastive/2311.08981.pdf
# File size: 1809737 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Speculative Contrastive Decoding
Hongyi Yuan12âˆ—, Keming Lu2, Fei Huang2, Zheng Yuan2, Chang Zhou2
1Tsinghua University,2Alibaba Inc.
yuanhy20@mails.tsinghua.edu.cn
{lukeming.lkm,feihu.hf}@alibaba-inc.com
{yuanzheng.yuanzhen,ericzhou.zc}@alibaba-inc.com
Abstract
Large language models (LLMs) exhibit ex-
ceptional performance in language tasks, yet
their auto-regressive inference is limited due to
high computational requirements and is sub-
optimal due to the exposure bias. Inspired
by speculative decoding and contrastive de-
coding, we introduce Speculative Contrastive
Decoding (SCD), a straightforward yet pow-
erful decoding approach that leverages predic-
tions from smaller language models (LMs) to
achieve both decoding acceleration and quality
improvement. Extensive evaluations and anal-
yses on four diverse language tasks demon-
strate the effectiveness of SCD, showing that
decoding efficiency and quality can compati-
bly benefit from one smaller LM.
1 Introduction
Large language models (LLMs) have advanced
the versatility and proficiency in approaching real-
world natural language tasks such as general in-
struction following (Ouyang et al., 2022; Taori
et al., 2023; Lu et al., 2023) and reasoning (Cobbe
et al., 2021; Wei et al., 2023; Yuan et al., 2023).
Most existing LLMs (Brown et al. (2020); Tou-
vron et al. (2023); Bai et al. (2023), inter alia ) are
built on decoder-only Transformers. Due to the
auto-regressive nature during inference, the run-
time of decoding inference can be excessive on
general computation infrastructure, and the gen-
eration quality can be sub-optimal due to the ex-
posure bias (Arora et al., 2022). Improving decod-
ing inference has been the spotlight of the research
community in language generation (Vijayakumar
et al., 2018; Holtzman et al., 2020; Su et al., 2022).
As for decoding acceleration, one prominent
method named speculative decoding (Leviathan
et al., 2022; Chen et al., 2023) has been pro-
posed and leverages relatively smaller language
models (LMs) to predict several successive token
âˆ— âˆ—Work done during internship at Alibaba Inc.generations of target LLMs. The LLMs only re-
quire one-time forward computation for check-
ing the validity of predictions from the smaller
LMs. The decoding method maintains the target
LLMsâ€™ token distributions and accelerates more
when smaller LMs can accurately predict the po-
tential target LLMsâ€™ generations.
As for the generation quality, contrastive de-
coding has been recently proposed (Li et al.,
2023a). Contrastive decoding assumes that con-
jugated smaller LMs may present higher system-
atic tendencies to generate erroneous tokens than
the larger ones, and the method seeks to elimi-
nate such systematic error by contrasting the to-
ken distribution between smaller LMs and larger
LMs. From either inference acceleration or qual-
ity improvement, these works have demonstrated
a promising direction by integrating smaller LMs
during auto-regressive generation.
Inspired by both speculative and contrastive de-
coding, we propose Speculative Contrastive De-
coding (SCD), which exploits a single smaller LM
for decoding improvement in speed and quality en
bloc. Comprehensive evaluations of four diverse
tasks show that SCD can achieve similar acceler-
ation factors of speculative decoding while main-
taining the quality improvement from contrastive
decoding. By further analyzing the token distri-
butions of the smaller and larger LMs in SCD, we
show the inherent compatibility of decoding accel-
eration and quality improvement. The contribu-
tions of this paper can be summarized as follows:
â€¢ We propose Speculative Contrastive Decoding
for efficacious LLM inference.
â€¢ Comprehensive experiments and analysis illus-
trate the compatibility of speculative and con-
trastive decoding on 4 diverse tasks.
2 Related Works
In terms of inference acceleration, recent research
has been devoted to developing various efficientarXiv:2311.08981v2  [cs.CL]  13 Mar 2024

--- PAGE 2 ---
decoding methods (Yao et al., 2022; Kwon et al.,
2023; Cai et al., 2023). Speculative decoding
Leviathan et al. (2022); Chen et al. (2023); Kim
et al. (2023) is one of these recent works and uti-
lizes smaller models for acceleration. Miao et al.
(2023); Spector and Re (2023) propose to orga-
nize predictions from small LMs into tree struc-
tures to accelerate speculative decoding further.
In terms of inference quality, rich research has
been suggested (Vijayakumar et al., 2018; Holtz-
man et al., 2020; Su et al., 2022; Su and Xu, 2022;
Finlayson et al., 2023) and contrastive decoding
achieves better decoding qualities by similarly in-
tegrating smaller LMs and devise contrastive to-
ken distributions (Li et al., 2023a; Oâ€™Brien and
Lewis, 2023). It can further be adjusted to other
variants such as the token distribution contrasting
between model layers (Chuang et al., 2023) or dif-
ferent inputs (Yona et al., 2023). SCD draws in-
spiration from these works and benefits both de-
coding speed and quality by incorporating smaller
LMs into generation.
3 Preliminaries
We follow the terminology in Li et al. (2023a), and
term the target larger LMs as the expert LMs while
the smaller LMs as the amateur LMs denoted as
MeandMarespectively.
3.1 Contrastive Decoding
The intrinsic rationale of contrastive decod-
ing (CD) is that amateur LMs have stronger sys-
tematic undesirable tendencies to produce unde-
sirable patterns (e.g., hallucination) than expert
LMs. By contrasting the token distributions be-
tween expert and amateur LMs, such tendencies
can be alleviated. There have been successively
proposed two versions of contrastive decoding by
Li et al. (2023a) and Oâ€™Brien and Lewis (2023),
which we term as Original contrastive decoding
andImproved contrastive decoding. The final con-
trastive logit scores for the original contrastive de-
coding sori(xi|x<i)and the improved contrastive
decoding simp(xi|x<i)are respectively:
sori(xi|x<i) =

logPMe(xi|x<i)âˆ’logPMa(xi|x<i), x iâˆˆ VÎ±
ori,i
âˆ’âˆž, x i/âˆˆ VÎ±
ori,i
simp(xi|x<i) =

(1 +Î²)YMe(xi|x<i)âˆ’Î²YMa(xi|x<i), x iâˆˆ VÎ±
imp,i
âˆ’âˆž, x i/âˆˆ VÎ±
imp,iAlgorithm 1: Speculative Contrastive Decoding
Data:Me,Ma, input prefix xinp
Result: [xinp, x1, .., x k]
1forifrom1toÎ³do
2 xiâˆ¼PMa(xi) =Ma(xi|xinp, x<i);
3PMe(x1), .., PMe(xÎ³+1) =Me(x1, .., x Î³|xinp);
4Calculate Pn(x1), .., P n(xÎ³)following Section Â§3.1;
5r1, .., r Î³i.i.d sampled from Uniform (0,1);
6k= min
{i|ri>Pn(xi)
PMa(xi)} âˆª {Î³+ 1}
;
7ifkâ‰¤Î³then
8 Pk(xk) = norm(max(0 , Pn(xk)âˆ’PMa(xk));
9 Resample xkâˆ¼Pk(xk);
10else
11 PMa(xÎ³+1) =Ma(xÎ³+1|xinp, x1, .., x Î³);
12 Calculate Pn(xÎ³+1)following Section Â§3.1;
13 xÎ³+1âˆ¼Pn(xÎ³+1);
where PÂ·andYÂ·are respectively the token prob-
ability and logit generated from LMs. VÎ±
Â·,idenotes
the adaptive plausibility constraint that dynami-
cally restricts the logits from producing the erro-
neous modes. The adaptive plausibility constraints
are calculated as
VÎ±
ori,i=
w|PMe(w|x<i)> Î±max
wâˆˆVPMe(w|x<i)
,
VÎ±
imp,i=
w|YMe(w|x<i)>logÎ±+ max
wâˆˆVYMe(w|x<i)
.
A token is generated from the contrastive token
distribution PÏ„
n(xi) = softmax Ï„(sn(xi|x<i)),
nâˆˆ {ori,imp}, where Ï„represents the softmax
temperature that determines the smoothness of the
contrastive token distribution.
3.2 Speculative Decoding
Instead of requiring one forward computation of
Mefor each token in vanilla decoding, specula-
tive decoding (SD) utilizes Mato primarily gener-
ateÎ³tokens at each iteration then Memakes one
forward computation to check the validity of the Î³
tokens. If Meaccepts all the Î³tokens, it finishes
the iteration with an additional generated token,
resulting in Î³+ 1tokens generated. Otherwise, if
Merejects a token at r, the token is re-sampled
according to Meto substitute the rejected token;
hence the iteration finishes with rtokens gener-
ated. With only one-time forward computation of
Me, multiple tokens are generated at each itera-
tion. When the ratio between the runtime required
ofMaandMe(the cost coefficient c, Leviathan
et al. (2022)) is low and the token acceptance rate
is high, there will present a notable acceleration.

--- PAGE 3 ---
4 Speculative Contrastive Decoding
Speculative decoding leverages smaller Maonly
for generation acceleration, while not making the
best of the token distributions from Ma. It is
natural to simultaneously apply the contrastive
token distribution, and with negligible computa-
tional overhead, the generation quality and ef-
ficiency can benefit from integrating speculative
and contrastive decoding. Therefore, we propose
Speculative Contrastive Decoding (SCD).
Concretely, at each iteration, Î³tokens are gen-
erated from the amateur model Ma. When check-
ing the validity of the tokens, the target distri-
bution becomes PÏ„
n, nâˆˆ {ori,imp}from con-
trastive distribution instead of PMein speculative
decoding. For a token xin theMa-generated to-
kens, it is rejected with probability 1âˆ’PÏ„
n(x)
PMa(x)
and then a new token in place of xis re-sampled
from norm(max(0 , PÏ„
n(x)âˆ’PMa(x)), where
norm ( f(x)) = f(x)/P
xf(x),s.t.f(x)â‰¥0. If
all the Ma-generated tokens are accepted, then an
additional token is sampled from PÏ„
n.
The sampling procedure of SCD is similar to
the original speculative decoding in Leviathan
et al. (2022); Chen et al. (2023). However, it is
worth noticing that in our SCD, when all the Ma-
generated tokens are accepted, we require an ad-
ditional forward computation from Mato acquire
its last token logit for calculating the contrastive
distribution PÏ„
nat that iteration, while in specula-
tive decoding, the additional token is sampled di-
rectly from Me. This computational overhead is
negligible when cis small. We detailed the algo-
rithm of our SCD in Algorithm Alg. 1. The dif-
ference from the original speculative decoding is
highlighted in blue.
5 Experiment
Experiment Setting. We evaluate SCD and other
baselines on four benchmarks: WikiText (Merity
et al., 2016), HumanEval (Chen et al., 2021), Al-
pacaEval (Li et al., 2023b), and GSM8k (Cobbe
et al., 2021). The four benchmarks span diverse
language tasks of open-ended generation, code
generation, human alignment, and mathematical
reasoning respectively. For WikiText, we use
the pre-trained Llama2 7Band Llama2 70B(Touvron
et al., 2023) as MaandMeand follow Li et al.
(2023a) to use diversity, MAUVE (Pillutla et al.,
2021) and coherence as evaluation metrics. ForWikiText A.Eval GSM8k H.Eval
Div. MAU. Coh. Score Acc. Pass@1
Ma 0.69.000.88.010.76.0088.791.141.77.0011.59.0
Me 0.75.000.88.010.75.0094.66.7964.19.0428.66.0
SD 0.75.000.90.010.75.0194.28.8364.27.0728.66.0
CD ori0.91.000.95.000.73.0094.56.8264.42.0337.20.0
SCD ori0.91.000.94.000.72.0194.91.7864.44.0637.20.0
E.A. ori Ã—1.78 Ã—2.92Ã—3.32Ã—3.01
CD imp0.73.010.90.010.74.0094.78.7964.91.0133.54.0
SCD imp0.73.000.91.010.74.0095.03.7764.90.0233.54.0
E.A. imp Ã—2.10 Ã—2.95Ã—3.32Ã—3.18
Table 1: Main results of SCD. H.Eval, and A.Eval
are shorts for HumanEval and AlpacaEval. MAU. and
Coh. are shorts for MAUVE and coherence. E.A.
presents the expected acceleration under c= 0.05. The
standard errors under 3 repetitions for each result are
marked in subscripts. The best choices of Î±andÎ²for
(S)CD are left to Appx. Â§A.3.
HumanEval, we use the pre-trained Llama2 7Band
Llama2 70Band assess the 1-round pass rate. For
AlpacaEval, we use human-aligned Llama2chat 7B
and Llama2chat 70Band report win-rates over text-
davinci-003 judged by GPT-4. For GSM8k, we
use fine-tuned Llama2 7Band Llama2 70Bon its
training set and report the accuracy of the test-
set results. We set Î³= 4 across all experi-
ments and set the temperature Ï„to 0.7 for Wiki-
Text and AlpacaEval and 0.001 for GSM8k and
HumanEval. We leave the detailed experiment set-
tings to Appx. Â§A.
Quality Results. As shown in Tab. 1, original
and improved SCD and CD demonstrate signifi-
cant improvement over Mein GSM8k and Hu-
manEval. On WikiText, only original CD and
SCD outperform Mein terms of diversity with
+0.16and MAUVE with +0.06. There is no ob-
vious improvement in Coherence. On AlpacaE-
val, although both versions of SCD and CD show
better results than Me, such improvement is not
significant due to the high variance of GPT4-as-a-
judge. We can see that different versions of SCD
suggest different levels of improvement. Original
SCD performs better on WikiText and HumanEval
while inferior on GSM8k to improved SCD. Re-
sults across four benchmarks show SCD can bene-
fit various LLMs on diverse language tasks, main-
taining the same generation quality improvement
as CD.
Acceleration. To demonstrate the inference accel-
eration of SCD, we primarily provide the expected
acceleration factor of SCD theoretically with re-

--- PAGE 4 ---
Figure 1: Hyper-parameter analysis on expected acceleration factors regarding empirical acceptance rate Î». The
best hyper-parameter settings as in Tab. 1 are the lines marked with triangles.
Figure 2: The averaged token distribution entropy with error bars of rejected and accepted tokens in SCD.
spect to the number of Matoken predictions per
iteration Î³, the acceptance rate Î», and the cost co-
efficient c, which proof is left to Appx. Â§B.
Theorem 5.1. The expected acceleration factor in
decoding runtime is1âˆ’Î»Î³+1
(1âˆ’Î»)(1+cÎ³+cÎ»Î³).
In Tab. 1, consistent acceleration is presented
across different benchmarks. We further visual-
ize the expected acceleration factor of SCD in
Fig. 1 according to the empirical acceptance rates
Î»in HumanEval with different hyper-parameter
settings. According to Theorem 5.1, the accel-
eration factors are depicted against the cost co-
efficient c, which is usually of small values rep-
resenting the ratio of runtime required of Ma
andMeand depends on the infrastructures (e.g.,
GPU) that serve the LLMs. We can see that the
acceptance rates hence the corresponding accel-
eration factors of original SCD are more sensi-
tive to hyper-parameters compared to improved
SCD. With proper hyper-parameters, SCD can
achieve similar acceleration to the speculative de-
coding (dotted lines), which indicates the negligi-
ble speed trade-off to incorporate the contrastive
token distributions. Results on GSM8k are listed
in Appx. Â§D presenting similar patterns.
6 Analysis
Compatibility. Results presented in Â§5 show SCD
can combine the benefits of CD and SD. We delve
deep into the reasons for such compatibility. We
calculate the average entropy of token probabili-
ties from MaandMeregarding the accepted and
Figure 3: Performance sensitivity regarding Î±andÎ².
rejected tokens in SCD. As shown in Fig. 2, to-
ken distribution entropy from both MaandMe
of accepted tokens is significantly higher than that
of rejected tokens. The phenomenon suggests
SCD enjoys acceleration from accepting easy to-
kens of lower entropy while benefiting from con-
trastive token distribution by rejecting hard tokens
of higher entropy. We also present a case study
from GSM8k in Appx. Â§C to demonstrate such
compatibility.
Sensitivity. Through Fig. 3, we show how per-
formances fluctuate with respect to the hyper-
parameter Î±andÎ². We can see that improved
SCD is less sensitive to both Î±andÎ²on GSM8k
compared to the original SCD. This is possibly due
to the better flexibility of manipulating logits than
probabilities. Results on HumanEval are listed in
Appx. Â§D presenting similar phenomenons.
7 Conclusion
In this paper, we propose speculative contrastive
decoding, a decoding strategy that naturally inte-
grates small amateur LMs for inference acceler-
ation and quality improvement of LLMs. Exten-
sive experiments show the effectiveness of SCD

--- PAGE 5 ---
and our delve-deep analysis also explains the com-
patibility through the scope of token distribution
entropy. Our method can be easily deployed to
improve the real-world serving of LLMs.
Limitation
In our experiments, we provide the expected accel-
eration factors of SCD on four benchmarks calcu-
lated according to the empirical token acceptance
rates Î»and selected cost coefficients c. The em-
pirical acceleration factor is highly correlated to
the actual infrastructures that serve both the larger
LMs and the smaller LMs. To compensate for this
demonstration limitation and better demonstrate
the acceleration performance, we visualize the ex-
pected acceleration factor by spanning across a
range of cin Fig. 1. This is a common limitation of
deploying speculative decoding in the real-world
LLM serving. For example, the runtime of switch-
ing between the forward computation of Maand
Mewould be non-negligible without properly op-
timized infrastructures, causing a relatively large
chence potentially resulting in deceleration even
with high acceptance rates.
Broader Impact
Although LLMs have demonstrated exceptional
performance and been helpful real-world assis-
tants recently, the massive computational demands
of LLMs forbid most users including potential re-
searchers from local deployments, who generally
alter to use APIs from LLM servings. Therefore,
effective methods, including our SCD, to improve
the speed and quality from the perspective of de-
coding inference have much potential to advance
LLM-based services.
References
Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and
Jackie Cheung. 2022. Why exposure bias mat-
ters: An imitation learning perspective of error ac-
cumulation in language generation. In Findings of
the Association for Computational Linguistics: ACL
2022 , pages 700â€“710, Dublin, Ireland. Association
for Computational Linguistics.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,
Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren,
Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
guang Wu, Benfeng Xu, Jin Xu, An Yang, Hao
Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen
Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
Xingxuan Zhang, Yichang Zhang, Zhenru Zhang,
Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and
Tianhang Zhu. 2023. Qwen technical report.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, T. J. Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeff Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot
learners. ArXiv , abs/2005.14165.
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu
Peng, and Tri Dao. 2023. Medusa: Simple
framework for accelerating llm generation with
multiple decoding heads. https://github.com/
FasterDecoding/Medusa .
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving,
Jean-Baptiste Lespiau, Laurent Sifre, and John
Jumper. 2023. Accelerating large language model
decoding with speculative sampling.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al. 2021. Evaluating large lan-
guage models trained on code. arXiv preprint
arXiv:2107.03374 .
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon
Kim, James Glass, and Pengcheng He. 2023. Dola:
Decoding by contrasting layers improves factu-
ality in large language models. arXiv preprint
arXiv:2309.03883 .
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Matthew Finlayson, John Hewitt, Alexander Koller,
Swabha Swayamdipta, and Ashish Sabharwal. 2023.
Closing the curious case of neural text degeneration.
Mingqi Gao and Xiaojun Wan. 2022. DialSummEval:
Revisiting summarization evaluation for dialogues.
InProceedings of the 2022 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 5693â€“5709, Seattle, United States. Associa-
tion for Computational Linguistics.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence

--- PAGE 6 ---
embeddings. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language
Processing , pages 6894â€“6910, Online and Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text de-
generation. In International Conference on Learn-
ing Representations .
Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Ji-
tendra Malik, Michael W. Mahoney, Amir Gholami,
and Kurt Keutzer. 2023. Speculative decoding with
big little decoder.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-
cient memory management for large language model
serving with pagedattention. In Proceedings of the
ACM SIGOPS 29th Symposium on Operating Sys-
tems Principles .
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
2022. Fast inference from transformers via specula-
tive decoding. In International Conference on Ma-
chine Learning .
Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy
Liang, Jason Eisner, Tatsunori Hashimoto, Luke
Zettlemoyer, and Mike Lewis. 2023a. Contrastive
decoding: Open-ended text generation as optimiza-
tion. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 12286â€“12312, Toronto,
Canada. Association for Computational Linguistics.
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B. Hashimoto. 2023b. Alpacaeval: An au-
tomatic evaluator of instruction-following models.
https://github.com/tatsu-lab/alpaca_eval .
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun-
yang Lin, Chuanqi Tan, Chang Zhou, and Jingren
Zhou. 2023. #instag: Instruction tagging for analyz-
ing supervised fine-tuning of large language models.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843 .
Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao
Cheng, Zeyu Wang, Rae Ying Yee Wong, Zhuoming
Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhi-
hao Jia. 2023. Specinfer: Accelerating generative
llm serving with speculative inference and token tree
verification. arXiv preprint arXiv:2305.09781 .
Sean Oâ€™Brien and Mike Lewis. 2023. Contrastive de-
coding improves reasoning in large language mod-
els.Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions
with human feedback.
Krishna Pillutla, Swabha Swayamdipta, Rowan
Zellers, John Thickstun, Sean Welleck, Yejin Choi,
and Zaid Harchaoui. 2021. MAUVE: Measuring the
gap between neural text and human text using diver-
gence frontiers. In Advances in Neural Information
Processing Systems .
Benjamin Spector and Chris Re. 2023. Accelerat-
ing llm inference with staged speculative decoding.
arXiv preprint arXiv:2308.04623 .
Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-
peng Kong, and Nigel Collier. 2022. A contrastive
framework for neural text generation.
Yixuan Su and Jialu Xu. 2022. An empirical
study on contrastive search and contrastive decod-
ing for open-ended text generation. arXiv preprint
arXiv:2211.10797 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony Hartshorn, Saghar Hos-
seini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel Kloumann, Artem
Korenev, Punit Singh Koura, Marie-Anne Lachaux,
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, Ranjan Subramanian, Xiaoqing Ellen Tan,
Binh Tang, Ross Taylor, Adina Williams, Jian Xi-
ang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,
Yuchen Zhang, Angela Fan, Melanie Kambadur,
Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. 2023. Llama
2: Open foundation and fine-tuned chat models.
Ashwin K Vijayakumar, Michael Cogswell, Ram-
prasath R. Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. 2018. Diverse beam
search: Decoding diverse solutions from neural se-
quence models.

--- PAGE 7 ---
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le,
and Denny Zhou. 2023. Chain-of-thought prompt-
ing elicits reasoning in large language models.
Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,
Xiaoxia Wu, Conglong Li, and Yuxiong He. 2022.
Zeroquant: Efficient and affordable post-training
quantization for large-scale transformers. Ad-
vances in Neural Information Processing Systems ,
35:27168â€“27183.
Gal Yona, Or Honovich, Itay Laish, and Roee Aha-
roni. 2023. Surfacing biases in large language mod-
els using contrastive input decoding. arXiv preprint
arXiv:2305.07378 .
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting
Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and
Jingren Zhou. 2023. Scaling relationship on learn-
ing mathematical reasoning with large language
models.
A Experiment Details
A.1 Benchmark Details
(1)WikiText (Merity et al., 2016) contains articles
from Wikipedia. We follow the pre-processing
scripts from Li et al. (2023a) and result in 1,733
samples. The generation starts with the first 32 to-
kens as prompts, and the max generation length is
set to 256. We report diversity, MAUVE (Pillutla
et al., 2021), and coherence as metrics, following
Li et al. (2023a).
Diversity metrics assess the unique multi-grams
in the completion generated from the LMs. Higher
diversity scores indicate better lexical diversity in
the completion. The diversity is calculated accord-
ing to:
Div.=4Y
n=2|Set(n-grams )|
|n-grams |.
MAUVE is a metric proposed by Pillutla et al.
(2021), which is empirically suggested to have
better agreement with human annotations (Gao
and Wan, 2022). Coherence evaluates the se-
mantic correlation between the input prefix and
the output generation via the similarity of embed-
dings. We use the sentence embeddings follow-
ing SimCSE (Gao et al., 2021) and the coherence
score is calculated as:
emb(xprefix)Â·emb(xgen)
âˆ¥emb(xprefix)âˆ¥âˆ¥emb(xgen)âˆ¥.
(2)GSM8k (Cobbe et al., 2021) contains train-
ing and evaluation sets of grade mathematical rea-
soning problems. We first fine-tune the Llama2 7Band Llama2 70Bby 3 epochs to produce the ama-
teur and expert LMs. We report the final accuracy
of the test sets.
(3)HumanEval (Chen et al., 2021) mea-
sures coding correctness for synthesizing pro-
grams from 164 doc-strings. We report the 1-
round pass rate (Pass@1).
(4)AlpacaEval (Li et al., 2023b) contains 805
samples from various evaluation sets to evaluate
the alignment abilities of LLMs by comparing
evaluated models with text-davinci-003 . We report
the win rate judged by GPT-4.
A.2 Configuration Details
We use Llama2 7Bas the amateur model while
Llama2 70Bas the expert model on WikiText and
HumanEval benchmarks to evaluate how SCD
performs with pre-trained models. Then, we fine-
tune Llama2 7Band Llama2 70Bon the GSM8k
training set to evaluate the SCD performance with
supervised fine-tuning models on the mathemat-
ical reasoning task. We also apply Llama2chat 7B
and Llama2chat 70Bon AlpacaEval to assess LLMs
for human alignment using SCD. We set the soft-
max temperature consistent to 0.7 on WikiText and
AlpacaEval while 0.001 on other benchmarks. In
SCD and SD, we always set the prediction tem-
perature from the amateur LMs to 1.0 for fair com-
parison. All experiments are conducted on 2 A100
80G GPUs with KV cache implementation.
A.3 Hyper-parameter Details
We conduct grid searches regarding Î±andÎ²for
the best performance of CD and SCD. The best
hyper-parameter settings for the results in Tab. 1
are listed in Tab. 2.
B Proof of Theorem Theorem 5.1
Theorem B.1. The expected acceleration factor in
decoding runtime is1âˆ’Î»Î³+1
(1âˆ’Î»)(1+cÎ³+cÎ»Î³).
Proof. Similar to Theorem 3.8 in Leviathan et al.
(2022), given the token acceptance rate Î»and the
runtime per forward computation step for Meand
MaareTandcT. The total runtime required
for each iteration is T+cÎ³T +cÎ»Î³T, where
Marequires Î³generation steps and possibly one
additional step forward computation if all Î³to-
kens are accepted while Marequires one forward
computation for token validity checking. Follow-
ing Equation (1) in Leviathan et al. (2022), the
expected generated token number per iteration is

--- PAGE 8 ---
WikiText AlpacaEval GSM8k HumanEval
Î± Î² Î± Î² Î± Î² Î± Î²
CD ori 0.1 - 0.5 - 0.5 - 0.5 -
SCD ori 0.1 - 0.5 - 0.5 - 0.5 -
CD ori 0.1 0.5 0.5 0.5 0.5 1.0 0.5 0.5
SCD ori 0.1 0.5 0.5 0.5 0.5 1.0 0.5 0.5
Table 2: The hyper-parameter settings for the results in Tab. 1
1âˆ’Î»Î³+1
1âˆ’Î». Therefore, the expected runtime needed
of SCD is1âˆ’Î»
1âˆ’Î»Î³+1(T+cÎ³T+cÎ»Î³T), hence the ex-
pected acceleration factor is1âˆ’Î»Î³+1
(1âˆ’Î»)(1+cÎ³+cÎ»Î³).
C Case Study
Case Study on GSM8k
Judy teaches 5 dance classes, every day, on the
weekdays and 8 classes on Saturday. If each class
has 15 students and she charges $15.00 per student,
how much money does she make in 1 week?
Responses
[RED and BLUE tokens are generated by the expert
LM after rejection and all acceptance, respectively.
Other tokens are generated by the amateur LM.]
She teaches 5 classes every weekday so thatâ€™s
5*5 = Â«5*5=25Â»25 classes
She teaches 25 weekday classes and 8 Saturday
classes for a total of 25+8 = Â«25+8=33Â»33 classes
Each class has 15 students so she teaches 15*33 =
Â«15*33=495Â»495 students in 1 week
She charges $15.00 per student and she
teaches 495 students so she makes 15*495 =
$Â«15*495=7425.00Â»7,425.00 in 1 week
In this case, we can see that the rejected and
re-sampled tokens are usually the beginning of a
sentence, numbers, operations, or named entities,
which are generally informative tokens in the rea-
soning chain of thoughts. This also indicates that
quality improvement originates from re-sampling
informative tokens by contrastive token distribu-
tion while the acceleration comes from speculative
prediction of the amateur LMs.
D Additional Results

--- PAGE 9 ---
Figure 4: Hyper-parameter analysis on expected acceleration factors regarding empirical acceptance rate Î». The
best hyper-parameter settings as in Tab. 1 are the lines marked with triangles.
Figure 5: Performance sensitivity regarding Î±andÎ².
