# Cải thiện Trích xuất Quan hệ Liên tục thông qua Học tập Đối chiếu Nguyên mẫu

Chengwei Hu1y, Deqing Yang1y , Haoliang Jin1z, Zhen Chen1z, Yanghua Xiao2y
1Trường Khoa học Dữ liệu, Đại học Fudan, Thượng Hải, Trung Quốc
2Trường Khoa học Máy tính, Đại học Fudan, Thượng Hải, Trung Quốc
y{cwhu20, yangdeqing, shawyh}@fudan.edu.cn
z{hljin21, zhenchen21}@m.fudan.edu.cn

## Tóm tắt

Trích xuất quan hệ liên tục (CRE) nhằm trích xuất các quan hệ đối với sự xuất hiện liên tục và lặp đi lặp lại của dữ liệu mới, trong đó thách thức chính là hiện tượng quên thảm khốc của các tác vụ cũ. Để giảm thiểu vấn đề quan trọng này nhằm cải thiện hiệu suất CRE, chúng tôi đề xuất một khung Trích xuất Quan hệ Liên tục mới với Học tập Đối chiếu, gọi là CRECL, được xây dựng với mạng phân loại và mạng đối chiếu nguyên mẫu để đạt được học tập lớp tăng dần của CRE. Cụ thể, trong mạng đối chiếu, một thể hiện được đưa ra sẽ được đối chiếu với nguyên mẫu của mỗi quan hệ ứng viên được lưu trữ trong mô-đun bộ nhớ. Cơ chế học tập đối chiếu như vậy đảm bảo phân phối dữ liệu của tất cả các tác vụ có thể phân biệt được nhiều hơn, do đó giảm thiểu thêm hiện tượng quên thảm khốc. Kết quả thí nghiệm của chúng tôi không chỉ chứng minh lợi thế của CRECL so với các baseline hiện đại trên hai bộ dữ liệu công khai, mà còn xác minh hiệu quả của học tập đối chiếu của CRECL trong việc cải thiện hiệu suất CRE.

## 1 Giới thiệu

Trong một số tình huống của trích xuất quan hệ (RE), dữ liệu mới khổng lồ bao gồm các quan hệ mới xuất hiện liên tục, điều này không thể được giải quyết bằng các phương pháp RE truyền thống. Để xử lý tình huống như vậy, trích xuất quan hệ liên tục (CRE) (Wang et al., 2019) đã được đề xuất. Do tài nguyên lưu trữ và tính toán hạn chế, việc lưu trữ tất cả dữ liệu huấn luyện của các tác vụ trước đó là không thực tế. Khi các tác vụ mới được học với các quan hệ mới xuất hiện liên tục, mô hình có xu hướng quên kiến thức hiện có về các quan hệ cũ. Do đó, vấn đề quên thảm khốc làm hỏng nghiêm trọng hiệu suất CRE (Hassabis et al., 2017; Thrun and Mitchell, 1995).

Trong những năm gần đây, một số nỗ lực đã tập trung vào việc giảm thiểu hiện tượng quên thảm khốc trong CRE, có thể được chia thành các phương pháp dựa trên củng cố (Kirkpatrick et al., 2017), phương pháp kiến trúc động (Chen et al., 2015; Fernando et al., 2017) và phương pháp dựa trên bộ nhớ (Chaudhry et al., 2018; Han et al., 2020; Cui et al., 2021). Mặc dù hiệu quả của các phương pháp này trên CRE, hầu hết chúng chưa tận dụng hết thông tin quan hệ tiêu cực trong tất cả các tác vụ để giảm thiểu hiện tượng quên thảm khốc một cách triệt để hơn, dẫn đến hiệu suất CRE chưa tối ưu.

Thông qua các nghiên cứu thực nghiệm, chúng tôi phát hiện ra rằng hiện tượng quên thảm khốc của một mô hình dẫn đến việc không thể phân biệt được giữa phân phối dữ liệu (thể hiện) của tất cả các tác vụ, khiến việc phân biệt các quan hệ của tất cả các tác vụ trở nên khó khăn. Chúng tôi minh họa điều này bằng bản đồ phân phối dữ liệu sau khi huấn luyện mô hình phân loại quan hệ cho một tác vụ mới, như được hiển thị trong Hình 1 trong đó các dấu chấm và dấu thập biểu thị dữ liệu của tác vụ cũ và mới tương ứng, và các màu khác nhau biểu thị các quan hệ khác nhau. Nó cho thấy rằng các điểm dữ liệu của các màu khác nhau trong nhóm dấu chấm (tác vụ cũ) hoặc nhóm dấu thập (tác vụ mới) đều có thể phân biệt được. Tuy nhiên, nhiều dấu chấm và dấu thập bị trộn lẫn, khiến việc phân biệt các quan hệ của tác vụ mới với các quan hệ của tác vụ cũ trở nên khó khăn. Do đó, làm cho phân phối dữ liệu của tất cả các tác vụ có thể phân biệt được nhiều hơn là điều quan trọng để đạt được CRE tốt hơn.

Để giải quyết vấn đề trên, trong bài báo này chúng tôi đề xuất một khung Trích xuất Quan hệ Liên tục mới với Học tập Đối chiếu, gọi là CRECL, được xây dựng với mạng phân loại và mạng đối chiếu. Để tận dụng đầy đủ thông tin của các quan hệ tiêu cực nhằm làm cho phân phối dữ liệu của tất cả các tác vụ có thể phân biệt được nhiều hơn, chúng tôi thiết kế một cơ chế học tập đối chiếu nguyên mẫu. Cụ thể, trong mạng đối chiếu của CRECL, một thể hiện được đưa ra sẽ được đối chiếu với nguyên mẫu của mỗi quan hệ ứng viên được lưu trữ trong mô-đun bộ nhớ. Những so sánh đầy đủ như vậy đảm bảo sự đồng bộ và đồng nhất giữa phân phối dữ liệu của các tác vụ cũ và mới. Do đó, hiện tượng quên thảm khốc trong CRECL được giảm thiểu một cách triệt để hơn, dẫn đến hiệu suất CRE được cải thiện. Ngoài ra, khác với phân loại cho một tập lớp (quan hệ) cố định như (Han et al., 2020; Cui et al., 2021), CRECL đạt được học tập lớp tăng dần của CRE, điều này khả thi hơn đối với các tình huống CRE trong thế giới thực.

Đóng góp của chúng tôi trong bài báo này được tóm tắt như sau:

1. Chúng tôi đề xuất một khung CRE mới CRECL kết hợp mạng phân loại và mạng đối chiếu nguyên mẫu để giảm thiểu hoàn toàn vấn đề quên thảm khốc.

2. Với cơ chế dựa trên đối chiếu, CRECL của chúng tôi có thể hiệu quả đạt được học tập tăng dần lớp, điều này thực tế hơn trong các tình huống CRE thế giới thực.

3. Các thí nghiệm rộng rãi của chúng tôi chứng minh lợi thế của CRECL so với các mô hình hiện đại (SOTA) trên hai bộ dữ liệu chuẩn, TACRED và FewRel. Hơn nữa, chúng tôi cung cấp những hiểu biết sâu sắc về lý do của hiệu suất khác biệt của các mô hình được so sánh.

## 2 Công trình liên quan

Trong phần này, chúng tôi giới thiệu ngắn gọn về học tập liên tục và học tập đối chiếu, cả hai đều liên quan đến công trình của chúng tôi.

Học tập liên tục (Delange et al., 2021; Parisi et al., 2019) tập trung vào việc học từ luồng dữ liệu liên tục. Các mô hình học tập liên tục có thể tích lũy kiến thức qua các tác vụ khác nhau mà không cần huấn luyện lại từ đầu. Thách thức chính trong học tập liên tục là giảm thiểu hiện tượng quên thảm khốc, đề cập đến việc hiệu suất trên các tác vụ trước đó không nên giảm đáng kể theo thời gian khi các tác vụ mới xuất hiện.

Để khắc phục hiện tượng quên thảm khốc, hầu hết các công trình gần đây có thể được chia thành ba danh mục.

1) Các phương pháp dựa trên chính quy hóa áp đặt ràng buộc lên việc cập nhật tham số. Ví dụ, phương pháp LwF (Li và Hoiem, 2016) buộc mạng của các tác vụ đã học trước đó phải tương tự với mạng của tác vụ hiện tại thông qua chưng cất kiến thức. Tuy nhiên, LwF phụ thuộc nhiều vào dữ liệu trong tác vụ mới và mối liên hệ của nó với các tác vụ trước đó. EWC (Kirkpatrick et al., 2016) áp dụng phạt bậc hai trên sự khác biệt giữa các tham số cho các tác vụ cũ và mới. Nó mô hình hóa tính liên quan của tham số đối với dữ liệu huấn luyện như một phân phối hậu nghiệm, được ước tính bằng xấp xỉ Laplace với độ chính xác được xác định bởi Ma trận Thông tin Fisher. WA (Zhao et al., 2020) duy trì sự phân biệt và công bằng giữa tác vụ mới và cũ bằng cách điều chỉnh các tham số của lớp cuối cùng.

2) Các phương pháp kiến trúc động thay đổi tính chất kiến trúc của mô hình khi có dữ liệu mới bằng cách động thích ứng các tài nguyên mạng thần kinh mới, như tăng số lượng nơ-ron. Ví dụ, PackNet (Mallya và Lazebnik, 2017) lặp đi lặp lại gán các tập con tham số cho các tác vụ liên tiếp bằng cách tạo ra các mặt nạ cắt tỉa, cố định tập con tham số tác vụ cho các tác vụ tương lai. DER (Yan et al., 2021) đề xuất một phương pháp học hai giai đoạn mới để có được biểu diễn có thể mở rộng động hiệu quả hơn.

3) Các phương pháp dựa trên bộ nhớ huấn luyện lại một cách rõ ràng các mô hình trên một tập con hạn chế của các mẫu được lưu trữ trong quá trình huấn luyện trên các tác vụ mới. Ví dụ, iCaRL (Rebuffi et al., 2017) tập trung vào việc học theo cách tăng dần lớp, chọn và lưu trữ các mẫu gần nhất với trung bình đặc trưng của mỗi lớp. Trong quá trình huấn luyện, mất mát chưng cất giữa các mục tiêu thu được từ các dự đoán mô hình trước đó và hiện tại được thêm vào mất mát tổng thể, để bảo tồn kiến thức đã học trước đó. RP-CRE (Cui et al., 2021) giới thiệu một mô-đun bộ nhớ dựa trên chú ý có thể cắm mới để tự động tính toán trọng số của các tác vụ cũ khi học các tác vụ mới.

Vì các phương pháp dựa trên phân loại yêu cầu lược đồ quan hệ trong lớp phân loại, các mô hình dựa trên phân loại có một nhược điểm không thể bỏ qua trong học tập tăng dần lớp. Nhiều nhà nghiên cứu tận dụng học tập metric để giải quyết vấn đề này. (Wang et al., 2019; Wu et al., 2021) sử dụng mô hình đồng bộ câu dựa trên Mất mát Xếp hạng Biên (Nayyeri et al., 2019), nhưng thiếu khả năng nội tại để thực hiện khai thác tích cực/tiêu cực khó, dẫn đến hiệu suất kém. Gần đây, học tập đối chiếu đã được nhập rộng rãi vào các khung học tập tự giám sát trong nhiều lĩnh vực bao gồm thị giác máy tính, xử lý ngôn ngữ tự nhiên và vv. Học tập đối chiếu là một cơ chế phân biệt nhằm nhóm các mẫu tương tự gần nhau hơn và các mẫu đa dạng xa nhau. (Wang và Liu, 2021) chứng minh rằng học tập đối chiếu có thể thúc đẩy sự đồng bộ và ổn định của phân phối dữ liệu, và (Khosla et al., 2020) xác minh rằng sử dụng các phương pháp đối chiếu batch hiện đại, như mất mát InfoNCE (Oord et al., 2018), vượt trội hơn các mất mát đối chiếu truyền thống, như mất mát xếp hạng biên, và cũng đạt được kết quả tốt trong các tác vụ học tập đối chiếu có giám sát.

## 3 Phương pháp

### 3.1 Hình thức hóa Tác vụ

Tác vụ CRE nhằm xác định quan hệ giữa hai thực thể được biểu thị bởi một câu trong chuỗi tác vụ. Chính thức, cho một chuỗi K tác vụ {T1, T2, ..., TK}, giả sử Dk và Rk biểu thị tập thể hiện và tập lớp quan hệ của tác vụ thứ k Tk, tương ứng. Dk chứa Nk thể hiện {(x1, t1, y1), ..., (xNk, tNk, yNk)} trong đó thể hiện (xi, ti, yi), 1 ≤ i ≤ Nk biểu thị rằng quan hệ của cặp thực thể ti trong câu xi là yi ∈ Rk. Một mô hình CRE nên hoạt động tốt trên tất cả các tác vụ lịch sử lên đến Tk, ký hiệu là T̃k = ∪ᵢ₌₁ᵏ Ti, trong đó tập lớp quan hệ là R̃k = ∪ᵢ₌₁ᵏ Ri. Chúng tôi cũng áp dụng một mô-đun bộ nhớ episo­dic Mr = {(x1, t1, r), ..., (xL, tL, r)} để lưu trữ các thể hiện điển hình của quan hệ r, tương tự như (Han et al., 2020; Cui et al., 2021), trong đó L là kích thước bộ nhớ (số lượng thể hiện điển hình). Bộ nhớ episo­dic tổng thể cho các quan hệ quan sát được trong tất cả các tác vụ là M̃k = ∪r∈R̃k Mr.

### 3.2 Tổng quan Khung

Cấu trúc tổng thể của CRECL được mô tả trong Hình 2, có hai thành phần chính, tức là mạng phân loại và mạng đối chiếu. Quy trình học tác vụ hiện tại trong CRECL được mô tả bởi thuật toán trong Alg. 1.

Đầu tiên, giả sử tác vụ hiện tại là Tk, biểu diễn của mỗi thể hiện trong Tk được thu thập thông qua bộ mã hóa và lớp dropout được chia sẻ bởi hai mạng. Trong mạng phân loại, quan hệ của mỗi thể hiện được dự đoán dựa trên biểu diễn của nó (dòng 1-3). Sau đó, chúng tôi áp dụng thuật toán K-means trên các biểu diễn thể hiện để chọn L thể hiện điển hình cho mỗi quan hệ trong Tk, được sử dụng để tạo ra các nguyên mẫu quan hệ và lưu trữ vào bộ nhớ M̃k cho việc đối chiếu tiếp theo (dòng 4-13). Có hai quy trình huấn luyện trong mạng đối chiếu. Thứ nhất là so sánh các thể hiện tác vụ hiện tại với các nguyên mẫu quan hệ được lưu trữ của T̃k (dòng 14-17). Thứ hai là so sánh mỗi thể hiện điển hình với tất cả các nguyên mẫu quan hệ đều được lưu trữ trong M̃k (dòng 18-24). Hai quy trình huấn luyện này đảm bảo mỗi thể hiện được so sánh giữ khoảng cách với đủ số quan hệ tiêu cực trong R̃k. Do đó, phân phối dữ liệu của R̃k có thể phân biệt đủ để giảm thiểu hiện tượng quên thảm khốc của CRECL đối với các tác vụ cũ. Tiếp theo, chúng tôi chi tiết các hoạt động trong CRECL.

### 3.3 Lớp Mã hóa Chia sẻ

Mạng phân loại và mạng đối chiếu trong CRECL được thiết kế để thúc đẩy lẫn nhau, trong đó mạng trước phân loại tác vụ hiện tại dựa trên các embedding thể hiện của nó, và mạng sau điều chỉnh hiệu quả các embedding thể hiện để duy trì tính đồng nhất và đồng bộ. Theo nguyên tắc này, hai mạng chia sẻ cùng các lớp trong CRECL.

Cụ thể, đối với một thể hiện i của tác vụ hiện tại Tk, chúng tôi sử dụng các token đặc biệt để biểu thị các thực thể trong i như (Cui et al., 2021). Như được hiển thị trong Hình 2, thực thể đầu và thực thể đuôi trong i được biểu thị bởi hai token vị trí đặc biệt [E11, E12] và [E21, E22], tương ứng. Embedding của thể hiện i trước lớp dropout, ký hiệu là ei ∈ R²ʰ, là sự nối của các token embedding của [E11, E12] và [E21, E22] được tạo ra bởi BERT (Devlin et al., 2019) trong đó h là chiều của hai token embedding. Sau đó, ei được đưa vào lớp dropout để thu được hidden embedding của i như

hi = WDropout(ei) + b ∈ Rᵈ, (1)

trong đó W ∈ Rᵈˣ²ʰ (d là chiều của lớp ẩn) và b ∈ Rᵈ đều là các tham số có thể huấn luyện. Trong CRECL, hi được coi là biểu diễn của i.

### 3.4 Phân loại Tác vụ Hiện tại

Với biểu diễn hi của thể hiện i, phân phối xác suất của i ký hiệu là Pi ∈ R|Rk|, được tính toán trong mạng phân loại như

Pi = softmax(W1LN(GELU(hi)) + b1), (2)

trong đó W1 ∈ R|Rk|×d, b1 ∈ R|Rk| là các tham số có thể huấn luyện, và |Rk| là số lượng quan hệ của tác vụ hiện tại Tk, nhỏ hơn nhiều so với số lượng quan hệ của tất cả các tác vụ. LN(·) là phép toán chuẩn hóa lớp. Sau đó, mất mát phân loại cho tác vụ hiện tại Tk được tính toán như

L1 = -1/Nk ∑ᵢ₌₁ᴺᵏ ∑ᵣ₌₁|Rk| yi,r log Pᵢʳ, (3)

trong đó yi,r = 1 nếu nhãn quan hệ thực của i là r, ngược lại yi,r = 0. Pᵢʳ là mục thứ r trong Pi.

### 3.5 Tạo ra Nguyên mẫu Quan hệ

Sau khi học tác vụ hiện tại, cho mỗi quan hệ r trong tác vụ hiện tại, chúng tôi trước tiên áp dụng thuật toán K-means trên các biểu diễn (hi) của tất cả các thể hiện thuộc về r để phân cụm chúng thành L cụm. Sau đó, cho mỗi cụm, chúng tôi chọn thể hiện gần nhất với tâm của cụm này làm một thể hiện điển hình. Do đó, L thể hiện điển hình của quan hệ r được chọn và sau đó lưu trữ vào mô-đun bộ nhớ. Với các thể hiện điển hình được lưu trữ của r, chúng tôi lấy trung bình các biểu diễn của chúng làm nguyên mẫu pr của r, tức là

pr = 1/L ∑ᵢ₌₁ᴸ hᵢʳ, (4)

trong đó hᵢʳ là biểu diễn của thể hiện điển hình i của quan hệ r. Nguyên mẫu như vậy biểu thị tốt nhất r vì L thể hiện điển hình có tổng khoảng cách nhỏ nhất đến L tâm cụm. Một ưu điểm khác của các nguyên mẫu như vậy để biểu thị quan hệ là tính không nhạy cảm của chúng với giá trị của L.

### 3.6 Mạng Đối chiếu

Trong mạng đối chiếu này, các thể hiện được so sánh với các nguyên mẫu quan hệ được lưu trữ trong mô-đun bộ nhớ để tinh chỉnh phân phối dữ liệu của tất cả các tác vụ, do đó giảm thiểu hiện tượng quên thảm khốc của CRECL. Nguyên tắc cơ bản của nó là biểu diễn của một thể hiện nên gần với nguyên mẫu của quan hệ (tích cực) của nó, và xa với các nguyên mẫu của các quan hệ còn lại (tiêu cực). Xin lưu ý rằng, các quan hệ tích cực và tiêu cực được xác định bởi các nhãn thực của các thể hiện huấn luyện. Do đó nó khác với học tập đối chiếu tự giám sát trong các mô hình khác (Chen et al., 2020).

**Mục tiêu Học tập Đối chiếu** Như được hiển thị trong phần bên phải của Hình 2, mạng đối chiếu được xây dựng với kiến trúc tháp đôi. Trong tháp bên trái, cho một quan hệ r, các nguyên mẫu pr ∈ Rᵈ của nó được thu thập bởi Eq. 4. Sau đó, embedding so sánh của r được ký hiệu là sr ∈ Rᵈ/² và được tính toán như

sr = W3GELU(W2pr + b2) + b3, (5)

trong đó W2 ∈ Rᵈˣᵈ, b2 ∈ Rᵈ, W3 ∈ Rᵈ/²ˣᵈ, b3 ∈ Rᵈ/² đều là các tham số có thể huấn luyện.

Trong tháp bên phải, cho một thể hiện so sánh i, embedding so sánh của nó được ký hiệu là si ∈ Rᵈ/² và thu được bằng cùng phép toán trong Eq. 5 trong đó chỉ pr được thay thế bởi hi từ Eq. 1.

Cho mỗi thể hiện i trong tác vụ hiện tại Tk, giả sử embedding so sánh của quan hệ yi của i là syi cũng có thể được thu thập bởi Eq. 5, vì các thể hiện điển hình của yi đã được lưu trữ trong M̃k trước đó. Chúng tôi áp dụng chuẩn Euclidean cho si và syi. Sau đó, chúng tôi sử dụng mất mát InfoNCE của học tập đối chiếu (Oord et al., 2018) để tính toán mất mát độ tương tự cosine của Tk như

Lcos = -1/Nk ∑ᵢ₌₁ᴺᵏ log(exp(si·syi/τ) / ∑r∈R̃k exp(si·sr/τ)), (6)

trong đó τ là siêu tham số nhiệt độ.

Để tăng khoảng cách điểm tương tự của nhãn đúng và nhãn sai gần nhất, được truyền cảm hứng bởi (Koch et al., 2015), chúng tôi đề xuất mất mát biên đối chiếu

Lmag = 1/Nk ∑ᵢ₌₁ᴺᵏ max(m - si·syi + si·ski, 0), (7)

trong đó quan hệ ki = arg maxk∈R̃k si·sk s.t. k ≠ yi, tức là nhãn quan hệ tiêu cực gần nhất của i. Mất mát biên phạt rằng khoảng cách tương tự nhỏ hơn m.

Cuối cùng, tổng mất mát được định nghĩa là

L2 = λ1Lcos + (1-λ1)Lmag, (8)

trong đó λ1 ∈ [0,1] là tham số điều khiển.

**Quy trình Huấn luyện của Học tập Đối chiếu**

Có hai quy trình huấn luyện trong mạng đối chiếu, cả hai đều sử dụng mất mát trong Eq. 8 để làm cho các tham số mạng phù hợp hơn với tác vụ hiện tại và tất cả các tác vụ lịch sử, tương ứng.

Quy trình huấn luyện thứ nhất được tiến hành với tác vụ hiện tại Tk và bổ sung cho mạng phân loại, đây là một bước tùy chọn với epoch huấn luyện tương đối nhỏ. Tuy nhiên, nó không thể đảm bảo mô hình phù hợp với tất cả các tác vụ, vì mô hình chú ý nhiều hơn đến tác vụ hiện tại thay vì các tác vụ cũ trong quá trình huấn luyện này. Nói cách khác, như chúng tôi đã giải thích trong ví dụ của Hình 1, mô hình có xu hướng đảm bảo các thể hiện của các quan hệ khác nhau trong Tk có thể phân biệt được, nhưng quên đồng thời duy trì các thể hiện của các quan hệ khác nhau trong tất cả các tác vụ lịch sử cũng có thể phân biệt được. Kết quả là, hiện tượng quên thảm khốc của mô hình vẫn xảy ra.

Để giảm thiểu hiện tượng quên thảm khốc của CRECL một cách triệt để hơn, chúng tôi giới thiệu quy trình huấn luyện thứ hai trong mạng đối chiếu. Trong quy trình này, tất cả các thể hiện điển hình được lưu trữ trong mô-đun bộ nhớ được so sánh với tất cả các nguyên mẫu của các quan hệ được lưu trữ, bao phủ tất cả các tác vụ. Chúng tôi cũng tiến hành M lần lan truyền tiến trong lớp dropout, để tạo ra M embedding cho mỗi quan hệ cũ trong R̃k-1. Do tính ngẫu nhiên của lớp dropout, chúng tôi có thể nhận được M phân phối xác suất cho một quan hệ cũ để giảm sự mất cân bằng của phân phối dữ liệu của quan hệ cũ và mới. Theo đó, quy trình huấn luyện này có thể hiệu quả ngăn chặn mô hình khỏi hiện tượng quên thảm khốc nghiêm trọng.

### 3.7 Dự đoán Quan hệ

Cho một thể hiện dự đoán i, chúng tôi chỉ đo độ tương tự của nó với mỗi quan hệ được lưu trữ, được tính toán như khoảng cách cosine giữa biểu diễn của i và nguyên mẫu của quan hệ. Sau đó, chúng tôi chọn quan hệ tương tự nhất (gần nhất) làm nhãn lớp dự đoán của i, tức là

ŷi = arg maxr∈R̃k si·sr. (9)

## 4 Thí nghiệm

### 4.1 Bộ dữ liệu

Các thí nghiệm của chúng tôi được tiến hành trên hai bộ dữ liệu CRE chuẩn sau đây.

**FewRel** (Han et al., 2018) là một bộ dữ liệu trích xuất quan hệ phổ biến ban đầu được xây dựng cho trích xuất quan hệ few-shot. Bộ dữ liệu được chú thích bởi các công nhân đám đông và chứa 100 quan hệ và 70.000 mẫu tổng cộng. Trong các thí nghiệm của chúng tôi, để giữ nhất quán với các baseline trước đó, chúng tôi đã sử dụng phiên bản 80 quan hệ của nó.

**TACRED** (Zhang et al., 2017) là một bộ dữ liệu trích xuất quan hệ quy mô lớn chứa 42 quan hệ (bao gồm no_relation) và 106.264 mẫu từ các tài liệu tin tức và web. Dựa trên giả định quan hệ mở của CRE, chúng tôi đã loại bỏ no_relation trong các thí nghiệm của mình. Để hạn chế sự mất cân bằng mẫu của TACRED, chúng tôi giới hạn số lượng mẫu huấn luyện của mỗi quan hệ là 320, và số lượng mẫu kiểm tra của mỗi quan hệ là 40, điều này cũng nhất quán với các baseline trước đó. So với FewRel, các tác vụ trong TACRED khó khăn hơn do sự mất cân bằng quan hệ và khó khăn ngữ nghĩa của nó.

### 4.2 Các Mô hình So sánh

Chúng tôi so sánh khung của mình với các baseline sau đây trong các thí nghiệm của chúng tôi.

**EA-EMR** (Wang et al., 2019) đề xuất một mô hình đồng bộ câu với mô-đun bộ nhớ phát lại để giảm thiểu hiện tượng quên thảm khốc.

**EMAR** (Han et al., 2020) đề xuất một phương pháp phát lại bộ nhớ, kích hoạt và tái củng cố mới để giảm thiểu hiện tượng quên thảm khốc một cách hiệu quả.

**EMAR+BERT** là phiên bản nâng cao của EMAR trong đó bộ mã hóa ban đầu (Bi-LSTM) được thay thế bằng BERT.

**CML** (Wu et al., 2021) đề xuất một phương pháp học tập meta chương trình để giải quyết tính nhạy cảm thứ tự và hiện tượng quên thảm khốc trong CRE.

**RP-CRE** (Cui et al., 2021) là một mô hình CRE SOTA giới thiệu một mô-đun bộ nhớ dựa trên chú ý có thể cắm mới để tự động tính toán trọng số của các tác vụ cũ khi học các tác vụ mới.

**RP-CRE+MA** là phiên bản nâng cao của RP-CRE trong đó một bước kích hoạt bộ nhớ được thêm vào trước phép toán chú ý.

Trong CRECL của chúng tôi, chúng tôi đã áp dụng Bert-base-uncased được huấn luyện trước bởi HuggingFace (Wolf et al., 2020) làm bộ mã hóa, cũng được sử dụng trong EMAR+BERT, RP-CRE và RP-CRE+MA. Các baseline khác không thể dễ dàng được thay thế bởi BERT do kiến trúc của chúng. Ngoài ra, chúng tôi đề xuất một phiên bản khác của CRECL, gọi là CRECL+ATM, kết hợp một mô-đun bộ nhớ chú ý được đề xuất bởi (Cui et al., 2021) trong mạng đối chiếu và được sử dụng để xác minh hiệu quả của việc tinh chỉnh các nguyên mẫu quan hệ.

### 4.3 Cài đặt Thí nghiệm

Chỉ số đánh giá của chúng tôi là Độ chính xác được sử dụng phổ biến trong các baseline trước đó.

Để so sánh công bằng, chúng tôi đã tuân theo cài đặt thí nghiệm trong RP-CRE. Đầu tiên, để xác minh liệu một mô hình CRE có bị hiện tượng quên thảm khốc hay không, chúng tôi sử dụng T̄k để biểu thị tập kiểm tra của tất cả các tác vụ tích lũy lịch sử từ tác vụ đầu tiên đến tác vụ thứ k Tk (Xin lưu ý sự khác biệt giữa T̄k và Tk). Trong các nghiên cứu loại bỏ của chúng tôi, chúng tôi cũng báo cáo hiệu suất trên tập kiểm tra của tác vụ hiện tại. Để mô phỏng các tác vụ khác nhau, chúng tôi ngẫu nhiên chia tất cả các thể hiện thành 10 nhóm (tương ứng với 10 tác vụ). Thứ tự tác vụ của tất cả các mô hình so sánh hoàn toàn giống nhau để giảm tính ngẫu nhiên. Chúng tôi cũng đặt kích thước bộ nhớ trong các baseline giống với của chúng tôi. Các quan hệ đầu tiên được chia thành 10 cụm để mô phỏng 10 tác vụ. Tất cả kết quả được báo cáo của các baseline liên quan đều giống như (Cui et al., 2021). Đối với những siêu tham số đặc biệt trong các thí nghiệm của chúng tôi như sau. Kích thước batch là 32, tốc độ học được đặt thành 5e-5, τ là 0.08. Chúng tôi áp dụng 10 và 15 epoch phân loại cho TACRED và FewRel, tương ứng. Chúng tôi cũng áp dụng 10 epoch cho quy trình huấn luyện thứ nhất (cho tác vụ hiện tại) và 5 epoch cho quy trình huấn luyện thứ hai (cho tất cả các tác vụ) trong mạng đối chiếu.

Vì tổng các phép toán ma trận và lượng dữ liệu của huấn luyện thứ hai trong học tập đối chiếu rất nhỏ, thời gian huấn luyện của CRECL (1h31min) rất gần với mô hình SOTA RP-CRE (1h28min). Để tái tạo kết quả thí nghiệm của chúng tôi một cách thuận tiện, mã nguồn của CRECL cùng với các bộ dữ liệu được cung cấp tại https://github.com/PaperDiscovery/CRECL.

### 4.4 Kết quả Thí nghiệm và Phân tích

Các kết quả được báo cáo sau đây của CRECL và các biến thể loại bỏ của nó là điểm trung bình của việc chạy mô hình 5 lần.

#### 4.4.1 So sánh Hiệu suất Tổng thể

Hiệu suất tổng thể của tất cả các baseline được so sánh được báo cáo trong Bảng 1, trong đó kết quả của các baseline trực tiếp đến từ (Cui et al., 2021) và cài đặt siêu tham số của các baseline giống như các bài báo gốc của chúng. Hàng cuối cùng trong bảng là tỷ lệ cải thiện của hiệu suất CRECL so với hiệu suất baseline tốt nhất (gạch dưới). Dựa trên những kết quả này, chúng tôi có những kết luận sau:

(1) CRECL của chúng tôi vượt trội hơn mô hình SOTA RP-CRE trên cả hai bộ dữ liệu. So với FewRel, CRECL có cải thiện rõ ràng hơn so với các baseline trên TACRED. Điều này có thể do các tác vụ của FewRel không đủ khó khăn như TACRED, chứng minh rằng CRECL giỏi trong việc xử lý các tác vụ khó khăn hơn.

(2) Trong T1, CRECL của chúng tôi kém hơn RP-CRE vì mạng phân loại và đối chiếu trong CRECL chưa được huấn luyện đầy đủ ở đầu. Khi nhiều tác vụ được tích lũy, CRECL được huấn luyện đầy đủ, dẫn đến sự vượt trội của nó so với các baseline và ít sụt giảm hiệu suất hơn. Vì hiện tượng quên thảm khốc trở nên nghiêm trọng hơn trong các tình huống như vậy, kết quả ngụ ý rằng CRECL có thể giải quyết hiện tượng quên thảm khốc tốt hơn.

(3) Hiệu suất của tất cả các mô hình được so sánh đều tốt trên T1, nhưng giảm khi có nhiều tác vụ mới đến do hiện tượng quên thảm khốc nghiêm trọng hơn. So với EA-EMR, EMAR và CML, sự sụt giảm hiệu suất của các mô hình còn lại nhẹ hơn. Ví dụ, từ so sánh giữa EMAR (sử dụng Bi-LSTM) và EMAR+BERT, chúng ta có thể thấy rằng sự sụt giảm hiệu suất của EMAR+BERT chậm lại đáng kể, chứng minh rằng BERT giúp mô hình giảm thiểu hiện tượng quên thảm khốc tốt hơn. Đó là vì BERT có khả năng phân biệt đặc trưng tốt và nắm bắt tốt hơn các đặc trưng liên quan, làm cho hiện tượng quên thảm khốc ít nghiêm trọng hơn.

(4) Kết quả của CREL+ATM cho thấy rằng việc kết hợp mô-đun bộ nhớ chú ý không cải thiện hiệu suất của CRECL tốt, vì mạng đối chiếu có thể duy trì tính đồng nhất và sự đồng bộ của phân phối dữ liệu. Do đó không cần mô-đun bộ nhớ chú ý bổ sung để giúp mô hình tinh chỉnh các nguyên mẫu quan hệ.

(5) So với các mô hình sử dụng học tập metric (EA-EMR, EMAR, CML, EMAR+BERT), chúng tôi áp dụng mất mát InfoNCE thay vì mất mát xếp hạng biên làm hàm mất mát của chúng tôi. Với mất mát này, CRECL được dạy bởi nhiều thông tin quan hệ tiêu cực hơn để hiểu cách điều chỉnh không gian biểu diễn dữ liệu, dẫn đến việc giảm thiểu hiện tượng quên thảm khốc nhiều hơn.

#### 4.4.2 Nghiên cứu Loại bỏ

Để xác minh hiệu quả và tính hợp lý của các thành phần (bước) quan trọng của khung của chúng tôi, chúng tôi tiến hành thêm một loạt thí nghiệm loại bỏ.

Các biến thể loại bỏ của CRECL bao gồm:

**CRECL-MAG**: Đây là biến thể không có mất mát biên Lmag trong mạng đối chiếu.

**CRECL-CL1**: Đây là biến thể không có quy trình huấn luyện thứ nhất trong mạng đối chiếu.

**CRECL-CL2**: Đây là biến thể không có quy trình huấn luyện thứ hai trong mạng đối chiếu.

**CRECL-CL**: Đây là biến thể chỉ có mạng phân loại.

**CRECL-K**: Trong biến thể này, các thể hiện điển hình của mỗi quan hệ được chọn ngẫu nhiên thay vì bằng thuật toán K-means.

**CRECL(C)**: Biến thể này sử dụng mạng phân loại để xác định quan hệ của một thể hiện kiểm tra thay vì so sánh độ tương tự trong Eq. 9.

Hình 3 (a) và (b) hiển thị độ chính xác của tất cả các mô hình được so sánh của các tác vụ tích lũy lịch sử và tác vụ hiện tại. Do hạn chế không gian, chỉ kết quả trên TACRED được hiển thị, dựa trên đó chúng tôi có những phân tích sau.

(1) CRECL-CL hoạt động rất tốt trên tác vụ hiện tại (hình con (b)) nhưng hoạt động rất kém trên các tác vụ lịch sử (hình con (a)), cho thấy rằng nó quá khớp tác vụ hiện tại và hiện tượng quên thảm khốc của nó rất nghiêm trọng. Đó là vì các tham số phân loại luôn được điều chỉnh để phù hợp với tác vụ hiện tại thay vì các tác vụ cũ. Nó cho thấy rằng mạng đối chiếu có ý nghĩa quan trọng để giảm thiểu hiện tượng quên thảm khốc.

(2) Khi có nhiều tác vụ mới đến, sự sụt giảm hiệu suất của CRECL-CL1 trên tác vụ hiện tại (hình con (b)) rõ ràng hơn sự sụt giảm hiệu suất của nó trên các tác vụ lịch sử (hình con (a)), vì CRECL-CL1 chú ý nhiều hơn đến việc phân biệt các quan hệ khác nhau trong các tác vụ cũ thay vì trong tác vụ hiện tại. Đó là do phân phối dữ liệu của tất cả các tác vụ lịch sử được điều chỉnh trong quy trình huấn luyện thứ hai mà CRECL-CL1 chỉ có trong mạng đối chiếu của nó. Tương đối, hiệu suất của CRECL-CL2 trên cả tác vụ lịch sử và tác vụ hiện tại đều giảm. Nó cho thấy rằng chỉ phân biệt phân phối dữ liệu của tác vụ hiện tại với của các tác vụ cũ trong quy trình huấn luyện thứ nhất của mạng đối chiếu, là không đủ để giảm thiểu hiện tượng quên thảm khốc của nó. Thậm chí tồi tệ hơn, việc điều chỉnh như vậy cũng làm hại độ chính xác của việc phân loại tác vụ hiện tại.

(4) CRECL-K kém hơn CRECL, cho thấy rằng các thể hiện được chọn ngẫu nhiên không thể biểu thị tốt các quan hệ như những thể hiện được chọn bởi thuật toán K-means. Kết quả là, phân phối dữ liệu của tất cả các tác vụ không thể được điều chỉnh chính xác, điều này không thể giảm thiểu hiện tượng quên thảm khốc một cách hiệu quả. Ngoài ra, độ chính xác của CRECL-K trên tác vụ hiện tại cũng không ổn định do tính ngẫu nhiên do chiến lược lựa chọn các thể hiện điển hình của nó.

(5) Mặc dù mất mát học tập đối chiếu L2 khác với mất mát L1 của mạng phân loại, và các tham số của lớp mã hóa được chia sẻ, các quy trình huấn luyện của mạng đối chiếu hầu như không làm suy yếu sự phù hợp của mạng phân loại với tác vụ hiện tại. Do đó CRECL(C) vẫn hoạt động tốt trên tác vụ hiện tại như được hiển thị trong Hình 3 (b). CRECL-MAG có sự sụt giảm tương đối nhỏ trên cả tác vụ hiện tại và lịch sử, chứng minh rằng mất mát biên Lmag cải thiện hiệu suất bằng cách tăng khoảng cách giữa kết quả tối ưu và không tối ưu.

#### 4.4.3 Ảnh hưởng Hiệu suất của Kích thước Bộ nhớ

Đối với các phương pháp CRE dựa trên bộ nhớ, hiệu suất mô hình thường liên quan đến dung lượng lưu trữ của các mô-đun bộ nhớ của chúng. Cụ thể, chúng tôi phát hiện ra rằng hiệu suất của các mô hình trước đó rất nhạy cảm với số lượng thể hiện điển hình được lưu trữ L. Nhớ lại rằng trong CRECL, một nguyên mẫu quan hệ là trung bình của các biểu diễn của L thể hiện điển hình. Khả năng biểu diễn của nguyên mẫu như vậy ít nhạy cảm với L khi L vượt quá một giá trị nhất định, dẫn đến hiệu suất của CRECL cũng ít nhạy cảm với L, như chúng tôi đã nhấn mạnh trong Phần 3.5.

Hình 4 hiển thị hiệu suất của CRECL và RP-CRE trên TACRED w.r.t. các kích thước bộ nhớ khác nhau (L), trong đó hiệu suất của hai mô hình được so sánh trong T9 và T10 được hiển thị đặc biệt trong hình con (c). Nó cho thấy rằng mặc dù hiệu suất của CRECL cũng giảm khi L trở nên nhỏ, nó ổn định hơn và cao hơn hiệu suất của RP-CRE, đặc biệt khi L ≥ 8. Những kết quả như vậy chứng minh cho tuyên bố của chúng tôi về tính ít nhạy cảm của CRECL với L. Ngoài ra, dao động hiệu suất của RP-CRE rõ ràng hơn, có thể vì nó tái xây dựng mạng bộ nhớ chú ý trên mỗi tác vụ, do đó các đặc trưng tác vụ khác nhau không được chia sẻ trong mạng.

#### 4.4.4 Hiệu quả của Học tập Đối chiếu trong việc Tinh chỉnh Phân phối Dữ liệu

Ngoài ra, để điều tra ảnh hưởng của học tập đối chiếu trong việc giảm thiểu hiện tượng quên thảm khốc, chúng tôi sử dụng t-SNE (Van der Maaten và Hinton, 2008) để trực quan hóa phân phối dữ liệu của cùng một trường hợp trong Hình 1 sau các quy trình huấn luyện của mạng phân loại và mạng đối chiếu của CRECL. Bản đồ phân phối dữ liệu được hiển thị trong Hình 5, có cùng cài đặt màu sắc và kiểu điểm như Hình 1. Thông qua so sánh hai bản đồ này, chúng tôi thấy rằng phân phối dữ liệu của các quan hệ khác nhau trong tác vụ mới và cũ trong Hình 5 có thể phân biệt được nhiều hơn so với trong Hình 1. Những kết quả như vậy chủ yếu được quy cho học tập đối chiếu nguyên mẫu trong CRECL trong việc điều chỉnh tất cả phân phối dữ liệu của tất cả các tác vụ, điều này rõ ràng giảm thiểu hiện tượng quên thảm khốc của CRECL. Điều này cũng đã được chứng minh bởi hiệu suất vượt trội của CRECL so với các baseline được hiển thị trong các thí nghiệm đã nêu trước đó. Chúng tôi cũng lưu ý rằng một số dấu chấm màu vàng vẫn giao với các dấu thập màu hồng, có thể do việc lấy mẫu không đủ trong học tập đối chiếu, dẫn đến độ bao phủ ít hơn trên tất cả các quan hệ. Chúng tôi có thể xử lý tình huống này bằng cách tăng kích thước batch.

## 5 Kết luận

Trong bài báo này, chúng tôi đề xuất một khung CRE mới, gọi là CRECL, bao gồm một mạng phân loại và một mạng đối chiếu được thiết kế để giảm thiểu hiện tượng quên thảm khốc trong CRE. Thông qua học tập đối chiếu nguyên mẫu trong CRECL, phân phối dữ liệu của các quan hệ khác nhau trong tất cả các tác vụ được điều chỉnh để có thể phân biệt được nhiều hơn, dẫn đến cải thiện hiệu suất CRE. Hơn nữa, CRECL có khả năng học tập tăng dần lớp do cơ chế dựa trên đối chiếu để đạt được phân loại quan hệ, điều này thực tế hơn trong các tình huống CRE thế giới thực so với các mô hình trước đó của cơ chế dựa trên phân loại. Kết quả thí nghiệm rộng rãi của chúng tôi chứng minh rằng CRECL vượt trội hơn các baseline CRE SOTA và đạt được hiệu suất tốt nhất trên hai bộ dữ liệu chuẩn.
