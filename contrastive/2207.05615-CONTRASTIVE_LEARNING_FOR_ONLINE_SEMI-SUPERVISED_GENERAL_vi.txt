# HỌC TƯƠNG PHẢN CHO HỌC LIÊN TỤC TỔNG QUÁT BÁN GIÁM SÁT TRỰC TUYẾN

Nicolas Michel, Romain Negrel, Giovanni Chierchia, Jean-François Bercher
Đại học Gustave Eiffel, CNRS, LIGM, F-77454 Marne-la-Vallée, Pháp

TÓM TẮT
Chúng tôi nghiên cứu Học Liên tục Trực tuyến với nhãn thiếu sót và đề xuất SemiCon, một hàm mất mát tương phản mới được thiết kế cho dữ liệu có nhãn một phần. Chúng tôi chứng minh hiệu quả của nó bằng cách thiết kế một phương pháp dựa trên bộ nhớ được huấn luyện trên một luồng dữ liệu không có nhãn, trong đó mỗi dữ liệu được thêm vào bộ nhớ đều được gán nhãn bằng một oracle. Phương pháp của chúng tôi vượt trội hơn các phương pháp bán giám sát hiện có khi có ít nhãn, và đạt kết quả tương tự với các phương pháp giám sát tiên tiến trong khi chỉ sử dụng 2,6% nhãn trên Split-CIFAR10 và 10% nhãn trên Split-CIFAR100.

Từ khóa chỉ mục — Học Liên tục, Học Tương phản, Học Bán giám sát, Bộ nhớ

1. GIỚI THIỆU

Trong thập kỷ qua, các mạng nơ-ron sâu đã chứng minh hiệu quả của chúng khi đạt được kết quả tiên tiến trong nhiều tác vụ thị giác máy tính, như phân loại hình ảnh hoặc phát hiện đối tượng. Việc huấn luyện hiệu quả các mạng như vậy dựa trên các giả định sau đây.

A1. Dữ liệu được phân phối độc lập và giống hệt nhau.
A2. Dữ liệu huấn luyện có thể được mô hình xem nhiều lần trong quá trình học.
A3. Dữ liệu huấn luyện được gán nhãn đầy đủ.

Tuy nhiên, những giả định này hiếm khi đúng trong môi trường thực tế với luồng dữ liệu liên tục. Trong các tình huống như vậy, Giả định A1 không thể được xác minh, Giả định A2 khó đảm bảo khi lượng dữ liệu tăng vô hạn, và Giả định A3 sẽ ngụ ý một oracle gán nhãn cho mọi dữ liệu mới đến. Trong khi nhiều chiến lược huấn luyện thành công đã được thiết kế để tận dụng dữ liệu không có nhãn [1, 2] khi A3 không thể được đáp ứng, việc đối phó với sự vắng mặt của A1 và A2 vẫn còn nhiều vấn đề [3]. Một hậu quả đã biết là Quên Thảm khốc [4], và có khả năng được quan sát nếu không có biện pháp cụ thể nào được thực hiện. Học Liên tục (CL) nhằm duy trì hiệu suất khi thiếu A1, và Học Liên tục Trực tuyến (OCL) giải quyết sự thiếu hụt của A1 và A2.

Trong bài báo này, chúng tôi đề xuất một thuật toán OCL cho phân loại hình ảnh với ít nhãn. Phương pháp của chúng tôi được thiết kế để huấn luyện một mạng nơ-ron khi thiếu A1, A2 và A3. Chúng tôi đạt được điều này bằng cách giới thiệu một Hàm mất mát Tương phản Bán giám sát thống nhất để tận dụng dữ liệu có nhãn và không có nhãn. Mặc dù thiếu A3, kết quả của chúng tôi cho thấy phương pháp được đề xuất có thể so sánh với các phương pháp OCL giám sát tiên tiến, và hoạt động tốt hơn các phương pháp OCL bán giám sát hiện có khi nhãn khan hiếm.

Bài báo được tổ chức như sau. Phần 2 thảo luận về các công trình liên quan. Phần 3 mô tả phương pháp của chúng tôi: thiết lập vấn đề, giới thiệu một Hàm mất mát Tương phản Bán giám sát mới, và mô tả thuật toán. Phần 4 đánh giá hiệu suất của phương pháp chúng tôi trên một benchmark tiêu chuẩn. Cuối cùng, Phần 5 kết luận bài báo.

2. CÔNG TRÌNH LIÊN QUAN

Phương pháp của chúng tôi đứng ở giao điểm của Học Tương phản, Học Liên tục và Học Bán giám sát. Trong phần sau, chúng tôi sẽ tóm tắt ngắn gọn các lĩnh vực này và elaborat về các tình huống cụ thể trong đó thông tin nhãn không đầy đủ.

2.1. Học Tương phản

Học Tương phản đã trở nên đặc biệt phổ biến trong những năm gần đây để học biểu diễn từ dữ liệu [1, 2, 5, 6, 7, 8]. Trực giác khá đơn giản. Các mẫu tương tự (được gọi là positive) nên có biểu diễn gần nhau, trong khi các mẫu không tương tự (được gọi là negative) nên có biểu diễn càng xa càng tốt.

Học Tương phản Tự giám sát. Học Tương phản ban đầu được thiết kế cho dữ liệu không có nhãn, trong đó các positive được hình thành bằng cách thêm nhiễu vào đầu vào. Đối với hình ảnh, các phiên bản nhiễu là các phép biến đổi hoặc góc nhìn của hình ảnh gốc. Tất cả các góc nhìn của cùng một đầu vào là positive, trong khi bất kỳ hình ảnh nào khác trong batch huấn luyện được coi là negative. Tự giám sát này có một số nhược điểm: cần batch lớn để lấy mẫu đủ negative [1], và hình ảnh từ cùng một lớp có thể được coi là negative [5, 6, 2].

Học Tương phản Giám sát. Một phương pháp tương phản giám sát đã được đề xuất bởi Khosla et al. [8] sử dụng thông tin nhãn để khắc phục một số hạn chế của tự giám sát. Các tác giả coi mọi hình ảnh từ cùng một lớp là positive và cho thấy cải thiện đáng kể.

2.2. Học Liên tục

Học Liên tục (CL) đã trở nên phổ biến trong những năm qua cho phân loại hình ảnh. Vấn đề như sau. Xem xét một thiết lập học tuần tự với một chuỗi {T1,...,TK} của K tác vụ, và Dk = (Xk,Yk) các cặp dữ liệu-nhãn tương ứng. Số lượng tác vụ K không được biết và có thể lớn vô hạn. Với bất kỳ giá trị k1,k2∈{1,...,K}, nếu k1≠k2 thì ta có Yk1∩Yk2=∅ và số lượng lớp trong mỗi tác vụ là như nhau. Quên Thảm khốc xảy ra khi hiệu suất của mô hình giảm drastically trên các tác vụ quá khứ trong khi học tác vụ hiện tại [4, 9].

Học Liên tục Tổng quát. Các nghiên cứu CL ban đầu, và vẫn là phần lớn các phương pháp hiện tại, dựa vào các thiết lập trong đó task-id k được biết [10, 11, 12], hoặc ít nhất là ranh giới tác vụ được biết (tức là biết khi nào xảy ra thay đổi tác vụ). Tuy nhiên, thông tin như vậy thường không có sẵn trong khi huấn luyện trong môi trường thực [10, 13, 14, 15]. Buzzega et al. [16] đã giới thiệu tình huống Học Liên tục Tổng quát (GCL) trong đó task-id và ranh giới tác vụ không có sẵn.

GCL Trực tuyến. Làm việc với một luồng dữ liệu, mô hình nên có thể học mà không lưu trữ tất cả dữ liệu đến. Tình huống thực tế nhất là Học Liên tục Tổng quát Trực tuyến (OGCL), được mô tả bởi Buzzega et al. [16], thêm một ràng buộc nữa vào GCL: dữ liệu được trình bày một lần trong luồng đến. Điều này có nghĩa là mô hình nên thích ứng với dữ liệu hiện tại mà không có quyền truy cập vào tất cả dữ liệu quá khứ, ngay cả khi chúng ta có thể lưu trữ một lượng giới hạn dữ liệu luồng.

CL với nhãn thiếu sót. Trong khi hầu hết nghiên cứu CL tập trung vào thiết lập giám sát, các công trình gần đây xem xét CL trong đó có ít hoặc không có nhãn [17, 18, 19]. Tuy nhiên, không có công trình nào trong số chúng xử lý thiết lập OGCL ngoại trừ kiến trúc STAM [20], trong đó các tác giả trình bày một clustering trực tuyến phù hợp cho OGCL không giám sát. Trong bài báo này, chúng tôi tập trung vào OGCL với ít nhãn, mà chúng tôi trình bày trong Phần 3.

3. HỌC LIÊN TỤC TỔNG QUÁT BÁN GIÁM SÁT TRỰC TUYẾN

Trong phần này, chúng tôi định nghĩa chính thức thiết lập Học Liên tục Tổng quát Bán giám sát Trực tuyến (OSSGCL), và đề xuất một phương pháp để giải quyết vấn đề cơ bản.

3.1. Định nghĩa Vấn đề

Trong trường hợp giám sát, mỗi giá trị trong Y=∪K k=1Yk có thể truy cập được. Chúng tôi xem xét trường hợp bán giám sát, trong đó chúng tôi lặp qua một luồng dữ liệu không có nhãn tăng dần S = {X1,...,XK} và sử dụng một oracle để gán nhãn cho dữ liệu được chọn cụ thể. Trong bối cảnh này, chúng tôi có quyền truy cập vào một tập con Yl⊂Y với p=|Yl|/|Y| là tỷ lệ phần trăm nhãn có sẵn. Thường trong học bán giám sát, chúng ta muốn p càng nhỏ càng tốt. Ngoài ra, chúng tôi xem xét rằng chúng tôi có các ví dụ có nhãn cho mọi lớp được gặp. Chúng tôi định nghĩa Xuk và (Xlk,Ylk) là các tập dữ liệu không có nhãn và có nhãn cho tác vụ k. Vấn đề sau đó trở thành học tuần tự từ {T1,..,TK} tác vụ trên các tập dữ liệu có nhãn một phần Dk=Xuk∪(Xlk,Ylk).

3.2. Phương pháp Đề xuất

Để giải quyết vấn đề OSSGCL, chúng tôi thiết kế một Hàm mất mát Tương phản Bán giám sát mới (SemiCon) kết hợp Hàm mất mát Tương phản Giám sát [8] và Hàm mất mát Tương phản Tự giám sát [1].

Framework Học Tương phản. Theo [1], xem xét các yếu tố sau: một quá trình biến đổi dữ liệu Aug(·) biến đổi bất kỳ đầu vào nào theo một thủ tục ngẫu nhiên với a≠b⇔Auga(·)≠Augb(·); một bộ mã hóa Encθ(·) ánh xạ đầu vào đến không gian tiềm ẩn; một đầu chiếu Projφ(.) ánh xạ biểu diễn tiềm ẩn đến một không gian khác nơi hàm mất mát được áp dụng. Bộ mã hóa có thể là bất kỳ hàm nào và biểu diễn tiềm ẩn thu được sẽ được sử dụng cho các tác vụ downstream. Đầu chiếu được loại bỏ khi hoàn thành huấn luyện để chỉ giữ lại bộ mã hóa.

Batch đa góc nhìn với nhãn thiếu sót. Chúng tôi định nghĩa B={xli,yli}i=1..bl∪{xuj}j=1..bu là batch đến của b=bl+bu đầu vào với dữ liệu có nhãn và không có nhãn. Xem xét a,b là các số ngẫu nhiên, chúng tôi sau đó làm việc trên BI = Auga(B)∪Augb(B), "batch đa góc nhìn" [8] của 2b mẫu trên các chỉ số i∈I và I=Il∪Iu với Il là các chỉ số trên các mẫu có nhãn và Iu là các chỉ số trên các mẫu không có nhãn. Hơn nữa, hi = Enc(xi) là biểu diễn tiềm ẩn của xi với HI={hi}i∈I, và zi = Proj(hi) là phép chiếu của hi với ZI={zi}i∈I. Chúng tôi cũng định nghĩa P(i) ={j∈I\{i}|yj=yi} là các chỉ số trên các positive của i (các mẫu tương tự), và τ là nhiệt độ.

Hàm mất mát SemiCon. Chúng tôi giới thiệu SemiCon, một hàm mất mát thống nhất được thiết kế để huấn luyện một mô hình tương phản trong bối cảnh nhãn thiếu sót. Chúng tôi xây dựng hàm mất mát này bằng cách kết hợp hai thành phần. Thành phần đầu tiên, Lm(1), tương ứng với hàm mất mát tương phản giám sát trên dữ liệu bộ nhớ có nhãn với dữ liệu luồng không có nhãn được coi là negative (các mẫu không tương tự).

Lm=−∑i∈Il(1/|P(i)|)∑p∈P(i)log(ezi·zp/τ)/(∑a∈I\{i}ezi·za/τ) (1)

Thành phần thứ hai, Lu(2), đại diện cho hàm mất mát tương phản không giám sát trên dữ liệu luồng không có nhãn với dữ liệu luồng có nhãn được coi là negative với j(i) là chỉ số sao cho i và j(i) là các chỉ số của các mẫu được biến đổi có cùng nguồn đầu vào.

Lu=−∑i∈Iulog(ezi·zj(i)/τ)/(∑a∈I\{i}ezi·za/τ) (2)

Chúng tôi định nghĩa một hàm mất mát thống nhất LSemiCon =Lm+αLu trong đó α∈[0,+∞[ là một siêu tham số trọng số biểu thị tầm quan trọng của dữ liệu không có nhãn trong quá trình huấn luyện. Chúng ta có thể thấy rằng (∀i∈Iu)P(i) ={j(i)} nên hàm mất mát có thể được biểu diễn dưới dạng

LSemiCon =−∑i∈I(gα(i)/|P(i)|)∑p∈P(i)log(ezi·zp/τ)/(∑a∈I\{i}ezi·za/τ) (3)

với gα(i) =α nếu i∈Il, và gα(i) = 1 otherwise. Các phương pháp replay dữ liệu quá khứ có thể bị overfit trên dữ liệu bộ nhớ [21] và trong khi SemiCon xử lý nhãn thiếu sót, nó cũng cung cấp kiểm soát về cách một mô hình nên cân bằng việc học từ dữ liệu bộ nhớ và luồng một cách riêng biệt.

Thủ tục Huấn luyện. Chúng tôi đề xuất một phương pháp lấy cảm hứng từ công trình của Mai et al. [22]. Họ đã định nghĩa Supervised Contrastive Replay (SCR) kết hợp Hàm mất mát Tương phản Giám sát [8] và chiến lược dựa trên bộ nhớ [23]. Phương pháp của họ đạt kết quả tiên tiến trong CL trực tuyến khi mọi dữ liệu đều có nhãn. Chúng tôi điều chỉnh SCR để làm việc với lượng dữ liệu có nhãn hạn chế bằng cách sử dụng SemiCon làm mục tiêu. Phương pháp của chúng tôi dựa trên hai điểm: (a) mỗi dữ liệu được thêm vào buffer bộ nhớ đều có nhãn, (b) chúng tôi tận dụng dữ liệu có nhãn và không có nhãn trong một mục tiêu tương phản thống nhất sử dụng SemiCon.

Trong giai đoạn huấn luyện, chúng tôi lặp qua một luồng dữ liệu không có nhãn S. Với mỗi batch luồng đến Bs∈S, chúng tôi ngẫu nhiên lấy mẫu một batch dữ liệu có nhãn Bm từ bộ nhớ M và làm việc trên B=Bs∪Bm. Mỗi batch dữ liệu B được biến đổi, và batch đa góc nhìn thu được BI được đưa vào mạng để tính toán các phép chiếu hình ảnh ZI. Hàm mục tiêu LSemiCon được tính toán trên ZI, và các tham số mô hình được cập nhật bằng Steepest Gradient Descent (SGD) vanilla. Sau mỗi bước SGD, dữ liệu bộ nhớ được cập nhật bằng Reservoir Sampling [24]. Mỗi dữ liệu luồng được chọn được gán nhãn bằng Oracle O trước khi lưu trữ bộ nhớ.

Trong giai đoạn thử nghiệm, các biểu diễn dữ liệu bộ nhớ HM={Encθ(x)}x∈M được tính toán và một bộ phân loại được huấn luyện trên HM. Phương pháp của chúng tôi được mô tả chi tiết trong Thuật toán 1. Tương tự như Mai et al., chúng tôi sử dụng bộ phân loại Nearest Class Mean (NCM) cho giai đoạn thử nghiệm. Bất kỳ bộ phân loại nào khác cũng có thể được sử dụng trên các biểu diễn thu được; tuy nhiên, chúng tôi không tìm thấy sự khác biệt đáng kể về hiệu suất.

4. KẾT QUẢ THỰC NGHIỆM

Trong phần này, chúng tôi mô tả hai tập dữ liệu CL, giới thiệu các baseline và trình bày kết quả của chúng tôi so với tiên tiến hiện tại.

4.1. Tập dữ liệu

Chúng tôi sử dụng các phiên bản được sửa đổi của tập dữ liệu phân loại hình ảnh tiêu chuẩn [25] để xây dựng môi trường học tăng dần. Các tập dữ liệu này được xây dựng trên CIFAR10 và CIFAR100 bằng cách chia chúng thành nhiều tác vụ của các lớp không chồng lấp. Cụ thể, chúng tôi làm việc trên Split-CIFAR10 và Split-CIFAR100. Chúng tôi chia CIFAR10 thành 5 tác vụ với 2 lớp mỗi tác vụ và CIFAR100 thành 10 tác vụ với 10 lớp mỗi tác vụ. Mỗi tập dữ liệu chứa 50.000 hình ảnh huấn luyện và 10.000 hình ảnh kiểm tra.

4.2. Baseline

Để đánh giá kết quả của chúng tôi, chúng tôi so sánh chúng với nhiều baseline tuân thủ thiết lập OGCL và được liệt kê dưới đây:

•offline: Cận trên giám sát. Mô hình được huấn luyện mà không có bất kỳ ràng buộc cụ thể nào của CL.
•fine-tuned: Cận dưới giám sát huấn luyện mô hình trong thiết lập CL mà không có biện pháp phòng ngừa để tránh quên.
•SCR: Tiên tiến hiện tại trên OGCL giám sát đầy đủ và phương pháp gần nhất với công trình của chúng tôi.
•SCR - Memory Only (SCR-MO): Phương pháp SCR, nhưng được huấn luyện chỉ sử dụng dữ liệu bộ nhớ.
•Experience Replay (ER) [23]: ER là một baseline đơn giản áp dụng reservoir sampling [24] cho cập nhật bộ nhớ, giống như SCR, nhưng được huấn luyện với hàm mất mát cross entropy thay vì hàm mất mát tương phản.
•Experience Replay - Memory Only (ER-MO): ER-MO về cơ bản là ER, nhưng được huấn luyện chỉ sử dụng dữ liệu bộ nhớ.

Mặc dù các phương pháp CL bán giám sát khác tồn tại, không phương pháp nào tuân thủ thiết lập OSSGCL và do đó không thể được sử dụng trong so sánh [14, 18, 26].

4.3. Chi tiết Triển khai

Cho mọi thí nghiệm, chúng tôi huấn luyện một ResNet-18 thu gọn [27] từ đầu theo các công trình trước [13, 22] và lớp chiếu Projφ là một multi layer perceptron [1] với một lớp ẩn, kích hoạt ReLU và kích thước đầu ra là 128. Đối với các phương pháp dựa trên bộ nhớ, chúng tôi sử dụng kích thước batch bộ nhớ |Bm| là 100 trên Split-CIFAR10 và 500 trên Split-CIFAR100. Chi tiết thêm về tác động của kích thước batch bộ nhớ có thể được tìm thấy trong phần 4. Đối với các phương pháp trực tuyến, chúng tôi sử dụng kích thước batch luồng |Bs| là 10, đảm bảo 5.000 bước SGD trên cả hai tập dữ liệu. Mỗi phương pháp được so sánh được huấn luyện bằng optimizer SGD với tốc độ học 0,1, không regularization, và nhiệt độ τ= 0,07 cho hàm mất mát tương phản. Cho mọi thí nghiệm, chúng tôi sử dụng cùng thủ tục biến đổi như trong [1] và đối với các phương pháp tương phản, chúng tôi xây dựng batch đa góc nhìn sử dụng một biến đổi cho mỗi góc nhìn, trong khi triển khai SCR gốc sử dụng hình ảnh gốc làm một góc nhìn và một biến đổi làm góc nhìn khác. Chúng tôi thu được kết quả tốt hơn về mặt thực nghiệm khi sử dụng một biến đổi cho mỗi góc nhìn. Tất cả thí nghiệm được thực hiện 10 lần. Kết quả trung bình và độ lệch chuẩn của chúng được hiển thị trong phần tiếp theo. Đối với baseline offline, chúng tôi sử dụng cùng optimizer và mạng như các phương pháp khác và huấn luyện trong 50 epoch.

4.4. Kết quả

Trong phần sau, chúng tôi tóm tắt ngắn gọn một metric CL tiêu chuẩn, mô tả tác động của hai siêu tham số và phân tích kết quả thu được.

Metric. Chúng tôi sử dụng độ chính xác trung bình trên tất cả các tác vụ sau khi huấn luyện trên tác vụ cuối cùng. Metric này được gọi là độ chính xác trung bình cuối cùng [11, 10].

Lựa chọn Kích thước Batch Bộ nhớ. Chúng tôi nghiên cứu tác động của kích thước batch bộ nhớ |Bm| lên hiệu suất. Chúng tôi sử dụng SCR, SCR-MO và phương pháp đề xuất của chúng tôi làm case study. Như được hiển thị trong hình 1, mọi phương pháp đều theo cùng xu hướng và hưởng lợi từ |Bm| lớn hơn. Chúng tôi chọn |Bm|= 100 cho mọi huấn luyện trên Split-CIFAR10 và tương tự |Bm|= 500 trên Split-CIFAR100. Hơn nữa, ngay cả với α= 0, phương pháp của chúng tôi vẫn liên tục vượt trội hơn SCR-MO trên các giá trị |Bm| nhỏ hơn. Điều này chứng minh rằng sử dụng negative không có nhãn có thể tăng cường hiệu suất đáng kể khi có ít nhãn.

Tác động của α. Chúng tôi đánh giá tác động của α lên hiệu suất phương pháp của chúng tôi. Chúng tôi giữ mọi tham số khác cố định khi thí nghiệm với các giá trị α. Theo trực giác, α tương ứng với tầm quan trọng chúng ta muốn dành cho dữ liệu luồng không có nhãn so với dữ liệu bộ nhớ có nhãn. Chúng tôi quan sát trong hình 2 rằng giá trị tối ưu cho α phụ thuộc vào kích thước bộ nhớ và có xu hướng gần với một. Hình 3 xác nhận quan sát này bằng cách so sánh hiệu suất cho α= 1 với hiệu suất tốt nhất cho bất kỳ α nào và ngụ ý rằng α= 1 là một tham số mặc định chấp nhận được cho phương pháp của chúng tôi. Ngoài ra, hiệu suất phương pháp của chúng tôi trở nên có thể so sánh với SCR-MO đối với các kích thước bộ nhớ lớn hơn. Chúng tôi giải thích quan sát trước đây là hậu quả của α đóng vai trò như một tham số regularization. Khi kích thước bộ nhớ nhỏ, mô hình có xu hướng overfit trên dữ liệu bộ nhớ và hoạt động tốt hơn khi α lớn hơn. Tương tự, khi kích thước bộ nhớ lớn, mô hình có đủ thông tin trong bộ nhớ và hoạt động tốt hơn khi α nhỏ hơn. Nhìn vào kết quả trong hình 2, hiệu suất tốt nhất được đạt được khi chúng ta sử dụng thông tin từ cả luồng và bộ nhớ.

Giải thích Kết quả. Bảng 1 hiển thị so sánh phương pháp của chúng tôi với các phương pháp giám sát tiên tiến hiện tại và các đối tác chỉ sử dụng bộ nhớ của chúng trên Split-CIFAR10 và Split-CIFAR100 với các kích thước bộ nhớ M khác nhau. Phương pháp của chúng tôi đạt kết quả tốt nhất so với các phương pháp bán giám sát khác và hoạt động tương đương với tiên tiến giám sát trong khi chỉ tận dụng một phần nhỏ nhãn. Điều này đặc biệt đáng chú ý đối với kích thước bộ nhớ nhỏ nơi chỉ có 2,6% nhãn được cung cấp cho mô hình. Hình 3 cho thấy phương pháp của chúng tôi vượt trội hơn SCR-MO khi có ít hơn 20% nhãn. Ngoài ra, phương pháp của chúng tôi ngang bằng với SCR chỉ sử dụng 10% nhãn trong khi SCR-MO cần 20% nhãn để đạt kết quả tương đương. Các thí nghiệm của chúng tôi cũng cho thấy rằng các baseline chỉ sử dụng thông tin có sẵn trong bộ nhớ hoạt động cạnh tranh, với kết quả gần với các đối tác giám sát của chúng đối với kích thước bộ nhớ lớn. Điều này có thể được giải thích bằng quan sát rằng bộ nhớ càng lớn, vấn đề càng gần với một vấn đề giám sát offline.

5. KẾT LUẬN

Trong bài báo này, chúng tôi đã định nghĩa một thiết lập OSSGCL mới và giới thiệu một Hàm mất mát Tương phản Bán giám sát mới (SemiCon). Chúng tôi đã chứng minh thực nghiệm rằng các phương pháp bán giám sát được huấn luyện chỉ sử dụng dữ liệu bộ nhớ có thể hoạt động cạnh tranh với các đối tác giám sát của chúng, trong khi tận dụng ít đến 2,6% nhãn trên split-CIFAR10. Chúng tôi đã đề xuất một phương pháp dựa trên bộ nhớ mới cho thiết lập OSSGCL kết hợp thành công dữ liệu có nhãn và không có nhãn sử dụng hàm mất mát SemiCon mới. Tiêu chí này cho phép kiểm soát cân bằng giữa dữ liệu có nhãn và không có nhãn trong quá trình huấn luyện. Chúng tôi đã cho thấy phương pháp của chúng tôi có thể tận dụng dữ liệu không có nhãn, vượt qua các baseline bán giám sát khác trên tập dữ liệu Split-CIFAR, và đạt hiệu suất tương tự với các phương pháp giám sát tiên tiến.
