# 2309.15038.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/contrastive/2309.15038.pdf
# File size: 3598936 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
HPCR: Holistic Proxy-based Contrastive Replay
for Online Continual Learning
Huiwei Lin, Shanshan Feng, Baoquan Zhang, Xutao Li, and Yunming Ye
Abstract â€”Online continual learning, aimed at developing a
neural network that continuously learns new data from a
single pass over an online data stream, generally suffers from
catastrophic forgetting. Existing replay-based methods alleviate
forgetting by replaying partial old data in a proxy-based or
contrastive-based replay manner, each with its own shortcom-
ings. Our previous work proposes a novel replay-based method
called proxy-based contrastive replay (PCR), which handles the
shortcomings by achieving complementary advantages of both
replay manners. In this work, we further conduct gradient and
limitation analysis of PCR. The analysis results show that PCR
still can be further improved in feature extraction, generaliza-
tion, and anti-forgetting capabilities of the model. Hence, we
develop a more advanced method named holistic proxy-based
contrastive replay (HPCR). HPCR consists of three components,
each tackling one of the limitations of PCR. The contrastive
component conditionally incorporates anchor-to-sample pairs
to PCR, improving the feature extraction ability. The second
is a temperature component that decouples the temperature
coefficient into two parts based on their gradient impacts and sets
different values for them to enhance the generalization ability.
The third is a distillation component that constrains the learning
process with additional loss terms to improve the anti-forgetting
ability. Experiments on four datasets consistently demonstrate
the superiority of HPCR over various state-of-the-art methods.
Index Terms â€”Neural Networks, Online Continual Learning,
Catastrophic Forgetting, Image Classification.
I. I NTRODUCTION
ONLINE continual learning (OCL) is a special scenario
of continual learning [1]. Taking the classification prob-
lem [2] as an example, OCL aims to develop a neural network
that can accumulate knowledge of new classes without forget-
ting information learned from old classes. It is essential to note
that the data stream encountered in OCL is non-stationary,
21-Sep-2024 Manuscript received 26 March 2024; revised 21 September
2024; accepted 31 December 2024. Date of publication XX XXXX 2024;
date of current version XX XXXX 2024. This work was supported in part
by National Nature Science Foundation of China under Grant 62272130
and Grant 62376072; in part by Nature Science Program of Shenzhen
under Grant JCYJ20210324120208022 and Grant JCYJ20200109113014456;
in part by Shenzhen Science and Technology Program under Grant
KCXFZ20211020163403005. (Corresponding authors: Yunming Ye; Shan-
shan Feng.)
Huiwei Lin, Baoquan Zhang, Xutao Li, Yunming Ye are with the Depart-
ment of Computer Science, Harbin Institute of Technology, Shenzhen 518055,
China, and also with Shenzhen Key Laboratory of Internet Information
Collaboration, Shenzhen 518055, China (e-mail: yeyunming@hit.edu.cn).
Shanshan Feng is with the Centre for Frontier AI Research, Insti-
tute of High Performance Computing, A*STAR, Singapore (e-mail: vic-
torfengss@foxmail.com).
Color versions of one or more figures in this article are available at
https://doi.org/XX.XXXX/TNNLS.2024.XXXXXXX.
Digital Object Identifier XX.XXXX/TNNLS.2024.XXXXXXXwith each sample being accessed only once during training.
Currently, the main challenge of OCL is catastrophic forgetting
(CF) [3], wherein the model experiences a significant decline
in performance for old classes when learning new classes[4].
The replay-based methods [5], which replay a portion
of previous samples, have shown superior performance for
OCL [6]. In general, there are two ways to replay as shown
in Fig. 1(a). The first is the proxy-based replay manner,
which replays using a proxy-based loss function and softmax
classifier. It calculates similarities between each anchor and the
proxies belonging to all classes. Here, a proxy can be regarded
as the representative of a class [7], and the anchor represents
a sample in a training batch. This manner is subjected to the
â€œbiasâ€ issue resulting from class imbalance [5]. The second
is the contrastive-based replay manner, which replays using a
contrastive-based loss function and nearest class mean (NCM)
classifier [8]. It computes similarities between each anchor and
all samples in the same training batch. This manner is unstable
and hard to converge during training. To sum up, these two
manners have their corresponding shortcomings.
Our previous work [9] proposes proxy-based contrastive re-
play (PCR) method to handle these shortcomings. Specifically,
we perform a comprehensive analysis and find that coupling
both replay manners achieves complementary advantages. The
proxy-based replay manner enables fast and reliable conver-
gence with the help of proxies, thereby overcoming the prob-
lems of being unstable and hard to converge in the contrastive-
based replay manner. Meanwhile, the contrastive-based replay
manner can provide the proxy-based replay manner with a new
way of selecting anchor-to-proxy pairs, which has been proven
to effectively address the â€œbiasâ€ issue [10], [11]. With these
inspirations, PCR calculates similarities between each anchor
and the proxies whose associated classes of samples appear in
the same batch as illustrated in Fig. 1(a).
In this work, we conduct an in-depth analysis of PCR and
explore its potential for improvement. Specifically, we derive
the gradient propagation process of PCR in the classifier based
on the one for general proxy-based loss. The gradient analysis
shows that PCR benefits from the number of samples of
different classes in each training batch, further confirming its
effectiveness and reliability. However, our limitation analysis
reveals that PCR still has shortcomings in three key areas: 1)
PCR considers the relations between anchor-to-proxy pairs but
ignores the relations between anchor-to-sample pairs. It results
in insufficient feature extraction capability of the model, par-
ticularly in scenarios with large training batches where anchor-
Copyright Â©2024 IEEE. Personal use of this material is permitted.
However, permission to use this material for any other purposes must be obtained from the IEEE by sending an email to pubs-permissions@ieee.org.arXiv:2309.15038v2  [cs.LG]  3 Jan 2025

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
: self-defined Step Function dependent on ğ‘Proxy-based replay mannerğ’™!ğ’˜"#ğ’˜$%ğ’˜&%ğ’˜'%
New classes
old classesModellearning
replaying
New classes
old classesModellearning
replayingContrastive Componentğœğ‘ ğœğœğ‘ ğœ(ğ‘ )ğœğ‘ ğœdecouple ğœ<ğ’™!,ğ’˜"#>/ğœ<ğ’™!,ğ’˜$%>/ğœ
ğ‘†(ğ‘)-<ğ’™!,ğ’™$#>/ğœTemperature ComponentPCRContrastive-based replay mannerğ’™!ğ’™&%ğ’™'%ğ’™(%ğ’™"#ğ’™$#<ğ’™!,ğ’™"#>/ğœ<ğ’™!,ğ’™(%>/ğœDistillation ComponentKDKDSCDPCDoldnewoldnew: proxies of different class: samples of different class: similarity calculation: selective similarity calculationğ‘: Number of samples in a batchğœ: Static temperatureğœ(ğ‘ ): Dynamic temperature dependent on training step ğ‘ ğ‘ (ğ‘)
additional loss(a) Proxy-based Contrastive Replay (PCR)(b) Holistic Proxy-based Contrastive Replay (HPCR)
Fig. 1. Illustration of our work. (a) The example of existing replay manners. For each anchor sample xa, the proxy-based replay manner calculates similarities
of all anchor-to-proxy pairs; the contrastive-based replay manner calculates similarities of all anchor-to-sample pairs in the same training batch; PCR only
calculates similarities of selective anchor-to-proxy pairs in the same training batch. (b) The example of the proposed HPCR. The contrastive component
conditionally produces anchor-to-sample pairs to PCR; the temperature component decouples the temperature coefficient into two parts based on their impacts
on gradients and sets them differently; the distillation component constrains new anchor-to-sample pairs using the old ones (SCD), and distills old anchor-to-
proxy pairs to the new ones (PCD).
to-sample pairs contain important semantic information. 2)
The simplistic setting of the temperature coefficient in PCR
overlooks its impact on the gradient propagation process. This
oversight undermines the modelâ€™s generalization capability ,
as the use of a static constant fails to optimize effectively. 3)
Old classes included in the learning process alongside new
classes continue to encounter bias issues during training. It
implies that there is still room for improvement in the modelâ€™s
anti-forgetting capability . Hence, PCR can be enhanced by
overcoming these limitations.
To this end, we develop a more comprehensive method
named holistic proxy-based contrastive replay (HPCR). As
illustrated in Fig. 1(b), HPCR consists of three components.
1) The contrastive component incorporates anchor-to-sample
pairs conditionally into PCR to improve the modelâ€™s feature
extraction capability, capturing more fine-grained semantic
information when dealing with large training batches. Specif-
ically, we introduce a step function s(N)to regulate the
importance of the anchor-to-sample pairs. When the number
of samples is small, their contribution is set as 0; other-
wise, it is 1. 2) Meanwhile, the temperature component
decomposes the temperature coefficient into two parts based
on their influence on gradients. One part is assigned a static
constant value, while the other is denoted as a dynamic
function value to strengthen the generalization capability,
promoting the acquisition of novel knowledge. 3) Additionally,
thedistillation component imposes further constraints on the
replaying process to enhance the anti-forgetting capability, pre-
serving more historical knowledge. This component consists
of two knowledge distillation (KD) mechanisms: proxy-based
contrastive distillation (PCD) and sample-based contrastive
distillation (SCD). The former distills the old anchor-to-proxy
correlations into the new ones, while the latter constrains thenew anchor-to-sample correlations based on the old ones.
Our main contributions can be summarized as follows:
1) We perform further analyses of the PCR. The gradient
analysis verifies the effectiveness and reliability of PCR,
while the limitation analysis suggests that PCR still has
three limitations and can be further improved.
2) We develop a more holistic method based on PCR, named
HPCR, which consists of a contrastive component, a
temperature component, and a distillation component.
These three components improve the modelâ€™s feature
extraction, generalization, and anti-forgetting capabilities,
respectively.
3) We conduct extensive experiments on four datasets, and
the empirical results consistently demonstrate the supe-
riority of HPCR over various state-of-the-art methods.
We also investigate and analyze the benefits of each
component by ablation studies. And the codes are open-
sourced at https://github.com/FelixHuiweiLin/PCR.
This work is an extension of our conference version pre-
sented in [9]. It offers a more comprehensive approach, which
can be delineated into four aspects: (i) This version includes
a gradient analysis of PCR, which enhances its theoretical
foundation and helps us discover its limitations. (ii) This
version conditionally integrates anchor-to-sample pairs into
PCR to augment the modelâ€™s feature extraction capability,
treating it as a contrastive component. (iii) This version
introduces a temperature component to enhance the modelâ€™s
generalization ability and a distillation component to improve
the modelâ€™s anti-forgetting capability. (iv) Furthermore, more
experiments are conducted to explore the proposed method,
which includes utilizing a new dataset (Split TinyImageNet),
conducting additional ablation studies, and visualizations.

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
II. R ELATED WORK
A. Continual Learning
There are many continual learning setups that have been
discussed recently [12]. First, continual learning can be divided
into task-based and task-free setups. Task-based continual
learning [13], [14] assumes that task-ID can be available while
the network in task-free continual learning [15] has no access
to task-ID at inference time. Next, continual learning can be
categorized into offline and online setups. Under the offline
setup [16], all training samples in the current learning stage
can be learned with multiple training steps. In contrast, each
training sample in the streamed data is seen only once for the
online setup [6]. Finally, following [17], continual learning
also can be separated into disjoint-task and blurry-task setups.
The disjoint-task setup requires that the intersection of the
label sets of any two learning stages is an empty set [18].
The blurry-task setup has been proposed to set some common
samples for the training samples of each learning stage [19].
Five main directions drive recent advances in continual
learning. 1) Architecture-based methods divide each task into
a set of specific parameters of the model. They dynamically
extend the model as the task increases [20] or gradually freeze
part of parameters to overcome the forgetting problem [21]. 2)
Regularization-based methods store the historical information
learned from old data as the prior knowledge of the network.
It consolidates past knowledge by extending the loss function
with an additional regularization term [22], [23]. 3) Replay-
based methods, which set a fixed-size memory buffer [24]
or generative model [25], [26], [27] to store, produce, and
replay historical samples during training. A large number of
approaches [28], [29] have been proposed to improve the
performance of replaying. Instead of replaying samples, some
approaches [30], [31] focus on the feature replaying. 4) The
prompt-based methods, such as S-prompt [32], L2P [33],
Dualprompt [34], and CODA-Prompt [35] utilize prompt
learning based on a pre-trained model, demonstrating better
performance. 5) The optimization-based methods explicitly
constrain the updating process of parameters. A typical idea is
the gradient projection [36], [37], [38] that projects the original
gradient in a specified direction for updating parameters more
effectively. It is different from the replay-based methods that
constrain the gradient by replaying old samples.
In this work, the setup is online continual learning based
on the disjoint-task setup with a task-free process. HPCR
is proposed as a novel replay-based method to address the
forgetting issue. Different from general replay-based methods,
HPCR is used to quickly learn novel knowledge and efficiently
retain historical knowledge in an online fashion.
B. Online Continual Learning
OCL is a subfield of continual learning that focuses on effi-
ciently learning from a single pass over the online data stream.
It is essential in scenarios where a model needs to continuously
evolve its knowledge and adapt to new information [6].
Researchers in this field focus on developing algorithms and
techniques to mitigate catastrophic forgetting, control model
adaptation, and efficiently use limited resources to achievethis continuous learning. At present, the replay-based methods
are the main solutions of OCL without AOP [39], which is
based on orthogonal projection. Although architecture-based
methods can be effective for continual learning, they are often
impractical for OCL. Since it is unfeasible for large numbers of
tasks [40] by the â€incremental network architecturesâ€. Mean-
while, it is known that regularization-based methods show poor
performance in OCL as it is hard to design a reasonable metric
to measure the importance of parameters [40].
Experience replay (ER) [41] that employs reservoir sam-
pling for memory management is usually a strong strategy.
Many subsequent methods have been improved based on
this method and can generally be divided into three groups.
1) Some approaches belong to memory retrieval strategy ,
which usually studies how to select more important samples
from the buffer for replaying. These representative methods
include MIR [42] using increases in loss, and ASER [43]
using adversarial shapley value. 2) In the meantime, some
approaches [44], [45] focus on saving more effective samples
to the memory buffer, belonging to the memory update
strategy . The core of this type of method is to use the limited
samples in the buffer to approximate all previous samples
as much as possible. 3) In addition to selecting important
samples, it is equally important to conduct efficient anti-
forgetting training. The model update strategy is a family
of methods [46], [47], [11], [48], [49], [50], [51], [52], [53],
[54], [55] to improve the learning efficiency, making the model
learn quickly and memory efficiently. All of them belong to
the proxy-based replay manners except SCR [46], which uses
a contrastive-based replay manner.
The proposed HPCR in this work is a novel model update
strategy for OCL. Different from existing approaches, it is
a more holistic and effective method proposed based on
PCR. After conducting an in-depth analysis of PCR, HPCR
improves the modelâ€™s feature extraction, generalization, and
anti-forgetting capabilities with three components, making
PCR better suited for OCL than previous studies.
C. Deep Metric Learning
Similar to [7], our method is inspired by deep metric
learning. Deep metric learning aims to measure the similarity
among samples by a deep neural network while using an
optimal distance metric for learning tasks [56]. Generally, the
loss functions for this learning task can be categorized into
pair-based and proxy-based losses. For one thing, the pair-
based methods [57], [58] can mine rich semantic information
from anchor-to-sample relations, but converge slowly due to
its high training complexity. Since contrastive-based loss is
good at learning anchor-to-sample pairs, it can be regarded
as a type of pair-based method. For another thing, the proxy-
based methods [59], [60] converge fast and stably, meanwhile
may miss partial semantic information by learning anchor-to-
proxy relations. The cross-entropy loss can be regarded as a
type of proxy-based method.
The proposed HPCR in this work integrates these two
types of methods. Different from existing studies, HPCR is
proposed to quickly learn novel knowledge and effectively
keep historical knowledge for OCL.

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
TABLE I
THE CHARACTERISTICS OF THE PROXY -BASED AND CONTRASTIVE -BASED REPLAY MANNERS .
Replay Manner Classifier Pairs of Loss Advantages Disadvantages
Proxy-based Softmax All Anchor-to-proxy PairsFast Convergence
StableBias
Incomplete Semantic Knowledge
Contrastive-based NCM Selective Anchor-to-sample PairsAvoid Bias
Complete Semantic KnowledgeSlow Convergence
Unstable
III. B ACKGROUND
A. Online Continual Learning
OCL divides a data stream into a sequence of learning tasks
asD={Dt}T
t=1, where Dt={XtÃ— Yt,Ct}contains the
samples Xt, corresponding labels Yt, and task-specific classes
Ct. Different tasks have no overlap in the classes. All of
learned classes are denoted as C1:t=St
k=1Ck. The neural
network is made up of a feature extractor z=h(x;Î¦)and a
softmax (proxy-based) classifier o=f(z;W) = [oc]C1:t
c=1[61],
where oc=âŸ¨z,wcâŸ©/Ï„is the ith dimension value of o,
âŸ¨Â·,Â·âŸ©is the cosine similarity, Ï„is a scale factor and W=
[w1,w2, ...,wc]contains trainable proxies of all classes. The
categorical probability that sample xbelongs to class yis
py=exp(oy)P
câˆˆC1:texp(oc). (1)
Generally, the model learns each mini-batch current samples
B âŠ‚ D tby the following loss function
L=E(x,y)âˆ¼B[âˆ’log(py)]. (2)
In this case, the model can learn new classes well and forget
old classes, resulting in the forgetting problem. To mitigate
the forgetting, existing replay-based methods replay part of
previous samples in two ways.
First, the proxy-based replay manners (such as ER [41])
select a mini-batch of previous samples BMâŠ‚ M to calculate
proxy-based loss by
LER=E(x,y)âˆ¼BâˆªB M[âˆ’log(exp(oy)P
câˆˆC1:texp(oc))], (3)
where Mis a memory buffer to store part of previous samples.
And it predicts unknown instances by a softmax classifier
yâˆ—= arg max
cpc, câˆˆC1:t. (4)
Moreover, other proxy-based replay manners [10], [11] im-
prove ER by selecting part of anchor-to-proxy pairs to calcu-
late the objective function. Although effective, they can easily
hurt the modelâ€™s generalization ability to learn new classes.
Second, the contrastive-based replay manners (such as
SCR [46]) replay BMâŠ‚ M using a contrastive-based loss
LPCR=E(x,y)âˆ¼BâˆªB M[âˆ’1
|P|X
pâˆˆPlogexp(âŸ¨z,zpâŸ©/Ï„)P
jâˆˆJexp(âŸ¨z,zjâŸ©/Ï„)].(5)
Different from the proxy-based loss, the selected pairs rely on
the number of samples in a training batch. And it inferences
by an NCM classifier
yâˆ—= arg min
câˆ¥zâˆ’Âµcâˆ¥, câˆˆC1:t, (6)
To obtain the classification centers Âµcof all classes, it has to
compute the embeddings of all samples in the memory buffer
before each inferring. Due to the high complexity of training,
its effect is unstable and the convergence speed is slow.B. Proxy-based Contrastive Replay
Our previous work [9] analyzes existing studies and pro-
poses to couple two replay manners, achieving complementary
advantages. The characteristics of these two replay manners
are summarized in Table I. For one thing, the anchor-to-
proxy pairs of proxy-based replay manner can overcome
the disadvantage of contrastive-based loss. For another thing,
the contrastive-based loss provides a heuristic way to select
anchor-to-proxy pairs for the proxy-based replay manner.
Moreover, the anchor-to-sample pairs contain richer fine-
grained semantic knowledge than the anchor-to-proxy pairs.
Although there are some coupling solutions, they can not
address the forgetting problem at all. For example, the most
intuitive way is to add different loss functions as
L1
couple =LER+LSCR. (7)
It is a two-stage strategy for offline learning [62], which is
unsuitable for OCL due to timeliness requirements. Another
way is to add anchor-to-sample pairs to cross-entropy loss as
L2
couple =E(x,y)âˆ¼BâˆªB M
[âˆ’log(exp(oy)P
câˆˆC1:texp(oc) +P
jâˆˆJexp(âŸ¨z,zjâŸ©/Ï„))].(8)
in the work [7]. Both of them cannot take advantage of each
replay manner to address the forgetting issue of OCL. Since
they ignore that the key is to select anchor-to-proxy pairs in
a contrastive way and train by the selective pairs.
Therefore, the proxy-based contrastive replay (PCR) frame-
work is proposed by replacing the samples of anchor-to-sample
pairs with proxies in the contrastive-based loss. Specifically,
the model is trained by learning samples of new classes and
replaying samples of old classes in the training procedure.
For each step, given current samples B, it randomly retrieves
previous samples BMfrom the memory buffer. Besides, these
samples are spliced together for the batch of training. Then,
the model is optimized by
LPCR =E(x,y)âˆ¼BâˆªB M[âˆ’log(exp(oy)P
câˆˆC1:tkcexp(oc))].(9)
where kcis the number of samples belonging to class cin the
training batch. Different from existing studies, its way of com-
puting categorical probability is changed for each mini-batch.
Finally, it updates the memory buffer by reservoir sampling
strategy, which can ensure that the probability of each sample
being extracted is equal. Conveniently, the memory buffer has
a fixed size, no matter how large the sample amount is.
The inference procedure is different from the training pro-
cedure. Each testing sample xkobtains its class probability
distribution by Equation (1). And PCR performs the inference
prediction to xkwith highest probability as
yâˆ—
k= arg max
cpc, yâˆˆC1:t. (10)

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
IV. A NALYSIS FOR PROXY -BASED CONTRASTIVE REPLAY
A. Gradient Analysis
As stated in the previous work [9], gradient analysis is of
great help in exploring forgetting problems and existing work.
Specifically, the gradient of the softmax classifier for xis
âˆ‚L
âˆ‚ <z,wc>=(
(pyâˆ’1)/Ï„, c =y
(pc)/Ï„, c Ì¸=y. (11)
Training xbelongs to class yprovides the positive gradient
âˆ’Î·(pyâˆ’1)/Ï„for the proxy of class yas<z,wy>=<
z,wy>âˆ’Î·(pyâˆ’1)/Ï„, and propagates the negative gradient
âˆ’Î·(pc)/Ï„to the other proxies as <z,wc>=<z,wc>
âˆ’Î·(pc)/Ï„, where the learning rate Î·is positive. If training as
Equation (2), the proxies of new classes receive more positive
gradients and the others obtain more negative gradients. It
causes the proxies of new classes to be close to the samples
of new classes, while the proxies of old classes are far away
from the samples of new classes. Hence, it is easy to classify
most samples into new classes, causing the forgetting problem.
If replaying as Equation (3), the proxies of old classes can
obtain more positive gradient, and the proxies of new classes
receive more negative gradient. Although the forgetting can be
mitigated, its effect is still limited by the imbalanced samples.
To explore the reliability of PCR, we analyze its gradient
propagation process. Similar to Equation (11), the gradient of
the classifier for a sample xin PCR is stated as Theorem 1.
Theorem 1: In PCR, the gradient for all proxies is
âˆ‚LPCR
âˆ‚ <z,wc>=(
(kypâˆ—
yâˆ’1)/Ï„, c =y
(kcpâˆ—
c)/Ï„, c Ì¸=y, (12)
where
pâˆ—
y=exp(oy)P
câˆˆC1:tkjexp(oc). (13)
When replaying, the number of classes is relatively small and
the number of samples is relatively large for new classes;
the number of classes is relatively large and the number of
samples is relatively small for old classes. It means that in a
training batch, the number of samples for new classes tends
to be multiple, while the one for old classes tends to be small,
or even non-existent. Based on this situation, Equation (12)
demonstrates two advantages of PCR for anti-forgetting.
For one thing, the gradient only influences the classes
whose associated samples exist in the same training batch. For
example, when learning a current sample xbelongs to class
y, the gradient for the proxy of class c(cÌ¸=y)isâˆ’Î·(kcpâˆ—
c)/Ï„
if there are kcsamples belonging to class cin the same batch.
Otherwise, if kc= 0, the proxy of class cdoes not participate
in the gradient propagation. Due to the lower probability of
old classes appearing in the training batch, the proxies of old
classes will receive fewer negative gradients from the new
classes. Thus, the model will keep more historical knowledge.
Another thing is that the gradient can be affected by the
number of samples in a training batch. Conveniently, we define
sum y=X
câˆˆC1:t,cÌ¸=ykcexp(oc).(14)With this representation, the Equation (12) can be changed to
âˆ‚LPCR
âˆ‚ <z,wc>=ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³(kyexp(oy)
sum y+kyexp(oy)âˆ’1)/Ï„, c =y
kcexp(oc)
sum y+kyexp(oy)/Ï„, c Ì¸=y.(15)
We take the training batch including ki(kiâ‰¥1)samples of
new class iandkj(kj= 1) samples of other old classes jas
an example. Training new class ipropagates the gradient as
âˆ‚LPCR
âˆ‚ <z,wc>=ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³âˆ’sum i
sum i+kiexp(oi)/Ï„, c =i
exp(oc)
sum i+kiexp(oi)/Ï„, c =j. (16)
At this moment, the larger the kiis, the smaller the
1
sumi+kiexp(oi)is, and the smaller the gradient value is. It
means that the larger kican reduce the positive gradient for
the proxy of new class iand the negative gradient for the proxy
of old classes j. Similarly, training any old class jpropagates
the gradient as
âˆ‚LPCR
âˆ‚ <z,wc>=ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³(exp(oc)
sum i+kiexp(oi)âˆ’1)/Ï„, c =j
(1âˆ’sum i
sum i+kiexp(oi))/Ï„, c =i.(17)
In this situation, the larger the kiis, the smaller the
1
sumi+kiexp(oi)is, and the larger the gradient value is. With
larger ki, both the positive gradient for old class jand
the negative gradient for the new class ican be increased.
Therefore, PCR helps old classes acquire some advantages
in the propagation of gradient, and the gradient propagation
efficiency of these selected proxies will be greatly improved.
B. Limitation Analysis
Based on the gradient analysis, we can find that PCR can
help the model alleviate the phenomenon of catastrophic for-
getting. It selects partial proxies instead of all to learn current
samples and replay previous samples. In such a learning way,
the old classes whose associated samples are not selected can
avoid the bias caused by unbalanced gradient competition.
However, PCR remains suboptimal due to several unad-
dressed limitations. Firstly, PCR exclusively considers the
relationships of anchor-to-proxy but ignores the relations of
anchor-to-sample, leading to the model losing some important
semantic information. Anchor-to-sample pairs play a crucial
role in enhancing the modelâ€™s feature extraction capability in
large training batches, a vital aspect of PCR. Secondly, the
simplistic setting of the temperature coefficient Ï„in PCR lacks
sophistication. As the temperature coefficient influences PCRâ€™s
gradient propagation process, employing a static constant is in-
sufficient to ensure the modelâ€™s generalization ability. Thirdly,
those old classes that have been selected to learn together with
new classes still face the issue of bias in the training process.
It means that the modelâ€™s anti-forgetting ability still can be
further improved. In a word, PCR can be improved into a
more holistic and effective approach.

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
V. H OLISTIC PROXY -BASED CONTRASTIVE REPLAY
In this work, we develop a more comprehensive and ef-
fective method called holistic proxy-based contrastive replay
(HPCR). HPCR consists of a contrastive component, a tem-
perature component, and a distillation component, resolving
the limitations of PCR in sequence.
A. Contrastive Component
The contrastive component conditionally incorporates the
anchor-to-sample pairs to PCR and improves the modelâ€™s abil-
ity of feature extraction. The anchor-to-sample pairs contain
richer fine-grained semantic information compared to anchor-
to-proxy pairs. Although extracting the relations of anchor-
to-sample is easily limited by the number of samples in a
training batch, it can still play a crucial role when the batch
size is sufficient. Hence, the contrastive component of HPCR
is denoted as
LPCR C=E(x,y)âˆ¼BâˆªB M[âˆ’1
|P|X
pâˆˆP
log(exp(oy) +s(N)Â·exp(âŸ¨z,zpâŸ©/Ï„)P
câˆˆC1:tkcexp(oc) +P
jâˆˆJs(N)Â·exp(âŸ¨z,zjâŸ©/Ï„))].(18)
Here, s(N)is a self-defined step function used to control the
importance of anchor-to-sample pairs, and it is defined as
s(N) =(
0, N < N min
1, Nâ‰¥Nmin. (19)
Nis the number of samples in the current training batch,
andNmin is a hyper-parameter used to determine whether
to use anchor-to-sample pairs. When the size of the training
batch is small, the correlation between samples is prone to
noise affecting the performance of the model. Therefore,
their importance should be set to 0. At this point, PCR C
degenerates into PCR; otherwise, the importance of these
samples should be set to 1 to improve the modelâ€™s feature
extraction ability. Compared to the PCR, PCR Ccan not only
mine the relations of anchor-to-proxy, but also explore the
relations of anchor-to-sample. It increases inter-class distance
as well as decreases intra-class distance.
B. Temperature Component
1) The Analysis of Temperature Coefficient: In Equation
(12), the gradient propagation is influenced by Ï„, which is
called the temperature coefficient. In a proxy-based classifier,
the norm of vectors has been proved to be unfriendly to
continual learning and thus abandoned [61]. However, the
retained similarity of vectors âŸ¨z,wâŸ©has a value range of
[âˆ’1,1], which brings certain difficulties to the optimization
of the model. Hence, Ï„inpâˆ—
cis used to control the strength
ofâŸ¨z,wâŸ©, which is vital to find the optimal solution.
Inspired by [63], we define a relative penalty r(âŸ¨z,wcâŸ©)on
negative proxy wc(cÌ¸=y)for the sample (x, y)by
r(âŸ¨z,wcâŸ©) =|âˆ‚L
âˆ‚<z,wc>|
|âˆ‚L
âˆ‚<z,wy>|=kcexp(oc)P
jÌ¸=ykjexp(oj), cÌ¸=y.(20)It can reflect the strength of penalties on all negative samples.
AsÏ„increases, the distribution of r(âŸ¨z,wcâŸ©)is relatively
uniform, and the model tends to treat all negative proxies with
the same strength. On the contrary, the model focuses on the
negative proxies with higher similarities when Ï„decreases.
Furthermore, within a reasonable interval, smaller Ï„is more
suitable for replaying old classes, while larger Ï„is more
effective for learning new classes in OCL. Generally, the
number of samples in old classes is small, while the number of
samples in new classes is large in OCL. It leads to the samples
of old classes being more likely to become negative samples
with shorter distances, while the samples of new classes tend
to become negative samples with larger distances. As a result,
large Ï„will prompt the model to pay attention to the samples
of new classes with larger distances, making it easy to ignore
samples from old classes. In contrast, small Ï„will urge the
model to focus on the samples of old classes with shorter
distances, resulting in the information loss from new classes.
Finally, as seen in Equation (12), Ï„affects the process in
two parts. First, it directly changes the value of the gradient
by1/Ï„(denoted as G part). Second, it smooths the probability
distribution pâˆ—
cof samples to implicitly adjust the gradient
(denoted as P part). Moreover, the P part has a more complex
impact than the G part due to its stronger non-linearity.
2) The Influence of Temperature Coefficient: The perfor-
mance of PCR would be highly sensitive to Ï„. To validate
this viewpoint, we conduct experiments for PCR with different
values of Ï„, and the results are shown in Figure 2. In addition
to the final accuracy rate of all classes, we also report the
ones of new and old classes separately. In Figure 2(a), we
can make the following observations: 1) The performances
of all, old, and new classes are sensitive to Ï„. For example,
when Ï„changes from 0.01 to 0.2, the performance of PCR
in all aspects would first increase and then decrease. 2) The
sensitivity of old and new classes w.r.t. Ï„is different, and their
bestÏ„are different. The performance on new classes reaches
the best result when Ï„âˆˆ[0.1,0.2]while the performance on
old classes reaches the best result when Ï„âˆˆ[0.0,0.1].
At the same time, we also demonstrate the performance
when different Ï„are taken in different parts. In Figure 2(b),
the lines of green, blue, and red denote the performance when
Ï„changes in the G part, the P part, and both parts. For the G
part, we keep Ï„= 0.09forpâˆ—
cand only change the value of Ï„
for1/Ï„. On the contrary, Ï„is set as 0.09 for 1/Ï„and changed
from 0.0 to 0.5 in the P part. Comparing the results of the
two cases, we find that the performance of the model is more
sensitive to changes in Ï„in the G part. Meanwhile, the model
exhibits greater fluctuations when Ï„reaches the optimal value
in the P part. Although the performance of the P part appears
stable with changes in Ï„, the accuracy of each class actually
changes. Conveniently, we report the best Ï„for each class on
Split CIFAR10 in the P part when the buffer size is 100. As
stated in Table II, to achieve the highest accuracy, the value of
Ï„for each class is different. In a word, the setting of Ï„in the
P part is sensitive, and a static value is more suitable for it.
While a dynamic value is more suitable for the G part since it
allows the model to consider different classes. Therefore, the
setting of Ï„for these two parts should be decoupled.

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Temperature51015202530354045Average Accuracy
all classes
new classes
old classes
(a) The performance on different classes
0.0 0.1 0.2 0.3 0.4 0.5
Temperature0.200.220.240.260.280.300.320.340.36Average Accuracy
 in the G part
 in the P part
 in both parts
 (b) The performance on different parts
Fig. 2. The performance of PCR on Split CIFAR100 (buffer size=1000) with different value of Ï„.
TABLE II
THE BEST Ï„FOR EACH CLASS ON SPLIT CIFAR10 ( BUFFER SIZE =100).
class 0 1 2 3 4 5 6 7 8 9
type old old old old old old old old new new
Ï„ 0.01 0.14 0.16 0.17 0.12 0.20 0.06 0.09 0.14 0.2
3) Decoupled Temperature Coefficient: To this end, we set
different values of Ï„for the P and G parts separately. For one
thing, since the performance of the model is more sensitive
to the P part, we set it to a static constant value. Meanwhile,
we obtain a moderate value to balance the modelâ€™s attention to
new and old classes using the grid search method. For another
thing, due to the different optimal Ï„for each class, we set a
dynamic one for the G part. However, it is highly difficult to
find an optimal Ï„for each class at any time, since the real-time
requirement of OCL is high. In such a situation, the dynamic
temperature is set as a robust function independent of the class,
and the function is denoted as
Ï„(s) = (Ï„maxâˆ’Ï„min)Ã—(1 +cos(2Ï€s/S ))/2 +Ï„min,(21)
where sis the current step, Sis the cycle length, Ï„max is
the upper bound, and Ï„min is the lower bound. With such
a periodic dynamic function, the model can select a wide
range of different temperature values during the OCL process.
It improves the accuracy of each class and thus enhances
the overall generalization ability of the model. Therefore, the
Equation (18) can be improved to
LPCR CT=E(x,y)âˆ¼BâˆªB M[âˆ’1
|P|X
pâˆˆPÏ„
Ï„(s)
log(exp(oy) +s(N)Â·exp(âŸ¨z,zpâŸ©/Ï„)P
câˆˆC1:tkcexp(oc) +P
jâˆˆJs(N)Â·exp(âŸ¨z,zjâŸ©/Ï„))].(22)
Moreover, its corresponding gradient is transformed into
âˆ‚LPCR CT
âˆ‚ <z,wc>=(
(kypâˆ—
yâˆ’1)/Ï„(s), c=y
(kcpâˆ—
c)/Ï„(s), cÌ¸=y. (23)
C. Distillation Component
As a method of knowledge distillation [64], DER [65] tries
to retain the logits output obtained from the model during
learning as historical knowledge for sample replaying. This is
to improve the diversity of historical knowledge and avoid the
problem of knowledge singularity caused by saving old modelsto store historical knowledge. However, existing DER has to
use all proxies to calculate a loss function of distillation. If
distilling as DER, the proxies excluded by PCR will continue
to participate in gradient propagation.
To avoid this situation, we propose a novel distillation
method called proxy-based contrastive distillation (PCD). Sim-
ilar to PCR, the categorical probability in PCD only uses the
proxies that appear in the training batch. Specifically, when
saving a current sample into the memory buffer, we not only
save its instance xand label y, but also store its logits output of
classifier oâˆ—. If the sample is selected as a previous sample to
replay, its oâˆ—can be used to keep richer historical knowledge.
Hence, we can calculate the PCD loss by minimizing the
Euclidean distance between the weighted logits as
LPCD =E(x,y,oâˆ—)âˆ¼BM[âˆ’C1:tX
ckc(ocâˆ’oâˆ—
c)2]. (24)
In addition to the distribution distillation of previous sam-
ples, relation distillation between samples is also necessary.
Since the relation of samples not only reflects intra-class
information, but also includes inter-class correlation. Hence,
we propose to compute sample-based contrastive distillation
(SCD) by zâˆ—in a training batch as
LSCD=E(x,y,zâˆ—)âˆ¼BM
[âˆ’X
iâˆˆJ(exp(âŸ¨z,ziâŸ©/Ï„)P
jâˆˆJexp(âŸ¨z,zjâŸ©/Ï„)logexp(âŸ¨zâˆ—,zâˆ—
iâŸ©/Ï„)P
jâˆˆJexp(âŸ¨zâˆ—,zâˆ—
jâŸ©/Ï„)],(25)
where Jis the indices set of samples except for anchor xin
the same batch BM. With these two distillation loss functions,
the model can transfer more positive gradients to the proxies
of old classes and more negative gradients to the proxies of
new classes. As a result, the modelâ€™s ability of anti-forgetting
can be improved.
D. Overall Procedure
Combining these three novel components, PCR will be
enhanced as HPCR. Compared to PCR, HPCR will greatly
improve the modelâ€™s capability of feature extraction, gener-
alization, and anti-forgetting in the meantime. Its objective
function is denoted as
LHPCR =LPCR CT+Î±LPCD +Î²LSCD, (26)

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
Algorithm 1 Holistic Proxy-based Contrastive Replay
Input : Dataset D, Learning Rate Î», Scale factor Ï„
Output :Network Parameters Î¸
Initialize :Memory Buffer M â† {} , Network Parameters Î¸=
{Î¦,W}
1:fortâˆˆ {1,2, ..., T}do
2: //Training Procedure
3: formini-batch B âŠ‚Dtdo
4: for(x, y)âˆˆ B do
5: z=h(x;Î¦)
6: o=f(z;W)
7: (x, y)â†(x, y,o,z)
8: end for
9: BMâ†RandomRetrieval (M)
10: B âˆª B Mâ†Concat ([B,BM])
11: LPCR CTâ†Equation (22)
12: LPCDâ†Equation (24)
13: LSCDâ†Equation (25)
14: Lâ†Equation (26)
15: Î¸â†Î¸+Î»âˆ‡Î¸L
16: M â† ReservoirUpdate (M,B)
17: end for
18: //Inference Procedure
19: mâ†number of testing samples
20: forkâˆˆ {1,2, ..., m}do
21: yâˆ—
kâ†arg maxcpc, câˆˆC1:t
22: end for
23: return Î¸
24:end for
where Î±andÎ²are hyper-parameters of scale for PCD and
SCD, respectively. Furthermore, the whole training and in-
ference procedures of HPCR are summarized in Algorithm
1. The main differences between PCR and HPCR lie in the
training procedure. The first difference is in the memory buffer.
In addition to saving the old samples and their corresponding
labels, HPCR also saves the logits and feature vectors obtained
during the training process of the old samples. The second one
is in the loss function. The objective function of HPCR is more
integrated, which is conducive to comprehensively improving
the overall performance of the model.
VI. P ERFORMANCE EVALUATION
A. Experiment Setup
1) Datasets: We conduct experiments on four image recog-
nition datasets for evaluation. Split CIFAR10 [68] is split into
5 tasks, and each task contains 2 classes. Split CIFAR100 [68]
as well as Split MiniImageNet [69] are organized into 10 tasks,
and each task is made up of samples from 10 classes. Split
TinyImageNet [70], which contains 200 classes, is divided into
20 tasks, and each task contains 10 classes.
2) Evaluation Metrics: We need to measure the perfor-
mance of the model for OCL. We first define ai,j(j <=i)
as the accuracy evaluated on the held-out test samples of the
jth task after the network has learned the training samples inthe first itasks. Similar with [43], we can further acquire the
average accuracy rate
Ai=1
iiX
j=1ai,j, (27)
at the ith task. If there are Ttasks, we can get the final
accuracy rate ATand the averaged anytime accuracy (AAA)
AAA =1
TTX
t=1At, (28)
which measures the performance of the learning process [11].
Meanwhile, we denote the average forgetting rate as
Fi=1
iâˆ’1iâˆ’1X
j=1fi,j, (29)
where fk,j= max lâˆˆ{1,2,...,kâˆ’1}(al,j)âˆ’ak,j. And FTis
equivalent to the final forgetting rate.
3) Implementation Details: The basic setting of the back-
bone is the same as the recent work [11]. In detail, we take the
Reduced ResNet18 (the number of filters is 20) as the feature
extractor for all datasets. The classifier is NCM classifier for
SCR and softmax classifier for other methods. During the
training phase, the network, which is randomly initialized
rather than pre-trained, is trained with the SGD optimizer,
and the learning rate is set as 0.1. For all datasets, the classes
are shuffled before division. And we set the memory buffer
with different sizes for all datasets. The model receives 10
current samples from the data stream and 10 previous samples
from the memory buffer at a time irrespective of the size of
the memory. Moreover, we employ a combination of various
augmentation techniques to get the augmented images. And the
usage of data augmentation is fair for all methods. Concerning
hyperparameters of PCR, we select them on a validation set
that is obtained by sampling 10% of the training set. As for
the testing phase, we set 256 as the batch size of validation.
B. Overall Performance
In this section, we conduct experiments to compare with
various state-of-the-art baselines. The overall performance of
PCR is advanced compared to all baselines. Moreover, HPCR
obtains significantly improved performance on four datasets.
Comparison on final accuracy. HPCR has consistently
achieved the best performance compared to all baseline meth-
ods. Table III shows the final accuracy rate for all datasets.
All reported scores are the average score of 10 runs with
95% confidence interval. In a multitude of experimental sce-
narios, where each dataset comprises four memory buffers of
varying sizes, HPCR consistently outshines all other methods.
In particular, it exhibits even more remarkable performance
compared to PCR. Specifically, HPCR shows a significant
advancement, incorporating contrastive, temperature, and dis-
tillation components to further refine the PCR framework. For
instance, on Split Cifar10, HPCR outperforms PCR by note-
worthy margins of 2.9%, 3.1%, and 4.1% for memory buffer
sizes of 100, 200, and 500, respectively. These compelling
results unequivocally highlight the crucial significance of the

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
TABLE III
FINAL ACCURACY RATE(â†‘). T HE BEST SCORES FOR OUR METHODS ARE IN BOLDFACE ,AND THE BEST SCORES FOR BASELINES ARE UNDERLINED .
Datasets Split CIFAR10 (%) Split CIFAR100 (%) Split MiniImageNet (%) Split TinyImageNet (%)
Buffer 100 200 500 1000 2000 5000 1000 2000 5000 2000 4000 10000
IID 58.1 Â±2.5 17.3 Â±0.8 18.2 Â±1.1 17.3 Â±1.7
IID++ [11] 64.2 Â±2.1 23.5 Â±0.8 20.7 Â±1.0 19.1 Â±1.3
FINE-TUNE 17.9 Â±0.4 5.9Â±0.2 4.3Â±0.2 4.3Â±0.2
ER [41] (NeurIPS2019) 33.8 Â±3.241.7 Â±2.846.0 Â±3.5 17.6 Â±0.919.7 Â±1.620.9 Â±1.2 13.4 Â±0.916.5 Â±0.916.2 Â±1.7 6.1Â±0.5 8.5Â±0.7 8.9Â±0.6
GSS [44] (NeurIPS2019) 23.1 Â±3.928.3 Â±4.636.3 Â±4.1 16.9 Â±1.419.0 Â±1.820.1 Â±1.1 13.9 Â±1.014.6 Â±1.115.5 Â±0.9 / / /
GMED [45] (NeurIPS2021) 32.8 Â±4.743.6 Â±5.152.5 Â±3.9 18.8 Â±0.721.1 Â±1.223.0 Â±1.5 15.3 Â±1.318.0 Â±0.819.6 Â±1.0 7.0Â±0.9 10.2 Â±0.711.3 Â±1.2
MIR [42] (NeurIPS2019) 34.8 Â±3.340.3 Â±3.342.6 Â±1.7 18.1 Â±0.720.3 Â±1.621.6 Â±1.7 14.8 Â±1.117.2 Â±0.817.2 Â±1.2 4.9Â±0.6 6.3Â±0.6 6.4Â±0.7
ASER [43] (AAAI2021) 33.7 Â±3.731.6 Â±3.442.1 Â±3.0 16.1 Â±1.117.7 Â±0.718.9 Â±1.0 13.8 Â±0.916.1 Â±0.918.1 Â±1.1 5.3Â±0.3 9.6Â±0.8 8.1Â±0.8
A-GEM [66] (ICLR2019) 17.5 Â±1.717.4 Â±2.117.9 Â±0.7 5.6Â±0.5 5.4Â±0.7 4.6Â±1.0 4.7Â±1.1 5.0Â±2.3 4.8Â±0.8 / / /
ER-WA [67] (CVPR2020) 36.9 Â±2.942.5 Â±3.448.6 Â±2.7 21.7 Â±1.223.6 Â±0.924.0 Â±1.8 17.1 Â±0.918.9 Â±1.418.5 Â±1.5 9.2Â±0.7 11.6 Â±1.311.4 Â±1.6
DER++ [65] (NeurIPS2020) 40.9 Â±1.445.3 Â±1.752.8 Â±2.2 17.2 Â±1.119.5 Â±1.220.2 Â±1.3 14.8 Â±0.716.1 Â±1.315.5 Â±1.3 6.8Â±0.4 8.7Â±0.7 9.2Â±0.7
SS-IL [10] (ICCV2021) 36.8 Â±2.142.2 Â±1.444.8 Â±1.6 21.9 Â±1.124.5 Â±1.424.7 Â±1.0 19.7 Â±0.921.7 Â±1.024.4 Â±1.6 13.2 Â±0.815.2 Â±1.018.7 Â±0.7
SCR [46] (CVPR-W2021) 35.0 Â±2.945.4 Â±1.055.7 Â±1.6 16.2 Â±1.318.2 Â±0.819.3 Â±1.0 14.7 Â±1.916.8 Â±0.618.6 Â±0.5 9.9Â±0.4 12.6 Â±0.611.1 Â±0.5
ER-DVC [49] (CVPR2022) 36.3 Â±2.645.4 Â±1.450.6 Â±2.9 19.7 Â±0.722.1 Â±0.924.1 Â±0.8 15.4 Â±0.717.2 Â±0.819.1 Â±0.9 7.6Â±0.5 9.9Â±0.7 10.4 Â±0.7
OCM [48] (ICML2022) 44.4 Â±1.549.9 Â±1.855.8 Â±2.3 20.6 Â±1.222.1 Â±1.022.7 Â±1.4 13.6 Â±0.716.5 Â±0.519.2 Â±0.7 8.6Â±0.8 11.9 Â±0.912.1 Â±0.6
ER-ACE [11] (ICLR2022) 44.3 Â±1.549.7 Â±2.454.9 Â±1.4 23.1 Â±0.824.8 Â±0.927.0 Â±1.2 20.3 Â±1.324.8 Â±1.126.2 Â±1.0 9.5Â±0.5 13.7 Â±0.718.2 Â±0.5
OBC [50] (ICLR2023) 40.5 Â±2.146.4 Â±1.653.4 Â±2.3 22.1 Â±0.624.0 Â±1.326.3 Â±1.0 16.4 Â±1.419.5 Â±1.521.6 Â±1.4 9.6Â±0.5 11.4 Â±0.914.6 Â±1.1
OnPro [53] (ICCV2023) 46.0 Â±1.652.9 Â±2.059.5 Â±0.7 17.4 Â±0.819.4 Â±0.421.6 Â±0.6 13.7 Â±0.916.8 Â±0.718.1 Â±1.1 10.2 Â±0.813.6 Â±0.716.5 Â±0.4
UER [52] (ACMMM2023) 41.5 Â±1.449.2 Â±1.755.8 Â±1.9 24.6 Â±0.827.0 Â±0.529.6 Â±1.1 21.9 Â±1.325.1 Â±1.127.5 Â±1.1 10.6 Â±0.513.8 Â±0.717.2 Â±0.6
HPCR (Algorithm 1) 48.3 Â±1.553.4 Â±1.460.1 Â±1.1 29.1 Â±0.730.7 Â±0.533.7 Â±0.6 27.1 Â±0.629.9 Â±0.731.3 Â±0.7 16.4 Â±0.319.5 Â±0.822.1 Â±0.5
,â†’PCR 45.4 Â±1.350.3 Â±1.556.0 Â±1.2 25.6 Â±0.627.4 Â±0.629.3 Â±1.1 24.2 Â±0.927.2 Â±1.228.4 Â±0.9 12.2 Â±0.917.4 Â±0.719.6 Â±0.8
TABLE IV
FINAL ACCURACY RATE(â†‘)ONSPLIT CIFAR100 UNDER THE EXPERIMENTAL SETTING OF [48].
Methods SCR [48] (%) OCM [48] (%) OnPro [53] (%) PCR (%) HPCR (%)
Buffer 1000 2000 5000 1000 2000 5000 1000 2000 5000 1000 2000 5000 1000 2000 5000
AT 26.5 Â±0.231.6 Â±0.536.5 Â±0.228.1 Â±0.335.0 Â±0.442.4 Â±0.530.0 Â±0.435.9 Â±0.642.7 Â±0.929.3 Â±0.636.3 Â±0.946.5 Â±0.833.6 Â±0.640.5 Â±1.549.1 Â±1.2
TABLE V
AVERAGED ANYTIME ACCURACY (â†‘). T HE BEST SCORES FOR OUR METHODS ARE IN BOLDFACE ,AND THE BEST FOR BASELINES ARE UNDERLINED .
Datasets Split CIFAR10 (%) Split CIFAR100 (%) Split MiniImageNet (%) Split TinyImageNet (%)
Buffer 100 200 500 1000 2000 5000 1000 2000 5000 2000 4000 10000
ER [41] (NeurIPS2019) 52.7 Â±2.752.1 Â±3.157.0 Â±2.7 25.5 Â±1.126.3 Â±1.425.3 Â±1.3 20.1 Â±0.922.2 Â±1.219.0 Â±1.5 11.9 Â±0.912.8 Â±0.913.0 Â±0.9
MIR (NeurIPS2019) 49.5 Â±3.450.8 Â±4.253.4 Â±4.0 25.8 Â±0.926.5 Â±2.126.3 Â±1.8 20.9 Â±1.622.8 Â±1.520.5 Â±1.3 9.1Â±0.6 9.6Â±0.6 9.9Â±0.7
ER-WA [67] (CVPR2020) 54.1 Â±4.954.4 Â±5.258.1 Â±4.2 29.4 Â±1.729.3 Â±1.927.6 Â±2.1 23.8 Â±1.123.5 Â±1.321.7 Â±1.2 14.8 Â±0.514.8 Â±1.513.6 Â±0.8
DER++ [65] (NeurIPS2020) 57.1 Â±2.658.0 Â±2.562.1 Â±4.3 26.7 Â±1.426.0 Â±1.526.2 Â±1.9 21.2 Â±0.921.3 Â±1.320.9 Â±1.7 13.3 Â±0.713.2 Â±0.913.7 Â±1.2
SS-IL [10] (ICCV2021) 50.5 Â±1.652.9 Â±1.254.5 Â±1.2 25.1 Â±1.427.2 Â±1.226.8 Â±1.1 22.9 Â±1.522.8 Â±0.925.5 Â±1.8 17.1 Â±0.818.6 Â±0.919.3 Â±0.9
SCR [46] (CVPR-W2021) 56.7 Â±2.859.4 Â±3.367.9 Â±2.9 25.1 Â±2.628.7 Â±2.427.5 Â±1.5 22.9 Â±1.324.5 Â±1.825.5 Â±1.1 17.1 Â±0.718.7 Â±0.717.3 Â±1.0
ER-DVC [49] (CVPR2022) 56.2 Â±3.157.0 Â±3.661.2 Â±3.5 27.7 Â±0.928.5 Â±1.127.8 Â±0.9 21.9 Â±1.523.8 Â±1.223.2 Â±1.1 13.3 Â±0.614.6 Â±0.814.0 Â±0.9
OCM [48] (ICML2022) 56.7 Â±3.057.2 Â±1.165.3 Â±4.2 26.1 Â±1.926.5 Â±2.326.3 Â±1.9 20.1 Â±0.921.9 Â±0.923.7 Â±1.4 16.2 Â±1.217.9 Â±1.118.8 Â±0.9
ER-ACE [11] (ICLR2022) 60.8 Â±2.762.0 Â±2.965.3 Â±2.6 31.5 Â±1.433.2 Â±1.133.3 Â±1.5 27.8 Â±1.830.7 Â±0.829.8 Â±1.2 18.1 Â±0.521.7 Â±0.523.4 Â±0.4
OBC [50] (ICLR2023) 57.3 Â±1.061.3 Â±1.065.0 Â±1.3 30.0 Â±1.130.1 Â±2.030.9 Â±1.9 22.4 Â±0.723.9 Â±1.825.9 Â±1.5 15.2 Â±0.716.1 Â±1.217.9 Â±1.1
OnPro [53] (ICCV2023) 60.9 Â±1.863.7 Â±2.367.9 Â±2.4 21.9 Â±0.523.4 Â±0.824.4 Â±0.7 19.5 Â±0.621.3 Â±0.821.9 Â±0.8 17.6 Â±1.119.9 Â±0.822.4 Â±0.9
UER [52] (ACMMM2023) 59.2 Â±3.660.8 Â±2.265.7 Â±2.4 32.4 Â±0.833.9 Â±1.234.3 Â±1.8 29.1 Â±1.430.8 Â±1.730.9 Â±1.6 17.7 Â±0.320.0 Â±0.921.1 Â±0.6
HPCR (Algorithm (1)) 65.2 Â±3.166.0 Â±2.771.0 Â±2.6 40.1 Â±1.440.6 Â±1.042.4 Â±1.4 36.0 Â±1.337.9 Â±1.138.2 Â±1.2 26.6 Â±0.828.3 Â±0.929.4 Â±0.6
,â†’PCR 60.7 Â±3.060.3 Â±2.866.7 Â±3.0 33.7 Â±1.334.2 Â±1.934.7 Â±1.9 30.6 Â±0.932.8 Â±1.033.0 Â±1.1 21.3 Â±0.524.3 Â±0.725.5 Â±0.5
components introduced in this article, as they play a pivotal
role in elevating the performance of PCR.
Moreover, we conduct a comprehensive comparative anal-
ysis between five methods, utilizing the experimental setup
outlined in [48]. In this particular study, the model is config-
ured as ResNet18 with 64 filters, and 64 samples are retrieved
from the memory buffer for each training batch. The training
process employs the Adam optimizer with a learning rate
0.001. It is important to note that these experimental conditions
differ from those employed in our own study. As presented
in Table IV, HPCR demonstrates remarkable superiority over
SCR, OCM, OnPro, and PCR under different experimental
settings. This evidence further reinforces the effectiveness and
robustness of HPCR in comparison to existing approaches.
Comparison on learning process. In the meantime, HPCRperforms the best overall throughout the entire learning pro-
cess. Specifically, we report the averaged anytime accuracy
for all datasets in Table V. The experimental results show
that HPCR consistently outperforms other baselines during the
whole learning process. For a more detailed comparison, we
also report the accuracy of each learning task in Figure 3,
which can further validate the overall performance of the
methods. The results show that the performance of PCR in the
first few tasks does not outperform other baselines. However,
its improvement becomes more and more visible as the number
of tasks increases, proving its power to overcome CF. For
instance, PCR does not perform best in the first task, but it
demonstrates outstanding advantages in the remaining tasks
on Split CIFAR100. Fortunately, this issue has been resolved
in HPCR. The comparison of the red and orange lines shows

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
1 2 3 4 5
Learning Tasks4050607080Average Accuracy
ER
DER++
ER-WA
MIR
SCRER-ACE
ER-DVC
UER
PCR
HPCR
(a) Split CIFAR10 (Buffer=200)
1 2 3 4 5 6 7 8 9 10
Learning Tasks2030405060Average Accuracy
ER
DER++
ER-WA
MIR
SCRER-ACE
ER-DVC
UER
PCR
HPCR (b) Split CIFAR100 (Buffer=1000)
1 2 3 4 5 6 7 8 9 10
Learning Tasks20304050Average Accuracy
ER
DER++
ER-WA
MIR
SCRER-ACE
ER-DVC
UER
PCR
HPCR
(c) Split MiniImageNet (Buffer=1000)
1234567891011121314151617181920
Learning Tasks1020304050Average Accuracy
ER
DER++
ER-WA
MIR
SCRER-ACE
ER-DVC
UER
PCR
HPCR (d) Split TinyImageNet (Buffer=2000)
Fig. 3. Average accuracy rate on observed learning tasks on all datasets.
1 2 3 4 5 6 7 8 9 10
Learning Tasks25303540455055Average Accuracy
SCR
ER-ACE
UERPCR
HPCR w/o HD
HPCR
(a) Performance on novel knowledge
1 2 3 4 5 6 7 8 9 10
Learning Tasks010203040Average Accuracy
SCR
ER-ACE
UERPCR
HPCR w/o HD
HPCR (b) Performance on historical knowledge
Fig. 4. Average accuracy rate on observed learning tasks Split MiniImageNet while the buffer size is 5000.
TABLE VI
FINAL FORGETTING RATE(â†“)FOR FOUR DATASETS .
Datasets Split CIFAR10 (%) Split CIFAR100 (%) Split MiniImageNet (%) Split TinyImageNet (%)
Buffer 100 200 500 1000 2000 5000 1000 2000 5000 2000 4000 10000
SCR [46] (CVPR-W2021) 51.7 Â±4.339.6 Â±2.027.7 Â±2.9 13.2 Â±1.312.9 Â±1.512.5 Â±0.6 11.6 Â±1.911.5 Â±1.410.4 Â±1.0 10.1 Â±0.6 8.7Â±0.6 9.0Â±0.8
ER-ACE [11] (ICLR2022) 18.7 Â±1.117.1 Â±2.513.8 Â±1.7 10.5 Â±0.8 9.8Â±0.5 7.9Â±1.5 9.8Â±1.1 7.3Â±1.4 6.2Â±0.9 9.2Â±0.9 7.5Â±0.4 6.4Â±0.7
UER [52] (ACMMM2023) 38.5 Â±3.430.7 Â±3.624.0 Â±2.5 13.2 Â±0.7 9.8Â±1.0 7.0Â±0.4 12.7 Â±1.2 8.4Â±0.7 6.1Â±0.9 13.3 Â±0.9 9.0Â±1.0 6.7Â±1.1
HPCR (Algorithm (1)) 28.5 Â±1.220.6 Â±2.415.5 Â±1.8 15.7 Â±0.711.1 Â±0.5 9.3Â±0.8 15.4 Â±0.611.1 Â±0.7 8.6Â±0.4 20.5 Â±0.813.6 Â±1.1 9.2Â±0.7
,â†’PCR 25.8 Â±2.319.4 Â±3.317.1 Â±2.4 15.5 Â±0.911.7 Â±2.4 9.5Â±0.8 13.7 Â±1.110.8 Â±1.4 9.5Â±1.5 20.6 Â±0.813.7 Â±0.710.2 Â±0.6
that HPCR still performs well in the first few tasks. Therefore,
our approaches have a stronger ability to resist forgetting.
Comparison on knowledge balance. Finally, HPCR also
has significant advantages in balancing historical and novel
knowledge. Before this, we calculate the final forgetting rate
of the model when using different methods. As reported in
Table VI, the existing methods, especially ER-ACE, have
shown outstanding performance in this evaluation metric. This
is because ER-ACE tends to learn less, making it easily
forget less. To verify it, we record the accuracy of novel and
historical knowledge in each task for part of effective methods
on Split MiniImageNet. As demonstrated in Figure 4, as ER-
ACE learns less novel knowledge, its performance on the final
forgetting rate can be better.Actually, we should not only focus on the modelâ€™s ability
to retain historical knowledge, but also ensure the modelâ€™s
ability to quickly learn novel knowledge. Although SCR,
ER-ACE, and UER can improve the anti-forgetting ability
of the model, they tend to limit the generalization ability
of the model. As the historical knowledge is consolidated,
the learning performance of the model on novel knowledge
becomes very poor. Different from existing studies, our model
can not only effectively alleviate the phenomenon of CF, but
also reduce the decline of the model at the generalization level
as much as possible. Compared with HPCR (red line) and PCR
(orange line), it is not difficult to find that there is a significant
improvement in the memory ability of old knowledge.

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
TABLE VII
FINAL ACCURACY RATE(HIGHER IS BETTER )FOR ABLATION STUDIES . ER ACTS AS A BASELINE METHOD .
Datasets Split CIFAR10 Split CIFAR100 Split MiniImageNet Split TinyImageNet
Buffer 100 200 500 1000 2000 5000 1000 2000 5000 2000 4000 10000
ER 33.8 Â±3.241.7 Â±2.846.0 Â±3.5 17.6 Â±0.919.7 Â±1.620.9 Â±1.2 13.4 Â±0.916.5 Â±0.916.2 Â±1.7 6.1Â±0.5 8.5Â±0.7 8.9Â±0.6
Couple (Equation (7)) 38.4 Â±3.043.2 Â±3.849.1 Â±3.9 179 Â±0.8 19.7 Â±0.821.6 Â±1.0 17.9 Â±0.719.9 Â±0.820.9 Â±1.6 5.2Â±0.4 10.7 Â±0.712.8 Â±0.5
Couple (Equation (8)) 34.1 Â±2.341.8 Â±3.946.3 Â±2.8 18.7 Â±0.920.7 Â±0.821.8 Â±0.9 16.3 Â±0.818.7 Â±0.719.8 Â±0.9 6.2Â±0.3 10.1 Â±0.611.0 Â±0.8
PCR 45.4 Â±1.350.3 Â±1.556.0 Â±1.2 25.6 Â±0.627.4 Â±0.629.3 Â±1.1 24.2 Â±0.927.2 Â±1.228.4 Â±0.9 12.2 Â±0.917.4 Â±0.719.6 Â±0.8
PCR+HC ( PCR C) 41.9 Â±2.048.3 Â±2.455.8 Â±1.2 24.1 Â±0.525.9 Â±0.527.3 Â±0.7 21.8 Â±0.824.5 Â±0.926.3 Â±1.0 11.1 Â±1.213.6 Â±0.715.3 Â±1.2
PCR+HC+HT ( PCR CT) 47.4 Â±2.251.3 Â±1.657.7 Â±1.1 27.2 Â±0.529.3 Â±0.731.6 Â±0.9 25.6 Â±1.028.5 Â±0.529.8 Â±0.7 14.8 Â±0.618.8 Â±0.520.8 Â±0.5
PCR CT+PCD 48.0 Â±1.952.4 Â±1.259.1 Â±1.2 28.6 Â±0.830.1 Â±0.733.0 Â±0.5 26.6 Â±0.529.3 Â±0.630.9 Â±0.7 15.6 Â±0.319.2 Â±0.521.8 Â±0.6
PCR CT+PCD+SCD (HPCR) 48.3 Â±1.553.4 Â±1.460.1 Â±1.1 29.1 Â±0.730.7 Â±0.533.7 Â±0.6 27.1 Â±0.629.9 Â±0.731.3 Â±0.7 16.4 Â±0.319.5 Â±0.822.1 Â±0.5
0 10 20 30 40 50 60 70 80 90 100
Batch Size of Previous Samples28303234363840Average Accuracy
 PCR
PCR+HC
Fig. 5. Final Accuracy Rate on Split CIFAR100 (buffer size=5000) with
different batch sizes of previous samples.
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Dimension Ratio0.000.050.100.150.200.250.300.350.40Accuracy
PCR+HC
PCR
Fig. 6. Final Accuracy Rate on Split CIFAR100 (buffer size=5000) with
different ratios of feature dimension.
C. Ablation Study
In this section, we decompose HPCR into several compo-
nents and demonstrate their functions. We conduct experi-
ments with different settings and record their final accuracy
rate in Table VII. In addition to ER, we also include the two
combination methods denoted as Equation (7) and Equation
(8). The experimental results show that they are not significant
since they do not solve the problem of forgetting.
Contrastive component (HC) conditionally incorporates
anchor-to-sample pairs to PCR. Although it provides more
knowledge about the relationships of samples, its performance
is limited by a small batch size of training samples, as the
â€œPCR+HCâ€ indicated in Table VII. Meanwhile, we conduct
experiments on Split CIFAR100 using PCR with and without
HC. The results revealed in Figure 5 show that the anchor-
to-sample pairs can reduce the performance of PCR when the
batch size of the training samples is small ( <60), and vice
versa. Hence, the usage of anchor-to-sample pairs should be
controlled by a stage function denoted as Equation (19). To
further explore HC, we conduct principal component analysis
(PCA) [71] on the features zextracted by the model. As
demonstrated in Figure 6, we report the final accuracy rate
on Split CIFAR100 with different ratios of feature dimension
when the batch size of previous samples is 100. In cases
where the dimension ratio is low, â€œPCR+HCâ€ performs muchTABLE VIII
ANALYSIS RESULTS ON CIFAR10 ( BUFFER SIZE =100) WITH AND
WITHOUT TEMPERATURE COMPONENT .
Different
SettingFinal
AccuracyCumulative
gradientLossDifference
(all classes)Difference
(new classes)
w/o HT 42.83 88.27/-95.39 1.8439 0.0042 0.0056
with HT 47.66 64.28/-70.40 1.6710 0.0025 0.0040
Lossold classes (w/o HT) new classes oldclasses (HT)
Fig. 7. Illustration of a steep region not using HT and a flat region using HT.
better than PCR, and this advantage is maintained as the ratio
increases. It means that the feature extraction capability of the
model has been improved with the help of HC, allowing for
the extraction of more critical features.
Temperature component (HT) is to improve the modelâ€™s
generalization ability. As stated in Table VII, with the help of
HT, the performance of PCR is greatly improved. Compared
with â€œPCRâ€ and â€œPCR w/o HDâ€ in Figure 4, we find that
the main improvement brought by HT lies in learning novel
knowledge by the model. To explore the role of HT on
gradient propagation and model convergence, we analyze the
training process of PCR on the CIFAR10 with and without
HT, and some important indicators are recorded in Table VIII.
The results show the final accuracy can be improved by HT
since the model can produce less gradient for old classes
(64.28<88.27) and new classes ( 70.40<95.39) during
the gradient propagation process. In the meantime, we find
that HT can make the model converge to a region with a
lower loss function value ( 1.6710 <1.8439 ) and a flatter
landscape during the optimization process. To validate it, we
add uniformly distributed noise to the parameters of the final
model, causing the position of the model to change in the
parameter space. At the same time, we record the differences
in training loss before and after the model changes. After
repeating the above operations 1000 times, we calculate the
average of these 1000 results. The results indicate that the
model has relatively small differences using HT for all classes
(0.0025 <0.0042 ) and new classes ( 0.0040 <0.0056 ).
Therefore, the optimal solution of the model using HT is in
a relatively flat region. As displayed in Figure 7, in such a

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
TABLE IX
ACCURACY RATE(HIGHER IS BETTER )OFHPCR ONSPLIT CIFAR100
WITH DIFFERENT Ï„min ANDÏ„max WHEN THE CYCLE LENGTH IS 500.
Ï„minÏ„max0.12 0.14 0.16 0.18 0.2
0.11 25.4, 34.9 25.0, 36.9 26.2, 36.6 26.3, 36.6 25.9, 36.7
0.09 25.5, 35.8 26.0, 36.7 26.1, 36.9 26.5, 37.6 26.1, 37.8
0.07 26.6, 36.8 26.3, 37.1 26.4, 37.6 26.2, 37.3 27.0, 38.1
0.05 26.6, 37.1 26.7, 37.7 27.7, 38.5 26.9, 37.9 26.5, 37.6
TABLE X
ACCURACY RATE(HIGHER IS BETTER )OFHPCR ONSPLIT CIFAR100
WITH DIFFERENT CYCLE LENGTHS S.
S 50 100 125 250 500 1000
Final Accuracy 24.7 25.7 25.8 26.7 27.7 23.2
Averaged Accuracy 34.6 36.5 36.8 37.8 38.5 32.4
TABLE XI
FINAL ACCURACY RATE(HIGHER IS BETTER )OF EACH TASK ON SPLIT
CIFAR10 USING DISTILLATION AS DER ORPCD.
Task 1 2 3 4 5 Average
DER [65] 0.3480 0.2205 0.4320 0.5315 0.7500 0.4564
PCD (ours) 0.3695 0.2270 0.488 0.4960 0.7405 0.4642
flat region, the model, where the initial solution is Î¸1, can
obtain a better solution Î¸2than Î¸3for both old and new
classes. Moreover, the selection of hyperparameters related
to Equation (21) is shown in Tables IX and X. And we set
Ï„max= 0.16,Ï„min= 0.05, and S= 500 based on the results.
Distillation component (HD) is to improve the anti-
forgetting ability of the model. Compared with â€œHPCRâ€ and
â€œHPCR w/o HDâ€ in Figure 4, we can find that HPCR performs
significantly better on historical knowledge with the help of
HD. Shown as Table VII, the distillation component, which
contains SCD and PCD, further improves the performance
of PCR. To improve the overall performance of the model,
both SCD and PCD are essential. For one thing, SCD can
better balance the distribution of historical knowledge while
improving the modelâ€™s ability to anti-forgetting. Actually, there
is an imbalance between old classes in OCL. As shown in
Table XI, although tasks 1-4 are all old tasks, the performance
of the model on these tasks is different. Compared with
the distillation method in DER, PCD can better balance the
performance of the model on old tasks (Tasks 1-4). For another
thing, SCD, which directly propagates gradient for feature
extractor, can produce less gradient for all proxies than PCD.
As shown in Figure 8, if HPCR only uses SCD, the gradient
is relatively small, and the final performance is better.
In summary, based on these three components, HPCR has a
comprehensive improvement compared to PCR. At the same
time, all of the components are tailored to break the limitations
of PCR and have their own originality.
D. Further study
1) Pre-trained Backbone: With the development of pre-
trained models, continual learning tends to take a pre-trained
model as the initial model. Hence, we further conduct ex-
periments with a pre-trained model on Split CIFAR100. For
fairness, we consider the ViT-B/16-IN21K in [72] as the pre-
trained model, where each frozen transformer block contains
a trainable residual term. Meanwhile, the training batch size
is set as 10 and the epoch is set as 1 for all methods. As
1 2 3 4 5 6 7 8 9 10
Index of Classes40
20
0204060Cumulative Gradients
using PCD only (46.4) using SCD only (47.1)Fig. 8. Cumulative gradient for all proxies using different distillation loss
functions when the model learns new classes (9/10) on Split CIFAR10.
1 2 3 4 5 6 7 8 9 10
Learning T asks5060708090100Average Accuracy
SLCA
EASE
ER (100)
HPCR (100)HPCR (500)
HPCR (1000)
HPCR (2000)
HPCR (5000)
Fig. 9. Average accuracy Rate on Split CIFAR100 with different methods.
0.0 0.25 0.5 0.75 1.00.00.250.50.751.0
(a) ER (previous samples=10)
0.0 0.25 0.5 0.75 1.00.00.250.50.751.0 (b) ER (previous samples=100)
0.0 0.25 0.5 0.75 1.00.00.250.50.751.0
(c) HPCR (previous samples=10)
0.0 0.25 0.5 0.75 1.00.00.250.50.751.0 (d) HPCR (previous samples=100)
Fig. 10. 2D t-SNE visualization of feature embeddings for the testing samples
at the end of the training on Split CIFAR10 (buffer size=500).
stated in Figure 9, SLCA [73] and EASE [72] are non-replay
methods while ER replays with a 100-size buffer. Compared
with ER (100) and HPCR (100), we can find that HPCR can
also play a role when using a pre-trained backbone. Moreover,
the performance of HPCR will gradually surpass SLCA and
EASE based on pre-trained models as the buffer size increases
(from 100 to 5000), further confirming its superiority for OCL.
2) Visualization: Visualization can help reflect the role of
these components more intuitively. We train the model by ER
and HPCR on Split CIFAR10 with 500 sizes of memory buffer,
respectively. At the end of the training, we report their 2D t-

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
TABLE XII
THE COMPUTATION AND MEMORY BUDGET OF DIFFERENT METHODS .
Method ER ER-ACE OCM OnPro UER PCR HPCR
Computation ( CD)1 1 16 16 1 1 1
Memory (Model) 1 1 2 1 1 1 1
Memory (Data) 1 1 1 1 1 1 Table XIII
TABLE XIII
THE MEMORY (DATA )ON DIFFERENT DATASETS FOR HPCR.
Dataset CIFAR10 CIFAR100 MiniImageNet TinyImageNet
Sample Size 32Ã—32Ã—3 32Ã—32Ã—3 84Ã—84Ã—3 64Ã—64Ã—3
Feature Size 160 160 640 640
Logits Size 10 100 100 200
Memory (Data) 1.06 1.08 1.03 1.07
Buffer Size 100 1000 1000 2000
Accuracy (Control) 47.7 Â±1.5 27.4 Â±0.5 26.5 Â±0.6 15.2 Â±0.4
Accuracy (HPCR) 48.3 Â±1.5 29.1 Â±0.7 27.1 Â±0.6 16.4 Â±0.3
SNE [74] visualization of feature embeddings for all testing
samples, as shown in Figure 10. The stars are proxies while
others are samples for all classes. The results demonstrate
that HPCR not only achieves great performance, but also
can indeed improve the distinguishability of samples in the
embedding space.
3) Computation and Memory Cost: For OCL, the overhead
of memory and computing resources is crucial, since it will
affect the practicality of a method. Therefore, we analyze
the memory and computational cost of different methods,
as reported in Table XII. 1) Similar to [75], we use CD
to compare the computation of different methods. The CD
is denoted as a relative complexity between the stream
and an underlying continual learning method. For example,
when current samples come, ER can immediately learn them
and then predict unknown samples, resulting in a relative
complexity of 1. Since UER, ER-ACE, PCR, and HPCR
only modify the loss objective of ER, their computational
complexities are equivalent to 1. However, OCM and OnPro
require an additional 16 augmentation to the original data,
making it require 16 times the FLOPs needed by ER. Thus,
its computational complexity is 16, which limits the ability
of real-time prediction. 2) We set the memory budget of the
model for ER as 1. Due to the OCM method requiring an
additional model to be saved for knowledge distillation, its
relative budget is 2 and others are 1. 3) We evaluate the
memory usage of different methods by considering the data
size of all samples stored in the buffer. As seen in Table XIII,
since HPCR has additional storage requirements for features
and logits embedding, it necessitates relatively more memory
space ( >1). However, the additional memory does not occupy
a high proportion ( <0.1). If the additional memory is used
to store old samples, the performance of the model is shown
in the row of â€œAccuracy (Control)â€. The results demonstrate
that storing features and logits as HPCR yields significant
benefits, justifying its worth. In summary, HPCR emerges
as a straightforward yet efficacious approach for real-time
prediction in OCL scenarios.
VII. C ONCLUSION
In this paper, we develop a more holistic OCL method
named HPCR based on PCR, which mainly consists of three
components. The contrastive component conditionally intro-
duces anchor-to-sample pairs to PCR, improving the featureextraction ability of PCR when the training batch size is large.
The temperature component decouples the influence of tem-
perature on gradients in two parts and effectively improves the
generalization ability of the model. The distillation component
improves the anti-forgetting ability of the model through PCD
and SCD. Extensive experiments on four datasets demonstrate
the superiority of HPCR over various state-of-the-art methods.
REFERENCES
[1] L. Wang, X. Zhang, H. Su, and J. Zhu, â€œA comprehensive survey of
continual learning: Theory, method and application,â€ IEEE Transactions
on Pattern Analysis and Machine Intelligence , 2024. 1
[2] Y . Liu, X. Hong, X. Tao, S. Dong, J. Shi, and Y . Gong, â€œModel behavior
preserving for class-incremental learning,â€ IEEE Transactions on Neural
Networks and Learning Systems , 2022. 1
[3] Y . Kong, L. Liu, H. Chen, J. Kacprzyk, and D. Tao, â€œOvercoming
catastrophic forgetting in continual learning by exploring eigenvalues of
hessian matrix,â€ IEEE Transactions on Neural Networks and Learning
Systems , 2023. 1
[4] H. Zhao, H. Wang, Y . Fu, F. Wu, and X. Li, â€œMemory-efficient class-
incremental learning for image classification,â€ IEEE Transactions on
Neural Networks and Learning Systems , vol. 33, no. 10, pp. 5966â€“5977,
2021. 1
[5] H. Lin, S. Feng, X. Li, W. Li, and Y . Ye, â€œAnchor assisted experience
replay for online class-incremental learning,â€ IEEE Transactions on
Circuits and Systems for Video Technology , 2022. 1
[6] Z. Mai, R. Li, J. Jeong, D. Quispe, H. Kim, and S. Sanner, â€œOnline
continual learning in image classification: An empirical survey,â€ Neuro-
computing , vol. 469, pp. 28â€“51, 2022. 1, 3
[7] X. Yao, Y . Bai, X. Zhang, Y . Zhang, Q. Sun, R. Chen, R. Li, and
B. Yu, â€œPcl: Proxy-based contrastive learning for domain generalization,â€
inProceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 7097â€“7107. 1, 3, 4
[8] T. Mensink, J. Verbeek, F. Perronnin, and G. Csurka, â€œDistance-based
image classification: Generalizing to new classes at near-zero cost,â€
IEEE transactions on pattern analysis and machine intelligence , vol. 35,
no. 11, pp. 2624â€“2637, 2013. 1
[9] H. Lin, B. Zhang, S. Feng, X. Li, and Y . Ye, â€œPcr: Proxy-based
contrastive replay for online class-incremental continual learning,â€ in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , June 2023, pp. 24 246â€“24 255. 1, 2, 4,
5
[10] H. Ahn, J. Kwak, S. Lim, H. Bang, H. Kim, and T. Moon, â€œSs-il:
Separated softmax for incremental learning,â€ in Proceedings of the
IEEE/CVF International Conference on Computer Vision , 2021, pp.
844â€“853. 1, 4, 9
[11] L. Caccia, R. Aljundi, N. Asadi, T. Tuytelaars, J. Pineau, and
E. Belilovsky, â€œNew insights on reducing abrupt representation change
in online continual learning,â€ in International Conference on Learning
Representations , 2021. 1, 3, 4, 8, 9, 10
[12] A. Prabhu, P. H. Torr, and P. K. Dokania, â€œGdumb: A simple approach
that questions our progress in continual learning,â€ in European confer-
ence on computer vision . Springer, 2020, pp. 524â€“540. 3
[13] M. Delange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis,
G. Slabaugh, and T. Tuytelaars, â€œA continual learning survey: Defying
forgetting in classification tasks,â€ IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2021. 3
[14] Y . Zhao, D. Saxena, and J. Cao, â€œAdaptcl: Adaptive continual learning
for tackling heterogeneity in sequential datasets,â€ IEEE Transactions on
Neural Networks and Learning Systems , 2023. 3
[15] D.-W. Zhou, Y . Yang, and D.-C. Zhan, â€œLearning to classify with
incremental new class,â€ IEEE Transactions on Neural Networks and
Learning Systems , vol. 33, no. 6, pp. 2429â€“2443, 2021. 3
[16] G. Sun, Y . Cong, Y . Zhang, G. Zhao, and Y . Fu, â€œContinual multiview
task learning via deep matrix factorization,â€ IEEE transactions on neural
networks and learning systems , vol. 32, no. 1, pp. 139â€“150, 2020. 3
[17] H. Koh, D. Kim, J.-W. Ha, and J. Choi, â€œOnline continual learning
on class incremental blurry task configuration with anytime inference,â€
arXiv preprint arXiv:2110.10031 , 2021. 3
[18] R. Aljundi, K. Kelchtermans, and T. Tuytelaars, â€œTask-free continual
learning,â€ in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2019, pp. 11 254â€“11 263. 3

--- PAGE 14 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14
[19] J. Bang, H. Kim, Y . Yoo, J.-W. Ha, and J. Choi, â€œRainbow memory:
Continual learning with a memory of diverse samples,â€ in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2021, pp. 8218â€“8227. 3
[20] Q. Gao, Z. Luo, D. Klabjan, and F. Zhang, â€œEfficient architecture search
for continual learning,â€ IEEE Transactions on Neural Networks and
Learning Systems , 2022. 3
[21] Z. Miao, Z. Wang, W. Chen, and Q. Qiu, â€œContinual learning with filter
atom swapping,â€ in International Conference on Learning Representa-
tions , 2021. 3
[22] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska
et al. , â€œOvercoming catastrophic forgetting in neural networks,â€ Pro-
ceedings of the national academy of sciences , vol. 114, no. 13, pp.
3521â€“3526, 2017. 3
[23] P. Dhar, R. V . Singh, K.-C. Peng, Z. Wu, and R. Chellappa, â€œLearning
without memorizing,â€ in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2019, pp. 5138â€“5146. 3
[24] P. Buzzega, M. Boschini, A. Porrello, and S. Calderara, â€œRethinking
experience replay: a bag of tricks for continual learning,â€ in 2020 25th
International Conference on Pattern Recognition (ICPR) . IEEE, 2021,
pp. 2180â€“2187. 3
[25] X. Su, S. Guo, T. Tan, and F. Chen, â€œGenerative memory for lifelong
learning,â€ IEEE transactions on neural networks and learning systems ,
vol. 31, no. 6, pp. 1884â€“1898, 2019. 3
[26] M. Dedeoglu, S. Lin, Z. Zhang, and J. Zhang, â€œContinual learning
of generative models with limited data: From wasserstein-1 barycenter
to adaptive coalescence,â€ IEEE Transactions on Neural Networks and
Learning Systems , 2023. 3
[27] G.-M. Park, S.-M. Yoo, and J.-H. Kim, â€œConvolutional neural network
with developmental memory for continual learning,â€ IEEE Transactions
on Neural Networks and Learning Systems , vol. 32, no. 6, pp. 2691â€“
2705, 2020. 3
[28] H. Cha, J. Lee, and J. Shin, â€œCo2l: Contrastive continual learning,â€ in
Proceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 9516â€“9525. 3
[29] A. Chaudhry, A. Gordo, P. Dokania, P. Torr, and D. Lopez-Paz, â€œUsing
hindsight to anchor past knowledge in continual learning,â€ in Proceed-
ings of the AAAI Conference on Artificial Intelligence , vol. 35, no. 8,
2021, pp. 6993â€“7001. 3
[30] Q. Lao, X. Jiang, M. Havaei, and Y . Bengio, â€œA two-stream continual
learning system with variational domain-agnostic feature replay,â€ IEEE
Transactions on Neural Networks and Learning Systems , vol. 33, no. 9,
pp. 4466â€“4478, 2021. 3
[31] S. Ho, M. Liu, L. Du, L. Gao, and Y . Xiang, â€œPrototype-guided memory
replay for continual learning,â€ IEEE Transactions on Neural Networks
and Learning Systems , 2023. 3
[32] Y . Wang, Z. Huang, and X. Hong, â€œS-prompts learning with pre-trained
transformers: An occamâ€™s razor for domain incremental learning,â€ Ad-
vances in Neural Information Processing Systems , vol. 35, pp. 5682â€“
5695, 2022. 3
[33] Z. Wang, Z. Zhang, C.-Y . Lee, H. Zhang, R. Sun, X. Ren, G. Su, V . Perot,
J. Dy, and T. Pfister, â€œLearning to prompt for continual learning,â€ in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2022, pp. 139â€“149. 3
[34] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C.-Y . Lee, X. Ren,
G. Su, V . Perot, J. Dy et al. , â€œDualprompt: Complementary prompting for
rehearsal-free continual learning,â€ in European Conference on Computer
Vision . Springer, 2022, pp. 631â€“648. 3
[35] J. S. Smith, L. Karlinsky, V . Gutta, P. Cascante-Bonilla, D. Kim,
A. Arbelle, R. Panda, R. Feris, and Z. Kira, â€œCoda-prompt: Contin-
ual decomposed attention-based prompting for rehearsal-free continual
learning,â€ in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2023, pp. 11 909â€“11 919. 3
[36] S. Wang, X. Li, J. Sun, and Z. Xu, â€œTraining networks in null space
of feature covariance for continual learning,â€ in Proceedings of the
IEEE/CVF conference on Computer Vision and Pattern Recognition ,
2021, pp. 184â€“193. 3
[37] T.-C. Kao, K. Jensen, G. van de Ven, A. Bernacchia, and G. Hennequin,
â€œNatural continual learning: success is a journey, not (just) a destination,â€
Advances in neural information processing systems , vol. 34, pp. 28 067â€“
28 079, 2021. 3
[38] D. Deng, G. Chen, J. Hao, Q. Wang, and P.-A. Heng, â€œFlattening
sharpness for dynamic gradient projection memory benefits continual
learning,â€ Advances in Neural Information Processing Systems , vol. 34,
pp. 18 710â€“18 721, 2021. 3[39] Y . Guo, W. Hu, D. Zhao, and B. Liu, â€œAdaptive orthogonal projection for
batch and online continual learning,â€ Proceedings of AAAI-2022 , vol. 2,
2022. 3
[40] F. Zhu, Z. Cheng, X.-y. Zhang, and C.-l. Liu, â€œClass-incremental learning
via dual augmentation,â€ Advances in Neural Information Processing
Systems , vol. 34, 2021. 3
[41] D. Rolnick, A. Ahuja, J. Schwarz, T. Lillicrap, and G. Wayne, â€œExpe-
rience replay for continual learning,â€ Advances in Neural Information
Processing Systems , vol. 32, 2019. 3, 4, 9
[42] R. Aljundi, E. Belilovsky, T. Tuytelaars, L. Charlin, M. Caccia, M. Lin,
and L. Page-Caccia, â€œOnline continual learning with maximal interfered
retrieval,â€ Advances in Neural Information Processing Systems , vol. 32,
pp. 11 849â€“11 860, 2019. 3, 9
[43] D. Shim, Z. Mai, J. Jeong, S. Sanner, H. Kim, and J. Jang, â€œOnline
class-incremental continual learning with adversarial shapley value,â€ in
Proceedings of the AAAI Conference on Artificial Intelligence , vol. 35,
no. 11, 2021, pp. 9630â€“9638. 3, 8, 9
[44] R. Aljundi, M. Lin, B. Goujaud, and Y . Bengio, â€œGradient based sample
selection for online continual learning,â€ Advances in neural information
processing systems , vol. 32, 2019. 3, 9
[45] X. Jin, A. Sadhu, J. Du, and X. Ren, â€œGradient-based editing of memory
examples for online task-free continual learning,â€ Advances in Neural
Information Processing Systems , vol. 34, 2021. 3, 9
[46] Z. Mai, R. Li, H. Kim, and S. Sanner, â€œSupervised contrastive replay:
Revisiting the nearest class mean classifier in online class-incremental
continual learning,â€ in Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , 2021, pp. 3589â€“3599. 3, 4,
9, 10
[47] H. Yin, P. Li et al. , â€œMitigating forgetting in online continual learning
with neuron calibration,â€ Advances in Neural Information Processing
Systems , vol. 34, 2021. 3
[48] Y . Guo, B. Liu, and D. Zhao, â€œOnline continual learning through mutual
information maximization,â€ in International Conference on Machine
Learning . PMLR, 2022, pp. 8109â€“8126. 3, 9
[49] Y . Gu, X. Yang, K. Wei, and C. Deng, â€œNot just selection, but
exploration: Online class-incremental continual learning via dual view
consistency,â€ in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2022, pp. 7442â€“7451. 3, 9
[50] A. Chrysakis and M.-F. Moens, â€œOnline bias correction for task-
free continual learning,â€ in The Eleventh International Conference on
Learning Representations . 3, 9
[51] Y . Zhang, B. Pfahringer, E. Frank, A. Bifet, N. J. S. Lim, and Y . Jia, â€œA
simple but strong baseline for online continual learning: Repeated aug-
mented rehearsal,â€ Advances in Neural Information Processing Systems ,
vol. 35, pp. 14 771â€“14 783, 2022. 3
[52] H. Lin, S. Feng, B. Zhang, H. Qiao, X. Li, and Y . Ye, â€œUer: A heuristic
bias addressing approach for online continual learning,â€ arXiv preprint
arXiv:2309.04081 , 2023. 3, 9, 10
[53] Y . Wei, J. Ye, Z. Huang, J. Zhang, and H. Shan, â€œOnline prototype
learning for online continual learning,â€ in Proceedings of the IEEE/CVF
International Conference on Computer Vision , 2023, pp. 18 764â€“18 774.
3, 9
[54] N. Michel, G. Chierchia, R. Negrel, and J.-F. Bercher, â€œLearning
representations on the unit sphere: Investigating angular gaussian and
von mises-fisher distributions for online continual learning,â€ in The 38th
Annual AAAI Conference on Artificial Intelligence . arXiv, 2024. 3
[55] B. Kim, M. Seo, and J. Choi, â€œOnline continual learning for interactive
instruction following agents,â€ arXiv preprint arXiv:2403.07548 , 2024. 3
[56] X. Yang, P. Zhou, and M. Wang, â€œPerson reidentification via structural
deep metric learning,â€ IEEE transactions on neural networks and
learning systems , vol. 30, no. 10, pp. 2987â€“2998, 2018. 3
[57] T. Milbich, K. Roth, B. Brattoli, and B. Ommer, â€œSharing matters for
generalization in deep metric learning,â€ IEEE Transactions on Pattern
Analysis and Machine Intelligence , vol. 44, no. 1, pp. 416â€“427, 2020.
3
[58] I. Elezi, J. Seidenschwarz, L. Wagner, S. Vascon, A. Torcinovich,
M. Pelillo, and L. Leal-Taixe, â€œThe group loss++: A deeper look into
group loss for deep metric learning,â€ IEEE Transactions on Pattern
Analysis and Machine Intelligence , 2022. 3
[59] W. Zheng, J. Lu, and J. Zhou, â€œDeep metric learning with adaptively
composite dynamic constraints,â€ IEEE Transactions on Pattern Analysis
and Machine Intelligence , 2023. 3
[60] K. Roth, O. Vinyals, and Z. Akata, â€œNon-isotropy regularization for
proxy-based deep metric learning,â€ in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , 2022, pp.
7420â€“7430. 3

--- PAGE 15 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15
[61] S. Hou, X. Pan, C. C. Loy, Z. Wang, and D. Lin, â€œLearning a
unified classifier incrementally via rebalancing,â€ in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2019, pp. 831â€“839. 4, 6
[62] T. Li, P. Cao, Y . Yuan, L. Fan, Y . Yang, R. S. Feris, P. Indyk, and
D. Katabi, â€œTargeted supervised contrastive learning for long-tailed
recognition,â€ in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2022, pp. 6918â€“6928. 4
[63] F. Wang and H. Liu, â€œUnderstanding the behaviour of contrastive loss,â€
inProceedings of the IEEE/CVF conference on computer vision and
pattern recognition , 2021, pp. 2495â€“2504. 6
[64] G. Hinton, O. Vinyals, J. Dean et al. , â€œDistilling the knowledge in a
neural network,â€ arXiv preprint arXiv:1503.02531 , vol. 2, no. 7, 2015.
7
[65] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara, â€œDark
experience for general continual learning: a strong, simple baseline,â€
Advances in neural information processing systems , vol. 33, pp. 15 920â€“
15 930, 2020. 7, 9, 12
[66] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny, â€œEfficient
lifelong learning with a-gem,â€ in International Conference on Learning
Representations , 2018. 9
[67] B. Zhao, X. Xiao, G. Gan, B. Zhang, and S.-T. Xia, â€œMaintaining dis-
crimination and fairness in class incremental learning,â€ in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2020, pp. 13 208â€“13 217. 9
[68] A. Krizhevsky, G. Hinton et al. , â€œLearning multiple layers of features
from tiny images,â€ 2009. 8
[69] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al. , â€œMatching net-
works for one shot learning,â€ Advances in neural information processing
systems , vol. 29, pp. 3630â€“3638, 2016. 8
[70] Y . Le and X. Yang, â€œTiny imagenet visual recognition challenge,â€ CS
231N , vol. 7, no. 7, p. 3, 2015. 8
[71] S. Wold, K. Esbensen, and P. Geladi, â€œPrincipal component analysis,â€
Chemometrics and intelligent laboratory systems , vol. 2, no. 1-3, pp.
37â€“52, 1987. 11
[72] D.-W. Zhou, H.-L. Sun, H.-J. Ye, and D.-C. Zhan, â€œExpandable subspace
ensemble for pre-trained model-based class-incremental learning,â€ in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2024, pp. 23 554â€“23 564. 12
[73] G. Zhang, L. Wang, G. Kang, L. Chen, and Y . Wei, â€œSlca: Slow learner
with classifier alignment for continual learning on a pre-trained model,â€
inProceedings of the IEEE/CVF International Conference on Computer
Vision , 2023, pp. 19 148â€“19 158. 12
[74] L. Van der Maaten and G. Hinton, â€œVisualizing data using t-sne.â€ Journal
of machine learning research , vol. 9, no. 11, 2008. 13
[75] Y . Ghunaim, A. Bibi, K. Alhamoud, M. Alfarra, H. A. A. K. Hammoud,
A. Prabhu, P. H. Torr, and B. Ghanem, â€œReal-time evaluation in online
continual learning: A new paradigm,â€ arXiv preprint arXiv:2302.01047 ,
2023. 13
Huiwei Lin is is currently a Postdoctoral Fellow at
The Chinese University of Hong Kong. He received
the B.S. degree from the South China University
of Technology, China, in 2017, the M.S. degree
from the Harbin Institute of Technology, Shenzhen,
China, in 2020, and the Ph.D. degree from the
Harbin Institute of Technology, Shenzhen, China, in
2024. His current research interests include continual
learning and knowledge reasoning.
Shanshan Feng is currently a senior research scien-
tist at the Centre for Frontier AI Research, Institute
of High Performance Computing, the Agency for
Science, Technology and Research (A*STAR), Sin-
gapore. He received the Ph.D. degree from Nanyang
Technological University, Singapore in 2017, and
his B.S. degree from University of Science and
Technology of China in 2011. His current research
interests include spatial-temporal data mining, social
graph learning, and recommender systems.
Baoquan Zhang is currently an assistant professor
with the School of Computer Science and Technol-
ogy, Harbin Institute of Technology, Shenzhen. He
received the B.S. degree from the Harbin Institute
of Technology, Weihai, China, in 2015, the M.S.
degree from the Harbin Institute of Technology,
Harbin, China, in 2017, and the Ph.D. degree from
the Harbin Institute of Technology, Shenzhen, in
2023. His current research interests include meta
learning, few-shot learning, machine learning, and
data mining.
Xutao Li is currently a Professor with the School of
Computer Science and Technology, Harbin Institute
of Technology, Shenzhen, China. He received the
Ph.D. and Master degrees in Computer Science from
Harbin Institute of Technology in 2013 and 2009,
and the Bachelor from Lanzhou University of Tech-
nology in 2007. His research interests include data
mining, machine learning, graph mining, and social
network analysis, especially tensor-based learning,
and mining algorithms.
Yunming Ye is currently a Professor with the School
of Computer Science and Technology, Harbin Insti-
tute of Technology, Shenzhen, China. He received
the PhD degree in Computer Science from Shanghai
Jiao Tong University, Shanghai, China, in 2004. His
research interests include data mining, text mining,
and ensemble learning algorithms.
APPENDIX
PROOF OF THEOREM 1
By chain-rule, we have
âˆ‚LPCR
âˆ‚ <z,wc>= (âˆ‚L
âˆ‚pâˆ—câˆ‚pâˆ—
c
âˆ‚f(z;W))âˆ‚f(z;W)
âˆ‚ <z,wc>. (30)
Ifc=y, we can get
âˆ‚LPCR
âˆ‚pâˆ—câˆ‚pâˆ—
c
âˆ‚f(z;W)
=âˆ’1
pâˆ—yexp(oy)(P
câˆˆC1:texp(oc)âˆ’kyexp(oy))
(P
câˆˆC1:tkcexp(oc))2
=âˆ’1
pâˆ—y(pâˆ—
yâˆ’kypâˆ—
ypâˆ—
y) =kypâˆ—
yâˆ’1.(31)
Otherwise, if cÌ¸=y, we can get
âˆ‚LPCR
âˆ‚pâˆ—câˆ‚pâˆ—
c
âˆ‚f(z;W)
=âˆ’exp(oc)
pâˆ—cexp(oy)âˆ’kcexp(oc)exp(oy)
(P
câˆˆC1:tkcexp(oc))2
=âˆ’1
pâˆ—c(âˆ’kcpâˆ—
cpâˆ—
c) =kcpâˆ—
c.(32)
Since âˆ‚f(z;W)/âˆ‚ < z,wc>= 1/Ï„, the final gradient can
be denoted as
âˆ‚LPCR
âˆ‚ <z,wc>=(
(kypâˆ—
yâˆ’1)/Ï„, c =y
(kcpâˆ—
c)/Ï„, c Ì¸=y. (33)
