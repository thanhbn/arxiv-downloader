# 2305.13477.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/contrastive/2305.13477.pdf
# File size: 749466 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Look-back Decoding for Open-Ended Text Generation
Nan Xu♢, Chunting Zhou♠, Asli Celikyilmaz♠, Xuezhe Ma♢
♢University of Southern California,♠Meta AI
♢{nanx,xuezhema}@usc.edu,♠{chuntinz,aslic}@meta.com
Abstract
Given a prefix (context), open-ended gener-
ation aims to decode texts that are coherent,
which do not abruptly drift from previous top-
ics, and informative, which do not suffer from
undesired repetitions. In this paper, we propose
Look-back , an improved decoding algorithm
that leverages the Kullback–Leibler divergence
to track the distribution distance between cur-
rent and historical decoding steps. Thus Look-
back can automatically predict potential repeti-
tive phrase and topic drift, and remove tokens
that may cause the failure modes, restricting
the next token probability distribution within a
plausible distance to the history. We perform
decoding experiments on document continua-
tion and story generation, and demonstrate that
Look-back is able to generate more fluent and
coherent text, outperforming other strong de-
coding methods significantly in both automatic
and human evaluations1.
1 Introduction
Despite the impressive success on generating flu-
ent and accurate sentences for low-entropy tasks
such as summarization or translation, large-scale
language models (LLMs) still suffer from serious
degeneration problems, such as undesired repeti-
tions (Holtzman et al., 2019) and unnatural topic
drifts, under open-ended settings (Eikema and
Aziz, 2020). Open-ended neural text generation
aims to generate coherent and diverse text from
LLMs, given contextual prefix (Nadeem et al.,
2020; Dhamala et al., 2022), and has spawned a
wide range of natural language applications, in-
cluding contextual text completion (Radford et al.,
2019), story generation (Fan et al., 2018), and re-
view generation (Cho et al., 2019).
To alleviate the degeneration problem in open-
ended text generation, a number of techniques have
1Code and resources are available at https://github.
com/xunannancy/LookBackDecoding .
0 50 100 150 200 250
Continuation Step0.00.20.40.60.81.0Normalized minKL(a) Current VS. History
0 50 100 150 200 250
Continuation Step0.00.20.40.60.81.0Normalized minKL(b) Current VS. Prefix
0.00.20.40.60.81.0
maxHidden
0.00.20.40.60.81.0
maxHidden
KL: Human
KL: RepetitiveHidden: Human
Hidden: RepetitiveKL: Human
KL: Off-topicHidden: Human
Hidden: Off-topicFigure 1: Maximum similarity of hidden states and
normalized minimum KL divergence between current
step and history (a) or prefix (b) from GPT2 on 1,000
instances of WikiText-103. Compared with human con-
tinuation, (a): repetition has much smaller minKL but
undistinguishable high maxHidden with history text, (b):
pseudo topic drift by switching to continuation of an-
other instance has much higher minKL but similar high
maxHidden with prefix text.
emerged over the recent years, which can be cate-
gorized into two directions: i) improved learning
proposing new learning objectives, e.g., unlikeli-
hood training (Welleck et al., 2019), contrastive
training (Su et al., 2022) and sequence likelihood
calibration (Zhao et al., 2022), to compensate for
the rooted deficiency of the conventional Maxi-
mum Likelihood Estimation (MLE)2; ii)improved
decoding remedying tedious and repetitive genera-
tions in decoding search (Su et al., 2022; Li et al.,
2022), or combating topic drifts in sampling proce-
dures (Hewitt et al., 2022).
In this work, we propose a new decoding algo-
rithm, named Look-back , which pays particular
attention to the probability distribution disparity
between continuation and history text. Unlike con-
trastive search (Su et al., 2022; Su and Xu, 2022)
which uses cosine similarity between the hidden
representation, Look-back leverages the Kullback-
Leibler (KL) divergence to track the distribution
distance between current and historical decoding
steps. The main motivation of Look-back is that
2The correlation between sequence probability and its
quality for MLE trained models can be low (Liu et al., 2022).arXiv:2305.13477v2  [cs.CL]  23 Oct 2023

--- PAGE 2 ---
KL divergence defines a distance between the prob-
ability distributions of decoding steps, which ar-
guably better aligns with the decoding practice. As
shown in Figure 1 (a), as the greedy algorithm
repeatedly outputs single sentences, the distance
with the closest past token distribution decreases to-
wards 0. Besides, when the continuation switches
to another topic in Figure 1 (b), the distribution
distance of continuation with prefix obtains much
higher levels compared with topic-relevant human
continuation. Based on our prior observations, for
informative and coherent generation, the probabil-
ity distribution should not be too close to history to
guarantee diversity, but relatively close to prefix to
maintain coherence.
Experimentally, through two tasks of open-
ended text generation, including document con-
tinuation and story generation, we demonstrate that
Look-back outperforms a variety of open-ended
decoding algorithms under different scales of pre-
trained LLMs (GPT2-XL and OPT-6.7B) by pro-
ducing much more coherent texts – high mauve
score compared with human continuation and high
similarity score measured against prefix, while
maintaining similar level of diversity.
2 Related Work
Improved Learning Algorithms Yang et al.
(2018); Adiwardana et al. (2020) observed that in-
creasing number of candidates in beam search or
sampling leads to worse quality of generated data.
They attribute this to the predominant training ob-
jective (i.e., Maximum Likelihood Estimation) that
might not accurately rank generated sequences by
quality (Zhao et al., 2022). Besides, Holtzman
et al. (2019) found that searching for the proba-
ble sequences always results in short and repetitive
texts, which further motivated recent efforts to im-
prove generation via revised learning objectives.
Welleck et al. (2019) proposed unlikelihood train-
ing to force unlikely generations to be assigned
lower probability by the model. To alleviate de-
generation, SimCTG (Su et al., 2022) introduced
a contrastive training objective to preserve sparse-
ness of the token similarity matrix of the generated
text. To avoid unintentionally boosting the prob-
ability of other irrelevant tokens in unlikelihood
training, Jiang et al. (2022) leveraged contrastive
token learning to explicitly teach the LLM to as-
sign negative tokens with a lower probability than
positive tokens through more focused contrast be-tween the two. Based on a BERTScore-style sim-
ilarity metric between model decodes and targets
measured in the model’s latent space, Zhao et al.
(2022) calibrated model-generated sequences with
sequence likelihood calibration to better align with
reference sequences via different types of losses
(e.g., rank and margin loss).
Improved Decoding Algorithms Liu et al.
(2022) observed that search methods (e.g., greedy
and beam) which optimize generation probabili-
ties may result in tedious and repetitive outputs in
open-ended text generation. Su et al. (2022) com-
plemented the contrastive training with contrastive
search for decoding, which selects tokens more dis-
tingushable from previous context. Li et al. (2022)
observed that degeneration is more prevalent in
larger LMs than smaller ones, and proposed con-
trastive decoding to remove these undesired behav-
ior by factoring out smaller LM’s behavior from the
larger LM. On the other hand, truncation sampling
methods such as nucleus (Holtzman et al., 2019)
and typical (Meister et al., 2022) decoding improve
sample quality with more diverse samples com-
pared to direct sampling, but at the expense of poor
coherence and undesired topic drift. Hewitt et al.
(2022) introduced η-sampling to truncate words be-
low an entropy-dependent probability threshold. A
concurrent work observed the strong correlation be-
tween good generation quality and narrow entropy
zone, hence proposed entropy-aware decoding to
promote good generation by constraining greedy
decoding into the narrow entropy zone (Arora et al.,
2023).
Without extra effort on fine-tuning LMs, the pro-
posed Look-back improves conventional search
method with reference from the given prefix and
prior generation, so that undesired repetitions and
topic drifts can be explicitly alleviated.
3 Background
3.1 Open-ended Text Generation
Given a sequence of mtokens sampled from nat-
ural text C={x1. . . x m}as context or prefix, the
neural text generation is to decode a n-token contin-
uation using the probability distribution provided
by pre-trained LMs:
p(xm+1:m+n|C) =nY
t=1P(xt|C, xm+1. . . x m+t−1),

--- PAGE 3 ---
Degeneration LM (Decoding) Continuation
Prefix : In addition to live broadcasts FIFA Fan Fests offer food and beverages, merchandise and various entertainment events by local and international
artists. The start of 2006 World Cup was
Repetitive
ContinuationS1:(GPT2-XL greedy) marked by the FIFA Fan Fest in Sao Paulo, Brazil.\n\n The FIFA Fan Fest is a unique event that brings together
fans from all over the world to celebrate the FIFA World Cup .The FIFA Fan Fest is a unique event that brings together fans from...
S2:(davinci-002 greedy) celebrated with a concert by Bon Jovi in Berlin.\n\n The FIFA Fan Fest in Munich was located at the Olympic
Park.\n\n The FIFA Fan Fest in Frankfurt was located at the Römerberg.\n\n The FIFA Fan Fest in Hamburg was located at the ...
Prefix : Burkan died from an attack of acute indigestion at his country home in Great Neck on June 6, 1936. Three thousand people attended his funeral at
Off-Topic
ContinuationS3:(ada-001 greedy) the local church.\n\n The authorities are investigating how Khedira managed to enter the house and what role he
played in the attack.
S4:(davinci-002 greedy) : Temple Emanu-El in New York City... Category:1868 births\nCategory:1936 deaths\nCategory:Austro-...
S5:(ada-001 nucleus) : aients home.\n 
The Lorraine weekend\nIn house of intensity and occupation, great law enforcement officers\n ...
Shanny Bankecived his way into the home of Runaan U Without giving any reason other than to marines and punch said home’s door ...
×2
Table 1: Degeneration examples with typical decoding algorithms by GPT2-XL and GPT3 (ada-001 and davinci-
002). Complete sentence repetition ( S1), repetition with minor location changes ( S2) or paragraph duplication ( S5)
is marked in green , while unnatural ( S3&S4) or stiff ( S5) topic drifts are in pink .
where the continuation is generated token-by-token
using a particular decoding strategy. For instance,
greedy algorithm selects the next token given con-
text with the highest probability, while nucleus sam-
pling (Holtzman et al., 2019) restricts the plausible
area of tokens with total mass above a threshold.
3.2 Degeneration Problems
There are two commonly observed degeneration
problems in open-ended text generation: repetition
and incoherence.
Repetition LLMs prefer to overestimate the prob-
ability of repeated sequences (Welleck et al., 2019)
especially for deterministic algorithms such as
greedy and beam search. Although decoding algo-
rithms such as nucleus sampling (Holtzman et al.,
2019) have been proposed to interrupt repeating se-
quences, we can still observe repetitive and tedious
continuation even from the state-of-the-art GPT-3
language model (Brown et al., 2020), as shown in
Table 1. Besides the consensus that probabilities
from conditional LMs often do not accurately rank-
order generated sequences by quality (Zhao et al.,
2022), a recent study provides a possible way to
explain the repetitive generation with the observed
analogical sequence copying pattern: prefix match-
ing and copying3(Olsson et al., 2022).
Incoherence Sampling algorithms sacrifice co-
herence for alleviating repetition during decoding.
3Prefix matching: the attention mechanism in transformer-
based LMs attends back to previous tokens that were followed
by the current and/or recent tokens. Copying: outputs in-
creased logit of the attended-to token or others similar in
embedding space.As shown in Table 1, given probabilities from GPT-
3 models, nucleus sampling fails to produce co-
herent generation, switching topic from Burkan’s
acute indigestion toShanny’s way to home with
ada-001 (S5). Recent decoding algorithms depend
on model confidence to “guarantee” coherence
while resolving repetition explicitly with certain
heuristics. For example, SimCTG (Su et al., 2022)
selects from most probable candidates predicted by
LM. Contrastive decoding (Li et al., 2022) exploits
coherence nature of the expert LMs. In both S3
andS4from Table 1, unfortunately, we find that the
coherence hypothesis of pretrained LMs in prior
work does not always hold in practice: it is likely to
produce incoherent sentences when powerful LMs
rigorously follow model confidence at each step
with greedy algorithm.
4 Proposed Method: Look-back
As presented in Algorithm 1, Look-back first lever-
ages probability distribution distance between cur-
rent and prior steps to avoid repetitions (§4.1), then
incorporates reference from given prefix to mitigate
topic drifts (§4.2).
4.1 Alleviating Repetitions with Reference
from Prior Texts
Signal for Surface or Semantic Repetitions In
the decoding process of open-ended text genera-
tion, one of the plausible tokens is selected/sampled
according to model probability. Inspired by the
decisive role of probability distribution, we inves-
tigate measuring the distance between current and
prior steps in disbrituion space via KL divergence:
DKL(pt|p′
t)for any 1≤t′< t. As the distance

--- PAGE 4 ---
 The
 FIFA
 Fan
 Fest
 is
 a
 unique
 event
 that
 brings
 together
 fansThe
 FIFA
 Fan
 Fest
 is
 a
 unique
 event
 that
 brings
 together
 fans
5101520(a) KL heatmap by GPT2-XL (greedy)
The
 FIFA
 Fan
 Fest
 in
 Frankfurt
 was
 located
 at
 the
 R
ö
mer
bergThe
 FIFA
 Fan
 Fest
 in
 Munich
 was
 located
 at
 the
 Olympic
 Park
2.55.07.510.012.515.017.5
(b) KL heatmap by davinci-002 (greedy)
0 50 100 150 200 250
Continuation Step051015minKLHuman GPT2-XL (greedy)(c)KLt
mintohistory by GPT2-XL (greedy)
0 50 100 150 200 250
Continuation Step051015minKLHuman davinci-02 (greedy)
(d)KLt
mintohistory by davinci-002 (greedy)
140 160 180 200 220 240
Second Half Continuation Step2.55.07.510.0minKLHuman ada-001 (nucleus) davinci-002 (greedy)
(e)KLt|C
mintoprefix by davinci-002 (greedy) and ada-001 (nucleus)
Figure 2: Probability distribution distance of GPT2-XL measured by KL divergence for repetitive (a,b,c,d) and
off-topic (e) continuation presented in Table 1. (a) and (b): Dark cells along diagonal indicate that steps of small
distance with history tend to produce repetitive tokens. (c) and (d): Compared with human continuation, minimum
distribution distance with past gradually approaches 0(red curves) as similar phrases keep repeating during decoding.
(e): distribution of incoherent continuation (green and blue curves) is prone to stay farther from given prefix as
decoding proceeds.
Algorithm 1 Look-back Decoding
Input: Prefix C={x1. . . x m}, language model
with vocabulary V, beam size kand threshold α
Output: Continuation G={xm+1. . . x m+n}
G ← {}
form+ 1≤t≤m+ndo
ifKLt
min≤αthen ▷Alleviate Repetitions
forv∈Vkdo
qv= softmax( −KLt+1,v|C
min )
end for
xt=v∼qv ▷Improve Coherence
else
xt= argmaxv∈Vpθ(v|x<t)
end if
G ← G ∪ { xt}
end for
heatmap shown in Figure 2a, for steps generating
identical tokens, their corresponding probability
distributions stay close to each other than those
with dissimilar outputs.
Note that neither the contrastive training objec-tive (SimCTG) (Su et al., 2022) nor its contrastive
search decoding algorithm (Su and Xu, 2022) can
be directly applied to LLMs such as GPT3, where
its hidden states are inaccesible. Fortunately, we
can directly detect surface or semantic repetitions
from GPT3 by analyzing available probability dis-
tribution: step pairs producing either identical to-
ken or tokens sharing similar semantic meaning
are distinguishable with distribution distance. Take
Figure 2b as an instance: output token pairs from
decoding steps with closest probability distribu-
tions are the 1st and 2nd FAN, city Munich and
Frankfurt , location Olympic andRofRömerberg .
As repetitive steps tend to stay extremely close
to prior steps with similar outputs in probability
distribution space, we calculate the probability dis-
tribution distance between the t-th and closest prior
step as KLt
minfor further analysis:
KLt
min= min
1≤j≤t−1KL (p(·|x<t)∥p(·|x<j))
As demonstrated in Figure 2c and Figure 2d, values
ofKLt
minbecome flat as repetition-style degenera-

--- PAGE 5 ---
tion advances4.
Alleviating Repetitions Since identical or simi-
lar repetition pattern could be forecasted via proba-
blity distribution analysis, Look-back attempts to
avoid repetitive sentences or phrases prior to actual
generation. Practically, when KLt
minhas been be-
low a pre-defined threshold α, an alarm is triggered
andLook-back attempts to sample a token from the
top-kmost probable tokens from the vocabulary V
rather than sticking to the top-1 token:
xt(
∼Unif( Vk), ifKLt
min≤α
= argmaxv∈Vpθ(v|x<t),Otherwise
where Vkis the set of top- kmost probable tokens
from the vocabulary V. To avoid false positive
cases where one step identified with high possibil-
ity to repeat may not necessarily lead to undesired
repetitions, we do not exclude its most probable
token from the plausible candidate set on purpose.
4.2 Improving Coherence with Reference
from Given Prefix
Signal for Topic Drift In open-ended generation,
in order to produce sentences coherent with the
given prefix, the decoding algorithm is required to
provide further elaboration of the major topic con-
veyed in the prefix. According to the prior obser-
vations (e.g., Munich andFrankfurt in Figure 2b),
decoding steps with tokens sharing similar seman-
tic meaning are close to each other with respect to
probability distribution distance. Therefore, we ex-
plore the KL divergence between current and prefix
msteps that should keep to the same topic:
KLt|C
min= min
1≤j≤mKL (p(·|x<t)∥p(·|x<j)
When comparing distribution distance of incoher-
ent generation with natural continuation to the same
prefix, the probability distribution divergence main-
tains a much higher level for generation with obvi-
ous topic drift, as shown in Figure 2e.
Improving Coherence When the model is prone
to provide repetitive tokens, one straightforward
solution for avoiding repetition is to randomly sam-
ple from the top- kplausible tokens. It is likely
to result in unnatural topic drift due to undesired
sampling choices accumulation over long sequence
4Spikes in Figure 2d in later decoding steps correspond
to multiple tokens for representing one single location, e.g., ö,
mer, berg for Römerberg in Figure 2b.decoding, which is frequently observed in sam-
pling algorithms (Eikema and Aziz, 2020; Maynez
et al., 2020). On the other side, the probability
distribution distance between current and prefix is
able to distinguish whether the generation is on-
topic or not. Therefore, Look-back wisely samples
from the plausible candidates according to their
influence on coherence reflected by next-step dis-
tribution distance with prefix:
KLt+1,v|C
min = min
1≤j≤mKL (p(·|x<t+1, v)∥p(·|x<j))
xt(
∼softmax( −KLt+1,v|C
min ),ifKLt
min≤α
= argmaxv∈Vpθ(v|x<t),Otherwise
where tokens with larger next-step distance to pre-
fix is less likely to be sampled given the softmax
operation upon KL divergence.
5 Experiments
In this section, we first introduce the datasets (§5.1)
and automatic metrics (§5.2) used to evaluate the
generation quality of the proposed Look-back and
other strong decoding baselines (§5.3). We then an-
alyze experimental results evaluated by automatic
metrics (§5.5) and human evaluators (§5.6). Lastly,
we show effectiveness of different techniques used
inLook-back through detailed analyses (§5.7).
5.1 Datasets
We consider two applications of open-ended
text generation: 1) document continuation on
WikiText-103 with articles fitting the Good or
Featured article criteria specified by editors on
Wikipedia (Merity et al., 2016), and 2) story gen-
eration on WritingPrompts, which is a challenging
task for inspiring continuations with abstract, high-
level story prompts submitted by online users and
continuations responded by others freely on Red-
dit (Fan et al., 2018).
5.2 Evaluation Metrics
We adopt the following automatic metrics to evalu-
ate generation quality:
Repetition We use rep-nto measure sequence-
level repetition according to the portion of dupli-
caten-grams (Welleck et al., 2019). For a sequence
x,rep-n = 1.0−|unique n-grams(x) |
|total n-grams(x)|.
Diversity Following (Su et al., 2022), we ob-
tain an overall assessment of model repetition by
considering repetition at different n-gram levels:
diversity =Q4
n=2(1.0−rep-n).

--- PAGE 6 ---
LM DecodingWikiText-103 WritingPrompts
rep-2↓rep-3↓rep-4↓diversity ↑MAUVE ↑coherence ↑rep-2↓rep-3↓rep-4↓diversity ↑MAUVE ↑coherence ↑
human 6.91 1.83 0.70 0.91 - 0.62 15.61 3.78 1.24 0.80 - 0.31GPT2-XLnucleus 5.29 1.97 1.42 0.92 0.69 0.53 5.40 2.41 1.72 0.91 0.22 0.34
typical 3.61 1.07 0.73 0.95 0.70 0.50 3.60 1.51 1.10 0.94 0.19 0.30
η-sampling 6.25 2.49 1.80 0.90 0.68 0.55 6.17 2.88 2.16 0.89 0.17 0.35
SimCTG 5.37 1.97 1.46 0.91 0.72 0.53 2.84 0.36 0.19 0.97 0.18 0.31
Look-back 8.22 1.34 0.38 0.90 0.81 0.65 7.94 1.25 0.33 0.91 0.24 0.52OPT-6.7Bnucleus 6.08 2.19 1.43 0.91 0.63 0.56 5.82 3.12 2.57 0.89 0.13 0.33
typical 6.58 2.25 1.37 0.90 0.61 0.57 5.80 2.67 1.93 0.90 0.14 0.33
η-sampling 6.07 2.26 1.55 0.90 0.66 0.56 4.72 1.93 1.36 0.92 0.15 0.34
SimCTG 5.44 1.97 1.38 0.91 0.56 0.55 7.49 4.25 3.10 0.86 0.08 0.20
Look-back 9.21 1.74 0.53 0.89 0.80 0.65 9.77 2.18 0.74 0.88 0.19 0.43
Table 2: Automatic evaluation results of different decoding algorithms for document continuation and story
generation. Continuation generated by Look-back is of similar level of diversity as human texts while much more
relevant to prefix (highest coherence ) and semantically similar to human continuation (highest MAUVE ).
MAUVE By computing information divergences
in a quantized embedding space5,MAUVE (Pillutla
et al., 2021) directly compares the learnt distribu-
tion from a text generation model to the distribution
of human-written continuation.
Coherence The semantic coherence between pre-
fix and continuation is measured as the cosine sim-
ilarity between their sentence embeddings repre-
sented by SimCSE (Gao et al., 2021).
Results measured by all metrics range from 0
to1, and higher scores indicate better generation
except rep-n, for which the lower the better.
5.3 Decoding Baselines
Given pretrained LMs with conventional MLE, we
evaluate Look-back together with various decoding
algorithms for fair comparisons.
Search Methods We consider the competitive
contrastive search proposed in SimCTG (Su et al.,
2022) that predicts the next token based on both the
output distribution and representation similarities
between candidates and past tokens6.
Sampling Methods Nucleus sampling (Holtz-
man et al., 2019) samples the next token from the
top-pportion of the probability mass. Typical de-
coding (Meister et al., 2022) samples from the set
of words whose negative log-probabilities are close
to the conditional entropy. η-sampling (Hewitt
et al., 2022) truncates any word whose probability
is smaller than an entropy-based threshold.
5We use GPT2-XL for text sequence embedding.
6We disregard greedy andbeam search as they kept pro-
ducing repetitive phrases/sentences in prior studies (Welleck
et al., 2019; Holtzman et al., 2019).5.4 Implementation Details
We randomly sample 1,000 instances from the orig-
inal training data of WikiText-103 and Writing-
Prompts as our validation and test sets. Given the
beginning several tokens as prefix7, we generate
256 tokens with different decoding algorithms and
disregard those after the end-of-text token during
evaluation. Practically, we consider a sliding win-
dow comprising 128prior tokens to avoid unde-
sired repetitions while allow necessary repetitions
of text far from the current decoding step. We
perform experiments with pre-trained LMs from
different families and scales: GPT2-XL (Radford
et al., 2019) and OPT-6.7B (Zhang et al., 2022).
The same set of hyperparameters is used to de-
code from different LMs: the beam size for beam
search is 10,p= 0.95fornucleus ,τ= 0.92for
typical, and η= 0.0003 forη-sampling. We fol-
low the recommended range for k={5,8,10}
andα= [0.5,0.9]inSimCTG and select the set
based on their MAUVE scores on the validation
set. For Look-back , the range of candidate amount
kis{5,8,10}and the threshold αis ranging from
[0.5,1.6]. We select hyperparameters that result in
therep-2 score closest to human’s and the optimal
MAUVE performance on the validation set.
5.5 Results
In Table 2, we show the performance of different
decoding algorithms as well as natural human con-
tinuation evaluated by automatic metrics. On both
datasets, Look-back consistently achieves the high-
estMAUVE scores and coherence scores, which in-
dicates that the generation of Look-back has token
7First 32tokens are used as prefix for WikiText-103,
while the original prompts are used for WritingPrompts.

--- PAGE 7 ---
LM CriterionLook-back better
(p-value)same SimCTG better
GPT2-XLFluency 0.46 (.0127) 0.27 0.27
Coherence 0.57 (.0004) 0.16 0.27
OPT-6.7BFluency 0.38 (.0508) 0.37 0.25
Coherence 0.53 (.0078) 0.16 0.31
Table 3: Human Evaluation on generations from Look-
back and the second best SimCTG with examples sam-
pled from WikiText-103. Continuation generated by
Look-back is preferred to SimCTG significantly by hu-
man evaluators w.r.t both fluency and coherence.
distribution closeness with human continuations
while staying relevant to the given prefixes. Mean-
while, Look-back is capable of producing texts
with similar repetition and diversity level as the
natural human text, which implies the fluency and
informativeness of the generated text. We also no-
tice that generations from all decoding algorithms
obtain relatively low MAUVE andcoherence scores
on WritingPrompts. This is because the given pre-
fixes are abstract and the human written references
are diverse and varied, which results in low coher-
ence andMAUVE w.r.t. various model continua-
tions.
5.6 Human Evaluation
To further evaluate the quality of generated texts,
we randomly sample two sets of 50 examples from
WikiText-103 to produce prefixes for GPT2-XL
and OPT-6.7B respectively and generate continu-
ations from them. Then, we ask 3 evaluators to
compare generated continuations from Look-back
and the second best baseline SimCTG in two di-
mensions: 1) fluency : diverse and natural content
without repeated words, phrases or sentences; 2) co-
herence : well-organized and easy to follow; being
consistent with the topics presented in the human-
written prefix without abrupt topic drifts. We ask
annotators to choose one out of three options: the
1st continuation is better, the 2nd is better, or the
two are of the same quality. As presented in Ta-
ble 3, for both evaluation dimensions, the content
generated by Look-back is preferred or marked as
equally good by evaluators around or more than
70% of the time compared with baseline, which
aligns well with the automatic metrics in Table 2.
5.7 Further Analyses
In this section, we analyze the effectiveness of dif-
ferent techniques used by Look-back individually.
0 50 100 150 200 250
Continuation Step01234minKL(a) Current VS. History
0 50 100 150 200 250
Continuation Step34567minKL(b) Current VS. Prefix
Repetitive
Off-topicnucleus
typical-sampling
SimCTGLook-back(a) WikiText-103 (GPT2-XL)
0 50 100 150 200 250
Continuation Step01234minKL(a) Current VS. History
0 50 100 150 200 250
Continuation Step468minKL(b) Current VS. Prefix
Repetitive
Off-topicnucleus
typical-sampling
SimCTGLook-back
(b) WikiText-103 (OPT-6.7B)
0 50 100 150 200 250
Continuation Step0123minKL(a) Current VS. History
0 50 100 150 200 250
Continuation Step234567minKL(b) Current VS. Prefix
Repetitive
Off-topicnucleus
typical-sampling
SimCTGLook-back
(c) WritingPrompts (GPT2-XL)
0 50 100 150 200 250
Continuation Step0123minKL(a) Current VS. History
0 50 100 150 200 250
Continuation Step468minKL(b) Current VS. Prefix
Repetitive
Off-topicnucleus
typical-sampling
SimCTGLook-back
(d) WritingPrompts (OPT-6.7B)
Figure 3: Minimum KL divergence between current
step and (a) history or (b) prefix from GPT2-XL and
OPT-6.7B decoded by different algorithms on the test
set of WikiText103 and WritingPrompts. Probability
distribution of Look-back keeps distance to history to
avoid repetitions but stays close to prefix to guarantee
coherence.
Analyzing Probability Distribution Distance.
To verify whether decoding with Look-back ap-
propriately constrains the probability distribution
distance to past steps, we compare KLt
minto his-
tory and KLt|C
minto prefix of degeneration and dif-
ferent decoding algorithms in Figure 3. Although
all improved decoding algorithms keep distance to
historical probability distribution to avoid repeti-
tions compared with greedy algorithm (Repetitive
in the left column of Figure 3, the probability dis-

--- PAGE 8 ---
LM Sampling diversity MAUVE coherence
WikiText-103
GPT2-XLUniform 0.93 0.71 0.61
Softmax 0.90 0.81 0.65
OPT-6.7BUniform 0.93 0.60 0.52
Softmax 0.89 0.80 0.65
WritingPrompts
GPT2-XLUniform 0.93 0.15 0.45
Softmax 0.91 0.24 0.52
OPT-6.7BUniform 0.91 0.14 0.29
Softmax 0.88 0.19 0.43
Table 4: Effects of probability distribution-guided sam-
pling of Look-back (Softmax) on generation quality.
With similar level of diverse content as human text,
Look-back samples according to softmax of negative
distribution distance to prefix, leading to improved co-
herence compared with Uniform.
tribution of Look-back (Look-back in the right col-
umn of Figure 3 is much closer to the given prefix,
which distinguishes it from off-topic continuation
compared with other algorithms.
Softmax vs. Uniform. According to the soft-
max operation on KLt|C
minintroduced in §4.2, the
closer the next step’s probability distribution to
prefix, the more likely the corresponding plausi-
ble token is selected to avoid undesired topic drift
compared with random sampling. In Table 4, we
empirically investigate the impact of plausible to-
ken sampling, uniform vs. softmax, on generation
quality and find Look-back significantly enhances
coherence on both datasets compared with random
sampling. Although diversity drops with distribu-
tion distance-guided sampling in Look-back , both
sampling strategies produce similar level of diverse
content as human texts listed in Table 2.
Effects of Candidate Amount and Threshold α.
In §4.1, the hyperparameter αdetermines whether
the current step is likely to produce repetitive con-
tinuation while krestricts the range of plausible
token candidates. The second best baseline Sim-
CTG has the similar candidate amount parameter k
and the αto balance model confidence and degen-
eration penalty. When GPT2-XL is used to decode
with Look-back and SimCTG on WikiText-103,
we visualize the impact of hyperparameters on gen-
eration quality in Figure 4 and Figure 5. The αin
Look-back is different from that in SimCTG, but
both control reliance on model confidence: a larger
0.50 0.750.60.81.0diversitySimCTG
0.5 1.0 1.50.60.81.0Look-back
0.50 0.750.00.5MAUVE
0.5 1.0 1.50.00.5
0.50 0.75
0.40.6coherence
0.5 1.0 1.5
0.40.6
k=10 k=5 k=8 HumanFigure 4: Impact of decoding hyperparameters on val-
idation set of WikiText-103. Compared with the other
search algorithm SimCTG (1st column), Look-back
(2nd column) keeps relatively higher MAUVE and co-
herence scores regardless of plausible token amount k
and the KLt
minthreshold α. See Figure 5 for more re-
sults in other settings.
αindicates the most probable token is less likely
to be adopted, hence more diversity is obtained.
We also observe that for Look-back , the relevance
of generated text to prefix (high coherence ) and
human continuation (high MAUVE ) is much more
robust to various hyperparameter values compared
with SimCTG.
5.8 Case Study
Given a prefix sampled from WikiText-103, we
present truncated human continuations as well as
generations from Look-back and SimCTG in Ta-
ble 5 and leave more examples in Appendix Table 6.
The prefix is talking about the design of a race car
game. Both human and Look-back continuations
focus on describing major difficulties encountered
during the game design, while SimCTG switches to
a different topic by pointing to an online introduc-
tion of the game in the second half of continuation.
Interestingly, Look-back explains how the team
of more than twenty people was formed, which is
coherent with the topic in the prefix.

--- PAGE 9 ---
0.50 0.750.60.81.0diversitySimCTG
0.5 1.0 1.50.60.81.0Look-back
0.50 0.750.00.5MAUVE
0.5 1.0 1.50.00.5
0.50 0.75
0.40.6coherence
0.5 1.0 1.5
0.40.6
k=10 k=5 k=8 Human(a) WikiText-103 (OPT-6.7B)
0.50 0.750.500.751.00diversitySimCTG
0.5 1.0 1.50.500.751.00Look-back
0.50 0.750.00.10.2MAUVE
0.5 1.0 1.50.00.10.2
0.50 0.75
0.20.4coherence
0.5 1.0 1.5
0.20.4
k=10 k=5 k=8 Human (b) WritingPrompts (GPT2-XL)
0.50 0.750.500.751.00diversitySimCTG
0.5 1.0 1.50.500.751.00Look-back
0.50 0.750.00.10.2MAUVE
0.5 1.0 1.50.00.10.2
0.50 0.75
0.20.4coherence
0.5 1.0 1.5
0.20.4
k=10 k=5 k=8 Human (c) WritingPrompts (OPT-6.7B)
Figure 5: (Continuation from Figure 4) Impact of decoding hyperparameters on validation set of WikiText103 and
WritingPrompts.
Prefix : Ridge Racer Revolution was developed over eight months by a team of more than
twenty people, most of whom joined just for Ridge Racer Revolution. The biggest
difficulties were
Human : the link-up mode, rear-view mirror, and running the game at high speeds. Each
member had worked on other console games, and they commented that the graphics detail
showed the improvements in skill and technique. The team wanted players to enjoy a more
in-depth game than the original, so rather than develop a port of Ridge Racer 2, as many
features as possible were added. The early designs for the new courses were made...
SimCTG : balancing the game, which had to be a mix of arcade and racing elements, along
wtih making an open world that felt like the future of video games.\n\nIn order to do this,
we used Unreal Engine 3, the game engine that powers ... You can learn more about the
game by clicking here, but be warned, there are spoilers in this article. If you’re planning
on reading this article, I suggest you stop reading now before it spoils the game for you...
Look-back : the lack thereof: the original game was built in a single year; Ridge Crash took
more. The original developers were all gone, and the original team of programmers, artists,
and designers from Ridge Revolution, including the lead programmer at Capcom, had all
left the company by 2007...In the end, a new team of twenty-five employees was assembled
for Revolution, who took nearly two years to complete the game. In all, this team of more...
Table 5: Case study of an instance sampled from
WikiText-103 with GPT2-XL. Continuation of both
human and Look-back discusses difficulties in game
design, while SimCTG gradually produces less informa-
tive sentences with slight topic drift to game introduc-
tion (in pink ). Refer to Table 6 for more examples.
6 Conclusion
The distance between output distributions signals
potential failure modes of text generation, such
as dull repetition and incoherence. We propose
Look-back , a novel decoding algorithm that uti-
lizes the KL divergence between the current and
historic decoding steps, to steer the output distri-
bution into a plausible subspace. Look-back can
generate higher-quality text and outperforms sev-
eral strong decoding algorithms in both automatic
and human evaluation. However, KL divergence
may not be the optimal measure for text output dis-
tributions and we leave the investigation of other
measures to future work. In addition, the idea pro-posed in this work can also be used for other spe-
cialized constrained decoding scenarios, such as
preventing hallucination.
Limitations
We discuss the limitations of our work as follows:
•Look-back penalizes next tokens that result
in low KL divergence with historic output dis-
tributions. However, we can not explicitly
distinguish if such tokens are natural or unde-
sired repetitions. This may lead to aggressive
eliminations of possible outputs. We leave the
distinction of different repetitions to future
work.
•Look-back tends to show a higher bi-gram
repetition score than other decoding methods
because it encourages the coherence with pre-
fix text at each decoding step. As we use a
short prefix text following previous evaluation
protocol, which might not be sufficiently infor-
mative, we will adopt a more comprehensive
evaluation setup in the future or prepend rele-
vant text in the beginning at decoding time.
•Most of our evaluations rely on automatic met-
rics, such as MAUVE scores. However, we
found that these metrics may not truthfully re-
flect the quality of text, for example, MAUVE
score is sensitive to the choice of sentence em-
bedding models. In general, open-ended text
generation still poses a great challenge to the
development of NLG algorithms.

--- PAGE 10 ---
References
Daniel Adiwardana, Minh-Thang Luong, David R So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al. 2020. Towards a human-like open-domain chat-
bot. arXiv preprint arXiv:2001.09977 .
Kushal Arora, Timothy J O’Donnell, Doina Precup, Ja-
son Weston, and Jackie CK Cheung. 2023. The stable
entropy hypothesis and entropy-aware decoding: An
analysis and algorithm for robust natural language
generation. arXiv preprint arXiv:2302.06784 .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.
Woon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiu-
jun Li, Michel Galley, Chris Brockett, Mengdi Wang,
and Jianfeng Gao. 2019. Towards coherent and co-
hesive long-form text generation. In Proceedings
of the First Workshop on Narrative Understanding ,
pages 1–11, Minneapolis, Minnesota. Association for
Computational Linguistics.
Jwala Dhamala, Varun Kumar, Rahul Gupta, Kai-Wei
Chang, and Aram Galstyan. 2022. An analysis of
the effects of decoding algorithms on fairness in
open-ended language generation. arXiv preprint
arXiv:2210.03826 .
Bryan Eikema and Wilker Aziz. 2020. Is map de-
coding all you need? the inadequacy of the mode
in neural machine translation. arXiv preprint
arXiv:2005.10283 .
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 889–898, Melbourne, Australia. Association
for Computational Linguistics.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
John Hewitt, Christopher D Manning, and Percy Liang.
2022. Truncation sampling as language model
desmoothing. arXiv preprint arXiv:2210.15191 .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751 .
Shaojie Jiang, Ruqing Zhang, Svitlana Vakulenko, and
Maarten de Rijke. 2022. A simple contrastive learn-
ing objective for alleviating neural text degeneration.
arXiv preprint arXiv:2205.02517 .Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang,
Jason Eisner, Tatsunori Hashimoto, Luke Zettle-
moyer, and Mike Lewis. 2022. Contrastive decoding:
Open-ended text generation as optimization. arXiv
preprint arXiv:2210.15097 .
Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham
Neubig. 2022. BRIO: Bringing order to abstractive
summarization. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2890–2903,
Dublin, Ireland. Association for Computational Lin-
guistics.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906–1919, On-
line. Association for Computational Linguistics.
Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan
Cotterell. 2022. Typical decoding for natural lan-
guage generation. arXiv preprint arXiv:2202.00666 .
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els.arXiv preprint arXiv:1609.07843 .
Moin Nadeem, Tianxing He, Kyunghyun Cho, and
James Glass. 2020. A systematic characterization
of sampling algorithms for open-ended language gen-
eration. In Proceedings of the 1st Conference of the
Asia-Pacific Chapter of the Association for Compu-
tational Linguistics and the 10th International Joint
Conference on Natural Language Processing , pages
334–346, Suzhou, China. Association for Computa-
tional Linguistics.
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022.
In-context learning and induction heads. arXiv
preprint arXiv:2209.11895 .
Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,
John Thickstun, Sean Welleck, Yejin Choi, and Zaid
Harchaoui. 2021. Mauve: Measuring the gap be-
tween neural text and human text using divergence
frontiers. Advances in Neural Information Process-
ing Systems , 34:4816–4828.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-
peng Kong, and Nigel Collier. 2022. A contrastive
framework for neural text generation. arXiv preprint
arXiv:2202.06417 .
Yixuan Su and Jialu Xu. 2022. An empirical study
on contrastive search and contrastive decoding
for open-ended text generation. arXiv preprint
arXiv:2211.10797 .

--- PAGE 11 ---
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2019. Neu-
ral text generation with unlikelihood training. arXiv
preprint arXiv:1908.04319 .
Yilin Yang, Liang Huang, and Mingbo Ma. 2018. Break-
ing the beam search curse: A study of (re-)scoring
methods and stopping criteria for neural machine
translation. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, pages 3054–3059, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 .
Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi
Narayan, Mohammad Saleh, and Peter J Liu.
2022. Calibrating sequence likelihood improves
conditional language generation. arXiv preprint
arXiv:2210.00045 .

--- PAGE 12 ---
Dataset
(LM)Prefix/ContinuationWikiText-103 (OPT-6.7B)Prefix : A new show in London in 1912 showcased the best of music hall\’s talent. The Royal Command Performance
took place at the Palace Theatre in London, which was
Human : managed by Alfred Butt. The show was organised by Oswald Stoll, an Australian impresario who managed
a string of West End and provincial theatres. Stoll, although a fan of Lloyd’s, disliked the vulgarity of her act and
championed a return to a more family @-@ friendly atmosphere within the music hall. Because of this, and her
participation in the earlier music hall war, Stoll left her out of the line @-@ up. He placed an advert in The Era on
the day of the performance warning that " Coarseness and vulgarity etc are not allowed...
SimCTG : one of the most popular theatres in the West End at the time.
Start the conversation, or Read more at BroadwayWorld.com.</s>I have been waiting for this. Thank you for putting
it together. You should cross post to /r/blunderyears as well...
Look-back : the home of the Royal Commandos during the First World War. The show starred the likes of Harry Lauder, who
played the role he was born to play, ’The King in Yellow’, and Fred Karno - who, as the ’King’, was the star attraction.
It was a huge success, and the Royal Variety Performance took its spot in the calendar. It’s a tradition that continues
to this day - and the King in Yellow is still a big draw at any show...WritingPrompts (GPT2-Xl)Prefix : The world is teetering on the brink of destruction . Only 2 people remain alive , both of them old ladies . They are
face to face with the eldritch creature that has singlehandedly destroyed their planet . And in its terrifying presence ... they
make cheeky small talk about what brought them here .
Human : FADE IN : EXT . A DESOLATE LANDSCAPE - AFTERNOON *A red sky covered with black clouds hangs
above an expanse of blasted countryside . Dark craters pockmark the land , and the corrupted hulks of destroyed war
machines lie rusting in the acrid air . Amidst this desolation , two elderly women sit on rocking chairs that are curiously
undamaged . These are DELILAH and TABITHA . A small table with a tea set is present between them . * **DELILAH :
** It ’s a bit hot today , is n’t it ?...
SimCTG :<|endoftext|>A group of high school students in New York’s Nassau-Suffolkshire school district are demanding
to be excused from participating in "Star Wars," "Harry Potter" and Disney movies because it’s the holiday season...
Look-back : \n\nThe two old ladies, who were not named in either the book or film, had been visiting from the future when their
ship was destroyed. They are not aware of this, but one has become a time traveler from Earth and knows that they were
never meant be together in life but are now destined by fate together. One day they will meet again at last on some other
planet where one of each woman will be born with powers over time which will allow them each one chance to save the
world, which is now destined to fall into the hands of their enemy. One day both are going in the same direction. And one
woman has decided that she needs to see to it that they both get there first!...WritingPrompts (OPT-6.7B)Prefix : For the first time ever , a person is born with a genuine superpower . They proceed to live out their entire life without
noticing or realizing it .
Human : For Francis , the good had always gone hand-in-hand with the bad . He had spent his life trying his best to be a good
person , or at least what he believed would be a good person . He realized now that he was n’t really sure anymore . When his
mother left his dad , Francis had chosen to stay with him . His friends had all told him he was smart to stay with the rich
parent , but that had n’t been the reason , had it ? At the time , he had told himself that his father needed him more . He did .
Dad had been really bad at taking care of himself . Without his mother , someone had to be there to deal with dad when he
was lonely and angry and ...
SimCTG : What if they have superpowers in real life?\nI’m going to need proof of this before I believe it. You know, for science.
It’s true. I saw it on the internet. So it must be true. </s>How to Get Rid of Pimples on Forehead...
Look-back : The person is born with the ability not necessarily in the form of super strength or super speed (although they may have
those abilities as well) but the superpower is the ability to see the future! They see the outcomes in their life and they live their
days according to the outcome they see for themselves! In other words, they plan their life out according how their futures will
unfold according. They see what will happen and they do what they can so that it will come. And because they live their whole
live without realizing they have this ability they are able live life to the fullest. Because the future is already set for them, their
entire life unfolds according. Their whole life, from the moment of their birth, unfolds according as planned in their mind...
Table 6: Case study of instances sampled from WikiText-103 and WritingPrompts. Unnatural topic drifts are
frequently observed in generations from SimCTG (in pink ).
