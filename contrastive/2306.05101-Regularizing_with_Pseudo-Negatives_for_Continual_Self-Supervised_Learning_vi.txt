# Chính Quy Hóa với Âm Tính Giả cho Học Tự Giám Sát Liên Tục

Sungmin Cha1 Kyunghyun Cho1 2 Taesup Moon3

## Tóm tắt

Chúng tôi giới thiệu một khung Chính Quy Hóa Âm Tính Giả (PNR) mới cho học tự giám sát liên tục (CSSL) hiệu quả. PNR của chúng tôi tận dụng các âm tính giả thu được thông qua tăng cường dựa trên mô hình theo cách mà các biểu diễn mới học được có thể không mâu thuẫn với những gì đã được học trong quá khứ. Cụ thể, đối với các phương pháp học đối lập dựa trên InfoNCE, chúng tôi định nghĩa các âm tính giả đối xứng thu được từ các mô hình hiện tại và trước đó và sử dụng chúng trong cả hai hạng mục tổn thất chính và chính quy hóa. Hơn nữa, chúng tôi mở rộng ý tưởng này cho các phương pháp học không đối lập vốn không dựa vào âm tính một cách cố hữu. Đối với các phương pháp này, âm tính giả được định nghĩa là đầu ra từ mô hình trước đó cho một phiên bản tăng cường khác của mẫu neo và được áp dụng bất đối xứng vào hạng mục chính quy hóa. Kết quả thực nghiệm mở rộng chứng minh rằng khung PNR của chúng tôi đạt được hiệu suất tiên tiến trong học biểu diễn trong CSSL bằng cách cân bằng hiệu quả sự đánh đổi giữa tính dẻo và ổn định.

## 1. Giới thiệu

Học Tự Giám Sát (SSL) gần đây đã nổi lên như một phương pháp tiết kiệm chi phí để huấn luyện mạng nơ-ron, loại bỏ nhu cầu gán nhãn dữ liệu tốn công (Gui et al., 2023). Cụ thể, các biểu diễn được học bởi các phương pháp SSL gần đây (ví dụ: MoCo (He et al., 2020), SimCLR (Chen et al., 2020a), BarlowTwins (Zbontar et al., 2021), BYOL (Grill et al., 2020), và VICReg (Bardes et al., 2022)) được chứng minh có chất lượng tuyệt vời, có thể so sánh với những gì được học từ học có giám sát. Mặc dù thành công như vậy, độ phức tạp bộ nhớ và tính toán khổng lồ là những nút thắt cổ chai rõ ràng để dễ dàng duy trì và cập nhật các mô hình học tự giám sát, vì chúng thường yêu cầu dữ liệu không giám sát quy mô lớn, kích thước mini-batch lớn, và nhiều bước cập nhật gradient để huấn luyện.

Vì vậy, Học Tự Giám Sát Liên Tục (CSSL), trong đó mục tiêu là học dần dần các biểu diễn được cải thiện từ một chuỗi dữ liệu không giám sát, có thể là một giải pháp thay thế hiệu quả cho học tự giám sát được huấn luyện chung có chi phí cao. Với động lực như vậy, một số nghiên cứu gần đây (Madaan et al., 2022; Hu et al., 2022; Fini et al., 2022) đã xem xét CSSL sử dụng các phương pháp SSL khác nhau và cho thấy hiệu quả của chúng trong việc duy trì tính liên tục biểu diễn. Mặc dù có kết quả tích cực, chúng tôi lưu ý rằng ý tưởng cốt lõi cho các phương pháp đó chủ yếu được mượn từ nghiên cứu học liên tục lớn cho học có giám sát (Parisi et al., 2019; Delange et al., 2021; Wang et al., 2024).

Cụ thể, một phương pháp học liên tục có giám sát điển hình có thể được mô tả chung là sử dụng một hạng mục tổn thất tác vụ đơn cho tác vụ mới (ví dụ: entropy chéo hoặc tổn thất đối lập có giám sát (Khosla et al., 2020)) cùng với một loại chính quy hóa nhất định (ví dụ: dựa trên chưng cất (Li & Hoiem, 2017; Douillard et al., 2020; Kang et al., 2022; Wang et al., 2022; Cha et al., 2021a) hoặc dựa trên chuẩn (Kirkpatrick et al., 2017; Aljundi et al., 2018; Jung et al., 2020; Ahn et al., 2019; Cha et al., 2021b) hoặc các hạng mục dựa trên mẫu phát lại (Wu et al., 2019; Rebuffi et al., 2017)) để ngăn chặn quên; các phương pháp CSSL tiên tiến gần đây chỉ đơn giản là theo phương pháp đó với các hạng mục tổn thất tự giám sát (ví dụ: CaSSLe (Fini et al., 2022)).

Về vấn đề này, chúng tôi đặt ra một vấn đề về phương pháp CSSL hiện tại; hiệu quả của việc đơn giản là kết hợp một hạng mục chính quy hóa vào tổn thất tự giám sát hiện có để đạt được CSSL thành công vẫn còn không chắc chắn. Cụ thể, các hạng mục chính quy hóa điển hình về cơ bản được thiết kế để duy trì các biểu diễn của mô hình trước đó, nhưng chúng có thể cản trở khả năng học các biểu diễn tốt hơn trong khi học từ tác vụ mới (tức là tính dẻo) (Cha et al., 2024; Kim & Han, 2023).

Để giải quyết những hạn chế này, chúng tôi đề xuất một phương pháp mới gọi là Chính Quy Hóa Âm Tính Giả (PNR) cho CSSL, sử dụng âm tính giả cho mỗi neo của một đầu vào cho trước, thu được bằng tăng cường dựa trên mô hình. Trong CSSL sử dụng các phương pháp SSL khác nhau, PNR định nghĩa các âm tính giả khác nhau được điều chỉnh cho từng phương pháp học đối lập và không đối lập, tương ứng. Đầu tiên, chúng tôi xem xét trường hợp sử dụng tổn thất đối lập loại InfoNCE (Oord et al., 2018), như SimCLR và MoCo, tận dụng rõ ràng các mẫu âm tính. Chúng tôi đề xuất các hàm tổn thất mới bằng cách sửa đổi tổn thất InfoNCE thông thường và chưng cất đối lập (Tian et al., 2019), đảm bảo rằng cái trước xem xét âm tính từ mô hình trước đó và cái sau bao gồm âm tính từ mô hình hiện tại như âm tính giả. Kết quả là, các hàm tổn thất này đảm bảo rằng các biểu diễn mới thu được không chồng chéo với những gì đã học trước đó, tăng cường tính dẻo. Hơn nữa, chúng cho phép chưng cất hiệu quả kiến thức trước đó vào mô hình hiện tại mà không can thiệp vào các biểu diễn đã được học bởi mô hình hiện tại, do đó cải thiện ổn định tổng thể. Thứ hai, chúng tôi mở rộng ý tưởng sử dụng âm tính giả cho CSSL sử dụng các phương pháp học không đối lập như BYOL, BarlowTwins, và VICReg, không sử dụng rõ ràng các mẫu âm tính trong việc triển khai ban đầu của chúng. Để làm điều này, chúng tôi đánh giá lại mối quan hệ giữa neo của mô hình hiện tại và âm tính từ mô hình trước đó được quan sát trong chưng cất đối lập. Dựa trên mối quan hệ này, chúng tôi đề xuất một chính quy hóa mới định nghĩa âm tính giả cho neo từ mô hình hiện tại như đặc trưng đầu ra của cùng một hình ảnh với các tăng cường khác nhau từ mô hình trước đó. Hàm tổn thất cuối cùng của chúng tôi nhằm tối thiểu hóa sự tương tự của chúng bằng cách kết hợp chính quy hóa này cùng với hạng mục chưng cất hiện có cho CSSL. Cuối cùng, thông qua các thực nghiệm mở rộng, phương pháp đề xuất của chúng tôi không chỉ đạt được hiệu suất tiên tiến trong các tình huống CSSL và tác vụ xuôi dòng mà còn cho thấy cả ổn định và tính dẻo tốt hơn.

## 2. Công trình liên quan

**Học biểu diễn tự giám sát** Đã có một số biến thể gần đây cho Học Tự Giám Sát (SSL) (Alexey et al., 2016; Doersch et al., 2015; Vincent et al., 2010; Zhang et al., 2016; Hadsell et al., 2006; Chen et al., 2020a; He et al., 2020). Trong số đó, các phương pháp dựa trên tổn thất đối lập đã nổi lên như một trong những phương pháp hàng đầu để học các biểu diễn phân biệt (Hadsell et al., 2006; Oord et al., 2018), trong đó các biểu diễn được học bằng cách kéo các cặp dương lại gần nhau và đẩy các mẫu âm tính ra xa. Một số phương pháp học đối lập hiệu quả, như MoCo (He et al., 2020; Chen et al., 2020b), SimCLR (Chen et al., 2020a), đã được đề xuất dựa trên tổn thất InfoNCE (Oord et al., 2018). Ngoài ra, các phương pháp học không đối lập, như Barlow Twins (Zbontar et al., 2021), BYOL (Grill et al., 2020) và VICReg (Bardes et al., 2022), đã được chứng minh tạo ra các biểu diễn học có chất lượng cao mà không sử dụng mẫu âm tính.

**Học liên tục** Học liên tục (CL) là quá trình thu nhận kiến thức mới trong khi giữ lại kiến thức đã học trước đó (Parisi et al., 2019; Masana et al., 2022) từ một chuỗi các tác vụ. Để cân bằng sự đánh đổi giữa tính dẻo, khả năng học tốt các tác vụ mới, và ổn định, khả năng giữ lại kiến thức của các tác vụ trước đó (Mermillod et al., 2013), nghiên cứu CL có giám sát đã được đề xuất trong một số danh mục. Để biết thêm chi tiết, có thể tham khảo (Wang et al., 2024; Delange et al., 2021).

**Học Tự Giám Sát Liên Tục** Gần đây, đã có sự quan tâm ngày càng tăng đối với Học Tự Giám Sát Liên Tục (CSSL), như được chứng minh bởi một số nghiên cứu liên quan (Rao et al., 2019; Madaan et al., 2022; Hu et al., 2022; Fini et al., 2022). Mặc dù tất cả chúng đều khám phá khả năng sử dụng các tập dữ liệu không giám sát cho CL, chúng khác nhau về quan điểm. (Rao et al., 2019) là người đầu tiên giới thiệu khái niệm học liên tục không giám sát và đề xuất một phương pháp mới để học các biểu diễn phân biệt lớp mà không cần bất kỳ kiến thức nào về danh tính tác vụ. (Madaan et al., 2022) đề xuất một phương pháp tăng cường dữ liệu mới cho CSSL và đầu tiên chứng minh rằng CSSL có thể vượt trội hơn các thuật toán CL có giám sát trong tình huống học tăng dần tác vụ. Một nghiên cứu khác (Hu et al., 2022) tập trung vào lợi ích của CSSL trong các tập dữ liệu quy mô lớn (ví dụ: ImageNet), chứng minh rằng một mô hình được tiền huấn luyện cạnh tranh có thể được thu được thông qua CSSL. Chính quy hóa đáng kể đầu tiên cho CSSL đã được đề xuất bởi CaSSLe (Fini et al., 2022). Họ đã thiết kế một chính quy hóa mới giúp vượt qua quên thảm khốc trong CSSL, đạt được hiệu suất tiên tiến trong các tình huống khác nhau mà không sử dụng bộ nhớ mẫu. Sau đó, một số bài báo đã được xuất bản nhưng chúng xem xét các cài đặt khác với CaSSLe. C2ASR (Cheng et al., 2023) xem xét việc sử dụng bộ nhớ mẫu và giới thiệu cả hàm tổn thất mới và chiến lược lấy mẫu mẫu. (Yu et al., 2024; Tang et al., 2024; Gomez-Villa et al., 2024) được điều chỉnh cho các tình huống học bán giám sát và chứng minh hiệu suất vượt trội trong các trường hợp như vậy. Ngoài ra, (Yu et al., 2024; Gomez-Villa et al., 2024) là các thuật toán dựa trên kiến trúc động trong đó mô hình mở rộng khi số lượng tác vụ tăng.

Trong bài báo này, chúng tôi đưa ra một số đóng góp đặc biệt so với các công trình liên quan đã đề cập ở trên. Đầu tiên, chúng tôi xác định những thiếu sót trong công thức tổn thất dựa trên chính quy hóa thông thường cho CSSL, như CaSSLe. Thứ hai, chúng tôi giới thiệu một khái niệm mới về âm tính giả và đề xuất một hàm tổn thất mới kết hợp khái niệm này, có thể áp dụng cho cả CSSL dựa trên học đối lập và không đối lập.

## 3. Thiết lập bài toán

**Ký hiệu và kiến thức cơ bản.** Chúng tôi đánh giá chất lượng của các phương pháp CSSL sử dụng thiết lập và dữ liệu như trong (Fini et al., 2022). Cụ thể, cho t là chỉ số tác vụ, trong đó t ∈ {1, . . . , T}, và T biểu thị số lượng tác vụ tối đa. Dữ liệu đầu vào và nhãn thực tương ứng của chúng được đưa ra tại tác vụ thứ t được ký hiệu bởi x ∈ Xt và y ∈ Yt, tương ứng¹. Cho D là toàn bộ tập dữ liệu. Chúng tôi giả định mỗi tập dữ liệu huấn luyện cho tác vụ t bao gồm M cặp có giám sát, ký hiệu là Dt = {(xi, yi)}ᴹᵢ₌₁, trong đó mỗi cặp được coi là được lấy mẫu từ một phân phối chung p(Xt, Yt). Lưu ý trong trường hợp học liên tục có giám sát (CSL) (Delange et al., 2021; Masana et al., 2022), cả đầu vào và nhãn đều được sử dụng, trong khi ở CSSL (Fini et al., 2022; Madaan et al., 2022), chỉ dữ liệu đầu vào được sử dụng để huấn luyện, trong khi nhãn thực chỉ được sử dụng để đánh giá các biểu diễn đã học, như đánh giá thăm dò tuyến tính hoặc đánh giá k-NN (Fini et al., 2022; Cha et al., 2024). Cho mψt ◦ hθt là mô hình bao gồm bộ mã hóa biểu diễn (với tham số θt) và một lớp MLP (với tham số ψt) được học sau tác vụ t. Để đánh giá chất lượng của hθt thông qua thăm dò tuyến tính, chúng tôi xem xét một bộ phân loại fΘt = oϕt ◦ hθt, trong đó Θt = (θt, ϕt) và oϕt là lớp đầu ra tuyến tính (với tham số ϕt) trên đầu hθt. Sau đó, chỉ oϕt được huấn luyện có giám sát (với hθt bị đóng băng) sử dụng tất cả tập dữ liệu huấn luyện D1:t, bao gồm các nhãn, và độ chính xác của fΘt kết quả trở thành đại diện cho chất lượng biểu diễn.

**Học tăng dần lớp/dữ liệu/miền.** Chúng tôi xem xét ba tình huống của học liên tục như được nêu trong (Van de Ven & Tolias, 2019; Wang et al., 2024; Fini et al., 2022). Chúng tôi sử dụng k và j để ký hiệu các số tác vụ tùy ý, trong đó k, j ∈ {1, . . . , T} và k ≠ j. Danh mục đầu tiên là học tăng dần lớp (Class-IL), trong đó tập dữ liệu của tác vụ thứ t bao gồm một tập hợp các lớp duy nhất cho dữ liệu đầu vào, cụ thể là p(Xk) ≠ p(Xj) và Yk ∩ Yj = ∅. Danh mục thứ hai là học tăng dần miền (Domain-IL), trong đó mỗi tập dữ liệu Dt có cùng một tập hợp nhãn thực nhưng với phân phối khác nhau trên Xt, ký hiệu là p(Xk) ≠ p(Xj) nhưng Yk = Yj. Nói cách khác, mỗi tập dữ liệu trong Domain-IL chứa các hình ảnh đầu vào được lấy mẫu từ một miền khác nhau, nhưng tập hợp nhãn thực tương ứng giống như cho các tác vụ khác. Cuối cùng, chúng tôi xem xét học tăng dần dữ liệu (Data-IL), trong đó một tập hợp hình ảnh đầu vào Xt được lấy mẫu từ một phân phối duy nhất, p(Xk) = p(Xj), nhưng Yk = Yj. Để triển khai tình huống Data-IL trong các thực nghiệm của chúng tôi, chúng tôi xáo trộn toàn bộ tập dữ liệu (như ImageNet-100) và chia nó thành T tập dữ liệu rời rạc.

## 4. Chính Quy Hóa Âm Tính Giả (PNR)

### 4.1. Động lực

Một số nghiên cứu đã được tiến hành với mục tiêu cải thiện dần chất lượng của các biểu diễn được học bởi bộ mã hóa (hθt) trong CSSL. Trong khi đã được lưu ý rằng việc tinh chỉnh đơn giản sử dụng Dt dẫn đến quên ít nghiêm trọng hơn so với học liên tục có giám sát (Madaan et al., 2022; Davari et al., 2022), CaSSLe (Fini et al., 2022) đã đạt được CSSL thành công hơn nữa. Nó giới thiệu một phương pháp chính quy hóa mới dựa trên các phương pháp SSL hiện có, sử dụng các đặc trưng đầu ra của cả bộ mã hóa tại thời điểm t và bộ mã hóa tại thời điểm t−1, để vượt qua quên thảm khốc trong các biểu diễn đã học. Mặc dù CaSSLe đạt được kết quả đầy hứa hẹn trong các thực nghiệm khác nhau, động lực của chúng tôi xuất phát từ niềm tin rằng việc kết hợp âm tính giả có thể dẫn đến kết quả thành công hơn nữa. Xem xét một hình ảnh được tăng cường thu được bằng cách áp dụng các tăng cường khác nhau cho hình ảnh đầu vào x, ký hiệu bởi xA và xB. Các đặc trưng đầu ra của mψt ◦ hθt và mψt−1 ◦ hθt−1 cho các hình ảnh được tăng cường này được ký hiệu bởi zᴬt, zᴮt, zᴬt−1, và zᴮt−1, tương ứng. Trái ngược với CaSSLe, phương pháp của chúng tôi liên quan đến việc sử dụng các đặc trưng đầu ra của cả hai mô hình như âm tính giả, như được minh họa trong Hình 1. Vì mục đích này, chúng tôi đề xuất một dạng tổn thất CSSL mới cho tác vụ t, được định nghĩa như sau:

LCSSL_t({xA, xB}; θt, θt−1)
= LSSL*_1({zᴬt, zᴮt, zᴬt−1, zᴮt−1})
+ LSSL*_2({g(zᴬt), zᴬt−1, zᴮt−1, zᴬt, zᴮt}). (1)

Ở đây, g(·) biểu thị một lớp MLP khác (được gọi là Predictor trong hình) được giới thiệu trong (Fini et al., 2022), có cùng hình dạng với mψt. Hàm tổn thất SSL LSSL*_1/2 là các dạng tổn thất chung được thiết kế mới kết hợp tất cả các âm tính giả ứng viên: zᴬt−1, zᴮt−1 cho LSSL*_1 và zᴬt, zᴮt cho LSSL*_2. Chúng giống với hai hàm tổn thất được sử dụng trong CaSSLe, và chúng tôi sẽ làm nổi bật sự khác biệt cụ thể sau này. Hơn nữa, để xem xét hai tăng cường khác nhau một cách đối xứng, chúng tôi sử dụng trung bình sau làm hàm tổn thất cuối cùng cho phương pháp của chúng tôi: 1/2(LCSSL_t(xA, xB) + LCSSL_t(xB, xA)).

Như đã đề cập ở trên, CaSSLe cũng sử dụng hai hàm tổn thất SSL, một hàm chịu trách nhiệm cho tính dẻo và hàm kia cho ổn định, nhưng chúng không sử dụng bất kỳ âm tính giả nào. Cụ thể, tổn thất tính dẻo của CaSSLe (tương ứng với LSSL*_1 của chúng tôi) được thiết kế để học các biểu diễn mới từ tác vụ mới t và chỉ sử dụng các đặc trưng đầu ra từ mô hình hiện tại t. Hơn nữa, tổn thất ổn định của CaSSLe (tương ứng với LSSL*_2 của chúng tôi) nhằm duy trì các biểu diễn được học từ tác vụ quá khứ t−1 và chỉ chính quy hóa với các đặc trưng đầu ra từ mô hình trước đó t−1. Chúng tôi lập luận rằng một hạng mục tổn thất ổn định như vậy, được thiết kế để bảo tồn các biểu diễn từ mô hình trước đó, có thể cản trở tổn thất tính dẻo, nhằm học các biểu diễn mới từ tác vụ mới nhưng không xem xét bất kỳ biểu diễn nào từ mô hình trước đó (Cha et al., 2024; Kim & Han, 2023). Để giải quyết những vấn đề này, chúng tôi đề xuất sử dụng các đặc trưng đầu ra của cả mô hình t−1 và t như âm tính giả, có thể hoạt động hiệu quả như các chất chính quy để các biểu diễn mới học được có thể không can thiệp vào các biểu diễn đã học trước đó.

Trong phần sắp tới, chúng tôi sẽ giới thiệu cụ thể các dạng hàm mới cho LSSL*_1 và LSSL*_2 được điều chỉnh để tích hợp âm tính giả vào các phương pháp học đối lập (ví dụ: SimCLR (Chen et al., 2020a), MoCo (He et al., 2020)). Ngoài ra, chúng tôi sẽ trình bày cách ý tưởng sử dụng âm tính giả có thể được áp dụng cho các phương pháp học không đối lập (ví dụ: BYOL (Grill et al., 2020), VICReg (Bardes et al., 2022), BarlowTwins (Zbontar et al., 2021)) cũng như.

### 4.2. Trường hợp Học Đối lập dựa trên InfoNCE

Ở đây, chúng tôi đề xuất các hàm tổn thất mới cho CSSL sử dụng các phương pháp SSL dựa trên học đối lập (ví dụ: SimCLR và MoCo). Đầu tiên, LSSL*_1 trong (1) được định nghĩa là:

LSSL*_1({zᴬt, zᴮt, zᴬt−1, zᴮt−1})
= −log(exp(zᴬt,i · zᴮt,i/τ) / ∑_{zj∈N₁(i)∪PN₁(i)} exp(zᴬt,i · zj/τ)), (2)

trong đó N₁(i) = {zᴬt, zᴮt}\{zᴬt,i} là tập hợp các âm tính ban đầu, và PN₁(i) = {zᴬt−1, zᴮt−1}\{zᴬt−1,i} là âm tính giả của LSSL*_1. Ngoài ra, LSSL*_2 được định nghĩa như sau:

LSSL*_2({g(zᴬt), zᴬt−1, zᴮt−1, zᴬt, zᴮt})
= −log(exp(g(zᴬt,i) · zᴬt−1,i/τ) / ∑_{zj∈N₂(i)∪PN₂(i)} exp(g(zᴬt,i) · zj/τ)), (3)

trong đó N₂(i) = {zᴬt−1, zᴮt−1}\{zᴬt−1,i} là âm tính ban đầu cho chưng cất đối lập (Tian et al., 2019; Fini et al., 2022), và PN₂(i) = {zᴬt, zᴮt}\{zᴬt,i} là âm tính giả của LSSL*_2. Ngoài ra, τ ký hiệu tham số nhiệt độ. Trong trường hợp SimCLR, N₁(i), PN₁(i), N₂(i), và PN₂(i) bao gồm các âm tính từ batch hiện tại. Khi sử dụng MoCo, hai hàng đợi lưu trữ dương tính của mỗi hàm tổn thất (LSSL*_1 và LSSL*_2) từ các vòng lặp trước đó được sử dụng cho chúng.

Lưu ý hai tổn thất này khá giống về dạng với InfoNCE (Oord et al., 2018), có một vài khác biệt chính. Đầu tiên, trong LSSL*_1, chúng tôi sử dụng âm tính giả trong PN₁(i) được thu từ mô hình trước đó hθt−1. Việc bổ sung các nhúng âm tính trong LSSL*_1 này buộc nhúng của xi phải được đẩy ra không chỉ từ các nhúng âm tính của mô hình hiện tại mà còn từ những nhúng của mô hình trước đó, do đó, nó thúc đẩy việc thu nhận các biểu diễn phân biệt hơn. Thứ hai, trong LSSL*_2, có dạng tương tự như chưng cất đối lập, chúng tôi cũng sử dụng âm tính giả trong PN₂(i). Sự sửa đổi như vậy có tác động đặt các ràng buộc bổ sung lên chưng cất, đảm bảo rằng các biểu diễn từ mô hình quá khứ được duy trì theo cách không mâu thuẫn với các biểu diễn của mô hình hiện tại. Lưu ý rằng các mẫu số của hai hàm tổn thất là giống hệt nhau ngoại trừ g(·) trong LSSL*_2. Do đó, với các mẫu số giống hệt nhau, việc cộng hai tổn thất sẽ dẫn đến việc đạt được sự đánh đổi tự nhiên giữa tính dẻo và ổn định để học biểu diễn.

Trực giác này được mô tả trong hình bên trái của Hình 2. Cụ thể, cả hai tổn thất đều xem xét đối xứng các nhúng từ mô hình hiện tại và trước đó, và như được hiển thị trong siêu cầu, biểu diễn của zᴬt,i bị thu hút đến zᴮt,i và zᴬt−1,i với ràng buộc rằng nó phải xa khỏi zᴮt−1,i. Do đó, biểu diễn mới sẽ phân biệt từ mô hình trước đó (tính dẻo) và mang theo kiến thức cũ (ổn định) theo cách không làm tổn hại mô hình hiện tại.

Phân tích gradient của hàm tổn thất đề xuất được chi tiết trong Phần A.1 của Phụ lục.

### 4.3. Trường hợp Học Không Đối lập

Các phương pháp học không đối lập, như Barlow, BYOL, và VICReg, không kết hợp các mẫu âm tính. Do đó, việc áp dụng trực tiếp âm tính giả được sử dụng cho các phương pháp học đối lập không khả thi cho các phương pháp này. Tuy nhiên, được động lực bởi cấu hình âm tính trong Phương trình (3), chúng tôi đề xuất một chính quy hóa mới xem xét âm tính giả từ một góc độ mới. Để đạt được điều này, chúng tôi đề xuất các công thức mới của LSSL*_1 và LSSL*_2, được điều chỉnh cho các phương pháp học không đối lập, như sau:

LSSL*_1({zᴬt, zᴮt, zᴬt−1, zᴮt−1}) = LSSL({zᴬt, zᴮt}), (4)

trong đó LSSL ký hiệu tổn thất SSL không đối lập, và

LSSL*_2({g(zᴬt), zᴬt−1, zᴮt−1, zᴬt, zᴮt})
= LSSL({g(zᴬt), zᴬt−1}) − λ* ∑_i ∑_{zj∈PN₂(i)} ∥g(zᴬt,i) − zj∥²₂, (5)

trong đó PN₂(i) = {zᴮt−1,i} là âm tính giả, ∥ · ∥²₂ biểu thị chuẩn L2 bình phương và λ là một siêu tham số. Ngoài ra, zᴬt−1 và zᴮt−1 của Phương trình (4), cũng như zᴬt và zᴮt của Phương trình (5), không được sử dụng khi sử dụng học không đối lập.

Lưu ý rằng LSSL của Phương trình (5) là chưng cất của CaSSLe cho một phương pháp học không đối lập, và chúng tôi giới thiệu một chính quy hóa mới để kết hợp âm tính giả. Cụ thể, zᴮt−1,i được gán như âm tính giả của zᴬt,i (neo). Việc gán này xuất phát từ cấu hình âm tính của Phương trình (3). Ví dụ, khi triển khai Phương trình (3) sử dụng SimCLR và N là kích thước mini-batch, N(i) bao gồm 2N−1 âm tính loại trừ zᴬt−1,i. Do đó, đối với một neo cho trước zᴬt,i, một đặc trưng đầu ra zᴮt−1,i từ cùng một hình ảnh xi nhưng được tác động bởi tăng cường khác nhau tự nhiên được coi là âm tính (điều này đúng khi sử dụng MoCo). Điều này dẫn đến việc tối thiểu hóa sự tương tự giữa g(zᴬt,i) và zᴮt−1,i cho tác vụ huấn luyện t. Để áp dụng khái niệm âm tính này cho CSSL sử dụng học không đối lập, chúng tôi đề xuất một chính quy hóa mới tối đa hóa sai số bình phương trung bình giữa g(zᴬt,i) và zᴮt−1,i, đảm bảo sự khác biệt của chúng.

Hình bên phải trong Hình 2 minh họa học biểu diễn với âm tính giả trong CSSL sử dụng BYOL. Học biểu diễn của CaSSLe dựa vào các hướng cập nhật khác biệt từ mỗi dương tính (ví dụ: zᴬt và zᴬt−1) để đạt được tính dẻo và ổn định được cải thiện, mà không cần xem xét âm tính. Tuy nhiên, việc kết hợp âm tính giả cho phép mô hình tránh xung đột trong việc học biểu diễn bằng cách xem xét âm tính giả từ mô hình t−1—học các biểu diễn xa khỏi âm tính giả từ mô hình t−1. Do đó, mô hình thu nhận các biểu diễn phân biệt hơn từ mô hình trước đó (tăng cường tính dẻo) trong khi giữ lại kiến thức trước đó (đảm bảo ổn định). Lưu ý rằng Phương trình (5) có thể được áp dụng kết hợp với các phương pháp SSL không đối lập khác nhau. Chi tiết triển khai cho BYOL và VICReg được cung cấp trong Phần A.2 của Phụ lục.

Chúng tôi sẽ gọi khung tổng thể sử dụng âm tính giả để chính quy hóa trong CSSL, có thể áp dụng cho cả học đối lập và không đối lập như được mô tả ở trên, là PNR (Chính Quy Hóa Âm Tính Giả).

## 5. Thực nghiệm

### 5.1. Chi tiết thực nghiệm

**Baseline** Để đánh giá PNR đề xuất, chúng tôi đặt CaSSLe (Fini et al., 2022) làm baseline chính của chúng tôi, đã cho thấy hiệu suất tiên tiến trong CSSL. Chúng tôi chọn năm phương pháp SSL: SimCLR (Chen et al., 2020a), MoCo v2 Plus (MoCo) (Chen et al., 2020b), BarlowTwins (Barlow) (Zbontar et al., 2021), BYOL (Grill et al., 2020), và VICReg (Bardes et al., 2022), đạt được hiệu suất vượt trội khi kết hợp với CaSSLe.

**Chi tiết triển khai** Chúng tôi triển khai PNR của chúng tôi dựa trên mã được cung cấp bởi CaSSLe. Chúng tôi tiến hành thực nghiệm trên bốn tập dữ liệu: CIFAR-100 (Krizhevsky et al., 2009), ImageNet-100 (Deng et al., 2009), DomainNet (Peng et al., 2019), và ImageNet-1k (Deng et al., 2009) theo quá trình huấn luyện và đánh giá được nêu trong (Fini et al., 2022). Đối với CIFAR-100 và ImageNet-100, chúng tôi thực hiện học tăng dần lớp và dữ liệu (Class- và Data-IL) cho 5 và 10 tác vụ (ký hiệu là 5T và 10T), tương ứng. Đối với học tăng dần miền (Domain-IL), chúng tôi sử dụng DomainNet (Peng et al., 2019), bao gồm sáu tập dữ liệu rời rạc từ sáu miền nguồn khác nhau. Theo các thực nghiệm được tiến hành bởi CaSSLe, chúng tôi thực hiện Domain-IL theo thứ tự tác vụ "Real → QuickDraw → Painting → Sketch → InfoGraph → Clipart". Tiếp theo, chúng tôi báo cáo độ chính xác top-1 trung bình đạt được bằng cách huấn luyện một bộ phân loại tuyến tính riêng biệt cho mỗi miền, sử dụng một bộ trích xuất đặc trưng bị đóng băng (đánh giá nhận biết miền). Mô hình ResNet-18 (He et al., 2016) được triển khai trong PyTorch được sử dụng ngoại trừ ImageNet-1k nơi ResNet-50 được sử dụng. Đối với tất cả thực nghiệm ngoại trừ ImageNet-1k, chúng tôi thực hiện mỗi thực nghiệm sử dụng ba hạt giống ngẫu nhiên và báo cáo hiệu suất trung bình qua các thử nghiệm này. Chi tiết thực nghiệm thêm có sẵn trong Phần C của Phụ lục.

**Chỉ số đánh giá** Để đo chất lượng của các biểu diễn được học trong CSSL, chúng tôi tiến hành đánh giá tuyến tính bằng cách chỉ huấn luyện lớp đầu ra trên tập dữ liệu cho trước trong khi duy trì bộ mã hóa hθt như một thành phần cố định, theo (Fini et al., 2022; Cha et al., 2024). Độ chính xác trung bình sau khi học tác vụ t được ký hiệu là At = 1/t ∑ᵀᵢ₌₁ ai,t, trong đó ai,j đại diện cho độ chính xác top-1 đánh giá tuyến tính của bộ mã hóa trên tập dữ liệu của tác vụ i sau khi kết thúc học tác vụ j. Hơn nữa, chúng tôi sử dụng các thước đo ổn định (S) và tính dẻo (P), và một lời giải thích toàn diện về các thuật ngữ này được cung cấp trong Phần B.1 của Phụ lục.

### 5.2. Thực nghiệm với các phương pháp SSL

Để đánh giá hiệu quả của PNR khi được áp dụng cho các phương pháp SSL khác nhau, chúng tôi tiến hành thực nghiệm bằng cách kết hợp PNR với MoCo, SimCLR, Barlow, BYOL, và VICReg, trong tình huống Class-IL (5T) sử dụng CIFAR-100. Hình 3 minh họa rằng cả PNR đề xuất và CaSSLe đều được kết hợp thành công với mỗi phương pháp SSL, chứng minh việc cải thiện dần chất lượng biểu diễn tại mỗi tác vụ. Tuy nhiên, PNR chứng minh tích hợp hiệu quả hơn với MoCo, SimCLR, BYOL, và VICReg, vượt trội hơn hiệu suất của CaSSLe.

Bảng 1 trình bày kết quả số của A5 trong cùng tình huống. Trong bảng này, "Joint" tương ứng với kết quả thực nghiệm trong tình huống Joint SSL (cận trên) và "FT" đại diện cho kết quả đạt được thông qua tinh chỉnh chỉ với phương pháp SSL. Bảng cho thấy PNR liên tục vượt trội hơn CaSSLe, thể hiện sự cải thiện tối đa khoảng 2-3%. Lưu ý rằng lợi ích hiệu suất bổ sung đạt được bởi PNR đặc biệt đáng chú ý, đặc biệt xem xét rằng hiệu suất của CaSSLe với MoCo, SimCLR, và BYOL đã gần với Joint.

### 5.3. Thực nghiệm với các tình huống CSSL đa dạng

**Class-IL** Bảng 2 trình bày kết quả thực nghiệm của Class-IL với các tập dữ liệu CIFAR-100 và ImageNet-100. Kết quả cho Class-IL với CIFAR-100 (5T) có thể được tìm thấy trong Bảng 1. So với phương pháp tiên tiến, CaSSLe, PNR chứng minh hiệu suất vượt trội qua tất cả các tình huống. Đáng chú ý, sự kết hợp của PNR với MoCo, BYOL, và VICReg thể hiện hiệu suất được cải thiện đáng kể so với sự kết hợp của chúng với CaSSLe. Ví dụ, trong các thực nghiệm ImageNet-100, "MoCo + PNR," "BYOL + PNR," và "VICReg + PNR" đạt được lợi ích đáng kể khoảng 2-6%, vượt trội hơn sự kết hợp của chúng với CaSSLe. Kết quả là, chúng tôi có thể xác nhận rằng sự kết hợp của PNR với mỗi phương pháp SSL đạt được hiệu suất tiên tiến trong các tình huống Class-IL.

**Class-IL với ImageNet-1k** Bảng 3 trình bày kết quả thực nghiệm của tập dữ liệu ImageNet-1k trong Class-IL (5T và 10T). Chúng tôi chỉ tiến hành thực nghiệm cho MoCo và BYOL, cả hai đều cho thấy hiệu suất vượt trội trong các thực nghiệm trước đó. Chúng tôi huấn luyện mô hình cho mỗi phương pháp sử dụng các siêu tham số giống nhau được sử dụng trong Class-IL với tập dữ liệu ImageNet-100. Kết quả thực nghiệm trong bảng làm nổi bật sự cải thiện hiệu suất đáng chú ý của PNR trong CSSL sử dụng tập dữ liệu quy mô lớn, rõ ràng trong cả phương pháp học đối lập (MoCo) và không đối lập (BYOL).

**Data-IL và Domain-IL** Bảng 4 trình bày kết quả thực nghiệm trong Data- và Domain-IL sử dụng tập dữ liệu ImageNet-100. Trong kết quả Data-IL, sự kết hợp với PNR liên tục đạt được hiệu suất tiên tiến trong hầu hết các trường hợp. Đáng chú ý là PNR của chúng tôi tích hợp thành công với MoCo một lần nữa, thể hiện hiệu suất vượt trội hoặc gần như vượt trội so với các phương pháp SSL khác ở 5T và 10T. Ví dụ, sự kết hợp "MoCo + PNR" chứng minh sự cải thiện hiệu suất 3-8% so với việc kết đôi với CaSSLe. Một xu hướng tương tự được quan sát trong Domain-IL, nơi sự kết hợp của CaSSLe với các phương pháp SSL đã chứng minh hiệu suất vượt trội gần với hiệu suất Joint tương ứng của chúng. Tuy nhiên, PNR vượt qua điều này bằng cách đạt được các cải thiện hiệu suất bổ sung, do đó thiết lập các tiêu chuẩn mới cho hiệu suất tiên tiến. Ngược lại, "BYOL + PNR" thực hiện tệ hơn "BYOL + CaSSLe" trong tình huống Data-IL. Điều này có thể được quy cho các đặc tính độc đáo của Data-IL, bao gồm xáo trộn và phân phối đều tập dữ liệu ImageNet-100 giữa các tác vụ, dẫn đến sự khác biệt phân phối tối thiểu giữa chúng. Thảo luận bổ sung về chủ đề này có sẵn trong Phần B.2 của Phụ lục.

Ngoài ra, chúng tôi đã tiến hành tất cả thực nghiệm sử dụng mã của CaSSLe và có thể tái tạo hiệu suất được báo cáo của CaSSLe trên CIFAR-100. Tuy nhiên, mặc dù có nhiều nỗ lực khác nhau, chúng tôi chỉ có thể đạt được hiệu suất thấp hơn 4-6% so với hiệu suất được báo cáo của CaSSLe trong các thực nghiệm sử dụng ImageNet-100 và DomainNet. Thảo luận thêm về vấn đề này được cung cấp trong Phụ lục B.3.

### 5.4. Phân tích thực nghiệm

**Phân tích về tính dẻo và ổn định** Hình 4 trình bày kết quả thực nghiệm của Class-IL (5T) sử dụng tập dữ liệu ImageNet-100, thể hiện biểu đồ của ak,t và Avg(a1:5,t). Từ Hình 4(a) cho thấy kết quả của "MoCo + PNR", chúng tôi quan sát một xu hướng tăng chung trong ak,t qua tất cả các tác vụ. Đáng chú ý, hiệu suất của tác vụ đầu tiên (ak=1,t) vẫn tương đối ổn định và thậm chí thể hiện sự cải thiện nhẹ khi các tác vụ tiếp theo được học. Ngược lại, kết quả được mô tả trong Hình 4(b) và 4(c) của "MoCo + CaSSLe" và "MoCo + FT" cho thấy rằng, trong khi Avg(a1:5,t) của chúng tăng dần, một số hiệu suất tác vụ trải qua sự suy giảm dần (ví dụ: ak=2,t của "MoCo + CaSSLe" và hầu hết k của "MoCo + FT"), cho thấy chịu quên thảm khốc hơn "MoCo + PNR". Hơn nữa, các đánh giá số của tính dẻo (P) và ổn định (S) của mỗi thuật toán, như được trình bày trong chú thích của Hình 4, chứng minh rằng "MoCo + PNR" đạt được sự cải thiện hiệu suất thông qua tính dẻo và ổn định vượt trội so với các baseline khác.

Hình 4(d), 4(e), và 4(f) minh họa kết quả phân tích thực nghiệm của chúng tôi được tiến hành trên "PNR + BYOL," "BYOL + CaSSLe," và "BYOL + FT" trong các thực nghiệm Class-IL (5T) sử dụng tập dữ liệu ImageNet-100. Những phát hiện này phù hợp với kết quả trước đó, cho thấy sự tăng dần trong Avg(a1:5,t). Tuy nhiên, sự suy giảm đáng kể được thấy trong ak=1,t và ak=3,t cho "BYOL + CaSSLe" qua các tác vụ, trong khi "BYOL + FT" cho thấy hiệu suất giảm ở hầu hết k. Ngược lại, việc áp dụng PNR đề xuất của chúng tôi không chỉ duy trì ak=1,t hiệu quả mà còn dẫn đến sự tăng dần trong ak=2,t và ak=4,t, cho thấy rằng PNR của chúng tôi vượt trội hơn "BYOL + CaSSLe" về tính dẻo và ổn định. Ngoài ra, các phép đo tính dẻo (P) và ổn định (S) được đề cập trong chú thích của Hình 4 hỗ trợ thêm những phát hiện thực nghiệm này.

Tóm lại, các phân tích này không chỉ làm nổi bật hiệu quả của việc tích hợp âm tính giả mà còn cung cấp bằng chứng thêm về hiệu suất vượt trội của PNR của chúng tôi. Phân tích bổ sung cho các baseline khác có thể được tìm thấy trong Phần B.4 của Phụ lục.

**Phân tích về tác động của âm tính giả** Bảng 5 trình bày kết quả thực nghiệm cho các kích thước hàng đợi khác nhau (tức là kích thước của PN₁(i) và PN₂(i)) trong "MoCo + PNR". Lưu ý rằng kích thước hàng đợi mặc định là 65536 cho tất cả các thực nghiệm trước đó. Chúng tôi quan sát rằng hiệu suất vẫn tương đối nhất quán khi kích thước hàng đợi vượt quá 16384. Tuy nhiên, sự suy giảm hiệu suất đáng kể rõ ràng với kích thước hàng đợi giảm (tức là âm tính giả giảm), đặc biệt ở 256. Dựa trên kết quả này, chúng tôi khẳng định rằng hiệu suất vượt trội của "MoCo + PNR" được quy cho việc sử dụng một số lượng lớn âm tính giả.

**Học bán giám sát và tác vụ xuôi dòng** Để đánh giá chất lượng của các biểu diễn đã học theo cách đa dạng hơn, chúng tôi tiến hành thực nghiệm trong tình huống bán giám sát. Cụ thể, chúng tôi xem xét một tình huống trong đó một bộ phân loại tuyến tính chỉ được huấn luyện sử dụng chỉ 1% hoặc 10% của toàn bộ tập dữ liệu ImageNet-100 có giám sát. Chúng tôi đánh giá mỗi bộ mã hóa được huấn luyện trong Class-IL (5T) và Data-IL (5T) sử dụng tập dữ liệu ImageNet-100. Kết quả thực nghiệm được trình bày trong các hàng trên của Bảng 6. Đáng chú ý, khi so với CaSSLe, việc áp dụng PNR cho cả MoCo và BYOL mang lại sự cải thiện hiệu suất khoảng 3-6% trong cả 10% và 1%, cho thấy hiệu suất tiên tiến mới. Các hàng dưới của Bảng 6 trình bày kết quả đánh giá tuyến tính cho các tác vụ xuôi dòng được tiến hành trên cùng các bộ mã hóa. Đối với ba tập dữ liệu, chúng tôi báo cáo độ chính xác trung bình của kết quả đánh giá tuyến tính trên các tập dữ liệu STL-10 (Coates et al., 2011), CIFAR-10, và CIFAR-100 (Krizhevsky et al., 2009). Ngoài ra, chúng tôi đặt Clipart trong DomainNet (Peng et al., 2019) như tác vụ xuôi dòng và báo cáo kết quả đánh giá tuyến tính sử dụng nó. Từ những kết quả thực nghiệm này, chúng tôi một lần nữa chứng minh rằng cả "MoCo + PNR" và "BYOL + PNR" đạt được sự cải thiện hiệu suất so với hiệu suất của CaSSLe.

Kết quả chi tiết hơn và các phát hiện bổ sung (ví dụ: thực nghiệm sử dụng mô hình được huấn luyện trong Class-IL (10T) và Data-IL (10T)) được cung cấp trong Phần B.6 của Phụ lục.

**Chi phí tính toán** Lưu ý cả PNR và CaSSLe đều phát sinh chi phí tính toán gần như giống hệt nhau, ngoại trừ "MoCo + PNR," liên quan đến một hàng đợi bổ sung để lưu trữ âm tính từ mô hình t−1. Tuy nhiên, kích thước bộ nhớ cần thiết cho hàng đợi bổ sung này là không đáng kể.

**Nghiên cứu loại bỏ** Bảng 7 trình bày kết quả của nghiên cứu loại bỏ được tiến hành trên CIFAR-100 trong tình huống Class-IL (5T). Hàng đầu tiên đại diện cho hiệu suất của "MoCo + PNR" khi tất cả âm tính được sử dụng. Trường hợp 1 đến 3 minh họa kết quả khi một trong những âm tính giả, PN₁(i) và PN₂(i) bị loại trừ hoặc khi cả hai đều bị bỏ qua. Kết quả thực nghiệm cho thấy rằng việc thiếu những âm tính này dẫn đến sự giảm hiệu suất dần, cho thấy rằng "MoCo + PNR" thu nhận các biểu diễn vượt trội bằng cách tận dụng cả âm tính giả. Cụ thể, khi PN₂(i) bị bỏ qua, sự suy giảm hiệu suất đáng kể hơn so với khi âm tính từ PN₁(i) vắng mặt. Hơn nữa, kết quả của Trường hợp 4, nơi kích thước hàng đợi gấp đôi được sử dụng cho "MoCo + CaSSLe", chứng minh rằng việc sử dụng âm tính giả đề xuất khác với việc đơn giản tăng kích thước hàng đợi. Lưu ý rằng nghiên cứu loại bỏ cho PNR với học không đối lập, như BYOL và VICReg, có thể được xác nhận bằng cách so sánh kết quả của "+CaSSLe" và "+PNR" trong Bảng 1, 2, 3, và 4.

Hơn nữa, chúng tôi trình bày kết quả thực nghiệm thêm về việc áp dụng PNR cho học đối lập có giám sát trong Phần B.5 của Phụ lục.

## 6. Hạn chế và công việc tương lai

Có một số hạn chế trong công việc của chúng tôi. Đầu tiên, chúng tôi tập trung vào Học Tự Giám Sát Liên Tục (CSSL) với kiến trúc dựa trên CNN (ví dụ: ResNet). Tuy nhiên, chúng tôi tin rằng ý tưởng đề xuất và hàm tổn thất của chúng tôi có thể được áp dụng cho CSSL sử dụng các mô hình dựa trên vision transformer. Thứ hai, chúng tôi chỉ xem xét CSSL trong lĩnh vực thị giác máy tính. Tuy nhiên, chúng tôi tin rằng khái niệm âm tính giả có thể được mở rộng cho CSSL trong các lĩnh vực khác, như xử lý ngôn ngữ tự nhiên. Chúng tôi hoãn những khám phá này cho công việc tương lai.

## 7. Nhận xét kết luận

Chúng tôi trình bày Chính Quy Hóa Âm Tính Giả (PNR), một phương pháp đơn giản nhưng mới sử dụng âm tính giả trong Học Tự Giám Sát Liên Tục (CSSL). Đầu tiên, chúng tôi làm nổi bật những hạn chế của công thức tổn thất CSSL truyền thống, có thể cản trở việc học các biểu diễn vượt trội khi huấn luyện một tác vụ mới. Để vượt qua thách thức này, chúng tôi đề xuất xem xét âm tính giả được tạo ra từ cả mô hình trước đó và hiện tại trong CSSL sử dụng các phương pháp học đối lập. Hơn nữa, chúng tôi mở rộng khái niệm PNR cho các phương pháp học không đối lập bằng cách kết hợp chính quy hóa bổ sung. Thông qua các thực nghiệm mở rộng, chúng tôi xác nhận rằng PNR của chúng tôi không chỉ có thể được áp dụng cho các phương pháp học tự giám sát mà còn đạt được hiệu suất tiên tiến với ổn định và tính dẻo vượt trội.
