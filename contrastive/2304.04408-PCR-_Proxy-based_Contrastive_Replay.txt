# 2304.04408.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/contrastive/2304.04408.pdf
# File size: 761052 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PCR: Proxy-based Contrastive Replay
for Online Class-Incremental Continual Learning
Huiwei Lin, Baoquan Zhang, Shanshan Feng*, Xutao Li, Yunming Ye
Harbin Institute of Technology, Shenzhen
flinhuiwei, zhangbaoquan g@stu.hit.edu.cn, fvictor fengss, lixutao, yeyunming g@hit.edu.cn
Abstract
Online class-incremental continual learning is a speciﬁc
task of continual learning. It aims to continuously learn new
classes from data stream and the samples of data stream
are seen only once, which suffers from the catastrophic for-
getting issue, i.e., forgetting historical knowledge of old
classes. Existing replay-based methods effectively allevi-
ate this issue by saving and replaying part of old data in a
proxy-based or contrastive-based replay manner. Although
these two replay manners are effective, the former would
incline to new classes due to class imbalance issues, and
the latter is unstable and hard to converge because of the
limited number of samples. In this paper, we conduct a
comprehensive analysis of these two replay manners and
ﬁnd that they can be complementary. Inspired by this ﬁnd-
ing, we propose a novel replay-based method called proxy-
based contrastive replay (PCR). The key operation is to re-
place the contrastive samples of anchors with correspond-
ing proxies in the contrastive-based way. It alleviates the
phenomenon of catastrophic forgetting by effectively ad-
dressing the imbalance issue, as well as keeps a faster con-
vergence of the model. We conduct extensive experiments
on three real-world benchmark datasets, and empirical re-
sults consistently demonstrate the superiority of PCR over
various state-of-the-art methods1.
1. Introduction
Online class-incremental continual learning (online
CICL) is a special scenario of continual learning [12]. Its
goal is to learn a deep model that can achieve knowledge
accumulation of new classes and not forget information
learned from old classes. In the meantime, the samples of a
continuously non-stationary data stream are accessed only
once during the learning process. At present, catastrophic
forgetting (CF) is the main problem of online CICL. It is as-
*Corresponding author
1https://github.com/FelixHuiweiLin/PCR
Softmax function
(a) Proxy -basedPositive pair Negative pairs
Softmax function
(b) Contrastive -basedPositive pair Negative pairs
Softmax function
(c) Proxy -based Contrastive Replay ( Ours )Positive pair Negative pairs: Anchor sample: Positive sample : Negative samples
: Positive proxy : Negative proxies: Number of samples in a batch
: Number of classes to learnFigure 1. Illustration of our work. (a) The example of proxy-based
replay manner. For each anchor sample, it calculates similarities
of all anchor-to-proxy pairs. (b) The example of contrastive-based
replay manner. For each anchor sample, it calculates similarities
of all anchor-to-sample pairs in the same batch. (c) The example
of our method. It calculates similarities of anchor-to-proxy pairs,
which is similar to the proxy-based method. However, the anchor-
to-proxy pairs are selected by the anchor-to-sample pairs in the
same batch, which performs in the contrastive-based manner.
sociated with the phenomenon that the model has a signif-
icant performance drop for old classes when learning new
classes. The main reason is historical knowledge of old data
would be overwritten by novel information of new data.
Among all types of methods proposed in continual learn-
ing, the replay-based methods have shown superior perfor-
mance for online CICL [25]. In this family of methods,
part of previous samples are saved in an episodic memory
buffer and then used to learn together with current samples.
In general, there are two ways to replay. The ﬁrst is the
proxy-based replay manner, which is to replay by using the
proxy-based loss and softmax classiﬁer. As shown in Fig-
ure 1(a), it calculates similarities between each anchor witharXiv:2304.04408v1  [cs.CV]  10 Apr 2023

--- PAGE 2 ---
all proxies belonging to Cclasses. A proxy can be regarded
as the representative of a sub-dataset [38], and the anchor is
one of the samples in the training batch. The second is the
contrastive-based replay manner that replays by using the
contrastive-based loss and nearest class mean (NCM) clas-
siﬁer [27]. Shown as Figure 1(b), it computes similarities
between each anchor with all Nsamples in the same train-
ing batch. Although these two manners are effective, they
have their corresponding limitations. The former is sub-
jected to the “bias” issue caused by class imbalance, tending
to classify most samples of old classes into new categories.
The latter is unstable and hard to converge in the training
process due to the small number of samples.
In this work, we comprehensively analyze their charac-
teristics and ﬁnd that the coupling of them can achieve com-
plementary advantages. On the one hand, the proxy-based
manner enables fast and reliable convergence with the help
of proxies. On the other hand, although the contrastive-
based manner is not very robust, it has advantages in the se-
lection of anchor-to-sample pairs. Only the classes associ-
ated with samples in anchor-to-sample pairs can be selected
to learn. Previous studies [1, 6] have proved that suitably
selecting of anchor-to-proxy pairs is effective to address the
“bias” issue. Therefore, it is necessary to develop a cou-
pling manner to jointly keep these advantages at the same
time. In other words, it not only takes proxies to improve
the robustness of the model as proxy-based manner, but also
overcomes the “bias” problem by selecting anchor-to-proxy
pairs as the pairs selection of contrastive-based manner.
With these inspirations, we propose a novel replay-based
method called proxy-based contrastive replay (PCR) to al-
leviate the phenomenon of CF for online CICL. The core
motivation is the coupling of proxy-based and contrastive-
based loss, and the key operation is to replace anchor-to-
sample pairs with anchor-to-proxy pairs in the contrastive-
based loss. As shown in Figure 1(c), our method calcu-
lates similarities between each anchor and other proxies,
which is similar to the proxy-based loss. However, it does
not straightly make full use of proxies from all classes. It
only takes the proxies whose associated classes of sam-
ples appear in the same batch, which is analogous to the
contrastive-based loss. For one thing, it keeps fast conver-
gence and stable performance with the help of proxies. For
another thing, it addresses the “bias” issue by only choosing
part of anchor-to-proxy pairs to calculate categorical proba-
bility. And the selected anchor-to-proxy pairs are generally
better than the ones selected by existing solutions [1, 6].
Our main contributions can be summarized as follows:
1) We theoretically analyze the characteristics of proxy-
based and contrastive-based replay manner, discover-
ing the coupling manner of them is beneﬁcial. To the
best of our knowledge, this work is the ﬁrst one to com-
bine these two manners for the online CICL problem.2) We develop a novel online CICL framework called
PCR to mitigate the forgetting problem. By replac-
ing the samples for anchor with proxies in contrastive-
based loss, we achieve the complementary advantages
of two existing approaches.
3) We conduct extensive experiments on three real-world
datasets, and the empirical results consistently demon-
strate the superiority of our PCR over various state-of-
the-art methods. We also investigate and analyze the
beneﬁts of each component by ablation studies.
2. Related work
2.1. Continual Learning
Recent advances on continual learning are driven by
three main directions. 1) Architecture-based methods [41],
also known as parameter-isolation methods, divide each
task into a set of speciﬁc parameters of the model. They
dynamically extend the model as the number of tasks in-
creases [31] or gradually freeze part of parameters to over-
come the forgetting problem [28]. 2) Regularization-based
methods [41], also called prior-based methods, store previ-
ous knowledge learned from old data as prior information
of network. It takes the historical knowledge to consolidate
past knowledge by extending the loss function with addi-
tional regularization term [13, 20]. 3) Replay-based meth-
ods, which set a ﬁxed-size memory buffer [9, 14, 22, 24, 33]
or generative model [10, 11, 34, 37] to store, produce, and
replay historical samples in the training process, also go
by the name rehearsal-based methods. This kind of meth-
ods [4, 7, 8, 23, 36] that replay old samples in the buffer are
still the most effective for anti-forgetting at present [5].
2.2. Online Class-Incremental Continual Learning.
Replay-based methods based on experience replay
(ER) [30] are the main solutions of online CICL. Some
approaches use the memory retrieval strategy to select
valuable samples from memory, such as MIR [2] and
ASER [32]. In the meantime, some approaches [3, 17, 19]
focus on saving more effective samples to the memory,
belonging to the memory update strategy . The oth-
ers [6, 15, 16, 26, 39] utilize the model update strategy to
improve the learning efﬁciency. Recently, a new method
AOP based on orthogonal projection has been proposed
without buffer. Most of them are proxy-based manners ex-
cept SCR [26], which is a contrastive-based manner.
The proposed PCR in this work exploits a new model
update strategy for online CICL, belonging to the family of
replay-based methods. Different from existing approaches,
it aims to combine the contrastive-based replay manner with
the proxy-based replay manner. By complementing their
advantages, the coupling manner can more effectively alle-
viate the phenomenon of catastrophic forgetting.

--- PAGE 3 ---
3. Problem Statement and Analysis
3.1. Problem Formulation
Online CICL divides a data stream into a sequence of
learning tasks asD=fDtgT
t=1, whereDt=fXtYt;Ctg
contains the samples Xt, corresponding labels Yt, and task-
speciﬁc classesCt. Different tasks have no overlap in the
classes. The neural network is made up of a feature extrac-
torz=h(x;)and a proxy-based classiﬁer f(z;W) =
hz;Wi[18], where Wcontains trainable proxies of all
classes,h;iis the cosine similarity, and is a scale factor.
All of learned classes are denoted as C1:t=St
k=1Ck. The
categorical probability that sample xbelongs to class cis
pc=ehh(x;);wci
P
j2C1:tehh(x;);wji: (1)
In the training process, the model can only access Dtand
each sample can be seen only once. Its objective function is
L=E(x;y)Dt[ log(ehh(x;);wyi
P
j2C1:tehh(x;);wji)]: (2)
3.2. Analysis of Catastrophic Forgetting.
A direct cause of CF is the unbalanced gradient propaga-
tion between old and new classes. The gradient for a single
sample xcan be expressed as
@L
@W=(
h(x;)(py 1); i=y
h(x;)(pc); c6=y: (3)
As Equation (3) shows, if a training sample xbelongs to
classy, it not only increases the logits value of the y-th di-
mension bypy 1<0, but also decreases the logits value of
other dimension by pc>0. Combining with the chain rule,
it provides the positive gradient for the proxy of class yas
wy=wy h(x;)(py 1), and propagates the negative
gradient to the other proxies as wc=wc h(x;)(pc).
Sinceis a positive learning rate and h(x;)is usually
non-negative by Relu [29]. Furthermore, the gradient is
transferred to the feature extractor, making it focus on the
features that can distinguish this class from other classes.
When directly optimizing Equation (2), which is known
as Finetune, the learning of new classes dominates the gra-
dient propagation, causing the phenomenon of CF. To better
analyse it, we show a case that learns the samples of cat and
dog at the ﬁrst task (Figure 2(a)), and then learns the sam-
ples of ship and airplane at the next task (Figure 2(b)-(f)).
As seen in the left part of Figure 2(b), the gradient is pro-
duced by learning new classes. As a result, the proxies of
new classes receive more positive gradient ( ") and the others
obtain more negative gradient ( #). Shown as the red arrows
in the left part of Figure 2(b), it causes the proxies of new
classes are close to the samples of new classes, while theproxies of old classes are far away from them. Meanwhile,
the feature extractor pays more attention to the features of
new classes. It causes the samples of new and old classes
are close [6] in the unit embedding space. Hence, it is easy
to classify samples to new classes.
3.3. Analysis of Proxy-based Manner
ER[30] allocates a memory buffer Mto temporarily
store part of previous samples of old classes, which are re-
trained with current samples. And its objective function is
LER=E(x;y)Dt[M[ log(ehh(x;);wyi
P
j2C1:tehh(x;);wji)];(4)
where the samples of all classes take the same way to cal-
culate categorical probability. As described in Figure 2(c),
previous samples of old classes acquire some advantages in
the propagation of gradient. Not only the proxies of old
classes obtain more positive gradient, but also the proxies
of new classes receive more negative gradient. Although
the phenomenon of CF can be alleviated to some extent, its
effect is still limited. Since the number of samples for each
class in the ﬁxed buffer will decrease as the learning process
goes on, the gradient of old classes are not enough.
SS-IL [1] separately calculates categorical probability
for old and new classes by separated softmax as
LSS=E(x;y)Dt[ log(ehh(x;);wyi
P
j2Ctehh(x;);wji)]
+E(x;y)M[ log(ehh(x;);wyi
P
j2C1:t 1ehh(x;);wji)]:(5)
As demonstrated in Figure 2(d), it cuts off the propaga-
tion from the learning of old classes to the proxies of new
classes, and prevents the propagation from the learning of
new classes to the proxies of old classes. It is able to avoid
that the gradient of new classes affect the proxies of old
classes. However, the model can not well distinguish new
classes from old classes, since the lacking of gradient makes
it difﬁcult for the model to classify classes across tasks.
ER-ACE [6] is also proposed to address the same issue
by an asymmetric cross-entropy loss, which is expressed as
LACE =E(x;y)Dt[ log(ehh(x;);wyi
P
j2Ctehh(x;);wji)]
+E(x;y)M[ log(ehh(x;);wyi
P
j2C1:tehh(x;);wji)]:(6)
Its categorical probability of new classes is similar with SS-
IL, and the categorical probability of old classes is the same
as ER. In detail, it only selects part of anchor-to-proxy pairs
for the learning of new classes. As shown in Figure 2(e),

--- PAGE 4 ---
↑
↑SO SN
POA
PON
PNA
PNN
(a) Finish the first work 
↓
↓
↑
↑SO SN
POA
PON
PNA
PNN
(b) Finetune↑
↑
↓
↓↓
↓
↑
↑SO SN
POA
PON
PNA
PNN
(c) ER
↑
↑
↑
↑SO SN
POA
PON
PNA
PNN
(d) SS -IL↑
↓↓
↑SO SN
POA
PON
PNA
PNN
(f) Ours↑
↑
↓
↓↑
↑SO SN
POA
PON
PNA
PNN
(e) ER -ACEGradients of learning samples from Old classes;
Gradients of learning samples from New classes;
Proxies of Old classes whose associated samples either Appear ( POA ) or Not appear ( PON ) in the same training batch;
Proxies of New classes whose associated samples either Appear ( PNA ) or Not appear ( PNN ) in the same training batch;Samples of Old classes ( SO); 
Samples of New classes ( SN); Figure 2. Analysis of existing proxy-based manners. In each sub-ﬁgure, the left part is the process of gradient propagation from samples to
all proxies, and the right part is the unit embedding space of samples and proxies. (a) The learning of the ﬁrst task. The gradient propagation
only exists in current two classes, denoted as the blue arrows. (b) The learning of the second task by Finetune. The new classes dominate
the gradient propagation, denoted as the red arrows. (c) The learning of the second task by ER. (d) The learning of the second task by
SS-IL. (e) The learning of the second task by ER-ACE. (f) The learning of the second task by our method. Different from existing studies,
our method controls the process of gradient propagation more effectively, improving the recognition of new and old classes.
it only breaks the gradient propagation from the learning
of new classes to the proxies of old classes. Keeping the
gradient from the learning of old classes to the proxies of
new classes helps to avoid the inseparable situation of SS-
IL. Although it is beneﬁcial for old classes, the performance
on new classes is harmed.
3.4. Analysis of Contrastive-based Manner
SCR [26] is proposed as a good alternative for online
CICL by contrastive-based loss, which is denoted as
LSCR =E(x;y)Dt[M
[ 1
jP(x)jX
p2P(x)logehh(x;);h(xp;)i=T
P
j2J(x)ehh(x;);h(xj;)i=T]: (7)
It splices current samples and previous samples into the
same batch and calculates the similarities of anchor-to-
samples pairs. J(x)is the indices set of samples except
for anchor xin the same batch, while P(x)denotes the set
of samples with the same labels as anchor x. Different from
proxy-based loss, the selected pairs do not rely on the num-
ber of classes, but are related to the number of samples in a
training batch. Hence, its effect is constrained by the size of
memory buffer and batch size. And its performance would
be not satisfactory when less samples to replay.4. Methodology
4.1. Motivation
From above analysis, we can draw three conclusions.
First and foremost, the unbalanced gradient propagation be-
tween new classes and old classes is the main cause of CF.
The new classes dominate this process, making the sam-
ples of new classes highly distinguishable but the ones of
old classes indivisible. Effectively controlling the gradi-
ent propagation between old and new classes can help the
model alleviate the forgetting problem. Second, existing
proxy-based approaches control the gradient propagation
by selecting part of anchor-to-proxy pairs to calculate the
objective function. Although they are effective, they are
easy to hurt the generalization ability of model to learn new
classes. Finally, the contrastive-based manner depends on
the samples from the same batch but lacks the support of
proxies. Its selection of anchor-to-sample pairs provides a
heuristic way to select anchor-to-proxy pairs.
Based on these conclusions, we ﬁnd that the coupling
of these two manners would lead to a better solution. To
avoid the limit caused by the size of samples, we do not take
the coupling method in [38], which adds anchor-to-sample
pairs to anchor-to-proxy pairs in cross-entropy loss. Specif-

--- PAGE 5 ---
ically, we replace the samples of anchor-to-sample pairs by
proxies for in contrastive-based loss, and obtain our manner
Lours=E(x;y)Dt[M
[ 1
jP(x)jX
p2P(x)logehh(x;);wpi=T
P
j2J(x)ehh(x;);wji=T]: (8)
Different from existing studies, its way of computing cat-
egorical probability is changed for each mini-batch. On the
one hand, such a loss has faster convergence speed and bet-
ter robustness, and can cope with a small number of samples
with the help of proxies. On the other hand, the replacing
proxies are only from the classes that appear in the train-
ing batch. As a result, the gradient for propagation are only
from the learning of these classes. As shown in Figure 2(f),
the gradient among all proxies are not completely separated
in the whole training process. The gradient propagation
only occurs when the corresponding classes appear in the
same batch. Meanwhile, in each learning step, only new
and old classes in current batch participate in the gradient
propagation. The proxies of old classes, which are affected
by the negative gradient of new classes, can also generate
the positive gradient for confrontation and further mitigate
the forgetting problem. Hence, the samples of all classes
can be recognized more correctly than existing methods.
4.2. Proxy-based Contrastive Replay
With these inspirations, we propose a novel proxy-based
contrastive replay (PCR) framework, and the technical de-
tails will be stated in this section. The framework consists of
a CNN-based backbone h(x;)and a proxy-based classi-
ﬁerf(z;W). The whole training and inference procedures
of PCR are summarized in Algorithm 1.
4.2.1 The Training Procedure of PCR
In this part, the model is trained by learning samples of new
classes and replaying samples of old classes. For each task,
given current samples (xc;yc), it randomly retrieves previ-
ous samples (xM;yM)from the memory buffer (line 1-4).
Besides, these original samples and their augmented sam-
ples are spliced together for the batch of training (line 5-7).
Then, the model is optimized by this training batch (line
8-9). The objective function is deﬁned as
LPCR =E(x;y)Dt[M[ log(ehh(x;);wyi
P
j2CBehh(x;);wji)];(9)
whereCBis the classes indices in current batch of training,
and the indices can be repeated. Finally, it updates the mem-
ory buffer by reservoir sampling strategy, which can ensure
that the probability of each sample being extracted is equal.
Conveniently, the memory buffer in our framework has a
ﬁx-sized, no matter how large the amount of samples is.Algorithm 1 Proxy-based Contrastive Replay
Input :DatasetD, Learning Rate , Scale factor 
Output :Network Parameters 
Initialize :Memory BufferM fg , Network Parameters =
f;Wg
1:fort2f1;2;:::;Tgdo
2: //TrainingProcedure
3: formini-batch (xc;yc)Dtdo
4: (xM;yM) RandomRetrieval (M).
5: (xori;yori) Concat ([(xc;yc);(xM;yM)]).
6: (xaug;yaug) DataAugmentation (xori;yori).
7: (x;y) Concat ([(xori;yori);(xaug;yaug)]).
8:L= log(ehh(x;);wyi
P
j2CBehh(x;);wji)
9: +rL.
10:M ReservoirUpdate (M;(xt;yt)).
11: end for
12: //InferenceProcedure
13: fork2f1;2;:::;mgdo
14:y
k = arg maxcehh(xk;);wci
P
j2C1:tehh(xk;);wji;c2C1:t
15: end for
16: return
17:end for
4.2.2 The Inference Procedure of PCR
The inference procedure (line 13-15) is different from the
training procedure. Each testing sample xkobtains its class
probability distribution by Equation (1). And we perform
the inference prediction to xkwith highest probability as
y
k= arg max
cehh(xk;);wci
P
j2C1:tehh(xk;);wji;c2C1:t:(10)
5. Performance Evaluation
5.1. Experiment Setup
5.1.1 Datasets
We conduct experiments on three real-world image datasets
for evaluation. Split CIFAR10 [21] is split into 5 tasks, and
each task contains 2 classes. Split CIFAR100 [21] as well
as Split MiniImageNet [35] are organized into 10 tasks, and
each task is made up of samples from 10 classes.
5.1.2 Evaluated Baselines
To evaluate the effectiveness of PCR, we compare it with
the following four methodological categories. None-replay
operations contain IID and FINE-TUNE. Memory up-
date strategies include ER [30], GSS [3], and GMED [19].
MIR [2] and ASER [32] are memory retrieval strategies .
And model update strategies contains A-GEM [9], ER-
WA [40], DER++ [4], SS-IL [1], SCR [26], ER-ACE [6],
ER-DVC [15], and OCM [16].

--- PAGE 6 ---
Table 1. Final Accuracy Rate (higher is better). The best scores are in boldface, and the second best scores are underlined.
Datasets [sample size] Split CIFAR10 [3232] Split CIFAR100 [32 32] Split MiniImageNet [84 84]
Buffer 100 200 500 1000 500 1000 2000 5000 500 1000 2000 5000
IID 55.9 ±0.4 17.1 ±1.0 17.3 ±1.7
FINE-TUNE 17.9 ±0.4 5.9±0.2 4.3±0.2
ER 33.8 ±3.2 41.7 ±2.8 46.0 ±3.5 46.1 ±4.3 14.5 ±0.8 17.6 ±0.9 19.7 ±1.6 20.9 ±1.2 11.2 ±0.6 13.4 ±0.9 16.5 ±0.9 16.2 ±1.7
GSS 23.1 ±3.9 28.3 ±4.6 36.3 ±4.1 44.8 ±3.6 14.6 ±1.3 16.9 ±1.4 19.0 ±1.8 20.1 ±1.1 10.3 ±1.5 13.9 ±1.0 14.6 ±1.1 15.5 ±0.9
GMED (NeurIPS2021) 32.8 ±4.7 43.6 ±5.1 52.5 ±3.9 51.3 ±3.6 15.0 ±0.9 18.8 ±0.7 21.1 ±1.2 23.0 ±1.5 11.9 ±1.2 15.3 ±1.3 18.0 ±0.8 19.6 ±1.0
MIR (NeurIPS2019) 34.8 ±3.3 40.3 ±3.3 42.6 ±1.7 47.4 ±4.1 14.8 ±0.7 18.1 ±0.7 20.3 ±1.6 21.6 ±1.7 11.9 ±0.6 14.8 ±1.1 17.2 ±0.8 17.2 ±1.2
ASER (AAAI2021) 33.7 ±3.7 31.6 ±3.4 42.1 ±3.0 42.3 ±2.9 13.0 ±0.9 16.1 ±1.1 17.7 ±0.7 18.9 ±1.0 10.5 ±1.1 13.8 ±0.9 16.1 ±0.9 18.1 ±1.1
A-GEM (ICLR2019) 17.5 ±1.7 17.4 ±2.1 17.9 ±0.7 18.2 ±1.5 5.4±0.6 5.6±0.5 5.4±0.7 4.6±1.0 5.0±1.0 4.7±1.1 5.0±2.3 4.8±0.8
ER-WA (CVPR2020) 36.9 ±2.9 42.5 ±3.4 48.6 ±2.7 45.9 ±5.3 18.3 ±0.7 21.7 ±1.2 23.6 ±0.9 24.0 ±1.8 15.1 ±0.7 17.1 ±0.9 18.9 ±1.4 18.5 ±1.5
DER++ (NeurIPS2020) 40.9 ±1.4 45.3 ±1.7 52.8 ±2.2 53.9 ±1.9 15.5 ±1.0 17.2 ±1.1 19.5 ±1.2 20.2 ±1.3 11.9 ±1.0 14.8 ±0.7 16.1 ±1.3 15.5 ±1.3
SS-IL (ICCV2021) 36.8 ±2.1 42.2 ±1.4 44.8 ±1.6 47.4 ±1.5 19.5 ±0.6 21.9 ±1.1 24.5 ±1.4 24.7 ±1.0 18.0 ±0.7 19.7 ±0.9 21.7 ±1.0 24.4 ±1.6
SCR (CVPR-W2021) 35.0 ±2.9 45.4 ±1.0 55.7 ±1.6 59.8 ±1.6 13.3 ±0.6 16.2 ±1.3 18.2 ±0.8 19.3 ±1.0 12.1 ±0.7 14.7 ±1.9 16.8 ±0.6 18.6 ±0.5
ER-DVC (CVPR2022) 36.3 ±2.6 45.4 ±1.4 50.6 ±2.9 52.1 ±2.5 16.8 ±0.8 19.7 ±0.7 22.1 ±0.9 24.1 ±0.8 13.9 ±0.6 15.4 ±0.7 17.2 ±0.8 19.1 ±0.9
OCM (ICML2022) 44.4 ±1.5 49.9 ±1.8 55.8 ±2.3 59.2 ±2.2 17.7 ±1.0 20.6 ±1.2 22.1 ±1.0 22.7 ±1.4 11.1 ±0.6 13.6 ±0.7 16.5 ±0.5 19.2 ±0.7
ER-ACE (ICLR2022) 44.3 ±1.5 49.7 ±2.4 54.9 ±1.4 57.5 ±1.9 19.7 ±0.8 23.1 ±0.8 24.8 ±0.9 27.0 ±1.2 18.1 ±0.5 20.3 ±1.3 24.8 ±1.1 26.2 ±1.0
,!ER-ACE-NCM (45.0±1.3) (51.0±1.2) (56.8±1.1) (60.1±1.0) (21.0±0.6) (24.2±0.7) (26.6±0.7) (29.1±1.0) (18.1±0.8) (22.3±0.5) (25.3±0.5) (27.1±0.9)
PCR (Ours) 45.4 ±1.3 50.3 ±1.5 56.0 ±1.2 58.8 ±1.6 21.8 ±0.9 25.6 ±0.6 27.4 ±0.6 29.3 ±1.1 20.9 ±0.9 24.2 ±0.9 27.2 ±1.2 28.4 ±0.9
,!PCR-NCM (43.7±1.2) (49.1±1.3) (56.2±1.2) (59.9±1.8) (22.6±0.6) (26.0±0.4) (28.2±0.6) (30.1±1.0) (19.8±0.6) (23.5±0.6) (26.8±0.5) (28.0±0.6)
5.1.3 Evaluation Metrics
We need to measure the performance of model for online
CICL. Most important of all, we deﬁne ai;j(j <=i)as the
accuracy evaluated on the held-out test samples of the jth
task after the network has learned the training samples in
the ﬁrstitasks. Similar with [32], we can acquire average
accuracy rate Aiat theith task based on ai;j(j <=i).
Ai=1
iiX
j=1ai;j (11)
If the model ﬁnish learning all of Ttasks,ATis equivalent
to the ﬁnal accuracy rate. Furthermore, we decompose the
accuracy rate to obtain the average accuracy rate of new data
An
i=ai;iand the one of old data Ao
iat theith task, where
Ao
i=1
i 1i 1X
j=1ai;j: (12)
5.1.4 Implementation Details
The basic setting of backbone model is the same as the
latest work [6]. In detail, we take the Reduced ResNet18
(the number of ﬁlters is 20) as the feature extractor for all
datasets. During the training phase, the network is trained
with the SGD optimizer and the learning rate is set as 0.1.
For all datasets, the classes are shufﬂed before division.
And we set the memory buffer with f100, 200, 500, 1000 g
for Split CIFAR10, and with f500, 1000, 2000, 5000 gfor
other two datasets. The model receives 10 current samples
from data stream and 10 previous samples from the mem-
ory buffer at a time irrespective of the size of the memory.Table 2. Final Accuracy Rate (higher is better) on Split CIFAR100.
Buffer 1000 2000 5000
SCR [16] 26.5 ±0.2 31.6 ±0.5 36.5 ±0.2
OCM [16] 28.1 ±0.3 35.0 ±0.4 42.4 ±0.5
PCR 29.3 ±0.6 36.3 ±0.9 46.5 ±0.8
PCR-NCM 31.0 ±0.9 37.7 ±0.8 47.9 ±0.6
Moreover, we employ a combination of various augmenta-
tion techniques to get the augmented images. And the usage
of data augmentation is fair for all methods. As for the test-
ing phase, we set 256 as the batch size of validation.
5.2. Overall Performance
In this section, we conduct experiments to compare with
various state-of-the-art baselines of continual learning. Ta-
ble 1 shows the ﬁnal average accuracy rate for Split CI-
FAR10, Split CIFAR100 and Split MiniImageNet. All re-
ported scores are the average score of 10 runs with the 95%
conﬁdence interval. It is easy to ﬁnd that our method ob-
tains signiﬁcantly improved performance on three datasets.
Comparison on ﬁnal accuracy. Table 1 reports the
accuracy performance of all baseline methods on three
datasets. By comparing the ﬁnal accuracy of all methods,
we can draw two conclusions. First, the model update
strategies are more effective and can greatly improve the
performance among all replay-based methods. Second, ER-
ACE achieves the highest accuracy rate as the latest method.
Our method PCR achieves the best performance, which
conﬁrms its effectiveness. In general, PCR achieves the
best performance under most experimental settings in which
each dataset contains four memory buffer of different sizes.
On the one hand, it has the most outstanding performance

--- PAGE 7 ---
1 2 3 4 5 6 7 8 9 10
Learning Tasks152025303540455055Average Accuracy
ER
DER++
ER-WA
MIR
SCR
ER-ACE
ER-DVC
PCR
PCR-NCM(a) Split CIFAR100
1 2 3 4 5 6 7 8 9 10
Learning Tasks15202530354045Average Accuracy
ER
DER++
ER-WA
MIR
SCR
ER-ACE
ER-DVC
PCR
PCR-NCM (b) Split MiniImageNet
Figure 3. Average accuracy rate on observed learning stages on Split CIFAR100 and Split MiniImageNet while the buffer size is 1000.
1 2 3 4 5 6 7 8 9 10
Learning Tasks2530354045Average Accuracy
SCR
ER-ACEPCR
PCR-NCM
(a) Performance on novel knowledge
1 2 3 4 5 6 7 8 9 10
Learning Tasks0510152025303540Average Accuracy
SCR
ER-ACEPCR
PCR-NCM (b) Performance on historical knowledge
Figure 4. Average accuracy rate on observed learning stages Split MiniImageNet while the buffer size is 5000.
on Split MiniImageNet among three benchmarks. On
the other hand, the growth of buffer size further improve
the performance of PCR. For example, PCR improves the
proxy-based baseline ER and contrastive-based baseline
SCR with a gap of more than 10%. Meanwhile, PCR out-
performs the strongest baseline ER-ACE by 2.8%, 3.9%,
2.4%, 2.2% on Split MiniImagenet when the size of mem-
ory buffer is 500, 1000, 2000 and 5000, respectively. In ad-
dition, PCR defeats ER-ACE with an improvement of 2.1%,
2.5%, 2.6% and 2.3% on Split CIFAR100 with 500, 1000,
2000 and 5000 size of memory buffer, respectively.
Although there is a scenario on Split CIFAR10 that does
not achieve the strongest result, the overall performance
on this dataset is the best. By the buffer with 1000 sam-
ples, SCR that uses NCM classiﬁer beats PCR on Split CI-
FAR10. We replace the proxy-based classiﬁer by the NCM
one and get the ER-ACE-NCM as well as PCR-NCM. By
NCM classiﬁer, there will be some signiﬁcant improvement
in their performance. And the results state that NCM clas-
siﬁer is more suitable for situation with smaller samples
(e.g. Split-CIFAR100) and a larger buffer. Since small-
sized samples are easy to obtain reliable features, and large-
sized buffer trends to obtain accurate classiﬁcation centers.
In addition, OCM performs better than PCR when the buffer
is large with the advantages of multiple data augmentations.
In the same time, we also compare PCR with SCR and
OCM under the experiment setting in [16], shown as Ta-
ble 2. In [16], the model is set as ResNet18 (the numberof ﬁlters is 64), and it retrieves 64 samples from memory
buffer for each training batch. Moreover, it is trained with
Adam optimizer and the learning rate is set as 0.001. These
experimental conditions are different from ours and can sig-
niﬁcantly improve the performance. The results suggest that
our method is signiﬁcantly better than SCR and OCM.
Comparison on learning process. For more detailed
comparison, we reveal the accuracy performance in each
task for part of effective approaches on all datasets, as
shown in Figure 3. The results of line chart show that PCR
not only achieves signiﬁcant results in the accuracy of ﬁ-
nal task, but also performs better than other baselines in
the whole learning process. In fact, with the help of NCM
classiﬁer, the overall performance of PCR-NCM is the best
one. However, it has to calculate classiﬁcation centers be-
fore its inference process, which greatly reduces its efﬁ-
ciency. Furthermore, the performance of PCR in the ﬁrst
few tasks does not outperform other baselines. However,
the improvement of it become more and more visible as the
number of tasks increases, proving its power to overcome
CF. For instance, PCR has no obvious advantages in the
ﬁrst task, but it shows real effect in the remaining tasks on
Split CIFAR100. Meanwhile, PCR and ER-ACE, especially
PCR, are far superior in performance to other baselines on
Split MiniImagenet. Therefore, our approach has a stronger
ability to resist forgetting in the case of less data.
Comparison on knowledge balance. Actually, we
should not only focus on the model’s ability to retain histori-

--- PAGE 8 ---
Table 3. Final Accuracy Rate (higher is better) on three datasets for ablation studies. ER is a baseline method.
Datasets Split CIFAR10 Split CIFAR100 Split MiniImageNet
Buffer 100 200 500 1000 500 1000 2000 5000 500 1000 2000 5000
ER 33.8 ±3.2 41.7 ±2.8 46.0 ±3.5 46.1 ±4.3 14.5 ±0.8 17.6 ±0.9 19.7 ±1.6 20.9 ±1.2 11.2 ±0.6 13.4 ±0.9 16.5 ±0.9 16.2 ±1.7
ER+A 39.4 ±3.2 48.8 ±1.1 52.3 ±1.7 53.9 ±3.4 20.5 ±0.9 25.5 ±0.6 26.0 ±0.7 28.2 ±0.7 19.4 ±1.3 22.8 ±0.8 25.6 ±1.1 27.5 ±1.0
ER+A+B (PCR) 45.4 ±1.3 50.3 ±1.5 56.0 ±1.2 58.8 ±1.6 21.8 ±0.9 25.6 ±0.6 27.4 ±0.6 29.3 ±1.1 20.9 ±0.9 24.2 ±0.9 27.2 ±1.2 28.4 ±0.9
ER+A+B+C 41.9 ±2.0 48.3 ±2.4 55.8 ±1.2 57.0 ±2.6 19.8 ±1.0 24.1 ±0.5 25.9 ±0.5 27.3 ±0.7 19.3 ±1.2 21.8 ±0.8 24.5 ±0.9 26.3 ±1.0
1 2 3 4 5 6 7 8 9 10
Index of Classes200
150
100
50
050Cumulative GradientsIID ER ER-ACE PCR
Figure 5. Cumulative gradients of different methods for all proxies
when the model learns new classes (9/10) on Split CIFAR10.
cal knowledge, but also ensure the model’s ability to quickly
learn novel knowledge. Shown in Figure 4, we record the
accuracy performance of novel and historical knowledge in
each task for part of effective methods on Split MiniIma-
geNet. Although SCR and ER-ACE can improve the anti-
forgetting ability of the model, they have a serious impact
on the generalization ability of the model. As the historical
knowledge is consolidated, the learning performance of the
model on novel knowledge becomes very poor. Different
from existing studies, our model can not only effectively al-
leviate the phenomenon of CF, but also reduce the decline
of the model at the generalization level as much as possible.
5.3. Ablation Study
In this section, we decompose PCR into several compo-
nents, and further demonstrate their functions.
“A” means the selection component , which selects the
anchor-to-proxy pairs as the contrastive-based replay man-
ner. Actually, there are some samples from the same classes
in a training batch. As a result, there are some duplicate
anchor-to-proxy pairs in PCR. To effectively verify the role
of the selected proxies, we remove the duplicate pairs. As
shown in Table 3, the performance of ER has signiﬁcant
improvement with the help of this component.
“B” is the duplication component to keep the dupli-
cate pairs in PCR. Although it contains the same knowledge
as the anchor-to-proxy pairs of selection component, it still
produces signiﬁcant performance. Since it can provide an-
chor samples with more negative pairs, which are vital to
contrastive-based loss. As stated in Table 3, PCR outper-
forms “ER+A” with the duplication component.
“C” denotes the addition component to exploit original
anchor-to-sample pairs of SCR. Combining PCR with this
part, we can get another coupling manner as [38]. It keeps
anchor-to-sample pairs of contrastive-based loss, when re-
placing anchor-to-sample pairs by anchor-to-proxy pairs as
PCR. Although it provides more knowledge about the re-
10 20 30 40 50 60 70 80 90100 110 120 130 140 150 160 170 180 190 200
Batch Size7.510.012.515.017.520.022.525.0Average Accuracy
ER
ER-ACE
PCRFigure 6. Final accuracy rate (higher is better) on Split CIFAR100
with different batch size when the buffer size is 1000.
lationships of samples, its performance is limited by little
number of samples as indicated in Table 3.
In conclusion, the selection component and the duplica-
tion component are the keys of PCR. As displayed in Fig-
ure 5, PCR produces uniform gradients for all proxies to
address the “bias” issue by the smart selection of anchor-
to-proxy pairs. Furthermore, since the way of selection de-
pends on the classes in the training batch, PCR is inﬂuenced
by the batch size. Figure 6 reports the performance of sev-
eral methods with different batch size. With the increase of
batch size, PCR consistently maintains its advantages.
6. Conclusion
In this paper, we develop a novel online CICL method
called PCR to alleviate the phenomenon of catastrophic
forgetting by the coupling of proxy-based and contrastive-
based replay manners. Based on the characteristics of these
two manners, we propose to replace the samples of anchor-
to-sample pairs by proxies. The coupling replay manner
realizes complementary advantages. With the help of prox-
ies, our method keeps the fast and stable convergence. In
the meantime, the same selection of anchor-to-proxy pairs
as contrastive samples is beneﬁcial for addressing the “bias”
issue of proxy-based manner. Extensive experiments on
three datasets demonstrate the superiority of PCR over a
large variety of state-of-the-art methods.
Acknowledgement
This work was supported in part by National Na-
ture Science Foundation of China (No.62202124 and
No.62272130), Shenzhen Science and Technology Pro-
gram (No.KCXFZ20211020163403005) and Nature Sci-
ence Program of Shenzhen (No.JCYJ20210324120208022
and No.JCYJ20200109113014456).

--- PAGE 9 ---
References
[1] Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang,
Hyojun Kim, and Taesup Moon. Ss-il: Separated softmax
for incremental learning. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 844–
853, 2021. 2, 3, 5
[2] Rahaf Aljundi, Eugene Belilovsky, Tinne Tuytelaars, Lau-
rent Charlin, Massimo Caccia, Min Lin, and Lucas Page-
Caccia. Online continual learning with maximal interfered
retrieval. Advances in Neural Information Processing Sys-
tems, 32:11849–11860, 2019. 2, 5
[3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-
gio. Gradient based sample selection for online continual
learning. Advances in neural information processing sys-
tems, 32, 2019. 2, 5
[4] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide
Abati, and Simone Calderara. Dark experience for gen-
eral continual learning: a strong, simple baseline. Advances
in neural information processing systems , 33:15920–15930,
2020. 2, 5
[5] Pietro Buzzega, Matteo Boschini, Angelo Porrello, and Si-
mone Calderara. Rethinking experience replay: a bag of
tricks for continual learning. In 2020 25th International Con-
ference on Pattern Recognition (ICPR) , pages 2180–2187.
IEEE, 2021. 2
[6] Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne Tuyte-
laars, Joelle Pineau, and Eugene Belilovsky. New insights
on reducing abrupt representation change in online continual
learning. In International Conference on Learning Repre-
sentations , 2021. 2, 3, 5, 6
[7] Hyuntak Cha, Jaeho Lee, and Jinwoo Shin. Co2l: Con-
trastive continual learning. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages 9516–
9525, 2021. 2
[8] Arslan Chaudhry, Albert Gordo, Puneet Dokania, Philip
Torr, and David Lopez-Paz. Using hindsight to anchor past
knowledge in continual learning. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence , volume 35, pages
6993–7001, 2021. 2
[9] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach,
and Mohamed Elhoseiny. Efﬁcient lifelong learning with a-
gem. In International Conference on Learning Representa-
tions , 2018. 2, 5
[10] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Dual-
teacher class-incremental learning with data-free genera-
tive replay. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages 3543–
3552, 2021. 2
[11] Bo Cui, Guyue Hu, and Shan Yu. Deepcollaboration: Col-
laborative generative and discriminative models for class in-
cremental learning. In Proceedings of the AAAI Confer-
ence on Artiﬁcial Intelligence , volume 35, pages 1175–1183,
2021. 2
[12] Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah
Parisot, Xu Jia, Ales Leonardis, Greg Slabaugh, and Tinne
Tuytelaars. A continual learning survey: Defying forgettingin classiﬁcation tasks. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence , 2021. 1
[13] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng,
Ziyan Wu, and Rama Chellappa. Learning without memoriz-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 5138–5146,
2019. 2
[14] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li.
Orthogonal gradient descent for continual learning. In Inter-
national Conference on Artiﬁcial Intelligence and Statistics ,
pages 3762–3773. PMLR, 2020. 2
[15] Yanan Gu, Xu Yang, Kun Wei, and Cheng Deng. Not just
selection, but exploration: Online class-incremental contin-
ual learning via dual view consistency. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 7442–7451, 2022. 2, 5
[16] Yiduo Guo, Bing Liu, and Dongyan Zhao. Online continual
learning through mutual information maximization. In In-
ternational Conference on Machine Learning , pages 8109–
8126. PMLR, 2022. 2, 5, 6, 7
[17] Jiangpeng He and Fengqing Zhu. Online continual learn-
ing for visual food classiﬁcation. In Proceedings of the
IEEE/CVF International Conference on Computer Vision ,
pages 2337–2346, 2021. 2
[18] Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and
Dahua Lin. Learning a uniﬁed classiﬁer incrementally via
rebalancing. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition , pages 831–839,
2019. 3
[19] Xisen Jin, Arka Sadhu, Junyi Du, and Xiang Ren. Gradient-
based editing of memory examples for online task-free con-
tinual learning. Advances in Neural Information Processing
Systems , 34, 2021. 2, 5
[20] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel
Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
Barwinska, et al. Overcoming catastrophic forgetting in neu-
ral networks. Proceedings of the national academy of sci-
ences , 114(13):3521–3526, 2017. 2
[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 5
[22] Huiwei Lin, Shanshan Feng, Xutao Li, Wentao Li, and Yun-
ming Ye. Anchor assisted experience replay for online class-
incremental learning. IEEE Transactions on Circuits and
Systems for Video Technology , 2022. 2
[23] Yaoyao Liu, Bernt Schiele, and Qianru Sun. Rmm: Re-
inforced memory management for class-incremental learn-
ing. Advances in Neural Information Processing Systems ,
34, 2021. 2
[24] David Lopez-Paz and Marc’Aurelio Ranzato. Gradient
episodic memory for continual learning. Advances in neu-
ral information processing systems , 30, 2017. 2
[25] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyun-
woo Kim, and Scott Sanner. Online continual learning in
image classiﬁcation: An empirical survey. Neurocomputing ,
469:28–51, 2022. 1
[26] Zheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner.
Supervised contrastive replay: Revisiting the nearest class

--- PAGE 10 ---
mean classiﬁer in online class-incremental continual learn-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 3589–3599,
2021. 2, 4, 5
[27] Thomas Mensink, Jakob Verbeek, Florent Perronnin, and
Gabriela Csurka. Distance-based image classiﬁcation: Gen-
eralizing to new classes at near-zero cost. IEEE transactions
on pattern analysis and machine intelligence , 35(11):2624–
2637, 2013. 2
[28] Zichen Miao, Ze Wang, Wei Chen, and Qiang Qiu. Con-
tinual learning with ﬁlter atom swapping. In International
Conference on Learning Representations , 2021. 2
[29] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units
improve restricted boltzmann machines. In Icml, 2010. 3
[30] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil-
licrap, and Gregory Wayne. Experience replay for continual
learning. Advances in Neural Information Processing Sys-
tems, 32, 2019. 2, 3, 5
[31] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins,
Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, Raz-
van Pascanu, and Raia Hadsell. Progressive neural networks.
arXiv preprint arXiv:1606.04671 , 2016. 2
[32] Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott San-
ner, Hyunwoo Kim, and Jongseong Jang. Online class-
incremental continual learning with adversarial shapley
value. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence , volume 35, pages 9630–9638, 2021. 2, 5, 6
[33] Shixiang Tang, Dapeng Chen, Jinguo Zhu, Shijie Yu, and
Wanli Ouyang. Layerwise optimization by gradient de-
composition for continual learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition , pages 9634–9643, 2021. 2
[34] Gido M van de Ven, Hava T Siegelmann, and Andreas S To-
lias. Brain-inspired replay for continual learning with arti-
ﬁcial neural networks. Nature communications , 11(1):1–14,
2020. 2
[35] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan
Wierstra, et al. Matching networks for one shot learning. Ad-
vances in neural information processing systems , 29:3630–
3638, 2016. 5
[36] Liyuan Wang, Xingxing Zhang, Kuo Yang, Longhui Yu,
Chongxuan Li, HONG Lanqing, Shifeng Zhang, Zhenguo
Li, Yi Zhong, and Jun Zhu. Memory replay with data com-
pression for continual learning. In International Conference
on Learning Representations , 2021. 2
[37] Ye Xiang, Ying Fu, Pan Ji, and Hua Huang. Incremental
learning using conditional adversarial networks. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision , pages 6619–6628, 2019. 2
[38] Xufeng Yao, Yang Bai, Xinyun Zhang, Yuechen Zhang, Qi
Sun, Ran Chen, Ruiyu Li, and Bei Yu. Pcl: Proxy-based
contrastive learning for domain generalization. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 7097–7107, 2022. 2, 4, 8
[39] Haiyan Yin, Ping Li, et al. Mitigating forgetting in online
continual learning with neuron calibration. Advances in Neu-
ral Information Processing Systems , 34, 2021. 2[40] Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, and Shu-
Tao Xia. Maintaining discrimination and fairness in class
incremental learning. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition , pages
13208–13217, 2020. 5
[41] Fei Zhu, Zhen Cheng, Xu-yao Zhang, and Cheng-lin Liu.
Class-incremental learning via dual augmentation. Advances
in Neural Information Processing Systems , 34, 2021. 2
