# 2204.03293.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/contrastive/2204.03293.pdf
# Kích thước tệp: 1058587 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CoCoSoDa: Học Tương Phản Hiệu Quả cho
Tìm Kiếm Mã Nguồn
Ensheng Shia,†Yanlin Wangb,§Wenchao Guc,†Lun Dud
Hongyu ZhangeShi HandDongmei ZhangdHongbin Suna,§
aĐại học Giao thông Tây ĂnbTrường Kỹ thuật Phần mềm, Đại học Trung Sơn
cĐại học Trung Văn Hồng KôngdMicrosoft ResearcheĐại học Trùng Khánh
s1530129650@stu.xjtu.edu.cn, wangylin36@mail.sysu.edu.cn
wcgu@cse.cuhk.edu.hk, flun.du, shihan, dongmeiz g@microsoft.com
hyzhang@cqu.edu.cn, hsun@mail.xjtu.edu.cn
Tóm tắt —Tìm kiếm mã nguồn nhằm truy xuất các đoạn mã
có ý nghĩa ngữ nghĩa phù hợp cho một truy vấn ngôn ngữ tự nhiên
cho trước. Gần đây, nhiều phương pháp sử dụng học tương phản
đã cho thấy kết quả khả quan trong việc học biểu diễn mã nguồn
và cải thiện đáng kể hiệu suất tìm kiếm mã nguồn. Tuy nhiên,
vẫn còn rất nhiều dư địa để cải thiện trong việc sử dụng học
tương phản cho tìm kiếm mã nguồn. Trong bài báo này, chúng
tôi đề xuất CoCoSoDa để sử dụng hiệu quả học tương phản
cho tìm kiếm mã nguồn thông qua hai yếu tố chính trong học
tương phản: tăng cường dữ liệu và mẫu âm. Cụ thể, tăng cường
dữ liệu mềm là việc che đi hoặc thay thế động một số token
bằng kiểu của chúng cho các chuỗi đầu vào để tạo ra mẫu dương.
Cơ chế momentum được sử dụng để tạo ra biểu diễn lớn và nhất
quán của các mẫu âm trong một mini-batch thông qua việc duy
trì một hàng đợi và một bộ mã hóa momentum. Ngoài ra, học
tương phản đa phương thức được sử dụng để kéo gần biểu diễn
của các cặp mã-truy vấn và đẩy xa các đoạn mã và truy vấn
không được ghép cặp. Chúng tôi tiến hành các thí nghiệm mở
rộng để đánh giá hiệu quả của phương pháp trên một tập dữ
liệu quy mô lớn với sáu ngôn ngữ lập trình. Kết quả thí nghiệm
cho thấy: (1) CoCoSoDa vượt trội hơn 18 phương pháp cơ sở
và đặc biệt vượt qua CodeBERT, GraphCodeBERT, và UniXcoder
lần lượt 13.3%, 10.5%, và 5.9% về điểm MRR trung bình. (2)
Các nghiên cứu khử bỏ cho thấy hiệu quả của từng thành phần
trong phương pháp của chúng tôi. (3) Chúng tôi áp dụng các
kỹ thuật của mình cho một số mô hình tiền huấn luyện khác
nhau như RoBERTa, CodeBERT, và GraphCodeBERT và quan
sát được sự cải thiện đáng kể trong hiệu suất tìm kiếm mã nguồn
của chúng. (4) Mô hình của chúng tôi hoạt động ổn định dưới
các siêu tham số khác nhau. Hơn nữa, chúng tôi thực hiện các
phân tích định tính và định lượng để khám phá lý do đằng sau
hiệu suất tốt của mô hình.
Thuật ngữ chỉ mục —tìm kiếm mã nguồn, học tương phản,
tăng cường dữ liệu mềm, cơ chế momentum

I. GIỚI THIỆU
Tìm kiếm mã nguồn đóng vai trò quan trọng trong phát
triển và bảo trì phần mềm [1], [2]. Để triển khai một chức
năng nhất định, các nhà phát triển thường tìm kiếm và tái sử
dụng mã nguồn đã được viết trước đó từ các kho mã nguồn
mở như GitHub hoặc từ một cơ sở mã địa phương lớn [3]–[5].
Thách thức chính trong nhiệm vụ này là tìm các đoạn mã có
ý nghĩa ngữ nghĩa phù hợp được viết bằng ngôn ngữ lập trình
dựa trên các truy vấn đầu vào được viết bằng ngôn ngữ tự
nhiên [6].

§Yanlin Wang và Hongbin Sun là các tác giả liên lạc.
†Công việc được thực hiện trong thời gian tác giả làm việc tại Microsoft Research Asia

Các nghiên cứu đầu [3], [7]–[9] về tìm kiếm mã nguồn chủ
yếu dựa vào thông tin từ vựng của các đoạn mã và sử dụng
các phương pháp truy xuất thông tin (IR). Sau đó, các phương
pháp dựa trên học sâu [10]–[21] sử dụng mạng nơ-ron để
nhúng mã nguồn và truy vấn vào một không gian nhúng chung
và đo độ tương tự ngữ nghĩa của chúng thông qua khoảng
cách vector được khám phá. Gần đây, các mô hình mã nguồn
tiền huấn luyện lớn [22]–[26], được tiền huấn luyện trên dữ
liệu đa ngôn ngữ lập trình lớn, cải thiện việc hiểu ngữ nghĩa
mã nguồn và đạt được hiệu suất tìm kiếm mã nguồn tốt hơn.
Một số nghiên cứu [27], [28] áp dụng học tương phản vào việc
học biểu diễn mã nguồn không giám sát và Corder [28] cũng
đạt được hiệu suất tốt trong tìm kiếm mã nguồn. Cụ thể, họ
trước tiên sử dụng các phép biến đổi bảo toàn ngữ nghĩa [28]
để tạo ra các đoạn mã tương tự như mẫu dương. Các phép
biến đổi này bao gồm Hoán vị Câu lệnh [28] (hoán đổi hai
câu lệnh không có phụ thuộc dữ liệu lẫn nhau trong một khối
cơ bản), Trao đổi Vòng lặp [28] (thay thế câu lệnh for bằng
câu lệnh while hoặc ngược lại), v.v. Thứ hai, họ coi các đoạn
mã khác trong cùng mini-batch là mẫu âm và sau đó tối ưu
hóa mô hình để kéo gần biểu diễn của các mẫu dương và đẩy
xa biểu diễn của các mẫu âm.

Các phương pháp này đã cho thấy kết quả khả quan trong
tìm kiếm mã nguồn. Ví dụ, Corder [28] vượt trội tất cả các
phương pháp và đạt điểm MRR 0.727 trên tập dữ liệu Java
của CodeSearchNet [29]. Tuy nhiên, vẫn còn rất nhiều dư
địa để cải thiện trong việc tận dụng học tương phản cho tìm
kiếm mã nguồn, chẳng hạn như sử dụng tăng cường dữ liệu
hiệu quả khác để tạo ra mẫu dương hoặc làm phong phú mẫu
âm.

Trong bài báo này, chúng tôi đề xuất CoCoSoDa (viết tắt
của tìm kiếm mã nguồn với học tương phản momentum đa
phương thức và tăng cường dữ liệu mềm) để sử dụng hiệu
quả học tương phản cho tìm kiếm mã nguồn thông qua hai
yếu tố chính trong học tương phản: tăng cường dữ liệu và
mẫu âm.¹ Để học biểu diễn ngữ nghĩa tổng thể tốt hơn của
đoạn mã và truy vấn thay vì tập trung vào việc học biểu diễn
ngữ nghĩa cấp token theo ngữ cảnh xung quanh, chúng tôi đề
xuất bốn phương pháp SoDa (viết tắt của tăng cường dữ liệu
mềm) (Phần III-C). Chúng thay thế động r (r ổn định từ 5%
đến 20%) token mã nguồn bằng kiểu của chúng hoặc đơn
giản che đi chúng để tạo ra các đoạn mã tương tự.² Để phân
biệt một mẫu với nhiều mẫu âm hơn tại mỗi vòng lặp, chúng
tôi áp dụng cơ chế momentum [30] để mở rộng mẫu âm trong
một mini-batch.³ Chúng tôi cũng sử dụng học tương phản đa
phương thức để giảm thiểu khoảng cách giữa biểu diễn của
cặp mã-truy vấn và tối đa hóa khoảng cách giữa biểu diễn
của truy vấn (đoạn mã) và nhiều đoạn mã (truy vấn) không
được ghép cặp khác. Khung tổng thể của CoCoSoDa được
thể hiện trong Hình 1. Bên trái là một ví dụ về tăng cường
dữ liệu mềm, và bên phải là kiến trúc chính của mô hình.
Chi tiết hơn được đưa ra trong Phần III.

--- TRANG 2 ---
Mã nguồn gốc c:
def save_file(dataframe , filename ):
    df = dataframe
    df.to_csv( filename  ,sep=',',
                      encoding='utf -8',  index=False)
Mã nguồn tăng cường c*:
def save_file(dataframe , filename ):
    df = dataframe
    df.to_csv( <Identifier>  ,sep=',',    
                      encoding= <String> , index=False)

Thay thế Token bằng Kiểu

𝜙q𝑒 𝜙𝑚q𝑒 𝜃𝑚𝑐𝑒 𝜃𝑐𝑒

Mini-batch Code
(q1, c1), (q 2, c2),…,(q1000, c1000),…

q1, q2

q1 ,  q2 

q1,* q 2*

Query 
Encoder

Momentum
Query 
Encoder

q3*,…, q4096*

Queue

Vq1*
Vq2*

Mini-batch

c1, c2

c1, c2 

c1*, c 2*

Code 
Encoder

Momentum
Code 
Encoder

c3*,…,c4096*

MultiModal Contrastive Learning Loss

Mini-batch

Soft data augmentation 

Soft data augmentation

...

Vq 4096*

Vq1
Vq2

Vc1*
Vc2*

...

Vc4096*

Vc1
Vc2

Soft data augmentatio n 

Truy vấn tăng cường q*:
How to  <MASK>  to CSV Files in Python

Truy vấn gốc q:
How to Write to CSV Files in Python

Dynamic Masking

Inter -modal loss 

Intra -modal  loss

Hình 1. Khung làm việc của CoCoSoDa.

Chúng tôi đánh giá hiệu quả của CoCoSoDa trên tập dữ
liệu quy mô lớn CodeSearchNet [29] với sáu ngôn ngữ lập
trình (Ruby, JavaScript, Go, Python, Java, PHP) và so sánh
CoCoSoDa với 18 phương pháp hiện đại (SOTA). Chúng tôi
cũng tiến hành nghiên cứu khử bỏ để nghiên cứu hiệu quả
của từng thành phần của CoCoSoDa. Hơn nữa, chúng tôi áp
dụng CoCoSoDa cho ba mô hình tiền huấn luyện quy mô lớn
khác, bao gồm mô hình tiền huấn luyện ngôn ngữ tự nhiên
RoBERTa [31], các mô hình tiền huấn luyện mã nguồn Code-
BERT [23] và GraphCodeBERT [22]. Chúng tôi cũng gán các
siêu tham số khác nhau để kiểm tra tác động của chúng đối
với tìm kiếm mã nguồn. Ngoài ra, chúng tôi thảo luận tại sao
CoCoSoDa hoạt động tốt thông qua các phân tích định tính
và định lượng. Kết quả thí nghiệm cho thấy: (1) CoCoSoDa
vượt trội đáng kể các phương pháp SOTA hiện có trong nhiệm
vụ tìm kiếm mã nguồn (Phần V-A). (2) Học tương phản momentum đa phương thức bao gồm học tương phản nội bộ và
liên phương thức cùng bốn phương pháp SoDa đóng vai trò
quan trọng riêng lẻ và có thể cải thiện hiệu suất của mô hình
tìm kiếm mã nguồn (Phần V-B). (3) CoCoSoDa có thể dễ
dàng được áp dụng cho các mô hình tiền huấn luyện khác và
rõ ràng tăng cường hiệu suất của chúng (Phần V-C). (4) CoCoSoDa hoạt động ổn định trên một phạm vi siêu tham số: tốc
độ học từ 5e-6 đến 7e-5, hệ số momentum m từ 0.910 đến
0.999, tỷ lệ che r từ 5% đến 20%, và siêu tham số nhiệt độ
thay đổi từ 0.03 đến 0.07 (Phần V-D).

Chúng tôi tóm tắt các đóng góp của bài báo này như sau:

• Chúng tôi áp dụng thuật toán học tương phản momentum
dựa trên Transform để tận dụng tốt hơn các kỹ thuật học
tương phản cho nhiệm vụ tìm kiếm mã nguồn. Nó cho
phép mô hình học biểu diễn mã nguồn hiệu quả bằng
cách học biểu diễn tốt hơn của một mẫu so với nhiều
mẫu âm hơn. Chúng tôi cũng đề xuất một phương pháp
mới kết hợp học tương phản momentum đa phương thức.
Nó có thể kéo gần biểu diễn của các cặp mã-truy vấn
phù hợp và đẩy xa biểu diễn của các cặp mã-truy vấn
không phù hợp.

• Chúng tôi đề xuất bốn phương pháp tăng cường dữ liệu
mềm đơn giản nhưng hiệu quả sử dụng che đi và thay
thế động cho tăng cường dữ liệu. Quan trọng hơn, SoDa
có thể dễ dàng được áp dụng cho tất cả các ngôn ngữ
lập trình.

• Chúng tôi tiến hành các thí nghiệm mở rộng để đánh giá
tính ưu việt của phương pháp trên tập dữ liệu đa ngôn
ngữ lập trình quy mô lớn. Kết quả cho thấy phương pháp
của chúng tôi vượt trội đáng kể các phương pháp cơ sở,
khung làm việc của chúng tôi có thể dễ dàng được áp
dụng cho các mô hình tiền huấn luyện khác và tăng cường
đáng kể hiệu suất của chúng, và CoCoSoDa hoạt động
ổn định trên một phạm vi siêu tham số.

--- TRANG 3 ---
II. CÔNG TRÌNH LIÊN QUAN

A. Tìm kiếm Mã nguồn

Học biểu diễn mã nguồn là một chủ đề mới nổi và đã được
tìm thấy hữu ích trong nhiều nhiệm vụ kỹ thuật phần mềm,
chẳng hạn như tóm tắt mã nguồn [32]–[36], tìm kiếm mã
nguồn [10], [15], [18], [37], [38], hoàn thành mã nguồn [39]–
[43], tạo thông điệp commit [44]–[48]. Trong số đó, tìm kiếm
mã nguồn đóng vai trò quan trọng trong phát triển và bảo trì
phần mềm [1], [2]. Các phương pháp truyền thống [3], [7]–[9]
dựa trên kỹ thuật truy xuất thông tin chủ yếu tập trung vào
thông tin từ vựng của mã nguồn và áp dụng các phương pháp
khớp từ khóa để tìm kiếm các đoạn mã có liên quan cho truy
vấn đã cho. Trong những năm gần đây, các phương pháp dựa
trên học sâu tận dụng mạng nơ-ron để học biểu diễn ngữ
nghĩa của mã nguồn và ngôn ngữ tự nhiên nhằm cải thiện
việc hiểu các đoạn mã và truy vấn. Gu et al. [10] là những
người đầu tiên sử dụng mạng nơ-ron sâu để nhúng mã nguồn
và truy vấn vào một không gian vector chung và đo độ tương
tự của chúng bằng khoảng cách vector. Tiếp theo, nhiều loại
cấu trúc mô hình khác nhau được áp dụng cho tìm kiếm mã
nguồn, bao gồm các mô hình tuần tự [16]–[20], mạng nơ-ron
tích chập [14], [15], [21], mạng nơ-ron cây [20], mô hình đồ
thị [13], [20], và transformer [11], [15]. Gần đây, các mô hình
tiền huấn luyện mã nguồn quy mô lớn [22]–[26], [49], được
tiền huấn luyện trên tập dữ liệu mã nguồn khổng lồ, cải thiện
việc hiểu ngữ nghĩa mã nguồn và đạt được những cải tiến
đáng kể trong nhiệm vụ tìm kiếm mã nguồn. Ví dụ, CodeBERT
được tiền huấn luyện với mô hình ngôn ngữ che đi (MLM),
nhằm dự đoán các token bị che, và phát hiện token được thay
thế (RTD), sử dụng một bộ phân biệt để xác định các token
được thay thế. GraphCodeBERT lấy mã nguồn, tóm tắt được
ghép cặp và luồng dữ liệu tương ứng làm đầu vào và được
tiền huấn luyện với MLM, dự đoán cạnh luồng dữ liệu, và
các nhiệm vụ căn chỉnh nút. PLBART [26] là một mô hình
tiền huấn luyện mã nguồn từ chuỗi đến chuỗi và được tiền
huấn luyện với mã hóa tự động khử nhiễu, phá hủy một khoảng
token và sau đó khôi phục chúng. Phương pháp của chúng
tôi có thể dễ dàng được áp dụng cho các mô hình tiền huấn
luyện này và tăng cường hiệu suất của chúng.

B. Học Biểu diễn Mã nguồn với Học Tương phản

Các phương pháp học tương phản [50], kéo gần các biểu
diễn tương tự và đẩy xa các biểu diễn khác nhau, đã được
sử dụng thành công trong việc học biểu diễn tự giám sát trên
hình ảnh [30], [51] và văn bản ngôn ngữ tự nhiên [52]–[54].
Để tạo ra các tăng cường riêng lẻ, hình ảnh thường sử dụng
phép biến đổi không gian [55], [56] và diện mạo [57], [58],
và văn bản ngôn ngữ tự nhiên chủ yếu sử dụng phương pháp
dịch ngược [53] và kỹ thuật khoảng [54]. Sau đó, một mô
hình được tiền huấn luyện để xác định liệu các mẫu tăng
cường có xuất phát từ cùng một mẫu gốc hay không.

Gần đây, một số nghiên cứu [24], [27], [28], [59], [60] cố
gắng sử dụng các phương pháp học tương phản trong việc
học biểu diễn mã nguồn không giám sát. Ví dụ, Jain et al.
[27] và Bui et al. [28] chủ yếu sử dụng các phép biến đổi
chương trình bảo toàn ngữ nghĩa để tạo ra các đoạn mã tương
đương về chức năng và tiền huấn luyện mô hình để nhận biết
các đoạn mã tương đương về ngữ nghĩa và không tương đương
thông qua các kỹ thuật học tương phản. Các phép biến đổi
này bao gồm đổi tên biến (đổi tên một biến bằng một token
ngẫu nhiên), câu lệnh không sử dụng (chèn một đoạn mã chết
như một câu lệnh khai báo không sử dụng), hoán vị câu lệnh
(hoán đổi hai câu lệnh không có phụ thuộc dữ liệu lẫn nhau),
v.v. Không giống như kỹ thuật tiền huấn luyện được đề cập
ở trên, mô hình của chúng tôi dựa trên học tương phản đa
phương thức với bộ mã hóa momentum, cho phép mô hình
học biểu diễn tốt dựa trên các mẫu trong mini-batch hiện tại
và các mini-batch trước đó. Hơn nữa, các tăng cường dữ liệu
trước đây yêu cầu bảo toàn ngữ nghĩa của mã nguồn, trong
khi chúng tôi sử dụng kỹ thuật che đi động đơn giản nhưng
hiệu quả cho phép tăng cường dữ liệu mềm linh hoạt hơn.

III. PHƯƠNG PHÁP ĐỀ XUẤT

Trong phần này, chúng tôi minh họa mô hình CoCoSoDa
cho tìm kiếm mã nguồn. Khung tổng thể của CoCoSoDa được
thể hiện trong Hình 1. Nó bao gồm các thành phần sau:

• Bộ mã hóa mã/truy vấn tiền huấn luyện nắm bắt thông
tin ngữ nghĩa của một đoạn mã hoặc một truy vấn ngôn
ngữ tự nhiên và ánh xạ nó vào một không gian nhúng
nhiều chiều.

• Bộ mã hóa mã/truy vấn momentum mã hóa các mẫu
(đoạn mã hoặc truy vấn) của các mini-batch hiện tại và
trước đó để làm phong phú các mẫu âm.

• Tăng cường dữ liệu mềm là việc che đi hoặc thay thế
động một số token trong một mẫu (mã/truy vấn) để tạo
ra một mẫu tương tự như một hình thức tăng cường dữ
liệu.

• Học tương phản đa phương thức được sử dụng làm mục
tiêu tối ưu hóa và bao gồm mất mát học tương phản
liên phương thức và nội phương thức. Chúng được sử
dụng để giảm thiểu khoảng cách của biểu diễn của các
mẫu tương tự và tối đa hóa khoảng cách của các mẫu
khác nhau trong không gian nhúng.

Trước tiên chúng tôi minh họa mô hình với một ví dụ cụ
thể được thể hiện trong Hình 1 và sau đó giới thiệu chi tiết
từng thành phần.

A. Một Ví dụ Minh họa

Trong phần này, chúng tôi giới thiệu mô hình thông qua
một ví dụ minh họa được thể hiện trong Hình 1. Phía bên trái
là một ví dụ về tăng cường dữ liệu mềm được thực hiện trên
cặp đoạn mã và truy vấn, và phía bên phải là kiến trúc chính
của mô hình. Cụ thể, đầu tiên, tại mỗi vòng lặp, chúng tôi
ngẫu nhiên thực hiện một trong bốn phương pháp SoDa bao
gồm che đi động (DM), thay thế động (DR), thay thế động
của loại được chỉ định (DRST), và che đi động của loại được
chỉ định (DMST), để tạo ra các mẫu dương. DM và DR ngẫu
nhiên lấy mẫu 15% token của một đoạn mã và thay thế mỗi
token bằng một token [MASK] hoặc loại của token đó. DRST
và DMST lấy mẫu tất cả các token của loại được chỉ định
(như toán tử, định danh) từ một đoạn mã, và 15% trong số
chúng được thay thế ngẫu nhiên bằng loại được chỉ định hoặc
token [MASK]. Đối với truy vấn, chỉ thực hiện DM vì ba
phương pháp SoDa khác yêu cầu thông tin loại của mã nguồn.

Thứ hai, chúng tôi áp dụng khung làm việc của học tương
phản momentum (MoCo) [30], và áp dụng bộ mã hóa Transformer đa lớp [61] làm xương sống của bộ mã hóa (momentum)
vì Transformer có thể mô hình hóa và biểu diễn mã nguồn
hiệu quả [22], [23]. Chúng tôi mở rộng thêm MoCo cho việc
học biểu diễn đa phương thức bằng cách nhân đôi bộ mã hóa
và bộ mã hóa momentum. Tiếp theo, chúng tôi sử dụng mô
hình tiền huấn luyện mã nguồn quy mô lớn UniXcoder [24]
để khởi tạo bộ mã hóa mã/truy vấn và bộ mã hóa momentum
mã/truy vấn (Hình 1), và sau đó đưa các mẫu gốc và được
tăng cường (mã hoặc truy vấn) vào các bộ mã hóa và bộ mã
hóa momentum, tương ứng, để thu được biểu diễn của các
mẫu. Bộ mã hóa momentum có thể tạo ra biểu diễn lớn và
nhất quán của các mẫu âm so với bộ mã hóa thông thường
được cập nhật bằng lan truyền ngược và bộ mã hóa cố định
[30]. Do đó, tại mỗi vòng lặp, CoCoSoDa có thể được huấn
luyện để phân biệt một mẫu với nhiều mẫu âm khác hơn, vì
vậy phương pháp của chúng tôi có thể học biểu diễn tốt hơn
của các đoạn mã và truy vấn.

Thứ ba, học tương phản đa phương thức bao gồm mất mát
nội phương thức và mất mát liên phương thức được sử dụng
để kéo gần các biểu diễn tương tự và đẩy xa các biểu diễn
không tương tự. Cụ thể, mất mát nội phương thức được sử
dụng để học phân phối đồng nhất của biểu diễn dữ liệu đơn
phương thức bằng cách kéo vào các mẫu tương tự (mã hoặc
truy vấn) và đẩy ra các mẫu khác nhau. Mất mát liên phương
thức được sử dụng để học sự căn chỉnh của dữ liệu đa phương
thức bằng cách kéo gần biểu diễn của cặp đoạn mã và truy
vấn được ghép cặp và đẩy xa biểu diễn của đoạn mã và truy
vấn không được ghép cặp.

Cuối cùng, mô hình được huấn luyện tốt được sử dụng cho
tìm kiếm mã nguồn. Chi tiết, các bộ mã hóa mã và truy vấn
ánh xạ các đoạn mã của cơ sở mã và truy vấn đã cho vào một
không gian nhiều chiều, đo độ tương tự giữa chúng bằng độ
tương tự cosin, và trả về đoạn mã liên quan nhất dựa trên độ
tương tự.

B. Bộ Mã hóa Tiền huấn luyện và Bộ Mã hóa Momentum

Trong phần này, chúng tôi giới thiệu kiến trúc cơ sở, mẫu
đầu vào, biểu diễn đầu ra và cơ chế cập nhật của bộ mã hóa
và bộ mã hóa momentum. Các bộ mã hóa và bộ mã hóa
momentum đều được xây dựng trên Bộ mã hóa Transformer
hai chiều đa lớp [61]. Vì các mô hình tiền huấn luyện như
UniXcoder [24] đã đạt được cải thiện đáng kể trong tìm kiếm
mã nguồn, chúng tôi khởi tạo bộ mã hóa mã và truy vấn với
các tham số của UniXcoder. Theo nghiên cứu trước đó [24],
chúng tôi lấy trung bình tất cả các trạng thái ẩn của lớp cuối
cùng làm biểu diễn cấp chuỗi toàn bộ của truy vấn/mã.

Trong khung làm việc MoCo [30], có một bộ mã hóa momentum mã hóa các mẫu của các mini-batch hiện tại và trước
đó. Cụ thể, bộ mã hóa momentum duy trì một hàng đợi bằng
cách đưa vào hàng đợi các mẫu trong mini-batch hiện tại và
đưa ra khỏi hàng đợi các mẫu trong mini-batch cũ nhất. Ở
đây, chúng tôi cũng sử dụng UniXcoder để khởi tạo các tham
số của bộ mã hóa momentum mã và truy vấn. Sự khác biệt
của cơ chế cập nhật giữa bộ mã hóa và bộ mã hóa momentum
là bộ mã hóa được cập nhật bằng thuật toán lan truyền ngược
trong khi bộ mã hóa momentum được cập nhật bằng nội suy
tuyến tính của bộ mã hóa và bộ mã hóa momentum. Do đó,
so với phương pháp ngân hàng bộ nhớ [62], cố định và lưu
biểu diễn của tất cả các mẫu của tập dữ liệu huấn luyện trước,
bộ mã hóa momentum có thể tạo ra biểu diễn nhất quán và
đã được chứng minh là hiệu quả [30].

Đối với phương pháp end-to-end [19], [22], nó có một bộ
mã hóa và lấy các mẫu khác trong mini-batch hiện tại làm
mẫu âm. Do đó, nó yêu cầu kích thước mini-batch lớn để
mở rộng số lượng mẫu âm [30]. Ví dụ, dưới cùng tài nguyên
tính toán như A100-PCIE-80GB [63], có thể sử dụng tối đa
199 mẫu âm trong một mini-batch cho các phương pháp end-
to-end, nhưng phương pháp của chúng tôi có thể sử dụng
hơn 4,000 mẫu âm. Do đó, để hỗ trợ nhiều mẫu âm hơn, các
phương pháp end-to-end yêu cầu tài nguyên tính toán bộ nhớ
lớn hơn so với phương pháp của chúng tôi.

Chúng tôi ký hiệu các tham số của bộ mã hóa mã là θce và
bộ mã hóa momentum mã là θmce, với các tham số là trọng
số của UniXcoder. Do đó, θmce được cập nhật bởi:

θmce = m × θmce + (1 - m) × θce  (1)

trong đó m ∈ [0;1) là hệ số momentum. Tương tự, chúng tôi
ký hiệu các tham số của bộ mã hóa truy vấn và bộ mã hóa
momentum truy vấn là φqe và φmqe. Khi đó φmqe được cập
nhật bởi:

φmqe = m × φmqe + (1 - m) × φqe  (2)

Cả θce và φqe đều là các tham số có thể học được và được
cập nhật bằng thuật toán lan truyền ngược.

C. Tăng cường Dữ liệu Mềm

Trong phần này, chúng tôi giới thiệu các phương pháp tăng
cường dữ liệu mềm (SoDa), là các phương pháp tăng cường
dữ liệu đơn giản không có ràng buộc bên ngoài cho mã nguồn
hoặc truy vấn. Chúng tôi trước tiên giới thiệu cách thu được
tăng cường dữ liệu mềm và sau đó giới thiệu cách sử dụng
dữ liệu được tăng cường.

Bốn phương pháp SoDa được thể hiện như sau:

• Che đi Động (DM): ngẫu nhiên lấy mẫu 15% token của
một đoạn mã và thay thế mỗi token bằng một token
[MASK].

• Thay thế Động (DR): ngẫu nhiên lấy mẫu 15% token
của một đoạn mã và thay thế mỗi token bằng loại của
token đó.

• Thay thế Động của Loại được Chỉ định (DRST): lấy
mẫu tất cả các token của một loại được chỉ định (như
toán tử, định danh) từ một đoạn mã, và 15% trong số
chúng được thay thế ngẫu nhiên bằng loại được chỉ định.

• Che đi Động của Loại được Chỉ định (DMST): lấy mẫu
tất cả các token của một loại được chỉ định (như toán
tử, định danh) từ một đoạn mã, và 15% trong số chúng
được che đi ngẫu nhiên.

Ở đây, động có nghĩa là trong xử lý dữ liệu, thao tác che
đi hoặc thay thế được thực hiện tại mỗi vòng lặp chứ không
chỉ được thực hiện một lần [31]. Đáng chú ý là chúng tôi
ngẫu nhiên thực hiện một trong bốn phương pháp SoDa cho
các đoạn mã tại mỗi vòng lặp. Ngoài ra, chỉ thực hiện DM
cho truy vấn vì ba phương pháp SoDa khác yêu cầu thông tin
loại của mã nguồn.

--- TRANG 4 ---
Chúng tôi ký hiệu mô-đun SoDa là G_soda thực hiện các
thao tác biến đổi dữ liệu cho chuỗi đầu vào đã cho để thu
được dữ liệu được tăng cường. Cụ thể, trước tiên chúng tôi
thực hiện một trong bốn phương pháp SoDa cho các đoạn
mã C = (c₁; :::; c_bs) và truy vấn Q = (q₁; :::; q_bs) trong
một mini-batch bằng:

c*ᵢ = G_soda(cᵢ); q*ᵢ = G_soda(qᵢ) (i = 1; :::; bs)  (3)

trong đó c*ᵢ và q*ᵢ là các mẫu được tăng cường của đoạn mã
cᵢ và truy vấn qᵢ, tương ứng. bs là kích thước mini-batch.
Sau đó đoạn mã cᵢ và truy vấn qᵢ được đưa vào bộ mã hóa
mã/truy vấn và các mẫu được tăng cường c*_k và q*_k (k =
1; :::; K và K là kích thước hàng đợi) trong các mini-batch
hiện tại và trước đó được đưa vào bộ mã hóa momentum
mã/truy vấn bằng:

v_cᵢ = f_ce(cᵢ); v_c*_k = f_mce(c*_k)
v_qᵢ = f_qe(qᵢ); v_q*_k = f_mqe(q*_k)  (4)

trong đó, v_cᵢ; v_qᵢ; v_c*_k, và v_q*_k là các biểu diễn tổng
thể cuối cùng của đoạn mã cᵢ, truy vấn qᵢ, đoạn mã được
tăng cường c*_k, và truy vấn được tăng cường q*_k, tương ứng.

D. Học Tương phản Đa phương thức

Học tương phản đa phương thức bao gồm hàm mất mát
liên phương thức và nội phương thức, và được sử dụng để
tối ưu hóa các tham số của mô hình. Cụ thể, cho một truy
vấn qᵢ, chúng tôi ký hiệu cᵢ được ghép cặp hoặc c*ᵢ là c⁺ᵢ
và c*_k không được ghép cặp là c⁻_k (i = 1; :::; bs và k =
1; :::; K). Đối với truy vấn qᵢ, với độ tương tự được đo bằng
độ tương tự cosin (sim(x;y) = xy/(||x|| ||y||)), chúng tôi định
nghĩa mất mát học tương phản liên phương thức và nội phương
thức [20], [64] như:

L^inter_qᵢ = -log[e^(sim(v_qᵢ;v⁺_cᵢ)/τ) / (e^(sim(v_qᵢ;v⁺_cᵢ)/τ) + Σ^K_{k=1} e^(sim(v_qᵢ;v⁻_c*_k)/τ))]

L^intra_qᵢ = -log[e^(sim(v_qᵢ;v⁺_q*ᵢ)/τ) / (e^(sim(v_qᵢ;v⁺_q*ᵢ)/τ) + Σ^K_{k=1} e^(sim(v_qᵢ;v⁻_q*_k)/τ))]  (5)

trong đó τ là siêu tham số nhiệt độ [30], [62] và được đặt
thành 0.07 theo các công trình trước đó [27], [30]. Một cách
trực quan, mục tiêu tối ưu hóa của hàm mất mát liên phương
thức là tối đa hóa độ tương tự ngữ nghĩa của truy vấn và
đoạn mã được ghép cặp với nó và giảm thiểu độ tương tự
ngữ nghĩa của truy vấn và các đoạn mã không được ghép
cặp với nó. Hàm mất mát nội phương thức là để học biểu
diễn tốt hơn của các truy vấn, trong đó các truy vấn tương
tự có biểu diễn gần nhau và các truy vấn khác nhau có biểu
diễn phân biệt. Tương tự, đối với một đoạn mã cᵢ, chúng tôi
định nghĩa mất mát học tương phản đa phương thức tương
ứng như:

L^inter_cᵢ = -log[e^(sim(v_cᵢ;v⁺_qᵢ)/τ) / (e^(sim(v_cᵢ;v⁺_qᵢ)/τ) + Σ^K_{k=1} e^(sim(v_cᵢ;v⁻_q*_k)/τ))]

L^intra_cᵢ = -log[e^(sim(v_cᵢ;v⁺_c*ᵢ)/τ) / (e^(sim(v_cᵢ;v⁺_c*ᵢ)/τ) + Σ^K_{k=1} e^(sim(v_cᵢ;v⁻_c*_k)/τ))]  (6)

trong đó q⁺ᵢ là truy vấn được ghép cặp của đoạn mã đầu vào
cᵢ, và q⁻_k ký hiệu truy vấn không được ghép cặp.

BẢNG I
THỐNG KÊ TẬP DỮ LIỆU.

Ngôn ngữ | Huấn luyện | Xác thực | Kiểm tra | Mã Ứng viên
Ruby | 24,927 | 1,400 | 1,261 | 4,360
JavaScript | 58,025 | 3,885 | 3,291 | 13,981
Java | 164,923 | 5,183 | 10,955 | 40,347
Go | 167,288 | 7,325 | 8,122 | 28,120
PHP | 241,241 | 12,982 | 14,014 | 52,660
Python | 251,820 | 13,914 | 14,918 | 43,827

Hàm mất mát liên phương thức và nội phương thức trong
một mini-batch có thể được thu được bằng:

L^inter = Σ^bs_{i=1}(L^inter_qᵢ + L^inter_cᵢ); L^intra = Σ^bs_{i=1}(L^intra_qᵢ + L^intra_cᵢ)  (7)

Cuối cùng, hàm mất mát học tương phản đa phương thức
tổng thể cho một mini-batch là:

L = Σ^bs_{i=1}(L^inter + L^intra)  (8)

Chúng tôi áp dụng AdamW [65] để tối ưu hóa toàn bộ mô
hình.

E. Tinh chỉnh cho Tìm kiếm Mã nguồn

Sau khi được tối ưu hóa bằng học tương phản đa phương
thức, mô hình có thể học biểu diễn tốt hơn của các mẫu, trong
đó các mẫu tương tự (mã hoặc truy vấn) có biểu diễn tương
tự và các mẫu khác nhau có biểu diễn khác nhau. Để cải
thiện thêm hiệu suất của mô hình trong tìm kiếm mã nguồn,
theo hầu hết các nghiên cứu trước đó [22]–[26], [49], [59],
chúng tôi tinh chỉnh nó trên tập dữ liệu huấn luyện liên quan
bằng:

L_f = Σ^bs_{i=1}[-log(e^(sim(v_cᵢ;v_qᵢ)/τ) / Σ^bs_{j=1} e^(sim(v_cᵢ;v_qⱼ)/τ))]  (9)

trong đó v_cᵢ và v_qᵢ là các biểu diễn ngữ nghĩa tổng thể của
đoạn mã cᵢ và truy vấn qᵢ, tương ứng. Chúng được thu được
bằng bộ mã hóa mã và truy vấn, tương ứng. τ là siêu tham
số nhiệt độ. Sau đó chúng tôi sử dụng tập dữ liệu xác thực
để chọn mô hình tốt nhất dựa trên giá trị MRR (chi tiết trong
Phần IV-D) và báo cáo điểm số trên tập kiểm tra trong bài
báo này.

IV. THIẾT KẾ THÍ NGHIỆM

A. Tập dữ liệu

Chúng tôi tiến hành thí nghiệm trên tập dữ liệu chuẩn quy
mô lớn CodeSearchNet [29] như được sử dụng trong Guo et
al. [22]. Nó chứa sáu ngôn ngữ lập trình, cụ thể là Ruby,
JavaScript, Go, Python, Java, và PHP. Tập dữ liệu này được
sử dụng rộng rãi trong các nghiên cứu trước đó [11], [22]–
[25], [29], [49], [66], [67]. Thống kê của tập dữ liệu được
thể hiện trong Bảng I. Theo các nghiên cứu trước đó [10],
[22], [68], mô hình là để truy xuất các đoạn mã chính xác từ
Mã Ứng viên (cột cuối cùng trong Bảng I) cho các truy vấn
đã cho khi thực hiện đánh giá.

--- TRANG 5 ---
B. Phương pháp Cơ sở

Để đánh giá hiệu quả của phương pháp, chúng tôi so sánh
CoCoSoDa với ba phương pháp dựa trên IR [69]–[71], bốn
phương pháp end-to-end sâu [29] và mười phương pháp dựa
trên tiền huấn luyện bao gồm ba mô hình liên quan đến học
tương phản.

Các phương pháp dựa trên IR bao gồm BOW [69], TF-IDF
[70] và Jaccard [71]. BOW và TF-IDF sử dụng kỹ thuật túi
từ và tần số thuật ngữ-tần số tài liệu nghịch đảo, tương ứng,
để trích xuất các đặc trưng từ các đoạn mã đầu vào và truy
vấn và chuyển đổi chúng thành vector. Sau đó, chúng đo độ
tương tự ngữ nghĩa giữa các đoạn mã và truy vấn bằng độ
tương tự cosin giữa các vector tương ứng của chúng. Jaccard
truy xuất đoạn mã tương tự cho truy vấn đã cho theo hệ số
độ tương tự Jaccard [71] giữa mã và truy vấn.

NBow, CNN, BiRNN và SelfAtnn [29] sử dụng các mô hình
mã hóa khác nhau như túi từ nơ-ron [72], mạng nơ-ron tích
chập 1D [73], GRU hai chiều [74], và attention đa đầu [61]
để thu được biểu diễn của các đoạn mã và truy vấn. Và chúng
đo độ tương tự ngữ nghĩa của biểu diễn bằng tích trong.

RoBERTa [31], RoBERTa (code) [23] được xây dựng trên
bộ mã hóa Transformer đa lớp [61] và được tiền huấn luyện
với MLM, nhằm dự đoán các token bị che. Các tập dữ liệu
tiền huấn luyện là corpus ngôn ngữ tự nhiên [31] và corpus
mã nguồn [29], tương ứng.

CodeBERT [23] và GraphCodeBERT [22] được tiền huấn
luyện trên corpus mã lớn. Cái trước được tiền huấn luyện với
MLM và RTD, sử dụng một bộ phân biệt để xác định các
token được thay thế. Cái sau xem xét thông tin cấu trúc mã
và được tiền huấn luyện với MLM, dự đoán cạnh luồng dữ
liệu, và căn chỉnh nút.

Corder [28] đầu tiên sử dụng phương pháp học tương phản
đơn phương thức (chỉ phương thức mã) để tiền huấn luyện
mô hình nhận biết các đoạn mã tương đương về ngữ nghĩa
và sau đó tinh chỉnh nó cho các nhiệm vụ downstream. Vì
Corder không phát hành việc triển khai các phép biến đổi
bảo toàn ngữ nghĩa và việc triển khai các phép biến đổi này
cho sáu ngôn ngữ lập trình là tốn kém, chúng tôi chỉ triển
khai cho ngôn ngữ Java vì Java là ngôn ngữ được nghiên
cứu nhiều nhất cho tìm kiếm mã nguồn [75]. Để có so sánh
công bằng, chúng tôi sử dụng kỹ thuật học tương phản đơn
phương thức để tiền huấn luyện liên tục UniXcoder và sau đó
tinh chỉnh nó cho nhiệm vụ tìm kiếm mã nguồn như việc triển
khai Corder.

ContraCode [27] cũng áp dụng học tương phản vào việc
học biểu diễn mã nguồn không giám sát và tiến hành thí
nghiệm về tóm tắt mã nguồn và suy luận kiểu. Cụ thể, họ
đầu tiên áp dụng một số phép biến đổi chương trình bảo toàn
ngữ nghĩa để tạo ra các đoạn mã tương đương chức năng.
Tiếp theo, họ tiền huấn luyện một mô hình mạng nơ-ron để
xác định các đoạn mã tương đương chức năng trong nhiều
yếu tố gây nhiễu. Cuối cùng, họ tinh chỉnh mô hình tiền huấn
luyện để thực hiện các nhiệm vụ downstream. Chúng tôi sử
dụng ContraCode được tiền huấn luyện làm bộ mã hóa mã/truy
vấn và tối ưu hóa nó bằng Phương trình 9 cho tìm kiếm mã
nguồn.

CodeT5 [25], PLBART [26], SPT-Code [49] là các mô hình
tiền huấn luyện mã từ chuỗi đến chuỗi. Cái đầu được tiền
huấn luyện với ba nhiệm vụ tiền huấn luyện nhận biết định
danh để cho phép mô hình xác định các định danh trong mã
nguồn hoặc khôi phục các định danh bị che. Cái thứ hai được
tiền huấn luyện với mã hóa tự động khử nhiễu, nhằm tái cấu
trúc chuỗi mã đầu vào bị hỏng. Cái thứ ba lấy mã nguồn,
AST tương ứng và tóm tắt được ghép cặp làm đầu vào và
được tiền huấn luyện với ba nhiệm vụ cụ thể về mã.

SyncoBERT [59] và UniXcoder [24] là tiền huấn luyện
tương phản đa phương thức cho biểu diễn mã. SyncoBERT
lấy mã nguồn, AST và tóm tắt làm đầu vào và được tiền huấn
luyện với dự đoán định danh và dự đoán cạnh AST để học
kiến thức từ vựng và cú pháp của mã nguồn. UniXcoder lấy
dữ liệu hai phương thức, tóm tắt và AST đơn giản hóa của
mã nguồn, làm đầu vào và được tiền huấn luyện với MLM,
mô hình ngôn ngữ một chiều, bộ tự động mã hóa khử nhiễu,
và hai nhiệm vụ liên quan đến học tương phản.

Chi tiết hơn về các phương pháp cơ sở có thể tìm thấy trong
Phụ lục của gói sao chép [76]. Trong các thí nghiệm của
chúng tôi, chúng tôi huấn luyện bốn phương pháp end-to-end
sâu từ đầu, và đối với mười phương pháp tiền huấn luyện,
chúng tôi khởi tạo chúng với các mô hình tiền huấn luyện và
tinh chỉnh (hoặc tiền huấn luyện liên tục và tinh chỉnh) chúng
theo mô tả trong các bài báo gốc hoặc mã nguồn đã phát
hành của chúng.

C. Cài đặt Thí nghiệm

Theo UniXcoder [24], chúng tôi sử dụng Transformer với
12 lớp, 768 trạng thái ẩn chiều, và 12 đầu attention. Kích
thước từ vựng của mã và truy vấn được đặt thành 51,451.
Độ dài chuỗi tối đa của đoạn mã và truy vấn lần lượt là 128
và 256. Đối với bộ tối ưu, chúng tôi sử dụng AdamW với
tốc độ học 2e-5. Theo các nghiên cứu trước đó [22], [23],
[68], bộ mã hóa mã và bộ mã hóa truy vấn chia sẻ tham số
để giảm tổng số tham số. Theo MoCo [30], siêu tham số nhiệt
độ được đặt là 0.07 và hệ số momentum m là 0.999. Kích
thước hàng đợi và kích thước batch lần lượt được đặt thành
4096 và 128. Bước huấn luyện của giai đoạn học tương phản
đa phương thức là 100K và số epoch tối đa của giai đoạn
tinh chỉnh là 5. Ngoài ra, chúng tôi chạy thí nghiệm 3 lần
với seed ngẫu nhiên 0,1,2 và hiển thị giá trị trung bình trong
bài báo. Tất cả thí nghiệm được tiến hành trên máy có bộ
nhớ chính 220 GB và GPU Tesla A100 80GB.

D. Thước đo Đánh giá

Chúng tôi đo hiệu suất của phương pháp bằng bốn thước
đo: xếp hạng tương hỗ trung bình (MRR) và top-k recall
(R@k, k=1,5,10), được sử dụng rộng rãi trong các nghiên
cứu trước đó [10], [11], [13]–[20], [20], [20]–[23], [29].
MRR là trung bình của xếp hạng tương hỗ của các đoạn mã
chính xác cho các truy vấn đã cho Q. R@k đo tỷ lệ phần
trăm của các truy vấn mà các đoạn mã được ghép cặp tồn
tại trong danh sách được xếp hạng top-k được trả về. Chúng
được tính như sau:

MRR = (1/|Q|) × Σ(i=1 to |Q|) (1/Rank_i); R@k = (1/|Q|) × Σ(i=1 to |Q|) I(Rank_i ≤ k)  (10)

--- TRANG 6 ---
BẢNG II
HIỆU SUẤT CỦA CÁC PHƯƠNG PHÁP KHÁC NHAU. JS LÀ VIẾT TẮT CỦA
JAVASCRIPT. Ý NGHĨA THỐNG KÊ CỦA CÁC THÍ NGHIỆM: p < 0.01.

Mô hình | Ruby | JS | Go | Python | Java | PHP | Trung bình
--- | --- | --- | --- | --- | --- | --- | ---
**Mô hình dựa trên IR**
BOW | 0.230 | 0.184 | 0.350 | 0.222 | 0.245 | 0.193 | 0.237
TF-IDF | 0.239 | 0.204 | 0.363 | 0.240 | 0.262 | 0.215 | 0.254
Jaccard | 0.220 | 0.191 | 0.345 | 0.243 | 0.235 | 0.182 | 0.236

**Mô hình end-to-end sâu**
NBow | 0.162 | 0.157 | 0.330 | 0.161 | 0.171 | 0.152 | 0.189
CNN | 0.276 | 0.224 | 0.680 | 0.242 | 0.263 | 0.260 | 0.324
BiRNN | 0.213 | 0.193 | 0.688 | 0.290 | 0.304 | 0.338 | 0.338
SelfAtt | 0.275 | 0.287 | 0.723 | 0.398 | 0.404 | 0.426 | 0.419

**Mô hình tiền huấn luyện**
RoBERTa | 0.587 | 0.523 | 0.855 | 0.590 | 0.605 | 0.561 | 0.620
RoBERTa (code) | 0.631 | 0.57 | 0.864 | 0.621 | 0.636 | 0.581 | 0.650
CodeBERT | 0.679 | 0.621 | 0.885 | 0.672 | 0.677 | 0.626 | 0.693
GraphCodeBERT | 0.703 | 0.644 | 0.897 | 0.692 | 0.691 | 0.649 | 0.713
Corder | - | - | - | - | 0.727 | - | -
ContraCode | - | 0.688 | - | - | - | - | -
PLBART | 0.675 | 0.616 | 0.887 | 0.663 | 0.663 | 0.611 | 0.685
CodeT5 | 0.719 | 0.655 | 0.888 | 0.698 | 0.686 | 0.645 | 0.715
SyncoBERT | 0.722 | 0.677 | 0.913 | 0.724 | 0.723 | 0.678 | 0.740
UniXcoder | 0.740 | 0.684 | 0.915 | 0.720 | 0.726 | 0.676 | 0.744
SPT-Code | 0.701 | 0.641 | 0.895 | 0.699 | 0.700 | 0.651 | 0.715

**CoCoSoDa của chúng tôi** | 0.818 | 0.764 | 0.921 | 0.757 | 0.763 | 0.703 | 0.788
Cải thiện | "10.54%" | "11.7%" | "0.66%" | "5.14%" | "5.10%" | "3.99%" | "5.91%"

trong đó Rank_i là thứ hạng của đoạn mã được ghép cặp liên
quan đến truy vấn thứ i. I là hàm chỉ báo trả về 1 nếu Rank_i
≤ k nếu không thì trả về 0.

V. KẾT QUẢ THÍ NGHIỆM

A. RQ1: Hiệu quả của CoCoSoDa là gì?

1) Kết quả tổng thể: Chúng tôi đánh giá hiệu quả của mô
hình CoCoSoDa bằng cách so sánh nó với ba mô hình dựa
trên IR, bốn mô hình tìm kiếm mã end-to-end sâu gần đây
và mười mô hình tiền huấn luyện được giới thiệu trong Phần
IV-B trên tập dữ liệu CodeSearchNet với sáu ngôn ngữ lập
trình. Vì không có mã nguồn được phát hành cho Corder và
việc tái tạo phương pháp này cho sáu ngôn ngữ lập trình là
tốn kém, chúng tôi tái tạo Corder trên ngôn ngữ Java để so
sánh. Kết quả thí nghiệm được thể hiện trong Bảng II. Chúng
tôi chỉ trình bày kết quả dưới thước đo MRR do giới hạn
không gian. Chúng tôi đặt kết quả dưới các thước đo khác
trong Phụ lục của gói sao chép [76]. Các kết luận đúng với
MRR cũng đúng với các thước đo khác.

Chúng ta có thể thấy rằng các mô hình end-to-end sâu vượt
trội các mô hình dựa trên IR vì chúng có thể học mối quan
hệ ngữ nghĩa tốt hơn giữa mã và truy vấn [10], [14], [20].
Mười mô hình tiền huấn luyện (hàng thứ ba của Bảng II)
hoạt động tốt hơn bốn mô hình end-to-end sâu (hàng thứ hai
của Bảng II) được huấn luyện từ đầu, điều này cho thấy hiệu
quả của kỹ thuật tiền huấn luyện. Vì GraphCodeBERT xem
xét thông tin luồng dữ liệu của mã nguồn, nó hoạt động tốt
hơn RoBERTa, RoBERTa (code) và CodeBERT. UniXcoder
và SyncoBERT xem xét thông tin cấu trúc của mã nguồn và
hoạt động tốt hơn các phương pháp khác trung bình. Corder
và ContraCode hoạt động tốt nhất trong các phương pháp cơ
sở vì chúng tiền huấn luyện mô hình để nhận biết các đoạn
mã tương đương chức năng từ nhiều yếu tố gây nhiễu và có
thể học biểu diễn mã tốt hơn. CoCoSoDa lấy UniXcoder làm
bộ mã hóa mã/truy vấn cơ sở, tối ưu hóa liên tục nó với học tương phản đa phương thức và tăng cường dữ liệu mềm và
hoạt động tốt nhất trong tất cả các phương pháp.

2) Nghiên cứu trường hợp: Tiếp theo, chúng tôi cho thấy
một số trường hợp để chứng minh hiệu quả của mô hình
CoCoSoDa. Đối với mỗi trường hợp, chúng tôi chỉ cho thấy
kết quả của phương pháp và phương pháp cơ sở tốt nhất,
đó là GraphCodeBERT cho trường hợp thứ 1 và UniXcoder
cho trường hợp thứ 2.

Hình 2 cho thấy kết quả được trả về bởi CoCoSoDa và
GraphCodeBERT cho truy vấn "Transform a hexadecimal
String to a byte array." từ tập dữ liệu Java. Truy vấn bao
gồm hai đối tượng thao tác: "hexadecimal String" và "byte
array" và một hành động "Transform". Để triển khai chức
năng của truy vấn, chúng ta thường lấy "hexadecimal String"
làm tham số đầu vào và sử dụng "toXXX(...)" để thực hiện
hành động "Transform". Mô hình CoCoSoDa có thể hiểu
thành công ngữ nghĩa của toàn bộ truy vấn và đoạn mã và
trả về kết quả chính xác, trong khi GraphCodeBERT thì không.
Điều này là do biểu diễn mã thu được bởi GraphCodeBERT
bị ảnh hưởng bởi ngữ nghĩa cấp token như String và byte,
do đó, trả về đoạn mã có nhiều token tương tự với truy vấn
nhưng ngữ nghĩa hoàn toàn ngược lại: "Converts bytes to
hex string.".

Trong Hình 3, chúng tôi so sánh kết quả được trả về bởi
CoCoSoDa và UniXcoder cho truy vấn "Iterator to read the
rows of the CSV file." trong tập dữ liệu Python. CoCoSoDa
trả về đoạn mã chính xác, trong khi UniXcoder trả về đoạn
mã với ngữ nghĩa khác "Yield a generator over the rows.".
Mô hình của chúng tôi có thể hiểu chính xác ý định của truy
vấn và trả về đoạn mã liên quan, trước tiên đọc tệp CSV và
tạo ra một iterator để đọc các hàng của nó. UniXcoder trả
về một đoạn mã liên quan một phần chỉ tạo ra một iterator
để đọc các hàng của tệp, thiếu việc đọc tệp CSV.

**Tóm tắt.** Phương pháp của chúng tôi vượt trội đáng kể
các phương pháp cơ sở trên sáu ngôn ngữ lập trình về bốn
thước đo. Các nghiên cứu trường hợp tiếp tục chứng minh
những ưu điểm của CoCoSoDa trong tìm kiếm mã nguồn.

Listing 1. Kết quả top-1 được trả về bởi CoCoSoDa.
```java
public static byte[] hexStringToByte(String hexString) {
    try {
        return Hex.decodeHex(hexString.toCharArray());
    }
    catch (DecoderException e) {
        throw new UnexpectedException(e);
    }
}
```

Listing 2. Kết quả top-1 được trả về bởi GraphCodeBERT.
```java
public static String toHexString(final byte[] bytes) {
    char[] chars = new char[bytes.length * 2];
    int i = 0;
    for (byte b : bytes) {
        chars[i++] = CharUtil.int2hex((b & 0xF0) >> 4);
        chars[i++] = CharUtil.int2hex(b & 0x0F);
    }
    return new String(chars);
}
```

Hình 2. Mã top-1 được trả về bởi CoCoSoDa và GraphCodeBERT cho truy vấn "Transform a hexadecimal String to a byte array." trên ngôn ngữ Java.

Listing 3. Kết quả top-1 được trả về bởi CoCoSoDa.
```python
def get_items(self):
    reader = csv.reader(self.source)
    headers = reader.next()
    for row in reader:
        if not row:
            continue
        yield dict(zip(headers, row))
```

Listing 4. Kết quả top-1 được trả về bởi UniXcoder.
```python
def iterrows(lines_or_file, namedtuples=False, dicts=False,
            encoding='utf-8', **kw):
    if namedtuples and dicts:
        raise ValueError('either namedtuples or dicts can '
                        'be chosen as output format')
    elif namedtuples:
        _reader = NamedTupleReader
    elif dicts:
        _reader = UnicodeDictReader
    else:
        _reader = UnicodeReader
    with _reader(lines_or_file, encoding=encoding, **fix_kw
                (kw)) as r:
        for item in r:
            yield item
```

Hình 3. Mã top-1 được trả về bởi CoCoSoDa và UniXcoder cho truy vấn "Iterator to read the rows of the CSV file." trên ngôn ngữ Python.

--- TRANG 7 ---
B. RQ2: Các Thành phần Khác nhau Đóng góp Bao nhiêu?

Trong phần này, chúng tôi nghiên cứu đóng góp của từng
thành phần trong phương pháp CoCoSoDa. Nó bao gồm học
tương phản đa phương thức như mất mát nội phương thức và
mất mát liên phương thức và bốn phương pháp SoDa (DR,
DM, DRST và DMST) được giới thiệu trong Phần III-C. Cụ
thể, chúng tôi loại bỏ một thành phần (như DM) của CoCoSoDa mỗi lần và sau đó nghiên cứu hiệu suất của mô hình bị
khử bỏ. Kết quả thí nghiệm được thể hiện trong Bảng III.
"w/o một thành phần" có nghĩa là loại bỏ thành phần này. Ví
dụ, CoCoSoDa w/o DR và inter-modal loss có nghĩa là loại
bỏ phương pháp SoDa DR và hàm mất mát liên phương thức
L^intra (Phương trình 7), tương ứng.

BẢNG III
NGHIÊN CỨU KHỬ BỎ CỦA COCOSODA TRÊN MRR.

Mô hình | Ruby | JS | Go | Python | Java | PHP | Trung bình
--- | --- | --- | --- | --- | --- | --- | ---
CoCoSoDa | 0.818 | 0.764 | 0.921 | 0.757 | 0.763 | 0.703 | 0.788
w/o DR | 0.794 | 0.714 | 0.905 | 0.730 | 0.743 | 0.688 | 0.762
w/o DM | 0.801 | 0.754 | 0.906 | 0.744 | 0.756 | 0.69 | 0.775
w/o DRST | 0.797 | 0.714 | 0.905 | 0.730 | 0.743 | 0.685 | 0.762
w/o DMST | 0.788 | 0.715 | 0.902 | 0.731 | 0.746 | 0.686 | 0.761
w/o tất cả SoDas | 0.775 | 0.711 | 0.907 | 0.736 | 0.738 | 0.683 | 0.758
w/o inter-modal loss | 0.776 | 0.703 | 0.906 | 0.729 | 0.739 | 0.674 | 0.755
w/o intra-modal loss | 0.780 | 0.705 | 0.903 | 0.727 | 0.744 | 0.680 | 0.756

Từ Bảng III, chúng ta có thể thấy rằng hiệu suất của mô
hình giảm sau khi loại bỏ bất kỳ thành phần nào. Điều này
chứng minh rằng mỗi thành phần đóng vai trò quan trọng trong
mô hình tìm kiếm mã nguồn. Đặc biệt, hiệu suất của CoCoSoDa w/o tất cả SoDas, học tương phản nội phương thức và
liên phương thức giảm rõ ràng. Điều này là do tăng cường
dữ liệu có thể tăng tính đa dạng của dữ liệu. Học tương phản
liên phương thức, nhằm học sự căn chỉnh của các đoạn mã
và truy vấn, có thể kéo gần mã và truy vấn được ghép cặp
và đẩy xa mã và truy vấn không được ghép cặp. Học tương
phản nội phương thức nhằm học phân phối đồng nhất của
biểu diễn của các mẫu đơn phương thức (đoạn mã hoặc truy
vấn) và có thể cải thiện hiệu suất tổng quát hóa của mô hình
tìm kiếm mã nguồn.

**Tóm tắt.** Nghiên cứu khử bỏ cho thấy hiệu quả của học
tương phản đa phương thức bao gồm mất mát nội phương
thức và mất mát liên phương thức và bốn phương pháp SoDa
bao gồm DR, DM, DRST và DMST.

C. RQ3: Hiệu suất của Phương pháp trên Các Mô hình Tiền
huấn luyện Khác là gì?

Chúng tôi nghiên cứu thêm hiệu suất của phương pháp trên
ba mô hình tiền huấn luyện khác được giới thiệu trong Phần
IV-B, bao gồm một mô hình tiền huấn luyện ngôn ngữ tự
nhiên RoBERTa và hai mô hình tiền huấn luyện mã nguồn
CodeBERT và GraphCodeBERT. Cụ thể, chúng tôi sử dụng
các mô hình tiền huấn luyện này làm bộ mã hóa mã/truy vấn
và bộ mã hóa momentum mã/truy vấn trong Hình 1. Đối với
chuỗi đoạn mã và truy vấn đầu vào, chúng tôi lấy trung bình
các trạng thái ẩn của lớp cuối cùng làm biểu diễn tổng thể
của đoạn mã hoặc truy vấn. Độ tương tự của biểu diễn được
đo bằng độ tương tự cosin. Các cài đặt thí nghiệm khác tương
tự như trong Phần IV-C.

Kết quả được thể hiện trong Bảng IV. CoCoSoDa RoBERTa
có nghĩa là sử dụng RoBERTa làm bộ mã hóa mã/truy vấn
và bộ mã hóa momentum mã/truy vấn trong khung làm việc
của chúng tôi. Nhìn chung, chúng ta có thể thấy rằng CoCoSoDa RoBERTa, CoCoSoDa CodeBERT và CoCoSoDa GraphCodeBERT rõ ràng vượt trội RoBERTa, CodeBERT và GraphCodeBERT, tương ứng trên tất cả sáu ngôn ngữ lập trình về
bốn thước đo. Những kết quả này chứng minh rằng phương
pháp của chúng tôi có thể được tổng quát hóa cho các mô
hình tiền huấn luyện khác và tăng cường hiệu suất của chúng.
Bên cạnh đó, CoCoSoDa RoBERTa, được tiền huấn luyện trên
corpus ngôn ngữ tự nhiên và được tinh chỉnh với phương
pháp của chúng tôi, đạt được hiệu suất tương đương với CodeBERT trên tập dữ liệu Go. CoCoSoDa GraphCodeBERT đạt được
hiệu suất tương đương với UniXcoder trên JavaScript, Java
và Python. CoCoSoDa GraphCodeBERT cũng vượt trội nhẹ
UniXcoder trên tập dữ liệu Ruby.

**Tóm tắt.** Phương pháp của chúng tôi trực giao với kỹ
thuật tiền huấn luyện về cải thiện hiệu suất cho các nhiệm
vụ tìm kiếm mã nguồn và có thể rõ ràng tăng cường hiệu
suất của các mô hình tiền huấn luyện hiện có.

D. RQ4: Tác động của Các Siêu tham số Khác nhau là gì?

Trong phần này, chúng tôi nghiên cứu tác động của các siêu
tham số khác nhau: tốc độ học, hệ số momentum m, tỷ lệ che
r, và siêu tham số nhiệt độ τ. Chúng tôi nghiên cứu các siêu
tham số khác nhau trong phạm vi điển hình, bao gồm tất cả
các cài đặt thí nghiệm của các nghiên cứu trước đó [22]–[25],
[28], [31], [49], [59] và kết quả thí nghiệm được thể hiện
trong Hình 4. Từ kết quả của việc thay đổi tốc độ học (góc
trên bên trái của Hình 4), chúng ta có thể thấy rằng hiệu
suất thường ổn định đối với tốc độ học nhỏ [77] (từ 5e-6
đến 7e-5). Các tốc độ học lớn hơn 7e-5 có tác động rõ ràng
đến hiệu suất mô hình. Kết quả của hệ số momentum m khác
nhau được thể hiện ở góc trên bên phải của Hình 4. Chúng
ta có thể thấy rằng hiệu suất tăng khi hệ số momentum m
trở nên lớn hơn. Điều này là do hệ số momentum lớn có lợi
cho việc thu được biểu diễn nhất quán cho hàng đợi [30].
Hệ số momentum nhỏ hơn 0.910 có tác động đáng kể đến
hiệu suất. Những phát hiện này phù hợp với công trình trước
đó [30]. Từ kết quả của việc thay đổi tỷ lệ che r (góc dưới
bên trái của Hình 4), chúng ta có thể thấy rằng hiệu suất
không nhạy cảm với tỷ lệ che r khi tỷ lệ che r nằm giữa 5%
và 20%. Tỷ lệ che lớn hơn như 50% mang lại sự suy giảm
hiệu suất đáng kể. Điều này hợp lý vì tỷ lệ che lớn hơn làm
cho đoạn mã mất quá nhiều thông tin. Kết quả của siêu tham
số nhiệt độ τ khác nhau được thể hiện ở góc dưới bên phải
của Hình 4. Chúng ta có thể thấy rằng hiệu suất ổn định khi
siêu tham số nhiệt độ thay đổi từ 0.03 đến 0.07.

**Tóm tắt.** Nói chung, mô hình của chúng tôi ổn định trên
một phạm vi giá trị siêu tham số (tốc độ học từ 5e-6 đến
7e-5, hệ số momentum từ 0.910 đến 0.999, tỷ lệ che r từ
5% đến 20%, và siêu tham số nhiệt độ thay đổi từ 0.03 đến
0.07).

--- TRANG 8 ---
BẢNG IV
KẾT QUẢ TRÊN CÁC MÔ HÌNH TIỀN HUẤN LUYỆN KHÁC. COCOSODAGRAPH LÀ VIẾT TẮT CỦA COCOSODAGRAPHCODEBERT. TỶ LỆ PHẦN TRĂM CẢI THIỆN ĐƯỢC HIỂN THỊ TRONG NGOẶC ĐƠN.

PL | Thước đo | RoBERTa | CoCoSoDa RoBERTa | CodeBERT | CoCoSoDa CodeBERT | GraphCodeBERT | CoCoSoDa Graph
--- | --- | --- | --- | --- | --- | --- | ---
Ruby | MRR | 0.587 | 0.640 ("9.03%) | 0.679 | 0.723 ("6.48%) | 0.703 | 0.752 ("6.97%)
 | R@1 | 0.469 | 0.533 ("13.65%) | 0.583 | 0.618 ("6.00%) | 0.607 | 0.655 ("7.91%)
 | R@5 | 0.717 | 0.764 ("6.56%) | 0.800 | 0.852 ("6.50%) | 0.824 | 0.875 ("6.19%)
 | R@10 | 0.785 | 0.825 ("5.10%) | 0.853 | 0.904 ("5.98%) | 0.872 | 0.916 ("5.05%)
JavaScript | MRR | 0.523 | 0.559 ("6.88%) | 0.621 | 0.648 ("4.35%) | 0.644 | 0.682 ("5.90%)
 | R@1 | 0.413 | 0.460 ("11.38%) | 0.514 | 0.545 ("6.03%) | 0.538 | 0.582 ("8.18%)
 | R@5 | 0.652 | 0.673 ("3.22%) | 0.752 | 0.772 ("2.66%) | 0.774 | 0.806 ("4.13%)
 | R@10 | 0.730 | 0.744 ("1.92%) | 0.814 | 0.839 ("3.07%) | 0.834 | 0.866 ("3.84%)
Go | MRR | 0.855 | 0.881 ("3.04%) | 0.885 | 0.905 ("2.26%) | 0.897 | 0.907 ("1.11%)
 | R@1 | 0.800 | 0.829 ("3.62%) | 0.837 | 0.859 ("2.63%) | 0.858 | 0.861 ("0.35%)
 | R@5 | 0.926 | 0.945 ("2.05%) | 0.944 | 0.962 ("1.91%) | 0.954 | 0.962 ("0.84%)
 | R@10 | 0.949 | 0.965 ("1.69%) | 0.962 | 0.975 ("1.35%) | 0.972 | 0.978 ("0.62%)
Python | MRR | 0.590 | 0.629 ("6.61%) | 0.672 | 0.690 ("2.68%) | 0.692 | 0.714 ("3.18%)
 | R@1 | 0.480 | 0.523 ("8.96%) | 0.574 | 0.589 ("2.61%) | 0.594 | 0.614 ("3.37%)
 | R@5 | 0.727 | 0.756 ("3.99%) | 0.792 | 0.809 ("2.15%) | 0.813 | 0.834 ("2.58%)
 | R@10 | 0.793 | 0.818 ("3.15%) | 0.850 | 0.867 ("2.00%) | 0.866 | 0.888 ("2.54%)
Java | MRR | 0.605 | 0.635 ("4.96%) | 0.677 | 0.705 ("4.14%) | 0.691 | 0.721 ("4.34%)
 | R@1 | 0.499 | 0.531 ("6.41%) | 0.580 | 0.606 ("4.48%) | 0.592 | 0.624 ("5.41%)
 | R@5 | 0.737 | 0.762 ("3.39%) | 0.796 | 0.826 ("3.77%) | 0.817 | 0.843 ("3.18%)
 | R@10 | 0.796 | 0.820 ("3.02%) | 0.852 | 0.878 ("3.05%) | 0.865 | 0.890 ("2.89%)
PHP | MRR | 0.561 | 0.598 ("6.60%) | 0.626 | 0.647 ("3.35%) | 0.649 | 0.668 ("2.93%)
 | R@1 | 0.450 | 0.490 ("8.89%) | 0.520 | 0.538 ("3.46%) | 0.545 | 0.561 ("2.94%)
 | R@5 | 0.694 | 0.730 ("5.19%) | 0.753 | 0.779 ("3.45%) | 0.785 | 0.798 ("1.66%)
 | R@10 | 0.764 | 0.797 ("4.32%) | 0.814 | 0.844 ("3.69%) | 0.832 | 0.863 ("3.73%)

[Hình 4 biểu đồ showing tác động của các siêu tham số khác nhau]

E. RQ5: Tại sao Mô hình CoCoSoDa hoạt động?

Những ưu điểm của CoCoSoDa chủ yếu đến từ tăng cường
dữ liệu mềm và học tương phản momentum đa phương thức.
Tăng cường dữ liệu mềm biến đổi chuỗi đầu vào với cơ chế
che đi hoặc thay thế và tạo ra một mẫu tương tự như ví dụ
"dương". Nó có thể giúp mô hình học biểu diễn của đoạn mã
và truy vấn từ góc nhìn toàn cục (cấp chuỗi) thay vì đơn
giản tổng hợp ngữ nghĩa cấp token. Do đó, mô hình của chúng
tôi có xu hướng trả về các đoạn mã liên quan đến chức năng
cấp chuỗi thay vì độ tương tự cấp token. Cơ chế momentum
mở rộng các mẫu âm, cho phép phân biệt một mẫu với nhiều
mẫu âm hơn tại mỗi vòng lặp. Đối với học tương phản đa
phương thức, học tương phản liên phương thức có thể kéo
gần biểu diễn của cặp mã-truy vấn và đẩy xa biểu diễn của
truy vấn và nhiều đoạn mã không được ghép cặp, trong khi
học tương phản nội phương thức có thể học phân phối đồng
nhất của biểu diễn về dữ liệu đơn phương thức (đoạn mã
hoặc truy vấn). Do đó, CoCoSoDa có thể học biểu diễn tốt
hơn của mã và truy vấn và hoạt động tốt trong tìm kiếm mã
nguồn. Chúng tôi khám phá thêm tại sao CoCoSoDa hoạt
động thông qua phân tích định lượng và định tính.

1) Phân tích định lượng: Chúng tôi khám phá để hiểu các
lý do đằng sau hiệu suất tốt của phương pháp thông qua ℓalign
và ℓuniform [78], thường được sử dụng như các chỉ báo để
phản ánh chất lượng của biểu diễn được học bởi các kỹ thuật
học tương phản [52], [78]–[80]. Về mặt toán học, chúng được
định nghĩa như:

ℓalign = E(x,y)∼Dpaired[||f(x) − f(y)||²₂]
ℓuniform = log E(x,y)i.i.d∼D[e^(-2||f(x)−f(y)||²₂)]  (11)

trong đó (x,y) ∼ Dpaired có nghĩa là x và y là các mẫu được
ghép cặp, trong khi (x,y) i.i.d ∼ D có nghĩa là x và y được
phân phối độc lập đồng nhất. f(x) và f(y) là các biểu diễn
được học và ||f(x) − f(y)||²₂ biểu diễn chuẩn 2 của khoảng
cách giữa chúng. Từ Phương trình 11, chúng ta có thể biết
rằng ℓalign luôn là một số không âm, trong khi ℓuniform là
một số không dương. Về nhiệm vụ của chúng tôi, ℓalign phản
ánh mức độ căn chỉnh của biểu diễn của các cặp mã-truy vấn.
Khoảng cách gần hơn của các đoạn mã và truy vấn được ghép
cặp, giá trị ℓalign nhỏ hơn (gần với không). ℓuniform phản
ánh tính đồng nhất của phân phối biểu diễn của tất cả các
đoạn mã hoặc truy vấn. Phân phối càng đồng nhất, giá trị
ℓalign càng nhỏ (gần với âm vô cùng). Trong trường hợp cực
đoan khi tất cả biểu diễn của đoạn mã và truy vấn đều giống
nhau, cả giá trị ℓalign và ℓuniform đều bằng không. Nghĩa là,
biểu diễn được học có sự căn chỉnh hoàn hảo nhưng tính
đồng nhất cực kỳ kém. Trên thực tế, một mô hình có thể học
biểu diễn với cả sự căn chỉnh tốt hơn (ℓalign thấp hơn) và
tính đồng nhất (ℓuniform thấp hơn) thường có thể đạt được
hiệu suất tốt hơn [52], [78].

Chúng tôi hiển thị biểu đồ ℓalign-ℓuniform trong Hình 5.
Chúng tôi có thể thấy rằng (1) CodeBERT và GraphCodeBERT
được tiền huấn luyện có sự căn chỉnh tốt hơn nhưng tính
đồng nhất kém, và hoạt động không tốt. Điều này là do biểu
diễn được học bởi chúng rất tương tự và không thể tổng
quát hóa tốt. UniXcoder có tính đồng nhất tốt hơn và hoạt
động tốt hơn chúng. CoCoSoDa được tiền huấn luyện có cả
sự căn chỉnh và tính đồng nhất tốt hơn CodeBERT và GraphCodeBERT và hoạt động tốt nhất trong bốn mô hình được
tiền huấn luyện. (2) Sau khi được tinh chỉnh, UniXcoder,
CodeBERT và GraphCodeBERT đều có ℓuniform và ℓalign thấp
hơn và vượt trội đáng kể những cái được tiền huấn luyện
riêng lẻ. CoCoSoDa được tinh chỉnh thường bảo toàn sự căn
chỉnh và có tính đồng nhất tốt hơn. Do đó, CoCoSoDa được
tinh chỉnh cải thiện thêm hiệu suất của nó được tiền huấn
luyện.

2) Phân tích định tính: Chúng tôi cũng trực quan hóa các
biểu diễn được học để giúp hiểu trực quan tại sao CoCoSoDa
hoạt động tốt. Cụ thể, đầu tiên, chúng tôi ngẫu nhiên lấy mẫu
X (X=100, 200, 300, hoặc 400) cặp mã-truy vấn từ tập dữ
liệu Java với mười seed ngẫu nhiên khác nhau từ 0 đến 9.
Chúng tôi hiển thị kết quả với X=300 với seed ngẫu nhiên
là 3 trong bài báo, và các kết quả trực quan hóa khác được
đặt trong Phụ lục của gói sao chép [76] do giới hạn không
gian. Các phát hiện và kết luận sau đây đúng với X và seed
ngẫu nhiên khác nhau.

Thứ hai, chúng tôi đưa các cặp được lấy mẫu bao gồm đoạn
mã và truy vấn vào CoCoSoDa và UniXcoder được huấn
luyện tốt riêng lẻ và thu được biểu diễn của chúng. Thứ ba,
chúng tôi áp dụng T-SNE [81] để giảm chiều của biểu diễn
thu được thành 2D và trực quan hóa biểu diễn được giảm
chiều trong Hình 6. Chi tiết, Hình 6(a) và Hình 6(b) hiển
thị biểu diễn được học bởi UniXcoder và CoCoSoDa, tương
ứng. Đoạn mã và truy vấn được đánh dấu màu cam và xanh
lam, tương ứng. Khoảng cách giữa mã và truy vấn được ghép
cặp được chỉ ra bằng một đường màu xanh lá cây. Từ Hình
6, chúng ta có thể thấy rằng (1) có nhiều đường màu xanh
lá cây rất ngắn trong Hình 6 (a) và (b), có nghĩa là cả CoCoSoDa và UniXcoder đều có thể ánh xạ hầu hết các đoạn mã
và truy vấn được ghép cặp thành các nhúng gần nhau. (2)
Hình 6(a) có nhiều đường màu xanh lá cây dài hơn Hình
6(b). Nó chỉ ra rằng UniXcoder có nhiều trường hợp hơn
CoCoSoDa, trong đó biểu diễn theo cặp của truy vấn và đoạn
mã cách xa nhau. Trong tương lai, chúng tôi sẽ tiến hành
phân tích lỗi để nghiên cứu các mẫu được ghép cặp với các
đường màu xanh lá cây dài. Tóm lại, kết quả trực quan hóa
chỉ ra một cách trực quan rằng CoCoSoDa có thể học biểu
diễn tốt hơn UniXcoder.

**Tóm tắt.** Mô hình của chúng tôi có thể học phân phối
đồng nhất hơn của biểu diễn của dữ liệu đơn phương thức
(đoạn mã hoặc truy vấn), và các biểu diễn được học của các
đoạn mã và truy vấn được ghép cặp có thể được căn chỉnh
tốt.

[Hình 5: Biểu đồ ℓalign-ℓuniform của các mô hình khác nhau]
[Hình 6: Trực quan hóa T-SNE của biểu diễn]

--- TRANG 9 ---
VI. THẢO LUẬN

A. Hiệu suất của CoCoSoDa Mà không được Tinh chỉnh

Để nghiên cứu thêm hiệu quả của CoCoSoDa, chúng tôi
đánh giá các mô hình tiền huấn luyện khác nhau dưới cài
đặt thí nghiệm zero-shot, trong đó các mô hình được sử dụng
trực tiếp để đánh giá mà không tinh chỉnh chúng. Kết quả
thí nghiệm được thể hiện trong Bảng V. Chúng ta có thể thấy
rằng CodeBERT, CodeT5, và GraphCodeBERT được tiền
huấn luyện mà không được tinh chỉnh hoạt động kém do vấn
đề suy thoái biểu diễn [82], [83]. Nghĩa là, các token tần số
cao chi phối biểu diễn chuỗi [82], dẫn đến biểu diễn ngữ
nghĩa cấp chuỗi kém của đoạn mã và truy vấn. UniXcoder
và các mô hình của chúng tôi áp dụng kỹ thuật liên quan đến
học tương phản để thu được biểu diễn tốt hơn của các đoạn
mã và truy vấn, và hoạt động tốt hơn hai mô hình khác. Đặc
biệt, phương pháp của chúng tôi có thể giúp mô hình học
phân phối đồng nhất của biểu diễn của dữ liệu đơn phương
thức, và học sự căn chỉnh tốt cho dữ liệu đa phương thức.
Do đó, mô hình của chúng tôi vượt trội các mô hình tiền
huấn luyện khác khoảng 50% về điểm MRR trung bình thậm
chí hơn 70% trên ngôn ngữ PHP. Hơn nữa, hiệu suất của
CoCoSoDa dưới cài đặt zero-shot thậm chí vượt qua nhiều
mô hình được tinh chỉnh như CodeBERT, CodeT5 và GraphCodeBERT (Bảng II) về MRR trung bình. Tóm lại, mô hình
tiền huấn luyện của chúng tôi hoạt động tốt hơn các mô hình
tiền huấn luyện khác khi tất cả chúng không được tinh chỉnh.
Hơn nữa, hiệu suất của CoCoSoDa mà không được tinh chỉnh
thậm chí còn tốt hơn nhiều mô hình được tinh chỉnh về MRR
trung bình.

BẢNG V
HIỆU SUẤT CỦA CÁC PHƯƠNG PHÁP KHÁC NHAU DƯỚI CÀI ĐẶT THÍ
NGHIỆM ZERO-SHOT ĐƯỢC ĐÁNH GIÁ BẰNG ĐIỂM MRR.

Mô hình | Ruby | JS | Go | Python | Java | PHP | Trung bình
--- | --- | --- | --- | --- | --- | --- | ---
CodeBERT | 0.002 | 0.001 | 0.002 | 0.001 | 0.001 | 0.001 | 0.001
CodeT5 | 0.006 | 0.003 | 0.009 | 0.001 | 0.001 | 0.001 | 0.004
GraphCodeBERT | 0.238 | 0.111 | 0.209 | 0.137 | 0.123 | 0.120 | 0.156
UniXcoder | 0.576 | 0.442 | 0.648 | 0.447 | 0.466 | 0.373 | 0.492
CoCoSoDa | 0.786 | 0.709 | 0.881 | 0.696 | 0.717 | 0.640 | 0.738
Cải thiện | "36.5%" | "60.4%" | "36.0%" | "55.7%" | "53.9%" | "71.6%" | "50.0%"

B. Hạn chế & Mối đe dọa đến Tính hợp lệ

Mặc dù CoCoSoDa có ưu thế tổng thể, mô hình của chúng
tôi vẫn có thể trả về kết quả không chính xác, đặc biệt đối
với các đoạn mã sử dụng API thư viện thứ ba hoặc các phương
thức tự định nghĩa. Điều này là do CoCoSoDa chỉ xem xét
thông tin của chính đoạn mã thay vì các ngữ cảnh khác như
các phương thức khác trong lớp hoặc dự án bao quanh [66],
[84]. Trong công việc tương lai, thêm thông tin ngữ cảnh
(như lớp/dự án bao quanh và API/phương thức được gọi)
có thể được xem xét trong mô hình để cải thiện thêm hiệu
suất của CoCoSoDa.

Chúng tôi cũng xác định các mối đe dọa sau đây đối với
phương pháp:

**Ngôn ngữ Lập trình.** Do nỗ lực nặng nề để đánh giá mô
hình trên tất cả các ngôn ngữ lập trình, chúng tôi tiến hành
thí nghiệm với càng nhiều ngôn ngữ lập trình càng tốt trên
các tập dữ liệu xây dựng hiện có. Mô hình của chúng tôi trên
các ngôn ngữ lập trình khác nhau sẽ có kết quả khác nhau.
Trong tương lai, chúng tôi sẽ đánh giá hiệu quả của phương
pháp với nhiều ngôn ngữ lập trình khác.

**Mô hình tiền huấn luyện.** Để chứng minh rằng phương
pháp của chúng tôi trực giao với kỹ thuật tiền huấn luyện
về cải thiện hiệu suất cho tìm kiếm mã nguồn, chúng tôi đã
áp dụng và đánh giá phương pháp trên bốn mô hình tiền
huấn luyện bao gồm một mô hình tiền huấn luyện ngôn ngữ
tự nhiên RoBERTa và ba mô hình tiền huấn luyện mã nguồn
CodeBERT, GraphCodeBERT và UniXcoder. Vẫn cần được
xác minh liệu phương pháp đề xuất có áp dụng được cho các
mô hình tiền huấn luyện khác như GPT [85] và T5 [25] hay
không.

**Benchmark được đánh giá.** Đoạn mã được ghép cặp thường
được sử dụng làm kết quả chính xác cho truy vấn đã cho.
Trên thực tế, một số đoạn mã không được ghép cặp cũng trả
lời truy vấn đã cho. Trong tương lai, chúng tôi sẽ mời một
số nhà phát triển chấm điểm thủ công mối tương quan ngữ
nghĩa giữa đoạn mã tùy ý và truy vấn và xây dựng một benchmark tìm kiếm mã chất lượng cao.

VII. KẾT LUẬN

Trong bài báo này, chúng tôi trình bày CoCoSoDa, tận dụng
học tương phản momentum đa phương thức và tăng cường
dữ liệu mềm cho tìm kiếm mã nguồn. Nó có thể giúp mô
hình học biểu diễn hiệu quả bằng cách kéo gần biểu diễn
của các cặp mã-truy vấn và đẩy xa các đoạn mã và truy vấn
không được ghép cặp. Chúng tôi tiến hành các thí nghiệm
mở rộng trên tập dữ liệu chuẩn quy mô lớn với sáu ngôn
ngữ lập trình và kết quả xác nhận tính ưu việt của nó. Trong
công việc tương lai, thêm thông tin ngữ cảnh (như lớp/dự
án bao quanh) có thể được xem xét trong mô hình để cải
thiện thêm hiệu suất của CoCoSoDa. Gói sao chép bao gồm
tập dữ liệu, mã nguồn, và Phụ lục có sẵn tại https://github.
com/DeepSoftwareAnalytics/CoCoSoDa.

LỜI CẢM ỢN

Chúng tôi cảm ơn các nhà đánh giá vì những nhận xét có
giá trị của họ về công trình này. Nghiên cứu này được hỗ
trợ bởi Chương trình R&D Quốc gia Trọng điểm của Trung
Quốc (Số 2017YFA0700800) và Quỹ Nghiên cứu Cơ bản
cho Các Trường Đại học Trung ương dưới Grant xtr072022001.
Chúng tôi muốn cảm ơn Jiaqi Guo vì những gợi ý và phản
hồi có giá trị trong quá trình thảo luận công việc.

TÀI LIỆU THAM KHẢO
[1] J. Singer, T. C. Lethbridge, N. G. Vinson, và N. Anquetil, "An examination of software engineering work practices," trong CASCON. IBM, 1997, p. 21.
[2] L. Nie, H. Jiang, Z. Ren, Z. Sun, và X. Li, "Query expansion based on crowd knowledge for code search," IEEE Trans. Serv. Comput., vol. 9, no. 5, pp. 771–783, 2016.
[3] C. McMillan, M. Grechanik, D. Poshyvanyk, Q. Xie, và C. Fu, "Portfolio: finding relevant functions and their usage," trong ICSE. ACM, 2011, pp. 111–120.
[4] S. P. Reiss, "Semantics-based code search," trong ICSE. IEEE, 2009, pp. 243–253.
[5] F. Zhang, H. Niu, I. Keivanloo, và Y. Zou, "Expanding queries for code search using semantically related API class-names," IEEE Trans. Software Eng., vol. 44, no. 11, pp. 1070–1082, 2018.
[6] M. Allamanis, E. T. Barr, P. T. Devanbu, và C. Sutton, "A survey of machine learning for big code and naturalness," ACM Comput. Surv., vol. 51, no. 4, pp. 81:1–81:37, 2018.

[Danh sách tài liệu tham khảo tiếp tục từ [7] đến [85] với các chi tiết tương tự...]
