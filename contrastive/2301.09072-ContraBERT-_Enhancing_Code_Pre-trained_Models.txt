# 2301.09072.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/contrastive/2301.09072.pdf
# File size: 674977 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ContraBERT: Enhancing Code Pre-trained Models
via Contrastive Learning
Shangqing Liu1, Bozhi Wu1, Xiaofei Xie2y, Guozhu Meng3, and Yang Liu1
1Nanyang Technological University, Singapore
2Singapore Management University, Singapore
3SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China
fshangqin001,bozhi001 g@e.ntu.edu.sg, xiaofei.xfxie@gmail.com, mengguozhu@iie.ac.cn, yangliu@ntu.edu.sg
Abstract ‚ÄîLarge-scale pre-trained models such as CodeBERT,
GraphCodeBERT have earned widespread attention from both
academia and industry. Attributed to the superior ability in
code representation, they have been further applied in multiple
downstream tasks such as clone detection, code search and code
translation. However, it is also observed that these state-of-the-
art pre-trained models are susceptible to adversarial attacks.
The performance of these pre-trained models drops signiÔ¨Åcantly
with simple perturbations such as renaming variable names.
This weakness may be inherited by their downstream models
and thereby ampliÔ¨Åed at an unprecedented scale. To this end,
we propose an approach namely ContraBERT that aims to
improve the robustness of pre-trained models via contrastive
learning. SpeciÔ¨Åcally, we design nine kinds of simple and complex
data augmentation operators on the programming language (PL)
and natural language (NL) data to construct different variants.
Furthermore, we continue to train the existing pre-trained models
by masked language modeling (MLM) and contrastive pre-
training task on the original samples with their augmented
variants to enhance the robustness of the model. The extensive ex-
periments demonstrate that ContraBERT can effectively improve
the robustness of the existing pre-trained models. Further study
also conÔ¨Årms that these robustness-enhanced models provide
improvements as compared to original models over four popular
downstream tasks.
Index Terms ‚ÄîCode Pre-trained Models, Contrastive Learning,
Model Robustness
I. I NTRODUCTION
It has already been conÔ¨Årmed that the ‚Äúbig code‚Äù era [1] is
coming due to the ubiquitousness of software in modern soci-
ety and the accelerated iteration of the software development
cycle (design, implementation and maintenance). According
to a GitHub ofÔ¨Åcial report [2] in 2018, GitHub has already
reached 100 million hosted repositories. The Evans Data
Corporation [3] also estimated that there are 23.9 million
professional developers in 2019 and that number is expected
to reach 28.7 million in 2024. As a result, the availability of
code-related data is massive (e.g., billions of code, millions
of changed code, bug Ô¨Åxes and code documentation), which
yields a hot topic in both academia and industry. That is how
to adopt the data-driven approach (e.g., deep learning) to solve
conventional software engineering (SE) problems.
Deep learning has been widely applied to diverse SE tasks
(AI4SE) such as software vulnerability detection [4], [5], [6],
yCorresponding author.source code summarization [7], [8], [9], deep code search [10],
[11] and source code completion [12], [13], [14]. Besides,
the early works [15], [16], [17], [18], [19] directly utilized
vanilla deep learning techniques such as Long-Short Memory
Networks (LSTMs) [20] and Convolutional Neural Networks
(CNNs) [21] for different tasks. Later works [5], [7], [10], [22],
[23], [24], [4], [8] customized different network architectures
to satisfy the characteristics of the speciÔ¨Åc task for achieving
the best performance. For example, since complicated data de-
pendencies and control dependencies are easier to trigger soft-
ware vulnerabilities, Devign [5] incorporated different kinds of
program structure information with Code Property Graph [25]
to Graph Neural Networks [26] for vulnerability detection.
Considering code duplication [27] is common in the ‚Äúbig
code‚Äù era, Liu et al. [7] combined the retrieved code-summary
pair to generate high-quality summaries. Although these cus-
tomized networks have achieved signiÔ¨Åcant improvements on
speciÔ¨Åc tasks, the generalization performance is still low. To
address this limitation, some researchers propose to utilize
unsupervised techniques with the massive amount of data to
pre-train a general model [28], [29], [30], [3], [31], [32], [33],
[34], [35] and then Ô¨Åne-tune it for different downstream tasks.
For example, CuBERT [36] pre-trained BERT [37] on a large
collected Python corpus (7.4M Ô¨Åles) and then Ô¨Åne-tuned it
on different tasks such as variable-misuse identiÔ¨Åcation and
wrong binary operator identiÔ¨Åcation. CodeBERT [29] pre-
trained RoBERTa [38] for programming languages (PL) with
their natural language (NL) comments on the open-source six
programming languages [16] and evaluated it on code search
and source code summarization. GraphCodeBERT [30] further
incorporated data Ô¨Çow information to encode the relation of
variables in a program for pre-training and demonstrated its
effectiveness on four downstream tasks.
The aforementioned pre-trained models have a profound
impact on the AI4SE community and have achieved promising
results on various tasks. With the widespread use of pre-trained
models, an important question is whether these models are
robust to represent code semantics. Our preliminary study has
demonstrated that state-of-the-art pre-trained models are not
robust to a simple label-preserving program mutation such as
variable renaming. SpeciÔ¨Åcally, we utilize the test data of clone
detection (POJ-104) [39] (a task to detect whether two func-
tions are semantic equivalence with different implementations)arXiv:2301.09072v1  [cs.SE]  22 Jan 2023

--- PAGE 2 ---
0 1 2 3 4 5 6 7 8
The number of renaming edits0.30.40.50.60.70.80.91.0Accuracy
CodeBERT
GraphCodeBERT
ContraBERT_C
ContraBERT_GFig. 1. Adversarial attacks on clone detection(POJ-104).
provided by CodeXGLUE [3] and select those samples that
are predicted correctly by the pre-trained CodeBERT [29] and
GraphCodeBERT [38]. Then we randomly rename variables
within these programs from 1 to 8 edits. For example, 8
edits mean that we randomly select 8 different variables in a
function and rename them for all occurrences with the newly
generated names. If one function has less than 8 variables,
we will rename the maximum number of variables. We then
utilize these newly generated mutated variants to evaluate the
model prediction accuracy based on the cosine similarity of
the embedded vectors of these programs. Surprisingly, we
Ô¨Ånd that either CodeBERT or GraphCodeBERT suffers greatly
from renaming operation and the accuracy reduces to around
0.4 when renaming edits reach to 8 (see Fig. 1). It conÔ¨Årms
that pre-trained models are not robust to adversarial examples.
However, it is challenging to improve the robustness of pre-
trained models. Although the latest work by Yang et al. [40]
proposed some attack strategies to make CodeBERT and
GraphCodeBERT have poor performance on adversarial sam-
ples. They further combined adversarial samples with original
samples to Ô¨Åne-tune pre-trained models without any changes
to the model architecture to improve prediction robustness
on downstream tasks. However, a newly designed model that
inherently solves the weakness of robustness is not involved
in their paper.
In this paper, we propose ContraBERT, an unsupervised
contrastive learning-based framework to enhance the robust-
ness of existing pre-trained models in code scenarios. Com-
pared with Yang et al. [40], we design a new pre-trained model
that takes masked language modeling (MLM) and contrastive
pre-training task as the pre-training tasks to improve model
robustness. To design a contrastive pre-training task to help the
model group similar samples while pushing away the dissim-
ilar samples, we deÔ¨Åne nine kinds of simple or complex data
augmentation operators that transform the original program
and natural language sequence into different variants. Given an
existing pre-trained model such as CodeBERT or GraphCode-
BERT, we take the original sample as well as its augmented
variants as the input to train the model with MLM and
contrastive pre-training task, where MLM is utilized to help
the model learn better token representations and contrastivepre-training task is utilized to help the model group the similar
vector representations to enhance model robustness. As shown
in Fig. 1, ContraBERT C and ContraBERT G denote the
models are pre-trained from CodeBERT and GraphCodeBERT
with our approach respectively, we observe that with the in-
creasing number of edits, although the performance continues
to drop, the curve for ContraBERT is much smoother. The
prediction accuracy of ContraBERT C and ContraBERT G
outperform CodeBERT and GraphCodeBERT signiÔ¨Åcantly,
indicating that ContraBERT C and ContraBERT G are more
robust than the original models. We further perform an ablation
study to conÔ¨Årm each type of deÔ¨Åned PL-NL augmentation
operator is effective to improve the model robustness. Finally,
we conduct broad research on four downstream tasks (i.e.,
clone detection, defect-detection, code-to-code-trans and code
search) to illustrate that these robustness-enhanced models
provide signiÔ¨Åcant improvements as compared to the original
models. In summary, our main contributions are as follows:
We present a framework ContraBERT that enhances the
robustness of existing pre-trained models in the code sce-
nario by the pre-training tasks of masked language modeling
and contrastive learning on original samples as well as the
augmented variants.
We design nine kinds of simple or complex data augmen-
tation operators on the programming language (PL) and
natural language sequence (NL). Each operator conÔ¨Årms its
effectiveness to improve the model‚Äôs robustness.
The broad research on four downstream tasks demonstrates
that the robustness-enhanced models provide improvements
as compared to the original models. Our code and model
are released on [41] for reproduction.
Organization: The remainder of this paper is organized as
follows: Section II describes the background of the original
models that ContraBERT will use. We elaborate our approach
in Section III. Section IV and Section V present the experimen-
tal setup and experimental results. In Section VI, we give some
discussions about our work. After a brief review of related
work in Section VII, we conclude this paper in Section VIII.
II. B ACKGROUND
In this section, we brieÔ¨Çy introduce CodeBERT and Graph-
CodeBERT which will be adopted as our original pre-trained
models for ContraBERT.
A. CodeBERT
CodeBERT [29] is pre-trained on an open-source bench-
mark CodeSearchNet [16], which includes 2.1M bimodal NL-
PL (comment-function) pairs and 6.4M unimodal functions
without comments across six programming languages. The
model architecture is the same with RoBERTa [38], which
utilizes multi-layer bidirectional Transformer [42] for un-
supervised learning. SpeciÔ¨Åcally, CodeBERT consists of 12
identical layers, 12 heads and the dimension size for each
layer is 768. In total, the number of model parameters reaches
125M. Two different pre-training objectives are used, the Ô¨Årst
one is masked language modeling (MLM), which is trained on

--- PAGE 3 ---
bimodal data. MLM objective targets predicting the original
tokens that are masked out in NL-PL pairs. To fully utilize uni-
modal data, CodeBERT further uses Replaced Token Detection
(RTD) objective on both bimodal and unimodal samples. RTD
objective is designed to determine whether a word is original
or not. At the Ô¨Åne-tuning phase, two downstream tasks (i.e.,
code search and source code documentation generation) are
used for evaluation. The experimental results demonstrate that
CodeBERT outperforms supervised approaches on both tasks.
B. GraphCodeBERT
GraphCodeBERT [30] is a pre-trained model for code,
which considers structures in code. SpeciÔ¨Åcally, it incorporates
the data Ô¨Çow of code to encode the relations of ‚Äúwhere
the value comes from‚Äù between variables in the pre-training
stage. In addition to the pre-training task of masked language
modeling (MLM), GraphCodeBERT further introduces two
new structure-aware pre-training tasks. The Ô¨Årst one edge
prediction is designed to predict whether two nodes in the data
Ô¨Çow are connected. The other node alignment is designed to
align edges between code tokens and nodes. GraphCodeBERT
utilizes NL-PL pairs for six programming languages from
CodeSearchNet [16] for pre-training. It is Ô¨Åne-tuned on four
downstream tasks including code search, clone detection, code
translation and code reÔ¨Ånement. The extensive experiments on
these tasks conÔ¨Årm that code structures and the deÔ¨Åned pre-
training tasks help the model achieve state-of-the-art perfor-
mance on these tasks.
III. A PPROACH
In this section, we Ô¨Årst present an overview of our approach,
then detail each component including PL-NL augmentation,
model design in pre-training and the Ô¨Åne-tuning settings for
downstream tasks.
A. Overview
The overview of ContraBERT is shown in Fig. 2. Specif-
ically, given a pair of the function Cwith its comment W
(i.e., (C;W )), we Ô¨Årst design a set of PL-NL augmentation
operatorsff()g;fg()gto construct the simple or com-
plex variants for CandWrespectively. In the pre-training
phase, initialized from existing pre-trained models such as
CodeBERT or GraphCodeBERT, we further pre-train these
models on the original samples and their augmented variants
with masked language modeling (MLM) and contrastive pre-
training task to enhance the model robustness. Finally, when
ContraBERT is pre-trained over a large amount of unlabeled
data, we Ô¨Åne-tune it for different types of tasks such as retrieval
tasks, classiÔ¨Åcation tasks and generation tasks with the task-
speciÔ¨Åc data in a supervised manner.
B. PL-NL Augmentation
Given a program C, clone detection [43] could help to
identify a semantically equivalent program C0. However, this
technique is unrealistic in practice. For any function in a Ô¨Åxed
dataset, we cannot guarantee that we will be able to Ô¨Ånd the
(C, W)PL-NL AugmentationOriginal(function, comment)
Pre-trainedContraBERTCodeClassification TasksGeneration TasksRetrieval TasksPre-trainingFine-tuning:
Downstream TasksC{ùëìùëñ(‚àó)}{C1,‚Ä¶,Cn}W{ùëîùëó(‚àó)}{W1,‚Ä¶,Wm}ContraBERT{C, W , C‚Äô,W‚Äô}Randomly Pick oneFig. 2. The Overview of ContraBERT.
semantically equivalent variants. Furthermore, clone detection
usually takes a project for analysis, which is not applicable to
a single function. Hence, we consider constructing augmented
variants based on the original samples. Compared with the ex-
isting works [44], [45] that only focus on program mutations,
we design a set of natural language (NL) sequence augmented
operators. SpeciÔ¨Åcally, we design a series of simple operators
and complex operators for both PL and NL to construct
variants.
1) Program (PL) Augmentation Operators: For program
augmented operators, we design four kinds of complex op-
erators and one kind of simple operator.
Complex Operators:
Rename Function Name (RFN). It is designed to replace
the function name with a new name that is taken randomly
from an extra vocabulary set constructed on the pre-training
dataset. We extract all function names in the pre-training
dataset for the construction. Since each sample in the dataset
is a single function, the renamed function preserves the
equivalent semantics to the original function.
Rename Variable (RV). It renames variables in a function. A
random number of variables for all occurrences in the func-
tion will be replaced with the new names taken randomly
from an extra vocabulary set. We extract all variable names
from the pre-training dataset to construct this vocabulary
set. This operator only mutates the variable names and all
occurrences of them with the new variable names, which
does not change the semantics of the original function.
Insert Dead Code (IDC). It means to insert unused state-
ments in a function. To generate unused code statements,
we traverse AST to identify the assignment statements and
then randomly select one assignment statement to rename
its variables with new names that have never appeared
in the same function. After that, we consider it as the
dead code and insert it at the position after the original
assignment statement. As the inserted dead code does not
change the original program behaviour, IDC is regarded as
the semantically equivalent operator.
Reorder (RO). It randomly swaps two lines of statements
that have no dependency on each other in a basic block in a

--- PAGE 4 ---
function body such as two declaration statements appearing
on two consecutive lines without other statements between
them. We traverse AST and analyze the data dependency for
extraction. Since the permuted statements are independent
without data dependency, this operator preserves the original
program semantics.
Simple Operators:
Sampling (SP). It randomly deletes one line statement from
a function body and preserves others. It can serve as
regularizers to avoid overÔ¨Åtting [45].
2) Comment (NL) Augmentation Operators: Apart from the
program augmentation, we further design one kind of complex
operator and three kinds of simple operators for comment
augmentation operators as follows:
Complex Operators:
Back Translation Mutation (Trans). It refers to translating
a source sequence into another language (target sequence)
and then converting this target sequence to the original
sequence [46]. We use the released tool [47] for the im-
plementation where the source is in English and the target
is in German.
Simple Operators:
Delete. It randomly deletes a word in a comment.
Switch. It randomly switches the positions of two words in
a comment.
Copy. It randomly copies a word and inserts it after this
word in a comment.
Given a function Cwith its paired comment W, we utilize
the above augmentation operators on CandWrespectively
to obtain the augmentation sets, which are deÔ¨Åned as SCand
SWrespectively. SpeciÔ¨Åcally, each operator is conducted once
to get its corresponding augmented variant and insert it into
the corresponding augmentation set. For the operator IDC,
which may not get its variant for some speciÔ¨Åc functions, we
ignore it and use other operators for the construction. Then
we randomly select an augmented version from SCandSW
(i.e.,C02SCandW02SW) and construct the quadruple
(C;W;C0;W0)for the pre-training. Note that during the pre-
training process, at each learning step, (C0;W0)is randomly
selected from the augmented sets SCandSWrespectively.
Hence, each augmented sample in the sets is used when the
model has sufÔ¨Åcient learning steps.
C. Model Design and Pre-training
Basically, ContraBERT is further trained from existing pre-
trained models. We directly utilize the existing pre-trained
model and further pre-train it with masked language mod-
eling (MLM) and contrastive pre-training task to enhance its
robustness. The model design of ContraBERT is presented in
Fig. 3.
1) Model Design: As shown in Fig. 3, ContraBERT con-
sists of two separate encoders M and M‚Äô, where M can be
represented by any pre-trained models such as CodeBERT.
The model architecture of M‚Äô is the same with the encoder M
and the initial weights are also the same with M. However, theweight update strategy is different with M. SpeciÔ¨Åcally, given
a quadruple (C;W;C0;W0)from Section III-B, we construct
two input sequences X=f[CLS ];W;[SEP ];C;[SEP ]g
andX0=f[CLS ];W0;[SEP ];C0;[SEP ]g, where ‚Äú [CLS ]‚Äù
indicates the beginning of a sequence and ‚Äú [SEP ]‚Äù is a
symbol that concatenates two kinds of sequence. We utilize
the encoder M and M‚Äô to encode the masked input sequence
XandX0respectively.
2) Pre-training Tasks: Masked language modeling (MLM)
is an effective and widely adopted pre-training task to learn the
effective token representations [37], [38], we also utilize it as
one of our pre-training tasks. However, by our preliminary
results in Section I, we observe that the models trained
by MLM are weak to the adversarial examples, we further
introduce a contrastive pre-training task to group the similar
data and push away the dissimilar data to reshape the learnt
space for encoder M to enhance the model robustness.
Masked Language Modeling (MLM). We utilize MLM to
learn token representations in a sequence. SpeciÔ¨Åcally, given
the sequence X=f[CLS ];W;[SEP ];C;[SEP ]g, a random
set of positions in X are masked out. We select 15% tokens
to mask out and obtain the masked token set. Furthermore,
we replace 80% of the masked tokens in this set with the
‚Äú[MASK]‚Äù symbol, 10% with the random tokens from the
vocabulary set and the remaining 10% unchanged. We conÔ¨Åg-
ure these settings since they are conÔ¨Årmed effective to learn
the token representations in a sequence [37], [38]. The loss
functionLMLM can be expressed as follows:
LMLM = X
xi2Mlogp(xijXmask) (1)
whereXmaskis the masked input sequence and Mis the
masked token set.
Contrastive Pre-training. We design a contrastive pre-
training task that uses InfoNCE [48] as the loss function to
enhance model robustness. It can be expressed as follows:
LInfoNCE = logexp(qk+=t)
exp(qk+=t) +Pn
i=1exp(qki=t)(2)
wheretis a temperature hyper-parameter [49], the query
vector qis the encoded vector representation, k+is a similar
key vector that qmatches, K=fk1;:::;kngis a set of
dissimilar encoded vectors. InfoNCE tries to classify the query
vector qinto its similar sample k+and pushes it away from
dissimilar samples in the set K. The similarity is measured
by dot product ()between two vectors. To obtain the query
representation qand the similar key representation k+, in-
spired by the recent advance [50] on the image recognition,
we adopt Momentum Contrast (MoCo) [50] for the encoding.
SpeciÔ¨Åcally, it introduces an extra encoder M‚Äô to get the key
representation k+, which can be expressed as follows:
q= LayerNorm(M( X)[0])
k+= LayerNorm(M0(X0)[0])(3)
whereXandX0denote the original masked sequence and its
mutated variant respectively. The index 0 denotes the position

--- PAGE 5 ---
[CLS] Print with [MASK] [SEP] ‚Ä¶ ind[MASK] = str ( [Mask] )‚Ä¶ [SEP]  indent
‚Äú def to_string( indent ) :ind_str= str (indent )Print ( ind_str) ‚Äú‚ÄúPrint with indent‚ÄùstrindentMasked Language Modeling
Comment W:Function C:MaximizeMinimizeEnqueue for next dissimilars‚Ä¶
‚Äúdef to_string( a ) :b = str (a )Print (b )‚Äù‚ÄúPrint with with indent‚ÄùMutated Comment W‚Äô:Mutated Function C‚Äô:M[CLS] Print with with [MASK] [SEP] def to ‚Ä¶ [MASK] = str ( a )‚Ä¶ [SEP]  M‚Äô[CLS] [CLS] InfoNCEquery key QueueMFig. 3. The model design for ContraBERT where the encoder M can be represented by the existing pre-trained models such
as CodeBERT. The initial weights of the encoder M‚Äô are the same as the encoder M while the weight update is different.
of ‚Äú[CLS]‚Äù in the sequence, which can be considered as the
aggregated sequence representation. The encoder M‚Äô is the
same as the encoder M, but during the learning phase, it
utilizes a momentum to update its learnt weights while the
encoder M uses the gradient descent:
M0 mM0+ (1 m)M (4)
wherem2[0;1)is a momentum coefÔ¨Åcient for scaling, M0
andMdenote the learnt weights for model M0and M.
From Eq 3, we obtain the query representation qand the key
representation k+, to compute the similarity with dissimilar
vectors from K, MoCo maintains a ‚Äú dynamic ‚Äùqueue of length
n. This queue stores the dissimilar keys from the previous
batches. SpeciÔ¨Åcally, during the learning phase, the current
queryqwill calculate the similarity with all dissimilar vectors
in this queue. Afterwards, the key vector k+will be enqueued
toqueue to replace the oldest one and we take it as the
dissimilar samples for the calculation of the next query. Hence,
it is namely dynamically updated .
Finally, we add both loss values with the scaled factor to
pre-train ContraBERT and this process is expressed as follows:
LLoss=LMLM +wLInfoNCE (5)
wherewis the hyper-parameter to scale the weight for both
pre-training tasks.
D. Fine-tuning
Once ContraBERT is further pre-trained from the original
pre-trained model, we can utilize it to obtain the vector
representation for a program. Furthermore, we can also trans-
fer it to different downstream tasks during the Ô¨Åne-tuning
phase. These downstream tasks can be roughly categorized
into three groups: (1) retrieval tasks (e.g., clone detection [39],
[51], code search [11], [16]); (2) classiÔ¨Åcation tasks (e.g.,
defect-detection [5]); (3) generation tasks (e.g., code-to-code
translation [52], [53], code-reÔ¨Ånement [54] and source code
summarization [7]). Since the output space may differ from the
pre-trained space, similar to CodeBERT and GraphCodeBERT,we add the task-speciÔ¨Åc module and then Ô¨Åne-tune the com-
pleted network on the labeled data. SpeciÔ¨Åcally, for retrieval
tasks, we further train ContraBERT on a labeled dataset; for
classiÔ¨Åcation tasks, we add a multi-layer perceptron (MLP)
to predict the probability for each class; for generation tasks,
we add a Transformer-based decoder to generate the target
sequence.
IV. E XPERIMENTAL SETUP
In experiments, we Ô¨Årst evaluate the effectiveness of our
approach (RQ1) in improving model robustness. Then we plot
the feature space learnt by different pre-trained models for
visualization to conÔ¨Årm the features are learnt better(RQ2).
Finally, we conduct extensive experiments to demonstrate the
robustness-enhanced models provide signiÔ¨Åcant improvements
on downstream tasks (RQ3-RQ4). The detailed research ques-
tions are described as follows:
RQ1: What is the performance of different augmentation
operators in enhancing the robustness of the pre-trained
model?
RQ2: Can ContraBERT reshape the vector space learnt from
the pre-trained models to obtain better vector representa-
tions?
RQ3: Can ContraBERT outperform the original pre-trained
models on different downstream tasks?
RQ4: Are the deÔ¨Åned pre-training tasks both effective in
improving the downstream task performance?
A. Evaluation Tasks, Datasets and Baselines
We select four downstream tasks for evaluation. They are
clone detection [39], [43], code search [16], defect detec-
tion [5] and code translation [52], [53]. We brieÔ¨Çy introduce
each task as follows:
Clone Detection (Code-Code Retrieval). This task is to iden-
tify semantically equivalent programs from a set of distractors
by measuring the semantic similarity between two programs.
AI for clone detection calculates cosine similarity between two
embedding vectors of programs produced by neural networks
and selects the top-k most similar programs as the candidates.

--- PAGE 6 ---
Defect Detection (Code ClassiÔ¨Åcation). It aims to detect
whether a function contains defects that will be exploited to
attack the software systems. Since the defects in a program
are still difÔ¨Åcult to be effectively detected by the traditional
techniques, recently advanced works [5], [55], [17] propose to
employ a deep neural network to learn program semantics to
facilitate the detection. These AI-based techniques predict the
probability of whether a function is vulnerable or not.
Code Translation (Code-Code Generation). It aims to trans-
late a program in a programming language (e.g., Java) to the
semantically equivalent one in another language (e.g., C#).
Some previous works [53] analogy it to machine transla-
tion [42], [56] in NLP and employ LSTMs [20] and Trans-
former [42] for code translation.
Code Search (Text-Code Retrieval). It aims at returning the
desired programs based on the query in a natural language.
Similar to clone detection, it measures the semantic relevance
between queries and programs. The input for the deep code
search system [16], [11] is a natural language query and
the output is programs that meet the query requirements.
The cosine similarity is used to compute semantic similarity
between the vectors of a query and programs.
In terms of the pre-training dataset, we use the released
dataset provided by CodeSearchNet [16] and this dataset
is also used by CodeBERT and GraphCodeBERT. We use
bimodal NL-PL pairs for pre-training, which consist of six
programming languages including Java, Python, Ruby, Go,
PHP and JavaScript. For the Ô¨Åne-tuning datasets, for the
tasks of clone detection (POJ-104), defect detection, and
code translation, we directly utilize the released task-speciÔ¨Åc
dataset provided by CodeXGLUE [3]. For code search, we
use the cleaned dataset provided by GraphCodeBERT [30] for
evaluation. For each task, we utilize the ofÔ¨Åcial scripts to make
a fair comparison. In addition, by the deÔ¨Åned augmentation
operators in Section III-B, we obtain a large amount of extra
data(C0;W0)used in ContraBERT as compared to the original
pre-training data used in CodeBERT and GraphCodeBERT.
Hence, we further add two baselines CodeBERT Intr and
GraphCodeBERT Intr, which utilize original data as well as
the dataset of the extra data (C0;W0)to pre-train CodeBERT
and GraphCodeBERT with MLM for comparison.
B. Evaluation Metrics
In ContraBERT, different metrics are used to evaluate down-
stream tasks. We follow the metrics that CodeXGLUE used
for evaluation, and the details are listed below:
MAP@R. It is the abbreviation of the mean of average
precision, which is used to evaluate the result of retrieving
R most similar samples in a set given a query. MAP@R is
used for clone detection, where R is set to 499 for evaluation.
Acc. It deÔ¨Ånes the ratio of correct predictions (i.e., the exact
match) in the testset. Acc is used for the evaluation of defect
detection and code translation.
BLEU-4. It is widely used to evaluate the text similarity
between the generated sequence with the ground-truth in the
generation systems. We use BLEU-4 for code translation.MRR. It is the abbreviation of Mean Reciprocal Rank, which
is widely adopted in information retrieval systems [11], [57].
We used it to evaluate the performance of code search. Instead
of retrieving 1,000 candidates like CodeBERT [29], we follow
the settings of GraphCodeBERT [30] to retrieve the answer for
each query from the whole test set.
C. Experimental Settings
We adopt CodeBERT and GraphCodeBERT as our original
models. We set the maximum input sequence length Xand
the mutated sequence X0as 512 following CodeBERT. We use
Adam for optimizing with 256 batch size and 2e-4 learning
rate. At each iteration, X0is constructed by C0andW0, which
are randomly picked from SCandSWrespectively. Following
He et al. [50], the momentum coefÔ¨Åcient m, temperature
parametertandqueue size is set to 0.999, 0.07 and 65536
accordingly. We set the weight win Eq 5 as 0.5 to accelerate
the coverage process. The model is trained on a DGX machine
with 4 NVIDIA Tesla V100 with 32GB memory. To alleviate
the bias towards the high-resource languages (i.e., the number
of samples for different programming languages is different),
we refer to GraphCodeBERT [30] and sample each batch from
the same programming language according to a multinomial
distribution with probabilities fqigi=1:::N.
qi=p
iPN
j=1p
jwith pi=niPN
k=1nk(6)
whereniis the number of samples for i-th programming
language,Nis the total number of languages and is set
to 0.7. The model is trained with 50K steps to ensure each
mutated sample is utilized for the learning process and it
takes about 2 days to Ô¨Ånish the pre-training process. At
Ô¨Åne-tuning phase, we directly utilize the default settings of
CodeXGLUE [3] and GraphCodeBERT [30] in ContraBERT
for downstream tasks. All experiments of downstream tasks
are conducted on Intel Xeon Silver 4214 Processor with 6
NVIDIA Quadro RTX 8000 with 48GB memory.
V. E XPERIMENTAL RESULTS
A. RQ1: Robustness Enhancement.
We investigate the augmentation operators in enhancing
model robustness by validating the accuracy of samples against
adversarial attacks on clone detection (POJ-104). The main
reason to choose clone detection is that it targets identifying
the semantically equivalent samples from other distractors.
Hence, although the variable renaming operator changes the
text of a program, the original program semantics are still un-
changed. We statistically analyse the correctly predicted results
under a different number of renaming edits for illustration. The
experiments are conducted in a zero-shot manner [58], which
means that it does not involve Ô¨Åne-tuning phase and we di-
rectly utilize the pre-trained model for evaluation. SpeciÔ¨Åcally,
we remove one operator and keep the remaining operators in
Section III-B to pre-train the model. For fairness, the other
settings in the experiments are the same as ContraBERT.
Then we utilize the testset (in total 12,000 samples) on clone

--- PAGE 7 ---
TABLE I. Results of ContraBERT against the variable renaming operator in a zero-shot manner.
Model NumN=0 N=1 N=4 N=8Model NumN=0 N=1 N=4 N=8
Acc Acc Acc Acc Acc Acc Acc Acc
ContraBERT C w/o RFN 10,087 1 0.977 0.868 0.634 ContraBERT G w/o RFN 10,375 1 0.975 0.873 0.634
ContraBERT C w/o RV 8,665 1 0.932 0.597 0.291 ContraBERT G w/o RV 9,042 1 0.955 0.657 0.309
ContraBERT C w/o IDC 9,997 1 0.969 0.865 0.618 ContraBERT G w/o IDC 10,530 1 0.963 0.862 0.612
ContraBERT C w/o RO 9,923 1 0.963 0.857 0.619 ContraBERT G w/o RO 10,509 1 0.968 0.868 0.617
ContraBERT C w/o SP 10,604 1 0.959 0.849 0.616 ContraBERT G w/o SP 11,140 1 0.969 0.860 0.613
ContraBERT C w/o Trans 9,536 1 0.971 0.856 0.621 ContraBERT G w/o Trans 10,360 1 0.973 0.859 0.617
ContraBERT C w/o Delete 10,199 1 0.978 0.871 0.639 ContraBERT G w/o Delete 10,376 1 0.981 0.878 0.643
ContraBERT C w/o Switch 9,809 1 0.975 0.877 0.637 ContraBERT G w/o Switch 10,457 1 0.978 0.876 0.647
ContraBERT C w/o Copy 10,749 1 0.977 0.874 0.635 ContraBERT G w/o Copy 10,859 1 0.981 0.880 0.641
ContraBERT C 10,463 1 0.981 0.882 0.649 ContraBERT G 10,565 1 0.985 0.888 0.654
detection (POJ-104) and randomly mutate the variables con-
tained in the correctly predicted samples produced by different
pre-trained models from 1 to 8 edits to test the prediction
accuracy. The experimental results are shown in Table I where
N is the number of edits and Num is the total number of
correctly predicted samples without any edits in the testset
for different models. ContraBERT C/G deÔ¨Ånes the model is
initialized by CodeBERT and GraphCodeBERT respectively
and w/odeÔ¨Ånes the removed operator .
From Table I, we Ô¨Ånd that in general, with the increasing
number of edits, the performance continues to drop. It is
reasonable, as the increasing number of edits, the difÔ¨Åculty
for corrected predictions is also increased. We also observe
that each augmented operator is beneÔ¨Åcial to improve model
robustness against the adversarial samples and when incor-
porating all operators, we obtain the best performance. It
demonstrates the effectiveness of our designed PL-NL aug-
mentation operators. In terms of NL augmentation operators,
the operators Delete/Switch/Copy are relatively weaker in the
robustness enhancement compared with the operator Trans.
Since the operators (Delete/Switch/Copy) just have a limited
extent of modiÔ¨Åcation on the original sequence (i.e., only one
or two words are modiÔ¨Åed), the text similarity between W
andW0is more similar than the operator Trans produces.
Hence, the data diversity is limited by Delete/Switch/Copy,
which leads to the robustness improvement is not as obvious
as the operator Trans. In terms of PL augmentation operators,
we Ô¨Ånd that the number of correctly predicted samples of
ContraBERT C/G w/o RV is the lowest (e.g., 8,665 and
9,042). With the increasing number of edits, the accuracy
drops by a great margin. This indicates that RV operator
plays a critical role against adversarial attacks and removing
it harms the performance signiÔ¨Åcantly. In addition, removing
RFN operator, ContraBERT also has higher accuracy than
other PL operators (i.e., RV , IDC, RO and SP), which indicates
that RFN has fewer contributions. It is caused by the generated
programC0by RFN (i.e., rename function name) is more
similar to the original program Ccompared with other PL
augmentation operators.
-IRQ1JEach operator in the designed PL-NL augmen-
tation is effective in improving model robustness and when
incorporating them, the robustness of pre-trained models is
further enhanced.B. RQ2: Visualization for Code Embeddings.
We visualize the code representation space learnt by dif-
ferent pre-trained models to conÔ¨Årm that the contrastive pre-
training task can reshape the learnt vector space to ensure the
model is more robust. SpeciÔ¨Åcally, we use the clone detection
(POJ-104) task provided by CodeXGLUE [3] for evaluation.
The main reason for selecting clone detection is that it is
more intuitive to observe and validate the similarity of code
representation on the semantic equivalence programs. The
dataset consists of 104 programming problems, where each
problem has 500 semantically equivalent programs with differ-
ent implementations. Theoretically, the program semantics for
one problem should be the same. Hence, the code vectors (i.e.,
representations) of programs from pre-train models for one
problem should be closer than the code vectors of programs
for other problems. We randomly select 5 different problems
with 100 samples and take them as the inputs to CodeBERT,
GraphCodeBERT, ContraBERT C and ContraBERT G for
visualization where C/G deÔ¨Ånes ContraBERT is initialized by
CodeBERT or GraphCodeBERT respectively. We utilize the
vector of the token ‚Äú[CLS]‚Äù as the program representation.
We further utilize T-SNE [59] to reduce the vector dimension
to a two-dimensional space for visualization. Similar to Sec-
tion V-A, this process is also zero-shot [58], which helps us to
validate the learnt space by different pre-training techniques.
As shown in Fig. 4, the vectors produced by Graph-
CodeBERT (See Fig. 4b) have a certain ability to group
some problems of programs compared with CodeBERT (See
Fig. 4a), which indicates that incorporating program structures
such as data Ô¨Çow graph into pre-training is beneÔ¨Åcial for the
model to learn program semantics. However, we also Ô¨Ånd
that the improvement is limited and the boundary in Fig. 4b
is not clear. Some data points are scattered, especially in
the upper-right part of Fig. 4b. In contrast, the visualization
of ContraBERT is shown in Fig. 4c and Fig. 4d. We see
that the programs in the same problem aggregate together
closely as a cluster and different clusters have much clearer
boundaries. This indicates that ContraBERT is more power-
ful than CodeBERT/GraphCodeBERT to group semantically
equivalent data and push away dissimilar data. We attribute
this ability to the deÔ¨Åned PL-NL augmentation operators to
capture the essence of programs. Furthermore, ContraBERT G
(See Fig. 4d) has a better clustering performance than Contra-
BERT C (See Fig. 4c). For example in Fig. 4c, the label 0 has

--- PAGE 8 ---
0.075
 0.050
 0.025
 0.000 0.025 0.050 0.075 0.1000.075
0.050
0.025
0.0000.0250.0500.075(a) CodeBERT
0.075
 0.050
 0.025
 0.000 0.025 0.050 0.0750.06
0.04
0.02
0.000.020.040.060.08 (b) GraphCodeBERT
0.100
 0.075
 0.050
 0.025
 0.000 0.025 0.050 0.0750.075
0.050
0.025
0.0000.0250.0500.075 (c) ContraBERT C
0.075
 0.050
 0.025
 0.000 0.025 0.050 0.0750.08
0.06
0.04
0.02
0.000.020.040.060.08 (d) ContraBERT G
Fig. 4. Visualization for vector representations of each 100 programs for 5 problems and they are randomly picked from clone
detection (POJ-104). The vectors are produced by CodeBERT, GraphCodeBERT, ContraBERT C and ContraBERT G. The
point with different colours indicates different problems that this function belongs to.
TABLE II. Results on clone detection and defect detection.
ModelClone Detection Defect Detection
MAP@R Acc
CodeBERT 84.29 62.08
CodeBERT Intr 86.34 62.41
ContraBERT C (MLM) 86.21 62.25
ContraBERT C (Contra) 81.44 62.22
ContraBERT C 90.46 64.17
GraphCodeBERT 85.16 62.85
GraphCodeBERT Intr 87.60 62.26
ContraBERT G (MLM) 87.30 62.01
ContraBERT G (Contra) 85.63 58.82
ContraBERT G 90.06 63.32
two distant clusters while in Fig. 4d, it only has one cluster.
The improvements are from the used original model that
GraphCodeBERT is superior to CodeBERT. In addition, we
compute the distortion distance1[60] of the selected samples
for these models to strengthen the conclusion. The distances
of CodeBERT, GraphCodeBERT, ContraBERT C and Contra-
BERT G are 0.333, 0.212, 0.202, and 0.194 respectively. We
can Ô¨Ånd that ContraBERT has a lower distortion distance than
CodeBERT and GraphCodeBERT, which demonstrates their
clusters are more compact.
-IRQ2JThrough contrastive pre-training tasks to learn
augmented variants constructed by a set of PL-NL operators,
ContraBERT is able to group the semantically equivalent
samples and push away the dissimilar samples, thus learning
better vector representations.
C. RQ3: Performance of ContraBERT on Downstream Tasks.
We conduct extensive experiments on four downstream
tasks to evaluate the performance of ContraBERT as com-
pared to the original CodeBERT and GraphCodeBERT. We
further add two baselines (i.e., CodeBERT Intr and Graph-
CodeBERT Intr), which are pre-trained by original data as
well as the augmented variants. We supplement these two
baselines to ensure the used scale of data is consistent with
ContraBERT for a fair comparison. The results of clone/defect
detection are shown in Table II. Table III presents the results
1The distortion distance refers to the sum of the squared distances of each
sample to their assigned cluster centre.TABLE III. Results on code translation.
ModelCode Translation
Java!C# C#!Java
BLEU-4 Acc BLEU-4 Acc
CodeBERT 79.92 59.00 72.14 58.00
CodeBERT Intr 79.93 59.20 75.71 58.60
ContraBERT C (MLM) 79.90 59.10 75.03 58.10
ContraBERT C (Contra) 51.99 34.60 46.75 38.30
ContraBERT C 79.95 59.00 75.92 59.60
GraphCodeBERT 80.58 59.40 72.64 58.80
GraphCodeBERT Intr 80.61 59.60 75.50 60.10
ContraBERT G (MLM) 80.36 59.40 75.10 60.00
ContraBERT G (Contra) 55.48 39.40 48.92 39.00
ContraBERT G 80.78 59.90 76.24 60.50
of code translation and Table IV presents the results of code
search where the rightmost ‚Äúoverall‚Äù column is the average
value for six programming languages. Because the values for
clone detection and defect detection of GraphCodeBERT are
not reported by their original paper [30], we use ofÔ¨Åcial code
for reproduction and report these values. The other values
of CodeBERT and GraphCodeBERT are directly taken from
CodeXGLUE [3] and Guo et al. [30].
From Table II and Table III, we Ô¨Ånd that ContraBERT C/G
outperforms original pre-trained models CodeBERT or Graph-
CodeBERT on clone detection (POJ-104), defect detection
and code translation. However, the absolute gains on code
search (see Table IV) are minor. For these improvements, we
attribute to the robustness-enhanced models providing better
performance on downstream tasks. When it comes to minor
improvements in code search, we ascribe to the difÔ¨Åculty of
this task. Code search requires learning the semantic mapping
between query and program. However, the semantic gap
between programs and natural languages is huge. It makes the
model difÔ¨Åcult to achieve signiÔ¨Åcant improvements. In total,
considering the scale of testset on code search, which contains
52,561 samples for six programming languages, the improve-
ments are still promising. Furthermore, we Ô¨Ånd that compared
with CodeBERT and GraphCodeBERT, CodeBERT Intr and
GraphCodeBERT Intr have better performance on these tasks.
It is reasonable since we add extra data to further pre-train
CodeBERT and GraphCodeBERT. However, the performance
of CodeBERT Intr and GraphCodeBERT Intr is worse than

--- PAGE 9 ---
TABLE IV. Results on code search where the evaluation metric is MRR.
Model Ruby Javascript Go Python Java PHP Overall
CodeBERT 0.679 0.620 0.882 0.672 0.676 0.628 0.693
CodeBERT Intr 0.686 0.623 0.883 0.676 0.678 0.630 0.696
ContraBERT C (MLM) 0.675 0.621 0.888 0.670 0.675 0.631 0.693
ContraBERT C (Contra) 0.593 0.532 0.864 0.622 0.618 0.584 0.636
ContraBERT C 0.688 0.626 0.892 0.678 0.685 0.634 0.701
GraphCodeBERT 0.703 0.644 0.897 0.692 0.691 0.649 0.713
GraphCodeBERT Intr 0.709 0.647 0.894 0.692 0.693 0.647 0.714
ContraBERT G (MLM) 0.692 0.642 0.897 0.690 0.690 0.647 0.710
ContraBERT G (Contra) 0.626 0.582 0.882 0.655 0.659 0.613 0.670
ContraBERT G 0.723 0.656 0.899 0.695 0.695 0.648 0.719
ContraBERT C/G. It demonstrates that even with the same
scale of data, ContraBERT C/G are still better than Code-
BERT and GraphCodeBERT, which further strengthen our
conclusion that the improvements are brought by our proposed
approach rather than the gains brought by the increased scale
of the data.
-IRQ3JContraBERT comprehensively improves the
performance of original CodeBERT and GraphCodeBERT
on four downstream tasks, we attribute the improvements to
the enhanced robustness of the model has better performance
on these tasks.
D. RQ4: Ablation Study for Pre-training Tasks.
ContraBERT utilizes two pre-training tasks, the Ô¨Årst one
is MLM, which learns the token representations and the
second one is the contrastive pre-training task, which improves
the model robustness by InfoNCE loss function. We further
investigate the impact of each pre-training strategy on down-
stream tasks. The experimental results are shown in Table II,
Table III and Table IV respectively, where the row of MLM or
Contra denotes the results obtained by purely using MLM or
contrastive pre-training task. For a fair comparison, the other
settings are the same when combining both pre-training tasks
for pre-training.
We can observe that the performance of purely using
contrastive pre-training tasks is worse than purely using MLM
on these downstream tasks, especially on the task of code
translation. It is acceptable since both pre-training tasks are
excellent in different aspects. SpeciÔ¨Åcally, MLM is designed
by randomly masking some tokens in a sequence to help the
model learn token representations. The learnt token repre-
sentations are important for generation tasks to generate a
target sequence such as code translation, so it will help the
model achieve good performance on these tasks. However,
the contrastive pre-training task is designed by grouping
the semantically equivalent samples while pushing away the
dissimilar samples through InfoNCE loss function. The model
robustness is enhanced by the contrastive pre-training task.
Furthermore, when combing both pre-training tasks, our model
achieves better performance compared with purely using one
of the pre-training tasks, which indicates that ContraBERT is
robust at the same time is able to achieve better performance
on the downstream tasks.-IRQ4JMasked language modeling (MLM) and the
contrastive pre-training task play different roles for Contra-
BERT. When combining them together, the model achieves
higher performance on different downstream tasks.
VI. D ISCUSSION
In this section, we Ô¨Årst discuss the implications of our work,
then discuss the limitations followed by threats to validity.
A. Implications
In this work, we Ô¨Ånd that the widely used pre-trained code
models such as CodeBERT [29] or GraphCodeBERT [30]
are not robust to adversarial attacks. Based on this Ô¨Ånding,
we further propose a contrastive learning-based approach for
improvement. We believe that this Ô¨Ånding in our paper will
inspire the following-up researchers when designing a new
model architecture for code, considering some other problems
in the model such as robustness, generalization and not just
focusing on the accuracy of the model on different tasks.
B. Limitations
By our experimental results, we Ô¨Ånd that the robustness of
the model is enhanced signiÔ¨Åcantly compared with the original
models. We attribute it to the contrastive pre-training task
to learn the semantically equivalent samples. However, these
robustness-enhanced models only have slight improvements
on the downstream task of code search. For this task, since it
requires learning the semantic mapping between a query and
its corresponding code, the designed augmentation operators
just modify the code or query itself, hence their correlations
are not captured and this leads to the improvements being
limited. For code search, a possible solution to further improve
the performance is to build the token relations between PL
and NL for augmented variants, however, it involves intensive
work to analyse the relations between the program and natural
language comment. We will explore it in our future work.
Another limitation is the designed augmentation operators
for PL and NL. We just design some basic operators to trans-
form programs and comments. These operators are straight-
forward, although they are conÔ¨Årmed their effectiveness in
improving model robustness. It is intriguing to explore more
complex augmentation strategies such as multiple operations
on these operators for a sample to construct complex aug-
mented variants.

--- PAGE 10 ---
C. Threats to Validity
Internal validity: The Ô¨Årst threat is the hyper-parameter tun-
ing for pre-training. More hyper-parameters need to tune than
CodeBERT or GraphCodeBERT for example the temperature
t, the momentum coefÔ¨Åcient mand queue size. We follow
the original settings from MoCo [50] and these parameters
may not be optimal as they are designed for the task of
image classiÔ¨Åcation in computer vision. Due to that, the pre-
training process is time-consuming and resource-consuming.
We need nearly 2 days to complete one training process
hence we ignore the hyper-parameter tuning process. However,
we also Ô¨Ånd that even with the original parameters used in
MoCo [50], ContraBERT still achieves higher performance
than the original models. The second threat is that we use
the same train-validation-test split that CodeXGLUE [3] and
GraphCodeBERT [30] used. Adjusting the data split ratio
or improving the training data quality may produce better
results, however, we do not take these strategies to ensure
a fair evaluation. The third threat is that we just use clone
detection(POJ-104) to verify the robustness of the model is
enhanced in Fig. 1 and Section V-A, we also plot the learnt
space in Section V-B. The reason to select clone detection is
that it aims at identifying the semantically equivalent programs
from other distractors, which is suitable for the evaluation.
External validity: Some other pre-training works in the code
scenario such as CuBERT [28] are not included for evaluation.
CuBERT was pre-trained on a large Python corpus with MLM.
Our approach is orthogonal to these pre-trained models and
we just need to replace the encoder M with other existing
pre-trained models for evaluation.
VII. R ELATED WORK
In this section, we brieÔ¨Çy introduce the related works on
contrastive learning, the pre-trained models for ‚Äúbig code‚Äù and
the adversarial robustness of models of code.
A. Contrastive Learning
Contrastive learning is to learn representations by minimiz-
ing the distance between similar samples while maximizing
the distance between different samples to help the similar
samples closer to each other and different samples far apart
from each other. Over the past few years, it has attracted
increasing attention with many successful applications in
computer vision [50], [61], [62], [63], [64], natural language
processing [65], [66], [67], [68]. Recently, there are some
works [44], [69], [45] that utilize contrastive learning for
different software engineering tasks. For example, Bui et
al. [44] proposed Corder, a contrastive learning approach for
code-to-code retrieval, text-to-code retrieval and code-to-text
summarization. VarCLR [69] aimed to learn the semantic
representations of variable names based on contrastive learn-
ing for different downstream tasks such as variable simi-
larity scoring and variable spelling error correction. Contra-
Code [45] generated variants by a source-to-source compiler
on JavaScript and further combined these generated mutatedsamples with contrastive learning for the task of clone detec-
tion, type inference and code summarization. Compared with
these existing works which only focus on designing mutated
variants for code, we Ô¨Årst illustrate the widely concerned
CodeBERT and GraphCodeBERT are weak to the adversarial
examples. Then we design a set of simple and complex
augmented operators on both programs and natural language
sequences to obtain different variants. By contrastive learning
to learn semantically equivalent variants, the robustness of
existing pre-trained models is enhanced. We further conÔ¨Årm
that the robustness-enhanced models provide improvements on
different downstream tasks.
B. Pre-trained Models for ‚ÄúBig Code‚Äù
Recently, pre-trained models are widely applied to the
‚Äúbig code‚Äù era [28], [29], [30], [3], [31], [32], [33], [34],
[35], [45], [70]. For example, Kanade et al. [28] pre-trained
CuBERT based on BERT [37] with a massive corpus of
Python programs from GitHub and then Ô¨Åne-tuned it for
some classiÔ¨Åcation tasks such as variable misuse classiÔ¨Åcation.
Feng et al. [29] proposed CodeBERT, a bimodal pre-trained
model for programming language (PL) and natural language
(NL) that learns the program representation to support code
search and source code summarization. GraphcodeBERT [30]
combines the variable data-Ô¨Çow graph in a program with
the code sequences and the natural language sequence to
enhance CodeBERT. CodeXGLUE [3] also utilized Code-
BERT and CodeGPT [71] to release a benchmark including
several software engineering tasks. Liu et al. [70] proposed
a CommitBART to support commit-related downstream tasks.
Compared with existing pre-trained models, we illustrate they
are not robust and further propose ContraBERT to enhance
model robustness.
C. Adversarial Robustness on Models of Code
The research about adversarial robustness analysis on the
models of code has attracted the attention [72], [73], [74],
[40], [75], [76]. Generally, these works can be categorized
into two groups: white-box and black-box manner, where the
white-box means that the approach provides some explanations
on the decision-making while the black-box mainly focuses
on the statistical evaluation. In terms of white-box works,
Yefet et al. [73] proposed DAMP to select the semantic
preserving perturbations by deriving the output distribution
of the model with the input. Srikant et al. [72] provided a
general formulation of a perturbed program that models site
locations and perturbation choices for each location. Then
based on this formulation, they further proposed a set of Ô¨Årst-
order optimization algorithms for the solving. In terms of the
black-box works, HMH [76] generated adversarial examples of
the source code by conducting iterative identiÔ¨Åer renaming and
evaluated on source code functionality classiÔ¨Åcation task. The
latest work by Yang et al. [40] proposed ALERT to transform
the inputs while preserving the optional semantics of original
inputs by replacing the variables with the substitutes. Their

--- PAGE 11 ---
experiments are conducted on the pre-trained models Code-
BERT and GraphCodeBERT. Compared with ALERT, which
only designed the rename variable operation, in this paper,
apart from the rename variable operation, we further design
eight augmented operators over PL-NL pairs. Furthermore, a
newly designed model to solve the weakness of robustness is
not involved in ALERT. In contrast, we propose our general
network architecture that uses contrastive learning to enhance
model robustness. The extensive experiments have conÔ¨Årmed
that our approach enhances the robustness of existing pre-
trained models. We also demonstrate that these robustness-
enhanced models provide improvements on different down-
stream tasks.
VIII. C ONCLUSION
In this paper, we observe that state-of-the-art pre-trained
models such as CodeBERT and GraphCodeBERT are not ro-
bust to adversarial attacks and a simple mutation operator (e.g.,
variable renaming) degrades their performance signiÔ¨Åcantly.
To address this problem, in this paper, we propose Contra-
BERT, a contrastive learning-based framework to enhance the
robustness of existing pre-trained models by designing nine
kinds of PL-NL augmented operators to group the semantically
equivalent variants. Through extensive experiments, we have
conÔ¨Årmed that the model‚Äôs robustness is enhanced. Further-
more, we also illustrate that these robustness-enhanced models
provide improvements on four downstream tasks.
IX. ACKNOWLEDGMENTS
We express our sincere gratitude to Mr Daya Guo from Sun
Yat-sen University for his assistance. This research is partially
supported by the National Research Foundation, Singapore
under its the AI Singapore Programme (AISG2-RP-2020-019),
the National Research Foundation, Prime Ministers OfÔ¨Åce,
Singapore under its National Cybersecurity R&D Program
(Award No. NRF2018NCR-NCR005-0001), NRF Investigator-
ship NRF-NRFI06-2020-0001, the National Research Founda-
tion through its National Satellite of Excellence in Trustworthy
Software Systems (NSOE-TSS) project under the National
Cybersecurity R&D (NCR) Grant award no. NRF2018NCR-
NSOE003-0001, the Ministry of Education, Singapore under
its Academic Research Tier 3 (MOET32020-0004). IIE au-
thors are supported in part by NSFC (61902395), Beijing
Nova Program. Any opinions, Ô¨Åndings and conclusions or
recommendations expressed in this material are those of the
author(s) and do not reÔ¨Çect the views of the Ministry of
Education, Singapore.
REFERENCES
[1] M. Allamanis, E. T. Barr, P. Devanbu, and C. Sutton, ‚ÄúA survey
of machine learning for big code and naturalness,‚Äù ACM Computing
Surveys (CSUR) , vol. 51, no. 4, pp. 1‚Äì37, 2018.
[2] ‚ÄúThe github blog,‚Äù https://github.blog/2018-11-08-100m-repos/.
[3] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,
C. Clement, D. Drain, D. Jiang, D. Tang et al. , ‚ÄúCodexglue: A machine
learning benchmark dataset for code understanding and generation,‚Äù
arXiv preprint arXiv:2102.04664 , 2021.
[4] M. Allamanis, M. Brockschmidt, and M. Khademi, ‚ÄúLearning to repre-
sent programs with graphs,‚Äù arXiv preprint arXiv:1711.00740 , 2017.[5] Y . Zhou, S. Liu, J. Siow, X. Du, and Y . Liu, ‚ÄúDevign: Effective vulner-
ability identiÔ¨Åcation by learning comprehensive program semantics via
graph neural networks,‚Äù arXiv preprint arXiv:1909.03496 , 2019.
[6] M. Allamanis, H. Jackson-Flux, and M. Brockschmidt, ‚ÄúSelf-supervised
bug detection and repair,‚Äù arXiv preprint arXiv:2105.12787 , 2021.
[7] S. Liu, Y . Chen, X. Xie, J. K. Siow, and Y . Liu, ‚ÄúRetrieval-augmented
generation for code summarization via hybrid gnn,‚Äù in International
Conference on Learning Representations , 2020.
[8] U. Alon, S. Brody, O. Levy, and E. Yahav, ‚Äúcode2seq: Generating
sequences from structured representations of code,‚Äù arXiv preprint
arXiv:1808.01400 , 2018.
[9] U. Alon, M. Zilberstein, O. Levy, and E. Yahav, ‚Äúcode2vec: Learning
distributed representations of code,‚Äù Proceedings of the ACM on Pro-
gramming Languages , vol. 3, no. POPL, pp. 1‚Äì29, 2019.
[10] S. Liu, X. Xie, L. Ma, J. Siow, and Y . Liu, ‚ÄúGraphsearchnet: Enhancing
gnns via capturing global dependency for semantic code search,‚Äù arXiv
preprint arXiv:2111.02671 , 2021.
[11] X. Gu, H. Zhang, and S. Kim, ‚ÄúDeep code search,‚Äù in 2018 IEEE/ACM
40th International Conference on Software Engineering (ICSE) . IEEE,
2018, pp. 933‚Äì944.
[12] A. Svyatkovskiy, S. Lee, A. HadjitoÔ¨Å, M. Riechert, J. V . Franco, and
M. Allamanis, ‚ÄúFast and memory-efÔ¨Åcient neural code completion,‚Äù in
2021 IEEE/ACM 18th International Conference on Mining Software
Repositories (MSR) . IEEE, 2021, pp. 329‚Äì340.
[13] U. Alon, R. Sadaka, O. Levy, and E. Yahav, ‚ÄúStructural language models
of code,‚Äù in International Conference on Machine Learning . PMLR,
2020, pp. 245‚Äì256.
[14] F. Liu, G. Li, Y . Zhao, and Z. Jin, ‚ÄúMulti-task learning based pre-
trained language model for code completion,‚Äù in Proceedings of the
35th IEEE/ACM International Conference on Automated Software En-
gineering , 2020, pp. 473‚Äì485.
[15] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, ‚ÄúSummarizing source
code using a neural attention model,‚Äù in Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , 2016, pp. 2073‚Äì2083.
[16] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
‚ÄúCodesearchnet challenge: Evaluating the state of semantic code search,‚Äù
arXiv preprint arXiv:1909.09436 , 2019.
[17] R. Russell, L. Kim, L. Hamilton, T. Lazovich, J. Harer, O. Ozdemir,
P. Ellingwood, and M. McConley, ‚ÄúAutomated vulnerability detection
in source code using deep representation learning,‚Äù in 2018 17th
IEEE international conference on machine learning and applications
(ICMLA) . IEEE, 2018, pp. 757‚Äì762.
[18] A. V . M. Barone and R. Sennrich, ‚ÄúA parallel corpus of python functions
and documentation strings for automated code documentation and code
generation,‚Äù arXiv preprint arXiv:1707.02275 , 2017.
[19] Y . Zhou, J. K. Siow, C. Wang, S. Liu, and Y . Liu, ‚ÄúSpi: Automated
identiÔ¨Åcation of security patches via commits,‚Äù ACM Transactions on
Software Engineering and Methodology (TOSEM) , vol. 31, no. 1, pp.
1‚Äì27, 2021.
[20] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural
computation , vol. 9, no. 8, pp. 1735‚Äì1780, 1997.
[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúImagenet classiÔ¨Åcation
with deep convolutional neural networks,‚Äù Advances in neural informa-
tion processing systems , vol. 25, pp. 1097‚Äì1105, 2012.
[22] S. Liu, C. Gao, S. Chen, N. L. Yiu, and Y . Liu, ‚ÄúAtom: Commit message
generation based on abstract syntax tree and hybrid ranking,‚Äù IEEE
Transactions on Software Engineering , 2020.
[23] B. Wu, S. Liu, R. Feng, X. Xie, J. Siow, and S.-W. Lin, ‚ÄúEnhancing
security patch identiÔ¨Åcation by capturing structures in commits,‚Äù IEEE
Transactions on Dependable and Secure Computing , 2022.
[24] X. Li, S. Liu, R. Feng, G. Meng, X. Xie, K. Chen, and Y . Liu,
‚ÄúTransrepair: Context-aware program repair for compilation errors,‚Äù
arXiv preprint arXiv:2210.03986 , 2022.
[25] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck, ‚ÄúModeling and discover-
ing vulnerabilities with code property graphs,‚Äù in 2014 IEEE Symposium
on Security and Privacy . IEEE, 2014, pp. 590‚Äì604.
[26] Y . Li, D. Tarlow, M. Brockschmidt, and R. Zemel, ‚ÄúGated graph
sequence neural networks,‚Äù arXiv preprint arXiv:1511.05493 , 2015.
[27] M. Allamanis, ‚ÄúThe adverse effects of code duplication in machine
learning models of code,‚Äù in Proceedings of the 2019 ACM SIGPLAN
International Symposium on New Ideas, New Paradigms, and ReÔ¨Çections
on Programming and Software , 2019, pp. 143‚Äì153.

--- PAGE 12 ---
[28] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, ‚ÄúPre-trained
contextual embedding of source code,‚Äù 2019.
[29] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang et al. , ‚ÄúCodebert: A pre-trained model for programming
and natural languages,‚Äù arXiv preprint arXiv:2002.08155 , 2020.
[30] D. Guo, S. Ren, S. Lu, Z. Feng, D. Tang, S. Liu, L. Zhou, N. Duan,
A. Svyatkovskiy, S. Fu et al. , ‚ÄúGraphcodebert: Pre-training code repre-
sentations with data Ô¨Çow,‚Äù arXiv preprint arXiv:2009.08366 , 2020.
[31] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, ‚ÄúIntellicode
compose: Code generation using transformer,‚Äù in Proceedings of the
28th ACM Joint Meeting on European Software Engineering Conference
and Symposium on the Foundations of Software Engineering , 2020, pp.
1433‚Äì1443.
[32] L. Buratti, S. Pujar, M. Bornea, S. McCarley, Y . Zheng, G. Rossiello,
A. Morari, J. Laredo, V . Thost, Y . Zhuang et al. , ‚ÄúExploring soft-
ware naturalness through neural language models,‚Äù arXiv preprint
arXiv:2006.12641 , 2020.
[33] R.-M. Karampatsis and C. Sutton, ‚ÄúScelmo: Source code embeddings
from language models,‚Äù arXiv preprint arXiv:2004.13214 , 2020.
[34] Y . Wang, W. Wang, S. Joty, and S. C. Hoi, ‚ÄúCodet5: IdentiÔ¨Åer-aware
uniÔ¨Åed pre-trained encoder-decoder models for code understanding and
generation,‚Äù arXiv preprint arXiv:2109.00859 , 2021.
[35] W. U. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, ‚ÄúUniÔ¨Åed
pre-training for program understanding and generation,‚Äù arXiv preprint
arXiv:2103.06333 , 2021.
[36] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, ‚ÄúLearning and
evaluating contextual embedding of source code,‚Äù in International
Conference on Machine Learning . PMLR, 2020, pp. 5110‚Äì5121.
[37] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‚ÄúBert: Pre-training
of deep bidirectional transformers for language understanding,‚Äù arXiv
preprint arXiv:1810.04805 , 2018.
[38] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
L. Zettlemoyer, and V . Stoyanov, ‚ÄúRoberta: A robustly optimized bert
pretraining approach,‚Äù arXiv preprint arXiv:1907.11692 , 2019.
[39] L. Mou, G. Li, L. Zhang, T. Wang, and Z. Jin, ‚ÄúConvolutional neural
networks over tree structures for programming language processing,‚Äù in
Thirtieth AAAI Conference on ArtiÔ¨Åcial Intelligence , 2016.
[40] Z. Yang, J. Shi, J. He, and D. Lo, ‚ÄúNatural attack for pre-trained models
of code,‚Äù arXiv preprint arXiv:2201.08698 , 2022.
[41] Authors, ‚ÄúContrabert: Enhancing code pre-trained models via contrastive
learning,‚Äù https://sites.google.com/view/contrabert.
[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in Advances
in neural information processing systems , 2017, pp. 5998‚Äì6008.
[43] J. Svajlenko, J. F. Islam, I. Keivanloo, C. K. Roy, and M. M. Mia,
‚ÄúTowards a big data curated benchmark of inter-project code clones,‚Äù
in2014 IEEE International Conference on Software Maintenance and
Evolution . IEEE, 2014, pp. 476‚Äì480.
[44] N. D. Bui, Y . Yu, and L. Jiang, ‚ÄúSelf-supervised contrastive learning for
code retrieval and summarization via semantic-preserving transforma-
tions,‚Äù in Proceedings of the 44th International ACM SIGIR Conference
on Research and Development in Information Retrieval , 2021, pp. 511‚Äì
521.
[45] P. Jain, A. Jain, T. Zhang, P. Abbeel, J. E. Gonzalez, and I. Stoica, ‚ÄúCon-
trastive code representation learning,‚Äù arXiv preprint arXiv:2007.04973 ,
2020.
[46] R. Sennrich, B. Haddow, and A. Birch, ‚ÄúImproving neural machine trans-
lation models with monolingual data,‚Äù arXiv preprint arXiv:1511.06709 ,
2015.
[47] E. Ma, ‚ÄúNlp augmentation,‚Äù https://github.com/makcedward/nlpaug,
2019.
[48] A. v. d. Oord, Y . Li, and O. Vinyals, ‚ÄúRepresentation learning with
contrastive predictive coding,‚Äù arXiv preprint arXiv:1807.03748 , 2018.
[49] Z. Wu, Y . Xiong, S. X. Yu, and D. Lin, ‚ÄúUnsupervised feature learning
via non-parametric instance discrimination,‚Äù in Proceedings of the IEEE
conference on computer vision and pattern recognition , 2018, pp. 3733‚Äì
3742.
[50] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, ‚ÄúMomentum contrast
for unsupervised visual representation learning,‚Äù in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2020, pp. 9729‚Äì9738.
[51] W. Wang, G. Li, B. Ma, X. Xia, and Z. Jin, ‚ÄúDetecting code clones with
graph neural network and Ô¨Çow-augmented abstract syntax tree,‚Äù in 2020IEEE 27th International Conference on Software Analysis, Evolution
and Reengineering (SANER) . IEEE, 2020, pp. 261‚Äì271.
[52] X. Chen, C. Liu, and D. Song, ‚ÄúTree-to-tree neural networks for program
translation,‚Äù arXiv preprint arXiv:1802.03691 , 2018.
[53] M.-A. Lachaux, B. Roziere, L. Chanussot, and G. Lample, ‚ÄúUn-
supervised translation of programming languages,‚Äù arXiv preprint
arXiv:2006.03511 , 2020.
[54] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and
D. Poshyvanyk, ‚ÄúAn empirical study on learning bug-Ô¨Åxing patches in
the wild via neural machine translation,‚Äù ACM Transactions on Software
Engineering and Methodology (TOSEM) , vol. 28, no. 4, pp. 1‚Äì29, 2019.
[55] Z. Li, D. Zou, S. Xu, X. Ou, H. Jin, S. Wang, Z. Deng, and Y . Zhong,
‚ÄúVuldeepecker: A deep learning-based system for vulnerability detec-
tion,‚Äù arXiv preprint arXiv:1801.01681 , 2018.
[56] D. Bahdanau, K. Cho, and Y . Bengio, ‚ÄúNeural machine translation by
jointly learning to align and translate,‚Äù arXiv preprint arXiv:1409.0473 ,
2014.
[57] J. Wang and J. Zhu, ‚ÄúPortfolio theory of information retrieval,‚Äù in Pro-
ceedings of the 32nd international ACM SIGIR conference on Research
and development in information retrieval , 2009, pp. 115‚Äì122.
[58] M. M. Palatucci, D. A. Pomerleau, G. E. Hinton, and T. Mitchell, ‚ÄúZero-
shot learning with semantic output codes,‚Äù 2009.
[59] L. Van der Maaten and G. Hinton, ‚ÄúVisualizing data using t-sne.‚Äù Journal
of machine learning research , vol. 9, no. 11, 2008.
[60] ‚ÄúK-means,‚Äù https://scikit-learn.org/stable/modules/generated/sklearn.
cluster.KMeans.html.
[61] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, ‚ÄúA simple framework
for contrastive learning of visual representations,‚Äù in International
conference on machine learning . PMLR, 2020, pp. 1597‚Äì1607.
[62] M. Kim, J. Tack, and S. J. Hwang, ‚ÄúAdversarial self-supervised con-
trastive learning,‚Äù arXiv preprint arXiv:2006.07589 , 2020.
[63] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark et al. , ‚ÄúLearning transfer-
able visual models from natural language supervision,‚Äù arXiv preprint
arXiv:2103.00020 , 2021.
[64] B. Dai and D. Lin, ‚ÄúContrastive learning for image captioning,‚Äù arXiv
preprint arXiv:1710.02534 , 2017.
[65] Z. Yang, Y . Cheng, Y . Liu, and M. Sun, ‚ÄúReducing word omission errors
in neural machine translation: A contrastive learning approach,‚Äù 2019.
[66] H. Fang, S. Wang, M. Zhou, J. Ding, and P. Xie, ‚ÄúCert: Contrastive
self-supervised learning for language understanding,‚Äù arXiv preprint
arXiv:2005.12766 , 2020.
[67] T. Gao, X. Yao, and D. Chen, ‚ÄúSimcse: Simple contrastive learning of
sentence embeddings,‚Äù arXiv preprint arXiv:2104.08821 , 2021.
[68] D. Shen, M. Zheng, Y . Shen, Y . Qu, and W. Chen, ‚ÄúA simple but tough-
to-beat data augmentation approach for natural language understanding
and generation,‚Äù arXiv preprint arXiv:2009.13818 , 2020.
[69] Q. Chen, J. Lacomis, E. J. Schwartz, G. Neubig, B. Vasilescu, and
C. L. Goues, ‚ÄúVarclr: Variable semantic representation pre-training via
contrastive learning,‚Äù arXiv preprint arXiv:2112.02650 , 2021.
[70] S. Liu, Y . Li, and Y . Liu, ‚ÄúCommitbart: A large pre-trained model for
github commits,‚Äù arXiv preprint arXiv:2208.08100 , 2022.
[71] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al. ,
‚ÄúLanguage models are unsupervised multitask learners,‚Äù OpenAI blog ,
vol. 1, no. 8, p. 9, 2019.
[72] S. Srikant, S. Liu, T. Mitrovska, S. Chang, Q. Fan, G. Zhang, and U.-M.
O‚ÄôReilly, ‚ÄúGenerating adversarial computer programs using optimized
obfuscations,‚Äù arXiv preprint arXiv:2103.11882 , 2021.
[73] N. Yefet, U. Alon, and E. Yahav, ‚ÄúAdversarial examples for models of
code,‚Äù Proceedings of the ACM on Programming Languages , vol. 4, no.
OOPSLA, pp. 1‚Äì30, 2020.
[74] G. Ramakrishnan, J. Henkel, Z. Wang, A. Albarghouthi, S. Jha, and
T. Reps, ‚ÄúSemantic robustness of models of source code,‚Äù arXiv preprint
arXiv:2002.03043 , 2020.
[75] P. Bielik and M. Vechev, ‚ÄúAdversarial robustness for code,‚Äù in Interna-
tional Conference on Machine Learning . PMLR, 2020, pp. 896‚Äì907.
[76] H. Zhang, Z. Li, G. Li, L. Ma, Y . Liu, and Z. Jin, ‚ÄúGenerating adversarial
examples for holding robustness of source code processing models,‚Äù in
Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , vol. 34,
no. 01, 2020, pp. 1169‚Äì1176.
