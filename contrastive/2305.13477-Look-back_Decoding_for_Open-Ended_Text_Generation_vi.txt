# 2305.13477.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/contrastive/2305.13477.pdf
# Kích thước tệp: 749466 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Giải mã Look-back cho Sinh văn bản Mở
Nan Xu♢, Chunting Zhou♠, Asli Celikyilmaz♠, Xuezhe Ma♢
♢Đại học Nam California,♠Meta AI
♢{nanx,xuezhema}@usc.edu,♠{chuntinz,aslic}@meta.com
Tóm tắt
Cho một tiền tố (ngữ cảnh), sinh văn bản mở 
nhằm giải mã các văn bản mạch lạc, không
chuyển hướng đột ngột khỏi các chủ đề trước đó,
và giàu thông tin, không bị lặp lại không mong
muốn. Trong bài báo này, chúng tôi đề xuất
Look-back, một thuật toán giải mã cải tiến
tận dụng phân kỳ Kullback–Leibler để theo dõi
khoảng cách phân phối giữa các bước giải mã
hiện tại và lịch sử. Do đó Look-back có thể tự
động dự đoán cụm từ lặp lại tiềm năng và chuyển
hướng chủ đề, và loại bỏ các token có thể gây ra
các chế độ thất bại, hạn chế phân phối xác suất
token tiếp theo trong một khoảng cách hợp lý
với lịch sử. Chúng tôi thực hiện các thí nghiệm
giải mã trên tiếp tục tài liệu và sinh câu chuyện,
và chứng minh rằng Look-back có thể sinh ra
văn bản trôi chảy và mạch lạc hơn, vượt trội
hơn các phương pháp giải mã mạnh khác đáng
kể trong cả đánh giá tự động và con người¹.

1 Giới thiệu
Mặc dù thành công ấn tượng trong việc sinh ra
các câu trôi chảy và chính xác cho các tác vụ
entropy thấp như tóm tắt hoặc dịch thuật, các
mô hình ngôn ngữ quy mô lớn (LLMs) vẫn
gặp phải các vấn đề thoái hóa nghiêm trọng,
như lặp lại không mong muốn (Holtzman et al.,
2019) và chuyển hướng chủ đề không tự nhiên,
trong các thiết lập mở (Eikema và Aziz, 2020).
Sinh văn bản thần kinh mở nhằm sinh ra văn
bản mạch lạc và đa dạng từ LLMs, cho trước
tiền tố ngữ cảnh (Nadeem et al., 2020; Dhamala
et al., 2022), và đã sinh ra một loạt các ứng
dụng ngôn ngữ tự nhiên, bao gồm hoàn thành
văn bản ngữ cảnh (Radford et al., 2019), sinh
câu chuyện (Fan et al., 2018), và sinh đánh giá
(Cho et al., 2019).

Để giảm nhẹ vấn đề thoái hóa trong sinh văn
bản mở, một số kỹ thuật đã

¹Mã và tài nguyên có sẵn tại https://github.
com/xunannancy/LookBackDecoding .

[Hình 1: Độ tương tự tối đa của các trạng thái ẩn và
phân kỳ KL tối thiểu được chuẩn hóa giữa bước hiện
tại và lịch sử (a) hoặc tiền tố (b) từ GPT2 trên 1.000
phiên bản của WikiText-103. So với tiếp tục của con
người, (a): lặp lại có minKL nhỏ hơn nhiều nhưng
maxHidden cao không thể phân biệt với văn bản lịch
sử, (b): chuyển hướng chủ đề giả bằng cách chuyển
sang tiếp tục của phiên bản khác có minKL cao hơn
nhiều nhưng maxHidden cao tương tự với văn bản
tiền tố.]

xuất hiện trong những năm gần đây, có thể được
phân loại thành hai hướng: i) học tập cải tiến
đề xuất các mục tiêu học tập mới, ví dụ: huấn
luyện không khả năng (Welleck et al., 2019),
huấn luyện tương phản (Su et al., 2022) và hiệu
chuẩn khả năng chuỗi (Zhao et al., 2022), để
bù đắp cho sự thiếu hụt gốc rễ của Ước lượng
Khả năng Tối đa truyền thống (MLE)²; ii) giải
mã cải tiến khắc phục các thế hệ tẻ nhạt và lặp
lại trong tìm kiếm giải mã (Su et al., 2022; Li
et al., 2022), hoặc chống lại chuyển hướng chủ
đề trong các thủ tục lấy mẫu (Hewitt et al., 2022).

Trong công trình này, chúng tôi đề xuất một
thuật toán giải mã mới, được đặt tên là Look-back,
đặc biệt chú ý đến sự khác biệt phân phối xác
suất giữa văn bản tiếp tục và lịch sử. Không
như tìm kiếm tương phản (Su et al., 2022; Su
và Xu, 2022) sử dụng độ tương tự cosine giữa
biểu diễn ẩn, Look-back tận dụng phân kỳ
Kullback-Leibler (KL) để theo dõi khoảng cách
phân phối giữa các bước giải mã hiện tại và
lịch sử. Động lực chính của Look-back là

²Tương quan giữa xác suất chuỗi và chất lượng của
nó đối với các mô hình được huấn luyện MLE có thể
thấp (Liu et al., 2022).

arXiv:2305.13477v2 [cs.CL] 23 Oct 2023

--- TRANG 2 ---
phân kỳ KL định nghĩa một khoảng cách giữa các
phân phối xác suất của các bước giải mã, có thể
cho rằng phù hợp hơn với thực hành giải mã.
Như được hiển thị trong Hình 1 (a), khi thuật
toán tham lam lặp đi lặp lại xuất ra các câu đơn,
khoảng cách với phân phối token gần nhất trong
quá khứ giảm về 0. Ngoài ra, khi việc tiếp tục
chuyển sang một chủ đề khác trong Hình 1 (b),
khoảng cách phân phối của việc tiếp tục với tiền
tố đạt được mức độ cao hơn nhiều so với việc
tiếp tục của con người có liên quan đến chủ đề.
Dựa trên các quan sát trước đó của chúng tôi,
đối với việc sinh ra thông tin và mạch lạc, phân
phối xác suất không nên quá gần với lịch sử để
đảm bảo tính đa dạng, nhưng tương đối gần với
tiền tố để duy trì tính mạch lạc.

Thông qua thí nghiệm, thông qua hai tác vụ
sinh văn bản mở, bao gồm tiếp tục tài liệu và
sinh câu chuyện, chúng tôi chứng minh rằng
Look-back vượt trội hơn nhiều thuật toán giải
mã mở khác nhau dưới các quy mô khác nhau
của LLMs được huấn luyện trước (GPT2-XL
và OPT-6.7B) bằng cách tạo ra các văn bản
mạch lạc hơn nhiều – điểm mauve cao so với
việc tiếp tục của con người và điểm tương tự
cao được đo so với tiền tố, trong khi duy trì
mức độ đa dạng tương tự.

2 Công trình Liên quan

Thuật toán Học tập Cải tiến Yang et al.
(2018); Adiwardana et al. (2020) quan sát thấy
rằng việc tăng số lượng ứng viên trong tìm kiếm
chùm tia hoặc lấy mẫu dẫn đến chất lượng tồi
tệ hơn của dữ liệu được sinh ra. Họ quy cho
điều này cho mục tiêu huấn luyện chủ đạo (tức
là Ước lượng Khả năng Tối đa) có thể không
xếp hạng chính xác các chuỗi được sinh ra theo
chất lượng (Zhao et al., 2022). Ngoài ra, Holtzman
et al. (2019) phát hiện rằng việc tìm kiếm các
chuỗi có khả năng xảy ra luôn dẫn đến các văn
bản ngắn và lặp lại, điều này đã thúc đẩy các
nỗ lực gần đây để cải thiện việc sinh ra thông
qua các mục tiêu học tập được sửa đổi. Welleck
et al. (2019) đề xuất huấn luyện không khả năng
để buộc các thế hệ không khả năng được gán
xác suất thấp hơn bởi mô hình. Để giảm nhẹ
thoái hóa, SimCTG (Su et al., 2022) giới thiệu
một mục tiêu huấn luyện tương phản để bảo tồn
tính thưa thớt của ma trận tương tự token của
văn bản được sinh ra. Để tránh việc vô tình tăng
cường xác suất của các token không liên quan
khác trong huấn luyện không khả năng, Jiang
et al. (2022) tận dụng học tập token tương phản
để dạy rõ ràng LLM gán các token âm tính với
xác suất thấp hơn các token tích cực thông qua
sự tương phản tập trung hơn giữa hai bên. Dựa
trên một số liệu tương tự kiểu BERTScore giữa
các giải mã mô hình và mục tiêu được đo trong
không gian tiềm ẩn của mô hình, Zhao et al.
(2022) đã hiệu chuẩn các chuỗi được sinh ra
bởi mô hình với hiệu chuẩn khả năng chuỗi để
phù hợp tốt hơn với các chuỗi tham chiếu thông
qua các loại mất mát khác nhau (ví dụ: mất mát
xếp hạng và biên).

Thuật toán Giải mã Cải tiến Liu et al. (2022)
quan sát thấy rằng các phương pháp tìm kiếm
(ví dụ: tham lam và chùm tia) tối ưu hóa xác
suất sinh ra có thể dẫn đến các kết quả đầu ra
tẻ nhạt và lặp lại trong sinh văn bản mở. Su
et al. (2022) bổ sung huấn luyện tương phản
với tìm kiếm tương phản cho giải mã, chọn lọc
các token có thể phân biệt hơn từ ngữ cảnh trước
đó. Li et al. (2022) quan sát thấy rằng thoái hóa
phổ biến hơn trong các LM lớn hơn so với các
LM nhỏ hơn, và đề xuất giải mã tương phản để
loại bỏ những hành vi không mong muốn này
bằng cách tách ra hành vi của LM nhỏ hơn khỏi
LM lớn hơn. Mặt khác, các phương pháp lấy
mẫu cắt ngắn như nucleus (Holtzman et al., 2019)
và typical (Meister et al., 2022) giải mã cải
thiện chất lượng mẫu với các mẫu đa dạng hơn
so với lấy mẫu trực tiếp, nhưng với cái giá của
tính mạch lạc kém và chuyển hướng chủ đề không
mong muốn. Hewitt et al. (2022) giới thiệu
η-sampling để cắt ngắn các từ dưới ngưỡng xác
suất phụ thuộc entropy. Một công trình đồng
thời quan sát mối tương quan mạnh giữa chất
lượng sinh ra tốt và vùng entropy hẹp, do đó
đề xuất giải mã nhận thức entropy để thúc đẩy
sinh ra tốt bằng cách hạn chế giải mã tham lam
vào vùng entropy hẹp (Arora et al., 2023).

Không cần nỗ lực thêm trong việc tinh chỉnh
LMs, Look-back được đề xuất cải thiện phương
pháp tìm kiếm thông thường với tham chiếu từ
tiền tố đã cho và sinh ra trước đó, để các lặp
lại không mong muốn và chuyển hướng chủ đề
có thể được giảm nhẹ rõ ràng.

3 Nền tảng

3.1 Sinh văn bản Mở

Cho một chuỗi m token được lấy mẫu từ văn
bản tự nhiên C={x₁...x_m} như ngữ cảnh hoặc
tiền tố, sinh văn bản thần kinh là để giải mã
một phần tiếp tục n-token sử dụng phân phối
xác suất được cung cấp bởi các LM được huấn
luyện trước:

p(x_{m+1:m+n}|C) = ∏_{t=1}^n P(x_t|C, x_{m+1}...x_{m+t-1}),

--- TRANG 3 ---
[Bảng 1: Các ví dụ thoái hóa với các thuật toán giải mã điển hình bởi GPT2-XL và GPT3 (ada-001 và davinci-002). Lặp lại câu hoàn chỉnh (S1), lặp lại với thay đổi vị trí nhỏ (S2) hoặc nhân đôi đoạn văn (S5) được đánh dấu màu xanh lá, trong khi chuyển hướng chủ đề không tự nhiên (S3&S4) hoặc cứng nhắc (S5) được đánh dấu màu hồng.]

nơi việc tiếp tục được sinh ra từng token một
sử dụng một chiến lược giải mã cụ thể. Ví dụ,
thuật toán tham lam chọn token tiếp theo cho
trước ngữ cảnh với xác suất cao nhất, trong khi
lấy mẫu nucleus (Holtzman et al., 2019) hạn
chế vùng hợp lý của các token với tổng khối
lượng trên một ngưỡng.

3.2 Các vấn đề Thoái hóa

Có hai vấn đề thoái hóa thường được quan sát
trong sinh văn bản mở: lặp lại và không mạch lạc.

Lặp lại LLMs có xu hướng đánh giá quá cao
xác suất của các chuỗi lặp lại (Welleck et al.,
2019) đặc biệt cho các thuật toán xác định như
tìm kiếm tham lam và chùm tia. Mặc dù các
thuật toán giải mã như lấy mẫu nucleus (Holtzman
et al., 2019) đã được đề xuất để ngắt các chuỗi
lặp lại, chúng ta vẫn có thể quan sát việc tiếp
tục lặp lại và tẻ nhạt ngay cả từ mô hình ngôn
ngữ GPT-3 hiện đại nhất (Brown et al., 2020),
như được hiển thị trong Bảng 1. Ngoài sự đồng
thuận rằng xác suất từ các LM có điều kiện
thường không xếp hạng chính xác các chuỗi
được sinh ra theo chất lượng (Zhao et al., 2022),
một nghiên cứu gần đây cung cấp một cách có
thể để giải thích việc sinh ra lặp lại với mô hình
sao chép chuỗi tương tự được quan sát: khớp
tiền tố và sao chép³ (Olsson et al., 2022).

Không mạch lạc Các thuật toán lấy mẫu hy
sinh tính mạch lạc để giảm nhẹ lặp lại trong
quá trình giải mã.

³Khớp tiền tố: cơ chế chú ý trong các LM dựa
trên transformer chú ý trở lại các token trước đó
đã được theo sau bởi các token hiện tại và/hoặc
gần đây. Sao chép: xuất ra logit tăng của token
được chú ý hoặc những token khác tương tự trong
không gian embedding.

Như được hiển thị trong Bảng 1, cho trước các
xác suất từ các mô hình GPT-3, lấy mẫu nucleus
thất bại trong việc tạo ra sinh ra mạch lạc, chuyển
chủ đề từ chứng khó tiêu cấp tính của Burkan
sang con đường của Shanny về nhà với ada-001
(S5). Các thuật toán giải mã gần đây phụ thuộc
vào độ tin cậy của mô hình để "đảm bảo" tính
mạch lạc trong khi giải quyết lặp lại một cách
rõ ràng với các phương pháp heuristic nhất định.
Ví dụ, SimCTG (Su et al., 2022) chọn từ các
ứng viên có khả năng nhất được dự đoán bởi
LM. Giải mã tương phản (Li et al., 2022) khai
thác tính chất mạch lạc của các LM chuyên gia.
Trong cả S3 và S4 từ Bảng 1, thật không may,
chúng tôi thấy rằng giả thuyết mạch lạc của
các LM được huấn luyện trước trong công trình
trước đó không phải lúc nào cũng đúng trong
thực tế: có khả năng tạo ra các câu không mạch
lạc khi các LM mạnh mẽ tuân thủ nghiêm ngặt
độ tin cậy của mô hình ở mỗi bước với thuật
toán tham lam.

4 Phương pháp Đề xuất: Look-back

Như được trình bày trong Thuật toán 1, Look-back
đầu tiên tận dụng khoảng cách phân phối xác
suất giữa các bước hiện tại và trước đó để tránh
lặp lại (§4.1), sau đó kết hợp tham chiếu từ
tiền tố đã cho để giảm thiểu chuyển hướng chủ
đề (§4.2).

4.1 Giảm nhẹ Lặp lại với Tham chiếu từ
Văn bản Trước đó

Tín hiệu cho Lặp lại Bề mặt hoặc Ngữ nghĩa
Trong quá trình giải mã của sinh văn bản mở,
một trong các token hợp lý được chọn/lấy mẫu
theo xác suất mô hình. Được truyền cảm hứng
bởi vai trò quyết định của phân phối xác suất,
chúng tôi điều tra việc đo khoảng cách giữa các
bước hiện tại và trước đó trong không gian phân
phối thông qua phân kỳ KL: D_KL(p_t||p'_t)
cho bất kỳ 1≤t'<t. Khi khoảng cách

--- TRANG 4 ---
[Hình 2: Khoảng cách phân phối xác suất của GPT2-XL được đo bằng phân kỳ KL cho việc tiếp tục lặp lại (a,b,c,d) và ngoài chủ đề (e) được trình bày trong Bảng 1. (a) và (b): Các ô tối dọc theo đường chéo chỉ ra rằng các bước có khoảng cách nhỏ với lịch sử có xu hướng tạo ra các token lặp lại. (c) và (d): So với việc tiếp tục của con người, khoảng cách phân phối tối thiểu với quá khứ dần tiến về 0 (đường cong đỏ) khi các cụm từ tương tự tiếp tục lặp lại trong quá trình giải mã. (e): phân phối của việc tiếp tục không mạch lạc (đường cong xanh lá và xanh dương) có xu hướng ở xa hơn từ tiền tố đã cho khi giải mã tiến hành.]

bản đồ nhiệt được hiển thị trong Hình 2a, cho
các bước sinh ra các token giống hệt nhau, các
phân phối xác suất tương ứng của chúng ở gần
nhau hơn so với những bước có kết quả đầu ra
khác biệt.

Lưu ý rằng cả mục tiêu huấn luyện tương phản
(SimCTG) (Su et al., 2022) và thuật toán giải
mã tìm kiếm tương phản của nó (Su và Xu, 2022)
đều không thể được áp dụng trực tiếp cho các
LLM như GPT3, nơi các trạng thái ẩn của nó
không thể truy cập được. May mắn thay, chúng
ta có thể trực tiếp phát hiện các lặp lại bề mặt
hoặc ngữ nghĩa từ GPT3 bằng cách phân tích
phân phối xác suất có sẵn: các cặp bước tạo ra
token giống hệt nhau hoặc các token chia sẻ
ý nghĩa ngữ nghĩa tương tự có thể phân biệt
được với khoảng cách phân phối. Lấy Hình 2b
làm ví dụ: các cặp token đầu ra từ các bước giải
mã với phân phối xác suất gần nhất là FAN thứ
1 và thứ 2, thành phố Munich và Frankfurt, vị
trí Olympic và Römerberg.

Khi các bước lặp lại có xu hướng ở cực kỳ gần
với các bước trước đó có kết quả đầu ra tương
tự trong không gian phân phối xác suất, chúng
tôi tính toán khoảng cách phân phối xác suất
giữa bước thứ t và bước trước đó gần nhất như
KL^t_min cho phân tích tiếp theo:

KL^t_min = min_{1≤j≤t-1} KL(p(·|x_{<t})||p(·|x_{<j}))

Như được chứng minh trong Hình 2c và Hình
2d, các giá trị của KL^t_min trở nên phẳng khi
thoái hóa kiểu lặp lại tiến triển⁴.

Giảm nhẹ Lặp lại Vì mô hình lặp lại giống hệt
nhau hoặc tương tự có thể được dự báo thông
qua phân tích phân phối xác suất, Look-back
cố gắng tránh các câu hoặc cụm từ lặp lại trước
khi sinh ra thực tế. Thực tế, khi KL^t_min đã
dưới ngưỡng được xác định trước α, một cảnh
báo được kích hoạt và Look-back cố gắng lấy
mẫu một token từ k token có khả năng nhất
từ từ vựng V thay vì bám vào token top-1:

x_t ~ {
    Unif(V_k), nếu KL^t_min ≤ α
    argmax_{v∈V} p_θ(v|x_{<t}), Ngược lại
}

nơi V_k là tập hợp các token có khả năng cao
nhất từ từ vựng V. Để tránh các trường hợp
dương tính giả nơi một bước được xác định
với khả năng cao để lặp lại có thể không nhất
thiết dẫn đến lặp lại không mong muốn, chúng
tôi có chủ ý không loại trừ token có khả năng
nhất từ tập hợp ứng viên hợp lý.

⁴Các đỉnh trong Hình 2d ở các bước giải mã sau
này tương ứng với nhiều token để biểu diễn một vị
trí duy nhất, ví dụ: ö, mer, berg cho Römerberg trong
Hình 2b.

[Thuật toán 1 Giải mã Look-back]

4.2 Cải thiện Tính mạch lạc với Tham chiếu
từ Tiền tố Đã cho

Tín hiệu cho Chuyển hướng Chủ đề Trong sinh
ra mở, để tạo ra các câu mạch lạc với tiền tố
đã cho, thuật toán giải mã được yêu cầu cung
cấp sự elaboration thêm về chủ đề chính được
truyền đạt trong tiền tố. Theo các quan sát trước
đó (ví dụ: Munich và Frankfurt trong Hình 2b),
các bước giải mã với các token chia sẻ ý nghĩa
ngữ nghĩa tương tự gần nhau đối với khoảng
cách phân phối xác suất. Do đó, chúng tôi khám
phá phân kỳ KL giữa m bước hiện tại và tiền
tố nên giữ cùng chủ đề:

KL^{t|C}_min = min_{1≤j≤m} KL(p(·|x_{<t})||p(·|x_{<j}))

Khi so sánh khoảng cách phân phối của sinh
ra không mạch lạc với việc tiếp tục tự nhiên
với cùng tiền tố, phân kỳ phân phối xác suất
duy trì mức độ cao hơn nhiều cho sinh ra với
chuyển hướng chủ đề rõ ràng, như được hiển
thị trong Hình 2e.

Cải thiện Tính mạch lạc Khi mô hình có xu
hướng cung cấp các token lặp lại, một giải pháp
đơn giản để tránh lặp lại là lấy mẫu ngẫu nhiên
từ k token hợp lý nhất. Nó có khả năng dẫn đến
chuyển hướng chủ đề không tự nhiên do tích
lũy các lựa chọn lấy mẫu không mong muốn
trong quá trình giải mã chuỗi dài, điều này
thường được quan sát trong các thuật toán lấy
mẫu (Eikema và Aziz, 2020; Maynez et al., 2020).
Mặt khác, khoảng cách phân phối xác suất giữa
hiện tại và tiền tố có thể phân biệt việc sinh ra
có đúng chủ đề hay không. Do đó, Look-back
khôn ngoan lấy mẫu từ các ứng viên hợp lý
theo ảnh hưởng của chúng lên tính mạch lạc
được phản ánh bởi khoảng cách phân phối bước
tiếp theo với tiền tố:

KL^{t+1,v|C}_min = min_{1≤j≤m} KL(p(·|x_{<t+1}, v)||p(·|x_{<j}))

x_t ~ {
    softmax(-KL^{t+1,v|C}_min), nếu KL^t_min ≤ α
    argmax_{v∈V} p_θ(v|x_{<t}), Ngược lại
}

nơi các token với khoảng cách bước tiếp theo
lớn hơn với tiền tố ít có khả năng được lấy mẫu
hơn do hoạt động softmax trên phân kỳ KL.

5 Thí nghiệm

Trong phần này, chúng tôi đầu tiên giới thiệu
các bộ dữ liệu (§5.1) và các thước đo tự động
(§5.2) được sử dụng để đánh giá chất lượng
sinh ra của Look-back được đề xuất và các
baseline giải mã mạnh khác (§5.3). Sau đó
chúng tôi phân tích kết quả thí nghiệm được
đánh giá bởi các thước đo tự động (§5.5) và
các đánh giá viên con người (§5.6). Cuối cùng,
chúng tôi cho thấy hiệu quả của các kỹ thuật
khác nhau được sử dụng trong Look-back thông
qua các phân tích chi tiết (§5.7).

5.1 Bộ dữ liệu

Chúng tôi xem xét hai ứng dụng của sinh văn
bản mở: 1) tiếp tục tài liệu trên WikiText-103
với các bài báo phù hợp với tiêu chí bài báo
Tốt hoặc Nổi bật được chỉ định bởi các biên
tập viên trên Wikipedia (Merity et al., 2016),
và 2) sinh câu chuyện trên WritingPrompts,
đây là một tác vụ thách thức cho việc tiếp tục
truyền cảm hứng với các lời nhắc câu chuyện
trừu tượng, cấp cao được gửi bởi người dùng
trực tuyến và các phần tiếp tục được phản hồi
bởi những người khác một cách tự do trên Reddit
(Fan et al., 2018).

5.2 Thước đo Đánh giá

Chúng tôi áp dụng các thước đo tự động sau
để đánh giá chất lượng sinh ra:

Lặp lại Chúng tôi sử dụng rep-n để đo lặp lại
cấp độ chuỗi theo tỷ lệ n-gram trùng lặp (Welleck
et al., 2019). Cho một chuỗi x, rep-n = 1.0 -
|unique n-grams(x)| / |total n-grams(x)|.

Đa dạng Theo (Su et al., 2022), chúng tôi thu
được một đánh giá tổng thể về lặp lại mô hình
bằng cách xem xét lặp lại ở các mức n-gram
khác nhau: diversity = ∏^4_{n=2}(1.0-rep-n).

--- TRANG 5 ---
[Bảng 2: Kết quả đánh giá tự động của các thuật toán giải mã khác nhau cho tiếp tục tài liệu và sinh câu chuyện. Tiếp tục được sinh ra bởi Look-back có mức độ đa dạng tương tự như văn bản con người trong khi liên quan hơn nhiều đến tiền tố (coherence cao nhất) và tương tự về mặt ngữ nghĩa với việc tiếp tục của con người (MAUVE cao nhất).]

MAUVE Bằng cách tính toán các phân kỳ thông
tin trong một không gian embedding được lượng
tử hóa⁵, MAUVE (Pillutla et al., 2021) trực tiếp
so sánh phân phối đã học từ một mô hình sinh
văn bản với phân phối của việc tiếp tục được
viết bởi con người.

Tính mạch lạc Tính mạch lạc ngữ nghĩa giữa
tiền tố và tiếp tục được đo như độ tương tự
cosine giữa các embedding câu của chúng được
biểu diễn bởi SimCSE (Gao et al., 2021).

Kết quả được đo bằng tất cả các thước đo dao
động từ 0 đến 1, và điểm cao hơn chỉ ra sinh
ra tốt hơn trừ rep-n, với điểm thấp hơn thì tốt hơn.

5.3 Baseline Giải mã

Cho trước các LM được huấn luyện trước với
MLE thông thường, chúng tôi đánh giá Look-back
cùng với các thuật toán giải mã khác nhau để
so sánh công bằng.

Phương pháp Tìm kiếm Chúng tôi xem xét
tìm kiếm tương phản cạnh tranh được đề xuất
trong SimCTG (Su et al., 2022) dự đoán token
tiếp theo dựa trên cả phân phối đầu ra và độ
tương tự biểu diễn giữa các ứng viên và các
token trong quá khứ⁶.

Phương pháp Lấy mẫu Lấy mẫu Nucleus (Holtzman
et al., 2019) lấy mẫu token tiếp theo từ phần
top-p của khối lượng xác suất. Giải mã Typical
(Meister et al., 2022) lấy mẫu từ tập hợp các
từ có log-xác suất âm gần với entropy có điều
kiện. η-sampling (Hewitt et al., 2022) cắt ngắn
bất kỳ từ nào có xác suất nhỏ hơn ngưỡng dựa
trên entropy.

⁵Chúng tôi sử dụng GPT2-XL cho embedding chuỗi văn bản.
⁶Chúng tôi bỏ qua tìm kiếm tham lam và chùm tia vì
chúng tiếp tục tạo ra các cụm từ/câu lặp lại trong các
nghiên cứu trước đó (Welleck et al., 2019; Holtzman
et al., 2019).

5.4 Chi tiết Triển khai

Chúng tôi lấy mẫu ngẫu nhiên 1.000 phiên bản
từ dữ liệu huấn luyện gốc của WikiText-103
và WritingPrompts làm tập validation và test
của chúng tôi. Cho trước vài token đầu tiên
như tiền tố⁷, chúng tôi sinh ra 256 token với
các thuật toán giải mã khác nhau và bỏ qua
những token sau token kết thúc văn bản trong
quá trình đánh giá. Thực tế, chúng tôi xem xét
một cửa sổ trượt bao gồm 128 token trước đó
để tránh lặp lại không mong muốn trong khi
cho phép lặp lại cần thiết của văn bản xa từ
bước giải mã hiện tại. Chúng tôi thực hiện thí
nghiệm với các LM được huấn luyện trước từ
các họ và quy mô khác nhau: GPT2-XL (Radford
et al., 2019) và OPT-6.7B (Zhang et al., 2022).

Cùng một bộ siêu tham số được sử dụng để
giải mã từ các LM khác nhau: kích thước chùm
tia cho tìm kiếm chùm tia là 10, p = 0.95 cho
nucleus, τ = 0.92 cho typical, và η = 0.0003
cho η-sampling. Chúng tôi tuân theo phạm vi
được khuyến nghị cho k={5,8,10} và α=[0.5,0.9]
trong SimCTG và chọn tập hợp dựa trên điểm
MAUVE của chúng trên tập validation. Cho
Look-back, phạm vi số lượng ứng viên k là
{5,8,10} và ngưỡng α dao động từ [0.5,1.6].
Chúng tôi chọn các siêu tham số dẫn đến điểm
rep-2 gần nhất với con người và hiệu suất
MAUVE tối ưu trên tập validation.

5.5 Kết quả

Trong Bảng 2, chúng tôi hiển thị hiệu suất của
các thuật toán giải mã khác nhau cũng như việc
tiếp tục tự nhiên của con người được đánh giá
bằng các thước đo tự động. Trên cả hai bộ dữ
liệu, Look-back luôn đạt được điểm MAUVE
và coherence cao nhất, điều này chỉ ra rằng
việc sinh ra của Look-back có sự gần gũi phân
phối token với các phần tiếp tục của con người
trong khi vẫn liên quan đến các tiền tố đã cho.
Trong khi đó, Look-back có khả năng tạo ra
các văn bản với mức độ lặp lại và đa dạng tương
tự như văn bản tự nhiên của con người, điều
này ngụ ý tính trôi chảy và giàu thông tin của
văn bản được sinh ra. Chúng tôi cũng chú ý
rằng các thế hệ từ tất cả các thuật toán giải mã
đạt được điểm MAUVE và coherence tương
đối thấp trên WritingPrompts. Điều này là do
các tiền tố đã cho là trừu tượng và các tham
chiếu được viết bởi con người đa dạng và biến
đổi, dẫn đến coherence và MAUVE thấp đối
với các phần tiếp tục mô hình khác nhau.

⁷32 token đầu tiên được sử dụng như tiền tố cho
WikiText-103, trong khi các lời nhắc gốc được sử
dụng cho WritingPrompts.

--- TRANG 6 ---
[Bảng 3: Đánh giá Con người về các thế hệ từ Look-back và SimCTG tốt thứ hai với các ví dụ được lấy mẫu từ WikiText-103. Tiếp tục được sinh ra bởi Look-back được các đánh giá viên con người ưa thích hơn SimCTG một cách đáng kể về cả tính trôi chảy và mạch lạc.]

5.6 Đánh giá Con người

Để đánh giá thêm chất lượng của các văn bản
được sinh ra, chúng tôi lấy mẫu ngẫu nhiên hai
bộ 50 ví dụ từ WikiText-103 để tạo ra các tiền
tố cho GPT2-XL và OPT-6.7B tương ứng và
sinh ra các phần tiếp tục từ chúng. Sau đó,
chúng tôi yêu cầu 3 đánh giá viên so sánh các
phần tiếp tục được sinh ra từ Look-back và
baseline tốt thứ hai SimCTG trong hai chiều:
1) tính trôi chảy: nội dung đa dạng và tự nhiên
không có từ, cụm từ hoặc câu lặp lại; 2) tính
mạch lạc: được tổ chức tốt và dễ theo dõi; nhất
quán với các chủ đề được trình bày trong tiền
tố được viết bởi con người mà không có chuyển
hướng chủ đề đột ngột. Chúng tôi yêu cầu các
chú thích viên chọn một trong ba tùy chọn:
phần tiếp tục thứ 1 tốt hơn, phần thứ 2 tốt hơn,
hoặc hai phần có chất lượng như nhau. Như
được trình bày trong Bảng 3, cho cả hai chiều
đánh giá, nội dung được sinh ra bởi Look-back
được ưa thích hoặc được đánh dấu là tốt như
nhau bởi các đánh giá viên khoảng hoặc hơn
70% thời gian so với baseline, điều này phù
hợp tốt với các thước đo tự động trong Bảng 2.

5.7 Phân tích Thêm

Trong phần này, chúng tôi phân tích hiệu quả
của các kỹ thuật khác nhau được sử dụng bởi
Look-back một cách riêng lẻ.

[Hình 3: Phân kỳ KL tối thiểu giữa bước hiện tại và (a) lịch sử hoặc (b) tiền tố từ GPT2-XL và OPT-6.7B được giải mã bằng các thuật toán khác nhau trên tập test của WikiText103 và WritingPrompts. Phân phối xác suất của Look-back giữ khoảng cách với lịch sử để tránh lặp lại nhưng ở gần tiền tố để đảm bảo tính mạch lạc.]

Phân tích Khoảng cách Phân phối Xác suất.
Để xác minh liệu giải mã với Look-back có thích
hợp hạn chế khoảng cách phân phối xác suất
với các bước trước đó hay không, chúng tôi so
sánh KL^t_min với lịch sử và KL^{t|C}_min với
tiền tố của thoái hóa và các thuật toán giải mã
khác nhau trong Hình 3. Mặc dù tất cả các thuật
toán giải mã cải tiến đều giữ khoảng cách với
phân phối xác suất lịch sử để tránh lặp lại so
với thuật toán tham lam (Repetitive trong cột
trái của Hình 3), phân phối xác suất của Look-back
(Look-back trong cột phải của Hình 3) gần
hơn nhiều với tiền tố đã cho, điều này phân biệt
nó khỏi việc tiếp tục ngoài chủ đề so với các
thuật toán khác.

--- TRANG 7 ---
[Bảng 4: Ảnh hưởng của lấy mẫu hướng dẫn phân phối xác suất của Look-back (Softmax) lên chất lượng sinh ra. Với mức độ nội dung đa dạng tương tự như văn bản con người, Look-back lấy mẫu theo softmax của khoảng cách phân phối âm với tiền tố, dẫn đến cải thiện tính mạch lạc so với Uniform.]

Softmax so với Uniform. Theo hoạt động softmax
trên KL^{t|C}_min được giới thiệu trong §4.2,
phân phối xác suất bước tiếp theo càng gần với
tiền tố, token hợp lý tương ứng càng có khả
năng được chọn để tránh chuyển hướng chủ đề
không mong muốn so với lấy mẫu ngẫu nhiên.
Trong Bảng 4, chúng tôi điều tra thực nghiệm
tác động của lấy mẫu token hợp lý, uniform so
với softmax, lên chất lượng sinh ra và thấy
Look-back cải thiện đáng kể tính mạch lạc trên
cả hai bộ dữ liệu so với lấy mẫu ngẫu nhiên.
Mặc dù tính đa dạng giảm với lấy mẫu hướng
dẫn khoảng cách phân phối trong Look-back,
cả hai chiến lược lấy mẫu đều tạo ra mức độ
nội dung đa dạng tương tự như các văn bản
con người được liệt kê trong Bảng 2.

Ảnh hưởng của Số lượng Ứng viên và Ngưỡng α.
Trong §4.1, siêu tham số α xác định liệu bước
hiện tại có khả năng tạo ra phần tiếp tục lặp lại
hay không trong khi k hạn chế phạm vi của các
ứng viên token hợp lý. Baseline tốt thứ hai
SimCTG có tham số số lượng ứng viên k tương
tự và α để cân bằng độ tin cậy mô hình và phạt
thoái hóa. Khi GPT2-XL được sử dụng để giải
mã với Look-back và SimCTG trên WikiText-103,
chúng tôi trực quan hóa tác động của các siêu
tham số lên chất lượng sinh ra trong Hình 4 và
Hình 5. α trong Look-back khác với α trong
SimCTG, nhưng cả hai đều kiểm soát sự phụ
thuộc vào độ tin cậy mô hình: α lớn hơn chỉ ra
token có khả năng nhất ít có khả năng được
áp dụng, do đó có được tính đa dạng hơn. Chúng
tôi cũng quan sát thấy rằng cho Look-back, sự
liên quan của văn bản được sinh ra với tiền tố
(coherence cao) và phần tiếp tục của con người
(MAUVE cao) mạnh mẽ hơn nhiều đối với các
giá trị siêu tham số khác nhau so với SimCTG.

[Hình 4: Tác động của các siêu tham số giải mã trên tập validation của WikiText-103. So với thuật toán tìm kiếm khác SimCTG (cột thứ 1), Look-back (cột thứ 2) giữ điểm MAUVE và coherence tương đối cao hơn bất kể số lượng token hợp lý k và ngưỡng KL^t_min α. Xem Hình 5 cho thêm kết quả trong các thiết lập khác.]

5.8 Nghiên cứu Trường hợp

Cho một tiền tố được lấy mẫu từ WikiText-103,
chúng tôi trình bày các phần tiếp tục của con
người bị cắt ngắn cũng như các thế hệ từ Look-back
và SimCTG trong Bảng 5 và để lại thêm ví dụ
trong Phụ lục Bảng 6. Tiền tố đang nói về thiết
kế của một trò chơi đua xe. Cả phần tiếp tục
của con người và Look-back đều tập trung vào
mô tả các khó khăn chính gặp phải trong quá
trình thiết kế trò chơi, trong khi SimCTG chuyển
sang một chủ đề khác bằng cách chỉ đến một
giới thiệu trực tuyến về trò chơi trong nửa sau
của phần tiếp tục. Thú vị thay, Look-back giải
thích cách nhóm hơn hai mươi người được hình
thành, điều này mạch lạc với chủ đề trong tiền tố.

--- TRANG 8 ---
[Bảng 5: Nghiên cứu trường hợp của một phiên bản được lấy mẫu từ WikiText-103 với GPT2-XL. Tiếp tục của cả con người và Look-back thảo luận về khó khăn trong thiết kế trò chơi, trong khi SimCTG dần tạo ra các câu ít thông tin hơn với chuyển hướng chủ đề nhẹ sang giới thiệu trò chơi (màu hồng). Tham khảo Bảng 6 cho thêm ví dụ.]

[Hình 5: (Tiếp tục từ Hình 4) Tác động của các siêu tham số giải mã trên tập validation của WikiText103 và WritingPrompts.]

6 Kết luận

Khoảng cách giữa các phân phối đầu ra báo hiệu
các chế độ thất bại tiềm năng của sinh văn bản,
như lặp lại tẻ nhạt và không mạch lạc. Chúng
tôi đề xuất Look-back, một thuật toán giải mã
mới sử dụng phân kỳ KL giữa các bước giải
mã hiện tại và lịch sử, để điều hướng phân phối
đầu ra vào một không gian con hợp lý. Look-back
có thể sinh ra văn bản chất lượng cao hơn và
vượt trội hơn một số thuật toán giải mã mạnh
trong cả đánh giá tự động và con người. Tuy
nhiên, phân kỳ KL có thể không phải là thước
đo tối ưu cho các phân phối đầu ra văn bản và
chúng tôi để lại việc điều tra các thước đo khác
cho công việc tương lai. Ngoài ra, ý tưởng được
đề xuất trong công trình này cũng có thể được
sử dụng cho các tình huống giải mã có ràng
buộc chuyên biệt khác, như ngăn chặn ảo giác.

Hạn chế

Chúng tôi thảo luận về các hạn chế của công
trình chúng tôi như sau:

• Look-back phạt các token tiếp theo dẫn đến
phân kỳ KL thấp với các phân phối đầu ra lịch
sử. Tuy nhiên, chúng ta không thể phân biệt
rõ ràng nếu những token như vậy là lặp lại tự
nhiên hay không mong muốn. Điều này có thể
dẫn đến việc loại bỏ tích cực các kết quả đầu
ra có thể. Chúng tôi để lại việc phân biệt các
lặp lại khác nhau cho công việc tương lai.

• Look-back có xu hướng hiển thị điểm lặp lại
bi-gram cao hơn các phương pháp giải mã khác
vì nó khuyến khích tính mạch lạc với văn bản
tiền tố ở mỗi bước giải mã. Vì chúng tôi sử
dụng văn bản tiền tố ngắn theo giao thức đánh
giá trước đó, có thể không đủ thông tin, chúng
tôi sẽ áp dụng thiết lập đánh giá toàn diện hơn
trong tương lai hoặc thêm văn bản liên quan
vào đầu tại thời điểm giải mã.

• Hầu hết các đánh giá của chúng tôi dựa vào
các thước đo tự động, như điểm MAUVE. Tuy
nhiên, chúng tôi thấy rằng những thước đo này
có thể không phản ánh trung thực chất lượng
của văn bản, ví dụ, điểm MAUVE nhạy cảm
với sự lựa chọn các mô hình embedding câu.
Nói chung, sinh văn bản mở vẫn đặt ra một
thách thức lớn cho việc phát triển các thuật
toán NLG.

--- TRANG 9 ---
Tài liệu Tham khảo

Daniel Adiwardana, Minh-Thang Luong, David R So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al. 2020. Towards a human-like open-domain chat-
bot. arXiv preprint arXiv:2001.09977.

Kushal Arora, Timothy J O'Donnell, Doina Precup, Ja-
son Weston, and Jackie CK Cheung. 2023. The stable
entropy hypothesis and entropy-aware decoding: An
analysis and algorithm for robust natural language
generation. arXiv preprint arXiv:2302.06784.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.

Woon Sang Cho, Pengchuan Zhang, Yizhe Zhang, Xiu-
jun Li, Michel Galley, Chris Brockett, Mengdi Wang,
and Jianfeng Gao. 2019. Towards coherent and co-
hesive long-form text generation. In Proceedings
of the First Workshop on Narrative Understanding,
pages 1–11, Minneapolis, Minnesota. Association for
Computational Linguistics.

Jwala Dhamala, Varun Kumar, Rahul Gupta, Kai-Wei
Chang, and Aram Galstyan. 2022. An analysis of
the effects of decoding algorithms on fairness in
open-ended language generation. arXiv preprint
arXiv:2210.03826.

Bryan Eikema and Wilker Aziz. 2020. Is map de-
coding all you need? the inadequacy of the mode
in neural machine translation. arXiv preprint
arXiv:2005.10283.

Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 889–898, Melbourne, Australia. Association
for Computational Linguistics.

Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence em-
beddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.

John Hewitt, Christopher D Manning, and Percy Liang.
2022. Truncation sampling as language model
desmoothing. arXiv preprint arXiv:2210.15191.

Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751.

Shaojie Jiang, Ruqing Zhang, Svitlana Vakulenko, and
Maarten de Rijke. 2022. A simple contrastive learn-
ing objective for alleviating neural text degeneration.
arXiv preprint arXiv:2205.02517.

Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang,
Jason Eisner, Tatsunori Hashimoto, Luke Zettle-
moyer, and Mike Lewis. 2022. Contrastive decoding:
Open-ended text generation as optimization. arXiv
preprint arXiv:2210.15097.

Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham
Neubig. 2022. BRIO: Bringing order to abstractive
summarization. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 2890–2903,
Dublin, Ireland. Association for Computational Lin-
guistics.

Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 1906–1919, On-
line. Association for Computational Linguistics.

Clara Meister, Tiago Pimentel, Gian Wiher, and Ryan
Cotterell. 2022. Typical decoding for natural lan-
guage generation. arXiv preprint arXiv:2202.00666.

Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2016. Pointer sentinel mixture mod-
els. arXiv preprint arXiv:1609.07843.

Moin Nadeem, Tianxing He, Kyunghyun Cho, and
James Glass. 2020. A systematic characterization
of sampling algorithms for open-ended language gen-
eration. In Proceedings of the 1st Conference of the
Asia-Pacific Chapter of the Association for Compu-
tational Linguistics and the 10th International Joint
Conference on Natural Language Processing, pages
334–346, Suzhou, China. Association for Computa-
tional Linguistics.

Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
Amanda Askell, Yuntao Bai, Anna Chen, et al. 2022.
In-context learning and induction heads. arXiv
preprint arXiv:2209.11895.

Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers,
John Thickstun, Sean Welleck, Yejin Choi, and Zaid
Harchaoui. 2021. Mauve: Measuring the gap be-
tween neural text and human text using divergence
frontiers. Advances in Neural Information Process-
ing Systems, 34:4816–4828.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.

Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-
peng Kong, and Nigel Collier. 2022. A contrastive
framework for neural text generation. arXiv preprint
arXiv:2202.06417.

Yixuan Su and Jialu Xu. 2022. An empirical study
on contrastive search and contrastive decoding
for open-ended text generation. arXiv preprint
arXiv:2211.10797.

--- TRANG 10 ---
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-
nan, Kyunghyun Cho, and Jason Weston. 2019. Neu-
ral text generation with unlikelihood training. arXiv
preprint arXiv:1908.04319.

Yilin Yang, Liang Huang, and Mingbo Ma. 2018. Break-
ing the beam search curse: A study of (re-)scoring
methods and stopping criteria for neural machine
translation. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, pages 3054–3059, Brussels, Belgium. Associa-
tion for Computational Linguistics.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068.

Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi
Narayan, Mohammad Saleh, and Peter J Liu.
2022. Calibrating sequence likelihood improves
conditional language generation. arXiv preprint
arXiv:2210.00045.

--- TRANG 11 ---
[Bảng 6: Nghiên cứu trường hợp của các phiên bản được lấy mẫu từ WikiText-103 và WritingPrompts. Chuyển hướng chủ đề không tự nhiên thường được quan sát trong các thế hệ từ SimCTG (màu hồng).]
