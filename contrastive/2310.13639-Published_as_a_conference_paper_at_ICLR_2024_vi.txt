# 2310.13639.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/contrastive/2310.13639.pdf
# Kích thước tệp: 1398683 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
HỌC PREFERENCE CONTRASTIVE: HỌC
TỪ PHẢN HỒI CON NGƯỜI KHÔNG CẦN RL
Joey Hejna
Stanford University
jhejna@cs.stanford.eduRafael Rafailov∗
Stanford University
rafailov@cs.stanford.eduHarshit Sikchi∗
UT Austin
hsikchi@utexas.edu
Chelsea Finn
Stanford UniversityScott Niekum
UMass AmherstW. Bradley Knox
UT AustinDorsa Sadigh
Stanford University
TÓM TẮT
Reinforcement Learning from Human Feedback (RLHF) đã nổi lên như một mô hình phổ biến để căn chỉnh các model với ý định của con người. Thông thường các thuật toán RLHF hoạt động trong hai giai đoạn: đầu tiên, sử dụng preference của con người để học một hàm reward và thứ hai, căn chỉnh model bằng cách tối ưu hóa reward đã học thông qua reinforcement learning (RL). Mô hình này giả định rằng preference của con người được phân phối theo reward, nhưng nghiên cứu gần đây cho thấy chúng thay vào đó tuân theo regret dưới policy tối ưu của người dùng. Do đó, việc học hàm reward từ phản hồi không chỉ dựa trên giả định sai lầm về preference của con người, mà còn dẫn đến những thách thức tối ưu hóa khó khăn xuất phát từ policy gradient hoặc bootstrapping trong giai đoạn RL. Vì những thách thức tối ưu hóa này, các phương pháp RLHF đương đại hạn chế bản thân trong các thiết lập contextual bandit (ví dụ, như trong large language model) hoặc giới hạn dimensionality quan sát (ví dụ, robotics dựa trên state). Chúng tôi vượt qua những hạn chế này bằng cách giới thiệu một họ thuật toán mới để tối ưu hóa hành vi từ phản hồi con người sử dụng model preference dựa trên regret. Sử dụng nguyên lý entropy cực đại, chúng tôi đưa ra Contrastive Preference Learning (CPL), một thuật toán để học các policy tối ưu từ preference mà không cần học hàm reward, vượt qua nhu cầu sử dụng RL. CPL hoàn toàn off-policy, chỉ sử dụng một objective contrastive đơn giản, và có thể được áp dụng cho các MDP tùy ý. Điều này cho phép CPL mở rộng một cách tinh tế đến các vấn đề RLHF có chiều cao và tuần tự trong khi đơn giản hơn các phương pháp trước đó.

1 GIỚI THIỆU
Khi các model pretrained lớn ngày càng hiệu quả, vấn đề căn chỉnh chúng với preference của con người đã trở thành vấn đề nghiên cứu hàng đầu. Việc căn chỉnh này đặc biệt khó khăn khi các dataset lớn hơn không tránh khỏi bao gồm các hành vi không tối ưu. Reinforcement learning from human feedback (RLHF) đã nổi lên như một giải pháp phổ biến cho vấn đề này. Sử dụng preference của con người, các kỹ thuật RLHF phân biệt giữa các hành vi mong muốn và không mong muốn với mục tiêu tinh chỉnh một policy đã học. Mô hình này đã cho thấy kết quả đầy hứa hẹn khi được áp dụng để tinh chỉnh large language model (LLM) (Ouyang et al., 2022), cải thiện các model tạo ảnh (Lee et al., 2023), và thích ứng các policy robot (Christiano et al., 2017) – tất cả từ dữ liệu không tối ưu. Đối với hầu hết các thuật toán RLHF, quá trình này bao gồm hai giai đoạn. Đầu tiên, một model reward được huấn luyện từ dữ liệu preference của người dùng. Và thứ hai, model reward đó được tối ưu hóa bởi một thuật toán reinforcement learning (RL) có sẵn.

Thật không may, mô hình hai giai đoạn này được thành lập dựa trên một giả định sai lầm. Các thuật toán học model reward từ dữ liệu preference yêu cầu preference của con người được phân phối theo tổng có trọng số giảm dần của reward hoặc partial return của mỗi đoạn hành vi. Tuy nhiên, nghiên cứu gần đây (Knox et al., 2022) đặt câu hỏi về điều này, cho rằng thay vào đó con người cung cấp preference dựa

∗Equal Contribution
Mã nguồn của chúng tôi được phát hành tại https://github.com/jhejna/cpl
1arXiv:2310.13639v3 [cs.LG] 30 Apr 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
trên regret của mỗi hành vi dưới policy tối ưu của hàm reward của expert. Trực quan, phán đoán của con người có thể dựa trên tính tối ưu, thay vì state và action nào có lượng reward cao hơn. Kết quả là, lượng chính xác cần học từ phản hồi có thể không phải là reward, mà thay vào đó là hàm advantage tối ưu hoặc nói cách khác, là regret được phủ định.

Trong giai đoạn thứ hai, các thuật toán RLHF hai giai đoạn tối ưu hóa hàm reward đã học từ giai đoạn đầu tiên với RL. Trong thực tế, các thuật toán RL gặp phải một loạt các thách thức tối ưu hóa xuất phát từ temporal credit assignment, chẳng hạn như high-variance của policy gradient (Marbach & Tsitsiklis, 2003) hoặc tính không ổn định của approximate dynamic programming (Van Hasselt et al., 2018). Do đó, các công trình trước đây hạn chế phạm vi của mình để tránh những vấn đề này. Ví dụ, các kỹ thuật RLHF cho LLM giả định một công thức contextual bandit (Ouyang et al., 2022), trong đó policy nhận một giá trị reward duy nhất để phản hồi một truy vấn đã cho tới người dùng. Mặc dù điều này làm giảm nhu cầu long-horizon credit assignment, và do đó giảm high variance của policy gradient, trong thực tế các tương tác của người dùng với LLM là đa bước và tuần tự, vi phạm giả định bandit một bước. Như một ví dụ khác, RLHF đã được áp dụng cho các vấn đề robotics có chiều thấp dựa trên state (Christiano et al., 2017; Sikchi et al., 2023a), một thiết lập mà approximate dynamic programming xuất sắc, nhưng chưa được mở rộng đến các domain continuous control có chiều cao thực tế hơn với input ảnh. Nhìn chung, các phương pháp RLHF không chỉ giả định không chính xác rằng hàm reward một mình thúc đẩy preference của con người, mà còn yêu cầu giảm thiểu các thách thức tối ưu hóa của RL bằng cách đưa ra các giả định hạn chế về tính chất tuần tự của vấn đề hoặc dimensionality.

Trong công trình này, chúng tôi giới thiệu một họ phương pháp RLHF mới sử dụng model preference dựa trên regret, thay vì model partial return thường được chấp nhận chỉ xem xét tổng của reward. Không giống như model partial return, model dựa trên regret trực tiếp cung cấp thông tin về policy tối ưu. Kết quả may mắn của điều này là nó hoàn toàn loại bỏ nhu cầu sử dụng RL, cho phép chúng tôi giải quyết các vấn đề RLHF trong framework MDP tổng quát với không gian state và action có chiều cao. Insight chính của chúng tôi là kết hợp framework preference dựa trên regret với nguyên lý Maximum Entropy (MaxEnt), dẫn đến một bijection giữa hàm advantage và policy. Bằng cách trao đổi tối ưu hóa trên advantage cho tối ưu hóa trên policy, chúng tôi có thể đưa ra một objective supervised learning hoàn toàn mà optimum của nó là policy tối ưu dưới reward của expert. Chúng tôi gọi phương pháp của mình là Contrastive Preference Learning do sự giống nhau với các objective contrastive learning thường được chấp nhận.

CPL có ba lợi ích chính so với công trình trước đó. Đầu tiên, CPL có thể mở rộng tốt như supervised learning bởi vì nó chỉ sử dụng các objective supervised để match advantage tối ưu mà không có policy gradient hoặc dynamic programming. Thứ hai, CPL hoàn toàn off-policy, cho phép sử dụng hiệu quả bất kỳ nguồn dữ liệu offline không tối ưu nào. Cuối cùng, CPL có thể được áp dụng cho các Markov Decision Process (MDP) tùy ý, cho phép học từ các query preference trên dữ liệu tuần tự. Theo hiểu biết của chúng tôi, không có phương pháp nào trước đây cho RLHF đồng thời đáp ứng cả ba nguyên tắc nêu trên. Để chứng minh sự tuân thủ của CPL với ba nguyên tắc nêu trên, chúng tôi chỉ ra hiệu quả của nó trên các vấn đề đưa ra quyết định tuần tự với dữ liệu off-policy không tối ưu và có chiều cao. Đáng chú ý, chúng tôi chỉ ra rằng CPL có thể sử dụng hiệu quả cùng quy trình fine tuning RLHF như các model dialog để học các policy manipulation mở rộng thời gian trong MetaWorld Benchmark. Cụ thể, chúng tôi pretrain các policy sử dụng supervised learning từ các quan sát ảnh có chiều cao, trước khi fine tune chúng với preference. Không có dynamic programming hoặc policy gradient, CPL có thể match hiệu suất của các phương pháp dựa trên RL trước đó. Đồng thời, nó nhanh hơn 1.6× và hiệu quả về tham số gấp bốn lần. Khi sử dụng dữ liệu preference dày đặc hơn, CPL có thể vượt qua hiệu suất của các baseline RL trên 5 trong 6 task.

2 KIẾN THỨC CƠ BẢN
Chúng tôi xem xét vấn đề reinforcement learning from human feedback (RLHF) tổng quát trong một MDP không reward M/r = (S,A, p, γ) với không gian state S, không gian action A, động lực chuyển tiếp p(st+1|st, at), và hệ số giảm γ. Chúng tôi giả định tất cả các state đều có thể đạt được bởi một policy nào đó. Mục tiêu của RLHF là học một policy π(a|s) tối đa hóa hàm reward rE(s, a) của người dùng expert. Tuy nhiên, vì hàm reward không được cung cấp trong MDP /r, nó phải được suy ra từ preference của expert. Thông thường, preference của người dùng sắp xếp hai đoạn hành vi. Một đoạn độ dài k được ký hiệu σ = (s1, a1, s2, a2, . . . , sk, ak). Chúng tôi sử dụng σ+ ≻ σ− để chỉ ra rằng đoạn σ+ được người dùng ưa thích hơn σ− không mất tính tổng quát và giả định chúng tôi được cung cấp một dataset Dpref = {(σ+i, σ−i)}ni=1 của các preference như vậy trong đó σ+ ≻ σ−.

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
RLHF Hai Giai Đoạn Tiêu Chuẩn
Giai Đoạn 1
Học Reward Giai Đoạn 2
RL𝑟𝜃
𝜎−𝜎+
Thuật Toán
RL
𝜋𝜃(𝑎|𝑠)Contrastive Preference Learning
Contrastive Learning
𝜎−𝜎+
෍
𝜎+log𝜋𝜃(𝑎𝑡+|𝑠𝑡+)
෍
𝜎−log𝜋𝜃(𝑎𝑡−|𝑠𝑡−)
Preference Dựa Trên Regret
𝐿𝐶𝑃𝐿=−𝔼log𝑃log𝜋𝜃𝜎+≻𝜎−𝑎+
𝑎−
𝜋𝜃(𝑎|𝑠)
𝑃𝐴∗𝜎+≻𝜎−=𝑒σ𝜎+𝐴∗(𝑠𝑡+,𝑎𝑡+)
𝑒σ𝜎+𝐴∗(𝑠𝑡+,𝑎𝑡+)+𝑒σ𝜎−𝐴∗(𝑠𝑡−,𝑎𝑡−)

Hình 1: Trong khi hầu hết các thuật toán RLHF sử dụng phương pháp học reward hai giai đoạn, sau đó là RL, CPL trực tiếp học một policy sử dụng objective contrastive. Điều này được kích hoạt bởi model preference regret.

Maximum Entropy Reinforcement Learning. Mục tiêu của maximum-entropy reinforcement learning là học một policy π tối đa hóa causal entropy của nó ngoài cumulative discounted return, dẫn đến objective:
max π E_π[∑_{t=0}^∞ γ^t(r(s_t, a_t) − α log π(a_t|s_t))]                    (1)

trong đó α là một tham số temperature. Tăng cường hàm reward với một thuật ngữ −log μ(a|s) bổ sung cho phân phối tham khảo μ(a|s) tạo ra objective KL-constrained được sử dụng trong offline RL (Levine & Koltun, 2013; Garg et al., 2023) và các phương pháp RLHF nổi bật cho LLM (Ziegler et al., 2019; Ouyang et al., 2022). Mặc dù chúng tôi áp dụng framework maximum entropy tiêu chuẩn, phương pháp của chúng tôi dễ dàng mở rộng đến thiết lập constrained. Dưới policy π và hàm reward r, chúng tôi ký hiệu hàm state-value bởi V^π_r(s) và hàm state-action value bởi Q^π_r(s, a). Hàm advantage, A^π_r(s, a) ≜ Q^π_r(s, a) − V^π_r(s), đo lường việc chọn action a tệ hơn bao nhiều so với hành động theo π. Chúng tôi sử dụng π* như viết tắt cho giải pháp của Eq. (1) với hàm reward r_E, và viết các hàm value tương ứng của nó như V*(s) và Q*(s, a) thay vì V^{π*}_{r_E} và Q^{π*}_{r_E}. Chúng tôi đo tính tối ưu của hành vi trực tiếp bằng cách sử dụng hàm advantage của π*, A*(s, a).

Model Preference Regret (hoặc Advantage). Học π* yêu cầu đặc tính hóa cách preference được tạo ra theo model preference P_E[σ+ ≻ σ−], hoặc xác suất mà expert ưa thích σ+ hơn σ−. Thông thường, model preference được chọn là phân phối Boltzmann rational trên discounted partial return của mỗi đoạn, ∑_{t=1}^k γ^t r_E(s_t, a_t), trong đó r_E là hàm reward ẩn của expert. Tuy nhiên, các model như vậy đã được chỉ ra là không nhất quán với preference thực tế của con người (Knox et al., 2022). Ví dụ, xem xét một sparse reward r_E(s, a) = 1{s=g}. Hai đoạn không đạt được mục tiêu sẽ có cùng partial return ngay cả khi một cái di chuyển về phía mục tiêu g trong khi cái khác di chuyển ra xa khỏi nó. Sự không nhất quán này được giải quyết bằng cách xem xét preference được phân phối theo phân phối Boltzmann rational trên negated discounted regret dưới r_E, hoặc −∑_{t=1}^k γ^t(V*(s_t) − Q*(s_t, a_t)). Trong framework này, preference của người dùng chỉ ra rằng một đoạn có regret thấp hơn so với policy tối ưu dự định của họ. Tận dụng sự tương đương của negated regret và tổng có trọng số giảm dần của optimal advantage, chúng tôi viết tương đương model preference dựa trên regret như

P_{A*}[σ+ ≻ σ−] = exp ∑_{σ+} γ^t A*(s_t^+, a_t^+) / (exp ∑_{σ+} γ^t A*(s_t^+, a_t^+) + exp ∑_{σ−} γ^t A*(s_t^−, a_t^−))     (2)

trong đó chúng tôi sử dụng viết tắt "+" và "−" để chỉ mục các state và action của các đoạn σ+ và σ−. Trong phần tiếp theo, chúng tôi sử dụng model preference regret kết hợp với nguyên lý maximum causal entropy để đưa ra CPL.

3 CONTRASTIVE PREFERENCE LEARNING
Mặc dù nghiên cứu gần đây đã chỉ ra rằng preference của con người được mô hình hóa tốt hơn bởi hàm advantage tối ưu hoặc regret, hầu hết các thuật toán RLHF hiện tại giả định ngược lại. Bằng cách học một hàm reward với model preference sai lầm và sau đó áp dụng RL, các phương pháp RLHF truyền thống phát sinh một chi phí tính toán lớn, không cần thiết (Knox et al., 2023). Mục tiêu của chúng tôi là đưa ra các thuật toán RLHF đơn giản và có thể mở rộng được xây dựng chuyên biệt cho model regret chính xác hơn của preference con người.

Mô hình hóa preference con người bằng regret không phải là mới, nhưng công trình trước đây gặp một số thiếu sót. Cụ thể, các thuật toán hiện tại sử dụng model preference regret rất dễ vỡ, vì chúng dựa vào việc ước tính gradient đối với một hàm reward thay đổi, do đó cho đến nay chỉ được xấp xỉ bằng cách tính toán successor feature và giả định biểu diễn tuyến tính hoặc tabular chính xác của hàm reward expert r_E (Knox et al., 2022; 2023). Do đó, các thuật toán này có vẻ không phù hợp cho các tình huống phức tạp vượt ra ngoài các môi trường grid world đơn giản mà chúng đã được kiểm tra.

Ý tưởng chính của phương pháp chúng tôi rất đơn giản: chúng tôi nhận ra rằng hàm advantage, được sử dụng trong model preference regret, có thể dễ dàng được thay thế bằng log-probability của policy khi sử dụng framework maximum entropy reinforcement learning. Tuy nhiên, lợi ích của sự thay thế đơn giản này là rất lớn. Việc sử dụng log-probability của policy vượt qua nhu cầu học hàm advantage hoặc vật lộn với các thách thức tối ưu hóa liên quan đến các thuật toán giống RL. Tóm lại, điều này cho phép chúng tôi không chỉ đón nhận một model preference regret phù hợp hơn, mà còn dựa hoàn toàn vào supervised learning khi học từ phản hồi con người.

Trong phần này, trước tiên chúng tôi đưa ra objective CPL và chỉ ra rằng nó hội tụ đến policy tối ưu cho r_E với dữ liệu không giới hạn. Sau đó, chúng tôi rút ra các mối liên hệ giữa CPL và các phương pháp supervised-learning khác. Cuối cùng, chúng tôi cung cấp các công thức để sử dụng CPL trong thực tế. Các thuật toán của chúng tôi là các ví dụ đầu tiên của một lớp phương pháp mới cho các vấn đề đưa ra quyết định tuần tự mà trực tiếp học một policy từ preference dựa trên regret mà không cần RL, làm cho chúng hiệu quả hơn nhiều.

3.1 TỪ OPTIMAL ADVANTAGE ĐẾN OPTIMAL POLICY
Dưới model preference regret, dataset preference D_pref của chúng tôi chứa thông tin về hàm optimal advantage A*(s, a), có thể được nhìn nhận trực quan như một thước đo việc action a đã cho tệ hơn bao nhiều so với một action được tạo ra bởi policy tối ưu tại state s. Do đó, các action tối đa hóa optimal advantage theo định nghĩa là các action tối ưu và việc học hàm optimal advantage từ preference trực quan nên cho phép chúng tôi trích xuất policy tối ưu.

Phương pháp naive. Khi được trình bày với D_pref, người ta có thể naive tuân theo công thức RLHF reward modeling tiêu chuẩn, nhưng với advantage. Điều này sẽ tương đương với việc tối ưu hóa một advantage có tham số A_θ để tối đa hóa log likelihood của D_pref được cung cấp model preference trong Eq. (2), hoặc max_{A_θ} E_{(σ+,σ−)∼D_pref}[log P_{A_θ}[σ+ ≻ σ−]], trong đó P_{A_θ} là model preference được tạo ra bởi hàm advantage đã học. Một khi một hàm advantage phù hợp với dữ liệu preference được học, nó có thể được chưng cất thành một policy có tham số. Thoạt nhìn, có vẻ như phương pháp hai bước đơn giản này có thể được sử dụng để khôi phục policy tối ưu từ dữ liệu preference. Tuy nhiên, hóa ra việc học một hàm advantage Bellman-consistent là không tầm thường trong cả standard và MaxEnt RL, làm cho việc học một hàm advantage trung gian hợp lệ không chỉ không cần thiết, mà còn khó khăn hơn trong thực tế.

Loại bỏ nhu cầu học advantage. Trong maximum entropy RL, Ziebart (2010) đã chỉ ra rằng mối quan hệ sau đây giữa hàm optimal advantage và optimal policy tồn tại:
π*(a|s) = e^{A*_r(s,a)/α}.

Điều này có nghĩa là để một hàm advantage đã học là tối ưu, nó phải được chuẩn hóa, tức là ∫_A e^{A*(s,a)/α} da = 1. Việc thực thi ràng buộc này là không thể xử lý được, đặc biệt trong không gian liên tục với các neural network lớn, làm cho việc học A_θ một cách naive thông qua maximum likelihood estimation khó khăn.

Tuy nhiên, người ta có thể thay vào đó nhận thấy rằng phương trình trên thiết lập một bijection giữa hàm advantage A*_r và policy π*, cụ thể là hàm optimal advantage tỉ lệ thuận với log-likelihood của optimal policy:
A*_r(s, a) = α log π*(a|s).                                                      (3)

Điều này có nghĩa là thay vì học hàm optimal advantage, chúng ta có thể trực tiếp học optimal policy. Được cung cấp preference được phân phối theo hàm optimal advantage cho hàm reward expert r_E, chúng ta có thể viết model preference theo optimal policy π* bằng cách thay thế Eq. (3) vào Eq. (2) như sau,

P_{A*}[σ+ ≻ σ−] = exp ∑_{σ+} γ^t α log π*(a_t^+|s_t^+) / (exp ∑_{σ+} γ^t α log π*(a_t^+|s_t^+) + exp ∑_{σ−} γ^t α log π*(a_t^−|s_t^−))    (4)

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Do đó, framework maximum entropy đã dẫn đến một model preference con người chỉ theo optimal policy π*. Sử dụng dạng tương đương này của model preference dựa trên advantage, chúng ta có thể trực tiếp tối ưu hóa một policy đã học π_θ để match model preference thông qua maximum likelihood với objective convex sau:

L_CPL(π_θ, D_pref) = E_{(σ+,σ−)∼D_pref}[−log (exp ∑_{σ+} γ^t α log π_θ(a_t^+|s_t^+))/(exp ∑_{σ+} γ^t α log π_θ(a_t^+|s_t^+) + exp ∑_{σ−} γ^t α log π_θ(a_t^−|s_t^−))]    (5)

Giả định sức mạnh biểu diễn đầy đủ, tại sự hội tụ π_θ sẽ mô hình hóa hoàn hảo preference của người dùng, và do đó khôi phục chính xác π* dưới model preference dựa trên advantage được cung cấp một lượng dữ liệu preference không giới hạn. Cụ thể, trong Phụ lục A, chúng tôi chứng minh Định lý sau:

Định lý 1. Giả định một số lượng preference không giới hạn được tạo ra từ một model preference regret rational có nhiễu với hàm advantage expert A*. CPL khôi phục optimal policy π* tương ứng với reward r_E.

Chứng minh này dựa vào bijection giữa hàm optimal advantage và policy trong maximum entropy RL và thực tế rằng model preference regret có thể nhận dạng được (Knox et al., 2022), có nghĩa là objective có thể đạt được loss bằng không.

Lợi ích của việc học trực tiếp policy. Việc học trực tiếp π theo cách này có một số lợi ích, cả thực tế và lý thuyết. Có lẽ rõ ràng nhất, việc học trực tiếp policy vượt qua nhu cầu học bất kỳ hàm nào khác, như hàm reward hoặc value function. Điều này làm cho CPL cực kỳ đơn giản so với công trình trước đây. Khi mở rộng đến các model lớn hơn, chỉ học policy làm giảm cả độ phức tạp và chi phí tính toán. Thứ hai, như được chỉ ra bởi các công trình trước đây (Christiano et al., 2017; Hejna & Sadigh, 2023), việc học reward có thể bị tổn hại bởi tính bất biến của model preference Boltzmann rational (Eq. (2)) đối với các shift; tức là việc thêm một hằng số vào mỗi exponent không thay đổi P[σ+ ≻ σ−]. Trong CPL, ràng buộc phân phối của policy (π_θ(a|s) ≥ 0 với mọi a và ∫_A π_θ(a|s)da = 1) tự động khắc phục vấn đề này, vì việc thêm một hằng số làm cho ∫_A π_θ(a|s)da ≠ 1. Cuối cùng, theo các lập luận trước đây, ràng buộc phân phối của policy đảm bảo rằng ∫_A e^{A_θ(s,a)/α}da = 1. Do đó, có thể chỉ ra rằng hàm advantage ngầm định đã học của CPL luôn là hàm optimal advantage cho một hàm reward nào đó. Chúng tôi gọi thuộc tính này, được định nghĩa bên dưới, là consistency và chứng minh Mệnh đề sau trong Phụ lục A.

Định nghĩa 1. Một hàm advantage A(s, a) là consistent nếu tồn tại một hàm reward r(s, a) nào đó mà A là optimal advantage, hoặc A(s, a) = A*_r(s, a).

Mệnh đề 1. CPL học một hàm advantage consistent.

Hậu quả của điều này là không quan trọng lượng dữ liệu preference được sử dụng, CPL sẽ luôn học optimal policy cho một hàm reward nào đó, và việc thêm dữ liệu preference bổ sung chỉ cải thiện ước tính ngầm định của r_E.

Kết nối với Contrastive Learning. Khi đưa ra CPL, chúng tôi có ý định chọn ký hiệu các đoạn hành vi được ưa thích và không được ưa thích bằng "+" và "-" để làm nổi bật sự tương đồng giữa CPL và các phương pháp contrastive learning. Mặc dù một số phương pháp RLHF hai giai đoạn đã rút ra các kết nối giữa giai đoạn học reward của họ và contrastive learning (Kang et al., 2023), CPL trực tiếp sử dụng một objective contrastive cho việc học policy. Cụ thể, Eq. (5) là một instance của objective Noise Contrastive Estimation (Gutmann & Hyvärinen, 2010) trong đó điểm của một đoạn là tổng có trọng số giảm dần của log-probability dưới policy, ví dụ positive là σ+ và negative là σ−. Trong phụ lục chúng tôi chỉ ra rằng khi được áp dụng cho dữ liệu ranking sử dụng Plackett-Luce Model, CPL khôi phục objective InfoNCE từ Oord et al. (2018) trong đó các ví dụ negative là tất cả các đoạn được xếp hạng dưới đoạn positive. Một cách hiệu quả, CPL đã hoàn toàn trao đổi objective reinforcement learning cho một objective supervised, representation learning trong khi vẫn hội tụ đến policy tối ưu. Vì thành công đáng kể đã đạt được khi áp dụng các objective contrastive learning cho các dataset và neural network quy mô lớn (Chen et al., 2020; He et al., 2020; Radford et al., 2021), chúng tôi mong đợi CPL mở rộng cạnh tranh với các phương pháp RLHF sử dụng thuật toán RL truyền thống.

3.2 CÁC CÂN NHẮC THỰC TẾ
Framework Contrastive Preference Learning cung cấp một hàm loss tổng quát để học policy từ preference dựa trên advantage, từ đó có thể đưa ra nhiều thuật toán. Trong phần này, chúng tôi chi tiết các cân nhắc thực tế cho một instance cụ thể của framework CPL mà chúng tôi thấy hoạt động tốt trong thực tế. Trong phụ lục, chúng tôi bao gồm một số instance của CPL cho các loại dữ liệu khác nhau và các regularizer conservative.

CPL với Dữ liệu Offline Hữu hạn. Mặc dù CPL hội tụ đến policy tối ưu với dữ liệu preference không giới hạn, trong thực tế chúng ta thường quan tâm đến việc học từ các dataset offline hữu hạn. Trong thiết lập này, các policy ngoại suy quá nhiều ngoài support của dataset hoạt động kém vì chúng thực hiện các action dẫn đến các state out of distribution. Giống như nhiều objective dựa trên preference khác, objective của CPL không strictly convex (Phụ lục A.3). Do đó, nhiều policy, ngay cả những policy có trọng số cao trên các action không có trong dataset, có thể đạt được cùng optima của Eq. (5). Chúng tôi chứng minh điều này bằng cách công thức hóa CPL như một vấn đề logistic regression. Đặt policy được biểu diễn bởi một vector một chiều π ∈ R^{|S×A|}. Sự khác biệt giữa các đoạn positive và negative, ∑_{σ+} γ^t α log π_θ(a_t^+|s_t^+) - ∑_{σ+} γ^t α log π_θ(a_t^−|s_t^−) có thể được viết lại như một dot-product giữa π và một vector "comparison" x, mà các giá trị của nó là γ^t, -γ^t, hoặc 0 chỉ ra thành viên của comparison σ+ ≻ σ−. Sử dụng hàm logistic, logistic(z) = 1/(1+e^{-z}), chúng tôi viết lại objective CPL trong trường hợp hữu hạn như

L_CPL(π_θ, D_pref) = -∑_{i=1}^{|D_pref|} log logistic(α x_i^T log π(a|s)), trong đó x_i[s, a] = {γ^t nếu σ_{i,t}^+ = (s, a), -γ^t nếu σ_{i,t}^− = (s, a), 0 ngược lại}

trong đó σ_{i,t}^+ ký hiệu timestep thứ t của đoạn được ưa thích từ comparison thứ i trong D_pref. Chúng ta có thể suy luận về tập hợp tất cả các policy tạo ra cùng CPL loss bằng cách tập hợp tất cả các comparison vector thành một ma trận X, trong đó hàng thứ i của X là vector x_i cho comparison thứ i trong dataset. Bất kỳ thay đổi nào đối với log π trong null space của X không có ảnh hưởng đến logit của hàm logistic, và do đó không có ảnh hưởng đến loss. Trong thực tế, |S × A| >> n, làm cho null space của X thường là non-trivial sao cho có nhiều minimizer của CPL loss, một số trong đó có thể đặt xác suất cao trên các cặp state-action không có trong dataset. Trong Phụ lục A.3 chúng tôi cung cấp các construction của X trong đó điều này đúng. Tiếp theo, chúng tôi chỉ ra cách vấn đề này có thể được giải quyết bằng cách kết hợp regularization vào objective CPL.

Regularization. Trong các thiết lập hữu hạn, chúng ta muốn chọn policy tối thiểu hóa hàm loss CPL trong khi đặt likelihood cao hơn trên các action trong dataset. Để hoàn thành điều này, chúng tôi sửa đổi Eq. (5) với một regularizer conservative gán loss thấp hơn khi policy có likelihood cao hơn trên các action trong D_pref, giữ nó in-distribution. Mặc dù có nhiều lựa chọn regularizer có thể, chúng tôi sử dụng regularizer "bias" asymmetric được điều chỉnh từ An et al. (2023) vì nó hoạt động tốt nhất trong các thí nghiệm của chúng tôi. Trong objective của chúng tôi, regularizer bias down-weight các đoạn negative bởi λ ∈ (0,1) như sau:

L_CPL^{(λ)}(π_θ, D_pref) = E_{D_pref}[−log (exp ∑_{σ+} γ^t α log π_θ(a_t^+|s_t^+))/(exp ∑_{σ+} γ^t α log π_θ(a_t^+|s_t^+) + exp λ∑_{σ−} γ^t α log π_θ(a_t^−|s_t^−))]    (6)

Nếu policy đặt trọng số nhiều hơn trên các action trong dataset, log π_θ(a|s) sẽ tăng. Trong model Boltzmann tiêu chuẩn, việc tăng log-probability của cả đoạn positive và negative cùng một lượng sẽ không có ảnh hưởng đến loss. Tuy nhiên, bias cân nhắc ít hơn log-probability tăng của các đoạn negative, cuối cùng làm giảm loss. Do đó, trong khi một minimizer của hàm loss CPL vanilla có thể đặt xác suất cao trên các action chưa thấy, Eq. (6) được tối thiểu hóa với trọng số cao hơn trên các action in-distribution. Điều này được nắm bắt chính thức bởi mệnh đề sau, cho thấy rằng, đối với một policy cố định, L_CPL^{(λ)} thấp hơn khi policy đặt likelihood cao hơn trên các action trong dataset so với các comparison khác với cùng CPL Loss.

Mệnh đề 2. Xem xét một comparison σ+ ≻ σ− từ D_pref và một comparison tùy ý σ'+ ≻ σ'− sao cho L_CPL(π, σ+ ≻ σ−) = L_CPL(π, σ'+ ≻ σ'−) cho một policy π cố định. Nếu ∑_{σ+} γ^t log π(a_t^+|s_t^+) > ∑_{σ'+} γ^t log π(a_t^+|s_t^+), thì L_CPL^{(λ)}(π, σ+ ≻ σ−) < L_CPL^{(λ)}(π, σ'+ ≻ σ'−).

Về cơ bản, điều này cho thấy rằng regularizer bias phá vỡ ties trong hàm loss CPL bằng cách phạt likelihood thấp hơn. Chúng tôi chứng minh điều này, cùng với một phiên bản tổng quát hơn, trong Phụ lục A.4. Trong Phụ lục B chúng tôi cũng xem xét các biến thể CPL với các dạng conservative regularization khác.

Pretraining. Pre-training policy π_θ với behavior cloning (BC) là một thực hành phổ biến trong RLHF (Ouyang et al., 2022). Do đó, trước khi fine-tuning với preference sử dụng loss CPL, chúng tôi đã huấn luyện policy sử dụng objective BC maximum likelihood tiêu chuẩn, min_θ E_{(s,a)∼D}[log π_θ(a|s)]. Mặc dù D có thể là bất kỳ dataset nào, chúng tôi đã chọn D_pref. Chúng tôi thấy rằng điều này giúp cải thiện hiệu suất trong một số trường hợp, nhưng tổn hại trong những trường hợp khác (Phụ lục C). Chúng tôi cho rằng pre-training giúp ích khi một policy gần với phân phối dữ liệu là mong muốn, đặc biệt khi các action out-of-distribution có hại.

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

4 THÍ NGHIỆM
Trong phần này, chúng tôi giải quyết các câu hỏi sau về CPL: Đầu tiên, CPL có hiệu quả trong việc fine-tuning policy từ preference dựa trên regret không? Thứ hai, CPL có mở rộng đến các vấn đề control có chiều cao và network lớn hơn không? Thứ ba, những thành phần nào của CPL quan trọng để đạt được hiệu suất cao? Và cuối cùng, CPL hoạt động như thế nào với dữ liệu preference thực tế của con người hạn chế? Các thí nghiệm và chi tiết bổ sung được bao gồm trong phụ lục.

Dữ liệu Preference. Chúng tôi đánh giá khả năng của CPL trong việc học policy cho MDP tổng quát từ dữ liệu rollout off-policy không tối ưu và preference. Cụ thể, chúng tôi xem xét quy trình huấn luyện thường được sử dụng cho các model nền tảng lớn: supervised learning tiếp theo là fine-tuning với RLHF. Để làm điều này, chúng tôi sử dụng sáu task từ benchmark robotics MetaWorld (Yu et al., 2020). Đầu tiên, chúng tôi huấn luyện các policy baseline cho đến khi chúng đạt khoảng 50% tỷ lệ thành công. Sau đó, chúng tôi rollout 2500 episode cho mỗi policy stochastic không tối ưu. Sau đó chúng tôi tạo thành các dataset preference D_pref synthetic với các kích thước khác nhau bằng cách lấy mẫu các đoạn có độ dài 64 đồng đều từ dữ liệu rollout. Chúng tôi ước tính các nhãn preference dựa trên regret sử dụng Q-function và policy của một model Soft Actor-Critic (SAC) oracle (Haarnoja et al., 2018) được huấn luyện đến 100% thành công trên sự kết hợp của rollout không tối ưu và dữ liệu online. Trong thực tế, chúng tôi xem xét hai loại dataset preference chính: dense, trong đó chúng tôi gán nhãn comparison giữa mỗi đoạn được lấy mẫu (effectively ranking tất cả các đoạn), và sparse, trong đó chúng tôi chỉ gán nhãn một comparison cho mỗi đoạn.

Các Phương pháp Baseline. Chúng tôi xem xét bốn baseline mạnh. Đầu tiên, supervised fine-tuning (SFT) huấn luyện một policy với BC trên tất cả các đoạn, sau đó fine-tune chỉ trên các đoạn được ưa thích, tức là tất cả σ+ trong D_pref. Baseline thứ hai là Preference IQL (P-IQL), học một hàm reward từ D_pref giả định model partial return, sau đó học một policy để tối đa hóa nó với Implicit Q-Learning (Kostrikov et al., 2022), một thuật toán offline RL tiên tiến. Mặc dù P-IQL lần đầu được sử dụng với model partial return, ở đây nó sử dụng một xấp xỉ của A*_rE như reward của nó, như chúng tôi chỉ ra trong Corollary 1 của Phụ lục A bảo tồn policy tối ưu. Thực tế, P-IQL nên hoạt động tốt hơn với nhãn dựa trên regret, vì A*_rE là một hàm reward được shaped cao cho r_E Ng et al. (1999); Knox et al. (2023). Chúng tôi sử dụng implementation P-IQL của Hejna & Sadigh (2023) vì nó vượt trội so với một số baseline đương thời. Baseline này cũng tương đương với TREX trên preference pairwise (Brown et al., 2019). Thứ ba, chúng tôi xem xét PPO với reward KL-constrained như thường được sử dụng cho RLHF với LLM (Ziegler et al., 2019). Đây không phải là một so sánh công bằng, vì PPO yêu cầu 3.84 triệu cặp state-action online bổ sung để ước tính policy gradient, tổng cộng 25× dữ liệu cho CPL 2.5K Dense và 4× dữ liệu cho CPL 20K Sparse. Không giống như thiết lập contextual bandit được sử dụng cho LLM, PPO ở đây yêu cầu rollout trajectory đầy đủ. Cuối cùng, để chứng minh khả năng ngoại suy của CPL vượt ra ngoài hiệu suất tốt nhất trong dữ liệu, chúng tôi so sánh với %BC, trong đó một policy được huấn luyện với behavior cloning trên X% rollout hàng đầu theo r_E ground truth.

4.1 CPL HOẠT ĐỘNG NHƯ THẾ NÀO?
CPL hoạt động như thế nào với quan sát dựa trên state? Kết quả chính dựa trên state của chúng tôi có thể được tìm thấy trong hàng 1 và 3 của Bảng 1. Khi sử dụng dữ liệu comparison thưa thớt hơn (hàng 3), CPL vượt trội so với các phương pháp trước đây trong 5 trong 6 môi trường, thường với margin đáng kể hơn P-IQL, đặc biệt trong các môi trường Button Press, Bin Picking, và Sweep Into. Khi được áp dụng cho dataset với comparison dày đặc hơn (hàng 1), CPL hoạt động thậm chí tốt hơn. Mặc dù dataset comparison dày đặc có ít state-action coverage hơn, chúng chứa đáng kể nhiều comparison hơn dataset comparison thưa thớt. Chúng tôi cho rằng nhiều comparison cho mỗi đoạn có lợi hơn cho CPL so với P-IQL vì objective contrastive của nó – các dataset giàu comparison hơn có thể có nhiều cặp positive-negative thông tin hơn giúp hình thành policy. PPO không thể consistently hoạt động tốt hơn CPL mặc dù truy cập vào lượng lớn dữ liệu online từ môi trường. Chúng tôi thấy PPO rất nhạy cảm với hệ số KL-constraint trên reward, điều này làm cho nó khó tune như được quan sát trong công trình trước (Touvron et al., 2023). Vấn đề này có thể được làm trầm trọng hơn bởi sự bất ổn của KL-divergence trong không gian liên tục và high variance của cả value estimation và policy gradient với horizon dài hơn trong các task robotics so với thiết lập contextual-bandit LLM. Chúng tôi thấy rằng CPL consistently vượt trội so với %BC, chỉ ra CPL thực sự thể hiện cải thiện policy vượt ra ngoài các hành vi tốt nhất trong dataset.

CPL mở rộng như thế nào đến quan sát có chiều cao? Để kiểm tra cách các objective supervised của CPL mở rộng đến các vấn đề continuous control có chiều cao, chúng tôi render các dataset MetaWorld được thảo luận ở trên thành ảnh 64×64. Chúng tôi sử dụng kiến trúc network từ DrQv2 (Yarats et al., 2022) và

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

[Bảng 1 với kết quả hiệu suất trên MetaWorld benchmark]

cùng hyperparameter như các thí nghiệm dựa trên state. Chúng tôi thêm sử dụng random shift augmentation, điều này cải thiện drastically hiệu suất của RL từ ảnh (Laskin et al., 2020). Kết quả dựa trên ảnh của chúng tôi có thể được tìm thấy trong hàng 2 và 4 của Bảng 1. Thú vị, chúng tôi thấy rằng hiệu suất tăng vừa phải cho SFT nhưng đáng kể cho P-IQL. Chúng tôi cho rằng điều này là do data-augmentation, không áp dụng được trong state, đóng vai trò quan trọng trong việc cải thiện biểu diễn value cho P-IQL. Mặc dù vậy, khi học từ dữ liệu preference dày đặc hơn (hàng 2), CPL vẫn vượt trội so với P-IQL trong 4 trong 6 môi trường và hòa trên Sweep Into. Khi học từ comparison thưa thớt hơn (hàng 4), CPL và P-IQL hoạt động tương đương trên hầu hết các task, mặc dù CPL đơn giản hơn drastically so với P-IQL. Một lần nữa, khoảng cách hiệu suất giữa CPL và P-IQL cao hơn với dữ liệu comparison dày đặc hơn, nhấn mạnh tầm quan trọng của negative thông tin.

[Bảng 2 về hiệu quả tính toán]

Những kết quả này ấn tượng hơn khi xem xét việc giảm độ phức tạp đáng kể của CPL. P-IQL phải học một hàm reward, một Q-function, một value function, và một policy. CPL tránh tất cả điều này, và chỉ học một policy, drastically giảm thời gian huấn luyện và số lượng parameter. Như chúng ta có thể thấy trong Bảng 2, CPL chạy nhanh hơn 1.62× so với P-IQL trên ảnh và có ít hơn một phần tư parameter. Khi network ngày càng lớn hơn, lợi ích hiệu suất từ việc sử dụng CPL sẽ chỉ tăng.

4.2 ĐIỀU GÌ ĐÓNG GÓP VÀO HIỆU SUẤT CỦA CPL?

Như được ám chỉ trong các phần trước, chúng tôi thấy rằng khoảng cách hiệu suất giữa CPL và baseline cao hơn cho các dataset với comparison dày đặc hơn. Điều này phù hợp với các công trình trước đây trong contrastive

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

learning (Robinson et al., 2021). Để nghiên cứu hiệu ứng này, chúng tôi đánh giá hiệu suất của CPL khi chúng tôi tăng số lượng comparison được lấy mẫu cho mỗi đoạn trên một dataset cố định gồm 5000 đoạn. Chúng tôi chỉ ra kết quả của điều này cho Drawer Open với quan sát dựa trên state ở bên trái của Hình 2 và bao gồm phần còn lại trong Phụ lục C.3 ngoài dense data scaling. Nhìn chung, chúng tôi thấy rằng CPL được hưởng lợi từ việc tăng số lượng comparison cho mỗi đoạn trong tất cả các task ngoại trừ Plate Slide. P-IQL ít bị ảnh hưởng hơn, mặc dù đôi khi hoạt động tệ hơn với nhiều comparison hơn, mà chúng tôi nghi ngờ là do reward under-fitting. Điều này làm nổi bật một nhược điểm khác của P-IQL – do số lượng thành phần cao hơn, nó có nhiều hyperparameter hơn và do đó nhạy cảm hơn với những thay đổi trong dataset. Chúng tôi đã tune hyperparameter cho tất cả các phương pháp với 10K comparison, sau đó để chúng giống nhau cho các thí nghiệm scaling.

Cuối cùng, chúng tôi ablate cả hai hyperparameter của CPL – giá trị temperature α và bias regularizer λ – cho Drawer Open ở bên phải của Hình 2. Trong khi CPL thường hoạt động tốt với tất cả các giá trị, chúng tôi thấy rằng hiệu suất cao hơn có thể đạt được với việc tune hyper-parameter thêm, đặc biệt cho λ. Trong Phụ lục B chúng tôi ablate nhiều quyết định thiết kế hơn, như lựa chọn conservative regularizer.

4.3 CPL HOẠT ĐỘNG NHƯ THẾ NÀO VỚI PREFERENCE CON NGƯỜI HẠN CHẾ?

Để chứng minh cả tính ứng dụng của model preference regret và hiệu suất của CPL, chúng tôi thực hiện các thí nghiệm bổ sung với preference thực tế của con người. Cụ thể, chúng tôi áp dụng benchmark từ Kim et al. (2023) sử dụng 100 (expert) hoặc 500 (replay) preference con người trên dataset từ benchmark D4RL (Fu et al., 2020). Để tạo điều kiện học từ số lượng query hạn chế như vậy, chúng tôi đã sửa đổi CPL bằng cách đầu tiên học một model logistic regression để dự đoán preference P[σ+ ≻ σ−] của người dùng theo An et al. (2023). Quan trọng, đây không phải là một hàm reward hoặc advantage vì nó lấy toàn bộ đoạn làm input, không phải các cặp state-action đơn lẻ. Sau đó chúng tôi sử dụng model này để relabel dữ liệu offline D4RL với preference dày đặc cho CPL. Chúng tôi mượn kiến trúc từ Kim et al. (2023) và không sử dụng BC pretraining. Kết quả của chúng tôi có thể được tìm thấy trong Bảng 3. CPL có hiệu suất tốt nhất với margin lớn trong Hopper-Medium-Exp, nhưng hoạt động tệ hơn trong Walker-Medium-Replay. Chúng tôi cho rằng điều này là do preference cho dataset này có thể không tuân thủ chặt chẽ model dựa trên regret như được thảo luận với các tác giả của Kim et al. (2023), chúng được thu thập bởi một người dùng duy nhất với phương pháp dựa trên quy tắc được lên kế hoạch trước. Tuy nhiên, CPL vẫn có thể hoạt động tốt trên các task này chỉ với 100 hoặc 500 preference con người, và không cần học value hoặc Q-function.

[Bảng 3 với kết quả D4RL]

5 CÔNG TRÌNH LIÊN QUAN
Mặc dù RLHF gần đây đã tăng vọt về độ phổ biến, việc học policy từ preference con người đã là một vấn đề được nghiên cứu lâu dài, được gọi là preference-based RL (PbRL).

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024

Các phương pháp PbRL thường bắt đầu bằng việc học một hàm reward, thường từ comparison pairwise, sau đó sử dụng một thuật toán RL để tối ưu hóa policy (Fürnkranz et al., 2012). Trong khi Akrour et al. (2012; 2011); Wilson et al. (2012) là một số ví dụ đầu tiên của PbRL, gần đây hơn một số công trình đã chỉ ra rằng, với hàng ngàn query hoặc pretraining đầy đủ, PbRL có thể huấn luyện policy deep neural-network cho control sử dụng comparison (Christiano et al., 2017; Lee et al., 2021; Ibarz et al., 2018; Brown et al., 2020; Hejna & Sadigh, 2022; Shin & Brown, 2021) hoặc ranking (Brown et al., 2019; Bıyık et al., 2019; Sikchi et al., 2023a). Tuy nhiên, các phương pháp này thường chỉ được chứng minh trên control dựa trên state có chiều thấp vì những thách thức mà RL gặp phải khi mở rộng đến input và network lớn hơn (Ota et al., 2021). Trong quá khứ, việc loại bỏ RL đã dẫn đến các thuật toán hiệu quả cho goal-conditioned RL từ ảnh (Hejna et al., 2023; Eysenbach et al., 2022). CPL làm điều tương tự nhưng cho PbRL. Các công trình khác giải quyết vấn đề lựa chọn phản hồi (Sadigh et al., 2017; Biyik et al., 2020; Daniel et al., 2015), mà chúng tôi xem xét bổ sung vì CPL có thể được hưởng lợi từ việc thu thập dữ liệu chất lượng cao hơn.

Để mở rộng RLHF, các phương pháp gần đây để tinh chỉnh LLM đã bỏ qua thành phần temporal của RL, và thay vào đó coi text-generation như một vấn đề contextual bandit (Ziegler et al., 2019). Trong khi phương pháp này đã tỏ ra hiệu quả tại các task như summarization (Stiennon et al., 2020; Wu & Hu, 2018), instruction following (Ouyang et al., 2022; Nakano et al., 2021), và thậm chí tạo ảnh (Lee et al., 2023; Black et al., 2023), nó fundamentally bỏ qua thực tế rằng tương tác với người dùng thường là tuần tự, kéo dài nhiều lượt. Không giống như các phương pháp này, CPL hoạt động với MDP tổng quát. Khả năng độc đáo của CPL trong việc học từ dữ liệu sequence chỉ với objective supervised làm cho nó trở thành ứng viên hàng đầu để mở rộng đến các vấn đề phức tạp hơn. Thực tế, Direct Preference Optimization (DPO) (Rafailov et al., 2023) gần đây đã chứng minh rằng một objective supervised tương tự như CPL có thể hoạt động tốt như RL trong thiết lập contextual bandit. Chúng tôi chỉ ra trong Phụ lục A rằng DPO có thể được đưa ra như một trường hợp đặc biệt của CPL trong đó các đoạn có độ dài 1 và bắt đầu tại cùng state. Điều này tương đồng với Knox et al. (2023), những người chỉ ra rằng phương pháp contextual bandit-approach phổ biến là một trường hợp đặc biệt của phương pháp naive từ Phần 3.

Để đưa ra objective của CPL, chúng tôi tận dụng kiến thức từ các công trình xây dựng dựa trên nguyên lý maximum entropy trong control (Ziebart et al., 2008; Ziebart, 2010; Haarnoja et al., 2017). Cập nhật contrastive kết quả trực tiếp học policy tối ưu với dữ liệu hoàn toàn off-policy. Điều này không giống như nhiều thuật toán RLHF dựa trên RL trong cả ngôn ngữ (Ziegler et al., 2019) hoặc control (Christiano et al., 2017) yêu cầu rollout on policy và các thành phần đã học bổ sung đã được chỉ ra làm tăng variance (Hejna & Sadigh, 2023). Các objective contrastive learning tương tự đã chỉ ra là hiệu quả cho temporal representation learning (Ma et al., 2023), ngay cả với dữ liệu preference (Kang et al., 2023; Xu et al., 2023; An et al., 2023).

6 THẢO LUẬN
Trong công trình này chúng tôi giới thiệu CPL, một framework novel cho RLHF sử dụng model preference regret. Về mặt lý thuyết, chúng tôi đã chứng minh rằng CPL luôn học một hàm advantage consistent và hội tụ đến policy tối ưu cho hàm reward của expert. Về mặt thực tế, chúng tôi đã chỉ ra rằng objective supervised của CPL có thể vượt trội so với baseline RL khi học các policy manipulation phức tạp từ dữ liệu preference dày đặc trong khi đơn giản hơn và nhanh hơn 1.6×.

Hạn chế. CPL, giống như các phương pháp RLHF khác, giả định kiến thức về temporal discounting của human rater (tức là của discount factor γ), trong thực tế sẽ khó truyền đạt. Vì hàm loss của CPL được tính toán trên các đoạn, nó yêu cầu một lượng đáng kể bộ nhớ GPU cho các kích thước đoạn lớn. Cuối cùng, không có model nào về hành vi con người là hoàn hảo.

Hướng Tương lai. Một số hướng nghiên cứu thú vị vẫn còn. Đầu tiên là mở rộng CPL đến các dataset và kiến trúc lớn hơn nơi chúng tôi tin rằng lợi ích của nó sẽ rõ ràng hơn. Một ứng dụng có thể thú vị là LLM, nơi CPL cho phép fine-tuning trên nhiều bước của turn-based dialogue. Theo hiểu biết của chúng tôi, không có dataset preference multi-step nào hiện tại tồn tại cho LLM. Thứ hai, công trình của chúng tôi chỉ xem xét dữ liệu offline được tạo ra bởi các policy không tối ưu. Một phiên bản online của CPL có thể được phát triển hoạt động với phản hồi con người online, cho phép policy liên tục cải thiện. Cuối cùng, để thư giãn giả định rằng γ đã biết, người ta có thể thay vào đó bao gồm nó trong expressivity của CPL hoặc các phương pháp RLHF khác.

--- TRANG 9 ---
[Phần này chứa các chi tiết về tài liệu tham khảo và đóng góp - tôi sẽ tiếp tục dịch phần còn lại]

10 LỜI CẢM ƠN
Công trình này được hỗ trợ bởi NSF Award 2006388, NSF Award 2218760, NSF Award 1933032, NSF Award 1953032, NSF Award 1941722, NSF Award 2125511, NSF Award IIS-1749204, AFOSR YIP, AFOSR (FA9550-20-1-0077), ARO (78372-CS, W911NF-19-2-0333), ONR (N00014-21-1-2685), ONR, Ford, DARPA YFA, và Center for AI Safety. JH được hỗ trợ bởi học bổng DoD NDSEG. CF là một Fellow CIFAR trong chương trình Learning in Machines and Brains. WBK được hỗ trợ bởi thách thức lớn Good Systems của UT Austin. Chúng tôi muốn cảm ơn Archit Sharma về những cuộc thảo luận quý báu về regularizer conservative được sử dụng trong CPL. Bất kỳ ý kiến, phát hiện, và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này đều thuộc về (các) tác giả và không nhất thiết phản ánh quan điểm của các nhà tài trợ.

ĐÓNG GÓP
JH dẫn dắt dự án, đóng góp vào tất cả các khía cạnh bao gồm ý tưởng, lý thuyết, thí nghiệm, và viết. RR đề xuất liên kết advantage và likelihood và đóng góp vào ý tưởng giai đoạn đầu. HS đóng góp vào lý thuyết, thiết kế thí nghiệm, và chạy thí nghiệm. CF, SN, WBK, DS giám sát, tư vấn, và cung cấp phản hồi về dự án.

[Phần tài liệu tham khảo và các phụ lục tiếp theo sẽ được dịch tiếp tục nếu cần]
