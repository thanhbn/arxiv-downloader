# 2310.13639.pdf
# Chuyá»ƒn Ä‘á»•i tá»« PDF sang TXT
# ÄÆ°á»ng dáº«n nguá»“n: /home/admin88/arxiv-downloader/contrastive/2310.13639.pdf
# KÃ­ch thÆ°á»›c tá»‡p: 1398683 bytes

===============================================
Ná»˜I DUNG Tá»†P PDF
===============================================

--- TRANG 1 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024
Há»ŒC PREFERENCE CONTRASTIVE: Há»ŒC
Tá»ª PHáº¢N Há»’I CON NGÆ¯á»œI KHÃ”NG Cáº¦N RL
Joey Hejna
Stanford University
jhejna@cs.stanford.eduRafael Rafailovâˆ—
Stanford University
rafailov@cs.stanford.eduHarshit Sikchiâˆ—
UT Austin
hsikchi@utexas.edu
Chelsea Finn
Stanford UniversityScott Niekum
UMass AmherstW. Bradley Knox
UT AustinDorsa Sadigh
Stanford University
TÃ“M Táº®T
Reinforcement Learning from Human Feedback (RLHF) Ä‘Ã£ ná»•i lÃªn nhÆ° má»™t mÃ´ hÃ¬nh phá»• biáº¿n Ä‘á»ƒ cÄƒn chá»‰nh cÃ¡c model vá»›i Ã½ Ä‘á»‹nh cá»§a con ngÆ°á»i. ThÃ´ng thÆ°á»ng cÃ¡c thuáº­t toÃ¡n RLHF hoáº¡t Ä‘á»™ng trong hai giai Ä‘oáº¡n: Ä‘áº§u tiÃªn, sá»­ dá»¥ng preference cá»§a con ngÆ°á»i Ä‘á»ƒ há»c má»™t hÃ m reward vÃ  thá»© hai, cÄƒn chá»‰nh model báº±ng cÃ¡ch tá»‘i Æ°u hÃ³a reward Ä‘Ã£ há»c thÃ´ng qua reinforcement learning (RL). MÃ´ hÃ¬nh nÃ y giáº£ Ä‘á»‹nh ráº±ng preference cá»§a con ngÆ°á»i Ä‘Æ°á»£c phÃ¢n phá»‘i theo reward, nhÆ°ng nghiÃªn cá»©u gáº§n Ä‘Ã¢y cho tháº¥y chÃºng thay vÃ o Ä‘Ã³ tuÃ¢n theo regret dÆ°á»›i policy tá»‘i Æ°u cá»§a ngÆ°á»i dÃ¹ng. Do Ä‘Ã³, viá»‡c há»c hÃ m reward tá»« pháº£n há»“i khÃ´ng chá»‰ dá»±a trÃªn giáº£ Ä‘á»‹nh sai láº§m vá» preference cá»§a con ngÆ°á»i, mÃ  cÃ²n dáº«n Ä‘áº¿n nhá»¯ng thÃ¡ch thá»©c tá»‘i Æ°u hÃ³a khÃ³ khÄƒn xuáº¥t phÃ¡t tá»« policy gradient hoáº·c bootstrapping trong giai Ä‘oáº¡n RL. VÃ¬ nhá»¯ng thÃ¡ch thá»©c tá»‘i Æ°u hÃ³a nÃ y, cÃ¡c phÆ°Æ¡ng phÃ¡p RLHF Ä‘Æ°Æ¡ng Ä‘áº¡i háº¡n cháº¿ báº£n thÃ¢n trong cÃ¡c thiáº¿t láº­p contextual bandit (vÃ­ dá»¥, nhÆ° trong large language model) hoáº·c giá»›i háº¡n dimensionality quan sÃ¡t (vÃ­ dá»¥, robotics dá»±a trÃªn state). ChÃºng tÃ´i vÆ°á»£t qua nhá»¯ng háº¡n cháº¿ nÃ y báº±ng cÃ¡ch giá»›i thiá»‡u má»™t há» thuáº­t toÃ¡n má»›i Ä‘á»ƒ tá»‘i Æ°u hÃ³a hÃ nh vi tá»« pháº£n há»“i con ngÆ°á»i sá»­ dá»¥ng model preference dá»±a trÃªn regret. Sá»­ dá»¥ng nguyÃªn lÃ½ entropy cá»±c Ä‘áº¡i, chÃºng tÃ´i Ä‘Æ°a ra Contrastive Preference Learning (CPL), má»™t thuáº­t toÃ¡n Ä‘á»ƒ há»c cÃ¡c policy tá»‘i Æ°u tá»« preference mÃ  khÃ´ng cáº§n há»c hÃ m reward, vÆ°á»£t qua nhu cáº§u sá»­ dá»¥ng RL. CPL hoÃ n toÃ n off-policy, chá»‰ sá»­ dá»¥ng má»™t objective contrastive Ä‘Æ¡n giáº£n, vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c MDP tÃ¹y Ã½. Äiá»u nÃ y cho phÃ©p CPL má»Ÿ rá»™ng má»™t cÃ¡ch tinh táº¿ Ä‘áº¿n cÃ¡c váº¥n Ä‘á» RLHF cÃ³ chiá»u cao vÃ  tuáº§n tá»± trong khi Ä‘Æ¡n giáº£n hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã³.

1 GIá»šI THIá»†U
Khi cÃ¡c model pretrained lá»›n ngÃ y cÃ ng hiá»‡u quáº£, váº¥n Ä‘á» cÄƒn chá»‰nh chÃºng vá»›i preference cá»§a con ngÆ°á»i Ä‘Ã£ trá»Ÿ thÃ nh váº¥n Ä‘á» nghiÃªn cá»©u hÃ ng Ä‘áº§u. Viá»‡c cÄƒn chá»‰nh nÃ y Ä‘áº·c biá»‡t khÃ³ khÄƒn khi cÃ¡c dataset lá»›n hÆ¡n khÃ´ng trÃ¡nh khá»i bao gá»“m cÃ¡c hÃ nh vi khÃ´ng tá»‘i Æ°u. Reinforcement learning from human feedback (RLHF) Ä‘Ã£ ná»•i lÃªn nhÆ° má»™t giáº£i phÃ¡p phá»• biáº¿n cho váº¥n Ä‘á» nÃ y. Sá»­ dá»¥ng preference cá»§a con ngÆ°á»i, cÃ¡c ká»¹ thuáº­t RLHF phÃ¢n biá»‡t giá»¯a cÃ¡c hÃ nh vi mong muá»‘n vÃ  khÃ´ng mong muá»‘n vá»›i má»¥c tiÃªu tinh chá»‰nh má»™t policy Ä‘Ã£ há»c. MÃ´ hÃ¬nh nÃ y Ä‘Ã£ cho tháº¥y káº¿t quáº£ Ä‘áº§y há»©a háº¹n khi Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ tinh chá»‰nh large language model (LLM) (Ouyang et al., 2022), cáº£i thiá»‡n cÃ¡c model táº¡o áº£nh (Lee et al., 2023), vÃ  thÃ­ch á»©ng cÃ¡c policy robot (Christiano et al., 2017) â€“ táº¥t cáº£ tá»« dá»¯ liá»‡u khÃ´ng tá»‘i Æ°u. Äá»‘i vá»›i háº§u háº¿t cÃ¡c thuáº­t toÃ¡n RLHF, quÃ¡ trÃ¬nh nÃ y bao gá»“m hai giai Ä‘oáº¡n. Äáº§u tiÃªn, má»™t model reward Ä‘Æ°á»£c huáº¥n luyá»‡n tá»« dá»¯ liá»‡u preference cá»§a ngÆ°á»i dÃ¹ng. VÃ  thá»© hai, model reward Ä‘Ã³ Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a bá»Ÿi má»™t thuáº­t toÃ¡n reinforcement learning (RL) cÃ³ sáºµn.

Tháº­t khÃ´ng may, mÃ´ hÃ¬nh hai giai Ä‘oáº¡n nÃ y Ä‘Æ°á»£c thÃ nh láº­p dá»±a trÃªn má»™t giáº£ Ä‘á»‹nh sai láº§m. CÃ¡c thuáº­t toÃ¡n há»c model reward tá»« dá»¯ liá»‡u preference yÃªu cáº§u preference cá»§a con ngÆ°á»i Ä‘Æ°á»£c phÃ¢n phá»‘i theo tá»•ng cÃ³ trá»ng sá»‘ giáº£m dáº§n cá»§a reward hoáº·c partial return cá»§a má»—i Ä‘oáº¡n hÃ nh vi. Tuy nhiÃªn, nghiÃªn cá»©u gáº§n Ä‘Ã¢y (Knox et al., 2022) Ä‘áº·t cÃ¢u há»i vá» Ä‘iá»u nÃ y, cho ráº±ng thay vÃ o Ä‘Ã³ con ngÆ°á»i cung cáº¥p preference dá»±a

âˆ—Equal Contribution
MÃ£ nguá»“n cá»§a chÃºng tÃ´i Ä‘Æ°á»£c phÃ¡t hÃ nh táº¡i https://github.com/jhejna/cpl
1arXiv:2310.13639v3 [cs.LG] 30 Apr 2024

--- TRANG 2 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024
trÃªn regret cá»§a má»—i hÃ nh vi dÆ°á»›i policy tá»‘i Æ°u cá»§a hÃ m reward cá»§a expert. Trá»±c quan, phÃ¡n Ä‘oÃ¡n cá»§a con ngÆ°á»i cÃ³ thá»ƒ dá»±a trÃªn tÃ­nh tá»‘i Æ°u, thay vÃ¬ state vÃ  action nÃ o cÃ³ lÆ°á»£ng reward cao hÆ¡n. Káº¿t quáº£ lÃ , lÆ°á»£ng chÃ­nh xÃ¡c cáº§n há»c tá»« pháº£n há»“i cÃ³ thá»ƒ khÃ´ng pháº£i lÃ  reward, mÃ  thay vÃ o Ä‘Ã³ lÃ  hÃ m advantage tá»‘i Æ°u hoáº·c nÃ³i cÃ¡ch khÃ¡c, lÃ  regret Ä‘Æ°á»£c phá»§ Ä‘á»‹nh.

Trong giai Ä‘oáº¡n thá»© hai, cÃ¡c thuáº­t toÃ¡n RLHF hai giai Ä‘oáº¡n tá»‘i Æ°u hÃ³a hÃ m reward Ä‘Ã£ há»c tá»« giai Ä‘oáº¡n Ä‘áº§u tiÃªn vá»›i RL. Trong thá»±c táº¿, cÃ¡c thuáº­t toÃ¡n RL gáº·p pháº£i má»™t loáº¡t cÃ¡c thÃ¡ch thá»©c tá»‘i Æ°u hÃ³a xuáº¥t phÃ¡t tá»« temporal credit assignment, cháº³ng háº¡n nhÆ° high-variance cá»§a policy gradient (Marbach & Tsitsiklis, 2003) hoáº·c tÃ­nh khÃ´ng á»•n Ä‘á»‹nh cá»§a approximate dynamic programming (Van Hasselt et al., 2018). Do Ä‘Ã³, cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y háº¡n cháº¿ pháº¡m vi cá»§a mÃ¬nh Ä‘á»ƒ trÃ¡nh nhá»¯ng váº¥n Ä‘á» nÃ y. VÃ­ dá»¥, cÃ¡c ká»¹ thuáº­t RLHF cho LLM giáº£ Ä‘á»‹nh má»™t cÃ´ng thá»©c contextual bandit (Ouyang et al., 2022), trong Ä‘Ã³ policy nháº­n má»™t giÃ¡ trá»‹ reward duy nháº¥t Ä‘á»ƒ pháº£n há»“i má»™t truy váº¥n Ä‘Ã£ cho tá»›i ngÆ°á»i dÃ¹ng. Máº·c dÃ¹ Ä‘iá»u nÃ y lÃ m giáº£m nhu cáº§u long-horizon credit assignment, vÃ  do Ä‘Ã³ giáº£m high variance cá»§a policy gradient, trong thá»±c táº¿ cÃ¡c tÆ°Æ¡ng tÃ¡c cá»§a ngÆ°á»i dÃ¹ng vá»›i LLM lÃ  Ä‘a bÆ°á»›c vÃ  tuáº§n tá»±, vi pháº¡m giáº£ Ä‘á»‹nh bandit má»™t bÆ°á»›c. NhÆ° má»™t vÃ­ dá»¥ khÃ¡c, RLHF Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c váº¥n Ä‘á» robotics cÃ³ chiá»u tháº¥p dá»±a trÃªn state (Christiano et al., 2017; Sikchi et al., 2023a), má»™t thiáº¿t láº­p mÃ  approximate dynamic programming xuáº¥t sáº¯c, nhÆ°ng chÆ°a Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘áº¿n cÃ¡c domain continuous control cÃ³ chiá»u cao thá»±c táº¿ hÆ¡n vá»›i input áº£nh. NhÃ¬n chung, cÃ¡c phÆ°Æ¡ng phÃ¡p RLHF khÃ´ng chá»‰ giáº£ Ä‘á»‹nh khÃ´ng chÃ­nh xÃ¡c ráº±ng hÃ m reward má»™t mÃ¬nh thÃºc Ä‘áº©y preference cá»§a con ngÆ°á»i, mÃ  cÃ²n yÃªu cáº§u giáº£m thiá»ƒu cÃ¡c thÃ¡ch thá»©c tá»‘i Æ°u hÃ³a cá»§a RL báº±ng cÃ¡ch Ä‘Æ°a ra cÃ¡c giáº£ Ä‘á»‹nh háº¡n cháº¿ vá» tÃ­nh cháº¥t tuáº§n tá»± cá»§a váº¥n Ä‘á» hoáº·c dimensionality.

Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i giá»›i thiá»‡u má»™t há» phÆ°Æ¡ng phÃ¡p RLHF má»›i sá»­ dá»¥ng model preference dá»±a trÃªn regret, thay vÃ¬ model partial return thÆ°á»ng Ä‘Æ°á»£c cháº¥p nháº­n chá»‰ xem xÃ©t tá»•ng cá»§a reward. KhÃ´ng giá»‘ng nhÆ° model partial return, model dá»±a trÃªn regret trá»±c tiáº¿p cung cáº¥p thÃ´ng tin vá» policy tá»‘i Æ°u. Káº¿t quáº£ may máº¯n cá»§a Ä‘iá»u nÃ y lÃ  nÃ³ hoÃ n toÃ n loáº¡i bá» nhu cáº§u sá»­ dá»¥ng RL, cho phÃ©p chÃºng tÃ´i giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» RLHF trong framework MDP tá»•ng quÃ¡t vá»›i khÃ´ng gian state vÃ  action cÃ³ chiá»u cao. Insight chÃ­nh cá»§a chÃºng tÃ´i lÃ  káº¿t há»£p framework preference dá»±a trÃªn regret vá»›i nguyÃªn lÃ½ Maximum Entropy (MaxEnt), dáº«n Ä‘áº¿n má»™t bijection giá»¯a hÃ m advantage vÃ  policy. Báº±ng cÃ¡ch trao Ä‘á»•i tá»‘i Æ°u hÃ³a trÃªn advantage cho tá»‘i Æ°u hÃ³a trÃªn policy, chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°a ra má»™t objective supervised learning hoÃ n toÃ n mÃ  optimum cá»§a nÃ³ lÃ  policy tá»‘i Æ°u dÆ°á»›i reward cá»§a expert. ChÃºng tÃ´i gá»i phÆ°Æ¡ng phÃ¡p cá»§a mÃ¬nh lÃ  Contrastive Preference Learning do sá»± giá»‘ng nhau vá»›i cÃ¡c objective contrastive learning thÆ°á»ng Ä‘Æ°á»£c cháº¥p nháº­n.

CPL cÃ³ ba lá»£i Ã­ch chÃ­nh so vá»›i cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã³. Äáº§u tiÃªn, CPL cÃ³ thá»ƒ má»Ÿ rá»™ng tá»‘t nhÆ° supervised learning bá»Ÿi vÃ¬ nÃ³ chá»‰ sá»­ dá»¥ng cÃ¡c objective supervised Ä‘á»ƒ match advantage tá»‘i Æ°u mÃ  khÃ´ng cÃ³ policy gradient hoáº·c dynamic programming. Thá»© hai, CPL hoÃ n toÃ n off-policy, cho phÃ©p sá»­ dá»¥ng hiá»‡u quáº£ báº¥t ká»³ nguá»“n dá»¯ liá»‡u offline khÃ´ng tá»‘i Æ°u nÃ o. Cuá»‘i cÃ¹ng, CPL cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng cho cÃ¡c Markov Decision Process (MDP) tÃ¹y Ã½, cho phÃ©p há»c tá»« cÃ¡c query preference trÃªn dá»¯ liá»‡u tuáº§n tá»±. Theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i, khÃ´ng cÃ³ phÆ°Æ¡ng phÃ¡p nÃ o trÆ°á»›c Ä‘Ã¢y cho RLHF Ä‘á»“ng thá»i Ä‘Ã¡p á»©ng cáº£ ba nguyÃªn táº¯c nÃªu trÃªn. Äá»ƒ chá»©ng minh sá»± tuÃ¢n thá»§ cá»§a CPL vá»›i ba nguyÃªn táº¯c nÃªu trÃªn, chÃºng tÃ´i chá»‰ ra hiá»‡u quáº£ cá»§a nÃ³ trÃªn cÃ¡c váº¥n Ä‘á» Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh tuáº§n tá»± vá»›i dá»¯ liá»‡u off-policy khÃ´ng tá»‘i Æ°u vÃ  cÃ³ chiá»u cao. ÄÃ¡ng chÃº Ã½, chÃºng tÃ´i chá»‰ ra ráº±ng CPL cÃ³ thá»ƒ sá»­ dá»¥ng hiá»‡u quáº£ cÃ¹ng quy trÃ¬nh fine tuning RLHF nhÆ° cÃ¡c model dialog Ä‘á»ƒ há»c cÃ¡c policy manipulation má»Ÿ rá»™ng thá»i gian trong MetaWorld Benchmark. Cá»¥ thá»ƒ, chÃºng tÃ´i pretrain cÃ¡c policy sá»­ dá»¥ng supervised learning tá»« cÃ¡c quan sÃ¡t áº£nh cÃ³ chiá»u cao, trÆ°á»›c khi fine tune chÃºng vá»›i preference. KhÃ´ng cÃ³ dynamic programming hoáº·c policy gradient, CPL cÃ³ thá»ƒ match hiá»‡u suáº¥t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn RL trÆ°á»›c Ä‘Ã³. Äá»“ng thá»i, nÃ³ nhanh hÆ¡n 1.6Ã— vÃ  hiá»‡u quáº£ vá» tham sá»‘ gáº¥p bá»‘n láº§n. Khi sá»­ dá»¥ng dá»¯ liá»‡u preference dÃ y Ä‘áº·c hÆ¡n, CPL cÃ³ thá»ƒ vÆ°á»£t qua hiá»‡u suáº¥t cá»§a cÃ¡c baseline RL trÃªn 5 trong 6 task.

2 KIáº¾N THá»¨C CÆ  Báº¢N
ChÃºng tÃ´i xem xÃ©t váº¥n Ä‘á» reinforcement learning from human feedback (RLHF) tá»•ng quÃ¡t trong má»™t MDP khÃ´ng reward M/r = (S,A, p, Î³) vá»›i khÃ´ng gian state S, khÃ´ng gian action A, Ä‘á»™ng lá»±c chuyá»ƒn tiáº¿p p(st+1|st, at), vÃ  há»‡ sá»‘ giáº£m Î³. ChÃºng tÃ´i giáº£ Ä‘á»‹nh táº¥t cáº£ cÃ¡c state Ä‘á»u cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c bá»Ÿi má»™t policy nÃ o Ä‘Ã³. Má»¥c tiÃªu cá»§a RLHF lÃ  há»c má»™t policy Ï€(a|s) tá»‘i Ä‘a hÃ³a hÃ m reward rE(s, a) cá»§a ngÆ°á»i dÃ¹ng expert. Tuy nhiÃªn, vÃ¬ hÃ m reward khÃ´ng Ä‘Æ°á»£c cung cáº¥p trong MDP /r, nÃ³ pháº£i Ä‘Æ°á»£c suy ra tá»« preference cá»§a expert. ThÃ´ng thÆ°á»ng, preference cá»§a ngÆ°á»i dÃ¹ng sáº¯p xáº¿p hai Ä‘oáº¡n hÃ nh vi. Má»™t Ä‘oáº¡n Ä‘á»™ dÃ i k Ä‘Æ°á»£c kÃ½ hiá»‡u Ïƒ = (s1, a1, s2, a2, . . . , sk, ak). ChÃºng tÃ´i sá»­ dá»¥ng Ïƒ+ â‰» Ïƒâˆ’ Ä‘á»ƒ chá»‰ ra ráº±ng Ä‘oáº¡n Ïƒ+ Ä‘Æ°á»£c ngÆ°á»i dÃ¹ng Æ°a thÃ­ch hÆ¡n Ïƒâˆ’ khÃ´ng máº¥t tÃ­nh tá»•ng quÃ¡t vÃ  giáº£ Ä‘á»‹nh chÃºng tÃ´i Ä‘Æ°á»£c cung cáº¥p má»™t dataset Dpref = {(Ïƒ+i, Ïƒâˆ’i)}ni=1 cá»§a cÃ¡c preference nhÆ° váº­y trong Ä‘Ã³ Ïƒ+ â‰» Ïƒâˆ’.

--- TRANG 3 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024
RLHF Hai Giai Äoáº¡n TiÃªu Chuáº©n
Giai Äoáº¡n 1
Há»c Reward Giai Äoáº¡n 2
RLð‘Ÿðœƒ
ðœŽâˆ’ðœŽ+
Thuáº­t ToÃ¡n
RL
ðœ‹ðœƒ(ð‘Ž|ð‘ )Contrastive Preference Learning
Contrastive Learning
ðœŽâˆ’ðœŽ+
à·
ðœŽ+logðœ‹ðœƒ(ð‘Žð‘¡+|ð‘ ð‘¡+)
à·
ðœŽâˆ’logðœ‹ðœƒ(ð‘Žð‘¡âˆ’|ð‘ ð‘¡âˆ’)
Preference Dá»±a TrÃªn Regret
ð¿ð¶ð‘ƒð¿=âˆ’ð”¼logð‘ƒlogðœ‹ðœƒðœŽ+â‰»ðœŽâˆ’ð‘Ž+
ð‘Žâˆ’
ðœ‹ðœƒ(ð‘Ž|ð‘ )
ð‘ƒð´âˆ—ðœŽ+â‰»ðœŽâˆ’=ð‘’ÏƒðœŽ+ð´âˆ—(ð‘ ð‘¡+,ð‘Žð‘¡+)
ð‘’ÏƒðœŽ+ð´âˆ—(ð‘ ð‘¡+,ð‘Žð‘¡+)+ð‘’ÏƒðœŽâˆ’ð´âˆ—(ð‘ ð‘¡âˆ’,ð‘Žð‘¡âˆ’)

HÃ¬nh 1: Trong khi háº§u háº¿t cÃ¡c thuáº­t toÃ¡n RLHF sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p há»c reward hai giai Ä‘oáº¡n, sau Ä‘Ã³ lÃ  RL, CPL trá»±c tiáº¿p há»c má»™t policy sá»­ dá»¥ng objective contrastive. Äiá»u nÃ y Ä‘Æ°á»£c kÃ­ch hoáº¡t bá»Ÿi model preference regret.

Maximum Entropy Reinforcement Learning. Má»¥c tiÃªu cá»§a maximum-entropy reinforcement learning lÃ  há»c má»™t policy Ï€ tá»‘i Ä‘a hÃ³a causal entropy cá»§a nÃ³ ngoÃ i cumulative discounted return, dáº«n Ä‘áº¿n objective:
max Ï€ E_Ï€[âˆ‘_{t=0}^âˆž Î³^t(r(s_t, a_t) âˆ’ Î± log Ï€(a_t|s_t))]                    (1)

trong Ä‘Ã³ Î± lÃ  má»™t tham sá»‘ temperature. TÄƒng cÆ°á»ng hÃ m reward vá»›i má»™t thuáº­t ngá»¯ âˆ’log Î¼(a|s) bá»• sung cho phÃ¢n phá»‘i tham kháº£o Î¼(a|s) táº¡o ra objective KL-constrained Ä‘Æ°á»£c sá»­ dá»¥ng trong offline RL (Levine & Koltun, 2013; Garg et al., 2023) vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p RLHF ná»•i báº­t cho LLM (Ziegler et al., 2019; Ouyang et al., 2022). Máº·c dÃ¹ chÃºng tÃ´i Ã¡p dá»¥ng framework maximum entropy tiÃªu chuáº©n, phÆ°Æ¡ng phÃ¡p cá»§a chÃºng tÃ´i dá»… dÃ ng má»Ÿ rá»™ng Ä‘áº¿n thiáº¿t láº­p constrained. DÆ°á»›i policy Ï€ vÃ  hÃ m reward r, chÃºng tÃ´i kÃ½ hiá»‡u hÃ m state-value bá»Ÿi V^Ï€_r(s) vÃ  hÃ m state-action value bá»Ÿi Q^Ï€_r(s, a). HÃ m advantage, A^Ï€_r(s, a) â‰œ Q^Ï€_r(s, a) âˆ’ V^Ï€_r(s), Ä‘o lÆ°á»ng viá»‡c chá»n action a tá»‡ hÆ¡n bao nhiá»u so vá»›i hÃ nh Ä‘á»™ng theo Ï€. ChÃºng tÃ´i sá»­ dá»¥ng Ï€* nhÆ° viáº¿t táº¯t cho giáº£i phÃ¡p cá»§a Eq. (1) vá»›i hÃ m reward r_E, vÃ  viáº¿t cÃ¡c hÃ m value tÆ°Æ¡ng á»©ng cá»§a nÃ³ nhÆ° V*(s) vÃ  Q*(s, a) thay vÃ¬ V^{Ï€*}_{r_E} vÃ  Q^{Ï€*}_{r_E}. ChÃºng tÃ´i Ä‘o tÃ­nh tá»‘i Æ°u cá»§a hÃ nh vi trá»±c tiáº¿p báº±ng cÃ¡ch sá»­ dá»¥ng hÃ m advantage cá»§a Ï€*, A*(s, a).

Model Preference Regret (hoáº·c Advantage). Há»c Ï€* yÃªu cáº§u Ä‘áº·c tÃ­nh hÃ³a cÃ¡ch preference Ä‘Æ°á»£c táº¡o ra theo model preference P_E[Ïƒ+ â‰» Ïƒâˆ’], hoáº·c xÃ¡c suáº¥t mÃ  expert Æ°a thÃ­ch Ïƒ+ hÆ¡n Ïƒâˆ’. ThÃ´ng thÆ°á»ng, model preference Ä‘Æ°á»£c chá»n lÃ  phÃ¢n phá»‘i Boltzmann rational trÃªn discounted partial return cá»§a má»—i Ä‘oáº¡n, âˆ‘_{t=1}^k Î³^t r_E(s_t, a_t), trong Ä‘Ã³ r_E lÃ  hÃ m reward áº©n cá»§a expert. Tuy nhiÃªn, cÃ¡c model nhÆ° váº­y Ä‘Ã£ Ä‘Æ°á»£c chá»‰ ra lÃ  khÃ´ng nháº¥t quÃ¡n vá»›i preference thá»±c táº¿ cá»§a con ngÆ°á»i (Knox et al., 2022). VÃ­ dá»¥, xem xÃ©t má»™t sparse reward r_E(s, a) = 1{s=g}. Hai Ä‘oáº¡n khÃ´ng Ä‘áº¡t Ä‘Æ°á»£c má»¥c tiÃªu sáº½ cÃ³ cÃ¹ng partial return ngay cáº£ khi má»™t cÃ¡i di chuyá»ƒn vá» phÃ­a má»¥c tiÃªu g trong khi cÃ¡i khÃ¡c di chuyá»ƒn ra xa khá»i nÃ³. Sá»± khÃ´ng nháº¥t quÃ¡n nÃ y Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng cÃ¡ch xem xÃ©t preference Ä‘Æ°á»£c phÃ¢n phá»‘i theo phÃ¢n phá»‘i Boltzmann rational trÃªn negated discounted regret dÆ°á»›i r_E, hoáº·c âˆ’âˆ‘_{t=1}^k Î³^t(V*(s_t) âˆ’ Q*(s_t, a_t)). Trong framework nÃ y, preference cá»§a ngÆ°á»i dÃ¹ng chá»‰ ra ráº±ng má»™t Ä‘oáº¡n cÃ³ regret tháº¥p hÆ¡n so vá»›i policy tá»‘i Æ°u dá»± Ä‘á»‹nh cá»§a há». Táº­n dá»¥ng sá»± tÆ°Æ¡ng Ä‘Æ°Æ¡ng cá»§a negated regret vÃ  tá»•ng cÃ³ trá»ng sá»‘ giáº£m dáº§n cá»§a optimal advantage, chÃºng tÃ´i viáº¿t tÆ°Æ¡ng Ä‘Æ°Æ¡ng model preference dá»±a trÃªn regret nhÆ°

P_{A*}[Ïƒ+ â‰» Ïƒâˆ’] = exp âˆ‘_{Ïƒ+} Î³^t A*(s_t^+, a_t^+) / (exp âˆ‘_{Ïƒ+} Î³^t A*(s_t^+, a_t^+) + exp âˆ‘_{Ïƒâˆ’} Î³^t A*(s_t^âˆ’, a_t^âˆ’))     (2)

trong Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng viáº¿t táº¯t "+" vÃ  "âˆ’" Ä‘á»ƒ chá»‰ má»¥c cÃ¡c state vÃ  action cá»§a cÃ¡c Ä‘oáº¡n Ïƒ+ vÃ  Ïƒâˆ’. Trong pháº§n tiáº¿p theo, chÃºng tÃ´i sá»­ dá»¥ng model preference regret káº¿t há»£p vá»›i nguyÃªn lÃ½ maximum causal entropy Ä‘á»ƒ Ä‘Æ°a ra CPL.

3 CONTRASTIVE PREFERENCE LEARNING
Máº·c dÃ¹ nghiÃªn cá»©u gáº§n Ä‘Ã¢y Ä‘Ã£ chá»‰ ra ráº±ng preference cá»§a con ngÆ°á»i Ä‘Æ°á»£c mÃ´ hÃ¬nh hÃ³a tá»‘t hÆ¡n bá»Ÿi hÃ m advantage tá»‘i Æ°u hoáº·c regret, háº§u háº¿t cÃ¡c thuáº­t toÃ¡n RLHF hiá»‡n táº¡i giáº£ Ä‘á»‹nh ngÆ°á»£c láº¡i. Báº±ng cÃ¡ch há»c má»™t hÃ m reward vá»›i model preference sai láº§m vÃ  sau Ä‘Ã³ Ã¡p dá»¥ng RL, cÃ¡c phÆ°Æ¡ng phÃ¡p RLHF truyá»n thá»‘ng phÃ¡t sinh má»™t chi phÃ­ tÃ­nh toÃ¡n lá»›n, khÃ´ng cáº§n thiáº¿t (Knox et al., 2023). Má»¥c tiÃªu cá»§a chÃºng tÃ´i lÃ  Ä‘Æ°a ra cÃ¡c thuáº­t toÃ¡n RLHF Ä‘Æ¡n giáº£n vÃ  cÃ³ thá»ƒ má»Ÿ rá»™ng Ä‘Æ°á»£c xÃ¢y dá»±ng chuyÃªn biá»‡t cho model regret chÃ­nh xÃ¡c hÆ¡n cá»§a preference con ngÆ°á»i.

MÃ´ hÃ¬nh hÃ³a preference con ngÆ°á»i báº±ng regret khÃ´ng pháº£i lÃ  má»›i, nhÆ°ng cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y gáº·p má»™t sá»‘ thiáº¿u sÃ³t. Cá»¥ thá»ƒ, cÃ¡c thuáº­t toÃ¡n hiá»‡n táº¡i sá»­ dá»¥ng model preference regret ráº¥t dá»… vá»¡, vÃ¬ chÃºng dá»±a vÃ o viá»‡c Æ°á»›c tÃ­nh gradient Ä‘á»‘i vá»›i má»™t hÃ m reward thay Ä‘á»•i, do Ä‘Ã³ cho Ä‘áº¿n nay chá»‰ Ä‘Æ°á»£c xáº¥p xá»‰ báº±ng cÃ¡ch tÃ­nh toÃ¡n successor feature vÃ  giáº£ Ä‘á»‹nh biá»ƒu diá»…n tuyáº¿n tÃ­nh hoáº·c tabular chÃ­nh xÃ¡c cá»§a hÃ m reward expert r_E (Knox et al., 2022; 2023). Do Ä‘Ã³, cÃ¡c thuáº­t toÃ¡n nÃ y cÃ³ váº» khÃ´ng phÃ¹ há»£p cho cÃ¡c tÃ¬nh huá»‘ng phá»©c táº¡p vÆ°á»£t ra ngoÃ i cÃ¡c mÃ´i trÆ°á»ng grid world Ä‘Æ¡n giáº£n mÃ  chÃºng Ä‘Ã£ Ä‘Æ°á»£c kiá»ƒm tra.

Ã tÆ°á»Ÿng chÃ­nh cá»§a phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i ráº¥t Ä‘Æ¡n giáº£n: chÃºng tÃ´i nháº­n ra ráº±ng hÃ m advantage, Ä‘Æ°á»£c sá»­ dá»¥ng trong model preference regret, cÃ³ thá»ƒ dá»… dÃ ng Ä‘Æ°á»£c thay tháº¿ báº±ng log-probability cá»§a policy khi sá»­ dá»¥ng framework maximum entropy reinforcement learning. Tuy nhiÃªn, lá»£i Ã­ch cá»§a sá»± thay tháº¿ Ä‘Æ¡n giáº£n nÃ y lÃ  ráº¥t lá»›n. Viá»‡c sá»­ dá»¥ng log-probability cá»§a policy vÆ°á»£t qua nhu cáº§u há»c hÃ m advantage hoáº·c váº­t lá»™n vá»›i cÃ¡c thÃ¡ch thá»©c tá»‘i Æ°u hÃ³a liÃªn quan Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n giá»‘ng RL. TÃ³m láº¡i, Ä‘iá»u nÃ y cho phÃ©p chÃºng tÃ´i khÃ´ng chá»‰ Ä‘Ã³n nháº­n má»™t model preference regret phÃ¹ há»£p hÆ¡n, mÃ  cÃ²n dá»±a hoÃ n toÃ n vÃ o supervised learning khi há»c tá»« pháº£n há»“i con ngÆ°á»i.

Trong pháº§n nÃ y, trÆ°á»›c tiÃªn chÃºng tÃ´i Ä‘Æ°a ra objective CPL vÃ  chá»‰ ra ráº±ng nÃ³ há»™i tá»¥ Ä‘áº¿n policy tá»‘i Æ°u cho r_E vá»›i dá»¯ liá»‡u khÃ´ng giá»›i háº¡n. Sau Ä‘Ã³, chÃºng tÃ´i rÃºt ra cÃ¡c má»‘i liÃªn há»‡ giá»¯a CPL vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p supervised-learning khÃ¡c. Cuá»‘i cÃ¹ng, chÃºng tÃ´i cung cáº¥p cÃ¡c cÃ´ng thá»©c Ä‘á»ƒ sá»­ dá»¥ng CPL trong thá»±c táº¿. CÃ¡c thuáº­t toÃ¡n cá»§a chÃºng tÃ´i lÃ  cÃ¡c vÃ­ dá»¥ Ä‘áº§u tiÃªn cá»§a má»™t lá»›p phÆ°Æ¡ng phÃ¡p má»›i cho cÃ¡c váº¥n Ä‘á» Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh tuáº§n tá»± mÃ  trá»±c tiáº¿p há»c má»™t policy tá»« preference dá»±a trÃªn regret mÃ  khÃ´ng cáº§n RL, lÃ m cho chÃºng hiá»‡u quáº£ hÆ¡n nhiá»u.

3.1 Tá»ª OPTIMAL ADVANTAGE Äáº¾N OPTIMAL POLICY
DÆ°á»›i model preference regret, dataset preference D_pref cá»§a chÃºng tÃ´i chá»©a thÃ´ng tin vá» hÃ m optimal advantage A*(s, a), cÃ³ thá»ƒ Ä‘Æ°á»£c nhÃ¬n nháº­n trá»±c quan nhÆ° má»™t thÆ°á»›c Ä‘o viá»‡c action a Ä‘Ã£ cho tá»‡ hÆ¡n bao nhiá»u so vá»›i má»™t action Ä‘Æ°á»£c táº¡o ra bá»Ÿi policy tá»‘i Æ°u táº¡i state s. Do Ä‘Ã³, cÃ¡c action tá»‘i Ä‘a hÃ³a optimal advantage theo Ä‘á»‹nh nghÄ©a lÃ  cÃ¡c action tá»‘i Æ°u vÃ  viá»‡c há»c hÃ m optimal advantage tá»« preference trá»±c quan nÃªn cho phÃ©p chÃºng tÃ´i trÃ­ch xuáº¥t policy tá»‘i Æ°u.

PhÆ°Æ¡ng phÃ¡p naive. Khi Ä‘Æ°á»£c trÃ¬nh bÃ y vá»›i D_pref, ngÆ°á»i ta cÃ³ thá»ƒ naive tuÃ¢n theo cÃ´ng thá»©c RLHF reward modeling tiÃªu chuáº©n, nhÆ°ng vá»›i advantage. Äiá»u nÃ y sáº½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c tá»‘i Æ°u hÃ³a má»™t advantage cÃ³ tham sá»‘ A_Î¸ Ä‘á»ƒ tá»‘i Ä‘a hÃ³a log likelihood cá»§a D_pref Ä‘Æ°á»£c cung cáº¥p model preference trong Eq. (2), hoáº·c max_{A_Î¸} E_{(Ïƒ+,Ïƒâˆ’)âˆ¼D_pref}[log P_{A_Î¸}[Ïƒ+ â‰» Ïƒâˆ’]], trong Ä‘Ã³ P_{A_Î¸} lÃ  model preference Ä‘Æ°á»£c táº¡o ra bá»Ÿi hÃ m advantage Ä‘Ã£ há»c. Má»™t khi má»™t hÃ m advantage phÃ¹ há»£p vá»›i dá»¯ liá»‡u preference Ä‘Æ°á»£c há»c, nÃ³ cÃ³ thá»ƒ Ä‘Æ°á»£c chÆ°ng cáº¥t thÃ nh má»™t policy cÃ³ tham sá»‘. Thoáº¡t nhÃ¬n, cÃ³ váº» nhÆ° phÆ°Æ¡ng phÃ¡p hai bÆ°á»›c Ä‘Æ¡n giáº£n nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ khÃ´i phá»¥c policy tá»‘i Æ°u tá»« dá»¯ liá»‡u preference. Tuy nhiÃªn, hÃ³a ra viá»‡c há»c má»™t hÃ m advantage Bellman-consistent lÃ  khÃ´ng táº§m thÆ°á»ng trong cáº£ standard vÃ  MaxEnt RL, lÃ m cho viá»‡c há»c má»™t hÃ m advantage trung gian há»£p lá»‡ khÃ´ng chá»‰ khÃ´ng cáº§n thiáº¿t, mÃ  cÃ²n khÃ³ khÄƒn hÆ¡n trong thá»±c táº¿.

Loáº¡i bá» nhu cáº§u há»c advantage. Trong maximum entropy RL, Ziebart (2010) Ä‘Ã£ chá»‰ ra ráº±ng má»‘i quan há»‡ sau Ä‘Ã¢y giá»¯a hÃ m optimal advantage vÃ  optimal policy tá»“n táº¡i:
Ï€*(a|s) = e^{A*_r(s,a)/Î±}.

Äiá»u nÃ y cÃ³ nghÄ©a lÃ  Ä‘á»ƒ má»™t hÃ m advantage Ä‘Ã£ há»c lÃ  tá»‘i Æ°u, nÃ³ pháº£i Ä‘Æ°á»£c chuáº©n hÃ³a, tá»©c lÃ  âˆ«_A e^{A*(s,a)/Î±} da = 1. Viá»‡c thá»±c thi rÃ ng buá»™c nÃ y lÃ  khÃ´ng thá»ƒ xá»­ lÃ½ Ä‘Æ°á»£c, Ä‘áº·c biá»‡t trong khÃ´ng gian liÃªn tá»¥c vá»›i cÃ¡c neural network lá»›n, lÃ m cho viá»‡c há»c A_Î¸ má»™t cÃ¡ch naive thÃ´ng qua maximum likelihood estimation khÃ³ khÄƒn.

Tuy nhiÃªn, ngÆ°á»i ta cÃ³ thá»ƒ thay vÃ o Ä‘Ã³ nháº­n tháº¥y ráº±ng phÆ°Æ¡ng trÃ¬nh trÃªn thiáº¿t láº­p má»™t bijection giá»¯a hÃ m advantage A*_r vÃ  policy Ï€*, cá»¥ thá»ƒ lÃ  hÃ m optimal advantage tá»‰ lá»‡ thuáº­n vá»›i log-likelihood cá»§a optimal policy:
A*_r(s, a) = Î± log Ï€*(a|s).                                                      (3)

Äiá»u nÃ y cÃ³ nghÄ©a lÃ  thay vÃ¬ há»c hÃ m optimal advantage, chÃºng ta cÃ³ thá»ƒ trá»±c tiáº¿p há»c optimal policy. ÄÆ°á»£c cung cáº¥p preference Ä‘Æ°á»£c phÃ¢n phá»‘i theo hÃ m optimal advantage cho hÃ m reward expert r_E, chÃºng ta cÃ³ thá»ƒ viáº¿t model preference theo optimal policy Ï€* báº±ng cÃ¡ch thay tháº¿ Eq. (3) vÃ o Eq. (2) nhÆ° sau,

P_{A*}[Ïƒ+ â‰» Ïƒâˆ’] = exp âˆ‘_{Ïƒ+} Î³^t Î± log Ï€*(a_t^+|s_t^+) / (exp âˆ‘_{Ïƒ+} Î³^t Î± log Ï€*(a_t^+|s_t^+) + exp âˆ‘_{Ïƒâˆ’} Î³^t Î± log Ï€*(a_t^âˆ’|s_t^âˆ’))    (4)

--- TRANG 4 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024
Do Ä‘Ã³, framework maximum entropy Ä‘Ã£ dáº«n Ä‘áº¿n má»™t model preference con ngÆ°á»i chá»‰ theo optimal policy Ï€*. Sá»­ dá»¥ng dáº¡ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng nÃ y cá»§a model preference dá»±a trÃªn advantage, chÃºng ta cÃ³ thá»ƒ trá»±c tiáº¿p tá»‘i Æ°u hÃ³a má»™t policy Ä‘Ã£ há»c Ï€_Î¸ Ä‘á»ƒ match model preference thÃ´ng qua maximum likelihood vá»›i objective convex sau:

L_CPL(Ï€_Î¸, D_pref) = E_{(Ïƒ+,Ïƒâˆ’)âˆ¼D_pref}[âˆ’log (exp âˆ‘_{Ïƒ+} Î³^t Î± log Ï€_Î¸(a_t^+|s_t^+))/(exp âˆ‘_{Ïƒ+} Î³^t Î± log Ï€_Î¸(a_t^+|s_t^+) + exp âˆ‘_{Ïƒâˆ’} Î³^t Î± log Ï€_Î¸(a_t^âˆ’|s_t^âˆ’))]    (5)

Giáº£ Ä‘á»‹nh sá»©c máº¡nh biá»ƒu diá»…n Ä‘áº§y Ä‘á»§, táº¡i sá»± há»™i tá»¥ Ï€_Î¸ sáº½ mÃ´ hÃ¬nh hÃ³a hoÃ n háº£o preference cá»§a ngÆ°á»i dÃ¹ng, vÃ  do Ä‘Ã³ khÃ´i phá»¥c chÃ­nh xÃ¡c Ï€* dÆ°á»›i model preference dá»±a trÃªn advantage Ä‘Æ°á»£c cung cáº¥p má»™t lÆ°á»£ng dá»¯ liá»‡u preference khÃ´ng giá»›i háº¡n. Cá»¥ thá»ƒ, trong Phá»¥ lá»¥c A, chÃºng tÃ´i chá»©ng minh Äá»‹nh lÃ½ sau:

Äá»‹nh lÃ½ 1. Giáº£ Ä‘á»‹nh má»™t sá»‘ lÆ°á»£ng preference khÃ´ng giá»›i háº¡n Ä‘Æ°á»£c táº¡o ra tá»« má»™t model preference regret rational cÃ³ nhiá»…u vá»›i hÃ m advantage expert A*. CPL khÃ´i phá»¥c optimal policy Ï€* tÆ°Æ¡ng á»©ng vá»›i reward r_E.

Chá»©ng minh nÃ y dá»±a vÃ o bijection giá»¯a hÃ m optimal advantage vÃ  policy trong maximum entropy RL vÃ  thá»±c táº¿ ráº±ng model preference regret cÃ³ thá»ƒ nháº­n dáº¡ng Ä‘Æ°á»£c (Knox et al., 2022), cÃ³ nghÄ©a lÃ  objective cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c loss báº±ng khÃ´ng.

Lá»£i Ã­ch cá»§a viá»‡c há»c trá»±c tiáº¿p policy. Viá»‡c há»c trá»±c tiáº¿p Ï€ theo cÃ¡ch nÃ y cÃ³ má»™t sá»‘ lá»£i Ã­ch, cáº£ thá»±c táº¿ vÃ  lÃ½ thuyáº¿t. CÃ³ láº½ rÃµ rÃ ng nháº¥t, viá»‡c há»c trá»±c tiáº¿p policy vÆ°á»£t qua nhu cáº§u há»c báº¥t ká»³ hÃ m nÃ o khÃ¡c, nhÆ° hÃ m reward hoáº·c value function. Äiá»u nÃ y lÃ m cho CPL cá»±c ká»³ Ä‘Æ¡n giáº£n so vá»›i cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y. Khi má»Ÿ rá»™ng Ä‘áº¿n cÃ¡c model lá»›n hÆ¡n, chá»‰ há»c policy lÃ m giáº£m cáº£ Ä‘á»™ phá»©c táº¡p vÃ  chi phÃ­ tÃ­nh toÃ¡n. Thá»© hai, nhÆ° Ä‘Æ°á»£c chá»‰ ra bá»Ÿi cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y (Christiano et al., 2017; Hejna & Sadigh, 2023), viá»‡c há»c reward cÃ³ thá»ƒ bá»‹ tá»•n háº¡i bá»Ÿi tÃ­nh báº¥t biáº¿n cá»§a model preference Boltzmann rational (Eq. (2)) Ä‘á»‘i vá»›i cÃ¡c shift; tá»©c lÃ  viá»‡c thÃªm má»™t háº±ng sá»‘ vÃ o má»—i exponent khÃ´ng thay Ä‘á»•i P[Ïƒ+ â‰» Ïƒâˆ’]. Trong CPL, rÃ ng buá»™c phÃ¢n phá»‘i cá»§a policy (Ï€_Î¸(a|s) â‰¥ 0 vá»›i má»i a vÃ  âˆ«_A Ï€_Î¸(a|s)da = 1) tá»± Ä‘á»™ng kháº¯c phá»¥c váº¥n Ä‘á» nÃ y, vÃ¬ viá»‡c thÃªm má»™t háº±ng sá»‘ lÃ m cho âˆ«_A Ï€_Î¸(a|s)da â‰  1. Cuá»‘i cÃ¹ng, theo cÃ¡c láº­p luáº­n trÆ°á»›c Ä‘Ã¢y, rÃ ng buá»™c phÃ¢n phá»‘i cá»§a policy Ä‘áº£m báº£o ráº±ng âˆ«_A e^{A_Î¸(s,a)/Î±}da = 1. Do Ä‘Ã³, cÃ³ thá»ƒ chá»‰ ra ráº±ng hÃ m advantage ngáº§m Ä‘á»‹nh Ä‘Ã£ há»c cá»§a CPL luÃ´n lÃ  hÃ m optimal advantage cho má»™t hÃ m reward nÃ o Ä‘Ã³. ChÃºng tÃ´i gá»i thuá»™c tÃ­nh nÃ y, Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a bÃªn dÆ°á»›i, lÃ  consistency vÃ  chá»©ng minh Má»‡nh Ä‘á» sau trong Phá»¥ lá»¥c A.

Äá»‹nh nghÄ©a 1. Má»™t hÃ m advantage A(s, a) lÃ  consistent náº¿u tá»“n táº¡i má»™t hÃ m reward r(s, a) nÃ o Ä‘Ã³ mÃ  A lÃ  optimal advantage, hoáº·c A(s, a) = A*_r(s, a).

Má»‡nh Ä‘á» 1. CPL há»c má»™t hÃ m advantage consistent.

Háº­u quáº£ cá»§a Ä‘iá»u nÃ y lÃ  khÃ´ng quan trá»ng lÆ°á»£ng dá»¯ liá»‡u preference Ä‘Æ°á»£c sá»­ dá»¥ng, CPL sáº½ luÃ´n há»c optimal policy cho má»™t hÃ m reward nÃ o Ä‘Ã³, vÃ  viá»‡c thÃªm dá»¯ liá»‡u preference bá»• sung chá»‰ cáº£i thiá»‡n Æ°á»›c tÃ­nh ngáº§m Ä‘á»‹nh cá»§a r_E.

Káº¿t ná»‘i vá»›i Contrastive Learning. Khi Ä‘Æ°a ra CPL, chÃºng tÃ´i cÃ³ Ã½ Ä‘á»‹nh chá»n kÃ½ hiá»‡u cÃ¡c Ä‘oáº¡n hÃ nh vi Ä‘Æ°á»£c Æ°a thÃ­ch vÃ  khÃ´ng Ä‘Æ°á»£c Æ°a thÃ­ch báº±ng "+" vÃ  "-" Ä‘á»ƒ lÃ m ná»•i báº­t sá»± tÆ°Æ¡ng Ä‘á»“ng giá»¯a CPL vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p contrastive learning. Máº·c dÃ¹ má»™t sá»‘ phÆ°Æ¡ng phÃ¡p RLHF hai giai Ä‘oáº¡n Ä‘Ã£ rÃºt ra cÃ¡c káº¿t ná»‘i giá»¯a giai Ä‘oáº¡n há»c reward cá»§a há» vÃ  contrastive learning (Kang et al., 2023), CPL trá»±c tiáº¿p sá»­ dá»¥ng má»™t objective contrastive cho viá»‡c há»c policy. Cá»¥ thá»ƒ, Eq. (5) lÃ  má»™t instance cá»§a objective Noise Contrastive Estimation (Gutmann & HyvÃ¤rinen, 2010) trong Ä‘Ã³ Ä‘iá»ƒm cá»§a má»™t Ä‘oáº¡n lÃ  tá»•ng cÃ³ trá»ng sá»‘ giáº£m dáº§n cá»§a log-probability dÆ°á»›i policy, vÃ­ dá»¥ positive lÃ  Ïƒ+ vÃ  negative lÃ  Ïƒâˆ’. Trong phá»¥ lá»¥c chÃºng tÃ´i chá»‰ ra ráº±ng khi Ä‘Æ°á»£c Ã¡p dá»¥ng cho dá»¯ liá»‡u ranking sá»­ dá»¥ng Plackett-Luce Model, CPL khÃ´i phá»¥c objective InfoNCE tá»« Oord et al. (2018) trong Ä‘Ã³ cÃ¡c vÃ­ dá»¥ negative lÃ  táº¥t cáº£ cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c xáº¿p háº¡ng dÆ°á»›i Ä‘oáº¡n positive. Má»™t cÃ¡ch hiá»‡u quáº£, CPL Ä‘Ã£ hoÃ n toÃ n trao Ä‘á»•i objective reinforcement learning cho má»™t objective supervised, representation learning trong khi váº«n há»™i tá»¥ Ä‘áº¿n policy tá»‘i Æ°u. VÃ¬ thÃ nh cÃ´ng Ä‘Ã¡ng ká»ƒ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c khi Ã¡p dá»¥ng cÃ¡c objective contrastive learning cho cÃ¡c dataset vÃ  neural network quy mÃ´ lá»›n (Chen et al., 2020; He et al., 2020; Radford et al., 2021), chÃºng tÃ´i mong Ä‘á»£i CPL má»Ÿ rá»™ng cáº¡nh tranh vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p RLHF sá»­ dá»¥ng thuáº­t toÃ¡n RL truyá»n thá»‘ng.

3.2 CÃC CÃ‚N NHáº®C THá»°C Táº¾
Framework Contrastive Preference Learning cung cáº¥p má»™t hÃ m loss tá»•ng quÃ¡t Ä‘á»ƒ há»c policy tá»« preference dá»±a trÃªn advantage, tá»« Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°a ra nhiá»u thuáº­t toÃ¡n. Trong pháº§n nÃ y, chÃºng tÃ´i chi tiáº¿t cÃ¡c cÃ¢n nháº¯c thá»±c táº¿ cho má»™t instance cá»¥ thá»ƒ cá»§a framework CPL mÃ  chÃºng tÃ´i tháº¥y hoáº¡t Ä‘á»™ng tá»‘t trong thá»±c táº¿. Trong phá»¥ lá»¥c, chÃºng tÃ´i bao gá»“m má»™t sá»‘ instance cá»§a CPL cho cÃ¡c loáº¡i dá»¯ liá»‡u khÃ¡c nhau vÃ  cÃ¡c regularizer conservative.

CPL vá»›i Dá»¯ liá»‡u Offline Há»¯u háº¡n. Máº·c dÃ¹ CPL há»™i tá»¥ Ä‘áº¿n policy tá»‘i Æ°u vá»›i dá»¯ liá»‡u preference khÃ´ng giá»›i háº¡n, trong thá»±c táº¿ chÃºng ta thÆ°á»ng quan tÃ¢m Ä‘áº¿n viá»‡c há»c tá»« cÃ¡c dataset offline há»¯u háº¡n. Trong thiáº¿t láº­p nÃ y, cÃ¡c policy ngoáº¡i suy quÃ¡ nhiá»u ngoÃ i support cá»§a dataset hoáº¡t Ä‘á»™ng kÃ©m vÃ¬ chÃºng thá»±c hiá»‡n cÃ¡c action dáº«n Ä‘áº¿n cÃ¡c state out of distribution. Giá»‘ng nhÆ° nhiá»u objective dá»±a trÃªn preference khÃ¡c, objective cá»§a CPL khÃ´ng strictly convex (Phá»¥ lá»¥c A.3). Do Ä‘Ã³, nhiá»u policy, ngay cáº£ nhá»¯ng policy cÃ³ trá»ng sá»‘ cao trÃªn cÃ¡c action khÃ´ng cÃ³ trong dataset, cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c cÃ¹ng optima cá»§a Eq. (5). ChÃºng tÃ´i chá»©ng minh Ä‘iá»u nÃ y báº±ng cÃ¡ch cÃ´ng thá»©c hÃ³a CPL nhÆ° má»™t váº¥n Ä‘á» logistic regression. Äáº·t policy Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi má»™t vector má»™t chiá»u Ï€ âˆˆ R^{|SÃ—A|}. Sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c Ä‘oáº¡n positive vÃ  negative, âˆ‘_{Ïƒ+} Î³^t Î± log Ï€_Î¸(a_t^+|s_t^+) - âˆ‘_{Ïƒ+} Î³^t Î± log Ï€_Î¸(a_t^âˆ’|s_t^âˆ’) cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t láº¡i nhÆ° má»™t dot-product giá»¯a Ï€ vÃ  má»™t vector "comparison" x, mÃ  cÃ¡c giÃ¡ trá»‹ cá»§a nÃ³ lÃ  Î³^t, -Î³^t, hoáº·c 0 chá»‰ ra thÃ nh viÃªn cá»§a comparison Ïƒ+ â‰» Ïƒâˆ’. Sá»­ dá»¥ng hÃ m logistic, logistic(z) = 1/(1+e^{-z}), chÃºng tÃ´i viáº¿t láº¡i objective CPL trong trÆ°á»ng há»£p há»¯u háº¡n nhÆ°

L_CPL(Ï€_Î¸, D_pref) = -âˆ‘_{i=1}^{|D_pref|} log logistic(Î± x_i^T log Ï€(a|s)), trong Ä‘Ã³ x_i[s, a] = {Î³^t náº¿u Ïƒ_{i,t}^+ = (s, a), -Î³^t náº¿u Ïƒ_{i,t}^âˆ’ = (s, a), 0 ngÆ°á»£c láº¡i}

trong Ä‘Ã³ Ïƒ_{i,t}^+ kÃ½ hiá»‡u timestep thá»© t cá»§a Ä‘oáº¡n Ä‘Æ°á»£c Æ°a thÃ­ch tá»« comparison thá»© i trong D_pref. ChÃºng ta cÃ³ thá»ƒ suy luáº­n vá» táº­p há»£p táº¥t cáº£ cÃ¡c policy táº¡o ra cÃ¹ng CPL loss báº±ng cÃ¡ch táº­p há»£p táº¥t cáº£ cÃ¡c comparison vector thÃ nh má»™t ma tráº­n X, trong Ä‘Ã³ hÃ ng thá»© i cá»§a X lÃ  vector x_i cho comparison thá»© i trong dataset. Báº¥t ká»³ thay Ä‘á»•i nÃ o Ä‘á»‘i vá»›i log Ï€ trong null space cá»§a X khÃ´ng cÃ³ áº£nh hÆ°á»Ÿng Ä‘áº¿n logit cá»§a hÃ m logistic, vÃ  do Ä‘Ã³ khÃ´ng cÃ³ áº£nh hÆ°á»Ÿng Ä‘áº¿n loss. Trong thá»±c táº¿, |S Ã— A| >> n, lÃ m cho null space cá»§a X thÆ°á»ng lÃ  non-trivial sao cho cÃ³ nhiá»u minimizer cá»§a CPL loss, má»™t sá»‘ trong Ä‘Ã³ cÃ³ thá»ƒ Ä‘áº·t xÃ¡c suáº¥t cao trÃªn cÃ¡c cáº·p state-action khÃ´ng cÃ³ trong dataset. Trong Phá»¥ lá»¥c A.3 chÃºng tÃ´i cung cáº¥p cÃ¡c construction cá»§a X trong Ä‘Ã³ Ä‘iá»u nÃ y Ä‘Ãºng. Tiáº¿p theo, chÃºng tÃ´i chá»‰ ra cÃ¡ch váº¥n Ä‘á» nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i quyáº¿t báº±ng cÃ¡ch káº¿t há»£p regularization vÃ o objective CPL.

Regularization. Trong cÃ¡c thiáº¿t láº­p há»¯u háº¡n, chÃºng ta muá»‘n chá»n policy tá»‘i thiá»ƒu hÃ³a hÃ m loss CPL trong khi Ä‘áº·t likelihood cao hÆ¡n trÃªn cÃ¡c action trong dataset. Äá»ƒ hoÃ n thÃ nh Ä‘iá»u nÃ y, chÃºng tÃ´i sá»­a Ä‘á»•i Eq. (5) vá»›i má»™t regularizer conservative gÃ¡n loss tháº¥p hÆ¡n khi policy cÃ³ likelihood cao hÆ¡n trÃªn cÃ¡c action trong D_pref, giá»¯ nÃ³ in-distribution. Máº·c dÃ¹ cÃ³ nhiá»u lá»±a chá»n regularizer cÃ³ thá»ƒ, chÃºng tÃ´i sá»­ dá»¥ng regularizer "bias" asymmetric Ä‘Æ°á»£c Ä‘iá»u chá»‰nh tá»« An et al. (2023) vÃ¬ nÃ³ hoáº¡t Ä‘á»™ng tá»‘t nháº¥t trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i. Trong objective cá»§a chÃºng tÃ´i, regularizer bias down-weight cÃ¡c Ä‘oáº¡n negative bá»Ÿi Î» âˆˆ (0,1) nhÆ° sau:

L_CPL^{(Î»)}(Ï€_Î¸, D_pref) = E_{D_pref}[âˆ’log (exp âˆ‘_{Ïƒ+} Î³^t Î± log Ï€_Î¸(a_t^+|s_t^+))/(exp âˆ‘_{Ïƒ+} Î³^t Î± log Ï€_Î¸(a_t^+|s_t^+) + exp Î»âˆ‘_{Ïƒâˆ’} Î³^t Î± log Ï€_Î¸(a_t^âˆ’|s_t^âˆ’))]    (6)

Náº¿u policy Ä‘áº·t trá»ng sá»‘ nhiá»u hÆ¡n trÃªn cÃ¡c action trong dataset, log Ï€_Î¸(a|s) sáº½ tÄƒng. Trong model Boltzmann tiÃªu chuáº©n, viá»‡c tÄƒng log-probability cá»§a cáº£ Ä‘oáº¡n positive vÃ  negative cÃ¹ng má»™t lÆ°á»£ng sáº½ khÃ´ng cÃ³ áº£nh hÆ°á»Ÿng Ä‘áº¿n loss. Tuy nhiÃªn, bias cÃ¢n nháº¯c Ã­t hÆ¡n log-probability tÄƒng cá»§a cÃ¡c Ä‘oáº¡n negative, cuá»‘i cÃ¹ng lÃ m giáº£m loss. Do Ä‘Ã³, trong khi má»™t minimizer cá»§a hÃ m loss CPL vanilla cÃ³ thá»ƒ Ä‘áº·t xÃ¡c suáº¥t cao trÃªn cÃ¡c action chÆ°a tháº¥y, Eq. (6) Ä‘Æ°á»£c tá»‘i thiá»ƒu hÃ³a vá»›i trá»ng sá»‘ cao hÆ¡n trÃªn cÃ¡c action in-distribution. Äiá»u nÃ y Ä‘Æ°á»£c náº¯m báº¯t chÃ­nh thá»©c bá»Ÿi má»‡nh Ä‘á» sau, cho tháº¥y ráº±ng, Ä‘á»‘i vá»›i má»™t policy cá»‘ Ä‘á»‹nh, L_CPL^{(Î»)} tháº¥p hÆ¡n khi policy Ä‘áº·t likelihood cao hÆ¡n trÃªn cÃ¡c action trong dataset so vá»›i cÃ¡c comparison khÃ¡c vá»›i cÃ¹ng CPL Loss.

Má»‡nh Ä‘á» 2. Xem xÃ©t má»™t comparison Ïƒ+ â‰» Ïƒâˆ’ tá»« D_pref vÃ  má»™t comparison tÃ¹y Ã½ Ïƒ'+ â‰» Ïƒ'âˆ’ sao cho L_CPL(Ï€, Ïƒ+ â‰» Ïƒâˆ’) = L_CPL(Ï€, Ïƒ'+ â‰» Ïƒ'âˆ’) cho má»™t policy Ï€ cá»‘ Ä‘á»‹nh. Náº¿u âˆ‘_{Ïƒ+} Î³^t log Ï€(a_t^+|s_t^+) > âˆ‘_{Ïƒ'+} Î³^t log Ï€(a_t^+|s_t^+), thÃ¬ L_CPL^{(Î»)}(Ï€, Ïƒ+ â‰» Ïƒâˆ’) < L_CPL^{(Î»)}(Ï€, Ïƒ'+ â‰» Ïƒ'âˆ’).

Vá» cÆ¡ báº£n, Ä‘iá»u nÃ y cho tháº¥y ráº±ng regularizer bias phÃ¡ vá»¡ ties trong hÃ m loss CPL báº±ng cÃ¡ch pháº¡t likelihood tháº¥p hÆ¡n. ChÃºng tÃ´i chá»©ng minh Ä‘iá»u nÃ y, cÃ¹ng vá»›i má»™t phiÃªn báº£n tá»•ng quÃ¡t hÆ¡n, trong Phá»¥ lá»¥c A.4. Trong Phá»¥ lá»¥c B chÃºng tÃ´i cÅ©ng xem xÃ©t cÃ¡c biáº¿n thá»ƒ CPL vá»›i cÃ¡c dáº¡ng conservative regularization khÃ¡c.

Pretraining. Pre-training policy Ï€_Î¸ vá»›i behavior cloning (BC) lÃ  má»™t thá»±c hÃ nh phá»• biáº¿n trong RLHF (Ouyang et al., 2022). Do Ä‘Ã³, trÆ°á»›c khi fine-tuning vá»›i preference sá»­ dá»¥ng loss CPL, chÃºng tÃ´i Ä‘Ã£ huáº¥n luyá»‡n policy sá»­ dá»¥ng objective BC maximum likelihood tiÃªu chuáº©n, min_Î¸ E_{(s,a)âˆ¼D}[log Ï€_Î¸(a|s)]. Máº·c dÃ¹ D cÃ³ thá»ƒ lÃ  báº¥t ká»³ dataset nÃ o, chÃºng tÃ´i Ä‘Ã£ chá»n D_pref. ChÃºng tÃ´i tháº¥y ráº±ng Ä‘iá»u nÃ y giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t trong má»™t sá»‘ trÆ°á»ng há»£p, nhÆ°ng tá»•n háº¡i trong nhá»¯ng trÆ°á»ng há»£p khÃ¡c (Phá»¥ lá»¥c C). ChÃºng tÃ´i cho ráº±ng pre-training giÃºp Ã­ch khi má»™t policy gáº§n vá»›i phÃ¢n phá»‘i dá»¯ liá»‡u lÃ  mong muá»‘n, Ä‘áº·c biá»‡t khi cÃ¡c action out-of-distribution cÃ³ háº¡i.

--- TRANG 5 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

4 THÃ NGHIá»†M
Trong pháº§n nÃ y, chÃºng tÃ´i giáº£i quyáº¿t cÃ¡c cÃ¢u há»i sau vá» CPL: Äáº§u tiÃªn, CPL cÃ³ hiá»‡u quáº£ trong viá»‡c fine-tuning policy tá»« preference dá»±a trÃªn regret khÃ´ng? Thá»© hai, CPL cÃ³ má»Ÿ rá»™ng Ä‘áº¿n cÃ¡c váº¥n Ä‘á» control cÃ³ chiá»u cao vÃ  network lá»›n hÆ¡n khÃ´ng? Thá»© ba, nhá»¯ng thÃ nh pháº§n nÃ o cá»§a CPL quan trá»ng Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao? VÃ  cuá»‘i cÃ¹ng, CPL hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o vá»›i dá»¯ liá»‡u preference thá»±c táº¿ cá»§a con ngÆ°á»i háº¡n cháº¿? CÃ¡c thÃ­ nghiá»‡m vÃ  chi tiáº¿t bá»• sung Ä‘Æ°á»£c bao gá»“m trong phá»¥ lá»¥c.

Dá»¯ liá»‡u Preference. ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ kháº£ nÄƒng cá»§a CPL trong viá»‡c há»c policy cho MDP tá»•ng quÃ¡t tá»« dá»¯ liá»‡u rollout off-policy khÃ´ng tá»‘i Æ°u vÃ  preference. Cá»¥ thá»ƒ, chÃºng tÃ´i xem xÃ©t quy trÃ¬nh huáº¥n luyá»‡n thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng cho cÃ¡c model ná»n táº£ng lá»›n: supervised learning tiáº¿p theo lÃ  fine-tuning vá»›i RLHF. Äá»ƒ lÃ m Ä‘iá»u nÃ y, chÃºng tÃ´i sá»­ dá»¥ng sÃ¡u task tá»« benchmark robotics MetaWorld (Yu et al., 2020). Äáº§u tiÃªn, chÃºng tÃ´i huáº¥n luyá»‡n cÃ¡c policy baseline cho Ä‘áº¿n khi chÃºng Ä‘áº¡t khoáº£ng 50% tá»· lá»‡ thÃ nh cÃ´ng. Sau Ä‘Ã³, chÃºng tÃ´i rollout 2500 episode cho má»—i policy stochastic khÃ´ng tá»‘i Æ°u. Sau Ä‘Ã³ chÃºng tÃ´i táº¡o thÃ nh cÃ¡c dataset preference D_pref synthetic vá»›i cÃ¡c kÃ­ch thÆ°á»›c khÃ¡c nhau báº±ng cÃ¡ch láº¥y máº«u cÃ¡c Ä‘oáº¡n cÃ³ Ä‘á»™ dÃ i 64 Ä‘á»“ng Ä‘á»u tá»« dá»¯ liá»‡u rollout. ChÃºng tÃ´i Æ°á»›c tÃ­nh cÃ¡c nhÃ£n preference dá»±a trÃªn regret sá»­ dá»¥ng Q-function vÃ  policy cá»§a má»™t model Soft Actor-Critic (SAC) oracle (Haarnoja et al., 2018) Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘áº¿n 100% thÃ nh cÃ´ng trÃªn sá»± káº¿t há»£p cá»§a rollout khÃ´ng tá»‘i Æ°u vÃ  dá»¯ liá»‡u online. Trong thá»±c táº¿, chÃºng tÃ´i xem xÃ©t hai loáº¡i dataset preference chÃ­nh: dense, trong Ä‘Ã³ chÃºng tÃ´i gÃ¡n nhÃ£n comparison giá»¯a má»—i Ä‘oáº¡n Ä‘Æ°á»£c láº¥y máº«u (effectively ranking táº¥t cáº£ cÃ¡c Ä‘oáº¡n), vÃ  sparse, trong Ä‘Ã³ chÃºng tÃ´i chá»‰ gÃ¡n nhÃ£n má»™t comparison cho má»—i Ä‘oáº¡n.

CÃ¡c PhÆ°Æ¡ng phÃ¡p Baseline. ChÃºng tÃ´i xem xÃ©t bá»‘n baseline máº¡nh. Äáº§u tiÃªn, supervised fine-tuning (SFT) huáº¥n luyá»‡n má»™t policy vá»›i BC trÃªn táº¥t cáº£ cÃ¡c Ä‘oáº¡n, sau Ä‘Ã³ fine-tune chá»‰ trÃªn cÃ¡c Ä‘oáº¡n Ä‘Æ°á»£c Æ°a thÃ­ch, tá»©c lÃ  táº¥t cáº£ Ïƒ+ trong D_pref. Baseline thá»© hai lÃ  Preference IQL (P-IQL), há»c má»™t hÃ m reward tá»« D_pref giáº£ Ä‘á»‹nh model partial return, sau Ä‘Ã³ há»c má»™t policy Ä‘á»ƒ tá»‘i Ä‘a hÃ³a nÃ³ vá»›i Implicit Q-Learning (Kostrikov et al., 2022), má»™t thuáº­t toÃ¡n offline RL tiÃªn tiáº¿n. Máº·c dÃ¹ P-IQL láº§n Ä‘áº§u Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i model partial return, á»Ÿ Ä‘Ã¢y nÃ³ sá»­ dá»¥ng má»™t xáº¥p xá»‰ cá»§a A*_rE nhÆ° reward cá»§a nÃ³, nhÆ° chÃºng tÃ´i chá»‰ ra trong Corollary 1 cá»§a Phá»¥ lá»¥c A báº£o tá»“n policy tá»‘i Æ°u. Thá»±c táº¿, P-IQL nÃªn hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n vá»›i nhÃ£n dá»±a trÃªn regret, vÃ¬ A*_rE lÃ  má»™t hÃ m reward Ä‘Æ°á»£c shaped cao cho r_E Ng et al. (1999); Knox et al. (2023). ChÃºng tÃ´i sá»­ dá»¥ng implementation P-IQL cá»§a Hejna & Sadigh (2023) vÃ¬ nÃ³ vÆ°á»£t trá»™i so vá»›i má»™t sá»‘ baseline Ä‘Æ°Æ¡ng thá»i. Baseline nÃ y cÅ©ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i TREX trÃªn preference pairwise (Brown et al., 2019). Thá»© ba, chÃºng tÃ´i xem xÃ©t PPO vá»›i reward KL-constrained nhÆ° thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng cho RLHF vá»›i LLM (Ziegler et al., 2019). ÄÃ¢y khÃ´ng pháº£i lÃ  má»™t so sÃ¡nh cÃ´ng báº±ng, vÃ¬ PPO yÃªu cáº§u 3.84 triá»‡u cáº·p state-action online bá»• sung Ä‘á»ƒ Æ°á»›c tÃ­nh policy gradient, tá»•ng cá»™ng 25Ã— dá»¯ liá»‡u cho CPL 2.5K Dense vÃ  4Ã— dá»¯ liá»‡u cho CPL 20K Sparse. KhÃ´ng giá»‘ng nhÆ° thiáº¿t láº­p contextual bandit Ä‘Æ°á»£c sá»­ dá»¥ng cho LLM, PPO á»Ÿ Ä‘Ã¢y yÃªu cáº§u rollout trajectory Ä‘áº§y Ä‘á»§. Cuá»‘i cÃ¹ng, Ä‘á»ƒ chá»©ng minh kháº£ nÄƒng ngoáº¡i suy cá»§a CPL vÆ°á»£t ra ngoÃ i hiá»‡u suáº¥t tá»‘t nháº¥t trong dá»¯ liá»‡u, chÃºng tÃ´i so sÃ¡nh vá»›i %BC, trong Ä‘Ã³ má»™t policy Ä‘Æ°á»£c huáº¥n luyá»‡n vá»›i behavior cloning trÃªn X% rollout hÃ ng Ä‘áº§u theo r_E ground truth.

4.1 CPL HOáº T Äá»˜NG NHÆ¯ THáº¾ NÃ€O?
CPL hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o vá»›i quan sÃ¡t dá»±a trÃªn state? Káº¿t quáº£ chÃ­nh dá»±a trÃªn state cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong hÃ ng 1 vÃ  3 cá»§a Báº£ng 1. Khi sá»­ dá»¥ng dá»¯ liá»‡u comparison thÆ°a thá»›t hÆ¡n (hÃ ng 3), CPL vÆ°á»£t trá»™i so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p trÆ°á»›c Ä‘Ã¢y trong 5 trong 6 mÃ´i trÆ°á»ng, thÆ°á»ng vá»›i margin Ä‘Ã¡ng ká»ƒ hÆ¡n P-IQL, Ä‘áº·c biá»‡t trong cÃ¡c mÃ´i trÆ°á»ng Button Press, Bin Picking, vÃ  Sweep Into. Khi Ä‘Æ°á»£c Ã¡p dá»¥ng cho dataset vá»›i comparison dÃ y Ä‘áº·c hÆ¡n (hÃ ng 1), CPL hoáº¡t Ä‘á»™ng tháº­m chÃ­ tá»‘t hÆ¡n. Máº·c dÃ¹ dataset comparison dÃ y Ä‘áº·c cÃ³ Ã­t state-action coverage hÆ¡n, chÃºng chá»©a Ä‘Ã¡ng ká»ƒ nhiá»u comparison hÆ¡n dataset comparison thÆ°a thá»›t. ChÃºng tÃ´i cho ráº±ng nhiá»u comparison cho má»—i Ä‘oáº¡n cÃ³ lá»£i hÆ¡n cho CPL so vá»›i P-IQL vÃ¬ objective contrastive cá»§a nÃ³ â€“ cÃ¡c dataset giÃ u comparison hÆ¡n cÃ³ thá»ƒ cÃ³ nhiá»u cáº·p positive-negative thÃ´ng tin hÆ¡n giÃºp hÃ¬nh thÃ nh policy. PPO khÃ´ng thá»ƒ consistently hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n CPL máº·c dÃ¹ truy cáº­p vÃ o lÆ°á»£ng lá»›n dá»¯ liá»‡u online tá»« mÃ´i trÆ°á»ng. ChÃºng tÃ´i tháº¥y PPO ráº¥t nháº¡y cáº£m vá»›i há»‡ sá»‘ KL-constraint trÃªn reward, Ä‘iá»u nÃ y lÃ m cho nÃ³ khÃ³ tune nhÆ° Ä‘Æ°á»£c quan sÃ¡t trong cÃ´ng trÃ¬nh trÆ°á»›c (Touvron et al., 2023). Váº¥n Ä‘á» nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c lÃ m tráº§m trá»ng hÆ¡n bá»Ÿi sá»± báº¥t á»•n cá»§a KL-divergence trong khÃ´ng gian liÃªn tá»¥c vÃ  high variance cá»§a cáº£ value estimation vÃ  policy gradient vá»›i horizon dÃ i hÆ¡n trong cÃ¡c task robotics so vá»›i thiáº¿t láº­p contextual-bandit LLM. ChÃºng tÃ´i tháº¥y ráº±ng CPL consistently vÆ°á»£t trá»™i so vá»›i %BC, chá»‰ ra CPL thá»±c sá»± thá»ƒ hiá»‡n cáº£i thiá»‡n policy vÆ°á»£t ra ngoÃ i cÃ¡c hÃ nh vi tá»‘t nháº¥t trong dataset.

CPL má»Ÿ rá»™ng nhÆ° tháº¿ nÃ o Ä‘áº¿n quan sÃ¡t cÃ³ chiá»u cao? Äá»ƒ kiá»ƒm tra cÃ¡ch cÃ¡c objective supervised cá»§a CPL má»Ÿ rá»™ng Ä‘áº¿n cÃ¡c váº¥n Ä‘á» continuous control cÃ³ chiá»u cao, chÃºng tÃ´i render cÃ¡c dataset MetaWorld Ä‘Æ°á»£c tháº£o luáº­n á»Ÿ trÃªn thÃ nh áº£nh 64Ã—64. ChÃºng tÃ´i sá»­ dá»¥ng kiáº¿n trÃºc network tá»« DrQv2 (Yarats et al., 2022) vÃ 

--- TRANG 6 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

[Báº£ng 1 vá»›i káº¿t quáº£ hiá»‡u suáº¥t trÃªn MetaWorld benchmark]

cÃ¹ng hyperparameter nhÆ° cÃ¡c thÃ­ nghiá»‡m dá»±a trÃªn state. ChÃºng tÃ´i thÃªm sá»­ dá»¥ng random shift augmentation, Ä‘iá»u nÃ y cáº£i thiá»‡n drastically hiá»‡u suáº¥t cá»§a RL tá»« áº£nh (Laskin et al., 2020). Káº¿t quáº£ dá»±a trÃªn áº£nh cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong hÃ ng 2 vÃ  4 cá»§a Báº£ng 1. ThÃº vá»‹, chÃºng tÃ´i tháº¥y ráº±ng hiá»‡u suáº¥t tÄƒng vá»«a pháº£i cho SFT nhÆ°ng Ä‘Ã¡ng ká»ƒ cho P-IQL. ChÃºng tÃ´i cho ráº±ng Ä‘iá»u nÃ y lÃ  do data-augmentation, khÃ´ng Ã¡p dá»¥ng Ä‘Æ°á»£c trong state, Ä‘Ã³ng vai trÃ² quan trá»ng trong viá»‡c cáº£i thiá»‡n biá»ƒu diá»…n value cho P-IQL. Máº·c dÃ¹ váº­y, khi há»c tá»« dá»¯ liá»‡u preference dÃ y Ä‘áº·c hÆ¡n (hÃ ng 2), CPL váº«n vÆ°á»£t trá»™i so vá»›i P-IQL trong 4 trong 6 mÃ´i trÆ°á»ng vÃ  hÃ²a trÃªn Sweep Into. Khi há»c tá»« comparison thÆ°a thá»›t hÆ¡n (hÃ ng 4), CPL vÃ  P-IQL hoáº¡t Ä‘á»™ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng trÃªn háº§u háº¿t cÃ¡c task, máº·c dÃ¹ CPL Ä‘Æ¡n giáº£n hÆ¡n drastically so vá»›i P-IQL. Má»™t láº§n ná»¯a, khoáº£ng cÃ¡ch hiá»‡u suáº¥t giá»¯a CPL vÃ  P-IQL cao hÆ¡n vá»›i dá»¯ liá»‡u comparison dÃ y Ä‘áº·c hÆ¡n, nháº¥n máº¡nh táº§m quan trá»ng cá»§a negative thÃ´ng tin.

[Báº£ng 2 vá» hiá»‡u quáº£ tÃ­nh toÃ¡n]

Nhá»¯ng káº¿t quáº£ nÃ y áº¥n tÆ°á»£ng hÆ¡n khi xem xÃ©t viá»‡c giáº£m Ä‘á»™ phá»©c táº¡p Ä‘Ã¡ng ká»ƒ cá»§a CPL. P-IQL pháº£i há»c má»™t hÃ m reward, má»™t Q-function, má»™t value function, vÃ  má»™t policy. CPL trÃ¡nh táº¥t cáº£ Ä‘iá»u nÃ y, vÃ  chá»‰ há»c má»™t policy, drastically giáº£m thá»i gian huáº¥n luyá»‡n vÃ  sá»‘ lÆ°á»£ng parameter. NhÆ° chÃºng ta cÃ³ thá»ƒ tháº¥y trong Báº£ng 2, CPL cháº¡y nhanh hÆ¡n 1.62Ã— so vá»›i P-IQL trÃªn áº£nh vÃ  cÃ³ Ã­t hÆ¡n má»™t pháº§n tÆ° parameter. Khi network ngÃ y cÃ ng lá»›n hÆ¡n, lá»£i Ã­ch hiá»‡u suáº¥t tá»« viá»‡c sá»­ dá»¥ng CPL sáº½ chá»‰ tÄƒng.

4.2 ÄIá»€U GÃŒ ÄÃ“NG GÃ“P VÃ€O HIá»†U SUáº¤T Cá»¦A CPL?

NhÆ° Ä‘Æ°á»£c Ã¡m chá»‰ trong cÃ¡c pháº§n trÆ°á»›c, chÃºng tÃ´i tháº¥y ráº±ng khoáº£ng cÃ¡ch hiá»‡u suáº¥t giá»¯a CPL vÃ  baseline cao hÆ¡n cho cÃ¡c dataset vá»›i comparison dÃ y Ä‘áº·c hÆ¡n. Äiá»u nÃ y phÃ¹ há»£p vá»›i cÃ¡c cÃ´ng trÃ¬nh trÆ°á»›c Ä‘Ã¢y trong contrastive

--- TRANG 7 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

learning (Robinson et al., 2021). Äá»ƒ nghiÃªn cá»©u hiá»‡u á»©ng nÃ y, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a CPL khi chÃºng tÃ´i tÄƒng sá»‘ lÆ°á»£ng comparison Ä‘Æ°á»£c láº¥y máº«u cho má»—i Ä‘oáº¡n trÃªn má»™t dataset cá»‘ Ä‘á»‹nh gá»“m 5000 Ä‘oáº¡n. ChÃºng tÃ´i chá»‰ ra káº¿t quáº£ cá»§a Ä‘iá»u nÃ y cho Drawer Open vá»›i quan sÃ¡t dá»±a trÃªn state á»Ÿ bÃªn trÃ¡i cá»§a HÃ¬nh 2 vÃ  bao gá»“m pháº§n cÃ²n láº¡i trong Phá»¥ lá»¥c C.3 ngoÃ i dense data scaling. NhÃ¬n chung, chÃºng tÃ´i tháº¥y ráº±ng CPL Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« viá»‡c tÄƒng sá»‘ lÆ°á»£ng comparison cho má»—i Ä‘oáº¡n trong táº¥t cáº£ cÃ¡c task ngoáº¡i trá»« Plate Slide. P-IQL Ã­t bá»‹ áº£nh hÆ°á»Ÿng hÆ¡n, máº·c dÃ¹ Ä‘Ã´i khi hoáº¡t Ä‘á»™ng tá»‡ hÆ¡n vá»›i nhiá»u comparison hÆ¡n, mÃ  chÃºng tÃ´i nghi ngá» lÃ  do reward under-fitting. Äiá»u nÃ y lÃ m ná»•i báº­t má»™t nhÆ°á»£c Ä‘iá»ƒm khÃ¡c cá»§a P-IQL â€“ do sá»‘ lÆ°á»£ng thÃ nh pháº§n cao hÆ¡n, nÃ³ cÃ³ nhiá»u hyperparameter hÆ¡n vÃ  do Ä‘Ã³ nháº¡y cáº£m hÆ¡n vá»›i nhá»¯ng thay Ä‘á»•i trong dataset. ChÃºng tÃ´i Ä‘Ã£ tune hyperparameter cho táº¥t cáº£ cÃ¡c phÆ°Æ¡ng phÃ¡p vá»›i 10K comparison, sau Ä‘Ã³ Ä‘á»ƒ chÃºng giá»‘ng nhau cho cÃ¡c thÃ­ nghiá»‡m scaling.

Cuá»‘i cÃ¹ng, chÃºng tÃ´i ablate cáº£ hai hyperparameter cá»§a CPL â€“ giÃ¡ trá»‹ temperature Î± vÃ  bias regularizer Î» â€“ cho Drawer Open á»Ÿ bÃªn pháº£i cá»§a HÃ¬nh 2. Trong khi CPL thÆ°á»ng hoáº¡t Ä‘á»™ng tá»‘t vá»›i táº¥t cáº£ cÃ¡c giÃ¡ trá»‹, chÃºng tÃ´i tháº¥y ráº±ng hiá»‡u suáº¥t cao hÆ¡n cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c vá»›i viá»‡c tune hyper-parameter thÃªm, Ä‘áº·c biá»‡t cho Î». Trong Phá»¥ lá»¥c B chÃºng tÃ´i ablate nhiá»u quyáº¿t Ä‘á»‹nh thiáº¿t káº¿ hÆ¡n, nhÆ° lá»±a chá»n conservative regularizer.

4.3 CPL HOáº T Äá»˜NG NHÆ¯ THáº¾ NÃ€O Vá»šI PREFERENCE CON NGÆ¯á»œI Háº N CHáº¾?

Äá»ƒ chá»©ng minh cáº£ tÃ­nh á»©ng dá»¥ng cá»§a model preference regret vÃ  hiá»‡u suáº¥t cá»§a CPL, chÃºng tÃ´i thá»±c hiá»‡n cÃ¡c thÃ­ nghiá»‡m bá»• sung vá»›i preference thá»±c táº¿ cá»§a con ngÆ°á»i. Cá»¥ thá»ƒ, chÃºng tÃ´i Ã¡p dá»¥ng benchmark tá»« Kim et al. (2023) sá»­ dá»¥ng 100 (expert) hoáº·c 500 (replay) preference con ngÆ°á»i trÃªn dataset tá»« benchmark D4RL (Fu et al., 2020). Äá»ƒ táº¡o Ä‘iá»u kiá»‡n há»c tá»« sá»‘ lÆ°á»£ng query háº¡n cháº¿ nhÆ° váº­y, chÃºng tÃ´i Ä‘Ã£ sá»­a Ä‘á»•i CPL báº±ng cÃ¡ch Ä‘áº§u tiÃªn há»c má»™t model logistic regression Ä‘á»ƒ dá»± Ä‘oÃ¡n preference P[Ïƒ+ â‰» Ïƒâˆ’] cá»§a ngÆ°á»i dÃ¹ng theo An et al. (2023). Quan trá»ng, Ä‘Ã¢y khÃ´ng pháº£i lÃ  má»™t hÃ m reward hoáº·c advantage vÃ¬ nÃ³ láº¥y toÃ n bá»™ Ä‘oáº¡n lÃ m input, khÃ´ng pháº£i cÃ¡c cáº·p state-action Ä‘Æ¡n láº». Sau Ä‘Ã³ chÃºng tÃ´i sá»­ dá»¥ng model nÃ y Ä‘á»ƒ relabel dá»¯ liá»‡u offline D4RL vá»›i preference dÃ y Ä‘áº·c cho CPL. ChÃºng tÃ´i mÆ°á»£n kiáº¿n trÃºc tá»« Kim et al. (2023) vÃ  khÃ´ng sá»­ dá»¥ng BC pretraining. Káº¿t quáº£ cá»§a chÃºng tÃ´i cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ¬m tháº¥y trong Báº£ng 3. CPL cÃ³ hiá»‡u suáº¥t tá»‘t nháº¥t vá»›i margin lá»›n trong Hopper-Medium-Exp, nhÆ°ng hoáº¡t Ä‘á»™ng tá»‡ hÆ¡n trong Walker-Medium-Replay. ChÃºng tÃ´i cho ráº±ng Ä‘iá»u nÃ y lÃ  do preference cho dataset nÃ y cÃ³ thá»ƒ khÃ´ng tuÃ¢n thá»§ cháº·t cháº½ model dá»±a trÃªn regret nhÆ° Ä‘Æ°á»£c tháº£o luáº­n vá»›i cÃ¡c tÃ¡c giáº£ cá»§a Kim et al. (2023), chÃºng Ä‘Æ°á»£c thu tháº­p bá»Ÿi má»™t ngÆ°á»i dÃ¹ng duy nháº¥t vá»›i phÆ°Æ¡ng phÃ¡p dá»±a trÃªn quy táº¯c Ä‘Æ°á»£c lÃªn káº¿ hoáº¡ch trÆ°á»›c. Tuy nhiÃªn, CPL váº«n cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t trÃªn cÃ¡c task nÃ y chá»‰ vá»›i 100 hoáº·c 500 preference con ngÆ°á»i, vÃ  khÃ´ng cáº§n há»c value hoáº·c Q-function.

[Báº£ng 3 vá»›i káº¿t quáº£ D4RL]

5 CÃ”NG TRÃŒNH LIÃŠN QUAN
Máº·c dÃ¹ RLHF gáº§n Ä‘Ã¢y Ä‘Ã£ tÄƒng vá»t vá» Ä‘á»™ phá»• biáº¿n, viá»‡c há»c policy tá»« preference con ngÆ°á»i Ä‘Ã£ lÃ  má»™t váº¥n Ä‘á» Ä‘Æ°á»£c nghiÃªn cá»©u lÃ¢u dÃ i, Ä‘Æ°á»£c gá»i lÃ  preference-based RL (PbRL).

--- TRANG 8 ---
Xuáº¥t báº£n nhÆ° má»™t bÃ i bÃ¡o há»™i nghá»‹ táº¡i ICLR 2024

CÃ¡c phÆ°Æ¡ng phÃ¡p PbRL thÆ°á»ng báº¯t Ä‘áº§u báº±ng viá»‡c há»c má»™t hÃ m reward, thÆ°á»ng tá»« comparison pairwise, sau Ä‘Ã³ sá»­ dá»¥ng má»™t thuáº­t toÃ¡n RL Ä‘á»ƒ tá»‘i Æ°u hÃ³a policy (FÃ¼rnkranz et al., 2012). Trong khi Akrour et al. (2012; 2011); Wilson et al. (2012) lÃ  má»™t sá»‘ vÃ­ dá»¥ Ä‘áº§u tiÃªn cá»§a PbRL, gáº§n Ä‘Ã¢y hÆ¡n má»™t sá»‘ cÃ´ng trÃ¬nh Ä‘Ã£ chá»‰ ra ráº±ng, vá»›i hÃ ng ngÃ n query hoáº·c pretraining Ä‘áº§y Ä‘á»§, PbRL cÃ³ thá»ƒ huáº¥n luyá»‡n policy deep neural-network cho control sá»­ dá»¥ng comparison (Christiano et al., 2017; Lee et al., 2021; Ibarz et al., 2018; Brown et al., 2020; Hejna & Sadigh, 2022; Shin & Brown, 2021) hoáº·c ranking (Brown et al., 2019; BÄ±yÄ±k et al., 2019; Sikchi et al., 2023a). Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y thÆ°á»ng chá»‰ Ä‘Æ°á»£c chá»©ng minh trÃªn control dá»±a trÃªn state cÃ³ chiá»u tháº¥p vÃ¬ nhá»¯ng thÃ¡ch thá»©c mÃ  RL gáº·p pháº£i khi má»Ÿ rá»™ng Ä‘áº¿n input vÃ  network lá»›n hÆ¡n (Ota et al., 2021). Trong quÃ¡ khá»©, viá»‡c loáº¡i bá» RL Ä‘Ã£ dáº«n Ä‘áº¿n cÃ¡c thuáº­t toÃ¡n hiá»‡u quáº£ cho goal-conditioned RL tá»« áº£nh (Hejna et al., 2023; Eysenbach et al., 2022). CPL lÃ m Ä‘iá»u tÆ°Æ¡ng tá»± nhÆ°ng cho PbRL. CÃ¡c cÃ´ng trÃ¬nh khÃ¡c giáº£i quyáº¿t váº¥n Ä‘á» lá»±a chá»n pháº£n há»“i (Sadigh et al., 2017; Biyik et al., 2020; Daniel et al., 2015), mÃ  chÃºng tÃ´i xem xÃ©t bá»• sung vÃ¬ CPL cÃ³ thá»ƒ Ä‘Æ°á»£c hÆ°á»Ÿng lá»£i tá»« viá»‡c thu tháº­p dá»¯ liá»‡u cháº¥t lÆ°á»£ng cao hÆ¡n.

Äá»ƒ má»Ÿ rá»™ng RLHF, cÃ¡c phÆ°Æ¡ng phÃ¡p gáº§n Ä‘Ã¢y Ä‘á»ƒ tinh chá»‰nh LLM Ä‘Ã£ bá» qua thÃ nh pháº§n temporal cá»§a RL, vÃ  thay vÃ o Ä‘Ã³ coi text-generation nhÆ° má»™t váº¥n Ä‘á» contextual bandit (Ziegler et al., 2019). Trong khi phÆ°Æ¡ng phÃ¡p nÃ y Ä‘Ã£ tá» ra hiá»‡u quáº£ táº¡i cÃ¡c task nhÆ° summarization (Stiennon et al., 2020; Wu & Hu, 2018), instruction following (Ouyang et al., 2022; Nakano et al., 2021), vÃ  tháº­m chÃ­ táº¡o áº£nh (Lee et al., 2023; Black et al., 2023), nÃ³ fundamentally bá» qua thá»±c táº¿ ráº±ng tÆ°Æ¡ng tÃ¡c vá»›i ngÆ°á»i dÃ¹ng thÆ°á»ng lÃ  tuáº§n tá»±, kÃ©o dÃ i nhiá»u lÆ°á»£t. KhÃ´ng giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y, CPL hoáº¡t Ä‘á»™ng vá»›i MDP tá»•ng quÃ¡t. Kháº£ nÄƒng Ä‘á»™c Ä‘Ã¡o cá»§a CPL trong viá»‡c há»c tá»« dá»¯ liá»‡u sequence chá»‰ vá»›i objective supervised lÃ m cho nÃ³ trá»Ÿ thÃ nh á»©ng viÃªn hÃ ng Ä‘áº§u Ä‘á»ƒ má»Ÿ rá»™ng Ä‘áº¿n cÃ¡c váº¥n Ä‘á» phá»©c táº¡p hÆ¡n. Thá»±c táº¿, Direct Preference Optimization (DPO) (Rafailov et al., 2023) gáº§n Ä‘Ã¢y Ä‘Ã£ chá»©ng minh ráº±ng má»™t objective supervised tÆ°Æ¡ng tá»± nhÆ° CPL cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t nhÆ° RL trong thiáº¿t láº­p contextual bandit. ChÃºng tÃ´i chá»‰ ra trong Phá»¥ lá»¥c A ráº±ng DPO cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Æ°a ra nhÆ° má»™t trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a CPL trong Ä‘Ã³ cÃ¡c Ä‘oáº¡n cÃ³ Ä‘á»™ dÃ i 1 vÃ  báº¯t Ä‘áº§u táº¡i cÃ¹ng state. Äiá»u nÃ y tÆ°Æ¡ng Ä‘á»“ng vá»›i Knox et al. (2023), nhá»¯ng ngÆ°á»i chá»‰ ra ráº±ng phÆ°Æ¡ng phÃ¡p contextual bandit-approach phá»• biáº¿n lÃ  má»™t trÆ°á»ng há»£p Ä‘áº·c biá»‡t cá»§a phÆ°Æ¡ng phÃ¡p naive tá»« Pháº§n 3.

Äá»ƒ Ä‘Æ°a ra objective cá»§a CPL, chÃºng tÃ´i táº­n dá»¥ng kiáº¿n thá»©c tá»« cÃ¡c cÃ´ng trÃ¬nh xÃ¢y dá»±ng dá»±a trÃªn nguyÃªn lÃ½ maximum entropy trong control (Ziebart et al., 2008; Ziebart, 2010; Haarnoja et al., 2017). Cáº­p nháº­t contrastive káº¿t quáº£ trá»±c tiáº¿p há»c policy tá»‘i Æ°u vá»›i dá»¯ liá»‡u hoÃ n toÃ n off-policy. Äiá»u nÃ y khÃ´ng giá»‘ng nhÆ° nhiá»u thuáº­t toÃ¡n RLHF dá»±a trÃªn RL trong cáº£ ngÃ´n ngá»¯ (Ziegler et al., 2019) hoáº·c control (Christiano et al., 2017) yÃªu cáº§u rollout on policy vÃ  cÃ¡c thÃ nh pháº§n Ä‘Ã£ há»c bá»• sung Ä‘Ã£ Ä‘Æ°á»£c chá»‰ ra lÃ m tÄƒng variance (Hejna & Sadigh, 2023). CÃ¡c objective contrastive learning tÆ°Æ¡ng tá»± Ä‘Ã£ chá»‰ ra lÃ  hiá»‡u quáº£ cho temporal representation learning (Ma et al., 2023), ngay cáº£ vá»›i dá»¯ liá»‡u preference (Kang et al., 2023; Xu et al., 2023; An et al., 2023).

6 THáº¢O LUáº¬N
Trong cÃ´ng trÃ¬nh nÃ y chÃºng tÃ´i giá»›i thiá»‡u CPL, má»™t framework novel cho RLHF sá»­ dá»¥ng model preference regret. Vá» máº·t lÃ½ thuyáº¿t, chÃºng tÃ´i Ä‘Ã£ chá»©ng minh ráº±ng CPL luÃ´n há»c má»™t hÃ m advantage consistent vÃ  há»™i tá»¥ Ä‘áº¿n policy tá»‘i Æ°u cho hÃ m reward cá»§a expert. Vá» máº·t thá»±c táº¿, chÃºng tÃ´i Ä‘Ã£ chá»‰ ra ráº±ng objective supervised cá»§a CPL cÃ³ thá»ƒ vÆ°á»£t trá»™i so vá»›i baseline RL khi há»c cÃ¡c policy manipulation phá»©c táº¡p tá»« dá»¯ liá»‡u preference dÃ y Ä‘áº·c trong khi Ä‘Æ¡n giáº£n hÆ¡n vÃ  nhanh hÆ¡n 1.6Ã—.

Háº¡n cháº¿. CPL, giá»‘ng nhÆ° cÃ¡c phÆ°Æ¡ng phÃ¡p RLHF khÃ¡c, giáº£ Ä‘á»‹nh kiáº¿n thá»©c vá» temporal discounting cá»§a human rater (tá»©c lÃ  cá»§a discount factor Î³), trong thá»±c táº¿ sáº½ khÃ³ truyá»n Ä‘áº¡t. VÃ¬ hÃ m loss cá»§a CPL Ä‘Æ°á»£c tÃ­nh toÃ¡n trÃªn cÃ¡c Ä‘oáº¡n, nÃ³ yÃªu cáº§u má»™t lÆ°á»£ng Ä‘Ã¡ng ká»ƒ bá»™ nhá»› GPU cho cÃ¡c kÃ­ch thÆ°á»›c Ä‘oáº¡n lá»›n. Cuá»‘i cÃ¹ng, khÃ´ng cÃ³ model nÃ o vá» hÃ nh vi con ngÆ°á»i lÃ  hoÃ n háº£o.

HÆ°á»›ng TÆ°Æ¡ng lai. Má»™t sá»‘ hÆ°á»›ng nghiÃªn cá»©u thÃº vá»‹ váº«n cÃ²n. Äáº§u tiÃªn lÃ  má»Ÿ rá»™ng CPL Ä‘áº¿n cÃ¡c dataset vÃ  kiáº¿n trÃºc lá»›n hÆ¡n nÆ¡i chÃºng tÃ´i tin ráº±ng lá»£i Ã­ch cá»§a nÃ³ sáº½ rÃµ rÃ ng hÆ¡n. Má»™t á»©ng dá»¥ng cÃ³ thá»ƒ thÃº vá»‹ lÃ  LLM, nÆ¡i CPL cho phÃ©p fine-tuning trÃªn nhiá»u bÆ°á»›c cá»§a turn-based dialogue. Theo hiá»ƒu biáº¿t cá»§a chÃºng tÃ´i, khÃ´ng cÃ³ dataset preference multi-step nÃ o hiá»‡n táº¡i tá»“n táº¡i cho LLM. Thá»© hai, cÃ´ng trÃ¬nh cá»§a chÃºng tÃ´i chá»‰ xem xÃ©t dá»¯ liá»‡u offline Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c policy khÃ´ng tá»‘i Æ°u. Má»™t phiÃªn báº£n online cá»§a CPL cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¡t triá»ƒn hoáº¡t Ä‘á»™ng vá»›i pháº£n há»“i con ngÆ°á»i online, cho phÃ©p policy liÃªn tá»¥c cáº£i thiá»‡n. Cuá»‘i cÃ¹ng, Ä‘á»ƒ thÆ° giÃ£n giáº£ Ä‘á»‹nh ráº±ng Î³ Ä‘Ã£ biáº¿t, ngÆ°á»i ta cÃ³ thá»ƒ thay vÃ o Ä‘Ã³ bao gá»“m nÃ³ trong expressivity cá»§a CPL hoáº·c cÃ¡c phÆ°Æ¡ng phÃ¡p RLHF khÃ¡c.

--- TRANG 9 ---
[Pháº§n nÃ y chá»©a cÃ¡c chi tiáº¿t vá» tÃ i liá»‡u tham kháº£o vÃ  Ä‘Ã³ng gÃ³p - tÃ´i sáº½ tiáº¿p tá»¥c dá»‹ch pháº§n cÃ²n láº¡i]

10 Lá»œI Cáº¢M Æ N
CÃ´ng trÃ¬nh nÃ y Ä‘Æ°á»£c há»— trá»£ bá»Ÿi NSF Award 2006388, NSF Award 2218760, NSF Award 1933032, NSF Award 1953032, NSF Award 1941722, NSF Award 2125511, NSF Award IIS-1749204, AFOSR YIP, AFOSR (FA9550-20-1-0077), ARO (78372-CS, W911NF-19-2-0333), ONR (N00014-21-1-2685), ONR, Ford, DARPA YFA, vÃ  Center for AI Safety. JH Ä‘Æ°á»£c há»— trá»£ bá»Ÿi há»c bá»•ng DoD NDSEG. CF lÃ  má»™t Fellow CIFAR trong chÆ°Æ¡ng trÃ¬nh Learning in Machines and Brains. WBK Ä‘Æ°á»£c há»— trá»£ bá»Ÿi thÃ¡ch thá»©c lá»›n Good Systems cá»§a UT Austin. ChÃºng tÃ´i muá»‘n cáº£m Æ¡n Archit Sharma vá» nhá»¯ng cuá»™c tháº£o luáº­n quÃ½ bÃ¡u vá» regularizer conservative Ä‘Æ°á»£c sá»­ dá»¥ng trong CPL. Báº¥t ká»³ Ã½ kiáº¿n, phÃ¡t hiá»‡n, vÃ  káº¿t luáº­n hoáº·c khuyáº¿n nghá»‹ nÃ o Ä‘Æ°á»£c thá»ƒ hiá»‡n trong tÃ i liá»‡u nÃ y Ä‘á»u thuá»™c vá» (cÃ¡c) tÃ¡c giáº£ vÃ  khÃ´ng nháº¥t thiáº¿t pháº£n Ã¡nh quan Ä‘iá»ƒm cá»§a cÃ¡c nhÃ  tÃ i trá»£.

ÄÃ“NG GÃ“P
JH dáº«n dáº¯t dá»± Ã¡n, Ä‘Ã³ng gÃ³p vÃ o táº¥t cáº£ cÃ¡c khÃ­a cáº¡nh bao gá»“m Ã½ tÆ°á»Ÿng, lÃ½ thuyáº¿t, thÃ­ nghiá»‡m, vÃ  viáº¿t. RR Ä‘á» xuáº¥t liÃªn káº¿t advantage vÃ  likelihood vÃ  Ä‘Ã³ng gÃ³p vÃ o Ã½ tÆ°á»Ÿng giai Ä‘oáº¡n Ä‘áº§u. HS Ä‘Ã³ng gÃ³p vÃ o lÃ½ thuyáº¿t, thiáº¿t káº¿ thÃ­ nghiá»‡m, vÃ  cháº¡y thÃ­ nghiá»‡m. CF, SN, WBK, DS giÃ¡m sÃ¡t, tÆ° váº¥n, vÃ  cung cáº¥p pháº£n há»“i vá» dá»± Ã¡n.

[Pháº§n tÃ i liá»‡u tham kháº£o vÃ  cÃ¡c phá»¥ lá»¥c tiáº¿p theo sáº½ Ä‘Æ°á»£c dá»‹ch tiáº¿p tá»¥c náº¿u cáº§n]
