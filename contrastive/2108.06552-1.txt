# 2108.06552.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/contrastive/2108.06552.pdf
# File size: 850833 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Pattern Recognition Letters
journal homepage: www.elsevier.com
Continual Semi-Supervised Learning through Contrastive Interpolation Consistency
Matteo Boschinia, Pietro Buzzegaa, Lorenzo Bonicellia,, Angelo Porrelloa, Simone Calderaraa
aUniversity of Modena and Reggio Emilia, Via Vivarelli 10, Modena, Italy
Article history :
Continual learning, deep learning,
semi-supervised learning, weak su-
pervision, catastrophic forgettingABSTRACT
Continual Learning (CL) investigates how to train Deep Networks on a stream
of tasks without incurring forgetting . CL settings proposed in literature assume
that every incoming example is paired with ground-truth annotations. However,
this clashes with many real-world applications: gathering labeled data, which is in
itself tedious and expensive, becomes infeasible when data ow as a stream. This
work explores Continual Semi-Supervised Learning (CSSL): here, only a small
fraction of labeled input examples are shown to the learner. We assess how current
CL methods ( e.g.: EWC, LwF, iCaRL, ER, GDumb, DER) perform in this novel
and challenging scenario, where overtting entangles forgetting. Subsequently,
we design a novel CSSL method that exploits metric learning and consistency
regularization to leverage unlabeled examples while learning. We show that our
proposal exhibits higher resilience to diminishing supervision and, even more sur-
prisingly, relying only on 25% supervision suces to outperform SOTA methods
trained under full supervision.
Â©2022 Elsevier Ltd. All rights reserved.
1. Introduction
Perceptual information ows as a continuous stream,
in which a certain data distribution may occur once and
not recur for a long time. Unfortunately, this violates the
i.i.d. assumption at the foundation of most Deep Learn-
ing algorithms and leads to the catastrophic forgetting [1]
problem, where the acquired knowledge is rapidly over-
written by the new one. In practical scenarios, we would
prefer a system that learns incrementally from the raw and
non-i.i.d. stream of data, possibly ready to provide answers
at any moment. The design of such lifelong-learning algo-
rithms is the aim of Continual Learning (CL) [2].
Works in this eld typically test the proposed meth-
ods on a series of image-classication tasks presented se-
quentially. The latter are built on top of image classi-
Corresponding author
e-mail: matteo.boschini@unimore.it (Matteo Boschini),
pietro.buzzega@unimore.it (Pietro Buzzega),
lorenzo.bonicelli@unimore.it (Lorenzo Bonicelli),
angelo.porrello@unimore.it (Angelo Porrello),
simone.calderara@unimore.it (Simone Calderara)cation datasets ( e.g.: MNIST, CIFAR, etc.) by allowing
the learner to see just a subset of classes at once. While
these experimental protocols validly highlight the eects of
forgetting, they assume that all incoming data are labeled.
In some scenarios, this condition does not represent an
issue and can be easily met. This may be the case when
ground-truth annotations can be directly and automati-
cally collected ( e.g.: a robot that explores the environment
and learns to avoid collisions by receiving direct feedback
from it [3]). However, when the labeling stage involves
human intervention (as holds in a number of computer vi-
sion tasks such as classication, object detection [4], etc.),
relying only on full supervision clashes with the pursuit of
lifelong learning. Indeed, the adaptability of the learner to
incoming tasks would be bottlenecked by the speed of the
human annotator: updating the model continually would
lose its appeal w.r.t. the trivial solution of re-training from
scratch. Therefore, we advocate taking into account the
rate at which annotations are available to the learner.
To address this point, the adjustment of the prediction
model can be simply limited to the fraction of examples
that can be labeled in real-time. Our experiments showarXiv:2108.06552v3  [stat.ML]  29 Aug 2022

--- PAGE 2 ---
2
that this results in an expected degradation in terms of
performance. Fortunately, the eorts recently made in
semi-supervised learning [5, 6] come to the rescue: by
revising these techniques to an incremental scenario, we
can still benet from the remaining part of the data rep-
resented by unlabeled observations. We argue that this
is true to the lifelong nature of the application and also
allows for exploiting the abundant source of information
given by unlabeled data. To sum up, our work incorpo-
rates the features described above in a new setting called
Continual Semi-Supervised Learning (CSSL) : a scenario
where just one out of kexamples is presented with its
ground-truth label. At training time, this corresponds to
providing a ground-truth label for any given example with
uniform probability 1=k(as shown in Fig. 1 for k= 2).
Taking one more step, our proposal aims at lling the
gap induced by partial annotations: Contrastive Continual
Interpolation Consistency (CCIC), which imposes consis-
tency among augmented and interpolated examples while
exploiting secondhand information peculiar to the Class-
Incremental setting. Doing so, we grant performance that
matches and even surpasses that of the fully-supervised
setting. We nally summarize our contributions:
â€¢We propose CSSL: a scenario in which the learner
must learn continually by exploiting both supervised
and unsupervised data at the same time;
â€¢We empirically review the performance of SOTA CL
models at varying label-per-example rates, highlight-
ing the subtle dierences between CL and CSSL;
â€¢Exploiting semi-supervised techniques, we introduce
a novel CSSL method that successfully addresses the
new setting and learns with limited labels;
â€¢Surprisingly, our evaluations show that full supervi-
sion does not necessarily upper-bound partial super-
vision in CL: 25% labels can be enough to outper-
form SOTA methods using all ground truth.
2. Related Work
2.1. Continual Learning Protocols
Continual Learning is an umbrella term encompass-
ing several slightly yet meaningfully dierent experimental
settings [7, 8]. Van de Ven et al. produced a taxonomy [8]
describing the following three well-known scenarios. Task-
Incremental Learning (Task-IL) organizes the dataset in
tasks comprising of disjoint sets of classes. The model
must only learn (and remember) how to correctly classify
examples within their original tasks. Domain-Incremental
Learning (Domain-IL) presents all classes since the rst
task: distinct tasks are obtained by processing the ex-
amples with distinct transformations ( e.g.: pixel permuta-
tions or image rotations) which change the input distribu-
tion. Class-Incremental Learning (Class-IL) operates on
the same assumptions as Task-IL, but requires the learner
to classify an example from any of the previously seen
classes with no hints about its original task. Unlike Task-
IL, this means that the model must learn the joint distri-
bution from partial observations, making this the hardestscenario [8]. For such a reason, we focus on limited labels
within the Class-IL formulation.
Towards realistic setups. Several recent works point out
that these classic settings lack realism [9] and consequently
dene new scenarios by imposing restrictions on what mod-
els are allowed to do while learning. Online Continual
Learning forbids multiple epochs on the training data on
the grounds that real-world CL systems would never see
the same input twice [10, 11, 12]. Task-Free Learning does
not provide task identities either at inference or at training
time [9]. This is in contrast with the classic settings that
signal task boundaries to the learner while training, thus
allowing it to prepare for the beginning of a new task.
This work also aims at providing a more realistic setup:
instead of focusing on model limitations, we acknowledge
that requiring fully labeled data can hinder the extension
of CL algorithms to real-time and in-the-wild scenarios.
Continual Learning with Unsupervised Data . Some at-
tempts have been recently made at improving CL methods
by exploiting unlabeled data. Zhang et al. proposed the
Deep Model Consolidation framework [13]; in it, a new
model is rst specialized on each new encountered task,
then a unied learner is produced by distilling knowledge
from both the new specialist and the previous incremen-
tal model. Alternatively, Lechat et al. introduced Semi-
Supervised Incremental Learning [14], which alternates
unsupervised feature learning on both input and auxiliary
data with supervised classication.
We remark that both these settings are signicantly
dierent from our proposed CSSL as we do not separate
the supervised and unsupervised training phases. On the
contrary, we intertwine both kinds of data in all drawn
batches in varying proportions and require that the model
learns from both at the same time. Additionally, we do not
exploit auxiliary unsupervised external data to supplement
the training set; instead, we reduce the original supervised
data to a fraction, thus modeling supervision becoming
available on the input stream at a much slower rate.
2.2. Continual Learning Methods
Continual Learning methods have been chiey catego-
rized in three families [7, 2].
Architectural methods employ tailored architectures in
which the number of parameters dynamically increases [15,
16] or a part of them is devoted to a distinct task [17].
While being usually very eective, they depend on the
availability of task labels at prediction time to prepare the
model for inference, which limits them to Task-IL.
Regularization methods condition the evolution of the model
to prevent it from forgetting previous tasks. This is at-
tained either by identifying important weights for each
task and preventing them from changing in later ones [18,
19] or by distilling the knowledge from previous model
snapshots to preserve the past responses [20, 21].
Rehearsal methods maintain a xed-size working mem-
ory of previously encountered exemplars and recall them
to prevent forgetting [22]. This simple solution has been

--- PAGE 3 ---
3
Task 1 Task 2Continual
Learning(CL)
Task 1
 Task 2Continual Semi-Supervised Learning (CSSL)
Fig. 1. Overview of the Continual Semi-Supervised Learning (CSSL) setting. Input batches include both labeled (green) and unlabeled (red) examples.
expanded upon in many ways, e.g.by adopting advanced
memory management policies [9, 23], exploiting meta-learning
algorithms [11], combining replay with knowledge distilla-
tion [24, 25], or using the memory to train the model in
an oine fashion [26].
2.3. Semi-Supervised Learning
Semi-Supervised Learning studies how to improve su-
pervised learning methods by leveraging additional unla-
beled data. We exploit the latter in light of specic as-
sumptions on how input and labels interact [5]. By as-
suming that close input data-points should correspond to
similar outputs, consistency regularization encourages the
model to produce consistent predictions for the same data-
point. This principle can be applied either by comparing
the predictions on the same exemplar by dierent learn-
ers [27, 6] or the predictions on dierent augmentations of
the same data-point by the same learner [28].
Recently, several works investigated the renement of
such regularization through adversarial training , produc-
ing either more challenging perturbations [29] or additional
unsupervised samples for regularization purposes [30].
Our proposal, which we introduce in Sec. 4.2, combines
within-task consistency regularization with the dual strat-
egy of maximizing cross-task feature dissimilarity. The
latter reinforces deep representation learning according to
the high-level structure of the target problem { specically,
cross-task class disjunction. This can be seen as a form of
Multi-Knowledge Representation [31] through the appli-
cation of descriptive knowledge; on the other hand, our
proposal remains open for further enrichment if additional
knowledge on the target task were available [32].
3. Continual Semi-Supervised Learning
A supervised Continual Learning classication problem
can be dened as a sequence Scomposed of Ttasks. Dur-
ing each of the latter ( S=t2f1;:::;Tg), input samples x
and their corresponding ground truth labels yare drawn
from an i.i.d. distribution Dt. Considering a function f
with parameters , we indicate its responses (logits) with
h(x) and the corresponding probability distribution over
the classes with f(x),softmax(h(x)). The goal is to
nd the optimal value for the parameters such thatf
performs best on average on all tasks without incurringcatastrophic forgetting; formally, we need to minimize the
empirical risk over all tasks:
argmin
tcX
t=1Lt;whereLt,E(x;y)Dt
`(y;f(x))
:(1)
InContinual Semi-Supervised Learning , we propose to dis-
tribute the samples coming from Dtinto two sets:Ds
t,
which contains a limited amount of pairs of labeled samples
and their ground-truth labels ( xs,ys) andDu
t, containing
the rest of the unsupervised samples. We dene this split
according to a given proportion ps=jDs
tj=(jDs
tj+jDu
tj)that
remains xed across all tasks. The objective of CSSL is op-
timizing Eq. 1 without having access to the ground-truth
supervision signal for Du
t. Data from the stream consists
of labeled pairsSDs
tand unlabeled items UDu
t.
We are interested in shedding further light on CL mod-
els by understanding i)how they perform under partial
lack of supervision and ii)how Semi-Supervised Learning
approaches can be combined with them to exploit unsu-
pervised data. Question i)is investigated experimentally
in Sec. 5.1 and 5.2 by evaluating methods that simply drop
unlabeled examples xu. Dierently, question ii)opens up
many possible solutions that we address by proposing Con-
trastive Continual Interpolation Consistency (CCIC).
4. Method
We build our proposal upon two state-of-the-art ap-
proaches: on the one hand, we take advantage of Expe-
rience Replay (ER) [22, 11] to mitigate catastrophic for-
getting; on the other, we exploit MixMatch [28] to learn
useful representations also from unlabeled examples. In
the following: i)to help the reader, we briey recap the
main traits of these algorithms (and let the original papers
provide a deeper comprehension); ii)we discuss how these
two former approaches can be favorably complemented.
4.1. Technical background
As a rst step, we equip the learner with a small mem-
ory buerM(based on reservoir sampling) and interleave
a batch of examples drawn from it with each batch of the
current task. Among all possible approaches, we opt for
ER due to its lightweight design and eectiveness [11, 23].

--- PAGE 4 ---
4
Unlabeled
anchor 
, ?
, 2
, ?
, 3
, 2
Task 2
, 0
, ?
Task 1
, 2Labeled
anchor
Task 1
, 2
Task 2
,3
Task 2A
PN
NA
NN
Task 2
Task 2
Fig. 2. CCIC exploits task identiers to enforce semantic constraints:
for each anchor (A), it asks the network to push away representations of
dierent (N) tasks and move closer representations of the same one (P).
When dealing with lack of supervision, self-training
represents a trivial strategy: here, the model itself pro-
duces the targets ( pseudo-labels ) for unlabeled examples [33,
34]. Unfortunately, this tends to become unstable with
only a few annotations at disposal: as shown in our exper-
iments, this encourages the model to overt the limited
supervised data available [35].
Such an issue raises the need for a dierent objec-
tive, the latter being independent from the accuracy of the
model on unlabeled examples. Consequently, we supple-
ment our proposal with MixMatch [28]: the predictions of
the network are not meant as training targets, but rather
as means for applying consistency regularization [6, 29].
Briey, a soft-label is assigned to each unsupervised ele-
ment by averaging and then sharpening the pre-softmax
predictions of several dierent augmentations.
To promote consistent responses to considerable vari-
ations of the data-points, labeled and unlabeled samples
are combined through the mixUp procedure [36]1. Start-
ing from the original sets SandU(respectively, labeled
and unlabeled examples from the current batch), we thus
obtain two nal augmented and mixed sets of examples
SandU: in order to compute the loss terms LSand
LU, we use the ground truth labels for the examples of the
former set and the soft-labels generated through response-
averaging for the ones of the latter.
4.2. Contrastive Continual Interpolation Consistency
Supposing that boundaries between tasks are provided,
we can associate in-memory exemplars with the task they
come from. In the following, we discuss how this allows
an additional weak form of supervision for unsupervised
examples even if we do not know their classes exactly.
1Dierently from MixMatch, we apply mixUp only on the input
images (and not to the corresponding labels).Unsupervised mining . As tasks are disjoint, examples
from dierent tasks necessarily belong to dierent classes:
we account for that by adding a contrastive loss term,
which pushes their responses away from each other (Fig. 2).
In details, we wish to maximize the Euclidean distance
D(x;x0),kh(x) h(x0)k2
2between embeddings of ex-
amples of dierent tasks. Hence, we minimize:
LUM=ExDu
tc
xNMt<tc
max( D(x;xN);0)
; (2)
wheretcis the index of the current task Dtc,Mt<tcindi-
cates past examples from the memory buer, and is a
constant margin beyond which no more eorts should be
put into enlarging the distance between negative pairs.
Supervised mining . For each incoming labeled example,
we also encourage the network to move its representation
close to those belonging to the same class. We look for
positive candidates xPwithin both the current batch and
the memory buer. In formal terms:
LSM=ExDs
tc[M
relu( D(x;xN) +D(x;xP)
:(3)
Overall objective . To sum up, the objective of CCIC com-
bines the consistency regularization term delivered by Mix-
Match with the two additional ones (Eq. 2 and Eq. 3) ap-
plied in feature space; the overall optimization problem
can be formalized as follows:
argminL=LS+LU+LSM+LUM; (4)
whereandare hyperparameters setting the importance
of the unsupervised examples.
Exploiting distance metric learning during inference . Once
we have introduced constraints in feature space (Eq. 2, 3),
we can also exploit them by devising a dierent inference
schema, which further contributes to relieve forgetting.
Similarly to [24], we employ the k-Nearest Neighbors al-
gorithm as nal classier, thus decoupling classication
from feature extraction. This has been shown benecial
in Continual Learning, as it saves the nal fully-connected
layer from continuously keeping up with the changing fea-
tures (and vice versa ). As kNN is non-parametric and
builds upon the feature space solely, it ts in harmony
with the rest of the model, controlling the damage caused
by catastrophic forgetting. We t the kNN classier using
the examples of memory buer as training set.
5. Experiments
We conduct our experiments on three standard datasets2.
Split SVHN : ve subsequent binary tasks built on top
of the Street View House Numbers (SVHN) dataset [37];
Split CIFAR-10 : equivalent to the previous one, but us-
ing the CIFAR-10 dataset [38]. Split CIFAR-100 : a longer
2Code available at https://github.com/loribonna/CSSL .

--- PAGE 5 ---
5
Table 1. Average Accuracy of CL Methods and of Our Proposals on CSSL Benchmarks.
Class-IL Â¦ SVHN (UB: 86:21:8) Â¦ CIFAR-10 (UB: 92:10:1) Â¦ CIFAR-100 (UB: 67:70:9)
Labels % Â¦0:8% 5% 25%Â·100%Â¦0:8% 5% 25%Â·100%Â¦0:8% 5% 25%Â·100%
Fine Tuning Â¦9:91:7 9:98:417:59:4Â·17:81:2Â¦13:62:918:20:419:22:2Â·19:68:4Â¦1:80:2 5:00:3 7:80:1Â·8:60:4
LwF Â¦9:90:3 9:91:914:83:6Â·16:90:1Â¦13:12:217:73:219:41:7Â·19:610:3Â¦1:60:1 4:50:1 8:00:1Â·8:40:5
oEWC Â¦9:90:2 9:90:714:70:5Â·17:90:2Â¦13:71:217:61:219:10:8Â·19:67:5Â¦1:40:1 4:70:1 7:80:4Â·7:80:1
SI Â¦9:91:210:25:917:17:7Â·18:20:2Â¦12:40:415:91:019:21:3Â·19:53:3Â¦1:30:2 3:40:2 7:50:5Â·8:11:2
ER500 Â¦32:57:156:02:059:71:8Â·66:52:8Â¦36:31:151:94:560:95:7Â·62:22:6Â¦8:20:113:70:617:10:7Â·21:30:2
iCaRL 500 Â¦8:90:410:01:519:91:2Â·23:12:4Â¦24:72:335:83:251:48:4Â·61:00:4Â¦3:60:111:30:327:60:4Â·37:80:3
DER 500 Â¦11:91:754:62:656:95:8Â·70:83:7Â¦29:10:435:38:350:02:3Â·67:11:6Â¦1:70:1 5:10:913:05:3Â·28:87:2
GDumb 500 Â¦34:65:141:88:359:28:5Â·59:99:7Â¦39:69:640:911:844:85:4Â·47:91:6Â¦8:60:1 9:90:410:10:4Â·11:01:8
PseudoER 500Â¦23:20:748:91:263:62:7Â· - Â¦37:81:644:92:356:31:6Â· - Â¦5:10:614:30:118:50:5Â· -
CCIC 500 Â¦55:33:270:13:975:91:5Â· - Â¦54:00:263:31:963:92:6Â· - Â¦11:50:719:50:220:30:3Â· -
ER5120 Â¦44:41:469:93:677:68:7Â·80:53:2Â¦37:42:364:15:379:71:2Â·83:32:8Â¦9:60:622:80:337:90:6Â·49:00:2
iCaRL 5120 Â¦9:30:211:50:519:53:7Â·23:94:5Â¦20:73:335:55:656:32:2Â·61:91:5Â¦4:30:112:20:330:91:0Â·41:20:4
DER 5120 Â¦23:11:067:85:274:72:4Â·75:37:6Â¦32:90:947:62:273:94:5Â·84:52:1Â¦1:60:1 4:70:611:93:4Â·38:63:6
GDumb 5120 Â¦46:58:074:42:374:63:8Â·78:32:3Â¦40:812:771:22:681:40:8Â·82:50:5Â¦9:61:123:30:133:22:2Â·42:91:7
PseudoER 5120Â¦45:82:874:62:477:90:8Â· - Â¦62:22:172:92:080:40:1Â· - Â¦8:21:425:11:640:00:4Â· -
CCIC 5120 Â¦59:35:381:02:383:90:2Â· - Â¦55:21:474:31:784:70:9Â· - Â¦12:00:329:50:444:30:1Â· -
and more challenging evaluation in which the model is pre-
sented ten subsequent tasks, each comprising of 10 classes
from the CIFAR-100 dataset [38].
We vary the fraction of labeled data shown to the
model (ps) to encompass dierent degrees of supervision
(0:8%, 5%, 25%, and 100%, i.e., 400, 2500, 25000, and
50000 samples for CIFAR-10/100). For fairness, we keep
the original balancing between classes in both train and
test sets; in presence of low rates, we make sure that each
class is represented by a proportional amount of labels.
Architectures. As in [39], experiments on Split SVHN are
conducted on a small CNN, comprising of three ReLU
layers interleaved by max-pooling. Instead, we rely on
ResNet18 for CIFAR-10 and CIFAR-100, as done in [25].
Metrics. We report the performance in terms of average
nal accuracy, as done in [10, 9]. Accuracies are averaged
across 5 runs (we also report standard deviations).
Implementation details. As discussed in Sec. 4, our pro-
posals rely on data augmentation to promote consistency
regularization. We apply random cropping and horizon-
tal ipping (except for Split SVHN); the same choice is
applied to competitors to ensure fairness. To perform
hyperparameters selection (learning rate, batch size, op-
timization algorithm, and regularization coecients), we
perform a grid search on top of a validation set (corre-
sponding to 10% of the training set), as done in [11, 25, 24].
For CCIC, we keep the number of augmentations xed to 3
and report chosen values for andin Tab. 2. To guaran-
tee fairness, we x the batch size and memory minibatch
size to 32 for all models. We train on each task for 10
epochs on SVHN, for 50 on CIFAR-10, and 30 on CIFAR-
100. All methods use SGD as an optimizer with the only
exception of CCIC, which employs Adam.
5.1. Baselines
Lower/Upper bounds . We bound the performance for our
experiments by including two reference measures. As a
lower bound, we evaluate the performance of a model trainedTable 2. Values of (;)for CCIC chosen after the grid-search.
Lab. %jMj SVHN CIFAR-10 CIFAR-100
0:8%500 (0:5;0:5) (0:5;0:5) (0:3;1:0)
5120 (0:1;0:5) (0:5;0:5) (0:3;0:3)
5%500 (0:1;0:5) (0:3;0:5) (0:5;0:5)
5120 (0:1;0:5) (0:5;0:5) (0:5;0:5)
25%500 (0:5;0:5) (0:1;0:5) (0:5;0:7)
5120 (0:5;0:5) (0:1;1:0) (0:5;0:5)
byFine Tuning exclusively on the set of supervised exam-
ples, without any countermeasure to catastrophic forget-
ting. We also provide an upper-bound (UB) given by a
model trained jointly, i.e., without dividing the dataset
into tasks or discarding any ground-truth annotation.
Drop-the-unlabeled . The most straightforward approach
to adapt existing methods to our setting consists in simply
discarding unlabeled examples from the current batch. In
this regard, we compare our proposal with Learning With-
out Forgetting (LwF) [20], online Elastic Weight Consol-
idation (oEWC) [21], Synaptic Intelligence (SI) [19], Ex-
perience Replay (ER) [11], iCaRL [24], Dark Experience
Replay (DER) [25] and GDumb [26]. By so doing, we
can verify whether our proposal is able to better sustain a
training regime with reduced supervision.
Pseudo-Labeling . Inspired by the line of works relying on
self-labeling [33, 34], we here introduce a simple CSSL
baseline that allows ER to prot from the unlabeled exam-
ples: given an unlabeled example xu, it pins as a pseudo-
label ~yu[34] the prediction of the model itself. Formally,
~yu= argmaxc2Cthc
(xu); (5)
whereCtis the set of classes of the current task. As dis-
cussed in Sec. 4.1, self training is likely to cause model
instability (especially at task boundaries, when the model
starts to experience new data): we mitigate this by apply-
ing a threshold to discard low-condence outputs and
their relative xu. Specically, we estimate the condence

--- PAGE 6 ---
6
as the dierence between the two highest values of hc
(xu).
After this step, a pair ( xu;~yu) is considered on a par with
any supervised pair ( xs;ys), and is therefore inserted into
the memory buer. We refer to this baseline as PseudoER .
5.2. Experimental Results
As revealed by the results in Tab. 1, CSSL proves to
be a challenging scenario. Unsurprisingly, its diculty in-
creases when fewer labels are provided to the learner.
Regularization methods are generally regarded as weak in
the Class-IL scenario [7, 9]. This conforms with our em-
pirical observations, as LwF, oEWC and SI underperform
across all datasets. Indeed, these methods rarely outper-
form our lower bound (Fine Tuning), indicating that they
are not eective outside of Task-IL and Domain-IL. This
becomes especially evident in the low-label regime.
Rehearsal methods overall show an expected decrease in
performance as supervision diminishes. This is especially
severe for DER and iCaRL, as their accuracy drops on av-
erage by more than 70% between 100% and 0 :8% labels.
As the model underts the task when less supervision is
provided, it produces less reliable targets that cannot be
successfully used for replay by these methods. In con-
trast, ER is able to replay information successfully as it
exploits hard targets; thus, it learns eectively even after
initially undertting the task. Indeed, its accuracy with
5% labels and buer 5120 is always higher than its fully-
supervised accuracy with a smaller buer. While ER is
able to overcome the lack of labels when paired with an ap-
propriate buer, knowledge-distillation based approaches
remarkably encounter a major hindrance in this setting.
We attribute the failure of iCaRL on SVHN to the low
complexity of the backbone network. Indeed, a shallow
backbone provides for a latent space that is less suitable for
its nearest-mean-of-exemplars classier. Conversely, this
method proves quite eective even with a reduced mem-
ory buer on CIFAR-100. In this benchmark, the herding
sampling of iCaRL ensures that all classes are fairly rep-
resented even in a small memory buer.
Finally, GDumb does not suer from lower supervision
as long as its buer can be lled completely: its operation
is not disrupted by unlabeled examples on the stream, as
it ignores the latter entirely. While it outperforms other
methods when few labels are available, CCIC surpasses it
consistently. This suggests that the stream oers potential
for further learning and should not be dismissed.
CSSL Methods . Our PseudoER baseline performs notably
well on CIFAR-10, maintaining high accuracy as the amount
of supervision decreases. However, while CIFAR-10 is a
complex benchmark, it only features two classes for each
task, which makes it easy for pseudo-labeling to produce
reasonable responses (it is noted that a random guess would
result in 50% accuracy). Conversely, PseudoER struggles
to produce valid targets and exhibits a swift performance
drop on CIFAR-100 as the availability of labeled data
decreases. Similarly, we nd the application of pseudo-
labeling benecial for SVHN only as the space reservedTable 3. Unsupervised Mining Techniques for CCIC on CIFAR-100.
Labels % (jMj= 5120 ) 5% 25%
Across-Task Mining (Eq. 2) 29 :50:444:30:1
Within-Task Mining 29 :30:244:00:2
Task-Agnostic Mining 29 :10:743:90:8
Table 4. Average Accuracy of alternative CSSL proposals on CIFAR-10
Labels % 0:8% 5% 25%
ER+EMA 500 21:40:526:31:043:31:2
CCIC 500 54:00:263:32:163:92:6
ER+EMA 5120 25:90:840:82:164:80:4
CCIC 5120 55:21:274:31:784:70:9
for the buer increases, demonstrating the pitfalls of this
approach in the online setting.
On the contrary, the compelling performance of CCIC
indicates successful blending of supervised information and
semi-supervised regularization. While ER encounters an
average performance drop of 47%, going from 25% to 0 :8%
labels on CIFAR-10, CCIC only loses 26% on average. Sur-
prisingly, we observe that { for the majority of evaluated
benchmarks { 25% supervision is enough to approach the
results of fully-supervised methods, even outperforming
the state-of-the-art in some circumstances (CIFAR-10 with
buer size 5120, SVHN with buer size 500 and 5120).
This hints that, when learning from a stream of data,
striving to provide full supervision is not as essential as it
might be expected: dierently from the oine scenario, a
greater amount of labels might not produce a proportion-
ate prot due to catastrophic forgetting . In this respect,
our experiments suggest that pairing few labeled examples
with semi-supervised techniques represents a more ecient
paradigm to achieve satisfying performance.
Unsupervised Mining in CCIC. In its unsupervised mining
loss termLUM, CCIC takes examples of previous tasks in
the memory buer as negatives ( Across-Task Mining ) and
requires their representations to be pushed away from cur-
rent data. In Tab. 3, we compare this design choice with
two alternative strategies: i) Within-Task Mining , where
we let the model choose the negatives from the current task
only; and ii) Task-Agnostic Mining , where the model can
freely pick a negative example from either the memory or
the current batch without any task-specic prior. As can
be observed, Task-Agnostic Mining and Within-Task Min-
ing lead to a small but consistent decrease in performance,
whileLUMproves to be the most rewarding strategy.
Model-driven Consistency. In addition with combining a
contrastive form of consistency regularization with ER, we
propose an additional temporal consistency baseline which
requires the activations of the model to match a slower
moving-average checkpoint. Results in Tab. 4 show, how-
ever, that such approach under-performs consistently, not
even reaching the performance of ER. This suggests that,
dierently from fully-supervised scenarios [6], exponential
moving average approaches do not necessarily scale to CL.

--- PAGE 7 ---
7
6. Conclusion
Catastrophic forgetting prevents most current state-of-
the-art models from sequentially learning multiple tasks,
forcing practitioners to heavy resource-demanding training
processes. Moreover, many of the applications that might
benet from CL algorithms are often characterized by la-
bel scarcity. For this reason, we investigate the possibility
of leveraging unlabeled data-points to enhance the perfor-
mance of Continual Learning models, a scenario that we
name Continual Semi-Supervised Learning (CSSL) .
We further propose Constrastive Continual Interpola-
tion Consistency (CCIC) , an incremental approach that
combines the benets of rehearsal with consistency reg-
ularization and distance-based constraints. Remarkably,
our experiments suggest that well-designed methods can
eectively exploit the unlabeled examples to prevent for-
getting. This indicates that the eort of annotating all
data may be unnecessary in a continual scenario.
Acknowledgement
This work was supported by the FF4EuroHPC: HPC
Innovation for European SMEs, Project Call 1. Project
FF4EuroHPC has received funding from the European
High-Performance Computing Joint Undertaking (JU) un-
der grant agreement No 951745.
References
[1] M. McCloskey, N. J. Cohen, Catastrophic interference in con-
nectionist networks: The sequential learning problem, Psychol
Learn Motiv doi: 10.1016/S0079-7421(08)60536-8 (1989).
[2] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia,
A. Leonardis, G. Slabaugh, T. Tuytelaars, A continual learn-
ing survey: Defying forgetting in classication tasks, IEEE
TPAMI doi: 10.1109/TPAMI.2021.3057446 (2021).
[3] R. Aljundi, K. Kelchtermans, T. Tuytelaars, Task-free continual
learning, in: CVPR, 2019.
[4] W. Zhou, S. Chang, N. Sosa, H. Hamann, D. Cox, Lifelong
object detection, arXiv:2009.01129 (2020).
[5] C. Olivier, S. Bernhard, Z. Alexander, Semi-supervised learning,
2006, doi: 10.7551/mitpress/9780262033589.001.0001 .
[6] A. Tarvainen, H. Valpola, Mean teachers are better role models:
Weight-averaged consistency targets improve semi-supervised
deep learning results, in: ANIPS, 2017.
[7] S. Farquhar, Y. Gal, Towards robust evaluations of continual
learning, in: ICML Workshop, 2018.
[8] G. M. van de Ven, A. S. Tolias, Three continual learning sce-
narios, in: ANIPS Workshop, 2018.
[9] R. Aljundi, M. Lin, B. Goujaud, Y. Bengio, Gradient based
sample selection for online continual learning, in: ANIPS, 2019.
[10] D. Lopez-Paz, M. Ranzato, Gradient episodic memory for con-
tinual learning, in: ANIPS, 2017.
[11] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y. Tu,
G. Tesauro, Learning to learn without forgetting by maximizing
transfer and minimizing interference, in: ICLR, 2019.
[12] A. Chaudhry, A. Gordo, P. K. Dokania, P. Torr, D. Lopez-Paz,
Using hindsight to anchor past knowledge in continual learning,
in: AAAI Conf. Artif. Intell., 2021.
[13] J. Zhang, J. Zhang, S. Ghosh, D. Li, S. Tasci, L. Heck, H. Zhang,
C.-C. J. Kuo, Class-incremental learning via deep model con-
solidation, in: WACV, 2020.
[14] A. Lechat, S. Herbin, F. Jurie, Semi-supervised class incremen-
tal learning, in: ICPR, 2021.[15] J. Serra, D. Suris, M. Miron, A. Karatzoglou, Overcoming catas-
trophic forgetting with hard attention to the task, in: ICML,
2018.
[16] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A.
Rusu, A. Pritzel, D. Wierstra, Pathnet: Evolution channels
gradient descent in super neural networks, arXiv:1701.08734
(2017).
[17] A. Mallya, S. Lazebnik, Packnet: Adding multiple tasks to a
single network by iterative pruning, in: CVPR, 2018.
[18] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Des-
jardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho,
A. Grabska-Barwinska, et al., Overcoming catastrophic forget-
ting in neural networks, PNAS doi: 10.1073/pnas.1611835114
(2017).
[19] F. Zenke, B. Poole, S. Ganguli, Continual learning through
synaptic intelligence, in: ICML, 2017.
[20] Z. Li, D. Hoiem, Learning without forgetting, IEEE
TPAMI doi: 10.1109/TPAMI.2017.2773081 (2017).
[21] J. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska,
Y. W. Teh, R. Pascanu, R. Hadsell, Progress & compress: A
scalable framework for continual learning, in: ICML, 2018.
[22] R. Ratcli, Connectionist models of recognition memory: con-
straints imposed by learning and forgetting functions., Psychol.
Rev. doi: 10.1037/0033-295x.97.2.285 (1990).
[23] P. Buzzega, M. Boschini, A. Porrello, S. Calderara, Rethinking
experience replay: a bag of tricks for continual learning, in:
ICPR, 2020.
[24] S. Rebu, A. Kolesnikov, G. Sperl, C. Lampert, icarl: Incre-
mental classier and representation learning, in: CVPR, 2017.
[25] P. Buzzega, M. Boschini, A. Porrello, D. Abati, S. Calderara,
Dark experience for general continual learning: a strong, simple
baseline, in: ANIPS, 2020.
[26] A. Prabhu, P. H. Torr, P. K. Dokania, Gdumb: A simple ap-
proach that questions our progress in continual learning, in:
ECCV, 2020.
[27] S. Laine, T. Aila, Temporal ensembling for semi-supervised
learning, in: ICLR, 2017.
[28] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver,
C. A. Rael, Mixmatch: A holistic approach to semi-supervised
learning, in: ANIPS, 2019.
[29] T. Miyato, S.-i. Maeda, M. Koyama, S. Ishii, Vir-
tual adversarial training: a regularization method
for supervised and semi-supervised learning, IEEE
TPAMI doi: 10.1109/TPAMI.2018.2858821 (2018).
[30] Z. Zheng, L. Zheng, Y. Yang, Unlabeled samples generated by
gan improve the person re-identication baseline in vitro, in:
ICCV, 2017.
[31] Y. Yang, Y. Zhuang, Y. Pan, Multiple knowledge repre-
sentation for big data articial intelligence: framework, ap-
plications, and case studies, Front. Inf. Technol. Electron.
Eng. doi: 10.1631/FITEE.2100463 (2021).
[32] Y. Pan, Multiple knowledge representation of articial intelli-
gence, Engineering doi: 10.1016/j.eng.2019.12.011 (2020).
[33] D. Yarowsky, Unsupervised word sense disambigua-
tion rivaling supervised methods, in: ACL, 1995,
doi:10.3115/981658.981684 .
[34] D.-H. Lee, Pseudo-label: The simple and ecient semi-
supervised learning method for deep neural networks, in: ICML
Workshop, 2013.
[35] A. Oliver, A. Odena, C. A. Rael, E. D. Cubuk, I. Goodfel-
low, Realistic evaluation of deep semi-supervised learning algo-
rithms, in: ANIPS, 2018.
[36] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup: Be-
yond empirical risk minimization, in: ICLR, 2018.
[37] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, A. Y.
Ng, Reading digits in natural images with unsupervised feature
learning, in: ANIPS, 2011.
[38] A. Krizhevsky, et al., Learning multiple layers of features from
tiny images, Tech. rep. (2009).
[39] D. Abati, J. Tomczak, T. Blankevoort, S. Calderara, R. Cuc-
chiara, B. E. Bejnordi, Conditional channel gated networks for
task-aware continual learning, in: CVPR, 2020.
