# 2207.05615.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/contrastive/2207.05615.pdf
# File size: 585346 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CONTRASTIVE LEARNING FOR ONLINE SEMI-SUPERVISED GENERAL
CONTINUAL LEARNING
Nicolas Michel, Romain Negrel, Giovanni Chierchia, Jean-Fran¸ cois Bercher
Univ Gustave Eiﬀel, CNRS, LIGM, F-77454 Marne-la-Vall´ ee, France
ABSTRACT
We study Online Continual Learning with missing labels and
propose SemiCon, a new contrastive loss designed for partly
labeled data. We demonstrate its eﬃciency by devising a
memory-based method trained on an unlabeled data stream,
where every data added to memory is labeled using an oracle.
Our approach outperforms existing semi-supervised meth-
ods when few labels are available, and obtain similar re-
sults to state-of-the-art supervised methods while using only
2.6% of labels on Split-CIFAR10 and 10% of labels on Split-
CIFAR100.
Index Terms —Continual Learning, Contrastive Learn-
ing, Semi-Supervised Learning, Memory
1. INTRODUCTION
In the last decade, deep neural networks have demonstrated
their eﬃciency achieving state-of-the-art results in several
computer vision tasks, such as image classiﬁcation or object
detection. Eﬃciently training such networks relies on the fol-
lowing assumptions.
A1. Data is identically and independently distributed.
A2. Training data can be seen multiple times by the model
during the learning process.
A3. Training data is fully labeled.
However, these assumptions are seldom true in real-world en-
vironments with a continuous data stream. In such scenarios,
Assumption A1 cannot be veriﬁed, Assumption A2 is diﬃ-
cult to ensure as the amount of data grows indeﬁnitely, and
Assumption A3 would imply an oracle labeling every new in-
coming data. While numerous successful training strategies
have been designed to leverage unlabeled data [1, 2] when A3
cannot be met, coping with the absence of A1 and A2 remains
problematic [3]. A known consequence is Catastrophic For-
getting [4], and is likely to be observed if no speciﬁc measure
is taken. Continual Learning (CL) aims to maintain perfor-
mances in the absence of A1, and Online Continual Learning
(OCL) addresses the lack of A1 and A2.
In this paper, we propose an OCL algorithm for image
classiﬁcation with few labels. Our method is designed to
train a neural network in the absence of A1, A2 and A3. We
achieve this by introducing a uniﬁed Semi-Supervised Con-
trastive Loss to leverage labeled and unlabeled data. Despite
This work has received support from Agence Nationale de la
Recherche (ANR) for the project APY, with reference ANR-20-
CE38-0011-02. This work was granted access to the HPC resources
of IDRIS under the allocation 2022-AD011012603 made by GENCI
Code is available at https://github.com/Nicolas1203/ossgclthe lack of A3, our results show that the proposed approach is
comparable to state-of-the-art supervised OCL methods, and
performs better than existing semi-supervised OCL methods
when labels are scarce.
The paper is organized as follows. Section 2 discusses the
related work. Section 3 describes our approach: it sets up
the problem, introduces a new Semi-Supervised Contrastive
Loss, and describes the algorithm. Section 4 assesses the per-
formance of our approach on a standard benchmark. Finally,
Section 5 concludes the paper.
2. RELATED WORK
Our method stands at the junction of Contrastive Learning,
Continual Learning and Semi-Supervised learning. In the fol-
lowing section we will brieﬂy recall these ﬁelds and elaborate
onto speciﬁc scenarios where label information is incomplete.
2.1. Contrastive Learning
Contrastive Learning has become especially popular in recent
years for learning representation from data [1, 2, 5, 6, 7, 8].
The intuition is quite simple. Similar samples (called posi-
tives) should have close representations, while dissimilar sam-
ples (called negatives) should have their representations as far
away as possible.
Self-Supervised Contrastive Learning. Contrastive
Learning was ﬁrst designed for unlabeled data, where posi-
tives are formed by adding noise to the input. For images,
noisy versions are augmentations or views of the original im-
age. All views of the same inputs are positives, while any
other image in the training batch is considered a negative.
This self-supervision has several drawbacks: large batches are
required for sampling enough negatives [1], and images from
the same class might be considered as negatives [5, 6, 2].
Supervised Contrastive Learning. A supervised con-
trastive approach has been proposed by Khosla et al. [8] us-
ing label information to overcome some limitations of self-
supervision. The authors consider every image from the same
class as positives and show signiﬁcant improvement.
2.2. Continual Learning
Continual Learning (CL) has gained popularity in the past
years for image classiﬁcation. The problem is as follows. Con-
sider a sequential learning setup with a sequence {T1,...,TK}
ofKtasks, andDk= (Xk,Yk) the corresponding data-label
pairs. The number of tasks Kis unknown and could poten-
tially be inﬁnitely large. For any values k1,k2∈{1,...,K},
ifk1/negationslash=k2then we have Yk1∩Yk2=∅and the number ofarXiv:2207.05615v2  [cs.LG]  22 Nov 2022

--- PAGE 2 ---
classes in each task is the same. Catastrophic Forgetting oc-
curs when a model’s performance drastically drops on past
tasks while learning the current task [4, 9].
General Continual Learning. Early CL studies, and
still the majority of current methods, rely on settings where
the task-id kis known [10, 11, 12], or at least the task-
boundary is known (i.e. knowing when the task change oc-
curs). However, such information is usually unavailable while
training in a real environment [10, 13, 14, 15]. Buzzega et al.
[16] introduced the General Continual Learning (GCL) sce-
nario where task-ids and task-boundaries are unavailable.
Online GCL. Working with a data stream, the model
should be able to learn without storing all incoming data.
The most realistic scenario Online General Continual Learn-
ing (OGCL), described by Buzzega et al. [16], adds one more
constraint to GCL: data are presented once in the incoming
stream. This means that the model should adapt to current
data without having access to all past data, even if we can
aﬀord to store a limited amount of stream data.
CL with missing labels. While most CL research fo-
cuses on the supervised setting, recent works consider CL
where few or no labels are available [17, 18, 19]. However,
none of them deals with the OGCL setting except the STAM
architecture [20], where the authors present an online clus-
tering suited for unsupervised OGCL. In this paper we focus
on the OGCL with few labels, which we present in Section 3.
3. ONLINE SEMI-SUPERVISED GENERAL
CONTINUAL LEARNING
In this section we formally deﬁne the Online Semi-Supervised
General Continual Learning (OSSGCL) setting, and propose
an approach to address the underlying problem.
3.1. Problem Deﬁnition
In the supervised case, each value in Y=∪K
k=1Ykis ac-
cessible. We consider the semi-supervised case, where we
iterate over an incremental unlabeled data stream S=
{X1,...,XK}and use an oracle to label speciﬁcally selected
data. In this context, we have access to a subset Yl⊂Ywith
p=|Yl|
|Y|being the percentage of labels available. Usually
in semi-supervised learning we want pas small as possible.
In addition, we consider that we have labeled examples for
every classes encountered. We deﬁne Xu
kand (Xl
k,Yl
k) the
sets of unlabeled and labeled data for task k. The problem
then becomes learning sequentially from {T1,..,TK}tasks on
partially labeled datasets Dk=Xu
k∪(Xl
k,Yl
k).
3.2. Proposed Approach
To tackle the OSSGCL problem, we design a new Semi-
supervised Contrastive Loss (SemiCon) combining a Super-
vised Contrastive Loss [8] and a Self-supervised Contrastive
Loss [1].
Contrastive Learning framework. Following [1], con-
sider the following elements: a data augmentation process
Aug(·) that transforms any given input according to some
random procedure with a/negationslash=b⇔Auga(·)/negationslash= Augb(·); an en-
coder Enc θ(·) that maps the input to the latent space; a pro-
jection head Projφ(.) that maps the latent representation to
another space where the loss is applied. The encoder can beany function and the obtained latent representation will be
used for downstream tasks. The projection head is discarded
when training is complete to keep only the encoder.
Multiview batch with missing labels. We deﬁneB=
{xl
i,yl
i}i=1..bl∪{xu
j}j=1..buas the incoming batch of b=bl+
buinputs with labeled and unlabeled data. Considering a,b
random numbers, we then work on BI= Auga(B)∪Augb(B),
the ”multiview batch” [8] of 2 bsamples over indices i∈Iand
I=Il∪IuwithIlthe indices over labeled samples and Iu
the indexes over unlabeled samples. Moreover, hi= Enc(xi)
is the latent representation of xiwithHI={hi}i∈I, and
zi= Proj(hi) is the projection of hiwithZI={zi}i∈I. We
also deﬁne P(i) ={j∈I\{i}|yj=yi}the indices over the
positives of i(similar samples), and τthe temperature.
SemiCon Loss. We introduce SemiCon, a uniﬁed loss
designed to train a contrastive model in the context of missing
labels. We construct this loss by combining two terms. The
ﬁrst one,Lm(1), corresponds to a supervised contrastive loss
on labeled memory data with unlabeled streaming data being
considered as negatives (dissimilar samples).
Lm=−/summationdisplay
i∈Il1
|P(i)|/summationdisplay
p∈P(i)logezi·zp/τ
/summationdisplay
a∈I\{i}ezi·za/τ(1)
The second term, Lu(2), stands for an unsupervised con-
trastive loss on unlabeled stream data with labeled steaming
data being considered as negatives with j(i) the index such
thatiandj(i) are indices of augmented samples having the
same input source.
Lu=−/summationdisplay
i∈Iulogezi·zj(i)/τ
/summationdisplay
a∈I\{i}ezi·za/τ(2)
We deﬁne a uniﬁed loss LSemiCon =Lm+αLuwhereα∈
[0,+∞[ is a weighting hyper-parameter representing the im-
portance of unlabeled data during training. We can see that
(∀i∈Iu)P(i) ={j(i)}so the loss can be expressed as
LSemiCon =−/summationdisplay
i∈Igα(i)
|P(i)|/summationdisplay
p∈P(i)logezi·zp/τ
/summationdisplay
a∈I\{i}ezi·za/τ(3)
withgα(i) =αifi∈Il, andgα(i) = 1 otherwise. Methods
replaying past data can suﬀer from overﬁtting on memory
data [21] and while SemiCon handles missing labels, it also
gives control over how a model should balance learning from
memory and streaming data separately.
Training Procedure. We propose an approach inspired
by the work of Mai et al. [22]. They deﬁned Supervised Con-
trastive Replay (SCR) which combines a Supervised Con-
trastive Loss [8] and a memory-based strategy [23]. Their
method achieves state-of-the-art results in online CL when
every data is labeled. We adapt SCR to work with a lim-
ited amount of labeled data using SemiCon as objective. Our
approach relies on two points: (a) each data added to the
memory buﬀer is labeled, (b) we leverage labeled and unla-
beled data in a uniﬁed contrastive objective using SemiCon.
During the training phase, we iterate over an unlabeled
data streamS. For each incoming stream batch Bs∈S, we
randomly sample a labeled data batch Bmfrom memoryM

--- PAGE 3 ---
Algorithm 1 Proposed Training Method
Input: Unlabeled data stream S; MemoryM; Aug(.);
Encθ(.); Projφ(.); OracleO
Output: Encoder Enc θ; MemoryM
forBs∈Sdo ⊿Unlabeled data
Bm←MemoryRetrieval( M) ⊿Labeled data
B←Bs∪Bm ⊿Combined Batch
BI←Auga(B)∪Augb(B) ⊿a,b randoms
ZI←Projφ(Encθ(BI))
θ,φ←SGD(LSemiCon (ZI),θ,φ) ⊿eq.(3)
M← MemoryUpdate(Bs,O,M)
return:θ;M
and work onB=Bs∪Bm. Each data batch Bis augmented,
and the obtained multiview batch BIis fed to the network
to compute image projections ZI. The objective function
LSemiCon is computed on ZI, and the model parameters are
updated using vanilla Steepest Gradient Descent (SGD). Af-
ter each SGD step, memory data is updated using Reservoir
Sampling [24]. Each selected stream data is labeled using the
OracleObefore memory storage.
During the testing phase, memory data representations
HM={Encθ(x)}x∈M are computed and a classiﬁer is
trained on HM. Our approach is detailed in Algorithm
1. Similarly to Mai et al. , we use the Nearest Class Mean
(NCM) classiﬁer for the testing phase. Any other classiﬁer
can be used on top of the obtained representations; however,
we found no signiﬁcant diﬀerence in performance.
4. EXPERIMENTAL RESULTS
In this section, we describe two CL datasets, introduce base-
lines and present our results compared to state-of-the-art.
4.1. Datasets
We use modiﬁed versions of standard image classiﬁcation
datasets [25] to build an incremental learning environment.
These datasets are built on CIFAR10 and CIFAR100 by
splitting them into several tasks of non-overlapping classes.
Speciﬁcally, we work on Split-CIFAR10 and Split-CIFAR100.
We divide CIFAR10 in 5 tasks with 2 classes per task and
CIFAR100 in 10 tasks with 10 classes per task. Each dataset
contains 50,000 train images and 10,000 test images.
4.2. Baselines
To assess our results, we compare them to several baselines
which respect the OGCL setting and are listed below:
•oﬄine : Supervised upper bound. The model is trained
without any CL speciﬁc constraints.
•ﬁne-tuned : Supervised lower bound that trains the model
in a CL setting without precautions to avoid forgetting.
•SCR : Current state-of-the-art on fully supervised OGCL
and closest method to our work.
•SCR - Memory Only (SCR-MO): SCR method, but
trained using memory data only.
•Experience Replay (ER) [23]: ER is a simple baseline
which applies reservoir sampling [24] for memory update,just as SCR, but trained with a cross entropy loss rather
than a contrastive loss.
•Experience Replay - Memory Only (ER-MO): ER-
MO is essentially ER, but trained using memory data only.
Even though other semi-supervised CL methods exist, none
respects the OSSGCL setting and thus cannot be used in the
comparison [14, 18, 26].
4.3. Implementation Detail
For every experiment we train a reduced ResNet-18 [27] from
scratch following previous works [13, 22] and the projection
layerProjφis a multi layer perceptron [1] with one hidden
layer, a ReLU activation and an output size of 128. For mem-
ory based methods, we use a memory batch size |Bm|of 100
on Split-CIFAR10 and of 500 on Split-CIFAR100. More de-
tails on the impact of memory batch size can be found in
section 4. For online methods, we use a stream batch size
|Bs|of 10, which ensures 5,000 SGD steps on both datasets.
Each compared method is trained using SGD optimizer with
a learning rate of 0.1, no regularization, and a temperature
τ= 0.07 for contrastive loss. For every experiment we used
the same augmentation procedure as in [1] and for contrastive
methods we construct the multiview batch using one augmen-
tation for each view, while the original SCR implementation
used the original image as one view and an augmentation as
the other view. We obtained experimentally better results
using one augmentation for each view. All experiments are
performed 10 times. The average results and their standard
deviations are shown in the next section. For the oﬄine base-
line, we use the same optimizer and network as other methods
and train for 50 epochs.
4.4. Results
In the following we brieﬂy recall a standard CL metric, de-
scribe the impact of two hyper-parameters and analyze the
obtained results.
Metrics. We use the accuracy averaged across all tasks
after training on the last task. This metric is referred to as
the ﬁnal average accuracy [11, 10].
Memory Batch Size selection. We study the impact
of the memory batch size |Bm|on the performance. We use
SCR, SCR-MO and our proposed approach as the case of
study. As shown in ﬁgure 1, every method follows the same
trend and beneﬁts from larger |Bm|. We select|Bm|= 100
for every training on Split-CIFAR10 and likewise |Bm|= 500
on Split-CIFAR100. Moreover, even with α= 0, our method
consistently outperforms SCR-MO on smaller |Bm|values.
This demonstrate that using unlabeled negatived can signiﬁ-
cantly enhance performance when few labels are available.
Impact of α.We evaluate the impact of αon our
method’s performance. We keep every other parameter ﬁxed
when experimenting on αvalues. Intuitively, αcorresponds
to the importance we want to give to unlabeled streaming
data against labeled memory data. We observe on ﬁgure 2
that the optimal value for αdepends on the memory size and
tends to be close to one. Figure 3 conﬁrms this observation
by comparing performance for α= 1 to best performance for
anyαand implies that α= 1 is an acceptable default param-
eter for our method. Also, our method performance becomes

--- PAGE 4 ---
101102103
Memory Batch Size1020304050Average Accuracy (%)
SCR CIFAR10
SCR-MO CIFAR10
Ours CIFAR10SCR CIFAR100
SCR-MO CIFAR100
Ours CIFAR100Fig. 1 . Impact of increasing Memory Batch Size |Bm|. We
setα= 0 while augmenting |Bm|. The memory size is set to
200 for Split-CIFAR10 and 2k for Split-CIFAR100
comparable to SCR-MO for larger memory sizes. We inter-
pret previous observation as the consequence of αplaying the
role of a regularization parameter. When the memory size is
small, the model tends to overﬁt on memory data and per-
forms better when αis larger. Likewise when the memory
size is large, the model has enough information in memory
and performs better when αis smaller. Looking at the re-
sults on ﬁgure 2, best performances are obtained when we use
the information from both stream and memory.
10−11004546474849CIFAR10 M=200
10−11005455565758CIFAR10 M=500Ours SCR SCR-MOAvg. Acc. (%)
a
Fig. 2 . Average Accuracy for α∈[10−1.5,100.75] for our
method. Every other parameter is the same for SCR, SCR-
MO and our method. Left ﬁgure corresponds to Split-
CIFAR10 with a memory size M of 200 and right ﬁgure cor-
responds to M=500.
Results interpretation. Table 1 shows the comparison
of our approach to current supervised state-of-the-art meth-
ods and their memory-only counterparts on Split-CIFAR10
and Split-CIFAR100 with varying memory sizes M. Our
approach achieves best results compared to other semi-
supervised methods and performs comparably to supervised
state-of-the-art while leveraging only a small portion of la-
bels. This is especially remarkable for small memory sizes
where only 2.6% labels are given to the model. Figure 3
indicates that our method outperforms SCR-MO when less
than 20% labels are available. Also, our method is on par
0 10 20 30
Labels available (%)−25−20−15−10−50Relative performance (%)Ours - Best α
Ours -α= 1
SCR-MO
SCRFig. 3 . Relative performance compared to SCR on Split-
CIFAR100 for SCR-MO and our method while increasing the
percentage of available labels.
with SCR using only 10% labels while SCR-MO needs 20%
labels to obtain comparable results. Our experiments also
show that baselines using the information only available in
memory perform competitively, with results close to their
supervised counterparts for large memory sizes. This can be
explained by the observation that the larger the memory, the
closer the problem is to an oﬄine supervised problem.
Method M=200 M=500 M=2k M=5k
Supervisedoﬄine 80.0±1.2 43.2±2.3
ﬁne-tuned 16.4±2.0 3.6±0.7
ER 42.6±1.9 52.3±4.7 23.8±1.3 28.5±1.0
SCR 49.2±2.2 58.7±1.3 31.3±0.8 39.3±0.8
% labels 2.6% 5.6% 16.9% 33%
Semi ER-MO 41.0±3.5 49.9±3.3 23.7±0.9 27.9±1.0
Supervised SCR-MO 46.0±2.0 56.4±1.4 30.9±0.6 38.9±0.8
Ours 48.8±1.1 57.9±1.1 31.0±0.9 38.9±0.5
Split-CIFAR10 Split-CIFAR100
Table 1 . Average accuracy on split-CIFAR10 and split-
CIFAR100. We use α= 1.78 andα= 0.18 for split-CIFAR10
and split-CIFAR100 respectively. Best results for each sce-
nario are displayed in bold. Displayed values correspond to
the average and standard deviation over 10 experiments.
5. CONCLUSION
In this paper, we deﬁned a novel OSSGCL setting, and intro-
duced a new Semi-supervised Contrastive Loss (SemiCon).
We demonstrated experimentally that semi-supervised ap-
proaches trained using memory data only can perform com-
petitively to their supervised counterparts, while leveraging
as few as 2.6% labels on split-CIFAR10. We proposed a new
memory-based approach for the OSSGCL setting which suc-
cessfully combines labeled and unlabeled data using the novel
SemiCon loss. This criterion allows user-controlled balance
between labeled and unlabeled data during training. We
showed that our method can take advantage of unlabeled
data, surpassing other semi-supervised baselines on Split-
CIFAR datasets, and achieving similar performance to state-
of-the-art supervised methods.

--- PAGE 5 ---
6. REFERENCES
[1] Ting Chen, Simon Kornblith, Mohammad Norouzi, and
Geoﬀrey Hinton, “A simple framework for contrastive
learning of visual representations,” PMLR , pp. 1597–
1607, 2020.
[2] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, “Rep-
resentation learning with contrastive predictive coding,”
NIPS , 2018.
[3] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe,
Hyunwoo Kim, and Scott Sanner, “Online continual
learning in image classiﬁcation: An empirical survey,”
Neurocomputing , vol. 469, pp. 28–51, 2022.
[4] Michael McCloskey and Neal J Cohen, “Catastrophic
interference in connectionist networks: The sequential
learning problem,” in Psychology of learning and moti-
vation , vol. 24, pp. 109–165. Elsevier, 1989.
[5] Michael Gutmann and Aapo Hyv¨ arinen, “Noise-
contrastive estimation: A new estimation principle for
unnormalized statistical models,” in Proceedings of the
thirteenth international conference on artiﬁcial intelli-
gence and statistics . JMLR Workshop and Conference
Proceedings, 2010, pp. 297–304.
[6] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua
Lin, “Unsupervised feature learning via non-parametric
instance discrimination,” CVPR , pp. 3733–3742, 2018.
[7] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming
He, “Improved baselines with momentum contrastive
learning,” NIPS , 2018.
[8] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron
Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot,
Ce Liu, and Dilip Krishnan, “Supervised contrastive
learning,” Advances in Neural Information Processing
Systems , vol. 33, pp. 18661–18673, 2020.
[9] Robert M French, “Catastrophic forgetting in connec-
tionist networks,” Trends in cognitive sciences , vol. 3,
no. 4, pp. 128–135, 1999.
[10] Yen-Chang Hsu, Yen-Cheng Liu, Anita Ramasamy, and
Zsolt Kira, “Re-evaluating continual learning scenar-
ios: A categorization and case for strong baselines,” in
Continual Learning Workshop of 32nd Conference on
Neural Information Processing Systems (NeurIPS 2018) ,
Montr´ eal, Qu´ ebec, dec 2018.
[11] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness,
G. Desjardins, . A. Rusu, K. Milan, J. Quan, T. Ra-
malho, A. Grabska-Barwinska, D. Hassabis, C. Clopath,
D. Kumaran, and R. Hadsell, “Overcoming catastrophic
forgetting in neural networks,” vol. 114, no. 13, pp.
3521–3526, Publisher: National Academy of Sciences
Section: Biological Sciences.
[12] Sylvestre-Alvise Rebuﬃ, Alexander Kolesnikov, Georg
Sperl, and Christoph H Lampert, “iCaRL: Incremental
classiﬁer and representation learning,” pp. 2001–2010,
2017.
[13] David Lopez-Paz and Marc’Aurelio Ranzato, “Gradient
episodic memory for continual learning,” Advances in
neural information processing systems , vol. 30, 2017.[14] Jiangpeng He and Fengqing Zhu, “Unsupervised contin-
ual learning via pseudo labels,” arXiv:2104.07164 , 2021,
version: 2.
[15] Pranshu Ranjan Singh, Saisubramaniam Gopalakrish-
nan, Qiao ZhongZheng, Ponnuthurai N. Suganthan,
Savitha Ramasamy, and ArulMurugan Ambikapathi,
“Task-Agnostic Continual Learning Using Base-Child
Classiﬁers,” in 2021 IEEE International Conference on
Image Processing (ICIP) , Sept. 2021, pp. 794–798, ISSN:
2381-8549.
[16] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Da-
vide Abati, and Simone Calderara, “Dark experience for
general continual learning: a strong, simple baseline,”
Advances in neural information processing systems , vol.
33, pp. 15920–15930, 2020.
[17] Dushyant Rao, Francesco Visin, Andrei Rusu, Razvan
Pascanu, Yee Whye Teh, and Raia Hadsell, “Contin-
ual unsupervised representation learning,” Advances in
Neural Information Processing Systems , vol. 32, 2019.
[18] James Smith, Jonathan Balloch, Yen-Chang Hsu, and
Zsolt Kira, “Memory-eﬃcient semi-supervised continual
learning: The world is its own replay buﬀer,” IJCNN ,
pp. 1–8, 2021.
[19] Matteo Boschini, Pietro Buzzega, Lorenzo Bonicelli, An-
gelo Porrello, and Simone Calderara, “Continual semi-
supervised learning through contrastive interpolation
consistency,” arXiv preprint arXiv:2108.06552 , 2021.
[20] James Smith, Cameron Taylor, Seth Baer, and Constan-
tine Dovrolis, “Unsupervised progressive learning and
the stam architecture,” International Joint Conferences
on Artiﬁcial Intelligence Organization , 2021.
[21] Arslan Chaudhry, Marcus Rohrbach, Mohamed Elho-
seiny, Thalaiyasingam Ajanthan, Puneet K. Dokania,
Philip H. S. Torr, and Marc’Aurelio Ranzato, “On
Tiny Episodic Memories in Continual Learning,” arXiv
preprint arXiv:1902.10486 , June 2019.
[22] Zheda Mai, Ruiwen Li, Hyunwoo Kim, and Scott Sanner,
“Supervised contrastive replay: Revisiting the nearest
class mean classiﬁer in online class-incremental continual
learning,” CVPR , pp. 3589–3599, 2021.
[23] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timo-
thy Lillicrap, and Gregory Wayne, “Experience replay
for continual learning,” in Advances in Neural Infor-
mation Processing Systems . vol. 32, Curran Associates,
Inc.
[24] Jeﬀrey S Vitter, “Random sampling with a reservoir,”
ACM Transactions on Mathematical Software (TOMS) ,
vol. 11, no. 1, pp. 37–57, 1985.
[25] Alex Krizhevsky, “Learning multiple layers of features
from tiny images,” M.S. thesis, Department of Computer
Science, University of Toronto, 2009.
[26] Dhanajit Brahma, Vinay Kumar Verma, and Piyush
Rai, “Hypernetworks for continual semi-supervised
learning,” arXiv preprint arXiv:2110.01856 , 2021.
[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun, “Deep residual learning for image recognition,”
CVPR , pp. 770–778, 2016.
