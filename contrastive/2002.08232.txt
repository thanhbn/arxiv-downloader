# 2002.08232.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/contrastive/2002.08232.pdf
# File size: 1024013 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
CoLES: Contrastive Learning for Event Sequences with
Self-Supervision
Dmitrii Babaev
AIRI
Sber AI Lab
Moscow, RussiaNikita Ovsov
Ivan Kireev
Maria Ivanova
Sber AI Lab
Moscow, RussiaGleb Gusev
Sber AI Lab
MIPT
Moscow, Russia
Ivan Nazarov
AIRI
Moscow, RussiaAlexander Tuzhilin
New York University
New York, USA
ABSTRACT
We address the problem of self-supervised learning on discrete
event sequences generated by real-world users. Self-supervised
learning incorporates complex information from the raw data in
low-dimensional fixed-length vector representations that could be
easily applied in various downstream machine learning tasks. In this
paper, we propose a new method “CoLES”, which adapts contrastive
learning, previously used for audio and computer vision domains,
to the discrete event sequences domain in a self-supervised setting.
We deployed CoLES embeddings based on sequences of transac-
tions at the large European financial services company. Usage of
CoLES embeddings significantly improves the performance of the
pre-existing models on downstream tasks and produces significant
financial gains, measured in hundreds of millions of dollars yearly.
We also evaluated CoLES on several public event sequences datasets
and showed that CoLES representations consistently outperform
other methods on different downstream tasks.
CCS CONCEPTS
•Information systems →Data management systems ;•Applied
computing→Online banking ;•Computing methodologies →
Machine learning algorithms .
KEYWORDS
representation learning, metric learning, contrastive learning, self-
supervised learning, event sequences, data management
ACM Reference Format:
Dmitrii Babaev, Nikita Ovsov, Ivan Kireev, Maria Ivanova, Gleb Gusev,
Ivan Nazarov, and Alexander Tuzhilin. 2022. CoLES: Contrastive Learning
for Event Sequences with Self-Supervision. In Proceedings of the 2022 In-
ternational Conference on Management of Data (SIGMOD ’22), June 12–17,
E-mail: dmitri.babaev@gmail.com.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGMOD ’22, June 12–17, 2022, Philadelphia, PA, USA
©2022 Association for Computing Machinery.
ACM ISBN 978-1-4503-9249-5/22/06. . . $15.00
https://doi.org/10.1145/3514221.35261292022, Philadelphia, PA, USA. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3514221.3526129
1 INTRODUCTION
As part of representation learning methods, data embedding aims
to represent the relevant intrinsic patterns of the point or sequen-
tial data into low-dimensional fixed-length vectors capturing its
“essence”, that are useful in related downstream tasks,[ 9,10,25,28,
38]. As such, pre-trained embeddings in different domains are used
either as informative out-of-the-box input features for Machine
Learning or Deep Learning models without extensive engineering
or deep domain knowledge on the part of practitioners, or as build-
ing blocks in representations of composite multi-modal data. In
big-data applications embeddings may be viewed as a learnable task-
aware data compression technique, which enables storage-efficient
data sharing arrangements, possibly with privacy guarantees de-
pending on used method.
Most research and application of embedding methods, however,
have been focused on the core machine learning domains, including
ELMO [ 28] and BERT [ 9] in the natural language processing (NLP),
CPC [ 38] in speech recognition, and various methods in computer
vision (CV) [10, 38].
The common feature of these domains is that that the data in
such modalities is context sensitive : a term can be accurately recon-
structed from the context-conditional language model, similarly
to the way a pixel can be inferred from its neighborhood. This
property underlies popular approaches for representation learning
in NLP, such as BERT’s Cloze task [ 9], and in audio and CV, such
as CPC [38].
However, not every sequential discrete data features high mutual
information between a single item and its immediate neighborhood.
For example, log entries, IoT telemetry, industrial maintenance, user
behavior [ 26], travel patterns, transactional data, and other indus-
trial and financial event sequences typically consist of interleaved
relatively independent sub-streams. For example, the transactions
generated either by individual or business customers feature irregu-
lar and periodic patterns, seen from the perspective of the financial
services company as a stream of unlabelled and apparently un-
related events. The most state-of-the-art representation learning
methods for token and sequence embedding from the NLP or CV are
not guaranteed to capture the peculiarities of such financial data,
which exhibits customer behavior of a certain type and constitutesarXiv:2002.08232v3  [cs.LG]  22 Jul 2022

--- PAGE 2 ---
valuable information for the fraud prevention and development of
efficient financial products.
In this paper, we propose a novel self-supervised method for
embedding discrete event sequences, called COntrastive Learning
for Event Sequences (CoLES) , which is based on contrastive learning,
[14,44], with a special data augmentation strategy. Contrastive
learning aims to learn a representation 𝑥↦→𝑀(𝑥), which brings
positive pairs , i.e. semantically similar objects, closer to each other
in the embedding space, while negative pairs , i.e. dissimilar objects,
further apart. Positive-negative pairs are obtained either explicitly
from the known ground-truth target data or implicitly using self-
supervised data augmentation strategies [ 12]. In the latter, the most
common approach is conditional generation: for a given pair of
distinct datapoints 𝑥≠𝑦the positive pairs(𝑧,𝑧′)are sampled from
the product 𝑝+(𝑧,𝑧′)=𝑝(𝑧|𝑥)𝑝(𝑧′|𝑥), while the negative pairs
– from𝑝−(𝑧,𝑧′)=𝑝(𝑧|𝑥)𝑝(𝑧′|𝑦)for𝑥≠𝑦, where𝑝(·|𝑥)is a
process sampling random augmentations of 𝑥.
As a self-supervised sequence embedding method, CoLES uses a
novel augmentation algorithm, which generates sub-sequences of
observed event sequences and uses them as different high-dimensional
views of the object (sequence) for contrastive learning. The pro-
posed generative process is specifically designed to address the
observed interleaved periodicity in financial transaction event se-
quences, which is the primary application of our method (see Sec-
tion 4.3). Representations learnt by CoLES can be used as feature
vectors in supervised domain-related tasks [ 25,35,48], e.g. fraud
detection or scoring tasks based on transaction history, or they can
be fine-tuned for out-of-domain tasks [47].
We have applied CoLES to four publicly available datasets of
event sequences from different domains, such as financial transac-
tions, retail receipts and game assessment records. As we show in
the experimental section, CoLES produces representations, which
achieve strong performance results, comparable to the hand-crafted
features produced by domain experts. We also demonstrate that
the fine-tuned CoLES representations consistently outperform rep-
resentations produces by alternative methods. Additionally, we
deployed CoLES embeddings in several applications in our orga-
nization and tested the method against the models currently used
in the company. Experimental results, demonstrate CoLES embed-
dings significantly improve the performance of the pre-existing
models on the downstream tasks, which resulted in significant
financial gains for the company.
This paper makes the following contributions. We
(1)present CoLES, a self-supervised method with a novel aug-
mentation method, which adapts contrastive learning to the
discrete event sequence domain;
(2)demonstrate that CoLES consistently outperforms existing
supervised, self-supervised and semi-supervised learning
baselines adapted to the event sequence domain;
(3)present the results of applying CoLES embeddings in the real-
world scenaria and show that the proposed method can be
of significant value for the day-to-day modelling in financial
services industry.
The rest of the paper is organized as follows. In the next sec-
tion, we discuss related studies on self-supervised and contrastive
learning. In Section 3, we introduce our new method CoLES fordiscrete event sequences. In Section 4, we demonstrate that CoLES
outperforms several strong baselines including previously proposed
contrastive learning methods adapted to event sequence datasets.
Section 5 is dedicated to the discussion of our results and conclu-
sions. We provide the source code for all the experiments on public
datasets described in this paper.1
2 RELATED WORK
Contrastive learning has been successfully applied to constructing
low-dimensional representations (embeddings) of various objects,
such as images [ 7,32], texts [ 30], and audio recordings [ 41]. Al-
though the aim of these studies is to identify the object based on its
sample [ 18,32,41], these supervised approaches are not applicable
to our setting, since their training datasets explicitly contain mul-
tiple independent samples per each particular object, which form
positive pairs as a critical component for learning.
For situations when positive pairs are not available or their
amount is limited, synthetic data generation and augmentation
techniques could be employed. One of the first such frameworks
was proposed by Dosovitskiy et al . [10] , who introduce surrogate
classes by randomly augmenting the same image. Several recent
works, e.g. [ 2,4,15], extended this idea by applying contrastive
learning methods (see [ 12]). Contrastive Predictive Coding (CPC)
is a self-supervised learning approach proposed for non-discrete se-
quential data in [ 38]. The CPC employs an autoregressive predictive
model of the input sequence in order to extract meaningful latent
representations, and as such can be adapted the domain of discrete
event sequences (see Section 4.2 for comparison with CoLES).
Several publications consider self-supervision for user behavior
sequences in the recommender system and user behavior analy-
sis domains. CPC-like approach for self-supervised learning on
user click histories is proposed in [ 51], while [ 23] use an auxiliary
self-supervised sequence-to-sequence loss term. In [ 52], it was pro-
posed to use “Cloze” task from BERT [ 9] for self-supervision on
purchase sequences. A SimCLR-like approach for text-based tasks
and tabular data was adapted in [ 45]. In [ 53] authors propose an
unsupervised autoregressive method to produce embeddings of
attributed sequences where a sequence of categorical tokens has
additional global attributes. The aforementioned works consider
sequences of “items”, where each element is an item identifier. We
consider more complex sequences of events where an element of
the sequence consists of several categorical and numerical fields.
There are papers dedicated to supervised learning for discrete
event sequences, e.g. [ 1,3,34,36,42], but self-supervised pre-
training is not used in those works.
3 PROBLEM FORMULATION AND
OVERVIEW OF THE COLES METHOD
3.1 Problem formulation
While the method proposed in this study could be studied in dif-
ferent domains, we focus on discrete sequences of events. Assume
there are some entities 𝑒and that each entity’s lifetime activity is
observed as a sequence of events 𝑥𝑒:={𝑥𝑒(𝑡)}𝑇𝑒
𝑡=1. Entities could
be people or organizations or some other abstractions. Events 𝑥𝑒(𝑡)
1https://github.com/dllllb/coles-paper

--- PAGE 3 ---
Figure 1: General framework. Phase 1: Self-supervised training. Phase 2.a Self-supervised embeddings as features for supevised
model. Phase 2.b: Pre-trained encoder fine-tuning.
may have any nature and structure (e.g., transactions of a client,
click logs of a user), and their components may contain numerical,
categorical, and textual fields (see datasets description in Section 4).
According to theoretical framework of contrastive learning pro-
posed in [ 31], each entity 𝑒is a latent class, which is associated
with a distribution 𝑃𝑒over its possible samples (event sequences).
However, unlike the problem setting of [ 31], we have no positive
pairs, i.e. pairs of event sequences representing the same entity
𝑒. Instead, only one sequence 𝑥𝑒is available for the entity 𝑒. For-
mally, each entity 𝑒is associated with a latent stochastic process
{𝑋𝑒(𝑡)}={𝑋𝑒(𝑡)}𝑡≥1, and we observe only a single finite realisa-
tion{𝑥𝑒}={𝑥𝑒(𝑡)}𝑇𝑒
𝑡=1of it. Our goal is to learn an encoder𝑀that
maps event sequences into a feature space R𝑑in such a way that
the obtained embedding{𝑥𝑒}↦→𝑐𝑒=𝑀({𝑥𝑒})∈R𝑑encodes the
essential properties of 𝑒and disregards irrelevant noise contained
in the sequence. That is, the embeddings 𝑀({𝑥′})and𝑀({𝑥′′})
should be close to each other, if 𝑥′and𝑥′′were paths generated
by the same process {𝑋𝑒(𝑡)}, and further apart, if generated by
distinct processes.
The quality of representations can be examined by downstream
tasks in the two ways:
(1)𝑐𝑒can be used as a feature vector for a task–specific model
(see Figure 1, Phase 2a),
(2)encoder𝑀can also be (jointly) fine-tuned [ 47] (see Figure 1,
Phase 2b).
3.2 Sampling of surrogate sequences as an
augmentation procedure
When there is no sampling access to the latent processes {𝑋𝑒(𝑡)},
one could employ synthetic augmentation strategies, which are
akin to bootstrapping. Most augmentation techniques proposed
for continuous domains, such as image displacement, color jitter
or random gray scale in CV, [ 12], are not applicable to discrete
events. Thus, generating sub-sequences from the same event se-
quence{𝑥𝑒(𝑡)}, could be used as a possible augmentation. The
idea proposed below resembles the bootstrap method, [ 11], which,
roughly, posits that the empirical distribution induced by the ob-
served sample of independent datapoints is a suitable proxy for the
distribution of the population . In our setting, however, the eventsare not independent observations, which prompts us to rely on
different data assumptions.
The key property of event sequences that represent lifetime ac-
tivity is periodicity and repeatability of its events (see Section 4.0.2
for the empirical observations of these properties for the consid-
ered datasets). This motivates the Random slices sampling method
applied in CoLES, as presented in Algorithm 1. Sub-sequences {˜𝑥𝑒}
are sampled from a given sequence {𝑥𝑒(𝑡)}as continuous segments,
“slices”, using the following three steps. First, the length of the slice
is chosen uniformly from the admissible values. Second, too short
(and, optionally, too long) sub-sequences are discarded. Third, the
starting position is uniformly chosen from all possible values. The
overview of the CoLES method is presented in Figure 1.
Algorithm 1: Random slices sub-sequence generation
strategy
hyperparameters: 𝑚,𝑀 : minimal and maximal possible
length of a sub-sequence; 𝑘: number of samples.
input: A sequence 𝑆={𝑧𝑗}𝑇−1
𝑗=0of length𝑇.
output:S: sub-sequences of 𝑆.
for𝑖←1to𝑘do
Generate a random integer 𝑇𝑖uniformly from[1,𝑇];
if𝑇𝑖∈[𝑚,𝑀]then
Generate a random integer 𝑠from[0,𝑇−𝑇𝑖);
Add the slice ˜𝑆𝑖:=
𝑧𝑠+𝑗	𝑇𝑖−1
𝑗=0toS;
end
3.3 Model training
Batch generation. The following procedure creates a batch for
CoLES.𝑁initial sequences are randomly taken and 𝐾sub-sequences
are produced for each of one. Pairs of sub-sequences of from same
sequence are used as positive samples and pairs from different
sequences – as negatives.
In the experiment section we consider several baseline empir-
ical strategies for the sub-sequence generation to compare with
Algorithm 1. The details of the comparison are presented in Sec-
tion 4.2.1.

--- PAGE 4 ---
Contrastive loss We consider a classical variant of the con-
trastive loss, proposed in [14], which minimizes the objective
L𝑢𝑣(𝑀)=𝑌𝑢𝑣1
2𝑑𝑀(𝑢,𝑣)2+(1−𝑌𝑢𝑣)1
2max{0,𝜌−𝑑𝑀(𝑢,𝑣)}2,
with respect to 𝑀:X→R𝑛, where𝑑𝑀(𝑢,𝑣)=𝑑(𝑐𝑢,𝑐𝑣)is the dis-
tance between embeddings of the pair (𝑢,𝑣),𝑐∗=𝑀({˜𝑥∗(𝜏)}),𝑌𝑢𝑣
is a binary variable identifying whether the pair (𝑢,𝑣)is positive,
and𝜌is the soft minimal margin between dissimilar objects. The
second term encourages separation of the embeddings in negative
pairs and prevents mode collapse in𝑀, when the entities are mapped
to the same point in the embedding space. 𝑑(𝑎,𝑏)is the Euclidean
distance,𝑑(𝑎,𝑏)=√︁Í
𝑘(𝑎𝑘−𝑏𝑘)2, as proposed in [ 14]. The se-
quences{˜𝑥𝑢(𝜏)}and{˜𝑥𝑣(𝜏)}of a pair with 𝑌𝑢𝑣=1are obtained
through random slice generation (Algorithm 1) from the same ob-
servation{𝑥𝑒(𝑡)}, while in pairs with 𝑌𝑢𝑣=0the sequences are
sampled from{𝑥𝑒(𝑡)}and{𝑥𝑔(𝑡)}, respectively, for 𝑒≠𝑔.
In the experiment section we compare the basic variant of the
contrastive loss with alternative variants. The results of the com-
parison is presented in Section 4.2.1.
Negative sampling. One challenge in contrastive learning ap-
proach is that positive pairs are overwhelmed by potential negative
pairs. Furthermore, some of negative pairs are distant enough, to
not provide any valuable feedback through Lto𝑀during training,
[32,33]. We compare the common negative sampling methods in
Section 4.2. Due to certain negative sampling approaches begin
distance-aware and in order to make the overall distance computa-
tion less inefficient we restrict the encoder 𝑀to the class of maps
that output unit-norm vectors in R𝑑. Therefore the pairwise dis-
tance𝑑𝑀(𝑢,𝑣)2is just 2−2𝑐⊤𝑢𝑐𝑣, which only requires pairwise dot
products between embeddings 𝑐𝑣=𝑀({˜𝑥𝑢(𝑡)}).
3.4 Encoder architecture
Embedding a sequence of events into a vector of fixed size requires
encoding individual events followed by aggregating the entire se-
quence. The composite encoder model 𝑀in CoLES is of the form
𝑀({𝑥𝑡}):=𝜙seq({𝜙evt(𝑥𝑡)}), where𝜙evtand𝜙seqare event- and
sequence-level embedding networks, respectively, trained in an
end-to-end manner to minimize the contrastive loss L(𝑀).
The event encoder 𝜙evttakes a set of attributes of each event
𝑥𝑡and outputs its intermediate representation in R𝑑:𝑧𝑡=𝜙evt(𝑥𝑡).
This encoder consists of several linear layers, for embedding one-
hot encoded categorical attributes, and batch normalization layers,
applied to numerical attributes of events. Outputs of these layers
are concatenated to produce the event embedding 𝑧𝑡.
The sequence encoder 𝜙seqtakes the intermediate representa-
tions of the events 𝑧1:𝑇=𝑧1,𝑧2,···𝑧𝑇and outputs the representa-
tion𝑐𝑡of their sequence up to the time 𝑡:𝑐𝑡=𝜙seq(𝑧1:𝑡). The last
𝑐𝑇is used as the embedding of the whole event sequence. In our
experiments, we use GRU, [ 5], a recurrent network which demon-
strates robust performance on the sequential data, [ 1]. In this case,
𝜙seqis computed by the recurrence 𝑐𝑡+1=GRU(𝑧𝑡+1,𝑐𝑡)starting
from a learnt 𝑐0. We note, that other architectures are possible,
including LSTM and transformers, [16, 39] (see Section 4.2.1).
To summarise, CoLES consists of three major ingredients: the
event sequence encoder, the positive and negative pair generation
strategy, and the loss function for contrastive learning.4 EXPERIMENTS
4.0.1 Datasets. We compare our method with existing baselines
on several publicly available datasets of event sequences from vari-
ous data science competitions. We chose datasets with sufficient
amounts of discrete events per user.
Age group prediction competition2The dataset of 44M an-
onymized credit card transactions representing 50K individuals was
used to predict the age group of a person. The multiclass target
label is known only for 30K records, and within this subset the
labels are balanced. Each transaction includes the date, type, and
amount being charged.
Churn prediction competition3The dataset of 1M anonymiz-
ed card transactions representing 10K clients was used to predict
churn probability. Each transaction is characterized by date, type,
amount and Merchant Category Code. The binary target label is
known only for 5K clients, and labels are almost balanced.
Assessment prediction competition4The task is to predict
the in-game assessment results based on the history of children’s
gameplay data. Target is one of four grades, with shares 0.50,0.24,
0.14,0.12. The dataset consists of 12M gameplay events combined in
330K gameplays representing 18K children. Only 17.7K gameplays
are labeled. Each gameplay event is characterized by a timestamp,
an event code, an incremental counter of events within a game
session, time since the start of the game session, etc.
Retail purchase history age group prediction5The task is
to predict the age group of a client based on their retail purchase
history. The age group is known for all clients. The group ratio is
balanced in the dataset. The dataset consists of 45.8M retail pur-
chases representing 400K clients. Each purchase is characterized
by time, product level, segment, amount, value, loyalty program
points received.
Scoring competition6The dataset of 443M anonymized credit
card transactions representing 1.47M persons was used to predict
the probability of credit product default. The label is known for
0.96M persons. The default rate is 2.76% in the dataset. Each trans-
action includes the set of date features, set of type features, and
amount being charged.
4.0.2 Repeatability and periodicity of the datasets. To check that
considered datasets follow our repeatability and periodicity assump-
tion made in Section 3.2 we performed the following experiments.
We measure the KL-divergence between two kinds of samples: (1)
between random slices of the same sequence, generated using a
modified version of Algorithm 1 where overlapping events are
dropped, and (2) between random sub-samples taken from different
sequences. The results are displayed in Figure 2, which shows that
the KL-divergence between sub-sequences of the same sequence of
events is relatively small compared to the typical KL-divergence
between sub-samples of different sequences of events. This obser-
vation supports our repeatability and periodicity assumption. Also
note that additional plot (2d) is provided as an example for data
without any repeatable structure.
2https://ods.ai/competitions/sberbank-sirius-lesson
3https://boosters.pro/championship/rosbank1/
4https://www.kaggle.com/c/data-science-bowl-2019
5https://ods.ai/competitions/x5-retailhero-uplift-modeling
6https://boosters.pro/championship/alfabattle2/overview

--- PAGE 5 ---
(a) Age group
0 5 10 15 20 25
KL0500100015002000250030003500Counttype
Same client sample
Random client sample (b) Assessment
0 5 10 15 20 25
KL05001000150020002500300035004000Counttype
Random client sample
Same client sample (c) Retail
0 5 10 15 20 25
KL020040060080010001200Counttype
Random client sample
Same client sample (d) Texts
0 5 10 15 20 25
KL050010001500200025003000Counttype
Random post sample
Same post sample
Figure 2: Periodicity and repeatability of the data. KL-divergence between event types of two random sub-sequences from the
same sequence is compared with KL-divergence between sub-sequences of different sequences. Additional plot (2d) is provided
as an example for data without any repeatable structure.
Table 1: Hyper-parameters for CoLES training
Dataset Age Churn Assess Retail
Embedding size 800 1024 100 800
Learning rate 0.001 0.004 0.002 0.002
N samples in batch 64 128 256 256
N epochs 150 60 100 30
Min sequence length 25 15 100 30
Max sequence length 200 150 500 180
Encoder GRU LSTM GRU GRU
For all methods, a random search on 5-fold cross-validation over the train
set is used for hyper-parameter selection. The number of sub-sequences
generated for each sequence was always 5 for each dataset.
4.0.3 Dataset split. For each dataset, we set apart 10% persons
from the labeled part of the data as the test set , used for evaluation.
The remaining 90% of labeled data and unlabeled data constitute
ourtraining set used for learning. The hyper-parameters of each
method were selected by random search over 5-fold cross-validation
on the training set.
For the learning of semi-supervised/self-supervised techniques
(including CoLES), we used all transactions of training sets in-
cluding unlabeled data. The unlabelled parts of the datasets were
ignored while training supervised models.
4.0.4 Training performance. Neural networks were trained on a
single Tesla P-100 GPU card. At the training stage of CoLES, each
single training batch is processed in 142 milliseconds. For example,
in the age group prediction dataset a single batch contains 64 unique
individuals with five sub-sequences per each, i.e. 320 training sub-
sequences in total, each averaging 90 transactions, which means
that each batch contains about 28800 transactions.
4.0.5 Hyperparameters. Unless specified otherwise, we use con-
trastive loss and random slices pair generation strategy for CoLES
in our experiments (see Section 4.2 for motivation). The final set of
hyper-parameters used for CoLES is shown in the Table 1.4.1 Baselines
4.1.1 LightGBM. The Gradient Boosting Machine (GBM) [ 13] is
generally considered a strong baseline in cases of tabular data
with heterogeneous features,[ 27,40,43,49]. In our experiments
we use LightGBM [ 20] as a model for the downstream task (see
Figure 1, Phase 2a) and consider alternative input features: (1) the
vector of of hand-crafted aggregate features produced from the
raw transactional data, or (2) the embedding of the sequence of
transactions, produced by the encoder network (see Section 3.4).
For the latter the encoder model is trained in the self-supervised
fashion either with CoLES or one of its alternatives, described in
the Section 4.1.3.
4.1.2 Hand-crafted features. All attributes of each transaction are
either numerical, e.g. amount, or categorical, e.g. merchant category
(MCC code), transaction type, etc. For the numerical attributes we
apply aggregation functions, such as ’sum’, ’mean’, ’std’, ’min’,
’max’, over all transactions per sequence. For example, if we apply
’sum’ over the numerical field ’amount’ we get a feature ’sum of
all transaction amounts per sequence’. Categorical attributes are
aggregated in a slightly different way. We group the transactions
separately by every unique value of each categorical attribute and
apply an aggregation function, such as ’count’, ’mean’, ’std’ over
all numerical attributes. For example, if we apply ’mean’ for the
numerical attribute ’amount’ grouped by categorical attribute ’MCC
code’ we obtain one feature ’mean amount of all transactions for
the specific MCC code’ per sequence.
4.1.3 Self-supervised baselines. We compared CoLES method against
major existing approaches to self-supervised embedding, which are
applicable to the event sequence data.
NSP. We consider a simple baseline inspired by the next sen-
tence prediction task in BERT [ 9]. Specifically, we generate two
sub-sequences A and B in a way that 50% of the time B follows
A in the same sequence (positive pair), and 50% of the time it is a
random fragment from another sequence (negative pair).
SOP. Another baseline is the same as sequence order prediction
task from ALBERT [ 22], which uses two consecutive sub-sequences

--- PAGE 6 ---
Table 2: Comparison of batch generation strategies
Sample
methodAge
AccuracyChurn
AUROCAssess
AccuracyRetail
Accuracy
Random samples 0.613 0.820 0.563 0.523
Random disjoint
samples0.619 0.819 0.563 0.505
Random slices 0.639 0.823 0.618 0.542
5-fold cross-validation metric is shown
as a positive pair, and two consecutive sub-sequences with swapped
order as a negative pair.
RTD. The replaced token detection approach from ELECTRA [ 8]
could be also adapted to event sequences. To get this baseline, we
replaced 15% of events from the sequence with random events,
taken from other sequences and train a model to predict whether
an event is replaced or not.
CPC. Additionally, we compare against Contrastive Predictive
Coding (CPC) [ 38] – a self-supervised learning method that demon-
strates state-of-the-art performance on sequential data audio, com-
puter vision, reinforcement learning and recommender systems
domains, [51].
4.2 Results
4.2.1 Discussion of design choices. To evaluate the proposed method
of sub-sequence generation (see Section 3.2) we compared it with
two alternatives: (1) The random sampling without replacement
strategy, similar to [ 45], and (2) random disjoint samples strategy,
which resembles the generation proposed in [ 23]. The first approach
generates a non-contiguous sub-sequence of events, by repeatedly
drawing a random event from the sequence without replacement
preserving the in-sequence order of the sampled events. The second
approach produces sub-sequences by the randomly splitting the
initial sequence into several non-overlapping contiguous segments.
The motivation is that overlaps between sub-sequences may possi-
bly lead to overfitting, since the exact sub-sequences of events are
the same and may be “memoized” by the encoder without learning
underlying similarities.
The results of the comparison are presented in Table 2. The
proposed strategy of generating random sub-sequence slices con-
sistently outperforms alternatives.
We evaluated several contrastive learning loss functions that
showed promising performance on different datasets [ 19] and some
classical variants, namely: contrastive loss [ 14], binomial deviance
loss [ 46], triplet loss [ 17], histogram loss [ 37], and margin loss [ 24].
The results of comparison are shown in Table 4. Noteworthy, al-
though the contrastive loss can be considered as the basic variant of
contrastive learning, yet it still manages to achieve strong results on
the downstream tasks. We speculate that an increase in the model’s
performance on contrastive learning task does not always lead to
increased performance on downstream tasks.
We also compared popular negative sampling strategies (distance-
weighted sampling [ 24], and hard-negative mining [ 32]) with ran-
dom negative sampling strategy (see Table 5). We can observe thatTable 3: Comparison of encoder types
DatasetAge group
AccuracyChurn
AUROCAssess
AccuracyRetail
Accuracy
LSTM 0.621 0.823 0.620 0.535
GRU 0.638 0.812 0.618 0.542
Transformer 0.622 0.780 0.542 0.499
5-fold cross-validation metric is shown
Table 4: Comparison of contrastive learning losses
DatasetAge group
AccuracyChurn
AUROCAssess
AccuracyRetail
Accuracy
Contrastive
margin=0.50.639 0.823 0.618 0.542
Binomial
deviance0.621 0.769 0.589 0.535
Histogram 0.632 0.815 0.615 0.533
Margin 0.638 0.823 0.612 0.541
Triplet 0.636 0.781 0.600 0.541
5-fold cross-validation metric is shown
Table 5: Comparison of negative sampling strategies
DatasetAge group
AccuracyChurn
AUROCAssessment
AccuracyRetail
Accuracy
Hard
negative
mining0.639 0.823 0.618 0.542
Random
negative
sampling0.626 0.815 0 .593 0 .530
Distance-
weighted
sampling0.629 0.821 0 .603 0 .536
5-fold cross-validation metric is shown
hard negative mining leads to a measurable increase in quality on
downstream tasks in comparison to random negative sampling.
Yet another possible design choice of the method is the encoder
architecture. We compared several popular options for the sequence
encoder: GRU [ 5], LSTM [ 16] and Transformer [ 39]. Table 3 shows
that the choice of the encoder architecture has little effect on the
performance of the proposed method.
Figure 3 shows that the performance on the downstream task
exhibits diminishing gains as the dimensionality of the embedding
increases. These results can be interpreted through the lens of the
bias-variance trade-off: when the dimensionality is too low, too
much information is discarded (high bias), however, when it is is
too high, then much more irrelevant noise seeps into the embed-
ding (high variance). Note, that the training time and the memory
consumption scale linearly with the embedding size.
4.2.2 Self-supervised embeddings. We compared CoLES with base-
lines described in Section 4.1 in two scenarios. First, we compared

--- PAGE 7 ---
(a) Age group
32 64 96 160 224 480 800 1200 2400
Embedding size0.590.600.610.620.630.64Accuracy (b) Churn
32 64 128 256 512 1024 3072
Embedding size0.780.790.800.810.820.83AUROC (c) Assessment
32 64 100 200 400
Embedding size0.5900.5950.6000.6050.6100.6150.6200.625Accuracy (d) Retail
64 160 480 800
Embedding size0.5200.5250.5300.5350.540Accuracy
Figure 3: Embedding dimensionality vs. quality
embeddings produced by CoLES with other types of embeddings,
including the manually created ones, by using them as input into
a downstream LightGBM model (see Figure 1, Phase 2a), trained
independently from the sequence encoder. As Table 6 demonstrates,
the sequence embeddings, produced by CoLES, perform on par
and sometimes even better than manually engineered features. Fur-
thermore, CoLES consistenlty outperforms other self-supervised
baselines on the four out of the five considered datasets .
On the scoring dataset, which is larger than the other datasets,
CoLES, CPC, and RTD, outperform the hand-crafted baseline. The
autoregressive nature of the CPC aligns well with the credit scoring
task, for which the more recent the information the more relevant
it is to the target. The RTD embeddings are also effective for credit
scoring, since the method relies on anomaly detection.
Simple hand-crafted features can achieve competitive results
if the events have a clear structure for designing them, e.g. it is
straightforward to compute the group-wise aggregate statistics of
historical data based on some attribute. In the commercial settings
(Section 4.3) the situation may be different: it is non-trivial to man-
ufacture meaningful features for transactions of legal entities, since
it is not clear what the natural groupings are. We discuss the differ-
ence between simple events (card transactions) and more complex
events (transactions of the legal entities) at the end of Section 4.3.
4.2.3 Fine-tuned embeddings. In the second scenario, we fine-tune
pre-trained models for specific downstream tasks (see Figure 1,
Phase 2b). The models are pre-trained using CoLES or another self-
supervised learning approaches and then are trained on the labeled
data for the specific end task.
The fine-tuning step is done by adding a classification network
ℎ(single-layer neural network with softmax activation) to the pre-
trained encoder network 𝑀(see Section 3.4). Both networks are
trained jointly on the downstream task, i.e. the classifier takes the
output of the encoder and produces a prediction ( ˆ𝑦=ℎ(𝑀({𝑥})))
and its error propagates back through both. In addition to the
aforementioned baselines, we compare our method to a supervised
learning approach, where the encoder network 𝑀is not pre-trained
using self-supervised target.
As Table 7 shows, CoLES representations obtained after fine-
tuning achieve superior performance on all the considered datasets,
outperforming other methods by significant margins.4.2.4 Semi-supervised setup. Here we study the applicability of
our method in scenarios where the amount of labeled examples is
limited. We performed a series of experiments where only a random
fraction of available labels is used to train the downstream task
models. As in the case of the supervised setup, we compare the
proposed method with hand-crafted features, CPC, and supervised
learning without pre-training. The results are presented in Figure 4.
Note that the performance improvement of CoLES in comparison to
the supervised-only methods increases as we decrease the portion
of labeled examples in the training dataset. Also note that CoLES
consistently outperforms CPC for different volumes of labeled data.
4.3 CoLES Embeddings in Commercial Settings
We applied the proposed self-supervised CoLES method to several
machine learning tasks, routinely considered in a large European
financial services company. In particular, two types embeddings
were created: (1) legal entity embeddings of the small- and medium-
size companies, based on commercial transactions and operational
histories of their businesses, and (2) individual embeddings of indi-
vidual/retail customers, based on their debit/credit card transaction
histories. Tables 8 and 9 provide examples of the transactional data
used for building individuals’ and legal entities’ embeddings. Over-
all, the dataset of ten million corporate clients with on average
200 transactions per client was used to train the embeddings of
type (1), and the dataset of five million individual clients with the
mean number of 400 transactions per client was used to train the
model for embeddings of type (2). These two “in-house” commercial
datasets are considerable larger than the public datasets outlined
in Section 4, and the extra volume of self-supervised training data
allowed us to generate embeddings of significantly higher quality
than on the publicly available data.
We performed extensive evaluation of CoLES embeddings on
these in-house datasets by applying them to different downstream
tasks. Legal entity embeddings were applied in the following use
cases:
•Corporate medical insurance lead generation. In this
task, the model should be able to predict client’s interest in
a corporate medical insurance product.

--- PAGE 8 ---
Table 6: Quality of unsupervised embeddings as features for the downstream task
MethodAge
AccuracyChurn
AUROCAssess
AccuracyRetail
AccuracyScoring
AUROC
Designed features 0.631±0.003 0.825±0.004 0.602±0.005 0.547±0.001 0.779±0.001
SOP 0.493±0.002 0.782±0.005 0.577±0.002 0.428±0.001 0.724±0.001
NSP 0.622±0.004 0.830±0.004 0.581±0.003 0.425±0.002 0.766±0.001
RTD 0.632±0.002 0.801±0.004 0.580±0.003 0.520±0.001 0.791±0.001
CPC 0.594±0.002 0.802±0.003 0.588±0.002 0.525±0.001 0.791±0.001
CoLES 0.638±0.007 0.843±0.003 0.601±0.002 0.539±0.001 0.792±0.001
average test set quality metric and its standard deviation for 5 runs on different folds is shown
(a) Age group
337 675 1350 2700 5400 10800 21600
Number of Labeled Datapoints0.5000.5250.5500.5750.6000.6250.650Accuracy
Setup
lightGBM on hand-crafted features
CPC Fine-tuning
CoLES Fine-tuning
Supervised learning (b) Churn
225 450 900 1800 3600
Number of Labeled Datapoints0.740.760.780.800.82AUROC
Setup
lightGBM on hand-crafted features
CPC Fine-tuning
CoLES Fine-tuning
Supervised learning (c) Assessment
496 994 1986 3971 7943 15887
Number of Labeled Datapoints0.520.540.560.580.60Accuracy
Setup
lightGBM on hand-crafted features
CPC Fine-tuning
CoLES Fine-tuning
Supervised learning (d) Retail
500 1000 3000 6000 12000 25000 50000 100000 200000290000
Number of Labeled Datapoints0.400.420.440.460.480.500.520.54Accuracy
Setup
lightGBM on hand-crafted features
CPC Fine-tuning
CoLES Fine-tuning
Supervised learning
Figure 4: Model quality for different dataset sizes
The rightmost point corresponds to all labels and supervised setup.
Table 7: Quality of the pre-trained model on the downstream
tasks
MethodAge
AccuracyChurn
AUROCAssess
AccuracyRetail
Accuracy
Designed
features0.631±0.003 0.825±0.004 0.602±0.005 0.547±0.001
Supervised
learning0.628±0.004 0.817±0.009 0.602±0.005 0.542±0.001
RTD pre-
train0.635±0.006 0.819±0.005 0.586±0.003 0.544±0.002
CPC pre-
train0.615±0.009 0.810±0.006 0.606±0.004 0.549±0.001
CoLES
pretrain0.644±0.004 0.827±0.004 0.615±0.003 0.552±0.001
average test set quality metric and its standard deviation for 5 runs on
different folds is shown
Table 8: Data structure of the credit card transactions
Date Amount Currency CountryMerchant
Type
Jun 21 230 EUR France Restaurant
Jun 21 5 USD US Transportation
Jun 22 40 USD USHousehold
ApplianceTable 9: Data structure of the money transfers between legal
entities
Date Amount Currency Sender Receiver Type
Jul 11 20000 EUR 1232323 6345433 23
Jul 11 5000 USD 5424443 1232323 12
Jul 12 14000 USD 1232323 5424443 14
The information about company business and the company region is
encoded in the first letters of the company identifier stored in “Sender” and
“Receiver” fields.
•Credit lead generation for small and medium size busi-
nesses. In this task, the model should predict an interest of a
company in taking a credit.
•Credit scoring for small and medium size businesses. In
this task, the model should predict the the probability of a
company’s default.
•Holding structure restoration. In this task, the model
should predict if a pair of companies are in the same holding
of companies.
•Fraudulent money transfers monitoring. In this task,
the model is used to estimate the likelihood that a particular
transaction is fraudulent.
As opposed to legal entities, individual embeddings are employed
in following downstream tasks:

--- PAGE 9 ---
•Retail credit scoring. In this task, the model should esti-
mate the probability of default when a client is taking a retail
credit.
•Customer churn prediction. In this task, the model should
predict the possibility that a client would stop using the
company products (cards and deposit accounts).
•Life insurance lead generation. In this task, the model
should predict an interest in a life insurance product.
We considered the following three scenarios in most of these
tasks: (1) the baseline scenario – only the hand-crafted features
were used; (2) the CoLES scenario – the produced self-supervised
embeddings serve as features; (3) the hybrid (Baseline + CoLES) sce-
nario, combining the hand-crafted features and CoLES embeddings.
In all of these three scenarios, we used the LightGBM method [ 20]
for modelling in the downstream task. In the first and the third
scenarios, we deployed the sets of hand-crafted features, previously
utilized in the organization (see Section 4.1.2 for examples of the
used hand-crafted features).
The results of our experiments are presented in Tables 10 and 11.
We observe that CoLES embeddings significantly improve upon the
hand-crafted features in terms of the test performance on end task.
Note that it is more difficult to design valuable hand-crafted
features for legal entities than for individual customers. A typical
feature is some statistic aggregated over groups of transactions on
some level. For example, one can aggregate card transactions of an
individual customer on the level of their “merchant type” (MCC)
field. In contrast, it is unclear how to group fund transfers of a
company by the “receiver” field (see examples in Table 9). It is hard
to manually find a perfect level of aggregation for receivers, since
they can be grouped in many different ways, e.g. by region, size, the
type of business, etc. We believe that CoLES is able to automatically
learn a suitable aggregation level. This is one of the reasons why in
our experiments the legal entity embeddings demonstrate higher
relative improvements with respect to hand-crafted features than
individual embeddings.
4.3.1 Deployment details. In production environments, our method
is applied in two stages: training of the encoder neural net 𝑀(the
training part), followed by calculation of the embeddings with it (the
inference part). We used only a part of the available data for training
(10 million corporate clients, and 5 million individual clients), but
applied the learnt embedder to all available transactional data, with
more than 90 million cards in total. We did not use any of the avail-
able distributed training techniques [ 21,29,50] during the training
part. During the inference state we leveraged horizontal scaling
scheme, wherein different sequences are processed independently
in parallel on different nodes of a Hadoop cluster.
In order to minimise the efforts of deploying CoLES embeddings
inside the company, we used an ETL process for incremental re-
calculation of embeddings upon arrival of new transactional data.
Specifically, unlike transformers, recurrent encoders 𝜙enc, such as
GRU [ 6], reuse prior computations and enable incremental calcula-
tion: the embedding 𝑐𝑡+𝑘can be computed iteratively from 𝑐𝑡and
(𝑧𝑡+𝑗)𝑘
𝑗=1, using𝑐𝑡+𝑗=𝜙enc(𝑧𝑡+𝑗,𝑐𝑡+𝑗−1). This architectural choice
reduces the inference time needed for updating the embeddings
online.Table 10: Performance comparison of CoLES-based models
with the baselines across downstream tasks for the legal en-
tities
Task Baseline CoLESBaseline
+ CoLES
Insurance lead genera-
tion0.71 0.85 0.85
Credit lead generation 0.75 0.79 0.79
Credit scoring 0.73 0.71 0.77
Holding structure
restoration0.92 0.97 0.97
Fraud monitoring 0.82 0.84 0.85
Baseline includes both transactional and non-transactional hand-crafted
features. AUROC is used as quality metric.
Table 11: Performance comparison of CoLES-based models
with the baselines across downstream tasks for the retail cus-
tomers
Task Baseline CoLESBaseline
+ CoLES
Retail credit scoring 0.88 0.87 0.92
Customer churn 0.74 0.65 0.76
Insurance lead generation 0.75 0.74 0.78
Baseline includes both transactional and non-transactional hand-crafted
features. AUROC is used as quality metric.
Furthermore, by employing a quantization technique it is possi-
ble to compress the sequence embeddings, without much loss in
performance on downstream tasks. For instance, single precision
values in the embedding could be mapped into the range from 0 to
15, which makes a 256-dimensional embedding which originally
took 1Kb, take only 128 bytes.
5 CONCLUSIONS
In this paper, we present Contrastive Learning for Event Sequences
(CoLES) , a novel self-supervised method for building embeddings of
discrete event sequences. CoLES can be efficiently used to produce
embeddings of complex event sequences for various downstream
tasks.
We empirically demonstrate that our approach achieves strong
performance results on several downstream tasks and consistently
outperforms both classical machine learning baselines on hand-
crafted features, as well as on several existing self-supervised and
semi-supervised learning baselines adapted to the domain of event
sequences. In the semi-supervised setting, where the number of
labeled data is limited, our method demonstrates confident perfor-
mance: the lesser is the labeled data, the larger is the performance
margin between CoLES and the supervised-only methods.
Finally, we demonstrate superior performance of CoLES in sev-
eral production-level applications, internally used in our financial
services company. The proposed method of generating embed-
dings appears to be useful in production environments since pre-
calculated embeddings can be easily used for different downstream

--- PAGE 10 ---
tasks without performing complex and time-consuming computa-
tions on the raw event data.
REFERENCES
[1]Dmitrii Babaev, Maxim Savchenko, Alexander Tuzhilin, and Dmitrii Umerenkov.
2019. E.T.-RNN: Applying Deep Learning to Credit Loan Applications. Proceedings
of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining (2019).
[2]Philip Bachman, R. Devon Hjelm, and William Buchwalter. 2019. Learning
Representations by Maximizing Mutual Information Across Views. ArXiv
abs/1906.00910 (2019).
[3]Patrali Chatterjee, Donna L Hoffman, and Thomas P Novak. 2003. Modeling the
clickstream: Implications for web-based advertising efforts. Marketing Science 22,
4 (2003), 520–541.
[4]Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020.
A Simple Framework for Contrastive Learning of Visual Representations. ArXiv
abs/2002.05709 (2020).
[5]Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Ben-
gio. 2014. On the Properties of Neural Machine Translation: Encoder–Decoder
Approaches. In SSST@EMNLP .
[6]Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
Representations using RNN Encoder-Decoder for Statistical Machine Translation.
ArXiv abs/1406.1078 (2014).
[7]Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a similarity metric
discriminatively, with application to face verification. 2005 IEEE Computer Society
Conference on Computer Vision and Pattern Recognition (CVPR’05) 1 (2005), 539–
546 vol. 1.
[8]K. Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
ArXiv abs/2003.10555 (2020).
[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
ArXiv abs/1810.04805 (2019).
[10] A. Dosovitskiy, Jost Tobias Springenberg, Martin A. Riedmiller, and T. Brox.
2014. Discriminative Unsupervised Feature Learning with Convolutional Neural
Networks. In NIPS .
[11] Bradley Efron and Robert J. Tibshirani. 1994. An introduction to the bootstrap.
CRC press (1994).
[12] W. Falcon and Kyunghyun Cho. 2020. A Framework For Contrastive Self-
Supervised Learning And Designing A New Approach. ArXiv abs/2009.00104
(2020).
[13] Jerome H. Friedman. 2001. Greedy function approximation: A gradient boosting
machine. The Annals of Statistics (2001).
[14] Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality Reduction
by Learning an Invariant Mapping. 2006 IEEE Computer Society Conference on
Computer Vision and Pattern Recognition (CVPR’06) 2 (2006), 1735–1742.
[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2019.
Momentum Contrast for Unsupervised Visual Representation Learning. ArXiv
abs/1911.05722 (2019).
[16] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Computation 9 (1997), 1735–1780.
[17] Elad Hoffer and Nir Ailon. 2015. Deep Metric Learning Using Triplet Network.
InSIMBAD .
[18] Junlin Hu, Jiwen Lu, and Yap-Peng Tan. 2014. Discriminative Deep Metric
Learning for Face Verification in the Wild. 2014 IEEE Conference on Computer
Vision and Pattern Recognition (2014), 1875–1882.
[19] Mahmut Kaya and Hasan Şakir Bilge. 2019. Deep Metric Learning: A Survey.
Symmetry 11 (2019), 1066.
[20] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,
Qiwei Ye, and Tie-Yan Liu. 2017. LightGBM: A Highly Efficient Gradient Boosting
Decision Tree. In NIPS .
[21] Robert K. L. Kennedy, Taghi M. Khoshgoftaar, Flavio Villanustre, and Timothy
Humphrey. 2019. A parallel and distributed stochastic gradient descent imple-
mentation using commodity clusters. Journal of Big Data 6 (2019), 1–23.
[22] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised
Learning of Language Representations. ArXiv abs/1909.11942 (2020).
[23] Jianxin Ma, C. Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu.
2020. Disentangled Self-Supervision in Sequential Recommenders. Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining (2020).
[24] R. Manmatha, Chao-Yuan Wu, Alexander J. Smola, and Philipp Krähenbühl.
2017. Sampling Matters in Deep Embedding Learning. 2017 IEEE International
Conference on Computer Vision (ICCV) (2017), 2859–2867.[25] Tomas Mikolov, Kai Chen, G. S. Corrado, and J. Dean. 2013. Efficient Estimation
of Word Representations in Vector Space. In ICLR .
[26] Yabo Ni, Dan Ou, Shichen Liu, Xiang Li, Wenwu Ou, Anxiang Zeng, and Luo
Si. 2018. Perceive Your Users in Depth: Learning Universal User Representa-
tions from Multiple E-commerce Tasks. Proceedings of the 24th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining (2018).
[27] Xuetong Niu, Li Wang, and Xulei Yang. 2019. A Comparison Study of Credit
Card Fraud Detection: Supervised versus Unsupervised. ArXiv abs/1904.10604
(2019).
[28] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word
representations. ArXiv abs/1802.05365 (2018).
[29] Benjamin Recht, Christopher Ré, Stephen J. Wright, and Feng Niu. 2011. Hogwild:
A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. In NIPS .
[30] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings
using Siamese BERT-Networks. In EMNLP/IJCNLP .
[31] Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and
Hrishikesh Khandeparkar. 2019. A Theoretical Analysis of Contrastive Un-
supervised Representation Learning. In In International Conference on Machine
Learning . 5628–5637.
[32] Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. FaceNet: A
unified embedding for face recognition and clustering. 2015 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) (2015), 815–823.
[33] Edgar Simo-Serra, Eduard Trulls, Luis Ferraz, Iasonas Kokkinos, Pascal Fua, and
Francesc Moreno-Noguer. 2015. Discriminative Learning of Deep Convolutional
Feature Point Descriptors. 2015 IEEE International Conference on Computer Vision
(ICCV) (2015), 118–126.
[34] Tanmay Sinha, Patrick Jermann, Nan Li, and Pierre Dillenbourg. 2014. Your click
decides your fate: Inferring information processing and attrition behavior from
mooc video clickstream interactions. arXiv preprint arXiv:1407.7131 (2014).
[35] Yang Song, Yuan Li, Bo Wu, Chao-Yeh Chen, Xiao Zhang, and Hartwig Adam. 2017.
Learning Unified Embedding for Apparel Recognition. 2017 IEEE International
Conference on Computer Vision Workshops (ICCVW) (2017), 2243–2246.
[36] Ellen Tobback and David Martens. 2019. Retail credit scoring using fine-grained
payment data. Journal of The Royal Statistical Society Series A-statistics in Society
182 (2019), 1227–1246.
[37] Evgeniya Ustinova and Victor S. Lempitsky. 2016. Learning Deep Embeddings
with Histogram Loss. In NIPS .
[38] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning
with Contrastive Predictive Coding. ArXiv abs/1807.03748 (2018).
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. ArXiv abs/1706.03762 (2017).
[40] Aleksandr Vorobev, Aleksei Ustimenko, Gleb Gusev, and Pavel Serdyukov. 2019.
Learning to select for a predefined ranking. In ICML .
[41] Lipeng Wan, Qi shan Wang, Alan Papir, and Ignacio Lopez-Moreno. 2018. General-
ized End-to-End Loss for Speaker Verification. 2018 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP) (2018), 4879–4883.
[42] Bénard Wiese and Christian W. Omlin. 2009. Credit Card Transactions, Fraud
Detection, and Machine Learning: Modelling Time with LSTM Recurrent Neural
Networks. In Innovations in Neural Information Paradigms and Applications .
[43] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao. 2009.
Adapting boosting for information retrieval measures. Information Retrieval 13
(2009), 254–270.
[44] Eric P. Xing, Andrew Y. Ng, Michael I. Jordan, and Stuart J. Russell. 2002. Distance
Metric Learning with Application to Clustering with Side-Information. In NIPS .
[45] Tiansheng Yao, Xinyang Yi, D. Cheng, F. Yu, Aditya Menon, L. Hong, Ed Huai
hsin Chi, Steve Tjoa, J. Kang, and Evan Ettinger. 2020. Self-supervised Learning
for Deep Models in Recommendations. ArXiv abs/2007.12865 (2020).
[46] Dong Yi, Zhen Lei, and Stan Z. Li. 2014. Deep Metric Learning for Practical
Person Re-Identification. ArXiv abs/1407.4979 (2014).
[47] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable
are features in deep neural networks? ArXiv abs/1411.1792 (2014).
[48] Andrew Zhai, Hao-Yu Wu, Eric Tzeng, Dong Huk Park, and Charles Rosenberg.
2019. Learning a Unified Embedding for Visual Search at Pinterest. Proceedings of
the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining (2019).
[49] Yanru Zhang and Ali Haghani. 2015. A gradient boosting method to improve
travel time prediction. Transportation Research Part C-emerging Technologies 58
(2015), 308–324.
[50] Shen-Yi Zhao and Wu-Jun Li. 2016. Fast Asynchronous Parallel Stochastic Gradi-
ent Descent: A Lock-Free Approach with Convergence Guarantee. In AAAI .
[51] Chang Zhou, Jianxin Ma, J. Zhang, Jingren Zhou, and Hongxia Yang. 2020. Con-
trastive Learning for Debiased Candidate Generation in Large-Scale Recom-
mender Systems. ArXiv (2020).
[52] Kun Zhou, Haibo Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang, Fuzheng Zhang,
Zhong yuan Wang, and Jirong Wen. 2020. S ˆ3-Rec: Self-Supervised Learning for

--- PAGE 11 ---
Sequential Recommendation with Mutual Information Maximization. ArXiv
abs/2008.07873 (2020).[53] Zhongfang Zhuang, Xiangnan Kong, Elke A. Rundensteiner, Jihane Zouaoui, and
Aditya Arora. 2019. Attributed Sequence Embedding. 2019 IEEE International
Conference on Big Data (Big Data) (2019), 1723–1728.
