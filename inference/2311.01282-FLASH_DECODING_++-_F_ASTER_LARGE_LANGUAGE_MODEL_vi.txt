# FLASH DECODING ++: TĂNG TỐC SUY LUẬN MÔ HÌNH NGÔN NGỮ LỚN TRÊN GPU

Ke Hong†
Đại học Thanh Hoa
& Infinigence-AI
Qiuli Mao
Đại học Thanh Hoa
& Infinigence-AI
Kangdi Chen
Infinigence-AI

Guohao Dai†
Đại học Giao thông Thượng Hải
& Infinigence-AI
Xiuhong Li
Đại học Bắc Kinh
Yuhan Dong
Đại học Thanh Hoa

Jiaming Xu†
Đại học Giao thông Thượng Hải
& Infinigence-AI
Jun Liu
Đại học Giao thông Thượng Hải
& Infinigence-AI
Yu Wang 
Đại học Thanh Hoa

daiguohao@sjtu.edu.cn, daiguohao@infini-ai.com, yu-wang@tsinghua.edu.cn

## TÓM TẮT

Khi Mô hình Ngôn ngữ Lớn (LLM) trở nên ngày càng quan trọng trong nhiều lĩnh vực khác nhau, hiệu suất suy luận LLM là rất quan trọng đối với các ứng dụng LLM lớn. Tuy nhiên, các thách thức sau vẫn chưa được giải quyết trong việc tăng tốc suy luận LLM: (1) Cập nhật softmax một phần đồng bộ. Phép toán softmax yêu cầu một thao tác cập nhật đồng bộ giữa mỗi kết quả softmax một phần, dẫn đến ~20% chi phí phụ cho tính toán attention trong LLM. (2) Tính toán chưa được tận dụng của GEMM phẳng. Hình dạng của các ma trận thực hiện GEMM trong suy luận LLM là phẳng, dẫn đến tính toán chưa được tận dụng và >50% mất hiệu suất sau khi đệm số không trong các thiết kế trước đây (ví dụ: cuBLAS, CUTLASS, v.v.). (3) Mất hiệu suất do luồng dữ liệu tĩnh. Hiệu suất kernel trong LLM phụ thuộc vào các đặc điểm dữ liệu đầu vào khác nhau, cấu hình phần cứng, v.v. Một luồng dữ liệu đơn lẻ và tĩnh có thể dẫn đến 50,25% mất hiệu suất cho GEMM có các hình dạng khác nhau trong suy luận LLM.

Chúng tôi trình bày FlashDecoding++, một engine suy luận LLM nhanh hỗ trợ các LLM chính thống và các back-end phần cứng. Để giải quyết các thách thức trên, FlashDecoding++ sáng tạo đề xuất: (1) Softmax không đồng bộ với giá trị max thống nhất. FlashDecoding++ giới thiệu kỹ thuật giá trị max thống nhất cho các tính toán softmax một phần khác nhau để tránh đồng bộ hóa. Dựa trên điều này, pipelining mịn được đề xuất. (2) Tối ưu hóa GEMM phẳng với double buffering. FlashDecoding++ chỉ ra rằng GEMM phẳng với các hình dạng khác nhau phải đối mặt với các nút thắt cổ chai khác nhau. Sau đó, các kỹ thuật như double buffering được giới thiệu. (3) Luồng dữ liệu heuristic với thích ứng tài nguyên phần cứng. FlashDecoding++ tối ưu hóa luồng dữ liệu một cách heuristic sử dụng tài nguyên phần cứng khác nhau (ví dụ: Tensor Core hoặc CUDA core) xem xét động lực đầu vào. Do tính linh hoạt của các tối ưu hóa trong FlashDecoding++, FlashDecoding++ có thể đạt được tốc độ lên đến 4,86× và 3,93× trên cả GPU NVIDIA và AMD so với các triển khai Hugging Face. FlashDecoding++ cũng đạt được tốc độ trung bình 1,37× so với các engine suy luận LLM hiện đại nhất trên các LLM chính thống.

†Những tác giả này đóng góp ngang nhau cho công trình này.
‡GS. Guohao Dai là Nhà khoa học trưởng tại Infinigence-AI, Ke Hong, Jiaming Xu, Qiuli Mao, và Jun Liu là thực tập sinh tại Infinigence-AI.
GS. Guohao Dai và GS. Yu Wang là các tác giả tương ứng của bài báo này.

---

## 1 Giới thiệu

Khi Mô hình Ngôn ngữ Lớn (LLM) đạt được thành công chưa từng có trong nhiều lĩnh vực khác nhau [2,3,4,5], khối lượng công việc suy luận LLM đang tăng vọt. Ví dụ, OpenAI báo cáo rằng suy luận GPT-4 với độ dài ngữ cảnh 8K có chi phí $0,03 cho 1K token đầu vào và $0,06 cho 1K token đầu ra [6]. Hiện tại, OpenAI có 180,5 triệu người dùng và nhận được hơn 10 triệu truy vấn mỗi ngày [7]. Do đó, chi phí vận hành mô hình của OpenAI như ChatGPT là khoảng $7 triệu mỗi ngày cho phần cứng tính toán cần thiết [8]. Vì vậy, các tối ưu hóa về hiệu suất suy luận LLM sẽ có tác động rất lớn xem xét các tình huống suy luận LLM lớn. Nhiều công trình gần đây đã đề xuất các kỹ thuật để tăng tốc các tác vụ suy luận LLM, bao gồm DeepSpeed [9], FlexGen [10], vLLM [11], OpenPPL [12], FlashDecoding [13], TensorRT-LLM [14], và v.v. [15, 16, 17, 12].

Tác vụ suy luận LLM tạo ra các token (ví dụ: từ) từ chuỗi đầu vào một cách tự hồi quy, và có thể được tổ chức thành hai giai đoạn điển hình: giai đoạn prefill và giai đoạn decode. Giai đoạn prefill tạo ra token đầu tiên bằng cách xử lý prompt đầu vào, và nghiên cứu trước đây (ví dụ: FlashAttention [18,19]) tối ưu hóa độ trễ cho giai đoạn này. Giai đoạn decode tạo ra các token tiếp theo theo thứ tự, và nhiều công trình [9,10,11,15,13,14,20] tập trung vào việc cải thiện thông lượng tạo token (tức là giảm độ trễ của mỗi token). Giai đoạn prefill chiếm ưu thế tổng thời gian cho các tình huống đầu vào chuỗi dài hoặc tạo ra đầu ra ngắn [21, 22], trong khi giai đoạn decode chiếm một phần đáng kể thời gian khi xử lý các chuỗi đầu ra dài [23].

Hình 2 hiển thị luồng dữ liệu chính của suy luận LLM với một lớp transformer cho cả giai đoạn prefill và giai đoạn decode. Một lớp transformer có thể được chia thành các phép toán GEMM (Nhân Ma trận Tổng quát) tuyến tính (ví dụ: chiếu trọng số K, Q, V, O và feedforward) và tính toán attention/softmax. Đối với tính toán attention, phép toán softmax được áp dụng cho một hàng trong ma trận attention. Để cải thiện tính song song, các thiết kế trước đây [18,13] chia ma trận attention thành các tile nhỏ hơn và các hàng cũng được phân chia để tính toán các kết quả softmax một phần. Một phép toán softmax đồng bộ được áp dụng để cập nhật các kết quả softmax một phần trước đó khi một kết quả softmax một phần mới được tính toán. Việc cập nhật softmax một phần đồng bộ như vậy chiếm 18,8% cho tính toán attention của suy luận Llama2-7B theo profiling của chúng tôi trên GPU NVIDIA Tesla A100 với độ dài đầu vào 1024, dẫn đến thách thức đầu tiên cho việc tăng tốc suy luận LLM. Thứ hai, tài nguyên tính toán chưa được tận dụng cho phép toán GEMM phẳng trong giai đoạn decode. Bởi vì giai đoạn decode tạo token theo thứ tự, phép toán GEMM tuyến tính có xu hướng có hình dạng phẳng (thậm chí biến thành phép toán GEMV (Nhân Ma trận-Vector Tổng quát) khi kích thước batch là 1). Đối với kích thước batch nhỏ (ví dụ: 8), các thiết kế trước đây [24,25] đệm ma trận bằng số không để thực hiện GEMM có kích thước lớn hơn (ví dụ: 64), dẫn đến hơn 50% chưa tận dụng tính toán. Thứ ba, hiệu suất suy luận LLM gặp phải vấn đề từ luồng dữ liệu tĩnh xem xét động lực đầu vào và cấu hình phần cứng. Ví dụ, kích thước batch nhỏ làm cho giai đoạn decode của suy luận LLM bị giới hạn bởi bộ nhớ và kích thước batch lớn làm cho nó bị giới hạn bởi tính toán. Một luồng dữ liệu đơn lẻ và tĩnh có thể dẫn đến 50,25% mất hiệu suất cho GEMM có các hình dạng khác nhau trong suy luận LLM.

[Hình 1: Tổng quan so sánh giữa FlashDecoding++ và các thiết kế hiện đại nhất. Kết quả trong hình được báo cáo với mô hình Llama2-7B [1]. Bên trái là với batch size=1 và độ dài đầu vào=1K, và TensorRT-LLM và Hugging Face là baseline SOTA cho NVIDIA/AMD theo kết quả thực nghiệm của chúng tôi. Bên phải hiển thị so sánh toàn diện của cả độ trễ token đầu tiên và độ trễ mỗi token.]

Để giải quyết những thách thức này và cho phép suy luận Mô hình Ngôn ngữ Lớn (LLM) nhanh hơn, chúng tôi trình bày FlashDecoding++ trong bài báo này. FlashDecoding++ sáng tạo đề xuất các đóng góp sau:

• **Softmax không đồng bộ với giá trị max thống nhất.** FlashDecoding++ tận dụng một giá trị max thống nhất cho các tính toán softmax một phần khác nhau. Mỗi kết quả softmax một phần có thể được xử lý riêng lẻ mà không cần cập nhật đồng bộ.

• **Tối ưu hóa GEMM phẳng với double buffering.** FlashDecoding++ chỉ đệm kích thước ma trận đến 8 thay vì 64 trong các thiết kế trước đây cho GEMM hình dạng phẳng để cải thiện việc sử dụng tính toán. Chúng tôi chỉ ra rằng GEMM phẳng với các hình dạng khác nhau phải đối mặt với các nút thắt cổ chai khác nhau, và cải thiện thêm hiệu suất kernel với các kỹ thuật như double buffering.

• **Luồng dữ liệu heuristic với thích ứng tài nguyên phần cứng.** FlashDecoding++ xem xét cả động lực đầu vào và cấu hình phần cứng và áp dụng động tối ưu hóa kernel cho luồng dữ liệu suy luận LLM.

Do tính linh hoạt của các tối ưu hóa, hiệu quả của FlashDecoding++ có thể được chứng minh trên cả GPU NVIDIA và AMD. FlashDecoding++ đạt được tốc độ lên đến 4,86× và 3,93× trên cả GPU NVIDIA và AMD so với các triển khai Hugging Face, tương ứng. Kết quả mở rộng của chúng tôi cho thấy FlashDecoding++ đạt được tốc độ trung bình 1,37× so với FlashDecoding [13], một engine suy luận LLM hiện đại nhất trên nhiều LLM khác nhau (ví dụ: Llama2, ChatGLM2, v.v.).

Phần còn lại của bài báo này được tổ chức như sau. Phần 2 giới thiệu các kiến thức cơ bản về LLM và các công trình liên quan về tăng tốc suy luận LLM. Ba kỹ thuật của chúng tôi, softmax không đồng bộ với giá trị max thống nhất, tối ưu hóa GEMM phẳng với double buffering, và luồng dữ liệu heuristic với thích ứng tài nguyên phần cứng được trình bày chi tiết trong Phần 3, 4, và 5, tương ứng. Phần 6 trình bày các kết quả đánh giá. Các công trình liên quan về suy luận LLM được giới thiệu trong Phần 7, và Phần 8 kết luận bài báo.

---

[Hình 2: Tổng quan về luồng dữ liệu suy luận Mô hình Ngôn ngữ Lớn. Chúng tôi hiển thị so sánh luồng dữ liệu giữa giai đoạn prefill và giai đoạn decode. Giai đoạn prefill chủ yếu liên quan đến phép toán GEMM, trong khi giai đoạn decode chủ yếu liên quan đến phép toán GEMV/Flat GEMM.]

[Hình 3: FlashDecoding++ đề xuất ba giải pháp cho các thách thức tương ứng trong suy luận Mô hình Ngôn ngữ Lớn. (a) FlashDecoding++ đề xuất kỹ thuật softmax không đồng bộ với giá trị max thống nhất, tránh cập nhật đồng bộ đến các kết quả attention một phần trước đó. (b) FlashDecoding++ tối ưu hóa GEMM phẳng bằng cách cải thiện việc sử dụng tính toán. (c) FlashDecoding++ tối ưu hóa luồng dữ liệu một cách heuristic.]

## 2 Kiến thức nền

### 2.1 Tổng quan Luồng dữ liệu Suy luận LLM

Nhiệm vụ suy luận LLM là tạo ra các token từ chuỗi đầu vào, có thể được sử dụng để hoàn thành một câu hoặc trả lời một câu hỏi. Tổng quan về luồng dữ liệu suy luận LLM được hiển thị trong Hình 2. Như chúng ta có thể thấy, luồng dữ liệu suy luận LLM có thể được tổ chức thành hai giai đoạn điển hình với các phép toán tương tự: một giai đoạn prefill và một số giai đoạn decode.

Giai đoạn prefill "hiểu" chuỗi đầu vào (tức là "Đại dương lớn nhất là gì?"). Mỗi token (chúng tôi đặt một từ làm một token trong Hình 2) được mã hóa như một vector nhúng, và chuỗi đầu vào được tổ chức thành một ma trận. Đầu ra chính của giai đoạn prefill là một token mới, được dự đoán là token tiếp theo sau chuỗi đầu vào (tức là "Pacific" trong hình này). Giai đoạn decode "tạo ra" chuỗi đầu ra (tức là "Pacific", "Ocean", v.v.) Token đầu ra của giai đoạn prefill được lấy làm đầu vào của giai đoạn decode. Giai đoạn decode được thực hiện tự hồi quy, và mỗi token đầu ra được sử dụng làm token đầu vào cho lần tiếp theo (ví dụ: "Ocean" được sử dụng thêm làm đầu vào).

### 2.2 Các Phép toán trong Suy luận LLM

Các phép toán chính trong suy luận LLM được mô tả như phép toán ① đến ⑥ trong Hình 2, bao gồm chiếu tuyến tính (① và ⑤), attention (②, ③, và ④), và mạng feedforward (⑥). Để đơn giản, các phép toán như position embedding [26], kích hoạt phi tuyến [27,28,29], mask [26], và khác không được hiển thị trong hình. Các phép toán trong giai đoạn prefill và giai đoạn decode khác nhau về hình dạng dữ liệu. Bởi vì chỉ một token (batch size =1) hoặc ít token (batch size >1) được xử lý tại một thời điểm, các ma trận đầu vào trong giai đoạn decode là ma trận hình dạng phẳng hoặc thậm chí là vector.

**Chiếu Tuyến tính.** Chiếu tuyến tính hoạt động như lớp kết nối đầy đủ, nhân đầu vào với ma trận trọng số (tức là WK, WQ, WV, WO, được gọi là chiếu K, Q, V và chiếu O). Đối với giai đoạn prefill, chiếu K, Q, V tạo ra ma trận K, Q, V. Đối với giai đoạn decode, chiếu K, Q, V tạo ra ba vector tương ứng và được nối với K và V (tức là KVcache, màu vàng và xanh nhạt trong Hình 2) trong giai đoạn prefill.

softmax (Q×K^T)×V                                                (1)

**Attention.** Phép toán attention chủ yếu được chia thành ba phép toán (② đến ④: Q×K, softmax, Attention ×V), như được hiển thị trong Phương trình (1). Đối với P=Q×K^T, phép toán softmax được thực hiện cho mỗi hàng của ma trận kết quả P. Tính toán softmax chi tiết được hiển thị trong Hình 4(a). Giá trị tối đa m(x) được tính toán trước. Số mũ của mỗi phần tử chia cho e^m(x), f(x), sau đó được xử lý. Những số mũ này được chuẩn hóa đến tổng của tất cả số mũ (tức là l(x)) để có được kết quả softmax.

**Mạng Feedforward.** Mạng feedforward chủ yếu bao gồm hai lớp kết nối đầy đủ. Lớp đầu tiên (⑥ FFN 1) mở rộng các chiều đặc trưng để tăng cường khả năng biểu diễn. Lớp thứ hai (⑥ FFN 2) khôi phục các chiều đặc trưng và phục vụ như lớp đầu ra.

### 2.3 Tối ưu hóa Attention

Phép toán softmax được hiển thị trong Hình 4(a) yêu cầu tất cả dữ liệu toàn cục được tính toán và lưu trữ trước khi nó có thể tiến hành. Điều này dẫn đến tiêu thụ bộ nhớ cao và tính song song thấp. Các công trình sau đó đề xuất kỹ thuật softmax một phần để giảm tiêu thụ bộ nhớ [18,19] hoặc cải thiện tính song song [13]. Hình 4(b) hiển thị sơ đồ của phép toán softmax một phần. Ý tưởng chính là chia vector x thành các vector một phần (tức là x' và x''). Các kết quả softmax một phần của x' và x'' được tính toán riêng biệt theo Hình 4(a), và sau đó được cập nhật đồng bộ bởi nhau. Tính toán chi tiết của cập nhật đồng bộ này được hiển thị trong Phương trình (2). Với việc triển khai softmax một phần, chúng ta có thể đạt được tính song song hiệu quả của tính toán trong khi giảm chi phí bộ nhớ cho tính toán attention.

m(x) = max(m(x'), m(x''))
f(x') = e^(m(x')-m(x))f(x')
f(x'') = e^(m(x'')-m(x))f(x'')
l(x) = f(x') + f(x'')
softmax ([x', x'']) = [f(x'), f(x'')] ÷ l(x)                    (2)

Tuy nhiên, vì softmax một phần cần được cập nhật theo các kết quả softmax một phần khác, nó không thể tránh khỏi việc giới thiệu các phép toán đồng bộ hóa dữ liệu. Theo kết quả profiling của chúng tôi, phép toán cập nhật đồng bộ như vậy dẫn đến 18,8% chi phí phụ trong tính toán attention cho suy luận Llama2-7B trên GPU NVIDIA Tesla A100 với độ dài đầu vào 1024.

---

[Hình 4: So sánh các sơ đồ tính toán softmax khác nhau. (a) Tính toán softmax cho toàn bộ vector. (b) Tính toán softmax một phần cho mỗi vector một phần, và một phép toán cập nhật đồng bộ được yêu cầu cho tất cả kết quả softmax một phần. (c) Tính toán softmax một phần sử dụng giá trị max thống nhất, và mỗi vector một phần được xử lý riêng lẻ mà không cần cập nhật đồng bộ.]

## 3 Softmax Không đồng bộ với Giá trị Tối đa Thống nhất

**Động lực.** Phép toán softmax một phần yêu cầu đồng bộ hóa giữa các vector một phần khác nhau, dẫn đến ~20% chi phí phụ của phép toán attention. Như được hiển thị trong Hình 3(a), đồng bộ hóa được yêu cầu sau khi giá trị tối đa của vector một phần được tính toán. Giá trị tối đa được sử dụng để cập nhật softmax một phần trước đó (tức là tính toán lại attention trước đó). Vì vậy, để giảm chi phí phụ đồng bộ hóa, vấn đề chính cần được giải quyết là làm thế nào để tính toán mỗi kết quả softmax một phần mà không yêu cầu kết quả từ tính toán softmax một phần khác.

**Thách thức.** Lý do mà đồng bộ hóa được yêu cầu nằm ở chỗ giá trị tối đa của mỗi vector một phần là khác nhau. Giá trị tối đa được sử dụng để tránh tràn của phép toán số mũ (f(x) trong Hình 4(a)), và các số mũ được tổng hợp (l(x) trong Hình 4(a)) như mẫu số của phép toán softmax. Phép toán phi tuyến như vậy trên mỗi giá trị tối đa một phần làm cho việc đồng bộ hóa giữa mỗi tính toán softmax một phần không thể tránh khỏi.

**Phân tích và Hiểu biết.** Theo công thức tính toán softmax, giá trị tối đa được sử dụng như yếu tố tỷ lệ cho cả tử số và mẫu số (tức là f(x) và l(x) trong Hình 4(a)). Hiểu biết chính của chúng tôi là, yếu tố tỷ lệ có thể là một số bất kỳ thay vì sử dụng giá trị tối đa về mặt toán học, được hiển thị trong Phương trình (3). Khi chúng ta đặt φ = 0, nó trở thành tính toán softmax ban đầu [30].

softmax (x) = [e^(x₁-m(x)), ..., e^(xₐ-m(x))] / Σᵢe^(xᵢ-m(x))
            = [e^(x₁-φ), ..., e^(xₐ-φ)] / Σᵢe^(xᵢ-φ), ∀φ∈ℝ        (3)

Tuy nhiên, yếu tố tỷ lệ không thể là một số bất kỳ xem xét việc tràn của tính toán số mũ. Đối với trường hợp xᵢ ≫ φ, e^(xᵢ-φ) tràn và không thể được biểu diễn bằng số dấu phẩy động có độ rộng cố định (ví dụ: float32 cho kết quả số mũ trong các engine LLM hiện tại). Đối với trường hợp khác xᵢ ≪ φ, e^(xᵢ-φ) → 0, dẫn đến mất độ chính xác. Vì vậy, một yếu tố tỷ lệ φ thích hợp nên được lựa chọn cẩn thận để tránh hai trường hợp trên. Hình 5 hiển thị phân bố thống kê của xᵢ (các phần tử trong vector đầu vào của softmax) trong các LLM điển hình với các đầu vào khác nhau [31]. Hiểu biết chính của chúng tôi là, >99,99% xᵢ nằm trong một phạm vi nhất định. Cụ thể, đối với Llama2-7B, chúng ta có -16,8 < xᵢ < 6,5 cho >99,99% xᵢ. Bởi vì e^(b-a) và e^(a-b) có thể được biểu diễn bằng định dạng float32, chúng ta có thể đặt φ = a trong Phương trình (3). Đối với OPT-6.7B, chúng tôi không áp dụng kỹ thuật trong phần này vì phạm vi lớn trong Hình 5.

[Hình 5: Phân bố thống kê của xᵢ (các phần tử trong vector đầu vào của softmax) trong các LLM điển hình với các đầu vào khác nhau.]

**Phương pháp: Không đồng bộ hóa.** Dựa trên các hiểu biết trên, mỗi tính toán softmax một phần chia sẻ một giá trị tối đa thống nhất, φ. Sau phép toán softmax, một phép toán tích vô hướng được thực hiện giữa kết quả softmax và một cột của V (tức là v). Giả sử rằng vector đầu vào x có thể được chia thành p vector một phần, x = [x⁽¹⁾, ..., x⁽ᵖ⁾] (v = [v⁽¹⁾, ..., v⁽ᵖ⁾] tương ứng), chúng ta có:

⟨softmax (x), v⟩ = Σᵢe^(xᵢ-φ)·vᵢ / Σᵢe^(xᵢ-φ)
                 = Σⱼ₌₁ᵖ Σᵢ₌₁^(d/p) e^(xᵢ⁽ʲ⁾-φ)·vᵢ⁽ʲ⁾ / Σⱼ₌₁ᵖ Σᵢ₌₁^(d/p) e^(xᵢ⁽ʲ⁾-φ)    (4)

Tích lũy bên trong trong cả tử số và mẫu số chỉ lấy các vector một phần x⁽ʲ⁾ và v⁽ʲ⁾ làm đầu vào, do đó chúng có thể được xử lý không đồng bộ và riêng lẻ. Tích lũy bên ngoài chỉ được xử lý sau khi tất cả vector một phần được xử lý. Như chúng ta có thể thấy trong Hình 4(c), mỗi f(x⁽ʲ⁾) được tính toán riêng lẻ, và softmax (x) được tính toán sau khi tất cả x⁽ʲ⁾ được tính toán.

**Phương pháp: Tính toán lại.** Không mất tính tổng quát, chúng ta giả sử a < xᵢ - φ < b cho mỗi xᵢ để đảm bảo độ chính xác và tránh tràn. Sau đó, phép toán softmax một phần được xử lý riêng lẻ. Tuy nhiên, khi xᵢ - φ ≤ a hoặc xᵢ - φ ≥ b, tính toán softmax một phần không đồng bộ bị chấm dứt cho vector x mà xᵢ thuộc về. Softmax sau đó được tính toán lại sử dụng sơ đồ softmax một phần đồng bộ (được sử dụng trong FlashAttention [18,19] và FlashDecoding [13]) được hiển thị trong Hình 4(b). Sơ đồ tính toán lại như vậy tránh tràn trong khi giới thiệu chi phí phụ không đáng kể dựa trên dữ liệu thống kê được hiển thị trong Hình 5.

**Ví dụ.** Hình 6 hiển thị một ví dụ về sơ đồ softmax không đồng bộ. Chúng tôi đặt a = -3, b = 3, φ = 6. Hai vector x và y được tính toán từ Q×K^T trong Phương trình (1), và được chia thành 2 vector một phần. Chúng tôi bỏ qua quá trình từ Q×K^T đến các vector một phần này. Đối với mỗi xᵢ, chúng ta có a < xᵢ - φ < b, chúng ta xử lý e^(x₁-φ)·v₁ + e^(x₂-φ)·v₂ và e^(x₁-φ) + e^(x₂-φ) cho vector một phần đầu tiên của x sử dụng hai luồng không đồng bộ. Sau đó, mỗi luồng chuyển sang vector một phần tiếp theo cho tính toán tương ứng (tức là e^(x₃-φ)·v₃ + e^(x₄-φ)·v₄ và e^(x₃-φ) + e^(x₄-φ)). Hai luồng được đồng bộ hóa khi tất cả vector một phần được xử lý, và thực hiện phép toán chia trong Phương trình (4). Đối với y, vector một phần đầu tiên được xử lý tương tự. Tuy nhiên, chúng ta thấy rằng y₃ - φ > b, sau đó hai luồng bị chấm dứt và luồng đầu tiên tính toán lại tất cả vector một phần theo sơ đồ softmax một phần đồng bộ trong Hình 4(b).

[Hình 6: Ví dụ về tính toán softmax một phần không đồng bộ. (a) Mỗi kết quả softmax một phần được xử lý riêng lẻ mà không cần cập nhật đồng bộ. (b) Quá trình tính toán lại cho tất cả tính toán softmax một phần được yêu cầu khi xảy ra tràn.]

## 4 Tối ưu hóa GEMM Phẳng với Double Buffering

**Động lực.** Quá trình giai đoạn decode chủ yếu bao gồm phép toán GEMV (batch size=1) hoặc GEMM phẳng (batch size>1). Không mất tính tổng quát, các phép toán GEMV/GEMM có thể được biểu diễn sử dụng M, N, K, trong đó kích thước của hai ma trận được nhân là M×K và K×N. Các engine suy luận LLM trước đây sử dụng Tensor Core để tăng tốc các phép toán này sử dụng các thư viện như cuBLAS [24] và CUTLASS [25]. Mặc dù các kiến trúc Tensor Core hiện đại [32] xử lý GEMM với M = 8, các thư viện này thường chia M-dimension thành 64 để ẩn độ trễ bộ nhớ. Tuy nhiên, đối với các phép toán GEMV hoặc GEMM phẳng trong giai đoạn decode, chúng ta thường có M ≪ 64 và M-dimension được đệm đến 64 bằng số không. Việc đệm dẫn đến tính toán chưa được tận dụng, và vấn đề chính là xử lý các phép toán GEMV hoặc GEMM phẳng với các tile nhỏ hơn (tức là đệm đến 8 tương ứng với các kiến trúc Tensor Core hiện đại) trong M-dimension.

**Thách thức.** Xử lý các phép toán GEMV hoặc GEMM phẳng không đơn giản khi M-dimension được đệm đến 8. Kỹ thuật tiling trong các thư viện hiện đại như cuBLAS [24] và CUTLASS [25] chỉ có thể được áp dụng cho N-dimension và K-dimension. Các tile trên K-dimension được xử lý tuần tự trong một khối GPU để tránh các phép toán atomic trong quá trình giảm. Tiling trên N-dimension ảnh hưởng đến cả tính song song và tỷ lệ tính toán/bộ nhớ, cả hai đều quan trọng cho việc tăng tốc GEMV và GEMM phẳng.

**Phân tích và Hiểu biết.** Giả sử rằng kích thước tiling của N-dimension và K-dimension là B_N và B_K, tương ứng. Tính toán của mỗi tile GEMM là 2×M×B_N×B_K với tổng B = N×K/(B_N×B_K) tile GEMM. Tổng truy cập bộ nhớ là (M×B_K + B_N×B_K)×B + M×N. Vì vậy, tỷ lệ tính toán/bộ nhớ là:

(2×M×B_N×B_K×B) / ((M×B_K + B_N×B_K)×B + M×N) = (2×M×K) / (K + M×K/B_N + M)    (5)

Mặt khác, tính song song là N/B_N. Vì vậy, tỷ lệ tính toán/bộ nhớ cho thấy tương quan dương với B_N trong khi tính song song cho thấy tương quan âm với B_N, phơi bày một mâu thuẫn trong việc cải thiện hiệu suất của GEMV hoặc GEMM phẳng. Chúng tôi mô tả hiệu suất chuẩn hóa của GEMM phẳng trong Hình 7 với N và B_N khác nhau. Hiểu biết chính của chúng tôi là, đối với N nhỏ hơn, GEMM phẳng bị giới hạn bởi tính song song. Có 108 Streaming Multiprocessors (SM) trong NVIDIA Tesla A100. N/B_N có xu hướng là một hằng số (ví dụ: 128 hoặc 256), có liên quan đến tính song song phần cứng (số lượng SM). Hiểu biết chính khác là, đối với N lớn hơn, GEMM phẳng trở nên bị giới hạn bởi bộ nhớ. Hiệu suất của những trường hợp này có thể được cải thiện bằng cách ẩn độ trễ truy cập bộ nhớ.

[Hình 7: Hiệu suất GEMM phẳng chuẩn hóa dưới các kích thước N-dimension và kích thước tiling N-dimension khác nhau. Chúng tôi đặt M = 8 và thực hiện GEMM trên GPU NVIDIA Tesla A100.]

**Phương pháp: Double Buffering.** Để ẩn độ trễ truy cập bộ nhớ, chúng tôi giới thiệu kỹ thuật double buffering cho phép toán GEMM phẳng. Chúng tôi phân bổ hai buffer riêng biệt trong bộ nhớ chia sẻ. Tile trong một buffer thực hiện phép toán GEMM, trong khi buffer khác tải một tile mới cho phép toán GEMM tiếp theo. Vì vậy, tính toán và truy cập bộ nhớ được chồng lấp. Chúng tôi áp dụng kỹ thuật như vậy khi N lớn trong thực tế của chúng tôi.

**Ví dụ.** Hình 8 hiển thị ví dụ về tối ưu hóa GEMM phẳng của chúng tôi với double buffering. Đối với M < 8, M-dimension được đệm trước đến 8 xem xét các kiến trúc Tensor Core hiện đại. Các khối lượng công việc trong K-dimension được xử lý trong một khối GPU (ví dụ: A1, A2, A3, ...), trong khi các khối lượng công việc trong N-dimension được xử lý song song sử dụng các khối GPU khác nhau (ví dụ: C1, C2, ...). Chúng tôi lấy GPU Block 1 làm ví dụ, tile đầu tiên cho mỗi ma trận trong K-dimension (tức là A1 và B1) được tải vào buffer bên trái trong bộ nhớ chia sẻ. Sau đó, phép toán GEMM được thực hiện giữa A1 và B1. Sau đó, A2 và B2 được tải vào buffer bên phải trong bộ nhớ chia sẻ. Các tile tiếp theo được xử lý tương tự theo sơ đồ double buffering.

[Hình 8: Double buffering cho GEMM phẳng khi N-dimension lớn. M-dimension được đệm đến 8 và không được chia tile.]

## 5 Luồng dữ liệu Heuristic với Thích ứng Tài nguyên Phần cứng

**Động lực.** Mặc dù FlashDecoding++ tối ưu hóa phép toán GEMM phẳng trong Phần 4, nó không bao phủ tất cả các phép toán (thậm chí chỉ cho GEMM) trong suy luận LLM. Như đã đề cập trong Hình 2, các hình dạng của GEMM trong các phép toán và hai giai đoạn khác nhau thay đổi. Vì vậy, khối lượng công việc GEMM trong suy luận LLM có thể là GEMV (batch size=1 cho giai đoạn decode), GEMM phẳng (kích thước batch nhỏ cho giai đoạn decode và độ dài chuỗi ngắn cho giai đoạn prefill) và GEMM thông thường (kích thước batch lớn hoặc độ dài chuỗi dài cho giai đoạn prefill). Để tận dụng khả năng tính toán mạnh mẽ của Tensor Core, các framework hiện tại như FasterTransformer [33] và DeepSpeed [9] có xu hướng sử dụng triển khai GEMM được tối ưu hóa cao từ cuBLAS [24] để xử lý các khối lượng công việc khác nhau. Tuy nhiên, triển khai Tensor Core thất bại với khối lượng công việc GEMV. Khối lượng công việc GEMV có thể được tối ưu hóa bằng cách sử dụng CUDA Core trong các thiết kế trước đây như FastGEMV [34]. Đối với một lớp tuyến tính Llama2-7B trong giai đoạn decode, triển khai Tensor Core từ cuBLAS chỉ đạt được 82,15% hiệu suất của triển khai CUDA Core sử dụng FastGEMV trên GPU NVIDIA A100. Mặt khác, sử dụng CUDA Core để thực hiện chiếu trên đầu vào decode batchsize=4 chỉ đạt được 49,75% hiệu suất so với triển khai Tensor Core. Vì vậy, để tiếp cận hiệu suất tính toán tối ưu, một luồng dữ liệu heuristic được cho là khai thác cho các khối lượng công việc khác nhau.

**Thách thức.** Mặc dù một luồng dữ liệu heuristic có thể tồn tại trong việc triển khai các khối lượng công việc tuyến tính khác nhau, việc xây dựng ánh xạ từ một khối lượng công việc nhất định đến một triển khai tối ưu là thách thức. Trong tình huống suy luận LLM, có nhiều yếu tố khác nhau ảnh hưởng đến hiệu suất triển khai của các khối lượng công việc tuyến tính: (a) Động lực đầu vào. Sự đa dạng của kích thước batch và độ dài chuỗi đầu vào mang lại các khối lượng công việc động. (b) Đa dạng mô hình. Khối lượng công việc tuyến tính thay đổi với các cấu trúc và kích thước mô hình khác nhau. (c) Khả năng GPU. Hiệu suất tương đối giữa các triển khai thay đổi với các đặc tính GPU, như băng thông bộ nhớ, kích thước cache, và khả năng tính toán. (d) Hiệu ứng kỹ thuật. Nỗ lực kỹ thuật cũng ảnh hưởng rất nhiều đến hiệu suất kernel. Tất cả những yếu tố ảnh hưởng này xây dựng một không gian tìm kiếm lớn, làm cho việc tạo ra một ánh xạ hiệu quả giữa khối lượng công việc tuyến tính và triển khai tối ưu tương ứng trở nên không đơn giản.

**Phân tích và Hiểu biết.** Mặc dù tất cả các yếu tố ảnh hưởng tạo thành một không gian tìm kiếm lớn, tính đồng nhất của các lớp khác nhau trong LLM giảm đáng kể không gian tìm kiếm cho tối ưu hóa toán tử. Hình 2 hiển thị bốn phép toán GEMV/GEMM tuyến tính trong giai đoạn prefill và giai đoạn decode, tức là chiếu K, Q, V, chiếu O, và hai phép toán feedforward. Mỗi phép toán GEMV/GEMM có thể được trừu tượng hóa như một phép nhân giữa ma trận hình dạng (M×K) và ma trận hình dạng (K×N). Hiểu biết chính của chúng tôi là, chỉ có bốn hình dạng [K, N] cho một LLM nhất định. Hơn nữa, M chỉ liên quan đến độ dài chuỗi đầu vào và kích thước batch cho giai đoạn prefill, và kích thước batch cho giai đoạn decode. Hình 9(a) hiển thị các hình dạng hạn chế của các phép toán GEMV/GEMM trong suy luận LLM.

**Phương pháp: Luồng quyết định cho các điểm uốn.** Bởi vì chỉ có bốn hình dạng [K, N] tồn tại cho một LLM nhất định, chúng tôi sử dụng ba loại triển khai cho các phép toán GEMV/GEMM khi M thay đổi: FastGEMV cho các phép toán GEMV và GEMM phẳng (ImplA), tối ưu hóa GEMM phẳng của chúng tôi trong Phần 4 (ImplB), và các thư viện CUTLASS [25] được tối ưu hóa cho GEMM thông thường (ImplC). Vì vậy, việc quyết định có áp dụng ImplA hay ImplB cho M nhỏ, và ImplB hay ImplC cho M lớn là quan trọng. Hình 9(b) hiển thị luồng quyết định. FlashDecoding++ profile hiệu suất của ImplA và ImplB cho một M nhất định, và tăng M để tìm điểm uốn M1 nơi hiệu suất của ImplB tốt hơn ImplA. Điểm uốn khác M2 được tìm thấy tương tự nơi hiệu suất của ImplC tốt hơn ImplB. Lưu ý rằng mỗi [N, K] có M1 và M2 riêng của nó.

**Phương pháp: Luồng dữ liệu heuristic.** Đối với suy luận LLM runtime, FlashDecoding++ áp dụng ImplA sử dụng CUDA Core khi M < M1, và ImplB/ImplC sử dụng Tensor Core khi M1 ≤ M < M2/M2 ≤ M. Lưu ý rằng luồng quyết định được thực hiện offline, nó không ảnh hưởng đến hiệu suất của suy luận LLM runtime.

**Ví dụ.** Hình 9(c) hiển thị một ví dụ về áp dụng luồng dữ liệu heuristic cho mô hình Llama2-7B. Bốn hình dạng [N, K] là [12288, 4096] cho chiếu K, Q, V, [4096, 4096] cho chiếu O, [11008, 4096] và [4096, 11008] cho FFN. Đối với mỗi [N, K], các điểm uốn được tìm thấy dựa trên luồng quyết định trong Hình 9(c). Sau đó, một bảng tra cứu được tạo thành, và mỗi phép toán GEMV/GEMM được thực hiện theo các triển khai tương ứng trong runtime. Trong ví dụ này, FastGEMV được áp dụng cho chiếu K, Q, V khi batch size=1 (M = 1) cho giai đoạn decode, và tối ưu hóa GEMM phẳng của chúng tôi được áp dụng khi batch size=1/độ dài chuỗi đầu vào=8 cho FFN 1 (M = 8).

[Hình 9: Luồng dữ liệu heuristic với thích ứng tài nguyên phần cứng trong FlashDecoding++. (a) Chỉ có bốn hình dạng [N, K] tồn tại cho một LLM nhất định. (b) Luồng quyết định. Chúng tôi duyệt qua tất cả các lựa chọn [N, K] và profile hiệu suất của ba triển khai đại diện. M được tăng để tìm hai điểm uốn cho luồng dữ liệu heuristic runtime. (c) FlashDecoding++ sử dụng Tensor Core/CUDA Core một cách heuristic với triển khai GEMV/GEMM tương ứng bằng cách tham chiếu đến bảng tra cứu.]

## 6 Đánh giá

### 6.1 Thiết lập Thực nghiệm

Chúng tôi đánh giá hiệu suất của FlashDecoding++ trên các GPU khác nhau với nhiều Mô hình Ngôn ngữ Lớn khác nhau. Chúng tôi so sánh hiệu suất với một số engine suy luận LLM hiện đại nhất.

#### 6.1.1 Nền tảng Phần cứng

Chúng tôi đánh giá hiệu suất của FlashDecoding++ và các engine LLM khác trên cả nền tảng NVIDIA và AMD để tạo ra một so sánh toàn diện. Chúng tôi chọn hai GPU khác nhau cho mỗi nền tảng: Tesla A100 và RTX3090 cho NVIDIA, MI210 và RX7900XTX cho AMD. Chúng tôi hiển thị cấu hình chi tiết trong Bảng 6.1.1.

[Bảng 1: Nền tảng Phần cứng]

#### 6.1.2 Baseline Engine LLM

Chúng tôi triển khai FlashDecoding++ của chúng tôi sử dụng front-end dựa trên Pytorch với backend C++ và CUDA cho GPU NVIDIA trong khi ROCm cho GPU AMD. Chúng tôi so sánh hiệu suất suy luận trong cả giai đoạn prefill và giai đoạn decode với các baseline engine LLM sau: Hugging Face (HF) [35], vLLM [11], DeepSpeed [9], TensorRT-LLM [14], OpenPPL [12], và FlashAttention2/FlashDecoding [19, 13]. Những baseline này được giới thiệu trong Phần 7.

#### 6.1.3 Mô hình

Chúng tôi đánh giá hiệu suất của FlashDecoding++ với các engine suy luận LLM khác trên ba Mô hình Ngôn ngữ Lớn điển hình: Llama2, OPT, và ChatGLM2. Bảng 6.1.2 hiển thị cấu hình chi tiết của những mô hình này. Lưu ý rằng có thể có một số mô hình trong một LLM (ví dụ: Llama2-7B, Llama2-13B) với các cấu hình khác nhau (ví dụ: số lượng head và lớp).

[Bảng 2: Cấu hình Mô hình]

• **Llama2** [1] là một bộ LLM mã nguồn mở chính thống được phát hành bởi Meta vào năm 2023. Đây là một bộ sưu tập các mô hình văn bản tạo sinh được huấn luyện trước và tinh chỉnh với quy mô từ 7B đến 70B tham số.

• **OPT** [36], là một bộ transformer chỉ decoder được huấn luyện trước với quy mô từ 125M đến 175B tham số được phát hành bởi Meta AI.

• **ChatGLM2** [37] là một LLM mã nguồn mở hỗ trợ chat song ngữ (Trung-Anh).

### 6.2 So sánh với Hiện đại nhất

Chúng tôi so sánh FlashDecoding++ với các engine suy luận LLM hiện đại nhất trong Hình 10 và Hình 11 trên GPU NVIDIA, Hình 12 và Hình 13 cho GPU AMD. Đối với giai đoạn decode, FlashDecoding++ đạt được tốc độ lên đến 4,86× so với các triển khai Hugging Face trên ba LLM và hai GPU. Tốc độ trung bình so với vLLM, DeepSpeed, TensorRT-LLM, OpenPPL, và FlashDecoding là 1,24×, 1,44×, 1,13×, 1,24×, và 1,21× (1,37× trên Tesla A100 so với FlashDecoding), tương ứng. Đối với giai đoạn prefill, FlashDecoding++ đạt được tốc độ lên đến 1,40× so với các triển khai Hugging Face. Tốc độ trung bình so với DeepSpeed, TensorRT-LLM, OpenPPL, FlashAttention2 và FlashDecoding là 1,05×, 1,06×, 1,08×, 1,09×, và 1,08×, tương ứng. Chúng tôi cũng hiển thị kết quả decode trên hai GPU AMD. Hiện tại, chỉ có triển khai Hugging Face gốc có thể được thực hiện trên GPU AMD làm baseline. FlashDecoding++ đạt được lên đến 2,27× và 3,93× so với baseline trên RX7900XTX và MI210, tương ứng.

[Hình 10: Tốc độ của giai đoạn decode trên GPU NVIDIA. Thanh trống biểu thị mô hình không thể được thực hiện (ví dụ: OpenPPL không hỗ trợ OPT-6.7B/ChatGLM2-6B, TensorRT-LLM thất bại trong việc biên dịch mô hình với độ dài đầu vào >8K, v.v.)]

[Hình 11: Tốc độ của giai đoạn prefill trên GPU NVIDIA.]

[Hình 12: Tốc độ của giai đoạn decode trên AMD RX7900XTX.]

[Hình 13: Tốc độ của giai đoạn decode trên AMD MI210.]

## 7 Các Công trình Liên quan

Tăng tốc suy luận mô hình ngôn ngữ lớn đã nhận được sự chú ý đáng kể trong nghiên cứu gần đây, với một số phương pháp và kỹ thuật đáng chú ý xuất hiện trong lĩnh vực này. DeepSpeed [9] là một engine toàn diện tối ưu hóa cả giai đoạn huấn luyện và suy luận cho LLM. Nó đạt được hiệu suất suy luận mạnh mẽ thông qua fusion kernel và quản lý bộ nhớ GPU hiệu quả, với trọng tâm đặc biệt vào tối ưu hóa việc sử dụng bộ nhớ cho KVcache. vLLM [11] cải thiện việc sử dụng bộ nhớ GPU thông qua các kỹ thuật quản lý bộ nhớ hiệu quả và phương pháp PageAttention, dẫn đến tăng kích thước batch tối đa và nâng cao giới hạn trên của hiệu suất suy luận. FlashAttention [18,19] tối ưu hóa quá trình tính toán self-attention trong giai đoạn prefill thông qua cải thiện tính song song và phân phối khối lượng công việc. FlashDecoding [13] là một mở rộng của FlashAttention và tăng cường tính song song thông qua việc phân chia K và V, hỗ trợ tính toán self-attention hiệu quả cho chuỗi dài trong giai đoạn decode. FasterTransformer [33] và OpenPPL [12] triển khai các engine suy luận mô hình lớn sử dụng C++ để giảm chi phí phụ từ việc lập lịch kernel, so với các triển khai Python. Chúng cũng sử dụng các kỹ thuật quản lý bộ nhớ và fusion kernel để đạt được suy luận LLM hiệu quả. TensorRT-LLM [14] được xây dựng dựa trên TensorRT [38] và engine FasterTransformer [33] (C++) và kết hợp các công nghệ mã nguồn mở tiên tiến như FlashAttention [18,19]. Ngoài ra, nó tăng cường tính dễ sử dụng bằng cách cung cấp API Python.

## 8 Kết luận

Chúng tôi đề xuất FlashDecoding++, một engine suy luận Mô hình Ngôn ngữ Lớn nhanh trong bài báo này. FlashDecoding++ tăng tốc các LLM chính thống với hỗ trợ nhiều backend phần cứng. FlashDecoding++ đề xuất ba thiết kế mới: softmax không đồng bộ với giá trị max thống nhất, tối ưu hóa GEMM phẳng với double buffering, và luồng dữ liệu heuristic với thích ứng tài nguyên phần cứng, đạt được tốc độ lên đến 4,86× và 3,93× trên GPU NVIDIA và AMD so với các triển khai Hugging Face. FlashDecoding++ cũng đạt được tốc độ trung bình 1,37× so với các engine suy luận LLM hiện đại nhất, FlashDecoding, trên nhiều LLM khác nhau.

## Tài liệu tham khảo

[1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[2] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine. Nature medicine, 29(8):1930–1940, 2023.

[3] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.

[4] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt, Narmin Ghaffari Laleh, Chiara Maria Lavinia Löffler, Sophie-Caroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al. The future landscape of large language models in medicine. Communications Medicine, 3(1):141, 2023.

[5] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran Wang. Receive, reason, and react: Drive as you say with large language models in autonomous vehicles. arXiv preprint arXiv:2310.08034, 2023.

[6] OpenAI. Openai pricing. [Online], 2023. https://openai.com/pricing.

[7] Nerdynav. Up-to-date chatgpt statistics & user numbers [oct 2023]. [Online], 2023. https://nerdynav.com/chatgpt-statistics.

[8] AFZAL AHMAD DYLAN PATEL. The inference cost of search disruption - large language model cost analysis. [Online], 2023. https://www.semianalysis.com/p/the-inference-cost-of-search-disruption.

[9] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1–15. IEEE, 2022.

[10] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a single gpu. 2023.

[11] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611–626, 2023.

[12] Sensetime. Openppl: A high-performance deep learning inference platform. [Online], 2023. https://openppl.ai/home.

[13] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference. [Online], 2023. https://crfm.stanford.edu/2023/10/12/flashdecoding.html.

[14] Neal Vaidya, Fred Oh, and Nick Comly. Optimizing inference on large language models with nvidia tensorrt-llm, now publicly available. [Online], 2023. https://github.com/NVIDIA/TensorRT-LLM.

[15] Sensetime. A light and fast inference service for llm. [Online], 2023. https://github.com/ModelTC/lightllm.

[16] Text generation inference: Fast inference optimize for llms. [Online], 2023. https://github.com/huggingface/text-generation-inference/.

[17] Mlc llm: Machine learning compilation for large language models. [Online], 2023. https://github.com/mlc-ai/mlc-llm.

[18] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.

[19] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.

[20] Aaron Pham, Chaoyu Yang, Sean Sheng, Shenyang Zhao, Sauyon Lee, Bo Jiang, Fog Dong, Xipeng Guan, and Frost Ming. OpenLLM: Operating LLMs in production, June 2023.

[21] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.

[22] Z Dong, T Tang, L Li, and WX Zhao. A survey on long text modeling with transformers. arxiv 2023. arXiv preprint arXiv:2302.14502.

[23] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.

[24] NVIDIA. cublas: Basic linear algebra on nvidia gpus. [Online], 2017. https://developer.nvidia.com/cublas.

[25] NVIDIA. Cutlass: Cuda templates for linear algebra subroutines. [Online], 2017. https://github.com/NVIDIA/cutlass.

[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

[27] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10), pages 807–814, 2010.

[28] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.

[29] Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017.

[30] John Bridle. Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. Advances in neural information processing systems, 2, 1989.

[31] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.

[32] NVIDIA. Nvidia tensor core. [Online], 2023. https://www.nvidia.com/en-us/data-center/tensor-cores/.

[33] NVIDIA. Fastertransformer: About transformer related optimization, including bert, gpt. [Online], 2017. https://github.com/NVIDIA/FasterTransformer.

[34] Siping Wang. Fastgemv: High-speed gemv kernels. [Online], 2023. https://github.com/wangsiping97/FastGEMV.

[35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online, October 2020. Association for Computational Linguistics.

[36] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer language models, 2022.

[37] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320–335, 2022.

[38] NVIDIA. Nvidia tensorrt: An sdk for high-performance deep learning inference. [Online]. https://developer.nvidia.com/tensorrt.
