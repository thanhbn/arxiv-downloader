# 2303.06182.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/inference/2303.06182.pdf
# Kích thước tệp: 2997572 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Hướng tới Triển khai MoE: Giảm thiểu Tính không hiệu quả
trong Suy luận Mixture-of-Expert (MoE)
Haiyang Huang*†, Newsha Ardalani*, Anna Sun*, Liu Ke* ‡, Hsien-Hsin S. Lee*, Anjali Sridhar*, Shruti Bhosale*,
Carole-Jean Wu*, Benjamin Lee*$
∗Meta AI†Đại học Duke$Đại học Pennsylvania‡Đại học Washington tại St. Louis

Tóm tắt —Các mô hình Mixture-of-Experts (MoE) đã trở nên phổ biến trong việc đạt được hiệu suất tốt nhất trong một loạt các nhiệm vụ trong thị giác máy tính và xử lý ngôn ngữ tự nhiên. Chúng mở rộng dung lượng mô hình một cách hiệu quả trong khi chỉ tăng tối thiểu chi phí tính toán trong quá trình huấn luyện. Tuy nhiên, việc triển khai các mô hình như vậy để suy luận là khó khăn do kích thước lớn và mô hình giao tiếp phức tạp của chúng. Trong công trình này, chúng tôi cung cấp đặc tính của hai khối lượng công việc MoE, cụ thể là Mô hình hóa Ngôn ngữ (LM) và Dịch máy (MT) và xác định các nguồn gây ra tính không hiệu quả tại triển khai.

Chúng tôi đề xuất ba kỹ thuật tối ưu hóa để giảm thiểu các nguồn gây ra tính không hiệu quả, cụ thể là (1) Gating động, (2) Expert Buffering, và (3) Cân bằng tải expert. Chúng tôi chỉ ra rằng gating động cải thiện thông lượng tối đa 6.21-11.23 × cho LM, 5.75-10.98× cho MT Encoder và 2.58-5.71 × cho MT Decoder. Nó cũng giảm việc sử dụng bộ nhớ lên đến 1.36 × cho LM và lên đến 1.1 × cho MT. Chúng tôi tiếp tục đề xuất Expert Buffering, một cơ chế bộ nhớ đệm mới chỉ giữ các experts nóng, hoạt động trong bộ nhớ GPU trong khi đệm phần còn lại trong bộ nhớ CPU. Điều này giảm phân bổ bộ nhớ tĩnh lên đến 1.47 ×. Cuối cùng, chúng tôi đề xuất một phương pháp cân bằng tải cung cấp khả năng mở rộng bổ sung cho khối lượng công việc.

I. GIỚI THIỆU
Khả năng dự đoán của một mô hình học máy có mối tương quan mạnh với dung lượng mô hình, (tức là số lượng tham số trong mạng). Theo đuổi độ chính xác, dung lượng đã tăng với tốc độ theo cấp số nhân 10 lần mỗi năm [28], kèm theo nhu cầu cao hơn về tài nguyên tính toán và chi phí huấn luyện đắt đỏ. Các mạng nơ-ron được kích hoạt thưa thớt, như Mixture of Experts (MoE), là các kiến trúc mô hình hấp dẫn tách rời yêu cầu cho nhiều tham số khỏi chi phí tính toán. Trong một mô hình được kích hoạt thưa thớt, các phần của mạng được kích hoạt có điều kiện, điều này giảm chi phí huấn luyện. Kết quả từ các công trình trước [2], [7], [21], [22], [30], [34] cho thấy các mô hình MoE giảm chi phí huấn luyện nhưng vẫn cải thiện hiệu suất dự đoán mô hình trong các nhiệm vụ như mô hình hóa ngôn ngữ [2], [5], [7], [27], dịch máy [22] và nhận dạng hình ảnh [26], [33]. Trong khi huấn luyện đã được nghiên cứu tương đối kỹ lưỡng, việc triển khai và suy luận MoE đã nhận được ít sự chú ý hơn nhiều.

Việc đặc tính hóa và tối ưu hóa suy luận ngày càng trở nên quan trọng khi các mô hình ngôn ngữ lớn, như ChatGPT, được triển khai cho các dịch vụ sản xuất. Hình 1 và 2 làm nổi bật các khả năng dự đoán mô hình cũng như chi phí huấn luyện và suy luận liên quan giữa các kiến trúc mô hình MoE và dense tiên tiến. Trong Hình 1, các mô hình MoE đạt được cùng mức độ hiệu suất và chất lượng (tức là perplexity) với một nửa chi phí huấn luyện (GPU-days) so với các đối tác dense của chúng. Tuy nhiên, khi được triển khai để suy luận, các mô hình MoE chậm hơn 15 × cho các mô hình ngôn ngữ (LM) và hơn 3 × chậm hơn cho dịch máy (MT) so với đối tác dense tương đương FLOP của chúng, như được thể hiện trong Hình 2.

Một số chiến lược đã được đề xuất để giảm độ trễ suy luận MoE. Chúng ta có thể chưng cất các mô hình MoE thành các mô hình dense nhỏ hơn nhiều với số lượng FLOP tương tự [2], [7]. Mặc dù chưng cất giảm kích thước mô hình và độ trễ suy luận, nó cũng giảm chất lượng mô hình. Lepikhin và cộng sự cho thấy

†‡Công việc được thực hiện trong thời gian thực tập tại Meta

Hình 1. So sánh các Mô hình Ngôn ngữ MoE và Dense về chi phí huấn luyện và perplexity (perplexity càng thấp càng tốt về chất lượng mô hình). Các mô hình MoE có thể đạt được hiệu suất tốt hơn so với các đối tác dense của chúng với chi phí huấn luyện thấp hơn (Nguồn: Artetxe et. al. [2]).

Hình 2. So sánh các mô hình MoE và Dense về độ trễ suy luận nút đơn. Trong khi về mặt lý thuyết các mô hình MoE nên có thể suy luận với độ trễ tương tự như các đối tác dense tương đương flop của chúng, chúng tôi thấy rằng trong thực tế chúng chậm hơn 15 × cho Mô hình hóa Ngôn ngữ (LM), chậm hơn 22 × cho Machine Translation (MT) encoder và chậm hơn 3 × cho Machine Translation decoder.

1arXiv:2303.06182v2  [cs.DC]  18 Jun 2023

--- TRANG 2 ---
rằng một mô hình Switch Transformer 14.7 tỷ tham số chỉ giữ lại 29% lợi ích perplexity trong mô hình hóa ngôn ngữ sau khi chưng cất [21]. DeepSpeed-MoE và Tutel [16], [24] tập trung vào việc tăng tính song song và tối ưu hóa pipeline để tăng sử dụng phần cứng khi triển khai các mô hình MoE trên hàng trăm GPU. Những tối ưu hóa này có phạm vi hẹp và giảm thiểu tính không hiệu quả trong các kernel cụ thể cho các collective giao tiếp và tính toán GPU. Tuy nhiên, những nghiên cứu này thiếu phân tích toàn diện về độ trễ suy luận và bỏ qua tính không hiệu quả trong chính các thuật toán MoE.

Trong bài báo này, chúng tôi cung cấp các chiến lược tối ưu hóa cho việc triển khai MoE hiệu quả, giảm chi phí suy luận với tác động tối thiểu đến chất lượng mô hình. Đầu tiên, chúng tôi đặc tính hóa việc triển khai MoE Transformer trên ba trục quan trọng: độ trễ suy luận, sử dụng bộ nhớ và kích hoạt expert. Đặc tính hóa chi tiết của chúng tôi thiết lập các mối tương quan đáng kể giữa các mô hình kích hoạt expert và hiệu quả triển khai. Độ trễ và sử dụng bộ nhớ cao vì các kích hoạt expert rất thưa thớt và tải truy vấn rất mất cân bằng giữa các experts.

Thứ hai, chúng tôi phân tích các mô hình kích hoạt expert độc đáo để đề xuất một chính sách gating mới, được tối ưu hóa—được gọi là Dynamic Gating—và triển khai nó trên một Transformer dựa trên MoE mã nguồn mở, tiên tiến [23]. Đối với Mô hình hóa Ngôn ngữ (LM) và Dịch máy (MT) trên các tập dữ liệu và nhiệm vụ phụ khác nhau [8], [22], nguyên mẫu hệ thống của chúng tôi cho gating động cải thiện thông lượng suy luận 6.21-11.23 × cho LM, 5.75-10.98 × cho MT Encoder và 2.58-5.71 × cho MT Decoder bằng cách cho phép kích thước batch lớn hơn và độ trễ nhỏ hơn. Các chiến lược tối ưu hóa của chúng tôi bổ sung cho các tối ưu hóa được đề xuất trước đây về chưng cất, collective giao tiếp và kernel GPU. Khi tích hợp với các tối ưu hóa khác, chính sách gating của chúng tôi có thể đạt được lợi ích thậm chí còn lớn hơn.

Cuối cùng, chúng tôi xem xét kỹ hơn vào các mô hình kích hoạt expert, khám phá sự mất cân bằng đáng kể trong phân phối tải giữa các experts nhưng có tính cục bộ thời gian cao. Dựa trên hai quan sát quan trọng này, chúng tôi đề xuất Expert Buffering, cải thiện hiệu quả bộ nhớ bằng cách phân bổ một lượng bộ nhớ GPU cố định nhưng hạn chế cho các experts nóng và hoạt động và dựa vào bộ nhớ CPU để đệm tất cả các experts khác. Các experts được truy cập ít thường xuyên hơn được đưa vào bộ nhớ GPU khi cần thiết, giảm đáng kể nhu cầu bộ nhớ GPU. Expert buffering là trực giao với các kỹ thuật quản lý bộ nhớ hiện có, chẳng hạn như offloading. Các thí nghiệm của chúng tôi cho thấy expert buffering giảm sử dụng bộ nhớ tĩnh lên đến 1.47 × trên các nhiệm vụ thể hiện độ thưa thớt expert đáng kể. Để cân bằng tải, chúng tôi tiếp tục đề xuất cân bằng tải tiên nghiệm dựa trên dữ liệu kích hoạt expert lịch sử, và phân tích lợi ích của nó cho thông lượng.

Tóm lại, những đóng góp của chúng tôi trong bài báo này như sau:
• Chúng tôi cung cấp đặc tính hóa triển khai MoE toàn diện, xác định các nguồn gây ra tính không hiệu quả bằng cách phân tích độ trễ suy luận và sử dụng bộ nhớ trên các thành phần khác nhau của kiến trúc mô hình.
• Chúng tôi xác định hàm gating như một yếu tố đóng góp chính cho độ trễ cao và dấu chân bộ nhớ lớn của các mô hình MoE. Chúng tôi đề xuất một chính sách gating mới giảm đáng kể độ trễ và tiêu thụ bộ nhớ trong khi cũng cho phép suy luận với kích thước batch lớn hơn và số lượng GPU nhỏ hơn.
• Chúng tôi phân tích các mô hình kích hoạt expert trong quá trình suy luận và khám phá sự mất cân bằng đáng kể trong phân phối tải giữa các experts nhưng có tính cục bộ thời gian cao.
• Chúng tôi đề xuất Expert Buffering, một cơ chế bộ nhớ đệm mới chỉ giữ các experts nóng hoặc hoạt động trong bộ nhớ GPU và đệm phần còn lại trong bộ nhớ CPU. Các experts được truy cập ít thường xuyên hơn được đưa vào bộ nhớ GPU khi cần thiết. Tối ưu hóa này có thể giảm phân bổ bộ nhớ tĩnh trong GPU 1.47 ×.
• Chúng tôi đề xuất các kỹ thuật để cân bằng tải giữa các experts để cải thiện thêm việc sử dụng bộ nhớ và độ mạnh mẽ của hệ thống.

II. KIẾN THỨC NỀN TẢNG
A. Module Mixture-of-Experts
Việc sử dụng các mô hình khác nhau cho các đầu vào khác nhau từ lâu đã được thảo luận như một cách để cải thiện tính linh hoạt và mạnh mẽ của mô hình. Module Mixture-of-Experts (MoE) [29] là một ứng dụng thực tế của ý tưởng này cho các mạng nơ-ron. Một module MoE (Hình 3) bao gồm nhiều mô hình độc lập (được gọi là experts), và một hàm gating gán đầu vào cho mỗi expert. Mỗi đầu vào chỉ kích hoạt mạng expert được gán của nó, về mặt lý thuyết cho phép dung lượng mô hình (tức là số lượng tham số trong mô hình) mở rộng "một cách táo bạo" với mất mát hiệu quả tính toán tối thiểu.

B. Kiến trúc Mô hình Transformer
Kiến trúc Transformer đã trở nên phổ biến trong thị giác máy tính và xử lý ngôn ngữ tự nhiên bằng cách định nghĩa tiên tiến trên nhiều nhiệm vụ trong các lĩnh vực này [4], [32]. Từ trên xuống, một Transformer bao gồm một tokenizer phân tích đầu vào thành tokens, và một kiến trúc encoder-decoder bao gồm các lớp transformer dense. Cấu trúc encoder có N lớp transformer dense, trong đó N khác nhau từ một chữ số đến hàng chục trên các kiến trúc mô hình khác nhau. Mỗi lớp transformer dense được cấu thành từ hai khối: một khối multi-head attention (MHA), và một khối Feed-Forward Network (FFN) được kết nối bởi một kết nối dư, như được thể hiện trong Hình 3. Cấu trúc của decoder rất giống với encoder, ngoại trừ một lớp MHA tùy chọn tham gia vào đầu ra encoder.

C. Kiến trúc Mô hình MoE Transformer
MoE Transformer kết hợp ý tưởng MoE với kiến trúc Transformer. Ngoài lớp Transformer dense bình thường, nó giới thiệu một loại lớp mới với các MoE thưa thớt. Các lớp MoE thưa thớt thay thế khối FFN bằng một khối MoE bao gồm nhiều expert FFN khác nhau. Thay vì áp dụng một FFN duy nhất cho tất cả các token đầu vào, nó đầu tiên sử dụng một hàm gating để quyết định (các) expert nào phù hợp nhất cho mỗi token, và sau đó định tuyến các token đến expert tương ứng của chúng. Thông thường, một token được định tuyến đến một hoặc hai experts trong một chính sách được gọi là top-1 hoặc top-2 gating. Các lớp MoE thưa thớt

--- TRANG 3 ---
Hình 3. Hình ảnh hóa module MoE, lớp encoder transformer dense, lớp encoder transformer MoE và lớp encoder transformer MoE được triển khai với song song expert. MHA viết tắt của khối Multi-head attention, trong khi FFN viết tắt của khối Feed-forward Network. (a)Module MoE được giới thiệu trong [29] (b)Lớp Encoder Transformer Dense. Một lớp transformer dense điển hình bao gồm Multi-head Attention (MHA) theo sau là một lớp FFN. (c)Lớp MoE Transformer naïve. Khối FFN duy nhất trong transformer dense được thay thế bởi một tập hợp các FFN, được gọi là experts, hoạt động song song. Không phải tất cả tokens đều được xử lý bởi tất cả experts. Hàm gating quyết định experts nào sẽ nhận tokens nào.. (d)MoE Transformer với song song expert. Mỗi thiết bị chỉ giữ một tập con của tất cả experts. Các tokens được gán cho các expert FFN không cục bộ được gửi đến expert được gán của chúng thông qua một collective giao tiếp all-to-all. .

thay thế các lớp transformer dense một cách gián đoạn trong kiến trúc mô hình đa lớp.

Những sửa đổi này cấp cho mô hình nhiều bậc tự do hơn và mở rộng kích thước mô hình một cách hiệu quả. So với các mô hình Transformer truyền thống, nơi số lượng FLOP mỗi batch tỷ lệ tuyến tính với số lượng tham số, các mạng MoE đòi hỏi ít tính toán hơn nhiều, do đó cho phép các mô hình lớn được huấn luyện một cách hiệu quả. MoE Transformers đã thành công trong việc giảm chi phí huấn luyện của các mô hình transformer lớn [2], [5], [7], [21] và đạt được độ chính xác cao trong lĩnh vực thị giác, văn bản, giọng nói và học tập đa nhiệm vụ [11], [12], [20], [27], [35].

D. Song song Expert
So với các mô hình Transformer truyền thống có cùng dung lượng mô hình, các mô hình MoE cung cấp một sự đánh đổi thú vị. MoE đòi hỏi ít tính toán hơn nhiều nhưng sử dụng bộ nhớ nhiều hơn nhiều. Các lớp expert triển khai nhiều FFN bổ sung, làm tăng kích thước mô hình và nhu cầu liên quan về dung lượng bộ nhớ gần thiết bị tính toán (ví dụ: GPU). Để xử lý vấn đề này, GShard [21] đề xuất song song expert, phân phối khối lượng công việc trên nhiều thiết bị để giảm bộ nhớ và tính toán mỗi thiết bị.

Với song song expert, các lớp MoE được phân phối trên nhiều thiết bị. Mỗi thiết bị chỉ giữ một tập con của expert FFN và một bản sao của tất cả các tham số khác. Khi một token được gán cho các experts cư trú trên các thiết bị khác, một collective giao tiếp all-to-all gửi token đến các thiết bị tương ứng. Các tokens được xử lý bởi expert và sau đó được gửi lại bởi một giao tiếp all-to-all khác.

Ở mức song song expert tối đa, phân bổ một expert mỗi thiết bị, việc sử dụng bộ nhớ và số lượng FLOP mỗi thiết bị có thể so sánh với đó từ một mô hình transformer dense. Vì hàm gating là một lớp tuyến tính nhẹ, độ phức tạp tính toán tổng thể của một batch gần như giống với đó của một transformer dense với ít tham số hơn nhiều. Tuy nhiên, kích thước khổng lồ, kích hoạt thưa thớt của experts, và mô hình giao tiếp phức tạp giữa các thiết bị lưu trữ các experts khác nhau đặt ra những thách thức nghiêm trọng trong quá trình triển khai mô hình và suy luận.

III. ĐẶC TÍNH HÓA CỦA MÔ HÌNH MOE
Để đặc tính hóa khối lượng công việc của các mô hình MoE Transformer, chúng tôi nghiên cứu hai trường hợp sử dụng chính: Mô hình hóa Ngôn ngữ và Dịch máy. Mô hình hóa ngôn ngữ tạo ra xác suất một chuỗi đầu vào xuất hiện trong văn bản tự nhiên trong khi dịch máy ánh xạ đầu vào từ một ngôn ngữ sang ngôn ngữ khác. Cả hai nhiệm vụ đều là vấn đề cốt lõi của xử lý ngôn ngữ tự nhiên, và hiện tại là những ứng dụng chính của MoE Transformers. Chúng tôi chọn các mô hình trong các ấn phẩm gần đây đã đạt được tiên tiến làm testbed của chúng tôi. Chi tiết của các tập dữ liệu và mô hình có thể được tìm thấy trong Bảng I.

Các đối tác dense của mô hình MoE được chọn để tương đương FLOP, vì vậy chúng chia sẻ hầu hết các siêu tham số với MoE Transformers quan tâm bao gồm các chiều ẩn, số lượng lớp và các đầu attention. Sự khác biệt duy nhất là MoE Transformer thay thế lớp FFN bằng một lớp MoE mỗi MF lớp. Hệ số dung lượng C, một tham số độc đáo của MoE Transformer, kiểm soát có bao nhiêu tokens có thể được xử lý bởi một expert duy nhất. Theo thiết kế ban đầu, bất kể có bao nhiêu tokens được gán cho một expert, expert sẽ luôn xử lý một số lượng tokens bằng C nhân với độ dài chuỗi. Khi quá nhiều tokens được gán cho một expert duy nhất, các tokens dư thừa bị bỏ và không

--- TRANG 4 ---
Loại Nhiệm vụ Kích thước E MF CF
LMDense 355M – – –
MoE 52B 512 2 0.05
MTDense 3.3B – – –
MoE 54.5B 128 4 1
Loại Nhiệm vụ Lớp TD HD V ocab
LMDense 24 1024 4096 51200
MoE 24 1024 4096 51200
MTDense 48 2048 8192 256206
MoE 48 2048 8192 256206
Thông số kỹ thuật Nền tảng
CPU2×Intel Xeon E5-2698 v4 tại 2.2GHz
với bộ nhớ 700GB
CPU-GPU 16GB/s qua PCIe 3.0
GPU8×NVIDIA Tesla V100, với 5120 lõi CUDA,
bộ nhớ HBM2 32GB tại 900GB/s
được kết nối bởi NVLink tại 300GB/s
BẢNG I
THIẾT LẬP THÍ NGHIỆM. LM: MÔ HÌNH HÓA NGÔN NGỮ. MT: DỊCH MÁY.
E: SỐ LƯỢNG EXPERTS. MF: TẦN SUẤT LỚP MOE.
CF: HỆ SỐ DUNG LƯỢNG. TD: CHIỀU TOKEN. HD: CHIỀU ẨN.
VOCAB: KÍCH THƯỚC TỪ VỰNG. E, MF VÀ CF KHÔNG ÁP DỤNG
CHO CÁC MÔ HÌNH DENSE.

được xử lý bởi bất kỳ expert nào. Khi quá ít tokens được gán, dung lượng không sử dụng sẽ được lấp đầy bởi các số không. Chúng tôi sử dụng các thiết lập hệ số dung lượng được khuyến nghị bởi [2], [22]. Bảng I chi tiết thiết lập thí nghiệm.

A. Độ trễ và Tiêu thụ Bộ nhớ
Các chỉ số quan trọng nhất trong triển khai mô hình học máy là thời gian thực thi (tức là độ trễ) và tiêu thụ bộ nhớ. Độ trễ ngắn hơn đảm bảo phản hồi kịp thời hơn từ dịch vụ, trong khi tiêu thụ bộ nhớ thấp hơn cho thấy sử dụng tài nguyên thấp hơn và tiềm năng chứa kích thước batch lớn hơn. Trong phần này, chúng tôi so sánh hiệu suất suy luận MoE dọc theo hai trục này. Kích thước mini batch được đặt là 8 cho mô hình hóa ngôn ngữ và 48 cho dịch máy. Chúng tôi sử dụng một mô hình dense có FLOP tương tự làm đường cơ sở để so sánh.

Độ trễ. Hình 2 cho thấy độ trễ và tiêu thụ bộ nhớ của MoE Transformers quan tâm và của các đối tác dense của chúng. Mặc dù về mặt lý thuyết, MoE Transformers thực thi một số lượng FLOP tương tự so với các mô hình dense cơ sở, trong thực tế chúng chậm hơn đáng kể. Đối với Mô hình Ngôn ngữ, mô hình dense yêu cầu 74.2ms, trong khi MoE Transformer yêu cầu hơn 1.09s. Đối với Dịch máy, mô hình dense thực thi encoder và decoder trong 101ms và 32ms, tương ứng, nhưng MoE Transformer yêu cầu 2.26s và 90ms.

Hình 5 phân tích độ trễ dưới các kịch bản khác nhau. Khoảng cách độ trễ trước đây được quy cho collective giao tiếp all-to-all thường xuyên trong các mô hình MoE. [21] Trong khi các collective all-to-all thực sự tăng độ trễ dưới triển khai đa nút, chúng tôi lưu ý rằng đây không phải là nguồn duy nhất của độ trễ.

Hình 4. So sánh dấu chân bộ nhớ mô hình MoE vs Dense trong quá trình suy luận. Các mô hình MoE yêu cầu sử dụng bộ nhớ nhiều hơn đáng kể khi được triển khai trên GPU. Bên cạnh việc tiêu thụ bộ nhớ lớn do dung lượng mô hình mở rộng (được giới thiệu bởi các tham số expert), nó cũng yêu cầu bộ nhớ nhiều hơn cho kích hoạt. (kết quả cho batchsize=48 cho MT, và batchsize=8 cho LM. Lưu ý rằng đây là các kích thước batch lớn nhất có thể chạy dưới triển khai cơ sở.)

Hình 5. Phân tích độ trễ Mô hình MoE. Bên cạnh giao tiếp all-to-all, các thành phần khác của mô hình, chẳng hạn như hàm gating và thực thi expert, cũng không hiệu quả. Chi phí giao tiếp tăng đáng kể khi có hơn một nút tham gia.(Kết quả cho batch size=8 cho LM và batch size=48 cho MT).

Trong Phần III-B, chúng tôi sẽ thảo luận về những nguồn độ trễ bổ sung này.

Bộ nhớ. Chúng tôi cũng quan sát thấy sự gia tăng lớn trong tiêu thụ bộ nhớ cho các mô hình MoE (xem Hình 4). Đối với LM, mô hình dense chỉ yêu cầu 2.2GB trên mỗi GPU trong khi mô hình MoE yêu cầu 18.88GB ở đỉnh, tăng 8.58×. Đối với MT, các mô hình dense và MoE sử dụng 7.02GB và 21.16GB, tương ứng, tăng 3.01 ×.

Chúng tôi thực hiện phân tích chi tiết bằng cách tách biệt việc sử dụng bộ nhớ tĩnh và động. Tiêu thụ bộ nhớ tĩnh đề cập đến bộ nhớ được phân bổ cho các tham số mô hình, trong khi tiêu thụ bộ nhớ động đề cập đến bộ nhớ được phân bổ theo yêu cầu, thường bởi các kích hoạt mạng. Do thực tế là mỗi GPU chứa hơn một expert trong quá trình suy luận, sự gia tăng trong bộ nhớ tĩnh là mong đợi. Tuy nhiên, chúng tôi quan sát thấy rằng tiêu thụ bộ nhớ động đỉnh cũng tăng đáng kể trong cả hai trường hợp, điều này đáng ngạc nhiên.

B. Tính không hiệu quả của Gating Tĩnh
Điều gì đằng sau chi phí chính trong độ trễ và tiêu thụ bộ nhớ? Một kiểm tra chi tiết về phân tích độ trễ làm sáng tỏ vấn đề này. Trong khi collective giao tiếp all-to-all đóng một vai trò quan trọng trong các kịch bản đa nút, các thành phần khác, chẳng hạn như hàm gating và thực thi expert, cũng đóng góp tính không hiệu quả đáng kể. Hơn nữa, một phân tích kỹ về dấu vết bộ nhớ cho thấy rằng phân bổ bộ nhớ xảy ra trong các giai đoạn gating và sắp xếp lại. Nguồn gốc của tính không hiệu quả trong các thành phần này đáng được điều tra thêm.

Nguyên nhân gốc rễ của chi phí hiệu suất và tài nguyên nằm ở chính sách gating tĩnh. Các triển khai gần đây [2], [18], [24], [27] của các mô hình MoE Transformer thường giả định số lượng tokens được gán cho mỗi expert là gần như giống nhau vì hàm mất mát trong quá trình huấn luyện tính đến cân bằng tải. Kết quả là, quá trình phân phối token được đơn giản hóa thành một collective all-to-all phân phối cùng số lượng tokens (xem Hình 8(a)).

Hệ số Dung lượng C định nghĩa số lượng tokens được xử lý bởi mỗi expert trong một batch. Nếu hàm gating gán ít tokens hơn dung lượng của một expert, phần còn lại của dung lượng sẽ được lấp đầy bởi placeholders (tức là các vectơ không). Nếu hàm gating gán nhiều tokens hơn dung lượng của một expert, các tokens dư thừa bị bỏ bởi expert và thông tin của chúng sẽ chỉ được giữ lại bởi kết nối dư. Token drop là không mong muốn vì nó gây hại độ chính xác. Để tránh mất thông tin và biến động độ chính xác, hệ số dung lượng C thường được đặt ở các giá trị cao trong quá trình suy luận. Trong khi điều này bảo vệ độ chính xác, nó tăng độ trễ và chi phí bộ nhớ.

Các Yếu tố Lãng phí. Đối với Mô hình Ngôn ngữ, nơi số lượng experts E = 512 và Hệ số Dung lượng C = 0.05, số lượng tokens được xử lý bởi một expert trong một chuỗi S là ECS = 512×0.05×S = 25.6S. Lượng tính toán mà thiết bị thực sự phải thực hiện, thay vào đó, chỉ là 2S vì mô hình triển khai top-2 gating và mỗi token sẽ được xử lý bởi hai experts. Do đó, yếu tố lãng phí là 25.6S/2S = 12.8.

Đối với Dịch máy, phân tích là tương tự. Số lượng tokens được xử lý bởi mỗi expert là ECS = 128 ×1×S = 128S. Tuy nhiên, lượng tính toán mà thiết bị thực sự phải thực hiện cũng là 2S, dẫn đến yếu tố lãng phí 128S/2S = 64. Yếu tố lãng phí khổng lồ cho thấy rằng các mô hình MoE điển hình thực hiện một lượng lớn tính toán và giao tiếp dư thừa cũng như tiêu thụ một lượng lớn bộ nhớ bổ sung.

Câu hỏi của chúng tôi là liệu việc sử dụng tài nguyên được cung cấp quá mức này có thể tránh được hay không. Nếu khối lượng công việc được cân bằng tốt sao cho phân bổ token giữa các experts có thể so sánh được, lãng phí tài nguyên có thể được giảm bằng cách đơn giản thu nhỏ Hệ số Dung lượng. Mặt khác, nếu kích hoạt expert thưa thớt, việc thu nhỏ hệ số dung lượng không phải là một lựa chọn vì làm như vậy tăng cơ hội bỏ tokens và gây hại độ chính xác mô hình.

IV. PHÂN TÍCH CÁC MÔ HÌNH KÍCH HOẠT EXPERT
Để hiểu liệu yếu tố lãng phí khổng lồ như vậy có cần thiết cho tính ổn định dịch vụ hay không, chúng tôi sẽ nghiên cứu các mô hình kích hoạt expert trên hai ứng dụng của các mô hình MoE, cụ thể là mô hình hóa ngôn ngữ và dịch máy. Hơn nữa, chúng tôi đề xuất hai tối ưu hóa (gating động và expert caching) để giảm yếu tố lãng phí và cải thiện độ trễ và tiêu thụ bộ nhớ.

A. Trường hợp Mô hình hóa Ngôn ngữ
Đối với Mô hình hóa Ngôn ngữ, chúng tôi sử dụng tập dữ liệu PILE [8] làm đầu vào, đây là tập xác thực được sử dụng trong công trình trước [2]. Chúng tôi chọn ba lĩnh vực (Wikipedia, PubMed và Github) từ tập dữ liệu PILE để nghiên cứu ảnh hưởng của dữ liệu đầu vào khác nhau lên các mô hình kích hoạt expert theo thời gian (tức là các batch liên tiếp).

Chúng tôi hình ảnh hóa kết quả trong Hình 6. Mỗi hàng đại diện cho một batch và mỗi cột đại diện cho tải của một expert cụ thể. Màu sắc cường độ hơn cho thấy expert nhận được tỷ lệ cao hơn của tất cả tokens trong một batch. Như được thể hiện trong Hình 6(a), phân phối tải giữa các experts rất mất cân bằng. Tồn tại nhiều experts nóng luôn nhận được một phần lớn tokens (nhiều dòng màu cường độ), và các experts khác liên tục nhận được một lượng nhỏ tokens (các dòng màu nhạt hơn).

Trong các trường hợp cực đoan nhất, Hình 7 cho thấy tồn tại các experts không bao giờ nhận được bất kỳ tokens nào. Do chính sách gating tĩnh, những experts này vẫn nhận và xử lý các placeholders token trống, giới thiệu một lãng phí lớn tài nguyên tính toán. Như được thể hiện trong Hình 6(a) và 7, tập hợp các experts nóng và mức độ nóng của chúng khác nhau giữa các lĩnh vực mặc dù tất cả các lĩnh vực đều liên tục thể hiện mức độ cao kích hoạt expert thưa thớt.

B. Trường hợp Dịch máy
Đối với Dịch máy (MT), chúng tôi sử dụng tập dữ liệu xác thực gốc NLLB-200 [22]. Chúng tôi sử dụng tiếng Anh làm ngôn ngữ nguồn, và chọn ba ngôn ngữ đích khác nhau (Pháp, Nhật và Asturian). Kích hoạt expert trên MT cho các lớp được chọn ngẫu nhiên được hình ảnh hóa trong Hình 6(b).

Các mô hình Dịch máy cũng thể hiện mất cân bằng tải và một phần nhỏ các experts nóng hơn những cái khác, và mất cân bằng tải thậm chí còn rõ rệt hơn. Một số experts trên cả encoder và decoder đã nhận được một phần lớn của tất cả tokens gần như một nửa của toàn bộ batch, trong khi nhiều experts duy trì mức độ kích hoạt thấp.

Chúng tôi tiếp tục kiểm tra liệu độ thưa thớt expert có tồn tại trên encoder và decoder của mô hình hay không. Hình 7 thể hiện mức độ thưa thớt expert trên encoder và decoder trên cả ba nhiệm vụ. Chúng tôi thấy rằng kích hoạt encoder chủ yếu là dense, rằng hầu hết các experts được kích hoạt mọi lúc. Kích hoạt decoder cực kỳ thưa thớt (khoảng 75%).

Chúng tôi hình ảnh hóa mô hình kích hoạt được chọn của encoder và decoder trong Hình 6(b). Kích hoạt được chuẩn hóa trong một batch, và cường độ màu sắc là thước đo cường độ tải, đại diện cho tỷ lệ phần trăm tokens được gán cho mỗi expert trong một batch. Kích hoạt chi tiết cho thấy rằng mô hình kích hoạt expert trong dịch máy tương tự trên các ngôn ngữ khác nhau. Kiến trúc encoder nắm bắt các thuộc tính ngôn ngữ nguồn giống nhau trên cả ba nhiệm vụ (tiếng Anh). Thật ngạc nhiên, chúng tôi thấy

--- TRANG 5 ---
Hình 6. Hình ảnh hóa mô hình kích hoạt expert trên lớp được chọn của (a) mô hình hóa ngôn ngữ và (b) dịch máy. Kích hoạt được chuẩn hóa. Mô hình kích hoạt expert thể hiện sự mất cân bằng mạnh trên tất cả các nhiệm vụ, và sự mất cân bằng là nhất quán. Cụ thể, trên decoder dịch máy, độ thưa thớt là khổng lồ, và expert cũng thể hiện mối tương quan thời gian mạnh.

Hình 7. Số lượng experts không hoạt động trung bình trên Mô hình hóa Ngôn ngữ và Dịch máy. Hầu hết, nếu không muốn nói là tất cả experts được kích hoạt trong suốt LM và MT encoder. Tuy nhiên, kích hoạt trên MT decoder cực kỳ thưa thớt, ngay cả khi chúng tôi sử dụng kích thước batch 96 dưới chính sách gating động.

rằng kích hoạt expert ít nhiều tương tự trên các ngôn ngữ đích khác nhau cũng như các kiến trúc decoder.

Một kiểm tra kỹ hơn về kích hoạt expert trên decoder cho thấy rằng độ thưa thớt expert có tính cục bộ thời gian mạnh. Màu cường độ đại diện cho tải cao của expert thường xuất hiện dưới dạng các dòng, gợi ý rằng một expert hoạt động trên các batch liên tiếp. Điều này ngụ ý tính cục bộ thời gian cho các experts nóng. Quan sát này là động lực chính cho expert caching được thảo luận trong Phần VI.

V. TỐI ƯU HÓA GATING ĐỘNG
Các mô hình kích hoạt quan sát được thể hiện một khoảng cách rõ rệt giữa các giả định trong thiết kế hệ thống và hiệu suất suy luận. Việc tăng naively dung lượng expert có thể vẫn không ngăn chặn token overflow cho một số experts, nhưng sẽ tạo ra sự dư thừa và lãng phí bổ sung cho các experts khác. Trong khi các nghiên cứu trước cũng nhận thấy kích hoạt mất cân bằng giữa các experts [16], [24], các giải pháp hiện có vẫn giữ lại chính sách gating tĩnh, tăng CF khi sự mất cân bằng nghiêm trọng xuất hiện [16]. Kết luận của chúng tôi là gating tĩnh tăng lãng phí tài nguyên và dung lượng expert cố định không phải là giải pháp tối ưu cho việc phân phối

Hình 8. So sánh giữa gating tĩnh trong [2], [21] và triển khai gating động của chúng tôi. Để đơn giản, chúng tôi giả định E=3, S=6, C=0.5 và top-1 gating trong ví dụ này. Hình dạng của tensors được ghi lại trong dấu ngoặc đơn. (a) Gating Tĩnh. Dưới chính sách gating tĩnh, mỗi expert luôn xử lý một lượng tokens được định trước, có thể dẫn đến token overflow hoặc tokens trống. Xem Phần III-B để biết chi tiết. (b) Gating Động. Dưới chính sách gating động, mỗi expert chỉ xử lý các tokens được gán cho nó. Cơ chế phân phối token được đơn giản hóa với độ phức tạp ít hơn, và giao tiếp và tính toán được giảm.

tokens cho experts. Các ràng buộc được áp đặt bởi dung lượng tĩnh nên được loại bỏ và hàm gating nên là động.

Tuy nhiên, việc thay đổi chính sách gating để cho phép kích thước động cho experts là không đơn giản. Các triển khai chính, hiện có [2], [16], [26] không hỗ trợ động. Chúng dựa vào dung lượng tĩnh để đảm bảo rằng kích thước thông điệp của các collective all-to-all giống nhau, điều này đơn giản hóa giao tiếp.

--- TRANG 6 ---
A. Lý do cho Gating Động
Hình 8(a) hình ảnh hóa chính sách gating tĩnh. Trong ví dụ này, chúng tôi giả định độ dài chuỗi (tổng số tokens trong một batch câu) S = 6, số lượng experts E = 3, hệ số dung lượng C = 0.5 sao cho dung lượng tĩnh là S×C = 3. Chúng tôi giả định top-1 gating sao cho mỗi token chỉ được gán cho một expert.

Sau khi hàm gating tạo ra quyết định gating, chính sách gating tĩnh dịch nó thành E dispatch masks, mỗi cái có kích thước (S,(S×C)). Dispatch mask được tạo ra như sau. Nếu token i được gán cho expert e, thì quá trình sẽ kiểm tra xem mask thứ e có còn dung lượng hay không. Nếu có, cột thứ i của hàng trống đầu tiên sẽ được đánh dấu là 1, trong khi các số khác được giữ là 0. Quá trình này cho chúng ta một dispatch mask thưa thớt là một tensor có kích thước (E, S, (S×C)), trong đó nhiều nhất S entries là 1 do có thể bỏ token. Lưu ý rằng ma trận này rất thưa thớt. Các tokens đầu vào sẽ được nhân với dispatch mask để sắp xếp lại đầu vào thành E tập hợp đầu vào, mỗi tập có S×C tokens. Mỗi tập hợp đầu vào sẽ được gửi đến thiết bị expert được gán của nó.

Hình 8(b) cho thấy cách hàm gating phải được thiết kế lại khi số lượng tokens được chuyển giữa các thiết bị là biến thiên. Triển khai của chúng tôi đơn giản hóa quá trình phân phối. Chúng tôi tìm một mảng các chỉ số có thể sắp xếp mảng bằng cách thực hiện một argsort. Tập hợp chỉ số tối ưu này chuẩn bị thứ tự của tokens để gửi đến các thiết bị. Bằng cách đếm số lần xuất hiện của mỗi expert, chúng tôi biết kích thước chính xác của đầu vào được gửi đến mỗi thiết bị.

Vì kích thước của đầu vào được gửi là biến thiên [13], chúng tôi áp dụng phương pháp hai bước. Đầu tiên, chúng tôi sử dụng một collective all-to-all để thông báo cho mỗi thiết bị kích thước của các tokens đến. Thứ hai, chúng tôi sử dụng một collective all-to-all khác để thực hiện chuyển token thực. Collective all-to-all kích thước được khởi chạy ngay khi kích thước được biết, tối đa hóa sự chồng chéo với các kernel khác. Trong khi đó, các đầu vào được sắp xếp lại dựa trên chỉ số tối ưu cho mỗi token và đầu vào được sắp xếp lại sẽ được phân tách dựa trên kích thước đầu vào.

Dispatch yêu cầu một sort của O(SlogS), một bin-count của O(S), và một thao tác indexing của O(SD). Độ phức tạp tổng thể O(SD+SlogS) nhỏ hơn nhiều so với nhân ma trận batch có kích thước O(S2EDC). Chi phí bổ sung là khiêm tốn và chỉ là một collective all-to-all bổ sung có kích thước thông điệp tối thiểu. Gating động của chúng tôi đảm bảo token dropping sẽ không xảy ra, cải thiện độ mạnh mẽ của mô hình. Gating động của chúng tôi cũng đảm bảo rằng không có placeholders trống nào sẽ được chuyển giữa các thiết bị, loại bỏ lãng phí trong phân bổ bộ nhớ của đầu vào được sắp xếp lại và khối lượng giao tiếp.

Sau khi tất cả tokens được xử lý bởi các experts được gán của chúng, chúng được thu thập thông qua một collective giao tiếp all-to-all khác, gửi đến thiết bị gốc của chúng, và được khôi phục về thứ tự ban đầu. Điều này thường được triển khai bằng cách sử dụng nhân ma trận batch (BMM) nhưng, như trong giai đoạn đầu tiên, BMM có thể được thay thế bằng một thao tác indexing giảm độ phức tạp.

B. Giảm Độ trễ và Sử dụng Bộ nhớ
Chúng tôi nghiên cứu tác động của chính sách gating động lên thời gian thực thi và sử dụng bộ nhớ trên các trường hợp sử dụng LM và MT MoE. Chúng tôi giữ cấu hình máy giống như trước để so sánh công bằng. Vì chính sách gating động giới thiệu mất cân bằng khối lượng công việc giữa các GPU khác nhau, cho mỗi trường hợp, chúng tôi nghiên cứu tác động của các tập dữ liệu và nhiệm vụ khác nhau lên hiệu suất của mô hình. Thí nghiệm được thực thi bằng cách chuyển tiếp mười epoch độc lập trên mỗi subset/subtask và ghi lại thông lượng trung bình và tiêu thụ bộ nhớ cho mỗi batch trên các thí nghiệm.

Hình 9 so sánh tác động của các chính sách gating khác nhau lên thông lượng. Kết quả về Expert Buffering và Load Balancing sẽ được giải thích trong Phần VI và VII, tương ứng. Chính sách gating động của chúng tôi tăng đáng kể thông lượng trên tất cả kích thước batch và nhiệm vụ, so với đường cơ sở và Tutel gating [16]. Bằng cách loại bỏ dispatch mask lớn, gating động cũng cho phép kích thước batch lớn hơn dưới cùng lượng tài nguyên. Điều này cải thiện thông lượng lên đến 6.21×/3.32× khi so sánh với static/Tutel gating dưới LM nút đơn, lên đến 5.75×/5.33× dưới MT Encoder, và lên đến 2.58×/1.88× dưới MT decoder.

Vì gating động loại bỏ yếu tố lãng phí bằng cách giảm khối lượng giao tiếp, nó cũng giảm chi phí giao tiếp và cải thiện thông lượng. Do đó, lợi ích của chính sách gating động mở rộng khi mô hình được triển khai trên nhiều nút, nơi chi phí giao tiếp nổi bật hơn. Gating động cải thiện thông lượng lên đến 11.55 ×, 10.98 ×, 5.71× trong LM, MT Encoder, và MT decoder, tương ứng, khi so sánh với gating tĩnh.

Hình 10 tóm tắt ảnh hưởng của các chính sách gating khác nhau lên tiêu thụ bộ nhớ, sử dụng các trường hợp nút đơn làm ví dụ. Gating động cho phép kích thước batch lớn hơn, chạy thậm chí nhanh hơn, khi so sánh với chính sách gating tĩnh với kích thước batch nhỏ hơn. Gating động giảm dấu chân bộ nhớ bằng cách loại bỏ dispatch mask, và cũng giảm phân bổ bộ nhớ lãng phí cho các padding và placeholders trống. Kết quả là, bộ nhớ được phân bổ cho kích hoạt (màu sáng trong hình) cho LM với kích thước batch 8 giảm từ 6.29GB xuống 1.28 GB, đây là mức giảm 79.6%. Đối với MT với batchsize 8, phân bổ bộ nhớ giảm từ 1.89GB xuống 1.05 GB, đây là mức giảm 44.2%. Giảm tiêu thụ bộ nhớ cũng cho phép kích thước batch lớn hơn được phân bổ dưới cùng cấu hình máy. Phiên bản gating động cho phép kích thước batch 64 cho LM và 96 cho MT, lớn hơn 8 × và 2× so với kích thước batch được phép trên các đối tác tĩnh.

VI. EXPERT BUFFERING
Mặc dù gating động giảm lãng phí trong tính toán và phân bổ bộ nhớ động liên quan đến hàm gating, việc sử dụng bộ nhớ tĩnh liên quan đến số lượng lớn các tham số MoE vẫn đặt gánh nặng lớn lên bộ nhớ GPU tại triển khai. Độ thưa thớt cao trong mô hình kích hoạt expert thúc đẩy chúng tôi điều tra xem có cách nào để giảm sử dụng bộ nhớ bằng cách loại bỏ các experts nhàn rỗi hay không.

--- TRANG 7 ---
Hình 9. So sánh thông lượng của các chính sách gating khác nhau trong các mô hình MoE, bao gồm gating tĩnh (đường cơ sở), Tutel gating [16], chính sách gating động của chúng tôi (Phần V), gating động với Load Balancing (LB, Phần VII), gating động với Expert Buffering (EB, Phần VI), và tất cả tối ưu hóa kết hợp. Các thanh thiếu đại diện cho các trường hợp không khả thi dưới chính sách và kích thước batch tương ứng. Ví dụ Tutel không thể hỗ trợ vượt quá batch size=32 cho LM-1 Node. Gating động giảm sử dụng bộ nhớ và kích thước thông điệp trong giao tiếp, cho phép kích thước batch lớn hơn và thời gian xử lý nhanh hơn đáng kể so với gating tĩnh. Expert buffering đánh đổi độ trễ để có sử dụng bộ nhớ nhỏ hơn trong khi vẫn đạt được thông lượng cao hơn trên MT Decoder. Load balancing cải thiện thêm độ trễ khi kết hợp với gating động và expert buffering. Lưu ý rằng load balancing chỉ có ý nghĩa trong bối cảnh gating động nơi mỗi expert nhận được số lượng tokens khác nhau. Load balancing đặc biệt nổi bật dưới thiết lập đa nút hoặc kết hợp với expert buffering vì nó có thể cải thiện tỷ lệ cache miss (Xem Hình 14).

A. Kích hoạt Expert Thưa thớt
Điều tra của chúng tôi trong mô hình kích hoạt expert cho thấy rằng mặc dù, trong mỗi batch, tồn tại một số experts không hoạt động, tất cả experts đã được kích hoạt vài lần theo thời gian và batches. Loại bỏ các experts không hoạt động thường xuyên có thể gây hại độ chính xác mô hình. Tuy nhiên, chúng ta có thể offload các experts được truy cập ít thường xuyên hơn đến bộ nhớ CPU và sử dụng bộ nhớ GPU cho các experts nóng và hoạt động.

Chúng tôi đề xuất cơ chế expert buffering để khai thác độ thưa thớt expert và giảm phân bổ bộ nhớ tĩnh. Hình 11 minh họa cơ chế, giảm tiêu thụ bộ nhớ tĩnh bằng cách offload các tham số expert đến bộ nhớ CPU. Vì CPU chậm hơn nhiều so với GPU cho nhân ma trận, chúng tôi chỉ sử dụng bộ nhớ CPU để giữ các experts nhưng không offload tính toán. Chúng tôi sử dụng bộ nhớ GPU để cache các experts hoạt động và thực hiện tính toán.

B. Quản lý Cache
Trong quá trình suy luận, dưới gating động, một khi quyết định gating được thực hiện bởi hàm gating, mỗi GPU nhận được số lượng tokens được gán cho các experts của nó. Nếu một expert nhận được số lượng token dương, nó được coi là hoạt động cho batch hiện tại. Quá trình sau đó kiểm tra xem expert hoạt động đã được cache trong bộ nhớ GPU chưa. Nếu không, thì quá trình sẽ khởi chạy một Memcopy để chuyển các tham số expert cần thiết vào cache. Việc sao chép các tham số expert từ bộ nhớ CPU đến DRAM GPU sẽ được khởi chạy song song với giao tiếp all-to-all, để cho phép chồng chéo chuyển dữ liệu và ẩn độ trễ.

Trong các trường hợp cache đã đầy nhưng cần thêm experts, eviction sẽ được kích hoạt để tạo không gian cho các experts mới. Chính sách eviction được thiết kế như sau. Đầu tiên, chúng tôi sẽ đầu tiên evict các experts không hoạt động trong batch này vì chúng cũng ít có khả năng được sử dụng trong tương lai do tính cục bộ thời gian. Tiếp theo, chúng tôi sẽ evict các tham số expert dưới chính sách Last In, First Out (LIFO).

Lý do áp dụng chính sách LIFO có nguồn gốc từ triển khai của MoE Transformers gần đây. Nếu nhiều experts được phân bổ cho một GPU duy nhất, MoE Transformer sẽ thực thi các experts tuần tự theo thứ tự tăng dần của id của chúng. Xem xét một ví dụ nhỏ về E = 4 experts và kích thước cache 2 experts, và giả định expert (1, 2, 3) cần thiết. Sau giai đoạn 1, expert 1 và 2 sẽ được đẩy vào cache, và chúng ta cần evict một trong số chúng để load expert 3. Bằng cách evict expert 2 thay vì 1, chúng tôi đảm bảo expert với khoảng cách tái sử dụng ngắn nhất được giữ trong cache.

--- TRANG 8 ---
Hình 10. So sánh tiêu thụ bộ nhớ giữa các mô hình MoE dưới chính sách gating tĩnh và động. Bóng mờ nhạt đại diện cho phân bổ bộ nhớ động (bộ nhớ kích hoạt). Bóng mờ đậm đại diện cho phân bổ bộ nhớ tĩnh (tham số mô hình). Các thanh thiếu trong mỗi biểu đồ nắm bắt các trường hợp không khả thi dưới chính sách tương ứng. So với Static và Tutel [16], Dynamic Gating giảm sử dụng bộ nhớ, do đó cho phép kích thước batch lớn hơn. Expert Buffering tiếp tục giảm tiêu thụ bộ nhớ của các tham số mô hình.

Hình 11. Minh họa cơ chế Expert Buffering. Chúng tôi di chuyển các tham số expert đến bộ nhớ CPU để giảm gánh nặng trên bộ nhớ GPU. Trên bộ nhớ GPU, chúng tôi chỉ phân bổ không gian cho một vài entries expert để đệm các experts hoạt động hoặc nóng. (1) Trong quá trình suy luận, thông điệp kích thước all-to-all được gửi trong giai đoạn 1 như được thể hiện trong Hình 8(b) báo hiệu experts nào nằm trong thiết bị hiện tại đang hoạt động. (2) Sau đó expert cache sẽ kiểm tra xem các experts hoạt động hiện tại có cư trú trong buffer hay không. (3a) Nếu tìm thấy (cache hit), các tham số trong expert buffer sẽ được sử dụng để xử lý các tokens. (3b) Nếu không tìm thấy (cache miss), thì các tham số expert sẽ được yêu cầu từ bộ nhớ CPU. Số lượng cache entries trên bộ nhớ GPU là một tham số có thể điều chỉnh để điều chỉnh cho việc sử dụng bộ nhớ GPU và độ trễ mong muốn (Xem Phần VI).

C. Tỷ lệ Cache Miss
Để ước tính tính khả thi kỹ thuật của Expert Buffering, chúng tôi tính toán tỷ lệ cache miss cho trường hợp sử dụng dịch máy trên các lớp thể hiện độ thưa thớt expert khổng lồ. Chúng tôi lưu ý rằng cache được triển khai mỗi thiết bị, và mỗi thiết bị cache các experts đã được gán cho nó. Kết quả là, chúng tôi có thể thay đổi kích thước cache

Hình 12. Tỷ lệ Cache Miss Trường hợp Xấu nhất thu được từ các dấu vết kích hoạt expert từ các decoder MT. Chúng tôi điều chỉnh kích thước cache mỗi GPU từ 1 đến 16, và kiểm tra tác động của kích thước cache lên Tỷ lệ Cache Miss trường hợp xấu nhất trên các lớp khác nhau của decoder MT trên các nhiệm vụ khác nhau. (a)hiệu suất caching với/không có bất kỳ reassignment nào. (b)hiệu suất caching so với Belady's MIN tối ưu lý thuyết. Tỷ lệ miss được giảm thêm bởi load balancing, và rất gần với Belady's MIN (Xem Phần VII).

từ 1 đến 16 experts, tạo ra tiết kiệm 0-32.2% trên tổng phân bổ bộ nhớ tĩnh. Chúng tôi tính toán tỷ lệ cache miss toàn cục dưới mỗi hoàn cảnh. Tỷ lệ cache miss trường hợp xấu nhất được thể hiện trong Hình 12. Chúng tôi nhận thấy Tỷ lệ Cache Miss bắt đầu giảm nhanh hơn khi kích thước cache lớn hơn 5 mỗi GPU, đây là kích thước cache 40 tổng cộng. Kết quả này phù hợp với quan sát trước của chúng tôi rằng sẽ có hơn 90 experts trống bên trong decoder. Chúng tôi tiếp tục so sánh chính sách caching của chúng tôi với Belady's MIN, chính sách tối ưu lý thuyết yêu cầu thông tin từ tương lai. Hình 12(b) cho thấy rằng chính sách LIFO tốt hơn FIFO, và với load balancing được giới thiệu trong Phần VII, chính sách của chúng tôi có thể đạt được tỷ lệ cache miss rất gần với Belady's MIN.

Để đánh giá tác động của cơ chế được đề xuất, chúng tôi thực hiện thí nghiệm về Expert Buffering trên MT Decoder. Kích thước cache được chọn khoảng 80 experts tổng cộng, phù hợp với 10 experts mỗi GPU dưới trường hợp nút đơn. Đây là điểm mà tỷ lệ cache miss bắt đầu cho thấy hành vi bão hòa trong Hình 12(a).

Hình 9 và Hình 10 cho thấy tác động của expert buffering lên thông lượng và phân bổ bộ nhớ tĩnh cho MT. Expert buffering đã thành công giảm tiêu thụ bộ nhớ tĩnh 2.25GB. Việc giảm bộ nhớ này đặc biệt hữu ích cho người dùng có số lượng GPU hạn chế. Mặc dù thông lượng trở nên nhỏ hơn so với gating động tốn bộ nhớ, thông lượng thu được bởi expert caching có thể so sánh với đường cơ sở dưới nút đơn, và 2.21 ×/4.30× dưới 2/4 nút.

Hơn nữa, chúng tôi nghiên cứu sự đánh đổi độ trễ-bộ nhớ phát sinh bởi cơ chế expert caching, và ước tính biên giới pareto. Hình 13 cho thấy độ trễ và tiêu thụ bộ nhớ dưới một loạt cấu hình cache. Chúng tôi thay đổi cache mỗi GPU và đo độ trễ decoder và tiêu thụ bộ nhớ đỉnh. Kết quả cho thấy rằng biên giới pareto tương tự như các ngoại lệ trên biểu đồ tỷ lệ cache miss trong Hình 12(a). Các phát hiện của chúng tôi cũng cho thấy rằng người đóng góp chính cho độ trễ tăng là băng thông CPU-GPU bị hạn chế, mà chúng tôi quan sát bão hòa ở 12GB/s trong các thí nghiệm của chúng tôi. Các phát hiện cho thấy rằng các lớp có tỷ lệ cache miss cao có thể cản trở hiệu suất. Việc áp dụng các công nghệ tăng cường băng thông CPU-GPU, chẳng hạn như NVIDIA Grace

--- TRANG 9 ---
Hình 13. Sự đánh đổi giữa bộ nhớ và độ trễ dưới cấu hình cache khác nhau trên MT Decoder. Kích thước cache tương ứng mỗi GPU được đánh dấu trên biểu đồ.

Hopper superchip và PCIe v5 có thể giảm thiểu các vấn đề độ trễ trong các tình huống mà bộ nhớ bị hạn chế.

Chúng tôi lưu ý rằng không có công trình trước nào khai thác các đặc tính độc đáo của MoE Transformers để tối ưu hóa sử dụng bộ nhớ. Như một chiến lược caching được thiết kế riêng cho các mô hình MoE, expert buffering là trực giao với các cơ chế tiết kiệm bộ nhớ trước như offloading [25], [30] và có thể được tích hợp một cách liền mạch để tiết kiệm bộ nhớ lớn hơn.

VII. CÂN BẰNG TẢI
Như chúng ta đã thấy trong Phần IV, việc gán token cho experts rất mất cân bằng, do đó tải được gán cho mỗi thiết bị cũng rất mất cân bằng. Những thiết bị lưu trữ experts nóng có thể trở thành nút thắt cổ chai và trở nên dễ bị lỗi hết bộ nhớ hơn. Hơn nữa, các thiết bị lưu trữ experts lạnh có thể ngồi nhàn rỗi trong khi chờ đợi các thiết bị lưu trữ experts nóng hoàn thành tải của chúng. Kết quả là, cân bằng tải rất quan trọng để có một mô hình mạnh mẽ và ổn định.

Chúng tôi đề xuất một lược đồ cân bằng tải đơn giản trong quá trình triển khai mô hình. Chúng tôi tối ưu hóa việc phân bổ experts bằng cách tận dụng dữ liệu tải lịch sử. Cụ thể, chúng tôi mã hóa phân bổ expert lịch sử thành một ma trận, và cân bằng tải trên mỗi thiết bị tương ứng. Chúng tôi kết hợp các experts tải cao hơn với các experts tải thấp hơn, để tải có thể được phân phối đến các thiết bị khác nhau. Chúng tôi ký hiệu việc đặt expert với Pmn, trong đó m là id expert (m = 1...E), n là id thiết bị (n = 1...D) và Pmn = 1 cho thấy rằng expert thứ m được phân bổ trên thiết bị thứ n. Chúng tôi cũng ký hiệu kích hoạt expert với Amb, trong đó m là id expert và b là id batch (b = 1...B), và Amb đại diện cho phần tokens được gán cho expert e tại batch id b. Vấn đề có thể được chính thức hóa như sau:

min max
m,b|X
nPmnAmb−1
D|subject toX
mPmn=E
D∀n

Vấn đề này có thể được rút gọn thành vấn đề phân chia số đa chiều [10], là NP-hard. Để cân bằng

Hình 14. Ảnh hưởng của cơ chế cân bằng tải được đề xuất của chúng tôi: Dựa trên dữ liệu kích hoạt lịch sử, thuật toán của chúng tôi có thể giảm đáng kể vấn đề mất cân bằng tải trên các nhiệm vụ LM và cải thiện độ mạnh mẽ bằng cách giới hạn khối lượng công việc tối đa trên một thiết bị duy nhất.

việc sử dụng bộ nhớ và đơn giản hóa quá trình giao tiếp, mỗi GPU nên được gán cùng số lượng experts.

A. Cân bằng Tham lam cho Kích hoạt Độc lập
Chúng tôi sử dụng một thuật toán tham lam để tạo ra các xấp xỉ cho việc gán tối ưu. Chúng tôi sắp xếp các experts theo khối lượng công việc trung bình của chúng trong dữ liệu lịch sử Ãm, và gán các experts cho GPU theo thứ tự giảm dần. Tại mỗi bước, một expert được gán cho GPU có tải nhỏ nhất, được tính bởi P
mPmnÃm. Một khi GPU đạt đến dung lượng được chỉ định, nó sẽ được loại bỏ khỏi danh sách ứng viên.

Hình 14 tóm tắt sự cân bằng của tải dưới thứ tự ban đầu và thứ tự mới. Sự cân bằng của tải được ước tính bằng cách sử dụng dữ liệu kích hoạt hiện có được giới thiệu trong Phần III-B. Để thực hiện thí nghiệm, chúng tôi tách dữ liệu thành hai nửa. Chúng tôi sử dụng nửa đầu của dữ liệu kích hoạt để tạo ra một gán thiết bị cho mỗi expert, sau đó ước tính khối lượng công việc dưới gán được tạo ra bằng cách sử dụng nửa thứ hai của dữ liệu kích hoạt. Kết quả được chuẩn hóa bởi tổng kích thước batch, có nghĩa là các số đại diện cho phần của tổng số tokens mỗi thiết bị sẽ xử lý trong một batch nhất định.

Chúng tôi ghi lại Max Load, đây là phần tối đa của tải từng xuất hiện trong tất cả batches trên tập kiểm tra, và Avg Max Load, đây là phần tối đa của tải trung bình trên tất cả batches. Max Load ước tính trường hợp xấu nhất liên quan đến lỗi hết bộ nhớ, và Avg Max Load ước tính trường hợp trung bình nơi mất cân bằng trong khối lượng công việc có thể dẫn đến nút thắt cổ chai và gây hại tốc độ suy luận.

Kết quả cho thấy rằng Greedy có thể cân bằng tải cho trường hợp sử dụng LM bằng cách cải thiện cả hai chỉ số (Max Load và Avg Max load mỗi thiết bị) đáng kể, giảm từ hơn 0.6 xuống dưới 0.4. Tương tự cho trường hợp sử dụng Dịch máy, Greedy có thể cân bằng việc gán tải expert cho MT encoder. Hình 9 cho thấy lợi ích của Greedy Rebalancing trên LM và MT Encoder: Rebalancing tăng thông lượng tối đa 10.1% và 19.5% so với gating động thuần túy. Hơn nữa, rebalancing cho phép LM đạt được batchsize 64 và 128 khi nó được triển khai trên bốn nút, làm cho mô hình mạnh mẽ hơn.

--- TRANG 10 ---
B. Cân bằng Chống tương quan cho Kích hoạt Tương quan
Greedy ít hiệu quả hơn cho Machine Translation Decoder. Chúng tôi thấy rằng mức độ kích hoạt expert trở thành một chỉ báo kém hiệu quả hơn trong trường hợp này do tương quan giữa các experts. Để xử lý vấn đề này, chúng tôi đề xuất Anti-correlation Balancing, tính đến tương quan. Ký hiệu tương quan Pearson giữa expert hiện tại a với expert b trong dữ liệu lịch sử là Sab, khối lượng công việc hiện tại có thể được sửa đổi từ P
mPmnÃm thành P
mPmn(Ãm+0.5∗Sam).

Thuật toán này thành công giảm Avg Max Load và Max Load trong hầu hết các trường hợp. Chúng tôi nhận thấy rằng một khối lượng công việc cân bằng hơn cũng có tác động tích cực đến tỷ lệ cache miss. Như được thể hiện trong Hình 12 và 9, tỷ lệ cache miss trường hợp xấu nhất giảm cho tất cả kích thước cache trên MT Decoders, dẫn đến sự gia tăng tối đa 1.9% trên thông lượng của chúng.

VIII. CÔNG TRÌNH LIÊN QUAN
Trong khi MoE Transformer giảm đáng kể chi phí huấn luyện và FLOPS cho các mô hình lớn, kích thước khổng lồ của MoE Transformers và expert parallelism phức tạp [21] đặt ra các trở ngại cho việc triển khai của nó, bao gồm yêu cầu bộ nhớ GPU cao và chi phí giao tiếp quá mức của việc gán expert. Các phương pháp khác nhau đã được phát minh để giảm bớt những trở ngại này. Switch Transformer [7] và ELSLM [2] sử dụng chưng cất kiến thức để chưng cất một MoE Transformer lớn thành một mô hình dense. Trong khi chưng cất giảm số lượng tham số, chỉ một phần nhỏ (khoảng 30%) của lợi ích độ chính xác có thể được giữ lại. Chiến lược MoS được đề xuất trong DeepSpeed-MoE [24] chưng cất kiến thức thành một MoE Transformer nhỏ hơn với ít lớp hơn và các experts được chia sẻ. SE-MoE [30] sử dụng pruning để giảm số lượng experts trong mô hình. WideNet [?] và MPoE [9] giảm số lượng tham số bằng cách thực thi chia sẻ tham số. Ngoài việc giảm tham số, các phương pháp khác trực tiếp giảm tính toán và giao tiếp. BASE Layer và Switch Transformer cũng giảm số lượng experts mà mỗi token được gán để giảm khối lượng giao tiếp và tính toán. V-MoE [26] tiếp tục giảm số lượng bằng cách bỏ một phần lớn tokens. Hash Layer [27] thay thế lớp gating bằng một hàm hash được tính toán trước, giảm chi phí tính toán, nhưng không giảm bớt chi phí giao tiếp. Vì MoE Transformer là một loại Transformer, các kỹ thuật và kiến trúc được tối ưu hóa tăng cường tốc độ suy luận Transformer có thể áp dụng. Các ví dụ liên quan bao gồm Reformer [19], Longformer [3], và Terraformer [17]. Tuy nhiên, có rất ít thảo luận về ứng dụng của chúng cho MoE Transformers.

Các chiến lược Offloading và swapping như [15] hoán đổi các tensors không sử dụng từ bộ nhớ GPU sang bộ nhớ chính để giảm yêu cầu tài nguyên. Tuy nhiên, các chiến lược hiện có chỉ có thể được áp dụng trên các mô hình dense. Việc áp dụng các chiến lược này một cách hiệu quả trên các mạng nơ-ron có điều kiện như MoE là không đơn giản, vì biểu đồ luồng dữ liệu không thể được xây dựng trước do tính toán có điều kiện. FastMoE [13], [14] thiết kế các primitive giao tiếp tùy chỉnh và kernel gating cho việc gán token để giảm chi phí giao tiếp, nhưng nó chưa được thử nghiệm trên các mạng nơ-ron cực lớn. Tutel [16] và DeepSpeed-MoE [24] cải thiện hiệu suất mô hình MoE trên các hệ thống quy mô datacenter bằng cách kết hợp các phương pháp hệ thống và kiến trúc với các kernel được thiết kế riêng cho cả lớp Transformer và MoE, và các primitive giao tiếp chuyên biệt. Phương pháp này kết hợp expert parallelism, model parallelism, và tensor parallelism để tăng đáng kể thông lượng và giảm độ trễ. Tuy nhiên, DeepSpeed-MoE không được thiết kế để bảo tồn tài nguyên GPU và do đó có thể không thực tế cho nhiều người dùng học thuật. SE-MoE [30] sử dụng Ring Memory offloading để giảm sử dụng GPU, đạt được thông lượng tốt hơn DeepSpeed-MoE trong các tình huống tài nguyên thấp. Tuy nhiên, phương pháp này không tận dụng expert parallelism từ MoE Transformers.

IX. KẾT LUẬN
Trong khi tại thời điểm huấn luyện, các mô hình mixture of expert (MoE) cho thấy hiệu suất vượt trội so với các mô hình dense tương đương flop, chúng nổi tiếng lớn, cần một số lượng lớn GPU để triển khai và khó dân chủ hóa. Các nhà nghiên cứu bên ngoài các phòng thí nghiệm công nghiệp lớn không có quyền truy cập vào hàng trăm hoặc hàng nghìn GPU để đủ khả năng khám phá các mô hình lớn như vậy. Hơn nữa, chúng chậm hơn nhiều so với các đối tác dense của chúng tại suy luận. Để vượt qua những thách thức này, chúng tôi đề xuất ba kỹ thuật tối ưu hóa (Dynamic Gating, Expert Buffering, và Expert Load Balancing) để cải thiện hồ sơ bộ nhớ và độ trễ của các mô hình như vậy để triển khai.

TÀI LIỆU THAM KHẢO
[1] "FasterTransformer," https://github.com/NVIDIA/FasterTransformer, truy cập: 2023-03.
[2] M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru et al., "Efficient large scale language modeling with mixtures of experts," arXiv preprint arXiv:2112.10684, 2021.
[3] I. Beltagy, M. E. Peters, and A. Cohan, "Longformer: The long-document transformer," arXiv preprint arXiv:2004.05150, 2020.
[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., "An image is worth 16x16 words: Transformers for image recognition at scale," arXiv preprint arXiv:2010.11929, 2020.
[5] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat et al., "Glam: Efficient scaling of language models with mixture-of-experts," arXiv preprint arXiv:2112.06905, 2021.
[6] W. Fedus, J. Dean, and B. Zoph, "A review of sparse expert models in deep learning," arXiv preprint arXiv:2209.01667, 2022.
[7] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," Journal of Machine Learning Research, vol. 23, no. 120, pp. 1–39, 2022. [Online]. Available: http://jmlr.org/papers/v23/21-0998.html
[8] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al., "The pile: An 800gb dataset of diverse text for language modeling," arXiv preprint arXiv:2101.00027, 2020.
[9] Z.-F. Gao, P. Liu, W. X. Zhao, Z.-Y. Lu, and J.-R. Wen, "Parameter-efficient mixture-of-experts architecture for pre-trained language models," arXiv preprint arXiv:2203.01104, 2022.
[10] R. L. Graham, "Bounds on multiprocessing timing anomalies," SIAM journal on Applied Mathematics, vol. 17, no. 2, pp. 416–429, 1969.
[11] S. Gupta, S. Mukherjee, K. Subudhi, E. Gonzalez, D. Jose, A. H. Awadallah, and J. Gao, "Sparsely activated mixture-of-experts are robust multi-task learners," arXiv preprint arXiv:2204.07689, 2022.

--- TRANG 11 ---
[12] H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoorthy, Y. Chen, R. Mazumder, L. Hong, and E. Chi, "Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning," Advances in Neural Information Processing Systems, vol. 34, pp. 29 335–29 347, 2021.
[13] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang, "Fastmoe: A fast mixture-of-expert training system," arXiv preprint arXiv:2103.13262, 2021.
[14] J. He, J. Zhai, T. Antunes, H. Wang, F. Luo, S. Shi, and Q. Li, "Fastermoe: Modeling and optimizing training of large-scale dynamic pre-trained models," in Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ser. PPoPP '22. New York, NY, USA: Association for Computing Machinery, 2022, p. 120–134. [Online]. Available: https://doi.org/10.1145/3503221.3508418
[15] C.-C. Huang, G. Jin, and J. Li, "Swapadvisor: Pushing deep learning beyond the gpu memory limit via smart swapping," in Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, 2020, pp. 1341–1355.
[16] C. Hwang, W. Cui, Y. Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram, J. Chau, P. Cheng, F. Yang, M. Yang, and Y. Xiong, "Tutel: Adaptive mixture-of-experts at scale," CoRR, vol. abs/2206.03382, Jun. 2022. [Online]. Available: https://arxiv.org/pdf/2206.03382.pdf
[17] S. Jaszczur, A. Chowdhery, A. Mohiuddin, L. Kaiser, W. Gajewski, H. Michalewski, and J. Kanerva, "Sparse is enough in scaling transformers," Advances in Neural Information Processing Systems, vol. 34, pp. 9895–9907, 2021.
[18] Y. J. Kim, A. A. Awan, A. Muzio, A. F. C. Salinas, L. Lu, A. Hendy, S. Rajbhandari, Y. He, and H. H. Awadalla, "Scalable and efficient moe training for multitask multilingual models," arXiv preprint arXiv:2109.10465, 2021.
[19] N. Kitaev, Ł. Kaiser, and A. Levskaya, "Reformer: The efficient transformer," arXiv preprint arXiv:2001.04451, 2020.
[20] S. Kudugunta, Y. Huang, A. Bapna, M. Krikun, D. Lepikhin, M.-T. Luong, and O. Firat, "Beyond distillation: Task-level mixture-of-experts for efficient inference," arXiv preprint arXiv:2110.03742, 2021.
[21] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "Gshard: Scaling giant models with conditional computation and automatic sharding," arXiv preprint arXiv:2006.16668, 2020.
[22] NLLB Team, M. R. Costa-jussà, J. Cross, O. Çelebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. Mejia-Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzmán, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang, "No language left behind: Scaling human-centered machine translation," 2022.
[23] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli, "fairseq: A fast, extensible toolkit for sequence modeling," in Proceedings of NAACL-HLT 2019: Demonstrations, 2019.
[24] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, and Y. He, "Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale," arXiv preprint arXiv:2201.05596, 2022.
[25] J. Ren, S. Rajbhandari, R. Y. Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y. He, "{ZeRO-Offload}: Democratizing {Billion-Scale} model training," in 2021 USENIX Annual Technical Conference (USENIX ATC 21), 2021, pp. 551–564.
[26] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and N. Houlsby, "Scaling vision with sparse mixture of experts," Advances in Neural Information Processing Systems, vol. 34, 2021.
[27] S. Roller, S. Sukhbaatar, J. Weston et al., "Hash layers for large sparse models," Advances in Neural Information Processing Systems, vol. 34, 2021.
[28] J. Sevilla, L. Heim, A. Ho, T. Besiroglu, M. Hobbhahn, and P. Villalobos, "Compute trends across three eras of machine learning," arXiv preprint arXiv:2202.05924, 2022.
[29] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," arXiv preprint arXiv:1701.06538, 2017.
[30] L. Shen, Z. Wu, W. Gong, H. Hao, Y. Bai, H. Wu, X. Wu, H. Xiong, D. Yu, and Y. Ma, "Se-moe: A scalable and efficient mixture-of-experts distributed training and inference system," arXiv preprint arXiv:2205.10034, 2022.
[31] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, "Efficient transformers: A survey," arXiv preprint arXiv:2009.06732, 2020.
[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017.
[33] F. Xue, Z. Shi, F. Wei, Y. Lou, Y. Liu, and Y. You, "Go wider instead of deeper," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 8, 2022, pp. 8779–8787.
[34] A. Yang, J. Lin, R. Men, C. Zhou, L. Jiang, X. Jia, A. Wang, J. Zhang, J. Wang, Y. Li et al., "Exploring sparse expert models and beyond," arXiv preprint arXiv:2105.15082, 2021.
[35] Z. You, S. Feng, D. Su, and D. Yu, "Speechmoe: Scaling to large acoustic models with dynamic routing mixture of experts," arXiv preprint arXiv:2105.03036, 2021.

12
