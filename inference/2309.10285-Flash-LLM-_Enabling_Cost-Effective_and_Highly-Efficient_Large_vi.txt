# Flash-LLM: KÃ­ch hoáº¡t Suy luáº­n MÃ´ hÃ¬nh Sinh lá»›n Chi phÃ­ hiá»‡u quáº£ vÃ  Hiá»‡u quáº£ cao vá»›i Sparsity khÃ´ng cÃ³ cáº¥u trÃºc

Haojun Xiaâˆ—â€ 
Äáº¡i há»c Sydney
hxia6845@uni.sydney.edu.au

Zhen Zhengâˆ—
Alibaba Group
james.zz@alibaba-inc.com

Yuchao Li
Alibaba Group
laiyin.lyc@alibaba-inc.com

Donglin Zhuang
Äáº¡i há»c Sydney
donglin.zhuang@sydney.edu.au

Zhongzhu Zhou
Äáº¡i há»c Sydney
zhongzhu.zhou@sydney.edu.au

Xiafei Qiu
Alibaba Group
xiafei.qiuxf@alibaba-inc.com

Yong Li
Alibaba Group
jiufeng.ly@alibaba-inc.com

Wei Lin
Alibaba Group
weilin.lw@alibaba-inc.com

Shuaiwen Leon Song
Äáº¡i há»c Sydney
shuaiwen.song@sydney.edu.au

## TÃ³m táº¯t

Vá»›i sá»± tÄƒng trÆ°á»Ÿng nhanh chÃ³ng cá»§a kÃ­ch thÆ°á»›c tham sá»‘, viá»‡c triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh sinh lá»›n trá»Ÿ nÃªn ngÃ y cÃ ng thÃ¡ch thá»©c vÃ¬ chÃºng thÆ°á»ng yÃªu cáº§u tiÃªu thá»¥ bá»™ nhá»› GPU lá»›n vÃ  tÃ­nh toÃ¡n khá»•ng lá»“. Cáº¯t tá»‰a mÃ´ hÃ¬nh khÃ´ng cÃ³ cáº¥u trÃºc Ä‘Ã£ lÃ  má»™t phÆ°Æ¡ng phÃ¡p phá»• biáº¿n Ä‘á»ƒ giáº£m cáº£ lÆ°á»£ng bá»™ nhá»› GPU vÃ  tÃ­nh toÃ¡n tá»•ng thá»ƒ trong khi váº«n giá»¯ Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh tá»‘t. Tuy nhiÃªn, cÃ¡c giáº£i phÃ¡p hiá»‡n táº¡i khÃ´ng cung cáº¥p há»— trá»£ hiá»‡u quáº£ cao cho viá»‡c xá»­ lÃ½ sparsity khÃ´ng cÃ³ cáº¥u trÃºc trÃªn GPU hiá»‡n Ä‘áº¡i, Ä‘áº·c biá»‡t lÃ  trÃªn pháº§n cá»©ng tensor core cÃ³ cáº¥u trÃºc cao. Do Ä‘Ã³, chÃºng tÃ´i Ä‘á» xuáº¥t Flash-LLM Ä‘á»ƒ kÃ­ch hoáº¡t suy luáº­n mÃ´ hÃ¬nh sinh lá»›n chi phÃ­ tháº¥p vÃ  hiá»‡u quáº£ cao vá»›i há»— trá»£ tinh vi cho sparsity khÃ´ng cÃ³ cáº¥u trÃºc trÃªn tensor core hiá»‡u suáº¥t cao nhÆ°ng cÃ³ tÃ­nh háº¡n cháº¿ cao. Dá»±a trÃªn quan sÃ¡t chÃ­nh cá»§a chÃºng tÃ´i ráº±ng nÃºt tháº¯t cá»• chai chÃ­nh cá»§a suy luáº­n mÃ´ hÃ¬nh sinh lÃ  vÃ i phÃ©p nhÃ¢n ma tráº­n má»ng mÃ  tensor core sáº½ bá»‹ sá»­ dá»¥ng dÆ°á»›i má»©c Ä‘Ã¡ng ká»ƒ do cÆ°á»ng Ä‘á»™ tÃ­nh toÃ¡n tháº¥p, chÃºng tÃ´i Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p chung Load-as-Sparse vÃ  Compute-as-Dense cho phÃ©p nhÃ¢n ma tráº­n thÆ°a khÃ´ng cÃ³ cáº¥u trÃºc (SpMM). Ã tÆ°á»Ÿng cÆ¡ báº£n lÃ  giáº£i quyáº¿t nÃºt tháº¯t cá»• chai bÄƒng thÃ´ng bá»™ nhá»› Ä‘Ã¡ng ká»ƒ trong khi chá»‹u Ä‘á»±ng cÃ¡c tÃ­nh toÃ¡n dÆ° thá»«a khÃ´ng quan trá»ng Ä‘á»‘i vá»›i hiá»‡u suáº¥t Ä‘áº§u cuá»‘i trÃªn tensor core. Dá»±a trÃªn Ä‘iá»u nÃ y, chÃºng tÃ´i thiáº¿t káº¿ má»™t framework pháº§n má»m hiá»‡u quáº£ cho SpMM khÃ´ng cÃ³ cáº¥u trÃºc dá»±a trÃªn tensor core, táº­n dá»¥ng tÃ i nguyÃªn on-chip cho viá»‡c trÃ­ch xuáº¥t dá»¯ liá»‡u thÆ°a hiá»‡u quáº£ vÃ  chá»“ng chÃ©o tÃ­nh toÃ¡n/truy cáº­p bá»™ nhá»›. CÃ¡c Ä‘Ã¡nh giÃ¡ má»Ÿ rá»™ng cho tháº¥y ráº±ng (1) á»Ÿ cáº¥p Ä‘á»™ kernel SpMM, Flash-LLM vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i thÆ° viá»‡n tiÃªn tiáº¿n nháº¥t, tá»©c lÃ  Sputnik vÃ  SparTA trung bÃ¬nh 2.9Ã— vÃ  1.5Ã—, tÆ°Æ¡ng á»©ng. (2) á» cáº¥p Ä‘á»™ framework Ä‘áº§u cuá»‘i trÃªn cÃ¡c mÃ´ hÃ¬nh OPT-30B/66B/175B, Ä‘á»‘i vá»›i token per GPU-second, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n lÃªn Ä‘áº¿n 3.8Ã— vÃ  3.6Ã— so vá»›i DeepSpeed vÃ  FasterTransformer, tÆ°Æ¡ng á»©ng, vá»›i chi phÃ­ suy luáº­n tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ. MÃ£ nguá»“n cá»§a Flash-LLM Ä‘Æ°á»£c cung cáº¥p cÃ´ng khai táº¡i https://github.com/AlibabaResearch/flash-llm.

## 1 Giá»›i thiá»‡u

CÃ¡c mÃ´ hÃ¬nh sinh Ä‘Ã£ chá»©ng minh hiá»‡u quáº£ cá»§a chÃºng trÃªn má»™t loáº¡t rá»™ng cÃ¡c nhiá»‡m vá»¥ ngÃ´n ngá»¯ vÃ  quáº£n lÃ½ dá»¯ liá»‡u [3,34,45,52,53]. Tuy nhiÃªn, vá»›i sá»± tÄƒng trÆ°á»Ÿng nhanh chÃ³ng cá»§a kÃ­ch thÆ°á»›c tham sá»‘ (vÃ­ dá»¥ GPT-2 [45] 1.5 tá»· tham sá»‘, GPT-3 [3] 175 tá»·, vÃ  Megatron-Turing NLG [50] 530 tá»·), viá»‡c triá»ƒn khai hiá»‡u quáº£ cÃ¡c mÃ´ hÃ¬nh nÃ y trá»Ÿ nÃªn ngÃ y cÃ ng thÃ¡ch thá»©c. Má»™t máº·t, trá»ng sá»‘ cá»§a chÃºng cÃ³ thá»ƒ quÃ¡ lá»›n Ä‘á»ƒ Ä‘Æ°á»£c Ä‘áº·t trÃªn GPU. VÃ­ dá»¥, mÃ´ hÃ¬nh GPT-3 yÃªu cáº§u 350GB bá»™ nhá»› Ä‘á»ƒ chá»‰ lÆ°u trá»¯ cÃ¡c tham sá»‘ vá»›i kiá»ƒu dá»¯ liá»‡u FP16, trong khi GPU NVIDIA A100 [36] chá»‰ cÃ³ tá»‘i Ä‘a 80 GB bá»™ nhá»›. Máº·t khÃ¡c, cÃ¡c mÃ´ hÃ¬nh sinh lá»›n thÆ°á»ng gÃ¢y ra Ä‘á»™ trá»… suy luáº­n ráº¥t cao ngay cáº£ khi sá»­ dá»¥ng nhiá»u GPU vÃ¬ cáº§n má»™t lÆ°á»£ng lá»›n tÃ­nh toÃ¡n vÃ  truy cáº­p bá»™ nhá»›.

CÃ³ ba Ä‘áº·c Ä‘iá»ƒm cÆ¡ báº£n cho suy luáº­n mÃ´ hÃ¬nh thá»±c táº¿: Ä‘á»™ chÃ­nh xÃ¡c, hiá»‡u quáº£ (tá»©c lÃ  Ä‘á»™ trá»… vÃ  thÃ´ng lÆ°á»£ng), vÃ  chi phÃ­ (tá»©c lÃ  bao nhiÃªu tÃ i nguyÃªn pháº§n cá»©ng nÃ³ tiÃªu thá»¥). PhÆ°Æ¡ng phÃ¡p phá»• biáº¿n Ä‘á»ƒ triá»ƒn khai cÃ¡c mÃ´ hÃ¬nh lá»›n báº±ng cÃ¡ch phÃ¢n vÃ¹ng trá»ng sá»‘ mÃ´ hÃ¬nh trÃªn nhiá»u thiáº¿t bá»‹ [49,63] cÃ³ thá»ƒ gáº·p pháº£i chi phÃ­ cao vÃ  hiá»‡u quáº£ tháº¥p. Má»™t máº·t, Ä‘á»‘i vá»›i cÃ¡c ká»‹ch báº£n sáº£n xuáº¥t trung tÃ¢m dá»¯ liá»‡u, viá»‡c sá»­ dá»¥ng nhiá»u GPU cho má»™t suy luáº­n duy nháº¥t cá»§a má»™t mÃ´ hÃ¬nh duy nháº¥t dáº«n Ä‘áº¿n ROI tháº¥p (lá»£i tá»©c Ä‘áº§u tÆ°) vÃ  cÃ³ thá»ƒ quÃ¡ tá»‘n kÃ©m trong thá»±c táº¿. Máº·t khÃ¡c, phÆ°Æ¡ng phÃ¡p thÃ´ng thÆ°á»ng nÃ y yÃªu cáº§u giao tiáº¿p giá»¯a cÃ¡c thiáº¿t bá»‹ bá»• sung, lÃ m tráº§m trá»ng thÃªm váº¥n Ä‘á» hiá»‡u quáº£. Giáº£m táº£i vÃ  hoÃ¡n Ä‘á»•i bá»™ nhá»› GPU lÃ  má»™t phÆ°Æ¡ng phÃ¡p khÃ¡c Ä‘á»ƒ há»— trá»£ trá»ng sá»‘ lá»›n vá»›i bá»™ nhá»› GPU háº¡n cháº¿ [1,48]. Tuy nhiÃªn, cÃ¡c phÆ°Æ¡ng phÃ¡p giáº£m táº£i vÃ  hoÃ¡n Ä‘á»•i thÆ°á»ng dáº«n Ä‘áº¿n Ä‘á»™ trá»… suy luáº­n dÃ i vÃ  do Ä‘Ã³ cÃ³ thá»ƒ khÃ´ng thá»±c táº¿ cho cÃ¡c dá»‹ch vá»¥ suy luáº­n trá»±c tuyáº¿n.

CÃ¡c phÆ°Æ¡ng phÃ¡p cáº¯t tá»‰a trá»ng sá»‘ [16] (sparsification) Ä‘Ã£ Ä‘Æ°á»£c chá»©ng minh lÃ  hiá»‡u quáº£ trong viá»‡c giáº£m sá»­ dá»¥ng bá»™ nhá»› vÃ  tÃ­nh toÃ¡n cho suy luáº­n mÃ´ hÃ¬nh trong khi váº«n duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c tá»‘t thÃ´ng qua viá»‡c loáº¡i bá» má»™t pháº§n cÃ¡c káº¿t ná»‘i Ã­t quan trá»ng trong máº¡ng nÆ¡ron. Trong thá»±c táº¿, cáº¯t tá»‰a khÃ´ng cÃ³ cáº¥u trÃºc thÆ°á»ng giá»¯ Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tá»‘t hÆ¡n so vá»›i cáº¯t tá»‰a cÃ³ cáº¥u trÃºc háº¡n cháº¿ hÆ¡n [8,12,14,16,28,51,54].

Tuy nhiÃªn, ráº¥t khÃ³ Ä‘á»ƒ há»— trá»£ sparsity khÃ´ng cÃ³ cáº¥u trÃºc trÃªn kiáº¿n trÃºc GPU hiá»‡n Ä‘áº¡i má»™t cÃ¡ch hiá»‡u quáº£, Ä‘áº·c biá»‡t lÃ  vÃ¬ SpMM khÃ´ng cÃ³ cáº¥u trÃºc (MatMul thÆ°a) khÃ³ há»— trá»£ trÃªn kiáº¿n trÃºc tensor core hiá»‡u suáº¥t cao nhÆ°ng cÃ³ cáº¥u trÃºc cao. Do Ä‘Ã³, hÆ°á»›ng thiáº¿t káº¿ nÃ y Ä‘Ã£ bá»‹ bá» qua pháº§n lá»›n cho Ä‘áº¿n nay vÃ¬ hiá»‡u suáº¥t cao ráº¥t khÃ³ Ä‘áº¡t Ä‘Æ°á»£c. VÃ­ dá»¥, cÃ¡c triá»ƒn khai SpMM khÃ´ng cÃ³ cáº¥u trÃºc tiÃªn tiáº¿n nháº¥t (vÃ­ dá»¥ cuSPARSE [40], Sputnik [10]) tháº­m chÃ­ khÃ´ng thá»ƒ vÆ°á»£t qua Ä‘á»‘i tÃ¡c dense (cuBLAS [39]) cho Ä‘áº¿n khi sparsity mÃ´ hÃ¬nh cao hÆ¡n 98% vÃ  86%, tÆ°Æ¡ng á»©ng. LÆ°u Ã½ ráº±ng tensor core trÃªn GPU hiá»‡n Ä‘áº¡i thÆ°á»ng cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t Ä‘á»‰nh cao hÆ¡n gáº§n má»™t báº­c Ä‘á»™ lá»›n so vá»›i SIMT core.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» quan trá»ng nÃ y lÃ m nÃºt tháº¯t cá»• chai hiá»‡u suáº¥t vÃ  chi phÃ­ suy luáº­n LLM, chÃºng tÃ´i Ä‘á» xuáº¥t Flash-LLM, má»™t thÆ° viá»‡n GPU hiá»‡u quáº£ Ä‘á»ƒ há»— trá»£ sparsity khÃ´ng cÃ³ cáº¥u trÃºc trÃªn tensor core cho suy luáº­n mÃ´ hÃ¬nh sinh lá»›n. Vá»›i sparsity khÃ´ng cÃ³ cáº¥u trÃºc, Flash-LLM giáº£i quyáº¿t váº¥n Ä‘á» dáº¥u chÃ¢n bá»™ nhá»› dáº«n Ä‘áº¿n chi phÃ­ tháº¥p hÆ¡n trong khi váº«n duy trÃ¬ Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh cao. Báº±ng cÃ¡ch cÃ³ thá»ƒ táº­n dá»¥ng hiá»‡u suáº¥t Ä‘á»‰nh cao cá»§a tensor core, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ trá»… tháº¥p hÆ¡n cho SpMM khÃ´ng cÃ³ cáº¥u trÃºc so vá»›i cÃ¡c giáº£i phÃ¡p MatMul thÆ°a/dense hiá»‡n cÃ³. Ã tÆ°á»Ÿng thiáº¿t káº¿ cáº¥p cao cá»§a Flash-LLM lÃ  chiáº¿n lÆ°á»£c Load-as-Sparse vÃ  Compute-as-Dense nÃ y. ChÃºng tÃ´i Ä‘Æ°a ra má»™t quan sÃ¡t quan trá»ng ráº±ng cÃ¡c MatMul chÃ­nh trong suy luáº­n mÃ´ hÃ¬nh sinh thÆ°á»ng ráº¥t má»ng. HÆ¡n ná»¯a, hiá»‡u suáº¥t cá»§a cÃ¡c MatMul má»ng nÃ y bá»‹ rÃ ng buá»™c bá»Ÿi truy cáº­p bá»™ nhá»› toÃ n cáº§u (hoáº·c bÄƒng thÃ´ng bá»™ nhá»›) hÆ¡n lÃ  kháº£ nÄƒng tÃ­nh toÃ¡n cá»§a tensor core. Do Ä‘iá»u nÃ y, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p sÃ¡ng táº¡o Ä‘á»ƒ há»— trá»£ sparsity khÃ´ng cÃ³ cáº¥u trÃºc trÃªn tensor core báº±ng cÃ¡ch táº­n dá»¥ng táº£i bá»™ nhá»› thÆ°a Ä‘á»ƒ cáº£i thiá»‡n bÄƒng thÃ´ng bá»™ nhá»› háº¡n cháº¿ trong khi chá»‹u Ä‘á»±ng hiá»‡u quáº£ tÃ­nh toÃ¡n tensor-core dÆ° thá»«a (Sec.3.2).

Dá»±a trÃªn Ã½ tÆ°á»Ÿng trÃªn, váº«n cÃ²n thÃ¡ch thá»©c Ä‘á»ƒ thá»±c sá»± thiáº¿t káº¿ vÃ  triá»ƒn khai phÆ°Æ¡ng phÃ¡p Load-as-Sparse vÃ  Compute-as-Dense cáº¥p cao nÃ y. Äáº§u tiÃªn, nÃ³ yÃªu cáº§u má»™t Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u Ä‘Æ°á»£c thiáº¿t káº¿ tá»‘t cho lÆ°u trá»¯ vÃ  trÃ­ch xuáº¥t dá»¯ liá»‡u thÆ°a hiá»‡u quáº£. Viá»‡c trÃ­ch xuáº¥t dá»¯ liá»‡u thÆ°a lÃ  khÃ´ng táº§m thÆ°á»ng, yÃªu cáº§u má»™t thiáº¿t káº¿ tinh vi Ä‘á»ƒ táº£i vÃ  trÃ­ch xuáº¥t dá»¯ liá»‡u thÆ°a vá»›i chi phÃ­ truy cáº­p tá»‘i thiá»ƒu trong bá»™ nhá»› GPU phÃ¢n cáº¥p vá»›i tÃ i nguyÃªn bá»™ nhá»› on-chip háº¡n cháº¿. NÃ³ cÅ©ng Ä‘Æ°a ra nhá»¯ng thÃ¡ch thá»©c má»›i trong viá»‡c thiáº¿t káº¿ pipeline tÃ­nh toÃ¡n MatMul vÆ°á»£t ra ngoÃ i cÃ¡c chiáº¿n lÆ°á»£c MatMul dense thÃ´ng thÆ°á»ng. Trong Flash-LLM, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t Ä‘á»‹nh dáº¡ng thÆ°a má»›i gá»i lÃ  (Tiled-CSL) Ä‘á»ƒ há»— trá»£ thá»±c thi SpMM tile-by-tile vá»›i tensor core (Sec.4.3.1). Dá»±a trÃªn Tiled-CSL, sau Ä‘Ã³ chÃºng tÃ´i thiáº¿t káº¿ phÆ°Æ¡ng phÃ¡p chuyá»ƒn Ä‘á»•i sparse-to-dense má»™t cÃ¡ch cáº©n tháº­n báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c thanh ghi phÃ¢n tÃ¡n vÃ  bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» cho viá»‡c trÃ­ch xuáº¥t dá»¯ liá»‡u thÆ°a (Sec.4.1). Sau Ä‘Ã³, má»™t chiáº¿n lÆ°á»£c chá»“ng chÃ©o hai cáº¥p hiá»‡u quáº£ cá»§a bá»™ nhá»› vÃ  tÃ­nh toÃ¡n Ä‘Æ°á»£c giá»›i thiá»‡u Ä‘á»ƒ phá»‘i há»£p viá»‡c chuyá»ƒn Ä‘á»•i sparse-to-dense trÃªn trá»ng sá»‘, viá»‡c táº£i dá»¯ liá»‡u feature map dense, vÃ  cÃ¡c hoáº¡t Ä‘á»™ng tensor core vá»›i má»™t pipeline pháº§n má»m Ä‘áº§y Ä‘á»§ (Sec.4.2). Cuá»‘i cÃ¹ng, chÃºng tÃ´i Ä‘á» xuáº¥t má»™t phÆ°Æ¡ng phÃ¡p sáº¯p xáº¿p láº¡i dá»¯ liá»‡u thÆ°a trÆ°á»›c thá»i háº¡n Ä‘á»ƒ giáº£m thÃªm xung Ä‘á»™t bank bá»™ nhá»› chia sáº»Â¹ (Sec.4.3.3). TÃ³m láº¡i, bÃ i bÃ¡o nÃ y Ä‘Ã³ng gÃ³p nhÆ° sau:

â€¢ ChÃºng tÃ´i Ä‘á» xuáº¥t Flash-LLM, framework pháº§n má»m chi phÃ­ hiá»‡u quáº£ vÃ  hiá»‡u quáº£ cao Ä‘áº§u tiÃªn Ä‘á»ƒ há»— trá»£ suy luáº­n mÃ´ hÃ¬nh sinh lá»›n, má»Ÿ ra pháº¡m vi khÃ¡m phÃ¡ sparsity khÃ´ng cÃ³ cáº¥u trÃºc trÃªn tensor core hiá»‡u suáº¥t cao.

â€¢ ChÃºng tÃ´i Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p chung Load-as-Sparse vÃ  Compute-as-Dense Ä‘á»ƒ giáº£m dáº¥u chÃ¢n bá»™ nhá»› vÃ  tÄƒng hiá»‡u quáº£ cá»§a cÃ¡c MatMul má»ng chÃ­nh báº±ng cÃ¡ch táº­n dá»¥ng Ã½ tÆ°á»Ÿng giáº£i quyáº¿t nÃºt tháº¯t cá»• chai bÄƒng thÃ´ng bá»™ nhá»› chÃ­nh trong suy luáº­n LLM trong khi chá»‹u Ä‘á»±ng cÃ¡c tÃ­nh toÃ¡n tensor-core dÆ° thá»«a.

â€¢ ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t thiáº¿t káº¿ pipeline pháº§n má»m hiá»‡u quáº£ Ä‘á»ƒ kÃ­ch hoáº¡t Flash-LLM báº±ng cÃ¡ch táº­n dá»¥ng hiá»‡u quáº£ Ä‘á»‹nh dáº¡ng thÆ°a má»›i cá»§a chÃºng tÃ´i, chuyá»ƒn Ä‘á»•i sparse-to-dense, vÃ  chiáº¿n lÆ°á»£c chá»“ng chÃ©o hai cáº¥p.

Â¹ Bá»™ nhá»› chia sáº» GPU Ä‘Æ°á»£c chia thÃ nh nhiá»u bank bá»™ nhá»› cÃ³ thá»ƒ Ä‘Æ°á»£c truy cáº­p Ä‘á»“ng thá»i. Xung Ä‘á»™t bank cÃ³ nghÄ©a lÃ  nhiá»u Ä‘á»‹a chá»‰ cá»§a má»™t yÃªu cáº§u bá»™ nhá»› Ã¡nh xáº¡ Ä‘áº¿n cÃ¹ng má»™t bank bá»™ nhá»›, gÃ¢y ra cÃ¡c truy cáº­p ná»‘i tiáº¿p.

â€¢ Flash-LLM Ä‘Æ°á»£c triá»ƒn khai vÃ  tÃ­ch há»£p vÃ o FasterTransformer Ä‘á»ƒ dá»… sá»­ dá»¥ng. Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ má»Ÿ rá»™ng Ä‘Ã£ cho tháº¥y ráº±ng (1) á»Ÿ cáº¥p Ä‘á»™ kernel, Flash-LLM vÆ°á»£t trá»™i so vá»›i cÃ¡c giáº£i phÃ¡p tiÃªn tiáº¿n nháº¥t Sputnik vÃ  SparTA trung bÃ¬nh 2.9Ã— vÃ  1.5Ã—, tÆ°Æ¡ng á»©ng. (2) á» cáº¥p Ä‘á»™ framework Ä‘áº§u cuá»‘i trÃªn cÃ¡c mÃ´ hÃ¬nh OPT-30B/66B/175B, Ä‘á»‘i vá»›i token per GPU-second, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n lÃªn Ä‘áº¿n 3.8Ã— vÃ  3.6Ã— so vá»›i DeepSpeed vÃ  FasterTransformer, vá»›i chi phÃ­ suy luáº­n tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ.

## 2 Ná»n táº£ng

### 2.1 Suy luáº­n MÃ´ hÃ¬nh Sinh

**Quy trÃ¬nh Suy luáº­n cá»§a CÃ¡c MÃ´ hÃ¬nh Sinh Hiá»‡n Ä‘áº¡i.** Suy luáº­n cá»§a cÃ¡c mÃ´ hÃ¬nh sinh hiá»‡n Ä‘áº¡i thÆ°á»ng Ä‘Æ°á»£c tiáº¿n hÃ nh trong hai giai Ä‘oáº¡n: xá»­ lÃ½ prompt vÃ  táº¡o token. NhÆ° minh há»a trong Fig.1a, mÃ´ hÃ¬nh sinh Ä‘áº§u tiÃªn thá»±c hiá»‡n xá»­ lÃ½ prompt Ä‘á»ƒ xá»­ lÃ½ cÃ¡c chuá»—i Ä‘áº§u vÃ o cá»§a ngÆ°á»i dÃ¹ng ('I love dogs') vÃ  táº¡o ra token má»›i Ä‘áº§u tiÃªn ('and'). Sau Ä‘Ã³ mÃ´ hÃ¬nh chuyá»ƒn sang giai Ä‘oáº¡n táº¡o token tá»± há»“i quy, nÆ¡i token Ä‘áº§u ra duy nháº¥t Ä‘Æ°á»£c táº¡o ra á»Ÿ bÆ°á»›c i-1 sáº½ Ä‘Æ°á»£c láº¥y lÃ m Ä‘áº§u vÃ o Ä‘á»ƒ táº¡o ra token má»›i á»Ÿ bÆ°á»›c i má»™t cÃ¡ch láº·p Ä‘i láº·p láº¡i.

Trong giai Ä‘oáº¡n xá»­ lÃ½ prompt, nhiá»u token trong chuá»—i Ä‘áº§u vÃ o sáº½ Ä‘Æ°á»£c xá»­ lÃ½ cÃ¹ng má»™t lÃºc, dáº«n Ä‘áº¿n cÃ¡c tensor Ä‘áº§u vÃ o vá»›i hÃ¬nh dáº¡ng [ğµ,ğ¿,ğ»] (Fig.1a). ğµ, ğ¿, vÃ  ğ» biá»ƒu thá»‹ kÃ­ch thÆ°á»›c batch suy luáº­n, Ä‘á»™ dÃ i chuá»—i prompt, vÃ  chiá»u áº©n cá»§a mÃ´ hÃ¬nh, tÆ°Æ¡ng á»©ng. Trong khi Ä‘Ã³ á»Ÿ giai Ä‘oáº¡n táº¡o token, chá»‰ cÃ³ token duy nháº¥t Ä‘Æ°á»£c táº¡o ra trong láº§n láº·p cuá»‘i cÃ¹ng sáº½ Ä‘Æ°á»£c láº¥y lÃ m Ä‘áº§u vÃ o do Ä‘Ã³ táº¡o thÃ nh cÃ¡c tensor Ä‘áº§u vÃ o vá»›i hÃ¬nh dáº¡ng [ğµ,1,ğ»]. LÆ°u Ã½ ráº±ng Ä‘á»ƒ ngÄƒn cháº·n tÃ­nh toÃ¡n láº¡i trÃªn cÃ¡c vector K vÃ  VÂ² cá»§a cÃ¡c token trÆ°á»›c Ä‘Ã³, má»™t bá»™ Ä‘á»‡m bá»™ nhá»› Ä‘Æ°á»£c phÃ¢n bá»• trÆ°á»›c (tá»©c lÃ  KV-Cache Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.1b) thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh táº¡o token. á» má»—i bÆ°á»›c, má»™t cáº·p KV má»›i Ä‘Æ°á»£c táº¡o ra (mÃ u vÃ ng) vÃ  Ä‘Æ°á»£c ghi vÃ o KV-Cache.

**Äiá»ƒm nÃ³ng Hiá»‡u suáº¥t Suy luáº­n cá»§a LLM.** Fig.2 minh há»a kiáº¿n trÃºc decoder Ä‘iá»ƒn hÃ¬nh cá»§a má»™t lá»›p duy nháº¥t trong cÃ¡c mÃ´ hÃ¬nh sinh dá»±a trÃªn attention hiá»‡n Ä‘áº¡i. CÃ³ tá»•ng cá»™ng bá»‘n phÃ©p nhÃ¢n ma tráº­n chÃ­nh (hoáº·c MatMul) trong lá»›p decoder: QKV Projection, Output Projection, MLP1, vÃ  MLP2. KhÃ´ng giá»‘ng nhÆ° cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ khÃ´ng sinh trung tÃ¢m encoder trÆ°á»›c Ä‘Ã¢y (vÃ­ dá»¥, BERT [22]) mÃ  nÃºt tháº¯t cá»• chai hiá»‡u suáº¥t chá»§ yáº¿u á»Ÿ tÃ­nh toÃ¡n multi-head attention, hiá»‡u suáº¥t suy luáº­n mÃ´ hÃ¬nh sinh bá»‹ rÃ ng buá»™c máº¡nh bá»Ÿi bá»‘n MatMul nÃ y. Theo cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i trÃªn OPT-66B [61]

Â² CÃ¡c vector K vÃ  V cá»§a cÃ¡c token trÆ°á»›c Ä‘Ã³ Ä‘Æ°á»£c cáº§n bá»Ÿi cÆ¡ cháº¿ attention [55].

suy luáº­n mÃ´ hÃ¬nh, bá»‘n MatMul nÃ y lÃ  nhá»¯ng Ä‘Ã³ng gÃ³p hÃ ng Ä‘áº§u cho Ä‘á»™ trá»… tá»•ng thá»ƒ chiáº¿m 76.8% thá»i gian thá»±c thi Ä‘áº§u cuá»‘i vÃ  cÅ©ng lÃ  Ä‘Ã³ng gÃ³p hÃ ng Ä‘áº§u cho tiÃªu thá»¥ bá»™ nhá»› chiáº¿m 83.8% sá»­ dá»¥ng bá»™ nhá»› tá»•ng thá»ƒ.

### 2.2 NhÃ¢n Ma tráº­n trong Suy luáº­n LLM

**NhÃ¢n Ma tráº­n Má»ng.** Matrix Multiply (MatMul) trong Fig.2 cÃ³ thá»ƒ Ä‘Æ°á»£c chÃ­nh thá»©c hÃ³a nhÆ° ğ¶=ğ´Ã—ğµ, trong Ä‘Ã³ ğ´ lÃ  ma tráº­n trá»ng sá»‘ cÃ³ hÃ¬nh dáº¡ng [ğ‘€,ğ¾] vÃ  ğµ lÃ  ma tráº­n feature map cÃ³ hÃ¬nh dáº¡ng [ğ¾,ğ‘]. Trong bÃ i bÃ¡o nÃ y, chÃºng tÃ´i gá»i nhá»¯ng MatMul nÃ y lÃ  "Skinny MatMul", vÃ¬ chiá»u ğ‘ cá»§a chÃºng nhá» hÆ¡n nhiá»u so vá»›i chiá»u ğ‘€ vÃ  ğ¾.Â³

**Sá»± khÃ¡c biá»‡t giá»¯a Tensor/SIMT core.** SIMT core (tá»©c lÃ  CUDA core) lÃ  cÃ¡c Ä‘Æ¡n vá»‹ thá»±c thi Ä‘a má»¥c Ä‘Ã­ch xá»­ lÃ½ má»™t loáº¡t rá»™ng cÃ¡c lá»‡nh Ä‘á»ƒ thá»±c thi song song, trong khi tensor core [36,38] lÃ  cÃ¡c Ä‘Æ¡n vá»‹ chuyÃªn biá»‡t Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ tÄƒng tá»‘c cÃ¡c tÃ­nh toÃ¡n MatMul dense. Tensor core cung cáº¥p gia tá»‘c Ä‘Ã¡ng ká»ƒ cho MatMul dense, vÃ­ dá»¥ thÃ´ng lÆ°á»£ng cao hÆ¡n 16Ã— so vá»›i SIMT core trong GPU A100 vá»›i tÃ­ch lÅ©y FP32.

CÃ¡c ká»¹ thuáº­t thÃ´ng thÆ°á»ng táº­n dá»¥ng SIMT core cho MatMul thÆ°a khÃ´ng thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng trá»±c tiáº¿p cho tensor core vÃ¬ SIMT vÃ  tensor core hoáº¡t Ä‘á»™ng á»Ÿ Ä‘á»™ chi tiáº¿t ráº¥t khÃ¡c nhau. SIMT core hoáº¡t Ä‘á»™ng á»Ÿ Ä‘á»™ chi tiáº¿t cá»§a cÃ¡c giÃ¡ trá»‹ vÃ´ hÆ°á»›ng, vÃ­ dá»¥ lá»‡nh FMA trÃªn giÃ¡ trá»‹ vÃ´ hÆ°á»›ng. Äá»™ chi tiáº¿t theo tá»«ng pháº§n tá»­ giÃºp dá»… dÃ ng thá»±c hiá»‡n bá» qua tÃ­nh toÃ¡n á»Ÿ cáº¥p Ä‘á»™ pháº§n tá»­ cho SpMM. Tuy nhiÃªn, tensor core hoáº¡t Ä‘á»™ng á»Ÿ Ä‘á»™ chi tiáº¿t thÃ´ hÆ¡n nhiá»u so vá»›i SIMT core, vÃ­ dá»¥ thá»±c hiá»‡n phÃ©p nhÃ¢n ma tráº­n giá»¯a hai ma tráº­n vá»›i hÃ¬nh dáº¡ng 16Ã—16 vÃ  16Ã—8 trong má»—i lá»‡nh. Do Ä‘Ã³, tensor core khÃ´ng cho phÃ©p bá» qua cÃ¡c tÃ­nh toÃ¡n cáº¥p Ä‘á»™ pháº§n tá»­ tÃ¹y Ã½.

## 3 CÆ¡ há»™i vÃ  Hiá»ƒu biáº¿t

### 3.1 Sparsity khÃ´ng cÃ³ cáº¥u trÃºc trÃªn Tensor Core

CÃ³ hai loáº¡i nguyÃªn táº¯c cáº¯t tá»‰a Ä‘iá»ƒn hÃ¬nh. Chiáº¿n lÆ°á»£c cáº¯t tá»‰a linh hoáº¡t nháº¥t (sparsity khÃ´ng cÃ³ cáº¥u trÃºc) lÃ  loáº¡i bá» cÃ¡c pháº§n tá»­ Ã­t quan trá»ng mÃ  khÃ´ng xem xÃ©t phÃ¢n phá»‘i cá»§a cÃ¡c pháº§n tá»­ Ä‘Æ°á»£c cáº¯t tá»‰a trong ma tráº­n trá»ng sá»‘. Láº¥y cáº¯t tá»‰a magnitude lÃ m vÃ­ dá»¥, chÃºng ta cÃ³ thá»ƒ xáº¿p háº¡ng táº¥t cáº£ cÃ¡c pháº§n tá»­ trong ma tráº­n dá»±a trÃªn giÃ¡ trá»‹ tuyá»‡t Ä‘á»‘i cá»§a chÃºng vÃ  sau Ä‘Ã³ loáº¡i bá» cÃ¡c trá»ng sá»‘ cÃ³ magnitude nhá» nháº¥t. Má»™t chiáº¿n lÆ°á»£c khÃ¡c (sparsity cÃ³ cáº¥u trÃºc) lÃ  cáº¯t tá»‰a cÃ¡c trá»ng sá»‘ Ã­t quan trá»ng, nhÆ°ng Ä‘á»“ng thá»i, má»™t sá»‘ tiÃªu chÃ­ cáº¥u trÃºc pháº£i Ä‘Æ°á»£c thá»±c thi. VÃ­ dá»¥, cÃ¡c ma tráº­n trá»ng sá»‘ cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh cÃ¡c vector 8Ã—1 [4,25] hoáº·c khá»‘i 32Ã—32 [13] khÃ´ng chá»“ng chÃ©o, vÃ  sau Ä‘Ã³ má»—i vector/khá»‘i Ä‘Æ°á»£c giá»¯ láº¡i hoáº·c loáº¡i bá» trong quÃ¡ trÃ¬nh cáº¯t tá»‰a.

TÃ³m láº¡i, sá»± khÃ¡c biá»‡t chÃ­nh giá»¯a cáº¯t tá»‰a cÃ³ cáº¥u trÃºc vÃ  khÃ´ng cÃ³ cáº¥u trÃºc lÃ  cÃ¡c rÃ ng buá»™c bá»• sung pháº£i Ä‘Æ°á»£c thá»a mÃ£n cho cáº¯t tá»‰a cÃ³ cáº¥u trÃºc. Máº·c dÃ¹ sparsity cÃ³ cáº¥u trÃºc thÃ¢n thiá»‡n vá»›i gia tá»‘c pháº§n cá»©ng, nÃ³ gáº·p pháº£i sá»± suy giáº£m Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh nghiÃªm trá»ng hÆ¡n [8,12,14,16,51,54] vÃ¬ nÃ³ háº¡n cháº¿ tá»± do quyáº¿t Ä‘á»‹nh pháº§n tá»­ nÃ o cáº§n cáº¯t tá»‰a. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong [28], so vá»›i sparsity cÃ³ cáº¥u trÃºc cÃ³ sá»¥t giáº£m Ä‘á»™ chÃ­nh xÃ¡c 5%, sparsity khÃ´ng cÃ³ cáº¥u trÃºc chá»‰ dáº«n Ä‘áº¿n sá»¥t giáº£m Ä‘á»™ chÃ­nh xÃ¡c 1%. Trong cÃ¡c thÃ­ nghiá»‡m cá»§a chÃºng tÃ´i, cÃ¡c mÃ´ hÃ¬nh giá»‘ng OPT cÃ³ thá»ƒ báº£o tá»“n Ä‘á»™ chÃ­nh xÃ¡c lá»›n thÃ´ng qua cáº¯t tá»‰a khÃ´ng cÃ³ cáº¥u trÃºc dá»±a trÃªn retraining [15,29] á»Ÿ 80% sparsity (vÃ­ dá»¥, Ä‘á»™ chÃ­nh xÃ¡c chá»‰ giáº£m tá»« 85.55% xuá»‘ng 84.11% cho OPT-30B).

Tuy nhiÃªn, cÃ¡c ká»¹ thuáº­t thÃ´ng thÆ°á»ng Ä‘á»ƒ há»— trá»£ sparsity khÃ´ng cÃ³ cáº¥u trÃºc ngáº«u nhiÃªn trong thá»±c thi SpMM khÃ´ng hiá»‡u quáº£ vÃ¬ chÃºng táº­p trung vÃ o viá»‡c táº­n dá»¥ng SIMT core mÃ  khÃ´ng cÃ³ cÃ¡ch tinh vi Ä‘á»ƒ sá»­ dá»¥ng tensor core hiá»‡u suáº¥t cao. Fig.3 cho tháº¥y so sÃ¡nh hiá»‡u suáº¥t cá»§a cÃ¡c ká»¹ thuáº­t khÃ¡c nhau cho SpMM trÃªn má»™t nhiá»‡m vá»¥ suy luáº­n OPT-66B vá»›i kÃ­ch thÆ°á»›c batch 8. LÆ°u Ã½ ráº±ng cáº¯t tá»‰a tiÃªu chuáº©n cho suy luáº­n LLM thÆ°á»ng yÃªu cáº§u má»©c Ä‘á»™ sparsity vá»«a pháº£i (vÃ­ dá»¥, 80%) Ä‘á»ƒ báº£o tá»“n cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh trong khi giáº£m dáº¥u chÃ¢n bá»™ nhá»›.

CuSparse [40], thÆ° viá»‡n SpMM cá»§a NVIDIA, cho tháº¥y hiá»‡u suáº¥t kÃ©m vÃ¬ nÃ³ chá»§ yáº¿u Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c á»©ng dá»¥ng khoa há»c nÆ¡i ma tráº­n cá»±c ká»³ thÆ°a (99%+). Sputnik [10], tá»‘i Æ°u hÃ³a trung tÃ¢m SIMT-core tiÃªn tiáº¿n nháº¥t cho SpMM khÃ´ng cÃ³ cáº¥u trÃºc trÃªn cÃ¡c nhiá»‡m vá»¥ DL váº«n khÃ´ng thá»ƒ vÆ°á»£t qua cuBLAS (dense) cho Ä‘áº¿n khi Ä‘áº¡t Ä‘Æ°á»£c sparsity cao.

ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng cÃ¡c kernel MatMul thÆ°a trong cÃ¡c thÆ° viá»‡n thÆ°a hiá»‡n cÃ³ thÆ°á»ng cháº­m hÆ¡n so vá»›i Ä‘á»‘i tÃ¡c dense cá»§a chÃºng (cuBLAS [39]). LÃ½ do lÃ  cuBLAS Ä‘Ã£ táº­n dá»¥ng tensor core, trong khi cÃ¡c kernel MatMul thÆ°a Ä‘ang táº­n dá»¥ng SIMT core trong cÃ¡c giáº£i phÃ¡p tiÃªn tiáº¿n nháº¥t. LÆ°u Ã½ ráº±ng GPU A100 [36] cÃ³ thá»ƒ cung cáº¥p thÃ´ng lÆ°á»£ng tÃ­nh toÃ¡n cao hÆ¡n 16Ã— sá»­ dá»¥ng tensor core so vá»›i sá»­ dá»¥ng SIMT core cho MatMul dense. Máº·c dÃ¹ Sputnik cÃ³ thá»ƒ táº­n dá»¥ng hiá»‡u quáº£ SIMT core cho xá»­ lÃ½ sparsity khÃ´ng cÃ³ cáº¥u trÃºc, hiá»‡u suáº¥t cá»§a nÃ³ váº«n bá»‹ háº¡n cháº¿ bá»Ÿi hiá»‡u suáº¥t Ä‘á»‰nh cá»§a SIMT core.

Â³ Äá»‘i vá»›i nhá»¯ng MatMul nÃ y, ğ‘€ vÃ  ğ¾ lÃ  bá»™i sá»‘ nguyÃªn cá»§a kÃ­ch thÆ°á»›c áº©n trong khi ğ‘ báº±ng kÃ­ch thÆ°á»›c batch suy luáº­n (thÆ°á»ng nhá» hÆ¡n hÃ ng báº­c so vá»›i kÃ­ch thÆ°á»›c áº©n).

Do sá»± chÃªnh lá»‡ch hiá»‡u suáº¥t Ä‘á»‰nh rÃµ rÃ ng giá»¯a SIMT vÃ  tensor core, cÃ³ nhu cáº§u máº¡nh máº½ vá» há»— trá»£ SpMM khÃ´ng cÃ³ cáº¥u trÃºc hiá»‡u suáº¥t cao cho suy luáº­n LLM. Tuy nhiÃªn, viá»‡c kÃ­ch hoáº¡t SpMM khÃ´ng cÃ³ cáº¥u trÃºc hiá»‡u suáº¥t cao trÃªn tensor core cÃ³ tÃ­nh háº¡n cháº¿ cao lÃ  khÃ´ng táº§m thÆ°á»ng vÃ¬ tensor core khÃ´ng cho phÃ©p bá» qua cÃ¡c tÃ­nh toÃ¡n cáº¥p Ä‘á»™ vÃ´ hÆ°á»›ng tÃ¹y Ã½ (Ä‘Æ°á»£c mÃ´ táº£ trong pháº§n 2.2). CÃ¡c cÃ´ng trÃ¬nh SpMM trÆ°á»›c Ä‘Ã¢y dá»±a trÃªn cÃ¡c ma tráº­n thÆ°a cÃ³ cáº¥u trÃºc cao [4,13,18] (khÃ´ng pháº£i sparsity khÃ´ng cÃ³ cáº¥u trÃºc ngáº«u nhiÃªn), hoáº·c cho tá»· lá»‡ sparsity cá»±c cao [57] (tá»©c lÃ  >95%, khÃ´ng phÃ¹ há»£p cho Ä‘á»™ chÃ­nh xÃ¡c suy luáº­n LLM), thay vÃ¬ sparsity khÃ´ng cÃ³ cáº¥u trÃºc ngáº«u nhiÃªn á»Ÿ pháº¡m vi tá»· lá»‡ sparsity vá»«a pháº£i cho Ä‘á»™ chÃ­nh xÃ¡c cao. SparTA [65] táº­n dá»¥ng sparse tensor core [32] cho cÃ¡c tÃ­nh toÃ¡n chÃ­nh. Tuy nhiÃªn, nÃ³ khÃ´ng thá»ƒ khai thÃ¡c hiá»‡u quáº£ sparsity cao vÃ¬ sparse tensor core chá»‰ há»— trá»£ 50% sparsity (tá»©c lÃ  sparsity 2:4). NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.3, hiá»‡u suáº¥t cá»§a SparTA tháº¥p hÆ¡n Flash-LLM Ä‘áº·c biá»‡t khi sparsity tÄƒng.

### 3.2 CÆ¡ há»™i Thiáº¿t káº¿

Do sá»± khÃ´ng phÃ¹ há»£p tá»± nhiÃªn giá»¯a tÃ­nh toÃ¡n SpMM khÃ´ng cÃ³ cáº¥u trÃºc vÃ  kiáº¿n trÃºc tensor core cÃ³ cáº¥u trÃºc cao, viá»‡c cÃ³ má»™t giáº£i phÃ¡p SpMM hiá»‡u quáº£ cao trÃªn tensor core theo Ä‘áº·c Ä‘iá»ƒm workload cá»§a LLM hiá»‡n Ä‘áº¡i lÃ  cáº§n thiáº¿t. NhÆ° Ä‘Ã£ tháº£o luáº­n trong Sec.2.2, MatMul trong suy luáº­n LLM hiá»‡n Ä‘áº¡i lÃ  má»ng. Do Ä‘Ã³, nÃºt tháº¯t cá»• chai cá»§a cÃ¡c tÃ­nh toÃ¡n MatMul má»ng lÃ  truy cáº­p bá»™ nhá»› off-chip vÃ  háº¡n cháº¿ bÄƒng thÃ´ng, thay vÃ¬ xá»­ lÃ½ sá»‘ há»c trÃªn tensor core. Dá»±a trÃªn quan sÃ¡t nÃ y, Ã½ tÆ°á»Ÿng cÆ¡ báº£n Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y lÃ  thÃ´ng qua phÆ°Æ¡ng phÃ¡p Load-as-Sparse vÃ  Compute-as-Dense. Cá»¥ thá»ƒ, kernel GPU táº£i cÃ¡c ma tráº­n trá»ng sá»‘ tá»« bá»™ nhá»› toÃ n cáº§u á»Ÿ Ä‘á»‹nh dáº¡ng thÆ°a vá»›i kÃ­ch thÆ°á»›c giáº£m vÃ  tÃ¡i táº¡o Ä‘á»‹nh dáº¡ng dense tÆ°Æ¡ng á»©ng trong bá»™ Ä‘á»‡m on-chip tá»‘c Ä‘á»™ cao cho tÃ­nh toÃ¡n tensor core. Ã tÆ°á»Ÿng chÃ­nh lÃ  nÃºt tháº¯t cá»• chai cho suy luáº­n LLM khÃ´ng á»Ÿ phÃ­a tÃ­nh toÃ¡n do Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ chá»‹u Ä‘á»±ng cÃ¡c tÃ­nh toÃ¡n dÆ° thá»«a vá»›i tensor core. ChÃºng tÃ´i mÃ´ táº£ má»™t cÃ¡ch cÃ³ há»‡ thá»‘ng cÃ¡ch cÃ¡c giao dá»‹ch bá»™ nhá»› off-chip trá»Ÿ thÃ nh nÃºt tháº¯t cá»• chai hiá»‡u suáº¥t trong Sec.3.2.1, vÃ  táº¡i sao cÃ³ thá»ƒ chá»‹u Ä‘á»±ng tÃ­nh toÃ¡n dÆ° thá»«a nhÆ° váº­y cho SpMM má»ng trong suy luáº­n LLM trong Sec.3.2.2.

#### 3.2.1 NÃºt tháº¯t cá»• chai Hiá»‡u suáº¥t cá»§a MatMul Má»ng trong Suy luáº­n LLM.

ChÃºng tÃ´i phÃ¢n tÃ­ch nÃºt tháº¯t cá»• chai hiá»‡u suáº¥t cá»§a thá»±c thi MatMul má»ng báº¯t Ä‘áº§u tá»« workload MatMul dense trong LLM. Theo káº¿t quáº£ profiling cá»§a chÃºng tÃ´i cho OPT-66B [61], viá»‡c sá»­ dá»¥ng trung bÃ¬nh cá»§a tensor core khoáº£ng 5.0%, 10.1%, 19.9%, vÃ  39.7% dÆ°á»›i kÃ­ch thÆ°á»›c batch Ä‘iá»ƒn hÃ¬nh lÃ  8, 16, 32 vÃ  64 nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.4, trong khi bÄƒng thÃ´ng cá»§a bá»™ nhá»› toÃ n cáº§u Ä‘Ã£ Ä‘Æ°á»£c bÃ£o hÃ²a hoÃ n toÃ n. NguyÃªn nhÃ¢n cÆ¡ báº£n cho Ä‘iá»u nÃ y lÃ  cÆ°á»ng Ä‘á»™ tÃ­nh toÃ¡n (tá»©c lÃ  FLOP/Byte) cá»§a MatMul má»ng ráº¥t tháº¥p. Äá»‘i vá»›i má»™t MatMul Ä‘Æ°á»£c mÃ´ táº£ trong sec. 2.2, tá»•ng sá»‘ hoáº¡t Ä‘á»™ng Ä‘Æ°á»£c tiáº¿n hÃ nh lÃ  2ğ‘€ğ‘ğ¾ hoáº¡t Ä‘á»™ng Ä‘iá»ƒm ná»•i (FLOP), vÃ  viá»‡c Ä‘á»c dá»¯ liá»‡u tÆ°Æ¡ng á»©ng lÃ  2(ğ‘€ğ¾+ğ¾ğ‘) byte vá»›i kiá»ƒu dá»¯ liá»‡u FP16. Do Ä‘Ã³, cÆ°á»ng Ä‘á»™ tÃ­nh toÃ¡n (ğ¶ğ¼) lÃ :

ğ¶ğ¼=ğ‘€Ã—ğ‘/ğ‘€+ğ‘  (1)

Theo PhÆ°Æ¡ng trÃ¬nh.1, dá»… dÃ ng chá»©ng minh ráº±ng ğ¶ğ¼ tá»•ng thá»ƒ cá»§a má»™t MatMul cÃ³ thá»ƒ dá»… dÃ ng bá»‹ háº¡n cháº¿ bá»Ÿi chiá»u M hoáº·c N nhá». VÃ­ dá»¥, Ä‘á»‘i vá»›i má»™t MatMul má»ng nÆ¡i ğ‘ lÃ  16, ğ¶ğ¼ sáº½ cÃ³ giá»›i háº¡n trÃªn nghiÃªm ngáº·t lÃ  16 báº¥t ká»ƒ chiá»u ğ‘€ lá»›n nhÆ° tháº¿ nÃ o. LÆ°u Ã½ ráº±ng trong cÃ¡c mÃ´ hÃ¬nh LLM sinh, chiá»u ğ‘ báº±ng kÃ­ch thÆ°á»›c batch suy luáº­n thÆ°á»ng ráº¥t nhá» trong mÃ´i trÆ°á»ng sáº£n xuáº¥t. Do Ä‘Ã³, ğ¶ğ¼ bá»‹ rÃ ng buá»™c máº¡nh bá»Ÿi chiá»u ğ‘ trong suy luáº­n LLM thá»i gian thá»±c. Theo mÃ´ hÃ¬nh roofline [58], hiá»‡u suáº¥t cá»§a má»™t kernel vá»›i cÆ°á»ng Ä‘á»™ tÃ­nh toÃ¡n tháº¥p sáº½ dá»… dÃ ng bá»‹ rÃ ng buá»™c bá»Ÿi bÄƒng thÃ´ng bá»™ nhá»›.

#### 3.2.2 Load as Sparse, Compute as Dense

Do nÃºt tháº¯t cá»• chai cá»§a MatMul má»ng Ä‘áº¿n tá»« truy cáº­p bá»™ nhá»›/bÄƒng thÃ´ng bá»™ nhá»› thay vÃ¬ tÃ­nh toÃ¡n sá»‘ há»c, chÃºng tÃ´i Ä‘á» xuáº¥t Ã½ tÆ°á»Ÿng cÆ¡ báº£n cá»§a Load-as-Sparse vÃ  Compute-as-Dense á»Ÿ Ä‘Ã¢y táº­n dá»¥ng sá»± gia tÄƒng hiá»‡u suáº¥t tá»« viá»‡c giáº£m truy cáº­p bá»™ nhá»› trong khi cho phÃ©p sá»­ dá»¥ng hiá»‡u quáº£ tensor core cho sparsity khÃ´ng cÃ³ cáº¥u trÃºc (tham kháº£o Sec.4 Ä‘á»ƒ biáº¿t chi tiáº¿t). DÆ°á»›i Ã½ tÆ°á»Ÿng cÆ¡ báº£n nÃ y, cho tá»· lá»‡ sparsity ğ›½ (ma tráº­n trá»ng sá»‘ ğ´ thÆ°a trong khi ma tráº­n feature map ğµ dense), cÆ°á»ng Ä‘á»™ tÃ­nh toÃ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n thÃ nh:

ğ¶ğ¼ğ‘†ğ‘ğ‘ğ‘Ÿğ‘ ğ‘’ğ¿ğ‘œğ‘ğ‘‘â´=ğ‘€Ã—ğ‘/ğ‘€Ã—(1âˆ’ğ›½)+ğ‘  (2)

Fig.5 cho tháº¥y cÃ¡c CI vÃ  hiá»‡u suáº¥t tensor core cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tÆ°Æ¡ng á»©ng cá»§a má»™t MatMul Ä‘iá»ƒn hÃ¬nh (ğ‘€: 48k, ğ‘: BS, ğ¾: 12k) trong suy luáº­n mÃ´ hÃ¬nh OPT-175B vá»›i cÃ¡c kÃ­ch thÆ°á»›c batch khÃ¡c nhau. Theo hÃ¬nh, MatMul trong suy luáº­n mÃ´ hÃ¬nh sinh vá»›i cÃ¡c kÃ­ch thÆ°á»›c batch khÃ¡c nhau Ä‘á»u Ä‘á»‘i máº·t vá»›i cÃ¡c váº¥n Ä‘á» vá» memory wall. Do Ä‘Ã³, cÃ¡c kernel MatMul dense chá»‰ cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c 5.1%, 10.3%, 20.5%, vÃ  40.1% hiá»‡u suáº¥t Ä‘á»‰nh cá»§a tensor core bá»‹ rÃ ng buá»™c bá»Ÿi bÄƒng thÃ´ng bá»™ nhá»› toÃ n cáº§u khÃ´ng Ä‘á»§. Nhá»¯ng giÃ¡ trá»‹ lÃ½ thuyáº¿t nÃ y phÃ¹ há»£p vá»›i cÃ¡c Ä‘o lÆ°á»ng thá»±c táº¿ cá»§a chÃºng tÃ´i trong Fig.4. Vá» lÃ½ thuyáº¿t, vá»›i phÆ°Æ¡ng phÃ¡p Load-as-Sparse vÃ  Compute-as-Dense dÆ°á»›i 40% sparsity, viá»‡c sá»­ dá»¥ng tensor core cÃ³ thá»ƒ Ä‘Æ°á»£c cáº£i thiá»‡n lÃªn 8.5%, 17.1%, 34.2%, vÃ  68.2%.

â´ ÄÃ¡ng chÃº Ã½ ráº±ng chÃºng tÃ´i khÃ´ng tÃ­nh overhead chá»‰ sá»‘ thÆ°a vÃ o xem xÃ©t lÃ½ thuyáº¿t á»Ÿ Ä‘Ã¢y. Trong thá»±c táº¿, CI thá»±c sáº½ tháº¥p hÆ¡n má»™t chÃºt so vá»›i phÆ°Æ¡ng trÃ¬nh nÃ y.

## 4 PhÆ°Æ¡ng phÃ¡p Thiáº¿t káº¿

Flash-LLM táº­n dá»¥ng cáº£ SIMT core vÃ  tensor core má»™t cÃ¡ch hiá»‡u quáº£ cho tÃ­nh toÃ¡n SpMM khÃ´ng cÃ³ cáº¥u trÃºc hiá»‡u quáº£. CÃ¡c SIMT core linh hoáº¡t Ä‘Æ°á»£c khai thÃ¡c cho Sparse-to-Dense Transformation (tá»©c lÃ  Load-as-Sparse) trong khi tensor core Ä‘Æ°á»£c sá»­ dá»¥ng cho cÃ¡c tÃ­nh toÃ¡n tensor tÃ­nh toÃ¡n-intensive (tá»©c lÃ  Compute-as-Dense). ChÃºng tÃ´i sáº½ Ä‘Æ°a ra tá»•ng quan vá» cÃ¡c tá»‘i Æ°u hÃ³a cáº¥p cao cá»§a Flash-LLM trong Sec.4.1. Sau Ä‘Ã³ chÃºng tÃ´i sáº½ mÃ´ táº£ thiáº¿t káº¿ pipeline tÃ­nh toÃ¡n cá»§a Flash-LLM trong Sec.4.2. ChÃºng tÃ´i minh há»a Ä‘á»‹nh dáº¡ng thÆ°a má»›i (Ä‘á»‹nh dáº¡ng tiled-CSL) vÃ  cÃ¡c ká»¹ thuáº­t truy cáº­p bá»™ nhá»› hiá»‡u quáº£ trong Sec.4.3. Cuá»‘i cÃ¹ng, chÃºng tÃ´i mÃ´ táº£ há»‡ thá»‘ng suy luáº­n Ä‘áº§u cuá»‘i Ä‘Æ°á»£c kÃ­ch hoáº¡t bá»Ÿi Flash-LLM cá»§a chÃºng tÃ´i trong Sec.5.

### 4.1 Tá»•ng quan Thiáº¿t káº¿

ChÃºng tÃ´i sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p dá»±a trÃªn tiling cho cÃ¡c tÃ­nh toÃ¡n SpMM trong Flash-LLM. Fig.6a cho tháº¥y phÆ°Æ¡ng phÃ¡p tiling cá»§a Flash-LLM, nÆ¡i má»—i thread block (TB) chá»‹u trÃ¡ch nhiá»‡m tÃ­nh toÃ¡n má»™t tile (vÃ­ dá»¥, tile mÃ u xanh vá»›i hÃ¬nh dáº¡ng ğ‘€ğ‘‡ğµâˆ—ğ‘ğ‘‡ğµ) trong ma tráº­n Ä‘áº§u ra ğ¶. Äá»‘i vá»›i má»—i láº§n láº·p, má»—i thread block táº£i ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ (hÃ¬nh dáº¡ng [ğ‘€ğ‘‡ğµ,ğ¾ğ‘‡ğµ]) á»Ÿ dáº¡ng thÆ°a vÃ  ğµğ‘‡ğ‘–ğ‘™ğ‘’ (hÃ¬nh dáº¡ng [ğ¾ğ‘‡ğµ,ğ‘ğ‘‡ğµ]) á»Ÿ dáº¡ng dense tá»« bá»™ nhá»› toÃ n cáº§u. ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ sau Ä‘Ã³ Ä‘Æ°á»£c chuyá»ƒn Ä‘á»•i thÃ nh Ä‘á»‹nh dáº¡ng dense vá»›i Sparse-to-Dense Transformation Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.6b vÃ  Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› chia sáº» trong khi ğµğ‘‡ğ‘–ğ‘™ğ‘’ Ä‘Æ°á»£c lÆ°u trá»¯ trá»±c tiáº¿p trong bá»™ nhá»› chia sáº». Cuá»‘i cÃ¹ng, má»—i thread block tiÃªu thá»¥ dá»¯ liá»‡u dense trong bá»™ nhá»› chia sáº» vÃ  táº¡o ra tile Ä‘áº§u ra thÃ´ng qua cÃ¡c tÃ­nh toÃ¡n tensor core.

Fig.6b cho tháº¥y hÃ nh vi kernel tá»•ng thá»ƒ cá»§a Flash-LLM tá»« khÃ­a cáº¡nh microarchitecture. Bá»™ nhá»› chia sáº» Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m khÃ´ng gian lÃ m viá»‡c Ä‘á»ƒ lÆ°u trá»¯ dá»¯ liá»‡u Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« Ä‘á»‹nh dáº¡ng thÆ°a sang dense, nÆ¡i táº¥t cáº£ cÃ¡c thread trong thread block lÃ m viá»‡c cÃ¹ng nhau má»™t cÃ¡ch há»£p tÃ¡c Ä‘á»ƒ táº£i sparse encoding (SE)âµ cá»§a ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ tá»« bá»™ nhá»› toÃ n cáº§u vÃ  lÆ°u trá»¯ chÃºng trong bá»™ nhá»› chia sáº» vá»›i Ä‘á»‹nh dáº¡ng dense. LÆ°u Ã½ ráº±ng má»—i tile trÃªn bá»™ nhá»› toÃ n cáº§u Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh sparse encoding vá»›i Ã­t byte hÆ¡n so vá»›i Ä‘á»‹nh dáº¡ng dense, do Ä‘Ã³ cÃ¡c chuyá»ƒn Ä‘á»™ng dá»¯ liá»‡u bá»™ nhá»› toÃ n cáº§u tÆ°Æ¡ng á»©ng cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£m so vá»›i MatMul dense. Cá»¥ thá»ƒ, Ã½ tÆ°á»Ÿng cÆ¡ báº£n cá»§a Sparse-to-Dense Transformation lÃ  trÃ­ch xuáº¥t cÃ¡c pháº§n tá»­ khÃ¡c khÃ´ng tá»« sparse encoding trÃªn bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng á»©ng cá»§a chÃºng trong Ä‘á»‹nh dáº¡ng dense trÃªn bá»™ nhá»› chia sáº» trong khi cÃ¡c vá»‹ trÃ­ khÃ¡c Ä‘Æ°á»£c Ä‘iá»n báº±ng sá»‘ khÃ´ng. ChÃºng tÃ´i sá»­ dá»¥ng cÃ¡c thanh ghi phÃ¢n tÃ¡n lÃ m bá»™ Ä‘á»‡m trung gian Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c pháº§n tá»­ khÃ¡c khÃ´ng trÆ°á»›c khi trÃ­ch xuáº¥t chÃºng vÃ o bá»™ nhá»› chia sáº». ChÃºng tÃ´i khÃ´ng sá»­ dá»¥ng bá»™ nhá»› chia sáº» lÃ m bá»™ Ä‘á»‡m trung gian nÃ y Ä‘á»ƒ trÃ¡nh truy cáº­p bá»™ nhá»› chia sáº» vÃ²ng quanh cá»§a sparse encoding.

âµ ChÃºng tÃ´i gá»i dá»¯ liá»‡u cá»§a ğ´ á»Ÿ Ä‘á»‹nh dáº¡ng thÆ°a lÃ  sparse encoding trong bÃ i bÃ¡o nÃ y.

### 4.2 Thiáº¿t káº¿ Pipeline TÃ­nh toÃ¡n cá»§a Flash-LLM

Do má»—i thread/thread-block tiÃªu thá»¥ má»™t pháº§n lá»›n cá»§a tá»•ng thá»ƒ registers/shared-memory lÃ m bá»™ Ä‘á»‡m cho tiling, song song cáº¥p thread GPU (TLP) vá»‘n dÄ© tháº¥p. Do Ä‘Ã³, viá»‡c tá»‘i Æ°u hÃ³a song song cáº¥p lá»‡nh lÃ  quan trá»ng. ChÃºng tÃ´i mÃ´ táº£ pipeline pháº§n má»m cá»§a Flash-LLM trong pháº§n nÃ y nÆ¡i viá»‡c táº£i bá»™ nhá»› off-chip (Sparse-to-Dense Transformation) vÃ  cÃ¡c tÃ­nh toÃ¡n tensor core Ä‘Æ°á»£c xá»­ lÃ½ theo cÃ¡ch pipeline hiá»‡u quáº£.

#### 4.2.1 Chá»“ng chÃ©o Hai cáº¥p cá»§a Bá»™ nhá»› vÃ  TÃ­nh toÃ¡n.

NhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong Sec.4.1, nÃ³ yÃªu cáº§u má»™t sá»‘ giai Ä‘oáº¡n Ä‘á»ƒ táº£i sparse encoding tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n bá»™ nhá»› chia sáº» á»Ÿ Ä‘á»‹nh dáº¡ng dense cho má»—i ğ´ğ‘‡ğ‘–ğ‘™ğ‘’. Cá»¥ thá»ƒ, nÃ³ yÃªu cáº§u táº£i sparse encoding tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n cÃ¡c thanh ghi phÃ¢n tÃ¡n (giai Ä‘oáº¡n gmem2reg), Ä‘áº·t láº¡i bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» Ä‘Ã­ch báº±ng sá»‘ khÃ´ng (giai Ä‘oáº¡n rst_smem), vÃ  ghi sparse encoding tá»« thanh ghi Ä‘áº¿n cÃ¡c vá»‹ trÃ­ tÆ°Æ¡ng á»©ng trÃªn bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» (giai Ä‘oáº¡n extract). Äá»‘i vá»›i ğµğ‘‡ğ‘–ğ‘™ğ‘’, Ä‘Ã£ á»Ÿ Ä‘á»‹nh dáº¡ng dense, nÃ³ chá»‰ yÃªu cáº§u táº£i trá»±c tiáº¿p tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» Ä‘Ã­ch (giai Ä‘oáº¡n ld_dense). Cuá»‘i cÃ¹ng, giai Ä‘oáº¡n smem2tc táº£i dá»¯ liá»‡u bá»™ nhá»› chia sáº» cá»§a ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ vÃ  ğµğ‘‡ğ‘–ğ‘™ğ‘’ vÃ  thá»±c hiá»‡n cÃ¡c tÃ­nh toÃ¡n tensor core.

NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.6c, Flash-LLM khai thÃ¡c chá»“ng chÃ©o hai cáº¥p cá»§a cÃ¡c giai Ä‘oáº¡n bá»™ nhá»› vÃ  tÃ­nh toÃ¡n trÃªn Ä‘á»ƒ thá»±c thi hiá»‡u quáº£. Má»™t máº·t, nÃ³ táº­n dá»¥ng pipeline pháº§n má»m thÃ´ng qua double buffering Ä‘á»ƒ chá»“ng chÃ©o táº£i bá»™ nhá»› off-chip vÃ  Sparse-to-Dense transformation vá»›i tÃ­nh toÃ¡n tensor core, gá»i lÃ  chá»“ng chÃ©o inter-iteration. Máº·t khÃ¡c, nÃ³ chá»“ng chÃ©o cÃ¡c giai Ä‘oáº¡n táº£i bá»™ nhá»› off-chip trong Sparse-to-Dense transformation cho cÃ¡c hoáº¡t Ä‘á»™ng bá»™ nhá»› hiá»‡u quáº£ hÆ¡n, gá»i lÃ  chá»“ng chÃ©o intra-iteration. Trá»¥c ngang cá»§a Fig.6c biá»ƒu thá»‹ thá»i gian thá»±c thi trong khi trá»¥c dá»c biá»ƒu thá»‹ cÃ¡c hoáº¡t Ä‘á»™ng vá»›i double buffer. NÃ³ sá»­ dá»¥ng hai bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» cho ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ (tÆ°Æ¡ng á»©ng vá»›i A1 vÃ  A2) vÃ  ğµğ‘‡ğ‘–ğ‘™ğ‘’ (tÆ°Æ¡ng á»©ng vá»›i B1 vÃ  B2), vÃ  má»™t bá»™ Ä‘á»‡m thanh ghi Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng trong cÃ¡c láº§n láº·p khÃ¡c nhau (tÆ°Æ¡ng á»©ng vá»›i SE). Cá»¥ thá»ƒ, SE trong Iteration-1/3 (Iteration-2/4) vÃ  A1 (A2) tÆ°Æ¡ng á»©ng vá»›i quÃ¡ trÃ¬nh Sparse-to-Dense transformation cá»§a ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ trÃªn táº­p bá»™ Ä‘á»‡m thá»© nháº¥t (thá»© hai), vÃ  B1 (B2) tÆ°Æ¡ng á»©ng vá»›i chuyá»ƒn Ä‘á»™ng dá»¯ liá»‡u cá»§a ğµğ‘‡ğ‘–ğ‘™ğ‘’ trÃªn táº­p bá»™ Ä‘á»‡m thá»© nháº¥t (thá»© hai). Äá»‘i vá»›i chá»“ng chÃ©o inter-iteration, nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Iteration-2 trong Fig.6c, trong khi táº£i dá»¯ liá»‡u tá»« bá»™ nhá»› chia sáº» vÃ  thá»±c hiá»‡n cÃ¡c tÃ­nh toÃ¡n tensor core cho dá»¯ liá»‡u trÃªn táº­p bá»™ Ä‘á»‡m thá»© nháº¥t, Flash-LLM táº£i vÃ  trÃ­ch xuáº¥t dá»¯ liá»‡u tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n bá»™ nhá»› chia sáº» cho táº­p bá»™ Ä‘á»‡m thá»© hai. Äá»‘i vá»›i chá»“ng chÃ©o intra-iteration, cÃ¡c hoáº¡t Ä‘á»™ng cá»§a A1 vÃ  B1 Ä‘Æ°á»£c xá»­ lÃ½ song song, vÃ  cÃ¡c giai Ä‘oáº¡n gmem2reg vÃ  rst_smem trÃªn ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ cÅ©ng Ä‘Æ°á»£c xá»­ lÃ½ song song. Báº±ng cÃ¡ch nÃ y, viá»‡c táº£i dá»¯ liá»‡u thÆ°a, táº£i dá»¯ liá»‡u dense, vÃ  cÃ¡c tÃ­nh toÃ¡n tensor core cÃ³ thá»ƒ Ä‘Æ°á»£c chá»“ng chÃ©o hiá»‡u quáº£.

Má»™t thiáº¿t káº¿ quan trá»ng cho Sparse-to-Dense transformation lÃ  sá»­ dá»¥ng rÃµ rÃ ng cÃ¡c thanh ghi lÃ m bá»™ Ä‘á»‡m dá»¯ liá»‡u giá»¯a bá»™ nhá»› toÃ n cáº§u vÃ  bá»™ nhá»› chia sáº». Trong Flash-LLM, viá»‡c chuyá»ƒn Ä‘á»™ng sparse encoding tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n bá»™ nhá»› chia sáº» Ä‘Æ°á»£c chia rÃµ rÃ ng thÃ nh hai giai Ä‘oáº¡n, tá»©c lÃ  lá»‡nh LDG (táº£i dá»¯ liá»‡u tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n thanh ghi) trong gmem2reg vÃ  lá»‡nh STS (lÆ°u trá»¯ dá»¯ liá»‡u vÃ o bá»™ nhá»› chia sáº» tá»« thanh ghi) trong extract nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.6c. Má»™t máº·t, thiáº¿t káº¿ hai giai Ä‘oáº¡n chia giÃºp tÄƒng instruction-level-parallelism (ILP) Ä‘á»ƒ áº©n Ä‘á»™ trá»… truy cáº­p bá»™ nhá»› toÃ n cáº§u cao. LÆ°u Ã½ ráº±ng má»—i cáº·p lá»‡nh LDG vÃ  STS cÃ³ phá»¥ thuá»™c load-use. Náº¿u chÃºng ta khÃ´ng chia gmem2reg vÃ  extract thÃ nh hai giai Ä‘oáº¡n mÃ  khá»Ÿi cháº¡y tá»«ng cáº·p lá»‡nh LDG vÃ  STS trong cÃ¡c chu ká»³ liá»n ká» (vÃ­ dá»¥ lÆ°u trá»¯ trá»±c tiáº¿p vÃ o bá»™ nhá»› chia sáº» sau khi táº£i tá»« bá»™ nhá»› toÃ n cáº§u), má»—i GPU thread sáº½ thá»±c hiá»‡n cÃ¡c lá»‡nh theo thá»© tá»± ğ¿ğ·ğºâ‚€,ğ‘†ğ‘‡ğ‘†â‚€,ğ¿ğ·ğºâ‚,ğ‘†ğ‘‡ğ‘†â‚,... mÃ  khÃ´ng cÃ³ ILP cá»§a táº£i bá»™ nhá»› toÃ n cáº§u (tá»©c lÃ  LDG). Báº±ng cÃ¡ch chia hai lá»‡nh thÃ nh hai giai Ä‘oáº¡n nhÆ° trong Fig.6c, thá»© tá»± thá»±c thi sáº½ lÃ  ğ¿ğ·ğºâ‚€,ğ¿ğ·ğºâ‚,...,ğ‘†ğ‘‡ğ‘†â‚€,ğ‘†ğ‘‡ğ‘†â‚,... vÃ  dáº«n Ä‘áº¿n ILP cao cá»§a táº£i bá»™ nhá»› toÃ n cáº§u. Máº·t khÃ¡c, viá»‡c chia chuyá»ƒn Ä‘á»™ng dá»¯ liá»‡u thÃ nh gmem2reg vÃ  extract cho phÃ©p cÆ¡ há»™i chá»“ng chÃ©o giá»¯a gmem2reg vÃ  rst_smem. LÆ°u Ã½ ráº±ng cÃ¡c lá»‡nh STS trong extract khÃ´ng nÃªn Ä‘Æ°á»£c khá»Ÿi cháº¡y trÆ°á»›c khi hoÃ n thÃ nh giai Ä‘oáº¡n rst_smem, náº¿u khÃ´ng, dá»¯ liá»‡u Ä‘Æ°á»£c ghi bá»Ÿi cÃ¡c lá»‡nh STS cÃ³ thá»ƒ bá»‹ ghi Ä‘Ã¨ bá»Ÿi rst_smem má»™t cÃ¡ch sai láº§m. Viá»‡c thá»±c thi gmem2reg vÃ  rst_smem cÃ³ thá»ƒ Ä‘Æ°á»£c chá»“ng chÃ©o má»™t khi gmem2reg khÃ´ng chá»©a ghi bá»™ nhá»› chia sáº» (táº¥t cáº£ cÃ¡c lá»‡nh STS Ä‘Æ°á»£c gÃ¡n cho giai Ä‘oáº¡n extract), Ä‘iá»u nÃ y cÃ ng tÄƒng ILP.

#### 4.2.2 Pháº¡m vi Tá»‘i thiá»ƒu cá»§a Äá»“ng bá»™ hÃ³a vÃ  RÃ o cáº£n Bá»™ nhá»›

Do pipeline phá»©c táº¡p trong Fig.6c, nÃ³ yÃªu cáº§u má»™t táº­p há»£p Ä‘á»“ng bá»™ hÃ³a thread vÃ  rÃ o cáº£n bá»™ nhá»› Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh Ä‘Ãºng Ä‘áº¯n. Flash-LLM chÃ¨n pháº¡m vi tá»‘i thiá»ƒu cá»§a Ä‘á»“ng bá»™ hÃ³a vÃ  rÃ o cáº£n bá»™ nhá»› Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh Ä‘Ãºng Ä‘áº¯n trong khi giá»¯ chá»“ng chÃ©o.

Äá»ƒ trÃ¡nh dá»¯ liá»‡u Ä‘Æ°á»£c ghi bá»Ÿi extract bá»‹ ghi Ä‘Ã¨ bá»Ÿi rst_smem má»™t cÃ¡ch sai láº§m, Flash-LLM chÃ¨n Ä‘á»“ng bá»™ hÃ³a cáº¥p thread-block rÃµ rÃ ng giá»¯a hai giai Ä‘oáº¡n Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c thread Ä‘Ã£ hoÃ n thÃ nh cÃ´ng viá»‡c Ä‘áº·t láº¡i bá»™ Ä‘á»‡m A1/A2 trong bá»™ nhá»› chia sáº», Ä‘Æ°á»£c hiá»ƒn thá»‹ nhÆ° cÃ¡c Ä‘Æ°á»ng mÃ u vÃ ng Ä‘áº§u tiÃªn trong má»—i láº§n láº·p trong Fig.6c. Trong khi Ä‘Ã³, nÃ³ cÅ©ng yÃªu cáº§u má»™t Ä‘á»“ng bá»™ hÃ³a khÃ¡c Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng táº¥t cáº£ chuyá»ƒn Ä‘á»™ng dá»¯ liá»‡u vÃ  hoáº¡t Ä‘á»™ng tensor core cá»§a láº§n láº·p hiá»‡n táº¡i Ä‘Æ°á»£c hoÃ n thÃ nh trÆ°á»›c khi báº¯t Ä‘áº§u láº§n láº·p tiáº¿p theo, Ä‘Æ°á»£c hiá»ƒn thá»‹ nhÆ° cÃ¡c Ä‘Æ°á»ng mÃ u vÃ ng thá»© hai trong má»—i láº§n láº·p trong Fig.6c. Láº¥y Iteration-1 lÃ m vÃ­ dá»¥, chÃºng ta nÃªn Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c thread Ä‘Ã£ hoÃ n thÃ nh viá»‡c ghi cÃ¡c bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» A1 vÃ  B1 trÆ°á»›c khi chÃºng ta báº¯t Ä‘áº§u Iteration-2, vÃ¬ dá»¯ liá»‡u trÃªn cÃ¡c bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» sáº½ Ä‘Æ°á»£c táº£i vÃ o tensor core lÃ m Ä‘áº§u vÃ o trong Iteration-2. BÃªn cáº¡nh Ä‘Ã³, chÃºng ta pháº£i Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c thread Ä‘Ã£ hoÃ n thÃ nh viá»‡c Ä‘á»c dá»¯ liá»‡u tá»« bá»™ Ä‘á»‡m thanh ghi cho extract trong Iteration-1 trÆ°á»›c khi Ä‘á»ƒ chÃºng bá»‹ ghi Ä‘Ã¨ bá»Ÿi rst_smem trong Iteration-2.

BÃªn cáº¡nh cÃ¡c Ä‘á»“ng bá»™ hÃ³a thread-block, nÃ³ cÅ©ng yÃªu cáº§u rÃ o cáº£n bá»™ nhá»› sau cÃ¡c hoáº¡t Ä‘á»™ng sao chÃ©p khÃ´ng Ä‘á»“ng bá»™ cá»§a chuyá»ƒn Ä‘á»™ng dá»¯ liá»‡u toÃ n cáº§u-Ä‘áº¿n-chia sáº». Flash-LLM sá»­ dá»¥ng cÃ¡c primitive sao chÃ©p khÃ´ng Ä‘á»“ng bá»™ trÃªn GPU Ä‘á»ƒ chá»“ng chÃ©o chuyá»ƒn Ä‘á»™ng dá»¯ liá»‡u vÃ  cÃ¡c hoáº¡t Ä‘á»™ng khÃ¡c. LÆ°u Ã½ ráº±ng primitive sao chÃ©p khÃ´ng Ä‘á»“ng bá»™ cp.async, báº¯t Ä‘áº§u tá»« GPU NVIDIA Ampere [36], cho phÃ©p chuyá»ƒn dá»¯ liá»‡u tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n bá»™ nhá»› chia sáº» trong ná»n khÃ´ng Ä‘á»“ng bá»™ trong khi thá»±c hiá»‡n cÃ¡c tÃ­nh toÃ¡n khÃ¡c á»Ÿ foreground. Cá»¥ thá»ƒ, cáº£ giai Ä‘oáº¡n rst_smem vÃ  ld_dense Ä‘á»u sá»­ dá»¥ng cÃ¡c primitive cp.async Ä‘á»ƒ cho phÃ©p thá»±c thi chá»“ng chÃ©o trÃªn double buffer.

Äá»ƒ cho phÃ©p thá»±c thi pipeline chi tiáº¿t, Flash-LLM sá»­ dá»¥ng cÃ¡c rÃ o cáº£n async-copy khÃ¡c nhau cho cÃ¡c giai Ä‘oáº¡n rst_smem vÃ  ld_dense. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.6c, giai Ä‘oáº¡n extract chá» hoÃ n thÃ nh chá»‰ rst_smem, trong khi rÃ o cáº£n thread-block cuá»‘i cÃ¹ng cá»§a má»—i láº§n láº·p chá» hoÃ n thÃ nh táº¥t cáº£ cÃ¡c hoáº¡t Ä‘á»™ng cp.async trÆ°á»›c Ä‘Ã³. Báº±ng cÃ¡ch nÃ y, giai Ä‘oáº¡n extract cÃ³ thá»ƒ Ä‘Æ°á»£c chá»“ng chÃ©o vá»›i cÃ¡c giai Ä‘oáº¡n ld_dense.

#### 4.2.3 Triá»ƒn khai Tá»•ng thá»ƒ

Alg.1 cho tháº¥y cÃ¡c triá»ƒn khai cá»§a tÃ­nh toÃ¡n pipeline trong Flash-LLM. Trong dÃ²ng 3, pipeline pháº§n má»m sáº½ Ä‘Æ°á»£c khá»Ÿi táº¡o, chuáº©n bá»‹ dá»¯ liá»‡u cá»§a ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ vÃ  ğµğ‘‡ğ‘–ğ‘™ğ‘’ trÃªn bá»™ nhá»› chia sáº» cho cÃ¡c tÃ­nh toÃ¡n tensor core cá»§a láº§n láº·p Ä‘áº§u tiÃªn trong vÃ²ng láº·p chÃ­nh. CÃ¡c láº§n láº·p tiáº¿p theo Ä‘Æ°á»£c mÃ´ táº£ trong Fig.6c Ä‘Æ°á»£c triá»ƒn khai trong dÃ²ng 7-28. Äá»‘i vá»›i má»—i láº§n láº·p, nÃ³ phÃ¡t hÃ nh cÃ¡c lá»‡nh cho viá»‡c táº£i dá»¯ liá»‡u khÃ´ng Ä‘á»“ng bá»™ cho láº§n láº·p tiáº¿p theo vÃ  thá»±c hiá»‡n tÃ­nh toÃ¡n tensor core cá»§a láº§n láº·p hiá»‡n táº¡i theo cÃ¡ch double buffer. Cá»¥ thá»ƒ, má»™t ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ cho láº§n láº·p tiáº¿p theo sáº½ Ä‘Æ°á»£c táº£i vÃ  trÃ­ch xuáº¥t tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n bá»™ nhá»› chia sáº» (rst_smem, gmem2reg, vÃ  extract), vÃ  má»™t ğµğ‘‡ğ‘–ğ‘™ğ‘’ dense sáº½ Ä‘Æ°á»£c táº£i trá»±c tiáº¿p tá»« bá»™ nhá»› toÃ n cáº§u (ld_dense). Giai Ä‘oáº¡n rst_smem á»Ÿ dÃ²ng 17, nÆ¡i má»—i thread phÃ¡t hÃ nh hoáº¡t Ä‘á»™ng cp.async Ä‘á»ƒ Ä‘áº·t bá»™ Ä‘á»‡m A thÃ nh sá»‘ khÃ´ng. Trong dÃ²ng 19, gmem2reg Ä‘Æ°á»£c hoÃ n thÃ nh, nÆ¡i sparse encoding Ä‘Æ°á»£c táº£i tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n cÃ¡c thanh ghi phÃ¢n tÃ¡n. Giai Ä‘oáº¡n ld_dense á»Ÿ dÃ²ng 20, nÆ¡i dá»¯ liá»‡u cho ğµğ‘‡ğ‘–ğ‘™ğ‘’ Ä‘Æ°á»£c táº£i tá»« bá»™ nhá»› toÃ n cáº§u Ä‘áº¿n bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» vá»›i cÃ¡c hoáº¡t Ä‘á»™ng cp.async. Sau khi khá»Ÿi cháº¡y cÃ¡c hoáº¡t Ä‘á»™ng bá»™ nhá»› khÃ´ng Ä‘á»“ng bá»™ nÃ y, cÃ¡c hoáº¡t Ä‘á»™ng tensor core Ä‘Æ°á»£c khá»Ÿi cháº¡y trong dÃ²ng 23. LÆ°u Ã½ ráº±ng chÃºng tÃ´i táº£i cÃ¡c ma tráº­n dense tá»« bá»™ nhá»› chia sáº» Ä‘áº¿n thanh ghi sá»­ dá»¥ng lá»‡nh ldmatrix.sync vÃ  sá»­ dá»¥ng tensor core cho cÃ¡c tÃ­nh toÃ¡n cá»‘t lÃµi báº±ng cÃ¡ch khá»Ÿi cháº¡y rÃµ rÃ ng lá»‡nh mma.sync trong hÃ m Pipelined_Shared2Reg_TensorCoreOps().

RÃ o cáº£n async-copy Ä‘áº§u tiÃªn trong Fig.6c á»Ÿ dÃ²ng 25, Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c hoáº¡t Ä‘á»™ng khÃ´ng Ä‘á»“ng bá»™ Ä‘Æ°á»£c khá»Ÿi cháº¡y trong dÃ²ng 17 Ä‘Æ°á»£c hoÃ n thÃ nh trong khi cÃ¡c hoáº¡t Ä‘á»™ng Ä‘Æ°á»£c khá»Ÿi cháº¡y trong dÃ²ng 20 váº«n cÃ³ thá»ƒ Ä‘ang tiáº¿n hÃ nh. Giai Ä‘oáº¡n extract á»Ÿ dÃ²ng 26, nÆ¡i dá»¯ liá»‡u trÃªn cÃ¡c thanh ghi phÃ¢n tÃ¡n Ä‘Æ°á»£c trÃ­ch xuáº¥t vÃ o bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» cá»§a ğ´ğ‘‡ğ‘–ğ‘™ğ‘’. Cuá»‘i cÃ¹ng, cÃ¡c rÃ o cáº£n async-copy vÃ  thread-block Ä‘Æ°á»£c gá»i trong dÃ²ng 28 Ä‘á»ƒ Ä‘áº£m báº£o ráº±ng táº¥t cáº£ cÃ¡c thread Ä‘Ã£ hoÃ n thÃ nh cÃ´ng viá»‡c cá»§a chÃºng trong láº§n láº·p nÃ y.

KhÃ¡c vá»›i MatMul dense nÆ¡i kÃ­ch thÆ°á»›c dá»¯ liá»‡u Ä‘Æ°á»£c táº£i tá»« bá»™ nhá»› toÃ n cáº§u cÃ³ thá»ƒ Ä‘Æ°á»£c suy luáº­n báº±ng kÃ­ch thÆ°á»›c tile, kÃ­ch thÆ°á»›c cá»§a sparse encoding Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi sá»‘ lÆ°á»£ng khÃ¡c khÃ´ng (nnz) trong ğ´ğ‘‡ğ‘–ğ‘™ğ‘’, khÃ´ng thá»ƒ dá»± Ä‘oÃ¡n Ä‘Æ°á»£c. TrÆ°á»›c khi táº£i sparse encoding cho má»—i tile, Flash-LLM xÃ¡c Ä‘á»‹nh offset báº¯t Ä‘áº§u vÃ  Ä‘á»™ dÃ i cá»§a nÃ³ trong cÃ¡c bá»™ Ä‘á»‡m bá»™ nhá»› toÃ n cáº§u. ThÃ´ng tin nhÆ° váº­y Ä‘Æ°á»£c duy trÃ¬ trong máº£ng TileOffsets, Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› toÃ n cáº§u (chi tiáº¿t thÃªm trong Sec.4.3). Äá»ƒ trÃ¡nh stall lá»‡nh do truy cáº­p bá»™ nhá»› toÃ n cáº§u Ä‘á»™ trá»… dÃ i, metadata nÃ y nÃªn Ä‘Æ°á»£c pre-fetch. Trong dÃ²ng 5-6, offset báº¯t Ä‘áº§u vÃ  kÃ­ch thÆ°á»›c cá»§a sparse encoding cho láº§n láº·p Ä‘áº§u tiÃªn Ä‘Æ°á»£c pre-fetch. á» Ä‘áº§u má»—i láº§n láº·p, offset báº¯t Ä‘áº§u vÃ  kÃ­ch thÆ°á»›c cá»§a ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ hiá»‡n táº¡i Ä‘Æ°á»£c cáº­p nháº­t sá»­ dá»¥ng giÃ¡ trá»‹ trong thanh ghi Ä‘Æ°á»£c pre-fetch trÆ°á»›c. Trong dÃ²ng 11-12, nÃ³ pre-fetch offset báº¯t Ä‘áº§u vÃ  Ä‘á»™ dÃ i cho láº§n láº·p tiáº¿p theo.

**Thuáº­t toÃ¡n 1** MÃ£ giáº£ kernel SpMM Flash-LLM.
```
1: Inputs: SparseMatrix ğ´, Matrix ğµ
2: Output: Matrix ğ¶
3: ğ¼ğ‘›ğ‘–ğ‘¡ğ‘–ğ‘ğ‘™ğ‘–ğ‘§ğ‘’_ğ‘ƒğ‘–ğ‘ğ‘’ğ‘™ğ‘–ğ‘›ğ‘’();
4: ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ = ğ‘ ğ‘¢ğ‘ğ´ğ‘Ÿğ‘Ÿğ‘ğ‘¦(ğ´.ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡);
5: int ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ğ‘ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘¡ğ‘â„ = ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡[1];
6: int ğ‘›ğ‘›ğ‘§ğ‘ï¿½ï¿½ğ‘’ğ‘“ğ‘’ğ‘¡ğ‘â„ = ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡[2] âˆ’ ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡[1];
7: for int ğ‘–ğ‘‘ = 0; ğ‘–ğ‘‘ < ğ¾ğºğ‘™ğ‘œğ‘ğ‘ğ‘™/ğ¾; ğ‘–ğ‘‘++ do
8:     // Prefetch startIdx and nnz.
9:     int ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ = ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ğ‘ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘¡ğ‘â„;
10:    int ğ‘›ğ‘›ğ‘§ = ğ‘›ğ‘›ğ‘§ğ‘ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘¡ğ‘â„;
11:    ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ğ‘ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘¡ğ‘â„ = ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡[ğ‘–ğ‘‘ + 2];
12:    ğ‘›ğ‘›ğ‘§ğ‘ğ‘Ÿğ‘’ğ‘“ğ‘’ğ‘¡ğ‘â„ = ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡[ğ‘–ğ‘‘ + 3] âˆ’ ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡[ğ‘–ğ‘‘ + 2];
13:    // Set pointers for double-buffer.
14:    half* ğ‘ ğ‘šğ‘’ğ‘š_ğ‘¤ = ğ‘ ğ‘šğ‘’ğ‘š + ((ğ‘–ğ‘‘ + 1) % 2) âˆ— ğ‘‚ğ¹ğ¹ğ‘†ğ¸ğ‘‡;
15:    half* ğ‘ ğ‘šğ‘’ğ‘š_ğ‘Ÿ = ğ‘ ğ‘šğ‘’ğ‘š + (ğ‘–ğ‘‘ % 2) âˆ— ğ‘‚ğ¹ğ¹ğ‘†ğ¸ğ‘‡;
16:    // Launch Asynchronous Memory Operations.
17:    ğ¼ğ‘›ğ‘–ğ‘¡ğ‘†â„ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘€ğ‘’ğ‘š(ğ‘ ğ‘šğ‘’ğ‘š_ğ‘¤); âŠ² rst_smem
18:    ğ‘ğ‘_ğ‘ğ‘ ğ‘¦ğ‘›ğ‘_ğ‘ğ‘œğ‘šğ‘šğ‘–ğ‘¡();
19:    ğ¶ğ‘œğ‘ğ‘¦ğºğ‘™ğ‘œğ‘ğ‘ğ‘™2ğ‘…ğ‘’ğ‘”(ğ´.ğ‘›ğ‘§ + ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡, ğ‘›ğ‘›ğ‘§) âŠ² gmem2reg
20:    ğ¶ğ‘œğ‘ğ‘¦ğºğ‘™ğ‘œğ‘ğ‘ğ‘™2ğ‘†â„ğ‘ğ‘Ÿğ‘’ğ‘‘(ğ‘ ğ‘šğ‘’ğ‘š_ğ‘¤, ğµ.ğ‘‘ğ‘ğ‘¡ğ‘) âŠ² ld_dense
21:    ğ‘ğ‘_ğ‘ğ‘ ğ‘¦ğ‘›ğ‘_ğ‘ğ‘œğ‘šğ‘šğ‘–ğ‘¡();
22:    // Math Computations.
23:    ğ‘ƒğ‘–ğ‘ğ‘’ğ‘™ğ‘–ğ‘›ğ‘’ğ‘‘_ğ‘†â„ğ‘ğ‘Ÿğ‘’ğ‘‘2ğ‘…ğ‘’ğ‘”_ğ‘‡ğ‘’ğ‘›ğ‘ ğ‘œğ‘Ÿğ¶ğ‘œğ‘Ÿğ‘’ğ‘‚ğ‘ğ‘ (ğ‘ ğ‘šğ‘’ğ‘š_ğ‘Ÿ);
24:    // barrier: initSharedMem()
25:    ğ‘ğ‘_ğ‘ğ‘ ğ‘¦ğ‘›ğ‘_ğ‘¤ğ‘ğ‘–ğ‘¡ <1>(); __ğ‘ ğ‘¦ğ‘›ğ‘ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ ();
26:    ğ¸ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ğ‘…ğ‘’ğ‘”ğ‘–ğ‘ ğ‘¡ğ‘’ğ‘Ÿ2ğ‘†â„ğ‘ğ‘Ÿğ‘’ğ‘‘(ğ‘ ğ‘šğ‘’ğ‘š_ğ‘¤) âŠ² extract
27:    // barrier: copyGlobal2Shared().
28:    ğ‘ğ‘_ğ‘ğ‘ ğ‘¦ğ‘›ğ‘_ğ‘¤ğ‘ğ‘–ğ‘¡ <0>(); __ğ‘ ğ‘¦ğ‘›ğ‘ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ğ‘ ();
29:    ğ‘Ÿğ‘’ğ‘ ğ‘¢ğ‘™ğ‘¡ğ‘ _ğ‘…ğ‘’ğ‘”2ğºğ‘™ğ‘œğ‘ğ‘ğ‘™(ğ¶.ğ‘‘ğ‘ğ‘¡ğ‘);
```

### 4.3 MÃ£ hÃ³a ThÆ°a vÃ  PhÃ¢n tÃ­ch Runtime

#### 4.3.1 Äá»‹nh dáº¡ng Tiled-CSL

Äá»‹nh dáº¡ng mÃ£ hÃ³a thÆ°a cá»§a ma tráº­n ğ´ lÃ  cáº§n thiáº¿t cho lÆ°u trá»¯ dá»¯ liá»‡u thÆ°a hiá»‡u quáº£ vÃ  Sparse-to-Dense Transformation. ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t Ä‘á»‹nh dáº¡ng mÃ£ hÃ³a thÆ°a tile-by-tile Ä‘á»ƒ há»— trá»£ cÃ¡c tá»‘i Æ°u hÃ³a trong Sec.4.2 má»™t cÃ¡ch hiá»‡u quáº£. CÃ¡c pháº§n tá»­ khÃ¡c khÃ´ng Ä‘Æ°á»£c tá»• chá»©c tile-by-tile, nÆ¡i má»—i tile duy trÃ¬ cÃ¡c pháº§n tá»­ khÃ¡c khÃ´ng cá»§a nÃ³ kÃ¨m theo chá»‰ sá»‘ thÆ°a. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.7, dá»¯ liá»‡u cá»§a má»—i tile trong ma tráº­n thÆ°a Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh má»™t máº£ng nhá», vÃ  káº¿t há»£p táº¥t cáº£ cÃ¡c tile sáº½ táº¡o thÃ nh máº£ng tá»•ng thá»ƒ (NonZeros Array). TileOffsets Array duy trÃ¬ offset báº¯t Ä‘áº§u cá»§a má»—i tile trong NonZeros Array. Sá»‘ lÆ°á»£ng pháº§n tá»­ khÃ¡c khÃ´ng cá»§a má»—i tile Tiled-CSL lÃ  sá»± khÃ¡c biá»‡t giá»¯a hai pháº§n tá»­ tÆ°Æ¡ng á»©ng trong TileOffsets Array. Äá»‘i vá»›i má»—i tile trong NonZeros Array, má»—i pháº§n tá»­ Ä‘Æ°á»£c lÆ°u trá»¯ cÃ¹ng vá»›i vá»‹ trÃ­ cá»§a nÃ³ trong tile. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.7, má»—i trá»ng sá»‘ khÃ¡c khÃ´ng á»Ÿ Ä‘á»™ chÃ­nh xÃ¡c half 16-bit vÃ  má»—i vá»‹ trÃ­ Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh má»™t sá»‘ nguyÃªn short 16-bit.

#### 4.3.2 TrÃ­ch xuáº¥t Dá»¯ liá»‡u tá»« Register Ä‘áº¿n Shared Memory dá»±a trÃªn Äá»‹nh dáº¡ng Tiled-CSL

NhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong Alg.2, má»—i thread trÃ­ch xuáº¥t ğ‘›ğ‘›ğ‘§_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ giÃ¡ trá»‹ khÃ¡c khÃ´ng tá»« bá»™ Ä‘á»‡m sparse encoding Reg[] cá»§a nÃ³ Ä‘áº¿n bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº» A vá»›i má»™t vÃ²ng láº·p. CÃ¡c hÃ m v() vÃ  idx() Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ trÃ­ch xuáº¥t giÃ¡ trá»‹ (16 bit cao) vÃ  vá»‹ trÃ­ cá»§a nÃ³ (16 bit tháº¥p). CÃ³ má»™t sá»‘ cÃ¢n nháº¯c Ä‘áº·c biá»‡t khi sá»­ dá»¥ng thanh ghi lÃ m bá»™ Ä‘á»‡m trung gian cho sparse encoding. KhÃ¡c vá»›i bá»™ nhá»› chia sáº» vÃ  bá»™ nhá»› toÃ n cáº§u, thanh ghi GPU khÃ´ng thá»ƒ Ä‘á»‹a chá»‰ hÃ³a, cÃ³ nghÄ©a lÃ  chÃºng ta khÃ´ng thá»ƒ truy cáº­p má»™t máº£ng thanh ghi sá»­ dá»¥ng offset biáº¿n. Do Ä‘Ã³, viá»‡c Ã©p má»™t máº£ng Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a trong CUDA vÃ o thanh ghi yÃªu cáº§u ráº±ng, táº¥t cáº£ cÃ¡c chá»‰ sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ truy cáº­p máº£ng cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh tÄ©nh táº¡i thá»i Ä‘iá»ƒm biÃªn dá»‹ch. Náº¿u khÃ´ng, máº£ng sáº½ Ä‘Æ°á»£c lÆ°u trá»¯ trong bá»™ nhá»› toÃ n cáº§u thay tháº¿. Trong dÃ²ng 1 trong Alg.2, #pragma unroll Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ thÃ´ng bÃ¡o cho trÃ¬nh biÃªn dá»‹ch GPU hoÃ n toÃ n unroll vÃ²ng láº·p chÃ­nh, Ä‘á»ƒ táº¥t cáº£ cÃ¡c chá»‰ sá»‘ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ truy cáº­p ğ‘…ğ‘’ğ‘”[] cÃ³ thá»ƒ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh tÄ©nh. LÆ°u Ã½ ráº±ng viá»‡c thÃªm chá»‰ thá»‹ biÃªn dá»‹ch nhÆ° váº­y má»™t mÃ¬nh lÃ  khÃ´ng Ä‘á»§ vÃ¬ má»™t vÃ²ng láº·p vá»›i sá»‘ láº§n láº·p biáº¿n khÃ´ng thá»ƒ Ä‘Æ°á»£c hoÃ n toÃ n unroll. Do Ä‘Ã³, chÃºng tÃ´i sá»­ dá»¥ng má»™t giÃ¡ trá»‹ háº±ng sá»‘ #ğ‘…ğ¸ğº (giá»›i háº¡n trÃªn cá»§a sá»‘ láº§n láº·p, thÆ°á»ng lÃ  32/64 trong triá»ƒn khai cá»§a chÃºng tÃ´i) trong dÃ²ng 2 thay vÃ¬ sá»­ dá»¥ng giÃ¡ trá»‹ biáº¿n ğ‘›ğ‘›ğ‘§_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘.

**Thuáº­t toÃ¡n 2** ExtractRegister2Shared
```
1: #pragma unroll
2: for int ğ‘– = 0; ğ‘– < #ğ‘…ğ¸ğº; ğ‘–++ do
3:     if ğ‘– â‰¥ ğ‘›ğ‘›ğ‘§_ğ‘¡â„ğ‘Ÿğ‘’ğ‘ğ‘‘ then
4:         break
5:     ğ´[ğ‘–ğ‘‘ğ‘¥(ğ‘…ğ‘’ğ‘”[ğ‘–])] = ğ‘£(ğ‘…ğ‘’ğ‘”[ğ‘–])
```

#### 4.3.3 Sáº¯p xáº¿p láº¡i Dá»¯ liá»‡u ThÆ°a TrÆ°á»›c thá»i háº¡n

CÃ³ hai loáº¡i truy cáº­p bá»™ nhá»› chia sáº» trong pipeline tÃ­nh toÃ¡n cho ma tráº­n trá»ng sá»‘ thÆ°a ğ´. Má»™t lÃ  táº£i bá»™ nhá»› chia sáº» cho tÃ­nh toÃ¡n tensor core trong giai Ä‘oáº¡n smem2tc. CÃ¡i khÃ¡c lÃ  lÆ°u trá»¯ bá»™ nhá»› chia sáº» trong giai Ä‘oáº¡n extract. Viá»‡c trÃ¡nh xung Ä‘á»™t bank cá»§a cáº£ táº£i vÃ  lÆ°u trá»¯ bá»™ nhá»› chia sáº» lÃ  cáº§n thiáº¿t cho hiá»‡u suáº¥t tá»‘t. Tuy nhiÃªn, sparsity ngáº«u nhiÃªn lÃ m cho viá»‡c trÃ¡nh xung Ä‘á»™t bank cá»§a cáº£ táº£i vÃ  lÆ°u trá»¯ bá»™ nhá»› chia sáº» trá»Ÿ nÃªn thÃ¡ch thá»©c.

Äá»‘i vá»›i giai Ä‘oáº¡n smem2tc, nÃ³ sá»­ dá»¥ng intrinsic ldmatrix Ä‘á»ƒ táº£i dá»¯ liá»‡u hiá»‡u quáº£ tá»« bá»™ nhá»› chia sáº» Ä‘áº¿n thanh ghi cho tÃ­nh toÃ¡n tensor core. Fig.8a cho tháº¥y hÃ nh vi cá»§a ldmatrix nÆ¡i tÃ¡m thread cÃ¹ng nhau táº£i má»™t ma tráº­n 8Ã—8 á»Ÿ FP16 tá»« bá»™ nhá»› chia sáº». Viá»‡c Ä‘á»c ma tráº­n 8Ã—8 nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c phá»¥c vá»¥ bá»Ÿi má»™t wavefront bá»™ nhá»› chia sáº» duy nháº¥tâ¶ náº¿u khÃ´ng cÃ³ xung Ä‘á»™t bank. Viá»‡c táº£i bá»™ nhá»› khÃ´ng xung Ä‘á»™t bank cá»§a ldmatrix yÃªu cáº§u ráº±ng táº¥t cáº£ cÃ¡c vÃ´ hÆ°á»›ng trong ma tráº­n 8Ã—8 cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘á»c tá»« cÃ¡c bank bá»™ nhá»› riÃªng biá»‡t. Fig.8a cho tháº¥y má»™t vÃ­ dá»¥ vá» bá»‘ cá»¥c dá»¯ liá»‡u bá»™ nhá»› chia sáº» chá»©ng minh viá»‡c gÃ¡n bank (ID bank tá»« 1 Ä‘áº¿n 32) Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c ldmatrix khÃ´ng xung Ä‘á»™t bank. Do Ä‘Ã³, má»—i vÃ´ hÆ°á»›ng cÃ³ thá»ƒ Ä‘Æ°á»£c gÃ¡n má»™t ID bank theo vá»‹ trÃ­ cá»§a nÃ³ trong ğ´ğ‘‡ğ‘–ğ‘™ğ‘’, cho má»™t bá»‘ cá»¥c dá»¯ liá»‡u cá»¥ thá»ƒ.

Tuy nhiÃªn, yÃªu cáº§u nÃ y sáº½ dá»… dÃ ng gÃ¢y ra xung Ä‘á»™t bank cá»§a lÆ°u trá»¯ bá»™ nhá»› chia sáº» trong giai Ä‘oáº¡n extract do vá»‹ trÃ­ ngáº«u nhiÃªn cá»§a cÃ¡c pháº§n tá»­ thÆ°a trong ma tráº­n ğ´. NÃ³i cÃ¡ch khÃ¡c, chÃºng ta cÃ³ thá»ƒ Ä‘áº£m báº£o táº£i khÃ´ng xung Ä‘á»™t bank theo yÃªu cáº§u bá»‘ cá»¥c cá»§a ldmatrix, nhÆ°ng khÃ´ng thá»ƒ trÃ¡nh xung Ä‘á»™t bank lÆ°u trá»¯ theo cÃ¡ch nÃ y trong giai Ä‘oáº¡n extract. Fig.8b Ä‘Æ°a ra má»™t vÃ­ dá»¥, nÆ¡i má»—i giÃ¡ trá»‹ khÃ¡c khÃ´ng (NonZero) nÃªn Ä‘Æ°á»£c lÆ°u trá»¯ trong má»™t bank bá»™ nhá»› chia sáº» Ä‘Ã­ch (SMemBank) theo vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a chÃºng trong ğ´ğ‘‡ğ‘–ğ‘™ğ‘’ Ä‘á»ƒ Ä‘Ã¡p á»©ng yÃªu cáº§u cá»§a ldmatrix khÃ´ng xung Ä‘á»™t bank. VÃ¬ phÃ¢n phá»‘i cá»§a NonZero lÃ  ngáº«u nhiÃªn, SMemBank Ä‘Ã­ch cho má»—i giÃ¡ trá»‹ NonZero cÅ©ng ngáº«u nhiÃªn. Do káº¿t quáº£ trong Fig.8b, táº¥t cáº£ CUDA WARP (cÃ¡c NonZero cÃ³ cÃ¹ng mÃ u Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi

â¶ Má»™t wavefront lÃ  Ä‘Æ¡n vá»‹ cÃ´ng viá»‡c tá»‘i Ä‘a cÃ³ thá»ƒ Ä‘i qua pipeline pháº§n cá»©ng GPU má»—i chu ká»³. Tá»‘i Ä‘a 1,024 bit cÃ³ thá»ƒ Ä‘Æ°á»£c táº£i má»—i wavefront cho bá»™ nhá»› chia sáº».

cÃ¹ng má»™t WARP) gáº·p pháº£i xung Ä‘á»™t bank vÃ  dáº«n Ä‘áº¿n nhiá»u wavefront bá»™ nhá»› chia sáº» (SMem WF).

Äá»ƒ giáº£m xung Ä‘á»™t bank, chÃºng tÃ´i Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p sáº¯p xáº¿p láº¡i dá»¯ liá»‡u thÆ°a trÆ°á»›c thá»i háº¡n. Ã tÆ°á»Ÿng cÆ¡ báº£n lÃ  ldmatrix khÃ´ng xung Ä‘á»™t bank Ä‘Ã£ xÃ¡c Ä‘á»‹nh bank Ä‘Ã­ch cá»§a má»—i pháº§n tá»­ dá»¯ liá»‡u, do Ä‘Ã³ chÃºng ta cÃ³ thá»ƒ sáº¯p xáº¿p láº¡i cÃ¡c pháº§n tá»­ dá»¯ liá»‡u trong má»—i tile Tiled-CSL Ä‘á»ƒ cÃ¡c pháº§n tá»­ tÆ°Æ¡ng á»©ng vá»›i cÃ¡c bank khÃ¡c nhau cÃ³ thá»ƒ Ä‘Æ°á»£c tá»• chá»©c vÃ o cÃ¹ng má»™t WARP Ä‘á»ƒ thá»±c thi extract. Cá»¥ thá»ƒ, nÃ³ láº·p Ä‘i láº·p láº¡i chá»n pháº§n tá»­ thÆ°a tÆ°Æ¡ng á»©ng vá»›i cÃ¡c bank bá»™ nhá»› khÃ¡c nhau khi táº¡o ra máº£ng con NonZeros cho má»—i tile Tiled-CSL. Fig.8c cho tháº¥y má»™t trÆ°á»ng há»£p lÃ½ tÆ°á»Ÿng nÆ¡i chá»‰ cáº§n má»™t wavefront bá»™ nhá»› chia sáº» Ä‘á»ƒ phá»¥c vá»¥ má»™t WARP thá»±c hiá»‡n giai Ä‘oáº¡n extract sau khi sáº¯p xáº¿p láº¡i dá»¯ liá»‡u. LÆ°u Ã½ ráº±ng viá»‡c sáº¯p xáº¿p láº¡i dá»¯ liá»‡u náº±m trong pháº¡m vi tile Tiled-CSL, chá»‰ thay Ä‘á»•i phÃ¢n phá»‘i dá»¯ liá»‡u trÃªn bá»™ nhá»› toÃ n cáº§u nhÆ°ng khÃ´ng thay Ä‘á»•i Ä‘iá»u Ä‘Ã³ trÃªn bá»™ Ä‘á»‡m bá»™ nhá»› chia sáº». NÃ³i cÃ¡ch khÃ¡c, Ä‘Ã³ lÃ  cÃ¡ch thay Ä‘á»•i vá»‹ trÃ­ dá»¯ liá»‡u trÃªn bá»™ nhá»› toÃ n cáº§u Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c truy cáº­p bá»™ nhá»› chia sáº» hiá»‡u quáº£ hÆ¡n.

**Thuáº­t toÃ¡n 3** Tiled-CSL_Gen_AOTSparseDataReordering
```
1: Input: Matrix A in size ğ‘€ Ã— ğ¾;
2: Output1: vector< vector<unsigned int> > NonZeros;
3: Output2: vector<int> TileOffsets;
4: vector<unsigned int> ğ‘ğ‘_ğµğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡[32];
5: for int ğ‘– = 0; ğ‘– < ğ‘€/128; ğ‘–++ do
6:     for int ğ‘— = 0; ğ‘— < ğ¾/64; ğ‘—++ do
7:         // classifying NonZeros.
8:         half* ğ‘‡ğ‘–ğ‘™ğ‘’ğ‘ƒğ‘‡ğ‘… = ğ´ + ğ‘– âˆ— 128 âˆ— ğ¾ + ğ‘— âˆ— 64;
9:         for int ğ‘¥ = 0; ğ‘¥ < 128; ğ‘¥++ do
10:            for int ğ‘¦ = 0; ğ‘¦ < 64; ğ‘¦++ do
11:                ğ‘£ğ‘ğ‘™ = ğ‘‡ğ‘–ğ‘™ğ‘’ğ‘ƒğ‘‡ğ‘…[ğ‘¥ âˆ— ğ¾ + ğ‘¦];
12:                short ğ‘™ğ‘œğ‘ = ğ‘™ğ‘œğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘›_ğ‘–ğ‘›_ğ‘†ğ‘€ğ‘’ğ‘š(ğ‘¥, ğ‘¦);
13:                int ğµğ‘ğ‘›ğ‘˜ğ¼ğ· = (ğ‘¥ % 8) âˆ— 4 + (ğ‘¦ % 8) // 2;
14:                ğ‘ğ‘_ğµğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡[ğµğ‘ğ‘›ğ‘˜ï¿½ï¿½ğ·].ğ‘ğ‘¢ğ‘ â„_ğ‘ğ‘ğ‘ğ‘˜(ğ‘£ğ‘ğ‘™, ğ‘™ğ‘œğ‘);
15:        // iteratively picking 32 NonZeros as a group.
16:        int ğ‘ğ‘ğ‘ = ğ‘ğ‘œğ‘¢ğ‘›ğ‘¡_ğ‘ğ‘ğ‘(ğ‘ğ‘_ğµğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡);
17:        for int ğ‘” = 0; ğ‘” < ğ‘ğ‘ğ‘/32; ğ‘”++ do
18:            vector<unsigned int> ğ‘ğ‘_ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘;
19:            for int ğ‘ = 0; ğ‘ < 32; ğ‘++ do
20:                int ğ‘–ğ‘‘ = ğµğ‘ğ‘›ğ‘˜ğ¼ğ·_ğ‘€ğ‘ğ‘¥(ğ‘ğ‘_ğµğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡);
21:                ğ‘ğ‘_ğºğ‘Ÿğ‘œğ‘¢ğ‘.ğ‘ğ‘¢ğ‘ â„_ğ‘ğ‘ğ‘ğ‘˜(ğ‘ğ‘_ğµğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡[ğ‘–ğ‘‘].ğ‘ğ‘ğ‘ğ‘˜());
22:                ğ‘ğ‘_ğµğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡[ğ‘–ğ‘‘].ğ‘ğ‘œğ‘_ğ‘ğ‘ğ‘ğ‘˜();
23:            ğ‘ğ‘œğ‘›ğ‘ğ‘’ğ‘Ÿğ‘œğ‘ .ğ‘ğ‘¢ğ‘ â„_ğ‘ğ‘ğ‘ğ‘˜(ğ‘ğ‘_ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘);
24:        ğ‘‡ğ‘–ğ‘™ğ‘’ğ‘‚ğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡.ğ‘ğ‘¢ğ‘ â„_ğ‘ğ‘ğ‘ğ‘˜(ğ‘ğ‘ğ‘);
```

Alg.3 cho tháº¥y thuáº­t toÃ¡n Ä‘á»ƒ táº¡o ra Ä‘á»‹nh dáº¡ng Tiled-CSL tá»« ma tráº­n thÆ°a gá»‘c theo phÆ°Æ¡ng phÃ¡p sáº¯p xáº¿p láº¡i dá»¯ liá»‡u thÆ°a trÆ°á»›c thá»i háº¡n. Äáº§u vÃ o lÃ  ma tráº­n thÆ°a A vá»›i ğ‘€ hÃ ng vÃ  ğ¾ cá»™t á»Ÿ Ä‘á»‹nh dáº¡ng dense, nÆ¡i má»™t sá»‘ pháº§n tá»­ Ä‘Ã£ Ä‘Æ°á»£c Ä‘áº·t thÃ nh 0 thÃ´ng qua cáº¯t tá»‰a mÃ´ hÃ¬nh. CÃ¡c Ä‘áº§u ra lÃ  NonZeros vÃ  TileOffsets, cÃ¡c thÃ nh pháº§n chÃ­nh cá»§a Ä‘á»‹nh dáº¡ng Tiled-CSL. NonZeros Ä‘Æ°á»£c chia thÃ nh cÃ¡c nhÃ³m mÃ  má»—i nhÃ³m chá»©a 32 khÃ¡c khÃ´ng trong dÃ²ng 2. Má»—i nhÃ³m khÃ¡c khÃ´ng sáº½ Ä‘Æ°á»£c ghi vÃ o bá»™ nhá»› chia sáº» trong giai Ä‘oáº¡n extract trong má»™t yÃªu cáº§u bá»™ nhá»› chia sáº». Táº¡i dÃ²ng 7-24, Ä‘á»‹nh dáº¡ng Tiled-CSL cá»§a má»™t tile (128Ã—64) sáº½ Ä‘Æ°á»£c táº¡o ra. Táº¡i dÃ²ng 11-14, má»—i khÃ¡c khÃ´ng trong ma tráº­n A sáº½ Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh má»™t tá»« 32-bit chá»©a (ğ‘£ğ‘ğ‘™, ğ‘™ğ‘œğ‘). BÃªn cáº¡nh Ä‘Ã³, cÃ¡c khÃ¡c khÃ´ng Ä‘Æ°á»£c phÃ¢n phá»‘i vÃ o 32 bucket khÃ¡c nhau (ğ‘ğ‘_ğµğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡[32] táº¡i dÃ²ng 4) theo ID bank bá»™ nhá»› chia sáº» Ä‘Ã­ch cá»§a chÃºng tá»« 0 Ä‘áº¿n 31 nhÆ° Ä‘Æ°á»£c tÃ­nh trong dÃ²ng 13. Trong dÃ²ng 16, tá»•ng sá»‘ khÃ¡c khÃ´ng (NNZ) Ä‘Æ°á»£c Ä‘áº¿m. Táº¡i dÃ²ng 19-22, má»™t nhÃ³m khÃ¡c khÃ´ng Ä‘Æ°á»£c hÃ¬nh thÃ nh báº±ng cÃ¡ch láº·p Ä‘i láº·p láº¡i chá»n cÃ¡c khÃ¡c khÃ´ng tá»« NZ_Bucket[id] nÆ¡i ğ‘ğ‘_ğµğ‘¢ğ‘ğ‘˜ğ‘’ğ‘¡[ğ‘–ğ‘‘] lÃ  bucket vá»›i nhiá»u khÃ¡c khÃ´ng nháº¥t chÆ°a Ä‘Æ°á»£c xá»­ lÃ½ táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³.

## 5 Triá»ƒn khai

ChÃºng tÃ´i cung cáº¥p má»™t táº­p há»£p API C++ cho kernel Flash-LLM hiá»‡u suáº¥t cao. ChÃºng tÃ´i tÃ­ch há»£p kernel Flash-LLM vÃ o FasterTransformer [37], cho phÃ©p suy luáº­n phÃ¢n tÃ¡n hiá»‡u quáº£ cao vá»›i cÃ¡c ma tráº­n trá»ng sá»‘ Ä‘Æ°á»£c sparsified. Cá»¥ thá»ƒ, chÃºng tÃ´i má»Ÿ rá»™ng Ä‘á»‹nh nghÄ©a lá»›p tÆ°Æ¡ng á»©ng cá»§a nÃ³ (tá»©c lÃ  lá»›p DenseWeight) Ä‘á»ƒ há»— trá»£ Ä‘á»‹nh dáº¡ng Tiled-CSL. BÃªn cáº¡nh Ä‘Ã³, chÃºng tÃ´i má»Ÿ rá»™ng wrapper thÆ° viá»‡n cá»§a nÃ³ (tá»©c lÃ  lá»›p cuBlasMMWrapper) Ä‘á»ƒ há»— trá»£ gá»i thÆ° viá»‡n MatMul dense hoáº·c kernel SpMM Flash-LLM theo Ä‘á»‹nh dáº¡ng dá»¯ liá»‡u Ä‘Ã£ cho. Flash-LLM cÅ©ng cÃ³ thá»ƒ dá»… dÃ ng Ä‘Æ°á»£c tÃ­ch há»£p vÃ o cÃ¡c framework deep learning khÃ¡c thÃ´ng qua cÃ¡c lá»i gá»i thÆ° viá»‡n vá»›i API Flash-LLM. ChÃºng tÃ´i cÅ©ng cung cáº¥p má»™t cÃ´ng cá»¥ Ä‘á»‹nh dáº¡ng láº¡i trá»ng sá»‘ Ä‘á»ƒ táº¡o ra cÃ¡c ma tráº­n thÆ°a á»Ÿ Ä‘á»‹nh dáº¡ng Tiled-CSL cho mÃ´ hÃ¬nh PyTorch dense Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c.

Trong triá»ƒn khai cá»§a chÃºng tÃ´i, kÃ­ch thÆ°á»›c cá»§a ğ‘€ğ‘‡ğµ trong Fig. 6a lÃ  128 hoáº·c 256, ğ¾ğ‘‡ğµ lÃ  64, vÃ  ğ‘ğ‘‡ğµ lÃ  8/16/32/32 khi chiá»u ğ‘ cá»§a MatMul (kÃ­ch thÆ°á»›c batch suy luáº­n) lÃ  8/16/32/64. Äá»‘i vá»›i cÃ¡c chiá»u ğ‘ lá»›n hÆ¡n, ğ‘ğ‘‡ğµ lÃ  64. KÃ­ch thÆ°á»›c thread block lÃ  128. Nhá»¯ng cáº¥u hÃ¬nh nÃ y hoáº¡t Ä‘á»™ng tá»‘t cho cÃ¡c workload chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ trong Sec.6. CÃ¡c cáº¥u hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c cáº¥u hÃ¬nh láº¡i dá»… dÃ ng cho cÃ¡c workload khÃ¡c. Viá»‡c Ä‘iá»u chá»‰nh cáº¥u hÃ¬nh khÃ´ng náº±m trong pháº¡m vi nghiÃªn cá»©u cá»§a bÃ i bÃ¡o nÃ y.

## 6 ÄÃ¡nh giÃ¡

ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a Flash-LLM trÃªn hai cáº¥p Ä‘á»™: benchmarking cáº¥p kernel vÃ  Ä‘Ã¡nh giÃ¡ cáº¥p mÃ´ hÃ¬nh. ÄÃ¡nh giÃ¡ Ä‘Æ°á»£c tiáº¿n hÃ nh trÃªn ná»n táº£ng NVIDIA A100-SMX8-80GB (CPU Intel Xeon Platinum 8369B 128-core @2.90GHz, 8 GPU NVIDIA A100 @80GB), vá»›i Ubuntu 18.04 vÃ  CUDA 11.8. ChÃºng tÃ´i kÃ­ch hoáº¡t auto-mixed precision (AMP) cho táº¥t cáº£ cÃ¡c Ä‘Ã¡nh giÃ¡. ChÃºng tÃ´i chá»§ yáº¿u thá»±c hiá»‡n thÃ­ nghiá»‡m trÃªn GPU NVIDIA A100. Tuy nhiÃªn, phÆ°Æ¡ng phÃ¡p chÃºng tÃ´i Ä‘á» xuáº¥t cÅ©ng lÃ  má»™t tham kháº£o tá»‘t cho thiáº¿t káº¿ kernel cá»§a TPU vÃ  CPU Intel Ä‘Æ°á»£c trang bá»‹ pháº§n cá»©ng tÃ¹y chá»‰nh cho phÃ©p nhÃ¢n ma tráº­n.

### 6.1 Hiá»‡u suáº¥t Kernel

**Workload, baseline, vÃ  cÃ i Ä‘áº·t.** ChÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Flash-LLM trÃªn MatMul dÆ°á»›i cÃ¡c hÃ¬nh dáº¡ng khÃ¡c nhau, Ä‘áº¿n tá»« bá»‘n MatMul Ä‘Æ°á»£c mÃ´ táº£ trong Sec.2.1 trong OPT-30B, OPT-66B, vÃ  OPT-175B [61] cho bá»‘n kÃ­ch thÆ°á»›c batch khÃ¡c nhau (8, 16, 32, vÃ  64). Äá»‘i vá»›i má»—i hÃ¬nh dáº¡ng MatMul, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Ä‘á»™ trá»… kernel dÆ°á»›i 70%, 80%, vÃ  90% sparsity ngáº«u nhiÃªn trong cÃ¡c ma tráº­n trá»ng sá»‘. CÃ¡c baseline chÃºng tÃ´i so sÃ¡nh bao gá»“m cuSPARSE [40], Sputnik (commit: 46e380c) [10,11], SparTA (commit: 1f61a36) [64,65], vÃ  cuBLAS [39]. CuSPARSE lÃ  má»™t pháº§n cá»§a CUDA Toolkit Ä‘á»ƒ xá»­ lÃ½ cÃ¡c ma tráº­n thÆ°a. Sputnik lÃ  má»™t thÆ° viá»‡n cÃ¡c kernel Ä‘áº¡i sá»‘ tuyáº¿n tÃ­nh thÆ°a cho deep learning, Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t SpMM tiÃªn tiáº¿n nháº¥t dá»±a trÃªn SIMT core. SparTA há»— trá»£ sparsity khÃ´ng cÃ³ cáº¥u trÃºc dá»±a trÃªn sparsity cÃ³ cáº¥u trÃºc 2:4 trÃªn tensor core [32]. VÃ¬ SparTA chá»‰ há»— trá»£ Ä‘á»™ chÃ­nh xÃ¡c FP32, chÃºng tÃ´i má»Ÿ rá»™ng SparTA Ä‘á»ƒ há»— trá»£ cÃ¡c ma tráº­n Ä‘áº§u vÃ o á»Ÿ FP16. CuBLAS nháº¯m Ä‘áº¿n MatMul dense thay vÃ¬ SpMM. ChÃºng tÃ´i bao gá»“m nÃ³ lÃ m baseline á»Ÿ Ä‘Ã¢y Ä‘á»ƒ cho tháº¥y má»©c tÄƒng/giáº£m hiá»‡u suáº¥t thá»±c táº¿ so vá»›i cÃ¡c triá»ƒn khai dense cÆ¡ báº£n cá»§a suy luáº­n LLM.

**Káº¿t quáº£.** Fig.9 cho tháº¥y hiá»‡u suáº¥t kernel (TFLOP) cá»§a Flash-LLM vÃ  cÃ¡c baseline. ThÃ´ng lÆ°á»£ng cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­nh báº±ng 2Ã—ğ‘€Ã—ğ¾Ã—ğ‘/ğ‘˜ğ‘’ğ‘Ÿğ‘›ğ‘’ğ‘™_ğ‘™ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.9, Flash-LLM hoáº¡t Ä‘á»™ng tá»‘t nháº¥t liÃªn tá»¥c so vá»›i cÃ¡c baseline. Trung bÃ¬nh, Flash-LLM vÆ°á»£t trá»™i so vá»›i Sputnik/SparTA báº±ng 3.6Ã—/1.4Ã—, 3.0Ã—/1.4Ã—, vÃ  2.0Ã—/1.6Ã— dÆ°á»›i sparsity 70%, 80%, vÃ  90% tÆ°Æ¡ng á»©ng. BÃªn cáº¡nh Ä‘Ã³, Flash-LLM cÅ©ng cÃ³ thá»ƒ vÆ°á»£t trá»™i so vá»›i cÃ¡c kernel dense tiÃªn tiáº¿n nháº¥t cuBLAS vá»›i tensor core Ä‘Æ°á»£c kÃ­ch hoáº¡t báº±ng 1.4Ã—, 1.7Ã—, vÃ  2.1Ã—.

CuSPARSE cho tháº¥y hiá»‡u suáº¥t kÃ©m dÆ°á»›i má»©c sparsity vá»«a pháº£i nhÆ° váº­y, vÃ¬ nÃ³ Ä‘Æ°á»£c thiáº¿t káº¿ cho cÃ¡c ma tráº­n vá»›i >95% sparsity [40]. Äá»‘i vá»›i Sputnik, ráº¥t thÃ¡ch thá»©c Ä‘á»ƒ vÆ°á»£t trá»™i so vá»›i cÃ¡c kernel cuBLAS vá»›i tensor core Ä‘Æ°á»£c kÃ­ch hoáº¡t. Theo káº¿t quáº£ benchmark, Sputnik cÃ³ thá»ƒ vÆ°á»£t trá»™i so vá»›i cuBLAS_TC chá»‰ khi sparsity >90% khi chiá»u N lÃ  8, 16, hoáº·c 32. Äá»‘i vá»›i SparTA, nhÆ° Ä‘Æ°á»£c mÃ´ táº£ trong Sec.3.1, nÃ³ táº­n dá»¥ng sparse tensor core [32] cho pháº§n chÃ­nh cá»§a cÃ¡c tÃ­nh toÃ¡n, khÃ´ng thá»ƒ khai thÃ¡c hiá»‡u quáº£ sparsity cÃ³ sáºµn vÃ¬ sparse tensor core chá»‰ há»— trá»£ 50% sparsity (sparsity 2:4). Náº¿u sparsity cÃ³ sáºµn cao hÆ¡n 50%, SparTA pháº£i Ä‘á»‡m sá»‘ khÃ´ng vÃ o ma tráº­n thÆ°a dáº«n Ä‘áº¿n truy cáº­p bá»™ nhá»› toÃ n cáº§u dÆ° thá»«a trong runtime. BÃªn cáº¡nh Ä‘Ã³, Ä‘á»‘i vá»›i cÃ¡c khÃ¡c khÃ´ng khÃ´ng thá»ƒ Ä‘Ã¡p á»©ng yÃªu cáº§u 2:4, má»™t kernel dá»±a trÃªn SIMT core khÃ¡c Ä‘Æ°á»£c khá»Ÿi cháº¡y cho cÃ¡c tÃ­nh toÃ¡n tÆ°Æ¡ng á»©ng, dáº«n Ä‘áº¿n overhead bá»• sung. Do Ä‘Ã³, Flash-LLM vÆ°á»£t trá»™i so vá»›i SparTA trong cÃ¡c Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i.

### 6.2 PhÃ¢n tÃ­ch Kernel

**Tá»‘i Æ°u hÃ³a Sá»­ dá»¥ng GPU.** Fig.10â· cho tháº¥y viá»‡c sá»­ dá»¥ng cÃ¡c Ä‘Æ¡n vá»‹ pháº§n cá»©ng GPU bao gá»“m tensor core (TC), bá»™ nhá»› L1 vÃ  chia sáº» káº¿t há»£p (L1), bá»™ nhá»› cache L2 (L2), vÃ  bá»™ nhá»› toÃ n cáº§u GPU (HBM) trong quÃ¡ trÃ¬nh thá»±c thi kernel Flash-LLM. Táº¥t cáº£ dá»¯ liá»‡u Ä‘Æ°á»£c thu tháº­p bá»Ÿi profiler NSight Compute [43]. ChÃºng tÃ´i trÃ¬nh bÃ y káº¿t quáº£ profiling cá»§a N = 16 vÃ  N = 32 dÆ°á»›i 90% sparsity. ChÃºng tÃ´i cÅ©ng bao gá»“m cuBLAS á»Ÿ Ä‘Ã¢y lÃ m baseline dense (sparsity = 0%). cuSPARSE vÃ  Sputnik lÃ  cÃ¡c thiáº¿t káº¿ dá»±a trÃªn SIMT nÆ¡i tensor core khÃ´ng Ä‘Æ°á»£c sá»­ dá»¥ng chÃºt nÃ o.

Máº·c dÃ¹ Sputnik Ä‘áº¡t Ä‘Æ°á»£c viá»‡c sá»­ dá»¥ng SIMT core tá»‘t (29.8% táº¡i BS=16, 40.1% táº¡i BS=32), nÃ³ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cháº­m hÆ¡n nhiá»u so vá»›i cÃ¡c kernel dá»±a trÃªn tensor-core khÃ¡c vÃ¬ SIMT core cho tháº¥y thÃ´ng lÆ°á»£ng tÃ­nh toÃ¡n Ä‘á»‰nh tháº¥p hÆ¡n nhiá»u so vá»›i tensor core. SparTA sá»­ dá»¥ng tensor core nhÆ°ng vá»›i viá»‡c sá»­ dá»¥ng tháº¥p hÆ¡n cuBLAS. Äá»‘i vá»›i cuBLAS,

â· Äá»‘i vá»›i cuBLAS [39], sá»‘ lá»‡nh tensor core (HMMA) Ä‘Æ°á»£c thá»±c thi sáº½ khÃ´ng thay Ä‘á»•i khi chiá»u N thay Ä‘á»•i (vÃ­ dá»¥ 8, 16, 32, 64). Äiá»u nÃ y cÃ³ nghÄ©a lÃ  cÃ³ cÃ¡c hoáº¡t Ä‘á»™ng tensor core dÆ° thá»«a Ä‘Æ°á»£c khá»Ÿi cháº¡y trong kernel cuBLAS cho N=8, 16, vÃ  32. Nhá»¯ng hoáº¡t Ä‘á»™ng HMMA dÆ° thá»«a nÃ y Ä‘Æ°á»£c loáº¡i trá»« Ä‘Ãºng cÃ¡ch trong cÃ¡c Ä‘Ã¡nh giÃ¡ cá»§a chÃºng tÃ´i.

bÄƒng thÃ´ng cá»§a bá»™ nhá»› cache L2 vÃ  GPU DRAM bá»‹ cáº¡n kiá»‡t trong khi tensor core chá»‰ Ä‘áº¡t 10.7% vÃ  21.0% hiá»‡u suáº¥t Ä‘á»‰nh cá»§a nÃ³ trung bÃ¬nh khi chiá»u N lÃ  16 vÃ  32 tÆ°Æ¡ng á»©ng. Trong Flash-LLM, cÃ¡c ma tráº­n thÆ°a á»Ÿ Ä‘á»‹nh dáº¡ng Tiled-CSL (Sec.3.2.2) vá»›i kÃ­ch thÆ°á»›c giáº£m theo byte. BÄƒng thÃ´ng bá»™ nhá»› toÃ n cáº§u khÃ´ng cÃ²n lÃ  nÃºt tháº¯t cá»• chai vá»›i phÆ°Æ¡ng phÃ¡p Load-as-Sparse vÃ  Compute-as-Dense. Trung bÃ¬nh, viá»‡c sá»­ dá»¥ng tensor core Ä‘Æ°á»£c cáº£i thiá»‡n lÃªn 24.4% vÃ  42.6%. Khi viá»‡c sá»­ dá»¥ng tensor core Ä‘Æ°á»£c cáº£i thiá»‡n, nÃ³ tiÃªu thá»¥ bÄƒng thÃ´ng cao hÆ¡n cá»§a bá»™ nhá»› chia sáº». NgoÃ i ra, extract trong Fig.6c sáº½ gÃ¢y ra cÃ¡c lÆ°u trá»¯ bá»™ nhá»› chia sáº» bá»• sung. Do Ä‘Ã³, bÄƒng thÃ´ng L1/shared-memory bá»‹ cáº¡n kiá»‡t bá»Ÿi Flash-LLM, Ä‘iá»u nÃ y ngÄƒn cáº£n cáº£i thiá»‡n hiá»‡u suáº¥t thÃªm ná»¯a. LÆ°u Ã½ ráº±ng viá»‡c sáº¯p xáº¿p láº¡i dá»¯ liá»‡u thÆ°a trÆ°á»›c thá»i háº¡n (Sec.4.3.3) Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tÄƒng hiá»‡u quáº£ truy cáº­p bá»™ nhá»› chia sáº», giÃºp giáº£m thiá»ƒu nÃºt tháº¯t cá»• chai bÄƒng thÃ´ng cá»§a bá»™ nhá»› chia sáº». CÃ¡ch giáº£m thÃªm Ã¡p lá»±c lÃªn bá»™ nhá»› chia sáº» cÃ³ thá»ƒ lÃ  cÃ´ng viá»‡c tÆ°Æ¡ng lai.

**Pipeline cÃ¢n báº±ng cho cÃ¡c hoáº¡t Ä‘á»™ng bá»™ nhá»›/tensor core.** Kernel Flash-LLM chá»©a ba loáº¡i hoáº¡t Ä‘á»™ng chÃ­nh: truy cáº­p bá»™ nhá»› toÃ n cáº§u (Gmem), truy cáº­p bá»™ nhá»› chia sáº» (Smem), vÃ  cÃ¡c hoáº¡t Ä‘á»™ng tensor core (TC). LÃ½ tÆ°á»Ÿng, ba loáº¡i hoáº¡t Ä‘á»™ng nÃ y nÃªn Ä‘Æ°á»£c chá»“ng chÃ©o vÃ  tiáº¿n hÃ nh song song Ä‘á»ƒ tá»‘i Ä‘a hÃ³a viá»‡c sá»­ dá»¥ng pháº§n cá»©ng GPU. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.11, chÃºng tÃ´i Ä‘o Ä‘á»™ trá»… cá»§a má»—i loáº¡i hoáº¡t Ä‘á»™ng riÃªng biá»‡t báº±ng cÃ¡ch xÃ³a cÃ¡c tÃ­nh toÃ¡n khÃ¡c trong mÃ£ nguá»“n. ChÃºng tÃ´i cÅ©ng triá»ƒn khai láº¡i kernel dense GeMM dá»±a trÃªn thiáº¿t káº¿ cá»§a NVIDIA cutlass [42], Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tÆ°Æ¡ng tá»± so vá»›i cuBLAS. Do hiá»‡u á»©ng Bucket, thá»i gian kernel tá»•ng thá»ƒ chá»§ yáº¿u Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh bá»Ÿi cÃ¡c hoáº¡t Ä‘á»™ng bá»™ nhá»› toÃ n cáº§u yÃªu cáº§u thá»i gian thá»±c thi dÃ i nháº¥t. Vá»›i phÆ°Æ¡ng phÃ¡p Load-as-Sparse vÃ  Compute-as-Dense, Ä‘á»™ trá»… cá»§a cÃ¡c hoáº¡t Ä‘á»™ng Gmem Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ, dáº«n Ä‘áº¿n cáº£i thiá»‡n hiá»‡u suáº¥t tá»•ng thá»ƒ.

Máº·c dÃ¹ Ä‘á»™ trá»… Smem tÄƒng do truy cáº­p bá»™ nhá»› chia sáº» bá»• sung Ä‘Æ°á»£c yÃªu cáº§u bá»Ÿi extract trong Fig.6c, nÃ³ khÃ´ng ngÄƒn cáº£n kernel Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t hÆ¡n cuBLAS. ÄÃ³ lÃ  má»™t sá»± Ä‘Ã¡nh Ä‘á»•i tá»‘t giá»¯a viá»‡c sá»­ dá»¥ng bá»™ nhá»› toÃ n cáº§u vÃ  bá»™ nhá»› chia sáº». HÆ¡n ná»¯a, Flash-LLM cho tháº¥y Ä‘á»™ trá»… tÆ°Æ¡ng tá»± vá»›i kernel dense GeMM vá» cÃ¡c hoáº¡t Ä‘á»™ng tensor core vÃ¬ chÃºng tÃ´i khÃ´ng bá» qua báº¥t ká»³ tÃ­nh toÃ¡n nÃ o á»Ÿ Ä‘Ã¢y. Tuy nhiÃªn, rÃµ rÃ ng lÃ  cÃ¡c hoáº¡t Ä‘á»™ng tensor core khÃ´ng pháº£i lÃ  nÃºt tháº¯t cá»• chai cho hiá»‡u suáº¥t kernel tá»•ng thá»ƒ.

**Hiá»‡u suáº¥t trÃªn nhiá»u hÃ¬nh dáº¡ng MatMul hÆ¡n.** NhÆ° Ä‘Ã£ tháº£o luáº­n trong Sec.3.2.1, chÃºng tÃ´i chá»§ yáº¿u muá»‘n giáº£m thiá»ƒu sá»± kÃ©m hiá»‡u quáº£ do Skinny MatMul trong suy luáº­n LLM phá»• biáº¿n. Äá»ƒ hiá»ƒu biáº¿t Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n hÆ¡n, chÃºng tÃ´i cung cáº¥p hiá»‡u suáº¥t kernel trÃªn nhiá»u hÃ¬nh dáº¡ng hÆ¡n ngay cáº£ khi hÃ¬nh dáº¡ng khÃ´ng phá»• biáº¿n cho suy luáº­n LLM. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.12, Flash-LLM trá»Ÿ nÃªn cháº­m hÆ¡n cuBLAS náº¿u chiá»u N lá»›n hÆ¡n 256, lÆ°u Ã½ ráº±ng dáº¥u chÃ¢n bá»™ nhá»› cá»§a Flash-LLM váº«n nhá» hÆ¡n so vá»›i cÃ¡c Ä‘á»‘i tÃ¡c dense. LÃ½ do Ä‘áº±ng sau Ä‘iá»u nÃ y cÃ³ hai máº·t. Äáº§u tiÃªn, sá»± kÃ©m hiá»‡u quáº£ cá»§a cuBLAS khÃ´ng cÃ²n Ä‘Ã¡ng ká»ƒ vÃ¬ chiá»u N Ä‘á»§ lá»›n, lÃ m cho cuBLAS hiá»‡u suáº¥t hÆ¡n. Thá»© hai, Flash-LLM cÃ³ thiáº¿t káº¿ kernel phá»©c táº¡p hÆ¡n vÃ  truy cáº­p bá»™ nhá»› chia sáº» bá»• sung, lÃ m cháº­m nÃ³ má»™t chÃºt. ChÃºng tÃ´i cÅ©ng nháº­n tháº¥y ráº±ng Sputnik trá»Ÿ nÃªn cháº­m hÆ¡n nhiá»u so vá»›i cÃ¡c thiáº¿t káº¿ dá»±a trÃªn tensor-core khÃ¡c khi chiá»u N tÄƒng. Máº·c dÃ¹ cÃ¡c giáº£i phÃ¡p dá»±a trÃªn SIMT-core nhÆ° Sputnik cÃ³ thá»ƒ bá» qua tÃ­nh toÃ¡n khai thÃ¡c sparsity, viá»‡c tiáº¿t kiá»‡m tÃ­nh toÃ¡n nhÆ° váº­y váº«n khÃ´ng thá»ƒ bÃ¹ Ä‘áº¯p cho khoáº£ng cÃ¡ch hiá»‡u suáº¥t khá»•ng lá»“ giá»¯a SIMT core vÃ  tensor core.

### 6.3 ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh Äáº§u cuá»‘i

**Baseline.** ChÃºng tÃ´i bao gá»“m NVIDIA FasterTransformer (FT) (git commit: 9770d30) [37] vÃ  DeepSpeed (DS) [1] lÃ m baseline, cÃ¡c framework suy luáº­n tiÃªn tiáº¿n nháº¥t há»— trá»£ song song mÃ´ hÃ¬nh [49] Ä‘á»ƒ phÃ¹ há»£p vá»›i cÃ¡c mÃ´ hÃ¬nh lá»›n mÃ  náº¿u khÃ´ng sáº½ khÃ´ng vá»«a trong bá»™ nhá»› GPU.

**Workload.** ChÃºng tÃ´i benchmark Ä‘á»™ trá»… suy luáº­n Ä‘áº§u cuá»‘i cá»§a cÃ¡c mÃ´ hÃ¬nh OPT [61], bao gá»“m OPT-30B, OPT-66B, vÃ  OPT-175B. Äá»ƒ chá»‰ chá»©a cÃ¡c tham sá»‘ mÃ´ hÃ¬nh á»Ÿ Ä‘á»‹nh dáº¡ng dense, cáº§n 60/132/350GB bá»™ nhá»› GPU cho OPT-30B/66B/175B. LÆ°u Ã½ ráº±ng GPU SOTA chá»‰ cÃ³ 80GB bá»™ nhá»› má»—i cÃ¡i, cáº§n Ã­t nháº¥t 1/2/8 GPUâ¸ cho suy luáº­n OPT-30B/66B/175B. BÃªn cáº¡nh Ä‘Ã³, bá»™ nhá»› GPU bá»• sung Ä‘Æ°á»£c yÃªu cáº§u Ä‘á»ƒ lÆ°u trá»¯ KV-Cache (tham kháº£o Fig. 1b, vá»›i kÃ­ch thÆ°á»›c cá»§a nÃ³ cÃ³ má»‘i quan há»‡ tÃ­ch cá»±c vá»›i kÃ­ch thÆ°á»›c batch suy luáº­n) trong runtime. Theo Báº£ng 1, cÃ¡c framework suy luáº­n hiá»‡n cÃ³ (FT-1GPU, DS-1GPU) cÃ³ thá»ƒ dá»… dÃ ng háº¿t bá»™ nhá»› GPU trong quÃ¡ trÃ¬nh suy luáº­n OPT-30B cho kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n 16 náº¿u lÆ°u trá»¯ cÃ¡c tham sá»‘ mÃ´ hÃ¬nh á»Ÿ Ä‘á»‹nh dáº¡ng dense. Äá»‘i vá»›i táº¥t cáº£ thÃ­ nghiá»‡m, Ä‘á»™ dÃ i chuá»—i Ä‘áº§u vÃ o/prompt lÃ  64 vÃ  Ä‘á»™ dÃ i chuá»—i Ä‘áº§u ra/Ä‘Æ°á»£c táº¡o lÃ  512.

**Metric.** ChÃºng tÃ´i Ä‘á» xuáº¥t metric token per GPU-second Ä‘á»ƒ chá»‰ ra thÃ´ng lÆ°á»£ng suy luáº­n chuáº©n hÃ³a vá»›i xem xÃ©t cáº£ thá»i gian thá»±c thi vÃ  chi phÃ­ pháº§n cá»©ng (tá»©c lÃ  sá»‘ GPU Ä‘Æ°á»£c sá»­ dá»¥ng). NÃ³ Ä‘Æ°á»£c tÃ­nh vá»›i phÆ°Æ¡ng trÃ¬nh sau:

ğ‘ƒğ‘’ğ‘Ÿğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘›ğ‘ğ‘’ = ğ‘ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› / Î£áµ¢â‚Œâ‚á´ºá´³á´¾áµ ğ‘‡áµ¢  (3)

ğ‘ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘› cÃ³ nghÄ©a lÃ  sá»‘ lÆ°á»£ng token Ä‘Æ°á»£c táº¡o ra, trong khi ğ‘ğºğ‘ƒğ‘ˆ vÃ  ğ‘‡áµ¢ cÃ³ nghÄ©a lÃ  sá»‘ GPU vÃ  thá»i gian dÃ nh cho thá»±c thi trÃªn GPU thá»© i. ChÃºng tÃ´i sá»­ dá»¥ng metric nÃ y Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng. LÆ°u Ã½ ráº±ng Ä‘á»‘i vá»›i dá»‹ch vá»¥ suy luáº­n thá»i gian thá»±c, thÃ´ng lÆ°á»£ng suy luáº­n cÃ ng cao cÃ ng tá»‘t má»™t khi Ä‘á»™ trá»… suy luáº­n Ã­t hÆ¡n má»™t ngÆ°á»¡ng cá»¥ thá»ƒ.

â¸ Sá»‘ GPU pháº£i lÃ  lÅ©y thá»«a cá»§a 2 cho song song mÃ´ hÃ¬nh [6].

#### 6.3.1 NghiÃªn cá»©u TrÆ°á»ng há»£p: Cáº¯t tá»‰a MÃ´ hÃ¬nh OPT-30B.

Äá»ƒ cho tháº¥y ráº±ng mÃ´ hÃ¬nh Ä‘Æ°á»£c cáº¯t tá»‰a cÃ³ hiá»‡u suáº¥t so sÃ¡nh Ä‘Æ°á»£c vá»›i mÃ´ hÃ¬nh gá»‘c, chÃºng tÃ´i Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh Ä‘Æ°á»£c cáº¯t tá»‰a vá»›i OPT-30B [61] vÃ  GPT-NEOX-20B [2,5] trÃªn nhiá»‡m vá»¥ Recognizing Textual Entailment trong SuperGLUE [56]. Äá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tá»‘t hÆ¡n, chÃºng tÃ´i Ã¡p dá»¥ng phÆ°Æ¡ng phÃ¡p cáº¯t tá»‰a phá»• biáº¿n Taylor Pruning [33] Ä‘á»ƒ cáº¯t tá»‰a cÃ¡c mÃ´ hÃ¬nh nÃ y vÃ  giá»¯ má»™t pháº§n tÆ° Ä‘áº§u vÃ  má»™t pháº§n tÆ° cuá»‘i cÃ¡c lá»›p Ä‘áº§u vÃ o feedforward dense. Dá»±a trÃªn Ä‘Ã³, chÃºng tÃ´i Ä‘áº¡t Ä‘Æ°á»£c 80% sparsity trÃªn OPT-30B vÃ  GPT-NEOX-20B vá»›i chá»‰ 1.44% vÃ  0.72% giáº£m Ä‘á»™ chÃ­nh xÃ¡c, tÆ°Æ¡ng á»©ng. Cá»¥ thá»ƒ, Ä‘á»™ chÃ­nh xÃ¡c giáº£m tá»« 85.55% xuá»‘ng 84.11% trÃªn OPT-30B, vÃ  tá»« 83.03% xuá»‘ng 82.31% trÃªn GPT-NEOX-20B.

**Káº¿t quáº£.** NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.13a, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao hÆ¡n 3.4Ã— vÃ  3.3Ã— so vá»›i DeepSpeed (DS) vÃ  FasterTransformer (FT) vá»›i má»™t GPU duy nháº¥t. DS vÃ  FT cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tá»‘i Ä‘a 348 vÃ  359 token per GPU-second vá»›i má»™t GPU A100 duy nháº¥t. Náº¿u tÄƒng thÃªm kÃ­ch thÆ°á»›c batch suy luáº­n, DS/FT sáº½ háº¿t bá»™ nhá»› vÃ¬ cÃ¡c nhiá»‡m vá»¥ suy luáº­n vá»›i kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n cáº§n nhiá»u bá»™ nhá»› GPU hÆ¡n Ä‘á»ƒ lÆ°u trá»¯ cached-KV vÃ  activation. NgÆ°á»£c láº¡i, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c lÃªn Ä‘áº¿n 1187 token per GPU-second trÃªn kÃ­ch thÆ°á»›c batch 64. ÄÃ³ lÃ  bá»Ÿi vÃ¬ bá»™ nhá»› Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÆ°u trá»¯ trá»ng sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c giáº£m vá»›i Ä‘á»‹nh dáº¡ng Tiled-CSL, vÃ  do Ä‘Ã³ nhiá»u Cached-KV vÃ  activation hÆ¡n cÃ³ thá»ƒ Ä‘Æ°á»£c chá»©a. ChÃºng tÃ´i cÅ©ng so sÃ¡nh hiá»‡u suáº¥t cá»§a Flash-LLM vá»›i DS vÃ  FT vá»›i song song mÃ´ hÃ¬nh hai chiá»u [49], vá»›i Ä‘Ã³ DS vÃ  FT cÃ³ thá»ƒ há»— trá»£ kÃ­ch thÆ°á»›c batch 64. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.13b, FT vÃ  DS Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tÆ°Æ¡ng tá»± vá» token per GPU-second. So vá»›i DS/FT, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao hÆ¡n 1.91Ã—/1.75Ã—, 1.87Ã—/1.70Ã—, 1.67Ã—/1.55Ã—, vÃ  1.54Ã—/1.41Ã— táº¡i kÃ­ch thÆ°á»›c batch 8, 16, 32, vÃ  64 tÆ°Æ¡ng á»©ng. Viá»‡c sá»­ dá»¥ng bá»™ nhá»› GPU chi tiáº¿t cá»§a Flash-LLM, FT, vÃ  DS Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Báº£ng 1.

**PhÃ¢n tÃ­ch.** Äá»ƒ tÃ¬m hiá»ƒu táº¡i sao Flash-LLM cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t tá»‘t hÆ¡n, chÃºng tÃ´i tiáº¿n hÃ nh phÃ¢n tÃ­ch thá»i gian cá»§a suy luáº­n Ä‘áº§u cuá»‘i Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.14a. LÆ°u Ã½ ráº±ng chÃºng tÃ´i tiáº¿n hÃ nh táº¥t cáº£ cÃ¡c phÃ¢n tÃ­ch Ä‘áº§u cuá»‘i trong bÃ i bÃ¡o nÃ y táº­n dá»¥ng NSight System [44]. So vá»›i FT-2GPU (FasterTransformer vá»›i 2 GPU Ä‘Æ°á»£c sá»­ dá»¥ng), Flash-LLM vá»›i 1 GPU cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ trá»… suy luáº­n chuáº©n hÃ³a tháº¥p hÆ¡nâ¹ chá»§ yáº¿u vÃ¬ 1) MatMul hiá»‡u quáº£ hÆ¡n vÃ  2) viá»‡c loáº¡i bá» overhead giao tiáº¿p cross-GPU.

â¹ Äá»ƒ cÅ©ng tÃ­nh Ä‘áº¿n chi phÃ­ suy luáº­n vÃ  so sÃ¡nh hiá»‡u quáº£ suy luáº­n vá»›i cÃ¡c cáº¥u hÃ¬nh há»‡ thá»‘ng khÃ¡c nhau (vÃ­ dá»¥ sá»‘ GPU khÃ¡c nhau cÃ³ thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng), chÃºng tÃ´i tá»•ng thá»i gian thá»±c thi trÃªn táº¥t cáº£ GPU Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m Ä‘á»™ trá»… chuáº©n hÃ³a.

#### 6.3.2 NghiÃªn cá»©u TrÆ°á»ng há»£p: Káº¿t quáº£ MÃ´ hÃ¬nh OPT-66B

NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.15a, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c thÃ´ng lÆ°á»£ng táº¡o token cao hÆ¡n 3.8Ã— vÃ  3.6Ã— so vá»›i DS vÃ  FT vá»›i hai GPU. DS vÃ  FT cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tá»‘i Ä‘a 139 vÃ  144 token per GPU-second vá»›i kÃ­ch thÆ°á»›c batch 16 vÃ¬ chÃºng sáº½ háº¿t bá»™ nhá»› náº¿u tÄƒng thÃªm kÃ­ch thÆ°á»›c batch. NgÆ°á»£c láº¡i, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c lÃªn Ä‘áº¿n 522 token per GPU-second táº¡i kÃ­ch thÆ°á»›c batch 64. ChÃºng tÃ´i cÅ©ng so sÃ¡nh hiá»‡u suáº¥t cá»§a Flash-LLM vá»›i DS-4GPU/FT-4GPU nÆ¡i Flash-LLM váº«n sá»­ dá»¥ng hai GPU, trong khi DS-4GPU/FT-4GPU sá»­ dá»¥ng bá»‘n GPU Ä‘á»ƒ cho phÃ©p kÃ­ch thÆ°á»›c batch lá»›n hÆ¡n cho cÃ¡c baseline. So vá»›i DS-4GPU/FT-4GPU, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao hÆ¡n 1.85Ã—/1.68Ã—, 1.78Ã—/1.61Ã—, 1.7Ã—/1.58Ã—, vÃ  1.55Ã—/1.45Ã— cá»§a token per GPU-second táº¡i kÃ­ch thÆ°á»›c batch 8, 16, 32, vÃ  64 tÆ°Æ¡ng á»©ng.

**PhÃ¢n tÃ­ch.** ChÃºng tÃ´i tiáº¿n hÃ nh phÃ¢n tÃ­ch thá»i gian cá»§a suy luáº­n Ä‘áº§u cuá»‘i vá»›i FT vÃ  Flash-LLM cho OPT-66B nhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong Fig.14b. So vá»›i FT-4GPU (FasterTransformer vá»›i 4 GPU Ä‘Æ°á»£c sá»­ dá»¥ng), Flash-LLM vá»›i hai GPU cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ trá»… suy luáº­n chuáº©n hÃ³a tháº¥p hÆ¡n so vá»›i FT-4GPU chá»§ yáº¿u vÃ¬ 1) viá»‡c giáº£m thá»i gian MatMul vÃ  2) viá»‡c giáº£m overhead giao tiáº¿p cross-GPU.

#### 6.3.3 NghiÃªn cá»©u TrÆ°á»ng há»£p: Káº¿t quáº£ & PhÃ¢n tÃ­ch MÃ´ hÃ¬nh OPT-175B

ChÃºng tÃ´i thÃ nh cÃ´ng cháº¡y suy luáº­n cá»§a cÃ¡c mÃ´ hÃ¬nh OPT-175B vá»›i Flash-LLM sá»­ dá»¥ng 4 GPU A100. NgÆ°á»£c láº¡i, trá»ng sá»‘ cá»§a OPT-175B khÃ´ng thá»ƒ vá»«a vÃ o 4 GPU A100 vá»›i cÃ¡c giáº£i phÃ¡p truyá»n thá»‘ng. Do Ä‘Ã³, chÃºng tÃ´i khÃ´ng hiá»ƒn thá»‹ hiá»‡u suáº¥t cá»§a FT/DS sá»­ dá»¥ng 4 GPU á»Ÿ Ä‘Ã¢y vÃ¬ táº¥t cáº£ Ä‘á»u háº¿t bá»™ nhá»› GPU. NgoÃ i ra, chÃºng tÃ´i tháº¥t báº¡i trong viá»‡c cháº¡y OPT-175B vá»›i DS sá»­ dá»¥ng 8 GPU. Fig.16a so sÃ¡nh hiá»‡u suáº¥t cá»§a Flash-LLM vÃ  FT nÆ¡i chÃºng tÃ´i chá»‰ sá»­ dá»¥ng 4 GPU trong khi FT sá»­ dá»¥ng 8 GPU. So vá»›i FT-8GPU, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao hÆ¡n 2.0Ã—, 1.9Ã—, 1.7Ã—, vÃ  1.5Ã— táº¡i kÃ­ch thÆ°á»›c batch 8, 16, 32, vÃ  64 tÆ°Æ¡ng á»©ng. NhÆ° Ä‘Æ°á»£c hiá»ƒn thá»‹ trong fig.16b, cáº£ thá»i gian MatMul vÃ  thá»i gian giao tiáº¿p cross-GPU Ä‘á»u Ä‘Æ°á»£c giáº£m Ä‘Ã¡ng ká»ƒ sá»­ dá»¥ng Flash-LLM.

## 7 CÃ´ng trÃ¬nh LiÃªn quan vÃ  Tháº£o luáº­n

Viá»‡c thá»±c thi tÃ¡c vá»¥ ML song song vÃ  phÃ¢n tÃ¡n Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i cho huáº¥n luyá»‡n mÃ´ hÃ¬nh [6,7,19â€“21,24,26,27,30,31,35,46,49,60,62]. Vá»›i sá»± tÄƒng trÆ°á»Ÿng cá»§a kÃ­ch thÆ°á»›c mÃ´ hÃ¬nh, má»i ngÆ°á»i báº¯t Ä‘áº§u há»— trá»£ suy luáº­n LLM thÃ´ng qua tensor parallelism [21,49] Ä‘á»ƒ Ä‘áº·t tham sá»‘ lÃªn cÃ¡c thiáº¿t bá»‹ phÃ¢n tÃ¡n [1,37]. Tuy nhiÃªn, viá»‡c thá»±c thi phÃ¢n tÃ¡n cá»§a suy luáº­n mÃ´ hÃ¬nh Ä‘Æ°a ra chi phÃ­ giao tiáº¿p cao vÃ  Ä‘áº§u tÆ° kinh táº¿ cao. Má»™t sá»‘ cÃ´ng trÃ¬nh há»— trá»£ suy luáº­n LLM trÃªn má»™t GPU duy nháº¥t thÃ´ng qua giáº£m táº£i bá»™ nhá»› dá»¯ liá»‡u lÃªn bá»™ nhá»› CPU vÃ  tháº­m chÃ­ Ä‘Ä©a [1,48]. PhÆ°Æ¡ng phÃ¡p giáº£m táº£i chá»‰ hoáº¡t Ä‘á»™ng cho cÃ¡c á»©ng dá»¥ng khÃ´ng nháº¡y cáº£m vá»›i Ä‘á»™ trá»… (vÃ­ dá»¥ suy luáº­n offline vá»›i kÃ­ch thÆ°á»›c batch lá»›n), thay vÃ¬ cÃ¡c nhiá»‡m vá»¥ suy luáº­n trá»±c tuyáº¿n Ä‘Ã²i há»i Ä‘á»™ trá»… ráº¥t tháº¥p. Trong cÃ´ng trÃ¬nh nÃ y, chÃºng tÃ´i cho phÃ©p thá»±c thi suy luáº­n LLM hiá»‡u quáº£ vá»›i Ã­t GPU hÆ¡n thÃ´ng qua thá»±c thi SpMM hiá»‡u quáº£.

Cáº¯t tá»‰a mÃ´ hÃ¬nh lÃ  má»™t phÆ°Æ¡ng phÃ¡p phá»• biáº¿n Ä‘á»ƒ giáº£m sá»‘ lÆ°á»£ng tham sá»‘. Sparsity cÃ³ cáº¥u trÃºc lÃ  Ä‘á»ƒ thá»±c thi má»™t phÃ¢n phá»‘i cÃ³ cáº¥u trÃºc cá»§a cÃ¡c pháº§n tá»­ khÃ¡c khÃ´ng trong quÃ¡ trÃ¬nh cáº¯t tá»‰a, thÆ°á»ng cÃ³ thá»ƒ thÃ¢n thiá»‡n vá»›i gia tá»‘c pháº§n cá»©ng. GPU NVIDIA Ampere [36] há»— trá»£ sparsity cÃ³ cáº¥u trÃºc 2:4 [32] Ä‘á»ƒ thá»±c thi trÃªn tensor core. CuSPARSE [40] cÃ³ thá»ƒ há»— trá»£ SpMM cÃ³ cáº¥u trÃºc trÃªn tensor core dá»±a trÃªn Ä‘á»‹nh dáº¡ng Blocked-ELL [13], lÃ  má»™t sparsity cÃ³ cáº¥u trÃºc thÃ´ mÃ  cÃ¡c tham sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c cáº¯t tá»‰a á»Ÿ Ä‘á»™ chi tiáº¿t cá»§a má»™t khá»‘i vuÃ´ng (vÃ­ dá»¥ 32Ã—32). Má»™t sá»‘ cÃ´ng trÃ¬nh cáº¯t tá»‰a cÃ¡c tham sá»‘ mÃ´ hÃ¬nh á»Ÿ Ä‘á»™ chi tiáº¿t cá»§a vector Ä‘á»ƒ táº¡o thÃ nh Ä‘á»‹nh dáº¡ng thÆ°a cÃ³ cáº¥u trÃºc vÃ  sá»­ dá»¥ng GPU tensor core [4,18,25]. Trong khi tensor core cÃ³ thá»ƒ Ä‘Æ°á»£c kÃ­ch hoáº¡t vá»›i sparsity cÃ³ cáº¥u trÃºc nháº¥t Ä‘á»‹nh, má»‘i quan tÃ¢m chÃ­nh lÃ  cÃ¡c mÃ´ hÃ¬nh deep learning Ä‘Æ°á»£c cáº¯t tá»‰a vá»›i sparsity cÃ³ cáº¥u trÃºc thÆ°á»ng gáº·p pháº£i sá»± suy giáº£m Ä‘á»™ chÃ­nh xÃ¡c mÃ´ hÃ¬nh nghiÃªm trá»ng hÆ¡n so vá»›i sparsity khÃ´ng cÃ³ cáº¥u trÃºc [8, 12, 14, 16, 51, 54].

Sparsity khÃ´ng cÃ³ cáº¥u trÃºc lÃ  cáº¯t tá»‰a cÃ¡c pháº§n tá»­ mÃ  khÃ´ng táº¡o thÃ nh má»™t phÃ¢n phá»‘i cÃ³ cáº¥u trÃºc, thÆ°á»ng khÃ³ gia tá»‘c trÃªn cÃ¡c kiáº¿n trÃºc pháº§n cá»©ng hiá»‡n Ä‘áº¡i. STOREL [47] vÃ  TACO [23] lÃ  cÃ¡c thiáº¿t káº¿ dá»±a trÃªn CPU cÃ³ thá»ƒ há»— trá»£ SpMM vá»›i sparsity khÃ´ng cÃ³ cáº¥u trÃºc. Thay vÃ o Ä‘Ã³, Flash-LLM chá»§ yáº¿u táº­p trung vÃ o SpMM dá»±a trÃªn GPU vá»›i sparsity khÃ´ng cÃ³ cáº¥u trÃºc, vÃ¬ GPU Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i hÆ¡n cho cÃ¡c tÃ¡c vá»¥ deep-learning quy mÃ´ lá»›n vÃ¬ nÃ³ thÆ°á»ng cÃ³ bÄƒng thÃ´ng bá»™ nhá»› cao hÆ¡n, thÃ´ng lÆ°á»£ng tÃ­nh toÃ¡n, vÃ  hiá»‡u quáº£ nÄƒng lÆ°á»£ng. PhÆ°Æ¡ng phÃ¡p Ä‘iá»ƒn hÃ¬nh Ä‘á»ƒ thá»±c thi SpMM khÃ´ng cÃ³ cáº¥u trÃºc trÃªn GPU lÃ  thÃ´ng qua SIMT core, vÃ­ dá»¥ cuSPARSE [40], ASpT [17], vÃ  Sputnik [10]. DÆ°á»›i má»©c Ä‘á»™ sparsity vá»«a pháº£i (<90%), Sputnik vÆ°á»£t trá»™i Ä‘Ã¡ng ká»ƒ so vá»›i cuSPARSE vÃ  ASpT, nhÆ°ng nÃ³ gáº·p khÃ³ khÄƒn Ä‘á»ƒ Ä‘Ã¡nh báº¡i Ä‘á»‘i tÃ¡c dense cuBLAS [39] vÃ¬ nÃ³ khÃ´ng thá»ƒ sá»­ dá»¥ng tensor core. TC-GNN [57] há»— trá»£ sparsity khÃ´ng cÃ³ cáº¥u trÃºc vá»›i tensor core, Ä‘Æ°á»£c tÃ¹y chá»‰nh cho GNN nÆ¡i tá»· lá»‡ sparsity cá»±c cao (vÃ­ dá»¥ >99%) vÃ  khÃ´ng hiá»‡u quáº£ cho cÃ¡c mÃ´ hÃ¬nh sinh yÃªu cáº§u sparsity má»©c Ä‘á»™ vá»«a pháº£i. SparTA [65] Ä‘á» xuáº¥t sá»­ dá»¥ng cáº£ sparse tensor core [32] vÃ  SIMT core Ä‘á»ƒ há»— trá»£ sparsity khÃ´ng cÃ³ cáº¥u trÃºc báº±ng cÃ¡ch chia ma tráº­n thÆ°a gá»‘c thÃ nh má»™t ma tráº­n thÆ°a cÃ³ cáº¥u trÃºc 2:4 cho thá»±c thi tensor core (bá»Ÿi cuSPARSELt [41]) vÃ  má»™t ma tráº­n thÆ°a khÃ´ng cÃ³ cáº¥u trÃºc cho thá»±c thi SIMT core (bá»Ÿi Sputnik [10]). Tuy nhiÃªn, náº¿u tá»· lá»‡ thÆ°a cao, nÃ³ pháº£i Ä‘á»‡m quÃ¡ má»©c sá»‘ khÃ´ng vÃ o ma tráº­n thÆ°a 2:4. BÃªn cáº¡nh Ä‘Ã³, náº¿u cÃ³ quÃ¡ nhiá»u khÃ¡c khÃ´ng khÃ´ng thá»ƒ Ä‘Ã¡p á»©ng yÃªu cáº§u 2:4, kernel SIMT sáº½ gÃ¢y ra Ä‘á»™ trá»… cao vÃ  lÃ m cháº­m xá»­ lÃ½ tá»•ng thá»ƒ. Flash-LLM há»— trá»£ sparsity má»©c Ä‘á»™ vá»«a pháº£i trÃªn tensor core hiá»‡u quáº£ vÃ  khÃ´ng yÃªu cáº§u phÃ¢n phá»‘i giá»‘ng 2:4. SparseTIR [59] há»— trá»£ sparsity khÃ´ng cÃ³ cáº¥u trÃºc vá»›i tensor core báº±ng cÃ¡ch chia cÃ¡c ma tráº­n thÆ°a thÃ nh cÃ¡c vector cá»™t 8Ã—1 vÃ  bá» qua cÃ¡c vector chá»‰ chá»©a sá»‘ khÃ´ng. NÃ³ khÃ´ng thá»ƒ hoáº¡t Ä‘á»™ng tá»‘t dÆ°á»›i sparsity vá»«a pháº£i (vÃ­ dá»¥, 80%) vÃ¬ ráº¥t Ã­t vector cÃ³ thá»ƒ Ä‘Æ°á»£c bá» qua. NÃ³ khÃ´ng thá»ƒ vÆ°á»£t trá»™i so vá»›i baseline dense cuBLAS cho Ä‘áº¿n khi sparsity cao hÆ¡n 95%, trong khi Flash-LLM cÃ³ thá»ƒ vÆ°á»£t trá»™i so vá»›i cuBLAS tá»« 60% sparsity.

Cáº¯t tá»‰a dá»±a trÃªn retraining [15,29] cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c sparsity má»©c Ä‘á»™ vá»«a pháº£i vá»›i Ä‘á»™ chÃ­nh xÃ¡c tá»‘t, trong khi cáº¯t tá»‰a post-training [8] chá»‰ cÃ³ thá»ƒ Ä‘áº¡t Ä‘Æ°á»£c tá»· lá»‡ thÆ°a khÃ¡ tháº¥p. Flash-LLM nháº¯m Ä‘áº¿n tá»‘i Æ°u hÃ³a SpMM vá»›i sparsity má»©c Ä‘á»™ vá»«a pháº£i (vÃ­ dá»¥ 60%-90%) Ä‘Æ°á»£c táº¡o ra vá»›i cáº¯t tá»‰a dá»±a trÃªn retraining. Cáº¯t tá»‰a dá»±a trÃªn retraining thÆ°á»ng tiÃªu thá»¥ chi phÃ­ fine-tuning cao, do Ä‘Ã³ trá»Ÿ thÃ nh má»™t háº¡n cháº¿ cá»§a Flash-LLM.

Quantization mÃ´ hÃ¬nh [16] lÃ  má»™t phÆ°Æ¡ng phÃ¡p khÃ¡c Ä‘á»ƒ giáº£m bá»™ nhá»› vÃ  tÃ­nh toÃ¡n cho cÃ¡c mÃ´ hÃ¬nh ML, báº±ng cÃ¡ch chuyá»ƒn Ä‘á»•i kiá»ƒu dá»¯ liá»‡u thÃ nh bit tháº¥p hÆ¡n (vÃ­ dá»¥, 8-bits, 4-bits) [9,25]. Cáº¯t tá»‰a mÃ´ hÃ¬nh vÃ  quantization lÃ  hai phÆ°Æ¡ng phÃ¡p trá»±c giao vÃ  bá»• sung cho nÃ©n mÃ´ hÃ¬nh. BÃ i bÃ¡o nÃ y chá»§ yáº¿u táº­p trung vÃ o há»— trá»£ cáº¯t tá»‰a mÃ´ hÃ¬nh, lÃ  trá»±c giao vá»›i quantization mÃ´ hÃ¬nh.

## 8 Káº¿t luáº­n

ChÃºng tÃ´i Ä‘á» xuáº¥t Flash-LLM, má»™t thÆ° viá»‡n Ä‘á»ƒ suy luáº­n mÃ´ hÃ¬nh sinh lá»›n hiá»‡u quáº£ thÃ´ng qua sparsity khÃ´ng cÃ³ cáº¥u trÃºc vá»›i tensor core. ChÃºng tÃ´i quan sÃ¡t tháº¥y ráº±ng MatMul trong suy luáº­n mÃ´ hÃ¬nh sinh thÆ°á»ng má»ng, vÃ  bá»‹ rÃ ng buá»™c bá»Ÿi truy cáº­p bá»™ nhá»› off-chip. ChÃºng tÃ´i Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p Load-as-Sparse vÃ  Compute-as-Dense cho tensor core SpMM, giáº£m dáº¥u chÃ¢n bá»™ nhá»› toÃ n cáº§u, giáº£i quyáº¿t nÃºt tháº¯t cá»• chai truy cáº­p bá»™ nhá»›, vÃ  chá»‹u Ä‘á»±ng cÃ¡c tÃ­nh toÃ¡n dÆ° thá»«a cá»§a MatMul má»ng. ChÃºng tÃ´i Ä‘á» xuáº¥t má»™t pipeline pháº§n má»m hiá»‡u quáº£ cho SpMM khÃ´ng cÃ³ cáº¥u trÃºc vá»›i tensor core, táº­n dá»¥ng hiá»‡u quáº£ tÃ i nguyÃªn on-chip cho trÃ­ch xuáº¥t dá»¯ liá»‡u thÆ°a vÃ  phá»‘i há»£p trÃ­ch xuáº¥t dá»¯ liá»‡u thÆ°a, táº£i dá»¯ liá»‡u dense, vÃ  tÃ­nh toÃ¡n tensor core theo cÃ¡ch chá»“ng chÃ©o. Flash-LLM vÆ°á»£t trá»™i so vá»›i cuBLAS/Sputnik/SparTA báº±ng 1.4Ã—/3.6Ã—/1.4Ã—, 1.7Ã—/3.0Ã—/1.4Ã—, 2.1Ã—/2.0Ã—/1.6Ã— dÆ°á»›i sparsity 70%, 80% vÃ  90%. ChÃºng tÃ´i tÃ­ch há»£p cÃ¡c kernel Flash-LLM vÃ o FasterTransformer cho suy luáº­n mÃ´ hÃ¬nh sinh Ä‘áº§u cuá»‘i. Äá»‘i vá»›i token per GPU-second, Flash-LLM Ä‘áº¡t Ä‘Æ°á»£c cáº£i thiá»‡n lÃªn Ä‘áº¿n 3.8Ã— vÃ  3.6Ã— so vá»›i DeepSpeed vÃ  FasterTransformer trÃªn cÃ¡c mÃ´ hÃ¬nh OPT-30B/66B/175B vá»›i chi phÃ­ suy luáº­n tháº¥p hÆ¡n Ä‘Ã¡ng ká»ƒ.

## TÃ i liá»‡u tham kháº£o

[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. DeepSpeed-inference: enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. 1â€“15.

[2] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. In Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models. https://arxiv.org/abs/2204.06745

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877â€“1901.

[4] Zhaodong Chen, Zheng Qu, Liu Liu, Yufei Ding, and Yuan Xie. 2021. Efficient tensor core-based GPU kernels for structured sparsity under reduced precision. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 1â€“14.

[5] EleutherAI. 2022. GPT-NeoX-20B. https://huggingface.co/EleutherAI/gpt-neox-20b

[6] Hugging Face. 2023. Model Parallelism. https://huggingface.co/docs/transformers/v4.15.0/parallelism

[7] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, and Wei Lin. 2021. DAPPLE: A pipelined data parallel approach for training large models. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. 431â€“445.

[8] Elias Frantar and Dan Alistarh. 2023. Massive Language Models Can Be Accurately Pruned in One-Shot. arXiv preprint arXiv:2301.00774 (2023).

[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. arXiv preprint arXiv:2210.17323 (2022).

[10] Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. 2020. Sparse gpu kernels for deep learning. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1â€“14.

[11] Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. 2020. sputnik github. https://github.com/google-research/sputnik/

[12] Aidan N Gomez, Ivan Zhang, Siddhartha Rao Kamalakara, Divyam Madaan, Kevin Swersky, Yarin Gal, and Geoffrey E Hinton. 2019. Learning sparse networks using targeted dropout. arXiv preprint arXiv:1905.13678 (2019).

[13] Scott Gray, Alec Radford, and Diederik P Kingma. 2017. Gpu kernels for block-sparse weights. arXiv preprint arXiv:1711.09224 3 (2017), 2.

[14] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).

[15] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural network. Advances in neural information processing systems 28 (2015).

[16] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 2021. Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res. 22, 241 (2021), 1â€“124.

[17] Changwan Hong, Aravind Sukumaran-Rajam, Israt Nisa, Kunal Singh, and P Sadayappan. 2019. Adaptive sparse tiling for sparse matrix multiplication. In Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming. 300â€“314.

[18] Guyue Huang, Haoran Li, Minghai Qin, Fei Sun, Yufei Ding, and Yuan Xie. 2022. Shfl-BW: Accelerating Deep Neural Network Inference with Tensor-Core Aware Weight Pruning. In Proceedings of the 59th ACM/IEEE Design Automation Conference (San Francisco, California) (DAC '22). Association for Computing Machinery, New York, NY, USA, 1153â€“1158. https://doi.org/10.1145/3489517.3530588

[19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems 32 (2019).

[20] Xianyan Jia, Le Jiang, Ang Wang, Wencong Xiao, Ziji Shi, Jie Zhang, Xinyuan Li, Langshi Chen, Yong Li, Zhen Zheng, Xiaoyong Liu, and Wei Lin. 2022. Whale: Efficient Giant Model Training over Heterogeneous GPUs. In 2022 USENIX Annual Technical Conference, USENIX ATC 2022, Carlsbad, CA, USA, July 11-13, 2022, Jiri Schindler and Noa Zilberman (Eds.). USENIX Association, 673â€“688.

[21] Zhihao Jia, Matei Zaharia, and Alex Aiken. 2019. Beyond Data and Model Parallelism for Deep Neural Networks. Proceedings of Machine Learning and Systems 1 (2019), 1â€“13.

[22] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT. 4171â€“4186.

[23] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe. 2017. The Tensor Algebra Compiler. Proc. ACM Program. Lang. 1, OOPSLA, Article 77 (oct 2017), 29 pages. https://doi.org/10.1145/3133901

[24] Alexandros Koliousis, Pijika Watcharapichat, Matthias Weidlich, Luo Mai, Paolo Costa, and Peter Pietzuch. 2019. CROSSBOW: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers. Proceedings of the VLDB Endowment 12, 11 (2019).

[25] Shigang Li, Kazuki Osawa, and Torsten Hoefler. 2022. Efficient quantized sparse matrix operations on tensor cores. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. 1â€“15.

[26] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. PyTorch Distributed: Experiences on Accelerating Data Parallel Training. Proceedings of the VLDB Endowment 13, 12 (2020).

[27] Youjie Li, Amar Phanishayee, Derek Murray, Jakub Tarnawski, and Nam Sung Kim. 2022. Harmony: Overcoming the Hurdles of GPU Memory Capacity to Train Massive DNN Models on Commodity Servers. Proc. VLDB Endow. 15, 11 (jul 2022), 2747â€“2760.

[28] Mingbao Lin, Yuxin Zhang, Yuchao Li, Bohong Chen, Fei Chao, Mengdi Wang, Shen Li, Yonghong Tian, and Rongrong Ji. 2022. 1xn pattern for pruning convolutional neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).

[29] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. 2018. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270 (2018).

[30] Xupeng Miao, Xiaonan Nie, Yingxia Shao, Zhi Yang, Jiawei Jiang, Lingxiao Ma, and Bin Cui. 2021. Heterogeneity-aware distributed machine learning training via partial reduce. In Proceedings of the 2021 International Conference on Management of Data. 2262â€“2270.

[31] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui. 2022. Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism. Proc. VLDB Endow. 16, 3 (nov 2022), 470â€“479.

[32] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021. Accelerating Sparse Deep Neural Networks. arXiv:2104.08378 [cs.LG]

[33] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. 2019. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 11264â€“11272.

[34] Avanika Narayan, Ines Chami, Laurel Orr, and Christopher RÃ©. 2022. Can Foundation Models Wrangle Your Data? Proc. VLDB Endow. 16, 4 (dec 2022), 738â€“746.

[35] Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma, Gang Cao, and Bin Cui. 2023. FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement. arXiv preprint arXiv:2304.03946 (2023).

[36] NVIDIA. 2020. NVIDIA A100 Tensor Core GPU Architecture. https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf

[37] NVIDIA. 2022. NVIDIA Faster-Transformer. https://github.com/NVIDIA/FasterTransformer

[38] NVIDIA. 2022. NVIDIA H100 Tensor Core GPU Architecture. https://www.hpctech.co.jp/catalog/gtc22-whitepaper-hopper_v1.01.pdf

[39] NVIDIA. 2023. cuBLAS Docs. https://docs.nvidia.com/cuda/cublas/index.html

[40] NVIDIA. 2023. cuSPARSE Library. https://docs.nvidia.com/cuda/cusparse/index.html

[41] NVIDIA. 2023. cuSPARSELt Library. https://docs.nvidia.com/cuda/cusparselt/

[42] NVIDIA. 2023. CUTLASS 3.2. https://github.com/NVIDIA/cutlass

[43] NVIDIA. 2023. Nsight Compute Profiling Guide. https://docs.nvidia.com/nsight-compute/ProfilingGuide/#introduction

[44] NVIDIA. 2023. Nsight System. https://developer.nvidia.com/nsight-systems

[45] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.

[46] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1â€“16.

[47] Maximilian Schleich, Amir Shaikhha, and Dan Suciu. 2023. Optimizing Tensor Programs on Flexible Storage. Proceedings of the ACM on Management of Data 1, 1 (2023), 1â€“27.

[48] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. 2023. High-throughput generative inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865 (2023).

[49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).

[50] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. arXiv:2201.11990 [cs.CL]

[51] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2023. A Simple and Effective Pruning Approach for Large Language Models. arXiv:2306.11695 [cs.CL]

[52] Immanuel Trummer. 2022. From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management. Proc. VLDB Endow. 15, 12 (aug 2022), 3770â€“3773.

[53] Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings. 2022. TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data. Proc. VLDB Endow. 15, 6 (feb 2022), 1201â€“1214.

[54] Karen Ullrich, Edward Meeds, and Max Welling. 2017. Soft weight-sharing for neural network compression. arXiv preprint arXiv:1702.04008 (2017).

[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).

[56] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems 32 (2019).

[57] Yuke Wang, Boyuan Feng, Zheng Wang, and Yufei Ding. 2023. TC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs. arXiv:2112.02052 [cs.LG]

[58] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM 52, 4 (2009), 65â€“76.

[59] Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze. 2023. SparseTIR: Composable abstractions for sparse compilation in deep learning. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3. 660â€“678.

[60] Xiaodong Yi, Shiwei Zhang, Ziyue Luo, Guoping Long, Lansong Diao, Chuan Wu, Zhen Zheng, Jun Yang, and Wei Lin. 2020. Optimizing distributed training deployment in heterogeneous GPU clusters. In CoNEXT '20: The 16th International Conference on emerging Networking EXperiments and Technologies, Barcelona, Spain, December, 2020, Dongsu Han and Anja Feldmann (Eds.). ACM, 93â€“107.

[61] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. arXiv:2205.01068 [cs.CL]

[62] Zhen Zhang, Shuai Zheng, Yida Wang, Justin Chiu, George Karypis, Trishul Chilimbi, Mu Li, and Xin Jin. 2022. MiCS: near-linear scaling for training gigantic model on public cloud. Proceedings of the VLDB Endowment 16, 1 (2022), 37â€“50.

[63] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, and Ion Stoica. 2022. Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning. In 16th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, Marcos K. Aguilera and Hakim Weatherspoon (Eds.). USENIX Association, 559â€“578.

[64] Ningxin Zheng. 2022. SparTA github. https://github.com/microsoft/SparTA/tree/sparta_artifact

[65] Ningxin Zheng, Bin Lin, Quanlu Zhang, Lingxiao Ma, Yuqing Yang, Fan Yang, Yang Wang, Mao Yang, and Lidong Zhou. 2022. {SparTA}:{Deep-Learning} Model Sparsity via{Tensor-with-Sparsity-Attribute}. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 213â€“232.
