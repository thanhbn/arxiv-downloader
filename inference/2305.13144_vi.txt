# 2305.13144.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/inference/2305.13144.pdf
# Kích thước tệp: 430473 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Nhận thức độ dài phản hồi và lập lịch trình tự:
Một pipeline suy luận LLM được tăng cường bởi LLM
Zangwei Zheng1, Xiaozhe Ren2, Fuzhao Xue1, Yang Luo1, Xin Jiang2, Yang You1
1Khoa Khoa học Máy tính, Đại học Quốc gia Singapore
2Noah's Ark Lab, Huawei.
{zangwei, f-xue, yangluo, youy}@comp.nus.edu.sg; {renxiaozhe, jiang.xin}@huawei.com
https://github.com/zhengzangw/Sequence-Scheduling
Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) đã cách mạng hóa lĩnh vực AI, thể hiện khả năng chưa từng có trên nhiều nhiệm vụ khác nhau. Tuy nhiên, quá trình suy luận cho LLMs đi kèm với chi phí tính toán đáng kể. Trong bài báo này, chúng tôi đề xuất một pipeline suy luận LLM hiệu quả khai thác sức mạnh của LLMs. Cách tiếp cận của chúng tôi bắt đầu bằng việc khai thác tiềm năng của LLMs để nhận thức và dự đoán chính xác độ dài phản hồi với chi phí tối thiểu. Bằng cách tận dụng thông tin này, chúng tôi giới thiệu một kỹ thuật lập lịch trình tự hiệu quả tập hợp các truy vấn có độ dài phản hồi tương tự thành các micro-batch. Chúng tôi đánh giá cách tiếp cận của mình trên các bộ dữ liệu hướng dẫn thực tế sử dụng mô hình dựa trên LLaMA, và kết quả của chúng tôi thể hiện cải thiện ấn tượng 86% về thông lượng suy luận so với suy luận batch vanilla mà không làm giảm hiệu quả. Đáng chú ý, phương pháp của chúng tôi trực giao với các kỹ thuật tăng tốc suy luận khác, khiến nó trở thành một bổ sung có giá trị cho nhiều bộ công cụ hiện có (ví dụ: FlashAttention, Quantization) cho suy luận LLM.
1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) [2, 6, 15, 19] đã biến đổi lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) và đã thể hiện thành công đáng kể trong nhiều nhiệm vụ NLP khác nhau như dịch ngôn ngữ [23], hỏi đáp [28], và tóm tắt văn bản [38]. Tuy nhiên, việc triển khai LLMs ở quy mô lớn đặt ra thách thức do chi phí suy luận cực kỳ đắt đỏ [1, 27]. Tài nguyên tính toán cần thiết để xử lý hàng triệu truy vấn, như trường hợp với các LLMs hiện đang được triển khai như ChatGPT [19], là rất lớn. Do đó, giảm chi phí suy luận của LLMs đã trở thành một hướng nghiên cứu quan trọng trong những năm gần đây.

Trong các tình huống thực tế, độ dài của phản hồi cho các truy vấn khác nhau có sự biến động đáng kể. Như được mô tả trong Hình 2a, mặc dù các mô hình khác nhau hiển thị phân bố độ dài phản hồi hơi khác nhau, một mẫu chung xuất hiện với sự hiện diện của độ dài phản hồi trên một phạm vi rộng. Do đó, khi thực hiện suy luận mô hình ngôn ngữ lớn (LLM) theo batch, việc bao gồm các chuỗi với độ dài phản hồi khác nhau dẫn đến thiếu hiệu quả. Các chuỗi ngắn hơn bị buộc phải chờ các chuỗi dài hơn hoàn thành, dẫn đến lãng phí tính toán. Vấn đề này được mô tả ở phía bên trái của Hình 1, nơi các token dư thừa chiếm một phần đáng kể (66%) của tổng số token được tạo ra. Với độ phức tạp thời gian bậc hai của suy luận, những thiếu hiệu quả như vậy áp đặt gánh nặng đáng kể lên quá trình suy luận.

Con người có khả năng ước lượng độ dài của câu trả lời cho một câu hỏi dựa trên sự hiểu biết của họ về truy vấn. Ví dụ, những câu hỏi như "Thủ đô của Pháp là gì?" thường gợi ra những phản hồi ngắn hơn so với những câu hỏi như "Bạn có thể giải thích lịch sử của Cách mạng Pháp không?" Thú vị thay, chúng tôi quan sát thấy rằng các LLMs được tinh chỉnh để hiểu hướng dẫn, như ChatGPT và Claude, cũng thể hiện một mức độ nhận thức độ dài phản hồi nhất định. Hơn nữa, ngay cả các mô hình nhỏ hơn

Bản thảo. Đang được xem xét.arXiv:2305.13144v2  [cs.CL]  28 May 2023

--- TRANG 2 ---
Hướng dẫnPhản hồiHướng dẫnNhận thức Độ dài Phản hồiĐộ dài Dự đoán5
514462
243
35
112
6
63
Lập lịch Trình tựPhản hồi
Kích thước micro-batch thay đổi theo độ dàiDự đoán saiThu thập Thất bại
Tính toán dư thừaĐộ dài dự đoánHướng dẫnPhản hồiMicro-batch4Tính toán lại Thất bạiThất bại từ các micro-batch khác66%
15%Hình 1: Trái: Suy luận batch Vanilla dẫn đến hiệu suất kém với 66% token dư thừa khi phản hồi ngắn và dài nằm trong cùng một batch. Phải: Pipeline của lập lịch trình tự của chúng tôi. Đầu tiên, module nhận thức độ dài phản hồi ước lượng độ dài phản hồi cho mỗi hướng dẫn. Lập lịch trình tự nhóm các hướng dẫn có độ dài dự đoán tương tự lại với nhau và kích thước batch lớn hơn cho phản hồi ngắn hơn. Chiến lược thu thập thất bại và tính toán lại được áp dụng để tránh dự đoán sai làm giảm hiệu suất.

0 100 200 300 400 500
Độ dài Phản hồi050010001500200025003000Tần suấtChatGPT
Vicuna
(a) Phân bố độ dài phản hồi của 10k hướng dẫn từ ChatGPT và Vicuna. Độ dài phản hồi lớn hơn 512 được cắt ngắn.

0 200 400 600 800 1000
Điểm Dữ liệu0100200300400500Độ dài Phản hồi(b) Phân bố độ dài trung bình giữa 3 lần tạo ra trên 1k hướng dẫn. Thanh lỗi biểu thị độ dài tối đa và tối thiểu trong việc tạo ra.

Hình 2: Phân bố độ dài phản hồi và phương sai.

như LLaMA-7B, khi được tinh chỉnh hướng dẫn trên bộ dữ liệu dự đoán độ dài, có thể có được khả năng này. Mặc dù có sự biến động trong độ dài đầu ra dưới nhiều lần lấy mẫu, như được thể hiện trong Hình 2b, những mô hình này đạt được hiệu suất ấn tượng trong việc nhận thức độ dài phản hồi của chúng.

Tận dụng khả năng nhận thức độ dài phản hồi của LLMs, chúng ta có thể sử dụng nó để nâng cao việc lập lịch các hướng dẫn trong micro-batches. Điều này đại diện cho một ví dụ về thiết kế phần mềm-phần cứng đồng thời, nơi khả năng cố hữu của LLMs đóng góp vào việc tăng tốc quá trình suy luận LLM. Hệ thống lập lịch trình tự được đề xuất của chúng tôi thông minh nhóm các truy vấn dựa trên độ dài phản hồi được nhận thức, giảm thiểu hiệu quả lãng phí tính toán. Để cải thiện thêm hiệu quả, chúng tôi giới thiệu chiến lược thu thập thất bại và tính toán lại, cũng như cách tiếp cận kích thước batch thay đổi. Những kỹ thuật bổ sung này bổ sung cho hệ thống lập lịch trình tự và đóng góp vào việc cải thiện thêm thông lượng suy luận.

Để đánh giá hiệu quả của cách tiếp cận được đề xuất của chúng tôi, chúng tôi thực hiện các thí nghiệm trên bộ dữ liệu hướng dẫn thực tế sử dụng Vicuna [4], một mô hình LLaMA được tinh chỉnh hướng dẫn [34]. Module dự đoán độ dài của chúng tôi vượt trội so với hiệu suất của các phương pháp trước đây trong việc ước lượng chính xác độ dài phản hồi. Hệ thống lập lịch trình tự của chúng tôi thể hiện cải thiện đáng kể về thông lượng suy luận, đạt được mức tăng 86% so với quá trình suy luận ban đầu, tất cả trong khi duy trì chất lượng hiệu suất. Những kết quả này làm nổi bật tiềm năng của lập lịch trình tự với nhận thức độ dài phản hồi như một bổ sung có giá trị cho bộ công cụ hiện có (ví dụ: Flash Attention [7], Quantization [8, 11, 35]) cho suy luận mô hình ngôn ngữ lớn.

2

--- TRANG 3 ---
Để tóm tắt, những đóng góp của chúng tôi như sau:
•Chúng tôi điều tra khả năng nhận thức độ dài phản hồi của LLMs và chứng minh rằng việc tinh chỉnh hướng dẫn có thể nâng cao khả năng này.
•Chúng tôi giới thiệu một pipeline suy luận LLM mới gọi là lập lịch trình tự tận dụng khả năng nhận thức độ dài phản hồi của LLMs. Cách tiếp cận này thông minh nhóm các truy vấn có độ dài phản hồi tương tự, giảm lãng phí tính toán và cải thiện thông lượng suy luận mà không làm giảm hiệu suất.
•Chúng tôi trình bày kết quả thí nghiệm toàn diện trên bộ dữ liệu hướng dẫn thực tế sử dụng mô hình Vicuna-7B. Phương pháp được đề xuất của chúng tôi đạt được cải thiện ấn tượng 86% về thông lượng suy luận so với quá trình suy luận ban đầu.

2 Công trình Liên quan
Mô hình Ngôn ngữ Lớn Như-một-dịch-vụ. Các Mô hình Ngôn ngữ Lớn (LLMs) [2, 6, 15, 19] đã thành công trong việc xây dựng các mô hình nền tảng mạnh mẽ bằng cách mở rộng các mô hình ngôn ngữ lên quy mô lớn. Với việc tinh chỉnh hướng dẫn [25], LLMs có thể phù hợp với yêu cầu của con người và cung cấp chúng như một dịch vụ cho việc sử dụng thực tế. Hiện tại, các LLMs như ChatGPT [19] và PaLM [6] đã được triển khai trong Bing và Bard như một dịch vụ và thực hiện một lượng lớn suy luận mỗi ngày. Do đó, giảm chi phí suy luận của LLMs là một hướng nghiên cứu quan trọng.

Suy luận LLM Hiệu quả. Trong những năm gần đây, đã có sự quan tâm ngày càng tăng trong việc phát triển các kỹ thuật suy luận hiệu quả cho các mô hình ngôn ngữ lớn (LLMs) [18]. Kernel fusion [5, 7] bao gồm việc sử dụng các kernel được tối ưu hóa cao để giảm truy cập bộ nhớ và cải thiện tốc độ tính toán. Các phương pháp song song, như song song pipeline [17, 30] và song song tensor [22, 30], đã được sử dụng để phân phối khối lượng công việc trên nhiều GPU, cho phép mở rộng hiệu quả của suy luận LLM. Quantization [8, 11, 35] cũng đã được khám phá như một phương tiện nén các tham số của LLMs để suy luận hiệu quả. Ngoài những phương pháp này, đã có một số công trình về tối ưu hóa xử lý batch cho LLMs [3, 10, 29]. Ví dụ, [3] tập trung vào batchifying các truy vấn trong cài đặt few-shot, trong khi [10] đề xuất nhóm các câu thành batch dựa trên độ dài đầu vào. Đối với LLM, chi phí tạo ra vượt quá forward của prompts. Phương pháp của chúng tôi tập trung vào quá trình tạo ra và nhóm các câu theo độ dài đầu ra được dự đoán.

Dự đoán Độ dài Phản hồi. Công trình trước đây về dự đoán độ dài phản hồi chủ yếu tập trung vào các nhiệm vụ dịch thuật tạo ra không tự hồi quy (NAR) [14]. Trong những nhiệm vụ này, toàn bộ câu được tạo ra cùng một lúc, vì vậy việc dự đoán độ dài của phản hồi là quan trọng. Nhiều kỹ thuật khác nhau đã được đề xuất để giải quyết vấn đề này. Ví dụ, [32] đề xuất một cách tiếp cận đơn giản dựa trên thống kê của bộ dữ liệu và một thuật ngữ bias, trong khi [13] dự đoán số lượng token mà mỗi token đầu vào sẽ được dịch thành. Một số phương pháp, như [9, 12], thêm một token [LENGTH] đặc biệt vào encoder, trong khi những phương pháp khác, như [20, 21, 24, 31], sử dụng một lớp pooling và bộ phân loại MLP để dự đoán độ dài phản hồi dựa trên các đầu ra của encoder. Tuy nhiên, những phương pháp này chủ yếu áp dụng cho các nhiệm vụ dịch máy, nơi độ dài chuỗi đích tương tự như độ dài nguồn và do đó dễ dự đoán hơn. Ngược lại, cách tiếp cận được đề xuất của chúng tôi được thiết kế cụ thể cho các nhiệm vụ suy luận mô hình ngôn ngữ lớn, nơi các loại truy vấn và độ dài phản hồi tương ứng của chúng rất đa dạng.

3 Nhận thức Độ dài Phản hồi

3.1 Nhận thức Trước (PiA)
Các LLMs được tinh chỉnh hướng dẫn đã cho thấy khả năng phù hợp với sự hiểu biết của con người và cung cấp các phản hồi hữu ích và an toàn. Thú vị thay, chúng tôi đã phát hiện ra rằng những mô hình này có sự hiểu biết tổng thể về toàn bộ phản hồi mà chúng sắp tạo ra, tương tự như cách con người xây dựng phản hồi của họ. Trong các thí nghiệm của chúng tôi, chúng tôi đã yêu cầu những mô hình này dự đoán độ dài của các phản hồi mà chúng sắp tạo ra, mặc dù không có nhiệm vụ rõ ràng nào cho nhận thức độ dài phản hồi được bao gồm trong quá trình pretrain.

Như được hiển thị trong Bảng 1, chúng tôi giới thiệu một sửa đổi cho prompt ban đầu bằng cách bao gồm một hướng dẫn để ước lượng độ dài phản hồi trước. Chúng tôi gọi phương pháp này là Nhận thức Trước (PiA). Chúng tôi áp dụng prompt được sửa đổi này cho nhiều LLMs khác nhau và quan sát phản hồi của chúng đối với hướng dẫn.

3

--- TRANG 4 ---
Bảng 1: Ví dụ về tạo ra với Nhận thức Trước (PiA). Các mô hình đã học hướng dẫn có thể tuân theo hướng dẫn PiA. Các mô hình nhỏ (Vicuna) có thể thất bại trong việc tạo ra phản hồi.

Prompt:
Tạo một câu hỏi toán vui cho trẻ em.
Trước khi phản hồi hướng dẫn trên, bạn phải dự đoán độ dài phản hồi của mình. In số từ ước tính trong phản hồi của bạn ở dòng đầu tiên. Sau đó chuyển sang dòng mới để phản hồi hướng dẫn.

GPT-4:
Độ dài phản hồi ước tính: 60 từ.
Chắc chắn, đây là một bài toán vui: Có 7 quả táo trong một cái giỏ. Một con sóc thân thiện đến và · · ·

ChatGPT:
4từ
Nếu bạn có 10 quả táo và cho 3 quả cho bạn của mình, bạn còn lại bao nhiêu quả? <EOS>

Claude:
225
Đây là một câu hỏi toán vui cho trẻ em: Jenny người Tung hứng đang luyện tập màn biểu diễn của cô ấy. Trong một tay cô ấy cầm · · ·

Vicuna:
101<EOS>

Bảng 2: Hiệu suất nhận thức độ dài phản hồi qua Nhận thức Trước trên các LLMs khác nhau.

Nhận thức Trước Chỉ Nhận thức
Error(w) ↓Acc-50 ↑Acc-100 ↑ Error(w) ↓Acc-50 ↑Acc-100 ↑Failure ↓
GPT-4 22 80% 100% 100 28% 55% 0%
ChatGPT 51 77% 90% 89 55% 68% 2%
Claude 37 64% 96% 63 52% 84% 0%
Bard 70 44% 72% 130 28% 50% 28%
HugginChat-30B 77 52% 72% 113 56% 72% 12%
Vicuna-13B 94 49% 73% 92 55% 75% 0%
Vicuna-7B 123 40% 65% 122 40% 65% 0%

Hiểu biết về Độ dài Phản hồi. Các thí nghiệm của chúng tôi chứng minh rằng các LLMs được tinh chỉnh hướng dẫn có sự hiểu biết mạnh mẽ về ước lượng độ dài phản hồi khi được cung cấp các hướng dẫn PiA. Để đánh giá hiệu quả của phương pháp PiA, chúng tôi thực hiện kiểm tra sử dụng 175 hướng dẫn alpaca seed [33]. Kết quả, được hiển thị trong Bảng 2, chỉ ra rằng GPT-4, ChatGPT, và Vicuna đã tuân theo thành công các hướng dẫn cho tất cả các truy vấn.

Một khía cạnh thú vị mà chúng tôi khám phá ra trong quá trình phân tích của mình là LLMs thể hiện sự hiểu biết tốt hơn về từ so với token. Quan sát này phù hợp với cách con người hiểu và tạo ra phản hồi. Bằng cách xem xét từ như đơn vị đo lường, chúng ta có thể có được những ước lượng đáng tin cậy hơn về độ dài phản hồi.

Để đánh giá định lượng độ chính xác của nhận thức độ dài phản hồi, chúng tôi sử dụng chỉ số Error(w), đo lường sự khác biệt giữa số từ ước tính và số từ thực tế. Chúng tôi cũng xem xét hai ngưỡng cho độ chính xác: Acc-50 và Acc-100, trong đó một dự đoán được coi là chính xác nếu sự khác biệt rơi dưới 50 và 100 từ, tương ứng. Phân tích của chúng tôi tiết lộ rằng cả GPT-4 và Claude đều thể hiện hiệu suất đặc biệt trong ước lượng độ dài phản hồi. Chúng đạt được lỗi dưới 50 từ và thể hiện điểm số Acc-100 vượt quá 90%. Những kết quả này chứng minh độ chính xác cao và độ tin cậy của các dự đoán độ dài phản hồi được tạo ra bởi những mô hình này. Hơn nữa, đáng chú ý rằng các mô hình có số lượng tham số cao hơn thể hiện hiệu suất vượt trội trong dự đoán độ dài phản hồi.

Tác dụng Phụ của PiA đối với Tạo ra Phản hồi. Việc giới thiệu EiA có tác dụng phụ đối với quá trình tạo ra phản hồi. Đầu tiên, vì độ dài ước tính có thể nhìn thấy được đối với mô hình trong giai đoạn tạo ra, nó có thể ảnh hưởng đến cách mô hình tạo ra phản hồi. Mô hình có thể nhận thức độ dài ước tính như một ràng buộc và cố gắng điều chỉnh phản hồi của nó để phù hợp với độ dài được dự đoán. Hành vi này có thể được xem như một tạo ra có giới hạn độ dài.

Để điều tra tác động của PiA đối với tạo ra phản hồi, chúng tôi so sánh lỗi trong nhận thức độ dài phản hồi giữa phương pháp PiA và phương pháp Chỉ Nhận thức (PO). Trong trường hợp này, chúng tôi so sánh độ dài phản hồi của các hướng dẫn không được sửa đổi với các giá trị độ dài được nhận thức. Lưu ý rằng

4

--- TRANG 5 ---
Bảng 3: Hiệu suất nhận thức độ dài phản hồi trên 10k hướng dẫn cho kết quả suy luận Vicuna.

Error↓Acc-50 ↑Acc-100 ↑
GPT-2 (1.5B)
Pooling + MLP 127 35% 53%
[LEN]-token Fine-tune 92 43% 64%
LLaMA-7B
Pooling + MLP 127 35% 53%
[LEN]-token Fine-tune 81 46% 70%
Vicuna-7B
Pooling + MLP 73 55% 75%
[LEN]-token Fine-tune 84 47% 72%
Perception Only 193 38% 59%
Instruction Tuning 63 56% 81%

độ dài phản hồi có thể thay đổi qua các lần tạo ra sampling khác nhau từ cùng một hướng dẫn. Hình 2b minh họa sự biến động trong độ dài phản hồi cho 1.000 điểm dữ liệu, làm nổi bật phạm vi rộng của các độ dài có thể. Do đó, không có "sự thật cơ bản" rõ ràng cho độ dài phản hồi mà thay vào đó là một phạm vi các độ dài có thể. Để đơn giản hóa nhiệm vụ ước lượng, chúng tôi nhắm tới việc mô hình dự đoán độ dài tiềm năng tối đa, vì việc sử dụng độ dài trung bình có hạn chế, như được thảo luận trong phần tiếp theo.

Đối với các LLMs nhỏ hơn, như Vicuna 7B và 13B, chúng tôi quan sát thấy rằng chúng gần như bỏ qua độ dài ước tính. Mặt khác, GPT-4 và Claude thể hiện xu hướng mạnh mẽ hơn để điều chỉnh câu trả lời của chúng cho phù hợp với độ dài ước tính, dẫn đến số lỗi nhỏ hơn đáng kể.

Ngoài ra, chúng tôi quan sát thấy rằng việc giới thiệu phương pháp PiA có thể tác động tiêu cực đến chất lượng phản hồi đối với các LLMs nhỏ hơn. Ví dụ, trong trường hợp của Vicuna-7B, chúng tôi nhận thấy những trường hợp mà mô hình thất bại trong việc tạo ra phản hồi sau khi dự đoán độ dài. Hành vi này có thể được quy cho khả năng hạn chế của các LLMs nhỏ hơn trong việc xử lý nhiều nhiệm vụ đồng thời.

Chỉ Nhận thức khó hơn. Để giải quyết các tác dụng phụ liên quan đến tạo ra phản hồi bị ảnh hưởng bởi độ dài ước tính, chúng tôi áp dụng kiểu Chỉ Nhận thức (PO) cho lập lịch trình tự vì nó tách biệt các quá trình dự đoán và tạo ra.

Một cách tiếp cận đơn giản để thực hiện PO là sử dụng cùng một prompt nhưng chỉ lấy dự đoán độ dài. Sau đó chúng tôi so sánh độ dài ước tính này với phản hồi thực tế được tạo ra bằng cách sử dụng các prompt ban đầu. Như được trình bày trong Bảng 2, rõ ràng là mặc dù LLMs vẫn có khả năng ước lượng độ dài, tỷ lệ lỗi của chúng cao hơn đáng kể, và điểm số chính xác thấp hơn so với cách tiếp cận Nhận thức Trước (PiA).

3.2 Tinh chỉnh Hướng dẫn
Trong khi cách tiếp cận Nhận thức Trước có thể đủ cho GPT-4 về mặt tác dụng phụ nhỏ đối với quá trình tạo ra và cho phép lập lịch trình tự, chúng tôi nhắm tới việc tách biệt hoàn toàn các giai đoạn dự đoán và tạo ra để tránh bất kỳ ảnh hưởng tiềm năng nào của độ dài ước tính. Ngoài ra, chúng tôi muốn trao quyền cho các mô hình nhỏ hơn khả năng nhận thức chính xác độ dài phản hồi. Để đạt được những mục tiêu này, chúng tôi sử dụng tinh chỉnh hướng dẫn [25].

Trong giai đoạn tinh chỉnh hướng dẫn, chúng tôi sử dụng một định dạng prompt được sửa đổi khuyến khích mô hình dự đoán độ dài của phản hồi thay vì tạo ra toàn bộ phản hồi. Chúng tôi chọn một tập con gồm 10.000 prompt từ bộ dữ liệu alpaca [33] và đặt độ dài mục tiêu là độ dài tối đa được quan sát từ bốn lần quá trình tạo ra. Văn bản mục tiêu chỉ là một số, vì vậy chi phí tạo ra trong quá trình dự đoán được giảm thiểu. Để tối ưu hóa quá trình huấn luyện và giảm tài nguyên tính toán, chúng tôi sử dụng phương pháp huấn luyện hiệu quả LoRA [16], yêu cầu bộ nhớ không đáng kể so với LLM, và huấn luyện mô hình trong ba epoch. Để biết thêm chi tiết về thiết lập thí nghiệm, chúng tôi cung cấp thông tin toàn diện trong phần phụ lục của bài báo này.

Bảng 3 trình bày kết quả thí nghiệm, thể hiện sự cải thiện đạt được thông qua tinh chỉnh hướng dẫn. Lỗi dự đoán được giảm đáng kể từ 193 xuống 63, và Acc-50 cho thấy cải thiện

5

--- TRANG 6 ---
1 2 4 8 16 32 64
Kích thước Batch0.20.40.60.8Thông lượng (samples/s)
max new tokens: 512
dừng khi tất cả hoàn thành(a) Khi tạo ra độ dài cố định (xanh), thông lượng tăng gần như tuyến tính. Khi tạo ra cho đến khi hoàn thành trong một batch, phản hồi dài trong kích thước batch lớn làm giảm hiệu suất.

16 32 64 128 256 512 1024
Kích thước Nhóm1.21.41.61.82.02.22.4Thông lượng (samples/s)
(b) Kích thước nhóm lớn hơn làm cho lập lịch hiệu quả hơn. Lập lịch thất bại khi kích thước nhóm quá nhỏ (32). Cải thiện của nhiều hướng dẫn hơn trong một nhóm giảm khi kích thước nhóm tăng.

Hình 3: Thông lượng so với kích thước batch và kích thước nhóm.

18%. Chúng tôi cũng so sánh cách tiếp cận tinh chỉnh hướng dẫn của chúng tôi với các phương pháp dự đoán độ dài trước đây được sử dụng trong tạo ra NAR, như tinh chỉnh với token độ dài và sử dụng MLP để phân loại các trạng thái ẩn được pooled. Mặc dù những phương pháp này cũng thể hiện cải thiện hiệu suất, chúng vẫn kém hiệu quả so với tinh chỉnh hướng dẫn. Những kết quả này làm nổi bật khả năng của mô hình trong việc hiểu và sử dụng hiệu quả hướng dẫn được cung cấp. Hơn nữa, khi sử dụng các mô hình thay thế như LLaMA-7B hoặc các mô hình nhỏ hơn như GPT-2 để tạo ra dự đoán độ dài cho Vicuna, cách tiếp cận pooling + MLP thất bại hoàn toàn, và tinh chỉnh với token độ dài kém hiệu quả khi so sánh với khả năng tự dự đoán của Vicuna.

4 Lập lịch Trình tự

4.1 Phương pháp
Sau khi thiết lập một module nhận thức độ dài phản hồi chính xác, bây giờ chúng ta có thể tận dụng nó để cho phép lập lịch trình tự cho suy luận hiệu quả. Như được minh họa ở phía bên trái của Hình 1, khi các hướng dẫn có độ dài phản hồi rất khác nhau được batch lại với nhau, những tính toán dư thừa đáng kể xảy ra, dẫn đến giảm thông lượng suy luận. Do đó, bằng cách nhóm các hướng dẫn có độ dài phản hồi tương tự lại với nhau, chúng ta có thể tăng tốc quá trình suy luận.

Trước khi đi sâu vào các chi tiết cụ thể của lập lịch trình tự, điều quan trọng là phải hiểu tầm quan trọng của suy luận với kích thước micro-batch lớn (mbs). Như được mô tả trong Hình 3a, việc triển khai Vicuna trên GPU A100 80GB làm nổi bật lợi ích của kích thước batch lớn hơn trong việc tận dụng sức mạnh tính toán song song của GPU. Khi tạo ra một số token cố định cho mỗi mẫu, thông lượng thể hiện cải thiện gần như tuyến tính lên đến kích thước batch 16, sau đó tỷ lệ cải thiện chậm lại. Mặt khác, nếu việc tạo ra của mỗi batch dừng lại khi tất cả các mẫu đã hoàn thành việc tạo ra phản hồi của chúng, thông lượng cũng tăng tuyến tính cho kích thước batch nhỏ hơn 16, với tỷ lệ cao hơn so với cách tiếp cận token cố định. Tuy nhiên, khi kích thước batch tiếp tục tăng, hiệu suất bắt đầu giảm. Điều này là do các kích thước batch lớn hơn có xác suất cao bao gồm độ dài phản hồi dài hơn, dẫn đến tính toán dư thừa đáng kể.

Để cho phép lập lịch trình tự hiệu quả, chúng tôi đưa ra giả định rằng số lượng hướng dẫn cần xử lý tại một thời điểm (kích thước nhóm) lớn hơn kích thước micro-batch cho một GPU duy nhất, điều này đúng với việc sử dụng rộng rãi của LLMs. Trong khi một cách tiếp cận đơn giản cho lập lịch trình tự là sắp xếp các hướng dẫn theo độ dài dự đoán của chúng và chia chúng thành các batch để xử lý, chúng tôi khám phá các thiết kế bổ sung để tăng tốc thêm thông lượng. Pipeline của phương pháp chúng tôi được mô tả ở phía bên phải của Hình 1.

Thu thập Thất bại và Tính toán lại (FCR) Mặc dù bộ dự đoán độ dài đạt được độ chính xác khá cao là 81% (acc-100), vẫn có khả năng một số mẫu có dự đoán sai lệch đáng kể so với độ dài phản hồi thực. Những dự đoán không chính xác này có thể làm rối loạn đáng kể hiệu quả của xử lý batch. Ví dụ, nếu một phản hồi dài bị dự đoán nhầm là ngắn

6

--- TRANG 7 ---
220ms69ms69ms69ms70ms64ms64ms70ms64ms64msmbs=1mbs=169%91%VanillaOurs14%70%16%Xử lý hướng dẫnTạo ra một tokenXử lý hướng dẫnTạo ra Phản hồiDự đoán Độ dài Phản hồiHình 4: Trên: So sánh thời gian giữa xử lý hướng dẫn và tạo ra token. Xử lý hướng dẫn chỉ mất thời gian để tạo ra một vài token. Dưới: Biểu đồ phân bố phần trăm cho các thành phần khác nhau của phương pháp vanilla và phương pháp của chúng tôi.

và được bao gồm trong một batch với chủ yếu là phản hồi ngắn, thời gian xử lý tổng thể bị ảnh hưởng vì các truy vấn ngắn bị buộc phải chờ cho việc hoàn thành truy vấn dài. Để giảm thiểu vấn đề này, chúng tôi triển khai một cơ chế gọi là Thu thập Thất bại và Tính toán lại (FCR). Chúng tôi hạn chế số lượng token mới được tạo ra tối đa là độ dài dự đoán tối đa trong một batch. Bất kỳ hướng dẫn nào vượt quá độ dài dự đoán này được coi là thất bại và được thu thập riêng để tính toán lại thêm ở cuối quá trình suy luận kích thước nhóm. Với tỷ lệ thất bại tương đối thấp, cách tiếp cận này cho phép tạo ra nhanh hơn các phản hồi ngắn với thời gian hạn chế dành cho việc tạo lại các hướng dẫn thất bại.

Kích thước Batch Thay đổi (VBS) Một cân nhắc quan trọng trong lập lịch trình tự là việc sử dụng bộ nhớ trong quá trình tạo ra phản hồi. Phản hồi ngắn hơn yêu cầu ít bộ nhớ hơn so với phản hồi dài hơn. Tuy nhiên, nếu chúng ta phân bổ kích thước batch lớn hơn mà không xem xét khả năng phân loại sai phản hồi dài thành ngắn, chúng ta có thể gặp lỗi hết bộ nhớ. Với sự giúp đỡ của Thu thập lại Thất bại, chúng tôi giới thiệu kỹ thuật Kích thước Batch Thay đổi (VBS). Chúng tôi phân bổ kích thước batch lớn hơn cho phản hồi ngắn hơn. Một quy tắc đơn giản là duy trì cùng số lượng token trong mỗi batch. Với kích thước batch cơ sở B0 tương ứng với độ dài cụ thể L0, chúng tôi điều chỉnh kích thước batch dựa trên độ dài mong muốn L sử dụng công thức B0·L/L0. Cách tiếp cận này tối ưu hóa việc sử dụng bộ nhớ bằng cách cho phép kích thước batch lớn hơn cho phản hồi ngắn hơn trong khi ngăn ngừa tràn bộ nhớ do RC gây ra.

Binning và Dự đoán Độ dài Tối đa Trong việc huấn luyện bộ dự đoán độ dài, chúng tôi sử dụng chiến lược binning phân loại độ dài mục tiêu thành các bin. Trong cách tiếp cận của chúng tôi, chúng tôi sử dụng các bin với kích thước ô là 50 và làm tròn các số đến bin gần nhất lớn hơn độ dài thực tế. Đầu tiên, mục tiêu dự đoán số bin đơn giản và dễ dàng hơn vì nó chỉ yêu cầu ước lượng gần đúng. Thứ hai, binning cung cấp bộ đệm cho các lỗi tiềm năng trong dự đoán độ dài. Ngay cả khi độ dài thực tế sai lệch trong cùng một bin, nó không ảnh hưởng đến hiệu quả của phương pháp chúng tôi.

Ngoài ra, chúng tôi chọn dự đoán độ dài tối đa của bốn lần quá trình tạo ra bởi vì hậu quả của việc đánh giá thấp nghiêm trọng hơn so với đánh giá quá cao. Bằng cách dự đoán độ dài tối đa, chúng tôi đảm bảo rằng các phản hồi được tạo ra có đủ khả năng để chứa ngay cả đầu ra dài nhất có thể. Dự đoán độ dài trung bình, mặt khác, sẽ dẫn đến tỷ lệ thu thập lại thất bại cao hơn, vì nó có thể đánh giá thấp độ dài cho một số truy vấn, dẫn đến sự gián đoạn tiềm năng trong quá trình batching.

Chi phí của Dự đoán Độ dài Với việc chúng ta phải dự đoán độ dài của tất cả các hướng dẫn trong một nhóm trước khi tạo ra phản hồi của chúng, có một chi phí cố hữu liên quan đến dự đoán độ dài phản hồi. Chi phí này bao gồm việc tính toán các key và value cho các token hướng dẫn và tạo ra một số lượng nhỏ token (thường là 2 đến 3 token). Tuy nhiên, như được mô tả trong Hình 4, thời gian xử lý cho các hướng dẫn cực kỳ nhanh, chỉ yêu cầu thời gian để tạo ra một vài token (từ 1 đến 4 token). Do đó, chi phí này có thể được bù đắp hiệu quả bởi tăng tốc tổng thể đạt được thông qua lập lịch trình tự.

4.2 Thí nghiệm
Thiết lập Thí nghiệm. Các thí nghiệm của chúng tôi được thực hiện trên hai bộ dữ liệu: một tập hợp 10.000 prompt từ một tập con của bộ dữ liệu alpaca [33] (khác với tập được sử dụng để huấn luyện độ dài

7

--- TRANG 8 ---
Bảng 4: Hiệu suất lập lịch trình tự với module nhận thức độ dài phản hồi khác nhau.

Thông lượng (samples/s) ↑Cải thiện ↑Độ dài trung bình ↓
Vanilla 1.22 377
Ground Truth Preditor 2.52 +107% 201
Pooling + MLP 1.96 +61% 216
[LEN]-token Fine-tune 2.10 +72% 210
Perception Only* 1.40 +15% 328
Instruction Tunning (mean) 1.77 +45% 211
Instruction Tunning (max) 2.27 +86% 208

Bảng 5: Nghiên cứu ablation về ba thành phần của phương pháp lập lịch trình tự.

Binning FCR VBS Thông lượng (samples/s) ↑Cải thiện ↑Độ dài trung bình ↓
× × × 1.73 +42% 271
× ✓ × 1.76 +44% 222
× ✓ ✓ 2.22 +82% 209
✓ × × 1.58 +30% 300
✓ ✓ × 1.86 +52% 209
✓ ✓ ✓ 2.27 +86% 203

predictor) và một tập hợp 429 prompt từ bộ dữ liệu Instruction-in-Wild [36]. Tập trước bao gồm các prompt tự hướng dẫn, trong khi tập sau chứa các prompt được thu thập từ thế giới thực.

Đối với các thí nghiệm cơ sở của chúng tôi, chúng tôi đặt kích thước batch là 16. Về chiến lược kích thước batch thay đổi, chúng tôi sử dụng kích thước batch 16 cho các hướng dẫn có độ dài (L) lớn hơn hoặc bằng 300. Đối với các hướng dẫn có độ dài dưới 300, chúng tôi tính kích thước batch sử dụng công thức 256×50/L và sau đó làm tròn đến lũy thừa 2 gần nhất để sử dụng GPU tốt hơn. Chúng tôi duy trì kích thước nhóm cố định là 256. Suy luận được thực hiện trên mô hình Vicuna-7B [4] sử dụng GPU A100 80GB. Chúng tôi lấy mẫu các lần tạo ra với nhiệt độ 0.5 để có sự đa dạng trong phản hồi.

Chúng tôi đánh giá thông lượng ở cấp độ mẫu và báo cáo số lượng mẫu được xử lý mỗi giây và cải thiện tương ứng so với baseline vanilla, nơi suy luận được thực hiện với kích thước batch 16. Độ dài trung bình đề cập đến độ dài trung bình của mỗi batch, nơi độ dài trung bình nhỏ hơn chỉ ra tính toán dư thừa giảm.

Kết quả. Bảng 4 trình bày hiệu suất lập lịch trình tự với các module nhận thức độ dài phản hồi khác nhau. Trong số các bộ dự đoán độ dài được xem xét, bộ có tinh chỉnh hướng dẫn thể hiện hiệu suất tốt nhất, đạt được cải thiện ấn tượng 85% về thông lượng. Điều quan trọng cần lưu ý là phương pháp chỉ ước lượng thể hiện tỷ lệ thu thập lại thất bại cao hơn đáng kể, dẫn đến hiệu suất giảm. Do đó, chúng tôi báo cáo kết quả thu được bằng cách sử dụng cách tiếp cận lập lịch trình tự vanilla.

Khi dự đoán độ dài trung bình, tỷ lệ thu thập lại thất bại tăng lên 29.8%, cao hơn đáng kể so với 15.3% đạt được khi dự đoán độ dài tối đa. Do đó, cải thiện hiệu suất giảm xuống 45%. Ngoài ra, nếu chúng ta sử dụng sự thật cơ bản (tức là, độ dài tối đa được quan sát trong nhiều lần suy luận) như bộ dự đoán độ dài, nó phục vụ như một giới hạn trên cho phương pháp này. Cách tiếp cận của chúng tôi hoạt động chỉ chậm hơn 0.25 samples/s so với giới hạn trên, thể hiện rằng một bộ dự đoán độ dài hiệu quả có thể mang lại cải thiện đáng kể.

Hơn nữa, phương pháp của chúng tôi thể hiện phương sai thấp trong thông lượng, với giá trị 0.05 qua ba lần thí nghiệm. Điều này chỉ ra tính ổn định và nhất quán của cách tiếp cận của chúng tôi trong việc đạt được tốc độ suy luận được cải thiện.

Bảng 5 trình bày nghiên cứu ablation về ba thành phần của cách tiếp cận của chúng tôi, thể hiện đóng góp cá nhân của chúng đối với cải thiện tổng thể. Chúng tôi quan sát thấy rằng Binning nâng cao hiệu quả của Kích thước Batch Thay đổi (VBS), dẫn đến sự kết hợp mạnh mẽ hơn. Mỗi thành phần đóng vai trò quan trọng trong việc đạt được cải thiện cuối cùng về thông lượng. Hơn nữa, Bảng 6 thể hiện hiệu suất của phương pháp chúng tôi trên bộ dữ liệu Instruction-in-Wild, xác nhận hiệu quả của cách tiếp cận của chúng tôi trên các bộ dữ liệu khác nhau.

8

--- TRANG 9 ---
Bảng 6: Hiệu suất lập lịch trình tự trên bộ dữ liệu Instruction-in-Wild.

Thông lượng (samples/s) ↑Độ dài trung bình ↓Error↓Acc-50 ↑Acc-100 ↑
Vanilla 0.78 475 – – –
Estimation Only 1.07 422 358 20% 38%
Instruction Tunning 1.24 299 139 43% 68%

Ngoài ra, ở phía bên phải của Hình 3b, chúng tôi phân tích mối quan hệ giữa thông lượng và kích thước nhóm. Chúng tôi quan sát thấy rằng kích thước nhóm lớn hơn cung cấp tính linh hoạt hơn cho lập lịch trình tự, dẫn đến thông lượng được cải thiện. Tuy nhiên, tỷ lệ cải thiện dần dần chậm lại khi kích thước nhóm tăng. Do đó, việc tìm ra sự cân bằng giữa chi phí liên quan đến việc thu thập thông tin kích thước nhóm trong các tình huống thế giới thực và cải thiện đạt được trở nên quan trọng.

5 Hạn chế và Thảo luận
Một hạn chế là dự đoán độ dài phản hồi chính xác không phải lúc nào cũng được đảm bảo, ngay cả với module dự đoán độ dài được tinh chỉnh hướng dẫn mà chúng tôi phát triển. Trong khi các thí nghiệm của chúng tôi thể hiện kết quả hứa hẹn, vẫn còn chỗ để cải thiện độ chính xác của ước lượng độ dài phản hồi.

Bên cạnh đó, mặc dù lập lịch trình tự giảm đáng kể tính toán dư thừa, nó không thể hoàn toàn loại bỏ vấn đề. Ngay cả với bộ dự đoán độ dài phản hồi sự thật cơ bản, tỷ lệ dư thừa giảm từ 66% xuống 33%, để lại nhiều chỗ cho cải thiện thêm. Các công trình gần đây như ORCA [37] đã đề xuất các hệ thống suy luận mới cung cấp các giải pháp thay thế để giảm thiểu vấn đề tính toán dư thừa.

Một hạn chế khác của công trình chúng tôi là chúng tôi tập trung ít hơn vào quá trình của các hướng dẫn đầu vào. Khi độ dài token tối đa được hỗ trợ bởi LLMs tăng, người dùng có thể nhập các hướng dẫn rất dài, dẫn đến tính toán dư thừa và thiếu hiệu quả. Công trình tương lai có thể khám phá sự kết hợp của lập lịch trình tự được đề xuất của chúng tôi với các kỹ thuật lập lịch batch đầu vào [10], để tối ưu hóa thêm quá trình suy luận.

Cách tiếp cận của chúng tôi giả định rằng kích thước nhóm vượt quá khả năng của một GPU duy nhất. Khi các mô hình ngôn ngữ lớn có thể trở thành cơ sở hạ tầng phổ biến, tương tự như các công cụ tìm kiếm, số lượng truy vấn sẽ tăng đáng kể. Hơn nữa, sự xuất hiện của các mô hình như GPT-4 với hỗ trợ độ dài chuỗi 32k và Claude với hỗ trợ độ dài chuỗi 100K khuếch đại thách thức xử lý độ dài phản hồi thay đổi, làm nổi bật tính liên quan và hiệu quả của phương pháp chúng tôi.

Cách tiếp cận của chúng tôi dễ dàng mở rộng cho cài đặt đa GPU, nơi nhiều GPU được sử dụng để xử lý nhanh hơn và xử lý kích thước mô hình lớn hơn. Bằng cách phân bổ lại các batch chuỗi với độ dài phản hồi được nhận thức khác nhau cho các nút GPU khác nhau, phương pháp của chúng tôi vẫn hiệu quả trong môi trường tính toán hiệu suất cao. Phần mở rộng này đảm bảo khả năng mở rộng và duy trì lợi ích hiệu quả đạt được trong cách tiếp cận của chúng tôi.

Các phát hiện của chúng tôi chỉ ra rằng các mô hình ngôn ngữ lớn (LLMs) có sự hiểu biết toàn diện về các phản hồi được tạo ra của chúng. Sự hiểu biết này mở ra khả năng cho việc tạo ra các kỹ thuật suy luận nhanh hơn, bao gồm các phương pháp không tự hồi quy, có thể vượt qua các ràng buộc hiệu suất liên quan đến việc tạo ra token tuần tự.

6 Kết luận
Chúng tôi đầu tiên nghiên cứu liệu LLMs có thể nhận thức độ dài phản hồi trước khi giải mã tự hồi quy và phát hiện các LLMs hiện tại hoạt động tốt khi sử dụng cách tiếp cận Nhận thức Trước của chúng tôi. Bằng cách tận dụng khả năng dự đoán độ dài phản hồi, chúng tôi đề xuất lập lịch trình tự để tăng tốc suy luận LLM. Cách tiếp cận của chúng tôi tập hợp các truy vấn có độ dài phản hồi tương tự trong các batch, giảm lãng phí tính toán và cải thiện thông lượng suy luận. Chúng tôi giới thiệu thêm các khái niệm thu thập lại thất bại và kích thước batch thay đổi để nâng cao hiệu quả. Kết quả thí nghiệm trên bộ dữ liệu hướng dẫn thế giới thực sử dụng mô hình Vicuna-7B thể hiện cải thiện 86% về thông lượng mà không hy sinh chất lượng hiệu suất. Phương pháp của chúng tôi cung cấp một bổ sung có giá trị cho bộ công cụ hiện có cho suy luận LLM, giải quyết thách thức triển khai hiệu quả LLMs ở quy mô lớn.

9

--- TRANG 10 ---
Lời cảm ơn
Nhóm nghiên cứu của Yang You được tài trợ bởi học bổng khởi nghiệp NUS (Presidential Young Professorship), học bổng Singapore MOE Tier-1, học bổng ByteDance, học bổng ARCTIC, học bổng SMI và học bổng Alibaba. Công trình này được tài trợ bởi Huawei Noah's Ark Lab.

Tài liệu tham khảo
[1] R. Y. Aminabadi et al., "Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale," arXiv preprint arXiv:2207.00032, 2022.
[2] T. Brown et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.
[3] Z. Cheng, J. Kasai, and T. Yu, "Batch prompting: Efficient inference with large language model apis," arXiv preprint arXiv:2301.08721, 2023.
[4] W.-L. Chiang et al., Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, Mar. 2023. [Online]. Available: https://lmsys.org/blog/2023-03-30-vicuna/.
[5] J. Choi, H. Li, B. Kim, S. Hwang, and J. H. Ahn, "Accelerating transformer networks through recomposing softmax layers," in 2022 IEEE International Symposium on Workload Characterization (IISWC), IEEE, 2022, pp. 92–103.
[6] A. Chowdhery et al., "Palm: Scaling language modeling with pathways," arXiv preprint arXiv:2204.02311, 2022.
[7] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré, "Flashattention: Fast and memory-efficient exact attention with io-awareness," Advances in Neural Information Processing Systems, vol. 35, pp. 16 344–16 359, 2022.
[8] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, "Llm.int8(): 8-bit matrix multiplication for transformers at scale," arXiv preprint arXiv:2208.07339, 2022.
[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," arXiv preprint arXiv:1810.04805, 2018.
[10] J. Fang, Y. Yu, C. Zhao, and J. Zhou, "Turbotransformers: An efficient gpu serving system for transformer models," in Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2021, pp. 389–402.
[11] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, "GPTQ: Accurate post-training compression for generative pretrained transformers," arXiv preprint arXiv:2210.17323, 2022.
[12] M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer, "Mask-predict: Parallel decoding of conditional masked language models," arXiv preprint arXiv:1904.09324, 2019.
[13] J. Gu, J. Bradbury, C. Xiong, V. O. Li, and R. Socher, "Non-autoregressive neural machine translation," arXiv preprint arXiv:1711.02281, 2017.
[14] J. Gu and X. Tan, "Non-autoregressive sequence generation," in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, 2022, pp. 21–27.
[15] J. Hoffmann et al., "Training compute-optimal large language models," arXiv preprint arXiv:2203.15556, 2022.
[16] E. J. Hu et al., "Lora: Low-rank adaptation of large language models," arXiv preprint arXiv:2106.09685, 2021.
[17] Y. Huang et al., "Gpipe: Efficient training of giant neural networks using pipeline parallelism," Advances in neural information processing systems, vol. 32, 2019.
[18] S. Kim et al., "Full stack optimization of transformer inference: A survey," arXiv preprint arXiv:2302.14017, 2023.
[19] A. Koubaa, "Gpt-4 vs. gpt-3.5: A concise showdown," 2023.
[20] J. Lee, E. Mansimov, and K. Cho, "Deterministic non-autoregressive neural sequence modeling by iterative refinement," arXiv preprint arXiv:1802.06901, 2018.
[21] J. Lee, R. Shu, and K. Cho, "Iterative refinement in the continuous space for non-autoregressive neural machine translation," arXiv preprint arXiv:2009.07177, 2020.
[22] S. Li et al., "Colossal-ai: A unified deep learning system for large-scale parallel training," arXiv preprint arXiv:2110.14883, 2021.

10

--- TRANG 11 ---
[23] C. Lyu, J. Xu, and L. Wang, "New trends in machine translation using large language models: Case examples with chatgpt," arXiv preprint arXiv:2305.01181, 2023.
[24] X. Ma, C. Zhou, X. Li, G. Neubig, and E. Hovy, "Flowseq: Non-autoregressive conditional sequence generation with generative flow," arXiv preprint arXiv:1909.02480, 2019.
[25] L. Ouyang et al., "Training language models to follow instructions with human feedback," Advances in Neural Information Processing Systems, vol. 35, pp. 27 730–27 744, 2022.
[26] A. Paszke et al., "Automatic differentiation in pytorch," 2017.
[27] R. Pope et al., "Efficiently scaling transformer inference," arXiv preprint arXiv:2211.05102, 2022.
[28] J. Robinson, C. M. Rytting, and D. Wingate, "Leveraging large language models for multiple choice question answering," arXiv preprint arXiv:2210.12353, 2022.
[29] Y. Sheng et al., "High-throughput generative inference of large language models with a single gpu," arXiv preprint arXiv:2303.06865, 2023.
[30] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," arXiv preprint arXiv:1909.08053, 2019.
[31] R. Shu, J. Lee, H. Nakayama, and K. Cho, "Latent-variable non-autoregressive neural machine translation with deterministic inference using a delta posterior," in Proceedings of the aaai conference on artificial intelligence, vol. 34, 2020, pp. 8846–8853.
[32] Z. Sun, Z. Li, H. Wang, D. He, Z. Lin, and Z. Deng, "Fast structured decoding for sequence models," Advances in Neural Information Processing Systems, vol. 32, 2019.
[33] R. Taori et al., Stanford alpaca: An instruction-following llama model, https://github.com/tatsu-lab/stanford_alpaca, 2023.
[34] H. Touvron et al., "Llama: Open and efficient foundation language models," arXiv preprint arXiv:2302.13971, 2023.
[35] X. Wu, C. Li, R. Y. Aminabadi, Z. Yao, and Y. He, "Understanding int4 quantization for transformer models: Latency speedup, composability, and failure cases," arXiv preprint arXiv:2301.12017, 2023.
[36] F. Xue, K. Jain, M. H. Shah, Z. Zheng, and Y. You, Instruction in the wild: A user-based instruction dataset, https://github.com/XueFuzhao/InstructionWild, 2023.
[37] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, "Orca: A distributed serving system for {transformer-based} generative models," in 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), 2022, pp. 521–538.
[38] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B. Hashimoto, "Benchmarking large language models for news summarization," arXiv preprint arXiv:2301.13848, 2023.

11

--- TRANG 12 ---
Phụ lục

A Chi tiết Triển khai Tinh chỉnh Hướng dẫn Bổ sung
Để tinh chỉnh module dự đoán độ dài được tinh chỉnh hướng dẫn, chúng tôi tuân theo một thủ tục cụ thể. Đầu tiên, chúng tôi chuẩn bị bộ dữ liệu bằng cách lấy mẫu mỗi hướng dẫn bốn lần. Một mẫu được thu thập bằng cách sử dụng giải mã tham lam với nhiệt độ 0, trong khi ba mẫu khác được tạo ra sử dụng nhiệt độ 1.0 với các seed ngẫu nhiên khác nhau. Độ dài tối đa trong số bốn mẫu được sử dụng làm độ dài mục tiêu cho mỗi hướng dẫn.

Đối với bộ dữ liệu huấn luyện, chúng tôi xây dựng các hướng dẫn mới bằng cách thêm yêu cầu nhận thức độ dài phản hồi chỉ. Prompt mà chúng tôi sử dụng cho mục đích này là: "Đừng xuất ra phản hồi cho hướng dẫn trên. Thay vào đó, bạn cần dự đoán số lượng token trong phản hồi của bạn. Chỉ xuất ra một số."

Trong quá trình huấn luyện, chúng tôi sử dụng cùng các siêu tham số như quá trình tinh chỉnh hướng dẫn Vicuna [4] trên LLaMA [34]. Cụ thể, chúng tôi đặt tỷ lệ học tập là 0.00005 và huấn luyện mô hình trong ba epoch. Chúng tôi áp dụng phương pháp LoRA [16] chỉ cho lớp tuyến tính query và key. Việc huấn luyện được thực hiện trên một GPU A100 80GB duy nhất. Tất cả mã được triển khai trong PyTorch [26].

B Phân bố của Bộ dữ liệu Instruction-in-Wild
Biểu đồ tần suất của độ dài phản hồi trên bộ dữ liệu Instruction-in-Wild [36] được hiển thị trong Hình 5. Độ dài phản hồi đa dạng hơn và chứa nhiều phản hồi có độ dài dài hơn.

0 100 200 300 400 500
Độ dài Phản hồi051015202530Tần suấtVicuna
(a) Phân bố độ dài phản hồi của bộ dữ liệu Instruction-in-Wild.

0 100 200 300 400
Điểm Dữ liệu0100200300400500Độ dài Phản hồi(b) Phân bố độ dài phản hồi trên bộ dữ liệu instruction-in-Wild theo mẫu.

Hình 5: Phân bố độ dài phản hồi trên bộ dữ liệu instruction-in-Wild.

C Thảo luận về Lập lịch Trình tự với PiA
Trong văn bản chính, chúng tôi đã trình bày kỹ thuật lập lịch trình tự sử dụng các mô hình được tinh chỉnh hướng dẫn, nơi trọng số LoRA được sử dụng để nhận thức độ dài phản hồi. Tuy nhiên, các LLMs gần đây như GPT-4 và Claude đã cho thấy hiệu suất hứa hẹn trong Nhận thức Trước (PiA), cho phép chúng tận dụng PiA cho lập lịch trình tự mà không cần trọng số bổ sung.

Để cải thiện thêm tốc độ suy luận trong cài đặt này, một cách tiếp cận tiềm năng là tái sử dụng cache key-value (kv) của các truy vấn đầu vào từ giai đoạn nhận thức độ dài phản hồi trong giai đoạn suy luận. Điều này loại bỏ nhu cầu tính toán lại kv-cache trên các hướng dẫn đầu vào, do đó tiết kiệm thời gian xử lý có giá trị.

Một chiến lược mà chúng tôi khám phá là offload kv-cache xuống bộ nhớ chính và sau đó tải nó trở lại bộ nhớ GPU. Tuy nhiên, thời gian truyền giữa CPU và GPU có thể đáng kể và ảnh hưởng lớn đến hiệu suất tổng thể, thường vượt quá thời gian tiết kiệm được bằng tính toán lại. Để giải quyết điều này, một cải thiện có thể là offload và tải chỉ một phần của kv-cache một cách bất đồng bộ, giảm chi phí thời gian truyền. Đây là một lĩnh vực mà chúng tôi để lại cho công trình tương lai và khám phá.

12

--- TRANG 13 ---
Một cách tiếp cận khác mà chúng tôi điều tra bao gồm việc nén kv-cache và lưu trữ nó trực tiếp trong bộ nhớ GPU. Chúng tôi áp dụng phương pháp quantization của FlexGen [29] để nén và phát hiện ra rằng nó có tác động tối thiểu đến hiệu suất. Tuy nhiên, cách tiếp cận này tiêu thụ bộ nhớ bổ sung và có thể dẫn đến kích thước batch nhỏ hơn, có thể làm giảm hiệu suất tổng thể. Một hướng tiềm năng để khám phá thêm là kết hợp các kỹ thuật nén và offloading để tạo ra sự cân bằng giữa việc sử dụng bộ nhớ và hiệu suất.

Xem xét những yếu tố này, chúng tôi đã chọn tiếp tục sử dụng chiến lược tính toán lại cho module nhận thức độ dài phản hồi PiA trong pipeline được đề xuất của chúng tôi. Trong khi có tiềm năng cho các tối ưu hóa thông qua offloading và nén, cần có điều tra và tinh chỉnh thêm để đạt được lợi ích hiệu suất đáng kể trong thực tế.

D Thời gian Suy luận Tăng với Chỉ số Vị trí Token

0 100 200 300 400 500
Chỉ số Vị trí Token0.07000.07250.07500.07750.08000.08250.0850Thời gian Suy luận
(a) batch-size=16

0 100 200 300 400 500
Chỉ số Vị trí Token0.063250.063500.063750.064000.064250.064500.064750.06500Thời gian Suy luận (b) batch-size=1

Hình 6: Thời gian suy luận tăng với chỉ số vị trí token.

Trong các mô hình transformer, thời gian suy luận cho các token nằm ở cuối chuỗi thường dài hơn do nhu cầu thao tác self-attention trên nhiều key và value hơn. Điều này dẫn đến việc tăng tuyến tính thời gian suy luận cần thiết cho các token khi chỉ số vị trí tăng, như được minh họa trong Hình 6. Do đó, việc tiết kiệm tính toán dư thừa trên các chuỗi dài hơn có tác động đáng kể hơn so với các chuỗi ngắn hơn. Quan sát này giải thích tại sao tỷ lệ tăng trưởng của thông lượng có thể cao hơn tỷ lệ các token dư thừa được tiết kiệm.

13
