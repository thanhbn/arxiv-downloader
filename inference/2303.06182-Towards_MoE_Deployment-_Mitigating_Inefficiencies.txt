# 2303.06182.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/inference/2303.06182.pdf
# File size: 2997572 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Towards MoE Deployment: Mitigating Inefficiencies
in Mixture-of-Expert (MoE) Inference
Haiyang Huang*†, Newsha Ardalani*, Anna Sun*, Liu Ke* ‡, Hsien-Hsin S. Lee*, Anjali Sridhar*, Shruti Bhosale*,
Carole-Jean Wu*, Benjamin Lee*$
∗Meta AI†Duke University$University of Pennsylvania‡Washington University in St. Louis
Abstract —Mixture-of-Experts (MoE) models have gained pop-
ularity in achieving state-of-the-art performance in a wide range
of tasks in computer vision and natural language processing.
They effectively expand the model capacity while incurring a
minimal increase in computation cost during training. However,
deploying such models for inference is difficult due to their
large size and complex communication pattern. In this work,
we provide a characterization of two MoE workloads, namely
Language Modeling (LM) and Machine Translation (MT) and
identify their sources of inefficiencies at deployment.
We propose three optimization techniques to mitigate sources
of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering,
and (3) Expert load balancing. We show that dynamic gating
improves maximum throughput by 6.21-11.23 ×for LM, 5.75-
10.98×for MT Encoder and 2.58-5.71 ×for MT Decoder. It also
reduces memory usage by up to 1.36 ×for LM and up to 1.1 ×
for MT. We further propose Expert Buffering, a new caching
mechanism that only keeps hot, active experts in GPU memory
while buffering the rest in CPU memory. This reduces static
memory allocation by up to 1.47 ×. We finally propose a load
balancing methodology that provides additional scalability to the
workload.
I. I NTRODUCTION
The prediction capability of a machine learning model is
strongly correlated with the model capacity, (i.e., the number
of parameters in the network). In pursuit of accuracy, capacity
has grown at an exponential pace of 10 times per year [28], ac-
companied by higher demand for computational resources and
extortionate training costs. Sparsely activated neural networks,
such as Mixture of Experts (MoE), are attractive model archi-
tectures that decouple the requirement for many parameters
from the computational costs. In a sparsely activated model,
parts of the network are conditionally activated, which reduces
training costs. Results from previous works [2], [7], [21],
[22], [30], [34] show that MoE models reduce training cost
yet improve model prediction performance in tasks such as
language modeling [2], [5], [7], [27], machine translation [22]
and image recognition [26], [33]. While training has been
relatively well studied, MoE deployment and inference has
received much less attention.
Characterizing and optimizing inference is increasingly im-
portant as large language models, like ChatGPT, are deployed
for production services. Figures 1 and 2 highlight model
†‡Work done while interning at Meta
Fig. 1. Comparison of MoE and Dense Language Models on training cost
and perplexity (the lower perplexity the better in model quality). MoE models
can achieve better performance than their dense counterparts at lower training
cost (Source: Artetxe et. al. [2]).
Fig. 2. Comparison of MoE and Dense models on single node inference
latency. While theoretically MoE models should be able to infer on a similar
latency as their flop-equivalent dense counterparts, we find that in practice
they are 15 ×slower for Language Modeling (LM), 22 ×slower for Machine
Translation (MT) encoder and 3 ×slower for Machine Translation decoder.
prediction capabilities as well as associated training and infer-
ence costs between the state-of-the-art MoE and dense model
architectures. In Figure 1, MoE models achieve the same level
of performance and quality ( i.e., perplexity) with half of the
training cost (GPU-days) compared to their dense counterparts.
However, when deployed for inference, MoE models are 15 ×
slower for language models (LM) and more than 3 ×slower for
machine translation (MT) compared to their FLOP-equivalent
dense counterpart, as shown in Figure 2.
A few strategies have been proposed to reduce MoE in-
ference latency. We might distill MoE models into much
smaller dense models with a similar number of FLOPs [2],
[7]. Although distillation reduces model size and inference
latency, it also reduces model quality. Lepikhin et. al. show
1arXiv:2303.06182v2  [cs.DC]  18 Jun 2023

--- PAGE 2 ---
that a 14.7 billion parameter Switch Transformer model retains
only 29% of its perplexity gain on language modeling after
distillation [21]. DeepSpeed-MoE and Tutel [16], [24] focus
on increasing parallelism and optimizing pipelines to increase
hardware utilization when deploying MoE models on hun-
dreds of GPUs. These optimizations are scoped narrowly and
mitigate inefficiencies in specific kernels for communication
collectives and GPU computation. However, these studies lack
a comprehensive analysis of inference latency and neglect
inefficiencies in the MoE algorithms themselves.
In this paper, we provide optimization strategies for effi-
cient MoE deployment, reducing inference costs with min-
imal impact on model quality. First, we characterize MoE
Transformer deployment on three important axes: inference
latency, memory usage, and expert activation. Our detailed
characterization establishes significant correlations between
expert activation patterns and deployment efficiency. Latency
and memory usage is high because expert activations are
highly sparse and query load is highly imbalanced across
experts,
Second, we analyze unique expert activation patterns to
propose a new, optimized gating policy—called Dynamic
Gating—and implement it on an open-source, state-of-the-
art MoE-based Transformer [23]. For Language Modeling
(LM) and Machine Translation (MT) across various datasets
and subtasks [8], [22], our system prototype for dynamic
gating improves inference throughput by 6.21-11.23 ×for
LM, 5.75-10.98 ×for MT Encoder and 2.58-5.71 ×for MT
Decoder by enabling larger batch sizes and smaller latencies.
Our optimization strategies complement previously proposed
optimizations on distillation, communication collectives, and
GPU kernels. When integrated with other optimizations, our
gating policy could achieve even greater benefits.
Finally, we take a closer look into expert activation patterns,
discovering significant imbalance in load distribution across
experts but high temporal locality. Based on these two key
observations, we propose Expert Buffering, which improves
memory efficiency by allocating a fixed, but limited, amount
of GPU memory for hot and active experts and relies on
CPU memory to buffer all other experts. The less frequently
accessed experts are brought into GPU memory as needed, re-
ducing demand for GPU memory significantly. Expert buffer-
ing is orthogonal to existing memory management techniques,
such as offloading. Our experiments show that expert buffering
reduces static memory usage by up to 1.47 ×on tasks that
demonstrate significant expert sparsity. To balance load, we
further propose a priori load balancing based on historical
expert activation data, and analyze its benefits for throughput.
To summarize, our contributions in this paper are as follows:
•We provide a thorough characterization of MoE deploy-
ment, identifying sources of inefficiencies by breaking
down inference latency and memory usage across differ-
ent components of the model architecture.
•We identify the gating function as a major contributing
factor to the high latency and large memory footprint of
MoE models. We propose a novel gating policy whichsignificantly reduces latency and memory consumption
while also enabling inference with larger batch sizes and
a smaller number of GPUs.
•We analyze expert activation patterns during inference
and discover a significant imbalance in load distribution
across experts but high temporal locality.
•We propose Expert Buffering, a new caching mechanism
that keeps only hot or active experts in GPU memory and
buffers the rest in CPU memory. The less frequently ac-
cessed experts are brought into GPU memory as needed.
This optimization can reduce static memory allocation in
GPU by 1.47 ×.
•We propose techniques to balance load across experts to
further improve memory usage and system robustness.
II. B ACKGROUND
A. Mixture-of-Experts Module
Using different models for different inputs has long been
discussed as a way to improve model versatility and robust-
ness. Mixture-of-Experts (MoE) module [29] is a practical
application of this idea for neural networks. An MoE module
(Figure 3) consists of multiple independent models (called ex-
perts), and a gating function that assigns inputs to each of the
experts. Each input only activates its assigned expert network,
which theoretically allows the model capacity ( i.e., the number
of parameters in the model) to expand ”outrageously” with
minimal computation efficiency loss.
B. Transformer Model Architecture
The Transformer architecture has gained popularity in com-
puter vision and natural language processing by defining the
state-of the-art on multiple tasks in these domains [4], [32].
From the top down, a Transformer consists of a tokenizer that
parses the input into tokens, and an encoder-decoder archi-
tecture consisting of dense transformer layers. The encoder
structure has N dense transformer layers, where N varies from
single digits to dozens across different model architectures.
Each dense transformer layer is composed of two blocks:
a multi-head attention (MHA) block, and a Feed-Forward
Network (FFN) block connected by a residual connection, as
shown in Figure 3. The decoder’s structure is very similar to
the encoder’s, except for an optional MHA layer that attends
to encoder output.
C. MoE Transformer Model Architecture
The MoE Transformer combines the MoE idea with the
Transformer architecture. In addition to the normal dense
Transformer layer, it introduces a new kind of layer with sparse
MoEs. Sparse MoE layers replace the FFN block with an MoE
block that consists of multiple different expert FFNs. Instead
of applying a single FFN to all the input tokens, it first uses a
gating function to decide which expert(s) is most suitable for
each token, and then routes the tokens to their corresponding
expert. Typically, a token is routed to one or two experts in a
policy that is referred to as top-1 or top-2 gating. Sparse MoE
2

--- PAGE 3 ---
Fig. 3. Visualization of MoE module, dense transformer encoder layer, MoE transformer encoder layer and MoE transformer encoder layer deployed with
expert parallelism. MHA stands for Multi-head attention block, whereas FFN stands for Feed-forward Network block. (a)MoE module introduced in [29]
(b)Dense Transformer Encoder Layer. A typical dense transformer layer consists of Multi-head Attention (MHA) followed by an FFN layer. (c)Naive MoE
Transformer layer. The single FFN block in dense transformer is replaced by a set of FFNs, called experts, that operate in parallel. Not all tokens are processed
by all experts. The gating function decides which experts will receive which tokens.. (d)MoE Transformer with expert parallelism. Each device only holds a
subset of all experts. Tokens assigned to non-local expert FFNs are dispatched to their assigned expert via an all-to-all communication collective. .
layers replace the dense transformer layers intermittently in
the multi-layer model architecture.
These modifications grant greater degrees of freedom to the
model and effectively expand the model size. Compared to
traditional Transformer models, where the FLOP count per
batch scales linearly with the number of parameters, MoE
networks require much less computation, thus allowing large
models to be trained efficiently. MoE Transformers have been
successful in reducing the training cost of large transformer
models [2], [5], [7], [21] and achieving high accuracy in vision,
text, speech and multitask learning area [11], [12], [20], [27],
[35].
D. Expert Parallelism
Compared to traditional Transformer models of the same
model capacity, MoE models offer an interesting trade-off.
MoE requires much less computation but much more memory
usage. Expert layers deploy many additional FFNs, which
increase model size and associated demands for memory
capacity near the compute device ( e.g., the GPU). To handle
this problem, GShard [21] proposes expert parallelism, which
distributes the workload across multiple devices to reduce
memory and computation per device.
With expert parallelism, MoE layers are distributed across
multiple devices. Each device holds only a subset of expert
FFNs and a copy of all the other parameters. When a token
is assigned to experts that reside on other devices, an all-to-
all communication collective sends the token to corresponding
devices. The tokens are processed by the expert and then sent
back by another all-to-all communication.
At maximum expert parallelism, which allocates one expert
per device, memory usage and FLOP count per device arecomparable to that from a dense transformer model. Since
the gating function is a lightweight linear layer, the overall
computational complexity of a batch is about the same as that
of a dense transformer with much fewer parameters. Never-
theless, the enormous size, the sparse activation of experts,
and the complex communication pattern between devices
hosting different experts poses severe challenges during model
deployment and inference.
III. C HARACTERIZATION OF THE MOE M ODEL
To characterize the workload of MoE Transformer models,
we study two major use cases: Language Modeling and Ma-
chine Translation. Language modeling generates the probabil-
ity an input sequence appears in natural text whereas machine
translation maps the input from one language to another. Both
tasks are core problems to natural language processing, and are
currently major applications of MoE Transformers. We choose
models in recent publications that achieved state-of-the-art as
our testbed. The details of the datasets and models can be
found in Table I.
The MoE model’s dense counterparts are selected to be
FLOP-equivalent, so they share most of the hyperparameters
with the MoE Transformers of interest including hidden di-
mensions, number of layers and attention heads. The only
difference is that the MoE Transformer replaces the FFN layer
with an MoE layer every MF layers. Capacity factor C,
a parameter unique to the MoE Transformer, controls how
many tokens can be processed by a single expert. Under the
original design, no matter how many tokens are assigned to an
expert, the expert will always process a number of tokens equal
toCtimes the sequence length. When too many tokens are
assigned to a single expert, excess tokens are dropped and not
3

--- PAGE 4 ---
Task Type Size E MF CF
LMDense 355M – – –
MoE 52B 512 2 0.05
MTDense 3.3B – – –
MoE 54.5B 128 4 1
Task Type Layers TD HD V ocab
LMDense 24 1024 4096 51200
MoE 24 1024 4096 51200
MTDense 48 2048 8192 256206
MoE 48 2048 8192 256206
Platform Specification
CPU2×Intel Xeon E5-2698 v4 at 2.2GHz
with 700GB memory
CPU-GPU 16GB/s via PCIe 3.0
GPU8×NVIDIA Tesla V100, with 5120 CUDA
cores, 32GB HBM2 memory at 900GB/s
connected by NVLink at 300GB/s
TABLE I
EXPERIMENTAL SETUP . LM: L ANGUAGE MODELING . MT: M ACHINE
TRANSLATION . E: NUMBER OF EXPERTS . MF: M OE L AYER FREQUENCY .
CF: C APACITY FACTOR . TD: T OKEN DIMENSION . HD: H IDDEN
DIMENSION . VOCAB : VOCABULARY SIZE . E, MF AND CF DO NOT APPLY
TO DENSE MODELS .
processed by any expert. When too few tokens are assigned,
unused capacity will be filled by zeros. We utilize the capacity
factor settings recommended by [2], [22]. Table I details the
experimental setup.
A. Latency and Memory Consumption
The most important metrics in machine learning model
deployment are execution time (i.e., latency) and memory
consumption. A shorter latency ensures a more timely re-
sponse from the service, whereas lower memory consumption
indicates lower resource usage and potential to accommodate
larger batch sizes. In this subsection, we compare MoE infer-
ence performance along these two axes. The mini batch size is
set to 8 for language modeling and 48 for machine translation.
We use a dense model of similar FLOPs as the baseline for
comparison.
Latency. Figures 2 shows the latency and memory con-
sumption of the MoE Transformers of interest and that of their
dense counterparts. Although in theory, MoE Transformers
exeucte a similar number of FLOPs compared to the baseline
dense models, in practice they are significantly slower. For the
Language Model, the dense model requires 74.2ms, whereas
the MoE Transformer requires more than 1.09s. For Machine
Translation, the dense model executes the encoder and decoder
in 101ms and 32ms, respectively, but the MoE Transformer
requires 2.26s and 90ms.
Figure 5 breaks down latency under different scenarios. The
latency gap has been previously attributed to the frequent all-
to-all communication collective in MoE models. [21] While
all-to-all collectives does increase latency under multi-node
deployment, we note that this is not the only source of latency.
Fig. 4. MoE vs Dense model memory footprint comparison during inference.
The MoE models require significantly more memory usage when deployed
on GPUs. Besides the large memory consumption due to the expanded model
capacity (introduced by expert parameters), it also requires more memory for
activation. (results for batchsize=48 for MT, and batchsize=8 for LM. Note
that these are the largest batch sizes that are feasible to run under the baseline
implementation.)
Fig. 5. MoE Model latency breakdown. Besides all-to-all communication,
other components of the model, such as gating function and expert execution,
are also inefficient. Communication overhead increases significantly when
more than one node is involved.(Results for batch size=8 for LM and batch
size=48 for MT).
In Section III-B, we will discuss these extra sources of latency.
Memory. We also observe a large increase in memory
consumption for MoE models (see Figure 4). For LM, the
dense model only requires 2.2GB on each GPU whereas the
MoE model requires 18.88GB at its peak, an increase of
8.58×. For MT, the dense and MoE models use 7.02GB and
21.16GB, respectively, an increase of 3.01 ×.
We perform a detailed analysis by separating static and
dynamic memory usage. Static memory consumption refers
to memory allocated to model parameters, whereas dynamic
memory consumption refers to memory allocated on demand,
usually by network activations. Due to the fact that each GPU
accommodates more than one expert during inference, the
increase in static memory is expected. However, we observe
that the peak dynamic memory consumption also increases
significantly in both cases, which is surprising.
B. Inefficiency of Static Gating
What is behind the major overhead in latency and mem-
ory consumption? A detailed examination of the latency
breakdown sheds light on this matter. While the all-to-all
communication collective plays a significant role in multi-
4

--- PAGE 5 ---
node scenarios, other components, such as the gating function
and expert execution, also contribute significant inefficiencies.
Furthermore, a close analysis of the memory trace indicates
that memory allocation occurs during the gating and reorder-
ing phases. The source of inefficiency in these components
warrants further investigation.
The root cause of performance and resource overheads lies
in the static gating policy. Recent implementations [2], [18],
[24], [27] of MoE Transformer models usually assume the
number of tokens assigned to each expert is roughly the same
because the loss function during training accounts for load
balance. As a result, the token distribution process is simplified
to an all-to-all collective that distributes the same number of
tokens (see Figure 8(a)).
The Capacity Factor Cdefines the number of tokens pro-
cessed by each expert in one batch. If the gating function
assigns fewer tokens than an expert’s capacity, the rest of the
capacity will be filled by placeholders ( i.e., zero vectors). If the
gating function assigns more tokens than an expert’s capacity,
excess tokens are dropped by the expert and their information
will be retained only by the residual connection. Token drop
is undesirable as it harms accuracy. To avoid information loss
and accuracy fluctuations, capacity factor Cis usually set at
high values during inference. While this safeguards accuracy,
it increases latency and memory costs.
Waste Factors. For Language Model, where the number
of experts E= 512 and the Capacity Factor C= 0.05, the
number of tokens processed by an expert in a sequence Sis
ECS = 512×0.05×S= 25.6S. The amount of computation
the device must actually perform, instead, is only 2Ssince
the model implements top-2 gating and each token would
be processed by two experts. Therefore, the waste factor is
25.6S/2S= 12.8.
For Machine Translation, the analysis is similar. The number
of tokens processed by each expert is ECS = 128 ×1×S=
128S. However, the amount of computation the device must
actually perform is 2Sas well, which leads to a waste factor
of128S/2S= 64 . The huge waste factor suggests that typical
MoE models perform a large amount of excess computation
and communication as well as consume a large amount of
extra memory.
Our question is whether this over-provisioned resource
usage is avoidable. If the workload is well balanced such
that token allocations across experts are comparable, resource
waste can be reduced by simply scaling down the Capacity
Factor. On the other hand, if the expert activation is sparse,
scaling down the capacity factor is not an option because doing
so increases the chance of dropping tokens and harming model
accuracy.
IV. A NALYSIS OF EXPERT ACTIVATION PATTERNS
To understand whether such a huge waste factor is necessary
for service stability, we will study expert activation patterns
across two applications of MoE models, namely language
modeling and machine translation. Moreover, we propose
two optimizations (dynamic gating and expert caching) toreduce the waste factor and improve latency and memory
consumption.
A. Language Modeling Case
For Language Modeling, we use the PILE dataset [8] as the
input, which is the validation set used in prior work [2]. We
select three domains (Wikipedia, PubMed and Github) from
the PILE dataset to study the effect of different input data
on the expert activation patterns across time ( i.e., consecutive
batches).
We visualize the results in Figure 6. Each row represents
a batch and each column represents the load of a particular
expert. A more intense color indicates the expert receiving
a higher portion of all tokens in a batch. As shown in Fig-
ure 6(a), load distribution across experts is highly imbalanced.
There exists multiple hot experts that always get a large share
of tokens (multiple lines of intense color), and the other experts
consistently receive a small amount of tokens (lines of lighter
colors).
In the most extreme cases, Figure 7 indicates there exist
experts that never get any tokens. Due to the static gating
policy, these experts still receive and process empty token
placeholders, introducing a huge waste of computational re-
sources. As shown in Figures 6(a) and 7, the set of hot experts
and their hotness level varies across domains even though all
domains consistently exhibit a high-degree of sparse expert
activation.
B. Machine Translation Case
For Machine Translation (MT), we use the original valida-
tion dataset NLLB-200 [22]. We use English as the source
language, and select three different target languages (French,
Japanese and Asturian). Expert activation on MT for randomly
selected layers is visualized in Figure 6(b).
Machine Translation models also exhibit load imbalance and
a small fraction of experts that are more hot than others, and
the load imbalance is even more pronounced. Certain experts
on both encoder and decoder has received a large share of
all tokens that is almost half of the full batch, whereas many
experts maintain a low degree of activation.
We further inspect whether expert sparsity exists on the
encoder and the decoder of the model. Figure 7 demonstrates
the expert sparsity level on the encoder and decoder on all
three tasks. We find that the encoder activation is mostly dense,
that most of the experts are activated at all times. The decoder
activation is extremely sparse (about 75%).
We visualize the selected activation pattern of the encoder
and decoder in Figure 6(b). The activation is normalized
within a batch, and the color intensity is a measure of load
intensity, representing the percentage of tokens assigned to
each expert within a batch. The detailed activation shows
that the expert activation pattern in machine translation is
similar across different languages. The encoder architecture
captures the source language properties which is the same
across all three tasks (English). To our surprise, we found
5

--- PAGE 6 ---
Fig. 6. Visualization of the expert activation pattern on selected layer of (a) language modeling and (b) machine translation. Activation is normalized. The
expert activation pattern exhibits strong imbalance on all the tasks, and the imbalance is consistent. Specifically, on machine translation decoder the sparseness
is enormous, and the expert also demonstrates strong temporal correlation.
Fig. 7. Average number of inactive experts on Language Modeling and
Machine Translation. Most, if not all experts are activated throughout the LM
and MT encoder. However, activation on MT decoder is extremely sparse,
even if we utilize a batch size of 96 under dynamic gating policy.
that expert activation is more or less similar across different
target languages as well as decoder architectures.
A closer inspection on the expert activation on the decoder
shows that the expert sparsity has a strong temporal locality.
The intense color representing high load of expert usually
appears as lines, suggesting that an expert is active across
consecutive batches. This implies temporal locality for hot
experts. This observation is a key motivation for expert caching
discussed in Section VI.
V. D YNAMIC GATING OPTIMIZATION
The observed activation patterns demonstrate a distinct
gap between assumptions in system design and inference
performance. Naively increasing expert capacity may still not
prevent token overflow for some experts, but will create extra
redundancy and waste for other experts. While previous studies
also notice the imbalanced activation across experts [16], [24],
existing solutions retain a static gating policy, which increases
CF when severe imbalance appears [16]. Our conclusion is
that static gating increases resource waste and fixed expert
capacity is not the optimal solution for the distribution of
Fig. 8. Comparison between the static gating in [2], [21] and our implemen-
tation of dynamic gating. For simplicity, we assume E=3, S=6, C=0.5 and
top-1 gating in this example. Shapes of tensors are recorded in parentheses.
(a) Static Gating. Under a static gating policy, each expert always processes
a predefined amount of tokens, which may lead to token overflow or empty
tokens. See Section III-B for details. (b) Dynamic Gating. Under a dynamic
gating policy, each expert only processes the tokens that are assigned to it.
The token distribution mechanism is simplified with less complexity, and the
communication and computation are reduced.
tokens to experts. The constraints imposed by static capacity
should be removed and the gating function should be dynamic.
Nevertheless, changing the gating policy to allow dynamic
sizes for experts is non-trivial. Major, existing implementa-
tions [2], [16], [26] do not support dynamic. They rely on
static capacity to guarantee that message sizes of all-to-all
collectives are the same, which simplifies the communication.
6

--- PAGE 7 ---
A. Case for Dynamic Gating
Figure 8(a) visualizes the static gating policy. In this exam-
ple, we assume a sequence length (total number of tokens in a
batch of sentences) S= 6, number of experts E= 3, capacity
factor C= 0.5such that static capacity is S×C= 3. We
assume top-1 gating such that each token is assigned to only
one expert.
After the gating function generates the gating decision, the
static gating policy translates it into Edispatch masks, each of
size(S,(S×C)). The dispatch mask is generated as follows.
If token iis assigned to expert e, then the process will check
if the e-th mask still has capacity. If so, the i-th column of the
first empty row will be marked as 1, whereas the other numbers
are kept to be 0. This process gives us a sparse dispatch mask
that is a tensor of the size (E, S, (S×C)), in which at most S
entries are 1’s due to potential token dropping. Note that this
matrix is highly sparse. The input tokens will be multiplied
with the dispatch mask to reorder inputs into Esets of inputs,
each with S×Ctokens. Each set of inputs will be sent to its
assigned expert’s device.
Figure 8(b) shows how the gating function must be re-
designed when the number of tokens transferred between
devices is variable. Our implementation simplifies the distribu-
tion process. We find an array of indices that can sort the array
by performing an argsort. This set of optimal index prepares
the order of tokens for dispatch to devices. By counting the
number of occurrences of each expert, we know the exact size
of the dispatched input to each device.
Because the sizes of dispatched input are variable [13], we
adopt a two-step approach. First, we use an all-to-all collective
to inform each device the size of the incoming tokens. Second,
we use another all-to-all collective to perform the real token
transfer. The size all-to-all collective is launched as soon
as sizes are known, maximizing overlap with other kernels.
Meanwhile, inputs are reordered based on the optimal index
for each token and the reordered input will be split based on
the size of input.
Dispatch requires a sort of O(SlogS), a bin-count of
O(S), and an indexing operation of O(SD). The overall
complexity O(SD+SlogS)is much smaller than the batch
matrix multiplication of size O(S2EDC ). The additional
cost is modest and only an extra all-to-all collective whose
message size is minimal. Our dynamic gating ensures token
dropping will not happen, improving the model’s robustness.
Our dynamic gating also ensures that no empty placeholders
will be transferred between devices, removing the waste in
memory allocation of the reordered input and communication
volume.
After all tokens are processed by their assigned experts,
they are collected through another all-to-all communication
collective, sent to their original device, and restored to their
original order. This is typically implemented using batch
matrix multiplication (BMM) but, as in the first stage, BMM
can be replaced with an indexing operation that reduces
complexity.B. Reduced Latency and Memory Usage
We study the impact of the dynamic gating policy on
execution time and memory usage on the LM and MT MoE
use cases. We keep the machine configuration same as before
for a fair comparison. As the dynamic gating policy introduces
workload imbalance between different GPUs, for each case,
we study the impact of different datasets and tasks on the
performance of the model. The experiment is executed by
forwarding ten independent epochs on each subset/subtask and
recording the average throughput and memory consumption
for each batch across the experiments.
Figure 9 compares the impact of different gating policies
on throughput. Results on Expert Buffering and Load Bal-
ancing will be explained in Section VI and VII, respectively.
Our dynamic gating policy significantly increases throughput
across all batch sizes and tasks, compared against the baseline
and Tutel gating [16]. By removing the large dispatch mask,
dynamic gating also enables larger batchsizes under the same
amount of resources. This improves throughput by up to
6.21×/3.32×when compared against static/Tutel gating under
single-node LM, by up to 5.75×/5.33×under MT Encoder,
and by up to 2.58×/1.88×under MT decoder.
Since dynamic gating removes the waste factor by reducing
the volume of communication, it also reduces communication
overheads and improves throughput. Therefore, the benefit
of the dynamic gating policy widens when the model is
deployed on multiple nodes, where communication overheads
are more prominent. Dynamic gating improves throughput by
up to 11.55 ×, 10.98 ×, 5.71×in LM, MT Encoder, and MT
decoder, respectively, when compared against static gating.
Figure 10 summarizes the effect of different gating policies
on memory consumption, using single node cases as an
example. Dynamic gating enables larger batch sizes, which
runs even faster, when compared against the static gating
policy with smaller batch sizes. Dynamic gating reduces
the memory footprint by removing the dispatch mask, and
also reduces wasted memory allocations for empty paddings
and placeholders. As a result, the memory allocated for the
activation (bright colors in the figure) for LM with batch size
of 8 falls from 6.29GB to 1.28 GB, which is a 79.6% decrease.
For MT with batchsize of 8, memory allocations fall from
1.89GB to 1.05 GB, which is a 44.2% decrease. Reducing
memory consumption also allows a larger batch size to be
allocated under the same machine configuration. The dynamic
gating version permits a batch size of 64 for LM and 96 for
MT, which is 8 ×and 2×larger than batch size permitted on
the static counterparts.
VI. E XPERT BUFFERING
Although dynamic gating reduces waste in computation and
dynamic memory allocation associated with the gating func-
tion, static memory usage associated with the large number
of MoE parameters still puts a huge burden on GPU memory
at deployment. The high sparsity in expert activation pattern
prompts us to investigate whether there is a way to reduce the
memory usage by pruning out the idle experts.
7

--- PAGE 8 ---
Fig. 9. Throughput comparison of different gating policies in MoE models, including static gating (baseline), Tutel gating [16], our dynamic gating policy
(Sec. V), dynamic gating with Load Balancing (LB, Sec. VII), dynamic gating with Expert Buffering (EB, Sec. VI), and all optimizations combined. Missing
bars represent infeasible cases under the corresponding policy and batch size. Eg. Tutel cannot support beyond batch size=32 for LM-1 Node. Dynamic gating
reduces memory usage and message sizes in communication, enables larger batch sizes and substantially faster processing times than static gating. Expert
buffering trades latency for smaller memory usage while still achieving higher throughput on the MT Decoder. Load balancing further improves latency when
combined with dynamic gating and expert buffering. Note that load balancing only makes sense in the context of dynamic gating where each expert gets
different number of tokens. Load balancing particularly shines under multi-node setting or combined with expert buffering as it can improve cache miss rate
(See Fig 14).
A. Sparse Expert Activation
Our investigation in the expert activation pattern shows that
although, in every batch, there exists some experts that are
inactive, all experts have been activated a few times across
time and batches. Pruning out experts that are not frequently
active can potentially hurt model accuracy. However, we can
offload the less frequently accessed experts to CPU memory
and use the GPU memory for hot and active experts.
We propose the expert buffering mechanism to exploit
expert sparsity and reduce static memory allocation. Figure 11
illustrates the mechanism, which reduces static memory con-
sumption by offloading expert parameters to CPU memory.
Since CPU is much slower than GPU for matrix multiplication,
we only use CPU memory to hold the experts but do not
offload the computation. We use GPU memory to cache active
experts and perform computation.
B. Cache Management
During inference, under dynamic gating, once the gating
decision is made by the gating function, each GPU receives
the number of tokens assigned to its experts. If an expert
receives a positive token count, it is considered active for the
current batch. The process then checks if the active expert is
already cached in GPU memory. If not, then the process willlaunch a Memcopy to transfer the required expert parameters
into the cache. Copying expert parameters from CPU memory
to GPU DRAM will be launched in parallel with all-to-all
communication, to allow for overlap of data transfers and
latency hiding.
In cases where the cache is already full but more experts
are needed, eviction will be triggered to make space for the
new experts. The eviction policy is designed as follows. First,
we will first evict experts that are not active in this batch since
they are also less likely to be used in the future due to temporal
locality. Next, we will evict expert parameters under a Last In,
First Out (LIFO) policy.
The reason for adopting a LIFO policy is rooted in the
implementation of recent MoE Transformers. If multiple ex-
perts are allocated to a single GPU, MoE Transformer will
execute the experts serially in the increasing order of their ids.
Consider a small example of E= 4 experts and cache size
of2experts, and assume expert (1, 2, 3) are needed. After
stage 1, expert 1 and 2 will be pushed into the cache, and we
need to evict one of them to load expert 3. By evicting expert
2 instead of 1, we ensure the expert with the shortest reuse
distance is kept in the cache.
8

--- PAGE 9 ---
Fig. 10. Comparison of memory consumption between MoE models under
static and dynamic gating policy. Light shade represents dynamic memory al-
location (activation memory). Dark shade represents static memory allocation
(model parameters). Missing bars in each plot capture the infeasible cases
under the corresponding policy. Compared to Static and Tutel [16], Dynamic
Gating reduces the memory usage, thus enabling larger batch sizes. Expert
Buffering further reduces the memory consumption of model parameters.
Fig. 11. Illustration of the Expert Buffering mechanism. We move the expert
parameters to CPU memory to reduce burden on GPU memory. On GPU
memory, we allocate space only for a few expert entries to buffer active or
hot experts. (1) During inference, the all-to-all size message sent in stage 1
as shown in Figure 8(b) signals which experts located in the current device
are active. (2) Then the expert cache will check whether the active experts
currently reside in the buffer. (3a) If found (cache hit), parameters in the
expert buffer will be used to process the tokens. (3b) If not found (cache
miss), then the expert parameters will be requested from the CPU memory.
The number of cache entries on GPU memory is a tunable parameter to adjust
for desirable GPU memory usage and latency (See Section VI).
C. Cache Miss Rates
To estimate the technical feasibility of Expert Buffering, we
calculate cache miss rate for machine translation use case on
layers that exhibit enormous expert sparsity. We note that the
cache is deployed per device, and each device caches experts
that have been assigned to it. As a result, we may vary cache
Fig. 12. Worst-Case Cache Miss Rate obtained from traces of expert
activations from MT decoders. We tune the cache size per GPU from 1 to
16, and examine the impact of cache size on the worst case Cache Miss
Rate on different layers of MT decoders over different tasks. (a)caching
performance with/without any reassignment. (b)caching performance against
the theoretical optimal Belady’s MIN. The miss rate is further reduced by
load balancing, and is very close to Belady’s MIN (See Sec. VII).
size from 1 to 16 experts, generating a saving of 0-32.2% on
total static memory allocation. We calculate the global cache
miss rate under each circumstance. The worst-case cache miss
rates are shown in Figure 12. We notice the Cache Miss Rate
starts to decrease faster when the cache size is larger than 5
per GPU, which is a cache size of 40 in total. This result
is consistent with our previous observation that there will be
more than 90 experts being empty inside the decoder. We
further compare our caching policy with Belady’s MIN, the
theoretical optimal policy that requires information from the
future. Fig. 12(b) shows that the LIFO policy is better than
FIFO, and with the load balancing introduced in Sec. VII, our
policy can obtain a cache miss rate very close to Belady’s
MIN.
To evaluate the impact of the proposed mechanism, we
perform experiments on Expert Buffering on the MT Decoder.
The cache size is selected to be around 80 experts in total,
which fits 10 experts per GPU under single node case. This is
the point where the cache miss rate starts showing saturation
behavior in Figure 12(a).
Figure 9 and Figure 10 show the impact of expert buffering
on throughput and the static memory allocations for MT. Ex-
pert buffering has successfully reduced the static memory con-
sumption by 2.25GB. This memory reduction is particularly
useful for users with limited number of GPUs. Although the
throughput becomes smaller compared to memory-intensive
dynamic gating, the throughput obtained by expert caching is
comparable to baselines under single-node, and 2.21 ×/4.30×
under 2/4 nodes.
Furthermore, we study the latency-memory tradeoff incurred
by the expert caching mechanism, and estimate the pareto
frontier. Figure 13 shows the latency and memory consumption
under a series of cache configurations. We vary the cache per
GPU and measure the decoder latency and peak memory con-
sumption. The result shows that the pareto frontier is similar
9

--- PAGE 10 ---
Fig. 13. Tradeoff between memory and latency under different cache
configuration on MT Decoder. Corresponding cache size per GPU is marked
on the plot.
to the outliers on the cache miss rate plot in Figure 12(a).
Our findings also indicate that the primary contributor to
increased latency is the constrained CPU-GPU bandwidth,
which we observed to saturate at 12GB/s during our exper-
iments. The findings indicate that layers with a high cache
miss rate may impede performance. Adopting technologies
that enhance CPU-GPU bandwidth, such as the NVIDIA Grace
Hopper superchip and PCIe v5 can mitigate the latency issues
in situations where memory is constrained.
We note that no prior work exploits the unique character-
istics of MoE Transformers to optimize memory usage. As a
caching strategy that is specifically tailored for MoE models,
expert buffering is orthogonal to prior memory saving mech-
anisms such as offloading [25], [30] and can be seamlessly
integrated for greater memory savings.
VII. L OAD BALANCING
As we saw in Section IV, token assignments to experts are
highly imbalanced, hence the load assigned to each device
is also highly imbalanced. Those devices hosting hot experts
can become bottlenecks and become more vulnerable to out
of memory error. Moreover, devices hosting cold experts may
sit idle while waiting for devices that are hosting hot experts
to finish their load. As a result, load balancing is critical for
having a robust and stable model.
We propose a simple load balancing scheme during the
model deployment. We optimize the allocation of experts
by leveraging historical load data. Specifically, we encode
historical expert allocation into a matrix, and balance the load
on each device accordingly. We combine higher-loaded experts
with lower-loaded experts, so that the load can be distributed to
different devices. We denote the expert placement with Pmn,
where m is the expert id ( m= 1. . . E ), n is the device id
(n= 1. . . D ) and Pmn= 1 indicates that the m-th expert
is allocated on the n-th device. We also denote the expert
activation with Amb, where m is the expert id and bis the
batch id ( b= 1. . . B ), and Ambrepresents the fraction of
tokens assigned to expert eat batch id b. The problem can be
thus formalized as follows:
min max
m,b|X
nPmnAmb−1
D|subject toX
mPmn=E
D∀n
This problem can be reduced to the multi-way number par-
titioning problem [10], which is NP-hard. To balance the
Fig. 14. The effect of our proposed load balancing mechanism: Based on
historical activation data, our algorithm is able to significantly reduce the
load imbalance problem on LM tasks and improve the robustness by limiting
the maximum workload on a single device.
memory usage and simplify the communication process, each
GPU should be assigned the same number of experts.
A. Greedy Balancing for Independent Activation
We utilize a greedy algorithm to generate approximations to
the optimal assignment. We sort the experts by their average
work load in historical data ˜Am, and assign the experts to
GPUs on a descending order. At each step, an expert is
assigned to the GPU with the smallest load, calculated byP
mPmn˜Am. Once a GPU reaches the designated capacity, it
will be removed from the list of candidates.
Figure 14 summarizes the balance of the load under the orig-
inal order and the new order. The balance of load is estimated
using existing activation data introduced in Section III-B. To
perform the experiment, we separate the data into two halves.
We use the first half of the activation data to generate a device
assignment for each expert, then estimate the work load under
generated assignment using the second half of the activation
data. Results are normalized by the total batch size, which
means the numbers represent the share of the total number of
tokens each device will handle in a certain batch.
We record the Max Load, which is the maximum share of
the load that has ever appeared in all batches on the test set,
and the Avg Max Load, which is the maximum share of the
load averaged over all batches. The Max Load estimates the
worst case scenario that relates to out-of-memory error, and the
Avg Max Load estimates the average case where imbalance on
work load can lead to bottlenecks and harm inference speed.
Results show that Greedy is able to balance the load for LM
use case by improving both metrics (Max Load and Avg Max
load per device) significantly, reducing from more than 0.6
to less than 0.4. Similarly for Machine Translation use case,
Greedy can balance the expert load assignment for the MT
encoder. Figure 9 shows the benefit of the Greedy Rebalancing
on the LM and MT Encoder: Rebalancing increases the
throughput by a maximum of 10.1% and 19.5% compared
against pure dynamic gating. Furthermore, rebalancing allows
the LM to achieve a batchsize of 64 and 128 when it is
deployed on four nodes, making the model more robust.
10

--- PAGE 11 ---
B. Anti-correlation Balancing for Correlated Activation
Greedy is less effective for the Machine Translation De-
coder. We found that expert activation level becomes a less
effective indicator in this case due to correlation between
experts. To handle this problem, we propose Anti-correlation
Balancing, which takes correlation into consideration. Denot-
ing the Pearson correlation between the current expert ato
expert bin the historical data as Sab, the current work load can
be modified fromP
mPmn˜AmtoP
mPmn(˜Am+0.5∗Sam).
This algorithm successfully reduces the Avg Max Load and
the Max Load on most cases. We notice that a more balanced
work load also has a positive impact on the cache miss rate.
As shown in Figure 12 and 9, the worst-case cache miss rate
decreases for all cache sizes over MT Decoders, which leads
to a maximum increase of 1.9% on their throughputs.
VIII. R ELATED WORK
While the MoE Transformer substantially reduces the train-
ing cost and FLOPS for large models, the outrageous size
of MoE Transformers and the complex expert parallelism
[21] poses obstacles for its deployment, including the high
GPU memory requirement and the excessive communication
overhead of expert assignment. Various approaches have been
invented to relieve these obstacles. Switch Transformer [7] and
ELSLM [2] use knowledge distillation to distill a large MoE
Transformer into a dense model. While distillation reduces the
number of parameters, only a small portion (about 30%) of the
accuracy gain can be retained. The MoS strategy proposed
in DeepSpeed-MoE [24] distills the knowledge to a smaller
MoE Transformer with less layers and shared experts. SE-
MoE [30] uses pruning to reduce the number of experts in
the model. WideNet [ ?] and MPoE [9] reduce the number of
parameters by enforcing parameter sharing. Beyond reducing
the parameters, other methods directly reduce computation and
communication. The BASE Layer and Switch Transformer
also reduce the number of experts each token is assigned to
reduce the communication volume and computation. V-MoE
[26] further reduces the number by dropping out a large por-
tion of tokens. Hash Layer [27] replaces the gating layer with
a precomputed hash function, which reduces the computation
cost, but doesn’t alleviate the communication overhead. As the
MoE Transformer is a type of Transformer, techniques and
optimized architectures that enhance Transformer inference
speed may apply. Relevant examples include Reformer [19],
Longformer [3], and Terraformer [17]. However, there is scant
discussion of their application to MoE Transformers.
Offloading and swapping strategies such as [15] swaps
unused tensors form the GPU memory to the main memory to
reduce the resource requirement. However, existing strategies
can only be applied on dense models. Applying these strategies
efficiently on conditional neural networks such as MoE is
non-trivial, since the data flow graph cannot be constructed
in advance due to the conditional computation. FastMoE
[13], [14] designed customized communication primitives and
gating kernels for token assignment to reduce the communi-
cation overhead, but it has not been tested on outrageouslylarge neural networks. Tutel [16] and DeepSpeed-MoE [24]
improve MoE model performance on datacenter-scale sys-
tems by combining system and architecture methods with
tailored kernels for both Transformer and MoE layers, and
specialized communication primitives. The approach combines
expert parallelism, model parallelism, and tensor parallelism
to significantly boost throughput and reduce latency. However,
DeepSpeed-MoE is not designed to conserve GPU resources
and therefore may be impractical for many academic users.
SE-MoE [30] utilizes Ring Memory offloading to reduce
GPU usage, achieving better throughput than DeepSpeed-MoE
in low-resource scenarios. However, this approach does not
leverage expert parallelism from MoE Transformers.
IX. C ONCLUSION
While at training time, mixtures of expert (MoE) models
show superior performance to their flop-equivalent dense coun-
terpart models, they are notoriously large, need a large number
of GPUs to deploy and hard to democratize. Researchers
outside large industry labs do not have access to hundreds
or thousands of GPUs to afford exploring such large models.
Moreover, they are much slower than their dense counterparts
at inference. To overcome these challenges, we propose three
optimization techniques (Dynamic Gating, Expert Buffering,
and Expert Load Balancing) to improve memory and latency
profile of such models for deployment.
REFERENCES
[1] “FasterTransformer,” https://github.com/NVIDIA/FasterTransformer, ac-
cessed: 2023-03.
[2] M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V .
Lin, J. Du, S. Iyer, R. Pasunuru et al. , “Efficient large scale language
modeling with mixtures of experts,” arXiv preprint arXiv:2112.10684 ,
2021.
[3] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-
document transformer,” arXiv preprint arXiv:2004.05150 , 2020.
[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,
“An image is worth 16x16 words: Transformers for image recognition
at scale,” arXiv preprint arXiv:2010.11929 , 2020.
[5] N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun,
Y . Zhou, A. W. Yu, O. Firat et al. , “Glam: Efficient scaling of language
models with mixture-of-experts,” arXiv preprint arXiv:2112.06905 ,
2021.
[6] W. Fedus, J. Dean, and B. Zoph, “A review of sparse expert models in
deep learning,” arXiv preprint arXiv:2209.01667 , 2022.
[7] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to
trillion parameter models with simple and efficient sparsity,” Journal
of Machine Learning Research , vol. 23, no. 120, pp. 1–39, 2022.
[Online]. Available: http://jmlr.org/papers/v23/21-0998.html
[8] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster,
J. Phang, H. He, A. Thite, N. Nabeshima et al. , “The pile: An
800gb dataset of diverse text for language modeling,” arXiv preprint
arXiv:2101.00027 , 2020.
[9] Z.-F. Gao, P. Liu, W. X. Zhao, Z.-Y . Lu, and J.-R. Wen, “Parameter-
efficient mixture-of-experts architecture for pre-trained language mod-
els,” arXiv preprint arXiv:2203.01104 , 2022.
[10] R. L. Graham, “Bounds on multiprocessing timing anomalies,” SIAM
journal on Applied Mathematics , vol. 17, no. 2, pp. 416–429, 1969.
[11] S. Gupta, S. Mukherjee, K. Subudhi, E. Gonzalez, D. Jose, A. H.
Awadallah, and J. Gao, “Sparsely activated mixture-of-experts are robust
multi-task learners,” arXiv preprint arXiv:2204.07689 , 2022.
11

--- PAGE 12 ---
[12] H. Hazimeh, Z. Zhao, A. Chowdhery, M. Sathiamoorthy, Y . Chen,
R. Mazumder, L. Hong, and E. Chi, “Dselect-k: Differentiable selection
in the mixture of experts with applications to multi-task learning,” Ad-
vances in Neural Information Processing Systems , vol. 34, pp. 29 335–
29 347, 2021.
[13] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang, “Fastmoe: A fast
mixture-of-expert training system,” arXiv preprint arXiv:2103.13262 ,
2021.
[14] J. He, J. Zhai, T. Antunes, H. Wang, F. Luo, S. Shi, and
Q. Li, “Fastermoe: Modeling and optimizing training of large-
scale dynamic pre-trained models,” in Proceedings of the 27th
ACM SIGPLAN Symposium on Principles and Practice of Parallel
Programming , ser. PPoPP ’22. New York, NY , USA: Association
for Computing Machinery, 2022, p. 120–134. [Online]. Available:
https://doi.org/10.1145/3503221.3508418
[15] C.-C. Huang, G. Jin, and J. Li, “Swapadvisor: Pushing deep learning
beyond the gpu memory limit via smart swapping,” in Proceedings of
the Twenty-Fifth International Conference on Architectural Support for
Programming Languages and Operating Systems , 2020, pp. 1341–1355.
[16] C. Hwang, W. Cui, Y . Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang,
R. Salas, J. Jose, P. Ram, J. Chau, P. Cheng, F. Yang, M. Yang, and
Y . Xiong, “Tutel: Adaptive mixture-of-experts at scale,” CoRR , vol.
abs/2206.03382, Jun. 2022. [Online]. Available: https://arxiv.org/pdf/
2206.03382.pdf
[17] S. Jaszczur, A. Chowdhery, A. Mohiuddin, L. Kaiser, W. Gajewski,
H. Michalewski, and J. Kanerva, “Sparse is enough in scaling trans-
formers,” Advances in Neural Information Processing Systems , vol. 34,
pp. 9895–9907, 2021.
[18] Y . J. Kim, A. A. Awan, A. Muzio, A. F. C. Salinas, L. Lu, A. Hendy,
S. Rajbhandari, Y . He, and H. H. Awadalla, “Scalable and effi-
cient moe training for multitask multilingual models,” arXiv preprint
arXiv:2109.10465 , 2021.
[19] N. Kitaev, Ł. Kaiser, and A. Levskaya, “Reformer: The efficient trans-
former,” arXiv preprint arXiv:2001.04451 , 2020.
[20] S. Kudugunta, Y . Huang, A. Bapna, M. Krikun, D. Lepikhin, M.-T.
Luong, and O. Firat, “Beyond distillation: Task-level mixture-of-experts
for efficient inference,” arXiv preprint arXiv:2110.03742 , 2021.
[21] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun,
N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional
computation and automatic sharding,” arXiv preprint arXiv:2006.16668 ,
2020.
[22] NLLB Team, M. R. Costa-juss `a, J. Cross, O. C ¸ elebi, M. Elbayad,
K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard,
A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault,
G. Mejia-Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan,
D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale,
S. Edunov, A. Fan, C. Gao, V . Goswami, F. Guzm ´an, P. Koehn,
A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and J. Wang,
“No language left behind: Scaling human-centered machine translation,”
2022.
[23] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier,
and M. Auli, “fairseq: A fast, extensible toolkit for sequence modeling,”
inProceedings of NAACL-HLT 2019: Demonstrations , 2019.
[24] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y . Aminabadi, A. A. Awan,
J. Rasley, and Y . He, “Deepspeed-moe: Advancing mixture-of-experts
inference and training to power next-generation ai scale,” arXiv preprint
arXiv:2201.05596 , 2022.
[25] J. Ren, S. Rajbhandari, R. Y . Aminabadi, O. Ruwase, S. Yang,
M. Zhang, D. Li, and Y . He, “ {ZeRO-Offload }: Democratizing {Billion-
Scale}model training,” in 2021 USENIX Annual Technical Conference
(USENIX ATC 21) , 2021, pp. 551–564.
[26] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton,
A. Susano Pinto, D. Keysers, and N. Houlsby, “Scaling vision with
sparse mixture of experts,” Advances in Neural Information Processing
Systems , vol. 34, 2021.
[27] S. Roller, S. Sukhbaatar, J. Weston et al. , “Hash layers for large sparse
models,” Advances in Neural Information Processing Systems , vol. 34,
2021.
[28] J. Sevilla, L. Heim, A. Ho, T. Besiroglu, M. Hobbhahn, and P. Villalobos,
“Compute trends across three eras of machine learning,” arXiv preprint
arXiv:2202.05924 , 2022.
[29] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,
and J. Dean, “Outrageously large neural networks: The sparsely-gated
mixture-of-experts layer,” arXiv preprint arXiv:1701.06538 , 2017.[30] L. Shen, Z. Wu, W. Gong, H. Hao, Y . Bai, H. Wu, X. Wu, H. Xiong,
D. Yu, and Y . Ma, “Se-moe: A scalable and efficient mixture-of-
experts distributed training and inference system,” arXiv preprint
arXiv:2205.10034 , 2022.
[31] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient transformers:
A survey,” arXiv preprint arXiv:2009.06732 , 2020.
[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in
neural information processing systems , vol. 30, 2017.
[33] F. Xue, Z. Shi, F. Wei, Y . Lou, Y . Liu, and Y . You, “Go wider
instead of deeper,” in Proceedings of the AAAI Conference on Artificial
Intelligence , vol. 36, no. 8, 2022, pp. 8779–8787.
[34] A. Yang, J. Lin, R. Men, C. Zhou, L. Jiang, X. Jia, A. Wang, J. Zhang,
J. Wang, Y . Li et al. , “Exploring sparse expert models and beyond,”
arXiv preprint arXiv:2105.15082 , 2021.
[35] Z. You, S. Feng, D. Su, and D. Yu, “Speechmoe: Scaling to large acous-
tic models with dynamic routing mixture of experts,” arXiv preprint
arXiv:2105.03036 , 2021.
12
