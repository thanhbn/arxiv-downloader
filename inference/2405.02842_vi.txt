# 2405.02842.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/inference/2405.02842.pdf
# Kích thước tệp: 2408448 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
ICEFORMER : TĂNG TỐC SUY LUẬN VỚI CÁC
TRANSFORMER CHUỖI DÀI TRÊN CPU
Yuzhen Mao, Martin Ester, Ke Li
School of Computing Science, Simon Fraser University
Burnaby, BC V5A 1S6, Canada
{yuzhenm,ester,keli }@sfu.ca
TÓM TẮT
Một hạn chế của các mô hình dựa trên Transformer hiện có là chúng không thể xử lý các chuỗi rất dài làm đầu vào vì các phép toán self-attention của chúng thể hiện độ phức tạp thời gian và không gian bậc hai. Vấn đề này trở nên đặc biệt nghiêm trọng khi các Transformer được triển khai trên các nền tảng phần cứng chỉ được trang bị CPU. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp mới để tăng tốc self-attention tại thời điểm suy luận hoạt động với các mô hình Transformer đã được huấn luyện trước ngay lập tức mà không cần huấn luyện lại. Chúng tôi thực nghiệm sử dụng phương pháp của mình để tăng tốc các Transformer chuỗi dài khác nhau, bao gồm một LLM hàng đầu dựa trên LLaMA 2, trên nhiều benchmark khác nhau và chứng minh tốc độ tăng 2.73× −7.63× trong khi vẫn duy trì 98.6%−99.6% độ chính xác của các mô hình đã được huấn luyện trước gốc. Mã nguồn có sẵn trên trang web dự án của chúng tôi tại https://yuzhenmao.github.io/IceFormer/ .

1 GIỚI THIỆU
Các Transformer (Vaswani et al., 2017) đã thúc đẩy những tiến bộ đáng kinh ngạc trong NLP, như được minh họa bởi các mô hình ngôn ngữ lớn (LLM) như GPT-4 và LLaMA 2. Ngày càng nhiều LLM được áp dụng cho các chuỗi đầu vào cực dài, điều này cho phép nhiều ứng dụng thú vị như tạo nội dung dài, cuộc trò chuyện mở rộng, và tìm kiếm và phân tích tài liệu lớn (OpenAI, 2023; Anthropic, 2023). Trong khi LLM có thể được huấn luyện khả thi với các bộ tăng tốc phần cứng đắt tiền (ví dụ GPU), chúng cần được triển khai trên các thiết bị thông thường, có thể chỉ được trang bị CPU.

Tuy nhiên, hiện tại việc triển khai LLM trên CPU là thách thức do chi phí tính toán cao (Dice & Kogan, 2021). Một nút thắt tính toán đáng kể phát sinh từ cơ chế self-attention là phần không thể thiếu của Transformer - cả độ phức tạp thời gian và không gian đều bậc hai theo độ dài chuỗi. Vấn đề này được làm trầm trọng hóa trong bối cảnh của LLM, thường được sử dụng trên các chuỗi rất dài.

Để xử lý các chuỗi đầu vào dài, đã có nghiên cứu đáng kể về việc giảm độ phức tạp thời gian bậc hai của self-attention - các phương pháp này được gọi chung là Transformer hiệu quả. Tuy nhiên, nhiều phương pháp không đáp ứng nhu cầu của LLM và do đó khó áp dụng cho LLM.

Một phương pháp tăng tốc lý tưởng cho LLM nên thỏa mãn bốn tiêu chí: (1) Không huấn luyện lại - phương pháp không nên yêu cầu mô hình được huấn luyện lại, do chi phí tính toán khổng lồ của việc huấn luyện LLM; (2) Tính tổng quát - phương pháp nên có thể áp dụng cho nhiều LLM khác nhau, thay vì chỉ những mô hình được huấn luyện với các ràng buộc cụ thể được tích hợp sẵn; (3) Độ chính xác cao - phương pháp không nên gây ra lỗi xấp xỉ lớn, vì LLM có nhiều lớp attention và do đó lỗi từ các lớp trước có thể tích lũy; (4) Suy luận nhanh - phương pháp nên đạt được hiệu suất thời gian kiểm tra nhanh.

Việc thỏa mãn đồng thời tất cả các tiêu chí này là khó khăn, và theo hiểu biết của chúng tôi, không có phương pháp hiện tại nào có thể làm được điều đó. Ví dụ, các Transformer với các mẫu attention cố định, ví dụ Longformer (Beltagy et al., 2020), yêu cầu huấn luyện lại mô hình trước khi có thể sử dụng. Reformer (Nikita et al., 2020) yêu cầu các key được chuẩn hóa - yêu cầu này không được đáp ứng trong hầu hết các mô hình đã được huấn luyện trước. Nyströmformer (Xiong et al., 2021) và LARA (Zheng et al., 2022) không hỗ trợ mặt nạ nhân quả, thường được tìm thấy trong LLM. Các phương pháp hạng thấp như Performer (Choromanski et al., 2020) gây ra lỗi xấp xỉ đáng kể, đặc biệt khi chúng không được huấn luyện lại/tinh chỉnh.

1arXiv:2405.02842v1  [cs.LG]  5 May 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 1: So sánh giữa Transformer (Vaswani et al., 2017) (hàng trên) và phương pháp được đề xuất, IceFormer (hàng dưới). Chúng tôi minh họa với một truy vấn và k= 2 trong k-NNS. Trong hai ma trận attention được trình bày, top-2 trọng số attention lớn nhất trong mỗi hàng được biểu diễn bằng màu tối. Các trọng số attention còn lại được hiển thị bằng màu nhạt trong ma trận attention vanilla, và được đặt thành 0 (được mô tả bằng màu trắng) trong ma trận attention thưa.

Trong bài báo này, chúng tôi đề xuất một phương pháp tăng tốc, mà chúng tôi gọi là IceFormer do khả năng được áp dụng trực tiếp trong các mô hình đông lạnh mà không cần huấn luyện lại, thỏa mãn đồng thời bốn tiêu chí trên. Cụ thể, IceFormer (1) không yêu cầu huấn luyện lại, (2) có thể được áp dụng cho hầu hết LLM, (3) có thể xấp xỉ attention vanilla một cách chính xác, và (4) đạt được tốc độ suy luận nhanh hơn đáng kể so với các phương pháp hiện có. Chúng tôi minh họa phương pháp của mình so với Transformer trong Hình 1. Như được hiển thị, Transformer tính toán các trọng số attention aij cho mọi kết hợp có thể của truy vấn qi và key kj (Giai đoạn 1) và liệt kê đầy đủ tất cả các vector giá trị vj cho mỗi truy vấn (Giai đoạn 2). Ngược lại, phương pháp của chúng tôi tận dụng tính thưa của ma trận attention và chỉ tính toán các trọng số attention cao nhất và chỉ liệt kê các vector giá trị liên quan đến chúng.

Chúng tôi tiến hành thực nghiệm trên CPU trên các benchmark LRA (Tay et al., 2020), ZeroSCROLLS (Shaham et al., 2023), và LongEval (Li et al., 2023). Trên cả ba benchmark, IceFormer thể hiện tốc độ suy luận nhanh hơn đáng kể so với các phương pháp hiện có trong khi đạt được hầu như không mất độ chính xác so với Transformer. Trên benchmark LRA, trung bình IceFormer đạt được tăng tốc 7.63× so với Transformer trong khi duy trì 98.6% độ chính xác của nó. So với Transformer hiệu quả tốt nhất với độ chính xác tương đương cho mỗi nhiệm vụ, IceFormer nhanh hơn trung bình 3.04×. Trên benchmark ZeroSCROLLS, IceFormer đạt được tăng tốc 2.73× trung bình so với một LLM hàng đầu dựa trên LLaMA 2 trong khi duy trì 99.6% độ chính xác của nó.

2 CÔNG TRÌNH LIÊN QUAN

Các Transformer hiệu quả có thể được phân loại theo hai trục: loại phương pháp và yêu cầu huấn luyện lại. Theo trục đầu tiên là các phương pháp dựa trên tính thưa và các phương pháp hạng thấp. Theo trục thứ hai là các phương pháp có thể và không thể được áp dụng cho các Transformer đã được huấn luyện trước thông thường mà không cần huấn luyện lại.

Các phương pháp dựa trên tính thưa sử dụng cơ chế attention thưa để nắm bắt thông tin toàn cục và tích hợp nó với kết quả attention cục bộ. Một số cách tiếp cận nhằm cải thiện độ phức tạp không gian so với cơ chế attention vanilla mà không cải thiện độ phức tạp thời gian, ví dụ top-k Attention (Gupta et al., 2021). Các cách tiếp cận khác nhằm cải thiện cả hai, ví dụ Sparse Transformer (Child et al., 2019), Longformer (Beltagy et al., 2020), và ETC (Ainslie et al., 2020). Một hạn chế đáng kể của các mô hình này là các token được chú ý đến được định nghĩa trước và vẫn tĩnh, không thích ứng với các chuỗi đầu vào khác nhau. Bởi vì phép toán attention gốc được phép chú ý đến bất kỳ token nào, các mô hình này phải được huấn luyện với các ràng buộc định nghĩa trước tương ứng trên các token được chú ý đến.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Reformer (Nikita et al., 2020) có thể chú ý đến các tập token khác nhau cho các chuỗi đầu vào khác nhau bằng cách sử dụng Locality Sensitive Hashing (LSH) (Andoni et al., 2015) để nhóm các token thành các chunk và sau đó chỉ chú ý đến các token trong cùng chunk với mỗi truy vấn và các chunk liền kề. Tuy nhiên, Reformer áp đặt hai ràng buộc không có trong phép toán attention gốc: các key phải được chuẩn hóa và các query và key phải giống nhau. Do đó, Reformer phải được huấn luyện với các ràng buộc này được tích hợp sẵn. Kết quả là, các phương pháp này không thể được áp dụng trực tiếp cho các mô hình đã được huấn luyện trước, không được sửa đổi; thay vào đó, các mô hình phải được huấn luyện lại với các ràng buộc cần thiết trước khi các phương pháp này có thể được sử dụng.

Các phương pháp hạng thấp xấp xỉ ma trận trọng số attention với một ma trận hạng thấp để giảm độ phức tạp thời gian và không gian bậc hai. Các ví dụ bao gồm Linformer (Wang et al., 2020) và Performer (Choromanski et al., 2020), phân tách ma trận trọng số attention thành tích của các ma trận cao và rộng bao gồm các đặc trưng tuyến tính đã học hoặc các đặc trưng ngẫu nhiên của các key và query, tương ứng. Tuy nhiên, các Transformer này thường gây ra lỗi xấp xỉ đáng kể bởi vì các ma trận trọng số attention được tạo ra bởi phép toán attention gốc, đặc biệt trong trường hợp các chuỗi đầu vào dài, thường có hạng cao. Do đó, các mô hình sử dụng các cách tiếp cận này phải được huấn luyện với các xấp xỉ hạng thấp được tích hợp sẵn, để học cách mạnh mẽ với các lỗi xấp xỉ liên quan. Kết quả là, các cách tiếp cận này không thể được áp dụng trực tiếp cho các mô hình đã được huấn luyện trước, không được sửa đổi; thay vào đó, các mô hình phải được huấn luyện lại với các xấp xỉ cần thiết trước khi các phương pháp này có thể được sử dụng. Các cách tiếp cận khác cung cấp các phương pháp tổng quát hơn có thể tận dụng các trọng số được huấn luyện trước với Transformer tiêu chuẩn mà không cần huấn luyện lại. Các Transformer này tăng tốc việc thực thi phép toán attention tiêu chuẩn mà không thay đổi kiến trúc cơ bản. Hai ví dụ là Nyströmformer (Xiong et al., 2021) và LARA (Zheng et al., 2022), thay thế cấu trúc softmax trong cơ chế self-attention bằng tích của các ma trận query và key được kích hoạt riêng biệt. Nyströmformer sử dụng phương pháp Nyström, trong khi LARA kết hợp randomized attention (RA) và random feature attentions (RFA) (Peng et al., 2021) để tái tạo ma trận trọng số attention. Trong một ví dụ khác, H-Transformer-1D (Zhu & Soricut, 2021) chia đệ quy ma trận trọng số attention thành các khối và cắt ngắn các giá trị số ít nhỏ của mỗi khối ngoài đường chéo. Tất cả các cách tiếp cận này tận dụng xấp xỉ hạng thấp, trái ngược với tính thưa.

Các công trình khác đề xuất tối ưu hóa cụ thể cho phần cứng mà không nhằm cải thiện độ phức tạp tính toán. Các ví dụ bao gồm FlashAttention (Dao et al., 2022), tối ưu hóa đọc và ghi giữa các cấp độ bộ nhớ GPU, và H2O (Zhang et al., 2023), giữ lại một cách động sự cân bằng của các token gần đây và quan trọng bằng chính sách loại bỏ KV cache. Các chiến lược này phụ thuộc vào việc triển khai và cụ thể cho các nền tảng phần cứng cụ thể (ví dụ GPU).

3 KÝ HIỆU VÀ KIẾN THỨC CHUẨN BỊ

Về mặt toán học, phép toán attention nhận ba ma trận làm đầu vào, K∈Rm×d,Q∈Rn×d,V∈Rm×d′, biểu thị các key, query và value tương ứng, và xuất ra một ma trận O∈Rn×d′. Tùy chọn, nó cũng có thể nhận vào một mặt nạ làm đầu vào, S∈Rn×m, có các phần tử là 0 hoặc 1. Các hàng thứ i của K,Q,V và O, ký hiệu là ki,qi,vi và oi, biểu thị key, query, value và output thứ i tương ứng. Phần tử của S trong hàng thứ i và cột thứ j, ký hiệu là si,j, biểu thị liệu query thứ i có được phép chú ý đến key thứ j hay không - nếu là 1, nó sẽ được phép; nếu là 0, nó sẽ không được. Một sơ đồ che mặt phổ biến là mặt nạ nhân quả, trong đó si,j là 1 nếu i≥j và 0 ngược lại. Các key và query có cùng chiều d, và mỗi key được liên kết với một value, và do đó số lượng key và value là như nhau và được ký hiệu là m.

Đầu tiên phép toán attention tính toán ma trận trọng số attention A∈Rn×m. Phần tử của nó trong hàng thứ i và cột thứ j, ký hiệu là ai,j, được tính toán với công thức sau:

ai,j=si,jexp(q⊤ikj/√d) / Σm j′=1si,j′exp(q⊤ikj′/√d) (1)

Sau đó phép toán attention kết hợp các value với các trọng số attention theo cách sau:

oi=Σm j=1ai,jvj (2)

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Ma trận attention A thường thưa (Nikita et al., 2020; Gupta et al., 2021), tức là trong mỗi hàng của A, chỉ một vài trọng số attention có giá trị đáng kể (lớn), trong khi phần lớn các giá trị còn lại gần bằng 0. Giả sử chúng ta có thể bằng cách nào đó xác định k key chưa được che mặt nhận trọng số attention cao nhất cho mỗi query qi mà không tính toán trọng số attention cho tất cả các key. Khi đó, ma trận attention gốc A có thể được xấp xỉ bằng cách chỉ tính toán tích vô hướng cho các key đã xác định, điều này có thể tiết kiệm đáng kể thời gian và tài nguyên tính toán.

4 ICEFORMER : SELF-ATTENTION TĂNG TỐC CHO CÁC KEY TỔNG QUÁT MÀ KHÔNG CẦN HUẤN LUYỆN LẠI

Để xây dựng một phương pháp tăng tốc đa năng không cần huấn luyện lại, cách tiếp cận của chúng tôi không được yêu cầu sửa đổi cơ chế attention để thay đổi các mẫu attention hoặc việc giới thiệu các tham số mô hình mới để nắm bắt các quy luật trong các mẫu attention. Điều này loại trừ các chiến lược phổ biến như cơ chế attention với các mẫu attention thưa định nghĩa trước, ví dụ (Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020), và giảm chiều đã học của key và query, ví dụ (Wang et al., 2020; Choromanski et al., 2020).

Do đó, khó để thiết kế một phương pháp tăng tốc khai thác các quy luật đã biết trong các mẫu attention mà không áp đặt yêu cầu huấn luyện lại. Do đó chúng tôi nhằm thiết kế một phương pháp tăng tốc không đưa ra giả định về sự tồn tại của quy luật trong các mẫu attention. Để cải thiện độ phức tạp O(mn) của attention vanilla, chúng ta cần xác định một cách thích ứng các key quan trọng nhất (tức là những key nhận trọng số attention cao nhất) mà không tính toán tất cả trọng số attention. Điều này có vẻ như một vấn đề con gà và quả trứng: làm thế nào chúng ta có thể biết trọng số attention nào cao nhất mà không so sánh chúng với tất cả các trọng số attention khác?

Đáng chú ý, trong trường hợp đặc biệt của các key được chuẩn hóa, như được đề xuất trong Nikita et al. (2020), điều này có thể được thực hiện bằng cách tận dụng tìm kiếm k-láng giềng gần nhất (k-NNS) để xác định k key quan trọng nhất cho mỗi query. Điều này dựa vào sự thật toán học sau, mà việc suy dẫn được bao gồm trong Phần B.1 của phụ lục: nếu ∥kj∥2= 1 cho tất cả j, arg max j ai,j= arg min j∥qi−kj∥2 2.

Tuy nhiên, sự thật này chỉ đúng khi tất cả các key có cùng chuẩn - nó không đúng khi các key khác nhau có chuẩn khác nhau. Trực quan, điều này là do chuẩn của các key có thể điều chỉnh trọng số attention mà chúng nhận được, tất cả các điều khác đều bằng nhau. Vì vậy nếu key A có chuẩn lớn hơn key B, key A có thể nhận trọng số attention cao hơn key B ngay cả khi key A xa query hơn key B. Kết quả là, việc áp dụng k-NNS một cách ngây thơ trong trường hợp tổng quát sẽ thất bại trong việc xác định các key quan trọng nhất.

Trong bài báo này, chúng tôi phát triển một phương pháp tăng tốc không yêu cầu huấn luyện lại hoặc áp đặt bất kỳ ràng buộc nào trên các key. Nó vừa chính xác vừa hiệu quả về mặt tính toán, và cũng có thể hoạt động với các mặt nạ attention phổ biến trong Transformer, như mặt nạ nhân quả. Dưới đây chúng tôi sẽ mô tả chi tiết.

4.1 ATTENTION TĂNG TỐC TỔNG QUÁT KHÔNG CẦN HUẤN LUYỆN LẠI

Thay vì áp dụng k-NNS trực tiếp lên các key gốc, đầu tiên chúng ta sẽ nhúng các key và query vào một không gian chiều cao hơn. Lấy cảm hứng từ Neyshabur & Srebro (2015), chúng ta chọn các hàm nhúng key và query sau, mà chúng ta ký hiệu là TK:Rd→Rd+1 và TQ:Rd→Rd+1:

TK(kj) = [kj/c, √(1− ∥kj∥2 2/c2)]⊤ (3)
TQ(qi) = [qi/∥qi∥2, 0]⊤ (4)

trong đó c≥max j′∥kj′∥2 ít nhất là chuẩn tối đa trên tất cả các key.

Hóa ra k key quan trọng nhất có thể được xác định bằng cách thực hiện k-NNS trên các nhúng key sử dụng nhúng query. Chúng tôi sẽ chỉ ra điều này dưới đây:

arg max j ai,j= arg max j softmax j (q⊤ikj′/√d) m j′=1 (5)

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

= arg max j q⊤ikj/√d (6)
= arg min j 1−2q⊤ikj/c∥qi∥2+ 1 (7)
= arg min j q⊤iqi/∥qi∥2 2−2q⊤ikj/c∥qi∥2+k⊤jkj/c2+ 1− ∥kj∥2 2/c2 (8)
= arg min j ∥qi/∥qi∥2−kj/c∥2 2+ 1− ∥kj∥2 2/c2 (9)
= arg min j ∥TQ(qi)−TK(kj)∥2 2 (10)

4.2 k-NNS CHÍNH XÁC CHO ATTENTION TĂNG TỐC

Vấn đề k-NNS là một trong những vấn đề được nghiên cứu nhiều nhất trong khoa học máy tính lý thuyết. Nhiều thuật toán đã được phát triển, và thường có thể đạt được tốc độ tăng đáng kể bằng cách cho phép sai sót với một xác suất nào đó. Các thuật toán như vậy được gọi là thuật toán ngẫu nhiên.

Trong bối cảnh của LLM, số lượng lớp attention thường cao và do đó lỗi từ các lớp trước có thể tích lũy. Do đó, điều quan trọng là thuật toán k-NNS đạt được độ chính xác cao. Việc chọn một thuật toán k-NNS phù hợp do đó là rất quan trọng.

Hầu hết các thuật toán k-NNS dựa trên bucking, đặt các key vào các bucket rời rạc và tìm kiếm trên các bucket chứa query. Mặt khác, các thuật toán dựa trên ranking so sánh thứ hạng của các key khác nhau so với query và tìm kiếm trên các key được xếp hạng cao. Một thuật toán dựa trên bucketing hiệu quả sử dụng một ngưỡng cố định về độ tương tự, và do đó một số lượng biến đổi (bao gồm cả 0) key có thể đạt ngưỡng; mặt khác, một thuật toán dựa trên ranking trả về một số lượng key cố định, điều này hiệu quả tương đương với việc chọn một ngưỡng biến đổi về độ tương tự dựa trên phân phối của các key, như được hiển thị trong Hình 2. Một ví dụ của thuật toán dựa trên bucketing là locality-sensitive hashing (LSH) (Indyk & Motwani, 1998), và một ví dụ của thuật toán dựa trên ranking là Prioritized DCI (Li & Malik, 2017). Như được hiển thị trong Hình 2, LSH băm mỗi key vào một bucket liên kết với giá trị băm, trong khi Prioritized DCI xếp hạng các key theo các hướng ngẫu nhiên.

Để tăng tốc attention, chúng tôi cho rằng các thuật toán dựa trên ranking phù hợp hơn các thuật toán dựa trên bucketing, bởi vì trọng số attention phụ thuộc vào cách các key khác nhau so sánh với nhau, thay vì một đánh giá tuyệt đối của mỗi key so với một ngưỡng cố định. Do đó, các thuật toán dựa trên ranking có thể mang lại khả năng nhớ lại tốt hơn các key thực sự quan trọng.

Hình 2: Sự khác biệt giữa k-NNS dựa trên ranking và dựa trên bucketing. Trái: minh họa hai phương pháp k-NNS, Prioritized DCI (dựa trên ranking) và LSH (dựa trên bucketing). Phải: số lượng key có phép chiếu nhỏ hơn một ngưỡng. Các thuật toán dựa trên ranking trả về một số lượng key cố định giống query nhất dưới phép chiếu (được hiển thị như một hàng có kích thước cố định), điều này hiệu quả lọc ra các điểm ngoài cửa sổ có kích thước biến đổi trên các phép chiếu. Các thuật toán dựa trên bucketing sử dụng cửa sổ có kích thước cố định (được hiển thị như một cột có kích thước cố định) và trả về tất cả các key có phép chiếu nằm trong đó.

4.3 k-NNS NHANH CHO ATTENTION TĂNG TỐC

Trong một Transformer, các key trong một lớp attention phụ thuộc vào đầu ra từ lớp attention trước đó. Do đó, một cơ sở dữ liệu cần được xây dựng cho mỗi lớp attention. Do đó, điều quan trọng là chọn một thuật toán k-NN đạt được cả việc xây dựng và truy vấn nhanh.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hơn nữa, trong bối cảnh của LLM, nhiều mô hình phổ biến sử dụng kiến trúc chỉ decoder. Các lớp attention trong các kiến trúc như vậy sử dụng mặt nạ nhân quả để ngăn token hiện tại được tạo phụ thuộc vào các token tương lai chưa được tạo. Attention có mặt nạ như vậy tương đương với việc loại trừ các key được che mặt khỏi tập hợp các key mà thuật toán k-NNS hoạt động trên. Vì vậy mỗi khi một token được tạo, một key trở nên không bị che mặt. Thay vì xây dựng một cơ sở dữ liệu mới mỗi khi một token được tạo, hiệu quả hơn là thêm các key một cách tăng dần vào cơ sở dữ liệu cho k-NNS.

May mắn thay, Prioritized DCI hiệu quả ở cả giai đoạn xây dựng và truy vấn. Nếu số lượng hướng chiếu ngẫu nhiên p gần bằng chiều nội tại của dữ liệu d′≥1 và số lượng láng giềng gần nhất k cần tìm là nhỏ, Prioritized DCI có thể trả về k-láng giềng gần nhất chính xác cho một truy vấn với xác suất cao trong khoảng thời gian ˜O(dkp/˜dm1−p/˜d), trong đó ˜O(·) ẩn các yếu tố log. Việc tiền xử lý của nó nhẹ, và do đó chỉ cần thời gian O(dpm). Nếu chúng ta so sánh điều này với độ phức tạp tính toán của attention vanilla là O(dmn), quan sát rằng không còn một thuật ngữ phụ thuộc vào mn, và do đó không còn sự phụ thuộc bậc hai vào độ dài chuỗi. Sau đó trong phần 5.1, chúng tôi cũng xác nhận thực nghiệm hiệu quả của Prioritized DCI và thấy rằng nó nhanh hơn mười một thuật toán k-NNS hàng đầu khác.

Để hỗ trợ che mặt nhân quả, chúng tôi đã mở rộng triển khai của Prioritized DCI để hỗ trợ cập nhật cơ sở dữ liệu tăng dần. Điều này có thể được thực hiện hiệu quả, vì cấu trúc dữ liệu bao gồm các danh sách đã sắp xếp, vì vậy việc chèn và xóa có thể được thực hiện trong thời gian O(logm) nếu chúng được triển khai như cây tìm kiếm nhị phân.

5 THỰC NGHIỆM

Hình 3: So sánh giữa mười hai thuật toán k-NNS trên tập dữ liệu fashion-mnist-784. Có tổng cộng 60.000 key và 10.000 query với 784 chiều. Nhiệm vụ là tìm top-10 láng giềng gần nhất từ toàn bộ tập key cho mỗi query. Trục X: Khả năng nhớ lại trung bình trên tất cả các query; Trục Y: Tổng thời gian chậm trễ (giây) bao gồm xây dựng cơ sở dữ liệu và truy vấn.

Trong phần này, chúng tôi sẽ so sánh sự đánh đổi recall-latency giữa các thuật toán k-NNS khác nhau và sau đó phân tích hiệu suất của IceFormer trên benchmark LRA (Tay et al., 2020), đây là một benchmark phổ biến cho các Transformer bối cảnh dài (Zhu & Soricut, 2021; Xiong et al., 2021; Zheng et al., 2022). Tiếp theo chúng tôi sẽ chứng minh các lợi thế của IceFormer áp dụng cho LLM với các lời nhắc dài làm đầu vào trên benchmark ZeroSCROLLS (Shaham et al., 2023) và benchmark LongEval (Li et al., 2023). Để đảm bảo tính mạnh mẽ của kết quả, chúng tôi đã sử dụng nhiều CPU khác nhau cho các thực nghiệm của mình - chúng tôi sử dụng Intel(R) Core(TM) i7-6850K 6-Core cho các thực nghiệm LRA, AMD Ryzen 9 5950X 16-Core cho các thực nghiệm ZeroSCROLLS, và AMD Ryzen 9 5900X 12-Core cho các thực nghiệm LongEval.

5.1 SO SÁNH CÁC THUẬT TOÁN k-NNS KHÁC NHAU

Chúng tôi so sánh khả năng nhớ lại các láng giềng gần nhất thực sự và tổng thời gian xây dựng và truy vấn của 12 thuật toán k-NNS, bao gồm Prioritized DCI và các thuật toán hoạt động tốt nhất từ các benchmark ANN (Aumüller et al., 2017), trên tập dữ liệu Fashion MNIST trong Hình 3. Như được hiển thị, Prioritized DCI đạt được sự đánh đổi recall-latency tốt nhất so với các thuật toán khác, điều này chứng minh tính phù hợp của nó trong bối cảnh của chúng tôi, yêu cầu xây dựng và truy vấn nhanh.

5.2 ĐÁNH GIÁ TRÊN BENCHMARK LONG RANGE ARENA (LRA)

Tập dữ liệu và Chỉ số. LRA bao gồm năm nhiệm vụ khác nhau: ListOps (Nangia & Bowman, 2018), truy xuất tài liệu (Retrieval) (Radev et al., 2013), phân loại văn bản (Text) (Maas et al., 2011), phân loại hình ảnh CIFAR-10 (Image) (Krizhevsky et al., 2009) và Pathfinder (Linsley et al., 2018). Cụ thể, cả năm nhiệm vụ đều bao gồm các chuỗi có tối đa 4k token. Chúng tôi tóm tắt thông tin tập dữ liệu trong phụ lục C.1 để biết thêm chi tiết. Trong thực nghiệm này, chúng tôi tuân theo phân chia train/test từ Tay et al. (2020) và báo cáo độ chính xác phân loại tập dữ liệu test, thời gian chạy trung bình của module attention, và sử dụng bộ nhớ CPU trong quá trình suy luận cho mỗi nhiệm vụ.

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Baseline. Ngoài Transformer vanilla, chúng tôi so sánh với Nyströmformer (Xiong et al., 2021), H-Transformer-1D (Zhu & Soricut, 2021), LARA (Zheng et al., 2022), Reformer (Nikita et al., 2020), Longformer (Beltagy et al., 2020), Performer (Choromanski et al., 2020), và Linformer (Wang et al., 2020). Để so sánh với Reformer, chúng tôi huấn luyện một mô hình Transformer với Q và K chung theo Nikita et al. (2020). Đối với Longformer và Linformer, vì chúng giới thiệu thêm tham số, chúng tôi khởi tạo ngẫu nhiên các tham số này khi tải trọng số đã được huấn luyện trước từ Transformer vanilla. Để so sánh công bằng, chúng tôi sử dụng benchmark đánh giá LRA được triển khai trong PyTorch bởi (Xiong et al., 2021), và chỉ thay thế module self-attention trong khi làm cho các phần khác của mỗi mô hình giống hệt với Transformer vanilla.

Chi tiết Triển khai. Đối với mỗi nhiệm vụ, chúng tôi bắt đầu bằng cách huấn luyện một mô hình cơ sở sử dụng GPU với kiến trúc Transformer vanilla. Sau đó chúng tôi thay thế module attention vanilla bằng một trong tám module attention hiệu quả được đề cập trước đó và áp dụng trực tiếp các trọng số đã được huấn luyện trước cho suy luận. Để đảm bảo so sánh công bằng, chúng tôi điều chỉnh kích thước batch thành 1, loại bỏ nhu cầu về mặt nạ padding vì IceFormer được đề xuất của chúng tôi tự động bỏ qua mặt nạ padding trong quá trình suy luận. Lưu ý rằng vì ràng buộc shared-KQ bổ sung, đối với nhiệm vụ Pathfinder, các nỗ lực của chúng tôi để huấn luyện một Transformer shared-KQ đã không thành công. Kết quả là, chúng tôi đã loại trừ các kết quả tương ứng khỏi phân tích tiếp theo. Ngoài ra, trong quá trình suy luận, chúng tôi sử dụng tổng cộng 4 thread CPU. Để biết thêm chi tiết toàn diện, vui lòng tham khảo phụ lục C.2.

Kết quả Suy luận. Lý tưởng nhất, độ chính xác của Transformer vanilla (không shared-KQ) phục vụ như một cận trên cho độ chính xác xấp xỉ của bảy mô hình khác (IceFormer (không shared-KQ), Nyströmformer, H-Transformer-1D, LARA, Longformer, Performer, và Linformer). Tương tự cho Transformer shared-KQ. Ngoài ra, thời gian suy luận module attention của Transformer vanilla sẽ là dài nhất, với các Transformer hiệu quả khác đạt được thời gian suy luận ngắn hơn với chi phí hy sinh độ chính xác dự đoán. Bảng 1 trình bày độ chính xác dự đoán và thời gian suy luận của module attention cho mỗi phương pháp. Các thiết lập siêu tham số được liệt kê trong phụ lục C.3. Nói chung, IceFormer được đề xuất của chúng tôi luôn luôn vượt trội hơn tất cả các Transformer hiệu quả, cung cấp xấp xỉ độ chính xác tốt nhất trong khi yêu cầu thời gian suy luận ít nhất trên tất cả năm nhiệm vụ. Điều này chứng minh tính tổng quát và hiệu quả của mô hình của chúng tôi.

Bảng 1: Hiệu suất của Transformer vanilla và tám phương pháp attention xấp xỉ trên các benchmark LRA.

[Bảng với kết quả so sánh các phương pháp khác nhau về độ chính xác và thời gian]

Đánh đổi Tốc độ & Độ chính xác. Đối với IceFormer, việc tăng mức độ xấp xỉ thường cải thiện hiệu quả mô hình nhưng có thể dẫn đến giảm hiệu suất dự đoán. Ở đây, chúng tôi nghiên cứu cách mức độ xấp xỉ ảnh hưởng đến tốc độ suy luận và độ chính xác bằng cách thay đổi số lượng ứng cử viên được trả về của IceFormer, k, từ 3 đến 10 cho mỗi nhiệm vụ và trình bày kết quả trong Hình 4. Từ hình, chúng tôi quan sát rằng trên tất cả các nhiệm vụ, khi k trở nên lớn hơn, IceFormer đạt được độ chính xác dự đoán được cải thiện nhưng trở nên kém hiệu quả hơn.

Phân tích Độ phức tạp Bộ nhớ. Bảng 2 tóm tắt việc sử dụng bộ nhớ tối đa cho mỗi phương pháp trong quá trình suy luận. Chúng tôi sử dụng cùng siêu tham số như trong Bảng 1 và duy trì kích thước batch là 1 để loại bỏ nhu cầu về mặt nạ padding. Bảng cho thấy IceFormer luôn luôn thể hiện việc sử dụng bộ nhớ đỉnh thấp nhất trên tất cả các nhiệm vụ. So với Transformer vanilla, IceFormer đạt được tiết kiệm bộ nhớ lên đến 0.862 GB.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 4: Đánh đổi giữa tốc độ và độ chính xác khi k thay đổi trên năm nhiệm vụ LRA. Trục ngang của mỗi biểu đồ là thời gian wall clock trung bình của module attention, và trục dọc là độ chính xác dự đoán của mô hình. Mỗi điểm tương ứng với một giá trị của k trong tập hợp sau: {3, 5, 8, 10}.

Bảng 2: Sử dụng bộ nhớ đỉnh (GB) trên benchmark LRA. Sử dụng bộ nhớ đỉnh là tổng sử dụng bộ nhớ của toàn bộ chương trình, bao gồm bộ nhớ cho cơ sở dữ liệu/chỉ mục Prioritized DCI.

[Bảng với kết quả sử dụng bộ nhớ của các phương pháp khác nhau]

5.3 ĐÁNH GIÁ TRÊN MÔ HÌNH NGÔN NGỮ LỚN (LLM)

Chúng tôi cũng đánh giá IceFormer trong bối cảnh LLM. Cụ thể, chúng tôi sử dụng IceFormer để tăng tốc quá trình xử lý lời nhắc trong LLM. Chúng tôi chọn Vicuna-7b-v1.5-16k (Zheng et al., 2023), được tinh chỉnh từ LLaMA 2 (Touvron et al., 2023) và là một trong những LLM mã nguồn mở hoạt động tốt nhất với độ dài bối cảnh lên đến 16K token, cho thực nghiệm sau. Để biết thêm chi tiết toàn diện bao gồm việc chọn k trong k-NNS của IceFormer, vui lòng tham khảo phụ lục E.1.

Đối với các thực nghiệm LLM sau đây, chúng tôi không so sánh IceFormer với Reformer, LARA và Nyströmformer vì các lý do sau: Reformer yêu cầu key và query được chia sẻ, điều này không có trong LLM đã được huấn luyện trước; Longformer chỉ đề xuất cách tăng tốc phần encoder của Transformer, do đó không thể áp dụng cho LLM chỉ decoder; LARA và Nyströmformer nhóm các token khác nhau thành các cluster khác nhau và do đó không thể xử lý mặt nạ nhân quả trong LLM, sử dụng kiến trúc chỉ decoder. Tất cả baseline yêu cầu huấn luyện lại (Longformer, Performer và Linformer) cũng được loại trừ khỏi so sánh. Thêm chi tiết có thể được tìm thấy trong phụ lục E.2.

Kết quả ZeroSCROLLS. Chúng tôi so sánh IceFormer với mô hình Vicuna-7b-v1.5-16k vanilla và H-Transformer-1D áp dụng cho Vicuna-7b-v1.5-16k trên benchmark ZeroSCROLLS (Shaham et al., 2023) được thiết kế đặc biệt cho LLM và chứa mười nhiệm vụ ngôn ngữ tự nhiên đa dạng yêu cầu hiểu bối cảnh đầu vào dài, bao gồm tóm tắt, trả lời câu hỏi, phân loại cảm xúc tổng hợp và sắp xếp lại thông tin. Mỗi nhiệm vụ có độ dài chuỗi khác nhau thay đổi từ 3k đến 10k. Chúng tôi đo điểm ZeroSCROLLS và độ trễ của module attention. Bảng 3 cho thấy IceFormer đạt được tăng tốc lên đến 3.0× so với self-attention tiêu chuẩn trong khi đạt được ít nhất 99.0% hiệu suất của mô hình vanilla không tăng tốc cùng lúc.

Kết quả LongEval & Phân tích Khả năng Mở rộng. Để cung cấp phân tích toàn diện hơn về khả năng mở rộng của IceFormer trong bối cảnh LLM, chúng tôi đã tiến hành thêm thực nghiệm trên benchmark LongEval (Li et al., 2023), được thiết kế để đo hiệu suất bối cảnh dài và bao gồm hai nhiệm vụ: nhiệm vụ truy xuất chủ đề với độ dài lời nhắc thay đổi từ 3k đến 16k, và nhiệm vụ truy xuất dòng với độ dài lời nhắc thay đổi từ 5k đến 16k. Trong Hình 5, chúng tôi trình bày độ trễ trung bình của module attention tương ứng với độ dài lời nhắc đầu vào khác nhau cũng như độ chính xác suy luận sử dụng mô hình Vicuna-7b-v1.5-16k vanilla và IceFormer. Từ hình, IceFormer có thể đạt được

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 3: Hiệu suất của mô hình Vicuna-7b-v1.5-16k vanilla, H-Transformer-1D và IceFormer trên các benchmark ZeroSCROLLS. Các số trong ngoặc cho biết so sánh tương đối với mô hình vanilla không tăng tốc, ký hiệu là Vicuna-7b-v1.5-16k. Chúng tôi sử dụng cùng viết tắt cho tên metric và task như được chỉ định trong bài báo gốc (Shaham et al., 2023). Chúng tôi giới thiệu độc giả quan tâm đến bài báo gốc để biết chi tiết.

[Bảng chi tiết với kết quả hiệu suất của các mô hình trên nhiều task khác nhau]

Hình 5: Phân tích khả năng mở rộng cho IceFormer trên benchmark LongEval. Hình bên trái hiển thị kết quả của nhiệm vụ truy xuất chủ đề; hình bên phải hiển thị kết quả của nhiệm vụ truy xuất dòng. Trục X: độ dài của lời nhắc đầu vào; Trục Y (Trái): độ chính xác truy xuất; Trục Y (Phải): thời gian wall clock xử lý trung bình (giây) của module attention.

độ chính xác suy luận gần như giống hệt so với Vicuna-7b-v1.5-16k vanilla. Đáng chú ý, khi độ dài lời nhắc tăng, có sự tăng tương ứng trong độ trễ suy luận cho cả hai phương pháp và cho cả hai nhiệm vụ. Tuy nhiên, ngay cả với độ dài lời nhắc rất dài, IceFormer vẫn duy trì khả năng mở rộng của nó và luôn luôn vượt trội hơn Transformer vanilla. Hơn nữa, khi độ dài của lời nhắc tăng, sự khác biệt trong độ trễ giữa IceFormer và Transformer vanilla trở nên lớn hơn, chứng minh khả năng mở rộng và hiệu quả vượt trội của IceFormer trong bối cảnh của LLM.

6 KẾT LUẬN

Trong bài báo này, chúng tôi trình bày IceFormer, một phương pháp mới để cải thiện hiệu quả thời gian suy luận của các Transformer đã được huấn luyện trước trên CPU. Đáng chú ý, trái ngược với các phương pháp khác, IceFormer không yêu cầu huấn luyện lại, không yêu cầu các ràng buộc đặc biệt được áp đặt trên cơ chế attention và đồng thời đạt được độ chính xác cao và suy luận nhanh. Những lợi thế này làm cho IceFormer rất phù hợp cho việc triển khai LLM trên CPU, đặc biệt khi LLM cần xử lý các chuỗi rất dài làm đầu vào. Các phát hiện thực nghiệm trên ba benchmark thuyết phục minh họa hiệu quả của cách tiếp cận của chúng tôi trong việc giảm độ phức tạp thời gian và không gian bậc hai của Transformer cả trong trường hợp với cơ chế attention hai chiều và nhân quả.

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

TÀI LIỆU THAM KHẢO

Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, và Li Yang. Etc: Encoding long and structured inputs in transformers. arXiv preprint arXiv:2004.08483, 2020.

Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, và Ludwig Schmidt. Practical and optimal lsh for angular distance. Advances in neural information processing systems, 28, 2015.

Anthropic. 100k context windows, 2023. URL https://www.anthropic.com/index/100k-context-windows.

Martin Aumüller, Erik Bernhardsson, và Alexander Faithfull. Ann-benchmarks: A benchmarking tool for approximate nearest neighbor algorithms. In International conference on similarity search and applications, pp. 34–49. Springer, 2017.

Iz Beltagy, Matthew E Peters, và Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

Rewon Child, Scott Gray, Alec Radford, và Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, và Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.

Dave Dice và Alex Kogan. Optimizing inference performance of transformers on cpus. arXiv preprint arXiv:2102.06621, 2021.

Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, và Jonathan Berant. Memory-efficient transformers via top-k attention. arXiv preprint arXiv:2106.06899, 2021.

Piotr Indyk và Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pp. 604–613, 1998.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, và Hao Zhang. How long can context length of open-source LLMs truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. URL https://openreview.net/forum?id=LywifFNXV5.

Ke Li và Jitendra Malik. Fast k-nearest neighbour search via prioritized dci. In International conference on machine learning, pp. 2081–2090. PMLR, 2017.

Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, và Thomas Serre. Learning long-range spatial dependencies with horizontal gated recurrent units. Advances in neural information processing systems, 31, 2018.

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, và Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142–150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P11-1015.

Nikita Nangia và Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv preprint arXiv:1804.06028, 2018.

Behnam Neyshabur và Nathan Srebro. On symmetric and asymmetric lshs for inner product search. In International Conference on Machine Learning, pp. 1926–1934. PMLR, 2015.

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Kitaev Nikita, Kaiser Lukasz, Levskaya Anselm, et al. Reformer: The efficient transformer. In Proceedings of International Conference on Learning Representations (ICLR), 2020.

OpenAI. Openai gpt-4, 2023. URL https://openai.com/gpt-4.

Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, và Lingpeng Kong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021.

Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, và Amjad Abu-Jbara. The acl anthology network corpus. Language Resources and Evaluation, 47:919–944, 2013.

Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, và Omer Levy. Zeroscrolls: A zero-shot benchmark for long text understanding, 2023.

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, và Donald Metzler. Long range arena: A benchmark for efficient transformers. arXiv preprint arXiv:2011.04006, 2020.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017.

Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, và Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.

Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, và Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 14138–14148, 2021.

Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. arXiv preprint arXiv:2306.14048, 2023.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, và Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

Lin Zheng, Chong Wang, và Lingpeng Kong. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011–27041. PMLR, 2022.

Zhenhai Zhu và Radu Soricut. H-transformer-1D: Fast one-dimensional hierarchical attention for sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3801–3815, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.294. URL https://aclanthology.org/2021.acl-long.294.

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

A MÃ GIẢ CHO ICEFORMER

Chúng tôi cung cấp mã giả dưới đây cho IceFormer. Để hoàn thiện, chúng tôi cũng bao gồm mã giả của Prioritized DCI, mà chúng tôi đã điều chỉnh từ (Li & Malik, 2017) và thêm các sửa đổi của chúng tôi. Trong mã giả sau đây, TK và TQ là các hàm nhúng key và query, tương ứng. Trong triển khai của chúng tôi, chúng tôi triển khai một phiên bản đệ quy của thuật toán, cung cấp hiệu suất nhanh hơn trong thực tế.

Thuật toán 1 IceFormer attention
Yêu cầu: Một tập Sq của n điểm q1, . . . , qn∈Rd, một tập Sk của n key k1, . . . , kn∈Rd, một tập Sv của n value v1, . . . , vn∈Rd′, số lượng chỉ mục đơn giản m tạo thành một chỉ mục tổng hợp, số lượng chỉ mục tổng hợp L, số lượng điểm cần truy xuất k0 và số lượng điểm cần thăm k1 trong mỗi chỉ mục tổng hợp

function ICEFORMER_ATTENTION (Sq, Sk, Sv, m, L, k0, k1)
    O←ma trận không ∈Rn×d′ với các hàng oi∈Rd′
    Sp←CONSTRUCT (Sk, m, L )
    for i= 1 to n do
        Sl←QUERY (qi, Sp, k0, k1)
        for j= 1 to n do
            if j∈Sl then
                ˜sij←q⊤ikj/√d
            else
                ˜sij←0
            end if
        end for
        for j= 1 to n do
            if j∈Sl then
                ˜ai,j←softmax j(˜sij) =exp(˜si,j)/ΣΣk∈Slexp(˜si,k)
            else
                ˜aij←0
            end if
        end for
        oi←Σj∈Sl˜ai,jvj
    end for
    return O
end function

Thuật toán 2 Quy trình xây dựng cấu trúc dữ liệu
Yêu cầu: Một tập dữ liệu D của n điểm k1, . . . , kn, số lượng chỉ mục đơn giản m tạo thành một chỉ mục tổng hợp và số lượng chỉ mục tổng hợp L

function CONSTRUCT (D, m, L )
    {ujl}j∈[m],l∈[L]←mL vector đơn vị ngẫu nhiên trong Rd
    {Tjl}j∈[m],l∈[L]←mL cây tìm kiếm nhị phân hoặc skip list rỗng
    for j= 1 to m do
        for l= 1 to L do
            for i= 1 to n do
                kijl← ⟨TK(ki), ujl⟩
                Chèn (kijl, i) vào Tjl với kijl là key và i là value
            end for
        end for
    end for
    return {(Tjl, ujl)}j∈[m],l∈[L]
end function

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Thuật toán 3 Quy trình truy vấn k-láng giềng gần nhất
Yêu cầu: Điểm truy vấn q trong Rd, cây tìm kiếm nhị phân/skip list và các vector chiếu liên quan {(Tjl, ujl)}j∈[m],l∈[L], số lượng điểm cần truy xuất k0 và số lượng điểm cần thăm k1 trong mỗi chỉ mục tổng hợp

function QUERY (q,{(Tjl, ujl)}j,l, k0, k1)
    Cl←mảng kích thước n với các phần tử được khởi tạo thành 0 ∀l∈[L]
    qjl← ⟨TQ(q), ujl⟩ ∀j∈[m], l∈[L]
    Sl← ∅ ∀ l∈[L]
    Pl←hàng đợi ưu tiên rỗng ∀l∈[L]
    for l= 1 to L do
        for j= 1 to m do
            (p(1)jl, h(1)jl)←nút trong Tjl có key gần nhất với qjl
            Chèn (p(1)jl, h(1)jl) với ưu tiên −|p(1)jl−qjl| vào Pl
        end for
    end for
    for i′= 1 to k1−1 do
        for l= 1 to L do
            if |Sl|< k0 then
                (p(i)jl, h(i)jl)←nút có ưu tiên cao nhất trong Pl
                Loại bỏ (p(i)jl, h(i)jl) khỏi Pl và chèn nút trong Tjl có key gần nhất tiếp theo với qjl,
                được ký hiệu là (p(i+1)jl, h(i+1)jl), với ưu tiên −|p(i+1)jl−qjl| vào Pl
                Cl[h(i)jl]←Cl[h(i)jl] + 1
                if Cl[h(i)jl] =m then
                    Sl←Sl∪ {h(i)jl}
                end if
            end if
        end for
    end for
    return k điểm trong S ≡ ∪l∈[L]Sl có giá trị tích vô hướng tối đa với q
end function

B CHỨNG MINH

B.1 CHỨNG MINH 1

Ở đây, chúng tôi cung cấp suy dẫn từng bước đầy đủ của sự tương đương toán học giữa việc tiến hành tìm kiếm k-láng giềng gần nhất trên các key được chuẩn hóa và xác định các key đạt được trọng số attention cao nhất.

arg max j ai,j= arg max j softmax j (q⊤ikj′/√d) m j′=1 (11)
= arg max j q⊤ikj/√d (12)
= arg min j ∥qi∥2 2−2q⊤ikj+ 1 (13)

Vì ∥kj′∥2= 1 cho tất cả j′, ∥qi∥2 2−2q⊤ikj+ 1 = ∥qi∥2 2−2q⊤ikj+∥kj∥2 2=∥qi−kj∥2 2,

arg max j ai,j= arg min j ∥qi∥2 2−2q⊤ikj+ 1 (14)
= arg min j ∥qi−kj∥2 2 (15)

B.2 CHỨNG MINH 2

Ở đây, chúng tôi cung cấp suy dẫn từng bước đầy đủ của kết quả trong 4.1 thiết lập sự tương đương toán học giữa việc tiến hành tìm kiếm k-láng giềng gần nhất trên các key đã biến đổi và xác định các key đạt được trọng số attention cao nhất.

arg max j ai,j= arg max j softmax j (q⊤ikj′/√d) m j′=1 (16)
= arg max j q⊤ikj/√d (17)
= arg max j q⊤ikj (18)
= arg min j −2q⊤ikj (19)
= arg min j 2−2q⊤ikj/c∥qi∥2 (20)
= arg min j 1−2q⊤ikj/c∥qi∥2+ 1 (21)
= arg min j ∥qi∥2 2/∥qi∥2 2−2q⊤ikj/c∥qi∥2+∥kj∥2 2/c2+ 1− ∥kj∥2 2/c2 (22)
= arg min j q⊤iqi/∥qi∥2 2−2q⊤ikj/c∥qi∥2+k⊤jkj/c2+ 1− ∥kj∥2 2/c2 (23)
= arg min j (qi/∥qi∥2−kj/c)⊤(qi/∥qi∥2−kj/c) + 1− ∥kj∥2 2/c2 (24)
= arg min j ∥qi/∥qi∥2−kj/c∥2 2+ 1− ∥kj∥2 2/c2 (25)
= arg min j ∥qi/∥qi∥2−kj/c∥2 2+ (0−√(1− ∥kj∥2 2/c2))2 (26)
= arg min j ∥TQ(qi)−TK(kj)∥2 2 (27)
= arg min j ∥TQ(qi)−TK(kj)∥2 (28)

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

C CHI TIẾT THÊM VỀ THIẾT LẬP THỰC NGHIỆM LRA

C.1 CHI TIẾT TẬP DỮ LIỆU

Trong các thực nghiệm LRA của chúng tôi, đối với Retrieval, Text và Pathfinder (phiên bản 64×64), chúng tôi sử dụng trực tiếp tập dữ liệu từ cơ sở mã LRA1. Bởi vì các tập dữ liệu gốc cho ListOps và Image chỉ chứa các chuỗi ngắn, chúng tôi tạo ra các mẫu dài hơn cho ListOps sử dụng cùng mã từ cơ sở mã LRA với 4000 là độ dài tối đa; đối với nhiệm vụ Image, chúng tôi sử dụng một phiên bản của tập dữ liệu CIFAR-10 được super-resolved thành 64×642 thay vì tập dữ liệu CIFAR-10 32×32 độ phân giải thấp gốc. Chúng tôi tuân theo chính xác cùng phân chia train/test như bài báo LRA gốc (Tay et al., 2020). Chi tiết của tập dữ liệu LRA được liệt kê trong Bảng 4.

Bảng 4: Chi tiết Tập dữ liệu LRA.

[Bảng chi tiết về các nhiệm vụ trong LRA benchmark]

C.2 CẤU HÌNH MÔ HÌNH CƠ SỞ

Chúng tôi tuân theo thiết lập thực nghiệm của công trình trước (Zhu & Soricut, 2021) để huấn luyện mô hình cơ sở. Tuy nhiên, vì chúng tôi không thể huấn luyện thành công các mô hình Transformer cơ sở đến độ chính xác thỏa đáng trên các tập dữ liệu Image và Pathfinder sử dụng thiết lập gốc, chúng tôi đã giảm số lượng head và layer cho hai nhiệm vụ này. Chi tiết của mô hình cơ sở cho mỗi nhiệm vụ được nêu trong Bảng 5.

Bảng 5: Cấu hình của các mô hình cơ sở cho các nhiệm vụ khác nhau.

[Bảng cấu hình mô hình cho từng task]

C.3 SIÊU THAM SỐ CHO CÁC BASELINE VÀ PHƯƠNG PHÁP ĐỀ XUẤT

Đối với LARA và Nyströmformer, chúng tôi điều chỉnh tham số num_landmarks bằng cách tối ưu hóa trong khoảng {64, 128, 256, 512, 1024}. Đối với H-Transformer-1D, chúng tôi điều chỉnh tham số block_size bằng cách tối ưu hóa trong khoảng {64, 128, 256, 512, 1024}. Đối với Reformer, chúng tôi điều chỉnh các tham số num_hash và bucket_size: chúng tôi xem xét các giá trị của num_hash trong khoảng {1, 2, 4} và các giá trị của bucket_size trong khoảng {64, 128, 256, 512, 1024}. Đối với Longformer, Performer, và Linformer yêu cầu huấn luyện lại, vì hiệu suất kém của chúng, chúng tôi chọn các giá trị siêu tham số dẫn đến ít xấp xỉ nhất. Đối với IceFormer, chúng tôi điều chỉnh tham số top_k trong khoảng {3, 5, 8, 10, 15, 20}. Nói chung, một giá trị lớn hơn cho bucket_size, num_landmarks, block_size, hoặc top_k cho biết ít xấp xỉ tích cực hơn, có nghĩa là hiệu suất mô hình gần hơn với Transformer vanilla. Chúng tôi chọn các giá trị của siêu tham số dẫn đến sự đánh đổi độ chính xác-thời gian tốt nhất cho mỗi mô hình, và liệt kê chúng trong Bảng 6.

Bảng 6: Thiết lập siêu tham số cho các phương pháp khác nhau.

[Bảng siêu tham số cho từng phương pháp và task]

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

D THỰC NGHIỆM BỔ SUNG TRÊN LRA

Chất lượng Xấp xỉ. Để đánh giá mức độ tốt của việc các Transformer hiệu quả khác nhau xấp xỉ đầu ra của module attention vanilla đã được sửa đổi, chúng tôi đo lỗi xấp xỉ bằng cách tính toán L2-norm của sự khác biệt giữa đầu ra module attention của chúng và của module attention vanilla tiêu chuẩn (oi trong Phương trình 2). Các lỗi xấp xỉ trung bình cho các Transformer hiệu quả khác nhau, sử dụng cùng thiết lập siêu tham số của Bảng 6, được tóm tắt trong Bảng 7. Như được chỉ ra trong bảng, IceFormer luôn luôn đạt được lỗi xấp xỉ thấp nhất trên tất cả các nhiệm vụ LRA, cung cấp bằng chứng thêm về hiệu quả xấp xỉ của nó.

Bảng 7: Chất lượng của xấp xỉ trên benchmark LRA. Lỗi xấp xỉ của đầu ra module attention được báo cáo cho mỗi phương pháp trên tất cả các nhiệm vụ.

[Bảng lỗi xấp xỉ của các phương pháp]

Trực quan hóa của Bảng 1&2. Các kết quả từ Bảng 1 và Bảng 2 được biểu diễn trực quan trong Hình 6 và 7, tương ứng.

Hình 6: Độ trễ suy luận của IceFormer và các baseline (Transformer vanilla, Reformer, LARA, Nyströmer, H-Transformer-1D) trên benchmark LRA (càng nhỏ càng tốt).

Hình 7: Sử dụng bộ nhớ đỉnh (GB) trên benchmark LRA. Sử dụng bộ nhớ đỉnh là tổng sử dụng bộ nhớ của toàn bộ chương trình, bao gồm bộ nhớ cho cơ sở dữ liệu/chỉ mục Prioritized DCI (càng nhỏ càng tốt).

E CHI TIẾT THÊM VỀ THỰC NGHIỆM LLM

E.1 THIẾT LẬP THỰC NGHIỆM LLM

Chúng tôi sử dụng vicuna-7b-v1.5-16k làm LLM được kiểm tra trong phần 5.3. Nó chứa 32 lớp attention, mỗi lớp chứa 32 head attention với chiều bằng 128. Độ dài chuỗi đầu vào tối đa của nó là 16.384. Chúng tôi quan sát các mức độ thưa khác nhau trên các lớp khác nhau của LLM, và tính thưa này vẫn nhất quán trên các lời nhắc khác nhau. Do đó, trong tất cả các thực nghiệm LLM trong phần 5.3, chúng tôi áp dụng IceFormer để xấp xỉ các lớp tương đối thưa từ thứ mười sáu đến thứ ba mười một trong vicuna-7b-v1.5-16k. Lựa chọn này bao gồm tổng cộng 16 lớp, tương đương với một nửa tổng số lớp trong mô hình.

k trong k-NNS của IceFormer cho mỗi nhiệm vụ của benchmark ZeroSCROLLS và benchmark LongEval được định nghĩa là:

k= max(min( ⌊n∗α⌋,50),30) (29)

trong đó n là số lượng token đầu vào, ⌊x⌋ là hàm sàn, và α là một siêu tham số được đặt bởi người dùng. Trong benchmark ZeroSCROLLS, chúng tôi đặt α bằng 4e-3 cho các nhiệm vụ SSFD và QMsm; 5e-3 cho các nhiệm vụ GvRp, SQAL, Qspr, Nrtv, MuSQ và BkSS; 6e-3 cho các nhiệm vụ QALT và SpDg. Trong benchmark LongEval, chúng tôi đặt α bằng 5e-3 cho tất cả các thiết lập của cả hai nhiệm vụ.

E.2 MẶT NẠ NHÂN QUẢ VÀ CÁC TRANSFORMER HIỆU QUẢ THỜI GIAN SUY LUẬN KHÁC

Trong bài báo chính, chúng tôi không so sánh IceFormer với LARA và Nyströmformer trên LLM. Trong phần này, chúng tôi giải thích chi tiết về các vấn đề của mặt nạ nhân quả cho hai phương pháp này.

Hầu hết các mô hình dựa trên random-feature như LARA và Nyströmformer nhóm các token khác nhau thành các cluster khác nhau, được gọi là landmark. Để cho phép che mặt nhân quả trong các mô hình này, không chỉ việc che mặt cần được áp dụng ở cấp độ landmark để ngăn rò rỉ thông tin từ các token tương lai, một tập hợp mặt nạ bổ sung cũng được yêu cầu để che mặt các số lượng token khác nhau trong cùng landmark cho các query khác nhau. Cái sau không được hỗ trợ tự nhiên và đặc biệt khó triển khai. Kết quả là, khó áp dụng LARA và Nyströmformer cho các mô hình có mặt nạ nhân quả.

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

F ĐẦU RA VẤN BẢN CỦA ICEFORMER + LLM

Trong phần này, chúng tôi cung cấp đầu ra văn bản của IceFormer khi được áp dụng cho LLM (vicuna-7b-v1.5-16k) trong Hình 8&9. Chúng tôi cũng bao gồm thông tin đầy đủ của các lời nhắc đầu vào trong thư mục bổ sung của chúng tôi.

Hình 8: Đầu ra của IceFormer (Top-30) + LLM với 4k token đầu vào. Chúng tôi yêu cầu LLM tóm tắt một bài báo có tiêu đề "Frozen Food: The World's Favorite Killer".

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 9: Đầu ra của IceFormer (Top-40) + LLM với 8k token đầu vào. Chúng tôi yêu cầu LLM tóm tắt một bài báo có tiêu đề "Research of How Online Behavioral Advertising Influences Consumers".

--- TRANG 18 ---
