# 2405.02842.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/inference/2405.02842.pdf
# File size: 2408448 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Published as a conference paper at ICLR 2024
ICEFORMER : ACCELERATED INFERENCE WITH LONG -
SEQUENCE TRANSFORMERS ON CPU S
Yuzhen Mao, Martin Ester, Ke Li
School of Computing Science, Simon Fraser University
Burnaby, BC V5A 1S6, Canada
{yuzhenm,ester,keli }@sfu.ca
ABSTRACT
One limitation of existing Transformer-based models is that they cannot handle very
long sequences as input since their self-attention operations exhibit quadratic time
and space complexity. This problem becomes especially acute when Transformers
are deployed on hardware platforms equipped only with CPUs. To address this
issue, we propose a novel method for accelerating self-attention at inference time
that works with pretrained Transformer models out-of-the-box without requiring
retraining. We experiment using our method to accelerate various long-sequence
Transformers, including a leading LLaMA 2-based LLM, on various benchmarks
and demonstrate a speedup of 2.73√ó ‚àí7.63√ówhile retaining 98.6%‚àí99.6%of
the accuracy of the original pretrained models. The code is available on our project
website at https://yuzhenmao.github.io/IceFormer/ .
1 I NTRODUCTION
Transformers (Vaswani et al., 2017) have powered incredible advances in NLP, as exemplified by
large language models (LLMs) such as GPT-4 and LLaMA 2. Increasingly LLMs are applied to
exceptionally long input sequences, which enables many exciting applications such as long-form
content creation, extended conversations, and large document search and analysis (OpenAI, 2023;
Anthropic, 2023). While LLMs can be feasibly trained with expensive hardware accelerators (e.g.
GPUs), they need to be deployed on commodity devices, which may only be equipped with CPUs.
However, it is currently challenging to deploy LLMs on CPUs due to their high computation cost (Dice
& Kogan, 2021). A significant computational bottleneck arises from the self-attention mechanism
that is integral to Transformers ‚Äì both time and space complexity are quadratic in the sequence length.
This problem is exacerbated in the context of LLMs, which are often used on very long sequences.
To handle long input sequences, there has been substantial research into reducing the quadratic
time complexity of self-attention ‚Äì these methods are collectively known as efficient Transformers .
However, many do not meet the needs of LLMs and are therefore difficult to apply to LLMs.
An ideal acceleration method for LLMs should satisfy four criteria: (1) No retraining ‚Äì the method
should not require the model to be retrained, given the enormous computational expense of training
LLMs; (2) Generality ‚Äì the method should be applicable to a variety of LLMs, rather than just those
trained with particular constraints built-in; (3) High accuracy ‚Äì the method should not introduce
large approximation errors, since LLMs have many attention layers and so errors from earlier layers
can compound; (4) Fast inference ‚Äì the method should achieve fast test-time performance.
Satisfying all these criteria simultaneously is difficult, and to our knowledge no existing methods
can do so. For example, Transformers with fixed attention patterns, e.g., Longformer (Beltagy et al.,
2020), require retraining the model before they can be used. Reformer (Nikita et al., 2020) requires
keys to be normalized ‚Äì this requirement is not met in most pretrained models. Nystr√∂mformer (Xiong
et al., 2021) and LARA (Zheng et al., 2022) do not support causal masks, which are commonly found
in LLMs. Low-rank methods such as Performer (Choromanski et al., 2020) introduce substantial
approximation errors, especially when they are not retrained/finetuned.
1arXiv:2405.02842v1  [cs.LG]  5 May 2024

--- PAGE 2 ---
Published as a conference paper at ICLR 2024
Figure 1: Comparison between Transformer (Vaswani et al., 2017) (top row) and the proposed
method, IceFormer (bottom row). We illustrate with one query and k= 2 ink-NNS. In the two
attention matrices presented, the top-2 largest attention weights in each row are represented by a dark
color. The remaining attention weights are shown in a pale color in the vanilla attention matrix, and
are set to zero (depicted in white) in the sparse attention matrix.
In this paper, we propose an acceleration method, which we dub IceFormer due to its ability to
be applied directly in frozen models without retraining, that simultaneously satisfies the above four
criteria. Specifically, IceFormer (1) does not require retraining, (2) can be applied to most LLMs, (3)
can approximate vanilla attention accurately, and (4) achieves significantly faster inference speeds
compared to existing methods. We illustrate our method in comparison to the Transformer in Figure 1.
As shown, the Transformer computes the attention weights aijfor every possible combination of
query qiand key kj(Phase 1) and exhaustively enumerates all value vectors vjfor each query (Phase
2). In contrast, our method takes advantage of sparsity of the attention matrix and only computes the
highest attention weights and enumerates only the value vectors associated with them.
We conduct experiments on CPUs on the LRA (Tay et al., 2020), ZeroSCROLLS (Shaham et al., 2023),
and LongEval (Li et al., 2023) benchmarks. Across all three benchmarks, IceFormer demonstrates
substantially faster inference speeds than existing methods while attaining almost no accuracy loss
compared to the Transformer. On the LRA benchmark, on average IceFormer achieves a 7.63√ó
speedup relative to the Transformer while retaining 98.6%of its accuracy. Compared to the best
efficient Transformer with comparable accuracy for each task, IceFormer is on average 3.04√ófaster.
On the ZeroSCROLLS benchmark, IceFormer achieves a 2.73√óspeedup on average compared to a
leading LLaMA 2-based LLM while retaining 99.6%of its accuracy.
2 R ELATED WORK
Efficient Transformers can be categorized along two axes: method type and retraining requirement.
Along the first axis are sparsity-based methods and low-rank methods. Along the second axis are
methods that can and cannot be applied to common pretrained Transformers without retraining.
Sparsity-based methods employ a sparsified attention mechanism to capture global information and
integrate it with local attention results. Some approaches aim to improve the space complexity
compared to the vanilla attention mechanism without improving the time complexity, e.g., top- k
Attention (Gupta et al., 2021). Other approaches aim to improve both, e.g., Sparse Transformer (Child
et al., 2019), Longformer (Beltagy et al., 2020), and ETC (Ainslie et al., 2020). A substantial
limitation of these models is that the tokens that are attended to are predefined and remain static,
which do not adapt to varying input sequences. Because the original attention operation is permitted
to attend to any token, these models must be trained with their respective predefined constraints
2

--- PAGE 3 ---
Published as a conference paper at ICLR 2024
on tokens to be attended to. Reformer (Nikita et al., 2020) can attend to different sets of tokens
for different input sequences by using Locality Sensitive Hashing (LSH) (Andoni et al., 2015) to
group tokens into chunks and subsequently attending only to tokens within the same chunk as each
query and adjacent chunks. However, Reformer imposes two constraints that are not in the original
attention operation: keys must be normalized and queries and keys must be the same. Therefore,
Reformer must be trained with these constraints built-in. As a result, these methods cannot be applied
to pretrained, non-modified, models directly; instead, the models must be retrained with the required
constraints before these methods can be used.
Low-rank methods approximate the attention weight matrix with a low-rank matrix to reduce the
quadratic time and space complexity. Examples include Linformer (Wang et al., 2020) and Per-
former (Choromanski et al., 2020), which decompose the attention weight matrix into a product of
tall and wide matrices consisting of learned linear features or random features of the keys and queries,
respectively. However, these Transformers typically introduce significant approximation errors be-
cause attention weight matrices produced by the original attention operation, especially in the case
of long input sequences, typically have high rank. Consequently, models that use these approaches
must be trained with low-rank approximations built-in, in order to learn to be robust to the associated
approximation errors. As a result, these approaches cannot be applied to pretrained, non-modified,
models directly; instead, the models must be retrained with the required approximations before
these methods can be used. Other approaches provide more general methodologies that can leverage
weights pretrained with standard Transformers without retraining. These Transformers accelerate
the execution of the standard attention operation without altering the underlying architecture. Two
examples are Nystr√∂mformer (Xiong et al., 2021) and LARA (Zheng et al., 2022), which replace
the softmax structure in the self-attention mechanism with the product of separately activated query
and key matrices. Nystr√∂mformer utilizes the Nystr√∂m method, while LARA combines randomized
attention (RA) and random feature attentions (RFA) (Peng et al., 2021) to reconstruct the attention
weight matrix. In another example, H-Transformer-1D (Zhu & Soricut, 2021) recursively divides
the attention weight matrix into blocks and truncates the small singular values of each off-diagonal
blocks. All these approaches leverage low-rank approximations, as opposed to sparsity.
Other works propose hardware-specific optimizations without aiming to improve the computational
complexity. Examples include FlashAttention (Dao et al., 2022), which optimizes reads and writes
between levels of GPU memory, and H2O (Zhang et al., 2023), which dynamically retains a balance
of recent and heavy hitters tokens by a KV cache eviction policy. These strategies are dependent on
implementation and are specific to particular hardware platforms (e.g. GPU).
3 N OTATION AND PRELIMINARIES
Mathematically, the attention operation takes three matrices as input, K‚ààRm√ód,Q‚ààRn√ód,V‚àà
Rm√ód‚Ä≤, which denote keys, queries and values respectively, and outputs a matrix O‚ààRn√ód‚Ä≤.
Optionally, it may also take in a mask as input, S‚ààRn√óm, whose entries are either 0 or 1. The ith
rows of K,Q,VandO, denoted as ki,qi,viandoi, represent the ith key, query, value and output
respectively. The entry of Sin the ith row and jth column, denoted as si,j, represents whether the ith
query is allowed to attend to the jth key ‚Äî if it is 1, it would be allowed; if it is 0, it would not be.
A common masking scheme is the causal mask, where si,jis1ifi‚â•jand0otherwise. Keys and
queries have the same dimension d, and each key is associated with a value, and so the number of
keys and values is the same and denoted as m.
First the attention operation computes the attention weight matrix A‚ààRn√óm. Its entry in the ith
row and jth column, denoted as ai,j, is computed with the following formula:
ai,j=si,jexp
q‚ä§
ikj‚àö
d
Pm
j‚Ä≤=1si,j‚Ä≤expq‚ä§
ikj‚Ä≤‚àö
d (1)
Then the attention operation combines the values with the attention weights in the following way:
oi=mX
j=1ai,jvj (2)
3

--- PAGE 4 ---
Published as a conference paper at ICLR 2024
The attention matrix Ais typically sparse (Nikita et al., 2020; Gupta et al., 2021), i.e., in each row of
A, only a few attention weights have significant (large) values, while the majority of the remaining
values are close to zero. Suppose we can somehow identify the kunmasked keys that receive the
highest attention weights for each query qiwithout computing the attention weights for all keys.
Then, the original attention matrix Acan be approximated by only computing the inner product for
the identified keys, which can save significant amount of time and computational resource.
4 I CEFORMER : ACCELERATED SELF-ATTENTION FOR GENERAL KEYS
WITHOUT RETRAINING
To build a general-purpose retraining-free acceleration method, our approach must not require
modifications to the attention mechanism to change attention patterns or the introduction of new
model parameters to capture regularities in the attention patterns. This precludes popular strategies
such as attention mechanisms with predefined sparse attention patterns, e.g., (Child et al., 2019;
Beltagy et al., 2020; Ainslie et al., 2020), and learned dimensionality reduction of keys and queries,
e.g., (Wang et al., 2020; Choromanski et al., 2020).
Consequently, it is difficult to design an acceleration method that exploits known regularities in
the attention patterns without imposing the retraining requirement. We therefore aim to design an
acceleration method that does not make assumptions on the existence of regularity in the attention
patterns. In order to improve on the O(mn)complexity of vanilla attention, we need to adaptively
identify the most important keys (i.e., those that receive the highest attention weights) without
computing all attention weights. This seems like a chicken-and-egg problem: how can we know
which attention weights are highest without comparing them to all the other attention weights?
Remarkably, in the special case of normalized keys, as proposed in Nikita et al. (2020), this can
be done by leveraging k-nearest neighbour search ( k-NNS) to identify the kmost important keys
for each query. This relies on the following mathematical fact, whose derivation is in included in
Sect. B.1 of the appendix: if ‚à•kj‚à•2= 1for all j,arg max jai,j= arg min j‚à•qi‚àíkj‚à•2
2.
However, this fact only holds when all the keys have the same norm ‚Äì it is not true when different
keys differ in their norms. Intuitively, this is because the norms of keys can modulate the attention
weights they receive, all else being equal. So if key A has a larger norm than key B, key A can receive
a higher attention weight than key B even if key A is farther from the query than key B. As a result,
na√Øvely applying k-NNS in the general case would fail to identify the most important keys.
In this paper, we develop an acceleration method that does not require retraining or impose any
constraints on keys. It is both accurate and computationally efficient, and can also work with attention
masks that are common in Transformers, such as causal masks. Below we will describe the details.
4.1 G ENERAL RETRAINING -FREE ACCELERATED ATTENTION
Instead of applying k-NNS to the original keys directly, we will first embed the keys and queries into
a higher dimensional space. Inspired by Neyshabur & Srebro (2015), we choose the following key
and query embedding functions, which we denote as TK:Rd‚ÜíRd+1andTQ:Rd‚ÜíRd+1:
TK(kj) =
kj/cp
1‚àí ‚à•kj‚à•2
2/c2‚ä§(3)
TQ(qi) = [qi/‚à•qi‚à•20]‚ä§(4)
where c‚â•max j‚Ä≤‚à•kj‚Ä≤‚à•2is at least the maximum norm across all keys.
It turns out that the kmost important keys can be identified by performing k-NNS on the key
embeddings using the query embedding. We will show this below:
arg max
jai,j= arg max
jsoftmax j q‚ä§
ikj‚Ä≤‚àö
dm
j‚Ä≤=1!
(5)
4

--- PAGE 5 ---
Published as a conference paper at ICLR 2024
= arg max
jq‚ä§
ikj‚àö
d(6)
= arg min
j1‚àí2q‚ä§
ikj/c‚à•qi‚à•2+ 1 (7)
= arg min
jq‚ä§
iqi/‚à•qi‚à•2
2‚àí2q‚ä§
ikj/c‚à•qi‚à•2+k‚ä§
jkj/c2+ 1‚àí ‚à•kj‚à•2
2/c2(8)
= arg min
j‚à•qi/‚à•qi‚à•2‚àíkj/c‚à•2
2+ 1‚àí ‚à•kj‚à•2
2/c2(9)
= arg min
j‚à•TQ(qi)‚àíTK(kj)‚à•2
2 (10)
4.2 A CCURATE k-NNS FOR ACCELERATED ATTENTION
The problem of k-NNS is one of the most well studied problems in theoretical computer science.
Many algorithms have been developed, and often significant speedups can be obtained by allowing
for mistakes with some probability. Such algorithms are known as randomized algorithms.
In the context of LLMs, the number of attention layers is typically high and so errors from earlier
layers can compound. Therefore, it is essential for the k-NNS algorithm to achieve high accuracy.
Choosing an appropriate k-NNS algorithm is therefore crucial.
Most k-NNS algorithms are bucketing-based, which places keys into discrete buckets and searches
over buckets that contain the query. On the other hand, ranking-based algorithms compares the
rankings of different keys relative to the query and searches over highly ranked keys. A bucketing-
based algorithm effectively uses a fixed threshold on similarity, and so a variable number (including
zero) of keys can meet the threshold; on the other hand, a ranking-based algorithm returns a fixed
number of keys, which effectively amounts to choosing a variable threshold on similarity based
on the distribution of keys, as shown in Figure 2. An example of a bucketing-based algorithm is
locality-sensitive hashing (LSH) (Indyk & Motwani, 1998), and an example of a ranking-based
algorithm is Prioritized DCI (Li & Malik, 2017). As shown in Figure 2, LSH hashes each key into a
bucket associated with the hash value, whereas Prioritized DCI ranks keys along random directions.
For accelerating attention, we posit that ranking-based algorithms are better suited than bucketing-
based algorithms, because attention weights depend on how different keys compare to one another,
rather than an absolute evaluation of each key against a fixed threshold. Therefore, ranking-based
algorithms can yield better recall of truly important keys.
Figure 2: Difference between ranking-based and bucketing-based k-NNS. Left: illustration of two
k-NNS methods, Prioritized DCI (ranking-based) and LSH (bucketing-based). Right: the number of
keys whose projections are less than a threshold. Ranking-based algorithms return a fixed number of
keys are most similar to the query under projection (shown as a fixed-size row), which effectively
filters out points outside a variable-sized window on the projections. Bucketing-based algorithms use
a fixed-size window (shown as a fixed-size column) and return all keys whose projections lie within
it.
4.3 F ASTk-NNS FOR ACCELERATED ATTENTION
In a Transformer, the keys in an attention layer depend on the output from the preceding attention
layer. Therefore, a database needs to be constructed for each attention layer. Therefore, it is important
to choose a k-NN algorithm that attains both fast construction and querying.
5

--- PAGE 6 ---
Published as a conference paper at ICLR 2024
Moreover, in the context of LLMs, many popular models use decoder-only architectures. The
attention layers in such architectures use causal masks to prevent the currently generated token to
depend on future yet-to-be-generated tokens. Such masked attention is equivalent to excluding the
masked out keys from the set of keys the k-NNS algorithm operates over. So each time a token is
generated, one key becomes unmasked. Instead of constructing a new database each time a token is
generated, it is more efficient to add keys incrementally to the database for k-NNS.
Fortunately, Prioritized DCI is efficient at both the construction and querying stages. If the number of
random projection directions pis nearly as large as the intrinsic dimensionality of the data d‚Ä≤‚â•1
and the number of nearest neighbours kto look for is small, Prioritized DCI can return the exact
k-nearest neighbours for a query with high probability within approximately ÀúO(dkp/Àúdm1‚àíp/Àúd)time,
where ÀúO(¬∑)suppresses log factors. Its preprocessing is lightweight, and so only needs O(dpm)time.
If we compare this to the computational complexity of vanilla attention of O(dmn), observe that
there is no longer a term that depends on mn, and so there is no longer the quadratic dependence on
sequence length. Later in section 5.1, we also empirically validate the efficiency of Prioritized DCI
and found it to be faster than eleven other leading k-NNS algorithms.
To support causal masking, we extended the implementation of Prioritized DCI to support incremental
database updates. This can be done efficiently, since the data structure consists of sorted lists, so
insertions and deletions can be done in O(logm)time if they are implemented as binary search trees.
5 E XPERIMENTS
Figure 3: Comparison between twelve
k-NNS algorithms on fashion-mnist-
784 dataset. There are in total 60,000
keys and 10,000 queries with 784 di-
mensions. The task is to find top-10
closest neighbours from the entire set of
keys for every query. X-axis: Average
recall across all the queries; Y-axis: To-
tal latency (seconds) including database
construction and querying.In this section, we will compare the recall-latency trade-
off between different k-NNS algorithms and then analyze
the performance of IceFormer on the LRA benchmark (Tay
et al., 2020), which is a popular benchmark for long-context
Transformers (Zhu & Soricut, 2021; Xiong et al., 2021;
Zheng et al., 2022). Next we will demonstrate the advan-
tages of IceFormer applied to LLMs with long prompts
as input on the ZeroSCROLLS benchmark (Shaham et al.,
2023) and the LongEval benchmark (Li et al., 2023). To
ensure robustness of results, we used a variety of CPUs for
our experiments ‚Äì we used Intel(R) Core(TM) i7-6850K
6-Core for the LRA experiments, AMD Ryzen 9 5950X
16-Core for the ZeroSCROLLS experiments, and AMD
Ryzen 9 5900X 12-Core for the LongEval experiments.
5.1 D IFFERENT k-NNS ALGORITHMS COMPARISON
We compare the recall of true nearest neighbours and total
construction and querying time of 12 k-NNS algorithms, in-
cluding Prioritized DCI and the best performing algorithms
from ANN benchmarks (Aum√ºller et al., 2017), on the
Fashion MNIST dataset in Figure 3. As shown, Prioritized
DCI achieves the best recall-latency trade-off compared to
other algorithms, which demonstrates its suitability in our
setting, which requires fast construction and querying.
5.2 E VALUATION ON LONG RANGE ARENA (LRA) B ENCHMARK
Datasets and Metrics. LRA consists of five different tasks: ListOps (Nangia & Bowman, 2018),
document retrieval (Retrieval) (Radev et al., 2013), text classification (Text) (Maas et al., 2011),
CIFAR-10 image classification (Image) (Krizhevsky et al., 2009) and Pathfinder (Linsley et al., 2018).
Specifically, all the five tasks consist of sequences with at most 4k tokens. We summarize the dataset
information in the appendix C.1 for more details. In this experiment, we follow the train/test splits
from Tay et al. (2020) and report the test dataset classification accuracy, average running time of the
attention module, and CPU memory usage during inference for each task.
6

--- PAGE 7 ---
Published as a conference paper at ICLR 2024
Baselines. In addition to the vanilla Transformer, we compare with Nystr√∂mformer (Xiong et al.,
2021), H-Transformer-1D (Zhu & Soricut, 2021), LARA (Zheng et al., 2022), Reformer (Nikita et al.,
2020), Longformer (Beltagy et al., 2020), Performer (Choromanski et al., 2020), and Linformer (Wang
et al., 2020). In order to compare with Reformer, we train a Transformer model with shared Qand
Kaccording to Nikita et al. (2020). For Longformer and Linformer, as they introduce additional
parameters, we randomly initialize these parameters when loading the pre-trained weight from the
vanilla Transformer. For fair comparisons, we use the LRA evaluation benchmark implemented in
PyTorch by (Xiong et al., 2021), and only replace the self-attention module while making other parts
of each model exactly the same as the vanilla Transformer.
Implementation Details. For each task, we begin by training a base model using GPU with a
vanilla Transformer architecture. Then we replace the vanilla attention module with one of the eight
efficient attention modules mentioned earlier and directly apply the pre-trained weights for inference.
To ensure fair comparison, we adjust the batch size to 1, eliminating the need for a padding mask since
our proposed IceFormer automatically ignores padding masks during inference. Note that because
of the additional shared-KQ constraint, for the Pathfinder task, our attempts to train a shared-KQ
Transformer were unsuccessful. As a result, we have excluded the corresponding results from the
subsequent analysis. Additionally, during the inference, we utilize a total of 4 CPU threads. For more
comprehensive details, please refer to the appendix C.2.
Inference Results. Ideally, the accuracy of the vanilla Transformer (non-shared-KQ) serves as an
upper bound for the approximated accuracy of the other seven models (IceFormer (non-shared-KQ),
Nystr√∂mformer, H-Transformer-1D, LARA, Longformer, Performer, and Linformer). Similar for
the shared-KQ Transformer. Also, the attention module inference time of the vanilla Transformer
would be the longest, with other efficient Transformers achieving shorter inference times at the cost
of sacrificing prediction accuracy. Table 1 presents the prediction accuracy and inference time of
the attention module for each method. The hyper-parameter settings are listed in the appendix C.3.
In general, our proposed IceFormer consistently outperforms all efficient Transformers, offering
the best accuracy approximation while requiring the least inference time across all five tasks. This
demonstrates the generalizability and effectiveness of our model.
Table 1: The performance of vanilla Transformer, and eight approximate attention methods on the
LRA benchmarks.
Method shared-KQListOps Text Retrieval Image Pathfinder
Acc Time (s) Acc Time (s) Acc Time (s) Acc Time (s) Acc Time (s)
Transformer (Vaswani et al., 2017)‚úó 0.4255 2.9208 0.6019 0.6933 0.6586 8.3588 0.4132 4.9303 0.7514 0.9620
‚úì 0.4145 2.9134 0.5986 0.6603 0.6681 6.7946 0.3844 5.9804 / /
Reformer (Nikita et al., 2020) ‚úì 0.4121 1.4281 0.5941 0.2288 0.6467 1.4751 0.3726 3.6927 / /
LARA (Zheng et al., 2022) ‚úó 0.4125 0.6146 0.5831 0.2348 0.6401 1.8605 0.3094 2.6720 0.7380 0.5961
Nystr√∂mformer (Xiong et al., 2021) ‚úó 0.4128 0.7994 0.5838 0.3542 0.6540 2.4179 0.3754 1.7644 0.7176 0.9927
H-Transformer-1D (Zhu & Soricut, 2021) ‚úó 0.3265 1.9301 0.5944 0.4811 0.5808 3.5605 0.2286 1.2586 0.5286 0.5708
Longformer (Beltagy et al., 2020) ‚úó 0.1975 0.7406 0.5236 0.9862 0.4918 1.0443 0.1488 0.5451 0.5009 0.5899
Performer (Choromanski et al., 2020) ‚úó 0.1975 0.6571 0.5000 0.3327 0.4974 1.2058 0.1345 0.6404 0.5056 0.6395
Linformer (Wang et al., 2020) ‚úó 0.1975 3.1532 0.5088 1.8912 0.4940 1.6878 0.1064 0.7387 0.5022 1.3141
IceFormer (ours)‚úó 0.4153 0.3766 0.5978 0.0921 0.6541 0.8337 0.4046 0.5076 0.7442 0.3058
‚úì 0.4124 0.4678 0.6001 0.0903 0.6602 0.8480 0.3752 0.9581 / /
Speed & Accuracy Trade-off. For IceFormer, increasing the extent of approximation generally
improves model efficiency but can lead to a decrease in prediction performance. Here, we study how
the extent of approximation affects inference speed and accuracy by varying the number of returned
candidates of IceFormer, k, from 3 to 10 for each task and present the results in Figure 4. From
the figure, we observe that across all tasks, when kbecomes larger, IceFormer achieves improved
prediction accuracy but becomes less efficient.
Memory Complexity Analysis. Table 2 summarizes the maximum memory usage for each method
during inference. We employ the same hyper-parameters as in Table 1 and maintain a batch size of 1
to eliminate the need for padding masks. The table reveals that IceFormer consistently exhibits the
lowest peak memory usage across all tasks. In comparison to the vanilla Transformer, IceFormer
achieves memory savings of up to 0.862 GB.
7

--- PAGE 8 ---
Published as a conference paper at ICLR 2024
Figure 4: Tradeoff between speed and accuracy as kvaries on five LRA tasks. The horizontal axis
of each plot is the averaged wall clock time of attention module, and the vertical axis is the model
prediction accuracy. Each point corresponds to a value of kin the following set: {3, 5, 8, 10}.
Table 2: Peak memory usage (GB) on LRA benchmark. The peak memory usage is the total memory
usage of the whole program, which includes the memory for the Prioritized DCI database/index.
Method shared-KQ ListOps Text Retrieval Image Pathfinder
Transformer (Vaswani et al., 2017)‚úó 3.729 4.327 5.031 3.778 3.926
‚úì 3.631 4.265 4.877 3.740 /
Reformer (Nikita et al., 2020) ‚úì 3.623 3.983 4.250 3.687 /
LARA (Zheng et al., 2022) ‚úó 3.584 4.129 4.566 3.772 3.943
Nystr√∂mformer (Xiong et al., 2021) ‚úó 3.478 3.982 4.375 3.463 3.845
H-Transformer-1D (Zhu & Soricut, 2021) ‚úó 3.883 4.328 4.543 3.553 3.603
IceFormer (ours)‚úó 3.374 3.834 4.169 3.304 3.465
‚úì 3.306 3.756 4.053 3.286 /
5.3 E VALUATION ON LARGE LANGUAGE MODEL (LLM)
We evaluate IceFormer in the LLM setting as well. Specifically, we utilize IceFormer to accelerate
the prompt processing process in LLMs. We pick Vicuna-7b-v1.5-16k (Zheng et al., 2023), which
is fine-tuned from LLaMA 2 (Touvron et al., 2023) and is one of the top-performing open-source
LLMs with a context length up to 16K tokens, for the following experiment. For more comprehensive
details including the choice of kink-NNS of IceFormer, please refer to the appendix E.1.
For the following LLM experiments, we do not compare IceFormer with Reformer, LARA and
Nystr√∂mformer for the following reasons: Reformer requires keys and queries to be shared, which
is not the case in pre-trained LLMs; Longformer only proposed a way to speed up the encoder
part of the Transformer, thus cannot be applied to decoder-only LLMs; LARA and Nystr√∂mformer
group different tokens into different clusters and so cannot handle causal masks in LLMs, which
use decoder-only architectures. All baselines that require retraining (Longformer, Performer and
Linformer) are also excluded from the comparison. More details can be found in the appendix E.2.
ZeroSCROLLS Results. We compare IceFormer with the vanilla Vicuna-7b-v1.5-16k model and
H-Transformer-1D applied to Vicuna-7b-v1.5-16k on the ZeroSCROLLS benchmark (Shaham et al.,
2023) which is specifically designed for LLMs and contains ten diverse natural language tasks that
require understanding long input contexts, including summarization, question answering, aggregated
sentiment classification and information reordering. Each task has a different sequence length
varying between 3k and 10k. We measure ZeroSCROLLS scores and latency of the attention module.
Table 3 shows that IceFormer achieves up to 3.0 √óspeed-up compared to standard self-attention while
attaining at least 99.0% of the vanilla unaccelerated model performance at the same time.
LongEval Results & Scalability Analysis. To provide a more comprehensive analysis of Ice-
Former‚Äôs scalability in the LLM setting, we conducted additional experiments on the LongEval
benchmark (Li et al., 2023), which is designed to measure long-context performance and consists of
two tasks: topic retrieval task with prompt length varying from 3k to 16k, and line retrieval task with
prompt length varying from 5k to 16k. In Figure 5, we present the averaged latency of the attention
module corresponding to different input prompt length as well as the inference accuracy using the
vanilla Vicuna-7b-v1.5-16k model and IceFormer. From the figure, IceFormer can achieve nearly
8

--- PAGE 9 ---
Published as a conference paper at ICLR 2024
Table 3: The performance of the vanilla Vicuna-7b-v1.5-16k model, H-Transformer-1D and IceFormer
on the ZeroSCROLLS benchmarks. Numbers in parentheses indicate the relative comparison to the
vanilla unaccelerated model, denoted as Vicuna-7b-v1.5-16k. We employ the same abbreviations for
metric and task names as specified in the original paper (Shaham et al., 2023). We refer interested
readers to the original paper for the details.
Task (#tokens) Metric Vicuna-7b-v1.5-16k H-Transformer-1D IceFormer
GvRp (8k)Rgeo‚Üë 11.0 (100%) 6.8 (61.8%) 11.0 (100%)
Time (s) 5.07 (1.0 √ó) 4.22 (1.2 √ó) 1.89 (2.7 √ó)
SSFD (8k)Rgeo‚Üë 13.5 (100%) 6.3 (46.7%) 13.5 (100%)
Time (s) 5.02 (1.0 √ó) 4.18 (1.2 √ó) 1.81 (2.8 √ó)
QMsm (9k)Rgeo‚Üë 16.9 (100%) 10.7 (63.3%) 16.8 (99.4%)
Time (s) 6.47 (1.0 √ó) 4.62 (1.4 √ó) 2.51 (2.6 √ó)
SQAL (8k)Rgeo‚Üë 18.9 (100%) 7.3 (38.6%) 18.9 (100%)
Time (s) 5.01 (1.0 √ó) 2.27 (2.2 √ó) 1.92 (2.6 √ó)
Qspr (5k)F1‚Üë 34.2 (100%) 6.2 (18.1%) 34.0 (99.4%)
Time (s) 2.03 (1.0 √ó) 1.70 (1.2 √ó) 0.89 (2.3 √ó)
Nrtv (10k)F1‚Üë 14.7 (100%) 2.0 (13.6%) 14.7 (100%)
Time (s) 6.82 (1.0 √ó) 4.55 (1.5 √ó) 2.85 (2.4 √ó)
QALT (7k)AC‚Üë 48.8 (100%) 6.8 (13.9%) 48.6 (99.6%)
Time (s) 3.76 (1.0 √ó) 2.09 (1.8 √ó) 1.26 (3.0 √ó)
MuSQ (3k)F1‚Üë 18.6 (100%) 16.9 (90.9%) 18.5 (99.5%)
Time (s) 0.70 (1.0 √ó) 0.63 (1.1 √ó) 0.37 (1.9 √ó)
SpDg (7.5k)ES‚Üë 42.5 (100%) 2.9 (6.8%) 42.3 (99.5%)
Time (s) 4.43 (1.0 √ó) 2.22 (2.0 √ó) 1.47 (3.0 √ó)
BkSS (7.5k)Cidx‚Üë 19.5 (100%) 11.7 (60.0%) 19.3 (99.0%)
Time (s) 4.52 (1.0 √ó) 2.26 (2.0 √ó) 1.55 (2.9 √ó)
Avg. (7.5k)/‚Üë 23.9 (100%) 7.8 (32.5%) 23.8 (99.6%)
Time (s) 4.38 (1.0 √ó) 2.92 (1.5 √ó) 1.60 (2.7 √ó)
Figure 5: Scalability analysis for IceFormer on the LongEval benchmark. The left figure shows the
results of the topic retrieval task; the right figure shows the results of the line retrieval task. X-axis:
length of the input prompt; Y-axis (Left): retrieval accuracy; Y-axis (Right): averaged process wall
clock time (second) of the attention module.
identical inference accuracy compared with the vanilla Vicuna-7b-v1.5-16k. Notably, as the prompt
length increases, there is a corresponding increase in the inference latency for both methods and
for both tasks. However, even with very long prompt lengths, IceFormer maintains its scalability
and consistently outperforms the vanilla Transformer. Furthermore, as the length of the prompt
increases, the difference in the latency between IceFormer and the vanilla Transformer becomes
larger, demonstrating the superior scalability and efficiency of IceFormer in the context of LLMs.
6 C ONCLUSION
In this paper, we present IceFormer, a new method for improving the inference time efficiency of
pretrained Transformers on the CPU. Notably, in contrast to other methods, IceFormer does not
require retraining, does not require special constraints imposed on the attention mechanism and
simultaneously achieves high accuracy and fast inference. These advantages make IceFormer very
well-suited to LLM deployment on CPUs, especially when the LLM needs to handle very long
sequences as input. The experimental findings on three benchmarks compellingly illustrate the
effectiveness of our approach in reducing the quadratic time and space complexity of Transformers
both in cases with bi-directional and causal attention mechanisms.
9

--- PAGE 10 ---
Published as a conference paper at ICLR 2024
REFERENCES
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured
inputs in transformers. arXiv preprint arXiv:2004.08483 , 2020.
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical
and optimal lsh for angular distance. Advances in neural information processing systems , 28, 2015.
Anthropic. 100k context windows, 2023. URL https://www.anthropic.com/index/
100k-context-windows .
Martin Aum√ºller, Erik Bernhardsson, and Alexander Faithfull. Ann-benchmarks: A benchmarking
tool for approximate nearest neighbor algorithms. In International conference on similarity search
and applications , pp. 34‚Äì49. Springer, 2017.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150 , 2020.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794 , 2020.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R√©. Flashattention: Fast and memory-
efficient exact attention with io-awareness. Advances in Neural Information Processing Systems ,
35:16344‚Äì16359, 2022.
Dave Dice and Alex Kogan. Optimizing inference performance of transformers on cpus. arXiv
preprint arXiv:2102.06621 , 2021.
Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient
transformers via top- kattention. arXiv preprint arXiv:2106.06899 , 2021.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing ,
pp. 604‚Äì613, 1998.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,
Xuezhe Ma, and Hao Zhang. How long can context length of open-source LLMs truly promise?
InNeurIPS 2023 Workshop on Instruction Tuning and Instruction Following , 2023. URL https:
//openreview.net/forum?id=LywifFNXV5 .
Ke Li and Jitendra Malik. Fast k-nearest neighbour search via prioritized dci. In International
conference on machine learning , pp. 2081‚Äì2090. PMLR, 2017.
Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning long-
range spatial dependencies with horizontal gated recurrent units. Advances in neural information
processing systems , 31, 2018.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies , pp. 142‚Äì150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015 .
Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv
preprint arXiv:1804.06028 , 2018.
Behnam Neyshabur and Nathan Srebro. On symmetric and asymmetric lshs for inner product search.
InInternational Conference on Machine Learning , pp. 1926‚Äì1934. PMLR, 2015.
10

--- PAGE 11 ---
Published as a conference paper at ICLR 2024
Kitaev Nikita, Kaiser Lukasz, Levskaya Anselm, et al. Reformer: The efficient transformer. In
Proceedings of International Conference on Learning Representations (ICLR) , 2020.
OpenAI. Openai gpt-4, 2023. URL https://openai.com/gpt-4 .
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.
Random feature attention. arXiv preprint arXiv:2103.02143 , 2021.
Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl
anthology network corpus. Language Resources and Evaluation , 47:919‚Äì944, 2013.
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot
benchmark for long text understanding, 2023.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient
transformers. arXiv preprint arXiv:2011.04006 , 2020.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems , pp. 5998‚Äì6008, 2017.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and
Vikas Singh. Nystr√∂mformer: A nystr√∂m-based algorithm for approximating self-attention. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pp. 14138‚Äì14148, 2021.
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,
Yuandong Tian, Christopher R√©, Clark Barrett, et al. H _2o: Heavy-hitter oracle for efficient
generative inference of large language models. arXiv preprint arXiv:2306.14048 , 2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.
Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention mecha-
nism. In International Conference on Machine Learning , pp. 27011‚Äì27041. PMLR, 2022.
Zhenhai Zhu and Radu Soricut. H-transformer-1D: Fast one-dimensional hierarchical attention for
sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume
1: Long Papers) , pp. 3801‚Äì3815, Online, August 2021. Association for Computational Lin-
guistics. doi: 10.18653/v1/2021.acl-long.294. URL https://aclanthology.org/2021.
acl-long.294 .
11

--- PAGE 12 ---
Published as a conference paper at ICLR 2024
A P SEUDOCODE FOR ICEFORMER
We provide the pseudocode below for IceFormer. For completeness, we also include the pseudocode
of Prioritized DCI, which we adapted from (Li & Malik, 2017) and added our modifications. In the
following pseudocode below, TKandTQare the key and query embedding functions, respectively.
In our implementation, we implement a recursive version of the algorithm, which offers faster
performance in practice.
Algorithm 1 IceFormer attention
Require: A set Sqofnpoints q1, . . . , qn‚ààRd, a set Skofnkeysk1, . . . , kn‚ààRd, a set Svofnvalues
v1, . . . , vn‚ààRd‚Ä≤, the number of simple indices mthat constitute a composite index, the number of composite
indices L, the number of points to retrieve k0and the number of points to visit k1in each composite index
function ICEFORMER _ATTENTION (Sq, Sk, Sv, m, L, k 0, k1)
O‚Üêzero matrix ‚ààRn√ód‚Ä≤with rows oi‚ààRd‚Ä≤
Sp‚ÜêCONSTRUCT (Sk, m, L )
fori= 1tondo
Sl‚ÜêQUERY (qi, Sp, k0, k1)
forj= 1tondo
ifj‚ààSlthen
Àúsij‚Üêq‚ä§
ikj‚àö
d
else
Àúsij‚Üê0
end if
end for
forj= 1tondo
ifj‚ààSlthen
Àúai,j‚Üêsoftmax j(Àúsij) =exp(Àúsi,j)
P
k‚ààSlexp(Àúsi,k)
else
Àúaij‚Üê0
end if
end for
oi‚ÜêP
j‚ààSlÀúai,jvj
end for
return O
end function
Algorithm 2 Data structure construction procedure
Require: A dataset Dofnpoints k1, . . . , kn, the number of simple indices mthat constitute a composite index
and the number of composite indices L
function CONSTRUCT (D, m, L )
{ujl}j‚àà[m],l‚àà[L]‚ÜêmLrandom unit vectors in Rd
{Tjl}j‚àà[m],l‚àà[L]‚ÜêmLempty binary search trees or skip lists
forj= 1tomdo
forl= 1toLdo
fori= 1tondo
ki
jl‚Üê ‚ü®TK(ki), ujl‚ü©
Insert (ki
jl, i)intoTjlwithki
jlbeing the key and ibeing the value
end for
end for
end for
return {(Tjl, ujl)}j‚àà[m],l‚àà[L]
end function
12

--- PAGE 13 ---
Published as a conference paper at ICLR 2024
Algorithm 3 k-nearest neighbour querying procedure
Require: Query point qinRd, binary search trees/skip lists and their associated projection vectors
{(Tjl, ujl)}j‚àà[m],l‚àà[L], the number of points to retrieve k0and the number of points to visit k1in each
composite index
function QUERY (q,{(Tjl, ujl)}j,l, k0, k1)
Cl‚Üêarray of size nwith entries initialized to 0 ‚àÄl‚àà[L]
qjl‚Üê ‚ü®TQ(q), ujl‚ü© ‚àÄj‚àà[m], l‚àà[L]
Sl‚Üê ‚àÖ ‚àÄ l‚àà[L]
Pl‚Üêempty priority queue ‚àÄl‚àà[L]
forl= 1toLdo
forj= 1tomdo
(p(1)
jl, h(1)
jl)‚Üêthe node in Tjlwhose key is the closest to qjl
Insert (p(1)
jl, h(1)
jl)with priority ‚àí|p(1)
jl‚àíqjl|intoPl
end for
end for
fori‚Ä≤= 1tok1‚àí1do
forl= 1toLdo
if|Sl|< k0then
(p(i)
jl, h(i)
jl)‚Üêthe node with the highest priority in Pl
Remove (p(i)
jl, h(i)
jl)fromPland insert the node in Tjlwhose key is the next closest to qjl,
which is denoted as (p(i+1)
jl, h(i+1)
jl), with priority ‚àí|p(i+1)
jl‚àíqjl|intoPl
Cl[h(i)
jl]‚ÜêCl[h(i)
jl] + 1
ifCl[h(i)
jl] =mthen
Sl‚ÜêSl‚à™ {h(i)
jl}
end if
end if
end for
end for
return kpoints inS
l‚àà[L]Slthat have the maximum inner-product value with q
end function
B P ROOFS
B.1 P ROOF 1
Here, we provide the full step-by-step derivation of the mathematical equivalence between conducting
k-nearest neighbour search on normalized keys and identifying the keys that obtain the highest
attention weight.
arg max
jai,j= arg max
jsoftmax j q‚ä§
ikj‚Ä≤‚àö
dm
j‚Ä≤=1!
(11)
= arg max
jq‚ä§
ikj‚àö
d(12)
= arg min
j‚à•qi‚à•2
2‚àí2q‚ä§
ikj+ 1 (13)
Since‚à•kj‚Ä≤‚à•2= 1for all j‚Ä≤,‚à•qi‚à•2
2‚àí2q‚ä§
ikj+ 1 = ‚à•qi‚à•2
2‚àí2q‚ä§
ikj+‚à•kj‚à•2
2=‚à•qi‚àíkj‚à•2
2,
arg max
jai,j= arg min
j‚à•qi‚à•2
2‚àí2q‚ä§
ikj+ 1 (14)
= arg min
j‚à•qi‚àíkj‚à•2
2 (15)
B.2 P ROOF 2
Here, we provide the full step-by-step derivation of the result in 4.1 establishing the mathematical
equivalence between conducting k-nearest neighbour search on transformed keys and identifying the
13

--- PAGE 14 ---
Published as a conference paper at ICLR 2024
keys that obtain the highest attention weight.
arg max
jai,j= arg max
jsoftmax j q‚ä§
ikj‚Ä≤‚àö
dm
j‚Ä≤=1!
(16)
= arg max
jq‚ä§
ikj‚àö
d(17)
= arg max
jq‚ä§
ikj (18)
= arg min
j‚àí2q‚ä§
ikj (19)
= arg min
j2‚àí2q‚ä§
ikj/c‚à•qi‚à•2 (20)
= arg min
j1‚àí2q‚ä§
ikj/c‚à•qi‚à•2+ 1 (21)
= arg min
j‚à•qi‚à•2
2/‚à•qi‚à•2
2‚àí2q‚ä§
ikj/c‚à•qi‚à•2+‚à•kj‚à•2
2/c2+ 1‚àí ‚à•kj‚à•2
2/c2(22)
= arg min
jq‚ä§
iqi/‚à•qi‚à•2
2‚àí2q‚ä§
ikj/c‚à•qi‚à•2+k‚ä§
jkj/c2+ 1‚àí ‚à•kj‚à•2
2/c2(23)
= arg min
j(qi/‚à•qi‚à•2‚àíkj/c)‚ä§(qi/‚à•qi‚à•2‚àíkj/c) + 1‚àí ‚à•kj‚à•2
2/c2(24)
= arg min
j‚à•qi/‚à•qi‚à•2‚àíkj/c‚à•2
2+ 1‚àí ‚à•kj‚à•2
2/c2(25)
= arg min
j‚à•qi/‚à•qi‚à•2‚àíkj/c‚à•2
2+
0‚àíq
1‚àí ‚à•kj‚à•2
2/c22
(26)
= arg min
j‚à•TQ(qi)‚àíTK(kj)‚à•2
2 (27)
= arg min
j‚à•TQ(qi)‚àíTK(kj)‚à•2 (28)
C M ORE DETAILS ON THE LRA E XPERIMENTAL SETTING
C.1 D ATASET DETAILS
In our LRA experiments, for Retrieval, Text and Pathfinder ( 64√ó64version), we directly use the
dataset from LRA codebase1. Because the original datasets for ListOps and Image only contain short
sequences, we generate longer samples for ListOps using the same code from the LRA codebase with
4000 as the maximum length; for Image task, we use a version of the CIFAR-10 dataset super-resolved
to64√ó642instead of the original low-resolution 32√ó32CIFAR-10 dataset. We follow the exact
same train/test split as the original LRA paper (Tay et al., 2020). The details of the LRA dataset is
listed in Table 4.
Table 4: LRA Dataset Details.
Task ListOps Text Retrieval Image Pathfinder
Max length 3,991 4,000 4,000 4,096 4,096
Avg. length 2,232 1,267 3,917 4,096 4,096
number of classes 10 2 2 10 2
Accuracy by chance 0.100 0.500 0.500 0.100 0.500
C.2 B ASE MODEL CONFIGURATION
We follow the experimental setup of prior work (Zhu & Soricut, 2021) for training the base model.
However, since we were not able to successfully train base Transformer models to satisfactory
accuracy on Image and Pathfinder datasets using the original setting, we decreased the number of
heads and layers for these two tasks. The details of the base model for each task are outlined in
Table 5.
1https://github.com/google-research/long-range-arena/tree/main
2https://www.kaggle.com/datasets/joaopauloschuler/cifar10-64x64-resized-via-cai-super-resolution
14

--- PAGE 15 ---
Published as a conference paper at ICLR 2024
Table 5: Configurations of the base models for different tasks.
Task ListOps Text Retrieval Image Pathfinder
head embedding size 512 512 512 512 512
feed-forward size 2048 2048 2048 1024 1024
number of heads 8 8 8 4 4
number of layers 6 6 6 4 4
C.3 H YPER -PARAMETERS FOR THE BASELINES AND THE PROPOSED METHOD
For LARA and Nystr√∂mformer, we tuned the parameter num_landmarks by optimizing over the range
{64, 128, 256, 512, 1024}. For H-Transformer-1D, we tuned the parameter block_size by optimizing
over the range {64, 128, 256, 512, 1024}. For Reformer, we tuned the parameters num_hash and
bucket_size : we considered the values of num_hash in range {1, 2, 4} and the values of bucket_size in
range {64, 128, 256, 512, 1024}.For Longformer, Performer, and Linformer which require retraining,
because of their poor performance, we choose hyper-parameter values that result in the least amount
of approximation. For IceFormer, we tuned the parameter top_k over the range {3, 5, 8, 10, 15, 20}. In
general, a larger value for bucket_size ,num_landmarks ,block_size , ortop_k indicates less aggressive
approximation, meaning that the model performance is closer to that of the vanilla Transformer. We
select the values of the hyper-parameters that lead to the best accuracy-time trade-off for each model,
and list them in Table 6.
Table 6: Hyper-parameter settings for different methods.
Method hyper-parameter ListOps Text Retrieval Image Pathfinder
Reformer (Nikita et al., 2020)num_hash 1 1 1 1 /
bucket_size 512 128 256 1024 /
LARA (Zheng et al., 2022) num_landmarks 256 256 512 1024 1024
Nystr√∂mformer (Xiong et al., 2021) num_landmarks 256 256 512 512 1024
H-Transformer-1D (Zhu & Soricut, 2021) block_size 1024 512 1024 256 1024
Longformer (Beltagy et al., 2020) attention_window 2048 2048 2048 2048 2048
Performer (Choromanski et al., 2020) num_rand_features 2048 2048 2048 2048 2048
Linformer (Wang et al., 2020) num_proj_dim 2048 2048 2048 2048 2048
IceFormer top_k 8 3 10 10 10
IceFormer (shared-QK) top_k 10 3 10 20 /
D A DDITIONAL EXPERIMENTS ON LRA
Approximation Quality. In order to assess how well various efficient Transformers approximate the
outputs of the vanilla modified attention module, we measure the approximation error by computing
the L2-norm of the difference between their attention module outputs and those of the standard
vanilla attention module ( oiin Equation 2). The averaged approximation errors for different efficient
Transformers, utilizing the same hyper-parameter settings of Table 6, are summarized in Table 7. As
indicated in the table, IceFormer consistently achieves the lowest approximation errors across all
LRA tasks, providing further evidence of its approximation efficacy.
Table 7: Quality of the approximation on LRA benchmark. The approximation error of the attention
module output is reported for each method across all the tasks.
Method shared-KQ ListOps Text Retrieval Image Pathfinder
Reformer (Nikita et al., 2020) ‚úì 3.823 3.926 5.452 2.130 /
LARA (Zheng et al., 2022) ‚úó 2.395 9.456 10.025 22.066 9.261
Nystr√∂mformer (Xiong et al., 2021) ‚úó 5.758 10.269 6.523 18.789 10.442
H-Transformer-1D (Zhu & Soricut, 2021) ‚úó 6.110 10.605 5.676 53.926 12.228
IceFormer (ours)‚úó 2.140 3.891 1.825 6.873 8.749
‚úì 1.562 1.686 2.499 2.127 /
Visualization of Tables 1&2. The results from Table 1 and Table 2 are visually represented in
Figures 6 and 7, respectively.
15

--- PAGE 16 ---
Published as a conference paper at ICLR 2024
Figure 6: The inference latency of IceFormer and the baselines (vanilla Transformer, Reformer,
LARA, Nystr√∂mer, H-Transformer-1D) on the LRA benchmark (the smaller the better).
Figure 7: Peak memory usage (GB) on the LRA benchmark. The peak memory usage is the
total memory usage of the whole program, which includes the memory for the Prioritized DCI
database/index (the smaller the better).
E M ORE DETAILS ON THE LLM E XPERIMENT
E.1 LLM E XPERIMENT SETTING
We use vicuna-7b-v1.5-16k as the tested LLM in section 5.3. It contains 32 attention layers, each of
which contains 32 attention heads with dimensionality equals to 128. Its maximum input sequence
length is 16,384. We observe varying levels of sparsity across different layers of LLMs, and this
sparsity remains consistent across different prompts. Therefore, in all the LLMs experiments in
section 5.3, we apply IceFormer to approximate relatively sparse layers ranging from the sixteenth to
the thirty-first in vicuna-7b-v1.5-16k. This selection encompassed a total of 16 layers, equivalent to
half of the total number of layers in the model.
Thekin the k-NNS of IceFormer for each task of the ZeroSCROLLS benchmark and the LongEval
benchmark is defined as:
k= max(min( ‚åän‚àóŒ±‚åã,50),30) (29)
where nis the number of input tokens, ‚åäx‚åãis the floor function, and Œ±is a hyper-parameter set by the
users. In the ZeroSCROLLS benchmark, we set Œ±equals to 4e-3 for tasks SSFD and QMsm; 5e-3 for
tasks GvRp, SQAL, Qspr, Nrtv, MuSQ and BkSS; 6e-3 for tasks QALT and SpDg. In the LongEval
benchmark, we set Œ±equals to 5e-3 for all the settings of both two tasks.
E.2 C AUSAL MASKS AND OTHER INFERENCE -TIMEEFFICIENT TRANSFORMERS
In the main paper, we did not compare IceFormer with LARA and Nystr√∂mformer on LLMs. In this
section, we elaborate on the problems of causal masks for these two methods.
Most random-feature-based models such as LARA and Nystr√∂mformer group different tokens into
different clusters, known as landmarks . In order to enable causal masking in these models, not only
does the masking need to be applied at the landmark level to prevent the leakage of information from
future tokens, an additional set of masks is also required to mask out different numbers of tokens
within the same landmark for different queries. The latter is not supported natively and is especially
difficult to implement. As a result, it is difficult to apply LARA and Nystr√∂mformer to models that
have causal masks.
16

--- PAGE 17 ---
Published as a conference paper at ICLR 2024
F T EXT OUTPUTS OF ICEFORMER + LLM
In this section, we provide the text outputs of IceFormer when applied to the LLM (vicuna-7b-v1.5-
16k) in Figure 8&9. We also include the full information of the input prompts in our supplementary
folder.
Figure 8: Output of IceFormer (Top-30) + LLM with 4k input tokens. We ask the LLM to summarize
an article titled ‚ÄúFrozen Food: The World‚Äôs Favorite Killer‚Äù.
17

--- PAGE 18 ---
Published as a conference paper at ICLR 2024
Figure 9: Output of IceFormer (Top-40) + LLM with 8k input tokens. We ask the LLM to summarize
an article titled ‚ÄúResearch of How Online Behavioral Advertising Influences Consumers‚Äù.
18
