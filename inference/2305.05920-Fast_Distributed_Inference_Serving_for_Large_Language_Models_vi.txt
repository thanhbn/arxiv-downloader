# Phục vụ Suy luận Phân tán Nhanh cho Các Mô hình Ngôn ngữ Lớn
Bingyang Wu∗Yinmin Zhong∗Zili Zhang∗Shengyu Liu
Fangyue Liu Yuanhang Sun Gang Huang Xuanzhe Liu Xin Jin
Đại học Bắc Kinh

## Tóm tắt
Các mô hình ngôn ngữ lớn (LLMs) cung cấp sức mạnh cho một thế hệ mới của các ứng dụng AI tương tác được minh họa bởi ChatGPT. Tính chất tương tác của các ứng dụng này đòi hỏi độ trễ thấp cho suy luận LLM. Các hệ thống phục vụ LLM hiện tại sử dụng xử lý chạy-đến-hoàn-thành cho các công việc suy luận, điều này gặp phải vấn đề chặn đầu dòng và độ trễ dài.

Chúng tôi giới thiệu FastServe, một hệ thống phục vụ suy luận phân tán cho LLMs. FastServe khai thác mẫu tự hồi quy của suy luận LLM để cho phép preemption ở mức độ chi tiết của mỗi token đầu ra. FastServe sử dụng lập lịch preemptive để tối thiểu hóa độ trễ với một bộ lập lịch skip-join Multi-Level Feedback Queue mới. Dựa trên thiết lập bán-bất khả tri thông tin mới của suy luận LLM, bộ lập lịch tận dụng thông tin độ dài đầu vào để gán một hàng đợi ban đầu phù hợp cho mỗi công việc đến để tham gia. Các hàng đợi có ưu tiên cao hơn hàng đợi đã tham gia được bỏ qua để giảm việc hạ cấp. Chúng tôi thiết kế một cơ chế quản lý bộ nhớ GPU hiệu quả mà chủ động offload và upload trạng thái trung gian giữa bộ nhớ GPU và bộ nhớ host cho suy luận LLM. Chúng tôi xây dựng một nguyên mẫu hệ thống của FastServe và kết quả thực nghiệm cho thấy so với giải pháp tối tân vLLM, FastServe cải thiện thông lượng lên đến 31.4× và 17.9× dưới cùng yêu cầu độ trễ trung bình và tail, tương ứng.

## 1 Giới thiệu
Những tiến bộ trong các mô hình ngôn ngữ lớn (LLMs) mở ra những khả năng mới trong nhiều lĩnh vực và kích hoạt một thế hệ mới của các ứng dụng AI tương tác. Đáng chú ý nhất là ChatGPT [1] cho phép người dùng tương tác với một đại lý AI theo cách đàm thoại để giải quyết các nhiệm vụ từ dịch thuật ngôn ngữ đến kỹ thuật phần mềm. Khả năng ấn tượng của ChatGPT khiến nó trở thành một trong những ứng dụng phát triển nhanh nhất trong lịch sử [2]. Nhiều tổ chức theo xu hướng này để phát hành LLMs và các ứng dụng giống ChatGPT, như New Bing từ Microsoft [3], Gemini từ Google [4], Claude-3 [5] từ Anthropic, Qwen [6] từ Alibaba, v.v.

Phục vụ suy luận là quan trọng đối với các ứng dụng AI tương tác dựa trên LLMs. Để cung cấp trải nghiệm người dùng hấp dẫn, tính chất tương tác của các ứng dụng này đòi hỏi độ trễ thấp cho suy luận LLM. Ví dụ, người dùng mong đợi đầu vào của họ đến ChatGPT được phản hồi ngay lập tức. Tuy nhiên, kích thước và độ phức tạp của LLMs gây áp lực to lớn lên cơ sở hạ tầng phục vụ suy luận bên dưới. Các doanh nghiệp cung cấp các cụm khổng lồ và đắt tiền bao gồm các bộ tăng tốc như GPUs và TPUs để xử lý các công việc suy luận LLM.

Suy luận LLM có những đặc điểm riêng biệt (§2) khác với suy luận mô hình mạng nơ-ron sâu (DNN) khác như ResNet [7]. Các công việc suy luận DNN thường là xác định và có thể dự đoán cao [8], tức là thời gian thực thi của một công việc suy luận chủ yếu được quyết định bởi mô hình và phần cứng. Ví dụ, các hình ảnh đầu vào khác nhau có thời gian thực thi tương tự trên cùng một mô hình ResNet trên một GPU cho trước. Ngược lại, các công việc suy luận LLM có một mẫu tự hồi quy đặc biệt. Một công việc suy luận LLM chứa nhiều lần lặp. Mỗi lần lặp tạo ra một token đầu ra, và mỗi token đầu ra được thêm vào đầu vào để tạo ra token đầu ra tiếp theo trong lần lặp tiếp theo. Thời gian thực thi phụ thuộc vào cả độ dài đầu vào và độ dài đầu ra, trong đó cái sau không được biết trước.

Các giải pháp phục vụ suy luận hiện tại như Clockwork [8] và Shepherd [9] nhắm đến suy luận mô hình xác định như ResNet [7]. Chúng dựa vào việc profiling thời gian thực thi chính xác để lập lịch công việc, điều này không hoạt động đối với suy luận LLM có thời gian thực thi biến đổi. Orca [10], được thiết kế cho suy luận LLM, giới thiệu lập lịch cấp độ lần lặp mà động thêm các công việc mới hoặc loại bỏ những công việc đã hoàn thành ở cuối mỗi lần lặp. vLLM [11] tiếp tục giới thiệu PagedAttention để giảm phân mảnh bộ nhớ của trạng thái trung gian của các công việc suy luận LLM. Tuy nhiên, cả hai đều sử dụng first-come-first-served (FCFS) để xử lý các công việc suy luận. Một khi một công việc được lập lịch, nó chạy cho đến khi hoàn thành. Do bộ nhớ GPU hạn chế và yêu cầu độ trễ nghiêm ngặt, batch xử lý hiện tại không thể được mở rộng với số lượng tùy ý các công việc đến, do đó một công việc dài có thể chặn những công việc đến, được gọi là head-of-line blocking [12]. Vấn đề đặc biệt nghiêm trọng đối với các công việc suy luận LLM. Một công việc suy luận LLM lớn, tức là với độ dài đầu vào và đầu ra dài, sẽ chạy trong thời gian dài để chặn các công việc ngắn đến.

Hình 1 minh họa vấn đề dựa trên các bộ dữ liệu thực tế. Thiết lập chi tiết trong §6. Lý tưởng, nếu độ dài đầu vào và độ dài đầu ra của các công việc đều giống nhau giữa các công việc, hầu như không có độ trễ xếp hàng ngay cả khi tải đạt đến khả năng (load≈1), như hai cột đầu trong Hình 1 gợi ý. Tuy nhiên, các bộ dữ liệu LLM như ShareGPT [13] và Alpaca [14] cho thấy các khối lượng công việc thực tế rất lệch. Phân phối đuôi dài của độ dài đầu ra dẫn đến độ trễ xếp hàng dài. Hình 1 cho thấy lên đến 90% tổng độ trễ là độ trễ xếp hàng cho các bộ dữ liệu thực tế. Trong trường hợp này, tối ưu hóa thời gian thực thi là không đủ, vì nó chỉ đóng góp một phần nhỏ của độ trễ đầu-cuối. Thay vào đó, chúng ta cần tối ưu hóa độ trễ xếp hàng, đây là yếu tố đóng góp chính cho độ trễ đầu-cuối.

Chúng tôi giới thiệu FastServe, một hệ thống phục vụ suy luận phân tán cho LLMs. FastServe khai thác mẫu tự hồi quy của suy luận LLM và lập lịch cấp độ lần lặp để cho phép preemption ở mức độ chi tiết của mỗi token đầu ra. Cụ thể, khi một công việc được lập lịch hoàn thành việc tạo ra một token đầu ra, FastServe có thể quyết định tiếp tục công việc này hay preempt nó với một công việc khác trong hàng đợi. Điều này cho phép FastServe sử dụng lập lịch preemptive để loại bỏ vấn đề head-of-line blocking và tối thiểu hóa độ trễ.

Cốt lõi của FastServe là một bộ lập lịch skip-join Multi-Level Feedback Queue (MLFQ) mới. MLFQ là một phương pháp cổ điển để tối thiểu hóa độ trễ trong các thiết lập bất khả tri thông tin [15]. Mỗi công việc đầu tiên vào hàng đợi ưu tiên cao nhất, và được hạ cấp xuống hàng đợi ưu tiên tiếp theo nếu nó không hoàn thành sau một ngưỡng. Sự khác biệt chính giữa suy luận LLM và thiết lập cổ điển là suy luận LLM là bán bất khả tri thông tin, tức là trong khi độ dài đầu ra không được biết trước, độ dài đầu vào được biết. Do mẫu tự hồi quy của suy luận LLM, độ dài đầu vào quyết định thời gian thực thi để tạo ra token đầu ra đầu tiên, có thể lớn hơn đáng kể so với những token sau đó (§4.1). Đối với một đầu vào dài và một đầu ra ngắn, thời gian thực thi của token đầu ra đầu tiên chiếm ưu thế toàn bộ công việc. Chúng tôi tận dụng đặc điểm này để mở rộng MLFQ cổ điển với skip-join. Thay vì luôn vào hàng đợi ưu tiên cao nhất, mỗi công việc đến tham gia một hàng đợi thích hợp bằng cách so sánh thời gian thực thi của token đầu ra đầu tiên với quantum của các hàng đợi. Các hàng đợi ưu tiên cao hơn được bỏ qua để giảm hạ cấp.

Lập lịch preemptive giới thiệu overhead bộ nhớ bổ sung để duy trì trạng thái trung gian cho các công việc đã bắt đầu nhưng chưa hoàn thành. LLMs duy trì một cache key-value cho mỗi lớp Transformer để lưu trữ trạng thái trung gian (§2.2). Trong FCFS, cache chỉ cần lưu trữ trạng thái trung gian của các công việc được lập lịch trong batch xử lý, bị giới hạn bởi kích thước batch tối đa. Nhưng trong MLFQ, nhiều công việc có thể đã bắt đầu nhưng được hạ cấp xuống các hàng đợi ưu tiên thấp hơn. Cache phải duy trì trạng thái trung gian cho tất cả các công việc đã bắt đầu nhưng chưa hoàn thành trong MLFQ. Cache có thể tràn, do kích thước lớn của trạng thái trung gian và khả năng bộ nhớ hạn chế của GPUs. Một cách ngây thơ, bộ lập lịch có thể tạm dừng bắt đầu các công việc mới khi cache đầy, nhưng điều này lại giới thiệu head-of-line blocking. Thay vào đó, chúng tôi thiết kế cơ chế quản lý bộ nhớ GPU chủ động mà chủ động offload trạng thái của các công việc trong các hàng đợi ưu tiên thấp xuống bộ nhớ host khi cache gần đầy, và upload trạng thái trở lại khi những công việc này sẽ được lập lịch. Chúng tôi sử dụng pipelining và các hoạt động bộ nhớ bất đồng bộ để cải thiện hiệu quả.

Đối với các mô hình lớn không vừa trong một GPU, FastServe tận dụng các chiến lược song song hóa bao gồm tensor parallelism [16] và pipeline parallelism [17] để thực hiện phục vụ suy luận phân tán với nhiều GPUs (§4.3). Bộ lập lịch chạy nhiều batch công việc đồng thời trong một pipeline để tối thiểu hóa các bong bóng pipeline. Trình quản lý cache key-value phân vùng cache key-value trên nhiều GPUs, và xử lý hoán đổi giữa bộ nhớ GPU và bộ nhớ host theo cách phân tán.

Chúng tôi triển khai một nguyên mẫu hệ thống của FastServe và tích hợp nhiều kỹ thuật tối ưu hóa như PagedAttention [11]. Chúng tôi đánh giá FastServe trên các cấu hình khác nhau của LLMs với các khối lượng công việc suy luận LLM thực tế. Cụ thể, chúng tôi đánh giá hiệu suất đầu-cuối của FastServe cho OPT-175B [18] (một LLM mã nguồn mở tương tự mô hình GPT-3 lớn nhất) trên 16 NVIDIA A100 GPUs. Các thực nghiệm cho thấy so với giải pháp tối tân vLLM [11], FastServe cải thiện thông lượng lên đến 31.4× và 17.9× dưới cùng yêu cầu độ trễ trung bình và tail, tương ứng.

## 2 Bối cảnh và Động lực

### 2.1 Suy luận LLM và Ứng dụng

**Suy luận LLM.** Họ LLM [18–20] bao gồm một tập hợp các mô hình ngôn ngữ được xây dựng trên nền tảng của Transformer [21]. Suy luận LLM hoạt động theo cách tự hồi quy, nơi đầu vào, thường được gọi là prompt, được xử lý như một chuỗi các token. Sau đó nó tạo ra một phân phối xác suất cho token tiếp theo được chọn. Cơ chế xử lý và lựa chọn này cho mỗi token đầu ra được gọi là một lần lặp. Một khi được huấn luyện với một kho ngữ liệu khổng lồ, LLM có khả năng thực hiện các nhiệm vụ ngôn ngữ chất lượng cao.

Ví dụ, cho đầu vào "knowledge is", nó có thể gán xác suất cao hơn cho "power" so với "apple". Token đầu ra đầu tiên được thêm vào prompt ban đầu và đưa vào LLM để tạo token tiếp theo. Quá trình này tiếp tục cho đến khi một token <EOS> duy nhất, tượng trưng cho kết thúc của chuỗi, được tạo ra, hoặc một độ dài đầu ra tối đa được định trước đạt đến. Quá trình suy luận này khác biệt rõ rệt so với những mô hình khác như ResNet, nơi thời gian thực thi thường là xác định và có thể dự đoán [8]. Trong khi thời gian thực thi của mỗi lần lặp (tạo ra một token đầu ra) có thể được xác định dựa trên kiến trúc mô hình và phần cứng, tổng số lần lặp (tức là độ dài chuỗi đầu ra) vẫn không biết và khó dự đoán vì nó phụ thuộc vào ngữ nghĩa của công việc. Các bộ dữ liệu thực tế được thu thập từ các cuộc hội thoại với LLMs, như ShareGPT [13] và Alpaca [14], thể hiện phân phối đuôi dài của độ dài đầu ra và độ dài đầu vào [11]. Do đó, SRPT không thể được áp dụng trực tiếp cho suy luận LLM để tối thiểu hóa độ trễ trung bình.

**Ứng dụng LLM.** Nhiệm vụ chính của LLMs là dự đoán token tiếp theo cho một prompt đầu vào. Sử dụng kỹ thuật prompt engineering [19], các nhiệm vụ NLP downstream có thể được định dạng lại thành các nhiệm vụ tạo sinh dựa trên LLMs. Cụ thể, đối với một nhiệm vụ dịch thuật, người ta có thể bắt đầu prompt bằng cách thêm "Translate the following English text into French text" trước văn bản gốc. Bằng cách làm như vậy, LLM sẽ được hướng dẫn để tạo ra văn bản tiếng Pháp được dịch mong muốn để phản hồi.

ChatGPT [1] là một ứng dụng dựa trên LLM đại diện. Sau khi fine-tuning có giám sát cho nhiệm vụ hội thoại và một quy trình alignment sử dụng Reinforcement Learning from Human Feedback (RLHF) trên mô hình GPT gốc [19], ChatGPT tạo điều kiện cho các cuộc hội thoại tương tác với một đại lý AI, cho phép người dùng giải quyết một loạt rộng các nhiệm vụ. Những nhiệm vụ này bao gồm dịch thuật, hỏi-đáp, tóm tắt, cũng như các công việc phức tạp hơn như phân tích cảm xúc, viết sáng tạo, và giải quyết vấn đề theo lĩnh vực cụ thể. Mặc dù có khả năng đáng kể, tính chất tương tác của ChatGPT gây áp lực đáng kể lên cơ sở hạ tầng phục vụ suy luận bên dưới. Do nhu cầu phản hồi nhanh, việc giữ độ trễ thấp là rất quan trọng để đảm bảo hiệu suất của các ứng dụng tương tác giống ChatGPT.

### 2.2 Hệ thống Phục vụ Suy luận

Hầu hết các hệ thống phục vụ suy luận hiện tại, như Tensorflow Serving [22] và Triton Inference Server [23], đều bất khả tri đối với các mô hình DNN. Chúng phục vụ như một abstraction phía trên engine thực thi bên dưới, điều này xếp hàng các công việc đến, phân phối công việc đến các tài nguyên tính toán có sẵn, và trả về kết quả cho các client. Để tận dụng đầy đủ GPUs, chúng thường batch các công việc lại với nhau để xử lý song song. Với batching, các tensor đầu vào từ nhiều công việc được nối lại và đưa vào mô hình như một khối. Mặc dù sử dụng tốt hơn, nhược điểm của batching là overhead bộ nhớ cao hơn. Kích thước đáng kể của LLMs và trạng thái trung gian khổng lồ hạn chế kích thước batch tối đa cho suy luận LLM [11].

Khi sự phổ biến của LLMs tăng nhanh, các hệ thống phục vụ suy luận đã phát triển để bao gồm các tối ưu hóa cụ thể cho kiến trúc độc đáo và mẫu tạo sinh lặp đi lặp lại của LLMs. Phần chính của kiến trúc LLM là một ngăn xếp các lớp Transformer, như được hiển thị trong Hình 2. Trong một lớp Transformer, module Masked Self-Attention là thành phần cốt lõi phân biệt nó với các kiến trúc khác như CNNs. Trong mỗi lần lặp của suy luận LLM, đối với mỗi token, toán tử attention yêu cầu các keys và values của các token trước đó. Một triển khai ngây thơ, stateless luôn tính lại tất cả các keys và values trước đó trong mỗi lần lặp. Để tránh overhead tính toán lại như vậy, fairseq [24] đề xuất lưu các keys và values trong một cache key-value qua các lần lặp. Tối ưu hóa này chia quy trình suy luận thành hai giai đoạn riêng biệt: giai đoạn khởi tạo và giai đoạn giải mã. Hình 3 minh họa việc sử dụng cache key-value trong cả hai giai đoạn. Trong giai đoạn khởi tạo, tương ứng với lần lặp đầu tiên, LLM tạo ra cache key-value cho mỗi token trong prompt đầu vào. Trong giai đoạn giải mã tiếp theo, LLM chỉ cần tính toán query, key, và value của một token mới được tạo ra, tận dụng cache key-value được tính toán trước để tạo điều kiện cho quá trình từng bước. Do đó, thời gian thực thi của các lần lặp trong giai đoạn giải mã thường nhỏ hơn so với giai đoạn khởi tạo, tức là lần lặp đầu tiên. Đáng chú ý là các hệ thống dựa trên Transformer khác, như HuggingFace [25] và FasterTransformer [26], cũng kết hợp kỹ thuật tối ưu hóa này, dẫn đến hiệu quả được cải thiện trong quá trình suy luận.

Một tối ưu hóa quan trọng khác là lập lịch cấp độ lần lặp được đề xuất bởi Orca [10]. Lập lịch cấp độ công việc ngây thơ thực thi một batch các công việc cho đến khi tất cả công việc hoàn thành. Các công việc hoàn thành sớm không thể được trả về cho client ngay lập tức, trong khi các công việc mới đến phải đợi cho đến khi batch đang thực hiện hoàn thành xử lý. Tuy nhiên, với lập lịch cấp độ lần lặp, engine thực thi chỉ thực hiện một lần lặp duy nhất trên batch tại một thời điểm, tạo ra một token đầu ra cho mỗi công việc. Sau mỗi lần lặp, các công việc đã hoàn thành rời khỏi batch, và các công việc mới đến có thể tham gia. Tuy nhiên, khả năng bộ nhớ GPU giới hạn kích thước batch tối đa, và các service-level-objects (SLOs) nghiêm ngặt của các ứng dụng tương tác cũng đóng vai trò trong việc xác định kích thước batch thích hợp. vLLM [11] tiếp tục cải thiện hiệu quả của suy luận LLM bằng cách giới thiệu PagedAttention, điều này phân bổ cache key-value dần dần theo khối trong quá trình suy luận thay vì phân bổ cho độ dài đầu ra tối đa ngay từ đầu.

### 2.3 Cơ hội và Thách thức

**Cơ hội: lập lịch preemptive.** Hạn chế chính của các hệ thống phục vụ suy luận hiện tại cho LLMs [10, 26] là sự phụ thuộc của chúng vào lập lịch FCFS (First-Come-First-Serve) đơn giản và thực thi chạy-đến-hoàn-thành. Như được hiển thị trong Hình 1, phương pháp này dẫn đến head-of-line blocking nghiêm trọng. Độ trễ xếp hàng đóng góp lên đến 90% tổng độ trễ trong khối lượng công việc thực, điều này ảnh hưởng đáng kể đến hiệu suất của suy luận LLM. Để vượt qua thách thức này, lập lịch preemptive có thể được sử dụng. Trong suy luận LLM, mỗi công việc bao gồm nhiều lần lặp, với mỗi lần lặp tạo ra một token đầu ra. Cơ hội nằm ở việc khai thác mẫu tự hồi quy này để cho phép preemption ở cấp độ lần lặp, có nghĩa là một công việc có thể được preempt khi nó hoàn thành việc tạo ra một token đầu ra cho một công việc khác. Tận dụng khả năng preemption, bộ lập lịch có thể sử dụng các chính sách lập lịch preemptive để ngăn chặn head-of-line blocking và tối ưu hóa độ trễ trung bình. Tuy nhiên, lập lịch preemptive đặt ra hai thách thức cho hệ thống suy luận LLM hiện tại.

**Thách thức 1: kích thước công việc biến đổi.** Shortest Remaining Processing Time (SRPT) [27] là một chính sách lập lịch preemptive được sử dụng rộng rãi để tối thiểu hóa độ trễ trung bình. Tuy nhiên, việc áp dụng SRPT cho suy luận LLM đặt ra những thách thức do tính chất lặp đi lặp lại của LLMs. Không giống như các nhiệm vụ dự đoán một lần như phân loại hình ảnh, suy luận LLM bao gồm nhiều lần lặp. Trong khi thời gian thực thi cho một lần lặp (tạo ra một token đầu ra) có thể được xác định dựa trên kiến trúc mô hình và phần cứng, tổng số lần lặp (tức là độ dài chuỗi đầu ra) vẫn không biết và khó dự đoán vì nó phụ thuộc vào ngữ nghĩa của công việc. Các bộ dữ liệu thực tế được thu thập từ các cuộc hội thoại với LLMs, như ShareGPT [13] và Alpaca [14], thể hiện phân phối đuôi dài của độ dài đầu ra và độ dài đầu vào [11]. Do đó, SRPT không thể được áp dụng trực tiếp cho suy luận LLM để tối thiểu hóa độ trễ trung bình.

**Thách thức 2: overhead bộ nhớ GPU.** Các chính sách lập lịch preemptive giới thiệu tiêu thụ bộ nhớ GPU bổ sung trong quá trình suy luận LLM, không giống như FCFS với chạy-đến-hoàn-thành, chỉ cần duy trì cache key-value cho các công việc đang tiếp diễn. Ngược lại, lập lịch preemptive phải giữ cache key-value trong bộ nhớ GPU cho tất cả các công việc preempted ở trạng thái chờ, để được sử dụng cho việc tạo token trong tương lai. Cache key-value này tiêu thụ một lượng lớn bộ nhớ GPU, dẫn đến những thách thức tiềm tàng. Ví dụ, một công việc duy nhất của OPT 175B với độ dài chuỗi đầu vào 512 yêu cầu ít nhất 2.3 GB bộ nhớ cho cache key-value (§4.2). Do khả năng bộ nhớ GPU khan hiếm, kích thước của cache key-value trở thành một yếu tố quan trọng ảnh hưởng đến hiệu quả của các chính sách lập lịch preemptive. Các nghiên cứu trước đã đề xuất các kỹ thuật tiết kiệm bộ nhớ cho cache key-value. Multi-Query Attention [28] và Group-Query Attention [29] cố gắng giảm tiêu thụ bộ nhớ bằng cách chia sẻ các tensor key-value giữa các attention head. Chúng có thể làm tổn hại khả năng của LLMs và tiêu thụ bộ nhớ vẫn tăng tuyến tính theo độ dài chuỗi. vLLM [11] quản lý cache key-value ở mức độ chi tiết các khối để giảm phân mảnh bộ nhớ GPU. Nó không thể giảm việc sử dụng bộ nhớ gây ra bởi chính cache key-value. Khi độ dài context tăng [30], tiêu thụ bộ nhớ của cache key-value là một vấn đề khó và đang trở nên ngày càng quan trọng.

## 3 Tổng quan FastServe

### 3.1 Các Thuộc tính Mong muốn

LLMs đi kèm với những đặc điểm độc đáo gây ra thách thức cho tính toán phân tán và tiêu thụ bộ nhớ GPU. Mục tiêu của chúng tôi là phát triển một hệ thống phục vụ suy luận hiệu quả cho LLMs đáp ứng ba yêu cầu sau.

• **Độ trễ thấp và thông lượng cao.** Trọng tâm của chúng tôi tập trung vào các ứng dụng LLM tương tác, nơi người dùng có kỳ vọng cao về phản hồi nhanh. Để đo lường nó một cách định lượng, chúng tôi muốn thông lượng tối đa càng cao càng tốt dưới một yêu cầu độ trễ nhất định.

• **Quản lý bộ nhớ GPU hiệu quả.** LLMs đặt ra một thách thức đáng kể về mặt tiêu thụ bộ nhớ GPU, điều này đòi hỏi một phương pháp quản lý bộ nhớ GPU hiệu quả cho cả mô hình và các trạng thái trung gian.

• **Thực thi phân tán có thể mở rộng.** Bản chất của LLMs đòi hỏi nhiều GPUs để cho phép suy luận phân tán hiệu quả, điều này yêu cầu hệ thống hỗ trợ thực thi phân tán có thể mở rộng trên các máy chủ GPU.

### 3.2 Kiến trúc Tổng thể

Hình 4 hiển thị kiến trúc của FastServe. Các công việc được gửi đến job pool. Bộ lập lịch sử dụng thông tin từ job profiler để xác định ưu tiên công việc ban đầu và sau đó đặt công việc trong skip-join MLFQ (§4.1) để giảm thiểu head-of-line blocking.

Để thực thi, bộ lập lịch chọn các công việc dựa trên ưu tiên của chúng trong skip-join MLFQ để tạo thành một kích thước batch tối đa được định trước và phân phối batch đến engine thực thi phân tán để thực hiện một lần lặp. Engine thực thi phân tán hợp tác với cache key-value phân tán để truy cập và cập nhật các tensor key-value liên quan đến công việc tương ứng. Để giải quyết thách thức của khả năng bộ nhớ GPU hạn chế, trình quản lý cache key-value chủ động hoán đổi các tensor key-value giữa bộ nhớ GPU và bộ nhớ host (§4.2).

Để phù hợp với các mô hình cực lớn như OPT-175B, FastServe sử dụng suy luận phân tán, cho phép cả tensor parallelism và pipeline parallelism. FastServe kết hợp các mở rộng vào bộ lập lịch và cache key-value để cho phép hỗ trợ liền mạch cho thực thi phân tán (§4.3).

## 4 Thiết kế FastServe

Trong phần này, đầu tiên chúng tôi giới thiệu bộ lập lịch skip-join MLFQ để tối thiểu hóa độ trễ (§4.1). Sau đó, chúng tôi trình bày một cơ chế quản lý cache KV chủ động được thiết kế để giảm thiểu hiệu quả ràng buộc khả năng bộ nhớ GPU (§4.2). Cuối cùng, chúng tôi minh họa cách áp dụng những kỹ thuật này vào các thiết lập phân tán (§4.3).

### 4.1 Bộ lập lịch Skip-Join MLFQ

**Strawman: lập lịch ưu tiên cố định.** Để hỗ trợ lập lịch preemptive, chúng ta cần một bộ lập lịch dựa trên ưu tiên để quyết định công việc nào sẽ preempt và công việc nào sẽ thực thi. Một giải pháp ngây thơ là gán một ưu tiên cố định cho mỗi công việc dựa trên độ dài đầu vào của nó. Trong trường hợp này, khi giai đoạn khởi tạo chiếm ưu thế tổng độ trễ, lập lịch ưu tiên cố định có thể xấp xỉ hiệu suất tối ưu như chính sách SRPT làm. Tuy nhiên, mặc dù giải pháp này tận dụng thông tin về giai đoạn khởi tạo, nó bỏ qua các đặc điểm của giai đoạn giải mã. Nhiều bộ dữ liệu thực tế như ShareGPT và Alpaca cho thấy phân phối đuôi dài ngụ ý rằng các công việc với độ dài đầu ra dài cũng tồn tại. Khi giai đoạn giải mã chiếm ưu thế tổng độ trễ, lập lịch ưu tiên cố định có thể lệch khỏi hiệu suất tối ưu của SRPT.

**Strawman: MLFQ ngây thơ.** Do kích thước công việc không xác định của suy luận LLM, việc áp dụng trực tiếp SRPT là không khả thi. Trong các thiết lập bất khả tri thông tin, Least-Attained Service (LAS) đã được chứng minh là xấp xỉ SRPT hiệu quả. Do overhead chuyển đổi công việc của LAS, phương pháp thực tế là Multi-Level Feedback Queue (MLFQ) đã trở nên phổ biến trong các hệ thống lập lịch khác nhau [15, 31–34]. MLFQ vận hành nhiều hàng đợi, mỗi hàng có một mức ưu tiên khác nhau. Khi đến, một công việc vào hàng đợi ưu tiên cao nhất và bị hạ cấp xuống hàng đợi cấp độ tiếp theo nếu thời gian thực thi của nó vượt quá một quantum. Giá trị của quantum là một tham số có thể điều chỉnh được gán cho mỗi hàng đợi, ví dụ, các hàng đợi ưu tiên cao hơn thường có giá trị quantum ngắn hơn.

Mặc dù MLFQ không giả định kiến thức trước về kích thước công việc, nó không phù hợp với phục vụ LLM. Hình 5 hiển thị thời gian lần lặp của OPT 2.7B trên một NVIDIA A100, thay đổi độ dài chuỗi đầu vào. Đáng chú ý, thời gian giai đoạn khởi tạo (tức là lần lặp đầu tiên) vượt quá thời gian giai đoạn giải mã. Khi độ dài chuỗi đầu vào tăng, thời gian giai đoạn khởi tạo cũng tăng. Hành vi này có thể được quy cho tối ưu hóa cache key-value (§2.2). Trong lần lặp đầu tiên, tính toán cho tất cả các tensor key-value của các token đầu vào được thực hiện và được cache. Trong các lần lặp tiếp theo, chỉ các tensor key-value của một token mới được tạo ra được tính toán, và phần còn lại được truy xuất từ cache.

Khi sử dụng MLFQ gốc, một công việc được gán ngay lập tức vào hàng đợi ưu tiên cao nhất khi đến. Tuy nhiên, do thời gian giai đoạn khởi tạo đáng kể, công việc có thể cạn kiệt quantum của nó trước khi hoàn thành lần lặp đầu tiên. Tình huống này đặt ra một khó khăn lập lịch. Nếu bộ lập lịch preempt công việc, các activation trung gian bị loại bỏ và tính toán lại sau, dẫn đến lãng phí tài nguyên tính toán và thời gian quý giá. Mặt khác, nếu bộ lập lịch chọn không preempt công việc, nó vi phạm mục đích thiết kế cơ bản của MLFQ và có thể gặp phải head-of-line blocking một lần nữa.

**Giải pháp của chúng tôi: skip-join MLFQ.** Insight chính của thiết kế chúng tôi là tận dụng thiết lập bán bất khả tri thông tin của suy luận LLM để giải quyết các vấn đề đã nêu của các giải pháp strawman. Trong khi số lượng lần lặp (tức là độ dài đầu ra) vẫn không biết trước, thời gian thực thi của mỗi lần lặp là có thể dự đoán. Đối với mỗi lần lặp, việc thực thi tương tự như suy luận DNN một lần truyền thống, có thời gian thực thi rất có thể dự đoán [8, 35]. Một quá trình profiling nhẹ có thể dễ dàng thu thập thời gian lần lặp chính xác dưới các phần cứng, đặc tả mô hình và độ dài đầu vào khác nhau trước. Xem xét lại Hình 5, thời gian giai đoạn khởi tạo thể hiện mối tương quan tích cực với độ dài đầu vào khi cố định phần cứng và mô hình. Đối với giai đoạn giải mã, thời gian lần lặp gần như không đổi.

Dựa trên insight này, chúng tôi đề xuất một bộ lập lịch skip-join MLFQ được thiết kế riêng cho suy luận LLM. Bộ lập lịch của chúng tôi quản lý hiệu quả việc di chuyển các công việc giữa các hàng đợi ưu tiên khác nhau theo cách skip-join. Trong MLFQ, chúng ta có n hàng đợi ưu tiên, cụ thể là Q1, Q2, ..., Qn, mỗi hàng có một quantum riêng biệt q1 < q2 < ... < qn. Bộ lập lịch MLFQ thông thường ban đầu gán một công việc mới đến cho hàng đợi ưu tiên cao nhất, tức là Q1. Một khi công việc cạn kiệt quantum được phân bổ trong Q1, nó sau đó được hạ cấp xuống Q2. Như được hiển thị trong Hình 6, FastServe khác với MLFQ gốc ở chỗ khi một công việc đến, FastServe tận dụng profiling chính xác để dự đoán thời gian giai đoạn khởi tạo (tinit) và ❶skip-joins công việc đến hàng đợi ưu tiên cao nhất (qi) sao cho qi ≥ tinit. Khi một công việc tiêu thụ quantum được phân bổ trước khi hoàn thành, bộ lập lịch hạ cấp ❷ công việc dựa trên ưu tiên hiện tại và thời gian lần lặp tiếp theo.

**Tránh đói perpetual.** Điều quan trọng cần lưu ý là các hoạt động skip-join và demotion có thể dẫn đến starvation cho các công việc có đầu vào và đầu ra dài. Để giải quyết vấn đề này, bộ lập lịch định kỳ kiểm tra các công việc bị starved và ❸promotes chúng lên hàng đợi ưu tiên cao nhất, tức là Q1. Điều này cho phép FastServe giải quyết head-of-line blocking trong khi giảm thiểu starvation. Chúng tôi đánh giá tail latency và hiển thị hiệu quả của cơ chế ngăn chặn starvation trong §6.2.

**Ví dụ.** Hình 7 hiển thị một ví dụ để minh họa hiệu quả của bộ lập lịch skip-join MLFQ của FastServe. Trong ví dụ, ba công việc đến cùng lúc theo thứ tự J1, J2, J3. T1(Ji) biểu thị thời gian giai đoạn khởi tạo của công việc Ji, và T2(Ji) biểu thị thời gian giai đoạn giải mã. Chúng tôi giả định rằng cả skip-join và MLFQ gốc sử dụng bốn hàng đợi ưu tiên với giá trị quantum 1, 2, 4, và 8. Ngoài ra, SRPT phục vụ như oracle với độ trễ trung bình tối ưu. Như Hình 7 hiển thị, độ trễ trung bình của FCFS, MLFQ gốc, skip-join MLFQ, và SRPT lần lượt là 4.23, 5, 3.3, và 3. FCFS và MLFQ gốc gặp phải vấn đề head-of-line blocking, nơi công việc J1 chặn các công việc còn lại, dẫn đến độ trễ trung bình dài. Skip-join MLFQ giải quyết vấn đề này bằng cách skip-joining công việc J1 đến hàng đợi ưu tiên thấp, đạt được hiệu suất tương tự SRPT tối ưu. Nói chung, các thuật toán có quyền truy cập vào nhiều thông tin hơn hoạt động tốt hơn những thuật toán có thông tin hạn chế.

**Thuật toán.** Thuật toán 1 hiển thị pseudo-code của bộ lập lịch skip-join MLFQ. Bộ lập lịch có một tập hợp các hàng đợi ưu tiên Q1, Q2, ..., Qn với giá trị quantum q1, q2, ..., qn, và nhận một tập hợp các công việc mới đến Jnew. Nó lập lịch một batch MaxBatchSize công việc để thực thi. Phần skip-join (❶ trong Hình 6) tương ứng với dòng 6–9, và các phần demotion và starvation prevention (❷ và ❸ trong Hình 6) tương ứng với dòng 16–17 và dòng 19–21, tương ứng. Có hai chi tiết đáng chú ý. Đầu tiên, bộ lập lịch hạ cấp một công việc xuống hàng đợi ưu tiên thấp hơn η lần dựa trên thời gian lần lặp tiếp theo của nó. FastServe đặt quantum của hàng đợi ưu tiên thấp hơn bằng hai lần quantum của hàng đợi ưu tiên cao hơn, điều này phù hợp với nghiên cứu trước [34] về MLFQ. Quantum của hàng đợi ưu tiên cao nhất được đặt thành thời gian lần lặp tối thiểu. Chi tiết thứ hai liên quan đến cách bộ lập lịch xác định các công việc bị starved được điều chỉnh bởi tham số α. FastServe điều chỉnh α dựa trên SLO được người dùng chỉ định, được đặt thành 300 ms theo mặc định.

### 4.2 Quản lý Cache Key-Value Chủ động

Mặc dù bộ lập lịch skip-join MLFQ cung cấp preemption cấp độ lần lặp để xấp xỉ SRPT để đạt được độ trễ thấp hơn mà không cần kiến thức trước về kích thước công việc chính xác, nó làm trầm trọng thêm áp lực tiêu thụ bộ nhớ GPU. Hình 8 hiển thị tiêu thụ bộ nhớ cache key-value của FCFS và skip-join MLFQ cho mô hình OPT 2.7B dưới một khối lượng công việc tổng hợp. Mặc dù chúng tôi chọn một mô hình tương đối nhỏ và giới hạn độ dài đầu ra tối đa là 20, overhead bộ nhớ cache KV đỉnh cho skip-join MLFQ có thể lớn hơn 7× so với FCFS. Nhu cầu bộ nhớ GPU trở nên càng rõ rệt hơn khi triển khai các LLMs lớn hơn như OPT 175B.

Lý do bên dưới là so với chính sách chạy-đến-hoàn-thành trong các hệ thống phục vụ hiện tại, preemption cấp độ lần lặp được cung cấp bởi skip-join MFLQ tăng số lượng công việc đang diễn ra trong hệ thống. Ngoại trừ các tensor key-value của các công việc đang chạy, bộ lập lịch skip-join cũng cần lưu trữ các tensor key-value cho các công việc preempted ở trạng thái chờ. Không giống như các trạng thái process trong các bộ lập lịch hệ điều hành truyền thống, trạng thái trung gian, tức là các tensor key-value, của mỗi công việc lớn hơn nhiều. Một cách chính thức, đối với một công việc phục vụ suy luận LLM cụ thể, ký hiệu độ dài chuỗi đầu vào bằng s, độ dài chuỗi đầu ra bằng t, chiều ẩn của transformer bằng h, và số lượng lớp transformer bằng l. Nếu trọng số mô hình và tất cả tính toán đều ở FP16, tổng số byte để lưu trữ cache key-value cho công việc đơn này là 4 × lh(s + t). Lấy OPT 175B làm ví dụ (l = 96, h = 12288). Cho độ dài chuỗi đầu vào s = 512 và độ dài chuỗi đầu ra tối thiểu t = 1, overhead bộ nhớ GPU cho một công việc đơn cao tới 2.3GB. Khi việc tạo sinh tiếp tục, độ dài chuỗi đầu ra t sẽ tăng, điều này càng tăng overhead bộ nhớ GPU.

Đồng thời, bộ nhớ GPU là một tài nguyên khan hiếm khi triển khai LLMs. Thông thường, bộ nhớ GPU nhỏ hơn nhiều so với bộ nhớ host. Ví dụ, NVIDIA A100 GPU có tối đa 80 GB bộ nhớ GPU. Bên cạnh đó, một phần lớn bộ nhớ GPU được cung cấp để lưu trữ trọng số của LLMs. Không gian để lưu trữ các tensor key-value cho các công việc bị hạn chế. Kết quả là, khả năng bộ nhớ GPU hạn chế các lợi ích tiềm tàng của bộ lập lịch skip-join MLFQ.

**Giải pháp strawman 1: trì hoãn các công việc mới đến.** Để tránh lỗi out-of-memory (OOM), một giải pháp ngây thơ là đơn giản trì hoãn việc thực thi các công việc mới đến khi bộ nhớ GPU không đủ và tiếp tục lập lịch các công việc in-memory hiện tại cho đến khi chúng hoàn thành. Giải pháp đơn giản này được sử dụng rộng rãi trong các hệ thống phục vụ hiện tại, như vLLM [11]. Theo cách này, mặc dù các công việc mới được gán ưu tiên cao hơn, chúng bị chặn để chờ không gian bộ nhớ trống. Dưới các thiết lập bị hạn chế bộ nhớ GPU nghiêm trọng (ví dụ, suy luận chuỗi dài), giải pháp này sẽ suy thoái MLFQ thành FCFS, điều này lại gặp phải head-of-line blocking.

**Giải pháp strawman 2: kill và tính toán lại các công việc ưu tiên thấp.** Một giải pháp đơn giản khác là kill một số công việc ưu tiên thấp và giải phóng cache key-value của chúng để tạo chỗ cho các công việc ưu tiên cao mới đến. Giải pháp này có hai vấn đề. Đầu tiên, các công việc bị killed mất trạng thái tạo sinh của chúng, đòi hỏi phải xây dựng lại các tensor key-value của chúng. Điều này dẫn đến lãng phí tài nguyên tính toán và thời gian quý giá. Thứ hai, nó có thể gây ra deadlock. Khi các công việc ưu tiên cao đến, các công việc đang diễn ra với ưu tiên thấp hơn bị killed. Do việc tránh starvation, các công việc bị killed có thể được promoted lên hàng đợi ưu tiên cao nhất nếu chúng chờ lâu hơn STARVE_LIMIT được chỉ định. Trong trường hợp này, công việc được promoted có thể kill công việc đang thực thi hiện tại, điều này có thể vừa kill công việc được promoted trong bước trước đó. Nó có thể dẫn đến deadlock.

**Giải pháp của chúng tôi: hoán đổi cache key-value chủ động.** Dưới các ràng buộc khả năng bộ nhớ GPU nghiêm ngặt, hai giải pháp strawman phải hy sinh hiệu suất của các công việc mới đến hoặc hiệu quả của các công việc ưu tiên thấp. Để vượt qua khó khăn này, quan sát chính của chúng tôi là các tensor key-value chỉ cần được dự trữ trong bộ nhớ GPU khi các công việc tương ứng của chúng được lập lịch. Dựa trên quan sát này, FastServe mở rộng không gian của cache key-value từ bộ nhớ GPU đến bộ nhớ host. FastServe hoán đổi ra các tensor key-value không hoạt động của các công việc đến bộ nhớ host để chứa các công việc chờ bổ sung, và hoán đổi vào các tensor key-value trở lại bộ nhớ GPU cho các công việc sắp tới.

Tuy nhiên, overhead của hoán đổi không thể bỏ qua so với thời gian tạo token. Khi triển khai OPT 175B trên 16 NVIDIA A100 GPUs, các tensor key-value của một công việc có thể chiếm 2.3 GB bộ nhớ. Thời gian tạo token trong giai đoạn giải mã khoảng 60 ms, trong khi thời gian hoán đổi các tensor key-value giữa bộ nhớ host và bộ nhớ GPU với băng thông đầy đủ PCIe 4.0 ×16 khoảng 36 ms. Do đó, một cơ chế hoán đổi reactive đơn giản xử lý hoán đổi và suy luận tuần tự giới thiệu overhead lớn.

Thay vào đó, FastServe sử dụng thuật toán hoán đổi cache key-value chủ động để giảm thiểu các tác động bất lợi của overhead hoán đổi. Insight chính là overlap suy luận LLM cho các công việc đang chạy với truyền dữ liệu cho các công việc chờ để overhead hoán đổi ra khỏi đường dẫn quan trọng của suy luận LLM. Hình 9 minh họa một ví dụ. Thay vì hoán đổi các tensor key-value của công việc chờ J2 sau khi công việc J1 bị preempted hoặc hoàn thành, thuật toán chủ động hoán đổi các tensor key-value của J2 trước. Theo cách này, overhead hoán đổi của J2 hiệu quả overlap với việc thực thi kernel GPU của J1, từ đó đạt được sử dụng GPU cao. Khi hoán đổi vào một công việc tiêu thụ bộ nhớ GPU đắt tiền, thứ tự hoán đổi công việc rất quan trọng cho hiệu quả bộ nhớ GPU.

**Thứ tự hoán đổi công việc.** Việc hoán đổi vào và ra thường xuyên các tensor key-value không cần thiết gây ra overhead thrashing bổ sung nếu hoán đổi vào và ra một công việc với ưu tiên cao. Overhead hoán đổi có thể tăng vượt quá thời gian thực thi, dẫn đến suy giảm hiệu suất của overlapping. Để giải quyết vấn đề này, FastServe tính toán thời gian lập lịch tiếp theo ước tính (ENST) cho mỗi công việc để quyết định thứ tự hoán đổi. ENST là thời gian khi công việc sẽ được lập lịch thực thi lần tiếp theo. Công việc với ENST lớn nhất sẽ được hoán đổi ra trước, và công việc với ENST nhỏ nhất sẽ được hoán đổi vào trước. Thông thường, một công việc với ưu tiên thấp hơn được lập lịch để thực thi sau. Tuy nhiên, do cơ chế ngăn chặn starvation, một công việc ưu tiên thấp hơn có thể được nâng lên hàng đợi ưu tiên cao hơn. Do đó, ngay cả một công việc ưu tiên thấp đôi khi có thể được thực thi trước.

Trong trường hợp này, đối với công việc i, FastServe xem xét thời gian để promote công việc này và tổng thời gian thực thi của tất cả các công việc có ưu tiên cao hơn trước khi thực thi i đồng thời. Một cách chính thức, để ngưỡng thời gian để promote công việc i là Tpromote(i). Đối với tổng thời gian thực thi của tất cả các công việc có ưu tiên cao hơn trước khi thực thi i, chúng tôi giả định những công việc đó không hoàn thành sớm hơn trước khi bị hạ cấp xuống hàng đợi ưu tiên của công việc i. Thời gian thực thi của công việc j với ưu tiên cao hơn có thể được tính toán như sau (tức là công việc j được hạ cấp từ j.priority đến i.priority từng cái một):

Texecute(i,j) = ∑[i.priority <k≤j.priority] qk

trong đó i.priority là ưu tiên của công việc i, và qk là quantum của hàng đợi ưu tiên với ưu tiên k. Dựa trên điều này, tổng thời gian thực thi của tất cả các công việc có ưu tiên cao hơn công việc i được định nghĩa là:

Texecute(i) = (1/B) ∑[i.priority <j.priority] Texecute(i,j)

trong đó B là kích thước batch tối đa của các công việc. Cuối cùng, xem xét cả promotion cho việc ngăn chặn starvation và việc thực thi các công việc ưu tiên cao hơn, ENST của công việc i được tính toán là:

ENST(i) = min(Tpromote(i), Texecute(i))

Định nghĩa ENST này phục vụ như một phương tiện để ước tính thời gian lập lịch dự kiến cho việc tạo sinh tiếp theo của công việc i. Do đó, sử dụng metric này để quyết định thứ tự hoán đổi làm cho các tensor key-value của các công việc hoạt động cư trú chủ yếu trong bộ nhớ GPU, và những tensor của các công việc không hoạt động có xu hướng cư trú trong bộ nhớ host hơn.

**Xử lý một burst các công việc mới.** Chiến lược hoán đổi cache key-value chủ động được thiết kế cho bộ lập lịch skip-join MLFQ. Trong các kịch bản mà một luồng đáng kể các công việc mới (với ưu tiên cao) xảy ra, hệ thống quản lý cache bị buộc phải evict các công việc một cách reactive, ảnh hưởng bất lợi đến hiệu suất của những công việc mới này. Để giảm thiểu điều này, FastServe dự trữ một số slot cache key-value nhàn rỗi đặc biệt cho các công việc mới, đảm bảo khả năng sẵn có ngay lập tức mà không cần hoán đổi công việc reactive. Phương pháp này đảm bảo hiệu suất của các công việc mới. Số lượng slot nhàn rỗi dựa trên các mẫu đến công việc lịch sử. Tần suất cao hơn của các burst công việc đòi hỏi số lượng slot dự trữ lớn hơn.

### 4.3 Hỗ trợ cho Phục vụ LLM Phân tán

Nghiên cứu trước đây cho thấy hiệu quả của LLMs thực nghiệm tuân theo luật scaling liên quan đến số lượng tham số mô hình [36]. Tuy nhiên, điều quan trọng cần lưu ý là việc sử dụng bộ nhớ của một LLM cũng thể hiện tỷ lệ thuận với số lượng tham số. Một ví dụ điển hình là OPT 175B, mà ngay cả khi được lưu trữ ở độ chính xác nửa, đòi hỏi 350GB bộ nhớ GPU đáng kinh ngạc chỉ để chứa trọng số của nó. Hơn nữa, bộ nhớ bổ sung được yêu cầu để xử lý các trạng thái trung gian trong runtime. Do đó, LLM thường cần được chia thành nhiều phần và phục vụ theo cách phân tán với nhiều GPUs.

Tensor parallelism [16,37] và pipeline parallelism [17,38] là hai kỹ thuật được sử dụng rộng rãi nhất cho phục vụ LLM phân tán. FastServe hỗ trợ hybrid của hai kỹ thuật song song này để phục vụ LLMs. Một LLM được cấu thành từ một loạt các toán tử trên các tensor đa chiều. Tensor parallelism chia mỗi toán tử trên nhiều thiết bị, với mỗi thiết bị thực thi một phần của tính toán song song. Overhead communication bổ sung được yêu cầu để chia đầu vào và thu thập đầu ra từ các GPUs khác nhau. Tensor parallelism tăng đáng kể cả tài nguyên tính toán và bộ nhớ có sẵn cho một công việc đơn, do đó giảm thời gian của mỗi lần lặp.

Pipeline parallelism chia toàn bộ các toán tử của một đồ thị tính toán LLM thành nhiều stage và thực thi chúng trên các GPUs khác nhau theo cách pipeline. Trong quá trình suy luận, mỗi stage tính toán một phần của toàn bộ đồ thị tính toán và truyền các kết quả trung gian đến stage tiếp theo song song. Pipeline parallelism yêu cầu overhead communication ít hơn so với tensor parallelism, đồng thời cũng cung cấp cho LLMs khả năng vượt qua ràng buộc bộ nhớ của một GPU riêng lẻ. Vì nhiều batch xử lý đang được xử lý đồng thời trong các stage khác nhau, FastServe cần xử lý nhiều batch trong engine phân tán cùng một lúc.

**Lập lịch công việc trong phục vụ phân tán.** Trong MLFQ truyền thống, nếu không có công việc mới đến, bộ lập lịch lập lịch công việc với ưu tiên cao nhất và thực thi nó cho đến khi hoàn thành hoặc bị hạ cấp. Tuy nhiên, với pipeline parallelism, bộ lập lịch lập lịch ở mức độ chi tiết của stage riêng lẻ. Một khi một công việc hoàn thành stage đầu tiên và truyền các kết quả trung gian đến stage tiếp theo, một điểm quyết định nảy sinh cho bộ lập lịch về công việc tiếp theo để bắt đầu. Trong trường hợp này, bộ lập lịch không thể theo MLFQ truyền thống mà tiếp tục lập lịch cùng một công việc cho đến khi hạ cấp, vì công việc vẫn đang tiến hành. Để bảo tồn ngữ nghĩa của MLFQ, FastServe vẫn giữ công việc đang chạy trong hàng đợi ưu tiên, nhưng lập lịch công việc có ưu tiên cao nhất ở trạng thái chờ. Do đó, các công việc sớm trong một hàng đợi có thể đẩy nhanh việc hoàn thành quantum của chúng.

**Quản lý cache key-value trong phục vụ phân tán.** Cho rằng cache key-value chiếm một phần lớn bộ nhớ GPU, cache key-value của FastServe cũng được phân vùng trên nhiều GPUs. Trong suy luận LLM, mỗi tensor key-value được sử dụng bởi cùng một stage của LLM. Do đó, FastServe phân vùng các tensor key-value như tensor parallelism yêu cầu, và gán mỗi tensor key-value cho GPU tương ứng để tất cả tính toán trên một GPU chỉ cần các tensor key-value cục bộ trên cùng một GPU.

Cơ chế hoán đổi cache key-value chủ động của FastServe cũng được phân tán. Vì các stage khác nhau của LLM xử lý các công việc khác nhau cùng một lúc, mỗi stage có thể offload hoặc upload các tensor key-value khác nhau một cách độc lập. Để giảm điều khiển dư thừa, trước khi xử lý kết quả trung gian được gửi từ stage trước đó, stage hiện tại thực hiện cùng hành động offloading hoặc uploading như stage trước đó làm. Việc truyền kết quả trung gian và hoán đổi cache key-value xảy ra song song, vì vậy overhead của hoán đổi cache key-value được giảm thêm. Như được hiển thị trong Hình 10, khi kết quả trung gian được gửi đến stage tiếp theo, stage tiếp theo nhận các hướng dẫn hoán đổi và có thể hoán đổi cache key-value cùng lúc nếu cần. Cơ chế hoán đổi cache key-value chỉ cần quyết định việc offloading hoặc uploading của stage đầu tiên. Khi sử dụng tensor parallelism chia stage đầu tiên thành nhiều chunk, một trình quản lý hoán đổi cache key-value tập trung hướng dẫn tất cả các chunk trong stage đầu tiên offload hoặc upload các tensor key-value thuộc sở hữu của cùng một công việc.

## 5 Triển khai

FastServe là một hệ thống phục vụ suy luận LLM phân tán với một frontend RESTful API, một bộ lập lịch, và một engine thực thi phân tán. Frontend và bộ lập lịch được triển khai với 2.9K dòng code Python. Engine thực thi phân tán được triển khai với 8.1K dòng code C++/CUDA. Frontend hỗ trợ giao diện tương thích API OpenAI nơi client có thể chỉ định các tham số sampling như độ dài đầu ra tối đa và temperature. Bộ lập lịch triển khai các chính sách skip-join MLFQ và hoán đổi chủ động. Engine thực thi phân tán sử dụng Ray [39] actor để triển khai các GPU worker thực thi suy luận LLM và quản lý cache key-value theo cách phân tán. Chúng tôi triển khai các LLMs mã nguồn mở phổ biến như OPT trong C++ để đạt được hiệu suất và khả năng mở rộng tốt hơn so với các triển khai Python phổ biến trong Huggingface [25]. Chúng tôi cũng triển khai các kernel CUDA tùy chỉnh để hỗ trợ lập lịch cấp độ lần lặp của Orca [10] và PagedAttention của vLLM [11].

## 6 Đánh giá

Trong phần này, trước tiên chúng tôi minh chứng các cải thiện hiệu suất đầu-cuối của FastServe so với các hệ thống phục vụ LLM tối tân. Sau đó, chúng tôi đánh giá các lựa chọn thiết kế của FastServe và hiển thị hiệu quả của mỗi thành phần.

### 6.1 Phương pháp

**Testbed.** Các thực nghiệm đầu-cuối (§6.2) sử dụng hai instance AWS EC2 p4d.24xlarge. Mỗi instance được cấu hình với tám NVIDIA A100 40GB GPUs được kết nối qua NVLink, 1152 GB bộ nhớ host, và PCIe 4.0 ×16. Do ngân sách hạn chế, các thực nghiệm cho các lựa chọn thiết kế (§6.3) sử dụng một NVIDIA A100 40GB GPU trong testbed riêng của chúng tôi để xác thực hiệu quả của mỗi thành phần.

**Mô hình LLM.** Chúng tôi chọn họ LLM đại diện, OPT [18], được sử dụng rộng rãi trong cả học thuật và công nghiệp. Chúng tôi chọn các kích thước mô hình phổ biến. Bảng 1 liệt kê các cấu hình mô hình. Chúng tôi sử dụng độ chính xác FP16 trong tất cả các thực nghiệm.

**Khối lượng công việc.** Tương tự như nghiên cứu trước về phục vụ LLM [11], chúng tôi tạo khối lượng công việc dựa trên các bộ dữ liệu ShareGPT [13] và Alpaca [14]. Những bộ dữ liệu này chứa các đầu vào và đầu ra thực tế của các dịch vụ LLM. Bộ dữ liệu ShareGPT được cấu thành từ các cuộc hội thoại được người dùng chia sẻ với ChatGPT [13]. Bộ dữ liệu Alpaca được tạo ra bởi GPT-3.5 với self-instruct [14]. Vì những bộ dữ liệu này không bao gồm thời gian đến, chúng tôi theo nghiên cứu trước [11] để tạo thời gian đến cho mỗi request theo một quá trình Poisson được tham số hóa bởi tỷ lệ đến.

**Metric đánh giá.** Độ trễ được người dùng cảm nhận là một đo lường quan trọng cho các ứng dụng tương tác như ChatGPT. Cụ thể, tương tự như nghiên cứu trước về phục vụ LLM [10, 11], độ trễ trung bình mỗi token được tính toán như trung bình của độ trễ đầu-cuối của mỗi công việc chia cho độ dài đầu ra của nó. Ngoài ra, chúng tôi cũng báo cáo độ trễ tail P95.

Để so sánh, chúng tôi đặt một SLO độ trễ và so sánh thông lượng tối đa mà mỗi hệ thống có thể đạt được dưới SLO. Chúng tôi theo nghiên cứu trước [40] để đặt SLO độ trễ thành 10× độ trễ của một lần lặp duy nhất trong giai đoạn giải mã. Cụ thể, chúng tôi đặt SLO thành 0.3 giây dựa trên profiling của chúng tôi.

**Baseline.** Chúng tôi so sánh FastServe với ba baseline. Để so sánh công bằng, tất cả các baseline sử dụng cùng kích thước tensor parallelism, kích thước pipeline parallelism, và kích thước batch như FastServe, ngoại trừ vLLM chỉ sử dụng tensor parallelism để phục vụ OPT-175B, vì nó không hỗ trợ pipeline parallelism. Bảng 2 hiển thị so sánh giữa FastServe và các baseline.

• **FasterTransformer [26]:** Đây là một engine suy luận cấp độ sản xuất từ NVIDIA. Nó hỗ trợ cả tensor parallelism và pipeline parallelism. Tuy nhiên, nó áp dụng lập lịch cấp độ công việc và các công việc ngắn bị chặn bởi các công việc dài trong cùng một batch. Chúng tôi sử dụng FasterTransformer v5.3.

• **vLLM [11]:** Đây là hệ thống phục vụ LLM tối tân hỗ trợ lập lịch cấp độ lần lặp [10] và PagedAttention [11] để giảm phân mảnh bộ nhớ gây ra bởi cache key-value. Tuy nhiên, nó sử dụng bộ lập lịch FCFS đơn giản với thực thi chạy-đến-hoàn-thành, điều này gặp phải head-of-line blocking. Chúng tôi sử dụng vLLM v0.1.7.

• **FastServe-FCFS:** Nó sử dụng cùng engine thực thi phân tán của FastServe, nhưng nó không sử dụng các kỹ thuật được đề xuất trong §4. Baseline này giúp phân biệt speedup được mang lại bởi các kỹ thuật được đề xuất trong bài báo này từ việc triển khai hiệu quả của FastServe.

### 6.2 Hiệu suất Đầu-cuối

Chúng tôi so sánh hiệu suất đầu-cuối của FastServe với ba hệ thống baseline dưới khối lượng công việc ShareGPT và Alpaca trên OPT-13B, OPT-66B và OPT-175B trong Bảng 1.

Hàng đầu tiên của Hình 11 hiển thị hiệu suất đầu-cuối của tất cả các hệ thống dưới bộ dữ liệu ShareGPT. Mặc dù FasterTransformer triển khai các kernel GPU được tối ưu hóa cao cho suy luận LLM, nó không hỗ trợ lập lịch cấp độ lần lặp. Nó không thể trả về các công việc hoàn thành sớm trong batch đang diễn ra và thêm các công việc mới vào batch để giảm độ trễ. Kết quả là, FasterTransformer gặp phải head-of-line blocking đáng kể ngay cả khi tỷ lệ đến công việc nhỏ. FastServe vượt trội hơn FasterTransformer 31.5–74.9× về mặt thông lượng dưới SLO. Là hệ thống phục vụ tối tân, vLLM được trang bị hầu hết các kỹ thuật để tăng tốc suy luận và giảm tiêu thụ bộ nhớ GPU. Tuy nhiên, vì vLLM lập lịch các công việc theo cách FCFS, một phần lớn độ trễ đầu-cuối là độ trễ xếp hàng. Tối ưu hóa thời gian thực thi của công việc suy luận LLM là không đủ. Được trang bị bộ lập lịch skip-join MLFQ, FastServe có thể giảm đáng kể độ trễ xếp hàng và vượt trội hơn vLLM 2.3–18.3×. Đáng chú ý là FastServe-FCFS cũng vượt trội hơn vLLM, vì nó sử dụng triển khai C++ hiệu quả hơn và fuse nhiều hoạt động hơn thành ít kernel GPU hơn. Nhưng nó vẫn gặp phải vấn đề head-of-line blocking, khiến nó chậm hơn FastServe 2–4×.

Hàng thứ hai của Hình 11 hiển thị kết quả thực nghiệm dưới bộ dữ liệu Alpaca. Vì kích thước công việc của Alpaca nhỏ hơn so với bộ dữ liệu ShareGPT, tất cả các hệ thống phục vụ có thể duy trì độ trễ thấp ngay cả khi tỷ lệ tương đối cao hơn so với dưới bộ dữ liệu ShareGPT. Tuy nhiên, mức tăng hiệu suất của FastServe tương tự. Không có lập lịch cấp độ lần lặp, FasterTransformer là hệ thống chậm nhất và FastServe vượt trội hơn nó 9.5–15.8×. vLLM đạt được hiệu suất tốt hơn FasterTransformer, nhưng nó vẫn gặp phải vấn đề head-of-line blocking. Kết quả là, FastServe vượt trội hơn vLLM 3–31.4×. Với triển khai hiệu quả của chúng tôi, FastServe-FCFS cũng vượt trội hơn vLLM, nhưng nó vẫn chậm hơn FastServe 1.6–2×.

**Tác động lên tail latency.** Một mối quan tâm tiềm tàng của lập lịch preemptive và MLFQ là nó có thể gây starvation cho các công việc dài và làm tổn hại tail latency. FastServe kết hợp cơ chế ngăn chặn starvation trong bộ lập lịch skip-join MLFQ của nó (§4.1). Để minh chứng hiệu quả của cơ chế ngăn chặn starvation, chúng tôi đo độ trễ 95% của tất cả các hệ thống dưới bộ dữ liệu ShareGPT. Như được hiển thị trong Hình 12, FastServe cải thiện đáng kể thông lượng của các công việc suy luận LLM dưới cùng yêu cầu SLO cho tail latency. Ví dụ, khi phục vụ OPT-175B, so với FastServe-FCFS, FastServe cải thiện thông lượng lên đến 1.5×. FastServe cũng vượt trội hơn FastServe-FCFS 2–2.8× khi phục vụ OPT-13B và OPT-66B. FastServe đạt được cải thiện hiệu suất lên đến 17.9× và 59.8× so với vLLM và FasterTransformer, tương ứng. Kết quả cho thấy mặc dù FastServe được thiết kế để giảm độ trễ trung bình, nó cũng có thể giảm đáng kể tail latency của các công việc suy luận LLM. Ưu tiên các công việc ngắn với bộ lập lịch skip-join MLFQ có thể hiệu quả giảm vấn đề head-of-line blocking và không làm tổn hại tail latency. Ngay cả đối với các công việc dài, FastServe vẫn có thể tăng tốc chúng bằng cách giảm độ trễ xếp hàng của chúng. Cơ chế ngăn chặn starvation đảm bảo rằng các công việc dài có thể được lập lịch trong thời gian hợp lý.

**Tác động lên goodput.** Chúng tôi tiếp tục điều tra tác động lên goodput cho các hệ thống khác nhau dưới các SLO khác nhau khi phục vụ mô hình OPT-13B. Tương tự như các nghiên cứu trước [35, 41, 42], chúng tôi đo P95 goodput, được định nghĩa là thông lượng khi 95% công việc có thể được hoàn thành trong SLO của giai đoạn khởi tạo và giai đoạn giải mã. Chúng tôi đặt SLO của hai giai đoạn thành 5×, 10×, và 20× độ trễ trong giai đoạn tương ứng dưới tải nhẹ. Như được hiển thị trong Hình 13, FastServe liên tục đạt được P95 goodput cao nhất trên các SLO khác nhau. Cụ thể, FastServe vượt trội hơn vLLM 4.1× đến 4.7× và FastServe-FCFS 1.46× đến 1.64×. Những kết quả này minh chứng rằng FastServe hiệu quả cải thiện thông lượng của hệ thống mà không vi phạm SLO của cả giai đoạn khởi tạo và giai đoạn giải mã.

### 6.3 Lựa chọn Thiết kế

**Hiệu quả của skip-join MLFQ.** Để hiển thị hiệu quả của skip-join MLFQ, chúng tôi tiến hành so sánh hiệu suất với FCFS, MLFQ ngây thơ, và Fixed Priority khi phục vụ OPT-13B. Chúng tôi sử dụng bộ dữ liệu ShareGPT để tạo các công việc và thay đổi tỷ lệ giữa độ dài đầu vào và đầu ra trong khi bảo tồn phân phối độ dài gốc. Điều chỉnh này phản ánh xu hướng công nghiệp hiện tại của việc mở rộng giới hạn token đầu vào trong LLMs [4, 5, 42]. Hình 14 hiển thị kết quả khi thay đổi tỷ lệ giữa độ dài đầu vào và đầu ra. Độ trễ được chuẩn hóa bởi hệ thống chậm nhất. FCFS liên tục trải qua độ trễ cao do vấn đề head-of-line blocking, bất kể tỷ lệ. MLFQ ngây thơ có hiệu suất tốt dưới tỷ lệ nhỏ vì sự khác biệt giữa giai đoạn khởi tạo và giải mã là tối thiểu. Tuy nhiên, khi tỷ lệ tăng, MLFQ ngây thơ gặp khó khăn với giai đoạn khởi tạo kéo dài. Ngược lại, Fixed Priority xuất sắc với tỷ lệ lớn hơn, nơi giai đoạn khởi tạo chiếm ưu thế thời gian thực thi, nhưng hoạt động kém khi tỷ lệ thấp, vì nó bỏ qua giai đoạn giải mã khi đặt ưu tiên. Được hưởng lợi từ chính sách lập lịch bán bất khả tri thông tin, skip-join MLFQ liên tục cải thiện hiệu suất so với FCFS, MLFQ ngây thơ, và Fixed Priority lên đến 8.9×, 1.87×, và 13.9×.

**Hiệu quả của quản lý cache key-value chủ động.** Để hiển thị hiệu quả của quản lý cache key-value chủ động, chúng tôi đánh giá hiệu suất của FastServe với hai chiến lược baseline Recompute và Reactive được đề cập trong §4.2 khi phục vụ OPT-13B trên bộ dữ liệu ShareGPT. Hình 15(a) hiển thị kết quả. Ở tỷ lệ đến thấp, bộ nhớ GPU đủ để chứa các cache key-value cho tất cả các công việc, làm cho hiệu suất của ba chiến lược tương tự. Khi tỷ lệ đến tăng, bộ nhớ GPU trở nên không đủ để các hệ thống phải preempt một số cache key-value cho các công việc khác, dẫn đến hiệu suất khác biệt.

Trong trường hợp như vậy, Recompute loại bỏ các cache key-value cho các công việc ưu tiên thấp, tăng overhead tính toán lại cho các cache KV của những công việc này. Như được hiển thị trong Hình 15(a), overhead tính toán lại này làm cho chiến lược hoán đổi chủ động vượt trội hơn tính toán lại 2.7×.

Đối với Reactive, nó hoán đổi ra các công việc ưu tiên thấp đến bộ nhớ host khi bộ nhớ GPU không hiệu quả và hoán đổi vào những công việc này nếu cần. Việc truyền dữ liệu nằm trong đường dẫn quan trọng. Tính toán tiếp theo phải chờ những truyền tải này. Ngược lại, hoán đổi chủ động dự đoán yêu cầu bộ nhớ của các công việc mới đến và preempt các cache ưu tiên thấp trước. Tương tự, khi cache của một công việc ưu tiên cao được phát hiện trong bộ nhớ host và bộ nhớ GPU có sẵn, nó sẽ được hoán đổi chủ động vào bộ nhớ GPU. Điều này cho phép việc truyền dữ liệu overlap với tính toán và đạt được cải thiện 1.7× so với phương pháp reactive.

Để điều tra thêm overhead của cơ chế hoán đổi chủ động, chúng tôi chia độ trễ đầu-cuối thành ba phần: độ trễ xếp hàng, thời gian thực thi, và thời gian hoán đổi. Thời gian hoán đổi là thời gian khi công việc bị chặn bởi cơ chế hoán đổi chủ động. Như được hiển thị trong Hình 15(b), thời gian hoán đổi ít hơn 5% độ trễ đầu-cuối, điều này không đáng kể so với thời gian thực thi và độ trễ xếp hàng. Lý do xác nhận rằng cơ chế hoán đổi chủ động có thể overlap hầu hết thời gian hoán đổi với thời gian thực thi của các công việc khác. Kết quả là, cơ chế hoán đổi chủ động gần như không ảnh hưởng đến độ trễ đầu-cuối.

## 7 Nghiên cứu Liên quan

**Lập lịch preemptive.** Nhiều giải pháp cho lập lịch công việc trong datacenter sử dụng lập lịch preemptive. Nhiều hệ thống mạng [15, 31, 32, 43] sử dụng lập lịch flow preemptive để tối thiểu hóa thời gian hoàn thành flow. Nhiều bộ lập lịch cho các khối lượng công việc datacenter nhạy cảm với độ trễ, như Shinjuku [44], Shenango [45], và Caladan [46], cũng sử dụng preemption và tái phân bổ tài nguyên có độ chi tiết cao để tối ưu hóa tail latency ở quy mô microsecond. Đối với các khối lượng công việc DL, Tiresias [34] sử dụng MLFQ để tối ưu hóa thời gian hoàn thành công việc cho các công việc training DL phân tán. Pipeswitch [47] và REEF [48] cung cấp preemption GPU hiệu quả để chạy cả các nhiệm vụ DL quan trọng về độ trễ và best-effort trên cùng một GPU. Khác với chúng, FastServe nhắm đến một kịch bản mới, phục vụ suy luận LLM.

**Phục vụ suy luận.** Nhiều hệ thống phục vụ mô hình truyền thống [8, 9, 22, 23, 49] chỉ tập trung vào phục vụ các mô hình tương đối nhỏ trong một cụm mà không nhận thức về các đặc điểm của LLMs. Gần đây, một số hệ thống phục vụ được đề xuất để tối ưu hóa các LLMs dựa trên Transformer [10, 35, 50–52]. Orca [10] và vLLM [11] xem xét mẫu tạo sinh tự hồi quy của LLMs. Tuy nhiên, do chính sách FCFS của chúng, chúng gặp phải vấn đề head-of-line blocking nghiêm trọng. VTC [52] tập trung vào công bằng của phục vụ LLM nhưng không xem xét kịch bản preemption. Splitwise [53] và DistServe [41] tách rời giai đoạn prefill và decoding để loại bỏ sự can thiệp giữa chúng và do đó tối ưu hóa độ trễ thực thi. LoongServe [42] sử dụng elastic sequence parallelism để động đặt mức độ song song cho các request khác nhau ở các giai đoạn khác nhau. Những hệ thống này trực giao với FastServe.

**Tối ưu hóa bộ nhớ cho LLMs.** Do việc sử dụng bộ nhớ cao cho LLMs, nhiều kỹ thuật đã được đề xuất để giảm overhead bộ nhớ. Một số nghiên cứu [54, 55] nhắm đến training, điều này trực giao với kịch bản phục vụ. Quantization [56–59] nén trọng số mô hình thành độ chính xác thấp hơn sau training để giảm footprint bộ nhớ trong quá trình suy luận. SparTA [60] khai thác độ thưa thớt của mô hình để tăng tốc tính toán. Tuy nhiên, những phương pháp này hy sinh độ chính xác của mô hình. vLLM [11] đề xuất PagedAttention để giảm phân mảnh bộ nhớ GPU. Điều này trực giao với bài báo này và FastServe cũng triển khai PagedAttention.

## 8 Kết luận

Chúng tôi giới thiệu FastServe, một hệ thống phục vụ suy luận phân tán cho LLMs. Chúng tôi khai thác mẫu tự hồi quy của suy luận LLM để cho phép preemption cấp độ lần lặp và thiết kế một bộ lập lịch skip-join MLFQ mới để giải quyết vấn đề head-of-line blocking. Chúng tôi đề xuất một cơ chế quản lý cache key-value chủ động để xử lý overhead bộ nhớ của cache key-value và ẩn độ trễ truyền dữ liệu với tính toán. Dựa trên những điều này, chúng tôi xây dựng một nguyên mẫu của FastServe. Các thực nghiệm cho thấy FastServe cải thiện thông lượng lên đến 31.4× và 17.9× dưới cùng SLO độ trễ trung bình và tail, tương ứng, so với vLLM.

## Tài liệu tham khảo

[1] "Introducing ChatGPT." https://openai.com/blog/chatgpt, 2022.

[2] "ChatGPT sets record for fastest-growing user base." https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/, 2023.

[3] "Reinventing search with a new ai-powered bing and edge, your copilot for the web." https://news.microsoft.com/the-new-Bing/, 2023.

[4] Google, "Our next-generation model: Gemini 1.5." https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/, 2024.

[5] Anthropic, "Introducing the next generation of Claude." https://www.anthropic.com/news/claude-3-family, 2024.

[6] "Introducing Qwen." https://qwenlm.github.io/blog/qwen/, 2023.

[7] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in IEEE Conference on Computer Vision and Pattern Recognition, 2016.

[8] A. Gujarati, R. Karimi, S. Alzayat, W. Hao, A. Kaufmann, Y. Vigfusson, and J. Mace, "Serving DNNs like clockwork: Performance predictability from the bottom up," in USENIX OSDI, 2020.

[9] H. Zhang, Y. Tang, A. Khandelwal, and I. Stoica, "Shepherd: Serving dnns in the wild," in USENIX NSDI, 2023.

[10] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, "Orca: A distributed serving system for Transformer-Based generative models," in USENIX OSDI, 2022.

[11] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica, "Efficient memory management for large language model serving with pagedattention," in ACM SOSP, 2023.

[12] K. Kaffes, T. Chong, J. T. Humphries, A. Belay, D. Mazières, and C. Kozyrakis, "Shinjuku: Preemptive scheduling for μsecond-scale tail latency," in USENIX NSDI, 2019.

[13] "Sharegpt teams." https://sharegpt.com/, 2023.

[14] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, "Stanford alpaca: An instruction-following llama model." https://github.com/tatsu-lab/stanford_alpaca, 2023.

[15] W. Bai, L. Chen, K. Chen, D. Han, C. Tian, and H. Wang, "Information-agnostic flow scheduling for commodity data centers," in USENIX OSDI, 2015.

[16] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," arXiv, 2020.

[17] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, and Z. Chen, "Gpipe: Efficient training of giant neural networks using pipeline parallelism," Neural Information Processing Systems, 2019.

[18] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer, "Opt: Open pre-trained transformer language models," arXiv, 2022.

[19] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, "Language models are few-shot learners," arXiv, 2020.

[20] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, "Llama: Open and efficient foundation language models," arXiv, 2023.

[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Neural Information Processing Systems, 2017.

[22] C. Olston, N. Fiedel, K. Gorovoy, J. Harmsen, L. Lao, F. Li, V. Rajashekhar, S. Ramesh, and J. Soyke, "Tensorflow-serving: Flexible, high-performance ml serving," arXiv, 2017.

[23] N. Corporation, "Triton inference server: An optimized cloud and edge inferencing solution.," 2019.

[24] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli, "fairseq: A fast, extensible toolkit for sequence modeling," arXiv, 2019.

[25] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, "Huggingface's transformers: State-of-the-art natural language processing," arXiv, 2020.

[26] N. Corporation, "Fastertransformer," 2019.

[27] L. Schrage, "A proof of the optimality of the shortest remaining processing time discipline," Operations Research, 1968.

[28] N. Shazeer, "Fast transformer decoding: One write-head is all you need," arXiv, 2019.

[29] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai, "Gqa: Training generalized multi-query transformer models from multi-head checkpoints," arXiv, 2023.

[30] D. Li*, R. Shao*, A. Xie, Y. Sheng, L. Zheng, J. E. Gonzalez, I. Stoica, X. Ma, and H. Zhang, "How long can open-source llms truly promise on context length?," 2023.

[31] C.-Y. Hong, M. Caesar, and P. B. Godfrey, "Finishing flows quickly with preemptive scheduling," in ACM SIGCOMM, 2012.

[32] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker, "pfabric: Minimal near-optimal datacenter transport," SIGCOMM CCR, 2013.

[33] M. Chowdhury and I. Stoica, "Efficient coflow scheduling without prior knowledge," SIGCOMM CCR, 2015.

[34] J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. H. Liu, and C. Guo, "Tiresias: A gpu cluster manager for distributed deep learning.," in USENIX NSDI, 2019.

[35] Z. Li, L. Zheng, Y. Zhong, V. Liu, Y. Sheng, X. Jin, Y. Huang, Z. Chen, H. Zhang, J. E. Gonzalez, et al., "AlpaServe: Statistical multiplexing with model parallelism for deep learning serving," in USENIX OSDI, 2023.

[36] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling laws for neural language models," arXiv, 2020.

[37] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. A. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia, "Efficient large-scale language model training on gpu clusters using megatron-lm," arXiv, 2021.

[38] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia, "Pipedream: Generalized pipeline parallelism for dnn training," in ACM SOSP, 2019.

[39] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang, M. Elibol, Z. Yang, W. Paul, M. I. Jordan, and I. Stoica, "Ray: A distributed framework for emerging AI applications," in USENIX OSDI, 2018.

[40] G. Prekas, M. Kogias, and E. Bugnion, "Zygos: Achieving low tail latency for microsecond-scale networked tasks," in ACM SOSP, 2017.

[41] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and H. Zhang, "Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving," in USENIX OSDI, 2024.

[42] B. Wu, S. Liu, Y. Zhong, P. Sun, X. Liu, and X. Jin, "Loongserve: Efficiently serving long-context large language models with elastic sequence parallelism," arXiv, 2024.

[43] M. Chowdhury, Y. Zhong, and I. Stoica, "Efficient coflow scheduling with varys," in ACM SIGCOMM, 2014.

[44] K. Kaffes, T. Chong, J. T. Humphries, A. Belay, D. Mazières, and C. Kozyrakis, "Shinjuku: Preemptive scheduling for μsecond-scale tail latency," in USENIX NSDI, 2019.

[45] A. Ousterhout, J. Fried, J. Behrens, A. Belay, and H. Balakrishnan, "Shenango: Achieving high cpu efficiency for latency-sensitive datacenter workloads.," in USENIX NSDI, 2019.

[46] J. Fried, Z. Ruan, A. Ousterhout, and A. Belay, "Caladan: Mitigating interference at microsecond timescales," in USENIX OSDI, 2020.

[47] Z. Bai, Z. Zhang, Y. Zhu, and X. Jin, "Pipeswitch: Fast pipelined context switching for deep learning applications," in USENIX OSDI, 2020.

[48] M. Han, H. Zhang, R. Chen, and H. Chen, "Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences," in USENIX OSDI, 2022.

[49] D. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E. Gonzalez, and I. Stoica, "Clipper: A low-latency online prediction serving system.," in USENIX NSDI, 2017.

[50] J. Fang, Y. Yu, C. Zhao, and J. Zhou, "Turbotransformers: an efficient gpu serving system for transformer models," in ACM PPoPP, 2021.

[51] D. Li, R. Shao, H. Wang, H. Guo, E. P. Xing, and H. Zhang, "Mpcformer: fast, performant and private transformer inference with mpc," arXiv, 2023.

[52] Y. Sheng, S. Cao, D. Li, B. Zhu, Z. Li, D. Zhuo, J. E. Gonzalez, and I. Stoica, "Fairness in serving large language models," in USENIX OSDI, 2024.

[53] P. Patel, E. Choukse, C. Zhang, A. Shah, Íñigo Goiri, S. Maleki, and R. Bianchini, "Splitwise: Efficient generative llm inference using phase splitting," in ACM/IEEE ISCA, 2024.

[54] Y. Bai, C. Li, Q. Zhou, J. Yi, P. Gong, F. Yan, R. Chen, and Y. Xu, "Gradient compression supercharged high-performance data parallel dnn training," in ACM SOSP, 2021.

[55] J. Wang, B. Yuan, L. Rimanic, Y. He, T. Dao, B. Chen, C. Re, and C. Zhang, "Fine-tuning language models over slow networks using activation quantization with guarantees," Neural Information Processing Systems, 2022.

[56] Z. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D. Klein, and J. Gonzalez, "Train big, then compress: Rethinking model size for efficient training and inference of transformers," in International Conference on Machine Learning (ICML), 2020.

[57] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han, "Smoothquant: Accurate and efficient post-training quantization for large language models," International Conference on Machine Learning, 2022.

[58] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, "Gptq: Accurate post-training quantization for generative pre-trained transformers," arXiv, 2022.

[59] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, "Llm. int8 (): 8-bit matrix multiplication for transformers at scale," arXiv, 2022.

[60] N. Zheng, B. Lin, Q. Zhang, L. Ma, Y. Yang, F. Yang, Y. Wang, M. Yang, and L. Zhou, "SparTA: Deep-Learning model sparsity via Tensor-with-Sparsity-Attribute," in USENIX OSDI, 2022.
