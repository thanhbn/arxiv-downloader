I'll translate the entire paper to Vietnamese while maintaining the exact structure. This is a substantial document, so I'll proceed systematically through each section.

# 2402.07033.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/inference/2402.07033.pdf
# Kích thước tệp: 2397054 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025
FIDDLER : ĐIỀU PHỐI CPU-GPU CHO
SUYLUẬN NHANH CỦA CÁC MÔ HÌNH MIXTURE-OF-EXPERTS
Keisuke Kamahori1∗Tian Tang1,2∗Yile Gu1Kan Zhu1Baris Kasikci1
1University of Washington2Tsinghua University
{kamahori,tian21,yilegu,kanzhu,baris}@cs.washington.edu
TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLMs) với kiến trúc Mixture-of-Experts (MoE) đã cho thấy hiệu suất đầy hứa hẹn trên nhiều nhiệm vụ khác nhau. Tuy nhiên, do kích thước mô hình khổng lồ, việc chạy chúng trong môi trường hạn chế tài nguyên nơi bộ nhớ GPU không dồi dào là một thách thức. Một số hệ thống hiện có đề xuất sử dụng tài nguyên CPU để giải quyết vấn đề đó, nhưng chúng hoặc gặp phải chi phí đáng kể khi thường xuyên di chuyển dữ liệu giữa CPU và GPU, hoặc không xem xét đến các đặc điểm riêng biệt của CPU và GPU. Bài báo này đề xuất Fiddler, một hệ thống suy luận tiết kiệm tài nguyên cho các mô hình MoE với tài nguyên GPU hạn chế. Fiddler sử dụng chiến lược tài nguyên CPU và GPU bằng cách xác định chiến lược thực thi tối ưu. Đánh giá của chúng tôi cho thấy rằng, không giống như các hệ thống tiên tiến được tối ưu hóa cho các tình huống cụ thể như suy luận batch đơn hoặc prefill dài, Fiddler hoạt động tốt hơn trong tất cả các tình huống. So với các baseline khác nhau, Fiddler đạt được tăng tốc 1.26 lần trong suy luận batch đơn, 1.30 lần trong xử lý prefill dài, và 11.57 lần trong suy luận beam search. Mã nguồn của Fiddler có sẵn công khai tại https://github.com/efeslab/fiddler.

Hình 1: Tổng quan cấp cao về Fiddler. Mỗi lớp của mô hình MoE được đặt hoặc trong bộ nhớ CPU hoặc bộ nhớ GPU, và Fiddler xác định chiến lược thực thi tối ưu sử dụng cả CPU và GPU dựa trên số lượng token đầu vào của mỗi expert.

1 GIỚI THIỆU
Gần đây, việc chạy Các Mô hình Ngôn ngữ Lớn (LLMs) trong môi trường hạn chế tài nguyên đang trở nên ngày càng quan trọng và phù hợp. Có sự quan tâm ngày càng tăng đối với việc chạy LLMs trong môi trường cục bộ như máy tính cá nhân hoặc thiết bị biên (Giacinto, 2023; Anand et al., 2023; Song et al., 2023) để cải thiện quyền riêng tư (Martínez Toro et al., 2023) và tùy chỉnh mô hình sử dụng dữ liệu độc quyền hoặc cá nhân (Lyu et al., 2023). Việc cho phép các mô hình này hoạt động trong cài đặt hạn chế tài nguyên dân chủ hóa việc tiếp cận các công nghệ LLM tiên tiến, đặc biệt là cho những người không có quyền truy cập vào GPU cao cấp hoặc số lượng lớn GPU. Xu hướng này được củng cố bởi các đề xuất sử dụng LLMs làm cốt lõi của tất cả các hệ thống máy tính (Packer et al., 2023; Berger & Zorn, 2024). Do đó, việc có thể chạy các mô hình lớn trên một loạt rộng các máy tính hoặc máy chủ là điều mong muốn, không giống như hiện trạng nơi LLMs thường được phục vụ với các cụm GPU lớn (Patel & Ahmad, 2023; Ye et al., 2025; Zhu et al., 2024).
∗Đóng góp bằng nhau.
1arXiv:2402.07033v3  [cs.LG]  1 May 2025

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025
LLMs sử dụng kiến trúc Mixture-of-Experts (MoE) đặc biệt phù hợp cho môi trường hạn chế tài nguyên. Các LLM dựa trên MoE đã chứng minh giá trị trên một loạt các nhiệm vụ (Du et al., 2022; Fedus et al., 2022; Jiang et al., 2024; Databricks, 2024) và chúng hoạt động bằng cách kích hoạt có chọn lọc một tập con các tham số thông qua cơ chế gating, do đó giảm yêu cầu tính toán cho cả huấn luyện và suy luận so với các đối tác dày đặc. Do đó, các mô hình MoE dễ dàng mở rộng lên kích thước lớn hơn, dẫn đến việc phát triển các mô hình mạnh mẽ (Rajbhandari et al., 2022).

Mặc dù các mô hình MoE có vẻ phù hợp cho môi trường hạn chế tài nguyên do yêu cầu tính toán tương đối thấp, việc triển khai suy luận cục bộ với các mô hình này đặt ra một số thách thức. Đầu tiên, kích thước mô hình thường rất lớn và tăng nhanh khi số lượng experts hoặc kích thước ẩn tăng lên. Các mô hình MoE được phát hành gần đây bao gồm Mixtral-8x7B (47B tham số), Mixtral-8x22B (141B tham số), DBRX (132B tham số), DeepSeek-V2 (236B tham số), Grok-1 (314B tham số), và Snowflake Arctic (479B tham số) (Jiang et al., 2024; AI, 2024; Databricks, 2024; DeepSeek-AI, 2024; xAI, 2024; Snowflake, 2024). Do đó, một số lượng rất lớn GPU được yêu cầu chỉ để lưu trữ các tham số mô hình, với khả năng thường bị hạn chế của bộ nhớ GPU. Tình hình càng trở nên tệ hơn bởi số lượng hầu như không giới hạn các thành phần expert trong các mô hình MoE; ví dụ, biến thể lớn nhất của Switch Transformer có 2048 experts trong mỗi lớp và có tổng cộng 1.6T tham số (Fedus et al., 2022). Không có các kỹ thuật nén hoặc lượng tử hóa mô hình, mô hình có thể chiếm 3.2TB dung lượng lưu trữ. Điều này có nghĩa là 40 GPU NVIDIA A100 80GB sẽ cần thiết chỉ để lưu trữ tất cả các trọng số mô hình.

Thứ hai, trong khi tất cả các tham số phải được lưu trữ trong bộ nhớ GPU để suy luận hiệu quả, thuộc tính độc đáo của các mô hình MoE có nghĩa là không phải tất cả các tham số được sử dụng để tạo ra một token mới. Thực tế là chỉ một tập con các tham số hoạt động trong quá trình tạo ra mỗi token dẫn đến việc sử dụng bộ nhớ GPU không đầy đủ. Điều này đặc biệt có vấn đề vì nhu cầu toàn cầu tăng vọt đối với các công nghệ AI sinh tạo đã đẩy giá GPU lên cao. Kết quả là, việc đầu tư vào một số lượng lớn GPU không hiệu quả về chi phí đối với hầu hết các tổ chức. Chỉ các nhà cung cấp đám mây lớn, được biết đến như hyperscalers, có thể đủ khả năng vì họ phục vụ nhiều người dùng đồng thời và đạt được việc sử dụng GPU tốt hơn thông qua batching đáng kể.

Một số hệ thống hiện có sử dụng tài nguyên CPU để giải quyết thách thức chạy các mô hình lớn trong môi trường hạn chế tài nguyên, nhưng chúng gặp khó khăn với việc thực thi hiệu quả các mô hình MoE. Một mặt, các phương pháp offload trọng số mô hình vào bộ nhớ CPU và chuyển các trọng số cần thiết vào bộ nhớ GPU theo yêu cầu giải quyết vấn đề khả năng bộ nhớ. Tuy nhiên, chúng gây ra chi phí runtime đáng kể do băng thông thấp của kết nối PCIe (Eliseev & Mazur, 2023; Xue et al., 2024b). Mặt khác, các framework suy luận dựa trên CPU sử dụng một phần GPU có thể giảm chi phí chuyển tham số (ggml authors, 2023). Tuy nhiên, chúng không tính đến các thuộc tính mô hình MoE hoặc các đặc điểm thiết bị khác nhau của CPU và GPU, dẫn đến hiệu suất dưới tối ưu trong các trường hợp sử dụng quan trọng như prefill dài hoặc beam search, điều này rất cần thiết để nâng cao chất lượng tạo ra (Dong et al., 2022; von Platen, 2023). Chúng tôi thảo luận về những hạn chế của các cách tiếp cận hiện có chi tiết trong §2.

Trong bài báo này, chúng tôi giải quyết thách thức chạy hiệu quả các mô hình MoE với tài nguyên GPU hạn chế, bằng cách sử dụng chiến lược cả tài nguyên CPU và GPU. Chúng tôi giới thiệu Fiddler, một hệ thống suy luận MoE tiết kiệm tài nguyên tận dụng thông minh kiến trúc tính toán đa dạng của cả CPU và GPU. Không giống như các công trình trước đây chỉ sử dụng bộ nhớ CPU hoặc chia nhỏ thực thi một cách ngây thơ giữa CPU và GPU, cách tiếp cận của chúng tôi tạo ra các chiến lược thực thi tối ưu bằng cách xem xét các đặc điểm khác nhau của CPU và GPU. Vì CPU có khả năng bộ nhớ lớn hơn mặc dù có sức mạnh tính toán yếu hơn, các mô hình MoE đặc biệt thú vị cho bối cảnh này do yêu cầu tính toán nhỏ tương đối so với kích thước tham số của chúng.

Trong quá trình suy luận, Fiddler phát triển một mô hình độ trễ dựa trên các hiệu ứng batching khác nhau của CPU và GPU để xác định chiến lược thực thi tối ưu cho các lớp MoE, như được hiển thị trong Hình 1. Khi các lớp expert được thực thi trên CPU, độ trễ tăng gần như tuyến tính với kích thước đầu vào (xem phân tích chi tiết trong §A). Ngược lại, độ trễ thực thi GPU vẫn gần như không đổi bất kể kích thước đầu vào nhưng gây ra chi phí nếu trọng số cần được chuyển từ bộ nhớ CPU sang bộ nhớ GPU. Do đó, đối với kích thước đầu vào nhỏ hơn, việc thực thi các lớp expert trên CPU hiệu quả hơn, tránh chi phí chuyển trọng số. Tuy nhiên, đối với kích thước đầu vào và kích thước batch lớn hơn, tính toán CPU trở nên quá tốn thời gian, làm cho việc chuyển trọng số vào bộ nhớ GPU và thực hiện tính toán trên GPU hiệu quả hơn. Fiddler chọn động các kế hoạch thực thi chạy các mô hình MoE hiệu quả với bộ nhớ GPU hạn chế trên các workload khác nhau, bao gồm prefill dài và beam search.

2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025
Chúng tôi cũng kết hợp một số tối ưu hóa vào thiết kế của Fiddler. Để tối đa hóa khả năng expert cần thiết có sẵn trong bộ nhớ GPU, chúng tôi đặt các expert được sử dụng thường xuyên trên GPU dựa trên profiling offline về độ phổ biến của expert. Ngoài ra, chúng tôi thiết kế một kernel tính toán chuyên biệt cho xử lý expert trên CPU sử dụng bộ lệnh AVX512_BF16, không được hỗ trợ trong triển khai PyTorch gốc (Paszke et al., 2019).

Chúng tôi đánh giá Fiddler với mô hình Mixtral-8x7B không nén (16-bit), có hơn 90GB tham số, trên hai môi trường với mỗi môi trường có một GPU. Fiddler đạt được trung bình tăng tốc 1.26 lần trong suy luận batch đơn, 1.30 lần trong xử lý prefill dài, và 11.57 lần trong suy luận beam search, so với các hệ thống tiên tiến khác nhau, trên các môi trường khác nhau (§4). Đáng chú ý, trong khi các hệ thống hiện có cho thấy các sự đánh đổi khác nhau (ví dụ, các cách tiếp cận dựa trên offloading xuất sắc trong các tình huống prefill dài, trong khi các phương pháp dựa trên CPU hoạt động tốt với độ trễ batch đơn), hệ thống của chúng tôi tích hợp các lợi thế của cả hai, đạt được kết quả cân bằng và hiệu quả trong các điều kiện đa dạng.

Để tóm tắt, các đóng góp của chúng tôi như sau:
• Chúng tôi thiết kế Fiddler, một hệ thống suy luận cho các mô hình MoE chạy trên kiến trúc đa dạng hạn chế tài nguyên, tìm ra chiến lược thực thi tối ưu sử dụng cả GPU và CPU.
• Chúng tôi đánh giá Fiddler và cho thấy rằng nó đạt được hiệu suất tốt hơn trong suy luận batch đơn, xử lý prefill dài, và suy luận beam search, so với các hệ thống tiên tiến khác nhau. Nó cho thấy rằng Fiddler tích hợp các lợi thế của các loại hệ thống hiện có khác nhau.

2 CÔNG TRÌNH LIÊN QUAN

2.1 MIXTURE-OF-EXPERTS
LLMs dựa trên kiến trúc MoE đã cho thấy hiệu suất đầy hứa hẹn trong nhiều ứng dụng khác nhau (Rajbhandari et al., 2022; Du et al., 2022; Fedus et al., 2022; Jiang et al., 2024; Xue et al., 2024a; Dai et al., 2024). Không giống như Transformers dày đặc ban đầu (Vaswani et al., 2017), các mô hình MoE thêm tính thưa thớt vào lớp feed-forward thông qua một hệ thống experts và cơ chế gating. Mỗi lớp MoE chứa nhiều lớp expert khớp với hình dạng của lớp feed-forward, và một mạng gating xác định experts nào được kích hoạt cho mỗi đầu vào. Trong khi một lớp MoE có thể bao gồm hàng nghìn experts (Fedus et al., 2022), chỉ một số ít được chọn được kích hoạt bởi mạng gating trong quá trình huấn luyện hoặc suy luận.

2.2 TRIỂN KHAI MÔ HÌNH LỚN VỚI KIẾN TRÚC ĐA DẠNG
Triển khai các mô hình MoE hiệu quả có thể là thách thức vì kích thước mô hình lớn, đặc biệt trong các cài đặt hạn chế tài nguyên. Một số hệ thống hiện có sử dụng tài nguyên CPU để giải quyết thách thức chạy các mô hình lớn trong môi trường hạn chế tài nguyên, nhưng chúng không đủ để chạy các mô hình MoE hiệu quả.

Offloading là một cách tiếp cận để chạy các mô hình lớn trong môi trường như vậy. Chúng lưu trữ một tập con trọng số mô hình trong bộ nhớ CPU thay vì bộ nhớ GPU để sử dụng khả năng lớn hơn (Sheng et al., 2023). Các trọng số cần thiết được chuyển theo yêu cầu từ bộ nhớ CPU sang bộ nhớ GPU trong quá trình tính toán để suy luận. Đối với các mô hình MoE, một số công trình trước đây đã cố gắng offload trọng số expert với các cơ chế caching hoặc prefetching (Eliseev & Mazur, 2023; Xue et al., 2024b). Những cách tiếp cận này giải quyết các hạn chế khả năng bộ nhớ và tốt cho các tình huống hướng thông lượng. Tuy nhiên, chúng gặp phải chi phí độ trễ đáng kể do việc chuyển thường xuyên trọng số expert giữa CPU và GPU qua kết nối PCIe, vì băng thông của nó nhỏ hơn băng thông truy cập bộ nhớ. Kết quả là, chúng cho thấy hiệu suất dưới tối ưu cho các cài đặt nơi độ trễ quan trọng cho trải nghiệm người dùng. Fiddler vượt qua thách thức này bằng cách sử dụng tài nguyên tính toán của CPU.

Một hướng công trình khác đề xuất các framework suy luận dựa trên CPU hỗ trợ chạy LLMs bằng cách sử dụng một phần GPU (ggml authors, 2023). Tùy thuộc vào tính khả dụng của bộ nhớ GPU, các hệ thống như vậy thực thi một tập con các lớp mô hình trên GPU và phần còn lại trên CPU. Mặc dù chúng có thể

3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Hình 2: Tổng quan về Fiddler. (a) Trong giai đoạn khởi tạo, các tham số của các lớp không phải expert và một tập con được chọn của các lớp expert được phân bổ vào bộ nhớ GPU khi có thể; các tham số còn lại được phân bổ vào bộ nhớ CPU. (b) Tại runtime, Fiddler xác định động chiến lược thực thi tối ưu bằng cách xem xét khối lượng đầu vào kích hoạt mỗi lớp expert cùng với các độ trễ xử lý dự kiến khác nhau của CPU và GPU.

giảm chi phí chuyển tham số mô hình, các cách tiếp cận như vậy cho thấy tốc độ thực thi dưới tối ưu cho các trường hợp sử dụng quan trọng, như prefill dài hoặc beam search, điều này rất cần thiết để nâng cao chất lượng tạo ra (Dong et al., 2022; von Platen, 2023). Điều này là do chúng không xem xét các hiệu ứng batching khác nhau của GPU và CPU (Chen, 2023) và các thuộc tính của mô hình MoE.

Mặc dù các kỹ thuật nén mô hình như lượng tử hóa (Frantar & Alistarh, 2023; Zhao et al., 2023) hoặc thưa thớt hóa (Alizadeh et al., 2023; Tang et al., 2024; Zhu et al., 2025) có thể giảm kích thước mô hình và cải thiện hiệu quả suy luận, chúng đi kèm với chất lượng đầu ra bị suy giảm của mô hình, đặc biệt khi cố gắng phù hợp với các mô hình lớn vào một GPU có khả năng bộ nhớ hạn chế (Eliseev & Mazur, 2023). Gần đây, (Song et al., 2023) đề xuất khai thác tính thưa thớt của LLMs để suy luận nhanh hơn với CPU offloading. Tuy nhiên, cách tiếp cận này yêu cầu mô hình sử dụng hàm Rectified Linear Units (ReLU) cho kích hoạt phi tuyến. Việc chuyển đổi các mô hình không phải ReLU, phổ biến trong các LLM tiên tiến, thành mô hình ReLU yêu cầu huấn luyện bổ sung và gây ra suy giảm chất lượng mô hình (Mirzadeh et al., 2023; SparseLLM). Ví dụ, Mixtral-8x7B sử dụng hàm Sigmoid Linear Units (SiLU) (Elfwing et al., 2018), và chỉ một phần nhỏ giá trị gần bằng không. Do đó, khó khai thác tính thưa thớt (một thảo luận chi tiết hơn được đưa ra trong Phụ lục B). Fiddler có thể đạt được hiệu suất tốt hơn mà không sửa đổi cấu trúc hoặc độ chính xác mô hình. Chúng tôi lưu ý rằng Fiddler trực giao với các kỹ thuật nén, và những tối ưu hóa này có thể được áp dụng trên Fiddler.

3 THIẾT KẾ

Phần này giải thích thiết kế của Fiddler. Fiddler được thiết kế cho tình huống nơi khả năng bộ nhớ GPU không đủ để lưu trữ tất cả các tham số mô hình MoE. Do đó, trọng số của một số experts được lưu trữ trong bộ nhớ CPU thay vì bộ nhớ GPU. Fiddler tìm ra chiến lược thực thi tối ưu cho các trường hợp như vậy, cho việc lựa chọn expert bởi đầu vào và hành vi batching khác nhau của CPU và GPU.

3.1 TỔNG QUAN

Hình 2 minh họa tổng quan về Fiddler. Trong giai đoạn khởi tạo, Fiddler phân bổ các tham số cho các lớp không phải expert cùng với những tham số cho một tập con được chọn của các lớp expert vào bộ nhớ GPU, nhiều nhất là khả năng bộ nhớ GPU cho phép. Fiddler chọn những experts đó để được đặt trên bộ nhớ GPU dựa trên độ phổ biến của chúng, mà chúng tôi giải thích trong §3.4. Fiddler luôn phân bổ trọng số của các lớp không phải expert trên bộ nhớ GPU vì chúng được sử dụng cho mọi token, bất kể lựa chọn expert. Kích thước của các lớp không phải expert thường không lớn (ví dụ, ít hơn 2 tỷ tham số cho mô hình Mixtral-8x7B), và chúng tôi giả định chúng vừa với bộ nhớ GPU trong bài báo này. Các tham số của các lớp expert không vừa trong bộ nhớ GPU do ràng buộc khả năng được lưu trữ trong bộ nhớ CPU.

4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Trong giai đoạn thực thi, Fiddler cẩn thận đánh giá và chọn chiến lược thực thi hiệu quả nhất. Quyết định này được thông báo bởi số lượng đầu vào mà mỗi lớp expert nhận được và độ trễ xử lý khác nhau của CPU và GPU. Lớp gating của mô hình xác định số lượng đầu vào mà mỗi expert nhận được, và độ trễ xử lý có thể được dự đoán bằng cách sử dụng các thuộc tính thiết bị.

Fiddler xem xét các hiệu ứng batching khác nhau của CPU và GPU. Trong xử lý các lớp expert, số lượng đầu vào ảnh hưởng đến độ trễ thực thi khác nhau trên CPU và GPU. Cụ thể, xử lý GPU thể hiện độ trễ tương đối ổn định trên các kích thước đầu vào khác nhau, có thể được quy cho khả năng xử lý song song của nó. Những khả năng này làm cho độ trễ thực thi bị giới hạn bởi thời gian cần thiết để tải tham số từ bộ nhớ. Ngược lại, độ trễ liên quan đến xử lý CPU có xu hướng tăng gần như tuyến tính với kích thước đầu vào. Sự tăng tuyến tính này là do khả năng tính toán yếu hơn của CPU so với GPU, làm cho độ trễ bị giới hạn bởi phần tính toán, không phải phần di chuyển bộ nhớ. Chúng tôi đưa ra một phân tích chi tiết hơn trong Phụ lục A.

Hình 3: Ba tình huống khác nhau cho việc thực thi các lớp expert. Khi trọng số expert có mặt trong bộ nhớ GPU, lớp expert có thể được thực thi tại GPU mà không có bất kỳ chuyển dữ liệu nào (a). Khi trọng số expert thiếu trong bộ nhớ GPU, trọng số expert có thể được sao chép từ bộ nhớ CPU sang bộ nhớ GPU và thực thi tại GPU (b), hoặc activation có thể được sao chép từ bộ nhớ GPU sang bộ nhớ CPU và thực thi tại CPU (c).

3.2 CHIẾN LƯỢC THỰC THI

Sau khi expert được chọn cho mỗi token đầu vào tại lớp không phải expert, có ba tình huống cho việc xử lý các lớp expert, như được hiển thị trong Hình 3.

Nếu trọng số tương ứng có mặt trên bộ nhớ GPU, lớp expert có thể được thực thi trên GPU mà không có bất kỳ chuyển dữ liệu nào giữa CPU và GPU (Hình 3 a).

Tuy nhiên, vì tất cả các tham số mô hình không vừa trong bộ nhớ GPU, đôi khi trọng số expert không có mặt trong bộ nhớ GPU. Trong trường hợp đó, tồn tại hai chiến lược khác nhau để thực thi lớp expert. Phương pháp đầu tiên là sao chép trọng số mô hình từ bộ nhớ CPU sang bộ nhớ GPU, sau đó thực thi expert sử dụng GPU (Hình 3 b). Khi một số trọng số expert thiếu trên bộ nhớ GPU (1), chúng được sao chép từ bộ nhớ CPU sang bộ nhớ GPU (2), và sau đó GPU thực thi lớp expert (3). Các hệ thống offloading hiện có sử dụng phương pháp này.

Một cách tiếp cận khác là sao chép các activations từ bộ nhớ GPU sang bộ nhớ CPU và thực thi lớp expert trên CPU (Hình 3 c). Trong cách tiếp cận này, khi một số trọng số expert thiếu trên bộ nhớ GPU (1), các giá trị activation được sao chép từ bộ nhớ GPU sang bộ nhớ CPU (2) thay vì sao chép trọng số. Sau đó, tính toán của lớp expert xảy ra trên CPU (3), và các activations đầu ra được sao chép trở lại GPU sau khi tính toán hoàn thành (4). Một phương pháp tương tự được sử dụng bởi llama.cpp.

Hai chiến lược sau này (b. và c. ở trên) có các sự đánh đổi khác nhau. Một mặt, GPU có khả năng tính toán mạnh hơn phù hợp cho xử lý expert. Do đó, (b) có lợi thế so với (c) về độ trễ tính toán. Hơn nữa, như đã thảo luận trước đây, độ trễ tính toán của (c) trở nên dài hơn khi kích thước đầu vào tăng do các hiệu ứng batching khác nhau của CPU và GPU.

Mặt khác, xem xét giao tiếp CPU-GPU, phương pháp trong (b) cần chuyển trọng số mô hình, trong khi (c) chỉ cần chuyển các giá trị activation. Vì kích thước của activations (kích thước đầu vào ×4096 cho Mixtral-8x7B) nhỏ hơn đáng kể so với kích thước trọng số (3 ma trận với kích thước 4096×14336 mỗi expert cho Mixtral-8x7B, tiêu thụ hơn 300MB với độ chính xác 16-bit) cho kích thước đầu vào nhỏ, (c) có lợi thế trong việc giảm chi phí giao tiếp.

Tổng thể, (c) có lợi thế khi kích thước đầu vào cho một expert nhỏ, trong khi (b) tốt hơn nếu kích thước đầu vào trên một ngưỡng nào đó, ngay cả với chi phí giao tiếp lớn. Khi xử lý prefill dài, kích thước đầu vào có thể đạt một nghìn. Tuy nhiên, trong các tình huống như vậy, độ trễ tính toán của phương pháp (c) trở nên cấm đoán hơn độ trễ chuyển trọng số của (b), làm cho (c) trở thành một lựa chọn không thực tế. Do đó, độ trễ chuyển cho activation là không đáng kể khi (c) được sử dụng. Chúng tôi đưa ra một phân tích định lượng chi tiết hơn trong Phụ lục A.

3.3 THUẬT TOÁN

Dựa trên phân tích được mô tả ở trên, Fiddler phục vụ các mô hình MoE theo cách sau.

Khởi tạo. Trước khi bắt đầu quá trình suy luận, Fiddler phân phối trọng số mô hình giữa bộ nhớ CPU và GPU. Đầu tiên, trọng số của các lớp không phải expert được đặt trên bộ nhớ GPU vì chúng được sử dụng cho mọi token, bất kể lựa chọn expert. Kích thước của các lớp không phải expert thường không lớn (ít hơn 2 tỷ tham số cho mô hình Mixtral-8x7B), và Fiddler giả định chúng vừa với bộ nhớ GPU trong bài báo này. Tiếp theo, Fiddler đặt một tập con các lớp expert vào bộ nhớ GPU. Để làm điều này, nó chọn nhiều experts nhất mà khả năng bộ nhớ cho phép để tối đa hóa tỷ lệ hit, tức là khả năng trọng số của một expert có trong bộ nhớ GPU. Để lựa chọn expert, chúng tôi áp dụng một tối ưu hóa như đã thảo luận trong §3.4.

Chúng tôi cũng đo độ trễ để sao chép trọng số và thực thi experts trên CPU hoặc GPU với các kích thước đầu vào khác nhau để thông báo quyết định tại runtime.

Thực thi. Tại runtime, Fiddler xác định các cấu hình tối ưu để thực thi các lớp expert trên GPU và CPU. Fiddler biết expert(s) nào nên được sử dụng cho mỗi token được xử lý sau khi thực thi hàm gating cho mỗi lớp. Điều này cho phép Fiddler học kích thước đầu vào cho mỗi expert. Lưu ý rằng nhiều đầu vào có thể được xử lý đồng thời, ngay cả cho một yêu cầu duy nhất, trong giai đoạn prefill hoặc khi beam search được sử dụng.

Dựa trên thông tin kích thước đầu vào, Fiddler xác định chiến lược thực thi hiệu quả nhất để phân phối workload trên CPU và GPU. Để đạt được điều này, Fiddler sử dụng Thuật toán 1. Hàm is_at_gpu(i, j) kiểm tra xem trọng số của expert j trong lớp thứ i có được đặt trong bộ nhớ GPU tại thời điểm khởi tạo không. Ngoài ra, cpu_lat(s) và gpu_lat(s) cung cấp độ trễ dự kiến cho việc thực thi một expert trên CPU và GPU, tương ứng, cho kích thước đầu vào s. Hàm transfer_lat() ước tính độ trễ cần thiết để chuyển trọng số của một expert từ bộ nhớ CPU sang bộ nhớ GPU.

Khi thực thi một expert trên GPU cùng với chuyển trọng số, độ trễ chủ yếu bị chi phối bởi thời gian cần thiết để chuyển trọng số của expert từ CPU sang bộ nhớ GPU, độc lập với kích thước batch. Ngược lại, việc thực thi một lớp expert trên CPU thể hiện hành vi khác nhau: khi số lượng token đầu vào tăng, độ trễ cũng tăng. Tuy nhiên, thời gian cần thiết để sao chép activation từ GPU sang CPU là không đáng kể, chiếm ít hơn 1% tổng độ trễ (xem Phụ lục A để biết thêm chi tiết).

Để tối ưu hóa việc xử lý giai đoạn prefill, chúng tôi sử dụng một mô hình nơi thời gian thực thi GPU được xem xét là hằng số, trong khi thời gian thực thi CPU được giả định tăng tuyến tính với số lượng token đầu vào. Cụ thể, cho số lượng token đầu vào s, gpu_lat(s) trả về một giá trị hằng số, trong khi cpu_lat(s) trả về một giá trị tỷ lệ thuận với s, nhân với một hằng số khác. Những hằng số này được xác định trong giai đoạn khởi tạo.

6

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Thuật toán 1 Chiến lược Thực thi Expert
1: Đầu vào:
2: ne: số lượng experts trong một lớp
3: i: lớp cần xem xét (chúng ta xem xét lớp thứ i)
4: inp_size: mảng kích thước đầu vào cho mỗi expert
5: for j = 1 to ne do
6:     s ← inp_size[j]
7:     if s == 0 then
8:         continue
9:     end if
10:    if is_at_gpu(i, j) then
11:        // chạy expert thứ j tại GPU
12:    else if cpu_lat(s) > gpu_lat(s) + trans_lat() then
13:        // chạy expert thứ j tại GPU
14:    else
15:        // chạy expert thứ j tại CPU
16:    end if
17: end for

3.4 TỐI ƯU HÓA

Hiệu suất thực thi tốt nhất đạt được khi cách tiếp cận của Hình 3 a được sử dụng thường xuyên nhất, tức là khi trọng số expert được yêu cầu bởi đầu vào có mặt trong bộ nhớ GPU thường xuyên nhất. Để tối đa hóa khả năng expert cần thiết có sẵn trong bộ nhớ GPU, chúng tôi đặt các experts được sử dụng thường xuyên trên GPU dựa trên profiling offline. Để làm điều này, chúng tôi chọn nhiều experts nhất mà khả năng bộ nhớ cho phép theo thứ tự độ phổ biến để tối đa hóa tỷ lệ hit, tức là khả năng trọng số của một expert có trong bộ nhớ GPU. Chúng tôi xác định các experts phổ biến dựa trên profile của việc lựa chọn expert sử dụng dữ liệu hiệu chuẩn. Chúng tôi giả định phương pháp này đủ vì việc lựa chọn expert được biết là dựa trên đặc điểm token, và độ phổ biến của experts gần như phổ quát trên các miền đầu vào khác nhau (Jiang et al., 2024; Xue et al., 2024a). Phụ lục C thảo luận về việc lựa chọn expert chi tiết hơn.

Ngoài ra, chúng tôi thiết kế một kernel tính toán chuyên biệt cho xử lý expert trên CPU sử dụng bộ lệnh AVX512_BF16, không được hỗ trợ trong triển khai PyTorch gốc (Paszke et al., 2019).

4 ĐÁNH GIÁ

Chúng tôi đánh giá hiệu suất của Fiddler cho suy luận mô hình MoE trong các cài đặt hạn chế tài nguyên, nơi một số lượng nhỏ truy vấn đồng thời được đưa ra, và độ trễ quan trọng cho trải nghiệm người dùng.

4.1 THIẾT LẬP

Mô hình và Dữ liệu. Chúng tôi sử dụng mô hình Mixtral-8x7B (Jiang et al., 2024) với độ chính xác 16-bit cho đánh giá. Mô hình này không chỉ đại diện cho kiến trúc của hầu hết các mô hình MoE gần đây mà còn được hỗ trợ bởi tất cả các hệ thống baseline, đảm bảo so sánh hiệu suất công bằng. Cho dữ liệu đánh giá và hiệu chuẩn, chúng tôi sử dụng ShareGPT (ShareGPT), một tập dữ liệu các cuộc trò chuyện giữa con người và chatbot, để mô phỏng hành vi thực tế của việc lựa chọn expert. Chúng tôi chọn tập con các cuộc trò chuyện một cách ngẫu nhiên. Chúng tôi triển khai Fiddler trên PyTorch (Paszke et al., 2019). Ngoài ra, chúng tôi đưa ra một nghiên cứu độ nhạy trên các tập dữ liệu khác nhau trong §D để cho thấy hiệu quả của Fiddler trong nhiều hành vi định tuyến đa dạng hơn.

Môi trường. Chúng tôi đánh giá Fiddler trên hai môi trường như được hiển thị trong Bảng 1. Môi trường 1 được trang bị CPU và GPU yếu hơn Môi trường 2 để cho thấy hiệu quả của Fiddler trên một loạt cấu hình phần cứng rộng. Không môi trường nào có đủ khả năng bộ nhớ GPU để lưu trữ tất cả các tham số mô hình. Hàng "Số lượng Experts trên GPU" cho thấy số lượng tối đa

7

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Bảng 1: Thiết lập đánh giá
Môi trường 1                          Môi trường 2
GPU          Quadro RTX 6000 (NVIDIA, b)      RTX 6000 Ada (NVIDIA, a)
Bộ nhớ GPU   24576MiB                         49140MiB
PCIe         Gen3 x16 (32GB/s)               Gen4 x16 (64GB/s)
CPU          Intel(R) Xeon(R) Gold 6126 (48 core)  Intel Xeon Platinum 8480+ (112 core)
Số lượng Experts trên GPU  56/256             125/256

experts có thể vừa trên bộ nhớ GPU trong số 256 experts (32 lớp × 8 experts/lớp), cho khả năng bộ nhớ.

Baselines. Cho baselines, chúng tôi đánh giá DeepSpeed-MII phiên bản v0.2.3 (Microsoft), Mixtral-Offloading (Eliseev & Mazur, 2023), và llama.cpp phiên bản b2956 (ggml authors, 2023).

Cho DeepSpeed-MII, chúng tôi kích hoạt tối ưu hóa ZeRO-Infinity (Rajbhandari et al., 2021) để nó offload các tham số mô hình vào bộ nhớ CPU và tải chúng từ CPU sang GPU động trong quá trình suy luận khi cần thiết. Chúng tôi kích hoạt pin_memory trong cấu hình để sử dụng bộ nhớ CPU được khóa trang, cải thiện hiệu suất đọc/ghi từ bộ nhớ CPU và giảm phân mảnh bộ nhớ.

Mixtral-Offloading ban đầu chỉ hỗ trợ phiên bản lượng tử hóa của mô hình Mixtral-8x7B theo mặc định. Để so sánh công bằng, chúng tôi mở rộng Mixtral-Offloading để hỗ trợ chạy phiên bản gốc của mô hình với độ chính xác 16-bit. Mixtral-Offloading cung cấp tham số offload_per_layer để xác định có bao nhiêu experts trong mỗi lớp expert để offload vào bộ nhớ CPU. Chúng tôi đặt tham số offload_per_layer thành 7 cho Môi trường 1 và 5 cho Môi trường 2 vì đây là cấu hình tốt nhất cho các môi trường chúng tôi kiểm tra. Cho llama.cpp, chúng tôi đặt các tham số ngl điều khiển số lượng lớp được thực thi trong GPU thành 8 cho Môi trường 1 và 16 cho Môi trường 2.

Metric. Chúng tôi đánh giá hiệu suất của Fiddler so với baselines trong ba tình huống khác nhau phục vụ một yêu cầu duy nhất: a độ trễ end-to-end¹ với các độ dài khác nhau của token đầu vào và đầu ra, b xử lý prefill cho đầu vào bối cảnh dài, và c độ trễ end-to-end của beam search với các độ rộng khác nhau. Những metric này phản ánh các trường hợp sử dụng quan trọng: đầu vào bối cảnh dài được sử dụng cho học trong bối cảnh hoặc tạo ra tăng cường truy xuất (Dong et al., 2022; Gao et al., 2023), và beam search được sử dụng để nâng cao chất lượng của các token được tạo ra (von Platen, 2023).

Chúng tôi báo cáo tốc độ suy luận được đo bằng token mỗi giây cho a và c (số token được tạo ra chia cho độ trễ end-to-end), và Thời gian Đến Token Đầu tiên (TTFT) cho b. Để đánh giá với N token đầu vào, chúng tôi chọn ngẫu nhiên các mẫu từ ShareGPT với N token hoặc nhiều hơn của prompt và sử dụng N token đầu tiên.

Cho a, độ dài đầu vào trong số [32, 64, 128, 256], và độ dài đầu ra trong số [64, 128, 256, 512]. Độ dài đầu vào cho b trong số [512, 1024, 2048, 4096]. Chúng tôi đặt độ rộng beam search cho c trong số [4, 8, 12, 16] với độ dài đầu vào 32 và độ dài đầu ra 64. Cho beam search, chúng tôi chỉ so sánh Fiddler với llama.cpp vì các baselines khác không hỗ trợ suy luận beam search.

4.2 KẾT QUẢ

Hình 4 cho thấy hiệu suất end-to-end của bốn phương pháp trong hai môi trường. Trung bình, trên tất cả các cấu hình và môi trường, Fiddler vượt trội so với baseline tốt nhất, llama.cpp, Fiddler đạt được hiệu suất nhanh hơn 1.26 lần trung bình trên các độ dài đầu vào/đầu ra và môi trường khác nhau.

Hình 5 cho thấy TTFT cho prefill bối cảnh dài. Trong trường hợp này, các phương pháp dựa trên offloading (DeepSpeed-MII và (Eliseev & Mazur, 2023)) tốt hơn llama.cpp. Tuy nhiên, Fiddler vẫn cho thấy hiệu suất tốt hơn bất kỳ phương pháp hiện có nào, vượt trội DeepSpeed-MII 1.07 lần và (Eliseev & Mazur, 2023) 1.65 lần trung bình trên các cấu hình khác nhau. Hình 6 cho thấy độ trễ end-to-end

¹Chúng tôi định nghĩa độ trễ end-to-end là thời gian từ lúc yêu cầu suy luận được nhận đến việc tạo ra token cuối cùng, bao gồm cả thời gian prefill và decode.

8

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Hình 4: So sánh hiệu suất end-to-end theo số token được tạo ra mỗi giây (tình huống a, cao hơn là tốt hơn), với 15 cấu hình độ dài đầu vào/đầu ra khác nhau. Tập thanh bên phải nhất cho thấy trung bình của 15 cấu hình.

của suy luận beam search với các độ rộng tìm kiếm khác nhau, so với llama.cpp. Trung bình, Fiddler đạt được hiệu suất tốt hơn 11.57 lần so với llama.cpp.

Hình 5: So sánh hiệu suất theo TTFT (tình huống b, thấp hơn là tốt hơn), với 4 cấu hình độ dài đầu vào khác nhau. Tập thanh bên phải nhất cho thấy trung bình của 4 độ dài khác nhau.

Hình 6: So sánh hiệu suất cho suy luận beam search được đo bằng số token được tạo ra mỗi giây (tình huống c, cao hơn là tốt hơn), với độ dài đầu vào 32 và độ dài đầu ra 64. Tập thanh bên phải nhất cho thấy trung bình của 4 độ rộng beam search.

Những kết quả này cho thấy rằng Fiddler hoạt động tốt hơn trong một loạt ứng dụng rộng so với các hệ thống hiện có. Lợi ích chủ yếu đến từ khả năng của Fiddler xác định chiến lược thực thi động dựa trên các hiệu ứng batching của CPU và GPU và đặt experts dựa trên profile độ phổ biến. Đáng chú ý, trong khi các hệ thống hiện có cho thấy các sự đánh đổi khác nhau (ví dụ, các cách tiếp cận dựa trên offloading xuất sắc trong các tình huống prefill dài và các phương pháp như llama.cpp hoạt động tốt với độ trễ batch đơn), hệ thống của chúng tôi tích hợp các lợi thế của cả hai, đạt được kết quả cân bằng và hiệu quả trong các điều kiện đa dạng.

5 KẾT LUẬN

Bài báo này đề xuất Fiddler, một hệ thống suy luận tiết kiệm tài nguyên cho các mô hình MoE với tài nguyên GPU hạn chế. Fiddler sử dụng chiến lược kiến trúc tính toán đa dạng của tài nguyên CPU và GPU bằng cách xác định chiến lược thực thi tối ưu. Fiddler đạt được hiệu suất tốt hơn trong tất cả các tình huống phổ biến cho suy luận cục bộ trong khi các hệ thống tiên tiến chỉ được tối ưu hóa cho một phần của chúng. Đánh giá của chúng tôi cho thấy rằng so với các hệ thống tiên tiến, Fiddler đạt được tăng tốc 1.26 lần trong suy luận batch đơn, 1.30 lần trong xử lý prefill dài, và 11.57 lần trong suy luận beam search.

9

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

TÀI LIỆU THAM KHẢO

Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024.

Mistral AI. Cheaper, better, faster, stronger, 2024. URL https://mistral.ai/news/mixtral-8x22b/.

Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. Llm in a flash: Efficient large language model inference with limited memory. arXiv preprint arXiv:2312.11514, 2023.

Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https://github.com/nomic-ai/gpt4all, 2023.

Emery Berger and Ben Zorn. Ai software should be more like plain old software, 2024. URL https://www.sigarch.org/ai-software-should-be-more-like-plain-old-software/.

Lequn Chen. Dissecting batching effects in gpt inference, 2023. URL https://le.qun.ch/en/blog/2023/05/13/transformer-batching/.

Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024.

Databricks. Introducing dbrx: A new state-of-the-art open llm, 2024. URL https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm.

DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.

Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547–5569. PMLR, 2022.

Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3–11, 2018.

Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.

William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1): 5232–5270, 2022.

Elias Frantar and Dan Alistarh. Qmoe: Practical sub-1-bit compression of trillion-parameter models. arXiv preprint arXiv:2310.16795, 2023.

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.

The ggml authors. llama.cpp, 2023. URL https://github.com/ggerganov/llama.cpp.

Ettore Di Giacinto. Localai. https://github.com/mudler/LocalAI, 2023.

10

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pp. 22137–22176. PMLR, 2023.

Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. Llm-rec: Personalized recommendation via prompting large language models. arXiv preprint arXiv:2307.15780, 2023.

Iván Martínez Toro, Daniel Gallego Vico, and Pablo Orgaz. PrivateGPT, 2023. URL https://github.com/imartinez/privateGPT.

Microsoft. Deepspeed-mii. https://github.com/microsoft/DeepSpeed-MII.

Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, and Mehrdad Farajtabar. Relu strikes back: Exploiting activation sparsity in large language models. arXiv preprint arXiv:2310.04564, 2023.

NVIDIA. Nvidia rtx 6000 ada generation. https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/proviz-print-rtx6000-datasheet-web-2504660.pdf, a.

NVIDIA. Nvidia quadro rtx 6000 pcie server card. https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/quadro-product-literature/NVIDIA-Quadro-RTX-6000-PCIe-Server-Card-PB-FINAL-1219.pdf, b.

Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.

Dylan Patel and Afzal Ahmad. The inference cost of search disruption – large language model cost analysis, 2023. URL https://www.semianalysis.com/p/the-inference-cost-of-search-disruption.

Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning, 2021.

Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. In International Conference on Machine Learning, pp. 18332–18346. PMLR, 2022.

ShareGPT. Sharegpt. https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered.

Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865, 2023.

Snowflake. Snowflake arctic: The best llm for enterprise ai — efficiently intelligent, truly open, 2024. URL https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/.

Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model serving with a consumer-grade gpu. arXiv preprint arXiv:2312.12456, 2023.

11

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

SparseLLM. Relullama-70b. https://huggingface.co/SparseLLM/ReluLLaMA-70B.

Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference, 2024. URL https://arxiv.org/abs/2406.10774.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Patrick von Platen. How to generate text: using different decoding methods for language generation with transformers, 2023. URL https://huggingface.co/blog/how-to-generate.

xAI. Open release of grok-1, 2024. URL https://x.ai/blog/grok-os.

Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early effort on open mixture-of-experts language models. 2024a.

Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh Marina. Moe-infinity: Activation-aware expert offloading for efficient moe serving. arXiv preprint arXiv:2401.14361, 2024b.

Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, and Luis Ceze. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. URL https://arxiv.org/abs/2501.01005.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2024. URL https://arxiv.org/abs/2309.11998.

Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei Gao, Qinyu Xu, Tian Tang, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Stephanie Wang, Arvind Krishnamurthy, and Baris Kasikci. Nanoflow: Towards optimal large language model serving throughput. CoRR, abs/2408.12757, 2024. doi: 10.48550/ARXIV.2408.12757. URL https://doi.org/10.48550/arXiv.2408.12757.

Kan Zhu, Tian Tang, Qinyu Xu, Yile Gu, Zhichen Zeng, Rohan Kadekodi, Liangyu Zhao, Ang Li, Arvind Krishnamurthy, and Baris Kasikci. Tactic: Adaptive sparse attention with clustering and distribution fitting for long-context llms, 2025. URL https://arxiv.org/abs/2502.12216.

A MICROBENCHMARKS

Trong phần này, chúng tôi hiển thị kết quả của các microbenchmarks. Hình 7 cho thấy độ trễ của các workload sau:

• W copy: Chuyển trọng số của một expert từ bộ nhớ CPU sang bộ nhớ GPU
• A copy: Chuyển một activation từ bộ nhớ GPU sang bộ nhớ CPU
• GPU N: Thực thi một expert tại GPU với kích thước đầu vào N (loại trừ thời gian chuyển trọng số từ CPU)
• CPU N: Thực thi một expert tại CPU với kích thước đầu vào N

12

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Đối với mỗi giá trị, chúng tôi thực thi workload 32 lần (một lần cho mỗi lớp của Mixtral-8x7B) và trình bày kết quả trung bình và độ lệch chuẩn.

Khi các tác vụ được thực thi trên GPU, độ trễ để chuyển trọng số từ bộ nhớ CPU sang bộ nhớ GPU dài hơn khoảng 2-5 lần so với thời gian tính toán thực tế. Độ trễ tính toán trên GPU vẫn phần lớn không đổi bất kể kích thước batch. Một ngoại lệ xảy ra trong Môi trường 1 khi kích thước batch là 1, vì PyTorch sử dụng các triển khai khác nhau cho các tình huống batch đơn và batch nhiều. Tuy nhiên, sự khác biệt này là nhỏ (khoảng 10%) so với tổng độ trễ, bao gồm cả chuyển trọng số. Do đó, chúng tôi mô hình hóa độ trễ GPU như một hằng số trong Phần 3.3.

Trên CPU, độ trễ thực thi thường tăng tuyến tính với kích thước của batch đầu vào. Tuy nhiên, thời gian cần thiết để chuyển activations là không đáng kể (ít hơn 1% độ trễ với một đầu vào đơn). Do tác động tối thiểu này, mô hình của chúng tôi trong Phần 3.3 giả định rằng độ trễ CPU có mối quan hệ tuyến tính với số lượng đầu vào.

Hình 7: Kết quả microbenchmark đo độ trễ của việc chuyển trọng số hoặc activations giữa CPU và GPU, cũng như thực thi một lớp expert trên CPU hoặc GPU với các kích thước đầu vào khác nhau. Trục y được hiển thị trên thang logarit.

B PHÂN TÍCH TÍNH THƯA THỚT

Phần này phân tích tính thưa thớt trong các mô hình Mixtral-8x7B, làm nổi bật những thách thức của việc áp dụng các kỹ thuật tối ưu hóa dựa trên tính thưa thớt truyền thống từ các nghiên cứu trước (Song et al., 2023; Alizadeh et al., 2023). Những phương pháp này chủ yếu nhắm vào các LLM sử dụng hàm kích hoạt ReLU, làm vô hiệu các đầu vào âm và cho phép cắt tỉa các kênh với đầu ra luôn bằng không. Cách tiếp cận này tận dụng tính chất nhị phân của đầu ra ReLU—hoặc bằng không hoặc dương—cho phép nhận dạng đơn giản và loại bỏ các kênh không hoạt động, do đó tối ưu hóa hiệu quả tính toán mà không hy sinh thông tin quan trọng.

Ngược lại, các mô hình MoE tiên tiến thường sử dụng các hàm kích hoạt khác nhau, làm phức tạp việc áp dụng trực tiếp các chiến lược khai thác tính thưa thớt này. Ví dụ, Mixtral-8x7B sử dụng SiLU làm hàm kích hoạt. Không giống như ReLU, SiLU không cung cấp một ngưỡng rõ ràng bằng không để cắt tỉa, đòi hỏi một cách tiếp cận tinh vi hơn để tận dụng tính thưa thớt. Việc cắt tỉa các kênh không đủ gần với không có thể ảnh hưởng tiêu cực đến chất lượng đầu ra của mô hình.

Bảng 2 trình bày một phân tích các giá trị tuyệt đối sau hàm SiLU trên các lớp của mô hình Mixtral-8x7B. Phân tích này dựa trên dữ liệu từ 100 mẫu trong tập dữ liệu ShareGPT (ShareGPT), không phân biệt giữa các experts khác nhau trong các lớp giống hệt nhau. Dữ liệu cho thấy sự xuất hiện thường thấp của các giá trị gần bằng không. Cụ thể, đối với tất cả các lớp, tỷ lệ các kênh có giá trị tuyệt đối dưới 0.001 ít hơn 2%, và đối với 30 trong số 32 lớp, con số này thậm chí dưới 1%. Ngoài ra, trong 28 trong số 32 lớp, ít hơn 5% giá trị nhỏ hơn 0.01, và trong 24 lớp, ít hơn 30% giá trị dưới 0.1. Bất chấp sự biến đổi giữa các lớp, những kết quả này cộng lại cho thấy một thách thức đáng kể trong việc khai thác tính thưa thớt trong mô hình này sử dụng các cách tiếp cận từ các công trình trước. Ngược lại, được báo cáo (Liu et al., 2023) rằng hơn 90% giá trị sau hàm ReLU bằng không cho các lớp MLP của mô hình OPT (Zhang et al., 2022). Việc sử dụng tính thưa thớt trong các mô hình như Mixtral-8x7B để tăng tốc suy luận với mất chất lượng có thể chấp nhận vẫn là một hướng nghiên cứu thú vị cho tương lai.

13

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Bảng 2: Phân phối các giá trị tuyệt đối sau hàm SiLU của mô hình Mixtral-8x7B trên tất cả các lớp. Mỗi ô hiển thị tỷ lệ phần trăm các giá trị có giá trị tuyệt đối dưới một ngưỡng được chỉ định.

Lớp    <0.001  <0.01   <0.1    <1.0
1      1.75    17.17   93.89   100.00
2      1.21    11.95   85.08   100.00
3      0.92    9.10    74.80   99.99
4      0.71    7.06    63.69   99.99
5      0.50    5.00    49.67   99.95
6      0.41    4.08    41.60   99.93
7      0.36    3.56    36.66   99.91
8      0.30    2.97    31.04   99.88
9      0.29    2.90    29.96   99.86
10     0.27    2.73    28.25   99.80
11     0.24    2.37    24.65   99.74
12     0.24    2.43    25.15   99.69
13     0.24    2.36    24.55   99.65
14     0.22    2.22    23.05   99.53
15     0.20    2.02    21.03   99.32
16     0.18    1.78    18.61   99.14
17     0.15    1.53    16.14   98.91
18     0.15    1.50    15.86   98.58
19     0.13    1.33    14.24   98.15
20     0.12    1.19    12.94   97.95
21     0.11    1.09    12.04   97.86
22     0.10    0.97    11.09   97.96
23     0.10    1.02    11.58   97.61
24     0.10    1.02    11.72   97.36
25     0.09    0.95    11.55   97.34
26     0.10    0.95    11.91   97.05
27     0.09    0.95    12.19   96.72
28     0.09    0.89    12.28   96.76
29     0.08    0.86    13.89   95.86
30     0.09    1.03    15.16   94.02
31     0.12    1.37    16.65   92.12
32     0.36    2.73    20.27   89.64

C ĐỘ PHỔ BIẾN CỦA EXPERT

Hình 8 hiển thị bản đồ nhiệt minh họa độ phổ biến của việc lựa chọn expert trong mô hình Mixtral-8x7B. Tương tự như phân tích trong Phụ lục B, profile này được tạo ra bằng cách chạy suy luận trên các mẫu ngẫu nhiên từ tập dữ liệu ShareGPT và đếm số lượng token được định tuyến đến mỗi expert. Cường độ màu của mỗi ô đại diện cho tần suất lựa chọn expert, tương đương với số lượng token kích hoạt expert. Giá trị của expert phổ biến nhất được chuẩn hóa thành 1, với độ phổ biến của các experts khác được biểu thị như một tỷ lệ so với giá trị này.

Trong số 256 experts, giá trị trung bình là 0.71, với độ lệch chuẩn 0.08, phân vị thứ 25 là 0.67, và phân vị thứ 75 là 0.76. Mặc dù giá trị tối thiểu là 0.22, chỉ 15 experts có giá trị dưới 0.6, và 27 experts vượt quá 0.8, cho thấy một phân phối tương đối cân bằng.

14

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Trong Môi trường 1, việc chọn 56 experts phổ biến nhất trong số 256 tạo ra tỷ lệ hit dự kiến tối đa (khả năng trọng số của một expert có sẵn trong bộ nhớ GPU) là 25.2%, so với tối thiểu 18.7%. Lựa chọn ngẫu nhiên dẫn đến tỷ lệ hit trung bình là 56/256 = 21.9%. Trong Môi trường 2, với khả năng bộ nhớ GPU cho 125 experts, tỷ lệ hit dự kiến cho lựa chọn tốt nhất, tệ nhất, và ngẫu nhiên lần lượt là 53.0%, 44.6%, và 48.8%. Do đó, chúng ta có thể kết luận rằng việc đặt các experts phổ biến trên GPU có thể cải thiện tỷ lệ hit khoảng 3 đến 5 điểm phần trăm so với việc đặt ngẫu nhiên.

Hình 8: Bản đồ nhiệt trực quan hóa tần suất lựa chọn expert trong mô hình Mixtral-8x7B, sử dụng cường độ màu để đại diện cho tần suất, với expert phổ biến nhất được chuẩn hóa thành 1.

D NGHIÊN CỨU ĐỘ NHẠY TRÊN TẬP DỮ LIỆU

Hình 9: So sánh hiệu suất end-to-end theo số token được tạo ra mỗi giây (giống như tình huống a, cao hơn là tốt hơn), với hai tập dữ liệu khác nhau. Tập thanh bên phải nhất cho thấy trung bình của 15 cấu hình.

Trong phần này, chúng tôi phân tích độ nhạy của hiệu suất Fiddler trên các tập dữ liệu đầu vào vì hành vi định tuyến của mô hình MoE có thể bị ảnh hưởng bởi đặc điểm của phân phối dữ liệu đầu vào. Hình 9 so sánh hiệu suất của Fiddler với các tập dữ liệu ShareGPT (ShareGPT) và LMSYS-Chat-1M (Zheng et al., 2024), cả hai đều là tập dữ liệu cuộc trò chuyện giữa con người và chatbot. Ngoài tập dữ liệu, thiết lập thí nghiệm giống như tình huống a trong §4, và chúng tôi sử dụng Môi trường 1. Trung bình, Fiddler vượt trội hệ thống tiên tiến (llama.cpp) 1.81 lần cho tập dữ liệu ShareGPT và 1.56 lần cho tập dữ liệu LMSYS. Những kết quả này cho thấy tính mạnh mẽ của Fiddler đối với các phân phối đầu vào khác nhau.

E TÍNH ÁP DỤNG CỦA Fiddler CHO CÁC MÔ HÌNH KHÁC NHAU

Trong §4, chúng tôi đánh giá mô hình Mixtral-8x7B vì đây là mô hình MoE duy nhất được hỗ trợ bởi tất cả các baselines. Tuy nhiên, hệ thống của chúng tôi được thiết kế để không phụ thuộc vào mô hình trong họ các mô hình MoE. Để chứng minh điều này, Hình 10 trình bày hiệu suất của Fiddler cho mô hình Phi-3.5-MoE (Abdin et al., 2024). Chúng tôi hiển thị so sánh với DeepSpeed-MII, vì đây là hệ thống baseline duy nhất hỗ trợ mô hình này.

15

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Hình 10: So sánh hiệu suất end-to-end của mô hình Phi-3.5-MoE theo số token được tạo ra mỗi giây (giống như tình huống a, cao hơn là tốt hơn.)

Kết quả nhất quán với mô hình Mixtral-8x7B, và Fiddler vượt trội DeepSpeed-MII với 6.5 lần trung bình. Nó cho thấy tính áp dụng của Fiddler ngoài mô hình Mixtral-8x7B.

F PHÂN TÍCH CHI TIẾT ĐỘ TRỄ

Hình 4 cho thấy hiệu suất được đo bằng số token được tạo ra chia cho độ trễ end-to-end. Để bổ sung cho dữ liệu, Hình 11 và Hình 12 cho thấy Thời gian Đến Token Đầu tiên (TTFT) và Độ trễ Giữa các Token (ITL) riêng biệt. Về TTFT, Fiddler cho thấy tăng tốc trung bình 1.13 lần trên các độ dài đầu vào khác nhau trong tất cả baselines trong 2 môi trường. Về ITL, Fiddler cho thấy tăng tốc trung bình 1.43 lần trên các độ dài đầu vào và đầu ra khác nhau trong tất cả baselines trong 2 môi trường.

Hình 11: So sánh Thời-Gian-Đến-Token-Đầu-tiên (TTFT) (thấp hơn là tốt hơn).

16

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2025

Hình 12: So sánh Độ trễ Giữa các Token (ITL) (thấp hơn là tốt hơn).

17
