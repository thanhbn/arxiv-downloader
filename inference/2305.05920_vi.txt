# Fast Distributed Inference Serving for Large Language Models
# Phục vụ Suy luận Phân tán Nhanh cho các Mô hình Ngôn ngữ Lớn
Bingyang Wu∗Yinmin Zhong∗Zili Zhang∗Shengyu Liu
Fangyue Liu Yuanhang Sun Gang Huang Xuanzhe Liu Xin Jin
Peking University

## Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) cung cấp năng lượng cho thế hệ mới của các ứng dụng AI tương tác được minh họa bởi ChatGPT. Bản chất tương tác của các ứng dụng này đòi hỏi độ trễ thấp cho việc suy luận LLM. Các hệ thống phục vụ LLM hiện tại sử dụng xử lý run-to-completion cho các công việc suy luận, điều này gặp phải vấn đề chặn đầu hàng đợi và độ trễ cao.

Chúng tôi trình bày FastServe, một hệ thống phục vụ suy luận phân tán cho LLM. FastServe khai thác mẫu tự hồi quy của suy luận LLM để cho phép preemption ở mức độ chi tiết của mỗi token đầu ra. FastServe sử dụng lập lịch preemptive để tối thiểu hóa độ trễ với một bộ lập lịch Multi-Level Feedback Queue skip-join mới. Dựa trên thiết lập semi information-agnostic mới của suy luận LLM, bộ lập lịch tận dụng thông tin độ dài đầu vào để gán một hàng đợi ban đầu thích hợp cho mỗi công việc đến tham gia. Các hàng đợi có mức độ ưu tiên cao hơn hàng đợi được tham gia sẽ bị bỏ qua để giảm số lần hạ cấp. Chúng tôi thiết kế một cơ chế quản lý bộ nhớ GPU hiệu quả mà chủ động offload và upload trạng thái trung gian giữa bộ nhớ GPU và bộ nhớ host cho suy luận LLM. Chúng tôi xây dựng một hệ thống prototype của FastServe và kết quả thí nghiệm cho thấy so với giải pháp tiên tiến nhất vLLM, FastServe cải thiện throughput lên đến 31.4× và 17.9× dưới cùng các yêu cầu độ trễ trung bình và tail tương ứng.

## 1 Giới thiệu
Những tiến bộ trong các mô hình ngôn ngữ lớn (LLM) mở ra những khả năng mới trong nhiều lĩnh vực đa dạng và kích hoạt thế hệ mới của các ứng dụng AI tương tác. Ứng dụng đáng chú ý nhất là ChatGPT [1] cho phép người dùng tương tác với một tác nhân AI theo cách đối thoại để giải quyết các nhiệm vụ từ dịch thuật ngôn ngữ đến kỹ thuật phần mềm. Khả năng ấn tượng của ChatGPT khiến nó trở thành một trong những ứng dụng phát triển nhanh nhất trong lịch sử [2]. Nhiều tổ chức theo xu hướng này để phát hành LLM và các ứng dụng giống ChatGPT, chẳng hạn như New Bing từ Microsoft [3], Gemini từ Google [4], Claude-3 [5] từ Anthropic, Qwen [6] từ Alibaba, v.v.

Phục vụ suy luận là rất quan trọng đối với các ứng dụng AI tương tác dựa trên LLM. Để cung cấp trải nghiệm người dùng hấp dẫn, bản chất tương tác của các ứng dụng này đòi hỏi độ trễ thấp cho suy luận LLM. Ví dụ, người dùng mong đợi đầu vào của họ đến ChatGPT được phản hồi ngay lập tức. Tuy nhiên, kích thước và độ phức tạp của LLM gây ra áp lực to lớn lên cơ sở hạ tầng phục vụ suy luận cơ bản. Các doanh nghiệp cung cấp các cụm lớn và đắt tiền bao gồm các bộ tăng tốc như GPU và TPU để xử lý các công việc suy luận LLM.

Suy luận LLM có những đặc điểm riêng biệt (§2) khác với suy luận mô hình mạng nơ-ron sâu (DNN) khác như ResNet [7]. Các công việc suy luận DNN thường mang tính xác định và có thể dự đoán cao [8], tức là thời gian thực thi của một công việc suy luận chủ yếu được quyết định bởi mô hình và phần cứng. Ví dụ, các hình ảnh đầu vào khác nhau có thời gian thực thi tương tự trên cùng mô hình ResNet trên một GPU nhất định.

Ngược lại, các công việc suy luận LLM có một mẫu tự hồi quy đặc biệt. Một công việc suy luận LLM chứa nhiều lần lặp. Mỗi lần lặp tạo ra một token đầu ra, và mỗi token đầu ra được thêm vào đầu vào để tạo ra token đầu ra tiếp theo trong lần lặp tiếp theo. Thời gian thực thi phụ thuộc vào cả độ dài đầu vào và độ dài đầu ra, trong đó độ dài đầu ra không được biết trước.

Các giải pháp phục vụ suy luận hiện có như Clockwork [8] và Shepherd [9] nhắm đến suy luận mô hình xác định như ResNet [7]. Chúng dựa vào profiling thời gian thực thi chính xác để lập lịch các công việc, điều này không hoạt động đối với suy luận LLM có thời gian thực thi biến đổi. Orca [10], được thiết kế cho suy luận LLM, giới thiệu lập lịch cấp độ lần lặp để động thêm công việc mới hoặc loại bỏ những công việc đã hoàn thành ở cuối mỗi lần lặp. vLLM [11] tiếp tục giới thiệu PagedAttention để giảm phân mảnh bộ nhớ của trạng thái trung gian của các công việc suy luận LLM. Tuy nhiên, cả hai đều sử dụng first-come-first-served (FCFS) để xử lý các công việc suy luận. Một khi một công việc được lập lịch, nó chạy cho đến khi hoàn thành. Do bộ nhớ GPU hạn chế và yêu cầu độ trễ nghiêm ngặt, batch xử lý hiện tại không thể được mở rộng với số lượng tùy ý các công việc đến, do đó một công việc dài có thể chặn những công việc đến, được gọi là chặn đầu hàng đợi [12]. Vấn đề đặc biệt nghiêm trọng đối với các công việc suy luận LLM. Một công việc suy luận LLM lớn, tức là với độ dài đầu vào và đầu ra dài, sẽ chạy trong thời gian dài để chặn các công việc ngắn đến.

Hình 1 chứng minh vấn đề dựa trên các bộ dữ liệu thực tế. Thiết lập chi tiết ở §6. Lý tưởng là nếu độ dài đầu vào và độ dài đầu ra của các công việc đều giống nhau giữa các công việc, hầu như không có độ trễ xếp hàng ngay cả khi tải đạt công suất (load≈1), như hai cột đầu tiên trong Hình 1 gợi ý. Tuy nhiên, các bộ dữ liệu LLM như ShareGPT [13] và Alpaca [14] cho thấy rằng khối lượng công việc thực tế có tính lệch cao. Phân phối đuôi dài của độ dài đầu ra dẫn đến độ trễ xếp hàng dài. Hình 1 cho thấy rằng lên đến 90% tổng độ trễ là độ trễ xếp hàng đối với các bộ dữ liệu thực tế. Trong trường hợp này, tối ưu hóa thời gian thực thi là không đủ, bởi vì nó chỉ đóng góp một phần nhỏ của độ trễ end-to-end. Thay vào đó, chúng ta cần tối ưu hóa độ trễ xếp hàng, đây là yếu tố đóng góp chính cho độ trễ end-to-end.

Chúng tôi trình bày FastServe, một hệ thống phục vụ suy luận phân tán cho LLM. FastServe khai thác mẫu tự hồi quy của suy luận LLM và lập lịch cấp độ lần lặp để cho phép preemption ở mức độ chi tiết của mỗi token đầu ra. Cụ thể, khi một công việc được lập lịch hoàn thành việc tạo ra một token đầu ra, FastServe có thể quyết định tiếp tục công việc này hay preempt nó với một công việc khác trong hàng đợi. Điều này cho phép FastServe sử dụng lập lịch preemptive để loại bỏ vấn đề chặn đầu hàng đợi và tối thiểu hóa độ trễ.

Cốt lõi của FastServe là một bộ lập lịch Multi-Level Feedback Queue (MLFQ) skip-join mới. MLFQ là một phương pháp cổ điển để tối thiểu hóa độ trễ trong các thiết lập information-agnostic [15]. Mỗi công việc đầu tiên vào hàng đợi có mức độ ưu tiên cao nhất, và bị hạ cấp xuống hàng đợi mức độ ưu tiên tiếp theo nếu nó không hoàn thành sau một ngưỡng. Sự khác biệt chính giữa suy luận LLM và thiết lập cổ điển là suy luận LLM là semi information-agnostic, tức là trong khi độ dài đầu ra không được biết trước, độ dài đầu vào đã được biết. Do mẫu tự hồi quy của suy luận LLM, độ dài đầu vào quyết định thời gian thực thi để tạo ra token đầu ra đầu tiên, có thể lớn hơn đáng kể so với các token sau (§4.1). Đối với đầu vào dài và đầu ra ngắn, thời gian thực thi của token đầu ra đầu tiên chi phối toàn bộ công việc. Chúng tôi tận dụng đặc điểm này để mở rộng MLFQ cổ điển với skip-join. Thay vì luôn vào hàng đợi có mức độ ưu tiên cao nhất, mỗi công việc đến tham gia một hàng đợi thích hợp bằng cách so sánh thời gian thực thi của token đầu ra đầu tiên với quantum của các hàng đợi. Các hàng đợi có mức độ ưu tiên cao hơn được bỏ qua để giảm số lần hạ cấp.

Lập lịch preemptive tạo ra overhead bộ nhớ phụ để duy trì trạng thái trung gian cho các công việc đã bắt đầu nhưng chưa hoàn thành. LLM duy trì một key-value cache cho mỗi lớp Transformer để lưu trữ trạng thái trung gian (§2.2). Trong FCFS, cache chỉ cần lưu trữ trạng thái trung gian của các công việc được lập lịch trong batch xử lý, bị giới hạn bởi kích thước batch tối đa. Nhưng trong MLFQ, nhiều công việc có thể đã bắt đầu nhưng bị hạ cấp xuống các hàng đợi có mức độ ưu tiên thấp hơn. Cache phải duy trì trạng thái trung gian cho tất cả các công việc đã bắt đầu nhưng chưa hoàn thành trong MLFQ. Cache có thể tràn, do kích thước lớn của trạng thái trung gian và dung lượng bộ nhớ hạn chế của GPU. Một cách ngây thơ, bộ lập lịch có thể tạm dừng bắt đầu các công việc mới khi cache đầy, nhưng điều này lại tạo ra chặn đầu hàng đợi. Thay vào đó, chúng tôi thiết kế cơ chế quản lý bộ nhớ GPU chủ động mà chủ động offload trạng thái của các công việc trong các hàng đợi có mức độ ưu tiên thấp xuống bộ nhớ host khi cache gần đầy, và upload trạng thái trở lại khi các công việc này sắp được lập lịch. Chúng tôi sử dụng pipelining và các hoạt động bộ nhớ không đồng bộ để cải thiện hiệu quả.

Đối với các mô hình lớn không vừa trong một GPU, FastServe tận dụng các chiến lược song song hóa bao gồm tensor parallelism [16] và pipeline parallelism [17] để thực hiện phục vụ suy luận phân tán với nhiều GPU (§4.3). Bộ lập lịch chạy nhiều batch công việc đồng thời trong một pipeline để tối thiểu hóa các bubble pipeline. Trình quản lý key-value cache phân chia key-value cache trên nhiều GPU, và xử lý việc hoán đổi giữa bộ nhớ GPU và bộ nhớ host một cách phân tán.

Chúng tôi triển khai một hệ thống prototype của FastServe và tích hợp nhiều kỹ thuật tối ưu hóa như PagedAttention [11]. Chúng tôi đánh giá FastServe trên các cấu hình khác nhau của LLM với khối lượng công việc suy luận LLM thực tế. Đặc biệt, chúng tôi đánh giá hiệu suất end-to-end của FastServe cho OPT-175B [18] (một LLM nguồn mở tương tự như mô hình GPT-3 lớn nhất) trên 16 GPU NVIDIA A100. Các thí nghiệm cho thấy rằng so với giải pháp tiên tiến nhất vLLM [11], FastServe cải thiện throughput lên đến 31.4× và 17.9× dưới cùng các yêu cầu độ trễ trung bình và tail tương ứng.

## 2 Bối cảnh và Động lực

### 2.1 Suy luận LLM và Ứng dụng

**Suy luận LLM.** Họ LLM [18-20] bao gồm một tập hợp các mô hình ngôn ngữ được xây dựng trên nền tảng Transformer [21]. Suy luận LLM hoạt động theo cách tự hồi quy, trong đó đầu vào, thường được gọi là prompt, được xử lý như một chuỗi các token. Sau đó nó tạo ra một phân phối xác suất cho token tiếp theo được chọn. Cơ chế xử lý và lựa chọn cho mỗi token đầu ra này được gọi là một lần lặp. Một khi được huấn luyện với một kho ngữ liệu khổng lồ, LLM có khả năng thực hiện các nhiệm vụ ngôn ngữ chất lượng cao.

Ví dụ, với đầu vào "knowledge is", nó có thể gán xác suất cao hơn cho "power" so với "apple". Token đầu ra đầu tiên được thêm vào prompt ban đầu và đưa vào LLM để tạo token tiếp theo. Quá trình này tiếp tục cho đến khi một token <EOS> duy nhất, tượng trưng cho kết thúc chuỗi, được tạo ra, hoặc đạt đến độ dài đầu ra tối đa được xác định trước. Quá trình suy luận này khác biệt rõ rệt so với các mô hình khác như ResNet, nơi thời gian thực thi thường mang tính xác định và có thể dự đoán [8]. Trong khi thời gian thực thi của mỗi lần lặp (tạo ra một token đầu ra) có thể được xác định dựa trên kiến trúc mô hình và phần cứng, tổng số lần lặp (tức là độ dài chuỗi đầu ra) vẫn chưa biết và khó dự đoán vì nó phụ thuộc vào ngữ nghĩa của công việc. Các bộ dữ liệu thực tế được thu thập từ các cuộc trò chuyện với LLM, như ShareGPT [13] và Alpaca [14], thể hiện phân phối đuôi dài của độ dài đầu ra và độ dài đầu vào [11]. Do đó, SRPT không thể được áp dụng trực tiếp cho suy luận LLM để tối thiểu hóa độ trễ trung bình.

**Ứng dụng LLM.** Nhiệm vụ chính của LLM là dự đoán token tiếp theo cho một prompt đầu vào. Sử dụng kỹ thuật prompt engineering [19], các nhiệm vụ NLP downstream có thể được tái cấu trúc thành các nhiệm vụ tạo sinh dựa trên LLM. Cụ thể, đối với một nhiệm vụ dịch thuật, người ta có thể bắt đầu prompt bằng cách thêm "Translate the following English text into French text" trước văn bản gốc. Bằng cách này, LLM sẽ được hướng dẫn để tạo ra văn bản tiếng Pháp được dịch mong muốn để phản hồi.

ChatGPT [1] là một ứng dụng dựa trên LLM đại diện. Sau khi fine-tuning có giám sát cho nhiệm vụ đối thoại và một quy trình alignment sử dụng Reinforcement Learning from Human Feedback (RLHF) trên mô hình GPT gốc [19], ChatGPT tạo điều kiện cho các cuộc trò chuyện tương tác với một tác nhân AI, cho phép người dùng giải quyết một loạt các nhiệm vụ. Những nhiệm vụ này bao gồm dịch thuật, hỏi đáp, tóm tắt, cũng như các công việc phức tạp hơn như phân tích cảm xúc, viết sáng tạo và giải quyết vấn đề cụ thể theo lĩnh vực. Mặc dù có khả năng đáng chú ý, bản chất tương tác của ChatGPT gây ra áp lực đáng kể lên cơ sở hạ tầng phục vụ suy luận cơ bản. Do nhu cầu phản hồi nhanh, việc giữ độ trễ thấp là rất quan trọng để đảm bảo hiệu suất của các ứng dụng tương tác giống ChatGPT.

### 2.2 Hệ thống Phục vụ Suy luận

Hầu hết các hệ thống phục vụ suy luận hiện có, như Tensorflow Serving [22] và Triton Inference Server [23], đều agnostic với các mô hình DNN. Chúng phục vụ như một lớp trừu tượng trên engine thực thi cơ bản, xếp hàng các công việc đến, điều phối các công việc đến tài nguyên máy tính có sẵn, và trả về kết quả cho khách hàng. Để tận dụng đầy đủ GPU, chúng thường batch các công việc lại với nhau để xử lý song song. Với batching, các tensor đầu vào từ nhiều công việc được nối lại và đưa vào mô hình như một khối. Mặc dù có lợi ích về việc sử dụng tốt hơn, nhược điểm của batching là overhead bộ nhớ cao hơn. Kích thước đáng kể của LLM và trạng thái trung gian khổng lồ hạn chế kích thước batch tối đa cho suy luận LLM [11].

Khi tính phổ biến của LLM tăng nhanh, các hệ thống phục vụ suy luận đã phát triển để bao gồm các tối ưu hóa cụ thể cho kiến trúc độc đáo và mẫu tạo sinh lặp lại của LLM. Phần chính của kiến trúc LLM là một stack các lớp Transformer, như được hiển thị trong Hình 2. Trong một lớp Transformer, module Masked Self-Attention là thành phần cốt lõi phân biệt nó với các kiến trúc khác như CNN. Trong mỗi lần lặp của suy luận LLM, đối với mỗi token, toán tử attention yêu cầu các keys và values của các token trước đó. Một triển khai ngây thơ, stateless luôn tính toán lại tất cả các keys và values trước đó trong mỗi lần lặp. Để tránh overhead tính toán lại như vậy, fairseq [24] đề xuất lưu các keys và values trong một key-value cache qua các lần lặp. Tối ưu hóa này chia quy trình suy luận thành hai giai đoạn riêng biệt: giai đoạn khởi tạo và giai đoạn giải mã. Hình 3 chứng minh việc sử dụng key-value cache trong cả hai giai đoạn.

Trong giai đoạn khởi tạo, tương ứng với lần lặp đầu tiên, LLM tạo ra key-value cache cho mỗi token trong prompt đầu vào. Trong giai đoạn giải mã tiếp theo, LLM chỉ cần tính toán query, key và value của một token mới được tạo, tận dụng key-value cache được tính trước để tạo điều kiện cho quá trình từng bước. Do đó, thời gian thực thi của các lần lặp trong giai đoạn giải mã thường nhỏ hơn so với giai đoạn khởi tạo, tức là lần lặp đầu tiên. Đáng chú ý là các hệ thống dựa trên Transformer khác, như HuggingFace [25] và FasterTransformer [26], cũng kết hợp kỹ thuật tối ưu hóa này, dẫn đến hiệu quả cải thiện trong quá trình suy luận.

Một tối ưu hóa quan trọng khác là lập lịch cấp độ lần lặp được đề xuất bởi Orca [10]. Lập lịch cấp độ công việc ngây thơ thực thi một batch công việc cho đến khi tất cả công việc hoàn thành. Các công việc hoàn thành sớm không thể được trả về cho khách hàng ngay lập tức, trong khi các công việc mới đến phải chờ đợi cho đến khi batch đang xử lý hoàn thành. Tuy nhiên, với lập lịch cấp độ lần lặp, engine thực thi chỉ thực hiện một lần lặp duy nhất trên batch tại một thời điểm, tạo ra một token đầu ra cho mỗi công việc. Sau mỗi lần lặp, các công việc đã hoàn thành rời khỏi batch, và các công việc mới đến có thể tham gia. Tuy nhiên, dung lượng bộ nhớ GPU giới hạn kích thước batch tối đa, và các service-level-objects (SLO) nghiêm ngặt của các ứng dụng tương tác cũng đóng vai trò trong việc xác định kích thước batch thích hợp. vLLM [11] tiếp tục cải thiện hiệu quả của suy luận LLM bằng cách giới thiệu PagedAttention, phân bổ key-value cache dần dần theo từng khối trong quá trình suy luận thay vì phân bổ cho độ dài đầu ra tối đa ngay từ đầu.

### 2.3 Cơ hội và Thách thức

**Cơ hội: lập lịch preemptive.** Hạn chế chính của các hệ thống phục vụ suy luận hiện có cho LLM [10, 26] là sự phụ thuộc vào lập lịch FCFS (First-Come-First-Serve) đơn giản và thực thi run-to-completion. Như được hiển thị trong Hình 1, phương pháp này dẫn đến chặn đầu hàng đợi nghiêm trọng. Độ trễ xếp hàng đóng góp lên đến 90% tổng độ trễ trong khối lượng công việc thực tế, điều này ảnh hưởng đáng kể đến hiệu suất của suy luận LLM. Để vượt qua thách thức này, lập lịch preemptive có thể được sử dụng. Trong suy luận LLM, mỗi công việc bao gồm nhiều lần lặp, với mỗi lần lặp tạo ra một token đầu ra. Cơ hội nằm ở việc khai thác mẫu tự hồi quy này để cho phép preemption ở cấp độ lần lặp, có nghĩa là một công việc có thể bị preempt khi nó hoàn thành việc tạo ra một token đầu ra cho một công việc khác. Tận dụng khả năng preemption, bộ lập lịch có thể sử dụng các chính sách lập lịch preemptive để ngăn chặn chặn đầu hàng đợi và tối ưu hóa độ trễ trung bình. Tuy nhiên, lập lịch preemptive đặt ra hai thách thức cho hệ thống suy luận LLM hiện có.

**Thách thức 1: kích thước công việc biến đổi.** Shortest Remaining Processing Time (SRPT) [27] là một chính sách lập lịch preemptive được sử dụng rộng rãi để tối thiểu hóa độ trễ trung bình. Tuy nhiên, việc áp dụng SRPT cho suy luận LLM đặt ra thách thức do bản chất lặp lại của LLM. Không giống như các nhiệm vụ dự đoán một lần như phân loại hình ảnh, suy luận LLM bao gồm nhiều lần lặp. Trong khi thời gian thực thi cho một lần lặp (tạo ra một token đầu ra) có thể được xác định dựa trên kiến trúc mô hình và phần cứng, tổng số lần lặp (tức là độ dài chuỗi đầu ra) vẫn chưa biết và khó dự đoán vì nó phụ thuộc vào ngữ nghĩa của công việc. Các bộ dữ liệu thực tế được thu thập từ các cuộc trò chuyện với LLM, như ShareGPT [13] và Alpaca [14], thể hiện phân phối đuôi dài của độ dài đầu ra và độ dài đầu vào [11]. Do đó, SRPT không thể được áp dụng trực tiếp cho suy luận LLM để tối thiểu hóa độ trễ trung bình.

**Thách thức 2: overhead bộ nhớ GPU.** Các chính sách lập lịch preemptive tạo ra việc tiêu thụ bộ nhớ GPU bổ sung trong quá trình suy luận LLM, không giống như FCFS với run-to-completion, chỉ cần duy trì key-value cache cho các công việc đang diễn ra. Ngược lại, lập lịch preemptive phải giữ key-value cache trong bộ nhớ GPU cho tất cả các công việc bị preempt ở trạng thái pending, để được sử dụng cho việc tạo token trong tương lai. Key-value cache này tiêu thụ một lượng lớn bộ nhớ GPU, dẫn đến các thách thức tiềm ẩn. Ví dụ, một công việc duy nhất của OPT 175B với độ dài chuỗi đầu vào là 512 yêu cầu ít nhất 2.3 GB bộ nhớ cho key-value cache (§4.2). Do dung lượng bộ nhớ GPU khan hiếm, kích thước của key-value cache trở thành một yếu tố quan trọng ảnh hưởng đến hiệu quả của các chính sách lập lịch preemptive. Các nghiên cứu trước đây đã đề xuất các kỹ thuật tiết kiệm bộ nhớ cho key-value cache. Multi-Query Attention [28] và Group-Query Attention [29] cố gắng giảm tiêu thụ bộ nhớ bằng cách chia sẻ key-value tensor giữa các attention head. Chúng có thể làm tổn hại khả năng của LLM và việc tiêu thụ bộ nhớ vẫn tăng tuyến tính với độ dài chuỗi. vLLM [11] quản lý key-value cache ở mức độ chi tiết của các khối để giảm phân mảnh bộ nhớ GPU. Nó không thể giảm việc sử dụng bộ nhớ do chính key-value cache gây ra. Khi độ dài ngữ cảnh tăng [30], việc tiêu thụ bộ nhớ của key-value cache là một vấn đề khó và đang trở nên ngày càng quan trọng.

## 3 Tổng quan FastServe

### 3.1 Các Thuộc tính Mong muốn

LLM đi kèm với những đặc điểm độc đáo gây ra thách thức cho tính toán phân tán và tiêu thụ bộ nhớ GPU. Mục tiêu của chúng tôi là phát triển một hệ thống phục vụ suy luận hiệu quả cho LLM thỏa mãn ba yêu cầu sau.

• **Độ trễ thấp và throughput cao.** Trọng tâm của chúng tôi tập trung vào các ứng dụng LLM tương tác, nơi người dùng có kỳ vọng cao về phản hồi nhanh. Để đo lường nó một cách định lượng, chúng tôi muốn throughput tối đa càng cao càng tốt dưới một yêu cầu độ trễ nhất định.

• **Quản lý bộ nhớ GPU hiệu quả.** LLM đặt ra thách thức đáng kể về tiêu thụ bộ nhớ GPU, điều này đòi hỏi một phương pháp quản lý bộ nhớ GPU hiệu quả cho cả mô hình và trạng thái trung gian.

• **Thực thi phân tán có thể mở rộng.** Bản chất của LLM đòi hỏi nhiều GPU để cho phép suy luận phân tán hiệu quả, điều này yêu cầu hệ thống hỗ trợ thực thi phân tán có thể mở rộng qua các server GPU.

### 3.2 Kiến trúc Tổng thể

Hình 4 cho thấy kiến trúc của FastServe. Các công việc được gửi đến job pool. Bộ lập lịch sử dụng thông tin từ job profiler để xác định mức độ ưu tiên ban đầu của công việc và sau đó đặt công việc trong skip-join MLFQ (§4.1) để giảm thiểu chặn đầu hàng đợi.

Để thực thi, bộ lập lịch chọn các công việc dựa trên mức độ ưu tiên của chúng trong skip-join MLFQ để tạo thành một kích thước batch tối đa được xác định trước và điều phối batch đến distributed execution engine để thực hiện một lần lặp. Distributed execution engine hợp tác với distributed key-value cache để truy cập và cập nhật các key-value tensor liên quan đến công việc tương ứng. Để giải quyết thách thức về dung lượng bộ nhớ GPU hạn chế, trình quản lý key-value cache chủ động hoán đổi các key-value tensor giữa bộ nhớ GPU và bộ nhớ host (§4.2).

Để phù hợp với các mô hình cực lớn như OPT-175B, FastServe sử dụng suy luận phân tán, cho phép cả tensor parallelism và pipeline parallelism. FastServe kết hợp các mở rộng vào bộ lập lịch và key-value cache để cho phép hỗ trợ liền mạch cho thực thi phân tán (§4.3).

## 4 Thiết kế FastServe

Trong phần này, chúng tôi đầu tiên giới thiệu bộ lập lịch skip-join MLFQ để tối thiểu hóa độ trễ (§4.1). Sau đó, chúng tôi trình bày một cơ chế quản lý KV cache chủ động được thiết kế để giảm thiểu hiệu quả ràng buộc dung lượng bộ nhớ GPU (§4.2). Cuối cùng, chúng tôi chứng minh cách áp dụng những kỹ thuật này vào các thiết lập phân tán (§4.3).

### 4.1 Bộ lập lịch Skip-Join MLFQ

**Strawman: lập lịch mức độ ưu tiên cố định.** Để hỗ trợ lập lịch preemptive, chúng ta cần một bộ lập lịch dựa trên mức độ ưu tiên để quyết định công việc nào cần preempt và công việc nào cần thực thi. Một giải pháp ngây thơ là gán một mức độ ưu tiên cố định cho mỗi công việc dựa trên độ dài đầu vào của nó. Trong trường hợp này, khi giai đoạn khởi tạo chi phối tổng độ trễ, lập lịch mức độ ưu tiên cố định có thể xấp xỉ hiệu suất tối ưu như chính sách SRPT. Tuy nhiên, mặc dù giải pháp này tận dụng thông tin về giai đoạn khởi tạo, nó bỏ qua các đặc điểm của giai đoạn giải mã. Nhiều bộ dữ liệu thực tế như ShareGPT và Alpaca cho thấy phân phối đuôi dài ngụ ý rằng các công việc với độ dài đầu ra dài cũng tồn tại. Khi giai đoạn giải mã chi phối tổng độ trễ, lập lịch mức độ ưu tiên cố định có thể lệch khỏi hiệu suất tối ưu của SRPT.

**Strawman: MLFQ ngây thơ.** Do kích thước công việc không xác định của suy luận LLM, việc áp dụng trực tiếp SRPT là không khả thi. Trong các thiết lập information-agnostic, Least-Attained Service (LAS) đã được chứng minh là xấp xỉ SRPT hiệu quả. Do overhead chuyển đổi công việc của LAS, phương pháp thực tế là Multi-Level Feedback Queue (MLFQ) đã trở nên phổ biến trong các hệ thống lập lịch khác nhau [15, 31-34]. MLFQ vận hành nhiều hàng đợi, mỗi hàng đợi có một mức độ ưu tiên khác nhau. Khi đến, một công việc vào hàng đợi có mức độ ưu tiên cao nhất và bị hạ cấp xuống hàng đợi cấp độ tiếp theo nếu thời gian thực thi của nó vượt quá một quantum. Giá trị của quantum là một tham số có thể điều chỉnh được gán cho mỗi hàng đợi, ví dụ, các hàng đợi có mức độ ưu tiên cao hơn thường có giá trị quantum ngắn hơn.

Mặc dù MLFQ giả định không có kiến thức trước về kích thước công việc, nó không phù hợp với phục vụ LLM. Hình 5 cho thấy thời gian lần lặp của OPT 2.7B trên NVIDIA A100, thay đổi độ dài chuỗi đầu vào. Đáng chú ý, thời gian giai đoạn khởi tạo (tức là lần lặp đầu tiên) vượt quá thời gian giai đoạn giải mã. Khi độ dài chuỗi đầu vào tăng, thời gian giai đoạn khởi tạo cũng tăng. Hành vi này có thể được quy cho tối ưu hóa key-value cache (§2.2). Trong lần lặp đầu tiên, các tính toán cho tất cả key-value tensor của các token đầu vào được thực hiện và cached. Trong các lần lặp tiếp theo, chỉ có key-value tensor của một token mới được tạo ra được tính toán, và phần còn lại được lấy từ cache.

Khi sử dụng MLFQ gốc, một công việc được gán ngay lập tức vào hàng đợi có mức độ ưu tiên cao nhất khi đến. Tuy nhiên, do thời gian giai đoạn khởi tạo đáng kể, công việc có thể cạn kiệt quantum của nó trước khi hoàn thành lần lặp đầu tiên. Tình huống này đặt ra một tiến thoái lưỡng nan trong lập lịch. Nếu bộ lập lịch preempt công việc, các activation trung gian bị loại bỏ và tính toán lại sau này, dẫn đến lãng phí tài nguyên máy tính và thời gian có giá trị. Mặt khác, nếu bộ lập lịch chọn không preempt công việc, nó vi phạm mục đích thiết kế cơ bản của MLFQ và có thể gặp phải chặn đầu hàng đợi một lần nữa.

**Giải pháp của chúng tôi: skip-join MLFQ.** Insight chính của thiết kế chúng tôi là tận dụng thiết lập semi information-agnostic của suy luận LLM để giải quyết các vấn đề nêu trên của các giải pháp strawman. Trong khi số lần lặp (tức là độ dài đầu ra) vẫn chưa biết trước, thời gian thực thi của mỗi lần lặp có thể dự đoán được. Đối với mỗi lần lặp, việc thực thi tương tự như suy luận DNN một lần truyền thống, thời gian thực thi có thể dự đoán cao [8, 35]. Một quá trình profiling nhẹ có thể dễ dàng thu thập thời gian lần lặp chính xác dưới các phần cứng, thông số kỹ thuật mô hình và độ dài đầu vào khác nhau trước. Xem lại Hình 5, thời gian giai đoạn khởi tạo thể hiện mối tương quan tích cực với độ dài đầu vào trong khi cố định phần cứng và mô hình. Đối với giai đoạn giải mã, thời gian lần lặp gần như không đổi.

Dựa trên insight này, chúng tôi đề xuất một bộ lập lịch skip-join MLFQ được thiết kế riêng cho suy luận LLM. Bộ lập lịch của chúng tôi quản lý hiệu quả việc di chuyển các công việc giữa các hàng đợi ưu tiên khác nhau theo cách skip-join. Trong MLFQ, chúng ta có n hàng đợi ưu tiên, cụ thể là Q1, Q2, ..., Qn, mỗi hàng đợi có một quantum riêng biệt q1 < q2 < ... < qn. Bộ lập lịch MLFQ thông thường ban đầu gán một công việc mới đến cho hàng đợi có mức độ ưu tiên cao nhất, tức là Q1. Một khi công việc cạn kiệt quantum được phân bổ trong Q1, nó sẽ được hạ cấp xuống Q2. Như được hiển thị trong Hình 6, FastServe khác với MLFQ gốc ở chỗ khi một công việc đến, FastServe tận dụng profiling chính xác để dự đoán thời gian giai đoạn khởi tạo (tinit) và ❶skip-join công việc đến hàng đợi có mức độ ưu tiên cao nhất (qi) sao cho qi ≥ tinit. Khi một công việc tiêu thụ quantum được phân bổ trước khi hoàn thành, bộ lập lịch hạ cấp ❷công việc dựa trên mức độ ưu tiên hiện tại và thời gian lần lặp tiếp theo.

**Tránh đói vĩnh viễn.** Điều quan trọng cần lưu ý là các hoạt động skip-join và demotion có thể dẫn đến đói cho các công việc có đầu vào và đầu ra dài. Để giải quyết vấn đề này, bộ lập lịch định kỳ kiểm tra các công việc bị đói và ❸thăng cấp chúng lên hàng đợi có mức độ ưu tiên cao nhất, tức là Q1. Điều này cho phép FastServe giải quyết chặn đầu hàng đợi trong khi giảm thiểu đói. Chúng tôi đánh giá tail latency và cho thấy hiệu quả của cơ chế ngăn chặn đói trong §6.2.

**Ví dụ.** Hình 7 cho thấy một ví dụ để chứng minh hiệu quả của bộ lập lịch skip-join MLFQ của FastServe. Trong ví dụ, ba công việc đến cùng lúc theo thứ tự J1, J2, J3. T1(Ji) biểu thị thời gian giai đoạn khởi tạo của công việc Ji, và T2(Ji) biểu thị thời gian giai đoạn giải mã. Chúng tôi giả định rằng cả skip-join và MLFQ gốc đều sử dụng bốn hàng đợi ưu tiên với giá trị quantum là 1, 2, 4 và 8. Ngoài ra, SRPT phục vụ như oracle với độ trễ trung bình tối ưu.

Như Hình 7 cho thấy, độ trễ trung bình của FCFS, MLFQ gốc, skip-join MLFQ và SRPT lần lượt là 4.23, 5, 3.3 và 3. FCFS và MLFQ gốc gặp phải vấn đề chặn đầu hàng đợi, nơi công việc J1 chặn các công việc còn lại, dẫn đến độ trễ trung bình dài. Skip-join MLFQ giải quyết vấn đề này bằng cách skip-join công việc J1 vào hàng đợi có mức độ ưu tiên thấp, đạt được hiệu suất tương tự như SRPT tối ưu. Nói chung, các thuật toán có quyền truy cập vào nhiều thông tin hơn hoạt động tốt hơn so với những thuật toán có thông tin hạn chế.

**Thuật toán.** Thuật toán 1 cho thấy pseudo-code của bộ lập lịch skip-join MLFQ. Bộ lập lịch có một tập hợp các hàng đợi ưu tiên Q1, Q2, ..., Qn với giá trị quantum q1, q2, ..., qn, và nhận một tập hợp các công việc mới đến Jnew. Nó lập lịch một batch MaxBatchSize công việc để thực thi. Phần skip-join (❶ trong Hình 6) tương ứng với dòng 6-9, và các phần demotion và ngăn chặn đói (❷ và ❸ trong Hình 6) tương ứng với dòng 16-17 và dòng 19-21. Có hai chi tiết đáng chú ý. Đầu tiên, bộ lập lịch hạ cấp một công việc xuống hàng đợi có mức độ ưu tiên thấp hơn η lần dựa trên thời gian lần lặp tiếp theo của nó. FastServe đặt quantum của hàng đợi có mức độ ưu tiên thấp hơn bằng hai lần quantum của hàng đợi có mức độ ưu tiên cao hơn, phù hợp với nghiên cứu trước [34] về MLFQ. Quantum của hàng đợi có mức độ ưu tiên cao nhất được đặt bằng thời gian lần lặp tối thiểu. Chi tiết thứ hai liên quan đến cách bộ lập lịch xác định các công việc bị đói được điều chỉnh bởi tham số α. FastServe điều chỉnh α dựa trên SLO do người dùng chỉ định, được đặt mặc định là 300 ms.

### 4.2 Quản lý Key-Value Cache Chủ động

Mặc dù bộ lập lịch skip-join MLFQ cung cấp preemption cấp độ lần lặp để xấp xỉ SRPT nhằm đạt được độ trễ thấp hơn mà không cần kiến thức trước về kích thước công việc chính xác, nó làm trầm trọng thêm áp lực tiêu thụ bộ nhớ GPU. Hình 8 cho thấy việc tiêu thụ bộ nhớ key-value cache của FCFS và skip-join MLFQ cho mô hình OPT 2.7B dưới một khối lượng công việc tổng hợp. Mặc dù chúng tôi chọn một mô hình tương đối nhỏ và giới hạn độ dài đầu ra tối đa là 20, overhead bộ nhớ KV cache đỉnh cho skip-join MLFQ có thể lớn hơn 7× so với FCFS. Nhu cầu bộ nhớ GPU trở nên nổi bật hơn khi triển khai các LLM lớn hơn như OPT 175B.

Lý do cơ bản là so với chính sách run-to-completion trong các hệ thống phục vụ hiện có, preemption cấp độ lần lặp được cung cấp bởi skip-join MFLQ tăng số lượng công việc đang diễn ra trong hệ thống. Ngoại trừ các key-value tensor của các công việc đang chạy, bộ lập lịch skip-join cũng cần lưu trữ các key-value tensor cho các công việc bị preempt ở trạng thái pending. Không giống như trạng thái process trong các bộ lập lịch hệ điều hành truyền thống, trạng thái trung gian, tức là key-value tensor, của mỗi công việc lớn hơn nhiều. Chính thức, đối với một công việc phục vụ suy luận LLM cụ thể, ký hiệu độ dài chuỗi đầu vào bằng s, độ dài chuỗi đầu ra bằng t, chiều ẩn của transformer bằng h, và số lớp transformer bằng l. Nếu trọng số mô hình và tất cả tính toán ở FP16, tổng số byte để lưu trữ key-value cache cho công việc đơn này là 4×lh(s+t). Lấy OPT 175B làm ví dụ (l=96, h=12288). Với độ dài chuỗi đầu vào s=512 và độ dài chuỗi đầu ra tối thiểu t=1, overhead bộ nhớ GPU cho một công việc đơn cao tới 2.3GB. Khi quá trình tạo sinh tiếp tục, độ dài chuỗi đầu ra t sẽ tăng, điều này càng làm tăng overhead bộ nhớ GPU.

Đồng thời, bộ nhớ GPU là một tài nguyên khan hiếm khi triển khai LLM. Thông thường, bộ nhớ GPU nhỏ hơn nhiều so với bộ nhớ host. Ví dụ, GPU NVIDIA A100 có tối đa 80 GB bộ nhớ GPU. Bên cạnh đó, một phần lớn bộ nhớ GPU được cung cấp để lưu trữ trọng số của LLM. Không gian để lưu trữ key-value tensor cho các công việc bị hạn chế. Kết quả là, dung lượng bộ nhớ GPU hạn chế các lợi ích tiềm năng của bộ lập lịch skip-join MLFQ.

**Giải pháp strawman 1: trì hoãn các công việc mới đến.** Để tránh lỗi out-of-memory (OOM), một giải pháp ngây thơ là đơn giản trì hoãn việc thực thi các công việc mới đến khi bộ nhớ GPU không đủ và tiếp tục lập lịch các công việc trong bộ nhớ hiện tại cho đến khi chúng hoàn thành. Giải pháp đơn giản này được sử dụng rộng rãi trong các hệ thống phục vụ hiện có, như vLLM [11]. Theo cách này, mặc dù các công việc mới được gán mức độ ưu tiên cao hơn, chúng bị chặn để chờ không gian bộ nhớ trống. Dưới các thiết lập hạn chế bộ nhớ GPU cực độ (ví dụ, suy luận chuỗi dài), giải pháp này sẽ làm suy thoái MLFQ thành FCFS, điều này lại gặp phải chặn đầu hàng đợi.

**Giải pháp strawman 2: giết và tính toán lại các công việc có mức độ ưu tiên thấp.** Một giải pháp đơn giản khác là giết một số công việc có mức độ ưu tiên thấp và giải phóng key-value cache của chúng để tạo chỗ cho các công việc có mức độ ưu tiên cao mới đến. Giải pháp này có hai vấn đề. Đầu tiên, các công việc bị giết mất trạng thái tạo sinh của chúng, đòi hỏi phải xây dựng lại key-value tensor của chúng. Điều này dẫn đến lãng phí tài nguyên máy tính và thời gian có giá trị. Thứ hai, nó có thể gây ra deadlock. Khi các công việc có mức độ ưu tiên cao đến, các công việc đang diễn ra với mức độ ưu tiên thấp hơn bị giết. Do việc tránh đói, các công việc bị giết có thể được thăng cấp lên hàng đợi có mức độ ưu tiên cao nhất nếu chúng chờ lâu hơn STARVE_LIMIT được chỉ định. Trong trường hợp này, công việc được thăng cấp có thể giết công việc đang thực thi hiện tại, công việc này có thể vừa giết công việc được thăng cấp trong bước trước. Nó có thể dẫn đến deadlock.

**Giải pháp của chúng tôi: hoán đổi key-value cache chủ động.** Dưới các ràng buộc dung lượng bộ nhớ GPU nghiêm ngặt, hai giải pháp strawman phải hy sinh hiệu suất của các công việc mới đến hoặc hiệu quả của các công việc có mức độ ưu tiên thấp. Để vượt qua tiến thoái lưỡng nan này, quan sát chính của chúng tôi là các key-value tensor chỉ cần được dự trữ trong bộ nhớ GPU khi các công việc tương ứng của chúng được lập lịch. Dựa trên quan sát này, FastServe mở rộng không gian của key-value cache từ bộ nhớ GPU sang bộ nhớ host. FastServe hoán đổi các key-value tensor không hoạt động của các công việc xuống bộ nhớ host để chứa các công việc pending bổ sung, và hoán đổi các key-value tensor trở lại bộ nhớ GPU cho các công việc sắp tới.

Tuy nhiên, overhead của việc hoán đổi không phải là không đáng kể so với thời gian tạo token. Khi triển khai OPT 175B trên 16 GPU NVIDIA A100, các key-value tensor của một công việc có thể chiếm 2.3 GB bộ nhớ. Thời gian tạo token trong giai đoạn giải mã khoảng 60 ms, trong khi thời gian hoán đổi các key-value tensor giữa bộ nhớ host và bộ nhớ GPU với băng thông đầy đủ PCIe 4.0 ×16 khoảng 36 ms. Do đó, một cơ chế hoán đổi reactive đơn giản xử lý hoán đổi và suy luận tuần tự tạo ra overhead lớn.

Thay vào đó, FastServe sử dụng một thuật toán hoán đổi key-value cache chủ động để giảm thiểu các tác động bất lợi của overhead hoán đổi. Insight chính là overlap suy luận LLM cho các công việc đang chạy với truyền dữ liệu cho các công việc pending để overhead hoán đổi nằm ngoài đường path quan trọng của suy luận LLM. Hình 9 minh họa một ví dụ. Thay vì hoán đổi key-value tensor của công việc pending J2 sau khi công việc J1 bị preempt hoặc hoàn thành, thuật toán chủ động hoán đổi các key-value tensor của J2 trước. Bằng cách này, overhead hoán đổi của J2 hiệu quả overlap với việc thực thi kernel GPU của J1, từ đó đạt được việc sử dụng GPU cao.

**Thứ tự hoán đổi công việc.** Thường xuyên hoán đổi vào và ra các key-value tensor không cần thiết tạo ra overhead thrashing bổ sung nếu hoán đổi vào và ra một công việc với mức độ ưu tiên cao. Overhead hoán đổi có thể tăng lên vượt quá thời gian thực thi, dẫn đến sự suy giảm hiệu suất của overlapping. Để giải quyết vấn đề này, FastServe tính toán estimated next scheduled time (ENST) cho mỗi công việc để quyết định thứ tự hoán đổi. ENST là thời gian khi công việc sẽ được lập lịch để thực thi lần tiếp theo. Công việc có ENST lớn nhất sẽ được hoán đổi ra trước, và công việc có ENST nhỏ nhất sẽ được hoán đổi vào trước. Thông thường, một công việc có mức độ ưu tiên thấp hơn được lập lịch để thực thi sau. Tuy nhiên, do cơ chế ngăn chặn đói, một công việc có mức độ ưu tiên thấp hơn có thể được nâng lên hàng đợi có mức độ ưu tiên cao hơn. Do đó, ngay cả một công việc có mức độ ưu tiên thấp đôi khi có thể được thực thi trước.

Trong trường hợp này, đối với công việc i, FastServe xem xét thời gian để thăng cấp công việc này và tổng thời gian thực thi của tất cả các công việc có mức độ ưu tiên cao hơn trước khi thực thi i đồng thời. Chính thức, để ngưỡng thời gian thăng cấp công việc i là Tpromote(i). Đối với tổng thời gian thực thi của tất cả các công việc có mức độ ưu tiên cao hơn trước khi thực thi i, chúng tôi giả định những công việc đó không hoàn thành sớm hơn trước khi bị hạ cấp xuống hàng đợi ưu tiên của công việc i. Thời gian thực thi của công việc j có mức độ ưu tiên cao hơn có thể được tính như sau (tức là công việc j bị hạ cấp từ j.priority xuống i.priority từng cái một):

Texecute(i,j) = ∑(i.priority<k≤j.priority) qk

trong đó i.priority là mức độ ưu tiên của công việc i, và qk là quantum của hàng đợi ưu tiên với mức độ ưu tiên k. Dựa trên điều này, tổng thời gian thực thi của tất cả các công việc có mức độ ưu tiên cao hơn công việc i được định nghĩa là:

Texecute(i) = (1/B) ∑(i.priority<j.priority) Texecute(i,j)

trong đó B là kích thước batch tối đa của các công việc. Cuối cùng, xem xét cả việc thăng cấp để ngăn chặn đói và việc thực thi các công việc có mức độ ưu tiên cao hơn, ENST của công việc i được tính là:

ENST(i) = min(Tpromote(i), Texecute(i))

Định nghĩa ENST này phục vụ như một phương tiện để ước tính thời gian lập lịch dự kiến cho lần tạo sinh tiếp theo của công việc i. Do đó, việc sử dụng metric này để quyết định thứ tự hoán đổi làm cho các key-value tensor của các công việc hoạt động chủ yếu nằm trong bộ nhớ GPU, và những công việc không hoạt động có xu hướng nằm trong bộ nhớ host.

**Xử lý một loạt công việc mới.** Chiến lược hoán đổi key-value cache chủ động được thiết kế cho bộ lập lịch skip-join MLFQ. Trong các tình huống có một lượng lớn công việc mới đến (với mức độ ưu tiên cao), hệ thống quản lý cache buộc phải đuổi các công việc một cách reactive, ảnh hưởng xấu đến hiệu suất của các công việc mới này. Để giảm thiểu điều này, FastServe dự trữ một số key-value cache slot nhàn rỗi đặc biệt cho các công việc mới, đảm bảo tính khả dụng ngay lập tức mà không cần hoán đổi công việc reactive. Phương pháp này đảm bảo hiệu suất của các công việc mới. Số lượng slot nhàn rỗi dựa trên các mẫu đến của công việc lịch sử. Tần suất cao hơn của các đợt công việc đột xuất đòi hỏi số lượng slot dự trữ lớn hơn.

### 4.3 Hỗ trợ cho Phục vụ LLM Phân tán

Nghiên cứu trước đây cho thấy rằng hiệu quả của LLM theo kinh nghiệm tuân thủ luật tỷ lệ liên quan đến số lượng tham số mô hình [36]. Tuy nhiên, điều quan trọng cần lưu ý là việc sử dụng bộ nhớ của LLM cũng thể hiện tính tỷ lệ thuận với số lượng tham số. Một ví dụ điển hình là OPT 175B, ngay cả khi được lưu trữ ở độ chính xác nửa, đòi hỏi một lượng bộ nhớ GPU khổng lồ 350GB chỉ để chứa trọng số của nó. Hơn nữa, bộ nhớ bổ sung được yêu cầu để xử lý các trạng thái trung gian trong quá trình runtime. Do đó, LLM thường cần được chia thành nhiều phần và phục vụ theo cách phân tán với nhiều GPU.

Tensor parallelism [16,37] và pipeline parallelism [17,38] là hai kỹ thuật được sử dụng rộng rãi nhất cho phục vụ LLM phân tán. FastServe hỗ trợ kết hợp của hai kỹ thuật song song này để phục vụ LLM. Một LLM bao gồm một loạt các toán tử trên các tensor đa chiều. Tensor parallelism chia mỗi toán tử qua nhiều thiết bị, với mỗi thiết bị thực hiện một phần tính toán song song. Overhead giao tiếp bổ sung được yêu cầu để chia đầu vào và thu thập đầu ra từ các GPU khác nhau. Tensor parallelism tăng cường đáng kể cả tài nguyên tính toán và bộ nhớ có sẵn cho một công việc đơn, do đó giảm thời gian của mỗi lần lặp.

Pipeline parallelism chia toàn bộ các toán tử của một đồ thị tính toán LLM thành nhiều giai đoạn và thực thi chúng trên các GPU khác nhau theo cách pipeline. Trong quá trình suy luận, mỗi giai đoạn tính toán một phần của toàn bộ đồ thị tính toán và truyền kết quả trung gian đến giai đoạn tiếp theo song song. Pipeline parallelism yêu cầu ít overhead giao tiếp hơn so với tensor parallelism, đồng thời cũng cung cấp cho LLM khả năng vượt qua ràng buộc bộ nhớ của một GPU riêng lẻ. Vì nhiều batch xử lý đang được xử lý đồng thời trong các giai đoạn khác nhau, FastServe cần xử lý nhiều batch trong distributed engine cùng một lúc.

**Lập lịch công việc trong phục vụ phân tán.** Trong MLFQ truyền thống, nếu không có công việc mới đến, bộ lập lịch lập lịch công việc có mức độ ưu tiên cao nhất và thực thi nó cho đến khi hoàn thành hoặc bị hạ cấp. Tuy nhiên, với pipeline parallelism, bộ lập lịch lập lịch ở mức độ chi tiết của giai đoạn riêng lẻ. Một khi một công việc hoàn thành giai đoạn đầu tiên và truyền kết quả trung gian đến giai đoạn tiếp theo, một điểm quyết định phát sinh cho bộ lập lịch về công việc tiếp theo để bắt đầu chuyển động. Trong trường hợp này, bộ lập lịch không thể theo MLFQ truyền thống tiếp tục lập lịch cùng một công việc cho đến khi hạ cấp, bởi vì công việc vẫn đang trong quá trình. Để bảo tồn ngữ nghĩa của MLFQ, FastServe vẫn giữ công việc đang chạy trong hàng đợi ưu tiên, nhưng lập lịch công việc có mức độ ưu tiên cao nhất ở trạng thái pending. Do đó, các công việc sớm trong một hàng đợi có thể đẩy nhanh việc hoàn thành quantum của chúng.

**Quản lý key-value cache trong phục vụ phân tán.** Do key-value cache chiếm một phần lớn bộ nhớ GPU, key-value cache của FastServe cũng được phân chia qua nhiều GPU. Trong suy luận LLM, mỗi key-value tensor được sử dụng bởi cùng giai đoạn của LLM. Do đó, FastServe phân chia key-value tensor như tensor parallelism yêu cầu, và gán mỗi key-value tensor cho GPU tương ứng để tất cả tính toán trên một GPU chỉ cần key-value tensor cục bộ trên cùng GPU.

Cơ chế hoán đổi key-value cache chủ động của FastServe cũng được phân tán. Vì các giai đoạn khác nhau của LLM xử lý các công việc khác nhau cùng một lúc, mỗi giai đoạn có thể offload hoặc upload các key-value tensor khác nhau một cách độc lập. Để giảm điều khiển dư thừa, trước khi xử lý kết quả trung gian được gửi từ giai đoạn trước, giai đoạn hiện tại thực hiện cùng hành động offload hoặc upload như giai đoạn trước. Việc truyền kết quả trung gian và hoán đổi key-value cache xảy ra song song, vì vậy overhead của hoán đổi key-value cache được giảm thêm. Như được hiển thị trong Hình 10, khi kết quả trung gian được gửi đến giai đoạn tiếp theo, giai đoạn tiếp theo nhận các hướng dẫn hoán đổi và có thể hoán đổi key-value cache cùng lúc nếu cần. Cơ chế hoán đổi key-value cache chỉ cần quyết định việc offload hoặc upload của giai đoạn đầu tiên. Khi sử dụng tensor parallelism chia giai đoạn đầu tiên thành nhiều chunk, một trình quản lý hoán đổi key-value cache tập trung hướng dẫn tất cả các chunk trong giai đoạn đầu tiên offload hoặc upload các key-value tensor thuộc sở hữu của cùng một công việc.

## 5 Triển khai

FastServe là một hệ thống phục vụ suy luận LLM phân tán với frontend RESTful API, một bộ lập lịch và một distributed execution engine. Frontend và bộ lập lịch được triển khai với 2.9K dòng mã Python. Distributed execution engine được triển khai với 8.1K dòng mã C++/CUDA. Frontend hỗ trợ giao diện tương thích OpenAI API nơi khách hàng có thể chỉ định các tham số sampling như độ dài đầu ra tối đa và temperature. Bộ lập lịch triển khai các chính sách skip-join MLFQ và hoán đổi chủ động. Distributed execution engine sử dụng Ray [39] actor để triển khai các GPU worker thực hiện suy luận LLM và quản lý key-value cache theo cách phân tán. Chúng tôi triển khai các LLM nguồn mở phổ biến như OPT bằng C++ để đạt được hiệu suất và khả năng mở rộng tốt hơn so với các triển khai Python phổ biến trong Huggingface [25]. Chúng tôi cũng triển khai các kernel CUDA tùy chỉnh để hỗ trợ lập lịch cấp độ lần lặp của Orca [10] và PagedAttention của vLLM [11].

## 6 Đánh giá

Trong phần này, chúng tôi đầu tiên chứng minh các cải thiện hiệu suất end-to-end của FastServe so với các hệ thống phục vụ LLM tiên tiến. Sau đó, chúng tôi đánh giá các lựa chọn thiết kế của FastServe và cho thấy hiệu quả của mỗi thành phần.

### 6.1 Phương pháp

**Testbed.** Các thí nghiệm end-to-end (§6.2) sử dụng hai instance AWS EC2 p4d.24xlarge. Mỗi instance được cấu hình với tám GPU NVIDIA A100 40GB kết nối qua NVLink, 1152 GB bộ nhớ host và PCIe 4.0 ×16. Do ngân sách hạn chế, các thí nghiệm cho lựa chọn thiết kế (§6.3) sử dụng một GPU NVIDIA A100 40GB trong testbed riêng của chúng tôi để xác nhận hiệu quả của mỗi thành phần.

**Mô hình LLM.** Chúng tôi chọn họ LLM đại diện, OPT [18], được sử dụng rộng rãi trong cả học thuật và công nghiệp. Chúng tôi chọn các kích thước mô hình phổ biến. Bảng 1 liệt kê các cấu hình mô hình. Chúng tôi sử dụng độ chính xác FP16 trong tất cả thí nghiệm.

**Khối lượng công việc.** Tương tự như nghiên cứu trước về phục vụ LLM [11], chúng tôi tạo khối lượng công việc dựa trên các bộ dữ liệu ShareGPT [13] và Alpaca [14]. Các bộ dữ liệu này chứa đầu vào và đầu ra thực tế của các dịch vụ LLM. Bộ dữ liệu ShareGPT bao gồm các cuộc trò chuyện do người dùng chia sẻ với ChatGPT [13]. Bộ dữ liệu Alpaca được tạo bởi GPT-3.5 với self-instruct [14]. Vì các bộ dữ liệu này không bao gồm thời gian đến, chúng tôi theo nghiên cứu trước [11] để tạo thời gian đến cho mỗi yêu cầu theo quy trình Poisson được tham số hóa bởi tỷ lệ đến.

**Metric đánh giá.** Độ trễ được người dùng cảm nhận là một phép đo quan trọng cho các ứng dụng tương tác như ChatGPT. Cụ thể, tương tự như nghiên cứu trước về phục vụ LLM [10, 11], độ trễ trung bình mỗi token được tính như là trung bình của độ trễ end-to-end của mỗi công việc chia cho độ dài đầu ra của nó. Ngoài ra, chúng tôi cũng báo cáo tail latency P95.

Để so sánh, chúng tôi đặt một SLO độ trễ và so sánh throughput tối đa mà mỗi hệ thống có thể đạt được dưới SLO. Chúng tôi theo nghiên cứu trước [40] để đặt SLO độ trễ bằng 10× độ trễ của một lần lặp duy nhất trong giai đoạn giải mã. Cụ thể, chúng tôi đặt SLO là 0.3 giây dựa trên profiling của chúng tôi.

**Baseline.** Chúng tôi so sánh FastServe với ba baseline. Để so sánh công bằng, tất cả baseline sử dụng cùng kích thước tensor parallelism, kích thước pipeline parallelism và kích thước batch như FastServe, ngoại trừ vLLM chỉ sử dụng tensor parallelism để phục vụ OPT-175B, vì nó không hỗ trợ pipeline parallelism. Bảng 2 cho thấy so sánh giữa FastServe và các baseline.

• **FasterTransformer [26]:** Đây là một inference engine chất lượng sản xuất từ NVIDIA. Nó hỗ trợ cả tensor parallelism và pipeline parallelism. Tuy nhiên, nó áp dụng lập lịch cấp độ công việc và các công việc ngắn bị chặn bởi các công việc dài trong cùng batch. Chúng tôi sử dụng FasterTransformer v5.3.

• **vLLM [11]:** Đây là hệ thống phục vụ LLM tiên tiến nhất hỗ trợ lập lịch cấp độ lần lặp [10] và PagedAttention [11] để giảm phân mảnh bộ nhớ do key-value cache gây ra. Tuy nhiên, nó sử dụng bộ lập lịch FCFS đơn giản với thực thi run-to-completion, gặp phải chặn đầu hàng đợi. Chúng tôi sử dụng vLLM v0.1.7.

• **FastServe-FCFS:** Nó sử dụng cùng distributed execution engine của FastServe, nhưng không sử dụng các kỹ thuật được đề xuất trong §4. Baseline này giúp phân biệt tăng tốc do các kỹ thuật đề xuất trong bài báo này với tăng tốc do triển khai hiệu quả của FastServe.

### 6.2 Hiệu suất End-to-End

Chúng tôi so sánh hiệu suất end-to-end của FastServe với ba hệ thống baseline dưới khối lượng công việc ShareGPT và Alpaca trên OPT-13B, OPT-66B và OPT-175B trong Bảng 1.

Hàng đầu tiên của Hình 11 cho thấy hiệu suất end-to-end của tất cả các hệ thống dưới bộ dữ liệu ShareGPT. Mặc dù FasterTransformer triển khai các kernel GPU được tối ưu hóa cao cho suy luận LLM, nó không hỗ trợ lập lịch cấp độ lần lặp. Nó không thể trả về các công việc hoàn thành sớm trong batch đang diễn ra và thêm các công việc mới vào batch để giảm độ trễ. Kết quả là, FasterTransformer gặp phải chặn đầu hàng đợi đáng kể ngay cả khi tỷ lệ đến công việc nhỏ. FastServe vượt trội hơn FasterTransformer 31.5-74.9× về throughput dưới SLO. Là hệ thống phục vụ tiên tiến, vLLM được trang bị hầu hết các kỹ thuật để tăng tốc suy luận và giảm tiêu thụ bộ nhớ GPU. Tuy nhiên, vì vLLM lập lịch các công việc theo cách FCFS, một phần lớn độ trễ end-to-end là độ trễ xếp hàng. Tối ưu hóa thời gian thực thi của công việc suy luận LLM là không đủ. Được trang bị bộ lập lịch skip-join MLFQ, FastServe có thể giảm đáng kể độ trễ xếp hàng và vượt trội hơn vLLM 2.3-18.3×. Đáng chú ý là FastServe-FCFS cũng vượt trội hơn vLLM, vì nó sử dụng triển khai C++ hiệu quả hơn và fuse nhiều operation thành ít kernel GPU hơn. Nhưng nó vẫn gặp phải vấn đề chặn đầu hàng đợi, khiến nó chậm hơn FastServe 2-4×.

Hàng thứ hai của Hình 11 cho thấy kết quả thí nghiệm dưới bộ dữ liệu Alpaca. Vì kích thước công việc của Alpaca nhỏ hơn so với bộ dữ liệu ShareGPT, tất cả các hệ thống phục vụ có thể duy trì độ trễ thấp ngay cả khi tỷ lệ cao hơn tương đối so với dưới bộ dữ liệu ShareGPT. Tuy nhiên, lợi ích hiệu suất của FastServe là tương tự. Không có lập lịch cấp độ lần lặp, FasterTransformer là hệ thống chậm nhất và FastServe vượt trội hơn nó 9.5-15.8×. vLLM đạt được hiệu suất tốt hơn FasterTransformer, nhưng nó vẫn gặp phải vấn đề chặn đầu hàng đợi. Kết quả là, FastServe vượt trội hơn vLLM 3-31.4×. Với triển khai hiệu quả của chúng tôi, FastServe-FCFS cũng vượt trội hơn vLLM, nhưng nó vẫn chậm hơn FastServe 1.6-2×.

**Tác động lên tail latency.** Một mối quan tâm tiềm ẩn của lập lịch preemptive và MLFQ là nó có thể gây ra đói cho các công việc dài và làm tổn hại tail latency. FastServe kết hợp một cơ chế ngăn chặn đói trong bộ lập lịch skip-join MLFQ của nó (§4.1). Để chứng minh hiệu quả của cơ chế ngăn chặn đói, chúng tôi đo độ trễ 95% của tất cả các hệ thống dưới bộ dữ liệu ShareGPT. Như được hiển thị trong Hình 12, FastServe cải thiện đáng kể throughput của các công việc suy luận LLM dưới cùng yêu cầu SLO cho tail latency. Ví dụ, khi phục vụ OPT-175B, so với FastServe-FCFS, FastServe cải thiện throughput lên đến 1.5×. FastServe cũng vượt trội hơn FastServe-FCFS 2-2.8× khi phục vụ OPT-13B và OPT-66B. FastServe đạt được cải thiện hiệu suất lên đến 17.9× và 59.8× so với vLLM và FasterTransformer tương ứng. Kết quả cho thấy rằng mặc dù FastServe được thiết kế để giảm độ trễ trung bình, nó cũng có thể giảm đáng kể tail latency của các công việc suy luận LLM. Ưu tiên các công việc ngắn với bộ lập lịch skip-join MLFQ có thể hiệu quả giảm vấn đề chặn đầu hàng đợi và không làm tổn hại tail latency. Ngay cả đối với các công việc dài, FastServe vẫn có thể tăng tốc chúng bằng cách giảm độ trễ xếp hàng của chúng. Cơ chế ngăn chặn đói đảm bảo rằng các công việc dài có thể được lập lịch trong thời gian hợp lý.

**Tác động lên goodput.** Chúng tôi tiếp tục điều tra tác động lên goodput cho các hệ thống khác nhau dưới các SLO khác nhau khi phục vụ mô hình OPT-13B. Tương tự như các nghiên cứu trước [35, 41, 42], chúng tôi đo P95 goodput, được định nghĩa là throughput khi 95% công việc có thể được hoàn thành trong SLO của giai đoạn khởi tạo và giai đoạn giải mã. Chúng tôi đặt SLO của hai giai đoạn là 5×, 10× và 20× độ trễ trong giai đoạn tương ứng dưới tải nhẹ. Như được hiển thị trong Hình 13, FastServe luôn đạt được P95 goodput cao nhất qua các SLO khác nhau. Cụ thể, FastServe vượt trội hơn vLLM 4.1× đến 4.7× và FastServe-FCFS 1.46× đến 1.64×. Các kết quả này chứng minh rằng FastServe hiệu quả cải thiện throughput của hệ thống mà không vi phạm SLO của cả giai đoạn khởi tạo và giai đoạn giải mã.

### 6.3 Lựa chọn Thiết kế

**Hiệu quả của skip-join MLFQ.** Để cho thấy hiệu quả của skip-join MLFQ, chúng tôi tiến hành so sánh hiệu suất với FCFS, MLFQ ngây thơ và Fixed Priority khi phục vụ OPT-13B. Chúng tôi sử dụng bộ dữ liệu ShareGPT để tạo các công việc và thay đổi tỷ lệ giữa độ dài đầu vào và đầu ra trong khi bảo tồn phân phối độ dài gốc. Điều chỉnh này phản ánh xu hướng ngành hiện tại về việc mở rộng giới hạn token đầu vào trong LLM [4, 5, 42]. Hình 14 cho thấy kết quả khi thay đổi tỷ lệ giữa độ dài đầu vào và đầu ra. Độ trễ được chuẩn hóa bởi hệ thống chậm nhất. FCFS luôn gặp phải độ trễ cao do vấn đề chặn đầu hàng đợi, bất kể tỷ lệ. MLFQ ngây thơ có hiệu suất tốt dưới tỷ lệ nhỏ vì sự khác biệt giữa giai đoạn khởi tạo và giải mã là tối thiểu. Tuy nhiên, khi tỷ lệ tăng, MLFQ ngây thơ gặp khó khăn với giai đoạn khởi tạo kéo dài. Ngược lại, Fixed Priority xuất sắc với tỷ lệ lớn hơn, nơi giai đoạn khởi tạo chi phối thời gian thực thi, nhưng hoạt động kém khi tỷ lệ thấp, vì nó bỏ qua giai đoạn giải mã khi đặt mức độ ưu tiên. Được hưởng lợi từ chính sách lập lịch semi information-agnostic, skip-join MLFQ luôn cải thiện hiệu suất so với FCFS, MLFQ ngây thơ và Fixed Priority lên đến 8.9×, 1.87× và 13.9×.

**Hiệu quả của quản lý key-value cache chủ động.** Để cho thấy hiệu quả của quản lý key-value cache chủ động, chúng tôi đánh giá hiệu suất của FastServe với hai chiến lược baseline Recompute và Reactive được đề cập trong §4.2 khi phục vụ OPT-13B trên bộ dữ liệu ShareGPT. Hình 15(a) cho thấy kết quả. Ở tỷ lệ đến thấp, bộ nhớ GPU đủ để chứa key-value cache cho tất cả các công việc, làm cho hiệu suất của ba chiến lược tương tự. Khi tỷ lệ đến tăng, bộ nhớ GPU trở nên không đủ để các hệ thống phải preempt một số key-value cache cho các công việc khác, dẫn đến hiệu suất khác biệt.

Trong trường hợp như vậy, Recompute loại bỏ key-value cache cho các công việc có mức độ ưu tiên thấp, tăng overhead tính toán lại cho KV cache của các công việc này. Như được hiển thị trong Hình 15(a), overhead tính toán lại này làm cho chiến lược proactive-swapping vượt trội hơn recomputation 2.7×.

Đối với Reactive, nó hoán đổi các công việc có mức độ ưu tiên thấp xuống bộ nhớ host khi bộ nhớ GPU không hiệu quả và hoán đổi các công việc này vào nếu cần. Việc truyền dữ liệu nằm trong path quan trọng. Tính toán tiếp theo phải chờ đợi các truyền này. Ngược lại, proactive-swapping dự đoán các yêu cầu bộ nhớ của các công việc mới đến và preempt cache có mức độ ưu tiên thấp trước. Tương tự, khi cache của một công việc có mức độ ưu tiên cao được phát hiện trong bộ nhớ host và bộ nhớ GPU có sẵn, nó sẽ được hoán đổi chủ động vào bộ nhớ GPU. Điều này cho phép việc truyền dữ liệu overlap với tính toán và đạt được cải thiện 1.7× so với phương pháp reactive.

Để điều tra thêm overhead của cơ chế hoán đổi chủ động, chúng tôi chia độ trễ end-to-end thành ba phần: độ trễ xếp hàng, thời gian thực thi và thời gian hoán đổi. Thời gian hoán đổi là thời gian khi công việc bị chặn bởi cơ chế hoán đổi chủ động. Như được hiển thị trong Hình 15(b), thời gian hoán đổi ít hơn 5% độ trễ end-to-end, có thể bỏ qua so với thời gian thực thi và độ trễ xếp hàng. Lý do xác nhận rằng cơ chế hoán đổi chủ động có thể overlap hầu hết thời gian hoán đổi với thời gian thực thi của các công việc khác. Kết quả là, cơ chế hoán đổi chủ động hầu như không ảnh hưởng đến độ trễ end-to-end.

## 7 Nghiên cứu Liên quan

**Lập lịch preemptive.** Nhiều giải pháp cho lập lịch công việc trong datacenter sử dụng lập lịch preemptive. Nhiều hệ thống mạng [15, 31, 32, 43] sử dụng lập lịch flow preemptive để tối thiểu hóa thời gian hoàn thành flow. Nhiều bộ lập lịch cho khối lượng công việc datacenter nhạy cảm với độ trễ, như Shinjuku [44], Shenango [45] và Caladan [46], cũng sử dụng preemption chi tiết và phân bổ lại tài nguyên để tối ưu hóa tail latency ở mức microsecond. Đối với khối lượng công việc DL, Tiresias [34] sử dụng MLFQ để tối ưu hóa thời gian hoàn thành công việc cho các công việc huấn luyện DL phân tán. Pipeswitch [47] và REEF [48] cung cấp preemption GPU hiệu quả để chạy cả các nhiệm vụ DL quan trọng về độ trễ và best-effort trên cùng GPU. Khác với chúng, FastServe nhắm đến một tình huống mới, phục vụ suy luận LLM.

**Phục vụ suy luận.** Nhiều hệ thống phục vụ mô hình truyền thống [8, 9, 22, 23, 49] chỉ tập trung vào phục vụ các mô hình tương đối nhỏ trong một cụm mà không nhận biết về đặc điểm của LLM. Gần đây, một số hệ thống phục vụ được đề xuất để tối ưu hóa các LLM dựa trên Transformer [10, 35, 50-52]. Orca [10] và vLLM [11] xem xét mẫu tạo sinh tự hồi quy của LLM. Tuy nhiên, do chính sách FCFS của chúng, chúng gặp phải vấn đề chặn đầu hàng đợi nghiêm trọng. VTC [52] tập trung vào tính công bằng của phục vụ LLM nhưng không xem xét tình huống preemption. Splitwise [53] và DistServe [41] tách biệt giai đoạn prefill và decoding để loại bỏ sự can thiệp giữa chúng và do đó tối ưu hóa độ trễ thực thi. LoongServe [42] sử dụng elastic sequence parallelism để đặt động mức độ song song cho các yêu cầu khác nhau ở các giai đoạn khác nhau. Các hệ thống này trực giao với FastServe.

**Tối ưu hóa bộ nhớ cho LLM.** Do việc sử dụng bộ nhớ cao cho LLM, nhiều kỹ thuật đã được đề xuất để giảm overhead bộ nhớ. Một số nghiên cứu [54, 55] nhắm đến huấn luyện, trực giao với tình huống phục vụ. Quantization [56-59] nén trọng số mô hình thành độ chính xác thấp hơn sau huấn luyện để giảm footprint bộ nhớ trong quá trình suy luận. SparTA [60] khai thác sparsity mô hình để tăng tốc tính toán. Tuy nhiên, các phương pháp này hy sinh độ chính xác mô hình. vLLM [11] đề xuất PagedAttention để giảm phân mảnh bộ nhớ GPU. Điều này trực giao với bài báo này và FastServe cũng triển khai PagedAttention.

## 8 Kết luận

Chúng tôi trình bày FastServe, một hệ thống phục vụ suy luận phân tán cho LLM. Chúng tôi khai thác mẫu tự hồi quy của suy luận LLM để cho phép preemption cấp độ lần lặp và thiết kế một bộ lập lịch skip-join MLFQ mới để giải quyết vấn đề chặn đầu hàng đợi. Chúng tôi đề xuất một cơ chế quản lý key-value cache chủ động để xử lý overhead bộ nhớ của key-value cache và ẩn độ trễ truyền dữ liệu với tính toán. Dựa trên những điều này, chúng tôi xây dựng một prototype của FastServe. Các thí nghiệm cho thấy rằng FastServe cải thiện throughput lên đến 31.4× và 17.9× dưới cùng SLO độ trễ trung bình và tail tương ứng, so với vLLM.

## Tài liệu tham khảo

[1] "Introducing ChatGPT." https://openai.com/blog/chatgpt, 2022.

[2] "ChatGPT sets record for fastest-growing user base." https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/, 2023.

[3] "Reinventing search with a new ai-powered bing and edge, your copilot for the web." https://news.microsoft.com/the-new-Bing/, 2023.

[4] Google, "Our next-generation model: Gemini 1.5." https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/, 2024.

[5] Anthropic, "Introducing the next generation of Claude." https://www.anthropic.com/news/claude-3-family, 2024.

[6] "Introducing Qwen." https://qwenlm.github.io/blog/qwen/, 2023.

[7] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in IEEE Conference on Computer Vision and Pattern Recognition, 2016.

[8] A. Gujarati, R. Karimi, S. Alzayat, W. Hao, A. Kaufmann, Y. Vigfusson, and J. Mace, "Serving DNNs like clockwork: Performance predictability from the bottom up," in USENIX OSDI, 2020.

[9] H. Zhang, Y. Tang, A. Khandelwal, and I. Stoica, "Shepherd: Serving dnns in the wild," in USENIX NSDI, 2023.

[10] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, "Orca: A distributed serving system for Transformer-Based generative models," in USENIX OSDI, 2022.

[11] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica, "Efficient memory management for large language model serving with pagedattention," in ACM SOSP, 2023.

[12] K. Kaffes, T. Chong, J. T. Humphries, A. Belay, D. Mazières, and C. Kozyrakis, "Shinjuku: Preemptive scheduling for µsecond-scale tail latency," in USENIX NSDI, 2019.

[13] "Sharegpt teams." https://sharegpt.com/, 2023.

[14] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, "Stanford alpaca: An instruction-following llama model." https://github.com/tatsu-lab/stanford_alpaca, 2023.

[15] W. Bai, L. Chen, K. Chen, D. Han, C. Tian, and H. Wang, "Information-agnostic flow scheduling for commodity data centers," in USENIX OSDI, 2015.

[16] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," arXiv, 2020.

[17] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee, J. Ngiam, Q. V. Le, Y. Wu, and Z. Chen, "Gpipe: Efficient training of giant neural networks using pipeline parallelism," Neural Information Processing Systems, 2019.

[18] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T. Mihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, and L. Zettlemoyer, "Opt: Open pre-trained transformer language models," arXiv, 2022.

[19] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, "Language models are few-shot learners," arXiv, 2020.

[20] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, "Llama: Open and efficient foundation language models," arXiv, 2023.

[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Neural Information Processing Systems, 2017.

[22] C. Olston, N. Fiedel, K. Gorovoy, J. Harmsen, L. Lao, F. Li, V. Rajashekhar, S. Ramesh, and J. Soyke, "Tensorflow-serving: Flexible, high-performance ml serving," arXiv, 2017.

[23] N. Corporation, "Triton inference server: An optimized cloud and edge inferencing solution.," 2019.

[24] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli, "fairseq: A fast, extensible toolkit for sequence modeling," arXiv, 2019.

[25] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, "Huggingface's transformers: State-of-the-art natural language processing," arXiv, 2020.

[26] N. Corporation, "Fastertransformer," 2019.

[27] L. Schrage, "A proof of the optimality of the shortest remaining processing time discipline," Operations Research, 1968.

[28] N. Shazeer, "Fast transformer decoding: One write-head is all you need," arXiv, 2019.

[29] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai, "Gqa: Training generalized multi-query transformer models from multi-head checkpoints," arXiv, 2023.

[30] D. Li*, R. Shao*, A. Xie, Y. Sheng, L. Zheng, J. E. Gonzalez, I. Stoica, X. Ma, and H. Zhang, "How long can open-source llms truly promise on context length?," 2023.

[31] C.-Y. Hong, M. Caesar, and P. B. Godfrey, "Finishing flows quickly with preemptive scheduling," in ACM SIGCOMM, 2012.

[32] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar, and S. Shenker, "pfabric: Minimal near-optimal datacenter transport," SIGCOMM CCR, 2013.

[33] M. Chowdhury and I. Stoica, "Efficient coflow scheduling without prior knowledge," SIGCOMM CCR, 2015.

[34] J. Gu, M. Chowdhury, K. G. Shin, Y. Zhu, M. Jeon, J. Qian, H. H. Liu, and C. Guo, "Tiresias: A gpu cluster manager for distributed deep learning.," in USENIX NSDI, 2019.

[35] Z. Li, L. Zheng, Y. Zhong, V. Liu, Y. Sheng, X. Jin, Y. Huang, Z. Chen, H. Zhang, J. E. Gonzalez, et al., "AlpaServe: Statistical multiplexing with model parallelism for deep learning serving," in USENIX OSDI, 2023.

[36] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling laws for neural language models," arXiv, 2020.

[37] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. A. Korthikanti, D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro, A. Phanishayee, and M. Zaharia, "Efficient large-scale language model training on gpu clusters using megatron-lm," arXiv, 2021.

[38] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur, G. R. Ganger, P. B. Gibbons, and M. Zaharia, "Pipedream: Generalized pipeline parallelism for dnn training," in ACM SOSP, 2019.

[39] P. Moritz, R. Nishihara, S. Wang, A. Tumanov, R. Liaw, E. Liang, M. Elibol, Z. Yang, W. Paul, M. I. Jordan, and I. Stoica, "Ray: A distributed framework for emerging AI applications," in USENIX OSDI, 2018.

[40] G. Prekas, M. Kogias, and E. Bugnion, "Zygos: Achieving low tail latency for microsecond-scale networked tasks," in ACM SOSP, 2017.

[41] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and H. Zhang, "Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving," in USENIX OSDI, 2024.

[42] B. Wu, S. Liu, Y. Zhong, P. Sun, X. Liu, and X. Jin, "Loongserve: Efficiently serving long-context large language models with elastic sequence parallelism," arXiv, 2024.

[43] M. Chowdhury, Y. Zhong, and I. Stoica, "Efficient coflow scheduling with varys," in ACM SIGCOMM, 2014.

[44] K. Kaffes, T. Chong, J. T. Humphries, A. Belay, D. Mazières, and C. Kozyrakis, "Shinjuku: Preemptive scheduling for µsecond-scale tail latency," in USENIX NSDI, 2019.

[45] A. Ousterhout, J. Fried, J. Behrens, A. Belay, and H. Balakrishnan, "Shenango: Achieving high cpu efficiency for latency-sensitive datacenter workloads.," in USENIX NSDI, 2019.

[46] J. Fried, Z. Ruan, A. Ousterhout, and A. Belay, "Caladan: Mitigating interference at microsecond timescales," in USENIX OSDI, 2020.

[47] Z. Bai, Z. Zhang, Y. Zhu, and X. Jin, "Pipeswitch: Fast pipelined context switching for deep learning applications," in USENIX OSDI, 2020.

[48] M. Han, H. Zhang, R. Chen, and H. Chen, "Microsecond-scale preemption for concurrent GPU-accelerated DNN inferences," in USENIX OSDI, 2022.

[49] D. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E. Gonzalez, and I. Stoica, "Clipper: A low-latency online prediction serving system.," in USENIX NSDI, 2017.

[50] J. Fang, Y. Yu, C. Zhao, and J. Zhou, "Turbotransformers: an efficient gpu serving system for transformer models," in ACM PPoPP, 2021.

[51] D. Li, R. Shao, H. Wang, H. Guo, E. P. Xing, and H. Zhang, "Mpcformer: fast, performant and private transformer inference with mpc," arXiv, 2023.

[52] Y. Sheng, S. Cao, D. Li, B. Zhu, Z. Li, D. Zhuo, J. E. Gonzalez, and I. Stoica, "Fairness in serving large language models," in USENIX OSDI, 2024.

[53] P. Patel, E. Choukse, C. Zhang, A. Shah, Íñigo Goiri, S. Maleki, and R. Bianchini, "Splitwise: Efficient generative llm inference using phase splitting," in ACM/IEEE ISCA, 2024.

[54] Y. Bai, C. Li, Q. Zhou, J. Yi, P. Gong, F. Yan, R. Chen, and Y. Xu, "Gradient compression supercharged high-performance data parallel dnn training," in ACM SOSP, 2021.

[55] J. Wang, B. Yuan, L. Rimanic, Y. He, T. Dao, B. Chen, C. Re, and C. Zhang, "Fine-tuning language models over slow networks using activation quantization with guarantees," Neural Information Processing Systems, 2022.

[56] Z. Li, E. Wallace, S. Shen, K. Lin, K. Keutzer, D. Klein, and J. Gonzalez, "Train big, then compress: Rethinking model size for efficient training and inference of transformers," in International Conference on Machine Learning (ICML), 2020.

[57] G. Xiao, J. Lin, M. Seznec, J. Demouth, and S. Han, "Smoothquant: Accurate and efficient post-training quantization for large language models," International Conference on Machine Learning, 2022.

[58] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, "Gptq: Accurate post-training quantization for generative pre-trained transformers," arXiv, 2022.

[59] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, "Llm. int8 (): 8-bit matrix multiplication for transformers at scale," arXiv, 2022.

[60] N. Zheng, B. Lin, Q. Zhang, L. Ma, Y. Yang, F. Yang, Y. Wang, M. Yang, and L. Zhou, "SparTA: Deep-Learning model sparsity via Tensor-with-Sparsity-Attribute," in USENIX OSDI, 2022.
