# 2211.17192.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/inference/2211.17192.pdf
# File size: 574982 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Fast Inference from Transformers via Speculative Decoding
Yaniv Leviathan* 1Matan Kalman* 1Yossi Matias1
Abstract
Inference from large autoregressive models like
Transformers is slow - decoding Ktokens takes
Kserial runs of the model. In this work we in-
troduce speculative decoding - an algorithm to
sample from autoregressive models faster without
any changes to the outputs , by computing several
tokens in parallel. At the heart of our approach lie
the observations that (1) hard language-modeling
tasks often include easier subtasks that can be ap-
proximated well by more efﬁcient models, and
(2) using speculative execution and a novel sam-
pling method, we can make exact decoding from
the large models faster, by running them in par-
allel on the outputs of the approximation mod-
els, potentially generating several tokens concur-
rently, and without changing the distribution. Our
method can accelerate existing off-the-shelf mod-
els without retraining or architecture changes. We
demonstrate it on T5-XXL and show a 2X-3X
acceleration compared to the standard T5X imple-
mentation, with identical outputs.
1. Introduction
Large autoregressive models, notably large Transformers
(Vaswani et al., 2017), are much more capable than smaller
models, as is evidenced countless times in recent years e.g.,
in the text or image domains, like GPT-3 (Brown et al.,
2020), LaMDA (Thoppilan et al., 2022), Parti (Yu et al.,
2022), and PaLM (Chowdhery et al., 2022). Unfortunately,
a single decode step from these larger models is signiﬁcantly
slower than a step from their smaller counterparts, and mak-
ing things worse, these steps are done serially - decoding K
tokens takes Kserial runs of the model.
Given the importance of large autoregressive models and
speciﬁcally large Transformers, several approaches were
*Equal contribution1Google Research, Mountain
View, CA, USA. Correspondence to: Yaniv Leviathan
<leviathan@google.com >.
Proceedings of the 40thInternational Conference on Machine
Learning , Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).developed to make inference from them faster. Some ap-
proaches aim to reduce the inference cost for allinputs
equally (e.g. Hinton et al., 2015; Jaszczur et al., 2021;
Hubara et al., 2016; So et al., 2021; Shazeer, 2019). Other
approaches stem from the observation that not all infer-
ence steps are born alike - some require a very large model,
while others can be approximated well by more efﬁcient
models. These adaptive computation methods (e.g. Han
et al., 2021; Sukhbaatar et al., 2019; Schuster et al., 2021;
Scardapane et al., 2020; Bapna et al., 2020; Elbayad et al.,
2019; Schwartz et al., 2020) aim to use less compute re-
sources for easier inference steps. While many of these
solutions have proven extremely effective in practice, they
usually require changing the model architecture, changing
the training-procedure and re-training the models, and don’t
maintain identical outputs.
The key observation above, that some inference steps are
“harder” and some are “easier”, is also a key motivator for
our work. We additionally observe that inference from large
models is often not bottlenecked on arithmetic operations,
but rather on memory bandwidth and communication, so
additional computation resources might be available. There-
fore we suggest increasing concurrency as a complemen-
tary approach to using an adaptive amount of computation.
Speciﬁcally, we are able to accelerate inference without
changing the model architectures, without changing the
training-procedures or needing to re-train the models, and
without changing the model output distribution. This is
accomplished via speculative execution .
Speculative execution (Burton, 1985; Hennessy & Patterson,
2012) is an optimization technique, common in processors,
where a task is performed in parallel to verifying if it’s
actually needed - the payoff being increased concurrency.
A well-known example of speculative execution is branch
prediction. For speculative execution to be effective, we
need an efﬁcient mechanism to suggest tasks to execute
that are likely to be needed. In this work, we generalize
speculative execution to the stochastic setting - where a
taskmight be needed with some probability. Applying this
to decoding from autoregressive models like Transformers,
we sample generations from more efﬁcient approximation
models as speculative preﬁxes for the slower target mod-
els. With a novel sampling method, speculative sampling ,
we maximize the probability of these speculative tasks to
1arXiv:2211.17192v2  [cs.LG]  18 May 2023

--- PAGE 2 ---
Fast Inference from Transformers via Speculative Decoding
Figure 1. Our technique illustrated in the case of unconditional language modeling. Each line represents one iteration of the algorithm.
Thegreen tokens are the suggestions made by the approximation model (here, a GPT-like Transformer decoder with 6M parameters
trained on lm1b with 8k tokens) that the target model (here, a GPT-like Transformer decoder with 97M parameters in the same setting)
accepted, while the red andblue tokens are the rejected suggestions and their corrections, respectively. For example, in the ﬁrst line the
target model was run only once, and 5 tokens were generated.
be accepted, while guaranteeing that the outputs from our
system have the same distribution as those from the target
model alone. For example, the sentence in Figure 1, con-
sisting of 38 tokens, was generated by our method with
only 9 serial runs of a larger target model (97M parameters)
thanks to a smaller and more efﬁcient approximation model
(6M parameters), while the probability of generating it is
unchanged.
We analyze our method in a variety of tasks and model
sizes: unconditional generation from a 97M parameter GPT-
like model trained on lm1b, English to German translation
and news article summarization with an 11B parameters
T5-XXL model, and a dialog task with a 137B parameter
LaMDA model. We implement our method and compare
actual walltimes for T5-XXL to those of the robust T5X
implementation (Roberts et al., 2022), showing an out-of-
the-box latency improvement of 2X-3X , without any change
to the outputs (Section 4).
Our method is easy to employ in actual production settings,
doesn’t require training new models, and doesn’t change the
outputs. Therefore, in common situations where memory
bandwidth is the bottleneck, and compute resources are
available, it may be a good default to accelerate sampling
from autoregressive models like Transformers.
To summarize, our main contributions are: (1) A generaliza-
tion of speculative execution to the stochastic setting, with
a novel sampling method we call speculative sampling , and
(2) A decoding mechanism we call speculative decoding that
can accelerate decoding from autoregressive models, with-
out any change to the model architectures, training regimes
and output distributions.2. Speculative Decoding
2.1. Overview
LetMpbe the target model, inference from which we’re
trying to accelerate, and p(xtjx<t)the distribution we get
from the model for a preﬁx x<t. LetMqbe a more efﬁ-
cient approximation model for the same task, and denote
byq(xtjx<t)the distribution we get from the model for a
preﬁxx<t1. The core idea is to (1) use the more efﬁcient
modelMqto generate2Z+completions (see Section 3.5
for how to optimally choose this parameter), then (2) use
the target model Mpto evaluate all of the guesses and their
respective probabilities from Mqin parallel , accepting all
those that canlead to an identical distribution, and (3) sam-
pling an additional token from an adjusted distribution to ﬁx
the ﬁrst one that was rejected, or to add an additional one
if they are all accepted. That way, each parallel run of the
target model Mpwill produce at least one new token (so the
number of serial runs of the target model can never, even
in the worst case, be larger than the simple autoregressive
method), but it can potentially generate many new tokens,
up to+ 1, depending on how well Mqapproximates Mp.
2.2. Standardized Sampling
First, note that while there are many methods and parame-
ters of sampling, like argmax, top-k, nucleus, and setting
a temperature, and popular implementations usually treat
them differently at the logits level, they can all easily be cast
into standard sampling from an adjusted probability distribu-
tion. For example, argmax sampling is equivalent to zeroing
out non-max elements of the distribution and normalizing.
We can therefore only deal with standard sampling from a
1We’ll usep(x)to denotep(xtjx<t)whenever the preﬁx x<t
is clear from the context, and similarly for q(x).
2

--- PAGE 3 ---
Fast Inference from Transformers via Speculative Decoding
probability distribution, and cast all of the other types of
sampling into that framework. Going forward we’ll assume
thatp(x)andq(x)are the distributions from MpandMq
respectively, adjusted for the sampling method.
2.3. Speculative Sampling
To samplexp(x), we instead sample xq(x), keeping
it ifq(x)p(x), and in case q(x)> p(x)we reject the
sample with probability 1 p(x)
q(x)and samplexagain from an
adjusted distribution p0(x) =norm (max(0;p(x) q(x)))
instead. It’s easy to show (see Appendix A.1) that for any
distributions p(x)andq(x), andxsampled in this way,
indeedxp(x).
Given the distribution q(x)obtained from running Mqon
a conditioning prefix , we can sample a token x1q(x).
We then calculate the distribution p(x)by runningMpon
prefix while in parallel speculatively calculating the distri-
bution of the next token x2by runningMponprefix +[x1].
Once both computations complete, we proceed as per above:
Ifx1is rejected, we discard the computation of x2and
re-samplex1from an adjusted distribution, and if x1is ac-
cepted, we keep both tokens. Algorithm 1 generalizes this
idea to sample between 1 and + 1tokens at once.
Algorithm 1 SpeculativeDecodingStep
Inputs:Mp;Mq;prefix .
.Sampleguessesx1;:::; fromMqautoregressively.
fori= 1todo
qi(x) Mq(prefix + [x1;:::;xi 1])
xiqi(x)
end for
.RunMpin parallel.
p1(x);:::;p+1(x) 
Mp(prefix );:::;Mp(prefix + [x1;:::;x])
.Determine the number of accepted guesses n.
r1U(0;1);:::;rU(0;1)
n min(fi 1j1i;ri>pi(x)
qi(x)g[fg)
.Adjust the distribution from Mpif needed.
p0(x) pn+1(x)
ifn< then
p0(x) norm (max(0;pn+1(x) qn+1(x)))
end if
.Return one token from Mp, andntokens from Mq.
tp0(x)
returnprefix + [x1;:::;xn;t]
3. Analysis
3.1. Number of Generated Tokens
Let’s analyze the reduction factor in the number of serial
calls to the target model, or equivalently, the expected num-ber of tokens produced by a single run of Algorithm 1.
Deﬁnition 3.1. The acceptance rate x<t, given a preﬁx
x<t, is the probability of accepting xtq(xtjx<t)by
speculative sampling, as per Section 2.32.
E()is then a natural measure of how well Mqapproxi-
matesMp. If we make the simplifying assumption that the
s are i.i.d., and denote =E(), then the number of
tokens produced by a single run of Algorithm 1 is a capped
geometric variable, with success probability 1 and cap
+ 1, and the expected number of tokens generated by
Algorithm 1 satisﬁes Equation (1). See Figure 2.
E(#generatedtokens ) =1 +1
1 (1)
0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90
0 02 24 46 68 810 10
1 1E(tokens per iteration)Baseline
=1
=3
=5
=7
=
Figure 2. The expected number of tokens generated by Algorithm 1
as a function of for various values of .
3.2. Calculating 
We’ll now derive a simple formula for calculating given a
preﬁx and the two models MpandMq. We start by deﬁning
a natural divergence DLK:
Deﬁnition 3.2. DLK(p;q) =P
xjp(x) M(x)j=P
xjq(x) M(x)jwhereM(x) =p(x)+q(x)
2.
Lemma 3.3. DLK(p;q) = 1 P
xmin(p(x);q(x))
Proof.DLK(p;q) =P
xjp(x) M(x)j=P
xjp qj
2=
1 P
xp+q jp qj
2= 1 P
xmin(p(x);q(x))
From Lemma 3.3 we immediately get the following results:
2As before, we’ll omit the x<tsubscript whenever the preﬁx is
clear from the context.
3

--- PAGE 4 ---
Fast Inference from Transformers via Speculative Decoding
Corollary 3.4. DLK(p;q)is a symmetric divergence in [0;1]:
DLK(p;q) = 0()p=q:
DLK(p;q) = 1() p and q have disjoint support :
Theorem 3.5. = 1 DLK(p;q)
Proof. =Exq(x)(
1q(x)p(x)
p(x)
q(x)q(x)>p(x)=
Exq(x)min(1;p(x)
q(x)) =P
xmin(p(x);q(x))
Finally we get:
Corollary 3.6. = 1 E(DLK(p;q)) =E(min(p;q))
See Table 3 for empirically observed values in our experi-
ments.
3.3. Walltime Improvement
We’ve shown that with the i.i.d. assumption our algorithm
reduces the number of calls to the target model by a factor
of1 +1
1 . Note that speculative execution in general, and
our algorithm in particular, assume that we have enough
compute resources to support the increased concurrency
(Section 3.4). For the walltime anaylsis, we’ll assume that
we can run+ 1concurrent evaluations of Mpin parallel
without increasing the walltime. To get the total walltime
improvement, we now consider the cost of running the ap-
proximation model Mq.
Deﬁnition 3.7. Letc, the cost coefﬁcient , be the ratio be-
tween the time for a single run of Mqand the time for a
single run of Mp.
Note that unlike which is an intrinsic property of the
models and the task, the value of cdepends on the hardware
conﬁguration and software implementation details. In our
experiments where Mqis typically a couple of orders of
magnitude smaller than Mp,cwas always less than 0:05
and often negligibly close to 0.
Theorem 3.8. The expected improvement factor in total
walltime by Algorithm 1 is1 +1
(1 )(c+1).
Proof. Denote the cost of running a single step of MpbyT.
Now, each run of Algorithm 1 costs Tc+T(for running
the approximation model Mqtimes and running Mponce)
and according to Equation (1) produces1 +1
1 tokens on
average. So the overall expected cost for producing a token
with Algorithm 1 is(c+1)(1 )
1 +1T. Since the cost of pro-
ducing a single token with the standard decoding algorithm
isT, we get the desired result.
Note that Theorem 3.8 assumes long enough generations
(for example, since we run Mpat least once, the improve-
ment factor is capped by the number of generated tokens).Corollary 3.9. If>c , there exists for which we’ll get
an improvement, and the improvement factor will be at least
1+
1+c.
Proof. If we get an improvement for , we’d also get an
improvement for any 0< < , so for our method to
yield an improvement, we can evaluate Theorem 3.8 for
= 1, yielding1 2
(1 )(c+1)=1+
1+c.
3.4. Number of Arithmetic Operations
Algorithm 1 does +1runs ofMpin parallel, so the number
ofconcurrent arithmetic operations grows by a factor of
+1. Now, since Algorithm 1 produces at most +1tokens
per run, the total number of arithmetic operations might be
higher than that of the standard decoding algorithm. When
we accept the sample from Mqthe increased concurrency
is “free” and the total number of operations isn’t increased3.
When we reject a guess though, computation is wasted. Let’s
now analyze the effect of our method on the total number
of arithmetic operations.
Deﬁnition 3.10. Let^cbe the ratio of arithmetic operations
per token of the approximation model Mqto that of the
target model Mp.
Theorem 3.11. The expected factor of increase in the num-
ber of total operations of Algorithm 1 is(1 )(^c++1)
1 +1 .
Proof. Denote by ^Tthe number of arithmetic operations
done by a standard decoding baseline per token, i.e. the
number of operations of a single run of Mp. Then a single
iteration of Algorithm 1 costs ^T^c+^T(+ 1) operations
(forruns ofMqand+ 1parallel runs of Mp). Dividing
by the expected number of tokens produced by Algorithm 1,
i.e. Equation (1), and by ^T, we get the desired result.
Ifis low, the increase in the number of arithmetic oper-
ations is high, and vice-versa. Note that for Transformer
decoders, the total number of arithmetic operations by Al-
gorithm 1 (not counting runs of Mq)can be bounded from
above by a single run of the same-size Transformer encoder .
Unlike the total number of arithmetic operations, the total
number of memory accesses can go down with our method.
Speciﬁcally, the target model’s weights and KV cache can
be read once per execution of Algorithm 1, so the number
of memory accesses for reading them shrinks by a factor of
1 +1
1 , according to Equation (1).
3.5. Choosing 
Givencandand assuming enough compute resources (see
Section 3.4), the optimal is the one maximizing the wall-
3Neglecting the cost of Mq.
4

--- PAGE 5 ---
Fast Inference from Transformers via Speculative Decoding
0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90
0123456789101112131415161718192021222324Optimal 
c=0.01
c=0.02
c=0.05
c=0.1
Figure 3. The optimal as a function of for various values of c.
time improvement equation (Theorem 3.8):1 +1
(1 )(c+1).
Sinceis an integer, it can be easily found numerically, see
Figure 3.
Table 1 and Figure 4 illustrate the trade-off between infer-
ence speed and the total number of arithmetic operations for
various values of and, assumingc= ^c= 0. Figure 5
shows a simpliﬁed trace diagram.
Table 1. The total number of arithmetic operations and the infer-
ence speed vs the baseline, for various values of and, assuming
c= ^c= 0.
  OPERATIONS SPEED
0.6 2 1.53X 1.96X
0.7 3 1.58X 2.53X
0.8 2 1.23X 2.44X
0.8 5 1.63X 3.69X
0.9 2 1.11X 2.71X
0.9 10 1.60X 6.86X
Instead of picking a single value for based on, since the
s aren’t constant, we could get further improvement by pre-
dicting the value of and accordingly varying the value of 
during the run of Algorithm 1. To get an upper bound on the
additional improvement factor, assume we had an oracle for
. We would then have E(#generatedtokens ) =1
1 .
For typical values of cand, and assuming unbounded com-
pute resources, the enhanced walltime improvement factor
can be up to60% higher than the improvement factor with
a ﬁxed. We leave exploring this for future work4.
4The above bound assumes that we still run Mpto verify the or-
acle’s predictions. If we skip those veriﬁcations the bound doesn’t
hold and we would get a substantial additional improvement.
0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90
0 02 24 46 68 810 10
1 1Speed =1
Ops =1
Speed =3
Ops =3
Speed =5
Ops =5
Speed =7
Ops =7
Speed =10
Ops =10
Figure 4. The speedup factor and the increase in number of arith-
metic operations as a function of for various values of .
3.6. Approximation Models
Speculative sampling, and therefore speculative decoding,
guarantee an identical output distribution for any choice
of approximation model Mqwithout restriction (see Ap-
pendix A.1). In our experiments, we mostly tested existing
off-the-shelf smaller Transformers as the approximation
models. Further, we only tested approximation models of
the same architecture as the target models Mpand using the
same probability standardization. In this setup, choosing
Mqto be around two orders of magnitude smaller than Mp
usually performed best, balancing andc(Theorem 3.8).
Another type of approximation models, negligible-cost mod-
els, are those for which c0, i.e. approximation models
with a negligible cost relative to the target model. In this
case, we get an expected walltime improvement of1 +1
1 ,
which is bounded from above by1
1 (we approach equal-
ity ifis large). One interesting type of negligible-cost
approximation models are n-gram models, where the evalu-
ation amounts to a table lookup. Interestingly, in empirical
tests (Section 4.2) we get non zero s even for these triv-
ial n-gram models. For example, for the English-German
translation task, with Mpbeing T5-XXL 11B and Mqbeing
a trivial bigram model, we get 0:2which leads to an
inference speed improvement factor of 1:25X with= 3.
Other simple heuristics can be used as negligible-cost ap-
proximation models. For example, in cases where long se-
quences are likely to repeat, such as for summarization tasks
or chat-like interfaces5, an approximation model that simply
5E.g. where a user and a language model iterate on content, like
text or code (“can you rewrite this story but change the ending”,
“can you make this function also do X”).
5

--- PAGE 6 ---
Fast Inference from Transformers via Speculative Decoding
Wall time 
=7
=3
 BaseMp encoder
Mq encoder
Mp decoder
Mq decoder
Figure 5. A simpliﬁed trace diagram for a full encoder-decoder Transformer stack. The top row shows speculative decoding with = 7
so each of the calls to Mp(the purple blocks) is preceded by 7 calls to Mq(the blue blocks). The yellow block on the left is the call to the
encoder forMpand the orange block is the call to the encoder for Mq. Likewise the middle row shows speculative decoding with = 3,
and the bottom row shows standard decoding.
copies tokens from the context in case we ﬁnd a matching
preﬁx, might yield high values of . These parameter-less
approximation models, have the additional advantage of
being even simpler to deploy from a production standpoint.
Another type of approximation models that can be used by
speculative decoding are non-autoregressive models, like
those from (Stern et al., 2018). Then, instead of the au-
togreressive loop in Algorithm 1 we’d just call the non-
autoregressive model once.
A ﬁnal example, interesting mostly from a theoretical per-
spective, is an approximation model which chooses tokens
at random, which guarantees some improvement (although
very small) for all models Mp.
4. Experiments
4.1. Empirical Walltime Improvement
We implement our algorithm and compare it to the imple-
mentation in the T5X codebase for accelerating T5-XXL.
Setup We test a standard encoder-decoder T5 version 1.1
model (Raffel et al., 2020) on two tasks from the T5 paper:
(1) English to German translation ﬁne tuned on WMT EnDe,
and (2) Text summarization ﬁne tuned on CCN/DM. For
both tasks, we use T5-XXL (11B) for Mp. For the approx-
imation model Mqwe test several existing conﬁgurations,
namely T5-large (800M), T5-base (250M), and T5-small
(77M) (Raffel et al., 2020). We use existing checkpoints
for all models. We measure walltime improvements with a
batch size of 1 on a single TPU-v4 for both argmax sampling
(temp=0) and standard sampling (temp=1).
Results Table 2 shows the empirical results from our
method. We see that T5-small (77M), with a good balance
ofcand, provides the highest speedup out of the testedapproximation models. As expected we see that increases
with the size of the approximation model. Interestingly, 
and walltime improvement are higher for argmax sampling
(temp=0). We observe speedups of 2.6X (temp=1) and 3.4X
(temp=0) on the translation task and slightly lower speedups
of 2.3X (temp=1) and 3.1X (temp=0) for the summarization
task. These empirical results match well with the theoreti-
cal predictions, with some variance due to implementation
details (see Appendix A.3).
Table 2. Empirical results for speeding up inference from a T5-
XXL 11B model.
TASKMq TEMP  SPEED
ENDE T5- SMALLF 0 7 0.75 3.4X
ENDE T5- BASE 0 7 0.8 2.8X
ENDE T5- LARGE 0 7 0.82 1.7X
ENDE T5- SMALLF 1 7 0.62 2.6X
ENDE T5- BASE 1 5 0.68 2.4X
ENDE T5- LARGE 1 3 0.71 1.4X
CNNDM T5- SMALLF 0 5 0.65 3.1X
CNNDM T5- BASE 0 5 0.73 3.0X
CNNDM T5- LARGE 0 3 0.74 2.2X
CNNDM T5- SMALLF 1 5 0.53 2.3X
CNNDM T5- BASE 1 3 0.55 2.2X
CNNDM T5- LARGE 1 3 0.56 1.7X
4.2. Empirical Values
While we only implemented our method for T5, we mea-
suredvalues for various tasks, sampling methods, target
modelsMp, and approximation models Mq. Speciﬁcally,
we evaluated the expectation from Corollary 3.6 on 10K
tokens generated by Mp, for each of the settings below.
GPT-like (97M params) We test a decoder-only Trans-
former model on unconditional language generation, trained
on lm1b (Chelba et al., 2013). The model here is a GPT-
like Transformer decoder with Gelu activations (Hendrycks
& Gimpel, 2016). For Mqwe experimented with a Trans-
6

--- PAGE 7 ---
Fast Inference from Transformers via Speculative Decoding
former decoder model with 6M parameters: dim 256, dim
feed-forward 1024, 2 layers, 4 attention heads, as well as
simple unigram and bigram models. Mphas 97M parame-
ters: dim 768, dim feed-forward 3072, 12 layers, 12 atten-
tion heads. We used Bert tokenization (Devlin et al., 2019)
with 8k tokens for all models.
LaMDA (137B params) We tested a decoder only
LaMDA model on a dialog task (Thoppilan et al., 2022).
We used existing checkpoints from LaMDA 137B as Mp
and LaMDA 8B, LaMDA 2B, and LaMDA 100M for Mq.
See Section 4.1 for the setup of the T5-XXL (11B params)
model.
Table 3 summarizes the values for the tested cases. We
observe that approximation models that are a couple of
orders of magnitude smaller than the target model tend to
producevalues between 0.5 and 0.9. Interestingly, we also
note that for all models, the sharper the adjusted distribution,
the higher the values. Finally, we note that even trivial
unigram and bigram approximations yield non negligible
values. For example, for the case of English to German
translation, the bigram model has an value of 0.2, and
sincec= 0in this case, yields a 1.25X speed improvement,
which is surprisingly high for this trivial approximation
model (but is still lower than the speedup we get from using
T5-small as the approximation model).
5. Related work
The efﬁciency of inference from large models was studied
extensively (Dehghani et al., 2021). Many approaches aim
to speed up inference from large models in general, and au-
toregressive models like Transformers in particular. Numer-
ous techniques try to make inference more efﬁcient for all
tokens, e.g. distillation (Hinton et al., 2015), sparciﬁcation
(Jaszczur et al., 2021), quantization (Hubara et al., 2016),
and architecture modiﬁcation (So et al., 2021; Shazeer,
2019). Closer to our approach are adaptive computation
methods which adapt the amount of computation to problem
difﬁculty (Han et al., 2021). Examples include attending to a
subset of the inputs (Sukhbaatar et al., 2019), and early exits
(Schuster et al., 2021; Scardapane et al., 2020; Bapna et al.,
2020; Elbayad et al., 2019; Schwartz et al., 2020). Notably,
Wisdom of Committees (Schwartz et al., 2020) leverages
off-the-shelf smaller models, but is an adaptive computation
approach, and so it uses a heuristic to determine when to
stop, losing the guarantee of identical outputs to those of
the target models. In general, adaptive computation meth-
ods usually learn, either within the model itself or with an
auxiliary model, when a computation shortcut can be taken.
Usually, these methods save on both inference time and
arithmetic operations, but require a change of architecture, a
change of training procedure and training custom models orTable 3. Empiricalvalues for various target models Mp, approx-
imation models Mq, and sampling settings. T=0 and T=1 denote
argmax and standard sampling respectively6.
Mp Mq SMPL
GPT- LIKE (97M) U NIGRAM T =0 0.03
GPT- LIKE (97M) B IGRAM T =0 0.05
GPT- LIKE (97M) GPT- LIKE (6M) T=0 0.88
GPT- LIKE (97M) U NIGRAM T =1 0.03
GPT- LIKE (97M) B IGRAM T =1 0.05
GPT- LIKE (97M) GPT- LIKE (6M) T=1 0.89
T5-XXL (E NDE) U NIGRAM T =0 0.08
T5-XXL (E NDE) B IGRAM T =0 0.20
T5-XXL (E NDE) T5- SMALL T =0 0.75
T5-XXL (E NDE) T5- BASE T =0 0.80
T5-XXL (E NDE) T5- LARGE T =0 0.82
T5-XXL (E NDE) U NIGRAM T =1 0.07
T5-XXL (E NDE) B IGRAM T =1 0.19
T5-XXL (E NDE) T5- SMALL T =1 0.62
T5-XXL (E NDE) T5- BASE T =1 0.68
T5-XXL (E NDE) T5- LARGE T =1 0.71
T5-XXL (CNNDM) U NIGRAM T =0 0.13
T5-XXL (CNNDM) B IGRAM T =0 0.23
T5-XXL (CNNDM) T5- SMALL T =0 0.65
T5-XXL (CNNDM) T5- BASE T =0 0.73
T5-XXL (CNNDM) T5- LARGE T =0 0.74
T5-XXL (CNNDM) U NIGRAM T =1 0.08
T5-XXL (CNNDM) B IGRAM T =1 0.16
T5-XXL (CNNDM) T5- SMALL T =1 0.53
T5-XXL (CNNDM) T5- BASE T =1 0.55
T5-XXL (CNNDM) T5- LARGE T =1 0.56
LAMDA (137B) L AMDA (100M) T=0 0.61
LAMDA (137B) L AMDA (2B) T=0 0.71
LAMDA (137B) L AMDA (8B) T=0 0.75
LAMDA (137B) L AMDA (100M) T=1 0.57
LAMDA (137B) L AMDA (2B) T=1 0.71
LAMDA (137B) L AMDA (8B) T=1 0.74
re-training of existing models. They usually also change the
outputs of the model. We note that while many of the meth-
ods above improve the memory to arithmetic-operations
ratio, in cases where the ratio remains high, these methods
and our speculative decoding method might be effective in
tandem.
Two prior methods leverage speculative execution for speed-
ing up decoding from autoregressive models. Blockwise
Parallel Decoding (Stern et al., 2018) decodes several to-
kens in parallel, similarly to our work. However, it only
supports greedy decoding (temperature=0) and not the gen-
eral stochastic setting, it requires additional training of a
custom model, and focuses on preserving down-stream task
quality, instead of guaranteeing identical outputs. Shallow
Aggressive Decoding (SAD) (Sun et al., 2021) also decodes
several tokens in parallel, similarly to our work. Unlike
our work, SAD only supports copying the input to the out-
7

--- PAGE 8 ---
Fast Inference from Transformers via Speculative Decoding
put, and not general approximation models, making it only
suitable for the cases where the inputs and outputs are very
similar like grammatical error correction. In addition, simi-
larly to Blockwise Parallel Decoding, SAD does not support
the general stochastic sampling setting.
After we initially published our work, an independent im-
plementation of speculative decoding (Chen et al., 2023)
showed similar 2X-2.5X improvements on Chinchilla 70B.
6. Discussion
We presented speculative sampling which enables efﬁcient
stochastic speculative execution - i.e. speculative execu-
tion in the stochastic setting. We analyzed its impact on
decoding from autoregressive models like Transformers via
speculative decoding and have shown that given enough
compute resources, we get meaningful 2X-3X speedups in
practice vs T5X, a popular optimized implementation.
One limitation of speculative execution in general, and of
speculative decoding in particular, is that latency is im-
proved through increased concurrency at the cost of an in-
creased number of arithmetic operations. Thus, our method
is not helpful for conﬁgurations where additional compu-
tation resources are not available. However, in common
cases where additional computation resources are available
(e.g. when memory bandwidth is the bottleneck) our method
provides the speedup with signiﬁcant beneﬁts: the model
architecture doesn’t change, retraining isn’t required, and
most importantly, the output distribution is guaranteed to
stay the same . Our method is easy to implement, and can
be used to speedup inference using out-of-the-box models
without developing and evaluating custom schemes.
There are several directions for follow up research, impor-
tantly, further investigating the compatibility of speculative
decoding with beam search (see Appendix A.4). Also, while
our method yields substantial speedups with existing off-the-
shelf approximation models, greater improvements might
be obtained via custom approximation models (Section 3.6),
such as those with custom architectures (e.g. custom sizes,
non-autoregressive models, or various heuristics) or with
custom training procedures (e.g. standard distillation with
soft targets from Mp, or optimizing Mqfordirectly). It
could also be interesting to explore a hierarchical version
of the algorithm, where the approximation model is itself
accelerated by an even faster model, which could allow
for more capable approximation models. In this work we
ﬁxed the approximation model and the number of guesses
throughout inference, but varying them during inference
could yield additional improvements (Section 3.5). In our
6Note that the outputs from the LaMDA model always go
through aTop 40ﬁlter. This has no effect on argmax, but does
have some effect on standard sampling.experiments we always performed the same standardization
on the distributions generated by the approximation model
as the desired one for the target model (Section 2.2), but fur-
ther improvements might be obtained by applying different
transformations. We tested speculative decoding only in the
text modality, but it might work well in other domains (e.g.
images) which would be interesting to experiment with.
Finally, we note that stochastic speculative execution and
speculative sampling can be helpful outside the scope of
speculative decoding from autoregressive models. For ex-
ample, given two slow functions, f(x)andg(y)such that
f(x)generates a distribution from which g’s input is sam-
pled, we could use our method to run fandgin parallel.
This setup might arise e.g. in physics simulations, or in rein-
forcement learning where fis a large model that produces a
distribution on actions, and gis the world simulation, which
would be interesting to explore.
Acknowledgments
We would like to extend a special thank you to YaGuang Li
for help with everything LaMDA related and for calculating
the LaMDA ﬁgures in the paper, and to Blake Hechtman
for great insights and help with XLA. We would also like
to thank the reviewers for insightful comments, as well
as Asaf Aharoni, Reiner Pope, Sasha Goldshtein, Nadav
Sherman, Eyal Segalis, Eyal Molad, Dani Valevski, Daniel
Wasserman, Valerie Nygaard, Danny Vainstein, the LaMDA
and Theta Labs teams at Google, and our families.
References
Bapna, A., Arivazhagan, N., and Firat, O. Controlling
computation versus quality for neural sequence models.
ArXiv , abs/2002.07106, 2020.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu,
J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
S., Radford, A., Sutskever, I., and Amodei, D. Lan-
guage models are few-shot learners. In Proceedings of
the 34th International Conference on Neural Informa-
tion Processing Systems , NIPS’20, Red Hook, NY , USA,
2020. Curran Associates Inc. ISBN 9781713829546.
Burton, F. W. Speculative computation, parallelism, and
functional programming. IEEE Transactions on Comput-
ers, C-34(12):1190–1193, 1985. doi: 10.1109/TC.1985.
6312218.
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T.,
Koehn, P. T., and Robinson, T. One billion word bench-
8

--- PAGE 9 ---
Fast Inference from Transformers via Speculative Decoding
mark for measuring progress in statistical language mod-
eling. In Interspeech , 2013.
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre,
L., and Jumper, J. M. Accelerating large language
model decoding with speculative sampling. ArXiv ,
abs/2302.01318, 2023.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S.,
Maynez, J., Rao, A., Barnes, P., Tay, Y ., Shazeer, N. M.,
Prabhakaran, V ., Reif, E., Du, N., Hutchinson, B. C.,
Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,
G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev,
S., Michalewski, H., Garc ´ıa, X., Misra, V ., Robinson,
K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim,
H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D.,
Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pel-
lat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov,
O., Lee, K., Zhou, Z., Wang, X., Saeta, B., D ´ıaz, M., Fi-
rat, O., Catasta, M., Wei, J., Meier-Hellstern, K. S., Eck,
D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling lan-
guage modeling with pathways. ArXiv , abs/2204.02311,
2022.
Dehghani, M., Arnab, A., Beyer, L., Vaswani, A., and Tay,
Y . The efﬁciency misnomer. ArXiv , abs/2110.12894,
2021.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. ArXiv , abs/1810.04805, 2019.
Elbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive
transformer. ArXiv , abs/1910.10073, 2019.
Han, Y ., Huang, G., Song, S., Yang, L., Wang, H., and Wang,
Y . Dynamic neural networks: A survey. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence , 44:
7436–7456, 2021.
Hendrycks, D. and Gimpel, K. Bridging nonlinearities and
stochastic regularizers with gaussian error linear units.
ArXiv , abs/1606.08415, 2016.
Hennessy, J. L. and Patterson, D. A. Computer Architecture:
A Quantitative Approach . Morgan Kaufmann, Amster-
dam, 5 edition, 2012. ISBN 978-0-12-383872-8.
Hinton, G. E., Vinyals, O., and Dean, J. Distilling the
knowledge in a neural network. ArXiv , abs/1503.02531,
2015.
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and
Bengio, Y . Quantized neural networks: Training neu-
ral networks with low precision weights and activations.
ArXiv , abs/1609.07061, 2016.Jaszczur, S., Chowdhery, A., Mohiuddin, A., Kaiser, L.,
Gajewski, W., Michalewski, H., and Kanerva, J. Sparse
is enough in scaling transformers. In Neural Information
Processing Systems , 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring
the limits of transfer learning with a uniﬁed text-to-text
transformer. The Journal of Machine Learning Research ,
21(1):5485–5551, 2020.
Roberts, A., Chung, H. W., Levskaya, A., Mishra, G., Brad-
bury, J., Andor, D., Narang, S., Lester, B., Gaffney, C.,
Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Salcianu,
A., van Zee, M., Austin, J., Goodman, S., Soares, L. B.,
Hu, H., Tsvyashchenko, S., Chowdhery, A., Bastings,
J., Bulian, J., Garc ´ıa, X., Ni, J., Chen, A., Kenealy, K.,
Clark, J., Lee, S., Garrette, D. H., Lee-Thorp, J., Raffel,
C., Shazeer, N. M., Ritter, M., Bosma, M., Passos, A.,
Maitin-Shepard, J. B., Fiedel, N., Omernick, M., Saeta,
B., Sepassi, R., Spiridonov, A., Newlan, J., and Ges-
mundo, A. Scaling up models and data with t5x and
seqio. ArXiv , abs/2203.17189, 2022.
Scardapane, S., Scarpiniti, M., Baccarelli, E., and Uncini,
A. Why should we add early exits to neural networks?
Cognitive Computation , 12(5):954–966, 2020.
Schuster, T., Fisch, A., Jaakkola, T., and Barzilay, R. Con-
sistent accelerated inference via conﬁdent adaptive trans-
formers. In Conference on Empirical Methods in Natural
Language Processing , 2021.
Schwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J.,
and Smith, N. A. The right tool for the job: Matching
model and instance complexities. In Annual Meeting of
the Association for Computational Linguistics , 2020.
Shazeer, N. M. Fast transformer decoding: One write-head
is all you need. ArXiv , abs/1911.02150, 2019.
So, D. R., Ma’nke, W., Liu, H., Dai, Z., Shazeer, N. M., and
Le, Q. V . Primer: Searching for efﬁcient transformers for
language modeling. ArXiv , abs/2109.08668, 2021.
Stern, M., Shazeer, N., and Uszkoreit, J. Blockwise parallel
decoding for deep autoregressive models. Advances in
Neural Information Processing Systems , 31, 2018.
Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.
Adaptive attention span in transformers. In Annual Meet-
ing of the Association for Computational Linguistics ,
2019.
Sun, X., Ge, T., Wei, F., and Wang, H. Instantaneous gram-
matical error correction with shallow aggressive decoding.
ArXiv , abs/2106.04970, 2021.
9

--- PAGE 10 ---
Fast Inference from Transformers via Speculative Decoding
Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N. M., Kul-
shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,
Du, Y ., Li, Y ., Lee, H., Zheng, H., Ghafouri, A., Mene-
gali, M., Huang, Y ., Krikun, M., Lepikhin, D., Qin, J.,
Chen, D., Xu, Y ., Chen, Z., Roberts, A., Bosma, M.,
Zhou, Y ., Chang, C.-C., Krivokon, I. A., Rusch, W. J.,
Pickett, M., Meier-Hellstern, K. S., Morris, M. R., Doshi,
T., Santos, R. D., Duke, T., Søraker, J. H., Zevenber-
gen, B., Prabhakaran, V ., D ´ıaz, M., Hutchinson, B., Ol-
son, K., Molina, A., Hoffman-John, E., Lee, J., Aroyo,
L., Rajakumar, R., Butryna, A., Lamm, M., Kuzmina,
V . O., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R.,
Aguera-Arcas, B., Cui, C., Croak, M., hsin Chi, E. H., and
Le, Q. Lamda: Language models for dialog applications.
ArXiv , abs/2201.08239, 2022.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. At-
tention is all you need. Advances in neural information
processing systems , 30, 2017.
Yu, J., Xu, Y ., Koh, J. Y ., Luong, T., Baid, G., Wang, Z.,
Vasudevan, V ., Ku, A., Yang, Y ., Ayan, B. K., Hutchinson,
B. C., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge,
J., and Wu, Y . Scaling autoregressive models for content-
rich text-to-image generation. ArXiv , abs/2206.10789,
2022.
10

--- PAGE 11 ---
Fast Inference from Transformers via Speculative Decoding
A. Appendix
A.1. Correctness of Speculative Sampling
We will now show that for any distributions p(x)andq(x), the tokens sampled via speculative sampling fromp(x)andq(x)
are distributed identically to those sampled from p(x)alone. Letbe the acceptance probability (Deﬁnition 3.1).
Note that as p0(x) =norm (max(0;p(x) q(x))) =p(x) min(q(x);p(x))P
x0(p(x0) min(q(x0);p(x0)))=p(x) min(q(x);p(x))
1 , the normalizing
constant for the adjusted distribution p0(x)is1 , where the last equation follows immediately from Lemma 3.3 and
Theorem 3.5.
Now:
P(x=x0) =P(guessaccepted;x =x0) +P(guessrejected;x =x0)
Where:
P(guessaccepted;x =x0) =q(x0) min(1;p(x0)
q(x0)) = min(q(x0);p(x0))
And:
P(guessrejected;x =x0) = (1 )p0(x0) =p(x0) min(q(x0);p(x0))
Overall:
P(x=x0) = min(p(x0);q(x0)) +p(x0) min(p(x0);q(x0)) =p(x0):
As desired.
A.2. Speculative Sampling vs. Rejection Sampling
Rejection sampling is the following iterative sampling procedure that looks superﬁcially similar to ours:
1. Samplexq(x)andrU(0;1).
2. Ifr<p(x)
Mq(x)returnx.
3. Go to 1.
WhereM=maxxp(x)
q(x). We could employ a non-iterative version of rejection sampling instead of speculative sampling
- speciﬁcally go through steps 1 and 2 above, and otherwise sample from an unmodiﬁed p(x)directly. That would
be much less efﬁcient than our method though. Speciﬁcally, the expected accept probability here is Exq(x)p(x)
Mq(x)=
P
xp(x) minx0q(x0)
p(x0)P
xp(x) min(1;q(x)
p(x)) =P
xmin(p(x);q(x)) =is (potentially much) lower than the expected
accept probability in our method .
A.3. Theoretical Predictions vs. Empirical Runtimes
Table 4 compares the expected runtime improvements based on Theorem 3.8 to the empirically measured runtimes from
Table 2. We estimated the values of cfor the various models based on proﬁler traces. We can see that the theoretical
predictions mostly match the measured runtimes. The larger differences are due to: (1) optimization differences between our
implementation and the baseline, and (2) the simplifying assumption that the s are i.i.d. being only an approximation (see
Section 3.1).
11

--- PAGE 12 ---
Fast Inference from Transformers via Speculative Decoding
Table 4. Expected improvement factor (E XP) vs. empirically measured improvement factor (E MP).
TASKMq TEMP  c EXP EMP
ENDE T5- SMALL 0 7 0.75 0.02 3.2 3.4
ENDE T5- BASE 0 7 0.8 0.04 3.3 2.8
ENDE T5- LARGE 0 7 0.82 0.11 2.5 1.7
ENDE T5- SMALL 1 7 0.62 0.02 2.3 2.6
ENDE T5- BASE 1 5 0.68 0.04 2.4 2.4
ENDE T5- LARGE 1 3 0.71 0.11 2.0 1.4
CNNDM T5- SMALL 0 5 0.65 0.02 2.4 3.1
CNNDM T5- BASE 0 5 0.73 0.04 2.6 3.0
CNNDM T5- LARGE 0 3 0.74 0.11 2.0 2.2
CNNDM T5- SMALL 1 5 0.53 0.02 1.9 2.3
CNNDM T5- BASE 1 3 0.55 0.04 1.8 2.2
CNNDM T5- LARGE 1 3 0.56 0.11 1.6 1.7
A.4. Application to Beam Search
Our method can be applied, with some performance penalty, to beam search sampling. Given the original beam width w, we
can perform beam search with the approximation model Mqand beam width uwforsteps. Then, we can use Mpto
check all of the candidates in parallel (costing a compute budget of (w+u)runs ofMp). Finally, for each step, we can
accept the guesses of Mqas long astopw(Mp)topu(Mq)to get identical results to regular beam search with Mpalone
(with a more elaborate procedure we could also accept cases where the candidates we got happen to have higher probabilities
than those of Mpalone). The analysis of our method in this setting is more involved and we leave it for future work.
A.5. Lenience
A strong property of Algorithm 1 is that the output distribution is guaranteed to remain unchanged. That said, if we’re
willing to allow some changes, with nice guarantees, we can get further inference speed improvements. To further motivate
this, note that when we train two models with identical architectures and sizes on the same dataset, the generated probability
distributions will not be identical, so some lenience might make sense. Note that the results in this paper except for this
section use the strictest version of Algorithm 1 and don’t allow lenience of any kind.
We could include a lenience parameter l2[0;1]and multiply q(x)bylbefore comparing with p(x)in Algorithm 1. This
still maintains the nice guarantee that no token can be sampled with probability greater thanp(x)
l. This means for example,
that withl=1
10no token can be sampled with more than 10X its ground truth probability, so we can guarantee that
extremely rare tokens will remain extremely rare (there is no guarantee on the minimum probability, so lenience could hurt
the diversity of the samples).
Speciﬁcally, with a lenience factor lwe have=Exq(x)(
1lq(x)p(x)
p(x)
lq(x)lq(x)>p(x)=Exq(x)p(x)
max(p(x);lq(x))=
P
xp(x)q(x)
max(p(x);lq(x))=1
lP
xmin(p(x);lq(x)) =P
xmin(p(x)
l;q(x)).
Table 5 shows values for different values of lwhenMpis T5-XXL (11B) and Mqis T5-small (77M). With c= 0:015,
using lenience values of 1, 0.5, 0.3, and 0.1 (meaning that no token can be sampled with probability greater than 1X, 2X, 3X
and 10X of the ground truth) we get improvement factors of 2.5X, 3.1X, 3.6X, and 5X respectively.
Table 5.values for various values of lwith standard sampling where Mpis T5-XXL (11B) on the EnDe translation task.
Mq l= 1l= 0:5l= 0:3l= 0:1
UNIGRAM 0.07 0.1 0.11 0.16
BIGRAM 0.19 0.23 0.25 0.32
T5- SMALL (77M) 0.62 0.71 0.76 0.84
T5- BASE (250M) 0.68 0.8 0.83 0.90
12

--- PAGE 13 ---
Fast Inference from Transformers via Speculative Decoding
Note that when using temperature = 0 (i.e. argmax sampling), we can no longer use lenience as above. Instead, we could
allow some lenience before standardizing the distributions. For example, we could accept the token xsampled from Mqin
casep(x)lmax(p). In this case, we measure similar empirical increases in values to those with temperature = 1. For
example, when using lenience values of 1, 0.5, 0.3, and 0.1 for MpT5-XXLMqT5-small for English-German translation,
we getvalues of 0.75, 0.75, 0.8, 0.87. Taking for example c= 0:015and= 8we get speed improvement factors of
3.3X, 3.3X, 3.9X, and 4.9X respectively7.
7In this case, unlike in the standard sampling case shown in Table 5, a lenience factor of 0.5 doesn’t improve the speed-up.
13
