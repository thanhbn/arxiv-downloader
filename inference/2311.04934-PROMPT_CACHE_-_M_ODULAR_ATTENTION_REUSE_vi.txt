# PROMPT CACHE : TÁI SỬ DỤNG ATTENTION MODULE HÓA
CHO SUY LUẬN ĐỘ TRỄ THẤP

In Gim1Guojun Chen1Seung-seob Lee1Nikhil Sarda2Anurag Khandelwal1Lin Zhong1

TÓM TẮT
Chúng tôi trình bày Prompt Cache, một phương pháp để tăng tốc quá trình suy luận cho các mô hình ngôn ngữ lớn (LLM) bằng cách tái sử dụng trạng thái attention giữa các prompt LLM khác nhau. Nhiều prompt đầu vào có các đoạn văn bản chồng chéo, chẳng hạn như thông điệp hệ thống, mẫu prompt và tài liệu được cung cấp để làm ngữ cảnh. Insight chính của chúng tôi là bằng cách tính toán trước và lưu trữ trạng thái attention của những đoạn văn bản xuất hiện thường xuyên này trên máy chủ suy luận, chúng tôi có thể tái sử dụng chúng một cách hiệu quả khi những đoạn này xuất hiện trong prompt của người dùng. Prompt Cache sử dụng một schema để định nghĩa rõ ràng những đoạn văn bản có thể tái sử dụng như vậy, được gọi là các module prompt. Schema đảm bảo độ chính xác về vị trí trong quá trình tái sử dụng trạng thái attention và cung cấp cho người dùng một giao diện để truy cập các trạng thái được cache trong prompt của họ. Sử dụng một triển khai nguyên mẫu, chúng tôi đánh giá Prompt Cache trên nhiều LLM. Chúng tôi cho thấy rằng Prompt Cache giảm đáng kể độ trễ trong time-to-first-token, đặc biệt đối với các prompt dài hơn như trả lời câu hỏi dựa trên tài liệu và khuyến nghị. Các cải thiện dao động từ 8× cho suy luận trên GPU đến 60× cho suy luận trên CPU, tất cả trong khi duy trì độ chính xác đầu ra và không cần sửa đổi tham số mô hình.

1 GIỚI THIỆU

Một phần đáng kể các prompt của mô hình ngôn ngữ lớn (LLM) được tái sử dụng thường xuyên. Ví dụ, các prompt thường bắt đầu bằng "thông điệp hệ thống" giống hệt nhau cung cấp hướng dẫn ban đầu cho chức năng của nó. Các tài liệu cũng có thể chồng chéo trong nhiều prompt. Trong một loạt các ứng dụng LLM ngữ cảnh dài, như phân tích pháp lý (Cui et al., 2023; Nay et al., 2023), ứng dụng chăm sóc sức khỏe (Steinberg et al., 2021; Rasmy et al., 2021), và giáo dục (Shen et al., 2021), prompt bao gồm một hoặc nhiều tài liệu từ một pool. Ngoài ra, các prompt thường được định dạng với các mẫu có thể tái sử dụng (White et al., 2023) như một kết quả của kỹ thuật prompt engineering. Những ví dụ như vậy thường gặp trong LLM cho robotics và tool learning (Huang et al., 2022; Driess et al., 2023; Qin et al., 2023). Điều này dẫn đến mức độ chồng chéo cao giữa các prompt sử dụng cùng một mẫu.

Chúng tôi giới thiệu một kỹ thuật mới được gọi là Prompt Cache để giảm overhead tính toán trong suy luận LLM sinh tạo. Prompt Cache được thúc đẩy bởi quan sát rằng các prompt đầu vào cho LLM thường có cấu trúc có thể tái sử dụng. Ý tưởng chính là tính toán trước trạng thái attention của các đoạn prompt được xem lại thường xuyên trong bộ nhớ, và tái sử dụng chúng khi những đoạn này xuất hiện trong prompt để giảm độ trễ.

Tái sử dụng trạng thái attention là một chiến lược phổ biến để tăng tốc việc phục vụ một prompt đơn (Pope et al., 2022). Phương pháp hiện có, thường được gọi là Key-Value (KV) Cache, tái sử dụng trạng thái attention key-value của các token đầu vào trong quá trình sinh token tự hồi quy. Điều này loại bỏ nhu cầu tính toán attention đầy đủ cho mỗi lần sinh token (§2.2). Bằng cách cache attention key-value được tính toán cho token được sinh trước đó, mỗi lần sinh token chỉ yêu cầu tính toán trạng thái attention key-value một lần.

Xây dựng trên KV Cache, Prompt Cache mở rộng việc tái sử dụng trạng thái attention từ một prompt đơn sang nhiều prompt bằng cách làm cho việc tái sử dụng trạng thái attention trở nên module hóa. Trong phương pháp của chúng tôi, các đoạn văn bản được tái sử dụng thường xuyên được tính toán trước riêng lẻ và lưu trữ trong bộ nhớ. Khi những đoạn "đã cache" như vậy xuất hiện trong prompt đầu vào, hệ thống sử dụng trạng thái attention key-value được tính toán trước từ bộ nhớ thay vì tính toán lại chúng. Kết quả là, tính toán attention chỉ được yêu cầu cho các đoạn văn bản chưa được cache. Hình 1 minh họa sự khác biệt giữa sinh tự hồi quy đầy đủ, KV Cache và Prompt Cache. Chúng tôi lưu ý rằng lợi thế về hiệu suất trở nên rõ rệt hơn khi kích thước của các đoạn được cache tăng lên vì overhead tính toán của trạng thái attention tăng theo cấp số hai với kích thước chuỗi đầu vào (Keles et al., 2022; Tay et al., 2023) trong khi độ phức tạp về không gian và tính toán của Prompt Cache tăng tuyến tính với kích thước.

Hai thách thức nảy sinh khi tái sử dụng trạng thái attention giữa các prompt khác nhau. Đầu tiên, trạng thái attention phụ thuộc vào vị trí do positional encoding trong Transformers. Do đó, trạng thái attention của một đoạn văn bản chỉ có thể được tái sử dụng nếu đoạn đó xuất hiện ở cùng vị trí. Thứ hai, hệ thống phải có khả năng nhận ra hiệu quả một đoạn văn bản có trạng thái attention có thể đã được cache để tái sử dụng.

Để giải quyết hai vấn đề này, Prompt Cache kết hợp hai ý tưởng. Đầu tiên là làm cho cấu trúc của prompt trở nên rõ ràng với Prompt Markup Language (PML). PML làm cho các đoạn văn bản có thể tái sử dụng trở nên rõ ràng như các module, tức là module prompt. Nó không chỉ giải quyết vấn đề thứ hai ở trên mà còn mở ra cánh cửa để giải quyết vấn đề đầu tiên, vì mỗi module prompt có thể được gán với các ID vị trí duy nhất. Ý tưởng thứ hai của chúng tôi là phát hiện thực nghiệm rằng LLM có thể hoạt động trên trạng thái attention với các ID vị trí không liên tục. Điều này có nghĩa là chúng ta có thể trích xuất các đoạn khác nhau của trạng thái attention và nối chúng lại để tạo thành tập hợp con các ý nghĩa. Chúng tôi tận dụng điều này để cho phép người dùng chọn các module prompt dựa trên nhu cầu của họ, hoặc thậm chí cập nhật một số module prompt trong thời gian chạy.

Chúng tôi giải thích cách Prompt Cache hoạt động trong §3. Tóm tắt, một người dùng LLM viết prompt của họ bằng PML, với ý định rằng họ có thể tái sử dụng trạng thái attention dựa trên các module prompt. Quan trọng là, họ phải tạo ra một prompt từ một schema, cũng được viết bằng PML. Hình 2 cho thấy một ví dụ prompt dựa trên một schema ví dụ. Khi Prompt Cache nhận được một prompt, trước tiên nó xử lý schema của nó và tính toán trạng thái attention cho các module prompt của nó. Nó tái sử dụng những trạng thái này cho các module prompt trong prompt và các prompt khác được tạo ra từ cùng một schema.

Trong §4, chúng tôi báo cáo một triển khai nguyên mẫu của Prompt Cache trên thư viện HuggingFace transformers (Wolf et al., 2020). Trong khi Prompt Cache có thể hoạt động với bất kỳ kiến trúc Transformer nào tương thích với KV Cache, chúng tôi thử nghiệm với ba kiến trúc Transformer phổ biến hỗ trợ các LLM mã nguồn mở sau: Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), và MPT (MosaicML, 2023). Chúng tôi xem xét hai loại bộ nhớ để lưu trữ các module prompt: bộ nhớ CPU và GPU. Trong khi bộ nhớ CPU có thể mở rộng đến mức terabyte, nó mang lại overhead sao chép bộ nhớ host-to-device. Ngược lại, bộ nhớ GPU không yêu cầu sao chép nhưng có dung lượng hạn chế.

Sử dụng nguyên mẫu, chúng tôi tiến hành đánh giá benchmark mở rộng để kiểm tra hiệu suất và định lượng độ chính xác của Prompt Cache trên các bộ dữ liệu ngữ cảnh dài khác nhau (§5). Chúng tôi sử dụng bộ LongBench (Bai et al., 2023), bao gồm các tác vụ khuyến nghị và trả lời câu hỏi (QA) dựa trên nhiều tài liệu. Trong đánh giá của chúng tôi, Prompt Cache giảm độ trễ time-to-first-token (TTFT) từ 1.5× đến 10× cho suy luận GPU với các module prompt trên bộ nhớ GPU và từ 20× đến 70× cho suy luận CPU, tất cả đều không có mất mát độ chính xác đáng kể. Ngoài ra, chúng tôi phân tích overhead bộ nhớ của các trạng thái attention được tính toán trước cho mỗi mô hình và thảo luận các hướng tối ưu hóa dấu chân bộ nhớ của Prompt Cache. Chúng tôi sau đó trình bày một số tác vụ sinh tạo, bao gồm cá nhân hóa, sinh mã và prompt tham số hóa, để chứng minh tính biểu đạt của schema prompt và cải thiện hiệu suất với sự suy giảm chất lượng không đáng kể.

Trong nghiên cứu hiện tại, chúng tôi chủ yếu tập trung vào các kỹ thuật tái sử dụng attention module hóa. Tuy nhiên, chúng tôi dự báo Prompt Cache sẽ được sử dụng như một thành phần nền tảng cho các hệ thống phục vụ LLM trong tương lai. Những hệ thống như vậy có thể kết hợp quản lý module prompt nâng cao và chiến lược thay thế cache GPU, tối ưu hóa lợi thế của cả host DRAM và GPU HBM. Mã nguồn và dữ liệu được sử dụng để đánh giá có sẵn tại github.com/yale-sys/prompt-cache.

2 BỐI CẢNH VÀ CÔNG TRÌNH LIÊN QUAN

Prompt Cache xây dựng trên các ý tưởng của KV Cache, tức là tái sử dụng trạng thái attention key-value trong quá trình giải mã tự hồi quy trong LLM. Phần này xem xét việc sinh token tự hồi quy trong LLM, giải thích cách việc kết hợp KV Cache có thể tăng tốc quá trình sinh token, xác định các xấp xỉ của nó, và khảo sát công việc gần đây tận dụng KV Cache để tăng tốc. Chúng tôi cũng thảo luận ngắn gọn các kỹ thuật khác hiện có để tăng tốc suy luận LLM.

2.1 Sinh Token Tự Hồi Quy

Một LLM sinh ra các token đầu ra một cách tự hồi quy (Radford et al., 2018). Nó bắt đầu với một đầu vào ban đầu, thường được gọi là prompt, và sinh ra token tiếp theo dựa trên prompt. Mô hình sau đó thêm token vào prompt và sử dụng nó để sinh ra token tiếp theo. Quá trình sinh tiếp tục cho đến khi một điều kiện dừng được đáp ứng. Điều này có thể là sau một số lượng token được xác định trước, khi sinh ra một token đặc biệt kết thúc chuỗi, hoặc khi chuỗi được sinh đạt được mức độ nhất quán hoặc hoàn chỉnh thỏa đáng. Quan trọng là, trong mỗi bước, mô hình lấy toàn bộ prompt và các token được sinh cho đến nay làm đầu vào, và lặp lại.

2.2 Key-Value Cache

Việc sinh token tự hồi quy được mô tả ở trên phát sinh overhead tính toán đáng kể do cơ chế self-attention được áp dụng trên toàn bộ đầu vào trong mỗi bước. Để cải thiện điều này, cơ chế Key-Value (KV) Cache (Pope et al., 2022) thường được sử dụng. Kỹ thuật này tính toán key và value embeddings cho mỗi token chỉ một lần trong suốt quá trình sinh token tự hồi quy.

Để làm rõ, ký hiệu prompt người dùng là một chuỗi n token: s1, ..., sn, và k token được sinh sau đó là sn+1, ..., sn+k. Trong việc sinh token tự hồi quy ngây thơ, trạng thái attention {(k1, v1), ..., (kn+k, vn+k)} được tính toán lại hoàn toàn ở mỗi bước. Ngược lại, KV Cache ban đầu tính toán trạng thái attention cho đầu vào, được biểu diễn bởi S0={(ki, vi)|i≤n}, và cache chúng trong bộ nhớ. Bước này thường được gọi là giai đoạn prefill. Đối với mỗi bước tiếp theo j≤k, mô hình tái sử dụng các giá trị đã cache Sj={(ki, vi)|i < n+j} để tính toán trạng thái attention (kn+j, vn+j) của token mới sn+j. Phương pháp này giảm đáng kể tính toán cần thiết cho self-attention. Cụ thể, tính toán trong mỗi bước, được đo bằng FLOPs cho các phép toán ma trận, được giảm bởi hệ số 1/n. Số lượng phép toán giảm từ khoảng 6nd² + 4n²d xuống 6d² + 4nd, trong đó d là kích thước chiều ẩn. Sau mỗi bước, trạng thái attention (kn+j, vn+j) mới được tính toán được thêm vào cache để sử dụng tiếp theo.

Trong các mô hình ngôn ngữ nhân quả, chiếm phần lớn LLM, việc sử dụng KV Cache không ảnh hưởng đến độ chính xác của mô hình, vì attention ở vị trí i được tính toán chỉ dựa trên các token ở các vị trí nằm trước token thứ i.

KV Cache đã thúc đẩy việc khám phá thêm về tăng tốc LLM. Các nghiên cứu tiếp theo đã tập trung hoặc vào việc tinh chỉnh quản lý bộ nhớ cho KV Cache, như được chứng minh trong paged attention (Kwon et al., 2023), hoặc pruning dữ liệu KV Cache thừa (Zhang et al., 2023), hoặc nén nó (Liu et al., 2023b). Có một số công việc sơ bộ khám phá việc tái sử dụng KV Cache qua các yêu cầu khác nhau. (Feng et al., 2023) tái sử dụng trạng thái attention được ghi nhớ dựa trên metric tương tự embedding. Paged attention cũng chứng minh việc chia sẻ prefix đơn giản, trong đó các prompt khác nhau với prefix giống hệt nhau chia sẻ KV Cache. Tuy nhiên, các phương pháp hiện có chỉ cụ thể cho một số kịch bản nhất định, trong khi chúng tôi nghiên cứu việc tái sử dụng attention cho các prompt LLM tổng quát.

2.3 Các Phương Pháp Khác cho Suy Luận LLM Độ Trễ Thấp

Prompt Cache giới thiệu một chiến lược tối ưu hóa trực giao bổ sung cho các hệ thống hiện có dành riêng cho suy luận LLM hiệu quả. Điều này bao gồm các hệ thống sử dụng nhiều GPU cho suy luận (Aminabadi et al., 2022) và những hệ thống với kernel GPU hiệu suất cao cho tính toán điểm attention softmax (Dao et al., 2022). Mặc dù mục tiêu hiện tại của chúng tôi là đạt được suy luận độ trễ thấp trong LLM, Prompt Cache cũng có thể mang lại lợi ích cho các hệ thống nhắm đến thông lượng cao (Sheng et al., 2023) thông qua việc giảm tính toán.

3 THIẾT KẾ PROMPT CACHE

Hiệu quả của KV Cache dẫn chúng ta đến câu hỏi tiếp theo: Liệu trạng thái attention có thể được tái sử dụng qua nhiều yêu cầu suy luận không? Chúng tôi quan sát thấy rằng các prompt khác nhau thường có các đoạn văn bản chồng chéo. Ví dụ, "thông điệp hệ thống" giống hệt nhau, hoặc metaprompt thường được chèn vào đầu prompt để thu hút phản hồi mong muốn từ LLM. Ví dụ khác, trong nhiều ứng dụng LLM pháp lý và y tế (Cui et al., 2023; Steinberg et al., 2021; Rasmy et al., 2021), cùng một bộ tài liệu thường được cung cấp làm ngữ cảnh cho các prompt khác nhau. Cuối cùng, các định dạng prompt có thể tái sử dụng, tức là mẫu prompt, thường được sử dụng bởi các ứng dụng LLM trong robotics và tool learning (Driess et al., 2023; Qin et al., 2023), vì hầu hết các tác vụ là biến thể của một vài tác vụ phổ biến. Trong phần này, chúng tôi mô tả phương pháp của chúng tôi được gọi là Prompt Cache, trả lời câu hỏi trên một cách khẳng định. Prompt Cache cải thiện hiệu quả tính toán thông qua việc tái sử dụng trạng thái attention giữa các yêu cầu bằng cách tận dụng các đoạn chia sẻ theo cách có cấu trúc.

3.1 Tổng Quan

Trạng thái attention của một đoạn văn bản chỉ có thể được tái sử dụng nếu đoạn đó xuất hiện ở cùng vị trí trong đầu vào LLM. Điều này là do các kiến trúc transformer tích hợp positional embeddings duy nhất vào trạng thái attention (k, v). Điều này không phải là vấn đề đối với việc phục vụ một prompt đơn sử dụng KV Cache, vì cùng một văn bản prompt được đặt ở cùng vị trí, tức là đầu đầu vào, trong tất cả các bước.

Mặt khác, các đoạn văn bản được chia sẻ có thể xuất hiện ở các vị trí khác nhau trong các prompt khác nhau. Để tái sử dụng trạng thái attention của chúng qua các prompt, một hệ thống cache phải giải quyết hai vấn đề. Đầu tiên, nó phải cho phép tái sử dụng mặc dù một đoạn văn bản xuất hiện ở các vị trí khác nhau trong các prompt khác nhau. Thứ hai, hệ thống phải có khả năng nhận ra hiệu quả một đoạn văn bản có trạng thái attention có thể đã được cache để tái sử dụng, khi hệ thống nhận được một prompt mới.

Để giải quyết hai vấn đề này, chúng tôi kết hợp hai ý tưởng. Đầu tiên là làm cho cấu trúc của prompt trở nên rõ ràng với Prompt Markup Language (PML). Như được minh họa bởi Hình 2, PML làm cho các đoạn văn bản có thể tái sử dụng trở nên rõ ràng như các module, tức là module prompt. Nó không chỉ giải quyết vấn đề thứ hai ở trên mà còn mở ra cánh cửa để giải quyết vấn đề đầu tiên, vì mỗi module prompt có thể được gán với các ID vị trí duy nhất.

Ý tưởng thứ hai của chúng tôi là phát hiện thực nghiệm rằng LLM có thể hoạt động trên trạng thái attention với các ID vị trí không liên tục. Miễn là vị trí tương đối của các token được bảo toàn, chất lượng đầu ra không bị ảnh hưởng. Điều này có nghĩa là chúng ta có thể trích xuất các đoạn khác nhau của trạng thái attention và nối chúng lại để tạo thành ý nghĩa mới. Chúng tôi tận dụng điều này để cho phép người dùng chọn các module prompt dựa trên nhu cầu của họ, hoặc thậm chí thay thế một số ý nghĩa trong thời gian chạy.

Prompt Cache kết hợp hai ý tưởng này như sau. Một người dùng LLM viết prompt của họ bằng PML, với ý định rằng họ có thể tái sử dụng trạng thái attention dựa trên các module prompt. Quan trọng là, họ phải tạo ra một prompt từ một schema, cũng được viết bằng PML. Hình 2 cho thấy một ví dụ prompt dựa trên một schema ví dụ. Khi Prompt Cache nhận được một prompt, trước tiên nó xử lý schema của nó và tính toán trạng thái attention cho các module prompt của nó. Nó tái sử dụng những trạng thái này cho các module prompt trong prompt và các prompt khác được tạo ra từ cùng một schema.

Chúng tôi trình bày chi tiết thiết kế của PML trong §3.2 với trọng tâm vào các kỹ thuật tối đa hóa cơ hội tái sử dụng. Chúng tôi giải thích cách Prompt Cache tính toán trạng thái attention của các module prompt trong một schema trong §3.3, và cách nó có thể ảnh hưởng đến chất lượng đầu ra. Chúng tôi giải thích cách Prompt Cache tái sử dụng trạng thái attention từ một schema cho việc phục vụ một prompt trong §3.4.

Việc xây dựng KV cache module hóa trong Prompt Cache có sự tương đồng với các xấp xỉ được quan sát trong locally masked attention (Beltagy et al., 2020; Tay et al., 2023), tối ưu hóa tính toán bằng cách đặt một cửa sổ hạn chế cho tính toán điểm attention thay vì mở rộng attention của nó qua mỗi token trong chuỗi đầu vào của nó. Xem xét một kịch bản trong Prompt Cache nơi mỗi module prompt được mã hóa độc lập. Cho rằng trạng thái attention được tính toán nghiêm ngặt trong phạm vi của module prompt, điều này gần giống với thiết lập của một attention mask lọc ra các chuỗi bên ngoài module prompt. Do đó, xấp xỉ được thực hiện bởi Prompt Cache là giới hạn cửa sổ attention trong mỗi module prompt. Chúng tôi lưu ý rằng việc sử dụng những attention mask như vậy không nhất thiết giảm chất lượng đầu ra, như chúng tôi sẽ thảo luận trong §5. Trong một số ngữ cảnh, những mask này thậm chí có thể giới thiệu inductive biases có lợi bằng cách lọc hiệu quả thông tin không liên quan.

3.2 Prompt Markup Language (PML)

Tiếp theo chúng tôi mô tả các tính năng chính của PML được sử dụng để định nghĩa cả schema và prompt được tạo ra từ schema.

3.2.1 Schema vs. Prompt

Một schema là một tài liệu định nghĩa các module prompt và vạch ra vị trí tương đối và thứ bậc của chúng. Mỗi schema có một định danh duy nhất (thông qua thuộc tính name) và chỉ định các module prompt với thẻ <module>. Các văn bản không được bao bọc bởi thẻ <module> hoặc không có định danh được xác định được coi là các module prompt ẩn danh và luôn được bao gồm trong các prompt được xây dựng từ schema.

Đối với người dùng LLM, schema phục vụ như một giao diện để tạo và tái sử dụng trạng thái attention cho các module prompt. Người dùng có thể xây dựng một prompt từ một schema, với thẻ <prompt>. Thẻ này xác định schema để sử dụng thông qua thuộc tính schema, liệt kê các module prompt để import và thêm bất kỳ hướng dẫn bổ sung (không được cache) nào. Ví dụ, để import module miami từ schema trong Hình 2, người ta sẽ biểu diễn nó là <miami/>. Prompt Cache sẽ chỉ tính toán trạng thái attention cho văn bản không được chỉ định trong schema, ví dụ Highlights the surf spots trong Hình 2, và tái sử dụng trạng thái attention cho các module được import, ví dụ trip-plan và miami, từ đó giảm độ trễ.

3.2.2 Tối Đa Hóa Tái Sử Dụng với Tham Số

PML cho phép một module prompt được tham số hóa để tối đa hóa cơ hội tái sử dụng. Một tham số là một placeholder được đặt tên với độ dài được chỉ định có thể xuất hiện ở bất kỳ đâu trong một module prompt trong schema. Nó được định nghĩa bằng thẻ <param>, với các thuộc tính name và len chỉ ra tên của nó và số lượng token tối đa cho đối số, tương ứng. Khi một prompt import module prompt, nó có thể cung cấp một giá trị cho tham số. Hình 2 cho thấy một ví dụ của một module prompt được tham số hóa (trip-plan) và cách một prompt sẽ bao gồm module prompt và cung cấp một giá trị (3 days) cho đối số của nó (duration). Giá trị tăng cường không được cache.

Có hai cách sử dụng quan trọng của các module prompt được tham số hóa. Đầu tiên, thường xảy ra rằng một module prompt khác với module khác chỉ ở một số nơi được định nghĩa rõ. Tham số cho phép người dùng cung cấp các đối số cụ thể để tùy chỉnh module tại runtime và vẫn hưởng lợi từ việc tái sử dụng. Hình 2 minh họa trường hợp sử dụng này với trip-plan. Điều này đặc biệt hữu ích cho các prompt theo mẫu. Thứ hai, một tham số có thể được sử dụng để tạo "buffer" ở đầu hoặc cuối của một module prompt trong schema. Buffer này cho phép người dùng thêm một đoạn văn bản tùy ý trong prompt miễn là đoạn đó không dài hơn độ dài token tham số mà nó thay thế.

3.2.3 Các Tính Năng Khác

Module union: Một số module prompt thể hiện mối quan hệ loại trừ lẫn nhau. Nghĩa là, trong một tập hợp các module, chỉ nên chọn một. Ví dụ, xem xét một prompt yêu cầu LLM gợi ý một cuốn sách để đọc dựa trên hồ sơ người đọc được mô tả bởi một module prompt. Có thể có nhiều module prompt mỗi cái mô tả một hồ sơ người đọc nhưng prompt chỉ có thể bao gồm một trong số chúng.

```
<union>
<module name="doc-en-US"> ... </module>
<module name="doc-zh-CN"> ... </module>
</union>
```

Để phù hợp với những mối quan hệ loại trừ này, chúng tôi giới thiệu khái niệm union cho các module prompt. Một union của các module được ký hiệu bằng thẻ <union>. Các module prompt được lồng trong cùng một union chia sẻ cùng một ID vị trí bắt đầu. Một union không chỉ hợp lý hóa việc tổ chức layout mà còn bảo tồn các ID vị trí được sử dụng để mã hóa các module prompt. Hơn nữa, hệ thống có thể sử dụng cấu trúc này cho các tối ưu hóa, chẳng hạn như prefetching.

Trong khi các module tham số hóa và union có vẻ tương tự, chúng khác nhau ở hai khía cạnh. Đầu tiên, như chúng tôi sẽ cho thấy trong §3.3, tham số và module union được mã hóa theo những cách khác nhau. Thứ hai, chúng phục vụ các mục đích khác nhau: tham số được sử dụng cho các sửa đổi inline để tối đa hóa việc tái sử dụng một module, trong khi module union được dành cho cấu trúc prompt tốt hơn và sử dụng hiệu quả hơn các ID vị trí.

Module lồng nhau: PML cũng hỗ trợ module lồng nhau để biểu đạt các module prompt phân cấp. Nghĩa là, một module prompt có thể bao gồm các module prompt hoặc union như các thành phần. Trong các prompt, các module lồng nhau được import như các module trong module như được hiển thị trong Hình 8.

Tương thích với mẫu LLM cụ thể: Các LLM được tinh chỉnh theo hướng dẫn thường tuân theo các mẫu cụ thể để định dạng cuộc trò chuyện. Ví dụ, trong Llama2, một tương tác đơn giữa người dùng và trợ lý tuân theo mẫu: `<s>[INST] user message [/INST] assistant message </s>`. Để giảm nỗ lực cần thiết để định dạng thủ công schema prompt để phù hợp với những mẫu như vậy cho các LLM khác nhau, chúng tôi giới thiệu ba thẻ chuyên dụng: <system> cho prompt cấp hệ thống, <user> cho prompt do người dùng tạo, và <assistant> cho các phản hồi mẫu được tạo bởi LLM. Prompt Cache tự động dịch và biên dịch những thẻ chuyên biệt này để phù hợp với mẫu prompt được chỉ định của LLM đang sử dụng.

3.2.4 Tạo PML từ Chương Trình Prompt

Để đơn giản hóa việc viết PML, Prompt Cache có thể tự động chuyển đổi các chương trình prompt (Beurer-Kellner et al., 2023; Guidence, 2023) từ các ngôn ngữ như Python thành PML, loại bỏ nhu cầu viết schema thủ công. Điều này chủ yếu được thực hiện bằng cách sử dụng Python API chuyển đổi các hàm Python thành các schema PML tương ứng. Quá trình chuyển đổi rất đơn giản: các câu lệnh if trở thành cấu trúc <module> trong PML, gói gọn các prompt có điều kiện trong đó. Khi một điều kiện đánh giá là true, module tương ứng được kích hoạt. Các câu lệnh choose-one, chẳng hạn như if-else hoặc switch, được ánh xạ đến thẻ <union>. Các lời gọi hàm được dịch thành các module prompt lồng nhau. Ngoài ra, chúng tôi đã triển khai một decorator để quản lý tham số, cụ thể để hạn chế độ dài đối số tối đa. Điều này tương ứng với thuộc tính len trong <param>. Việc biên dịch Python-to-PML này ẩn độ phức tạp của PML khỏi người dùng và cung cấp khả năng bảo trì tốt hơn cho prompt.

3.3 Mã Hóa Schema

Lần đầu tiên trạng thái attention của một module prompt được cần, chúng phải được tính toán và lưu trữ trong bộ nhớ thiết bị, mà chúng tôi gọi là mã hóa module prompt. Đầu tiên, Prompt Cache trích xuất chuỗi token của một module prompt từ schema. Sau đó nó gán ID vị trí cho mỗi token. ID vị trí bắt đầu được xác định bởi vị trí tuyệt đối của module prompt trong schema. Ví dụ, nếu hai module prompt trước đó có kích thước chuỗi token lần lượt là 50 và 60, module prompt được gán ID vị trí bắt đầu là 110. Một ngoại lệ tồn tại cho các module union. Vì các module prompt trong union bắt đầu từ cùng vị trí, kích thước chuỗi token của chúng được xem xét với kích thước của con lớn nhất.

Từ chuỗi token của module prompt và các ID vị trí tương ứng, chúng sau đó được truyền cho LLM để tính toán trạng thái attention (k, v). Chúng tôi lưu ý rằng các ID vị trí được gán không bắt đầu từ số không. Điều này có thể chấp nhận được về mặt ngữ nghĩa vì khoảng trắng không làm thay đổi ý nghĩa của văn bản được tính toán trước. Tuy nhiên, nhiều triển khai positional encoding transformer hiện có, chẳng hạn như RoPE, thường yêu cầu điều chỉnh để phù hợp với các ID vị trí không liên tục, mà chúng tôi sẽ thảo luận trong (§4.2).

Để mã hóa các module prompt được tham số hóa, chúng tôi sử dụng ý tưởng rằng việc có khoảng trắng trong prompt không ảnh hưởng đến ngữ nghĩa của nó. Tham số được thay thế bằng một số lượng token <unk> được xác định trước, tương đương với giá trị thuộc tính len của chúng. Các ID vị trí tương ứng với những token <unk> này được ghi lại để thay thế trong tương lai. Khi module này được tích hợp vào prompt của người dùng và được ghép nối với các đối số liên quan, chuỗi token của những đối số được cung cấp này áp dụng các ID vị trí trước đó được liên kết với các token <unk>. Trạng thái attention (k, v) kết quả sau đó thay thế trạng thái attention ban đầu được phân bổ cho các token <unk>. Chúng tôi lưu ý rằng độ dài của các token mới được cung cấp có thể nhỏ hơn độ dài tham số được chỉ định, vì khoảng trắng kéo dài không thay đổi ngữ nghĩa.

Hiệu ứng attention masking: Prompt Cache giới hạn tính toán điểm attention trong phạm vi của mỗi module prompt, che khuất trạng thái attention qua các module. Hiệu ứng masking này có thể nâng cao hoặc làm giảm chất lượng đầu ra tùy thuộc vào tính độc lập ngữ nghĩa của các module. Đối với các module độc lập về mặt ngữ nghĩa, masking giảm nhiễu và cải thiện chất lượng. Tuy nhiên, đối với các module phụ thuộc về mặt ngữ nghĩa, nó có thể có hiệu ứng ngược lại. Do đó, mỗi module prompt nên tự chứa và độc lập về mặt ngữ nghĩa với các module khác. Một cách để loại bỏ hiệu ứng masking là sử dụng phương pháp chúng tôi gọi là scaffolding. Với chi phí bộ nhớ bổ sung, chúng tôi cho phép người dùng chỉ định "scaffold", là các tập hợp module prompt được mã hóa cùng nhau để chia sẻ phạm vi attention, ngoài trạng thái attention riêng lẻ của chúng. Khi tất cả các module prompt trong một scaffold được import trong một prompt, trạng thái attention của scaffold sẽ ghi đè trạng thái attention riêng lẻ. Scaffolding đánh đổi bộ nhớ bổ sung cho tính nhất quán đầu ra, có thể hữu ích cho các ứng dụng cần kết quả xác định.

3.4 Suy Luận Được Cache

Khi một prompt được cung cấp cho Prompt Cache, Prompt Cache phân tích nó để đảm bảo phù hợp với schema được tuyên bố. Nó xác minh tính hợp lệ của các module được import. Sau đó, như được minh họa trong Hình 2, Prompt Cache lấy trạng thái attention (k, v) cho các module prompt được import từ cache (2), tính toán những trạng thái cho các đoạn văn bản mới (3 và 4), và nối chúng lại để tạo ra trạng thái attention cho toàn bộ prompt (5), thay thế phép toán prefill.

Để chi tiết quá trình, Prompt Cache bắt đầu bằng cách nối các tensor trạng thái KV tương ứng với mỗi module prompt được import trong prompt. Ví dụ, khi prompt người dùng sử dụng module A, B, tensor KV được nối được hình thành như: (kC, vC) = (concat(kA, kB), concat(vA, vB)). Đáng chú ý là thứ tự nối không quan trọng do tính bất biến hoán vị của transformer (Dufter et al., 2022). Bước này chỉ yêu cầu sao chép bộ nhớ. Sau đó, Prompt Cache tính toán trạng thái attention cho các đoạn của prompt không được cache, cụ thể, chuỗi token không được định nghĩa trong schema và đối số cho các module prompt được tham số hóa. Prompt Cache trước tiên xác định các ID vị trí của văn bản không được cache dựa trên vị trí tương đối của chúng với các module prompt được sử dụng khác. Ví dụ, nếu văn bản nằm giữa module A và B, nó được gán ID vị trí bắt đầu từ vị trí kết thúc của A, giả sử có khoảng cách giữa vị trí của A và B. Đối số cho các module prompt được tham số hóa được gán cho các ID vị trí của token <unk>. Sau đó, chuỗi token và ID vị trí được tổng hợp và truyền cho LLM sử dụng (kC, vC) như một KV Cache, để tính toán trạng thái attention cho toàn bộ prompt. Quan trọng là độ phức tạp tính toán để sinh các token tiếp theo vẫn nhất quán với KV Cache, vì các module prompt không được sử dụng ngoài token ban đầu. Về bản chất, Prompt Cache giảm độ trễ liên quan đến việc tạo ra token đầu tiên, hoặc time-to-first-token (TTFT).

Tối ưu hóa bộ nhớ trong batch inference: Các prompt thường được phục vụ trong một batch để sử dụng GPU tốt hơn. Các prompt khác nhau được tạo ra từ cùng một schema có thể bao gồm cùng các module prompt, chẳng hạn như prompt hệ thống. Điều này mở ra cơ hội tối ưu hóa bổ sung bằng cách giảm redundancy KV Cache trong một batch. Paged attention (Kwon et al., 2023) có thể giải quyết vấn đề này bằng cách chia sẻ con trỏ tới cùng một module prompt qua các prompt khác nhau, thay vì nhân đôi trạng thái attention. Ở đây, việc sử dụng Prompt Cache có thể cải thiện ngầm thông lượng hệ thống bằng cách cho phép nhiều prompt được xử lý song song hơn.

4 TRIỂN KHAI

Chúng tôi xây dựng nguyên mẫu Prompt Cache sử dụng thư viện HuggingFace transformers (Wolf et al., 2020) trong PyTorch và bao gồm 3K dòng mã Python. Chúng tôi nhắm đến việc tích hợp liền mạch với codebase LLM hiện có và tái sử dụng trọng số của nó. Chúng tôi triển khai Prompt Cache để sử dụng cả bộ nhớ CPU và GPU để chứa các module prompt và đánh giá nó trên cả hai nền tảng.

4.1 Lưu Trữ Module Prompt Trong Bộ Nhớ

Chúng tôi lưu trữ các module prompt được mã hóa trong hai loại bộ nhớ: bộ nhớ CPU (host DRAM) và bộ nhớ GPU (HBM). Để quản lý tensor qua cả hai loại bộ nhớ, chúng tôi sử dụng bộ phân bổ bộ nhớ PyTorch (Paszke et al., 2019). Ngoài việc đơn giản ghép nối CPU với các module prompt trong bộ nhớ CPU và GPU với bộ nhớ GPU, chúng tôi cũng cho phép GPU truy cập các module prompt được lưu trữ trong bộ nhớ CPU. Điều này được thực hiện bằng cách sao chép các module prompt từ host đến device khi cần. Quá trình này phát sinh overhead sao chép bộ nhớ host-to-device. Tuy nhiên, nó cho phép GPU tận dụng bộ nhớ CPU dồi dào, có thể mở rộng đến mức terabyte. Như chúng tôi sẽ cho thấy trong §5, việc tiết kiệm tính toán từ Prompt Cache bù đắp hơn cho độ trễ gây ra bởi các phép toán sao chép bộ nhớ. Sử dụng GPU phơi bày sự đánh đổi giữa dung lượng bộ nhớ và độ trễ: bộ nhớ GPU nhanh hơn nhưng hạn chế về dung lượng, trong khi bộ nhớ CPU có thể mở rộng dễ dàng nhưng phát sinh overhead sao chép bộ nhớ bổ sung. Có vẻ khả thi để xem xét một cơ chế cache tận dụng cả bộ nhớ CPU và GPU. Chúng tôi để lại việc phát triển một hệ thống kết hợp các chiến lược thay thế cache và prefetching cho nghiên cứu trong tương lai.

4.2 Điều Chỉnh Kiến Trúc Transformer

Triển khai Prompt Cache yêu cầu hỗ trợ cho các ID vị trí không liên tục (§3.2). Mặc dù thư viện Transformers hiện tại không cung cấp những tính năng này, chúng có thể được tích hợp với các sửa đổi nhỏ. Ví dụ, khoảng 20 dòng mã bổ sung được cần cho mỗi LLM. Chúng tôi phác thảo các điều chỉnh cần thiết:

Bảng embedding: Các mô hình đầu như BERT (Vaswani et al., 2023) và GPT-2 (Radford et al., 2018) sử dụng bảng tra cứu để ánh xạ ID vị trí tới embedding đã học hoặc bias cố định, không yêu cầu thay đổi.

RoPE: Các LLM như Llama2 (Touvron et al., 2023) và Falcon (Penedo et al., 2023) áp dụng RoPE (Su et al., 2021), sử dụng ma trận xoay cho positional encoding trong tính toán attention. Chúng tôi tạo một bảng tra cứu cho mỗi ma trận xoay, cho phép truy xuất dựa trên ID vị trí.

ALiBi: Được sử dụng trong các mô hình như MPT (MosaicML, 2023) và Bloom (Scao et al., 2022), ALiBi (Press et al., 2022) tích hợp một bias tĩnh trong tính toán điểm softmax. Tương tự như RoPE, chúng tôi thiết kế một bảng tra cứu để điều chỉnh ma trận bias theo các ID vị trí được cung cấp.

Chúng tôi cũng ghi đè toán tử nối của PyTorch để phân bổ bộ nhớ hiệu quả hơn. PyTorch chỉ hỗ trợ tensor liên tục, và do đó, việc nối hai tensor luôn dẫn đến phân bổ bộ nhớ mới. Prompt Cache cần nối trạng thái attention của các module prompt, và hành vi mặc định sẽ dẫn đến phân bổ bộ nhớ dư thừa. Chúng tôi triển khai một toán tử nối có buffer tái sử dụng bộ nhớ khi nối tensor. Tối ưu hóa này cải thiện dấu chân bộ nhớ của Prompt Cache và giảm overhead phân bổ bộ nhớ.

5 ĐÁNH GIÁ

Đánh giá của chúng tôi về Prompt Cache tập trung vào việc trả lời ba câu hỏi nghiên cứu sau: (i) Tác động của Prompt Cache đối với độ trễ time-to-first-token (TTFT) và chất lượng đầu ra là gì (§5.2 – §5.4), (ii) Overhead lưu trữ bộ nhớ là gì (§5.5), và (iii) Những ứng dụng nào phù hợp với Prompt Cache (§5.6). Chúng tôi sử dụng KV Cache thông thường (Pope et al., 2022) làm baseline. Prompt Cache và KV Cache chia sẻ chính xác cùng một pipeline suy luận trừ việc tính toán trạng thái attention. Chúng tôi sử dụng độ trễ TTFT để so sánh, đo thời gian để sinh token đầu tiên, vì Prompt Cache và KV Cache có cùng độ trễ giải mã sau token đầu tiên.

5.1 Môi Trường Đánh Giá

Chúng tôi đánh giá Prompt Cache trên hai cấu hình CPU: Intel i9-13900K đi kèm với 128 GB DDR5 RAM ở 5600 MT/s và AMD Ryzen 9 7950X được ghép nối với 128 GB DDR4 RAM ở 3600 MT/s. Đối với benchmark GPU, chúng tôi triển khai ba GPU NVIDIA: RTX 4090, được ghép nối với Intel i9-13900K, và A40 và A100, cả hai đều là các node ảo được lưu trữ trên NCSA Delta, mỗi node được cung cấp với 16-core AMD EPIC 7763 và 224 GB RAM. Chúng tôi sử dụng một số LLM mã nguồn mở, bao gồm Llama2, CodeLlama, MPT và Falcon. Chúng tôi sử dụng các LLM vừa với dung lượng bộ nhớ của một GPU đơn (40 GB). Chúng tôi sử dụng bộ LongBench (Bai et al., 2023) để đánh giá cải thiện TTFT và thay đổi chất lượng đầu ra. LongBench bao gồm một tập con được tuyển chọn của dữ liệu kéo dài, dao động từ 4K đến 10K độ dài ngữ cảnh, trích xuất từ 21 bộ dữ liệu qua 6 danh mục, bao gồm các tác vụ như trả lời câu hỏi đa tài liệu (Yang et al., 2018; Ho et al., 2020; Trivedi et al., 2022; Koˇcisk`y et al., 2018; Joshi et al., 2017), tóm tắt (Huang et al., 2021; Zhong et al., 2021; Fabbri et al., 2019), và hoàn thành mã (Guo et al., 2023; Liu et al., 2023a). Chúng tôi định nghĩa các tài liệu trong các bộ dữ liệu LongBench, chẳng hạn như trang wiki và bài báo tin tức, là các module prompt. Chúng tôi giữ các chỉ thị cụ thể cho tác vụ như văn bản người dùng không được cache.

5.2 Cải Thiện Độ Trễ Trên Các Bộ Dữ Liệu Benchmark

Chúng tôi đo độ trễ TTFT trên cả GPU và CPU sử dụng Llama 7B, như được hiển thị trong Hình 3 và Hình 4. Trong đánh giá GPU của chúng tôi, chúng tôi sử dụng hai thiết lập bộ nhớ: lưu trữ các module prompt trong bộ nhớ CPU hoặc GPU. Đối với thí nghiệm CPU, chúng tôi sử dụng bộ nhớ CPU. Do hạn chế về không gian, chúng tôi chỉ trình bày 8 benchmark. Benchmark hoàn chỉnh từ 21 bộ dữ liệu có thể tìm thấy trong Phụ lục.

5.2.1 Độ Trễ Suy Luận GPU

Chúng tôi tóm tắt các phát hiện của chúng tôi trong Hình 3, được đánh giá trên ba GPU NVIDIA: RTX 4090, A40 và A100. Thanh màu vàng đại diện cho việc tải các module prompt từ bộ nhớ CPU, trong khi thanh màu xanh đại diện cho trường hợp trong bộ nhớ GPU. Có một xu hướng độ trễ nhất quán qua các bộ dữ liệu vì các mẫu LongBench có độ dài tương đương, trung bình 5K token.

Chúng tôi quan sát được giảm độ trễ TTFT đáng kể qua tất cả các bộ dữ liệu và GPU, dao động từ 1.5× đến 3× khi sử dụng bộ nhớ CPU, và từ 5× đến 10× khi sử dụng bộ nhớ GPU. Những kết quả này phân định ranh giới trên và dưới của việc giảm độ trễ có thể với Prompt Cache. Việc giảm độ trễ thực tế trong thực tế sẽ nằm giữa những ranh giới này, dựa trên mức độ sử dụng mỗi loại bộ nhớ.

5.2.2 Độ Trễ Suy Luận CPU

Hình 4 cho thấy rằng Prompt Cache đạt được giảm độ trễ lên đến 70× và 20× trên CPU Intel và AMD, tương ứng. Chúng tôi suy đoán rằng sự khác biệt này bị ảnh hưởng bởi sự khác biệt về băng thông bộ nhớ trong thiết lập hệ thống (5600MT/s DDR5 RAM trên CPU Intel so với 3600MT/s DDR4 RAM trên CPU AMD). Như mong đợi, độ trễ cao hơn đối với các bộ dữ liệu có tỷ lệ prompt không được cache lớn hơn, chẳng hạn như TriviaQA. Thú vị là, suy luận CPU hưởng lợi đáng kể hơn từ Prompt Cache so với suy luận GPU. Điều này được quy cho độ trễ tính toán attention lớn hơn nhiều trong CPU, đặc biệt khi các chuỗi trở nên dài hơn (ví dụ, FP16/FP32 FLOPs thấp hơn so với GPU). Điều này chỉ ra rằng Prompt Cache đặc biệt có lợi cho việc tối ưu hóa suy luận trong môi trường hạn chế tài nguyên, chẳng hạn như thiết bị edge hoặc máy chủ cloud với tài nguyên GPU hạn chế.

5.3 Độ Chính Xác với Prompt Cache

Để xác minh tác động của Prompt Cache đối với chất lượng phản hồi LLM, không có scaffolding, chúng tôi đo điểm độ chính xác với bộ LongBench. Để chứng minh khả năng áp dụng tổng quát, chúng tôi áp dụng Prompt Cache cho ba LLM có kiến trúc transformer khác nhau (§4.2): Llama2, MPT và Falcon. Kết quả benchmark độ chính xác được hiển thị trong Bảng 1 chứng minh Prompt Cache bảo tồn độ chính xác của đầu ra. Chúng tôi sử dụng lấy mẫu xác định nơi token có xác suất cao nhất được chọn ở mỗi bước để kết quả với và không có Prompt Cache có thể so sánh được. Qua tất cả các bộ dữ liệu, độ chính xác của đầu ra với Prompt Cache có thể so sánh với baseline.

5.4 Hiểu Về Cải Thiện Độ Trễ

Về mặt lý thuyết, Prompt Cache nên cung cấp giảm độ trễ TTFT theo cấp số hai so với KV Cache thông thường. Điều này là do, trong khi overhead memcpy của Prompt Cache tăng tuyến tính với độ dài chuỗi, việc tính toán self-attention có độ phức tạp tính toán theo cấp số hai đối với độ dài chuỗi. Để xác nhận điều này, chúng tôi thử nghiệm Prompt Cache trên một bộ dữ liệu tổng hợp với độ dài chuỗi thay đổi, giả sử tất cả các prompt đều được cache. Chúng tôi so sánh độ trễ TTFT của Prompt Cache với KV Cache thông thường sử dụng CPU Intel i9-13900K và hai GPU (NVIDIA RTX 4090 và A40) với mô hình Llama2 7B. Đối với cả CPU và GPU, bộ nhớ CPU được sử dụng để lưu trữ module prompt.

Cải thiện theo cấp số hai: Các phát hiện của chúng tôi, được trình bày trong Hình 5, cho thấy rằng độ trễ KV Cache tăng theo cấp số hai với độ dài chuỗi, trong khi chi phí sao chép bộ nhớ của Prompt Cache tăng tuyến tính. Điều này có nghĩa là lợi thế độ trễ của Prompt Cache (khoảng cách giữa hai đường cong) mở rộng theo cấp số hai với độ dài chuỗi. Sự khác biệt này rõ rệt hơn trên CPU so với GPU vì CPU trải qua độ trễ tính toán attention cao hơn, trong khi sự khác biệt giữa overhead của Prompt Cache, tức là memcpy host-to-device trong GPU và memcpy host-to-host trong CPU không đáng kể. Với trạng thái attention có 5K token, độ trễ cho memcpy host-to-host, host-to-device và device-to-device lần lượt là 3.79 ms, 5.34 ms và 0.23 ms.

Hiệu ứng của kích thước mô hình: Hơn nữa, khi kích thước tham số của mô hình tăng lên, overhead tính toán cho KV Cache cũng tăng theo. Ví dụ, chuyển từ mô hình 7B sang 13B ở độ dài token 3K thêm 220 ms độ trễ, trong khi Prompt Cache chỉ thêm 30 ms. Sự khác biệt này xuất phát từ việc độ phức tạp LLM cũng tăng theo cấp số hai với kích thước chiều ẩn. Ví dụ, FLOPS của attention là 6nd² + 4n²d, cho phép toán prefill. Điều này gợi ý rằng lợi thế của Prompt Cache so với KV Cache cũng tăng theo cấp số hai với kích thước mô hình (tức là chiều ẩn).

Độ trễ end-to-end: Vì Prompt Cache chỉ giảm TTFT, tác động của nó đối với thời gian cần thiết để nhận phản hồi LLM hoàn chỉnh giảm khi số lượng token được sinh tăng lên. Ví dụ, trên RTX 4090 với Llama 7B cho ngữ cảnh 3K, Prompt Cache nâng cao TTFT từ 900 ms xuống 90 ms, trong khi thời gian sinh token hoặc time-to-subsequent-token (TTST) vẫn nhất quán giữa KV Cache và Prompt Cache ở trung bình 32 ms mỗi token, bất kể độ dài token. Tuy nhiên, thời gian phản hồi nhanh hơn đóng góp tích cực vào trải nghiệm người dùng và độ trễ end-to-end tổng thể (Lew et al., 2018; Liu et al., 2023b). Ví dụ, cho rằng Prompt Cache nâng cao TTFT từ 900 ms xuống 90 ms, điều này tương đương với việc sinh thêm 25 token trong cùng khung thời gian. Một yếu tố khác là Prompt Cache cho phép chia sẻ trạng thái attention trong cùng batch, như chúng tôi đã thảo luận trong §3.4. Tùy thuộc vào đặc điểm workload, Prompt Cache có thể cải thiện thông lượng tổng thể bằng cách sử dụng kích thước batch lớn hơn được cho phép bởi dấu chân bộ nhớ giảm. Ví dụ, giả sử có 100 yêu cầu, mỗi yêu cầu có prompt 2K token. Nếu tất cả các prompt chia sẻ cùng module 1K token, Prompt Cache có thể giảm dấu chân bộ nhớ 50% khi kết hợp với các phương pháp như paged attention, cho phép kích thước batch làm việc lớn hơn và do đó thông lượng cao hơn.

5.5 Overhead Bộ Nhớ

Overhead bộ nhớ liên quan đến Prompt Cache tỷ lệ thuận với số lượng token được cache tổng hợp. Overhead này có thể được xác định bằng cách tham chiếu cả schema prompt và LLM mục tiêu. Trong Bảng 2, chúng tôi làm sáng tỏ overhead bộ nhớ trên cơ sở mỗi token, dưới giả định sử dụng độ chính xác 16-bit cho các điểm floating point. Đối với các mô hình nhỏ gọn, chẳng hạn như Falcon 1B, việc cache một tài liệu chứa 1K token sẽ yêu cầu khoảng 180 MB bộ nhớ. Nếu có hàng trăm module prompt, mức tiêu thụ bộ nhớ kết hợp sẽ nằm trong hàng chục gigabyte—một lượng trong phạm vi bộ nhớ của GPU cấp máy chủ. Ngược lại, đối với các mô hình lớn hơn như Llama 70B, việc cache một module độ dài 1K sẽ đòi hỏi 2.5 GB bộ nhớ đáng kể cho mỗi tài liệu, khiến bộ nhớ CPU trở thành lựa chọn duy nhất để lưu trữ module prompt. Với những cân nhắc này, các phương pháp nén cho trạng thái attention (Zhang et al., 2023) vẫn là một hướng nghiên cứu trong tương lai cho các kỹ thuật prompt caching.

5.6 Ứng Dụng của Prompt Cache

Chúng tôi chứng minh tính biểu đạt của PML với các trường hợp sử dụng ví dụ yêu cầu cấu trúc prompt phức tạp hơn và các tính năng nâng cao (§3.2) so với các benchmark LongBench: (i) nhiều module trong một truy vấn, (ii) union, và (iii) tham số hóa. Hơn nữa, những tác vụ này nhấn mạnh việc giảm độ trễ đáng chú ý khi số lượng token được cache tăng lên trong những trường hợp sử dụng phức tạp như vậy. Qua các trường hợp sử dụng, chúng tôi cung cấp một đánh giá định tính về đầu ra bằng cách đối chiếu việc sinh cached và non-cached, cho thấy rằng Prompt Cache duy trì chất lượng đầu ra, cùng với việc giảm độ trễ được đạt bởi Prompt Cache. Chúng tôi sử dụng Llama2 7B và lưu trữ các module prompt trong bộ nhớ local (tức là bộ nhớ GPU cho suy luận GPU). Schema đầy đủ cho những tác vụ này có sẵn trong Phụ lục B.

5.6.1 Sinh Mã

LLM thường được sử dụng cho sinh mã (Guo et al., 2023; Liu et al., 2023a), hỗ trợ lập trình viên trong việc hỗ trợ hoặc trực tiếp sinh mã. Các phương pháp hiện có, chẳng hạn như Copilot (GitHub, 2023), thường tập trung vào các tệp nguồn riêng lẻ. Tuy nhiên, Prompt Cache có thể mở rộng điều này sang nhiều tệp tận dụng bản chất module hóa của mã nguồn. Ví dụ, mỗi class hoặc function có thể là một module prompt riêng biệt. Hình 6 minh họa sinh mã đa nguồn sử dụng CodeLlama 7B (Roziè̀re et al., 2023). Chúng tôi coi các class riêng lẻ như Unit, Map, Game và Player là các module prompt trong schema của chúng tôi cho lập trình game. Người dùng sau đó có thể bao gồm những module prompt này bất cứ khi nào họ cần chúng trong mã. Có cải thiện 4× trong độ trễ TTFT trên GPU trong khi đầu ra vẫn giống hệt nhau.

5.6.2 Cá Nhân Hóa

Hình 7 cho thấy lợi ích về độ trễ và chất lượng đầu ra của Prompt Cache trong trường hợp sử dụng cá nhân hóa. Cá nhân hóa là không thể thiếu đối với nhiều hệ thống khuyến nghị (Wu et al., 2023), tìm thấy các ứng dụng nổi bật trong các ngữ cảnh LLM như giáo dục, khuyến nghị nội dung và marketing có mục tiêu. Chúng tôi nêu bật hiệu quả của cá nhân hóa dựa trên tính năng thông qua Prompt Cache. Ở đây, cá nhân hóa phụ thuộc vào một tập hợp tính năng được định nghĩa. Mỗi tính năng được biểu diễn như một module prompt riêng biệt, với mối quan hệ giữa các tính năng được ký hiệu bằng thẻ union như cấp độ, trình độ thành thạo, lịch sử học tập, phong cách học tập và loại đánh giá.

5.6.3 Prompt Tham Số Hóa

Trong Hình 8, chúng tôi cho thấy trường hợp sử dụng lập kế hoạch du lịch tận dụng tham số hóa (§3.2). Schema được sử dụng trong trường hợp sử dụng này bao gồm một tham số có thể điều chỉnh để chỉ định thời gian du lịch cùng với hai module union để chọn điểm đến. Người dùng có thể tái sử dụng prompt theo mẫu với các tham số tùy chỉnh, tận hưởng độ trễ TTFT thấp hơn và chất lượng phản hồi LLM tương tự được hỗ trợ bởi Prompt Cache.

6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Chúng tôi giới thiệu Prompt Cache, một kỹ thuật tăng tốc dựa trên insight rằng trạng thái attention có thể được tái sử dụng qua các prompt LLM. Prompt Cache sử dụng một schema prompt để vạch ra những đoạn văn bản được tái sử dụng như vậy, hình thành chúng thành một cấu trúc module hóa và nhất quán về vị trí được gọi là "module prompt". Điều này cho phép người dùng LLM kết hợp những module này một cách liền mạch vào prompt của họ, từ đó tận dụng chúng cho ngữ cảnh với ý nghĩa độ trễ không đáng kể. Đánh giá của chúng tôi trên các bộ dữ liệu benchmark chỉ ra việc giảm độ trễ TTFT lên đến 8× trên GPU và 60× trên CPU.

Đối với công việc tương lai, chúng tôi dự định sử dụng Prompt Cache như một khối xây dựng cho các hệ thống phục vụ LLM trong tương lai. Một hệ thống như vậy có thể được trang bị các chiến lược thay thế cache GPU được tối ưu hóa để đạt được ranh giới độ trễ thấp nhất có thể với Prompt Cache. Các chiến lược khác nhau để giảm overhead bộ nhớ host-to-device cũng có thể có lợi, chẳng hạn như tích hợp các kỹ thuật nén trong KV cache, hoặc sử dụng grouped query attention. Một khám phá hứa hẹn khác là GPU primitives để chia sẻ trạng thái attention qua các yêu cầu đồng thời, như chúng tôi đã thảo luận ngắn gọn trong §3.4. Điều này không chỉ có thể giảm độ trễ TTFT mà còn độ trễ time-per-output-token (TPOT) bằng cách đóng gói nhiều yêu cầu hơn vào một batch duy nhất. Cuối cùng, Prompt Cache có thể trực tiếp tăng tốc các phương pháp in-context retrieval augmented generation (RAG), nơi hệ thống truy xuất thông tin về cơ bản phục vụ như một cơ sở dữ liệu của các module prompt. Prompt Cache có thể đặc biệt hữu ích cho các ứng dụng RAG nhạy cảm với độ trễ trong trả lời câu hỏi thời gian thực và hệ thống đối thoại.

LỜI CẢM ơN

Công việc này được hỗ trợ một phần bởi NSF Athena AI Institute (Giải thưởng #2112562), NSF Award #2047220, và Đại học Yale. Công việc này đã sử dụng hệ thống Delta tại Trung tâm Quốc gia về Ứng dụng Siêu máy tính (NCSA) thông qua phân bổ CIS230289 từ chương trình Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS), được hỗ trợ bởi các khoản tài trợ của Quỹ Khoa học Quốc gia (Giải thưởng #2138259, #2138286, #2138307, #2137603, và #2138296).

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo giữ nguyên định dạng gốc]
