# 2405.02842.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/inference/2405.02842.pdf
# Kích thước tệp: 2408448 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
ICEFORMER : TĂNG TỐC SUY LUẬN VỚI CÁC TRANSFORMER CHUỖI DÀI TRÊN CPU
Yuzhen Mao, Martin Ester, Ke Li
School of Computing Science, Simon Fraser University
Burnaby, BC V5A 1S6, Canada
{yuzhenm,ester,keli }@sfu.ca
TÓM TẮT
Một hạn chế của các mô hình dựa trên Transformer hiện tại là chúng không thể xử lý các chuỗi rất dài làm đầu vào vì các hoạt động self-attention của chúng thể hiện độ phức tạp thời gian và không gian bậc hai. Vấn đề này trở nên đặc biệt nghiêm trọng khi Transformer được triển khai trên các nền tảng phần cứng chỉ được trang bị CPU. Để giải quyết vấn đề này, chúng tôi đề xuất một phương pháp mới để tăng tốc self-attention tại thời điểm suy luận hoạt động với các mô hình Transformer đã được huấn luyện trước một cách ngay lập tức mà không cần huấn luyện lại. Chúng tôi thử nghiệm sử dụng phương pháp của mình để tăng tốc các Transformer chuỗi dài khác nhau, bao gồm một LLM hàng đầu dựa trên LLaMA 2, trên các benchmark khác nhau và chứng minh tốc độ tăng 2.73× −7.63× trong khi duy trì 98.6%−99.6% độ chính xác của các mô hình đã được huấn luyện trước ban đầu. Mã nguồn có sẵn trên trang web dự án của chúng tôi tại https://yuzhenmao.github.io/IceFormer/ .
1 GIỚI THIỆU
Transformer (Vaswani et al., 2017) đã thúc đẩy những tiến bộ đáng kinh ngạc trong NLP, được minh họa bởi các mô hình ngôn ngữ lớn (LLM) như GPT-4 và LLaMA 2. Ngày càng có nhiều LLM được áp dụng cho các chuỗi đầu vào cực kỳ dài, điều này cho phép nhiều ứng dụng thú vị như tạo nội dung dạng dài, các cuộc trò chuyện mở rộng, và tìm kiếm và phân tích tài liệu lớn (OpenAI, 2023; Anthropic, 2023). Trong khi LLM có thể được huấn luyện khả thi với các bộ tăng tốc phần cứng đắt tiền (ví dụ: GPU), chúng cần được triển khai trên các thiết bị thông thường, có thể chỉ được trang bị CPU.
Tuy nhiên, hiện tại việc triển khai LLM trên CPU là một thách thức do chi phí tính toán cao (Dice & Kogan, 2021). Một nút thắt tính toán đáng kể phát sinh từ cơ chế self-attention là một phần không thể thiếu của Transformer – cả độ phức tạp thời gian và không gian đều là bậc hai theo độ dài chuỗi.
Vấn đề này trở nên tồi tệ hơn trong bối cảnh của LLM, thường được sử dụng trên các chuỗi rất dài.
Để xử lý các chuỗi đầu vào dài, đã có nghiên cứu đáng kể về việc giảm độ phức tạp thời gian bậc hai của self-attention – các phương pháp này được gọi chung là efficient Transformer .
Tuy nhiên, nhiều phương pháp không đáp ứng nhu cầu của LLM và do đó khó áp dụng cho LLM.
Một phương pháp tăng tốc lý tưởng cho LLM nên thỏa mãn bốn tiêu chí: (1) Không cần huấn luyện lại – phương pháp không nên yêu cầu mô hình phải được huấn luyện lại, xét đến chi phí tính toán khổng lồ của việc huấn luyện LLM; (2) Tính tổng quát – phương pháp nên có thể áp dụng cho nhiều loại LLM, thay vì chỉ những LLM được huấn luyện với các ràng buộc cụ thể được tích hợp sẵn; (3) Độ chính xác cao – phương pháp không nên gây ra lỗi xấp xỉ lớn, vì LLM có nhiều lớp attention và do đó lỗi từ các lớp trước có thể tích lũy; (4) Suy luận nhanh – phương pháp nên đạt được hiệu suất nhanh tại thời điểm kiểm tra.
Việc thỏa mãn đồng thời tất cả các tiêu chí này là khó khăn, và theo hiểu biết của chúng tôi, không có phương pháp hiện tại nào có thể làm được như vậy. Ví dụ, Transformer với các mẫu attention cố định, ví dụ: Longformer (Beltagy et al., 2020), yêu cầu huấn luyện lại mô hình trước khi chúng có thể được sử dụng. Reformer (Nikita et al., 2020) yêu cầu các key phải được chuẩn hóa – yêu cầu này không được đáp ứng trong hầu hết các mô hình đã được huấn luyện trước. Nyströmformer (Xiong et al., 2021) và LARA (Zheng et al., 2022) không hỗ trợ causal mask, thường được tìm thấy trong LLM. Các phương pháp hạng thấp như Performer (Choromanski et al., 2020) gây ra lỗi xấp xỉ đáng kể, đặc biệt khi chúng không được huấn luyện lại/tinh chỉnh.
1arXiv:2405.02842v1  [cs.LG]  5 May 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hình 1: So sánh giữa Transformer (Vaswani et al., 2017) (hàng trên) và phương pháp được đề xuất, IceFormer (hàng dưới). Chúng tôi minh họa với một query và k= 2 trong k-NNS. Trong hai ma trận attention được trình bày, top-2 trọng số attention lớn nhất trong mỗi hàng được biểu diễn bằng màu tối. Các trọng số attention còn lại được hiển thị bằng màu nhạt trong ma trận attention vanilla, và được đặt bằng không (miêu tả bằng màu trắng) trong ma trận attention thưa.
Trong bài báo này, chúng tôi đề xuất một phương pháp tăng tốc, mà chúng tôi gọi là IceFormer do khả năng có thể được áp dụng trực tiếp trong các mô hình đóng băng mà không cần huấn luyện lại, đồng thời thỏa mãn bốn tiêu chí trên. Cụ thể, IceFormer (1) không yêu cầu huấn luyện lại, (2) có thể được áp dụng cho hầu hết LLM, (3) có thể xấp xỉ vanilla attention một cách chính xác, và (4) đạt được tốc độ suy luận nhanh hơn đáng kể so với các phương pháp hiện tại. Chúng tôi minh họa phương pháp của mình so với Transformer trong Hình 1.
Như được hiển thị, Transformer tính toán các trọng số attention aij cho mọi kết hợp có thể có của query qi và key kj (Giai đoạn 1) và liệt kê một cách đầy đủ tất cả các vector giá trị vj cho mỗi query (Giai đoạn 2). Ngược lại, phương pháp của chúng tôi tận dụng tính thưa của ma trận attention và chỉ tính toán các trọng số attention cao nhất và chỉ liệt kê các vector giá trị liên quan đến chúng.
Chúng tôi tiến hành thử nghiệm trên CPU trên các benchmark LRA (Tay et al., 2020), ZeroSCROLLS (Shaham et al., 2023), và LongEval (Li et al., 2023). Trên cả ba benchmark, IceFormer thể hiện tốc độ suy luận nhanh hơn đáng kể so với các phương pháp hiện tại trong khi đạt được gần như không có mất mát độ chính xác so với Transformer. Trên benchmark LRA, trung bình IceFormer đạt được tăng tốc 7.63× so với Transformer trong khi duy trì 98.6% độ chính xác của nó. So với efficient Transformer tốt nhất với độ chính xác tương đương cho mỗi tác vụ, IceFormer nhanh hơn trung bình 3.04×.
Trên benchmark ZeroSCROLLS, IceFormer đạt được tăng tốc 2.73× trung bình so với một LLM hàng đầu dựa trên LLaMA 2 trong khi duy trì 99.6% độ chính xác của nó.
2 CÔNG TRÌNH LIÊN QUAN
Efficient Transformer có thể được phân loại theo hai trục: loại phương pháp và yêu cầu huấn luyện lại.
Theo trục đầu tiên là các phương pháp dựa trên tính thưa và các phương pháp hạng thấp. Theo trục thứ hai là các phương pháp có thể và không thể được áp dụng cho các Transformer đã được huấn luyện trước phổ biến mà không cần huấn luyện lại.
Các phương pháp dựa trên tính thưa sử dụng cơ chế attention thưa để nắm bắt thông tin toàn cục và tích hợp nó với kết quả attention cục bộ. Một số phương pháp nhằm cải thiện độ phức tạp không gian so với cơ chế vanilla attention mà không cải thiện độ phức tạp thời gian, ví dụ: top-k Attention (Gupta et al., 2021). Các phương pháp khác nhằm cải thiện cả hai, ví dụ: Sparse Transformer (Child et al., 2019), Longformer (Beltagy et al., 2020), và ETC (Ainslie et al., 2020). Một hạn chế đáng kể của các mô hình này là các token được attention được xác định trước và vẫn tĩnh, không thích ứng với các chuỗi đầu vào khác nhau. Bởi vì hoạt động attention ban đầu được phép attention đến bất kỳ token nào, các mô hình này phải được huấn luyện với các ràng buộc đã được xác định trước tương ứng trên các token sẽ được attention. Reformer (Nikita et al., 2020) có thể attention đến các tập hợp token khác nhau cho các chuỗi đầu vào khác nhau bằng cách sử dụng Locality Sensitive Hashing (LSH) (Andoni et al., 2015) để nhóm các token thành các chunk và sau đó chỉ attention đến các token trong cùng chunk với mỗi query và các chunk liền kề. Tuy nhiên, Reformer áp đặt hai ràng buộc không có trong hoạt động attention ban đầu: key phải được chuẩn hóa và query và key phải giống nhau. Do đó, Reformer phải được huấn luyện với các ràng buộc này được tích hợp sẵn. Kết quả là, các phương pháp này không thể được áp dụng trực tiếp cho các mô hình đã được huấn luyện trước, không được chỉnh sửa; thay vào đó, các mô hình phải được huấn luyện lại với các ràng buộc cần thiết trước khi các phương pháp này có thể được sử dụng.
Các phương pháp hạng thấp xấp xỉ ma trận trọng số attention với một ma trận hạng thấp để giảm độ phức tạp thời gian và không gian bậc hai. Các ví dụ bao gồm Linformer (Wang et al., 2020) và Performer (Choromanski et al., 2020), phân rã ma trận trọng số attention thành một tích của các ma trận cao và rộng bao gồm các đặc trưng tuyến tính đã học hoặc các đặc trưng ngẫu nhiên của key và query, tương ứng. Tuy nhiên, các Transformer này thường gây ra lỗi xấp xỉ đáng kể vì các ma trận trọng số attention được tạo ra bởi hoạt động attention ban đầu, đặc biệt trong trường hợp các chuỗi đầu vào dài, thường có hạng cao. Do đó, các mô hình sử dụng các phương pháp này phải được huấn luyện với các xấp xỉ hạng thấp được tích hợp sẵn, để học cách mạnh mẽ trước các lỗi xấp xỉ liên quan. Kết quả là, các phương pháp này không thể được áp dụng trực tiếp cho các mô hình đã được huấn luyện trước, không được chỉnh sửa; thay vào đó, các mô hình phải được huấn luyện lại với các xấp xỉ cần thiết trước khi các phương pháp này có thể được sử dụng. Các phương pháp khác cung cấp các phương pháp luận tổng quát hơn có thể tận dụng các trọng số đã được huấn luyện trước với Transformer tiêu chuẩn mà không cần huấn luyện lại. Các Transformer này tăng tốc việc thực thi hoạt động attention tiêu chuẩn mà không thay đổi kiến trúc cơ bản. Hai ví dụ là Nyströmformer (Xiong et al., 2021) và LARA (Zheng et al., 2022), thay thế cấu trúc softmax trong cơ chế self-attention bằng tích của các ma trận query và key được kích hoạt riêng biệt. Nyströmformer sử dụng phương pháp Nyström, trong khi LARA kết hợp randomized attention (RA) và random feature attentions (RFA) (Peng et al., 2021) để tái tạo ma trận trọng số attention. Trong một ví dụ khác, H-Transformer-1D (Zhu & Soricut, 2021) chia đệ quy ma trận trọng số attention thành các khối và cắt bỏ các giá trị kỳ dị nhỏ của mỗi khối ngoài đường chéo. Tất cả các phương pháp này tận dụng các xấp xỉ hạng thấp, trái với tính thưa.
Các công trình khác đề xuất các tối ưu hóa cụ thể cho phần cứng mà không nhằm cải thiện độ phức tạp tính toán. Các ví dụ bao gồm FlashAttention (Dao et al., 2022), tối ưu hóa việc đọc và ghi giữa các cấp độ bộ nhớ GPU, và H2O (Zhang et al., 2023), động lực giữ lại một sự cân bằng giữa các token gần đây và quan trọng bằng chính sách loại bỏ KV cache. Các chiến lược này phụ thuộc vào việc triển khai và cụ thể cho các nền tảng phần cứng cụ thể (ví dụ: GPU).
3 KÝ HIỆU VÀ KIẾN THỨC CƠ BẢN
Về mặt toán học, hoạt động attention nhận ba ma trận làm đầu vào, K∈Rm×d,Q∈Rn×d,V∈Rm×d′, biểu thị key, query và value tương ứng, và xuất ra một ma trận O∈Rn×d′.
Tùy chọn, nó cũng có thể nhận một mask làm đầu vào, S∈Rn×m, có các mục là 0 hoặc 1. Hàng thứ i của K,Q,V và O, được ký hiệu là ki,qi,vi và oi, biểu thị key, query, value và output thứ i tương ứng. Mục của S ở hàng thứ i và cột thứ j, được ký hiệu là si,j, biểu thị liệu query thứ i có được phép attention đến key thứ j hay không — nếu là 1, nó sẽ được phép; nếu là 0, nó sẽ không được phép.
Một sơ đồ masking phổ biến là causal mask, trong đó si,j là 1 nếu i≥j và 0 ngược lại. Key và query có cùng chiều d, và mỗi key được liên kết với một value, và do đó số lượng key và value là như nhau và được ký hiệu là m.
Đầu tiên, hoạt động attention tính toán ma trận trọng số attention A∈Rn×m. Mục của nó ở hàng thứ i và cột thứ j, được ký hiệu là ai,j, được tính với công thức sau:
ai,j=si,jexp
q⊤
ikj√
d
Pm
j′=1si,j′expq⊤
ikj′√
d (1)
Sau đó hoạt động attention kết hợp các value với các trọng số attention theo cách sau:
oi=mX
j=1ai,jvj (2)
3

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Ma trận attention A thường thưa (Nikita et al., 2020; Gupta et al., 2021), tức là, trong mỗi hàng của A, chỉ một vài trọng số attention có giá trị đáng kể (lớn), trong khi phần lớn các giá trị còn lại gần bằng không. Giả sử chúng ta có thể bằng cách nào đó xác định k key không bị mask nhận được trọng số attention cao nhất cho mỗi query qi mà không tính toán trọng số attention cho tất cả key.
Khi đó, ma trận attention ban đầu A có thể được xấp xỉ bằng cách chỉ tính toán tích vô hướng cho các key đã được xác định, điều này có thể tiết kiệm đáng kể thời gian và tài nguyên tính toán.
4 ICEFORMER : SELF-ATTENTION TĂNG TỐC CHO KEY TỔNG QUÁT MÀ KHÔNG CẦN HUẤN LUYỆN LẠI
Để xây dựng một phương pháp tăng tốc tổng quát không cần huấn luyện lại, phương pháp của chúng tôi không được yêu cầu thay đổi cơ chế attention để thay đổi các mẫu attention hoặc đưa vào các tham số mô hình mới để nắm bắt các quy luật trong các mẫu attention. Điều này loại trừ các chiến lược phổ biến như cơ chế attention với các mẫu attention thưa đã được xác định trước, ví dụ: (Child et al., 2019; Beltagy et al., 2020; Ainslie et al., 2020), và giảm chiều đã học của key và query, ví dụ: (Wang et al., 2020; Choromanski et al., 2020).
Do đó, khó thiết kế một phương pháp tăng tốc khai thác các quy luật đã biết trong các mẫu attention mà không áp đặt yêu cầu huấn luyện lại. Do đó, chúng tôi nhằm thiết kế một phương pháp tăng tốc không đưa ra giả định về sự tồn tại của quy luật trong các mẫu attention. Để cải thiện độ phức tạp O(mn) của vanilla attention, chúng ta cần xác định một cách thích ứng các key quan trọng nhất (tức là những key nhận được trọng số attention cao nhất) mà không tính toán tất cả trọng số attention. Điều này có vẻ như một vấn đề con gà và quả trứng: làm sao chúng ta có thể biết trọng số attention nào cao nhất mà không so sánh chúng với tất cả các trọng số attention khác?
Đáng chú ý, trong trường hợp đặc biệt của key được chuẩn hóa, như được đề xuất trong Nikita et al. (2020), điều này có thể được thực hiện bằng cách tận dụng tìm kiếm k láng giềng gần nhất (k-NNS) để xác định k key quan trọng nhất cho mỗi query. Điều này dựa trên sự thật toán học sau, có phần chứng minh được bao gồm trong Phần B.1 của phụ lục: nếu ∥kj∥2= 1 cho tất cả j, arg max jai,j= arg min j∥qi−kj∥2
2.
Tuy nhiên, sự thật này chỉ đúng khi tất cả key có cùng norm – nó không đúng khi các key khác nhau có norm khác nhau. Theo trực giác, điều này là vì norm của key có thể điều chỉnh trọng số attention mà chúng nhận được, trong điều kiện khác bằng nhau. Vậy nếu key A có norm lớn hơn key B, key A có thể nhận được trọng số attention cao hơn key B ngay cả khi key A xa query hơn key B. Kết quả là, việc áp dụng k-NNS một cách ngây thơ trong trường hợp tổng quát sẽ thất bại trong việc xác định các key quan trọng nhất.
Trong bài báo này, chúng tôi phát triển một phương pháp tăng tốc không yêu cầu huấn luyện lại hoặc áp đặt bất kỳ ràng buộc nào lên key. Nó vừa chính xác vừa hiệu quả về mặt tính toán, và cũng có thể hoạt động với attention mask phổ biến trong Transformer, như causal mask. Dưới đây chúng tôi sẽ mô tả chi tiết.
4.1 ATTENTION TĂNG TỐC TỔNG QUÁT KHÔNG CẦN HUẤN LUYỆN LẠI
Thay vì áp dụng k-NNS trực tiếp lên các key ban đầu, trước tiên chúng ta sẽ nhúng key và query vào một không gian chiều cao hơn. Lấy cảm hứng từ Neyshabur & Srebro (2015), chúng tôi chọn các hàm nhúng key và query sau, mà chúng tôi ký hiệu là TK:Rd→Rd+1 và TQ:Rd→Rd+1:
TK(kj) =
kj/cp
1− ∥kj∥2
2/c2⊤(3)
TQ(qi) = [qi/∥qi∥20]⊤(4)
trong đó c≥max j′∥kj′∥2 ít nhất là norm tối đa trên tất cả key.
Hóa ra k key quan trọng nhất có thể được xác định bằng cách thực hiện k-NNS trên các key nhúng sử dụng query nhúng. Chúng tôi sẽ chỉ ra điều này dưới đây:
arg max
jai,j= arg max
jsoftmax j q⊤
ikj′√
dm
j′=1!
(5)
4

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
= arg max
jq⊤
ikj√
d(6)
= arg min
j1−2q⊤
ikj/c∥qi∥2+ 1 (7)
= arg min
jq⊤
iqi/∥qi∥2
2−2q⊤
ikj/c∥qi∥2+k⊤
jkj/c2+ 1− ∥kj∥2
2/c2(8)
= arg min
j∥qi/∥qi∥2−kj/c∥2
2+ 1− ∥kj∥2
2/c2(9)
= arg min
j∥TQ(qi)−TK(kj)∥2
2 (10)
4.2 k-NNS CHÍNH XÁC CHO ATTENTION TĂNG TỐC
Vấn đề k-NNS là một trong những vấn đề được nghiên cứu kỹ lưỡng nhất trong khoa học máy tính lý thuyết.
Nhiều thuật toán đã được phát triển, và thường có thể đạt được tăng tốc đáng kể bằng cách cho phép sai sót với một xác suất nào đó. Những thuật toán như vậy được gọi là thuật toán ngẫu nhiên.
Trong bối cảnh LLM, số lượng lớp attention thường cao và do đó lỗi từ các lớp trước có thể tích lũy. Do đó, việc thuật toán k-NNS đạt được độ chính xác cao là cần thiết.
Việc chọn một thuật toán k-NNS phù hợp do đó là rất quan trọng.
Hầu hết các thuật toán k-NNS đều dựa trên bucketing, đặt key vào các bucket rời rạc và tìm kiếm qua các bucket chứa query. Mặt khác, các thuật toán dựa trên ranking so sánh thứ hạng của các key khác nhau so với query và tìm kiếm qua các key có thứ hạng cao. Một thuật toán dựa trên bucketing sử dụng hiệu quả một ngưỡng cố định về độ tương tự, và do đó một số lượng biến đổi (bao gồm cả không) của key có thể đáp ứng ngưỡng; mặt khác, một thuật toán dựa trên ranking trả về một số lượng key cố định, điều này hiệu quả tương đương với việc chọn một ngưỡng biến đổi về độ tương tự dựa trên phân phối của key, như được hiển thị trong Hình 2. Một ví dụ về thuật toán dựa trên bucketing là locality-sensitive hashing (LSH) (Indyk & Motwani, 1998), và một ví dụ về thuật toán dựa trên ranking là Prioritized DCI (Li & Malik, 2017). Như được hiển thị trong Hình 2, LSH hash mỗi key vào một bucket liên kết với giá trị hash, trong khi Prioritized DCI xếp hạng key theo các hướng ngẫu nhiên.
Để tăng tốc attention, chúng tôi cho rằng các thuật toán dựa trên ranking phù hợp hơn các thuật toán dựa trên bucketing, vì trọng số attention phụ thuộc vào cách các key khác nhau so sánh với nhau, thay vì một đánh giá tuyệt đối của mỗi key so với một ngưỡng cố định. Do đó, các thuật toán dựa trên ranking có thể mang lại recall tốt hơn của các key thực sự quan trọng.
Hình 2: Sự khác biệt giữa k-NNS dựa trên ranking và dựa trên bucketing. Trái: minh họa hai phương pháp k-NNS, Prioritized DCI (dựa trên ranking) và LSH (dựa trên bucketing). Phải: số lượng key có phép chiếu nhỏ hơn một ngưỡng. Các thuật toán dựa trên ranking trả về một số lượng key cố định tương tự nhất với query dưới phép chiếu (hiển thị như một hàng có kích thước cố định), điều này hiệu quả lọc ra các điểm ngoài một cửa sổ có kích thước biến đổi trên các phép chiếu. Các thuật toán dựa trên bucketing sử dụng một cửa sổ có kích thước cố định (hiển thị như một cột có kích thước cố định) và trả về tất cả key có phép chiếu nằm trong đó.
4.3 k-NNS NHANH CHO ATTENTION TĂNG TỐC
Trong một Transformer, các key trong một lớp attention phụ thuộc vào đầu ra từ lớp attention trước đó. Do đó, một cơ sở dữ liệu cần được xây dựng cho mỗi lớp attention. Do đó, việc chọn một thuật toán k-NN đạt được cả việc xây dựng và truy vấn nhanh là quan trọng.
5

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hơn nữa, trong bối cảnh LLM, nhiều mô hình phổ biến sử dụng kiến trúc chỉ decoder. Các lớp attention trong những kiến trúc như vậy sử dụng causal mask để ngăn token hiện tại được tạo ra phụ thuộc vào các token tương lai chưa được tạo ra. Masked attention như vậy tương đương với việc loại trừ các key bị mask ra khỏi tập hợp key mà thuật toán k-NNS hoạt động trên. Vì vậy mỗi khi một token được tạo ra, một key trở nên không bị mask. Thay vì xây dựng một cơ sở dữ liệu mới mỗi khi một token được tạo ra, việc thêm key một cách tăng dần vào cơ sở dữ liệu cho k-NNS sẽ hiệu quả hơn.
May mắn thay, Prioritized DCI hiệu quả ở cả giai đoạn xây dựng và truy vấn. Nếu số hướng chiếu ngẫu nhiên p gần bằng chiều nội tại của dữ liệu d′≥1 và số láng giềng gần nhất k cần tìm là nhỏ, Prioritized DCI có thể trả về k láng giềng gần nhất chính xác cho một query với xác suất cao trong khoảng thời gian ˜O(dkp/˜dm1−p/˜d), trong đó ˜O(·) bỏ qua các yếu tố log. Tiền xử lý của nó nhẹ, và do đó chỉ cần O(dpm) thời gian.
Nếu chúng ta so sánh điều này với độ phức tạp tính toán của vanilla attention là O(dmn), quan sát rằng không còn một số hạng phụ thuộc vào mn, và do đó không còn sự phụ thuộc bậc hai vào độ dài chuỗi. Sau đó trong phần 5.1, chúng tôi cũng xác thực thực nghiệm hiệu quả của Prioritized DCI và thấy rằng nó nhanh hơn mười một thuật toán k-NNS hàng đầu khác.
Để hỗ trợ causal masking, chúng tôi đã mở rộng việc triển khai Prioritized DCI để hỗ trợ cập nhật cơ sở dữ liệu tăng dần. Điều này có thể được thực hiện một cách hiệu quả, vì cấu trúc dữ liệu bao gồm các danh sách đã được sắp xếp, do đó việc chèn và xóa có thể được thực hiện trong thời gian O(logm) nếu chúng được triển khai như cây tìm kiếm nhị phân.
5 THỬ NGHIỆM
Hình 3: So sánh giữa mười hai thuật toán k-NNS trên tập dữ liệu fashion-mnist-784. Có tổng cộng 60.000 key và 10.000 query với 784 chiều. Nhiệm vụ là tìm top-10 láng giềng gần nhất từ toàn bộ tập hợp key cho mỗi query. Trục X: Recall trung bình trên tất cả query; Trục Y: Độ trễ tổng (giây) bao gồm xây dựng cơ sở dữ liệu và truy vấn.
Trong phần này, chúng tôi sẽ so sánh đánh đổi recall-latency giữa các thuật toán k-NNS khác nhau và sau đó phân tích hiệu suất của IceFormer trên benchmark LRA (Tay et al., 2020), là một benchmark phổ biến cho Transformer ngữ cảnh dài (Zhu & Soricut, 2021; Xiong et al., 2021; Zheng et al., 2022). Tiếp theo chúng tôi sẽ chứng minh những ưu điểm của IceFormer được áp dụng cho LLM với prompt dài làm đầu vào trên benchmark ZeroSCROLLS (Shaham et al., 2023) và benchmark LongEval (Li et al., 2023). Để đảm bảo tính mạnh mẽ của kết quả, chúng tôi đã sử dụng nhiều loại CPU khác nhau cho các thử nghiệm của mình – chúng tôi đã sử dụng Intel(R) Core(TM) i7-6850K 6-Core cho các thử nghiệm LRA, AMD Ryzen 9 5950X 16-Core cho các thử nghiệm ZeroSCROLLS, và AMD Ryzen 9 5900X 12-Core cho các thử nghiệm LongEval.
5.1 SO SÁNH CÁC THUẬT TOÁN k-NNS KHÁC NHAU
Chúng tôi so sánh recall của láng giềng gần nhất thực và tổng thời gian xây dựng và truy vấn của 12 thuật toán k-NNS, bao gồm Prioritized DCI và các thuật toán hiệu suất tốt nhất từ benchmark ANN (Aumüller et al., 2017), trên tập dữ liệu Fashion MNIST trong Hình 3. Như được hiển thị, Prioritized DCI đạt được đánh đổi recall-latency tốt nhất so với các thuật toán khác, điều này chứng minh tính phù hợp của nó trong bối cảnh của chúng tôi, yêu cầu xây dựng và truy vấn nhanh.
5.2 ĐÁNH GIÁ TRÊN BENCHMARK LONG RANGE ARENA (LRA)
Tập dữ liệu và Metrics. LRA bao gồm năm nhiệm vụ khác nhau: ListOps (Nangia & Bowman, 2018), truy xuất tài liệu (Retrieval) (Radev et al., 2013), phân loại văn bản (Text) (Maas et al., 2011), phân loại hình ảnh CIFAR-10 (Image) (Krizhevsky et al., 2009) và Pathfinder (Linsley et al., 2018).
Cụ thể, tất cả năm nhiệm vụ đều bao gồm các chuỗi có tối đa 4k token. Chúng tôi tóm tắt thông tin tập dữ liệu trong phụ lục C.1 để biết thêm chi tiết. Trong thử nghiệm này, chúng tôi tuân theo phân chia train/test từ Tay et al. (2020) và báo cáo độ chính xác phân loại tập dữ liệu test, thời gian chạy trung bình của module attention, và việc sử dụng bộ nhớ CPU trong quá trình suy luận cho mỗi nhiệm vụ.
6

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Baseline. Ngoài vanilla Transformer, chúng tôi so sánh với Nyströmformer (Xiong et al., 2021), H-Transformer-1D (Zhu & Soricut, 2021), LARA (Zheng et al., 2022), Reformer (Nikita et al., 2020), Longformer (Beltagy et al., 2020), Performer (Choromanski et al., 2020), và Linformer (Wang et al., 2020). Để so sánh với Reformer, chúng tôi huấn luyện một mô hình Transformer với Q và K được chia sẻ theo Nikita et al. (2020). Đối với Longformer và Linformer, vì chúng đưa vào các tham số bổ sung, chúng tôi khởi tạo ngẫu nhiên các tham số này khi tải trọng số đã được huấn luyện trước từ vanilla Transformer. Để so sánh công bằng, chúng tôi sử dụng benchmark đánh giá LRA được triển khai trong PyTorch bởi (Xiong et al., 2021), và chỉ thay thế module self-attention trong khi làm cho các phần khác của mỗi mô hình giống hệt như vanilla Transformer.
Chi tiết Triển khai. Đối với mỗi nhiệm vụ, chúng tôi bắt đầu bằng việc huấn luyện một mô hình cơ sở sử dụng GPU với kiến trúc vanilla Transformer. Sau đó chúng tôi thay thế module vanilla attention bằng một trong tám module attention hiệu quả được đề cập trước đó và áp dụng trực tiếp các trọng số đã được huấn luyện trước cho suy luận.
Để đảm bảo so sánh công bằng, chúng tôi điều chỉnh kích thước batch thành 1, loại bỏ nhu cầu padding mask vì IceFormer được đề xuất của chúng tôi tự động bỏ qua padding mask trong quá trình suy luận. Lưu ý rằng do ràng buộc shared-KQ bổ sung, đối với nhiệm vụ Pathfinder, các nỗ lực của chúng tôi để huấn luyện một shared-KQ Transformer đã không thành công. Kết quả là, chúng tôi đã loại trừ các kết quả tương ứng khỏi phân tích tiếp theo. Ngoài ra, trong quá trình suy luận, chúng tôi sử dụng tổng cộng 4 luồng CPU. Để biết thêm chi tiết toàn diện, vui lòng tham khảo phụ lục C.2.
Kết quả Suy luận. Lý tưởng, độ chính xác của vanilla Transformer (non-shared-KQ) phục vụ như một giới hạn trên cho độ chính xác xấp xỉ của bảy mô hình khác (IceFormer (non-shared-KQ), Nyströmformer, H-Transformer-1D, LARA, Longformer, Performer, và Linformer). Tương tự cho shared-KQ Transformer. Ngoài ra, thời gian suy luận module attention của vanilla Transformer sẽ là dài nhất, với các Transformer hiệu quả khác đạt được thời gian suy luận ngắn hơn với chi phí hy sinh độ chính xác dự đoán. Bảng 1 trình bày độ chính xác dự đoán và thời gian suy luận của module attention cho mỗi phương pháp. Các cài đặt siêu tham số được liệt kê trong phụ lục C.3.
Nói chung, IceFormer được đề xuất của chúng tôi luôn vượt trội hơn tất cả efficient Transformer, cung cấp xấp xỉ độ chính xác tốt nhất trong khi yêu cầu thời gian suy luận ít nhất trên tất cả năm nhiệm vụ. Điều này chứng minh tính tổng quát và hiệu quả của mô hình của chúng tôi.
Bảng 1: Hiệu suất của vanilla Transformer, và tám phương pháp attention xấp xỉ trên benchmark LRA.
Phương pháp shared-KQListOps Text Retrieval Image Pathfinder
Acc Time (s) Acc Time (s) Acc Time (s) Acc Time (s) Acc Time (s)
Transformer (Vaswani et al., 2017)✗ 0.4255 2.9208 0.6019 0.6933 0.6586 8.3588 0.4132 4.9303 0.7514 0.9620
✓ 0.4145 2.9134 0.5986 0.6603 0.6681 6.7946 0.3844 5.9804 / /
Reformer (Nikita et al., 2020) ✓ 0.4121 1.4281 0.5941 0.2288 0.6467 1.4751 0.3726 3.6927 / /
LARA (Zheng et al., 2022) ✗ 0.4125 0.6146 0.5831 0.2348 0.6401 1.8605 0.3094 2.6720 0.7380 0.5961
Nyströmformer (Xiong et al., 2021) ✗ 0.4128 0.7994 0.5838 0.3542 0.6540 2.4179 0.3754 1.7644 0.7176 0.9927
H-Transformer-1D (Zhu & Soricut, 2021) ✗ 0.3265 1.9301 0.5944 0.4811 0.5808 3.5605 0.2286 1.2586 0.5286 0.5708
Longformer (Beltagy et al., 2020) ✗ 0.1975 0.7406 0.5236 0.9862 0.4918 1.0443 0.1488 0.5451 0.5009 0.5899
Performer (Choromanski et al., 2020) ✗ 0.1975 0.6571 0.5000 0.3327 0.4974 1.2058 0.1345 0.6404 0.5056 0.6395
Linformer (Wang et al., 2020) ✗ 0.1975 3.1532 0.5088 1.8912 0.4940 1.6878 0.1064 0.7387 0.5022 1.3141
IceFormer (ours)✗ 0.4153 0.3766 0.5978 0.0921 0.6541 0.8337 0.4046 0.5076 0.7442 0.3058
✓ 0.4124 0.4678 0.6001 0.0903 0.6602 0.8480 0.3752 0.9581 / /
Đánh đổi Tốc độ & Độ chính xác. Đối với IceFormer, việc tăng mức độ xấp xỉ thường cải thiện hiệu quả mô hình nhưng có thể dẫn đến giảm hiệu suất dự đoán. Ở đây, chúng tôi nghiên cứu cách mức độ xấp xỉ ảnh hưởng đến tốc độ suy luận và độ chính xác bằng cách thay đổi số lượng ứng viên được trả về của IceFormer, k, từ 3 đến 10 cho mỗi nhiệm vụ và trình bày kết quả trong Hình 4. Từ hình, chúng tôi quan sát rằng trên tất cả các nhiệm vụ, khi k trở nên lớn hơn, IceFormer đạt được độ chính xác dự đoán được cải thiện nhưng trở nên kém hiệu quả hơn.
Phân tích Độ phức tạp Bộ nhớ. Bảng 2 tóm tắt việc sử dụng bộ nhớ tối đa cho mỗi phương pháp trong quá trình suy luận. Chúng tôi sử dụng cùng siêu tham số như trong Bảng 1 và duy trì kích thước batch là 1 để loại bỏ nhu cầu padding mask. Bảng cho thấy IceFormer luôn thể hiện việc sử dụng bộ nhớ đỉnh thấp nhất trên tất cả các nhiệm vụ. So với vanilla Transformer, IceFormer đạt được tiết kiệm bộ nhớ lên đến 0.862 GB.
7

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hình 4: Đánh đổi giữa tốc độ và độ chính xác khi k thay đổi trên năm nhiệm vụ LRA. Trục ngang của mỗi biểu đồ là thời gian wall clock trung bình của module attention, và trục dọc là độ chính xác dự đoán mô hình. Mỗi điểm tương ứng với một giá trị k trong tập hợp sau: {3, 5, 8, 10}.
Bảng 2: Sử dụng bộ nhớ đỉnh (GB) trên benchmark LRA. Sử dụng bộ nhớ đỉnh là tổng việc sử dụng bộ nhớ của toàn bộ chương trình, bao gồm bộ nhớ cho cơ sở dữ liệu/index Prioritized DCI.
Phương pháp shared-KQ ListOps Text Retrieval Image Pathfinder
Transformer (Vaswani et al., 2017)✗ 3.729 4.327 5.031 3.778 3.926
✓ 3.631 4.265 4.877 3.740 /
Reformer (Nikita et al., 2020) ✓ 3.623 3.983 4.250 3.687 /
LARA (Zheng et al., 2022) ✗ 3.584 4.129 4.566 3.772 3.943
Nyströmformer (Xiong et al., 2021) ✗ 3.478 3.982 4.375 3.463 3.845
H-Transformer-1D (Zhu & Soricut, 2021) ✗ 3.883 4.328 4.543 3.553 3.603
IceFormer (ours)✗ 3.374 3.834 4.169 3.304 3.465
✓ 3.306 3.756 4.053 3.286 /
5.3 ĐÁNH GIÁ TRÊN MÔ HÌNH NGÔN NGỮ LỚN (LLM)
Chúng tôi cũng đánh giá IceFormer trong bối cảnh LLM. Cụ thể, chúng tôi sử dụng IceFormer để tăng tốc quá trình xử lý prompt trong LLM. Chúng tôi chọn Vicuna-7b-v1.5-16k (Zheng et al., 2023), được tinh chỉnh từ LLaMA 2 (Touvron et al., 2023) và là một trong những LLM mã nguồn mở hiệu suất hàng đầu với độ dài ngữ cảnh lên đến 16K token, cho thử nghiệm sau. Để biết thêm chi tiết toàn diện bao gồm việc lựa chọn k trong k-NNS của IceFormer, vui lòng tham khảo phụ lục E.1.
Đối với các thử nghiệm LLM sau, chúng tôi không so sánh IceFormer với Reformer, LARA và Nyströmformer vì các lý do sau: Reformer yêu cầu key và query phải được chia sẻ, điều này không đúng trong các LLM đã được huấn luyện trước; Longformer chỉ đề xuất một cách để tăng tốc phần encoder của Transformer, do đó không thể được áp dụng cho các LLM chỉ decoder; LARA và Nyströmformer nhóm các token khác nhau thành các cluster khác nhau và do đó không thể xử lý causal mask trong LLM, sử dụng kiến trúc chỉ decoder. Tất cả baseline yêu cầu huấn luyện lại (Longformer, Performer và Linformer) cũng được loại trừ khỏi so sánh. Thêm chi tiết có thể được tìm thấy trong phụ lục E.2.
Kết quả ZeroSCROLLS. Chúng tôi so sánh IceFormer với mô hình Vicuna-7b-v1.5-16k vanilla và H-Transformer-1D được áp dụng cho Vicuna-7b-v1.5-16k trên benchmark ZeroSCROLLS (Shaham et al., 2023) được thiết kế đặc biệt cho LLM và chứa mười nhiệm vụ ngôn ngữ tự nhiên đa dạng yêu cầu hiểu ngữ cảnh đầu vào dài, bao gồm tóm tắt, trả lời câu hỏi, phân loại cảm xúc tổng hợp và sắp xếp lại thông tin. Mỗi nhiệm vụ có độ dài chuỗi khác nhau thay đổi từ 3k đến 10k. Chúng tôi đo điểm ZeroSCROLLS và độ trễ của module attention.
Bảng 3 cho thấy IceFormer đạt được tăng tốc lên đến 3.0 × so với self-attention tiêu chuẩn trong khi đạt được ít nhất 99.0% hiệu suất mô hình không được tăng tốc vanilla cùng lúc.
Kết quả LongEval & Phân tích Khả năng mở rộng. Để cung cấp phân tích toàn diện hơn về khả năng mở rộng của IceFormer trong bối cảnh LLM, chúng tôi đã tiến hành các thử nghiệm bổ sung trên benchmark LongEval (Li et al., 2023), được thiết kế để đo hiệu suất ngữ cảnh dài và bao gồm hai nhiệm vụ: nhiệm vụ truy xuất chủ đề với độ dài prompt thay đổi từ 3k đến 16k, và nhiệm vụ truy xuất dòng với độ dài prompt thay đổi từ 5k đến 16k. Trong Hình 5, chúng tôi trình bày độ trễ trung bình của module attention tương ứng với độ dài prompt đầu vào khác nhau cũng như độ chính xác suy luận sử dụng mô hình Vicuna-7b-v1.5-16k vanilla và IceFormer. Từ hình, IceFormer có thể đạt được gần
8

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 3: Hiệu suất của mô hình Vicuna-7b-v1.5-16k vanilla, H-Transformer-1D và IceFormer trên benchmark ZeroSCROLLS. Các số trong ngoặc đơn chỉ ra so sánh tương đối với mô hình không được tăng tốc vanilla, được ký hiệu là Vicuna-7b-v1.5-16k. Chúng tôi sử dụng cùng viết tắt cho tên metric và nhiệm vụ như được chỉ định trong bài báo gốc (Shaham et al., 2023). Chúng tôi giới thiệu độc giả quan tâm đến bài báo gốc để biết chi tiết.
Nhiệm vụ (#token) Metric Vicuna-7b-v1.5-16k H-Transformer-1D IceFormer
GvRp (8k)Rgeo↑ 11.0 (100%) 6.8 (61.8%) 11.0 (100%)
Time (s) 5.07 (1.0 ×) 4.22 (1.2 ×) 1.89 (2.7 ×)
SSFD (8k)Rgeo↑ 13.5 (100%) 6.3 (46.7%) 13.5 (100%)
Time (s) 5.02 (1.0 ×) 4.18 (1.2 ×) 1.81 (2.8 ×)
QMsm (9k)Rgeo↑ 16.9 (100%) 10.7 (63.3%) 16.8 (99.4%)
Time (s) 6.47 (1.0 ×) 4.62 (1.4 ×) 2.51 (2.6 ×)
SQAL (8k)Rgeo↑ 18.9 (100%) 7.3 (38.6%) 18.9 (100%)
Time (s) 5.01 (1.0 ×) 2.27 (2.2 ×) 1.92 (2.6 ×)
Qspr (5k)F1↑ 34.2 (100%) 6.2 (18.1%) 34.0 (99.4%)
Time (s) 2.03 (1.0 ×) 1.70 (1.2 ×) 0.89 (2.3 ×)
Nrtv (10k)F1↑ 14.7 (100%) 2.0 (13.6%) 14.7 (100%)
Time (s) 6.82 (1.0 ×) 4.55 (1.5 ×) 2.85 (2.4 ×)
QALT (7k)AC↑ 48.8 (100%) 6.8 (13.9%) 48.6 (99.6%)
Time (s) 3.76 (1.0 ×) 2.09 (1.8 ×) 1.26 (3.0 ×)
MuSQ (3k)F1↑ 18.6 (100%) 16.9 (90.9%) 18.5 (99.5%)
Time (s) 0.70 (1.0 ×) 0.63 (1.1 ×) 0.37 (1.9 ×)
SpDg (7.5k)ES↑ 42.5 (100%) 2.9 (6.8%) 42.3 (99.5%)
Time (s) 4.43 (1.0 ×) 2.22 (2.0 ×) 1.47 (3.0 ×)
BkSS (7.5k)Cidx↑ 19.5 (100%) 11.7 (60.0%) 19.3 (99.0%)
Time (s) 4.52 (1.0 ×) 2.26 (2.0 ×) 1.55 (2.9 ×)
Avg. (7.5k)/↑ 23.9 (100%) 7.8 (32.5%) 23.8 (99.6%)
Time (s) 4.38 (1.0 ×) 2.92 (1.5 ×) 1.60 (2.7 ×)
Hình 5: Phân tích khả năng mở rộng cho IceFormer trên benchmark LongEval. Hình bên trái hiển thị kết quả của nhiệm vụ truy xuất chủ đề; hình bên phải hiển thị kết quả của nhiệm vụ truy xuất dòng. Trục X: độ dài của prompt đầu vào; Trục Y (Trái): độ chính xác truy xuất; Trục Y (Phải): thời gian wall clock xử lý trung bình (giây) của module attention.
độ chính xác suy luận giống hệt như Vicuna-7b-v1.5-16k vanilla. Đáng chú ý, khi độ dài prompt tăng, có sự tăng tương ứng trong độ trễ suy luận cho cả hai phương pháp và cả hai nhiệm vụ. Tuy nhiên, ngay cả với độ dài prompt rất dài, IceFormer vẫn duy trì khả năng mở rộng của nó và luôn vượt trội hơn vanilla Transformer. Hơn nữa, khi độ dài prompt tăng, sự khác biệt về độ trễ giữa IceFormer và vanilla Transformer trở nên lớn hơn, chứng minh khả năng mở rộng và hiệu quả vượt trội của IceFormer trong bối cảnh LLM.
6 KẾT LUẬN
Trong bài báo này, chúng tôi trình bày IceFormer, một phương pháp mới để cải thiện hiệu quả thời gian suy luận của Transformer đã được huấn luyện trước trên CPU. Đáng chú ý, trái ngược với các phương pháp khác, IceFormer không yêu cầu huấn luyện lại, không yêu cầu các ràng buộc đặc biệt được áp đặt lên cơ chế attention và đồng thời đạt được độ chính xác cao và suy luận nhanh. Những ưu điểm này làm cho IceFormer rất phù hợp cho việc triển khai LLM trên CPU, đặc biệt khi LLM cần xử lý các chuỗi rất dài làm đầu vào. Các phát hiện thử nghiệm trên ba benchmark minh họa một cách thuyết phục hiệu quả của phương pháp của chúng tôi trong việc giảm độ phức tạp thời gian và không gian bậc hai của Transformer cả trong trường hợp với cơ chế attention hai chiều và causal.
9

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
TÀI LIỆU THAM KHẢO
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham,
Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured
inputs in transformers. arXiv preprint arXiv:2004.08483 , 2020.
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical
and optimal lsh for angular distance. Advances in neural information processing systems , 28, 2015.
Anthropic. 100k context windows, 2023. URL https://www.anthropic.com/index/
100k-context-windows .
Martin Aumüller, Erik Bernhardsson, and Alexander Faithfull. ann-benchmarks: A benchmarking
tool for approximate nearest neighbor algorithms. In International conference on similarity search
and applications , pp. 34–49. Springer, 2017.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150 , 2020.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509 , 2019.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794 , 2020.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. Advances in Neural Information Processing Systems ,
35:16344–16359, 2022.
Dave Dice and Alex Kogan. Optimizing inference performance of transformers on cpus. arXiv
preprint arXiv:2102.06621 , 2021.
Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient
transformers via top- kattention. arXiv preprint arXiv:2106.06899 , 2021.
Piotr Indyk and Rajeev Motwani. Approximate nearest neighbors: towards removing the curse of
dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing ,
pp. 604–613, 1998.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica,
Xuezhe Ma, and Hao Zhang. How long can context length of open-source LLMs truly promise?
InNeurIPS 2023 Workshop on Instruction Tuning and Instruction Following , 2023. URL https:
//openreview.net/forum?id=LywifFNXV5 .
Ke Li and Jitendra Malik. Fast k-nearest neighbour search via prioritized dci. In International
conference on machine learning , pp. 2081–2090. PMLR, 2017.
Drew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning long-
range spatial dependencies with horizontal gated recurrent units. Advances in neural information
processing systems , 31, 2018.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies , pp. 142–150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015 .
Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv
preprint arXiv:1804.06028 , 2018.
Behnam Neyshabur and Nathan Srebro. On symmetric and asymmetric lshs for inner product search.
InInternational Conference on Machine Learning , pp. 1926–1934. PMLR, 2015.
10

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Kitaev Nikita, Kaiser Lukasz, Levskaya Anselm, et al. Reformer: The efficient transformer. In
Proceedings of International Conference on Learning Representations (ICLR) , 2020.
OpenAI. Openai gpt-4, 2023. URL https://openai.com/gpt-4 .
Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong.
Random feature attention. arXiv preprint arXiv:2103.02143 , 2021.
Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl
anthology network corpus. Language Resources and Evaluation , 47:919–944, 2013.
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot
benchmark for long text understanding, 2023.
Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient
transformers. arXiv preprint arXiv:2011.04006 , 2020.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems , pp. 5998–6008, 2017.
Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768 , 2020.
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and
Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 35, pp. 14138–14148, 2021.
Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song,
Yuandong Tian, Christopher Ré, Clark Barrett, et al. H _2o: Heavy-hitter oracle for efficient
generative inference of large language models. arXiv preprint arXiv:2306.14048 , 2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.
Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention mecha-
nism. In International Conference on Machine Learning , pp. 27011–27041. PMLR, 2022.
Zhenhai Zhu and Radu Soricut. H-transformer-1D: Fast one-dimensional hierarchical attention for
sequences. In Proceedings of the 59th Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume
1: Long Papers) , pp. 3801–3815, Online, August 2021. Association for Computational Lin-
guistics. doi: 10.18653/v1/2021.acl-long.294. URL https://aclanthology.org/2021.
acl-long.294 .
11

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
A MÃ GIẢ CHO ICEFORMER
Chúng tôi cung cấp mã giả dưới đây cho IceFormer. Để hoàn thiện, chúng tôi cũng bao gồm mã giả của Prioritized DCI, mà chúng tôi đã điều chỉnh từ (Li & Malik, 2017) và thêm các chỉnh sửa của chúng tôi. Trong mã giả sau đây, TK và TQ là các hàm nhúng key và query, tương ứng.
Trong việc triển khai của chúng tôi, chúng tôi triển khai một phiên bản đệ quy của thuật toán, cung cấp hiệu suất nhanh hơn trong thực tế.
Algorithm 1 IceFormer attention
Require: Một tập Sq của n điểm q1, . . . , qn∈Rd, một tập Sk của n key k1, . . . , kn∈Rd, một tập Sv của n value
v1, . . . , vn∈Rd′, số lượng simple index m tạo thành một composite index, số lượng composite index L, số lượng điểm để truy xuất k0 và số lượng điểm để ghé thăm k1 trong mỗi composite index
function ICEFORMER _ATTENTION (Sq, Sk, Sv, m, L, k 0, k1)
O←zero matrix ∈Rn×d′ với các hàng oi∈Rd′
Sp←CONSTRUCT (Sk, m, L )
for i= 1to n do
Sl←QUERY (qi, Sp, k0, k1)
for j= 1to n do
if j∈Sl then
˜sij←q⊤
ikj√
d
else
˜sij←0
end if
end for
for j= 1to n do
if j∈Sl then
˜ai,j←softmax j(˜sij) =exp(˜si,j)
P
k∈Sl exp(˜si,k)
else
˜aij←0
end if
end for
oi←P
j∈Sl˜ai,jvj
end for
return O
end function
Algorithm 2 Quy trình xây dựng cấu trúc dữ liệu
Require: Một tập dữ liệu D của n điểm k1, . . . , kn, số lượng simple index m tạo thành một composite index
và số lượng composite index L
function CONSTRUCT (D, m, L )
{ujl}j∈[m],l∈[L]←mL vector đơn vị ngẫu nhiên trong Rd
{Tjl}j∈[m],l∈[L]←mL cây tìm kiếm nhị phân hoặc skip list rỗng
for j= 1to m do
for l= 1to L do
for i= 1to n do
ki
jl← ⟨TK(ki), ujl⟩
Insert (ki
jl, i) vào Tjl với ki
jl là key và i là value
end for
end for
end for
return {(Tjl, ujl)}j∈[m],l∈[L]
end function
12

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Algorithm 3 Quy trình truy vấn k láng giềng gần nhất
Require: Điểm truy vấn q trong Rd, cây tìm kiếm nhị phân/skip list và vector chiếu liên quan của chúng
{(Tjl, ujl)}j∈[m],l∈[L], số lượng điểm để truy xuất k0 và số lượng điểm để ghé thăm k1 trong mỗi
composite index
function QUERY (q,{(Tjl, ujl)}j,l, k0, k1)
Cl←mảng có kích thước n với các mục được khởi tạo thành 0 ∀l∈[L]
qjl← ⟨TQ(q), ujl⟩ ∀j∈[m], l∈[L]
Sl← ∅ ∀ l∈[L]
Pl←hàng đợi ưu tiên rỗng ∀l∈[L]
for l= 1to L do
for j= 1to m do
(p(1)
jl, h(1)
jl)←node trong Tjl có key gần nhất với qjl
Insert (p(1)
jl, h(1)
jl) với độ ưu tiên −|p(1)
jl−qjl| vào Pl
end for
end for
for i′= 1to k1−1 do
for l= 1to L do
if |Sl|< k0 then
(p(i)
jl, h(i)
jl)←node có độ ưu tiên cao nhất trong Pl
Remove (p(i)
jl, h(i)
jl) từ Pl và chèn node trong Tjl có key gần nhất tiếp theo với qjl,
được ký hiệu là (p(i+1)
jl, h(i+1)
jl), với độ ưu tiên −|p(i+1)
jl−qjl| vào Pl
Cl[h(i)
jl]←Cl[h(i)
jl] + 1
if Cl[h(i)
jl] =m then
Sl←Sl∪ {h(i)
jl}
end if
end if
end for
end for
return k điểm trong S
l∈[L]Sl có giá trị tích vô hướng tối đa với q
end function
B CHỨNG MINH
B.1 CHỨNG MINH 1
Ở đây, chúng tôi cung cấp sự suy luận từng bước đầy đủ của sự tương đương toán học giữa việc tiến hành tìm kiếm k láng giềng gần nhất trên key được chuẩn hóa và xác định các key có được trọng số attention cao nhất.
arg max
jai,j= arg max
jsoftmax j q⊤
ikj′√
dm
j′=1!
(11)
= arg max
jq⊤
ikj√
d(12)
= arg min
j∥qi∥2
2−2q⊤
ikj+ 1 (13)
Vì ∥kj′∥2= 1 cho tất cả j′,∥qi∥2
2−2q⊤
ikj+ 1 = ∥qi∥2
2−2q⊤
ikj+∥kj∥2
2=∥qi−kj∥2
2,
arg max
jai,j= arg min
j∥qi∥2
2−2q⊤
ikj+ 1 (14)
= arg min
j∥qi−kj∥2
2 (15)
B.2 CHỨNG MINH 2
Ở đây, chúng tôi cung cấp sự suy luận từng bước đầy đủ của kết quả trong 4.1 thiết lập sự tương đương toán học giữa việc tiến hành tìm kiếm k láng giềng gần nhất trên key được biến đổi và xác định
13

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
các key có được trọng số attention cao nhất.
arg max
jai,j= arg max
jsoftmax j q⊤
ikj′√
dm
j′=1!
(16)
= arg max
jq⊤
ikj√
d(17)
= arg max
jq⊤
ikj (18)
= arg min
j−2q⊤
ikj (19)
= arg min
j2−2q⊤
ikj/c∥qi∥2 (20)
= arg min
j1−2q⊤
ikj/c∥qi∥2+ 1 (21)
= arg min
j∥qi∥2
2/∥qi∥2
2−2q⊤
ikj/c∥qi∥2+∥kj∥2
2/c2+ 1− ∥kj∥2
2/c2(22)
= arg min
jq⊤
iqi/∥qi∥2
2−2q⊤
ikj/c∥qi∥2+k⊤
jkj/c2+ 1− ∥kj∥2
2/c2(23)
= arg min
j(qi/∥qi∥2−kj/c)⊤(qi/∥qi∥2−kj/c) + 1− ∥kj∥2
2/c2(24)
= arg min
j∥qi/∥qi∥2−kj/c∥2
2+ 1− ∥kj∥2
2/c2(25)
= arg min
j∥qi/∥qi∥2−kj/c∥2
2+
0−q
1− ∥kj∥2
2/c22
(26)
= arg min
j∥TQ(qi)−TK(kj)∥2
2 (27)
= arg min
j∥TQ(qi)−TK(kj)∥2 (28)
C THÊM CHI TIẾT VỀ THIẾT LẬP THỬ NGHIỆM LRA
C.1 CHI TIẾT TẬP DỮ LIỆU
Trong các thử nghiệm LRA của chúng tôi, đối với Retrieval, Text và Pathfinder (phiên bản 64×64), chúng tôi sử dụng trực tiếp tập dữ liệu từ codebase LRA1. Bởi vì các tập dữ liệu ban đầu cho ListOps và Image chỉ chứa các chuỗi ngắn, chúng tôi tạo ra các mẫu dài hơn cho ListOps bằng cùng mã từ codebase LRA với 4000 là độ dài tối đa; đối với nhiệm vụ Image, chúng tôi sử dụng một phiên bản của tập dữ liệu CIFAR-10 được tăng độ phân giải lên 64×642 thay vì tập dữ liệu CIFAR-10 độ phân giải thấp 32×32 ban đầu. Chúng tôi tuân theo chính xác cùng phân chia train/test như bài báo LRA ban đầu (Tay et al., 2020). Chi tiết của tập dữ liệu LRA được liệt kê trong Bảng 4.
Bảng 4: Chi tiết Tập dữ liệu LRA.
Nhiệm vụ ListOps Text Retrieval Image Pathfinder
Độ dài tối đa 3,991 4,000 4,000 4,096 4,096
Độ dài trung bình 2,232 1,267 3,917 4,096 4,096
số lượng lớp 10 2 2 10 2
Độ chính xác theo cơ hội 0.100 0.500 0.500 0.100 0.500
C.2 CẤU HÌNH MÔ HÌNH CƠ SỞ
Chúng tôi tuân theo thiết lập thử nghiệm của công trình trước đó (Zhu & Soricut, 2021) để huấn luyện mô hình cơ sở.
Tuy nhiên, vì chúng tôi không thể huấn luyện thành công các mô hình Transformer cơ sở đến độ chính xác thỏa mãn trên các tập dữ liệu Image và Pathfinder bằng cách sử dụng thiết lập ban đầu, chúng tôi đã giảm số lượng head và layer cho hai nhiệm vụ này. Chi tiết của mô hình cơ sở cho mỗi nhiệm vụ được nêu trong Bảng 5.
1https://github.com/google-research/long-range-arena/tree/main
2https://www.kaggle.com/datasets/joaopauloschuler/cifar10-64x64-resized-via-cai-super-resolution
14

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 5: Cấu hình của các mô hình cơ sở cho các nhiệm vụ khác nhau.
Nhiệm vụ ListOps Text Retrieval Image Pathfinder
kích thước nhúng head 512 512 512 512 512
kích thước feed-forward 2048 2048 2048 1024 1024
số lượng head 8 8 8 4 4
số lượng layer 6 6 6 4 4
C.3 SIÊU THAM SỐ CHO CÁC BASELINE VÀ PHƯƠNG PHÁP ĐỀ XUẤT
Đối với LARA và Nyströmformer, chúng tôi điều chỉnh tham số num_landmarks bằng cách tối ưu hóa trên phạm vi {64, 128, 256, 512, 1024}. Đối với H-Transformer-1D, chúng tôi điều chỉnh tham số block_size bằng cách tối ưu hóa trên phạm vi {64, 128, 256, 512, 1024}. Đối với Reformer, chúng tôi điều chỉnh các tham số num_hash và bucket_size : chúng tôi xem xét các giá trị của num_hash trong phạm vi {1, 2, 4} và các giá trị của bucket_size trong phạm vi {64, 128, 256, 512, 1024}. Đối với Longformer, Performer, và Linformer yêu cầu huấn luyện lại, do hiệu suất kém của chúng, chúng tôi chọn các giá trị siêu tham số dẫn đến mức độ xấp xỉ ít nhất. Đối với IceFormer, chúng tôi điều chỉnh tham số top_k trên phạm vi {3, 5, 8, 10, 15, 20}. Nói chung, một giá trị lớn hơn cho bucket_size, num_landmarks, block_size, hoặc top_k chỉ ra xấp xỉ ít tích cực hơn, có nghĩa là hiệu suất mô hình gần hơn với vanilla Transformer. Chúng tôi chọn các giá trị của siêu tham số dẫn đến đánh đổi độ chính xác-thời gian tốt nhất cho mỗi mô hình, và liệt kê chúng trong Bảng 6.
Bảng 6: Thiết lập siêu tham số cho các phương pháp khác nhau.
Phương pháp siêu tham số ListOps Text Retrieval Image Pathfinder
Reformer (Nikita et al., 2020)num_hash 1 1 1 1 /
bucket_size 512 128 256 1024 /
LARA (Zheng et al., 2022) num_landmarks 256 256 512 1024 1024
Nyströmformer (Xiong et al., 2021) num_landmarks 256 256 512 512 1024
H-Transformer-1D (Zhu & Soricut, 2021) block_size 1024 512 1024 256 1024
Longformer (Beltagy et al., 2020) attention_window 2048 2048 2048 2048 2048
Performer (Choromanski et al., 2020) num_rand_features 2048 2048 2048 2048 2048
Linformer (Wang et al., 2020) num_proj_dim 2048 2048 2048 2048 2048
IceFormer top_k 8 3 10 10 10
IceFormer (shared-QK) top_k 10 3 10 20 /
D THỬ NGHIỆM BỔ SUNG TRÊN LRA
Chất lượng Xấp xỉ. Để đánh giá mức độ tốt của các efficient Transformer khác nhau xấp xỉ đầu ra của module attention vanilla đã được chỉnh sửa, chúng tôi đo lỗi xấp xỉ bằng cách tính toán L2-norm của sự khác biệt giữa đầu ra module attention của chúng và của module vanilla attention tiêu chuẩn (oi trong Phương trình 2). Các lỗi xấp xỉ trung bình cho các efficient Transformer khác nhau, sử dụng cùng thiết lập siêu tham số của Bảng 6, được tóm tắt trong Bảng 7. Như được chỉ ra trong bảng, IceFormer luôn đạt được các lỗi xấp xỉ thấp nhất trên tất cả các nhiệm vụ LRA, cung cấp bằng chứng thêm về hiệu quả xấp xỉ của nó.
Bảng 7: Chất lượng của xấp xỉ trên benchmark LRA. Lỗi xấp xỉ của đầu ra module attention được báo cáo cho mỗi phương pháp trên tất cả các nhiệm vụ.
Phương pháp shared-KQ ListOps Text Retrieval Image Pathfinder
Reformer (Nikita et al., 2020) ✓ 3.823 3.926 5.452 2.130 /
LARA (Zheng et al., 2022) ✗ 2.395 9.456 10.025 22.066 9.261
Nyströmformer (Xiong et al., 2021) ✗ 5.758 10.269 6.523 18.789 10.442
H-Transformer-1D (Zhu & Soricut, 2021) ✗ 6.110 10.605 5.676 53.926 12.228
IceFormer (ours)✗ 2.140 3.891 1.825 6.873 8.749
✓ 1.562 1.686 2.499 2.127 /
Hình ảnh hóa của Bảng 1&2. Các kết quả từ Bảng 1 và Bảng 2 được biểu diễn trực quan trong Hình 6 và 7, tương ứng.
15

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hình 6: Độ trễ suy luận của IceFormer và các baseline (vanilla Transformer, Reformer, LARA, Nyströmer, H-Transformer-1D) trên benchmark LRA (càng nhỏ càng tốt).
Hình 7: Sử dụng bộ nhớ đỉnh (GB) trên benchmark LRA. Sử dụng bộ nhớ đỉnh là tổng việc sử dụng bộ nhớ của toàn bộ chương trình, bao gồm bộ nhớ cho cơ sở dữ liệu/index Prioritized DCI (càng nhỏ càng tốt).
E THÊM CHI TIẾT VỀ THỬ NGHIỆM LLM
E.1 THIẾT LẬP THỬ NGHIỆM LLM
Chúng tôi sử dụng vicuna-7b-v1.5-16k làm LLM được kiểm tra trong phần 5.3. Nó chứa 32 lớp attention, mỗi lớp chứa 32 head attention với dimensionality bằng 128. Độ dài chuỗi đầu vào tối đa của nó là 16,384. Chúng tôi quan sát các mức độ thưa khác nhau trên các lớp khác nhau của LLM, và tính thưa này vẫn nhất quán trên các prompt khác nhau. Do đó, trong tất cả các thử nghiệm LLM trong phần 5.3, chúng tôi áp dụng IceFormer để xấp xỉ các lớp tương đối thưa từ thứ mười sáu đến thứ ba mười một trong vicuna-7b-v1.5-16k. Lựa chọn này bao gồm tổng cộng 16 lớp, tương đương với một nửa tổng số lớp trong mô hình.
k trong k-NNS của IceFormer cho mỗi nhiệm vụ của benchmark ZeroSCROLLS và benchmark LongEval được định nghĩa là:
k= max(min( ⌊n∗α⌋,50),30) (29)
trong đó n là số lượng token đầu vào, ⌊x⌋ là hàm floor, và α là một siêu tham số được đặt bởi người dùng. Trong benchmark ZeroSCROLLS, chúng tôi đặt α bằng 4e-3 cho các nhiệm vụ SSFD và QMsm; 5e-3 cho các nhiệm vụ GvRp, SQAL, Qspr, Nrtv, MuSQ và BkSS; 6e-3 cho các nhiệm vụ QALT và SpDg. Trong benchmark LongEval, chúng tôi đặt α bằng 5e-3 cho tất cả các thiết lập của cả hai nhiệm vụ.
E.2 CAUSAL MASK VÀ CÁC EFFICIENT TRANSFORMER KHÁC Ở THỜI ĐIỂM SUY LUẬN
Trong bài báo chính, chúng tôi không so sánh IceFormer với LARA và Nyströmformer trên LLM. Trong phần này, chúng tôi mô tả chi tiết các vấn đề của causal mask đối với hai phương pháp này.
Hầu hết các mô hình dựa trên random-feature như LARA và Nyströmformer nhóm các token khác nhau vào các cluster khác nhau, được gọi là landmark. Để kích hoạt causal masking trong các mô hình này, không chỉ masking cần được áp dụng ở cấp độ landmark để ngăn sự rò rỉ thông tin từ các token tương lai, một tập hợp mask bổ sung cũng được yêu cầu để mask ra số lượng token khác nhau trong cùng landmark cho các query khác nhau. Cái sau không được hỗ trợ natively và đặc biệt khó triển khai. Kết quả là, rất khó áp dụng LARA và Nyströmformer cho các mô hình có causal mask.
16

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F ĐẦU RA VẢN BẢN CỦA ICEFORMER + LLM
Trong phần này, chúng tôi cung cấp các đầu ra văn bản của IceFormer khi được áp dụng cho LLM (vicuna-7b-v1.5-16k) trong Hình 8&9. Chúng tôi cũng bao gồm thông tin đầy đủ của các prompt đầu vào trong thư mục bổ sung của chúng tôi.
Hình 8: Đầu ra của IceFormer (Top-30) + LLM với 4k token đầu vào. Chúng tôi yêu cầu LLM tóm tắt một bài viết có tiêu đề "Frozen Food: The World's Favorite Killer".
17

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hình 9: Đầu ra của IceFormer (Top-40) + LLM với 8k token đầu vào. Chúng tôi yêu cầu LLM tóm tắt một bài viết có tiêu đề "Research of How Online Behavioral Advertising Influences Consumers".
18
