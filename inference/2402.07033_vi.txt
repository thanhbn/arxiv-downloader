# 2402.07033.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/inference/2402.07033.pdf
# Kích thước tệp: 2397054 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025
FIDDLER : ĐIỀU PHỐI CPU-GPU CHO
SUY LUẬN NHANH CỦA CÁC MÔ HÌNH MIXTURE-OF-EXPERTS
Keisuke Kamahori1∗Tian Tang1,2∗Yile Gu1Kan Zhu1Baris Kasikci1
1University of Washington2Tsinghua University
{kamahori,tian21,yilegu,kanzhu,baris}@cs.washington.edu
TÓM TẮT
Các Mô hình Ngôn ngữ Lớn (LLMs) với kiến trúc Mixture-of-Experts (MoE) đã cho thấy hiệu suất đầy hứa hẹn trên nhiều tác vụ khác nhau. Tuy nhiên, do kích thước mô hình rất lớn, việc chạy chúng trong các môi trường hạn chế tài nguyên nơi bộ nhớ GPU không dồi dào là một thách thức. Một số hệ thống hiện có đề xuất sử dụng tài nguyên CPU để giải quyết vấn đề đó, nhưng chúng hoặc gặp phải chi phí đáng kể từ việc di chuyển dữ liệu thường xuyên giữa CPU và GPU, hoặc không xem xét các đặc điểm riêng biệt của CPU và GPU. Bài báo này đề xuất Fiddler, một hệ thống suy luận tiết kiệm tài nguyên cho các mô hình MoE với tài nguyên GPU hạn chế. Fiddler sử dụng chiến lược các tài nguyên CPU và GPU bằng cách xác định chiến lược thực thi tối ưu. Đánh giá của chúng tôi cho thấy rằng, không giống như các hệ thống tiên tiến tối ưu hóa cho các tình huống cụ thể như suy luận batch đơn hoặc prefill dài, Fiddler hoạt động tốt hơn trong tất cả các tình huống. So sánh với các baseline khác nhau, Fiddler đạt được tăng tốc 1.26 lần trong suy luận batch đơn, 1.30 lần trong xử lý prefill dài, và 11.57 lần trong suy luận beam search. Mã nguồn của Fiddler được công khai tại https://github.com/efeslab/fiddler.

Hình 1: Tổng quan cấp cao về Fiddler. Mỗi lớp của mô hình MoE được đặt trên bộ nhớ CPU hoặc bộ nhớ GPU, và Fiddler xác định chiến lược thực thi tối ưu sử dụng cả CPU và GPU dựa trên số lượng token đầu vào của mỗi expert.

1 GIỚI THIỆU
Gần đây, việc chạy Các Mô hình Ngôn ngữ Lớn (LLMs) trong môi trường hạn chế tài nguyên đang trở nên ngày càng quan trọng và có liên quan. Có sự quan tâm ngày càng tăng đối với việc chạy LLMs trong các môi trường cục bộ như máy tính cá nhân hoặc thiết bị biên (Giacinto, 2023; Anand et al., 2023; Song et al., 2023) để cải thiện quyền riêng tư (Martínez Toro et al., 2023) và tùy chỉnh các mô hình sử dụng dữ liệu độc quyền hoặc cá nhân (Lyu et al., 2023). Việc cho phép các mô hình này hoạt động trong các cài đặt hạn chế tài nguyên sẽ dân chủ hóa quyền tiếp cận các công nghệ LLM tiên tiến, đặc biệt cho những người không có quyền truy cập vào GPU cao cấp hoặc số lượng lớn GPU. Xu hướng này được củng cố bởi các đề xuất sử dụng LLMs làm lõi của tất cả các hệ thống máy tính (Packer et al., 2023; Berger & Zorn, 2024). Do đó, việc có thể chạy các mô hình lớn trên một loạt rộng các máy tính hoặc máy chủ là mong muốn, không giống như hiện trạng nơi LLMs thường được phục vụ với các cụm GPU lớn (Patel & Ahmad, 2023; Ye et al., 2025; Zhu et al., 2024).

∗Đóng góp ngang nhau.
1arXiv:2402.07033v3 [cs.LG] 1 May 2025

--- TRANG 2 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025
LLMs sử dụng kiến trúc Mixture-of-Experts (MoE) đặc biệt phù hợp cho các môi trường hạn chế tài nguyên. LLMs dựa trên MoE đã chứng minh giá trị trên một loạt các tác vụ (Du et al., 2022; Fedus et al., 2022; Jiang et al., 2024; Databricks, 2024) và chúng hoạt động bằng cách kích hoạt có chọn lọc một tập con của các tham số thông qua cơ chế gating, do đó giảm yêu cầu tính toán cho cả huấn luyện và suy luận so với các đối tác dày đặc. Do đó, các mô hình MoE dễ dàng mở rộng đến kích thước lớn hơn, dẫn đến việc phát triển các mô hình mạnh mẽ (Rajbhandari et al., 2022).

Mặc dù các mô hình MoE có vẻ phù hợp với các môi trường hạn chế tài nguyên do yêu cầu tính toán tương đối thấp của chúng, việc triển khai suy luận cục bộ với các mô hình này đặt ra nhiều thách thức. Đầu tiên, kích thước mô hình thường rất lớn và mở rộng nhanh chóng khi số lượng experts hoặc chiều ẩn tăng. Các mô hình MoE được phát hành gần đây bao gồm Mixtral-8x7B (47B tham số), Mixtral-8x22B (141B tham số), DBRX (132B tham số), DeepSeek-V2 (236B tham số), Grok-1 (314B tham số), và Snowflake Arctic (479B tham số) (Jiang et al., 2024; AI, 2024; Databricks, 2024; DeepSeek-AI, 2024; xAI, 2024; Snowflake, 2024). Do đó, một số lượng rất lớn GPU cần thiết chỉ để lưu trữ các tham số mô hình, với khả năng thường bị hạn chế của bộ nhớ GPU. Tình huống này càng trở nên tồi tệ hơn bởi số lượng hầu như không giới hạn của các thành phần expert trong các mô hình MoE; ví dụ, biến thể lớn nhất của Switch Transformer có 2048 experts trong mỗi lớp và có tổng cộng 1.6T tham số (Fedus et al., 2022). Không có các kỹ thuật nén hoặc lượng tử hóa mô hình, mô hình có thể chiếm 3.2TB dung lượng lưu trữ. Điều này có nghĩa là 40 GPU NVIDIA A100 80GB sẽ cần thiết chỉ để lưu trữ tất cả các trọng số mô hình.

Thứ hai, trong khi tất cả các tham số phải được lưu trữ trong bộ nhớ GPU để suy luận hiệu quả, tính chất độc đáo của các mô hình MoE có nghĩa là không phải tất cả các tham số đều được sử dụng để tạo ra một token mới. Thực tế là chỉ một tập con của các tham số hoạt động trong quá trình tạo ra mỗi token dẫn đến việc sử dụng không hiệu quả bộ nhớ GPU. Điều này đặc biệt có vấn đề vì nhu cầu toàn cầu tăng vọt đối với các công nghệ AI tạo sinh đã đẩy giá GPU lên cao. Kết quả là, việc đầu tư vào một số lượng lớn GPU không hiệu quả về mặt chi phí cho hầu hết các tổ chức. Chỉ các nhà cung cấp đám mây lớn, được gọi là hyperscalers, mới có thể đủ khả năng vì họ phục vụ nhiều người dùng đồng thời và đạt được việc sử dụng GPU tốt hơn thông qua việc batching đáng kể.

Một số hệ thống hiện có sử dụng tài nguyên CPU để giải quyết thách thức chạy các mô hình lớn trong môi trường hạn chế tài nguyên, nhưng chúng gặp khó khăn với việc thực thi hiệu quả các mô hình MoE. Một mặt, các phương pháp offload trọng số mô hình vào bộ nhớ CPU và chuyển các trọng số cần thiết vào bộ nhớ GPU theo yêu cầu giải quyết các vấn đề về dung lượng bộ nhớ. Tuy nhiên, chúng gây ra chi phí runtime đáng kể do băng thông thấp của kết nối PCIe (Eliseev & Mazur, 2023; Xue et al., 2024b). Mặt khác, các framework suy luận dựa trên CPU sử dụng một phần GPU có thể giảm chi phí chuyển tham số (ggml authors, 2023). Tuy nhiên, chúng không tính đến các tính chất của mô hình MoE hoặc các đặc điểm thiết bị khác nhau của CPU và GPU, dẫn đến hiệu suất không tối ưu trong các trường hợp sử dụng quan trọng như prefill dài hoặc beam search, điều này rất cần thiết cho chất lượng tạo sinh nâng cao (Dong et al., 2022; von Platen, 2023). Chúng tôi thảo luận chi tiết về những thiếu sót của các phương pháp hiện có trong §2.

Trong bài báo này, chúng tôi giải quyết thách thức chạy hiệu quả các mô hình MoE với tài nguyên GPU hạn chế, bằng cách sử dụng chiến lược cả tài nguyên CPU và GPU. Chúng tôi giới thiệu Fiddler, một hệ thống suy luận MoE tiết kiệm tài nguyên mà thông minh tận dụng kiến trúc tính toán không đồng nhất của cả CPU và GPU. Không giống như công việc trước đây chỉ sử dụng bộ nhớ CPU hoặc chia tách thực thi một cách ngây thơ giữa CPU và GPU, phương pháp của chúng tôi tạo ra các chiến lược thực thi tối ưu bằng cách xem xét các đặc điểm khác nhau của CPU và GPU. Vì CPU có dung lượng bộ nhớ lớn hơn mặc dù có khả năng tính toán yếu hơn, các mô hình MoE đặc biệt thú vị cho bối cảnh này do yêu cầu tính toán nhỏ tương đối so với kích thước tham số của chúng.

Trong quá trình suy luận, Fiddler phát triển một mô hình độ trễ dựa trên các hiệu ứng batching khác nhau của CPU và GPU để xác định chiến lược thực thi tối ưu cho các lớp MoE, như được hiển thị trong Hình 1. Khi các lớp expert được thực thi trên CPU, độ trễ tăng gần như tuyến tính với kích thước đầu vào (xem phân tích chi tiết trong §A). Ngược lại, độ trễ thực thi GPU gần như không đổi bất kể kích thước đầu vào nhưng phát sinh chi phí nếu các trọng số cần được chuyển từ bộ nhớ CPU sang bộ nhớ GPU. Do đó, đối với các kích thước đầu vào nhỏ hơn, việc thực thi các lớp expert trên CPU hiệu quả hơn, tránh chi phí chuyển trọng số. Tuy nhiên, đối với các kích thước đầu vào và kích thước batch lớn hơn, việc tính toán CPU trở nên quá tốn thời gian, làm cho việc chuyển trọng số vào bộ nhớ GPU và thực hiện tính toán trên GPU hiệu quả hơn. Fiddler lựa chọn động các kế hoạch thực thi chạy các mô hình MoE hiệu quả với bộ nhớ GPU hạn chế trên các workload khác nhau, bao gồm prefill dài và beam search.

2

--- TRANG 3 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025
Chúng tôi cũng tích hợp một số tối ưu hóa vào thiết kế của Fiddler. Để tối đa hóa khả năng expert cần thiết có sẵn trong bộ nhớ GPU, chúng tôi đặt các expert được sử dụng thường xuyên trên GPU dựa trên profiling offline về mức độ phổ biến của expert. Ngoài ra, chúng tôi thiết kế một kernel tính toán chuyên biệt cho xử lý expert trên CPU sử dụng bộ lệnh AVX512_BF16, không được hỗ trợ trong triển khai PyTorch native (Paszke et al., 2019).

Chúng tôi đánh giá Fiddler với mô hình Mixtral-8x7B không nén (16-bit), có hơn 90GB tham số, trên hai môi trường với mỗi GPU đơn. Fiddler đạt được trung bình tăng tốc 1.26 lần trong suy luận batch đơn, 1.30 lần trong xử lý prefill dài, và 11.57 lần trong suy luận beam search, so sánh với các hệ thống tiên tiến khác nhau, trên các môi trường khác nhau (§4). Đáng chú ý, trong khi các hệ thống hiện có cho thấy các trade-off khác nhau (ví dụ, các phương pháp dựa trên offloading xuất sắc trong các tình huống prefill dài, trong khi các phương pháp dựa trên CPU hoạt động tốt với độ trễ batch đơn), hệ thống của chúng tôi tích hợp các lợi thế của cả hai, đạt được kết quả cân bằng và hiệu quả trong các điều kiện đa dạng.

Để tóm tắt, các đóng góp của chúng tôi như sau:
• Chúng tôi thiết kế Fiddler, một hệ thống suy luận cho các mô hình MoE chạy trên kiến trúc không đồng nhất hạn chế tài nguyên, tìm ra chiến lược thực thi tối ưu sử dụng cả GPU và CPU.
• Chúng tôi đánh giá Fiddler và cho thấy rằng nó đạt được hiệu suất tốt hơn trong suy luận batch đơn, xử lý prefill dài, và suy luận beam search, so với các hệ thống tiên tiến khác nhau. Nó cho thấy rằng Fiddler tích hợp các lợi thế của các loại hệ thống hiện có khác nhau.

2 CÔNG VIỆC LIÊN QUAN

2.1 MIXTURE-OF-EXPERTS
LLMs dựa trên kiến trúc MoE đã cho thấy hiệu suất đầy hứa hẹn trong nhiều ứng dụng khác nhau (Rajbhandari et al., 2022; Du et al., 2022; Fedus et al., 2022; Jiang et al., 2024; Xue et al., 2024a; Dai et al., 2024). Không giống như Transformers dày đặc ban đầu (Vaswani et al., 2017), các mô hình MoE thêm độ thưa thớt vào lớp feed-forward thông qua một hệ thống experts và cơ chế gating. Mỗi lớp MoE chứa nhiều lớp expert khớp với hình dạng của lớp feed-forward, và một mạng gating xác định experts nào được kích hoạt cho mỗi đầu vào. Trong khi một lớp MoE có thể bao gồm hàng nghìn experts (Fedus et al., 2022), chỉ một số ít được chọn lọc được kích hoạt bởi mạng gating trong quá trình huấn luyện hoặc suy luận.

2.2 TRIỂN KHAI MÔ HÌNH LỚN VỚI KIẾN TRÚC KHÔNG ĐỒNG NHẤT
Triển khai hiệu quả các mô hình MoE có thể là thách thức vì kích thước mô hình lớn của chúng, đặc biệt trong các cài đặt hạn chế tài nguyên. Một số hệ thống hiện có sử dụng tài nguyên CPU để giải quyết thách thức chạy các mô hình lớn trong môi trường hạn chế tài nguyên, nhưng chúng không đạt được việc chạy hiệu quả các mô hình MoE.

Offloading là một phương pháp để chạy các mô hình lớn trong môi trường như vậy. Chúng lưu trữ một tập con của trọng số mô hình trong bộ nhớ CPU thay vì bộ nhớ GPU để sử dụng dung lượng lớn hơn (Sheng et al., 2023). Các trọng số cần thiết được chuyển theo yêu cầu từ bộ nhớ CPU sang bộ nhớ GPU trong quá trình tính toán để suy luận. Đối với các mô hình MoE, một số công việc trước đây đã cố gắng offload trọng số expert với cơ chế caching hoặc prefetching (Eliseev & Mazur, 2023; Xue et al., 2024b). Các phương pháp này giải quyết các hạn chế về dung lượng bộ nhớ và tốt cho các tình huống hướng throughput. Tuy nhiên, chúng gặp phải chi phí độ trễ đáng kể do việc chuyển thường xuyên trọng số expert giữa CPU và GPU qua kết nối PCIe, vì băng thông của nó nhỏ hơn băng thông truy cập bộ nhớ. Kết quả là, chúng cho thấy hiệu suất không tối ưu cho các cài đặt nơi độ trễ là quan trọng cho trải nghiệm người dùng. Fiddler vượt qua thách thức này bằng cách sử dụng tài nguyên tính toán của CPU.

Một dòng công việc khác đề xuất các framework suy luận dựa trên CPU hỗ trợ chạy LLMs bằng cách sử dụng một phần GPU (ggml authors, 2023). Tùy thuộc vào tính khả dụng của bộ nhớ GPU, các hệ thống như vậy thực thi một tập con của các lớp mô hình trên GPU và phần còn lại trên CPU. Mặc dù chúng có thể

3

--- TRANG 4 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Hình 2: Tổng quan về Fiddler. (a) Trong giai đoạn khởi tạo, các tham số của các lớp không phải expert và một tập con được chọn của các lớp expert được phân bổ cho bộ nhớ GPU theo khả năng cho phép; các tham số còn lại được phân bổ cho bộ nhớ CPU. (b) Tại thời gian chạy, Fiddler xác định động chiến lược thực thi tối ưu bằng cách xem xét khối lượng đầu vào kích hoạt mỗi lớp expert cùng với các độ trễ xử lý dự kiến khác nhau của CPU và GPU.

giảm chi phí chuyển các tham số mô hình, các phương pháp như vậy cho thấy tốc độ thực thi không tối ưu cho các trường hợp sử dụng quan trọng, như prefill dài hoặc beam search, điều này rất cần thiết cho chất lượng tạo sinh nâng cao (Dong et al., 2022; von Platen, 2023). Điều này là do chúng không xem xét các hiệu ứng batching khác nhau của GPU và CPU (Chen, 2023) và các tính chất của mô hình MoE.

Mặc dù các kỹ thuật nén mô hình như lượng tử hóa (Frantar & Alistarh, 2023; Zhao et al., 2023) hoặc sparsification (Alizadeh et al., 2023; Tang et al., 2024; Zhu et al., 2025) có thể giảm kích thước mô hình và cải thiện hiệu quả suy luận, chúng đi kèm với chất lượng đầu ra mô hình giảm sút, đặc biệt khi cố gắng khớp các mô hình lớn vào GPU với dung lượng bộ nhớ hạn chế (Eliseev & Mazur, 2023). Gần đây, (Song et al., 2023) đã đề xuất khai thác độ thưa thớt của LLMs để suy luận nhanh hơn với CPU offloading. Tuy nhiên, phương pháp này yêu cầu mô hình sử dụng hàm Rectified Linear Units (ReLU) cho kích hoạt phi tuyến. Chuyển đổi các mô hình không phải ReLU, phổ biến trong các LLMs tiên tiến, sang mô hình ReLU yêu cầu huấn luyện bổ sung và gây ra suy giảm chất lượng mô hình (Mirzadeh et al., 2023; SparseLLM). Ví dụ, Mixtral-8x7B sử dụng hàm Sigmoid Linear Units (SiLU) (Elfwing et al., 2018), và chỉ một phần nhỏ giá trị gần bằng không. Do đó, rất khó để khai thác độ thưa thớt (một thảo luận chi tiết hơn được đưa ra trong Phụ lục B). Fiddler có thể đạt được hiệu suất tốt hơn mà không sửa đổi cấu trúc mô hình hoặc độ chính xác. Chúng tôi lưu ý rằng Fiddler trực giao với các kỹ thuật nén, và những tối ưu hóa này có thể được áp dụng lên trên Fiddler.

3 THIẾT KẾ

Phần này giải thích thiết kế của Fiddler. Fiddler được thiết kế cho tình huống nơi dung lượng bộ nhớ GPU không đủ để lưu trữ tất cả các tham số mô hình MoE. Do đó, trọng số của một số experts được lưu trữ trong bộ nhớ CPU thay vì bộ nhớ GPU. Fiddler tìm ra chiến lược thực thi tối ưu cho các trường hợp như vậy, với việc chọn expert bởi đầu vào và hành vi batching khác nhau của CPU và GPU.

3.1 TỔNG QUAN

Hình 2 minh họa tổng quan về Fiddler. Trong giai đoạn khởi tạo, Fiddler phân bổ các tham số cho các lớp không phải expert cùng với những tham số cho một tập con được chọn của các lớp expert vào bộ nhớ GPU, nhiều nhất là dung lượng bộ nhớ GPU cho phép. Fiddler chọn những experts đó để được đặt trên bộ nhớ GPU dựa trên mức độ phổ biến của chúng, mà chúng tôi giải thích trong §3.4. Fiddler luôn phân bổ trọng số của các lớp không phải expert trên bộ nhớ GPU vì chúng được sử dụng cho mọi token, bất kể lựa chọn expert. Kích thước của các lớp không phải expert thường không lớn (ví dụ, ít hơn 2 tỷ tham số cho mô hình Mixtral-8x7B), và chúng tôi giả định chúng phù hợp với bộ nhớ GPU trong bài báo này. Các tham số của các lớp expert không phù hợp với bộ nhớ GPU do hạn chế dung lượng được lưu trữ trong bộ nhớ CPU.

4

--- TRANG 5 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Trong giai đoạn thực thi, Fiddler cẩn thận đánh giá và lựa chọn chiến lược thực thi hiệu quả nhất. Quyết định này được thông báo bởi số lượng đầu vào mà mỗi lớp expert nhận được và độ trễ xử lý khác nhau của CPU và GPU. Lớp gating của mô hình xác định số lượng đầu vào mà mỗi expert nhận được, và độ trễ xử lý có thể được dự đoán sử dụng các tính chất thiết bị.

Fiddler xem xét các hiệu ứng batching khác nhau của CPU và GPU. Trong xử lý các lớp expert, số lượng đầu vào ảnh hưởng đến độ trễ thực thi khác nhau trên CPU và GPU. Cụ thể, xử lý GPU thể hiện độ trễ tương đối ổn định qua các kích thước đầu vào khác nhau, có thể được gán cho khả năng xử lý song song của nó. Những khả năng này làm cho độ trễ thực thi bị giới hạn bởi thời gian cần thiết để tải tham số từ bộ nhớ. Ngược lại, độ trễ liên quan đến xử lý CPU có xu hướng mở rộng gần như tuyến tính với kích thước đầu vào. Sự tăng tuyến tính này là do khả năng tính toán yếu hơn của CPU so với GPU, làm cho độ trễ bị giới hạn bởi phần tính toán, không phải phần di chuyển bộ nhớ. Chúng tôi đưa ra phân tích chi tiết hơn trong Phụ lục A.

Hình 3: Ba tình huống khác nhau cho việc thực thi các lớp expert. Khi trọng số expert có mặt trong bộ nhớ GPU, lớp expert có thể được thực thi tại GPU mà không có bất kỳ chuyển dữ liệu nào (a). Khi trọng số expert thiếu trong bộ nhớ GPU, trọng số expert có thể được sao chép từ bộ nhớ CPU sang bộ nhớ GPU và thực thi tại GPU (b), hoặc activation có thể được sao chép từ bộ nhớ GPU sang bộ nhớ CPU và thực thi tại CPU (c).

3.2 CHIẾN LƯỢC THỰC TỬI

Sau khi expert được chọn cho mỗi token đầu vào tại lớp không phải expert, có ba tình huống cho việc xử lý các lớp expert, như được hiển thị trong Hình 3.

Nếu trọng số tương ứng có mặt trên bộ nhớ GPU, lớp expert có thể được thực thi trên GPU mà không có bất kỳ chuyển dữ liệu nào giữa CPU và GPU (Hình 3 a).

Tuy nhiên, vì tất cả các tham số mô hình không phù hợp với bộ nhớ GPU, đôi khi trọng số expert không có mặt trong bộ nhớ GPU. Trong trường hợp đó, tồn tại hai chiến lược khác nhau để thực thi lớp expert. Phương pháp đầu tiên là sao chép trọng số mô hình từ bộ nhớ CPU sang bộ nhớ GPU, sau đó thực thi expert sử dụng GPU (Hình 3 b). Khi một số trọng số expert thiếu trên bộ nhớ GPU (1), chúng được sao chép từ bộ nhớ CPU sang bộ nhớ GPU (2), và sau đó GPU thực thi lớp expert (3). Các hệ thống offloading hiện có sử dụng phương pháp này.

Một phương pháp khác là sao chép các activations từ bộ nhớ GPU sang bộ nhớ CPU và thực thi lớp expert trên CPU (Hình 3 c). Trong phương pháp này, khi một số trọng số expert thiếu trên bộ nhớ GPU (1), các giá trị activation được sao chép từ bộ nhớ GPU sang bộ nhớ CPU (2) thay vì sao chép trọng số. Sau đó, việc tính toán lớp expert xảy ra trên CPU (3), và các activations đầu ra được sao chép trở lại GPU sau khi tính toán hoàn thành (4). Một phương pháp tương tự được sử dụng bởi llama.cpp.

Hai chiến lược sau (b. và c. ở trên) có các trade-off khác nhau. Một mặt, GPU có khả năng tính toán mạnh hơn phù hợp cho xử lý expert. Do đó, (b) có lợi thế so với (c) về độ trễ tính toán. Hơn nữa, như đã thảo luận trước đây, độ trễ tính toán của (c) trở nên dài hơn khi kích thước đầu vào tăng do các hiệu ứng batching khác nhau của CPU và GPU.

5

--- TRANG 6 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Mặt khác, xem xét giao tiếp CPU-GPU, phương pháp trong (b) cần chuyển trọng số mô hình, trong khi (c) chỉ cần chuyển các giá trị activation. Vì kích thước của activations (kích thước đầu vào × 4096 cho Mixtral-8x7B) nhỏ hơn đáng kể so với kích thước trọng số (3 ma trận với kích thước 4096×14336 mỗi expert cho Mixtral-8x7B, tiêu thụ hơn 300MB với độ chính xác 16-bit) cho các kích thước đầu vào nhỏ, (c) có lợi thế trong việc giảm chi phí giao tiếp.

Nhìn chung, (c) có lợi thế khi kích thước đầu vào cho một expert nhỏ, trong khi (b) tốt hơn nếu kích thước đầu vào trên một ngưỡng nào đó, ngay cả với chi phí giao tiếp lớn. Khi xử lý prefill dài, kích thước đầu vào có thể đạt đến một nghìn. Tuy nhiên, trong các tình huống như vậy, độ trễ tính toán của phương pháp (c) trở nên cấm hơn độ trễ chuyển trọng số của (b), làm cho (c) trở thành lựa chọn không thực tế. Do đó, độ trễ chuyển cho activation là không đáng kể khi (c) được sử dụng. Chúng tôi đưa ra phân tích định lượng chi tiết hơn trong Phụ lục A.

3.3 THUẬT TOÁN

Dựa trên phân tích được mô tả ở trên, Fiddler phục vụ các mô hình MoE theo cách sau.

Khởi tạo. Trước khi bắt đầu quá trình suy luận, Fiddler phân phối trọng số mô hình giữa bộ nhớ CPU và GPU. Đầu tiên, trọng số của các lớp không phải expert được đặt trên bộ nhớ GPU vì chúng được sử dụng cho mọi token, bất kể lựa chọn expert. Kích thước của các lớp không phải expert thường không lớn (ít hơn 2 tỷ tham số cho mô hình Mixtral-8x7B), và Fiddler giả định chúng phù hợp với bộ nhớ GPU trong bài báo này. Tiếp theo, Fiddler đặt một tập con của các lớp expert vào bộ nhớ GPU. Để làm điều này, nó chọn nhiều experts nhất mà dung lượng bộ nhớ cho phép để tối đa hóa tỷ lệ hit, tức là khả năng trọng số của một expert có trong bộ nhớ GPU. Để lựa chọn expert, chúng tôi áp dụng một tối ưu hóa như đã thảo luận trong §3.4.

Chúng tôi cũng đo độ trễ để sao chép trọng số và thực thi experts trên CPU hoặc GPU với các kích thước đầu vào khác nhau để thông báo cho quyết định tại runtime.

Thực thi. Tại runtime, Fiddler xác định các cấu hình tối ưu để thực thi các lớp expert trên GPU và CPU. Fiddler biết expert(s) nào nên được sử dụng cho mỗi token được xử lý sau khi thực thi hàm gating cho mỗi lớp. Điều này cho phép Fiddler học kích thước đầu vào cho mỗi expert. Lưu ý rằng nhiều đầu vào có thể được xử lý đồng thời, ngay cả cho một yêu cầu duy nhất, trong giai đoạn prefill hoặc khi beam search được sử dụng.

Dựa trên thông tin kích thước đầu vào, Fiddler xác định chiến lược thực thi hiệu quả nhất để phân phối workload trên CPU và GPU. Để đạt được điều này, Fiddler sử dụng Thuật toán 1. Hàm is_at_gpu(i, j) kiểm tra xem trọng số của expert j trong lớp thứ i có được đặt trong bộ nhớ GPU tại thời điểm khởi tạo hay không. Ngoài ra, cpu_lat(s) và gpu_lat(s) cung cấp độ trễ dự kiến để thực thi một expert trên CPU và GPU, tương ứng, với kích thước đầu vào s. Hàm transfer_lat() ước tính độ trễ cần thiết để chuyển trọng số của một expert từ bộ nhớ CPU sang bộ nhớ GPU.

Khi thực thi một expert trên GPU cùng với chuyển trọng số, độ trễ chủ yếu bị chi phối bởi thời gian cần thiết để chuyển trọng số của expert từ CPU sang bộ nhớ GPU, độc lập với kích thước batch. Ngược lại, thực thi một lớp expert trên CPU thể hiện hành vi khác nhau: khi số lượng token đầu vào tăng, độ trễ cũng tăng. Tuy nhiên, thời gian cần thiết để sao chép activation từ GPU sang CPU là không đáng kể, chiếm ít hơn 1% tổng độ trễ (xem Phụ lục A để biết thêm chi tiết).

Để tối ưu hóa việc xử lý giai đoạn prefill, chúng tôi sử dụng một mô hình nơi thời gian thực thi GPU được coi là hằng số, trong khi thời gian thực thi CPU được giả định tăng tuyến tính với số lượng token đầu vào. Cụ thể, cho số lượng token đầu vào s, gpu_lat(s) trả về một giá trị hằng số, trong khi cpu_lat(s) trả về một giá trị tỷ lệ với s, nhân với một hằng số khác. Những hằng số này được xác định trong giai đoạn khởi tạo.

6

--- TRANG 7 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Thuật toán 1 Chiến lược Thực thi Expert
1: Đầu vào:
2: ne: số lượng experts trong một lớp
3: i: lớp cần xem xét (chúng ta xem xét lớp thứ i)
4: inp_size: mảng kích thước đầu vào cho mỗi expert
5: for j = 1 to ne do
6: s ← inp_size[j]
7: if s == 0 then
8: continue
9: end if
10: if is_at_gpu(i, j) then
11: // chạy expert thứ j tại GPU
12: else if cpu_lat(s) > gpu_lat(s) + trans_lat() then
13: // chạy expert thứ j tại GPU
14: else
15: // chạy expert thứ j tại CPU
16: end if
17: end for

3.4 TỐI ƯU HÓA

Hiệu suất thực thi tốt nhất đạt được khi phương pháp của Hình 3 a được sử dụng thường xuyên nhất có thể, tức là khi trọng số expert cần thiết bởi đầu vào có mặt trong bộ nhớ GPU thường xuyên nhất có thể. Để tối đa hóa khả năng expert cần thiết có sẵn trong bộ nhớ GPU, chúng tôi đặt các experts được sử dụng thường xuyên trên GPU dựa trên profiling offline. Để làm điều này, chúng tôi chọn nhiều experts nhất mà dung lượng bộ nhớ cho phép theo thứ tự mức độ phổ biến để tối đa hóa tỷ lệ hit, tức là khả năng trọng số của một expert có trong bộ nhớ GPU. Chúng tôi xác định các experts phổ biến dựa trên profile của việc lựa chọn expert sử dụng dữ liệu hiệu chuẩn. Chúng tôi giả định phương pháp này đủ vì việc lựa chọn expert được biết là dựa trên đặc điểm token, và mức độ phổ biến của experts gần như phổ quát trên các miền đầu vào khác nhau (Jiang et al., 2024; Xue et al., 2024a). Phụ lục C thảo luận việc lựa chọn expert chi tiết hơn.

Ngoài ra, chúng tôi thiết kế một kernel tính toán chuyên biệt cho xử lý expert trên CPU sử dụng bộ lệnh AVX512_BF16, không được hỗ trợ trong triển khai PyTorch native (Paszke et al., 2019).

4 ĐÁNH GIÁ

Chúng tôi đánh giá hiệu suất của Fiddler cho suy luận mô hình MoE trong các cài đặt hạn chế tài nguyên, nơi một số lượng nhỏ truy vấn đồng thời được đưa ra, và độ trễ là quan trọng cho trải nghiệm người dùng.

4.1 THIẾT LẬP

Mô hình và Dữ liệu. Chúng tôi sử dụng mô hình Mixtral-8x7B (Jiang et al., 2024) với độ chính xác 16-bit cho đánh giá. Mô hình này không chỉ đại diện cho kiến trúc của hầu hết các mô hình MoE gần đây mà còn được hỗ trợ bởi tất cả các hệ thống baseline, đảm bảo so sánh hiệu suất công bằng. Cho dữ liệu đánh giá và hiệu chuẩn, chúng tôi sử dụng ShareGPT (ShareGPT), một bộ dữ liệu các cuộc trò chuyện giữa con người và chatbot, để mô hình hóa hành vi thực tế của việc lựa chọn expert. Chúng tôi chọn tập con các cuộc trò chuyện một cách ngẫu nhiên. Chúng tôi triển khai Fiddler trên nền tảng PyTorch (Paszke et al., 2019). Ngoài ra, chúng tôi đưa ra một nghiên cứu độ nhạy trên các bộ dữ liệu khác nhau trong §D để cho thấy hiệu quả của Fiddler trong một loạt rộng hơn các hành vi routing.

Môi trường. Chúng tôi đánh giá Fiddler trên hai môi trường như được hiển thị trong Bảng 1. Môi trường 1 được trang bị CPU và GPU yếu hơn so với Môi trường 2 để cho thấy hiệu quả của Fiddler trên một loạt rộng cấu hình phần cứng. Không có môi trường nào có đủ dung lượng bộ nhớ GPU để lưu trữ tất cả các tham số mô hình. Hàng "Số lượng Experts trên GPU" cho thấy số lượng tối đa

7

--- TRANG 8 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Bảng 1: Thiết lập đánh giá
Môi trường 1 | Môi trường 2
GPU | Quadro RTX 6000 (NVIDIA, b) | RTX 6000 Ada (NVIDIA, a)
Bộ nhớ GPU | 24576MiB | 49140MiB
PCIe | Gen3 x16 (32GB/s) | Gen4 x16 (64GB/s)
CPU | Intel(R) Xeon(R) Gold 6126 (48 core) | Intel Xeon Platinum 8480+ (112 core)
Số lượng Experts trên GPU | 56/256 | 125/256

số lượng experts có thể phù hợp với bộ nhớ GPU trong số 256 experts (32 lớp × 8 experts/lớp), cho dung lượng bộ nhớ.

Baselines. Cho baselines, chúng tôi đánh giá DeepSpeed-MII phiên bản v0.2.3 (Microsoft), Mixtral-Offloading (Eliseev & Mazur, 2023), và llama.cpp phiên bản b2956 (ggml authors, 2023).

Cho DeepSpeed-MII, chúng tôi kích hoạt tối ưu hóa ZeRO-Infinity (Rajbhandari et al., 2021) để nó offload các tham số mô hình vào bộ nhớ CPU và tải chúng từ CPU sang GPU một cách động trong quá trình suy luận khi cần thiết. Chúng tôi kích hoạt pin_memory trong cấu hình để sử dụng bộ nhớ CPU được khóa trang, cải thiện hiệu suất đọc/ghi từ bộ nhớ CPU và giảm phân mảnh bộ nhớ.

Mixtral-Offloading ban đầu chỉ hỗ trợ phiên bản lượng tử hóa của mô hình Mixtral-8x7B theo mặc định. Để so sánh công bằng, chúng tôi mở rộng Mixtral-Offloading để hỗ trợ chạy phiên bản gốc của mô hình với độ chính xác 16-bit. Mixtral-Offloading cung cấp tham số offload_per_layer để xác định số lượng experts trong mỗi lớp expert để offload vào bộ nhớ CPU. Chúng tôi đặt tham số offload_per_layer thành 7 cho Môi trường 1 và 5 cho Môi trường 2 vì đây là cấu hình tốt nhất cho các môi trường chúng tôi thử nghiệm. Cho llama.cpp, chúng tôi đặt các tham số ngl kiểm soát số lượng lớp được thực thi trong GPU thành 8 cho Môi trường 1 và 16 cho Môi trường 2.

Metrics. Chúng tôi đánh giá hiệu suất của Fiddler so với baselines trong ba tình huống khác nhau phục vụ một yêu cầu duy nhất: a độ trễ end-to-end với các độ dài khác nhau của token đầu vào và đầu ra, b xử lý prefill cho đầu vào ngữ cảnh dài, và c độ trễ end-to-end của beam search với các độ rộng khác nhau. Những metrics này phản ánh các trường hợp sử dụng quan trọng: đầu vào ngữ cảnh dài được sử dụng cho in-context learning hoặc retrieval augmented generation (Dong et al., 2022; Gao et al., 2023), và beam search được sử dụng cho chất lượng nâng cao của các token được tạo ra (von Platen, 2023).

Chúng tôi báo cáo tốc độ suy luận được đo bằng token mỗi giây cho a và c (số lượng token được tạo chia cho độ trễ end-to-end), và Time To First Token (TTFT) cho b. Cho đánh giá với N token đầu vào, chúng tôi chọn ngẫu nhiên các mẫu từ ShareGPT với N token hoặc nhiều hơn của prompt và sử dụng N token đầu tiên.

Cho a, độ dài đầu vào trong số [32, 64, 128, 256], và độ dài đầu ra trong số [64, 128, 256, 512]. Độ dài đầu vào cho b trong số [512, 1024, 2048, 4096]. Chúng tôi đặt độ rộng beam search cho c trong số [4, 8, 12, 16] với độ dài đầu vào 32 và độ dài đầu ra 64. Cho beam search, chúng tôi chỉ so sánh Fiddler với llama.cpp vì các baselines khác không hỗ trợ suy luận beam search.

4.2 KẾT QUẢ

Hình 4 cho thấy hiệu suất end-to-end của bốn phương pháp trong hai môi trường. Trung bình, trên tất cả các cấu hình và môi trường, Fiddler vượt trội so với baseline tốt nhất, llama.cpp, Fiddler đạt hiệu suất nhanh hơn 1.26 lần trung bình trên các độ dài đầu vào/đầu ra và môi trường khác nhau.

Hình 5 cho thấy TTFT cho prefill ngữ cảnh dài. Trong trường hợp này, các phương pháp dựa trên offloading (DeepSpeed-MII và (Eliseev & Mazur, 2023)) tốt hơn llama.cpp. Tuy nhiên, Fiddler vẫn cho thấy hiệu suất tốt hơn bất kỳ phương pháp hiện có nào, vượt trội DeepSpeed-MII 1.07 lần và (Eliseev & Mazur, 2023) 1.65 lần trung bình trên các cấu hình khác nhau. Hình 6 cho thấy độ trễ end-to-end

1 Chúng tôi định nghĩa độ trễ end-to-end là thời gian từ khi yêu cầu suy luận được nhận đến việc tạo ra token cuối cùng, bao gồm cả thời gian prefill và decode.

8

--- TRANG 9 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Hình 4: So sánh hiệu suất end-to-end theo số lượng token được tạo ra mỗi giây (tình huống a, cao hơn là tốt hơn), với 15 cấu hình độ dài đầu vào/đầu ra khác nhau. Bộ thanh bên phải nhất cho thấy trung bình của 15 cấu hình.

của suy luận beam search với các độ rộng search khác nhau, so sánh với llama.cpp. Trung bình, Fiddler đạt hiệu suất tốt hơn 11.57 lần so với llama.cpp.

Hình 5: So sánh hiệu suất theo TTFT (tình huống b, thấp hơn là tốt hơn), với 4 cấu hình độ dài đầu vào khác nhau. Bộ thanh bên phải nhất cho thấy trung bình của 4 độ dài khác nhau.

Hình 6: So sánh hiệu suất cho suy luận beam search được đo bằng số lượng token được tạo ra mỗi giây (tình huống c, cao hơn là tốt hơn), với độ dài đầu vào 32 và độ dài đầu ra 64. Bộ thanh bên phải nhất cho thấy trung bình của 4 độ rộng beam search.

Những kết quả này cho thấy rằng Fiddler hoạt động tốt hơn trong một loạt rộng các ứng dụng so với các hệ thống hiện có. Lợi ích chủ yếu đến từ khả năng của Fiddler xác định chiến lược thực thi một cách động dựa trên các hiệu ứng batching của CPU và GPU và đặt experts dựa trên profile mức độ phổ biến. Đáng chú ý, trong khi các hệ thống hiện có cho thấy các trade-off khác nhau (ví dụ, các phương pháp dựa trên offloading xuất sắc trong các tình huống prefill dài và các phương pháp như llama.cpp hoạt động tốt với độ trễ batch đơn), hệ thống của chúng tôi tích hợp các lợi thế của cả hai, đạt được kết quả cân bằng và hiệu quả trong các điều kiện đa dạng.

5 KẾT LUẬN

Bài báo này đề xuất Fiddler, một hệ thống suy luận tiết kiệm tài nguyên cho các mô hình MoE với tài nguyên GPU hạn chế. Fiddler sử dụng chiến lược kiến trúc tính toán không đồng nhất của tài nguyên CPU và GPU bằng cách xác định chiến lược thực thi tối ưu. Fiddler đạt được hiệu suất tốt hơn trong tất cả các tình huống phổ biến cho suy luận cục bộ trong khi các hệ thống tiên tiến chỉ được tối ưu hóa cho một phần của chúng. Đánh giá của chúng tôi cho thấy rằng so với các hệ thống tiên tiến, Fiddler đạt tăng tốc 1.26 lần trong suy luận batch đơn, 1.30 lần trong xử lý prefill dài, và 11.57 lần trong suy luận beam search.

9

--- TRANG 10 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

TÀI LIỆU THAM KHẢO

Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024.

Mistral AI. Cheaper, better, faster, stronger, 2024. URL https://mistral.ai/news/mixtral-8x22b/.

Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. Llm in a flash: Efficient large language model inference with limited memory. arXiv preprint arXiv:2312.11514, 2023.

Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar. Gpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo. https://github.com/nomic-ai/gpt4all, 2023.

Emery Berger and Ben Zorn. Ai software should be more like plain old software, 2024. URL https://www.sigarch.org/ai-software-should-be-more-like-plain-old-software/.

Lequn Chen. Dissecting batching effects in gpt inference, 2023. URL https://le.qun.ch/en/blog/2023/05/13/transformer-batching/.

Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024.

Databricks. Introducing dbrx: A new state-of-the-art open llm, 2024. URL https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm.

DeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.

Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022.

Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547–5569. PMLR, 2022.

Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks, 107:3–11, 2018.

Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.

William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232–5270, 2022.

Elias Frantar and Dan Alistarh. Qmoe: Practical sub-1-bit compression of trillion-parameter models. arXiv preprint arXiv:2310.16795, 2023.

Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.

The ggml authors. llama.cpp, 2023. URL https://github.com/ggerganov/llama.cpp.

Ettore Di Giacinto. Localai. https://github.com/mudler/LocalAI, 2023.

10

--- TRANG 11 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.

Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pp. 22137–22176. PMLR, 2023.

Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo Luo. Llm-rec: Personalized recommendation via prompting large language models. arXiv preprint arXiv:2307.15780, 2023.

Iván Martínez Toro, Daniel Gallego Vico, and Pablo Orgaz. PrivateGPT, 2023. URL https://github.com/imartinez/privateGPT.

Microsoft. Deepspeed-mii. https://github.com/microsoft/DeepSpeed-MII.

Iman Mirzadeh, Keivan Alizadeh, Sachin Mehta, Carlo C Del Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, and Mehrdad Farajtabar. Relu strikes back: Exploiting activation sparsity in large language models. arXiv preprint arXiv:2310.04564, 2023.

NVIDIA. Nvidia rtx 6000 ada generation. https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/rtx-6000/proviz-print-rtx6000-datasheet-web-2504660.pdf, a.

NVIDIA. Nvidia quadro rtx 6000 pcie server card. https://www.nvidia.com/content/dam/en-zz/Solutions/design-visualization/quadro-product-literature/NVIDIA-Quadro-RTX-6000-PCIe-Server-Card-PB-FINAL-1219.pdf, b.

Charles Packer, Vivian Fang, Shishir G Patil, Kevin Lin, Sarah Wooders, and Joseph E Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.

Dylan Patel and Afzal Ahmad. The inference cost of search disruption – large language model cost analysis, 2023. URL https://www.semianalysis.com/p/the-inference-cost-of-search-disruption.

Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He. Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning, 2021.

Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. In International Conference on Machine Learning, pp. 18332–18346. PMLR, 2022.

ShareGPT. Sharegpt. https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered.

Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865, 2023.

Snowflake. Snowflake arctic: The best llm for enterprise ai — efficiently intelligent, truly open, 2024. URL https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/.

Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model serving with a consumer-grade gpu. arXiv preprint arXiv:2312.12456, 2023.

11

--- TRANG 12 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

SparseLLM. Relullama-70b. https://huggingface.co/SparseLLM/ReluLLaMA-70B.

Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference, 2024. URL https://arxiv.org/abs/2406.10774.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Patrick von Platen. How to generate text: using different decoding methods for language generation with transformers, 2023. URL https://huggingface.co/blog/how-to-generate.

xAI. Open release of grok-1, 2024. URL https://x.ai/blog/grok-os.

Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. Openmoe: An early effort on open mixture-of-experts language models. 2024a.

Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh Marina. Moe-infinity: Activation-aware expert offloading for efficient moe serving. arXiv preprint arXiv:2401.14361, 2024b.

Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, and Luis Ceze. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. URL https://arxiv.org/abs/2501.01005.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.

Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom: Low-bit quantization for efficient and accurate llm serving. arXiv preprint arXiv:2310.19102, 2023.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, and Hao Zhang. Lmsys-chat-1m: A large-scale real-world llm conversation dataset, 2024. URL https://arxiv.org/abs/2309.11998.

Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei Gao, Qinyu Xu, Tian Tang, Zihao Ye, Keisuke Kamahori, Chien-Yu Lin, Stephanie Wang, Arvind Krishnamurthy, and Baris Kasikci. Nanoflow: Towards optimal large language model serving throughput. CoRR, abs/2408.12757, 2024. doi: 10.48550/ARXIV.2408.12757. URL https://doi.org/10.48550/arXiv.2408.12757.

Kan Zhu, Tian Tang, Qinyu Xu, Yile Gu, Zhichen Zeng, Rohan Kadekodi, Liangyu Zhao, Ang Li, Arvind Krishnamurthy, and Baris Kasikci. Tactic: Adaptive sparse attention with clustering and distribution fitting for long-context llms, 2025. URL https://arxiv.org/abs/2502.12216.

A MICROBENCHMARKS

Trong phần này, chúng tôi cho thấy kết quả của các microbenchmarks. Hình 7 cho thấy độ trễ của các workload sau:

• W copy: Chuyển trọng số của một expert từ bộ nhớ CPU sang bộ nhớ GPU
• A copy: Chuyển một activation từ bộ nhớ GPU sang bộ nhớ CPU
• GPU N: Thực thi một expert tại GPU với kích thước đầu vào N (không bao gồm thời gian chuyển trọng số từ CPU)
• CPU N: Thực thi một expert tại CPU với kích thước đầu vào N

12

--- TRANG 13 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Cho mỗi giá trị, chúng tôi thực thi workload 32 lần (một lần cho mỗi lớp của Mixtral-8x7B) và trình bày kết quả trung bình và độ lệch chuẩn.

Khi các tác vụ được thực thi trên GPU, độ trễ để chuyển trọng số từ bộ nhớ CPU sang bộ nhớ GPU dài hơn khoảng 2-5 lần so với thời gian tính toán thực tế. Độ trễ tính toán trên GPU hầu như không đổi bất kể kích thước batch. Một ngoại lệ xảy ra trong Môi trường 1 khi kích thước batch là 1, vì PyTorch sử dụng các triển khai khác nhau cho các tình huống batch đơn và batch đa. Tuy nhiên, sự khác biệt này là nhỏ (khoảng 10%) so với độ trễ tổng thể, bao gồm chuyển trọng số. Do đó, chúng tôi mô hình hóa độ trễ GPU như một hằng số trong Phần 3.3.

Trên CPU, độ trễ thực thi thường tăng tuyến tính với kích thước của batch đầu vào. Tuy nhiên, thời gian cần thiết để chuyển activations là không đáng kể (ít hơn 1% độ trễ với một đầu vào duy nhất). Do tác động tối thiểu này, mô hình của chúng tôi trong Phần 3.3 giả định rằng độ trễ CPU có mối quan hệ tuyến tính với số lượng đầu vào.

Hình 7: Kết quả microbenchmark đo độ trễ của việc chuyển trọng số hoặc activations giữa CPU và GPU, cũng như thực thi một lớp expert trên CPU hoặc GPU với các kích thước đầu vào khác nhau. Trục y được hiển thị trên thang đo log.

B PHÂN TÍCH ĐỘ THƯA THỚT

Phần này phân tích độ thưa thớt trong các mô hình Mixtral-8x7B, làm nổi bật những thách thức của việc áp dụng các kỹ thuật tối ưu hóa dựa trên độ thưa thớt truyền thống từ các nghiên cứu trước đây (Song et al., 2023; Alizadeh et al., 2023). Những phương pháp này chủ yếu nhắm mục tiêu các LLMs sử dụng hàm kích hoạt ReLU, làm vô hiệu hóa các đầu vào âm và cho phép cắt tỉa các kênh với đầu ra liên tục bằng không. Phương pháp này tận dụng tính chất nhị phân của đầu ra ReLU—hoặc bằng không hoặc dương—cho phép xác định và loại bỏ trực tiếp các kênh không hoạt động, do đó tối ưu hóa hiệu quả tính toán mà không hy sinh thông tin quan trọng.

Ngược lại, các mô hình MoE tiên tiến thường sử dụng các hàm kích hoạt khác nhau, phức tạp hóa việc áp dụng trực tiếp các chiến lược khai thác độ thưa thớt này. Ví dụ, Mixtral-8x7B sử dụng SiLU như hàm kích hoạt. Không giống như ReLU, SiLU không cung cấp ngưỡng rõ ràng bằng không để cắt tỉa, đòi hỏi một phương pháp tinh vi hơn để tận dụng độ thưa thớt. Việc cắt tỉa các kênh không đủ gần với không có thể ảnh hưởng tiêu cực đến chất lượng đầu ra của mô hình.

Bảng 2 trình bày phân tích các giá trị tuyệt đối sau hàm SiLU trên các lớp của mô hình Mixtral-8x7B. Phân tích này dựa trên dữ liệu từ 100 mẫu trong bộ dữ liệu ShareGPT (ShareGPT), không phân biệt giữa các experts khác nhau trong các lớp giống hệt nhau. Dữ liệu chỉ ra sự xuất hiện thường thấp của các giá trị gần bằng không. Cụ thể, cho tất cả các lớp, tỷ lệ các kênh với giá trị tuyệt đối dưới 0.001 ít hơn 2%, và cho 30 trong số 32 lớp, con số này thậm chí dưới 1%. Ngoài ra, trong 28 trong số 32 lớp, ít hơn 5% giá trị nhỏ hơn 0.01, và trong 24 lớp, ít hơn 30% giá trị dưới 0.1. Mặc dù có sự thay đổi giữa các lớp, những kết quả này tập thể cho thấy một thách thức đáng kể trong việc khai thác độ thưa thớt trong mô hình này sử dụng các phương pháp từ công việc trước đây. Ngược lại, được báo cáo (Liu et al., 2023) rằng hơn 90% giá trị sau hàm ReLU bằng không cho các lớp MLP của mô hình OPT (Zhang et al., 2022). Việc sử dụng độ thưa thớt trong các mô hình như Mixtral-8x7B để tăng tốc suy luận với mất mát chất lượng có thể chấp nhận được vẫn là một hướng nghiên cứu thú vị cho tương lai.

13

--- TRANG 14 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Bảng 2: Phân bố giá trị tuyệt đối sau hàm SiLU của mô hình Mixtral-8x7B trên tất cả các lớp. Mỗi ô hiển thị tỷ lệ phần trăm giá trị có giá trị tuyệt đối dưới ngưỡng được chỉ định.

Lớp | <0.001 | <0.01 | <0.1 | <1.0
1 | 1.75 | 17.17 | 93.89 | 100.00
2 | 1.21 | 11.95 | 85.08 | 100.00
3 | 0.92 | 9.10 | 74.80 | 99.99
4 | 0.71 | 7.06 | 63.69 | 99.99
5 | 0.50 | 5.00 | 49.67 | 99.95
6 | 0.41 | 4.08 | 41.60 | 99.93
7 | 0.36 | 3.56 | 36.66 | 99.91
8 | 0.30 | 2.97 | 31.04 | 99.88
9 | 0.29 | 2.90 | 29.96 | 99.86
10 | 0.27 | 2.73 | 28.25 | 99.80
11 | 0.24 | 2.37 | 24.65 | 99.74
12 | 0.24 | 2.43 | 25.15 | 99.69
13 | 0.24 | 2.36 | 24.55 | 99.65
14 | 0.22 | 2.22 | 23.05 | 99.53
15 | 0.20 | 2.02 | 21.03 | 99.32
16 | 0.18 | 1.78 | 18.61 | 99.14
17 | 0.15 | 1.53 | 16.14 | 98.91
18 | 0.15 | 1.50 | 15.86 | 98.58
19 | 0.13 | 1.33 | 14.24 | 98.15
20 | 0.12 | 1.19 | 12.94 | 97.95
21 | 0.11 | 1.09 | 12.04 | 97.86
22 | 0.10 | 0.97 | 11.09 | 97.96
23 | 0.10 | 1.02 | 11.58 | 97.61
24 | 0.10 | 1.02 | 11.72 | 97.36
25 | 0.09 | 0.95 | 11.55 | 97.34
26 | 0.10 | 0.95 | 11.91 | 97.05
27 | 0.09 | 0.95 | 12.19 | 96.72
28 | 0.09 | 0.89 | 12.28 | 96.76
29 | 0.08 | 0.86 | 13.89 | 95.86
30 | 0.09 | 1.03 | 15.16 | 94.02
31 | 0.12 | 1.37 | 16.65 | 92.12
32 | 0.36 | 2.73 | 20.27 | 89.64

C MỨC ĐỘ PHỔ BIẾN CỦA EXPERT

Hình 8 hiển thị bản đồ nhiệt minh họa mức độ phổ biến của việc lựa chọn expert trong mô hình Mixtral-8x7B. Tương tự như phân tích trong Phụ lục B, profile này được tạo ra bằng cách chạy suy luận trên các mẫu ngẫu nhiên từ bộ dữ liệu ShareGPT và đếm số lượng token được định tuyến đến mỗi expert. Cường độ màu của mỗi ô đại diện cho tần suất lựa chọn expert, tương đương với số lượng token đã kích hoạt expert. Giá trị của expert phổ biến nhất được chuẩn hóa thành 1, với mức độ phổ biến của các experts khác được biểu thị dưới dạng tỷ lệ so với giá trị này.

Trong số 256 experts, giá trị trung bình là 0.71, với độ lệch chuẩn 0.08, phần trăm thứ 25 là 0.67, và phần trăm thứ 75 là 0.76. Mặc dù giá trị tối thiểu là 0.22, chỉ có 15 experts có giá trị dưới 0.6, và 27 experts vượt quá 0.8, cho thấy phân bố tương đối cân bằng.

14

--- TRANG 15 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Trong Môi trường 1, việc chọn 56 experts phổ biến nhất trong số 256 mang lại tỷ lệ hit dự kiến tối đa (khả năng trọng số của một expert có sẵn trong bộ nhớ GPU) là 25.2%, so với tối thiểu 18.7%. Lựa chọn ngẫu nhiên dẫn đến tỷ lệ hit trung bình 56/256 = 21.9%. Trong Môi trường 2, với dung lượng bộ nhớ GPU cho 125 experts, tỷ lệ hit dự kiến cho lựa chọn tốt nhất, tệ nhất và ngẫu nhiên lần lượt là 53.0%, 44.6%, và 48.8%. Do đó, chúng ta có thể kết luận rằng việc đặt các experts phổ biến trên GPU có thể cải thiện tỷ lệ hit khoảng 3 đến 5 điểm phần trăm so với việc đặt ngẫu nhiên.

Hình 8: Bản đồ nhiệt trực quan hóa tần suất lựa chọn expert trong mô hình Mixtral-8x7B, sử dụng cường độ màu để đại diện cho tần suất, với expert phổ biến nhất được chuẩn hóa thành 1.

D NGHIÊN CỨU ĐỘ NHẠY TRÊN BỘ DỮ LIỆU

Hình 9: So sánh hiệu suất end-to-end theo số lượng token được tạo ra mỗi giây (giống như tình huống a, cao hơn là tốt hơn), với hai bộ dữ liệu khác nhau. Bộ thanh bên phải nhất cho thấy trung bình của 15 cấu hình.

Trong phần này, chúng tôi phân tích độ nhạy của hiệu suất Fiddler trên các bộ dữ liệu đầu vào vì hành vi routing của các mô hình MoE có thể bị ảnh hưởng bởi đặc điểm của phân bố dữ liệu đầu vào. Hình 9 so sánh hiệu suất của Fiddler với các bộ dữ liệu ShareGPT (ShareGPT) và LMSYS-Chat-1M (Zheng et al., 2024), cả hai đều là bộ dữ liệu cuộc trò chuyện giữa con người và chatbot. Ngoài bộ dữ liệu, thiết lập thí nghiệm giống như tình huống a trong §4, và chúng tôi sử dụng Môi trường 1.

Trung bình, Fiddler vượt trội so với hệ thống tiên tiến (llama.cpp) 1.81 lần cho bộ dữ liệu ShareGPT và 1.56 lần cho bộ dữ liệu LMSYS. Những kết quả này cho thấy tính mạnh mẽ của Fiddler đối với các phân bố đầu vào khác nhau.

E TÍNH ÁP DỤNG CỦA Fiddler CHO CÁC MÔ HÌNH KHÁC NHAU

Trong §4, chúng tôi đánh giá mô hình Mixtral-8x7B vì nó là mô hình MoE duy nhất được hỗ trợ bởi tất cả các baselines. Tuy nhiên, hệ thống của chúng tôi được thiết kế để không phụ thuộc vào mô hình trong họ các mô hình MoE. Để chứng minh điều này, Hình 10 trình bày hiệu suất của Fiddler cho mô hình Phi-3.5-MoE (Abdin et al., 2024). Chúng tôi cho thấy so sánh với DeepSpeed-MII, vì nó là hệ thống baseline duy nhất hỗ trợ mô hình này.

15

--- TRANG 16 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Hình 10: So sánh hiệu suất end-to-end của mô hình Phi-3.5-MoE theo số lượng token được tạo ra mỗi giây (giống như tình huống a, cao hơn là tốt hơn.)

Kết quả nhất quán với mô hình Mixtral-8x7B, và Fiddler vượt trội so với DeepSpeed-MII trung bình 6.5 lần. Nó cho thấy tính ứng dụng của Fiddler ngoài mô hình Mixtral-8x7B.

F PHÂN TÍCH ĐỘ TRỄ

Hình 4 cho thấy hiệu suất được đo bằng số lượng token được tạo ra chia cho độ trễ end-to-end. Để bổ sung cho dữ liệu, Hình 11 và Hình 12 cho thấy Time To First Token (TTFT) và Inter-Token Latency (ITL) riêng biệt. Về TTFT, Fiddler cho thấy tăng tốc trung bình 1.13 lần trên các độ dài đầu vào khác nhau giữa tất cả baselines trong 2 môi trường. Về ITL, Fiddler cho thấy tăng tốc trung bình 1.43 lần trên các độ dài đầu vào và đầu ra khác nhau giữa tất cả baselines trong 2 môi trường.

Hình 11: So sánh Time-To-First-Token (TTFT) (thấp hơn là tốt hơn).

16

--- TRANG 17 ---
Đã xuất bản như một bài báo hội nghị tại ICLR 2025

Hình 12: So sánh Inter-Token Latency (ITL) (thấp hơn là tốt hơn).

17
