# Flash-LLM: Cho phép suy luận mô hình sinh lớn tiết kiệm chi phí và hiệu quả cao với độ thưa thớt không có cấu trúc
Haojun Xia∗†
Đại học Sydney
hxia6845@uni.sydney.edu.auZhen Zheng∗
Alibaba Group
james.zz@alibaba-inc.comYuchao Li
Alibaba Group
laiyin.lyc@alibaba-inc.com
Donglin Zhuang
Đại học Sydney
donglin.zhuang@sydney.edu.auZhongzhu Zhou
Đại học Sydney
zhongzhu.zhou@sydney.edu.auXiafei Qiu
Alibaba Group
xiafei.qiuxf@alibaba-inc.com
Yong Li
Alibaba Group
jiufeng.ly@alibaba-inc.comWei Lin
Alibaba Group
weilin.lw@alibaba-inc.comShuaiwen Leon Song
Đại học Sydney
shuaiwen.song@sydney.edu.au

## Tóm tắt
Với sự phát triển nhanh chóng về kích thước tham số, việc triển khai các mô hình sinh lớn trở nên ngày càng thách thức vì chúng thường yêu cầu tiêu thụ bộ nhớ GPU lớn và tính toán khổng lồ. Việc cắt tỉa mô hình không có cấu trúc đã là một phương pháp phổ biến để giảm cả dấu chân bộ nhớ GPU và tính toán tổng thể trong khi vẫn duy trì độ chính xác mô hình tốt. Tuy nhiên, các giải pháp hiện tại không cung cấp hỗ trợ hiệu quả cao cho việc xử lý độ thưa thớt không có cấu trúc trên GPU hiện đại, đặc biệt là trên phần cứng tensor core có cấu trúc cao. Do đó, chúng tôi đề xuất Flash-LLM để cho phép suy luận mô hình sinh lớn chi phí thấp và hiệu quả cao với hỗ trợ tinh vi cho độ thưa thớt không có cấu trúc trên tensor core hiệu suất cao nhưng hạn chế cao. Dựa trên quan sát chính của chúng tôi rằng nút thắt cổ chai chính của suy luận mô hình sinh là vài phép nhân ma trận mỏng mà tensor core sẽ bị sử dụng kém đáng kể do cường độ tính toán thấp, chúng tôi đề xuất phương pháp tổng quát Load-as-Sparse và Compute-as-Dense cho phép nhân ma trận thưa thớt không có cấu trúc (SpMM). Ý tưởng cơ bản là giải quyết nút thắt cổ chai băng thông bộ nhớ đáng kể trong khi chấp nhận các tính toán dư thừa không quan trọng đối với hiệu suất end-to-end trên tensor core. Dựa trên điều này, chúng tôi thiết kế một khung phần mềm hiệu quả cho SpMM không có cấu trúc dựa trên tensor core, tận dụng tài nguyên on-chip để trích xuất dữ liệu thưa thớt hiệu quả và chồng chéo tính toán/truy cập bộ nhớ. Các đánh giá mở rộng chứng minh rằng (1) ở mức kernel SpMM, Flash-LLM vượt trội đáng kể so với thư viện tiên tiến nhất, tức là Sputnik và SparTA lần lượt trung bình 2.9× và 1.5×. (2) Ở mức khung framework end-to-end trên các mô hình OPT-30B/66B/175B, đối với token trên GPU-giây, Flash-LLM đạt được cải thiện lên đến 3.8× và 3.6× so với DeepSpeed và FasterTransformer tương ứng, với chi phí suy luận thấp hơn đáng kể. Mã nguồn của Flash-LLM được công khai tại https://github.com/AlibabaResearch/flash-llm.

## 1 Giới thiệu
Các mô hình sinh đã chứng minh hiệu quả của chúng trên một loạt rộng các tác vụ ngôn ngữ và quản lý dữ liệu [3,34,45,52,53]. Tuy nhiên, với sự phát triển nhanh chóng của kích thước tham số (ví dụ GPT-2 [45] 1.5 tỷ tham số, GPT-3 [3] 175 tỷ, và Megatron-Turing NLG [50] 530 tỷ), việc triển khai hiệu quả các mô hình này trở nên ngày càng thách thức. Một mặt, trọng số của chúng có thể quá lớn để được đặt trên GPU. Ví dụ, mô hình GPT-3 yêu cầu 350GB bộ nhớ chỉ để lưu trữ các tham số với kiểu dữ liệu FP16, trong khi GPU NVIDIA A100 [36] chỉ có tối đa 80 GB bộ nhớ. Mặt khác, các mô hình sinh lớn thường gây ra độ trễ suy luận rất cao ngay cả khi sử dụng nhiều GPU vì cần lượng lớn tính toán và truy cập bộ nhớ.

Có ba đặc điểm cơ bản cho suy luận mô hình thực tế: độ chính xác, hiệu quả (tức là độ trễ và thông lượng), và chi phí (tức là mức tiêu thụ tài nguyên phần cứng). Phương pháp phổ biến để triển khai các mô hình lớn bằng cách phân chia trọng số mô hình onto nhiều thiết bị [49,63] có thể gặp phải chi phí cao và hiệu quả thấp. Một mặt, đối với các tình huống sản xuất trung tâm dữ liệu, việc sử dụng nhiều GPU cho một suy luận đơn lẻ của một mô hình đơn lẻ dẫn đến ROI (lợi tức đầu tư) thấp và có thể quá tốn kém trong thực tế. Mặt khác, phương pháp thông thường này yêu cầu thêm giao tiếp liên thiết bị, làm trầm trọng thêm vấn đề hiệu quả. Việc tải xuống và hoán đổi bộ nhớ GPU là một phương pháp khác để hỗ trợ trọng số lớn với bộ nhớ GPU hạn chế [1,48]. Tuy nhiên, các phương pháp tải xuống và hoán đổi thường dẫn đến độ trễ suy luận dài và do đó có thể không thực tế cho các dịch vụ suy luận trực tuyến.

Các phương pháp cắt tỉa trọng số [16] (sparsification) đã được chứng minh là hiệu quả trong việc giảm sử dụng bộ nhớ và tính toán cho suy luận mô hình trong khi vẫn duy trì độ chính xác tốt thông qua việc loại bỏ một phần các kết nối ít quan trọng trong mạng neural. Trong thực tế, việc cắt tỉa không có cấu trúc thường duy trì độ chính xác tốt hơn so với việc cắt tỉa có cấu trúc hạn chế hơn [8,12,14,16,28,51,54].

Tuy nhiên, việc hỗ trợ độ thưa thớt không có cấu trúc trên các kiến trúc GPU hiện đại một cách hiệu quả là khó khăn, đặc biệt là vì SpMM không có cấu trúc (SpMM) khó hỗ trợ trên kiến trúc tensor core hiệu suất cao nhưng có cấu trúc cao. Do đó, hướng thiết kế này đã bị bỏ qua phần lớn cho đến nay vì hiệu suất cao rất khó đạt được. Ví dụ, các triển khai SpMM không có cấu trúc tiên tiến nhất (ví dụ cuSPARSE [40], Sputnik [10]) thậm chí không thể vượt trội so với phương pháp dense tương ứng (cuBLAS [39]) cho đến khi độ thưa thớt mô hình cao hơn 98% và 86% tương ứng. Lưu ý rằng tensor core trên GPU hiện đại thường có thể đạt được hiệu suất đỉnh cao hơn gần một bậc độ lớn so với SIMT core.

Để giải quyết vấn đề quan trọng này làm nút thắt cổ chai hiệu suất và chi phí suy luận LLM, chúng tôi đề xuất Flash-LLM, một thư viện GPU hiệu quả để hỗ trợ độ thưa thớt không có cấu trúc trên tensor core cho suy luận mô hình sinh lớn. Với độ thưa thớt không có cấu trúc, Flash-LLM giải quyết vấn đề dấu chân bộ nhớ dẫn đến chi phí thấp hơn trong khi vẫn duy trì độ chính xác mô hình cao. Bằng cách có thể tận dụng hiệu suất đỉnh cao của tensor core, Flash-LLM đạt được độ trễ thấp hơn cho SpMM không có cấu trúc so với các giải pháp MatMul thưa thớt/dense hiện có. Ý tưởng thiết kế cấp cao của Flash-LLM là chiến lược Load-as-Sparse và Compute-as-Dense này. Chúng tôi thực hiện một quan sát quan trọng rằng các MatMul chính trong suy luận mô hình sinh thường rất mỏng. Hơn nữa, hiệu suất của các MatMul mỏng này bị ràng buộc bởi truy cập bộ nhớ toàn cục (hoặc băng thông bộ nhớ) hơn là khả năng tính toán của tensor core. Do điều này, chúng tôi đề xuất một phương pháp sáng tạo để hỗ trợ độ thưa thớt không có cấu trúc trên tensor core bằng cách tận dụng tải bộ nhớ thưa thớt để cải thiện băng thông bộ nhớ hạn chế trong khi hiệu quả chấp nhận tính toán tensor-core dư thừa (Mục 3.2).

Với ý tưởng trên, việc thực sự thiết kế và triển khai phương pháp Load-as-Sparse và Compute-as-Dense cấp cao này vẫn còn thách thức. Đầu tiên, nó yêu cầu một định dạng dữ liệu được thiết kế tốt cho việc lưu trữ và trích xuất dữ liệu thưa thớt hiệu quả. Việc trích xuất dữ liệu thưa thớt không đơn giản, đòi hỏi thiết kế tinh vi để tải và trích xuất dữ liệu thưa thớt với chi phí truy cập tối thiểu trong bộ nhớ GPU phân cấp với tài nguyên bộ nhớ on-chip hạn chế. Nó cũng đưa ra những thách thức mới trong việc thiết kế pipeline tính toán MatMul vượt ra ngoài các chiến lược MatMul dense thông thường. Trong Flash-LLM, chúng tôi đề xuất một định dạng thưa thớt mới gọi là (Tiled-CSL) để hỗ trợ việc thực thi SpMM theo từng tile với tensor core (Mục 4.3.1). Dựa trên Tiled-CSL, sau đó chúng tôi thiết kế phương pháp biến đổi sparse-to-dense một cách cẩn thận bằng cách sử dụng các thanh ghi phân tán và bộ đệm shared memory để trích xuất dữ liệu thưa thớt (Mục 4.1). Sau đó, một chiến lược chồng chéo hai cấp hiệu quả của bộ nhớ và tính toán được giới thiệu để điều phối việc biến đổi sparse-to-dense trên trọng số, việc tải dữ liệu feature map dense, và các hoạt động tensor core với một pipeline phần mềm đầy đủ (Mục 4.2). Cuối cùng, chúng tôi đề xuất một phương pháp sắp xếp lại dữ liệu thưa thớt trước thời gian để giảm thêm xung đột bank shared memory (Mục 4.3.3). Tóm lại, bài báo này đóng góp như sau:

• Chúng tôi đề xuất Flash-LLM, khung phần mềm hiệu quả chi phí và hiệu quả cao đầu tiên để hỗ trợ suy luận mô hình sinh lớn, mở ra phạm vi cho phép khám phá độ thưa thớt không có cấu trúc trên tensor core hiệu suất cao.

• Chúng tôi đề xuất một phương pháp tổng quát Load-as-Sparse và Compute-as-Dense để giảm dấu chân bộ nhớ và tăng hiệu quả của các MatMul mỏng chính bằng cách tận dụng ý tưởng giải quyết nút thắt cổ chai băng thông bộ nhớ chính trong suy luận LLM trong khi chấp nhận các tính toán tensor-core dư thừa.

• Chúng tôi đề xuất một thiết kế pipeline phần mềm hiệu quả để cho phép Flash-LLM bằng cách hiệu quả tận dụng định dạng thưa thớt mới của chúng tôi, việc biến đổi sparse-to-dense, và một chiến lược chồng chéo hai cấp.

• Flash-LLM được triển khai và tích hợp vào FasterTransformer để dễ sử dụng. Các kết quả đánh giá mở rộng đã cho thấy rằng (1) ở cấp kernel, Flash-LLM vượt trội so với các giải pháp tiên tiến nhất Sputnik và SparTA lần lượt trung bình 2.9× và 1.5×. (2) Ở cấp framework end-to-end trên các mô hình OPT-30B/66B/175B, đối với token trên GPU-giây, Flash-LLM đạt được cải thiện lên đến 3.8× và 3.6× so với DeepSpeed và FasterTransformer, với chi phí suy luận thấp hơn đáng kể.

## 2 Kiến thức nền

### 2.1 Suy luận mô hình sinh

**Quy trình suy luận của các mô hình sinh hiện đại.** Suy luận của các mô hình sinh hiện đại thường được tiến hành trong hai giai đoạn: xử lý prompt và sinh token. Như minh họa trong Hình 1a, mô hình sinh đầu tiên thực hiện xử lý prompt để xử lý chuỗi đầu vào của người dùng ('I love dogs') và sinh token mới đầu tiên ('and'). Sau đó mô hình chuyển sang giai đoạn sinh token tự hồi quy, trong đó token đầu ra đơn lẻ được sinh ra ở bước i-1 sẽ được lấy làm đầu vào để sinh token mới ở bước i một cách lặp lại.

Trong giai đoạn xử lý prompt, nhiều token trong chuỗi đầu vào sẽ được xử lý cùng lúc, dẫn đến các tensor đầu vào có hình dạng [B,L,H] (Hình 1a). B, L, và H biểu thị kích thước batch suy luận, độ dài chuỗi prompt, và chiều ẩn mô hình tương ứng. Trong khi đó, trong giai đoạn sinh token, chỉ token đơn lẻ được sinh ra trong lần lặp cuối sẽ được lấy làm đầu vào do đó tạo thành các tensor đầu vào có hình dạng [B,1,H]. Lưu ý rằng để ngăn chặn việc tính toán lại trên các vector K và V của các token trước đó, một bộ đệm bộ nhớ được phân bổ trước (được gọi là KV-Cache như trong Hình 1b) thường được sử dụng trong quá trình sinh token. Ở mỗi bước, một cặp KV mới được sinh ra (màu vàng) và được ghi vào KV-Cache.

**Điểm nóng hiệu suất suy luận của LLM.** Hình 2 minh họa kiến trúc decoder tiêu biểu của một lớp đơn trong các mô hình sinh dựa trên attention hiện đại. Có tổng cộng bốn phép nhân ma trận chính (hoặc MatMul) trong lớp decoder: QKV Projection, Output Projection, MLP1, và MLP2. Không giống như các mô hình ngôn ngữ phi sinh tập trung vào encoder trước đây (ví dụ, BERT [22]) mà nút thắt cổ chai hiệu suất chủ yếu ở tính toán multi-head attention, hiệu suất suy luận mô hình sinh bị ràng buộc nhiều bởi bốn MatMul này. Theo các thí nghiệm của chúng tôi trên suy luận mô hình OPT-66B [61], bốn MatMul này là những đóng góp hàng đầu cho độ trễ tổng thể chiếm 76.8% thời gian thực thi end-to-end và cũng là đóng góp hàng đầu cho việc tiêu thụ bộ nhớ chiếm 83.8% tổng sử dụng bộ nhớ.

### 2.2 Nhân ma trận trong suy luận LLM

**Nhân ma trận mỏng.** Các Matrix Multiply (MatMul) trong Hình 2 có thể được hình thức hóa thành C = A × B, trong đó A là ma trận trọng số có hình dạng [M,K] và B là ma trận feature map có hình dạng [K,N]. Trong bài báo này, chúng tôi gọi những MatMul này là "Skinny MatMul", vì chiều N của chúng nhỏ hơn nhiều so với chiều M và K.

**Sự khác biệt giữa Tensor/SIMT core.** SIMT core (hay CUDA core) là các đơn vị thực thi đa năng xử lý một loạt rộng lệnh để thực thi song song, trong khi tensor core [36,38] là các đơn vị chuyên biệt được thiết kế đặc biệt để tăng tốc tính toán MatMul dense. Tensor core cung cấp tăng tốc đáng kể cho MatMul dense, ví dụ thông lượng cao hơn 16× so với SIMT core trong GPU A100 với tích lũy FP32.

Các kỹ thuật thông thường tận dụng SIMT core cho MatMul thưa thớt không thể được áp dụng trực tiếp cho tensor core vì SIMT và tensor core hoạt động ở độ chi tiết rất khác nhau. SIMT core hoạt động ở độ chi tiết của các giá trị vô hướng, ví dụ lệnh FMA trên giá trị vô hướng. Độ chi tiết từng phần tử giúp dễ dàng bỏ qua tính toán ở cấp phần tử cho SpMM. Tuy nhiên, tensor core hoạt động ở độ chi tiết thô hơn nhiều so với SIMT core, ví dụ thực hiện phép nhân ma trận giữa hai ma trận có hình dạng 16×16 và 16×8 trong mỗi lệnh. Do đó, tensor core không cho phép bỏ qua các tính toán cấp phần tử tùy ý.

## 3 Cơ hội và hiểu biết

### 3.1 Độ thưa thớt không có cấu trúc trên Tensor Core

Có hai loại nguyên tắc cắt tỉa điển hình. Chiến lược cắt tỉa linh hoạt nhất (độ thưa thớt không có cấu trúc) là loại bỏ các phần tử ít quan trọng mà không xem xét phân phối của các phần tử bị cắt tỉa trong ma trận trọng số. Lấy ví dụ magnitude pruning, chúng ta có thể xếp hạng tất cả các phần tử trong ma trận dựa trên giá trị tuyệt đối của chúng và sau đó loại bỏ các trọng số có magnitude nhỏ nhất. Một chiến lược khác (độ thưa thớt có cấu trúc) là cắt tỉa các trọng số ít quan trọng, nhưng cùng lúc đó, một số tiêu chí cấu trúc phải được thực thi. Ví dụ, các ma trận trọng số có thể được chia thành các vector 8×1 [4,25] hoặc block 32×32 [13] không chồng chéo, và sau đó mỗi vector/block được giữ lại hoặc loại bỏ trong quá trình cắt tỉa.

Tóm lại, sự khác biệt chính giữa cắt tỉa có cấu trúc và không có cấu trúc là các ràng buộc bổ sung phải được thỏa mãn cho cắt tỉa có cấu trúc. Mặc dù độ thưa thớt có cấu trúc thân thiện với tăng tốc phần cứng, nó gặp phải suy giảm độ chính xác mô hình nghiêm trọng hơn [8,12,14,16,51,54] vì nó hạn chế tự do quyết định phần tử nào cần cắt tỉa. Như đã chỉ ra trong [28], so với độ thưa thớt có cấu trúc có 5% sụt giảm độ chính xác, độ thưa thớt không có cấu trúc chỉ dẫn đến 1% sụt giảm độ chính xác. Trong các thí nghiệm của chúng tôi, các mô hình giống OPT có thể bảo tồn độ chính xác rất tốt thông qua cắt tỉa không có cấu trúc dựa trên huấn luyện lại [15,29] ở độ thưa thớt 80% (ví dụ, độ chính xác chỉ giảm từ 85.55% xuống 84.11% cho OPT-30B).

Tuy nhiên, các kỹ thuật thông thường để hỗ trợ độ thưa thớt không có cấu trúc ngẫu nhiên trong việc thực thi SpMM không hiệu quả vì chúng tập trung vào tận dụng SIMT core mà không có cách tinh vi để sử dụng tensor core hiệu suất cao. Hình 3 cho thấy so sánh hiệu suất của các kỹ thuật khác nhau cho SpMM trên một tác vụ suy luận OPT-66B với kích thước batch 8. Lưu ý rằng việc cắt tỉa tiêu chuẩn cho suy luận LLM thường yêu cầu mức độ thưa thớt vừa phải (ví dụ 80%) để bảo tồn chất lượng mô hình trong khi giảm dấu chân bộ nhớ. CuSparse [40], thư viện SpMM NVIDIA, cho thấy hiệu suất kém vì nó chủ yếu được thiết kế cho các ứng dụng khoa học nơi ma trận cực kỳ thưa thớt (99%+). Sputnik [10], tối ưu hóa tập trung vào SIMT-core tiên tiến nhất cho SpMM không có cấu trúc trên các tác vụ DL vẫn không thể vượt trội so với cuBLAS (dense) cho đến khi đạt được độ thưa thớt cao.

Chúng tôi quan sát thấy rằng các kernel MatMul thưa thớt trong các thư viện thưa thớt hiện có thường chậm hơn so với phương pháp dense tương ứng (cuBLAS [39]). Lý do là cuBLAS đã tận dụng tensor core, trong khi các kernel MatMul thưa thớt đang tận dụng SIMT core trong các giải pháp tiên tiến nhất. Lưu ý rằng GPU A100 [36] có thể cung cấp thông lượng tính toán cao hơn 16× sử dụng tensor core so với sử dụng SIMT core cho MatMul dense. Mặc dù Sputnik có thể hiệu quả tận dụng SIMT core cho xử lý độ thưa thớt không có cấu trúc, hiệu suất của nó vẫn bị hạn chế bởi hiệu suất đỉnh của SIMT core.

Do sự chênh lệch hiệu suất đỉnh rõ ràng giữa SIMT và tensor core, có nhu cầu mạnh mẽ về hỗ trợ SpMM không có cấu trúc hiệu suất cao cho suy luận LLM. Tuy nhiên, việc cho phép SpMM không có cấu trúc hiệu suất cao trên tensor core hạn chế cao là không đơn giản vì tensor core không cho phép bỏ qua các tính toán cấp vô hướng tùy ý (được mô tả trong mục 2.2). Các công trình SpMM trước đây dựa trên ma trận thưa thớt có cấu trúc cao [4,13,18] (không phải độ thưa thớt không có cấu trúc ngẫu nhiên), hoặc cho tỷ lệ độ thưa thớt cực kỳ cao [57] (tức là >95%, không phù hợp cho độ chính xác suy luận LLM), thay vì độ thưa thớt không có cấu trúc ngẫu nhiên ở phạm vi tỷ lệ độ thưa thớt vừa phải cho độ chính xác cao. SparTA [65] tận dụng sparse tensor core [32] cho các tính toán chính. Tuy nhiên, nó không thể khai thác hiệu quả độ thưa thớt cao vì sparse tensor core chỉ hỗ trợ 50% độ thưa thớt (tức là độ thưa thớt 2:4). Như được chỉ ra trong Hình 3, hiệu suất của SparTA thấp hơn Flash-LLM đặc biệt khi độ thưa thớt tăng.

### 3.2 Cơ hội thiết kế

Với sự không khớp tự nhiên giữa tính toán SpMM không có cấu trúc và kiến trúc tensor core có cấu trúc cao, điều cần thiết là phải có một giải pháp SpMM hiệu quả cao trên tensor core theo đặc điểm khối lượng công việc của LLM hiện đại. Như đã thảo luận trong Mục 2.2, MatMul trong suy luận LLM hiện đại là mỏng. Do đó, nút thắt cổ chai của các tính toán MatMul mỏng là truy cập bộ nhớ off-chip và hạn chế băng thông, thay vì xử lý số học trên tensor core. Dựa trên quan sát này, ý tưởng cơ bản để giải quyết vấn đề này là thông qua phương pháp Load-as-Sparse và Compute-as-Dense. Cụ thể, kernel GPU tải các ma trận trọng số từ bộ nhớ toàn cục ở định dạng thưa thớt với kích thước giảm và tái tạo định dạng dense tương ứng trong bộ đệm on-chip tốc độ cao cho tính toán tensor core. Ý tưởng chính là nút thắt cổ chai cho suy luận LLM không ở phía tính toán do đó chúng ta có thể chấp nhận các tính toán dư thừa với tensor core. Chúng tôi mô tả một cách có hệ thống cách các giao dịch bộ nhớ off-chip trở thành nút thắt cổ chai hiệu suất trong Mục 3.2.1, và tại sao có thể chấp nhận tính toán dư thừa như vậy cho SpMM mỏng trong suy luận LLM trong Mục 3.2.2.

#### 3.2.1 Nút thắt cổ chai hiệu suất của MatMul mỏng trong suy luận LLM.

Chúng tôi phân tích nút thắt cổ chai hiệu suất của việc thực thi MatMul mỏng bắt đầu từ khối lượng công việc MatMul dense trong LLM. Theo kết quả profiling của chúng tôi cho OPT-66B [61], việc sử dụng trung bình của tensor core là khoảng 5.0%, 10.1%, 19.9%, và 39.7% dưới kích thước batch điển hình 8, 16, 32 và 64 như được chỉ ra trong Hình 4, trong khi băng thông của bộ nhớ toàn cục đã bị bão hòa hoàn toàn. Nguyên nhân cơ bản cho điều này là cường độ tính toán (tức là FLOP/Byte) của MatMul mỏng rất thấp. Đối với một MatMul được mô tả trong mục 2.2, tổng số phép toán được thực hiện là 2MNK phép toán dấu phẩy động (FLOP), và việc đọc dữ liệu tương ứng là 2(MK+KN) byte với kiểu dữ liệu FP16. Do đó, cường độ tính toán (CI) là:

CI = M×N/(M+N) (1)

Theo Phương trình 1, dễ dàng chứng minh rằng CI tổng thể của một MatMul có thể dễ dàng bị hạn chế bởi chiều M hoặc N nhỏ. Ví dụ, đối với một MatMul mỏng trong đó N là 16, CI sẽ có giới hạn trên nghiêm ngặt là 16 bất kể chiều M lớn đến mức nào. Lưu ý rằng trong các mô hình LLM sinh, chiều N bằng kích thước batch suy luận thường rất nhỏ trong môi trường sản xuất. Do đó, CI bị ràng buộc mạnh bởi chiều N trong suy luận LLM thời gian thực. Theo mô hình roofline [58], hiệu suất của một kernel với cường độ tính toán thấp sẽ dễ dàng bị ràng buộc bởi băng thông bộ nhớ.

#### 3.2.2 Load as Sparse, Compute as Dense

Với việc nút thắt cổ chai của MatMul mỏng đến từ truy cập bộ nhớ/băng thông bộ nhớ thay vì tính toán số học, chúng tôi đề xuất ý tưởng cơ bản của Load-as-Sparse và Compute-as-Dense ở đây để tận dụng sự tăng tốc hiệu suất từ việc giảm truy cập bộ nhớ trong khi cho phép sử dụng hiệu quả tensor core cho độ thưa thớt không có cấu trúc (tham khảo Mục 4 để biết chi tiết). Dưới ý tưởng cơ bản này, với tỷ lệ độ thưa thớt β (ma trận trọng số A thưa thớt trong khi ma trận feature map B dense), cường độ tính toán có thể được cải thiện thành:

CI_SparseLoad = M×N/(M×(1-β)+N) (2)

Hình 5 cho thấy CI và hiệu suất tensor core có thể đạt được tương ứng của một MatMul điển hình (M: 48k, N: BS, K: 12k) trong suy luận mô hình OPT-175B với các kích thước batch khác nhau. Theo hình, MatMul trong suy luận mô hình sinh với các kích thước batch khác nhau đều đối mặt với vấn đề memory wall. Do đó, các kernel MatMul dense chỉ có thể đạt được 5.1%, 10.3%, 20.5%, và 40.1% hiệu suất đỉnh của tensor core bị ràng buộc bởi băng thông bộ nhớ toàn cục không đủ. Những giá trị lý thuyết này phù hợp với các phép đo thực tế của chúng tôi trong Hình 4. Về lý thuyết, với phương pháp Load-as-Sparse và Compute-as-Dense dưới độ thưa thớt 40%, việc sử dụng tensor core có thể được cải thiện thành 8.5%, 17.1%, 34.2%, và 68.2%.

## 4 Phương pháp thiết kế

Flash-LLM tận dụng cả SIMT core và tensor core một cách hiệu quả cho tính toán SpMM không có cấu trúc hiệu quả. SIMT core linh hoạt được khai thác cho Sparse-to-Dense Transformation (tức là Load-as-Sparse) trong khi tensor core được sử dụng cho tính toán tensor tính toán-thâm dụng (tức là Compute-as-Dense). Chúng tôi sẽ đưa ra tổng quan về các tối ưu hóa cấp cao của Flash-LLM trong Mục 4.1. Sau đó chúng tôi sẽ mô tả thiết kế pipeline tính toán của Flash-LLM trong Mục 4.2. Chúng tôi minh họa định dạng thưa thớt mới (định dạng tiled-CSL) và các kỹ thuật truy cập bộ nhớ hiệu quả trong Mục 4.3. Cuối cùng, chúng tôi mô tả hệ thống suy luận end-to-end được hỗ trợ bởi Flash-LLM của chúng tôi trong Mục 5.

### 4.1 Tổng quan thiết kế

Chúng tôi sử dụng phương pháp dựa trên tiling cho các tính toán SpMM trong Flash-LLM. Hình 6a cho thấy phương pháp tiling của Flash-LLM, trong đó mỗi thread block (TB) chịu trách nhiệm tính toán một tile (ví dụ, tile màu xanh lá có hình dạng M_TB * N_TB) trong ma trận đầu ra C. Đối với mỗi lần lặp, mỗi thread block tải A_Tile (hình dạng [M_TB, K_TB]) ở dạng thưa thớt và B_Tile (hình dạng [K_TB, N_TB]) ở dạng dense từ bộ nhớ toàn cục. A_Tile sau đó được biến đổi thành định dạng dense với Sparse-to-Dense Transformation được hiển thị trong Hình 6b và được lưu trữ trong shared memory trong khi B_Tile được lưu trữ trực tiếp trong shared memory. Cuối cùng, mỗi thread block tiêu thụ dữ liệu dense trong shared memory và sinh ra tile đầu ra thông qua tính toán tensor core.

Hình 6b cho thấy hành vi kernel tổng thể của Flash-LLM từ khía cạnh kiến trúc vi mô. Shared memory được sử dụng như không gian làm việc để lưu trữ dữ liệu được trích xuất từ định dạng thưa thớt sang dense, nơi tất cả các thread trong thread block làm việc cùng nhau một cách hợp tác để tải sparse encoding (SE) của A_Tile từ bộ nhớ toàn cục và lưu trữ chúng trong shared memory với định dạng dense. Lưu ý rằng mỗi tile trên bộ nhớ toàn cục được mã hóa thành sparse encoding với ít byte hơn so với định dạng dense, do đó các chuyển động dữ liệu bộ nhớ toàn cục tương ứng có thể được giảm so với MatMul dense. Cụ thể, ý tưởng cơ bản của Sparse-to-Dense Transformation là trích xuất các phần tử khác không từ sparse encoding trên bộ nhớ toàn cục đến các vị trí tương ứng của chúng trong định dạng dense trên shared memory trong khi các vị trí khác được điền bằng số không. Chúng tôi sử dụng các thanh ghi phân tán như bộ đệm trung gian để lưu trữ các phần tử khác không trước khi trích xuất chúng vào shared memory. Chúng tôi không sử dụng shared memory như bộ đệm trung gian này để tránh truy cập shared memory quay vòng của sparse encoding.

### 4.2 Thiết kế pipeline tính toán của Flash-LLM

Với việc mỗi thread/thread-block tiêu thụ một phần lớn của tổng số thanh ghi/shared-memory như bộ đệm cho tiling, song song cấp thread GPU (TLP) vốn dĩ thấp. Do đó, việc tối ưu hóa song song cấp lệnh là quan trọng. Chúng tôi mô tả pipeline phần mềm của Flash-LLM trong mục này nơi việc tải bộ nhớ off-chip (Sparse-to-Dense Transformation) và tính toán tensor core được xử lý theo cách pipeline một cách hiệu quả.

#### 4.2.1 Chồng chéo hai cấp của Bộ nhớ và Tính toán.

Như được mô tả trong Mục 4.1, nó yêu cầu nhiều giai đoạn để tải sparse encoding từ bộ nhớ toàn cục đến shared memory ở định dạng dense cho mỗi A_Tile. Cụ thể, nó yêu cầu tải sparse encoding từ bộ nhớ toàn cục đến các thanh ghi phân tán (giai đoạn gmem2reg), đặt lại bộ đệm shared memory đích bằng không (giai đoạn rst_smem), và ghi sparse encoding từ thanh ghi đến các vị trí tương ứng trên bộ đệm shared memory (giai đoạn extract). Đối với B_Tile, đã ở định dạng dense, nó chỉ yêu cầu tải trực tiếp từ bộ nhớ toàn cục đến bộ đệm shared memory đích (giai đoạn ld_dense). Cuối cùng, giai đoạn smem2tc tải dữ liệu shared memory của A_Tile và B_Tile và thực thi tính toán tensor core.

Như được hiển thị trong Hình 6c, Flash-LLM khai thác chồng chéo hai cấp của các giai đoạn bộ nhớ và tính toán trên để thực thi hiệu quả. Một mặt, nó tận dụng pipeline phần mềm thông qua double buffering để chồng chéo tải bộ nhớ off-chip và Sparse-to-Dense transformation với tính toán tensor core, được gọi là chồng chéo liên lần lặp. Mặt khác, nó chồng chéo các giai đoạn tải bộ nhớ off-chip trong Sparse-to-Dense transformation để có hoạt động bộ nhớ hiệu quả hơn, được gọi là chồng chéo nội lần lặp. Trục ngang của Hình 6c đại diện cho thời gian thực thi trong khi trục dọc đại diện cho các hoạt động với double buffer. Nó sử dụng hai bộ đệm shared memory cho A_Tile (tương ứng với A1 và A2) và B_Tile (tương ứng với B1 và B2), và một bộ đệm thanh ghi được tái sử dụng trong các lần lặp khác nhau (tương ứng với SE). Cụ thể, SE trong Iteration-1/3 (Iteration-2/4) và A1 (A2) tương ứng với quá trình Sparse-to-Dense transformation của A_Tile trên tập bộ đệm đầu tiên (thứ hai), và B1 (B2) tương ứng với chuyển động dữ liệu của B_Tile trên tập bộ đệm đầu tiên (thứ hai). Đối với chồng chéo liên lần lặp, như được hiển thị trong Iteration-2 trong Hình 6c, trong khi tải dữ liệu từ shared memory và thực thi tính toán tensor core cho dữ liệu trên tập bộ đệm đầu tiên, Flash-LLM tải và trích xuất dữ liệu từ bộ nhớ toàn cục đến shared memory cho tập bộ đệm thứ hai. Đối với chồng chéo nội lần lặp, các hoạt động của A1 và B1 được xử lý song song, và các giai đoạn gmem2reg và rst_smem trên A_Tile cũng được xử lý song song. Theo cách này, việc tải dữ liệu thưa thớt, tải dữ liệu dense, và tính toán tensor core có thể được chồng chéo một cách hiệu quả.

Một thiết kế quan trọng cho Sparse-to-Dense transformation là việc sử dụng rõ ràng thanh ghi như bộ đệm dữ liệu giữa bộ nhớ toàn cục và shared memory. Trong Flash-LLM, việc chuyển động sparse encoding từ bộ nhớ toàn cục đến shared memory được chia rõ ràng thành hai giai đoạn, tức là lệnh LDG (tải dữ liệu từ bộ nhớ toàn cục đến thanh ghi) trong gmem2reg và lệnh STS (lưu trữ dữ liệu đến shared memory từ thanh ghi) trong extract như được hiển thị trong Hình 6c. Một mặt, thiết kế chia hai giai đoạn giúp tăng song song cấp lệnh (ILP) để ẩn độ trễ truy cập bộ nhớ toàn cục cao. Lưu ý rằng mỗi cặp lệnh LDG và STS có phụ thuộc load-use. Nếu chúng ta không chia gmem2reg và extract thành hai giai đoạn mà khởi chạy mỗi cặp lệnh LDG và STS trong các chu kỳ liền kề (ví dụ lưu trữ trực tiếp vào shared memory sau khi tải từ bộ nhớ toàn cục), mỗi GPU thread sẽ thực thi lệnh theo thứ tự LDG_0, STS_0, LDG_1, STS_1, ... mà không có ILP của global memory load (tức là LDG). Bằng cách chia hai lệnh thành hai giai đoạn như trong Hình 6c, thứ tự thực thi sẽ là LDG_0, LDG_1, ..., STS_0, STS_1, ... và dẫn đến ILP cao của global memory load. Mặt khác, việc chia chuyển động dữ liệu thành gmem2reg và extract cho phép cơ hội chồng chéo giữa gmem2reg và rst_smem. Lưu ý rằng các lệnh STS trong extract không nên được khởi chạy trước khi hoàn thành giai đoạn rst_smem, nếu không, dữ liệu được ghi bởi lệnh STS có thể bị ghi đè bởi rst_smem một cách không chính xác. Việc thực thi gmem2reg và rst_smem có thể được chồng chéo một khi gmem2reg không chứa ghi shared memory (tất cả lệnh STS được gán cho giai đoạn extract), điều này càng tăng ILP.

#### 4.2.2 Phạm vi tối thiểu của Đồng bộ hóa và Rào cản Bộ nhớ

Với pipeline phức tạp trong Hình 6c, nó yêu cầu một tập hợp đồng bộ hóa thread và rào cản bộ nhớ để đảm bảo tính đúng đắn. Flash-LLM chèn phạm vi tối thiểu của đồng bộ hóa và rào cản bộ nhớ để đảm bảo tính đúng đắn trong khi duy trì chồng chéo.

Để tránh dữ liệu được ghi bởi extract bị ghi đè bởi rst_smem một cách không chính xác, Flash-LLM chèn đồng bộ hóa cấp thread-block rõ ràng giữa hai giai đoạn để đảm bảo rằng tất cả các thread đã hoàn thành công việc đặt lại bộ đệm A1/A2 trong shared memory, được hiển thị như các đường vàng đầu tiên trong mỗi lần lặp trong Hình 6c. Trong khi đó, nó cũng yêu cầu một đồng bộ hóa khác để đảm bảo rằng tất cả chuyển động dữ liệu và hoạt động tensor core của lần lặp hiện tại được hoàn thành trước khi bắt đầu lần lặp tiếp theo, được hiển thị như các đường vàng thứ hai trong mỗi lần lặp trong Hình 6c. Lấy Iteration-1 làm ví dụ, chúng ta nên đảm bảo rằng tất cả các thread đã hoàn thành việc ghi bộ đệm shared memory A1 và B1 trước khi bắt đầu Iteration-2, vì dữ liệu trên bộ đệm shared memory sẽ được tải vào tensor core như đầu vào trong Iteration-2. Bên cạnh đó, chúng ta phải đảm bảo rằng tất cả các thread đã hoàn thành việc đọc dữ liệu từ bộ đệm thanh ghi cho extract trong Iteration-1 trước khi để chúng bị ghi đè bởi rst_smem trong Iteration-2.

Ngoài đồng bộ hóa thread-block, nó cũng yêu cầu rào cản bộ nhớ sau các hoạt động sao chép bất đồng bộ của chuyển động dữ liệu global-to-shared. Flash-LLM sử dụng các primitive sao chép bất đồng bộ trên GPU cho việc chồng chéo chuyển động dữ liệu và các hoạt động khác. Lưu ý rằng primitive sao chép bất đồng bộ cp.async, bắt đầu từ GPU NVIDIA Ampere [36], cho phép chuyển dữ liệu từ bộ nhớ toàn cục đến shared memory trong nền một cách bất đồng bộ trong khi thực thi các tính toán khác ở tiền cảnh. Cụ thể, cả giai đoạn rst_smem và ld_dense đều sử dụng primitive cp.async để cho phép thực thi chồng chéo trên double buffer.

Để cho phép thực thi pipeline chi tiết, Flash-LLM sử dụng các rào cản async-copy khác nhau cho các giai đoạn rst_smem và ld_dense. Như được hiển thị trong Hình 6c, giai đoạn extract chờ hoàn thành chỉ rst_smem, trong khi rào cản thread-block cuối cùng của mỗi lần lặp chờ hoàn thành tất cả các hoạt động cp.async trước đó. Theo cách này, giai đoạn extract có thể được chồng chéo với các giai đoạn ld_dense.

#### 4.2.3 Triển khai tổng thể

Thuật toán 1 cho thấy các triển khai của tính toán pipeline trong Flash-LLM. Ở dòng 3, pipeline phần mềm sẽ được khởi tạo, chuẩn bị dữ liệu của A_Tile và B_Tile trên shared memory cho tính toán tensor core của lần lặp đầu tiên trong vòng lặp chính. Các lần lặp tiếp theo được mô tả trong Hình 6c được triển khai trong dòng 7-28. Đối với mỗi lần lặp, nó phát hành các lệnh cho việc tải dữ liệu bất đồng bộ cho lần lặp tiếp theo và thực hiện tính toán tensor core của lần lặp hiện tại theo cách double buffer. Cụ thể, một A_Tile cho lần lặp tiếp theo sẽ được tải và trích xuất từ bộ nhớ toàn cục đến shared memory (rst_smem, gmem2reg, và extract), và một B_Tile dense sẽ được tải trực tiếp từ bộ nhớ toàn cục (ld_dense). Giai đoạn rst_smem ở dòng 17, nơi mỗi thread phát hành hoạt động cp.async để đặt buffer A thành không. Ở dòng 19, gmem2reg được hoàn thành, nơi sparse encoding được tải từ bộ nhớ toàn cục đến các thanh ghi phân tán. Giai đoạn ld_dense ở dòng 20, nơi dữ liệu cho B_Tile được tải từ bộ nhớ toàn cục đến bộ đệm shared memory với các hoạt động cp.async. Sau khi khởi chạy các hoạt động bộ nhớ bất đồng bộ này, các hoạt động tensor core được khởi chạy ở dòng 23. Lưu ý rằng chúng tôi tải ma trận dense từ shared memory đến thanh ghi sử dụng lệnh ldmatrix.sync và sử dụng tensor core cho các tính toán lõi bằng cách khởi chạy rõ ràng lệnh mma.sync trong hàm Pipelined_Shared2Reg_TensorCoreOps().

Rào cản async-copy đầu tiên trong Hình 6c ở dòng 25, đảm bảo rằng tất cả các hoạt động bất đồng bộ được khởi chạy ở dòng 17 được hoàn thành trong khi các hoạt động được khởi chạy ở dòng 20 vẫn có thể đang tiến hành. Giai đoạn extract ở dòng 26, nơi dữ liệu trên các thanh ghi phân tán được trích xuất vào bộ đệm shared memory của A_Tile. Cuối cùng, các rào cản async-copy và thread-block được gọi ở dòng 28 để đảm bảo rằng tất cả các thread đã hoàn thành công việc của chúng trong lần lặp này.

Khác với MatMul dense nơi kích thước dữ liệu cần được tải từ bộ nhớ toàn cục có thể được suy ra bởi kích thước tile, kích thước của sparse encoding được xác định bởi số lượng phần tử khác không (nnz) trong A_Tile, điều này không thể dự đoán được. Trước khi tải sparse encoding cho mỗi tile, Flash-LLM xác định offset bắt đầu và độ dài trong bộ đệm bộ nhớ toàn cục. Thông tin như vậy được duy trì trong mảng TileOffsets, được lưu trữ trong bộ nhớ toàn cục (chi tiết hơn trong Mục 4.3). Để tránh việc dừng lệnh do truy cập bộ nhớ toàn cục độ trễ dài, metadata này nên được pre-fetch. Ở dòng 5-6, offset bắt đầu và kích thước của sparse encoding cho lần lặp đầu tiên được pre-fetch. Ở đầu mỗi lần lặp, offset bắt đầu và kích thước của A_Tile hiện tại được cập nhật sử dụng giá trị trong thanh ghi được pre-fetch trước. Ở dòng 11-12, nó pre-fetch offset bắt đầu và độ dài cho lần lặp tiếp theo.

### 4.3 Mã hóa thưa thớt và Phân tích thời gian chạy

#### 4.3.1 Định dạng Tiled-CSL

Định dạng sparse encoding của ma trận A là cần thiết cho việc lưu trữ dữ liệu thưa thớt hiệu quả và Sparse-to-Dense Transformation. Chúng tôi đề xuất định dạng sparse encoding từng tile để hỗ trợ các tối ưu hóa trong Mục 4.2 một cách hiệu quả. Các phần tử khác không được tổ chức từng tile, nơi mỗi tile duy trì các phần tử khác không của nó kèm theo chỉ số thưa thớt. Như được hiển thị trong Hình 7, dữ liệu của mỗi tile trong ma trận thưa thớt được mã hóa thành một mảng nhỏ, và việc kết hợp tất cả các tile sẽ tạo thành mảng tổng thể (NonZeros Array). TileOffsets Array duy trì offset bắt đầu của mỗi tile trong NonZeros Array. Số lượng phần tử khác không của mỗi tile Tiled-CSL là sự khác biệt giữa hai phần tử tương ứng trong TileOffsets Array. Đối với mỗi tile trong NonZeros Array, mỗi phần tử được lưu trữ cùng với vị trí của nó trong tile. Như được hiển thị trong Hình 7, mỗi trọng số khác không ở độ chính xác half 16-bit và mỗi vị trí được mã hóa thành một số nguyên short 16-bit.

#### 4.3.2 Trích xuất dữ liệu từ Register đến Shared Memory dựa trên định dạng Tiled-CSL

Như được mô tả trong Thuật toán 2, mỗi thread trích xuất nnz_thread giá trị khác không từ bộ đệm sparse encoding Reg[] của nó đến bộ đệm shared memory A với một vòng lặp. Các hàm v() và idx() được sử dụng để trích xuất giá trị (16 bit cao) và vị trí của nó (16 bit thấp). Có một số cân nhắc đặc biệt khi sử dụng thanh ghi như bộ đệm trung gian cho sparse encoding. Khác với shared memory và global memory, thanh ghi GPU không thể địa chỉ hóa, có nghĩa là chúng ta không thể truy cập một mảng thanh ghi sử dụng một offset biến. Do đó, việc buộc một mảng được định nghĩa trong CUDA vào thanh ghi yêu cầu rằng, tất cả các chỉ số được sử dụng để truy cập mảng có thể được xác định tĩnh tại thời gian biên dịch. Nếu không, mảng sẽ được lưu trữ trong bộ nhớ toàn cục thay thế. Ở dòng 1 trong Thuật toán 2, #pragma unroll được sử dụng để thông báo cho trình biên dịch GPU hoàn toàn unroll vòng lặp chính, để tất cả các chỉ số được sử dụng để truy cập Reg[] có thể được xác định tĩnh. Lưu ý rằng việc thêm chỉ thị biên dịch như vậy một mình là không đủ vì một vòng lặp với số lần lặp biến không thể được hoàn toàn unroll. Do đó, chúng tôi sử dụng một giá trị hằng số #REG (giới hạn trên của số lần lặp, thường là 32/64 trong triển khai của chúng tôi) ở dòng 2 thay vì sử dụng giá trị biến nnz_thread.

#### 4.3.3 Sắp xếp lại dữ liệu thưa thớt trước thời gian

Có hai loại truy cập shared memory trong pipeline tính toán cho ma trận trọng số thưa thớt A. Một là shared memory load cho tính toán tensor core trong giai đoạn smem2tc. Cái khác là shared memory store trong giai đoạn extract. Điều cần thiết là tránh xung đột bank để có hiệu suất tốt. Tuy nhiên, độ thưa thớt ngẫu nhiên làm cho việc tránh xung đột bank của cả shared memory load và store trở nên thách thức.

Đối với giai đoạn smem2tc, nó sử dụng intrinsic ldmatrix để tải dữ liệu hiệu quả từ shared memory đến thanh ghi cho tính toán tensor core. Hình 8a cho thấy hành vi của ldmatrix nơi tám thread tập thể tải một ma trận 8×8 trong FP16 từ shared memory. Việc đọc ma trận 8×8 này có thể được phục vụ bởi một wavefront shared memory đơn lẻ nếu không có xung đột bank. Việc tải bộ nhớ không xung đột bank của ldmatrix yêu cầu rằng tất cả các scalar trong ma trận 8×8 có thể được đọc từ các bank bộ nhớ riêng biệt. Hình 8a cho thấy một ví dụ về bố cục dữ liệu shared memory minh họa việc gán bank (ID bank từ 1 đến 32) để đạt được ldmatrix không xung đột bank. Do đó, mỗi scalar có thể được gán một ID bank theo vị trí của nó trong A_Tile, với một bố cục dữ liệu cụ thể.

Tuy nhiên, yêu cầu này sẽ dễ dàng gây ra xung đột bank của shared memory store trong giai đoạn extract do vị trí ngẫu nhiên của các phần tử thưa thớt trong ma trận A. Nói cách khác, chúng ta có thể đảm bảo load không xung đột bank theo yêu cầu bố cục của ldmatrix, nhưng không thể tránh xung đột bank store theo cách này trong giai đoạn extract. Hình 8b đưa ra một ví dụ, nơi mỗi giá trị khác không (NonZero) nên được lưu trữ trong một bank shared memory đích (SMemBank) theo vị trí tương đối của chúng trong A_Tile để đáp ứng yêu cầu ldmatrix không xung đột bank. Vì phân phối của NonZeros là ngẫu nhiên, SMemBank đích cho mỗi giá trị NonZero cũng ngẫu nhiên. Do đó trong Hình 8b, tất cả CUDA WARP (các NonZeros có cùng màu được xử lý bởi cùng WARP) gặp phải xung đột bank và dẫn đến nhiều wavefront shared memory (SMem WF).

Để giảm xung đột bank, chúng tôi đề xuất phương pháp sắp xếp lại dữ liệu thưa thớt trước thời gian. Ý tưởng cơ bản là ldmatrix không xung đột bank đã xác định bank đích của mỗi phần tử dữ liệu, do đó chúng ta có thể sắp xếp lại các phần tử dữ liệu trong mỗi tile Tiled-CSL để các phần tử tương ứng với các bank khác nhau có thể được tổ chức thành cùng WARP để thực thi extract. Cụ thể, nó lặp lại chọn phần tử thưa thớt tương ứng với các bank bộ nhớ khác nhau khi sinh mảng con NonZeros cho mỗi tile Tiled-CSL. Hình 8c cho thấy trường hợp lý tưởng nơi chỉ cần một wavefront shared memory để phục vụ một WARP thực thi giai đoạn extract sau khi sắp xếp lại dữ liệu. Lưu ý rằng việc sắp xếp lại dữ liệu nằm trong phạm vi tile Tiled-CSL, chỉ thay đổi phân phối dữ liệu trên bộ nhớ toàn cục nhưng không thay đổi trên bộ đệm shared memory. Nói cách khác, đó là một cách để thay đổi việc đặt dữ liệu trên bộ nhớ toàn cục để đạt được truy cập shared memory hiệu quả hơn.

Thuật toán 3 cho thấy thuật toán để sinh định dạng Tiled-CSL từ ma trận thưa thớt gốc theo phương pháp sắp xếp lại dữ liệu thưa thớt trước thời gian. Đầu vào là ma trận thưa thớt A với M hàng và K cột ở định dạng dense, nơi một số phần tử đã được đặt thành 0 thông qua cắt tỉa mô hình. Đầu ra là NonZeros và TileOffsets, các thành phần chính của định dạng Tiled-CSL. NonZeros được chia thành các nhóm mỗi nhóm chứa 32 phần tử khác không ở dòng 2. Mỗi nhóm phần tử khác không sẽ được ghi vào shared memory trong giai đoạn extract trong một yêu cầu shared memory. Ở dòng 7-24, định dạng Tiled-CSL của một tile (128×64) sẽ được sinh ra. Ở dòng 11-14, mỗi phần tử khác không trong ma trận A sẽ được mã hóa thành một từ 32-bit chứa (val,loc). Bên cạnh đó, các phần tử khác không được phân phối vào 32 bucket khác nhau (NZ_Bucket[32] ở dòng 4) theo ID bank shared memory đích của chúng từ 0 đến 31 như được tính toán ở dòng 13. Ở dòng 16, tổng số phần tử khác không (NNZ) được đếm. Ở dòng 19-22, một nhóm phần tử khác không được hình thành bằng cách lặp lại chọn phần tử khác không từ NZ_Bucket[id] nơi NZ_Bucket[id] là bucket có nhiều phần tử khác không chưa được xử lý nhất tại thời điểm đó.

## 5 Triển khai

Chúng tôi cung cấp một tập hợp API C++ cho kernel Flash-LLM hiệu suất cao. Chúng tôi tích hợp kernel Flash-LLM vào FasterTransformer [37], cho phép suy luận phân tán hiệu quả cao với ma trận trọng số đã sparsified. Cụ thể, chúng tôi mở rộng định nghĩa lớp tương ứng (tức là lớp DenseWeight) để hỗ trợ định dạng Tiled-CSL. Bên cạnh đó, chúng tôi mở rộng wrapper thư viện của nó (tức là lớp cuBlasMMWrapper) để hỗ trợ gọi thư viện MatMul dense hoặc kernel SpMM Flash-LLM theo định dạng dữ liệu đã cho. Flash-LLM cũng có thể được tích hợp dễ dàng vào các framework deep learning khác thông qua các lệnh gọi thư viện với API Flash-LLM. Chúng tôi cũng cung cấp một công cụ định dạng lại trọng số để sinh ma trận thưa thớt ở định dạng Tiled-CSL với mô hình PyTorch dense đã huấn luyện trước.

Trong triển khai của chúng tôi, kích thước của M_TB trong Hình 6a là 128 hoặc 256, K_TB là 64, và N_TB là 8/16/32/32 khi chiều N của MatMul (kích thước batch suy luận) là 8/16/32/64. Đối với chiều N lớn hơn, N_TB là 64. Kích thước thread block là 128. Những cấu hình này hoạt động tốt cho các khối lượng công việc chúng tôi đánh giá trong Mục 6. Các cấu hình có thể được cấu hình lại dễ dàng cho các khối lượng công việc khác. Việc điều chỉnh cấu hình không nằm trong phạm vi nghiên cứu của bài báo này.

## 6 Đánh giá

Chúng tôi đánh giá hiệu suất của Flash-LLM ở hai cấp: benchmarking cấp kernel và đánh giá cấp mô hình. Đánh giá được thực hiện trên nền tảng NVIDIA A100-SMX8-80GB (CPU Intel Xeon Platinum 8369B 128-core @2.90GHz, 8 GPU NVIDIA A100 @80GB), với Ubuntu 18.04 và CUDA 11.8. Chúng tôi bật auto-mixed precision (AMP) cho tất cả các đánh giá. Chúng tôi chủ yếu thực hiện thí nghiệm trên GPU NVIDIA A100. Tuy nhiên, phương pháp chúng tôi đề xuất cũng là một tham khảo tốt cho thiết kế kernel của TPU và CPU Intel được trang bị phần cứng tùy chỉnh cho phép nhân ma trận.

### 6.1 Hiệu suất Kernel

**Khối lượng công việc, baseline, và cài đặt.** Chúng tôi đánh giá Flash-LLM trên MatMul dưới các hình dạng khác nhau, đến từ bốn MatMul được mô tả trong Mục 2.1 trong OPT-30B, OPT-66B, và OPT-175B [61] với bốn kích thước batch khác nhau (8, 16, 32, và 64). Đối với mỗi hình dạng MatMul, chúng tôi đánh giá độ trễ kernel dưới 70%, 80%, và 90% độ thưa thớt ngẫu nhiên trong ma trận trọng số. Các baseline chúng tôi so sánh bao gồm cuSPARSE [40], Sputnik (commit: 46e380c) [10,11], SparTA (commit: 1f61a36) [64,65], và cuBLAS [39]. CuSPARSE là một phần của CUDA Toolkit để xử lý ma trận thưa thớt. Sputnik là một thư viện kernel đại số tuyến tính thưa thớt cho deep learning, đạt được hiệu suất SpMM tiên tiến nhất dựa trên SIMT core. SparTA hỗ trợ độ thưa thớt không có cấu trúc dựa trên độ thưa thớt có cấu trúc 2:4 trên tensor core [32]. Vì SparTA chỉ hỗ trợ độ chính xác FP32, chúng tôi mở rộng SparTA để hỗ trợ ma trận đầu vào trong FP16. CuBLAS nhắm đến MatMul dense thay vì SpMM. Chúng tôi bao gồm nó như một baseline ở đây để hiển thị lợi ích/tổn thất hiệu suất thực tế so với các triển khai dense cơ bản của suy luận LLM.

**Kết quả.** Hình 9 cho thấy hiệu suất kernel (TFLOPs) của Flash-LLM và các baseline. Thông lượng có thể được tính bởi 2×M×K×N/kernel_latency. Như được hiển thị trong Hình 9, Flash-LLM thực hiện tốt nhất liên tục so với các baseline. Trung bình, Flash-LLM vượt trội so với Sputnik/SparTA lần lượt 3.6×/1.4×, 3.0×/1.4×, và 2.0×/1.6× dưới độ thưa thớt 70%, 80%, và 90% tương ứng. Bên cạnh đó, Flash-LLM cũng có thể vượt trội so với các kernel dense tiên tiến nhất cuBLAS với tensor core được bật lần lượt 1.4×, 1.7×, và 2.1×. CuSPARSE cho thấy hiệu suất kém dưới mức độ thưa thớt vừa phải như vậy, vì nó được thiết kế cho ma trận với >95% độ thưa thớt [40]. Đối với Sputnik, việc vượt trội so với kernel cuBLAS với tensor core được bật là rất thách thức. Theo kết quả benchmark, Sputnik có thể vượt trội so với cuBLAS_TC chỉ khi độ thưa thớt >90% khi chiều N là 8, 16, hoặc 32. Đối với SparTA, như được mô tả trong Mục 3.1, nó tận dụng sparse tensor core [32] cho phần chính của các tính toán, không thể khai thác hiệu quả độ thưa thớt có sẵn vì sparse tensor core chỉ hỗ trợ 50% độ thưa thớt (độ thưa thớt 2:4). Nếu độ thưa thớt có sẵn cao hơn 50%, SparTA phải pad không vào ma trận thưa thớt dẫn đến truy cập bộ nhớ toàn cục dư thừa trong thời gian chạy. Bên cạnh đó, đối với các phần tử khác không không thể đáp ứng yêu cầu 2:4, một kernel dựa trên SIMT core khác được khởi chạy cho các tính toán tương ứng, dẫn đến overhead bổ sung. Do đó, Flash-LLM vượt trội so với SparTA trong các đánh giá của chúng tôi.

### 6.2 Phân tích Kernel

**Tối ưu hóa việc sử dụng GPU.** Hình 10 cho thấy việc sử dụng các đơn vị phần cứng GPU bao gồm tensor core (TC), L1 và shared memory kết hợp (L1), L2 cache (L2), và bộ nhớ toàn cục GPU (HBM) trong quá trình thực thi kernel Flash-LLM. Tất cả dữ liệu được thu thập bởi profiler NSight Compute [43]. Chúng tôi trình bày kết quả profiling của N = 16 và N = 32 dưới độ thưa thớt 90%. Chúng tôi cũng bao gồm cuBLAS ở đây như baseline dense (độ thưa thớt = 0%). cuSPARSE và Sputnik là các thiết kế dựa trên SIMT nơi tensor core không được sử dụng. Mặc dù Sputnik đạt được việc sử dụng SIMT core tốt (29.8% ở BS=16, 40.1% ở BS=32), nó đạt được hiệu suất chậm hơn nhiều so với các kernel dựa trên tensor-core khác vì SIMT core cho thấy thông lượng tính toán đỉnh thấp hơn nhiều so với tensor core. SparTA sử dụng tensor core nhưng với việc sử dụng thấp hơn cuBLAS. Đối với cuBLAS, băng thông của L2 cache và GPU DRAM bị cạn kiệt trong khi tensor core chỉ đạt 10.7% và 21.0% hiệu suất đỉnh trung bình khi chiều N là 16 và 32 tương ứng. Trong Flash-LLM, các ma trận thưa thớt ở định dạng Tiled-CSL (Mục 3.2.2) với kích thước giảm tính bằng byte. Băng thông bộ nhớ toàn cục không còn là nút thắt cổ chai với phương pháp Load-as-Sparse và Compute-as-Dense. Trung bình, việc sử dụng tensor core được cải thiện thành 24.4% và 42.6%. Khi việc sử dụng tensor core được cải thiện, nó tiêu thụ băng thông shared memory cao hơn. Ngoài ra, extract trong Hình 6c sẽ gây ra shared memory store bổ sung. Do đó, băng thông L1/shared-memory bị cạn kiệt bởi Flash-LLM, điều này ngăn cản các cải thiện hiệu suất thêm. Lưu ý rằng việc sắp xếp lại dữ liệu thưa thớt trước thời gian (Mục 4.3.3) được thiết kế để tăng hiệu quả truy cập shared memory, giúp giảm thiểu nút thắt cổ chai băng thông của shared memory. Cách giảm thêm áp lực lên shared memory có thể là công việc tương lai.

**Pipeline cân bằng cho hoạt động bộ nhớ/tensor core.** Kernel Flash-LLM chứa ba loại hoạt động chính: truy cập bộ nhớ toàn cục (Gmem), truy cập shared memory (Smem), và hoạt động tensor core (TC). Lý tưởng, ba loại hoạt động này nên được chồng chéo và thực hiện song song để sử dụng phần cứng GPU tối đa. Như được hiển thị trong Hình 11, chúng tôi đo độ trễ của mỗi loại hoạt động riêng biệt bằng cách xóa các tính toán khác trong mã nguồn. Chúng tôi cũng triển khai lại kernel GeMM dense dựa trên thiết kế của NVIDIA cutlass [42], đạt được hiệu suất tương tự so với cuBLAS. Do hiệu ứng Buckets, thời gian kernel tổng thể chủ yếu được xác định bởi các hoạt động bộ nhớ toàn cục yêu cầu thời gian thực thi dài nhất. Với phương pháp Load-as-Sparse và Compute-as-Dense, độ trễ của hoạt động Gmem được giảm đáng kể, dẫn đến cải thiện hiệu suất tổng thể. Mặc dù độ trễ Smem tăng do truy cập shared memory bổ sung yêu cầu bởi extract trong Hình 6c, nó không ngăn kernel Flash-LLM đạt được hiệu suất tốt hơn cuBLAS. Đó là một sự cân bằng tốt giữa việc sử dụng bộ nhớ toàn cục và shared memory. Hơn nữa, Flash-LLM cho thấy độ trễ tương tự với kernel GeMM dense về hoạt động tensor core vì chúng tôi không bỏ qua bất kỳ tính toán nào ở đây. Tuy nhiên, rõ ràng là hoạt động tensor core không phải là nút thắt cổ chai cho hiệu suất kernel tổng thể.

**Hiệu suất trên nhiều hình dạng MatMul hơn.** Như đã thảo luận trong Mục 3.2.1, chúng tôi chủ yếu muốn giảm thiểu sự kém hiệu quả gây ra bởi Skinny MatMul trong suy luận LLM thông thường. Để hiểu đánh giá toàn diện hơn, chúng tôi cung cấp hiệu suất kernel trên nhiều hình dạng hơn ngay cả khi hình dạng không phổ biến cho suy luận LLM. Như được hiển thị trong Hình 12, Flash-LLM trở nên chậm hơn cuBLAS nếu chiều N lớn hơn 256, lưu ý rằng dấu chân bộ nhớ của Flash-LLM vẫn nhỏ hơn so với phương pháp dense tương ứng. Lý do đằng sau điều này là hai mặt. Đầu tiên, sự kém hiệu quả của cuBLAS không còn đáng kể vì chiều N đủ lớn, điều này làm cho cuBLAS hiệu suất hơn. Thứ hai, Flash-LLM có thiết kế kernel phức tạp hơn và truy cập shared memory bổ sung, làm chậm nó một chút. Chúng tôi cũng lưu ý rằng Sputnik trở nên chậm hơn nhiều so với các thiết kế dựa trên tensor-core khác khi chiều N tăng. Mặc dù các giải pháp dựa trên SIMT-core như Sputnik có thể bỏ qua tính toán khai thác độ thưa thớt, việc tiết kiệm tính toán như vậy vẫn không thể bù đắp khoảng cách hiệu suất lớn giữa SIMT core và tensor core.

### 6.3 Đánh giá mô hình End-to-End

**Baseline.** Chúng tôi bao gồm NVIDIA FasterTransformer (FT) (git commit: 9770d30) [37] và DeepSpeed (DS) [1] như baseline, các framework suy luận tiên tiến nhất hỗ trợ song song mô hình [49] để phù hợp với các mô hình lớn mà nếu không sẽ không vừa trong bộ nhớ GPU.

**Khối lượng công việc.** Chúng tôi benchmark độ trễ suy luận end-to-end của các mô hình OPT [61], bao gồm OPT-30B, OPT-66B, và OPT-175B. Để chỉ chứa các tham số mô hình ở định dạng dense, bộ nhớ GPU 60/132/350GB được yêu cầu cho OPT-30B/66B/175B. Lưu ý rằng GPU SOTA chỉ có 80GB bộ nhớ mỗi cái, ít nhất 1/2/8 GPU được yêu cầu cho suy luận OPT-30B/66B/175B. Bên cạnh đó, bộ nhớ GPU bổ sung được yêu cầu để lưu trữ KV-Cache (tham khảo Hình 1b, với kích thước tích cực liên quan đến kích thước batch suy luận) trong thời gian chạy. Theo Bảng 1, các framework suy luận hiện có (FT-1GPU, DS-1GPU) có thể dễ dàng hết bộ nhớ GPU trong quá trình suy luận OPT-30B cho kích thước batch lớn hơn 16 nếu lưu trữ các tham số mô hình ở định dạng dense. Đối với tất cả thí nghiệm, độ dài chuỗi đầu vào/prompt là 64 và độ dài chuỗi đầu ra/sinh ra là 512.

**Metric.** Chúng tôi đề xuất metric token trên GPU-giây để chỉ ra thông lượng suy luận chuẩn hóa với việc xem xét cả thời gian thực thi và chi phí phần cứng (tức là số lượng GPU được sử dụng). Nó được tính toán với phương trình sau:

Performance = N_token / (∑_{i=1}^{N_GPU} T_i) (3)

N_token có nghĩa là số lượng token được sinh ra, trong khi N_GPU và T_i có nghĩa là số GPU và thời gian được chi cho thực thi trên GPU thứ i. Chúng tôi sử dụng metric này để đánh giá hiệu suất của hệ thống. Lưu ý rằng đối với phục vụ suy luận thời gian thực, thông lượng suy luận càng cao càng tốt một khi độ trễ suy luận nhỏ hơn một ngưỡng cụ thể.

#### 6.3.1 Nghiên cứu trường hợp: Cắt tỉa mô hình OPT-30B.

Để cho thấy rằng mô hình đã cắt tỉa có hiệu suất tương đương với mô hình gốc, chúng tôi đánh giá độ chính xác của mô hình đã cắt tỉa với OPT-30B [61] và GPT-NEOX-20B [2,5] trên tác vụ Recognizing Textual Entailment trong SuperGLUE [56]. Để đạt được độ chính xác tốt hơn, chúng tôi áp dụng phương pháp cắt tỉa phổ biến Taylor Pruning [33] để cắt tỉa các mô hình này và giữ lại một phần tư đầu và một phần tư cuối của các lớp đầu vào feedforward dense. Dựa trên đó, chúng tôi đạt được độ thưa thớt 80% trên OPT-30B và GPT-NEOX-20B với chỉ 1.44% và 0.72% giảm độ chính xác tương ứng. Cụ thể, độ chính xác giảm từ 85.55% xuống 84.11% trên OPT-30B, và từ 83.03% xuống 82.31% trên GPT-NEOX-20B.

**Kết quả.** Như được hiển thị trong Hình 13a, Flash-LLM đạt được hiệu suất cao hơn 3.4× và 3.3× so với DeepSpeed (DS) và FasterTransformer (FT) với một GPU đơn lẻ. DS và FT tối đa có thể đạt được 348 và 359 token trên GPU-giây với một GPU A100 đơn lẻ. Nếu tăng thêm kích thước batch suy luận, DS/FT sẽ hết bộ nhớ vì các tác vụ suy luận với kích thước batch lớn hơn cần nhiều bộ nhớ GPU hơn để lưu trữ cached-KV và activation. Ngược lại, Flash-LLM đạt được lên đến 1187 token trên GPU-giây ở kích thước batch 64. Đó là vì bộ nhớ được sử dụng để lưu trữ trọng số mô hình được giảm với định dạng Tiled-CSL, và do đó nhiều Cached-KV và activation hơn có thể được chứa. Chúng tôi cũng so sánh hiệu suất của Flash-LLM với DS và FT với song song mô hình hai chiều [49], với đó DS và FT có thể hỗ trợ kích thước batch 64. Như được hiển thị trong Hình 13b, FT và DS đạt được hiệu suất tương tự về token trên GPU-giây. So với DS/FT, Flash-LLM đạt được hiệu suất cao hơn 1.91×/1.75×, 1.87×/1.70×, 1.67×/1.55×, và 1.54×/1.41× ở kích thước batch 8, 16, 32, và 64 tương ứng. Việc sử dụng bộ nhớ GPU chi tiết của Flash-LLM, FT, và DS được hiển thị trong Bảng 1.

**Phân tích.** Để tìm hiểu tại sao Flash-LLM có thể đạt được hiệu suất tốt hơn, chúng tôi thực hiện phân tích thời gian của suy luận end-to-end được hiển thị trong Hình 14a. Lưu ý rằng chúng tôi thực hiện tất cả các phân tích end-to-end trong bài báo này tận dụng NSight System [44]. So với FT-2GPU (FasterTransformer với 2 GPU được sử dụng), Flash-LLM với 1 GPU có thể đạt được độ trễ suy luận chuẩn hóa thấp hơn chủ yếu vì 1) MatMul hiệu quả hơn và 2) việc loại bỏ overhead giao tiếp liên GPU.

#### 6.3.2 Nghiên cứu trường hợp: Kết quả mô hình OPT-66B

Như được hiển thị trong Hình 15a, Flash-LLM đạt được thông lượng sinh token cao hơn 3.8× và 3.6× so với DS và FT với hai GPU. DS và FT tối đa có thể đạt được 139 và 144 token trên GPU-giây với kích thước batch 16 vì chúng sẽ hết bộ nhớ nếu tăng thêm kích thước batch. Ngược lại, Flash-LLM đạt được lên đến 522 token trên GPU-giây ở kích thước batch 64. Chúng tôi cũng so sánh hiệu suất của Flash-LLM với DS-4GPU/FT-4GPU nơi Flash-LLM vẫn sử dụng hai GPU, trong khi DS-4GPU/FT-4GPU sử dụng bốn GPU để cho phép kích thước batch lớn hơn cho các baseline. So với DS-4GPU/FT-4GPU, Flash-LLM đạt được hiệu suất token trên GPU-giây cao hơn 1.85×/1.68×, 1.78×/1.61×, 1.7×/1.58×, và 1.55×/1.45× ở kích thước batch 8, 16, 32, và 64 tương ứng.

**Phân tích.** Chúng tôi thực hiện phân tích thời gian của suy luận end-to-end với FT và Flash-LLM cho OPT-66B như được hiển thị trong Hình 14b. So với FT-4GPU (FasterTransformer với 4 GPU được sử dụng), Flash-LLM với hai GPU có thể đạt được độ trễ suy luận chuẩn hóa thấp hơn FT-4GPU chủ yếu vì 1) việc giảm thời gian MatMul và 2) việc giảm overhead giao tiếp liên GPU.

#### 6.3.3 Nghiên cứu trường hợp: Kết quả và Phân tích mô hình OPT-175B

Chúng tôi thành công chạy suy luận của các mô hình OPT-175B với Flash-LLM sử dụng 4 GPU A100. Ngược lại, trọng số của OPT-175B không thể vừa trong 4 GPU A100 với các giải pháp truyền thống. Do đó, chúng tôi không hiển thị hiệu suất của FT/DS sử dụng 4 GPU ở đây vì tất cả đều hết bộ nhớ GPU. Ngoài ra, chúng tôi thất bại trong việc chạy OPT-175B với DS sử dụng 8 GPU. Hình 16a so sánh hiệu suất của Flash-LLM và FT nơi chúng tôi chỉ sử dụng 4 GPU trong khi FT sử dụng 8 GPU. So với FT-8GPU, Flash-LLM đạt được hiệu suất cao hơn 2.0×, 1.9×, 1.7×, và 1.5× ở kích thước batch 8, 16, 32, và 64 tương ứng. Như được hiển thị trong hình 16b, cả thời gian MatMul và thời gian giao tiếp liên GPU đều được giảm đáng kể sử dụng Flash-LLM.

## 7 Công trình liên quan và Thảo luận

Việc thực thi tác vụ ML song song và phân tán được sử dụng rộng rãi cho huấn luyện mô hình [6,7,19–21,24,26,27,30,31,35,46,49,60,62]. Với sự phát triển của kích thước mô hình, mọi người bắt đầu hỗ trợ suy luận LLM thông qua song song tensor [21,49] để đặt tham số lên các thiết bị phân tán [1,37]. Tuy nhiên, việc thực thi phân tán của suy luận mô hình đưa ra chi phí giao tiếp cao và đầu tư kinh tế cao. Một số công trình hỗ trợ suy luận LLM trên một GPU đơn lẻ thông qua tải xuống bộ nhớ dữ liệu lên bộ nhớ CPU và thậm chí đĩa [1,48]. Phương pháp tải xuống chỉ hoạt động cho các ứng dụng không nhạy cảm với độ trễ (ví dụ suy luận offline với kích thước batch lớn), thay vì các tác vụ suy luận trực tuyến đòi hỏi độ trễ rất thấp. Trong công trình này, chúng tôi cho phép thực thi suy luận LLM hiệu quả với ít GPU hơn thông qua thực thi SpMM hiệu quả.

Cắt tỉa mô hình là một phương pháp phổ biến để giảm số lượng tham số. Độ thưa thớt có cấu trúc là để thực thi phân phối có cấu trúc của các phần tử khác không trong quá trình cắt tỉa, thường có thể thân thiện với tăng tốc phần cứng. GPU NVIDIA Ampere [36] hỗ trợ độ thưa thớt có cấu trúc 2:4 [32] để thực thi trên tensor core. CuSPARSE [40] có thể hỗ trợ SpMM có cấu trúc trên tensor core dựa trên định dạng Blocked-ELL [13], là độ thưa thớt có cấu trúc thô nơi các tham số mô hình được cắt tỉa ở độ chi tiết của một block vuông (ví dụ 32×32). Một số công trình cắt tỉa các tham số mô hình ở độ chi tiết của vector để tạo thành định dạng thưa thớt có cấu trúc và sử dụng GPU tensor core [4,18,25]. Trong khi tensor core có thể được bật với độ thưa thớt có cấu trúc nhất định, mối quan tâm chính là các mô hình deep learning được cắt tỉa với độ thưa thớt có cấu trúc thường gặp phải suy giảm độ chính xác mô hình nghiêm trọng hơn so với độ thưa thớt không có cấu trúc [8, 12, 14, 16, 51, 54].

Độ thưa thớt không có cấu trúc là cắt tỉa các phần tử mà không tạo thành phân phối có cấu trúc, thường khó tăng tốc trên các kiến trúc phần cứng hiện đại. STOREL [47] và TACO [23] là các thiết kế dựa trên CPU có thể hỗ trợ SpMM với độ thưa thớt không có cấu trúc. Thay vào đó, Flash-LLM chủ yếu tập trung vào SpMM dựa trên GPU với độ thưa thớt không có cấu trúc, vì GPU được sử dụng rộng rãi hơn cho các tác vụ deep-learning quy mô lớn vì nó thường có băng thông bộ nhớ cao hơn, thông lượng tính toán, và hiệu quả năng lượng. Phương pháp điển hình để thực thi SpMM không có cấu trúc trên GPU là thông qua SIMT core, ví dụ cuSPARSE [40], ASpT [17], và Sputnik [10]. Dưới mức độ thưa thớt vừa phải (<90%), Sputnik vượt trội đáng kể so với cuSPARSE và ASpT, nhưng nó gặp khó khăn để đánh bại phương pháp dense tương ứng cuBLAS [39] vì nó không thể sử dụng tensor core. TC-GNN [57] hỗ trợ độ thưa thớt không có cấu trúc với tensor core, được tùy chỉnh cho GNN nơi tỷ lệ độ thưa thớt cực kỳ cao (ví dụ >99%) và không hiệu quả cho các mô hình sinh yêu cầu độ thưa thớt mức độ vừa phải. SparTA [65] đề xuất sử dụng cả sparse tensor core [32] và SIMT core để hỗ trợ độ thưa thớt không có cấu trúc bằng cách chia ma trận thưa thớt gốc thành ma trận thưa thớt có cấu trúc 2:4 để thực thi tensor core (bởi cuSPARSELt [41]) và ma trận thưa thớt không có cấu trúc để thực thi SIMT core (bởi Sputnik [10]). Tuy nhiên, nếu tỷ lệ thưa thớt cao, nó phải pad quá nhiều số không vào ma trận thưa thớt 2:4. Bên cạnh đó, nếu có quá nhiều phần tử khác không không thể đáp ứng yêu cầu 2:4, kernel SIMT sẽ gây ra độ trễ cao và làm chậm xử lý tổng thể. Flash-LLM hỗ trợ độ thưa thớt mức độ vừa phải trên tensor core một cách hiệu quả và không yêu cầu phân phối giống 2:4. SparseTIR [59] hỗ trợ độ thưa thớt không có cấu trúc với tensor core bằng cách chia ma trận thưa thớt thành vector cột 8×1 và bỏ qua các vector chỉ chứa số không. Nó không thể hoạt động tốt dưới độ thưa thớt vừa phải (ví dụ 80%) vì rất ít vector có thể được bỏ qua. Nó không thể vượt trội so với baseline dense cuBLAS cho đến khi độ thưa thớt cao hơn 95%, trong khi Flash-LLM có thể vượt trội so với cuBLAS từ độ thưa thớt 60%.

Cắt tỉa dựa trên huấn luyện lại [15,29] có thể đạt được độ thưa thớt mức độ vừa phải với độ chính xác tốt, trong khi cắt tỉa sau huấn luyện [8] chỉ có thể đạt được tỷ lệ thưa thớt khá thấp. Flash-LLM nhằm tối ưu hóa SpMM với độ thưa thớt mức độ vừa phải (ví dụ 60%-90%) được sinh ra với cắt tỉa dựa trên huấn luyện lại. Cắt tỉa dựa trên huấn luyện lại thường tiêu thụ chi phí fine-tuning cao, do đó trở thành một hạn chế của Flash-LLM.

Lượng tử hóa mô hình [16] là một phương pháp khác để giảm bộ nhớ và tính toán cho các mô hình ML, bằng cách chuyển đổi kiểu dữ liệu thành bit thấp hơn (ví dụ 8-bit, 4-bit) [9,25]. Cắt tỉa mô hình và lượng tử hóa là hai phương pháp trực giao và bổ sung cho nén mô hình. Bài báo này chủ yếu tập trung vào hỗ trợ cắt tỉa mô hình, trực giao với lượng tử hóa mô hình.

## 8 Kết luận

Chúng tôi đề xuất Flash-LLM, một thư viện cho suy luận mô hình sinh lớn hiệu quả thông qua độ thưa thớt không có cấu trúc với tensor core. Chúng tôi quan sát thấy rằng MatMul trong suy luận mô hình sinh thường mỏng, và bị ràng buộc bởi truy cập bộ nhớ off-chip. Chúng tôi đề xuất phương pháp Load-as-Sparse và Compute-as-Dense cho tensor core SpMM, giảm dấu chân bộ nhớ toàn cục, giải quyết nút thắt cổ chai truy cập bộ nhớ, và chấp nhận tính toán dư thừa của MatMul mỏng. Chúng tôi đề xuất một pipeline phần mềm hiệu quả cho SpMM không có cấu trúc với tensor core, hiệu quả tận dụng tài nguyên on-chip cho trích xuất dữ liệu thưa thớt và điều phối trích xuất dữ liệu thưa thớt, tải dữ liệu dense, và tính toán tensor core theo cách chồng chéo. Flash-LLM vượt trội so với cuBLAS/Sputnik/SparTA lần lượt 1.4×/3.6×/1.4×, 1.7×/3.0×/1.4×, 2.1×/2.0×/1.6× dưới độ thưa thớt 70%, 80% và 90%. Chúng tôi tích hợp kernel Flash-LLM vào FasterTransformer cho suy luận mô hình sinh end-to-end. Đối với token trên GPU-giây, Flash-LLM đạt được cải thiện lên đến 3.8× và 3.6× so với DeepSpeed và FasterTransformer trên các mô hình OPT-30B/66B/175B với chi phí suy luận thấp hơn đáng kể.

## Tài liệu tham khảo

[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. DeepSpeed-inference: enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. 1–15.

[2] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autoregressive Language Model. In Proceedings of the ACL Workshop on Challenges & Perspectives in Creating Large Language Models. https://arxiv.org/abs/2204.06745

[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.

[4] Zhaodong Chen, Zheng Qu, Liu Liu, Yufei Ding, and Yuan Xie. 2021. Efficient tensor core-based GPU kernels for structured sparsity under reduced precision. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 1–14.

[5] EleutherAI. 2022. GPT-NeoX-20B. https://huggingface.co/EleutherAI/gpt-neox-20b

[6] Hugging Face. 2023. Model Parallelism. https://huggingface.co/docs/transformers/v4.15.0/parallelism

[7] Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, and Wei Lin. 2021. DAPPLE: A pipelined data parallel approach for training large models. In Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. 431–445.

[8] Elias Frantar and Dan Alistarh. 2023. Massive Language Models Can Be Accurately Pruned in One-Shot. arXiv preprint arXiv:2301.00774 (2023).

[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. arXiv preprint arXiv:2210.17323 (2022).

[10] Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. 2020. Sparse gpu kernels for deep learning. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–14.

[11] Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. 2020. sputnik github. https://github.com/google-research/sputnik/

[12] Aidan N Gomez, Ivan Zhang, Siddhartha Rao Kamalakara, Divyam Madaan, Kevin Swersky, Yarin Gal, and Geoffrey E Hinton. 2019. Learning sparse networks using targeted dropout. arXiv preprint arXiv:1905.13678 (2019).

[13] Scott Gray, Alec Radford, and Diederik P Kingma. 2017. Gpu kernels for block-sparse weights. arXiv preprint arXiv:1711.09224 3 (2017), 2.

[14] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149 (2015).

[15] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural network. Advances in neural information processing systems 28 (2015).

[16] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. 2021. Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks. J. Mach. Learn. Res. 22, 241 (2021), 1–124.

[17] Changwan Hong, Aravind Sukumaran-Rajam, Israt Nisa, Kunal Singh, and P Sadayappan. 2019. Adaptive sparse tiling for sparse matrix multiplication. In Proceedings of the 24th Symposium on Principles and Practice of Parallel Programming. 300–314.

[18] Guyue Huang, Haoran Li, Minghai Qin, Fei Sun, Yufei Ding, and Yuan Xie. 2022. Shfl-BW: Accelerating Deep Neural Network Inference with Tensor-Core Aware Weight Pruning. In Proceedings of the 59th ACM/IEEE Design Automation Conference (San Francisco, California) (DAC '22). Association for Computing Machinery, New York, NY, USA, 1153–1158. https://doi.org/10.1145/3489517.3530588

[19] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems 32 (2019).

[20] Xianyan Jia, Le Jiang, Ang Wang, Wencong Xiao, Ziji Shi, Jie Zhang, Xinyuan Li, Langshi Chen, Yong Li, Zhen Zheng, Xiaoyong Liu, and Wei Lin. 2022. Whale: Efficient Giant Model Training over Heterogeneous GPUs. In 2022 USENIX Annual Technical Conference, USENIX ATC 2022, Carlsbad, CA, USA, July 11-13, 2022, Jiri Schindler and Noa Zilberman (Eds.). USENIX Association, 673–688.

[21] Zhihao Jia, Matei Zaharia, and Alex Aiken. 2019. Beyond Data and Model Parallelism for Deep Neural Networks. Proceedings of Machine Learning and Systems 1 (2019), 1–13.

[22] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT. 4171–4186.

[23] Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe. 2017. The Tensor Algebra Compiler. Proc. ACM Program. Lang. 1, OOPSLA, Article 77 (oct 2017), 29 pages. https://doi.org/10.1145/3133901

[24] Alexandros Koliousis, Pijika Watcharapichat, Matthias Weidlich, Luo Mai, Paolo Costa, and Peter Pietzuch. 2019. CROSSBOW: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers. Proceedings of the VLDB Endowment 12, 11 (2019).

[25] Shigang Li, Kazuki Osawa, and Torsten Hoefler. 2022. Efficient quantized sparse matrix operations on tensor cores. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis. 1–15.

[26] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. 2020. PyTorch Distributed: Experiences on Accelerating Data Parallel Training. Proceedings of the VLDB Endowment 13, 12 (2020).

[27] Youjie Li, Amar Phanishayee, Derek Murray, Jakub Tarnawski, and Nam Sung Kim. 2022. Harmony: Overcoming the Hurdles of GPU Memory Capacity to Train Massive DNN Models on Commodity Servers. Proc. VLDB Endow. 15, 11 (jul 2022), 2747–2760.

[28] Mingbao Lin, Yuxin Zhang, Yuchao Li, Bohong Chen, Fei Chao, Mengdi Wang, Shen Li, Yonghong Tian, and Rongrong Ji. 2022. 1xn pattern for pruning convolutional neural networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022).

[29] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. 2018. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270 (2018).

[30] Xupeng Miao, Xiaonan Nie, Yingxia Shao, Zhi Yang, Jiawei Jiang, Lingxiao Ma, and Bin Cui. 2021. Heterogeneity-aware distributed machine learning training via partial reduce. In Proceedings of the 2021 International Conference on Management of Data. 2262–2270.

[31] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui. 2022. Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism. Proc. VLDB Endow. 16, 3 (nov 2022), 470–479.

[32] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. 2021. Accelerating Sparse Deep Neural Networks. arXiv:2104.08378 [cs.LG]

[33] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. 2019. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 11264–11272.

[34] Avanika Narayan, Ines Chami, Laurel Orr, and Christopher Ré. 2022. Can Foundation Models Wrangle Your Data? Proc. VLDB Endow. 16, 4 (dec 2022), 738–746.

[35] Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma, Gang Cao, and Bin Cui. 2023. FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement. arXiv preprint arXiv:2304.03946 (2023).

[36] NVIDIA. 2020. NVIDIA A100 Tensor Core GPU Architecture. https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf

[37] NVIDIA. 2022. NVIDIA Faster-Transformer. https://github.com/NVIDIA/FasterTransformer

[38] NVIDIA. 2022. NVIDIA H100 Tensor Core GPU Architecture. https://www.hpctech.co.jp/catalog/gtc22-whitepaper-hopper_v1.01.pdf

[39] NVIDIA. 2023. cuBLAS Docs. https://docs.nvidia.com/cuda/cublas/index.html

[40] NVIDIA. 2023. cuSPARSE Library. https://docs.nvidia.com/cuda/cusparse/index.html

[41] NVIDIA. 2023. cuSPARSELt Library. https://docs.nvidia.com/cuda/cusparselt/

[42] NVIDIA. 2023. CUTLASS 3.2. https://github.com/NVIDIA/cutlass

[43] NVIDIA. 2023. Nsight Compute Profiling Guide. https://docs.nvidia.com/nsight-compute/ProfilingGuide/#introduction

[44] NVIDIA. 2023. Nsight System. https://developer.nvidia.com/nsight-systems

[45] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.

[46] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–16.

[47] Maximilian Schleich, Amir Shaikhha, and Dan Suciu. 2023. Optimizing Tensor Programs on Flexible Storage. Proceedings of the ACM on Management of Data 1, 1 (2023), 1–27.

[48] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. 2023. High-throughput generative inference of large language models with a single gpu. arXiv preprint arXiv:2303.06865 (2023).

[49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).

[50] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro. 2022. Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. arXiv:2201.11990 [cs.CL]

[51] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2023. A Simple and Effective Pruning Approach for Large Language Models. arXiv:2306.11695 [cs.CL]

[52] Immanuel Trummer. 2022. From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management. Proc. VLDB Endow. 15, 12 (aug 2022), 3770–3773.

[53] Shreshth Tuli, Giuliano Casale, and Nicholas R. Jennings. 2022. TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data. Proc. VLDB Endow. 15, 6 (feb 2022), 1201–1214.

[54] Karen Ullrich, Edward Meeds, and Max Welling. 2017. Soft weight-sharing for neural network compression. arXiv preprint arXiv:1702.04008 (2017).

[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).

[56] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems 32 (2019).

[57] Yuke Wang, Boyuan Feng, Zheng Wang, and Yufei Ding. 2023. TC-GNN: Accelerating Sparse Graph Neural Network Computation Via Dense Tensor Core on GPUs. arXiv:2112.02052 [cs.LG]

[58] Samuel Williams, Andrew Waterman, and David Patterson. 2009. Roofline: an insightful visual performance model for multicore architectures. Commun. ACM 52, 4 (2009), 65–76.

[59] Zihao Ye, Ruihang Lai, Junru Shao, Tianqi Chen, and Luis Ceze. 2023. SparseTIR: Composable abstractions for sparse compilation in deep learning. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3. 660–678.

[60] Xiaodong Yi, Shiwei Zhang, Ziyue Luo, Guoping Long, Lansong Diao, Chuan Wu, Zhen Zheng, Jun Yang, and Wei Lin. 2020. Optimizing distributed training deployment in heterogeneous GPU clusters. In CoNEXT '20: The 16th International Conference on emerging Networking EXperiments and Technologies, Barcelona, Spain, December, 2020, Dongsu Han and Anja Feldmann (Eds.). ACM, 93–107.

[61] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models. arXiv:2205.01068 [cs.CL]

[62] Zhen Zhang, Shuai Zheng, Yida Wang, Justin Chiu, George Karypis, Trishul Chilimbi, Mu Li, and Xin Jin. 2022. MiCS: near-linear scaling for training gigantic model on public cloud. Proceedings of the VLDB Endowment 16, 1 (2022), 37–50.

[63] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P. Xing, Joseph E. Gonzalez, and Ion Stoica. 2022. Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning. In 16th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2022, Carlsbad, CA, USA, July 11-13, 2022, Marcos K. Aguilera and Hakim Weatherspoon (Eds.). USENIX Association, 559–578.

[64] Ningxin Zheng. 2022. SparTA github. https://github.com/microsoft/SparTA/tree/sparta_artifact

[65] Ningxin Zheng, Bin Lin, Quanlu Zhang, Lingxiao Ma, Yuqing Yang, Fan Yang, Yang Wang, Mao Yang, and Lidong Zhou. 2022. {SparTA}: {Deep-Learning} Model Sparsity via {Tensor-with-Sparsity-Attribute}. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 213–232.
